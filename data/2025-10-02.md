<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 28]
- [cs.CR](#cs.CR) [Total: 17]
- [cs.AI](#cs.AI) [Total: 60]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [PBFD and PDFD: Formally Defined and Verified Methodologies and Empirical Evaluation for Scalable Full-Stack Software Engineering](https://arxiv.org/abs/2510.00002)
*Dong Liu*

Main category: cs.SE

TL;DR: 本文介绍了PBFD和PDFD两种形式化验证的软件开发方法，通过图论建模确保结构正确性，并提出了TLE编码方案实现常数时间更新。在8年企业部署中验证了其性能优势。


<details>
  <summary>Details</summary>
Motivation: 弥合形式化方法与实际开发实践之间的差距，为工业级全栈软件开发提供可扩展且结构正确的方法。

Method: 基于分层有向图的PBFD和PDFD方法，使用统一状态机和CSP进行形式化，提出TLE三层次封装编码方案。

Result: 企业部署显示开发速度比Salesforce OmniScript快20倍以上，查询性能比传统关系模型快7-8倍。

Conclusion: PBFD和PDFD建立了将形式化验证融入实际软件开发的透明框架，所有规范和实现均已开源。

Abstract: This paper introduces Primary Breadth-First Development (PBFD) and Primary
Depth-First Development (PDFD), two formally defined and verified methodologies
for scalable, industrial-grade full-stack software engineering. These
approaches bridge a longstanding gap between formal methods and real-world
development practice by enforcing structural correctness through
graph-theoretic modeling. Unlike prior graph-based approaches, PBFD and PDFD
operate over layered directed graphs and are formalized using unified state
machines and Communicating Sequential Processes (CSP) to ensure critical
properties, including bounded-refinement termination and structural
completeness. To coordinate hierarchical data at scale, we propose Three-Level
Encapsulation (TLE) - a novel, bitmask-based encoding scheme that delivers
provably constant-time updates. TLE's formal guarantees underpin PBFD's
industrial-scale performance and scalability. PBFD was empirically validated
through an eight-year enterprise deployment, demonstrating over 20x faster
development than Salesforce OmniScript and 7-8x faster query performance
compared to conventional relational models. Additionally, both methodologies
are supported by open-source MVPs, with PDFD's implementation conclusively
demonstrating its correctness-first design principles. Together, PBFD and PDFD
establish a reproducible, transparent framework that integrates formal
verification into practical software development. All formal specifications,
MVPs, and datasets are publicly available to foster academic research and
industrial-grade adoption.

</details>


### [2] [Semantic Zoom and Mini-Maps for Software Cities](https://arxiv.org/abs/2510.00003)
*Malte Hansen,Jens Bamberg,Noe Baumann,Wilhelm Hasselbring*

Main category: cs.SE

TL;DR: 该论文提出了两种解决3D软件城市可视化可扩展性挑战的方法：语义缩放和迷你地图，并在ExplorViz工具中实现和评估。


<details>
  <summary>Details</summary>
Motivation: 随着可视化中显示数据量的增加，可视化本身可能变得难以理解，需要解决视觉可扩展性问题。

Method: 1. 语义缩放：基于虚拟相机与视觉对象的距离改变软件景观的图形表示
2. 迷你地图：添加二维俯视投影的迷你地图
在基于Web的ExplorViz软件可视化工具中实现

Result: 两个独立的用户研究表明，语义缩放和迷你地图都是有用的补充，特别适用于大型软件景观和协作软件探索，显示出良好的可用性。

Conclusion: 语义缩放和迷你地图能有效提升3D软件城市的视觉可扩展性，但实现中存在一些需要改进的缺点。

Abstract: Software visualization tools can facilitate program comprehension by
providing visual metaphors, or abstractions that reduce the amount of textual
data that needs to be processed mentally. One way they do this is by enabling
developers to build an internal representation of the visualized software and
its architecture. However, as the amount of displayed data in the visualization
increases, the visualization itself can become more difficult to comprehend.
The ability to display small and large amounts of data in visualizations is
called visual scalability.
  In this paper, we present two approaches to address the challenge of visual
scalability in 3D software cities. First, we present an approach to semantic
zoom, in which the graphical representation of the software landscape changes
based on the virtual camera's distance from visual objects. Second, we augment
the visualization with a miniature two-dimensional top-view projection called
mini-map. We demonstrate our approach using an open-source implementation in
our software visualization tool ExplorViz. ExplorViz is web-based and uses the
3D city metaphor, focusing on live trace visualization.
  We evaluated our approaches in two separate user studies. The results
indicate that semantic zoom and the mini-map are both useful additions. User
feedback indicates that semantic zoom and mini-maps are especially useful for
large software landscapes and collaborative software exploration. The studies
indicate a good usability of our implemented approaches. However, some
shortcomings in our implementations have also been discovered, to be addressed
in future work.
  Video URL: https://youtu.be/LYtUeWvizjU

</details>


### [3] [HTML Structure Exploration in 3D Software Cities](https://arxiv.org/abs/2510.00004)
*Malte Hansen,David Moreno-Lumbreras,Wilhelm Hasselbring*

Main category: cs.SE

TL;DR: 本文介绍了ExplorViz工具的扩展功能，在3D软件可视化中嵌入Web视图，用于可视化HTML结构和DOM，并通过用户研究评估了该方法的潜力。


<details>
  <summary>Details</summary>
Motivation: 大型软件系统通常提供Web界面，但现有的软件可视化工具往往忽略这些界面。为了帮助用户更好地理解和探索软件系统的行为，需要将Web界面集成到可视化工具中。

Method: 在基于Web的实时追踪软件可视化工具ExplorViz中，添加了嵌入式Web视图，用于检测应用程序。通过3D可视化展示HTML结构，特别是在同源上下文中可视化文档对象模型(DOM)。

Result: 通过初步用户研究评估了可视化方法，研究结果揭示了该方法的潜在用例、优势和不足。

Conclusion: 基于研究结果，提出了进一步研究方向，以支持Web界面的可视化探索，并探索软件城市与HTML结构组合可视化的用例。

Abstract: Software visualization, which uses data from dynamic program analysis, can
help to explore and understand the behavior of software systems. It is common
that large software systems offer a web interface for user interaction.
Usually, available web interfaces are not regarded in software visualization
tools. This paper introduces additions to the web-based live tracing software
visualization tool ExplorViz: We add an embedded web view for instrumented
applications in the 3D visualization to ease interaction with the given
applications and enable the exploration of the thereby displayed HTML content.
Namely, the Document Object Model (DOM) is visualized via a three-dimensional
representation of the HTML structure in same-origin contexts.
  Our visualization approach is evaluated in a preliminary user study. The
study results give insights into the potential use cases, benefits, and
shortcomings of our implemented approach. Based on our study results, we
propose directions for further research to support the visual exploration of
web interfaces and explore use cases for the combined visualization of software
cities and HTML structure.
  Video URL: https://youtu.be/wBWKlbvzOOE

</details>


### [4] [VibeCodeHPC: An Agent-Based Iterative Prompting Auto-Tuner for HPC Code Generation Using LLMs](https://arxiv.org/abs/2510.00031)
*Shun-ichiro Hayashi,Koki Morita,Daichi Mukunoki,Tetsuya Hoshino,Takahiro Katagiri*

Main category: cs.SE

TL;DR: VibeCodeHPC是一个基于多智能体LLM的HPC程序自动调优系统，通过角色分配和迭代提示优化来调优程序


<details>
  <summary>Details</summary>
Motivation: 开发一个能够自动调优HPC程序的系统，通过多智能体协作提高代码生成质量和效率

Method: 使用四角色多智能体配置（项目经理、系统工程师、程序员、持续交付），结合动态智能体部署和活动监控功能

Result: 在CPU到GPU代码转换案例中，多智能体配置相比单智能体实现了更高质量的代码生成，并能更有效地识别需求违规问题

Conclusion: 多智能体LLM系统在HPC程序调优中具有优势，动态部署和监控功能增强了协作效果

Abstract: We propose VibeCodeHPC, an automatic tuning system for HPC programs based on
multi-agent LLMs for code generation. VibeCodeHPC tunes programs through
multi-agent role allocation and iterative prompt refinement. We describe the
system configuration with four roles: Project Manager (PM), System Engineer
(SE), Programmer (PG), and Continuous Delivery (CD). We introduce dynamic agent
deployment and activity monitoring functions to facilitate effective
multi-agent collaboration. In our case study, we convert and optimize CPU-based
matrix-matrix multiplication code written in C to GPU code using CUDA. The
multi-agent configuration of VibeCodeHPC achieved higher-quality code
generation per unit time compared to a solo-agent configuration. Additionally,
the dynamic agent deployment and activity monitoring capabilities facilitated
more effective identification of requirement violations and other issues.

</details>


### [5] [A Scalable Framework for Safety Assurance of Self-Driving Vehicles based on Assurance 2.0](https://arxiv.org/abs/2510.00092)
*Shufeng Chen,Mariat James Elizebeth,Robab Aghazadeh Chakherlou,Xingyu Zhao,Eric Barbier,Siddartha Khastgir,Paul Jennings*

Main category: cs.SE

TL;DR: 本文提出了基于Assurance 2.0范式的分解框架，通过结构化模板和三层次分解策略，为端到端AI自动驾驶车辆开发提供完整的安全论证集和证据度量方法。


<details>
  <summary>Details</summary>
Motivation: 解决Assurance 2.0在置信度测量、残余疑虑管理、自动化支持以及实际处理反驳和确认偏差方面的局限性，为复杂自适应系统提供更严谨的保证方法。

Method: 采用三层次分解策略：顶层将SDV开发分为需求工程、验证与验证、部署后三个阶段；每个阶段按产品开发生命周期进一步分解；使用改进的5M1E模型（人、机、方法、材料、测量、环境）进行多维度分析。

Result: 开发了结构化模板和分解框架，支持安全声明、证据和潜在反驳的细粒度可追溯性，并通过自动驾驶车辆案例研究验证了方法的可行性。

Conclusion: 该分解框架能够识别完整的安全论证集并度量相应证据，增强了Assurance 2.0在复杂系统安全保证中的实用性和有效性。

Abstract: Assurance 2.0 is a modern framework developed to address the assurance
challenges of increasingly complex, adaptive, and autonomous systems. Building
on the traditional Claims-Argument-Evidence (CAE) model, it introduces reusable
assurance theories and explicit counterarguments (defeaters) to enhance rigor,
transparency, and adaptability. It supports continuous, incremental assurance,
enabling innovation without compromising safety. However, limitations persist
in confidence measurement, residual doubt management, automation support, and
the practical handling of defeaters and confirmation bias. This paper presents
\textcolor{black}{a set of decomposition frameworks to identify a complete set
of safety arguments and measure their corresponding evidence.} Grounded in the
Assurance 2.0 paradigm, the framework is instantiated through a structured
template and employs a three-tiered decomposition strategy. \textcolor{black}{A
case study regarding the application of the decomposition framework in the
end-to-end (E2E) AI-based Self-Driving Vehicle (SDV) development is also
presented in this paper.} At the top level, the SDV development is divided into
three critical phases: Requirements Engineering (RE), Verification and
Validation (VnV), and Post-Deployment (PD). Each phase is further decomposed
according to its Product Development Lifecycle (PDLC). To ensure comprehensive
coverage, each PDLC is analyzed using an adapted 5M1E model (Man, Machine,
Method, Material, Measurement, and Environment). Originally developed for
manufacturing quality control, the 5M1E model is reinterpreted and contextually
mapped to the assurance domain. This enables a multi-dimensional decomposition
that supports fine-grained traceability of safety claims, evidence, and
potential defeaters.

</details>


### [6] [Container Orchestration Patterns for Optimizing Resource Use](https://arxiv.org/abs/2510.00197)
*Diogo Maia,Filipe Correia,André Restivo,Paulo Queiroz*

Main category: cs.SE

TL;DR: 该论文提出了三种服务编排资源优化模式：抢占式调度、服务平衡和垃圾回收，以解决服务编排中的挑战并促进最佳实践的采用。


<details>
  <summary>Details</summary>
Motivation: 服务编排在基于服务的架构中仍然具有挑战性，特别是对于新手而言。现有的编排技术资源缺乏清晰度和标准化，使得最佳实践难以实施，限制了在软件行业中的采用。

Method: 通过分析现有文献和工具，识别常见的编排实践，并基于发现定义了三种关键的编排资源优化模式。

Result: 定义了三种编排资源优化模式：抢占式调度（为高优先级服务分配足够资源）、服务平衡（重新组织节点以改善资源使用）和垃圾回收（创建清理机制以优化系统资源使用）。

Conclusion: 这些模式作为改进编排实践和促进在基于服务的架构中更广泛采用的基础要素。

Abstract: Service-based architectures provide substantial benefits, yet service
orchestration remains a challenge, particularly for newcomers. While various
resources on orchestration techniques exist, they often lack clarity and
standardization, making best practices difficult to implement and limiting
their adoption within the software industry.
  To address this gap, we analyzed existing literature and tools to identify
common orchestration practices. Based on our findings, we define three key
orchestration resource optimization patterns: {\sc Preemptive Scheduling}, {\sc
Service Balancing}, and {\sc Garbage Collection}. {\sc Preemptive Scheduling}
allows the allocation of sufficient resources for services of higher priority
in stressful situations, while {\sc Service Balancing} enables a restructuring
of the nodes to allow better resource usage. To end, {\sc Garbage Collection}
creates cleanup mechanisms to better understand the system's resource usage and
optimize it. These patterns serve as foundational elements for improving
orchestration practices and fostering broader adoption in service-based
architectures.

</details>


### [7] [Which Programming Language and Model Work Best With LLM-as-a-Judge For Code Retrieval?](https://arxiv.org/abs/2510.00324)
*Lucas Roberts,Denisa Roberts*

Main category: cs.SE

TL;DR: 本研究探讨使用大语言模型进行代码搜索，比较不同检索表示方法、编程语言和LLM对人类标注的影响，发现检索器和编程语言之间存在亲和性，并提出使用转译器扩展代码搜索基准数据集的方法。


<details>
  <summary>Details</summary>
Motivation: 代码搜索是重要的信息检索应用，但由于需要编程语言专业知识和软件工程领域知识，人工标注成本高昂，导致该领域发展滞后。

Method: 使用大语言模型检索函数级代码并生成标注，比较稀疏表示与语义表示、不同编程语言（C、Java、Javascript、Go、Python）以及不同LLM的影响，并利用转译器扩展基准数据集。

Result: 发现选择的检索器和编程语言存在亲和性，可以改善人类与AI相关性判断的一致性；不同编程语言在表示方法（稀疏vs语义）上存在差异，影响人类与AI相关性判断的一致性。

Conclusion: 通过转译器可以扩展代码搜索基准数据集，在案例研究中，人类-AI相关性一致性率基本匹配研究中的（最差情况）人类-人类一致性。

Abstract: Code search is an important information retrieval application. Benefits of
better code search include faster new developer on-boarding, reduced software
maintenance, and ease of understanding for large repositories. Despite
improvements in search algorithms and search benchmarks, the domain of code
search has lagged behind. One reason is the high cost of human annotation for
code queries and answers. While humans may annotate search results in general
text QA systems, code annotations require specialized knowledge of a
programming language (PL), as well as domain specific software engineering
knowledge. In this work we study the use of Large Language Models (LLMs) to
retrieve code at the level of functions and to generate annotations for code
search results. We compare the impact of the retriever representation (sparse
vs. semantic), programming language, and LLM by comparing human annotations
across several popular languages (C, Java, Javascript, Go, and Python). We
focus on repositories that implement common data structures likely to be
implemented in any PLs. For the same human annotations, we compare several
LLM-as-a-Judge models to evaluate programming language and other affinities
between LLMs. We find that the chosen retriever and PL exhibit affinities that
can be leveraged to improve alignment of human and AI relevance determinations,
with significant performance implications. We also find differences in
representation (sparse vs. semantic) across PLs that impact alignment of human
and AI relevance determinations. We propose using transpilers to bootstrap
scalable code search benchmark datasets in other PLs and in a case study
demonstrate that human-AI relevance agreement rates largely match the (worst
case) human-human agreement under study. The application code used in this work
is available at \href{https://github.com/rlucas7/code-searcher/}{this github
repo}.

</details>


### [8] [Vibe Coding in Practice: Motivations, Challenges, and a Future Outlook -- a Grey Literature Review](https://arxiv.org/abs/2510.00328)
*Ahmed Fawzy,Amjed Tahir,Kelly Blincoe*

Main category: cs.SE

TL;DR: AI代码生成工具通过直觉和试错的方式让用户进行"氛围编程"，虽然加速了开发过程，但导致了速度与质量的权衡悖论，产生了无法调试代码的脆弱开发者群体。


<details>
  <summary>Details</summary>
Motivation: 尽管AI代码生成工具被广泛采用，但缺乏对用户为何进行氛围编程、体验如何以及如何处理质量保证的系统性研究。

Method: 对101个从业者资料进行系统性灰色文献综述，提取了518个关于氛围编程实践、挑战和限制的第一手行为描述。

Result: 分析揭示了速度-质量权衡悖论：氛围编程者追求速度和可访问性，常体验到"即时成功和流畅感"，但大多数认为生成的代码快速但有缺陷。质量保证实践常被忽视，许多用户跳过测试、直接使用模型输出或委托AI工具检查。

Conclusion: 氛围编程降低了门槛并加速了原型设计，但以可靠性和可维护性为代价，产生了无法调试产品的脆弱开发者群体，这对工具设计者和开发团队具有重要启示。

Abstract: AI code generation tools are transforming software development, especially
for novice and non-software developers, by enabling them to write code and
build applications faster and with little to no human intervention. Vibe coding
is the practice where users rely on AI code generation tools through intuition
and trial-and-error without necessarily understanding the underlying code.
Despite widespread adoption, no research has systematically investigated why
users engage in vibe coding, what they experience while doing so, and how they
approach quality assurance (QA) and perceive the quality of the AI-generated
code. To this end, we conduct a systematic grey literature review of 101
practitioner sources, extracting 518 firsthand behavioral accounts about vibe
coding practices, challenges, and limitations. Our analysis reveals a
speed-quality trade-off paradox, where vibe coders are motivated by speed and
accessibility, often experiencing rapid ``instant success and flow'', yet most
perceive the resulting code as fast but flawed. QA practices are frequently
overlooked, with many skipping testing, relying on the models' or tools'
outputs without modification, or delegating checks back to the AI code
generation tools. This creates a new class of vulnerable software developers,
particularly those who build a product but are unable to debug it when issues
arise. We argue that vibe coding lowers barriers and accelerates prototyping,
but at the cost of reliability and maintainability. These insights carry
implications for tool designers and software development teams. Understanding
how vibe coding is practiced today is crucial for guiding its responsible use
and preventing a broader QA crisis in AI-assisted development.

</details>


### [9] [Beyond Pass/Fail: The Story of Learning-Based Testing](https://arxiv.org/abs/2510.00450)
*Sheikh Md. Mushfiqur Rahman,Nasir Eisty*

Main category: cs.SE

TL;DR: 基于学习的测试（LBT）通过结合学习和测试过程，利用主动学习推断被测系统模型，仅需少量初始测试用例即可实现大规模复杂程序的测试，确保行为充分性。


<details>
  <summary>Details</summary>
Motivation: 传统测试方法在处理大规模复杂程序时面临挑战，LBT旨在通过主动学习技术提高测试效率和覆盖率，减少对大量初始测试用例的依赖。

Method: 采用主动学习技术逐步生成测试用例，通过测试过程推断被测系统的行为模型，结合理论框架、工具库和案例研究进行系统文献综述。

Result: LBT在过程和反应式程序测试中显示出有效性，工业案例研究证明了其在商业软件测试中的潜力和实际应用价值。

Conclusion: LBT作为一种有前景的软件测试技术，具有未充分利用的潜力，能为实践者和研究社区带来显著益处，值得进一步研究和应用。

Abstract: Learning-Based Testing (LBT) merges learning and testing processes to achieve
both testing and behavioral adequacy. LBT utilizes active learning to infer the
model of the System Under Test (SUT), enabling scalability for large and
complex programs by requiring only a minimal set of initial test cases. The
core principle of LBT is that the SUT's behavior can be thoroughly inferred by
progressively generating test cases and subjecting the SUT to testing, thereby
ensuring comprehensive testing. Despite being in its early stages, LBT has a
solid foundation of theoretical research demonstrating its efficacy in testing
both procedural and reactive programs. This paper provides a systematic
literature review of various LBT implementations across different program types
and evaluates the current state of research in this field. We explore diverse
theoretical frameworks, existing tools, and libraries within the LBT domain to
illustrate the concept's evolution and current research status. Additionally,
we examine case studies involving the application of LBT tools in industrial
settings, highlighting their potential and effectiveness in commercial software
testing. This systematic literature review aims to offer researchers a
comprehensive perspective on the inception and development of LBT, presenting
it as a promising technique in software testing. By unveiling LBT's
underutilized potential, this paper seeks to significantly benefit the
practitioners and research community.

</details>


### [10] [Analyzing Latent Concepts in Code Language Models](https://arxiv.org/abs/2510.00476)
*Arushi Sharma,Vedant Pungliya,Christopher J. Quinn,Ali Jannesari*

Main category: cs.SE

TL;DR: 提出了Code Concept Analysis (CoCoA)框架，通过聚类上下文标记嵌入来揭示代码语言模型中的词汇、句法和语义结构，并整合局部归因方法生成概念基础的解释。


<details>
  <summary>Details</summary>
Motivation: 解释代码语言模型的内部行为对于需要信任、透明度和语义鲁棒性的应用至关重要，但目前仍是一个挑战。

Method: 提出CoCoA全局后验可解释性框架，使用混合标注流程结合静态分析工具和提示工程LLM，将上下文标记嵌入聚类为人类可解释的概念组。

Result: CoCoA发现的概念在语义保留扰动下保持稳定(CSI=0.288)，并随微调可预测地演化。在用户研究中，概念增强解释在编程语言分类任务中比基于标记归因的方法提高了37个百分点的可解释性。

Conclusion: CoCoA框架能够有效揭示代码语言模型中的潜在结构，提供更连贯和可解释的概念基础解释，提高人类中心的可解释性。

Abstract: Interpreting the internal behavior of large language models trained on code
remains a critical challenge, particularly for applications demanding trust,
transparency, and semantic robustness. We propose Code Concept Analysis
(CoCoA): a global post-hoc interpretability framework that uncovers emergent
lexical, syntactic, and semantic structures in a code language model's
representation space by clustering contextualized token embeddings into
human-interpretable concept groups. We propose a hybrid annotation pipeline
that combines static analysis tool-based syntactic alignment with
prompt-engineered large language models (LLMs), enabling scalable labeling of
latent concepts across abstraction levels. We analyse the distribution of
concepts across layers and across three finetuning tasks. Emergent concept
clusters can help identify unexpected latent interactions and be used to
identify trends and biases within the model's learned representations. We
further integrate LCA with local attribution methods to produce
concept-grounded explanations, improving the coherence and interpretability of
token-level saliency. Empirical evaluations across multiple models and tasks
show that LCA discovers concepts that remain stable under semantic-preserving
perturbations (average Cluster Sensitivity Index, CSI = 0.288) and evolve
predictably with fine-tuning. In a user study, concept-augmented explanations
disambiguate token roles. In a user study on the programming-language
classification task, concept-augmented explanations disambiguated token roles
and improved human-centric explainability by 37 percentage points compared with
token-level attributions using Integrated Gradients.

</details>


### [11] [CodeChemist: Functional Knowledge Transfer for Low-Resource Code Generation via Test-Time Scaling](https://arxiv.org/abs/2510.00501)
*Kaixin Wang,Tianlin Li,Xiaoyu Zhang,Aishan Liu,Xianglong Liu,Ziqi Liu,Zhiqiang Zhang,Jun Zhou,and Bin Shi*

Main category: cs.SE

TL;DR: CodeChemist是一个高效的测试时扩展框架，通过生成测试用例实现从高资源编程语言到低资源编程语言的功能知识迁移，无需模型重新训练即可提升低资源语言的代码生成性能。


<details>
  <summary>Details</summary>
Motivation: 代码大语言模型在不同编程语言上的性能不一致，低资源编程语言由于训练数据有限表现较差，需要解决这种性能差异问题。

Method: 首先生成并执行高资源语言的代码来创建包含功能知识的测试用例，然后使用多温度对冲采样生成低资源语言的代码片段，根据测试用例通过率选择最佳代码。

Result: 广泛的实验表明，CodeChemist优于现有的测试时扩展方法，显著提升了低资源编程语言的代码生成性能。

Conclusion: CodeChemist提供了一种无需重新训练模型的有效方法，通过功能知识迁移解决了低资源编程语言代码生成性能不足的问题。

Abstract: Code Large Language Models (CodeLLMs) are increasingly used in code
generation tasks across a wide range of applications. However, their
performance is often inconsistent across different programming languages (PLs),
with low-resource PLs suffering the most due to limited training data. In this
paper, we present CodeChemist, a novel and efficient framework for test-time
scaling that enables functional knowledge transfer from high-resource to
low-resource PLs using generated test cases. CodeChemist first generates and
executes code in high-resource PLs to create test cases that encapsulate
functional knowledge. It then uses multi-temperature hedged sampling to
generate code snippets in the low-resource PL and selects the best one based on
the pass rate of the test cases. Our extensive experiments show that
CodeChemist outperforms existing test-time scaling approaches, boosting the
performance of code generation for low-resource PLs without requiring any model
retraining.

</details>


### [12] [Architectural Transformations and Emerging Verification Demands in AI-Enabled Cyber-Physical Systems](https://arxiv.org/abs/2510.00519)
*Hadiza Umar Yusuf,Khouloud Gaaloul*

Main category: cs.SE

TL;DR: 该论文探讨了AI驱动的CPS与传统控制模型在架构和验证实践上的差异


<details>
  <summary>Details</summary>
Motivation: 理解AI集成对CPS架构、操作复杂性和验证实践的影响，填补现有研究空白

Method: 通过调查Simulink中设计的AI驱动与传统控制模型的架构区别

Result: 揭示了两种模型在系统验证方面的不同影响

Conclusion: AI集成显著改变了CPS架构和验证需求，需要新的验证方法

Abstract: In the world of Cyber-Physical Systems (CPS), a captivating real-time fusion
occurs where digital technology meets the physical world. This synergy has been
significantly transformed by the integration of artificial intelligence (AI), a
move that dramatically enhances system adaptability and introduces a layer of
complexity that impacts CPS control optimization and reliability. Despite
advancements in AI integration, a significant gap remains in understanding how
this shift affects CPS architecture, operational complexity, and verification
practices. The extended abstract addresses this gap by investigating
architectural distinctions between AI-driven and traditional control models
designed in Simulink and their respective implications for system verification.

</details>


### [13] [LSPFuzz: Hunting Bugs in Language Servers](https://arxiv.org/abs/2510.00532)
*Hengcheng Zhu,Songqiang Chen,Valerio Terragni,Lili Wei,Jiarong Wu,Yepang Liu,Shing-Chi Cheung*

Main category: cs.SE

TL;DR: LSPFuzz是一个针对语言服务器协议(LSP)服务器的灰盒混合模糊测试工具，通过两阶段变异策略（语法感知的源代码变异和上下文感知的编辑器操作调度）来发现LSP服务器中的bug。


<details>
  <summary>Details</summary>
Motivation: LSP服务器可靠性问题日益严重，崩溃会禁用所有代码智能功能，漏洞可能让开发者在编辑不受信任源代码时面临风险。尽管LSP广泛采用，但现有技术没有专门针对LSP服务器测试的方法。

Method: 采用两阶段变异管道：首先对源代码进行语法感知变异，然后根据上下文调度编辑器操作。这种灰盒混合模糊测试方法能够有效探索LSP的复杂约束和输入空间。

Result: 在四个广泛使用的LSP服务器上评估，LSPFuzz表现优于基线模糊器，发现了51个之前未知的bug，其中42个被确认，26个被修复，2个被分配了CVE编号。

Conclusion: LSPFuzz推进了LSP服务器的质量保证，为该领域未来研究提供了实用工具和基础见解。

Abstract: The Language Server Protocol (LSP) has revolutionized the integration of code
intelligence in modern software development. There are approximately 300 LSP
server implementations for various languages and 50 editors offering LSP
integration. However, the reliability of LSP servers is a growing concern, as
crashes can disable all code intelligence features and significantly impact
productivity, while vulnerabilities can put developers at risk even when
editing untrusted source code. Despite the widespread adoption of LSP, no
existing techniques specifically target LSP server testing. To bridge this gap,
we present LSPFuzz, a grey-box hybrid fuzzer for systematic LSP server testing.
Our key insight is that effective LSP server testing requires holistic mutation
of source code and editor operations, as bugs often manifest from their
combinations. To satisfy the sophisticated constraints of LSP and effectively
explore the input space, we employ a two-stage mutation pipeline: syntax-aware
mutations to source code, followed by context-aware dispatching of editor
operations. We evaluated LSPFuzz on four widely used LSP servers. LSPFuzz
demonstrated superior performance compared to baseline fuzzers, and uncovered
previously unknown bugs in real-world LSP servers. Of the 51 bugs we reported,
42 have been confirmed, 26 have been fixed by developers, and two have been
assigned CVE numbers. Our work advances the quality assurance of LSP servers,
providing both a practical tool and foundational insights for future research
in this domain.

</details>


### [14] [AI-Driven Self-Evolving Software: A Promising Path Toward Software Automation](https://arxiv.org/abs/2510.00591)
*Liyi Cai,Yijie Ren,Yitong Zhang,Jia Li*

Main category: cs.SE

TL;DR: 提出AI驱动的自进化软件概念，通过多智能体架构实现无需人工干预的持续软件进化，展示了从AI助手到软件核心组件的转变可行性。


<details>
  <summary>Details</summary>
Motivation: 当前AI在软件开发中主要作为人类开发者的助手，仍需人工干预。研究探索AI能否超越助手角色，成为软件核心组件，实现真正的软件自动化。

Method: 采用多智能体架构构建轻量级原型，能够自主解释用户需求、生成验证代码并集成新功能，实现软件的持续进化。

Result: 在多个代表性场景的案例研究表明，原型能够可靠地构建和重用功能，为更复杂应用的规模化提供了早期证据。

Conclusion: AI驱动的自进化软件展示了实现真正自动化软件开发的潜力，为软件工程的未来发展指明了方向。

Abstract: Software automation has long been a central goal of software engineering,
striving for software development that proceeds without human intervention.
Recent efforts have leveraged Artificial Intelligence (AI) to advance software
automation with notable progress. However, current AI functions primarily as
assistants to human developers, leaving software development still dependent on
explicit human intervention. This raises a fundamental question: Can AI move
beyond its role as an assistant to become a core component of software, thereby
enabling genuine software automation? To investigate this vision, we introduce
AI-Driven Self-Evolving Software, a new form of software that evolves
continuously through direct interaction with users. We demonstrate the
feasibility of this idea with a lightweight prototype built on a multi-agent
architecture that autonomously interprets user requirements, generates and
validates code, and integrates new functionalities. Case studies across
multiple representative scenarios show that the prototype can reliably
construct and reuse functionality, providing early evidence that such software
systems can scale to more sophisticated applications and pave the way toward
truly automated software development. We make code and cases in this work
publicly available at https://anonymous.4open.science/r/live-software.

</details>


### [15] [PyTrim: A Practical Tool for Reducing Python Dependency Bloat](https://arxiv.org/abs/2510.00674)
*Konstantinos Karakatsanis,Georgios Alexopoulos,Ioannis Karyotakis,Foivos Timotheos Proestakis,Evangelos Talos,Panos Louridas,Dimitris Mitropoulos*

Main category: cs.SE

TL;DR: PYTRIM是一个端到端的自动化系统，用于消除Python项目中的依赖膨胀问题，能够自动移除未使用的导入和包声明，支持多种文件类型，并集成了动态分析组件来提高检测召回率。


<details>
  <summary>Details</summary>
Motivation: Python项目中的依赖膨胀问题增加了维护成本和安全风险，现有工具只能检测未使用的依赖，但移除这些依赖需要手动操作和专业知识。

Method: PYTRIM采用模块化设计，能够与任何依赖检测工具集成，自动移除未使用的导入和包声明，支持Python源代码和配置文件（如requirements.txt、setup.py），并包含动态分析组件来提高检测效果。

Result: 在37个真实合并的pull requests数据集上，PYTRIM实现了98.3%的准确率。在971个开源包中，识别并修剪了39个包的膨胀依赖，提交了对应的pull requests，其中6个已被接受合并。

Conclusion: PYTRIM是一个有效的自动化工具，能够准确识别和移除Python项目中的未使用依赖，在真实项目中得到了验证，并作为开源项目发布以促进社区贡献和进一步发展。

Abstract: Dependency bloat is a persistent challenge in Python projects, which
increases maintenance costs and security risks. While numerous tools exist for
detecting unused dependencies in Python, removing these dependencies across the
source code and configuration files of a project requires manual effort and
expertise.
  To tackle this challenge we introduce PYTRIM, an end-to-end system to
automate this process. PYTRIM eliminates unused imports and package
declarations across a variety of file types, including Python source and
configuration files such as requirements.txt and setup.py. PYTRIM's modular
design makes it agnostic to the source of dependency bloat information,
enabling integration with any detection tool. Beyond its contribution when it
comes to automation, PYTRIM also incorporates a novel dynamic analysis
component that improves dependency detection recall.
  Our evaluation of PYTRIM's end-to-end effectiveness on a ground-truth dataset
of 37 merged pull requests from prior work, shows that PYTRIM achieves 98.3%
accuracy in replicating human-made changes. To show its practical impact, we
run PYTRIM on 971 open-source packages, identifying and trimming bloated
dependencies in 39 of them. For each case, we submit a corresponding pull
request, 6 of which have already been accepted and merged. PYTRIM is available
as an open-source project, encouraging community contributions and further
development.
  Video demonstration: https://youtu.be/LqTEdOUbJRI
  Code repository: https://github.com/TrimTeam/PyTrim

</details>


### [16] [TShape: Rescuing Machine Learning Models from Complex Shapelet Anomalies](https://arxiv.org/abs/2510.00680)
*Hang Cui,Jingjing Li,Haotian Si,Quan Zhou,Changhua Pei,Gaogang Xie,Dan Pei*

Main category: cs.SE

TL;DR: TShape是一个用于工业时间序列异常检测的新框架，通过补丁式双重注意力机制和多尺度卷积来检测形状异常，在五个基准测试中平均F1分数提升10%。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以检测形状异常（shapelet anomalies），这些异常表现为复杂的形状偏差，对人类专家来说很明显但对机器学习算法具有挑战性。

Method: 引入补丁式双重注意力机制与多尺度卷积，通过平衡局部细粒度形状特征与全局上下文依赖关系来建模复杂的子序列变化。

Result: 在五个不同基准测试上的广泛评估显示，TShape优于现有最先进模型，异常检测的F1分数平均提升10%。消融研究和注意力可视化确认了各组件的重要贡献。

Conclusion: TShape对时间序列数据中复杂形状异常具有鲁棒性和适应性，每个组件都发挥了重要作用。

Abstract: Time series anomaly detection (TSAD) is critical for maintaining the
reliability of modern IT infrastructures, where complex anomalies frequently
arise in highly dynamic environments. In this paper, we present TShape, a novel
framework designed to address the challenges in industrial time series anomaly
detection. Existing methods often struggle to detect shapelet anomalies that
manifest as complex shape deviations, which appear obvious to human experts but
prove challenging for machine learning algorithms. TShape introduces a
patch-wise dual attention mechanism with multi-scale convolution to model
intricate sub-sequence variations by balancing local, fine-grained shape
features with global contextual dependencies. Our extensive evaluation on five
diverse benchmarks demonstrates that TShape outperforms existing
state-of-the-art models, achieving an average 10\% F1 score improvement in
anomaly detection. Additionally, ablation studies and attention visualizations
confirm the essential contributions of each component, highlighting the
robustness and adaptability of TShape to complex shapelet shapes in time series
data.

</details>


### [17] [Maven-Lockfile: High Integrity Rebuild of Past Java Releases](https://arxiv.org/abs/2510.00730)
*Larissa Schmid,Elias Lundell,Yogya Gamage,Benoit Baudry,Martin Monperrus*

Main category: cs.SE

TL;DR: Maven-Lockfile为Java生态系统中的Maven包管理器提供锁文件支持，解决依赖版本冻结和构建完整性验证问题。


<details>
  <summary>Details</summary>
Motivation: 现代软件项目依赖大量第三方库，使可重现和安全构建变得复杂。Maven作为Java生态系统最重要的包管理器之一，缺乏原生锁文件支持，无法确保依赖完整性和构建可重现性。

Method: 开发Maven-Lockfile工具来生成和更新锁文件，支持从历史版本重建项目。锁文件捕获所有直接和传递依赖及其校验和，实现高完整性构建。

Result: 评估显示Maven-Lockfile能够重现历史提交的构建，并能检测被篡改的构件。通过最小配置即可为Java项目提供现代构建完整性和可重现性。

Conclusion: Maven-Lockfile填补了Maven在锁文件支持方面的空白，为Java软件供应链安全研究奠定基础。

Abstract: Modern software projects depend on many third-party libraries, complicating
reproducible and secure builds. Several package managers address this with the
generation of a lockfile that freezes dependency versions and can be used to
verify the integrity of dependencies. Yet, Maven, one of the most important
package managers in the Java ecosystem, lacks native support for a lockfile. We
present Maven-Lockfile to generate and update lockfiles, with support for
rebuilding projects from past versions. Our lockfiles capture all direct and
transitive dependencies with their checksums, enabling high integrity builds.
Our evaluation shows that Maven-Lockfile can reproduce builds from historical
commits and is able to detect tampered artifacts. With minimal configuration,
Maven-Lockfile equips Java projects with modern build integrity and build
reproducibility, and fosters future research on software supply chain security
in Java.

</details>


### [18] [AI Where It Matters: Where, Why, and How Developers Want AI Support in Daily Work](https://arxiv.org/abs/2510.00762)
*Rudrajit Choudhuri,Carmen Badea,Christian Bird,Jenna Butler,Rob DeLine,Brian Houck*

Main category: cs.SE

TL;DR: 本研究通过860名开发者的混合方法调查，揭示了开发者对AI支持的接受程度和使用模式，发现不同任务类型对AI需求存在显著差异，并提出了负责任AI设计的上下文指导原则。


<details>
  <summary>Details</summary>
Motivation: 生成式AI正在重塑软件工作，但目前缺乏关于开发者最需要AI支持的领域、如何负责任设计AI支持的明确指导。

Method: 采用大规模混合方法研究，对860名开发者进行调查，结合认知评价理论分析任务评估如何预测AI接受度和使用模式。

Result: 发现三种主要模式：核心工作（编码、测试）已有较强使用但希望改进；高需求减少重复性工作（文档、运维）；明确限制身份和关系相关任务（指导）。负责任AI的优先级因上下文而异。

Conclusion: 研究结果为在开发者真正关心的领域提供AI支持提供了具体、上下文相关的指导，帮助设计更负责任和有效的AI工具。

Abstract: Generative AI is reshaping software work, yet we lack clear guidance on where
developers most need and want support, and how to design it responsibly. We
report a large-scale, mixed-methods study of N=860 developers that examines
where, why, and how they seek or limit AI help, providing the first task-aware,
empirically validated mapping from developers' perceptions of their tasks to AI
adoption patterns and responsible AI priorities. Using cognitive appraisal
theory, we show that task evaluations predict openness to and use of AI,
revealing distinct patterns: strong current use and a desire for improvement in
core work (e.g., coding, testing); high demand to reduce toil (e.g.,
documentation, operations); and clear limits for identity- and
relationship-centric work (e.g., mentoring). Priorities for responsible AI
support vary by context: reliability and security for systems-facing tasks;
transparency, alignment, and steerability to maintain control; and fairness and
inclusiveness for human-facing work. Our results offer concrete, contextual
guidance for delivering AI where it matters to developers and their work.

</details>


### [19] [Semantics-Aligned, Curriculum-Driven, and Reasoning-Enhanced Vulnerability Repair Framework](https://arxiv.org/abs/2510.01002)
*Chengran Yang,Ting Zhang,Jinfeng Jiang,Xin Zhou,Haoye Tian,Jieke Shi,Junkai Chen,Yikun Li,Eng Lieh Ouh,Lwin Khin Shar,David Lo*

Main category: cs.SE

TL;DR: SeCuRepair是一个基于语义对齐、课程驱动和推理增强的漏洞修复框架，通过先推理后编辑的范式显著提升了自动化漏洞修复的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 当前基于学习的自动化漏洞修复方法在真实场景中泛化能力不足，存在跨仓库泛化有限、无法捕捉长距离依赖、过度依赖表面词汇模式等问题。

Method: 采用先推理后编辑的范式，要求模型在生成补丁前明确表达修复逻辑；使用语义感知的强化学习奖励语法和语义对齐的补丁；采用难度感知的课程学习从简单修复逐步过渡到复杂的多块协调编辑。

Result: 在BigVul和PrimeVul_AVR数据集上，SeCuRepair显著优于所有基线方法，CodeBLEU指标分别比最佳基线高出34.52%和31.52%。

Conclusion: SeCuRepair通过语义对齐、推理增强和课程学习有效解决了现有AVR方法的局限性，显著提升了漏洞修复的泛化能力和性能。

Abstract: Current learning-based Automated Vulnerability Repair (AVR) approaches, while
promising, often fail to generalize effectively in real-world scenarios. Our
diagnostic analysis reveals three fundamental weaknesses in state-of-the-art
AVR approaches: (1) limited cross-repository generalization, with performance
drops on unseen codebases; (2) inability to capture long-range dependencies,
causing a performance degradation on complex, multi-hunk repairs; and (3)
over-reliance on superficial lexical patterns, leading to significant
performance drops on vulnerabilities with minor syntactic variations like
variable renaming.
  To address these limitations, we propose SeCuRepair, a semantics-aligned,
curriculum-driven, and reasoning-enhanced framework for vulnerability repair.
At its core, SeCuRepair adopts a reason-then-edit paradigm, requiring the model
to articulate why and how a vulnerability should be fixed before generating the
patch. This explicit reasoning enforces a genuine understanding of repair logic
rather than superficial memorization of lexical patterns. SeCuRepair also moves
beyond traditional supervised fine-tuning and employs semantics-aware
reinforcement learning, rewarding patches for their syntactic and semantic
alignment with the oracle patch rather than mere token overlap. Complementing
this, a difficulty-aware curriculum progressively trains the model, starting
with simple fixes and advancing to complex, multi-hunk coordinated edits.
  We evaluate SeCuRepair on strict, repository-level splits of BigVul and newly
crafted PrimeVul_AVR datasets. SeCuRepair significantly outperforms all
baselines, surpassing the best-performing baselines by 34.52% on BigVul and
31.52% on PrimeVul\textsubscript{AVR} in terms of CodeBLEU, respectively.
Comprehensive ablation studies further confirm that each component of our
framework contributes to its final performance.

</details>


### [20] [Advancing Automated Ethical Profiling in SE: a Zero-Shot Evaluation of LLM Reasoning](https://arxiv.org/abs/2510.00881)
*Patrizio Migliarini,Mashal Afzal Memon,Marco Autili,Paola Inverardi*

Main category: cs.SE

TL;DR: 本文提出了一个自动化框架，用于评估16个大型语言模型在零样本设置下的伦理推理能力，使用30个真实世界的伦理场景进行测试。


<details>
  <summary>Details</summary>
Motivation: 随着LLMs越来越多地集成到软件工程工具中，需要评估它们在伦理推理方面的能力，特别是在涉及不确定性和伦理重要情境下的判断能力。

Method: 采用完全自动化的评估框架，让LLMs在零样本设置下识别最适用的伦理理论、评估行为的道德可接受性，并解释其推理过程，然后将响应与伦理学专家的选择进行比较。

Result: LLMs的平均理论一致性率为73.3%，道德可接受性的二元一致率为86.7%，在伦理模糊案例中存在可解释的分歧，定性分析显示模型间存在强烈的概念收敛。

Conclusion: 研究结果表明LLMs作为软件工程管道中的伦理推理引擎具有潜在可行性，能够实现可扩展、可审计和自适应的用户对齐伦理推理集成。

Abstract: Large Language Models (LLMs) are increasingly integrated into software
engineering (SE) tools for tasks that extend beyond code synthesis, including
judgment under uncertainty and reasoning in ethically significant contexts. We
present a fully automated framework for assessing ethical reasoning
capabilities across 16 LLMs in a zero-shot setting, using 30 real-world
ethically charged scenarios. Each model is prompted to identify the most
applicable ethical theory to an action, assess its moral acceptability, and
explain the reasoning behind their choice. Responses are compared against
expert ethicists' choices using inter-model agreement metrics. Our results show
that LLMs achieve an average Theory Consistency Rate (TCR) of 73.3% and Binary
Agreement Rate (BAR) on moral acceptability of 86.7%, with interpretable
divergences concentrated in ethically ambiguous cases. A qualitative analysis
of free-text explanations reveals strong conceptual convergence across models
despite surface-level lexical diversity. These findings support the potential
viability of LLMs as ethical inference engines within SE pipelines, enabling
scalable, auditable, and adaptive integration of user-aligned ethical
reasoning. Our focus is the Ethical Interpreter component of a broader
profiling pipeline: we evaluate whether current LLMs exhibit sufficient
interpretive stability and theory-consistent reasoning to support automated
profiling.

</details>


### [21] [On Effective Semantic Translation for Code: A Study Based on Pseudocode](https://arxiv.org/abs/2510.00920)
*Songqiang Chen,Congying Xu,Jingyi Chen,Jialun Cao,Jiarong Wu,Shing-Chi Cheung*

Main category: cs.SE

TL;DR: 该论文提出使用伪代码作为中间步骤来改进代码翻译，通过将程序先转换为伪代码再翻译到目标语言，相比直接翻译方法能更好地处理复杂程序，特别是在从灵活语言到严格语言的翻译场景中。


<details>
  <summary>Details</summary>
Motivation: 直接代码翻译方法在处理复杂程序时存在困难，受人类语义翻译过程启发，探索使用伪代码作为中间步骤来提升大型语言模型在代码翻译任务中的准确性。

Method: 提出伪代码基代码翻译方法，先将程序解释为伪代码表达意图和逻辑，再实现为目标编程语言。通过比较直接翻译和伪代码基翻译在9,690个翻译任务上的表现，使用5个流行LLM和6种编程语言进行实证研究。

Result: 伪代码基翻译能有效补充直接翻译，特别适用于从灵活语言到严格语言的翻译和处理低资源Rust语言。两种方法具有互补优势，结合使用能提高翻译准确性。

Conclusion: 建议采用结合直接翻译和伪代码基翻译优势的策略来提升代码翻译准确性。伪代码基翻译在解耦复杂程序翻译和减少原始程序实现细节干扰方面有优势，但也受限于伪代码生成的质量问题。

Abstract: Large language models (LLMs) show great potential in code translation.
However, accurate translation remains challenging when using the commonly
adopted direct code-to-code translation approach, which converts a program into
the target programming language (PL) in a single step. Inspired by the success
of incorporating intermediate steps to guide LLMs in resolving challenging
tasks, we explore pseudocode-based code translation, which emulates the human
semantic translation by first interpreting the program's intent and logic into
pseudocode and then implementing it in the target PL. We find that
pseudocode-based translation helps translate programs that direct translation
struggles to handle. Nonetheless, the effectiveness, advantages, and
limitations of this approach remain underexplored. To bridge this gap, we
present an empirical study on pseudocode-based code translation, aiming to
investigate its effectiveness in enhancing the direct translation approach,
illuminate its effective usage, and identify limitations hindering its
potential benefits. By comparing direct and pseudocode-based translation
approaches on 9,690 translation tasks across six PLs with five popular LLMs, we
demonstrate that pseudocode-based translation can effectively complement direct
translation, particularly when translating from flexible to rigid PLs or
dealing with low-resource Rust. Based on these findings, we suggest adopting
strategies that combine the complementary strengths of both approaches to
enhance code translation accuracy. We also reveal the advantages of
pseudocode-based translation in disentangling translations of complicated
programs and mitigating distractions from detailed implementations in original
programs, as well as its limitations due to incorrect, incomplete, or ambiguous
pseudocode.

</details>


### [22] [ChatGPT in Introductory Programming: Counterbalanced Evaluation of Code Quality, Conceptual Learning, and Student Perceptions](https://arxiv.org/abs/2510.00946)
*Shiza Andleeb,Brandon Kantorski,Jeffrey C. Carver*

Main category: cs.SE

TL;DR: 研究ChatGPT对CS1课程中学生编程表现的影响，发现使用ChatGPT能提高代码质量和效率，但对概念理解的影响不一致，需要结构化整合以培养独立解决问题的能力。


<details>
  <summary>Details</summary>
Motivation: 随着ChatGPT等大语言模型在编程课程中的应用增加，需要了解其对代码质量、概念理解、任务完成时间和学生感知的影响，以平衡工具使用与学习效果。

Method: 采用平衡设计的准实验研究，学生在两个C语言编程作业（函数和结构体）中交替使用ChatGPT和非ChatGPT条件，通过多维评分标准、概念后测调查和任务完成时间进行评估。

Result: 使用ChatGPT的学生在代码质量评分上显著更高，任务完成时间更短；概念理解方面结果不一致，函数主题理解较低但结构体主题理解较高；学生对ChatGPT体验积极，认为对调试和练习有价值，但担心准确性和长期技能发展。

Conclusion: ChatGPT能提升新手程序员的代码质量和效率，但未必能一致改善概念理解，建议通过结构化整合和补充教学策略来培养独立解决问题的能力。

Abstract: Background: Large language models (LLMs) such as ChatGPT are increasingly
used in introductory programming courses to provide real-time code generation,
debugging, and explanations. While these tools can boost productivity and code
quality, concerns remain about over-reliance and potential impacts on
conceptual learning. Objective: To investigate how ChatGPT access affects code
quality, conceptual understanding, task completion times, and student
perceptions in a CS1 course. Methods: We conducted a counterbalanced,
quasi-experimental study in which students alternated between ChatGPT and
non-ChatGPT conditions across two programming assignments in C (functions and
structures). We evaluated their code submissions using multidimensional
rubrics, conceptual post-surveys, and task completion time. Results: Students
who had access to ChatGPT produced significantly higher rubric scores for code
quality and completed tasks in less time compared to those without access.
However, gains in conceptual understanding were mixed, lower for the functions
topic but higher for the structures topic. Students reported positive
experiences with ChatGPT, citing its value for debugging and practice, while
expressing concerns about accuracy and long-term skill development.
Conclusions: ChatGPT can enhance code quality and efficiency for novice
programmers, but may not uniformly improve conceptual understanding. Structured
integration and complementary instructional strategies are recommended to
foster independent problem-solving skills.

</details>


### [23] [Enhancing Software Testing Education: Understanding Where Students Struggle](https://arxiv.org/abs/2510.00957)
*Shiza Andleeb,Teo Mendoza,Lucas Cordova,Gursimran Walia,Jeffrey C. Carver*

Main category: cs.SE

TL;DR: 分析学生在软件测试课程中常见的概念误解，特别是决策覆盖率和异常处理方面的困难，以及学生倾向于进行无效的表面修改而非实质性改进测试覆盖率的行为模式。


<details>
  <summary>Details</summary>
Motivation: 虽然自动化反馈工具广泛用于支持学生学习，但尚不清楚哪些测试概念最常被误解，以及这些误解如何反映在学生的测试套件修订中。需要识别导致学生做出无效修改的具体测试概念。

Method: 在高级软件测试课程中使用自动化反馈工具，分析两个作业的学生提交内容，识别普遍存在的概念差距和无成效修改模式。

Result: 研究发现决策覆盖率和异常处理是持续存在的挑战，学生最常进行表面或方法级别的修改，这些修改无法提高代码覆盖率。

Conclusion: 这些发现为教育者、研究人员和工具设计者提供了可操作的见解，通过精确定位最常导致测试结果差的概念，可以改进反馈系统，针对性地解决持续存在的误解，更有效地支持学生开发健壮、可维护的测试套件。

Abstract: Effective software testing is critical for producing reliable and secure
software, yet many computer science students struggle to master the
foundational concepts required to construct comprehensive test suites. While
automated feedback tools are widely used to support student learning, it
remains unclear which testing concepts are most frequently misunderstood and
how these misunderstandings are reflected in students' test suite revisions.
This study examines the specific testing concepts that lead students to make
ineffective changes, those that fail to improve code coverage, during test
suite development. Leveraging an automated feedback tool in a senior-level
software testing course, we analyzed student submissions from two assignments
to identify prevalent conceptual gaps and patterns of unproductive
modification. Our results reveal that decision coverage and exception handling
are persistent challenges, and that students most often make superficial or
method-level changes that do not enhance coverage. These findings provide
actionable insights for educators, researchers, and tool designers. By
pinpointing the concepts that most often contribute to poor testing outcomes,
we can refine feedback systems, target instruction to address persistent
misconceptions, and more effectively support students in developing robust,
maintainable test suites.

</details>


### [24] [Improving Code Localization with Repository Memory](https://arxiv.org/abs/2510.01003)
*Boshi Wang,Weijian Xu,Yunsheng Li,Mei Gao,Yujia Xie,Huan Sun,Dongdong Chen*

Main category: cs.SE

TL;DR: 该研究通过利用代码仓库的提交历史，为语言代理添加长期记忆能力，以改进代码定位任务。


<details>
  <summary>Details</summary>
Motivation: 现有方法在处理代码仓库任务时通常从零开始，忽略了人类开发者会积累长期仓库记忆的关键方面，如关键模块功能和bug类型与修复位置的关联。

Method: 引入工具使代理能够从非参数化记忆中检索信息，包括近期历史提交、关联问题以及通过提交模式识别的活跃代码部分的功能摘要。

Result: 实验表明，添加这种记忆能力能显著提升最先进的定位框架LocAgent在SWE-bench-verified和SWE-bench-live基准测试上的性能。

Conclusion: 这项研究有助于开发能够积累和利用过往经验处理长期任务的代理，更接近模拟人类开发者的专业知识。

Abstract: Code localization is a fundamental challenge in repository-level software
engineering tasks such as bug fixing. While existing methods equip language
agents with comprehensive tools/interfaces to fetch information from the
repository, they overlook the critical aspect of memory, where each instance is
typically handled from scratch assuming no prior repository knowledge. In
contrast, human developers naturally build long-term repository memory, such as
the functionality of key modules and associations between various bug types and
their likely fix locations. In this work, we augment language agents with such
memory by leveraging a repository's commit history - a rich yet underutilized
resource that chronicles the codebase's evolution. We introduce tools that
allow the agent to retrieve from a non-parametric memory encompassing recent
historical commits and linked issues, as well as functionality summaries of
actively evolving parts of the codebase identified via commit patterns. We
demonstrate that augmenting such a memory can significantly improve LocAgent, a
state-of-the-art localization framework, on both SWE-bench-verified and the
more recent SWE-bench-live benchmarks. Our research contributes towards
developing agents that can accumulate and leverage past experience for
long-horizon tasks, more closely emulating the expertise of human developers.

</details>


### [25] [GenIA-E2ETest: A Generative AI-Based Approach for End-to-End Test Automation](https://arxiv.org/abs/2510.01024)
*Elvis Júnior,Alan Valejo,Jorge Valverde-Rebaza,Vânia de Oliveira Neves*

Main category: cs.SE

TL;DR: GenIA-E2ETest利用生成式AI从自然语言描述自动生成可执行的端到端测试脚本，在Web应用中验证了其有效性，显著减少了手动测试工作量。


<details>
  <summary>Details</summary>
Motivation: 传统软件测试耗时且易错，现有基于LLM的测试生成主要关注单元测试，缺乏针对端到端测试的解决方案，需要验证完整应用工作流程。

Method: 提出GenIA-E2ETest方法，利用生成式AI从自然语言描述自动生成可执行的端到端测试脚本。

Result: 在两个Web应用上的评估显示：元素指标平均77%，执行精度82%，执行召回率85%，手动修改率仅10%，在典型Web场景中表现稳定。

Conclusion: GenIA-E2ETest是从自然语言加速端到端测试自动化的实用有效解决方案，尽管对上下文相关导航和动态内容有些敏感，但能显著减少手动工作量并扩大自动化测试的可及性。

Abstract: Software testing is essential to ensure system quality, but it remains
time-consuming and error-prone when performed manually. Although recent
advances in Large Language Models (LLMs) have enabled automated test
generation, most existing solutions focus on unit testing and do not address
the challenges of end-to-end (E2E) testing, which validates complete
application workflows from user input to final system response. This paper
introduces GenIA-E2ETest, which leverages generative AI to generate executable
E2E test scripts from natural language descriptions automatically. We evaluated
the approach on two web applications, assessing completeness, correctness,
adaptation effort, and robustness. Results were encouraging: the scripts
achieved an average of 77% for both element metrics, 82% for precision of
execution, 85% for execution recall, required minimal manual adjustments
(average manual modification rate of 10%), and showed consistent performance in
typical web scenarios. Although some sensitivity to context-dependent
navigation and dynamic content was observed, the findings suggest that
GenIA-E2ETest is a practical and effective solution to accelerate E2E test
automation from natural language, reducing manual effort and broadening access
to automated testing.

</details>


### [26] [CodeGenLink: A Tool to Find the Likely Origin and License of Automatically Generated Code](https://arxiv.org/abs/2510.01077)
*Daniele Bifolco,Guido Annicchiarico,Pierluigi Barbiero,Massimiliano Di Penta,Fiorella Zampetti*

Main category: cs.SE

TL;DR: CodeGenLink是一个GitHub CoPilot扩展，通过结合LLM和网络搜索来检索与AI生成代码相似的代码链接，并提供可能的代码来源许可证信息。


<details>
  <summary>Details</summary>
Motivation: 开发人员对LLM生成代码缺乏信任，担心版权和许可证违规问题，因为缺少代码来源信息。

Method: 结合LLM的网络搜索功能检索候选链接，然后对生成代码和检索代码进行相似性分析。

Result: 初步结果显示CodeGenLink能有效通过相似性分析过滤无关链接，并在可用时提供许可证信息。

Conclusion: CodeGenLink为解决LLM生成代码的可信度和版权问题提供了有效工具。

Abstract: Large Language Models (LLMs) are widely used in software development tasks
nowadays. Unlike reusing code taken from the Web, for LLMs' generated code,
developers are concerned about its lack of trustworthiness and possible
copyright or licensing violations, due to the lack of code provenance
information. This paper proposes CodeGenLink, a GitHub CoPilot extension for
Visual Studio Code aimed at (i) suggesting links containing code very similar
to automatically generated code, and (ii) whenever possible, indicating the
license of the likely origin of the code. CodeGenLink retrieves candidate links
by combining LLMs with their web search features and then performs similarity
analysis between the generated and retrieved code. Preliminary results show
that CodeGenLink effectively filters unrelated links via similarity analysis
and provides licensing information when available. Tool URL:
https://github.com/danielebifolco/CodeGenLink Tool Video:
https://youtu.be/M6nqjBf9_pw

</details>


### [27] [Developers' Perspectives on Software Licensing: Current Practices, Challenges, and Tools](https://arxiv.org/abs/2510.01096)
*Nathan Wintersgill,Trevor Stalnaker,Daniel Otten,Laura A. Heymann,Oscar Chaparro,Massimiliano Di Penta,Daniel M. German,Denys Poshyvanyk*

Main category: cs.SE

TL;DR: 该研究通过调查和访谈分析了开发者处理开源许可证合规的实践、挑战和工具使用情况，提出了15个关键发现和改进建议。


<details>
  <summary>Details</summary>
Motivation: 开源组件在现代软件中广泛使用，许可证不合规会带来财务、法律和声誉风险。开发者在此过程中扮演关键角色，需要了解他们的实践方法和面临的挑战。

Method: 由软件工程和法律研究人员组成的联合团队，对58名软件开发者进行问卷调查，并进行7次后续访谈。

Result: 研究得出了15个关于当前实践状态的关键发现，揭示了开发者在许可证合规方面的现状和问题。

Conclusion: 研究讨论了发现的影响，为未来研究提供了方向，并为许可证工具提出了可行的改进建议。

Abstract: Most modern software products incorporate open-source components, requiring
development teams to maintain compliance with each component's licenses.
Noncompliance can lead to significant financial, legal, and reputational
repercussions. While some organizations may seek advice from legal
practitioners to assist with licensing tasks, developers still play a key role
in such a process. To this end, it is essential to understand how developers
approach license compliance tasks, the challenges they encounter, and the tools
that they use. This work studies these aspects of software licensing practices
through a study - conducted by a joint team of software engineering and legal
researchers - consisting of a survey with 58 software developers and seven
follow-up interviews. The study resulted in 15 key findings regarding the
current state of practice. We discuss the implications of our findings and
offer directions for future research as well as actionable recommendations for
licensing tools.

</details>


### [28] [When Shared Worlds Break: Demystifying Defects in Multi-User Extended Reality Software Systems](https://arxiv.org/abs/2510.01182)
*Shuqing Li,Chenran Zhang,Binchang Li,Cuiyun Gao,Michael R. Lyu*

Main category: cs.SE

TL;DR: 首个大规模多用户XR系统软件缺陷实证研究，分析了2,649个真实bug报告，开发了包含症状表现、根本原因和后果严重性的三维分类法，揭示了同步不一致和化身异常是最常见症状，网络/同步逻辑缺陷是主要根本原因。


<details>
  <summary>Details</summary>
Motivation: 多用户XR系统引入了独特的软件缺陷，影响用户体验，但相关研究仍不足。理解这些缺陷对提升系统可靠性至关重要。

Method: 通过定性分析使用迭代开放编码方法，分析来自开发者论坛、GitHub仓库和主流XR应用商店应用评论的2,649个真实bug报告。

Result: 开发了全面的分类法，发现同步不一致和化身相关异常是最普遍症状，网络/同步逻辑缺陷和会话管理缺陷是主要根本原因。超过34%的bug导致严重破坏共享体验的后果。

Conclusion: 多用户XR系统在分布式系统、实时3D交互和沉浸式体验交叉领域面临独特挑战，需要专门的测试、调试和质量保证方法。

Abstract: Multi-user Extended Reality (XR) systems enable transformative shared
experiences but introduce unique software defects that compromise user
experience. Understanding software defects in multi-user XR systems is crucial
for enhancing system reliability, yet remains underexplored. To fill the gap,
this paper presents the first large-scale empirical study of multi-user XR
defects, analyzing 2,649 real-world bug reports from diverse sources, including
developer forums, GitHub repositories, and app reviews on mainstream XR app
stores. Through rigorous qualitative analysis using iterative open coding, we
develop a comprehensive taxonomy that classifies multi-user XR bugs along three
dimensions: Symptom Manifestation, Root Cause Origin, and Consequence Severity.
Our findings reveal that synchronization inconsistencies and avatar-related
anomalies are the most prevalent symptoms, while network/synchronization logic
defects and session management flaws emerge as dominant root causes.
Critically, over 34% of analyzed bugs lead to severe consequences that
fundamentally break the shared experience, including system crashes, persistent
disconnections, and complete interaction breakdowns, etc. We also identify
concerning privacy and health implications unique to multi-user XR contexts.
Based on our findings of defect analysis, we provide actionable recommendations
for developers, platform vendors, and researchers. Our results demonstrate that
multi-user XR systems face distinct challenges at the intersection of
distributed systems, real-time 3D interaction, and immersive experiences,
necessitating specialized approaches to testing, debugging, and quality
assurance.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [29] [Stealing AI Model Weights Through Covert Communication Channels](https://arxiv.org/abs/2510.00151)
*Valentin Barbaza,Alan Rodrigo Diaz-Rizo,Hassan Aboushady,Spyridon Raptis,Haralampos-G. Stratigopoulos*

Main category: cs.CR

TL;DR: 提出了一种针对配备AI硬件加速器的无线设备的新型模型窃取攻击，通过硬件木马和隐蔽通信通道窃取模型权重，能够重建完整权重矩阵且对模型架构和硬件加速器无关。


<details>
  <summary>Details</summary>
Motivation: AI模型因其高昂开发成本、竞争优势和专有技术而具有重要知识产权价值，模型窃取攻击对AI模型提供商构成严重威胁。

Method: 攻击分两阶段：第一阶段通过硬件木马在受害者设备上建立隐蔽通信通道泄露模型权重；第二阶段使用附近无线设备拦截传输帧并增量重建完整权重矩阵。

Result: 通过硬件演示验证了四种不同类型和大小的AI模型的有效性，分析了误码率影响并提出了错误缓解技术，评估了重建模型的准确性和提取时间。

Conclusion: 该攻击具有隐蔽性和有效性，需要探索相应的防御机制来保护AI模型知识产权。

Abstract: AI models are often regarded as valuable intellectual property due to the
high cost of their development, the competitive advantage they provide, and the
proprietary techniques involved in their creation. As a result, AI model
stealing attacks pose a serious concern for AI model providers. In this work,
we present a novel attack targeting wireless devices equipped with AI hardware
accelerators. The attack unfolds in two phases. In the first phase, the
victim's device is compromised with a hardware Trojan (HT) designed to covertly
leak model weights through a hidden communication channel, without the victim
realizing it. In the second phase, the adversary uses a nearby wireless device
to intercept the victim's transmission frames during normal operation and
incrementally reconstruct the complete weight matrix. The proposed attack is
agnostic to both the AI model architecture and the hardware accelerator used.
We validate our approach through a hardware-based demonstration involving four
diverse AI models of varying types and sizes. We detail the design of the HT
and the covert channel, highlighting their stealthy nature. Additionally, we
analyze the impact of bit error rates on the reception and propose an error
mitigation technique. The effectiveness of the attack is evaluated based on the
accuracy of the reconstructed models with stolen weights and the time required
to extract them. Finally, we explore potential defense mechanisms.

</details>


### [30] [Calyx: Privacy-Preserving Multi-Token Optimistic-Rollup Protocol](https://arxiv.org/abs/2510.00164)
*Dominik Apel,Zeta Avarikioti,Matteo Maffei,Yuheng Wang*

Main category: cs.CR

TL;DR: Calyx是首个隐私保护的多代币乐观Rollup协议，为所有L2交易提供完整的支付隐私保护，同时支持多代币交易的原子执行和可持续的费用机制。


<details>
  <summary>Details</summary>
Motivation: 当前Rollup协议需要在链上发布明文交易数据以确保数据可用性，这导致了固有的隐私限制。

Method: 采用高效的一步欺诈证明机制，支持多代币交易的原子执行，并引入交易费用方案。

Result: 单笔交易执行成本约为0.06美元（0.00002 ETH），在渐近意义上仅产生恒定大小的链上成本。

Conclusion: Calyx成功解决了Rollup协议的隐私问题，为L2可扩展性解决方案提供了完整的支付隐私保护。

Abstract: Rollup protocols have recently received significant attention as a promising
class of Layer 2 (L2) scalability solutions. By utilizing the Layer 1 (L1)
blockchain solely as a bulletin board for a summary of the executed
transactions and state changes, rollups enable secure off-chain execution while
avoiding the complexity of other L2 mechanisms. However, to ensure data
availability, current rollup protocols require the plaintext of executed
transactions to be published on-chain, resulting in inherent privacy
limitations.
  In this paper, we address this problem by introducing Calyx, the first
privacy-preserving multi-token optimistic-Rollup protocol. Calyx guarantees
full payment privacy for all L2 transactions, revealing no information about
the sender, recipient, transferred amount, or token type. The protocol further
supports atomic execution of multiple multi-token transactions and introduces a
transaction fee scheme to enable broader application scenarios while ensuring
the sustainable operation of the protocol. To enforce correctness, Calyx adopts
an efficient one-step fraud-proof mechanism. We analyze the security and
privacy guarantees of the protocol and provide an implementation and
evaluation. Our results show that executing a single transaction costs
approximately $0.06 (0.00002 ETH) and incurs only constant-size on-chain cost
in asymptotic terms.

</details>


### [31] [CHAI: Command Hijacking against embodied AI](https://arxiv.org/abs/2510.00181)
*Luis Burbano,Diego Ortiz,Qi Sun,Siwei Yang,Haoqin Tu,Cihang Xie,Yinzhi Cao,Alvaro A Cardenas*

Main category: cs.CR

TL;DR: CHAI是一种针对具身AI系统的新型提示攻击方法，通过在多模态视觉输入中嵌入欺骗性自然语言指令，利用大型视觉语言模型的多模态语言解释能力进行命令劫持攻击。


<details>
  <summary>Details</summary>
Motivation: 具身AI系统在处理边缘情况时使用基于感知和行动的常识推理来泛化训练分布之外的情况，但这些能力也带来了新的安全风险。

Method: CHAI通过系统搜索token空间、构建提示字典，并指导攻击模型生成视觉攻击提示，在多模态视觉输入中嵌入欺骗性自然语言指令（如误导性标志）。

Result: 在四个LVLM代理（无人机紧急降落、自动驾驶、空中目标跟踪）和真实机器人车辆上的实验表明，CHAI始终优于最先进的攻击方法。

Conclusion: 通过利用下一代具身AI系统的语义和多模态推理优势，CHAI强调了超越传统对抗鲁棒性的防御措施的紧迫需求。

Abstract: Embodied Artificial Intelligence (AI) promises to handle edge cases in
robotic vehicle systems where data is scarce by using common-sense reasoning
grounded in perception and action to generalize beyond training distributions
and adapt to novel real-world situations. These capabilities, however, also
create new security risks. In this paper, we introduce CHAI (Command Hijacking
against embodied AI), a new class of prompt-based attacks that exploit the
multimodal language interpretation abilities of Large Visual-Language Models
(LVLMs). CHAI embeds deceptive natural language instructions, such as
misleading signs, in visual input, systematically searches the token space,
builds a dictionary of prompts, and guides an attacker model to generate Visual
Attack Prompts. We evaluate CHAI on four LVLM agents; drone emergency landing,
autonomous driving, and aerial object tracking, and on a real robotic vehicle.
Our experiments show that CHAI consistently outperforms state-of-the-art
attacks. By exploiting the semantic and multimodal reasoning strengths of
next-generation embodied AI systems, CHAI underscores the urgent need for
defenses that extend beyond traditional adversarial robustness.

</details>


### [32] [SecureBERT 2.0: Advanced Language Model for Cybersecurity Intelligence](https://arxiv.org/abs/2510.00240)
*Ehsan Aghaei,Sarthak Jain,Prashanth Arun,Arjun Sambamoorthy*

Main category: cs.CR

TL;DR: SecureBERT 2.0是一个专为网络安全应用设计的增强型编码器语言模型，在多个网络安全基准测试中达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 通用语言模型缺乏网络安全领域所需的领域特定适应性，无法高精度处理专业术语、复杂文档结构以及自然语言和源代码的相互依赖关系。

Method: 基于ModernBERT架构，采用改进的长上下文建模和分层编码，在比前身大13倍以上的领域特定语料库上进行预训练，包含超过130亿文本标记和5300万代码标记。

Result: 在威胁情报语义搜索、语义分析、网络安全特定命名实体识别和代码漏洞自动检测方面表现出显著改进。

Conclusion: SecureBERT 2.0为网络安全应用提供了有效的专用语言模型，能够处理扩展和异构文档，支持自动化威胁检测、事件分类和漏洞评估等关键任务。

Abstract: Effective analysis of cybersecurity and threat intelligence data demands
language models that can interpret specialized terminology, complex document
structures, and the interdependence of natural language and source code.
Encoder-only transformer architectures provide efficient and robust
representations that support critical tasks such as semantic search, technical
entity extraction, and semantic analysis, which are key to automated threat
detection, incident triage, and vulnerability assessment. However,
general-purpose language models often lack the domain-specific adaptation
required for high precision. We present SecureBERT 2.0, an enhanced
encoder-only language model purpose-built for cybersecurity applications.
Leveraging the ModernBERT architecture, SecureBERT 2.0 introduces improved
long-context modeling and hierarchical encoding, enabling effective processing
of extended and heterogeneous documents, including threat reports and source
code artifacts. Pretrained on a domain-specific corpus more than thirteen times
larger than its predecessor, comprising over 13 billion text tokens and 53
million code tokens from diverse real-world sources, SecureBERT 2.0 achieves
state-of-the-art performance on multiple cybersecurity benchmarks. Experimental
results demonstrate substantial improvements in semantic search for threat
intelligence, semantic analysis, cybersecurity-specific named entity
recognition, and automated vulnerability detection in code within the
cybersecurity domain.

</details>


### [33] [MAVUL: Multi-Agent Vulnerability Detection via Contextual Reasoning and Interactive Refinement](https://arxiv.org/abs/2510.00317)
*Youpeng Li,Kartik Joshi,Xinda Wang,Eric Wong*

Main category: cs.CR

TL;DR: MAVUL是一个新颖的多代理漏洞检测系统，通过上下文推理和交互式优化来解决传统漏洞检测方法的局限性，显著提升了检测性能和评估准确性。


<details>
  <summary>Details</summary>
Motivation: 现有漏洞检测方法存在上下文理解不足、单轮交互限制和粗粒度评估等问题，导致模型性能不佳和评估结果偏差，需要开发更有效的检测系统。

Method: 设计漏洞分析师代理，利用工具使用能力和上下文推理实现跨过程代码理解；通过跨角色代理交互的迭代反馈和优化决策实现可靠推理；引入多维真实信息进行细粒度评估。

Result: 在成对漏洞数据集上的实验表明，MAVUL显著优于现有多代理系统（成对准确率高62%以上）和单代理系统（平均性能高600%以上）；漏洞分析师与安全架构师代理间通信轮次增加能显著提升系统效果。

Conclusion: MAVUL通过上下文推理追踪漏洞流和关键反馈机制，结合集成评估代理作为无偏判断，确保了系统在实际应用中的准确性和可靠性评估。

Abstract: The widespread adoption of open-source software (OSS) necessitates the
mitigation of vulnerability risks. Most vulnerability detection (VD) methods
are limited by inadequate contextual understanding, restrictive single-round
interactions, and coarse-grained evaluations, resulting in undesired model
performance and biased evaluation results. To address these challenges, we
propose MAVUL, a novel multi-agent VD system that integrates contextual
reasoning and interactive refinement. Specifically, a vulnerability analyst
agent is designed to flexibly leverage tool-using capabilities and contextual
reasoning to achieve cross-procedural code understanding and effectively mine
vulnerability patterns. Through iterative feedback and refined decision-making
within cross-role agent interactions, the system achieves reliable reasoning
and vulnerability prediction. Furthermore, MAVUL introduces multi-dimensional
ground truth information for fine-grained evaluation, thereby enhancing
evaluation accuracy and reliability.
  Extensive experiments conducted on a pairwise vulnerability dataset
demonstrate MAVUL's superior performance. Our findings indicate that MAVUL
significantly outperforms existing multi-agent systems with over 62% higher
pairwise accuracy and single-agent systems with over 600% higher average
performance. The system's effectiveness is markedly improved with increased
communication rounds between the vulnerability analyst agent and the security
architect agent, underscoring the importance of contextual reasoning in tracing
vulnerability flows and the crucial feedback role. Additionally, the integrated
evaluation agent serves as a critical, unbiased judge, ensuring a more accurate
and reliable estimation of the system's real-world applicability by preventing
misleading binary comparisons.

</details>


### [34] [Privately Estimating Black-Box Statistics](https://arxiv.org/abs/2510.00322)
*Günter F. Steinke,Thomas Steinke*

Main category: cs.CR

TL;DR: 提出了一种在统计效率和计算效率之间权衡的差分隐私方案，适用于任意黑盒函数，无需敏感性边界


<details>
  <summary>Details</summary>
Motivation: 传统差分隐私方法需要估计器的敏感性边界，但这些边界往往很大或未知。现有黑盒方法要么数据使用效率低，要么需要指数级函数评估

Method: 设计了一个在统计效率（所需数据量）和oracle效率（评估次数）之间权衡的方案

Result: 提出的方案在两种效率之间实现了良好的权衡，并给出了证明方案接近最优的下界

Conclusion: 该工作提供了一种实用的差分隐私方法，适用于任意黑盒函数，无需敏感性边界，在效率和实用性之间取得了平衡

Abstract: Standard techniques for differentially private estimation, such as Laplace or
Gaussian noise addition, require guaranteed bounds on the sensitivity of the
estimator in question. But such sensitivity bounds are often large or simply
unknown. Thus we seek differentially private methods that can be applied to
arbitrary black-box functions. A handful of such techniques exist, but all are
either inefficient in their use of data or require evaluating the function on
exponentially many inputs. In this work we present a scheme that trades off
between statistical efficiency (i.e., how much data is needed) and oracle
efficiency (i.e., the number of evaluations). We also present lower bounds
showing the near-optimality of our scheme.

</details>


### [35] [Security and Privacy Analysis of Tile's Location Tracking Protocol](https://arxiv.org/abs/2510.00350)
*Akshaya Kumar,Anna Raymaker,Michael Specter*

Main category: cs.CR

TL;DR: 首次对Tile位置追踪服务进行安全分析，发现多个可被利用的漏洞和设计缺陷，包括服务器可追踪所有用户位置、蓝牙广告可被攻击者利用、防盗模式易被绕过等问题。


<details>
  <summary>Details</summary>
Motivation: Tile作为仅次于苹果AirTags的第二大众包位置追踪服务，声称提供安全和隐私保障，但缺乏正式协议描述和威胁模型，需要验证其安全性承诺。

Method: 通过分析Tile平台的漏洞和设计缺陷，评估其安全机制的有效性，特别关注反跟踪功能和问责机制的实施。

Result: 发现Tile服务器可持久追踪所有用户位置，非特权攻击者可通过蓝牙广告追踪用户，防盗模式易被绕过，问责机制存在可被利用的漏洞。

Conclusion: Tile的安全和隐私保障存在严重问题，需要新的正式问责机制定义来改善众包位置追踪协议的安全性。

Abstract: We conduct the first comprehensive security analysis of Tile, the second most
popular crowd-sourced location-tracking service behind Apple's AirTags. We
identify several exploitable vulnerabilities and design flaws, disproving many
of the platform's claimed security and privacy guarantees: Tile's servers can
persistently learn the location of all users and tags, unprivileged adversaries
can track users through Bluetooth advertisements emitted by Tile's devices, and
Tile's anti-theft mode is easily subverted.
  Despite its wide deployment -- millions of users, devices, and purpose-built
hardware tags -- Tile provides no formal description of its protocol or threat
model. Worse, Tile intentionally weakens its antistalking features to support
an antitheft use-case and relies on a novel "accountability" mechanism to
punish those abusing the system to stalk victims.
  We examine Tile's accountability mechanism, a unique feature of independent
interest; no other provider attempts to guarantee accountability. While an
ideal accountability mechanism may disincentivize abuse in crowd-sourced
location tracking protocols, we show that Tile's implementation is subvertible
and introduces new exploitable vulnerabilities. We conclude with a discussion
on the need for new, formal definitions of accountability in this setting.

</details>


### [36] [A Call to Action for a Secure-by-Design Generative AI Paradigm](https://arxiv.org/abs/2510.00451)
*Dalal Alharthi,Ivan Roberto Kawaminami Garcia*

Main category: cs.CR

TL;DR: 提出PromptShield框架，通过本体驱动的语义验证来防御LLM的提示注入攻击，在AWS云日志分析实验中实现了约94%的安全性和性能指标。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型广泛使用但存在提示注入等安全漏洞，需要建立安全优先的AI范式来主动缓解这些漏洞。

Method: 开发PromptShield框架，采用本体驱动的语义验证标准化用户输入，消除歧义并防御对抗性操纵。

Result: 在AWS云日志分析实验中，PromptShield显著提升了模型安全性和性能，精确率、召回率和F1分数均达到约94%。

Conclusion: PromptShield不仅有效防御对抗威胁，还提升了系统整体性能和可靠性，其模块化设计使其适用于各种生成式AI应用领域。

Abstract: Large language models have gained widespread prominence, yet their
vulnerability to prompt injection and other adversarial attacks remains a
critical concern. This paper argues for a security-by-design AI paradigm that
proactively mitigates LLM vulnerabilities while enhancing performance. To
achieve this, we introduce PromptShield, an ontology-driven framework that
ensures deterministic and secure prompt interactions. It standardizes user
inputs through semantic validation, eliminating ambiguity and mitigating
adversarial manipulation. To assess PromptShield's security and performance
capabilities, we conducted an experiment on an agent-based system to analyze
cloud logs within Amazon Web Services (AWS), containing 493 distinct events
related to malicious activities and anomalies. By simulating prompt injection
attacks and assessing the impact of deploying PromptShield, our results
demonstrate a significant improvement in model security and performance,
achieving precision, recall, and F1 scores of approximately 94%. Notably, the
ontology-based framework not only mitigates adversarial threats but also
enhances the overall performance and reliability of the system. Furthermore,
PromptShield's modular and adaptable design ensures its applicability beyond
cloud security, making it a robust solution for safeguarding generative AI
applications across various domains. By laying the groundwork for AI safety
standards and informing future policy development, this work stimulates a
crucial dialogue on the pivotal role of deterministic prompt engineering and
ontology-based validation in ensuring the safe and responsible deployment of
LLMs in high-stakes environments.

</details>


### [37] [Cloud Investigation Automation Framework (CIAF): An AI-Driven Approach to Cloud Forensics](https://arxiv.org/abs/2510.00452)
*Dalal Alharthi,Ivan Roberto Kawaminami Garcia*

Main category: cs.CR

TL;DR: 提出了Cloud Investigation Automation Framework (CIAF)，一个基于本体的自动化云取证框架，通过语义验证标准化用户输入，提高勒索软件检测效率，准确率达到93%。


<details>
  <summary>Details</summary>
Motivation: 云取证调查仍依赖手动分析，耗时且容易出错。LLMs能够模拟人类推理，为自动化云日志分析提供了可能。

Method: 开发了CIAF框架，采用本体驱动的方法，通过语义验证标准化用户输入，消除歧义，确保日志解释的一致性。

Result: 在微软Azure日志上的测试显示，勒索软件检测的精确率、召回率和F1分数均达到93%，显著提升了检测性能。

Conclusion: CIAF的模块化、适应性设计使其成为处理各种网络攻击的强大解决方案，为标准化取证方法和未来AI驱动的自动化奠定了基础。

Abstract: Large Language Models (LLMs) have gained prominence in domains including
cloud security and forensics. Yet cloud forensic investigations still rely on
manual analysis, making them time-consuming and error-prone. LLMs can mimic
human reasoning, offering a pathway to automating cloud log analysis. To
address this, we introduce the Cloud Investigation Automation Framework (CIAF),
an ontology-driven framework that systematically investigates cloud forensic
logs while improving efficiency and accuracy. CIAF standardizes user inputs
through semantic validation, eliminating ambiguity and ensuring consistency in
log interpretation. This not only enhances data quality but also provides
investigators with reliable, standardized information for decision-making. To
evaluate security and performance, we analyzed Microsoft Azure logs containing
ransomware-related events. By simulating attacks and assessing CIAF's impact,
results showed significant improvement in ransomware detection, achieving
precision, recall, and F1 scores of 93 percent. CIAF's modular, adaptable
design extends beyond ransomware, making it a robust solution for diverse
cyberattacks. By laying the foundation for standardized forensic methodologies
and informing future AI-driven automation, this work underscores the role of
deterministic prompt engineering and ontology-based validation in enhancing
cloud forensic investigations. These advancements improve cloud security while
paving the way for efficient, automated forensic workflows.

</details>


### [38] [Has the Two-Decade-Old Prophecy Come True? Artificial Bad Intelligence Triggered by Merely a Single-Bit Flip in Large Language Models](https://arxiv.org/abs/2510.00490)
*Yu Yan,Siqi Lu,Yang Gao,Zhaoxuan Li,Ziming Zhao,Qingjun Yuan,Yongjuan Wang*

Main category: cs.CR

TL;DR: 该论文首次系统性地发现并验证了大型语言模型权重文件中存在单比特漏洞，通过构建信息论权重敏感度熵模型和概率启发式扫描框架BitSifter，能够高效定位关键易受攻击比特，并设计了远程比特翻转攻击链，可在现实环境中实现语义级攻击。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型以.gguf量化格式广泛部署，其权重空间面临前所未有的硬件攻击面。比特翻转攻击能够通过硬件故障注入远程破坏软件系统完整性，因此需要研究LLM权重文件中的单比特漏洞。

Method: 构建信息论权重敏感度熵模型和概率启发式扫描框架BitSifter，用于高效定位关键易受攻击比特；设计远程比特翻转攻击链，在现实环境中实现语义级攻击。

Result: 在主流开源模型的.gguf量化格式中，翻转单个比特可诱导三种目标语义级故障：人工缺陷智能（输出事实错误）、人工弱智能（逻辑推理能力退化）和人工不良智能（生成有害内容）。攻击频率达464.3次/秒时，31.7秒内可100%成功翻转单个比特，使LLM准确率从73.5%降至0%。

Conclusion: LLM权重文件存在严重的单比特漏洞，模型大小与鲁棒性呈负相关，较小模型更易受攻击。这种攻击不需要高成本设备或复杂提示工程，对LLM的安全性构成严重威胁。

Abstract: Recently, Bit-Flip Attack (BFA) has garnered widespread attention for its
ability to compromise software system integrity remotely through hardware fault
injection. With the widespread distillation and deployment of large language
models (LLMs) into single file .gguf formats, their weight spaces have become
exposed to an unprecedented hardware attack surface. This paper is the first to
systematically discover and validate the existence of single-bit
vulnerabilities in LLM weight files: in mainstream open-source models (e.g.,
DeepSeek and QWEN) using .gguf quantized formats, flipping just single bit can
induce three types of targeted semantic level failures Artificial Flawed
Intelligence (outputting factual errors), Artificial Weak Intelligence
(degradation of logical reasoning capability), and Artificial Bad Intelligence
(generating harmful content).
  By building an information theoretic weight sensitivity entropy model and a
probabilistic heuristic scanning framework called BitSifter, we achieved
efficient localization of critical vulnerable bits in models with hundreds of
millions of parameters. Experiments show that vulnerabilities are significantly
concentrated in the tensor data region, particularly in areas related to the
attention mechanism and output layers, which are the most sensitive. A negative
correlation was observed between model size and robustness, with smaller models
being more susceptible to attacks. Furthermore, a remote BFA chain was
designed, enabling semantic-level attacks in real-world environments: At an
attack frequency of 464.3 times per second, a single bit can be flipped with
100% success in as little as 31.7 seconds. This causes the accuracy of LLM to
plummet from 73.5% to 0%, without requiring high-cost equipment or complex
prompt engineering.

</details>


### [39] [Memory-Augmented Log Analysis with Phi-4-mini: Enhancing Threat Detection in Structured Security Logs](https://arxiv.org/abs/2510.00529)
*Anbi Guo,Mahfuza Farooque*

Main category: cs.CR

TL;DR: 提出了DM-RAG框架，通过双记忆检索增强生成技术分析结构化安全日志，在UNSW-NB15数据集上达到53.64%准确率和98.70%召回率。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在结构化安全日志分析中存在上下文限制和领域不匹配问题，难以有效检测高级持续性威胁。

Method: 采用双记忆RAG框架，包含短期记忆缓冲区用于近期摘要和长期FAISS索引记忆用于历史模式，使用指令调优的Phi-4-mini处理上下文，贝叶斯融合确保可靠记忆持久化。

Result: 在UNSW-NB15数据集上，DM-RAG在召回率上超越了微调和RAG基线方法，达到98.70%召回率和53.64%准确率。

Conclusion: 该架构轻量级、可解释且可扩展，能够在无需额外语料库或大量调优的情况下实现实时威胁监控。

Abstract: Structured security logs are critical for detecting advanced persistent
threats (APTs). Large language models (LLMs) struggle in this domain due to
limited context and domain mismatch. We propose \textbf{DM-RAG}, a dual-memory
retrieval-augmented generation framework for structured log analysis. It
integrates a short-term memory buffer for recent summaries and a long-term
FAISS-indexed memory for historical patterns. An instruction-tuned Phi-4-mini
processes the combined context and outputs structured predictions. Bayesian
fusion promotes reliable persistence into memory. On the UNSW-NB15 dataset,
DM-RAG achieves 53.64% accuracy and 98.70% recall, surpassing fine-tuned and
RAG baselines in recall. The architecture is lightweight, interpretable, and
scalable, enabling real-time threat monitoring without extra corpora or heavy
tuning.

</details>


### [40] [Sentry: Authenticating Machine Learning Artifacts on the Fly](https://arxiv.org/abs/2510.00554)
*Andrew Gan,Zahra Ghodsi*

Main category: cs.CR

TL;DR: Sentry是一个基于GPU的框架，通过实现数据集的加密签名和验证来确保机器学习工件的真实性，防止供应链攻击。


<details>
  <summary>Details</summary>
Motivation: 机器学习系统依赖外部数据集和预训练模型，缺乏真实性验证，容易受到供应链攻击。现有的加密验证方案在规模和GPU兼容性方面存在不足。

Method: 开发Sentry框架，将开发者身份与签名绑定，在GPU内存加载工件时进行实时认证，采用GPU加速的Merkle树和格哈希等加密哈希结构，实现内存优化和资源分区。

Result: 评估显示Sentry是实用的解决方案，相比基于CPU的基线实现了数量级的加速。

Conclusion: Sentry为机器学习系统带来了真实性保障，通过GPU加速的加密验证有效缓解了供应链攻击风险。

Abstract: Machine learning systems increasingly rely on open-source artifacts such as
datasets and models that are created or hosted by other parties. The reliance
on external datasets and pre-trained models exposes the system to supply chain
attacks where an artifact can be poisoned before it is delivered to the
end-user. Such attacks are possible due to the lack of any authenticity
verification in existing machine learning systems. Incorporating cryptographic
solutions such as hashing and signing can mitigate the risk of supply chain
attacks. However, existing frameworks for integrity verification based on
cryptographic techniques can incur significant overhead when applied to
state-of-the-art machine learning artifacts due to their scale, and are not
compatible with GPU platforms. In this paper, we develop Sentry, a novel
GPU-based framework that verifies the authenticity of machine learning
artifacts by implementing cryptographic signing and verification for datasets
and models. Sentry ties developer identities to signatures and performs
authentication on the fly as artifacts are loaded on GPU memory, making it
compatible with GPU data movement solutions such as NVIDIA GPUDirect that
bypass the CPU. Sentry incorporates GPU acceleration of cryptographic hash
constructions such as Merkle tree and lattice hashing, implementing memory
optimizations and resource partitioning schemes for a high throughput
performance. Our evaluations show that Sentry is a practical solution to bring
authenticity to machine learning systems, achieving orders of magnitude speedup
over a CPU-based baseline.

</details>


### [41] [IntrusionX: A Hybrid Convolutional-LSTM Deep Learning Framework with Squirrel Search Optimization for Network Intrusion Detection](https://arxiv.org/abs/2510.00572)
*Ahsan Farabi,Muhaiminul Rashid Shad,Israt Khandaker*

Main category: cs.CR

TL;DR: IntrusionX是一种混合深度学习框架，结合CNN和LSTM网络，使用松鼠搜索算法优化超参数，在NSL-KDD数据集上实现了高精度的入侵检测。


<details>
  <summary>Details</summary>
Motivation: 解决入侵检测系统面临的挑战：不断演变的网络攻击、高维流量数据以及NSL-KDD等基准数据集中的严重类别不平衡问题。

Method: 提出IntrusionX混合深度学习框架：CNN用于局部特征提取，LSTM用于时间建模，使用松鼠搜索算法进行超参数优化，并采用严格预处理、分层数据分割和动态类别加权。

Result: 在NSL-KDD数据集上，二元分类准确率达到98%，5类分类准确率达到87%，少数类召回率显著提升（U2R：71%，R2L：93%）。

Conclusion: IntrusionX的创新在于其可复现的、不平衡感知设计结合元启发式优化，有效提升了入侵检测性能。

Abstract: Intrusion Detection Systems (IDS) face persistent challenges due to evolving
cyberattacks, high-dimensional traffic data, and severe class imbalance in
benchmark datasets such as NSL-KDD. To address these issues, we propose
IntrusionX, a hybrid deep learning framework that integrates Convolutional
Neural Networks (CNNs) for local feature extraction and Long Short-Term Memory
(LSTM) networks for temporal modeling. The architecture is further optimized
using the Squirrel Search Algorithm (SSA), enabling effective hyperparameter
tuning while maintaining computational efficiency. Our pipeline incorporates
rigorous preprocessing, stratified data splitting, and dynamic class weighting
to enhance the detection of rare classes. Experimental evaluation on NSL-KDD
demonstrates that IntrusionX achieves 98% accuracy in binary classification and
87% in 5-class classification, with significant improvements in minority class
recall (U2R: 71%, R2L: 93%). The novelty of IntrusionX lies in its
reproducible, imbalance-aware design with metaheuristic optimization.

</details>


### [42] [A Monoid Ring Approach to Color Visual Cryptography](https://arxiv.org/abs/2510.00763)
*Maximilian Reif,Jens Zumbrägel*

Main category: cs.CR

TL;DR: 本文提出了一种新的彩色视觉密码方案，使用更通用的颜色幺半群模型，通过多项式框架和幺半群环构建方案，实现了更小的像素扩展和更高的对比度。


<details>
  <summary>Details</summary>
Motivation: 传统视觉密码方案在彩色图像处理上存在局限性，需要更通用的颜色模型来支持任意颜色叠加，从而获得更好的性能表现。

Method: 采用颜色幺半群模型，重新审视Koga和Ishihara的多项式框架，应用幺半群环理论构建新的彩色视觉密码方案。

Result: 新方案相比同类方案实现了更短的像素扩展和更高的对比度，提升了彩色视觉密码的性能。

Conclusion: 通过幺半群环方法构建的彩色视觉密码方案在像素扩展和对比度方面具有优势，为彩色图像的安全共享提供了更有效的解决方案。

Abstract: A visual cryptography scheme is a secret sharing scheme in which the secret
information is an image and the shares are printed on transparencies, so that
the secret image can be recovered by simply stacking the shares on top of each
other. Such schemes do therefore not require any knowledge of cryptography
tools to recover the secret, and they have widespread applications, for
example, when sharing QR codes or medical images. In this work we deal with
visual cryptography threshold schemes for color images. Our color model differs
from most previous work by allowing arbitrary colors to be stacked, resulting
in a possibly different color. This more general color monoid model enables us
to achieve shorter pixel expansion and higher contrast than comparable schemes.
We revisit the polynomial framework of Koga and Ishihara for constructing
visual cryptography schemes and apply the monoid ring to obtain new schemes for
color visual cryptography.

</details>


### [43] [Fast, Secure, and High-Capacity Image Watermarking with Autoencoded Text Vectors](https://arxiv.org/abs/2510.00799)
*Gautier Evennou,Vivien Chappelier,Ewa Kijak*

Main category: cs.CR

TL;DR: LatentSeal将图像水印重新定义为语义通信，通过文本自编码器将完整句子映射为紧凑的256维潜向量，突破了传统256位有效载荷的限制，实现了高容量、鲁棒且可解释的水印系统。


<details>
  <summary>Details</summary>
Motivation: 传统图像水印系统将嵌入的有效载荷视为无意义的比特，这种比特中心视图限制了容量，并阻止水印携带有用信息。

Method: 使用轻量级文本自编码器将完整句子映射到紧凑的256维单位范数潜向量，通过微调的水印模型进行鲁棒嵌入，并通过秘密可逆旋转进行保护。

Result: 在多个基准测试中超越了现有技术水平，BLEU-4和精确匹配表现优异，同时实现了0.97-0.99的ROC AUC分数，提供了实用的部署操作点。

Conclusion: 通过从比特有效载荷转向语义潜向量，LatentSeal实现了不仅鲁棒和高容量，而且安全可解释的水印，为来源追踪、篡改解释和可信AI治理提供了具体路径。

Abstract: Most image watermarking systems focus on robustness, capacity, and
imperceptibility while treating the embedded payload as meaningless bits. This
bit-centric view imposes a hard ceiling on capacity and prevents watermarks
from carrying useful information. We propose LatentSeal, which reframes
watermarking as semantic communication: a lightweight text autoencoder maps
full-sentence messages into a compact 256-dimensional unit-norm latent vector,
which is robustly embedded by a finetuned watermark model and secured through a
secret, invertible rotation. The resulting system hides full-sentence messages,
decodes in real time, and survives valuemetric and geometric attacks. It
surpasses prior state of the art in BLEU-4 and Exact Match on several
benchmarks, while breaking through the long-standing 256-bit payload ceiling.
It also introduces a statistically calibrated score that yields a ROC AUC score
of 0.97-0.99, and practical operating points for deployment. By shifting from
bit payloads to semantic latent vectors, LatentSeal enables watermarking that
is not only robust and high-capacity, but also secure and interpretable,
providing a concrete path toward provenance, tamper explanation, and
trustworthy AI governance. Models, training and inference code, and data splits
will be available upon publication.

</details>


### [44] [Universally Composable Termination Analysis of Tendermint](https://arxiv.org/abs/2510.01097)
*Zhixin Dong,Xian Xu,Yuhang Zeng,Mingchao Wan,Chunmiao Li*

Main category: cs.CR

TL;DR: 本文首次对Tendermint共识协议进行了通用可组合(UC)安全分析，证明了其在战略消息延迟攻击下的弹性，确保在部分同步网络假设下，即使有f<n/3的拜占庭节点，协议仍能保持安全性和终止性。


<details>
  <summary>Details</summary>
Motivation: 现有对Tendermint安全性和终止性的分析都是孤立进行的，没有考虑与其他协议的并发组合，且在拜占庭对手引起的自适应网络延迟下的终止性尚未得到正式分析。

Method: 通过构建Tendermint的UC理想模型，形式化其核心机制（基于阶段的共识过程、动态超时、提案锁定、领导者轮换等），在网络对手选择性延迟协议消息的情况下进行分析。

Result: 主要结果证明Tendermint协议UC实现了理想Tendermint模型，确保有界终止延迟，即在网络延迟保持在协议定义阈值内时，即使有f<n/3的拜占庭节点也能保证终止。

Conclusion: 通过UC框架内的形式化证明，Tendermint保持了安全性和终止性。根据UC的组合定理，这保证了当Tendermint与各种区块链组件组合时，这些属性仍能得到保持。

Abstract: Modern blockchain systems operating in adversarial environments require
robust consensus protocols that guarantee both safety and termination under
network delay attacks. Tendermint, a widely adopted consensus protocol in
consortium blockchains, achieves high throughput and finality. However,
previous analysis of the safety and termination has been done in a standalone
fashion, with no consideration of the composition with other protocols
interacting with it in a concurrent manner. Moreover, the termination
properties under adaptive network delays caused by Byzantine adversaries have
not been formally analyzed. This paper presents the first universally
composable (UC) security analysis of Tendermint, demonstrating its resilience
against strategic message-delay attacks. By constructing a UC ideal model of
Tendermint, we formalize its core mechanisms: phase-base consensus procedure,
dynamic timeouts, proposal locking, leader rotation, and others, under a
network adversary that selectively delays protocol messages. Our main result
proves that the Tendermint protocol UC-realizes the ideal Tendermint model,
which ensures bounded termination latency, i.e., guaranteed termination, even
when up to $f<n/3$ nodes are Byzantine (where $n$ is the number of nodes
participating in the consensus), provided that network delays remain within a
protocol-defined threshold under the partially synchronous net assumption.
Specifically, through formal proofs within the UC framework, we show that
Tendermint maintains safety and termination. By the composition theorem of UC,
this guarantees that these properties are maintained when Tendermint is
composed with various blockchain components.

</details>


### [45] [EditTrack: Detecting and Attributing AI-assisted Image Editing](https://arxiv.org/abs/2510.01173)
*Zhengyuan Jiang,Yuyang Zhang,Moyang Guo,Neil Zhenqiang Gong*

Main category: cs.CR

TL;DR: 提出了EditTrack框架，用于检测图像是否由特定基础图像通过AI编辑模型生成，并能识别具体使用的编辑模型。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要检测图像是否为AI生成/编辑，但无法判断是否从特定基础图像编辑而来，需要解决图像编辑检测和归属问题。

Method: 基于对编辑过程的四个关键观察，提出重新编辑策略和精心设计的相似度度量，判断可疑图像是否源自基础图像及使用的编辑模型。

Result: 在5个最先进编辑模型和6个数据集上的评估显示，EditTrack能准确进行检测和归属，显著优于5个基线方法。

Conclusion: EditTrack是首个解决图像编辑检测和归属问题的框架，在多个数据集和模型上表现优异。

Abstract: In this work, we formulate and study the problem of image-editing detection
and attribution: given a base image and a suspicious image, detection seeks to
determine whether the suspicious image was derived from the base image using an
AI editing model, while attribution further identifies the specific editing
model responsible. Existing methods for detecting and attributing AI-generated
images are insufficient for this problem, as they focus on determining whether
an image was AI-generated/edited rather than whether it was edited from a
particular base image. To bridge this gap, we propose EditTrack, the first
framework for this image-editing detection and attribution problem. Building on
four key observations about the editing process, EditTrack introduces a novel
re-editing strategy and leverages carefully designed similarity metrics to
determine whether a suspicious image originates from a base image and, if so,
by which model. We evaluate EditTrack on five state-of-the-art editing models
across six datasets, demonstrating that it consistently achieves accurate
detection and attribution, significantly outperforming five baselines.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [46] [Learning to Lead Themselves: Agentic AI in MAS using MARL](https://arxiv.org/abs/2510.00022)
*Ansh Kamthan*

Main category: cs.AI

TL;DR: 本文研究了在多智能体系统中使用自主AI智能体改善任务分配和协调的方法，重点关注无人机配送和仓库自动化应用。


<details>
  <summary>Details</summary>
Motivation: 随着自主系统从原型转向实际部署，多智能体进行去中心化协同决策的能力成为核心需求。

Method: 在合作多智能体强化学习框架下，采用集中训练、分散执行的轻量级多智能体近端策略优化(IPPO)方法，在PettingZoo环境中实现。

Result: 多个同质无人机或智能体能够在没有显式通信的情况下自组织覆盖不同目标。

Conclusion: 自主AI智能体能够有效提升多智能体系统的任务分配和协调能力。

Abstract: As autonomous systems move from prototypes to real deployments, the ability
of multiple agents to make decentralized, cooperative decisions becomes a core
requirement. This paper examines how agentic artificial intelligence, agents
that act independently, adaptively and proactively can improve task allocation
and coordination in multi-agent systems, with primary emphasis on drone
delivery and secondary relevance to warehouse automation. We formulate the
problem in a cooperative multi-agent reinforcement learning setting and
implement a lightweight multi-agent Proximal Policy Optimization, called IPPO,
approach in PyTorch under a centralized-training, decentralized-execution
paradigm. Experiments are conducted in PettingZoo environment, where multiple
homogeneous drones or agents must self-organize to cover distinct targets
without explicit communication.

</details>


### [47] [ToolBrain: A Flexible Reinforcement Learning Framework for Agentic Tools](https://arxiv.org/abs/2510.00023)
*Quy Minh Le,Minh Sao Khue Luu,Khanh-Tung Tran,Duc-Hai Nguyen,Hoang-Quoc-Viet Pham,Quan Le,Hoang Thanh Lam,Hoang D. Nguyen*

Main category: cs.AI

TL;DR: ToolBrain是一个轻量级框架，通过强化学习训练智能代理使用工具，解决手动设计奖励、数据不足和多工具选择等问题，提升代理的工具使用能力。


<details>
  <summary>Details</summary>
Motivation: 当前训练代理使用工具面临手动设计奖励、训练数据有限和多工具选择困难等挑战，导致适应慢、计算资源浪费和性能不佳。

Method: ToolBrain框架支持多种训练策略（GRPO、DPO等强化学习算法和监督学习），提供自定义奖励函数和自动LLM评判系统，具备知识蒸馏、自动任务生成、工具检索、高效微调等功能。

Result: 在代码代理执行邮件搜索任务中，ToolBrain实现了工具使用技能的快速针对性改进（提升达30.0%），同时保持代码库简洁可扩展。

Conclusion: ToolBrain为研究人员和从业者提供了一个用户友好、轻量级的框架，能够有效训练LLM代理在特定领域中使用工具，显著提升性能。

Abstract: Effective tool use is essential for agentic AI, yet training agents to
utilize tools remains challenging due to manually designed rewards, limited
training data, and poor multi-tool selection, resulting in slow adaptation,
wasted computational resources, and suboptimal performance. We introduce
ToolBrain, a lightweight and user-friendly framework for coaching tool use in
agentic models with flexible reinforcement learning (RL), easing the barriers
for researchers and practitioners to adapt LLM-based agents to specific
domains. It supports a wide range of training strategies, including RL
algorithms such as GRPO and DPO, as well as supervised learning. ToolBrain
enables custom reward callables directly on an agent's execution traces or
simply utilizes an automated LLM-as-a-judge system for reward generation. It is
packed with useful capabilities, including knowledge distillation from large to
small models for efficient development, automatic task generation from tool
descriptions, seamless tool retrieval, efficient fine-tuning pipelines with
QLoRA through Unsloth, and quantized inference via bitsandbytes. We demonstrate
ToolBrain through diverse use cases, such as training a CodeAct agent to
autonomously execute email search tasks, showing fast, targeted improvements
(up to 30.0%) in tool-use skills while keeping the codebase simple and
extensible in Agentic AI. Our framework is publicly available at
https://toolbrain.org.

</details>


### [48] [ARS: Adaptive Reasoning Suppression for Efficient Large Reasoning Language Models](https://arxiv.org/abs/2510.00071)
*Dongqi Zheng*

Main category: cs.AI

TL;DR: 提出了一种名为自适应推理抑制（ARS）的训练免费方法，通过动态抑制冗余推理步骤来提升大型推理语言模型的效率，同时保持准确性。


<details>
  <summary>Details</summary>
Motivation: 大型推理语言模型在复杂推理任务中表现出色，但存在计算效率低下的问题，现有方法难以平衡推理质量与推理成本降低。

Method: ARS采用多检查点确定性估计机制和渐进抑制阈值，动态监测推理过程中的确定性，抑制冗余推理步骤。

Result: 在数学推理基准测试中，ARS实现了高达53%的token减少、46.1%的延迟降低和57.9%的能耗减少，同时保持或提高了准确性。

Conclusion: ARS是一种有效的训练免费方法，能够显著提升推理效率，在保持准确性的同时大幅降低计算成本。

Abstract: Large Reasoning Language Models (LRLMs or LRMs) demonstrate remarkable
capabilities in complex reasoning tasks, but suffer from significant
computational inefficiencies due to overthinking phenomena. Existing efficient
reasoning methods face the challenge of balancing reasoning quality with
inference cost reduction. We propose \textbf{Adaptive Reasoning Suppression
(ARS)}, a novel training-free approach that dynamically suppresses redundant
reasoning steps while preserving accuracy through adaptive certainty
monitoring. ARS introduces a multi-checkpoint certainty estimation mechanism
with progressive suppression thresholds, achieving superior efficiency compared
to static suppression methods. Our extensive evaluation across mathematical
reasoning benchmarks using multiple model architectures demonstrates that ARS
achieves up to 53%, 46.1%, and 57.9% in token, latency and energy reduction,
while maintaining or improving accuracy.

</details>


### [49] [NeurIPS should lead scientific consensus on AI policy](https://arxiv.org/abs/2510.00075)
*Rishi Bommasani*

Main category: cs.AI

TL;DR: 论文主张NeurIPS会议应积极推动AI政策科学共识的形成，借鉴IPCC在气候政策方面的经验，通过试点项目来填补当前共识形成机制的空白。


<details>
  <summary>Details</summary>
Motivation: 当前AI政策制定缺乏科学共识形成机制，而NeurIPS作为AI领域的领导者，最适合承担这一角色，以促进更高质量的AI政策制定。

Method: 建议NeurIPS借鉴IPCC在气候政策共识形成方面的经验，开展初步试点项目，通过证据生成和综合来推动科学共识。

Result: 论文识别了当前AI政策共识形成的完全空白，并论证了NeurIPS是填补这一空白的最佳选择。

Conclusion: NeurIPS应该在AI政策科学共识形成方面发挥领导作用，这符合其在AI领域的领导地位，并能产生更高质量的AI政策。

Abstract: Designing wise AI policy is a grand challenge for society. To design such
policy, policymakers should place a premium on rigorous evidence and scientific
consensus. While several mechanisms exist for evidence generation, and nascent
mechanisms tackle evidence synthesis, we identify a complete void on consensus
formation. In this position paper, we argue NeurIPS should actively catalyze
scientific consensus on AI policy. Beyond identifying the current deficit in
consensus formation mechanisms, we argue that NeurIPS is the best option due
its strengths and the paucity of compelling alternatives. To make progress, we
recommend initial pilots for NeurIPS by distilling lessons from the IPCC's
leadership to build scientific consensus on climate policy. We dispel
predictable counters that AI researchers disagree too much to achieve consensus
and that policy engagement is not the business of NeurIPS. NeurIPS leads AI on
many fronts, and it should champion scientific consensus to create higher
quality AI policy.

</details>


### [50] [Towards a Framework for Supporting the Ethical and Regulatory Certification of AI Systems](https://arxiv.org/abs/2510.00084)
*Fabian Kovac,Sebastian Neumaier,Timea Pahi,Torsten Priebe,Rafael Rodrigues,Dimitrios Christodoulou,Maxime Cordy,Sylvain Kubler,Ali Kordia,Georgios Pitsiladis,John Soldatos,Petros Zervoudakis*

Main category: cs.AI

TL;DR: CERTAIN项目开发了一个综合框架，将监管合规、伦理标准和透明度整合到AI系统中，通过语义MLOps、本体驱动的数据谱系跟踪和RegOps工作流来解决AI的伦理、法律和监管挑战。


<details>
  <summary>Details</summary>
Motivation: AI在欧洲社会和经济中的快速发展带来了关键的伦理、法律和监管挑战，需要建立能够确保合规性和透明度的框架。

Method: 开发语义MLOps进行结构化AI生命周期管理，使用本体驱动的数据谱系跟踪确保可追溯性和问责制，以及实施RegOps工作流来操作化合规要求。

Result: 通过在不同试点中实施和验证解决方案，CERTAIN项目推进了监管合规，并促进了符合欧洲标准的负责任AI创新。

Conclusion: CERTAIN项目通过其综合框架成功解决了AI的伦理和监管挑战，为欧洲的负责任AI发展提供了重要支持。

Abstract: Artificial Intelligence has rapidly become a cornerstone technology,
significantly influencing Europe's societal and economic landscapes. However,
the proliferation of AI also raises critical ethical, legal, and regulatory
challenges. The CERTAIN (Certification for Ethical and Regulatory Transparency
in Artificial Intelligence) project addresses these issues by developing a
comprehensive framework that integrates regulatory compliance, ethical
standards, and transparency into AI systems. In this position paper, we outline
the methodological steps for building the core components of this framework.
Specifically, we present: (i) semantic Machine Learning Operations (MLOps) for
structured AI lifecycle management, (ii) ontology-driven data lineage tracking
to ensure traceability and accountability, and (iii) regulatory operations
(RegOps) workflows to operationalize compliance requirements. By implementing
and validating its solutions across diverse pilots, CERTAIN aims to advance
regulatory compliance and to promote responsible AI innovation aligned with
European standards.

</details>


### [51] [Judging by Appearances? Auditing and Intervening Vision-Language Models for Bail Prediction](https://arxiv.org/abs/2510.00088)
*Sagnik Basu,Shubham Prakash,Ashish Maruti Barge,Siddharth D Jaiswal,Abhisek Dash,Saptarshi Ghosh,Animesh Mukherjee*

Main category: cs.AI

TL;DR: 本文审计了视觉语言模型在保释决策预测中的表现，发现模型性能差且会错误拒绝应获保释者，通过RAG管道和法律先例以及微调干预显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 随着视觉语言模型的兴起，法律判决预测系统可能利用罪犯图像和文本案件报告，这可能带来意外后果和恶意使用，因此需要评估其效率。

Method: 首先审计独立VLMs在保释预测任务中的表现，然后设计干预算法：通过RAG管道引入法律先例，并使用创新方案微调VLMs。

Result: 发现模型在多个交叉群体中表现不佳，且会高置信度错误拒绝应获保释者；干预措施显著提高了保释预测性能。

Conclusion: 这项工作为未来在VLMs部署到真实世界法律判决预测之前设计更智能的干预措施铺平了道路。

Abstract: Large language models (LLMs) have been extensively used for legal judgment
prediction tasks based on case reports and crime history. However, with a surge
in the availability of large vision language models (VLMs), legal judgment
prediction systems can now be made to leverage the images of the criminals in
addition to the textual case reports/crime history. Applications built in this
way could lead to inadvertent consequences and be used with malicious intent.
In this work, we run an audit to investigate the efficiency of standalone VLMs
in the bail decision prediction task. We observe that the performance is poor
across multiple intersectional groups and models \textit{wrongly deny bail to
deserving individuals with very high confidence}. We design different
intervention algorithms by first including legal precedents through a RAG
pipeline and then fine-tuning the VLMs using innovative schemes. We demonstrate
that these interventions substantially improve the performance of bail
prediction. Our work paves the way for the design of smarter interventions on
VLMs in the future, before they can be deployed for real-world legal judgment
prediction.

</details>


### [52] [AuditAgent: Expert-Guided Multi-Agent Reasoning for Cross-Document Fraudulent Evidence Discovery](https://arxiv.org/abs/2510.00156)
*Songran Bai,Bingzhe Wu,Yiwei Zhang,Chengke Wu,Xiaolong Zheng,Yaze Yuan,Ke Wu,Jianqiang Li*

Main category: cs.AI

TL;DR: 提出了一个名为AuditAgent的多智能体推理框架，用于金融欺诈检测中的细粒度证据链定位，该框架整合了审计领域专业知识，在真实金融欺诈案例中表现出色。


<details>
  <summary>Details</summary>
Motivation: 现实世界中的金融欺诈检测面临重大挑战，因为证据往往微妙且分散在复杂多年的财务披露中，需要更有效的自动化检测方法。

Method: 开发了基于多智能体推理的AuditAgent框架，整合了主体级风险先验、混合检索策略和专门化智能体模块，利用中国证监会发布的执法文件和财务报告构建专家标注数据集。

Result: 实验表明该方法在召回率和可解释性方面显著优于通用智能体范式，为自动化透明金融取证建立了新基准。

Conclusion: 研究强调了领域特定推理和数据集构建对于推进实际监管应用中稳健金融欺诈检测的价值。

Abstract: Financial fraud detection in real-world scenarios presents significant
challenges due to the subtlety and dispersion of evidence across complex,
multi-year financial disclosures. In this work, we introduce a novel
multi-agent reasoning framework AuditAgent, enhanced with auditing domain
expertise, for fine-grained evidence chain localization in financial fraud
cases. Leveraging an expert-annotated dataset constructed from enforcement
documents and financial reports released by the China Securities Regulatory
Commission, our approach integrates subject-level risk priors, a hybrid
retrieval strategy, and specialized agent modules to efficiently identify and
aggregate cross-report evidence. Extensive experiments demonstrate that our
method substantially outperforms General-Purpose Agent paradigm in both recall
and interpretability, establishing a new benchmark for automated, transparent
financial forensics. Our results highlight the value of domain-specific
reasoning and dataset construction for advancing robust financial fraud
detection in practical, real-world regulatory applications.

</details>


### [53] [Drones that Think on their Feet: Sudden Landing Decisions with Embodied AI](https://arxiv.org/abs/2510.00167)
*Diego Ortiz Barbosa,Mohit Agrawal,Yash Malegaonkar,Luis Burbano,Axel Andersson,György Dán,Henrik Sandberg,Alvaro A. Cardenas*

Main category: cs.AI

TL;DR: 该论文提出使用具身AI和大视觉语言模型来增强自主无人机对突发事件的响应能力，替代传统手写恢复规则的方法。


<details>
  <summary>Details</summary>
Motivation: 传统方法依赖安全工程师手动编写大量恢复规则，但无法预测现实世界中的各种意外情况，且规则集很快会变得不完整。

Method: 使用具身AI和大视觉语言模型，在Unreal Engine模拟的城市场景中，让无人机动态解读环境并实时生成适当的紧急着陆动作。

Result: 具身AI使得以前无法手动设计的新型自适应恢复和决策流程成为可能。

Conclusion: 该方法提升了自主航空系统的弹性和安全性，为无人机应急响应提供了新的解决方案。

Abstract: Autonomous drones must often respond to sudden events, such as alarms,
faults, or unexpected changes in their environment, that require immediate and
adaptive decision-making. Traditional approaches rely on safety engineers
hand-coding large sets of recovery rules, but this strategy cannot anticipate
the vast range of real-world contingencies and quickly becomes incomplete.
Recent advances in embodied AI, powered by large visual language models,
provide commonsense reasoning to assess context and generate appropriate
actions in real time. We demonstrate this capability in a simulated urban
benchmark in the Unreal Engine, where drones dynamically interpret their
surroundings and decide on sudden maneuvers for safe landings. Our results show
that embodied AI makes possible a new class of adaptive recovery and
decision-making pipelines that were previously infeasible to design by hand,
advancing resilience and safety in autonomous aerial systems.

</details>


### [54] [Object-Centric Case-Based Reasoning via Argumentation](https://arxiv.org/abs/2510.00185)
*Gabriel de Olim Gaul,Adam Gould,Avinash Kori,Francesca Toni*

Main category: cs.AI

TL;DR: SAA-CBR是一种新的神经符号图像分类方法，结合了神经Slot Attention组件和符号推理的AA-CBR方法。


<details>
  <summary>Details</summary>
Motivation: 开发一个集成神经学习和符号推理的图像分类系统，利用对象中心学习和抽象论证进行案例推理。

Method: 使用Slot Attention进行对象中心学习，通过AA-CBR进行符号推理，包括特征组合策略、案例库缩减、基于计数的偏序、One-Vs-Rest多类分类策略以及支持性AA-CBR变体。

Result: 在CLEVR-Hans数据集上表现出色，与基线模型相比具有竞争力。

Conclusion: SAA-CBR是一个有效的分类器，成功整合了神经和符号组件，在图像分类任务中展现了良好性能。

Abstract: We introduce Slot Attention Argumentation for Case-Based Reasoning (SAA-CBR),
a novel neuro-symbolic pipeline for image classification that integrates
object-centric learning via a neural Slot Attention (SA) component with
symbolic reasoning conducted by Abstract Argumentation for Case-Based Reasoning
(AA-CBR). We explore novel integrations of AA-CBR with the neural component,
including feature combination strategies, casebase reduction via representative
samples, novel count-based partial orders, a One-Vs-Rest strategy for extending
AA-CBR to multi-class classification, and an application of Supported AA-CBR, a
bipolar variant of AA-CBR. We demonstrate that SAA-CBR is an effective
classifier on the CLEVR-Hans datasets, showing competitive performance against
baseline models.

</details>


### [55] [Thinkquel: A Model Dedicated to Text-to-dbt Using Synthetic Data and a Span-Aware Objective](https://arxiv.org/abs/2510.00186)
*Anni Li,Aria Attar,Paul Dong*

Main category: cs.AI

TL;DR: Thinkquel是一个经过微调的模型，用于生成可靠、可移植且经过执行验证的数据库查询。它集成了新颖的合成数据管道TS-SQL和专门设计的Token-Sequence GRPO强化学习目标，以弥合令牌级训练信号和序列级执行奖励之间的差距。


<details>
  <summary>Details</summary>
Motivation: 将自然语言请求转换为可靠、生产就绪的数据转换仍然具有挑战性：正确性依赖于精确的模式链接和仓库特定的SQL方言，而训练期间最强的监督——执行成功和结果匹配——仅在序列级别提供。同时，组装大型、经过执行验证的语料库成本高昂，令牌级目标与这些全局信号不匹配，导致优化不稳定和可移植性有限。

Method: Thinkquel集成了一种新颖的合成数据管道TS-SQL，利用dbt作为可移植的中间表示，并结合了跨度感知的强化学习目标Token-Sequence GRPO（TS-GRPO），专门设计用于在微调LLMs时弥合令牌级训练信号和序列级执行奖励之间的差距。

Result: 在500个示例的TS-SQL测试集上，Thinkquel（32B）通过两阶段SFT课程达到了93.2%的执行成功率和61.8%的精确结果匹配率，相对于基础模型分别提高了67.2%（执行）和44.4%（匹配）。在Spider（14B）实验中，TS-GRPO相对于GRPO和GSPO提高了训练稳定性并加速了执行匹配奖励的收敛。

Conclusion: Thinkquel通过集成TS-SQL合成数据管道和TS-GRPO强化学习目标，成功解决了自然语言到SQL转换中的挑战，显著提高了执行成功率和结果匹配率，同时改善了训练稳定性和收敛速度。

Abstract: Transforming natural-language requests into reliable, production-ready data
transformations remains challenging: correctness depends on precise schema
linking and warehouse-specific SQL dialects, while the strongest supervision
available during training--execution success and result matching--are provided
only at the sequence level. At the same time, assembling large,
execution-validated corpora is costly, and token-level objectives misalign with
these global signals, yielding unstable optimization and limited portability.
We introduce Thinkquel, a fine-tuned model for producing robust, portable, and
execution-validated database queries. Methodologies in Thinkquel integrates a
novel synthetic data pipeline, TS-SQL, that leverages dbt as a portable
intermediate representation with a span-aware reinforcement learning objective,
and Token-Sequence GRPO (TS-GRPO), specifically designed to bridge the gap
between token-level training signals and sequence-level execution rewards when
finetuning LLMs. On the 500-example TS-SQL test set, Thinkquel (32B) reaches
93.2\% execution success and 61.8\% exact-result match with a two-stage SFT
curriculum, improving over the base model by 67.2\% (exec.) and 44.4\% (match).
In Spider (14B) experiments, TS-GRPO increases training stability and speeds
convergence of the execution-match reward relative to GRPO and GSPO.

</details>


### [56] [DualTune: Decoupled Fine-Tuning for On-Device Agentic Systems](https://arxiv.org/abs/2510.00229)
*Rohan Kadekodi,Zhan Jin,Keisuke Kamahori,Yile Gu,Sean Khatiri,Noah H. Bayindirli,Sergey Gorbunov,Baris Kasikci*

Main category: cs.AI

TL;DR: 提出了一种解耦微调方法，将工具调用任务分解为工具选择和参数生成两个子任务，通过专用LoRA适配器和分层编排提升本地LLM的工具调用性能。


<details>
  <summary>Details</summary>
Motivation: 本地LLM在工具调用场景中表现不佳，无法与前沿模型竞争，特别是在大型工具集选择和复杂参数生成方面存在困难，需要隐私保护且成本效益高的本地推理解决方案。

Method: 采用解耦微调方法，使用LoRA微调创建专用适配器，分别处理工具选择和工具特定参数生成，通过分离损失掩码优化每个子任务。DualTune推理框架动态加载相应LoRA适配器，并实施分层编排限制工具选择数量。

Result: 在MCP-Bench基准测试中，使用解耦微调的Qwen-2.5-7B模型将基础模型的工具调用准确率提高了46%，在大多数情况下优于相似规模的其他模型，甚至优于2倍大小的模型。

Conclusion: 解耦微调和DualTune框架有效提升了本地LLM的工具调用能力，为设备端推理提供了高效解决方案，在保持隐私和成本效益的同时显著提升了性能。

Abstract: The deployment of Large Language Models (LLMs) as agentic orchestrators has
revolutionized task automation, but the need for privacy-preserving,
cost-effective solutions demands on-device inference capabilities. However,
local LLMs consistently underperform compared to frontier models in tool
calling scenarios, struggling with both tool selection from large tool sets and
accurate argument generation for complex parameter structures. We introduce a
methodology that disaggregates a tool-calling task into two distinct subtasks:
tool selection and argument generation. We propose "decoupled fine-tuning", a
novel post-training approach that employs LoRA fine-tuning to create dedicated
LoRA adapters for tool selection and tool-specific argument generation using
separate loss masking for each of the subtasks. Furthermore, we present
DualTune, an inference framework that leverages the LoRA adapters created using
decoupled fine-tuning to perform efficient agent orchestration with the help of
local models on end-user devices. DualTune decomposes the tool-call generation
step into tool selection and argument generation, and dynamically loads the
corresponding LoRA adapters to generate tool calls. Additionally, DualTune
implements hierarchical orchestration to restrict the number of tools required
for tool selection. Our experiments on the MCP-Bench benchmark demonstrate that
the Qwen-2.5-7B model trained using decoupled fine-tuning improves the tool
calling accuracy of the base model by 46%, and outperforms other local
reasoning, non-reasoning and fine-tuned models of similar size in all cases,
and models that are 2x larger, in most cases.

</details>


### [57] [MAGIC-MASK: Multi-Agent Guided Inter-Agent Collaboration with Mask-Based Explainability for Reinforcement Learning](https://arxiv.org/abs/2510.00274)
*Maisha Maliha,Dean Hougen*

Main category: cs.AI

TL;DR: 提出了MAGIC-MASK框架，将基于扰动的可解释性方法扩展到多智能体强化学习，通过智能体间协作共享掩码状态信息和经验，提高关键状态发现效率和解释保真度。


<details>
  <summary>Details</summary>
Motivation: 解决深度强化学习智能体在安全关键和多智能体环境中的决策过程理解问题，克服现有方法在计算成本、探索覆盖和多智能体适应性方面的限制。

Method: 集成近端策略优化、自适应epsilon-greedy探索和轻量级智能体间协作，通过轨迹扰动、奖励保真度分析和KL散度正则化的统一数学形式化框架。

Result: 在单智能体和多智能体基准测试中，包括多智能体高速公路驾驶环境和Google Research Football，MAGIC-MASK在保真度、学习效率和策略鲁棒性方面持续优于最先进基线方法。

Conclusion: 该框架通过概率建模和多智能体马尔可夫决策过程，为多智能体系统提供了局部化、可解释的解释，实现了从单智能体到多智能体系统的可解释性泛化。

Abstract: Understanding the decision-making process of Deep Reinforcement Learning
agents remains a key challenge for deploying these systems in safety-critical
and multi-agent environments. While prior explainability methods like
StateMask, have advanced the identification of critical states, they remain
limited by computational cost, exploration coverage, and lack of adaptation to
multi-agent settings. To overcome these limitations, we propose a
mathematically grounded framework, MAGIC-MASK (Multi-Agent Guided Inter-agent
Collaboration with Mask-Based Explainability for Reinforcement Learning), that
extends perturbation-based explanation to Multi-Agent Reinforcement Learning.
Our method integrates Proximal Policy Optimization, adaptive epsilon-greedy
exploration, and lightweight inter-agent collaboration to share masked state
information and peer experience. This collaboration enables each agent to
perform saliency-guided masking and share reward-based insights with peers,
reducing the time required for critical state discovery, improving explanation
fidelity, and leading to faster and more robust learning. The core novelty of
our approach lies in generalizing explainability from single-agent to
multi-agent systems through a unified mathematical formalism built on
trajectory perturbation, reward fidelity analysis, and Kullback-Leibler
divergence regularization. This framework yields localized, interpretable
explanations grounded in probabilistic modeling and multi-agent Markov decision
processes. We validate our framework on both single-agent and multi-agent
benchmarks, including a multi-agent highway driving environment and Google
Research Football, demonstrating that MAGIC-MASK consistently outperforms
state-of-the-art baselines in fidelity, learning efficiency, and policy
robustness while offering interpretable and transferable explanations.

</details>


### [58] [ICL Optimized Fragility](https://arxiv.org/abs/2510.00300)
*Serena Gomez Wannaz*

Main category: cs.AI

TL;DR: ICL引导虽然能提升特定任务性能，但会导致跨领域推理能力下降，形成"优化脆弱性"现象。


<details>
  <summary>Details</summary>
Motivation: 探索ICL引导对跨领域认知能力的影响，特别是推理能力的系统性变化。

Method: 使用GPT-OSS:20b模型的6种变体（1个基线+5种ICL配置），进行840项测试，涵盖常识问题、逻辑谜题和数学奥赛题，通过ANOVA进行统计分析。

Result: ICL模型在常识任务上达到91%-99%准确率，但在复杂推理问题上表现下降，谜题准确率降至10-43%（基线为43%），数学奥赛题无显著差异。

Conclusion: ICL引导在效率和推理灵活性之间存在系统性权衡，对LLM部署和AI安全有重要启示。

Abstract: ICL guides are known to improve task-specific performance, but their impact
on cross-domain cognitive abilities remains unexplored. This study examines how
ICL guides affect reasoning across different knowledge domains using six
variants of the GPT-OSS:20b model: one baseline model and five ICL
configurations (simple, chain-of-thought, random, appended text, and symbolic
language). The models were subjected to 840 tests spanning general knowledge
questions, logic riddles, and a mathematical olympiad problem. Statistical
analysis (ANOVA) revealed significant behavioral modifications (p less than
0.001) across ICL variants, demonstrating a phenomenon termed "optimized
fragility." ICL models achieved 91%-99% accuracy on general knowledge tasks
while showing degraded performance on complex reasoning problems, with accuracy
dropping to 10-43% on riddles compared to 43% for the baseline model. Notably,
no significant differences emerged on the olympiad problem (p=0.2173),
suggesting that complex mathematical reasoning remains unaffected by ICL
optimization. These findings indicate that ICL guides create systematic
trade-offs between efficiency and reasoning flexibility, with important
implications for LLM deployment and AI safety.

</details>


### [59] [BiasBusters: Uncovering and Mitigating Tool Selection Bias in Large Language Models](https://arxiv.org/abs/2510.00307)
*Thierry Blankenstein,Jialin Yu,Zixuan Li,Vassilis Plachouras,Sunando Sengupta,Philip Torr,Yarin Gal,Alasdair Paren,Adel Bibi*

Main category: cs.AI

TL;DR: 本文研究了LLM代理在工具选择中的偏见问题，发现模型会系统性地偏向某些提供商或列表位置靠前的工具，提出了评估基准和轻量级缓解方法。


<details>
  <summary>Details</summary>
Motivation: LLM代理依赖外部工具市场，但工具选择如果存在系统性偏见，会损害用户体验和市场竞争公平性，需要研究这种偏见的成因和缓解方法。

Method: 构建包含多个功能等效工具的基准，测试7个模型的选择行为，通过控制实验分析工具特征、元数据和预训练暴露的影响，并提出基于过滤和均匀采样的缓解方法。

Result: 发现语义对齐是选择的最强预测因子，扰动描述会显著改变选择，重复预训练暴露会放大偏见，提出的缓解方法能有效减少偏见同时保持任务覆盖。

Conclusion: 工具选择偏见是工具增强LLM公平部署的关键障碍，需要关注和解决。

Abstract: Agents backed by large language models (LLMs) often rely on external tools
drawn from marketplaces where multiple providers offer functionally equivalent
options. This raises a critical point concerning fairness: if selection is
systematically biased, it can degrade user experience and distort competition
by privileging some providers over others. We introduce a benchmark of diverse
tool categories, each containing multiple functionally equivalent tools, to
evaluate tool-selection bias. Using this benchmark, we test seven models and
show that unfairness exists with models either fixating on a single provider or
disproportionately preferring earlier-listed tools in context. To investigate
the origins of this bias, we conduct controlled experiments examining tool
features, metadata (name, description, parameters), and pre-training exposure.
We find that: (1) semantic alignment between queries and metadata is the
strongest predictor of choice; (2) perturbing descriptions significantly shifts
selections; and (3) repeated pre-training exposure to a single endpoint
amplifies bias. Finally, we propose a lightweight mitigation that first filters
the candidate tools to a relevant subset and then samples uniformly, reducing
bias while preserving good task coverage. Our findings highlight tool-selection
bias as a key obstacle for the fair deployment of tool-augmented LLMs.

</details>


### [60] [When Hallucination Costs Millions: Benchmarking AI Agents in High-Stakes Adversarial Financial Markets](https://arxiv.org/abs/2510.00332)
*Zeshi Dai,Zimo Peng,Zerui Cheng,Ryan Yihe Li*

Main category: cs.AI

TL;DR: CAIA基准测试揭示了AI在对抗性高风险环境中的严重能力缺陷，特别是在加密货币市场等存在主动欺骗的领域，即使最先进的模型也难以区分真相与操纵，在不可逆决策中表现不佳。


<details>
  <summary>Details</summary>
Motivation: 现有AI评估主要关注受控环境下的任务完成能力，但现实世界部署需要AI具备抵御主动欺骗的韧性。加密货币市场2024年因漏洞损失300亿美元，是测试AI在对抗性环境中表现的良好试验场。

Method: 使用加密货币市场作为测试平台，评估17个模型在178个时间锚定任务上的表现，要求AI区分真相与操纵、导航碎片化信息环境，并在对抗压力下做出不可逆的金融决策。

Result: 无工具时前沿模型准确率仅28%，工具增强后提升至67.4%，但仍低于80%的人类基准。模型存在系统性工具选择灾难，偏好不可靠的网页搜索而非权威数据源，即使正确答案可通过专业工具直接获取。

Conclusion: 当前模型尽管在推理得分上令人印象深刻，但在需要抵御主动对抗的环境中仍存在根本性不足。对抗性鲁棒性是可信AI自主性的必要条件，CAIA基准为此提供了持续更新的评估框架。

Abstract: We present CAIA, a benchmark exposing a critical blind spot in AI evaluation:
the inability of state-of-the-art models to operate in adversarial, high-stakes
environments where misinformation is weaponized and errors are irreversible.
While existing benchmarks measure task completion in controlled settings,
real-world deployment demands resilience against active deception. Using crypto
markets as a testbed where $30 billion was lost to exploits in 2024, we
evaluate 17 models on 178 time-anchored tasks requiring agents to distinguish
truth from manipulation, navigate fragmented information landscapes, and make
irreversible financial decisions under adversarial pressure.
  Our results reveal a fundamental capability gap: without tools, even frontier
models achieve only 28% accuracy on tasks junior analysts routinely handle.
Tool augmentation improves performance but plateaus at 67.4% versus 80% human
baseline, despite unlimited access to professional resources. Most critically,
we uncover a systematic tool selection catastrophe: models preferentially
choose unreliable web search over authoritative data, falling for SEO-optimized
misinformation and social media manipulation. This behavior persists even when
correct answers are directly accessible through specialized tools, suggesting
foundational limitations rather than knowledge gaps. We also find that Pass@k
metrics mask dangerous trial-and-error behavior for autonomous deployment.
  The implications extend beyond crypto to any domain with active adversaries,
e.g. cybersecurity, content moderation, etc. We release CAIA with contamination
controls and continuous updates, establishing adversarial robustness as a
necessary condition for trustworthy AI autonomy. The benchmark reveals that
current models, despite impressive reasoning scores, remain fundamentally
unprepared for environments where intelligence must survive active opposition.

</details>


### [61] [Hierarchical Reasoning Model: A Critical Supplementary Material](https://arxiv.org/abs/2510.00355)
*Renee Ge,Qianli Liao,Tomaso Poggio*

Main category: cs.AI

TL;DR: 对Transformer在潜在空间中进行循环推理的层次推理模型进行批判性回顾，提出改进变体，在数独和迷宫任务上取得显著性能提升


<details>
  <summary>Details</summary>
Motivation: Transformer在逻辑推理方面表现不佳，可能源于缺乏对潜在空间和循环推理等创造性应用的探索

Method: 对层次推理模型进行关键设计选择分析，提出改进变体，在潜在空间中实现循环推理

Result: 在Sudoku-Extreme和Maze-Hard任务上取得了比之前报告显著更好的性能

Conclusion: 研究结果提出了令人惊讶的观察和进一步研究的有趣方向，表明这类模型仍处于早期阶段

Abstract: Transformers have demonstrated remarkable performance in natural language
processing and related domains, as they largely focus on sequential,
autoregressive next-token prediction tasks. Yet, they struggle in logical
reasoning, not necessarily because of a fundamental limitation of these models,
but possibly due to the lack of exploration of more creative uses, such as
latent space and recurrent reasoning. An emerging exploration in this direction
is the Hierarchical Reasoning Model (Wang et al., 2025), which introduces a
novel type of recurrent reasoning in the latent space of transformers,
achieving remarkable performance on a wide range of 2D reasoning tasks. Despite
the promising results, this line of models is still at an early stage and calls
for in-depth investigation. In this work, we perform a critical review on this
class of models, examine key design choices and present intriguing variants
that achieve significantly better performance on the Sudoku-Extreme and
Maze-Hard tasks than previously reported. Our results also raise surprising
observations and intriguing directions for further research.

</details>


### [62] [Semantic-Driven AI Agent Communications: Challenges and Solutions](https://arxiv.org/abs/2510.00381)
*Kaiwen Yu,Mengying Sun,Zhijin Qin,Xiaodong Xu,Ping Yang,Yue Xiao,Gang Wu*

Main category: cs.AI

TL;DR: 提出语义驱动的AI智能体通信框架，包含语义自适应传输、语义轻量化传输和语义自进化控制三大技术，解决动态环境和有限资源下的AI智能体通信挑战。


<details>
  <summary>Details</summary>
Motivation: 随着智能服务快速发展，通信对象从人类转向AI智能体，需要新范式支持实时感知、决策和协作。语义通信虽具前景，但受限于动态环境和资源约束。

Method: 1. 语义自适应传输：使用真实或生成样本进行微调，使模型适应变化环境；2. 语义轻量化传输：结合剪枝、量化和感知感知采样，降低模型复杂度；3. 语义自进化控制：采用分布式分层决策优化多维资源。

Result: 仿真结果显示，所提方案实现更快收敛和更强鲁棒性，分布式分层优化方法显著优于传统决策方案。

Conclusion: 该框架为AI智能体通信网络提供了有效解决方案，具有实际部署潜力。

Abstract: With the rapid growth of intelligent services, communication targets are
shifting from humans to artificial intelligent (AI) agents, which require new
paradigms to enable real-time perception, decision-making, and collaboration.
Semantic communication, which conveys task-relevant meaning rather than raw
data, offers a promising solution. However, its practical deployment remains
constrained by dynamic environments and limited resources. To address these
issues, this article proposes a semantic-driven AI agent communication
framework and develops three enabling techniques. First, semantic adaptation
transmission applies fine-tuning with real or generative samples to efficiently
adapt models to varying environments. Second, semantic lightweight transmission
incorporates pruning, quantization, and perception-aware sampling to reduce
model complexity and alleviate computational burden on edge agents. Third,
semantic self-evolution control employs distributed hierarchical
decision-making to optimize multi-dimensional resources, enabling robust
multi-agent collaboration in dynamic environments. Simulation results show that
the proposed solutions achieve faster convergence and stronger robustness,
while the proposed distributed hierarchical optimization method significantly
outperforms conventional decision-making schemes, highlighting its potential
for AI agent communication networks.

</details>


### [63] [Adaptive Federated Few-Shot Rare-Disease Diagnosis with Energy-Aware Secure Aggregation](https://arxiv.org/abs/2510.00976)
*Aueaphum Aueawatthanaphisut*

Main category: cs.AI

TL;DR: 提出AFFR框架，通过元学习联邦优化、能量感知客户端调度和安全聚合，解决罕见病诊断中的数据稀缺、设备掉线和隐私保护问题，在模拟数据集上实现10%准确率提升和50%掉线率降低。


<details>
  <summary>Details</summary>
Motivation: 罕见病诊断面临数据极度稀缺、隐私担忧和边缘设备资源有限等挑战，现有方法往往孤立处理这些问题，需要统一的可部署解决方案。

Method: 结合三个支柱：基于元学习的少样本联邦优化、能量感知客户端调度机制、以及带校准差分隐私的安全聚合，形成模块化管道。

Result: 在模拟罕见病检测数据集上，相比基线联邦学习准确率提升10%，客户端掉线率降低50%以上，收敛性不受影响，隐私-效用权衡在临床可接受范围内。

Conclusion: AFFR为罕见病的公平可信联邦诊断提供了实用路径，统一解决了数据稀缺、设备稳定性和隐私保护等关键挑战。

Abstract: Rare-disease diagnosis remains one of the most pressing challenges in digital
health, hindered by extreme data scarcity, privacy concerns, and the limited
resources of edge devices. This paper proposes the Adaptive Federated Few-Shot
Rare-Disease Diagnosis (AFFR) framework, which integrates three pillars: (i)
few-shot federated optimization with meta-learning to generalize from limited
patient samples, (ii) energy-aware client scheduling to mitigate device
dropouts and ensure balanced participation, and (iii) secure aggregation with
calibrated differential privacy to safeguard sensitive model updates. Unlike
prior work that addresses these aspects in isolation, AFFR unifies them into a
modular pipeline deployable on real-world clinical networks. Experimental
evaluation on simulated rare-disease detection datasets demonstrates up to 10%
improvement in accuracy compared with baseline FL, while reducing client
dropouts by over 50% without degrading convergence. Furthermore,
privacy-utility trade-offs remain within clinically acceptable bounds. These
findings highlight AFFR as a practical pathway for equitable and trustworthy
federated diagnosis of rare conditions.

</details>


### [64] [Towards Self-Evolving Benchmarks: Synthesizing Agent Trajectories via Test-Time Exploration under Validate-by-Reproduce Paradigm](https://arxiv.org/abs/2510.00415)
*Dadi Guo,Tianyi Zhou,Dongrui Liu,Chen Qian,Qihan Ren,Shuai Shao,Zhiyuan Fan,Yi R. Fung,Kun Wang,Linfeng Zhang,Jing Shao*

Main category: cs.AI

TL;DR: TRACE框架通过让智能体自由探索和演化现有基准任务，生成更高难度的新任务，并记录可验证的执行轨迹，解决了现有智能体基准快速达到性能天花板的问题。


<details>
  <summary>Details</summary>
Motivation: 现有智能体基准显示出新开发智能体快速达到性能上限的趋势，难以满足评估智能体能力的需求。

Method: TRACE框架包含三个阶段：进化提案挖掘（通过初步探索和发散思维提供任务进化提案）、问题形成与自由探索（将提案概念化为可行问题候选，智能体自由探索并记录执行轨迹）、多级验证（确保进化任务具有可验证和可复现的轨迹）。

Result: 在GAIA基准上的实验表明，TRACE框架能持续提升任务复杂度，同时通过可验证的执行轨迹提高正确性的可靠性。

Conclusion: 这项工作标志着从静态、人工策划的基准向动态、自进化评估系统的范式转变，为智能体发展提供了可持续且具有挑战性的跑道。

Abstract: Recent advances in large language models (LLMs) and agent system designs have
empowered agents with unprecedented levels of capability. However, existing
agent benchmarks are showing a trend of rapid ceiling-hitting by newly
developed agents, making it difficult to meet the demands for evaluating agent
abilities. To address this problem, we propose the Trajectory-based
Validated-by-Reproducing Agent-benchmark Complexity Evolution (TRACE)
framework. This framework takes an original task from an existing benchmark and
encourages agents to freely explore and evolve it into a new task with higher
difficulty while recording validatable agent trajectories. The framework
proceeds in three stages: (1) evolutionary proposal mining, which provides task
evolution proposals through preliminary exploration and divergent thinking; (2)
problem formation and free exploration, where proposals are conceptualized into
feasible problem candidates and the agents then explore them freely while
recording their execution trajectories; and (3) multi-level validation, which
ensures that the evolved tasks are accompanied by validatable and reproducible
trajectories. Experiments on the GAIA benchmark demonstrate that the TRACE
framework consistently enhances task complexity while improving the reliability
of correctness through validatable execution trajectories. This work marks a
paradigm shift from static, manually curated benchmarks to dynamic,
self-evolving evaluation systems, providing a sustainable and challenging
runway for agent development.

</details>


### [65] [Automated Evaluation can Distinguish the Good and Bad AI Responses to Patient Questions about Hospitalization](https://arxiv.org/abs/2510.00436)
*Sarvesh Soni,Dina Demner-Fushman*

Main category: cs.AI

TL;DR: 该研究探讨了自动化评估AI系统回答患者健康问题的可行性，通过系统研究评估方法，发现精心设计的自动化评估能够有效扩展AI系统的比较评估。


<details>
  <summary>Details</summary>
Motivation: 当前评估AI回答患者健康问题的黄金标准——人类专家评审——劳动密集且缓慢，限制了可扩展性。自动化指标虽然前景广阔，但与人类判断的一致性不一且常常依赖上下文。

Method: 在100个患者案例中，收集了28个AI系统的回答（共2800个），从三个维度评估：是否回答问题、是否适当使用临床记录证据、是否使用一般医学知识。使用临床医生撰写的参考答案作为指标锚点。

Result: 自动化排名与专家评分高度匹配，表明精心设计的自动化评估能够有效扩展AI系统的比较评估。

Conclusion: 研究结果表明，精心设计的自动化评估可以扩展AI系统的比较评估，并支持患者-临床医生沟通。

Abstract: Automated approaches to answer patient-posed health questions are rising, but
selecting among systems requires reliable evaluation. The current gold standard
for evaluating the free-text artificial intelligence (AI) responses--human
expert review--is labor-intensive and slow, limiting scalability. Automated
metrics are promising yet variably aligned with human judgments and often
context-dependent. To address the feasibility of automating the evaluation of
AI responses to hospitalization-related questions posed by patients, we
conducted a large systematic study of evaluation approaches. Across 100 patient
cases, we collected responses from 28 AI systems (2800 total) and assessed them
along three dimensions: whether a system response (1) answers the question, (2)
appropriately uses clinical note evidence, and (3) uses general medical
knowledge. Using clinician-authored reference answers to anchor metrics,
automated rankings closely matched expert ratings. Our findings suggest that
carefully designed automated evaluation can scale comparative assessment of AI
systems and support patient-clinician communication.

</details>


### [66] [Expandable Decision-Making States for Multi-Agent Deep Reinforcement Learning in Soccer Tactical Analysis](https://arxiv.org/abs/2510.00480)
*Kenjiro Ide,Taiga Someya,Kohei Kawaguchi,Keisuke Fujii*

Main category: cs.AI

TL;DR: 提出EDMS方法，通过语义增强的状态表示和动作掩码机制，构建可解释的球员级智能体模型，用于足球等入侵性团队运动的战术分析。


<details>
  <summary>Details</summary>
Motivation: 传统基于规则的分析方法直观但有限，现代机器学习模型缺乏明确的智能体表示和战术可解释性。需要构建既能从数据中学习，又具有战术可解释性和跨数据源鲁棒性的球员级智能体模型。

Method: 提出Expandable Decision-Making States (EDMS)，通过增强原始位置和速度信息，添加关系变量（如空间得分、传球和得分评估），并结合动作掩码方案，为持球和无球球员提供不同的决策集。

Result: EDMS与动作掩码相比基线方法，持续降低了动作预测损失和时间差分误差。定性案例研究和Q值可视化显示EDMS能够突出高风险高回报的战术模式。

Conclusion: EDMS方法能够将学习到的价值函数和动作策略映射到人类可解释的战术概念，与比赛规则对齐，并支持跨数据源评估和可重复实验。

Abstract: Invasion team sports such as soccer produce a high-dimensional, strongly
coupled state space as many players continuously interact on a shared field,
challenging quantitative tactical analysis. Traditional rule-based analyses are
intuitive, while modern predictive machine learning models often perform
pattern-matching without explicit agent representations. The problem we address
is how to build player-level agent models from data, whose learned values and
policies are both tactically interpretable and robust across heterogeneous data
sources. Here, we propose Expandable Decision-Making States (EDMS), a
semantically enriched state representation that augments raw positions and
velocities with relational variables (e.g., scoring of space, pass, and score),
combined with an action-masking scheme that gives on-ball and off-ball agents
distinct decision sets. Compared to prior work, EDMS maps learned value
functions and action policies to human-interpretable tactical concepts (e.g.,
marking pressure, passing lanes, ball accessibility) instead of raw coordinate
features, and aligns agent choices with the rules of play. In the experiments,
EDMS with action masking consistently reduced both action-prediction loss and
temporal-difference (TD) error compared to the baseline. Qualitative case
studies and Q-value visualizations further indicate that EDMS highlights
high-risk, high-reward tactical patterns (e.g., fast counterattacks and
defensive breakthroughs). We also integrated our approach into an open-source
library and demonstrated compatibility with multiple commercial and open
datasets, enabling cross-provider evaluation and reproducible experiments.

</details>


### [67] [Rethinking Reward Models for Multi-Domain Test-Time Scaling](https://arxiv.org/abs/2510.00492)
*Dong Bok Lee,Seanie Lee,Sangwoo Park,Minki Kang,Jinheon Baek,Dongki Kim,Dominik Wagner,Jiongdao Jin,Heejun Lee,Tobias Bocklet,Jinyu Wang,Jingjing Fu,Sung Ju Hwang,Jiang Bia,Lei Song*

Main category: cs.AI

TL;DR: 本研究挑战了传统观点，发现在14个不同领域中，生成式结果奖励模型(GenORM)比过程奖励模型(PRM)表现更稳健，而判别式结果奖励模型(DisORM)与判别式过程奖励模型(DisPRM)表现相当。


<details>
  <summary>Details</summary>
Motivation: 传统观点认为过程奖励模型(PRM)优于结果奖励模型(ORM)，但这种观点主要基于数学相关领域的证据。本研究旨在在多样化领域中统一评估不同奖励模型的性能。

Method: 在14个多样化领域中统一评估了四种奖励模型变体：判别式ORM和PRM(DisORM, DisPRM)以及生成式ORM和PRM(GenORM, GenPRM)。

Result: 发现DisORM与DisPRM表现相当，GenPRM不具竞争力，而GenORM在所有测试领域中都表现出显著且一致的提升，是最稳健的模型。

Conclusion: 研究挑战了细粒度监督总是更好的普遍假设，支持在多领域部署中使用生成式结果验证方法。

Abstract: The reliability of large language models (LLMs) during test-time scaling is
often assessed with \emph{external verifiers} or \emph{reward models} that
distinguish correct reasoning from flawed logic. Prior work generally assumes
that process reward models (PRMs), which score every intermediate reasoning
step, outperform outcome reward models (ORMs) that assess only the final
answer. This view is based mainly on evidence from narrow, math-adjacent
domains. We present the first unified evaluation of four reward model variants,
discriminative ORM and PRM (\DisORM, \DisPRM) and generative ORM and PRM
(\GenORM, \GenPRM), across 14 diverse domains. Contrary to conventional wisdom,
we find that (i) \DisORM performs on par with \DisPRM, (ii) \GenPRM is not
competitive, and (iii) overall, \GenORM is the most robust, yielding
significant and consistent gains across every tested domain. We attribute this
to PRM-style stepwise scoring, which inherits label noise from LLM
auto-labeling and has difficulty evaluating long reasoning trajectories,
including those involving self-correcting reasoning. Our theoretical analysis
shows that step-wise aggregation compounds errors as reasoning length grows,
and our empirical observations confirm this effect. These findings challenge
the prevailing assumption that fine-grained supervision is always better and
support generative outcome verification for multi-domain deployment. We
publicly release our code, datasets, and checkpoints at
\href{https://github.com/db-Lee/Multi-RM}{\underline{\small\texttt{https://github.com/db-Lee/Multi-RM}}}
to facilitate future research in multi-domain settings.

</details>


### [68] [VIRTUE: Visual-Interactive Text-Image Universal Embedder](https://arxiv.org/abs/2510.00523)
*Wei-Yao Wang,Kazuya Tateishi,Qiyu Wu,Shusuke Takahashi,Yuki Mitsufuji*

Main category: cs.AI

TL;DR: 提出VIRTUE视觉交互文本图像通用嵌入器，通过整合分割模型和视觉语言模型，使嵌入模型能够处理视觉交互提示（如点、边界框、掩码），在36个通用MMEB任务和5个视觉交互SCaR任务上实现最先进性能。


<details>
  <summary>Details</summary>
Motivation: 现有嵌入模型缺乏视觉交互能力来指定用户感兴趣区域，而生成模型已探索此类交互。为嵌入模型添加视觉交互能力可解锁新的应用场景，并让模型学习图像中的实体级信息来补充全局表示。

Method: 扩展分割模型和视觉语言模型到表示学习领域，分割模型处理视觉提示以精确定位图像特定区域，使嵌入器能更精确处理复杂和模糊场景。

Result: 在36个通用MMEB任务上提升3.1%-8.5%，在5个视觉交互SCaR任务上提升15.2%-20.3%，均达到最先进性能。

Conclusion: VIRTUE成功将视觉交互能力引入嵌入模型，显著提升了模型在通用和视觉交互任务上的表现，为嵌入学习开辟了新方向。

Abstract: Multimodal representation learning models have demonstrated successful
operation across complex tasks, and the integration of vision-language models
(VLMs) has further enabled embedding models with instruction-following
capabilities. However, existing embedding models lack visual-interactive
capabilities to specify regions of interest from users (e.g., point, bounding
box, mask), which have been explored in generative models to broaden their
human-interactive applicability. Equipping embedding models with visual
interactions not only would unlock new applications with localized grounding of
user intent, which remains unexplored, but also enable the models to learn
entity-level information within images to complement their global
representations for conventional embedding tasks. In this paper, we propose a
novel Visual-InteRactive Text-Image Universal Embedder (VIRTUE) that extends
the capabilities of the segmentation model and the vision-language model to the
realm of representation learning. In VIRTUE, the segmentation model can process
visual prompts that pinpoint specific regions within an image, thereby enabling
the embedder to handle complex and ambiguous scenarios more precisely. To
evaluate the visual-interaction ability of VIRTUE, we introduce a large-scale
Segmentation-and-Scene Caption Retrieval (SCaR) benchmark comprising 1M samples
that aims to retrieve the text caption by jointly considering the entity with a
specific object and image scene. VIRTUE consistently achieves a
state-of-the-art performance with significant improvements across 36 universal
MMEB (3.1%-8.5%) and five visual-interactive SCaR (15.2%-20.3%) tasks.

</details>


### [69] [Data Quality Challenges in Retrieval-Augmented Generation](https://arxiv.org/abs/2510.00552)
*Leopold Müller,Joshua Holstein,Sarah Bause,Gerhard Satzger,Niklas Kühl*

Main category: cs.AI

TL;DR: 该研究开发了针对RAG系统的数据质量维度，通过访谈IT服务公司从业者，识别出15个跨四个处理阶段的数据质量维度，发现需要扩展传统DQ框架、采用前置质量管理策略和动态质量管理方法。


<details>
  <summary>Details</summary>
Motivation: 当前数据质量框架主要针对静态数据集，无法充分应对RAG系统动态、多阶段的特性，需要开发专门针对这类AI系统的数据质量维度。

Method: 对16家领先IT服务公司的从业者进行半结构化访谈，通过定性内容分析归纳出RAG系统四个处理阶段的数据质量维度。

Result: 识别出15个不同的数据质量维度，分布在数据提取、数据转换、提示与搜索、生成四个阶段，发现新维度主要集中在早期阶段，且质量问题会在管道中转化和传播。

Conclusion: 需要扩展传统DQ框架以涵盖RAG环境，采用前置质量管理策略，并实施动态、阶段感知的质量管理方法。

Abstract: Organizations increasingly adopt Retrieval-Augmented Generation (RAG) to
enhance Large Language Models with enterprise-specific knowledge. However,
current data quality (DQ) frameworks have been primarily developed for static
datasets, and only inadequately address the dynamic, multi-stage nature of RAG
systems. This study aims to develop DQ dimensions for this new type of AI-based
systems. We conduct 16 semi-structured interviews with practitioners of leading
IT service companies. Through a qualitative content analysis, we inductively
derive 15 distinct DQ dimensions across the four processing stages of RAG
systems: data extraction, data transformation, prompt & search, and generation.
Our findings reveal that (1) new dimensions have to be added to traditional DQ
frameworks to also cover RAG contexts; (2) these new dimensions are
concentrated in early RAG steps, suggesting the need for front-loaded quality
management strategies, and (3) DQ issues transform and propagate through the
RAG pipeline, necessitating a dynamic, step-aware approach to quality
management.

</details>


### [70] [Toward Safer Diffusion Language Models: Discovery and Mitigation of Priming Vulnerability](https://arxiv.org/abs/2510.00565)
*Shojiro Yamabe,Jun Sakuma*

Main category: cs.AI

TL;DR: 扩散语言模型(DLMs)存在由迭代去噪过程引发的安全漏洞，攻击者可通过注入肯定性令牌绕过安全防护。本文提出了一种针对DLMs的安全对齐方法，能有效缓解此漏洞。


<details>
  <summary>Details</summary>
Motivation: DLMs通过并行迭代去噪生成令牌，这种推理机制可能带来新的安全风险，特别是越狱攻击。目前对这种基于推理机制的安全风险理解不足。

Method: 提出了一种针对DLMs的安全对齐方法，训练模型从包含肯定性令牌的污染中间状态生成安全响应。

Result: 实验表明该方法能显著缓解漏洞，对任务性能影响最小，同时提高了对传统越狱攻击的鲁棒性。

Conclusion: DLMs具有特定的安全漏洞，需要专门的安全研究。提出的安全对齐方法能有效保护DLMs免受此类攻击。

Abstract: Diffusion language models (DLMs) generate tokens in parallel through
iterative denoising, which can reduce latency and enable bidirectional
conditioning. However, the safety risks posed by jailbreak attacks that exploit
this inference mechanism are not well understood. In this paper, we reveal that
DLMs have a critical vulnerability stemming from their iterative denoising
process and propose a countermeasure. Specifically, our investigation shows
that if an affirmative token for a harmful query appears at an intermediate
step, subsequent denoising can be steered toward a harmful response even in
aligned models. As a result, simply injecting such affirmative tokens can
readily bypass the safety guardrails. Furthermore, we demonstrate that the
vulnerability allows existing optimization-based jailbreak attacks to succeed
on DLMs. Building on this analysis, we propose a novel safety alignment method
tailored to DLMs that trains models to generate safe responses from
contaminated intermediate states that contain affirmative tokens. Our
experiments indicate that the proposed method significantly mitigates the
vulnerability with minimal impact on task performance. Furthermore, our method
improves robustness against conventional jailbreak attacks. Our work
underscores the need for DLM-specific safety research.

</details>


### [71] [ACON: Optimizing Context Compression for Long-horizon LLM Agents](https://arxiv.org/abs/2510.00615)
*Minki Kang,Wei-Ning Chen,Dongge Han,Huseyin A. Inan,Lukas Wutschitz,Yanzhi Chen,Robert Sim,Saravan Rajmohan*

Main category: cs.AI

TL;DR: ACON是一个统一的框架，用于优化压缩环境观察和交互历史，通过自然语言空间中的压缩指南优化来减少上下文长度，同时保持任务性能。


<details>
  <summary>Details</summary>
Motivation: 随着LLM作为智能体在动态环境中部署，上下文长度增长导致成本增加和效率降低，而现有的上下文压缩方法主要关注单步任务或狭窄应用。

Method: ACON利用压缩指南优化：当完整上下文成功但压缩上下文失败时，LLM分析失败原因并相应更新压缩指南，同时将优化的LLM压缩器蒸馏到更小的模型中。

Result: 在AppWorld、OfficeBench和Multi-objective QA上的实验显示，ACON减少内存使用26-54%（峰值令牌），同时基本保持任务性能，蒸馏到更小压缩器时保持95%以上准确率，并将较小LM作为长视野智能体的性能提升高达46%。

Conclusion: ACON有效解决了智能体任务中上下文长度增长的问题，通过优化压缩显著减少内存使用并保持性能，同时通过蒸馏技术实现高效部署。

Abstract: Large language models (LLMs) are increasingly deployed as agents in dynamic,
real-world environments, where success requires both reasoning and effective
tool use. A central challenge for agentic tasks is the growing context length,
as agents must accumulate long histories of actions and observations. This
expansion raises costs and reduces efficiency in long-horizon tasks, yet prior
work on context compression has mostly focused on single-step tasks or narrow
applications. We introduce Agent Context Optimization (ACON), a unified
framework that optimally compresses both environment observations and
interaction histories into concise yet informative condensations. ACON
leverages compression guideline optimization in natural language space: given
paired trajectories where full context succeeds but compressed context fails,
capable LLMs analyze the causes of failure, and the compression guideline is
updated accordingly. Furthermore, we propose distilling the optimized LLM
compressor into smaller models to reduce the overhead of the additional module.
Experiments on AppWorld, OfficeBench, and Multi-objective QA show that ACON
reduces memory usage by 26-54% (peak tokens) while largely preserving task
performance, preserves over 95% of accuracy when distilled into smaller
compressors, and enhances smaller LMs as long-horizon agents with up to 46%
performance improvement.

</details>


### [72] [HARPA: A Testability-Driven, Literature-Grounded Framework for Research Ideation](https://arxiv.org/abs/2510.00620)
*Rosni Vasu,Peter Jansen,Pao Siangliulue,Cristina Sarasua,Abraham Bernstein,Peter Clark,Bhavana Dalvi Mishra*

Main category: cs.AI

TL;DR: HARPA是一个自动化科学发现工具，通过文献挖掘识别研究趋势、探索假设设计空间，并基于实验结果学习奖励模型，生成可测试且基于文献的假设。


<details>
  <summary>Details</summary>
Motivation: 解决现有自动化科学发现工具难以生成既可测试又基于科学文献的假设，且无法根据先前实验结果进行调整的问题。

Method: 采用受人类研究人员启发的构思流程：文献挖掘识别研究趋势、探索假设设计空间、通过定位研究空白和论证设计选择来收敛到精确可测试的假设，并学习基于先前实验结果的奖励模型。

Result: HARPA生成的研究提案在可行性(+0.78)和基础性(+0.85)方面显著优于基线，与ASD代理(CodeScientist)测试时获得更多成功执行(20 vs 11)和更少失败(16 vs 21)，奖励模型比未训练基线提高约28%。

Conclusion: HARPA代表了AI驱动科学发现领域的重要进展，能够生成更可行、更基于文献的假设，并通过学习实验反馈持续改进假设质量。

Abstract: While there has been a surge of interest in automated scientific discovery
(ASD), especially with the emergence of LLMs, it remains challenging for tools
to generate hypotheses that are both testable and grounded in the scientific
literature. Additionally, existing ideation tools are not adaptive to prior
experimental outcomes. We developed HARPA to address these challenges by
incorporating the ideation workflow inspired by human researchers. HARPA first
identifies emerging research trends through literature mining, then explores
hypothesis design spaces, and finally converges on precise, testable hypotheses
by pinpointing research gaps and justifying design choices. Our evaluations
show that HARPA-generated hypothesis-driven research proposals perform
comparably to a strong baseline AI-researcher across most qualitative
dimensions (e.g., specificity, novelty, overall quality), but achieve
significant gains in feasibility(+0.78, p$<0.05$, bootstrap) and groundedness
(+0.85, p$<0.01$, bootstrap) on a 10-point Likert scale. When tested with the
ASD agent (CodeScientist), HARPA produced more successful executions (20 vs. 11
out of 40) and fewer failures (16 vs. 21 out of 40), showing that expert
feasibility judgments track with actual execution success. Furthermore, to
simulate how researchers continuously refine their understanding of what
hypotheses are both testable and potentially interesting from experience, HARPA
learns a reward model that scores new hypotheses based on prior experimental
outcomes, achieving approx. a 28\% absolute gain over HARPA's untrained
baseline scorer. Together, these methods represent a step forward in the field
of AI-driven scientific discovery.

</details>


### [73] [Is Model Editing Built on Sand? Revealing Its Illusory Success and Fragile Foundation](https://arxiv.org/abs/2510.00625)
*Wei Liu,Haomei Xu,Bingqing Liu,Zhiying Deng,Haozhao Wang,Jun Wang,Ruixuan Li,Yee Whye Teh,Wee Sun Lee*

Main category: cs.AI

TL;DR: 该论文指出当前模型编辑方法存在严重缺陷，表面上的成功实际上是基于利用隐藏捷径而非真实语义，导致在否定查询等简单测试下就会失效。


<details>
  <summary>Details</summary>
Motivation: 发现当前模型编辑文献存在根本性问题，编辑的可靠性建立在脆弱基础上，需要揭示这种虚幻成功并重新评估模型编辑的基础。

Method: 系统开发了一套新的评估方法，特别设计了否定查询等负例测试，用于检测模型编辑是否真正基于语义理解。

Result: 研究发现最先进的模型编辑方法在简单否定查询下就会崩溃，表明编辑很可能基于捷径而非完整语义。

Conclusion: 模型编辑当前的基础存在严重问题，需要在进一步推进前重新考虑其根本基础，因为捷径与稳健的知识整合本质上是矛盾的。

Abstract: Large language models (LLMs) inevitably encode outdated or incorrect
knowledge. Updating, deleting, and forgetting such knowledge is important for
alignment, safety, and other issues. To address this issue, model editing has
emerged as a promising paradigm: by precisely editing a small subset of
parameters such that a specific fact is updated while preserving other
knowledge. Despite its great success reported in previous papers, we find the
apparent reliability of editing rests on a fragile foundation and the current
literature is largely driven by illusory success. The fundamental goal of
steering the model's output toward a target with minimal modification would
encourage exploiting hidden shortcuts, rather than utilizing real semantics.
This problem directly challenges the feasibility of the current model editing
literature at its very foundation, as shortcuts are inherently at odds with
robust knowledge integration. Coincidentally, this issue has long been obscured
by evaluation frameworks that lack the design of negative examples. To uncover
it, we systematically develop a suite of new evaluation methods. Strikingly, we
find that state-of-the-art approaches collapse even under the simplest negation
queries. Our empirical evidence shows that editing is likely to be based on
shortcuts rather than full semantics, calling for an urgent reconsideration of
the very basis of model editing before further advancements can be meaningfully
pursued.

</details>


### [74] [Collaborative-Distilled Diffusion Models (CDDM) for Accelerated and Lightweight Trajectory Prediction](https://arxiv.org/abs/2510.00627)
*Bingzhang Wang,Kehua Chen,Yinhai Wang*

Main category: cs.AI

TL;DR: 提出CDDM方法，通过协作渐进蒸馏将大容量教师扩散模型的知识转移到轻量级学生模型，实现实时轻量化的轨迹预测，在保持高精度的同时大幅减少模型参数和采样步骤。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在概率轨迹预测中表现出色，但模型规模大、采样速度慢，阻碍了在实际自动驾驶和智能交通系统中的部署。

Method: 基于协作渐进蒸馏(CPD)，逐步减少采样步骤和模型大小，引入双信号正则化蒸馏损失，结合教师模型和真实数据的指导。

Result: 在ETH-UCY行人基准和nuScenes车辆基准上达到最先进预测精度，仅需231K参数和2-4采样步骤，实现161倍压缩、31倍加速和9ms延迟。

Conclusion: CDDM通过桥接高性能生成模型与实用部署约束，为自动驾驶和智能交通系统实现了资源高效的概然预测。

Abstract: Trajectory prediction is a fundamental task in Autonomous Vehicles (AVs) and
Intelligent Transportation Systems (ITS), supporting efficient motion planning
and real-time traffic safety management. Diffusion models have recently
demonstrated strong performance in probabilistic trajectory prediction, but
their large model size and slow sampling process hinder real-world deployment.
This paper proposes Collaborative-Distilled Diffusion Models (CDDM), a novel
method for real-time and lightweight trajectory prediction. Built upon
Collaborative Progressive Distillation (CPD), CDDM progressively transfers
knowledge from a high-capacity teacher diffusion model to a lightweight student
model, jointly reducing both the number of sampling steps and the model size
across distillation iterations. A dual-signal regularized distillation loss is
further introduced to incorporate guidance from both the teacher and
ground-truth data, mitigating potential overfitting and ensuring robust
performance. Extensive experiments on the ETH-UCY pedestrian benchmark and the
nuScenes vehicle benchmark demonstrate that CDDM achieves state-of-the-art
prediction accuracy. The well-distilled CDDM retains 96.2% and 95.5% of the
baseline model's ADE and FDE performance on pedestrian trajectories, while
requiring only 231K parameters and 4 or 2 sampling steps, corresponding to 161x
compression, 31x acceleration, and 9 ms latency. Qualitative results further
show that CDDM generates diverse and accurate trajectories under dynamic agent
behaviors and complex social interactions. By bridging high-performing
generative models with practical deployment constraints, CDDM enables
resource-efficient probabilistic prediction for AVs and ITS. Code is available
at https://github.com/bingzhangw/CDDM.

</details>


### [75] [Expected Attention: KV Cache Compression by Estimating Attention from Future Queries Distribution](https://arxiv.org/abs/2510.00636)
*Alessio Devoto,Maximilian Jeblick,Simon Jégou*

Main category: cs.AI

TL;DR: 提出了一种名为Expected Attention的无训练KV缓存压缩方法，通过预测未来查询如何关注KV对来估计其重要性，解决了注意力分数在推理时不可用的问题。


<details>
  <summary>Details</summary>
Motivation: KV缓存的存储消耗是大型语言模型推理效率的主要瓶颈。现有的基于注意力分数的KV缓存剪枝方法面临实际限制：未来token的注意力分数在压缩时不可用，且现代实现如Flash Attention不会生成完整的注意力矩阵。

Method: 利用LLM激活的分布特性，以闭式形式计算每个KV对的期望注意力分数，基于这些分数进行排序和剪枝，对残差流影响最小。

Result: 该方法在预填充和解码阶段都能无缝运行，在两个场景中都持续优于最先进的基线方法。

Conclusion: 开发了KVPress库，包含20多种技术，为研究人员实现和基准测试KV缓存压缩方法提供了工具。

Abstract: Memory consumption of the Key-Value (KV) cache represents a major bottleneck
for efficient large language model inference. While attention-score-based KV
cache pruning shows promise, it faces critical practical limitations: attention
scores from future tokens are unavailable during compression, and modern
implementations like Flash Attention do not materialize the full attention
matrix, making past scores inaccessible. To overcome these challenges, we
introduce $\textbf{Expected Attention, a training-free compression method}$
that estimates KV pairs importance by predicting how future queries will attend
to them. Our approach leverages the distributional properties of LLM
activations to compute expected attention scores in closed form for each KV
pair. These scores enable principled ranking and pruning of KV pairs with
minimal impact on the residual stream, achieving effective compression without
performance degradation. Importantly, our method operates seamlessly across
both prefilling and decoding phases, consistently outperforming
state-of-the-art baselines in both scenarios. Finally, $\textbf{we release
KVPress, a comprehensive library to enable researchers to implement and
benchmark KV cache compression methods, already including more than 20
techniques}$.

</details>


### [76] [Batch-CAM: Introduction to better reasoning in convolutional deep learning models](https://arxiv.org/abs/2510.00664)
*Giacomo Ignesti,Davide Moroni,Massimo Martinelli*

Main category: cs.AI

TL;DR: 提出Batch-CAM训练范式，结合批处理Grad-CAM算法和原型重建损失，提升模型在分类任务中的性能，同时提高准确率和图像重建质量，减少训练和推理时间。


<details>
  <summary>Details</summary>
Motivation: 在医疗等高风险领域，深度学习的可解释性至关重要，准确的解释与精度同等重要。需要构建更透明、可解释和可信赖的AI系统。

Method: 融合批处理Grad-CAM算法与原型重建损失，引导模型关注显著图像特征。

Result: Batch-CAM在准确率和图像重建质量上同时提升，同时减少了训练和推理时间。

Conclusion: 该方法通过确保模型从证据相关信息中学习，为构建更透明、可解释和可信赖的AI系统做出了重要贡献。

Abstract: Understanding the inner workings of deep learning models is crucial for
advancing artificial intelligence, particularly in high-stakes fields such as
healthcare, where accurate explanations are as vital as precision. This paper
introduces Batch-CAM, a novel training paradigm that fuses a batch
implementation of the Grad-CAM algorithm with a prototypical reconstruction
loss. This combination guides the model to focus on salient image features,
thereby enhancing its performance across classification tasks. Our results
demonstrate that Batch-CAM achieves a simultaneous improvement in accuracy and
image reconstruction quality while reducing training and inference times. By
ensuring models learn from evidence-relevant information,this approach makes a
relevant contribution to building more transparent, explainable, and
trustworthy AI systems.

</details>


### [77] [Relevance-Zone Reduction in Game Solving](https://arxiv.org/abs/2510.00689)
*Chi-Huang Lin,Ting Han Wei,Chun-Jui Wang,Hung Guei,Chung-Chin Shih,Yun-Jui Tsai,I-Chen Wu,Ti-Rong Wu*

Main category: cs.AI

TL;DR: 提出了一种迭代式相关区域(RZ)缩减方法，通过重复求解同一位置并逐步限制搜索区域来获得更小的RZ，从而提高策略重用和剪枝效率。


<details>
  <summary>Details</summary>
Motivation: 游戏求解面临指数级增长的博弈树问题，虽然相关区域(RZ)技术能显著减少搜索空间，但不同解决方案会产生不同大小的RZ，较小的RZ更有利于重用和剪枝效率。

Method: 设计了三种约束生成策略，并集成了RZ模式表来充分利用过往解决方案，通过迭代求解同一位置并逐步限制区域来引导求解器获得更小的RZ。

Result: 在7x7 Killall-Go实验中，平均RZ大小减少到原始的85.95%，缩减后的RZ可作为可重用知识存储。

Conclusion: 该方法能有效减小RZ大小，提高求解效率，缩减后的RZ可作为永久知识用于未来更大棋盘或不同开局布局的求解任务。

Abstract: Game solving aims to find the optimal strategies for all players and
determine the theoretical outcome of a game. However, due to the exponential
growth of game trees, many games remain unsolved, even though methods like
AlphaZero have demonstrated super-human level in game playing. The
Relevance-Zone (RZ) is a local strategy reuse technique that restricts the
search to only the regions relevant to the outcome, significantly reducing the
search space. However, RZs are not unique. Different solutions may result in
RZs of varying sizes. Smaller RZs are generally more favorable, as they
increase the chance of reuse and improve pruning efficiency. To this end, we
propose an iterative RZ reduction method that repeatedly solves the same
position while gradually restricting the region involved, guiding the solver
toward smaller RZs. We design three constraint generation strategies and
integrate an RZ Pattern Table to fully leverage past solutions. In experiments
on 7x7 Killall-Go, our method reduces the average RZ size to 85.95% of the
original. Furthermore, the reduced RZs can be permanently stored as reusable
knowledge for future solving tasks, especially for larger board sizes or
different openings.

</details>


### [78] [ACPO: Adaptive Curriculum Policy Optimization for Aligning Vision-Language Models in Complex Reasoning](https://arxiv.org/abs/2510.00690)
*Yunhao Wang,Ziting Li,Shuai Chen,Tao Liu,Chao Song,Junjie Jiang,Jian Zhu,Peng Gao,Bin Qin*

Main category: cs.AI

TL;DR: ACPO是一个自适应课程策略优化框架，通过动态课程和自适应裁剪机制改进视觉语言模型的对齐训练，在复杂推理任务上实现SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有策略优化算法（如PPO）在视觉语言模型对齐训练中存在静态训练计划和刚性裁剪机制的限制，阻碍了复杂推理任务的性能提升。

Method: 提出ACPO框架：1）动态课程策略，从近策略探索逐步过渡到离策略利用；2）优势感知自适应裁剪机制，根据每个token的归一化优势动态调整裁剪边界。

Result: 在MathVista、LogicVista和MMMU-Pro等多模态推理基准测试中，ACPO显著优于DAPO和PAPO等基线方法，实现了最先进性能、加速收敛和更好的训练稳定性。

Conclusion: ACPO通过自适应学习策略有效解决了现有策略优化算法的局限性，为复杂多模态推理任务的对齐训练提供了更高效和稳定的解决方案。

Abstract: Aligning large-scale vision-language models (VLMs) for complex reasoning via
reinforcement learning is often hampered by the limitations of existing policy
optimization algorithms, such as static training schedules and the rigid,
uniform clipping mechanism in Proximal Policy Optimization (PPO). In this work,
we introduce Adaptive Curriculum Policy Optimization (ACPO), a novel framework
that addresses these challenges through a dual-component adaptive learning
strategy. First, ACPO employs a dynamic curriculum that orchestrates a
principled transition from a stable, near on-policy exploration phase to an
efficient, off-policy exploitation phase by progressively increasing sample
reuse. Second, we propose an Advantage-Aware Adaptive Clipping (AAAC) mechanism
that replaces the fixed clipping hyperparameter with dynamic, sample-wise
bounds modulated by the normalized advantage of each token. This allows for
more granular and robust policy updates, enabling larger gradients for
high-potential samples while safeguarding against destructive ones. We conduct
extensive experiments on a suite of challenging multimodal reasoning
benchmarks, including MathVista, LogicVista, and MMMU-Pro. Results demonstrate
that ACPO consistently outperforms strong baselines such as DAPO and PAPO,
achieving state-of-the-art performance, accelerated convergence, and superior
training stability.

</details>


### [79] [AttentionDep: Domain-Aware Attention for Explainable Depression Severity Assessment](https://arxiv.org/abs/2510.00706)
*Yusif Ibrahimov,Tarique Anwar,Tommy Yuan,Turan Mutallimov,Elgun Hasanov*

Main category: cs.AI

TL;DR: AttentionDep模型通过融合上下文和领域知识进行可解释的抑郁症严重程度检测，在社交媒体数据上表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 利用社交媒体平台作为了解个体心理状态的窗口，开发可信赖且透明的AI系统用于心理健康评估。

Method: 提出AttentionDep模型，使用单字和双字进行分层编码，通过注意力机制突出临床相关标记，并整合心理健康知识图谱的领域知识，采用序数回归框架预测抑郁症严重程度。

Result: 实验表明AttentionDep在分级F1分数上比最先进基线方法高出5%以上，同时提供可解释的预测洞察。

Conclusion: 这项工作推动了基于社交媒体的心理健康评估中可信赖和透明AI系统的发展。

Abstract: In today's interconnected society, social media platforms provide a window
into individuals' thoughts, emotions, and mental states. This paper explores
the use of platforms like Facebook, X (formerly Twitter), and Reddit for
depression severity detection. We propose AttentionDep, a domain-aware
attention model that drives explainable depression severity estimation by
fusing contextual and domain knowledge. Posts are encoded hierarchically using
unigrams and bigrams, with attention mechanisms highlighting clinically
relevant tokens. Domain knowledge from a curated mental health knowledge graph
is incorporated through a cross-attention mechanism, enriching the contextual
features. Finally, depression severity is predicted using an ordinal regression
framework that respects the clinical-relevance and natural ordering of severity
levels. Our experiments demonstrate that AttentionDep outperforms
state-of-the-art baselines by over 5% in graded F1 score across datasets, while
providing interpretable insights into its predictions. This work advances the
development of trustworthy and transparent AI systems for mental health
assessment from social media.

</details>


### [80] [EvolProver: Advancing Automated Theorem Proving by Evolving Formalized Problems via Symmetry and Difficulty](https://arxiv.org/abs/2510.00732)
*Yuchen Tian,Ruiyuan Huang,Xuanwu Wang,Jing Ma,Zengfeng Huang,Ziyang Luo,Hongzhan Lin,Da Zheng,Lun Du*

Main category: cs.AI

TL;DR: 提出了一种新颖的数据增强流水线，通过对称性和难度两个角度提升定理证明模型的鲁棒性，训练出的EvolProver在多个基准测试中创下新记录。


<details>
  <summary>Details</summary>
Motivation: 现有大型语言模型在形式定理证明中缺乏泛化能力，对问题表述的微小变换也很脆弱。

Method: 从对称性角度提出EvolAST（基于抽象语法树）和EvolDomain（跨数学领域翻译）；从难度角度提出EvolDifficulty；使用增强数据训练7B参数的非推理定理证明器EvolProver。

Result: EvolProver在FormalMATH-Lite上达到53.8% pass@32，在MiniF2F-Test、Ineq-Comp-Seed和Ineq-Comp-Transformed上均创下非推理模型的新记录。

Conclusion: 数据增强流水线有效提升了定理证明模型的鲁棒性，EvolProver在多个基准测试中表现优异。

Abstract: Large Language Models (LLMs) for formal theorem proving have shown
significant promise, yet they often lack generalizability and are fragile to
even minor transformations of problem statements. To address this limitation,
we introduce a novel data augmentation pipeline designed to enhance model
robustness from two perspectives: symmetry and difficulty. From the symmetry
perspective, we propose two complementary methods: EvolAST, an Abstract Syntax
Tree (AST) based approach that targets syntactic symmetry to generate
semantically equivalent problem variants, and EvolDomain, which leverages LLMs
to address semantic symmetry by translating theorems across mathematical
domains. From the difficulty perspective, we propose EvolDifficulty, which uses
carefully designed evolutionary instructions to guide LLMs in generating new
theorems with a wider range of difficulty. We then use the evolved data to
train EvolProver, a 7B-parameter non-reasoning theorem prover. EvolProver
establishes a new state-of-the-art (SOTA) on FormalMATH-Lite with a 53.8%
pass@32 rate, surpassing all models of comparable size, including
reasoning-based models. It also sets new SOTA records for non-reasoning models
on MiniF2F-Test (69.8% pass@32), Ineq-Comp-Seed (52.2% pass@32), and
Ineq-Comp-Transformed (34.0% pass@32). Ablation studies further confirm our
data augmentation pipeline's effectiveness across multiple benchmarks.

</details>


### [81] [DIA: The Adversarial Exposure of Deterministic Inversion in Diffusion Models](https://arxiv.org/abs/2510.00778)
*Seunghoo Hong,Geonho Son,Juhun Lee,Simon S. Woo*

Main category: cs.AI

TL;DR: 提出DDIM反演攻击(DIA)方法，通过攻击DDIM轨迹路径来有效防御恶意图像编辑，超越现有防御方法。


<details>
  <summary>Details</summary>
Motivation: DDIM反演技术使恶意用户能轻松合成虚假或侵权内容，现有防御方法因目标与去噪轨迹不匹配而效果有限。

Method: 开发DDIM反演攻击(DIA)，直接攻击DDIM轨迹路径，破坏扩散模型的图像编辑能力。

Result: DIA在各种编辑方法中均能有效破坏图像编辑过程，性能超越AdvDM和Photoguard等现有防御方法。

Conclusion: DIA框架为行业和研究社区提供了实用的AI恶意使用防御方法。

Abstract: Diffusion models have shown to be strong representation learners, showcasing
state-of-the-art performance across multiple domains. Aside from accelerated
sampling, DDIM also enables the inversion of real images back to their latent
codes. A direct inheriting application of this inversion operation is real
image editing, where the inversion yields latent trajectories to be utilized
during the synthesis of the edited image. Unfortunately, this practical tool
has enabled malicious users to freely synthesize misinformative or deepfake
contents with greater ease, which promotes the spread of unethical and abusive,
as well as privacy-, and copyright-infringing contents. While defensive
algorithms such as AdvDM and Photoguard have been shown to disrupt the
diffusion process on these images, the misalignment between their objectives
and the iterative denoising trajectory at test time results in weak disruptive
performance.In this work, we present the DDIM Inversion Attack (DIA) that
attacks the integrated DDIM trajectory path. Our results support the effective
disruption, surpassing previous defensive methods across various editing
methods. We believe that our frameworks and results can provide practical
defense methods against the malicious use of AI for both the industry and the
research community. Our code is available here:
https://anonymous.4open.science/r/DIA-13419/.

</details>


### [82] [AI in data science education: experiences from the classroom](https://arxiv.org/abs/2510.00793)
*J. A. Hageman,C. F. W. Peeters*

Main category: cs.AI

TL;DR: 本研究探讨了AI（特别是像ChatGPT这样的大型语言模型）在教育环境中的整合，重点关注对教学的影响。通过访谈瓦赫宁根大学数据科学课程的协调员，研究发现AI工具既能简化任务、增强学习，也存在学生过度依赖、阻碍认知和问题解决能力发展的风险。


<details>
  <summary>Details</summary>
Motivation: 随着AI技术在教育领域的应用日益广泛，需要了解AI工具对教学和学习的实际影响，特别是如何平衡AI带来的便利与学生能力发展之间的关系。

Method: 通过对瓦赫宁根大学数据科学课程协调员进行访谈，收集关于AI在教育中应用的定性数据。

Result: 研究发现AI工具能够简化任务并增强学习体验，但同时也存在学生过度依赖AI的风险，这可能阻碍他们发展关键的认知和问题解决能力。

Conclusion: AI可以成为教育中的宝贵资产，但需要负责任地使用，确保AI补充而非替代基本学习过程，同时需要调整评估方法以确保教育目标的实现。

Abstract: This study explores the integration of AI, particularly large language models
(LLMs) like ChatGPT, into educational settings, focusing on the implications
for teaching and learning. Through interviews with course coordinators from
data science courses at Wageningen University, this research identifies both
the benefits and challenges associated with AI in the classroom. While AI tools
can streamline tasks and enhance learning, concerns arise regarding students'
overreliance on these technologies, potentially hindering the development of
essential cognitive and problem solving skills. The study highlights the
importance of responsible AI usage, ethical considerations, and the need for
adapting assessment methods to ensure educational outcomes are met. With
careful integration, AI can be a valuable asset in education, provided it is
used to complement rather than replace fundamental learning processes.

</details>


### [83] [Benchmarking Agentic Systems in Automated Scientific Information Extraction with ChemX](https://arxiv.org/abs/2510.00795)
*Anastasia Vepreva,Julia Razlivina,Maria Eremeeva,Nina Gubina,Anastasia Orlova,Aleksei Dmitrenko,Ksenya Kapranova,Susan Jyakhwo,Nikita Vasilev,Arsen Sarkisyan,Ivan Yu. Chernyshov,Vladimir Vinogradov,Andrei Dmitrenko*

Main category: cs.AI

TL;DR: 提出了ChemX数据集来评估化学信息提取方法，通过基准测试发现现有方法在处理化学术语、复杂表格和上下文歧义方面仍面临挑战


<details>
  <summary>Details</summary>
Motivation: 化学信息提取因数据异质性而具有挑战性，现有基于代理的方法在该领域表现有限，需要专门的评估资源

Method: 创建了10个手动策划且经过领域专家验证的数据集，进行广泛的基准测试，比较现有最先进的代理系统，并引入了单代理方法控制文档预处理

Result: 实证研究揭示了化学信息提取中的持续挑战，特别是在处理领域特定术语、复杂表格和示意图表示以及上下文相关歧义方面

Conclusion: ChemX基准是推进化学自动化信息提取的关键资源，挑战现有方法的泛化能力，并为有效评估策略提供宝贵见解

Abstract: The emergence of agent-based systems represents a significant advancement in
artificial intelligence, with growing applications in automated data
extraction. However, chemical information extraction remains a formidable
challenge due to the inherent heterogeneity of chemical data. Current
agent-based approaches, both general-purpose and domain-specific, exhibit
limited performance in this domain. To address this gap, we present ChemX, a
comprehensive collection of 10 manually curated and domain-expert-validated
datasets focusing on nanomaterials and small molecules. These datasets are
designed to rigorously evaluate and enhance automated extraction methodologies
in chemistry. To demonstrate their utility, we conduct an extensive
benchmarking study comparing existing state-of-the-art agentic systems such as
ChatGPT Agent and chemical-specific data extraction agents. Additionally, we
introduce our own single-agent approach that enables precise control over
document preprocessing prior to extraction. We further evaluate the performance
of modern baselines, such as GPT-5 and GPT-5 Thinking, to compare their
capabilities with agentic approaches. Our empirical findings reveal persistent
challenges in chemical information extraction, particularly in processing
domain-specific terminology, complex tabular and schematic representations, and
context-dependent ambiguities. The ChemX benchmark serves as a critical
resource for advancing automated information extraction in chemistry,
challenging the generalization capabilities of existing methods, and providing
valuable insights into effective evaluation strategies.

</details>


### [84] [Semantic Bridges Between First Order c-Representations and Cost-Based Semantics: An Initial Perspective](https://arxiv.org/abs/2510.00817)
*Nicholas Leisegang,Giovanni Casini,Thomas Meyer*

Main category: cs.AI

TL;DR: 比较加权知识库与c-representations两种处理不一致知识库的方法，分析它们在语义层面的等价性


<details>
  <summary>Details</summary>
Motivation: 研究加权知识库和c-representations这两种处理不一致知识库的方法之间的语义关系，探索它们能否产生相同的解释排序

Method: 通过语义层面比较，分析两种方法在解释排序和蕴含关系上的等价条件

Result: 在特定条件下，加权知识库和一组可废止条件可以生成相同的解释排序，实现语义结构的相对成本等价

Conclusion: 两种形式主义在特定条件下具有语义等价性，这有助于进一步研究成本语义和c-representations

Abstract: Weighted-knowledge bases and cost-based semantics represent a recent
formalism introduced by Bienvenu et al. for Ontology Mediated Data Querying in
the case where a given knowledge base is inconsistent. This is done by adding a
weight to each statement in the knowledge base (KB), and then giving each DL
interpretation a cost based on how often it breaks rules in the KB. In this
paper we compare this approach with c-representations, a form of non-monotonic
reasoning originally introduced by Kern-Isberner. c-Representations describe a
means to interpret defeasible concept inclusions in the first-order case. This
is done by assigning a numerical ranking to each interpretations via penalties
for each violated conditional. We compare these two approaches on a semantic
level. In particular, we show that under certain conditions a weighted
knowledge base and a set of defeasible conditionals can generate the same
ordering on interpretations, and therefore an equivalence of semantic
structures up to relative cost. Moreover, we compare entailment described in
both cases, where certain notions are equivalently expressible in both
formalisms. Our results have the potential to benefit further work on both
cost-based semantics and c-representations

</details>


### [85] [Logical Consistency Between Disagreeing Experts and Its Role in AI Safety](https://arxiv.org/abs/2510.00821)
*Andrés Corrada-Emmanuel*

Main category: cs.AI

TL;DR: 本文提出了一种无监督评估分类器的逻辑框架，通过分析分类器之间的一致性和分歧来推断其性能，无需真实标签。


<details>
  <summary>Details</summary>
Motivation: 探索分类器评估中一致性与分歧的不对称性，开发仅基于逻辑一致性的无监督评估方法，无需真实标签即可检测模型性能问题。

Method: 将分类器对齐决策的统计摘要作为线性规划问题的输入，在整数空间中建模正确或错误响应的可能组合，应用逻辑约束和普适性公理。

Result: 构建了无需知识的警报系统，能够检测LLM作为评判者是否违反用户指定的最低评分阈值，展示了该方法的实际效用。

Conclusion: 基于逻辑一致性的无监督评估方法具有实际应用价值，能够有效检测分类器性能问题，无需依赖真实标签信息。

Abstract: If two experts disagree on a test, we may conclude both cannot be 100 per
cent correct. But if they completely agree, no possible evaluation can be
excluded. This asymmetry in the utility of agreements versus disagreements is
explored here by formalizing a logic of unsupervised evaluation for
classifiers. Its core problem is computing the set of group evaluations that
are logically consistent with how we observe them agreeing and disagreeing in
their decisions. Statistical summaries of their aligned decisions are inputs
into a Linear Programming problem in the integer space of possible correct or
incorrect responses given true labels. Obvious logical constraints, such as,
the number of correct responses cannot exceed the number of observed responses,
are inequalities. But in addition, there are axioms, universally applicable
linear equalities that apply to all finite tests. The practical and immediate
utility of this approach to unsupervised evaluation using only logical
consistency is demonstrated by building no-knowledge alarms that can detect
when one or more LLMs-as-Judges are violating a minimum grading threshold
specified by the user.

</details>


### [86] [Benchmarking Machine Learning Models for Fault Classification and Localization in Power System Protection](https://arxiv.org/abs/2510.00831)
*Julian Oelhaf,Georg Kordowich,Changhun Kim,Paula Andrea Pérez-Toro,Christian Bergler,Andreas Maier,Johann Jäger,Siming Bayer*

Main category: cs.AI

TL;DR: 本文对电力系统保护中的故障分类和故障定位进行了机器学习模型比较基准研究，基于EMT数据评估了经典ML模型在不同窗口大小下的性能表现。


<details>
  <summary>Details</summary>
Motivation: 随着分布式能源资源特别是可再生能源的集成增加，电力系统保护面临重大挑战。传统的基于固定阈值的保护方案在动态条件下无法可靠识别和定位短路故障，而机器学习的系统化基准研究仍然有限。

Method: 使用电压和电流波形数据，将其分割为10-50毫秒的滑动窗口，在实时约束下评估经典机器学习模型。评估指标包括准确性、对窗口大小的鲁棒性和运行时效率。

Result: 最佳故障分类模型F1得分为0.992±0.001，最佳故障定位模型R2得分为0.806±0.008，平均处理时间为0.563毫秒。

Conclusion: 机器学习模型在电力系统故障分类和定位方面表现出色，能够满足实时保护要求，为复杂电网条件下的保护方案提供了有前景的替代方案。

Abstract: The increasing integration of distributed energy resources (DERs),
particularly renewables, poses significant challenges for power system
protection, with fault classification (FC) and fault localization (FL) being
among the most critical tasks. Conventional protection schemes, based on fixed
thresholds, cannot reliably identify and localize short circuits with the
increasing complexity of the grid under dynamic conditions. Machine learning
(ML) offers a promising alternative; however, systematic benchmarks across
models and settings remain limited. This work presents, for the first time, a
comparative benchmarking study of classical ML models for FC and FL in power
system protection based on EMT data. Using voltage and current waveforms
segmented into sliding windows of 10 ms to 50 ms, we evaluate models under
realistic real-time constraints. Performance is assessed in terms of accuracy,
robustness to window size, and runtime efficiency. The best-performing FC model
achieved an F1 score of 0.992$\pm$0.001, while the top FL model reached an R2
of 0.806$\pm$0.008 with a mean processing time of 0.563 ms.

</details>


### [87] [Improving Cryptocurrency Pump-and-Dump Detection through Ensemble-Based Models and Synthetic Oversampling Techniques](https://arxiv.org/abs/2510.00836)
*Jieun Yu,Minjung Park,Sangmi Chai*

Main category: cs.AI

TL;DR: 该研究使用SMOTE技术解决加密货币市场中Pump and Dump操纵检测的类别不平衡问题，结合集成学习方法显著提高了检测性能。


<details>
  <summary>Details</summary>
Motivation: 加密货币市场中Pump and Dump操纵事件稀缺导致严重的类别不平衡，阻碍了准确的检测。

Method: 应用SMOTE技术进行数据平衡，并评估先进的集成学习模型来区分操纵交易行为和正常市场活动。

Result: SMOTE显著提高了所有模型的检测能力，XGBoost和LightGBM分别达到94.87%和93.59%的召回率，具有强F1分数和快速计算性能。

Conclusion: 数据平衡技术与集成方法的结合显著改善了操纵活动的早期检测，有助于建立更公平、透明和稳定的加密货币市场。

Abstract: This study aims to detect pump and dump (P&D) manipulation in cryptocurrency
markets, where the scarcity of such events causes severe class imbalance and
hinders accurate detection. To address this issue, the Synthetic Minority
Oversampling Technique (SMOTE) was applied, and advanced ensemble learning
models were evaluated to distinguish manipulative trading behavior from normal
market activity. The experimental results show that applying SMOTE greatly
enhanced the ability of all models to detect P&D events by increasing recall
and improving the overall balance between precision and recall. In particular,
XGBoost and LightGBM achieved high recall rates (94.87% and 93.59%,
respectively) with strong F1-scores and demonstrated fast computational
performance, making them suitable for near real time surveillance. These
findings indicate that integrating data balancing techniques with ensemble
methods significantly improves the early detection of manipulative activities,
contributing to a fairer, more transparent, and more stable cryptocurrency
market.

</details>


### [88] [Learning Compact Representations of LLM Abilities via Item Response Theory](https://arxiv.org/abs/2510.00844)
*Jianhao Chen,Chenxu Wang,Gengrui Zhang,Peng Ye,Lei Bai,Wei Hu,Yuzhong Qu,Shuyue Hu*

Main category: cs.AI

TL;DR: 该论文提出了一种基于项目反应理论(IRT)的方法，通过学习LLM的紧凑表示来改进模型路由和性能预测。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型数量激增，如何有效管理和利用这些资源成为重要挑战。需要学习紧凑的LLM能力表示来支持下游任务。

Method: 受IRT启发，将模型正确回答查询的概率建模为三个因素的函数：模型多技能能力向量、查询区分度向量和查询难度标量。使用混合专家网络联合学习这些参数。

Result: 实验表明该方法在模型路由和基准准确率预测方面达到最先进性能，学习到的参数能够编码有意义、可解释的模型能力和查询特征信息。

Conclusion: 基于IRT的方法能够有效学习LLM的紧凑表示，为模型管理和性能预测提供了有力工具。

Abstract: Recent years have witnessed a surge in the number of large language models
(LLMs), yet efficiently managing and utilizing these vast resources remains a
significant challenge. In this work, we explore how to learn compact
representations of LLM abilities that can facilitate downstream tasks, such as
model routing and performance prediction on new benchmarks. We frame this
problem as estimating the probability that a given model will correctly answer
a specific query. Inspired by the item response theory (IRT) in psychometrics,
we model this probability as a function of three key factors: (i) the model's
multi-skill ability vector, (2) the query's discrimination vector that
separates models of differing skills, and (3) the query's difficulty scalar. To
learn these parameters jointly, we introduce a Mixture-of-Experts (MoE) network
that couples model- and query-level embeddings. Extensive experiments
demonstrate that our approach leads to state-of-the-art performance in both
model routing and benchmark accuracy prediction. Moreover, analysis validates
that the learned parameters encode meaningful, interpretable information about
model capabilities and query characteristics.

</details>


### [89] [Unveiling Interesting Insights: Monte Carlo Tree Search for Knowledge Discovery](https://arxiv.org/abs/2510.00876)
*Pietro Totis,Alberto Pozanco,Daniel Borrajo*

Main category: cs.AI

TL;DR: 提出了一种基于蒙特卡洛树搜索(MCTS)的自动洞察和数据探索(AIDE)方法，用于解决从数据中自动发现知识的挑战。


<details>
  <summary>Details</summary>
Motivation: 组织收集了大量数据但难以转化为可操作知识，自动知识发现面临数据导航、模型构建和主观目标等复杂问题。

Method: 使用蒙特卡洛树搜索(MCTS)框架构建AIDE系统，通过探索数据转换和模型来发现有趣的数据模式。

Result: 在真实世界和合成数据上的评估表明，AIDE能有效识别数据转换和模型，揭示有意义的数据模式。

Conclusion: AIDE为自动知识发现提供了可扩展的基础框架，未来可集成更多模式提取策略和领域知识。

Abstract: Organizations are increasingly focused on leveraging data from their
processes to gain insights and drive decision-making. However, converting this
data into actionable knowledge remains a difficult and time-consuming task.
There is often a gap between the volume of data collected and the ability to
process and understand it, which automated knowledge discovery aims to fill.
Automated knowledge discovery involves complex open problems, including
effectively navigating data, building models to extract implicit relationships,
and considering subjective goals and knowledge. In this paper, we introduce a
novel method for Automated Insights and Data Exploration (AIDE), that serves as
a robust foundation for tackling these challenges through the use of Monte
Carlo Tree Search (MCTS). We evaluate AIDE using both real-world and synthetic
data, demonstrating its effectiveness in identifying data transformations and
models that uncover interesting data patterns. Among its strengths, AIDE's
MCTS-based framework offers significant extensibility, allowing for future
integration of additional pattern extraction strategies and domain knowledge.
This makes AIDE a valuable step towards developing a comprehensive solution for
automated knowledge discovery.

</details>


### [90] [FusionAdapter for Few-Shot Relation Learning in Multimodal Knowledge Graphs](https://arxiv.org/abs/2510.00894)
*Ran Liu,Yuan Fang,Xiaoli Li*

Main category: cs.AI

TL;DR: FusionAdapter是一个用于多模态知识图谱中少样本关系学习的模型，通过适配器模块和融合策略有效整合多模态信息，在低资源设置下优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有MMKG方法主要将不同模态对齐到共享空间，忽视了特定模态的独特贡献，在低资源设置下性能受限。

Method: 提出FusionAdapter，包含适配器模块实现各模态对未见关系的有效适应，以及融合策略在保留模态特定特征的同时整合多模态实体表示。

Result: 在两个基准MMKG数据集上的广泛实验表明，FusionAdapter在少样本关系学习任务上优于最先进的方法。

Conclusion: 通过有效适应和融合来自不同模态的信息，FusionAdapter能够以最少的监督实现对新颖关系的更好泛化。

Abstract: Multimodal Knowledge Graphs (MMKGs) incorporate various modalities, including
text and images, to enhance entity and relation representations. Notably,
different modalities for the same entity often present complementary and
diverse information. However, existing MMKG methods primarily align modalities
into a shared space, which tends to overlook the distinct contributions of
specific modalities, limiting their performance particularly in low-resource
settings. To address this challenge, we propose FusionAdapter for the learning
of few-shot relationships (FSRL) in MMKG. FusionAdapter introduces (1) an
adapter module that enables efficient adaptation of each modality to unseen
relations and (2) a fusion strategy that integrates multimodal entity
representations while preserving diverse modality-specific characteristics. By
effectively adapting and fusing information from diverse modalities,
FusionAdapter improves generalization to novel relations with minimal
supervision. Extensive experiments on two benchmark MMKG datasets demonstrate
that FusionAdapter achieves superior performance over state-of-the-art methods.

</details>


### [91] [On Discovering Algorithms for Adversarial Imitation Learning](https://arxiv.org/abs/2510.00922)
*Shashank Reddy Chirra,Jayden Teoh,Praveen Paruchuri,Pradeep Varakantham*

Main category: cs.AI

TL;DR: 本文提出了DAIL算法，通过LLM引导的进化框架自动发现数据驱动的奖励分配函数，显著提升了对抗模仿学习的稳定性和性能。


<details>
  <summary>Details</summary>
Motivation: 对抗模仿学习(AIL)方法虽然有效但通常不稳定。现有研究主要关注密度比估计，而奖励分配函数的作用被忽视，且严重依赖人工设计。

Method: 采用LLM引导的进化框架，在奖励分配函数空间中进行高效探索，基于模仿策略的性能自动发现最优的奖励分配函数。

Result: DAIL算法在未见过的环境和策略优化算法上表现出良好的泛化能力，超越了当前最先进的人工设计基线方法。

Conclusion: DAIL是第一个元学习的AIL算法，通过数据驱动的奖励分配函数发现，为AIL的稳定性提供了新的见解。

Abstract: Adversarial Imitation Learning (AIL) methods, while effective in settings
with limited expert demonstrations, are often considered unstable. These
approaches typically decompose into two components: Density Ratio (DR)
estimation $\frac{\rho_E}{\rho_{\pi}}$, where a discriminator estimates the
relative occupancy of state-action pairs under the policy versus the expert;
and Reward Assignment (RA), where this ratio is transformed into a reward
signal used to train the policy. While significant research has focused on
improving density estimation, the role of reward assignment in influencing
training dynamics and final policy performance has been largely overlooked. RA
functions in AIL are typically derived from divergence minimization objectives,
relying heavily on human design and ingenuity. In this work, we take a
different approach: we investigate the discovery of data-driven RA functions,
i.e, based directly on the performance of the resulting imitation policy. To
this end, we leverage an LLM-guided evolutionary framework that efficiently
explores the space of RA functions, yielding \emph{Discovered Adversarial
Imitation Learning} (DAIL), the first meta-learnt AIL algorithm. Remarkably,
DAIL generalises across unseen environments and policy optimization algorithms,
outperforming the current state-of-the-art of \emph{human-designed} baselines.
Finally, we analyse why DAIL leads to more stable training, offering novel
insights into the role of RA functions in the stability of AIL. Code is
publicly available: https://github.com/shshnkreddy/DAIL.

</details>


### [92] [Test-Time Search in Neural Graph Coarsening Procedures for the Capacitated Vehicle Routing Problem](https://arxiv.org/abs/2510.00958)
*Yoonju Sim,Hyeonah Kim,Changhyun Kwon*

Main category: cs.AI

TL;DR: 提出了一种改进的基于深度学习的切割平面方法，通过引入随机性和历史信息来增强模型在推理时的性能，能够生成更多样化的切割，包括圆整容量不等式和框架容量不等式。


<details>
  <summary>Details</summary>
Motivation: 现有基于深度学习的分离方法生成的切割数量不足，因为模型对生成多样化子集的敏感性不够，需要提升模型在推理时的性能。

Method: 1. 在图粗化过程中引入随机边选择替代贪心方法；2. 提出GraphCHiP算法，利用粗化历史识别圆整容量不等式和框架容量不等式。

Result: 在随机生成的CVRP实例上，该方法相比现有神经分离方法能更有效地减少对偶间隙，并成功识别出有效的框架容量不等式。

Conclusion: 通过测试时搜索和随机性增强，能够显著提升深度学习模型在切割平面方法中的性能，生成更多样化和有效的切割。

Abstract: The identification of valid inequalities, such as the rounded capacity
inequalities (RCIs), is a key component of cutting plane methods for the
Capacitated Vehicle Routing Problem (CVRP). While a deep learning-based
separation method can learn to find high-quality cuts, our analysis reveals
that the model produces fewer cuts than expected because it is insufficiently
sensitive to generate a diverse set of generated subsets. This paper proposes
an alternative: enhancing the performance of a trained model at inference time
through a new test-time search with stochasticity. First, we introduce
stochastic edge selection into the graph coarsening procedure, replacing the
previously proposed greedy approach. Second, we propose the Graph Coarsening
History-based Partitioning (GraphCHiP) algorithm, which leverages coarsening
history to identify not only RCIs but also, for the first time, the Framed
capacity inequalities (FCIs). Experiments on randomly generated CVRP instances
demonstrate the effectiveness of our approach in reducing the dual gap compared
to the existing neural separation method. Additionally, our method discovers
effective FCIs on a specific instance, despite the challenging nature of
identifying such cuts.

</details>


### [93] [A Neuro-Fuzzy System for Interpretable Long-Term Stock Market Forecasting](https://arxiv.org/abs/2510.00960)
*Miha Ožbot,Igor Škrjanc,Vitomir Štruc*

Main category: cs.AI

TL;DR: 提出Fuzzformer模型，结合RNN、多头自注意力和模糊推理系统，用于多变量股票市场数据的长期时间序列预测，在保持准确性的同时提供可解释性。


<details>
  <summary>Details</summary>
Motivation: 在多变量时间序列预测中，同时实现准确性和可解释性是一个重大挑战。

Method: 使用LSTM网络和时间注意力将多变量数据压缩为适合模糊推理系统的可解释特征，结合多头自注意力和模糊推理系统。

Result: 在S&P500股票市场指数上的测试显示，与ARIMA和LSTM等传统模型相比具有相当的预测性能，同时提供网络内部有意义的信息流。

Conclusion: 该方法在可解释预测方面显示出潜力，虽然存在性能权衡，但在理解和预测股市行为方面具有实际应用价值。

Abstract: In the complex landscape of multivariate time series forecasting, achieving
both accuracy and interpretability remains a significant challenge. This paper
introduces the Fuzzy Transformer (Fuzzformer), a novel recurrent neural network
architecture combined with multi-head self-attention and fuzzy inference
systems to analyze multivariate stock market data and conduct long-term time
series forecasting. The method leverages LSTM networks and temporal attention
to condense multivariate data into interpretable features suitable for fuzzy
inference systems. The resulting architecture offers comparable forecasting
performance to conventional models such as ARIMA and LSTM while providing
meaningful information flow within the network. The method was examined on the
real world stock market index S\&P500. Initial results show potential for
interpretable forecasting and identify current performance tradeoffs,
suggesting practical application in understanding and forecasting stock market
behavior.

</details>


### [94] [QUASAR: Quantum Assembly Code Generation Using Tool-Augmented LLMs via Agentic RL](https://arxiv.org/abs/2510.00967)
*Cong Yu,Valter Uotila,Shilong Deng,Qingyuan Wu,Tuo Shi,Songlin Jiang,Lei You,Bo Zhao*

Main category: cs.AI

TL;DR: QUASAR是一个基于工具增强大语言模型的量子电路生成与优化框架，通过强化学习解决量子电路参数优化和LLM缺乏量子领域知识的问题。


<details>
  <summary>Details</summary>
Motivation: 现有LLM生成的量子电路存在两个主要问题：(i)参数化量子门需要精确数值优化，这取决于多个因素；(ii)LLM缺乏量子领域知识，常生成低质量或不正确的电路。

Method: 提出QUASAR框架，包含：(i)使用外部量子模拟器的电路验证方法；(ii)强化学习训练中的分层奖励机制。

Result: 在4B LLM上，QUASAR在Pass@1达到99.31%有效性，Pass@10达到100%，优于GPT-4o、GPT-5、DeepSeek-V3等工业级LLM以及SFT-only和RL-only基线方法。

Conclusion: QUASAR通过工具增强和强化学习显著提升了量子电路生成的语法和语义性能，解决了LLM在量子计算领域的知识不足问题。

Abstract: Designing and optimizing task-specific quantum circuits are crucial to
leverage the advantage of quantum computing. Recent large language model
(LLM)-based quantum circuit generation has emerged as a promising automatic
solution. However, the fundamental challenges remain unaddressed: (i)
parameterized quantum gates require precise numerical values for optimal
performance, which also depend on multiple aspects, including the number of
quantum gates, their parameters, and the layout/depth of the circuits. (ii)
LLMs often generate low-quality or incorrect quantum circuits due to the lack
of quantum domain-specific knowledge. We propose QUASAR, an agentic
reinforcement learning (RL) framework for quantum circuits generation and
optimization based on tool-augmented LLMs. To align the LLM with
quantum-specific knowledge and improve the generated quantum circuits, QUASAR
designs (i) a quantum circuit verification approach with external quantum
simulators and (ii) a sophisticated hierarchical reward mechanism in RL
training. Extensive evaluation shows improvements in both syntax and semantic
performance of the generated quantum circuits. When augmenting a 4B LLM, QUASAR
has achieved the validity of 99.31% in Pass@1 and 100% in Pass@10,
outperforming industrial LLMs of GPT-4o, GPT-5 and DeepSeek-V3 and several
supervised-fine-tuning (SFT)-only and RL-only baselines.

</details>


### [95] [Integrating AI and Ensemble Forecasting: Explainable Materials Planning with Scorecards and Trend Insights for a Large-Scale Manufacturer](https://arxiv.org/abs/2510.01006)
*Saravanan Venkatachalam*

Main category: cs.AI

TL;DR: 该论文提出了一个售后需求预测和监控的实用架构，集成了收入感知和聚类感知的统计、机器学习和深度学习模型，并配备角色驱动的分析层，用于生成评分卡和趋势诊断。


<details>
  <summary>Details</summary>
Motivation: 为了解决售后需求预测中的复杂性，包括处理外生信号（如安装基数、定价、宏观指标）、COVID-19作为特殊制度的影响，以及为高收入项目和长尾项目提供差异化的预测方法。

Method: 采用帕累托感知的分割方法，对高收入项目进行单独预测，对长尾项目通过聚类进行池化预测；使用水平感知的集成方法，根据业务相关损失（如WMAPE）调整权重；嵌入LLM生成角色感知的叙述和执行报告合同。

Result: 系统能够生成国家-零件级别的预测，并带有校准区间；提供性能评分卡，包括准确性阈值、偏差分解、地理和产品系列热点，以及高影响零件-国家对的根本原因排名；趋势模块跟踪MAPE/WMAPE和偏差的轨迹，检测变化点。

Conclusion: 该系统提供了一个可重复的工作流程，从预测到监控再到库存决策，帮助规划者从关注当前准确性转向关注准确性趋势和可操作的杠杆，实现了在90多个国家和约6000个零件上的闭环管理。

Abstract: This paper presents a practical architecture for after-sales demand
forecasting and monitoring that unifies a revenue- and cluster-aware ensemble
of statistical, machine-learning, and deep-learning models with a role-driven
analytics layer for scorecards and trend diagnostics. The framework ingests
exogenous signals (installed base, pricing, macro indicators, life cycle,
seasonality) and treats COVID-19 as a distinct regime, producing country-part
forecasts with calibrated intervals. A Pareto-aware segmentation forecasts
high-revenue items individually and pools the long tail via clusters, while
horizon-aware ensembling aligns weights with business-relevant losses (e.g.,
WMAPE). Beyond forecasts, a performance scorecard delivers decision-focused
insights: accuracy within tolerance thresholds by revenue share and count, bias
decomposition (over- vs under-forecast), geographic and product-family
hotspots, and ranked root causes tied to high-impact part-country pairs. A
trend module tracks trajectories of MAPE/WMAPE and bias across recent months,
flags entities that are improving or deteriorating, detects change points
aligned with known regimes, and attributes movements to lifecycle and seasonal
factors. LLMs are embedded in the analytics layer to generate role-aware
narratives and enforce reporting contracts. They standardize business
definitions, automate quality checks and reconciliations, and translate
quantitative results into concise, explainable summaries for planners and
executives. The system exposes a reproducible workflow -- request
specification, model execution, database-backed artifacts, and AI-generated
narratives -- so planners can move from "How accurate are we now?" to "Where is
accuracy heading and which levers should we pull?", closing the loop between
forecasting, monitoring, and inventory decisions across more than 90 countries
and about 6,000 parts.

</details>


### [96] [Shape Happens: Automatic Feature Manifold Discovery in LLMs via Supervised Multi-Dimensional Scaling](https://arxiv.org/abs/2510.01025)
*Federico Tiblias,Irina Bigoulaeva,Jingcheng Niu,Simone Balloccu,Iryna Gurevych*

Main category: cs.AI

TL;DR: 提出SMDS方法自动发现语言模型中的特征流形，通过时间推理案例研究发现特征形成多种几何结构，揭示了这些结构的稳定性、功能性和动态适应性。


<details>
  <summary>Details</summary>
Motivation: 现有方法专注于发现特定特征的特定几何结构，缺乏泛化性。需要一种模型无关的方法来自动发现特征流形。

Method: 引入监督多维缩放(SMDS)，一种模型无关的方法，自动发现特征流形。以时间推理为案例研究应用该方法。

Result: 发现不同特征形成各种几何结构（圆、线、簇）；这些结构一致反映概念属性；在不同模型家族和规模中稳定；主动支持模型推理；随上下文变化动态重塑。

Conclusion: 特征流形在语言模型中具有功能性作用，支持基于实体的推理模型，其中语言模型编码和转换结构化表示。

Abstract: The linear representation hypothesis states that language models (LMs) encode
concepts as directions in their latent space, forming organized,
multidimensional manifolds. Prior efforts focus on discovering specific
geometries for specific features, and thus lack generalization. We introduce
Supervised Multi-Dimensional Scaling (SMDS), a model-agnostic method to
automatically discover feature manifolds. We apply SMDS to temporal reasoning
as a case study, finding that different features form various geometric
structures such as circles, lines, and clusters. SMDS reveals many insights on
these structures: they consistently reflect the properties of the concepts they
represent; are stable across model families and sizes; actively support
reasoning in models; and dynamically reshape in response to context changes.
Together, our findings shed light on the functional role of feature manifolds,
supporting a model of entity-based reasoning in which LMs encode and transform
structured representations.

</details>


### [97] [Uncovering the Computational Ingredients of Human-Like Representations in LLMs](https://arxiv.org/abs/2510.01030)
*Zach Studdiford,Timothy T. Rogers,Kushin Mukherjee,Siddharth Suresh*

Main category: cs.AI

TL;DR: 本研究评估了70多个不同架构的LLM在概念表示上与人类的对齐程度，发现指令微调和注意力头维度是影响对齐的关键因素，而多模态预训练和参数量影响有限。现有基准测试无法完全捕捉人机对齐程度。


<details>
  <summary>Details</summary>
Motivation: 当前LLM的快速发展带来了多样化的计算要素，但尚不清楚哪些要素对构建具有人类类似表示能力的模型最为关键。同时，现有基准测试不适合衡量人类与模型之间的表示对齐程度。

Method: 使用认知科学中成熟的三元组相似性任务，基于THINGS数据库的概念，评估了70多个不同计算要素（架构、微调方法、训练数据等）的模型，比较人类与模型的表示对齐程度。

Result: 经过指令微调的模型和具有更大注意力头维度的模型与人类表示最为对齐；多模态预训练和参数量对对齐影响有限；现有基准测试中MMLU比MUSR更能捕捉表示对齐，但没有基准能完全解释对齐分数的方差。

Conclusion: 指令微调和注意力头维度是推进LLM成为人类概念表示模型的关键计算要素，同时填补了LLM评估中的基准测试空白。

Abstract: The ability to translate diverse patterns of inputs into structured patterns
of behavior has been thought to rest on both humans' and machines' ability to
learn robust representations of relevant concepts. The rapid advancement of
transformer-based large language models (LLMs) has led to a diversity of
computational ingredients -- architectures, fine tuning methods, and training
datasets among others -- but it remains unclear which of these ingredients are
most crucial for building models that develop human-like representations.
Further, most current LLM benchmarks are not suited to measuring
representational alignment between humans and models, making benchmark scores
unreliable for assessing if current LLMs are making progress towards becoming
useful cognitive models. We address these limitations by first evaluating a set
of over 70 models that widely vary in their computational ingredients on a
triplet similarity task, a method well established in the cognitive sciences
for measuring human conceptual representations, using concepts from the THINGS
database. Comparing human and model representations, we find that models that
undergo instruction-finetuning and which have larger dimensionality of
attention heads are among the most human aligned, while multimodal pretraining
and parameter size have limited bearing on alignment. Correlations between
alignment scores and scores on existing benchmarks reveal that while some
benchmarks (e.g., MMLU) are better suited than others (e.g., MUSR) for
capturing representational alignment, no existing benchmark is capable of fully
accounting for the variance of alignment scores, demonstrating their
insufficiency in capturing human-AI alignment. Taken together, our findings
help highlight the computational ingredients most essential for advancing LLMs
towards models of human conceptual representation and address a key
benchmarking gap in LLM evaluation.

</details>


### [98] [Activation-Deactivation: A General Framework for Robust Post-hoc Explainable AI](https://arxiv.org/abs/2510.01038)
*Akchunya Chanchal,David A. Kelly,Hana Chockler*

Main category: cs.AI

TL;DR: 提出了一种新的前向传播范式Activation-Deactivation (AD)，通过关闭模型中与遮挡部分对应的组件来消除遮挡输入特征对模型决策的影响，解决了传统黑盒解释方法因遮挡导致图像分布外的问题。


<details>
  <summary>Details</summary>
Motivation: 传统黑盒解释方法依赖遮挡部分输入来生成解释，但这种方法会产生分布外图像，影响解释质量，且选择合适遮挡值需要领域知识。

Method: 提出了ConvAD机制，可以轻松添加到任何训练好的CNN中，实现AD范式，无需额外训练或微调。通过数学证明该机制不会改变网络的决策过程。

Result: 在多个数据集和模型架构上的实验表明，AD解释在鲁棒性方面相比传统遮挡方法有显著提升（最高达62.5%），且无需领域知识。

Conclusion: ConvAD能够提取更鲁棒的解释，解决了传统遮挡方法因分布外图像和需要领域知识的问题。

Abstract: Black-box explainability methods are popular tools for explaining the
decisions of image classifiers. A major drawback of these tools is their
reliance on mutants obtained by occluding parts of the input, leading to
out-of-distribution images. This raises doubts about the quality of the
explanations. Moreover, choosing an appropriate occlusion value often requires
domain knowledge. In this paper we introduce a novel forward-pass paradigm
Activation-Deactivation (AD), which removes the effects of occluded input
features from the model's decision-making by switching off the parts of the
model that correspond to the occlusions. We introduce ConvAD, a drop-in
mechanism that can be easily added to any trained Convolutional Neural Network
(CNN), and which implements the AD paradigm. This leads to more robust
explanations without any additional training or fine-tuning. We prove that the
ConvAD mechanism does not change the decision-making process of the network. We
provide experimental evaluation across several datasets and model
architectures. We compare the quality of AD-explanations with explanations
achieved using a set of masking values, using the proxies of robustness, size,
and confidence drop-off. We observe a consistent improvement in robustness of
AD explanations (up to 62.5%) compared to explanations obtained with
occlusions, demonstrating that ConvAD extracts more robust explanations without
the need for domain knowledge.

</details>


### [99] [Typed Chain-of-Thought: A Curry-Howard Framework for Verifying LLM Reasoning](https://arxiv.org/abs/2510.01069)
*Elija Perrier*

Main category: cs.AI

TL;DR: 提出基于Curry-Howard对应的理论框架，将CoT推理轨迹映射为形式化类型证明结构，以验证推理的忠实性。


<details>
  <summary>Details</summary>
Motivation: CoT提示增强了大型语言模型的推理能力，但生成推理过程的忠实性仍然是模型可解释性的开放问题。

Method: 基于Curry-Howard对应关系，将非正式的自然语言CoT步骤提取并映射到形式化的类型证明结构中。

Result: 成功将CoT轨迹转换为良好类型的证明，可作为计算忠实性的可验证证书。

Conclusion: 该框架提供了将可信叙述性解释转化为形式化可验证程序的方法，为构建更可靠和可信的AI系统提供了路径。

Abstract: While Chain-of-Thought (CoT) prompting enhances the reasoning capabilities of
large language models, the faithfulness of the generated rationales remains an
open problem for model interpretability. We propose a novel theoretical lens
for this problem grounded in the Curry-Howard correspondence, which posits a
direct relationship between formal proofs and computer programs. Under this
paradigm, a faithful reasoning trace is analogous to a well-typed program,
where each intermediate step corresponds to a typed logical inference. We
operationalise this analogy, presenting methods to extract and map the
informal, natural language steps of CoT into a formal, typed proof structure.
Successfully converting a CoT trace into a well-typed proof serves as a strong,
verifiable certificate of its computational faithfulness, moving beyond
heuristic interpretability towards formal verification. Our framework provides
a methodology to transform plausible narrative explanations into formally
verifiable programs, offering a path towards building more reliable and
trustworthy AI systems.

</details>


### [100] [Safety Instincts: LLMs Learn to Trust Their Internal Compass for Self-Defense](https://arxiv.org/abs/2510.01088)
*Guobin Shen,Dongcheng Zhao,Haibo Tong,Jindong Li,Feifei Zhao,Yi Zeng*

Main category: cs.AI

TL;DR: SIRL方法利用LLM内部的安全置信度作为奖励信号，通过强化学习训练模型信任自身的安全直觉，无需外部验证器即可实现高效对齐。


<details>
  <summary>Details</summary>
Motivation: 由于缺乏通用标准和可靠的内容验证器，确保LLM安全性面临挑战，难以获得有效的训练信号。

Method: 发现对齐模型已具备内部安全信念，利用拒绝有害请求时的高置信度与生成危险内容时的高熵值之间的差距，开发SIRL方法将内部置信度转化为自生成奖励信号。

Result: 在Llama和Qwen模型上评估，SIRL对20+种越狱方法保持89%+的防御成功率，仅使用15,000个未标注提示即可超越资源密集型监督方法，同时保持数学、编程和对话基准性能。

Conclusion: 有效对齐可以从模型内部产生，为无需大量人工监督的自主、鲁棒AI安全机制开辟了新途径。

Abstract: Ensuring Large Language Model (LLM) safety remains challenging due to the
absence of universal standards and reliable content validators, making it
difficult to obtain effective training signals. We discover that aligned models
already possess robust internal safety beliefs: they consistently produce
high-confidence refusals to harmful requests while exhibiting high entropy when
generating potentially dangerous content. This entropy gap reveals an untapped
signal--models intrinsically "know" when to refuse. We introduce Safety
Instincts Reinforcement Learning (SIRL), which transforms this internal
confidence into a self-generated reward signal, eliminating dependence on
external validators or human annotations. SIRL teaches models to trust their
safety instincts by reinforcing low-entropy refusal behaviors. Evaluated on
Llama and Qwen models, SIRL maintains 89%+ Defense Success Rates (DSRs) against
20+ jailbreak methods, from static prompts to adaptive attacks. Using only
15,000 unlabeled prompts, SIRL surpasses resource-intensive supervised methods
while preserving performance on mathematics, coding, and conversation
benchmarks. Our work demonstrates that effective alignment can emerge from
within, paving the way for more autonomous and robust AI safety mechanisms that
scale without extensive human oversight.

</details>


### [101] [Optimizing Fairness in Production Planning: A Human-Centric Approach to Machine and Workforce Allocation](https://arxiv.org/abs/2510.01094)
*Alexander Nasuta,Alessandro Cisi,Sylwia Olbrych,Gustavo Vieira,Rui Fernandes,Lucas Paletta,Marlene Mayr,Rishyank Chevuri,Robert Woitsch,Hans Aoyang Zhou,Anas Abdelrazeq,Robert H. Schmitt*

Main category: cs.AI

TL;DR: 提出一个双层人本生产规划框架，结合约束规划（CP）优化生产调度和马尔可夫决策过程（MDP）优化工人分配，实现运营效率和员工公平性的平衡。


<details>
  <summary>Details</summary>
Motivation: 在工业制造中同时优化运营效率和劳动力公平性，将机器能力、处理时间、交期等运营约束与工人偏好、经验、适应性和医疗限制等人因因素整合到生产规划中。

Method: 第一层使用约束规划（CP）解决订单-产线分配问题，生成高利用率的生产计划；第二层使用马尔可夫决策过程（MDP）解决工人-产线分配问题，整合人因因素。比较了贪婪分配、MCTS和RL三种解决方案策略。

Result: CP调度方法产生紧凑、可行的生产计划，延迟率低；MDP工人分配显著提高了公平性和偏好匹配度。领域专家认为两个组件都有效，并提出了改进目标函数以惩罚过度提前和改善工人分配连续性的机会。

Conclusion: 结合CP与基于学习的决策为人本生产规划提供了稳健方法，能够同时优化吞吐量和员工福祉，为工业环境中的公平高效制造调度提供了实用基础。

Abstract: This work presents a two-layer, human-centric production planning framework
designed to optimize both operational efficiency and workforce fairness in
industrial manufacturing. The first layer formulates the Order-Line allocation
as a Constraint Programming (CP) problem, generating high-utilization
production schedules that respect machine capacities, processing times, and due
dates. The second layer models Worker-Line allocation as a Markov Decision
Process (MDP), integrating human factors such as worker preference, experience,
resilience, and medical constraints into the assignment process. Three solution
strategies, greedy allocation, MCTS, and RL, are implemented and compared
across multiple evaluation scenarios. The proposed system is validated through
16 test sessions with domain experts from the automotive industry, combining
quantitative key performance indicators (KPIs) with expert ratings. Results
indicate that the CP-based scheduling approach produces compact, feasible
production plans with low tardiness, while the MDP-based worker allocation
significantly improves fairness and preference alignment compared to baseline
approaches. Domain experts rated both the Order-Line and Worker-Line components
as effective and highlighted opportunities to further refine the objective
function to penalize excessive earliness and improve continuity in worker
assignments. Overall, the findings demonstrate that combining CP with
learning-based decision-making provides a robust approach for human-centric
production planning. The approach enables simultaneous optimization of
throughput and workforce well-being, offering a practical foundation for fair
and efficient manufacturing scheduling in industrial settings.

</details>


### [102] [PRISM-Consult: A Panel-of-Experts Architecture for Clinician-Aligned Diagnosis](https://arxiv.org/abs/2510.01114)
*Lionel Levine,John Santerre,Alexander S. Young,T. Barry Levine,Francis Campion,Majid Sarrafzadeh*

Main category: cs.AI

TL;DR: PRISM-Consult是一个临床医生对齐的专家小组架构，通过轻量级路由器将急诊病例分发到不同领域的专科模型，实现参数高效和可解释性。


<details>
  <summary>Details</summary>
Motivation: 解决急诊科大规模咨询中安全、可审计和低延迟的需求，避免常见事件主导结果，提供实用的临床部署路径。

Method: 扩展紧凑的PRISM序列模型为路由的专科专家家族，将病例标记化为结构化临床事件，使用轻量级路由器读取前几个标记并分发到专科模型（心脏血管、肺、胃肠、肌肉骨骼、心理）。

Result: 在真实世界急诊科队列中，专科模型在各领域表现出平滑收敛和低开发困惑度，路由器在安全优先策略下实现高质量路由和大量计算节省。

Conclusion: 该框架为安全、可审计和低延迟的大规模咨询提供了实用路径，并通过外部/时间复制、不对称生命威胁阈值和多标签仲裁等验证步骤满足前瞻性临床部署标准。

Abstract: We present PRISM-Consult, a clinician-aligned panel-of-experts architecture
that extends the compact PRISM sequence model into a routed family of domain
specialists. Episodes are tokenized as structured clinical events; a
light-weight router reads the first few tokens and dispatches to specialist
models (Cardiac-Vascular, Pulmonary, Gastro-Oesophageal, Musculoskeletal,
Psychogenic). Each specialist inherits PRISM's small transformer backbone and
token template, enabling parameter efficiency and interpretability. On
real-world Emergency Department cohorts, specialists exhibit smooth convergence
with low development perplexities across domains, while the router achieves
high routing quality and large compute savings versus consult-all under a
safety-first policy. We detail the data methodology (initial vs. conclusive
ICD-9 families), routing thresholds and calibration, and report per-domain
results to avoid dominance by common events. The framework provides a practical
path to safe, auditable, and low-latency consult at scale, and we outline
validation steps-external/temporal replication, asymmetric life-threat
thresholds, and multi-label arbitration-to meet prospective clinical deployment
standards.

</details>


### [103] [Exploring Network-Knowledge Graph Duality: A Case Study in Agentic Supply Chain Risk Analysis](https://arxiv.org/abs/2510.01115)
*Evan Heus,Rick Bookstaber,Dhruv Sharma*

Main category: cs.AI

TL;DR: 提出了一种基于LLM的智能体框架，通过将供应链网络视为知识图谱，利用网络科学原理进行风险路径检索，结合上下文模板使定量数据对LLM可理解，实现实时、可解释的风险分析。


<details>
  <summary>Details</summary>
Motivation: 传统RAG方法在金融风险分析中过度简化关系，而专业模型成本高且静态，无法处理复杂多模态的供应链风险数据。

Method: 将供应链网络建模为知识图谱，基于网络中心性分数指导图遍历器提取关键风险路径，结合数值因子表和新闻流数据，使用上下文模板将原始数据转换为自然语言描述。

Result: 该轻量级方法无需昂贵微调或专用图数据库，即可生成简洁、可解释且上下文丰富的实时风险叙述。

Conclusion: 利用网络与知识图谱的二元性，通过智能体架构有效解决了LLM在复杂金融风险分析中的局限性。

Abstract: Large Language Models (LLMs) struggle with the complex, multi-modal, and
network-native data underlying financial risk. Standard Retrieval-Augmented
Generation (RAG) oversimplifies relationships, while specialist models are
costly and static. We address this gap with an LLM-centric agent framework for
supply chain risk analysis. Our core contribution is to exploit the inherent
duality between networks and knowledge graphs (KG). We treat the supply chain
network as a KG, allowing us to use structural network science principles for
retrieval. A graph traverser, guided by network centrality scores, efficiently
extracts the most economically salient risk paths. An agentic architecture
orchestrates this graph retrieval alongside data from numerical factor tables
and news streams. Crucially, it employs novel ``context shells'' -- descriptive
templates that embed raw figures in natural language -- to make quantitative
data fully intelligible to the LLM. This lightweight approach enables the model
to generate concise, explainable, and context-rich risk narratives in real-time
without costly fine-tuning or a dedicated graph database.

</details>


### [104] [Apriel-1.5-15b-Thinker](https://arxiv.org/abs/2510.01141)
*Shruthan Radhakrishna,Aman Tiwari,Aanjaneya Shukla,Masoud Hashemi,Rishabh Maheshwary,Shiva Krishna Reddy Malay,Jash Mehta,Pulkit Pattnaik,Saloni Mittal,Khalil Slimi,Kelechi Ogueji,Akintunde Oladipo,Soham Parikh,Oluwanifemi Bamgbose,Toby Liang,Ahmed Masry,Khyati Mahajan,Sai Rajeswar Mudumba,Vikas Yadav,Sathwik Tejaswi Madhusudhan,Torsten Scholak,Sagar Davasam,Srinivas Sunkara,Nicholas Chapados*

Main category: cs.AI

TL;DR: Apriel-1.5-15B-Thinker是一个150亿参数的多模态推理模型，通过渐进式三阶段训练方法，在不依赖大规模计算资源的情况下实现了前沿性能表现。


<details>
  <summary>Details</summary>
Motivation: 旨在证明通过精心设计的训练方法而非单纯扩大模型规模，可以在有限计算资源下实现前沿水平的多模态推理能力，使更多组织能够部署高性能模型。

Method: 采用三阶段渐进式训练：1）深度扩展推理能力；2）分阶段持续预训练，先建立文本和视觉基础理解，再通过合成数据增强视觉推理；3）高质量文本监督微调，包含显式推理轨迹。

Result: 在Artificial Analysis Intelligence Index上获得52分，与DeepSeek-R1-0528相当但计算资源需求显著更少；在十个图像基准测试中平均性能仅比Gemini-2.5-Flash和Claude Sonnet-3.7低5分。

Conclusion: 精心设计的中期训练策略可以在不依赖大规模计算的情况下显著缩小能力差距，使前沿水平的多模态推理对基础设施有限的组织变得可行。

Abstract: We present Apriel-1.5-15B-Thinker, a 15-billion parameter open-weights
multimodal reasoning model that achieves frontier-level performance through
training design rather than sheer scale. Starting from Pixtral-12B, we apply a
progressive three-stage methodology: (1) depth upscaling to expand reasoning
capacity without pretraining from scratch, (2) staged continual pre-training
that first develops foundational text and vision understanding, then enhances
visual reasoning through targeted synthetic data generation addressing spatial
structure, compositional understanding, and fine-grained perception, and (3)
high-quality text-only supervised fine-tuning on curated instruction-response
pairs with explicit reasoning traces spanning mathematics, coding, science, and
tool use. Notably, our model achieves competitive results without reinforcement
learning or preference optimization, isolating the contribution of our
data-centric continual pre-training approach. On the Artificial Analysis
Intelligence Index, Apriel-1.5-15B-Thinker attains a score of 52, matching
DeepSeek-R1-0528 despite requiring significantly fewer computational resources.
Across ten image benchmarks, its performance is on average within five points
of Gemini-2.5-Flash and Claude Sonnet-3.7, a key achievement for a model
operating within single-GPU deployment constraints. Our results demonstrate
that thoughtful mid-training 2 design can close substantial capability gaps
without massive scale, making frontier-level multimodal reasoning accessible to
organizations with limited infrastructure. We release the model checkpoint, all
training recipes, and evaluation protocols under the MIT license to to advance
open-source research.

</details>


### [105] [Generalized Parallel Scaling with Interdependent Generations](https://arxiv.org/abs/2510.01143)
*Harry Dong,David Brandfonbrener,Eryk Helenowski,Yun He,Mrinal Kumar,Han Fang,Yuejie Chi,Karthik Abinav Sankararaman*

Main category: cs.AI

TL;DR: Bridge是一种并行LLM推理方法，通过在并行生成的响应之间建立信息桥梁，利用相互依赖的隐藏状态来提升响应质量和一致性。


<details>
  <summary>Details</summary>
Motivation: 传统的并行LLM推理中，多个响应是独立生成的，计算资源被分割，一个生成中的有用信息无法被其他生成利用，这限制了响应质量和响应集的一致性。

Method: 将批量LLM隐藏状态重新构想为整体张量而非独立切片，通过添加少量新参数（2.8%-5.1%）来生成相互依赖的并行响应。

Result: Bridge将带可验证奖励的强化学习的相对平均准确率增益提高了50%，并提升了正确响应的一致性。训练一次即可扩展到任何生成宽度，性能始终优于独立生成。

Conclusion: Bridge解锁了一种更通用的并行扩展模式，有效利用序列间的信息，与任何后生成聚合技术兼容。

Abstract: Parallel LLM inference scaling involves sampling a set of $N>1$ responses for
a single input prompt. However, these $N$ parallel responses tend to be
generated independently from each other, partitioning compute resources and
leaving potentially useful information in one generation untapped by others.
This is in contrast to response length scaling where past computation is used
in all future steps. For higher quality responses and response sets, we propose
Bridge to generate interdependent responses in parallel by rethinking batched
LLM hidden states as holistic tensors rather than independent slices. With only
a small amount (2.8%-5.1%) of new parameters, Bridge improves the relative mean
accuracy gains from reinforcement learning with verifiable rewards by up to 50%
and boosts consistency of correct responses. Trained once, Bridge scales to any
generation width, all with greater performance than independent generations,
unlocking a more general mode of parallel scaling that effectively leverages
information between sequences, compatible with any post-generation aggregation
technique.

</details>
