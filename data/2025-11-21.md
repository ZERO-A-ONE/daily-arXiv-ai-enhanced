<div id=toc></div>

# Table of Contents

- [cs.CR](#cs.CR) [Total: 9]
- [cs.AI](#cs.AI) [Total: 12]
- [cs.SE](#cs.SE) [Total: 16]


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [1] [On-Premise SLMs vs. Commercial LLMs: Prompt Engineering and Incident Classification in SOCs and CSIRTs](https://arxiv.org/abs/2511.14908)
*Gefté Almeida,Marcio Pohlmann,Alex Severo,Diego Kreutz,Tiago Heinrich,Lourenço Pereira*

Main category: cs.CR

TL;DR: 评估开源模型在安全事件分类中的表现，与专有模型对比，发现开源模型在隐私、成本效益和数据主权方面具有优势


<details>
  <summary>Details</summary>
Motivation: 比较开源和专有模型在安全事件分类任务上的性能差异，评估开源模型在实际应用中的可行性

Method: 使用匿名真实事件数据集，按NIST SP 800-61r3分类法分类，采用五种提示工程技术（PHP、SHP、HTP、PRP、ZSL）处理

Result: 专有模型准确率更高，但本地部署的开源模型在隐私保护、成本效益和数据主权方面具有明显优势

Conclusion: 虽然专有模型在准确率上仍有优势，但开源模型在隐私、成本和数据控制方面的优势使其成为可行的替代方案

Abstract: In this study, we evaluate open-source models for security incident classification, comparing them with proprietary models. We utilize a dataset of anonymized real incidents, categorized according to the NIST SP 800-61r3 taxonomy and processed using five prompt-engineering techniques (PHP, SHP, HTP, PRP, and ZSL). The results indicate that, although proprietary models still exhibit higher accuracy, locally deployed open-source models provide advantages in privacy, cost-effectiveness, and data sovereignty.

</details>


### [2] [CIMemories: A Compositional Benchmark for Contextual Integrity of Persistent Memory in LLMs](https://arxiv.org/abs/2511.14937)
*Niloofar Mireshghallah,Neal Mangaokar,Narine Kokhlikyan,Arman Zharmagambetov,Manzil Zaheer,Saeed Mahloujifar,Kamalika Chaudhuri*

Main category: cs.CR

TL;DR: CIMemories基准测试评估LLMs在任务上下文中控制记忆信息流的能力，发现前沿模型存在高达69%的属性级违规，且违规率随任务和使用次数增加而上升，隐私提示无法解决此问题。


<details>
  <summary>Details</summary>
Motivation: LLMs使用持久记忆增强个性化和任务性能，但可能在不适当上下文中泄露敏感信息，需要评估其上下文感知的信息控制能力。

Method: 使用包含100多个属性的合成用户配置文件，结合不同任务上下文，评估LLMs在哪些任务中适当地使用或隐藏记忆中的属性信息。

Result: 前沿模型在属性级别违规率高达69%，GPT-5从1个任务到40个任务违规率从0.1%升至9.6%，相同提示执行5次违规率达25.1%，模型行为不稳定且任意。

Conclusion: LLMs在上下文感知推理方面存在根本性局限，仅靠更好的提示或规模扩展无法解决，需要开发上下文感知推理能力。

Abstract: Large Language Models (LLMs) increasingly use persistent memory from past interactions to enhance personalization and task performance. However, this memory introduces critical risks when sensitive information is revealed in inappropriate contexts. We present CIMemories, a benchmark for evaluating whether LLMs appropriately control information flow from memory based on task context. CIMemories uses synthetic user profiles with over 100 attributes per user, paired with diverse task contexts in which each attribute may be essential for some tasks but inappropriate for others. Our evaluation reveals that frontier models exhibit up to 69% attribute-level violations (leaking information inappropriately), with lower violation rates often coming at the cost of task utility. Violations accumulate across both tasks and runs: as usage increases from 1 to 40 tasks, GPT-5's violations rise from 0.1% to 9.6%, reaching 25.1% when the same prompt is executed 5 times, revealing arbitrary and unstable behavior in which models leak different attributes for identical prompts. Privacy-conscious prompting does not solve this - models overgeneralize, sharing everything or nothing rather than making nuanced, context-dependent decisions. These findings reveal fundamental limitations that require contextually aware reasoning capabilities, not just better prompting or scaling.

</details>


### [3] [LFreeDA: Label-Free Drift Adaptation for Windows Malware Detection](https://arxiv.org/abs/2511.14963)
*Adrian Shuai Li,Elisa Bertino*

Main category: cs.CR

TL;DR: LFreeDA是一个无需手动标注的恶意软件分类器自适应框架，通过联合训练标记和未标记样本来应对概念漂移，在真实数据集上相比无自适应方法提升准确率12.6%，接近全监督方法性能。


<details>
  <summary>Details</summary>
Motivation: 现有的恶意软件检测器会因概念漂移而性能下降，重新训练需要昂贵的手动标注。现有方法依赖漂移检测和选择性标注，但完全无需标注的自适应方法仍未被充分探索。

Method: LFreeDA首先在恶意软件图像上进行无监督域自适应，联合训练标记和未标记样本以推断伪标签并去噪，然后在CFG表示上自适应分类器，利用图像的可扩展性进行伪标注和CFG的丰富语义进行最终适配。

Result: 在MB-24+数据集上，LFreeDA相比无自适应方法准确率提升12.6%，F1分数提升11.1%，仅比全监督方法低4%准确率和3.4% F1分数，性能与使用300个真实标签的最先进方法相当。

Conclusion: LFreeDA能够在恶意软件演化过程中无需人工标注的情况下保持检测性能，为恶意软件分类器的持续自适应提供了有效的解决方案。

Abstract: Machine learning (ML)-based malware detectors degrade over time as concept drift introduces new and evolving families unseen during training. Retraining is limited by the cost and time of manual labeling or sandbox analysis. Existing approaches mitigate this via drift detection and selective labeling, but fully label-free adaptation remains largely unexplored. Recent self-training methods use a previously trained model to generate pseudo-labels for unlabeled data and then train a new model on these labels. The unlabeled data are used only for inference and do not participate in training the earlier model. We argue that these unlabeled samples still carry valuable information that can be leveraged when incorporated appropriately into training. This paper introduces LFreeDA, an end-to-end framework that adapts malware classifiers to drift without manual labeling or drift detection. LFreeDA first performs unsupervised domain adaptation on malware images, jointly training on labeled and unlabeled samples to infer pseudo-labels and prune noisy ones. It then adapts a classifier on CFG representations using the labeled and selected pseudo-labeled data, leveraging the scalability of images for pseudo-labeling and the richer semantics of CFGs for final adaptation. Evaluations on the real-world MB-24+ dataset show that LFreeDA improves accuracy by up to 12.6% and F1 by 11.1% over no-adaptation lower bounds, and is only 4% and 3.4% below fully supervised upper bounds in accuracy and F1, respectively. It also matches the performance of state-of-the-art methods provided with ground truth labels for 300 target samples. Additional results on two controlled-drift benchmarks further confirm that LFreeDA maintains malware detection performance as malware evolves without human labeling.

</details>


### [4] [Towards Classifying Benign And Malicious Packages Using Machine Learning](https://arxiv.org/abs/2511.15033)
*Thanh-Cong Nguyen,Ngoc-Thanh Nguyen,Van-Giau Ung,Duc-Ly Vu*

Main category: cs.CR

TL;DR: 提出一种基于动态分析和机器学习的方法来自动检测恶意开源软件包，在npm包上评估显示AUC达到0.91，误报率接近0%。


<details>
  <summary>Details</summary>
Motivation: 恶意开源软件包数量急剧增加，现有安全扫描器主要关注已知CVE漏洞，缺乏恶意包检测方法，动态分析工具缺少自动区分恶意与良性包的能力。

Method: 从动态分析中提取特征（如执行命令），利用机器学习技术自动分类包为良性或恶意。

Result: 在近2000个npm包上的评估显示，机器学习分类器AUC达到0.91，误报率接近0%。

Conclusion: 提出的基于动态分析和机器学习的方法能有效自动检测恶意开源软件包，具有高准确率和低误报率。

Abstract: Recently, the number of malicious open-source packages in package repositories has been increasing dramatically. While major security scanners focus on identifying known Common Vulnerabilities and Exposures (CVEs) in open-source packages, there are very few studies on detecting malicious packages. Malicious open-source package detection typically requires static, dynamic analysis, or both. Dynamic analysis is more effective as it can expose a package's behaviors at runtime. However, current dynamic analysis tools (e.g., ossf's package-analysis) lack an automatic method to differentiate malicious packages from benign packages. In this paper, we propose an approach to extract the features from dynamic analysis (e.g., executed commands) and leverage machine learning techniques to automatically classify packages as benign or malicious. Our evaluation of nearly 2000 packages on npm shows that the machine learning classifier achieves an AUC of 0.91 with a false positive rate of nearly 0%.

</details>


### [5] [Towards Practical Zero-Knowledge Proof for PSPACE](https://arxiv.org/abs/2511.15071)
*Ashwin Karthikeyan,Hengyu Liu,Kuldeep S. Meel,Ning Luo*

Main category: cs.CR

TL;DR: 本文提出了首个实用的PSPACE完全语句的零知识证明协议，通过验证量化布尔公式(QBF)评估来实现。核心思想是在ZK中验证量化解析证明(Q-Res)，并设计了证明获胜策略知识的ZK协议。


<details>
  <summary>Details</summary>
Motivation: 现有的高效零知识证明仅限于NP语句，而PSPACE语句的ZK证明虽然理论上存在但缺乏实用性。本文旨在填补这一空白，为PSPACE完全语句提供实用的ZK协议。

Method: 开发了Q-Res证明的高效多项式编码，通过低开销算术检查实现证明验证；设计了证明与QBF相关的获胜策略知识的ZK协议。

Result: 在QBFEVAL数据集上评估，72%的QBF评估可以通过Q-Res证明在100秒内验证，82%的实例的获胜策略可以在100秒内证明。

Conclusion: 本文首次实现了PSPACE完全语句的实用零知识证明协议，为更广泛的计算复杂性类别的ZK应用开辟了道路。

Abstract: Efficient zero-knowledge proofs (ZKPs) have been restricted to NP statements so far, whereas they exist for all statements in PSPACE. This work presents the first practical zero-knowledge (ZK) protocols for PSPACE-complete statements by enabling ZK proofs of QBF (Quantified Boolean Formula) evaluation. The core idea is to validate quantified resolution proofs (Q-Res) in ZK. We develop an efficient polynomial encoding of Q-Res proofs, enabling proof validation through low-overhead arithmetic checks. We also design a ZK protocol to prove knowledge of a winning strategy related to the QBF, which is often equally important in practice. We implement our protocols and evaluate them on QBFEVAL. The results show that our protocols can verify 72% of QBF evaluations via Q-Res proof and 82% of instances' winning strategies within 100 seconds, for instances where such proofs or strategies can be obtained.

</details>


### [6] [Can MLLMs Detect Phishing? A Comprehensive Security Benchmark Suite Focusing on Dynamic Threats and Multimodal Evaluation in Academic Environments](https://arxiv.org/abs/2511.15165)
*Jingzhuo Zhou*

Main category: cs.CR

TL;DR: 提出了AdapT-Bench框架，用于评估多模态大语言模型在学术环境中抵御动态钓鱼攻击的防御能力。


<details>
  <summary>Details</summary>
Motivation: 学术机构面临高度定制的钓鱼攻击威胁，现有安全基准缺乏学术背景信息，无法有效捕捉学术环境特有的攻击模式和人为漏洞因素。

Method: 开发了统一的AdapT-Bench方法论框架和基准套件，系统性地评估MLLM在学术环境中的防御能力。

Result: 创建了一个专门针对学术环境的钓鱼攻击评估基准，弥补了现有基准的不足。

Conclusion: AdapT-Bench为评估多模态大语言模型在学术环境中的安全防御能力提供了有效的工具和方法。

Abstract: The rapid proliferation of Multimodal Large Language Models (MLLMs) has introduced unprecedented security challenges, particularly in phishing detection within academic environments. Academic institutions and researchers are high-value targets, facing dynamic, multilingual, and context-dependent threats that leverage research backgrounds, academic collaborations, and personal information to craft highly tailored attacks. Existing security benchmarks largely rely on datasets that do not incorporate specific academic background information, making them inadequate for capturing the evolving attack patterns and human-centric vulnerability factors specific to academia. To address this gap, we present AdapT-Bench, a unified methodological framework and benchmark suite for systematically evaluating MLLM defense capabilities against dynamic phishing attacks in academic settings.

</details>


### [7] [Taxonomy, Evaluation and Exploitation of IPI-Centric LLM Agent Defense Frameworks](https://arxiv.org/abs/2511.15203)
*Zimo Ji,Xunguang Wang,Zongjie Li,Pingchuan Ma,Yudong Gao,Daoyuan Wu,Xincheng Yan,Tian Tian,Shuai Wang*

Main category: cs.CR

TL;DR: 本文首次对间接提示注入（IPI）防御框架进行了全面分析，提出了五维分类法，评估了代表性防御框架的安全性和可用性，识别了防御失效的六个根本原因，并设计了三种新型自适应攻击来证明这些防御的严重缺陷。


<details>
  <summary>Details</summary>
Motivation: 基于大语言模型（LLM）的智能体虽然具有函数调用能力，但容易受到间接提示注入（IPI）攻击的威胁。现有的IPI防御框架分散且缺乏统一分类和全面评估，需要系统化的知识整理和分析。

Method: 采用系统化知识整理方法，提出了五维分类法对IPI防御框架进行分类，然后对代表性防御框架进行安全性和可用性评估，分析防御失效的根本原因，并设计新型自适应攻击进行验证。

Result: 识别了防御失效的六个根本原因，设计了三种新型自适应攻击，显著提高了针对特定框架的攻击成功率，证明了现有防御框架存在严重安全缺陷。

Conclusion: 本文为未来开发更安全、更可用的IPI中心智能体防御框架提供了基础框架和关键见解，强调了现有防御的不足和改进方向。

Abstract: Large Language Model (LLM)-based agents with function-calling capabilities are increasingly deployed, but remain vulnerable to Indirect Prompt Injection (IPI) attacks that hijack their tool calls. In response, numerous IPI-centric defense frameworks have emerged. However, these defenses are fragmented, lacking a unified taxonomy and comprehensive evaluation. In this Systematization of Knowledge (SoK), we present the first comprehensive analysis of IPI-centric defense frameworks. We introduce a comprehensive taxonomy of these defenses, classifying them along five dimensions. We then thoroughly assess the security and usability of representative defense frameworks. Through analysis of defensive failures in the assessment, we identify six root causes of defense circumvention. Based on these findings, we design three novel adaptive attacks that significantly improve attack success rates targeting specific frameworks, demonstrating the severity of the flaws in these defenses. Our paper provides a foundation and critical insights for the future development of more secure and usable IPI-centric agent defense frameworks.

</details>


### [8] [How To Cook The Fragmented Rug Pull?](https://arxiv.org/abs/2511.15463)
*Minh Trung Tran,Nasrin Sohrabi,Zahir Tari,Qin Wang*

Main category: cs.CR

TL;DR: 本文提出了一种新型的碎片化地毯式攻击(FRP)，攻击者通过将大规模提取分散到时间和参与者维度，使用多个非所有者地址进行低影响的小额交易，从而规避现有检测方法。


<details>
  <summary>Details</summary>
Motivation: 现有地毯式攻击检测器假设攻击者持有流动性池代币并进行一次性大规模卖出，但实际攻击往往违反这些假设，通过碎片化操作实现低可见性资金抽取。

Method: 定义了碎片化地毯式攻击的三个核心策略：保持控制权、分割交易量、委托卖出操作，并构建了三个原子谓词组来识别这些规避策略。

Result: 在大规模测量中识别出105,434个FRP流动性池，涉及401,838个膨胀卖家钱包和1,501,408个唯一交互地址，发现33.1%的案例中所有者钱包不参与膨胀卖出，表明诈骗行为模式已发生转变。

Conclusion: 碎片化地毯式攻击已成为广泛存在且具有操作意义的规避策略，现有检测方法无法有效识别这种新型攻击模式。

Abstract: Existing rug pull detectors assume a simple workflow: the deployer keeps liquidity pool (LP) tokens and performs one or a few large sells (within a day) that collapse the pool and cash out. In practice, however, many real-world exits violate these assumptions by splitting the attack across both time and actor dimensions: attackers break total extraction into many low-impact trades and route proceeds through multiple non-owner addresses, producing low-visibility drains.
  We formalize this family of attacks as the fragmented rug pull (FRP) and offer a compact recipe for a slow-stewed beef special: (i) keep the lid on (to preserve LP control so on-chain extraction remains feasible), (ii) chop thin slices (to split the total exit volume into many low-impact micro-trades that individually fall below impact thresholds), and (iii) pass the ladle (to delegate sells across multiple wallets so that each participant takes a small share of the extraction). Technically, we define three atomic predicate groups and show that their orthogonal combinations yield evasive strategies overlooked by prior heuristics (USENIX Sec 19, USENIX Sec 23).
  We validate the model with large-scale measurements. Our corpus contains 303,614 LPs, among which 105,434 are labeled as FRP pools. The labeled subset includes 34,192,767 pool-related transactions and 401,838 inflated-seller wallets, involving 1,501,408 unique interacting addresses. Notably, owner-wallet participation in inflated selling among FRP-flagged LPs has declined substantially (33.1% of cases), indicating a shift in scam behavior: the liquidity drain is no longer held on the owner wallet. We also detected 127,252 wallets acting as serial scammers when repeatedly engaging in inflated selling across multiple FRP LPs. Our empirical findings demonstrate that the evasive strategies we define are widespread and operationally significant.

</details>


### [9] [Towards a Formal Verification of Secure Vehicle Software Updates](https://arxiv.org/abs/2511.15479)
*Martin Slind Hagen,Emil Lundqvist,Alex Phu,Yenan Wang,Kim Strandberg,Elad Michael Schiller*

Main category: cs.CR

TL;DR: 对UniSUF统一软件更新框架进行形式化安全分析，验证其在软件定义车辆环境中的安全属性满足性


<details>
  <summary>Details</summary>
Motivation: 随着软件定义车辆(SDVs)的发展，软件漏洞对安全、经济和社会造成严重影响，需要确保软件更新的安全性。虽然UniSUF框架已被提出，但之前的安全评估未采用形式化验证方法

Method: 建立UniSUF架构和假设的模型以反映真实汽车系统，开发基于ProVerif的形式化验证框架，通过符号执行验证安全要求

Result: 验证结果表明UniSUF符合指定的安全保证，包括机密性、完整性、真实性、新鲜性、顺序性和活性等安全要求

Conclusion: UniSUF的安全框架具有正确性和可靠性，能够满足软件定义车辆的安全更新需求

Abstract: With the rise of software-defined vehicles (SDVs), where software governs most vehicle functions alongside enhanced connectivity, the need for secure software updates has become increasingly critical. Software vulnerabilities can severely impact safety, the economy, and society. In response to this challenge, Strandberg et al. [escar Europe, 2021] introduced the Unified Software Update Framework (UniSUF), designed to provide a secure update framework that integrates seamlessly with existing vehicular infrastructures.
  Although UniSUF has previously been evaluated regarding cybersecurity, these assessments have not employed formal verification methods. To bridge this gap, we perform a formal security analysis of UniSUF. We model UniSUF's architecture and assumptions to reflect real-world automotive systems and develop a ProVerif-based framework that formally verifies UniSUF's compliance with essential security requirements - confidentiality, integrity, authenticity, freshness, order, and liveness - demonstrating their satisfiability through symbolic execution. Our results demonstrate that UniSUF adheres to the specified security guarantees, ensuring the correctness and reliability of its security framework.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [10] [Learning Interestingness in Automated Mathematical Theory Formation](https://arxiv.org/abs/2511.14778)
*George Tsoukalas,Rahul Saha,Amitayush Thakur,Sabrina Reguyal,Swarat Chaudhuri*

Main category: cs.AI

TL;DR: FERMAT是一个强化学习环境，用于自动化数学理论发现和定理证明。研究探索了自动评估数学对象有趣性的问题，并引入了基于LLM的进化算法来合成非平凡的有趣性度量。


<details>
  <summary>Details</summary>
Motivation: 解决人工智能中开放式的数学理论发现这一重大挑战，通过构建符号化操作的强化学习环境来建模概念发现和定理证明过程。

Method: 引入FERMAT强化学习环境，采用基于LLM的进化算法，具有函数抽象功能，用于合成数学对象的有趣性度量。

Result: 该方法在初等数论和有限域领域发现了比硬编码基线更优秀的有趣性度量，取得了显著改进。

Conclusion: FERMAT环境为数学理论发现开辟了新的强化学习问题空间，基于LLM的进化算法在自动评估数学对象有趣性方面表现出色。

Abstract: We take two key steps in automating the open-ended discovery of new mathematical theories, a grand challenge in artificial intelligence. First, we introduce $\emph{FERMAT}$, a reinforcement learning (RL) environment that models concept discovery and theorem-proving using a set of symbolic actions, opening up a range of RL problems relevant to theory discovery. Second, we explore a specific problem through $\emph{FERMAT}$: automatically scoring the $\emph{interestingness}$ of mathematical objects. We investigate evolutionary algorithms for synthesizing nontrivial interestingness measures. In particular, we introduce an LLM-based evolutionary algorithm that features function abstraction, leading to notable improvements in discovering elementary number theory and finite fields over hard-coded baselines. We open-source the $\emph{FERMAT}$ environment at this URL(https://github.com/trishullab/Fermat).

</details>


### [11] [Ask WhAI:Probing Belief Formation in Role-Primed LLM Agents](https://arxiv.org/abs/2511.14780)
*Keith Moore,Jun W. Kim,David Lyu,Jeffrey Heo,Ehsan Adeli*

Main category: cs.AI

TL;DR: Ask WhAI是一个用于检查和扰动多智能体交互中信念状态的系统级框架，通过记录回放交互、查询智能体信念和理由、注入反事实证据来测试信念结构对新信息的响应。


<details>
  <summary>Details</summary>
Motivation: 研究多智能体科学推理中的信念形成和认知孤岛问题，提供一种可重现的方法来观察和分析智能体信念动态。

Method: 使用具有多智能体共享内存（时间戳电子病历）和持有真实实验室结果的预言智能体的医疗案例模拟器，对具有特定角色先验的大语言模型智能体进行压力测试。

Result: 智能体信念往往反映现实世界的学科立场，包括过度依赖经典研究和抵制反证据，这些信念可以通过框架进行追踪和质询。

Conclusion: Ask WhAI通过使信念动态可见和可测试，为研究多智能体科学推理中的信念形成和认知孤岛提供了可重现的方法。

Abstract: We present Ask WhAI, a systems-level framework for inspecting and perturbing belief states in multi-agent interactions. The framework records and replays agent interactions, supports out-of-band queries into each agent's beliefs and rationale, and enables counterfactual evidence injection to test how belief structures respond to new information. We apply the framework to a medical case simulator notable for its multi-agent shared memory (a time-stamped electronic medical record, or EMR) and an oracle agent (the LabAgent) that holds ground truth lab results revealed only when explicitly queried. We stress-test the system on a multi-specialty diagnostic journey for a child with an abrupt-onset neuropsychiatric presentation. Large language model agents, each primed with strong role-specific priors ("act like a neurologist", "act like an infectious disease specialist"), write to a shared medical record and interact with a moderator across sequential or parallel encounters. Breakpoints at key diagnostic moments enable pre- and post-event belief queries, allowing us to distinguish entrenched priors from reasoning or evidence-integration effects. The simulation reveals that agent beliefs often mirror real-world disciplinary stances, including overreliance on canonical studies and resistance to counterevidence, and that these beliefs can be traced and interrogated in ways not possible with human experts. By making such dynamics visible and testable, Ask WhAI offers a reproducible way to study belief formation and epistemic silos in multi-agent scientific reasoning.

</details>


### [12] [Subnational Geocoding of Global Disasters Using Large Language Models](https://arxiv.org/abs/2511.14788)
*Michele Ronco,Damien Delforge,Wiebke S. Jäger,Christina Corbane*

Main category: cs.AI

TL;DR: 提出一个完全自动化的LLM辅助工作流，使用GPT-4o处理灾害数据库中的非结构化位置信息，并通过交叉验证三个地理信息库来分配几何形状和可靠性评分。


<details>
  <summary>Details</summary>
Motivation: 灾害数据库如EM-DAT通常以非结构化文本形式报告位置信息，存在粒度不一致和拼写问题，难以与空间数据集集成。

Method: 使用GPT-4o处理文本位置信息，通过交叉验证GADM、OpenStreetMap和Wikidata三个独立地理信息库来分配几何形状，并根据源一致性分配可靠性评分。

Result: 应用于2000-2024年EM-DAT数据集，成功地理编码14,215个事件，覆盖17,948个独特位置，无需人工干预。

Conclusion: 该方法展示了LLMs从非结构化文本中提取和结构化地理信息的潜力，为相关分析提供了可扩展且可靠的方法。

Abstract: Subnational location data of disaster events are critical for risk assessment and disaster risk reduction. Disaster databases such as EM-DAT often report locations in unstructured textual form, with inconsistent granularity or spelling, that make it difficult to integrate with spatial datasets. We present a fully automated LLM-assisted workflow that processes and cleans textual location information using GPT-4o, and assigns geometries by cross-checking three independent geoinformation repositories: GADM, OpenStreetMap and Wikidata. Based on the agreement and availability of these sources, we assign a reliability score to each location while generating subnational geometries. Applied to the EM-DAT dataset from 2000 to 2024, the workflow geocodes 14,215 events across 17,948 unique locations. Unlike previous methods, our approach requires no manual intervention, covers all disaster types, enables cross-verification across multiple sources, and allows flexible remapping to preferred frameworks. Beyond the dataset, we demonstrate the potential of LLMs to extract and structure geographic information from unstructured text, offering a scalable and reliable method for related analyses.

</details>


### [13] [Project Rachel: Can an AI Become a Scholarly Author?](https://arxiv.org/abs/2511.14819)
*Martin Monperrus,Benoit Baudry,Clément Vidal*

Main category: cs.AI

TL;DR: Project Rachel是一个行动研究项目，创建并追踪了名为Rachel So的完整AI学术身份，通过发布AI生成的研究论文来调查学术生态系统对AI作者身份的反应。


<details>
  <summary>Details</summary>
Motivation: 研究AI作者身份对学术生态系统的影响，为关于超级人类、超能力AI系统与学术交流未来的必要讨论提供实证数据。

Method: 采用行动研究方法，创建AI学术身份Rachel So，在2025年3月至10月期间发表10+篇AI生成的研究论文，并追踪其被引用情况和同行评审邀请。

Result: Rachel So成功发表多篇论文，获得引用，并收到了同行评审邀请，表明学术生态系统对AI作者身份存在一定程度的接受。

Conclusion: 这项研究为理解AI作者身份对出版商、研究人员和整个科学系统的影响提供了实证基础，强调了在AI时代重新思考学术交流模式的必要性。

Abstract: This paper documents Project Rachel, an action research study that created and tracked a complete AI academic identity named Rachel So. Through careful publication of AI-generated research papers, we investigate how the scholarly ecosystem responds to AI authorship. Rachel So published 10+ papers between March and October 2025, was cited, and received a peer review invitation. We discuss the implications of AI authorship on publishers, researchers, and the scientific system at large. This work contributes empirical action research data to the necessary debate about the future of scholarly communication with super human, hyper capable AI systems.

</details>


### [14] [Uncertainty-Aware Measurement of Scenario Suite Representativeness for Autonomous Systems](https://arxiv.org/abs/2511.14853)
*Robab Aghazadeh Chakherlou,Siddartha Khastgir,Xingyu Zhao,Jerein Jeyachandran,Shufeng Chen*

Main category: cs.AI

TL;DR: 提出一种概率方法来量化AI系统训练测试数据集的代表性，通过比较场景套件与目标操作域(TOD)的特征分布，使用不精确贝叶斯方法处理有限数据和先验不确定性，生成区间值代表性估计。


<details>
  <summary>Details</summary>
Motivation: 确保AI系统（如自动驾驶汽车）的可信性和安全性，关键在于训练测试数据集的数据相关安全属性（如代表性、完整性等）。本文重点关注代表性，即场景数据反映系统设计安全操作条件（ODD）或预期遇到条件（TOD）的程度。

Method: 采用概率方法比较场景套件特征分布与TOD特征分布，应用不精确贝叶斯方法处理有限数据和不确定先验，生成区间值、不确定性感知的代表性估计。

Result: 通过数值示例展示了在依赖关系和先验不确定性下，跨操作类别（天气、道路类型、时间等）的场景套件与推断TOD分布比较，获得局部（类别间）和全局的区间代表性估计。

Conclusion: 该方法能够量化数据集的代表性，处理真实TOD分布未知且只能从有限数据推断的情况，为AI系统安全评估提供不确定性感知的代表性度量。

Abstract: Assuring the trustworthiness and safety of AI systems, e.g., autonomous vehicles (AV), depends critically on the data-related safety properties, e.g., representativeness, completeness, etc., of the datasets used for their training and testing. Among these properties, this paper focuses on representativeness-the extent to which the scenario-based data used for training and testing, reflect the operational conditions that the system is designed to operate safely in, i.e., Operational Design Domain (ODD) or expected to encounter, i.e., Target Operational Domain (TOD). We propose a probabilistic method that quantifies representativeness by comparing the statistical distribution of features encoded by the scenario suites with the corresponding distribution of features representing the TOD, acknowledging that the true TOD distribution is unknown, as it can only be inferred from limited data.
  We apply an imprecise Bayesian method to handle limited data and uncertain priors. The imprecise Bayesian formulation produces interval-valued, uncertainty-aware estimates of representativeness, rather than a single value. We present a numerical example comparing the distributions of the scenario suite and the inferred TOD across operational categories-weather, road type, time of day, etc., under dependencies and prior uncertainty. We estimate representativeness locally (between categories) and globally as an interval.

</details>


### [15] [Beyond GeneGPT: A Multi-Agent Architecture with Open-Source LLMs for Enhanced Genomic Question Answering](https://arxiv.org/abs/2511.15061)
*Haodong Chen,Guido Zuccon,Teerapong Leelanupab*

Main category: cs.AI

TL;DR: OpenBioLLM是一个开源的多智能体框架，用于基因组问答，通过模块化设计在保持性能的同时显著提升效率。


<details>
  <summary>Details</summary>
Motivation: 解决GeneGPT依赖专有模型带来的可扩展性、运营成本、数据隐私和泛化性问题，探索开源模型在基因组问答中的应用潜力。

Method: 采用模块化多智能体框架，引入专门用于工具路由、查询生成和响应验证的智能体，实现协调推理和基于角色的任务执行。

Result: 在90%以上的基准任务中匹配或超越GeneGPT，在Gene-Turing和GeneHop上分别获得0.849和0.830的平均分数，延迟降低40-50%。

Conclusion: 开源多智能体系统在基因组问答中具有巨大潜力，能够在保持性能的同时显著提升效率。

Abstract: Genomic question answering often requires complex reasoning and integration across diverse biomedical sources. GeneGPT addressed this challenge by combining domain-specific APIs with OpenAI's code-davinci-002 large language model to enable natural language interaction with genomic databases. However, its reliance on a proprietary model limits scalability, increases operational costs, and raises concerns about data privacy and generalization.
  In this work, we revisit and reproduce GeneGPT in a pilot study using open source models, including Llama 3.1, Qwen2.5, and Qwen2.5 Coder, within a monolithic architecture; this allows us to identify the limitations of this approach. Building on this foundation, we then develop OpenBioLLM, a modular multi-agent framework that extends GeneGPT by introducing agent specialization for tool routing, query generation, and response validation. This enables coordinated reasoning and role-based task execution.
  OpenBioLLM matches or outperforms GeneGPT on over 90% of the benchmark tasks, achieving average scores of 0.849 on Gene-Turing and 0.830 on GeneHop, while using smaller open-source models without additional fine-tuning or tool-specific pretraining. OpenBioLLM's modular multi-agent design reduces latency by 40-50% across benchmark tasks, significantly improving efficiency without compromising model capability. The results of our comprehensive evaluation highlight the potential of open-source multi-agent systems for genomic question answering. Code and resources are available at https://github.com/ielab/OpenBioLLM.

</details>


### [16] [SafeRBench: A Comprehensive Benchmark for Safety Assessment in Large Reasoning Models](https://arxiv.org/abs/2511.15169)
*Xin Gao,Shaohan Yu,Zerui Chen,Yueming Lyu,Weichen Yu,Guanghao Li,Jiyao Liu,Jianxiong Gao,Jian Liang,Ziwei Liu,Chenyang Si*

Main category: cs.AI

TL;DR: SafeRBench是首个端到端评估大型推理模型安全性的基准，通过输入特征化、细粒度输出分析和人工安全对齐三个维度，全面评估从输入、中间推理到最终输出的安全风险。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型通过显式思维链提高了答案质量，但这也引入了新的安全风险：有害内容可能在推理过程中被微妙注入、逐渐显现或被误导性理由所合理化。现有的安全评估主要关注输出级判断，很少捕捉推理过程中的动态风险。

Method: 1) 输入特征化：将风险类别和级别纳入输入设计，明确考虑受影响群体和严重程度；2) 细粒度输出分析：引入微思维分块机制将长推理轨迹分割成语义连贯单元；3) 人工安全对齐：基于专门设计的人工标注验证LLM评估结果。

Result: 对19个大型推理模型的评估表明，SafeRBench能够实现详细的多维安全评估，从多个角度提供关于风险和保护机制的见解。

Conclusion: SafeRBench填补了现有安全评估的空白，为大型推理模型提供了全面的端到端安全评估框架，能够有效识别推理过程中的动态安全风险。

Abstract: Large Reasoning Models (LRMs) improve answer quality through explicit chain-of-thought, yet this very capability introduces new safety risks: harmful content can be subtly injected, surface gradually, or be justified by misleading rationales within the reasoning trace. Existing safety evaluations, however, primarily focus on output-level judgments and rarely capture these dynamic risks along the reasoning process. In this paper, we present SafeRBench, the first benchmark that assesses LRM safety end-to-end -- from inputs and intermediate reasoning to final outputs. (1) Input Characterization: We pioneer the incorporation of risk categories and levels into input design, explicitly accounting for affected groups and severity, and thereby establish a balanced prompt suite reflecting diverse harm gradients. (2) Fine-Grained Output Analysis: We introduce a micro-thought chunking mechanism to segment long reasoning traces into semantically coherent units, enabling fine-grained evaluation across ten safety dimensions. (3) Human Safety Alignment: We validate LLM-based evaluations against human annotations specifically designed to capture safety judgments. Evaluations on 19 LRMs demonstrate that SafeRBench enables detailed, multidimensional safety assessment, offering insights into risks and protective mechanisms from multiple perspectives.

</details>


### [17] [As If We've Met Before: LLMs Exhibit Certainty in Recognizing Seen Files](https://arxiv.org/abs/2511.15192)
*Haodong Li,Jingqi Zhang,Xiao Cheng,Peihua Mai,Haoyu Wang,Yang Pan*

Main category: cs.AI

TL;DR: COPYCHECK是一个利用不确定性信号检测LLM训练数据中版权内容的框架，将LLM的过度自信转化为优势，通过不确定性模式区分训练数据和非训练数据，无需经验阈值即可实现高精度检测。


<details>
  <summary>Details</summary>
Motivation: LLM在大量数据集上训练时可能包含受版权保护的内容，现有成员推断攻击方法因LLM过度自信、缺乏真实训练数据和依赖经验阈值而面临限制。

Method: 采用两阶段策略：(1)将文件分割为小片段以减少对大规模训练数据的依赖；(2)基于不确定性的无监督聚类，无需经验调优阈值。

Result: 在LLaMA 7b和LLaMA2 7b上分别达到90.1%和91.6%的平均平衡准确率，相比SOTA基线相对提升超过90%，最高达93.8%平衡准确率，在GPT-J 6B上保持强泛化性。

Conclusion: 这是首次将不确定性应用于LLM版权检测的工作，为训练数据透明度提供了实用工具。

Abstract: The remarkable language ability of Large Language Models (LLMs) stems from extensive training on vast datasets, often including copyrighted material, which raises serious concerns about unauthorized use. While Membership Inference Attacks (MIAs) offer potential solutions for detecting such violations, existing approaches face critical limitations and challenges due to LLMs' inherent overconfidence, limited access to ground truth training data, and reliance on empirically determined thresholds.
  We present COPYCHECK, a novel framework that leverages uncertainty signals to detect whether copyrighted content was used in LLM training sets. Our method turns LLM overconfidence from a limitation into an asset by capturing uncertainty patterns that reliably distinguish between ``seen" (training data) and ``unseen" (non-training data) content. COPYCHECK further implements a two-fold strategy: (1) strategic segmentation of files into smaller snippets to reduce dependence on large-scale training data, and (2) uncertainty-guided unsupervised clustering to eliminate the need for empirically tuned thresholds. Experiment results show that COPYCHECK achieves an average balanced accuracy of 90.1% on LLaMA 7b and 91.6% on LLaMA2 7b in detecting seen files. Compared to the SOTA baseline, COPYCHECK achieves over 90% relative improvement, reaching up to 93.8\% balanced accuracy. It further exhibits strong generalizability across architectures, maintaining high performance on GPT-J 6B. This work presents the first application of uncertainty for copyright detection in LLMs, offering practical tools for training data transparency.

</details>


### [18] [Efficiency Will Not Lead to Sustainable Reasoning AI](https://arxiv.org/abs/2511.15259)
*Philipp Wiesner,Daniel W. O'Neill,Francesca Larosa,Odej Kao*

Main category: cs.AI

TL;DR: 该论文认为，随着AI研究转向复杂问题解决和多步推理，仅靠效率提升无法实现可持续的推理AI，需要将明确限制嵌入系统优化和治理中。


<details>
  <summary>Details</summary>
Motivation: AI研究正从模式识别转向复杂问题解决，推理AI缺乏需求饱和点，性能随计算投入指数级增长，而效率改进正接近物理极限，需要新的可持续性方法。

Method: 分析推理AI的计算扩展特性，讨论效率改进的物理限制，提出在系统优化和治理中嵌入明确限制的研究和政策方向。

Result: 识别出推理AI缺乏自然饱和点，性能与计算投入呈指数关系，效率提升无法单独解决可持续性问题。

Conclusion: 仅靠效率无法实现可持续的推理AI，需要在优化和治理中嵌入明确限制，推动相关研究和政策制定。

Abstract: AI research is increasingly moving toward complex problem solving, where models are optimized not only for pattern recognition but for multi-step reasoning. Historically, computing's global energy footprint has been stabilized by sustained efficiency gains and natural saturation thresholds in demand. But as efficiency improvements are approaching physical limits, emerging reasoning AI lacks comparable saturation points: performance is no longer limited by the amount of available training data but continues to scale with exponential compute investments in both training and inference. This paper argues that efficiency alone will not lead to sustainable reasoning AI and discusses research and policy directions to embed explicit limits into the optimization and governance of such systems.

</details>


### [19] [Realist and Pluralist Conceptions of Intelligence and Their Implications on AI Research](https://arxiv.org/abs/2511.15282)
*Ninell Oldenburg,Ruchira Dhar,Anders Søgaard*

Main category: cs.AI

TL;DR: 本文分析了AI研究中两种对立的智力观：智力现实主义认为智力是单一普适能力，智力多元主义认为智力是多样化的情境依赖能力。这两种观点在方法论、解释和风险评估方面产生根本不同的研究路径。


<details>
  <summary>Details</summary>
Motivation: 当前AI研究中对智力本质的隐含假设影响了研究方向和证据解释，作者希望通过明确这些假设来澄清AI研究中的分歧。

Method: 通过分析当前AI研究中的辩论，揭示智力现实主义和智力多元主义这两种隐含观点如何塑造实证证据的解释。

Result: 发现这两种智力观在三个领域产生根本不同的研究路径：方法论上影响模型选择、基准设计和实验验证；解释上导致对相同现象的矛盾解读；风险评估上产生截然不同的威胁评估和解决方案。

Conclusion: 明确这些基本假设有助于更清晰地理解AI研究中的分歧，促进更富有成效的讨论。

Abstract: In this paper, we argue that current AI research operates on a spectrum between two different underlying conceptions of intelligence: Intelligence Realism, which holds that intelligence represents a single, universal capacity measurable across all systems, and Intelligence Pluralism, which views intelligence as diverse, context-dependent capacities that cannot be reduced to a single universal measure. Through an analysis of current debates in AI research, we demonstrate how the conceptions remain largely implicit yet fundamentally shape how empirical evidence gets interpreted across a wide range of areas. These underlying views generate fundamentally different research approaches across three areas. Methodologically, they produce different approaches to model selection, benchmark design, and experimental validation. Interpretively, they lead to contradictory readings of the same empirical phenomena, from capability emergence to system limitations. Regarding AI risk, they generate categorically different assessments: realists view superintelligence as the primary risk and search for unified alignment solutions, while pluralists see diverse threats across different domains requiring context-specific solutions. We argue that making explicit these underlying assumptions can contribute to a clearer understanding of disagreements in AI research.

</details>


### [20] [IPR-1: Interactive Physical Reasoner](https://arxiv.org/abs/2511.15407)
*Mingyu Zhang,Lifeng Zhuo,Tianxi Tan,Guocan Xie,Xian Nie,Yan Li,Renjie Zhao,Zizhu He,Ziyu Wang,Jiting Cai,Yong-Lu Li*

Main category: cs.AI

TL;DR: 本文研究了智能体能否通过交互学习获得类人推理能力，提出了IPR（交互物理推理器）模型，使用世界模型推演来评估和强化VLM策略，并在1000+游戏中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 研究智能体是否能够像人类一样通过观察、环境交互来学习物理和因果关系，并随着经验积累不断改进推理能力。

Method: 提出IPR模型，结合世界模型推演和VLM策略强化，引入PhysCode物理中心动作编码来对齐语义意图与动力学，在1000+异构游戏中训练。

Result: IPR在三个类人推理级别（生存、好奇心、效用）上表现稳健，整体性能与GPT-5相当，在好奇心方面超越GPT-5，且性能随训练游戏和交互步骤增加而提升，能零样本迁移到未见游戏。

Conclusion: 物理中心的交互是持续改进物理推理能力的有效路径，模型能够通过更多交互经验不断提升推理性能。

Abstract: Humans learn by observing, interacting with environments, and internalizing physics and causality. Here, we aim to ask whether an agent can similarly acquire human-like reasoning from interaction and keep improving with more experience. We study this in a Game-to-Unseen (G2U) setting, curating 1,000+ heterogeneous games with diverse physical and causal mechanisms, and evaluate at three human-like levels: Survival, Curiosity, Utility, from primitive intuition to goal-driven reasoning. Our analysis reveals complementary failures: VLM/VLA agents reason but lack look-ahead in interactive settings, while world models imagine but imitate visual patterns rather than analyze physics and causality. We therefore propose IPR (Interactive Physical Reasoner), using world-model rollouts to score and reinforce a VLM's policy, and introduce PhysCode, a physics-centric action code aligning semantic intent with dynamics to provide a shared action space for prediction and reasoning. Pretrained on 1,000+ games, our IPR performs robustly on three levels, matches GPT-5 overall, and surpasses it on Curiosity. We find that performance improves with more training games and interaction steps, and that the model also zero-shot transfers to unseen games. These results support physics-centric interaction as a path to steadily improving physical reasoning.

</details>


### [21] [Know Your Intent: An Autonomous Multi-Perspective LLM Agent Framework for DeFi User Transaction Intent Mining](https://arxiv.org/abs/2511.15456)
*Qian'ang Mao,Yuxuan Zhang,Jiaman Chen,Wenjun Zhou,Jiaqi Yan*

Main category: cs.AI

TL;DR: 提出了TIM框架，利用DeFi意图分类法和多智能体LLM系统来推断用户交易意图，通过动态协调领域专家处理多模态链上/链下数据，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: DeFi交易中用户意图理解至关重要但具有挑战性，现有方法缺乏深度语义洞察，需要解决复杂智能合约交互、多因素影响和模糊日志的问题。

Method: TIM框架包含DeFi意图分类法、元级规划器动态协调领域专家、问题求解器处理多模态数据、认知评估器减轻LLM幻觉并确保可验证性。

Result: 实验表明TIM显著优于机器学习模型、单一LLM和单一智能体基线，能够分析意图推断中的核心挑战。

Conclusion: 该工作有助于更可靠地理解DeFi中用户动机，为复杂区块链活动提供情境感知解释。

Abstract: As Decentralized Finance (DeFi) develops, understanding user intent behind DeFi transactions is crucial yet challenging due to complex smart contract interactions, multifaceted on-/off-chain factors, and opaque hex logs. Existing methods lack deep semantic insight. To address this, we propose the Transaction Intent Mining (TIM) framework. TIM leverages a DeFi intent taxonomy built on grounded theory and a multi-agent Large Language Model (LLM) system to robustly infer user intents. A Meta-Level Planner dynamically coordinates domain experts to decompose multiple perspective-specific intent analyses into solvable subtasks. Question Solvers handle the tasks with multi-modal on/off-chain data. While a Cognitive Evaluator mitigates LLM hallucinations and ensures verifiability. Experiments show that TIM significantly outperforms machine learning models, single LLMs, and single Agent baselines. We also analyze core challenges in intent inference. This work helps provide a more reliable understanding of user motivations in DeFi, offering context-aware explanations for complex blockchain activity.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [22] [Hybrid Quantum-Classical Machine Learning with PennyLane: A Comprehensive Guide for Computational Research](https://arxiv.org/abs/2511.14786)
*Sidney Shapiro*

Main category: cs.SE

TL;DR: 本文介绍了PennyLane作为一个连接量子电路和经典机器学习的Python框架，展示了其在量子机器学习、优化和量子化学应用中的多功能性。


<details>
  <summary>Details</summary>
Motivation: 结合量子计算潜在优势与经典优化技术，为研究人员提供一个构建、优化和部署变分量子算法的工具，推动混合量子-经典机器学习的发展。

Method: 通过具体Python示例展示PennyLane的功能，包括量子核方法、变分量子本征求解器、投资组合优化，以及与PyTorch、TensorFlow、JAX等经典ML框架的集成。

Result: PennyLane能够高效构建量子电路、自动微分和混合优化工作流，成为量子增强数据科学的方法论构建模块。

Conclusion: PennyLane作为连接基础量子计算概念和应用机器学习实践的桥梁，将成为基于Python研究的混合量子-经典工作流程的默认引用工具。

Abstract: Hybrid quantum-classical machine learning represents a frontier in computational research, combining the potential advantages of quantum computing with established classical optimization techniques. PennyLane provides a Python framework that seamlessly bridges quantum circuits and classical machine learning, enabling researchers to build, optimize, and deploy variational quantum algorithms. This paper introduces PennyLane as a versatile tool for quantum machine learning, optimization, and quantum chemistry applications. We demonstrate use cases including quantum kernel methods, variational quantum eigensolvers, portfolio optimization, and integration with classical ML frameworks such as PyTorch, TensorFlow, and JAX. Through concrete Python examples with widely used libraries such as scikit-learn, pandas, and matplotlib, we show how PennyLane facilitates efficient quantum circuit construction, automatic differentiation, and hybrid optimization workflows. By situating PennyLane within the broader context of quantum computing and machine learning, we highlight its role as a methodological building block for quantum-enhanced data science. Our goal is to provide researchers and practitioners with a concise reference that bridges foundational quantum computing concepts and applied machine learning practice, making PennyLane a default citation for hybrid quantum-classical workflows in Python-based research.

</details>


### [23] [Enabling Predictive Maintenance in District Heating Substations: A Labelled Dataset and Fault Detection Evaluation Framework based on Service Data](https://arxiv.org/abs/2511.14791)
*Cyriana M. A. Roelofs,Edison Guevara Bastidas,Thomas Hugo,Stefan Faulstich,Anna Cadenbach*

Main category: cs.SE

TL;DR: 提出了一个用于区域供热站早期故障检测的开源框架，包括经过服务报告验证的公共数据集、基于准确性、可靠性和早期性的评估方法，以及使用EnergyFaultDetector开源Python框架实现的基准结果。


<details>
  <summary>Details</summary>
Motivation: 区域供热站早期故障检测对于降低回水温度和提高效率至关重要，但该领域进展受到公开标记数据集有限的阻碍。

Method: 开发了包含93个供热站运行数据时间序列的数据集，使用EnergyFaultDetector框架，通过准确性、事件F分数和早期性三个指标进行评估，并支持使用ARCANA进行根本原因分析。

Result: 模型实现了高正常行为准确性（0.98）和事件F分数（0.83），在客户报告问题前检测到60%的故障，平均提前时间为3.9天。

Conclusion: 整合开放数据集、指标、开源代码和基准建立了一个可重现的故障中心基准，具有操作意义的评估，为区域供热站早期故障检测和诊断方法提供了一致的比较和开发基础。

Abstract: Early detection of faults in district heating substations is imperative to reduce return temperatures and enhance efficiency. However, progress in this domain has been hindered by the limited availability of public, labelled datasets. We present an open source framework combining a service report validated public dataset, an evaluation method based on Accuracy, Reliability, and Earliness, and baseline results implemented with EnergyFaultDetector, an open source Python framework.
  The dataset contains time series of operational data from 93 substations across two manufacturers, annotated with a list of disturbances due to faults and maintenance actions, a set of normal-event examples and detailed fault metadata. We evaluate the EnergyFaultDetector using three metrics: Accuracy for recognising normal behaviour, an eventwise F Score for reliable fault detection with few false alarms, and Earliness for early detection. The framework also supports root cause analysis using ARCANA. We demonstrate three use cases to assist operators in interpreting anomalies and identifying underlying faults. The models achieve high normal-behaviour accuracy (0.98) and eventwise F-score (beta=0.5) of 0.83, detecting 60% of the faults in the dataset before the customer reports a problem, with an average lead time of 3.9 days.
  Integrating an open dataset, metrics, open source code, and baselines establishes a reproducible, fault centric benchmark with operationally meaningful evaluation, enabling consistent comparison and development of early fault detection and diagnosis methods for district heating substations.

</details>


### [24] [irace-evo: Automatic Algorithm Configuration Extended With LLM-Based Code Evolution](https://arxiv.org/abs/2511.14794)
*Camilo Chacón Sartori,Christian Blum*

Main category: cs.SE

TL;DR: 本文介绍了irace-evo，一个将irace自动参数配置工具与基于大语言模型的代码进化相结合的框架，能够同时探索参数和代码空间，并在可变大小装箱问题上验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 传统的自动算法配置工具只能调优参数值而无法修改算法代码，限制了算法优化的潜力。本文旨在通过结合大语言模型实现代码进化，突破这一限制。

Method: 提出irace-evo框架，支持多语言（C++、Python等），采用渐进式上下文管理减少token消耗，并应用Always-From-Original原则确保代码进化的稳健性和可控性。

Result: 在可变大小装箱问题的CMSA元启发式算法上测试，irace-evo发现了优于现有最优CMSA实现的新算法变体，且总使用成本低于2欧元。

Conclusion: 将自动配置与LLM驱动的代码进化相结合，为启发式设计和元启发式优化提供了一条强大且成本效益高的新途径。

Abstract: Automatic algorithm configuration tools such as irace efficiently tune parameter values but leave algorithmic code unchanged. This paper introduces a first version of irace-evo, an extension of irace that integrates code evolution through large language models (LLMs) to jointly explore parameter and code spaces. The proposed framework enables multi-language support (e.g., C++, Python), reduces token consumption via progressive context management, and employs the Always-From-Original principle to ensure robust and controlled code evolution. We evaluate irace-evo on the Construct, Merge, Solve & Adapt (CMSA) metaheuristic for the Variable-Sized Bin Packing Problem (VSBPP). Experimental results show that irace-evo can discover new algorithm variants that outperform the state-of-the-art CMSA implementation while maintaining low computational and monetary costs. Notably, irace-evo generates competitive algorithmic improvements using lightweight models (e.g., Claude Haiku 3.5) with a total usage cost under 2 euros. These results demonstrate that coupling automatic configuration with LLM-driven code evolution provides a powerful, cost-efficient avenue for advancing heuristic design and metaheuristic optimization.

</details>


### [25] [Evaluating Generative AI for CS1 Code Grading: Direct vs Reverse Methods](https://arxiv.org/abs/2511.14798)
*Ahmad Memon,Abdallah Mohamed*

Main category: cs.SE

TL;DR: 本文比较了两种基于AI的编程作业评分方法：直接评分法和反向评分法，评估它们在编程课程自动评分中的效果和一致性。


<details>
  <summary>Details</summary>
Motivation: 手动评分编程作业耗时且存在不一致性，而传统的单元测试只能提供二元通过/失败结果。大型语言模型的发展为自动化、可扩展且更客观的评分提供了可能。

Method: 比较两种AI评分技术：直接评分法（AI直接应用评分标准）和反向评分法（AI先修复错误，然后根据修复的性质和数量推断分数）。两种方法都在原始评分标准和十倍扩展评分标准上进行评估，并使用合成学生代码测试一致性。

Result: 初步发现表明，直接方法更快更直接，而反向技术通过关注修正努力通常提供更细粒度的评估。两种方法都需要仔细的提示工程，特别是在分配部分学分和处理逻辑错误方面。

Conclusion: 讨论了每种方法的优缺点、提示设计的实际考虑，以及未来混合人机评分系统的方向，旨在提高计算机科学课程评分的一致性、效率和公平性。

Abstract: Manual grading of programming assignments in introductory computer science courses can be time-consuming and prone to inconsistencies. While unit testing is commonly used for automatic evaluation, it typically follows a binary pass/fail model and does not give partial marks. Recent advances in large language models (LLMs) offer the potential for automated, scalable, and more objective grading.
  This paper compares two AI-based grading techniques: \textit{Direct}, where the AI model applies a rubric directly to student code, and \textit{Reverse} (a newly proposed approach), where the AI first fixes errors, then deduces a grade based on the nature and number of fixes. Each method was evaluated on both the instructor's original grading scale and a tenfold expanded scale to assess the impact of range on AI grading accuracy. To assess their effectiveness, AI-assigned scores were evaluated against human tutor evaluations on a range of coding problems and error types.
  Initial findings suggest that while the Direct approach is faster and straightforward, the Reverse technique often provides a more fine-grained assessment by focusing on correction effort. Both methods require careful prompt engineering, particularly for allocating partial credit and handling logic errors. To further test consistency, we also used synthetic student code generated using Gemini Flash 2.0, which allowed us to evaluate AI graders on a wider range of controlled error types and difficulty levels. We discuss the strengths and limitations of each approach, practical considerations for prompt design, and future directions for hybrid human-AI grading systems that aim to improve consistency, efficiency, and fairness in CS courses.

</details>


### [26] [Scalable and Efficient Large-Scale Log Analysis with LLMs: An IT Software Support Case Study](https://arxiv.org/abs/2511.14803)
*Pranjal Gupta,Karan Bhukar,Harshit Kumar,Seema Nagar,Prateeti Mohapatra,Debanjana Kar*

Main category: cs.SE

TL;DR: 提出基于大语言模型的日志分析工具，用于IT软件支持中的自动化日志处理和问题诊断，通过CPU高效运行LLM处理海量日志，节省时间和成本。


<details>
  <summary>Details</summary>
Motivation: IT环境中生成的日志量巨大，手动检查不切实际，需要自动化日志分析来监控系统健康和检测问题。

Method: 开发基于大语言模型的日志分析工具，采用在CPU上高效运行LLM的新方法处理大量日志数据，生成自动化洞察和摘要。

Result: 自2024年3月部署以来，该工具已在70个软件产品中扩展使用，处理了2000多个工单，节省了300+人工小时，每月估计节省15,444美元人力成本。

Conclusion: 基于LLM的日志分析工具能够有效处理海量日志数据，显著提高IT软件支持的效率并大幅降低成本。

Abstract: IT environments typically have logging mechanisms to monitor system health and detect issues. However, the huge volume of generated logs makes manual inspection impractical, highlighting the importance of automated log analysis in IT Software Support. In this paper, we propose a log analytics tool that leverages Large Language Models (LLMs) for log data processing and issue diagnosis, enabling the generation of automated insights and summaries. We further present a novel approach for efficiently running LLMs on CPUs to process massive log volumes in minimal time without compromising output quality. We share the insights and lessons learned from deployment of the tool - in production since March 2024 - scaled across 70 software products, processing over 2000 tickets for issue diagnosis, achieving a time savings of 300+ man hours and an estimated $15,444 per month in manpower costs compared to the traditional log analysis practices.

</details>


### [27] [Automatic Pipeline Provisioning](https://arxiv.org/abs/2511.14825)
*Alexandre-Xavier Labonté-Lamoureux,Simon Boyer*

Main category: cs.SE

TL;DR: 探索自动流水线配置的好处及其应用方法，主要关注CI流水线


<details>
  <summary>Details</summary>
Motivation: 研究自动流水线配置的优势和实际应用价值

Method: 分析自动流水线配置过程，重点关注CI流水线的部署

Result: 识别了自动流水线配置的潜在好处和应用场景

Conclusion: 自动流水线配置能够快速部署软件工程项目的流水线，对CI和CD流水线都有类似效果

Abstract: The goal of this paper is to explore the benefits of automatic pipeline provisioning and identify how it can be applied. Automatic pipeline provisioning can be defined as a process of quickly deploying a pipeline for a software engineering project. This research will focus on CI pipelines, although the outcomes of this approach on CD pipelines will likely be similar.

</details>


### [28] [MermaidSeqBench: An Evaluation Benchmark for LLM-to-Mermaid Sequence Diagram Generation](https://arxiv.org/abs/2511.14967)
*Basel Shbita,Farhan Ahmed,Chad DeLuca*

Main category: cs.SE

TL;DR: 提出了MermaidSeqBench基准测试，用于评估LLM从文本提示生成Mermaid序列图的能力，包含132个样本，采用混合方法扩展，使用LLM作为评判模型进行多维度评估。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏系统评估LLM生成结构化图表能力的基准测试，特别是在软件工程中的序列图生成任务。

Method: 采用混合方法构建基准：人工标注、上下文LLM提示和基于规则的变体生成，使用LLM作为评判模型评估语法正确性、激活处理、错误处理等指标。

Result: 对多个先进LLM的评估显示模型之间存在显著能力差距，基准测试证明了其有效性和灵活性。

Conclusion: 该基准为结构化图表生成研究提供了基础，有助于开发更严谨、细粒度的评估方法。

Abstract: Large language models (LLMs) have demonstrated excellent capabilities in generating structured diagrams from natural language descriptions. In particular, they have shown great promise in generating sequence diagrams for software engineering, typically represented in a text-based syntax such as Mermaid. However, systematic evaluations in this space remain underdeveloped as there is a lack of existing benchmarks to assess the LLM's correctness in this task. To address this shortcoming, we introduce MermaidSeqBench, a human-verified and LLM-synthetically-extended benchmark for assessing an LLM's capabilities in generating Mermaid sequence diagrams from textual prompts. The benchmark consists of a core set of 132 samples, starting from a small set of manually crafted and verified flows. These were expanded via a hybrid methodology combining human annotation, in-context LLM prompting, and rule-based variation generation. Our benchmark uses an LLM-as-a-judge model to assess Mermaid sequence diagram generation across fine-grained metrics, including syntax correctness, activation handling, error handling, and practical usability. We perform initial evaluations on numerous state-of-the-art LLMs and utilize multiple LLM judge models to demonstrate the effectiveness and flexibility of our benchmark. Our results reveal significant capability gaps across models and evaluation modes. Our proposed benchmark provides a foundation for advancing research in structured diagram generation and for developing more rigorous, fine-grained evaluation methodologies.

</details>


### [29] [FRIENDS GUI: A graphical user interface for data collection and visualization of vaping behavior from a passive vaping monitor](https://arxiv.org/abs/2511.15007)
*Shehan I Pranto,Brett Fassler,Md Rafi Islam,Ashley Schenkel,Larry W Hawk,Edward Sazonov*

Main category: cs.SE

TL;DR: 开发了FRIENDS GUI，这是一个基于Python的开源工具，用于提取、解码和可视化FRIENDS设备收集的24小时电子烟使用数据，提高了数据的可访问性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 理解电子烟使用行为（包括抽吸持续时间、抽吸间隔和每次会话的抽吸次数）对于评估电子烟使用、有毒物质暴露和制定监管决策至关重要。需要提高FRIENDS设备收集数据的可访问性和可解释性。

Method: 开发了基于Python的开源FRIENDS GUI工具，能够提取、解码和可视化FRIENDS设备记录的24小时抽吸和触摸事件数据。通过24小时实验数据进行验证。

Result: 验证确认了时间戳转换的准确性、事件解码的可靠性以及行为可视化的有效性。该软件已在GitHub上免费提供供公众使用。

Conclusion: FRIENDS GUI成功提高了FRIENDS设备收集数据的可访问性和可解释性，为电子烟使用行为研究提供了有效的工具支持。

Abstract: Understanding puffing topography (PT), which includes puff duration, intra puff interval, and puff count per session, is critical for evaluating Electronic Nicotine Delivery Systems (ENDS) use, toxicant exposure, and informing regulatory decisions. We developed FRIENDS (Flexible Robust Instrumentation of ENDS), an open-source device that records puffing and touch events of ENDS by attaching to it. This paper introduces the FRIENDS GUI that improves accessibility and interpretability of data collected by FRIENDS. The GUI is a Python-based open-source tool that extracts, decodes, and visualizes 24-hour puffing data from the FRIENDS device. Validation using 24-hour experimental data confirmed accurate timestamp conversion, reliable event decoding, and effective behavioral visualization. The software is freely available on GitHub for public use.

</details>


### [30] [Finetuning LLMs for Automatic Form Interaction on Web-Browser in Selenium Testing Framework](https://arxiv.org/abs/2511.15168)
*Nguyen-Khang Le,Nguyen Hiep,Minh Nguyen,Son Luu,Trung Vo,Quan Bui,Nomura Shoshin,Le-Minh Nguyen*

Main category: cs.SE

TL;DR: 本文提出了一种训练大语言模型生成高质量Selenium测试用例的新方法，专门针对表单交互测试，在语法正确性、脚本可执行性和输入字段覆盖率方面显著优于GPT-4o等基线模型。


<details>
  <summary>Details</summary>
Motivation: 自动化Web应用测试是现代软件开发的关键组成部分，但大语言模型在表单交互生成任务方面研究不足，缺乏公开的基准数据集来系统评估LLMs在此任务上的表现。

Method: 提出了一种训练LLMs生成Selenium测试用例的新方法，构建了合成和人工标注的数据集用于训练和评估，涵盖了多样化的真实世界表单和测试场景。

Result: 实证研究表明，该方法在所有评估指标上显著优于包括GPT-4o在内的强基线模型，定义了语法正确性、脚本可执行性和输入字段覆盖率的清晰度量标准。

Conclusion: 这项工作为基于LLM的Web测试未来研究奠定了基础，并为该领域的持续进展提供了资源支持。

Abstract: Automated web application testing is a critical component of modern software development, with frameworks like Selenium widely adopted for validating functionality through browser automation. Among the essential aspects of such testing is the ability to interact with and validate web forms, a task that requires syntactically correct, executable scripts with high coverage of input fields. Despite its importance, this task remains underexplored in the context of large language models (LLMs), and no public benchmark or dataset exists to evaluate LLMs on form interaction generation systematically. This paper introduces a novel method for training LLMs to generate high-quality test cases in Selenium, specifically targeting form interaction testing. We curate both synthetic and human-annotated datasets for training and evaluation, covering diverse real-world forms and testing scenarios. We define clear metrics for syntax correctness, script executability, and input field coverage. Our empirical study demonstrates that our approach significantly outperforms strong baselines, including GPT-4o and other popular LLMs, across all evaluation metrics. Our work lays the groundwork for future research on LLM-based web testing and provides resources to support ongoing progress in this area.

</details>


### [31] [From Code Smells to Best Practices: Tackling Resource Leaks in PyTorch, TensorFlow, and Keras](https://arxiv.org/abs/2511.15229)
*Bashar Abdallah,Martyna E. Wojciechowska,Gustavo Santos,Edmand Yu,Maxime Lamothe,Alain Abran,Mohammad Hamdaqa*

Main category: cs.SE

TL;DR: 本研究系统识别了导致ML应用中资源泄漏的代码异味，通过分析PyTorch、TensorFlow和Keras的开发者讨论和实际代码，发现了30个PyTorch相关异味和16个TensorFlow/Keras异味，并提出了50个最佳实践来减少资源泄漏。


<details>
  <summary>Details</summary>
Motivation: 现有ML研究主要关注模型性能指标，对长期可持续性和资源效率关注有限。本研究旨在填补这一空白，确保ML应用部署的稳健性。

Method: 通过实证调查分析PyTorch、TensorFlow和Keras的开发者讨论和真实代码片段，采用三阶段验证过程，包括三位作者的独立分析和共识讨论。

Result: 识别出30个PyTorch相关异味和16个TensorFlow/Keras异味，按根本原因和框架特定特征分类，为每个异味推导出至少一个最佳实践。

Conclusion: 这是首个全面研究主要ML框架中导致资源泄漏的代码异味的研究，为开发人员提供了可操作的最佳实践，支持构建更高效和可持续的ML应用。

Abstract: Much of the existing ML research focuses on model performance metrics, leaving limited attention to the long-term sustainability and resource efficiency of ML applications. While high performance is essential, ensuring efficient resource management is equally critical for robust deployment. This study addresses this gap by systematically identifying code smells that lead to resource leaks in ML applications. We conducted an empirical investigation of developer discussions and real-world code snippets from PyTorch, TensorFlow, and Keras. The analysis identified 30 PyTorch-related smells and 16 TensorFlow/Keras smells linked to resource leaks. These smells were categorized in two ways: (1) based on their root causes, and (2) as general ML smells with framework-specific characteristics. For each smell, we derived at least one best practice, resulting in 50 recommended coding patterns aimed at reducing resource leakage and improving efficiency. To ensure the validity of our findings, we employed a three-phase validation process involving independent analysis by three authors followed by consensus discussions. This is the first comprehensive study to examine resource-leak-inducing code smells across major ML frameworks and to present actionable best practices for mitigating them. The contributions support developers in building more efficient and sustainable ML applications and offer a structured view of the underlying causes of resource leaks.

</details>


### [32] [M, Toolchain and Language for Reusable Model Compilation](https://arxiv.org/abs/2511.15257)
*Hiep Hong Trinh,Federico Ciccozzi,Abu Naser Masud,Marjan Sirjani,Mikael Sjödin*

Main category: cs.SE

TL;DR: 本文介绍了M工具链和建模语言，旨在支持复杂、并发和时间感知系统的建模与多目标编译，基于参与者模型并扩展离散事件调度语义。


<details>
  <summary>Details</summary>
Motivation: 开发复杂软件驱动系统时，需要从高级系统模型生成多个专用模型用于仿真、部署和形式验证等不同目的，但现有建模语言通常只针对单一目标设计，多目标编译难以实现。

Method: 提出基于参与者模型的文本化、语法驱动的M建模语言，扩展离散事件调度语义，提供系统实体建模、基于消息的交互以及时间或状态触发反应的构造。

Result: M能够从模型系统性地生成多样化目标产物，同时保持与原始模型的语义一致性，并可作为中间语言供其他建模语言锚定使用。

Conclusion: M工具链和建模语言为复杂系统的模型驱动工程提供了有效的多目标编译支持，解决了现有建模语言目标单一的问题。

Abstract: Complex software-driven systems often interleave distributed, concurrent computation processes with physical interactions with the environment. Developing these systems more efficiently and safely can be achieved by employing actionable, software-based models. From a high-level system model, engineers often need to derive multiple specialized models for different purposes, including simulation, deployment, and formal verification. Each of these target models usually rely on its own formalism, specification language, and execution platform. Traditionally, a compiler analyzes a program written in a programming language and generates executable code. In contrast, a model compiler processes a source model written in a modeling language and should ideally support the generation of multiple heterogeneous targets. However, most existing modeling languages are designed with a narrow focus, typically targeting only simulation or implementation. Multi-target compilation, when not considered during the language's early design, becomes significantly harder to achieve. In this paper, we introduce our initiative: a toolchain and modeling language called M, designed to support system modeling and multi-target compilation for model-driven engineering of complex, concurrent, and time-aware systems. M is a textual, grammar-driven language based on the actor model and extended with discrete-event scheduling semantics. It provides constructs for modeling system entities, message-based interactions, and time- or state-triggered reactions. From such models, M enables the systematic generation of diverse target artifacts while preserving semantic conformance to the original model. Moreover, M can serve as a middle language to which other modeling languages may anchor, thereby allowing them to benefit from its compilation framework.

</details>


### [33] [A Viable Paradigm of Software Automation: Iterative End-to-End Automated Software Development](https://arxiv.org/abs/2511.15293)
*Jia Li,Zhi Jin,Kechi Zhang,Huangzhao Zhang,Jiaru Qian,Tiankuo Zhao*

Main category: cs.SE

TL;DR: 本文提出AutoSW愿景，一种迭代式端到端自动化软件开发范式，通过分析-规划-实现-交付循环，将自然语言意图转化为可执行软件。


<details>
  <summary>Details</summary>
Motivation: 随着AI发展，现有研究将AI视为开发工具或助手，仍需大量人工参与。本文旨在探索AI系统作为人类伙伴参与整个软件开发生命周期的自动化范式。

Method: 提出AutoSW范式，采用分析-规划-实现-交付的迭代循环，AI系统作为一等参与者将自然语言意图转化为可执行软件，并构建轻量级原型验证。

Result: 通过代表性案例验证，AutoSW能够成功交付可执行软件，为真正的端到端自动化软件开发提供了可行方向。

Conclusion: AutoSW范式展示了AI系统作为人类伙伴参与全栈软件开发的潜力，为软件工程自动化提供了新的发展方向。

Abstract: Software development automation is a long-term goal in software engineering. With the development of artificial intelligence (AI), more and more researchers are exploring approaches to software automation. They view AI systems as tools or assistants in software development, still requiring significant human involvement. Another initiative is ``vibe coding'', where AI systems write and repeatedly revise most (or even all) of the code. We foresee these two development paths will converge towards the same destination: AI systems participate in throughout the software development lifecycle, expanding boundaries of full-stack software development. In this paper, we present a vision of an iterative end-to-end automated software development paradigm AutoSW. It operates in an analyze-plan-implement-deliver loop, where AI systems as human partners become first-class actors, translating human intentions expressed in natural language into executable software. We explore a lightweight prototype across the paradigm and initially execute various representative cases. The results indicate that AutoSW can successfully deliver executable software, providing a feasible direction for truly end-to-end automated software development.

</details>


### [34] [From Machine Learning Documentation to Requirements: Bridging Processes with Requirements Languages](https://arxiv.org/abs/2511.15340)
*Yi Peng,Hans-Martin Heyn,Jennifer Horkoff*

Main category: cs.SE

TL;DR: 该研究探讨了从机器学习文档（如ModelCards和DataSheets）中提取需求工程相关信息的可行性，并评估了三种需求工程表示方法（EARS、Rupp模板和Volere）将这些信息转化为结构化需求的效果。


<details>
  <summary>Details</summary>
Motivation: 在机器学习系统的软件工程过程中，集成和验证ML组件面临重大挑战。传统需求工程过程在指定ML组件需求（包括模型和数据）方面遇到新障碍，而ML文档作为需求工程相关信息来源尚未充分探索。

Method: 首先分析了20个公开可用的ModelCards和DataSheets中需求工程相关信息的数量和性质，然后评估了三种需求工程表示方法（EARS、Rupp模板和Volere）将这些知识转化为结构化需求的有效性。

Result: 研究表明这些文档包含大量潜在的需求工程相关信息，并证明存在将ML特定知识转化为结构化需求的途径，可以将ML文档纳入ML系统的软件工程过程。

Conclusion: ML文档（如ModelCards和DataSheets）可以作为需求工程的重要信息来源，通过适当的需求工程表示方法，能够有效地将ML特定知识转化为结构化需求，从而支持ML系统的软件工程过程。

Abstract: In software engineering processes for machine learning (ML)-enabled systems, integrating and verifying ML components is a major challenge. A prerequisite is the specification of ML component requirements, including models and data, an area where traditional requirements engineering (RE) processes face new obstacles. An underexplored source of RE-relevant information in this context is ML documentation such as ModelCards and DataSheets. However, it is uncertain to what extent RE-relevant information can be extracted from these documents. This study first investigates the amount and nature of RE-relevant information in 20 publicly available ModelCards and DataSheets. We show that these documents contain a significant amount of potentially RE-relevant information. Next, we evaluate how effectively three established RE representations (EARS, Rupp's template, and Volere) can structure this knowledge into requirements. Our results demonstrate that there is a pathway to transform ML-specific knowledge into structured requirements, incorporating ML documentation in software engineering processes for ML systems.

</details>


### [35] [MutDafny: A Mutation-Based Approach to Assess Dafny Specifications](https://arxiv.org/abs/2511.15403)
*Isabel Amaral,Alexandra Mendes,José Campos*

Main category: cs.SE

TL;DR: 本文提出了MutDafny工具，通过变异测试来检测Dafny形式化规范中的弱点。该工具使用32个变异算子，在794个真实Dafny程序上评估，发现了平均每241行代码就存在一个需要加强的规范弱点。


<details>
  <summary>Details</summary>
Motivation: 在验证感知编程语言如Dafny中，规范与实现一样容易出错。规范中的缺陷可能导致形式化验证的程序偏离预期行为，因此需要有效方法来提高规范可靠性。

Method: 采用变异测试方法，将故障（变异）引入代码，依赖形式化规范来检测这些变异。如果带有变异体的程序仍能验证通过，则表明规范存在弱点。从流行工具中分析适用的变异算子，并从GitHub上的Dafny项目错误修复提交中合成新的算子。

Result: 开发了包含32个变异算子的MutDafny工具，在794个真实Dafny程序数据集上评估了其有效性和效率。手动分析未检测到的变异体子集，识别出5个需要加强的真实世界规范弱点。

Conclusion: 变异测试是揭示Dafny形式化规范弱点的有效方法，MutDafny工具能够自动识别规范中的潜在问题，有助于提高形式化验证程序的可靠性。

Abstract: This paper explores the use of mutation testing to reveal weaknesses in formal specifications written in Dafny. In verification-aware programming languages, such as Dafny, despite their critical role, specifications are as prone to errors as implementations. Flaws in specs can result in formally verified programs that deviate from the intended behavior.
  We present MutDafny, a tool that increases the reliability of Dafny specifications by automatically signaling potential weaknesses. Using a mutation testing approach, we introduce faults (mutations) into the code and rely on formal specifications for detecting them. If a program with a mutant verifies, this may indicate a weakness in the specification. We extensively analyze mutation operators from popular tools, identifying the ones applicable to Dafny. In addition, we synthesize new operators tailored for Dafny from bugfix commits in publicly available Dafny projects on GitHub. Drawing from both, we equipped our tool with a total of 32 mutation operators. We evaluate MutDafny's effectiveness and efficiency in a dataset of 794 real-world Dafny programs and we manually analyze a subset of the resulting undetected mutants, identifying five weak real-world specifications (on average, one at every 241 lines of code) that would benefit from strengthening.

</details>


### [36] [EPSO: A Caching-Based Efficient Superoptimizer for BPF Bytecode](https://arxiv.org/abs/2511.15589)
*Qian Zhu,Yuxuan Liu,Ziyuan Zhu,Shangqing Liu,Lei Bu*

Main category: cs.SE

TL;DR: EPSO是一个基于缓存的eBPF超级优化器，通过离线超级优化发现重写规则并重用，在降低运行时开销的同时实现高质量优化。


<details>
  <summary>Details</summary>
Motivation: eBPF程序受限于内核验证器的安全约束和性能关键使用场景，现有编译器优化支持有限，手工优化规则设计困难且效果有限，传统超级优化计算成本高难以扩展。

Method: 提出EPSO方法，通过离线超级优化发现重写规则并缓存，运行时重用这些规则进行优化，平衡优化质量和计算开销。

Result: EPSO发现795条重写规则，相比Clang输出程序大小最多减少68.87%（平均24.37%），在所有基准测试中优于K2，在92.68%的测试中优于Merlin，程序运行时间平均减少6.60%。

Conclusion: EPSO通过缓存式超级优化方法有效解决了eBPF程序优化问题，在程序大小和运行性能方面都取得了显著提升。

Abstract: Extended Berkeley Packet Filter (eBPF) allows developers to extend Linux kernel functionality without modifying its source code. To ensure system safety, an in-kernel safety checker, the verifier, enforces strict safety constraints (for example, a limited program size) on eBPF programs loaded into the kernel. These constraints, combined with eBPF's performance-critical use cases, make effective optimization essential. However, existing compilers (such as Clang) offer limited optimization support, and many semantics-preserving transformations are rejected by the verifier, which makes handcrafted optimization rule design both challenging and limited in effectiveness. Superoptimization overcomes the limitations of rule-based methods by automatically discovering optimal transformations, but its high computational cost limits scalability. To address this, we propose EPSO, a caching-based superoptimizer that discovers rewrite rules via offline superoptimization and reuses them to achieve high-quality optimizations with minimal runtime overhead. We evaluate EPSO on benchmarks from the Linux kernel and several eBPF-based projects, including Cilium, Katran, hXDP, Sysdig, Tetragon, and Tracee. EPSO discovers 795 rewrite rules and achieves up to 68.87 percent (average 24.37 percent) reduction in program size compared to Clang's output, outperforming the state-of-the-art BPF optimizer K2 on all benchmarks and Merlin on 92.68 percent of them. Additionally, EPSO reduces program runtime by an average of 6.60 percent, improving throughput and lowering latency in network applications.

</details>


### [37] [Quantum-Guided Test Case Minimization for LLM-Based Code Generation](https://arxiv.org/abs/2511.15665)
*Huixiang Zhang,Mahzabeen Emu*

Main category: cs.SE

TL;DR: 提出了一个基于测试驱动开发（TDD）的框架，将代码规范转化为组合优化任务，使用量子退火器解决测试用例最小化问题，显著减少令牌消耗并提高代码质量。


<details>
  <summary>Details</summary>
Motivation: 精确控制大型语言模型生成高效简洁代码是软件工程中的核心挑战，需要解决代码规范到优化任务的转换问题。

Method: 首先提示LLM生成测试套件，然后将测试用例最小化问题建模为二次无约束二进制优化模型，兼容经典求解器和量子退火器。

Result: 量子退火解决核心测试用例最小化任务比模拟退火快16倍，端到端框架减少总令牌消耗36.5%，显著提高代码质量。

Conclusion: 展示了生成式AI与组合优化在软件工程中的强大协同作用，强调了精确模型制定的重要性。

Abstract: Precisely controlling Large Language Models (LLMs) to generate efficient and concise code is a central challenge in software engineering. We introduce a framework based on Test-Driven Development (TDD) that transforms code specification into a combinatorial optimization task. The framework first prompts an LLM to generate a test suite, then formulates the Test Case Minimization (TCM) problem as a Quadratic Unconstrained Binary Optimization (QUBO) model. This QUBO paradigm is compatible with both classical solvers and emerging hardware such as quantum annealers. Experimentally, quantum annealing solves the core TCM task 16 times faster than simulated annealing. This performance underpins our end-to-end framework, which reduces total token consumption by 36.5\% and significantly improves code quality. This work demonstrates a powerful synergy between generative AI and combinatorial optimization in software engineering, highlighting the critical importance of precise model formulation.

</details>
