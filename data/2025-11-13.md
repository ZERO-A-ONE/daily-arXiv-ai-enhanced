<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 23]
- [cs.CR](#cs.CR) [Total: 12]
- [cs.SE](#cs.SE) [Total: 10]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [Bridging Natural Language and ASP: A Hybrid Approach Using LLMs and AMR Parsing](https://arxiv.org/abs/2511.08715)
*Connar Hite,Sean Saud,Raef Taha,Nayim Rahman,Tanvir Atahary,Scott Douglass,Tarek Taha*

Main category: cs.AI

TL;DR: 本文提出了一种使用LLM和AMR图将无约束英语翻译成ASP程序的新方法，用于解决逻辑谜题，生成完整的ASP程序来求解组合逻辑问题。


<details>
  <summary>Details</summary>
Motivation: 随着越来越多不熟悉编程语言的人需要与代码交互，需要一种方法让非专业人士也能使用ASP解决组合问题。当前方法过度依赖LLM，需要更轻量级、可解释的系统。

Method: 使用LLM简化自然语言句子、识别关键词和生成简单事实，然后从简化语言解析AMR图，系统性地生成ASP约束，最小化LLM的作用。

Result: 系统成功创建了完整的ASP程序来解决组合逻辑问题，展示了在逻辑谜题示例中的能力。

Conclusion: 该方法是在创建轻量级、可解释的自然语言到复杂逻辑问题求解系统方面的重要第一步。

Abstract: Answer Set Programming (ASP) is a declarative programming paradigm based on logic programming and non-monotonic reasoning. It is a tremendously powerful tool for describing and solving combinatorial problems. Like any other language, ASP requires users to learn how it works and the syntax involved. It is becoming increasingly required for those unfamiliar with programming languages to interact with code. This paper proposes a novel method of translating unconstrained English into ASP programs for logic puzzles using an LLM and Abstract Meaning Representation (AMR) graphs. Everything from ASP rules, facts, and constraints is generated to fully represent and solve the desired problem. Example logic puzzles are used to demonstrate the capabilities of the system. While most current methods rely entirely on an LLM, our system minimizes the role of the LLM only to complete straightforward tasks. The LLM is used to simplify natural language sentences, identify keywords, and generate simple facts. The AMR graphs are then parsed from simplified language and used to generate ASP constraints systematically. The system successfully creates an entire ASP program that solves a combinatorial logic problem. This approach is a significant first step in creating a lighter-weight, explainable system that converts natural language to solve complex logic problems.

</details>


### [2] [Vector Symbolic Algebras for the Abstraction and Reasoning Corpus](https://arxiv.org/abs/2511.08747)
*Isaac Joffe,Chris Eliasmith*

Main category: cs.AI

TL;DR: 本文提出了一种基于向量符号代数的认知合理ARC-AGI求解器，结合系统1直觉和系统2推理，通过面向对象的程序合成方法解决ARC-AGI基准测试。


<details>
  <summary>Details</summary>
Motivation: ARC-AGI是人类轻松解决但对AI系统极其困难的流体智力基准测试，受神经科学和心理学的人类智能建模方法启发，旨在开发认知合理的求解器。

Method: 使用向量符号代数(VSA)的神经符号方法，集成系统1直觉和系统2推理，通过面向对象的程序合成，利用VSA表示抽象对象、指导解决方案搜索并实现样本高效的神经学习。

Result: 在ARC-AGI-1-Train上得分10.8%，在ARC-AGI-1-Eval上得分3.0%；在Sort-of-ARC上得分94.5%，在1D-ARC上得分83.1%且以极低计算成本超越GPT-4。

Conclusion: 这是首个将VSA应用于ARC-AGI的方法，开发了迄今为止最认知合理的ARC-AGI求解器，在保持计算效率的同时展现了良好的性能。

Abstract: The Abstraction and Reasoning Corpus for Artificial General Intelligence (ARC-AGI) is a generative, few-shot fluid intelligence benchmark. Although humans effortlessly solve ARC-AGI, it remains extremely difficult for even the most advanced artificial intelligence systems. Inspired by methods for modelling human intelligence spanning neuroscience to psychology, we propose a cognitively plausible ARC-AGI solver. Our solver integrates System 1 intuitions with System 2 reasoning in an efficient and interpretable process using neurosymbolic methods based on Vector Symbolic Algebras (VSAs). Our solver works by object-centric program synthesis, leveraging VSAs to represent abstract objects, guide solution search, and enable sample-efficient neural learning. Preliminary results indicate success, with our solver scoring 10.8% on ARC-AGI-1-Train and 3.0% on ARC-AGI-1-Eval. Additionally, our solver performs well on simpler benchmarks, scoring 94.5% on Sort-of-ARC and 83.1% on 1D-ARC -- the latter outperforming GPT-4 at a tiny fraction of the computational cost. Importantly, our approach is unique; we believe we are the first to apply VSAs to ARC-AGI and have developed the most cognitively plausible ARC-AGI solver yet. Our code is available at: https://github.com/ijoffe/ARC-VSA-2025.

</details>


### [3] [UCO: A Multi-Turn Interactive Reinforcement Learning Method for Adaptive Teaching with Large Language Models](https://arxiv.org/abs/2511.08873)
*Shouang Wei,Min Zhang,Xin Lin,Bo Jiang,Kun Kuang,Zhongxiang Dai*

Main category: cs.AI

TL;DR: 提出了单向认知优化（UCO）方法，通过多轮交互强化学习解决LLM作为智能导师时的两个关键挑战：无法区分学生是否真正理解，以及无法感知学生认知状态变化。


<details>
  <summary>Details</summary>
Motivation: 当前LLM作为智能导师的监督微调方法只能学习表面教学模式，缺乏动态适应能力；而现有强化学习方法存在两个关键问题：无法区分学生是否真正理解，以及无法实时感知学生认知状态变化。

Method: UCO采用多轮交互强化学习范式，包含两个协同奖励函数：进展奖励捕捉学生认知进步，评估学生是否真正从困惑转向理解；支架奖励动态识别每个学生的最远发展区，鼓励教师在该区域内保持高效教学。

Result: 在BigMath和MathTutorBench基准测试中，UCO模型优于所有同等规模的基线模型，并达到与先进闭源模型相当的性能。

Conclusion: UCO方法通过进展奖励和支架奖励的协同作用，有效解决了LLM作为智能导师时的认知状态感知和教学策略动态适应问题，显著提升了教学效果。

Abstract: Large language models (LLMs) are shifting from answer providers to intelligent tutors in educational settings, yet current supervised fine-tuning methods only learn surface teaching patterns without dynamic adaptation capabilities. Recent reinforcement learning approaches address this limitation but face two critical challenges. First, they evaluate teaching effectiveness solely based on whether students produce correct outputs, unable to distinguish whether students genuinely understand or echo teacher-provided answers during interaction. Second, they cannot perceive students' evolving cognitive states in real time through interactive dialogue, thus failing to adapt teaching strategies to match students' cognitive levels dynamically. We propose the Unidirectional Cognitive Optimization (UCO) method to address these challenges. UCO uses a multi-turn interactive reinforcement learning paradigm where the innovation lies in two synergistic reward functions: the Progress Reward captures students' cognitive advancement, evaluating whether students truly transition from confusion to comprehension, while the Scaffold Reward dynamically identifies each student's Zone of Proximal Development (ZPD), encouraging teachers to maintain productive teaching within this zone. We evaluate UCO by comparing it against 11 baseline models on BigMath and MathTutorBench benchmarks. Experimental results demonstrate that our UCO model outperforms all models of equivalent scale and achieves performance comparable to advanced closed-source models. The code and data are available at https://github.com/Mind-Lab-ECNU/UCO.

</details>


### [4] [Lumine: An Open Recipe for Building Generalist Agents in 3D Open Worlds](https://arxiv.org/abs/2511.08892)
*Weihao Tan,Xiangyang Li,Yunhao Fang,Heyuan Yao,Shi Yan,Hao Luo,Tenglong Ao,Huihui Li,Hongbin Ren,Bairen Yi,Yujia Qin,Bo An,Libin Liu,Guang Shi*

Main category: cs.AI

TL;DR: Lumine是首个能够在3D开放世界中实时完成数小时复杂任务的通用智能体，采用端到端的视觉语言模型统一感知、推理和行动，在Genshin Impact中训练后能完成5小时主线剧情，并具备零样本跨游戏泛化能力。


<details>
  <summary>Details</summary>
Motivation: 开发能够在复杂3D开放世界中完成长时间任务的通用智能体，解决现有方法在处理实时、长时间任务方面的局限性。

Method: 采用人机交互范式，通过视觉语言模型以5Hz处理原始像素，生成30Hz的键盘鼠标动作，仅在必要时进行推理。在Genshin Impact中进行训练。

Result: 成功完成Genshin Impact中5小时Mondstadt主线剧情，效率达到人类水平；在Wuthering Waves中完成100分钟任务，在Honkai: Star Rail中完成5小时第一章，均无需微调。

Conclusion: Lumine在开放世界环境中的有效性得到了验证，展示了跨不同世界和交互动态的强泛化能力，是迈向通用智能体的重要一步。

Abstract: We introduce Lumine, the first open recipe for developing generalist agents capable of completing hours-long complex missions in real time within challenging 3D open-world environments. Lumine adopts a human-like interaction paradigm that unifies perception, reasoning, and action in an end-to-end manner, powered by a vision-language model. It processes raw pixels at 5 Hz to produce precise 30 Hz keyboard-mouse actions and adaptively invokes reasoning only when necessary. Trained in Genshin Impact, Lumine successfully completes the entire five-hour Mondstadt main storyline on par with human-level efficiency and follows natural language instructions to perform a broad spectrum of tasks in both 3D open-world exploration and 2D GUI manipulation across collection, combat, puzzle-solving, and NPC interaction. In addition to its in-domain performance, Lumine demonstrates strong zero-shot cross-game generalization. Without any fine-tuning, it accomplishes 100-minute missions in Wuthering Waves and the full five-hour first chapter of Honkai: Star Rail. These promising results highlight Lumine's effectiveness across distinct worlds and interaction dynamics, marking a concrete step toward generalist agents in open-ended environments.

</details>


### [5] [Argus: Resilience-Oriented Safety Assurance Framework for End-to-End ADSs](https://arxiv.org/abs/2511.09032)
*Dingji Wang,You Lu,Bihuan Chen,Shuo Hao,Haowen Jiang,Yifan Tian,Xin Peng*

Main category: cs.AI

TL;DR: 提出Argus框架，通过持续监控自动驾驶系统轨迹并适时接管控制来缓解驾驶危险，提升系统韧性。


<details>
  <summary>Details</summary>
Motivation: 端到端自动驾驶系统在部署时面临各种驾驶危险，需要具备持续监控和自适应响应的韧性能力来确保安全。

Method: 设计运行时韧性导向框架Argus，包含轨迹监控和危险缓解器，当检测到不安全状态时无缝接管控制。

Result: 与TCP、UniAD和VAD三个先进端到端自动驾驶系统集成测试，驾驶评分平均提升150.30%，违规预防率达64.38%，时间开销小。

Conclusion: Argus能有效提升自动驾驶系统的韧性，显著改善驾驶性能并预防安全违规，具有实用价值。

Abstract: End-to-end autonomous driving systems (ADSs), with their strong capabilities in environmental perception and generalizable driving decisions, are attracting growing attention from both academia and industry. However, once deployed on public roads, ADSs are inevitably exposed to diverse driving hazards that may compromise safety and degrade system performance. This raises a strong demand for resilience of ADSs, particularly the capability to continuously monitor driving hazards and adaptively respond to potential safety violations, which is crucial for maintaining robust driving behaviors in complex driving scenarios.
  To bridge this gap, we propose a runtime resilience-oriented framework, Argus, to mitigate the driving hazards, thus preventing potential safety violations and improving the driving performance of an ADS. Argus continuously monitors the trajectories generated by the ADS for potential hazards and, whenever the EGO vehicle is deemed unsafe, seamlessly takes control through a hazard mitigator. We integrate Argus with three state-of-the-art end-to-end ADSs, i.e., TCP, UniAD and VAD. Our evaluation has demonstrated that Argus effectively and efficiently enhances the resilience of ADSs, improving the driving score of the ADS by up to 150.30% on average, and preventing up to 64.38% of the violations, with little additional time overhead.

</details>


### [6] [A Research on Business Process Optimisation Model Integrating AI and Big Data Analytics](https://arxiv.org/abs/2511.08934)
*Di Liao,Ruijia Liang,Ziyi Ye*

Main category: cs.AI

TL;DR: 本研究构建了一个融合人工智能与大数据的业务流程优化模型，采用三层架构实现流程全生命周期的智能化管理，通过实验验证显著提升了企业运营效率。


<details>
  <summary>Details</summary>
Motivation: 随着数字化转型的深入，业务流程优化成为提升企业竞争力的关键，需要构建智能化的流程管理解决方案。

Method: 采用包含数据处理、AI算法和业务逻辑的三层架构模型，结合分布式计算和深度学习技术，实现实时流程监控和优化。

Result: 实验验证显示，模型缩短流程处理时间42%，提高资源利用率28%，降低运营成本35%，在高并发负载下保持99.9%的可用性。

Conclusion: 研究成果对企业数字化转型具有重要理论价值和实践意义，为提升企业运营效率提供了新思路。

Abstract: With the deepening of digital transformation, business process optimisation has become the key to improve the competitiveness of enterprises. This study constructs a business process optimisation model integrating artificial intelligence and big data to achieve intelligent management of the whole life cycle of processes. The model adopts a three-layer architecture incorporating data processing, AI algorithms, and business logic to enable real-time process monitoring and optimization. Through distributed computing and deep learning techniques, the system can handle complex business scenarios while maintaining high performance and reliability. Experimental validation across multiple enterprise scenarios shows that the model shortens process processing time by 42%, improves resource utilisation by 28%, and reduces operating costs by 35%. The system maintained 99.9% availability under high concurrent loads. The research results have important theoretical and practical value for promoting the digital transformation of enterprises, and provide new ideas for improving the operational efficiency of enterprises.

</details>


### [7] [AlphaCast: A Human Wisdom-LLM Intelligence Co-Reasoning Framework for Interactive Time Series Forecasting](https://arxiv.org/abs/2511.08947)
*Xiaohan Zhang,Tian Gao,Mingyue Cheng,Bokai Pan,Ze Guo,Yaguo Liu,Xiaoyu Tao*

Main category: cs.AI

TL;DR: AlphaCast是一个人类智慧与大型语言模型协同推理的时序预测框架，将预测重新定义为交互过程，通过两阶段协作实现更准确的预测。


<details>
  <summary>Details</summary>
Motivation: 当前时序预测方法缺乏人类专家的交互、推理和适应性，限制了在复杂现实环境中的实用性。

Method: 采用两阶段框架：1）自动预测准备，构建多源认知基础；2）生成推理和反思优化，触发元推理循环进行持续自校正。

Result: 在短期和长期数据集上的广泛实验表明，AlphaCast在预测准确性方面持续优于最先进的基线方法。

Conclusion: AlphaCast通过人类智慧与LLM智能的协同推理，显著提升了时序预测的准确性和实用性。

Abstract: Time series forecasting plays a critical role in high-stakes domains such as energy, healthcare, and climate. Although recent advances have improved accuracy, most approaches still treat forecasting as a static one-time mapping task, lacking the interaction, reasoning, and adaptability of human experts. This gap limits their usefulness in complex real-world environments. To address this, we propose AlphaCast, a human wisdom-large language model (LLM) intelligence co-reasoning framework that redefines forecasting as an interactive process. The key idea is to enable step-by-step collaboration between human wisdom and LLM intelligence to jointly prepare, generate, and verify forecasts. The framework consists of two stages: (1) automated prediction preparation, where AlphaCast builds a multi-source cognitive foundation comprising a feature set that captures key statistics and time patterns, a domain knowledge base distilled from corpora and historical series, a contextual repository that stores rich information for each time window, and a case base that retrieves optimal strategies via pattern clustering and matching; and (2) generative reasoning and reflective optimization, where AlphaCast integrates statistical temporal features, prior knowledge, contextual information, and forecasting strategies, triggering a meta-reasoning loop for continuous self-correction and strategy refinement. Extensive experiments on short- and long-term datasets show that AlphaCast consistently outperforms state-of-the-art baselines in predictive accuracy. Code is available at this repository: https://github.com/SkyeGT/AlphaCast_Official .

</details>


### [8] [AI Founding Fathers: A Case Study of GIS Search in Multi-Agent Pipelines](https://arxiv.org/abs/2511.09005)
*Alvin Chauhan*

Main category: cs.AI

TL;DR: 本文提出通过构建多智能体管道来实现渐进式、增量式和顺序式的搜索空间遍历，从而增强大语言模型的推理能力，并通过递归精炼方法验证了这一框架的有效性。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型表现出卓越的流畅性，但研究者仍在努力提升其推理能力。本文基于搜索导向的LLM计算解释，旨在系统化理解LLM推理和优化。

Method: 采用递归精炼方法，包括自我批评、对抗性压力测试和关键反馈整合的迭代过程。设计了简单线性管道与复杂结构化管道的对比实验，使用三位美国开国元勋的历史人物角色，通过RAG增强语料库生成对当代政治议题的回应。

Result: 复杂模型在所有九个测试案例中均优于简单模型，平均仲裁分数为88.3对71.7。复杂模型的论证在分析深度、结构细微差别和战略框架方面表现更优。

Conclusion: 递归精炼是通过GIS搜索增强LLM推理的强大架构特征，高质量推理是受控的增量搜索过程。

Abstract: Although Large Language Models (LLMs) show exceptional fluency, efforts persist to extract stronger reasoning capabilities from them. Drawing on search-based interpretations of LLM computation, this paper advances a systematic framework for understanding LLM reasoning and optimization. Namely, that enhancing reasoning is best achieved by structuring a multi-agent pipeline to ensure a traversal of the search space in a gradual, incremental, and sequential (GIS) manner. Stated succinctly, high-quality reasoning is a controlled, incremental search. To test this framework, we investigate the efficacy of recursive refinement (RR)--an iterative process of self-criticism, adversarial stress-testing, and integrating critical feedback--as a practical method for implementing GIS search. We designed an experiment comparing a simple, linear pipeline against a complex, explicitly structured pipeline leveraging a recursive refinement layer. The multi-agent models were constructed to reflect the historical personas of three US Founding Fathers (Hamilton, Jefferson, and Madison) using RAG-powered corpora and were prompted to generate responses to three contemporary political issues. Model performance was evaluated using a two-tiered approach: a quantitative score from an LLM arbiter agent and qualitative human judgment. Our results revealed that the complex model consistently outperformed the simple model across all nine test cases with an average arbiter-outputted score of 88.3 versus 71.7. The complex model's arguments were superior in analytical depth, structural nuance, and strategic framing. We conclude that recursive refinement is a robust architectural feature for enhancing LLM reasoning via GIS search.

</details>


### [9] [Advancing Autonomous Emergency Response Systems: A Generative AI Perspective](https://arxiv.org/abs/2511.09044)
*Yousef Emami,Radha Reddy,Azadeh Pourkabirian,Miguel Gutierrez Gaitan*

Main category: cs.AI

TL;DR: 本文综述了下一代自动驾驶车辆在应急服务中的优化策略，重点分析了从传统强化学习向扩散模型增强强化学习和大型语言模型辅助上下文学习的转变，为理解基于生成式AI的自主应急响应系统提供了框架。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶车辆有望通过更快、更安全、更高效的响应来革新应急服务，但传统强化学习方法在动态应急场景中存在样本效率低和适应性不足的问题，需要新的优化策略来解决这些局限性。

Method: 分析了从传统强化学习到扩散模型增强强化学习的转变，扩散模型通过合成数据生成增强策略鲁棒性；同时探索了大型语言模型辅助上下文学习的新范式，支持无需重新训练的快速现场适应。

Result: 扩散模型增强强化学习提高了策略鲁棒性但增加了计算成本；大型语言模型辅助上下文学习提供了轻量级且可解释的替代方案，能够实现快速适应。

Conclusion: 通过综述自动驾驶智能、扩散模型增强强化学习和大型语言模型辅助上下文学习的最新技术，为理解下一代自主应急响应系统从生成式AI角度提供了关键框架。

Abstract: Autonomous Vehicles (AVs) are poised to revolutionize emergency services by enabling faster, safer, and more efficient responses. This transformation is driven by advances in Artificial Intelligence (AI), particularly Reinforcement Learning (RL), which allows AVs to navigate complex environments and make critical decisions in real time. However, conventional RL paradigms often suffer from poor sample efficiency and lack adaptability in dynamic emergency scenarios. This paper reviews next-generation AV optimization strategies to address these limitations. We analyze the shift from conventional RL to Diffusion Model (DM)-augmented RL, which enhances policy robustness through synthetic data generation, albeit with increased computational cost. Additionally, we explore the emerging paradigm of Large Language Model (LLM)-assisted In-Context Learning (ICL), which offers a lightweight and interpretable alternative by enabling rapid, on-the-fly adaptation without retraining. By reviewing the state of the art in AV intelligence, DM-augmented RL, and LLM-assisted ICL, this paper provides a critical framework for understanding the next generation of autonomous emergency response systems from a Generative AI perspective.

</details>


### [10] [OR-R1: Automating Modeling and Solving of Operations Research Optimization Problem via Test-Time Reinforcement Learning](https://arxiv.org/abs/2511.09092)
*Zezhen Ding,Zhen Tan,Jiheng Zhang,Tianlong Chen*

Main category: cs.AI

TL;DR: OR-R1是一个数据高效的训练框架，用于自动化优化建模和求解。它通过监督微调和测试时组相对策略优化的两阶段设计，仅需1/10的数据量就能达到67.7%的平均求解准确率，超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于大语言模型的优化建模方法需要大量标注或合成数据，导致成本高且可扩展性差。OR-R1旨在开发一个数据效率高、泛化能力强的自动化优化建模和求解框架。

Method: 采用两阶段训练框架：1）监督微调从有限标注数据中学习问题建模和代码生成的推理模式；2）测试时组相对策略优化利用大量未标注数据提升能力和一致性。

Result: OR-R1仅需1/10的合成数据就达到67.7%的平均求解准确率，比ORLM高出4.2%。在仅100个合成样本下仍优于ORLM 2.4%。TGRPO带来3.1%-6.4%的额外提升，将单次尝试与多次尝试的性能差距从13%缩小到7%。

Conclusion: OR-R1为自动化运筹学优化问题建模和求解提供了稳健、可扩展且成本效益高的解决方案，显著降低了工业应用的专业知识和数据门槛。

Abstract: Optimization modeling and solving are fundamental to the application of Operations Research (OR) in real-world decision making, yet the process of translating natural language problem descriptions into formal models and solver code remains highly expertise intensive. While recent advances in large language models (LLMs) have opened new opportunities for automation, the generalization ability and data efficiency of existing LLM-based methods are still limited, asmost require vast amounts of annotated or synthetic data, resulting in high costs and scalability barriers. In this work, we present OR-R1, a data-efficient training framework for automated optimization modeling and solving. OR-R1 first employs supervised fine-tuning (SFT) to help the model acquire the essential reasoning patterns for problem formulation and code generation from limited labeled data. In addition, it improves the capability and consistency through Test-Time Group Relative Policy Optimization (TGRPO). This two-stage design enables OR-R1 to leverage both scarce labeled and abundant unlabeled data for effective learning. Experiments show that OR-R1 achieves state-of-the-art performance with an average solving accuracy of $67.7\%$, using only $1/10$ the synthetic data required by prior methods such as ORLM, exceeding ORLM's solving accuracy by up to $4.2\%$. Remarkably, OR-R1 outperforms ORLM by over $2.4\%$ with just $100$ synthetic samples. Furthermore, TGRPO contributes an additional $3.1\%-6.4\%$ improvement in accuracy, significantly narrowing the gap between single-attempt (Pass@1) and multi-attempt (Pass@8) performance from $13\%$ to $7\%$. Extensive evaluations across diverse real-world benchmarks demonstrate that OR-R1 provides a robust, scalable, and cost-effective solution for automated OR optimization problem modeling and solving, lowering the expertise and data barriers for industrial OR applications.

</details>


### [11] [Efficient Reasoning via Reward Model](https://arxiv.org/abs/2511.09158)
*Yuhao Wang,Xiaopeng Li,Cheng Gong,Ziru Liu,Suiyun Zhang,Rui Liu,Xiangyu Zhao*

Main category: cs.AI

TL;DR: 提出了一种训练简洁性奖励模型（CRM）的管道，通过新颖的简洁性奖励函数（CRF）解决大型推理模型中的过度思考问题，在减少响应长度的同时提升推理准确性。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型（如DeepSeek-R1和OpenAI o1）经常生成包含冗余推理步骤的冗长响应（过度思考现象），显著增加计算成本。现有方法通过长度惩罚存在长度崩溃和训练崩溃问题。

Method: 提出训练简洁性奖励模型（CRM）的管道来评估推理路径的简洁性，并引入具有结果奖励与简洁性得分明确依赖关系的简洁性奖励函数（CRF）。

Result: 在五个数学基准数据集上的实验显示，该方法在Qwen2.5-7B上实现了8.1%的准确率提升和19.9%的响应长度减少，且在Llama和Mistral等其他LLMs上泛化良好。

Conclusion: 该方法从理论上证明了新奖励在方差减少和收敛性改进方面的优势，实践上有效解决了过度思考问题，实现了更高效和更有效的推理。

Abstract: Reinforcement learning with verifiable rewards (RLVR) has been shown to enhance the reasoning capabilities of large language models (LLMs), enabling the development of large reasoning models (LRMs). However, LRMs such as DeepSeek-R1 and OpenAI o1 often generate verbose responses containing redundant or irrelevant reasoning step-a phenomenon known as overthinking-which substantially increases computational costs. Prior efforts to mitigate this issue commonly incorporate length penalties into the reward function, but we find they frequently suffer from two critical issues: length collapse and training collapse, resulting in sub-optimal performance. To address them, we propose a pipeline for training a Conciseness Reward Model (CRM) that scores the conciseness of reasoning path. Additionally, we introduce a novel reward formulation named Conciseness Reward Function (CRF) with explicit dependency between the outcome reward and conciseness score, thereby fostering both more effective and more efficient reasoning. From a theoretical standpoint, we demonstrate the superiority of the new reward from the perspective of variance reduction and improved convergence properties. Besides, on the practical side, extensive experiments on five mathematical benchmark datasets demonstrate the method's effectiveness and token efficiency, which achieves an 8.1% accuracy improvement and a 19.9% reduction in response token length on Qwen2.5-7B. Furthermore, the method generalizes well to other LLMs including Llama and Mistral. The implementation code and datasets are publicly available for reproduction: https://anonymous.4open.science/r/CRM.

</details>


### [12] [Perspectives on a Reliability Monitoring Framework for Agentic AI Systems](https://arxiv.org/abs/2511.09178)
*Niclas Flehmig,Mary Ann Lundteigen,Shen Yin*

Main category: cs.AI

TL;DR: 本文提出了一个双层可靠性监控框架，用于解决智能AI系统在运行期间的可靠性不足问题，通过异常检测和透明度层为操作员提供决策支持。


<details>
  <summary>Details</summary>
Motivation: 智能AI系统虽然能在各种应用中提供更有帮助的AI系统，但由于可靠性不足，特别是在高风险领域如医疗保健或流程工业中，其不可靠性带来了意外行为的风险，需要缓解技术。

Method: 提出一个双层可靠性监控框架，包括用于检测新输入的分布外检测层和揭示内部操作的AI透明度层，为人类操作员提供决策支持以判断输出是否可能不可靠并进行干预。

Result: 该框架为开发缓解技术提供了基础，以减少运行期间不确定性可靠性带来的风险。

Conclusion: 双层监控方法能够有效支持人类操作员判断AI系统输出的可靠性，并为降低运行风险提供了可行的技术路径。

Abstract: The implementation of agentic AI systems has the potential of providing more helpful AI systems in a variety of applications. These systems work autonomously towards a defined goal with reduced external control. Despite their potential, one of their flaws is the insufficient reliability which makes them especially unsuitable for high-risk domains such as healthcare or process industry. Unreliable systems pose a risk in terms of unexpected behavior during operation and mitigation techniques are needed. In this work, we derive the main reliability challenges of agentic AI systems during operation based on their characteristics. We draw the connection to traditional AI systems and formulate a fundamental reliability challenge during operation which is inherent to traditional and agentic AI systems. As our main contribution, we propose a two-layered reliability monitoring framework for agentic AI systems which consists of a out-of-distribution detection layer for novel inputs and AI transparency layer to reveal internal operations. This two-layered monitoring approach gives a human operator the decision support which is needed to decide whether an output is potential unreliable or not and intervene. This framework provides a foundation for developing mitigation techniques to reduce risk stemming from uncertain reliability during operation.

</details>


### [13] [MedFuse: Multiplicative Embedding Fusion For Irregular Clinical Time Series](https://arxiv.org/abs/2511.09247)
*Yi-Hsien Hsieh,Ta-Jung Chien,Chun-Kai Huang,Shao-Hua Sun,Che Lin*

Main category: cs.AI

TL;DR: MedFuse提出了一种用于不规则临床时间序列的框架，通过乘法嵌入融合模块将特征值和特征身份嵌入进行乘法调制，能够更好地捕捉特征间的高阶依赖关系。


<details>
  <summary>Details</summary>
Motivation: 电子健康记录中的临床时间序列具有不规则性、异步采样、缺失值和异质特征动态等问题。现有的嵌入策略通常通过加法操作结合特征身份和值嵌入，限制了捕捉值依赖特征交互的能力。

Method: 提出了MedFuse框架，核心是MuFuse（乘法嵌入融合）模块，通过乘法调制融合值和特征嵌入，在保留特征特定信息的同时建模跨特征的高阶依赖关系。

Result: 在涵盖重症和慢性护理的三个真实世界数据集上的实验表明，MedFuse在关键预测任务上始终优于最先进的基线方法。学习表示的分析进一步证明乘法融合增强了表达能力并支持跨数据集预训练。

Conclusion: MedFuse为建模不规则临床时间序列提供了一种可泛化的方法，乘法融合机制显著提升了模型性能。

Abstract: Clinical time series derived from electronic health records (EHRs) are inherently irregular, with asynchronous sampling, missing values, and heterogeneous feature dynamics. While numerical laboratory measurements are highly informative, existing embedding strategies usually combine feature identity and value embeddings through additive operations, which constrains their ability to capture value-dependent feature interactions. We propose MedFuse, a framework for irregular clinical time series centered on the MuFuse (Multiplicative Embedding Fusion) module. MuFuse fuses value and feature embeddings through multiplicative modulation, preserving feature-specific information while modeling higher-order dependencies across features. Experiments on three real-world datasets covering both intensive and chronic care show that MedFuse consistently outperforms state-of-the-art baselines on key predictive tasks. Analysis of the learned representations further demonstrates that multiplicative fusion enhances expressiveness and supports cross-dataset pretraining. These results establish MedFuse as a generalizable approach for modeling irregular clinical time series.

</details>


### [14] [HyperD: Hybrid Periodicity Decoupling Framework for Traffic Forecasting](https://arxiv.org/abs/2511.09275)
*Minlan Shao,Zijian Zhang,Yili Wang,Yiwei Dai,Xu Shen,Xin Wang*

Main category: cs.AI

TL;DR: HyperD是一个用于交通预测的新框架，通过将交通数据解耦为周期性和残差分量来处理复杂的时空依赖性和多尺度周期性模式。


<details>
  <summary>Details</summary>
Motivation: 交通预测面临两大挑战：复杂的空间依赖性和多尺度周期性模式与不规则波动的共存。现有方法难以同时有效处理这些因素。

Method: 提出HyperD框架，包含混合周期性表示模块（处理周期性分量）和频率感知残差表示模块（处理非周期性波动），并引入双视图对齐损失来强制语义分离。

Result: 在四个真实世界交通数据集上的实验表明，HyperD实现了最先进的预测精度，在干扰下具有更好的鲁棒性，计算效率也优于现有方法。

Conclusion: HyperD通过解耦周期性和非周期性分量，有效解决了交通预测中的关键挑战，为智能交通系统提供了更准确和鲁棒的预测方案。

Abstract: Accurate traffic forecasting plays a vital role in intelligent transportation systems, enabling applications such as congestion control, route planning, and urban mobility optimization.However, traffic forecasting remains challenging due to two key factors: (1) complex spatial dependencies arising from dynamic interactions between road segments and traffic sensors across the network, and (2) the coexistence of multi-scale periodic patterns (e.g., daily and weekly periodic patterns driven by human routines) with irregular fluctuations caused by unpredictable events (e.g., accidents, weather, or construction). To tackle these challenges, we propose HyperD (Hybrid Periodic Decoupling), a novel framework that decouples traffic data into periodic and residual components. The periodic component is handled by the Hybrid Periodic Representation Module, which extracts fine-grained daily and weekly patterns using learnable periodic embeddings and spatial-temporal attention. The residual component, which captures non-periodic, high-frequency fluctuations, is modeled by the Frequency-Aware Residual Representation Module, leveraging complex-valued MLP in frequency domain. To enforce semantic separation between the two components, we further introduce a Dual-View Alignment Loss, which aligns low-frequency information with the periodic branch and high-frequency information with the residual branch. Extensive experiments on four real-world traffic datasets demonstrate that HyperD achieves state-of-the-art prediction accuracy, while offering superior robustness under disturbances and improved computational efficiency compared to existing methods.

</details>


### [15] [From Model Training to Model Raising -- A call to reform AI model training paradigms from post-hoc alignment to intrinsic, identity-based development](https://arxiv.org/abs/2511.09287)
*Roland Aydin,Christian Cyron,Steve Bachelor,Ashton Anderson,Robert West*

Main category: cs.AI

TL;DR: 论文提出从"模型训练"转向"模型培养"的新范式，将价值观对齐融入模型开发全过程，通过重构训练语料库实现知识与价值观的内在融合。


<details>
  <summary>Details</summary>
Motivation: 当前AI训练方法在模型核心能力建立后才进行价值观对齐，导致模型易错位且缺乏深层次价值体系。在大模型能力超越人类的背景下，需要从根本上改变训练范式。

Method: 重新设计训练语料库：采用第一人称视角重构训练数据、将信息重新情境化为生活体验、模拟社会互动、搭建训练数据排序的脚手架。

Result: 预期这种训练语料库的重新设计将实现从第一个训练标记开始的早期价值观承诺，使知识、技能和价值观内在难以分离。

Conclusion: 在大型语言模型能力开始超越人类的生态系统中，这种从训练到培养的范式转变是至关重要的需求。

Abstract: Current AI training methods align models with human values only after their core capabilities have been established, resulting in models that are easily misaligned and lack deep-rooted value systems. We propose a paradigm shift from "model training" to "model raising", in which alignment is woven into a model's development from the start. We identify several key components for this paradigm, all centered around redesigning the training corpus: reframing training data from a first-person perspective, recontextualizing information as lived experience, simulating social interactions, and scaffolding the ordering of training data. We expect that this redesign of the training corpus will lead to an early commitment to values from the first training token onward, such that knowledge, skills, and values are intrinsically much harder to separate. In an ecosystem in which large language model capabilities start overtaking human capabilities in many tasks, this seems to us like a critical need.

</details>


### [16] [Not Everything That Counts Can Be Counted: A Case for Safe Qualitative AI](https://arxiv.org/abs/2511.09325)
*Stine Beltoft,Lukas Galke*

Main category: cs.AI

TL;DR: 本文主张开发专门针对定性研究的AI系统，以解决当前定性研究者在AI应用中的困境。现有通用AI工具存在偏见、不透明、不可复现和隐私问题，而定性方法对于科学理解至关重要。


<details>
  <summary>Details</summary>
Motivation: AI和大型语言模型正在重塑科学，但定性研究被忽视。研究者对AI持保留态度，只能依赖有缺陷的通用工具如ChatGPT，这造成了关键差距。

Method: 主张从零开始构建专门用于解释性研究的定性AI系统，这些系统必须透明、可复现且保护隐私。回顾文献展示如何通过强大的定性能力增强现有自动化发现流程。

Result: 识别了安全定性AI可以推动多学科和混合方法研究的关键机会。

Conclusion: 需要开发专门的定性AI系统来弥合定量方法与定性维度之间的差距，确保科学发现的全面性。

Abstract: Artificial intelligence (AI) and large language models (LLM) are reshaping science, with most recent advances culminating in fully-automated scientific discovery pipelines. But qualitative research has been left behind. Researchers in qualitative methods are hesitant about AI adoption. Yet when they are willing to use AI at all, they have little choice but to rely on general-purpose tools like ChatGPT to assist with interview interpretation, data annotation, and topic modeling - while simultaneously acknowledging these system's well-known limitations of being biased, opaque, irreproducible, and privacy-compromising. This creates a critical gap: while AI has substantially advanced quantitative methods, the qualitative dimensions essential for meaning-making and comprehensive scientific understanding remain poorly integrated. We argue for developing dedicated qualitative AI systems built from the ground up for interpretive research. Such systems must be transparent, reproducible, and privacy-friendly. We review recent literature to show how existing automated discovery pipelines could be enhanced by robust qualitative capabilities, and identify key opportunities where safe qualitative AI could advance multidisciplinary and mixed-methods research.

</details>


### [17] [The 2025 Planning Performance of Frontier Large Language Models](https://arxiv.org/abs/2511.09378)
*Augusto B. Corrêa,André G. Pereira,Jendrik Seipp*

Main category: cs.AI

TL;DR: 评估2025年三个前沿大语言模型在PDDL规划任务上的表现，GPT-5在标准PDDL领域表现与LAMA规划器相当，但在混淆测试中所有模型性能下降。


<details>
  <summary>Details</summary>
Motivation: 评估前沿LLM在端到端规划任务中的推理能力，特别是与专业规划器的性能差距。

Method: 使用国际规划竞赛学习赛道中的PDDL领域和任务子集，测试DeepSeek R1、Gemini 2.5 Pro、GPT-5和LAMA规划器，包括标准PDDL和混淆PDDL两种情况。

Result: GPT-5在标准PDDL领域解决任务数量与LAMA相当；所有LLM在混淆PDDL测试中性能下降，但下降程度比之前模型报告的要小。

Conclusion: 前沿LLM在规划任务上相比前代模型有显著改进，与规划器在挑战性基准测试上的性能差距正在缩小。

Abstract: The capacity of Large Language Models (LLMs) for reasoning remains an active area of research, with the capabilities of frontier models continually advancing. We provide an updated evaluation of the end-to-end planning performance of three frontier LLMs as of 2025, where models are prompted to generate a plan from PDDL domain and task descriptions. We evaluate DeepSeek R1, Gemini 2.5 Pro, GPT-5 and as reference the planner LAMA on a subset of domains from the most recent Learning Track of the International Planning Competition. Our results show that on standard PDDL domains, the performance of GPT-5 in terms of solved tasks is competitive with LAMA. When the PDDL domains and tasks are obfuscated to test for pure reasoning, the performance of all LLMs degrades, though less severely than previously reported for other models. These results show substantial improvements over prior generations of LLMs, reducing the performance gap to planners on a challenging benchmark.

</details>


### [18] [What We Don't C: Representations for scientific discovery beyond VAEs](https://arxiv.org/abs/2511.09433)
*Brian Rogers,Micah Bowles,Chris J. Lintott,Steve Croft*

Main category: cs.AI

TL;DR: 提出了一种基于潜在流匹配和分类器自由引导的新方法，通过显式分离条件信息和残差表示来解耦潜在子空间，实现了对高维数据有意义特征的访问。


<details>
  <summary>Details</summary>
Motivation: 在高维领域中访问学习表示中的信息对于科学发现至关重要，需要一种机制来分析、控制和重新利用潜在表示。

Method: 使用潜在流匹配结合分类器自由引导，显式分离条件信息和残差表示，从而解耦潜在子空间。

Result: 在三个实验中（合成2D高斯问题、彩色MNIST和Galaxy10天文数据集）均成功访问到高维数据的有意义特征。

Conclusion: 该方法为使用生成模型进行科学探索提供了一条简单而强大的途径，能够分析我们未捕获、考虑或编目的信息。

Abstract: Accessing information in learned representations is critical for scientific discovery in high-dimensional domains. We introduce a novel method based on latent flow matching with classifier-free guidance that disentangles latent subspaces by explicitly separating information included in conditioning from information that remains in the residual representation. Across three experiments -- a synthetic 2D Gaussian toy problem, colored MNIST, and the Galaxy10 astronomy dataset -- we show that our method enables access to meaningful features of high dimensional data. Our results highlight a simple yet powerful mechanism for analyzing, controlling, and repurposing latent representations, providing a pathway toward using generative models for scientific exploration of what we don't capture, consider, or catalog.

</details>


### [19] [CrochetBench: Can Vision-Language Models Move from Describing to Doing in Crochet Domain?](https://arxiv.org/abs/2511.09483)
*Peiyu Li,Xiaobao Huang,Nitesh V. Chawla*

Main category: cs.AI

TL;DR: CrochetBench是一个评估多模态大语言模型在钩针编织领域进行细粒度、低层次程序推理能力的基准。它要求模型识别针法、选择结构合适的指令并生成可编译的钩针程序，通过执行验证功能正确性。


<details>
  <summary>Details</summary>
Motivation: 现有基准主要关注高层次描述或视觉问答，而CrochetBench将重点从描述转向实际操作，评估模型在真实世界创意领域中的程序能力。

Method: 采用CrochetPARADE DSL作为中间表示，支持结构验证和通过执行的功能评估。基准涵盖针法分类、指令接地、自然语言到DSL翻译和图像到DSL翻译等任务。

Result: 在所有任务中，当评估从表面相似性转向可执行正确性时，性能急剧下降，暴露了模型在长距离符号推理和3D感知程序合成方面的局限性。

Conclusion: CrochetBench为评估多模态模型的程序能力提供了新视角，突显了表面理解与真实世界创意领域中可执行精度之间的差距。

Abstract: We present CrochetBench, a benchmark for evaluating the ability of multimodal large language models to perform fine-grained, low-level procedural reasoning in the domain of crochet. Unlike prior benchmarks that focus on high-level description or visual question answering, CrochetBench shifts the emphasis from describing to doing: models are required to recognize stitches, select structurally appropriate instructions, and generate compilable crochet procedures. We adopt the CrochetPARADE DSL as our intermediate representation, enabling structural validation and functional evaluation via execution. The benchmark covers tasks including stitch classification, instruction grounding, and both natural language and image-to-DSL translation. Across all tasks, performance sharply declines as the evaluation shifts from surface-level similarity to executable correctness, exposing limitations in long-range symbolic reasoning and 3D-aware procedural synthesis. CrochetBench offers a new lens for assessing procedural competence in multimodal models and highlights the gap between surface-level understanding and executable precision in real-world creative domains. Code is available at https://github.com/Peiyu-Georgia-Li/crochetBench.

</details>


### [20] [Consensus Sampling for Safer Generative AI](https://arxiv.org/abs/2511.09493)
*Adam Tauman Kalai,Yael Tauman Kalai,Or Zamir*

Main category: cs.AI

TL;DR: 本文提出了一种通过聚合多个生成模型来增强AI安全性的方法，使用共识采样算法从k个模型中选择最安全的s个模型，在模型间达成足够共识时输出结果，否则弃权。


<details>
  <summary>Details</summary>
Motivation: 现有的AI安全方法主要依赖检查模型输出或激活，但某些风险仅通过检查无法检测。需要一种架构无关的补充方法来增强安全性。

Method: 提出共识采样算法，利用模型计算输出概率的能力，从k个模型中选择最安全的s个模型，当这些模型间达成足够共识时输出结果，否则弃权。

Result: 该方法能够实现与最安全的s个模型平均风险相当的安全性，并在足够多模型安全且达成充分共识时限制弃权概率。

Conclusion: 该方法提供了一种新的模型无关的AI安全方法，通过从模型集合中的未知安全子集放大安全保证到单个可靠模型。

Abstract: Many approaches to AI safety rely on inspecting model outputs or activations, yet certain risks are inherently undetectable by inspection alone. We propose a complementary, architecture-agnostic approach that enhances safety through the aggregation of multiple generative models, with the aggregated model inheriting its safety from the safest subset of a given size among them. Specifically, we present a consensus sampling algorithm that, given $k$ models and a prompt, achieves risk competitive with the average risk of the safest $s$ of the $k$ models, where $s$ is a chosen parameter, while abstaining when there is insufficient agreement between them. The approach leverages the models' ability to compute output probabilities, and we bound the probability of abstention when sufficiently many models are safe and exhibit adequate agreement. The algorithm is inspired by the provable copyright protection algorithm of Vyas et al. (2023). It requires some overlap among safe models, offers no protection when all models are unsafe, and may accumulate risk over repeated use. Nonetheless, our results provide a new, model-agnostic approach for AI safety by amplifying safety guarantees from an unknown subset of models within a collection to that of a single reliable model.

</details>


### [21] [Fundamentals of Physical AI](https://arxiv.org/abs/2511.09497)
*Vahid Salehi*

Main category: cs.AI

TL;DR: 本文从科学和系统角度阐述了物理人工智能的基本原理，旨在为智能系统的物理体现、感官感知、行动能力、学习过程和情境敏感性建立统一的理论框架。


<details>
  <summary>Details</summary>
Motivation: 创建描述智能系统物理体现、感官感知、行动能力、学习过程和情境敏感性的理论基础，将智能理解为身体、环境和经验之间真实交互的涌现现象。

Method: 提出六个基本原理：体现、感官感知、运动行为、学习、自主性和情境敏感性，这些原理构成封闭控制循环，能量、信息、控制和情境在其中持续交互。

Result: 建立了一个理论模型，将智能理解为物理体现过程，学习被视为智能体与环境之间结构耦合的变化，而非参数调整。

Conclusion: 物理人工智能将智能理解为物理体现过程，通过康复诊所中自适应辅助机器人的实例展示了六个基本原理在真实系统中的相互作用。

Abstract: This work will elaborate the fundamental principles of physical artificial intelligence (Physical AI) from a scientific and systemic perspective. The aim is to create a theoretical foundation that describes the physical embodiment, sensory perception, ability to act, learning processes, and context sensitivity of intelligent systems within a coherent framework. While classical AI approaches rely on symbolic processing and data driven models, Physical AI understands intelligence as an emergent phenomenon of real interaction between body, environment, and experience. The six fundamentals presented here are embodiment, sensory perception, motor action, learning, autonomy, and context sensitivity, and form the conceptual basis for designing and evaluating physically intelligent systems. Theoretically, it is shown that these six principles do not represent loose functional modules but rather act as a closed control loop in which energy, information, control, and context are in constant interaction. This circular interaction enables a system to generate meaning not from databases, but from physical experience, a paradigm shift that understands intelligence as an physical embodied process. Physical AI understands learning not as parameter adjustment, but as a change in the structural coupling between agents and the environment. To illustrate this, the theoretical model is explained using a practical scenario: An adaptive assistant robot supports patients in a rehabilitation clinic. This example illustrates that physical intelligence does not arise from abstract calculation, but from immediate, embodied experience. It shows how the six fundamentals interact in a real system: embodiment as a prerequisite, perception as input, movement as expression, learning as adaptation, autonomy as regulation, and context as orientation.

</details>


### [22] [Robust and Diverse Multi-Agent Learning via Rational Policy Gradient](https://arxiv.org/abs/2511.09535)
*Niklas Lauffer,Ameesh Shah,Micah Carroll,Sanjit A. Seshia,Stuart Russell,Michael Dennis*

Main category: cs.AI

TL;DR: 本文提出了Rationality-preserving Policy Optimization (RPO)框架和Rational Policy Gradient (RPG)方法，用于在合作和一般和环境中避免对抗优化中的自破坏问题，确保智能体保持理性。


<details>
  <summary>Details</summary>
Motivation: 对抗优化算法在多智能体环境中能有效发现鲁棒和多样化的策略，但在合作环境中会导致智能体非理性地自破坏，阻碍任务完成和学习进程。

Method: 开发了RPG方法，通过对手塑造技术在修改后的游戏版本中训练智能体最大化自身奖励，同时保持理性（即其策略相对于某些可能的伙伴策略是最优的）。

Result: RPG方法在多个流行的合作和一般和环境中实现了强劲性能，能够发现对抗样本、提高鲁棒性和适应性，并学习多样化策略。

Conclusion: RPO框架和RPG方法成功解决了对抗优化在合作环境中的自破坏问题，扩展了现有对抗优化算法的应用范围。

Abstract: Adversarial optimization algorithms that explicitly search for flaws in agents' policies have been successfully applied to finding robust and diverse policies in multi-agent settings. However, the success of adversarial optimization has been largely limited to zero-sum settings because its naive application in cooperative settings leads to a critical failure mode: agents are irrationally incentivized to self-sabotage, blocking the completion of tasks and halting further learning. To address this, we introduce Rationality-preserving Policy Optimization (RPO), a formalism for adversarial optimization that avoids self-sabotage by ensuring agents remain rational--that is, their policies are optimal with respect to some possible partner policy. To solve RPO, we develop Rational Policy Gradient (RPG), which trains agents to maximize their own reward in a modified version of the original game in which we use opponent shaping techniques to optimize the adversarial objective. RPG enables us to extend a variety of existing adversarial optimization algorithms that, no longer subject to the limitations of self-sabotage, can find adversarial examples, improve robustness and adaptability, and learn diverse policies. We empirically validate that our approach achieves strong performance in several popular cooperative and general-sum environments. Our project page can be found at https://rational-policy-gradient.github.io.

</details>


### [23] [Breadth-First Search vs. Restarting Random Walks for Escaping Uninformed Heuristic Regions](https://arxiv.org/abs/2511.09549)
*Daniel Platnick,Dawson Tomasz,Eamon Earl,Sourena Khanzadeh,Richard Valenzano*

Main category: cs.AI

TL;DR: 本文比较了广度优先搜索(BrFS)和重启随机游走(RRWs)两种方法在逃离无信息启发式区域(UHRs)时的表现，推导了它们的期望运行时间，并开发了使用RRWs的EHC-RRW变体，在PDDL规划基准测试中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 贪婪搜索方法如GBFS和EHC在遇到启发式局部最小值或平台等无信息启发式区域(UHRs)时常常陷入困境，需要有效的逃离机制。

Method: 理论推导BrFS和RRWs逃离UHRs的期望运行时间，开发EHC-RRW变体使用随机游走替代广度优先搜索来逃离UHRs，并在PDDL规划基准上进行实验验证。

Result: 确定了RRWs比BrFS更快的条件，EHC-RRW在EHC有效的场景下具有强大的期望运行时间保证，实验验证了这些方法在逃离UHRs方面的相对有效性。

Conclusion: 重启随机游走(RRWs)在某些条件下比广度优先搜索(BrFS)能更有效地逃离无信息启发式区域，EHC-RRW变体在规划问题中表现出良好的性能。

Abstract: Greedy search methods like Greedy Best-First Search (GBFS) and Enforced Hill-Climbing (EHC) often struggle when faced with Uninformed Heuristic Regions (UHRs) like heuristic local minima or plateaus. In this work, we theoretically and empirically compare two popular methods for escaping UHRs in breadth-first search (BrFS) and restarting random walks (RRWs). We first derive the expected runtime of escaping a UHR using BrFS and RRWs, based on properties of the UHR and the random walk procedure, and then use these results to identify when RRWs will be faster in expectation than BrFS. We then evaluate these methods for escaping UHRs by comparing standard EHC, which uses BrFS to escape UHRs, to variants of EHC called EHC-RRW, which use RRWs for that purpose. EHC-RRW is shown to have strong expected runtime guarantees in cases where EHC has previously been shown to be effective. We also run experiments with these approaches on PDDL planning benchmarks to better understand their relative effectiveness for escaping UHRs.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [24] [Automated Hardware Trojan Insertion in Industrial-Scale Designs](https://arxiv.org/abs/2511.08703)
*Yaroslav Popryho,Debjit Pal,Inna Partin-Vaisband*

Main category: cs.CR

TL;DR: 本文提出了一种自动化、可扩展的方法，用于在工业规模网表中生成硬件木马(HT)类模式，以在不改变用户可见功能的情况下对检测工具进行压力测试。该方法通过解析门级设计、探索罕见区域和应用功能保持的图变换来合成触发-负载对，模拟隐蔽HT的统计特征。


<details>
  <summary>Details</summary>
Motivation: 工业SoC通常包含数十万到数百万个网表和数百万到数千万个连接边，使得在现实设计上对硬件木马检测器进行实证评估既必要又困难。公共基准测试仍然显著较小且是手工制作的，而发布真正的恶意RTL会带来伦理和操作风险。

Method: 该方法包括三个步骤：(i)将大型门级设计解析为连接图，(ii)使用SCOAP可测试性指标探索罕见区域，(iii)应用参数化、功能保持的图变换来合成模拟隐蔽HT统计特征的触发-负载对。

Result: 在本文生成的基准测试上评估时，代表性的最先进图学习模型未能检测到木马。

Conclusion: 该框架通过提供可重复的挑战实例，在不共享逐步攻击指令的情况下推进安全研究，从而缩小了学术电路与现代SoC之间的评估差距。

Abstract: Industrial Systems-on-Chips (SoCs) often comprise hundreds of thousands to millions of nets and millions to tens of millions of connectivity edges, making empirical evaluation of hardware-Trojan (HT) detectors on realistic designs both necessary and difficult. Public benchmarks remain significantly smaller and hand-crafted, while releasing truly malicious RTL raises ethical and operational risks. This work presents an automated and scalable methodology for generating HT-like patterns in industry-scale netlists whose purpose is to stress-test detection tools without altering user-visible functionality. The pipeline (i) parses large gate-level designs into connectivity graphs, (ii) explores rare regions using SCOAP testability metrics, and (iii) applies parameterized, function-preserving graph transformations to synthesize trigger-payload pairs that mimic the statistical footprint of stealthy HTs. When evaluated on the benchmarks generated in this work, representative state-of-the-art graph-learning models fail to detect Trojans. The framework closes the evaluation gap between academic circuits and modern SoCs by providing reproducible challenge instances that advance security research without sharing step-by-step attack instructions.

</details>


### [25] [Channel-Robust RFF for Low-Latency 5G Device Identification in SIMO Scenarios](https://arxiv.org/abs/2511.08902)
*Yingjie Sun,Guyue Li,Hongfu Chou,Aiqun Hu*

Main category: cs.CR

TL;DR: 本文提出了一种基于多天线信号的新型射频指纹提取技术，通过计算共时信道频率响应的对数线性增量比来解决多径问题，无需增加延迟即可实现96.13%的设备识别准确率。


<details>
  <summary>Details</summary>
Motivation: 5G超低延迟通信对设备识别提出了严格的时序要求，现有加密方案会增加计算开销和识别延迟。射频指纹识别虽然能降低延迟，但多径信道会影响其准确性，而现有的抗多径方法需要反馈或多时间点处理，会带来额外的信令延迟。

Method: 提出Log-Linear Delta Ratio (LLDR)方法，利用多接收天线的共时信道频率响应来提取射频指纹特征。将频带划分为子带，在每个子带内单独计算LLDR，以克服对最小信道变化的依赖。该方法避免了多时间采样，减少了采集时间。

Result: 在20路径信道和20dB信噪比条件下，对30个用户设备的识别准确率达到96.13%。使用Roofline模型评估理论延迟，空中接口延迟为0.491ms，满足超可靠低延迟通信的延迟要求。

Conclusion: 所提出的方案能够有效解决多径信道对射频指纹识别的影响，在保持高识别准确率的同时满足5G超低延迟通信的时序要求，为物理层设备识别提供了可行的解决方案。

Abstract: Ultra-low latency, the hallmark of fifth-generation mobile communications (5G), imposes exacting timing demands on identification as well. Current cryptographic solutions introduce additional computational overhead, which results in heightened identification delays. Radio frequency fingerprint (RFF) identifies devices at the physical layer, blocking impersonation attacks while significantly reducing latency. Unfortunately, multipath channels compromise RFF accuracy, and existing channel-resilient methods demand feedback or processing across multiple time points, incurring extra signaling latency. To address this problem, the paper introduces a new RFF extraction technique that employs signals from multiple receiving antennas to address multipath issues without adding latency. Unlike single-domain methods, the Log-Linear Delta Ratio (LLDR) of co-temporal channel frequency responses (CFRs) from multiple antennas is employed to preserve discriminative RFF features, eliminating multi-time sampling and reducing acquisition time. To overcome the challenge of the reliance on minimal channel variation, the frequency band is segmented into sub-bands, and the LLDR is computed within each sub-band individually. Simulation results indicate that the proposed scheme attains a 96.13% identification accuracy for 30 user equipments (UEs) within a 20-path channel under a signal-to-noise ratio (SNR) of 20 dB. Furthermore, we evaluate the theoretical latency using the Roofline model, resulting in the air interface latency of 0.491 ms, which satisfies ultra-reliable and low-latency communications (URLLC) latency requirements.

</details>


### [26] [iSeal: Encrypted Fingerprinting for Reliable LLM Ownership Verification](https://arxiv.org/abs/2511.08905)
*Zixun Xiong,Gaoyi Wu,Qingyang Yu,Mingyu Derek Ma,Lingfeng Yao,Miao Pan,Xiaojiang Du,Hao Wang*

Main category: cs.CR

TL;DR: iSeal是一种针对LLM知识产权保护的指纹方法，能够在攻击者完全控制模型推理过程的情况下进行可靠验证，通过模型和外部模块的特征注入、纠错机制和相似性验证策略来抵御验证时攻击。


<details>
  <summary>Details</summary>
Motivation: 由于LLM训练成本高昂，保护其知识产权变得至关重要。现有指纹方法在验证过程中容易受到攻击，当模型窃贼完全控制LLM推理过程时，这些方法会失效。

Method: iSeal通过在模型和外部模块中注入独特特征，结合纠错机制和基于相似性的验证策略，设计出能够抵抗验证时攻击的指纹系统。

Result: 在12个LLM上对抗10多种攻击时，iSeal实现了100%的指纹成功率，而基线方法在指纹遗忘和响应操纵攻击下失效。

Conclusion: iSeal是首个能够在模型窃贼端到端控制可疑LLM时进行可靠验证的指纹方法，通过理论分析和实证结果证明了其有效性。

Abstract: Given the high cost of large language model (LLM) training from scratch, safeguarding LLM intellectual property (IP) has become increasingly crucial. As the standard paradigm for IP ownership verification, LLM fingerprinting thus plays a vital role in addressing this challenge. Existing LLM fingerprinting methods verify ownership by extracting or injecting model-specific features. However, they overlook potential attacks during the verification process, leaving them ineffective when the model thief fully controls the LLM's inference process. In such settings, attackers may share prompt-response pairs to enable fingerprint unlearning or manipulate outputs to evade exact-match verification. We propose iSeal, the first fingerprinting method designed for reliable verification when the model thief controls the suspected LLM in an end-to-end manner. It injects unique features into both the model and an external module, reinforced by an error-correction mechanism and a similarity-based verification strategy. These components are resistant to verification-time attacks, including collusion-based fingerprint unlearning and response manipulation, backed by both theoretical analysis and empirical results. iSeal achieves 100 percent Fingerprint Success Rate (FSR) on 12 LLMs against more than 10 attacks, while baselines fail under unlearning and response manipulations.

</details>


### [27] [DeepTracer: Tracing Stolen Model via Deep Coupled Watermarks](https://arxiv.org/abs/2511.08985)
*Yunfei Yang,Xiaojun Chen,Yuexin Xuan,Zhendong Zhao,Xin Zhao,He Li*

Main category: cs.CR

TL;DR: 本文提出了DeepTracer框架，通过新颖的水印样本构建方法和同类耦合损失约束，在模型窃取攻击下实现鲁棒的水印保护。


<details>
  <summary>Details</summary>
Motivation: 现有模型水印技术在面对模型窃取攻击时容易被移除，导致模型所有者难以有效验证被盗模型的版权。

Method: 采用水印样本构建方法和同类耦合损失约束，使水印任务与主要任务高度耦合，迫使攻击者在窃取主要功能时不可避免地学习隐藏的水印任务。同时提出水印样本过滤机制，精心选择用于模型所有权验证的水印关键样本。

Result: 在多个数据集和模型上的广泛实验表明，该方法在防御各种模型窃取攻击和水印攻击方面超越了现有方法，达到了最先进的有效性和鲁棒性。

Conclusion: DeepTracer框架能够有效应对模型窃取攻击，为模型版权保护提供了可靠的解决方案。

Abstract: Model watermarking techniques can embed watermark information into the protected model for ownership declaration by constructing specific input-output pairs. However, existing watermarks are easily removed when facing model stealing attacks, and make it difficult for model owners to effectively verify the copyright of stolen models. In this paper, we analyze the root cause of the failure of current watermarking methods under model stealing scenarios and then explore potential solutions. Specifically, we introduce a robust watermarking framework, DeepTracer, which leverages a novel watermark samples construction method and a same-class coupling loss constraint. DeepTracer can incur a high-coupling model between watermark task and primary task that makes adversaries inevitably learn the hidden watermark task when stealing the primary task functionality. Furthermore, we propose an effective watermark samples filtering mechanism that elaborately select watermark key samples used in model ownership verification to enhance the reliability of watermarks. Extensive experiments across multiple datasets and models demonstrate that our method surpasses existing approaches in defending against various model stealing attacks, as well as watermark attacks, and achieves new state-of-the-art effectiveness and robustness.

</details>


### [28] [MedHE: Communication-Efficient Privacy-Preserving Federated Learning with Adaptive Gradient Sparsification for Healthcare](https://arxiv.org/abs/2511.09043)
*Farjana Yesmin*

Main category: cs.CR

TL;DR: MedHE框架结合自适应梯度稀疏化和CKKS同态加密，在医疗联邦学习中实现隐私保护，通信量减少97.5%，同时保持模型性能。


<details>
  <summary>Details</summary>
Motivation: 医疗联邦学习需要在资源受限的医疗机构间提供强隐私保护，同时保持计算效率。

Method: 采用自适应梯度稀疏化与CKKS同态加密相结合的方法，引入带误差补偿的动态阈值机制进行top-k梯度选择。

Result: 在5次独立试验中达到89.5±0.8%准确率，通信量从1277MB降至32MB/轮，与标准联邦学习性能相当(p=0.32)，满足ε≤1.0的差分隐私保证。

Conclusion: MedHE框架具有HIPAA合规性，可扩展至100+机构，为现实医疗部署提供实用可行的隐私保护解决方案。

Abstract: Healthcare federated learning requires strong privacy guarantees while maintaining computational efficiency across resource-constrained medical institutions. This paper presents MedHE, a novel framework combining adaptive gradient sparsification with CKKS homomorphic encryption to enable privacy-preserving collaborative learning on sensitive medical data. Our approach introduces a dynamic threshold mechanism with error compensation for top-k gradient selection, achieving 97.5 percent communication reduction while preserving model utility. We provide formal security analysis under Ring Learning with Errors assumptions and demonstrate differential privacy guarantees with epsilon less than or equal to 1.0. Statistical testing across 5 independent trials shows MedHE achieves 89.5 percent plus or minus 0.8 percent accuracy, maintaining comparable performance to standard federated learning (p=0.32) while reducing communication from 1277 MB to 32 MB per training round. Comprehensive evaluation demonstrates practical feasibility for real-world medical deployments with HIPAA compliance and scalability to 100 plus institutions.

</details>


### [29] [Attack-Centric by Design: A Program-Structure Taxonomy of Smart Contract Vulnerabilities](https://arxiv.org/abs/2511.09051)
*Parsa Hedayatnia,Tina Tavakkoli,Hadi Amini,Mohammad Allahbakhsh,Haleh Amintoosi*

Main category: cs.CR

TL;DR: 本文提出了一种基于攻击的Solidity漏洞分类法，将漏洞统一为8个根本原因家族，包括控制流、外部调用、状态完整性等，为研究人员和从业者提供一致的词汇表和实用检查清单。


<details>
  <summary>Details</summary>
Motivation: 智能合约将高价值资产和复杂逻辑集中在小型不可变程序中，即使是小错误也可能导致重大损失。现有的分类法和工具仍然分散，围绕症状而非结构原因组织。

Method: 引入基于攻击的程序结构分类法，将Solidity漏洞统一为8个根本原因家族，每个家族通过简洁的Solidity示例、利用机制和缓解措施进行说明，并与静态、动态和基于学习的工具的检测信号相关联。

Result: 分类法提供了统一的词汇表和实用检查清单，能够实现更可解释的检测、可重现的审计和结构化的安全教育。进一步将传统数据集映射到该分类法，揭示了标签漂移和覆盖范围差距。

Conclusion: 该分类法为研究人员和从业者提供了一致的词汇表和实用检查清单，实现了更可解释的检测、可重现的审计和结构化的安全教育。

Abstract: Smart contracts concentrate high value assets and complex logic in small, immutable programs, where even minor bugs can cause major losses. Existing taxonomies and tools remain fragmented, organized around symptoms such as reentrancy rather than structural causes. This paper introduces an attack-centric, program-structure taxonomy that unifies Solidity vulnerabilities into eight root-cause families covering control flow, external calls, state integrity, arithmetic safety, environmental dependencies, access control, input validation, and cross-domain protocol assumptions. Each family is illustrated through concise Solidity examples, exploit mechanics, and mitigations, and linked to the detection signals observable by static, dynamic, and learning-based tools. We further cross-map legacy datasets (SmartBugs, SolidiFI) to this taxonomy to reveal label drift and coverage gaps. The taxonomy provides a consistent vocabulary and practical checklist that enable more interpretable detection, reproducible audits, and structured security education for both researchers and practitioners.

</details>


### [30] [One Signature, Multiple Payments: Demystifying and Detecting Signature Replay Vulnerabilities in Smart Contracts](https://arxiv.org/abs/2511.09134)
*Zexu Wang,Jiachi Chen,Zewei Lin,Wenqing Chen,Kaiwen Ning,Jianxing Yu,Yuming Feng,Yu Zhang,Weizhe Zhang,Zibin Zheng*

Main category: cs.CR

TL;DR: 本文首次对智能合约中的签名重放漏洞(SRV)进行实证研究，提出了基于大语言模型的自动检测工具LASiR，在15,383个合约中发现SRV普遍存在，涉及476万美元活跃资产。


<details>
  <summary>Details</summary>
Motivation: 智能合约中数字签名验证缺乏使用条件检查会导致重复验证，增加权限滥用风险，威胁合约资产安全。现有研究缺乏对签名重放漏洞的系统性分析。

Method: 设计LASiR工具，结合大语言模型的语义理解能力辅助静态污点分析，识别签名重用行为，并通过符号执行进行路径可达性验证。

Result: 从37家区块链安全公司的1,419份审计报告中识别出108个SRV案例，分类为5种类型。在15,383个合约中发现SRV普遍存在，以太坊上19.63%使用签名的合约存在SRV，涉及476万美元资产。LASiR检测F1分数达87.90%。

Conclusion: 签名重放漏洞在智能合约中普遍存在且危害严重，LASiR结合LLM语义理解能有效提升检测性能，为智能合约安全提供重要保障。

Abstract: Smart contracts have significantly advanced blockchain technology, and digital signatures are crucial for reliable verification of contract authority. Through signature verification, smart contracts can ensure that signers possess the required permissions, thus enhancing security and scalability. However, lacking checks on signature usage conditions can lead to repeated verifications, increasing the risk of permission abuse and threatening contract assets. We define this issue as the Signature Replay Vulnerability (SRV). In this paper, we conducted the first empirical study to investigate the causes and characteristics of the SRVs. From 1,419 audit reports across 37 blockchain security companies, we identified 108 with detailed SRV descriptions and classified five types of SRVs. To detect these vulnerabilities automatically, we designed LASiR, which utilizes the general semantic understanding ability of Large Language Models (LLMs) to assist in the static taint analysis of the signature state and identify the signature reuse behavior. It also employs path reachability verification via symbolic execution to ensure effective and reliable detection. To evaluate the performance of LASiR, we conducted large-scale experiments on 15,383 contracts involving signature verification, selected from the initial dataset of 918,964 contracts across four blockchains: Ethereum, Binance Smart Chain, Polygon, and Arbitrum. The results indicate that SRVs are widespread, with affected contracts holding $4.76 million in active assets. Among these, 19.63% of contracts that use signatures on Ethereum contain SRVs. Furthermore, manual verification demonstrates that LASiR achieves an F1-score of 87.90% for detection. Ablation studies and comparative experiments reveal that the semantic information provided by LLMs aids static taint analysis, significantly enhancing LASiR's detection performance.

</details>


### [31] [Improving Sustainability of Adversarial Examples in Class-Incremental Learning](https://arxiv.org/abs/2511.09088)
*Taifeng Liu,Xinjing Liu,Liangqiu Dong,Yang Liu,Yilong Yang,Zhuo Ma*

Main category: cs.CR

TL;DR: 本文提出SAE方法，旨在增强对抗样本在类增量学习(CIL)更新后的可持续性，通过语义修正和过滤增强模块来应对领域漂移问题。


<details>
  <summary>Details</summary>
Motivation: 当前对抗样本通常针对静态模型设计，但在类增量学习场景下，模型会不断更新，导致现有对抗样本因显著的领域漂移而失效。

Method: 提出SAE方法，包含语义修正模块和过滤增强模块。语义修正模块利用视觉语言模型生成通用语义，并结合CIL模型修正对抗样本语义优化方向；过滤增强模块识别潜在空间中具有目标类语义的非目标样本并进行增强。

Result: 综合实验表明，SAE在类别数量增加9倍的情况下，平均性能比基线方法高出31.28%。

Conclusion: SAE能够有效增强对抗样本在类增量学习更新后的可持续性，通过语义泛化和稳定性提升来应对领域漂移挑战。

Abstract: Current adversarial examples (AEs) are typically designed for static models. However, with the wide application of Class-Incremental Learning (CIL), models are no longer static and need to be updated with new data distributed and labeled differently from the old ones. As a result, existing AEs often fail after CIL updates due to significant domain drift. In this paper, we propose SAE to enhance the sustainability of AEs against CIL. The core idea of SAE is to enhance the robustness of AE semantics against domain drift by making them more similar to the target class while distinguishing them from all other classes. Achieving this is challenging, as relying solely on the initial CIL model to optimize AE semantics often leads to overfitting. To resolve the problem, we propose a Semantic Correction Module. This module encourages the AE semantics to be generalized, based on a visual-language model capable of producing universal semantics. Additionally, it incorporates the CIL model to correct the optimization direction of the AE semantics, guiding them closer to the target class. To further reduce fluctuations in AE semantics, we propose a Filtering-and-Augmentation Module, which first identifies non-target examples with target-class semantics in the latent space and then augments them to foster more stable semantics. Comprehensive experiments demonstrate that SAE outperforms baselines by an average of 31.28% when updated with a 9-fold increase in the number of classes.

</details>


### [32] [Unveiling Hidden Threats: Using Fractal Triggers to Boost Stealthiness of Distributed Backdoor Attacks in Federated Learning](https://arxiv.org/abs/2511.09252)
*Jian Wang,Hong Shen,Chan-Tong Lam*

Main category: cs.CR

TL;DR: 本文提出了一种基于分形特征的分布式后门攻击方法FTDBA，利用分形自相似性增强子触发器的特征强度，显著降低了达到相同攻击强度所需的投毒数据量，并通过动态角度扰动机制平衡攻击效率和隐蔽性。


<details>
  <summary>Details</summary>
Motivation: 传统分布式后门攻击通过将全局触发器分解为子触发器来提高隐蔽性，但这需要更多投毒数据来维持攻击强度，增加了暴露风险。为了克服这一缺陷，需要开发一种低暴露、高效率的攻击方法。

Method: 提出FTDBA方法，利用分形的自相似性增强子触发器的特征强度；引入动态角度扰动机制，在训练阶段自适应调整扰动强度以平衡效率和隐蔽性。

Result: 实验表明FTDBA仅需传统DBA方法62.4%的投毒数据量就能达到92.3%的攻击成功率，同时检测率降低22.8%，KL散度降低41.2%。

Conclusion: 该研究为联邦后门攻击提供了一种低暴露、高效率的范式，并扩展了分形特征在对抗样本生成中的应用。

Abstract: Traditional distributed backdoor attacks (DBA) in federated learning improve stealthiness by decomposing global triggers into sub-triggers, which however requires more poisoned data to maintian the attck strength and hence increases the exposure risk. To overcome this defect, This paper proposes a novel method, namely Fractal-Triggerred Distributed Backdoor Attack (FTDBA), which leverages the self-similarity of fractals to enhance the feature strength of sub-triggers and hence significantly reduce the required poisoning volume for the same attack strength. To address the detectability of fractal structures in the frequency and gradient domains, we introduce a dynamic angular perturbation mechanism that adaptively adjusts perturbation intensity across the training phases to balance efficiency and stealthiness. Experiments show that FTDBA achieves a 92.3\% attack success rate with only 62.4\% of the poisoning volume required by traditional DBA methods, while reducing the detection rate by 22.8\% and KL divergence by 41.2\%. This study presents a low-exposure, high-efficiency paradigm for federated backdoor attacks and expands the application of fractal features in adversarial sample generation.

</details>


### [33] [Quantum Meet-in-the-Middle Attacks on Key-Length Extension Constructions](https://arxiv.org/abs/2511.09351)
*Min Liang,Ruihao Gao,Jiali Wu*

Main category: cs.CR

TL;DR: 本文提出了针对两种密钥长度扩展构造的量子中间相遇攻击：对2kTE的两种Q2模型攻击（基于量子爪查找和Grover算法），以及对3XCE的Q1模型攻击。还扩展了量子筛选中相遇攻击框架，适用于更广泛的构造。


<details>
  <summary>Details</summary>
Motivation: 研究密钥长度扩展技术在量子计算模型下的安全性，评估现有KLE构造在量子攻击下的脆弱性，为量子安全密码设计提供参考。

Method: 采用量子中间相遇攻击方法，结合量子爪查找算法、Grover算法和量子筛选中相遇攻击技术，针对2kTE和3XCE构造设计具体攻击方案。

Result: 对2kTE的攻击在Q2模型下分别达到O(2^{2κ/3})和O(2^{κ/2})时间复杂度；对3XCE的Q1模型攻击达到O(2^{(κ+n)/2})时间复杂度，相比经典攻击实现二次加速。

Conclusion: 2kTE在Q2模型下无法提供安全增强，3XCE在Q1模型下存在安全风险，量子SITM攻击框架可应用于更广泛的密码分析场景。

Abstract: Key-length extension (KLE) techniques provide a general approach to enhancing the security of block ciphers by using longer keys. There are mainly two classes of KLE techniques, cascade encryption and XOR-cascade encryption. This paper presents several quantum meet-in-the-middle (MITM) attacks against two specific KLE constructions.
  For the two-key triple encryption (2kTE), we propose two quantum MITM attacks under the Q2 model. The first attack, leveraging the quantum claw-finding (QCF) algorithm, achieves a time complexity of $O(2^{2κ/3})$ with $O(2^{2κ/3})$ quantum random access memory (QRAM). The second attack, based on Grover's algorithm, achieves a time complexity of $O(2^{κ/2})$ with $O(2^κ)$ QRAM. The latter complexity is nearly identical to Grover-based brute-force attack on the underlying block cipher, indicating that 2kTE does not enhance security under the Q2 model when sufficient QRAM resources are available.
  For the 3XOR-cascade encryption (3XCE), we propose a quantum MITM attack applicable to the Q1 model. This attack requires no QRAM and has a time complexity of $O(2^{(κ+n)/2})$ ($κ$ and $n$ are the key length and block length of the underlying block cipher, respectively.), achieving a quadratic speedup over classical MITM attack.
  Furthermore, we extend the quantum MITM attack to quantum sieve-in-the-middle (SITM) attack, which is applicable for more constructions. We present a general quantum SITM framework for the construction $ELE=E^2\circ L\circ E^1$ and provide specific attack schemes for three different forms of the middle layer $L$. The quantum SITM attack technique can be further applied to a broader range of quantum cryptanalysis scenarios.

</details>


### [34] [Intelligent Carrier Allocation: A Cross-Modal Reasoning Framework for Adaptive Multimodal Steganography](https://arxiv.org/abs/2511.09552)
*Abhirup Das,Pranav Dudani,Shruti Sharma,Ravi Kumar C.*

Main category: cs.CR

TL;DR: 提出了一种基于跨模态推理引擎的智能载体分配框架，通过分析图像、音频和文本等多种载体的熵值、信号复杂度和词汇丰富度等指标，为每种模态生成可靠性评分，并据此自适应地分配秘密比特流，提高隐写系统的安全性和抗检测能力。


<details>
  <summary>Details</summary>
Motivation: 传统隐写方法通常固定且仅适用于单一载体类型，无法适应多种媒体类型，导致系统安全性不足且容易被检测。需要一种能够智能评估不同载体并自适应分配秘密数据的框架。

Method: 开发跨模态推理引擎，分析图像、音频和文本等多种载体的熵值、信号复杂度和词汇丰富度等指标，生成可靠性评分，并基于评分智能分配秘密比特流，优先分配给更强更复杂的载体。

Result: 该推理方法相比静态非自适应多模态技术具有更高的安全性和数据保护能力，能够构建更强大、更智能的秘密通信系统。

Conclusion: 基于跨模态推理的智能载体分配框架能够显著提高隐写系统的安全性和抗检测能力，为构建更强大的秘密通信系统提供了有效解决方案。

Abstract: In today's digital world, which has many different types of media, steganography, the art of secret communication, has a lot of problems to deal with. Traditional methods are often fixed and only work with one type of carrier media. This means they don't work well with all the different types of media that are out there. This system doesn't send data to "weak" or easily detectable carriers because it can't adapt. This makes the system less safe and less secret in general. This paper proposes a novel Intelligent Carrier Allocation framework founded on a Cross-Modal Reasoning (CMR) Engine. This engine looks at a wide range of carriers, such as images, audio, and text, to see if they are good for steganography. It uses important measurements like entropy, signal complexity, and vocabulary richness to come up with a single reliability score for each modality. The framework uses these scores to fairly and intelligently share the secret bitstream, giving more data to carriers that are thought to be stronger and more complex. This adaptive allocation strategy makes the system as hard to find as possible and as strong as possible against steganalysis. We demonstrate that this reasoning-based approach is more secure and superior in data protection compared to static, non-adaptive multimodal techniques. This makes it possible to build stronger and smarter secret communication systems.

</details>


### [35] [Differentially Private Rankings via Outranking Methods and Performance Data Aggregation](https://arxiv.org/abs/2511.09120)
*Luis Del Vasto-Terrientes*

Main category: cs.CR

TL;DR: 本文提出了一种将多准则决策(MCDM)排序方法与差分隐私(DP)相结合的方法，在保护个人贡献隐私的同时进行排名决策。


<details>
  <summary>Details</summary>
Motivation: 随着MCDM方法在动态和数据驱动领域(如推荐系统)的应用扩展，敏感数据的处理和隐私保护变得至关重要，但目前隐私机制与MCDM方法的集成还不够成熟。

Method: 采用预处理步骤将多个用户评估聚合为综合性能矩阵，并将MCDM排序方法与差分隐私相结合。

Result: 评估结果显示真实排名与其匿名对应物之间存在强到非常强的统计相关性，同时确保了强大的隐私参数保证。

Conclusion: 该方法成功实现了在保护个人隐私的同时进行有效的多准则决策排名，为数据驱动的决策系统提供了隐私保护解决方案。

Abstract: Multiple-Criteria Decision Making (MCDM) is a sub-discipline of Operations Research that helps decision-makers in choosing, ranking, or sorting alternatives based on conflicting criteria. Over time, its application has been expanded into dynamic and data-driven domains, such as recommender systems. In these contexts, the availability and handling of personal and sensitive data can play a critical role in the decision-making process. Despite this increased reliance on sensitive data, the integration of privacy mechanisms with MCDM methods is underdeveloped. This paper introduces an integrated approach that combines MCDM outranking methods with Differential Privacy (DP), safeguarding individual contributions' privacy in ranking problems. This approach relies on a pre-processing step to aggregate multiple user evaluations into a comprehensive performance matrix. The evaluation results show a strong to very strong statistical correlation between the true rankings and their anonymized counterparts, ensuring robust privacy parameter guarantees.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [36] [Triage in Software Engineering: A Systematic Review of Research and Practice](https://arxiv.org/abs/2511.08607)
*Yongxin Zhao,Shenglin Zhang,Yujia Wu,Yuxin Sun,Yongqian Sun,Dan Pei,Chetan Bansal,Minghua Ma*

Main category: cs.SE

TL;DR: 本文对2004年至今的234篇论文进行了全面综述，深入探讨了软件系统故障分诊的基本概念、系统架构和问题陈述，总结了开源数据集和评估指标，并指出了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 随着软件系统复杂性增加，故障分诊已成为系统运维的基本过程。海量异构数据使得有效分诊对于维护可靠性、促进可维护性和实现快速问题响应变得不可或缺。

Method: 通过比较学术和工业研究的不同目标，分析工业实践的实证研究，识别限制分诊系统实际部署的主要障碍，并总结广泛采用的开源数据集和评估指标。

Result: 提供了故障分诊的统一视角，识别了实际部署中的主要障碍，为从业者提供了方法选择和性能评估的指导。

Conclusion: 概述了未来潜在方向和新兴机遇，以促进学术创新与工业应用更紧密的整合，所有综述论文和项目可在指定GitHub仓库获取。

Abstract: As modern software systems continue to grow in complexity, triage has become a fundamental process in system operations and maintenance. Triage aims to efficiently prioritize, assign, and assess issues to ensure the reliability of complex environments. The vast amount of heterogeneous data generated by software systems has made effective triage indispensable for maintaining reliability, facilitating maintainability, and enabling rapid issue response. Motivated by these challenges, researchers have devoted extensive effort to advancing triage automation and have achieved significant progress over the past two decades. This survey provides a comprehensive review of 234 papers from 2004 to the present, offering an in-depth examination of the fundamental concepts, system architecture, and problem statement. By comparing the distinct goals of academic and industrial research and by analyzing empirical studies of industrial practices, we identify the major obstacles that limit the practical deployment of triage systems. To assist practitioners in method selection and performance evaluation, we summarize widely adopted open-source datasets and evaluation metrics, providing a unified perspective on the measurement of triage effectiveness. Finally, we outline potential future directions and emerging opportunities to foster a closer integration between academic innovation and industrial application. All reviewed papers and projects are available at https://github.com/AIOps-Lab-NKU/TriageSurvey.

</details>


### [37] [Energy Consumption of Dataframe Libraries for End-to-End Deep Learning Pipelines:A Comparative Analysis](https://arxiv.org/abs/2511.08644)
*Punit Kumar,Asif Imran,Tevfik Kosar*

Main category: cs.SE

TL;DR: 对Pandas、Polars和Dask三个Python数据操作库在深度学习训练和推理管道中的性能进行对比分析，重点关注它们与GPU工作负载的交互表现。


<details>
  <summary>Details</summary>
Motivation: 现有文献缺乏对这些数据操作库在深度学习完整管道中与GPU工作负载交互的研究，特别是在数据加载、预处理和批次输入等关键阶段。

Method: 测量了包括运行时间、内存使用、磁盘使用和能耗（CPU和GPU）在内的关键性能指标，使用不同的机器学习模型和数据集进行测试。

Result: 研究提供了三个库在深度学习管道中的详细性能对比数据，涵盖了多个维度的性能指标。

Conclusion: 该研究填补了数据操作库在深度学习完整工作流中性能评估的空白，为选择合适的数据处理工具提供了实证依据。

Abstract: This paper presents a detailed comparative analysis of the performance of three major Python data manipulation libraries - Pandas, Polars, and Dask - specifically when embedded within complete deep learning (DL) training and inference pipelines. The research bridges a gap in existing literature by studying how these libraries interact with substantial GPU workloads during critical phases like data loading, preprocessing, and batch feeding. The authors measured key performance indicators including runtime, memory usage, disk usage, and energy consumption (both CPU and GPU) across various machine learning models and datasets.

</details>


### [38] [An insight into the technical debt-fix trade off in software backporting](https://arxiv.org/abs/2511.09000)
*Jarin Tasnim,Debasish Chakroborti,Chanchal K. Roy,Kevin A. Schneider*

Main category: cs.SE

TL;DR: 本研究分析了87个仓库中31,076个回传源的105,396次提交，发现约4.3%的回传会引入新的技术债务，不同生态系统和开发阶段的技术债务模式存在差异。


<details>
  <summary>Details</summary>
Motivation: 研究回传维护活动中技术债务的产生情况，识别在稳定源代码回传过程中何时以及为何会产生新的技术债务。

Method: 分析了三个软件生态系统（Apache、Eclipse和Python）中87个仓库的31,076个回传源的105,396次提交，通过量化分析识别技术债务引入模式。

Result: 约4.3%的回传引入了新的技术债务；Apache贡献了最多的绝对实例，而Python和Eclipse的债务提交比几乎是Apache的三倍；不同生态系统在不同发布周期阶段的技术债务积累模式不同；经验不足、工作负荷高或非所有者的开发者更可能在回传中引入技术债务。

Conclusion: 回传维护活动确实会引入技术债务，其模式和程度因生态系统、发布周期阶段和开发者特征而异，需要针对性地管理回传过程以减少技术债务的引入。

Abstract: Maintaining software is an ongoing process that stretches beyond the initial release. Stable software versions continuously evolve to fix bugs, add improvements, address security issues, and ensure compatibility. This ongoing support involves Backporting, which means taking a fix or update from a newer version and applying it to an older version of the same software. As software versions evolve, new technical debt can arise during backport maintenance activities. This study examines the technical debt involved in fixing 105,396 commits from 31,076 backport sources across 87 repositories in three software ecosystems (Apache, Eclipse, and Python). The goal is to identify when and why new technical debt arises during backporting in stable source code. Our results indicate that approximately 4.3% of backports introduce new technical debt. Apache contributes the most absolute instances, while Python and Eclipse exhibit nearly three times higher debt-to-commit ratios than Apache. Feature migrations make older Apache releases debt-prone in the early phase, whereas Python and Eclipse releases tend to accumulate technical debt mostly during the middle phase of their release cycles. Additionally, developers who are inexperienced, under high workloads, or non-owners are more likely to introduce technical debt during backporting.

</details>


### [39] [Test Plan Generation for Live Testing of Cloud Services](https://arxiv.org/abs/2511.09038)
*Oussama Jebbar,Ferhat Khendek,Maria Toeroe*

Main category: cs.SE

TL;DR: 提出自动化测试计划生成方法，旨在减少生产环境中测试活动可能引起的服务中断


<details>
  <summary>Details</summary>
Motivation: 手动设计测试计划繁琐且容易出错，特别是在大型复杂系统中更为困难，需要自动化方法来避免测试活动对生产流量的干扰

Method: 提出自动化测试计划生成方法，包括测试配置选择/生成、部署规划、测试运行调度以及干扰风险缓解策略选择等任务

Result: 通过案例研究说明了该方法的不同方面，展示了自动化测试计划生成的实际应用

Conclusion: 自动化测试计划生成方法能够有效减少生产环境中测试活动引起的服务中断，提高测试效率并降低人为错误

Abstract: Live testing is performed in the production environment ideally without causing unacceptable disturbance to the production traffic. Thus, test activities have to be orchestrated properly to avoid interferences with the production traffic. A test plan is the road map that specifies how the test activities need to be orchestrated. Developing a test plan includes tasks such as test configuration selection/generation, test configuration deployment planning, creating the test runs schedule, choosing strategies to mitigate the risk of interferences, etc. The manual design of a test plan is tedious and error prone. This task becomes harder especially when the systems are large and complex. In this paper we propose an approach for automating test plans generation. With this approach we aim at reducing service disruption that may be induced by the testing activities in production. We illustrate our approach with a case study and discuss its different aspects.

</details>


### [40] [Vendor-Aware Industrial Agents: RAG-Enhanced LLMs for Secure On-Premise PLC Code Generation](https://arxiv.org/abs/2511.09122)
*Joschka Kersting,Michael Rummel,Gesa Benndorf*

Main category: cs.SE

TL;DR: 本文提出了一种面向工业领域的低数据领域编程助手解决方案，通过检索增强生成(RAG)技术，在不微调大模型的情况下实现高质量代码生成，支持边缘设备使用的小模型微调。


<details>
  <summary>Details</summary>
Motivation: 可编程逻辑控制器使用专有代码方言，难以训练编码助手；现有LLM不了解特定功能块和相关项目代码；企业不信任云服务提供商，需要本地化解决方案。

Method: 采用检索增强生成(RAG)技术，结合广泛提示工程和定向检索，让多个AI模型相互竞争，使用推理能力，自动修正错误，并在聊天界面中直接编译代码验证有效性。

Result: 通过广泛的评估显示，该工具能够提供代码编译统计和用户评分，证明RAG支持的编码助手可以在低数据领域有效工作。

Conclusion: 在低数据领域，通过检索增强生成技术结合提示工程和定向检索，可以实现高质量的代码生成，满足工业应用需求。

Abstract: Programmable Logic Controllers are operated by proprietary code dialects; this makes it challenging to train coding assistants. Current LLMs are trained on large code datasets and are capable of writing IEC 61131-3 compatible code out of the box, but they neither know specific function blocks, nor related project code. Moreover, companies like Mitsubishi Electric and their customers do not trust cloud providers. Hence, an own coding agent is the desired solution to cope with this. In this study, we present our work on a low-data domain coding assistant solution for industrial use. We show how we achieved high quality code generation without fine-tuning large models and by fine-tuning small local models for edge device usage. Our tool lets several AI models compete with each other, uses reasoning, corrects bugs automatically and checks code validity by compiling it directly in the chat interface. We support our approach with an extensive evaluation that comes with code compilation statistics and user ratings. We found that a Retrieval-Augmented Generation (RAG) supported coding assistant can work in low-data domains by using extensive prompt engineering and directed retrieval.

</details>


### [41] [Leveraging Self-Paced Learning for Software Vulnerability Detection](https://arxiv.org/abs/2511.09212)
*Zeru Cheng,Yanjing Yang,He Zhang,Lanxin Yang,Jinghao Hu,Jinwei Xu,Bohan Liu,Haifeng Shen*

Main category: cs.SE

TL;DR: SPLVD是一种基于自步学习的软件漏洞检测方法，通过动态选择训练数据模拟人类从易到难的学习过程，在多个数据集上取得了优于现有方法的效果。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习漏洞检测方法准确率有限，主要原因是训练数据质量低。需要改进训练数据选择策略来提高检测性能。

Method: 提出SPLVD方法，包含专门设计的数据选择器，在每轮训练前重新计算源代码难度，选择新的训练数据并更新选择器，模拟人类从易到难的学习过程。

Result: 在三个基准数据集（超过239K源代码，其中25K有漏洞）上，SPLVD分别达到89.2%、68.7%和43.5%的最高F1分数。在OpenHarmony项目上达到90.9%的最高精确率。

Conclusion: SPLVD通过自步学习策略有效提升了软件漏洞检测性能，在标准评估和实际新生态系统上都表现出优越性能。

Abstract: Software vulnerabilities are major risks to software systems. Recently, researchers have proposed many deep learning approaches to detect software vulnerabilities. However, their accuracy is limited in practice. One of the main causes is low-quality training data (i.e., source code). To this end, we propose a new approach: SPLVD (Self-Paced Learning for Software Vulnerability Detection). SPLVD dynamically selects source code for model training based on the stage of training, which simulates the human learning process progressing from easy to hard. SPLVD has a data selector that is specifically designed for the vulnerability detection task, which enables it to prioritize the learning of easy source code. Before each training epoch, SPLVD uses the data selector to recalculate the difficulty of the source code, select new training source code, and update the data selector. When evaluating SPLVD, we first use three benchmark datasets with over 239K source code in which 25K are vulnerable for standard evaluations. Experimental results demonstrate that SPLVD achieves the highest F1 of 89.2%, 68.7%, and 43.5%, respectively, outperforming the state-of-the-art approaches. Then we collect projects from OpenHarmony, a new ecosystem that has not been learned by general LLMs, to evaluate SPLVD further. SPLVD achieves the highest precision of 90.9%, demonstrating its practical effectiveness.

</details>


### [42] [AILINKPREVIEWER: Enhancing Code Reviews with LLM-Powered Link Previews](https://arxiv.org/abs/2511.09223)
*Panya Trakoolgerntong,Tao Xiao,Masanari Kondo,Chaiyong Ragkhitwetsagul,Morakot Choetkiertikul,Pattaraporn Sangaroonsilp,Yasutaka Kamei*

Main category: cs.SE

TL;DR: AILINKPREVIEWER工具利用LLM生成PR中链接的预览，通过结合PR元数据提供更丰富的上下文信息，提升代码审查效率。


<details>
  <summary>Details</summary>
Motivation: 当前代码审查中，PR中的链接信息在自动化任务中被丢弃，导致信息不完整，增加评审者的认知负担和上下文切换成本。

Method: 分析50个GitHub仓库，比较三种方法：上下文LLM摘要、非上下文LLM摘要和基于元数据的预览，使用BLEU、BERTScore和压缩率等指标评估。

Result: 上下文摘要在指标上表现最佳，但用户研究显示大多数参与者更喜欢非上下文摘要，表明指标性能与用户体验之间存在权衡。

Conclusion: LLM驱动的链接预览有潜力提升代码审查效率，为开发者和自动化工具提供更丰富的上下文信息。

Abstract: Code review is a key practice in software engineering, where developers evaluate code changes to ensure quality and maintainability. Links to issues and external resources are often included in Pull Requests (PRs) to provide additional context, yet they are typically discarded in automated tasks such as PR summarization and code review comment generation. This limits the richness of information available to reviewers and increases cognitive load by forcing context-switching. To address this gap, we present AILINKPREVIEWER, a tool that leverages Large Language Models (LLMs) to generate previews of links in PRs using PR metadata, including titles, descriptions, comments, and link body content. We analyzed 50 engineered GitHub repositories and compared three approaches: Contextual LLM summaries, Non-Contextual LLM summaries, and Metadata-based previews. The results in metrics such as BLEU, BERTScore, and compression ratio show that contextual summaries consistently outperform other methods. However, in a user study with seven participants, most preferred non-contextual summaries, suggesting a trade-off between metric performance and perceived usability. These findings demonstrate the potential of LLM-powered link previews to enhance code review efficiency and to provide richer context for developers and automation in software engineering.
  The video demo is available at https://www.youtube.com/watch?v=h2qH4RtrB3E, and the tool and its source code can be found at https://github.com/c4rtune/AILinkPreviewer.

</details>


### [43] [Leveraging Large Language Models for Use Case Model Generation from Software Requirements](https://arxiv.org/abs/2511.09231)
*Tobias Eisenreich,Nicholas Friedlaender,Stefan Wagner*

Main category: cs.SE

TL;DR: 本研究探索使用大型语言模型辅助用例建模，通过集成开源LLM和高级提示工程技术从软件需求中提取参与者和用例，相比传统手动方法建模时间减少60%，同时保持模型质量。


<details>
  <summary>Details</summary>
Motivation: 用例建模采用用户中心场景来概述系统需求，有助于相关利益相关者达成共识。但由于手动创建用例模型耗时费力，实践中常被跳过。本研究探索LLM在辅助这一繁琐过程中的潜力。

Method: 提出的方法集成开源权重LLM，通过高级提示工程技术从软件需求中系统提取参与者和用例。采用探索性研究，由五名专业软件工程师比较传统手动建模与LLM方法。

Result: 结果显示建模时间大幅加速，减少60%。同时模型质量保持同等水平。参与者表示该方法在过程中提供了有价值的指导。

Conclusion: LLM辅助的用例建模方法显著提高了建模效率，同时保持质量，为软件需求工程提供了有价值的自动化支持。

Abstract: Use case modeling employs user-centered scenarios to outline system requirements. These help to achieve consensus among relevant stakeholders. Because the manual creation of use case models is demanding and time-consuming, it is often skipped in practice. This study explores the potential of Large Language Models (LLMs) to assist in this tedious process. The proposed method integrates an open-weight LLM to systematically extract actors and use cases from software requirements with advanced prompt engineering techniques. The method is evaluated using an exploratory study conducted with five professional software engineers, which compares traditional manual modeling to the proposed LLM-based approach. The results show a substantial acceleration, reducing the modeling time by 60\%. At the same time, the model quality remains on par. Besides improving the modeling efficiency, the participants indicated that the method provided valuable guidance in the process.

</details>


### [44] [Decoding the Configuration of AI Coding Agents: Insights from Claude Code Projects](https://arxiv.org/abs/2511.09268)
*Helio Victor F. Santos,Vitor Costa,Joao Eduardo Montandon,Marco Tulio Valente*

Main category: cs.SE

TL;DR: 本文对Claude Code智能代码助手的配置生态系统进行了实证研究，分析了328个公开配置文件，揭示了这些配置文件中定义的软件工程关注点和实践，特别强调了架构规范的重要性。


<details>
  <summary>Details</summary>
Motivation: 智能代码助手的行为和效果严重依赖于定义架构约束、编码实践和工具使用策略的配置文件，但目前对这些配置工件的结构和内容了解甚少。

Method: 收集并分析了来自公共Claude Code项目的328个配置文件，识别了(i)它们指定的软件工程关注点和实践，以及(ii)这些关注点在单个文件中的共现模式。

Result: 研究结果强调了在智能体配置文件中定义广泛关注点和实践的重要性，特别是指定智能体应遵循的架构。

Conclusion: 配置文件中需要明确定义各种软件工程关注点和实践，其中架构规范尤为关键，这对智能代码助手的有效运行至关重要。

Abstract: Agentic code assistants are a new generation of AI systems capable of performing end-to-end software engineering tasks. While these systems promise unprecedented productivity gains, their behavior and effectiveness depend heavily on configuration files that define architectural constraints, coding practices, and tool usage policies. However, little is known about the structure and content of these configuration artifacts. This paper presents an empirical study of the configuration ecosystem of Claude Code, one of the most widely used agentic coding systems. We collected and analyzed 328 configuration files from public Claude Code projects to identify (i) the software engineering concerns and practices they specify and (ii) how these concerns co-occur within individual files. The results highlight the importance of defining a wide range of concerns and practices in agent configuration files, with particular emphasis on specifying the architecture the agent should follow.

</details>


### [45] [Routesplain: Towards Faithful and Intervenable Routing for Software-related Tasks](https://arxiv.org/abs/2511.09373)
*Adam Štorek,Vikas Upadhyay,Marianne Menglin Liu,Daniel W. Peterson,Anshul Mittal,Sujeeth Bharadwaj,Fahad Shah,Dan Roth*

Main category: cs.SE

TL;DR: Routesplain是一个专为软件相关任务设计的LLM路由系统，通过提取人类可解释的概念来智能选择最适合的LLM，在准确性和成本方面都优于单个模型，并达到或超过所有黑盒基线。


<details>
  <summary>Details</summary>
Motivation: 当前LLM在软件相关任务中表现差异显著，通过路由用户查询到合适的LLM可以提高响应质量并降低成本，但现有工作主要关注通用LLM路由。

Method: Routesplain首先从每个查询中提取人类可解释的概念（如任务类型、领域、推理复杂度），然后仅基于这些概念进行路由，提供可理解的、可信的推理过程。

Result: 在16个最先进的LLM和8个软件相关任务上的评估显示，Routesplain在准确性和成本方面都优于单个模型，并等于或超过所有黑盒基线。

Conclusion: Routesplain是第一个专门针对软件相关任务的LLM路由器，通过概念级干预为路由器进一步改进提供了途径。

Abstract: LLMs now tackle a wide range of software-related tasks, yet we show that their performance varies markedly both across and within these tasks. Routing user queries to the appropriate LLMs can therefore help improve response quality while reducing cost. Prior work, however, has focused mainly on general-purpose LLM routing via black-box models. We introduce Routesplain, the first LLM router for software-related tasks, including multilingual code generation and repair, input/output prediction, and computer science QA. Unlike existing routing approaches, Routesplain first extracts human-interpretable concepts from each query (e.g., task, domain, reasoning complexity) and only routes based on these concepts, thereby providing intelligible, faithful rationales. We evaluate Routesplain on 16 state-of-the-art LLMs across eight software-related tasks; Routesplain outperforms individual models both in terms of accuracy and cost, and equals or surpasses all black-box baselines, with concept-level intervention highlighting avenues for further router improvements.

</details>
