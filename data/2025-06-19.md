<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 8]
- [cs.CR](#cs.CR) [Total: 24]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [An Empirical Study of Bugs in Data Visualization Libraries](https://arxiv.org/abs/2506.15084)
*Weiqi Lu,Yongqiang Tian,Xiaohan Zhong,Haoyang Ma,Zhenyang Xu,Shing-Chi Cheung,Chengnian Sun*

Main category: cs.SE

TL;DR: 该研究首次全面分析了数据可视化库中的错误，收集了564个错误并系统分析了其症状和根本原因，提出了详细的分类法。研究发现错误/不准确的图表普遍存在，主要原因是图形计算错误，并提出了八种触发步骤和两种测试预言。此外，探索了视觉语言模型（VLMs）在检测错误图表中的可行性，发现其效果因提示而异。


<details>
  <summary>Details</summary>
Motivation: 数据可视化库的准确性对用户体验和决策至关重要，但其中的视觉错误可能误导用户，因此需要深入理解这些错误的特性以改进库的质量。

Method: 研究收集了五个广泛使用的数据可视化库中的564个错误，系统分析了其症状和根本原因，并提出了分类法。同时探索了VLMs在检测错误图表中的应用。

Result: 研究发现错误/不准确的图表普遍存在，图形计算错误是主要原因。VLMs的检测效果在29%到57%之间，提示内容对效果影响显著。

Conclusion: 研究为数据可视化库的错误检测和修复提供了重要见解，并指出了未来自动化测试技术的研究方向。

Abstract: Data visualization (DataViz) libraries play a crucial role in presentation,
data analysis, and application development, underscoring the importance of
their accuracy in transforming data into visual representations. Incorrect
visualizations can adversely impact user experience, distort information
conveyance, and influence user perception and decision-making processes. Visual
bugs in these libraries can be particularly insidious as they may not cause
obvious errors like crashes, but instead mislead users of the underlying data
graphically, resulting in wrong decision making. Consequently, a good
understanding of the unique characteristics of bugs in DataViz libraries is
essential for researchers and developers to detect and fix bugs in DataViz
libraries.
  This study presents the first comprehensive analysis of bugs in DataViz
libraries, examining 564 bugs collected from five widely-used libraries. Our
study systematically analyzes their symptoms and root causes, and provides a
detailed taxonomy. We found that incorrect/inaccurate plots are pervasive in
DataViz libraries and incorrect graphic computation is the major root cause,
which necessitates further automated testing methods for DataViz libraries.
Moreover, we identified eight key steps to trigger such bugs and two test
oracles specific to DataViz libraries, which may inspire future research in
designing effective automated testing techniques. Furthermore, with the recent
advancements in Vision Language Models (VLMs), we explored the feasibility of
applying these models to detect incorrect/inaccurate plots. The results show
that the effectiveness of VLMs in bug detection varies from 29% to 57%,
depending on the prompts, and adding more information in prompts does not
necessarily increase the effectiveness. More findings can be found in our
manuscript.

</details>


### [2] [Program Feature-based Fuzzing Benchmarking](https://arxiv.org/abs/2506.15088)
*Miao Miao*

Main category: cs.SE

TL;DR: 该论文提出了一种新的基准测试方法，通过可配置的细粒度程序特征来评估模糊测试工具的性能。


<details>
  <summary>Details</summary>
Motivation: 传统模糊测试评估通常关注整体性能，而忽略了细粒度程序特征对测试效果的影响，因此需要一种更精细的评估方法。

Method: 通过分析25项灰盒模糊测试研究，提取了7个与控制和数据流相关的程序特征，生成了包含153个程序的基准测试集，并评估了11种流行模糊测试工具。

Result: 结果显示，模糊测试工具的性能受程序特征及其强度影响显著，表明程序特征在评估中的重要性。

Conclusion: 研究强调了在模糊测试评估中考虑程序特征的必要性，并提出了一种有效的基准测试方法。

Abstract: Fuzzing is a powerful software testing technique renowned for its
effectiveness in identifying software vulnerabilities. Traditional fuzzing
evaluations typically focus on overall fuzzer performance across a set of
target programs, yet few benchmarks consider how fine-grained program features
influence fuzzing effectiveness. To bridge this gap, we introduce a novel
benchmark designed to generate programs with configurable, fine-grained program
features to enhance fuzzing evaluations. We reviewed 25 recent grey-box fuzzing
studies, extracting 7 program features related to control-flow and data-flow
that can impact fuzzer performance. Using these features, we generated a
benchmark consisting of 153 programs controlled by 10 fine-grained configurable
parameters. We evaluated 11 popular fuzzers using this benchmark. The results
indicate that fuzzer performance varies significantly based on the program
features and their strengths, highlighting the importance of incorporating
program characteristics into fuzzing evaluations.

</details>


### [3] [Enhancement Report Approval Prediction: A Comparative Study of Large Language Models](https://arxiv.org/abs/2506.15098)
*Haosheng Zuo,Feifei Niu,Chuanyi Li*

Main category: cs.SE

TL;DR: 论文研究了利用大语言模型（LLM）提升增强报告（ER）审批预测（ERAP）的准确性，比较了18种LLM与传统方法，发现LoRA微调的Llama 3.1 8B Instruct表现最佳，准确率达79%，并解决了类别不平衡问题。


<details>
  <summary>Details</summary>
Motivation: 增强报告（ER）是用户与开发者之间的重要沟通渠道，但手动处理资源密集且效率低，因此需要自动化解决方案。

Method: 系统评估了18种LLM（包括BERT、RoBERTa、GPT等）与传统方法（CNN/LSTM-BERT/GloVe），并引入创建者档案和LoRA微调技术。

Result: LoRA微调的Llama 3.1 8B Instruct准确率达79%，召回率显著提升（76.1% vs. 64.1%），优于传统方法5%。

Conclusion: LLM在ERAP中表现优越，能优化软件维护流程，但需进一步研究其局限性。

Abstract: Enhancement reports (ERs) serve as a critical communication channel between
users and developers, capturing valuable suggestions for software improvement.
However, manually processing these reports is resource-intensive, leading to
delays and potential loss of valuable insights. To address this challenge,
enhancement report approval prediction (ERAP) has emerged as a research focus,
leveraging machine learning techniques to automate decision-making. While
traditional approaches have employed feature-based classifiers and deep
learning models, recent advancements in large language models (LLM) present new
opportunities for enhancing prediction accuracy. This study systematically
evaluates 18 LLM variants (including BERT, RoBERTa, DeBERTa-v3, ELECTRA, and
XLNet for encoder models; GPT-3.5-turbo, GPT-4o-mini, Llama 3.1 8B, Llama 3.1
8B Instruct and DeepSeek-V3 for decoder models) against traditional methods
(CNN/LSTM-BERT/GloVe). Our experiments reveal two key insights: (1)
Incorporating creator profiles increases unfine-tuned decoder-only models'
overall accuracy by 10.8 percent though it may introduce bias; (2) LoRA
fine-tuned Llama 3.1 8B Instruct further improve performance, reaching 79
percent accuracy and significantly enhancing recall for approved reports (76.1
percent vs. LSTM-GLOVE's 64.1 percent), outperforming traditional methods by 5
percent under strict chronological evaluation and effectively addressing class
imbalance issues. These findings establish LLM as a superior solution for ERAP,
demonstrating their potential to streamline software maintenance workflows and
improve decision-making in real-world development environments. We also
investigated and summarized the ER cases where the large models underperformed,
providing valuable directions for future research.

</details>


### [4] [Towards Bug-Free Distributed Go Programs](https://arxiv.org/abs/2506.15135)
*Zhengqun Koo*

Main category: cs.SE

TL;DR: 本文提出了一种验证框架，用于证明使用Go语言子集的分布式程序中不存在通信竞争。


<details>
  <summary>Details</summary>
Motivation: 分布式系统中的程序员需要处理并发问题以避免竞争，但并发推理困难，通信竞争会导致接收错误消息或无消息的问题。

Method: 通过静态分析分布式程序的执行，扩展了happens-before顺序以涵盖缓冲和非缓冲通道。

Result: 该框架能够证明程序中没有通信竞争。

Conclusion: 该验证框架为分布式程序提供了一种静态分析通信竞争的方法，扩展了happens-before顺序的应用范围。

Abstract: Programmers of distributed systems need to reason about concurrency to avoid
races. However, reasoning about concurrency is difficult, and unexpected races
show up as bugs. Data race detection in shared memory systems is well-studied
(dynamic data race detection [13], behavioral types [15], dynamic race
detection [31]). Similar to how a data race consists of reads and writes not
related by happens-before at a shared memory location, a communication race
consists of receives and sends not related by happens-before on a shared
channel. Communication races are problematic: a receiver expects a specific
message from a specific sender, but with a communication race, the receiver can
receive a message meant for another receiver, or not receive anything at all.
In this work, we describe a verification framework that can prove the absence
of communication races for distributed programs that use a subset of the Go
programming language, where synchronization is mainly achieved via message
passing. We statically reason about how a distributed program executes, using a
happens-before order, extended to buffered and unbuffered channels.

</details>


### [5] [Advanced approach for Agile/Scrum Process: RetroAI++](https://arxiv.org/abs/2506.15172)
*Maria Spichkova,Kevin Iwan,Madeleine Zwart,Hina Lee,Yuwon Yoon,Xiaohan Qin*

Main category: cs.SE

TL;DR: RetroAI++是一个基于智能技术的原型工具，旨在自动化并优化敏捷/Scrum开发中的冲刺计划和回顾分析。


<details>
  <summary>Details</summary>
Motivation: 支持软件开发者在敏捷/Scrum项目中的冲刺计划和回顾活动，提升效率和智能化水平。

Method: 利用AI技术开发RetroAI++原型工具，自动化冲刺计划、开发和回顾阶段的多项流程，并提供智能建议和深入分析。

Result: RetroAI++能够为冲刺组织提供智能建议，并为回顾反思提供有意义的洞察。

Conclusion: RetroAI++通过AI技术优化了敏捷/Scrum开发的实践应用，提升了项目管理的智能化水平。

Abstract: In Agile/Scrum software development, sprint planning and retrospective
analysis are the key elements of project management. The aim of our work is to
support software developers in these activities. In this paper, we present our
prototype tool RetroAI++, based on emerging intelligent technologies. In our
RetroAI++ prototype, we aim to automate and refine the practical application of
Agile/Scrum processes within Sprint Planning and Retrospectives. Leveraging AI
insights, our prototype aims to automate and refine the many processes involved
in the Sprint Planning, Development and Retrospective stages of Agile/Scrum
development projects, offering intelligent suggestions for sprint organisation
as well as meaningful insights for retrospective reflection.

</details>


### [6] [Large Language Models for Unit Testing: A Systematic Literature Review](https://arxiv.org/abs/2506.15227)
*Quanjun Zhang,Chunrong Fang,Siqi Gu,Ye Shang,Zhenyu Chen,Liang Xiao*

Main category: cs.SE

TL;DR: 本文首次系统综述了截至2025年3月大语言模型（LLMs）在单元测试中的应用，分析了相关论文，总结了现有成果、挑战和未来方向。


<details>
  <summary>Details</summary>
Motivation: 随着LLMs的快速发展，其在单元测试中的应用日益广泛，但缺乏系统性总结，研究者难以全面了解现状与未来机会。

Method: 通过文献综述，从单元测试和LLMs两个角度分析相关论文，分类现有任务（如测试生成、预言生成），并讨论模型使用、适应策略和混合方法等关键方面。

Result: 总结了LLMs在单元测试中的现有成果、未解决的关键挑战，并提出了未来研究方向。

Conclusion: 本文为单元测试社区提供了系统性综述，帮助研究者全面了解现状并推动未来研究，相关资源已公开在GitHub。

Abstract: Unit testing is a fundamental practice in modern software engineering, with
the aim of ensuring the correctness, maintainability, and reliability of
individual software components. Very recently, with the advances in Large
Language Models (LLMs), a rapidly growing body of research has leveraged LLMs
to automate various unit testing tasks, demonstrating remarkable performance
and significantly reducing manual effort. However, due to ongoing explorations
in the LLM-based unit testing field, it is challenging for researchers to
understand existing achievements, open challenges, and future opportunities.
This paper presents the first systematic literature review on the application
of LLMs in unit testing until March 2025. We analyze \numpaper{} relevant
papers from the perspectives of both unit testing and LLMs. We first categorize
existing unit testing tasks that benefit from LLMs, e.g., test generation and
oracle generation. We then discuss several critical aspects of integrating LLMs
into unit testing research, including model usage, adaptation strategies, and
hybrid approaches. We further summarize key challenges that remain unresolved
and outline promising directions to guide future research in this area.
Overall, our paper provides a systematic overview of the research landscape to
the unit testing community, helping researchers gain a comprehensive
understanding of achievements and promote future research. Our artifacts are
publicly available at the GitHub repository:
https://github.com/iSEngLab/AwesomeLLM4UT.

</details>


### [7] [Uncovering Intention through LLM-Driven Code Snippet Description Generation](https://arxiv.org/abs/2506.15453)
*Yusuf Sulistyo Nugroho,Farah Danisha Salam,Brittany Reid,Raula Gaikovina Kula,Kazumasa Shimari,Kenichi Matsumoto*

Main category: cs.SE

TL;DR: 研究探讨了开发者常用的代码片段描述类型，并评估了Llama模型在生成描述方面的表现。基于NPM代码片段数据集，研究发现大多数描述为示例用途，LLM能准确识别此类描述，但生成的描述仍有改进空间。


<details>
  <summary>Details</summary>
Motivation: 代码片段文档化对开发者和用户至关重要，尤其是第三方库。研究旨在了解常见描述类型，并评估LLM（如Llama）在生成描述时的效果。

Method: 使用NPM代码片段数据集（185,412个包，1,024,579个片段），选取400个样本进行手动分类和LLM评估。

Result: 55.5%的原始描述为示例用途，LLM准确识别率为79.75%。生成的描述相似度平均为0.7173，显示相关性但需改进。

Conclusion: 代码片段的文档意图因任务而异，LLM在生成描述时表现良好但仍有优化空间。

Abstract: Documenting code snippets is essential to pinpoint key areas where both
developers and users should pay attention. Examples include usage examples and
other Application Programming Interfaces (APIs), which are especially important
for third-party libraries. With the rise of Large Language Models (LLMs), the
key goal is to investigate the kinds of description developers commonly use and
evaluate how well an LLM, in this case Llama, can support description
generation. We use NPM Code Snippets, consisting of 185,412 packages with
1,024,579 code snippets. From there, we use 400 code snippets (and their
descriptions) as samples. First, our manual classification found that the
majority of original descriptions (55.5%) highlight example-based usage. This
finding emphasizes the importance of clear documentation, as some descriptions
lacked sufficient detail to convey intent. Second, the LLM correctly identified
the majority of original descriptions as "Example" (79.75%), which is identical
to our manual finding, showing a propensity for generalization. Third, compared
to the originals, the produced description had an average similarity score of
0.7173, suggesting relevance but room for improvement. Scores below 0.9
indicate some irrelevance. Our results show that depending on the task of the
code snippet, the intention of the document may differ from being instructions
for usage, installations, or descriptive learning examples for any user of a
library.

</details>


### [8] [cAST: Enhancing Code Retrieval-Augmented Generation with Structural Chunking via Abstract Syntax Tree](https://arxiv.org/abs/2506.15655)
*Yilin Zhang,Xinran Zhao,Zora Zhiruo Wang,Chenyang Yang,Jiayi Wei,Tongshuang Wu*

Main category: cs.SE

TL;DR: 论文提出了一种基于抽象语法树（AST）的代码分块方法（\ourwork），以解决现有基于行的分块方法破坏语义结构的问题，显著提升了代码生成任务的表现。


<details>
  <summary>Details</summary>
Motivation: 现有的基于行的代码分块方法容易破坏代码的语义结构（如拆分函数或合并无关代码），从而降低生成质量。因此，需要一种结构感知的分块方法。

Method: 提出了一种基于抽象语法树（AST）的分块方法，递归地将大型AST节点分解为较小的块，并在大小限制内合并兄弟节点，生成语义连贯的单元。

Result: 该方法在多种代码生成任务中表现优异，例如在RepoEval检索任务中Recall@5提升了4.3点，在SWE-bench生成任务中Pass@1提升了2.67点。

Conclusion: 结构感知的分块方法对提升检索增强的代码智能至关重要。

Abstract: Retrieval-Augmented Generation (RAG) has become essential for large-scale
code generation, grounding predictions in external code corpora to improve
actuality. However, a critical yet underexplored aspect of RAG pipelines is
chunking -- the process of dividing documents into retrievable units. Existing
line-based chunking heuristics often break semantic structures, splitting
functions or merging unrelated code, which can degrade generation quality. We
propose chunking via Abstract Syntax Trees (\ourwork), a structure-aware method
that recursively breaks large AST nodes into smaller chunks and merges sibling
nodes while respecting size limits. This approach generates self-contained,
semantically coherent units across programming languages and tasks, improving
performance on diverse code generation tasks, e.g., boosting Recall@5 by 4.3
points on RepoEval retrieval and Pass@1 by 2.67 points on SWE-bench generation.
Our work highlights the importance of structure-aware chunking for scaling
retrieval-enhanced code intelligence.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [9] [Winter Soldier: Backdooring Language Models at Pre-Training with Indirect Data Poisoning](https://arxiv.org/abs/2506.14913)
*Wassim Bouaziz,Mathurin Videau,Nicolas Usunier,El-Mahdi El-Mhamdi*

Main category: cs.CR

TL;DR: 论文提出了一种间接数据投毒方法，通过梯度优化提示调优，使语言模型学习秘密序列，从而在不影响性能的情况下保护数据集并追踪其使用。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLM）的训练依赖于难以管理的大规模文本数据，现有方法依赖数据记忆，但模型提供者试图限制这一点。本文旨在探索一种不依赖记忆的间接数据投毒方法。

Method: 使用基于梯度优化的提示调优技术，使模型学习训练数据中不存在的秘密提示和响应序列。

Result: 实验表明，仅需少于0.005%的投毒标记即可使模型学习秘密，并以极高置信度（p < 10^{-55}）检测到，且不影响模型性能。

Conclusion: 间接数据投毒是一种可行且有效的方法，可用于数据集保护和追踪，且无需依赖数据记忆。

Abstract: The pre-training of large language models (LLMs) relies on massive text
datasets sourced from diverse and difficult-to-curate origins. Although
membership inference attacks and hidden canaries have been explored to trace
data usage, such methods rely on memorization of training data, which LM
providers try to limit. In this work, we demonstrate that indirect data
poisoning (where the targeted behavior is absent from training data) is not
only feasible but also allow to effectively protect a dataset and trace its
use. Using gradient-based optimization prompt-tuning, we make a model learn
arbitrary secret sequences: secret responses to secret prompts that are absent
from the training corpus. We validate our approach on language models
pre-trained from scratch and show that less than 0.005% of poisoned tokens are
sufficient to covertly make a LM learn a secret and detect it with extremely
high confidence ($p < 10^{-55}$) with a theoretically certifiable scheme.
Crucially, this occurs without performance degradation (on LM benchmarks) and
despite secrets never appearing in the training set.

</details>


### [10] [Fair Data Exchange with Constant-Time Proofs](https://arxiv.org/abs/2506.14944)
*Majid Khabbazian*

Main category: cs.CR

TL;DR: 本文提出了一种改进的Fair Data Exchange (FDE)协议，通过将文件视为Reed-Solomon码字并加密，显著降低了证明和验证的计算开销，同时保持了公平性。


<details>
  <summary>Details</summary>
Motivation: 现有FDE协议的证明和验证时间随文件长度线性增长，效率较低，需要优化。

Method: 将文件视为Reed-Solomon码字，扩展为低速率码并加密，仅对少量随机密文子集进行验证，利用RS解码修复错误。

Result: 显著降低了计算开销，同时保持了客户端和服务器的公平性，通信冗余可调。

Conclusion: 改进后的协议在效率和公平性上均取得显著提升，并支持比特币链下运行。

Abstract: The Fair Data Exchange (FDE) protocol introduced at CCS 2024 offers atomic
pay-per-file transfers with constant-size proofs, but its prover and verifier
runtimes still scale linearly with the file length n. We collapse these costs
to essentially constant by viewing the file as a rate-1 Reed-Solomon (RS)
codeword, extending it to a lower-rate RS code with constant redundancy,
encrypting this extended vector, and then proving correctness for only a small
random subset of the resulting ciphertexts; RS decoding repairs any corrupted
symbols with negligible failure probability. Our protocol preserves full
client- and server-fairness, and adds only a tunable communication redundancy
overhead.
  Finally, we patch the elliptic-curve mismatch in the Bitcoin instantiation of
FDE with a compact zk-SNARK, enabling the entire exchange to run off-chain and
falling back to just two on-chain transactions when channels are unavailable.

</details>


### [11] [Narrowing the Gap between TEEs Threat Model and Deployment Strategies](https://arxiv.org/abs/2506.14964)
*Filip Rezabek,Jonathan Passerat-Palmbach,Moe Mahhouk,Frieder Erdmann,Andrew Miller*

Main category: cs.CR

TL;DR: 论文探讨了机密虚拟机（CVM）在物理层面保护不足的问题，提出通过扩展TEE认证绑定CVM与云提供商以增强安全性。


<details>
  <summary>Details</summary>
Motivation: 当前CVM的威胁模型未涵盖物理攻击和侧信道攻击，用户无法准确评估风险，需依赖可信云提供商。

Method: 提出利用受保护平台标识符（PPID）扩展TEE认证，将CVM与提供商绑定。

Result: 现有TEE实现和认证流程的多样性导致验证、迁移和应用开发困难。

Conclusion: 需解决TEE认证的扩展和强化问题，以推动CVM的广泛应用。

Abstract: Confidential Virtual Machines (CVMs) provide isolation guarantees for data in
use, but their threat model does not include physical level protection and
side-channel attacks. Therefore, current deployments rely on trusted cloud
providers to host the CVMs' underlying infrastructure. However, TEE
attestations do not provide information about the operator hosting a CVM.
Without knowing whether a Trusted Execution Environment (TEE) runs within a
provider's infrastructure, a user cannot accurately assess the risks of
physical attacks. We observe a misalignment in the threat model where the
workloads are protected against other tenants but do not offer end-to-end
security assurances to external users without relying on cloud providers. The
attestation should be extended to bind the CVM with the provider. A possible
solution can rely on the Protected Platform Identifier (PPID), a unique CPU
identifier. However, the implementation details of various TEE manufacturers,
attestation flows, and providers vary. This makes verification of attestations,
ease of migration, and building applications without relying on a trusted party
challenging, highlighting a key limitation that must be addressed for the
adoption of CVMs. We discuss two points focusing on hardening and extensions of
TEEs' attestation.

</details>


### [12] [Private Continual Counting of Unbounded Streams](https://arxiv.org/abs/2506.15018)
*Ben Jacobsen,Kassem Fawaz*

Main category: cs.CR

TL;DR: 论文研究了在输入规模未知的情况下，如何实现差分隐私的持续计数问题，提出了一种基于对数扰动的新型矩阵分解方法，显著降低了误差和计算复杂度。


<details>
  <summary>Details</summary>
Motivation: 现有基于矩阵机制的最优算法需要预先知道输入规模，而常见的‘倍增技巧’会导致误差不理想且不光滑。因此，需要一种新方法来解决这一问题。

Method: 提出了一种基于对数扰动的新型矩阵分解方法，利用函数1/√(1-z)的变形，实现了光滑误差和高效计算。

Result: 算法能够在O(t)空间和O(log t)每轮时间下，以O(log^{2+2α}(t))的方差估计前t个数据点的和，性能优于现有方法。

Conclusion: 新算法在理论和实际性能上均优于现有方法，尤其是在输入规模未知的情况下表现突出。

Abstract: We study the problem of differentially private continual counting in the
unbounded setting where the input size $n$ is not known in advance. Current
state-of-the-art algorithms based on optimal instantiations of the matrix
mechanism cannot be directly applied here because their privacy guarantees only
hold when key parameters are tuned to $n$. Using the common `doubling trick'
avoids knowledge of $n$ but leads to suboptimal and non-smooth error. We solve
this problem by introducing novel matrix factorizations based on logarithmic
perturbations of the function $\frac{1}{\sqrt{1-z}}$ studied in prior works,
which may be of independent interest. The resulting algorithm has smooth error,
and for any $\alpha > 0$ and $t\leq n$ it is able to privately estimate the sum
of the first $t$ data points with $O(\log^{2+2\alpha}(t))$ variance. It
requires $O(t)$ space and amortized $O(\log t)$ time per round, compared to
$O(\log(n)\log(t))$ variance, $O(n)$ space and $O(n \log n)$ pre-processing
time for the nearly-optimal bounded-input algorithm of Henzinger et al. (SODA
2023). Empirically, we find that our algorithm's performance is also comparable
to theirs in absolute terms: our variance is less than $1.5\times$ theirs for
$t$ as large as $2^{24}$.

</details>


### [13] [Systems-Theoretic and Data-Driven Security Analysis in ML-enabled Medical Devices](https://arxiv.org/abs/2506.15028)
*Gargi Mitra,Mohammadreza Hallajiyan,Inji Kim,Athish Pranav Dharmalingam,Mohammed Elnawawy,Shahrear Iqbal,Karthik Pattabiraman,Homa Alemzadeh*

Main category: cs.CR

TL;DR: 论文探讨了AI/ML在医疗设备中的网络安全风险，提出了一套工具和技术，以帮助制造商在设计阶段嵌入安全原则。


<details>
  <summary>Details</summary>
Motivation: AI/ML在医疗设备中的应用带来了显著的网络安全风险，可能威胁患者安全，因此需在设计阶段解决这些问题。

Method: 通过分析设备召回、不良事件和已知漏洞数据，开发了一套工具和技术，用于全面的上市前风险评估。

Result: 提出了一套工具和技术，帮助制造商在设计阶段嵌入网络安全原则，提高设备安全性。

Conclusion: 强调在AI/ML医疗设备的上市前阶段解决网络安全问题的紧迫性，以确保患者安全。

Abstract: The integration of AI/ML into medical devices is rapidly transforming
healthcare by enhancing diagnostic and treatment facilities. However, this
advancement also introduces serious cybersecurity risks due to the use of
complex and often opaque models, extensive interconnectivity, interoperability
with third-party peripheral devices, Internet connectivity, and vulnerabilities
in the underlying technologies. These factors contribute to a broad attack
surface and make threat prevention, detection, and mitigation challenging.
Given the highly safety-critical nature of these devices, a cyberattack on
these devices can cause the ML models to mispredict, thereby posing significant
safety risks to patients. Therefore, ensuring the security of these devices
from the time of design is essential. This paper underscores the urgency of
addressing the cybersecurity challenges in ML-enabled medical devices at the
pre-market phase. We begin by analyzing publicly available data on device
recalls and adverse events, and known vulnerabilities, to understand the threat
landscape of AI/ML-enabled medical devices and their repercussions on patient
safety. Building on this analysis, we introduce a suite of tools and techniques
designed by us to assist security analysts in conducting comprehensive
premarket risk assessments. Our work aims to empower manufacturers to embed
cybersecurity as a core design principle in AI/ML-enabled medical devices,
thereby making them safe for patients.

</details>


### [14] [MECHA: Multithreaded and Efficient Cryptographic Hardware Access](https://arxiv.org/abs/2506.15034)
*Pratama Derry,Laksmono Agus Mahardika Ari,Iqbal Muhammad,Howon Kim*

Main category: cs.CR

TL;DR: MECHA是一种高效的多线程加密硬件访问方案，通过UNIX域套接字管理多应用请求，提升加密操作速度和效率。


<details>
  <summary>Details</summary>
Motivation: 传统加密接口设计存在上下文切换开销，MECHA旨在消除这一瓶颈，提供更高效的并发加密操作解决方案。

Method: MECHA采用多线程架构，包括服务器线程、客户端线程、收发线程及发送/接收队列，支持多种通信协议。

Result: 实验结果显示，MECHA比传统接口设计在并发加密请求处理速度上提升了83%。

Conclusion: MECHA在云计算和物联网等安全通信领域具有广泛应用潜力，为并发加密操作提供了更高效的解决方案。

Abstract: This paper presents a multithread and efficient cryptographic hardware access
(MECHA) for efficient and fast cryptographic operations that eliminates the
need for context switching. Utilizing a UNIX domain socket, MECHA manages
multiple requests from multiple applications simultaneously, resulting in
faster processing and improved efficiency. We comprise several key components,
including the Server thread, Client thread, Transceiver thread, and a pair of
Sender and Receiver queues. MECHA design is portable and can be used with any
communication protocol, with experimental results demonstrating a 83% increase
in the speed of concurrent cryptographic requests compared to conventional
interface design. MECHA architecture has significant potential in the field of
secure communication applications ranging from cloud computing to the IoT,
offering a faster and more efficient solution for managing multiple
cryptographic operation requests concurrently.

</details>


### [15] [deepSURF: Detecting Memory Safety Vulnerabilities in Rust Through Fuzzing LLM-Augmented Harnesses](https://arxiv.org/abs/2506.15648)
*Georgios Androutsopoulos,Antonio Bianchi*

Main category: cs.CR

TL;DR: deepSURF是一个结合静态分析和LLM引导的模糊测试工具，用于检测Rust库中的内存安全漏洞，特别是在不安全代码中。


<details>
  <summary>Details</summary>
Motivation: 现有工具在检测Rust内存漏洞时能力有限，无法充分处理Rust特有类型或依赖人工干预。

Method: deepSURF通过替换泛型为自定义类型并生成特质实现，结合LLM动态增强模糊测试用例，探索复杂API交互。

Result: 在27个真实Rust库中测试，成功复现20个已知漏洞并发现6个新漏洞，优于现有工具。

Conclusion: deepSURF显著提升了Rust内存漏洞检测能力，特别是在处理泛型和复杂API交互方面。

Abstract: Although Rust ensures memory safety by default, it also permits the use of
unsafe code, which can introduce memory safety vulnerabilities if misused.
Unfortunately, existing tools for detecting memory bugs in Rust typically
exhibit limited detection capabilities, inadequately handle Rust-specific
types, or rely heavily on manual intervention.
  To address these limitations, we present deepSURF, a tool that integrates
static analysis with Large Language Model (LLM)-guided fuzzing harness
generation to effectively identify memory safety vulnerabilities in Rust
libraries, specifically targeting unsafe code. deepSURF introduces a novel
approach for handling generics by substituting them with custom types and
generating tailored implementations for the required traits, enabling the
fuzzer to simulate user-defined behaviors within the fuzzed library.
Additionally, deepSURF employs LLMs to augment fuzzing harnesses dynamically,
facilitating exploration of complex API interactions and significantly
increasing the likelihood of exposing memory safety vulnerabilities. We
evaluated deepSURF on 27 real-world Rust crates, successfully rediscovering 20
known memory safety bugs and uncovering 6 previously unknown vulnerabilities,
demonstrating clear improvements over state-of-the-art tools.

</details>


### [16] [Advanced Prediction of Hypersonic Missile Trajectories with CNN-LSTM-GRU Architectures](https://arxiv.org/abs/2506.15043)
*Amir Hossein Baradaran*

Main category: cs.CR

TL;DR: 提出了一种结合CNN、LSTM和GRU的混合深度学习模型，用于高精度预测高超音速导弹的复杂轨迹，为防御策略提供支持。


<details>
  <summary>Details</summary>
Motivation: 高超音速导弹因其高速和机动性对防御系统构成重大挑战，准确预测其轨迹是有效拦截的关键。

Method: 采用CNN、LSTM和GRU的混合深度学习模型，结合各架构优势进行轨迹预测。

Result: 模型成功实现了高超音速导弹轨迹的高精度预测。

Conclusion: 研究表明先进机器学习技术能显著提升防御系统的预测能力。

Abstract: Advancements in the defense industry are paramount for ensuring the safety
and security of nations, providing robust protection against emerging threats.
Among these threats, hypersonic missiles pose a significant challenge due to
their extreme speeds and maneuverability, making accurate trajectory prediction
a critical necessity for effective countermeasures. This paper addresses this
challenge by employing a novel hybrid deep learning approach, integrating
Convolutional Neural Networks (CNNs), Long Short-Term Memory (LSTM) networks,
and Gated Recurrent Units (GRUs). By leveraging the strengths of these
architectures, the proposed method successfully predicts the complex
trajectories of hypersonic missiles with high accuracy, offering a significant
contribution to defense strategies and missile interception technologies. This
research demonstrates the potential of advanced machine learning techniques in
enhancing the predictive capabilities of defense systems.

</details>


### [17] [Toward a Lightweight, Scalable, and Parallel Secure Encryption Engine](https://arxiv.org/abs/2506.15070)
*Rasha Karakchi,Rye Stahle-Smith,Nishant Chinnasami,Tiffany Yu*

Main category: cs.CR

TL;DR: SPiME是一种轻量级、可扩展且兼容FPGA的安全处理器内存加密架构，通过将AES-128直接集成到内存处理框架中，解决了传统CPU加密方法的性能瓶颈和数据移动问题。


<details>
  <summary>Details</summary>
Motivation: 物联网（IoT）应用的快速增长对边缘计算的高效、高吞吐量和节能数据处理提出了更高要求，传统CPU加密方法在延迟敏感和资源受限环境中表现不佳。

Method: SPiME设计为一个模块化的并行内存处理单元阵列，每个单元结合AES核心和最小控制单元，实现分布式就地加密。架构用Verilog实现并在多款AMD FPGA上测试。

Result: SPiME可扩展至4000多个并行单元，高端FPGA资源利用率低于5%，持续加密吞吐量超过25Gbps，具有可预测的低延迟性能。

Conclusion: SPiME的便携性、可配置性和资源效率使其成为安全边缘计算、嵌入式加密系统和可定制硬件加速器的理想解决方案。

Abstract: The exponential growth of Internet of Things (IoT) applications has
intensified the demand for efficient, high-throughput, and energy-efficient
data processing at the edge. Conventional CPU-centric encryption methods suffer
from performance bottlenecks and excessive data movement, especially in
latency-sensitive and resource-constrained environments. In this paper, we
present SPiME, a lightweight, scalable, and FPGA-compatible Secure
Processor-in-Memory Encryption architecture that integrates the Advanced
Encryption Standard (AES-128) directly into a Processing-in-Memory (PiM)
framework. SPiME is designed as a modular array of parallel PiM units, each
combining an AES core with a minimal control unit to enable distributed
in-place encryption with minimal overhead. The architecture is fully
implemented in Verilog and tested on multiple AMD UltraScale and UltraScale+
FPGAs. Evaluation results show that SPiME can scale beyond 4,000 parallel units
while maintaining less than 5\% utilization of key FPGA resources on high-end
devices. It delivers over 25~Gbps in sustained encryption throughput with
predictable, low-latency performance. The design's portability,
configurability, and resource efficiency make it a compelling solution for
secure edge computing, embedded cryptographic systems, and customizable
hardware accelerators.

</details>


### [18] [CWGAN-GP Augmented CAE for Jamming Detection in 5G-NR in Non-IID Datasets](https://arxiv.org/abs/2506.15075)
*Samhita Kuili,Mohammadreza Amini,Burak Kantarci*

Main category: cs.CR

TL;DR: 论文提出了一种基于卷积自编码器（CAE）的方法，用于检测5G-NR无线蜂窝网络中的空中干扰攻击，并通过生成对抗网络（CWGAN-GP）平衡数据集，最终在性能上优于基准模型。


<details>
  <summary>Details</summary>
Motivation: 5G-NR网络中干扰攻击频发，影响信号质量，需要一种高效的检测方法。

Method: 使用卷积自编码器（CAE）检测干扰，并通过CWGAN-GP平衡数据集。

Result: CAE在干扰检测中表现出色，平均精度97.33%，召回率91.33%，F1分数94.08%，准确率94.35%。

Conclusion: CAE在复杂数据环境下具有鲁棒性，优于其他基准模型。

Abstract: In the ever-expanding domain of 5G-NR wireless cellular networks,
over-the-air jamming attacks are prevalent as security attacks, compromising
the quality of the received signal. We simulate a jamming environment by
incorporating additive white Gaussian noise (AWGN) into the real-world In-phase
and Quadrature (I/Q) OFDM datasets. A Convolutional Autoencoder (CAE) is
exploited to implement a jamming detection over various characteristics such as
heterogenous I/Q datasets; extracting relevant information on Synchronization
Signal Blocks (SSBs), and fewer SSB observations with notable class imbalance.
Given the characteristics of datasets, balanced datasets are acquired by
employing a Conv1D conditional Wasserstein Generative Adversarial
Network-Gradient Penalty(CWGAN-GP) on both majority and minority SSB
observations. Additionally, we compare the performance and detection ability of
the proposed CAE model on augmented datasets with benchmark models:
Convolutional Denoising Autoencoder (CDAE) and Convolutional Sparse Autoencoder
(CSAE). Despite the complexity of data heterogeneity involved across all
datasets, CAE depicts the robustness in detection performance of jammed signal
by achieving average values of 97.33% precision, 91.33% recall, 94.08%
F1-score, and 94.35% accuracy over CDAE and CSAE.

</details>


### [19] [Flexible Hardware-Enabled Guarantees for AI Compute](https://arxiv.org/abs/2506.15093)
*James Petrie,Onni Aarne,Nora Ammann,David Dalrymple*

Main category: cs.CR

TL;DR: 提出了一种名为flexHEGs的硬件保障系统，用于解决AI发展中的国际安全风险，支持隐私保护的验证和治理。


<details>
  <summary>Details</summary>
Motivation: 随着AI系统日益强大，其对国际安全的威胁增加，现有治理方法难以在不泄露敏感信息或国家安全的情况下解决问题。

Method: flexHEGs由可审计的保障处理器和安全外壳组成，提供物理防篡改保护，支持开源、灵活的验证能力。

Result: flexHEGs可实现隐私保护模型评估、受控部署、训练计算限制和自动安全协议执行等多种治理机制。

Conclusion: 尽管技术挑战大，flexHEGs为解决前沿AI发展中的监管和国际安全问题提供了一种可行方案。

Abstract: As artificial intelligence systems become increasingly powerful, they pose
growing risks to international security, creating urgent coordination
challenges that current governance approaches struggle to address without
compromising sensitive information or national security. We propose flexible
hardware-enabled guarantees (flexHEGs), that could be integrated with AI
accelerators to enable trustworthy, privacy-preserving verification and
enforcement of claims about AI development. FlexHEGs consist of an auditable
guarantee processor that monitors accelerator usage and a secure enclosure
providing physical tamper protection. The system would be fully open source
with flexible, updateable verification capabilities. FlexHEGs could enable
diverse governance mechanisms including privacy-preserving model evaluations,
controlled deployment, compute limits for training, and automated safety
protocol enforcement. In this first part of a three part series, we provide a
comprehensive introduction of the flexHEG system, including an overview of the
governance and security capabilities it offers, its potential development and
adoption paths, and the remaining challenges and limitations it faces. While
technically challenging, flexHEGs offer an approach to address emerging
regulatory and international security challenges in frontier AI development.

</details>


### [20] [International Security Applications of Flexible Hardware-Enabled Guarantees](https://arxiv.org/abs/2506.15100)
*Onni Aarne,James Petrie*

Main category: cs.CR

TL;DR: flexHEGs（灵活硬件保障）通过标准化设计和国际治理框架，为AI安全提供技术基础，解决恶意使用、失控风险等问题。


<details>
  <summary>Details</summary>
Motivation: 应对AI快速发展带来的国际安全挑战，如恶意使用、失控风险等，需通过硬件层面的治理框架。

Method: 分析四种安全应用场景，提出两种治理模型：基于验证的协议和基于规则的协议，并通过博弈论验证稳定性。

Result: flexHEGs在合理假设下能稳定运行，但需解决技术门槛、现有硬件管理等挑战。

Conclusion: flexHEGs为AI风险治理提供了可行的技术路径，但需国际协调。

Abstract: As AI capabilities advance rapidly, flexible hardware-enabled guarantees
(flexHEGs) offer opportunities to address international security challenges
through comprehensive governance frameworks. This report examines how flexHEGs
could enable internationally trustworthy AI governance by establishing
standardized designs, robust ecosystem defenses, and clear operational
parameters for AI-relevant chips. We analyze four critical international
security applications: limiting proliferation to address malicious use,
implementing safety norms to prevent loss of control, managing risks from
military AI systems, and supporting strategic stability through
balance-of-power mechanisms while respecting national sovereignty. The report
explores both targeted deployments for specific high-risk facilities and
comprehensive deployments covering all AI-relevant compute. We examine two
primary governance models: verification-based agreements that enable
transparent compliance monitoring, and ruleset-based agreements that
automatically enforce international rules through cryptographically-signed
updates. Through game-theoretic analysis, we demonstrate that comprehensive
flexHEG agreements could remain stable under reasonable assumptions about state
preferences and catastrophic risks. The report addresses critical
implementation challenges including technical thresholds for AI-relevant chips,
management of existing non-flexHEG hardware, and safeguards against abuse of
governance power. While requiring significant international coordination,
flexHEGs could provide a technical foundation for managing AI risks at the
scale and speed necessary to address emerging threats to international security
and stability.

</details>


### [21] [EVA-S2PMLP: Secure and Scalable Two-Party MLP via Spatial Transformation](https://arxiv.org/abs/2506.15102)
*Shizhao Peng,Shoumo Li,Tianle Tao*

Main category: cs.CR

TL;DR: EVA-S2PMLP是一个高效、可验证且准确的隐私保护神经网络训练框架，适用于垂直分区场景，通过空间尺度优化提升隐私和性能。


<details>
  <summary>Details</summary>
Motivation: 在垂直分区场景下，保护隐私的神经网络训练对跨机构协作建模至关重要。

Method: 提出EVA-S2PMLP框架，包括安全转换管道和原子协议，支持线性与非线性安全计算。

Result: 实验表明，EVA-S2PMLP在保持高推理精度的同时，显著降低通信开销，性能提升12.3倍。

Conclusion: EVA-S2PMLP在严格保护数据隐私的同时保持模型实用性，适用于金融、医疗等领域的隐私保护训练。

Abstract: Privacy-preserving neural network training in vertically partitioned
scenarios is vital for secure collaborative modeling across institutions. This
paper presents \textbf{EVA-S2PMLP}, an Efficient, Verifiable, and Accurate
Secure Two-Party Multi-Layer Perceptron framework that introduces spatial-scale
optimization for enhanced privacy and performance. To enable reliable
computation under real-number domain, EVA-S2PMLP proposes a secure
transformation pipeline that maps scalar inputs to vector and matrix spaces
while preserving correctness. The framework includes a suite of atomic
protocols for linear and non-linear secure computations, with modular support
for secure activation, matrix-vector operations, and loss evaluation.
Theoretical analysis confirms the reliability, security, and asymptotic
complexity of each protocol. Extensive experiments show that EVA-S2PMLP
achieves high inference accuracy and significantly reduced communication
overhead, with up to $12.3\times$ improvement over baselines. Evaluation on
benchmark datasets demonstrates that the framework maintains model utility
while ensuring strict data confidentiality, making it a practical solution for
privacy-preserving neural network training in finance, healthcare, and
cross-organizational AI applications.

</details>


### [22] [PDLRecover: Privacy-preserving Decentralized Model Recovery with Machine Unlearning](https://arxiv.org/abs/2506.15112)
*Xiangman Li,Xiaodong Wu,Jianbing Ni,Mohamed Mahmoud,Maazen Alsabaan*

Main category: cs.CR

TL;DR: PDLRecover是一种高效恢复中毒全局模型的新方法，利用历史模型信息并保护隐私。


<details>
  <summary>Details</summary>
Motivation: 现有防御方法难以恢复已中毒的全局模型，且重新训练成本高、耗时长。

Method: 通过线性近似Hessian矩阵计算和秘密共享技术保护历史更新，结合客户端准备、周期性恢复更新和最终精确更新。

Result: 实验表明，恢复的全局模型性能接近完全重新训练的模型，但计算和时间成本显著降低。

Conclusion: PDLRecover在恢复模型时既保证了准确性，又保护了隐私。

Abstract: Decentralized learning is vulnerable to poison attacks, where malicious
clients manipulate local updates to degrade global model performance. Existing
defenses mainly detect and filter malicious models, aiming to prevent a limited
number of attackers from corrupting the global model. However, restoring an
already compromised global model remains a challenge. A direct approach is to
remove malicious clients and retrain the model using only the benign clients.
Yet, retraining is time-consuming, computationally expensive, and may
compromise model consistency and privacy.
  We propose PDLRecover, a novel method to recover a poisoned global model
efficiently by leveraging historical model information while preserving
privacy. The main challenge lies in protecting shared historical models while
enabling parameter estimation for model recovery. By exploiting the linearity
of approximate Hessian matrix computation, we apply secret sharing to protect
historical updates, ensuring local models are not leaked during transmission or
reconstruction. PDLRecover introduces client-side preparation, periodic
recovery updates, and a final exact update to ensure robustness and convergence
of the recovered model. Periodic updates maintain accurate curvature
information, and the final step ensures high-quality convergence. Experiments
show that the recovered global model achieves performance comparable to a fully
retrained model but with significantly reduced computation and time cost.
Moreover, PDLRecover effectively prevents leakage of local model parameters,
ensuring both accuracy and privacy in recovery.

</details>


### [23] [CipherMind: The Longest Codebook in the World](https://arxiv.org/abs/2506.15117)
*Ming Nie,Zhixiong Yang,Bingsheng Wei*

Main category: cs.CR

TL;DR: CipherMind利用大语言模型的确定性微调中间结果作为传输内容，实现通信加密。


<details>
  <summary>Details</summary>
Motivation: 大语言模型的广泛应用启发了利用其推理能力进行通信加密的想法。

Method: 利用大语言模型的语义参数特性（如底层实现不透明、解释性弱）作为加密方法。

Result: 该方法适用于网关内传输等场景，理论上可基于任何大模型实现。

Conclusion: CipherMind为通信加密提供了一种新的可行方案。

Abstract: In recent years, the widespread application of large language models has
inspired us to consider using inference for communication encryption. We
therefore propose CipherMind, which utilizes intermediate results from
deterministic fine-tuning of large model inferences as transmission content.
The semantic parameters of large models exhibit characteristics like opaque
underlying implementations and weak interpretability, thus enabling their use
as an encryption method for data transmission. This communication paradigm can
be applied in scenarios like intra-gateway transmission, and theoretically, it
can be implemented using any large model as its foundation.

</details>


### [24] [From LLMs to MLLMs to Agents: A Survey of Emerging Paradigms in Jailbreak Attacks and Defenses within LLM Ecosystem](https://arxiv.org/abs/2506.15170)
*Yanxu Mao,Tiehan Cui,Peipei Liu,Datao You,Hongsong Zhu*

Main category: cs.CR

TL;DR: 本文系统综述了多模态大语言模型（LLMs）和智能代理中的越狱攻击及其防御机制，分析了攻击与防御的分类、代表性方法及未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 随着LLMs从单模态发展为多模态和智能代理，其安全风险日益严重，需要系统研究越狱攻击和防御机制。

Method: 通过分类主流越狱技术（攻击影响和可见性视角）和防御策略（响应时机和技术方法），结合相关数据集和评估指标进行分析。

Result: 总结了现有研究的局限性（如代理安全问题关注不足、混合越狱方法分类不清等），并提出了未来研究方向。

Conclusion: 研究旨在增强对越狱机制的理解，推动更鲁棒和自适应的防御策略发展。

Abstract: Large language models (LLMs) are rapidly evolving from single-modal systems
to multimodal LLMs and intelligent agents, significantly expanding their
capabilities while introducing increasingly severe security risks. This paper
presents a systematic survey of the growing complexity of jailbreak attacks and
corresponding defense mechanisms within the expanding LLM ecosystem. We first
trace the developmental trajectory from LLMs to MLLMs and Agents, highlighting
the core security challenges emerging at each stage. Next, we categorize
mainstream jailbreak techniques from both the attack impact and visibility
perspectives, and provide a comprehensive analysis of representative attack
methods, related datasets, and evaluation metrics. On the defense side, we
organize existing strategies based on response timing and technical approach,
offering a structured understanding of their applicability and implementation.
Furthermore, we identify key limitations in existing surveys, such as
insufficient attention to agent-specific security issues, the absence of a
clear taxonomy for hybrid jailbreak methods, a lack of detailed analysis of
experimental setups, and outdated coverage of recent advancements. To address
these limitations, we provide an updated synthesis of recent work and outline
future research directions in areas such as dataset construction, evaluation
framework optimization, and strategy generalization. Our study seeks to enhance
the understanding of jailbreak mechanisms and facilitate the advancement of
more resilient and adaptive defense strategies in the context of ever more
capable LLMs.

</details>


### [25] [LLM vs. SAST: A Technical Analysis on Detecting Coding Bugs of GPT4-Advanced Data Analysis](https://arxiv.org/abs/2506.15212)
*Madjid G. Tehrani,Eldar Sultanow,William J. Buchanan,Mahkame Houmani,Christel H. Djaha Fodja*

Main category: cs.CR

TL;DR: GPT-4在漏洞扫描中表现优于传统SAST工具，准确率达94%。


<details>
  <summary>Details</summary>
Motivation: 评估GPT-4在识别软件漏洞中的效果，并与传统SAST工具对比。

Method: 通过分析多种安全错误，测试GPT-4的漏洞检测能力。

Result: GPT-4在检测32种可利用漏洞时准确率达94%，优于SAST工具。

Conclusion: GPT-4在漏洞扫描中具有潜力，但需结合安全设计最佳实践。

Abstract: With the rapid advancements in Natural Language Processing (NLP), large
language models (LLMs) like GPT-4 have gained significant traction in diverse
applications, including security vulnerability scanning. This paper
investigates the efficacy of GPT-4 in identifying software vulnerabilities
compared to traditional Static Application Security Testing (SAST) tools.
Drawing from an array of security mistakes, our analysis underscores the potent
capabilities of GPT-4 in LLM-enhanced vulnerability scanning. We unveiled that
GPT-4 (Advanced Data Analysis) outperforms SAST by an accuracy of 94% in
detecting 32 types of exploitable vulnerabilities. This study also addresses
the potential security concerns surrounding LLMs, emphasising the imperative of
security by design/default and other security best practices for AI.

</details>


### [26] [Facility Location Problem under Local Differential Privacy without Super-set Assumption](https://arxiv.org/abs/2506.15224)
*Kevin Pfisterer,Quentin Hillebrand,Vorapong Suppakitpaisarn*

Main category: cs.CR

TL;DR: 本文提出了一种设施位置问题的改进版本，并在本地差分隐私（LDP）框架下进行分析，展示了如何在不牺牲隐私的情况下实现常数近似比。


<details>
  <summary>Details</summary>
Motivation: 传统设施位置问题在差分隐私下的近似比下界为Ω(√n)，而现有方法依赖超集假设可能损害隐私。本文旨在解决这一问题。

Method: 提出了一种改进的设施位置问题模型，并设计了一种LDP算法，实现了常数近似比和较小加性因子。

Result: 实验结果表明，该算法在合成和真实数据集上均优于直接方法。

Conclusion: 本文证明了传统下界不适用于改进模型，并通过LDP算法实现了高效且隐私保护的解决方案。

Abstract: In this paper, we introduce an adaptation of the facility location problem
and analyze it within the framework of local differential privacy (LDP). Under
this model, we ensure the privacy of client presence at specific locations.
When n is the number of points, Gupta et al. established a lower bound of
$\Omega(\sqrt{n})$ on the approximation ratio for any differentially private
algorithm applied to the original facility location problem. As a result,
subsequent works have adopted the super-set assumption, which may, however,
compromise user privacy. We show that this lower bound does not apply to our
adaptation by presenting an LDP algorithm that achieves a constant
approximation ratio with a relatively small additive factor. Additionally, we
provide experimental results demonstrating that our algorithm outperforms the
straightforward approach on both synthetically generated and real-world
datasets.

</details>


### [27] [RAS-Eval: A Comprehensive Benchmark for Security Evaluation of LLM Agents in Real-World Environments](https://arxiv.org/abs/2506.15253)
*Yuchuan Fu,Xiaohan Yuan,Dongxia Wang*

Main category: cs.CR

TL;DR: RAS-Eval是一个针对LLM代理的安全评估基准，包含80个测试用例和3,802个攻击任务，覆盖11个CWE类别。评估显示攻击显著降低任务完成率，大型模型表现更好。


<details>
  <summary>Details</summary>
Motivation: 由于LLM代理在关键领域的快速部署，缺乏标准化安全评估基准，需要动态环境下的安全框架。

Method: 引入RAS-Eval基准，支持模拟和真实工具执行，评估6种先进LLM。

Result: 攻击平均降低任务完成率36.78%，学术环境中成功率85.65%，大型模型表现更优。

Conclusion: 研究揭示了LLM代理部署中的安全风险，为未来安全研究提供了基础框架。

Abstract: The rapid deployment of Large language model (LLM) agents in critical domains
like healthcare and finance necessitates robust security frameworks. To address
the absence of standardized evaluation benchmarks for these agents in dynamic
environments, we introduce RAS-Eval, a comprehensive security benchmark
supporting both simulated and real-world tool execution. RAS-Eval comprises 80
test cases and 3,802 attack tasks mapped to 11 Common Weakness Enumeration
(CWE) categories, with tools implemented in JSON, LangGraph, and Model Context
Protocol (MCP) formats. We evaluate 6 state-of-the-art LLMs across diverse
scenarios, revealing significant vulnerabilities: attacks reduced agent task
completion rates (TCR) by 36.78% on average and achieved an 85.65% success rate
in academic settings. Notably, scaling laws held for security capabilities,
with larger models outperforming smaller counterparts. Our findings expose
critical risks in real-world agent deployments and provide a foundational
framework for future security research. Code and data are available at
https://github.com/lanzer-tree/RAS-Eval.

</details>


### [28] [Evaluation Pipeline for systematically searching for Anomaly Detection Systems](https://arxiv.org/abs/2506.15388)
*Florian Rokohl,Alexander Lehnert,Marc Reichenbach*

Main category: cs.CR

TL;DR: 提出了一种基于FPGA的硬件异常检测系统，用于实时检测医疗数字化环境中的恶意客户端。


<details>
  <summary>Details</summary>
Motivation: 医疗数字化带来便利的同时也面临网络安全威胁，需要实时且高效的解决方案。

Method: 使用FPGA满足实时性和功耗限制，通过整体系统评估实现性能优化。

Result: 系统能够在实时条件下有效检测恶意客户端。

Conclusion: 基于FPGA的硬件方案为医疗数字化安全提供了可行的解决方案。

Abstract: Digitalization in the medical world provides major benefits while making it a
target for attackers and thus hard to secure. To deal with network intruders we
propose an anomaly detection system on hardware to detect malicious clients in
real-time. We meet real-time and power restrictions using FPGAs. Overall system
performance is achieved via the presented holistic system evaluation.

</details>


### [29] [Detecting Hardware Trojans in Microprocessors via Hardware Error Correction Code-based Modules](https://arxiv.org/abs/2506.15417)
*Alessandro Palumbo,Ruben Salvador*

Main category: cs.CR

TL;DR: 提出了一种基于硬件的方法，利用ECC在RISC-V微处理器上检测运行时硬件木马（HT）激活，实现100%检测率且无额外开销。


<details>
  <summary>Details</summary>
Motivation: 硬件木马（HT）可能通过注入恶意指令破坏正常执行流程，需要一种高效检测方法。

Method: 采用Hamming单错误校正（HSEC）架构的硬件安全检查器（HSC）检测HT激活。

Result: 实验显示，该方法对潜在HT激活的检测率为100%，无假阳性或漏检，且硬件开销极低。

Conclusion: 该方法高效且低开销，适用于实时检测硬件木马攻击。

Abstract: Software-exploitable Hardware Trojans (HTs) enable attackers to execute
unauthorized software or gain illicit access to privileged operations. This
manuscript introduces a hardware-based methodology for detecting runtime HT
activations using Error Correction Codes (ECCs) on a RISC-V microprocessor.
Specifically, it focuses on HTs that inject malicious instructions, disrupting
the normal execution flow by triggering unauthorized programs. To counter this
threat, the manuscript introduces a Hardware Security Checker (HSC) leveraging
Hamming Single Error Correction (HSEC) architectures for effective HT
detection. Experimental results demonstrate that the proposed solution achieves
a 100% detection rate for potential HT activations, with no false positives or
undetected attacks. The implementation incurs minimal overhead, requiring only
72 #LUTs, 24 #FFs, and 0.5 #BRAM while maintaining the microprocessor's
original operating frequency and introducing no additional time delay.

</details>


### [30] [Side-Channel Extraction of Dataflow AI Accelerator Hardware Parameters](https://arxiv.org/abs/2506.15432)
*Guillaume Lomet,Ruben Salvador,Brice Colombier,Vincent Grosso,Olivier Sentieys,Cedric Killian*

Main category: cs.CR

TL;DR: 本文提出了一种通过侧信道攻击（SCA）恢复FINN框架生成的数据流加速器硬件配置的方法，显著降低了计算开销，并在短时间内高精度恢复参数。


<details>
  <summary>Details</summary>
Motivation: 数据流神经网络加速器在FPGA上高效处理AI任务，但其便利性使其易受恶意攻击者通过侧信道攻击窃取知识产权（IP）。

Method: 通过无监督降维减少计算开销，结合轻量级分类器恢复折叠和量化参数。

Result: 攻击阶段仅需337毫秒恢复硬件参数（准确率>95%），421毫秒完全恢复参数（平均4次跟踪），比现有方法快940倍和110倍。

Conclusion: 该方法提供了更现实的攻击场景，且无需平均跟踪即可优于现有方法。

Abstract: Dataflow neural network accelerators efficiently process AI tasks on FPGAs,
with deployment simplified by ready-to-use frameworks and pre-trained models.
However, this convenience makes them vulnerable to malicious actors seeking to
reverse engineer valuable Intellectual Property (IP) through Side-Channel
Attacks (SCA). This paper proposes a methodology to recover the hardware
configuration of dataflow accelerators generated with the FINN framework.
Through unsupervised dimensionality reduction, we reduce the computational
overhead compared to the state-of-the-art, enabling lightweight classifiers to
recover both folding and quantization parameters. We demonstrate an attack
phase requiring only 337 ms to recover the hardware parameters with an accuracy
of more than 95% and 421 ms to fully recover these parameters with an averaging
of 4 traces for a FINN-based accelerator running a CNN, both using a random
forest classifier on side-channel traces, even with the accelerator dataflow
fully loaded. This approach offers a more realistic attack scenario than
existing methods, and compared to SoA attacks based on tsfresh, our method
requires 940x and 110x less time for preparation and attack phases,
respectively, and gives better results even without averaging traces.

</details>


### [31] [An efficient construction of Raz's two-source randomness extractor with improved parameters](https://arxiv.org/abs/2506.15547)
*Cameron Foreman,Lewis Wooltorton,Kevin Milner,Florian J. Curchod*

Main category: cs.CR

TL;DR: 改进Raz的提取器，使其计算时间从多项式降至准线性，并降低熵需求，提供分析和数值比较，以及开源实现。


<details>
  <summary>Details</summary>
Motivation: 解决Raz提取器因高计算复杂度而不实用的问题。

Method: 提出改进的Raz提取器，优化计算时间和熵需求，提供分析和数值比较。

Result: 实现准线性计算时间，降低熵需求，并提供开源代码和参数计算模块。

Conclusion: 改进后的提取器更高效实用，适用于实际应用。

Abstract: Randomness extractors are algorithms that distill weak random sources into
near-perfect random numbers. Two-source extractors enable this distillation
process by combining two independent weak random sources. Raz's extractor (STOC
'05) was the first to achieve this in a setting where one source has linear
min-entropy (i.e., proportional to its length), while the other has only
logarithmic min-entropy in its length. However, Raz's original construction is
impractical due to a polynomial computation time of at least degree 4. Our work
solves this problem by presenting an improved version of Raz's extractor with
quasi-linear computation time, as well as a new analytic theorem with reduced
entropy requirements. We provide comprehensive analytical and numerical
comparisons of our construction with others in the literature, and we derive
strong and quantum-proof versions of our efficient Raz extractor. Additionally,
we offer an easy-to-use, open-source code implementation of the extractor and a
numerical parameter calculation module.

</details>


### [32] [PhishDebate: An LLM-Based Multi-Agent Framework for Phishing Website Detection](https://arxiv.org/abs/2506.15656)
*Wenhao Li,Selvakumar Manickam,Yung-wey Chong,Shankar Karuppayah*

Main category: cs.CR

TL;DR: PhishDebate是一个基于多代理LLM的辩论框架，用于钓鱼网站检测，通过四个专业代理分析网页的不同文本方面，显著提高了准确性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有钓鱼检测方法多为单代理分类，存在幻觉风险且缺乏可解释性或鲁棒性。

Method: PhishDebate采用四个专业代理（URL结构、HTML组成、语义内容和品牌冒充）在协调员和法官的指导下进行结构化辩论。

Result: 在真实钓鱼数据集上，PhishDebate实现了98.2%的召回率和真阳性率，优于单代理和CoT基线。

Conclusion: PhishDebate通过模块化设计和多代理辩论，显著提升了钓鱼检测的准确性、可解释性和适应性。

Abstract: Phishing websites continue to pose a significant cybersecurity threat, often
leveraging deceptive structures, brand impersonation, and social engineering
tactics to evade detection. While recent advances in large language models
(LLMs) have enabled improved phishing detection through contextual
understanding, most existing approaches rely on single-agent classification
facing the risks of hallucination and lack interpretability or robustness. To
address these limitations, we propose PhishDebate, a modular multi-agent
LLM-based debate framework for phishing website detection. PhishDebate employs
four specialized agents to independently analyze different textual aspects of a
webpage--URL structure, HTML composition, semantic content, and brand
impersonation--under the coordination of a Moderator and a final Judge. Through
structured debate and divergent thinking, the framework delivers more accurate
and interpretable decisions. Extensive evaluations on commercial LLMs
demonstrate that PhishDebate achieves 98.2% recall and 98.2% True Positive Rate
(TPR) on a real-world phishing dataset, and outperforms single-agent and Chain
of Thought (CoT) baselines. Additionally, its modular design allows agent-level
configurability, enabling adaptation to varying resource and application
requirements.

</details>
