<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 6]
- [cs.CR](#cs.CR) [Total: 14]
- [cs.AI](#cs.AI) [Total: 11]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [Is It Safe To Learn And Share? On Psychological Safety and Social Learning in (Agile) Communities of Practice](https://arxiv.org/abs/2507.01065)
*Christiaan Verwijs,Evelien Acun-Roos,Daniel Russo*

Main category: cs.SE

TL;DR: 研究发现，在敏捷社区实践中，线上互动的心理安全感显著低于面对面互动，且低心理安全感会降低参与者的持续贡献意愿。


<details>
  <summary>Details</summary>
Motivation: 随着混合、分布式和异步工作模式的普及，敏捷软件开发中的持续学习变得更重要，但心理安全感在这些环境中的作用尚未充分理解。

Method: 采用混合方法研究，通过143名参与者的调查数据，分析心理安全感在敏捷社区实践中的表现。

Result: 线上互动的心理安全感较低，且低心理安全感会减少参与者的贡献意愿。未发现性别、社区资历或内容创作活动的显著差异，但角色和年龄组存在差异。

Conclusion: 研究建议通过明确规范、结构化引导和积极管理来提升心理安全感，为组织者提供了实践指导。

Abstract: As hybrid, distributed, and asynchronous work models become more prevalent,
continuous learning in Agile Software Development (ASD) gains renewed
importance. Communities of Practice (CoPs) are increasingly adopted to support
social learning beyond formal education, often relying on virtual
communication. Psychological safety, a prerequisite for effective learning,
remains insufficiently understood in these settings. This mixed-methods study
investigates psychological safety within Agile CoPs through survey data from
143 participants. Results indicate that psychological safety is significantly
lower in online interactions compared to face-to-face settings. Moreover, low
psychological safety reduces participants' intent to continue contributing and
avoidance of interpersonal risk. No significant differences emerged based on
gender, community seniority, or content creation activity. However, differences
by role and age group suggest potential generational or role-related effects.
Thematic analysis revealed exclusionary behavior, negative interaction
patterns, and hostility as primary threats to psychological safety, often
reinforced by tribalism and specific community dynamics. Suggested
interventions include establishing explicit norms, structured facilitation, and
active moderation. The findings were validated through member checking with 30
participants. This study provides a comparative perspective on interaction
modalities and offers practical guidance for organizers seeking to cultivate
inclusive, high-impact CoPs and similarly structured virtual or hybrid work
environments.

</details>


### [2] [Bugs in the Shadows: Static Detection of Faulty Python Refactorings](https://arxiv.org/abs/2507.01103)
*Jonhnanthan Oliveira,Rohit Gheyi,Márcio Ribeiro,Alessandro Garcia*

Main category: cs.SE

TL;DR: 提出了一种静态分析技术，用于检测Python重构过程中引入的类型错误，并在实际项目中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: Python的动态类型系统在重构时容易引入类型错误，影响软件可靠性和开发效率。

Method: 采用静态分析技术，对Rope重构实现进行评估，应用于开源Python项目。

Result: 在1,152次重构尝试中发现了29个错误，部分问题存在于主流IDE中。

Conclusion: 当前Python重构工具的鲁棒性需提升，以确保代码转换的正确性和软件维护的可靠性。

Abstract: Python is a widely adopted programming language, valued for its simplicity
and flexibility. However, its dynamic type system poses significant challenges
for automated refactoring - an essential practice in software evolution aimed
at improving internal code structure without changing external behavior.
Understanding how type errors are introduced during refactoring is crucial, as
such errors can compromise software reliability and reduce developer
productivity. In this work, we propose a static analysis technique to detect
type errors introduced by refactoring implementations for Python. We evaluated
our technique on Rope refactoring implementations, applying them to open-source
Python projects. Our analysis uncovered 29 bugs across four refactoring types
from a total of 1,152 refactoring attempts. Several of these issues were also
found in widely used IDEs such as PyCharm and PyDev. All reported bugs were
submitted to the respective developers, and some of them were acknowledged and
accepted. These results highlight the need to improve the robustness of current
Python refactoring tools to ensure the correctness of automated code
transformations and support reliable software maintenance.

</details>


### [3] [Context-Aware Code Wiring Recommendation with LLM-based Agent](https://arxiv.org/abs/2507.01315)
*Taiming Wang,Yanjie Jiang,Chunhao Dong,Yuxia Zhang,Hui Liu*

Main category: cs.SE

TL;DR: WIRL是一个基于LLM的代码接线代理，通过检索增强生成（RAG）任务解决代码粘贴中的变量替换问题，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有代码粘贴修改方法未能充分利用上下文信息，而超过一半的修改案例依赖上下文。

Method: WIRL结合LLM、定制工具包和协调模块，采用混合策略（规则和状态机）进行智能变量替换。

Result: 在真实数据集上，WIRL的精确率和召回率分别达到91.7%和90.0%，显著优于现有方法。

Conclusion: WIRL为现代IDE提供了更智能、上下文感知的开发者辅助工具，具有实际应用价值。

Abstract: Copy-paste-modify is a widespread and pragmatic practice in software
development, where developers adapt reused code snippets, sourced from
platforms such as Stack Overflow, GitHub, or LLM outputs, into their local
codebase. A critical yet underexplored aspect of this adaptation is code
wiring, which involves substituting unresolved variables in the pasted code
with suitable ones from the surrounding context. Existing solutions either rely
on heuristic rules or historical templates, often failing to effectively
utilize contextual information, despite studies showing that over half of
adaptation cases are context-dependent. In this paper, we introduce WIRL, an
LLM-based agent for code wiring framed as a Retrieval-Augmented Generation
(RAG) infilling task. WIRL combines an LLM, a customized toolkit, and an
orchestration module to identify unresolved variables, retrieve context, and
perform context-aware substitutions. To balance efficiency and autonomy, the
agent adopts a mixed strategy: deterministic rule-based steps for common
patterns, and a state-machine-guided decision process for intelligent
exploration. We evaluate WIRL on a carefully curated, high-quality dataset
consisting of real-world code adaptation scenarios. Our approach achieves an
exact match precision of 91.7% and a recall of 90.0%, outperforming advanced
LLMs by 22.6 and 13.7 percentage points in precision and recall, respectively,
and surpassing IntelliJ IDEA by 54.3 and 49.9 percentage points. These results
underscore its practical utility, particularly in contexts with complex
variable dependencies or multiple unresolved variables. We believe WIRL paves
the way for more intelligent and context-aware developer assistance in modern
IDEs.

</details>


### [4] [Combining Type Inference and Automated Unit Test Generation for Python](https://arxiv.org/abs/2507.01477)
*Lukas Krodinger,Stephan Lukasczyk,Gordon Fraser*

Main category: cs.SE

TL;DR: 论文提出了一种通过运行时类型追踪（type tracing）来改进动态类型语言（如Python）的自动化单元测试生成方法，显著提升了代码覆盖率和突变分数。


<details>
  <summary>Details</summary>
Motivation: 动态类型语言缺乏静态类型信息，限制了测试生成器的效果。通过运行时类型追踪，可以提取并逐步优化类型信息，从而改进测试生成。

Method: 在Pynguin测试生成框架中引入类型追踪，通过运行时观察参数使用和返回值类型，逐步推断类型信息并用于生成测试用例。

Result: 实验表明，该方法使分支覆盖率提升高达90.0%，突变分数提高，且类型信息质量与现有先进类型推断工具相当。

Conclusion: 类型追踪是一种有效的方法，能够显著提升动态类型语言自动化测试生成的效果。

Abstract: Automated unit test generation is an established research field that has so
far focused on statically-typed programming languages. The lack of type
information in dynamically-typed programming languages, such as Python,
inhibits test generators, which heavily rely on information about parameter and
return types of functions to select suitable arguments when constructing test
cases. Since automated test generators inherently rely on frequent execution of
candidate tests, we make use of these frequent executions to address this
problem by introducing type tracing, which extracts type-related information
during execution and gradually refines the available type information. We
implement type tracing as an extension of the Pynguin test-generation framework
for Python, allowing it (i) to infer parameter types by observing how
parameters are used during runtime, (ii) to record the types of values that
function calls return, and (iii) to use this type information to increase code
coverage. The approach leads to up to 90.0% more branch coverage, improved
mutation scores, and to type information of similar quality to that produced by
other state-of-the-art type-inference tools.

</details>


### [5] [DaiFu: In-Situ Crash Recovery for Deep Learning Systems](https://arxiv.org/abs/2507.01628)
*Zilong He,Pengfei Chen,Hongyu Zhang,Xiaoyun Li,Guangba Yu,Hongyang Chen,Zibin Zheng*

Main category: cs.SE

TL;DR: DaiFu是一个轻量级的深度学习系统崩溃恢复框架，通过代码转换实现即时恢复，显著减少恢复时间。


<details>
  <summary>Details</summary>
Motivation: 深度学习系统崩溃频繁且浪费资源，现有恢复方案（如检查点重试）效率低下，需要更轻量级的解决方案。

Method: DaiFu通过对DL系统进行轻量级代码转换，拦截崩溃并动态更新运行上下文（如代码、配置等），实现快速恢复。

Result: DaiFu将恢复时间缩短1372倍，开销低于0.40%，并在7种崩溃场景中验证了其有效性。

Conclusion: DaiFu为深度学习系统提供了一种高效、低开销的崩溃恢复方案，显著提升了开发效率。

Abstract: Deep learning (DL) systems have been widely adopted in many areas, and are
becoming even more popular with the emergence of large language models.
However, due to the complex software stacks involved in their development and
execution, crashes are unavoidable and common. Crashes severely waste computing
resources and hinder development productivity, so efficient crash recovery is
crucial. Existing solutions, such as checkpoint-retry, are too heavyweight for
fast recovery from crashes caused by minor programming errors or transient
runtime errors. Therefore, we present DaiFu, an in-situ recovery framework for
DL systems. Through a lightweight code transformation to a given DL system,
DaiFu augments it to intercept crashes in situ and enables dynamic and instant
updates to its program running context (e.g., code, configurations, and other
data) for agile crash recovery. Our evaluation shows that DaiFu helps reduce
the restore time for crash recovery, achieving a 1372x speedup compared with
state-of-the-art solutions. Meanwhile, the overhead of DaiFu is negligible
(under 0.40%). We also construct a benchmark spanning 7 distinct crash
scenarios in DL systems, and show the effectiveness of DaiFu in diverse
situations.

</details>


### [6] [APRMCTS: Improving LLM-based Automated Program Repair with Iterative Tree Search](https://arxiv.org/abs/2507.01827)
*Haichuan Hu,Congqing He,Hao Zhang,Xiaochen Xie,Quanjun Zhang*

Main category: cs.SE

TL;DR: APRMCTS利用蒙特卡洛树搜索改进基于LLM的自动程序修复，解决了局部探索和冗余搜索的问题，显著提升了修复效率和效果。


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的自动程序修复技术采用试错策略，存在局部探索导致的修复效果有限和冗余搜索效率低的问题。

Method: APRMCTS结合蒙特卡洛树搜索（MCTS），通过全局评估和选择最有希望的补丁进行迭代优化。

Result: 在Defects4J的835个错误上，APRMCTS与GPT-3.5结合修复了201个错误，优于现有方法，且时间和成本更低。

Conclusion: APRMCTS在修复复杂错误时表现出高效和低成本的优势，为自动程序修复提供了新思路。

Abstract: Automated Program Repair (APR) attempts to fix software bugs without human
intervention, which plays a crucial role in software development and
maintenance. Recently, with the advances in Large Language Models (LLMs), a
rapidly increasing number of APR techniques have been proposed with remarkable
performance. However, existing LLM-based APR techniques typically adopt
trial-and-error strategies, which suffer from two major drawbacks: (1)
inherently limited patch effectiveness due to local exploration, and (2) low
search efficiency due to redundant exploration. In this paper, we propose
APRMCTS, which uses iterative tree search to improve LLM-based APR. APRMCTS
incorporates Monte Carlo Tree Search (MCTS) into patch searching by performing
a global evaluation of the explored patches and selecting the most promising
one for subsequent refinement and generation. APRMCTS effectively resolves the
problems of falling into local optima and thus helps improve the efficiency of
patch searching. Our experiments on 835 bugs from Defects4J demonstrate that,
when integrated with GPT-3.5, APRMCTS can fix a total of 201 bugs, which
outperforms all state-of-the-art baselines. Besides, APRMCTS helps GPT-4o-mini,
GPT-3.5, Yi-Coder-9B, and Qwen2.5-Coder-7B to fix 30, 27, 37, and 28 more bugs,
respectively. More importantly, APRMCTS boasts a significant performance
advantage while employing small patch size (16 and 32), notably fewer than the
500 and 10,000 patches adopted in previous studies. In terms of cost, compared
to existing state-of-the-art LLM-based APR methods, APRMCTS has time and
monetary costs of less than 20% and 50%, respectively. Our extensive study
demonstrates that APRMCTS exhibits good effectiveness and efficiency, with
particular advantages in addressing complex bugs.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [7] [A Systematic Review of Security Vulnerabilities in Smart Home Devices and Mitigation Techniques](https://arxiv.org/abs/2507.01018)
*Mohammed K. Alzaylaee*

Main category: cs.CR

TL;DR: 研究探讨智能家居生态系统的安全威胁，提出后量子加密与AI异常检测的有效性，但计算资源需求高；区块链认证与零信任结构增强安全性，但需基础设施调整。


<details>
  <summary>Details</summary>
Motivation: 智能家居因物联网设备增多面临网络安全风险，需探索有效安全策略。

Method: 分类网络层、设备层及云/AI系统的漏洞，评估后量子加密、AI检测、区块链认证等方法。

Result: 后量子加密与AI检测有效但资源密集；区块链与零信任结构需调整基础设施；策略有效性通过统计测试验证，但可扩展性不足。

Conclusion: 需改进加密技术、AI威胁检测和自适应安全模型，平衡性能与实时性。

Abstract: Smart homes that integrate Internet of Things (IoT) devices face increasing
cybersecurity risks, posing significant challenges to these environments. The
study explores security threats in smart homes ecosystems, categorizing them
into vulnerabilities at the network layer, device level, and those from
cloud-based and AI-driven systems. Research findings indicate that post-quantum
encryption, coupled with AI-driven anomaly detection, is highly effective in
enhancing security; however, computational resource demands present significant
challenges. Blockchain authentication together with zero-trust structures
builds security resilience, although they need changes to existing
infrastructure. The specific security strategies show their effectiveness
through ANOVA, Chi-square tests, and Monte Carlo simulations yet lack
sufficient scalability according to the results. The research demonstrates the
requirement for improvement in cryptographic techniques, alongside AI-enhanced
threat detection and adaptive security models which must achieve a balance
between performance and efficiency and real-time applicability within smart
home ecosystems.

</details>


### [8] [AutoAdv: Automated Adversarial Prompting for Multi-Turn Jailbreaking of Large Language Models](https://arxiv.org/abs/2507.01020)
*Aashray Reddy,Andrew Zagula,Nicholas Saban*

Main category: cs.CR

TL;DR: AutoAdv是一个自动化生成对抗性提示的框架，用于评估LLM的安全机制漏洞，通过多轮攻击方法实现高达86%的越狱成功率。


<details>
  <summary>Details</summary>
Motivation: 大语言模型（LLMs）容易受到越狱攻击，现有安全机制存在漏洞，需要系统化的评估方法。

Method: 利用参数化攻击者LLM生成语义伪装的恶意提示，结合多轮攻击策略（如角色扮演、误导和上下文操控）迭代优化攻击。

Result: 在ChatGPT、Llama和DeepSeek等先进模型上，AutoAdv实现了高达86%的越狱成功率。

Conclusion: 当前安全机制对复杂多轮攻击仍脆弱，亟需更强大的防御策略。

Abstract: Large Language Models (LLMs) continue to exhibit vulnerabilities to
jailbreaking attacks: carefully crafted malicious inputs intended to circumvent
safety guardrails and elicit harmful responses. As such, we present AutoAdv, a
novel framework that automates adversarial prompt generation to systematically
evaluate and expose vulnerabilities in LLM safety mechanisms. Our approach
leverages a parametric attacker LLM to produce semantically disguised malicious
prompts through strategic rewriting techniques, specialized system prompts, and
optimized hyperparameter configurations. The primary contribution of our work
is a dynamic, multi-turn attack methodology that analyzes failed jailbreak
attempts and iteratively generates refined follow-up prompts, leveraging
techniques such as roleplaying, misdirection, and contextual manipulation. We
quantitatively evaluate attack success rate (ASR) using the StrongREJECT
(arXiv:2402.10260 [cs.CL]) framework across sequential interaction turns.
Through extensive empirical evaluation of state-of-the-art models--including
ChatGPT, Llama, and DeepSeek--we reveal significant vulnerabilities, with our
automated attacks achieving jailbreak success rates of up to 86% for harmful
content generation. Our findings reveal that current safety mechanisms remain
susceptible to sophisticated multi-turn attacks, emphasizing the urgent need
for more robust defense strategies.

</details>


### [9] [Quasi-twisted codes: decoding and applications in code-based cryptography](https://arxiv.org/abs/2507.01118)
*Bhagyalekshmy S,Rutuja Kshirsagar*

Main category: cs.CR

TL;DR: 提出了一种针对拟扭码的基于伴随式的解码方法，并构建了一种基于拟扭码的Niederreiter类密码系统。


<details>
  <summary>Details</summary>
Motivation: 拟扭码具有广泛应用潜力，但目前缺乏高效解码算法。

Method: 提出了一种基于伴随式的高效解码方法，并构建了一种Niederreiter类密码系统。

Result: 解码方法能高效纠正最多(d*-1)/2个错误；密码系统对经典攻击和某些量子攻击具有抵抗力。

Conclusion: 该方法填补了拟扭码解码的空白，同时为密码学提供了新的工具。

Abstract: Quasi-twisted (QT) codes generalize several important families of linear
codes, including cyclic, constacyclic, and quasi-cyclic codes. Despite their
potential, to the best of our knowledge, there exists no efficient decoding
algorithm for QT codes. In this work, we propose a syndrome-based decoding
method capable of efficiently correcting up to (d* - 1)/2 errors, where d*
denotes an HT-like lower bound on the minimum distance of QT codes, which we
formalize here. Additionally, we introduce a Niederreiter-like cryptosystem
constructed from QT codes. This cryptosystem is resistant to some classical
attacks as well as some quantum attacks based on Quantum Fourier Sampling.

</details>


### [10] [A Compact 16-bit S-box over Tower Field $\F_{(((2^2)^2)^2)^2}$ with High Security](https://arxiv.org/abs/2507.01423)
*Bahram Rashidi,Behrooz Khadem*

Main category: cs.CR

TL;DR: 本文提出了一种紧凑且安全的16位S盒，基于复合域设计，优化了硬件效率和密码学鲁棒性。通过子域分解和塔域架构，降低了硬件资源消耗，同时保持了高安全性。


<details>
  <summary>Details</summary>
Motivation: 满足数据敏感应用中对可扩展密码原语的需求，证明更大的S盒可以在不增加硬件成本的情况下提升安全性。

Method: 采用复合域架构，将操作分解为子域，优化域反转和低开销仿射变换。

Result: S盒在65 nm CMOS技术中表现出较低的硬件资源消耗和关键路径延迟，同时具备高安全性（如非线性度32512、差分均匀性4等）。

Conclusion: 复合域架构在平衡现代分组密码的安全性和效率方面具有可行性。

Abstract: This paper introduces a compact and secure 16-bit substitution box (S-box)
designed over the composite field $\F_{(((2^2)^2)^2)^2}$, optimized for both
hardware efficiency and cryptographic robustness. The proposed S-box decomposes
operations into subfields, leveraging a tower field architecture. This enables
significant hardware reduction through optimized field inversion and a low-cost
affine transformation. Security evaluations confirm resilience against linear,
differential, algebraic and DPA attacks, validated via metrics including
Nonlinearity (32512), Differential Uniformity (4), Algebraic Degree (15),
Transparency order (15.9875) and SNR (0.34e-08). The hardware results, in 65 nm
CMOS technology, show the proposed 16-bit S-box has lower hardware resources
consumption and lower critical path delay (CPD) than those of other 16-bit
S-boxes. By integrating high algebraic complexity with resource-efficient
structures, this work addresses the growing demand for scalable cryptographic
primitives in data-sensitive applications, demonstrating that larger S-boxes
can enhance security without proportional hardware costs. The results
underscore the viability of composite field-based architectures in balancing
security and efficiency for modern block ciphers.

</details>


### [11] [A new efficient RPKI Design](https://arxiv.org/abs/2507.01465)
*Haya Schulmann,Niklas Vogel*

Main category: cs.CR

TL;DR: iRPKI通过优化RPKI的设计，显著提升了性能并减少了资源消耗，同时保持了安全性。


<details>
  <summary>Details</summary>
Motivation: RPKI的现有设计复杂且效率低下，限制了其大规模部署。

Method: 分析了RPKI设计中的复杂性根源，并提出了iRPKI，一种向后兼容的优化方案。

Result: iRPKI在验证速度、带宽需求和内存占用上分别提升了20倍、18倍和8倍。

Conclusion: iRPKI显著提升了RPKI的可行性和安全性，适合大规模部署。

Abstract: Resource Public Key Infrastructure (RPKI) is a critical security mechanism
for BGP, but the complexity of its architecture is a growing concern as its
adoption scales. Current RPKI design heavily reuses legacy PKI components, such
as X.509 EE-certificates, ASN.1 encoding, and XML-based repository protocols,
all these introduce excessive cryptographic validation, redundant metadata, and
inefficiencies in both storage and processing. We show that these design
choices, although based on established standards, create significant
performance bottlenecks, increase the vulnerability surface, and hinder
scalability for wide-scale Internet deployment.
  In this paper, we perform the first systematic analysis of the root causes of
complexity in RPKI's design and experimentally quantify their real-world
impact. We show that over 70% of validation time in RPKI relying parties is
spent on certificate parsing and signature verification, much of it
unnecessary. Building on this insight, we introduce the improved RPKI (iRPKI),
a backwards-compatible redesign that preserves all security guarantees while
substantially reducing protocol overhead. iRPKI eliminates EE-certificates and
ROA signatures, merges revocation and integrity objects, replaces verbose
encodings with Protobuf, and restructures repository metadata for more
efficient access. We experimentally demonstrate that our implementation of
iRPKI in the Routinator validator achieves a 20x speed-up of processing time,
18x improvement of bandwidth requirements and 8x reduction in cache memory
footprint, while also eliminating classes of vulnerabilities that have led to
at least 10 vulnerabilities in RPKI software. iRPKI significantly increases the
feasibility of deploying RPKI at scale in the Internet, and especially in
constrained environments. Our design may be deployed incrementally without
impacting existing operations.

</details>


### [12] [How to Securely Shuffle? A survey about Secure Shufflers for privacy-preserving computations](https://arxiv.org/abs/2507.01487)
*Marc Damie,Florian Hahn,Andreas Peter,Jan Ramon*

Main category: cs.CR

TL;DR: 本文综述了安全混洗器的定义、分类和比较，提出了统一的评估标准，并探讨了其应用和未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 安全混洗器在隐私数据聚合和差分隐私中具有重要作用，但现有研究多将其视为黑盒，忽略了实际漏洞和性能权衡。本文旨在回答“什么是好的安全混洗器”这一问题。

Method: 通过识别、分类和比较26种安全协议，统一安全定义，并提供选择协议的实用指南。

Result: 提出了一个一致的评估框架，总结了依赖安全混洗器的隐私保护技术，并指出了未来研究方向。

Conclusion: 安全混洗器的设计和选择需综合考虑安全性和性能，未来研究应进一步优化协议并探索新应用。

Abstract: Ishai et al. (FOCS'06) introduced secure shuffling as an efficient building
block for private data aggregation. Recently, the field of differential privacy
has revived interest in secure shufflers by highlighting the privacy
amplification they can provide in various computations. Although several works
argue for the utility of secure shufflers, they often treat them as black
boxes; overlooking the practical vulnerabilities and performance trade-offs of
existing implementations. This leaves a central question open: what makes a
good secure shuffler?
  This survey addresses that question by identifying, categorizing, and
comparing 26 secure protocols that realize the necessary shuffling
functionality. To enable a meaningful comparison, we adapt and unify existing
security definitions into a consistent set of properties. We also present an
overview of privacy-preserving technologies that rely on secure shufflers,
offer practical guidelines for selecting appropriate protocols, and outline
promising directions for future work.

</details>


### [13] [SafePTR: Token-Level Jailbreak Defense in Multimodal LLMs via Prune-then-Restore Mechanism](https://arxiv.org/abs/2507.01513)
*Beitao Chen,Xinyu Lyu,Lianli Gao,Jingkuan Song,Heng Tao Shen*

Main category: cs.CR

TL;DR: 论文分析了多模态大语言模型（MLLMs）的安全漏洞，提出了一种无需训练的防御框架SafePTR，通过选择性修剪有害令牌来提升安全性。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型（MLLMs）在扩展视觉推理能力的同时，引入了新的安全漏洞，现有防御方法未能有效解决这些漏洞。

Method: 论文首先分析了有害多模态令牌如何绕过MLLMs的安全机制，随后提出了SafePTR框架，通过修剪有害令牌并恢复良性特征来防御攻击。

Result: 实验表明，SafePTR在三种MLLMs和五个基准测试中均显著提升了安全性，且不影响模型效率。

Conclusion: SafePTR是一种高效且无需训练的防御方法，能够显著降低MLLMs的越狱风险。

Abstract: By incorporating visual inputs, Multimodal Large Language Models (MLLMs)
extend LLMs to support visual reasoning. However, this integration also
introduces new vulnerabilities, making MLLMs susceptible to multimodal
jailbreak attacks and hindering their safe deployment.Existing defense methods,
including Image-to-Text Translation, Safe Prompting, and Multimodal Safety
Tuning, attempt to address this by aligning multimodal inputs with LLMs'
built-in safeguards.Yet, they fall short in uncovering root causes of
multimodal vulnerabilities, particularly how harmful multimodal tokens trigger
jailbreak in MLLMs? Consequently, they remain vulnerable to text-driven
multimodal jailbreaks, often exhibiting overdefensive behaviors and imposing
heavy training overhead.To bridge this gap, we present an comprehensive
analysis of where, how and which harmful multimodal tokens bypass safeguards in
MLLMs. Surprisingly, we find that less than 1% tokens in early-middle layers
are responsible for inducing unsafe behaviors, highlighting the potential of
precisely removing a small subset of harmful tokens, without requiring safety
tuning, can still effectively improve safety against jailbreaks. Motivated by
this, we propose Safe Prune-then-Restore (SafePTR), an training-free defense
framework that selectively prunes harmful tokens at vulnerable layers while
restoring benign features at subsequent layers.Without incurring additional
computational overhead, SafePTR significantly enhances the safety of MLLMs
while preserving efficiency. Extensive evaluations across three MLLMs and five
benchmarks demonstrate SafePTR's state-of-the-art performance in mitigating
jailbreak risks without compromising utility.

</details>


### [14] [Cybersecurity Issues in Local Energy Markets](https://arxiv.org/abs/2507.01536)
*Al Hussein Dabashi,Sajjad Maleki,Biswarup Mukherjee,Gregory Epiphaniou,Carsten Maple,Charalambos Konstantinou,Subhash Lakshminarayana*

Main category: cs.CR

TL;DR: 论文探讨了本地能源市场（LEMs）面临的网络安全威胁，分析了潜在漏洞的影响，并提出安全建议。


<details>
  <summary>Details</summary>
Motivation: LEMs依赖智能电网通信标准和易受攻击的IoT设备，可能导致市场操作被操控、隐私泄露和电网不稳定，亟需研究其安全风险。

Method: 通过映射LEM通信流到现有标准，识别关键漏洞，并在隐私保护LEM模型上模拟网络攻击场景。

Result: 研究发现攻击者可能扭曲定价和需求模式，揭示了漏洞的实际影响。

Conclusion: 提出了针对研究人员、开发者、政策制定者和LEM利益相关者的安全建议，以保障未来LEM部署的安全性。

Abstract: Local Energy Markets (LEMs), though pivotal to the energy transition, face
growing cybersecurity threats due to their reliance on smart grid communication
standards and vulnerable Internet-of-Things (IoT)-enabled devices. This is a
critical issue because such vulnerabilities can be exploited to manipulate
market operations, compromise participants' privacy, and destabilize power
distribution networks. This work maps LEM communication flows to existing
standards, highlights potential impacts of key identified vulnerabilities, and
simulates cyberattack scenarios on a privacy-preserving LEM model to assess
their impacts. Findings reveal how attackers could distort pricing and demand
patterns. We finally present recommendations for researchers, industry
developers, policymakers, and LEM stakeholders to secure future LEM
deployments.

</details>


### [15] [On the Effect of Ruleset Tuning and Data Imbalance on Explainable Network Security Alert Classifications: a Case-Study on DeepCASE](https://arxiv.org/abs/2507.01571)
*Koen T. W. Teuwen,Sam Baggen,Emmanuele Zambon,Luca Allodi*

Main category: cs.CR

TL;DR: 研究了标签不平衡对网络入侵警报分类的影响，发现其会影响分类性能和解释的正确性，建议通过调整SOC检测规则来减少不平衡。


<details>
  <summary>Details</summary>
Motivation: 自动化方法在SOC中用于警报分类和事件升级，但需在数据不平衡时保持鲁棒性并提供可解释性。

Method: 使用最先进的DeepCASE方法评估标签不平衡对警报分类的影响。

Result: 标签不平衡影响分类性能和解释的正确性。

Conclusion: 调整SOC检测规则可减少不平衡，提升自动化和解释性。

Abstract: Automation in Security Operations Centers (SOCs) plays a prominent role in
alert classification and incident escalation. However, automated methods must
be robust in the presence of imbalanced input data, which can negatively affect
performance. Additionally, automated methods should make explainable decisions.
In this work, we evaluate the effect of label imbalance on the classification
of network intrusion alerts. As our use-case we employ DeepCASE, the
state-of-the-art method for automated alert classification. We show that label
imbalance impacts both classification performance and correctness of the
classification explanations offered by DeepCASE. We conclude tuning the
detection rules used in SOCs can significantly reduce imbalance and may benefit
the performance and explainability offered by alert post-processing methods
such as DeepCASE. Therefore, our findings suggest that traditional methods to
improve the quality of input data can benefit automation.

</details>


### [16] [EGNInfoLeaker: Unveiling the Risks of Public Key Reuse and User Identity Leakage in Blockchain](https://arxiv.org/abs/2507.01635)
*Chenyu Li,Xueping Liang,Xiaorui Gong,Xiu Zhang*

Main category: cs.CR

TL;DR: 论文揭示了以太坊发现协议（Discv4/Discv5）中公钥复用的隐私漏洞，设计了一个名为EGNInfoLeaker的系统，通过分析300个网络快照，识别出83名用户控制483个服务节点，导致身份泄露。


<details>
  <summary>Details</summary>
Motivation: 研究动机是揭示以太坊网络中公钥复用的实际隐私风险，尽管协议设计有加密保护，但用户行为可能导致隐私泄露。

Method: 方法包括设计EGNInfoLeaker系统，分析网络快照，并通过基于图的身份关联算法生成用户画像。

Result: 结果显示83名用户通过公钥复用控制483个节点，导致身份信息（如IP、位置等）泄露。

Conclusion: 结论强调协议安全性不仅依赖设计，还需用户严格遵守，EGNInfoLeaker为去中心化网络的隐私保护提供了改进基础。

Abstract: While Ethereum's discovery protocols (Discv4/ Discv5) incorporate robust
cryptographic designs to protect user privacy, real-world deployment reveals
critical vulnerabilities when users deviate from security guidelines. In this
paper, we design a system called EGNInfoLeaker. Our study is the first work
that uncovers widespread public key reuse across Ethereum's peer-to-peer
networks - a practice that fundamentally undermines the protocol's privacy
guarantees. Through systematic analysis of 300 real-world network snapshots, we
identify 83 users controlling 483 service nodes via public key reuse, enabling
precise de-anonymization through IP correlation. Using evidence collected by
EGNInfoLeaker, our Graph-Based Identity Association Algorithm links users to
network entities and generates comprehensive user profiles. For User27, it
exposes the public key, IP, network ID, location (country/region/city), and
ISP/ORG details. The EGNInfoLeaker system demonstrates how such cryptographic
misuse transforms theoretical anonymity into practical identity leakage,
exposing users to surveillance and targeted attacks. These findings establish
that protocol security depends not only on sound design but also on strict user
compliance. Going forward, our detection framework provides a foundation for
enhancing real-world privacy preservation in decentralized networks.

</details>


### [17] [Graph Representation-based Model Poisoning on Federated LLMs in CyberEdge Networks](https://arxiv.org/abs/2507.01694)
*Hanlin Cai,Haofan Dong,Houtianfu Wang,Kai Li,Ozgur B. Akan*

Main category: cs.CR

TL;DR: 论文探讨了联邦大语言模型（FedLLMs）在保护数据隐私的同时易受模型投毒攻击的问题，提出了一种新的攻击范式GRMP，并呼吁加强防御研究。


<details>
  <summary>Details</summary>
Motivation: FedLLMs在保护数据隐私的同时具有强大的生成能力，但其易受模型投毒攻击，尤其是在非独立同分布（non-IID）文本数据下。现有防御机制存在局限性，需进一步研究。

Method: 文章回顾了现有模型投毒技术和防御机制，并研究了新型攻击范式GRMP，该攻击利用诚实客户端梯度的高阶相关性合成恶意更新。

Result: GRMP能有效规避高级防御机制，导致显著的准确性损失和性能下降。

Conclusion: 文章提出研究路线图，强调图感知的安全聚合方法、FedLLMs特定漏洞指标和评估框架的重要性，以增强未来联邦语言模型的鲁棒性。

Abstract: Federated large language models (FedLLMs) provide powerful generative
capabilities in CyberEdge networks while protecting data privacy. However,
FedLLMs remains highly vulnerable to model poisoning attacks. This article
first reviews recent model poisoning techniques and existing defense mechanisms
for FedLLMs, highlighting critical limitations, particularly under non-IID text
distributions. In particular, current defenses primarily utilize distance-based
outlier detection or norm constraints, operating under the assumption that
adversarial updates significantly diverge from benign statistics. This
assumption can fail when facing adaptive attackers targeting billionparameter
LLMs. Next, this article investigates emerging Graph Representation-Based Model
Poisoning (GRMP), a novel attack paradigm that leverages higher-order
correlations among honest client gradients to synthesize malicious updates
indistinguishable from legitimate model updates. GRMP can effectively evade
advanced defenses, resulting in substantial accuracy loss and performance
degradation. Moreover, this article outlines a research roadmap emphasizing the
importance of graph-aware secure aggregation methods, FedLLMs-specific
vulnerability metrics, and evaluation frameworks to strengthen the robustness
of future federated language model deployments.

</details>


### [18] [Towards Better Attribute Inference Vulnerability Measures](https://arxiv.org/abs/2507.01710)
*Paul Francis,David Wagner*

Main category: cs.CR

TL;DR: 本文提出了一种结合精确度和召回率的属性推断度量方法，改进了现有方法仅考虑精确度的局限性，并在实验中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 保护匿名化数据隐私的同时保留其统计特性，现有属性推断度量方法未考虑召回率，导致部分攻击被错误标记为安全。

Method: 设计并实现了一种结合精确度和召回率的属性推断度量方法，改进了基线属性推断的计算方式。

Result: 在实验中，新方法在25%以上的攻击中正确标记为风险，而现有方法错误标记为安全。

Conclusion: 新方法通过结合精确度和召回率，显著提高了属性推断攻击的检测准确性。

Abstract: The purpose of anonymizing structured data is to protect the privacy of
individuals in the data while retaining the statistical properties of the data.
An important class of attack on anonymized data is attribute inference, where
an attacker infers the value of an unknown attribute of a target individual
given knowledge of one or more known attributes. A major limitation of recent
attribute inference measures is that they do not take recall into account, only
precision. It is often the case that attacks target only a fraction of
individuals, for instance data outliers. Incorporating recall, however,
substantially complicates the measure, because one must determine how to
combine recall and precision in a composite measure for both the attack and
baseline. This paper presents the design and implementation of an attribute
inference measure that incorporates both precision and recall. Our design also
improves on how the baseline attribute inference is computed. In experiments
using a generic best row match attack on moderately-anonymized microdata, we
show that in over 25\% of the attacks, our approach correctly labeled the
attack to be at risk while the prior approach incorrectly labeled the attack to
be safe.

</details>


### [19] [Signals and Symptoms: ICS Attack Dataset From Railway Cyber Range](https://arxiv.org/abs/2507.01768)
*Anis Yusof,Yuancheng Liu,Niklaus Kang,Choon Meng Seah,Zhenkai Liang,Ee-Chien Chang*

Main category: cs.CR

TL;DR: 论文通过模拟两种ICS网络攻击，生成反映当前威胁的数据集，以增强安全系统和分析师应对ICS网络威胁的能力。


<details>
  <summary>Details</summary>
Motivation: 工业控制系统（ICS）网络攻击频发，尤其是OT与IT系统融合后，亟需反映当代威胁的数据集以支持分析师。

Method: 在铁路网络靶场模拟两种ICS网络攻击，结合历史攻击模式和当前趋势设计攻击场景，并收集数据。

Result: 生成的数据集包含当前威胁环境的关键指标，可作为网络攻击分析的重要资源。

Conclusion: 该数据集能有效提升安全系统和分析师应对ICS网络威胁的能力。

Abstract: The prevalence of cyberattacks on Industrial Control Systems (ICS) has
highlighted the necessity for robust security measures and incident response to
protect critical infrastructure. This is prominent when Operational Technology
(OT) systems undergo digital transformation by integrating with Information
Technology (IT) systems to enhance operational efficiency, adaptability, and
safety. To support analysts in staying abreast of emerging attack patterns,
there is a need for ICS datasets that reflect indicators representative of
contemporary cyber threats. To address this, we conduct two ICS cyberattack
simulations to showcase the impact of trending ICS cyberattacks on a railway
cyber range that resembles the railway infrastructure. The attack scenario is
designed to blend trending attack trends with attack patterns observed from
historical ICS incidents. The resulting evidence is collected as datasets,
serving as an essential resource for cyberattack analysis. This captures key
indicators that are relevant to the current threat landscape, augmenting the
effectiveness of security systems and analysts to protect against ICS cyber
threats.

</details>


### [20] [Empowering Manufacturers with Privacy-Preserving AI Tools: A Case Study in Privacy-Preserving Machine Learning to Solve Real-World Problems](https://arxiv.org/abs/2507.01808)
*Xiaoyu Ji,Jessica Shorland,Joshua Shank,Pascal Delpe-Brice,Latanya Sweeney,Jan Allebach,Ali Shakouri*

Main category: cs.CR

TL;DR: 论文提出了一种隐私保护平台，帮助中小型制造商安全共享数据，以开发创新工具，并通过食品晶体质量控制的案例展示了其有效性。


<details>
  <summary>Details</summary>
Motivation: 中小型制造商因竞争和隐私问题不愿共享数据，但需要创新工具解决实际问题。

Method: 开发隐私保护平台，制造商通过安全方法共享数据，研究人员开发工具后回传平台供使用。

Result: 成功开发并部署了自动分析食品晶体的工具，提高了效率和准确性。

Conclusion: 隐私保护平台有效解决了数据共享问题，未来可进一步扩展应用。

Abstract: Small- and medium-sized manufacturers need innovative data tools but, because
of competition and privacy concerns, often do not want to share their
proprietary data with researchers who might be interested in helping. This
paper introduces a privacy-preserving platform by which manufacturers may
safely share their data with researchers through secure methods, so that those
researchers then create innovative tools to solve the manufacturers' real-world
problems, and then provide tools that execute solutions back onto the platform
for others to use with privacy and confidentiality guarantees. We illustrate
this problem through a particular use case which addresses an important problem
in the large-scale manufacturing of food crystals, which is that quality
control relies on image analysis tools. Previous to our research, food crystals
in the images were manually counted, which required substantial and
time-consuming human efforts, but we have developed and deployed a crystal
analysis tool which makes this process both more rapid and accurate. The tool
enables automatic characterization of the crystal size distribution and numbers
from microscope images while the natural imperfections from the sample
preparation are automatically removed; a machine learning model to count high
resolution translucent crystals and agglomeration of crystals was also
developed to aid in these efforts. The resulting algorithm was then packaged
for real-world use on the factory floor via a web-based app secured through the
originating privacy-preserving platform, allowing manufacturers to use it while
keeping their proprietary data secure. After demonstrating this full process,
future directions are also explored.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [21] [Rethinking the Illusion of Thinking](https://arxiv.org/abs/2507.01231)
*Iñaki Dellibarda Varela,Pablo Romero-Sorozabal,Eduardo Rocon,Manuel Cebrian*

Main category: cs.AI

TL;DR: 论文澄清了关于大型推理模型（LRMs）是否具备真正推理能力的争议，通过复制和改进两项争议性实验（河内塔和河流穿越），发现LRMs在复杂性增加时存在认知限制，但在可解问题中表现优异。


<details>
  <summary>Details</summary>
Motivation: 解决AI社区对LRMs是否具备真正推理能力的争议，澄清先前研究的实验缺陷和结论夸大问题。

Method: 复制并改进河内塔和河流穿越实验，引入逐步提示和协作对话机制。

Result: 河内塔实验中，LRMs在复杂性增加（如8个盘子）时表现不佳；河流穿越实验中，LRMs在可解问题中表现优异。

Conclusion: 当前LRMs是离散状态空间中的随机搜索器，真正的符号推理进展需要更精细的实验分析。

Abstract: Earlier this year, Apple ignited controversy by publishing "The Illusion of
Thinking," prompting heated debate within the AI community. Critics seized upon
the findings as conclusive evidence that Large Reasoning Models (LRMs) lack
genuine reasoning capabilities, branding them as mere stochastic parrots.
Meanwhile, defenders-spearheaded by Lawsen et al. (2025)-fired back, condemning
the experimental setup as flawed and the conclusions overstated. We clarify
this debate by replicating and refining two of the original study's most
contentious benchmarks: Towers of Hanoi and River Crossing. By introducing
incremental stepwise prompting and agentic collaborative dialogue, we show that
previously reported failures solving the Towers of Hanoi were not purely result
of output constraints, but also partly a result of cognition limitations: LRMs
still stumble when complexity rises moderately (around 8 disks). Moreover, the
River Crossing results initially heralded as catastrophic failures turn out to
hinge upon testing unsolvable configurations. Once we limit tests strictly to
solvable problems-LRMs effortlessly solve large instances involving over 100
agent pairs. Our findings ultimately defy simplistic narratives: today's LRMs
are stochastic, RL-tuned searchers in a discrete state space we barely
understand. Real progress in symbolic, long-horizon reasoning demands mapping
that terrain through fine-grained ablations like those introduced here.

</details>


### [22] [Beyond Black-Box AI: Interpretable Hybrid Systems for Dementia Care](https://arxiv.org/abs/2507.01282)
*Matthew JY Kang,Wenli Yang,Monica R Roberts,Byeong Ho Kang,Charles B Malpas*

Main category: cs.AI

TL;DR: 论文探讨了大型语言模型（LLMs）在医疗诊断中的局限性，尤其是在痴呆症诊断和护理中，并提出了结合统计学习和专家知识的混合方法以提高可解释性和临床实用性。


<details>
  <summary>Details</summary>
Motivation: 尽管LLMs在基准测试中表现优异，但在实际临床环境中未能显著提升诊断效果，因此需要探索其局限性并提出改进方向。

Method: 通过文献综述分析了LLMs在临床中的局限性，并提出了结合统计学习和专家规则知识的混合方法，如PEIRS和ATHENA-CDS。

Result: 研究发现LLMs存在黑箱输出、幻觉问题和弱因果推理等局限性，而混合方法能提高可解释性和临床工作流程的适配性。

Conclusion: 未来AI决策支持应注重解释性，结合神经符号或混合AI，并衡量其对临床理解和患者结果的实际影响。

Abstract: The recent boom of large language models (LLMs) has re-ignited the hope that
artificial intelligence (AI) systems could aid medical diagnosis. Yet despite
dazzling benchmark scores, LLM assistants have yet to deliver measurable
improvements at the bedside. This scoping review aims to highlight the areas
where AI is limited to make practical contributions in the clinical setting,
specifically in dementia diagnosis and care.
  Standalone machine-learning models excel at pattern recognition but seldom
provide actionable, interpretable guidance, eroding clinician trust. Adjacent
use of LLMs by physicians did not result in better diagnostic accuracy or
speed. Key limitations trace to the data-driven paradigm: black-box outputs
which lack transparency, vulnerability to hallucinations, and weak causal
reasoning. Hybrid approaches that combine statistical learning with expert
rule-based knowledge, and involve clinicians throughout the process help bring
back interpretability. They also fit better with existing clinical workflows,
as seen in examples like PEIRS and ATHENA-CDS.
  Future decision-support should prioritise explanatory coherence by linking
predictions to clinically meaningful causes. This can be done through
neuro-symbolic or hybrid AI that combines the language ability of LLMs with
human causal expertise. AI researchers have addressed this direction, with
explainable AI and neuro-symbolic AI being the next logical steps in further
advancement in AI. However, they are still based on data-driven knowledge
integration instead of human-in-the-loop approaches. Future research should
measure success not only by accuracy but by improvements in clinician
understanding, workflow fit, and patient outcomes. A better understanding of
what helps improve human-computer interactions is greatly needed for AI systems
to become part of clinical practice.

</details>


### [23] [AI Agents and Agentic AI-Navigating a Plethora of Concepts for Future Manufacturing](https://arxiv.org/abs/2507.01376)
*Yinwang Ren,Yangyang Liu,Tang Ji,Xun Xu*

Main category: cs.AI

TL;DR: 本文探讨了基于生成式AI（GenAI）的AI代理（如LLM-Agents和MLLM-Agents）以及Agentic AI在智能制造中的潜力、挑战与整合问题。


<details>
  <summary>Details</summary>
Motivation: 随着生成式AI的快速发展，AI代理的能力显著提升，但其在智能制造中的定义、边界和实际应用尚不明确，需要系统性研究。

Method: 通过系统回顾AI和AI代理技术的演进，分析LLM-Agents、MLLM-Agents和Agentic AI的核心概念与技术进展，并探讨其在智能制造中的应用与挑战。

Result: 研究揭示了这些新兴AI范式在智能制造中的潜力，同时也指出了定义模糊、能力边界不清等挑战。

Conclusion: LLM-Agents、MLLM-Agents和Agentic AI为智能制造开辟了新途径，但需进一步明确其定义和解决实际应用中的问题。

Abstract: AI agents are autonomous systems designed to perceive, reason, and act within
dynamic environments. With the rapid advancements in generative AI (GenAI),
large language models (LLMs) and multimodal large language models (MLLMs) have
significantly improved AI agents' capabilities in semantic comprehension,
complex reasoning, and autonomous decision-making. At the same time, the rise
of Agentic AI highlights adaptability and goal-directed autonomy in dynamic and
complex environments. LLMs-based AI Agents (LLM-Agents), MLLMs-based AI Agents
(MLLM-Agents), and Agentic AI contribute to expanding AI's capabilities in
information processing, environmental perception, and autonomous
decision-making, opening new avenues for smart manufacturing. However, the
definitions, capability boundaries, and practical applications of these
emerging AI paradigms in smart manufacturing remain unclear. To address this
gap, this study systematically reviews the evolution of AI and AI agent
technologies, examines the core concepts and technological advancements of
LLM-Agents, MLLM-Agents, and Agentic AI, and explores their potential
applications in and integration into manufacturing, along with the potential
challenges they may face.

</details>


### [24] [A Fuzzy Approach to the Specification, Verification and Validation of Risk-Based Ethical Decision Making Models](https://arxiv.org/abs/2507.01410)
*Abeer Dyoub,Francesca A. Lisi*

Main category: cs.AI

TL;DR: 论文提出了一种基于伦理风险评估的形式化方法，用于描述道德决策模型，并通过模糊Petri网验证和验证这些模型。


<details>
  <summary>Details</summary>
Motivation: 道德领域的本体论和认识论复杂性使得难以建立明确的道德机器评估标准。

Method: 使用模糊规则描述道德决策模型，并通过模糊Petri网进行验证和验证。

Result: 通过医学领域的案例研究验证了方法的有效性。

Conclusion: 该方法为道德机器的伦理决策提供了一种可行的形式化框架。

Abstract: The ontological and epistemic complexities inherent in the moral domain make
it challenging to establish clear standards for evaluating the performance of a
moral machine. In this paper, we present a formal method to describe Ethical
Decision Making models based on ethical risk assessment. Then, we show how
these models that are specified as fuzzy rules can be verified and validated
using fuzzy Petri nets. A case study from the medical field is considered to
illustrate the proposed approach.

</details>


### [25] [Pensieve Grader: An AI-Powered, Ready-to-Use Platform for Effortless Handwritten STEM Grading](https://arxiv.org/abs/2507.01431)
*Yoonseok Yang,Minjune Kim,Marlon Rondinelli,Keren Shao*

Main category: cs.AI

TL;DR: Pensieve是一个AI辅助评分平台，利用大型语言模型（LLMs）转录和评估学生作业，显著减少评分时间并保持高准确性。


<details>
  <summary>Details</summary>
Motivation: 解决大规模STEM课程中手写开放答案评分的瓶颈问题。

Method: 结合LLMs和人工循环界面，支持从扫描提交到最终反馈的完整评分流程。

Result: 在20多所机构的实际课程中部署，评分30万份答案，评分时间减少65%，高置信度预测与教师评分一致率达95.4%。

Conclusion: Pensieve有效提升评分效率，同时保持高准确性，适用于多学科STEM课程。

Abstract: Grading handwritten, open-ended responses remains a major bottleneck in large
university STEM courses. We introduce Pensieve (https://www.pensieve.co), an
AI-assisted grading platform that leverages large language models (LLMs) to
transcribe and evaluate student work, providing instructors with rubric-aligned
scores, transcriptions, and confidence ratings. Unlike prior tools that focus
narrowly on specific tasks like transcription or rubric generation, Pensieve
supports the entire grading pipeline-from scanned student submissions to final
feedback-within a human-in-the-loop interface.
  Pensieve has been deployed in real-world courses at over 20 institutions and
has graded more than 300,000 student responses. We present system details and
empirical results across four core STEM disciplines: Computer Science,
Mathematics, Physics, and Chemistry. Our findings show that Pensieve reduces
grading time by an average of 65%, while maintaining a 95.4% agreement rate
with instructor-assigned grades for high-confidence predictions.

</details>


### [26] [Using multi-agent architecture to mitigate the risk of LLM hallucinations](https://arxiv.org/abs/2507.01446)
*Abd Elrahman Amer,Magdi Amer*

Main category: cs.AI

TL;DR: 提出了一种多代理系统，结合LLM和模糊逻辑，用于处理客户短信请求，以减少幻觉风险。


<details>
  <summary>Details</summary>
Motivation: 提高客户服务质量和响应时间是维持客户忠诚度和增加市场份额的关键，但采用LLM等技术时幻觉风险是主要挑战。

Method: 设计了一个多代理系统，整合基于LLM的代理和模糊逻辑来处理客户短信请求。

Result: 系统通过模糊逻辑有效减少了LLM的幻觉风险。

Conclusion: 多代理系统结合模糊逻辑是减少LLM幻觉风险的有效方法。

Abstract: Improving customer service quality and response time are critical factors for
maintaining customer loyalty and increasing a company's market share. While
adopting emerging technologies such as Large Language Models (LLMs) is becoming
a necessity to achieve these goals, the risk of hallucination remains a major
challenge. In this paper, we present a multi-agent system to handle customer
requests sent via SMS. This system integrates LLM based agents with fuzzy logic
to mitigate hallucination risks.

</details>


### [27] [Agent-as-Tool: A Study on the Hierarchical Decision Making with Reinforcement Learning](https://arxiv.org/abs/2507.01489)
*Yanfei Zhang*

Main category: cs.AI

TL;DR: 提出了一个分层框架Agent-as-tool，将工具调用过程与推理过程分离，以减轻模型负担，并在少量样本微调下取得优异性能。


<details>
  <summary>Details</summary>
Motivation: 现有研究同时处理工具调用和推理过程，导致模型负担重且推理效率低。

Method: 提出分层框架Agent-as-tool，分离工具调用与推理过程，由不同代理处理。

Result: 在180个样本微调下取得优异性能，Bamboogle任务中表现突出。

Conclusion: 分层框架有效提升推理效率，性能优于现有方法。

Abstract: Large Language Models (LLMs) have emerged as one of the most significant
technological advancements in artificial intelligence in recent years. Their
ability to understand, generate, and reason with natural language has
transformed how we interact with AI systems. With the development of LLM-based
agents and reinforcement-learning-based reasoning models, the study of applying
reinforcement learning in agent frameworks has become a new research focus.
However, all previous studies face the challenge of deciding the tool calling
process and the reasoning process simultaneously, and the chain of reasoning
was solely relied on the unprocessed raw result with redundant information and
symbols unrelated to the task from the tool, which impose a heavy burden on the
model's capability to reason. Therefore, in our research, we proposed a
hierarchical framework Agent-as-tool that detach the tool calling process and
the reasoning process, which enables the model to focus on the verbally
reasoning process while the tool calling process is handled by another agent.
Our work had achieved comparable results with only a slight reinforcement
fine-tuning on 180 samples, and had achieved exceptionally well performance in
Bamboogle with 63.2% of exact match and 75.2% in cover exact match, exceeding
Search-R1 by 4.8% in exact match and 3.2% in cover exact match.

</details>


### [28] [T3DM: Test-Time Training-Guided Distribution Shift Modelling for Temporal Knowledge Graph Reasoning](https://arxiv.org/abs/2507.01597)
*Yuehang Si,Zefan Zeng,Jincai Huang,Qing Cheng*

Main category: cs.AI

TL;DR: 论文提出了一种新的分布特征建模方法T3DM，用于解决TKG推理中的分布偏移和负采样质量问题，并通过实验验证了其优越性。


<details>
  <summary>Details</summary>
Motivation: 现有TKG推理方法在建模训练与测试样本间的事件分布偏移时表现不足，且负采样质量低。

Method: 提出T3DM方法，通过测试时训练调整分布偏移，并设计基于对抗训练的负采样策略。

Result: 实验表明T3DM在多数情况下优于现有基线方法。

Conclusion: T3DM能有效解决分布偏移和负采样问题，提升推理性能。

Abstract: Temporal Knowledge Graph (TKG) is an efficient method for describing the
dynamic development of facts along a timeline. Most research on TKG reasoning
(TKGR) focuses on modelling the repetition of global facts and designing
patterns of local historical facts. However, they face two significant
challenges: inadequate modeling of the event distribution shift between
training and test samples, and reliance on random entity substitution for
generating negative samples, which often results in low-quality sampling. To
this end, we propose a novel distributional feature modeling approach for
training TKGR models, Test-Time Training-guided Distribution shift Modelling
(T3DM), to adjust the model based on distribution shift and ensure the global
consistency of model reasoning. In addition, we design a negative-sampling
strategy to generate higher-quality negative quadruples based on adversarial
training. Extensive experiments show that T3DM provides better and more robust
results than the state-of-the-art baselines in most cases.

</details>


### [29] [Agent Ideate: A Framework for Product Idea Generation from Patents Using Agentic AI](https://arxiv.org/abs/2507.01717)
*Gopichand Kanumolu,Ashok Urlana,Charaka Vinayak Kumar,Bala Mallikarjunarao Garlapati*

Main category: cs.AI

TL;DR: 利用大型语言模型（LLMs）和自主代理从专利中挖掘并生成产品概念，提出Agent Ideate框架，实验证明代理方法在创意质量、相关性和新颖性上优于单独使用LLMs。


<details>
  <summary>Details</summary>
Motivation: 专利蕴含丰富的技术知识，但获取和解读这些信息仍具挑战性，希望通过LLMs和代理方法挖掘其创新潜力。

Method: 设计Agent Ideate框架，结合开源LLMs和代理架构，在计算机科学、自然语言处理和材料化学三个领域进行实验。

Result: 代理方法在创意质量、相关性和新颖性上均优于单独使用LLMs。

Conclusion: 结合LLMs与代理工作流可显著提升从专利数据生成商业创意的潜力。

Abstract: Patents contain rich technical knowledge that can inspire innovative product
ideas, yet accessing and interpreting this information remains a challenge.
This work explores the use of Large Language Models (LLMs) and autonomous
agents to mine and generate product concepts from a given patent. In this work,
we design Agent Ideate, a framework for automatically generating product-based
business ideas from patents. We experimented with open-source LLMs and
agent-based architectures across three domains: Computer Science, Natural
Language Processing, and Material Chemistry. Evaluation results show that the
agentic approach consistently outperformed standalone LLMs in terms of idea
quality, relevance, and novelty. These findings suggest that combining LLMs
with agentic workflows can significantly enhance the innovation pipeline by
unlocking the untapped potential of business idea generation from patent data.

</details>


### [30] [Joint Matching and Pricing for Crowd-shipping with In-store Customers](https://arxiv.org/abs/2507.01749)
*Arash Dehghan,Mucahit Cevik,Merve Bodur,Bissan Ghaddar*

Main category: cs.AI

TL;DR: 论文探讨了利用店内顾客作为配送员的集中式众包配送系统，提出了一种结合MDP、NeurADP和DDQN的动态优化策略，显著降低了配送成本。


<details>
  <summary>Details</summary>
Motivation: 针对城市最后一公里配送效率的需求增长，研究如何利用现有顾客资源优化配送。

Method: 提出基于MDP的模型，结合NeurADP和DDQN进行动态订单分配和定价。

Result: 实验显示，该策略比固定定价和短视基准分别节省6.7%和18%的成本，灵活配送和多目的地路由进一步降低成本。

Conclusion: 动态前瞻性策略在众包配送系统中具有显著优势，为城市物流运营商提供了实用指导。

Abstract: This paper examines the use of in-store customers as delivery couriers in a
centralized crowd-shipping system, targeting the growing need for efficient
last-mile delivery in urban areas. We consider a brick-and-mortar retail
setting where shoppers are offered compensation to deliver time-sensitive
online orders. To manage this process, we propose a Markov Decision Process
(MDP) model that captures key uncertainties, including the stochastic arrival
of orders and crowd-shippers, and the probabilistic acceptance of delivery
offers. Our solution approach integrates Neural Approximate Dynamic Programming
(NeurADP) for adaptive order-to-shopper assignment with a Deep Double Q-Network
(DDQN) for dynamic pricing. This joint optimization strategy enables multi-drop
routing and accounts for offer acceptance uncertainty, aligning more closely
with real-world operations. Experimental results demonstrate that the
integrated NeurADP + DDQN policy achieves notable improvements in delivery cost
efficiency, with up to 6.7\% savings over NeurADP with fixed pricing and
approximately 18\% over myopic baselines. We also show that allowing flexible
delivery delays and enabling multi-destination routing further reduces
operational costs by 8\% and 17\%, respectively. These findings underscore the
advantages of dynamic, forward-looking policies in crowd-shipping systems and
offer practical guidance for urban logistics operators.

</details>


### [31] [Refining Gelfond Rationality Principle Towards More Comprehensive Foundational Principles for Answer Set Semantics](https://arxiv.org/abs/2507.01833)
*Yi-Dong Shen,Thomas Eiter*

Main category: cs.AI

TL;DR: 论文探讨了非单调逻辑编程中的答案集语义，质疑了现有条件的普适性，并提出了基于Gelfond答案集原则的改进方法。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于探讨现有答案集语义的强制性条件是否过于严格，以及如何通过改进原则来更合理地定义答案集语义。

Method: 方法包括：1) 分析现有条件的局限性；2) 改进Gelfond答案集原则，提出支持性、默认否定最小化和认知否定最小化；3) 扩展支持性概念；4) 定义新的答案集语义；5) 评估现有语义；6) 计算复杂性分析。

Result: 结果表明，现有条件有时过于严格，改进后的原则能更合理地定义答案集语义，并支持更广泛的预期答案集。

Conclusion: 结论指出，改进的Gelfond答案集原则为答案集语义提供了更灵活且合理的基础，同时保持了计算复杂性。

Abstract: Non-monotonic logic programming is the basis for a declarative problem
solving paradigm known as answer set programming (ASP). Departing from the
seminal definition by Gelfond and Lifschitz in 1988 for simple normal logic
programs, various answer set semantics have been proposed for extensions. We
consider two important questions: (1) Should the minimal model property,
constraint monotonicity and foundedness as defined in the literature be
mandatory conditions for an answer set semantics in general? (2) If not, what
other properties could be considered as general principles for answer set
semantics? We address the two questions. First, it seems that the three
aforementioned conditions may sometimes be too strong, and we illustrate with
examples that enforcing them may exclude expected answer sets. Second, we
evolve the Gelfond answer set (GAS) principles for answer set construction by
refining the Gelfond's rationality principle to well-supportedness, minimality
w.r.t. negation by default and minimality w.r.t. epistemic negation. The
principle of well-supportedness guarantees that every answer set is
constructible from if-then rules obeying a level mapping and is thus free of
circular justification, while the two minimality principles ensure that the
formalism minimizes knowledge both at the level of answer sets and of world
views. Third, to embody the refined GAS principles, we extend the notion of
well-supportedness substantially to answer sets and world views, respectively.
Fourth, we define new answer set semantics in terms of the refined GAS
principles. Fifth, we use the refined GAS principles as an alternative baseline
to intuitively assess the existing answer set semantics. Finally, we analyze
the computational complexity.

</details>
