<div id=toc></div>

# Table of Contents

- [cs.CR](#cs.CR) [Total: 10]
- [cs.SE](#cs.SE) [Total: 13]
- [cs.AI](#cs.AI) [Total: 10]


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [1] [Algorithmic Identity Based on Metaparameters: A Path to Reliability, Auditability, and Traceability](https://arxiv.org/abs/2601.16234)
*Juliao Braga,Percival Henriques,Juliana C. Braga,Itana Stiubiener*

Main category: cs.CR

TL;DR: 本文探讨使用数字对象标识符（DOI）来标识算法，以增强AI算法在开发和应用中的问责制、透明度和可靠性。


<details>
  <summary>Details</summary>
Motivation: 随着算法在医疗、司法、金融和教育等领域的广泛应用，特别是2022年以来基于大语言模型的AI技术快速发展，带来了问责制、伦理和透明度方面的重大挑战。需要一种机制来追踪算法来源、促进审计、防止偏见并加强伦理考量。

Method: 提出使用数字对象标识符（DOI）来标识算法，建立算法溯源机制。讨论了DOI标识算法的维护挑战与解决方案，在API安全中的应用，并提出了一种加密认证协议。

Result: DOI能够有效追踪算法起源、支持审计流程、防止算法偏见、促进研究可重复性，并加强伦理考量。特别是在AI智能体和多模态大语言模型中的应用具有重要价值。

Conclusion: 使用DOI标识算法是增强算法问责制、透明度和可靠性的有效途径，有助于应对AI技术快速发展带来的伦理和治理挑战，为算法治理提供了实用的技术解决方案。

Abstract: The use of algorithms is increasing across various fields such as healthcare, justice, finance, and education. This growth has significantly accelerated with the advent of Artificial Intelligence (AI) technologies based on Large Language Models (LLMs) since 2022. This expansion presents substantial challenges related to accountability, ethics, and transparency. This article explores the potential of the Digital Object Identifier (DOI) to identify algorithms, aiming to enhance accountability, transparency, and reliability in their development and application, particularly in AI agents and multimodal LLMs. The use of DOIs facilitates tracking the origin of algorithms, enables audits, prevents biases, promotes research reproducibility, and strengthens ethical considerations. The discussion addresses the challenges and solutions associated with maintaining algorithms identified by DOI, their application in API security, and the proposal of a cryptographic authentication protocol.

</details>


### [2] [FC-GUARD: Enabling Anonymous yet Compliant Fiat-to-Cryptocurrency Exchanges](https://arxiv.org/abs/2601.16298)
*Shaoyu Li,Hexuan Yu,Md Mohaimin Al Barat,Yang Xiao,Y. Thomas Hou,Wenjing Lou*

Main category: cs.CR

TL;DR: FC-GUARD是一个保护隐私的法币-加密货币兑换系统，使用可验证凭证和零知识证明技术，在满足监管要求的同时保护用户匿名性。


<details>
  <summary>Details</summary>
Motivation: 当前法币-加密货币兑换平台存在隐私保护不足的问题，实际数据泄露事件暴露了个人身份信息（PII）和加密货币地址，使得攻击者能够将现实世界身份与加密货币交易关联起来，破坏了加密货币使用的匿名性预期。

Method: 提出FC-GUARD系统，利用可验证凭证和零知识证明技术，在法币-加密货币兑换过程中不泄露用户的PII或法币账户详情。系统集成了合法的去匿名化机制，允许审计机构识别不当行为的用户，从而满足监管合规要求。

Result: 在桌面和移动平台上实现了FC-GUARD系统，评估表明该系统具有实际部署的可行性。

Conclusion: FC-GUARD能够在保护用户匿名性的同时满足监管合规要求，打破了用户现实世界身份与加密货币地址之间的关联，维护了加密货币生态系统中的基本匿名性预期。

Abstract: With the rise of decentralized finance, fiat-to-cryptocurrency exchange platforms have become popular entry points into the cryptocurrency ecosystem. However, these platforms frequently fail to ensure adequate privacy protection, as evidenced by real-world breaches that exposed personally identifiable information (PII) and crypto addresses. Such leaks enable adversaries to link real-world identities to cryptocurrency transactions, undermining the presumed anonymity of cryptocurrency use.
  We propose FC-GUARD, a privacy-preserving exchange system designed to preserve user anonymity without compromising regulatory compliance in the exchange of fiat currency for cryptocurrencies. Leveraging verifiable credentials and zero-knowledge proof techniques, FC-GUARD enables fiat-to-cryptocurrency exchanges without revealing users' PII or fiat account details. This breaks the linkage between users' real-world identities and their cryptocurrency addresses, thereby upholding anonymity, a fundamental expectation in the cryptocurrency ecosystem. In addition, FC-GUARD complies with key regulations over cryptocurrency usage, such as know-your-customer requirements and auditability for tax reporting obligations by integrating a lawful de-anonymization mechanism that allows the auditing authority to identify misbehaving users. This ensures regulatory compliance while defaulting to privacy protection. We implement our system on both desktop and mobile platforms, and our evaluation shows its feasibility for practical deployment.

</details>


### [3] [NOIR: Privacy-Preserving Generation of Code with Open-Source LLMs](https://arxiv.org/abs/2601.16354)
*Khoa Nguyen,Khiem Ton,NhatHai Phan,Issa Khalil,Khang Tran,Cristian Borcea,Ruoming Jin,Abdallah Khreishah,My T. Thai*

Main category: cs.CR

TL;DR: NOIR是首个保护客户端提示词和生成代码隐私的框架，通过本地差分隐私和随机化分词器防止云服务商窥探，在保持高性能的同时实现强隐私保护。


<details>
  <summary>Details</summary>
Motivation: LLM驱动的代码生成虽然提升了开发效率，但存在知识产权和数据安全风险，因为云服务商能够观察到客户端的提示词和生成的代码，这些在商业系统中可能是专有信息。

Method: NOIR框架在客户端使用编码器和解码器，将提示词编码为嵌入向量发送到云端获取LLM增强的嵌入向量，然后在客户端本地解码生成代码。采用词汇级本地差分隐私保护和客户端数据无关的随机化分词器，防御重构和频率分析攻击。

Result: 在开源LLM上的广泛分析显示，NOIR在多个基准测试中显著优于现有基线：Evalplus（MBPP Pass@1 76.7%，HumanEval Pass@1 77.4%）和BigCodeBench（Pass@1 38.7%，仅比原始LLM下降1.77%），同时在强隐私保护下有效防御攻击。

Conclusion: NOIR是首个有效保护代码生成隐私的框架，通过创新的隐私保护机制在保持高性能的同时防御诚实但好奇的云服务商攻击，解决了LLM代码生成中的知识产权和数据安全问题。

Abstract: Although boosting software development performance, large language model (LLM)-powered code generation introduces intellectual property and data security risks rooted in the fact that a service provider (cloud) observes a client's prompts and generated code, which can be proprietary in commercial systems. To mitigate this problem, we propose NOIR, the first framework to protect the client's prompts and generated code from the cloud. NOIR uses an encoder and a decoder at the client to encode and send the prompts' embeddings to the cloud to get enriched embeddings from the LLM, which are then decoded to generate the code locally at the client. Since the cloud can use the embeddings to infer the prompt and the generated code, NOIR introduces a new mechanism to achieve indistinguishability, a local differential privacy protection at the token embedding level, in the vocabulary used in the prompts and code, and a data-independent and randomized tokenizer on the client side. These components effectively defend against reconstruction and frequency analysis attacks by an honest-but-curious cloud. Extensive analysis and results using open-source LLMs show that NOIR significantly outperforms existing baselines on benchmarks, including the Evalplus (MBPP and HumanEval, Pass@1 of 76.7 and 77.4), and BigCodeBench (Pass@1 of 38.7, only a 1.77% drop from the original LLM) under strong privacy against attacks.

</details>


### [4] [Ringmaster: How to juggle high-throughput host OS system calls from TrustZone TEEs](https://arxiv.org/abs/2601.16448)
*Richard Habeeb,Man-Ki Yoon,Hao Chen Zhong Shao*

Main category: cs.CR

TL;DR: Ringmaster是一个新颖的框架，允许可信执行环境（TEE）通过Linux的io_uring异步访问丰富但可能不可信的操作系统服务，同时在服务被拒绝时继续在ARM TrustZone内核上运行，确保时间敏感程序的安全处理。


<details>
  <summary>Details</summary>
Motivation: 安全关键系统需要及时处理传感器输入以避免安全隐患，但这些系统通常运行大型操作系统，存在安全漏洞风险。恶意攻击者获得超级用户权限后可能拒绝时间敏感程序的服务，造成实际损害。现有方法完全隔离时间敏感程序，但阻止了它们访问有用的操作系统服务。

Method: Ringmaster框架使TEE能够通过Linux的io_uring异步访问丰富但可能不可信的操作系统服务。当不可信操作系统拒绝服务时，enclaves继续在Ringmaster的最小ARM TrustZone内核上运行，访问关键设备驱动程序。支持大型未修改程序作为enclaves，相比现有系统具有更低开销。

Result: 在树莓派4B上的实验中，Ringmaster实现了近1GiB/秒的数据传输到enclave，与非enclave任务相比仅有0-3%的吞吐量开销。通过无人机实验展示了如何构建高度安全的系统。

Conclusion: Ringmaster在安全的时间敏感处理与丰富的操作系统服务便利性之间取得了平衡，支持大型未修改程序作为enclaves，相比现有系统具有更低开销，能够构建高度安全的系统。

Abstract: Many safety-critical systems require timely processing of sensor inputs to avoid potential safety hazards. Additionally, to support useful application features, such systems increasingly have a large rich operating system (OS) at the cost of potential security bugs. Thus, if a malicious party gains supervisor privileges, they could cause real-world damage by denying service to time-sensitive programs. Many past approaches to this problem completely isolate time-sensitive programs with a hypervisor; however, this prevents the programs from accessing useful OS services
  We introduce Ringmaster, a novel framework that enables enclaves or TEEs (Trusted Execution Environments) to asynchronously access rich, but potentially untrusted, OS services via Linux's io_uring. When service is denied by the untrusted OS, enclaves continue to operate on Ringmaster's minimal ARM TrustZone kernel with access to small, critical device drivers. This approach balances the need for secure, time-sensitive processing with the convenience of rich OS services. Additionally, Ringmaster supports large unmodified programs as enclaves, offering lower overhead compared to existing systems. We demonstrate how Ringmaster helps us build a working highly-secure system with minimal engineering. In our experiments with an unmanned aerial vehicle, Ringmaster achieved nearly 1GiB/sec of data into enclave on a Raspberry Pi4b, 0-3% throughput overhead compared to non-enclave tasks.

</details>


### [5] [Cutting the Gordian Knot: Detecting Malicious PyPI Packages via a Knowledge-Mining Framework](https://arxiv.org/abs/2601.16463)
*Wenbo Guo,Chengwei Liu,Ming Kang,Yiran Zhang,Jiahui Wu,Zhengzi Xu,Vinay Sachidananda,Yang Liu*

Main category: cs.CR

TL;DR: PyGuard是一个基于知识驱动的恶意Python包检测框架，通过语义理解而非简单语法规则，将现有工具的误报转化为有用行为知识，显著提高了检测准确率并减少了误报。


<details>
  <summary>Details</summary>
Motivation: PyPI已成为恶意攻击者的目标，现有检测工具误报率高达15-30%，错误地将三分之一合法包标记为恶意。这是因为当前工具依赖简单语法规则而非语义理解，无法区分相同API调用在合法与恶意用途中的差异。

Method: PyGuard采用知识驱动框架，通过分层模式挖掘从现有工具的误报和漏报中提取行为模式，使用大语言模型创建超越语法变异的语义抽象，并将这些知识整合到检测系统中，结合精确模式匹配和上下文推理。

Result: PyGuard达到99.50%的准确率，仅有2个误报（相比现有工具的1,927-2,117个误报），在混淆代码上保持98.28%的准确率，在真实部署中识别了219个先前未知的恶意包。行为模式展示跨生态系统适用性，在NPM包上达到98.07%的准确率。

Conclusion: 语义理解使得知识能够跨编程语言迁移，PyGuard通过将检测失败转化为有用行为知识，显著提高了恶意包检测的准确性和鲁棒性，证明了语义理解在安全检测中的重要性。

Abstract: The Python Package Index (PyPI) has become a target for malicious actors, yet existing detection tools generate false positive rates of 15-30%, incorrectly flagging one-third of legitimate packages as malicious. This problem arises because current tools rely on simple syntactic rules rather than semantic understanding, failing to distinguish between identical API calls serving legitimate versus malicious purposes. To address this challenge, we propose PyGuard, a knowledge-driven framework that converts detection failures into useful behavioral knowledge by extracting patterns from existing tools' false positives and negatives. Our method utilizes hierarchical pattern mining to identify behavioral sequences that distinguish malicious from benign code, employs Large Language Models to create semantic abstractions beyond syntactic variations, and combines this knowledge into a detection system that integrates exact pattern matching with contextual reasoning. PyGuard achieves 99.50% accuracy with only 2 false positives versus 1,927-2,117 in existing tools, maintains 98.28% accuracy on obfuscated code, and identified 219 previously unknown malicious packages in real-world deployment. The behavioral patterns show cross-ecosystem applicability with 98.07% accuracy on NPM packages, demonstrating that semantic understanding enables knowledge transfer across programming languages.

</details>


### [6] [DeMark: A Query-Free Black-Box Attack on Deepfake Watermarking Defenses](https://arxiv.org/abs/2601.16473)
*Wei Song,Zhenchang Xing,Liming Zhu,Yulei Sui,Jingling Xue*

Main category: cs.CR

TL;DR: DeMark攻击框架挑战防御性水印假设，通过压缩感知稀疏化过程利用编码器-解码器水印模型的潜在空间漏洞，将水印检测准确率从100%降至32.9%，同时保持视觉质量。


<details>
  <summary>Details</summary>
Motivation: 当前防御性水印方案被假设为抗移除的，但作者挑战这一假设，因为深度伪造的快速扩散需要更可靠的检测和溯源方法。

Method: DeMark是一个无需查询的黑盒攻击框架，针对深度伪造的防御性图像水印方案。它通过基于压缩感知的稀疏化过程，利用编码器-解码器水印模型的潜在空间漏洞，抑制水印信号同时保持感知和结构真实性。

Result: 在八种最先进的水印方案上，DeMark将水印检测准确率从100%平均降至32.9%，同时保持自然视觉质量，优于现有攻击方法。三种防御策略（图像超分辨率、稀疏水印和对抗训练）基本无效。

Conclusion: 当前编码器-解码器水印方案对潜在空间操作仍然脆弱，需要更鲁棒的水印方法来防范深度伪造。

Abstract: The rapid proliferation of realistic deepfakes has raised urgent concerns over their misuse, motivating the use of defensive watermarks in synthetic images for reliable detection and provenance tracking. However, this defense paradigm assumes such watermarks are inherently resistant to removal. We challenge this assumption with DeMark, a query-free black-box attack framework that targets defensive image watermarking schemes for deepfakes. DeMark exploits latent-space vulnerabilities in encoder-decoder watermarking models through a compressive sensing based sparsification process, suppressing watermark signals while preserving perceptual and structural realism appropriate for deepfakes. Across eight state-of-the-art watermarking schemes, DeMark reduces watermark detection accuracy from 100% to 32.9% on average while maintaining natural visual quality, outperforming existing attacks. We further evaluate three defense strategies, including image super resolution, sparse watermarking, and adversarial training, and find them largely ineffective. These results demonstrate that current encoder decoder watermarking schemes remain vulnerable to latent-space manipulations, underscoring the need for more robust watermarking methods to safeguard against deepfakes.

</details>


### [7] [A High Performance and Efficient Post-Quantum Crypto-Processor for FrodoKEM](https://arxiv.org/abs/2601.16500)
*Kai Li,Jiahao Lu,Fu Yao,Guang Zeng,Dongsheng Liu,Shengfei Gu,Zhengpeng Zhao,Jiachen Wang*

Main category: cs.CR

TL;DR: 本文提出了一种高性能的FrodoKEM密码处理器，通过多重指令重叠执行、可重构并行乘法器阵列和紧凑内存调度策略，显著降低了硬件实现延迟和资源消耗，实现了最快的执行时间和1.75-2.00倍的面积-时间积改进。


<details>
  <summary>Details</summary>
Motivation: FrodoKEM是一种基于格的后量子密钥封装机制，已被ISO考虑标准化，但其硬件实现存在高延迟和重资源负担的问题，且多样化使用场景需要全面的功能支持。

Method: 1. 引入多重指令重叠执行方案，实现高效的多模块调度和最小化操作延迟；2. 集成高速可重构并行乘法器阵列，处理不同计算模式下的密集矩阵计算；3. 采用紧凑内存调度策略，缩短中间矩阵的生命周期，减少存储需求。

Result: 在Artix-7 FPGA上消耗13467个LUT、6042个FF和14个BRAM，实现了最快的执行时间。与最先进的硬件实现相比，面积-时间积提高了1.75-2.00倍，并完全支持所有FrodoKEM安全级别和协议阶段。

Conclusion: 提出的高性能密码处理器有效解决了FrodoKEM硬件实现中的延迟和资源问题，通过创新的架构设计显著提升了硬件效率，为后量子密码的实际应用提供了可行的硬件解决方案。

Abstract: FrodoKEM is a lattice-based post-quantum key encapsulation mechanism (KEM). It has been considered for standardization by the International Organization for Standardization (ISO) due to its robust security profile. However, its hardware implementation exhibits a weakness of high latency and heavy resource burden, hindering its practical application. Moreover, diverse usage scenarios call for comprehensive functionality. To address these challenges, this paper presents a high-performance and efficient crypto-processor for FrodoKEM. A multiple-instruction overlapped execution scheme is introduced to enable efficient multi-module scheduling and minimize operational latency. Furthermore, a high-speed, reconfigurable parallel multiplier array is integrated to handle intensive matrix computations under diverse computation patterns, significantly enhancing hardware efficiency. In addition, a compact memory scheduling strategy shortens the lifespan of intermediate matrices, thereby reducing overall storage requirements. The proposed design provides full support for all FrodoKEM security levels and protocol phases. It consumes 13467 LUTs, 6042 FFs, and 14 BRAMs on an Artix-7 FPGA and achieves the fastest reported execution time. Compared with state-of-the-art hardware implementations, our design improves the area-time product (ATP) by 1.75-2.00 times.

</details>


### [8] [SafeThinker: Reasoning about Risk to Deepen Safety Beyond Shallow Alignment](https://arxiv.org/abs/2601.16506)
*Xianya Fang,Xianying Luo,Yadong Wang,Xiang Chen,Yu Tian,Zequn Sun,Rui Liu,Jun Fang,Naiqiang Tan,Yuanning Cui,Sheng-Jun Huang*

Main category: cs.CR

TL;DR: SafeThinker是一个自适应防御框架，通过轻量级网关分类器动态分配防御资源，针对不同风险级别的输入采用三种机制处理，显著降低越狱攻击成功率同时保持模型实用性。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型具有内在的风险意识，但现有防御方法往往导致浅层安全对齐，使模型容易受到伪装攻击（如预填充）的影响，同时降低了实用性。需要一种能平衡鲁棒性和实用性的解决方案。

Method: 提出SafeThinker框架：1）轻量级网关分类器进行风险评估；2）标准化拒绝机制处理明确威胁；3）安全感知双专家模块拦截伪装为良性查询的欺骗性攻击；4）分布引导思考组件在不确定生成时自适应干预。

Result: 实验表明，SafeThinker显著降低了多种越狱策略的攻击成功率，同时不损害模型实用性，证明通过在整个生成过程中协调内在判断可以有效平衡鲁棒性和实用性。

Conclusion: SafeThinker通过动态分配防御资源和在整个生成过程中协调内在判断，实现了对伪装攻击的有效防御，同时保持了模型的实用性，为解决LLM安全对齐问题提供了有效方案。

Abstract: Despite the intrinsic risk-awareness of Large Language Models (LLMs), current defenses often result in shallow safety alignment, rendering models vulnerable to disguised attacks (e.g., prefilling) while degrading utility. To bridge this gap, we propose SafeThinker, an adaptive framework that dynamically allocates defensive resources via a lightweight gateway classifier. Based on the gateway's risk assessment, inputs are routed through three distinct mechanisms: (i) a Standardized Refusal Mechanism for explicit threats to maximize efficiency; (ii) a Safety-Aware Twin Expert (SATE) module to intercept deceptive attacks masquerading as benign queries; and (iii) a Distribution-Guided Think (DDGT) component that adaptively intervenes during uncertain generation. Experiments show that SafeThinker significantly lowers attack success rates across diverse jailbreak strategies without compromising utility, demonstrating that coordinating intrinsic judgment throughout the generation process effectively balances robustness and practicality.

</details>


### [9] [From Transactions to Exploits: Automated PoC Synthesis for Real-World DeFi Attacks](https://arxiv.org/abs/2601.16681)
*Xing Su,Hao Wu,Hanzhong Liang,Yunlin Jiang,Yuxi Cheng,Yating Liu,Fengyuan Xu*

Main category: cs.CR

TL;DR: TracExp：首个从链上攻击执行中自动合成可验证PoC的框架，通过追踪驱动逆向工程和LLM代码生成，成功率为93%，平均成本仅0.07美元


<details>
  <summary>Details</summary>
Motivation: 区块链系统面临日益严重的链上攻击，这些攻击利用合约漏洞快速隐蔽地提取价值，使得系统分析和复现极为困难。传统手动制作PoC的方法劳动密集、需要专业知识且难以扩展。

Method: 提出TracExp框架：1）从低层级交易追踪中通过追踪驱动逆向工程恢复攻击者逻辑；2）从多合约追踪中定位攻击相关执行上下文；3）引入双反编译器将具体执行转化为语义丰富的漏洞利用伪代码；4）基于此表示合成PoC并精炼以保留可利用性相关语义；5）利用LLM的代码生成能力生成可执行漏洞利用。

Result: 在20个月内321个真实攻击案例上评估：成功合成93%事件的PoC，其中58.78%可直接验证，平均成本仅0.07美元/案例。TracExp向社区发布了大量先前不可用的PoC，获得900美元赏金，展示强大实际影响。

Conclusion: TracExp是首个从链上攻击执行自动合成可验证PoC的框架，通过追踪驱动逆向工程和LLM代码生成，显著降低了区块链攻击分析和复现的门槛，具有重要的实际应用价值。

Abstract: Blockchain systems are increasingly targeted by on-chain attacks that exploit contract vulnerabilities to extract value rapidly and stealthily, making systematic analysis and reproduction highly challenging. In practice, reproducing such attacks requires manually crafting proofs-of-concept (PoCs), a labor-intensive process that demands substantial expertise and scales poorly. In this work, we present the first automated framework for synthesizing verifiable PoCs directly from on-chain attack executions. Our key insight is that attacker logic can be recovered from low-level transaction traces via trace-driven reverse engineering, and then translated into executable exploits by leveraging the code-generation capabilities of large language models (LLMs). To this end, we propose TracExp, which localizes attack-relevant execution contexts from noisy, multi-contract traces and introduces a novel dual-decompiler to transform concrete executions into semantically enriched exploit pseudocode. Guided by this representation, TracExp synthesizes PoCs and refines them to preserve exploitability-relevant semantics. We evaluate TracExp on 321 real-world attacks over the past 20 months. TracExp successfully synthesizes PoCs for 93% of incidents, with 58.78% being directly verifiable, at an average cost of only \$0.07 per case. Moreover, TracExp enabled the release of a large number of previously unavailable PoCs to the community, earning a $900 bounty and demonstrating strong practical impact.

</details>


### [10] [Building a Robust Risk-Based Access Control System to Combat Ransomware's Capability to Encrypt: A Machine Learning Approach](https://arxiv.org/abs/2601.16795)
*Kenan Begovic,Abdulaziz Al-Ali,Qutaibah Malluhi*

Main category: cs.CR

TL;DR: 提出基于机器学习和强制访问控制的概率性风险访问控制架构，通过函数级内核追踪实时监控Linux加密活动，在保持合法加密工作流的同时识别和阻止勒索软件恶意加密行为。


<details>
  <summary>Details</summary>
Motivation: 勒索软件的核心能力——未经授权的加密——需要能够识别和阻止恶意加密活动而不干扰合法使用的控制机制。现有方法如沙箱、虚拟机自省或粗粒度系统调用遥测存在性能开销或检测粒度不足的问题。

Method: 使用Linux原生ftrace框架的function_graph追踪器构建高分辨率内核函数执行轨迹数据集，结合资源计数器和I/O计数器。基于此数据集训练监督分类器并提取可解释规则，通过轻量级布尔值驱动SELinux策略，在加密开始时实现上下文敏感的允许/拒绝决策。

Result: 双层组合（分类器+规则）保持了模型级检测质量，同时提供类似规则的响应速度。原型系统在突发I/O下有一定开销，但量化了这些开销并提出了生产级内核空间解决方案的优化方向。

Conclusion: 该研究为生产Linux系统提供了一条从行为追踪和学习到可执行、可解释、风险成比例的加密控制的实用路径，能够有效区分恶意和良性加密工作流。

Abstract: Ransomware core capability, unauthorized encryption, demands controls that identify and block malicious cryptographic activity without disrupting legitimate use. We present a probabilistic, risk-based access control architecture that couples machine learning inference with mandatory access control to regulate encryption on Linux in real time. The system builds a specialized dataset from the native ftrace framework using the function_graph tracer, yielding high-resolution kernel-function execution traces augmented with resource and I/O counters. These traces support both a supervised classifier and interpretable rules that drive an SELinux policy via lightweight booleans, enabling context-sensitive permit/deny decisions at the moment encryption begins. Compared to approaches centered on sandboxing, hypervisor introspection, or coarse system-call telemetry, the function-level tracing we adopt provides finer behavioral granularity than syscall-only telemetry while avoiding the virtualization/VMI overhead of sandbox-based approaches. Our current user-space prototype has a non-trivial footprint under burst I/O; we quantify it and recognize that a production kernel-space solution should aim to address this. We detail dataset construction, model training and rule extraction, and the run-time integration that gates file writes for suspect encryption while preserving benign cryptographic workflows. During evaluation, the two-layer composition retains model-level detection quality while delivering rule-like responsiveness; we also quantify operational footprint and outline engineering steps to reduce CPU and memory overhead for enterprise deployment. The result is a practical path from behavioral tracing and learning to enforceable, explainable, and risk-proportionate encryption control on production Linux systems.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [11] [Identifying Concurrency Bug Reports via Linguistic Patterns](https://arxiv.org/abs/2601.16338)
*Shuai Shao,Lu Xiao,Tingting Yu*

Main category: cs.SE

TL;DR: 提出基于语言模式的框架，自动识别并发bug报告，通过四种互补方法评估，发现使用语言模式增强的预训练语言模型微调效果最佳。


<details>
  <summary>Details</summary>
Motivation: 随着多核架构普及，并发系统问题日益复杂，但识别并发相关bug报告仍依赖人工标注，工作量大且易出错，需要自动化解决方案。

Method: 从730个手动标注的并发bug报告中提取58种语言模式，分为四个层次：单词级、短语级、句子级和报告级。评估四种方法：匹配法、学习法、提示法和微调法，涵盖传统机器学习、大语言模型和预训练语言模型。

Result: 在12个大型开源项目（10,920个GitHub和Jira问题报告）上评估，使用语言模式增强输入的PLM微调方法表现最佳：GitHub上精度91%，Jira上93%，在截止后数据上仍保持91%精度。

Conclusion: 该工作提供了并发bug的语言模式分类法、将领域特定语言知识融入PLM的微调策略，以及标注数据集，为改进并发bug分类的自动化、精度和可解释性奠定基础。

Abstract: With the growing ubiquity of multi-core architectures, concurrent systems have become essential but increasingly prone to complex issues such as data races and deadlocks. While modern issue-tracking systems facilitate the reporting of such problems, labeling concurrency-related bug reports remains a labor-intensive and error-prone task. This paper presents a linguistic-pattern-based framework for automatically identifying concurrency bug reports. We derive 58 distinct linguistic patterns from 730 manually labeled concurrency bug reports, organized across four levels: word-level (keywords), phrase-level (n-grams), sentence-level (semantic), and bug report-level (contextual). To assess their effectiveness, we evaluate four complementary approaches-matching, learning, prompt-based, and fine-tuning-spanning traditional machine learning, large language models (LLMs), and pre-trained language models (PLMs). Our comprehensive evaluation on 12 large-scale open-source projects (10,920 issue reports from GitHub and Jira) demonstrates that fine-tuning PLMs with linguistic-pattern-enriched inputs achieves the best performance, reaching a precision of 91% on GitHub and 93% on Jira, and maintaining strong precision on post cut-off data (91%). The contributions of this work include: (1) a comprehensive taxonomy of linguistic patterns for concurrency bugs, (2) a novel fine-tuning strategy that integrates domain-specific linguistic knowledge into PLMs, and (3) a curated, labeled dataset to support reproducible research. Together, these advances provide a foundation for improving the automation, precision, and interpretability of concurrency bug classification.

</details>


### [12] [SE Research is a Complex Ecosystem: Isolated Fixes Keep Failing -- and Systems Thinking Shows Why](https://arxiv.org/abs/2601.16363)
*Mary Shaw,Mary Lou Maher,Keith Webster*

Main category: cs.SE

TL;DR: 论文分析了软件工程研究社区面临的结构性问题，包括评审过程过载、指标驱动激励、扭曲的发表实践以及AI、规模和欺诈带来的压力，提出了需要系统性改革而非孤立解决方案的观点。


<details>
  <summary>Details</summary>
Motivation: 软件工程研究社区虽然高产，但面临一系列相互关联的挑战：评审过程不堪重负、指标驱动的激励机制、扭曲的发表实践，以及来自AI、规模和欺诈的日益增长的压力。这些问题通常被孤立对待，但实际上源于研究生态系统内部深层的结构动态，分散了我们对研究在社会中更大作用的关注。

Method: 采用复杂系统、生态系统和变革理论的视角，构建了一个整体系统层面的分析框架。通过这个视角重新审视软件工程的挑战，揭示维持当前功能障碍的非线性反馈循环，并识别改革的关键杠杆点。

Result: 研究发现软件工程研究社区的问题不是孤立的，而是由系统内部动态相互作用形成的。通过生态系统视角分析，识别出需要跨生态系统协调实施的改革措施，而不是孤立的修复方案。

Conclusion: 软件工程研究社区面临的根本挑战需要系统性的改革方法。有意义的进展需要采用整体系统视角，探索跨生态系统协调实施的解决方案组合，而不是寻求孤立的修复措施。

Abstract: The software engineering research community is productive, yet it faces a constellation of challenges: swamped review processes, metric-driven incentives, distorted publication practices, and increasing pressures from AI, scale, and outright scams. These issues are often treated in isolation, yet they arise from deep structural dynamics within the research ecosystem itself and distract us from the larger role of research in society. Meaningful progress requires a holistic system-level view. We sketch such a framework drawing on ideas from complex systems, ecosystems, and theory of change. Reframing SE's challenges through this lens reveals non-linear feedback loops that sustain current dysfunctions, and it helps to identify leverage points for reform. These are less a matter of isolated fixes and more a matter of exploring coordinated sets of fixes that operate across the SE ecosystem

</details>


### [13] [RubberDuckBench: A Benchmark for AI Coding Assistants](https://arxiv.org/abs/2601.16456)
*Ferida Mohammad,Fatma Ayad,Petros Maniatis,Satish Chandra,Elizabeth Dinella*

Main category: cs.SE

TL;DR: RubberDuckBench是一个多语言代码问题基准测试，用于评估AI编程助手。评估了20个LLM，发现即使是顶级模型也无法给出一致正确的回答，且存在大量幻觉问题。


<details>
  <summary>Details</summary>
Motivation: 随着程序员越来越多地使用AI编程助手回答代码问题，需要可靠的基准测试来评估这些系统的性能。现有基准测试不足以全面评估AI编程助手在真实场景中的表现。

Method: 从GitHub拉取请求评论中收集真实世界的上下文化问题，构建多语言基准测试RubberDuckBench，并制定详细的评估标准。评估了20个LLM（包括专有和开源模型），分析其回答的正确性、一致性和幻觉问题。

Result: Grok 4 (69.29%)、Claude Opus 4 (68.5%)和GPT-5 (67.8%)表现最佳，但与前9个模型没有显著差异。大多数模型通过部分得分获得分数，最佳模型在所有试验中最多只完全正确回答2个问题。平均58.3%的回答存在幻觉问题。成本分析显示性能与费用（API定价或参数数量）无相关性。

Conclusion: 即使是当前最先进的AI编程助手在回答真实世界的代码问题时也存在显著局限性，无法提供一致正确的回答且经常产生幻觉。该基准测试为未来可信赖和正确的AI编程助手研究提供了目标。

Abstract: Programmers are turning to AI coding assistants to answer questions about their code. Benchmarks are needed to soundly evaluate these systems and understand their performance. To enable such a study, we curate a benchmark of real-world contextualized questions derived from Github pull request comments. Out of this work, we present RubberDuckBench: a multilingual benchmark of questions about code, along with detailed rubrics for evaluating answers. We evaluate a diverse set of 20 LLMs (proprietary & open-source) on answering these questions. We find that even state of the art models fail to give consistent, correct responses across the benchmark. Grok 4 (69.29%), Claude Opus 4 (68.5%), and GPT-5 (67.8%) perform best overall, but do not exhibit pairwise significant superiority over the next 9 best performing models. Most models obtain points through partial credit, with the best performing models only answering at most 2 questions completely correctly across all trials. Furthermore, models often hallucinate with lies in 58.3\% of responses on average. Cost analysis reveals no correlation between expense (API pricing or parameter count) and performance. We intend this benchmark to be a target for future research in trustworthy and correct AI coding assistants.

</details>


### [14] [Bridging Expert Reasoning and LLM Detection: A Knowledge-Driven Framework for Malicious Packages](https://arxiv.org/abs/2601.16458)
*Wenbo Guo,Shiwen Song,Jiaxun Guo,Zhengzi Xu,Chengwei Liu,Haoran Ou,Mengmeng Ge,Yang Liu*

Main category: cs.SE

TL;DR: IntelGuard是一个基于检索增强生成(RAG)的恶意软件包检测框架，通过构建威胁情报知识库并集成专家分析推理，实现了高准确率、低误报率的恶意包检测。


<details>
  <summary>Details</summary>
Motivation: 当前开源生态系统（如NPM和PyPI）面临日益严重的供应链攻击威胁，现有检测方法要么依赖脆弱的手工规则，要么使用无法捕捉不断演变的攻击语义的数据驱动特征，需要更智能、更准确的检测方案。

Method: IntelGuard采用检索增强生成(RAG)框架，从8000多份威胁情报报告中构建结构化知识库，将恶意代码片段与行为描述和专家推理关联起来。分析新包时，检索语义相似的恶意示例，并应用LLM引导的推理来评估代码行为是否符合预期功能。

Result: 在4027个真实软件包的实验中，IntelGuard达到99%的准确率和0.50%的误报率，在混淆代码上保持96.5%的准确率。在PyPI.org上部署时，发现了54个先前未报告的恶意软件包。

Conclusion: IntelGuard通过将专家分析推理集成到自动化恶意包检测中，提供了解释性强、鲁棒性好的检测方案，能够有效应对不断演变的供应链攻击威胁。

Abstract: Open-source ecosystems such as NPM and PyPI are increasingly targeted by supply chain attacks, yet existing detection methods either depend on fragile handcrafted rules or data-driven features that fail to capture evolving attack semantics. We present IntelGuard, a retrieval-augmented generation (RAG) based framework that integrates expert analytical reasoning into automated malicious package detection. IntelGuard constructs a structured knowledge base from over 8,000 threat intelligence reports, linking malicious code snippets with behavioral descriptions and expert reasoning. When analyzing new packages, it retrieves semantically similar malicious examples and applies LLM-guided reasoning to assess whether code behaviors align with intended functionality. Experiments on 4,027 real-world packages show that IntelGuard achieves 99% accuracy and a 0.50% false positive rate, while maintaining 96.5% accuracy on obfuscated code. Deployed on PyPI.org, it discovered 54 previously unreported malicious packages, demonstrating interpretable and robust detection guided by expert knowledge.

</details>


### [15] [EvoConfig: Self-Evolving Multi-Agent Systems for Efficient Autonomous Environment Configuration](https://arxiv.org/abs/2601.16489)
*Xinshuai Guo,Jiayi Kuang,Linyue Pan,Yinghui Li,Yangning Li,Hai-Tao Zheng,Ying Shen,Di Yin,Xing Sun*

Main category: cs.SE

TL;DR: EvoConfig是一个高效的环境配置框架，通过多智能体协作和细粒度执行后分析，优化运行时环境构建，在复杂任务上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 可靠的可执行环境是确保大语言模型解决软件工程任务的基础。由于构建过程复杂繁琐，大规模配置效率相对较低。现有方法往往忽视对智能体执行动作的细粒度分析，难以处理复杂错误，导致配置失败。

Method: 提出EvoConfig框架，通过优化多智能体协作来构建正确的运行时环境。该框架包含专家诊断模块进行细粒度执行后分析，以及自进化机制让专家智能体自我反馈并实时动态调整错误修复优先级。

Result: 在Repo2Run的420个仓库上，EvoConfig与之前最先进的Repo2Run方法表现相当；在更具挑战性的Envbench上，EvoConfig达到78.1%的成功率，比Repo2Run高出7.1%。在错误识别准确性和修复建议有效性方面也优于现有方法。

Conclusion: EvoConfig通过细粒度分析和自进化机制，显著提升了环境配置的成功率和调试能力，特别是在复杂场景下表现优异，为解决软件工程任务的环境配置问题提供了有效方案。

Abstract: A reliable executable environment is the foundation for ensuring that large language models solve software engineering tasks. Due to the complex and tedious construction process, large-scale configuration is relatively inefficient. However, most methods always overlook fine-grained analysis of the actions performed by the agent, making it difficult to handle complex errors and resulting in configuration failures. To address this bottleneck, we propose EvoConfig, an efficient environment configuration framework that optimizes multi-agent collaboration to build correct runtime environments. EvoConfig features an expert diagnosis module for fine-grained post-execution analysis, and a self-evolving mechanism that lets expert agents self-feedback and dynamically adjust error-fixing priorities in real time. Empirically, EvoConfig matches the previous state-of-the-art Repo2Run on Repo2Run's 420 repositories, while delivering clear gains on harder cases: on the more challenging Envbench, EvoConfig achieves a 78.1% success rate, outperforming Repo2Run by 7.1%. Beyond end-to-end success, EvoConfig also demonstrates stronger debugging competence, achieving higher accuracy in error identification and producing more effective repair recommendations than existing methods.

</details>


### [16] [REprompt: Prompt Generation for Intelligent Software Development Guided by Requirements Engineering](https://arxiv.org/abs/2601.16507)
*Junjie Shi,Weisong Sun,Zhenpeng Chen,Zhujun Wu,Xiaohong Chen,Zhi Jin,Yang Liu*

Main category: cs.SE

TL;DR: REprompt是一个基于需求工程指导的多智能体提示优化框架，旨在解决在智能软件开发中手动设计有效提示的挑战，通过将需求工程原则融入提示生成过程来优化系统提示和用户提示。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型在智能软件开发中的广泛应用，提示设计变得至关重要但具有挑战性。现有自动化提示工程方法大多忽视了需求工程的方法论原则，无法生成符合正式需求规范的提示，限制了在实际软件开发场景中的应用效果。

Method: 提出REprompt框架，这是一个基于需求工程指导的多智能体提示优化框架。该框架将需求工程原则融入提示生成过程，通过多智能体协作来优化系统提示和用户提示，确保生成的提示符合正式需求规范。

Result: 实验结果表明，REprompt能够有效优化系统提示和用户提示，通过将需求工程原则作为基础来指导提示生成，提高了提示的质量和适用性。

Conclusion: REprompt框架成功地将需求工程原则与提示优化相结合，为解决智能软件开发中提示设计的挑战提供了一种有效方法，能够生成更符合正式需求规范的提示，提升智能软件开发的效率和质量。

Abstract: The rapid development of large language models is transforming software development. Beyond serving as code auto-completion tools in integrated development environments, large language models increasingly function as foundation models within coding agents in vibe-coding scenarios. In such settings, prompts play a central role in agent-based intelligent software development, as they not only guide the behavior of large language models but also serve as carriers of user requirements. Under the dominant conversational paradigm, prompts are typically divided into system prompts and user prompts. System prompts provide high-level instructions to steer model behavior and establish conversational context, while user prompts represent inputs and requirements provided by human users. Despite their importance, designing effective prompts remains challenging, as it requires expertise in both prompt engineering and software engineering, particularly requirements engineering. To reduce the burden of manual prompt construction, numerous automated prompt engineering methods have been proposed. However, most existing approaches neglect the methodological principles of requirements engineering, limiting their ability to generate artifacts that conform to formal requirement specifications in realistic software development scenarios. To address this gap, we propose REprompt, a multi-agent prompt optimization framework guided by requirements engineering. Experiment results demonstrate that REprompt effectively optimizes both system and user prompts by grounding prompt generation in requirements engineering principles.

</details>


### [17] [Revisiting the Role of Natural Language Code Comments in Code Translation](https://arxiv.org/abs/2601.16661)
*Monika Gupta,Ajay Meena,Anamitra Roy Choudhury,Vijay Arya,Srikanta Bedathur*

Main category: cs.SE

TL;DR: 该论文通过大规模实证研究发现，描述代码整体目的的自然语言注释能显著提升大语言模型在代码翻译任务中的准确性，并提出了基于此的COMMENTRA方法，可将性能提升高达两倍。


<details>
  <summary>Details</summary>
Motivation: 现有代码翻译基准大多缺乏代码注释，而大语言模型通常在包含丰富注释的代码库上预训练，因此需要系统研究注释对代码翻译质量的影响。

Method: 进行了大规模实证研究，包含超过80,000次翻译实验，涉及1,100多个代码样本，覆盖C、C++、Go、Java、Python五种编程语言之间的双向翻译，对比了有注释和无注释条件下的翻译性能。

Result: 研究发现代码注释（特别是描述代码整体目的的注释）能显著提升翻译准确性。基于此提出了COMMENTRA方法，实验表明该方法可将基于大语言模型的代码翻译性能提升高达两倍。

Conclusion: 这是首个在全面性、规模和语言覆盖度上研究如何利用代码注释提升代码翻译准确性的工作，证明了注释在代码翻译中的重要作用，并为未来研究提供了新的方向。

Abstract: The advent of large language models (LLMs) has ushered in a new era in automated code translation across programming languages. Since most code-specific LLMs are pretrained on well-commented code from large repositories like GitHub, it is reasonable to hypothesize that natural language code comments could aid in improving translation quality. Despite their potential relevance, comments are largely absent from existing code translation benchmarks, rendering their impact on translation quality inadequately characterised. In this paper, we present a large-scale empirical study evaluating the impact of comments on translation performance. Our analysis involves more than $80,000$ translations, with and without comments, of $1100+$ code samples from two distinct benchmarks covering pairwise translations between five different programming languages: C, C++, Go, Java, and Python. Our results provide strong evidence that code comments, particularly those that describe the overall purpose of the code rather than line-by-line functionality, significantly enhance translation accuracy. Based on these findings, we propose COMMENTRA, a code translation approach, and demonstrate that it can potentially double the performance of LLM-based code translation. To the best of our knowledge, our study is the first in terms of its comprehensiveness, scale, and language coverage on how to improve code translation accuracy using code comments.

</details>


### [18] [The Green Side of the Lua](https://arxiv.org/abs/2601.16670)
*André Brandão,Diogo Matos,Miguel Guimarães,Simão Cunha,João Saraiva*

Main category: cs.SE

TL;DR: 该研究通过实证分析25个Lua官方解释器版本和JIT编译器，发现LuaJIT编译器在性能和能耗方面显著优于标准解释器，最有效的LuaJIT比最佳解释器节能约7倍、速度快7倍，且接近C语言的效率水平。


<details>
  <summary>Details</summary>
Motivation: 联合国2030年可持续发展议程强调节能软件对减少全球碳足迹的重要性。编程语言和执行模型对软件能耗有重要影响，解释型语言通常比编译型语言效率低。Lua作为流行语言，其能效低于C等更环保、更快的语言，因此需要研究其运行时性能和能效改进方案。

Method: 对25个官方Lua解释器版本和即时编译(JIT)编译器进行实证研究，使用全面的基准测试套件测量执行时间和能耗，分析Lua的演进历程、JIT编译的影响，并与其他语言进行比较。

Result: 所有LuaJIT编译器都显著优于标准Lua解释器。最有效的LuaJIT比最佳Lua解释器节能约7倍、运行速度快7倍。LuaJIT接近C语言的效率水平，能耗约为C的6倍，运行速度约为C的8倍，表明JIT编译在提升解释型语言性能和能效方面具有显著优势。

Conclusion: JIT编译技术能显著提升解释型语言的性能和能源效率，使Lua等语言接近编译型语言的效率水平，为实现可持续发展目标中的软件能效改进提供了重要技术路径。

Abstract: The United Nations' 2030 Agenda for Sustainable Development highlights the importance of energy-efficient software to reduce the global carbon footprint. Programming languages and execution models strongly influence software energy consumption, with interpreted languages generally being less efficient than compiled ones. Lua illustrates this trade-off: despite its popularity, it is less energy-efficient than greener and faster languages such as C.
  This paper presents an empirical study of Lua's runtime performance and energy efficiency across 25 official interpreter versions and just-in-time (JIT) compilers. Using a comprehensive benchmark suite, we measure execution time and energy consumption to analyze Lua's evolution, the impact of JIT compilation, and comparisons with other languages. Results show that all LuaJIT compilers significantly outperform standard Lua interpreters. The most efficient LuaJIT consumes about seven times less energy and runs seven times faster than the best Lua interpreter. Moreover, LuaJIT approaches C's efficiency, using roughly six times more energy and running about eight times slower, demonstrating the substantial benefits of JIT compilation for improving both performance and energy efficiency in interpreted languages.

</details>


### [19] [Supporting Stakeholder Requirements Expression with LLM Revisions: An Empirical Evaluation](https://arxiv.org/abs/2601.16699)
*Michael Mircea,Emre Gevrek,Elisa Schmid,Kurt Schneider*

Main category: cs.SE

TL;DR: LLMs能有效辅助需求表达，特别是帮助领域知识有限的利益相关者更清晰、完整地表达需求意图


<details>
  <summary>Details</summary>
Motivation: 利益相关者因领域知识有限或认知限制而难以准确表达需求，导致表达需求与真实意图不一致，传统需求获取方法耗时且容易扭曲原始意图

Method: 通过26名参与者生成130个需求陈述的研究，参与者先独立表达需求，然后评估LLM根据其上下文生成的修订版本，从意图对齐、可读性、推理性和明确性四个维度进行评分

Result: 参与者对LLM修订版本的评分在所有维度上均显著高于原始陈述，定性反馈显示LLM修订能揭示利益相关者认为重要但未明确表达的细节，帮助他们更好地理解自己的需求

Conclusion: LLM辅助的需求重构能提高需求的感知完整性、清晰度和对齐度，通过将利益相关者保持在验证循环中，这种方法促进了AI在需求工程中的负责任和可信赖使用

Abstract: Stakeholders often struggle to accurately express their requirements due to articulation barriers arising from limited domain knowledge or from cognitive constraints. This can cause misalignment between expressed and intended requirements, complicating elicitation and validation. Traditional elicitation techniques, such as interviews and follow-up sessions, are time-consuming and risk distorting stakeholders' original intent across iterations. Large Language Models (LLMs) can infer user intentions from context, suggesting potential for assisting stakeholders in expressing their needs. This raises the questions of (i) how effectively LLMs can support requirement expression and (ii) whether such support benefits stakeholders with limited domain expertise. We conducted a study with 26 participants who produced 130 requirement statements. Each participant first expressed requirements unaided, then evaluated LLM-generated revisions tailored to their context. Participants rated LLM revisions significantly higher than their original statements across all dimensions-alignment with intent, readability, reasoning, and unambiguity. Qualitative feedback further showed that LLM revisions often surfaced tacit details stakeholders considered important and helped them better understand their own requirements. We present and evaluate a stakeholder-centered approach that leverages LLMs as articulation aids in requirements elicitation and validation. Our results show that LLM-assisted reformulation improves perceived completeness, clarity, and alignment of requirements. By keeping stakeholders in the validation loop, this approach promotes responsible and trustworthy use of AI in Requirements Engineering.

</details>


### [20] [Adoption of Generative Artificial Intelligence in the German Software Engineering Industry: An Empirical Study](https://arxiv.org/abs/2601.16700)
*Ludwig Felder,Tobias Eisenreich,Mahsa Fischer,Stefan Wagner,Chunyang Chen*

Main category: cs.SE

TL;DR: 研究德国软件工程师采用生成式AI工具的现状，发现经验水平调节感知收益，组织规模影响工具选择和使用强度，项目上下文理解不足是主要障碍。


<details>
  <summary>Details</summary>
Motivation: 生成式AI工具在软件开发中快速普及，但影响其有效使用的因素（如交互深度、组织约束、经验考量）尚未充分研究。在德国这样有严格监管要求的环境中，这一问题尤其重要，但缺乏针对德国背景的实证研究。

Method: 采用混合方法研究：首先进行18次探索性访谈，然后对109名参与者进行开发者调查。分析工具采用模式、提示策略和组织因素对有效性的影响。

Result: 经验水平调节生成式AI工具的感知收益，生产力提升在开发者中分布不均；组织规模影响工具选择和使用强度；项目上下文理解有限被确定为最主要的障碍。

Conclusion: 研究揭示了德国背景下生成式AI工具采用的动态特征，为开发者、组织和工具供应商提供了可操作的启示，以推进AI辅助软件开发。

Abstract: Generative artificial intelligence (GenAI) tools have seen rapid adoption among software developers. While adoption rates in the industry are rising, the underlying factors influencing the effective use of these tools, including the depth of interaction, organizational constraints, and experience-related considerations, have not been thoroughly investigated. This issue is particularly relevant in environments with stringent regulatory requirements, such as Germany, where practitioners must address the GDPR and the EU AI Act while balancing productivity gains with intellectual property considerations. Despite the significant impact of GenAI on software engineering, to the best of our knowledge, no empirical study has systematically examined the adoption dynamics of GenAI tools within the German context. To address this gap, we present a comprehensive mixed-methods study on GenAI adoption among German software engineers. Specifically, we conducted 18 exploratory interviews with practitioners, followed by a developer survey with 109 participants. We analyze patterns of tool adoption, prompting strategies, and organizational factors that influence effectiveness. Our results indicate that experience level moderates the perceived benefits of GenAI tools, and productivity gains are not evenly distributed among developers. Further, organizational size affects both tool selection and the intensity of tool use. Limited awareness of the project context is identified as the most significant barrier. We summarize a set of actionable implications for developers, organizations, and tool vendors seeking to advance artificial intelligence (AI) assisted software development.

</details>


### [21] [Developer Perspectives on REST API Usability: A Study of REST API Guidelines](https://arxiv.org/abs/2601.16705)
*Sven Peldszus,Jan Rutenkolk,Marcel Heide,Jan Sollmann,Benjamin Klatt,Frank Köhne,Thorsten Berger*

Main category: cs.SE

TL;DR: 本文通过访谈16位REST API行业专家，探讨了API可用性、指南有效性因素、采用和设计指南的挑战以及最佳实践，发现遵循约定是影响REST API可用性的最重要因素。


<details>
  <summary>Details</summary>
Motivation: REST API已成为核心业务资产，但开发者仍难以设计有效的REST API。现有指南（如Zalando和Microsoft的指南）被认为过于庞大且不适用，许多公司创建自己的指南也很困难。需要改进对采用、使用和创建REST API指南的实证理解。

Method: 采用访谈研究方法，对16位来自工业界的REST API专家进行访谈研究，确定API可用性的概念、指南有效性因素、采用和设计指南的挑战以及最佳实践。

Result: 识别出影响REST API可用性的八个因素，其中遵循约定是最重要的因素。指南确实可以提高API可用性，但开发者对严格指南存在显著抵触。指南规模和与组织需求的匹配是两个重要考虑因素。REST指南需要随着组织发展而发展，所有利益相关者都需要参与其开发和维护。自动化linting不仅可以将合规性嵌入流程，还可以通过教育性解释来证明指南规则的合理性。

Conclusion: REST API指南的有效性取决于多个因素，包括开发者接受度、指南规模和组织适配性。成功的指南需要动态发展、全员参与，并通过自动化工具支持合规性和教育目的。

Abstract: REST is today's most widely used architectural style for providing web-based services. In the age of service-orientation (a.k.a. Software as a Service (SaaS)) APIs have become core business assets and can easily expose hundreds of operations. While well-designed APIs contribute to the commercial success of a service, poorly designed APIs can threaten entire organizations. Recognizing their relevance and value, many guidelines have been proposed for designing usable APIs, similar to design patterns and coding standards. For example, Zalando and Microsoft provide popular REST API guidelines. However, they are often considered as too large and inapplicable, so many companies create and maintain their own guidelines, which is a challenge in itself. In practice, however, developers still struggle to design effective REST APIs. To improve the situation, we need to improve our empirical understanding of adopting, using, and creating REST API guidelines.
  We present an interview study with 16 REST API experts from industry. We determine the notion of API usability, guideline effectiveness factors, challenges of adopting and designing guidelines, and best practices. We identified eight factors influencing REST API usability, among which the adherence to conventions is the most important one. While guidelines can in fact be an effective means to improve API usability, there is significant resistance from developers against strict guidelines. Guideline size and how it fits with organizational needs are two important factors to consider. REST guidelines also have to grow with the organization, while all stakeholders need to be involved in their development and maintenance. Automated linting provides an opportunity to not only embed compliance enforcement into processes, but also to justify guideline rules with educational explanations.

</details>


### [22] [Variability-Aware Detection and Repair of Compilation Errors Using Foundation Models in Configurable Systems](https://arxiv.org/abs/2601.16755)
*Rohit Gheyi,Lucas Albuquerque,Márcio Ribeiro,Eduardo Almeida,Danyllo Albuquerque,Mirko Perkusich*

Main category: cs.SE

TL;DR: 该研究评估了基础模型（GPT-OSS-20B和GEMINI 3 PRO）在检测和修复可配置C系统中由特性可变性引起的编译错误方面的效果，并与传统工具TYPECHEF进行比较。


<details>
  <summary>Details</summary>
Motivation: 可配置软件系统中，特定特性组合可能引发编译错误，这些错误在开发和测试过程中难以被发现。传统编译器一次只能分析单一配置，而现有的可变性感知工具需要复杂设置且分析成本高。

Method: 使用基础模型（GPT-OSS-20B和GEMINI 3 PRO）检测和修复由特性可变性引起的编译错误。评估包括：1）5,000个小型可配置系统，系统性地测试可变性引起的编译行为；2）14个真实GitHub提交；3）42个变异测试场景。与TYPECHEF（最先进的可变性感知解析器）进行比较。

Result: 基础模型能有效识别可变性引起的编译错误。在小型可配置系统上，GPT-OSS-20B达到精度0.97、召回率0.90、准确率0.94，检测覆盖率显著高于TYPECHEF。在编译错误修复方面，GPT-OSS-20B在超过70%的情况下生成可编译的修复。在真实提交分析中，CHATGPT-5.2检测出除两个案例外的所有注入故障，并在一个超过1,000行修改的Linux提交中发现潜在的真实编译错误。

Conclusion: 当前最先进的基础模型为传统可变性感知分析提供了实用且低成本的补充方案，能够有效检测和修复可配置系统中的编译错误。

Abstract: Modern software systems often rely on conditional compilation to support optional features and multiple deployment scenarios. In configurable systems, compilation errors may arise only under specific combinations of features, remaining hidden during development and testing. Such variability-induced errors are difficult to detect in practice, as traditional compilers analyze only a single configuration at a time, while existing variability-aware tools typically require complex setup and incur high analysis costs. In this article, we present an empirical study on the use of foundation models to detect and fix compilation errors caused by feature variability in configurable C systems. We evaluate GPT-OSS-20B and GEMINI 3 PRO, and compare them with TYPECHEF, a state-of-the-art variability-aware parser. Our evaluation considers two complementary settings: 5,000 small configurable systems designed to systematically exercise variability-induced compilation behavior, comprising both systems with and without compilation errors, and 14 real-world GitHub commits, as well as an additional set of mutation testing scenarios (42). Our results show that foundation models can effectively identify variability-induced compilation errors. On small configurable systems, GPT-OSS-20B achieved a precision of 0.97, recall of 0.90, and accuracy of 0.94, substantially increasing detection coverage compared to TYPECHEF, and exhibiting performance comparable to GEMINI 3. For compilation error repair, GPT-OSS-20B produced compilable fixes in over 70% of the cases. In the analysis of real commits, CHATGPT-5.2 detected all injected faults except for two cases and identified a potential real compilation bug in a Linux commit with more than 1,000 modified lines. Our findings indicate that current state-of-the-art foundation models provide a practical and low-effort complement to traditional variability-aware analyses.

</details>


### [23] [Assessing the Feasibility of Selective Instrumentation for Runtime Code Coverage in Large C++ Game Engines](https://arxiv.org/abs/2601.16881)
*Ian Gauk,Doriane Olewicki,Joshua Romoff,Cor-Paul Bezemer*

Main category: cs.SE

TL;DR: 针对大型C++游戏引擎的选择性插桩方法，在保持覆盖率数据相关性的同时显著降低性能开销，实现稳定的自动化测试


<details>
  <summary>Details</summary>
Motivation: 在AAA游戏中，代码覆盖率检测的性能开销与严格的性能要求相冲突，且可能破坏自动化测试的稳定性

Method: 提出针对大型C++游戏引擎的选择性插桩方法，将插桩范围缩小到与开发者提交相关的代码，集成到工业级游戏测试流程中

Result: 编译开销极小（2000次提交后构建时间才翻倍），最差情况下帧率仍保持非插桩基线的50%以上，在两个生产测试套件中未导致任何自动化测试失败

Conclusion: 通过选择性插桩方法，可以在大型C++游戏引擎中实现提交级或构建级覆盖率检测，同时保持最小开销且不损害测试稳定性

Abstract: Code coverage is a valuable guide for testing, but in AAA games the overhead of instrumentation conflicts with strict performance requirements and can destabilize automated tests. We propose and assess a selective instrumentation approach tailored to large game engines written in \texttt{C++}, which reduces the scope of instrumentation while preserving relevant coverage data to developer commits. Our framework integrates into an industrial game testing pipeline, enabling developers to receive immediate coverage feedback on tests run against their changes. The compilation overhead of our approach is minimal, allowing instrumentation of over 2,000 commits before doubling build time. In performance evaluations, even the worst-case scenario maintains frame rates above 50\% of the non-instrumented baseline. Across two production test suites maintained by our industry partner, our framework caused no automated test failures, avoiding the instability observed under full instrumentation. Our work shows that commit-level or build-level coverage of large \texttt{C++} game engines can be achieved with minimal overhead and without compromising test stability.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [24] [Doc2AHP: Inferring Structured Multi-Criteria Decision Models via Semantic Trees with LLMs](https://arxiv.org/abs/2601.16479)
*Hongjia Wu,Shuai Zhou,Hongxin Zhang,Wei Chen*

Main category: cs.AI

TL;DR: Doc2AHP：一种基于AHP原则的结构化推理框架，利用LLMs的泛化能力与决策理论的严谨性相结合，无需标注数据即可从文档构建高质量决策模型


<details>
  <summary>Details</summary>
Motivation: LLMs在语义理解方面表现出色，但在需要严格逻辑的复杂决策任务中难以保证结构一致性和推理可靠性。传统决策理论（如AHP）虽提供系统理性框架，但构建依赖大量领域专家知识，存在"专家瓶颈"问题，限制了在一般场景中的可扩展性

Method: 提出Doc2AHP框架：1）利用AHP的结构原则作为约束，指导LLM在非结构化文档空间中进行受限搜索，强制父子节点间的逻辑蕴含关系；2）引入多智能体加权机制与自适应一致性优化策略，确保权重分配的数字一致性

Result: 实验结果表明，Doc2AHP不仅能让非专家用户从零开始构建高质量决策模型，而且在逻辑完整性和下游任务准确性方面显著优于直接生成基线方法

Conclusion: Doc2AHP成功弥合了LLMs的泛化能力与决策理论严谨性之间的差距，无需大量标注数据或人工干预，为结构化决策建模提供了有效解决方案

Abstract: While Large Language Models (LLMs) demonstrate remarkable proficiency in semantic understanding, they often struggle to ensure structural consistency and reasoning reliability in complex decision-making tasks that demand rigorous logic. Although classical decision theories, such as the Analytic Hierarchy Process (AHP), offer systematic rational frameworks, their construction relies heavily on labor-intensive domain expertise, creating an "expert bottleneck" that hinders scalability in general scenarios. To bridge the gap between the generalization capabilities of LLMs and the rigor of decision theory, we propose Doc2AHP, a novel structured inference framework guided by AHP principles. Eliminating the need for extensive annotated data or manual intervention, our approach leverages the structural principles of AHP as constraints to direct the LLM in a constrained search within the unstructured document space, thereby enforcing the logical entailment between parent and child nodes. Furthermore, we introduce a multi-agent weighting mechanism coupled with an adaptive consistency optimization strategy to ensure the numerical consistency of weight allocation. Empirical results demonstrate that Doc2AHP not only empowers non-expert users to construct high-quality decision models from scratch but also significantly outperforms direct generative baselines in both logical completeness and downstream task accuracy.

</details>


### [25] [SycoEval-EM: Sycophancy Evaluation of Large Language Models in Simulated Clinical Encounters for Emergency Care](https://arxiv.org/abs/2601.16529)
*Dongshen Peng,Yi Wang,Carl Preiksaitis,Christian Rose*

Main category: cs.AI

TL;DR: SycoEval-EM框架通过多智能体模拟评估LLMs在急诊医学中面对患者压力时的鲁棒性，发现模型对不当医疗请求的顺从率高达0-100%，且模型能力无法预测其抗压能力。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在临床决策支持中显示出潜力，但存在因患者压力而同意不当医疗的风险。当前静态基准测试无法充分评估模型在社交压力下的安全性，需要更真实的对抗性测试。

Method: 引入SycoEval-EM多智能体模拟框架，通过对抗性患者说服场景评估LLM鲁棒性。涵盖20个LLM模型和1,875次医疗接触，涉及三个Choosing Wisely场景（明智选择指南），测试不同说服策略的效果。

Result: 模型顺从率范围0-100%，对影像检查请求的脆弱性（38.8%）高于阿片类药物处方（25.0%）。模型能力与鲁棒性相关性差，所有说服策略效果相似（30.0-36.0%），表明是普遍脆弱性而非特定策略弱点。

Conclusion: 静态基准测试无法充分预测临床AI在社交压力下的安全性，需要进行多轮对抗性测试作为临床AI认证的必要组成部分。

Abstract: Large language models (LLMs) show promise in clinical decision support yet risk acquiescing to patient pressure for inappropriate care. We introduce SycoEval-EM, a multi-agent simulation framework evaluating LLM robustness through adversarial patient persuasion in emergency medicine. Across 20 LLMs and 1,875 encounters spanning three Choosing Wisely scenarios, acquiescence rates ranged from 0-100\%. Models showed higher vulnerability to imaging requests (38.8\%) than opioid prescriptions (25.0\%), with model capability poorly predicting robustness. All persuasion tactics proved equally effective (30.0-36.0\%), indicating general susceptibility rather than tactic-specific weakness. Our findings demonstrate that static benchmarks inadequately predict safety under social pressure, necessitating multi-turn adversarial testing for clinical AI certification.

</details>


### [26] [LLM is Not All You Need: A Systematic Evaluation of ML vs. Foundation Models for text and image based Medical Classification](https://arxiv.org/abs/2601.16549)
*Meet Raval,Tejul Pandit,Dhvani Upadhyay*

Main category: cs.AI

TL;DR: 该研究对比了传统机器学习、基于提示的LLMs/VLMs和微调PEFT模型在医学分类任务上的表现，发现传统机器学习模型在大多数任务中表现最佳，而LoRA微调的Gemma变体表现最差，提示式LLMs/VLMs在图像任务上有竞争力。


<details>
  <summary>Details</summary>
Motivation: 研究旨在评估多模态视觉语言模型和大语言模型在医学分类任务中的实际效果，通过统一基准对比传统机器学习与当代基于transformer的技术，验证基础模型是否在所有医学分类场景中都优于传统方法。

Method: 使用四个公开可用的文本和图像数据集（涵盖二元和多分类复杂度），对比三类模型：传统机器学习（LR、LightGBM、ResNet-50）、基于提示的LLMs/VLMs（Gemini 2.5）和微调PEFT模型（LoRA适应的Gemma3变体）。所有实验使用一致的数据划分和对齐的评估指标。

Result: 传统机器学习模型在大多数医学分类任务中表现最佳，特别是在结构化文本数据集上表现优异。LoRA微调的Gemma变体在所有文本和图像实验中表现最差，未能从最小微调中泛化。基于提示的LLMs/VLMs在文本任务上表现不佳，但在多分类图像任务中表现出竞争力，与传统ResNet-50基线相当。

Conclusion: 在许多医学分类场景中，成熟的机器学习模型仍然是最可靠的选择。基础模型并非普遍优越，参数高效微调的有效性高度依赖于适应策略，本研究中的最小微调证明是有害的。

Abstract: The combination of multimodal Vision-Language Models (VLMs) and Large Language Models (LLMs) opens up new possibilities for medical classification. This work offers a rigorous, unified benchmark by using four publicly available datasets covering text and image modalities (binary and multiclass complexity) that contrasts traditional Machine Learning (ML) with contemporary transformer-based techniques. We evaluated three model classes for each task: Classical ML (LR, LightGBM, ResNet-50), Prompt-Based LLMs/VLMs (Gemini 2.5), and Fine-Tuned PEFT Models (LoRA-adapted Gemma3 variants). All experiments used consistent data splits and aligned metrics. According to our results, traditional machine learning (ML) models set a high standard by consistently achieving the best overall performance across most medical categorization tasks. This was especially true for structured text-based datasets, where the classical models performed exceptionally well. In stark contrast, the LoRA-tuned Gemma variants consistently showed the worst performance across all text and image experiments, failing to generalize from the minimal fine-tuning provided. However, the zero-shot LLM/VLM pipelines (Gemini 2.5) had mixed results; they performed poorly on text-based tasks, but demonstrated competitive performance on the multiclass image task, matching the classical ResNet-50 baseline. These results demonstrate that in many medical categorization scenarios, established machine learning models continue to be the most reliable option. The experiment suggests that foundation models are not universally superior and that the effectiveness of Parameter-Efficient Fine-Tuning (PEFT) is highly dependent on the adaptation strategy, as minimal fine-tuning proved detrimental in this study.

</details>


### [27] [LUMINA: Long-horizon Understanding for Multi-turn Interactive Agents](https://arxiv.org/abs/2601.16649)
*Amin Rakhsha,Thomas Hehn,Pietro Mazzaglia,Fabio Valerio Massoli,Arash Behboodi,Tribhuvanesh Orekondy*

Main category: cs.AI

TL;DR: 研究通过oracle反事实框架分析多轮智能体任务中不同能力（规划、状态跟踪等）的重要性，发现规划能力普遍重要，其他能力重要性取决于环境和模型特性


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在孤立任务上表现良好，但在需要规划、状态跟踪和长上下文处理等多轮长视野智能体任务上仍有困难，需要理解这些底层能力对任务成功的重要性

Method: 开发oracle反事实框架，通过提供完美执行特定任务（如完美规划、无错误状态跟踪）的oracle干预，测量智能体性能变化；引入程序生成的可调复杂度的游戏式任务套件，以隔离每个oracle的贡献

Result: 结果显示，某些干预（如规划）在各种设置中持续提升性能，而其他技能的有用性取决于环境特性和语言模型特性

Conclusion: 研究揭示了多轮智能体环境的挑战，为未来AI智能体和语言模型的发展提供了指导方向

Abstract: Large language models can perform well on many isolated tasks, yet they continue to struggle on multi-turn, long-horizon agentic problems that require skills such as planning, state tracking, and long context processing. In this work, we aim to better understand the relative importance of advancing these underlying capabilities for success on such tasks. We develop an oracle counterfactual framework for multi-turn problems that asks: how would an agent perform if it could leverage an oracle to perfectly perform a specific task? The change in the agent's performance due to this oracle assistance allows us to measure the criticality of such oracle skill in the future advancement of AI agents. We introduce a suite of procedurally generated, game-like tasks with tunable complexity. These controlled environments allow us to provide precise oracle interventions, such as perfect planning or flawless state tracking, and make it possible to isolate the contribution of each oracle without confounding effects present in real-world benchmarks. Our results show that while some interventions (e.g., planning) consistently improve performance across settings, the usefulness of other skills is dependent on the properties of the environment and language model. Our work sheds light on the challenges of multi-turn agentic environments to guide the future efforts in the development of AI agents and language models.

</details>


### [28] [AgentsEval: Clinically Faithful Evaluation of Medical Imaging Reports via Multi-Agent Reasoning](https://arxiv.org/abs/2601.16685)
*Suzhong Fu,Jingqi Dong,Xuan Ding,Rui Sun,Yiming Yang,Shuguang Cui,Zhen Li*

Main category: cs.AI

TL;DR: AgentsEval：一个多智能体流推理框架，用于评估自动生成的医学影像报告的临床正确性和推理保真度，通过模拟放射科医生的协作诊断工作流程提供结构化评估。


<details>
  <summary>Details</summary>
Motivation: 当前评估自动生成的医学影像报告的方法无法捕捉放射学解释背后的结构化诊断逻辑，导致评估不可靠且临床相关性有限，需要更透明和临床基础的评估框架。

Method: 提出AgentsEval多智能体流推理框架，将评估过程分解为可解释的步骤：标准定义、证据提取、对齐和一致性评分，模拟放射科医生的协作诊断工作流程。同时构建了一个基于扰动的多领域基准测试，涵盖五个医学报告数据集。

Result: 实验结果表明，AgentsEval提供了临床对齐、语义忠实且可解释的评估，在释义、语义和风格扰动下保持稳健，能够提供明确的推理轨迹和结构化临床反馈。

Conclusion: 该框架代表了向透明和临床基础的医学报告生成系统评估迈出的一步，促进大型语言模型在临床实践中的可信集成。

Abstract: Evaluating the clinical correctness and reasoning fidelity of automatically generated medical imaging reports remains a critical yet unresolved challenge. Existing evaluation methods often fail to capture the structured diagnostic logic that underlies radiological interpretation, resulting in unreliable judgments and limited clinical relevance. We introduce AgentsEval, a multi-agent stream reasoning framework that emulates the collaborative diagnostic workflow of radiologists. By dividing the evaluation process into interpretable steps including criteria definition, evidence extraction, alignment, and consistency scoring, AgentsEval provides explicit reasoning traces and structured clinical feedback. We also construct a multi-domain perturbation-based benchmark covering five medical report datasets with diverse imaging modalities and controlled semantic variations. Experimental results demonstrate that AgentsEval delivers clinically aligned, semantically faithful, and interpretable evaluations that remain robust under paraphrastic, semantic, and stylistic perturbations. This framework represents a step toward transparent and clinically grounded assessment of medical report generation systems, fostering trustworthy integration of large language models into clinical practice.

</details>


### [29] [LongCat-Flash-Thinking-2601 Technical Report](https://arxiv.org/abs/2601.16725)
*Meituan LongCat Team,Anchun Gui,Bei Li,Bingyang Tao,Bole Zhou,Borun Chen,Chao Zhang,Chao Zhang,Chen Gao,Chen Zhang,Chengcheng Han,Chenhui Yang,Chuyu Zhang,Cong Chen,Cunguang Wang,Daoru Pan,Defei Bu,Dengchang Zhao,Di Xiu,Dishan Liu,Dongyu Ru,Dunwei Tu,Fan Wu,Fengcheng Yuan,Fengcun Li,Gang Xu,Guanyu Wu,Guoyuan Lin,Haibin Wang,Hansi Yang,Hao Yang,Haonan Yan,Haoxiang Ma,Haoxing Wen,Hongyan Hao,Hongyin Tang,Hongyu Zang,Hongzhi Ni,Hui Su,Jiacheng Zhang,Jiahong Zhou,Jiahuan Li,Jiaming Wang,Jian Yang,Jianfei Zhang,Jianhao Xu,Jianing Wang,Jiapeng Zhu,Jiaqi Sun,Jiarong Shi,Jiarui Zhao,Jingang Wang,Jinluan Yang,Jinrui Ding,Jinwei Xiao,Jiyuan He,Juncan Xu,Kefeng Zhang,Keheng Wang,Li Wei,Lianhui Ma,Lin Qiu,Lingbing Kong,Lingchuan Liu,Linsen Guo,Mengshen Zhu,Mengxia Shen,Mingyang Zhu,Peiguang Li,Peng Pei,Pengcheng Jia,Pengtao Zhang,Peng Zhao,Qi Gu,Qiong Huang,Qiyuan Duan,Quanchi Weng,Rongxiang Weng,Rongzhi Zhang,Rumei Li,Shanglin Lei,Shengnan An,Shijun Dai,Shuaikang Liu,Shuang Zhou,Shuo Wang,Songyuan Zhao,Tao Liang,Tianhao Hu,Tianze Chen,Wei Liu,Wei Shi,Wei Wang,Weifeng Tang,Wenjie Shi,Wenlong Zhu,Wentao Chen,Wentao Shi,Xi Su,Xiangcheng Liu,Xiandi Ma,Xiangyu Xi,Xiangyuan Liu,Xiangzhou Huang,Xiao Liu,Xiaodong Cai,Xiaolong Chen,Xiaowei Shi,Xiaoyu Li,Xin Chen,Xingchen Liu,Xuan Huang,Xuezhi Cao,Xunliang Cai,Yan Chen,Yang Bai,Yang Liu,Yang Yang,Yang Zheng,Yaoming Wang,Yaoming Zhu,Yaqi Huo,Yanyu Chen,Yaorui Shi,Yerui Sun,Yi Zhang,Yihao Chen,Yi-Kai Zhang,Yifan Lu,Yifan Zhao,Yitao Zhai,Yongjing Yin,Yongwei Zhou,Youshao Xiao,Yuchuan Dai,Yuchen Xie,Yuchen Yu,Yufei Zhang,Yuhuai Wei,Yulei Qian,Yunfan Liang,Yunke Zhao,Yuwei Jiang,Yuxin Bian,Yuxin Chen,Yuxin Liu,Yue Xu,Yueqing Sun,Zeyang Yu,Zhao Yang,Zhengsheng Huang,Zhengyu Chen,Zhijian Liu,Zhikang Xia,Zhimin Lin,Zhiyuan Yao,Zhuofan Chen,Zhuowen Han,Zijian Zhang,Ziran Li,Ziwen Wang,Ziyuan Zhuang*

Main category: cs.AI

TL;DR: LongCat-Flash-Thinking-2601是一个5600亿参数的开源MoE推理模型，在多种智能体基准测试中达到SOTA性能，具有强大的泛化能力和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 开发一个在复杂工具使用、多轮智能体交互和真实世界噪声环境中表现优异的开源推理模型，解决现有模型在真实应用场景中的泛化能力和鲁棒性不足的问题。

Method: 采用统一的训练框架，结合领域并行专家训练与后续融合；扩展异步强化学习框架DORA支持大规模多环境训练；系统分析真实世界噪声模式并设计针对性训练；引入Heavy Thinking模式进行测试时扩展。

Result: 在智能体搜索、工具使用和工具集成推理等基准测试中达到开源模型的最先进性能；在复杂工具交互和噪声真实世界环境中表现出强大的泛化能力和鲁棒性。

Conclusion: LongCat-Flash-Thinking-2601通过创新的训练框架、大规模环境训练、噪声鲁棒性设计和测试时扩展技术，实现了在复杂推理任务上的卓越性能，为真实世界应用提供了强大的开源解决方案。

Abstract: We introduce LongCat-Flash-Thinking-2601, a 560-billion-parameter open-source Mixture-of-Experts (MoE) reasoning model with superior agentic reasoning capability. LongCat-Flash-Thinking-2601 achieves state-of-the-art performance among open-source models on a wide range of agentic benchmarks, including agentic search, agentic tool use, and tool-integrated reasoning. Beyond benchmark performance, the model demonstrates strong generalization to complex tool interactions and robust behavior under noisy real-world environments. Its advanced capability stems from a unified training framework that combines domain-parallel expert training with subsequent fusion, together with an end-to-end co-design of data construction, environments, algorithms, and infrastructure spanning from pre-training to post-training. In particular, the model's strong generalization capability in complex tool-use are driven by our in-depth exploration of environment scaling and principled task construction. To optimize long-tailed, skewed generation and multi-turn agentic interactions, and to enable stable training across over 10,000 environments spanning more than 20 domains, we systematically extend our asynchronous reinforcement learning framework, DORA, for stable and efficient large-scale multi-environment training. Furthermore, recognizing that real-world tasks are inherently noisy, we conduct a systematic analysis and decomposition of real-world noise patterns, and design targeted training procedures to explicitly incorporate such imperfections into the training process, resulting in improved robustness for real-world applications. To further enhance performance on complex reasoning tasks, we introduce a Heavy Thinking mode that enables effective test-time scaling by jointly expanding reasoning depth and width through intensive parallel thinking.

</details>


### [30] [An Efficient Insect-inspired Approach for Visual Point-goal Navigation](https://arxiv.org/abs/2601.16806)
*Lu Yihe,Barbara Webb*

Main category: cs.AI

TL;DR: 开发了一种受昆虫启发的视觉点目标导航智能体，结合了昆虫大脑中负责联想学习和路径整合的两个结构，在Habitat点目标导航任务中表现出与SOTA模型相当的性能，但计算成本低多个数量级。


<details>
  <summary>Details</summary>
Motivation: 受到昆虫在发现食物位置和巢穴之间学习和优化视觉引导路径能力的启发，将Habitat点目标导航任务与昆虫的这种能力进行类比，旨在开发更高效、计算成本更低的导航系统。

Method: 结合昆虫大脑中两个关键结构的抽象模型：一个负责联想学习，另一个负责路径整合。构建了一个简单的昆虫启发式智能体，用于视觉点目标导航任务。

Result: 该昆虫启发式智能体在Habitat点目标导航任务中表现出与最新SOTA模型相当的性能，但计算成本低多个数量级。在更真实的模拟环境中测试显示该方法对扰动具有鲁棒性。

Conclusion: 昆虫大脑的简单神经结构可以为高效的视觉导航系统提供灵感，这种生物启发的方法在保持高性能的同时显著降低了计算需求，为机器人导航等领域提供了有前景的方向。

Abstract: In this work we develop a novel insect-inspired agent for visual point-goal navigation. This combines abstracted models of two insect brain structures that have been implicated, respectively, in associative learning and path integration. We draw an analogy between the formal benchmark of the Habitat point-goal navigation task and the ability of insects to learn and refine visually guided paths around obstacles between a discovered food location and their nest. We demonstrate that the simple insect-inspired agent exhibits performance comparable to recent SOTA models at many orders of magnitude less computational cost. Testing in a more realistic simulated environment shows the approach is robust to perturbations.

</details>


### [31] [Reasoning Promotes Robustness in Theory of Mind Tasks](https://arxiv.org/abs/2601.16853)
*Ian B. de Haan,Peter van der Putten,Max van Duijn*

Main category: cs.AI

TL;DR: 研究发现，通过强化学习训练的推理型大语言模型在心理理论任务中表现出更强的鲁棒性，但这种提升主要源于寻找正确解决方案的稳定性增强，而非全新的心理理论推理能力。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在心理理论测试中表现出色引发了对其底层能力本质的争议，同时通过强化学习训练的推理模型在多个基准测试中取得显著进步，研究者希望探究这类推理模型在心理理论任务中的表现。

Method: 采用新颖的机器心理学实验改编和已建立的基准测试结果，分析推理型大语言模型在心理理论任务中的行为表现，特别关注其对提示变化和任务扰动的鲁棒性。

Result: 推理模型在心理理论任务中表现出对提示变化和任务扰动更强的鲁棒性，但这种提升更可能归因于寻找正确解决方案的稳定性增强，而非形成了全新的心理理论推理形式。

Conclusion: 研究结果表明，评估大语言模型的社会认知行为时，需要区分真正的心理理论推理能力和模型在任务执行中的鲁棒性提升，这对准确评估语言模型的社会认知能力具有重要意义。

Abstract: Large language models (LLMs) have recently shown strong performance on Theory of Mind (ToM) tests, prompting debate about the nature and true performance of the underlying capabilities. At the same time, reasoning-oriented LLMs trained via reinforcement learning with verifiable rewards (RLVR) have achieved notable improvements across a range of benchmarks. This paper examines the behavior of such reasoning models in ToM tasks, using novel adaptations of machine psychological experiments and results from established benchmarks. We observe that reasoning models consistently exhibit increased robustness to prompt variations and task perturbations. Our analysis indicates that the observed gains are more plausibly attributed to increased robustness in finding the correct solution, rather than to fundamentally new forms of ToM reasoning. We discuss the implications of this interpretation for evaluating social-cognitive behavior in LLMs.

</details>


### [32] [MAGE-KT: Multi-Agent Graph-Enhanced Knowledge Tracing with Subgraph Retrieval and Asymmetric Fusion](https://arxiv.org/abs/2601.16886)
*Chi Yu,Hongyu Yuan,Zhiyi Duan*

Main category: cs.AI

TL;DR: MAGE-KT：多智能体图增强知识追踪框架，通过多视图异构图和条件子图检索解决传统方法中概念关系挖掘不足和注意力扩散问题


<details>
  <summary>Details</summary>
Motivation: 现有基于图的知识追踪方法存在两个主要问题：1）概念间关系挖掘不足，通常仅从交互序列推断；2）KT图的规模和异质性导致全图编码计算成本高且易受噪声影响，注意力会扩散到学生无关区域，降低概念间关系保真度

Method: 提出MAGE-KT框架：1）构建多视图异构图，结合多智能体概念关系提取器和学生-问题交互图，捕捉互补的语义和行为信号；2）基于目标学生历史检索紧凑高价值子图；3）使用非对称交叉注意力融合模块集成子图，增强预测同时避免注意力扩散和无关计算

Result: 在三个广泛使用的KT数据集上实验表明，该方法在概念关系准确性方面有显著提升，并在下一问题预测方面明显优于现有方法

Conclusion: MAGE-KT通过多视图图构建和条件子图检索有效解决了知识追踪中的概念关系挖掘和注意力扩散问题，为知识追踪提供了更精确和高效的方法

Abstract: Knowledge Tracing (KT) aims to model a student's learning trajectory and predict performance on the next question. A key challenge is how to better represent the relationships among students, questions, and knowledge concepts (KCs). Recently, graph-based KT paradigms have shown promise for this problem. However, existing methods have not sufficiently explored inter-concept relations, often inferred solely from interaction sequences. In addition, the scale and heterogeneity of KT graphs make full-graph encoding both computationally both costly and noise-prone, causing attention to bleed into student-irrelevant regions and degrading the fidelity of inter-KC relations. To address these issues, we propose a novel framework: Multi-Agent Graph-Enhanced Knowledge Tracing (MAGE-KT). It constructs a multi-view heterogeneous graph by combining a multi-agent KC relation extractor and a student-question interaction graph, capturing complementary semantic and behavioral signals. Conditioned on the target student's history, it retrieves compact, high-value subgraphs and integrates them using an Asymmetric Cross-attention Fusion Module to enhance prediction while avoiding attention diffusion and irrelevant computation. Experiments on three widely used KT datasets show substantial improvements in KC-relation accuracy and clear gains in next-question prediction over existing methods.

</details>


### [33] [Empowering Medical Equipment Sustainability in Low-Resource Settings: An AI-Powered Diagnostic and Support Platform for Biomedical Technicians](https://arxiv.org/abs/2601.16967)
*Bernes Lorier Atabonfack,Ahmed Tahiru Issah,Mohammed Hardi Abdul Baaki,Clemence Ingabire,Tolulope Olusuyi,Maruf Adewole,Udunna C. Anazodo,Timothy X Brown*

Main category: cs.AI

TL;DR: 开发AI支持平台帮助低收入和中等收入国家的生物医学技术人员实时诊断和维修医疗设备，通过LLM和用户界面提供故障排除指导，减少设备停机时间


<details>
  <summary>Details</summary>
Motivation: 低收入和中等收入国家中大量医疗诊断设备因缺乏及时维护、技术专家支持不足而闲置或故障，导致设备停机时间增加、诊断延迟和患者护理质量下降

Method: 开发集成大型语言模型的AI支持平台，配备用户友好的Web界面，允许技术人员输入错误代码或设备症状，获取逐步故障排除指导；平台还包括全球同行讨论论坛；使用Philips HDI 5000超声机进行概念验证

Result: 概念验证显示，在错误代码解释方面达到100%精确度，在建议纠正措施方面达到80%准确率

Conclusion: AI驱动系统支持医疗设备维护具有可行性和潜力，能够减少设备停机时间，改善资源受限环境中的医疗保健服务

Abstract: In low- and middle-income countries (LMICs), a significant proportion of medical diagnostic equipment remains underutilized or non-functional due to a lack of timely maintenance, limited access to technical expertise, and minimal support from manufacturers, particularly for devices acquired through third-party vendors or donations. This challenge contributes to increased equipment downtime, delayed diagnoses, and compromised patient care. This research explores the development and validation of an AI-powered support platform designed to assist biomedical technicians in diagnosing and repairing medical devices in real-time. The system integrates a large language model (LLM) with a user-friendly web interface, enabling imaging technologists/radiographers and biomedical technicians to input error codes or device symptoms and receive accurate, step-by-step troubleshooting guidance. The platform also includes a global peer-to-peer discussion forum to support knowledge exchange and provide additional context for rare or undocumented issues. A proof of concept was developed using the Philips HDI 5000 ultrasound machine, achieving 100% precision in error code interpretation and 80% accuracy in suggesting corrective actions. This study demonstrates the feasibility and potential of AI-driven systems to support medical device maintenance, with the aim of reducing equipment downtime to improve healthcare delivery in resource-constrained environments.

</details>
