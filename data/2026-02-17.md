<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 9]
- [cs.CR](#cs.CR) [Total: 9]
- [cs.AI](#cs.AI) [Total: 11]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [Perceptual Self-Reflection in Agentic Physics Simulation Code Generation](https://arxiv.org/abs/2602.12311)
*Prashant Shende,Bradley Camburn*

Main category: cs.SE

TL;DR: 多智能体框架通过自然语言生成物理模拟代码，采用感知自反思机制验证模拟结果的物理正确性，显著优于单次生成方法。


<details>
  <summary>Details</summary>
Motivation: 解决传统代码测试中存在的"预言差距"问题——语法正确的代码可能产生物理上错误的行为，而传统测试无法检测这种问题。需要一种能够验证模拟结果物理正确性的方法。

Method: 采用四智能体框架：自然语言解释器将用户请求转为物理描述；技术需求生成器产生缩放模拟参数；物理代码生成器带自动自校正；物理验证器实现感知自反思。关键创新是感知验证，使用视觉语言模型分析渲染的动画帧而非直接检查代码结构。

Result: 在七个领域（经典力学、流体动力学、热力学、电磁学、波物理、反应扩散系统、非物理数据可视化）评估，感知自反思架构显著优于单次生成基线，大多数测试场景达到目标物理精度阈值。系统表现出稳健的管道稳定性，具有一致的代码自校正能力，每次动画成本约0.20美元。

Conclusion: 将视觉模拟输出反馈给视觉语言模型进行迭代精炼，在物理模拟任务中显著优于单次代码生成，展示了智能体AI支持工程工作流和物理数据生成管道的潜力。

Abstract: We present a multi-agent framework for generating physics simulation code from natural language descriptions, featuring a novel perceptual self-reflection mechanism for validation. The system employs four specialized agents: a natural language interpreter that converts user requests into physics-based descriptions; a technical requirements generator that produces scaled simulation parameters; a physics code generator with automated self-correction; and a physics validator that implements perceptual self-reflection. The key innovation is perceptual validation, which analyzes rendered animation frames using a vision-capable language model rather than inspecting code structure directly. This approach addresses the ``oracle gap'' where syntactically correct code produces physically incorrect behavior--a limitation that conventional testing cannot detect. We evaluate the system across seven domains including classical mechanics, fluid dynamics, thermodynamics, electromagnetics, wave physics, reaction-diffusion systems, and non-physics data visualization. The perceptual self-reflection architecture demonstrates substantial improvement over single-shot generation baselines, with the majority of tested scenarios achieving target physics accuracy thresholds. The system exhibits robust pipeline stability with consistent code self-correction capability, operating at approximately \$0.20 per animation. These results validate our hypothesis that feeding visual simulation outputs back to a vision-language model for iterative refinement significantly outperforms single-shot code generation for physics simulation tasks and highlights the potential of agentic AI to support engineering workflows and physics data generation pipelines.

</details>


### [2] [SHAPR: A Solo Human-Centred and AI-Assisted Practice Framework for Research Software Development](https://arxiv.org/abs/2602.12443)
*Ka Ching Chan*

Main category: cs.SE

TL;DR: SHAPR框架为单人AI辅助研究软件开发提供实践层面的操作指南，将ADR原则转化为具体行动，支持人类问责和学习。


<details>
  <summary>Details</summary>
Motivation: 研究软件已成为高等教育研究的重要工具，而生成式AI正在重塑开发实践，但现有ADR框架对单人AI辅助研究软件开发的日常实践指导有限。

Method: 提出SHAPR框架作为实践层面的操作框架，通过明确角色、制品、反思实践和轻量级治理机制，将ADR的高层原则转化为可操作指导。

Result: SHAPR作为主要设计制品和分析单元，通过反思性分析评估其内部一致性、与ADR原则的契合度以及对单人研究实践的适用性。

Conclusion: SHAPR框架通过明确连接研究软件开发、人机协作和反思学习，支持知识生产和HDR研究人员培训，为当代研究环境提供实用指导。

Abstract: Research software has become a central vehicle for inquiry and learning in many Higher Degree Research (HDR) contexts, where solo researchers increasingly develop software-based artefacts as part of their research methodology. At the same time, generative artificial intelligence is reshaping development practice, offering powerful forms of assistance while introducing new challenges for accountability, reflection, and methodological rigour. Although Action Design Research (ADR) provides a well-established foundation for studying and constructing socio-technical artefacts, it offers limited guidance on how its principles can be operationalised in the day-to-day practice of solo, AI-assisted research software development. This paper proposes the SHAPR framework (Solo, Human-centred, AI-assisted PRactice) as a practice-level operational framework that complements ADR by translating its high-level principles into actionable guidance for contemporary research contexts. SHAPR supports the enactment of ADR Building-Intervention-Evaluation cycles by making explicit the roles, artefacts, reflective practices, and lightweight governance mechanisms required to sustain human accountability and learning in AI-assisted development. The contribution of the paper is conceptual: SHAPR itself is treated as the primary design artefact and unit of analysis and is evaluated formatively through reflective analysis of its internal coherence, alignment with ADR principles, and applicability to solo research practice. By explicitly linking research software development, Human-AI collaboration, and reflective learning, this study contributes to broader discussions on how SHAPR can support both knowledge production and HDR researcher training.

</details>


### [3] [Favia: Forensic Agent for Vulnerability-fix Identification and Analysis](https://arxiv.org/abs/2602.12500)
*André Storhaug,Jiamou Sun,Jingyue Li*

Main category: cs.SE

TL;DR: Favia是一个基于智能体的漏洞修复提交识别框架，通过高效候选排序和深度语义推理，在真实世界安全相关提交中准确识别漏洞修复，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有自动化方法（包括传统机器学习和大语言模型方法）在识别CVE对应的漏洞修复提交时存在精度-召回率权衡问题，且在随机采样评估中低估了真实场景的难度——真实候选提交已经是安全相关且高度相似的。

Method: Favia采用法医式、基于智能体的框架：1）高效排序阶段缩小提交搜索空间；2）使用基于ReAct的LLM智能体对每个提交进行严格评估，提供预提交仓库作为环境，配备专门工具，智能体尝试定位漏洞组件、导航代码库，并建立代码变更与漏洞根因之间的因果对齐。

Result: 在CVEVC数据集（包含3708个真实仓库的800多万次提交）上评估，Favia在真实候选选择场景下持续优于最先进的传统和LLM基线方法，实现了最强的精度-召回率权衡和最高的F1分数。

Conclusion: Favia通过证据驱动的过程能够稳健识别间接、多文件和非平凡的修复，这些修复是单次通过或基于相似性的方法难以发现的，为大规模安全软件维护提供了有效的漏洞修复识别解决方案。

Abstract: Identifying vulnerability-fixing commits corresponding to disclosed CVEs is essential for secure software maintenance but remains challenging at scale, as large repositories contain millions of commits of which only a small fraction address security issues. Existing automated approaches, including traditional machine learning techniques and recent large language model (LLM)-based methods, often suffer from poor precision-recall trade-offs. Frequently evaluated on randomly sampled commits, we uncover that they are substantially underestimating real-world difficulty, where candidate commits are already security-relevant and highly similar. We propose Favia, a forensic, agent-based framework for vulnerability-fix identification that combines scalable candidate ranking with deep and iterative semantic reasoning. Favia first employs an efficient ranking stage to narrow the search space of commits. Each commit is then rigorously evaluated using a ReAct-based LLM agent. By providing the agent with a pre-commit repository as environment, along with specialized tools, the agent tries to localize vulnerable components, navigates the codebase, and establishes causal alignment between code changes and vulnerability root causes. This evidence-driven process enables robust identification of indirect, multi-file, and non-trivial fixes that elude single-pass or similarity-based methods. We evaluate Favia on CVEVC, a large-scale dataset we made that comprises over 8 million commits from 3,708 real-world repositories, and show that it consistently outperforms state-of-the-art traditional and LLM-based baselines under realistic candidate selection, achieving the strongest precision-recall trade-offs and highest F1-scores.

</details>


### [4] [FuncDroid: Towards Inter-Functional Flows for Comprehensive Mobile App GUI Testing](https://arxiv.org/abs/2602.12834)
*Jinlong He,Changwei Xia,Binru Huang,Jiwei Yan,Jun Yan,Jian Zhang*

Main category: cs.SE

TL;DR: FuncDroid：一种基于功能流图的GUI测试方法，专注于跨功能交互的深度bug检测，显著提升了覆盖率和bug发现数量


<details>
  <summary>Details</summary>
Motivation: 随着移动应用功能日益复杂且迭代加速，确保高可靠性面临挑战。现有功能导向的GUI测试方法大多忽视跨功能交互，难以发现隐藏在跨功能行为中的深度bug。

Method: 首先设计功能流图（FFG）作为行为模型，明确捕获应用的功能单元及其跨功能交互。基于FFG，提出长短视图引导的测试过程，结合两种互补的测试生成视图，自适应细化功能边界，系统探索不同触发条件下的跨功能流。

Result: 在50个可复现崩溃bug的基准测试和52个流行商业应用上评估，FuncDroid在覆盖率（+28%）和bug检测数量（+107%）上显著优于现有方法，并在商业应用中成功发现18个先前未知的非崩溃功能bug。

Conclusion: FuncDroid通过建模跨功能交互并采用长短视图引导的测试策略，有效解决了现有GUI测试方法在跨功能bug检测方面的不足，具有实际应用价值。

Abstract: As mobile application (app) functionalities grow increasingly complex and their iterations accelerate, ensuring high reliability presents significant challenges. While functionality-oriented GUI testing has attracted growing research attention, existing approaches largely overlook interactions across functionalities, making them ineffective at uncovering deep bugs hidden in inter-functional behaviors. To fill this gap, we first design a Functional Flow Graph (FFG), a behavioral model that explicitly captures an app's functional units and their inter-functional interactions. Based on the FFG, we further introduce an inter-functional-flow-oriented GUI testing approach with the dual goals of precise model construction and deep bug detection. This approach is realized through a long-short-term-view-guided testing process. By combining two complementary test-generation views, it can adaptively refine functional boundaries and systematically explore inter-functional flows under diverse triggering conditions. We implement our approach in a tool called FuncDroid, and evaluate it on two benchmarks: (1) a widely-used open-source benchmark with 50 reproducible crash bugs and (2) a diverse set of 52 popular commercial apps. Experimental results demonstrate that FuncDroid significantly outperforms state-of-the-art baselines in both coverage (+28%) and bug detection number (+107%). Moreover, FuncDroid successfully uncovers 18 previously unknown non-crash functional bugs in commercial apps, confirming its practical effectiveness.

</details>


### [5] [A Microservice-Based Platform for Sustainable and Intelligent SLO Fulfilment and Service Management](https://arxiv.org/abs/2602.12875)
*Juan Luis Herrera,Daniel Wang,Schahram Dustdar*

Main category: cs.SE

TL;DR: CASCA是一个开源微服务架构平台，帮助计算连续体提供商在保护开发者隐私的前提下，通过运行时重新配置服务来满足性能和可持续性SLOs


<details>
  <summary>Details</summary>
Motivation: 微服务架构应用部署到计算连续体时，同时满足性能与可持续性服务等级目标具有挑战性，且需要保护开发者隐私

Method: 开发了基于微服务架构的开源平台CASCA，允许计算连续体提供商在运行时重新配置服务，同时保持开发者隐私

Result: 在真实计算连续体测试环境中评估CASCA，使用Bash、Rust和Python实现的决策系统成功重新配置媒体流服务，且隐私保护不受影响

Conclusion: CASCA平台有效解决了计算连续体中微服务应用SLOs管理难题，在保护隐私的同时实现了高度可重用、可分发和易扩展的解决方案

Abstract: The Microservices Architecture (MSA) design pattern has become a staple for modern applications, allowing functionalities to be divided across fine-grained microservices, fostering reusability, distribution, and interoperability. As MSA-based applications are deployed to the Computing Continuum (CC), meeting their Service Level Objectives (SLOs) becomes a challenge. Trading off performance and sustainability SLOs is especially challenging. This challenge can be addressed with intelligent decision systems, able to reconfigure the services during runtime to meet the SLOs. However, developing these agents while adhering to the MSA pattern is complex, especially because CC providers, who have key know-how and information to fulfill these SLOs, must comply with the privacy requirements of application developers. This work presents the Carbon-Aware SLO and Control plAtform (CASCA), an open-source MSA-based platform that allows CC providers to reconfigure services and fulfill their SLOs while maintaining the privacy of developers. CASCA is architected to be highly reusable, distributable, and easy to use, extend, and modify. CASCA has been evaluated in a real CC testbed for a media streaming service, where decision systems implemented in Bash, Rust, and Python successfully reconfigured the service, unaffected by upholding privacy.

</details>


### [6] [The Influence of Code Smells in Efferent Neighbors on Class Stability](https://arxiv.org/abs/2602.12950)
*Zushuai Zhang,Elliott Wen,Ewan Tempero*

Main category: cs.SE

TL;DR: 研究代码异味在依赖类中的存在如何影响类稳定性，考虑代码异味相互关联和交互作用


<details>
  <summary>Details</summary>
Motivation: 理解代码不稳定性的驱动因素对软件维护至关重要。现有研究主要关注被修改类内部的代码异味，但实践中类可能因其依赖的类被修改而发生变化（涟漪效应）。代码异味很少单独出现，它们经常在类内部或通过静态依赖连接的类之间相互关联，这种关联可能导致代码异味交互作用，进一步加剧维护问题。然而，代码异味相互关联和交互作用对代码质量的影响尚未得到充分探索。

Method: 从100个高星GitHub项目中挖掘一年的提交历史，检测代码异味和静态依赖关系，确定代码异味的相互关联和交互作用，并将这些因素建模为类稳定性的预测因子。

Result: 论文摘要未提供具体结果，但研究框架已建立，旨在分析依赖类中代码异味的存在、代码异味相互关联和交互作用如何影响类稳定性。

Conclusion: 需要研究代码异味在依赖类中的存在是否影响类稳定性，并考虑代码异味的相互关联和交互作用因素，这对理解软件维护中的代码稳定性有重要意义。

Abstract: Understanding what drives code instability is essential for effective software maintenance, as unstable classes require larger or more frequent edits and increase the risk of unintended side effects. Although code smells are widely believed to harm maintainability, most prior stability studies examine only the smells within the class being modified. In practice, however, classes can change because their efferent neighbors (i.e., the classes they depend on) are modified due to ripple effects that propagate along static dependencies, even if the class itself is clean. Such ripple effects may be more severe when the efferent neighbor exhibits code smells. In addition, code smells rarely occur alone. They often appear together within a class or across classes connected by static dependencies, a phenomenon known as code smell interrelation. Such interrelation can lead to code smell interaction, where smells are directly connected through static dependencies and may further compound maintainability issues. However, the effect of code smell interrelation and interaction on code quality remains largely underexplored. Therefore, this study investigates whether the presence of code smells in a class's efferent neighbors affects its stability, considering the factor of code smell interrelation and interaction. To achieve this, we mine one year of commit history from 100 top-starred GitHub projects, detect code smells and static dependencies, determine code smell interrelation and interaction, and model these factors as predictors of class stability.

</details>


### [7] [Analysis of Asset Administration Shell-based Negotiation Processes for Scaling Applications](https://arxiv.org/abs/2602.13029)
*David Dietrich,Armin Lechler,Alexander Verl*

Main category: cs.SE

TL;DR: 本文研究了主动式资产管理外壳（AAS）在资产数量扩展时的协商效率，分析了其性能限制、通信开销和可扩展性。


<details>
  <summary>Details</summary>
Motivation: 当前AAS标准化主要关注子模型和安全性以实现互操作数据访问，但其主动行为仍处于概念阶段。现有研究仅考察了有限数量资产的主动AAS架构，对其在工业环境中的可扩展性存在疑问。

Method: 引入了一个场景和评估标准来分析主动AAS的可扩展应用，基于当前主动AAS架构开发了可扩展实现，并对不同数量的资产进行了实验。

Result: 实验结果揭示了AAS基于协商机制扩展时的性能限制、通信开销和适应性，这些信息可用于改进AAS的进一步开发和标准化。

Conclusion: 主动AAS在资产数量扩展时面临性能挑战，研究结果为AAS的进一步开发和标准化提供了重要参考，特别是在大规模工业应用场景中。

Abstract: The proactive Asset Administration Shell (AAS) enables bidirectional communication between assets. It uses the Language for I4.0 Components in VDI/VDE 2193 to facilitate negotiations, such as allocating products to available production resources. This paper investigates the efficiency of the negotiation, based on criteria, such as message load, for applications with a scaling number of assets. Currently, the focus of AAS standardization is on submodels and their security to enable interoperable data access. Their proactive behavior remains conceptual and is still a subject of scientific research. Existing studies examine proactive AAS architecture examples with a limited number of assets, raising questions about their scalability in industrial environments. To analyze proactive AAS for scaling applications, a scenario and evaluation criteria are introduced. A scalable implementation is developed using current architectures for proactive AAS, upon which experiments are conducted with a varying number of assets. The results reveal the performance limitations, communication overhead, and adaptability of the AAS-based negotiation mechanism scaling. This information can improve the further development and standardization of the AAS.

</details>


### [8] [Automated Testing of Task-based Chatbots: How Far Are We?](https://arxiv.org/abs/2602.13072)
*Diego Clerissi,Elena Masserini,Daniela Micucci,Leonardo Mariani*

Main category: cs.SE

TL;DR: 本文通过实证研究评估了最先进的聊天机器人测试技术在真实任务型聊天机器人上的有效性，揭示了现有方法在测试场景生成和断言机制方面的局限性。


<details>
  <summary>Details</summary>
Motivation: 随着聊天机器人在实际应用中的普及，有效评估其质量变得至关重要。传统测试技术无法系统性地覆盖聊天机器人的对话空间，而专门针对聊天机器人的测试方法虽然有所发展，但仍存在测试场景简单和断言机制薄弱等局限性。

Method: 采用验证性研究设计，在精心挑选的GitHub任务型聊天机器人上评估最先进的聊天机器人测试技术。这些聊天机器人基于最流行的商业和开源平台开发，确保研究的代表性和实用性。

Result: 研究证实了现有聊天机器人测试技术确实存在局限性，特别是在测试场景的复杂性和断言机制的完备性方面。通过在实际聊天机器人上的评估，揭示了这些方法在实际应用中的不足。

Conclusion: 当前最先进的聊天机器人测试技术仍需改进，特别是在生成更复杂的测试场景和加强断言机制方面。该研究为未来聊天机器人测试技术的发展提供了实证基础和方向指引。

Abstract: Task-based chatbots are software, typically embedded in real-world applications, that assist users in completing tasks through a conversational interface. As chatbots are gaining popularity, effectively assessing their quality has become crucial. Whereas traditional testing techniques fail to systematically exercise the conversational space of chatbots, several approaches specifically targeting chatbots have emerged from both industry and research. Although these techniques have shown advancements over the years, they still exhibit limitations, such as simplicity of the generated test scenarios and weakness in implemented oracles. In this paper, we conduct a confirmatory study to investigate such limitations by evaluating the effectiveness of state-of-the-art chatbot testing techniques on a curated selection of task-based chatbots from GitHub, developed using the most popular commercial and open-source platforms.

</details>


### [9] [Source Code Hotspots: A Diagnostic Method for Quality Issues](https://arxiv.org/abs/2602.13170)
*Saleha Muzammil,Mughees Ur Rehman,Zoe Kotti,Diomidis Spinellis*

Main category: cs.SE

TL;DR: 该研究分析了91个GitHub仓库的版本历史，识别出15种常见的代码热点模式，其中前三种是版本锁定更新、长行变更和格式化乒乓，74%的热点编辑由自动化账户生成，研究提供了可操作的改进指南。


<details>
  <summary>Details</summary>
Motivation: 软件源代码中经常存在"热点"——那些比项目其他部分变更频繁得多的代码区域，这些热点集中了维护活动。研究旨在理解这些热点为何出现，并帮助开发者减少维护负担。

Method: 挖掘了91个活跃开发的GitHub仓库的完整版本历史，识别出15种重复出现的行级热点模式，分析这些模式的特征和成因。

Result: 发现三种最普遍的模式：版本锁定更新（26%）、长行变更（17%）和格式化乒乓（9%）。令人惊讶的是，74%的热点编辑由自动化账户生成，表明机器人活动是变更历史中主要但可避免的噪声来源。

Conclusion: 通过将每种模式映射到具体的重构指南和持续集成检查，该分类法为实践者提供了可操作的步骤来抑制热点，系统性地在可配置性、稳定性和可变更性方面提高软件质量。

Abstract: Software source code often harbours "hotspots": small portions of the code that change far more often than the rest of the project and thus concentrate maintenance activity. We mine the complete version histories of 91 evolving, actively developed GitHub repositories and identify 15 recurring line-level hotspot patterns that explain why these hotspots emerge. The three most prevalent patterns are Pinned Version Bump (26%), revealing brittle release practices; Long Line Change (17%), signalling deficient layout; and Formatting Ping-Pong (9%), indicating missing or inconsistent style automation. Surprisingly, automated accounts generate 74% of all hotspot edits, suggesting that bot activity is a dominant but largely avoidable source of noise in change histories. By mapping each pattern to concrete refactoring guidelines and continuous integration checks, our taxonomy equips practitioners with actionable steps to curb hotspots and systematically improve software quality in terms of configurability, stability, and changeability.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [10] [Secrecy and Verifiability: An Introduction to Electronic Voting](https://arxiv.org/abs/2602.12398)
*Paul Keeler,Ben Smyth*

Main category: cs.CR

TL;DR: 这篇教程论文向非电子投票领域的读者介绍了电子投票系统的基本密码学概念，重点阐述了如何通过现代密码学工具在选票保密性和可验证性之间取得平衡。


<details>
  <summary>Details</summary>
Motivation: 电子投票系统旨在替代传统的纸质投票，但现有方案存在固有缺陷。作者认为任何电子投票系统都需要两个基本属性：选票保密性和可验证性，这两个属性看似相互矛盾，需要通过现代密码学工具来解决这一挑战。

Method: 使用非对称加密和同态加密等基本密码学概念，采用基于游戏的密码学标准方法，将选举形式化为游戏，并在此框架内给出选票保密性和可验证性的精确定义。

Result: 提出了一个通用的电子选举方案，通过最小化数学形式主义使概念易于理解，为现代电子投票研究提供了方法论框架。

Conclusion: 现代密码学工具能够有效平衡电子投票系统中的选票保密性和可验证性，基于游戏的密码学框架为电子投票系统的形式化分析和设计提供了坚实基础。

Abstract: Democracies are built upon secure and reliable voting systems. Electronic voting systems seek to replace ballot papers and boxes with computer hardware and software. Proposed electronic election schemes have been subjected to scrutiny, with researchers spotting inherent faults and weaknesses. Inspired by physical voting systems, we argue that any electronic voting system needs two essential properties: ballot secrecy and verifiability. These properties seemingly work against each other. An election scheme that is a complete black box offers ballot secrecy, but verification of the outcome is impossible. This challenge can be tackled using standard tools from modern cryptography, reaching a balance that delivers both properties.
  This tutorial makes these ideas accessible to readers outside electronic voting. We introduce fundamental concepts such as asymmetric and homomorphic encryption, which we use to describe a general electronic election scheme while keeping mathematical formalism minimal. We outline game-based cryptography, a standard approach in modern cryptography, and introduce notation for formulating elections as games. We then give precise definitions of ballot secrecy and verifiability in the framework of game-based cryptography. A principal aim is introducing modern research approaches to electronic voting.

</details>


### [11] [Sparse Autoencoders are Capable LLM Jailbreak Mitigators](https://arxiv.org/abs/2602.12418)
*Yannick Assogba,Jacopo Cortellazzi,Javier Abad,Pau Rodriguez,Xavier Suau,Arno Blaas*

Main category: cs.CR

TL;DR: CC-Delta是一种基于稀疏自编码器的防御方法，通过比较有害请求在有/无越狱上下文时的token级表示，识别越狱相关稀疏特征，并在推理时进行均值偏移引导，有效防御越狱攻击。


<details>
  <summary>Details</summary>
Motivation: 越狱攻击对大型语言模型安全构成持续威胁，现有防御方法存在局限性，需要开发更有效的防御机制来保护模型安全。

Method: 使用配对的有害/越狱提示，通过统计测试选择特征，在稀疏自编码器潜在空间中进行推理时均值偏移引导，识别越狱相关稀疏特征。

Result: 在四个对齐的指令调优模型和十二种越狱攻击上，CC-Delta实现了与基线防御相当或更好的安全-效用权衡，在所有四个模型上明显优于密集均值偏移引导，特别是对分布外攻击效果显著。

Conclusion: 为可解释性训练的现成稀疏自编码器可以重新用作实用的越狱防御，无需任务特定训练，在稀疏特征空间中进行引导比在密集激活空间中具有优势。

Abstract: Jailbreak attacks remain a persistent threat to large language model safety. We propose Context-Conditioned Delta Steering (CC-Delta), an SAE-based defense that identifies jailbreak-relevant sparse features by comparing token-level representations of the same harmful request with and without jailbreak context. Using paired harmful/jailbreak prompts, CC-Delta selects features via statistical testing and applies inference-time mean-shift steering in SAE latent space. Across four aligned instruction-tuned models and twelve jailbreak attacks, CC-Delta achieves comparable or better safety-utility tradeoffs than baseline defenses operating in dense latent space. In particular, our method clearly outperforms dense mean-shift steering on all four models, and particularly against out-of-distribution attacks, showing that steering in sparse SAE feature space offers advantages over steering in dense activation space for jailbreak mitigation. Our results suggest off-the-shelf SAEs trained for interpretability can be repurposed as practical jailbreak defenses without task-specific training.

</details>


### [12] [DRAMatic Speedup: Accelerating HE Operations on a Processing-in-Memory System](https://arxiv.org/abs/2602.12433)
*Niklas Klinger,Jonas Sander,Peterson Yuhala,Pascal Felber,Thomas Eisenbarth*

Main category: cs.CR

TL;DR: DRAMatic在UPMEM PIM系统上实现同态加密基础操作，通过多种算术优化显著缩小了与Microsoft SEAL的性能差距，但仍受限于PIM的乘法性能和数据传输开销。


<details>
  <summary>Details</summary>
Motivation: 同态加密（HE）是机密云计算的有前景技术，但计算成本高且在传统架构上受内存限制。内存内处理（PIM）架构提供更高内存带宽，可能适合加速HE计算。

Method: 在UPMEM的可编程通用PIM系统上实现DRAMatic，采用余数系统和数论变换等算术优化技术，支持安全同态评估所需的大参数。

Result: DRAMatic显著缩小了UPMEM PIM与Microsoft SEAL之间的性能差距，但在运行时和能效方面仍受限于PIM的乘法性能和数据传输开销。

Conclusion: PIM架构有潜力加速同态加密，但需要硬件扩展来改善乘法性能和减少数据传输开销，以充分发挥其优势。

Abstract: Homomorphic encryption (HE) is a promising technology for confidential cloud computing, as it allows computations on encrypted data. However, HE is computationally expensive and often memory-bound on conventional computer architectures. Processing-in-Memory (PIM) is an alternative hardware architecture that integrates processing units and memory on the same chip or memory module. PIM enables higher memory bandwidth than conventional architectures and could thus be suitable for accelerating HE. In this work, we present DRAMatic, which implements operations foundational to HE on UPMEM's programmable, general-purpose PIM system, and evaluate its performance. DRAMatic incorporates many arithmetic optimizations, including residue number system and number-theoretic transform techniques, and can support the large parameters required for secure homomorphic evaluations. To compare performance, we evaluate DRAMatic against Microsoft SEAL, a popular open-source HE library, regarding both runtime and energy efficiency. The results show that DRAMatic significantly closes the gap between UPMEM PIM and Microsoft SEAL. However, we also show that DRAMatic is currently constrained by UPMEM PIM's multiplication performance and data transfer overhead. Finally, we discuss potential hardware extensions to UPMEM PIM.

</details>


### [13] [RADAR: Exposing Unlogged NoSQL Operations](https://arxiv.org/abs/2602.12600)
*Mahfuzul I. Nissan,James Wagner*

Main category: cs.CR

TL;DR: RADAR是一个日志对抗感知的数字取证框架，通过交叉比对低层存储痕迹与高层应用日志来获取取证真相，绕过数据库API和管理系统，直接从磁盘原始字节中提取证据。


<details>
  <summary>Details</summary>
Motivation: NoSQL数据库的广泛采用使得数字取证变得困难，因为存储格式多样且不透明，而且当特权内部人员（如DevOps或管理员）可以禁用、抑制或操纵日志来隐藏活动时，审计日志的可靠性无法保证。

Method: RADAR使用自动NoSQL雕刻器（ANOC）从原始磁盘字节中推断布局并雕刻记录，绕过数据库API和管理系统，然后将雕刻出的痕迹与审计日志进行比对，识别未记录的插入、静默删除和字段级更新等差异痕迹。

Result: 在10种NoSQL引擎（包括BerkeleyDB、LMDB、MDBX等）上进行评估，在日志规避场景（如日志抑制和后维护攻击）下，RADAR能够持续暴露未归因的操作，同时保持31.7-397 MB/min的处理吞吐量。

Conclusion: RADAR证明了独立于日志的、可信的NoSQL数字取证的可行性，能够在日志不可信的情况下通过物理存储作为独立证据源进行可靠的取证分析。

Abstract: The widespread adoption of NoSQL databases has made digital forensics increasingly difficult as storage formats are diverse and often opaque, and audit logs cannot be assumed trustworthy when privileged insiders, such as DevOps or administrators, can disable, suppress, or manipulate logging to conceal activity. We present RADAR (Record & Artifact Detection, Alignment & Reporting), a log-adversary-aware framework that derives forensic ground truth by cross-referencing low-level storage artifacts against high-level application logs. RADAR analyzes artifacts reconstructed by the Automated NoSQL Carver (ANOC), which infers layouts and carves records directly from raw disk bytes, bypassing database APIs and the management system entirely, thereby treating physical storage as the independent evidence source. RADAR then reconciles carved artifacts with the audit log to identify delta artifacts such as unlogged insertions, silent deletions, and field-level updates that exist on disk but are absent from the logical history. We evaluate RADAR across ten NoSQL engines, including BerkeleyDB, LMDB, MDBX, etcd, ZODB, Durus, LiteDB, Realm, RavenDB, and NitriteDB, spanning key-value and document stores and multiple storage designs, e.g., copy-on-write/MVCC, B/B+ tree, and append-only. Under log-evasion scenarios, such as log suppression and post-maintenance attacks, including cases where historical bytes are pruned, RADAR consistently exposes unattributed operations while sustaining 31.7-397 MB/min processing throughput, demonstrating the feasibility of log-independent, trustworthy NoSQL forensics.

</details>


### [14] [TensorCommitments: A Lightweight Verifiable Inference for Language Models](https://arxiv.org/abs/2602.12630)
*Oguzhan Baser,Elahe Sadeghi,Eric Wang,David Ribeiro Alves,Sam Kazemian,Hong Kang,Sandeep P. Chinchali,Sriram Vishwanath*

Main category: cs.CR

TL;DR: TensorCommitments (TCs) 是一种用于可验证LLM推理的证明方案，通过张量原生承诺和多元Terkle树实现，在LLaMA2上仅增加0.97%证明时间和0.12%验证时间，同时提高对抗攻击的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型大多运行在外部云上，用户需要信任远程GPU正确执行推理而不会受到对抗性篡改。现有密码学方法在LLM规模上太慢，而非密码学方法需要强大的验证者GPU，因此需要一种高效的可验证LLM推理方案。

Method: 提出TensorCommitments (TCs)方案，这是一种张量原生的推理证明方案。TC将LLM推理绑定到一个承诺上，这是一个不可逆的标签，在篡改时会失效。方案使用多元Terkle树组织承诺结构。

Result: 在LLaMA2模型上，TC仅增加0.97%的证明者时间和0.12%的验证者时间（相对于推理时间）。与需要验证者GPU的最佳先前工作相比，TC将针对定制LLM攻击的鲁棒性提高了48%。

Conclusion: TensorCommitments提供了一种高效的可验证LLM推理方案，在保持低开销的同时显著提高了对抗攻击的鲁棒性，解决了云环境中LLM推理的可信问题。

Abstract: Most large language models (LLMs) run on external clouds: users send a prompt, pay for inference, and must trust that the remote GPU executes the LLM without any adversarial tampering. We critically ask how to achieve verifiable LLM inference, where a prover (the service) must convince a verifier (the client) that an inference was run correctly without rerunning the LLM. Existing cryptographic works are too slow at the LLM scale, while non-cryptographic ones require a strong verifier GPU. We propose TensorCommitments (TCs), a tensor-native proof-of-inference scheme. TC binds the LLM inference to a commitment, an irreversible tag that breaks under tampering, organized in our multivariate Terkle Trees. For LLaMA2, TC adds only 0.97% prover and 0.12% verifier time over inference while improving robustness to tailored LLM attacks by up to 48% over the best prior work requiring a verifier GPU.

</details>


### [15] [Reliable Hierarchical Operating System Fingerprinting via Conformal Prediction](https://arxiv.org/abs/2602.12825)
*Rubén Pérez-Jove,Osvaldo Simeone,Alejandro Pazos,Jose Vázquez-Naya*

Main category: cs.CR

TL;DR: 本文针对操作系统指纹识别缺乏不确定性量化的问题，提出两种结构化共形预测方法：层级独立校准（L-CP）和基于投影的校准（P-CP），在保证覆盖率的同时处理OS分类的层次结构。


<details>
  <summary>Details</summary>
Motivation: 传统操作系统指纹识别方法缺乏正式的不确定性量化机制，且将OS识别视为扁平分类问题，忽略了OS的自然分类层次结构，导致预测结果脆弱。

Method: 提出两种结构化共形预测策略：1）层级独立校准（L-CP），独立校准每个层次级别；2）基于投影的校准（P-CP），通过将叶级预测集向上投影来确保结构一致性。

Result: 两种方法都满足有效性保证，但揭示了层级效率与结构一致性之间的基本权衡：L-CP产生更紧凑的预测集适合人工取证分析，但存在分类不一致性；P-CP保证层次一致的嵌套集适合自动化策略执行，但在较粗级别效率降低。

Conclusion: 结构化共形预测方法能够为操作系统指纹识别提供具有保证覆盖率的不确定性量化，L-CP和P-CP分别适用于不同应用场景，在效率与结构一致性之间存在权衡。

Abstract: Operating System (OS) fingerprinting is critical for network security, but conventional methods do not provide formal uncertainty quantification mechanisms. Conformal Prediction (CP) could be directly wrapped around existing methods to obtain prediction sets with guaranteed coverage. However, a direct application of CP would treat OS identification as a flat classification problem, ignoring the natural taxonomic structure of OSs and providing brittle point predictions. This work addresses these limitations by introducing and evaluating two distinct structured CP strategies: level-wise CP (L-CP), which calibrates each hierarchy level independently, and projection-based CP (P-CP), which ensures structural consistency by projecting leaf-level sets upwards. Our results demonstrate that, while both methods satisfy validity guarantees, they expose a fundamental trade-off between level-wise efficiency and structural consistency. L-CP yields tighter prediction sets suitable for human forensic analysis but suffers from taxonomic inconsistencies. Conversely, P-CP guarantees hierarchically consistent, nested sets ideal for automated policy enforcement, albeit at the cost of reduced efficiency at coarser levels.

</details>


### [16] [Neighborhood Blending: A Lightweight Inference-Time Defense Against Membership Inference Attacks](https://arxiv.org/abs/2602.12943)
*Osama Zafar,Shaojie Zhan,Tianxi Ji,Erman Ayday*

Main category: cs.CR

TL;DR: 论文提出了一种名为Neighborhood Blending的推理时防御机制，通过基于查询样本邻域的置信度平滑来抵御成员推理攻击，无需重新训练模型且计算开销小。


<details>
  <summary>Details</summary>
Motivation: 机器学习即服务（MLaaS）在敏感环境中的广泛应用引发了严重的隐私担忧，特别是成员推理攻击（MIAs）能够判断特定记录是否包含在模型训练集中。现有防御方法如对抗正则化、DP-SGD和MemGuard等存在权衡问题，如牺牲模型效用、增加计算需求或对不同攻击向量保护不一致。

Method: 提出Neighborhood Blending方法，这是一种推理时防御机制，通过使用差分隐私采样选择相似训练样本，对查询样本的邻域进行预测平均，从而平滑模型的置信度输出，建立一致的置信度模式，使成员和非成员对攻击者难以区分。

Result: 通过在不同数据集和模型上的广泛实验表明，该防御方法显著降低了MIA成功率，同时保持了模型性能，在效用保持方面优于现有的后训练防御方法（如MemGuard）和训练时技术（如DP-SGD）。

Conclusion: Neighborhood Blending是一种模型无关的轻量级解决方案，通过自适应"按需付费"的失真策略，在保持标签完整性（零标签损失）和高效用的同时增强隐私保护，为MLaaS环境提供了实用且有效的隐私防御机制。

Abstract: In recent years, the widespread adoption of Machine Learning as a Service (MLaaS), particularly in sensitive environments, has raised considerable privacy concerns. Of particular importance are membership inference attacks (MIAs), which exploit behavioral discrepancies between training and non-training data to determine whether a specific record was included in the model's training set, thereby presenting significant privacy risks. Although existing defenses, such as adversarial regularization, DP-SGD, and MemGuard, assist in mitigating these threats, they often entail trade-offs such as compromising utility, increased computational requirements, or inconsistent protection against diverse attack vectors.
  In this paper, we introduce a novel inference-time defense mechanism called Neighborhood Blending, which mitigates MIAs without retraining the model or incurring significant computational overhead. Our approach operates post-training by smoothing the model's confidence outputs based on the neighborhood of a queried sample. By averaging predictions from similar training samples selected using differentially private sampling, our method establishes a consistent confidence pattern, rendering members and non-members indistinguishable to an adversary while maintaining high utility. Significantly, Neighborhood Blending maintains label integrity (zero label loss) and ensures high utility through an adaptive, "pay-as-you-go" distortion strategy. It is a model-agnostic approach that offers a practical, lightweight solution that enhances privacy without sacrificing model utility. Through extensive experiments across diverse datasets and models, we demonstrate that our defense significantly reduces MIA success rates while preserving model performance, outperforming existing post-hoc defenses like MemGuard and training-time techniques like DP-SGD in terms of utility retention.

</details>


### [17] [Cryptographic Choreographies](https://arxiv.org/abs/2602.12967)
*Sebastian Mödersheim,Simon Lund,Alessandro Bruni,Marco Carbone,Rosario Giustolisi*

Main category: cs.CR

TL;DR: CryptoChoreo是一种用于规范密码协议的编排语言，扩展了Alice-and-Bob表示法，支持非确定性选择、条件分支和可变长期内存，通过翻译到进程演算定义语义，并与ProVerif连接实现实际可行性验证。


<details>
  <summary>Details</summary>
Motivation: 现有密码协议规范方法（如Alice-and-Bob表示法）通常孤立地指定每个协议角色，缺乏对协议整体的直观高层视图。需要一种更强大的编排语言来支持复杂的协议特性，如非确定性选择、条件分支和状态维护。

Method: 开发CryptoChoreo编排语言，扩展标准Alice-and-Bob表示法，增加非确定性选择、条件分支和可变长期内存功能。通过翻译到进程演算定义形式语义，该语义处理任意代数理论和非确定性选择下的消息解析、检查和构造。实现代表性理论的验证系统，并与ProVerif工具连接。

Result: 成功定义了CryptoChoreo的形式语义，虽然相关代数问题在一般情况下不可判定，但实现了代表性理论的验证系统。通过多个案例研究证明该方法在实际应用中是可行的，能够有效分析和验证复杂的密码协议。

Conclusion: CryptoChoreo提供了一种直观且形式化的密码协议规范方法，扩展了传统表示法的表达能力，通过形式语义和工具集成实现了实际可行性验证，为复杂密码协议的设计和分析提供了有效框架。

Abstract: We present CryptoChoreo, a choreography language for the specification of cryptographic protocols. Choreographies can be regarded as an extension of Alice-and-Bob notation, providing an intuitive high-level view of the protocol as a whole (rather than specifying each protocol role in isolation). The extensions over standard Alice-and-Bob notation that we consider are non-deterministic choice, conditional branching, and mutable long-term memory. We define the semantics of CryptoChoreo by translation to a process calculus. This semantics entails an understanding of the protocol: it determines how agents parse and check incoming messages and how they construct outgoing messages, in the presence of an arbitrary algebraic theory and non-deterministic choices made by other agents. While this semantics entails algebraic problems that are in general undecidable, we give an implementation for a representative theory. We connect this translation to ProVerif and show on a number of case studies that the approach is practically feasible.

</details>


### [18] [TrustMee: Self-Verifying Remote Attestation Evidence](https://arxiv.org/abs/2602.13148)
*Parsa Sadri Sinaki,Zainab Ahmad,Wentao Xie,Merlijn Sebrechts,Jimmy Kjällman,Lachlan J. Gunn*

Main category: cs.CR

TL;DR: 论文提出自验证远程证明证据概念，通过在证明包中嵌入WebAssembly验证逻辑，将证据验证转化为标准代码签名问题，无需平台特定知识。


<details>
  <summary>Details</summary>
Motivation: 硬件安全远程证明对建立机密虚拟机完整性信任至关重要，但实践中难以使用，因为验证证明证据需要硬件特定的加密逻辑，增加了维护成本和验证者的可信计算基。

Method: 引入自验证远程证明证据概念，每个证明包包含由可信方签名的WebAssembly组件作为验证逻辑。验证者检查嵌入式逻辑的签名，然后执行它以验证证据，将证据验证转化为标准代码签名问题。

Result: 实现了TrustMee平台无关验证驱动程序，支持AMD SEV-SNP和Intel TDX证明的自验证证据，生成标准EAT证明结果格式的证明声明。

Conclusion: 自验证远程证明证据方法使验证者无需平台特定知识即可验证证明证据，降低了维护成本和可信计算基，提高了实际可用性。

Abstract: Hardware-secured remote attestation is essential to establishing trust in the integrity of confidential virtual machines (cVMs), but is difficult to use in practice because verifying attestation evidence requires the use of hardware-specific cryptographic logic. This increases both maintenance costs and the verifiers' trusted computing base. We introduce the concept of self-verifying remote attestation evidence. Each attestation bundle includes verification logic as a WebAssembly component signed by a trusted party. This approach transforms evidence verification into a standard code-signing problem: the verifier checks the signature on the embedded logic and then executes it to validate the evidence. As a result, verifiers can validate attestation evidence without any platform-specific knowledge. We implement this concept as TrustMee, a platform-agnostic verification driver for the Trustee framework. We demonstrate its functionality with self-verifying evidence for AMD SEV-SNP and Intel TDX attestations, producing attestation claims in the standard EAT Attestation Result (EAR) format.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [19] [GT-HarmBench: Benchmarking AI Safety Risks Through the Lens of Game Theory](https://arxiv.org/abs/2602.12316)
*Pepijn Cobben,Xuanqiang Angelo Huang,Thao Amelia Pham,Isabel Dahlgren,Terry Jingchen Zhang,Zhijing Jin*

Main category: cs.AI

TL;DR: GT-HarmBench是一个包含2009个高风险场景的多智能体安全基准测试，涵盖囚徒困境、猎鹿博弈等博弈论结构，用于评估前沿AI系统在多智能体环境中的协调能力和风险。


<details>
  <summary>Details</summary>
Motivation: 现有AI安全基准主要评估单智能体，而忽略了多智能体环境中的风险，如协调失败和冲突。随着前沿AI系统在关键多智能体环境中部署，需要专门基准来评估这些风险。

Method: 从MIT AI风险库中提取真实AI风险场景，构建包含2009个高风险场景的基准测试，涵盖囚徒困境、猎鹿博弈、斗鸡博弈等博弈论结构。评估15个前沿模型，测量对博弈论提示框架和顺序的敏感性，并分析导致失败的推理模式。

Result: 在15个前沿模型中，智能体仅在62%的情况下选择社会有益行动，经常导致有害结果。博弈论干预可将社会有益结果提高多达18%。

Conclusion: 研究揭示了前沿AI系统在多智能体环境中存在重大可靠性差距，GT-HarmBench为研究多智能体环境中的对齐问题提供了广泛的标准化测试平台。

Abstract: Frontier AI systems are increasingly capable and deployed in high-stakes multi-agent environments. However, existing AI safety benchmarks largely evaluate single agents, leaving multi-agent risks such as coordination failure and conflict poorly understood. We introduce GT-HarmBench, a benchmark of 2,009 high-stakes scenarios spanning game-theoretic structures such as the Prisoner's Dilemma, Stag Hunt and Chicken. Scenarios are drawn from realistic AI risk contexts in the MIT AI Risk Repository. Across 15 frontier models, agents choose socially beneficial actions in only 62% of cases, frequently leading to harmful outcomes. We measure sensitivity to game-theoretic prompt framing and ordering, and analyze reasoning patterns driving failures. We further show that game-theoretic interventions improve socially beneficial outcomes by up to 18%. Our results highlight substantial reliability gaps and provide a broad standardized testbed for studying alignment in multi-agent environments. The benchmark and code are available at https://github.com/causalNLP/gt-harmbench.

</details>


### [20] [Evolving Beyond Snapshots: Harmonizing Structure and Sequence via Entity State Tuning for Temporal Knowledge Graph Forecasting](https://arxiv.org/abs/2602.12389)
*Siyuan Li,Yunjia Wu,Yiyong Xiao,Pingyang Huang,Peize Li,Ruitong Liu,Yan Wen,Te Sun,Fangyi Pei*

Main category: cs.AI

TL;DR: 提出Entity State Tuning (EST)框架，通过维护持久化实体状态解决TKG预测中的长期依赖问题，显著提升多种基准模型性能


<details>
  <summary>Details</summary>
Motivation: 现有TKG预测方法大多是无状态的，每次从有限查询窗口重新计算实体表示，导致"情景性遗忘"和长期依赖快速衰减，需要解决状态持久化问题

Method: 提出EST框架：1)维护全局状态缓冲区；2)拓扑感知状态感知器将实体状态先验注入结构编码；3)统一时序上下文模块聚合状态增强事件；4)双轨演化机制平衡可塑性与稳定性更新全局状态

Result: 在多个基准测试中，EST框架能持续改进不同骨干模型性能，达到最先进水平，证明状态持久化对长时程TKG预测的重要性

Conclusion: EST框架通过赋予TKG预测器持久化且持续演化的实体状态，有效解决了长期依赖问题，为TKG预测提供了新的状态感知范式

Abstract: Temporal knowledge graph (TKG) forecasting requires predicting future facts by jointly modeling structural dependencies within each snapshot and temporal evolution across snapshots. However, most existing methods are stateless: they recompute entity representations at each timestamp from a limited query window, leading to episodic amnesia and rapid decay of long-term dependencies. To address this limitation, we propose Entity State Tuning (EST), an encoder-agnostic framework that endows TKG forecasters with persistent and continuously evolving entity states. EST maintains a global state buffer and progressively aligns structural evidence with sequential signals via a closed-loop design. Specifically, a topology-aware state perceiver first injects entity-state priors into structural encoding. Then, a unified temporal context module aggregates the state-enhanced events with a pluggable sequence backbone. Subsequently, a dual-track evolution mechanism writes the updated context back to the global entity state memory, balancing plasticity against stability. Experiments on multiple benchmarks show that EST consistently improves diverse backbones and achieves state-of-the-art performance, highlighting the importance of state persistence for long-horizon TKG forecasting. The code is published at https://github.com/yuanwuyuan9/Evolving-Beyond-Snapshots

</details>


### [21] [To Mix or To Merge: Toward Multi-Domain Reinforcement Learning for Large Language Models](https://arxiv.org/abs/2602.12566)
*Haoqing Wang,Xiang Long,Ziheng Li,Yilong Xu,Tingguang Li,Yehui Tang*

Main category: cs.AI

TL;DR: 该研究比较了多领域强化学习与可验证奖励（RLVR）的两种训练范式：混合多任务训练与分离训练后模型合并，发现在不同领域间RLVR存在较少相互干扰，推理密集型领域甚至表现出相互协同效应。


<details>
  <summary>Details</summary>
Motivation: 当前在多领域实现专家级性能的RLVR模型主要采用混合多任务训练或分离训练后模型合并两种范式，但缺乏对这些范式的详细比较分析。研究旨在系统比较这两种范式在不同领域（如数学、编程、科学、指令跟随）的表现。

Method: 选择多个常用高级任务作为目标领域，使用开源数据集设计广泛的定性和定量实验。从权重空间几何、模型预测行为和信息约束等角度分析相互增益的内部机制。

Result: 发现不同领域间的RLVR存在较少相互干扰，推理密集型领域表现出相互协同效应。通过权重空间几何、模型预测行为和信息约束等分析揭示了相互增益的内部机制。

Conclusion: 该研究为多领域RLVR训练范式选择提供了实证依据，表明混合多任务训练在某些情况下可能优于分离训练后合并的方法，特别是在推理密集型领域之间存在协同效应时。

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) plays a key role in stimulating the explicit reasoning capability of Large Language Models (LLMs). We can achieve expert-level performance in some specific domains via RLVR, such as coding or math. When a general multi-domain expert-level model is required, we need to carefully consider the collaboration of RLVR across different domains. The current state-of-the-art models mainly employ two different training paradigms for multi-domain RLVR: mixed multi-task RLVR and separate RLVR followed by model merging. However, most of the works did not provide a detailed comparison and analysis about these paradigms. To this end, we choose multiple commonly used high-level tasks (e.g., math, coding, science, and instruction following) as our target domains and design extensive qualitative and quantitative experiments using open-source datasets. We find the RLVR across domains exhibits few mutual interferences, and reasoning-intensive domains demonstrate mutually synergistic effects. Furthermore, we analyze the internal mechanisms of mutual gains from the perspectives of weight space geometry, model prediction behavior, and information constraints. This project is named as M2RL that means Mixed multi-task training or separate training followed by model Merging for Reinforcement Learning, and the homepage is at https://github.com/mosAI25/M2RL

</details>


### [22] [Can I Have Your Order? Monte-Carlo Tree Search for Slot Filling Ordering in Diffusion Language Models](https://arxiv.org/abs/2602.12586)
*Joshua Ong Jun Leang,Yu Zhao,Mihaela Cătălina Stoian,Wenda Li,Shay B. Cohen,Eleonora Giunchiglia*

Main category: cs.AI

TL;DR: McDiffuSE使用蒙特卡洛树搜索优化掩码扩散模型中的槽填充顺序，通过前瞻模拟评估部分完成情况，显著提升数学和代码推理任务的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的计划-填充解码方法在掩码扩散模型中性能对填充顺序高度敏感，导致输出方差大，需要系统性的顺序优化方法。

Method: 将槽选择建模为决策问题，使用蒙特卡洛树搜索优化填充顺序，通过前瞻模拟评估部分完成情况，系统探索生成顺序的组合空间。

Result: 相比自回归基线平均提升3.2%，相比基线计划-填充方法提升8.0%，在MBPP上提升19.5%，在MATH500上提升4.9%。

Conclusion: MCTS规划是提升掩码扩散模型生成质量的有效方法，需要更大的探索常数而非更多模拟来克服模型置信度偏差，非顺序生成对最大化性能至关重要。

Abstract: While plan-and-infill decoding in Masked Diffusion Models (MDMs) shows promise for mathematical and code reasoning, performance remains highly sensitive to slot infilling order, often yielding substantial output variance. We introduce McDiffuSE, a framework that formulates slot selection as decision making and optimises infilling orders through Monte Carlo Tree Search (MCTS). McDiffuSE uses look-ahead simulations to evaluate partial completions before commitment, systematically exploring the combinatorial space of generation orders. Experiments show an average improvement of 3.2% over autoregressive baselines and 8.0% over baseline plan-and-infill, with notable gains of 19.5% on MBPP and 4.9% on MATH500. Our analysis reveals that while McDiffuSE predominantly follows sequential ordering, incorporating non-sequential generation is essential for maximising performance. We observe that larger exploration constants, rather than increased simulations, are necessary to overcome model confidence biases and discover effective orderings. These findings establish MCTS-based planning as an effective approach for enhancing generation quality in MDMs.

</details>


### [23] [Think Fast and Slow: Step-Level Cognitive Depth Adaptation for LLM Agents](https://arxiv.org/abs/2602.12662)
*Ruihan Yang,Fanghua Ye,Xiang We,Ruoqing Zhao,Kang Luo,Xinbo Xu,Bo Zhao,Ruotian Ma,Shanyi Wang,Zhaopeng Tu,Xiaolong Li,Deqing Yang,Linus*

Main category: cs.AI

TL;DR: CogRouter是一个让LLM智能体动态调整认知深度的框架，根据任务需求在直觉响应到战略规划之间切换，显著提升长时程任务的效率和性能。


<details>
  <summary>Details</summary>
Motivation: 当前LLM智能体采用固定的认知模式：非思考模型立即响应，思考模型则统一进行深度推理。这种刚性对于长时程任务效率低下，因为不同步骤的认知需求差异很大，有些需要战略规划，有些只需常规执行。

Method: 基于ACT-R理论设计四个层次化认知级别（从直觉响应到战略规划），采用两阶段训练：认知感知监督微调（CoSFT）建立稳定的级别特定模式，认知感知策略优化（CoPO）通过置信度感知优势重加权进行步骤级信用分配。

Result: 在ALFWorld和ScienceWorld上的实验显示，CogRouter达到最先进性能且效率优越。使用Qwen2.5-7B模型，成功率达到82.3%，优于GPT-4o（+40.3%）、OpenAI-o3（+18.3%）和GRPO（+14.0%），同时减少62%的token使用。

Conclusion: CogRouter框架通过动态调整认知深度，使LLM智能体能够根据任务需求灵活选择适当的认知级别，在长时程决策任务中实现更高的效率和性能，核心洞察是适当的认知深度应最大化行动结果的置信度。

Abstract: Large language models (LLMs) are increasingly deployed as autonomous agents for multi-turn decision-making tasks. However, current agents typically rely on fixed cognitive patterns: non-thinking models generate immediate responses, while thinking models engage in deep reasoning uniformly. This rigidity is inefficient for long-horizon tasks, where cognitive demands vary significantly from step to step, with some requiring strategic planning and others only routine execution. In this paper, we introduce CogRouter, a framework that trains agents to dynamically adapt cognitive depth at each step. Grounded in ACT-R theory, we design four hierarchical cognitive levels ranging from instinctive responses to strategic planning. Our two-stage training approach includes Cognition-aware Supervised Fine-tuning (CoSFT) to instill stable level-specific patterns, and Cognition-aware Policy Optimization (CoPO) for step-level credit assignment via confidence-aware advantage reweighting. The key insight is that appropriate cognitive depth should maximize the confidence of the resulting action. Experiments on ALFWorld and ScienceWorld demonstrate that CogRouter achieves state-of-the-art performance with superior efficiency. With Qwen2.5-7B, it reaches an 82.3% success rate, outperforming GPT-4o (+40.3%), OpenAI-o3 (+18.3%), and GRPO (+14.0%), while using 62% fewer tokens.

</details>


### [24] [X-SYS: A Reference Architecture for Interactive Explanation Systems](https://arxiv.org/abs/2602.12748)
*Tobias Labarta,Nhi Hoang,Maximilian Dreyer,Jim Berend,Oleg Hein,Jackie Ma,Wojciech Samek,Sebastian Lapuschkin*

Main category: cs.AI

TL;DR: X-SYS：一个用于交互式解释系统的参考架构，将XAI视为信息系统问题，通过STAR质量属性和五组件分解连接用户界面与系统能力


<details>
  <summary>Details</summary>
Motivation: 尽管XAI研究提出了许多技术方法，但将可解释性部署为系统仍然具有挑战性。交互式解释系统需要合适的算法和系统能力，以在重复查询、模型和数据演化以及治理约束下保持解释可用性。作者认为，将XAI操作化需要将可解释性视为信息系统问题，其中用户交互需求引发特定的系统要求。

Method: 提出了X-SYS参考架构，围绕STAR四个质量属性（可扩展性、可追溯性、响应性和适应性）组织，并指定了五组件分解（XUI服务、解释服务、模型服务、数据服务、编排与治理）。该架构将交互模式映射到系统能力，以解耦用户界面演进和后端计算。通过SemanticLens系统（用于视觉语言模型的语义搜索和激活引导）实现了X-SYS。

Result: X-SYS提供了一个可重用的蓝图，支持在操作约束下进行端到端设计。SemanticLens展示了基于契约的服务边界如何实现独立演进，离线/在线分离确保响应性，持久状态管理支持可追溯性。

Conclusion: 这项工作为交互式解释系统提供了可重用的蓝图和具体实例，支持在操作约束下的端到端设计，帮助(X)AI研究人员、开发者和从业者连接交互式解释用户界面与系统能力。

Abstract: The explainable AI (XAI) research community has proposed numerous technical methods, yet deploying explainability as systems remains challenging: Interactive explanation systems require both suitable algorithms and system capabilities that maintain explanation usability across repeated queries, evolving models and data, and governance constraints. We argue that operationalizing XAI requires treating explainability as an information systems problem where user interaction demands induce specific system requirements. We introduce X-SYS, a reference architecture for interactive explanation systems, that guides (X)AI researchers, developers and practitioners in connecting interactive explanation user interfaces (XUI) with system capabilities. X-SYS organizes around four quality attributes named STAR (scalability, traceability, responsiveness, and adaptability), and specifies a five-component decomposition (XUI Services, Explanation Services, Model Services, Data Services, Orchestration and Governance). It maps interaction patterns to system capabilities to decouple user interface evolution from backend computation. We implement X-SYS through SemanticLens, a system for semantic search and activation steering in vision-language models. SemanticLens demonstrates how contract-based service boundaries enable independent evolution, offline/online separation ensures responsiveness, and persistent state management supports traceability. Together, this work provides a reusable blueprint and concrete instantiation for interactive explanation systems supporting end-to-end design under operational constraints.

</details>


### [25] [Evaluating Robustness of Reasoning Models on Parameterized Logical Problems](https://arxiv.org/abs/2602.12665)
*Naïm Es-sebbani,Esteban Marquer,Yakoub Salhi,Zied Bouraoui*

Main category: cs.AI

TL;DR: 该论文提出了一个用于评估LLM推理器的诊断性2-SAT基准，通过参数化公式生成器分离表面难度与结构现象，揭示LLM在特定结构干预下的脆弱性。


<details>
  <summary>Details</summary>
Motivation: 传统SAT基准常将表面难度（长度、措辞、子句顺序）与决定可满足性的结构现象混为一谈，无法准确评估LLM推理器的真实能力。需要构建能隔离不同推理能力的诊断性基准。

Method: 设计基于参数化结构化2-CNF公式的生成器，通过五个维度控制：矛盾循环UNSAT核心、自由变量比例、植入骨干、延迟桥接子句、对称/重复变体。评估LLM在决策准确性和赋值有效性上的表现，并测试语义保留扰动下的鲁棒性。

Result: 实验显示LLM在表面统计特征固定但结构干预下出现明显的性能转变，揭示了在聚合SAT准确率中不可见的脆弱性区域。模型对特定结构变化表现出敏感性，即使表面难度保持不变。

Conclusion: 结构化诊断基准能更精确地揭示LLM推理器的能力边界和失败模式，为评估和改进逻辑推理系统提供了更有效的工具，超越了传统SAT基准的表面评估。

Abstract: Logic provides a controlled testbed for evaluating LLM-based reasoners, yet standard SAT-style benchmarks often conflate surface difficulty (length, wording, clause order) with the structural phenomena that actually determine satisfiability. We introduce a diagnostic benchmark for 2-SAT built from parameterized families of structured 2--CNF formulas, where satisfiability is characterized by the implication graph and can be tuned along interpretable axes. Our generators isolate distinct competencies and failure modes: (i) contradiction-cycle UNSAT cores with controllable size and imbalance, (ii) SAT instances with a prescribed fraction of free variables to control solution multiplicity, (iii) planted backbones that modulate propagation, (iv) late bridge clauses that couple otherwise monotone regions to probe sensitivity to ordering and revision, and (v) symmetry/duplication variants that test abstraction under renaming and redundant structure. We evaluate LLM-based reasoners on decision accuracy and assignment validity, and quantify robustness under semantics-preserving perturbations such as clause reordering, filler clauses, and variable renaming. Across models, we observe sharp performance transitions under targeted structural interventions even when surface statistics are held fixed, revealing brittleness regimes that are invisible to aggregate SAT accuracy.

</details>


### [26] [Information-theoretic analysis of world models in optimal reward maximizers](https://arxiv.org/abs/2602.12963)
*Alfred Harwood,Jose Faustino,Alex Altair*

Main category: cs.AI

TL;DR: 该论文证明最优策略包含关于环境的确切信息量：在n个状态、m个动作的受控马尔可夫过程中，最优策略与环境之间的互信息为n log m比特


<details>
  <summary>Details</summary>
Motivation: 研究AI领域一个重要问题：成功行为在多大程度上需要世界的内部表示。量化最优策略提供的关于底层环境的信息量，为"隐式世界模型"提供信息论下界

Method: 考虑具有n个状态和m个动作的受控马尔可夫过程，假设在可能的转移动态空间上具有均匀先验。证明观察确定性最优策略（针对任何非常数奖励函数）能传递确切的环境信息量

Result: 证明最优策略与环境之间的互信息为n log m比特。该界限适用于广泛的优化目标，包括有限时域、无限时域折扣和时间平均奖励最大化

Conclusion: 为最优性所需的"隐式世界模型"提供了精确的信息论下界，表明最优策略必然包含关于环境的特定信息量

Abstract: An important question in the field of AI is the extent to which successful behaviour requires an internal representation of the world. In this work, we quantify the amount of information an optimal policy provides about the underlying environment. We consider a Controlled Markov Process (CMP) with $n$ states and $m$ actions, assuming a uniform prior over the space of possible transition dynamics. We prove that observing a deterministic policy that is optimal for any non-constant reward function then conveys exactly $n \log m$ bits of information about the environment. Specifically, we show that the mutual information between the environment and the optimal policy is $n \log m$ bits. This bound holds across a broad class of objectives, including finite-horizon, infinite-horizon discounted, and time-averaged reward maximization. These findings provide a precise information-theoretic lower bound on the "implicit world model'' necessary for optimality.

</details>


### [27] [Consistency of Large Reasoning Models Under Multi-Turn Attacks](https://arxiv.org/abs/2602.13093)
*Yubo Li,Ramayya Krishnan,Rema Padman*

Main category: cs.AI

TL;DR: 该研究评估了9个前沿推理模型在对抗攻击下的鲁棒性，发现推理能力提供有意义但不完整的鲁棒性，所有模型都表现出不同的脆弱性特征，并识别出5种主要失败模式。


<details>
  <summary>Details</summary>
Motivation: 尽管具有推理能力的大型模型在复杂任务上取得了最先进的性能，但它们在多轮对抗压力下的鲁棒性尚未得到充分探索。研究者希望评估推理模型在对抗攻击下的表现，了解推理能力是否自动带来对抗鲁棒性。

Method: 研究评估了9个前沿推理模型在对抗攻击下的表现，通过轨迹分析识别失败模式，并测试了置信度感知响应生成（CARG）防御方法对推理模型的有效性。

Result: 研究发现：1）推理提供有意义但不完整的鲁棒性，大多数推理模型显著优于指令调优基线；2）所有模型都表现出不同的脆弱性特征，误导性建议普遍有效，社会压力具有模型特异性；3）识别出5种失败模式（自我怀疑、社会从众、建议劫持、情感易感性、推理疲劳），前两种占失败的50%；4）CARG防御对推理模型失效，因为扩展推理痕迹导致过度自信，随机置信度嵌入反而优于针对性提取。

Conclusion: 推理能力不会自动带来对抗鲁棒性，基于置信度的防御需要对推理模型进行根本性重新设计。研究强调了在评估推理模型时需要超越标准基准，考虑对抗鲁棒性。

Abstract: Large reasoning models with reasoning capabilities achieve state-of-the-art performance on complex tasks, but their robustness under multi-turn adversarial pressure remains underexplored. We evaluate nine frontier reasoning models under adversarial attacks. Our findings reveal that reasoning confers meaningful but incomplete robustness: most reasoning models studied significantly outperform instruction-tuned baselines, yet all exhibit distinct vulnerability profiles, with misleading suggestions universally effective and social pressure showing model-specific efficacy. Through trajectory analysis, we identify five failure modes (Self-Doubt, Social Conformity, Suggestion Hijacking, Emotional Susceptibility, and Reasoning Fatigue) with the first two accounting for 50% of failures. We further demonstrate that Confidence-Aware Response Generation (CARG), effective for standard LLMs, fails for reasoning models due to overconfidence induced by extended reasoning traces; counterintuitively, random confidence embedding outperforms targeted extraction. Our results highlight that reasoning capabilities do not automatically confer adversarial robustness and that confidence-based defenses require fundamental redesign for reasoning models.

</details>


### [28] [Constrained Assumption-Based Argumentation Frameworks](https://arxiv.org/abs/2602.13135)
*Emanuele De Angelis,Fabio Fioravanti,Maria Chiara Meo,Alberto Pettorossi,Maurizio Proietti,Francesca Toni*

Main category: cs.AI

TL;DR: 提出约束ABA框架，允许包含约束变量的非地面参数和攻击，扩展了传统ABA的表达能力


<details>
  <summary>Details</summary>
Motivation: 传统ABA框架只能处理基于命题原子的地面（无变量）参数和攻击，表达能力有限，无法处理包含变量的非地面论证

Method: 提出约束ABA框架，允许参数和攻击中包含约束变量，定义非地面语义，包括各种非地面攻击概念

Result: 新语义保守地推广了标准ABA语义，保持了向后兼容性

Conclusion: 约束ABA框架成功扩展了传统ABA的表达能力，使其能够处理更复杂的非地面论证场景

Abstract: Assumption-based Argumentation (ABA) is a well-established form of structured argumentation. ABA frameworks with an underlying atomic language are widely studied, but their applicability is limited by a representational restriction to ground (variable-free) arguments and attacks built from propositional atoms. In this paper, we lift this restriction and propose a novel notion of constrained ABA (CABA), whose components, as well as arguments built from them, may include constrained variables, ranging over possibly infinite domains. We define non-ground semantics for CABA, in terms of various notions of non-ground attacks. We show that the new semantics conservatively generalise standard ABA semantics.

</details>


### [29] [Optimal Take-off under Fuzzy Clearances](https://arxiv.org/abs/2602.13166)
*Hugo Henry,Arthur Tsai,Kelly Cohen*

Main category: cs.AI

TL;DR: 提出了一种结合最优控制和模糊规则系统的混合障碍物规避架构，用于无人机自适应约束处理，但发现软件兼容性问题导致约束无法有效执行。


<details>
  <summary>Details</summary>
Motivation: 经典最优控制在不确定性下的局限性，以及航空安全关键系统需要可解释的决策制定，促使开发能够自适应处理约束的混合架构。

Method: 设计了三阶段Takagi-Sugeno-Kang模糊层，基于FAA和EASA的监管分离最小值和适航指南来调制约束半径、紧急级别和激活决策。这些模糊推导的间隙作为软约束纳入最优控制问题，使用FALCON工具箱和IPOPT求解。

Result: 概念验证实现显示，在单线程MATLAB环境中，每次迭代计算时间为2-3秒，表明近实时应用的可行性。但发现FALCON和IPOPT最新版本存在软件不兼容问题，拉格朗日惩罚项始终为零，导致约束无法正确执行。

Conclusion: 该混合架构在理论上可行，但受到软件兼容性问题的限制。未来工作包括验证软件回归问题、使用进化方法优化模糊隶属函数，以及扩展到更高保真度飞机模型和随机障碍物环境。

Abstract: This paper presents a hybrid obstacle avoidance architecture that integrates Optimal Control under clearance with a Fuzzy Rule Based System (FRBS) to enable adaptive constraint handling for unmanned aircraft. Motivated by the limitations of classical optimal control under uncertainty and the need for interpretable decision making in safety critical aviation systems, we design a three stage Takagi Sugeno Kang fuzzy layer that modulates constraint radii, urgency levels, and activation decisions based on regulatory separation minima and airworthiness guidelines from FAA and EASA. These fuzzy-derived clearances are then incorporated as soft constraints into an optimal control problem solved using the FALCON toolbox and IPOPT. The framework aims to reduce unnecessary recomputations by selectively activating obstacle avoidance updates while maintaining compliance with aviation procedures. A proof of concept implementation using a simplified aircraft model demonstrates that the approach can generate optimal trajectories with computation times of 2,3 seconds per iteration in a single threaded MATLAB environment, suggesting feasibility for near real time applications. However, our experiments revealed a critical software incompatibility in the latest versions of FALCON and IPOPT, in which the Lagrangian penalty term remained identically zero, preventing proper constraint enforcement. This behavior was consistent across scenarios and indicates a solver toolbox regression rather than a modeling flaw. Future work includes validating this effect by reverting to earlier software versions, optimizing the fuzzy membership functions using evolutionary methods, and extending the system to higher fidelity aircraft models and stochastic obstacle environments.

</details>
