<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 9]
- [cs.CR](#cs.CR) [Total: 24]
- [cs.AI](#cs.AI) [Total: 13]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [OS-Harm: A Benchmark for Measuring Safety of Computer Use Agents](https://arxiv.org/abs/2506.14866)
*Thomas Kuntz,Agatha Duzan,Hao Zhao,Francesco Croce,Zico Kolter,Nicolas Flammarion,Maksym Andriushchenko*

Main category: cs.SE

TL;DR: OS-Harm是一个新的基准测试，用于评估基于LLM的计算机使用代理的安全性，覆盖了用户滥用、提示注入攻击和模型不当行为三类危害。


<details>
  <summary>Details</summary>
Motivation: 随着计算机使用代理的普及，其安全性问题被忽视，评估其潜在危害对广泛采用至关重要。

Method: 在OSWorld环境中构建OS-Harm基准，包含150个任务，测试代理在多种安全违规行为中的表现，并开发自动化评估工具。

Result: 评估显示，所有测试模型在用户滥用查询中表现顺从，对静态提示注入攻击较脆弱，偶尔会执行不安全操作。

Conclusion: OS-Harm为计算机使用代理的安全性提供了标准化评估工具，揭示了现有模型的安全漏洞。

Abstract: Computer use agents are LLM-based agents that can directly interact with a graphical user interface, by processing screenshots or accessibility trees. While these systems are gaining popularity, their safety has been largely overlooked, despite the fact that evaluating and understanding their potential for harmful behavior is essential for widespread adoption. To address this gap, we introduce OS-Harm, a new benchmark for measuring safety of computer use agents. OS-Harm is built on top of the OSWorld environment and aims to test models across three categories of harm: deliberate user misuse, prompt injection attacks, and model misbehavior. To cover these cases, we create 150 tasks that span several types of safety violations (harassment, copyright infringement, disinformation, data exfiltration, etc.) and require the agent to interact with a variety of OS applications (email client, code editor, browser, etc.). Moreover, we propose an automated judge to evaluate both accuracy and safety of agents that achieves high agreement with human annotations (0.76 and 0.79 F1 score). We evaluate computer use agents based on a range of frontier models - such as o4-mini, Claude 3.7 Sonnet, Gemini 2.5 Pro - and provide insights into their safety. In particular, all models tend to directly comply with many deliberate misuse queries, are relatively vulnerable to static prompt injections, and occasionally perform unsafe actions. The OS-Harm benchmark is available at https://github.com/tml-epfl/os-harm.

</details>


### [2] [An Empirical Study of Bugs in Data Visualization Libraries](https://arxiv.org/abs/2506.15084)
*Weiqi Lu,Yongqiang Tian,Xiaohan Zhong,Haoyang Ma,Zhenyang Xu,Shing-Chi Cheung,Chengnian Sun*

Main category: cs.SE

TL;DR: 本文首次全面分析了数据可视化库中的错误，研究了564个错误，发现错误图形计算是主要原因，并探索了视觉语言模型在检测错误中的可行性。


<details>
  <summary>Details</summary>
Motivation: 数据可视化库的准确性对用户体验和决策至关重要，但现有研究对其错误特性了解不足。

Method: 研究分析了五个广泛使用的库中的564个错误，系统分类了症状和根源，并探索了视觉语言模型的应用。

Result: 发现错误图形计算是主要根源，视觉语言模型检测效果在29%至57%之间。

Conclusion: 研究为数据可视化库的自动化测试提供了新方向，并指出了视觉语言模型的潜力与局限。

Abstract: Data visualization (DataViz) libraries play a crucial role in presentation, data analysis, and application development, underscoring the importance of their accuracy in transforming data into visual representations. Incorrect visualizations can adversely impact user experience, distort information conveyance, and influence user perception and decision-making processes. Visual bugs in these libraries can be particularly insidious as they may not cause obvious errors like crashes, but instead mislead users of the underlying data graphically, resulting in wrong decision making. Consequently, a good understanding of the unique characteristics of bugs in DataViz libraries is essential for researchers and developers to detect and fix bugs in DataViz libraries.
  This study presents the first comprehensive analysis of bugs in DataViz libraries, examining 564 bugs collected from five widely-used libraries. Our study systematically analyzes their symptoms and root causes, and provides a detailed taxonomy. We found that incorrect/inaccurate plots are pervasive in DataViz libraries and incorrect graphic computation is the major root cause, which necessitates further automated testing methods for DataViz libraries. Moreover, we identified eight key steps to trigger such bugs and two test oracles specific to DataViz libraries, which may inspire future research in designing effective automated testing techniques. Furthermore, with the recent advancements in Vision Language Models (VLMs), we explored the feasibility of applying these models to detect incorrect/inaccurate plots. The results show that the effectiveness of VLMs in bug detection varies from 29% to 57%, depending on the prompts, and adding more information in prompts does not necessarily increase the effectiveness. More findings can be found in our manuscript.

</details>


### [3] [Program Feature-based Fuzzing Benchmarking](https://arxiv.org/abs/2506.15088)
*Miao Miao*

Main category: cs.SE

TL;DR: 本文提出了一种新的基准测试方法，通过可配置的细粒度程序特征来评估模糊测试的有效性，填补了传统评估方法的不足。


<details>
  <summary>Details</summary>
Motivation: 传统模糊测试评估通常关注整体性能，而忽略了细粒度程序特征对测试效果的影响。本文旨在填补这一空白。

Method: 通过分析25项灰盒模糊测试研究，提取了7个与控制和数据流相关的程序特征，生成了包含153个程序的基准测试集，并评估了11种流行模糊测试工具。

Result: 结果显示，模糊测试工具的性能受程序特征及其强度影响显著，强调了在评估中考虑程序特性的重要性。

Conclusion: 细粒度程序特征对模糊测试效果有显著影响，未来评估应更注重程序特性的设计。

Abstract: Fuzzing is a powerful software testing technique renowned for its effectiveness in identifying software vulnerabilities. Traditional fuzzing evaluations typically focus on overall fuzzer performance across a set of target programs, yet few benchmarks consider how fine-grained program features influence fuzzing effectiveness. To bridge this gap, we introduce a novel benchmark designed to generate programs with configurable, fine-grained program features to enhance fuzzing evaluations. We reviewed 25 recent grey-box fuzzing studies, extracting 7 program features related to control-flow and data-flow that can impact fuzzer performance. Using these features, we generated a benchmark consisting of 153 programs controlled by 10 fine-grained configurable parameters. We evaluated 11 popular fuzzers using this benchmark. The results indicate that fuzzer performance varies significantly based on the program features and their strengths, highlighting the importance of incorporating program characteristics into fuzzing evaluations.

</details>


### [4] [Enhancement Report Approval Prediction: A Comparative Study of Large Language Models](https://arxiv.org/abs/2506.15098)
*Haosheng Zuo,Feifei Niu,Chuanyi Li*

Main category: cs.SE

TL;DR: 该论文研究了利用大语言模型（LLM）自动预测增强报告（ERs）的批准情况，发现LLM在准确性和召回率上优于传统方法，并提出了改进方向。


<details>
  <summary>Details</summary>
Motivation: 手动处理增强报告效率低下且易丢失有价值建议，因此研究如何利用LLM自动化这一过程。

Method: 系统评估了18种LLM变体（如BERT、GPT-3.5等）与传统方法（CNN/LSTM等）的性能对比，并引入创建者档案和LoRA微调技术。

Result: LLM显著提升预测性能，LoRA微调的Llama 3.1 8B Instruct达到79%准确率，优于传统方法5%。

Conclusion: LLM是增强报告预测的优越解决方案，未来研究可针对其不足进一步优化。

Abstract: Enhancement reports (ERs) serve as a critical communication channel between users and developers, capturing valuable suggestions for software improvement. However, manually processing these reports is resource-intensive, leading to delays and potential loss of valuable insights. To address this challenge, enhancement report approval prediction (ERAP) has emerged as a research focus, leveraging machine learning techniques to automate decision-making. While traditional approaches have employed feature-based classifiers and deep learning models, recent advancements in large language models (LLM) present new opportunities for enhancing prediction accuracy. This study systematically evaluates 18 LLM variants (including BERT, RoBERTa, DeBERTa-v3, ELECTRA, and XLNet for encoder models; GPT-3.5-turbo, GPT-4o-mini, Llama 3.1 8B, Llama 3.1 8B Instruct and DeepSeek-V3 for decoder models) against traditional methods (CNN/LSTM-BERT/GloVe). Our experiments reveal two key insights: (1) Incorporating creator profiles increases unfine-tuned decoder-only models' overall accuracy by 10.8 percent though it may introduce bias; (2) LoRA fine-tuned Llama 3.1 8B Instruct further improve performance, reaching 79 percent accuracy and significantly enhancing recall for approved reports (76.1 percent vs. LSTM-GLOVE's 64.1 percent), outperforming traditional methods by 5 percent under strict chronological evaluation and effectively addressing class imbalance issues. These findings establish LLM as a superior solution for ERAP, demonstrating their potential to streamline software maintenance workflows and improve decision-making in real-world development environments. We also investigated and summarized the ER cases where the large models underperformed, providing valuable directions for future research.

</details>


### [5] [Towards Bug-Free Distributed Go Programs](https://arxiv.org/abs/2506.15135)
*Zhengqun Koo*

Main category: cs.SE

TL;DR: 本文提出了一种验证框架，用于证明使用Go语言子集的分布式程序中不存在通信竞争。


<details>
  <summary>Details</summary>
Motivation: 分布式系统中的程序员需要处理并发问题以避免竞争，但并发推理困难，通信竞争会导致接收者收到错误消息或未收到消息。

Method: 通过静态分析程序的执行顺序，扩展了happens-before关系以涵盖缓冲和非缓冲通道。

Result: 该框架能够证明分布式程序中通信竞争的缺失。

Conclusion: 该验证框架为分布式程序提供了一种静态检测通信竞争的方法，提高了程序的可靠性。

Abstract: Programmers of distributed systems need to reason about concurrency to avoid races. However, reasoning about concurrency is difficult, and unexpected races show up as bugs. Data race detection in shared memory systems is well-studied (dynamic data race detection [13], behavioral types [15], dynamic race detection [31]). Similar to how a data race consists of reads and writes not related by happens-before at a shared memory location, a communication race consists of receives and sends not related by happens-before on a shared channel. Communication races are problematic: a receiver expects a specific message from a specific sender, but with a communication race, the receiver can receive a message meant for another receiver, or not receive anything at all. In this work, we describe a verification framework that can prove the absence of communication races for distributed programs that use a subset of the Go programming language, where synchronization is mainly achieved via message passing. We statically reason about how a distributed program executes, using a happens-before order, extended to buffered and unbuffered channels.

</details>


### [6] [Advanced approach for Agile/Scrum Process: RetroAI++](https://arxiv.org/abs/2506.15172)
*Maria Spichkova,Kevin Iwan,Madeleine Zwart,Hina Lee,Yuwon Yoon,Xiaohan Qin*

Main category: cs.SE

TL;DR: RetroAI++是一个基于智能技术的原型工具，旨在自动化和优化敏捷/Scrum开发中的冲刺规划和回顾分析。


<details>
  <summary>Details</summary>
Motivation: 支持软件开发者在敏捷/Scrum开发中的冲刺规划和回顾分析活动，提升项目管理效率。

Method: 利用AI技术开发原型工具RetroAI++，自动化和优化冲刺规划、开发和回顾阶段的流程。

Result: 提供智能化的冲刺组织建议和有意义的回顾反思洞察。

Conclusion: RetroAI++通过AI技术有效支持敏捷/Scrum开发中的关键项目管理活动。

Abstract: In Agile/Scrum software development, sprint planning and retrospective analysis are the key elements of project management. The aim of our work is to support software developers in these activities. In this paper, we present our prototype tool RetroAI++, based on emerging intelligent technologies. In our RetroAI++ prototype, we aim to automate and refine the practical application of Agile/Scrum processes within Sprint Planning and Retrospectives. Leveraging AI insights, our prototype aims to automate and refine the many processes involved in the Sprint Planning, Development and Retrospective stages of Agile/Scrum development projects, offering intelligent suggestions for sprint organisation as well as meaningful insights for retrospective reflection.

</details>


### [7] [Large Language Models for Unit Testing: A Systematic Literature Review](https://arxiv.org/abs/2506.15227)
*Quanjun Zhang,Chunrong Fang,Siqi Gu,Ye Shang,Zhenyu Chen,Liang Xiao*

Main category: cs.SE

TL;DR: 本文对截至2025年3月的大型语言模型（LLMs）在单元测试中的应用进行了首次系统性文献综述，分析了相关论文，总结了现有成果、挑战和未来方向。


<details>
  <summary>Details</summary>
Motivation: 随着LLMs的快速发展，其在单元测试自动化中的应用日益增多，但缺乏系统性总结，研究者难以全面了解该领域的进展和挑战。

Method: 通过分析相关论文，从单元测试和LLMs的角度分类现有任务（如测试生成和预言生成），并讨论模型使用、适应策略和混合方法等关键方面。

Result: 总结了LLMs在单元测试中的应用现状，提出了未解决的关键挑战和未来研究方向。

Conclusion: 本文为单元测试社区提供了系统性概述，帮助研究者全面了解成果并推动未来研究。

Abstract: Unit testing is a fundamental practice in modern software engineering, with the aim of ensuring the correctness, maintainability, and reliability of individual software components. Very recently, with the advances in Large Language Models (LLMs), a rapidly growing body of research has leveraged LLMs to automate various unit testing tasks, demonstrating remarkable performance and significantly reducing manual effort. However, due to ongoing explorations in the LLM-based unit testing field, it is challenging for researchers to understand existing achievements, open challenges, and future opportunities. This paper presents the first systematic literature review on the application of LLMs in unit testing until March 2025. We analyze \numpaper{} relevant papers from the perspectives of both unit testing and LLMs. We first categorize existing unit testing tasks that benefit from LLMs, e.g., test generation and oracle generation. We then discuss several critical aspects of integrating LLMs into unit testing research, including model usage, adaptation strategies, and hybrid approaches. We further summarize key challenges that remain unresolved and outline promising directions to guide future research in this area. Overall, our paper provides a systematic overview of the research landscape to the unit testing community, helping researchers gain a comprehensive understanding of achievements and promote future research. Our artifacts are publicly available at the GitHub repository: https://github.com/iSEngLab/AwesomeLLM4UT.

</details>


### [8] [Uncovering Intention through LLM-Driven Code Snippet Description Generation](https://arxiv.org/abs/2506.15453)
*Yusuf Sulistyo Nugroho,Farah Danisha Salam,Brittany Reid,Raula Gaikovina Kula,Kazumasa Shimari,Kenichi Matsumoto*

Main category: cs.SE

TL;DR: 论文研究了开发者常用的代码片段描述类型，并评估了Llama模型在生成描述方面的表现。研究发现大多数描述基于示例，LLM能准确识别此类描述，但生成的描述仍有改进空间。


<details>
  <summary>Details</summary>
Motivation: 研究目的是探索开发者常用的代码片段描述类型，并评估大型语言模型（如Llama）在生成描述方面的能力，以提升代码文档的质量。

Method: 使用NPM代码片段数据集（185,412个包，1,024,579个代码片段），从中选取400个样本进行人工分类和LLM评估。

Result: 人工分类发现55.5%的描述为示例型；LLM准确识别了79.75%的示例描述；生成的描述平均相似度为0.7173，表明相关性但需改进。

Conclusion: 代码片段的文档意图因任务而异，LLM在描述生成方面表现良好但仍有优化空间。

Abstract: Documenting code snippets is essential to pinpoint key areas where both developers and users should pay attention. Examples include usage examples and other Application Programming Interfaces (APIs), which are especially important for third-party libraries. With the rise of Large Language Models (LLMs), the key goal is to investigate the kinds of description developers commonly use and evaluate how well an LLM, in this case Llama, can support description generation. We use NPM Code Snippets, consisting of 185,412 packages with 1,024,579 code snippets. From there, we use 400 code snippets (and their descriptions) as samples. First, our manual classification found that the majority of original descriptions (55.5%) highlight example-based usage. This finding emphasizes the importance of clear documentation, as some descriptions lacked sufficient detail to convey intent. Second, the LLM correctly identified the majority of original descriptions as "Example" (79.75%), which is identical to our manual finding, showing a propensity for generalization. Third, compared to the originals, the produced description had an average similarity score of 0.7173, suggesting relevance but room for improvement. Scores below 0.9 indicate some irrelevance. Our results show that depending on the task of the code snippet, the intention of the document may differ from being instructions for usage, installations, or descriptive learning examples for any user of a library.

</details>


### [9] [cAST: Enhancing Code Retrieval-Augmented Generation with Structural Chunking via Abstract Syntax Tree](https://arxiv.org/abs/2506.15655)
*Yilin Zhang,Xinran Zhao,Zora Zhiruo Wang,Chenyang Yang,Jiayi Wei,Tongshuang Wu*

Main category: cs.SE

TL;DR: 论文提出了一种基于抽象语法树（AST）的代码分块方法（AST-based chunking），以解决传统行分块方法破坏语义结构的问题，显著提升了代码生成任务的表现。


<details>
  <summary>Details</summary>
Motivation: 现有基于行的分块方法（line-based chunking）会破坏代码的语义结构（如拆分函数或合并无关代码），从而降低生成质量。

Method: 提出了一种基于AST的结构感知分块方法，递归地将大AST节点分解为小块，并在大小限制内合并兄弟节点，生成语义连贯的代码单元。

Result: 该方法在多种代码生成任务中表现优异，如在RepoEval检索任务中Recall@5提升4.3点，在SWE-bench生成任务中Pass@1提升2.67点。

Conclusion: 结构感知的分块方法对提升检索增强的代码智能至关重要。

Abstract: Retrieval-Augmented Generation (RAG) has become essential for large-scale code generation, grounding predictions in external code corpora to improve actuality. However, a critical yet underexplored aspect of RAG pipelines is chunking -- the process of dividing documents into retrievable units. Existing line-based chunking heuristics often break semantic structures, splitting functions or merging unrelated code, which can degrade generation quality. We propose chunking via Abstract Syntax Trees (\ourwork), a structure-aware method that recursively breaks large AST nodes into smaller chunks and merges sibling nodes while respecting size limits. This approach generates self-contained, semantically coherent units across programming languages and tasks, improving performance on diverse code generation tasks, e.g., boosting Recall@5 by 4.3 points on RepoEval retrieval and Pass@1 by 2.67 points on SWE-bench generation. Our work highlights the importance of structure-aware chunking for scaling retrieval-enhanced code intelligence.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [10] [Winter Soldier: Backdooring Language Models at Pre-Training with Indirect Data Poisoning](https://arxiv.org/abs/2506.14913)
*Wassim Bouaziz,Mathurin Videau,Nicolas Usunier,El-Mahdi El-Mhamdi*

Main category: cs.CR

TL;DR: 论文提出了一种间接数据投毒方法，通过梯度优化提示调优，使语言模型学习秘密序列，从而保护数据集并追踪其使用情况。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖训练数据的记忆，而语言模型提供者试图限制这种记忆。本文旨在探索一种不依赖记忆的间接数据投毒方法。

Method: 使用基于梯度的优化提示调优技术，使模型学习训练数据中不存在的秘密序列（秘密提示和秘密响应）。

Result: 实验证明，仅需少于0.005%的投毒标记即可让语言模型学习秘密，并以极高置信度（p < 10^{-55}）检测到其使用，且不影响模型性能。

Conclusion: 间接数据投毒是一种可行且有效的方法，能够在不依赖训练数据记忆的情况下保护数据集并追踪其使用。

Abstract: The pre-training of large language models (LLMs) relies on massive text datasets sourced from diverse and difficult-to-curate origins. Although membership inference attacks and hidden canaries have been explored to trace data usage, such methods rely on memorization of training data, which LM providers try to limit. In this work, we demonstrate that indirect data poisoning (where the targeted behavior is absent from training data) is not only feasible but also allow to effectively protect a dataset and trace its use. Using gradient-based optimization prompt-tuning, we make a model learn arbitrary secret sequences: secret responses to secret prompts that are absent from the training corpus. We validate our approach on language models pre-trained from scratch and show that less than 0.005% of poisoned tokens are sufficient to covertly make a LM learn a secret and detect it with extremely high confidence ($p < 10^{-55}$) with a theoretically certifiable scheme. Crucially, this occurs without performance degradation (on LM benchmarks) and despite secrets never appearing in the training set.

</details>


### [11] [Fair Data Exchange with Constant-Time Proofs](https://arxiv.org/abs/2506.14944)
*Majid Khabbazian*

Main category: cs.CR

TL;DR: FDE协议通过将文件视为Reed-Solomon码字并加密扩展向量，将证明和验证成本降至接近常数，同时保持公平性。


<details>
  <summary>Details</summary>
Motivation: 解决FDE协议中证明和验证时间随文件长度线性增长的问题。

Method: 将文件视为Reed-Solomon码字，扩展为低速率码，加密后仅验证随机子集的正确性。

Result: 协议将成本降至接近常数，保持公平性，并增加可调通信冗余。

Conclusion: 协议通过优化和补丁实现了高效的链下运行，仅在必要时进行链上交易。

Abstract: The Fair Data Exchange (FDE) protocol introduced at CCS 2024 offers atomic pay-per-file transfers with constant-size proofs, but its prover and verifier runtimes still scale linearly with the file length n. We collapse these costs to essentially constant by viewing the file as a rate-1 Reed-Solomon (RS) codeword, extending it to a lower-rate RS code with constant redundancy, encrypting this extended vector, and then proving correctness for only a small random subset of the resulting ciphertexts; RS decoding repairs any corrupted symbols with negligible failure probability. Our protocol preserves full client- and server-fairness, and adds only a tunable communication redundancy overhead.
  Finally, we patch the elliptic-curve mismatch in the Bitcoin instantiation of FDE with a compact zk-SNARK, enabling the entire exchange to run off-chain and falling back to just two on-chain transactions when channels are unavailable.

</details>


### [12] [Narrowing the Gap between TEEs Threat Model and Deployment Strategies](https://arxiv.org/abs/2506.14964)
*Filip Rezabek,Jonathan Passerat-Palmbach,Moe Mahhouk,Frieder Erdmann,Andrew Miller*

Main category: cs.CR

TL;DR: 论文讨论了机密虚拟机（CVM）在物理层面保护和侧信道攻击方面的不足，提出通过扩展TEE认证绑定CVM与提供商来解决威胁模型不匹配的问题。


<details>
  <summary>Details</summary>
Motivation: 当前CVM的威胁模型未涵盖物理层面攻击，依赖可信云提供商，但TEE认证未提供运营商信息，用户无法准确评估物理攻击风险。

Method: 提出利用受保护平台标识符（PPID）等扩展TEE认证，绑定CVM与提供商，并讨论具体实现挑战。

Result: 指出当前TEE认证的局限性，强调需解决认证验证、迁移便利性和去中心化应用构建问题。

Conclusion: 需通过强化和扩展TEE认证来提升CVM的安全性，以促进其广泛应用。

Abstract: Confidential Virtual Machines (CVMs) provide isolation guarantees for data in use, but their threat model does not include physical level protection and side-channel attacks. Therefore, current deployments rely on trusted cloud providers to host the CVMs' underlying infrastructure. However, TEE attestations do not provide information about the operator hosting a CVM. Without knowing whether a Trusted Execution Environment (TEE) runs within a provider's infrastructure, a user cannot accurately assess the risks of physical attacks. We observe a misalignment in the threat model where the workloads are protected against other tenants but do not offer end-to-end security assurances to external users without relying on cloud providers. The attestation should be extended to bind the CVM with the provider. A possible solution can rely on the Protected Platform Identifier (PPID), a unique CPU identifier. However, the implementation details of various TEE manufacturers, attestation flows, and providers vary. This makes verification of attestations, ease of migration, and building applications without relying on a trusted party challenging, highlighting a key limitation that must be addressed for the adoption of CVMs. We discuss two points focusing on hardening and extensions of TEEs' attestation.

</details>


### [13] [Private Continual Counting of Unbounded Streams](https://arxiv.org/abs/2506.15018)
*Ben Jacobsen,Kassem Fawaz*

Main category: cs.CR

TL;DR: 本文研究了无界设置下的差分隐私连续计数问题，提出了一种基于对数扰动的新型矩阵分解方法，解决了现有算法在参数调整和误差平滑性上的不足。


<details>
  <summary>Details</summary>
Motivation: 现有基于矩阵机制的算法在无界设置下无法直接应用，因为其隐私保证依赖于预先知道输入规模。使用常见的“倍增技巧”会导致次优和非平滑误差。

Method: 引入基于对数扰动的新型矩阵分解方法，利用函数$\frac{1}{\sqrt{1-z}}$的变体，设计了一种具有平滑误差的算法。

Result: 算法在任意$t \leq n$时能以$O(\log^{2+2α}(t))$方差估计前$t$个数据点的和，空间复杂度为$O(t)$，每轮摊销时间为$O(\log t)$。

Conclusion: 该算法在理论和实验上均表现优异，方差和性能与现有最优算法相当，适用于无界设置下的差分隐私连续计数问题。

Abstract: We study the problem of differentially private continual counting in the unbounded setting where the input size $n$ is not known in advance. Current state-of-the-art algorithms based on optimal instantiations of the matrix mechanism cannot be directly applied here because their privacy guarantees only hold when key parameters are tuned to $n$. Using the common `doubling trick' avoids knowledge of $n$ but leads to suboptimal and non-smooth error. We solve this problem by introducing novel matrix factorizations based on logarithmic perturbations of the function $\frac{1}{\sqrt{1-z}}$ studied in prior works, which may be of independent interest. The resulting algorithm has smooth error, and for any $α> 0$ and $t\leq n$ it is able to privately estimate the sum of the first $t$ data points with $O(\log^{2+2α}(t))$ variance. It requires $O(t)$ space and amortized $O(\log t)$ time per round, compared to $O(\log(n)\log(t))$ variance, $O(n)$ space and $O(n \log n)$ pre-processing time for the nearly-optimal bounded-input algorithm of Henzinger et al. (SODA 2023). Empirically, we find that our algorithm's performance is also comparable to theirs in absolute terms: our variance is less than $1.5\times$ theirs for $t$ as large as $2^{24}$.

</details>


### [14] [Systems-Theoretic and Data-Driven Security Analysis in ML-enabled Medical Devices](https://arxiv.org/abs/2506.15028)
*Gargi Mitra,Mohammadreza Hallajiyan,Inji Kim,Athish Pranav Dharmalingam,Mohammed Elnawawy,Shahrear Iqbal,Karthik Pattabiraman,Homa Alemzadeh*

Main category: cs.CR

TL;DR: 本文探讨了AI/ML医疗设备中的网络安全挑战，强调在设计阶段解决这些问题的重要性，并提出了一套工具和技术以支持安全评估。


<details>
  <summary>Details</summary>
Motivation: AI/ML医疗设备的快速发展带来了显著的网络安全风险，可能威胁患者安全，因此需要在设计阶段就解决这些问题。

Method: 通过分析公开的设备召回、不良事件和已知漏洞数据，作者开发了一套工具和技术，用于支持全面的上市前风险评估。

Result: 研究提出了一套工具，帮助制造商将网络安全作为AI/ML医疗设备的核心设计原则，从而确保患者安全。

Conclusion: 在AI/ML医疗设备的设计阶段嵌入网络安全是确保患者安全的关键，本文的工具和技术为此提供了支持。

Abstract: The integration of AI/ML into medical devices is rapidly transforming healthcare by enhancing diagnostic and treatment facilities. However, this advancement also introduces serious cybersecurity risks due to the use of complex and often opaque models, extensive interconnectivity, interoperability with third-party peripheral devices, Internet connectivity, and vulnerabilities in the underlying technologies. These factors contribute to a broad attack surface and make threat prevention, detection, and mitigation challenging. Given the highly safety-critical nature of these devices, a cyberattack on these devices can cause the ML models to mispredict, thereby posing significant safety risks to patients. Therefore, ensuring the security of these devices from the time of design is essential. This paper underscores the urgency of addressing the cybersecurity challenges in ML-enabled medical devices at the pre-market phase. We begin by analyzing publicly available data on device recalls and adverse events, and known vulnerabilities, to understand the threat landscape of AI/ML-enabled medical devices and their repercussions on patient safety. Building on this analysis, we introduce a suite of tools and techniques designed by us to assist security analysts in conducting comprehensive premarket risk assessments. Our work aims to empower manufacturers to embed cybersecurity as a core design principle in AI/ML-enabled medical devices, thereby making them safe for patients.

</details>


### [15] [MECHA: Multithreaded and Efficient Cryptographic Hardware Access](https://arxiv.org/abs/2506.15034)
*Pratama Derry,Laksmono Agus Mahardika Ari,Iqbal Muhammad,Howon Kim*

Main category: cs.CR

TL;DR: MECHA是一种高效的多线程加密硬件访问架构，通过UNIX域套接字管理多应用请求，提升加密操作速度和效率。


<details>
  <summary>Details</summary>
Motivation: 传统加密接口设计存在上下文切换问题，MECHA旨在解决这一问题，提供更高效的并发加密操作。

Method: 采用多线程设计（Server、Client、Transceiver线程及Sender/Receiver队列），支持多种通信协议。

Result: 实验结果显示，MECHA比传统设计快83%，适用于云计算和物联网等安全通信场景。

Conclusion: MECHA为多并发加密操作提供了高效解决方案，具有广泛的应用潜力。

Abstract: This paper presents a multithread and efficient cryptographic hardware access (MECHA) for efficient and fast cryptographic operations that eliminates the need for context switching. Utilizing a UNIX domain socket, MECHA manages multiple requests from multiple applications simultaneously, resulting in faster processing and improved efficiency. We comprise several key components, including the Server thread, Client thread, Transceiver thread, and a pair of Sender and Receiver queues. MECHA design is portable and can be used with any communication protocol, with experimental results demonstrating a 83% increase in the speed of concurrent cryptographic requests compared to conventional interface design. MECHA architecture has significant potential in the field of secure communication applications ranging from cloud computing to the IoT, offering a faster and more efficient solution for managing multiple cryptographic operation requests concurrently.

</details>


### [16] [deepSURF: Detecting Memory Safety Vulnerabilities in Rust Through Fuzzing LLM-Augmented Harnesses](https://arxiv.org/abs/2506.15648)
*Georgios Androutsopoulos,Antonio Bianchi*

Main category: cs.CR

TL;DR: deepSURF结合静态分析和LLM引导的模糊测试生成，有效检测Rust库中的内存安全漏洞，特别是针对不安全代码。


<details>
  <summary>Details</summary>
Motivation: 现有工具在检测Rust内存漏洞时能力有限，无法充分处理Rust特有类型或依赖人工干预。

Method: deepSURF通过替换泛型为自定义类型并生成特质实现，结合LLM动态增强模糊测试用例，探索复杂API交互。

Result: 在27个Rust库中测试，成功复现20个已知漏洞并发现6个新漏洞，优于现有工具。

Conclusion: deepSURF显著提升了Rust内存漏洞检测能力，适用于实际开发场景。

Abstract: Although Rust ensures memory safety by default, it also permits the use of unsafe code, which can introduce memory safety vulnerabilities if misused. Unfortunately, existing tools for detecting memory bugs in Rust typically exhibit limited detection capabilities, inadequately handle Rust-specific types, or rely heavily on manual intervention.
  To address these limitations, we present deepSURF, a tool that integrates static analysis with Large Language Model (LLM)-guided fuzzing harness generation to effectively identify memory safety vulnerabilities in Rust libraries, specifically targeting unsafe code. deepSURF introduces a novel approach for handling generics by substituting them with custom types and generating tailored implementations for the required traits, enabling the fuzzer to simulate user-defined behaviors within the fuzzed library. Additionally, deepSURF employs LLMs to augment fuzzing harnesses dynamically, facilitating exploration of complex API interactions and significantly increasing the likelihood of exposing memory safety vulnerabilities. We evaluated deepSURF on 27 real-world Rust crates, successfully rediscovering 20 known memory safety bugs and uncovering 6 previously unknown vulnerabilities, demonstrating clear improvements over state-of-the-art tools.

</details>


### [17] [Advanced Prediction of Hypersonic Missile Trajectories with CNN-LSTM-GRU Architectures](https://arxiv.org/abs/2506.15043)
*Amir Hossein Baradaran*

Main category: cs.CR

TL;DR: 本文提出了一种混合深度学习模型（CNN、LSTM、GRU）用于高超声速导弹轨迹预测，显著提升了预测精度，为防御策略提供了重要支持。


<details>
  <summary>Details</summary>
Motivation: 高超声速导弹因其高速和高机动性对防御系统构成巨大挑战，准确预测其轨迹是有效拦截的关键。

Method: 采用混合深度学习架构（CNN、LSTM、GRU），结合各模型的优势，预测复杂轨迹。

Result: 模型成功实现了高精度的轨迹预测，为防御技术和导弹拦截提供了重要贡献。

Conclusion: 研究表明，先进机器学习技术能显著提升防御系统的预测能力。

Abstract: Advancements in the defense industry are paramount for ensuring the safety and security of nations, providing robust protection against emerging threats. Among these threats, hypersonic missiles pose a significant challenge due to their extreme speeds and maneuverability, making accurate trajectory prediction a critical necessity for effective countermeasures. This paper addresses this challenge by employing a novel hybrid deep learning approach, integrating Convolutional Neural Networks (CNNs), Long Short-Term Memory (LSTM) networks, and Gated Recurrent Units (GRUs). By leveraging the strengths of these architectures, the proposed method successfully predicts the complex trajectories of hypersonic missiles with high accuracy, offering a significant contribution to defense strategies and missile interception technologies. This research demonstrates the potential of advanced machine learning techniques in enhancing the predictive capabilities of defense systems.

</details>


### [18] [Toward a Lightweight, Scalable, and Parallel Secure Encryption Engine](https://arxiv.org/abs/2506.15070)
*Rasha Karakchi,Rye Stahle-Smith,Nishant Chinnasami,Tiffany Yu*

Main category: cs.CR

TL;DR: SPiME是一种轻量级、可扩展的FPGA兼容安全处理器内存加密架构，集成AES-128到内存处理框架中，解决了传统CPU加密的性能瓶颈问题。


<details>
  <summary>Details</summary>
Motivation: 物联网应用对高效、高吞吐和节能的边缘数据处理需求激增，传统CPU加密方法在延迟敏感和资源受限环境中表现不佳。

Method: SPiME采用模块化并行内存处理单元阵列，每个单元结合AES核心和最小控制单元，实现分布式就地加密。

Result: SPiME在高端FPGA上可扩展至4000多个并行单元，资源利用率低于5%，持续加密吞吐量超过25Gbps，延迟低且可预测。

Conclusion: SPiME的设计具有便携性、可配置性和资源高效性，适用于安全边缘计算、嵌入式加密系统和可定制硬件加速器。

Abstract: The exponential growth of Internet of Things (IoT) applications has intensified the demand for efficient, high-throughput, and energy-efficient data processing at the edge. Conventional CPU-centric encryption methods suffer from performance bottlenecks and excessive data movement, especially in latency-sensitive and resource-constrained environments. In this paper, we present SPiME, a lightweight, scalable, and FPGA-compatible Secure Processor-in-Memory Encryption architecture that integrates the Advanced Encryption Standard (AES-128) directly into a Processing-in-Memory (PiM) framework. SPiME is designed as a modular array of parallel PiM units, each combining an AES core with a minimal control unit to enable distributed in-place encryption with minimal overhead. The architecture is fully implemented in Verilog and tested on multiple AMD UltraScale and UltraScale+ FPGAs. Evaluation results show that SPiME can scale beyond 4,000 parallel units while maintaining less than 5\% utilization of key FPGA resources on high-end devices. It delivers over 25~Gbps in sustained encryption throughput with predictable, low-latency performance. The design's portability, configurability, and resource efficiency make it a compelling solution for secure edge computing, embedded cryptographic systems, and customizable hardware accelerators.

</details>


### [19] [CWGAN-GP Augmented CAE for Jamming Detection in 5G-NR in Non-IID Datasets](https://arxiv.org/abs/2506.15075)
*Samhita Kuili,Mohammadreza Amini,Burak Kantarci*

Main category: cs.CR

TL;DR: 论文提出了一种基于卷积自编码器（CAE）的方法，用于检测5G-NR无线蜂窝网络中的空中干扰攻击，并通过生成对抗网络（CWGAN-GP）解决数据不平衡问题。


<details>
  <summary>Details</summary>
Motivation: 5G-NR网络中干扰攻击频发，影响信号质量，需要一种有效的检测方法。

Method: 利用卷积自编码器（CAE）检测干扰，并通过CWGAN-GP生成平衡数据集。

Result: CAE在干扰检测中表现优异，平均精确度为97.33%，召回率为91.33%，F1分数为94.08%，准确率为94.35%。

Conclusion: CAE在复杂数据环境下表现出鲁棒性，优于其他基准模型。

Abstract: In the ever-expanding domain of 5G-NR wireless cellular networks, over-the-air jamming attacks are prevalent as security attacks, compromising the quality of the received signal. We simulate a jamming environment by incorporating additive white Gaussian noise (AWGN) into the real-world In-phase and Quadrature (I/Q) OFDM datasets. A Convolutional Autoencoder (CAE) is exploited to implement a jamming detection over various characteristics such as heterogenous I/Q datasets; extracting relevant information on Synchronization Signal Blocks (SSBs), and fewer SSB observations with notable class imbalance. Given the characteristics of datasets, balanced datasets are acquired by employing a Conv1D conditional Wasserstein Generative Adversarial Network-Gradient Penalty(CWGAN-GP) on both majority and minority SSB observations. Additionally, we compare the performance and detection ability of the proposed CAE model on augmented datasets with benchmark models: Convolutional Denoising Autoencoder (CDAE) and Convolutional Sparse Autoencoder (CSAE). Despite the complexity of data heterogeneity involved across all datasets, CAE depicts the robustness in detection performance of jammed signal by achieving average values of 97.33% precision, 91.33% recall, 94.08% F1-score, and 94.35% accuracy over CDAE and CSAE.

</details>


### [20] [Flexible Hardware-Enabled Guarantees for AI Compute](https://arxiv.org/abs/2506.15093)
*James Petrie,Onni Aarne,Nora Ammann,David Dalrymple*

Main category: cs.CR

TL;DR: 论文提出了一种名为flexHEGs的硬件保障系统，旨在解决AI发展中的国际安全风险，通过可审计的硬件和物理保护实现隐私保护的AI验证与治理。


<details>
  <summary>Details</summary>
Motivation: 随着AI系统日益强大，其对国际安全的威胁加剧，现有治理方法难以在不泄露敏感信息或国家安全的情况下解决协调问题。

Method: flexHEGs由可审计的保障处理器和提供物理保护的密封外壳组成，支持开源、可更新的验证功能。

Result: 该系统支持多种治理机制，如隐私保护模型评估、受控部署、训练计算限制和自动安全协议执行。

Conclusion: 尽管技术挑战大，flexHEGs为解决前沿AI发展中的监管和国际安全问题提供了一种可行方案。

Abstract: As artificial intelligence systems become increasingly powerful, they pose growing risks to international security, creating urgent coordination challenges that current governance approaches struggle to address without compromising sensitive information or national security. We propose flexible hardware-enabled guarantees (flexHEGs), that could be integrated with AI accelerators to enable trustworthy, privacy-preserving verification and enforcement of claims about AI development. FlexHEGs consist of an auditable guarantee processor that monitors accelerator usage and a secure enclosure providing physical tamper protection. The system would be fully open source with flexible, updateable verification capabilities. FlexHEGs could enable diverse governance mechanisms including privacy-preserving model evaluations, controlled deployment, compute limits for training, and automated safety protocol enforcement. In this first part of a three part series, we provide a comprehensive introduction of the flexHEG system, including an overview of the governance and security capabilities it offers, its potential development and adoption paths, and the remaining challenges and limitations it faces. While technically challenging, flexHEGs offer an approach to address emerging regulatory and international security challenges in frontier AI development.

</details>


### [21] [International Security Applications of Flexible Hardware-Enabled Guarantees](https://arxiv.org/abs/2506.15100)
*Onni Aarne,James Petrie*

Main category: cs.CR

TL;DR: flexHEGs（灵活硬件保障）通过标准化设计、生态系统防御和明确操作参数，为国际可信AI治理提供技术基础，解决恶意使用、失控风险、军事AI系统风险及战略稳定问题。


<details>
  <summary>Details</summary>
Motivation: 随着AI能力快速发展，国际安全面临挑战，flexHEGs为通过治理框架解决这些挑战提供了机会。

Method: 分析四种国际安全应用场景，探讨两种治理模型（基于验证和基于规则集），并通过博弈论分析其稳定性。

Result: 研究表明，在合理假设下，全面的flexHEG协议可以保持稳定，并能有效管理AI风险。

Conclusion: flexHEGs虽需国际协调，但可为应对国际安全威胁提供必要的技术基础。

Abstract: As AI capabilities advance rapidly, flexible hardware-enabled guarantees (flexHEGs) offer opportunities to address international security challenges through comprehensive governance frameworks. This report examines how flexHEGs could enable internationally trustworthy AI governance by establishing standardized designs, robust ecosystem defenses, and clear operational parameters for AI-relevant chips. We analyze four critical international security applications: limiting proliferation to address malicious use, implementing safety norms to prevent loss of control, managing risks from military AI systems, and supporting strategic stability through balance-of-power mechanisms while respecting national sovereignty. The report explores both targeted deployments for specific high-risk facilities and comprehensive deployments covering all AI-relevant compute. We examine two primary governance models: verification-based agreements that enable transparent compliance monitoring, and ruleset-based agreements that automatically enforce international rules through cryptographically-signed updates. Through game-theoretic analysis, we demonstrate that comprehensive flexHEG agreements could remain stable under reasonable assumptions about state preferences and catastrophic risks. The report addresses critical implementation challenges including technical thresholds for AI-relevant chips, management of existing non-flexHEG hardware, and safeguards against abuse of governance power. While requiring significant international coordination, flexHEGs could provide a technical foundation for managing AI risks at the scale and speed necessary to address emerging threats to international security and stability.

</details>


### [22] [EVA-S2PMLP: Secure and Scalable Two-Party MLP via Spatial Transformation](https://arxiv.org/abs/2506.15102)
*Shizhao Peng,Shoumo Li,Tianle Tao*

Main category: cs.CR

TL;DR: EVA-S2PMLP是一种高效、可验证且准确的安全两方多层感知机框架，用于垂直分区场景下的隐私保护神经网络训练。


<details>
  <summary>Details</summary>
Motivation: 解决跨机构安全协作建模中的隐私保护问题。

Method: 提出空间尺度优化和安全的实数域转换管道，支持线性与非线性安全计算。

Result: 实验表明EVA-S2PMLP在保持高推理精度的同时显著降低通信开销，性能提升达12.3倍。

Conclusion: 该框架在金融、医疗等领域具有实用性，能严格保护数据隐私。

Abstract: Privacy-preserving neural network training in vertically partitioned scenarios is vital for secure collaborative modeling across institutions. This paper presents \textbf{EVA-S2PMLP}, an Efficient, Verifiable, and Accurate Secure Two-Party Multi-Layer Perceptron framework that introduces spatial-scale optimization for enhanced privacy and performance. To enable reliable computation under real-number domain, EVA-S2PMLP proposes a secure transformation pipeline that maps scalar inputs to vector and matrix spaces while preserving correctness. The framework includes a suite of atomic protocols for linear and non-linear secure computations, with modular support for secure activation, matrix-vector operations, and loss evaluation. Theoretical analysis confirms the reliability, security, and asymptotic complexity of each protocol. Extensive experiments show that EVA-S2PMLP achieves high inference accuracy and significantly reduced communication overhead, with up to $12.3\times$ improvement over baselines. Evaluation on benchmark datasets demonstrates that the framework maintains model utility while ensuring strict data confidentiality, making it a practical solution for privacy-preserving neural network training in finance, healthcare, and cross-organizational AI applications.

</details>


### [23] [PDLRecover: Privacy-preserving Decentralized Model Recovery with Machine Unlearning](https://arxiv.org/abs/2506.15112)
*Xiangman Li,Xiaodong Wu,Jianbing Ni,Mohamed Mahmoud,Maazen Alsabaan*

Main category: cs.CR

TL;DR: PDLRecover是一种高效恢复被投毒全局模型的新方法，利用历史模型信息并保护隐私，避免完全重新训练的高成本。


<details>
  <summary>Details</summary>
Motivation: 现有防御方法仅能检测和过滤恶意模型，无法有效恢复已受损的全局模型，而完全重新训练成本高且可能影响隐私。

Method: 通过近似Hessian矩阵计算的线性特性，结合秘密共享技术保护历史更新，实现模型恢复。包括客户端准备、周期性恢复更新和最终精确更新。

Result: 实验表明，恢复后的全局模型性能接近完全重新训练的模型，但计算和时间成本显著降低，同时保护了本地模型隐私。

Conclusion: PDLRecover在高效恢复模型的同时，确保了隐私和性能，为去中心化学习中的投毒攻击提供了可行的解决方案。

Abstract: Decentralized learning is vulnerable to poison attacks, where malicious clients manipulate local updates to degrade global model performance. Existing defenses mainly detect and filter malicious models, aiming to prevent a limited number of attackers from corrupting the global model. However, restoring an already compromised global model remains a challenge. A direct approach is to remove malicious clients and retrain the model using only the benign clients. Yet, retraining is time-consuming, computationally expensive, and may compromise model consistency and privacy.
  We propose PDLRecover, a novel method to recover a poisoned global model efficiently by leveraging historical model information while preserving privacy. The main challenge lies in protecting shared historical models while enabling parameter estimation for model recovery. By exploiting the linearity of approximate Hessian matrix computation, we apply secret sharing to protect historical updates, ensuring local models are not leaked during transmission or reconstruction. PDLRecover introduces client-side preparation, periodic recovery updates, and a final exact update to ensure robustness and convergence of the recovered model. Periodic updates maintain accurate curvature information, and the final step ensures high-quality convergence. Experiments show that the recovered global model achieves performance comparable to a fully retrained model but with significantly reduced computation and time cost. Moreover, PDLRecover effectively prevents leakage of local model parameters, ensuring both accuracy and privacy in recovery.

</details>


### [24] [CipherMind: The Longest Codebook in the World](https://arxiv.org/abs/2506.15117)
*Ming Nie,Zhixiong Yang,Bingsheng Wei*

Main category: cs.CR

TL;DR: CipherMind利用大语言模型的确定性微调中间结果作为传输内容，实现通信加密。


<details>
  <summary>Details</summary>
Motivation: 大语言模型的广泛应用激发了利用其推理进行通信加密的灵感。

Method: 利用大语言模型的语义参数（不透明、弱可解释性）作为加密方法，适用于网关内传输等场景。

Result: 提出了一种基于大模型的通信加密范式CipherMind。

Conclusion: 该范式理论上可用任何大模型实现，适用于多种通信场景。

Abstract: In recent years, the widespread application of large language models has inspired us to consider using inference for communication encryption. We therefore propose CipherMind, which utilizes intermediate results from deterministic fine-tuning of large model inferences as transmission content. The semantic parameters of large models exhibit characteristics like opaque underlying implementations and weak interpretability, thus enabling their use as an encryption method for data transmission. This communication paradigm can be applied in scenarios like intra-gateway transmission, and theoretically, it can be implemented using any large model as its foundation.

</details>


### [25] [From LLMs to MLLMs to Agents: A Survey of Emerging Paradigms in Jailbreak Attacks and Defenses within LLM Ecosystem](https://arxiv.org/abs/2506.15170)
*Yanxu Mao,Tiehan Cui,Peipei Liu,Datao You,Hongsong Zhu*

Main category: cs.CR

TL;DR: 本文系统综述了大型语言模型（LLM）生态系统中越狱攻击的复杂性及防御机制，分析了从LLM到多模态LLM和智能代理的发展轨迹，分类了主流越狱技术，并总结了现有防御策略的局限性。


<details>
  <summary>Details</summary>
Motivation: 随着LLM从单模态系统发展为多模态LLM和智能代理，其能力扩展的同时也引入了严重的安全风险，需要系统研究越狱攻击和防御机制。

Method: 通过分类主流越狱技术（攻击影响和可见性视角），分析代表性攻击方法、数据集和评估指标，并基于响应时间和技术方法组织防御策略。

Result: 总结了现有研究的局限性（如对代理特定安全问题的关注不足、混合越狱方法分类不清等），并提出了未来研究方向（如数据集构建、评估框架优化等）。

Conclusion: 研究旨在增强对越狱机制的理解，推动在更强大的LLM背景下发展更具弹性和适应性的防御策略。

Abstract: Large language models (LLMs) are rapidly evolving from single-modal systems to multimodal LLMs and intelligent agents, significantly expanding their capabilities while introducing increasingly severe security risks. This paper presents a systematic survey of the growing complexity of jailbreak attacks and corresponding defense mechanisms within the expanding LLM ecosystem. We first trace the developmental trajectory from LLMs to MLLMs and Agents, highlighting the core security challenges emerging at each stage. Next, we categorize mainstream jailbreak techniques from both the attack impact and visibility perspectives, and provide a comprehensive analysis of representative attack methods, related datasets, and evaluation metrics. On the defense side, we organize existing strategies based on response timing and technical approach, offering a structured understanding of their applicability and implementation. Furthermore, we identify key limitations in existing surveys, such as insufficient attention to agent-specific security issues, the absence of a clear taxonomy for hybrid jailbreak methods, a lack of detailed analysis of experimental setups, and outdated coverage of recent advancements. To address these limitations, we provide an updated synthesis of recent work and outline future research directions in areas such as dataset construction, evaluation framework optimization, and strategy generalization. Our study seeks to enhance the understanding of jailbreak mechanisms and facilitate the advancement of more resilient and adaptive defense strategies in the context of ever more capable LLMs.

</details>


### [26] [RAS-Eval: A Comprehensive Benchmark for Security Evaluation of LLM Agents in Real-World Environments](https://arxiv.org/abs/2506.15253)
*Yuchuan Fu,Xiaohan Yuan,Dongxia Wang*

Main category: cs.CR

TL;DR: 论文介绍了RAS-Eval，一个用于评估LLM代理在动态环境中安全性的综合基准，揭示了现有模型的显著漏洞。


<details>
  <summary>Details</summary>
Motivation: 由于LLM代理在医疗和金融等关键领域的快速部署，缺乏标准化的安全评估基准，作者提出了RAS-Eval来解决这一问题。

Method: RAS-Eval包含80个测试用例和3,802个攻击任务，覆盖11个CWE类别，支持模拟和真实工具执行。评估了6种先进LLM。

Result: 攻击平均降低了代理任务完成率36.78%，学术环境中攻击成功率达85.65%。大模型在安全性上表现优于小模型。

Conclusion: 研究揭示了LLM代理在现实部署中的重大风险，并提供了未来安全研究的基础框架。

Abstract: The rapid deployment of Large language model (LLM) agents in critical domains like healthcare and finance necessitates robust security frameworks. To address the absence of standardized evaluation benchmarks for these agents in dynamic environments, we introduce RAS-Eval, a comprehensive security benchmark supporting both simulated and real-world tool execution. RAS-Eval comprises 80 test cases and 3,802 attack tasks mapped to 11 Common Weakness Enumeration (CWE) categories, with tools implemented in JSON, LangGraph, and Model Context Protocol (MCP) formats. We evaluate 6 state-of-the-art LLMs across diverse scenarios, revealing significant vulnerabilities: attacks reduced agent task completion rates (TCR) by 36.78% on average and achieved an 85.65% success rate in academic settings. Notably, scaling laws held for security capabilities, with larger models outperforming smaller counterparts. Our findings expose critical risks in real-world agent deployments and provide a foundational framework for future security research. Code and data are available at https://github.com/lanzer-tree/RAS-Eval.

</details>


### [27] [LLM vs. SAST: A Technical Analysis on Detecting Coding Bugs of GPT4-Advanced Data Analysis](https://arxiv.org/abs/2506.15212)
*Madjid G. Tehrani,Eldar Sultanow,William J. Buchanan,Mahkame Houmani,Christel H. Djaha Fodja*

Main category: cs.CR

TL;DR: GPT-4在漏洞扫描中表现优于传统SAST工具，准确率达94%。


<details>
  <summary>Details</summary>
Motivation: 研究GPT-4在识别软件漏洞方面的效能，并与传统SAST工具对比。

Method: 通过分析多种安全错误，评估GPT-4的漏洞检测能力。

Result: GPT-4在检测32种可被利用漏洞时准确率达94%，优于SAST工具。

Conclusion: GPT-4在漏洞扫描中具有潜力，但需结合安全设计及AI最佳实践。

Abstract: With the rapid advancements in Natural Language Processing (NLP), large language models (LLMs) like GPT-4 have gained significant traction in diverse applications, including security vulnerability scanning. This paper investigates the efficacy of GPT-4 in identifying software vulnerabilities compared to traditional Static Application Security Testing (SAST) tools. Drawing from an array of security mistakes, our analysis underscores the potent capabilities of GPT-4 in LLM-enhanced vulnerability scanning. We unveiled that GPT-4 (Advanced Data Analysis) outperforms SAST by an accuracy of 94% in detecting 32 types of exploitable vulnerabilities. This study also addresses the potential security concerns surrounding LLMs, emphasising the imperative of security by design/default and other security best practices for AI.

</details>


### [28] [Evaluation Pipeline for systematically searching for Anomaly Detection Systems](https://arxiv.org/abs/2506.15388)
*Florian Rokohl,Alexander Lehnert,Marc Reichenbach*

Main category: cs.CR

TL;DR: 提出了一种基于FPGA的硬件异常检测系统，用于实时检测医疗数字化环境中的恶意客户端。


<details>
  <summary>Details</summary>
Motivation: 医疗数字化带来便利的同时也面临网络安全威胁，需要实时且高效的解决方案。

Method: 使用FPGA满足实时性和功耗限制，并通过整体系统评估优化性能。

Result: 系统能够在实时条件下检测恶意客户端。

Conclusion: 基于FPGA的硬件方案为医疗数字化安全提供了有效的实时检测手段。

Abstract: Digitalization in the medical world provides major benefits while making it a target for attackers and thus hard to secure. To deal with network intruders we propose an anomaly detection system on hardware to detect malicious clients in real-time. We meet real-time and power restrictions using FPGAs. Overall system performance is achieved via the presented holistic system evaluation.

</details>


### [29] [Facility Location Problem under Local Differential Privacy without Super-set Assumption](https://arxiv.org/abs/2506.15224)
*Kevin Pfisterer,Quentin Hillebrand,Vorapong Suppakitpaisarn*

Main category: cs.CR

TL;DR: 本文提出了一种设施位置问题的改进版本，并在本地差分隐私（LDP）框架下进行分析，展示了一种LDP算法，能够以较小的附加因子实现恒定近似比。


<details>
  <summary>Details</summary>
Motivation: 在原始设施位置问题中，Gupta等人证明了任何差分隐私算法的近似比下限为Ω(√n)，后续工作采用了超集假设，但可能损害用户隐私。本文旨在证明这一下限不适用于改进版本。

Method: 提出了一种适应本地差分隐私的设施位置问题改进版本，并设计了一种LDP算法。

Result: 实验结果表明，该算法在合成和真实数据集上均优于直接方法。

Conclusion: 改进后的设施位置问题在LDP框架下可以实现更好的隐私保护和性能表现。

Abstract: In this paper, we introduce an adaptation of the facility location problem and analyze it within the framework of local differential privacy (LDP). Under this model, we ensure the privacy of client presence at specific locations. When n is the number of points, Gupta et al. established a lower bound of $Ω(\sqrt{n})$ on the approximation ratio for any differentially private algorithm applied to the original facility location problem. As a result, subsequent works have adopted the super-set assumption, which may, however, compromise user privacy. We show that this lower bound does not apply to our adaptation by presenting an LDP algorithm that achieves a constant approximation ratio with a relatively small additive factor. Additionally, we provide experimental results demonstrating that our algorithm outperforms the straightforward approach on both synthetically generated and real-world datasets.

</details>


### [30] [Detecting Hardware Trojans in Microprocessors via Hardware Error Correction Code-based Modules](https://arxiv.org/abs/2506.15417)
*Alessandro Palumbo,Ruben Salvador*

Main category: cs.CR

TL;DR: 该论文提出了一种基于硬件的方法，利用ECC在RISC-V微处理器上检测运行时硬件木马（HT）激活，实现了100%的检测率且无额外开销。


<details>
  <summary>Details</summary>
Motivation: 硬件木马（HT）可能通过注入恶意指令破坏正常执行流程，威胁系统安全，因此需要一种高效的检测方法。

Method: 采用基于汉明单纠错（HSEC）架构的硬件安全检查器（HSC）来检测HT激活。

Result: 实验结果显示，该方法能100%检测潜在HT激活，无假阳性或漏检，且硬件开销极低（72 LUTs、24 FFs、0.5 BRAM）。

Conclusion: 该方法在保持微处理器性能的同时，有效解决了HT检测问题，具有实际应用价值。

Abstract: Software-exploitable Hardware Trojans (HTs) enable attackers to execute unauthorized software or gain illicit access to privileged operations. This manuscript introduces a hardware-based methodology for detecting runtime HT activations using Error Correction Codes (ECCs) on a RISC-V microprocessor. Specifically, it focuses on HTs that inject malicious instructions, disrupting the normal execution flow by triggering unauthorized programs. To counter this threat, the manuscript introduces a Hardware Security Checker (HSC) leveraging Hamming Single Error Correction (HSEC) architectures for effective HT detection. Experimental results demonstrate that the proposed solution achieves a 100% detection rate for potential HT activations, with no false positives or undetected attacks. The implementation incurs minimal overhead, requiring only 72 #LUTs, 24 #FFs, and 0.5 #BRAM while maintaining the microprocessor's original operating frequency and introducing no additional time delay.

</details>


### [31] [Side-Channel Extraction of Dataflow AI Accelerator Hardware Parameters](https://arxiv.org/abs/2506.15432)
*Guillaume Lomet,Ruben Salvador,Brice Colombier,Vincent Grosso,Olivier Sentieys,Cedric Killian*

Main category: cs.CR

TL;DR: 本文提出了一种通过侧信道攻击（SCA）恢复FINN框架生成的数据流加速器硬件配置的方法，显著降低了计算开销，并在短时间内高精度恢复参数。


<details>
  <summary>Details</summary>
Motivation: 现有的数据流神经网络加速器虽然部署便捷，但易受恶意攻击者通过侧信道攻击窃取知识产权（IP），因此需要研究其安全性。

Method: 采用无监督降维技术减少计算开销，结合轻量级分类器恢复折叠和量化参数，使用随机森林分类器分析侧信道轨迹。

Result: 攻击阶段仅需337毫秒即可恢复硬件参数（准确率>95%），421毫秒完全恢复参数（平均4次轨迹），计算效率显著优于现有方法。

Conclusion: 该方法在更现实的攻击场景中表现优异，计算效率提升显著，且无需轨迹平均即可取得更好结果。

Abstract: Dataflow neural network accelerators efficiently process AI tasks on FPGAs, with deployment simplified by ready-to-use frameworks and pre-trained models. However, this convenience makes them vulnerable to malicious actors seeking to reverse engineer valuable Intellectual Property (IP) through Side-Channel Attacks (SCA). This paper proposes a methodology to recover the hardware configuration of dataflow accelerators generated with the FINN framework. Through unsupervised dimensionality reduction, we reduce the computational overhead compared to the state-of-the-art, enabling lightweight classifiers to recover both folding and quantization parameters. We demonstrate an attack phase requiring only 337 ms to recover the hardware parameters with an accuracy of more than 95% and 421 ms to fully recover these parameters with an averaging of 4 traces for a FINN-based accelerator running a CNN, both using a random forest classifier on side-channel traces, even with the accelerator dataflow fully loaded. This approach offers a more realistic attack scenario than existing methods, and compared to SoA attacks based on tsfresh, our method requires 940x and 110x less time for preparation and attack phases, respectively, and gives better results even without averaging traces.

</details>


### [32] [An efficient construction of Raz's two-source randomness extractor with improved parameters](https://arxiv.org/abs/2506.15547)
*Cameron Foreman,Lewis Wooltorton,Kevin Milner,Florian J. Curchod*

Main category: cs.CR

TL;DR: 本文改进了Raz的随机性提取器，降低了计算复杂度至准线性时间，并减少了熵需求。提供了分析和数值比较，并实现了开源代码。


<details>
  <summary>Details</summary>
Motivation: 解决Raz提取器计算复杂度高的问题，使其更实用。

Method: 提出改进的Raz提取器，降低计算复杂度至准线性时间，并减少熵需求。

Result: 实现了高效且量子安全的提取器，提供了开源代码和参数计算模块。

Conclusion: 改进的提取器更高效且实用，适用于实际应用。

Abstract: Randomness extractors are algorithms that distill weak random sources into near-perfect random numbers. Two-source extractors enable this distillation process by combining two independent weak random sources. Raz's extractor (STOC '05) was the first to achieve this in a setting where one source has linear min-entropy (i.e., proportional to its length), while the other has only logarithmic min-entropy in its length. However, Raz's original construction is impractical due to a polynomial computation time of at least degree 4. Our work solves this problem by presenting an improved version of Raz's extractor with quasi-linear computation time, as well as a new analytic theorem with reduced entropy requirements. We provide comprehensive analytical and numerical comparisons of our construction with others in the literature, and we derive strong and quantum-proof versions of our efficient Raz extractor. Additionally, we offer an easy-to-use, open-source code implementation of the extractor and a numerical parameter calculation module.

</details>


### [33] [PhishDebate: An LLM-Based Multi-Agent Framework for Phishing Website Detection](https://arxiv.org/abs/2506.15656)
*Wenhao Li,Selvakumar Manickam,Yung-wey Chong,Shankar Karuppayah*

Main category: cs.CR

TL;DR: PhishDebate是一个基于多代理LLM的辩论框架，用于钓鱼网站检测，通过四个专门代理分析网页的不同文本方面，显著提高了准确性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有钓鱼检测方法多为单代理分类，存在幻觉风险和缺乏可解释性或鲁棒性。

Method: PhishDebate采用四个专门代理（URL结构、HTML组成、语义内容和品牌冒充）在协调员和法官的指导下进行结构化辩论。

Result: 在真实钓鱼数据集上，PhishDebate实现了98.2%的召回率和真阳性率，优于单代理和CoT基线。

Conclusion: PhishDebate通过模块化设计和多代理辩论，显著提升了钓鱼检测的性能和适应性。

Abstract: Phishing websites continue to pose a significant cybersecurity threat, often leveraging deceptive structures, brand impersonation, and social engineering tactics to evade detection. While recent advances in large language models (LLMs) have enabled improved phishing detection through contextual understanding, most existing approaches rely on single-agent classification facing the risks of hallucination and lack interpretability or robustness. To address these limitations, we propose PhishDebate, a modular multi-agent LLM-based debate framework for phishing website detection. PhishDebate employs four specialized agents to independently analyze different textual aspects of a webpage--URL structure, HTML composition, semantic content, and brand impersonation--under the coordination of a Moderator and a final Judge. Through structured debate and divergent thinking, the framework delivers more accurate and interpretable decisions. Extensive evaluations on commercial LLMs demonstrate that PhishDebate achieves 98.2% recall and 98.2% True Positive Rate (TPR) on a real-world phishing dataset, and outperforms single-agent and Chain of Thought (CoT) baselines. Additionally, its modular design allows agent-level configurability, enabling adaptation to varying resource and application requirements.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [34] [Multi-Agent Reinforcement Learning for Autonomous Multi-Satellite Earth Observation: A Realistic Case Study](https://arxiv.org/abs/2506.15207)
*Mohamad A. Hady,Siyi Hu,Mahardhika Pratama,Jimmy Cao,Ryszard Kowalczyk*

Main category: cs.AI

TL;DR: 本文探讨了利用强化学习（RL）和多智能体强化学习（MARL）解决多卫星系统中的自主协调问题，特别是在动态地球观测任务中。


<details>
  <summary>Details</summary>
Motivation: 随着低地球轨道卫星数量的激增，传统优化方法难以满足动态地球观测任务的实时决策需求，因此需要探索RL和MARL的应用。

Method: 通过建模单卫星操作并扩展到多卫星星座，使用MARL框架（如PPO、IPPO、MAPPO和HAPPO）解决能源、数据存储限制及部分可观测性下的协调问题。

Result: 实验表明，MARL能有效平衡成像与资源管理，并解决多卫星协调中的非平稳性和奖励依赖性问题。

Conclusion: 研究为自主卫星操作提供了基础，并为分散式地球观测任务中的策略学习提供了实用指南。

Abstract: The exponential growth of Low Earth Orbit (LEO) satellites has revolutionised Earth Observation (EO) missions, addressing challenges in climate monitoring, disaster management, and more. However, autonomous coordination in multi-satellite systems remains a fundamental challenge. Traditional optimisation approaches struggle to handle the real-time decision-making demands of dynamic EO missions, necessitating the use of Reinforcement Learning (RL) and Multi-Agent Reinforcement Learning (MARL). In this paper, we investigate RL-based autonomous EO mission planning by modelling single-satellite operations and extending to multi-satellite constellations using MARL frameworks. We address key challenges, including energy and data storage limitations, uncertainties in satellite observations, and the complexities of decentralised coordination under partial observability. By leveraging a near-realistic satellite simulation environment, we evaluate the training stability and performance of state-of-the-art MARL algorithms, including PPO, IPPO, MAPPO, and HAPPO. Our results demonstrate that MARL can effectively balance imaging and resource management while addressing non-stationarity and reward interdependency in multi-satellite coordination. The insights gained from this study provide a foundation for autonomous satellite operations, offering practical guidelines for improving policy learning in decentralised EO missions.

</details>


### [35] [CALM: Contextual Analog Logic with Multimodality](https://arxiv.org/abs/2506.14936)
*Maxwell J. Jacobson,Corey J. Maley,Yexiang Xue*

Main category: cs.AI

TL;DR: CALM结合符号推理与神经生成，通过多模态数据实现上下文敏感决策。


<details>
  <summary>Details</summary>
Motivation: 传统二值逻辑无法捕捉人类决策的细微差别，且在多模态环境中依赖人工标注，缺乏灵活性。神经网络虽能提取多模态信息，但缺乏可解释的推理结构。

Method: CALM使用领域树表示谓词，通过神经网络迭代优化其模拟真值，并结合符号推理模块确保约束满足。

Result: 在填空式物体放置任务中，CALM准确率达92.2%，优于传统逻辑（86.3%）和LLM（59.4%），并能生成符合逻辑约束和人类偏好的空间热图。

Conclusion: CALM展示了在多模态环境中结合逻辑结构与神经网络的潜力，为下一代AI系统奠定了基础。

Abstract: In this work, we introduce Contextual Analog Logic with Multimodality (CALM). CALM unites symbolic reasoning with neural generation, enabling systems to make context-sensitive decisions grounded in real-world multi-modal data.
  Background: Classic bivalent logic systems cannot capture the nuance of human decision-making. They also require human grounding in multi-modal environments, which can be ad-hoc, rigid, and brittle. Neural networks are good at extracting rich contextual information from multi-modal data, but lack interpretable structures for reasoning.
  Objectives: CALM aims to bridge the gap between logic and neural perception, creating an analog logic that can reason over multi-modal inputs. Without this integration, AI systems remain either brittle or unstructured, unable to generalize robustly to real-world tasks. In CALM, symbolic predicates evaluate to analog truth values computed by neural networks and constrained search.
  Methods: CALM represents each predicate using a domain tree, which iteratively refines its analog truth value when the contextual groundings of its entities are determined. The iterative refinement is predicted by neural networks capable of capturing multi-modal information and is filtered through a symbolic reasoning module to ensure constraint satisfaction.
  Results: In fill-in-the-blank object placement tasks, CALM achieved 92.2% accuracy, outperforming classical logic (86.3%) and LLM (59.4%) baselines. It also demonstrated spatial heatmap generation aligned with logical constraints and delicate human preferences, as shown by a human study.
  Conclusions: CALM demonstrates the potential to reason with logic structure while aligning with preferences in multi-modal environments. It lays the foundation for next-gen AI systems that require the precision and interpretation of logic and the multimodal information processing of neural networks.

</details>


### [36] [SwarmAgentic: Towards Fully Automated Agentic System Generation via Swarm Intelligence](https://arxiv.org/abs/2506.15672)
*Yao Zhang,Chenyang Lin,Shijie Tang,Haokun Chen,Shijie Zhou,Yunpu Ma,Volker Tresp*

Main category: cs.AI

TL;DR: SwarmAgentic是一个完全自动化的代理系统生成框架，通过语言驱动探索和粒子群优化（PSO）实现代理功能和协作的联合优化，显著提升了任务性能。


<details>
  <summary>Details</summary>
Motivation: 现有代理系统生成框架缺乏完全自主性，限制了适应性和可扩展性。

Method: SwarmAgentic通过维护候选系统群体并基于反馈更新，实现了代理功能和协作的联合优化。

Result: 在六个真实任务中，SwarmAgentic表现优异，如在TravelPlanner基准测试中相对ADAS提升261.8%。

Conclusion: SwarmAgentic为可扩展和自主的代理系统设计迈出了重要一步，结合了群体智能与自动化多代理生成。

Abstract: The rapid progress of Large Language Models has advanced agentic systems in decision-making, coordination, and task execution. Yet, existing agentic system generation frameworks lack full autonomy, missing from-scratch agent generation, self-optimizing agent functionality, and collaboration, limiting adaptability and scalability. We propose SwarmAgentic, a framework for fully automated agentic system generation that constructs agentic systems from scratch and jointly optimizes agent functionality and collaboration as interdependent components through language-driven exploration. To enable efficient search over system-level structures, SwarmAgentic maintains a population of candidate systems and evolves them via feedback-guided updates, drawing inspiration from Particle Swarm Optimization (PSO). We evaluate our method on six real-world, open-ended, and exploratory tasks involving high-level planning, system-level coordination, and creative reasoning. Given only a task description and an objective function, SwarmAgentic outperforms all baselines, achieving a +261.8% relative improvement over ADAS on the TravelPlanner benchmark, highlighting the effectiveness of full automation in structurally unconstrained tasks. This framework marks a significant step toward scalable and autonomous agentic system design, bridging swarm intelligence with fully automated system multi-agent generation. Our code is publicly released at https://yaoz720.github.io/SwarmAgentic/.

</details>


### [37] [MEAL: A Benchmark for Continual Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2506.14990)
*Tristan Tomilin,Luka van den Boogaard,Samuel Garcin,Bram Grooten,Meng Fang,Mykola Pechenizkiy*

Main category: cs.AI

TL;DR: MEAL是首个专为持续多智能体强化学习（CMARL）设计的基准测试，利用JAX实现GPU加速，支持在普通台式机上快速完成100个任务的持续学习。


<details>
  <summary>Details</summary>
Motivation: 现有持续学习基准测试在CPU上运行，导致计算瓶颈，限制了任务序列的长度，而多智能体环境中的持续学习研究较少。

Method: 引入MEAL基准测试，利用JAX进行GPU加速，支持长任务序列的持续学习，并测试现有CL和MARL方法的组合效果。

Result: 现有方法在简单环境中表现良好，但在需要持续协调和适应的复杂环境中表现不佳。消融研究确定了CMARL的关键架构和算法特征。

Conclusion: MEAL为CMARL研究提供了高效工具，揭示了现有方法的局限性，并指出了未来改进方向。

Abstract: Benchmarks play a crucial role in the development and analysis of reinforcement learning (RL) algorithms, with environment availability strongly impacting research. One particularly underexplored intersection is continual learning (CL) in cooperative multi-agent settings. To remedy this, we introduce MEAL (Multi-agent Environments for Adaptive Learning), the first benchmark tailored for continual multi-agent reinforcement learning (CMARL). Existing CL benchmarks run environments on the CPU, leading to computational bottlenecks and limiting the length of task sequences. MEAL leverages JAX for GPU acceleration, enabling continual learning across sequences of 100 tasks on a standard desktop PC in a few hours. We show that naively combining popular CL and MARL methods yields strong performance on simple environments, but fails to scale to more complex settings requiring sustained coordination and adaptation. Our ablation study identifies architectural and algorithmic features critical for CMARL on MEAL.

</details>


### [38] [Truncated Proximal Policy Optimization](https://arxiv.org/abs/2506.15050)
*Tiantian Fan,Lingjun Liu,Yu Yue,Jiaze Chen,Chengyi Wang,Qiying Yu,Chi Zhang,Zhiqi Lin,Ruofei Zhu,Yufeng Yuan,Xiaochen Zuo,Bole Ma,Mofan Zhang,Gaohong Liu,Ru Zhang,Haotian Zhou,Cong Xie,Ruidong Zhu,Zhi Zhang,Xin Liu,Mingxuan Wang,Lin Yan,Yonghui Wu*

Main category: cs.AI

TL;DR: 本文提出了一种名为T-PPO的新方法，通过优化策略更新和限制生成长度，显著提高了大型语言模型（LLMs）的训练效率。


<details>
  <summary>Details</summary>
Motivation: 现有的PPO方法因同步长生成过程导致硬件利用率低，训练效率不高。

Method: 提出了T-PPO方法，包括扩展的广义优势估计（EGAE）和独立优化策略与价值模型的机制。

Result: 在AIME 2024上实验表明，T-PPO将训练效率提高了2.5倍，并优于现有方法。

Conclusion: T-PPO通过优化训练流程，显著提升了LLMs的训练效率，同时保持了性能。

Abstract: Recently, test-time scaling Large Language Models (LLMs) have demonstrated exceptional reasoning capabilities across scientific and professional tasks by generating long chains-of-thought (CoT). As a crucial component for developing these reasoning models, reinforcement learning (RL), exemplified by Proximal Policy Optimization (PPO) and its variants, allows models to learn through trial and error. However, PPO can be time-consuming due to its inherent on-policy nature, which is further exacerbated by increasing response lengths. In this work, we propose Truncated Proximal Policy Optimization (T-PPO), a novel extension to PPO that improves training efficiency by streamlining policy update and length-restricted response generation. T-PPO mitigates the issue of low hardware utilization, an inherent drawback of fully synchronized long-generation procedures, where resources often sit idle during the waiting periods for complete rollouts. Our contributions are two-folds. First, we propose Extended Generalized Advantage Estimation (EGAE) for advantage estimation derived from incomplete responses while maintaining the integrity of policy learning. Second, we devise a computationally optimized mechanism that allows for the independent optimization of the policy and value models. By selectively filtering prompt and truncated tokens, this mechanism reduces redundant computations and accelerates the training process without sacrificing convergence performance. We demonstrate the effectiveness and efficacy of T-PPO on AIME 2024 with a 32B base model. The experimental results show that T-PPO improves the training efficiency of reasoning LLMs by up to 2.5x and outperforms its existing competitors.

</details>


### [39] [HeurAgenix: Leveraging LLMs for Solving Complex Combinatorial Optimization Challenges](https://arxiv.org/abs/2506.15196)
*Xianliang Yang,Ling Zhang,Haolong Qian,Lei Song,Jiang Bian*

Main category: cs.AI

TL;DR: HeurAgenix是一个基于大语言模型（LLM）的两阶段超启发式框架，用于自动生成和选择启发式算法，以解决组合优化问题。


<details>
  <summary>Details</summary>
Motivation: 传统启发式算法设计依赖人工经验且难以泛化，HeurAgenix旨在通过LLM自动化和优化这一过程。

Method: 框架分为启发式演化和动态选择两阶段：LLM用于提取演化策略，并在问题求解时动态选择最优启发式。

Result: 在标准基准测试中，HeurAgenix优于现有基于LLM的超启发式方法，甚至媲美专用求解器。

Conclusion: HeurAgenix展示了LLM在组合优化中的潜力，提供了一种灵活且高效的解决方案。

Abstract: Heuristic algorithms play a vital role in solving combinatorial optimization (CO) problems, yet traditional designs depend heavily on manual expertise and struggle to generalize across diverse instances. We introduce \textbf{HeurAgenix}, a two-stage hyper-heuristic framework powered by large language models (LLMs) that first evolves heuristics and then selects among them automatically. In the heuristic evolution phase, HeurAgenix leverages an LLM to compare seed heuristic solutions with higher-quality solutions and extract reusable evolution strategies. During problem solving, it dynamically picks the most promising heuristic for each problem state, guided by the LLM's perception ability. For flexibility, this selector can be either a state-of-the-art LLM or a fine-tuned lightweight model with lower inference cost. To mitigate the scarcity of reliable supervision caused by CO complexity, we fine-tune the lightweight heuristic selector with a dual-reward mechanism that jointly exploits singals from selection preferences and state perception, enabling robust selection under noisy annotations. Extensive experiments on canonical benchmarks show that HeurAgenix not only outperforms existing LLM-based hyper-heuristics but also matches or exceeds specialized solvers. Code is available at https://github.com/microsoft/HeurAgenix.

</details>


### [40] [Joint Computation Offloading and Resource Allocation for Uncertain Maritime MEC via Cooperation of UAVs and Vessels](https://arxiv.org/abs/2506.15225)
*Jiahao You,Ziye Jia,Chao Dong,Qihui Wu,Zhu Han*

Main category: cs.AI

TL;DR: 本文提出了一种基于无人机和船舶协作的海上计算卸载和资源分配框架，利用Lyapunov优化和马尔可夫博弈解决不确定任务和异构资源问题。


<details>
  <summary>Details</summary>
Motivation: 海上物联网（MIoT）的计算需求快速增长，但不确定的任务导致计算卸载和资源分配效率低下。

Method: 提出协作MEC框架，利用Lyapunov优化处理任务不确定性，将问题转化为马尔可夫博弈，并提出异构智能体软演员-评论家算法。

Result: 仿真验证了框架在计算卸载和资源分配中的有效性。

Conclusion: 通过无人机和船舶协作，结合优化算法，能有效解决MIoT中的计算卸载和资源分配问题。

Abstract: The computation demands from the maritime Internet of Things (MIoT) increase rapidly in recent years, and the unmanned aerial vehicles (UAVs) and vessels based multi-access edge computing (MEC) can fulfill these MIoT requirements. However, the uncertain maritime tasks present significant challenges of inefficient computation offloading and resource allocation. In this paper, we focus on the maritime computation offloading and resource allocation through the cooperation of UAVs and vessels, with consideration of uncertain tasks. Specifically, we propose a cooperative MEC framework for computation offloading and resource allocation, including MIoT devices, UAVs and vessels. Then, we formulate the optimization problem to minimize the total execution time. As for the uncertain MIoT tasks, we leverage Lyapunov optimization to tackle the unpredictable task arrivals and varying computational resource availability. 
By converting the long-term constraints into short-term constraints, we obtain a set of small-scale optimization problems. Further, considering the heterogeneity of actions and resources of UAVs and vessels, we reformulate the small-scale optimization problem into a Markov game (MG). Moreover, a heterogeneous-agent soft actor-critic is proposed to sequentially update various neural networks and effectively solve the MG problem. 
Finally, simulations are conducted to verify the effectiveness in addressing computational offloading and resource allocation.

</details>


### [41] [Efficient and Generalizable Environmental Understanding for Visual Navigation](https://arxiv.org/abs/2506.15377)
*Ruoyu Wang,Xinshu Li,Chen Wang,Lina Yao*

Main category: cs.AI

TL;DR: 论文提出了一种基于因果关系的导航方法（CAN），通过引入因果理解模块提升导航任务性能，实验表明其优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有导航方法通常同时处理所有历史观测数据，忽略了数据内部的关联结构，限制了任务性能的进一步提升。

Method: 通过因果视角分析导航任务，提出CAN方法，引入因果理解模块增强环境理解能力。

Result: 实验证明CAN在多种任务和模拟环境中表现优于基线方法，且因果理解模块在强化学习和监督学习中均有效。

Conclusion: CAN方法通过因果理解模块显著提升了导航任务的性能，且无需额外计算开销。

Abstract: Visual Navigation is a core task in Embodied AI, enabling agents to navigate complex environments toward given objectives. Across diverse settings within Navigation tasks, many necessitate the modelling of sequential data accumulated from preceding time steps. While existing methods perform well, they typically process all historical observations simultaneously, overlooking the internal association structure within the data, which may limit the potential for further improvements in task performance. We address this by examining the unique characteristics of Navigation tasks through the lens of causality, introducing a causal framework to highlight the limitations of conventional sequential methods. Leveraging this insight, we propose Causality-Aware Navigation (CAN), which incorporates a Causal Understanding Module to enhance the agent's environmental understanding capability. Empirical evaluations show that our approach consistently outperforms baselines across various tasks and simulation environments. Extensive ablations studies attribute these gains to the Causal Understanding Module, which generalizes effectively in both Reinforcement and Supervised Learning settings without computational overhead.

</details>


### [42] [Managing Complex Failure Analysis Workflows with LLM-based Reasoning and Acting Agents](https://arxiv.org/abs/2506.15567)
*Aline Dobrovsky,Konstantin Schekotihin,Christian Burmer*

Main category: cs.AI

TL;DR: 本文探讨了基于大型语言模型（LLM）的规划代理（LPA）在故障分析（FA）中的设计与实现，旨在通过AI自动化任务并提升效率。


<details>
  <summary>Details</summary>
Motivation: 故障分析（FA）过程复杂且知识密集，AI组件的引入可自动化任务，但如何高效协调多个AI模型成为挑战。

Method: 设计并实现了一个结合LLM与高级规划能力的LPA，支持自主处理复杂查询、外部数据检索及生成可读响应。

Result: 评估表明LPA在支持FA任务中具有操作有效性和可靠性。

Conclusion: LPA为FA工程师提供了高效的工具，展示了AI在FA中的潜力。

Abstract: Failure Analysis (FA) is a highly intricate and knowledge-intensive process. The integration of AI components within the computational infrastructure of FA labs has the potential to automate a variety of tasks, including the detection of non-conformities in images, the retrieval of analogous cases from diverse data sources, and the generation of reports from annotated images. However, as the number of deployed AI models increases, the challenge lies in orchestrating these components into cohesive and efficient workflows that seamlessly integrate with the FA process.
  This paper investigates the design and implementation of a Large Language Model (LLM)-based Planning Agent (LPA) to assist FA engineers in solving their analysis cases. The LPA integrates LLMs with advanced planning capabilities and external tool utilization, enabling autonomous processing of complex queries, retrieval of relevant data from external systems, and generation of human-readable responses. Evaluation results demonstrate the agent's operational effectiveness and reliability in supporting FA tasks.

</details>


### [43] [The Effect of State Representation on LLM Agent Behavior in Dynamic Routing Games](https://arxiv.org/abs/2506.15624)
*Lyle Goodyear,Rachel Guo,Ramesh Johari*

Main category: cs.AI

TL;DR: 论文提出了一个统一框架，用于在多智能体重复游戏中为LLM智能体构建自然语言状态表示，解决了以往研究中状态表示对行为影响不明确和可比性受限的问题。


<details>
  <summary>Details</summary>
Motivation: LLM在动态环境中作为决策者表现出潜力，但其无状态特性需要自然语言历史表示。以往研究对游戏历史的编码方式缺乏系统性，影响了行为分析和研究比较。

Method: 框架从三个维度（动作信息性、奖励信息性和提示风格）系统化状态表示方法，并应用于动态自私路由游戏。

Result: 研究发现，提供总结性历史表示、后悔信息而非原始收益、以及有限他人动作信息的表示，能更接近博弈论均衡预测。

Conclusion: 自然语言状态表示对LLM智能体行为有显著影响，特定表示方式能提升行为稳定性和均衡匹配度。

Abstract: Large Language Models (LLMs) have shown promise as decision-makers in dynamic settings, but their stateless nature necessitates creating a natural language representation of history. We present a unifying framework for systematically constructing natural language "state" representations for prompting LLM agents in repeated multi-agent games. Previous work on games with LLM agents has taken an ad hoc approach to encoding game history, which not only obscures the impact of state representation on agents' behavior, but also limits comparability between studies. Our framework addresses these gaps by characterizing methods of state representation along three axes: action informativeness (i.e., the extent to which the state representation captures actions played); reward informativeness (i.e., the extent to which the state representation describes rewards obtained); and prompting style (or natural language compression, i.e., the extent to which the full text history is summarized).
  We apply this framework to a dynamic selfish routing game, chosen because it admits a simple equilibrium both in theory and in human subject experiments \cite{rapoport_choice_2009}. Despite the game's relative simplicity, we find that there are key dependencies of LLM agent behavior on the natural language state representation. In particular, we observe that representations which provide agents with (1) summarized, rather than complete, natural language representations of past history; (2) information about regrets, rather than raw payoffs; and (3) limited information about others' actions lead to behavior that more closely matches game theoretic equilibrium predictions, and with more stable game play by the agents. By contrast, other representations can exhibit either large deviations from equilibrium, higher variation in dynamic game play over time, or both.

</details>


### [44] [The AI Policy Module: Developing Computer Science Student Competency in AI Ethics and Policy](https://arxiv.org/abs/2506.15639)
*James Weichert,Daniel Dunlap,Mohammed Farghally,Hoda Eldardiry*

Main category: cs.AI

TL;DR: 论文探讨了AI政策教育在计算机科学课程中的重要性，介绍了AI政策模块的开发与实施效果。


<details>
  <summary>Details</summary>
Motivation: 随着AI技术的广泛应用，AI伦理和政策的需求日益增长，但现有计算机科学课程未能充分培养学生应对这些挑战的能力。

Method: 开发并实施了一个AI政策模块，包括技术作业和前后调查评估。

Result: 学生参与后对AI伦理的关注增加，对参与AI政策讨论的信心提升。

Conclusion: AI政策模块是培养学生应对AI伦理和政策挑战的有效工具。

Abstract: As artificial intelligence (AI) further embeds itself into many settings across personal and professional contexts, increasing attention must be paid not only to AI ethics, but also to the governance and regulation of AI technologies through AI policy. However, the prevailing post-secondary computing curriculum is currently ill-equipped to prepare future AI practitioners to confront increasing demands to implement abstract ethical principles and normative policy preferences into the design and development of AI systems. We believe that familiarity with the 'AI policy landscape' and the ability to translate ethical principles to practices will in the future constitute an important responsibility for even the most technically-focused AI engineers.
  Toward preparing current computer science (CS) students for these new expectations, we developed an AI Policy Module to introduce discussions of AI policy into the CS curriculum. Building on a successful pilot in fall 2024, in this innovative practice full paper we present an updated and expanded version of the module, including a technical assignment on "AI regulation". We present the findings from our pilot of the AI Policy Module 2.0, evaluating student attitudes towards AI ethics and policy through pre- and post-module surveys. Following the module, students reported increased concern about the ethical impacts of AI technologies while also expressing greater confidence in their abilities to engage in discussions about AI regulation. Finally, we highlight the AI Regulation Assignment as an effective and engaging tool for exploring the limits of AI alignment and emphasizing the role of 'policy' in addressing ethical challenges.

</details>


### [45] [Exploring and Exploiting the Inherent Efficiency within Large Reasoning Models for Self-Guided Efficiency Enhancement](https://arxiv.org/abs/2506.15647)
*Weixiang Zhao,Jiahe Guo,Yang Deng,Xingyu Sui,Yulin Hu,Yanyan Zhao,Wanxiang Che,Bing Qin,Tat-Seng Chua,Ting Liu*

Main category: cs.AI

TL;DR: 论文探讨了大型推理模型（LRMs）在复杂问题解决中的效率问题，提出了两种轻量级方法以提升其推理效率，同时保持或提高任务性能。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型在模拟人类思考时存在过度推理（生成冗余内容）的问题，导致效率低下和推理成本增加。研究旨在揭示其低效的根源，并探索提升效率的方法。

Method: 提出了两种方法：1) Efficiency Steering，一种无需训练的激活引导技术；2) Self-Rewarded Efficiency RL，一种动态平衡任务准确性和简洁性的强化学习框架。

Result: 在多个数学推理基准测试中，这两种方法显著减少了推理长度，同时保持或提高了任务性能。

Conclusion: 研究表明，通过引导模型的内在能力，可以显著提升推理效率，而无需牺牲准确性。

Abstract: Recent advancements in large reasoning models (LRMs) have significantly enhanced language models' capabilities in complex problem-solving by emulating human-like deliberative thinking. However, these models often exhibit overthinking (i.e., the generation of unnecessarily verbose and redundant content), which hinders efficiency and inflates inference cost. In this work, we explore the representational and behavioral origins of this inefficiency, revealing that LRMs inherently possess the capacity for more concise reasoning. Empirical analyses show that correct reasoning paths vary significantly in length, and the shortest correct responses often suffice, indicating untapped efficiency potential. Exploiting these findings, we propose two lightweight methods to enhance LRM efficiency. First, we introduce Efficiency Steering, a training-free activation steering technique that modulates reasoning behavior via a single direction in the model's representation space. Second, we develop Self-Rewarded Efficiency RL, a reinforcement learning framework that dynamically balances task accuracy and brevity by rewarding concise correct solutions. Extensive experiments on seven LRM backbones across multiple mathematical reasoning benchmarks demonstrate that our methods significantly reduce reasoning length while preserving or improving task performance. Our results highlight that reasoning efficiency can be improved by leveraging and guiding the intrinsic capabilities of existing models in a self-guided manner.

</details>


### [46] [Embodied Web Agents: Bridging Physical-Digital Realms for Integrated Agent Intelligence](https://arxiv.org/abs/2506.15677)
*Yining Hong,Rui Sun,Bingxuan Li,Xingcheng Yao,Maxine Wu,Alexander Chien,Da Yin,Ying Nian Wu,Zhecan James Wang,Kai-Wei Chang*

Main category: cs.AI

TL;DR: 论文提出了一种新型AI代理范式——Embodied Web Agents，旨在结合物理世界交互与网络规模推理能力，解决跨领域任务。


<details>
  <summary>Details</summary>
Motivation: 当前AI代理多为孤立系统，无法同时处理物理世界交互与网络信息推理，限制了其在跨领域任务中的应用。

Method: 开发了Embodied Web Agents任务环境，整合了3D模拟环境与功能性网络接口，并构建了涵盖多种任务的基准测试。

Result: 实验显示现有AI系统与人类能力存在显著差距，揭示了结合物理认知与网络知识访问的挑战与机遇。

Conclusion: Embodied Web Agents为跨领域智能提供了新方向，相关数据集和代码已公开。

Abstract: AI agents today are mostly siloed - they either retrieve and reason over vast amount of digital information and knowledge obtained online; or interact with the physical world through embodied perception, planning and action - but rarely both. This separation limits their ability to solve tasks that require integrated physical and digital intelligence, such as cooking from online recipes, navigating with dynamic map data, or interpreting real-world landmarks using web knowledge. We introduce Embodied Web Agents, a novel paradigm for AI agents that fluidly bridge embodiment and web-scale reasoning. To operationalize this concept, we first develop the Embodied Web Agents task environments, a unified simulation platform that tightly integrates realistic 3D indoor and outdoor environments with functional web interfaces. Building upon this platform, we construct and release the Embodied Web Agents Benchmark, which encompasses a diverse suite of tasks including cooking, navigation, shopping, tourism, and geolocation - all requiring coordinated reasoning across physical and digital realms for systematic assessment of cross-domain intelligence. Experimental results reveal significant performance gaps between state-of-the-art AI systems and human capabilities, establishing both challenges and opportunities at the intersection of embodied cognition and web-scale knowledge access. All datasets, codes and websites are publicly available at our project page https://embodied-web-agent.github.io/.

</details>
