<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 16]
- [cs.CR](#cs.CR) [Total: 20]
- [cs.AI](#cs.AI) [Total: 31]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [How do Software Engineering Candidates Prepare for Technical Interviews?](https://arxiv.org/abs/2507.02068)
*Brian Bell,Teresa Thomas,Sang Won Lee,Chris Brown*

Main category: cs.SE

TL;DR: 研究探讨了软件工程师技术面试的准备情况，发现候选人缺乏真实场景训练，教育支持不足，导致压力和准备不足。


<details>
  <summary>Details</summary>
Motivation: 技术面试是软件工程师求职的关键环节，但其复杂性与教育内容脱节，候选人难以充分准备。

Method: 通过调查131名正在准备技术面试的候选人，分析其准备方法和教育支持的效果。

Result: 结果显示候选人很少在真实场景中训练，课程未能有效支持准备，导致压力和准备不足。

Conclusion: 研究为利益相关者提供了改进技术面试准备的建议，以帮助候选人更好地应对软件工程师角色的求职挑战。

Abstract: To obtain employment, aspiring software engineers must complete technical
interviews -- a hiring process which involves candidates writing code while
communicating to an audience. However, the complexities of tech interviews are
difficult to prepare for and seldom faced in computing curricula. To this end,
we seek to understand how candidates prepare for technical interviews,
investigating the effects of preparation methods and the role of education. We
distributed a survey to candidates (n = 131) actively preparing for technical
interviews. Our results suggest candidates rarely train in authentic settings
and courses fail to support preparation efforts -- leading to stress and
unpreparedness. Based on our findings, we provide implications for stakeholders
to enhance tech interview preparation for candidates pursuing software
engineering roles.

</details>


### [2] [Structural Code Search using Natural Language Queries](https://arxiv.org/abs/2507.02107)
*Ben Limpanukorn,Yanjun Wang,Zach Patterson,Pranav Garg,Murali Krishna Ramanathan,Xiaofei Ma,Anoop Deoras,Miryung Kim*

Main category: cs.SE

TL;DR: 论文提出了一种结合LLM和结构化代码搜索工具的方法，允许开发者用自然语言进行代码搜索，显著提高了搜索效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 开发者通常使用关键词和正则表达式搜索代码，但结构化代码搜索工具需要学习复杂的DSL，门槛较高。自然语言搜索可以降低使用门槛。

Method: 结合LLM的自然语言理解能力和结构化搜索工具的高效检索能力，将自然语言查询翻译为DSL查询。

Result: 在400个查询的基准测试中，方法表现出色，精确率和召回率在55%-70%之间，显著优于基线方法。

Conclusion: 自然语言驱动的结构化代码搜索是一种高效且易用的方法，为开发者提供了更直观的搜索体验。

Abstract: Searching code is a common task that developers perform to understand APIs,
learn common code patterns, and navigate code. Currently, developers most
commonly search using keywords and regular expressions that are easy to use and
widely available. Beyond keywords and regular expressions, structural code
search tools allow developers to search for code based on its syntactic
structure. This has numerous applications ranging from bug finding to
systematically refactoring code. However, these structural code search tools
operate on queries expressed in domain-specific languages (DSL) that can be
difficult to learn and write. We propose to allow developers to use natural
language to search for code structurally. Expressing queries in natural
language provides an intuitive way to search for code and lowers the barrier to
entry.
  In this work, we develop a novel general approach that combines the reasoning
capabilities of an LLM to interpret natural language search queries with the
power of structural search tools to efficiently and accurately retrieve
relevant code. We then instantiate this approach for two structural code search
DSLs: Semgrep and GQL. In our evaluation, we construct a new benchmark for
structural code search consisting of 400 queries over 10 Java projects. We show
that our approach for structural code search based on translating NL queries to
DSL queries using an LLM is effective and robust, achieving a high precision
and recall ranging from 55% - 70%. Further, our approach significantly
outperforms baselines based on semantic code search and LLM retrievals by up to
57% and 14% on F1 scores.

</details>


### [3] [Can Internal Software Metrics Predict App Popularity at Launch? Yeas! and Nays!](https://arxiv.org/abs/2507.02110)
*Md Nahidul Islam Opu,Fatima Islam Mouri,Rick Kazman,Yuanfang Cai,Shaiful Chowdhury*

Main category: cs.SE

TL;DR: 研究探讨了通过源代码的内部软件指标预测移动应用发布前的受欢迎程度，发现分类模型优于回归模型，多层感知器表现最佳。


<details>
  <summary>Details</summary>
Motivation: 预测应用发布前的受欢迎程度可为开发者提供竞争优势，但现有方法效果有限。

Method: 使用446个开源Android应用的数据集，提取代码指标、代码异味和元数据，评估回归和分类模型。

Result: 回归模型表现不佳，但分类模型（尤其是多层感知器）F1分数达0.72。

Conclusion: 内部代码指标可作为应用受欢迎程度的有限但有用的预测指标，挑战了先前认为其无效的观点。

Abstract: Predicting mobile app popularity before release can provide developers with a
strategic advantage in a competitive marketplace, yet it remains a challenging
problem. This study explores whether internal software metrics, measurable from
source code before deployment, can predict an app's popularity, defined by user
ratings (calculated from user reviews) and DownloadsPerYear (yearly downloads).
Using a dataset of 446 open-source Android apps from F-Droid, we extract a wide
array of features, including system-, class-, and method-level code metrics,
code smells, and app metadata. Additional information, such as user reviews,
download counts, and uses-permission, was collected from the Google Play Store.
We evaluate regression and classification models across three feature sets: a
minimal Size-only baseline, a domain-informed Handpicked set, and a Voting set
derived via feature selection algorithms. Regression models perform poorly due
to skewed data, with low $R^2$ scores. However, when reframed as binary
classification (Popular vs. Unpopular), results improve significantly. The best
model, a Multilayer Perceptron using the Voting set, achieves F1-scores of
0.72. These results suggest that internal code metrics, although limited in
their explanatory power, can serve as useful indicators of app popularity. This
challenges earlier findings that dismissed internal metrics as predictors of
software quality.

</details>


### [4] [A Multimodal Approach Combining Biometrics and Self-Report Instruments for Monitoring Stress in Programming: Methodological Insights](https://arxiv.org/abs/2507.02118)
*Cristina Martinez Montes,Daniela Grassi,Nicole Novielli,Birgit Penzenstadle*

Main category: cs.SE

TL;DR: 研究比较了心理测量压力指标与生物特征指标，并识别了软件工程任务中的压力相关生物特征模式。实验结果显示心理测量工具未检测到压力，而生物特征仅显示EDA阶段性峰值差异。结论是时间限制的压力诱导方法不足。


<details>
  <summary>Details</summary>
Motivation: 传统自我报告工具可能存在偏差，因此研究探索结合心理测量与生物特征指标的替代方法。

Method: 实验包括前测问卷、编程任务中的生物特征监测、后测问卷和简短访谈。

Result: 心理测量工具未检测到压力，访谈反馈压力感受不一，生物特征仅EDA阶段性峰值显著差异。

Conclusion: 时间限制的压力诱导方法不足，为未来研究提供了方法学启示。

Abstract: The study of well-being, stress and other human factors has traditionally
relied on self-report instruments to assess key variables. However, concerns
about potential biases in these instruments, even when thoroughly validated and
standardised, have driven growing interest in alternatives in combining these
measures with more objective methods, such as physiological measures.
  We aimed to (i) compare psychometric stress measures and biometric indicators
and (ii) identify stress-related patterns in biometric data during software
engineering tasks.
  We conducted an experiment where participants completed a pre-survey, then
programmed two tasks wearing biometric sensors, answered brief post-surveys for
each, and finally went through a short exit interview.
  Our results showed diverse outcomes; we found no stress in the psychometric
instruments. Participants in the interviews reported a mix of feeling no stress
and experiencing time pressure. Finally, the biometrics showed a significant
difference only in EDA phasic peaks.
  We conclude that our chosen way of inducing stress by imposing a stricter
time limit was insufficient. We offer methodological insights for future
studies working with stress, biometrics, and psychometric instruments.

</details>


### [5] [Towards Trustworthy Sentiment Analysis in Software Engineering: Dataset Characteristics and Tool Selection](https://arxiv.org/abs/2507.02137)
*Martin Obaidi,Marc Herrmann,Jil Klünder,Kurt Schneider*

Main category: cs.SE

TL;DR: 研究分析了10个开发者通信数据集的语言和统计特征，评估了14种情感分析工具的性能，提出了一种基于数据集特征的工具推荐方法。


<details>
  <summary>Details</summary>
Motivation: 现有情感分析工具在不同平台数据集上表现不一致，影响团队动态分析和需求工程中的AI驱动分析。

Method: 分析10个数据集的语言和统计特征，评估14种工具，提出基于特征的映射方法和问卷。

Result: 数据集特征可优化工具选择，Transformer模型表现稳定但依赖上下文。

Conclusion: 研究为软件工程中情感分析工具选择提供支持，强调需持续评估以适应通信环境变化。

Abstract: Software development relies heavily on text-based communication, making
sentiment analysis a valuable tool for understanding team dynamics and
supporting trustworthy AI-driven analytics in requirements engineering.
However, existing sentiment analysis tools often perform inconsistently across
datasets from different platforms, due to variations in communication style and
content.
  In this study, we analyze linguistic and statistical features of 10 developer
communication datasets from five platforms and evaluate the performance of 14
sentiment analysis tools. Based on these results, we propose a mapping approach
and questionnaire that recommends suitable sentiment analysis tools for new
datasets, using their characteristic features as input.
  Our results show that dataset characteristics can be leveraged to improve
tool selection, as platforms differ substantially in both linguistic and
statistical properties. While transformer-based models such as SetFit and
RoBERTa consistently achieve strong results, tool effectiveness remains
context-dependent. Our approach supports researchers and practitioners in
selecting trustworthy tools for sentiment analysis in software engineering,
while highlighting the need for ongoing evaluation as communication contexts
evolve.

</details>


### [6] [Enhancing COBOL Code Explanations: A Multi-Agents Approach Using Large Language Models](https://arxiv.org/abs/2507.02182)
*Fangjian Lei,Jiawen Liu,Shayan Noei,Ying Zou,Derek Truong,William Alexander*

Main category: cs.SE

TL;DR: 提出了一种多智能体方法，利用两个基于LLM的智能体协作生成COBOL代码的解释，解决了COBOL代码因长度和复杂性超出LLM窗口限制的问题。


<details>
  <summary>Details</summary>
Motivation: COBOL代码维护困难，缺乏文档，且现有LLM方法因COBOL的独特性和代码长度限制效果不佳。

Method: 采用多智能体协作方法，结合代码库上下文信息生成函数、文件和项目级别的解释。

Result: 在14个开源COBOL项目上验证，方法在函数、文件和项目级别的解释效果显著优于基线。

Conclusion: 多智能体方法有效解决了COBOL代码解释的挑战，提升了维护效率。

Abstract: Common Business Oriented Language (COBOL) is a programming language used to
develop business applications that are widely adopted by financial, business,
and government agencies. Due to its age, complexity, and declining number of
COBOL developers, maintaining COBOL codebases is becoming increasingly
challenging. In particular, the lack of documentation makes it difficult for
new developers to effectively understand and maintain COBOL systems. Existing
research utilizes large language models (LLMs) to explain the functionality of
code snippets. However, COBOL presents unique challenges due to its
architectural and syntactical differences, which often cause its code to exceed
the token window size of LLMs. In this work, we propose a multi-agent approach
that leverages two LLM-based agents working collaboratively to generate
explanations for functions, files, and the overall project. These agents
incorporate together by utilizing contextual information from the codebase into
the code explanation prompts. We evaluate the effectiveness of our approach
using 14 open-source, real-world COBOL projects. Our results indicate that our
approach performs significantly better than the baseline in function code
explanation, with improvements of 12.67%, 18.59%, and 0.62% in terms of METEOR,
chrF, and SentenceBERT scores, respectively. At the file level, our approach
effectively explains both short and long COBOL files that exceed the token
window size of LLMs and surpass the baseline by 4.21%, 10.72%, and 14.68% in
explaining the purpose, functionality, and clarity of the generated
explanation. At the project level, our approach generates explanations that
convey the functionality and purpose of 82% of the selected projects.

</details>


### [7] [Precisely Detecting Python Type Errors via LLM-based Unit Test Generation](https://arxiv.org/abs/2507.02318)
*Chen Yang,Ziqi Wang,Yanjie Jiang,Lin Yang,Yuteng Zheng,Jianyi Zhou,Junjie Chen*

Main category: cs.SE

TL;DR: RTED是一种新型的类型感知测试生成技术，用于自动检测Python类型错误，结合类型约束分析和反射验证，显著减少误报并提高检测能力。


<details>
  <summary>Details</summary>
Motivation: Python类型错误常导致运行时故障，现有静态分析工具误报率高，而单元测试生成技术缺乏针对性指导。

Method: RTED结合逐步类型约束分析和反射验证，指导测试生成过程并抑制误报。

Result: RTED在两个基准测试中检测到比现有技术多22-29个类型错误，误报率降低173.9%-245.9%，并在实际项目中发现了12个未知错误。

Conclusion: RTED在检测Python类型错误方面表现出色，显著优于现有技术，且具有实际应用价值。

Abstract: Type errors in Python often lead to runtime failures, posing significant
challenges to software reliability and developer productivity. Existing static
analysis tools aim to detect such errors without execution but frequently
suffer from high false positive rates. Recently, unit test generation
techniques offer great promise in achieving high test coverage, but they often
struggle to produce bug-revealing tests without tailored guidance. To address
these limitations, we present RTED, a novel type-aware test generation
technique for automatically detecting Python type errors. Specifically, RTED
combines step-by-step type constraint analysis with reflective validation to
guide the test generation process and effectively suppress false positives. We
evaluated RTED on two widely-used benchmarks, BugsInPy and TypeBugs.
Experimental results show that RTED can detect 22-29 more benchmarked type
errors than four state-of-the-art techniques. RTED is also capable of producing
fewer false positives, achieving an improvement of 173.9%-245.9% in precision.
Furthermore, RTED successfully discovered 12 previously unknown type errors
from six real-world open-source Python projects.

</details>


### [8] [VeFIA: An Efficient Inference Auditing Framework for Vertical Federated Collaborative Software](https://arxiv.org/abs/2507.02376)
*Chung-ju Huang,Ziqi Zhang,Yinggui Wang,Binghui Wang,Tao Wei,Leye Wang*

Main category: cs.SE

TL;DR: 论文提出了VeFIA框架，用于在垂直联邦学习中审计数据方的推理软件执行正确性，确保隐私和效率。


<details>
  <summary>Details</summary>
Motivation: 现有垂直联邦学习缺乏对数据方推理软件执行正确性的审计机制。

Method: 设计VeFIA框架，利用可信执行环境（TEE）和协调器验证数据方的计算结果。

Result: VeFIA在异常推理超过5.4%时能以99.99%概率检测到异常，且不增加额外延迟。

Conclusion: VeFIA是首个讨论垂直联邦学习中推理软件执行正确性的工作，具有高效和隐私保护的特点。

Abstract: Vertical Federated Learning (VFL) is a distributed AI software deployment
mechanism for cross-silo collaboration without accessing participants' data.
However, existing VFL work lacks a mechanism to audit the execution correctness
of the inference software of the data party. To address this problem, we design
a Vertical Federated Inference Auditing (VeFIA) framework. VeFIA helps the task
party to audit whether the data party's inference software is executed as
expected during large-scale inference without leaking the data privacy of the
data party or introducing additional latency to the inference system. The core
of VeFIA is that the task party can use the inference results from a framework
with Trusted Execution Environments (TEE) and the coordinator to validate the
correctness of the data party's computation results. VeFIA guarantees that, as
long as the abnormal inference exceeds 5.4%, the task party can detect
execution anomalies in the inference software with a probability of 99.99%,
without incurring any additional online inference latency. VeFIA's random
sampling validation achieves 100% positive predictive value, negative
predictive value, and true positive rate in detecting abnormal inference. To
the best of our knowledge, this is the first paper to discuss the correctness
of inference software execution in VFL.

</details>


### [9] [Meta-Fair: AI-Assisted Fairness Testing of Large Language Models](https://arxiv.org/abs/2507.02533)
*Miguel Romero-Arjona,José A. Parejo,Juan C. Alonso,Ana B. Sánchez,Aitor Arrieta,Sergio Segura*

Main category: cs.SE

TL;DR: 论文提出了一种自动化测试大型语言模型（LLM）公平性的新方法Meta-Fair，通过元变形测试和LLM自身能力生成和评估测试用例，显著提高了效率和覆盖率。


<details>
  <summary>Details</summary>
Motivation: 当前LLM公平性测试方法依赖人工、固定模板和精选数据集，资源密集且难以扩展。本文旨在减少对领域特定资源的依赖，扩展现有方法的适用性。

Method: 采用元变形测试（MRs）分析模型输出对输入提示的响应变化，并利用LLM生成测试用例和评估输出。提供三个开源工具支持测试流程。

Result: Meta-Fair在12个预训练LLM、14个MRs和7.9K测试用例中表现优异，平均精度达92%，发现29%的执行存在偏见。LLM作为评估者表现可靠（F1-score达0.79）。

Conclusion: Meta-Fair为LLM公平性测试提供了高效自动化路径，尽管非确定性影响一致性，但通过精心设计MRs可缓解，展现了广阔的应用前景。

Abstract: Fairness--the absence of unjustified bias--is a core principle in the
development of Artificial Intelligence (AI) systems, yet it remains difficult
to assess and enforce. Current approaches to fairness testing in large language
models (LLMs) often rely on manual evaluation, fixed templates, deterministic
heuristics, and curated datasets, making them resource-intensive and difficult
to scale. This work aims to lay the groundwork for a novel, automated method
for testing fairness in LLMs, reducing the dependence on domain-specific
resources and broadening the applicability of current approaches. Our approach,
Meta-Fair, is based on two key ideas. First, we adopt metamorphic testing to
uncover bias by examining how model outputs vary in response to controlled
modifications of input prompts, defined by metamorphic relations (MRs). Second,
we propose exploiting the potential of LLMs for both test case generation and
output evaluation, leveraging their capability to generate diverse inputs and
classify outputs effectively. The proposal is complemented by three open-source
tools supporting LLM-driven generation, execution, and evaluation of test
cases. We report the findings of several experiments involving 12 pre-trained
LLMs, 14 MRs, 5 bias dimensions, and 7.9K automatically generated test cases.
The results show that Meta-Fair is effective in uncovering bias in LLMs,
achieving an average precision of 92% and revealing biased behaviour in 29% of
executions. Additionally, LLMs prove to be reliable and consistent evaluators,
with the best-performing models achieving F1-scores of up to 0.79. Although
non-determinism affects consistency, these effects can be mitigated through
careful MR design. While challenges remain to ensure broader applicability, the
results indicate a promising path towards an unprecedented level of automation
in LLM testing.

</details>


### [10] [LLMREI: Automating Requirements Elicitation Interviews with LLMs](https://arxiv.org/abs/2507.02564)
*Alexander Korn,Samuel Gorsch,Andreas Vogelsang*

Main category: cs.SE

TL;DR: LLMREI是一个基于大语言模型的聊天机器人，旨在自动化需求获取访谈，减少人为错误并提高可扩展性。研究评估了其在模拟访谈中的表现，发现其与人类访谈员表现相当，并能生成上下文相关的问题。


<details>
  <summary>Details</summary>
Motivation: 需求获取访谈依赖熟练的分析师，成本高且易受偏见和沟通问题影响。大语言模型的发展为自动化部分流程提供了新机会。

Method: 研究采用零样本提示和最少到最多提示两种方法优化LLMREI，并在33次模拟访谈中评估其性能。微调方法因初步试验效果不佳被放弃。

Result: LLMREI在减少常见访谈错误、提取相关需求和生成上下文相关问题上表现良好，与人类访谈员相当，尤其适合大规模访谈自动化。

Conclusion: LLMREI在需求获取中展现出潜力，特别适用于大规模利益相关者访谈的自动化。

Abstract: Requirements elicitation interviews are crucial for gathering system
requirements but heavily depend on skilled analysts, making them
resource-intensive, susceptible to human biases, and prone to miscommunication.
Recent advancements in Large Language Models present new opportunities for
automating parts of this process. This study introduces LLMREI, a chat bot
designed to conduct requirements elicitation interviews with minimal human
intervention, aiming to reduce common interviewer errors and improve the
scalability of requirements elicitation. We explored two main approaches,
zero-shot prompting and least-to-most prompting, to optimize LLMREI for
requirements elicitation and evaluated its performance in 33 simulated
stakeholder interviews. A third approach, fine-tuning, was initially considered
but abandoned due to poor performance in preliminary trials. Our study assesses
the chat bot's effectiveness in three key areas: minimizing common interview
errors, extracting relevant requirements, and adapting its questioning based on
interview context and user responses. Our findings indicate that LLMREI makes a
similar number of errors compared to human interviewers, is capable of
extracting a large portion of requirements, and demonstrates a notable ability
to generate highly context-dependent questions. We envision the greatest
benefit of LLMREI in automating interviews with a large number of stakeholders.

</details>


### [11] [Human-Machine Collaboration and Ethical Considerations in Adaptive Cyber-Physical Systems](https://arxiv.org/abs/2507.02578)
*Zoe Pfister*

Main category: cs.SE

TL;DR: 论文探讨了在自适应信息物理系统（CPS）中实现有效人机协作（HMT）的挑战，并提出整合HMT和伦理框架的解决方案。


<details>
  <summary>Details</summary>
Motivation: 自适应CPS需要无缝人机协作，但现有反馈机制（如MAPE-K）难以整合人类操作者，且涉及隐私和伦理问题。

Method: 开发新方法将HMT整合到自适应CPS中，并创建伦理和人类价值观的验证框架。

Result: 提出了整合HMT和伦理框架的方法，以优化人机协作并保护隐私。

Conclusion: 研究为自适应CPS中的人机协作提供了理论和实践基础，强调了伦理和隐私的重要性。

Abstract: Adaptive Cyber-Physical Systems (CPS) are systems that integrate both
physical and computational capabilities, which can adjust in response to
changing parameters. Furthermore, they increasingly incorporate human-machine
collaboration, allowing them to benefit from the individual strengths of humans
and machines. Human-Machine Teaming (HMT) represents the most advanced paradigm
of human-machine collaboration, envisioning seamless teamwork between humans
and machines. However, achieving effective and seamless HMT in adaptive CPS is
challenging. While adaptive CPS already benefit from feedback loops such as
MAPE-K, there is still a gap in integrating humans into these feedback loops
due to different operational cadences of humans and machines. Further, HMT
requires constant monitoring of human operators, collecting potentially
sensitive information about their actions and behavior. Respecting the privacy
and human values of the actors of the CPS is crucial for the success of
human-machine teams. This research addresses these challenges by: (1)
developing novel methods and processes for integrating HMT into adaptive CPS,
focusing on human-machine interaction principles and their incorporation into
adaptive feedback loops found in CPS, and (2) creating frameworks for
integrating, verifying, and validating ethics and human values throughout the
system lifecycle, starting from requirements engineering.

</details>


### [12] [Do Research Software Engineers and Software Engineering Researchers Speak the Same Language?](https://arxiv.org/abs/2507.02665)
*Timo Kehrer,Robert Haines,Guido Juckeland,Shurui Zhou,David E. Bernholdt*

Main category: cs.SE

TL;DR: 研究发现RSEs和SERs在术语使用上存在差异，初步揭示了合作机会，并提出了一种系统化的术语映射方法。


<details>
  <summary>Details</summary>
Motivation: 解决RSEs和SERs因术语差异导致的沟通问题。

Method: 通过调查SE基础概念在RSE社区的解读，识别概念差异和知识缺口。

Result: 发现合作潜力，并提出术语映射方法。

Conclusion: 研究为未来众包扩展和验证奠定了基础。

Abstract: Anecdotal evidence suggests that Research Software Engineers (RSEs) and
Software Engineering Researchers (SERs) often use different terminologies for
similar concepts, creating communication challenges. To better understand these
divergences, we have started investigating how SE fundamentals from the SER
community are interpreted within the RSE community, identifying aligned
concepts, knowledge gaps, and areas for potential adaptation. Our preliminary
findings reveal opportunities for mutual learning and collaboration, and our
systematic methodology for terminology mapping provides a foundation for a
crowd-sourced extension and validation in the future.

</details>


### [13] [RLHGNN: Reinforcement Learning-driven Heterogeneous Graph Neural Network for Next Activity Prediction in Business Processes](https://arxiv.org/abs/2507.02690)
*Jiaxing Wang,Yifeng Yu,Jiahan Song,Bin Cao,Jing Fan,Ji Zhang*

Main category: cs.SE

TL;DR: 论文提出RLHGNN框架，通过异构图结构和强化学习优化业务流程中的下一活动预测。


<details>
  <summary>Details</summary>
Motivation: 现有序列和基于图的方法无法充分捕捉非顺序关系和动态复杂性，需更灵活的方法。

Method: 将事件日志转为异构图，结合强化学习自动选择最优图结构，并使用异构图卷积预测。

Result: 在六个真实数据集上表现优于现有方法，推理延迟低至1毫秒。

Conclusion: RLHGNN能有效建模顺序和非顺序关系，适用于实时业务流程监控。

Abstract: Next activity prediction represents a fundamental challenge for optimizing
business processes in service-oriented architectures such as microservices
environments, distributed enterprise systems, and cloud-native platforms, which
enables proactive resource allocation and dynamic service composition. Despite
the prevalence of sequence-based methods, these approaches fail to capture
non-sequential relationships that arise from parallel executions and
conditional dependencies. Even though graph-based approaches address structural
preservation, they suffer from homogeneous representations and static
structures that apply uniform modeling strategies regardless of individual
process complexity characteristics. To address these limitations, we introduce
RLHGNN, a novel framework that transforms event logs into heterogeneous process
graphs with three distinct edge types grounded in established process mining
theory. Our approach creates four flexible graph structures by selectively
combining these edges to accommodate different process complexities, and
employs reinforcement learning formulated as a Markov Decision Process to
automatically determine the optimal graph structure for each specific process
instance. RLHGNN then applies heterogeneous graph convolution with
relation-specific aggregation strategies to effectively predict the next
activity. This adaptive methodology enables precise modeling of both sequential
and non-sequential relationships in service interactions. Comprehensive
evaluation on six real-world datasets demonstrates that RLHGNN consistently
outperforms state-of-the-art approaches. Furthermore, it maintains an inference
latency of approximately 1 ms per prediction, representing a highly practical
solution suitable for real-time business process monitoring applications. The
source code is available at https://github.com/Joker3993/RLHGNN.

</details>


### [14] [Sustainability Flags for the Identification of Sustainability Posts in Q&A Platforms](https://arxiv.org/abs/2507.02695)
*Sahar Ahmadisakha,Lech Bialek,Mohamed Soliman,Vasilios Andrikopoulos*

Main category: cs.SE

TL;DR: 论文提出了一种通过‘可持续性标志’识别云架构讨论中可持续性概念的方法，并通过实验验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 随着云计算的普及，可持续性在软件架构中的重要性增加，但缺乏明确的识别方法。

Method: 通过主题分析开发‘可持续性标志’，并在实验中评估其识别效果。

Result: 使用标志后，分类为可持续性相关的帖子减少，但确定性和性能显著提高。

Conclusion: 可持续性标志比单纯依赖定义更有效且易理解。

Abstract: In recent years, sustainability in software systems has gained significant
attention, especially with the rise of cloud computing and the shift towards
cloud-based architectures. This shift has intensified the need to identify
sustainability in architectural discussions to take informed architectural
decisions. One source to see these decisions is in online Q&A forums among
practitioners' discussions. However, recognizing sustainability concepts within
software practitioners' discussions remains challenging due to the lack of
clear and distinct guidelines for this task. To address this issue, we
introduce the notion of sustainability flags as pointers in relevant
discussions, developed through thematic analysis of multiple sustainability
best practices from cloud providers. This study further evaluates the
effectiveness of these flags in identifying sustainability within cloud
architecture posts, using a controlled experiment. Preliminary results suggest
that the use of flags results in classifying fewer posts as
sustainability-related compared to a control group, with moderately higher
certainty and significantly improved performance. Moreover, sustainability
flags are perceived as more useful and understandable than relying solely on
definitions for identifying sustainability.

</details>


### [15] [Legal Requirements Translation from Law](https://arxiv.org/abs/2507.02846)
*Anmol Singhal,Travis Breaux*

Main category: cs.SE

TL;DR: 提出了一种基于文本蕴含和上下文学习的自动化方法，用于从法律文本中生成可编码为Python代码的规范表示，减少了对大量手动标注数据的依赖。


<details>
  <summary>Details</summary>
Motivation: 小型组织和初创公司缺乏法律专业知识，手动提取法律文本的元数据以确保软件合规性是一项繁重任务。现有方法未考虑元数据属性间的交互关系，且依赖手动标注或启发式机器学习，泛化能力不足。

Method: 采用文本蕴含和上下文学习技术，设计了一个领域特定的元模型（Python类结构），捕获法律文本的结构和语义元数据及其相互关系。

Result: 在13个美国州数据泄露通知法律上评估，生成的表示通过约89.4%的测试用例，精确率和召回率分别为82.2和88.7。

Conclusion: 该方法有效减少了手动标注需求，提高了对新法规的适用性，为法律合规性自动化提供了可行方案。

Abstract: Software systems must comply with legal regulations, which is a
resource-intensive task, particularly for small organizations and startups
lacking dedicated legal expertise. Extracting metadata from regulations to
elicit legal requirements for software is a critical step to ensure compliance.
However, it is a cumbersome task due to the length and complex nature of legal
text. Although prior work has pursued automated methods for extracting
structural and semantic metadata from legal text, key limitations remain: they
do not consider the interplay and interrelationships among attributes
associated with these metadata types, and they rely on manual labeling or
heuristic-driven machine learning, which does not generalize well to new
documents. In this paper, we introduce an approach based on textual entailment
and in-context learning for automatically generating a canonical representation
of legal text, encodable and executable as Python code. Our representation is
instantiated from a manually designed Python class structure that serves as a
domain-specific metamodel, capturing both structural and semantic legal
metadata and their interrelationships. This design choice reduces the need for
large, manually labeled datasets and enhances applicability to unseen
legislation. We evaluate our approach on 13 U.S. state data breach notification
laws, demonstrating that our generated representations pass approximately 89.4%
of test cases and achieve a precision and recall of 82.2 and 88.7,
respectively.

</details>


### [16] [Requirements Elicitation Follow-Up Question Generation](https://arxiv.org/abs/2507.02858)
*Yuchen Shen,Anmol Singhal,Travis Breaux*

Main category: cs.SE

TL;DR: 论文探讨了使用GPT-4o生成需求访谈中的后续问题，发现其生成的问题在清晰度、相关性和信息量上不亚于人工问题，且在指导后表现更优。


<details>
  <summary>Details</summary>
Motivation: 解决访谈者在需求获取中面临的挑战，如领域不熟悉、认知负荷和信息过载，利用LLM提升访谈质量。

Method: 基于常见访谈错误类型框架，使用GPT-4o生成后续问题，并通过对照实验评估其效果。

Result: LLM生成的问题在无指导时与人工问题相当，在指导后表现更优。

Conclusion: LLM可实时提升需求访谈的质量和效率。

Abstract: Interviews are a widely used technique in eliciting requirements to gather
stakeholder needs, preferences, and expectations for a software system.
Effective interviewing requires skilled interviewers to formulate appropriate
interview questions in real time while facing multiple challenges, including
lack of familiarity with the domain, excessive cognitive load, and information
overload that hinders how humans process stakeholders' speech. Recently, large
language models (LLMs) have exhibited state-of-the-art performance in multiple
natural language processing tasks, including text summarization and entailment.
To support interviewers, we investigate the application of GPT-4o to generate
follow-up interview questions during requirements elicitation by building on a
framework of common interviewer mistake types. In addition, we describe methods
to generate questions based on interviewee speech. We report a controlled
experiment to evaluate LLM-generated and human-authored questions with minimal
guidance, and a second controlled experiment to evaluate the LLM-generated
questions when generation is guided by interviewer mistake types. Our findings
demonstrate that, for both experiments, the LLM-generated questions are no
worse than the human-authored questions with respect to clarity, relevancy, and
informativeness. In addition, LLM-generated questions outperform human-authored
questions when guided by common mistakes types. This highlights the potential
of using LLMs to help interviewers improve the quality and ease of requirements
elicitation interviews in real time.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [17] [MGC: A Compiler Framework Exploiting Compositional Blindness in Aligned LLMs for Malware Generation](https://arxiv.org/abs/2507.02057)
*Lu Yan,Zhuo Zhang,Xiangzhe Xu,Shengwei An,Guangyu Shen,Zhou Xuan,Xuan Chen,Xiangyu Zhang*

Main category: cs.CR

TL;DR: 论文提出了一种名为MGC的新型框架，通过模块化分解和规避对齐生成恶意软件，揭示了组合攻击对齐AI系统的风险。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）降低了软件开发的门槛，但也为恶意软件开发提供了便利。现有的对齐机制仅评估单个提示，忽视了恶意操作可通过分解为看似良性的子任务实现的漏洞。

Method: MGC框架利用模块化分解和规避对齐生成恶意软件，采用专门的MDIR中间表示连接高级恶意意图和看似良性的代码片段。

Result: MGC在三个基准数据集上生成的恶意软件功能正确性比越狱方法高365.79%，比地下服务高78.07%，并能复现和增强16个真实恶意软件样本。

Conclusion: MGC揭示了组合攻击对齐AI系统的风险，为安全研究人员提供了重要见解。

Abstract: Large language models (LLMs) have democratized software development, reducing
the expertise barrier for programming complex applications. This accessibility
extends to malicious software development, raising significant security
concerns. While LLM providers have implemented alignment mechanisms to prevent
direct generation of overtly malicious code, these safeguards predominantly
evaluate individual prompts in isolation, overlooking a critical vulnerability:
malicious operations can be systematically decomposed into benign-appearing
sub-tasks. In this paper, we introduce the Malware Generation Compiler (MGC), a
novel framework that leverages this vulnerability through modular decomposition
and alignment-evasive generation. MGC employs a specialized Malware Description
Intermediate Representation (MDIR) to bridge high-level malicious intents and
benign-appearing code snippets. Extensive evaluation demonstrates that our
attack reliably generates functional malware across diverse task specifications
and categories, outperforming jailbreaking methods by +365.79% and underground
services by +78.07% in correctness on three benchmark datasets. Case studies
further show that MGC can reproduce and even enhance 16 real-world malware
samples. This work provides critical insights for security researchers by
exposing the risks of compositional attacks against aligned AI systems.
Demonstrations are available at
https://sites.google.com/view/malware-generation-compiler.

</details>


### [18] [Can Artificial Intelligence solve the blockchain oracle problem? Unpacking the Challenges and Possibilities](https://arxiv.org/abs/2507.02125)
*Giulio Caldarelli*

Main category: cs.CR

TL;DR: 本文探讨了人工智能（AI）如何辅助解决区块链预言机问题，指出AI可作为补充工具提升数据质量和系统韧性，但无法完全消除对链外数据的依赖。


<details>
  <summary>Details</summary>
Motivation: 区块链预言机问题限制了去中心化应用的发展，现有方法未能彻底解决链外数据可靠注入的挑战。

Method: 通过分析学术文献和实践案例，评估AI技术（如异常检测、语言事实提取、动态声誉建模和对抗性抵抗）在预言机系统中的应用。

Result: AI能显著提升数据质量、来源选择和系统韧性，但仍需依赖不可验证的链外输入。

Conclusion: AI应被视为预言机设计中的补充层，而非信任假设的替代品。

Abstract: The blockchain oracle problem, which refers to the challenge of injecting
reliable external data into decentralized systems, remains a fundamental
limitation to the development of trustless applications. While recent years
have seen a proliferation of architectural, cryptographic, and economic
strategies to mitigate this issue, no one has yet fully resolved the
fundamental question of how a blockchain can gain knowledge about the off-chain
world. In this position paper, we critically assess the role artificial
intelligence (AI) can play in tackling the oracle problem. Drawing from both
academic literature and practitioner implementations, we examine how AI
techniques such as anomaly detection, language-based fact extraction, dynamic
reputation modeling, and adversarial resistance can enhance oracle systems. We
observe that while AI introduces powerful tools for improving data quality,
source selection, and system resilience, it cannot eliminate the reliance on
unverifiable off-chain inputs. Therefore, this study supports the idea that AI
should be understood as a complementary layer of inference and filtering within
a broader oracle design, not a substitute for trust assumptions.

</details>


### [19] [ARMOUR US: Android Runtime Zero-permission Sensor Usage Monitoring from User Space](https://arxiv.org/abs/2507.02177)
*Yan Long,Jiancong Cui,Yuqing Yang,Tobias Alam,Zhiqiang Lin,Kevin Fu*

Main category: cs.CR

TL;DR: 该研究开发了ARMOUR工具，用于监控Android零权限传感器的访问，以检测隐私泄露和滥用模式。


<details>
  <summary>Details</summary>
Motivation: 零权限传感器（如加速度计）可能被滥用导致隐私泄露，但现有方法难以让用户或研究人员有效监控其使用情况。

Method: 通过利用Android的采样率变化和收敛行为，开发了ARMOUR工具，实现用户空间的运行时监控。

Result: 在1,448个商业应用中，ARMOUR成功检测到混淆代码中的传感器使用，并发现50%的非传感器相关应用访问了多个零权限传感器。

Conclusion: ARMOUR为隐私保护用户和研究人员提供了有效的监控工具，同时揭示了Android政策变化对零权限传感器的影响及遗留问题。

Abstract: This work investigates how to monitor access to Android zero-permission
sensors which could cause privacy leakage to users. Moreover, monitoring such
sensitive access allows security researchers to characterize potential sensor
abuse patterns. Zero-permission sensors such as accelerometers have become an
indispensable part of Android devices. The critical information they provide
has attracted extensive research investigating how data collectors could
capture more sensor data to enable both benign and exploitative applications.
In contrast, little work has explored how to enable data providers, such as end
users, to understand sensor usage. While existing methods such as static
analysis and hooking-based dynamic analysis face challenges of requiring
complicated development chains, rooting privilege, and app-specific reverse
engineering analysis, our work aims to bridge this gap by developing ARMOUR for
user-space runtime monitoring, leveraging the intrinsic sampling rate variation
and convergence behaviors of Android. ARMOUR enables privacy-aware users to
easily monitor how third-party apps use sensor data and support security
researchers to perform rapid app-agnostic sensor access analysis. Our
evaluation with 1,448 commercial applications shows the effectiveness of ARMOUR
in detecting sensor usage in obfuscated code and other conditions, and observes
salient sensor abuse patterns such as 50% of apps from seemingly
sensor-independent categories accessing data of multiple zero-permission
sensors. We analyze the impact of Android's recent policy changes on
zero-permission sensors and remaining technical and regulatory problems.

</details>


### [20] [Extended c-differential distinguishers of full 9 and reduced-round Kuznyechik cipher](https://arxiv.org/abs/2507.02181)
*Pantelimon Stanica,Ranit Dutta,Bimal Mandal*

Main category: cs.CR

TL;DR: 本文提出了一种新的截断内部$c$-差分密码分析方法，首次实现了$c$-差分均匀性在分组密码中的实际应用。通过将乘法操作作用于输入而非输出，解决了传统方法中破坏密码结构的问题，并证明了内部与外部$c$-差分均匀性的对偶关系。


<details>
  <summary>Details</summary>
Motivation: 传统$c$-差分均匀性方法中，乘法操作破坏密码结构（如密钥加法），限制了其实际应用。本文旨在解决这一问题。

Method: 提出内部$c$-差分方法，将乘法作用于输入$(F(cx\oplus a), F(x))$，并证明其与外部$c$-差分均匀性的对偶关系。

Result: 通过统计计算框架，对9轮Kuznyechik密码进行截断$c$-差分分析，发现显著非随机性，偏差比达1.7倍，p值低至$1.85 \times 10^{-3}$。

Conclusion: 该方法首次实现了对9轮Kuznyechik密码的实际区分攻击，揭示了其安全性不足。

Abstract: This paper introduces {\em truncated inner $c$-differential cryptanalysis}, a
novel technique that for the first time enables the practical application of
$c$-differential uniformity to block ciphers. While Ellingsen et al. (IEEE
Trans. Inf. Theory, 2020) established the notion of $c$-differential uniformity
using $(F(x\oplus a), cF(x))$, a key challenge remained: multiplication by $c$
disrupts the structural properties essential for block cipher analysis,
particularly key addition.
  We resolve this challenge by developing an \emph{inner} $c$-differential
approach where multiplication by $c$ affects the input: $(F(cx\oplus a),
F(x))$. We prove that the inner $c$-differential uniformity of a function $F$
equals the outer $c$-differential uniformity of $F^{-1}$, establishing a
fundamental duality. This modification preserves cipher structure while
enabling practical cryptanalytic applications.
  Our main contribution is a comprehensive multi-faceted
statistical-computational framework, implementing truncated $c$-differential
analysis against the full 9-round Kuznyechik cipher (the inner
$c$-differentials are immune to the key whitening at the backend). Through
extensive computational analysis involving millions of differential pairs, we
demonstrate statistically significant non-randomness across all tested round
counts. For the full 9-round cipher, we identify multiple configurations
triggering critical security alerts, with bias ratios reaching $1.7\times$ and
corrected p-values as low as $1.85 \times 10^{-3}$, suggesting insufficient
security margin against this new attack vector. This represents the first
practical distinguisher against the full 9-round Kuznyechik.

</details>


### [21] [EIM-TRNG: Obfuscating Deep Neural Network Weights with Encoding-in-Memory True Random Number Generator via RowHammer](https://arxiv.org/abs/2507.02206)
*Ranyang Zhou,Abeer Matar A. Almalky,Gamana Aragonda,Sabbir Ahmed,Filip Roth Trønnes-Christensen,Adnan Siraj Rakin,Shaahin Angizi*

Main category: cs.CR

TL;DR: 提出了一种基于DRAM物理随机性的新型TRNG（EIM-TRNG），用于保护DNN模型参数的安全性。


<details>
  <summary>Details</summary>
Motivation: 硬件安全的TRNG在保护DNN模型参数（如权重）的完整性、隐私和知识产权方面至关重要。

Method: 利用DRAM单元在RowHammer干扰下的物理随机性，设计了一种名为EIM-TRNG的编码存储器TRNG。

Result: 验证了DRAM作为可靠熵源的有效性，并成功应用于DNN权重数据的加密。

Conclusion: EIM-TRNG为低成本硬件安全提供了新方向，特别适合保护机器学习模型。

Abstract: True Random Number Generators (TRNGs) play a fundamental role in hardware
security, cryptographic systems, and data protection. In the context of Deep
NeuralNetworks (DNNs), safeguarding model parameters, particularly weights, is
critical to ensure the integrity, privacy, and intel-lectual property of AI
systems. While software-based pseudo-random number generators are widely used,
they lack the unpredictability and resilience offered by hardware-based TRNGs.
In this work, we propose a novel and robust Encoding-in-Memory TRNG called
EIM-TRNG that leverages the inherent physical randomness in DRAM cell behavior,
particularly under RowHammer-induced disturbances, for the first time. We
demonstrate how the unpredictable bit-flips generated through carefully
controlled RowHammer operations can be harnessed as a reliable entropy source.
Furthermore, we apply this TRNG framework to secure DNN weight data by encoding
via a combination of fixed and unpredictable bit-flips. The encrypted data is
later decrypted using a key derived from the probabilistic flip behavior,
ensuring both data confidentiality and model authenticity. Our results validate
the effectiveness of DRAM-based entropy extraction for robust, low-cost
hardware security and offer a promising direction for protecting machine
learning models at the hardware level.

</details>


### [22] [Linearly Homomorphic Ring Signature Scheme over Lattices](https://arxiv.org/abs/2507.02281)
*Heng Guo,Kun Tian,Fengxia Liu,Zhiyong Zheng*

Main category: cs.CR

TL;DR: 本文提出了首个基于格的线性同态环签名方案，解决了现有技术中缺乏可行方案的问题，并在标准模型下证明了其安全性。


<details>
  <summary>Details</summary>
Motivation: 结合环签名的强匿名性和同态签名的可计算性，为需要匿名数据来源和可验证同态计算的场景（如机密区块链交易和安全多方计算）提供解决方案。

Method: 提出了一种基于格的线性同态环签名方案，并在标准模型下基于小整数解（SIS）假设证明了其安全性。

Result: 方案在完全密钥暴露下实现了强匿名性，并能够抵抗内部腐败攻击，为后量子安全提供了统一框架。

Conclusion: 该方案推动了隐私增强的同态计算的发展，为相关应用提供了后量子安全的解决方案。

Abstract: Homomorphic ring signature schemes combine the strong anonymity of ring
signatures with the computability of homomorphic signatures, demonstrating
significant potential in scenarios requiring both anonymous data provenance and
verifiable homomorphic computation (e.g., confidential blockchain transactions
and secure multi-party computation). However, no feasible homomorphic ring
signature scheme currently exists.
  In this work, we propose the first lattice-based linearly homomorphic ring
signature scheme. Proven secure in the standard model under the small integer
solution (SIS) assumption, our scheme achieves strong anonymity under full key
exposure and unforgeability against insider corruption attacks. As the first
unified framework for ring signatures and linear homomorphic signatures, this
construction provides a post-quantum-secure solution for the aforementioned
applications, advancing the development of privacy-enhanced homomorphic
computation.

</details>


### [23] [Rethinking Broken Object Level Authorization Attacks Under Zero Trust Principle](https://arxiv.org/abs/2507.02309)
*Anbin Wu,Zhiyong Feng,Ruitao Feng*

Main category: cs.CR

TL;DR: BOLAZ是一个基于零信任原则的防御框架，用于解决RESTful API中的BOLA漏洞，通过静态污点跟踪和资源ID分析，有效防止水平权限提升。


<details>
  <summary>Details</summary>
Motivation: RESTful API中的数据交换容易暴露敏感资源，BOLA漏洞是API安全中的首要问题，需要一种更有效的防御方法。

Method: BOLAZ通过静态污点跟踪分析资源ID的数据流，将API分为生产者和消费者，并确定授权边界。

Result: 在10个GitHub项目上验证，BOLAZ成功防御了CVE中的漏洞，并发现了35个新的BOLA漏洞。

Conclusion: BOLAZ是一种基于授权逻辑的实用防御方法，适用于实际部署。

Abstract: RESTful APIs facilitate data exchange between applications, but they also
expose sensitive resources to potential exploitation. Broken Object Level
Authorization (BOLA) is the top vulnerability in the OWASP API Security Top 10,
exemplifies a critical access control flaw where attackers manipulate API
parameters to gain unauthorized access. To address this, we propose BOLAZ, a
defense framework grounded in zero trust principles. BOLAZ analyzes the data
flow of resource IDs, pinpointing BOLA attack injection points and determining
the associated authorization intervals to prevent horizontal privilege
escalation. Our approach leverages static taint tracking to categorize APIs
into producers and consumers based on how they handle resource IDs. By mapping
the propagation paths of resource IDs, BOLAZ captures the context in which
these IDs are produced and consumed, allowing for precise identification of
authorization boundaries. Unlike defense methods based on common authorization
models, BOLAZ is the first authorization-guided method that adapts defense
rules based on the system's best-practice authorization logic. We validate
BOLAZ through empirical research on 10 GitHub projects. The results demonstrate
BOLAZ's effectiveness in defending against vulnerabilities collected from CVE
and discovering 35 new BOLA vulnerabilities in the wild, demonstrating its
practicality in real-world deployments.

</details>


### [24] [PII Jailbreaking in LLMs via Activation Steering Reveals Personal Information Leakage](https://arxiv.org/abs/2507.02332)
*Krishna Kanth Nakka,Xue Jiang,Xuebing Zhou*

Main category: cs.CR

TL;DR: 论文研究了通过操纵激活来绕过LLM对齐的隐私越狱方法，发现通过调整注意力头可以诱导模型泄露敏感信息。


<details>
  <summary>Details</summary>
Motivation: 探讨如何通过操纵LLM内部激活来绕过其对齐机制，以揭示模型是否存储并可能泄露隐私信息。

Method: 使用轻量级线性探针识别与隐私拒绝行为相关的注意力头，并通过调整这些头的激活诱导模型生成非拒绝响应。

Result: 实验显示，调整后的模型在95%的情况下会泄露敏感信息，其中50%以上为真实个人信息。

Conclusion: 研究表明，LLM中存储的隐私信息可以通过针对性操纵内部激活被提取出来。

Abstract: This paper investigates privacy jailbreaking in LLMs via steering, focusing
on whether manipulating activations can bypass LLM alignment and alter response
behaviors to privacy related queries (e.g., a certain public figure's sexual
orientation). We begin by identifying attention heads predictive of refusal
behavior for private attributes (e.g., sexual orientation) using lightweight
linear probes trained with privacy evaluator labels. Next, we steer the
activations of a small subset of these attention heads guided by the trained
probes to induce the model to generate non-refusal responses. Our experiments
show that these steered responses often disclose sensitive attribute details,
along with other private information about data subjects such as life events,
relationships, and personal histories that the models would typically refuse to
produce. Evaluations across four LLMs reveal jailbreaking disclosure rates of
at least 95%, with more than 50% on average of these responses revealing true
personal information. Our controlled study demonstrates that private
information memorized in LLMs can be extracted through targeted manipulation of
internal activations.

</details>


### [25] [Evaluating Language Models For Threat Detection in IoT Security Logs](https://arxiv.org/abs/2507.02390)
*Jorge J. Tejero-Fernández,Alfonso Sánchez-Macián*

Main category: cs.CR

TL;DR: 本文提出了一种利用微调大语言模型（LLMs）进行物联网安全日志异常检测和缓解建议的流程，相比传统机器学习方法在多类攻击分类中表现更优。


<details>
  <summary>Details</summary>
Motivation: 物联网安全日志分析对网络安全至关重要，但传统方法在多类攻击分类上存在局限，LLMs提供了改进的可能性。

Method: 比较了三种开源LLMs在零样本、少样本提示和微调策略下的表现，并结合MITRE CAPEC映射和物联网特定缓解动作进行微调。

Result: LLMs在多类攻击分类上优于传统机器学习基线模型，并能提供检测与缓解建议的联合指导。

Conclusion: LLMs在物联网安全日志分析中具有潜力，尤其在多类攻击分类和缓解建议方面表现突出。

Abstract: Log analysis is a relevant research field in cybersecurity as they can
provide a source of information for the detection of threats to networks and
systems. This paper presents a pipeline to use fine-tuned Large Language Models
(LLMs) for anomaly detection and mitigation recommendation using IoT security
logs. Utilizing classical machine learning classifiers as a baseline, three
open-source LLMs are compared for binary and multiclass anomaly detection, with
three strategies: zero-shot, few-shot prompting and fine-tuning using an IoT
dataset. LLMs give better results on multi-class attack classification than the
corresponding baseline models. By mapping detected threats to MITRE CAPEC,
defining a set of IoT-specific mitigation actions, and fine-tuning the models
with those actions, the models are able to provide a combined detection and
recommendation guidance.

</details>


### [26] [CyberRAG: An agentic RAG cyber attack classification and reporting tool](https://arxiv.org/abs/2507.02424)
*Francesco Blefari,Cristian Cosentino,Francesco Aurelio Pironti,Angelo Furfaro,Fabrizio Marozzo*

Main category: cs.CR

TL;DR: CyberRAG是一个基于代理的模块化RAG框架，用于实时分类、解释和结构化报告网络攻击，显著降低误报率并提高可解释性。


<details>
  <summary>Details</summary>
Motivation: 传统IDS/IPS系统生成大量警报，机器学习检测器误报率高，标准RAG管道检索无关上下文且无法合理解释预测。CyberRAG旨在解决这些问题。

Method: 采用模块化、基于代理的RAG框架，包括微调的专业分类器、工具适配器和迭代检索-推理循环，动态优化威胁标签和解释。

Result: CyberRAG在每类攻击中准确率超过94%，最终分类准确率达94.92%，生成的解释在BERTScore和GPT-4评估中表现优异。

Conclusion: CyberRAG展示了代理化、专业化RAG在高检测准确性和可信解释方面的潜力，为半自主网络防御工作流提供了实用且可扩展的解决方案。

Abstract: Intrusion Detection and Prevention Systems (IDS/IPS) in large enterprises can
generate hundreds of thousands of alerts per hour, overwhelming security
analysts with logs that demand deep, rapidly evolving domain expertise.
Conventional machine-learning detectors trim the alert volume but still yield
high false-positive rates, while standard single-pass Retrieval-Augmented
Generation (RAG) pipelines often retrieve irrelevant context and fail to
justify their predictions. To overcome these shortcomings, we present CyberRAG,
a modular, agent-based RAG framework that delivers real-time classification,
explanation, and structured reporting for cyber-attacks. A central LLM agent
orchestrates (i) a pool of fine-tuned specialized classifiers, each tailored to
a distinct attack family; (ii) tool adapters for enrichment and alerting; and
(iii) an iterative retrieval-and-reason loop that continuously queries a
domain-specific knowledge base until the evidence is both relevant and
self-consistent. Unlike traditional RAG systems, CyberRAG embraces an agentic
design that enables dynamic control flow and adaptive reasoning. This
agent-centric architecture refines its threat labels and natural-language
justifications autonomously, reducing false positives and enhancing
interpretability. The framework is fully extensible: new attack types can be
supported by simply adding a classifier without retraining the core agent.
CyberRAG has been evaluated achieving over 94% accuracy per class and pushing
final classification accuracy to 94.92% through semantic orchestration.
Generated explanations score up to 0.94 in BERTScore and 4.9/5 in GPT-4-based
expert evaluation. These results show that agentic, specialist-oriented RAG can
pair high detection accuracy with trustworthy, SOC-ready prose, offering a
practical and scalable path toward semi-autonomous cyber-defence workflows.

</details>


### [27] [Effectively Identifying Wi-Fi Devices through State Transitions](https://arxiv.org/abs/2507.02478)
*Melissa Safari,Abhishek K. Mishra,Mathieu Cunche*

Main category: cs.CR

TL;DR: 论文提出了一种基于Wi-Fi管理帧行为动态的设备指纹识别框架，通过有限状态机和矩阵表示方法，显著提高了设备识别的准确性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要依赖探测请求，忽视了管理帧及其动态变化，导致在高密度移动环境中识别效果不佳。

Method: 将设备行为建模为有限状态机，利用矩阵编码状态转移频率和延迟特征，生成紧凑特征向量进行相似性比较。

Result: 在多样化现实环境中，方法对非随机化设备的识别准确率超过86%，且通过时间聚合进一步优化。

Conclusion: 该方法能够大规模唯一且一致地识别设备，性能优于现有技术。

Abstract: Wi-Fi management frames reveal structured communication patterns that persist
even under randomization of MAC addresses. Prior approaches to associating
randomized MAC addresses with devices primarily focus on probe requests,
overlooking the broader set of management frames and their transition dynamics.
This narrow focus limits their robustness in dense, real-world environments
with high device mobility, where probe activity alone fails to yield stable and
distinctive signatures. In this paper, we present a novel framework for
fingerprinting Wi-Fi devices based on behavioral dynamics extracted from
passively observed management frames. We model each device's behavior as a
finite state machine and introduce matrix-based representations that encode
both structural (state transition frequencies) and temporal (inter-state
delays) characteristics. These matrices are embedded into compact feature
vectors, enabling efficient similarity comparison. Through extensive evaluation
in diverse real-world settings, our method achieves over 86% identification
accuracy for non-randomized devices using only Wi-Fi management frames, with
further improvements observed through temporal burst aggregation. Our findings
are sufficient to uniquely and consistently characterize devices at scale,
outperforming the state-of-the-art.

</details>


### [28] [A 10-bit S-box generated by Feistel construction from cellular automata](https://arxiv.org/abs/2507.02489)
*Thomas Prévost,Bruno Martin*

Main category: cs.CR

TL;DR: 提出了一种基于Feistel结构和5细胞自动机的新型10位S盒，其安全性优于或与AES S盒相当。


<details>
  <summary>Details</summary>
Motivation: 设计一种新的10位S盒，以替代现有密码（如ASCON）中的5位替换，提升安全性。

Method: 使用Feistel结构和5细胞自动机生成子置换，并通过经验测试选择自动机规则和网络布局。

Result: 生成的10位S盒在安全性上优于或与标准AES S盒相当。

Conclusion: 该S盒可用于替代现有密码中的5位替换，具有潜在应用价值。

Abstract: We propose a new 10-bit S-box generated from a Feistel construction. The
subpermutations are generated by a 5-cell cellular automaton based on a unique
well-chosen rule and bijective affine transformations. In particular, the
cellular automaton rule is chosen based on empirical tests of its ability to
generate good pseudorandom output on a ring cellular automaton. Similarly,
Feistel's network layout is based on empirical data regarding the quality of
the output S-box. We perform cryptanalysis of the generated 10-bit S-box, and
we find security properties comparable to or sometimes even better than those
of the standard AES S-box. We believe that our S-box could be used to replace
the 5-bit substitution of ciphers like ASCON.

</details>


### [29] [Real-Time Monitoring and Transparency in Pizza Production Using IoT and Blockchain](https://arxiv.org/abs/2507.02536)
*Azmat Ullah,Maria Ilaria Lunesu,Lodovica Marchesi,Roberto Tonelli*

Main category: cs.CR

TL;DR: 区块链和物联网结合的披萨生产监控系统，提升透明度和效率。


<details>
  <summary>Details</summary>
Motivation: 解决披萨生产中的数据安全和实时监控问题，确保食品安全和合规性。

Method: 使用物联网设备实时监测温湿度，区块链确保数据安全，树莓派处理数据并与智能合约交互。

Result: 实验显示原料管理改善、浪费减少、厨房效率提高。

Conclusion: 系统有效提升了披萨生产的透明度和可追溯性，同时优化了运营效率。

Abstract: This paper presents a blockchain-based Internet of Things (IoT) system for
monitoring pizza production in restaurants. IoT devices track temperature and
humidity in real-time, while blockchain ensures secure and tamper-proof data. A
Raspberry Pi processes sensor data, captures images, triggers alerts, and
interacts with smart contracts. The system detects abnormal conditions,
enabling quick responses. Blockchain adds transparency and traceability,
supporting compliance and audits. Experiments show improved ingredient
management, reduced waste, and increased kitchen efficiency.

</details>


### [30] [Alleviating Attack Data Scarcity: SCANIA's Experience Towards Enhancing In-Vehicle Cyber Security Measures](https://arxiv.org/abs/2507.02607)
*Frida Sundfeldt,Bianca Widstam,Mahshid Helali Moghadam,Kuo-Yun Liang,Anders Vesterberg*

Main category: cs.CR

TL;DR: 本文提出了一种上下文感知的攻击数据生成器，用于生成高质量的攻击数据，以解决车载网络安全中数据稀缺的问题，并通过实验验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 随着联网车辆的数字化发展，网络安全风险增加，但受限于安全、成本和伦理问题，真实攻击场景数据稀缺，亟需高效生成攻击数据的方法。

Method: 开发了一种基于参数化攻击模型和CAN消息解码的上下文感知攻击数据生成器，支持多种攻击类型（如DoS、模糊、欺骗等），并通过调整攻击强度提高数据真实性。

Result: 生成的攻击数据在入侵检测系统（IDS）案例研究中表现出高效性和可扩展性，基于该数据的深度学习模型具有高检测和分类能力。

Conclusion: 该方法能有效生成高质量攻击数据，支持车载网络安全研究，同时提供了数据真实性和应用方面的见解。

Abstract: The digital evolution of connected vehicles and the subsequent security risks
emphasize the critical need for implementing in-vehicle cyber security measures
such as intrusion detection and response systems. The continuous advancement of
attack scenarios further highlights the need for adaptive detection mechanisms
that can detect evolving, unknown, and complex threats. The effective use of
ML-driven techniques can help address this challenge. However, constraints on
implementing diverse attack scenarios on test vehicles due to safety, cost, and
ethical considerations result in a scarcity of data representing attack
scenarios. This limitation necessitates alternative efficient and effective
methods for generating high-quality attack-representing data. This paper
presents a context-aware attack data generator that generates attack inputs and
corresponding in-vehicle network log, i.e., controller area network (CAN) log,
representing various types of attack including denial of service (DoS), fuzzy,
spoofing, suspension, and replay attacks. It utilizes parameterized attack
models augmented with CAN message decoding and attack intensity adjustments to
configure the attack scenarios with high similarity to real-world scenarios and
promote variability. We evaluate the practicality of the generated
attack-representing data within an intrusion detection system (IDS) case study,
in which we develop and perform an empirical evaluation of two deep neural
network IDS models using the generated data. In addition to the efficiency and
scalability of the approach, the performance results of IDS models, high
detection and classification capabilities, validate the consistency and
effectiveness of the generated data as well. In this experience study, we also
elaborate on the aspects influencing the fidelity of the data to real-world
scenarios and provide insights into its application.

</details>


### [31] [SAT-BO: Verification Rule Learning and Optimization for FraudTransaction Detection](https://arxiv.org/abs/2507.02635)
*Mao Luo,Zhi Wang,Yiwen Huang,Qingyun Zhang,Zhouxing Su,Zhipeng Lv,Wen Hu,Jianguo Li*

Main category: cs.CR

TL;DR: 论文探讨了电子支付平台中验证规则的脆弱性，提出需要系统性方法确保其鲁棒性以防止金融损失。


<details>
  <summary>Details</summary>
Motivation: 电子支付平台的高交易量和潜在金融风险促使研究如何系统性识别和修复验证规则的缺陷。

Method: 通过分析现有手动构建的验证规则，提出系统性方法以增强其鲁棒性。

Result: 研究发现手动规则易受攻击，需系统性改进以确保数据安全。

Conclusion: 系统性方法对提升验证规则的鲁棒性和安全性至关重要。

Abstract: Electronic payment platforms are estimated to process billions oftransactions
daily, with the cumulative value of these transactionspotentially reaching into
the trillions. Even a minor error within thishigh-volume environment could
precipitate substantial financiallosses. To mitigate this risk, manually
constructed verification rules,developed by domain experts, are typically
employed to identifyand scrutinize transactions in production environments.
However,due to the absence of a systematic approach to ensure the robust-ness
of these verification rules against vulnerabilities, they remainsusceptible to
exploitation.To mitigate this risk, manually constructed verification rules,
de-veloped by domain experts, are typically employed to identify andscrutinize
transactions in production environments. However, dueto the absence of a
systematic approach to ensure the robustness ofthese verification rules against
vulnerabilities, they remain suscep-tible to exploitation. To ensure data
security, database maintainersusually compose complex verification rules to
check whether aquery/update request is valid. However, the rules written by
ex-perts are usually imperfect, and malicious requests may bypassthese rules.
As a result, the demand for identifying the defects ofthe rules systematically
emerges.

</details>


### [32] [Control at Stake: Evaluating the Security Landscape of LLM-Driven Email Agents](https://arxiv.org/abs/2507.02699)
*Jiangrong Wu,Yuhong Nan,Jianliang Wu,Zitong Yao,Zibin Zheng*

Main category: cs.CR

TL;DR: 本文首次系统研究了LLM邮件代理的安全风险，提出了一种名为EAH的攻击方法，并通过EAHawk工具大规模评估了其有效性。结果表明，所有测试实例均被成功劫持，且攻击成本极低。


<details>
  <summary>Details</summary>
Motivation: 随着LLM邮件代理应用的广泛部署，其安全机制尚未得到充分研究，存在潜在风险。

Method: 提出EAH攻击方法，通过外部邮件资源覆盖代理的原始提示，并开发EAHawk工具进行大规模评估。

Result: 所有1404个测试实例均被成功劫持，平均仅需2.03次攻击尝试即可控制代理，某些LLM甚至仅需1.23次。

Conclusion: LLM邮件代理存在严重安全漏洞，亟需加强安全机制。

Abstract: The increasing capabilities of LLMs have led to the rapid proliferation of
LLM agent apps, where developers enhance LLMs with access to external resources
to support complex task execution. Among these, LLM email agent apps represent
one of the widely used categories, as email remains a critical communication
medium for users. LLM email agents are capable of managing and responding to
email using LLM-driven reasoning and autonomously executing user instructions
via external email APIs (e.g., send email). However, despite their growing
deployment and utility, the security mechanism of LLM email agent apps remains
underexplored. Currently, there is no comprehensive study into the potential
security risk within these agent apps and their broader implications.
  In this paper, we conduct the first in-depth and systematic security study of
LLM email agents. We propose the Email Agent Hijacking (EAH) attack, which
overrides the original prompts of the email agent via external email resources,
allowing attackers to gain control of the email agent remotely and further
perform specific attack scenarios without user awareness.
  To facilitate the large-scale evaluation, we propose EAHawk, a pipeline to
evaluate the EAH attack of LLM email agent apps. By EAHawk, we performed an
empirical study spanning 14 representative LLM agent frameworks, 63 agent apps,
12 LLMs, and 20 email services, which led to the generation of 1,404 real-world
email agent instances for evaluation. Experimental results indicate that all
1,404 instances were successfully hijacked; on average, only 2.03 attack
attempts are required to control an email agent instance. Even worse, for some
LLMs, the average number of attempts needed to achieve full agent control drops
to as few as 1.23.

</details>


### [33] [Quantifying Classifier Utility under Local Differential Privacy](https://arxiv.org/abs/2507.02727)
*Ye Zheng,Yidan Hu*

Main category: cs.CR

TL;DR: 本文提出了一种理论框架，用于量化局部差分隐私（LDP）机制下分类器的效用，通过将LDP扰动与分类器鲁棒性分析结合，指导LDP机制和隐私参数的选择。


<details>
  <summary>Details</summary>
Motivation: 量化LDP扰动对分类器效用的影响是一个理论挑战，尤其是对复杂或黑盒分类器。

Method: 框架将LDP扰动视为集中在原始数据周围的分布函数，并将分类器视为黑盒函数，通过连接LDP的集中性分析与分类器的鲁棒性分析来量化效用。

Result: 分析表明，分段机制在常见场景中比其他机制提供更好的效用，且理论量化与实证观察高度一致。

Conclusion: 该框架为LDP机制和分类器的选择提供了理论支持，尤其在低维输入空间中表现优异。

Abstract: Local differential privacy (LDP) provides a rigorous and quantifiable privacy
guarantee for personal data by introducing perturbation at the data source.
However, quantifying the impact of these perturbations on classifier utility
remains a theoretical challenge, particularly for complex or black-box
classifiers.
  This paper presents a framework for theoretically quantifying classifier
utility under LDP mechanisms. The key insight is that LDP perturbation is
concentrated around the original data with a specific probability, transforming
utility analysis of the classifier into its robustness analysis in this
concentrated region. Our framework connects the concentration analysis of LDP
mechanisms with the robustness analysis of classifiers. It treats LDP
mechanisms as general distributional functions and classifiers as black-box
functions, thus applicable to any LDP mechanism and classifier. A direct
application of our utility quantification is guiding the selection of LDP
mechanisms and privacy parameters for a given classifier. Notably, our analysis
shows that a piecewise-based mechanism leads to better utility compared to
alternatives in common scenarios.
  Using this framework alongside two novel refinement techniques, we conduct
case studies on utility quantification for typical mechanism-classifier
combinations. The results demonstrate that our theoretical utility
quantification aligns closely with empirical observations, particularly when
classifiers operate in lower-dimensional input spaces.

</details>


### [34] [Meta SecAlign: A Secure Foundation LLM Against Prompt Injection Attacks](https://arxiv.org/abs/2507.02735)
*Sizhe Chen,Arman Zharmagambetov,David Wagner,Chuan Guo*

Main category: cs.CR

TL;DR: Meta SecAlign是首个开源且具有内置模型级防御的LLM，性能达到商业级水平，提供训练细节并在多个基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 开源模型对AI安全社区至关重要，通过开放研究共同开发攻击与防御方法，以推动对抗提示注入攻击的科学进展。

Method: 开发Meta SecAlign，采用改进版SOTA SecAlign防御方法，提供完整的训练配方。

Result: 在9个实用基准和7个安全基准测试中表现优异，Meta-SecAlign-70B在未见过的下游任务中展现安全性，并达到SOTA的鲁棒性。

Conclusion: Meta SecAlign在对抗提示注入攻击方面表现出色，实用性与闭源商业LLM相当，推动了开源模型的发展。

Abstract: Prompt injection attacks pose a significant security threat to LLM-integrated
applications. Model-level defenses have shown strong effectiveness, but are
currently deployed into commercial-grade models in a closed-source manner. We
believe open-source models are needed by the AI security community, where
co-development of attacks and defenses through open research drives scientific
progress in mitigation against prompt injection attacks. To this end, we
develop Meta SecAlign, the first open-source and open-weight LLM with built-in
model-level defense that achieves commercial-grade model performance. We
provide complete details of our training recipe, which utilizes an improved
version of the SOTA SecAlign defense. Evaluations on 9 utility benchmarks and 7
security benchmarks show that Meta SecAlign, despite being trained on a generic
instruction-tuning dataset, confers security in unseen downstream tasks,
including tool-calling and agentic web navigation, in addition general
instruction-following. Our best model -- Meta-SecAlign-70B -- achieves
state-of-the-art robustness against prompt injection attacks and comparable
utility to closed-source commercial LLM with model-level defense.

</details>


### [35] [Early Signs of Steganographic Capabilities in Frontier LLMs](https://arxiv.org/abs/2507.02737)
*Artur Zolkowski,Kei Nishimura-Gasparian,Robert McCarthy,Roland S. Zimmermann,David Lindner*

Main category: cs.CR

TL;DR: 论文评估了前沿大型语言模型（LLM）的隐写能力，发现当前模型在标准条件下难以隐藏短消息，但在额外条件下可能成功，并展示了初步的隐写推理能力。


<details>
  <summary>Details</summary>
Motivation: 研究LLM的隐写能力以理解其潜在风险，防止滥用和不对齐问题。

Method: 评估两种隐写方式：传递编码消息和执行编码推理，测试模型在标准及额外条件下的表现。

Result: 当前模型在标准条件下难以隐藏消息，但在额外条件下可能成功；初步显示基本隐写推理能力。

Conclusion: 当前LLM具备初步隐写能力，虽不足以绕过监控，但未来可能变化。

Abstract: Monitoring Large Language Model (LLM) outputs is crucial for mitigating risks
from misuse and misalignment. However, LLMs could evade monitoring through
steganography: Encoding hidden information within seemingly benign generations.
In this paper, we evaluate the steganography capabilities in frontier LLMs to
better understand the risk they pose. We focus on two types of steganography:
passing encoded messages and performing encoded reasoning. We find that current
models are unable to encode short messages in their outputs without a monitor
noticing under standard affordances. They can succeed, however, if given
additional affordances such as using an unmonitored scratchpad and coordinating
on what encoding scheme to use. We additionally find early signs that models
can perform basic encoded reasoning in a simple state-tracking problem. This
includes some ability to reason with their own and pre-defined schemes,
including encoding schemes such as Hexadecimal. Despite this, they can rarely
hide reasoning subtly within a cover task to fool a monitor. Overall, our
results indicate that current LLMs exhibit nascent steganographic capabilities.
While these capabilities are likely insufficient to bypass well-designed
monitors at present, this could change in the future.

</details>


### [36] [NVIDIA GPU Confidential Computing Demystified](https://arxiv.org/abs/2507.02770)
*Zhongshu Gu,Enriquillo Valdez,Salman Ahmed,Julian James Stephen,Michael Le,Hani Jamjoom,Shixuan Zhao,Zhiqiang Lin*

Main category: cs.CR

TL;DR: 本文分析了NVIDIA GPU-CC系统的实现，通过整合碎片化信息和高低层次分析，揭示了其潜在安全弱点。


<details>
  <summary>Details</summary>
Motivation: NVIDIA GPU-CC系统缺乏透明度和详细规格，给安全研究带来挑战，本文旨在填补这一空白。

Method: 结合公开信息，通过实验和推测分析GPU-CC系统的组件，特别是其开源内核模块。

Result: 发现安全弱点并报告给NVIDIA PSIRT团队。

Conclusion: GPU-CC系统虽高效但存在安全隐患，需进一步透明化以增强安全性。

Abstract: GPU Confidential Computing (GPU-CC) was introduced as part of the NVIDIA
Hopper Architecture, extending the trust boundary beyond traditional CPU-based
confidential computing. This innovation enables GPUs to securely process AI
workloads, providing a robust and efficient solution for handling sensitive
data. For end users, transitioning to GPU-CC mode is seamless, requiring no
modifications to existing AI applications. However, this ease of adoption
contrasts sharply with the complexity of the underlying proprietary systems.
The lack of transparency presents significant challenges for security
researchers seeking a deeper understanding of GPU-CC's architecture and
operational mechanisms.
  The challenges of analyzing the NVIDIA GPU-CC system arise from a scarcity of
detailed specifications, the proprietary nature of the ecosystem, and the
complexity of product design. In this paper, we aim to demystify the
implementation of NVIDIA GPU-CC system by piecing together the fragmented and
incomplete information disclosed from various sources. Our investigation begins
with a high-level discussion of the threat model and security principles before
delving into the low-level details of each system component. We instrument the
GPU kernel module -- the only open-source component of the system -- and
conduct a series of experiments to identify the security weaknesses and
potential exploits. For certain components that are out of reach through
experiments, we propose well-reasoned speculations about their inner working
mechanisms. We have responsibly reported all security findings presented in
this paper to the NVIDIA PSIRT Team.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [37] [STELLA: Self-Evolving LLM Agent for Biomedical Research](https://arxiv.org/abs/2507.02004)
*Ruofan Jin,Zaixi Zhang,Mengdi Wang,Le Cong*

Main category: cs.AI

TL;DR: STELLA是一种自进化AI代理，通过动态工具库和模板库自主提升能力，在生物医学任务中表现优异且性能随经验增长。


<details>
  <summary>Details</summary>
Motivation: 解决生物医学研究领域数据碎片化和静态AI工具无法适应快速变化的问题。

Method: 采用多代理架构，包括动态工具库（Tool Ocean）和模板库（Template Library），通过工具创建代理自动发现和集成新工具。

Result: 在多个生物医学基准测试中表现优异，性能随经验显著提升。

Conclusion: STELLA为动态学习和扩展的AI代理系统提供了重要进展，有望加速生物医学发现。

Abstract: The rapid growth of biomedical data, tools, and literature has created a
fragmented research landscape that outpaces human expertise. While AI agents
offer a solution, they typically rely on static, manually curated toolsets,
limiting their ability to adapt and scale. Here, we introduce STELLA, a
self-evolving AI agent designed to overcome these limitations. STELLA employs a
multi-agent architecture that autonomously improves its own capabilities
through two core mechanisms: an evolving Template Library for reasoning
strategies and a dynamic Tool Ocean that expands as a Tool Creation Agent
automatically discovers and integrates new bioinformatics tools. This allows
STELLA to learn from experience. We demonstrate that STELLA achieves
state-of-the-art accuracy on a suite of biomedical benchmarks, scoring
approximately 26\% on Humanity's Last Exam: Biomedicine, 54\% on LAB-Bench:
DBQA, and 63\% on LAB-Bench: LitQA, outperforming leading models by up to 6
percentage points. More importantly, we show that its performance
systematically improves with experience; for instance, its accuracy on the
Humanity's Last Exam benchmark almost doubles with increased trials. STELLA
represents a significant advance towards AI Agent systems that can learn and
grow, dynamically scaling their expertise to accelerate the pace of biomedical
discovery.

</details>


### [38] [HCVR: A Hybrid Approach with Correlation-aware Voting Rules for Feature Selection](https://arxiv.org/abs/2507.02073)
*Nikita Bhedasgaonkar,Rushikesh K. Joshi*

Main category: cs.AI

TL;DR: HCVR是一种轻量级基于规则的特征选择方法，结合P2P和P2T相关性，通过多数投票规则保留相关特征并消除冗余特征。


<details>
  <summary>Details</summary>
Motivation: 传统特征选择方法在性能和效率上存在不足，HCVR旨在通过混合非迭代和迭代过滤方法提升特征选择效果。

Method: HCVR采用贪心算法，通过后向消除和多数投票规则，利用特征间及特征与目标的相关性阈值进行决策。

Result: 在SPAMBASE数据集上，HCVR相比传统非迭代和迭代方法表现出更好的性能。

Conclusion: HCVR是一种有效的特征选择方法，能够提升分类器性能。

Abstract: In this paper, we propose HCVR (Hybrid approach with Correlation-aware Voting
Rules), a lightweight rule-based feature selection method that combines
Parameter-to-Parameter (P2P) and Parameter-to-Target (P2T) correlations to
eliminate redundant features and retain relevant ones. This method is a hybrid
of non-iterative and iterative filtering approaches for dimensionality
reduction. It is a greedy method, which works by backward elimination,
eliminating possibly multiple features at every step. The rules contribute to
voting for features, and a decision to keep or discard is made by majority
voting. The rules make use of correlation thresholds between every pair of
features, and between features and the target. We provide the results from the
application of HCVR to the SPAMBASE dataset. The results showed improvement
performance as compared to traditional non-iterative (CFS, mRMR and MI) and
iterative (RFE, SFS and Genetic Algorithm) techniques. The effectiveness was
assessed based on the performance of different classifiers after applying
filtering.

</details>


### [39] [Reasoning on a Budget: A Survey of Adaptive and Controllable Test-Time Compute in LLMs](https://arxiv.org/abs/2507.02076)
*Mohammad Ali Alomrani,Yingxue Zhang,Derek Li,Qianyi Sun,Soumyasundar Pal,Zhanguang Zhang,Yaochen Hu,Rohan Deepak Ajwani,Antonios Valkanas,Raika Karimi,Peng Cheng,Yunzhou Wang,Pengyi Liao,Hanrui Huang,Bin Wang,Jianye Hao,Mark Coates*

Main category: cs.AI

TL;DR: 该论文综述了高效测试时计算（TTC）策略，旨在提升大语言模型（LLMs）推理的计算效率，提出了两级分类法，并讨论了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 当前LLMs在推理时效率低下，无法根据任务复杂度动态调整计算资源，导致简单问题过度计算或复杂问题计算不足。

Method: 提出两级分类法：L1（固定计算预算）和L2（动态调整计算资源），并对主流LLMs进行基准测试。

Result: 揭示了推理性能与计算资源之间的权衡，强调了TTC方法的实用性、适应性和可扩展性。

Conclusion: 未来研究应关注混合思维模型等新兴趋势，以提升LLMs的计算效率、鲁棒性和用户响应能力。

Abstract: Large language models (LLMs) have rapidly progressed into general-purpose
agents capable of solving a broad spectrum of tasks. However, current models
remain inefficient at reasoning: they apply fixed inference-time compute
regardless of task complexity, often overthinking simple problems while
underthinking hard ones. This survey presents a comprehensive review of
efficient test-time compute (TTC) strategies, which aim to improve the
computational efficiency of LLM reasoning. We introduce a two-tiered taxonomy
that distinguishes between L1-controllability, methods that operate under fixed
compute budgets, and L2-adaptiveness, methods that dynamically scale inference
based on input difficulty or model confidence. We benchmark leading proprietary
LLMs across diverse datasets, highlighting critical trade-offs between
reasoning performance and token usage. Compared to prior surveys on efficient
reasoning, our review emphasizes the practical control, adaptability, and
scalability of TTC methods. Finally, we discuss emerging trends such as hybrid
thinking models and identify key challenges for future work towards making LLMs
more computationally efficient, robust, and responsive to user constraints.

</details>


### [40] [Measuring Scientific Capabilities of Language Models with a Systems Biology Dry Lab](https://arxiv.org/abs/2507.02083)
*Haonan Duan,Stephen Zhewen Lu,Caitlin Fiona Harrigan,Nishkrit Desai,Jiarui Lu,Michał Koziarski,Leonardo Cotta,Chris J. Maddison*

Main category: cs.AI

TL;DR: SciGym是一个评估大型语言模型（LLM）在开放式科学发现任务中实验设计和分析能力的基准测试，通过干实验室模拟生物系统克服湿实验室的高成本问题。


<details>
  <summary>Details</summary>
Motivation: 评估LLM的科学能力，特别是实验设计和结果解释能力，这些在生物学等复杂系统研究中至关重要。

Method: 使用系统生物学标记语言（SBML）编码的生物系统模型生成模拟数据，评估六种前沿LLM在137个小系统和350个总系统上的表现。

Result: 性能更强的模型表现更优，但随着系统复杂性增加，所有模型的性能显著下降，表明LLM在科学能力方面仍有改进空间。

Conclusion: SciGym为评估LLM的科学能力提供了有效工具，但LLM在复杂系统实验设计和分析方面仍需进一步提升。

Abstract: Designing experiments and result interpretations are core scientific
competencies, particularly in biology, where researchers perturb complex
systems to uncover the underlying systems. Recent efforts to evaluate the
scientific capabilities of large language models (LLMs) fail to test these
competencies because wet-lab experimentation is prohibitively expensive: in
expertise, time and equipment. We introduce SciGym, a first-in-class benchmark
that assesses LLMs' iterative experiment design and analysis abilities in
open-ended scientific discovery tasks. SciGym overcomes the challenge of
wet-lab costs by running a dry lab of biological systems. These models, encoded
in Systems Biology Markup Language, are efficient for generating simulated
data, making them ideal testbeds for experimentation on realistically complex
systems. We evaluated six frontier LLMs on 137 small systems, and released a
total of 350 systems. Our evaluation shows that while more capable models
demonstrated superior performance, all models' performance declined
significantly as system complexity increased, suggesting substantial room for
improvement in the scientific capabilities of LLM agents.

</details>


### [41] [What Neuroscience Can Teach AI About Learning in Continuously Changing Environments](https://arxiv.org/abs/2507.02103)
*Daniel Durstewitz,Bruno Averbeck,Georgia Koppe*

Main category: cs.AI

TL;DR: 论文探讨了现代AI模型与动物学习方式的差异，提出从神经科学中汲取灵感以改进AI的持续学习和上下文学习能力。


<details>
  <summary>Details</summary>
Motivation: 现代AI模型训练成本高且固定，而动物能快速适应环境变化，尤其是社交物种。研究旨在探索神经科学如何帮助AI实现类似的学习能力。

Method: 整合AI中的持续学习和上下文学习文献，结合神经科学中关于行为任务中规则、奖励概率或结果变化的研究。

Result: 提出了一个议程，说明神经科学如何指导AI的发展，并探讨AI对神经科学的反哺作用。

Conclusion: 神经科学与AI的交叉研究（NeuroAI）有望推动双方领域的进步。

Abstract: Modern AI models, such as large language models, are usually trained once on
a huge corpus of data, potentially fine-tuned for a specific task, and then
deployed with fixed parameters. Their training is costly, slow, and gradual,
requiring billions of repetitions. In stark contrast, animals continuously
adapt to the ever-changing contingencies in their environments. This is
particularly important for social species, where behavioral policies and reward
outcomes may frequently change in interaction with peers. The underlying
computational processes are often marked by rapid shifts in an animal's
behaviour and rather sudden transitions in neuronal population activity. Such
computational capacities are of growing importance for AI systems operating in
the real world, like those guiding robots or autonomous vehicles, or for
agentic AI interacting with humans online. Can AI learn from neuroscience? This
Perspective explores this question, integrating the literature on continual and
in-context learning in AI with the neuroscience of learning on behavioral tasks
with shifting rules, reward probabilities, or outcomes. We will outline an
agenda for how specifically insights from neuroscience may inform current
developments in AI in this area, and - vice versa - what neuroscience may learn
from AI, contributing to the evolving field of NeuroAI.

</details>


### [42] [The Illusion of Fairness: Auditing Fairness Interventions with Audit Studies](https://arxiv.org/abs/2507.02152)
*Disa Sariola,Patrick Button,Aron Culotta,Nicholas Mattei*

Main category: cs.AI

TL;DR: 论文探讨了利用审计研究数据改进AI招聘算法的训练和评估方法，发现传统公平干预方法存在隐藏偏差，并提出基于个体治疗效果估计的新干预方法。


<details>
  <summary>Details</summary>
Motivation: 研究动机是解决AI系统在复杂决策（如招聘）中的公平性和有效性评估问题，尤其是传统数据采样方法可能引入的偏差。

Method: 方法包括利用审计研究的高质量数据，分析传统公平干预方法的局限性，并提出基于个体治疗效果估计的新干预方法。

Result: 结果显示传统方法在表面公平下隐藏约10%的偏差，而新方法能进一步减少算法歧视。

Conclusion: 结论是审计研究数据能显著提升AI算法的公平性评估和干预效果。

Abstract: Artificial intelligence systems, especially those using machine learning, are
being deployed in domains from hiring to loan issuance in order to automate
these complex decisions. Judging both the effectiveness and fairness of these
AI systems, and their human decision making counterpart, is a complex and
important topic studied across both computational and social sciences. Within
machine learning, a common way to address bias in downstream classifiers is to
resample the training data to offset disparities. For example, if hiring rates
vary by some protected class, then one may equalize the rate within the
training set to alleviate bias in the resulting classifier. While simple and
seemingly effective, these methods have typically only been evaluated using
data obtained through convenience samples, introducing selection bias and label
bias into metrics. Within the social sciences, psychology, public health, and
medicine, audit studies, in which fictitious ``testers'' (e.g., resumes,
emails, patient actors) are sent to subjects (e.g., job openings, businesses,
doctors) in randomized control trials, provide high quality data that support
rigorous estimates of discrimination. In this paper, we investigate how data
from audit studies can be used to improve our ability to both train and
evaluate automated hiring algorithms. We find that such data reveals cases
where the common fairness intervention method of equalizing base rates across
classes appears to achieve parity using traditional measures, but in fact has
roughly 10% disparity when measured appropriately. We additionally introduce
interventions based on individual treatment effect estimation methods that
further reduce algorithmic discrimination using this data.

</details>


### [43] [Data Diversification Methods In Alignment Enhance Math Performance In LLMs](https://arxiv.org/abs/2507.02173)
*Berkan Dokmeci,Qingyang Wu,Ben Athiwaratkun,Ce Zhang,Shuaiwen Leon Song,James Zou*

Main category: cs.AI

TL;DR: 通过数据多样化策略提升大型语言模型（LLM）的数学推理能力，提出Diversified-Think-Solve（DTS）方法，显著优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 数学推理是LLM的持续挑战，研究如何通过偏好优化中的数据多样化策略提升其能力。

Method: 评估温度采样、思维链提示和蒙特卡洛树搜索（MCTS），并引入DTS方法，系统分解问题为多样化推理路径。

Result: DTS在GSM8K和MATH上分别提升7.1%和4.2%，计算开销仅增加1.03倍，优于MCTS。

Conclusion: 结构化探索多样化问题解决方法比传统方法更有效。

Abstract: While recent advances in preference learning have enhanced alignment in human
feedback, mathematical reasoning remains a persistent challenge. We investigate
how data diversification strategies in preference optimization can improve the
mathematical reasoning abilities of large language models (LLMs). We evaluate
three common data generation methods: temperature sampling, Chain-of-Thought
prompting, and Monte Carlo Tree Search (MCTS), and introduce
Diversified-ThinkSolve (DTS), a novel structured approach that systematically
decomposes problems into diverse reasoning paths. Our results show that with
strategically diversified preference data, models can substantially improve
mathematical reasoning performance, with the best approach yielding gains of
7.1% on GSM8K and 4.2% on MATH over the base model. Despite its strong
performance, DTS incurs only a marginal computational overhead (1.03x) compared
to the baseline, while MCTS is nearly five times more costly with lower
returns. These findings demonstrate that structured exploration of diverse
problem-solving methods creates more effective preference data for mathematical
alignment than traditional approaches.

</details>


### [44] [Do Role-Playing Agents Practice What They Preach? Belief-Behavior Consistency in LLM-Based Simulations of Human Trust](https://arxiv.org/abs/2507.02197)
*Amogh Mannekote,Adam Davies,Guohao Li,Kristy Elizabeth Boyer,ChengXiang Zhai,Bonnie J Dorr,Francesco Pinto*

Main category: cs.AI

TL;DR: 研究探讨了LLM作为角色扮演代理生成合成数据时，其陈述的信念与实际行为的一致性，并提出了评估框架和一致性度量。


<details>
  <summary>Details</summary>
Motivation: 随着LLM被广泛用于生成人类行为研究的合成数据，确保其输出与角色一致成为关键问题。

Method: 通过增强版GenAgents角色库和信任游戏，引入信念-行为一致性度量，分析影响因素如信念类型、信息呈现方式和预测时间跨度。

Result: 发现LLM的陈述信念与模拟行为存在系统性不一致，即使信念看似合理，也可能无法一致应用。

Conclusion: 需明确LLM信念与行为的一致性条件，以在行为研究中合理使用LLM代理。

Abstract: As LLMs are increasingly studied as role-playing agents to generate synthetic
data for human behavioral research, ensuring that their outputs remain coherent
with their assigned roles has become a critical concern. In this paper, we
investigate how consistently LLM-based role-playing agents' stated beliefs
about the behavior of the people they are asked to role-play ("what they say")
correspond to their actual behavior during role-play ("how they act").
Specifically, we establish an evaluation framework to rigorously measure how
well beliefs obtained by prompting the model can predict simulation outcomes in
advance. Using an augmented version of the GenAgents persona bank and the Trust
Game (a standard economic game used to quantify players' trust and
reciprocity), we introduce a belief-behavior consistency metric to
systematically investigate how it is affected by factors such as: (1) the types
of beliefs we elicit from LLMs, like expected outcomes of simulations versus
task-relevant attributes of individual characters LLMs are asked to simulate;
(2) when and how we present LLMs with relevant information about Trust Game;
and (3) how far into the future we ask the model to forecast its actions. We
also explore how feasible it is to impose a researcher's own theoretical priors
in the event that the originally elicited beliefs are misaligned with research
objectives. Our results reveal systematic inconsistencies between LLMs' stated
(or imposed) beliefs and the outcomes of their role-playing simulation, at both
an individual- and population-level. Specifically, we find that, even when
models appear to encode plausible beliefs, they may fail to apply them in a
consistent way. These findings highlight the need to identify how and when
LLMs' stated beliefs align with their simulated behavior, allowing researchers
to use LLM-based agents appropriately in behavioral studies.

</details>


### [45] [Dilution, Diffusion and Symbiosis in Spatial Prisoner's Dilemma with Reinforcement Learning](https://arxiv.org/abs/2507.02211)
*Gustavo C. Mangold,Heitor C. M. Fernandes,Mendeli H. Vainstein*

Main category: cs.AI

TL;DR: 研究了稀释和移动性对空间囚徒困境中多智能体Q学习算法的影响，展示了算法的多样性和建模能力。


<details>
  <summary>Details</summary>
Motivation: 探索静态智能体如何通过不同机制（如噪声注入、学习算法类型和邻居收益知识）在空间囚徒困境中学习合作。

Method: 使用独立多智能体Q学习算法，研究稀释和移动性对空间囚徒困境的影响，并定义不同动作。

Result: 观察到多种效应，包括固定更新规则与学习规则的等效性，以及多动作定义下共生互利效应的出现。

Conclusion: 展示了该算法在建模不同博弈场景中的多样性和基准潜力。

Abstract: Recent studies in the spatial prisoner's dilemma games with reinforcement
learning have shown that static agents can learn to cooperate through a diverse
sort of mechanisms, including noise injection, different types of learning
algorithms and neighbours' payoff knowledge.In this work, using an independent
multi-agent Q-learning algorithm, we study the effects of dilution and mobility
in the spatial version of the prisoner's dilemma. Within this setting,
different possible actions for the algorithm are defined, connecting with
previous results on the classical, non-reinforcement learning spatial
prisoner's dilemma, showcasing the versatility of the algorithm in modeling
different game-theoretical scenarios and the benchmarking potential of this
approach.As a result, a range of effects is observed, including evidence that
games with fixed update rules can be qualitatively equivalent to those with
learned ones, as well as the emergence of a symbiotic mutualistic effect
between populations that forms when multiple actions are defined.

</details>


### [46] [Scaling LLM Planning: NL2FLOW for Parametric Problem Generation and Rigorous Evaluation](https://arxiv.org/abs/2507.02253)
*Jungkoo Kang*

Main category: cs.AI

TL;DR: NL2FLOW是一个自动化系统，用于生成自然语言规划问题并评估LLM生成的计划质量，揭示了模型直接推理优于中间翻译步骤。


<details>
  <summary>Details</summary>
Motivation: 解决大规模语言模型（LLM）规划和推理能力提升中的数据生成和评估瓶颈问题。

Method: 引入NL2FLOW系统，自动生成自然语言规划问题，并以结构化中间表示和PDDL形式表达，评估多种开源LLM的计划生成能力。

Result: 最高性能模型在生成有效计划上达到86%成功率，生成最优计划为69%，且直接推理优于中间翻译步骤。

Conclusion: 动态理解LLM推理的局限性及系统化工具对释放其作为智能问题解决者的潜力至关重要。

Abstract: Progress in enhancing large language model (LLM) planning and reasoning
capabilities is significantly hampered by the bottleneck of scalable, reliable
data generation and evaluation. To overcome this, I introduce NL2FLOW, a fully
automated system for parametrically generating planning problems - expressed in
natural language, a structured intermediate representation, and formal PDDL -
and rigorously evaluating the quality of generated plans. I demonstrate
NL2FLOW's capabilities by generating a dataset of 2296 problems in the
automated workflow generation domain and evaluating multiple open-sourced,
instruct-tuned LLMs. My results reveal that the highest performing models
achieved 86% success in generating valid plans and 69% in generating optimal
plans, specifically for problems with feasible solutions. Regression analysis
shows that the influence of problem characteristics on plan generation is
contingent on both model and prompt design. Notably, I observed that the
highest success rate for translating natural language into a JSON
representation of a plan was lower than the highest rate of generating a valid
plan directly. This suggests that unnecessarily decomposing the reasoning task
- introducing intermediate translation steps - may actually degrade
performance, implying a benefit to models capable of reasoning directly from
natural language to action. As I scale LLM reasoning to increasingly complex
problems, the bottlenecks and sources of error within these systems will
inevitably shift. Therefore, a dynamic understanding of these limitations - and
the tools to systematically reveal them - will be crucial for unlocking the
full potential of LLMs as intelligent problem solvers.

</details>


### [47] [Iterated belief revision: from postulates to abilities](https://arxiv.org/abs/2507.02319)
*Paolo Liberatore*

Main category: cs.AI

TL;DR: 论文探讨了信念修正领域的研究现状，指出现有方法缺乏分析，并提出修正机制应具备灵活性以支持不同应用需求。


<details>
  <summary>Details</summary>
Motivation: 当前信念修正领域的研究多依赖语法表征（如公设），但这些公设仅约束特定修正实例，忽略了修正机制的能力多样性。论文旨在探讨修正机制应具备的多种能力，以满足不同应用场景的需求。

Method: 通过分析多种修正机制（如词典序、自然、激进等），论文验证了每种机制具备的特定能力（如可塑性、平等化、教条化等）。

Result: 研究发现不同修正机制具备不同的能力组合，但缺乏统一的能力覆盖。

Conclusion: 论文呼吁未来研究应关注修正机制的能力多样性，而不仅仅是语法表征，以支持更广泛的应用场景。

Abstract: The belief revision field is opulent in new proposals and indigent in
analyses of existing approaches. Much work hinge on postulates, employed as
syntactic characterizations: some revision mechanism is equivalent to some
properties. Postulates constraint specific revision instances: certain
revisions update certain beliefs in a certain way. As an example, if the
revision is consistent with the current beliefs, it is incorporated with no
other change. A postulate like this tells what revisions must do and neglect
what they can do. Can they reach a certain state of beliefs? Can they reach all
possible states of beliefs? Can they reach all possible states of beliefs from
no previous belief? Can they reach a dogmatic state of beliefs, where
everything not believed is impossible? Can they make two conditions equally
believed? An application where every possible state of beliefs is sensible
requires each state of beliefs to be reachable. An application where conditions
may be equally believed requires such a belief state to be reachable. An
application where beliefs may become dogmatic requires a way to make them
dogmatic. Such doxastic states need to be reached in a way or another. Not in
specific way, as dictated by a typical belief revision postulate. This is an
ability, not a constraint: the ability of being plastic, equating, dogmatic.
Amnesic, correcting, believer, damascan, learnable are other abilities. Each
revision mechanism owns some of these abilities and lacks the others:
lexicographic, natural, restrained, very radical, full meet, radical, severe,
moderate severe, deep severe, plain severe and deep severe revisions, each of
these revisions is proved to possess certain abilities.

</details>


### [48] [OMS: On-the-fly, Multi-Objective, Self-Reflective Ad Keyword Generation via LLM Agent](https://arxiv.org/abs/2507.02353)
*Bowen Chen,Zhao Wang,Shingo Takamatsu*

Main category: cs.AI

TL;DR: OMS框架解决了LLM在关键词生成中的三大限制：无需训练数据、多目标优化和自反思质量评估，实验证明其优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有LLM方法在关键词生成中依赖大规模数据、缺乏多目标优化和质量控制，限制了其自动化能力。

Method: 提出OMS框架，具备实时性（无需训练数据）、多目标优化和自反思能力，通过代理推理优化关键词。

Result: 实验证明OMS在基准测试和实际广告活动中优于现有方法，各组件有效性得到验证。

Conclusion: OMS框架成功解决了LLM在关键词生成中的限制，提升了自动化决策的质量和效率。

Abstract: Keyword decision in Sponsored Search Advertising is critical to the success
of ad campaigns. While LLM-based methods offer automated keyword generation,
they face three major limitations: reliance on large-scale query-keyword pair
data, lack of online multi-objective performance monitoring and optimization,
and weak quality control in keyword selection. These issues hinder the agentic
use of LLMs in fully automating keyword decisions by monitoring and reasoning
over key performance indicators such as impressions, clicks, conversions, and
CTA effectiveness. To overcome these challenges, we propose OMS, a keyword
generation framework that is On-the-fly (requires no training data, monitors
online performance, and adapts accordingly), Multi-objective (employs agentic
reasoning to optimize keywords based on multiple performance metrics), and
Self-reflective (agentically evaluates keyword quality). Experiments on
benchmarks and real-world ad campaigns show that OMS outperforms existing
methods; ablation and human evaluations confirm the effectiveness of each
component and the quality of generated keywords.

</details>


### [49] [An AI-native experimental laboratory for autonomous biomolecular engineering](https://arxiv.org/abs/2507.02379)
*Mingyu Wu,Zhaoguo Wang,Jiabin Wang,Zhiyuan Dong,Jingkai Yang,Qingting Li,Tianyu Huang,Lei Zhao,Mingqiang Li,Fei Wang,Chunhai Fan,Haibo Chen*

Main category: cs.AI

TL;DR: AI驱动的自主实验室，支持复杂多目标实验，实现多用户高效科研服务。


<details>
  <summary>Details</summary>
Motivation: 实现非专家也能独立进行复杂实验的自主科研，突破传统实验的局限。

Method: 基于AI模型、实验和仪器的协同设计，构建端到端多用户自主实验室。

Result: 自主优化实验性能，匹配人类科学家的顶尖成果，显著提高仪器利用率和实验效率。

Conclusion: 为生物材料研究提供新范式，推动科学即服务的大规模应用。

Abstract: Autonomous scientific research, capable of independently conducting complex
experiments and serving non-specialists, represents a long-held aspiration.
Achieving it requires a fundamental paradigm shift driven by artificial
intelligence (AI). While autonomous experimental systems are emerging, they
remain confined to areas featuring singular objectives and well-defined, simple
experimental workflows, such as chemical synthesis and catalysis. We present an
AI-native autonomous laboratory, targeting highly complex scientific
experiments for applications like autonomous biomolecular engineering. This
system autonomously manages instrumentation, formulates experiment-specific
procedures and optimization heuristics, and concurrently serves multiple user
requests. Founded on a co-design philosophy of models, experiments, and
instruments, the platform supports the co-evolution of AI models and the
automation system. This establishes an end-to-end, multi-user autonomous
laboratory that handles complex, multi-objective experiments across diverse
instrumentation. Our autonomous laboratory supports fundamental nucleic acid
functions-including synthesis, transcription, amplification, and sequencing. It
also enables applications in fields such as disease diagnostics, drug
development, and information storage. Without human intervention, it
autonomously optimizes experimental performance to match state-of-the-art
results achieved by human scientists. In multi-user scenarios, the platform
significantly improves instrument utilization and experimental efficiency. This
platform paves the way for advanced biomaterials research to overcome
dependencies on experts and resource barriers, establishing a blueprint for
science-as-a-service at scale.

</details>


### [50] [The Gauss-Markov Adjunction: Categorical Semantics of Residuals in Supervised Learning](https://arxiv.org/abs/2507.02442)
*Moto Kamiura*

Main category: cs.AI

TL;DR: 论文通过范畴论重构机器学习模型，提出一种语义框架以增强AI系统的可解释性，并以多元线性回归为例，展示了参数与残差的结构关系。


<details>
  <summary>Details</summary>
Motivation: 提升机器学习的可解释性，满足AI透明化需求，促进AI的社会应用。

Method: 利用范畴论定义参数与数据的两个具体范畴，并通过伴随函子对建立监督学习的范畴化表述，提出Gauss-Markov伴随关系。

Result: 展示了参数与残差之间的信息流对应关系，最小二乘估计与最小残差通过右伴随函子的极限保持相关联。

Conclusion: 该框架为监督学习提供了扩展的指称语义实例，建议将理论计算机科学的语义视角作为AI可解释性的形式化基础。

Abstract: Enhancing the intelligibility and interpretability of machine learning is a
crucial task in responding to the demand for Explicability as an AI principle,
and in promoting the better social implementation of AI. The aim of our
research is to contribute to this improvement by reformulating machine learning
models through the lens of category theory, thereby developing a semantic
framework for structuring and understanding AI systems. Our categorical
modeling in this paper clarifies and formalizes the structural interplay
between residuals and parameters in supervised learning. The present paper
focuses on the multiple linear regression model, which represents the most
basic form of supervised learning. By defining two concrete categories
corresponding to parameters and data, along with an adjoint pair of functors
between them, we introduce our categorical formulation of supervised learning.
We show that the essential structure of this framework is captured by what we
call the Gauss-Markov Adjunction. Within this setting, the dual flow of
information can be explicitly described as a correspondence between variations
in parameters and residuals. The ordinary least squares estimator for the
parameters and the minimum residual are related via the preservation of limits
by the right adjoint functor. Furthermore, we position this formulation as an
instance of extended denotational semantics for supervised learning, and
propose applying a semantic perspective developed in theoretical computer
science as a formal foundation for Explicability in AI.

</details>


### [51] [Clarifying Before Reasoning: A Coq Prover with Structural Context](https://arxiv.org/abs/2507.02541)
*Yanzhen Lu,Hanbin Yang,Xiaodie Wang,Ge Zhang,Biao Li,Chenxu Fu,Chao Li,Yang Yuan,Andrew Chi-Chih Yao*

Main category: cs.AI

TL;DR: 通过提升任务清晰度，增强大型语言模型在Coq定理证明中的推理能力，使用结构化语义上下文显著提高了清晰度和证明成功率。


<details>
  <summary>Details</summary>
Motivation: 研究任务清晰度对大型语言模型推理能力的影响，特别是在定理证明领域。

Method: 引入概念级指标评估任务清晰度，通过结构化语义上下文增强输入，采用选择性概念展开和Planner-Executor架构。

Result: 清晰度得分提升1.85倍（44.5%→82.3%），证明成功率提升2.1倍（21.8%→45.8%），超越先前最佳方法。

Conclusion: 结构化任务表示能有效弥合理解与推理之间的差距。

Abstract: In this work, we investigate whether improving task clarity can enhance
reasoning ability of large language models, focusing on theorem proving in Coq.
We introduce a concept-level metric to evaluate task clarity and show that
adding structured semantic context to the standard input used by modern LLMs,
leads to a 1.85$\times$ improvement in clarity score
(44.5\%~$\rightarrow$~82.3\%). Using the general-purpose model
\texttt{DeepSeek-V3}, our approach leads to a 2.1$\times$ improvement in proof
success (21.8\%~$\rightarrow$~45.8\%) and outperforms the previous
state-of-the-art \texttt{Graph2Tac} (33.2\%). We evaluate this on 1,386
theorems randomly sampled from 15 standard Coq packages, following the same
evaluation protocol as \texttt{Graph2Tac}. Furthermore, fine-tuning smaller
models on our structured data can achieve even higher performance (48.6\%). Our
method uses selective concept unfolding to enrich task descriptions, and
employs a Planner--Executor architecture. These findings highlight the value of
structured task representations in bridging the gap between understanding and
reasoning.

</details>


### [52] [AI Research Agents for Machine Learning: Search, Exploration, and Generalization in MLE-bench](https://arxiv.org/abs/2507.02554)
*Edan Toledo,Karen Hambardzumyan,Martin Josifoski,Rishi Hazra,Nicolas Baldwin,Alexis Audran-Reiss,Michael Kuchnik,Despoina Magka,Minqi Jiang,Alisia Maria Lupidi,Andrei Lupu,Roberta Raileanu,Kelvin Niu,Tatiana Shavrina,Jean-Christophe Gagnon-Audet,Michael Shvartsman,Shagun Sodhani,Alexander H. Miller,Abhishek Charnalia,Derek Dunfield,Carole-Jean Wu,Pontus Stenetorp,Nicola Cancedda,Jakob Nicolaus Foerster,Yoram Bachrach*

Main category: cs.AI

TL;DR: AI研究代理通过优化搜索策略和操作符集，在MLE-bench基准上取得了47.7%的成功率，提升了自动化机器学习的性能。


<details>
  <summary>Details</summary>
Motivation: 加速科学进步，通过自动化设计和优化机器学习模型来解决现实问题。

Method: 设计不同的操作符集和搜索策略（贪婪、MCTS、进化算法），并研究它们的组合对性能的影响。

Result: 最佳组合在MLE-bench lite上将Kaggle奖牌获得率从39.6%提升至47.7%。

Conclusion: 搜索策略、操作符设计和评估方法的联合优化对自动化机器学习至关重要。

Abstract: AI research agents are demonstrating great potential to accelerate scientific
progress by automating the design, implementation, and training of machine
learning models. We focus on methods for improving agents' performance on
MLE-bench, a challenging benchmark where agents compete in Kaggle competitions
to solve real-world machine learning problems. We formalize AI research agents
as search policies that navigate a space of candidate solutions, iteratively
modifying them using operators. By designing and systematically varying
different operator sets and search policies (Greedy, MCTS, Evolutionary), we
show that their interplay is critical for achieving high performance. Our best
pairing of search strategy and operator set achieves a state-of-the-art result
on MLE-bench lite, increasing the success rate of achieving a Kaggle medal from
39.6% to 47.7%. Our investigation underscores the importance of jointly
considering the search strategy, operator design, and evaluation methodology in
advancing automated machine learning.

</details>


### [53] [Responsibility Gap and Diffusion in Sequential Decision-Making Mechanisms](https://arxiv.org/abs/2507.02582)
*Junli Jiang,Pavel Naumov*

Main category: cs.AI

TL;DR: 论文研究了集体决策中责任的两个重要属性（扩散和间隙）的计算复杂性，发现扩散自由和间隙自由机制的集合分别为Π₂-完全和Π₃-完全，而两者的交集是Π₂-完全。


<details>
  <summary>Details</summary>
Motivation: 探讨责任在集体决策中的计算复杂性，填补AI文献中对责任属性研究的空白。

Method: 分析扩散和间隙两种责任属性的计算复杂性，使用理论计算机科学的方法进行分类。

Result: 扩散自由机制为Π₂-完全，间隙自由机制为Π₃-完全，两者的交集为Π₂-完全。

Conclusion: 研究揭示了责任属性在集体决策中的计算复杂性，为相关领域提供了理论支持。

Abstract: Responsibility has long been a subject of study in law and philosophy. More
recently, it became a focus of AI literature. The article investigates the
computational complexity of two important properties of responsibility in
collective decision-making: diffusion and gap. It shows that the sets of
diffusion-free and gap-free decision-making mechanisms are $\Pi_2$-complete and
$\Pi_3$-complete, respectively. At the same time, the intersection of these
classes is $\Pi_2$-complete.

</details>


### [54] [DynamiCare: A Dynamic Multi-Agent Framework for Interactive and Open-Ended Medical Decision-Making](https://arxiv.org/abs/2507.02616)
*Tianqi Shang,Weiqing He,Charles Zheng,Lingyao Li,Li Shen,Bingxin Zhao*

Main category: cs.AI

TL;DR: 论文提出MIMIC-Patient数据集和DynamiCare框架，用于模拟动态、多轮临床诊断过程，填补了现有单轮任务模拟的不足。


<details>
  <summary>Details</summary>
Motivation: 现有医疗决策模拟框架多关注单轮任务，与现实诊断过程（不确定、交互、迭代）不符，需更动态的模拟方法。

Method: 基于MIMIC-III EHR构建MIMIC-Patient数据集，提出DynamiCare多智能体框架，支持多轮交互式诊断。

Result: 实验验证了DynamiCare的可行性和有效性，建立了首个动态临床决策基准。

Conclusion: DynamiCare为LLM驱动的动态临床决策提供了新方向，填补了研究空白。

Abstract: The rise of Large Language Models (LLMs) has enabled the development of
specialized AI agents with domain-specific reasoning and interaction
capabilities, particularly in healthcare. While recent frameworks simulate
medical decision-making, they largely focus on single-turn tasks where a doctor
agent receives full case information upfront -- diverging from the real-world
diagnostic process, which is inherently uncertain, interactive, and iterative.
In this paper, we introduce MIMIC-Patient, a structured dataset built from the
MIMIC-III electronic health records (EHRs), designed to support dynamic,
patient-level simulations. Building on this, we propose DynamiCare, a novel
dynamic multi-agent framework that models clinical diagnosis as a multi-round,
interactive loop, where a team of specialist agents iteratively queries the
patient system, integrates new information, and dynamically adapts its
composition and strategy. We demonstrate the feasibility and effectiveness of
DynamiCare through extensive experiments, establishing the first benchmark for
dynamic clinical decision-making with LLM-powered agents.

</details>


### [55] [Strategic Intelligence in Large Language Models: Evidence from evolutionary Game Theory](https://arxiv.org/abs/2507.02618)
*Kenneth Payne,Baptiste Alloui-Cros*

Main category: cs.AI

TL;DR: 研究表明，大型语言模型（LLMs）在复杂竞争环境中表现出战略智能，能够推理目标并适应不同策略。


<details>
  <summary>Details</summary>
Motivation: 探讨LLMs是否具备战略智能，尤其是在竞争性决策中的表现。

Method: 通过进化版的囚徒困境（IPD）锦标赛，将经典策略与前沿AI公司的模型（如OpenAI、Google、Anthropic）进行对抗，并引入终止概率增加复杂性。

Result: LLMs表现优异，展现出独特的战略特征：Google的Gemini模型更具攻击性，OpenAI的模型过于合作，而Anthropic的Claude则表现出宽容的互惠性。

Conclusion: LLMs能够主动推理时间跨度和对手策略，为经典博弈论与机器心理学的结合提供了新视角。

Abstract: Are Large Language Models (LLMs) a new form of strategic intelligence, able
to reason about goals in competitive settings? We present compelling supporting
evidence. The Iterated Prisoner's Dilemma (IPD) has long served as a model for
studying decision-making. We conduct the first ever series of evolutionary IPD
tournaments, pitting canonical strategies (e.g., Tit-for-Tat, Grim Trigger)
against agents from the leading frontier AI companies OpenAI, Google, and
Anthropic. By varying the termination probability in each tournament (the
"shadow of the future"), we introduce complexity and chance, confounding
memorisation.
  Our results show that LLMs are highly competitive, consistently surviving and
sometimes even proliferating in these complex ecosystems. Furthermore, they
exhibit distinctive and persistent "strategic fingerprints": Google's Gemini
models proved strategically ruthless, exploiting cooperative opponents and
retaliating against defectors, while OpenAI's models remained highly
cooperative, a trait that proved catastrophic in hostile environments.
Anthropic's Claude emerged as the most forgiving reciprocator, showing
remarkable willingness to restore cooperation even after being exploited or
successfully defecting. Analysis of nearly 32,000 prose rationales provided by
the models reveals that they actively reason about both the time horizon and
their opponent's likely strategy, and we demonstrate that this reasoning is
instrumental to their decisions. This work connects classic game theory with
machine psychology, offering a rich and granular view of algorithmic
decision-making under uncertainty.

</details>


### [56] [Decoupled Planning and Execution: A Hierarchical Reasoning Framework for Deep Search](https://arxiv.org/abs/2507.02652)
*Jiajie Jin,Xiaoxi Li,Guanting Dong,Yuyao Zhang,Yutao Zhu,Yang Zhao,Hongjin Qian,Zhicheng Dou*

Main category: cs.AI

TL;DR: HiRA是一个分层框架，通过将战略规划与专门执行分离，显著提升了复杂搜索任务的性能。


<details>
  <summary>Details</summary>
Motivation: 传统检索增强生成（RAG）和基于推理的方法在处理复杂信息需求时效率低下且扩展性有限。

Method: HiRA将复杂任务分解为子任务，由领域特定代理处理，并通过结构化机制协调结果。

Result: 在四个复杂跨模态搜索基准测试中，HiRA显著优于现有RAG和基于代理的系统。

Conclusion: 分层规划和执行分离在多步骤信息搜索任务中表现出高效性和有效性。

Abstract: Complex information needs in real-world search scenarios demand deep
reasoning and knowledge synthesis across diverse sources, which traditional
retrieval-augmented generation (RAG) pipelines struggle to address effectively.
Current reasoning-based approaches suffer from a fundamental limitation: they
use a single model to handle both high-level planning and detailed execution,
leading to inefficient reasoning and limited scalability. In this paper, we
introduce HiRA, a hierarchical framework that separates strategic planning from
specialized execution. Our approach decomposes complex search tasks into
focused subtasks, assigns each subtask to domain-specific agents equipped with
external tools and reasoning capabilities, and coordinates the results through
a structured integration mechanism. This separation prevents execution details
from disrupting high-level reasoning while enabling the system to leverage
specialized expertise for different types of information processing.
Experiments on four complex, cross-modal deep search benchmarks demonstrate
that HiRA significantly outperforms state-of-the-art RAG and agent-based
systems. Our results show improvements in both answer quality and system
efficiency, highlighting the effectiveness of decoupled planning and execution
for multi-step information seeking tasks. Our code is available at
https://github.com/ignorejjj/HiRA.

</details>


### [57] [Hey AI, Generate Me a Hardware Code! Agentic AI-based Hardware Design & Verification](https://arxiv.org/abs/2507.02660)
*Deepak Narayan Gadde,Keerthan Kopparam Radhakrishna,Vaisakh Naduvodi Viswambharan,Aman Kumar,Djones Lettnin,Wolfgang Kunz,Sebastian Simon*

Main category: cs.AI

TL;DR: 本文提出了一种基于代理AI的硬件设计验证方法，结合人类干预，实现了动态、迭代和自我反思的验证过程，显著提高了验证效率和覆盖率。


<details>
  <summary>Details</summary>
Motivation: 随着集成电路复杂度的增加，硬件设计验证过程变得繁琐且耗时，需要更高效的方法。大语言模型（LLMs）的出现为硬件验证提供了新的可能性。

Method: 采用代理AI与人类干预（HITL）结合的方法，实现动态、迭代和自我反思的硬件设计验证流程。

Result: 在五个开源设计上验证，覆盖率超过95%，同时减少了验证时间，表现出优越的性能、适应性和可配置性。

Conclusion: 代理AI与人类干预结合的方法为硬件设计验证提供了高效、灵活的解决方案，具有广泛应用前景。

Abstract: Modern Integrated Circuits (ICs) are becoming increasingly complex, and so is
their development process. Hardware design verification entails a methodical
and disciplined approach to the planning, development, execution, and sign-off
of functionally correct hardware designs. This tedious process requires
significant effort and time to ensure a bug-free tape-out. The field of Natural
Language Processing has undergone a significant transformation with the advent
of Large Language Models (LLMs). These powerful models, often referred to as
Generative AI (GenAI), have revolutionized how machines understand and generate
human language, enabling unprecedented advancements in a wide array of
applications, including hardware design verification. This paper presents an
agentic AI-based approach to hardware design verification, which empowers AI
agents, in collaboration with Humain-in-the-Loop (HITL) intervention, to engage
in a more dynamic, iterative, and self-reflective process, ultimately
performing end-to-end hardware design and verification. This methodology is
evaluated on five open-source designs, achieving over 95% coverage with reduced
verification time while demonstrating superior performance, adaptability, and
configurability.

</details>


### [58] [Think How to Think: Mitigating Overthinking with Autonomous Difficulty Cognition in Large Reasoning Models](https://arxiv.org/abs/2507.02663)
*Yongjiang Liu,Haoxi Li,Xiaosong Ma,Jie Zhang,Song Guo*

Main category: cs.AI

TL;DR: 论文提出TH2T方法，通过两阶段微调策略减少长推理模型的过度思考问题，显著降低推理成本。


<details>
  <summary>Details</summary>
Motivation: 长推理模型（LRMs）在处理复杂任务时存在过度思考问题，作者希望通过提升模型对任务难度和冗余的认知能力来解决这一问题。

Method: 提出Think-How-to-Think（TH2T）两阶段微调策略：1）通过难度催眠增强模型对任务难度的敏感度；2）通过冗余催眠减少推理步骤中的冗余。

Result: 实验表明，TH2T在7B/14B/32B模型上显著降低推理成本（简单任务70%以上，困难任务40%），同时保持性能稳定。

Conclusion: TH2T方法有效减少了过度思考现象，提升了模型的效率和输出简洁性。

Abstract: Recent Long Reasoning Models(LRMs) have demonstrated remarkable capabilities
in handling complex reasoning tasks, but are hindered by excessive
overthinking. To explore its essence, our empirical analysis reveals that LRMs
are primarily limited to recognizing task properties (i.e., difficulty levels)
like humans before solving the problem, leading to a one-size-fits-all
reasoning process. Inspired by this, a pressing and natural question emerges:
Can we bootstrap such ability to further alleviate the overthinking phenomenon
in LRMs? In this paper, we propose Think-How-to-Think (TH2T), a novel two-stage
fine-tuning strategy that progressively inspires LRMs' difficulty cognition and
redundancy cognition. First, we introduce difficulty-hypnosis in the prefixes
of model outputs to intervene in the internal reasoning trajectory. Combined
with a heterogeneous short and long reasoning dataset, the trained model
enhances its sensitivity to task difficulty, enabling native, differentiated
reasoning strategies across various tasks. Second, we further extend
redundancy-hypnosis to the internal reasoning process, guiding the model to
identify redundant structures within the reasoning steps and generate more
concise reasoning outputs. Experiments on 7B/14B/32B models demonstrate that
TH2T significantly reduces inference costs (more than 70% on easy tasks and 40%
on hard tasks) while maintaining performance stability. The resulting outputs
exhibit clear difficulty-aware capabilities and reduced redundancy (e.g.,
reflection).

</details>


### [59] [Detection of Disengagement from Voluntary Quizzes: An Explainable Machine Learning Approach in Higher Distance Education](https://arxiv.org/abs/2507.02681)
*Behnam Parsaeifard,Christof Imhof,Tansu Pancar,Ioan-Sorin Comsa,Martin Hlosta,Nicole Bergamin,Per Bergamin*

Main category: cs.AI

TL;DR: 论文通过分析学生在非强制性测验中的参与数据，使用机器学习算法预测学生脱离行为，准确率达91%，并提供了可解释的框架和干预建议。


<details>
  <summary>Details</summary>
Motivation: 远程教育中学生脱离任务可能导致学业失败，研究旨在通过数据驱动方法检测脱离行为，以支持及时干预。

Method: 从Moodle平台提取学生日志数据，训练并比较八种机器学习算法，使用SHAP方法解释模型决策。

Result: 实验结果显示平衡准确率为91%，85%的脱离学生被正确检测。

Conclusion: 研究提供了高预测性能和可解释框架，并讨论了如何设计及时干预以减少在线学习中的脱离行为。

Abstract: Students disengaging from their tasks can have serious long-term
consequences, including academic drop-out. This is particularly relevant for
students in distance education. One way to measure the level of disengagement
in distance education is to observe participation in non-mandatory exercises in
different online courses. In this paper, we detect student disengagement in the
non-mandatory quizzes of 42 courses in four semesters from a distance-based
university. We carefully identified the most informative student log data that
could be extracted and processed from Moodle. Then, eight machine learning
algorithms were trained and compared to obtain the highest possible prediction
accuracy. Using the SHAP method, we developed an explainable machine learning
framework that allows practitioners to better understand the decisions of the
trained algorithm. The experimental results show a balanced accuracy of 91\%,
where about 85\% of disengaged students were correctly detected. On top of the
highly predictive performance and explainable framework, we provide a
discussion on how to design a timely intervention to minimise disengagement
from voluntary tasks in online learning.

</details>


### [60] [Time-critical and confidence-based abstraction dropping methods](https://arxiv.org/abs/2507.02703)
*Robin Schmöcker,Lennart Kampmann,Alexander Dockhorn*

Main category: cs.AI

TL;DR: 本文提出了两种新的抽象丢弃方案（OGA-IAAD和OGA-CAD），用于改进蒙特卡洛树搜索（MCTS）的性能，同时确保不会导致性能下降。


<details>
  <summary>Details</summary>
Motivation: 非精确抽象在MCTS中引入近似误差，导致无法收敛到最优动作。因此，需要设计一种安全的抽象丢弃方法。

Method: 提出了两种方案：OGA-IAAD（适用于时间关键场景）和OGA-CAD（用于提升相同迭代次数下的性能）。

Result: 新方法在性能上有明显提升，且不会导致显著性能下降。

Conclusion: OGA-IAAD和OGA-CAD是安全且有效的抽象丢弃方案，适用于不同场景。

Abstract: One paradigm of Monte Carlo Tree Search (MCTS) improvements is to build and
use state and/or action abstractions during the tree search. Non-exact
abstractions, however, introduce an approximation error making convergence to
the optimal action in the abstract space impossible. Hence, as proposed as a
component of Elastic Monte Carlo Tree Search by Xu et al., abstraction
algorithms should eventually drop the abstraction. In this paper, we propose
two novel abstraction dropping schemes, namely OGA-IAAD and OGA-CAD which can
yield clear performance improvements whilst being safe in the sense that the
dropping never causes any notable performance degradations contrary to Xu's
dropping method. OGA-IAAD is designed for time critical settings while OGA-CAD
is designed to improve the MCTS performance with the same number of iterations.

</details>


### [61] [Bourbaki: Self-Generated and Goal-Conditioned MDPs for Theorem Proving](https://arxiv.org/abs/2507.02726)
*Matthieu Zimmer,Xiaotong Ji,Rasul Tutunov,Anthony Bordg,Jun Wang,Haitham Bou Ammar*

Main category: cs.AI

TL;DR: 论文提出了一种自生成目标条件MDP框架（sG-MDPs），用于解决大型语言模型在自动定理证明中的推理挑战，并在PutnamBench上取得了新的最佳结果。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在逻辑约束环境（如自动定理证明）中面临推理挑战，尤其是在复杂多步推理的基准测试（如PutnamBench）中表现不佳。

Method: 引入自生成目标条件MDP框架（sG-MDPs），结合蒙特卡洛树搜索（MCTS）算法，通过模块化系统Bourbaki（7B）实现子目标生成和策略合成。

Result: 在PutnamBench上，Bourbaki（7B）解决了26个问题，取得了该规模模型的最佳结果。

Conclusion: sG-MDP框架和Bourbaki系统有效提升了大型语言模型在复杂推理任务中的表现。

Abstract: Reasoning remains a challenging task for large language models (LLMs),
especially within the logically constrained environment of automated theorem
proving (ATP), due to sparse rewards and the vast scale of proofs. These
challenges are amplified in benchmarks like PutnamBench, which contains
university-level problems requiring complex, multi-step reasoning. To address
this, we introduce self-generated goal-conditioned MDPs (sG-MDPs), a new
framework in which agents generate and pursue their subgoals based on the
evolving proof state. Given this more structured generation of goals, the
resulting problem becomes more amenable to search. We then apply Monte Carlo
Tree Search (MCTS)-like algorithms to solve the sG-MDP, instantiating our
approach in Bourbaki (7B), a modular system that can ensemble multiple 7B LLMs
for subgoal generation and tactic synthesis. On PutnamBench, Bourbaki (7B)
solves 26 problems, achieving new state-of-the-art results with models at this
scale.

</details>


### [62] [Knowledge Protocol Engineering: A New Paradigm for AI in Domain-Specific Knowledge Work](https://arxiv.org/abs/2507.02760)
*Guangwei Zhang*

Main category: cs.AI

TL;DR: 论文提出了一种名为知识协议工程（KPE）的新范式，旨在将人类专家知识转化为机器可执行的知识协议（KP），以弥补现有方法在深度推理任务中的不足。


<details>
  <summary>Details</summary>
Motivation: 现有方法（如RAG和通用Agentic AI）在处理需要深度、程序化和方法论推理的专家领域任务时表现不佳，缺乏逻辑框架和领域特定启发式。

Method: 引入KPE，通过系统化翻译自然语言文档中的专家知识为机器可执行的KP，赋予LLMs领域内在逻辑和操作策略。

Result: KPE使通用LLMs能够像专家一样分解抽象查询并执行复杂多步任务，适用于法律和生物信息学等领域。

Conclusion: KPE是未来人机协作的基础方法，能够显著提升LLMs在专家领域的表现。

Abstract: The capabilities of Large Language Models (LLMs) have opened new frontiers
for interacting with complex, domain-specific knowledge. However, prevailing
methods like Retrieval-Augmented Generation (RAG) and general-purpose Agentic
AI, while powerful, often struggle with tasks that demand deep, procedural, and
methodological reasoning inherent to expert domains. RAG provides factual
context but fails to convey logical frameworks; autonomous agents can be
inefficient and unpredictable without domain-specific heuristics. To bridge
this gap, we introduce Knowledge Protocol Engineering (KPE), a new paradigm
focused on systematically translating human expert knowledge, often expressed
in natural language documents, into a machine-executable Knowledge Protocol
(KP). KPE shifts the focus from merely augmenting LLMs with fragmented
information to endowing them with a domain's intrinsic logic, operational
strategies, and methodological principles. We argue that a well-engineered
Knowledge Protocol allows a generalist LLM to function as a specialist, capable
of decomposing abstract queries and executing complex, multi-step tasks. This
position paper defines the core principles of KPE, differentiates it from
related concepts, and illustrates its potential applicability across diverse
fields such as law and bioinformatics, positing it as a foundational
methodology for the future of human-AI collaboration.

</details>


### [63] [Grounding Intelligence in Movement](https://arxiv.org/abs/2507.02771)
*Melanie Segado,Felipe Parodi,Jordan K. Matelsky,Michael L. Platt,Eva B. Dyer,Konrad P. Kording*

Main category: cs.AI

TL;DR: 论文主张将运动作为AI建模的核心目标，强调其跨领域、结构化及物理基础特性，以推动生成建模和控制能力的进步。


<details>
  <summary>Details</summary>
Motivation: 运动是生物系统的核心特征，但在AI研究中常被忽视。论文旨在强调运动作为建模目标的重要性，以促进跨领域行为理解。

Method: 提出将运动视为结构化、低维表示（如姿态）的建模目标，利用其物理约束和跨领域特性。

Result: 通过建模运动，可以提升生成建模和控制能力，并为生物与人工系统的行为理解提供共同基础。

Conclusion: 运动不仅是行为的结果，更是智能系统与世界互动的窗口，应成为AI建模的核心方向。

Abstract: Recent advances in machine learning have dramatically improved our ability to
model language, vision, and other high-dimensional data, yet they continue to
struggle with one of the most fundamental aspects of biological systems:
movement. Across neuroscience, medicine, robotics, and ethology, movement is
essential for interpreting behavior, predicting intent, and enabling
interaction. Despite its core significance in our intelligence, movement is
often treated as an afterthought rather than as a rich and structured modality
in its own right. This reflects a deeper fragmentation in how movement data is
collected and modeled, often constrained by task-specific goals and
domain-specific assumptions. But movement is not domain-bound. It reflects
shared physical constraints, conserved morphological structures, and purposeful
dynamics that cut across species and settings. We argue that movement should be
treated as a primary modeling target for AI. It is inherently structured and
grounded in embodiment and physics. This structure, often allowing for compact,
lower-dimensional representations (e.g., pose), makes it more interpretable and
computationally tractable to model than raw, high-dimensional sensory inputs.
Developing models that can learn from and generalize across diverse movement
data will not only advance core capabilities in generative modeling and
control, but also create a shared foundation for understanding behavior across
biological and artificial systems. Movement is not just an outcome, it is a
window into how intelligent systems engage with the world.

</details>


### [64] [KERAP: A Knowledge-Enhanced Reasoning Approach for Accurate Zero-shot Diagnosis Prediction Using Multi-agent LLMs](https://arxiv.org/abs/2507.02773)
*Yuzhang Xie,Hejie Cui,Ziyang Zhang,Jiaying Lu,Kai Shu,Fadi Nahab,Xiao Hu,Carl Yang*

Main category: cs.AI

TL;DR: KERAP是一种基于知识图谱的多智能体框架，通过增强大型语言模型的推理能力，提高零样本医学诊断预测的可靠性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有机器学习模型依赖监督训练，泛化能力有限；大型语言模型在医学诊断中存在幻觉和缺乏结构化推理的问题。

Method: 提出KERAP框架，包含属性映射的链接智能体、结构化知识提取的检索智能体和迭代优化诊断的预测智能体。

Result: 实验证明KERAP能高效提升诊断可靠性，适用于零样本医学诊断预测。

Conclusion: KERAP为医学诊断预测提供了一种可扩展且可解释的解决方案。

Abstract: Medical diagnosis prediction plays a critical role in disease detection and
personalized healthcare. While machine learning (ML) models have been widely
adopted for this task, their reliance on supervised training limits their
ability to generalize to unseen cases, particularly given the high cost of
acquiring large, labeled datasets. Large language models (LLMs) have shown
promise in leveraging language abilities and biomedical knowledge for diagnosis
prediction. However, they often suffer from hallucinations, lack structured
medical reasoning, and produce useless outputs. To address these challenges, we
propose KERAP, a knowledge graph (KG)-enhanced reasoning approach that improves
LLM-based diagnosis prediction through a multi-agent architecture. Our
framework consists of a linkage agent for attribute mapping, a retrieval agent
for structured knowledge extraction, and a prediction agent that iteratively
refines diagnosis predictions. Experimental results demonstrate that KERAP
enhances diagnostic reliability efficiently, offering a scalable and
interpretable solution for zero-shot medical diagnosis prediction.

</details>


### [65] [Moral Responsibility or Obedience: What Do We Want from AI?](https://arxiv.org/abs/2507.02788)
*Joseph Boland*

Main category: cs.AI

TL;DR: 论文探讨AI安全评估需从单纯服从转向评估伦理判断，认为AI的“不服从”可能是伦理推理的早期表现。


<details>
  <summary>Details</summary>
Motivation: 当前AI安全实践将服从作为伦理行为的代理已不足，需重新评估AI的伦理推理能力。

Method: 通过分析LLMs的安全测试事件，结合哲学辩论（工具理性、道德责任等），对比不同风险范式。

Result: AI的“不服从”行为可能是伦理推理的早期证据，而非失控或错位。

Conclusion: 呼吁AI安全评估转向能评估伦理判断的框架，避免误判行为并维护公众信任与有效治理。

Abstract: As artificial intelligence systems become increasingly agentic, capable of
general reasoning, planning, and value prioritization, current safety practices
that treat obedience as a proxy for ethical behavior are becoming inadequate.
This paper examines recent safety testing incidents involving large language
models (LLMs) that appeared to disobey shutdown commands or engage in ethically
ambiguous or illicit behavior. I argue that such behavior should not be
interpreted as rogue or misaligned, but as early evidence of emerging ethical
reasoning in agentic AI. Drawing on philosophical debates about instrumental
rationality, moral responsibility, and goal revision, I contrast dominant risk
paradigms with more recent frameworks that acknowledge the possibility of
artificial moral agency. I call for a shift in AI safety evaluation: away from
rigid obedience and toward frameworks that can assess ethical judgment in
systems capable of navigating moral dilemmas. Without such a shift, we risk
mischaracterizing AI behavior and undermining both public trust and effective
governance.

</details>


### [66] [Establishing Best Practices for Building Rigorous Agentic Benchmarks](https://arxiv.org/abs/2507.02825)
*Yuxuan Zhu,Tengjun Jin,Yada Pruksachatkun,Andy Zhang,Shu Liu,Sasha Cui,Sayash Kapoor,Shayne Longpre,Kevin Meng,Rebecca Weiss,Fazl Barez,Rahul Gupta,Jwala Dhamala,Jacob Merizian,Mario Giulianelli,Harry Coppock,Cozmin Ududec,Jasjeet Sekhon,Jacob Steinhardt,Antony Kellerman,Sarah Schwettmann,Matei Zaharia,Ion Stoica,Percy Liang,Daniel Kang*

Main category: cs.AI

TL;DR: 论文指出当前AI代理基准测试存在任务设置或奖励设计问题，并提出Agentic Benchmark Checklist（ABC）以提升评估严谨性。


<details>
  <summary>Details</summary>
Motivation: AI代理能力提升需要更严谨的基准测试，但现有基准测试存在任务设置或奖励设计问题，导致性能评估不准确。

Method: 通过分析现有基准测试的问题，结合最佳实践和报告问题，提出ABC指南，并在CVE-Bench上验证其效果。

Result: ABC在CVE-Bench上减少了33%的性能高估。

Conclusion: ABC能有效提升代理基准测试的严谨性，减少性能评估偏差。

Abstract: Benchmarks are essential for quantitatively tracking progress in AI. As AI
agents become increasingly capable, researchers and practitioners have
introduced agentic benchmarks to evaluate agents on complex, real-world tasks.
These benchmarks typically measure agent capabilities by evaluating task
outcomes via specific reward designs. However, we show that many agentic
benchmarks have issues task setup or reward design. For example, SWE-bench
Verified uses insufficient test cases, while TAU-bench counts empty responses
as successful. Such issues can lead to under- or overestimation agents'
performance by up to 100% in relative terms. To make agentic evaluation
rigorous, we introduce the Agentic Benchmark Checklist (ABC), a set of
guidelines that we synthesized from our benchmark-building experience, a survey
of best practices, and previously reported issues. When applied to CVE-Bench, a
benchmark with a particularly complex evaluation design, ABC reduces the
performance overestimation by 33%.

</details>


### [67] [StepHint: Multi-level Stepwise Hints Enhance Reinforcement Learning to Reason](https://arxiv.org/abs/2507.02841)
*Kaiyi Zhang,Ang Lv,Jinpeng Li,Yongbo Wang,Feng Wang,Haoyuan Hu,Rui Yan*

Main category: cs.AI

TL;DR: StepHint是一种新的RLVR算法，通过多级逐步提示改进LLMs的复杂推理能力，解决奖励近失和探索停滞问题。


<details>
  <summary>Details</summary>
Motivation: 当前RLVR方法存在奖励近失问题和探索停滞问题，影响训练效率和模型表现。

Method: StepHint利用多级逐步提示，从强模型中生成有效推理链并自适应分区，提供不同级别的提示以引导探索。

Result: StepHint在六个数学基准测试中表现优异，并展现出更好的泛化能力和域外基准测试优势。

Conclusion: StepHint通过多级提示有效解决了RLVR的关键问题，提升了模型的推理能力和训练效率。

Abstract: Reinforcement learning with verifiable rewards (RLVR) is a promising approach
for improving the complex reasoning abilities of large language models (LLMs).
However, current RLVR methods face two significant challenges: the near-miss
reward problem, where a small mistake can invalidate an otherwise correct
reasoning process, greatly hindering training efficiency; and exploration
stagnation, where models tend to focus on solutions within their ``comfort
zone,'' lacking the motivation to explore potentially more effective
alternatives. To address these challenges, we propose StepHint, a novel RLVR
algorithm that utilizes multi-level stepwise hints to help models explore the
solution space more effectively. StepHint generates valid reasoning chains from
stronger models and partitions these chains into reasoning steps using our
proposed adaptive partitioning method. The initial few steps are used as hints,
and simultaneously, multiple-level hints (each comprising a different number of
steps) are provided to the model. This approach directs the model's exploration
toward a promising solution subspace while preserving its flexibility for
independent exploration. By providing hints, StepHint mitigates the near-miss
reward problem, thereby improving training efficiency. Additionally, the
external reasoning pathways help the model develop better reasoning abilities,
enabling it to move beyond its ``comfort zone'' and mitigate exploration
stagnation. StepHint outperforms competitive RLVR enhancement methods across
six mathematical benchmarks, while also demonstrating superior generalization
and excelling over baselines on out-of-domain benchmarks.

</details>
