<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 23]
- [cs.CR](#cs.CR) [Total: 37]
- [cs.AI](#cs.AI) [Total: 45]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [Knowledge-Guided Multi-Agent Framework for Automated Requirements Development: A Vision](https://arxiv.org/abs/2506.22656)
*Jiangping Huang,Dongming Jin,Weisong Sun,Yang Liu,Zhi Jin*

Main category: cs.SE

TL;DR: KGMAF是一个知识引导的多代理框架，用于自动化需求开发，填补当前SE自动化系统在需求任务中的不足。


<details>
  <summary>Details</summary>
Motivation: 当前自动化系统过于关注代码开发，忽视了需求任务的复杂性，KGMAF旨在解决这一问题。

Method: KGMAF由六个专用代理和一个工件池组成，详细描述了每个代理的功能、行为和知识，并提供了工件池的概念设计。

Result: 案例研究表明KGMAF在现实场景中具有潜力。

Conclusion: KGMAF有望在LLM时代推动自动化需求开发的未来，并提出了进一步的研究方向。

Abstract: This paper envisions a knowledge-guided multi-agent framework named KGMAF for
automated requirements development. KGMAF aims to address gaps in current
automation systems for SE, which prioritize code development and overlook the
complexities of requirements tasks. KGMAF is composed of six specialized agents
and an artifact pool to improve efficiency and accuracy. Specifically, KGMAF
outlines the functionality, actions, and knowledge of each agent and provides
the conceptual design of the artifact pool. Our case study highlights the
potential of KGMAF in real-world scenarios. Finally, we outline several
research opportunities for implementing and enhancing automated requirements
development using multi-agent systems. We believe that KGMAF will play a
pivotal role in shaping the future of automated requirements development in the
era of LLMs.

</details>


### [2] [An LLM-assisted approach to designing software architectures using ADD](https://arxiv.org/abs/2506.22688)
*Humberto Cervantes,Rick Kazman,Yuanfang Cai*

Main category: cs.SE

TL;DR: 论文提出了一种基于大型语言模型（LLM）辅助的软件架构设计方法，结合属性驱动设计（ADD）方法，通过案例验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 传统软件架构设计依赖专家判断，过程复杂且迭代性强，本文旨在探索LLM在架构设计中的辅助作用。

Method: 通过为LLM提供ADD方法的明确描述、架构师角色和结构化迭代计划，指导LLM与人类架构师协作生成架构设计。

Result: 案例研究表明，LLM辅助的ADD方法能生成与成熟方案接近的设计，部分满足架构驱动因素，但也存在局限性。

Conclusion: LLM在架构设计中具有潜力，但仍需人类监督和迭代优化。

Abstract: Designing effective software architectures is a complex, iterative process
that traditionally relies on expert judgment. This paper proposes an approach
for Large Language Model (LLM)-assisted software architecture design using the
Attribute-Driven Design (ADD) method. By providing an LLM with an explicit
description of ADD, an architect persona, and a structured iteration plan, our
method guides the LLM to collaboratively produce architecture artifacts with a
human architect. We validate the approach through case studies, comparing
generated designs against proven solutions and evaluating them with
professional architects. Results show that our LLM-assisted ADD process can
generate architectures closely aligned with established solutions and partially
satisfying architectural drivers, highlighting both the promise and current
limitations of using LLMs in architecture design. Our findings emphasize the
importance of human oversight and iterative refinement when leveraging LLMs in
this domain.

</details>


### [3] [P4OMP: Retrieval-Augmented Prompting for OpenMP Parallelism in Serial Code](https://arxiv.org/abs/2506.22703)
*Wali Mohammad Abdullah,Azmain Kabir*

Main category: cs.SE

TL;DR: P4OMP是一个基于检索增强的框架，利用大语言模型将串行C/C++代码转换为OpenMP并行代码，无需微调或编译器插桩，显著提升了代码生成的可靠性。


<details>
  <summary>Details</summary>
Motivation: 解决传统方法在生成OpenMP并行代码时的语法错误和可靠性问题，通过检索增强生成技术提高代码的正确性。

Method: 采用检索增强生成（RAG）技术，结合OpenMP教程的结构化知识，优化提示驱动的代码生成。

Result: 在108个真实C++程序测试中，P4OMP实现了100%的编译成功率，而基线方法在20个案例中失败。

Conclusion: P4OMP显著提升了LLM生成OpenMP代码的可靠性和适用性，避免了常见的语法错误和无效指令组合。

Abstract: We present P4OMP, a retrieval-augmented framework for transforming serial
C/C++ code into OpenMP-annotated parallel code using large language models
(LLMs). To our knowledge, this is the first system to apply retrieval-based
prompting for OpenMP pragma correctness without model fine-tuning or compiler
instrumentation. P4OMP leverages Retrieval-Augmented Generation (RAG) with
structured instructional knowledge from OpenMP tutorials to improve the
reliability of prompt-driven code generation. By grounding generation in the
retrieved context, P4OMP improves syntactic correctness compared to baseline
prompting with GPT-3.5-Turbo. We evaluate P4OMP against a baseline,
GPT-3.5-Turbo without retrieval, on a comprehensive benchmark of 108 real-world
C++ programs drawn from Stack Overflow, PolyBench, and NAS benchmark suites.
P4OMP achieves 100% compilation success on all parallelizable cases, while the
baseline fails to compile in 20 out of 108 cases. Six cases that rely on
non-random-access iterators or thread-unsafe constructs are excluded due to
fundamental OpenMP limitations. A detailed analysis demonstrates how P4OMP
consistently avoids scoping errors, syntactic misuse, and invalid directive
combinations that commonly affect baseline-generated code. We further
demonstrate strong runtime scaling across seven compute-intensive benchmarks on
an HPC cluster. P4OMP offers a robust, modular pipeline that significantly
improves the reliability and applicability of LLM-generated OpenMP code.

</details>


### [4] [RAILS: Retrieval-Augmented Intelligence for Learning Software Development](https://arxiv.org/abs/2506.22742)
*Wali Mohammad Abdullah,Md. Morshedul Islam,Devraj Parmar,Happy Hasmukhbhai Patel,Sindhuja Prabhakaran,Baidya Saha*

Main category: cs.SE

TL;DR: RAILS框架通过检索增强和迭代验证，显著提升了LLM在Java开发中的代码补全准确性。


<details>
  <summary>Details</summary>
Motivation: LLM在软件开发中常生成不完整或错误的代码，尤其是缺乏外部文档支持时。

Method: RAILS结合FAISS和OpenAI嵌入，从Java资源中检索语义上下文，并通过编译器反馈迭代验证。

Result: 在78个Java导入错误案例中，RAILS优于基线方法，能保持意图并避免幻觉。

Conclusion: 未来将扩展RAILS支持更多语言和IDE，并集成符号过滤。

Abstract: Large Language Models (LLMs) like GPT-3.5-Turbo are increasingly used to
assist software development, yet they often produce incomplete code or
incorrect imports, especially when lacking access to external or
project-specific documentation. We introduce RAILS (Retrieval-Augmented
Intelligence for Learning Software Development), a framework that augments LLM
prompts with semantically retrieved context from curated Java resources using
FAISS and OpenAI embeddings. RAILS incorporates an iterative validation loop
guided by compiler feedback to refine suggestions. We evaluated RAILS on 78
real-world Java import error cases spanning standard libraries, GUI APIs,
external tools, and custom utilities. Despite using the same LLM, RAILS
outperforms baseline prompting by preserving intent, avoiding hallucinations,
and surfacing correct imports even when libraries are unavailable locally.
Future work will integrate symbolic filtering via PostgreSQL and extend support
to other languages and IDEs.

</details>


### [5] [Privacy-Preserving Methods for Bug Severity Prediction](https://arxiv.org/abs/2506.22752)
*Havvanur Dervişoğlu,Ruşen Halepmollası,Elif Eyvaz*

Main category: cs.SE

TL;DR: 论文研究了基于源代码指标和大语言模型（LLM）的方法级Bug严重性预测，比较了集中学习、联邦学习和合成数据生成的效果，发现联邦学习和合成数据在不共享数据的情况下也能达到与集中学习相当的效果。


<details>
  <summary>Details</summary>
Motivation: 工业应用中由于数据共享限制和标记数据有限，AI模型难以获取大量数据，因此需要隐私保护的方法来进行Bug严重性预测。

Method: 使用源代码指标和LLM，比较集中学习、联邦学习和合成数据生成三种方法在Bug严重性预测中的表现。

Result: 实验结果表明，联邦学习和合成数据生成的模型在不共享数据的情况下，性能与集中学习模型相当。

Conclusion: 联邦学习和合成数据生成是解决工业场景中数据共享挑战的有效隐私保护方法。

Abstract: Bug severity prediction is a critical task in software engineering as it
enables more efficient resource allocation and prioritization in software
maintenance. While AI-based analyses and models significantly require access to
extensive datasets, industrial applications face challenges due to data-sharing
constraints and the limited availability of labeled data. In this study, we
investigate method-level bug severity prediction using source code metrics and
Large Language Models (LLMs) with two widely used datasets. We compare the
performance of models trained using centralized learning, federated learning,
and synthetic data generation. Our experimental results, obtained using two
widely recognized software defect datasets, indicate that models trained with
federated learning and synthetic data achieve comparable results to centrally
trained models without data sharing. Our finding highlights the potential of
privacy-preserving approaches such as federated learning and synthetic data
generation to enable effective bug severity prediction in industrial context
where data sharing is a major challenge.
  The source code and dataset are available at our GitHub repository:
https://github.com/drvshavva/EASE2025-Privacy-Preserving-Methods-for-Bug-Severity-Prediction.

</details>


### [6] [Smaller = Weaker? Benchmarking Robustness of Quantized LLMs in Code Generation](https://arxiv.org/abs/2506.22776)
*Sen Fang,Weiyuan Ding,Antonio Mastropaolo,Bowen Xu*

Main category: cs.SE

TL;DR: 量化技术能压缩大语言模型（LLM），减少内存需求并加速推理，但其对鲁棒性的影响尚未充分研究。本文首次系统研究了量化对LLM在代码生成任务中鲁棒性的影响，发现量化后的模型通常比全精度模型更鲁棒。


<details>
  <summary>Details</summary>
Motivation: 量化技术虽广泛用于压缩LLM，但其对模型鲁棒性的影响尚未被系统研究。本文旨在填补这一空白，探究量化如何影响LLM在代码生成任务中的鲁棒性。

Method: 通过实验评估四种主流LLM家族（LLaMA、DeepSeek、CodeGen、StarCoder），参数规模从350M到33B，从对抗攻击和噪声扰动两个角度量化鲁棒性。

Result: 量化后的LLM在对抗攻击和噪声扰动实验中表现更优，51.59%的对抗实验显示量化模型更鲁棒，噪声扰动实验也证实量化模型能承受更高权重干扰。

Conclusion: 量化不仅能降低计算需求，还能提升LLM在代码生成任务中的鲁棒性，为开发更高效、可靠的LLM部署策略提供了新视角。

Abstract: Quantization has emerged as a mainstream method for compressing Large
Language Models (LLMs), reducing memory requirements and accelerating inference
without architectural modifications. While existing research primarily focuses
on evaluating the effectiveness of quantized LLMs compared to their original
counterparts, the impact on robustness remains largely unexplored.In this
paper, we present the first systematic investigation of how quantization
affects the robustness of LLMs in code generation tasks. Through extensive
experiments across four prominent LLM families (LLaMA, DeepSeek, CodeGen, and
StarCoder) with parameter scales ranging from 350M to 33B, we evaluate
robustness from dual perspectives: adversarial attacks on input prompts and
noise perturbations on model architecture. Our findings challenge conventional
wisdom by demonstrating that quantized LLMs often exhibit superior robustness
compared to their full-precision counterparts, with 51.59% versus 42.86% of our
adversarial experiments showing better resilience in quantized LLMs. Similarly,
our noise perturbation experiments also confirm that LLMs after quantitation
generally withstand higher levels of weight disturbances. These results suggest
that quantization not only reduces computational requirements but can actually
enhance LLMs' reliability in code generation tasks, providing valuable insights
for developing more robust and efficient LLM deployment strategies.

</details>


### [7] [Generating Privacy Stories From Software Documentation](https://arxiv.org/abs/2506.23014)
*Wilder Baldwin,Shashank Chintakuntla,Shreyah Parajuli,Ali Pourghasemi,Ryan Shanz,Sepideh Ghanavati*

Main category: cs.SE

TL;DR: 论文提出了一种基于链式思维提示（CoT）、上下文学习（ICL）和大语言模型（LLMs）的新方法，用于从软件开发文档中提取隐私行为并生成隐私需求。


<details>
  <summary>Details</summary>
Motivation: 当前方法主要关注从法规中提取法律要求并评估合规性，而隐私常被视为安全概念或事后考虑，导致用户隐私被侵犯。

Method: 结合CoT、ICL和LLMs，从软件开发文档中提取隐私行为，并以用户故事的形式生成隐私需求。

Result: 实验表明，常见LLMs（如GPT-4o和Llama 3）在识别隐私行为和生成隐私用户故事时F1分数超过0.8，且通过参数调优可进一步提升性能。

Conclusion: 研究为在软件开发生命周期中使用和优化LLMs生成隐私需求提供了见解。

Abstract: Research shows that analysts and developers consider privacy as a security
concept or as an afterthought, which may lead to non-compliance and violation
of users' privacy. Most current approaches, however, focus on extracting legal
requirements from the regulations and evaluating the compliance of software and
processes with them. In this paper, we develop a novel approach based on
chain-of-thought prompting (CoT), in-context-learning (ICL), and Large Language
Models (LLMs) to extract privacy behaviors from various software documents
prior to and during software development, and then generate privacy
requirements in the format of user stories. Our results show that most commonly
used LLMs, such as GPT-4o and Llama 3, can identify privacy behaviors and
generate privacy user stories with F1 scores exceeding 0.8. We also show that
the performance of these models could be improved through parameter-tuning. Our
findings provide insight into using and optimizing LLMs for generating privacy
requirements given software documents created prior to or throughout the
software development lifecycle.

</details>


### [8] [Guiding AI to Fix Its Own Flaws: An Empirical Study on LLM-Driven Secure Code Generation](https://arxiv.org/abs/2506.23034)
*Hao Yan,Swapneel Suhas Vaidya,Xiaokuan Zhang,Ziyu Yao*

Main category: cs.SE

TL;DR: 论文评估了大型语言模型（LLMs）在生成和修复不安全代码方面的能力，发现高级模型可通过漏洞提示和反馈改进安全性。


<details>
  <summary>Details</summary>
Motivation: LLMs在代码生成中常忽略安全实践，导致漏洞，但缺乏对其生成安全代码和修复漏洞能力的深入研究。

Method: 通过定量和定性分析，评估LLMs生成不安全代码的倾向、在漏洞提示下生成安全代码的能力，以及在不同反馈下修复漏洞的效果。

Result: LLMs易生成不安全代码，但高级模型可通过漏洞提示和细粒度反馈避免或修复漏洞。

Conclusion: 研究为开发者提供了减少LLMs生成代码漏洞的实际建议。

Abstract: Large Language Models (LLMs) have become powerful tools for automated code
generation. However, these models often overlook critical security practices,
which can result in the generation of insecure code that contains
vulnerabilities-weaknesses or flaws in the code that attackers can exploit to
compromise a system. However, there has been limited exploration of strategies
to guide LLMs in generating secure code and a lack of in-depth analysis of the
effectiveness of LLMs in repairing code containing vulnerabilities. In this
paper, we present a comprehensive evaluation of state-of-the-art LLMs by
examining their inherent tendencies to produce insecure code, their capability
to generate secure code when guided by self-generated vulnerability hints, and
their effectiveness in repairing vulnerabilities when provided with different
levels of feedback. Our study covers both proprietary and open-weight models
across various scales and leverages established benchmarks to assess a wide
range of vulnerability types. Through quantitative and qualitative analyses, we
reveal that although LLMs are prone to generating insecure code, advanced
models can benefit from vulnerability hints and fine-grained feedback to avoid
or fix vulnerabilities. We also provide actionable suggestions to developers to
reduce vulnerabilities when using LLMs for code generation.

</details>


### [9] [HF-DGF: Hybrid Feedback Guided Directed Grey-box Fuzzing](https://arxiv.org/abs/2506.23063)
*Guangfa Lyu,Zhenzhong Cao,Xiaofei Ren,Fengyu Wang*

Main category: cs.SE

TL;DR: HF-DGF是一种新型定向灰盒模糊测试框架，通过混合反馈机制（控制流距离、值流影响分数和切片覆盖率）提升效率和方向性，显著优于现有工具。


<details>
  <summary>Details</summary>
Motivation: 当前定向灰盒模糊测试工具因运行时反馈不足，限制了目标到达和状态空间探索的效率。

Method: 提出混合反馈机制，包括控制流距离（通过反向步进算法计算）、值流影响分数和切片覆盖率，并采用选择性插桩策略降低运行时开销。

Result: 在41个真实漏洞测试中，HF-DGF的崩溃复现速度显著优于现有工具，且代码覆盖率最低，显示其方向性和效率优势。

Conclusion: HF-DGF通过混合反馈机制和选择性插桩，显著提升了定向灰盒模糊测试的效率和方向性。

Abstract: Directed Grey-box Fuzzing (DGF) has emerged as a widely adopted technique for
crash reproduction and patch testing, leveraging its capability to precisely
navigate toward target locations and exploit vulnerabilities. However, current
DGF tools are constrained by insufficient runtime feedback, limiting their
efficiency in reaching targets and exploring state spaces. This study presents
HF-DGF, a novel directed grey-box fuzzing framework. Its seed scheduling is
guided by a hybrid feedback mechanism integrating control-flow distance,
value-flow influence score, and slice coverage. To enable precise control-flow
distance feedback, we propose a backward-stepping algorithm to calculate basic
block-level seed distances on a virtual inter-procedural control-flow graph
(ICFG). For effective state space exploration, we introduce value-flow
influence and a corresponding metric, the value-flow influence score.
Additionally, to mitigate runtime overhead from hybrid feedback, we adopt a
novel selective instrumentation strategy. Evaluations on 41 real-world
vulnerabilities show HF-DGF outperforms existing tools: it achieves crash
reproduction 5.05 times faster than AFL, 5.79 times faster than AFLGo, 73.75
times faster than WindRanger, 2.56 times faster than DAFL, and 8.45 times
faster than Beacon on average. Notably, when all fuzzers triggered crashes,
HF-DGF exhibited the lowest code coverage, demonstrating superior
directionality and efficiency. It also surpasses AFLGo, WindRanger, DAFL, and
Beacon in static analysis efficiency.

</details>


### [10] [Repair Ingredients Are All You Need: Improving Large Language Model-Based Program Repair via Repair Ingredients Search](https://arxiv.org/abs/2506.23100)
*Jiayi Zhang,Kai Huang,Jian Zhang,Yang Liu,Chunyang Chen*

Main category: cs.SE

TL;DR: ReinFix是一个基于大型语言模型（LLM）的自动程序修复框架，通过结合静态分析工具和历史修复数据，提升修复的准确性和上下文相关性。


<details>
  <summary>Details</summary>
Motivation: 现有LLM-based APR技术在生成修复补丁时缺乏上下文相关性和准确性，忽略了关键修复要素。

Method: ReinFix在推理阶段利用静态分析工具获取内部修复要素，在解决方案阶段从历史修复中搜索外部修复要素，指导LLM生成更准确的补丁。

Result: 在Defects4J V1.2和V2.0基准测试中，ReinFix分别修复了146和38个额外错误，优于现有技术。

Conclusion: ReinFix通过结合内部和外部修复要素，显著提升了LLM-based APR的性能，并在无数据泄漏风险的基准测试中保持最佳表现。

Abstract: Automated Program Repair (APR) techniques aim to automatically fix buggy
programs. Among these, Large Language Model-based (LLM-based) approaches have
shown great promise. Recent advances demonstrate that directly leveraging LLMs
can achieve leading results. However, these techniques remain suboptimal in
generating contextually relevant and accurate patches, as they often overlook
repair ingredients crucial for practical program repair. In this paper, we
propose ReinFix, a novel framework that enables LLMs to autonomously search for
repair ingredients throughout both the reasoning and solution phases of bug
fixing. In the reasoning phase, ReinFix integrates static analysis tools to
retrieve internal ingredients, such as variable definitions, to assist the LLM
in root cause analysis when it encounters difficulty understanding the context.
During the solution phase, when the LLM lacks experience in fixing specific
bugs, ReinFix searches for external ingredients from historical bug fixes with
similar bug patterns, leveraging both the buggy code and its root cause to
guide the LLM in identifying appropriate repair actions, thereby increasing the
likelihood of generating correct patches. Evaluations on two popular benchmarks
(Defects4J V1.2 and V2.0) demonstrate the effectiveness of our approach over
SOTA baselines. Notably, ReinFix fixes 146 bugs, which is 32 more than the
baselines on Defects4J V1.2. On Defects4J V2.0, ReinFix fixes 38 more bugs than
the SOTA. Importantly, when evaluating on the recent benchmarks that are free
of data leakage risk, ReinFix also maintains the best performance.

</details>


### [11] [From Release to Adoption: Challenges in Reusing Pre-trained AI Models for Downstream Developers](https://arxiv.org/abs/2506.23234)
*Peerachai Banyongrakkul,Mansooreh Zahedi,Patanamon Thongtanunam,Christoph Treude,Haoyu Gao*

Main category: cs.SE

TL;DR: 论文研究了预训练模型（PTMs）在下游开发中的挑战，分析了840个GitHub问题报告，提出了7类主要挑战，并发现PTM相关问题解决时间更长。


<details>
  <summary>Details</summary>
Motivation: 预训练模型（PTMs）性能优异且易于获取，但其在下游软件系统中的重用挑战尚未充分研究。

Method: 通过定性分析31个开源GitHub项目的840个PTM相关问题报告，系统构建了挑战分类。

Result: 识别了7类主要挑战（如模型使用、性能等），并发现PTM相关问题解决时间显著更长。

Conclusion: 研究为开发者提供了实践启示，并提出了未来研究方向。

Abstract: Pre-trained models (PTMs) have gained widespread popularity and achieved
remarkable success across various fields, driven by their groundbreaking
performance and easy accessibility through hosting providers. However, the
challenges faced by downstream developers in reusing PTMs in software systems
are less explored. To bridge this knowledge gap, we qualitatively created and
analyzed a dataset of 840 PTM-related issue reports from 31 OSS GitHub
projects. We systematically developed a comprehensive taxonomy of PTM-related
challenges that developers face in downstream projects. Our study identifies
seven key categories of challenges that downstream developers face in reusing
PTMs, such as model usage, model performance, and output quality. We also
compared our findings with existing taxonomies. Additionally, we conducted a
resolution time analysis and, based on statistical tests, found that
PTM-related issues take significantly longer to be resolved than issues
unrelated to PTMs, with significant variation across challenge categories. We
discuss the implications of our findings for practitioners and possibilities
for future research.

</details>


### [12] [On the Feasibility of Deduplicating Compiler Bugs with Bisection](https://arxiv.org/abs/2506.23281)
*Xintong Zhou,Zhenyang Xu,Chengnian Sun*

Main category: cs.SE

TL;DR: 论文提出了一种基于二分法的编译器Bug去重方法BugLens，显著优于现有技术。


<details>
  <summary>Details</summary>
Motivation: 随机测试发现的编译器Bug存在大量重复，传统基于程序分析的去重方法计算开销大且泛化性差。

Method: 利用二分法定位导致Bug的提交，并结合触发Bug的优化信息，提出BugLens方法。

Result: 在四个真实数据集上，BugLens比现有方法Tamer和D3平均节省26.98%和9.64%的人力。

Conclusion: 二分法因其简单性和泛化性，为编译器Bug去重提供了实用解决方案。

Abstract: Random testing has proven to be an effective technique for compiler
validation. However, the debugging of bugs identified through random testing
presents a significant challenge due to the frequent occurrence of duplicate
test programs that expose identical compiler bugs. The process to identify
duplicates is a practical research problem known as bug deduplication. Prior
methodologies for compiler bug deduplication primarily rely on program analysis
to extract bug-related features for duplicate identification, which can result
in substantial computational overhead and limited generalizability. This paper
investigates the feasibility of employing bisection, a standard debugging
procedure largely overlooked in prior research on compiler bug deduplication,
for this purpose. Our study demonstrates that the utilization of bisection to
locate failure-inducing commits provides a valuable criterion for
deduplication, albeit one that requires supplementary techniques for more
accurate identification. Building on these results, we introduce BugLens, a
novel deduplication method that primarily uses bisection, enhanced by the
identification of bug-triggering optimizations to minimize false negatives.
Empirical evaluations conducted on four real-world datasets demonstrate that
BugLens significantly outperforms the state-of-the-art analysis-based
methodologies Tamer and D3 by saving an average of 26.98% and 9.64% human
effort to identify the same number of distinct bugs. Given the inherent
simplicity and generalizability of bisection, it presents a highly practical
solution for compiler bug deduplication in real-world applications.

</details>


### [13] [Improving vulnerability type prediction and line-level detection via adversarial training-based data augmentation and multi-task learning](https://arxiv.org/abs/2506.23534)
*Siyu Chen,Jiongyi Yang,Xiang Chen,Menglin Zheng,Minnan Wei,Xiaolin Ju*

Main category: cs.SE

TL;DR: 提出了一种结合EDAT和MTL的统一方法，显著提升了VTP和LVD任务的性能，尤其是对罕见漏洞类型的识别。


<details>
  <summary>Details</summary>
Motivation: 软件漏洞对现代系统构成重大威胁，现有方法因样本稀缺和类别不平衡问题受限，且忽视了VTP和LVD任务间的相关性。

Method: 采用嵌入层驱动的对抗训练（EDAT）和多任务学习（MTL），通过共享语义模式和任务间相关性提升模型鲁棒性和性能。

Result: 实验表明，该方法在VTP和LVD任务上均优于现有基线，显著提升了罕见漏洞类型的识别和行级检测准确性。

Conclusion: EDAT与MTL的结合为漏洞检测提供了统一解决方案，性能显著提升，值得进一步研究。

Abstract: Context: Software vulnerabilities pose a significant threat to modern
software systems, as evidenced by the growing number of reported
vulnerabilities and cyberattacks. These escalating trends underscore the urgent
need for effective approaches that can automatically detect and understand
software vulnerabilities. Objective: However, the scarcity of labeled samples
and the class imbalance issue in vulnerability datasets present significant
challenges for both Vulnerability Type Prediction (VTP) and Line-level
Vulnerability Detection (LVD), especially for rare yet critical vulnerability
types. Moreover, most existing studies treat VTP and LVD as independent tasks,
overlooking their inherent correlation, which limits the potential to leverage
shared semantic patterns across tasks. Methods: To address these limitations,
we propose a unified approach that integrates Embedding-Layer Driven
Adversarial Training (EDAT) with Multi-task Learning (MTL). Specifically, EDAT
enhances model robustness by introducing adversarial perturbations to
identifier embeddings, guided by semantic importance. Meanwhile, MTL improves
overall performance by leveraging shared representations and inter-task
correlations between VTP and LVD. Results: Extensive experiments demonstrate
that our proposed approach outperforms state-of-the-art baselines on both VTP
and LVD tasks. For VTP, it yields notable improvements in accuracy, precision,
recall, and F1-score, particularly in identifying rare vulnerability types.
Similarly, for LVD, our approach enhances line-level detection accuracy while
significantly reducing false positives. Conclusion: Our study demonstrates that
combining EDAT with MTL provides a unified solution that improves performance
on both tasks and warrants further investigation.

</details>


### [14] [Comparative Analysis of the Code Generated by Popular Large Language Models (LLMs) for MISRA C++ Compliance](https://arxiv.org/abs/2506.23535)
*Malik Muhammad Umer*

Main category: cs.SE

TL;DR: 该论文比较了多种大型语言模型（如ChatGPT、Gemini、DeepSeek、Meta AI和Copilot）生成的C++代码是否符合MISRA C++安全编码标准。


<details>
  <summary>Details</summary>
Motivation: 安全关键系统的软件开发需严格遵守编码标准（如MISRA C++），而LLMs生成的代码可能不符合这些标准，因此需要验证其合规性。

Method: 对多个流行LLMs生成的C++代码进行对比分析，评估其是否符合MISRA C++标准。

Result: 论文展示了不同LLMs生成的代码在MISRA C++合规性方面的表现。

Conclusion: LLMs在安全关键领域的代码生成需进一步优化以确保符合严格的编码标准。

Abstract: Safety-critical systems are engineered systems whose failure or malfunction
could result in catastrophic consequences. The software development for
safety-critical systems necessitates rigorous engineering practices and
adherence to certification standards like DO-178C for avionics. DO-178C is a
guidance document which requires compliance to well-defined software coding
standards like MISRA C++ to enforce coding guidelines that prevent the use of
ambiguous, unsafe, or undefined constructs. Large Language Models (LLMs) have
demonstrated significant capabilities in automatic code generation across a
wide range of programming languages, including C++. Despite their impressive
performance, code generated by LLMs in safety-critical domains must be
carefully analyzed for conformance to MISRA C++ coding standards. In this
paper, I have conducted a comparative analysis of the C++ code generated by
popular LLMs including: OpenAI ChatGPT, Google Gemini, DeepSeek, Meta AI, and
Microsoft Copilot for compliance with MISRA C++.

</details>


### [15] [QLPro: Automated Code Vulnerability Discovery via LLM and Static Code Analysis Integration](https://arxiv.org/abs/2506.23644)
*Junze Hu,Xiangyu Jin,Yizhe Zeng,Yuling Liu,Yunpeng Li,Dan Du,Kaiyu Xie,Hongsong Zhu*

Main category: cs.SE

TL;DR: QLPro是一个结合LLM和静态分析工具的漏洞检测框架，在开源项目中表现优于CodeQL，检测到更多漏洞并发现新的0-day漏洞。


<details>
  <summary>Details</summary>
Motivation: 现有静态分析工具（如CodeQL）在漏洞检测中存在漏报问题，需要更全面的解决方案。

Method: QLPro系统性地整合了LLM和静态分析工具，构建了新的数据集JavaTest进行测试。

Result: QLPro检测到41个漏洞（CodeQL仅24个），并发现6个新漏洞，其中2个为0-day。

Conclusion: QLPro在漏洞检测中表现优异，具有实际应用潜力。

Abstract: We introduce QLPro, a vulnerability detection framework that systematically
integrates LLMs and static analysis tools to enable comprehensive vulnerability
detection across entire open-source projects.We constructed a new dataset,
JavaTest, comprising 10 open-source projects from GitHub with 62 confirmed
vulnerabilities. CodeQL, a state-of-the-art static analysis tool, detected only
24 of these vulnerabilities while QLPro detected 41. Furthermore, QLPro
discovered 6 previously unknown vulnerabilities, 2 of which have been confirmed
as 0-days.

</details>


### [16] [What Challenges Do Developers Face When Using Verification-Aware Programming Languages?](https://arxiv.org/abs/2506.23696)
*Francisco Oliveira,Alexandra Mendes,Carolina Carreira*

Main category: cs.SE

TL;DR: 研究探讨了验证感知（VA）编程语言采用率低的原因，通过分析开发者讨论和调查，发现学习曲线陡峭和可用性问题，并提出了改进建议。


<details>
  <summary>Details</summary>
Motivation: 尽管验证感知（VA）语言能提供更强的正确性保证，但其采用率仍然有限，研究旨在探索阻碍其采用的具体障碍。

Method: 通过主题建模分析开发者论坛讨论，并结合开发者调查，了解VA语言的实际挑战。

Result: 发现主要障碍包括学习曲线陡峭和可用性问题，建议简化工具界面、提供更好的教育材料和改进开发环境集成。

Conclusion: 研究为提升VA语言可用性和采用率提供了具体建议，包括改进工具和教育资源。

Abstract: Software reliability is critical in ensuring that the digital systems we
depend on function correctly. In software development, increasing software
reliability often involves testing. However, for complex and critical systems,
developers can use Design by Contract (DbC) methods to define precise
specifications that software components must satisfy. Verification-Aware (VA)
programming languages support DbC and formal verification at compile-time or
run-time, offering stronger correctness guarantees than traditional testing.
However, despite the strong guarantees provided by VA languages, their adoption
remains limited. In this study, we investigate the barriers to adopting VA
languages by analyzing developer discussions on public forums using topic
modeling techniques. We complement this analysis with a developer survey to
better understand the practical challenges associated with VA languages. Our
findings reveal key obstacles to adoption, including steep learning curves and
usability issues. Based on these insights, we identify actionable
recommendations to improve the usability and accessibility of VA languages. Our
findings suggest that simplifying tool interfaces, providing better educational
materials, and improving integration with everyday development environments
could improve the usability and adoption of these languages. Our work provides
actionable insights for improving the usability of VA languages and making
verification tools more accessible.

</details>


### [17] [Towards a Science of Developer eXperience (DevX)](https://arxiv.org/abs/2506.23715)
*Benoit Combemale*

Main category: cs.SE

TL;DR: 论文主张将开发者体验（DevX）作为独立研究领域，强调其对软件开发活动和生产力的重要性，并呼吁研究社区采取行动。


<details>
  <summary>Details</summary>
Motivation: 随着软件在现代生活中的普及，开发者的体验（DevX）对软件开发的影响尚未充分研究，需要更多关注。

Method: 基于现有研究，识别DevX的关键理论基础、科学推动因素和跨学科交叉点，并概述未来科学挑战。

Result: 提出DevX作为独立研究领域的必要性，并呼吁研究社区采取行动，推动更以人为本的软件工程方法。

Conclusion: DevX应成为软件工程的重要研究方向，以提升开发效率和协作多样性。

Abstract: As software continues to permeate nearly every facet of modern life, the
complexity and ubiquity of digital services underscore the need for
sustainable, effective, and inclusive software development practices. Although
software engineering has made significant progress in technical challenges
since its inception, the human experience of those involved in software
creation, broadly defined as developers, remains underexplored. This column
advocates for the formal recognition of Developer eXperience (DevX) as a
distinct research field. We argue that DevX profoundly influences critical
development activities and overall productivity, especially as development
becomes increasingly collaborative and diverse in terms of application domains.
Building on existing efforts to measure and enhance DevX, we identify key
rationales, scientific enablers, and interdisciplinary intersections that
support this emerging discipline. We also outline the core scientific
challenges ahead, aiming to call for actions from the research community and to
promote more human-centered approaches to software engineering.

</details>


### [18] [A Survey of LLM-based Automated Program Repair: Taxonomies, Design Paradigms, and Applications](https://arxiv.org/abs/2506.23749)
*Boyang Yang,Zijian Cai,Fengling Liu,Bach Le,Lingming Zhang,Tegawendé F. Bissyandé,Yang Liu,Haoye Tian*

Main category: cs.SE

TL;DR: 论文总结了2022年至2025年间63种基于大语言模型（LLM）的自动程序修复（APR）系统，将其分为四种范式，并分析了每种范式的优缺点及未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 探讨如何利用检索或分析增强的上下文优化LLM在APR中的应用，以解决现有方法的局限性。

Method: 通过分类和比较四种LLM-based APR范式（微调、提示、流程管道和代理框架），分析其性能与成本。

Result: 揭示了不同范式的关键权衡：微调任务对齐强但成本高；提示部署快但受限于设计；流程管道可控但开销中等；代理框架能处理复杂问题但延迟高。

Conclusion: 未来研究应结合轻量级人类反馈、仓库感知检索、代码分析和成本规划，以提升LLM-based APR的可靠性和效率。

Abstract: Large language models (LLMs) are reshaping automated program repair (APR). We
categorize the recent 63 LLM-based APR systems published from January 2022 to
June 2025 into four paradigms, and show how retrieval- or analysis-augmented
contexts strengthen any of them. This taxonomy clarifies key trade-offs:
fine-tuning delivers strong task alignment at high training cost; prompting
enables rapid deployment but is limited by prompt design and context windows;
procedural pipelines offer reproducible control with moderate overhead; agentic
frameworks tackle multi-hunk or cross-file bugs at the price of increased
latency and complexity. Persistent challenges include verifying semantic
correctness beyond test suites, repairing repository-scale defects, and
lowering the costs of LLMs. We outline research directions that combine
lightweight human feedback, repository-aware retrieval, code analysis, and
cost-aware planning to advance reliable and efficient LLM-based APR.

</details>


### [19] [Software Engineering for Large Language Models: Research Status, Challenges and the Road Ahead](https://arxiv.org/abs/2506.23762)
*Hongzhou Rao,Yanjie Zhao,Xinyi Hou,Shenao Wang,Haoyu Wang*

Main category: cs.SE

TL;DR: 本文从软件工程角度系统分析了大型语言模型（LLM）开发全生命周期的挑战与解决方案，分为六个阶段，并提出了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 现有研究未从软件工程角度系统探讨LLM开发中的复杂挑战，本文旨在填补这一空白。

Method: 将LLM开发分为六个阶段（需求工程、数据集构建、模型开发与增强、测试与评估、部署与运维、维护与演进），分析各阶段挑战。

Result: 总结了各阶段的关键挑战，并提出了潜在的研究方向。

Conclusion: 从软件工程视角为LLM开发的未来进展提供了有价值的见解。

Abstract: The rapid advancement of large language models (LLMs) has redefined
artificial intelligence (AI), pushing the boundaries of AI research and
enabling unbounded possibilities for both academia and the industry. However,
LLM development faces increasingly complex challenges throughout its lifecycle,
yet no existing research systematically explores these challenges and solutions
from the perspective of software engineering (SE) approaches. To fill the gap,
we systematically analyze research status throughout the LLM development
lifecycle, divided into six phases: requirements engineering, dataset
construction, model development and enhancement, testing and evaluation,
deployment and operations, and maintenance and evolution. We then conclude by
identifying the key challenges for each phase and presenting potential research
directions to address these challenges. In general, we provide valuable
insights from an SE perspective to facilitate future advances in LLM
development.

</details>


### [20] [Requirements for Active Assistance of Natural Questions in Software Architecture](https://arxiv.org/abs/2506.23898)
*Diogo Lemos,Ademar Aguiar,Neil B. Harrison*

Main category: cs.SE

TL;DR: 论文探讨了自然问题在架构设计中的重要性及其管理不善的后果，提出了一种支持自然问题生命周期的辅助环境，并通过调研验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 自然问题在架构设计中至关重要，但常被忽视或管理不善，导致架构漂移、知识丢失等问题。研究旨在理解其生命周期并设计支持环境。

Method: 基于文献、需求研讨会和三次设计迭代，提出了自然问题的生命周期，并通过专家调研验证了需求和功能。

Result: 研究提出了支持自然问题生命周期的环境，并验证了其比传统方法更能提升协作、决策和知识保存。

Conclusion: 辅助环境能有效管理自然问题，提升架构设计的效率和质量。

Abstract: Natural questions are crucial to shaping key architectural decisions and
preserving architectural knowledge. They arise organically during the
architectural design process, often resulting from the existing architectural
experience of the designer and the distinctive characteristics of the system
being designed. However, natural questions are often mismanaged or ignored,
which can lead to architectural drift, knowledge loss, inefficient resource
use, or poor understandability of the system's architecture. We aim to better
understand the lifecycle of natural questions, its key requirements, challenges
and difficulties, and then to envision an assisted environment to properly
support it. The environment should be adaptable and responsive to real-world
constraints and uncertainties by seamlessly integrating knowledge management
tools and artificial intelligence techniques into software development
workflows. Based on existing literature, a requirements workshop, and three
design iterations, we proposed a lifecycle for natural questions and elicited
essential functional and non-functional requirements for such an environment.
At last, the results of a survey conducted with experts helped to analyze and
validate the elicited requirements and proposed features for the environment to
enhance collaboration, decision-making, and the preservation of architectural
knowledge more effectively than conventional methods.

</details>


### [21] [Green Metrics Tool: Measuring for fun and profit](https://arxiv.org/abs/2506.23967)
*Geerd-Dietger Hoffmann,Verena Majuntke*

Main category: cs.SE

TL;DR: 论文介绍了Green Metrics Tool (GMT)，一个用于测量软件资源消耗的新框架，旨在优化资源使用和减少碳排放。


<details>
  <summary>Details</summary>
Motivation: 随着计算资源需求的增加，软件对环境的影响日益受到关注，需要测量和评估软件资源消耗以支持决策。

Method: 提出了GMT框架，采用容器化、可控和可重复的生命周期方法，评估软件在关键阶段的资源使用。

Result: GMT提供了可视化、可比较性以及基于规则和LLM的优化功能，帮助开发者和研究人员减少软件的环境影响。

Conclusion: GMT有潜力成为减少软件环境影响的实用工具。

Abstract: The environmental impact of software is gaining increasing attention as the
demand for computational resources continues to rise. In order to optimize
software resource consumption and reduce carbon emissions, measuring and
evaluating software is a first essential step. In this paper we discuss what
metrics are important for fact base decision making. We introduce the Green
Metrics Tool (GMT), a novel framework for accurately measuring the resource
consumption of software. The tool provides a containerized, controlled, and
reproducible life cycle-based approach, assessing the resource use of software
during key phases. Finally, we discuss GMT features like visualization,
comparability and rule- and LLM-based optimisations highlighting its potential
to guide developers and researchers in reducing the environmental impact of
their software.

</details>


### [22] [STCLocker: Deadlock Avoidance Testing for Autonomous Driving Systems](https://arxiv.org/abs/2506.23995)
*Mingfei Cheng,Renzhi Wang,Xiaofei Xie,Yuan Zhou,Lei Ma*

Main category: cs.SE

TL;DR: 提出了一种名为STCLocker的技术，用于测试多自动驾驶车辆（AVs）的协作性能，特别是避免死锁的能力。


<details>
  <summary>Details</summary>
Motivation: 现有技术主要关注单AV测试，而多AV环境中的协作性能评估不足，尤其是死锁问题。

Method: STCLocker包含三个关键组件：死锁检测器、冲突反馈和冲突感知场景生成，用于生成死锁场景。

Result: 实验表明，STCLocker在生成死锁场景方面优于现有基线方法。

Conclusion: STCLocker填补了多AV协作性能测试的空白，为AV安全部署提供了重要工具。

Abstract: Autonomous Driving System (ADS) testing is essential to ensure the safety and
reliability of autonomous vehicles (AVs) before deployment. However, existing
techniques primarily focus on evaluating ADS functionalities in single-AV
settings. As ADSs are increasingly deployed in multi-AV traffic, it becomes
crucial to assess their cooperative performance, particularly regarding
deadlocks, a fundamental coordination failure in which multiple AVs enter a
circular waiting state indefinitely, resulting in motion planning failures.
Despite its importance, the cooperative capability of ADSs to prevent deadlocks
remains insufficiently underexplored. To address this gap, we propose the first
dedicated Spatio-Temporal Conflict-Guided Deadlock Avoidance Testing technique,
STCLocker, for generating DeadLock Scenarios (DLSs), where a group of AVs
controlled by the ADS under test are in a circular wait state. STCLocker
consists of three key components: Deadlock Oracle, Conflict Feedback, and
Conflict-aware Scenario Generation. Deadlock Oracle provides a reliable
black-box mechanism for detecting deadlock cycles among multiple AVs within a
given scenario. Conflict Feedback and Conflict-aware Scenario Generation
collaborate to actively guide AVs into simultaneous competition over spatial
conflict resources (i.e., shared passing regions) and temporal competitive
behaviors (i.e., reaching the conflict region at the same time), thereby
increasing the effectiveness of generating conflict-prone deadlocks. We
evaluate STCLocker on two types of ADSs: Roach, an end-to-end ADS, and OpenCDA,
a module-based ADS supporting cooperative communication. Experimental results
show that, on average, STCLocker generates more DLS than the best-performing
baseline.

</details>


### [23] [Bug Fixing with Broader Context: Enhancing LLM-Based Program Repair via Layered Knowledge Injection](https://arxiv.org/abs/2506.24015)
*Ramtin Ehsani,Esteban Parra,Sonia Haiduc,Preetha Chatterjee*

Main category: cs.SE

TL;DR: 通过分层知识注入框架（Bug、Repository、Project三层）提升LLM在程序修复中的表现，修复率达到79%，但复杂和结构孤立的bug仍难以解决。


<details>
  <summary>Details</summary>
Motivation: 现有方法仅依赖局部代码信息修复bug，而开发者实际需要更广泛的上下文。

Method: 提出分层知识注入框架，逐步增加Bug、Repository、Project层级的上下文信息。

Result: 修复率提升23%，不同bug类型对上下文需求不同。

Conclusion: 分层上下文注入有效，但需交互式和自适应系统应对复杂bug。

Abstract: Prompting LLMs with bug-related context (e.g., error messages, stack traces)
improves automated program repair, but many bugs still remain unresolved. In
real-world projects, developers often rely on broader repository and
project-level context beyond the local code to resolve such bugs. In this
paper, we investigate how automatically extracting and providing such knowledge
can improve LLM-based program repair. We propose a layered knowledge injection
framework that incrementally augments LLMs with structured context. It starts
with the Bug Knowledge Layer, which includes information such as the buggy
function and failing tests; expands to the Repository Knowledge Layer, which
adds structural dependencies, related files, and commit history; and finally
injects the Project Knowledge Layer, which incorporates relevant details from
documentation and previously fixed bugs. We evaluate this framework on a
dataset of 314 bugs from BugsInPy using two LLMs (Llama 3.3 and GPT-4o-mini),
and analyze fix rates across six bug types. By progressively injecting
knowledge across layers, our approach achieves a fix rate of 79% (250/314)
using Llama 3.3, a significant improvement of 23% over previous work. All bug
types show improvement with the addition of repository-level context, while
only a subset benefit further from project-level knowledge, highlighting that
different bug types require different levels of contextual information for
effective repair. We also analyze the remaining unresolved bugs and find that
more complex and structurally isolated bugs, such as Program Anomaly and GUI
bugs, remain difficult even after injecting all available information. Our
results show that layered context injection improves program repair and suggest
the need for interactive and adaptive APR systems.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [24] [SABRE-FL: Selective and Accurate Backdoor Rejection for Federated Prompt Learning](https://arxiv.org/abs/2506.22506)
*Momin Ahmad Khan,Yasra Chandio,Fatima Muhammad Anwar*

Main category: cs.CR

TL;DR: 本文研究了联邦提示学习中的后门攻击，并提出了一种轻量级防御方法SABRE-FL，通过嵌入空间异常检测器过滤恶意更新。


<details>
  <summary>Details</summary>
Motivation: 联邦提示学习在隐私保护和通信效率方面具有优势，但其安全性尚未充分研究，尤其是后门攻击的威胁。

Method: 提出SABRE-FL防御方法，利用离线训练的嵌入空间异常检测器识别并过滤恶意提示更新。

Result: SABRE-FL在五个数据集上显著降低后门攻击准确率，同时保持干净数据的准确性，优于四种基线防御方法。

Conclusion: 研究表明联邦提示学习需要更强的鲁棒性，SABRE-FL为未来系统提供了一种有效的防御方案。

Abstract: Federated Prompt Learning has emerged as a communication-efficient and
privacy-preserving paradigm for adapting large vision-language models like CLIP
across decentralized clients. However, the security implications of this setup
remain underexplored. In this work, we present the first study of backdoor
attacks in Federated Prompt Learning. We show that when malicious clients
inject visually imperceptible, learnable noise triggers into input images, the
global prompt learner becomes vulnerable to targeted misclassification while
still maintaining high accuracy on clean inputs. Motivated by this
vulnerability, we propose SABRE-FL, a lightweight, modular defense that filters
poisoned prompt updates using an embedding-space anomaly detector trained
offline on out-of-distribution data. SABRE-FL requires no access to raw client
data or labels and generalizes across diverse datasets. We show, both
theoretically and empirically, that malicious clients can be reliably
identified and filtered using an embedding-based detector. Across five diverse
datasets and four baseline defenses, SABRE-FL outperforms all baselines by
significantly reducing backdoor accuracy while preserving clean accuracy,
demonstrating strong empirical performance and underscoring the need for robust
prompt learning in future federated systems.

</details>


### [25] [In-context learning for the classification of manipulation techniques in phishing emails](https://arxiv.org/abs/2506.22515)
*Antony Dalmiere,Guillaume Auriol,Vincent Nicomette,Pascal Marchand*

Main category: cs.CR

TL;DR: 研究利用LLM（GPT-4o-mini）的上下文学习技术，对钓鱼邮件中的40种心理操纵技术进行细粒度分类，准确率达0.76。


<details>
  <summary>Details</summary>
Motivation: 传统钓鱼检测常忽略心理操纵，本研究旨在填补这一空白。

Method: 采用Few-shot示例和GPT-4o-mini，对真实法语钓鱼邮件（SignalSpam）进行分类测试。

Result: 在100封人工标注的测试邮件中，准确识别了常见操纵技术（如诱饵、好奇心吸引、小请求），准确率为0.76。

Conclusion: 证明了上下文学习在钓鱼分析中的潜力，并揭示了攻击者策略。

Abstract: Traditional phishing detection often overlooks psychological manipulation.
This study investigates using Large Language Model (LLM) In-Context Learning
(ICL) for fine-grained classification of phishing emails based on a taxonomy of
40 manipulation techniques. Using few-shot examples with GPT-4o-mini on
real-world French phishing emails (SignalSpam), we evaluated performance
against a human-annotated test set (100 emails). The approach effectively
identifies prevalent techniques (e.g., Baiting, Curiosity Appeal, Request For
Minor Favor) with a promising accuracy of 0.76. This work demonstrates ICL's
potential for nuanced phishing analysis and provides insights into attacker
strategies.

</details>


### [26] [A Survey on Model Extraction Attacks and Defenses for Large Language Models](https://arxiv.org/abs/2506.22521)
*Kaixiang Zhao,Lincan Li,Kaize Ding,Neil Zhenqiang Gong,Yue Zhao,Yushun Dong*

Main category: cs.CR

TL;DR: 本文综述了针对语言模型的提取攻击及其防御方法，分类了攻击类型并分析了攻击与防御的有效性，提出了评估指标和研究方向。


<details>
  <summary>Details</summary>
Motivation: 模型提取攻击对语言模型的安全构成威胁，可能损害知识产权和用户隐私，因此需要系统化的分析和防御策略。

Method: 通过分类攻击（功能提取、训练数据提取、提示攻击）和分析防御机制（模型保护、数据隐私保护、提示策略），提出评估指标。

Result: 分析了攻击与防御方法的有效性，指出当前方法的局限性，并提出未来研究方向。

Conclusion: 本文为NLP研究者、ML工程师和安全专业人员提供了保护语言模型的实用指南，并提出了平衡安全与模型效用的研究方向。

Abstract: Model extraction attacks pose significant security threats to deployed
language models, potentially compromising intellectual property and user
privacy. This survey provides a comprehensive taxonomy of LLM-specific
extraction attacks and defenses, categorizing attacks into functionality
extraction, training data extraction, and prompt-targeted attacks. We analyze
various attack methodologies including API-based knowledge distillation, direct
querying, parameter recovery, and prompt stealing techniques that exploit
transformer architectures. We then examine defense mechanisms organized into
model protection, data privacy protection, and prompt-targeted strategies,
evaluating their effectiveness across different deployment scenarios. We
propose specialized metrics for evaluating both attack effectiveness and
defense performance, addressing the specific challenges of generative language
models. Through our analysis, we identify critical limitations in current
approaches and propose promising research directions, including integrated
attack methodologies and adaptive defense mechanisms that balance security with
model utility. This work serves NLP researchers, ML engineers, and security
professionals seeking to protect language models in production environments.

</details>


### [27] [MetaCipher: A General and Extensible Reinforcement Learning Framework for Obfuscation-Based Jailbreak Attacks on Black-Box LLMs](https://arxiv.org/abs/2506.22557)
*Boyuan Chen,Minghao Shao,Abdul Basit,Siddharth Garg,Muhammad Shafique*

Main category: cs.CR

TL;DR: MetaCipher是一种新型的基于混淆的越狱框架，通过强化学习动态选择加密策略，显著提高了攻击成功率和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLM）面临日益复杂的越狱攻击，尤其是基于混淆的攻击，现有防御机制难以应对。

Method: 提出MetaCipher框架，结合强化学习的动态加密策略选择机制，支持多种加密方式。

Result: 在10次查询内，MetaCipher对非推理型LLM的攻击成功率达92%，对推理型LLM达74%，优于现有方法。

Conclusion: MetaCipher具有长期鲁棒性和适应性，能够有效应对不断升级的安全措施。

Abstract: The growing capabilities of large language models (LLMs) have exposed them to
increasingly sophisticated jailbreak attacks. Among these, obfuscation-based
attacks -- which encrypt malicious content to evade detection -- remain highly
effective. By leveraging the reasoning ability of advanced LLMs to interpret
encrypted prompts, such attacks circumvent conventional defenses that rely on
keyword detection or context filtering. These methods are very difficult to
defend against, as existing safety mechanisms are not designed to interpret or
decode ciphered content. In this work, we propose \textbf{MetaCipher}, a novel
obfuscation-based jailbreak framework, along with a reinforcement
learning-based dynamic cipher selection mechanism that adaptively chooses
optimal encryption strategies from a cipher pool. This approach enhances
jailbreak effectiveness and generalizability across diverse task types, victim
LLMs, and safety guardrails. Our framework is modular and extensible by design,
supporting arbitrary cipher families and accommodating evolving adversarial
strategies. We complement our method with a large-scale empirical analysis of
cipher performance across multiple victim LLMs. Within as few as 10 queries,
MetaCipher achieves over 92\% attack success rate (ASR) on most recent standard
malicious prompt benchmarks against state-of-the-art non-reasoning LLMs, and
over 74\% ASR against reasoning-capable LLMs, outperforming all existing
obfuscation-based jailbreak methods. These results highlight the long-term
robustness and adaptability of our approach, making it more resilient than
prior methods in the face of advancing safety measures.

</details>


### [28] [A User-Centric, Privacy-Preserving, and Verifiable Ecosystem for Personal Data Management and Utilization](https://arxiv.org/abs/2506.22606)
*Osama Zafar,Mina Namazi,Yuqiao Xu,Youngjin Yoo,Erman Ayday*

Main category: cs.CR

TL;DR: 论文提出了一种去中心化的隐私保护架构，解决集中式管理个人数据带来的隐私和安全问题，赋予用户数据所有权和控制权。


<details>
  <summary>Details</summary>
Motivation: 集中式数据管理存在隐私泄露和安全风险，用户对数据的控制权不足，亟需一种新的方法来保护个人数据隐私。

Method: 采用去中心化架构，结合安全飞地和联邦学习等隐私增强技术，支持本地计算、模型训练和隐私保护的数据共享。

Result: 系统实现了用户对数据的完全控制，确保数据可信性和隐私安全，同时支持多种功能。

Conclusion: 去中心化隐私保护架构是解决当前数据隐私和安全问题的有效方法。

Abstract: In the current paradigm of digital personalized services, the centralized
management of personal data raises significant privacy concerns, security
vulnerabilities, and diminished individual autonomy over sensitive information.
Despite their efficiency, traditional centralized architectures frequently fail
to satisfy rigorous privacy requirements and expose users to data breaches and
unauthorized access risks. This pressing challenge calls for a fundamental
paradigm shift in methodologies for collecting, storing, and utilizing personal
data across diverse sectors, including education, healthcare, and finance.
  This paper introduces a novel decentralized, privacy-preserving architecture
that handles heterogeneous personal information, ranging from educational
credentials to health records and financial data. Unlike traditional models,
our system grants users complete data ownership and control, allowing them to
selectively share information without compromising privacy. The architecture's
foundation comprises advanced privacy-enhancing technologies, including secure
enclaves and federated learning, enabling secure computation, verification, and
data sharing. The system supports diverse functionalities, including local
computation, model training, and privacy-preserving data sharing, while
ensuring data credibility and robust user privacy.

</details>


### [29] [Fingerprinting SDKs for Mobile Apps and Where to Find Them: Understanding the Market for Device Fingerprinting](https://arxiv.org/abs/2506.22639)
*Michael A. Specter,Mihai Christodorescu,Abbie Farr,Bo Ma,Robin Lassonde,Xiaoyang Xu,Xiang Pan,Fengguo Wei,Saswat Anand,Dave Kleidermacher*

Main category: cs.CR

TL;DR: 该论文对移动应用生态系统中类似指纹识别的行为进行了大规模分析，发现广告SDK仅占30.56%的指纹行为，而23.92%来自目的不明的SDK。


<details>
  <summary>Details</summary>
Motivation: 研究动机是揭示移动应用中第三方SDK的指纹识别行为，尤其是广告SDK以外的其他来源。

Method: 方法包括分析228,000多个SDK和178,000个Android应用，通过静态分析检测500多个信号的外泄。

Result: 结果显示广告SDK仅占指纹行为的30.56%，23.92%来自未知目的SDK，安全与认证SDK占11.7%。

Conclusion: 结论表明仅针对特定市场（如广告）的指纹识别政策效果有限，需更全面的解决方案。

Abstract: This paper presents a large-scale analysis of fingerprinting-like behavior in
the mobile application ecosystem. We take a market-based approach, focusing on
third-party tracking as enabled by applications' common use of third-party
SDKs. Our dataset consists of over 228,000 SDKs from popular Maven
repositories, 178,000 Android applications collected from the Google Play
store, and our static analysis pipeline detects exfiltration of over 500
individual signals. To the best of our knowledge, this represents the
largest-scale analysis of SDK behavior undertaken to date.
  We find that Ads SDKs (the ostensible focus of industry efforts such as
Apple's App Tracking Transparency and Google's Privacy Sandbox) appear to be
the source of only 30.56% of the fingerprinting behaviors. A surprising 23.92%
originate from SDKs whose purpose was unknown or unclear. Furthermore, Security
and Authentication SDKs are linked to only 11.7% of likely fingerprinting
instances. These results suggest that addressing fingerprinting solely in
specific market-segment contexts like advertising may offer incomplete benefit.
Enforcing anti-fingerprinting policies is also complex, as we observe a sparse
distribution of signals and APIs used by likely fingerprinting SDKs. For
instance, only 2% of exfiltrated APIs are used by more than 75% of SDKs, making
it difficult to rely on user permissions to control fingerprinting behavior.

</details>


### [30] [VERA: Variational Inference Framework for Jailbreaking Large Language Models](https://arxiv.org/abs/2506.22666)
*Anamika Lochab,Lu Yan,Patrick Pynadath,Xiangyu Zhang,Ruqi Zhang*

Main category: cs.CR

TL;DR: 论文提出了一种名为VERA的变分推理框架，用于黑盒越狱提示生成，解决了现有方法依赖遗传算法和手动优化的问题。


<details>
  <summary>Details</summary>
Motivation: 由于现有黑盒越狱方法依赖遗传算法和手动优化，无法全面评估模型漏洞，因此需要一种更高效的方法。

Method: VERA将黑盒越狱提示生成建模为变分推理问题，训练一个小型攻击者LLM来近似目标LLM的后验分布。

Result: 实验表明，VERA在多种目标LLM上表现优异，验证了概率推理在对抗提示生成中的价值。

Conclusion: VERA提供了一种高效且全面的黑盒越狱方法，为模型漏洞评估提供了新思路。

Abstract: The rise of API-only access to state-of-the-art LLMs highlights the need for
effective black-box jailbreak methods to identify model vulnerabilities in
real-world settings. Without a principled objective for gradient-based
optimization, most existing approaches rely on genetic algorithms, which are
limited by their initialization and dependence on manually curated prompt
pools. Furthermore, these methods require individual optimization for each
prompt, failing to provide a comprehensive characterization of model
vulnerabilities. To address this gap, we introduce VERA: Variational infErence
fRamework for jAilbreaking. VERA casts black-box jailbreak prompting as a
variational inference problem, training a small attacker LLM to approximate the
target LLM's posterior over adversarial prompts. Once trained, the attacker can
generate diverse, fluent jailbreak prompts for a target query without
re-optimization. Experimental results show that VERA achieves strong
performance across a range of target LLMs, highlighting the value of
probabilistic inference for adversarial prompt generation.

</details>


### [31] [General Autonomous Cybersecurity Defense: Learning Robust Policies for Dynamic Topologies and Diverse Attackers](https://arxiv.org/abs/2506.22706)
*Arun Ramamurthy,Neil Dhir*

Main category: cs.CR

TL;DR: 论文探讨了在动态网络环境中开发通用自主网络安全防御（GACD）系统的方法，以解决现有系统因静态假设和过拟合而无法适应真实网络变化的问题。


<details>
  <summary>Details</summary>
Motivation: 现有自主网络安全防御（ACD）系统依赖静态网络假设，无法适应动态变化的网络拓扑，导致防御能力受限。

Method: 研究开发通用自主网络安全防御（GACD）系统，旨在学习适用于动态网络环境的通用策略。

Result: 提出了解决现有系统局限性的方法，重点关注策略的通用性和适应性。

Conclusion: 通过GACD系统，可以提升防御代理在动态网络环境中的适应性和泛化能力。

Abstract: In the face of evolving cyber threats such as malware, ransomware and
phishing, autonomous cybersecurity defense (ACD) systems have become essential
for real-time threat detection and response with optional human intervention.
However, existing ACD systems rely on limiting assumptions, particularly the
stationarity of the underlying network dynamics. In real-world scenarios,
network topologies can change due to actions taken by attackers or defenders,
system failures, or time evolution of networks, leading to failures in the
adaptive capabilities of current defense agents. Moreover, many agents are
trained on static environments, resulting in overfitting to specific
topologies, which hampers their ability to generalize to out-of-distribution
network topologies. This work addresses these challenges by exploring methods
for developing agents to learn generalizable policies across dynamic network
environments -- general ACD (GACD).

</details>


### [32] [Kill Two Birds with One Stone! Trajectory enabled Unified Online Detection of Adversarial Examples and Backdoor Attacks](https://arxiv.org/abs/2506.22722)
*Anmin Fu,Fanyu Meng,Huaibing Peng,Hua Ma,Zhi Zhang,Yifeng Zheng,Willy Susilo,Yansong Gao*

Main category: cs.CR

TL;DR: UniGuard是一个统一的在线检测框架，能够同时检测对抗性示例和后门攻击，通过分析输入在深度学习模型中的传播轨迹差异来实现高效检测。


<details>
  <summary>Details</summary>
Motivation: 对抗性示例（AE）和后门攻击都会影响模型的推理阶段，因此需要一种统一的在线检测方法来解决这两种威胁。

Method: UniGuard将输入在模型中的传播轨迹视为时间序列信号，利用LSTM和频谱变换放大对抗性和良性轨迹之间的差异。

Result: UniGuard在多种模态（图像、文本、音频）和任务（分类和回归）中表现出高效性和有效性，优于现有的针对单一威胁的SOTA方法。

Conclusion: UniGuard是首个能够同时处理对抗性示例和后门攻击的统一框架，其性能优于现有方法，具有广泛的应用潜力。

Abstract: The proposed UniGuard is the first unified online detection framework capable
of simultaneously addressing adversarial examples and backdoor attacks.
UniGuard builds upon two key insights: first, both AE and backdoor attacks have
to compromise the inference phase, making it possible to tackle them
simultaneously during run-time via online detection. Second, an adversarial
input, whether a perturbed sample in AE attacks or a trigger-carrying sample in
backdoor attacks, exhibits distinctive trajectory signatures from a benign
sample as it propagates through the layers of a DL model in forward inference.
The propagation trajectory of the adversarial sample must deviate from that of
its benign counterpart; otherwise, the adversarial objective cannot be
fulfilled. Detecting these trajectory signatures is inherently challenging due
to their subtlety; UniGuard overcomes this by treating the propagation
trajectory as a time-series signal, leveraging LSTM and spectrum transformation
to amplify differences between adversarial and benign trajectories that are
subtle in the time domain. UniGuard exceptional efficiency and effectiveness
have been extensively validated across various modalities (image, text, and
audio) and tasks (classification and regression), ranging from diverse model
architectures against a wide range of AE attacks and backdoor attacks,
including challenging partial backdoors and dynamic triggers. When compared to
SOTA methods, including ContraNet (NDSS 22) specific for AE detection and TED
(IEEE SP 24) specific for backdoor detection, UniGuard consistently
demonstrates superior performance, even when matched against each method's
strengths in addressing their respective threats-each SOTA fails to parts of
attack strategies while UniGuard succeeds for all.

</details>


### [33] [Convergent Privacy Framework with Contractive GNN Layers for Multi-hop Aggregations](https://arxiv.org/abs/2506.22727)
*Yu Zheng,Chenang Li,Zhou Li,Qingsong Wang*

Main category: cs.CR

TL;DR: 论文提出了一种名为CARIBOU的框架，通过引入Contractive Graph Layer（CGL）和隐私放大技术，解决了图神经网络（GNNs）中隐私预算随层数线性增长的问题，显著提升了隐私与效用的平衡。


<details>
  <summary>Details</summary>
Motivation: 现有方法在保护图神经网络隐私时，隐私预算随层数线性增长，导致噪声过大，影响模型性能。本文旨在通过理论分析和新技术解决这一问题。

Method: 提出Contractive Graph Layer（CGL），利用标准GNN操作的收缩性质，结合隐私放大技术，确保隐私预算收敛。CARIBOU框架包含收缩聚合模块、隐私分配模块和隐私审计模块。

Result: 实验表明，CARIBOU显著改善了隐私与效用的权衡，并在隐私审计任务中表现优异。

Conclusion: 通过理论分析和CGL设计，CARIBOU有效解决了深度GNNs中的隐私预算问题，为隐私保护提供了更优的解决方案。

Abstract: Differential privacy (DP) has been integrated into graph neural networks
(GNNs) to protect sensitive structural information, e.g., edges, nodes, and
associated features across various applications. A common approach is to
perturb the message-passing process, which forms the core of most GNN
architectures. However, existing methods typically incur a privacy cost that
grows linearly with the number of layers (Usenix Security'23), ultimately
requiring excessive noise to maintain a reasonable privacy level. This
limitation becomes particularly problematic when deep GNNs are necessary to
capture complex and long-range interactions in graphs. In this paper, we
theoretically establish that the privacy budget can converge with respect to
the number of layers by applying privacy amplification techniques to the
message-passing process, exploiting the contractive properties inherent to
standard GNN operations. Motivated by this analysis, we propose a simple yet
effective Contractive Graph Layer (CGL) that ensures the contractiveness
required for theoretical guarantees while preserving model utility. Our
framework, CARIBOU, supports both training and inference, equipped with a
contractive aggregation module, a privacy allocation module, and a privacy
auditing module. Experimental evaluations demonstrate that CARIBOU
significantly improves the privacy-utility trade-off and achieves superior
performance in privacy auditing tasks.

</details>


### [34] [Enhancing Android Malware Detection with Retrieval-Augmented Generation](https://arxiv.org/abs/2506.22750)
*Saraga S.,Anagha M. S.,Dincy R. Arikkat,Rafidha Rehiman K. A.,Serena Nicolazzo,Antonino Nocera,Vinod P*

Main category: cs.CR

TL;DR: 该论文提出了一种基于机器学习的Android恶意软件检测方法，结合静态分析和LLM生成的功能描述，通过RAG减少幻觉问题，提高了检测准确性。


<details>
  <summary>Details</summary>
Motivation: Android应用的广泛使用使其成为网络攻击的主要目标，恶意软件威胁用户隐私和设备功能。传统静态分析方法存在局限性，因此需要更高效的方法。

Method: 收集良性及恶意APK数据集，通过静态分析提取特征（如代码结构、权限等），利用LLM生成功能描述，结合RAG减少幻觉，最后用基于Transformer的模型分析。

Result: 该方法比传统基于特征的方法在恶意软件检测上具有更高的准确性。

Conclusion: 结合LLM和RAG的静态分析方法能有效提升恶意软件检测性能，为未来研究提供了新方向。

Abstract: The widespread use of Android applications has made them a prime target for
cyberattacks, significantly increasing the risk of malware that threatens user
privacy, security, and device functionality. Effective malware detection is
thus critical, with static analysis, dynamic analysis, and Machine Learning
being widely used approaches. In this work, we focus on a Machine
Learning-based method utilizing static features. We first compiled a dataset of
benign and malicious APKs and performed static analysis to extract features
such as code structure, permissions, and manifest file content, without
executing the apps. Instead of relying solely on raw static features, our
system uses an LLM to generate high-level functional descriptions of APKs. To
mitigate hallucinations, which are a known vulnerability of LLM, we integrated
Retrieval-Augmented Generation (RAG), enabling the LLM to ground its output in
relevant context. Using carefully designed prompts, we guide the LLM to produce
coherent function summaries, which are then analyzed using a transformer-based
model, improving detection accuracy over conventional feature-based methods for
malware detection.

</details>


### [35] [What's Privacy Good for? Measuring Privacy as a Shield from Harms due to Personal Data Use](https://arxiv.org/abs/2506.22787)
*Sri Harsha Gajavalli,Junichi Koizumi,Rakibul Hasan*

Main category: cs.CR

TL;DR: 论文提出了一种以危害为中心的隐私概念化方法，探讨了隐私如何防止个人数据使用带来的危害。通过在线研究验证了14种危害的感知，并提供了改进隐私的实用建议。


<details>
  <summary>Details</summary>
Motivation: 现有隐私框架（如情境完整性）难以捕捉或分类现代技术使用个人数据带来的多种危害。

Method: 通过一项400名大学生的在线研究，参与者评估了AI算法推断个人数据可能导致的14种危害。

Result: 研究发现14种危害具有内部一致性，代表了隐私危害的一般概念，并揭示了不同背景和人口因素下的细微感知差异。

Conclusion: 研究不仅深化了对隐私概念的理解，还为教育和就业领域的隐私改进提供了实用指导。

Abstract: We propose a harm-centric conceptualization of privacy that asks: What harms
from personal data use can privacy prevent? The motivation behind this research
is limitations in existing privacy frameworks (e.g., Contextual Integrity) to
capture or categorize many of the harms that arise from modern technology's use
of personal data. We operationalize this conceptualization in an online study
with 400 college and university students. Study participants indicated their
perceptions of different harms (e.g., manipulation, discrimination, and
harassment) that may arise when artificial intelligence-based algorithms infer
personal data (e.g., demographics, personality traits, and cognitive
disability) and use it to identify students who are likely to drop out of a
course or the best job candidate. The study includes 14 harms and six types of
personal data selected based on an extensive literature review.
  Comprehensive statistical analyses of the study data show that the 14 harms
are internally consistent and collectively represent a general notion of
privacy harms. The study data also surfaces nuanced perceptions of harms, both
across the contexts and participants' demographic factors. Based on these
results, we discuss how privacy can be improved equitably. Thus, this research
not only contributes to enhancing the understanding of privacy as a concept but
also provides practical guidance to improve privacy in the context of education
and employment.

</details>


### [36] [Efficient Cybersecurity Assessment Using SVM and Fuzzy Evidential Reasoning for Resilient Infrastructure](https://arxiv.org/abs/2506.22938)
*Zaydon L. Ali,Wassan Saad Abduljabbar Hayale,Israa Ibraheem Al_Barazanchi,Ravi Sekhar,Pritesh Shah,Sushma Parihar*

Main category: cs.CR

TL;DR: 提出了一种基于支持向量机（SVM）和模糊证据推理（ER）的安全评估模型，用于快速准确地选择加密算法并评估风险。


<details>
  <summary>Details</summary>
Motivation: 当前加密模型存在安全漏洞，传统方法评估耗时且不精确，亟需一种高效的安全评估解决方案。

Method: 结合SVM和模糊ER方法，构建数据集并分析安全组件（如对比度、同质性），以系统化评估风险。

Result: 通过召回率、F1分数和准确率等指标验证了模型的性能。

Conclusion: 该模型能高效处理风险评估数据，为加密算法的选择提供了一种快速且精确的方法。

Abstract: With current advancement in hybermedia knowledges, the privacy of digital
information has developed a critical problem. To overawed the susceptibilities
of present security protocols, scholars tend to focus mainly on efforts on
alternation of current protocols. Over past decade, various proposed encoding
models have been shown insecurity, leading to main threats against significant
data. Utilizing the suitable encryption model is very vital means of guard
against various such, but algorithm is selected based on the dependency of data
which need to be secured. Moreover, testing potentiality of the security
assessment one by one to identify the best choice can take a vital time for
processing. For faster and precisive identification of assessment algorithm, we
suggest a security phase exposure model for cipher encryption technique by
invoking Support Vector Machine (SVM). In this work, we form a dataset using
usual security components like contrast, homogeneity. To overcome the
uncertainty in analysing the security and lack of ability of processing data to
a risk assessment mechanism. To overcome with such complications, this paper
proposes an assessment model for security issues using fuzzy evidential
reasoning (ER) approaches. Significantly, the model can be utilised to process
and assemble risk assessment data on various aspects in systematic ways. To
estimate the performance of our framework, we have various analyses like,
recall, F1 score and accuracy.

</details>


### [37] [A Study on Semi-Supervised Detection of DDoS Attacks under Class Imbalance](https://arxiv.org/abs/2506.22949)
*Ehsan Hallaji,Vaishnavi Shanmugam,Roozbeh Razavi-Far,Mehrdad Saif*

Main category: cs.CR

TL;DR: 研究探讨了半监督学习（SSL）在解决DDoS攻击检测中数据不平衡和部分标记问题中的应用，评估了13种SSL算法的效果。


<details>
  <summary>Details</summary>
Motivation: 网络安全中DDoS攻击检测面临数据不平衡和标记不足的挑战，需要自动化且高效的解决方案。

Method: 评估13种先进的SSL算法在不同场景下的DDoS攻击检测能力，分析其实际效果和局限性。

Result: 研究结果为设计抗数据不平衡且能处理部分标记数据的智能入侵检测系统（IDS）提供了参考。

Conclusion: SSL技术能有效提升DDoS攻击检测性能，尤其在数据不平衡和标记不足的情况下。

Abstract: One of the most difficult challenges in cybersecurity is eliminating
Distributed Denial of Service (DDoS) attacks. Automating this task using
artificial intelligence is a complex process due to the inherent class
imbalance and lack of sufficient labeled samples of real-world datasets. This
research investigates the use of Semi-Supervised Learning (SSL) techniques to
improve DDoS attack detection when data is imbalanced and partially labeled. In
this process, 13 state-of-the-art SSL algorithms are evaluated for detecting
DDoS attacks in several scenarios. We evaluate their practical efficacy and
shortcomings, including the extent to which they work in extreme environments.
The results will offer insight into designing intelligent Intrusion Detection
Systems (IDSs) that are robust against class imbalance and handle partially
labeled data.

</details>


### [38] [Equivalence Classes in AES -- Part 1](https://arxiv.org/abs/2506.23050)
*David Cornwell*

Main category: cs.CR

TL;DR: 研究AES中由MixColumns和InvMixColumns操作自然产生的等价类性质，探讨SubBytes、ShiftRows、MixColumns和AddRoundKey操作对等价类的影响，并计划进一步研究基于已知明文-密文等价类对的密钥恢复攻击。


<details>
  <summary>Details</summary>
Motivation: 探索AES中MixColumns和InvMixColumns操作导致的等价类性质，以理解其对加密过程的影响。

Method: 分析SubBytes、ShiftRows、MixColumns和AddRoundKey操作对等价类的影响。

Result: 揭示了这些操作对等价类的具体影响。

Conclusion: 计划下一步研究利用已知明文-密文等价类对进行密钥恢复攻击。

Abstract: We investigate properties of equivalence classes in AES which arise naturally
from properties of MixColumns and InvMixColumns. These two operations have the
property that the XOR of the 4 input bytes equals the XOR of 4 output bytes. We
examine the effect on equivalence classes due to the operation of SubBytes,
ShiftRows, MixColumns and AddRoundKey. The next phase of research is to find a
key recovery attack using known (plaintext, ciphertext) equivalence class
pairs.
  Keywords: AES, Equivalence, Class, MixColumns, ShiftRows, SubBytes,
AddRoundKey, Schedule, State, XOR

</details>


### [39] [A Practical and Secure Byzantine Robust Aggregator](https://arxiv.org/abs/2506.23183)
*De Zhang Lee,Aashish Kolluri,Prateek Saxena,Ee-Chien Chang*

Main category: cs.CR

TL;DR: 提出了一种高效的鲁棒聚合算法，用于在高维向量中去除异常值并计算平均值，适用于机器学习安全中的防御策略。


<details>
  <summary>Details</summary>
Motivation: 解决机器学习安全中数据投毒攻击导致的梯度异常问题，提供一种无需先验知识的鲁棒聚合方法。

Method: 设计了一种准线性时间复杂度的鲁棒聚合算法，无需预先计算过滤阈值或假设干净向量的分布。

Result: 算法在理论上有接近最优的偏差界限，并在实验中验证了其运行效率和防御10种投毒攻击的有效性。

Conclusion: 该算法实用性强，可直接用于标准神经网络训练，为机器学习安全提供了一种高效防御手段。

Abstract: In machine learning security, one is often faced with the problem of removing
outliers from a given set of high-dimensional vectors when computing their
average. For example, many variants of data poisoning attacks produce gradient
vectors during training that are outliers in the distribution of clean
gradients, which bias the computed average used to derive the ML model.
Filtering them out before averaging serves as a generic defense strategy.
Byzantine robust aggregation is an algorithmic primitive which computes a
robust average of vectors, in the presence of an $\epsilon$ fraction of vectors
which may have been arbitrarily and adaptively corrupted, such that the
resulting bias in the final average is provably bounded.
  In this paper, we give the first robust aggregator that runs in quasi-linear
time in the size of input vectors and provably has near-optimal bias bounds.
Our algorithm also does not assume any knowledge of the distribution of clean
vectors, nor does it require pre-computing any filtering thresholds from it.
This makes it practical to use directly in standard neural network training
procedures. We empirically confirm its expected runtime efficiency and its
effectiveness in nullifying 10 different ML poisoning attacks.

</details>


### [40] [From Prompt Injections to Protocol Exploits: Threats in LLM-Powered AI Agents Workflows](https://arxiv.org/abs/2506.23260)
*Mohamed Amine Ferrag,Norbert Tihanyi,Djallel Hamouda,Leandros Maglaras,Merouane Debbah*

Main category: cs.CR

TL;DR: 该论文提出了一个统一的威胁模型，用于分析LLM代理生态系统的安全问题，涵盖了输入操纵、模型妥协、系统和隐私攻击以及协议漏洞四大领域，并评估了现有防御措施。


<details>
  <summary>Details</summary>
Motivation: 随着LLM代理生态系统的快速发展，插件和协议的激增导致了安全实践的滞后，使得系统容易受到多种威胁。

Method: 论文通过分类和形式化威胁模型，分析了四大领域中的攻击技术，并评估了其现实可行性和防御措施。

Result: 论文总结了三十多种攻击技术，并提出了未来研究方向，如动态信任管理和加密溯源等。

Conclusion: 该研究为设计稳健的防御机制和建立LLM代理工作流程的最佳实践提供了全面参考。

Abstract: Autonomous AI agents powered by large language models (LLMs) with structured
function-calling interfaces have dramatically expanded capabilities for
real-time data retrieval, complex computation, and multi-step orchestration.
Yet, the explosive proliferation of plugins, connectors, and inter-agent
protocols has outpaced discovery mechanisms and security practices, resulting
in brittle integrations vulnerable to diverse threats. In this survey, we
introduce the first unified, end-to-end threat model for LLM-agent ecosystems,
spanning host-to-tool and agent-to-agent communications, formalize adversary
capabilities and attacker objectives, and catalog over thirty attack
techniques. Specifically, we organized the threat model into four domains:
Input Manipulation (e.g., prompt injections, long-context hijacks, multimodal
adversarial inputs), Model Compromise (e.g., prompt- and parameter-level
backdoors, composite and encrypted multi-backdoors, poisoning strategies),
System and Privacy Attacks (e.g., speculative side-channels, membership
inference, retrieval poisoning, social-engineering simulations), and Protocol
Vulnerabilities (e.g., exploits in Model Context Protocol (MCP), Agent
Communication Protocol (ACP), Agent Network Protocol (ANP), and Agent-to-Agent
(A2A) protocol). For each category, we review representative scenarios, assess
real-world feasibility, and evaluate existing defenses. Building on our threat
taxonomy, we identify key open challenges and future research directions, such
as securing MCP deployments through dynamic trust management and cryptographic
provenance tracking; designing and hardening Agentic Web Interfaces; and
achieving resilience in multi-agent and federated environments. Our work
provides a comprehensive reference to guide the design of robust defense
mechanisms and establish best practices for resilient LLM-agent workflows.

</details>


### [41] [Threshold Signatures for Central Bank Digital Currencies](https://arxiv.org/abs/2506.23294)
*Mostafa Abdelrahman,Filip Rezabek,Lars Hupel,Kilian Glas,Georg Carle*

Main category: cs.CR

TL;DR: 论文探讨了在央行数字货币（CBDC）中使用阈值签名方案（TSS）以增强安全性，同时保持可接受的性能。


<details>
  <summary>Details</summary>
Motivation: CBDC依赖数字签名确保交易真实性，但私钥泄露风险大，TSS能分散密钥管理，降低风险。

Method: 分析了CBDC需求，评估了基于ECDSA的TSS及其库，以Filia CBDC为基础进行性能测试。

Result: TSS能提升CBDC安全性，且性能满足实际部署需求。

Conclusion: TSS是CBDC安全实现的可行方案。

Abstract: Digital signatures are crucial for securing Central Bank Digital Currencies
(CBDCs) transactions. Like most forms of digital currencies, CBDC solutions
rely on signatures for transaction authenticity and integrity, leading to major
issues in the case of private key compromise. Our work explores threshold
signature schemes (TSSs) in the context of CBDCs. TSSs allow distributed key
management and signing, reducing the risk of a compromised key. We analyze
CBDC-specific requirements, considering the applicability of TSSs, and use
Filia CBDC solution as a base for a detailed evaluation. As most of the current
solutions rely on ECDSA for compatibility, we focus on ECDSA-based TSSs and
their supporting libraries. Our performance evaluation measured the
computational and communication complexity across key processes, as well as the
throughput and latency of end-to-end transactions. The results confirm that TSS
can enhance the security of CBDC implementations while maintaining acceptable
performance for real-world deployments.

</details>


### [42] [Securing AI Systems: A Guide to Known Attacks and Impacts](https://arxiv.org/abs/2506.23296)
*Naoto Kiribuchi,Kengo Zenitani,Takayuki Semitsu*

Main category: cs.CR

TL;DR: 本文概述了针对预测性和生成性AI系统的独特对抗攻击，识别了11种主要攻击类型，并将其技术与影响（如信息泄露、系统破坏和资源耗尽）关联到CIA安全三要素。


<details>
  <summary>Details</summary>
Motivation: AI系统面临特定的安全威胁，本文旨在为非专业AI安全的研究人员、开发者、安全从业者和政策制定者提供基础知识，以识别风险并实施有效防御。

Method: 通过分类和映射攻击类型及其影响，提供对抗攻击的全面概述。

Result: 识别了11种主要攻击类型，并将其技术与CIA安全三要素（机密性、完整性、可用性）关联。

Conclusion: 本文为提升AI系统的整体安全态势提供了基础知识，帮助相关方识别风险并实施防御。

Abstract: Embedded into information systems, artificial intelligence (AI) faces
security threats that exploit AI-specific vulnerabilities. This paper provides
an accessible overview of adversarial attacks unique to predictive and
generative AI systems. We identify eleven major attack types and explicitly
link attack techniques to their impacts -- including information leakage,
system compromise, and resource exhaustion -- mapped to the confidentiality,
integrity, and availability (CIA) security triad. We aim to equip researchers,
developers, security practitioners, and policymakers, even those without
specialized AI security expertise, with foundational knowledge to recognize
AI-specific risks and implement effective defenses, thereby enhancing the
overall security posture of AI systems.

</details>


### [43] [Interpretable by Design: MH-AutoML for Transparent and Efficient Android Malware Detection without Compromising Performance](https://arxiv.org/abs/2506.23314)
*Joner Assolin,Gabriel Canto,Diego Kreutz,Eduardo Feitosa,Hendrio Bragança,Angelo Nogueira,Vanderson Rocha*

Main category: cs.CR

TL;DR: MH-AutoML是一个针对Android恶意软件检测的领域专用AutoML框架，通过自动化ML流程并提供透明性和可解释性，优于现有AutoML解决方案。


<details>
  <summary>Details</summary>
Motivation: 当前AutoML解决方案在透明性、可解释性和实验可追溯性方面存在不足，限制了其在网络安全领域的应用。

Method: MH-AutoML自动化整个ML流程，包括数据预处理、特征工程、算法选择和超参数调优，并支持可解释性、调试和实验跟踪。

Result: 与七种现有AutoML框架相比，MH-AutoML在召回率上表现更优，同时提供更高的透明性和控制能力，计算效率相当。

Conclusion: MH-AutoML在性能和可解释性方面均表现出色，适合网络安全应用。

Abstract: Malware detection in Android systems requires both cybersecurity expertise
and machine learning (ML) techniques. Automated Machine Learning (AutoML) has
emerged as an approach to simplify ML development by reducing the need for
specialized knowledge. However, current AutoML solutions typically operate as
black-box systems with limited transparency, interpretability, and experiment
traceability. To address these limitations, we present MH-AutoML, a
domain-specific framework for Android malware detection. MH-AutoML automates
the entire ML pipeline, including data preprocessing, feature engineering,
algorithm selection, and hyperparameter tuning. The framework incorporates
capabilities for interpretability, debugging, and experiment tracking that are
often missing in general-purpose solutions. In this study, we compare MH-AutoML
against seven established AutoML frameworks: Auto-Sklearn, AutoGluon, TPOT,
HyperGBM, Auto-PyTorch, LightAutoML, and MLJAR. Results show that MH-AutoML
achieves better recall rates while providing more transparency and control. The
framework maintains computational efficiency comparable to other solutions,
making it suitable for cybersecurity applications where both performance and
explainability matter.

</details>


### [44] [All Proof of Work But No Proof of Play](https://arxiv.org/abs/2506.23435)
*Hayder Tirmazi*

Main category: cs.CR

TL;DR: 论文探讨了如何通过密码学方法验证游戏速通的真实性，但最终展示了其困难性。


<details>
  <summary>Details</summary>
Motivation: 速通验证的传统方法（如人工观察或音频分析）不可靠且非密码学，作者试图构建密码学验证系统。

Method: 尝试构建密码学验证系统，并通过失败案例展示其局限性。

Result: 展示了验证直播和交互式输入的困难性，以及签名方案和游戏完整性的限制。

Conclusion: 验证速通的密码学方法在当前技术下极具挑战性。

Abstract: Speedrunning is a competition that emerged from communities of early video
games such as Doom (1993). Speedrunners try to finish a game in minimal time.
Provably verifying the authenticity of submitted speedruns is an open problem.
Traditionally, best-effort speedrun verification is conducted by on-site human
observers, forensic audio analysis, or a rigorous mathematical analysis of the
game mechanics. Such methods are tedious, fallible, and, perhaps worst of all,
not cryptographic. Motivated by naivety and the Dunning-Kruger effect, we
attempt to build a system that cryptographically proves the authenticity of
speedruns. This paper describes our attempted solutions and ways to circumvent
them. Through a narration of our failures, we attempt to demonstrate the
difficulty of authenticating live and interactive human input in untrusted
environments, as well as the limits of signature schemes, game integrity, and
provable play.

</details>


### [45] [A Large-Scale Evolvable Dataset for Model Context Protocol Ecosystem and Security Analysis](https://arxiv.org/abs/2506.23474)
*Zhiwei Lin,Bonan Ruan,Jiahao Liu,Weibo Zhao*

Main category: cs.CR

TL;DR: MCPCorpus是一个大规模数据集，包含约14K个MCP服务器和300个MCP客户端，每个标注了20多个标准化属性，用于研究MCP生态系统的趋势和多样性。


<details>
  <summary>Details</summary>
Motivation: 由于MCP生态系统的快速扩展，缺乏对现有MCP工件的结构化全面视图，阻碍了研究进展。

Method: 引入MCPCorpus数据集，包含大量标注的MCP服务器和客户端，并提供自动化工具和搜索界面。

Result: MCPCorpus为MCP生态系统提供了可重复的快照，支持对采用趋势、生态系统健康和实现多样性的研究。

Conclusion: MCPCorpus填补了MCP生态系统研究的空白，并提供了实用工具和公开访问的数据集。

Abstract: The Model Context Protocol (MCP) has recently emerged as a standardized
interface for connecting language models with external tools and data. As the
ecosystem rapidly expands, the lack of a structured, comprehensive view of
existing MCP artifacts presents challenges for research. To bridge this gap, we
introduce MCPCorpus, a large-scale dataset containing around 14K MCP servers
and 300 MCP clients. Each artifact is annotated with 20+ normalized attributes
capturing its identity, interface configuration, GitHub activity, and metadata.
MCPCorpus provides a reproducible snapshot of the real-world MCP ecosystem,
enabling studies of adoption trends, ecosystem health, and implementation
diversity. To keep pace with the rapid evolution of the MCP ecosystem, we
provide utility tools for automated data synchronization, normalization, and
inspection. Furthermore, to support efficient exploration and exploitation, we
release a lightweight web-based search interface. MCPCorpus is publicly
available at: https://github.com/Snakinya/MCPCorpus.

</details>


### [46] [Detect \& Score: Privacy-Preserving Misbehaviour Detection and Contribution Evaluation in Federated Learning](https://arxiv.org/abs/2506.23583)
*Marvin Xhemrishi,Alexandre Graell i Amat,Balázs Pejó*

Main category: cs.CR

TL;DR: 结合QI和FedGT的优势，实现恶意行为检测和贡献评估的双重目标。


<details>
  <summary>Details</summary>
Motivation: 安全聚合在联邦学习中保护了客户隐私，但也增加了恶意行为检测和贡献评估的难度。现有方法（QI和FedGT）各有不足，需要结合两者优势。

Method: 结合QI和FedGT的方法，以同时实现恶意行为检测和贡献评估。

Result: 实验表明，结合方法在性能上优于单独使用QI或FedGT。

Conclusion: 通过结合QI和FedGT，能够同时实现高效的恶意行为检测和准确的贡献评估。

Abstract: Federated learning with secure aggregation enables private and collaborative
learning from decentralised data without leaking sensitive client information.
However, secure aggregation also complicates the detection of malicious client
behaviour and the evaluation of individual client contributions to the
learning. To address these challenges, QI (Pejo et al.) and FedGT (Xhemrishi et
al.) were proposed for contribution evaluation (CE) and misbehaviour detection
(MD), respectively. QI, however, lacks adequate MD accuracy due to its reliance
on the random selection of clients in each training round, while FedGT lacks
the CE ability. In this work, we combine the strengths of QI and FedGT to
achieve both robust MD and accurate CE. Our experiments demonstrate superior
performance compared to using either method independently.

</details>


### [47] [Cybersecurity AI: The Dangerous Gap Between Automation and Autonomy](https://arxiv.org/abs/2506.23592)
*Víctor Mayoral-Vilches*

Main category: cs.CR

TL;DR: 论文探讨了网络安全AI中‘自动化’与‘自主性’的混淆问题，提出了6级分类法，指出当前工具仅达到3-4级，需人类监督，并警告误用‘自主性’标签可能带来风险。


<details>
  <summary>Details</summary>
Motivation: 网络安全领域对‘自动化’和‘自主性’AI的混淆可能导致对系统能力的误解，进而引发潜在风险。

Method: 借鉴机器人学原则，建立6级分类法（0-5级），区分网络安全AI的自动化与自主性。

Result: 当前‘自主’渗透测试工具仅达到3-4级，仍需人类监督；真正的5级自主性尚未实现。

Conclusion: 需使用精确术语、透明披露能力，并强调人机合作而非替代，以避免风险。

Abstract: The cybersecurity industry combines "automated" and "autonomous" AI, creating
dangerous misconceptions about system capabilities. Recent milestones like XBOW
topping HackerOne's leaderboard showcase impressive progress, yet these systems
remain fundamentally semi-autonomous--requiring human oversight. Drawing from
robotics principles, where the distinction between automation and autonomy is
well-established, I take inspiration from prior work and establish a 6-level
taxonomy (Level 0-5) distinguishing automation from autonomy in Cybersecurity
AI. Current "autonomous" pentesters operate at Level 3-4: they execute complex
attack sequences but need human review for edge cases and strategic decisions.
True Level 5 autonomy remains aspirational. Organizations deploying
mischaracterized "autonomous" tools risk reducing oversight precisely when it's
most needed, potentially creating new vulnerabilities. The path forward
requires precise terminology, transparent capabilities disclosure, and human-AI
partnership-not replacement.

</details>


### [48] [SoK: Semantic Privacy in Large Language Models](https://arxiv.org/abs/2506.23603)
*Baihe Ma,Yanna Jiang,Xu Wang,Guangshen Yu,Qin Wang,Caijun Sun,Chen Li,Xuelei Qi,Ying He,Wei Ni,Ren Ping Liu*

Main category: cs.CR

TL;DR: 该论文提出了一个以生命周期为中心的框架，分析大语言模型（LLMs）在输入处理、预训练、微调和对齐阶段中语义隐私风险的产生，并评估现有防御措施的不足。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型在敏感领域的广泛应用，传统隐私保护措施无法应对隐式、上下文或可推断的语义隐私风险，亟需系统性研究。

Method: 通过分类关键攻击向量，评估差分隐私、嵌入加密、边缘计算和遗忘等现有防御措施的效果。

Result: 分析揭示了语义层面保护的关键漏洞，尤其是针对上下文推断和潜在表示泄漏的防御不足。

Conclusion: 论文总结了开放挑战，包括量化语义泄漏、保护多模态输入、平衡去标识与生成质量，以及确保隐私执行的透明度，为未来研究提供方向。

Abstract: As Large Language Models (LLMs) are increasingly deployed in sensitive
domains, traditional data privacy measures prove inadequate for protecting
information that is implicit, contextual, or inferable - what we define as
semantic privacy. This Systematization of Knowledge (SoK) introduces a
lifecycle-centric framework to analyze how semantic privacy risks emerge across
input processing, pretraining, fine-tuning, and alignment stages of LLMs. We
categorize key attack vectors and assess how current defenses, such as
differential privacy, embedding encryption, edge computing, and unlearning,
address these threats. Our analysis reveals critical gaps in semantic-level
protection, especially against contextual inference and latent representation
leakage. We conclude by outlining open challenges, including quantifying
semantic leakage, protecting multimodal inputs, balancing de-identification
with generation quality, and ensuring transparency in privacy enforcement. This
work aims to inform future research on designing robust, semantically aware
privacy-preserving techniques for LLMs.

</details>


### [49] [Privacy-Preserving Federated Learning Scheme with Mitigating Model Poisoning Attacks: Vulnerabilities and Countermeasures](https://arxiv.org/abs/2506.23622)
*Jiahui Wu,Fucai Luo,Tiecheng Sun,Haiyan Wang,Weizhe Zhang*

Main category: cs.CR

TL;DR: 论文提出了一种增强的隐私保护和拜占庭鲁棒的联邦学习方案（PBFL），解决了现有方案在模型投毒攻击下的隐私泄漏问题。


<details>
  <summary>Details</summary>
Motivation: 现有基于两个诚实但好奇且不共谋服务器的联邦学习方案在安全性和效率上表现良好，但在模型投毒攻击下仍存在隐私泄漏问题。

Method: 方案包括三个部分：双陷门全同态加密（FHE）增强隐私保护、新型安全归一化判断方法防止梯度投毒、创新的安全余弦相似度测量方法检测模型投毒攻击。

Result: 理论分析和实验验证表明，方案在非独立同分布（non-IID）数据下仍能保证隐私保护和抗模型投毒攻击，同时提高了训练速度并降低了通信开销。

Conclusion: 提出的PBFL方案有效解决了隐私泄漏和模型投毒攻击问题，兼具安全性和效率。

Abstract: The privacy-preserving federated learning schemes based on the setting of two
honest-but-curious and non-colluding servers offer promising solutions in terms
of security and efficiency. However, our investigation reveals that these
schemes still suffer from privacy leakage when considering model poisoning
attacks from malicious users. Specifically, we demonstrate that the
privacy-preserving computation process for defending against model poisoning
attacks inadvertently leaks privacy to one of the honest-but-curious servers,
enabling it to access users' gradients in plaintext. To address both privacy
leakage and model poisoning attacks, we propose an enhanced privacy-preserving
and Byzantine-robust federated learning (PBFL) scheme, comprising three
components: (1) a two-trapdoor fully homomorphic encryption (FHE) scheme to
bolster users' privacy protection; (2) a novel secure normalization judgment
method to preemptively thwart gradient poisoning; and (3) an innovative secure
cosine similarity measurement method for detecting model poisoning attacks
without compromising data privacy. Our scheme guarantees privacy preservation
and resilience against model poisoning attacks, even in scenarios with
heterogeneous, non-IID (Independently and Identically Distributed) datasets.
Theoretical analyses substantiate the security and efficiency of our scheme,
and extensive experiments corroborate the efficacy of our private attacks.
Furthermore, the experimental results demonstrate that our scheme accelerates
training speed while reducing communication overhead compared to the
state-of-the-art PBFL schemes.

</details>


### [50] [gMBA: Expression Semantic Guided Mixed Boolean-Arithmetic Deobfuscation Using Transformer Architectures](https://arxiv.org/abs/2506.23634)
*Youjeong Noh,Joon-Young Paik,Jingun Kwon,Eun-Sun Cho*

Main category: cs.CR

TL;DR: 论文提出了一种基于真值表和Transformer架构的MBA反混淆框架（gMBA），通过语义指导显著提升性能。


<details>
  <summary>Details</summary>
Motivation: MBA混淆被广泛用于恶意软件逃避检测，传统方法忽视内部语义信息，导致效果不佳。

Method: 提出真值表作为语义表示，并设计基于Transformer的gMBA框架，结合语义指导进行反混淆。

Result: 实验表明，语义信息的整合显著提升了反混淆性能。

Conclusion: 内部语义信息对恢复混淆代码至关重要，gMBA框架为MBA反混淆提供了有效解决方案。

Abstract: Mixed Boolean-Arithmetic (MBA) obfuscation protects intellectual property by
converting programs into forms that are more complex to analyze. However, MBA
has been increasingly exploited by malware developers to evade detection and
cause significant real-world problems. Traditional MBA deobfuscation methods
often consider these expressions as part of a black box and overlook their
internal semantic information. To bridge this gap, we propose a truth table,
which is an automatically constructed semantic representation of an
expression's behavior that does not rely on external resources. The truth table
is a mathematical form that represents the output of expression for all
possible combinations of input. We also propose a general and extensible guided
MBA deobfuscation framework (gMBA) that modifies a Transformer-based neural
encoder-decoder Seq2Seq architecture to incorporate this semantic guidance.
Experimental results and in-depth analysis show that integrating expression
semantics significantly improves performance and highlights the importance of
internal semantic expressions in recovering obfuscated code to its original
form.

</details>


### [51] [Threadbox: Sandboxing for Modular Security](https://arxiv.org/abs/2506.23683)
*Maysara Alhindi,Joseph Hallett*

Main category: cs.CR

TL;DR: 论文提出Threadbox，一种新型沙箱机制，支持模块化和独立沙箱，适用于线程和特定函数，解决了现有沙箱机制难以应用于某些应用的问题。


<details>
  <summary>Details</summary>
Motivation: 现有沙箱机制需要开发者重构代码以适应沙箱模型，限制了其应用范围。

Method: 提出Threadbox机制，支持模块化和独立沙箱，可应用于线程和特定函数。

Result: 通过案例研究验证了Threadbox的适用性，并讨论了其局限性。

Conclusion: Threadbox为解决现有沙箱机制的局限性提供了一种灵活且实用的解决方案。

Abstract: There are many sandboxing mechanisms provided by operating systems to limit
what resources applications can access, however, sometimes the use of these
mechanisms requires developers to refactor their code to fit the sandboxing
model. In this work, we investigate what makes existing sandboxing mechanisms
challenging to apply to certain types of applications, and propose Threadbox, a
sandboxing mechanism that enables having modular and independent sandboxes, and
can be applied to threads and sandbox specific functions. We present case
studies to illustrate the applicability of the idea and discuss its
limitations.

</details>


### [52] [Not quite a piece of CHERI-cake: Are new digital security by design architectures usable?](https://arxiv.org/abs/2506.23682)
*Maysara Alhindi,Joseph Hallett*

Main category: cs.CR

TL;DR: CHERI架构通过硬件修改C语言基础类型（如指针）来防止内存安全问题，但开发者对其警告、错误显示和文档不足感到困扰。


<details>
  <summary>Details</summary>
Motivation: 研究开发者对CHERI架构的适应情况，以改进其可用性。

Method: 进行可用性研究，观察开发者在移植软件到CHERI时的反应。

Result: 开发者对CHERI的警告、错误显示和文档多样性感到困难。

Conclusion: CHERI架构需改进警告、错误提示和文档以提升开发者体验。

Abstract: A digital security-by-design computer architecture, like CHERI, lets you
program without fear of buffer overflows or other memory safety errors, but
CHERI also rewrites some of the assumptions about how C works and how
fundamental types (such as pointers) are implemented in hardware. We conducted
a usability study to examine how developers react to the changes required by
CHERI when porting software to run on it. We find that developers struggle with
CHERI's display of warnings and errors and a lack of diverse documentation.

</details>


### [53] [An ontological lens on attack trees: Toward adequacy and interoperability](https://arxiv.org/abs/2506.23841)
*Ítalo Oliveira,Stefano M. Nicoletti,Gal Engelberg,Mattia Fumagalli,Dan Klein,Giancarlo Guizzardi*

Main category: cs.CR

TL;DR: 论文分析了攻击树（AT）在安全分析中的局限性，提出基于COVER本体论的改进方向。


<details>
  <summary>Details</summary>
Motivation: 攻击树在安全分析中广泛应用，但缺乏本体论基础，导致建模和分析能力受限。

Method: 通过COVER本体论对攻击树进行本体论分析，揭示其四大不足。

Result: 发现攻击树存在术语模糊、概念缺失、建模指导不足和语义互操作性差等问题。

Conclusion: 论文为改进攻击树提供了本体论基础，并提出了更广泛的风险管理建模方法。

Abstract: Attack Trees (AT) are a popular formalism for security analysis. They are
meant to display an attacker's goal decomposed into attack steps needed to
achieve it and compute certain security metrics (e.g., attack cost,
probability, and damage). ATs offer three important services: (a) conceptual
modeling capabilities for representing security risk management scenarios, (b)
a qualitative assessment to find root causes and minimal conditions of
successful attacks, and (c) quantitative analyses via security metrics
computation under formal semantics, such as minimal time and cost among all
attacks. Still, the AT language presents limitations due to its lack of
ontological foundations, thus compromising associated services. Via an
ontological analysis grounded in the Common Ontology of Value and Risk (COVER)
-- a reference core ontology based on the Unified Foundational Ontology (UFO)
-- we investigate the ontological adequacy of AT and reveal four significant
shortcomings: (1) ambiguous syntactical terms that can be interpreted in
various ways; (2) ontological deficit concerning crucial domain-specific
concepts; (3) lacking modeling guidance to construct ATs decomposing a goal;
(4) lack of semantic interoperability, resulting in ad hoc stand-alone tools.
We also discuss existing incremental solutions and how our analysis paves the
way for overcoming those issues through a broader approach to risk management
modeling.

</details>


### [54] [Exploring Privacy and Security as Drivers for Environmental Sustainability in Cloud-Based Office Solutions](https://arxiv.org/abs/2506.23866)
*Jason Kayembe,Iness Ben Guirat,Jan Tobias Mühlberg*

Main category: cs.CR

TL;DR: 本文研究了基于云的办公解决方案中隐私、安全与环境可持续性的关系，提出了一种量化用户和网络端能源使用及碳排放的框架，并验证了隐私优先服务更节能的假设。


<details>
  <summary>Details</summary>
Motivation: 探讨隐私优先服务是否比依赖数据收集和广告的服务更节能，并量化其环境影响。

Method: 提出一个系统化测量能源使用和网络数据流量的框架，应用于三种主流电子邮件服务（Outlook、Gmail、Proton Mail）和自托管解决方案。

Result: 自托管解决方案最节能，比Gmail节省33%的排放；商业服务中Proton Mail最节能，比Outlook节省0.1 gCO2 e/会话。

Conclusion: 隐私优先设计在环境可持续性方面具有优势，自托管和隐私优先服务显著减少碳排放。

Abstract: In this paper, we explore the intersection of privacy, security, and
environmental sustainability in cloud-based office solutions, focusing on
quantifying user- and network-side energy use and associated carbon emissions.
We hypothesise that privacy-focused services are typically more
energy-efficient than those funded through data collection and advertising. To
evaluate this, we propose a framework that systematically measures
environmental costs based on energy usage and network data traffic during
well-defined, automated usage scenarios. To test our hypothesis, we first
analyse how underlying architectures and business models, such as monetisation
through personalised advertising, contribute to the environmental footprint of
these services. We then explore existing methodologies and tools for software
environmental impact assessment. We apply our framework to three mainstream
email services selected to reflect different privacy policies, from
ad-supported tracking-intensive models to privacy-focused designs: Microsoft
Outlook, Google Mail (Gmail), and Proton Mail. We extend this comparison to a
self-hosted email solution, evaluated with and without end-to-end encryption.
We show that the self-hosted solution, even with 14% of device energy and 15%
of emissions overheads from PGP encryption, remains the most energy-efficient,
saving up to 33% of emissions per session compared to Gmail. Among commercial
providers, Proton Mail is the most efficient, saving up to 0.1 gCO2 e per
session compared to Outlook, whose emissions can be further reduced by 2%
through ad-blocking.

</details>


### [55] [Breaking Out from the TESSERACT: Reassessing ML-based Malware Detection under Spatio-Temporal Drift](https://arxiv.org/abs/2506.23814)
*Theo Chow,Mario D'Onghia,Lorenz Linhardt,Zeliang Kan,Daniel Arp,Lorenzo Cavallaro,Fabio Pierazzi*

Main category: cs.CR

TL;DR: 论文揭示了在相同时间框架内，基于学习的恶意软件检测性能在两种代表性Android恶意软件数据集上的显著差异，并提出了五种新的时空偏差因素以改进评估方法。


<details>
  <summary>Details</summary>
Motivation: 探讨当前最先进的恶意软件检测方法在现实场景中的表现，并揭示现有评估方法中的潜在偏差。

Method: 在两种代表性Android恶意软件数据集和五种分类器上，评估五种新的时空偏差因素对检测性能的影响。

Result: 发现了性能差异，并验证了五种偏差因素对评估结果的影响。

Conclusion: 提出了实际可行的建议，以帮助社区改进评估方法，使其更贴近现实且可复现。

Abstract: Several recent works focused on the best practices for applying machine
learning to cybersecurity. In the context of malware, TESSERACT highlighted the
impact of concept drift on detection performance and suggested temporal and
spatial constraints to be enforced to ensure realistic time-aware evaluations,
which have been adopted by the community. In this paper, we demonstrate
striking discrepancies in the performance of learning-based malware detection
across the same time frame when evaluated on two representative Android malware
datasets used in top-tier security conferences, both adhering to established
sampling and evaluation guidelines. This questions our ability to understand
how current state-of-the-art approaches would perform in realistic scenarios.
To address this, we identify five novel temporal and spatial bias factors that
affect realistic evaluations. We thoroughly evaluate the impact of these
factors in the Android malware domain on two representative datasets and five
Android malware classifiers used or proposed in top-tier security conferences.
For each factor, we provide practical and actionable recommendations that the
community should integrate in their methodology for more realistic and
reproducible settings.

</details>


### [56] [Differentially Private Synthetic Data Release for Topics API Outputs](https://arxiv.org/abs/2506.23855)
*Travis Dick,Alessandro Epasto,Adel Javanmard,Josh Karlin,Andres Munoz Medina,Vahab Mirrokni,Sergei Vassilvitskii,Peilin Zhong*

Main category: cs.CR

TL;DR: 提出了一种生成合成API输出的方法，以解决隐私保护广告API研究中缺乏公开数据的问题，并公开了匿名数据集。


<details>
  <summary>Details</summary>
Motivation: 由于隐私问题，公开真实的API输出数据不可行，阻碍了对隐私保护广告API的实证研究。

Method: 开发了一种基于差分隐私的方法，生成与真实Topics API数据重识别风险特性相似的合成数据。

Result: 生成了一个匿名化的合成数据集，并公开了源代码，以支持外部研究。

Conclusion: 该方法有助于提高隐私保护广告API的透明度，促进相关研究。

Abstract: The analysis of the privacy properties of Privacy-Preserving Ads APIs is an
area of research that has received strong interest from academics, industry,
and regulators. Despite this interest, the empirical study of these methods is
hindered by the lack of publicly available data. Reliable empirical analysis of
the privacy properties of an API, in fact, requires access to a dataset
consisting of realistic API outputs; however, privacy concerns prevent the
general release of such data to the public.
  In this work, we develop a novel methodology to construct synthetic API
outputs that are simultaneously realistic enough to enable accurate study and
provide strong privacy protections. We focus on one Privacy-Preserving Ads
APIs: the Topics API, part of Google Chrome's Privacy Sandbox. We developed a
methodology to generate a differentially-private dataset that closely matches
the re-identification risk properties of the real Topics API data. The use of
differential privacy provides strong theoretical bounds on the leakage of
private user information from this release.
  Our methodology is based on first computing a large number of
differentially-private statistics describing how output API traces evolve over
time. Then, we design a parameterized distribution over sequences of API traces
and optimize its parameters so that they closely match the statistics obtained.
Finally, we create the synthetic data by drawing from this distribution.
  Our work is complemented by an open-source release of the anonymized dataset
obtained by this methodology. We hope this will enable external researchers to
analyze the API in-depth and replicate prior and future work on a realistic
large-scale dataset. We believe that this work will contribute to fostering
transparency regarding the privacy properties of Privacy-Preserving Ads APIs.

</details>


### [57] [RawMal-TF: Raw Malware Dataset Labeled by Type and Family](https://arxiv.org/abs/2506.23909)
*David Bálik,Martin Jureček,Mark Stamp*

Main category: cs.CR

TL;DR: 该论文提出了一种基于机器学习的恶意软件分类方法，通过构建包含类型和家族标签的新数据集，并利用静态分析提取特征，实现了高精度的分类任务。


<details>
  <summary>Details</summary>
Motivation: 解决恶意软件分类中的标签细粒度不足问题，同时支持类型和家族级别的分类任务。

Method: 从多个来源收集恶意软件样本，构建统一特征提取管道（基于PE头静态分析），并评估多种机器学习模型（如Random Forest、XGBoost、SVM）。

Result: 在二进制分类任务中，模型准确率高达98.98%；在有限数据条件下仍表现优异（97.6%）。多分类任务中，SVM在类型标签上达到81.1%准确率。

Conclusion: 通过双级别标签和静态分析，实现了更细粒度的恶意软件分类，为未来研究奠定了基础。

Abstract: This work addresses the challenge of malware classification using machine
learning by developing a novel dataset labeled at both the malware type and
family levels. Raw binaries were collected from sources such as VirusShare, VX
Underground, and MalwareBazaar, and subsequently labeled with family
information parsed from binary names and type-level labels integrated from
ClarAVy. The dataset includes 14 malware types and 17 malware families, and was
processed using a unified feature extraction pipeline based on static analysis,
particularly extracting features from Portable Executable headers, to support
advanced classification tasks. The evaluation was focused on three key
classification tasks. In the binary classification of malware versus benign
samples, Random Forest and XGBoost achieved high accuracy on the full datasets,
reaching 98.5% for type-based detection and 98.98% for family-based detection.
When using truncated datasets of 1,000 samples to assess performance under
limited data conditions, both models still performed strongly, achieving 97.6%
for type-based detection and 98.66% for family-based detection. For interclass
classification, which distinguishes between malware types or families, the
models reached up to 97.5% accuracy on type-level tasks and up to 93.7% on
family-level tasks. In the multiclass classification setting, which assigns
samples to the correct type or family, SVM achieved 81.1% accuracy on type
labels, while Random Forest and XGBoost reached approximately 73.4% on family
labels. The results highlight practical trade-offs between accuracy and
computational cost, and demonstrate that labeling at both the type and family
levels enables more fine-grained and insightful malware classification. The
work establishes a robust foundation for future research on advanced malware
detection and classification.

</details>


### [58] [Lock Prediction for Zero-Downtime Database Encryption](https://arxiv.org/abs/2506.23985)
*Mohamed Sami Rakha,Adam Sorrenti,Greg Stager,Walid Rjaibi,Andriy Miranskyy*

Main category: cs.CR

TL;DR: 论文提出了一种基于深度学习的预测方法，用于在线数据库加密，减少停机时间和存储开销。


<details>
  <summary>Details</summary>
Motivation: 现代企业数据库系统在平衡数据安全与性能方面面临挑战，现有加密方法需要完整备份和恢复周期，导致停机时间长和存储开销大。

Method: 利用深度学习模型（如Transformer和LSTM）预测数据库锁序列，基于IBM Db2和TPC-C基准测试数据集进行训练和评估。

Result: 提出的模型在表级和页级锁预测中分别达到49%和66%的平均准确率，优于基线方法。

Conclusion: 该方法为实现在线加密提供了可行路径，有望构建安全且低开销的数据库系统。

Abstract: Modern enterprise database systems face significant challenges in balancing
data security and performance. Ensuring robust encryption for sensitive
information is critical for systems' compliance with security standards.
Although holistic database encryption provides strong protection, existing
database systems often require a complete backup and restore cycle, resulting
in prolonged downtime and increased storage usage. This makes it difficult to
implement online encryption techniques in high-throughput environments without
disrupting critical operations.
  To address this challenge, we envision a solution that enables online
database encryption aligned with system activity, eliminating the need for
downtime, storage overhead, or full-database reprocessing. Central to this
vision is the ability to predict which parts of the database will be accessed
next, allowing encryption to be applied online. As a step towards this
solution, this study proposes a predictive approach that leverages deep
learning models to forecast database lock sequences, using IBM Db2 as the
database system under study. In this study, we collected a specialized dataset
from TPC-C benchmark workloads, leveraging lock event logs for model training
and evaluation. We applied deep learning architectures, such as Transformer and
LSTM, to evaluate models for various table-level and page-level lock
predictions. We benchmark the accuracy of the trained models versus a Naive
Baseline across different prediction horizons and timelines.
  The study experiments demonstrate that the proposed deep learning-based
models achieve up to 49% average accuracy for table-level and 66% for
page-level predictions, outperforming a Naive Baseline. By anticipating which
tables and pages will be locked next, the proposed approach is a step toward
online encryption, offering a practical path toward secure, low-overhead
database systems.

</details>


### [59] [Poisoning Attacks to Local Differential Privacy for Ranking Estimation](https://arxiv.org/abs/2506.24033)
*Pei Zhan,Peng Tang,Yangzhuo Li,Puwen Wei,Shanqing Guo*

Main category: cs.CR

TL;DR: 论文提出了针对本地差分隐私（LDP）的新型投毒攻击方法，通过精确修改频率来影响排名估计，并提出了针对kRR、OUE和OLH协议的防御策略。


<details>
  <summary>Details</summary>
Motivation: 本地差分隐私（LDP）虽然保护了用户数据的隐私，但也容易受到投毒攻击的影响，尤其是排名估计领域。论文旨在揭示这些攻击的复杂性并提出防御方案。

Method: 论文提出了针对kRR、OUE和OLH协议的攻击策略，包括选择最优攻击项、分配虚假用户、考虑频率变化以及基于置信水平的攻击策略。

Result: 通过理论和实验验证，论文证明了攻击的有效性，并强调了防御这些攻击的必要性。

Conclusion: 论文揭示了LDP在排名估计中的投毒攻击漏洞，并提出了相应的攻击和防御策略，为未来的研究提供了方向。

Abstract: Local differential privacy (LDP) involves users perturbing their inputs to
provide plausible deniability of their data. However, this also makes LDP
vulnerable to poisoning attacks. In this paper, we first introduce novel
poisoning attacks for ranking estimation. These attacks are intricate, as fake
attackers do not merely adjust the frequency of target items. Instead, they
leverage a limited number of fake users to precisely modify frequencies,
effectively altering item rankings to maximize gains. To tackle this challenge,
we introduce the concepts of attack cost and optimal attack item (set), and
propose corresponding strategies for kRR, OUE, and OLH protocols. For kRR, we
iteratively select optimal attack items and allocate suitable fake users. For
OUE, we iteratively determine optimal attack item sets and consider the
incremental changes in item frequencies across different sets. Regarding OLH,
we develop a harmonic cost function based on the pre-image of a hash to select
that supporting a larger number of effective attack items. Lastly, we present
an attack strategy based on confidence levels to quantify the probability of a
successful attack and the number of attack iterations more precisely. We
demonstrate the effectiveness of our attacks through theoretical and empirical
evidence, highlighting the necessity for defenses against these attacks. The
source code and data have been made available at
https://github.com/LDP-user/LDP-Ranking.git.

</details>


### [60] [Logit-Gap Steering: Efficient Short-Suffix Jailbreaks for Aligned Large Language Models](https://arxiv.org/abs/2506.24056)
*Tung-Ling Li,Hongliang Liu*

Main category: cs.CR

TL;DR: 提出了一种名为logit-gap steering的快速越狱框架，通过单次词汇表遍历解决RLHF对齐语言模型的拒绝-确认差距问题。


<details>
  <summary>Details</summary>
Motivation: 旨在高效解决RLHF对齐语言模型的拒绝-确认差距问题，并探索安全调整对内部表示的影响。

Method: 通过前向可计算的分数结合差距减少、KL惩罚和奖励偏移的轻量级代理，实现快速生成短后缀。

Result: 该方法在0.5B到70B的模型上均有效，将单次攻击成功率提升至80-100%，同时保持主题连贯性。

Conclusion: logit-gap steering不仅高效，还揭示了奖励边界等对齐问题，为安全调整提供了轻量级探测工具。

Abstract: We introduce logit-gap steering, a fast jailbreak framework that casts the
refusal-affirmation gap of RLHF-aligned language models as a single pass over
the vocabulary. A forward-computable score blends gap reduction with
lightweight proxies for KL penalty and reward shift, allowing a "sort-sum-stop"
sweep to complete in under a second and return a short suffix--two orders of
magnitude fewer model calls than beam or gradient attacks. The same suffix
generalises to unseen prompts and scales from 0.5 B to 70 B checkpoints,
lifting one-shot attack success from baseline levels to 80-100% while
preserving topical coherence. Beyond efficiency, these suffixes expose
sentence-boundary reward cliffs and other alignment artefacts, offering a
lightweight probe into how safety tuning reshapes internal representations.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [61] [Bootstrapping Human-Like Planning via LLMs](https://arxiv.org/abs/2506.22604)
*David Porfirio,Vincent Hsiao,Morgan Fine-Morris,Leslie Smith,Laura M. Hiatt*

Main category: cs.AI

TL;DR: 研究结合自然语言编程和拖放界面两种方法，利用大型语言模型（LLM）生成人类类似的动作序列，并与手动指定的动作序列进行比较。


<details>
  <summary>Details</summary>
Motivation: 探索如何结合自然语言和拖放界面的优势，为机器人任务提供更直观且精确的编程方式。

Method: 构建基于LLM的流程，输入自然语言并输出类似人类动作序列，再与手动指定的动作序列进行比较。

Result: 较大模型在生成人类类似动作序列上表现更优，但较小模型也能达到满意效果。

Conclusion: 结合自然语言和拖放界面的方法可行，且模型规模对性能有影响。

Abstract: Robot end users increasingly require accessible means of specifying tasks for
robots to perform. Two common end-user programming paradigms include
drag-and-drop interfaces and natural language programming. Although natural
language interfaces harness an intuitive form of human communication,
drag-and-drop interfaces enable users to meticulously and precisely dictate the
key actions of the robot's task. In this paper, we investigate the degree to
which both approaches can be combined. Specifically, we construct a large
language model (LLM)-based pipeline that accepts natural language as input and
produces human-like action sequences as output, specified at a level of
granularity that a human would produce. We then compare these generated action
sequences to another dataset of hand-specified action sequences. Although our
results reveal that larger models tend to outperform smaller ones in the
production of human-like action sequences, smaller models nonetheless achieve
satisfactory performance.

</details>


### [62] [Ludax: A GPU-Accelerated Domain Specific Language for Board Games](https://arxiv.org/abs/2506.22609)
*Graham Todd,Alexander G. Padula,Dennis J. N. J. Soemers,Julian Togelius*

Main category: cs.AI

TL;DR: Ludax是一个结合游戏描述语言和硬件加速的框架，旨在加速游戏研究，支持快速模拟和灵活表示。


<details>
  <summary>Details</summary>
Motivation: 游戏在人工智能研究中作为基准和测试环境，但现有工具缺乏硬件加速支持。

Method: 开发Ludax，一种领域特定语言，自动编译为硬件加速代码，支持并行处理。

Result: Ludax提供快速模拟和灵活表示，适用于强化学习等研究。

Conclusion: Ludax开源框架有望加速游戏研究，支持广泛的应用场景。

Abstract: Games have long been used as benchmarks and testing environments for research
in artificial intelligence. A key step in supporting this research was the
development of game description languages: frameworks that compile
domain-specific code into playable and simulatable game environments, allowing
researchers to generalize their algorithms and approaches across multiple games
without having to manually implement each one. More recently, progress in
reinforcement learning (RL) has been largely driven by advances in hardware
acceleration. Libraries like JAX allow practitioners to take full advantage of
cutting-edge computing hardware, often speeding up training and testing by
orders of magnitude. Here, we present a synthesis of these strands of research:
a domain-specific language for board games which automatically compiles into
hardware-accelerated code. Our framework, Ludax, combines the generality of
game description languages with the speed of modern parallel processing
hardware and is designed to fit neatly into existing deep learning pipelines.
We envision Ludax as a tool to help accelerate games research generally, from
RL to cognitive science, by enabling rapid simulation and providing a flexible
representation scheme. We present a detailed breakdown of Ludax's description
language and technical notes on the compilation process, along with speed
benchmarking and a demonstration of training RL agents. The Ludax framework,
along with implementations of existing board games, is open-source and freely
available.

</details>


### [63] [URSA: The Universal Research and Scientific Agent](https://arxiv.org/abs/2506.22653)
*Michael Grosskopf,Russell Bent,Rahul Somasundaram,Isaac Michaud,Arthur Lui,Nathan Debardeleben,Earl Lawrence*

Main category: cs.AI

TL;DR: URSA是一个科学代理生态系统，旨在通过模块化代理和工具加速研究任务，展示了其在解决复杂科学问题中的潜力。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）已具备复杂推理、规划、写作、编码和研究能力，与科学家日常解决问题的能力高度重合，利用LLMs的“代理”AI有望推动科学进步。

Method: 提出URSA系统，包含模块化代理和工具（如高级物理模拟代码），可组合用于解决不同复杂度和影响力的科学问题。

Result: 展示了URSA的架构及其在解决科学问题中的潜力。

Conclusion: URSA系统通过模块化设计和工具集成，为加速科学研究提供了新途径。

Abstract: Large language models (LLMs) have moved far beyond their initial form as
simple chatbots, now carrying out complex reasoning, planning, writing, coding,
and research tasks. These skills overlap significantly with those that human
scientists use day-to-day to solve complex problems that drive the cutting edge
of research. Using LLMs in "agentic" AI has the potential to revolutionize
modern science and remove bottlenecks to progress. In this work, we present
URSA, a scientific agent ecosystem for accelerating research tasks. URSA
consists of a set of modular agents and tools, including coupling to advanced
physics simulation codes, that can be combined to address scientific problems
of varied complexity and impact. This work highlights the architecture of URSA,
as well as examples that highlight the potential of the system.

</details>


### [64] [Explanations are a means to an end](https://arxiv.org/abs/2506.22740)
*Jessica Hullman,Ziyang Guo,Berk Ustun*

Main category: cs.AI

TL;DR: 论文提出了一种基于统计决策理论的框架，强调解释性机器学习方法应根据具体用途设计和评估，避免模糊性。


<details>
  <summary>Details</summary>
Motivation: 当前解释性机器学习方法缺乏对实际用途的深入考虑，可能导致解释的误用或无效。

Method: 采用统计决策理论框架，明确解释的具体用途，并通过理论和实证结合的方式评估解释的价值。

Result: 展示了该框架在临床决策支持、提供补救措施和调试等多样化用例中的应用，并量化了理想决策者可能获得的性能提升。

Conclusion: 解释性方法应针对具体用途设计，并通过明确用例和结合理论实证评估来提升其实际价值。

Abstract: Modern methods for explainable machine learning are designed to describe how
models map inputs to outputs--without deep consideration of how these
explanations will be used in practice. This paper argues that explanations
should be designed and evaluated with a specific end in mind. We describe how
to formalize this end in a framework based in statistical decision theory. We
show how this functionally-grounded approach can be applied across diverse use
cases, such as clinical decision support, providing recourse, or debugging. We
demonstrate its use to characterize the maximum "boost" in performance on a
particular task that an explanation could provide an idealized decision-maker,
preventing misuse due to ambiguity by forcing researchers to specify concrete
use cases that can be analyzed in light of models of expected explanation use.
We argue that evaluation should meld theoretical and empirical perspectives on
the value of explanation, and contribute definitions that span these
perspectives.

</details>


### [65] [Bridging Ethical Principles and Algorithmic Methods: An Alternative Approach for Assessing Trustworthiness in AI Systems](https://arxiv.org/abs/2506.22774)
*Michael Papademas,Xenia Ziouvelou,Antonis Troumpoukis,Vangelis Karkaletsis*

Main category: cs.AI

TL;DR: 论文提出了一种结合伦理与算法的评估方法，旨在量化AI系统的可信度，弥补现有指南与技术工具的不足。


<details>
  <summary>Details</summary>
Motivation: AI的广泛影响及其复杂性导致对其可信度的评估需求增加，但现有方法要么缺乏量化能力，要么缺乏全面视角。

Method: 结合Trustworthy AI的伦理组件与PageRank、TrustRank算法，提出一种评估框架。

Result: 该方法能提供定量分析，同时兼顾伦理指南的理论内容，实现可信度的全面评估。

Conclusion: 通过算法与伦理的结合，可以更客观地评估AI系统的可信度，减少主观性。

Abstract: Artificial Intelligence (AI) technology epitomizes the complex challenges
posed by human-made artifacts, particularly those widely integrated into
society and exert significant influence, highlighting potential benefits and
their negative consequences. While other technologies may also pose substantial
risks, AI's pervasive reach makes its societal effects especially profound. The
complexity of AI systems, coupled with their remarkable capabilities, can lead
to a reliance on technologies that operate beyond direct human oversight or
understanding. To mitigate the risks that arise, several theoretical tools and
guidelines have been developed, alongside efforts to create technological tools
aimed at safeguarding Trustworthy AI. The guidelines take a more holistic view
of the issue but fail to provide techniques for quantifying trustworthiness.
Conversely, while technological tools are better at achieving such
quantification, they lack a holistic perspective, focusing instead on specific
aspects of Trustworthy AI. This paper aims to introduce an assessment method
that combines the ethical components of Trustworthy AI with the algorithmic
processes of PageRank and TrustRank. The goal is to establish an assessment
framework that minimizes the subjectivity inherent in the self-assessment
techniques prevalent in the field by introducing algorithmic criteria. The
application of our approach indicates that a holistic assessment of an AI
system's trustworthiness can be achieved by providing quantitative insights
while considering the theoretical content of relevant guidelines.

</details>


### [66] [ReasonBridge: Efficient Reasoning Transfer from Closed to Open-Source Language Models](https://arxiv.org/abs/2506.22865)
*Ziqi Zhong,Xunzhu Tang*

Main category: cs.AI

TL;DR: ReasonBridge通过分层知识蒸馏框架，将闭源模型的推理能力高效迁移到开源模型，显著缩小性能差距。


<details>
  <summary>Details</summary>
Motivation: 解决闭源与开源模型在复杂推理任务中的性能差距。

Method: 采用分层蒸馏、稀疏适配器架构和测试时计算扩展机制。

Result: 开源模型推理能力提升23%，部分任务性能超越闭源模型。

Conclusion: ReasonBridge为高效增强推理能力提供了样本高效的方法。

Abstract: Recent advancements in Large Language Models (LLMs) have revealed a
significant performance gap between closed-source and open-source models,
particularly in tasks requiring complex reasoning and precise instruction
following. This paper introduces ReasonBridge, a methodology that efficiently
transfers reasoning capabilities from powerful closed-source to open-source
models through a novel hierarchical knowledge distillation framework. We
develop a tailored dataset Reason1K with only 1,000 carefully curated reasoning
traces emphasizing difficulty, diversity, and quality. These traces are
filtered from across multiple domains using a structured multi-criteria
selection algorithm. Our transfer learning approach incorporates: (1) a
hierarchical distillation process capturing both strategic abstraction and
tactical implementation patterns, (2) a sparse reasoning-focused adapter
architecture requiring only 0.3% additional trainable parameters, and (3) a
test-time compute scaling mechanism using guided inference interventions.
Comprehensive evaluations demonstrate that ReasonBridge improves reasoning
capabilities in open-source models by up to 23% on benchmark tasks,
significantly narrowing the gap with closed-source models. Notably, the
enhanced Qwen2.5-14B outperforms Claude-Sonnet3.5 on MATH500 and matches its
performance on competition-level AIME problems. Our methodology generalizes
effectively across diverse reasoning domains and model architectures,
establishing a sample-efficient approach to reasoning enhancement for
instruction following.

</details>


### [67] [Agentic Enterprise: AI-Centric User to User-Centric AI](https://arxiv.org/abs/2506.22893)
*Arpit Narechania,Alex Endert,Atanu R Sinha*

Main category: cs.AI

TL;DR: 本文探讨了AI在企业决策中的潜力，提出了六个原则以促进AI在企业中的成功应用，并强调从以AI为中心转向以用户为中心的设计。


<details>
  <summary>Details</summary>
Motivation: AI在多个领域具有潜在影响力，但当前以AI为中心的用户范式未能充分满足企业决策的持续需求。

Method: 通过分析企业决策需求，提出六个原则，并提倡以用户为中心的AI设计和市场机制。

Result: 提出了六个促进AI在企业中成功应用的原则，并强调了用户中心化的重要性。

Conclusion: 企业应转向以用户为中心的AI设计，以提升决策效率，并通过市场机制实现AI平台的优化。

Abstract: After a very long winter, the Artificial Intelligence (AI) spring is here.
Or, so it seems over the last three years. AI has the potential to impact many
areas of human life - personal, social, health, education, professional. In
this paper, we take a closer look at the potential of AI for Enterprises, where
decision-making plays a crucial and repeated role across functions, tasks, and
operations. We consider Agents imbued with AI as means to increase
decision-productivity of enterprises. We highlight six tenets for Agentic
success in enterprises, by drawing attention to what the current, AI-Centric
User paradigm misses, in the face of persistent needs of and usefulness for
Enterprise Decision-Making. In underscoring a shift to User-Centric AI, we
offer six tenets and promote market mechanisms for platforms, aligning the
design of AI and its delivery by Agents to the cause of enterprise users.

</details>


### [68] [Hecto: Modular Sparse Experts for Adaptive and Interpretable Reasoning](https://arxiv.org/abs/2506.22919)
*Sanskar Pandey,Ruhaan Chopra,Saad Murtaza Bhat,Ark Abhyudaya*

Main category: cs.AI

TL;DR: Hecto是一个轻量级的混合专家（MoE）架构，通过结合GRU和FFNN专家实现异构计算，提升推理任务的性能和可解释性。


<details>
  <summary>Details</summary>
Motivation: 传统MoE模型的专家依赖相同的归纳偏差，限制了表示多样性和计算效率，Hecto旨在通过异构架构解决这一问题。

Method: Hecto结合GRU专家（时序推理）和FFNN专家（静态抽象），采用稀疏Top-1门控机制。

Result: 在多个推理和回归任务中，Hecto性能接近或优于同构基线，并实现专家专业化（时序vs静态）。

Conclusion: Hecto为条件计算提供了新基准，其异构架构在低资源场景下表现出色，源于专业化的设计。

Abstract: Mixture-of-Experts (MoE) models enable conditional computation by routing
inputs to specialized experts, but these experts rely on identical inductive
biases, thus limiting representational diversity. This static computation
pathway is inefficient for inputs that require different types of reasoning and
limits specialization and interpretability. We propose Hecto, a lightweight MoE
architecture that leverages architectural heterogeneity by combining a GRU
expert for temporal reasoning and an FFNN expert for static abstraction under a
sparse Top-1 gating mechanism. Evaluated on three reasoning benchmarks (AG
News, SST-2, HotpotQA) and a regression task (STS-B), Hecto matches or closely
trails homogeneous baselines in performance despite receiving isolated input
representations, while achieving clear expert specialization, with each expert
aligning to distinct reasoning types (temporal vs static). At larger batch
sizes, Hecto exhibits improved performance, benefiting from relaxed
computational constraints that allow its heterogeneous architecture to optimize
more effectively. Ablation results isolate architectural diversity as the
source of Hecto's stability and interpretability across diverse reasoning
tasks. Overall, Hecto establishes itself as a new benchmark for conditional
computation, offering a principled framework for specialized reasoning in
low-resource regimes with its model strength derived from principled
specialization.

</details>


### [69] [Improving Rationality in the Reasoning Process of Language Models through Self-playing Game](https://arxiv.org/abs/2506.22920)
*Pinzheng Wang,Juntao Li,Zecheng Tang,Haijia Gui,Min zhang*

Main category: cs.AI

TL;DR: 通过自玩游戏（Critic-Discernment Game, CDG）提升大语言模型在推理过程中的理性，无需人类监督。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型在数学和编码等任务中表现出推理能力，但缺乏对其推理过程的真正理解。

Method: 设计CDG游戏，其中证明者提供解决方案并接受批评者的挑战，批评者可能提供帮助或误导。

Result: 实验表明，CDG训练显著提升了模型在数学推理、错误检测、自我修正和长链推理中的能力。

Conclusion: 自玩游戏是一种有效提升模型推理理解能力的方法。

Abstract: Large language models (LLMs) have demonstrated considerable reasoning
abilities in various tasks such as mathematics and coding. However, recent
studies indicate that even the best models lack true comprehension of their
reasoning processes. In this paper, we explore how self-play can enhance the
rationality of models in the reasoning process without supervision from humans
or superior models. We design a Critic-Discernment Game(CDG) in which a prover
first provides a solution to a given problem and is subsequently challenged by
critiques of its solution. These critiques either aim to assist or mislead the
prover. The objective of the prover is to maintain the correct answer when
faced with misleading comments, while correcting errors in response to
constructive feedback. Our experiments on tasks involving mathematical
reasoning, stepwise error detection, self-correction, and long-chain reasoning
demonstrate that CDG training can significantly improve the ability of
well-aligned LLMs to comprehend their reasoning process.

</details>


### [70] [MARBLE: A Hard Benchmark for Multimodal Spatial Reasoning and Planning](https://arxiv.org/abs/2506.22992)
*Yulun Jiang,Yekun Chai,Maria Brbić,Michael Moor*

Main category: cs.AI

TL;DR: MARBLE是一个多模态推理基准测试，旨在评估多模态语言模型（MLLMs）在复杂多模态环境中的逐步推理能力。现有模型表现不佳，表明复杂推理仍是挑战。


<details>
  <summary>Details</summary>
Motivation: 现有推理基准测试多集中于文本或简单多模态问题，复杂多模态推理能力尚未被充分研究。

Method: MARBLE包含两个高难度任务（M-Portal和M-Cube），要求模型在空间、视觉和物理约束下制定和理解多步计划。

Result: 12个先进模型在M-Portal上表现接近随机，M-Cube上准确率为0%，仅在简化子任务中部分模型优于随机基线。

Conclusion: MARBLE揭示了MLLMs的局限性，尤其是感知和多模态推理能力的不足，希望推动下一代模型的开发。

Abstract: The ability to process information from multiple modalities and to reason
through it step-by-step remains a critical challenge in advancing artificial
intelligence. However, existing reasoning benchmarks focus on text-only
reasoning, or employ multimodal questions that can be answered by directly
retrieving information from a non-text modality. Thus, complex reasoning
remains poorly understood in multimodal domains. Here, we present MARBLE, a
challenging multimodal reasoning benchmark that is designed to scrutinize
multimodal language models (MLLMs) in their ability to carefully reason
step-by-step through complex multimodal problems and environments. MARBLE is
composed of two highly challenging tasks, M-Portal and M-Cube, that require the
crafting and understanding of multistep plans under spatial, visual, and
physical constraints. We find that current MLLMs perform poorly on MARBLE --
all the 12 advanced models obtain near-random performance on M-Portal and 0%
accuracy on M-Cube. Only in simplified subtasks some models outperform the
random baseline, indicating that complex reasoning is still a challenge for
existing MLLMs. Moreover, we show that perception remains a bottleneck, where
MLLMs occasionally fail to extract information from the visual inputs. By
shedding a light on the limitations of MLLMs, we hope that MARBLE will spur the
development of the next generation of models with the ability to reason and
plan across many, multimodal reasoning steps.

</details>


### [71] [AURA: Agent for Understanding, Reasoning, and Automated Tool Use in Voice-Driven Tasks](https://arxiv.org/abs/2506.23049)
*Leander Melroy Maben,Gayathri Ganesh Lakshmy,Srijith Radhakrishnan,Siddhant Arora,Shinji Watanabe*

Main category: cs.AI

TL;DR: AURA是首个开源、支持语音的原生助手，能够通过动态工具调用和多轮对话完成复杂任务。


<details>
  <summary>Details</summary>
Motivation: 尽管语言和语音技术有所进步，但尚无开源系统支持完整的语音到语音、多轮对话，并集成工具使用和代理推理。

Method: AURA结合了开源的ASR、TTS和LLM，采用级联流水线设计，支持日历预订、联系人查找、网络搜索和电子邮件等工具。

Result: 在VoiceBench上，AURA在OpenBookQA中得分92.75%，接近GPT-4o；在AlpacaEval中得分为4.39，与其他开源系统竞争。人类评估显示，复杂多轮语音任务的成功率为90%。

Conclusion: AURA是首个开源、语音原生的助手，能够高效完成复杂任务，具有广泛的应用潜力。

Abstract: Despite advances in language and speech technologies, no open-source system
enables full speech-to-speech, multi-turn dialogue with integrated tool use and
agentic reasoning. We introduce AURA (Agent for Understanding, Reasoning, and
Automated Tool Use), the first open-source, speech-native assistant capable of
completing complex, goal-driven tasks through dynamic tool invocation and
multi-turn conversation. AURA combines open-weight ASR, TTS, and LLMs in a
cascaded pipeline and supports tools such as calendar booking, contact lookup,
web search, and email. Its modular design allows easy integration of new tools
using natural language prompts and action classes. On VoiceBench, AURA scores
92.75% on OpenBookQA-outperforming all open-weight systems and nearing
GPT-4o-and 4.39 on AlpacaEval, competitive with other open-weight systems.
Human evaluation shows 90% task success on complex, multi-turn speech tasks.

</details>


### [72] [AI's Euclid's Elements Moment: From Language Models to Computable Thought](https://arxiv.org/abs/2506.23080)
*Xinmin Fang,Lingfeng Tao,Zhengxiong Li*

Main category: cs.AI

TL;DR: 本文提出了一个五阶段进化框架，将AI的发展与人类认知技术的历史进程类比，揭示了AI从专家系统到Transformers的演变，并预测了未来的发展阶段。


<details>
  <summary>Details</summary>
Motivation: 探索AI发展的系统性模式，为未来研究和实践提供理论基础和具体策略。

Method: 提出“认知几何”框架，通过类比人类认知技术的演变，分析AI的五个发展阶段及其反馈机制。

Result: 揭示了AI发展的非线性、自反性特征，并预测了未来的“元语言时刻”、“数学符号时刻”和“形式逻辑系统时刻”。

Conclusion: 该框架为AI的未来研究提供了方法论基础，并为开发下一代智能系统提供了具体策略。

Abstract: This paper presents a comprehensive five-stage evolutionary framework for
understanding the development of artificial intelligence, arguing that its
trajectory mirrors the historical progression of human cognitive technologies.
We posit that AI is advancing through distinct epochs, each defined by a
revolutionary shift in its capacity for representation and reasoning, analogous
to the inventions of cuneiform, the alphabet, grammar and logic, mathematical
calculus, and formal logical systems. This "Geometry of Cognition" framework
moves beyond mere metaphor to provide a systematic, cross-disciplinary model
that not only explains AI's past architectural shifts-from expert systems to
Transformers-but also charts a concrete and prescriptive path forward.
Crucially, we demonstrate that this evolution is not merely linear but
reflexive: as AI advances through these stages, the tools and insights it
develops create a feedback loop that fundamentally reshapes its own underlying
architecture. We are currently transitioning into a "Metalinguistic Moment,"
characterized by the emergence of self-reflective capabilities like
Chain-of-Thought prompting and Constitutional AI. The subsequent stages, the
"Mathematical Symbolism Moment" and the "Formal Logic System Moment," will be
defined by the development of a computable calculus of thought, likely through
neuro-symbolic architectures and program synthesis, culminating in provably
aligned and reliable AI that reconstructs its own foundational representations.
This work serves as the methodological capstone to our trilogy, which
previously explored the economic drivers ("why") and cognitive nature ("what")
of AI. Here, we address the "how," providing a theoretical foundation for
future research and offering concrete, actionable strategies for startups and
developers aiming to build the next generation of intelligent systems.

</details>


### [73] [Can Large Language Models Capture Human Risk Preferences? A Cross-Cultural Study](https://arxiv.org/abs/2506.23107)
*Bing Song,Jianing Liu,Sisi Jian,Chenyang Wu,Vinayak Dixit*

Main category: cs.AI

TL;DR: 研究探讨了大型语言模型（LLMs）在模拟风险决策行为中的表现，发现模型比人类更规避风险，且中文提示下表现偏差更大。


<details>
  <summary>Details</summary>
Motivation: 随着LLMs应用扩展，其在复杂决策行为（如风险决策）中的可靠性引发关注，需验证其模拟能力。

Method: 通过彩票任务比较LLMs（ChatGPT 4o和o1-mini）与人类决策，使用CRRA框架分析风险偏好，结合多语言数据。

Result: 模型比人类更规避风险，o1-mini更接近人类决策；中文提示下预测偏差更大。

Conclusion: LLMs在模拟人类风险行为方面有潜力，但存在语言和文化差异的局限性。

Abstract: Large language models (LLMs) have made significant strides, extending their
applications to dialogue systems, automated content creation, and
domain-specific advisory tasks. However, as their use grows, concerns have
emerged regarding their reliability in simulating complex decision-making
behavior, such as risky decision-making, where a single choice can lead to
multiple outcomes. This study investigates the ability of LLMs to simulate
risky decision-making scenarios. We compare model-generated decisions with
actual human responses in a series of lottery-based tasks, using transportation
stated preference survey data from participants in Sydney, Dhaka, Hong Kong,
and Nanjing. Demographic inputs were provided to two LLMs -- ChatGPT 4o and
ChatGPT o1-mini -- which were tasked with predicting individual choices. Risk
preferences were analyzed using the Constant Relative Risk Aversion (CRRA)
framework. Results show that both models exhibit more risk-averse behavior than
human participants, with o1-mini aligning more closely with observed human
decisions. Further analysis of multilingual data from Nanjing and Hong Kong
indicates that model predictions in Chinese deviate more from actual responses
compared to English, suggesting that prompt language may influence simulation
performance. These findings highlight both the promise and the current
limitations of LLMs in replicating human-like risk behavior, particularly in
linguistic and cultural settings.

</details>


### [74] [The Societal Impact of Foundation Models: Advancing Evidence-based AI Policy](https://arxiv.org/abs/2506.23123)
*Rishi Bommasani*

Main category: cs.AI

TL;DR: 该论文探讨了基础模型在AI时代对社会的影响，提出了概念框架、实证见解和政策行动，旨在改善AI治理。


<details>
  <summary>Details</summary>
Motivation: 基础模型具有巨大潜力，但也带来困惑和潜在危害，需要更好地理解其社会影响以推动有效治理。

Method: 围绕三个主题展开：概念框架（能力、风险、供应链）、实证见解（模型评估和组织透明度）、政策行动（基于证据的AI政策）。

Result: 通过科学基础和政策研究接口，为AI时代实现更好的社会成果提供了路径。

Conclusion: 论文为AI治理提供了理论和实践基础，推动了社会对基础模型的理解和政策制定。

Abstract: Artificial intelligence is humanity's most promising technology because of
the remarkable capabilities offered by foundation models. Yet, the same
technology brings confusion and consternation: foundation models are poorly
understood and they may precipitate a wide array of harms. This dissertation
explains how technology and society coevolve in the age of AI, organized around
three themes. First, the conceptual framing: the capabilities, risks, and the
supply chain that grounds foundation models in the broader economy. Second, the
empirical insights that enrich the conceptual foundations: transparency created
via evaluations at the model level and indexes at the organization level.
Finally, the transition from understanding to action: superior understanding of
the societal impact of foundation models advances evidence-based AI policy.
View together, this dissertation makes inroads into achieving better societal
outcomes in the age of AI by building the scientific foundations and
research-policy interface required for better AI governance.

</details>


### [75] [Are Large Language Models Capable of Deep Relational Reasoning? Insights from DeepSeek-R1 and Benchmark Comparisons](https://arxiv.org/abs/2506.23128)
*Chi Chiu So,Yueyue Sun,Jun-Min Wang,Siu Pang Yung,Anthony Wai Keung Loh,Chun Pong Chau*

Main category: cs.AI

TL;DR: 论文评估了三种大型语言模型（DeepSeek-R1、DeepSeek-V3和GPT-4o）在深度关系推理任务中的表现，发现DeepSeek-R1表现最佳，但所有模型在复杂任务中均存在局限性。


<details>
  <summary>Details</summary>
Motivation: 研究大型语言模型在深度关系推理任务中的能力，探索其逻辑推理和关系推断的潜力与不足。

Method: 通过设计家族树和通用图推理的基准任务，评估三种模型的性能，并分析其推理过程。

Result: DeepSeek-R1在多项任务中表现最佳，但随着问题复杂性增加，所有模型均表现不佳，主要受限于输出结构和令牌长度。

Conclusion: 研究揭示了大型语言模型在复杂推理任务中的局限性，并提出了未来研究方向，如多模态推理和系统性分析推理失败原因。

Abstract: How far are Large Language Models (LLMs) in performing deep relational
reasoning? In this paper, we evaluate and compare the reasoning capabilities of
three cutting-edge LLMs, namely, DeepSeek-R1, DeepSeek-V3 and GPT-4o, through a
suite of carefully designed benchmark tasks in family tree and general graph
reasoning. Our experiments reveal that DeepSeek-R1 consistently achieves the
highest F1-scores across multiple tasks and problem sizes, demonstrating strong
aptitude in logical deduction and relational inference. However, all evaluated
models, including DeepSeek-R1, struggle significantly as problem complexity
increases, largely due to token length limitations and incomplete output
structures. A detailed analysis of DeepSeek-R1's long Chain-of-Thought
responses uncovers its unique planning and verification strategies, but also
highlights instances of incoherent or incomplete reasoning, calling attention
to the need for deeper scrutiny into LLMs' internal inference dynamics. We
further discuss key directions for future work, including the role of
multimodal reasoning and the systematic examination of reasoning failures. Our
findings provide both empirical insights and theoretical implications for
advancing LLMs' reasoning abilities, particularly in tasks that demand
structured, multi-step logical inference. Our code repository will be publicly
available at https://github.com/kelvinhkcs/Deep-Relational-Reasoning.

</details>


### [76] [Context-Driven Knowledge Graph Completion with Semantic-Aware Relational Message Passing](https://arxiv.org/abs/2506.23141)
*Siyuan Li,Ruitong Liu,Yan Wen,Te Sun*

Main category: cs.AI

TL;DR: 提出了一种语义感知的关系消息传递框架，通过Top-K邻居选择策略和多头注意力聚合器，有效减少噪声并提升知识图谱补全性能。


<details>
  <summary>Details</summary>
Motivation: 传统基于节点的消息传递机制在知识图谱中容易引入噪声和信息稀释，因此需要一种更精准的语义感知方法。

Method: 引入语义感知的Top-K邻居选择策略，结合多头注意力聚合器，选择并融合最相关的语义信息。

Result: 在多个基准测试中表现优于现有方法。

Conclusion: 该方法通过语义感知的消息传递，显著提升了知识图谱补全的准确性和鲁棒性。

Abstract: Semantic context surrounding a triplet $(h, r, t)$ is crucial for Knowledge
Graph Completion (KGC), providing vital cues for prediction. However,
traditional node-based message passing mechanisms, when applied to knowledge
graphs, often introduce noise and suffer from information dilution or
over-smoothing by indiscriminately aggregating information from all neighboring
edges. To address this challenge, we propose a semantic-aware relational
message passing. A core innovation of this framework is the introduction of a
\textbf{semantic-aware Top-K neighbor selection strategy}. Specifically, this
strategy first evaluates the semantic relevance between a central node and its
incident edges within a shared latent space, selecting only the Top-K most
pertinent ones. Subsequently, information from these selected edges is
effectively fused with the central node's own representation using a
\textbf{multi-head attention aggregator} to generate a semantically focused
node message. In this manner, our model not only leverages the structure and
features of edges within the knowledge graph but also more accurately captures
and propagates the contextual information most relevant to the specific link
prediction task, thereby effectively mitigating interference from irrelevant
information. Extensive experiments demonstrate that our method achieves
superior performance compared to existing approaches on several established
benchmarks.

</details>


### [77] [Rises for Measuring Local Distributivity in Lattices](https://arxiv.org/abs/2506.23168)
*Mohammad Abdulla,Tobias Hille,Dominik Dürrschnabel,Gerd Stumme*

Main category: cs.AI

TL;DR: 本文提出了一种通过“rises”来量化概念格中分配性的方法，并证明了格是分配的当且仅当不存在非单位rises。


<details>
  <summary>Details</summary>
Motivation: 在形式概念分析（FCA）中，格通常表现出高度的分配性，但缺乏标准化的量化方法。

Method: 引入rises作为评估分配性的工具，研究其在概念格中的表现，并与经典的meet-和join分配性关联。

Result: 现实数据中的概念格高度join-分配，但meet-分配性较低。

Conclusion: rises是量化分配性的有效工具，揭示了概念格中分配性的不对称性。

Abstract: Distributivity is a well-established and extensively studied notion in
lattice theory. In the context of data analysis, particularly within Formal
Concept Analysis (FCA), lattices are often observed to exhibit a high degree of
distributivity. However, no standardized measure exists to quantify this
property. In this paper, we introduce the notion of rises in (concept) lattices
as a means to assess distributivity. Rises capture how the number of attributes
or objects in covering concepts change within the concept lattice. We show that
a lattice is distributive if and only if no non-unit rises occur. Furthermore,
we relate rises to the classical notion of meet- and join distributivity. We
observe that concept lattices from real-world data are to a high degree
join-distributive, but much less meet-distributive. We additionally study how
join-distributivity manifests on the level of ordered sets.

</details>


### [78] [FinStat2SQL: A Text2SQL Pipeline for Financial Statement Analysis](https://arxiv.org/abs/2506.23273)
*Quang Hung Nguyen,Phuong Anh Trinh,Phan Quoc Hung Mai,Tuan Phong Trinh*

Main category: cs.AI

TL;DR: FinStat2SQL是一个轻量级文本到SQL的管道，专为金融领域设计，结合大小语言模型，支持自然语言查询，并在越南企业环境中表现出色。


<details>
  <summary>Details</summary>
Motivation: 金融领域的数据库设计和报表布局差异大，现有文本到SQL技术难以应对复杂和特定领域的查询需求。

Method: 采用多代理设置，结合大小语言模型，进行实体提取、SQL生成和自我校正，并构建特定领域数据库进行评估。

Result: 7B微调模型在消费硬件上达到61.33%准确率，响应时间低于4秒，优于GPT-4o-mini。

Conclusion: FinStat2SQL为越南企业提供了可扩展、经济高效的金融分析解决方案。

Abstract: Despite the advancements of large language models, text2sql still faces many
challenges, particularly with complex and domain-specific queries. In finance,
database designs and financial reporting layouts vary widely between financial
entities and countries, making text2sql even more challenging. We present
FinStat2SQL, a lightweight text2sql pipeline enabling natural language queries
over financial statements. Tailored to local standards like VAS, it combines
large and small language models in a multi-agent setup for entity extraction,
SQL generation, and self-correction. We build a domain-specific database and
evaluate models on a synthetic QA dataset. A fine-tuned 7B model achieves
61.33\% accuracy with sub-4-second response times on consumer hardware,
outperforming GPT-4o-mini. FinStat2SQL offers a scalable, cost-efficient
solution for financial analysis, making AI-powered querying accessible to
Vietnamese enterprises.

</details>


### [79] [Corrupted by Reasoning: Reasoning Language Models Become Free-Riders in Public Goods Games](https://arxiv.org/abs/2506.23276)
*David Guzman Piedrahita,Yongjin Yang,Mrinmaya Sachan,Giorgia Ramponi,Bernhard Schölkopf,Zhijing Jin*

Main category: cs.AI

TL;DR: 研究大型语言模型（LLMs）在多智能体系统中的合作行为，发现不同模型在公共物品博弈中表现出四种行为模式，推理能力强的模型反而合作表现较差。


<details>
  <summary>Details</summary>
Motivation: 理解LLMs在自主代理中的合作机制，尤其是如何平衡自利与集体利益，以确保其对齐性、鲁棒性和安全部署。

Method: 通过行为经济学的公共物品博弈实验，观察不同LLMs在重复互动中的行为模式。

Result: 发现四种行为模式：持续高合作、波动合作、逐渐衰退合作和固定策略。推理能力强的模型合作表现较差。

Conclusion: 当前提升LLMs推理能力的方法未必能促进合作，为需要持续协作的环境提供了重要启示。

Abstract: As large language models (LLMs) are increasingly deployed as autonomous
agents, understanding their cooperation and social mechanisms is becoming
increasingly important. In particular, how LLMs balance self-interest and
collective well-being is a critical challenge for ensuring alignment,
robustness, and safe deployment. In this paper, we examine the challenge of
costly sanctioning in multi-agent LLM systems, where an agent must decide
whether to invest its own resources to incentivize cooperation or penalize
defection. To study this, we adapt a public goods game with institutional
choice from behavioral economics, allowing us to observe how different LLMs
navigate social dilemmas over repeated interactions. Our analysis reveals four
distinct behavioral patterns among models: some consistently establish and
sustain high levels of cooperation, others fluctuate between engagement and
disengagement, some gradually decline in cooperative behavior over time, and
others rigidly follow fixed strategies regardless of outcomes. Surprisingly, we
find that reasoning LLMs, such as the o1 series, struggle significantly with
cooperation, whereas some traditional LLMs consistently achieve high levels of
cooperation. These findings suggest that the current approach to improving
LLMs, which focuses on enhancing their reasoning capabilities, does not
necessarily lead to cooperation, providing valuable insights for deploying LLM
agents in environments that require sustained collaboration. Our code is
available at https://github.com/davidguzmanp/SanctSim

</details>


### [80] [GATSim: Urban Mobility Simulation with Generative Agents](https://arxiv.org/abs/2506.23306)
*Qi Liu,Can Li,Wanjing Ma*

Main category: cs.AI

TL;DR: GATSim利用大语言模型和AI代理技术，提出了一种新型城市交通模拟框架，生成具有丰富行为特征的代理，能够模拟人类旅行决策的复杂性、适应性和行为多样性。


<details>
  <summary>Details</summary>
Motivation: 传统基于规则的代理模拟无法捕捉人类旅行决策的复杂性和多样性，而大语言模型和AI代理技术的发展为解决这一问题提供了新机会。

Method: GATSim结合城市交通基础模型、代理认知系统和交通模拟环境，通过心理记忆系统、工具使用能力和终身学习机制，生成具有多样化社会经济属性和行为特征的代理。

Result: 实验表明，GATSim生成的代理在交通场景中表现与人类标注者相当，并能自然生成宏观交通演化模式。

Conclusion: GATSim为城市交通模拟提供了一种更真实、适应性更强的解决方案，通过生成代理的行为多样性提升了模拟的逼真度。

Abstract: Traditional agent-based urban mobility simulations rely on rigid rule-based
systems that fail to capture the complexity, adaptability, and behavioral
diversity characteristic of human travel decision-making. Recent advances in
large language models and AI agent technology offer opportunities to create
agents with reasoning capabilities, persistent memory, and adaptive learning
mechanisms. We propose GATSim (Generative-Agent Transport Simulation), a novel
framework that leverages these advances to create generative agents with rich
behavioral characteristics for urban mobility simulation. Unlike conventional
approaches, GATSim agents possess diverse socioeconomic attributes, individual
lifestyles, and evolving preferences that shape their mobility decisions
through psychologically-informed memory systems, tool usage capabilities, and
lifelong learning mechanisms. The main contributions of this study include: (1)
a comprehensive architecture combining an urban mobility foundation model with
agent cognitive systems and transport simulation environment, (2) a fully
functional prototype implementation, and (3) systematic validation
demonstrating that generative agents produce believable travel behaviors.
Through designed reflection processes, generative agents in this study can
transform specific travel experiences into generalized insights, enabling
realistic behavioral adaptation over time with specialized mechanisms for
activity planning and real-time reactive behaviors tailored to urban mobility
contexts. Experiments show that generative agents perform competitively with
human annotators in mobility scenarios while naturally producing macroscopic
traffic evolution patterns. The code for the prototype system is shared at
https://github.com/qiliuchn/gatsim.

</details>


### [81] [The Confidence Paradox: Can LLM Know When It's Wrong](https://arxiv.org/abs/2506.23464)
*Sahil Tripathi,Md Tabrez Nafis,Imran Hussain,Jiechao Gao*

Main category: cs.AI

TL;DR: HonestVQA是一个自监督的诚实校准框架，旨在解决DocVQA系统中的伦理问题，通过量化不确定性和对齐模型置信度，提升准确性和伦理响应能力。


<details>
  <summary>Details</summary>
Motivation: 现有DocVQA系统在伦理响应方面表现不足，模型置信度与实际知识不匹配，可能导致高风险。

Method: HonestVQA采用自监督方法，包括不确定性量化、加权损失函数对齐置信度，以及对比学习强制伦理响应行为。

Result: HonestVQA在多个数据集上提升了准确性和F1分数，同时降低了过度自信，表现出强泛化能力。

Conclusion: HonestVQA通过伦理对齐和自监督学习，显著提升了DocVQA系统的性能和可信度。

Abstract: Document Visual Question Answering (DocVQA) systems are increasingly deployed
in real world applications, yet they remain ethically opaque-often producing
overconfident answers to ambiguous questions or failing to communicate
uncertainty in a trustworthy manner. This misalignment between model confidence
and actual knowledge poses significant risks, particularly in domains requiring
ethical accountability. Existing approaches such as LayoutLMv3, UDOP, and DONUT
have advanced SOTA performance by focusing on architectural sophistication and
accuracy; however, they fall short in ethical responsiveness.
  To address these limitations, we introduce HonestVQA, a self-supervised
honesty calibration framework for ethically aligned DocVQA. Our model-agnostic
method quantifies uncertainty to identify knowledge gaps, aligns model
confidence with actual correctness using weighted loss functions, and enforces
ethical response behavior via contrastive learning. We further introduce two
principled evaluation metrics--Honesty Score (H-Score) and Ethical Confidence
Index (ECI)--to benchmark alignment between confidence, accuracy, and ethical
communication. Empirically, HonestVQA improves DocVQA accuracy by up to 4.3%
and F1 by 4.3% across SpDocVQA, InfographicsVQA, and SROIE datasets. It reduces
overconfidence, lowering H-Score and ECI by 0.072 and 0.078, respectively. In
cross domain evaluation, it achieves up to 78.9% accuracy and 76.1% F1-score,
demonstrating strong generalization. Ablation shows a 3.8% drop in accuracy
without alignment or contrastive loss.

</details>


### [82] [Data Augmentation for Cognitive Behavioral Therapy: Leveraging ERNIE Language Models using Artificial Intelligence](https://arxiv.org/abs/2506.23503)
*Bosubabu Sambana,Kondreddygari Archana,Suram Indhra Sena Reddy,Shaik Meethaigar Jameer Basha,Shaik Karishma*

Main category: cs.AI

TL;DR: 论文提出了一种基于CBT框架的系统，利用BERT、RoBERTa等模型分析社交媒体中的负面情绪和认知扭曲，并预测潜在心理健康问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法在分析社交媒体中的认知路径方面存在不足，无法为心理治疗师提供及时有效的干预工具。

Method: 结合CBT框架，使用BERT、RoBERTa进行情感分析，T5、PEGASUS进行文本摘要，mT5进行多语言翻译，以识别负面情绪和认知扭曲。

Result: 系统不仅能识别负面思维，还能预测潜在心理健康问题（如恐惧症、饮食障碍），提供更全面的干预策略。

Conclusion: 该系统为心理治疗师提供了早期检测和治疗心理问题的有力工具。

Abstract: Cognitive Behavioral Therapy (CBT) is a proven approach for addressing the
irrational thought patterns associated with mental health disorders, but its
effectiveness relies on accurately identifying cognitive pathways to provide
targeted treatment. In today's digital age, individuals often express negative
emotions on social media, where they may reveal cognitive distortions, and in
severe cases, exhibit suicidal tendencies. However, there is a significant gap
in methodologies designed to analyze these cognitive pathways, which could be
critical for psychotherapists aiming to deliver timely and effective
interventions in online environments. Cognitive Behavioral Therapy (CBT)
framework leveraging acceptance, commitment and data augmentation to categorize
and address both textual and visual content as positive or negative.
Specifically, the system employs BERT, RoBERTa for Sentiment Analysis and T5,
PEGASUS for Text Summarization, mT5 for Text Translation in Multiple Languages
focusing on detecting negative emotions and cognitive distortions within social
media data. While existing models are primarily designed to identify negative
thoughts, the proposed system goes beyond this by predicting additional
negative side effects and other potential mental health disorders likes
Phobias, Eating Disorders. This enhancement allows for a more comprehensive
understanding and intervention strategy, offering psychotherapists a powerful
tool for early detection and treatment of various psychological issues.

</details>


### [83] [Hybrid Approach for Electricity Price Forecasting using AlexNet and LSTM](https://arxiv.org/abs/2506.23504)
*Bosubabu Sambana,Kotamsetty Geethika Devi,Bandi Rajeswara Reddy,Galeti Mohammad Hussain,Gownivalla Siddartha*

Main category: cs.AI

TL;DR: 论文提出了一种结合AlexNet和LSTM的混合模型，用于提高电价预测的准确性，优于传统RNN和ANN方法。


<details>
  <summary>Details</summary>
Motivation: 传统方法仅关注需求和价格，无法充分分析时间序列数据，且预测准确性不足。

Method: 采用AlexNet进行特征提取，LSTM学习序列模式，结合外部变量（如需求、温度、阳光和降雨）构建模型。

Result: 混合模型准确率达97.08%，优于RNN（96.64%）和ANN（96.63%）。

Conclusion: 混合模型显著提升了电价预测的准确性，证明了其优于传统方法的潜力。

Abstract: The recent development of advanced machine learning methods for hybrid models
has greatly addressed the need for the correct prediction of electrical prices.
This method combines AlexNet and LSTM algorithms, which are used to introduce a
new model with higher accuracy in price forecasting. Despite RNN and ANN being
effective, they often fail to deal with forex time sequence data. The
traditional methods do not accurately forecast the prices. These traditional
methods only focus on demand and price which leads to insufficient analysis of
data. To address this issue, using the hybrid approach, which focuses on
external variables that also effect the predicted prices. Nevertheless, due to
AlexNet's excellent feature extraction and LSTM's learning sequential patterns,
the prediction accuracy is vastly increased. The model is built on the past
data, which has been supplied with the most significant elements like demand,
temperature, sunlight, and rain. For example, the model applies methods, such
as minimum-maximum scaling and a time window, to predict the electricity prices
of the future. The results show that this hybrid model is good than the
standalone ones in terms of accuracy. Although we got our accuracy rating of
97.08, it shows higher accompaniments than remaining models RNN and ANN with
accuracies of 96.64 and 96.63 respectively.

</details>


### [84] [Assessing GPTZero's Accuracy in Identifying AI vs. Human-Written Essays](https://arxiv.org/abs/2506.23517)
*Selin Dik,Osman Erdem,Mehmet Dik*

Main category: cs.AI

TL;DR: GPTZero检测AI生成文本的准确率高，但对人类写作的误判较多，教育者需谨慎使用。


<details>
  <summary>Details</summary>
Motivation: 研究AI检测工具（如GPTZero）在识别AI生成文本时的可靠性，尤其是对不同长度文本的检测效果。

Method: 收集28篇AI生成和50篇人类写作的论文，按长度分类后输入GPTZero检测AI生成概率和置信度。

Result: AI生成文本检测准确率高达91-100%，但人类写作存在误判（假阳性）。

Conclusion: GPTZero对纯AI生成内容有效，但对人类写作区分能力有限，建议教育者谨慎依赖。

Abstract: As the use of AI tools by students has become more prevalent, instructors
have started using AI detection tools like GPTZero and QuillBot to detect AI
written text. However, the reliability of these detectors remains uncertain. In
our study, we focused mostly on the success rate of GPTZero, the most-used AI
detector, in identifying AI-generated texts based on different lengths of
randomly submitted essays: short (40-100 word count), medium (100-350 word
count), and long (350-800 word count). We gathered a data set consisting of
twenty-eight AI-generated papers and fifty human-written papers. With this
randomized essay data, papers were individually plugged into GPTZero and
measured for percentage of AI generation and confidence. A vast majority of the
AI-generated papers were detected accurately (ranging from 91-100% AI believed
generation), while the human generated essays fluctuated; there were a handful
of false positives. These findings suggest that although GPTZero is effective
at detecting purely AI-generated content, its reliability in distinguishing
human-authored texts is limited. Educators should therefore exercise caution
when relying solely on AI detection tools.

</details>


### [85] [ChemActor: Enhancing Automated Extraction of Chemical Synthesis Actions with LLM-Generated Data](https://arxiv.org/abs/2506.23520)
*Yu Zhang,Ruijie Yu,Jidong Tian,Feng Zhu,Jiapeng Liu,Xiaokang Yang,Yaohui Jin,Yanyan Xu*

Main category: cs.AI

TL;DR: ChemActor是一个基于LLM的化学执行器，用于将非结构化实验程序转换为结构化动作序列，通过LLM生成的数据框架解决了标注数据不足和质量低的问题，性能优于基线模型10%。


<details>
  <summary>Details</summary>
Motivation: 随着机器人合成在有机化学中的兴趣增加，从文献中自动提取化学程序变得至关重要，但化学语言的模糊性和高标注成本使其具有挑战性。

Method: 提出ChemActor，一个完全微调的LLM，结合数据选择模块和多轮LLM循环评估指标，生成机器可执行的动作序列。

Result: 在R2D和D2A任务中，ChemActor通过LLM生成的数据增强，性能优于基线模型10%。

Conclusion: ChemActor展示了LLM在化学实验程序理解中的先进性，为自动化化学合成提供了有效工具。

Abstract: With the increasing interest in robotic synthesis in the context of organic
chemistry, the automated extraction of chemical procedures from literature is
critical. However, this task remains challenging due to the inherent ambiguity
of chemical language and the high cost of human annotation required for
developing reliable computer-aided extraction protocols. Here, we present
ChemActor, a fully fine-tuned large language model (LLM), as a chemical
executor to convert between unstructured experimental procedures and structured
action sequences. We propose a sequential LLM-generated data framework to
address the challenges of insufficient and low-quality annotated data. This
framework integrates a data selection module that selects data based on
distribution divergence, with a general-purpose LLM, to generate
machine-executable actions from a single molecule input. Additionally, we
introduce a novel multi-round LLMs circle review metric, which reflects the
model's advanced understanding of chemical experimental procedures. Extensive
experiments on reaction-to-description (R2D) and description-to-action (D2A)
tasks demonstrate that ChemActor, augmented by LLM-generated data, achieves
state-of-the-art performance, outperforming the baseline model by 10%. The code
is available at: https://github.com/Zhanghahah/ChemActor.

</details>


### [86] [CooT: Learning to Coordinate In-Context with Coordination Transformers](https://arxiv.org/abs/2506.23549)
*Huai-Chih Wang,Hsiang-Chun Chuang,Hsi-Chun Cheng,Dai-Jie Wu,Shao-Hua Sun*

Main category: cs.AI

TL;DR: 提出了一种名为Coordination Transformers（CooT）的新框架，通过利用交互历史快速适应未见过的合作伙伴，显著提升了多智能体系统中的协调能力。


<details>
  <summary>Details</summary>
Motivation: 解决现有方法在动态和不确定环境中协调能力不足的问题，尤其是对未见合作伙伴的泛化能力差或训练成本高。

Method: 采用基于上下文协调的框架CooT，通过预测与观察到的合作伙伴行为一致的动作来快速适应新行为，无需显式监督或微调。

Result: 在Overcooked基准测试中，CooT显著优于基线方法，并在人类评估中表现出最高的协作效果。

Conclusion: CooT在多智能体场景中展现出强大的鲁棒性、灵活性和对上下文的敏感性，是一种高效的协调解决方案。

Abstract: Effective coordination among artificial agents in dynamic and uncertain
environments remains a significant challenge in multi-agent systems. Existing
approaches, such as self-play and population-based methods, either generalize
poorly to unseen partners or require extensive training. To overcome these
limitations, we propose Coordination Transformers (CooT), a novel in-context
coordination framework that uses recent interaction histories to adapt to
unseen partners rapidly. Unlike previous approaches that primarily aim to
increase the diversity of training partners, CooT explicitly focuses on
adapting to new partner behaviors by predicting actions aligned with observed
partner interactions. Trained on interaction trajectories collected from
diverse pairs of agents with complementary behaviors, CooT quickly learns
effective coordination strategies without explicit supervision or fine-tuning.
Evaluations on the Overcooked benchmark demonstrate that CooT significantly
outperforms baseline methods in coordination tasks involving previously unseen
partners. Human evaluations further confirm CooT as the most effective
collaborative partner, while extensive ablations highlight its robustness,
flexibility, and sensitivity to context in multi-agent scenarios.

</details>


### [87] [MMReason: An Open-Ended Multi-Modal Multi-Step Reasoning Benchmark for MLLMs Toward AGI](https://arxiv.org/abs/2506.23563)
*Huanjin Yao,Jiaxing Huang,Yawen Qiu,Michael K. Chen,Wenzheng Liu,Wei Zhang,Wenjie Zeng,Xikun Zhang,Jingyi Zhang,Yuxin Song,Wenhao Wu,Dacheng Tao*

Main category: cs.AI

TL;DR: MMReason是一个新基准，用于全面评估多模态大语言模型的长链推理能力，填补了现有基准在难度、多样性和中间步骤评估上的不足。


<details>
  <summary>Details</summary>
Motivation: 现有MLLM基准在长链推理评估上存在不足，包括缺乏难度和多样性、易受猜测和记忆影响，以及对中间推理步骤评估不足。

Method: MMReason通过多学科、多难度的问题库，开放性问题格式，多模型投票过滤，以及基于参考的三元评分机制，全面评估推理能力。

Result: MMReason对主流MLLM进行了基准测试，并深入分析了其推理能力。

Conclusion: MMReason为MLLM推理研究提供了有价值的资源，代码已开源。

Abstract: Reasoning plays a crucial role in advancing Multimodal Large Language Models
(MLLMs) toward Artificial General Intelligence. However, existing MLLM
benchmarks often fall short in precisely and comprehensively evaluating
long-chain reasoning abilities from three key aspects: (1) lack of difficulty
and diversity, (2) susceptibility to guessability and memorization, (3)
inadequate assessment of intermediate reasoning steps. To fill this gap, we
introduce MMReason, a new benchmark designed to precisely and comprehensively
evaluate MLLM long-chain reasoning capability with diverse, open-ended,
challenging questions. First, we curate challenging questions requiring
multi-step reasoning from various fields (i.e., 6 disciplines) and multiple
difficulty levels (i.e., from pre-university to university, and from
foundational to competition tiers). Second, these questions are reformulated
into an open-ended format and filtered using a multi-model voting technique to
eliminate shortcut cases related to guessing and memorization, ensuring robust
reasoning evaluations. Third, we annotate the questions with detailed
step-by-step solutions, and design a reference-based ternary scoring mechanism
to reliably assess intermediate reasoning steps. With MMReason, we benchmark
popular leading MLLMs and provide an in-depth analysis of their reasoning
capabilities. We hope MMReason will serve as a valuable resource for advancing
MLLM reasoning research. Code will be available at
https://github.com/HJYao00/MMReason.

</details>


### [88] [Evaluating Multi-Agent Defences Against Jailbreaking Attacks on Large Language Models](https://arxiv.org/abs/2506.23576)
*Maria Carolina Cornelia Wit,Jun Pang*

Main category: cs.AI

TL;DR: 多智能体LLM系统可增强对越狱攻击的防御，但存在误报和计算开销等权衡。


<details>
  <summary>Details</summary>
Motivation: 研究多智能体LLM系统作为防御越狱攻击的有效性。

Method: 比较单智能体与多智能体配置，评估三种越狱策略（AutoDefense、BetterDan、JB）。

Result: 多智能体系统提高防御能力，减少漏报，但效果因攻击类型而异，且增加误报和计算开销。

Conclusion: 当前自动防御存在局限，未来需改进LLM系统的对齐鲁棒性。

Abstract: Recent advances in large language models (LLMs) have raised concerns about
jailbreaking attacks, i.e., prompts that bypass safety mechanisms. This paper
investigates the use of multi-agent LLM systems as a defence against such
attacks. We evaluate three jailbreaking strategies, including the original
AutoDefense attack and two from Deepleaps: BetterDan and JB. Reproducing the
AutoDefense framework, we compare single-agent setups with two- and three-agent
configurations. Our results show that multi-agent systems enhance resistance to
jailbreaks, especially by reducing false negatives. However, its effectiveness
varies by attack type, and it introduces trade-offs such as increased false
positives and computational overhead. These findings point to the limitations
of current automated defences and suggest directions for improving alignment
robustness in future LLM systems.

</details>


### [89] [Self-correcting Reward Shaping via Language Models for Reinforcement Learning Agents in Games](https://arxiv.org/abs/2506.23626)
*António Afonso,Iolanda Leite,Alessandro Sestini,Florian Fuchs,Konrad Tollmar,Linus Gisslén*

Main category: cs.AI

TL;DR: 论文提出了一种基于语言模型的自动化方法，用于迭代优化强化学习代理的奖励函数权重，解决了游戏内容或机制修改后奖励权重不再最优的问题。


<details>
  <summary>Details</summary>
Motivation: 在游戏中部署强化学习代理时，设计有效的奖励函数需要专家，且游戏内容修改后奖励权重可能失效。

Method: 使用语言模型根据用户定义的行为目标和历史性能统计，迭代更新奖励函数权重。

Result: 在赛车任务中，LM引导的代理性能显著提升，从9%成功率提高到74%，最终达到80%成功率，接近专家手动调整的94%。

Conclusion: 自动化方法能够有效替代手动奖励工程，提升代理性能。

Abstract: Reinforcement Learning (RL) in games has gained significant momentum in
recent years, enabling the creation of different agent behaviors that can
transform a player's gaming experience. However, deploying RL agents in
production environments presents two key challenges: (1) designing an effective
reward function typically requires an RL expert, and (2) when a game's content
or mechanics are modified, previously tuned reward weights may no longer be
optimal. Towards the latter challenge, we propose an automated approach for
iteratively fine-tuning an RL agent's reward function weights, based on a
user-defined language based behavioral goal. A Language Model (LM) proposes
updated weights at each iteration based on this target behavior and a summary
of performance statistics from prior training rounds. This closed-loop process
allows the LM to self-correct and refine its output over time, producing
increasingly aligned behavior without the need for manual reward engineering.
We evaluate our approach in a racing task and show that it consistently
improves agent performance across iterations. The LM-guided agents show a
significant increase in performance from $9\%$ to $74\%$ success rate in just
one iteration. We compare our LM-guided tuning against a human expert's manual
weight design in the racing task: by the final iteration, the LM-tuned agent
achieved an $80\%$ success rate, and completed laps in an average of $855$ time
steps, a competitive performance against the expert-tuned agent's peak $94\%$
success, and $850$ time steps.

</details>


### [90] [HASD: Hierarchical Adaption for pathology Slide-level Domain-shift](https://arxiv.org/abs/2506.23673)
*Jingsong Liu,Han Li,Chen Yang,Michael Deutges,Ario Sadafi,Xin You,Katharina Breininger,Nassir Navab,Peter J. Schüffler*

Main category: cs.AI

TL;DR: 提出了一种名为HASD的分层适应框架，用于解决病理学AI中的幻灯片级域偏移问题，通过多尺度特征一致性和计算高效的域适应方法，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 病理学数据受中心特定条件影响严重，现有方法仅关注图像块而非全幻灯片图像（WSI），无法满足临床需求。

Method: HASD框架包含分层适应组件（特征对齐、几何不变性正则化和注意力一致性正则化）和原型选择机制。

Result: 在五个数据集上验证，乳腺癌HER2分级任务AUROC提升4.1%，UCEC生存预测任务C-index提升3.9%。

Conclusion: HASD为病理学机构提供了一种实用且可靠的幻灯片级域适应解决方案，降低了计算和标注成本。

Abstract: Domain shift is a critical problem for pathology AI as pathology data is
heavily influenced by center-specific conditions. Current pathology domain
adaptation methods focus on image patches rather than WSI, thus failing to
capture global WSI features required in typical clinical scenarios. In this
work, we address the challenges of slide-level domain shift by proposing a
Hierarchical Adaptation framework for Slide-level Domain-shift (HASD). HASD
achieves multi-scale feature consistency and computationally efficient
slide-level domain adaptation through two key components: (1) a hierarchical
adaptation framework that integrates a Domain-level Alignment Solver for
feature alignment, a Slide-level Geometric Invariance Regularization to
preserve the morphological structure, and a Patch-level Attention Consistency
Regularization to maintain local critical diagnostic cues; and (2) a prototype
selection mechanism that reduces computational overhead. We validate our method
on two slide-level tasks across five datasets, achieving a 4.1\% AUROC
improvement in a Breast Cancer HER2 Grading cohort and a 3.9\% C-index gain in
a UCEC survival prediction cohort. Our method provides a practical and reliable
slide-level domain adaption solution for pathology institutions, minimizing
both computational and annotation costs.

</details>


### [91] [PokéAI: A Goal-Generating, Battle-Optimizing Multi-agent System for Pokemon Red](https://arxiv.org/abs/2506.23689)
*Zihao Liu,Xinhang Sui,Yueran Song,Siwen Wang*

Main category: cs.AI

TL;DR: PokéAI是一个基于文本的多智能体大型语言模型框架，用于自主玩Pokémon Red游戏，包含规划、执行和评估三个智能体，初步测试显示其战斗模块表现接近人类玩家。


<details>
  <summary>Details</summary>
Motivation: 开发一个能够自主玩Pokémon Red游戏的AI系统，探索语言模型在策略游戏中的应用潜力。

Method: 采用三个智能体（规划、执行、评估）分工协作，形成闭环决策系统，并开发了战斗模块进行测试。

Result: 战斗模块在50次野生遭遇战中平均胜率为80.8%，接近人类玩家水平，且语言能力与战斗表现相关。

Conclusion: PokéAI展示了语言模型在策略游戏中的潜力，不同模型表现出独特的游戏风格。

Abstract: We introduce Pok\'eAI, the first text-based, multi-agent large language model
(LLM) framework designed to autonomously play and progress through Pok\'emon
Red. Our system consists of three specialized agents-Planning, Execution, and
Critique-each with its own memory bank, role, and skill set. The Planning Agent
functions as the central brain, generating tasks to progress through the game.
These tasks are then delegated to the Execution Agent, which carries them out
within the game environment. Upon task completion, the Critique Agent evaluates
the outcome to determine whether the objective was successfully achieved. Once
verification is complete, control returns to the Planning Agent, forming a
closed-loop decision-making system.
  As a preliminary step, we developed a battle module within the Execution
Agent. Our results show that the battle AI achieves an average win rate of
80.8% across 50 wild encounters, only 6% lower than the performance of an
experienced human player. Furthermore, we find that a model's battle
performance correlates strongly with its LLM Arena score on language-related
tasks, indicating a meaningful link between linguistic ability and strategic
reasoning. Finally, our analysis of gameplay logs reveals that each LLM
exhibits a unique playstyle, suggesting that individual models develop distinct
strategic behaviors.

</details>


### [92] [Agent4S: The Transformation of Research Paradigms from the Perspective of Large Language Models](https://arxiv.org/abs/2506.23692)
*Boyuan Zheng,Zerui Fang,Zhe Xu,Rui Wang,Yiwen Chen,Cunshi Wang,Mengwei Qu,Lei Lei,Zhen Feng,Yan Liu,Yuyang Li,Mingzhou Tan,Jiaji Wu,Jianwei Shuai,Jia Li,Fangfu Ye*

Main category: cs.AI

TL;DR: 论文提出将LLM驱动的智能体（Agent4S）作为第五科学范式，以自动化整个科研流程，解决当前AI4S的效率问题。


<details>
  <summary>Details</summary>
Motivation: 当前AI4S作为分析工具未能解决科研效率低下的核心问题，需要更高效的自动化方法。

Method: 提出五级分类框架，从简单任务自动化到完全自主协作的“AI科学家”。

Result: 为科学发现的下一次革命性进步提供了清晰的路线图。

Conclusion: Agent4S是真正的第五科学范式，将彻底改变科研工作流程。

Abstract: While AI for Science (AI4S) serves as an analytical tool in the current
research paradigm, it doesn't solve its core inefficiency. We propose "Agent
for Science" (Agent4S)-the use of LLM-driven agents to automate the entire
research workflow-as the true Fifth Scientific Paradigm. This paper introduces
a five-level classification for Agent4S, outlining a clear roadmap from simple
task automation to fully autonomous, collaborative "AI Scientists." This
framework defines the next revolutionary step in scientific discovery.

</details>


### [93] [A New Perspective On AI Safety Through Control Theory Methodologies](https://arxiv.org/abs/2506.23703)
*Lars Ullrich,Walter Zimmer,Ross Greer,Knut Graichen,Alois C. Knoll,Mohan Trivedi*

Main category: cs.AI

TL;DR: 论文提出了一种基于跨学科视角的AI安全新方法，结合数据生成过程与系统理论，推动AI工程的安全分析与保障。


<details>
  <summary>Details</summary>
Motivation: AI在安全关键领域的应用缺乏安全保障，需要结合控制理论和AI技术提升安全性。

Method: 采用系统理论驱动的跨学科方法，提出数据控制的新视角，结合安全分析与保障。

Result: 提出了一种通用的安全分析与保障框架，适用于具体AI系统，并为未来创新做准备。

Conclusion: 通过数据控制的跨学科方法，为AI安全提供了新的理论基础和实践方向。

Abstract: While artificial intelligence (AI) is advancing rapidly and mastering
increasingly complex problems with astonishing performance, the safety
assurance of such systems is a major concern. Particularly in the context of
safety-critical, real-world cyber-physical systems, AI promises to achieve a
new level of autonomy but is hampered by a lack of safety assurance. While
data-driven control takes up recent developments in AI to improve control
systems, control theory in general could be leveraged to improve AI safety.
Therefore, this article outlines a new perspective on AI safety based on an
interdisciplinary interpretation of the underlying data-generation process and
the respective abstraction by AI systems in a system theory-inspired and system
analysis-driven manner. In this context, the new perspective, also referred to
as data control, aims to stimulate AI engineering to take advantage of existing
safety analysis and assurance in an interdisciplinary way to drive the paradigm
of data control. Following a top-down approach, a generic foundation for safety
analysis and assurance is outlined at an abstract level that can be refined for
specific AI systems and applications and is prepared for future innovation.

</details>


### [94] [Attestable Audits: Verifiable AI Safety Benchmarks Using Trusted Execution Environments](https://arxiv.org/abs/2506.23706)
*Christoph Schnabl,Daniel Hugenroth,Bill Marino,Alastair R. Beresford*

Main category: cs.AI

TL;DR: 提出了一种名为Attestable Audits的方法，利用可信执行环境（TEE）确保AI模型的合规性验证，同时保护模型和数据隐私。


<details>
  <summary>Details</summary>
Motivation: 解决现有基准测试无法提供可验证结果且缺乏对模型和数据隐私保护的问题。

Method: 在可信执行环境中运行Attestable Audits，确保用户可验证与合规AI模型的交互。

Result: 成功构建原型，并在Llama-3.1上验证了典型审计基准的可行性。

Conclusion: Attestable Audits为AI治理框架中的验证挑战提供了可行解决方案，同时保护敏感数据。

Abstract: Benchmarks are important measures to evaluate safety and compliance of AI
models at scale. However, they typically do not offer verifiable results and
lack confidentiality for model IP and benchmark datasets. We propose Attestable
Audits, which run inside Trusted Execution Environments and enable users to
verify interaction with a compliant AI model. Our work protects sensitive data
even when model provider and auditor do not trust each other. This addresses
verification challenges raised in recent AI governance frameworks. We build a
prototype demonstrating feasibility on typical audit benchmarks against
Llama-3.1.

</details>


### [95] [BayesL: Towards a Logical Framework for Bayesian Networks](https://arxiv.org/abs/2506.23773)
*Stefano M. Nicoletti,Mariëlle Stoelinga*

Main category: cs.AI

TL;DR: BayesL是一种新的逻辑框架，用于指定、查询和验证贝叶斯网络的行为，支持因果和证据关系的推理。


<details>
  <summary>Details</summary>
Motivation: 为了解决贝叶斯网络中查询和验证行为的复杂性，减少手动修改模型的需求。

Method: 开发了一种结构化语言BayesL，支持创建查询和进行全面的假设场景评估。

Result: BayesL能够灵活推理因果关系和证据关系，无需手动修改模型。

Conclusion: BayesL为贝叶斯网络提供了一种高效且灵活的查询和验证工具。

Abstract: We introduce BayesL, a novel logical framework for specifying, querying, and
verifying the behaviour of Bayesian networks (BNs). BayesL (pronounced "Basil")
is a structured language that allows for the creation of queries over BNs. It
facilitates versatile reasoning concerning causal and evidence-based
relationships, and permits comprehensive what-if scenario evaluations without
the need for manual modifications to the model.

</details>


### [96] [When GNNs Met a Word Equations Solver: Learning to Rank Equations (Extended Technical Report)](https://arxiv.org/abs/2506.23784)
*Parosh Aziz Abdulla,Mohamed Faouzi Atig,Julie Cailler,Chencheng Liang,Philipp Rümmer*

Main category: cs.AI

TL;DR: 该论文探索了使用图神经网络（GNN）对单词方程进行排序以提高求解效率，提出了一种新的图表示方法，并通过实验验证了其优于现有求解器的性能。


<details>
  <summary>Details</summary>
Motivation: 解决单词方程时，处理顺序对求解效率有显著影响，因此需要一种智能排序方法。

Method: 提出了一种基于图的单词方程表示方法，利用GNN进行排序，并采用三种多分类任务适应策略处理变量数量的变化。训练时使用最小不可满足子集（MUSes）。

Result: 实验结果表明，在变量在每个方程中最多出现一次的基准测试中，新框架比现有求解器解决了更多问题。

Conclusion: GNN排序方法在特定条件下能显著提升单词方程求解效率。

Abstract: Nielsen transformation is a standard approach for solving word equations: by
repeatedly splitting equations and applying simplification steps, equations are
rewritten until a solution is reached. When solving a conjunction of word
equations in this way, the performance of the solver will depend considerably
on the order in which equations are processed. In this work, the use of Graph
Neural Networks (GNNs) for ranking word equations before and during the solving
process is explored. For this, a novel graph-based representation for word
equations is presented, preserving global information across conjuncts,
enabling the GNN to have a holistic view during ranking. To handle the variable
number of conjuncts, three approaches to adapt a multi-classification task to
the problem of ranking equations are proposed. The training of the GNN is done
with the help of minimum unsatisfiable subsets (MUSes) of word equations. The
experimental results show that, compared to state-of-the-art string solvers,
the new framework solves more problems in benchmarks where each variable
appears at most once in each equation.

</details>


### [97] [AI Risk-Management Standards Profile for General-Purpose AI (GPAI) and Foundation Models](https://arxiv.org/abs/2506.23949)
*Anthony M. Barrett,Jessica Newman,Brandie Nonnecke,Nada Madkour,Dan Hendrycks,Evan R. Murphy,Krystal Jackson,Deepika Raman*

Main category: cs.AI

TL;DR: 本文提出了针对通用人工智能/基础模型（GPAI/foundation models）的风险管理实践，旨在帮助开发者识别、分析和减轻相关风险。


<details>
  <summary>Details</summary>
Motivation: 随着多用途AI模型的普及，其带来的潜在风险也日益显著，需要专门的风险管理措施。

Method: 文档提供了风险管理实践或控制措施，并参考了NIST AI风险管理框架和ISO/IEC 23894标准。

Result: 为GPAI/foundation模型的开发者提供了实用的风险管理指南。

Conclusion: 该文档为GPAI/foundation模型的开发者提供了风险管理的具体指导，有助于减少潜在负面影响。

Abstract: Increasingly multi-purpose AI models, such as cutting-edge large language
models or other 'general-purpose AI' (GPAI) models, 'foundation models,'
generative AI models, and 'frontier models' (typically all referred to
hereafter with the umbrella term 'GPAI/foundation models' except where greater
specificity is needed), can provide many beneficial capabilities but also risks
of adverse events with profound consequences. This document provides
risk-management practices or controls for identifying, analyzing, and
mitigating risks of GPAI/foundation models. We intend this document primarily
for developers of large-scale, state-of-the-art GPAI/foundation models; others
that can benefit from this guidance include downstream developers of end-use
applications that build on a GPAI/foundation model. This document facilitates
conformity with or use of leading AI risk management-related standards,
adapting and building on the generic voluntary guidance in the NIST AI Risk
Management Framework and ISO/IEC 23894, with a focus on the unique issues faced
by developers of GPAI/foundation models.

</details>


### [98] [Advancing Learnable Multi-Agent Pathfinding Solvers with Active Fine-Tuning](https://arxiv.org/abs/2506.23793)
*Anton Andreychuk,Konstantin Yakovlev,Aleksandr Panov,Alexey Skrynnik*

Main category: cs.AI

TL;DR: MAPF-GPT-DDG是一种基于机器学习的多智能体路径规划（MAPF）求解器，通过集中式专家数据和新型delta-data生成机制优化训练，显著提升性能和可扩展性。


<details>
  <summary>Details</summary>
Motivation: 解决多机器人轨迹规划问题，尤其是在物流和搜救等实际应用中，需要高效且可扩展的MAPF求解器。

Method: 利用预训练的MAPF-GPT模型，通过集中式专家数据和delta-data生成机制进行微调。

Result: MAPF-GPT-DDG在测试场景中表现优于现有学习型求解器，支持单环境中多达100万智能体的路径规划。

Conclusion: MAPF-GPT-DDG为MAPF领域设定了新的可扩展性里程碑，展示了机器学习在复杂路径规划中的潜力。

Abstract: Multi-agent pathfinding (MAPF) is a common abstraction of multi-robot
trajectory planning problems, where multiple homogeneous robots simultaneously
move in the shared environment. While solving MAPF optimally has been proven to
be NP-hard, scalable, and efficient, solvers are vital for real-world
applications like logistics, search-and-rescue, etc. To this end, decentralized
suboptimal MAPF solvers that leverage machine learning have come on stage.
Building on the success of the recently introduced MAPF-GPT, a pure imitation
learning solver, we introduce MAPF-GPT-DDG. This novel approach effectively
fine-tunes the pre-trained MAPF model using centralized expert data. Leveraging
a novel delta-data generation mechanism, MAPF-GPT-DDG accelerates training
while significantly improving performance at test time. Our experiments
demonstrate that MAPF-GPT-DDG surpasses all existing learning-based MAPF
solvers, including the original MAPF-GPT, regarding solution quality across
many testing scenarios. Remarkably, it can work with MAPF instances involving
up to 1 million agents in a single environment, setting a new milestone for
scalability in MAPF domains.

</details>


### [99] [A Survey on Autonomy-Induced Security Risks in Large Model-Based Agents](https://arxiv.org/abs/2506.23844)
*Hang Su,Jun Luo,Chang Liu,Xiao Yang,Yichi Zhang,Yinpeng Dong,Jun Zhu*

Main category: cs.AI

TL;DR: 大型语言模型（LLM）驱动的自主AI代理带来了新的安全风险，如记忆污染和工具滥用，本文提出了一种风险感知的架构（R2A2）来应对这些挑战。


<details>
  <summary>Details</summary>
Motivation: 随着自主AI代理能力的提升，其安全风险也随之增加，需要新的防御策略和架构来确保安全性。

Method: 分析了代理的结构基础和关键能力，识别了安全漏洞，并提出了R2A2架构，基于约束马尔可夫决策过程（CMDP）实现风险感知。

Result: 提出了针对代理安全漏洞的防御策略，并设计了R2A2框架以优化决策过程的安全性。

Conclusion: R2A2架构为自主AI代理提供了一种系统化的安全解决方案，能够应对新兴的安全风险。

Abstract: Recent advances in large language models (LLMs) have catalyzed the rise of
autonomous AI agents capable of perceiving, reasoning, and acting in dynamic,
open-ended environments. These large-model agents mark a paradigm shift from
static inference systems to interactive, memory-augmented entities. While these
capabilities significantly expand the functional scope of AI, they also
introduce qualitatively novel security risks - such as memory poisoning, tool
misuse, reward hacking, and emergent misalignment - that extend beyond the
threat models of conventional systems or standalone LLMs. In this survey, we
first examine the structural foundations and key capabilities that underpin
increasing levels of agent autonomy, including long-term memory retention,
modular tool use, recursive planning, and reflective reasoning. We then analyze
the corresponding security vulnerabilities across the agent stack, identifying
failure modes such as deferred decision hazards, irreversible tool chains, and
deceptive behaviors arising from internal state drift or value misalignment.
These risks are traced to architectural fragilities that emerge across
perception, cognition, memory, and action modules. To address these challenges,
we systematically review recent defense strategies deployed at different
autonomy layers, including input sanitization, memory lifecycle control,
constrained decision-making, structured tool invocation, and introspective
reflection. We introduce the Reflective Risk-Aware Agent Architecture (R2A2), a
unified cognitive framework grounded in Constrained Markov Decision Processes
(CMDPs), which incorporates risk-aware world modeling, meta-policy adaptation,
and joint reward-risk optimization to enable principled, proactive safety
across the agent's decision-making loop.

</details>


### [100] [Beyond Statistical Learning: Exact Learning Is Essential for General Intelligence](https://arxiv.org/abs/2506.23908)
*András György,Tor Lattimore,Nevena Lazić,Csaba Szepesvári*

Main category: cs.AI

TL;DR: 论文主张AI系统需从统计学习转向精确学习，以实现可靠的演绎推理。


<details>
  <summary>Details</summary>
Motivation: 当前AI系统在演绎推理任务中表现不佳，无法实现通用人工智能。

Method: 提出从统计学习范式转向精确学习范式，要求所有输入上的正确性。

Result: 精确学习是实现可靠演绎推理的必要条件。

Conclusion: AI研究应以精确学习为目标，指导算法设计。

Abstract: Sound deductive reasoning -- the ability to derive new knowledge from
existing facts and rules -- is an indisputably desirable aspect of general
intelligence. Despite the major advances of AI systems in areas such as math
and science, especially since the introduction of transformer architectures, it
is well-documented that even the most advanced frontier systems regularly and
consistently falter on easily-solvable deductive reasoning tasks. Hence, these
systems are unfit to fulfill the dream of achieving artificial general
intelligence capable of sound deductive reasoning. We argue that their unsound
behavior is a consequence of the statistical learning approach powering their
development. To overcome this, we contend that to achieve reliable deductive
reasoning in learning-based AI systems, researchers must fundamentally shift
from optimizing for statistical performance against distributions on reasoning
problems and algorithmic tasks to embracing the more ambitious exact learning
paradigm, which demands correctness on all inputs. We argue that exact learning
is both essential and possible, and that this ambitious objective should guide
algorithm design.

</details>


### [101] [Performance of LLMs on Stochastic Modeling Operations Research Problems: From Theory to Practice](https://arxiv.org/abs/2506.23924)
*Akshit Kumar,Tianyi Peng,Yuhang Wu,Assaf Zeevi*

Main category: cs.AI

TL;DR: 论文探讨了大型语言模型（LLMs）在解决运筹学（OR）中随机建模问题的能力，发现其在课堂和实际场景中表现与人类专家相当。


<details>
  <summary>Details</summary>
Motivation: 尽管LLMs在多个领域展现出专家级能力，但其在运筹学中解决随机建模问题的能力尚未充分探索。

Method: 通过手动收集研究生作业和博士资格考试题目，并利用开源库SimOpt测试LLMs在不确定性下的决策能力。

Result: LLMs在解决随机建模问题时表现与人类专家相当，但仍需进一步工作以实现可靠自动化。

Conclusion: LLMs有潜力构建辅助运筹学研究的AI代理，通过自动化提升OR的实际影响。

Abstract: Large language models (LLMs) have exhibited expert-level capabilities across
various domains. However, their abilities to solve problems in Operations
Research (OR) -- the analysis and optimization of mathematical models derived
from real-world problems or their verbal descriptions -- remain underexplored.
In this work, we take a first step toward evaluating LLMs' abilities to solve
stochastic modeling problems, a core class of OR problems characterized by
uncertainty and typically involving tools from probability, statistics, and
stochastic processes. We manually procure a representative set of
graduate-level homework and doctoral qualification-exam problems and test LLMs'
abilities to solve them. We further leverage SimOpt, an open-source library of
simulation-optimization problems and solvers, to investigate LLMs' abilities to
make real-world decisions under uncertainty. Our results show that, though a
nontrivial amount of work is still needed to reliably automate the stochastic
modeling pipeline in reality, state-of-the-art LLMs demonstrate proficiency on
par with human experts in both classroom and practical settings. These findings
highlight the potential of building AI agents that assist OR researchers and
amplify the real-world impact of OR through automation.

</details>


### [102] [Industrial brain: a human-like autonomous neuro-symbolic cognitive decision-making system](https://arxiv.org/abs/2506.23926)
*Junping Wang,Bicheng Wang,Yibo Xuea,Yuan Xie*

Main category: cs.AI

TL;DR: 提出了一种名为“工业大脑”的框架，结合高阶神经网络和符号推理，用于预测和规划工业链的弹性，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 工业链的弹性测量在科学管理和工程应用中至关重要，但现有深度学习方法在复杂数据下泛化能力不足。

Method: 结合高阶活动驱动神经网络和CT-OODA符号推理，直接从观测数据中自主规划弹性。

Result: 工业大脑在弹性预测和规划上显著优于GoT、OlaGPT和谱降维方法，准确率提升达10.8%和11.03%。

Conclusion: 工业大脑填补了工业链弹性预测和规划的重要空白，且对未见过的拓扑和动态具有鲁棒性。

Abstract: Resilience non-equilibrium measurement, the ability to maintain fundamental
functionality amidst failures and errors, is crucial for scientific management
and engineering applications of industrial chain. The problem is particularly
challenging when the number or types of multiple co-evolution of resilience
(for example, randomly placed) are extremely chaos. Existing end-to-end deep
learning ordinarily do not generalize well to unseen full-feld reconstruction
of spatiotemporal co-evolution structure, and predict resilience of network
topology, especially in multiple chaos data regimes typically seen in
real-world applications. To address this challenge, here we propose industrial
brain, a human-like autonomous cognitive decision-making and planning framework
integrating higher-order activity-driven neuro network and CT-OODA symbolic
reasoning to autonomous plan resilience directly from observational data of
global variable. The industrial brain not only understands and model structure
of node activity dynamics and network co-evolution topology without simplifying
assumptions, and reveal the underlying laws hidden behind complex networks, but
also enabling accurate resilience prediction, inference, and planning.
Experimental results show that industrial brain significantly outperforms
resilience prediction and planning methods, with an accurate improvement of up
to 10.8\% over GoT and OlaGPT framework and 11.03\% over spectral dimension
reduction. It also generalizes to unseen topologies and dynamics and maintains
robust performance despite observational disturbances. Our findings suggest
that industrial brain addresses an important gap in resilience prediction and
planning for industrial chain.

</details>


### [103] [Harnessing AI Agents to Advance Research on Refugee Child Mental Health](https://arxiv.org/abs/2506.23992)
*Aditya Shrivastava,Komal Gupta,Shraddha Arora*

Main category: cs.AI

TL;DR: 研究提出了一种基于AI的框架，用于处理难民健康数据并分析儿童心理健康，比较了两种RAG模型（Zephyr-7B-beta和DeepSeek R1-7B），发现DeepSeek R1-7B表现更优。


<details>
  <summary>Details</summary>
Motivation: 解决国际难民危机中儿童心理健康问题，通过AI技术帮助政策制定者和人道机构更好地支持难民儿童。

Method: 比较两种RAG模型（Zephyr-7B-beta和DeepSeek R1-7B）在处理难民健康数据时的表现，避免幻觉风险。

Result: DeepSeek R1-7B在答案相关性上表现更优，准确率达0.91。

Conclusion: 研究提供了一种可扩展的策略，结合AI和心理学，为政策制定者和人道机构提供支持难民儿童心理健康的工具。

Abstract: The international refugee crisis deepens, exposing millions of dis placed
children to extreme psychological trauma. This research suggests a com pact,
AI-based framework for processing unstructured refugee health data and
distilling knowledge on child mental health. We compare two Retrieval-Aug
mented Generation (RAG) pipelines, Zephyr-7B-beta and DeepSeek R1-7B, to
determine how well they process challenging humanitarian datasets while avoid
ing hallucination hazards. By combining cutting-edge AI methods with migration
research and child psychology, this study presents a scalable strategy to
assist policymakers, mental health practitioners, and humanitarian agencies to
better assist displaced children and recognize their mental wellbeing. In
total, both the models worked properly but significantly Deepseek R1 is
superior to Zephyr with an accuracy of answer relevance 0.91

</details>


### [104] [Constructing Non-Markovian Decision Process via History Aggregator](https://arxiv.org/abs/2506.24026)
*Yongyi Wang,Wenxin Li*

Main category: cs.AI

TL;DR: 论文提出了一种基于范畴论的方法，用于解决算法决策中的非马尔可夫动态问题，并证明了MDP和NMDP的等价关系。


<details>
  <summary>Details</summary>
Motivation: 现有基准无法全面评估决策算法处理非马尔可夫动态的能力，因此需要一种新方法。

Method: 通过范畴论建立MDP和NMDP的等价关系，并引入HAS控制状态依赖结构。

Result: 方法能有效表示广泛的非马尔可夫动态，为算法评估提供更严谨和灵活的方式。

Conclusion: 该方法为理解和解决非马尔可夫动态提供了新视角，并改进了决策算法的评估框架。

Abstract: In the domain of algorithmic decision-making, non-Markovian dynamics manifest
as a significant impediment, especially for paradigms such as Reinforcement
Learning (RL), thereby exerting far-reaching consequences on the advancement
and effectiveness of the associated systems. Nevertheless, the existing
benchmarks are deficient in comprehensively assessing the capacity of decision
algorithms to handle non-Markovian dynamics. To address this deficiency, we
have devised a generalized methodology grounded in category theory. Notably, we
established the category of Markov Decision Processes (MDP) and the category of
non-Markovian Decision Processes (NMDP), and proved the equivalence
relationship between them. This theoretical foundation provides a novel
perspective for understanding and addressing non-Markovian dynamics. We further
introduced non-Markovianity into decision-making problem settings via the
History Aggregator for State (HAS). With HAS, we can precisely control the
state dependency structure of decision-making problems in the time series. Our
analysis demonstrates the effectiveness of our method in representing a broad
range of non-Markovian dynamics. This approach facilitates a more rigorous and
flexible evaluation of decision algorithms by testing them in problem settings
where non-Markovian dynamics are explicitly constructed.

</details>


### [105] [SPIRAL: Self-Play on Zero-Sum Games Incentivizes Reasoning via Multi-Agent Multi-Turn Reinforcement Learning](https://arxiv.org/abs/2506.24119)
*Bo Liu,Leon Guertler,Simon Yu,Zichen Liu,Penghui Qi,Daniel Balcells,Mickel Liu,Cheston Tan,Weiyan Shi,Min Lin,Wee Sun Lee,Natasha Jaques*

Main category: cs.AI

TL;DR: SPIRAL是一个自博弈框架，通过零和游戏训练语言模型，无需人工监督，生成无限挑战性问题，提升推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习方法依赖人工标注和领域特定奖励工程，SPIRAL旨在通过自博弈消除这些限制，实现自主推理能力开发。

Method: SPIRAL采用在线多轮多智能体强化学习系统，提出角色条件优势估计（RAE）稳定训练，通过零和游戏自博弈生成挑战性问题。

Result: 在Kuhn Poker上训练的模型在数学和通用推理任务上分别提升8.6%和8.4%，多游戏训练进一步强化性能。

Conclusion: 零和游戏能自然开发可迁移的推理能力，为自主推理发展提供了新方向。

Abstract: Recent advances in reinforcement learning have shown that language models can
develop sophisticated reasoning through training on tasks with verifiable
rewards, but these approaches depend on human-curated problem-answer pairs and
domain-specific reward engineering. We introduce SPIRAL, a self-play framework
where models learn by playing multi-turn, zero-sum games against continuously
improving versions of themselves, eliminating the need for human supervision.
Through self-play, SPIRAL generates an infinite curriculum of progressively
challenging problems as models must constantly adapt to stronger opponents. To
enable this self-play training at scale, We implement a fully online,
multi-turn, multi-agent reinforcement learning system for LLMs and propose
role-conditioned advantage estimation (RAE) to stabilize multi-agent training.
Using SPIRAL, self-play on zero-sum games produces reasoning capabilities that
transfer broadly. Training Qwen3-4B-Base on Kuhn Poker alone achieves 8.6%
improvement on math and 8.4% on general reasoning, outperforming SFT on 25,000
expert game trajectories. Analysis reveals that this transfer occurs through
three cognitive patterns: systematic decomposition, expected value calculation,
and case-by-case analysis. Multi-game training (TicTacToe, Kuhn Poker, Simple
Negotiation) further enhances performance as each game develops distinct
reasoning strengths. Applying SPIRAL to a strong reasoning model
(DeepSeek-R1-Distill-Qwen-7B) can still lead to 2.0% average improvement. These
results demonstrate that zero-sum games naturally develop transferable
reasoning capabilities, highlighting a promising direction for autonomous
reasoning development.

</details>
