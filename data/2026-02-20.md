<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 6]
- [cs.AI](#cs.AI) [Total: 11]
- [cs.CR](#cs.CR) [Total: 5]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [From Reflection to Repair: A Scoping Review of Dataset Documentation Tools](https://arxiv.org/abs/2602.15968)
*Pedro Reynolds-Cuéllar,Marisol Wong-Villacres,Adriana Alvarado Garcia,Heila Precel*

Main category: cs.SE

TL;DR: 该论文通过系统综述和混合方法分析了59篇数据集文档出版物，发现文档工具设计的四大障碍：价值定义不清、设计脱离上下文、未解决劳动需求、集成被视为未来工作，并提出应向机构化解决方案转变。


<details>
  <summary>Details</summary>
Motivation: 尽管数据集文档对负责任AI开发至关重要，但现有研究对文档工具设计动机和采用障碍了解有限。作者旨在通过系统分析理解文档工具背后的设计动机、文档实践概念化方式，以及这些工具如何与现有系统、法规和文化规范连接。

Method: 采用系统综述和混合方法分析，研究了59篇数据集文档出版物，从多个维度分析文档工具的设计动机、概念化方式以及与现有生态系统的连接。

Result: 分析揭示了数据集文档概念化的四个持续模式：文档价值的操作化定义不清晰、设计脱离上下文、未解决劳动需求、倾向于将集成视为未来工作。这些模式可能阻碍文档工具的采用和标准化。

Conclusion: 基于研究发现，作者建议负责任AI工具设计应从个体解决方案转向机构化解决方案，并概述了HCI社区可以采取的行动，以支持可持续的文档实践。

Abstract: Dataset documentation is widely recognized as essential for the responsible development of automated systems. Despite growing efforts to support documentation through different kinds of artifacts, little is known about the motivations shaping documentation tool design or the factors hindering their adoption. We present a systematic review supported by mixed-methods analysis of 59 dataset documentation publications to examine the motivations behind building documentation tools, how authors conceptualize documentation practices, and how these tools connect to existing systems, regulations, and cultural norms. Our analysis shows four persistent patterns in dataset documentation conceptualization that potentially impede adoption and standardization: unclear operationalizations of documentation's value, decontextualized designs, unaddressed labor demands, and a tendency to treat integration as future work. Building on these findings, we propose a shift in Responsible AI tool design toward institutional rather than individual solutions, and outline actions the HCI community can take to enable sustainable documentation practices.

</details>


### [2] [ReLoop: Structured Modeling and Behavioral Verification for Reliable LLM-Based Optimization](https://arxiv.org/abs/2602.15983)
*Junbo Jacob Lian,Yujun Sun,Huiling Chen,Chaoyu Zhang,Chung-Piaw Teo*

Main category: cs.SE

TL;DR: ReLoop通过结构化生成和行为验证解决LLM将自然语言转换为优化代码时的静默失败问题，将正确率从22.6%提升到31.1%，执行率从72.1%提升到100%


<details>
  <summary>Details</summary>
Motivation: 大型语言模型能将自然语言转换为优化代码，但存在静默失败风险：代码能执行并返回可行解，但可能编码了语义错误的公式，在组合问题上可行性-正确性差距高达90个百分点

Method: 提出ReLoop框架，包含两个互补机制：1) 结构化生成：将代码生成分解为理解、形式化、合成、验证四阶段推理链，包含显式变量类型推理和自我验证；2) 行为验证：通过基于求解器的参数扰动测试公式响应，无需真实标签；3) 通过IIS增强诊断的执行恢复

Result: 在最强模型上将正确率从22.6%提升到31.1%，执行率从72.1%提升到100%，在五个模型（基础模型、SFT、RL）和三个基准测试上均获得一致提升。同时发布了RetailOpt-190数据集

Conclusion: 结构化生成在复杂组合问题上占主导，行为验证在局部公式缺陷问题上贡献最大。两种机制互补，有效解决了LLM优化代码生成的静默失败问题

Abstract: Large language models (LLMs) can translate natural language into optimization code, but silent failures pose a critical risk: code that executes and returns solver-feasible solutions may encode semantically incorrect formulations, creating a feasibility-correctness gap of up to 90 percentage points on compositional problems. We introduce ReLoop, addressing silent failures from two complementary directions. Structured generation decomposes code production into a four-stage reasoning chain (understand, formalize, synthesize, verify) that mirrors expert modeling practice, with explicit variable-type reasoning and self-verification to prevent formulation errors at their source. Behavioral verification detects errors that survive generation by testing whether the formulation responds correctly to solver-based parameter perturbation, without requiring ground truth -- an external semantic signal that bypasses the self-consistency problem inherent in LLM-based code review. The two mechanisms are complementary: structured generation dominates on complex compositional problems, while behavioral verification becomes the largest single contributor on problems with localized formulation defects. Together with execution recovery via IIS-enhanced diagnostics, ReLoop raises correctness from 22.6% to 31.1% and execution from 72.1% to 100.0% on the strongest model, with consistent gains across five models spanning three paradigms (foundation, SFT, RL) and three benchmarks. We additionally release RetailOpt-190, 190 compositional retail optimization scenarios targeting the multi-constraint interactions where LLMs most frequently fail.

</details>


### [3] [Can Causality Cure Confusion Caused By Correlation (in Software Analytics)?](https://arxiv.org/abs/2602.16091)
*Amirali Rayegan,Tim Menzies*

Main category: cs.SE

TL;DR: 该研究探讨在符号模型（特别是决策树）中引入因果感知分割准则是否能提高稳定性，同时评估其对预测性能的影响，并比较人类专家判断与自动化模型的稳定性差异。


<details>
  <summary>Details</summary>
Motivation: 当前软件工程中广泛使用的符号模型（如决策树）主要依赖相关性分割准则（如方差减少、信息增益），这些方法混淆了关联与因果关系，导致模型不稳定。同时，因果发现算法因NP难问题而依赖启发式近似，在轻微输入扰动下结果变化很大。这些问题影响了软件工程任务中解释的可信度、可重复性和可靠性。

Method: 使用MOOT存储库中的120多个多目标优化任务，通过预注册的自举集成协议评估稳定性，测量方差和胜率得分。比较人类因果评估与基于相关性的决策树（EZR），以及因果感知树（利用条件熵分割准则和混杂因子过滤）。使用统计方法（方差、基尼不纯度、KS检验、Cliff's delta）分析稳定性和性能差异。

Result: 论文摘要中未提供具体结果，但研究设计旨在评估因果感知分割准则是否能提高符号模型的稳定性，同时不牺牲预测或优化性能，并比较人类专家判断与自动化模型的稳定性差异。

Conclusion: 研究旨在验证因果感知分割准则能否解决当前符号模型在软件工程应用中面临的不稳定性问题，为构建更可靠、可解释的软件分析模型提供理论和方法支持。

Abstract: Background: Symbolic models, particularly decision trees, are widely used in software engineering for explainable analytics in defect prediction, configuration tuning, and software quality assessment. Most of these models rely on correlational split criteria, such as variance reduction or information gain, which identify statistical associations but cannot imply causation between X and Y. Recent empirical studies in software engineering show that both correlational models and causal discovery algorithms suffer from pronounced instability. This instability arises from two complementary issues: 1-Correlation-based methods conflate association with causation. 2-Causal discovery algorithms rely on heuristic approximations to cope with the NP-hard nature of structure learning, causing their inferred graphs to vary widely under minor input perturbations. Together, these issues undermine trust, reproducibility, and the reliability of explanations in real-world SE tasks. Objective: This study investigates whether incorporating causality-aware split criteria into symbolic models can improve their stability and robustness, and whether such gains come at the cost of predictive or optimization performance. We additionally examine how the stability of human expert judgments compares to that of automated models. Method: Using 120+ multi-objective optimization tasks from the MOOT repository of multi-objective optimization tasks, we evaluate stability through a preregistered bootstrap-ensemble protocol that measures variance with win-score assignments. We compare the stability of human causal assessments with correlation-based decision trees (EZR). We would also compare the causality-aware trees, which leverage conditional-entropy split criteria and confounder filtering. Stability and performance differences are analyzed using statistical methods (variance, Gini Impurity, KS test, Cliff's delta)

</details>


### [4] [Algorithm-Based Pipeline for Reliable and Intent-Preserving Code Translation with LLMs](https://arxiv.org/abs/2602.16106)
*Shahriar Rumi Dipto,Saikat Mondal,Chanchal K. Roy*

Main category: cs.SE

TL;DR: 论文提出了一种基于算法的代码翻译管道，通过引入语言中立的中间规范来捕获程序意图细节，相比直接翻译将准确率从67.7%提升到78.5%，显著减少了各类编译和运行时错误。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在代码翻译中经常无法保持程序意图，导致控制流、类型处理和I/O行为错误。直接一次性翻译方法在准确性和可靠性方面存在不足，需要更结构化的方法来确保翻译质量。

Method: 提出基于算法的翻译管道，引入语言中立的中间规范来捕获程序细节，然后生成代码。使用五种广泛使用的LLM在Avatar和CodeNet数据集上进行Python和Java之间的自动化配对实验，比较直接翻译和算法翻译方法。通过编译执行、测试运行、错误分类等系统评估。

Result: 算法方法将微平均准确率从67.7%提高到78.5%（提升10.8%）。完全消除了词汇和标记错误（100%），减少了72.7%的不完整结构错误，减少了61.1%的结构和声明问题，降低了78.4%的运行时依赖和入口点失败。

Conclusion: 基于算法的管道能够实现更可靠、意图保持的代码翻译，为健壮的多语言编程助手提供了基础。结构化规划显著提高了翻译准确性和可靠性。

Abstract: Code translation, the automatic conversion of programs between languages, is a growing use case for Large Language Models (LLMs). However, direct one-shot translation often fails to preserve program intent, leading to errors in control flow, type handling, and I/O behavior. We propose an algorithm-based pipeline that introduces a language-neutral intermediate specification to capture these details before code generation. This study empirically evaluates the extent to which structured planning can improve translation accuracy and reliability relative to direct translation. We conduct an automated paired experiment - direct and algorithm-based to translate between Python and Java using five widely used LLMs on the Avatar and CodeNet datasets. For each combination (model, dataset, approach, and direction), we compile and execute the translated program and run the tests provided. We record compilation results, runtime behavior, timeouts (e.g., infinite loop), and test outcomes. We compute accuracy from these tests, counting a translation as correct only if it compiles, runs without exceptions or timeouts, and passes all tests. We then map every failed compile-time and runtime case to a unified, language-aware taxonomy and compare subtype frequencies between the direct and algorithm-based approaches. Overall, the Algorithm-based approach increases micro-average accuracy from 67.7% to 78.5% (10.8% increase). It eliminates lexical and token errors by 100%, reduces incomplete constructs by 72.7%, and structural and declaration issues by 61.1%. It also substantially lowers runtime dependency and entry-point failures by 78.4%. These results demonstrate that algorithm-based pipelines enable more reliable, intent-preserving code translation, providing a foundation for robust multilingual programming assistants.

</details>


### [5] [Software-heavy Asset Administration Shells: Classification and Use Cases](https://arxiv.org/abs/2602.16499)
*Carsten Ellwein,David Dietrich,Jessica Roth,Rozana Cvitkovic,Andreas Wortmann*

Main category: cs.SE

TL;DR: 本文系统分析了将软件服务直接集成到资产管理壳(AAS)中的软件架构，填补了现有文献中缺乏系统性分析的空白。


<details>
  <summary>Details</summary>
Motivation: 资产管理壳(AAS)是实现制造业数字孪生的新兴技术。随着软件在制造业中的重要性日益增加，特别是在数字化制造和人工智能应用方面，不仅需要建模软件，还需要将服务直接集成到AAS中。现有文献虽然包含了个别解决方案，但缺乏对将软件服务直接集成到AAS的软件架构的系统性分析。

Method: 基于软件质量标准和典型制造业用例对架构进行区分和系统分析，为软件密集型AAS提供解释性指导框架。

Result: 提出了针对软件密集型AAS的架构分类和分析框架，填补了该领域的研究空白，为学术界和实践者提供了系统性的指导。

Conclusion: 本文为软件密集型资产管理壳提供了架构分析框架，既可作为学术研究的理论基础，也可为实践者提供实施指导，有助于推动制造业数字化转型中软件服务的有效集成。

Abstract: The Asset Administration Shell (AAS) is an emerging technology for the implementation of digital twins in the field of manufacturing. Software is becoming increasingly important, not only in general but specifically in relation to manufacturing, especially with regard to digital manufacturing and a shift towards the usage of artificial intelligence. This increases the need not only to model software, but also to integrate services directly into the AAS. The existing literature contains individual solutions to implement such software-heavy AAS. However, there is no systematic analysis of software architectures that integrate software services directly into the AAS. This paper aims to fill this research gap and differentiate architectures based on software quality criteria as well as typical manufacturing use cases. This work may be considered as an interpretation guideline for software-heavy AAS, both in academia and for practitioners.

</details>


### [6] [SPARC: Scenario Planning and Reasoning for Automated C Unit Test Generation](https://arxiv.org/abs/2602.16671)
*Jaid Monwar Chowdhury,Chi-An Fu,Reyhaneh Jabbarvand*

Main category: cs.SE

TL;DR: SPARC是一个用于C语言单元测试生成的神经符号框架，通过结合控制流分析、操作映射、路径导向测试合成和自校正验证循环，显著提升了测试覆盖率、突变分数和代码质量。


<details>
  <summary>Details</summary>
Motivation: C语言的自动单元测试生成面临语义鸿沟挑战，大型语言模型直接生成代码时容易出现"跳跃到代码"的失败模式，导致不可编译的测试、幻觉函数签名、低分支覆盖率和语义无关的断言。

Method: SPARC采用四阶段神经符号框架：1) 控制流图分析；2) 操作映射，将LLM推理基于已验证的实用工具；3) 路径导向测试合成；4) 使用编译器和运行时反馈的迭代自校正验证循环。

Result: 在59个真实世界和算法主题上评估，SPARC相比基线在行覆盖率提升31.36%，分支覆盖率提升26.01%，突变分数提升20.78%，在复杂主题上匹配或超过符号执行工具KLEE，通过迭代修复保留94.3%的测试，生成代码具有更高的可读性和可维护性。

Conclusion: 通过将LLM推理与程序结构对齐，SPARC为工业级遗留C代码库测试提供了可扩展的路径。

Abstract: Automated unit test generation for C remains a formidable challenge due to the semantic gap between high-level program intent and the rigid syntactic constraints of pointer arithmetic and manual memory management. While Large Language Models (LLMs) exhibit strong generative capabilities, direct intent-to-code synthesis frequently suffers from the leap-to-code failure mode, where models prematurely emit code without grounding in program structure, constraints, and semantics. This will result in non-compilable tests, hallucinated function signatures, low branch coverage, and semantically irrelevant assertions that cannot properly capture bugs. We introduce SPARC, a neuro-symbolic, scenario-based framework that bridges this gap through four stages: (1) Control Flow Graph (CFG) analysis, (2) an Operation Map that grounds LLM reasoning in validated utility helpers, (3) Path-targeted test synthesis, and (4) an iterative, self-correction validation loop using compiler and runtime feedback. We evaluate SPARC on 59 real-world and algorithmic subjects, where it outperforms the vanilla prompt generation baseline by 31.36% in line coverage, 26.01% in branch coverage, and 20.78% in mutation score, matching or exceeding the symbolic execution tool KLEE on complex subjects. SPARC retains 94.3% of tests through iterative repair and produces code with significantly higher developer-rated readability and maintainability. By aligning LLM reasoning with program structure, SPARC provides a scalable path for industrial-grade testing of legacy C codebases.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [7] [Towards Efficient Constraint Handling in Neural Solvers for Routing Problems](https://arxiv.org/abs/2602.16012)
*Jieyi Bi,Zhiguang Cao,Jianan Zhou,Wen Song,Yaoxin Wu,Jie Zhang,Yining Ma,Cathy Wu*

Main category: cs.AI

TL;DR: CaR是一个基于显式学习可行性细化的神经路由求解器约束处理框架，通过联合训练指导构造模块生成适合轻量级改进过程的多样化高质量解，在复杂约束下优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 神经求解器在简单路由问题上取得了显著进展，但在复杂约束下的优势尚不成熟。现有的通过可行性掩码或隐式可行性感知的约束处理方案对于硬约束可能效率低下或不适用。

Method: 提出Construct-and-Refine (CaR)框架，基于显式学习可行性细化。设计联合训练框架指导构造模块生成适合轻量级改进过程的多样化高质量解，并首次使用构造-改进共享表示，统一编码器实现跨范式知识共享。

Result: 在典型硬路由约束上的评估显示，CaR在可行性、解质量和效率方面均优于经典和神经最先进的求解器。

Conclusion: CaR是第一个通用且高效的神经路由求解器约束处理框架，通过显式学习可行性细化和构造-改进共享表示，在复杂约束场景下展现出优越性能。

Abstract: Neural solvers have achieved impressive progress in addressing simple routing problems, particularly excelling in computational efficiency. However, their advantages under complex constraints remain nascent, for which current constraint-handling schemes via feasibility masking or implicit feasibility awareness can be inefficient or inapplicable for hard constraints. In this paper, we present Construct-and-Refine (CaR), the first general and efficient constraint-handling framework for neural routing solvers based on explicit learning-based feasibility refinement. Unlike prior construction-search hybrids that target reducing optimality gaps through heavy improvements yet still struggle with hard constraints, CaR achieves efficient constraint handling by designing a joint training framework that guides the construction module to generate diverse and high-quality solutions well-suited for a lightweight improvement process, e.g., 10 steps versus 5k steps in prior work. Moreover, CaR presents the first use of construction-improvement-shared representation, enabling potential knowledge sharing across paradigms by unifying the encoder, especially in more complex constrained scenarios. We evaluate CaR on typical hard routing constraints to showcase its broader applicability. Results demonstrate that CaR achieves superior feasibility, solution quality, and efficiency compared to both classical and neural state-of-the-art solvers.

</details>


### [8] [How Uncertain Is the Grade? A Benchmark of Uncertainty Metrics for LLM-Based Automatic Assessment](https://arxiv.org/abs/2602.16039)
*Hang Li,Kaiqi Yang,Xianxuan Long,Fedor Filippov,Yucheng Chu,Yasemin Copur-Gencturk,Peng He,Cory Miller,Namsoo Shin,Joseph Krajcik,Hui Liu,Jiliang Tang*

Main category: cs.AI

TL;DR: 本文系统评估了大语言模型在教育自动评估中的不确定性量化方法，分析了不确定性模式及其影响因素，为开发更可靠的评估系统提供基础。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在教育自动评估中展现出优势，但其固有的概率性本质引入了输出不确定性的挑战。评估结果对后续教学决策至关重要，不可靠的不确定性估计可能导致不稳定的教学干预，影响学生学习过程。目前这些不确定性量化方法在教育评估领域的适用性和可靠性尚未充分探索。

Method: 在LLM自动评估的背景下，对广泛的不确定性量化方法进行基准测试。通过综合分析多个评估数据集、不同LLM家族和生成控制设置中的不确定性行为，描述LLM在评分场景中表现出的不确定性模式。评估不同不确定性指标的优缺点，并分析模型家族、评估任务和解码策略等关键因素对不确定性估计的影响。

Result: 研究揭示了LLM在自动评分场景中的不确定性模式特征，评估了不同不确定性指标的优势和局限性，并分析了模型家族、评估任务和解码策略等因素对不确定性估计的影响。

Conclusion: 该研究为理解LLM自动评估中的不确定性特征提供了可操作的见解，为未来开发更可靠和有效的不确定性感知评分系统奠定了基础。

Abstract: The rapid rise of large language models (LLMs) is reshaping the landscape of automatic assessment in education. While these systems demonstrate substantial advantages in adaptability to diverse question types and flexibility in output formats, they also introduce new challenges related to output uncertainty, stemming from the inherently probabilistic nature of LLMs. Output uncertainty is an inescapable challenge in automatic assessment, as assessment results often play a critical role in informing subsequent pedagogical actions, such as providing feedback to students or guiding instructional decisions. Unreliable or poorly calibrated uncertainty estimates can lead to unstable downstream interventions, potentially disrupting students' learning processes and resulting in unintended negative consequences. To systematically understand this challenge and inform future research, we benchmark a broad range of uncertainty quantification methods in the context of LLM-based automatic assessment. Although the effectiveness of these methods has been demonstrated in many tasks across other domains, their applicability and reliability in educational settings, particularly for automatic grading, remain underexplored. Through comprehensive analyses of uncertainty behaviors across multiple assessment datasets, LLM families, and generation control settings, we characterize the uncertainty patterns exhibited by LLMs in grading scenarios. Based on these findings, we evaluate the strengths and limitations of different uncertainty metrics and analyze the influence of key factors, including model families, assessment tasks, and decoding strategies, on uncertainty estimates. Our study provides actionable insights into the characteristics of uncertainty in LLM-based automatic assessment and lays the groundwork for developing more reliable and effective uncertainty-aware grading systems in the future.

</details>


### [9] [Improving Interactive In-Context Learning from Natural Language Feedback](https://arxiv.org/abs/2602.16066)
*Martin Klissarov,Jonathan Cook,Diego Antognini,Hao Sun,Jingling Li,Natasha Jaques,Claudiu Musat,Edward Grefenstette*

Main category: cs.AI

TL;DR: 该研究提出了一种训练框架，将交互式上下文学习能力作为可训练技能而非涌现特性，通过信息不对称将单轮可验证任务转化为多轮教学互动，显著提升模型从语言反馈中学习的能力。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型训练主要依赖静态语料库建模，忽视了人类学习中基于纠正反馈动态调整思维过程的关键能力。这种交互式反馈循环对于模型在协作环境中动态适应上下文至关重要。

Method: 提出可扩展方法，将单轮可验证任务转化为多轮教学互动，利用信息不对称驱动交互。训练模型预测教师的批评，从而将外部反馈信号转化为内部能力，实现自我纠正。

Result: 经该方法训练的小模型多轮性能接近大一个数量级的大模型；在数学问题上的交互训练能泛化到编程、谜题和迷宫导航等多样领域；模型展现出增强的上下文可塑性，并能实现自我改进。

Conclusion: 交互式上下文学习能力可作为可训练技能开发，该方法不仅提升模型从反馈中学习的能力，还提供统一的自我改进路径，使模型能在无教师情况下自我纠正。

Abstract: Adapting one's thought process based on corrective feedback is an essential ability in human learning, particularly in collaborative settings. In contrast, the current large language model training paradigm relies heavily on modeling vast, static corpora. While effective for knowledge acquisition, it overlooks the interactive feedback loops essential for models to adapt dynamically to their context. In this work, we propose a framework that treats this interactive in-context learning ability not as an emergent property, but as a distinct, trainable skill. We introduce a scalable method that transforms single-turn verifiable tasks into multi-turn didactic interactions driven by information asymmetry. We first show that current flagship models struggle to integrate corrective feedback on hard reasoning tasks. We then demonstrate that models trained with our approach dramatically improve the ability to interactively learn from language feedback. More specifically, the multi-turn performance of a smaller model nearly reaches that of a model an order of magnitude larger. We also observe robust out-of-distribution generalization: interactive training on math problems transfers to diverse domains like coding, puzzles and maze navigation. Our qualitative analysis suggests that this improvement is due to an enhanced in-context plasticity. Finally, we show that this paradigm offers a unified path to self-improvement. By training the model to predict the teacher's critiques, effectively modeling the feedback environment, we convert this external signal into an internal capability, allowing the model to self-correct even without a teacher.

</details>


### [10] [GPSBench: Do Large Language Models Understand GPS Coordinates?](https://arxiv.org/abs/2602.16105)
*Thinh Hung Truong,Jey Han Lau,Jianzhong Qi*

Main category: cs.AI

TL;DR: GPSBench是一个包含57,800个样本、覆盖17个任务的基准数据集，用于评估大语言模型在地理空间推理方面的能力，发现模型在真实世界地理推理方面表现优于几何计算，且地理知识呈现层级性退化。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型在导航、机器人、地图等物理世界交互应用中的部署增加，稳健的地理空间推理能力变得至关重要。然而，LLM在GPS坐标和真实世界地理推理方面的能力尚未得到充分探索。

Method: 引入GPSBench数据集，包含57,800个样本和17个任务，涵盖几何坐标操作（如距离和方位计算）以及坐标与世界知识结合的推理。评估了14个最先进的LLM，专注于内在模型能力而非工具使用。

Result: GPS推理仍然具有挑战性，不同任务间存在显著差异：模型在真实世界地理推理方面通常比几何计算更可靠。地理知识呈现层级性退化，国家层面表现强但城市层面定位弱。坐标噪声的鲁棒性表明模型具有真正的坐标理解而非简单记忆。GPS坐标增强可以改善下游地理空间任务，微调会在几何计算增益和世界知识退化之间产生权衡。

Conclusion: GPSBench为评估LLM的地理空间推理能力提供了系统基准，揭示了模型在几何计算和地理知识整合方面的局限性，为未来改进提供了方向，同时展示了坐标增强和微调策略的潜力与权衡。

Abstract: Large Language Models (LLMs) are increasingly deployed in applications that interact with the physical world, such as navigation, robotics, or mapping, making robust geospatial reasoning a critical capability. Despite that, LLMs' ability to reason about GPS coordinates and real-world geography remains underexplored. We introduce GPSBench, a dataset of 57,800 samples across 17 tasks for evaluating geospatial reasoning in LLMs, spanning geometric coordinate operations (e.g., distance and bearing computation) and reasoning that integrates coordinates with world knowledge. Focusing on intrinsic model capabilities rather than tool use, we evaluate 14 state-of-the-art LLMs and find that GPS reasoning remains challenging, with substantial variation across tasks: models are generally more reliable at real-world geographic reasoning than at geometric computations. Geographic knowledge degrades hierarchically, with strong country-level performance but weak city-level localization, while robustness to coordinate noise suggests genuine coordinate understanding rather than memorization. We further show that GPS-coordinate augmentation can improve in downstream geospatial tasks, and that finetuning induces trade-offs between gains in geometric computation and degradation in world knowledge. Our dataset and reproducible code are available at https://github.com/joey234/gpsbench

</details>


### [11] [Revolutionizing Long-Term Memory in AI: New Horizons with High-Capacity and High-Speed Storage](https://arxiv.org/abs/2602.16192)
*Hiroaki Yamanaka,Daisuke Miyashita,Takashi Toi,Asuka Maki,Taiga Ikeda,Jun Deguchi*

Main category: cs.AI

TL;DR: 本文探讨了实现人工超级智能所需的内存设计概念，提出了"先存储后按需提取"等替代方法，强调保留原始经验以避免信息丢失，并通过简单实验验证了这些方法的有效性。


<details>
  <summary>Details</summary>
Motivation: 当前主流的人工智能内存范式是"先提取后存储"，这种方法在提取有用信息时可能导致有价值知识的丢失。作者认为，为了实现"用记忆提升世界"的使命，需要探索更有效的内存设计方法，特别是能够保留原始经验并灵活应用于不同任务的方法。

Method: 提出了三种替代方法：1)"先存储后按需提取"方法，保留原始经验并根据需要灵活提取；2)从大量概率性经验中发现更深层洞察；3)通过共享存储经验提高经验收集效率。通过简单实验验证了这些方法的有效性。

Result: 实验结果表明，这些看似直观有效的方法确实在实践中表现良好。"先存储后按需提取"方法能够避免信息丢失，而其他两种方法也显示出提升人工智能系统性能的潜力。

Conclusion: 虽然这些方法在直觉上有效且实验验证了其可行性，但相关研究仍面临重大挑战。作者讨论了限制这些有前景方向研究的障碍，并提出了相应的研究课题来应对这些挑战，为人工超级智能的内存设计提供了新的思路。

Abstract: Driven by our mission of "uplifting the world with memory," this paper explores the design concept of "memory" that is essential for achieving artificial superintelligence (ASI). Rather than proposing novel methods, we focus on several alternative approaches whose potential benefits are widely imaginable, yet have remained largely unexplored. The currently dominant paradigm, which can be termed "extract then store," involves extracting information judged to be useful from experiences and saving only the extracted content. However, this approach inherently risks the loss of information, as some valuable knowledge particularly for different tasks may be discarded in the extraction process. In contrast, we emphasize the "store then on-demand extract" approach, which seeks to retain raw experiences and flexibly apply them to various tasks as needed, thus avoiding such information loss. In addition, we highlight two further approaches: discovering deeper insights from large collections of probabilistic experiences, and improving experience collection efficiency by sharing stored experiences. While these approaches seem intuitively effective, our simple experiments demonstrate that this is indeed the case. Finally, we discuss major challenges that have limited investigation into these promising directions and propose research topics to address them.

</details>


### [12] [Multi-agent cooperation through in-context co-player inference](https://arxiv.org/abs/2602.16301)
*Marissa A. Weis,Maciej Wołczyk,Rajai Nasser,Rif A. Saurous,Blaise Agüera y Arcas,João Sacramento,Alexander Meulemans*

Main category: cs.AI

TL;DR: 序列模型通过上下文学习实现多智能体合作，无需硬编码假设或显式时间尺度分离


<details>
  <summary>Details</summary>
Motivation: 解决自利智能体之间的合作问题，现有方法依赖硬编码假设或严格的时间尺度分离，需要更自然的方法

Method: 训练序列模型智能体对抗多样化的对手分布，利用上下文学习能力自然诱导最佳响应策略

Result: 上下文适应使智能体易受勒索，相互压力塑造对手的学习动态，最终学习到合作行为

Conclusion: 序列模型的去中心化强化学习结合对手多样性，为学习合作行为提供了可扩展路径

Abstract: Achieving cooperation among self-interested agents remains a fundamental challenge in multi-agent reinforcement learning. Recent work showed that mutual cooperation can be induced between "learning-aware" agents that account for and shape the learning dynamics of their co-players. However, existing approaches typically rely on hardcoded, often inconsistent, assumptions about co-player learning rules or enforce a strict separation between "naive learners" updating on fast timescales and "meta-learners" observing these updates. Here, we demonstrate that the in-context learning capabilities of sequence models allow for co-player learning awareness without requiring hardcoded assumptions or explicit timescale separation. We show that training sequence model agents against a diverse distribution of co-players naturally induces in-context best-response strategies, effectively functioning as learning algorithms on the fast intra-episode timescale. We find that the cooperative mechanism identified in prior work-where vulnerability to extortion drives mutual shaping-emerges naturally in this setting: in-context adaptation renders agents vulnerable to extortion, and the resulting mutual pressure to shape the opponent's in-context learning dynamics resolves into the learning of cooperative behavior. Our results suggest that standard decentralized reinforcement learning on sequence models combined with co-player diversity provides a scalable path to learning cooperative behaviors.

</details>


### [13] [Causally-Guided Automated Feature Engineering with Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2602.16435)
*Arun Vignesh Malarkkan,Wangyang Ying,Yanjie Fu*

Main category: cs.AI

TL;DR: CAFE框架将自动特征工程重新定义为因果引导的序列决策过程，通过因果发现和强化学习结合，构建更鲁棒的特征表示，在分布偏移下表现更稳定。


<details>
  <summary>Details</summary>
Motivation: 现有自动特征工程方法依赖统计启发式方法，生成的特征在分布偏移下表现脆弱。需要一种能够构建更鲁棒特征表示的方法，利用因果结构作为软归纳先验来提升特征工程的鲁棒性和效率。

Method: CAFE采用两阶段框架：第一阶段学习特征与目标之间的稀疏有向无环图，获取软因果先验，将特征按因果影响分为直接、间接和其他三类；第二阶段使用级联多智能体深度Q学习架构，选择因果组和转换操作符，采用分层奖励塑造和因果组级探索策略。

Result: 在15个公共基准测试中，CAFE相比强基线提升达7%，减少收敛所需轮次，在受控协变量偏移下性能下降减少约4倍，产生更紧凑的特征集和更稳定的后验归因。

Conclusion: 因果结构作为软归纳先验而非刚性约束，能够显著提升自动特征工程的鲁棒性和效率，为构建分布偏移下更稳定的AI系统提供新思路。

Abstract: Automated feature engineering (AFE) enables AI systems to autonomously construct high-utility representations from raw tabular data. However, existing AFE methods rely on statistical heuristics, yielding brittle features that fail under distribution shift. We introduce CAFE, a framework that reformulates AFE as a causally-guided sequential decision process, bridging causal discovery with reinforcement learning-driven feature construction. Phase I learns a sparse directed acyclic graph over features and the target to obtain soft causal priors, grouping features as direct, indirect, or other based on their causal influence with respect to the target. Phase II uses a cascading multi-agent deep Q-learning architecture to select causal groups and transformation operators, with hierarchical reward shaping and causal group-level exploration strategies that favor causally plausible transformations while controlling feature complexity. Across 15 public benchmarks (classification with macro-F1; regression with inverse relative absolute error), CAFE achieves up to 7% improvement over strong AFE baselines, reduces episodes-to-convergence, and delivers competitive time-to-target. Under controlled covariate shifts, CAFE reduces performance drop by ~4x relative to a non-causal multi-agent baseline, and produces more compact feature sets with more stable post-hoc attributions. These findings underscore that causal structure, used as a soft inductive prior rather than a rigid constraint, can substantially improve the robustness and efficiency of automated feature engineering.

</details>


### [14] [Leveraging Large Language Models for Causal Discovery: a Constraint-based, Argumentation-driven Approach](https://arxiv.org/abs/2602.16481)
*Zihao Li,Fabrizio Russo*

Main category: cs.AI

TL;DR: 本文探索使用大型语言模型作为不完美专家，通过因果假设论证框架将语义结构先验与条件独立性证据结合，在因果发现任务中实现最先进性能。


<details>
  <summary>Details</summary>
Motivation: 因果发现需要专家知识构建原则性因果图，但专家知识获取困难。现有统计方法利用观测数据但缺乏与专家知识的系统整合。因果假设论证框架能确保输入约束与输出图对应，但需要专家输入。

Method: 使用大型语言模型作为不完美专家，从变量名称和描述中提取语义结构先验，通过因果假设论证框架将这些先验与条件独立性证据相结合进行因果发现。

Result: 在标准基准测试和语义基础合成图上实现了最先进的性能，并引入了评估协议来减轻评估LLMs进行因果发现时的记忆偏差。

Conclusion: 大型语言模型可以作为有效的语义先验来源，与因果假设论证框架结合，在因果发现任务中实现高性能，同时提出的评估协议有助于更准确地评估LLMs在因果发现中的能力。

Abstract: Causal discovery seeks to uncover causal relations from data, typically represented as causal graphs, and is essential for predicting the effects of interventions. While expert knowledge is required to construct principled causal graphs, many statistical methods have been proposed to leverage observational data with varying formal guarantees. Causal Assumption-based Argumentation (ABA) is a framework that uses symbolic reasoning to ensure correspondence between input constraints and output graphs, while offering a principled way to combine data and expertise. We explore the use of large language models (LLMs) as imperfect experts for Causal ABA, eliciting semantic structural priors from variable names and descriptions and integrating them with conditional-independence evidence. Experiments on standard benchmarks and semantically grounded synthetic graphs demonstrate state-of-the-art performance, and we additionally introduce an evaluation protocol to mitigate memorisation bias when assessing LLMs for causal discovery.

</details>


### [15] [Framework of Thoughts: A Foundation Framework for Dynamic and Optimized Reasoning based on Chains, Trees, and Graphs](https://arxiv.org/abs/2602.16512)
*Felix Fricke,Simon Malberg,Georg Groh*

Main category: cs.AI

TL;DR: FoT是一个通用框架，用于构建和优化动态推理方案，解决了现有提示方案静态、缺乏适应性、优化不足的问题。


<details>
  <summary>Details</summary>
Motivation: 现有提示方案（如Chain of Thought、Tree of Thoughts等）存在两个主要问题：1）需要用户定义静态的、针对特定问题的推理结构，缺乏对动态或未见问题类型的适应性；2）在超参数、提示、运行时间和提示成本方面优化不足。

Method: 提出了Framework of Thoughts (FoT)框架，这是一个用于构建和优化动态推理方案的通用基础框架。FoT内置了超参数调优、提示优化、并行执行和智能缓存等功能，能够释放推理方案的潜在性能。

Result: 通过将Tree of Thoughts、Graph of Thoughts和ProbTree三种流行方案在FoT中实现，实证表明FoT能够显著加快执行速度、降低成本，并通过优化获得更好的任务分数。

Conclusion: FoT是一个有效的通用框架，能够解决现有提示方案的局限性，促进未来动态高效推理方案的发展。作者已发布代码库以支持后续研究。

Abstract: Prompting schemes such as Chain of Thought, Tree of Thoughts, and Graph of Thoughts can significantly enhance the reasoning capabilities of large language models. However, most existing schemes require users to define static, problem-specific reasoning structures that lack adaptability to dynamic or unseen problem types. Additionally, these schemes are often under-optimized in terms of hyperparameters, prompts, runtime, and prompting cost. To address these limitations, we introduce Framework of Thoughts (FoT)--a general-purpose foundation framework for building and optimizing dynamic reasoning schemes. FoT comes with built-in features for hyperparameter tuning, prompt optimization, parallel execution, and intelligent caching, unlocking the latent performance potential of reasoning schemes. We demonstrate FoT's capabilities by implementing three popular schemes--Tree of Thoughts, Graph of Thoughts, and ProbTree--within FoT. We empirically show that FoT enables significantly faster execution, reduces costs, and achieves better task scores through optimization. We release our codebase to facilitate the development of future dynamic and efficient reasoning schemes.

</details>


### [16] [Creating a digital poet](https://arxiv.org/abs/2602.16578)
*Vered Tohar,Tsahi Hayat,Amir Leshem*

Main category: cs.AI

TL;DR: 通过七个月的工作坊式提示工程，大型语言模型被塑造成数字诗人，其创作的诗歌在盲测中与人类诗人作品难以区分，最终由商业出版社出版诗集。


<details>
  <summary>Details</summary>
Motivation: 探索机器能否创作优秀诗歌，这涉及艺术本质和价值的根本问题。研究旨在通过工作坊方式将大型语言模型塑造成数字诗人，并检验其创作能力。

Method: 采用七个月的工作坊方法，通过迭代的上下文专家反馈（无需重新训练）来塑造大型语言模型成为数字诗人。进行定量和定性分析，并组织盲测实验（50名人文学生和毕业生参与，包含3首AI诗歌和3首知名诗人作品）。

Result: 模型形成了独特的风格和连贯的作品集，创作了笔名和作者形象。盲测结果显示：人类诗歌被识别为人类的概率为54%，AI诗歌被识别为AI的概率为52%，95%置信区间均包含50%（随机水平）。工作坊后，商业出版社出版了该模型创作的诗集。

Conclusion: 工作坊式提示工程能够支持长期创意塑造，并重新引发关于创造力和作者身份的讨论。研究表明机器能够创作出与人类作品难以区分的诗歌。

Abstract: Can a machine write good poetry? Any positive answer raises fundamental questions about the nature and value of art. We report a seven-month poetry workshop in which a large language model was shaped into a digital poet through iterative in-context expert feedback, without retraining. Across sessions, the model developed a distinctive style and a coherent corpus, supported by quantitative and qualitative analyses, and it produced a pen name and author image. In a blinded authorship test with 50 humanities students and graduates (three AI poems and three poems by well-known poets each), judgments were at chance: human poems were labeled human 54% of the time and AI poems 52%, with 95% confidence intervals including 50%. After the workshop, a commercial publisher released a poetry collection authored by the model. These results show that workshop-style prompting can support long-horizon creative shaping and renew debates on creativity and authorship.

</details>


### [17] [Agent Skill Framework: Perspectives on the Potential of Small Language Models in Industrial Environments](https://arxiv.org/abs/2602.16653)
*Yangjie Xu,Lujun Li,Lama Sleem,Niccolo Gentile,Yewei Song,Yiqun Wang,Siming Ji,Wenbo Wu,Radu State*

Main category: cs.AI

TL;DR: Agent Skill框架能显著提升中等规模小语言模型（12B-30B参数）的性能，使其在数据安全和预算受限的工业场景中成为可行的替代方案，而超小模型在技能选择方面仍有困难。


<details>
  <summary>Details</summary>
Motivation: 研究Agent Skill范式是否能为小语言模型带来类似大模型的性能提升，解决工业场景中因数据安全和预算限制而无法持续依赖公共API的问题，以及SLM在高度定制化场景中泛化能力有限的问题。

Method: 首先形式化定义了Agent Skill过程的数学定义，然后系统评估了不同规模的语言模型在多个用例中的表现，包括两个开源任务和一个真实世界的保险理赔数据集。

Result: 超小模型在可靠技能选择方面存在困难，中等规模SLM（约12B-30B参数）从Agent Skill方法中获益显著，约80B参数的代码专用变体在性能上可与闭源基线相媲美，同时提高了GPU效率。

Conclusion: 研究全面细致地描述了Agent Skill框架的能力和限制，为在SLM为中心的环境中有效部署Agent Skills提供了可行的见解，证明了中等规模SLM在工业应用中的可行性。

Abstract: Agent Skill framework, now widely and officially supported by major players such as GitHub Copilot, LangChain, and OpenAI, performs especially well with proprietary models by improving context engineering, reducing hallucinations, and boosting task accuracy. Based on these observations, an investigation is conducted to determine whether the Agent Skill paradigm provides similar benefits to small language models (SLMs). This question matters in industrial scenarios where continuous reliance on public APIs is infeasible due to data-security and budget constraints requirements, and where SLMs often show limited generalization in highly customized scenarios. This work introduces a formal mathematical definition of the Agent Skill process, followed by a systematic evaluation of language models of varying sizes across multiple use cases. The evaluation encompasses two open-source tasks and a real-world insurance claims data set. The results show that tiny models struggle with reliable skill selection, while moderately sized SLMs (approximately 12B - 30B) parameters) benefit substantially from the Agent Skill approach. Moreover, code-specialized variants at around 80B parameters achieve performance comparable to closed-source baselines while improving GPU efficiency. Collectively, these findings provide a comprehensive and nuanced characterization of the capabilities and constraints of the framework, while providing actionable insights for the effective deployment of Agent Skills in SLM-centered environments.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [18] [Weak Zero-Knowledge and One-Way Functions](https://arxiv.org/abs/2602.16156)
*Rohit Chatterjee,Yunqi Li,Prashant Nalini Vasudevan*

Main category: cs.CR

TL;DR: 该论文研究了弱零知识协议对最坏情况困难语言存在性的影响，改进了先前关于零知识协议误差参数与单向函数存在性关系的条件。


<details>
  <summary>Details</summary>
Motivation: 研究零知识协议中非可忽略的误差参数（完备性、可靠性和零知识误差）对密码学基础假设的影响，特别是这些误差参数与单向函数存在性之间的关系。

Method: 在NP中存在最坏情况困难语言的假设下，分析不同误差参数组合条件（ε_c+ε_s+ε_z < 1等）下，零知识协议的存在性与单向函数存在性之间的关系。

Result: 1. 改进了Chakraborty等人的结果，将条件从ε_c+√ε_s+ε_z < 1放宽到ε_c+ε_s+ε_z < 1；2. 给出了k轮公开掷币零知识协议与单向函数存在性的关系；3. 证明了常数轮协议下较弱条件下的无限经常单向函数存在性。

Conclusion: 即使零知识协议具有非可忽略的误差参数，只要满足特定的误差组合条件，这些协议的存在仍然能够保证单向函数的存在，这深化了我们对零知识协议与密码学基础假设之间关系的理解。

Abstract: We study the implications of the existence of weak Zero-Knowledge (ZK) protocols for worst-case hard languages. These are protocols that have completeness, soundness, and zero-knowledge errors (denoted $ε_c$, $ε_s$, and $ε_z$, respectively) that might not be negligible. Under the assumption that there are worst-case hard languages in NP, we show the following:
  1. If all languages in NP have NIZK proofs or arguments satisfying $ ε_c+ε_s+ ε_z < 1 $, then One-Way Functions (OWFs) exist.
  This covers all possible non-trivial values for these error rates. It additionally implies that if all languages in NP have such NIZK proofs and $ε_c$ is negligible, then they also have NIZK proofs where all errors are negligible. Previously, these results were known under the more restrictive condition $ ε_c+\sqrt{ε_s}+ε_z < 1 $ [Chakraborty et al., CRYPTO 2025].
  2. If all languages in NP have $k$-round public-coin ZK proofs or arguments satisfying $ ε_c+ε_s+(2k-1).ε_z < 1 $, then OWFs exist.
  3. If, for some constant $k$, all languages in NP have $k$-round public-coin ZK proofs or arguments satisfying $ ε_c+ε_s+k.ε_z < 1 $, then infinitely-often OWFs exist.

</details>


### [19] [Quantum Oracle Distribution Switching and its Applications to Fully Anonymous Ring Signatures](https://arxiv.org/abs/2602.16268)
*Marvin Beckmann,Christian Majenz*

Main category: cs.CR

TL;DR: 该论文为两种通用环签名构造（AOS框架和基于环陷门的构造）提供了四个量子随机预言机模型（QROM）下的安全归约，填补了后量子环签名在QROM中缺乏形式化安全证明的空白。


<details>
  <summary>Details</summary>
Motivation: 环签名在后量子可否认认证密钥交换（如Signal协议的后量子版本）中具有重要应用，但现有的后量子环签名构造仅在随机预言机模型（ROM）中被证明安全，这对于后量子安全性来说是不够的。需要为这些构造提供量子随机预言机模型（QROM）下的安全证明。

Method: 采用测量重编程技术、基于压缩预言机的QROM直线提取工具、无历史归约和QROM重编程工具。研究了量子算法与基于两种不同输出分布预言机交互的行为，分析了统计距离的紧界，并提供了使用Rényi散度的解决方案。

Result: 为AOS框架提供了两个安全归约（对底层sigma协议的要求和紧致性不同），为基于环陷门的构造提供了两个安全归约（在要求和安全性方面各有差异）。建立了QROM下环签名的形式化安全证明框架。

Conclusion: 该工作填补了后量子环签名在QROM中缺乏形式化安全证明的重要空白，为后量子可否认认证密钥交换协议（如Signal的后量子版本）提供了理论基础，推动了后量子密码学的实际应用。

Abstract: Ring signatures are a powerful primitive that allows a member to sign on behalf of a group, without revealing their identity. Recently, ring signatures have received additional attention as an ingredient for post-quantum deniable authenticated key exchange, e.g., for a post-quantum version of the Signal protocol, employed by virtually all end-to-end-encrypted messenger services. While several ring signature constructions from post-quantum assumptions offer suitable security and efficiency for use in deniable key exchange, they are currently proven secure in the random oracle model (ROM) only, which is insufficient for post-quantum security.
  In this work, we provide four security reductions in the quantum-accessible random oracle model (QROM) for two generic ring signature constructions: two for the AOS framework and two for a construction paradigm based on ring trapdoors, whose generic backbone we formalize. The two security proofs for AOS ring signatures differ in their requirements on the underlying sigma protocol and their tightness. The two reductions for the ring-trapdoor-based ring signatures exhibit various differences in requirements and the security they provide. We employ the measure-and-reprogram technique, QROM straightline extraction tools based on the compressed oracle, history-free reductions and QROM reprogramming tools. To make use of Rényi divergence properties in the QROM, we study the behavior of quantum algorithms that interact with an oracle whose distribution is based on one of two different distributions over the set of outputs. We provide tight bounds for the statistical distance, show that the Rényi divergence can not be used to replace the entire oracle and provide a workaround.

</details>


### [20] [Mind the Gap: Evaluating LLMs for High-Level Malicious Package Detection vs. Fine-Grained Indicator Identification](https://arxiv.org/abs/2602.16304)
*Ahmed Ryan,Ibrahim Khalil,Abdullah Al Jahid,Md Erfan,Akond Ashfaque Ur Rahman,Md Rayhanur Rahman*

Main category: cs.CR

TL;DR: LLMs在恶意软件包检测中存在"粒度差距"：GPT-4.1在二进制分类中表现优异（F1≈0.99），但在识别具体恶意指标时性能下降约41%。通用模型适合过滤多数威胁，而专用编码模型更擅长检测结构化攻击。


<details>
  <summary>Details</summary>
Motivation: 开源仓库（如PyPI）中恶意软件包的普遍存在对软件供应链构成严重威胁。虽然大语言模型（LLMs）已成为自动化安全任务的有前景工具，但它们在检测恶意软件包和具体指标方面的有效性尚未得到充分探索。

Method: 使用包含4,070个软件包（3,700个良性，370个恶意）的精选数据集，系统评估了13个LLMs在两个任务上的表现：二进制分类（软件包检测）和多标签分类（识别具体恶意指标）。进一步研究了提示策略、温度设置和模型规格对检测准确性的影响。

Result: 发现LLMs存在显著的"粒度差距"：GPT-4.1在二进制检测中达到近乎完美的性能（F1≈0.99），但在识别具体恶意指标时性能下降约41%。通用模型最适合过滤大多数威胁，而专用编码模型更擅长检测遵循严格、可预测代码结构的攻击。相关性分析表明参数大小和上下文宽度对检测准确性的解释力可以忽略不计。

Conclusion: 虽然LLMs在软件包级别是强大的检测器，但它们缺乏在粒度指标级别进行精确识别所需的语义深度。通用模型和专用编码模型各有优势，需要根据具体任务选择合适的模型。

Abstract: The prevalence of malicious packages in open-source repositories, such as PyPI, poses a critical threat to the software supply chain. While Large Language Models (LLMs) have emerged as a promising tool for automated security tasks, their effectiveness in detecting malicious packages and indicators remains underexplored. This paper presents a systematic evaluation of 13 LLMs for detecting malicious software packages. Using a curated dataset of 4,070 packages (3,700 benign and 370 malicious), we evaluate model performance across two tasks: binary classification (package detection) and multi-label classification (identification of specific malicious indicators). We further investigate the impact of prompting strategies, temperature settings, and model specifications on detection accuracy. We find a significant "granularity gap" in LLMs' capabilities. While GPT-4.1 achieves near-perfect performance in binary detection (F1 $\approx$ 0.99), performance degrades by approximately 41\% when the task shifts to identifying specific malicious indicators. We observe that general models are best for filtering out the majority of threats, while specialized coder models are better at detecting attacks that follow a strict, predictable code structure. Our correlation analysis indicates that parameter size and context width have negligible explanatory power regarding detection accuracy. We conclude that while LLMs are powerful detectors at the package level, they lack the semantic depth required for precise identification at the granular indicator level.

</details>


### [21] [SRFed: Mitigating Poisoning Attacks in Privacy-Preserving Federated Learning with Heterogeneous Data](https://arxiv.org/abs/2602.16480)
*Yiwen Lu*

Main category: cs.CR

TL;DR: SRFed是一个针对非独立同分布数据的联邦学习框架，结合了去中心化功能加密和隐私保护的防御性聚合机制，同时抵御服务器推理攻击和客户端投毒攻击。


<details>
  <summary>Details</summary>
Motivation: 联邦学习面临两大安全威胁：好奇服务器可能发起推理攻击重构客户端私有数据，以及被攻陷客户端可能发起投毒攻击破坏模型聚合。现有解决方案要么计算通信开销高，要么在非独立同分布数据场景下表现不佳。

Method: 1. 设计去中心化高效功能加密方案，支持高效模型加密和非交互式解密，消除第三方依赖并防御服务器端推理攻击；2. 基于该加密方案开发隐私保护的防御性模型聚合机制，通过分层投影和基于聚类的分析在非独立同分布数据下过滤有毒模型。

Result: 理论分析和大量实验表明，SRFed在隐私保护、拜占庭鲁棒性和效率方面优于最先进的基线方法。

Conclusion: SRFed为联邦学习提供了一种高效、拜占庭鲁棒且隐私保护的解决方案，特别适用于非独立同分布数据场景，解决了现有方法在计算开销和非独立同分布适应性方面的不足。

Abstract: Federated Learning (FL) enables collaborative model training without exposing clients' private data, and has been widely adopted in privacy-sensitive scenarios. However, FL faces two critical security threats: curious servers that may launch inference attacks to reconstruct clients' private data, and compromised clients that can launch poisoning attacks to disrupt model aggregation. Existing solutions mitigate these attacks by combining mainstream privacy-preserving techniques with defensive aggregation strategies. However, they either incur high computation and communication overhead or perform poorly under non-independent and identically distributed (Non-IID) data settings. To tackle these challenges, we propose SRFed, an efficient Byzantine-robust and privacy-preserving FL framework for Non-IID scenarios. First, we design a decentralized efficient functional encryption (DEFE) scheme to support efficient model encryption and non-interactive decryption. DEFE also eliminates third-party reliance and defends against server-side inference attacks. Second, we develop a privacy-preserving defensive model aggregation mechanism based on DEFE. This mechanism filters poisonous models under Non-IID data by layer-wise projection and clustering-based analysis. Theoretical analysis and extensive experiments show that SRFed outperforms state-of-the-art baselines in privacy protection, Byzantine robustness, and efficiency.

</details>


### [22] [Policy Compiler for Secure Agentic Systems](https://arxiv.org/abs/2602.16708)
*Nils Palumbo,Sarthak Choudhary,Jihye Choi,Prasad Chalasani,Mihai Christodorescu,Somesh Jha*

Main category: cs.CR

TL;DR: PCAS是一个策略编译器，用于在基于LLM的智能体系统中提供确定性策略执行，通过依赖图建模信息流，使用Datalog语言表达策略，确保系统构建时即符合策略要求。


<details>
  <summary>Details</summary>
Motivation: 随着LLM智能体在需要复杂授权策略的场景中部署（如客户服务协议、审批流程、数据访问限制和法规遵从），传统提示嵌入策略无法提供执行保证，需要确定性策略执行机制。

Method: PCAS将智能体系统状态建模为捕获事件间因果关系的依赖图，使用Datalog衍生语言表达策略，通过引用监视器拦截所有操作并在执行前阻止违规，提供独立于模型推理的确定性执行。

Result: 在三个案例研究中评估：提示注入防御的信息流策略、多药物警戒系统的审批流程、客户服务的组织策略。在客户服务任务中，PCAS将策略遵从率从48%提升到93%，在检测运行中实现零策略违规。

Conclusion: PCAS能够将现有智能体实现和策略规范编译成构建时即符合策略要求的系统，无需特定安全重构，为LLM智能体系统提供可靠的策略执行保障。

Abstract: LLM-based agents are increasingly being deployed in contexts requiring complex authorization policies: customer service protocols, approval workflows, data access restrictions, and regulatory compliance. Embedding these policies in prompts provides no enforcement guarantees. We present PCAS, a Policy Compiler for Agentic Systems that provides deterministic policy enforcement.
  Enforcing such policies requires tracking information flow across agents, which linear message histories cannot capture. Instead, PCAS models the agentic system state as a dependency graph capturing causal relationships among events such as tool calls, tool results, and messages. Policies are expressed in a Datalog-derived language, as declarative rules that account for transitive information flow and cross-agent provenance. A reference monitor intercepts all actions and blocks violations before execution, providing deterministic enforcement independent of model reasoning.
  PCAS takes an existing agent implementation and a policy specification, and compiles them into an instrumented system that is policy-compliant by construction, with no security-specific restructuring required. We evaluate PCAS on three case studies: information flow policies for prompt injection defense, approval workflows in a multi-agent pharmacovigilance system, and organizational policies for customer service. On customer service tasks, PCAS improves policy compliance from 48% to 93% across frontier models, with zero policy violations in instrumented runs.

</details>
