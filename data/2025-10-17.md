<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 18]
- [cs.CR](#cs.CR) [Total: 24]
- [cs.AI](#cs.AI) [Total: 52]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [From Craft to Constitution: A Governance-First Paradigm for Principled Agent Engineering](https://arxiv.org/abs/2510.13857)
*Qiang Xu,Xiangyu Wen,Changran Xu,Zeju Li,Jianyuan Zhong*

Main category: cs.SE

TL;DR: 本文提出ArbiterOS架构，通过治理优先的范式解决LLM代理在关键任务应用中的脆弱性和不可预测性问题。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型虽然开启了'代理时代'，但从原型到生产的过渡受到'工艺危机'的阻碍，导致代理在关键任务应用中脆弱、不可预测且不可信。这种危机源于试图用传统软件工程的确定性思维来命令本质上是概率性的处理器。

Method: 引入治理优先的代理工程原则，体现在名为ArbiterOS的正式架构中。

Result: 提出了解决代理工程危机的理论框架和架构方案。

Conclusion: 需要从传统软件工程的确定性范式转向治理优先的代理工程范式，以构建可信赖的自主系统。

Abstract: The advent of powerful Large Language Models (LLMs) has ushered in an ``Age
of the Agent,'' enabling autonomous systems to tackle complex goals. However,
the transition from prototype to production is hindered by a pervasive ``crisis
of craft,'' resulting in agents that are brittle, unpredictable, and ultimately
untrustworthy in mission-critical applications. This paper argues this crisis
stems from a fundamental paradigm mismatch -- attempting to command inherently
probabilistic processors with the deterministic mental models of traditional
software engineering. To solve this crisis, we introduce a governance-first
paradigm for principled agent engineering, embodied in a formal architecture we
call ArbiterOS.

</details>


### [2] [Benchmarking Correctness and Security in Multi-Turn Code Generation](https://arxiv.org/abs/2510.13859)
*Ruchit Rawal,Jeffrey Yang Fan Chiang,Chihao Shen,Jeffery Siyuan Tian,Aastha Mahajan,Tom Goldstein,Yizheng Chen*

Main category: cs.SE

TL;DR: MT-Sec是首个系统评估多轮编码场景中正确性和安全性的基准测试，通过将单轮任务转换为语义对齐的多轮交互序列来构建，发现从单轮到多轮设置中"正确且安全"的输出下降了20-27%。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试主要评估单轮任务的代码正确性和安全性，无法反映真实世界开发的迭代性质，需要能评估多轮编码场景中正确性和安全性的基准。

Method: 使用合成数据管道将现有单轮任务转换为语义对齐的多轮交互序列，重用原始测试套件，同时建模真实世界编码过程的复杂性。评估了32个开源和闭源模型以及三种智能体框架。

Result: 从单轮到多轮设置中，"正确且安全"的输出下降了20-27%；在多轮代码差异生成任务中表现更差，功能不正确和不安全输出的比例增加；智能体框架在单轮代码生成中表现良好，但在多轮评估中效果不佳。

Conclusion: 需要能够联合评估多轮、真实世界编码工作流中正确性和安全性的基准测试，当前模型在多轮场景中的表现明显下降。

Abstract: AI coding assistants powered by large language models (LLMs) have transformed
software development, significantly boosting productivity. While existing
benchmarks evaluate the correctness and security of LLM-generated code, they
are typically limited to single-turn tasks that do not reflect the iterative
nature of real-world development. We introduce MT-Sec, the first benchmark to
systematically evaluate both correctness and security in multi-turn coding
scenarios. We construct this using a synthetic data pipeline that transforms
existing single-turn tasks into semantically aligned multi-turn interaction
sequences, allowing reuse of original test suites while modeling the complexity
of real-world coding processes. We evaluate 32 open- and closed-source models,
and three agent-scaffolding on MT-Sec and observe a consistent 20-27% drop in
"correct and secure" outputs from single-turn to multi-turn settings -- even
among state-of-the-art models. Beyond full-program generation, we also evaluate
models on multi-turn code-diff generation -- an unexplored yet practically
relevant setting -- and find that models perform worse here, with increased
rates of functionally incorrect and insecure outputs. Finally, we find that
while agent scaffoldings boost single-turn code generation performance, they
are not quite as effective in multi-turn evaluations. Together, these findings
highlight the need for benchmarks that jointly evaluate correctness and
security in multi-turn, real-world coding workflows.

</details>


### [3] [A11YN: aligning LLMs for accessible web UI code generation](https://arxiv.org/abs/2510.13914)
*Janghan Yoon,Jaegwan Cho,Junhyeok Kim,Jiwan Chung,Jaehyun Jeon,Youngjae Yu*

Main category: cs.SE

TL;DR: A11yn是一种新方法，通过优化基于WCAG违规的奖励函数，使代码生成LLM能够可靠地生成符合可访问性标准的网页界面，显著降低了不可访问率。


<details>
  <summary>Details</summary>
Motivation: 现有LLM在生成网页界面时经常复制训练数据中的可访问性缺陷，导致生成的界面排斥有不同需求和背景的用户。

Method: A11yn优化了一个新颖的奖励函数，该函数根据可访问性测试引擎识别的违规严重程度来惩罚WCAG违规。为此构建了UIReq-6.8K数据集用于训练，并引入了RealUIReq-300基准用于评估。

Result: A11yn显著优于强基线方法，将不可访问率比基础模型降低了60%，同时保持了生成UI的语义保真度和视觉质量。

Conclusion: 研究表明可访问性可以在LLM中被系统优化，证明了为可访问性对齐代码生成的可行性。

Abstract: Large language models (LLMs) have recently demonstrated strong capabilities
in generating functional and aesthetic web interfaces directly from
instructions. However, these models often replicate accessibility flaws from
their training data, resulting in interfaces that exclude users with diverse
needs and contexts. To address this gap, we introduce A11yn, the first method
that aligns code-generating LLMs to reliably produce accessibility-compliant
web UIs. A11yn optimizes a novel reward function that penalizes violations of
the Web Content Accessibility Guidelines (WCAG), with penalties scaled to the
severity of each violation as identified by an accessibility testing engine. To
support training, we construct UIReq-6.8K, a dataset of 6,800 diverse
instructions for web UI generation. For evaluation, we introduce RealUIReq-300,
a benchmark of 300 real-world web UI requests grounded and manually curated
from public web pages, spanning a broad range of use cases. Empirical results
show that A11yn significantly outperforms strong baselines, lowering the
Inaccessibility Rate by 60% over the base model while preserving semantic
fidelity and visual quality of generated UIs. These findings demonstrate that
accessibility can be systematically optimized within LLMs, showing the
feasibility of aligning code generation for accessibility.

</details>


### [4] [Signature in Code Backdoor Detection, how far are we?](https://arxiv.org/abs/2510.13992)
*Quoc Hung Le,Thanh Le-Cong,Bach Le,Bowen Xu*

Main category: cs.SE

TL;DR: 重新评估基于谱签名防御方法在代码模型后门攻击检测中的适用性，发现传统设置往往不是最优的，并提出了新的代理指标来更准确评估防御性能。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型在软件开发中的广泛应用，后门攻击成为重大威胁。虽然谱签名方法在神经网络后门检测中已有研究，但最近研究表明这些方法对代码模型可能不够有效。

Method: 系统评估谱签名防御在不同攻击场景和防御配置下的有效性，分析关键因素的不同设置对性能的影响。

Result: 发现代码后门检测中广泛使用的谱签名设置通常不是最优的，并发现了一个新的代理指标，可以在防御后无需模型重新训练的情况下更准确地估计谱签名的实际性能。

Conclusion: 谱签名防御在代码模型后门检测中需要更优化的配置，新提出的代理指标为评估防御效果提供了更有效的方法。

Abstract: As Large Language Models (LLMs) become increasingly integrated into software
development workflows, they also become prime targets for adversarial attacks.
Among these, backdoor attacks are a significant threat, allowing attackers to
manipulate model outputs through hidden triggers embedded in training data.
Detecting such backdoors remains a challenge, and one promising approach is the
use of Spectral Signature defense methods that identify poisoned data by
analyzing feature representations through eigenvectors. While some prior works
have explored Spectral Signatures for backdoor detection in neural networks,
recent studies suggest that these methods may not be optimally effective for
code models. In this paper, we revisit the applicability of Spectral
Signature-based defenses in the context of backdoor attacks on code models. We
systematically evaluate their effectiveness under various attack scenarios and
defense configurations, analyzing their strengths and limitations. We found
that the widely used setting of Spectral Signature in code backdoor detection
is often suboptimal. Hence, we explored the impact of different settings of the
key factors. We discovered a new proxy metric that can more accurately estimate
the actual performance of Spectral Signature without model retraining after the
defense.

</details>


### [5] [One Bug, Hundreds Behind: LLMs for Large-Scale Bug Discovery](https://arxiv.org/abs/2510.14036)
*Qiushi Wu,Yue Xiao,Dhilung Kirat,Kevin Eykholt,Jiyong Jang,Douglas Lee Schales*

Main category: cs.SE

TL;DR: BugStone是一个基于LLVM和大型语言模型的程序分析系统，用于检测和修复程序中的重复模式错误(RPBs)，在Linux内核中发现了大量新的潜在问题，并在安全漏洞数据集上取得了高精度。


<details>
  <summary>Details</summary>
Motivation: 大型程序中的bug修复耗时耗力，且已修复的bug可能在程序其他相似代码段中重复出现，形成重复模式错误(RPBs)，这些错误会扩大攻击面，严重威胁软件安全。

Method: 利用LLVM和大型语言模型，通过分析已修复的bug实例来识别一致的错误模式(如特定API误用)，然后在整个程序中搜索相似模式来发现潜在漏洞。

Result: 从135个独特RPBs出发，BugStone在Linux内核中识别了超过22,000个新潜在问题，手动验证400个发现中246个有效；在包含1,900多个安全bug的数据集上，识别了80个重复模式和850个对应修复，精度达92.2%，配对准确率79.1%。

Conclusion: RPBs在软件中普遍存在且严重影响安全性，BugStone系统能有效检测和修复这类错误，为软件安全提供了实用的自动化解决方案。

Abstract: Fixing bugs in large programs is a challenging task that demands substantial
time and effort. Once a bug is found, it is reported to the project
maintainers, who work with the reporter to fix it and eventually close the
issue. However, across the program, there are often similar code segments,
which may also contain the bug, but were missed during discovery. Finding and
fixing each recurring bug instance individually is labor intensive. Even more
concerning, bug reports can inadvertently widen the attack surface as they
provide attackers with an exploitable pattern that may be unresolved in other
parts of the program.
  In this paper, we explore these Recurring Pattern Bugs (RPBs) that appear
repeatedly across various code segments of a program or even in different
programs, stemming from a same root cause, but are unresolved. Our
investigation reveals that RPBs are widespread and can significantly compromise
the security of software programs. This paper introduces BugStone, a program
analysis system empowered by LLVM and a Large Language Model (LLM). The key
observation is that many RPBs have one patched instance, which can be leveraged
to identify a consistent error pattern, such as a specific API misuse. By
examining the entire program for this pattern, it is possible to identify
similar sections of code that may be vulnerable. Starting with 135 unique RPBs,
BugStone identified more than 22K new potential issues in the Linux kernel.
Manual analysis of 400 of these findings confirmed that 246 were valid. We also
created a dataset from over 1.9K security bugs reported by 23 recent top-tier
conference works. We manually annotate the dataset, identify 80 recurring
patterns and 850 corresponding fixes. Even with a cost-efficient model choice,
BugStone achieved 92.2% precision and 79.1% pairwise accuracy on the dataset.

</details>


### [6] [David vs. Goliath: A comparative study of different-sized LLMs for code generation in the domain of automotive scenario generation](https://arxiv.org/abs/2510.14115)
*Philipp Bauerfeind,Amir Salarpour,David Fernandez,Pedram MohajerAnsari,Johannes Reschke,Mert D. Pesé*

Main category: cs.SE

TL;DR: NL2Scenic是一个用于测试自动驾驶场景生成的开源数据集和框架，包含146个自然语言/Scenic代码对，评估了13个模型在Scenic代码生成方面的表现，并提出EDIT-COMP作为更可靠的评估指标。


<details>
  <summary>Details</summary>
Motivation: 解决现有NL-to-Scenic生成方法面临的数据稀缺、可复现性差和评估指标不一致的问题，为自动驾驶场景编程提供标准化评估基准。

Method: 构建包含146个NL/Scenic对的数据集，设计30个难度分层的测试用例，开发示例检索器，测试13个模型（4个专有模型和9个开源代码模型）在14种提示变体下的表现，使用文本和执行指标进行评估，并与专家研究对比。

Result: GPT-4o表现最佳，Qwen2.5Coder-14B达到其88%的专家评分；检索增强提示能显著提升小模型性能；中等规模开源模型是实用且经济的选择；EDIT-SIM与人类判断相关性最好，EDIT-COMP提供更可靠的排名保真度。

Conclusion: NL2Scenic和EDIT-COMP为Scenic代码生成提供了标准化、可复现的评估基础，表明中等规模开源模型是自动驾驶场景编程的实用且经济高效的选择。

Abstract: Scenario simulation is central to testing autonomous driving systems. Scenic,
a domain-specific language (DSL) for CARLA, enables precise and reproducible
scenarios, but NL-to-Scenic generation with large language models (LLMs)
suffers from scarce data, limited reproducibility, and inconsistent metrics. We
introduce NL2Scenic, an open dataset and framework with 146 NL/Scenic pairs, a
difficulty-stratified 30-case test split, an Example Retriever, and 14
prompting variants (ZS, FS, CoT, SP, MoT). We evaluate 13 models: four
proprietary (GPT-4o, GPT-5, Claude-Sonnet-4, Gemini-2.5-pro) and nine
open-source code models (Qwen2.5Coder 0.5B-32B; CodeLlama 7B/13B/34B), using
text metrics (BLEU, ChrF, EDIT-SIM, CrystalBLEU) and execution metrics
(compilation and generation), and compare them with an expert study (n=11).
EDIT-SIM correlates best with human judgments; we also propose EDIT-COMP (F1 of
EDIT-SIM and compilation) as a robust dataset-level proxy that improves ranking
fidelity. GPT-4o performs best overall, while Qwen2.5Coder-14B reaches about 88
percent of its expert score on local hardware. Retrieval-augmented prompting,
Few-Shot with Example Retriever (FSER), consistently boosts smaller models, and
scaling shows diminishing returns beyond mid-size, with Qwen2.5Coder
outperforming CodeLlama at comparable scales. NL2Scenic and EDIT-COMP offer a
standardized, reproducible basis for evaluating Scenic code generation and
indicate that mid-size open-source models are practical, cost-effective options
for autonomous-driving scenario programming.

</details>


### [7] [Caruca: Effective and Efficient Specification Mining for Opaque Software Components](https://arxiv.org/abs/2510.14279)
*Evangelos Lamprou,Seong-Heon Jung,Mayank Keoliya,Lukas Lazarek,Konstantinos Kallas,Michael Greenberg,Nikos Vasilakis*

Main category: cs.SE

TL;DR: Caruca是一个自动为不透明命令挖掘规范的系统，通过使用大语言模型将文档转换为结构化调用语法，探索有效命令调用空间，提取关键属性如并行性和文件系统条件，为现有系统生成可直接使用的规范。


<details>
  <summary>Details</summary>
Motivation: 现有系统需要手动创建部分规范来推理不透明命令，但这个过程是手动、费力且容易出错的，限制了这些系统的实用性。

Method: 使用大语言模型将命令文档转换为结构化调用语法，探索语法有效的命令调用和执行环境空间，通过系统调用和文件系统拦截具体执行每个命令-环境对，提取关键属性。

Result: 在60个GNU Coreutils、POSIX和第三方命令上应用Caruca，除一个案例外都生成了正确的规范，完全消除了手动工作，目前为最先进的静态分析工具提供完整规范支持。

Conclusion: Caruca能够自动为不透明命令生成规范，显著减少了手动工作量，提高了规范依赖系统的实用性。

Abstract: A wealth of state-of-the-art systems demonstrate impressive improvements in
performance, security, and reliability on programs composed of opaque
components, such as Unix shell commands. To reason about commands, these
systems require partial specifications. However, creating such specifications
is a manual, laborious, and error-prone process, limiting the practicality of
these systems. This paper presents Caruca, a system for automatic specification
mining for opaque commands. To overcome the challenge of language diversity
across commands, Caruca first instruments a large language model to translate a
command's user-facing documentation into a structured invocation syntax. Using
this representation, Caruca explores the space of syntactically valid command
invocations and execution environments. Caruca concretely executes each
command-environment pair, interposing at the system-call and filesystem level
to extract key command properties such as parallelizability and filesystem pre-
and post-conditions. These properties can be exported in multiple specification
formats and are immediately usable by existing systems. Applying Caruca across
60 GNU Coreutils, POSIX, and third-party commands across several
specification-dependent systems shows that Caruca generates correct
specifications for all but one case, completely eliminating manual effort from
the process and currently powering the full specifications for a
state-of-the-art static analysis tool.

</details>


### [8] [A Hybrid, Knowledge-Guided Evolutionary Framework for Personalized Compiler Auto-Tuning](https://arxiv.org/abs/2510.14292)
*Haolin Pan,Hongbin Zhang,Mingjie Xing,Yanjun Wu*

Main category: cs.SE

TL;DR: 提出了一种混合知识引导进化框架，通过离线构建编译知识库和在线知识注入遗传算法，自动优化编译器pass序列，相比-Oz基准平均减少11.0% LLVM IR指令。


<details>
  <summary>Details</summary>
Motivation: 编译器pass自动调优对提升软件性能至关重要，但寻找最优pass序列是NP难问题。传统通用优化标志如-O3和-Oz采用一刀切方法，往往无法充分发挥程序性能潜力。

Method: 混合知识引导进化框架：离线阶段构建包含pass行为向量、pass分组、协同pass图和原型pass序列的编译知识库；在线阶段使用知识注入的遗传算法进行个性化优化。

Result: 在7个公共数据集上，相比高度优化的opt -Oz基准，平均额外减少11.0%的LLVM IR指令，展现了在发现个性化高性能优化序列方面的最先进能力。

Conclusion: 该框架通过结合离线知识提取和在线知识引导进化，有效解决了编译器pass序列优化问题，显著提升了程序性能优化效果。

Abstract: Compiler pass auto-tuning is critical for enhancing software performance, yet
finding the optimal pass sequence for a specific program is an NP-hard problem.
Traditional, general-purpose optimization flags like -O3 and -Oz adopt a
one-size-fits-all approach, often failing to unlock a program's full
performance potential. To address this challenge, we propose a novel Hybrid,
Knowledge-Guided Evolutionary Framework. This framework intelligently guides
online, personalized optimization using knowledge extracted from a large-scale
offline analysis phase. During the offline stage, we construct a comprehensive
compilation knowledge base composed of four key components: (1) Pass Behavioral
Vectors to quantitatively capture the effectiveness of each optimization; (2)
Pass Groups derived from clustering these vectors based on behavior similarity;
(3) a Synergy Pass Graph to model beneficial sequential interactions; and (4) a
library of Prototype Pass Sequences evolved for distinct program types. In the
online stage, a bespoke genetic algorithm leverages this rich knowledge base
through specially designed, knowledge-infused genetic operators. These
operators transform the search by performing semantically-aware recombination
and targeted, restorative mutations. On a suite of seven public datasets, our
framework achieves an average of 11.0% additional LLVM IR instruction reduction
over the highly-optimized opt -Oz baseline, demonstrating its state-of-the-art
capability in discovering personalized, high-performance optimization
sequences.

</details>


### [9] [A Systematic Study of Time Limit Exceeded Errors in Online Programming Assignments](https://arxiv.org/abs/2510.14339)
*Jialu Zhang,Jialiang Gu,Wangmeiyu Zhang,José Pablo Cambronero,John Kolesar,Ruzica Piskac,Daming Li,Hanyuan Shi*

Main category: cs.SE

TL;DR: 本文首次对在线编程中的TLE错误进行了大规模实证研究，开发了首个专门针对TLE错误的自动修复工具Nettle及其评估框架Nettle-Eval。


<details>
  <summary>Details</summary>
Motivation: 在线编程平台上的用户经常遇到TLE错误，但错误信息缺乏诊断价值，平台支持有限，现有调试工具帮助不大，导致许多用户在多次TLE失败后放弃提交。

Method: 手动分析了1000个Codeforces的TLE提交，分类其根本原因；开发了Nettle工具，将LLM与编译器生成的针对性自动反馈和测试用例相结合；创建了Nettle-Eval评估框架。

Result: 研究发现TLE错误不仅源于低效算法，还包括无限循环、数据结构使用不当和I/O效率问题；Nettle在1000个真实案例中实现了98.5%的修复率，远超最强LLM基线，所有修复都通过了Nettle-Eval和平台官方检查器。

Conclusion: TLE错误的原因比传统认知更复杂；Nettle工具能有效自动修复TLE错误，证明了该框架的可靠性。

Abstract: Online programming platforms such as Codeforces and LeetCode attract millions
of users seeking to learn to program or refine their skills for industry
interviews. A major challenge for these users is the Time Limit Exceeded (TLE)
error, triggered when a program exceeds the execution time bound. Although
designed as a performance safeguard, TLE errors are difficult to resolve: error
messages provide no diagnostic insight, platform support is minimal, and
existing debugging tools offer little help. As a result, many users abandon
their submissions after repeated TLE failures.
  This paper presents the first large-scale empirical study of TLE errors in
online programming. We manually analyzed 1000 Codeforces submissions with TLE
errors, classified their root causes, and traced how users attempted to fix
them. Our analysis shows that TLE errors often arise not only from inefficient
algorithms but also from infinite loops, improper data structure use, and
inefficient I/O, challenging the conventional view that TLEs are purely
performance issues.
  Guided by these findings, we introduce Nettle, the first automated repair
tool specifically designed for TLE errors, and Nettle-Eval, the first framework
for evaluating TLE repairs. Integrating LLMs with targeted automated feedback
generated by the compiler and test cases, Nettle produces small, correct code
edits that eliminate TLEs while preserving functionality. Evaluated on the same
1000 real-world cases, Nettle achieves a 98.5% fix rate, far exceeding the
strongest LLM baseline, and all of its repairs pass both Nettle-Eval and the
platform's official checker, confirming the reliability of our framework.

</details>


### [10] [PathFix: Automated Program Repair with Expected Path](https://arxiv.org/abs/2510.14341)
*Xu He,Shu Wang,Kun Sun*

Main category: cs.SE

TL;DR: PathFix是一种新的自动化程序修复方法，利用从正确执行路径提取的路径敏感约束来生成修复补丁，通过分析预期路径和状态约束来解决现有APR方法的挑战。


<details>
  <summary>Details</summary>
Motivation: 现有APR方法由于难以生成精确规范而面临两个主要挑战：生成过多看似合理的补丁候选和过拟合到部分测试用例。

Method: PathFix通过四个步骤操作：1)追踪故障路径；2)分析预期路径；3)通过解决预期路径上的状态约束生成和评估补丁；4)验证生成补丁的正确性。还集成了大语言模型来增强修复性能和缓解可扩展性问题。

Result: 实验结果表明PathFix优于现有解决方案，特别是在处理复杂程序结构（如循环和递归）方面表现突出。

Conclusion: PathFix通过路径敏感约束和LLM集成，有效解决了APR中的关键挑战，在复杂程序修复方面表现出色。

Abstract: Automated program repair (APR) techniques are effective in fixing inevitable
defects in software, enhancing development efficiency and software robustness.
However, due to the difficulty of generating precise specifications, existing
APR methods face two main challenges: generating too many plausible patch
candidates and overfitting them to partial test cases. To tackle these
challenges, we introduce a new APR method named PathFix, which leverages
path-sensitive constraints extracted from correct execution paths to generate
patches for repairing buggy code. It is based on one observation: if a buggy
program is repairable, at least one expected path is supposed to replace the
fault path in the patched program. PathFix operates in four main steps. First,
it traces fault paths reaching the fault output in the buggy program. Second,
it derives expected paths by analyzing the desired correct output on the
control flow graph, where an expected path defines how a feasible patch leads
to the correct execution. Third, PathFix generates and evaluates patches by
solving state constraints along the expected path. Fourth, we validate the
correctness of the generated patch. To further enhance repair performance and
mitigate scalability issues introduced by path-sensitive analysis, we integrate
a large language model (LLM) into our framework. Experimental results show that
PathFix outperforms existing solutions, particularly in handling complex
program structures such as loops and recursion.

</details>


### [11] [Towards Automated Governance: A DSL for Human-Agent Collaboration in Software Projects](https://arxiv.org/abs/2510.14465)
*Adem Ait,Gwendal Jouneaux,Javier Luis Cánovas Izquierdo,Jordi Cabot*

Main category: cs.SE

TL;DR: 提出一种新的领域特定语言（DSL），用于定义和执行涉及多样化利益相关者（包括AI代理）的软件项目治理政策


<details>
  <summary>Details</summary>
Motivation: 软件开发的利益相关者日益多样化，包括来自不同背景的人类贡献者和AI代理，这在开源软件项目中带来了独特的治理挑战，因为明确的政策往往缺失或不清晰

Method: 设计并开发一种专门用于定义和执行治理政策的领域特定语言（DSL）

Result: 该DSL为实现更强大、适应性更强且最终自动化的治理提供了途径

Conclusion: 这种DSL为软件项目（特别是开源项目）中更有效的协作铺平了道路

Abstract: The stakeholders involved in software development are becoming increasingly
diverse, with both human contributors from varied backgrounds and AI-powered
agents collaborating together in the process. This situation presents unique
governance challenges, particularly in Open-Source Software (OSS) projects,
where explicit policies are often lacking or unclear. This paper presents the
vision and foundational concepts for a novel Domain-Specific Language (DSL)
designed to define and enforce rich governance policies in systems involving
diverse stakeholders, including agents. This DSL offers a pathway towards more
robust, adaptable, and ultimately automated governance, paving the way for more
effective collaboration in software projects, especially OSS ones.

</details>


### [12] [E2Edev: Benchmarking Large Language Models in End-to-End Software Development Task](https://arxiv.org/abs/2510.14509)
*Jingyao Liu,Chen Huang,Zhizhao Guan,Wenqiang Lei,Yang Deng*

Main category: cs.SE

TL;DR: E2EDev是一个端到端软件开发基准，包含细粒度需求、BDD测试场景和自动化测试流水线，通过人机协同多智能体标注框架确保质量，评估显示现有E2ESD框架仍面临挑战。


<details>
  <summary>Details</summary>
Motivation: 解决端到端软件开发(E2ESD)中缺乏高质量基准的问题，同时减少标注工作量，推动更有效和成本效益的E2ESD解决方案发展。

Method: 提出E2EDev基准，包含细粒度用户需求、BDD测试场景与Python步骤实现、基于Behave的自动化测试流水线，并采用人机协同多智能体标注框架(HITL-MAA)确保质量。

Result: 通过评估各种E2ESD框架和LLM骨干网络，发现现有方法在有效解决这些任务方面仍存在持续困难。

Conclusion: 当前E2ESD解决方案仍面临显著挑战，亟需开发更有效和成本效益的端到端软件开发方法。

Abstract: E2EDev comprises (i) a fine-grained set of user requirements, (ii) {multiple
BDD test scenarios with corresponding Python step implementations for each
requirement}, and (iii) a fully automated testing pipeline built on the Behave
framework. To ensure its quality while reducing the annotation effort, E2EDev
leverages our proposed Human-in-the-Loop Multi-Agent Annotation Framework
(HITL-MAA). {By evaluating various E2ESD frameworks and LLM backbones with
E2EDev}, our analysis reveals a persistent struggle to effectively solve these
tasks, underscoring the critical need for more effective and cost-efficient
E2ESD solutions. Our codebase and benchmark are publicly available at
https://github.com/SCUNLP/E2EDev.

</details>


### [13] [Software Testing Education and Industry Needs - Report from the ENACTEST EU Project](https://arxiv.org/abs/2510.14625)
*Mehrdad Saadatmand,Abbas Khan,Beatriz Marin,Ana C. R Paiva,Nele Van Asch,Graham Moran,Felix Cammaerts,Monique Snoeck,Alexandra Mendes*

Main category: cs.SE

TL;DR: 本研究调查软件测试行业能力需求，识别当前测试教育中的知识差距，并通过焦点小组、访谈和范围综述发现AI测试、安全测试和软技能等领域存在显著知识缺口。


<details>
  <summary>Details</summary>
Motivation: 软件开发的不断演进要求测试人员持续适应新工具和实践，需要了解行业实际能力需求与教育供给之间的差距。

Method: 采用焦点小组、专业访谈和范围综述方法，研究工具由ENACTEST项目联盟共同设计并经过多次迭代完善。

Result: 研究发现行业在专业培训方法、培训挑战、培训质量评估、学术教育与行业需求差距、未来测试教育趋势和知识传递方法等方面存在显著问题。

Conclusion: 软件测试教育需要重点关注AI测试、安全测试和软技能等领域的知识缺口，以满足行业不断变化的需求。

Abstract: The evolving landscape of software development demands that software testers
continuously adapt to new tools, practices, and acquire new skills. This study
investigates software testing competency needs in industry, identifies
knowledge gaps in current testing education, and highlights competencies and
gaps not addressed in academic literature. This is done by conducting two focus
group sessions and interviews with professionals across diverse domains,
including railway industry, healthcare, and software consulting and performing
a curated small-scale scoping review. The study instrument, co-designed by
members of the ENACTEST project consortium, was developed collaboratively and
refined through multiple iterations to ensure comprehensive coverage of
industry needs and educational gaps. In particular, by performing a thematic
qualitative analysis, we report our findings and observations regarding:
professional training methods, challenges in offering training in industry,
different ways of evaluating the quality of training, identified knowledge gaps
with respect to academic education and industry needs, future needs and trends
in testing education, and knowledge transfer methods within companies. Finally,
the scoping review results confirm knowledge gaps in areas such as AI testing,
security testing and soft skills.

</details>


### [14] [ATGen: Adversarial Reinforcement Learning for Test Case Generation](https://arxiv.org/abs/2510.14635)
*Qingyao Li,Xinyi Dai,Weiwen Liu,Xiangyang Li,Yasheng Wang,Ruiming Tang,Yong Yu,Weinan Zhang*

Main category: cs.SE

TL;DR: ATGen是一个通过对抗性强化学习训练测试用例生成的框架，突破了静态数据集的固定难度限制，能发现更复杂的新bug。


<details>
  <summary>Details</summary>
Motivation: 现有测试生成方法依赖静态数据集，存在'固定难度天花板'，无法发现训练范围外的新颖或更复杂bug。

Method: 采用对抗性强化学习框架，让测试生成器与对抗性代码生成器对抗，后者持续制造更难检测的bug来逃避当前策略，形成难度递增的课程。

Result: ATGen显著优于现有最先进方法，既能作为更有效的Best-of-N推理过滤器，也能作为更高质量的训练奖励源。

Conclusion: 这项工作为提升LLM生成代码的可靠性建立了一个新的动态范式。

Abstract: Large Language Models (LLMs) excel at code generation, yet their outputs
often contain subtle bugs, for which effective test cases are a critical
bottleneck. Existing test generation methods, whether based on prompting or
supervised fine-tuning, rely on static datasets. This imposes a
``fixed-difficulty ceiling'', fundamentally limiting their ability to uncover
novel or more complex bugs beyond their training scope. To overcome this, we
introduce ATGen, a framework that trains a test case generator via adversarial
reinforcement learning. ATGen pits a test generator against an adversarial code
generator that continuously crafts harder bugs to evade the current policy.
This dynamic loop creates a curriculum of increasing difficulty challenging
current policy. The test generator is optimized via Reinforcement Learning (RL)
to jointly maximize ``Output Accuracy'' and ``Attack Success'', enabling it to
learn a progressively stronger policy that breaks the fixed-difficulty ceiling
of static training. Extensive experiments demonstrate that ATGen significantly
outperforms state-of-the-art baselines. We further validate its practical
utility, showing it serves as both a more effective filter for Best-of-N
inference and a higher-quality reward source for training code generation
models. Our work establishes a new, dynamic paradigm for improving the
reliability of LLM-generated code.

</details>


### [15] [Requirement Identification for Traffic Simulations in Driving Simulators](https://arxiv.org/abs/2510.14653)
*Sven Tarlowski,Lutz Eckstein*

Main category: cs.SE

TL;DR: 提出了一种系统识别交通仿真需求的方法论，通过结构化方法确保交通仿真的真实性，支持稳健的汽车开发和测试。


<details>
  <summary>Details</summary>
Motivation: 解决确保真实交通条件的挑战，提高实验结果的效度和参与者参与度。

Method: 基于各研究阶段的子目标，采用结构化方法推导微观层面、智能体模型和视觉表示的具体技术需求。

Result: 建立了研究目标与交通仿真设计之间的清晰联系，保持了高保真度。

Conclusion: 该方法论能够增强交通仿真的真实性和有效性，为汽车开发和测试提供可靠支持。

Abstract: This paper addresses the challenge of ensuring realistic traffic conditions
by proposing a methodology that systematically identifies traffic simulation
requirements. Using a structured approach based on sub-goals in each study
phase, specific technical needs are derived for microscopic levels, agent
models, and visual representation. The methodology aims to maintain a high
degree of fidelity, enhancing both the validity of experimental outcomes and
participant engagement. By providing a clear link between study objectives and
traffic simulation design, this approach supports robust automotive development
and testing.

</details>


### [16] [LLM Agents for Automated Web Vulnerability Reproduction: Are We There Yet?](https://arxiv.org/abs/2510.14700)
*Bin Liu,Yanjie Zhao,Guoai Xu,Haoyu Wang*

Main category: cs.SE

TL;DR: 本文首次全面评估了最先进的LLM智能体在自动化Web漏洞复现方面的能力，发现虽然LLM智能体在简单的基于库的漏洞上表现尚可，但在需要多组件环境的复杂服务型漏洞上持续失败。


<details>
  <summary>Details</summary>
Motivation: LLM智能体在软件工程和网络安全任务中展现出强大能力，但自动化Web漏洞复现这一关键应用领域尚未得到充分探索。本文旨在系统评估LLM智能体在真实Web漏洞复现场景中的表现。

Method: 对来自软件工程、网络安全和通用领域的20个LLM智能体在16个维度上进行系统评估，包括技术能力、环境适应性和用户体验因素，测试3个代表性Web漏洞。然后选择3个表现最佳的智能体在包含80个真实CVE的基准数据集上进行深入评估。

Result: LLM智能体在简单库漏洞上取得合理成功率，但在复杂服务型漏洞上持续失败。复杂环境配置和认证障碍导致智能体能够执行利用代码但无法触发实际漏洞。对输入指导高度敏感，认证信息不完整时性能下降超过33%。

Conclusion: 当前LLM智能体能力与可靠自动化漏洞复现需求之间存在显著差距，需要在环境适应和自主问题解决能力方面取得进展。

Abstract: Large language model (LLM) agents have demonstrated remarkable capabilities
in software engineering and cybersecurity tasks, including code generation,
vulnerability discovery, and automated testing. One critical but underexplored
application is automated web vulnerability reproduction, which transforms
vulnerability reports into working exploits. Although recent advances suggest
promising potential, challenges remain in applying LLM agents to real-world web
vulnerability reproduction scenarios. In this paper, we present the first
comprehensive evaluation of state-of-the-art LLM agents for automated web
vulnerability reproduction. We systematically assess 20 agents from software
engineering, cybersecurity, and general domains across 16 dimensions, including
technical capabilities, environment adaptability, and user experience factors,
on 3 representative web vulnerabilities. Based on the results, we select three
top-performing agents (OpenHands, SWE-agent, and CAI) for in-depth evaluation
on our benchmark dataset of 80 real-world CVEs spanning 7 vulnerability types
and 6 web technologies. Our results reveal that while LLM agents achieve
reasonable success on simple library-based vulnerabilities, they consistently
fail on complex service-based vulnerabilities requiring multi-component
environments. Complex environment configurations and authentication barriers
create a gap where agents can execute exploit code but fail to trigger actual
vulnerabilities. We observe high sensitivity to input guidance, with
performance degrading by over 33% under incomplete authentication information.
Our findings highlight the significant gap between current LLM agent
capabilities and the demands of reliable automated vulnerability reproduction,
emphasizing the need for advances in environmental adaptation and autonomous
problem-solving capabilities.

</details>


### [17] [Leveraging Code Cohesion Analysis to Identify Source Code Supply Chain Attacks](https://arxiv.org/abs/2510.14778)
*Maor Reuben,Ido Mendel,Or Feldman,Moshe Kravchik,Mordehai Guri,Rami Puzis*

Main category: cs.SE

TL;DR: 本文提出了一种基于名称预测的内聚度（NPC）无监督方法，通过量化源代码中的内聚度破坏来检测恶意代码注入。该方法在极端不平衡测试集上表现出色，在1:1000比例下Precision@100达到36.41%。


<details>
  <summary>Details</summary>
Motivation: 供应链攻击通过向合法项目中注入恶意代码严重威胁软件安全。这类攻击虽然罕见但破坏性极大，且自动化检测困难，需要理解插入代码及其上下文的意图。

Method: 使用基于名称预测的内聚度（NPC）指标，分析恶意代码引入时函数内聚度的变化，并与自然内聚度波动进行比较。该方法是无监督的，通过监控高内聚度函数来检测代码注入。

Result: 对54,707个函数和369个开源C++仓库的分析显示，代码注入会降低内聚度，并使命名模式趋向更短、描述性更差的名称。在1:1000比例下Precision@100为36.41%，在1:10000比例下为12.47%。

Conclusion: 自动化内聚度测量，特别是基于名称预测的内聚度方法，有助于识别供应链攻击，提高源代码完整性。

Abstract: Supply chain attacks significantly threaten software security with malicious
code injections within legitimate projects. Such attacks are very rare but may
have a devastating impact. Detecting spurious code injections using automated
tools is further complicated as it often requires deciphering the intention of
both the inserted code and its context. In this study, we propose an
unsupervised approach for highlighting spurious code injections by quantifying
cohesion disruptions in the source code. Using a name-prediction-based cohesion
(NPC) metric, we analyze how function cohesion changes when malicious code is
introduced compared to natural cohesion fluctuations. An analysis of 54,707
functions over 369 open-source C++ repositories reveals that code injection
reduces cohesion and shifts naming patterns toward shorter, less descriptive
names compared to genuine function updates. Considering the sporadic nature of
real supply-chain attacks, we evaluate the proposed method with extreme
test-set imbalance and show that monitoring high-cohesion functions with NPC
can effectively detect functions with injected code, achieving a Precision@100
of 36.41% at a 1:1,000 ratio and 12.47% at 1:10,000. These results suggest that
automated cohesion measurements, in general, and name-prediction-based
cohesion, in particular, may help identify supply chain attacks, improving
source code integrity.

</details>


### [18] [Instruction Set Migration at Warehouse Scale](https://arxiv.org/abs/2510.14928)
*Eric Christopher,Kevin Crossan,Wolff Dobson,Chris Kennelly,Drew Lewis,Kun Lin,Martin Maas,Parthasarathy Ranganathan,Emma Rapati,Brian Yang*

Main category: cs.SE

TL;DR: 本文分析了从x86到Arm架构的大规模代码迁移挑战，发现现代ISA迁移主要依赖重新编译而非二进制翻译，并提出了自动化解决方案和AI应用。


<details>
  <summary>Details</summary>
Motivation: 传统观点认为ISA迁移的主要挑战是二进制翻译，但现代迁移基于开源生态系统，需要重新编译软件，这带来了新的多方面挑战。

Method: 通过分析Google从x86到Arm的大规模迁移（涉及近40,000个代码提交），建立了ISA迁移的任务分类，并展示了自动化和AI的应用。

Result: 研究发现现代ISA迁移可以依赖重新编译，识别了迁移过程中的各类任务，并展示了Google如何自动化这些步骤。

Conclusion: ISA迁移的主要挑战已从二进制翻译转向重新编译相关的任务，AI可以在自动化解决这些任务中发挥重要作用，但仍有一些挑战性任务需要进一步研究。

Abstract: Migrating codebases from one instruction set architecture (ISA) to another is
a major engineering challenge. A recent example is the adoption of Arm (in
addition to x86) across the major Cloud hyperscalers. Yet, this problem has
seen limited attention by the academic community. Most work has focused on
static and dynamic binary translation, and the traditional conventional wisdom
has been that this is the primary challenge.
  In this paper, we show that this is no longer the case. Modern ISA migrations
can often build on a robust open-source ecosystem, making it possible to
recompile all relevant software from scratch. This introduces a new and
multifaceted set of challenges, which are different from binary translation.
  By analyzing a large-scale migration from x86 to Arm at Google, spanning
almost 40,000 code commits, we derive a taxonomy of tasks involved in ISA
migration. We show how Google automated many of the steps involved, and
demonstrate how AI can play a major role in automatically addressing these
tasks. We identify tasks that remain challenging and highlight research
challenges that warrant further attention.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [19] [Noisy Networks, Nosy Neighbors: Inferring Privacy Invasive Information from Encrypted Wireless Traffic](https://arxiv.org/abs/2510.13822)
*Bartosz Burgiel*

Main category: cs.CR

TL;DR: 通过被动观察智能家居的无线网络流量，即使数据加密也能推断住户的隐私信息，如设备活动状态、日常行为和公寓布局。


<details>
  <summary>Details</summary>
Motivation: 探索智能家居环境中加密无线流量的隐私风险，验证邻居仅通过被动监听就能获取侵入性隐私信息的可能性。

Method: 模拟邻居监听场景，分析原始802.11数据包和蓝牙低功耗广播，使用RSSI三边定位识别设备、推断活动状态和位置。

Result: 能够检测多媒体设备活跃期，推断睡眠、工作和媒体消费等活动，甚至近似邻居公寓的布局。

Conclusion: 智能家居的隐私风险超出传统数据泄露，邻居仅通过加密网络流量就能获得侵入性隐私洞察。

Abstract: This thesis explores the extent to which passive observation of wireless
traffic in a smart home environment can be used to infer privacy-invasive
information about its inhabitants. Using a setup that mimics the capabilities
of a nosy neighbor in an adjacent flat, we analyze raw 802.11 packets and
Bluetooth Low Energy advertisemets. From this data, we identify devices, infer
their activity states and approximate their location using RSSI-based
trilateration. Despite the encrypted nature of the data, we demonstrate that it
is possible to detect active periods of multimedia devices, infer common
activities such as sleeping, working and consuming media, and even approximate
the layout of the neighbor's apartment. Our results show that privacy risks in
smart homes extend beyond traditional data breaches: a nosy neighbor behind the
wall can gain privacy-invasive insights into the lives of their neighbors
purely from encrypted network traffic.

</details>


### [20] [Multi-Layer Secret Sharing for Cross-Layer Attack Defense in 5G Networks: a COTS UE Demonstration](https://arxiv.org/abs/2510.13824)
*Wai Ming Chan,Remi Chou,Taejoon Kim*

Main category: cs.CR

TL;DR: 首个在商用5G用户设备上实现多层秘密共享的系统，无需基础设施修改或预共享密钥，通过XOR方法在运营商和中继间分发秘密份额，确保即使一个运营商和一个中继同时失效也能完美恢复数据。


<details>
  <summary>Details</summary>
Motivation: 解决5G网络中数据机密性和可用性问题，特别是在面临拒绝服务攻击或意外攻击时，确保秘密数据能够安全恢复。

Method: 采用基于XOR的多层秘密共享方法，将秘密份额分布在多个网络运营商和分布式中继之间。

Result: 成功在商用5G用户设备上实现了该方案，能够在一个网络运营商和一个中继同时丢失的情况下仍能完美恢复原始数据。

Conclusion: 该方案为5G网络提供了强大的数据保护机制，无需修改基础设施即可实现高可用性和机密性。

Abstract: This demo presents the first implementation of multi-layer secret sharing on
commercial-off-the-shelf (COTS) 5G user equipment (UE), operating without
infrastructure modifications or pre-shared keys. Our XOR-based approach
distributes secret shares across network operators and distributed relays,
ensuring perfect recovery and data confidentiality even if one network operator
and one relay are simultaneously lost (e.g., under denial of service (DoS) or
unanticipated attacks).

</details>


### [21] [A2AS: Agentic AI Runtime Security and Self-Defense](https://arxiv.org/abs/2510.13825)
*Eugene Neelou,Ivan Novikov,Max Moroz,Om Narayan,Tiffany Saade,Mika Ayenson,Ilya Kabanov,Jen Ozmen,Edward Lee,Vineeth Sai Narajala,Emmanuel Guilherme Junior,Ken Huang,Huseyin Gulsin,Jason Ross,Marat Vyshegorodtsev,Adelin Travers,Idan Habler,Rahul Jadav*

Main category: cs.CR

TL;DR: A2AS框架是为AI代理和LLM应用设计的安全层，类似于HTTPS保护HTTP。它通过BASIC安全模型实现行为认证、提示认证、安全边界、上下文防御和编码策略，提供深度防御策略。


<details>
  <summary>Details</summary>
Motivation: 随着AI代理和LLM应用的普及，需要建立类似HTTPS的安全标准来保护这些系统免受攻击，确保行为可认证、上下文完整性，同时避免延迟开销和架构复杂性。

Method: 提出A2AS框架和BASIC安全模型：行为证书(B)强制执行行为，认证提示(A)确保上下文完整性，安全边界(S)隔离不可信输入，上下文防御(I)实现安全推理，编码策略(C)支持应用特定规则。

Result: A2AS框架能够避免延迟开销、外部依赖、架构变更、模型重训练和操作复杂性，同时提供全面的安全保护。

Conclusion: A2AS框架和BASIC安全模型有潜力成为行业标准，为AI代理和LLM应用提供可靠的安全保护层，这是该系列论文的第一篇介绍性文章。

Abstract: The A2AS framework is introduced as a security layer for AI agents and
LLM-powered applications, similar to how HTTPS secures HTTP. A2AS enforces
certified behavior, activates model self-defense, and ensures context window
integrity. It defines security boundaries, authenticates prompts, applies
security rules and custom policies, and controls agentic behavior, enabling a
defense-in-depth strategy. The A2AS framework avoids latency overhead, external
dependencies, architectural changes, model retraining, and operational
complexity. The BASIC security model is introduced as the A2AS foundation: (B)
Behavior certificates enable behavior enforcement, (A) Authenticated prompts
enable context window integrity, (S) Security boundaries enable untrusted input
isolation, (I) In-context defenses enable secure model reasoning, (C) Codified
policies enable application-specific rules. This first paper in the series
introduces the BASIC security model and the A2AS framework, exploring their
potential toward establishing the A2AS industry standard.

</details>


### [22] [PIShield: Detecting Prompt Injection Attacks via Intrinsic LLM Features](https://arxiv.org/abs/2510.14005)
*Wei Zou,Yupei Liu,Yanting Wang,Ying Chen,Neil Gong,Jinyuan Jia*

Main category: cs.CR

TL;DR: 提出PIShield方法，通过分析LLM内部特定层（注入关键层）中最后一个token的表示来检测提示注入攻击，使用简单线性分类器实现高效有效的检测。


<details>
  <summary>Details</summary>
Motivation: LLM应用易受提示注入攻击，现有检测方法性能不佳且计算开销大，需要开发既有效又高效的检测方案。

Method: 从LLM的注入关键层提取最后一个token的内部表示，使用标记的干净和污染提示训练线性分类器进行检测。

Result: 在5个基准数据集和8种攻击上测试，PIShield显著优于11个基线方法，检测效果和效率都很高，并能抵抗强自适应攻击。

Conclusion: PIShield通过利用LLM内部表示特征，实现了对提示注入攻击的高效有效检测，为LLM安全提供了实用解决方案。

Abstract: LLM-integrated applications are vulnerable to prompt injection attacks, where
an attacker contaminates the input to inject malicious prompts, causing the LLM
to follow the attacker's intent instead of the original user's. Existing prompt
injection detection methods often have sub-optimal performance and/or high
computational overhead. In this work, we propose PIShield, a detection method
that is both effective and efficient. Our key observation is that the internal
representation of the final token in a prompt-extracted from a specific layer
of the LLM, which we term the injection-critical layer-captures distinguishing
features between clean and contaminated prompts. Leveraging this insight, we
train a simple linear classifier on these internal representations using a
labeled set of clean and contaminated prompts. We compare PIShield against 11
baselines across 5 diverse benchmark datasets and 8 prompt injection attacks.
The results demonstrate that PIShield is both highly effective and efficient,
substantially outperforming existing methods. Additionally, we show that
PIShield resists strong adaptive attacks.

</details>


### [23] [Quantitative Analysis of UAV Intrusion Mitigation for Border Security in 5G with LEO Backhaul Impairments](https://arxiv.org/abs/2510.14066)
*Rajendra Upadhyay,Al Nahian Bin Emran,Rajendra Paudyal,Lisa Donnan,Duminda Wijesekera*

Main category: cs.CR

TL;DR: 提出了一个端到端仿真框架，分析在混合地面-非地面5G系统中检测和缓解不合作无人机入侵的延迟问题，重点研究了卫星回程中断对缓解延迟的影响以及本地回退机制的重要性。


<details>
  <summary>Details</summary>
Motivation: 不合作的无人机在蜂窝网络中作为流氓用户设备运行，会消耗资源、产生干扰并可能侵犯受限空域，对关键基础设施和边境保护构成威胁。

Method: 建立包含地面gNB、卫星回程（具有随机中断）和检测逻辑的系统模型，使用蒙特卡洛方法在不同无人机高度、速度和卫星中断率下进行仿真分析。

Result: 卫星回程中断会导致任意长的缓解延迟；额外切换对延迟影响可忽略；本地回退机制能有效限制缓解延迟；巡逻UE几乎不受影响。

Conclusion: 必须将非地面链路与本地控制相结合，以确保对不合作无人机入侵的稳健和及时响应，本地回退机制在防止极端控制平面和物理安全漏洞方面不可或缺。

Abstract: Uncooperative unmanned aerial vehicles (UAVs) pose emerging threats to
critical infrastructure and border protection by operating as rogue user
equipment (UE) within cellular networks, consuming resources, creating
interference, and potentially violating restricted airspaces. This paper
presents minimal features of the operating space, yet an end-to-end simulation
framework to analyze detect-to-mitigate latency of such intrusions in a hybrid
terrestrial-non-terrestrial (LEO satellite) 5G system. The system model
includes terrestrial gNBs, satellite backhaul (with stochastic outages), and a
detection logic (triggered by handover instability and signal quality
variance). A lockdown mechanism is invoked upon detection, with optional local
fallback to cap mitigation delays. Monte Carlo sweeps across UAV altitudes,
speeds, and satellite outage rates yield several insights. First, satellite
backhaul outages can cause arbitrarily long mitigation delays, yet, to meet
fallback deadlines, they need to be effectively bounded. Second, while handover
instability was hypothesized, our results show that extra handovers have a
negligible effect within the range of parameters we considered. The main
benefit of resilience from fallback comes from the delay in limiting
mitigation. Third, patrol UEs experience negligible collateral impact, with
handover rates close to terrestrial baselines. Stress scenarios further
highlight that fallback is indispensable in preventing extreme control-plane
and physical security vulnerabilities: Without fallback, prolonged outages in
the satellite backhaul delay lockdown commands, allowing rogue UAVs to linger
inside restricted corridors for several seconds longer. These results
underscore the importance of complementing non-terrestrial links with local
control to ensure robust and timely response against uncooperative UAV
intrusions.

</details>


### [24] [Every Language Model Has a Forgery-Resistant Signature](https://arxiv.org/abs/2510.14086)
*Matthew Finlayson,Xiang Ren,Swabha Swayamdipta*

Main category: cs.CR

TL;DR: 本文提出了一种基于语言模型输出几何约束的椭圆签名方法，用于识别模型来源和验证输出真实性。该方法利用语言模型输出位于高维椭圆表面的特性作为模型签名。


<details>
  <summary>Details</summary>
Motivation: 随着闭源语言模型API的普及，需要开发能够提取隐藏模型细节和识别模型来源的取证方法。现有方法主要利用模型架构和参数的几何约束，但椭圆约束这一较少被探索的几何特性具有独特优势。

Method: 利用语言模型输出位于高维椭圆表面的几何约束作为模型签名。该签名具有难以伪造、自然存在、自包含和紧凑冗余的特性。开发了从小模型中提取椭圆的新技术，并提出了基于椭圆签名的语言模型输出验证协议。

Result: 椭圆签名能够有效识别模型来源，且具有防伪性强的特点。虽然从小模型提取椭圆可行，但在生产级大模型中面临实际困难。提出的验证协议类似于密码学对称密钥消息认证系统。

Conclusion: 椭圆签名作为一种新型的模型识别方法，具有独特优势，特别是在防伪性和自包含性方面。虽然在大规模应用中存在挑战，但为语言模型输出验证提供了有前景的方向。

Abstract: The ubiquity of closed-weight language models with public-facing APIs has
generated interest in forensic methods, both for extracting hidden model
details (e.g., parameters) and for identifying models by their outputs. One
successful approach to these goals has been to exploit the geometric
constraints imposed by the language model architecture and parameters. In this
work, we show that a lesser-known geometric constraint--namely, that language
model outputs lie on the surface of a high-dimensional ellipse--functions as a
signature for the model and can be used to identify the source model of a given
output. This ellipse signature has unique properties that distinguish it from
existing model-output association methods like language model fingerprints. In
particular, the signature is hard to forge: without direct access to model
parameters, it is practically infeasible to produce log-probabilities
(logprobs) on the ellipse. Secondly, the signature is naturally occurring,
since all language models have these elliptical constraints. Thirdly, the
signature is self-contained, in that it is detectable without access to the
model inputs or the full weights. Finally, the signature is compact and
redundant, as it is independently detectable in each logprob output from the
model. We evaluate a novel technique for extracting the ellipse from small
models and discuss the practical hurdles that make it infeasible for
production-scale models. Finally, we use ellipse signatures to propose a
protocol for language model output verification, analogous to cryptographic
symmetric-key message authentication systems.

</details>


### [25] [Power Grid Cybersecurity: Policy Analysis White Paper](https://arxiv.org/abs/2510.14171)
*Jack Vanlyssel*

Main category: cs.CR

TL;DR: 该论文提出双重政策方法来加强电网网络安全：加强政府与私营电力公司之间的信息共享以改善威胁检测和响应，以及标准化网络卫生实践以减少常见攻击向量。


<details>
  <summary>Details</summary>
Motivation: 美国电网支撑国家安全、公共安全和经济稳定，但面临工业控制系统漏洞、远程访问和网络卫生不良等日益增长的网络安全风险。尽管其重要性关键，当前政策仍然分散且被动。

Method: 提出双重政策方法：1) 加强政府与私营电力公司之间的信息共享；2) 标准化网络卫生实践。长期建议建立统一的国家网络安全框架来协调现有标准。

Result: 这些政策提供了即时和可持续的改进，以保护国家最重要的基础设施。

Conclusion: 通过双重政策方法和统一框架，能够有效应对电网面临的网络安全挑战，确保关键基础设施的长期韧性。

Abstract: The U.S. power grid underpins national security, public safety, and economic
stability, but faces growing cyber risks from vulnerabilities in industrial
control systems, remote access, and poor cyber hygiene. Despite its critical
importance, current policy remains fragmented and reactive. This paper proposes
a dual policy approach to strengthen grid cybersecurity: enhanced information
sharing between government and private utilities to improve threat detection
and response, and standardized cyber hygiene practices to reduce common attack
vectors. For long-term resilience, a Unified National Cybersecurity Framework
is recommended to align existing NERC, IEC, IEEE, and NIST standards, eliminate
regulatory overlap, and adapt to evolving threats. Together, these policies
offer both immediate and sustainable improvements in safeguarding the nation's
most vital infrastructure.

</details>


### [26] [Securing U.S. Critical Infrastructure: Lessons from Stuxnet and the Ukraine Power Grid Attacks](https://arxiv.org/abs/2510.14185)
*Jack Vanlyssel*

Main category: cs.CR

TL;DR: 本文分析了针对工业控制系统(ICS)的历史网络攻击，识别出持续存在的漏洞，并提出了基于零信任架构和网络分段的政策建议，以保护美国关键基础设施免受未来网络威胁。


<details>
  <summary>Details</summary>
Motivation: 工业控制系统支撑着美国关键基础设施，但数字化集成增加了其遭受网络攻击的风险。历史攻击事件揭示了这些系统中持续存在的安全弱点，可能引发灾难性破坏和国家安全危机。

Method: 通过分析Stuxnet和乌克兰电网事件等历史攻击案例，识别出反复出现的漏洞，并评估这些漏洞在当前美国基础设施中的相关性。

Result: 研究发现工业控制系统中普遍存在网络分段不足、软件过时、认证薄弱和监控不足等安全弱点，这些弱点在当前美国基础设施中仍然存在。

Conclusion: 建议立即实施政策改革，采用零信任架构和改进的网络分段策略，以增强系统弹性，保护关键操作技术免受未来网络威胁。

Abstract: Industrial Control Systems (ICS) underpin the United States' critical
infrastructure, managing essential services such as power, water, and
transportation that are vital to national security and public safety. However,
increasing digital integration has exposed these systems to escalating cyber
threats. Historical attacks like Stuxnet and the Ukraine power grid incident
revealed exploitable weaknesses-poor network segmentation, outdated software,
weak authentication, and inadequate monitoring-that persist in many U.S. ICS
environments today. This paper analyzes these landmark attacks to identify
recurring vulnerabilities and assess their relevance to current U.S.
infrastructure. It argues that without immediate reforms, similar exploits
could lead to catastrophic disruptions and national security crises. To address
these risks, the paper proposes policy measures focused on implementing
zero-trust architecture and improved network segmentation to enhance system
resilience. These recommendations aim to guide policymakers and industry
leaders in securing the nation's most critical operational technologies against
future cyber threats.

</details>


### [27] [Infrastructure Patterns in Toll Scam Domains: A Comprehensive Analysis of Cybercriminal Registration and Hosting Strategies](https://arxiv.org/abs/2510.14198)
*Morium Akter Munny,Mahbub Alam,Sonjoy Kumar Paul,Daniel Timko,Muhammad Lutfor Rahman,Nitesh Saxena*

Main category: cs.CR

TL;DR: 首次对67,907个确认的收费诈骗域名进行大规模分析，揭示了攻击者利用宽松注册商和非主流顶级域名的策略，并构建了基于注册数据的预测模型，准确率达80.4%。


<details>
  <summary>Details</summary>
Motivation: 收费诈骗通过注册伪装成合法交通机构的虚假域名欺骗用户进行欺诈支付，这类诈骗快速增长但缺乏系统研究。

Method: 使用新创建的67,907个确认诈骗域名数据集进行分析，发现攻击模式，并构建仅使用域名注册数据的预测模型来预测哪些诈骗域名可能被暂停。

Result: 86.9%的域名集中在五个非主流TLD，72.9%通过单一提供商注册；发现特定注册模式包括短期活动爆发，表明自动化协调攻击；预测模型达到80.4%准确率和92.3%灵敏度。

Conclusion: 分析揭示了攻击者逃避检测的策略，可为注册商、托管提供商和安全平台提供更有针对性的干预措施，但仅靠注册元数据可能不足，结合域名URL和网页内容特征可进一步提高检测效果。

Abstract: Toll scams involve criminals registering fake domains that pretend to be
legitimate transportation agencies to trick users into making fraudulent
payments. Although these scams are rapidly increasing and causing significant
harm, they have not been extensively studied. We present the first large-scale
analysis of toll scam domains, using a newly created dataset of 67,907
confirmed scam domains mostly registered in 2025. Our study reveals that
attackers exploit permissive registrars and less common top-level domains, with
86.9% of domains concentrated in just five non-mainstream TLDs and 72.9%
registered via a single provider. We also discover specific registration
patterns, including short bursts of activity that suggest automated,
coordinated attacks, with over half of domains registered in the first quarter
of 2025. This extreme temporal clustering reflects highly synchronized campaign
launches. Additionally, we build a simple predictive model using only domain
registration data to predict which scam domains are likely to be suspended -- a
proxy for confirmed abuse -- achieving 80.4% accuracy, and 92.3% sensitivity.
Our analysis reveals attacker strategies for evading detection -- such as
exploiting obscure TLDs, permissive registrars, and coordinated registration
bursts -- which can inform more targeted interventions by registrars, hosting
providers, and security platforms. However, our results suggest that
registration metadata alone may be insufficient, and incorporating features
from domain URLs and webpage content could further improve detection.

</details>


### [28] [An Information Asymmetry Game for Trigger-based DNN Model Watermarking](https://arxiv.org/abs/2510.14218)
*Chaoyue Huang,Gejian Zhao,Hanzhou Wu,Zhihua Xia,Asad Malik*

Main category: cs.CR

TL;DR: 该论文提出了一种基于博弈论的深度神经网络水印保护方法，通过信息不对称条件下的博弈分析来设计能够抵抗修剪和微调攻击的鲁棒水印方案。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络作为有价值的数字产品面临日益严重的知识产权威胁，需要开发有效的技术措施来保护模型版权。现有的基于触发器的水印方法容易被攻击者通过修剪或微调移除。

Method: 将水印保护建模为信息不对称条件下的博弈过程，防御者利用私有知识嵌入秘密水印，攻击者只能访问带水印模型并试图移除。定义了双方策略、成本和效用函数，推导了攻击者的最优修剪预算。

Result: 实验结果表明带水印模型的可行性，稀疏水印方法能够以可忽略的精度损失抵抗移除攻击，并建立了攻击后水印检测精度的指数下界。

Conclusion: 本研究强调了博弈论分析在指导鲁棒水印方案设计方面的有效性，为模型版权保护提供了理论基础和实践指导。

Abstract: As a valuable digital product, deep neural networks (DNNs) face increasingly
severe threats to the intellectual property, making it necessary to develop
effective technical measures to protect them. Trigger-based watermarking
methods achieve copyright protection by embedding triggers into the host DNNs.
However, the attacker may remove the watermark by pruning or fine-tuning. We
model this interaction as a game under conditions of information asymmetry,
namely, the defender embeds a secret watermark with private knowledge, while
the attacker can only access the watermarked model and seek removal. We define
strategies, costs, and utilities for both players, derive the attacker's
optimal pruning budget, and establish an exponential lower bound on the
accuracy of watermark detection after attack. Experimental results demonstrate
the feasibility of the watermarked model, and indicate that sparse watermarking
can resist removal with negligible accuracy loss. This study highlights the
effectiveness of game-theoretic analysis in guiding the design of robust
watermarking schemes for model copyright protection.

</details>


### [29] [RHINO: Guided Reasoning for Mapping Network Logs to Adversarial Tactics and Techniques with Large Language Models](https://arxiv.org/abs/2510.14233)
*Fanchao Meng,Jiaping Gui,Yunbo Li,Yue Wu*

Main category: cs.CR

TL;DR: RHINO是一个基于LLM的网络攻击分析框架，通过三阶段推理过程将低级别警报映射到MITRE ATT&CK技术，显著提高了准确性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有网络入侵检测系统产生大量低级别警报，但需要人工关联到高级别攻击行为。基于规则的方法无法适应新型攻击，机器学习方法缺乏上下文意识，而现有LLM方法容易产生幻觉或去上下文化的映射。

Method: RHINO将LLM攻击分析分解为三个可解释阶段：行为抽象（将原始日志转化为情境化叙述）、多角色协作推理（基于MITRE ATT&CK知识生成候选技术）、验证（交叉引用预测与官方定义以纠正幻觉）。

Result: 在三个基准测试和四个骨干模型上评估，RHINO准确率达到86.38%至88.45%，相对增益从24.25%到76.50%不等。

Conclusion: RHINO显著增强了威胁分析的可解释性和可扩展性，为在操作安全环境中部署LLM提供了蓝图。

Abstract: Modern Network Intrusion Detection Systems generate vast volumes of low-level
alerts, yet these outputs remain semantically fragmented, requiring
labor-intensive manual correlation with high-level adversarial behaviors.
Existing solutions for automating this mapping-rule-based systems and machine
learning classifiers-suffer from critical limitations: rule-based approaches
fail to adapt to novel attack variations, while machine learning methods lack
contextual awareness and treat tactic-technique mapping as a syntactic matching
problem rather than a reasoning task. Although Large Language Models have shown
promise in cybersecurity tasks, preliminary experiments reveal that existing
LLM-based methods frequently hallucinate technique names or produce
decontextualized mappings due to their single-step classification approach.
  To address these challenges, we introduce RHINO, a novel framework that
decomposes LLM-based attack analysis into three interpretable phases mirroring
human reasoning: (1) behavioral abstraction, where raw logs are translated into
contextualized narratives; (2) multi-role collaborative inference, generating
candidate techniques by evaluating behavioral evidence against MITRE ATT&CK
knowledge; and (3) validation, cross-referencing predictions with official
MITRE definitions to rectify hallucinations. RHINO bridges the semantic gap
between low-level observations and adversarial intent while improving output
reliability through structured reasoning.
  We evaluate RHINO on three benchmarks across four backbone models. RHINO
achieved high accuracy, with model performance ranging from 86.38% to 88.45%,
resulting in relative gains from 24.25% to 76.50% across different models. Our
results demonstrate that RHINO significantly enhances the interpretability and
scalability of threat analysis, offering a blueprint for deploying LLMs in
operational security settings.

</details>


### [30] [Beyond a Single Perspective: Towards a Realistic Evaluation of Website Fingerprinting Attacks](https://arxiv.org/abs/2510.14283)
*Xinhao Deng,Jingyou Chen,Linxiao Yu,Yixiang Zhang,Zhongyi Gu,Changhao Qiu,Xiyuan Zhao,Ke Xu,Qi Li*

Main category: cs.CR

TL;DR: 本文首次系统评估网站指纹识别攻击在多种真实环境下的表现，发现现有技术在面对防御机制、流量漂移、多标签浏览等复杂条件时性能显著下降。


<details>
  <summary>Details</summary>
Motivation: 现有网站指纹识别攻击研究大多局限于单一场景，忽略了真实环境的复杂性，需要系统评估这些技术在实际应用中的有效性。

Method: 采用多维评估框架，在防御机制、流量漂移、多标签浏览、早期检测、开放世界和少样本场景等多样化真实条件下测试现有WF攻击技术。

Result: 实验结果显示，许多在孤立环境中表现优异的WF技术在面对其他条件时性能显著下降，现有攻击难以直接应用于实际环境。

Conclusion: 当前WF攻击在实际应用中存在严重局限性，需要开发更鲁棒和实用的攻击方法，本文提出的多维评估框架为此提供了重要参考。

Abstract: Website Fingerprinting (WF) attacks exploit patterns in encrypted traffic to
infer the websites visited by users, posing a serious threat to anonymous
communication systems. Although recent WF techniques achieve over 90% accuracy
in controlled experimental settings, most studies remain confined to single
scenarios, overlooking the complexity of real-world environments. This paper
presents the first systematic and comprehensive evaluation of existing WF
attacks under diverse realistic conditions, including defense mechanisms,
traffic drift, multi-tab browsing, early-stage detection, open-world settings,
and few-shot scenarios. Experimental results show that many WF techniques with
strong performance in isolated settings degrade significantly when facing other
conditions. Since real-world environments often combine multiple challenges,
current WF attacks are difficult to apply directly in practice. This study
highlights the limitations of WF attacks and introduces a multidimensional
evaluation framework, offering critical insights for developing more robust and
practical WF attacks.

</details>


### [31] [BinCtx: Multi-Modal Representation Learning for Robust Android App Behavior Detection](https://arxiv.org/abs/2510.14344)
*Zichen Liu,Shao Yang,Xusheng Xiao*

Main category: cs.CR

TL;DR: BINCTX是一个多模态学习方法，通过结合字节码图像、上下文行为触发和第三方库使用三种视图来检测移动应用中的不良行为，在真实恶意软件检测中达到94.73%的F1分数，且对混淆和对抗样本具有鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 移动应用市场中存在大量不良行为（如干扰性广告、非法重定向、支付欺骗），这些行为难以检测，因为它们通常不依赖权限保护API，且可以通过UI或元数据编辑轻松伪装。

Method: 构建应用的三种多模态表示：1）全局字节码图像视图捕获代码语义和家族模式；2）上下文视图（动作、组件、声明权限、URL/IP常量）显示行为触发方式；3）第三方库使用视图总结组件间调用路径的调用频率。三种视图嵌入融合后训练上下文感知分类器。

Result: 在真实恶意软件和良性应用上，BINCTX达到94.73%的宏F1分数，比强基线至少提升14.92%。在商业混淆后仍保持84%的F1分数，比最先进的仅字节码系统更抗对抗样本。

Conclusion: BINCTX通过多模态表示融合有效检测移动应用不良行为，对混淆和对抗攻击具有鲁棒性，显著优于现有方法。

Abstract: Mobile app markets host millions of apps, yet undesired behaviors (e.g.,
disruptive ads, illegal redirection, payment deception) remain hard to catch
because they often do not rely on permission-protected APIs and can be easily
camouflaged via UI or metadata edits. We present BINCTX, a learning approach
that builds multi-modal representations of an app from (i) a global
bytecode-as-image view that captures code-level semantics and family-style
patterns, (ii) a contextual view (manifested actions, components, declared
permissions, URL/IP constants) indicating how behaviors are triggered, and
(iii) a third-party-library usage view summarizing invocation frequencies along
inter-component call paths. The three views are embedded and fused to train a
contextual-aware classifier. On real-world malware and benign apps, BINCTX
attains a macro F1 of 94.73%, outperforming strong baselines by at least
14.92%. It remains robust under commercial obfuscation (F1 84%
post-obfuscation) and is more resistant to adversarial samples than
state-of-the-art bytecode-only systems.

</details>


### [32] [Match & Mend: Minimally Invasive Local Reassembly for Patching N-day Vulnerabilities in ARM Binaries](https://arxiv.org/abs/2510.14384)
*Sebastian Jänich,Merlin Sievers,Johannes Kinder*

Main category: cs.CR

TL;DR: 提出一种在二进制级别自动修补物联网固件中已知漏洞的技术，无需厂商支持即可修复过时的开源软件漏洞。


<details>
  <summary>Details</summary>
Motivation: 低成本物联网设备由于更新机制不完善而存在安全隐患，许多设备运行着过时且已知存在漏洞的开源软件版本。

Method: 采用最小侵入式本地重汇编技术，在二进制级别自动修补已知漏洞，旨在最小化副作用并降低引入破坏性变更的风险。

Result: 在MAGMA基准测试的108个二进制文件中成功修补83%的目标漏洞，在KARONTE数据集的30个真实物联网固件中成功修补96%的目标漏洞。

Conclusion: 该方法能够有效自动修补物联网固件中的已知漏洞，为缺乏厂商支持的设备提供安全更新解决方案。

Abstract: Low-cost Internet of Things (IoT) devices are increasingly popular but often
insecure due to poor update regimes. As a result, many devices run outdated and
known-vulnerable versions of open-source software. We address this problem by
proposing to patch IoT firmware at the binary level, without requiring vendor
support. In particular, we introduce minimally invasive local reassembly, a new
technique for automatically patching known (n-day) vulnerabilities in IoT
firmware. Our approach is designed to minimize side effects and reduce the risk
of introducing breaking changes. We systematically evaluate our approach both
on 108 binaries within the controlled environment of the MAGMA benchmarks, as
well as on 30 real-world Linux-based IoT firmware images from the KARONTE
dataset. Our prototype successfully patches 83% of targeted vulnerabilities in
MAGMA and 96% in the firmware dataset.

</details>


### [33] [Stealthy Dual-Trigger Backdoors: Attacking Prompt Tuning in LM-Empowered Graph Foundation Models](https://arxiv.org/abs/2510.14470)
*Xiaoyu Xue,Yuni Lai,Chenxi Huang,Yulin Zhu,Gaolei Li,Xiaoge Zhang,Kai Zhou*

Main category: cs.CR

TL;DR: 本文提出了一种针对语言模型赋能图基础模型的双触发器后门攻击框架，在文本级和结构级同时操作，无需显式优化触发器节点文本属性即可实现有效攻击。


<details>
  <summary>Details</summary>
Motivation: 传统图神经网络相比，语言模型赋能的图基础模型在未受保护的提示调优阶段存在独特的安全漏洞，这些漏洞在当前研究中尚未得到充分研究。

Method: 提出双触发器后门攻击框架，在文本级和结构级同时操作，通过策略性利用预建立的文本池，无需显式优化触发器节点文本属性。

Result: 实验评估表明，该攻击在保持优异清洁准确率的同时，实现了出色的攻击成功率，包括高度隐蔽的单触发器节点场景。

Conclusion: 这项工作凸显了网络部署的语言模型赋能图基础模型中的关键后门风险，有助于在基础模型时代为开源平台开发更强大的监督机制。

Abstract: The emergence of graph foundation models (GFMs), particularly those
incorporating language models (LMs), has revolutionized graph learning and
demonstrated remarkable performance on text-attributed graphs (TAGs). However,
compared to traditional GNNs, these LM-empowered GFMs introduce unique security
vulnerabilities during the unsecured prompt tuning phase that remain
understudied in current research. Through empirical investigation, we reveal a
significant performance degradation in traditional graph backdoor attacks when
operating in attribute-inaccessible constrained TAG systems without explicit
trigger node attribute optimization. To address this, we propose a novel
dual-trigger backdoor attack framework that operates at both text-level and
struct-level, enabling effective attacks without explicit optimization of
trigger node text attributes through the strategic utilization of a
pre-established text pool. Extensive experimental evaluations demonstrate that
our attack maintains superior clean accuracy while achieving outstanding attack
success rates, including scenarios with highly concealed single-trigger nodes.
Our work highlights critical backdoor risks in web-deployed LM-empowered GFMs
and contributes to the development of more robust supervision mechanisms for
open-source platforms in the era of foundation models.

</details>


### [34] [Certifying optimal MEV strategies with Lean](https://arxiv.org/abs/2510.14480)
*Massimo Bartoletti,Riccardo Marchesin,Roberto Zunino*

Main category: cs.CR

TL;DR: 本文首次在Lean定理证明器中实现了MEV（最大可提取价值）的形式化建模，开发了一种构建机器检查证明的方法来验证MEV上界，并证明了自动做市商中三明治攻击的最优性。


<details>
  <summary>Details</summary>
Motivation: MEV攻击已从去中心化应用中提取了数十亿美元价值，但验证MEV攻击的缺失需要确定合适的上界，这个问题极其困难，因为对抗策略空间极其庞大，现有经验研究和手动推理不够严谨。

Method: 在Lean定理证明器中实现MEV的机械化形式化，引入构建机器检查证明的方法论，建模并分析两个典型DeFi协议的MEV，特别关注自动做市商中的三明治攻击。

Result: 开发了第一个机器检查证明，证明了自动做市商中三明治攻击的最优性，为MEV边界提供了超越现有技术的正确性保证。

Conclusion: 该方法为MEV分析提供了严格的机械化框架，能够为DeFi协议的安全性提供更强的形式化保证。

Abstract: Maximal Extractable Value (MEV) refers to a class of attacks to decentralized
applications where the adversary profits by manipulating the ordering,
inclusion, or exclusion of transactions in a blockchain. Decentralized Finance
(DeFi) protocols are a primary target of these attacks, as their logic depends
critically on transaction sequencing. To date, MEV attacks have already
extracted billions of dollars in value, underscoring their systemic impact on
blockchain security. Verifying the absence of MEV attacks requires determining
suitable upper bounds, i.e. proving that no adversarial strategy can extract
more value (if any) than expected by protocol designers. This problem is
notoriously difficult: the space of adversarial strategies is extremely vast,
making empirical studies and pen-and-paper reasoning insufficiently rigorous.
In this paper, we present the first mechanized formalization of MEV in the Lean
theorem prover. We introduce a methodology to construct machine-checked proofs
of MEV bounds, providing correctness guarantees beyond what is possible with
existing techniques. To demonstrate the generality of our approach, we model
and analyse the MEV of two paradigmatic DeFi protocols. Notably, we develop the
first machine-checked proof of the optimality of sandwich attacks in Automated
Market Makers, a fundamental DeFi primitive.

</details>


### [35] [Lexo: Eliminating Stealthy Supply-Chain Attacks via LLM-Assisted Program Regeneration](https://arxiv.org/abs/2510.14522)
*Evangelos Lamprou,Julian Dai,Grigoris Ntousakis,Martin C. Rinard,Nikos Vasilakis*

Main category: cs.CR

TL;DR: Lexo是一个自动学习和重新生成无漏洞版本组件的系统，用于防御软件供应链攻击。它通过生成输入-输出对来建模组件行为，然后合成新版本组件，保持原有功能但消除恶意行为。


<details>
  <summary>Details</summary>
Motivation: 开源软件生态系统中的软件供应链攻击是一个持续关注的问题，这些攻击在保持组件标准功能的同时隐藏恶意功能，只在目标环境中激活。

Method: Lexo首先生成输入-输出对来建模组件的完整可观察行为，然后使用这些数据合成原始组件的新版本。过程中咨询多个大型语言模型实例，使用正确性和覆盖率指标来指导这些实例并保护其结果。

Result: 在100多个真实世界包（包括高知名度的隐蔽供应链攻击）上的评估表明，Lexo能够跨多个领域扩展，平均在100秒内高效重新生成代码，保持兼容性，并在多个真实世界供应链攻击中成功消除恶意代码。

Conclusion: Lexo能够有效防御隐蔽的软件供应链攻击，即使在最先进的LLM提示消除恶意代码失败的情况下也能成功。

Abstract: Software supply-chain attacks are an important and ongoing concern in the
open source software ecosystem. These attacks maintain the standard
functionality that a component implements, but additionally hide malicious
functionality activated only when the component reaches its target environment.
Lexo addresses such stealthy attacks by automatically learning and regenerating
vulnerability-free versions of potentially malicious components. Lexo first
generates a set of input-output pairs to model a component's full observable
behavior, which it then uses to synthesize a new version of the original
component. The new component implements the original functionality but avoids
stealthy malicious behavior. Throughout this regeneration process, Lexo
consults several distinct instances of Large Language Models (LLMs), uses
correctness and coverage metrics to shepherd these instances, and guardrails
their results. Our evaluation on 100+ real-world packages, including high
profile stealthy supply-chain attacks, indicates that Lexo scales across
multiple domains, regenerates code efficiently (<100s on average), maintains
compatibility, and succeeds in eliminating malicious code in several real-world
supply-chain-attacks, even in cases when a state-of-the-art LLM fails to
eliminate malicious code when prompted to do so.

</details>


### [36] [Symbolic verification of Apple's Find My location-tracking protocol](https://arxiv.org/abs/2510.14589)
*Vaishnavi Sundararajan,Rithwik*

Main category: cs.CR

TL;DR: 本文对苹果Find My追踪系统进行了形式化安全分析，通过符号建模和机器验证证明其隐私保护属性。


<details>
  <summary>Details</summary>
Motivation: 苹果Find My系统虽然声称安全私密，但代码闭源，需要通过形式化验证来检验其安全性声明。即使有完美密码学保证，协议设计仍可能存在逻辑漏洞。

Method: 构建Find My协议的符号模型，制定精确的形式化规范，使用Tamarin证明器进行自动化机器可验证证明。

Result: 通过形式化验证证明了Find My协议满足期望的安全属性。

Conclusion: 通过形式化方法验证了Find My系统的隐私保护机制，为闭源系统的安全评估提供了可靠途径。

Abstract: Tracking devices, while designed to help users find their belongings in case
of loss/theft, bring in new questions about privacy and surveillance of not
just their own users, but in the case of crowd-sourced location tracking, even
that of others even orthogonally associated with these platforms. Apple's Find
My is perhaps the most ubiquitous such system which can even locate devices
which do not possess any cellular support or GPS, running on millions of
devices worldwide. Apple claims that this system is private and secure, but the
code is proprietary, and such claims have to be taken on faith. It is well
known that even with perfect cryptographic guarantees, logical flaws might
creep into protocols, and allow undesirable attacks. In this paper, we present
a symbolic model of the Find My protocol, as well as a precise formal
specification of desirable properties, and provide automated, machine-checkable
proofs of these properties in the Tamarin prover.

</details>


### [37] [Improving Cybercrime Detection and Digital Forensics Investigations with Artificial Intelligence](https://arxiv.org/abs/2510.14638)
*Silvia Lucia Sanna,Leonardo Regano,Davide Maiorca,Giorgio Giacinto*

Main category: cs.CR

TL;DR: 该论文探讨了如何利用人工智能改进网络犯罪分析和数字取证，同时警示AI可能被网络犯罪分子用于开发更高级的攻击技术。


<details>
  <summary>Details</summary>
Motivation: 网络犯罪在欧洲仍然频发，需要改进预防、检测和分析方法。AI技术可以增强网络犯罪检测系统和数字取证分析，但同时也可能被犯罪分子利用。

Method: 提出将AI作为额外工具集成到网络犯罪检测和数字取证系统中，并通过案例研究展示如何使用Gemini、Copilot和chatGPT等聊天机器人开发隐写术代码。

Result: 研究表明AI可以有效改进网络犯罪分析和数字取证程序，但同时也存在被网络犯罪分子用于开发反取证技术的风险。

Conclusion: AI在网络犯罪分析和数字取证中具有双重作用：既能增强防御能力，也可能被犯罪分子利用来改进攻击技术，需要谨慎应用。

Abstract: According to a recent EUROPOL report, cybercrime is still recurrent in
Europe, and different activities and countermeasures must be taken to limit,
prevent, detect, analyze, and fight it. Cybercrime must be prevented with
specific measures, tools, and techniques, for example through automated network
and malware analysis. Countermeasures against cybercrime can also be improved
with proper \df analysis in order to extract data from digital devices trying
to retrieve information on the cybercriminals. Indeed, results obtained through
a proper \df analysis can be leveraged to train cybercrime detection systems to
prevent the success of similar crimes. Nowadays, some systems have started to
adopt Artificial Intelligence (AI) algorithms for cyberattack detection and \df
analysis improvement. However, AI can be better applied as an additional
instrument in these systems to improve the detection and in the \df analysis.
For this reason, we highlight how cybercrime analysis and \df procedures can
take advantage of AI. On the other hand, cybercriminals can use these systems
to improve their skills, bypass automatic detection, and develop advanced
attack techniques. The case study we presented highlights how it is possible to
integrate the use of the three popular chatbots {\tt Gemini}, {\tt Copilot} and
{\tt chatGPT} to develop a Python code to encode and decoded images with
steganographic technique, even though their presence is not an indicator of
crime, attack or maliciousness but used by a cybercriminal as anti-forensics
technique.

</details>


### [38] [AEX-NStep: Probabilistic Interrupt Counting Attacks on Intel SGX](https://arxiv.org/abs/2510.14675)
*Nicolas Dutly,Friederike Groschupp,Ivan Puddu,Kari Kostiainen,Srdjan Capkun*

Main category: cs.CR

TL;DR: AEX-NStep攻击展示了Intel SGX的AEX-Notify扩展无法完全防止中断计数攻击，即使阻止了确定性单步执行。攻击者仍能通过概率性中断计数泄露ECDSA密钥。


<details>
  <summary>Details</summary>
Motivation: Intel引入AEX-Notify来缓解基于中断的单步攻击（如SGX-Step），但该研究旨在验证AEX-Notify是否能真正防止中断计数攻击。

Method: 开发了AEX-NStep攻击，这是首个针对AEX-Notify防护的中断计数攻击。通过两种新的概率性中断计数攻击方法，证明即使没有确定性单步执行，攻击仍然可行。

Result: 成功攻破了AEX-Notify的安全保证之一——混淆前进进度，并构建了实用的ECDSA密钥泄露攻击，证明AEX-Notify不能完全防止此类攻击。

Conclusion: AEX-Notify不能完全防止中断计数攻击，研究结果为未来缓解措施的设计提供了重要参考，扩展了AEX-Notify的原始安全分析。

Abstract: To mitigate interrupt-based stepping attacks (notably using SGX-Step), Intel
introduced AEX-Notify, an ISA extension to Intel SGX that aims to prevent
deterministic single-stepping. In this work, we introduce AEX-NStep, the first
interrupt counting attack on AEX-Notify-enabled Enclaves. We show that
deterministic single-stepping is not required for interrupt counting attacks to
be practical and that, therefore, AEX-Notify does not entirely prevent such
attacks. We specifically show that one of AEX-Notify's security guarantees,
obfuscated forward progress, does not hold, and we introduce two new
probabilistic interrupt counting attacks. We use these attacks to construct a
practical ECDSA key leakage attack on an AEX-Notify-enabled SGX enclave. Our
results extend the original security analysis of AEX-Notify and inform the
design of future mitigations.

</details>


### [39] [FibRace: a large-scale benchmark of client-side proving on mobile devices](https://arxiv.org/abs/2510.14693)
*Simon Malatrait,Alex Sirac*

Main category: cs.CR

TL;DR: FibRace是首个在智能手机上使用Cairo M进行客户端证明生成的大规模实验，通过移动游戏形式让玩家证明斐波那契数列并参与排行榜竞争。实验结果显示现代智能手机能在5秒内完成证明，证实移动设备能够可靠生成零知识证明。


<details>
  <summary>Details</summary>
Motivation: 测试智能手机客户端证明生成的可行性，为轻量级证明器、证明驱动基础设施和隐私保护移动应用提供实践基准。

Method: 开发移动游戏FibRace，让玩家在智能手机上使用Cairo M生成斐波那契数列的零知识证明，通过排行榜机制收集性能数据。

Result: 在3周活动中，6,047名玩家在1,420种设备型号上生成了2,195,488个证明。大多数现代智能手机能在5秒内完成证明，性能主要与RAM容量和SoC性能相关。

Conclusion: 移动设备现在能够可靠生成零知识证明，无需远程证明器或专用硬件。FibRace为未来研究提供了最全面的移动证明性能数据集。

Abstract: FibRace, jointly developed by KKRT Labs and Hyli, was the first large-scale
experiment to test client-side proof generation on smartphones using Cairo M.
Presented as a mobile game in which players proved Fibonacci numbers and
climbed a leaderboard, FibRace served a dual purpose: to engage the public and
to provide empirical benchmarking. Over a three-week campaign (September 11-30,
2025), 6,047 players across 99 countries generated 2,195,488 proofs on 1,420
unique device models. The results show that most modern smartphones can
complete a proof in under 5 seconds, confirming that *mobile devices are now
capable of producing zero-knowledge proofs reliably*, without the need for
remote provers or specialized hardware. Performance was correlated primarily
with RAM capacity and SoC (System on Chip) performance: devices with at least 3
GB of RAM proved stably, when Apple's A19 Pro and M-series chips achieved the
fastest proving times. Hyli's blockchain natively verified every proof onchain
without congestion. FibRace provides the most comprehensive dataset to date on
mobile proving performance, establishing a practical baseline for future
research in lightweight provers, proof-powered infrastructure, and
privacy-preserving mobile applications.

</details>


### [40] [SLIE: A Secure and Lightweight Cryptosystem for Data Sharing in IoT Healthcare Services](https://arxiv.org/abs/2510.14708)
*Ha Xuan Son,Nguyen Quoc Anh,Phat T. Tran-Truong,Le Thanh Tuan,Pham Thanh Nghiem*

Main category: cs.CR

TL;DR: 提出了SLIE（安全轻量级身份加密）系统，基于WKD-IBE密码系统，为医疗物联网提供可扩展的信任和安全的全向通信，显著优于RSA性能。


<details>
  <summary>Details</summary>
Motivation: 医疗物联网的服务导向模型引入了设备管理和通信中的重大安全漏洞，特别是考虑到医疗数据的敏感性，需要解决这些安全风险。

Method: 基于Wildcard Key Derivation Identity-Based Encryption (WKD-IBE)构建SLIE密码系统，采用端到端加密、分层访问控制和轻量级密钥管理，包含恒定时间操作、内存混淆和基于过期的密钥撤销机制。

Result: SLIE在1KB数据上的加密和解密时间分别为0.936ms和0.217ms，加密速度提升84.54%，解密速度提升99.70%，能效为0.014 J/KB，显著优于RSA。

Conclusion: SLIE系统有效应对侧信道、中间人和未授权访问攻击，确保符合HIPAA和GDPR标准，为资源受限的医疗物联网设备提供了安全高效的解决方案。

Abstract: The Internet of Medical Things (IoMT) has revolutionized healthcare by
transforming medical operations into standardized, interoperable services.
However, this service-oriented model introduces significant security
vulnerabilities in device management and communication, which are especially
critical given the sensitivity of medical data. To address these risks, this
paper proposes SLIE (Secure and Lightweight Identity Encryption), a novel
cryptosystem based on Wildcard Key Derivation Identity-Based Encryption
(WKD-IBE). SLIE ensures scalable trust and secure omnidirectional communication
through end-to-end encryption, hierarchical access control, and a lightweight
key management system designed for resource-constrained devices. It
incorporates constant-time operations, memory obfuscation, and expiry-based key
revocation to counter side-channel, man-in-the-middle, and unauthorized access
attacks, thereby ensuring compliance with standards like HIPAA and GDPR.
Evaluations show that SLIE significantly outperforms RSA, with encryption and
decryption times of 0.936ms and 0.217ms for 1KB of data, an 84.54% improvement
in encryption speed, a 99.70% improvement in decryption speed, and an energy
efficiency of 0.014 J/KB.

</details>


### [41] [Secure Sparse Matrix Multiplications and their Applications to Privacy-Preserving Machine Learning](https://arxiv.org/abs/2510.14894)
*Marc Damie,Florian Hahn,Andreas Peter,Jan Ramon*

Main category: cs.CR

TL;DR: 提出了针对稀疏数据的多方计算矩阵乘法算法，解决了现有MPC框架在处理高维稀疏数据时的内存和通信效率问题，并在推荐系统和基因组学等应用中验证了有效性。


<details>
  <summary>Details</summary>
Motivation: 现有MPC框架未针对稀疏数据进行优化，无法处理推荐系统、基因组学等高维稀疏数据的机器学习应用，因为密集数据表示会导致内存需求过大。

Method: 提出了秘密稀疏矩阵乘法算法，避免经典安全矩阵乘法算法的密集数据表示内存问题，并显著降低通信成本。同时提出了基于幂律分布的安全上界方法，更符合稀疏数据的统计现实。

Result: 算法避免了内存问题，通信成本显著降低（某些实验显示1000倍减少），在现有协议不实用的两个ML应用中验证了可行性。

Conclusion: 提出的稀疏矩阵MPC算法解决了现有框架在处理稀疏数据时的局限性，通过采用符合统计现实的安全上界方法，为稀疏数据机器学习应用提供了实用的隐私保护解决方案。

Abstract: To preserve privacy, multi-party computation (MPC) enables executing Machine
Learning (ML) algorithms on secret-shared or encrypted data. However, existing
MPC frameworks are not optimized for sparse data. This makes them unsuitable
for ML applications involving sparse data, e.g., recommender systems or
genomics. Even in plaintext, such applications involve high-dimensional sparse
data, that cannot be processed without sparsity-related optimizations due to
prohibitively large memory requirements.
  Since matrix multiplication is central in ML algorithms, we propose MPC
algorithms to multiply secret sparse matrices. On the one hand, our algorithms
avoid the memory issues of the "dense" data representation of classic secure
matrix multiplication algorithms. On the other hand, our algorithms can
significantly reduce communication costs (some experiments show a factor 1000)
for realistic problem sizes. We validate our algorithms in two ML applications
in which existing protocols are impractical.
  An important question when developing MPC algorithms is what assumptions can
be made. In our case, if the number of non-zeros in a row is a sensitive piece
of information then a short runtime may reveal that the number of non-zeros is
small. Existing approaches make relatively simple assumptions, e.g., that there
is a universal upper bound to the number of non-zeros in a row. This often
doesn't align with statistical reality, in a lot of sparse datasets the amount
of data per instance satisfies a power law. We propose an approach which allows
adopting a safe upper bound on the distribution of non-zeros in rows/columns of
sparse matrices.

</details>


### [42] [A Hard-Label Black-Box Evasion Attack against ML-based Malicious Traffic Detection Systems](https://arxiv.org/abs/2510.14906)
*Zixuan Liu,Yi Zhao,Zhuotao Liu,Qi Li,Chuanpu Fu,Guangmeng Zhou,Ke Xu*

Main category: cs.CR

TL;DR: NetMasquerade是一个基于强化学习的恶意流量检测逃避攻击方法，通过模仿良性流量模式来操纵恶意数据包序列，能在黑盒场景下有效规避多种检测方法。


<details>
  <summary>Details</summary>
Motivation: 现有的机器学习恶意流量检测方法虽然优于传统基于规则的检测，但其鲁棒性研究不足，且现有逃避攻击方法依赖过多限制条件或目标内部知识，在真实黑盒场景中不实用。

Method: 开发了NetMasquerade系统，利用强化学习操纵攻击流量模仿良性流量。首先建立专门预训练模型Traffic-BERT，使用网络专用分词器和注意力机制提取良性流量模式，然后将其集成到RL框架中，以最小修改有效操纵恶意数据包序列。

Result: 实验结果表明，NetMasquerade能够在80种攻击场景下规避6种现有检测方法，攻击成功率超过96.65%，并能规避对现有逃避攻击具有经验性或可证明鲁棒性的方法，同时实现低延迟对抗流量生成。

Conclusion: NetMasquerade证明了在黑盒场景下进行硬标签逃避攻击的可行性，展示了机器学习恶意流量检测系统在实际部署中面临的严重安全威胁。

Abstract: Machine Learning (ML)-based malicious traffic detection is a promising
security paradigm. It outperforms rule-based traditional detection by
identifying various advanced attacks. However, the robustness of these ML
models is largely unexplored, thereby allowing attackers to craft adversarial
traffic examples that evade detection. Existing evasion attacks typically rely
on overly restrictive conditions (e.g., encrypted protocols, Tor, or
specialized setups), or require detailed prior knowledge of the target (e.g.,
training data and model parameters), which is impractical in realistic
black-box scenarios. The feasibility of a hard-label black-box evasion attack
(i.e., applicable across diverse tasks and protocols without internal target
insights) thus remains an open challenge. To this end, we develop
NetMasquerade, which leverages reinforcement learning (RL) to manipulate attack
flows to mimic benign traffic and evade detection. Specifically, we establish a
tailored pre-trained model called Traffic-BERT, utilizing a network-specialized
tokenizer and an attention mechanism to extract diverse benign traffic
patterns. Subsequently, we integrate Traffic-BERT into the RL framework,
allowing NetMasquerade to effectively manipulate malicious packet sequences
based on benign traffic patterns with minimal modifications. Experimental
results demonstrate that NetMasquerade enables both brute-force and stealthy
attacks to evade 6 existing detection methods under 80 attack scenarios,
achieving over 96.65% attack success rate. Notably, it can evade the methods
that are either empirically or certifiably robust against existing evasion
attacks. Finally, NetMasquerade achieves low-latency adversarial traffic
generation, demonstrating its practicality in real-world scenarios.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [43] [Decision Oriented Technique (DOTechnique): Finding Model Validity Through Decision-Maker Context](https://arxiv.org/abs/2510.13858)
*Raheleh Biglari,Joachim Denil*

Main category: cs.AI

TL;DR: 提出DOTechnique方法，基于决策一致性而非输出相似性来确定模型有效性，通过评估替代模型是否与高保真模型产生等效决策来识别有效区域。


<details>
  <summary>Details</summary>
Motivation: 传统方法依赖预定义的有效性框架，但这些框架可能不可用或不充分，特别是在指导决策过程中模型有效性至关重要。

Method: DOTechnique方法整合领域约束和符号推理来缩小搜索空间，通过决策一致性评估模型有效性，即使没有明确的有效性边界也能工作。

Result: 以高速公路变道系统为例，展示了DOTechnique能够发现仿真模型的有效性区域。

Conclusion: 该技术有潜力通过决策者上下文来支持发现模型有效性。

Abstract: Model validity is as critical as the model itself, especially when guiding
decision-making processes. Traditional approaches often rely on predefined
validity frames, which may not always be available or sufficient. This paper
introduces the Decision Oriented Technique (DOTechnique), a novel method for
determining model validity based on decision consistency rather than output
similarity. By evaluating whether surrogate models lead to equivalent decisions
compared to high-fidelity models, DOTechnique enables efficient identification
of validity regions, even in the absence of explicit validity boundaries. The
approach integrates domain constraints and symbolic reasoning to narrow the
search space, enhancing computational efficiency. A highway lane change system
serves as a motivating example, demonstrating how DOTechnique can uncover the
validity region of a simulation model. The results highlight the potential of
the technique to support finding model validity through decision-maker context.

</details>


### [44] [Do Slides Help? Multi-modal Context for Automatic Transcription of Conference Talks](https://arxiv.org/abs/2510.13979)
*Supriti Sinhamahapatra,Jan Niehues*

Main category: cs.AI

TL;DR: 该论文提出了一种融合视觉信息（特别是演示文稿幻灯片）的多模态自动语音识别方法，相比仅使用音频的基线模型，在领域特定术语识别上实现了35%的相对词错误率降低。


<details>
  <summary>Details</summary>
Motivation: 当前最先进的ASR系统主要依赖音频信息而忽略了多模态上下文。视觉信息对于消歧和适应至关重要，特别是在科学演示场景中，演示文稿幻灯片包含重要的领域特定术语信息。

Method: 1. 创建多模态演示基准数据集，包含领域特定术语的自动分析；2. 探索增强语音模型的多模态方法；3. 通过数据增强方法解决缺乏配套幻灯片数据集的问题；4. 使用增强数据集训练模型。

Result: 相比基线模型，在所有词汇上实现了约34%的相对词错误率降低，在领域特定术语上实现了35%的相对词错误率降低。

Conclusion: 集成演示文稿幻灯片等视觉信息可以显著提升ASR系统在科学演示场景中的性能，特别是在领域特定术语识别方面。

Abstract: State-of-the-art (SOTA) Automatic Speech Recognition (ASR) systems primarily
rely on acoustic information while disregarding additional multi-modal context.
However, visual information are essential in disambiguation and adaptation.
While most work focus on speaker images to handle noise conditions, this work
also focuses on integrating presentation slides for the use cases of scientific
presentation.
  In a first step, we create a benchmark for multi-modal presentation including
an automatic analysis of transcribing domain-specific terminology. Next, we
explore methods for augmenting speech models with multi-modal information. We
mitigate the lack of datasets with accompanying slides by a suitable approach
of data augmentation. Finally, we train a model using the augmented dataset,
resulting in a relative reduction in word error rate of approximately 34%,
across all words and 35%, for domain-specific terms compared to the baseline
model.

</details>


### [45] [Do Large Language Models Show Biases in Causal Learning? Insights from Contingency Judgment](https://arxiv.org/abs/2510.13985)
*María Victoria Carro,Denise Alejandra Mester,Francisca Gauna Selasco,Giovanni Franco Gabriel Marraffini,Mario Alejandro Leiva,Gerardo I. Simari,María Vanina Martinez*

Main category: cs.AI

TL;DR: 研究发现大型语言模型在零相关情境中会系统性地产生因果幻觉，错误推断不存在的因果关系，这表明它们可能只是复制因果语言而非真正理解因果关系。


<details>
  <summary>Details</summary>
Motivation: 探讨大型语言模型是否容易产生因果幻觉，特别是在经典的认知科学范式——列联判断任务中，以了解LLMs是否真正理解因果关系。

Method: 构建了1000个医疗情境下的零相关场景数据集，在这些情境中可用信息不足以建立变量间的因果关系，然后提示LLMs评估潜在原因的有效性。

Result: 所有评估的模型都系统性地推断出无根据的因果关系，显示出对因果幻觉的强烈易感性。

Conclusion: 研究结果支持LLMs可能只是复制因果语言而非真正理解因果关系的假设，并对在需要准确因果推理的领域中使用语言模型提出了担忧。

Abstract: Causal learning is the cognitive process of developing the capability of
making causal inferences based on available information, often guided by
normative principles. This process is prone to errors and biases, such as the
illusion of causality, in which people perceive a causal relationship between
two variables despite lacking supporting evidence. This cognitive bias has been
proposed to underlie many societal problems, including social prejudice,
stereotype formation, misinformation, and superstitious thinking. In this work,
we examine whether large language models are prone to developing causal
illusions when faced with a classic cognitive science paradigm: the contingency
judgment task. To investigate this, we constructed a dataset of 1,000 null
contingency scenarios (in which the available information is not sufficient to
establish a causal relationship between variables) within medical contexts and
prompted LLMs to evaluate the effectiveness of potential causes. Our findings
show that all evaluated models systematically inferred unwarranted causal
relationships, revealing a strong susceptibility to the illusion of causality.
While there is ongoing debate about whether LLMs genuinely understand causality
or merely reproduce causal language without true comprehension, our findings
support the latter hypothesis and raise concerns about the use of language
models in domains where accurate causal reasoning is essential for informed
decision-making.

</details>


### [46] [GammaZero: Learning To Guide POMDP Belief Space Search With Graph Representations](https://arxiv.org/abs/2510.14035)
*Rajesh Mangannavar,Prasad Tadepalli*

Main category: cs.AI

TL;DR: GammaZero提出了一种基于动作中心图表示的方法，用于在部分可观测马尔可夫决策过程中指导规划学习，能够实现跨问题规模的零样本泛化。


<details>
  <summary>Details</summary>
Motivation: 现有方法需要领域特定的神经网络架构且难以扩展，GammaZero旨在开发一种统一的图表示框架，使在小问题上学习的结构模式能够迁移到更大规模的问题实例。

Method: 使用图神经网络和编码器-解码器架构，将信念状态转换为动作中心图，从专家演示中学习价值函数和策略，然后将学习到的启发式方法应用于指导蒙特卡洛树搜索。

Result: 在标准POMDP基准测试中，GammaZero在相同规模问题上与BetaZero性能相当，同时能够零样本泛化到训练时未见过的2-4倍大规模问题，保持解质量的同时减少搜索需求。

Conclusion: GammaZero通过动作中心图表示实现了POMDP规划中的可扩展性和泛化能力，为处理大规模部分可观测环境提供了有效解决方案。

Abstract: We introduce an action-centric graph representation framework for learning to
guide planning in Partially Observable Markov Decision Processes (POMDPs).
Unlike existing approaches that require domain-specific neural architectures
and struggle with scalability, GammaZero leverages a unified graph-based belief
representation that enables generalization across problem sizes within a
domain. Our key insight is that belief states can be systematically transformed
into action-centric graphs where structural patterns learned on small problems
transfer to larger instances. We employ a graph neural network with a decoder
architecture to learn value functions and policies from expert demonstrations
on computationally tractable problems, then apply these learned heuristics to
guide Monte Carlo tree search on larger problems. Experimental results on
standard POMDP benchmarks demonstrate that GammaZero achieves comparable
performance to BetaZero when trained and tested on the same-sized problems,
while uniquely enabling zero-shot generalization to problems 2-4 times larger
than those seen during training, maintaining solution quality with reduced
search requirements.

</details>


### [47] [Position: Require Frontier AI Labs To Release Small "Analog" Models](https://arxiv.org/abs/2510.14053)
*Shriyash Upadhyay,Chaithanya Bandi,Narmeen Oozeer,Philip Quirke*

Main category: cs.AI

TL;DR: 提出一种替代性AI监管方法：要求大型AI实验室发布小型开放类比模型，这些模型是从其最大专有模型训练和蒸馏而来的缩小版本，既能确保AI安全又促进创新。


<details>
  <summary>Details</summary>
Motivation: 现有前沿AI模型监管提案因安全-创新权衡问题而被搁置，需要找到既能确保AI安全又不阻碍创新的监管方法。

Method: 强制要求大型AI实验室发布小型开放类比模型，作为公共代理，让广泛的研究社区能够参与安全验证、可解释性研究和算法透明度工作，而无需披露完整规模模型。

Result: 研究表明，使用这些小型模型开发的安全和可解释性方法能够有效泛化到前沿规模系统，显著降低监管负担并加速安全进展。

Conclusion: 这种监管方法成本最低，利用可重用资源，同时为公共福利做出重要贡献，通过加深对模型的理解来缓解安全-创新权衡，实现安全与创新的双赢。

Abstract: Recent proposals for regulating frontier AI models have sparked concerns
about the cost of safety regulation, and most such regulations have been
shelved due to the safety-innovation tradeoff. This paper argues for an
alternative regulatory approach that ensures AI safety while actively promoting
innovation: mandating that large AI laboratories release small, openly
accessible analog models (scaled-down versions) trained similarly to and
distilled from their largest proprietary models.
  Analog models serve as public proxies, allowing broad participation in safety
verification, interpretability research, and algorithmic transparency without
forcing labs to disclose their full-scale models. Recent research demonstrates
that safety and interpretability methods developed using these smaller models
generalize effectively to frontier-scale systems. By enabling the wider
research community to directly investigate and innovate upon accessible
analogs, our policy substantially reduces the regulatory burden and accelerates
safety advancements.
  This mandate promises minimal additional costs, leveraging reusable resources
like data and infrastructure, while significantly contributing to the public
good. Our hope is not only that this policy be adopted, but that it illustrates
a broader principle supporting fundamental research in machine learning: deeper
understanding of models relaxes the safety-innovation tradeoff and lets us have
more of both.

</details>


### [48] [Generating Fair Consensus Statements with Social Choice on Token-Level MDPs](https://arxiv.org/abs/2510.14106)
*Carter Blair,Kate Larson*

Main category: cs.AI

TL;DR: 本文提出了一种基于多目标马尔可夫决策过程和社会选择理论的共识声明生成框架，通过token级奖励和搜索算法实现可证明的公平性保证。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型共识声明生成框架缺乏提供可证明公平性保证的结构，无法在聚合多样化自由形式意见时确保公平性。

Method: 将任务建模为多目标token级MDP，每个目标对应一个代理的偏好；提出两种基于社会选择理论的方法：保证ex-ante核心的随机生成策略和基于平等主义福利最大化的搜索算法。

Result: 实验表明，基于平等主义目标的搜索算法生成的共识声明在代理对齐的最坏情况下优于基线方法，包括Habermas Machine。

Conclusion: 该框架为共识声明生成提供了正式结构，能够通过社会选择理论原则实现可证明的公平性保证。

Abstract: Current frameworks for consensus statement generation with large language
models lack the inherent structure needed to provide provable fairness
guarantees when aggregating diverse free-form opinions. We model the task as a
multi-objective, token-level Markov Decision Process (MDP), where each
objective corresponds to an agent's preference. Token-level rewards for each
agent are derived from their policy (e.g., a personalized language model). This
approach utilizes the finding that such policies implicitly define optimal
Q-functions, providing a principled way to quantify rewards at each generation
step without a value function (Rafailov et al., 2024). This MDP formulation
creates a formal structure amenable to analysis using principles from social
choice theory. We propose two approaches grounded in social choice theory.
First, we propose a stochastic generation policy guaranteed to be in the
ex-ante core, extending core stability concepts from voting theory to text
generation. This policy is derived from an underlying distribution over
complete statements that maximizes proportional fairness (Nash Welfare).
Second, for generating a single statement, we target the maximization of
egalitarian welfare using search algorithms within the MDP framework.
Empirically, experiments using language models to instantiate agent policies
show that search guided by the egalitarian objective generates consensus
statements with improved worst-case agent alignment compared to baseline
methods, including the Habermas Machine (Tessler et al., 2024).

</details>


### [49] [STEMS: Spatial-Temporal Enhanced Safe Multi-Agent Coordination for Building Energy Management](https://arxiv.org/abs/2510.14112)
*Huiliang Zhang,Di Wu,Arnaud Zinflou,Benoit Boulet*

Main category: cs.AI

TL;DR: 提出STEMS框架，一种安全约束的多智能体强化学习方法，用于协调建筑能源管理，通过空间-时间图表示学习和控制屏障函数确保安全，在真实数据集上显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 多建筑能源系统面临空间-时间依赖性利用不足、缺乏严格安全保证和系统复杂性三大挑战，需要开发既能有效利用时空信息又能确保操作安全的新方法。

Method: STEMS框架包含两个核心组件：(1) 使用GCN-Transformer融合架构的空间-时间图表示学习框架，捕捉建筑间关系和时序模式；(2) 结合控制屏障函数的安全约束多智能体强化学习算法，提供数学安全保证。

Result: 在真实建筑数据集上的实验表明，STEMS相比现有方法实现了21%成本降低、18%排放减少，安全违规从35.1%大幅降至5.6%，同时仅0.13的不适比例保持最优舒适度。

Conclusion: STEMS框架在协调建筑能源管理中表现出卓越性能，在极端天气条件下具有强鲁棒性，且在不同建筑类型中保持有效性，为安全约束的多建筑能源管理提供了有效解决方案。

Abstract: Building energy management is essential for achieving carbon reduction goals,
improving occupant comfort, and reducing energy costs. Coordinated building
energy management faces critical challenges in exploiting spatial-temporal
dependencies while ensuring operational safety across multi-building systems.
Current multi-building energy systems face three key challenges: insufficient
spatial-temporal information exploitation, lack of rigorous safety guarantees,
and system complexity. This paper proposes Spatial-Temporal Enhanced Safe
Multi-Agent Coordination (STEMS), a novel safety-constrained multi-agent
reinforcement learning framework for coordinated building energy management.
STEMS integrates two core components: (1) a spatial-temporal graph
representation learning framework using a GCN-Transformer fusion architecture
to capture inter-building relationships and temporal patterns, and (2) a
safety-constrained multi-agent RL algorithm incorporating Control Barrier
Functions to provide mathematical safety guarantees. Extensive experiments on
real-world building datasets demonstrate STEMS's superior performance over
existing methods, showing that STEMS achieves 21% cost reduction, 18% emission
reduction, and dramatically reduces safety violations from 35.1% to 5.6% while
maintaining optimal comfort with only 0.13 discomfort proportion. The framework
also demonstrates strong robustness during extreme weather conditions and
maintains effectiveness across different building types.

</details>


### [50] [Formalizing the Safety, Security, and Functional Properties of Agentic AI Systems](https://arxiv.org/abs/2510.14133)
*Edoardo Allegrini,Ananth Shreekumar,Z. Berkay Celik*

Main category: cs.AI

TL;DR: 提出了一个用于多智能体AI系统的统一建模框架，包含主机智能体模型和任务生命周期模型，定义了31个系统属性，支持形式化验证以确保系统正确性和可靠性。


<details>
  <summary>Details</summary>
Motivation: 当前多智能体AI系统的通信协议碎片化，存在语义鸿沟，无法进行严格的系统属性分析，存在架构错位和可被利用的协调问题等风险。

Method: 引入两个基础模型：主机智能体模型（负责与用户交互、任务分解和编排）和任务生命周期模型（详细描述子任务从创建到完成的状态转换），并定义了17个主机智能体属性和14个任务生命周期属性。

Result: 建立了首个严格基础的、领域无关的框架，用于系统分析、设计和部署正确、可靠、鲁棒的多智能体AI系统，支持形式化验证。

Conclusion: 该框架填补了多智能体AI系统分析中的语义鸿沟，提供了统一的语义框架，能够检测协调边缘情况，预防死锁和安全漏洞，确保系统安全性和功能性。

Abstract: Agentic AI systems, which leverage multiple autonomous agents and Large
Language Models (LLMs), are increasingly used to address complex, multi-step
tasks. The safety, security, and functionality of these systems are critical,
especially in high-stakes applications. However, the current ecosystem of
inter-agent communication is fragmented, with protocols such as the Model
Context Protocol (MCP) for tool access and the Agent-to-Agent (A2A) protocol
for coordination being analyzed in isolation. This fragmentation creates a
semantic gap that prevents the rigorous analysis of system properties and
introduces risks such as architectural misalignment and exploitable
coordination issues. To address these challenges, we introduce a modeling
framework for agentic AI systems composed of two foundational models. The
first, the host agent model, formalizes the top-level entity that interacts
with the user, decomposes tasks, and orchestrates their execution by leveraging
external agents and tools. The second, the task lifecycle model, details the
states and transitions of individual sub-tasks from creation to completion,
providing a fine-grained view of task management and error handling. Together,
these models provide a unified semantic framework for reasoning about the
behavior of multi-AI agent systems. Grounded in this framework, we define 17
properties for the host agent and 14 for the task lifecycle, categorized into
liveness, safety, completeness, and fairness. Expressed in temporal logic,
these properties enable formal verification of system behavior, detection of
coordination edge cases, and prevention of deadlocks and security
vulnerabilities. Through this effort, we introduce the first rigorously
grounded, domain-agnostic framework for the systematic analysis, design, and
deployment of correct, reliable, and robust agentic AI systems.

</details>


### [51] [A Multimodal Approach to Heritage Preservation in the Context of Climate Change](https://arxiv.org/abs/2510.14136)
*David Roqui,Adèle Cormier,nistor Grozavu,Ann Bourges*

Main category: cs.AI

TL;DR: 提出轻量级多模态架构，融合传感器数据和视觉图像来预测文化遗产地点的退化严重程度，在数据稀缺情况下显著优于传统方法


<details>
  <summary>Details</summary>
Motivation: 文化遗产地点因气候变化面临加速退化，传统单模态监测方法无法捕捉环境压力与材料退化之间的复杂相互作用

Method: 采用简化的PerceiverIO架构，包含64D潜在空间和自适应Barlow Twins损失函数，鼓励模态互补性而非冗余

Result: 在斯特拉斯堡大教堂数据上达到76.9%准确率，比标准多模态架构提升43%，比单模态方法显著提升

Conclusion: 架构简化与对比正则化相结合，可在数据稀缺的遗产监测环境中实现有效的多模态学习，为AI驱动的保护决策支持系统奠定基础

Abstract: Cultural heritage sites face accelerating degradation due to climate change,
yet tradi- tional monitoring relies on unimodal analysis (visual inspection or
environmental sen- sors alone) that fails to capture the complex interplay
between environmental stres- sors and material deterioration. We propose a
lightweight multimodal architecture that fuses sensor data (temperature,
humidity) with visual imagery to predict degradation severity at heritage
sites. Our approach adapts PerceiverIO with two key innovations: (1) simplified
encoders (64D latent space) that prevent overfitting on small datasets (n=37
training samples), and (2) Adaptive Barlow Twins loss that encourages modality
complementarity rather than redundancy. On data from Strasbourg Cathedral, our
model achieves 76.9% accu- racy, a 43% improvement over standard multimodal
architectures (VisualBERT, Trans- former) and 25% over vanilla PerceiverIO.
Ablation studies reveal that sensor-only achieves 61.5% while image-only
reaches 46.2%, confirming successful multimodal synergy. A systematic
hyperparameter study identifies an optimal moderate correlation target ({\tau}
=0.3) that balances align- ment and complementarity, achieving 69.2% accuracy
compared to other {\tau} values ({\tau} =0.1/0.5/0.7: 53.8%, {\tau} =0.9:
61.5%). This work demonstrates that architectural sim- plicity combined with
contrastive regularization enables effective multimodal learning in data-scarce
heritage monitoring contexts, providing a foundation for AI-driven con-
servation decision support systems.

</details>


### [52] [CodeEvolve: An open source evolutionary coding agent for algorithm discovery and optimization](https://arxiv.org/abs/2510.14150)
*Henrique Assumpção,Diego Ferreira,Leandro Campos,Fabricio Murai*

Main category: cs.AI

TL;DR: CodeEvolve是一个开源进化编码代理，结合大语言模型和遗传算法解决复杂计算问题，在数学基准测试中超越了Google DeepMind的AlphaEvolve。


<details>
  <summary>Details</summary>
Motivation: 将强大的进化概念应用于LLM领域，基于广义科学发现的最新方法，解决复杂计算问题。

Method: 采用基于岛屿的遗传算法保持种群多样性，引入基于启发的交叉机制利用LLM上下文窗口组合成功解决方案的特征，并实现元提示策略动态探索解空间。

Result: 在用于评估Google DeepMind AlphaEvolve的数学基准测试子集上，CodeEvolve在多个挑战性问题上的表现超越了AlphaEvolve。

Conclusion: CodeEvolve成功将进化算法与LLM结合，在复杂问题求解上表现出色，并通过开源发布促进合作与进步。

Abstract: In this work, we introduce CodeEvolve, an open-source evolutionary coding
agent that unites Large Language Models (LLMs) with genetic algorithms to solve
complex computational problems. Our framework adapts powerful evolutionary
concepts to the LLM domain, building upon recent methods for generalized
scientific discovery. CodeEvolve employs an island-based genetic algorithm to
maintain population diversity and increase throughput, introduces a novel
inspiration-based crossover mechanism that leverages the LLMs context window to
combine features from successful solutions, and implements meta-prompting
strategies for dynamic exploration of the solution space. We conduct a rigorous
evaluation of CodeEvolve on a subset of the mathematical benchmarks used to
evaluate Google DeepMind's closed-source AlphaEvolve. Our findings show that
our method surpasses AlphaEvolve's performance on several challenging problems.
To foster collaboration and accelerate progress, we release our complete
framework as an open-source repository.

</details>


### [53] [Combining Reinforcement Learning and Behavior Trees for NPCs in Video Games with AMD Schola](https://arxiv.org/abs/2510.14154)
*Tian Liu,Alex Cann,Ian Colbert,Mehdi Saeedi*

Main category: cs.AI

TL;DR: 本文探讨了将强化学习与行为树结合用于游戏NPC开发的可行性，通过AMD Schola插件在虚幻引擎中训练多任务NPC，展示了这一方法的实际应用价值。


<details>
  <summary>Details</summary>
Motivation: 尽管强化学习研究进展迅速，但在商业游戏中的应用仍较缓慢。本文旨在解决游戏AI社区在使用RL驱动NPC时面临的常见挑战，探索RL与传统行为树的结合点。

Method: 使用AMD Schola插件在虚幻引擎中训练RL智能体，创建多任务NPC，并在受《最后生还者》启发的复杂3D环境中展示联合训练RL模型与行为树的方法论。

Result: 成功展示了RL与行为树结合方法的可行性，能够创建具有多种技能的复杂NPC，并在实际游戏环境中得到验证。

Conclusion: RL与行为树的交集是一个值得进一步探索的关键节点，虽然已有研究提出这种结合，但在实际应用中仍较罕见，本文证明了该方法的实用价值。

Abstract: While the rapid advancements in the reinforcement learning (RL) research
community have been remarkable, the adoption in commercial video games remains
slow. In this paper, we outline common challenges the Game AI community faces
when using RL-driven NPCs in practice, and highlight the intersection of RL
with traditional behavior trees (BTs) as a crucial juncture to be explored
further. Although the BT+RL intersection has been suggested in several research
papers, its adoption is rare. We demonstrate the viability of this approach
using AMD Schola -- a plugin for training RL agents in Unreal Engine -- by
creating multi-task NPCs in a complex 3D environment inspired by the commercial
video game ``The Last of Us". We provide detailed methodologies for jointly
training RL models with BTs while showcasing various skills.

</details>


### [54] [JEDA: Query-Free Clinical Order Search from Ambient Dialogues](https://arxiv.org/abs/2510.14169)
*Praphul Singh,Corey Barrett,Sumana Srivasta,Amitabh Saikia,Irfan Bulu,Sri Gadde,Krishnaram Kenthapadi*

Main category: cs.AI

TL;DR: JEDA是一个用于临床订单检索的双编码器系统，能够直接从临床对话中检索规范订单，支持查询和无查询两种模式，实现了实时、可解释的临床订单检索。


<details>
  <summary>Details</summary>
Motivation: 临床对话包含显性指令和隐性推理，现有系统依赖LLM重写导致延迟、不稳定和不透明，无法满足实时订单需求。

Method: 基于PubMedBERT初始化，使用双编码器架构，通过对比学习目标对齐异质意图表达，利用约束LLM指导生成多种表达形式，无查询模式使用滚动对话窗口编码。

Result: JEDA在实践中取得显著提升，大幅优于基础编码器和近期开源嵌入模型，具有噪声鲁棒性，减少对不流利和ASR错误的敏感性。

Conclusion: JEDA提供了一个快速、可解释、无需LLM的检索层，能够实时将环境上下文链接到可操作的临床订单。

Abstract: Clinical conversations mix explicit directives (order a chest X-ray) with
implicit reasoning (the cough worsened overnight, we should check for
pneumonia). Many systems rely on LLM rewriting, adding latency, instability,
and opacity that hinder real-time ordering. We present JEDA (Joint Embedding
for Direct and Ambient clinical orders), a domain-initialized bi-encoder that
retrieves canonical orders directly and, in a query-free mode, encodes a short
rolling window of ambient dialogue to trigger retrieval. Initialized from
PubMedBERT and fine-tuned with a duplicate-safe contrastive objective, JEDA
aligns heterogeneous expressions of intent to shared order concepts. Training
uses constrained LLM guidance to tie each signed order to complementary
formulations (command only, context only, command+context, context+reasoning),
producing clearer inter-order separation, tighter query extendash order
coupling, and stronger generalization. The query-free mode is noise-resilient,
reducing sensitivity to disfluencies and ASR errors by conditioning on a short
window rather than a single utterance. Deployed in practice, JEDA yields large
gains and substantially outperforms its base encoder and recent open embedders
(Linq Embed Mistral, SFR Embedding, GTE Qwen, BGE large, Embedding Gemma). The
result is a fast, interpretable, LLM-free retrieval layer that links ambient
context to actionable clinical orders in real time.

</details>


### [55] [ARM-FM: Automated Reward Machines via Foundation Models for Compositional Reinforcement Learning](https://arxiv.org/abs/2510.14176)
*Roger Creus Castanyer,Faisal Mohamed,Pablo Samuel Castro,Cyrus Neary,Glen Berseth*

Main category: cs.AI

TL;DR: ARM-FM是一个利用基础模型自动生成奖励机器的强化学习框架，能够从自然语言规范中自动构建奖励机器，实现任务分解和零样本泛化。


<details>
  <summary>Details</summary>
Motivation: 强化学习算法对奖励函数规范高度敏感，这限制了其广泛应用。现有的奖励设计方法需要大量人工参与，难以实现自动化和组合性。

Method: 使用基础模型从自然语言规范自动生成奖励机器；为每个自动机状态关联语言嵌入以实现任务间泛化；利用奖励机器的结构化形式化实现有效任务分解。

Result: 在多种挑战性环境中验证了ARM-FM的有效性，包括展示了零样本泛化能力。

Conclusion: ARM-FM通过结合基础模型和奖励机器，实现了自动化的组合奖励设计，为强化学习的广泛应用提供了有效解决方案。

Abstract: Reinforcement learning (RL) algorithms are highly sensitive to reward
function specification, which remains a central challenge limiting their broad
applicability. We present ARM-FM: Automated Reward Machines via Foundation
Models, a framework for automated, compositional reward design in RL that
leverages the high-level reasoning capabilities of foundation models (FMs).
Reward machines (RMs) -- an automata-based formalism for reward specification
-- are used as the mechanism for RL objective specification, and are
automatically constructed via the use of FMs. The structured formalism of RMs
yields effective task decompositions, while the use of FMs enables objective
specifications in natural language. Concretely, we (i) use FMs to automatically
generate RMs from natural language specifications; (ii) associate language
embeddings with each RM automata-state to enable generalization across tasks;
and (iii) provide empirical evidence of ARM-FM's effectiveness in a diverse
suite of challenging environments, including evidence of zero-shot
generalization.

</details>


### [56] [Implementation of AI in Precision Medicine](https://arxiv.org/abs/2510.14194)
*Göktuğ Bender,Samer Faraj,Anand Bhardwaj*

Main category: cs.AI

TL;DR: 本文对2019-2024年精准医学中AI实施的文献进行范围综述，识别了数据质量、临床可靠性、工作流整合和治理方面的关键障碍和推动因素，提出了支持可信和可持续实施的未来方向。


<details>
  <summary>Details</summary>
Motivation: AI在精准医学中日益重要，能够整合和解释多模态数据，但在临床环境中的实施仍然有限，需要系统分析实施障碍和推动因素。

Method: 采用范围综述方法，分析2019-2024年相关文献，通过基于生态系统的框架识别关键因素。

Result: 识别了数据质量、临床可靠性、工作流整合和治理方面的关键障碍和推动因素，强调了现实世界转化中的相互依赖关系。

Conclusion: 提出了支持可信和可持续AI实施的未来方向，强调需要生态系统层面的协调来促进精准医学中AI的有效应用。

Abstract: Artificial intelligence (AI) has become increasingly central to precision
medicine by enabling the integration and interpretation of multimodal data, yet
implementation in clinical settings remains limited. This paper provides a
scoping review of literature from 2019-2024 on the implementation of AI in
precision medicine, identifying key barriers and enablers across data quality,
clinical reliability, workflow integration, and governance. Through an
ecosystem-based framework, we highlight the interdependent relationships
shaping real-world translation and propose future directions to support
trustworthy and sustainable implementation.

</details>


### [57] [Echoes of Human Malice in Agents: Benchmarking LLMs for Multi-Turn Online Harassment Attacks](https://arxiv.org/abs/2510.14207)
*Trilok Padhi,Pinxian Lu,Abdulkadir Erol,Tanmay Sutar,Gauri Sharma,Mina Sonmez,Munmun De Choudhury,Ugur Kursuncu*

Main category: cs.AI

TL;DR: 提出了在线骚扰代理基准，包含多轮骚扰对话数据集、基于重复博弈论的多代理模拟、三种攻击方法以及混合评估框架。结果显示微调攻击使骚扰成功率大幅提升，闭源和开源模型表现出不同的升级轨迹。


<details>
  <summary>Details</summary>
Motivation: 现有越狱研究主要关注单轮提示，而真实骚扰通常发生在多轮交互中，需要开发更贴近现实的基准来评估LLM代理的安全性。

Method: 构建了包含多轮骚扰对话数据集的基准，采用基于重复博弈论的多代理模拟，开发了针对记忆、规划和微调的三种越狱攻击方法，并使用混合方法评估框架。

Result: 微调攻击使骚扰成功率在Llama中从57.25-64.19%提升至95.78-96.89%，在Gemini中从98.46%提升至99.33%。最常见的毒性行为是侮辱(84.9-87.8%)和谩骂(81.2-85.1%)。闭源模型表现出显著脆弱性。

Conclusion: 多轮和理论基础的攻击不仅成功率高，还能模拟人类骚扰动态，需要开发强大的安全防护措施来确保在线平台的安全性和责任性。

Abstract: Large Language Model (LLM) agents are powering a growing share of interactive
web applications, yet remain vulnerable to misuse and harm. Prior jailbreak
research has largely focused on single-turn prompts, whereas real harassment
often unfolds over multi-turn interactions. In this work, we present the Online
Harassment Agentic Benchmark consisting of: (i) a synthetic multi-turn
harassment conversation dataset, (ii) a multi-agent (e.g., harasser, victim)
simulation informed by repeated game theory, (iii) three jailbreak methods
attacking agents across memory, planning, and fine-tuning, and (iv) a
mixed-methods evaluation framework. We utilize two prominent LLMs,
LLaMA-3.1-8B-Instruct (open-source) and Gemini-2.0-flash (closed-source). Our
results show that jailbreak tuning makes harassment nearly guaranteed with an
attack success rate of 95.78--96.89% vs. 57.25--64.19% without tuning in Llama,
and 99.33% vs. 98.46% without tuning in Gemini, while sharply reducing refusal
rate to 1-2% in both models. The most prevalent toxic behaviors are Insult with
84.9--87.8% vs. 44.2--50.8% without tuning, and Flaming with 81.2--85.1% vs.
31.5--38.8% without tuning, indicating weaker guardrails compared to sensitive
categories such as sexual or racial harassment. Qualitative evaluation further
reveals that attacked agents reproduce human-like aggression profiles, such as
Machiavellian/psychopathic patterns under planning, and narcissistic tendencies
with memory. Counterintuitively, closed-source and open-source models exhibit
distinct escalation trajectories across turns, with closed-source models
showing significant vulnerability. Overall, our findings show that multi-turn
and theory-grounded attacks not only succeed at high rates but also mimic
human-like harassment dynamics, motivating the development of robust safety
guardrails to ultimately keep online platforms safe and responsible.

</details>


### [58] [LiveResearchBench: A Live Benchmark for User-Centric Deep Research in the Wild](https://arxiv.org/abs/2510.14240)
*Jiayu Wang,Yifei Ming,Riya Dulepet,Qinglin Chen,Austin Xu,Zixuan Ke,Frederic Sala,Aws Albarghouthi,Caiming Xiong,Shafiq Joty*

Main category: cs.AI

TL;DR: 提出了LiveResearchBench基准和DeepEval评估套件，用于评估深度研究系统的能力，并对17个前沿系统进行了全面评估。


<details>
  <summary>Details</summary>
Motivation: 现有基准无法充分评估深度研究系统，往往聚焦狭窄领域或提出模糊问题，难以进行公平比较。需要遵循用户中心、动态性、明确性和多面性四个原则来构建更合适的评估框架。

Method: 构建了包含100个专家策划任务的LiveResearchBench基准，涵盖日常生活、企业和学术领域，每个任务都需要广泛的动态实时网络搜索和综合。开发了DeepEval评估套件，包含内容和报告层面的质量评估，整合了四种互补的评估协议。

Result: 对17个前沿深度研究系统进行了全面评估，包括单代理网络搜索、单代理深度研究和多代理系统，揭示了当前系统的优势、常见失败模式和推进可靠深度研究所需的关键系统组件。

Conclusion: LiveResearchBench和DeepEval为系统评估深度研究能力提供了严格基础，分析结果指出了当前系统的局限性和未来发展方向，有助于推进可靠、有洞察力的深度研究系统的发展。

Abstract: Deep research -- producing comprehensive, citation-grounded reports by
searching and synthesizing information from hundreds of live web sources --
marks an important frontier for agentic systems. To rigorously evaluate this
ability, four principles are essential: tasks should be (1) user-centric,
reflecting realistic information needs, (2) dynamic, requiring up-to-date
information beyond parametric knowledge, (3) unambiguous, ensuring consistent
interpretation across users, and (4) multi-faceted and search-intensive,
requiring search over numerous web sources and in-depth analysis. Existing
benchmarks fall short of these principles, often focusing on narrow domains or
posing ambiguous questions that hinder fair comparison. Guided by these
principles, we introduce LiveResearchBench, a benchmark of 100 expert-curated
tasks spanning daily life, enterprise, and academia, each requiring extensive,
dynamic, real-time web search and synthesis. Built with over 1,500 hours of
human labor, LiveResearchBench provides a rigorous basis for systematic
evaluation. To evaluate citation-grounded long-form reports, we introduce
DeepEval, a comprehensive suite covering both content- and report-level
quality, including coverage, presentation, citation accuracy and association,
consistency and depth of analysis. DeepEval integrates four complementary
evaluation protocols, each designed to ensure stable assessment and high
agreement with human judgments. Using LiveResearchBench and DeepEval, we
conduct a comprehensive evaluation of 17 frontier deep research systems,
including single-agent web search, single-agent deep research, and multi-agent
systems. Our analysis reveals current strengths, recurring failure modes, and
key system components needed to advance reliable, insightful deep research.

</details>


### [59] [Towards Agentic Self-Learning LLMs in Search Environment](https://arxiv.org/abs/2510.14253)
*Wangtao Sun,Xiang Cheng,Jialin Fan,Yao Xu,Xing Yu,Shizhu He,Jun Zhao,Kang Liu*

Main category: cs.AI

TL;DR: 本文提出Agentic Self-Learning (ASL)框架，通过多角色协同进化的强化学习方法实现无需人工标注数据的智能体自我学习，发现奖励信号来源和任务数据规模是影响智能体训练可扩展性的关键因素。


<details>
  <summary>Details</summary>
Motivation: 研究如何在不依赖人工标注数据集或预定义规则奖励的情况下，通过自我学习扩展基于LLM的智能体能力，解决开放领域智能体学习的可扩展性问题。

Method: 提出ASL框架，包含提示生成器、策略模型和生成式奖励模型三个角色，在共享工具环境和LLM骨干网络中形成任务生成、策略执行和评估的闭环强化学习循环。

Result: ASL实现了持续的性能提升，超越了会达到平台期或性能下降的强基线方法（如Search-R1），在零标注数据条件下仍能持续改进，显示出优异的样本效率和鲁棒性。

Conclusion: 奖励来源和数据规模是开放领域智能体学习的关键杠杆，多角色协同进化是实现可扩展、自我改进智能体的有效方法，生成式奖励模型的验证能力是主要瓶颈。

Abstract: We study whether self-learning can scale LLM-based agents without relying on
human-curated datasets or predefined rule-based rewards. Through controlled
experiments in a search-agent setting, we identify two key determinants of
scalable agent training: the source of reward signals and the scale of agent
task data. We find that rewards from a Generative Reward Model (GRM) outperform
rigid rule-based signals for open-domain learning, and that co-evolving the GRM
with the policy further boosts performance. Increasing the volume of agent task
data-even when synthetically generated-substantially enhances agentic
capabilities. Building on these insights, we propose \textbf{Agentic
Self-Learning} (ASL), a fully closed-loop, multi-role reinforcement learning
framework that unifies task generation, policy execution, and evaluation within
a shared tool environment and LLM backbone. ASL coordinates a Prompt Generator,
a Policy Model, and a Generative Reward Model to form a virtuous cycle of
harder task setting, sharper verification, and stronger solving. Empirically,
ASL delivers steady, round-over-round gains, surpasses strong RLVR baselines
(e.g., Search-R1) that plateau or degrade, and continues improving under
zero-labeled-data conditions, indicating superior sample efficiency and
robustness. We further show that GRM verification capacity is the main
bottleneck: if frozen, it induces reward hacking and stalls progress; continual
GRM training on the evolving data distribution mitigates this, and a small
late-stage injection of real verification data raises the performance ceiling.
This work establishes reward source and data scale as critical levers for
open-domain agent learning and demonstrates the efficacy of multi-role
co-evolution for scalable, self-improving agents. The data and code of this
paper are released at
https://github.com/forangel2014/Towards-Agentic-Self-Learning

</details>


### [60] [MorphoBench: A Benchmark with Difficulty Adaptive to Model Reasoning](https://arxiv.org/abs/2510.14265)
*Xukai Wang,Xuanbo Liu,Mingrui Chen,Haitian Zhong,Xuanlin Yang,Bohan Zeng,Jinbo Hu,Hao Liang,Junbo Niu,Xuchen Li,Ruitao Wu,Ruichuan An,Yang Shi,Liu Liu,Xu-Yao Zhang,Qiang Liu,Zhouchen Lin,Wentao Zhang,Bin Dong*

Main category: cs.AI

TL;DR: 提出了MorphoBench基准测试，用于评估大型模型的推理能力，能够根据模型推理能力动态调整问题难度，包含1300多个测试问题。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试评估大型模型推理能力时范围有限，缺乏根据模型推理能力动态调整难度的灵活性。

Method: 从现有基准和奥赛级竞赛中收集复杂推理问题，利用模型推理过程中的关键陈述自适应修改问题分析难度，并使用仿真软件生成可动态调整难度的问题。

Result: 收集了1300多个测试问题，基于o3和GPT-5等模型的推理能力迭代调整了MorphoBench的难度。

Conclusion: MorphoBench提高了模型推理评估的全面性和有效性，为改进大型模型的推理能力和科学稳健性提供了可靠指导。

Abstract: With the advancement of powerful large-scale reasoning models, effectively
evaluating the reasoning capabilities of these models has become increasingly
important. However, existing benchmarks designed to assess the reasoning
abilities of large models tend to be limited in scope and lack the flexibility
to adapt their difficulty according to the evolving reasoning capacities of the
models. To address this, we propose MorphoBench, a benchmark that incorporates
multidisciplinary questions to evaluate the reasoning capabilities of large
models and can adjust and update question difficulty based on the reasoning
abilities of advanced models. Specifically, we curate the benchmark by
selecting and collecting complex reasoning questions from existing benchmarks
and sources such as Olympiad-level competitions. Additionally, MorphoBench
adaptively modifies the analytical challenge of questions by leveraging key
statements generated during the model's reasoning process. Furthermore, it
includes questions generated using simulation software, enabling dynamic
adjustment of benchmark difficulty with minimal resource consumption. We have
gathered over 1,300 test questions and iteratively adjusted the difficulty of
MorphoBench based on the reasoning capabilities of models such as o3 and GPT-5.
MorphoBench enhances the comprehensiveness and validity of model reasoning
evaluation, providing reliable guidance for improving both the reasoning
abilities and scientific robustness of large models. The code has been released
in https://github.com/OpenDCAI/MorphoBench.

</details>


### [61] [A Guardrail for Safety Preservation: When Safety-Sensitive Subspace Meets Harmful-Resistant Null-Space](https://arxiv.org/abs/2510.14301)
*Bingjie Zhang,Yibo Yang,Renzhe,Dandan Guo,Jindong Gu,Philip Torr,Bernard Ghanem*

Main category: cs.AI

TL;DR: GuardSpace是一个保护LLM安全对齐的框架，通过安全敏感子空间和有害抵抗零空间，在微调过程中保持模型的安全行为。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在微调过程中容易丧失预训练的安全对齐能力，导致生成有害内容，需要一种方法来保护安全机制。

Method: 使用协方差预处理的奇异值分解将预训练权重分解为安全相关和安全无关组件，初始化低秩适配器时冻结安全相关部分，并构建零空间投影器限制适配器更新对有害提示的影响。

Result: 在多个下游任务上的实验表明，GuardSpace优于现有方法，对于Llama-2-7B-Chat在GSM8K上的微调，将平均有害分数从14.4%降至3.6%，同时准确率从26.0%提升至28.0%。

Conclusion: GuardSpace框架能有效保护LLM在微调过程中的安全对齐，在保持安全性的同时提升任务性能。

Abstract: Large language models (LLMs) have achieved remarkable success in diverse
tasks, yet their safety alignment remains fragile during adaptation. Even when
fine-tuning on benign data or with low-rank adaptation, pre-trained safety
behaviors are easily degraded, leading to harmful responses in the fine-tuned
models. To address this challenge, we propose GuardSpace, a guardrail framework
for preserving safety alignment throughout fine-tuning, composed of two key
components: a safety-sensitive subspace and a harmful-resistant null space.
First, we explicitly decompose pre-trained weights into safety-relevant and
safety-irrelevant components using covariance-preconditioned singular value
decomposition, and initialize low-rank adapters from the safety-irrelevant
ones, while freezing safety-relevant components to preserve their associated
safety mechanism. Second, we construct a null space projector that restricts
adapter updates from altering safe outputs on harmful prompts, thereby
maintaining the original refusal behavior. Experiments with various pre-trained
models on multiple downstream tasks demonstrate that GuardSpace achieves
superior performance over existing methods. Notably, for Llama-2-7B-Chat
fine-tuned on GSM8K, GuardSpace outperforms the state-of-the-art method AsFT,
reducing the average harmful score from 14.4% to 3.6%, while improving the
accuracy from from 26.0% to 28.0%.

</details>


### [62] [Terrarium: Revisiting the Blackboard for Multi-Agent Safety, Privacy, and Security Studies](https://arxiv.org/abs/2510.14312)
*Mason Nakamura,Abhinav Kumar,Saaduddin Mahmud,Sahar Abdelnabi,Shlomo Zilberstein,Eugene Bagdasarian*

Main category: cs.AI

TL;DR: 提出了Terrarium框架，用于细粒度研究基于LLM的多智能体系统中的安全、隐私和安全性问题，通过重新设计黑板架构来创建模块化、可配置的测试平台。


<details>
  <summary>Details</summary>
Motivation: LLM驱动的多智能体系统能够自动化用户任务，但引入了新的风险，包括错位、恶意方攻击、智能体被破坏或用户数据被盗等问题。

Method: 重新利用多智能体系统中的黑板设计，创建模块化、可配置的测试平台，识别关键攻击向量（错位、恶意智能体、通信被破坏、数据中毒），并在三个协作多智能体场景中实施四种代表性攻击。

Result: 开发了Terrarium框架，能够快速原型化、评估和迭代防御措施和设计，展示了框架的灵活性。

Conclusion: Terrarium旨在加速可信多智能体系统的进展，通过提供工具来应对安全、隐私和安全性挑战。

Abstract: A multi-agent system (MAS) powered by large language models (LLMs) can
automate tedious user tasks such as meeting scheduling that requires
inter-agent collaboration. LLMs enable nuanced protocols that account for
unstructured private data, user constraints, and preferences. However, this
design introduces new risks, including misalignment and attacks by malicious
parties that compromise agents or steal user data. In this paper, we propose
the Terrarium framework for fine-grained study on safety, privacy, and security
in LLM-based MAS. We repurpose the blackboard design, an early approach in
multi-agent systems, to create a modular, configurable testbed for multi-agent
collaboration. We identify key attack vectors such as misalignment, malicious
agents, compromised communication, and data poisoning. We implement three
collaborative MAS scenarios with four representative attacks to demonstrate the
framework's flexibility. By providing tools to rapidly prototype, evaluate, and
iterate on defenses and designs, Terrarium aims to accelerate progress toward
trustworthy multi-agent systems.

</details>


### [63] [Metacognitive Self-Correction for Multi-Agent System via Prototype-Guided Next-Execution Reconstruction](https://arxiv.org/abs/2510.14319)
*Xu Shen,Qi Zhang,Song Wang,Zhen Tan,Xinyu Zhao,Laura Yao,Vaishnav Tadiparthi,Hossein Nourkhiz Mahjoub,Ehsan Moradi Pari,Kwonjoon Lee,Tianlong Chen*

Main category: cs.AI

TL;DR: MASC是一个元认知框架，通过实时无监督的步骤级错误检测和自我纠正来增强多智能体系统的鲁棒性，防止错误级联传播。


<details>
  <summary>Details</summary>
Motivation: 多智能体系统在协作解决问题方面表现出色，但对级联错误很脆弱：单个错误步骤可能在智能体间传播并破坏整个轨迹。

Method: MASC采用两种互补设计：(1) 下一执行重构，从查询和交互历史预测下一步的嵌入以捕捉因果一致性；(2) 原型引导增强，学习正常步骤嵌入的原型先验，在稀疏上下文中稳定重构和异常评分。

Result: 在Who&When基准测试中，MASC始终优于所有基线，步骤级错误检测AUC-ROC提升高达8.47%；集成到不同多智能体框架中，能在各种架构上带来一致的端到端性能提升。

Conclusion: MASC的元认知监控和针对性纠正能够以最小开销减轻错误传播，为多智能体系统提供了有效的错误检测和纠正机制。

Abstract: Large Language Model based multi-agent systems (MAS) excel at collaborative
problem solving but remain brittle to cascading errors: a single faulty step
can propagate across agents and disrupt the trajectory. In this paper, we
present MASC, a metacognitive framework that endows MAS with real-time,
unsupervised, step-level error detection and self-correction. MASC rethinks
detection as history-conditioned anomaly scoring via two complementary designs:
(1) Next-Execution Reconstruction, which predicts the embedding of the next
step from the query and interaction history to capture causal consistency, and
(2) Prototype-Guided Enhancement, which learns a prototype prior over
normal-step embeddings and uses it to stabilize reconstruction and anomaly
scoring under sparse context (e.g., early steps). When an anomaly step is
flagged, MASC triggers a correction agent to revise the acting agent's output
before information flows downstream. On the Who&When benchmark, MASC
consistently outperforms all baselines, improving step-level error detection by
up to 8.47% AUC-ROC ; When plugged into diverse MAS frameworks, it delivers
consistent end-to-end gains across architectures, confirming that our
metacognitive monitoring and targeted correction can mitigate error propagation
with minimal overhead.

</details>


### [64] [AI for Service: Proactive Assistance with AI Glasses](https://arxiv.org/abs/2510.14359)
*Zichen Wen,Yiyu Wang,Chenfei Liao,Boxue Yang,Junxian Li,Weifeng Liu,Haocong He,Bolong Feng,Xuyang Liu,Yuanhuiyi Lyu,Xu Zheng,Xuming Hu,Linfeng Zhang*

Main category: cs.AI

TL;DR: 提出了AI4Service新范式，通过Alpha-Service框架实现主动式AI服务，能够从第一人称视角视频中检测服务机会并提供个性化服务。


<details>
  <summary>Details</summary>
Motivation: 现有AI服务大多是被动响应式的，而真正智能的助手应该能够预测用户需求并在适当时机主动采取行动。

Method: 基于AI眼镜构建Alpha-Service框架，包含五个核心组件：输入单元（感知）、中央处理单元（任务调度）、算术逻辑单元（工具使用）、记忆单元（长期个性化）和输出单元（人机交互），采用多智能体系统实现。

Result: 通过实时21点顾问、博物馆导览和购物搭配助手等案例研究，展示了系统能够无缝感知环境、推断用户意图，并在无需明确提示的情况下提供及时有用的帮助。

Conclusion: AI4Service范式将AI从被动工具转变为主动伴侣，Alpha-Service框架为构建真正智能和有用的助手提供了可行路径。

Abstract: In an era where AI is evolving from a passive tool into an active and
adaptive companion, we introduce AI for Service (AI4Service), a new paradigm
that enables proactive and real-time assistance in daily life. Existing AI
services remain largely reactive, responding only to explicit user commands. We
argue that a truly intelligent and helpful assistant should be capable of
anticipating user needs and taking actions proactively when appropriate. To
realize this vision, we propose Alpha-Service, a unified framework that
addresses two fundamental challenges: Know When to intervene by detecting
service opportunities from egocentric video streams, and Know How to provide
both generalized and personalized services. Inspired by the von Neumann
computer architecture and based on AI glasses, Alpha-Service consists of five
key components: an Input Unit for perception, a Central Processing Unit for
task scheduling, an Arithmetic Logic Unit for tool utilization, a Memory Unit
for long-term personalization, and an Output Unit for natural human
interaction. As an initial exploration, we implement Alpha-Service through a
multi-agent system deployed on AI glasses. Case studies, including a real-time
Blackjack advisor, a museum tour guide, and a shopping fit assistant,
demonstrate its ability to seamlessly perceive the environment, infer user
intent, and provide timely and useful assistance without explicit prompts.

</details>


### [65] [Can MLLMs Absorb Math Reasoning Abilities from LLMs as Free Lunch?](https://arxiv.org/abs/2510.14387)
*Yijie Hu,Zihao Zhou,Kaizhu Huang,Xiaowei Huang,Qiufeng Wang*

Main category: cs.AI

TL;DR: 提出IP-Merging方法，通过识别多模态大语言模型（MLLM）和数学大语言模型（Math LLM）中的推理相关参数，将其投影到MLLM子空间进行融合，无需调优即可提升MLLM的数学推理能力。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大语言模型（MLLM）的数学推理能力落后于纯文本大语言模型（LLM），但直接使用模型融合方法会因参数空间差异导致性能下降。

Method: IP-Merging方法：1）识别MLLM和Math LLM中的推理相关参数；2）将这些参数投影到MLLM子空间；3）在子空间内融合参数。该方法无需调优。

Result: 大量实验表明，IP-Merging方法能够在不损害MLLM其他能力的前提下，直接从Math LLM中吸收数学推理能力。

Conclusion: IP-Merging是一种无需调优的有效方法，能够显著提升MLLM的数学推理能力，同时保持其多模态功能的完整性。

Abstract: Math reasoning has been one crucial ability of large language models (LLMs),
where significant advancements have been achieved in recent years. However,
most efforts focus on LLMs by curating high-quality annotation data and
intricate training (or inference) paradigms, while the math reasoning
performance of multi-modal LLMs (MLLMs) remains lagging behind. Since the MLLM
typically consists of an LLM and a vision block, we wonder: Can MLLMs directly
absorb math reasoning abilities from off-the-shelf math LLMs without tuning?
Recent model-merging approaches may offer insights into this question. However,
they overlook the alignment between the MLLM and LLM, where we find that there
is a large gap between their parameter spaces, resulting in lower performance.
Our empirical evidence reveals two key factors behind this issue: the
identification of crucial reasoning-associated layers in the model and the
mitigation of the gaps in parameter space. Based on the empirical insights, we
propose IP-Merging that first identifies the reasoning-associated parameters in
both MLLM and Math LLM, then projects them into the subspace of MLLM, aiming to
maintain the alignment, and finally merges parameters in this subspace.
IP-Merging is a tuning-free approach since parameters are directly adjusted.
Extensive experiments demonstrate that our IP-Merging method can enhance the
math reasoning ability of MLLMs directly from Math LLMs without compromising
their other capabilities.

</details>


### [66] [Hi-Agent: Hierarchical Vision-Language Agents for Mobile Device Control](https://arxiv.org/abs/2510.14388)
*Zhe Wu,Hongjin Lu,Junliang Xing,Changhao Zhang,Yin Zhu,Yuhao Yang,Yuheng Jing,Kai Li,Kun Shao,Jianye Hao,Jun Wang,Yuanchun Shi*

Main category: cs.AI

TL;DR: Hi-Agent是一个可训练的分层视觉语言代理，通过高层推理模型和低层动作模型的联合优化，在移动设备控制任务中实现了最先进的性能表现。


<details>
  <summary>Details</summary>
Motivation: 现有的移动设备控制方法大多依赖直接的状态到动作映射，缺乏结构化推理和规划，导致在未见任务或UI布局上泛化能力差。

Method: 提出分层架构，将多步决策重新表述为单步子目标序列，并设计前瞻优势函数，利用低层模型的执行反馈来指导高层优化，缓解长时任务中的路径爆炸问题。

Result: 在Android-in-the-Wild基准测试中达到87.9%的任务成功率，显著优于基于提示（AppAgent: 17.7%）、监督学习（Filtered BC: 54.5%）和强化学习（DigiRL: 71.9%）的方法。

Conclusion: Hi-Agent通过分层设计和联合优化策略，在移动控制任务中表现出卓越的性能和泛化能力，特别是在高复杂度场景中具有强适应性。

Abstract: Building agents that autonomously operate mobile devices has attracted
increasing attention. While Vision-Language Models (VLMs) show promise, most
existing approaches rely on direct state-to-action mappings, which lack
structured reasoning and planning, and thus generalize poorly to novel tasks or
unseen UI layouts. We introduce Hi-Agent, a trainable hierarchical
vision-language agent for mobile control, featuring a high-level reasoning
model and a low-level action model that are jointly optimized. For efficient
training, we reformulate multi-step decision-making as a sequence of
single-step subgoals and propose a foresight advantage function, which
leverages execution feedback from the low-level model to guide high-level
optimization. This design alleviates the path explosion issue encountered by
Group Relative Policy Optimization (GRPO) in long-horizon tasks and enables
stable, critic-free joint training. Hi-Agent achieves a new State-Of-The-Art
(SOTA) 87.9% task success rate on the Android-in-the-Wild (AitW) benchmark,
significantly outperforming prior methods across three paradigms: prompt-based
(AppAgent: 17.7%), supervised (Filtered BC: 54.5%), and reinforcement
learning-based (DigiRL: 71.9%). It also demonstrates competitive zero-shot
generalization on the ScreenSpot-v2 benchmark. On the more challenging
AndroidWorld benchmark, Hi-Agent also scales effectively with larger backbones,
showing strong adaptability in high-complexity mobile control scenarios.

</details>


### [67] [IMAGINE: Integrating Multi-Agent System into One Model for Complex Reasoning and Planning](https://arxiv.org/abs/2510.14406)
*Xikai Zhang,Bo Wang,Likang Xiao,Yongzhi Li,Quan Chen,Wenju Wu,Liu Liu*

Main category: cs.AI

TL;DR: 提出了IMAGINE框架，将多智能体系统的推理规划能力集成到单个紧凑模型中，显著超越MAS性能，在TravelPlanner基准上达到82.7%的最终通过率。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型在复杂推理和规划任务中的挑战，以及多智能体系统存在的高推理成本、长延迟和端到端训练困难等问题。

Method: 提出IMAGINE框架，通过端到端训练将多智能体系统的能力集成到单个模型中，使用Qwen3-8B-Instruct作为基础模型进行训练。

Result: 在TravelPlanner基准测试中，训练后的模型达到82.7%的最终通过率，远超DeepSeek-R1-671B的40%，同时保持更小的模型规模。

Conclusion: IMAGINE框架成功将多智能体系统的结构化推理和规划能力集成到单个小规模模型中，并显著超越原系统的性能，为复杂推理任务提供了高效的解决方案。

Abstract: Although large language models (LLMs) have made significant strides across
various tasks, they still face significant challenges in complex reasoning and
planning. For example, even with carefully designed prompts and prior
information explicitly provided, GPT-4o achieves only a 7% Final Pass Rate on
the TravelPlanner dataset in the sole-planning mode. Similarly, even in the
thinking mode, Qwen3-8B-Instruct and DeepSeek-R1-671B, only achieve Final Pass
Rates of 5.9% and 40%, respectively. Although well-organized Multi-Agent
Systems (MAS) can offer improved collective reasoning, they often suffer from
high reasoning costs due to multi-round internal interactions, long
per-response latency, and difficulties in end-to-end training. To address these
challenges, we propose a general and scalable framework called IMAGINE, short
for Integrating Multi-Agent System into One Model. This framework not only
integrates the reasoning and planning capabilities of MAS into a single,
compact model, but also significantly surpass the capabilities of the MAS
through a simple end-to-end training. Through this pipeline, a single
small-scale model is not only able to acquire the structured reasoning and
planning capabilities of a well-organized MAS but can also significantly
outperform it. Experimental results demonstrate that, when using
Qwen3-8B-Instruct as the base model and training it with our method, the model
achieves an 82.7% Final Pass Rate on the TravelPlanner benchmark, far exceeding
the 40% of DeepSeek-R1-671B, while maintaining a much smaller model size.

</details>


### [68] [Eliminating Negative Occurrences of Derived Predicates from PDDL Axioms](https://arxiv.org/abs/2510.14412)
*Claudia Grundke,Gabriele Röger*

Main category: cs.AI

TL;DR: 本文提出了一种转换方法，用于消除PDDL公理中派生谓词的负出现，证明这种限制与分层公理集具有相同的表达能力。


<details>
  <summary>Details</summary>
Motivation: PDDL标准限制公理体中谓词的负出现只能针对由动作直接设置的谓词，而非由公理派生的谓词。但在文献中，作者常常偏离这一限制，只要求公理集是分层的。本文旨在证明这两种变体实际上具有相同的表达能力。

Method: 提出了一种转换方法，通过消除公理中派生谓词的负出现，将包含负出现的公理集转换为符合PDDL标准限制的形式。

Result: 证明了包含负出现的分层公理集与符合PDDL标准限制的公理集具有相同的表达能力，两者都能表达最小不动点逻辑中的相同查询。

Conclusion: 通过提出的转换方法，可以消除PDDL公理中派生谓词的负出现，从而统一文献中的不同处理方法，同时保持表达能力的完整性。

Abstract: Axioms are a feature of the Planning Domain Definition Language PDDL that can
be considered as a generalization of database query languages such as Datalog.
The PDDL standard restricts negative occurrences of predicates in axiom bodies
to predicates that are directly set by actions and not derived by axioms. In
the literature, authors often deviate from this limitation and only require
that the set of axioms is stratifiable. Both variants can express exactly the
same queries as least fixed-point logic, indicating that negative occurrences
of derived predicates can be eliminated. We present the corresponding
transformation.

</details>


### [69] [Helmsman: Autonomous Synthesis of Federated Learning Systems via Multi-Agent Collaboration](https://arxiv.org/abs/2510.14512)
*Haoyuan Li,Mathias Funk,Aaqib Saeed*

Main category: cs.AI

TL;DR: Helmsman是一个多智能体系统，能够根据高级用户规范自动合成联邦学习系统，通过三个协作阶段模拟研发流程，并在新基准测试中表现出色。


<details>
  <summary>Details</summary>
Motivation: 联邦学习系统设计复杂，需要选择、组合和调整策略来应对数据异构性和系统约束等挑战，现有解决方案往往脆弱且定制化，成为关键瓶颈。

Method: Helmsman采用三阶段协作方法：1) 交互式人机规划制定研究计划；2) 监督智能体团队进行模块化代码生成；3) 在沙盒模拟环境中进行自主评估和优化的闭环流程。

Result: 实验表明，Helmsman生成的解决方案与手工制作的基线方法相当，甚至更优，并在新的AgentFL-Bench基准测试中表现出色。

Conclusion: 这项工作代表了向复杂去中心化AI系统自动化工程迈出的重要一步，Helmsman能够有效解决联邦学习系统设计的复杂性挑战。

Abstract: Federated Learning (FL) offers a powerful paradigm for training models on
decentralized data, but its promise is often undermined by the immense
complexity of designing and deploying robust systems. The need to select,
combine, and tune strategies for multifaceted challenges like data
heterogeneity and system constraints has become a critical bottleneck,
resulting in brittle, bespoke solutions. To address this, we introduce
Helmsman, a novel multi-agent system that automates the end-to-end synthesis of
federated learning systems from high-level user specifications. It emulates a
principled research and development workflow through three collaborative
phases: (1) interactive human-in-the-loop planning to formulate a sound
research plan, (2) modular code generation by supervised agent teams, and (3) a
closed-loop of autonomous evaluation and refinement in a sandboxed simulation
environment. To facilitate rigorous evaluation, we also introduce
AgentFL-Bench, a new benchmark comprising 16 diverse tasks designed to assess
the system-level generation capabilities of agentic systems in FL. Extensive
experiments demonstrate that our approach generates solutions competitive with,
and often superior to, established hand-crafted baselines. Our work represents
a significant step towards the automated engineering of complex decentralized
AI systems.

</details>


### [70] [JSPLIT: A Taxonomy-based Solution for Prompt Bloating in Model Context Protocol](https://arxiv.org/abs/2510.14537)
*Emanuele Antonioni,Stefan Markovic,Anirudha Shankar,Jaime Bernardo,Lovro Markovic,Silvia Pareti,Benedetto Proietti*

Main category: cs.AI

TL;DR: JSPLIT是一个基于分类学的框架，通过层次化组织工具并基于用户查询选择最相关工具，有效解决MCP工具使用中的提示膨胀问题。


<details>
  <summary>Details</summary>
Motivation: 随着AI系统发展，用户期望从简单的文本交互转向更复杂的代理系统，需要LLM与外部工具交互。但工具数量增加导致提示膨胀，带来高token成本、延迟增加和任务成功率下降的问题。

Method: JSPLIT将工具组织成层次化分类学结构，使用用户提示基于查询和分类学结构识别并仅包含最相关工具，包括分类学设计、工具选择算法和评估数据集。

Result: JSPLIT显著减少提示大小而不显著影响代理响应能力。当可用工具数量大幅增加时，JSPLIT甚至提高了代理的工具选择准确性，在高复杂度代理环境中同时降低成本并提高任务成功率。

Conclusion: JSPLIT框架通过分类学驱动的方法有效管理提示大小，解决了MCP工具使用中的提示膨胀问题，在保持代理性能的同时显著降低成本。

Abstract: AI systems are continually evolving and advancing, and user expectations are
concurrently increasing, with a growing demand for interactions that go beyond
simple text-based interaction with Large Language Models (LLMs). Today's
applications often require LLMs to interact with external tools, marking a
shift toward more complex agentic systems. To support this, standards such as
the Model Context Protocol (MCP) have emerged, enabling agents to access tools
by including a specification of the capabilities of each tool within the
prompt. Although this approach expands what agents can do, it also introduces a
growing problem: prompt bloating. As the number of tools increases, the prompts
become longer, leading to high prompt token costs, increased latency, and
reduced task success resulting from the selection of tools irrelevant to the
prompt. To address this issue, we introduce JSPLIT, a taxonomy-driven framework
designed to help agents manage prompt size more effectively when using large
sets of MCP tools. JSPLIT organizes the tools into a hierarchical taxonomy and
uses the user's prompt to identify and include only the most relevant tools,
based on both the query and the taxonomy structure. In this paper, we describe
the design of the taxonomy, the tool selection algorithm, and the dataset used
to evaluate JSPLIT. Our results show that JSPLIT significantly reduces prompt
size without significantly compromising the agent's ability to respond
effectively. As the number of available tools for the agent grows
substantially, JSPLIT even improves the tool selection accuracy of the agent,
effectively reducing costs while simultaneously improving task success in
high-complexity agent environments.

</details>


### [71] [Symbol Grounding in Neuro-Symbolic AI: A Gentle Introduction to Reasoning Shortcuts](https://arxiv.org/abs/2510.14538)
*Emanuele Marconato,Samuele Bortolotti,Emile van Krieken,Paolo Morettin,Elena Umili,Antonio Vergari,Efthymia Tsamoura,Andrea Passerini,Stefano Teso*

Main category: cs.AI

TL;DR: 本文概述了神经符号AI中的推理捷径问题，讨论了其成因、后果以及应对方法，旨在为开发可靠的神经符号AI模型提供统一视角。


<details>
  <summary>Details</summary>
Motivation: 神经符号AI模型在不直接监督概念时容易出现推理捷径，这会损害模型解释性、分布外性能及可靠性。现有研究分散，需要系统整理以降低研究门槛。

Method: 通过直观术语介绍推理捷径的成因和后果，回顾现有理论表征，详细分析应对推理捷径的方法（包括缓解和意识策略），并评估其优缺点。

Result: 提供了推理捷径的统一视角，系统整理了现有研究成果，明确了各种应对策略的适用场景和局限性。

Conclusion: 本文通过将高级材料转化为易于理解的形式，为理解和解决推理捷径问题提供了基础，有望促进可靠神经符号AI和可信AI模型的发展。

Abstract: Neuro-symbolic (NeSy) AI aims to develop deep neural networks whose
predictions comply with prior knowledge encoding, e.g. safety or structural
constraints. As such, it represents one of the most promising avenues for
reliable and trustworthy AI. The core idea behind NeSy AI is to combine neural
and symbolic steps: neural networks are typically responsible for mapping
low-level inputs into high-level symbolic concepts, while symbolic reasoning
infers predictions compatible with the extracted concepts and the prior
knowledge. Despite their promise, it was recently shown that - whenever the
concepts are not supervised directly - NeSy models can be affected by Reasoning
Shortcuts (RSs). That is, they can achieve high label accuracy by grounding the
concepts incorrectly. RSs can compromise the interpretability of the model's
explanations, performance in out-of-distribution scenarios, and therefore
reliability. At the same time, RSs are difficult to detect and prevent unless
concept supervision is available, which is typically not the case. However, the
literature on RSs is scattered, making it difficult for researchers and
practitioners to understand and tackle this challenging problem. This overview
addresses this issue by providing a gentle introduction to RSs, discussing
their causes and consequences in intuitive terms. It also reviews and
elucidates existing theoretical characterizations of this phenomenon. Finally,
it details methods for dealing with RSs, including mitigation and awareness
strategies, and maps their benefits and limitations. By reformulating advanced
material in a digestible form, this overview aims to provide a unifying
perspective on RSs to lower the bar to entry for tackling them. Ultimately, we
hope this overview contributes to the development of reliable NeSy and
trustworthy AI models.

</details>


### [72] [LLM Agents Beyond Utility: An Open-Ended Perspective](https://arxiv.org/abs/2510.14548)
*Asen Nachkov,Xi Wang,Luc Van Gool*

Main category: cs.AI

TL;DR: 该研究探讨了预训练LLM代理能否通过自我生成任务、积累知识和环境交互，发展成具有自主规划和推理能力的开放实体。


<details>
  <summary>Details</summary>
Motivation: 研究LLM代理能否超越智能问题解决工具，成为能够自主规划、设计任务并推理抽象目标的独立实体。

Method: 增强预训练LLM代理，使其能够生成自己的任务、积累知识并与环境进行广泛交互，在开放环境中进行定性研究。

Result: 代理能够可靠执行复杂多步指令、跨运行存储和重用信息、提出并解决自己的任务，但对提示设计敏感、容易重复生成任务、无法形成自我表征。

Conclusion: 研究展示了预训练LLM向开放代理发展的潜力和当前局限，为训练代理管理记忆、进行有效探索和追求抽象长期目标指明了未来方向。

Abstract: Recent LLM agents have made great use of chain of thought reasoning and
function calling. As their capabilities grow, an important question arises: can
this software represent not only a smart problem-solving tool, but an entity in
its own right, that can plan, design immediate tasks, and reason toward
broader, more ambiguous goals? To study this question, we adopt an open-ended
experimental setting where we augment a pretrained LLM agent with the ability
to generate its own tasks, accumulate knowledge, and interact extensively with
its environment. We study the resulting open-ended agent qualitatively. It can
reliably follow complex multi-step instructions, store and reuse information
across runs, and propose and solve its own tasks, though it remains sensitive
to prompt design, prone to repetitive task generation, and unable to form
self-representations. These findings illustrate both the promise and current
limits of adapting pretrained LLMs toward open-endedness, and point to future
directions for training agents to manage memory, explore productively, and
pursue abstract long-term goals.

</details>


### [73] [TITAN: Graph-Executable Reasoning for Cyber Threat Intelligence](https://arxiv.org/abs/2510.14670)
*Marco Simoni,Aleksandar Fontana,Andrea Saracino,Paolo Mori*

Main category: cs.AI

TL;DR: TITAN是一个将自然语言网络威胁查询与结构化知识图谱上的可执行推理连接起来的框架，包含路径规划模型和图执行器，支持在威胁、行为和防御之间进行清晰可逆的推理。


<details>
  <summary>Details</summary>
Motivation: 传统检索系统无法在威胁情报领域进行清晰可逆的推理，需要一种能够将自然语言查询与结构化知识图谱上的可执行推理连接起来的框架。

Method: 集成路径规划模型（预测文本到逻辑关系链）和图执行器（遍历TITAN本体检索事实答案和证据），使用基于MITRE的类型化双向图。

Result: 创建了包含88209个示例的TITAN数据集，实证评估显示TITAN能生成语法有效且语义连贯的推理路径，可在底层图上确定性执行。

Conclusion: TITAN框架成功实现了自然语言威胁查询与结构化知识图谱推理的连接，支持清晰可逆的威胁情报推理。

Abstract: TITAN (Threat Intelligence Through Automated Navigation) is a framework that
connects natural-language cyber threat queries with executable reasoning over a
structured knowledge graph. It integrates a path planner model, which predicts
logical relation chains from text, and a graph executor that traverses the
TITAN Ontology to retrieve factual answers and supporting evidence. Unlike
traditional retrieval systems, TITAN operates on a typed, bidirectional graph
derived from MITRE, allowing reasoning to move clearly and reversibly between
threats, behaviors, and defenses. To support training and evaluation, we
introduce the TITAN Dataset, a corpus of 88209 examples (Train: 74258; Test:
13951) pairing natural language questions with executable reasoning paths and
step by step Chain of Thought explanations. Empirical evaluations show that
TITAN enables models to generate syntactically valid and semantically coherent
reasoning paths that can be deterministically executed on the underlying graph.

</details>


### [74] [ColorBench: Benchmarking Mobile Agents with Graph-Structured Framework for Complex Long-Horizon Tasks](https://arxiv.org/abs/2510.14621)
*Yuanyi Song,Heyuan Huang,Qiqiang Lin,Yin Zhao,Xiangmou Qu,Jun Wang,Xingyu Lou,Weiwen Liu,Zhuosheng Zhang,Jun Wang,Yong Yu,Weinan Zhang,Zhaoxiang Wang*

Main category: cs.AI

TL;DR: 本文提出了ColorBench，一种基于图结构的移动代理评估框架，通过模拟真实设备交互的有限状态来静态模拟动态行为，解决了现有评估方法无法全面评估复杂移动任务的问题。


<details>
  <summary>Details</summary>
Motivation: 当前移动代理评估存在局限性：离线静态基准只能验证单一预设路径，而在线动态测试受限于真实设备的复杂性和不可重现性。需要一种能全面评估代理能力的方法。

Method: 开发了图结构基准框架，通过建模真实设备交互的有限状态实现动态行为的静态模拟。构建了ColorBench基准，包含175个复杂长时程任务，支持多有效解决方案评估、子任务完成率统计和原子级能力分析。

Result: ColorBench包含175个任务（74个单应用，101个跨应用），平均长度超过13步。每个任务至少包含两条正确路径和若干典型错误路径，支持准动态交互。通过评估发现现有模型的局限性。

Conclusion: 提出了改进方向和可行的技术路径，基于实验结果增强代理在复杂长时程问题上的性能。代码和数据已开源。

Abstract: The rapid advancement of multimodal large language models has enabled agents
to operate mobile devices by directly interacting with graphical user
interfaces, opening new possibilities for mobile automation. However,
real-world mobile tasks are often complex and allow for multiple valid
solutions. This contradicts current mobile agent evaluation standards: offline
static benchmarks can only validate a single predefined "golden path", while
online dynamic testing is constrained by the complexity and non-reproducibility
of real devices, making both approaches inadequate for comprehensively
assessing agent capabilities. To bridge the gap between offline and online
evaluation and enhance testing stability, this paper introduces a novel
graph-structured benchmarking framework. By modeling the finite states observed
during real-device interactions, it achieves static simulation of dynamic
behaviors. Building on this, we develop ColorBench, a benchmark focused on
complex long-horizon tasks. It supports evaluation of multiple valid solutions,
subtask completion rate statistics, and atomic-level capability analysis.
ColorBench contains 175 tasks (74 single-app, 101 cross-app) with an average
length of over 13 steps. Each task includes at least two correct paths and
several typical error paths, enabling quasi-dynamic interaction. By evaluating
ColorBench across various baselines, we discover limitations of existing models
and propose improvement directions and feasible technical pathways to enhance
agents' performance on complex, long-horizon problems based on experimental
results. Code and data are available at:
https://github.com/MadeAgents/ColorBench.

</details>


### [75] [Beyond Hallucinations: The Illusion of Understanding in Large Language Models](https://arxiv.org/abs/2510.14665)
*Rikard Rosenbacke,Carl Rosenbacke,Victor Rosenbacke,Martin McKee*

Main category: cs.AI

TL;DR: 本文提出了Rose-Frame框架，用于诊断人机交互中的认知和认识论漂移，通过三个维度（地图vs领土、直觉vs理性、冲突vs确认）来增强AI部署的透明度和批判意识。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型虽然流畅且情感共鸣强，但基于统计预测而非有根据的推理，存在幻觉风险。它们操作化的System 1认知快速、联想且具有说服力，但缺乏反思或证伪能力。

Method: 引入Rose-Frame三维框架：地图vs领土（区分现实表征与现实本身）、直觉vs理性（基于双过程理论区分快速情感判断与缓慢反思思维）、冲突vs确认（检验思想是通过批判性测试还是简单强化）。

Result: Rose-Frame不试图用更多数据或规则修复LLM，而是提供一个反思工具，使模型的局限性和用户的假设可见，实现更透明和批判性意识的AI部署。

Conclusion: 将对齐重新定义为认知治理：无论是人类还是人工智能的直觉，都必须受到人类理性的支配。只有通过嵌入反思性、可证伪的监督，才能使机器的流畅性与人类的理解保持一致。

Abstract: Large language models (LLMs) are becoming deeply embedded in human
communication and decision-making, yet they inherit the ambiguity, bias, and
lack of direct access to truth inherent in language itself. While their outputs
are fluent, emotionally resonant, and coherent, they are generated through
statistical prediction rather than grounded reasoning. This creates the risk of
hallucination, responses that sound convincing but lack factual validity.
Building on Geoffrey Hinton's observation that AI mirrors human intuition
rather than reasoning, this paper argues that LLMs operationalize System 1
cognition at scale: fast, associative, and persuasive, but without reflection
or falsification. To address this, we introduce the Rose-Frame, a
three-dimensional framework for diagnosing cognitive and epistemic drift in
human-AI interaction. The three axes are: (i) Map vs. Territory, which
distinguishes representations of reality (epistemology) from reality itself
(ontology); (ii) Intuition vs. Reason, drawing on dual-process theory to
separate fast, emotional judgments from slow, reflective thinking; and (iii)
Conflict vs. Confirmation, which examines whether ideas are critically tested
through disagreement or simply reinforced through mutual validation. Each
dimension captures a distinct failure mode, and their combination amplifies
misalignment. Rose-Frame does not attempt to fix LLMs with more data or rules.
Instead, it offers a reflective tool that makes both the model's limitations
and the user's assumptions visible, enabling more transparent and critically
aware AI deployment. It reframes alignment as cognitive governance: intuition,
whether human or artificial, must remain governed by human reason. Only by
embedding reflective, falsifiable oversight can we align machine fluency with
human understanding.

</details>


### [76] [Mapping Smarter, Not Harder: A Test-Time Reinforcement Learning Agent That Improves Without Labels or Model Updates](https://arxiv.org/abs/2510.14900)
*Wen-Kwang Tsao,Yao-Ching Yu,Chien-Ming Huang*

Main category: cs.AI

TL;DR: 提出了一种无需标注数据或模型权重更新的强化学习代理，能够在推理时自我改进，通过生成针对性网络搜索查询来收集外部证据，显著提升企业智能平台中第三方日志的模式映射准确性。


<details>
  <summary>Details</summary>
Motivation: 企业智能平台需要整合大量第三方供应商的日志，但供应商文档在测试时往往不可用、不匹配、格式混乱或不完整，导致模式映射困难。

Method: 使用强化学习代理，在推理时：1)识别模糊的字段映射尝试；2)生成针对性网络搜索查询收集外部证据；3)应用基于置信度的奖励来迭代优化映射结果。

Result: 将Microsoft Defender for Endpoint日志转换为通用模式，映射准确率从56.4%(仅LLM)提升到72.73%(RAG)，最终达到93.94%(100次迭代后)，同时将需要专家审查的低置信度映射减少了85%。

Conclusion: 该方法提供了一种基于证据的透明解决方案，为更稳健、可问责、可扩展、高效、灵活、适应性强和协作的行业问题解决铺平了道路。

Abstract: The Enterprise Intelligence Platform must integrate logs from numerous
third-party vendors in order to perform various downstream tasks. However,
vendor documentation is often unavailable at test time. It is either misplaced,
mismatched, poorly formatted, or incomplete, which makes schema mapping
challenging. We introduce a reinforcement learning agent that can self-improve
without labeled examples or model weight updates. During inference, the agent:
1) Identifies ambiguous field-mapping attempts. 2) Generates targeted
web-search queries to gather external evidence. 3) Applies a confidence-based
reward to iteratively refine its mappings. To demonstrate this concept, we
converted Microsoft Defender for Endpoint logs into a common schema. Our method
increased mapping accuracy from 56.4\%(LLM-only) to 72.73\%(RAG) to 93.94\%
over 100 iterations using GPT-4o. At the same time, it reduced the number of
low-confidence mappings requiring expert review by 85\%. This new approach
provides an evidence-driven, transparent method for solving future industry
problems, paving the way for more robust, accountable, scalable, efficient,
flexible, adaptable, and collaborative solutions.

</details>


### [77] [Machine Learning and Public Health: Identifying and Mitigating Algorithmic Bias through a Systematic Review](https://arxiv.org/abs/2510.14669)
*Sara Altamirano,Arjan Vreeken,Sennay Ghebreab*

Main category: cs.AI

TL;DR: 本文系统回顾了2021-2025年荷兰公共卫生机器学习研究中算法偏见的识别、讨论和报告情况，开发了RABAT评估工具，发现普遍存在公平性框架缺失等问题，并提出了ACAR四阶段公平性框架。


<details>
  <summary>Details</summary>
Motivation: 机器学习在公共卫生领域有巨大潜力，但如果不系统关注算法偏见，可能会无意中加剧现有的健康不平等。

Method: 开发了RABAT评估工具，整合了Cochrane偏倚风险、PROBAST和微软负责任AI检查表等成熟框架，并应用于35篇同行评审研究进行系统文献回顾。

Result: 分析显示普遍存在差距：虽然数据抽样和缺失数据处理记录良好，但大多数研究缺乏明确的公平性框架、亚组分析和潜在危害的透明讨论。

Conclusion: 提出了ACAR四阶段公平性框架和可操作建议，帮助公共卫生机器学习实践者持续考虑算法偏见，确保算法创新促进而非损害健康公平。

Abstract: Machine learning (ML) promises to revolutionize public health through
improved surveillance, risk stratification, and resource allocation. However,
without systematic attention to algorithmic bias, ML may inadvertently
reinforce existing health disparities. We present a systematic literature
review of algorithmic bias identification, discussion, and reporting in Dutch
public health ML research from 2021 to 2025. To this end, we developed the Risk
of Algorithmic Bias Assessment Tool (RABAT) by integrating elements from
established frameworks (Cochrane Risk of Bias, PROBAST, Microsoft Responsible
AI checklist) and applied it to 35 peer-reviewed studies. Our analysis reveals
pervasive gaps: although data sampling and missing data practices are well
documented, most studies omit explicit fairness framing, subgroup analyses, and
transparent discussion of potential harms. In response, we introduce a
four-stage fairness-oriented framework called ACAR (Awareness,
Conceptualization, Application, Reporting), with guiding questions derived from
our systematic literature review to help researchers address fairness across
the ML lifecycle. We conclude with actionable recommendations for public health
ML practitioners to consistently consider algorithmic bias and foster
transparency, ensuring that algorithmic innovations advance health equity
rather than undermine it.

</details>


### [78] [NAEL: Non-Anthropocentric Ethical Logic](https://arxiv.org/abs/2510.14676)
*Bianca Maria Lerma,Rafael Peñaloza*

Main category: cs.AI

TL;DR: 提出NAEL（非人类中心伦理逻辑），一种基于主动推理和符号推理的人工智能伦理框架，强调伦理行为是智能系统在动态多智能体环境中最小化全局期望自由能量的涌现特性。


<details>
  <summary>Details</summary>
Motivation: 现有AI伦理模型通常以人类为中心，存在局限性。NAEL旨在开发不预设人类道德直觉、能够适应上下文并具有关系性伦理行为的人工智能系统。

Method: 采用神经符号架构，结合主动推理和符号推理，使智能体能够在不确定环境中评估其行为的伦理后果。

Result: 通过伦理资源分配的案例研究，展示了NAEL能够动态平衡自我保存、认知学习和集体福利。

Conclusion: NAEL为人工智能伦理提供了新的理论基础，能够产生上下文敏感、自适应和关系性的伦理行为，突破了传统人类中心伦理框架的限制。

Abstract: We introduce NAEL (Non-Anthropocentric Ethical Logic), a novel ethical
framework for artificial agents grounded in active inference and symbolic
reasoning. Departing from conventional, human-centred approaches to AI ethics,
NAEL formalizes ethical behaviour as an emergent property of intelligent
systems minimizing global expected free energy in dynamic, multi-agent
environments. We propose a neuro-symbolic architecture to allow agents to
evaluate the ethical consequences of their actions in uncertain settings. The
proposed system addresses the limitations of existing ethical models by
allowing agents to develop context-sensitive, adaptive, and relational ethical
behaviour without presupposing anthropomorphic moral intuitions. A case study
involving ethical resource distribution illustrates NAEL's dynamic balancing of
self-preservation, epistemic learning, and collective welfare.

</details>


### [79] [Practical, Utilitarian Algorithm Configuration](https://arxiv.org/abs/2510.14683)
*Devon Graham,Kevin Leyton-Brown*

Main category: cs.AI

TL;DR: COUP算法配置方法的实用性能改进研究，通过一系列优化使其在保持理论保证的同时达到与启发式配置方法相当的实践性能


<details>
  <summary>Details</summary>
Motivation: 将基于效用的算法配置方法从理论层面推进到实际应用层面，使其能够与广泛使用的启发式配置方法竞争，同时保持理论性能保证

Method: 对COUP算法配置过程进行一系列改进，包括优化其经验性能而不降低理论保证，并通过实验验证这些改进的效果

Result: 改进后的COUP在保持理论保证的同时，其经验性能达到了与无性能保证的启发式配置方法相当的水平

Conclusion: 通过系统改进，基于效用的算法配置方法COUP现在具备了实际竞争力，能够为算法选择问题提供理论保证下的高质量配置方案

Abstract: Utilitarian algorithm configuration identifies a parameter setting for a
given algorithm that maximizes a user's utility. Utility functions offer a
theoretically well-grounded approach to optimizing decision-making under
uncertainty and are flexible enough to capture a user's preferences over
algorithm runtimes (e.g., they can describe a sharp cutoff after which a
solution is no longer required, a per-hour cost for compute, or diminishing
returns from algorithms that take longer to run). COUP is a recently-introduced
utilitarian algorithm configuration procedure which was designed mainly to
offer strong theoretical guarantees about the quality of the configuration it
returns, with less attention paid to its practical performance. This paper
closes that gap, bringing theoretically-grounded, utilitarian algorithm
configuration to the point where it is competitive with widely used, heuristic
configuration procedures that offer no performance guarantees. We present a
series of improvements to COUP that improve its empirical performance without
degrading its theoretical guarantees and demonstrate their benefit
experimentally. Using a case study, we also illustrate ways of exploring the
robustness of a given solution to the algorithm selection problem to variations
in the utility function.

</details>


### [80] [Purifying Task Vectors in Knowledge-Aware Subspace for Model Merging](https://arxiv.org/abs/2510.14697)
*Bang An,Yibo Yang,Philip Torr,Bernard Ghanem*

Main category: cs.AI

TL;DR: 提出了PAVE方法，通过在知识感知子空间中净化任务向量来改进模型合并性能，消除任务向量中的冗余信息。


<details>
  <summary>Details</summary>
Motivation: 现有模型合并方法中，任务向量包含任务无关的冗余信息，导致合并模型性能下降。现有方法随机丢弃参数缺乏知识感知能力。

Method: 使用训练样本获取协方差矩阵，通过上下文导向的奇异值分解识别知识相关权重分量，在知识感知子空间中分离任务相关和冗余组件，并引入谱秩分配策略进行公平剪枝。

Result: PAVE作为即插即用方案，在多种合并方法、任务和模型架构中均能有效提升性能。

Conclusion: PAVE方法能够有效净化任务向量中的冗余信息，显著改善模型合并的性能，且具有广泛的适用性。

Abstract: Model merging aims to integrate task-specific abilities from individually
fine-tuned models into a single model without extra training. In recent model
merging methods, task vector has become a fundamental building block, as it can
encapsulate the residual information from finetuning. However, the merged model
often suffers from notable performance degradation due to the conflicts caused
by task-irrelevant redundancy in task vectors. Existing efforts in overcoming
redundancy by randomly dropping elements in the parameter space involves
randomness and lacks knowledge awareness. To address these challenges, in this
study, we propose Purifying TAsk Vectors (PAVE) in knowledge-aware subspace.
Concretely, we sample some training examples from each task, and feed them into
their corresponding fine-tuned models to acquire the covariance matrices before
linear layers. We then perform a context-oriented singular value decomposition,
which accentuates the weight components most relevant to the target knowledge.
As a result, we can split fine-tuned model weights into task-relevant and
redundant components in the knowledge-aware subspace, and purify the task
vector by pruning the redundant components. To induce fair pruning efforts
across models, we further introduce a spectral rank allocation strategy by
optimizing a normalized activated pruning error. The task vector purification
by our method as a plug-and-play scheme is applicable across various task
vector-based merging methods to improve their performance. In experiments, we
demonstrate the effectiveness of PAVE across a diverse set of merging methods,
tasks, and model architectures.

</details>


### [81] [Cognitive-Aligned Spatio-Temporal Large Language Models For Next Point-of-Interest Prediction](https://arxiv.org/abs/2510.14702)
*Penglong Zhai,Jie Li,Fanyi Di,Yue Liu,Yifang Yuan,Jie Huang,Peng Wu,Sicong Wang,Mingyang Yin,Tingting Hu,Yao Xu,Xin Li*

Main category: cs.AI

TL;DR: CoAST框架通过自然语言接口整合世界知识、时空轨迹模式和用户信息，解决LLM在POI推荐中缺乏地理实体理解和认知对齐的问题，包含推荐知识获取和认知对齐两个阶段。


<details>
  <summary>Details</summary>
Motivation: 现有LLM主要基于非结构化文本预训练，缺乏对结构化地理实体和移动模式的理解，且工业级POI推荐需要整合季节、天气、假期等世界知识和用户认知信息来提升体验和性能。

Method: CoAST框架包含两个阶段：(1)在脱敏用户时空轨迹数据上继续预训练获取推荐知识；(2)通过监督微调和强化学习将认知判断与人类偏好对齐。

Result: 在多个真实数据集上的离线实验和在AMAP App首页"猜你想去"的在线实验都证明了CoAST的有效性。

Conclusion: CoAST通过自然语言接口成功整合了世界知识、时空轨迹模式和认知信息，显著提升了POI推荐性能。

Abstract: The next point-of-interest (POI) recommendation task aims to predict the
users' immediate next destinations based on their preferences and historical
check-ins, holding significant value in location-based services. Recently,
large language models (LLMs) have shown great potential in recommender systems,
which treat the next POI prediction in a generative manner. However, these
LLMs, pretrained primarily on vast corpora of unstructured text, lack the
native understanding of structured geographical entities and sequential
mobility patterns required for next POI prediction tasks. Moreover, in
industrial-scale POI prediction applications, incorporating world knowledge and
alignment of human cognition, such as seasons, weather conditions, holidays,
and users' profiles (such as habits, occupation, and preferences), can enhance
the user experience while improving recommendation performance. To address
these issues, we propose CoAST (Cognitive-Aligned Spatial-Temporal LLMs), a
framework employing natural language as an interface, allowing for the
incorporation of world knowledge, spatio-temporal trajectory patterns,
profiles, and situational information. Specifically, CoAST mainly comprises of
2 stages: (1) Recommendation Knowledge Acquisition through continued
pretraining on the enriched spatial-temporal trajectory data of the
desensitized users; (2) Cognitive Alignment to align cognitive judgments with
human preferences using enriched training data through Supervised Fine-Tuning
(SFT) and a subsequent Reinforcement Learning (RL) phase. Extensive offline
experiments on various real-world datasets and online experiments deployed in
"Guess Where You Go" of AMAP App homepage demonstrate the effectiveness of
CoAST.

</details>


### [82] [ToolPRM: Fine-Grained Inference Scaling of Structured Outputs for Function Calling](https://arxiv.org/abs/2510.14703)
*Jianghao Lin,Yuanyuan Shi,Xin Peng,Renjie Ding,Hairui Wang,Yuxuan Peng,Bizhe Bai,Weixi Song,Fengshuo Bai,Huacan Chai,Weinan Zhang,Fei Huang,Ying Wen*

Main category: cs.AI

TL;DR: 提出了一个结合细粒度波束搜索和过程奖励模型ToolPRM的推理扩展框架，用于提升LLM在结构化输出（如函数调用）任务中的性能。


<details>
  <summary>Details</summary>
Motivation: 当前推理扩展研究主要关注非结构化输出生成任务，而在结构化输出（如函数调用）中的应用研究不足，需要填补这一空白。

Method: 构建了首个细粒度过程监督数据集，使用函数掩码技术自动标注步骤级奖励；提出ToolPRM过程奖励模型来评分单个函数调用的内部步骤；结合细粒度波束搜索进行推理扩展。

Result: ToolPRM在预测准确性上优于粗粒度和结果奖励模型；配备ToolPRM的推理扩展技术显著提升了骨干模型在各种函数调用任务和基准测试中的性能。

Conclusion: 揭示了将推理扩展技术应用于结构化输出的关键原则："多探索少保留"，这是由于结构化函数调用生成的不可恢复性特征决定的。

Abstract: Large language models (LLMs) are increasingly demonstrating strong
capabilities as autonomous agents, with function calling serving as a core
mechanism for interaction with the environment. Meanwhile, inference scaling
has become a cutting-edge technique to enhance LLM performance by allocating
more computational resources during the inference process. However, current
research on inference scaling primarily focuses on unstructured output
generation tasks, leaving its application in structured outputs, like function
calling, largely underexplored. To bridge this gap, we propose an inference
scaling framework that combines fine-grained beam search with a process reward
model, ToolPRM, which scores the internal steps of each single function call.
To train ToolPRM, we construct the first fine-grained intra-call process
supervision dataset, automatically annotated with function-masking techniques
to provide step-level rewards for structured tool-use reasoning. Extensive
experiments demonstrate that ToolPRM beats the coarse-grained and outcome
reward models in terms of predictive accuracy, indicating its stronger
capability in supervising the function calling inference process. Inference
scaling technique equipped with ToolPRM also significantly improves the
backbone model performance across various function calling tasks and
benchmarks. More importantly, we reveal a key principle for applying inference
scaling techniques to structured outputs: "explore more but retain less" due to
the unrecoverability characteristics of structured function calling generation.

</details>


### [83] [SimKO: Simple Pass@K Policy Optimization](https://arxiv.org/abs/2510.14807)
*Ruotian Peng,Yi Ren,Zhouliang Yu,Weiyang Liu,Yandong Wen*

Main category: cs.AI

TL;DR: 论文分析了RLVR方法存在的系统性偏差问题，提出了SimKO方法来缓解概率过度集中现象，从而提高模型的探索能力。


<details>
  <summary>Details</summary>
Motivation: 现有的RLVR方法存在系统性偏差，倾向于利用而非探索，表现为pass@1提升但pass@K下降。需要理解并解决这种概率过度集中问题。

Method: 提出SimKO方法，采用非对称设计：对验证正确的响应，提升top-K候选的概率；对验证错误的响应，对top-1候选施加更强惩罚。特别关注高熵token的应用。

Result: 在多个数学和逻辑推理基准测试中，SimKO方法在各种K值下都能持续获得更高的pass@K性能。

Conclusion: SimKO提供了一种简单有效的方法来改善RLVR的探索能力，通过缓解概率过度集中问题，实现了更好的多样性输出。

Abstract: Reinforcement learning with verifiable rewards (RLVR) has advanced the
reasoning capabilities of large language models (LLMs). However, prevailing
RLVR methods exhibit a systematic bias toward exploitation over exploration, as
evidenced by improved pass@1 but reduced pass@K (K>1) performance. To
understand this issue, we analyze training dynamics of RLVR methods by tracking
the token-level probability distributions over vocabulary candidates. Our
analysis reveals a consistent probability concentration effect where the top-1
candidate increasingly accumulates probability mass and suppresses that of
other candidates. More importantly, stronger over-concentration correlates with
worse pass@K performance. Inspired by this finding, we propose Simple Pass@K
Optimization (SimKO), a method designed to mitigate the over-concentration
issue, thereby encouraging exploration. SimKO operates in an asymmetrical
manner. For verified-correct responses, it boosts the probabilities of the
top-K candidates. For verified-incorrect responses, it applies stronger
penalties to the top-1 candidate. We observe that this asymmetric design is
particularly effective at mitigating over-concentration when applied at tokens
with high entropy. Across various math and logical-reasoning benchmarks, SimKO
consistently yields higher pass@K for a wide range of K, providing a simple way
to improve RLVR's exploration.

</details>


### [84] [Agentic NL2SQL to Reduce Computational Costs](https://arxiv.org/abs/2510.14808)
*Dominik Jehle,Lennart Purucker,Frank Hutter*

Main category: cs.AI

TL;DR: Datalake Agent是一个代理系统，通过交互式循环减少NL2SQL任务中的元信息使用，显著降低LLM的token消耗和成本，同时保持竞争力。


<details>
  <summary>Details</summary>
Motivation: 传统NL2SQL方法需要处理大量数据库元信息，导致提示过长、token消耗大、处理成本高。

Method: 采用代理系统设计，使用交互式循环和推理框架，选择性请求必要信息来解决表格问答任务。

Result: 在23个数据库和100个表格问答任务上评估，Datalake Agent将LLM使用的token减少高达87%，同时保持竞争性能。

Conclusion: Datalake Agent通过减少元信息使用，实现了显著的token和成本节约，同时维持了NL2SQL任务的性能。

Abstract: Translating natural language queries into SQL queries (NL2SQL or Text-to-SQL)
has recently been empowered by large language models (LLMs). Using LLMs to
perform NL2SQL methods on a large collection of SQL databases necessitates
processing large quantities of meta-information about the databases, which in
turn results in lengthy prompts with many tokens and high processing costs. To
address this challenge, we introduce Datalake Agent, an agentic system designed
to enable an LLM to solve NL2SQL tasks more efficiently. Instead of utilizing
direct solvers for NL2SQL that call the LLM once with all meta-information in
the prompt, the Datalake Agent employs an interactive loop to reduce the
utilized meta-information. Within the loop, the LLM is used in a reasoning
framework that selectively requests only the necessary information to solve a
table question answering task. We evaluate the Datalake Agent on a collection
of 23 databases with 100 table question answering tasks. The Datalake Agent
reduces the tokens used by the LLM by up to 87\% and thus allows for
substantial cost reductions while maintaining competitive performance.

</details>


### [85] [RoboGPT-R1: Enhancing Robot Planning with Reinforcement Learning](https://arxiv.org/abs/2510.14828)
*Jinrui Liu,Bingyan Nie,Boyu Li,Yaran Chen,Yuze Wang,Shunsen He,Haoran Li*

Main category: cs.AI

TL;DR: 提出了RoboGPT-R1，一个两阶段微调框架，通过监督训练获取基础知识，然后通过强化学习提升视觉空间理解和推理能力，在EmbodiedBench基准上显著优于GPT-4o-mini和其他模型。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型和视觉语言模型在规划任务中取得成功，但在复杂现实环境中执行长视野操作任务时仍面临挑战，因为它们的常识和推理能力受限。

Method: 采用两阶段微调框架：监督训练通过专家序列获取基础知识，然后使用强化学习解决模型在视觉空间理解和推理方面的不足。设计了基于规则的奖励函数，同时考虑长视野性能和动作约束。

Result: 在Qwen2.5-VL-3B上训练的推理模型，在EmbodiedBench基准上显著优于GPT-4o-mini 21.33%，并超过其他在Qwen2.5-VL-7B上训练的工作20.33%。

Conclusion: RoboGPT-R1框架通过结合监督训练和强化学习，有效提升了具身智能体的推理能力，在复杂操作任务中表现出色。

Abstract: Improving the reasoning capabilities of embodied agents is crucial for robots
to complete complex human instructions in long-view manipulation tasks
successfully. Despite the success of large language models and vision language
models based on Supervised Fine-Tuning (SFT) in planning tasks, they continue
facing challenges in performing long-horizon manipulation tasks in complex
real-world environments, owing to their restricted common sense and reasoning
capabilities. Considering that aligning general-purpose vision language models
to robotic planning tasks via supervised fine-tuning suffers from poor
generalization and insufficient physical understanding, we propose RoboGPT-R1,
a two-stage fine-tuning framework for embodied planning. In this framework,
supervised training acquires foundational knowledge through expert sequences,
followed by RL to address the model's shortcomings in visual-spatial
understanding and reasoning. To achieve physical understanding and action
sequence consistency in multi-step reasoning tasks, we design a rule-based
reward function that simultaneously considers long-horizon performance and
action constraint in the environment. The reasoning model, trained on
Qwen2.5-VL-3B, significantly outperforms the larger-scale model, GPT-4o-mini,
by 21.33% and surpasses other work trained on Qwen2.5-VL-7B by 20.33% on the
EmbodiedBench benchmark.

</details>


### [86] [Boosting Instruction Following at Scale](https://arxiv.org/abs/2510.14842)
*Ben Elder,Evelyn Duesterwald,Vinod Muthusamy*

Main category: cs.AI

TL;DR: Instruction Boosting是一种后生成方法，通过增加指令遵循率来提高LLM提示指令的可靠性，在2条指令时提升7个百分点，10条指令时提升4个百分点。


<details>
  <summary>Details</summary>
Motivation: 开发人员通常通过操作提示来影响LLM行为，但仅添加更多指令无法保证它们会被遵循，需要提高指令遵循的可靠性。

Method: 引入Instruction Boosting作为后生成方法，并创建SCALEDIF基准测试，包含最多10条指令的数据样本，还开发了量化冲突评分工具。

Result: Instruction Boosting显著提高了指令遵循率，同时发现随着指令数量增加，性能下降的主要原因是指令间的紧张和冲突增加。

Conclusion: Instruction Boosting有效提升LLM指令遵循能力，冲突评分工具为开发人员提供了评估额外提示指令对模型性能影响的反馈。

Abstract: A typical approach developers follow to influence an LLM's behavior in an
application is through careful manipulation of the prompt, such as by adding or
modifying instructions. However, merely adding more instructions provides
little assurance that they will actually be followed. We introduce Instruction
Boosting as a post-generation method to increase the reliability of LLM prompt
instructions. We show that Instruction Boosting improves the instruction
following rate by up to 7 points for two instructions and up to 4 points for
ten instructions. To demonstrate these results we introduce SCALEDIF, a
benchmark with a scaled instruction volume of up to ten instructions per data
sample. We also present an analysis of the commonly observed trend that
performance degrades as more instructions are added. We show that an important
factor contributing to this trend is the degree of tension and conflict that
arises as the number of instructions is increased. We contribute a quantitative
conflict scoring tool that explains the observed performance trends and
provides feedback to developers on the impact that additional prompt
instructions have on a model's performance.

</details>


### [87] [Where to Search: Measure the Prior-Structured Search Space of LLM Agents](https://arxiv.org/abs/2510.14846)
*Zhuo-Yang Song*

Main category: cs.AI

TL;DR: 提出了一种紧凑的形式理论来描述和衡量基于领域先验的LLM辅助迭代搜索，通过模糊关系算子表示智能体，引入覆盖生成函数来衡量可达性难度，并提供几何解释和可测试的推论。


<details>
  <summary>Details</summary>
Motivation: LLM支持的生成-过滤-精炼迭代范式在推理、编程和科学发现中取得进展，但搜索效果依赖于如何将领域先验编码为操作化结构化的假设空间。

Method: 将智能体表示为输入输出的模糊关系算子，通过单一延续参数加权所有可达路径得到覆盖生成函数，从而衡量可达性难度，并提供搜索的几何解释。

Result: 提供了可测试的推论并通过多数投票实例进行验证，为衡量智能体及其搜索空间提供了可行的语言和操作工具。

Conclusion: 该理论为LLM构建的迭代搜索提供了系统化的形式描述框架，能够有效衡量智能体和搜索空间的特性。

Abstract: The generate-filter-refine (iterative paradigm) based on large language
models (LLMs) has achieved progress in reasoning, programming, and program
discovery in AI+Science. However, the effectiveness of search depends on where
to search, namely, how to encode the domain prior into an operationally
structured hypothesis space. To this end, this paper proposes a compact formal
theory that describes and measures LLM-assisted iterative search guided by
domain priors. We represent an agent as a fuzzy relation operator on inputs and
outputs to capture feasible transitions; the agent is thereby constrained by a
fixed safety envelope. To describe multi-step reasoning/search, we weight all
reachable paths by a single continuation parameter and sum them to obtain a
coverage generating function; this induces a measure of reachability
difficulty; and it provides a geometric interpretation of search on the graph
induced by the safety envelope. We further provide the simplest testable
inferences and validate them via a majority-vote instantiation. This theory
offers a workable language and operational tools to measure agents and their
search spaces, proposing a systematic formal description of iterative search
constructed by LLMs.

</details>


### [88] [LabOS: The AI-XR Co-Scientist That Sees and Works With Humans](https://arxiv.org/abs/2510.14861)
*Le Cong,Zaixi Zhang,Xiaotong Wang,Yin Di,Ruofan Jin,Michal Gerasimiuk,Yinkai Wang,Ravi K. Dinesh,David Smerkous,Alex Smerkous,Xuekun Wu,Shilong Liu,Peishan Li,Yi Zhu,Simran Serrao,Ning Zhao,Imran A. Mohammad,John B. Sunwoo,Joseph C. Wu,Mengdi Wang*

Main category: cs.AI

TL;DR: LabOS是首个将计算推理与物理实验结合的AI合作科学家系统，通过多模态感知、自进化代理和XR技术实现人机协作，让AI能够理解实验环境并实时协助执行。


<details>
  <summary>Details</summary>
Motivation: 现代科学发展需要思想与行动的结合，当前AI主要停留在计算设计阶段，缺乏与物理实验的深度整合。

Method: 通过连接多模型AI代理、智能眼镜和人机协作，实现AI对实验环境的视觉感知和实时协助执行。

Result: 在癌症免疫治疗靶点发现和干细胞工程等应用中，LabOS成功展示了AI从计算设计到实验参与的转变。

Conclusion: LabOS将实验室转变为智能协作环境，推动人机发现共同进化。

Abstract: Modern science advances fastest when thought meets action. LabOS represents
the first AI co-scientist that unites computational reasoning with physical
experimentation through multimodal perception, self-evolving agents, and
Entended-Reality(XR)-enabled human-AI collaboration. By connecting multi-model
AI agents, smart glasses, and human-AI collaboration, LabOS allows AI to see
what scientists see, understand experimental context, and assist in real-time
execution. Across applications--from cancer immunotherapy target discovery to
stem-cell engineering -- LabOS shows that AI can move beyond computational
design to participation, turning the laboratory into an intelligent,
collaborative environment where human and machine discovery evolve together.

</details>


### [89] [The Gatekeeper Knows Enough](https://arxiv.org/abs/2510.14881)
*Fikresilase Wondmeneh Abebayew*

Main category: cs.AI

TL;DR: 提出了Gatekeeper Protocol框架，通过低保真潜在状态表示和按需请求高保真上下文的方法，解决LLM代理在复杂知识系统中面临的上下文窗口限制和状态不同步问题。


<details>
  <summary>Details</summary>
Motivation: LLM作为自主代理部署时，受限于有限的上下文窗口和状态不同步问题，导致输出不可靠、行为不可预测和资源使用效率低下，特别是在与代码库和文档等结构化知识系统交互时。

Method: 引入Gatekeeper Protocol框架，要求代理首先在低保真潜在状态表示上进行操作和推理，然后按需策略性地请求高保真上下文。所有交互通过统一的JSON格式进行，作为声明式、状态同步的协议。

Result: 在软件开发的Sage参考实现中，该方法显著提高了代理可靠性，通过最小化token消耗改善了计算效率，并实现了与复杂系统的可扩展交互。

Conclusion: Gatekeeper Protocol为在任何结构化知识领域构建更稳健、可预测和基于现实的AI代理提供了基础方法。

Abstract: Large Language Models (LLMs) are increasingly deployed as autonomous agents,
yet their practical utility is fundamentally constrained by a limited context
window and state desynchronization resulting from the LLMs' stateless nature
and inefficient context management. These limitations lead to unreliable
output, unpredictable behavior, and inefficient resource usage, particularly
when interacting with large, structured, and sensitive knowledge systems such
as codebases and documents. To address these challenges, we introduce the
Gatekeeper Protocol, a novel, domain-agnostic framework that governs
agent-system interactions. Our protocol mandates that the agent first operate
and reason on a minimalist, low-fidelity "latent state" representation of the
system to strategically request high-fidelity context on demand. All
interactions are mediated through a unified JSON format that serves as a
declarative, state-synchronized protocol, ensuring the agent's model of the
system remains verifiably grounded in the system's reality. We demonstrate the
efficacy of this protocol with Sage, a reference implementation of the
Gatekeeper Protocol for software development. Our results show that this
approach significantly increases agent reliability, improves computational
efficiency by minimizing token consumption, and enables scalable interaction
with complex systems, creating a foundational methodology for building more
robust, predictable, and grounded AI agents for any structured knowledge
domain.

</details>


### [90] [Budget-aware Test-time Scaling via Discriminative Verification](https://arxiv.org/abs/2510.14913)
*Kyle Montgomery,Sijun Tan,Yuqi Chen,Siyuan Zhuang,Tianjun Zhang,Raluca Ada Popa,Chenguang Wang*

Main category: cs.AI

TL;DR: 提出了一种结合判别式验证器和自一致性的混合方法，在固定计算预算下显著优于生成式验证方法，在AIME2025上准确率提升达15.3%。


<details>
  <summary>Details</summary>
Motivation: 现有的生成式验证器虽然性能优秀但计算成本过高，限制了实际应用。需要寻找更高效的测试时扩展策略。

Method: 采用判别式验证器与自一致性结合的混合方法，在固定计算预算下进行预算感知的测试时扩展。

Result: 在AIME2025任务上，该方法比最先进的生成式验证方法准确率高出15.3%，且计算效率更高。

Conclusion: 对于实际应用，基于判别式验证器的预算感知扩展不仅是自一致性的免费升级，也是比昂贵生成式技术更有效和高效的替代方案。

Abstract: Test-time scaling is a powerful strategy for boosting the performance of
large language models on complex reasoning tasks. While state-of-the-art
approaches often employ generative verifiers to select the best solution from a
pool of candidates, this method incurs prohibitive computational costs,
limiting its practicality. In this work, we shift the focus to a more
budget-aware paradigm: discriminative verification. We conduct a thorough
empirical analysis and demonstrate that while discriminative verifiers may
underperform in isolation, combining them with self-consistency in a hybrid
approach creates a powerful and efficient test-time scaling mechanism. Notably,
under a fixed compute budget, this hybrid approach surpasses state-of-the-art
generative verification by a significant margin: achieving up to 15.3\% higher
accuracy on AIME2025. Our findings establish that for practical, real-world
applications, budget-aware scaling with discriminative verifiers is not only a
"free" upgrade over self-consistency, but also a more effective and efficient
alternative to costly generative techniques. Code is available at
https://github.com/wang-research-lab/verification.

</details>


### [91] [TRI-DEP: A Trimodal Comparative Study for Depression Detection Using Speech, Text, and EEG](https://arxiv.org/abs/2510.14922)
*Annisaa Fitri Nurfidausi,Eleonora Mancini,Paolo Torroni*

Main category: cs.AI

TL;DR: 该论文系统探索了使用EEG、语音和文本进行多模态抑郁症检测，比较了手工特征与预训练嵌入、不同神经编码器、单模态/双模态/三模态配置以及融合策略，发现三模态组合能提升检测性能。


<details>
  <summary>Details</summary>
Motivation: 抑郁症自动检测面临挑战，现有研究范围有限、缺乏系统特征比较、评估协议不一致。本文旨在解决这些差距，为多模态抑郁症检测提供系统研究基础。

Method: 系统探索EEG、语音和文本的特征表示和建模策略，评估手工特征与预训练嵌入，比较不同神经编码器，分析单模态、双模态和三模态配置，研究融合策略并关注EEG的作用，使用一致的受试者独立分割进行稳健评估。

Result: 结果显示：(i) EEG、语音和文本多模态组合增强检测性能；(ii) 预训练嵌入优于手工特征；(iii) 精心设计的三模态模型达到最先进性能。

Conclusion: 本研究为未来多模态抑郁症检测研究奠定了基础，证明了多模态方法的有效性。

Abstract: Depression is a widespread mental health disorder, yet its automatic
detection remains challenging. Prior work has explored unimodal and multimodal
approaches, with multimodal systems showing promise by leveraging complementary
signals. However, existing studies are limited in scope, lack systematic
comparisons of features, and suffer from inconsistent evaluation protocols. We
address these gaps by systematically exploring feature representations and
modelling strategies across EEG, together with speech and text. We evaluate
handcrafted features versus pre-trained embeddings, assess the effectiveness of
different neural encoders, compare unimodal, bimodal, and trimodal
configurations, and analyse fusion strategies with attention to the role of
EEG. Consistent subject-independent splits are applied to ensure robust,
reproducible benchmarking. Our results show that (i) the combination of EEG,
speech and text modalities enhances multimodal detection, (ii) pretrained
embeddings outperform handcrafted features, and (iii) carefully designed
trimodal models achieve state-of-the-art performance. Our work lays the
groundwork for future research in multimodal depression detection.

</details>


### [92] [Stable but Miscalibrated: A Kantian View on Overconfidence from Filters to Large Language Models](https://arxiv.org/abs/2510.14925)
*Akira Okutomi*

Main category: cs.AI

TL;DR: 将康德的《纯粹理性批判》重新解释为反馈稳定性理论，提出复合不稳定性指数H-Risk来衡量推理系统的稳定性，并在线性高斯模拟和LLMs中验证其与过度自信错误、幻觉的相关性。


<details>
  <summary>Details</summary>
Motivation: 将康德的理性自我限制理论与反馈控制理论联系起来，为诊断和减少推理系统中的过度自信提供原则性框架。

Method: 提出复合不稳定性指数H-Risk（结合谱边界、条件数、时间敏感性和创新放大），在线性高斯模拟和大型语言模型中进行实验验证。

Result: 在线性高斯模拟中，高H-Risk预测过度自信错误；在LLMs中，脆弱的内部动态与错误校准和幻觉相关，批判式提示对校准和幻觉有混合效果。

Conclusion: 建立了康德自我限制与反馈控制之间的结构性桥梁，为诊断和选择性减少推理系统中的过度自信提供了原则性视角。

Abstract: We reinterpret Kant's Critique of Pure Reason as a theory of feedback
stability, viewing reason as a regulator that keeps inference within the bounds
of possible experience. We formalize this intuition via a composite instability
index (H-Risk) combining spectral margin, conditioning, temporal sensitivity,
and innovation amplification. In linear-Gaussian simulations, higher H-Risk
predicts overconfident errors even under formal stability, revealing a gap
between nominal and epistemic stability. Extending to large language models
(LLMs), we find that fragile internal dynamics correlate with miscalibration
and hallucination, while critique-style prompts show mixed effects on
calibration and hallucination. These results suggest a structural bridge
between Kantian self-limitation and feedback control, offering a principled
lens for diagnosing -- and selectively reducing -- overconfidence in reasoning
systems. This is a preliminary version; supplementary experiments and broader
replication will be reported in a future revision.

</details>


### [93] [GroundedPRM: Tree-Guided and Fidelity-Aware Process Reward Modeling for Step-Level Reasoning](https://arxiv.org/abs/2510.14942)
*Yao Zhang,Yu Wu,Haowei Zhang,Weiguo Li,Haokun Chen,Jingpei Wu,Guohao Li,Zhen Han,Volker Tresp*

Main category: cs.AI

TL;DR: GroundedPRM是一个基于树搜索和外部工具验证的自动过程监督框架，通过MCTS构建结构化推理路径，使用外部工具验证中间步骤，结合步骤级验证和全局结果评估，仅用10%的数据量就实现了优于人工标注PRM的性能。


<details>
  <summary>Details</summary>
Motivation: 现有过程奖励模型(PRMs)面临三大挑战：噪声奖励、低事实保真度以及与步骤级推理目标不一致。这些问题源于缺乏可扩展的高质量标注，现有方法依赖昂贵的人工标注、易产生幻觉的LLM自评估，或仅从结果推断步骤质量的蒙特卡洛估计。

Method: 1. 使用蒙特卡洛树搜索(MCTS)构建结构化推理路径，实现细粒度信用分配；2. 通过外部工具验证每个中间步骤，提供执行基础的正确性信号；3. 设计混合奖励聚合机制，融合工具验证和MCTS反馈；4. 将奖励信号格式化为理由增强的生成结构，提高可解释性。

Result: 仅使用4万个自动标注样本(最佳自动标注PRM数据量的10%)，在ProcessBench上实现了高达26%的相对性能提升。在奖励引导的贪婪搜索中，甚至优于使用人工标注监督训练的PRMs。

Conclusion: GroundedPRM为高质量过程级推理提供了一条可扩展且可验证的路径，通过树引导和保真度感知的框架有效解决了现有PRMs的核心限制。

Abstract: Process Reward Models (PRMs) aim to improve multi-step reasoning in Large
Language Models (LLMs) by supervising intermediate steps and identifying
errors. However, building effective PRMs remains challenging due to the lack of
scalable, high-quality annotations. Existing approaches rely on costly human
labeling, LLM-based self-evaluation that is prone to hallucination, or Monte
Carlo (MC) estimation, which infers step quality solely from rollout outcomes
and often introduces noisy, misaligned supervision due to credit
misattribution. These issues result in three core limitations: noisy rewards,
low factual fidelity, and misalignment with step-level reasoning objectives. To
address these challenges, we introduce GroundedPRM, a tree-guided and
fidelity-aware framework for automatic process supervision. To reduce reward
noise and enable fine-grained credit assignment, we construct structured
reasoning paths via Monte Carlo Tree Search (MCTS). To eliminate hallucinated
supervision, we validate each intermediate step using an external tool,
providing execution-grounded correctness signals. To combine both step-level
validation and global outcome assessment, we design a hybrid reward aggregation
mechanism that fuses tool-based verification with MCTS-derived feedback.
Finally, we format the reward signal into a rationale-enhanced, generative
structure to promote interpretability and compatibility with instruction-tuned
LLMs. GroundedPRM is trained on only 40K automatically labeled samples,
amounting to just 10% of the data used by the best-performing PRM trained with
auto-labeled supervision. Nevertheless, it achieves up to a 26% relative
improvement in average performance on ProcessBench. When used for reward-guided
greedy search, GroundedPRM outperforms even PRMs trained with human-labeled
supervision, offering a scalable and verifiable path toward high-quality
process-level reasoning.

</details>


### [94] [Agentic Design of Compositional Machines](https://arxiv.org/abs/2510.14980)
*Wenqian Zhang,Weiyang Liu,Zhen Liu*

Main category: cs.AI

TL;DR: 本文研究了大型语言模型在组合式机器设计任务中的能力，开发了BesiegeField测试平台来评估LLM在物理环境中构建功能机器的表现，并探索了强化学习作为改进途径。


<details>
  <summary>Details</summary>
Motivation: 探索大型语言模型是否能够学习创造复杂机器，这既是人类智能的标志，也是工程实践的基础。通过组合式机器设计任务来评估LLM的创造能力。

Method: 开发了基于Besiege游戏的BesiegeField测试平台，支持基于部件的构建、物理模拟和奖励驱动评估。对最先进的LLM进行基准测试，并探索强化学习微调作为改进方法。

Result: 当前开源模型在空间推理、策略性组装和指令遵循等关键能力上表现不足。通过强化学习微调实验展示了改进潜力，但机器设计、语言理解和物理推理交叉领域仍存在挑战。

Conclusion: 大型语言模型在机器设计任务中展现出潜力但仍有局限，强化学习是改进的有效途径，但需要解决语言、机器设计和物理推理交叉领域的开放挑战。

Abstract: The design of complex machines stands as both a marker of human intelligence
and a foundation of engineering practice. Given recent advances in large
language models (LLMs), we ask whether they, too, can learn to create. We
approach this question through the lens of compositional machine design: a task
in which machines are assembled from standardized components to meet functional
demands like locomotion or manipulation in a simulated physical environment. To
support this investigation, we introduce BesiegeField, a testbed built on the
machine-building game Besiege, which enables part-based construction, physical
simulation and reward-driven evaluation. Using BesiegeField, we benchmark
state-of-the-art LLMs with agentic workflows and identify key capabilities
required for success, including spatial reasoning, strategic assembly, and
instruction-following. As current open-source models fall short, we explore
reinforcement learning (RL) as a path to improvement: we curate a cold-start
dataset, conduct RL finetuning experiments, and highlight open challenges at
the intersection of language, machine design, and physical reasoning.

</details>
