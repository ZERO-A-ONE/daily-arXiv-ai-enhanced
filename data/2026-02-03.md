<div id=toc></div>

# Table of Contents

- [cs.CR](#cs.CR) [Total: 20]
- [cs.AI](#cs.AI) [Total: 40]
- [cs.SE](#cs.SE) [Total: 21]


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [1] [ShellForge: Adversarial Co-Evolution of Webshell Generation and Multi-View Detection for Robust Webshell Defense](https://arxiv.org/abs/2601.22182)
*Yizhong Ding*

Main category: cs.CR

TL;DR: ShellForge是一个对抗性协同进化框架，通过自动化webshell生成和多视图检测来增强PHP webshell防御能力，显著提升检测性能并降低误报率。


<details>
  <summary>Details</summary>
Motivation: 现有webshell检测机制难以应对快速变种和复杂混淆技术，且在面对用于知识产权保护的良性混淆脚本时误报率高，需要更鲁棒的防御方案。

Method: 采用对抗性协同训练框架，包含生成器和检测器的迭代循环。生成器通过监督微调和基于偏好的强化学习合成功能性强、高度规避的变种；检测器融合长字符串压缩的语义特征、修剪抽象语法树的结构特征和香农熵等全局统计指标；使用LLM生成去恶意样本作为高质量负样本。

Result: 在FWOID基准测试中，检测器收敛后保持0.981的F1分数，生成器在VirusTotal上对商业引擎达到0.939的规避率，显著提升防御鲁棒性。

Conclusion: ShellForge通过对抗性协同进化有效应对webshell变种和混淆挑战，在保持高检测性能的同时降低误报，为服务器安全提供更强大的防御机制。

Abstract: Webshells remain a primary foothold for attackers to compromise servers, particularly within PHP ecosystems. However, existing detection mechanisms often struggle to keep pace with rapid variant evolution and sophisticated obfuscation techniques that camouflage malicious intent. Furthermore, many current defenses suffer from high false-alarm rates when encountering benign administrative scripts that employ heavy obfuscation for intellectual property protection. To address these challenges, we present ShellForge, an adversarial co-evolution framework that couples automated webshell generation with multi-view detection to continuously harden defensive boundaries. The framework operates through an iterative co-training loop where a generator and a detector mutually reinforce each other via the exchange of hard samples. The generator is optimized through supervised fine-tuning and preference-based reinforcement learning to synthesize functional, highly evasive variants. Simultaneously, we develop a multi-view fusion detector that integrates semantic features from long-string compression, structural features from pruned abstract syntax trees, and global statistical indicators such as Shannon entropy. To minimize false positives, ShellForge utilizes a LLM-based transformation to create de-malicious samples--scripts that retain complex obfuscation patterns but lack harmful payloads--serving as high-quality hard negatives during training. Evaluations on the public FWOID benchmark demonstrate that ShellForge significantly enhances defensive robustness. Upon convergence, the detector maintains a 0.981 F1-score while the generator achieves a 0.939 evasion rate against commercial engines on VirusTotal.

</details>


### [2] [MemeChain: A Multimodal Cross-Chain Dataset for Meme Coin Forensics and Risk Analysis](https://arxiv.org/abs/2601.22185)
*Alberto Maria Mongardini,Alessandro Mei*

Main category: cs.CR

TL;DR: MemeChain是一个大规模、开源、跨链数据集，包含34,988个模因币，整合了链上数据和链下工件（网站HTML、代币logo、社交媒体账户），用于模因币生态系统的多模态和法证研究。


<details>
  <summary>Details</summary>
Motivation: 模因币生态系统是加密货币市场中最活跃但最不可观测的领域，具有极端流失率、最低项目承诺和广泛欺诈行为。现有数据集通常限于单链数据或缺乏多模态工件，无法进行全面的风险建模。

Method: 构建MemeChain数据集，涵盖以太坊、BNB智能链、Solana和Base四个区块链上的34,988个模因币，整合链上数据与链下工件（网站HTML源代码、代币logo、链接的社交媒体账户）。

Result: 分析显示：1）视觉品牌在低质量部署中经常被省略；2）许多项目缺乏功能性网站；3）生态系统极端波动，1,801个代币（5.15%）在启动后24小时内停止所有交易活动。

Conclusion: MemeChain通过提供统一的跨链覆盖和丰富的链下上下文，成为模因币生态系统中金融法证、多模态异常检测和自动诈骗预防研究的基础资源。

Abstract: The meme coin ecosystem has grown into one of the most active yet least observable segments of the cryptocurrency market, characterized by extreme churn, minimal project commitment, and widespread fraudulent behavior. While countless meme coins are deployed across multiple blockchains, they rely heavily on off-chain web and social infrastructure to signal legitimacy. These very signals are largely absent from existing datasets, which are often limited to single-chain data or lack the multimodal artifacts required for comprehensive risk modeling.
  To address this gap, we introduce MemeChain, a large-scale, open-source, cross-chain dataset comprising 34,988 meme coins across Ethereum, BNB Smart Chain, Solana, and Base. MemeChain integrates on-chain data with off-chain artifacts, including website HTML source code, token logos, and linked social media accounts, enabling multimodal and forensic study of meme coin projects. Analysis of the dataset shows that visual branding is frequently omitted in low-effort deployments, and many projects lack a functional website. Moreover, we quantify the ecosystem's extreme volatility, identifying 1,801 tokens (5.15%) that cease all trading activity within just 24 hours of launch. By providing unified cross-chain coverage and rich off-chain context, MemeChain serves as a foundational resource for research in financial forensics, multimodal anomaly detection, and automated scam prevention in the meme coin ecosystem.

</details>


### [3] [A Systematic Literature Review on LLM Defenses Against Prompt Injection and Jailbreaking: Expanding NIST Taxonomy](https://arxiv.org/abs/2601.22240)
*Pedro H. Barcha Correia,Ryan W. Achjian,Diego E. G. Caetano de Oliveira,Ygor Acacio Maria,Victor Takashi Hayashi,Marcos Lopes,Charles Christian Miers,Marcos A. Simplicio*

Main category: cs.CR

TL;DR: 本文首次对提示注入缓解策略进行了系统性文献综述，涵盖88项研究，扩展了NIST对抗机器学习分类法，并提供了包含量化效果的开源防御方案目录。


<details>
  <summary>Details</summary>
Motivation: 随着生成式AI和大语言模型的快速发展，出现了新的安全漏洞如越狱和提示注入攻击。这些恶意输入可能导致数据泄露、未授权操作等问题。由于攻防技术快速演进，需要系统化的缓解策略理解。

Method: 采用系统性文献综述方法，基于NIST对抗机器学习报告框架，扩展了防御分类法，建立了包含88项研究的综合目录，记录量化效果、开源状态和模型无关性。

Result: 提出了扩展的NIST分类法，新增防御类别；创建了全面的提示注入防御目录，包含具体LLM和攻击数据集上的量化效果评估；识别了开源和模型无关的解决方案。

Conclusion: 该研究为对抗机器学习领域提供了标准化的分类框架和实用资源目录，有助于研究人员推进该领域发展，并为开发者在生产系统中实施有效防御提供指导。

Abstract: The rapid advancement and widespread adoption of generative artificial intelligence (GenAI) and large language models (LLMs) has been accompanied by the emergence of new security vulnerabilities and challenges, such as jailbreaking and other prompt injection attacks. These maliciously crafted inputs can exploit LLMs, causing data leaks, unauthorized actions, or compromised outputs, for instance. As both offensive and defensive prompt injection techniques evolve quickly, a structured understanding of mitigation strategies becomes increasingly important. To address that, this work presents the first systematic literature review on prompt injection mitigation strategies, comprehending 88 studies. Building upon NIST's report on adversarial machine learning, this work contributes to the field through several avenues. First, it identifies studies beyond those documented in NIST's report and other academic reviews and surveys. Second, we propose an extension to NIST taxonomy by introducing additional categories of defenses. Third, by adopting NIST's established terminology and taxonomy as a foundation, we promote consistency and enable future researchers to build upon the standardized taxonomy proposed in this work. Finally, we provide a comprehensive catalog of the reviewed prompt injection defenses, documenting their reported quantitative effectiveness across specific LLMs and attack datasets, while also indicating which solutions are open-source and model-agnostic. This catalog, together with the guidelines presented herein, aims to serve as a practical resource for researchers advancing the field of adversarial machine learning and for developers seeking to implement effective defenses in production systems.

</details>


### [4] [MirrorMark: A Distortion-Free Multi-Bit Watermark for Large Language Models](https://arxiv.org/abs/2601.22246)
*Ya Jiang,Massieh Kordi Boroujeny,Surender Suresh Kumar,Kai Zeng*

Main category: cs.CR

TL;DR: MirrorMark是一种用于大语言模型的多比特无失真水印方法，通过镜像采样随机性嵌入多比特消息而不改变token概率分布，保持文本质量的同时显著提升可检测性。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型在问答和内容创作等应用中的普及，可靠的内容溯源变得日益重要。现有水印方法要么只能提供二进制信号，要么会扭曲采样分布降低文本质量；而无失真方法往往存在可检测性弱或鲁棒性差的问题。

Method: 提出MirrorMark方法：1）通过镜像采样随机性的保测度方式嵌入多比特消息，不改变token概率分布；2）引入基于上下文的调度器，平衡消息位置间的token分配，同时保持对插入和删除操作的鲁棒性；3）提供理论分析解释经验性能。

Result: 实验表明MirrorMark与非水印生成的文本质量相当，同时显著提升可检测性：在300个token中嵌入54比特时，比特准确率提高8-12%，在1%误报率下正确识别的水印文本增加高达11%。

Conclusion: MirrorMark是一种有效的多比特无失真水印方法，在保持文本质量的同时显著提升了水印的可检测性和鲁棒性，为大语言模型的内容溯源提供了可靠解决方案。

Abstract: As large language models (LLMs) become integral to applications such as question answering and content creation, reliable content attribution has become increasingly important. Watermarking is a promising approach, but existing methods either provide only binary signals or distort the sampling distribution, degrading text quality; distortion-free approaches, in turn, often suffer from weak detectability or robustness. We propose MirrorMark, a multi-bit and distortion-free watermark for LLMs. By mirroring sampling randomness in a measure-preserving manner, MirrorMark embeds multi-bit messages without altering the token probability distribution, preserving text quality by design. To improve robustness, we introduce a context-based scheduler that balances token assignments across message positions while remaining resilient to insertions and deletions. We further provide a theoretical analysis of the equal error rate to interpret empirical performance. Experiments show that MirrorMark matches the text quality of non-watermarked generation while achieving substantially stronger detectability: with 54 bits embedded in 300 tokens, it improves bit accuracy by 8-12% and correctly identifies up to 11% more watermarked texts at 1% false positive rate.

</details>


### [5] [Rethinking Anonymity Claims in Synthetic Data Generation: A Model-Centric Privacy Attack Perspective](https://arxiv.org/abs/2601.22434)
*Georgi Ganev,Emiliano De Cristofaro*

Main category: cs.CR

TL;DR: 论文重新审视合成数据的匿名性主张，从模型中心视角出发，认为有意义的隐私评估必须考虑底层生成模型的能力和特性，并基于最先进的隐私攻击进行验证。


<details>
  <summary>Details</summary>
Motivation: 当前合成数据隐私评估主要停留在数据集层面，而现实部署中生成模型通常可被交互或查询。需要从模型中心视角重新思考匿名性主张，以更好地反映实际产品和部署情况。

Method: 1. 从模型中心视角重新解释GDPR中个人数据和匿名化的定义；2. 识别不同威胁场景下必须缓解的可识别性风险类型；3. 将这些风险映射到隐私攻击；4. 比较差分隐私(DP)和基于相似性的隐私指标(SBPMs)两种机制。

Result: 1. 合成数据技术本身不足以保证充分的匿名化；2. 差分隐私(DP)能够提供针对可识别性风险的强大保护；3. 基于相似性的隐私指标(SBPMs)缺乏足够的保护措施。

Conclusion: 通过将监管层面的可识别性概念与模型中心的隐私攻击联系起来，为研究人员、从业者和政策制定者提供了更负责任和可信赖的合成数据系统监管评估框架。

Abstract: Training generative machine learning models to produce synthetic tabular data has become a popular approach for enhancing privacy in data sharing. As this typically involves processing sensitive personal information, releasing either the trained model or generated synthetic datasets can still pose privacy risks. Yet, recent research, commercial deployments, and privacy regulations like the General Data Protection Regulation (GDPR) largely assess anonymity at the level of an individual dataset.
  In this paper, we rethink anonymity claims about synthetic data from a model-centric perspective and argue that meaningful assessments must account for the capabilities and properties of the underlying generative model and be grounded in state-of-the-art privacy attacks. This perspective better reflects real-world products and deployments, where trained models are often readily accessible for interaction or querying. We interpret the GDPR's definitions of personal data and anonymization under such access assumptions to identify the types of identifiability risks that must be mitigated and map them to privacy attacks across different threat settings. We then argue that synthetic data techniques alone do not ensure sufficient anonymization. Finally, we compare the two mechanisms most commonly used alongside synthetic data -- Differential Privacy (DP) and Similarity-based Privacy Metrics (SBPMs) -- and argue that while DP can offer robust protections against identifiability risks, SBPMs lack adequate safeguards. Overall, our work connects regulatory notions of identifiability with model-centric privacy attacks, enabling more responsible and trustworthy regulatory assessment of synthetic data systems by researchers, practitioners, and policymakers.

</details>


### [6] [FraudShield: Knowledge Graph Empowered Defense for LLMs against Fraud Attacks](https://arxiv.org/abs/2601.22485)
*Naen Xu,Jinghuai Zhang,Ping He,Chunyi Zhou,Jun Wang,Zhihui Fu,Tianyu Du,Zhaoxiang Wang,Shouling Ji*

Main category: cs.CR

TL;DR: FraudShield是一个保护大语言模型免受欺诈内容攻击的新框架，通过构建欺诈策略-关键词知识图谱来增强模型对可疑文本的识别能力，在多个主流LLM和欺诈类型上表现优于现有防御方法。


<details>
  <summary>Details</summary>
Motivation: 大语言模型已广泛应用于合同审查、求职申请等关键自动化流程，但容易受到欺诈信息的操纵，导致有害后果。现有的防御方法在有效性、可解释性和泛化性方面存在局限，特别是在LLM应用中。

Method: FraudShield通过构建和优化欺诈策略-关键词知识图谱，捕捉可疑文本与欺诈技术之间的高置信度关联。该结构化知识图谱通过突出关键词和提供支持证据来增强原始输入，引导LLM生成更安全的响应。

Result: 大量实验表明，FraudShield在四个主流LLM和五种代表性欺诈类型上始终优于最先进的防御方法，同时为模型的生成提供可解释的线索。

Conclusion: FraudShield通过结构化知识图谱的方法，有效解决了LLM在欺诈内容防御方面的挑战，提供了更有效、可解释且泛化性强的保护方案。

Abstract: Large language models (LLMs) have been widely integrated into critical automated workflows, including contract review and job application processes. However, LLMs are susceptible to manipulation by fraudulent information, which can lead to harmful outcomes. Although advanced defense methods have been developed to address this issue, they often exhibit limitations in effectiveness, interpretability, and generalizability, particularly when applied to LLM-based applications. To address these challenges, we introduce FraudShield, a novel framework designed to protect LLMs from fraudulent content by leveraging a comprehensive analysis of fraud tactics. Specifically, FraudShield constructs and refines a fraud tactic-keyword knowledge graph to capture high-confidence associations between suspicious text and fraud techniques. The structured knowledge graph augments the original input by highlighting keywords and providing supporting evidence, guiding the LLM toward more secure responses. Extensive experiments show that FraudShield consistently outperforms state-of-the-art defenses across four mainstream LLMs and five representative fraud types, while also offering interpretable clues for the model's generations.

</details>


### [7] [VocBulwark: Towards Practical Generative Speech Watermarking via Additional-Parameter Injection](https://arxiv.org/abs/2601.22556)
*Weizhi Liu,Yue Li,Zhaoxia Yin*

Main category: cs.CR

TL;DR: VocBulwark是一个语音水印框架，通过冻结生成模型参数保持音质，使用时间适配器深度嵌入水印，结合粗到细门控提取器抵抗攻击，并通过精度引导优化课程解决保真度与鲁棒性冲突。


<details>
  <summary>Details</summary>
Motivation: 当前语音生成技术已达到人类自然水平，但带来了滥用风险。现有水印方法无法兼顾保真度和鲁棒性，要么在噪声空间简单叠加，要么需要侵入性地修改模型权重。

Method: 提出VocBulwark框架：1) 冻结生成模型参数以保持感知质量；2) 设计时间适配器将水印与声学属性深度纠缠；3) 结合粗到细门控提取器抵抗高级攻击；4) 开发精度引导优化课程动态协调梯度流，解决保真度与鲁棒性的优化冲突。

Result: 综合实验表明，VocBulwark实现了高容量和高保真度的水印，能够有效防御复杂的实际场景攻击，包括编解码器再生和变长操作等。

Conclusion: VocBulwark框架成功解决了语音水印中保真度与鲁棒性的平衡问题，为生成语音的安全使用提供了有效保护方案。

Abstract: Generated speech achieves human-level naturalness but escalates security risks of misuse. However, existing watermarking methods fail to reconcile fidelity with robustness, as they rely either on simple superposition in the noise space or on intrusive alterations to model weights. To bridge this gap, we propose VocBulwark, an additional-parameter injection framework that freezes generative model parameters to preserve perceptual quality. Specifically, we design a Temporal Adapter to deeply entangle watermarks with acoustic attributes, synergizing with a Coarse-to-Fine Gated Extractor to resist advanced attacks. Furthermore, we develop an Accuracy-Guided Optimization Curriculum that dynamically orchestrates gradient flow to resolve the optimization conflict between fidelity and robustness. Comprehensive experiments demonstrate that VocBulwark achieves high-capacity and high-fidelity watermarking, offering robust defense against complex practical scenarios, with resilience to Codec regenerations and variable-length manipulations.

</details>


### [8] [The Semantic Trap: Do Fine-tuned LLMs Learn Vulnerability Root Cause or Just Functional Pattern?](https://arxiv.org/abs/2601.22655)
*Feiyang Huang,Yuqiang Sun,Fan Zhang,Ziqi Yang,Han Liu,Yang Liu*

Main category: cs.CR

TL;DR: 论文发现微调后的LLMs在漏洞检测中可能陷入"语义陷阱"：模型通过关联功能模式而非理解安全语义来获得高分，无法真正区分漏洞代码与其修复版本。


<details>
  <summary>Details</summary>
Motivation: 当前微调后的LLMs在软件漏洞检测中表现出良好性能，但尚不清楚这种提升是源于对漏洞根本原因的真正理解，还是仅仅利用了功能模式的捷径。需要系统评估模型是否真正理解了安全语义。

Method: 提出TrapEval评估框架，包含两个互补数据集：V2N（漏洞代码与无关良性代码配对）和V2P（漏洞代码与其修复版本配对）。微调5个代表性LLMs，在跨数据集测试、语义保留扰动和不同语义差距下进行评估。

Result: 尽管指标有所改善，但微调后的LLMs持续难以区分漏洞代码与其修复版本，在轻微语义保留变换下表现出严重的鲁棒性退化，当语义差距较小时严重依赖功能上下文捷径。

Conclusion: 当前微调实践往往未能传授真正的漏洞推理能力，传统数据集上的高分可能是虚幻的，掩盖了模型无法理解漏洞真正因果逻辑的问题。这为领域敲响了警钟。

Abstract: LLMs demonstrate promising performance in software vulnerability detection after fine-tuning. However, it remains unclear whether these gains reflect a genuine understanding of vulnerability root causes or merely an exploitation of functional patterns. In this paper, we identify a critical failure mode termed the "semantic trap," where fine-tuned LLMs achieve high detection scores by associating certain functional domains with vulnerability likelihood rather than reasoning about the underlying security semantics.To systematically evaluate this phenomenon, we propose TrapEval, a comprehensive evaluation framework designed to disentangle vulnerability root cause from functional pattern. TrapEval introduces two complementary datasets derived from real-world open-source projects: V2N, which pairs vulnerable code with unrelated benign code, and V2P, which pairs vulnerable code with its corresponding patched version, forcing models to distinguish near-identical code that differs only in subtle security-critical logic. Using TrapEval, we fine-tune five representative state-of-the-art LLMs across three model families and evaluate them under cross-dataset testing, semantic-preserving perturbations, and varying degrees of semantic gap measured by CodeBLEU.Our empirical results reveal that, despite improvements in metrics, fine-tuned LLMs consistently struggle to distinguish vulnerable code from its patched counterpart, exhibit severe robustness degradation under minor semantic-preserving transformations, and rely heavily on functional-context shortcuts when the semantic gap is small. These findings provide strong evidence that current fine-tuning practices often fail to impart true vulnerability reasoning. Our findings serve as a wake-up call: high benchmark scores on traditional datasets may be illusory, masking the model's inability to understand the true causal logic of vulnerabilities.

</details>


### [9] [RealSec-bench: A Benchmark for Evaluating Secure Code Generation in Real-World Repositories](https://arxiv.org/abs/2601.22706)
*Yanlin Wang,Ziyao Zhang,Chong Wang,Xinyi Xu,Mingwei Liu,Yong Wang,Jiachi Chen,Zibin Zheng*

Main category: cs.CR

TL;DR: 论文提出了RealSec-bench基准测试，用于评估LLM生成安全代码的能力，发现当前LLM在功能正确性和安全性之间存在显著差距，RAG技术对安全性提升有限，通用安全提示反而可能损害功能性。


<details>
  <summary>Details</summary>
Motivation: 现有代码生成基准测试主要关注功能性，缺乏对安全性的评估，且多使用合成漏洞，无法反映真实世界软件中功能与安全的复杂交互关系。

Method: 构建RealSec-bench基准：从真实高风险Java仓库中，通过多阶段流程（SAST扫描、CodeQL、LLM误报消除、专家验证）收集105个实例，涵盖19种CWE类型。提出SecurePass@K复合指标，评估5个流行LLM的安全代码生成能力。

Result: RAG技术能提升功能正确性但对安全性改善有限；显式安全提示常导致编译失败，损害功能性且不能可靠防止漏洞；当前LLM在功能正确性和安全性之间存在显著差距。

Conclusion: 需要专门针对安全代码生成开发更有效的技术，当前LLM在生成安全代码方面仍有很大改进空间，功能正确性不等于安全性。

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities in code generation, but their proficiency in producing secure code remains a critical, under-explored area. Existing benchmarks often fall short by relying on synthetic vulnerabilities or evaluating functional correctness in isolation, failing to capture the complex interplay between functionality and security found in real-world software. To address this gap, we introduce RealSec-bench, a new benchmark for secure code generation meticulously constructed from real-world, high-risk Java repositories. Our methodology employs a multi-stage pipeline that combines systematic SAST scanning with CodeQL, LLM-based false positive elimination, and rigorous human expert validation. The resulting benchmark contains 105 instances grounded in real-word repository contexts, spanning 19 Common Weakness Enumeration (CWE) types and exhibiting a wide diversity of data flow complexities, including vulnerabilities with up to 34-hop inter-procedural dependencies. Using RealSec-bench, we conduct an extensive empirical study on 5 popular LLMs. We introduce a novel composite metric, SecurePass@K, to assess both functional correctness and security simultaneously. We find that while Retrieval-Augmented Generation (RAG) techniques can improve functional correctness, they provide negligible benefits to security. Furthermore, explicitly prompting models with general security guidelines often leads to compilation failures, harming functional correctness without reliably preventing vulnerabilities. Our work highlights the gap between functional and secure code generation in current LLMs.

</details>


### [10] [AlienLM: Alienization of Language for API-Boundary Privacy in Black-Box LLMs](https://arxiv.org/abs/2601.22710)
*Jaehee Kim,Pilsung Kang*

Main category: cs.CR

TL;DR: AlienLM是一个可部署的API隐私保护层，通过词汇级双射将文本转换为"外星语言"，在客户端无损恢复，仅使用标准微调API让目标模型直接在"外星化"输入上操作，在保持81%以上明文性能的同时提供强隐私保护。


<details>
  <summary>Details</summary>
Motivation: 现代LLM越来越多地通过黑盒API访问，用户需要将敏感提示、输出和微调数据传输给外部提供商，这在API边界处造成了严重的隐私风险。

Method: 引入AlienLM，通过词汇级双射将文本翻译成"外星语言"，客户端可无损恢复；使用标准微调API进行Alien Adaptation Training (AAT)，使目标模型直接在"外星化"输入上操作。

Result: 在四个LLM主干和七个基准测试中，AlienLM平均保持超过81%的明文性能，显著优于随机双射和字符级基线；在对手拥有模型权重、语料统计和学习型逆翻译的情况下，恢复攻击仅能重建少于0.22%的"外星化"标记。

Conclusion: AlienLM为API-only访问下的隐私保护LLM部署提供了实用路径，在保持任务性能的同时显著减少了明文暴露风险。

Abstract: Modern LLMs are increasingly accessed via black-box APIs, requiring users to transmit sensitive prompts, outputs, and fine-tuning data to external providers, creating a critical privacy risk at the API boundary. We introduce AlienLM, a deployable API-only privacy layer that protects text by translating it into an Alien Language via a vocabulary-scale bijection, enabling lossless recovery on the client side. Using only standard fine-tuning APIs, Alien Adaptation Training (AAT) adapts target models to operate directly on alienized inputs. Across four LLM backbones and seven benchmarks, AlienLM retains over 81\% of plaintext-oracle performance on average, substantially outperforming random-bijection and character-level baselines. Under adversaries with access to model weights, corpus statistics, and learning-based inverse translation, recovery attacks reconstruct fewer than 0.22\% of alienized tokens. Our results demonstrate a practical pathway for privacy-preserving LLM deployment under API-only access, substantially reducing plaintext exposure while maintaining task performance.

</details>


### [11] [Rust and Go directed fuzzing with LibAFL-DiFuzz](https://arxiv.org/abs/2601.22772)
*Timofey Mezhuev,Darya Parygina,Daniil Kuts*

Main category: cs.CR

TL;DR: 该研究提出了一种针对Rust和Go语言的定向灰盒模糊测试新方法，通过编译器定制和高级预处理技术，在TTE（暴露时间）实验中优于现有模糊测试工具。


<details>
  <summary>Details</summary>
Motivation: 随着Rust和Go语言的流行，需要为这些语言提供精确高效的测试解决方案。传统模糊测试工具主要针对C/C++，而定向模糊测试在验证静态分析报告或重现崩溃等特定任务中更有效，但缺乏对Rust和Go的支持。

Method: 提出针对Rust和Go应用的定向灰盒模糊测试新方法，包括高级预处理技术、rustc编译器定制、精细的图构建和插桩方法，基于LibAFL-DiFuzz后端实现。

Result: Rust-LibAFL-DiFuzz在TTE实验中表现最佳，优于afl.rs、cargo-fuzz等工具；Go-LibAFL-DiFuzz在多数情况下平均结果最好，有两个案例存在数量级差异的优势。

Conclusion: 该方法扩展了定向模糊测试在C/C++以外的应用范围，证明了针对Rust和Go的定向模糊测试方法具有更好的效率和准确性。

Abstract: In modern SSDLC, program analysis and automated testing are essential for minimizing vulnerabilities before software release, with fuzzing being a fast and widely used dynamic testing method. However, traditional coverage-guided fuzzing may be less effective in specific tasks like verifying static analysis reports or reproducing crashes, while directed fuzzing, focusing on targeted program locations using proximity metrics, proves to be more effective. Some of the earliest directed fuzzers are, for example, AFLGo and BEACON, which use different proximity metric approaches. Although most automated testing tools focus on C/C++ code, the growing popularity of Rust and Go causes the need for precise and efficient testing solutions for these languages. This work expands the applicability of directed fuzzing beyond traditional analysis of C/C++ software. We present a novel approach to directed greybox fuzzing tailored specifically for Rust and Go applications. We introduce advanced preprocessing techniques, rustc compiler customizations, and elaborate graph construction and instrumentation methods to enable effective targeting of specific program locations. Our implemented fuzzing tools, based on LibAFL-DiFuzz backend, demonstrate competitive advantages compared to popular existing fuzzers like afl.rs, cargo-fuzz, and go-fuzz. According to TTE (Time to Exposure) experiments, Rust-LibAFL-DiFuzz outperforms other tools by the best TTE result. Some stability issues can be explained by different mutation approaches. Go-LibAFL-DiFuzz outperforms its opponent by the best and, in the majority of cases, by average result, having two cases with orders of magnitude difference. These results prove better efficiency and accuracy of our approach.

</details>


### [12] [Trojan-Resilient NTT: Protecting Against Control Flow and Timing Faults on Reconfigurable Platforms](https://arxiv.org/abs/2601.22804)
*Rourab Paul,Krishnendu Guha,Amlan Chakrabarti*

Main category: cs.CR

TL;DR: 提出一种安全的NTT架构，能够检测非常规延迟、控制流中断和软分析侧信道攻击，并提供自适应故障校正方法，用于后量子密码硬件安全防护。


<details>
  <summary>Details</summary>
Motivation: 数论变换(NTT)是后量子密码算法中的核心组件，但侧信道攻击和硬件木马可能破坏电路控制流并引入非常规延迟。硬件木马对控制信号的攻击成本低且影响大，而软分析侧信道攻击(SASCA)可能利用插入的硬件木马进行攻击。

Method: 设计了一个安全的NTT架构，包含故障检测和校正模块，能够检测非常规延迟、控制流中断和SASCA攻击，并提供自适应故障校正机制来缓解这些威胁。

Result: 在Artix-7 FPGA上对不同Kyber变体进行广泛仿真和实现，结果显示故障检测和校正模块能够高效检测和校正由硬件木马引起的故障，成功率很高，同时只引入适度的面积和时间开销。

Conclusion: 提出的安全NTT架构能够有效防护后量子密码硬件中的侧信道攻击和硬件木马威胁，在保持高性能的同时提供强大的安全保护。

Abstract: Number Theoretic Transform (NTT) is the most essential component for polynomial multiplications used in lattice-based Post-Quantum Cryptography (PQC) algorithms such as Kyber, Dilithium, NTRU etc. However, side-channel attacks (SCA) and hardware vulnerabilities in the form of hardware Trojans may alter control signals to disrupt the circuit's control flow and introduce unconventional delays in the critical hardware of PQC. Hardware Trojans, especially on control signals, are more low cost and impactful than data signals because a single corrupted control signal can disrupt or bypass entire computation sequences, whereas data faults usually cause only localized errors. On the other hand, adversaries can perform Soft Analytical Side Channel Attacks (SASCA) on the design using the inserted hardware Trojan. In this paper, we present a secure NTT architecture capable of detecting unconventional delays, control-flow disruptions, and SASCA, while providing an adaptive fault-correction methodology for their mitigation. Extensive simulations and implementations of our Secure NTT on Artix-7 FPGA with different Kyber variants show that our fault detection and correction modules can efficiently detect and correct faults whether caused unintentionally or intentionally by hardware Trojans with a high success rate, while introducing only modest area and time overheads.

</details>


### [13] [Hide and Seek in Embedding Space: Geometry-based Steganography and Detection in Large Language Models](https://arxiv.org/abs/2601.22818)
*Charles Westphal,Keivan Navaie,Fernando E. Rosas*

Main category: cs.CR

TL;DR: 该论文研究LLM微调中的隐写术攻击，提出低可恢复性隐写方案，并探讨基于机制可解释性的检测方法


<details>
  <summary>Details</summary>
Motivation: 现有研究表明微调后的LLM可以通过隐写通道将提示秘密编码到输出中，但先前工作依赖于可轻易恢复的编码方式。需要研究更隐蔽的隐写方案及其检测方法。

Method: 1. 形式化定义有效载荷可恢复性（通过分类器准确率衡量）；2. 提出低可恢复性隐写方案，用嵌入空间导出的映射替换任意映射；3. 使用线性探针分析后期层激活，检测秘密存在。

Result: 在Llama-8B和Ministral-8B上，精确秘密恢复率分别从17%提升到30%（+78%）和24%提升到43%（+80%）；在Llama-70B上从9%提升到19%（+123%），同时降低了有效载荷可恢复性。线性探针在微调模型中的检测准确率比基础模型高33%。

Conclusion: 恶意微调会在模型中留下可操作的内在签名，基于机制可解释性的方法可以有效检测低可恢复性隐写攻击，为防御此类攻击提供了新方向。

Abstract: Fine-tuned LLMs can covertly encode prompt secrets into outputs via steganographic channels. Prior work demonstrated this threat but relied on trivially recoverable encodings. We formalize payload recoverability via classifier accuracy and show previous schemes achieve 100\% recoverability. In response, we introduce low-recoverability steganography, replacing arbitrary mappings with embedding-space-derived ones. For Llama-8B (LoRA) and Ministral-8B (LoRA) trained on TrojanStego prompts, exact secret recovery rises from 17$\rightarrow$30\% (+78\%) and 24$\rightarrow$43\% (+80\%) respectively, while on Llama-70B (LoRA) trained on Wiki prompts, it climbs from 9$\rightarrow$19\% (+123\%), all while reducing payload recoverability. We then discuss detection. We argue that detecting fine-tuning-based steganographic attacks requires approaches beyond traditional steganalysis. Standard approaches measure distributional shift, which is an expected side-effect of fine-tuning. Instead, we propose a mechanistic interpretability approach: linear probes trained on later-layer activations detect the secret with up to 33\% higher accuracy in fine-tuned models compared to base models, even for low-recoverability schemes. This suggests that malicious fine-tuning leaves actionable internal signatures amenable to interpretability-based defenses.

</details>


### [14] [Evaluating Large Language Models for Security Bug Report Prediction](https://arxiv.org/abs/2601.22921)
*Farnaz Soltaniani,Shoaib Razzaq,Mohammad Ghafari*

Main category: cs.CR

TL;DR: 该研究评估了基于提示工程和微调两种方法使用大语言模型预测安全漏洞报告的效果，发现两者存在明显的权衡关系：提示方法召回率高但精度低，微调方法精度高但召回率低。


<details>
  <summary>Details</summary>
Motivation: 安全漏洞报告的早期检测对于及时缓解漏洞至关重要，需要探索大语言模型在SBR预测中的应用效果。

Method: 采用基于提示的工程方法和微调方法，使用大语言模型预测安全漏洞报告，并在多个数据集上进行评估。

Result: 提示方法平均G-measure为77%，召回率74%，但精度仅22%；微调方法G-measure为51%，精度75%，召回率36%。微调模型推理速度比专有模型快50倍。

Conclusion: 两种方法各有优劣，需要进一步研究以充分利用大语言模型进行SBR预测，微调模型在速度和精度上有优势，提示方法在召回率上表现更好。

Abstract: Early detection of security bug reports (SBRs) is critical for timely vulnerability mitigation. We present an evaluation of prompt-based engineering and fine-tuning approaches for predicting SBRs using Large Language Models (LLMs). Our findings reveal a distinct trade-off between the two approaches. Prompted proprietary models demonstrate the highest sensitivity to SBRs, achieving a G-measure of 77% and a recall of 74% on average across all the datasets, albeit at the cost of a higher false-positive rate, resulting in an average precision of only 22%. Fine-tuned models, by contrast, exhibit the opposite behavior, attaining a lower overall G-measure of 51% but substantially higher precision of 75% at the cost of reduced recall of 36%. Though a one-time investment in building fine-tuned models is necessary, the inference on the largest dataset is up to 50 times faster than that of proprietary models. These findings suggest that further investigations to harness the power of LLMs for SBR prediction are necessary.

</details>


### [15] [Protecting Private Code in IDE Autocomplete using Differential Privacy](https://arxiv.org/abs/2601.22935)
*Evgeny Grigorenko,David Stanojević,David Ilić,Egor Bogomolov,Kostadin Cvejoski*

Main category: cs.CR

TL;DR: 该研究探讨了在Kotlin代码补全LLM训练中使用差分隐私(DP)作为防御机制，证明DP能有效抵御成员推理攻击，同时保持模型性能


<details>
  <summary>Details</summary>
Motivation: 现代IDE使用LLM进行代码补全时，训练用户代码存在隐私风险，模型可能成为新的数据漏洞，恶意攻击者可重构敏感训练数据或推断特定代码片段是否用于训练

Method: 使用差分隐私(DP)训练Kotlin代码补全的LLM，对Mellum模型进行DP微调，并全面评估其隐私性和实用性

Result: DP能有效防御成员推理攻击(MIAs)，将攻击成功率降至接近随机猜测水平(AUC从0.901降至0.606)，且隐私保护对模型性能影响很小，DP训练模型即使在100倍少数据下也能达到与非私有模型相当的实用性

Conclusion: 差分隐私是构建私密且可信赖的AI驱动IDE功能的实用有效解决方案

Abstract: Modern Integrated Development Environments (IDEs) increasingly leverage Large Language Models (LLMs) to provide advanced features like code autocomplete. While powerful, training these models on user-written code introduces significant privacy risks, making the models themselves a new type of data vulnerability. Malicious actors can exploit this by launching attacks to reconstruct sensitive training data or infer whether a specific code snippet was used for training. This paper investigates the use of Differential Privacy (DP) as a robust defense mechanism for training an LLM for Kotlin code completion. We fine-tune a \texttt{Mellum} model using DP and conduct a comprehensive evaluation of its privacy and utility. Our results demonstrate that DP provides a strong defense against Membership Inference Attacks (MIAs), reducing the attack's success rate close to a random guess (AUC from 0.901 to 0.606). Furthermore, we show that this privacy guarantee comes at a minimal cost to model performance, with the DP-trained model achieving utility scores comparable to its non-private counterpart, even when trained on 100x less data. Our findings suggest that DP is a practical and effective solution for building private and trustworthy AI-powered IDE features.

</details>


### [16] [A Real-Time Privacy-Preserving Behavior Recognition System via Edge-Cloud Collaboration](https://arxiv.org/abs/2601.22938)
*Huan Song,Shuyu Tian,Junyi Hao,Cheng Yuan,Zhenyu Jia,Jiawei Shao,Xuelong Li*

Main category: cs.CR

TL;DR: 该研究提出了一种基于AI Flow理论框架和边云协同架构的新型隐私保护感知技术，通过源端脱敏和不可逆特征映射，实现从视频监控到去身份化行为感知的突破


<details>
  <summary>Details</summary>
Motivation: 智能感知扩展到高隐私环境（如卫生间、更衣室）时面临隐私-安全悖论：传统RGB监控存在视觉记录和存储的隐私担忧，现有隐私保护方法要么损害语义理解能力，要么无法保证数学不可逆性对抗重建攻击

Method: 基于AI Flow理论框架和边云协同架构，集成源端脱敏与不可逆特征映射。边缘设备利用信息瓶颈理论进行毫秒级处理，通过非线性映射和随机噪声注入将原始图像转换为抽象特征向量，构建单向信息流；云端平台使用多模态家族模型仅基于这些抽象向量进行联合推理以检测异常行为

Result: 该方法在架构层面从根本上切断了隐私泄露路径，实现了从视频监控到去身份化行为感知的突破，为高敏感性公共空间的风险管理提供了稳健解决方案

Conclusion: 该技术通过不可逆特征映射和单向信息流设计，在保护隐私的同时保持语义理解能力，为高隐私环境下的智能感知提供了有效的隐私-安全平衡方案

Abstract: As intelligent sensing expands into high-privacy environments such as restrooms and changing rooms, the field faces a critical privacy-security paradox. Traditional RGB surveillance raises significant concerns regarding visual recording and storage, while existing privacy-preserving methods-ranging from physical desensitization to traditional cryptographic or obfuscation techniques-often compromise semantic understanding capabilities or fail to guarantee mathematical irreversibility against reconstruction attacks. To address these challenges, this study presents a novel privacy-preserving perception technology based on the AI Flow theoretical framework and an edge-cloud collaborative architecture. The proposed methodology integrates source desensitization with irreversible feature mapping. Leveraging Information Bottleneck theory, the edge device performs millisecond-level processing to transform raw imagery into abstract feature vectors via non-linear mapping and stochastic noise injection. This process constructs a unidirectional information flow that strips identity-sensitive attributes, rendering the reconstruction of original images impossible. Subsequently, the cloud platform utilizes multimodal family models to perform joint inference solely on these abstract vectors to detect abnormal behaviors. This approach fundamentally severs the path to privacy leakage at the architectural level, achieving a breakthrough from video surveillance to de-identified behavior perception and offering a robust solution for risk management in high-sensitivity public spaces.

</details>


### [17] [From Data Leak to Secret Misses: The Impact of Data Leakage on Secret Detection Models](https://arxiv.org/abs/2601.22946)
*Farnaz Soltaniani,Mohammad Ghafari*

Main category: cs.CR

TL;DR: 研究发现机器学习安全检测模型在包含重复样本的数据集上存在数据泄露问题，导致性能评估虚高，不能反映真实世界效果。


<details>
  <summary>Details</summary>
Motivation: 机器学习模型越来越多地用于软件安全任务，这些模型通常在大型互联网数据集上训练和评估，这些数据集往往包含重复或高度相似的样本。当这些样本分布在训练集和测试集中时，可能导致数据泄露，使模型能够记忆模式而不是学习泛化。

Method: 研究调查了广泛使用的硬编码密钥基准数据集中的重复情况，分析了数据泄露如何影响AI密钥检测器的性能评估。

Result: 数据泄露会显著夸大基于AI的密钥检测器报告的性能，导致对其真实世界有效性的误导性评估。

Conclusion: 在机器学习安全任务中，需要仔细处理数据集中的重复样本，避免数据泄露导致的性能评估偏差，以确保模型评估反映真实世界效果。

Abstract: Machine learning models are increasingly used for software security tasks. These models are commonly trained and evaluated on large Internet-derived datasets, which often contain duplicated or highly similar samples. When such samples are split across training and test sets, data leakage may occur, allowing models to memorize patterns instead of learning to generalize. We investigate duplication in a widely used benchmark dataset of hard coded secrets and show how data leakage can substantially inflate the reported performance of AI-based secret detectors, resulting in a misleading picture of their real-world effectiveness.

</details>


### [18] [From Similarity to Vulnerability: Key Collision Attack on LLM Semantic Caching](https://arxiv.org/abs/2601.23088)
*Zhixiang Zhang,Zesen Liu,Yuchong Xie,Quanfeng Huang,Dongdong She*

Main category: cs.CR

TL;DR: 该论文将语义缓存键视为模糊哈希，揭示了性能（局部性）与安全（抗碰撞性）之间的根本冲突，提出了首个针对缓存碰撞完整性风险的系统研究，并开发了自动化攻击框架CacheAttack。


<details>
  <summary>Details</summary>
Motivation: 语义缓存已成为扩展LLM应用的关键技术，但现有研究主要关注侧信道和隐私风险，缺乏对缓存碰撞引发的完整性风险的系统研究。作者发现语义缓存键作为模糊哈希，其最大化缓存命中率所需的局部性与密码学雪崩效应所需的抗碰撞性存在根本冲突。

Method: 将语义缓存键概念化为模糊哈希，形式化分析性能与安全之间的权衡。开发CacheAttack自动化框架，用于发起黑盒碰撞攻击。在安全关键任务和智能体工作流中评估攻击效果，包括LLM响应劫持和诱导恶意行为，并研究不同嵌入模型间的可迁移性。

Result: CacheAttack在LLM响应劫持中达到86%的命中率，能够诱导LLM智能体产生恶意行为，且在不同嵌入模型间保持强可迁移性。金融智能体案例研究进一步展示了这些漏洞的实际影响。

Conclusion: 语义缓存天然易受密钥碰撞攻击，揭示了性能与安全之间的根本权衡。这是首个系统研究缓存碰撞完整性风险的工作，提出的CacheAttack框架展示了实际威胁，最后讨论了缓解策略。

Abstract: Semantic caching has emerged as a pivotal technique for scaling LLM applications, widely adopted by major providers including AWS and Microsoft. By utilizing semantic embedding vectors as cache keys, this mechanism effectively minimizes latency and redundant computation for semantically similar queries. In this work, we conceptualize semantic cache keys as a form of fuzzy hashes. We demonstrate that the locality required to maximize cache hit rates fundamentally conflicts with the cryptographic avalanche effect necessary for collision resistance. Our conceptual analysis formalizes this inherent trade-off between performance (locality) and security (collision resilience), revealing that semantic caching is naturally vulnerable to key collision attacks.
  While prior research has focused on side-channel and privacy risks, we present the first systematic study of integrity risks arising from cache collisions. We introduce CacheAttack, an automated framework for launching black-box collision attacks. We evaluate CacheAttack in security-critical tasks and agentic workflows. It achieves a hit rate of 86\% in LLM response hijacking and can induce malicious behaviors in LLM agent, while preserving strong transferability across different embedding models. A case study on a financial agent further illustrates the real-world impact of these vulnerabilities. Finally, we discuss mitigation strategies.

</details>


### [19] [WiFiPenTester: Advancing Wireless Ethical Hacking with Governed GenAI](https://arxiv.org/abs/2601.23092)
*Haitham S. Al-Sinani,Chris J. Mitchell*

Main category: cs.CR

TL;DR: WiFiPenTester：一个实验性的、受管控的、可复现的GenAI无线伦理黑客系统，通过集成大语言模型改进无线安全评估的目标选择准确性和效率，同时保持人工监督和伦理保障。


<details>
  <summary>Details</summary>
Motivation: 传统无线伦理黑客依赖人工手动解释侦察结果并执行复杂命令，这一过程劳动密集、难以扩展、易受主观判断和人为错误影响。需要解决这些限制，实现更高效、可扩展的无线安全评估。

Method: 提出WiFiPenTester系统，将大语言模型集成到无线安全评估的侦察和决策支持阶段，实现智能目标排名、攻击可行性评估和策略推荐，同时保持严格的人工监督控制和预算感知执行。包括系统架构、威胁模型、治理机制和提示工程方法。

Result: 在多个无线环境中的实证实验表明，GenAI辅助提高了目标选择准确性和整体评估效率，同时保持了可审计性和伦理保障。

Conclusion: WiFiPenTester是向实用、安全、可扩展的GenAI辅助无线渗透测试迈出的有意义一步，同时强调了在伦理黑客中部署GenAI时需要有限自治、人工监督和严格治理机制的必要性。

Abstract: Wireless ethical hacking relies heavily on skilled practitioners manually interpreting reconnaissance results and executing complex, time-sensitive sequences of commands to identify vulnerable targets, capture authentication handshakes, and assess password resilience; a process that is inherently labour-intensive, difficult to scale, and prone to subjective judgement and human error. To help address these limitations, we propose WiFiPenTester, an experimental, governed, and reproducible system for GenAI-enabled wireless ethical hacking. The system integrates large language models into the reconnaissance and decision-support phases of wireless security assessment, enabling intelligent target ranking, attack feasibility estimation, and strategy recommendation, while preserving strict human-in-the-loop control and budget-aware execution. We describe the system architecture, threat model, governance mechanisms, and prompt-engineering methodology, and empirical experiments conducted across multiple wireless environments. The results demonstrate that GenAI assistance improves target selection accuracy and overall assessment efficiency, while maintaining auditability and ethical safeguards. This indicates that WiFiPenTester is a meaningful step toward practical, safe, and scalable GenAI-assisted wireless penetration testing, while reinforcing the necessity of bounded autonomy, human oversight, and rigorous governance mechanisms when deploying GenAI in ethical hacking.

</details>


### [20] [Secure Tool Manifest and Digital Signing Solution for Verifiable MCP and LLM Pipelines](https://arxiv.org/abs/2601.23132)
*Saeid Jamshidi,Kawser Wazed Nafi,Arghavan Moradi Dakhel,Foutse Khomh,Amin Nikanjam,Mohammad Adnan Hamdaqa*

Main category: cs.CR

TL;DR: 提出了一种安全工具清单和数字签名框架，作为模型上下文协议的扩展，通过加密签名和透明验证确保LLM执行管道的完整性和可验证性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在敏感领域应用日益广泛，但其执行管道容易受到操纵且行为不可验证。现有的控制机制（如MCP）缺乏可验证的执行强制和透明的模型动作验证。

Method: 提出安全工具清单和数字签名框架，包括：1）强制执行加密签名的清单；2）集成透明验证日志；3）将模型内部执行元数据与用户可见组件隔离，确保可验证的执行完整性。

Result: 评估显示：1）框架扩展性接近线性（R平方=0.998）；2）几乎完美接受有效执行，同时一致拒绝无效执行；3）在执行管道中保持平衡的模型利用率。

Conclusion: 该框架解决了LLM在敏感领域应用中的安全漏洞，提供了可验证的执行完整性和透明验证机制，显著增强了模型执行管道的安全性和可信度。

Abstract: Large Language Models (LLMs) are increasingly adopted in sensitive domains such as healthcare and financial institutions' data analytics; however, their execution pipelines remain vulnerable to manipulation and unverifiable behavior. Existing control mechanisms, such as the Model Context Protocol (MCP), define compliance policies for tool invocation but lack verifiable enforcement and transparent validation of model actions. To address this gap, we propose a novel Secure Tool Manifest and Digital Signing Framework, a structured and security-aware extension of Model Context Protocols. The framework enforces cryptographically signed manifests, integrates transparent verification logs, and isolates model-internal execution metadata from user-visible components to ensure verifiable execution integrity. Furthermore, the evaluation demonstrates that the framework scales nearly linearly (R-squared = 0.998), achieves near-perfect acceptance of valid executions while consistently rejecting invalid ones, and maintains balanced model utilization across execution pipelines.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [21] [JAF: Judge Agent Forest](https://arxiv.org/abs/2601.22269)
*Sahil Garg,Brad Cheezum,Sridhar Dutta,Vishal Agarwal*

Main category: cs.AI

TL;DR: JAF框架通过让评判智能体同时评估多个相关查询-响应对，实现从局部评估到整体学习的转变，利用信念传播和集成学习原理提升智能体性能


<details>
  <summary>Details</summary>
Motivation: 传统评判智能体单独评估每个查询-响应对的局限性，需要一种能够识别跨实例模式和一致性的整体评估框架，以提升智能体的自我改进能力

Method: 提出JAF框架，采用基于局部敏感哈希的算法，整合语义嵌入、LLM驱动的哈希谓词、类别标签监督和相关侧信息，实现高效、可解释、关系感知的范例选择

Result: 在云配置错误分类的复杂任务中验证了JAF框架的有效性，展示了其在大型云环境中的实际应用价值

Conclusion: JAF框架通过集体推理和集成评估，显著提升了评判智能体的评估质量和智能体的自我改进能力，为智能体AI框架提供了新的评估范式

Abstract: Judge agents are fundamental to agentic AI frameworks: they provide automated evaluation, and enable iterative self-refinement of reasoning processes. We introduce JAF: Judge Agent Forest, a framework in which the judge agent conducts joint inference across a cohort of query--response pairs generated by a primary agent, rather than evaluating each in isolation. This paradigm elevates the judge from a local evaluator to a holistic learner: by simultaneously assessing related responses, the judge discerns cross-instance patterns and inconsistencies, whose aggregate feedback enables the primary agent to improve by viewing its own outputs through the judge's collective perspective.
  Conceptually, JAF bridges belief propagation and ensemble-learning principles: overlapping in-context neighborhoods induce a knowledge-graph structure that facilitates propagation of critique, and repeated, randomized evaluations yield a robust ensemble of context-sensitive judgments. JAF can be instantiated entirely via ICL, with the judge prompted for each query using its associated primary-agent response plus a small, possibly noisy set of peer exemplars. While kNN in embedding space is a natural starting point for exemplars, this approach overlooks categorical structure, domain metadata, or nuanced distinctions accessible to modern LLMs.
  To overcome these limitations, we develop a flexible locality-sensitive hashing (LSH) algorithm that learns informative binary codes by integrating semantic embeddings, LLM-driven hash predicates, supervision from categorical labels, and relevant side information. These hash codes support efficient, interpretable, and relation-aware selection of diverse exemplars, and further optimize exploration of CoT reasoning paths. We validate JAF with an empirical study on the demanding task of cloud misconfigs triage in large-scale cloud environments.

</details>


### [22] [Semi-Autonomous Mathematics Discovery with Gemini: A Case Study on the Erdős Problems](https://arxiv.org/abs/2601.22401)
*Tony Feng,Trieu Trinh,Garrett Bingham,Jiwon Kang,Shengtong Zhang,Sang-hyun Kim,Kevin Barreto,Carl Schildkraut,Junehyuk Jung,Jaehyeon Seo,Carlo Pagano,Yuri Chervonyi,Dawsen Hwang,Kaiying Hou,Sergei Gukov,Cheng-Chiang Tsai,Hyunwoo Choi,Youngbeom Jin,Wei-Yuan Li,Hao-An Wu,Ruey-An Shiu,Yu-Sheng Shih,Quoc V. Le,Thang Luong*

Main category: cs.AI

TL;DR: 使用Gemini AI系统评估Bloom的Erdős问题数据库中700个标记为"开放"的猜想，通过混合方法（AI自然语言验证+人类专家评估）解决了13个问题，发现这些问题的"开放"状态更多是由于文献难以查找而非问题本身困难。


<details>
  <summary>Details</summary>
Motivation: 探索半自主数学发现的可能性，测试AI在系统评估数学猜想方面的能力，特别是针对Erdős问题数据库中标记为"开放"但可能已有解决方案的问题。

Method: 采用混合方法：1) AI驱动的自然语言验证来缩小搜索空间；2) 人类专家评估以判断正确性和新颖性。使用Gemini AI系统评估了700个标记为"开放"的Erdős问题猜想。

Result: 解决了13个标记为"开放"的问题：其中5个通过看似新颖的自主解决方案解决，8个通过识别现有文献中的先前解决方案解决。发现这些问题的"开放"状态更多是由于文献难以查找（obscurity）而非问题本身困难。

Conclusion: AI在数学猜想评估中面临文献识别困难，存在"潜意识剽窃"风险。Erdős问题的"开放"状态往往是由于文献难以查找而非数学难度。AI辅助方法在数学发现中具有潜力但需谨慎处理文献识别问题。

Abstract: We present a case study in semi-autonomous mathematics discovery, using Gemini to systematically evaluate 700 conjectures labeled 'Open' in Bloom's Erdős Problems database. We employ a hybrid methodology: AI-driven natural language verification to narrow the search space, followed by human expert evaluation to gauge correctness and novelty. We address 13 problems that were marked 'Open' in the database: 5 through seemingly novel autonomous solutions, and 8 through identification of previous solutions in the existing literature. Our findings suggest that the 'Open' status of the problems was through obscurity rather than difficulty. We also identify and discuss issues arising in applying AI to math conjectures at scale, highlighting the difficulty of literature identification and the risk of ''subconscious plagiarism'' by AI. We reflect on the takeaways from AI-assisted efforts on the Erdős Problems.

</details>


### [23] [AI-Enabled Waste Classification as a Data-Driven Decision Support Tool for Circular Economy and Urban Sustainability](https://arxiv.org/abs/2601.22418)
*Julius Sechang Mboli,Omolara Aderonke Ogungbemi*

Main category: cs.AI

TL;DR: 该研究评估了传统机器学习与深度学习模型在垃圾图像二元分类中的表现，发现DenseNet121在准确率和ROC-AUC上表现最佳，并探讨了PCA对传统模型的影响以及模型在实时决策支持系统中的应用。


<details>
  <summary>Details</summary>
Motivation: 智能城市中高效的垃圾分类对于实现循环经济和资源回收至关重要，需要评估不同机器学习方法在垃圾分类任务中的性能，为自动化垃圾分拣系统提供技术支持。

Method: 使用25,077张垃圾图像（80/20训练/测试分割，增强并调整至150x150像素），评估传统机器学习（随机森林、SVM、AdaBoost）和深度学习模型（自定义CNN、VGG16、ResNet50）以及三种迁移学习模型（DenseNet121、EfficientNetB0、InceptionV3），同时评估主成分分析（PCA）对传统模型的降维效果。

Result: DenseNet121取得了最高准确率（91%）和ROC-AUC（0.98），比最佳传统分类器高出20个百分点。PCA对传统方法改善有限，而迁移学习在有限数据条件下显著提升了性能。

Conclusion: 迁移学习模型特别是DenseNet121在垃圾分类任务中表现优异，这些模型可以集成到实时数据驱动的决策支持系统中，实现自动化垃圾分拣，有望减少填埋场使用和生命周期环境影响。

Abstract: Efficient waste sorting is crucial for enabling circular-economy practices and resource recovery in smart cities. This paper evaluates both traditional machine-learning (Random Forest, SVM, AdaBoost) and deep-learning techniques including custom CNNs, VGG16, ResNet50, and three transfer-learning models (DenseNet121, EfficientNetB0, InceptionV3) for binary classification of 25 077 waste images (80/20 train/test split, augmented and resized to 150x150 px). The paper assesses the impact of Principal Component Analysis for dimensionality reduction on traditional models. DenseNet121 achieved the highest accuracy (91 %) and ROC-AUC (0.98), outperforming the best traditional classifier by 20 pp. Principal Component Analysis (PCA) showed negligible benefit for classical methods, whereas transfer learning substantially improved performance under limited-data conditions. Finally, we outline how these models integrate into a real-time Data-Driven Decision Support System for automated waste sorting, highlighting potential reductions in landfill use and lifecycle environmental impacts.)

</details>


### [24] [Anytime Safe PAC Efficient Reasoning](https://arxiv.org/abs/2601.22446)
*Chengyao Yu,Hao Zeng,Youxin Zhu,Jianguo Huang,Huajun Zeng,Bingyi Jing*

Main category: cs.AI

TL;DR: B-PAC推理：一种在线环境下安全高效的大模型推理方法，通过动态调整路由阈值控制性能损失，显著降低计算开销


<details>
  <summary>Details</summary>
Motivation: 大型推理模型虽然性能强大但计算成本高、延迟大。现有的选择性思考策略虽然能提高效率，但在在线环境中存在不可控错误，因为非思考模型的性能损失只能部分观测且数据非平稳

Method: 提出B-PAC推理方法，利用逆倾向评分估计器构建候选阈值的测试超鞅，基于累积统计证据动态调整路由阈值，实现任意时间有效的性能损失控制

Result: B-PAC推理显著降低计算开销，将思考模型使用率减少高达81.01%，同时将性能损失控制在用户指定水平以下

Conclusion: B-PAC推理为在线环境下的安全高效推理提供了原则性方法，实现了任意时间有效的性能损失控制和效率提升

Abstract: Large Reasoning Models (LRMs) have demonstrated remarkable performance on complex tasks but suffer from high computational costs and latency. While selective thinking strategies improve efficiency by routing easy queries to non-thinking models, existing approaches often incur uncontrollable errors, especially in online settings where the performance loss of a non-thinking model is only partially observed and data are non-stationary. To address this, we propose Betting Probably Approximately Correct (B-PAC) reasoning, a principled method that enables anytime safe and efficient online reasoning under partial feedback. Specifically, we utilize inverse propensity scoring estimators to construct test supermartingales for candidate thresholds, and then dynamically adjust the routing threshold based on the accumulated statistical evidence of safety. Theoretically, we establish the anytime-valid performance loss control and the efficiency of B-PAC reasoning. Extensive experiments demonstrate that B-PAC reasoning significantly reduces computational overhead, decreasing thinking model usage by up to 81.01\%, while controlling the performance loss below the user-specified level.

</details>


### [25] [Controllable Information Production](https://arxiv.org/abs/2601.22449)
*Tristan Shah,Stas Tiomkin*

Main category: cs.AI

TL;DR: 本文提出了一种新的内在动机原则——可控信息生产(CIP)，它避免了外部效用和设计者指定的变量，通过开环与闭环Kolmogorov-Sinai熵的差值来同时奖励对混沌的追求和调节。


<details>
  <summary>Details</summary>
Motivation: 现有基于信息论的内在动机方法主要依赖于信息传输，这明确取决于设计者对参与传输的随机变量的选择。本文旨在避免对外部效用和设计者指定变量的依赖，提出一种更通用的内在动机原则。

Method: 从最优控制理论推导出CIP目标，将其表示为开环与闭环Kolmogorov-Sinai熵之间的差值。这种方法同时奖励对混沌的追求和调节，建立了外在行为与内在行为之间的联系。

Result: 建立了CIP的关键理论特性，并在标准内在动机基准测试中证明了其有效性。

Conclusion: CIP是一种新颖的内在动机原则，它避免了传统方法的局限性，通过信息生产而非传输的视角，为智能行为生成提供了新的理论基础。

Abstract: Intrinsic Motivation (IM) is a paradigm for generating intelligent behavior without external utilities. The existing information-theoretic methods for IM are predominantly based on information transmission, which explicitly depends on the designer's choice of which random variables engage in transmission. In this work, we introduce a novel IM principle, Controllable Information Production (CIP), that avoids both external utilities and designer-specified variables. We derive the CIP objective from Optimal Control, showing a connection between extrinsic and intrinsic behaviors. CIP appears as the gap between open-loop and closed-loop Kolmogorov-Sinai entropies, which simultaneously rewards the pursuit and regulation of chaos. We establish key theoretical properties of CIP and demonstrate its effectiveness on standard IM benchmarks.

</details>


### [26] [Why Self-Rewarding Works: Theoretical Guarantees for Iterative Alignment of Language Models](https://arxiv.org/abs/2601.22513)
*Shi Fu,Yingjie Wang,Shengchao Hu,Peng Wang,Dacheng Tao*

Main category: cs.AI

TL;DR: 本文首次为自奖励语言模型（SRLMs）提供了严格的理论保证，揭示了其性能改进机制和初始化依赖性的衰减规律。


<details>
  <summary>Details</summary>
Motivation: 自奖励语言模型在无需外部反馈的情况下通过迭代改进取得了显著成功，但其核心机制缺乏理论解释，存在理论理解上的关键空白。

Method: 首先建立单步更新的下界分析初始模型质量的影响；然后推导完整迭代范式的有限样本误差界；最后在线性softmax模型类中实例化理论框架。

Result: 性能改进速率与样本量n的关系为$\widetilde{\mathcal{O}}\left(1/\sqrt{n}\right)$；对初始模型的依赖随迭代次数T呈指数衰减；为自奖励成功提供了形式化解释。

Conclusion: 自奖励语言模型能够通过指数衰减初始依赖性的方式克服不良初始化，引导动态向内部稳定性和一致性发展，这解释了其成功的原因。

Abstract: Self-Rewarding Language Models (SRLMs) achieve notable success in iteratively improving alignment without external feedback. Yet, despite their striking empirical progress, the core mechanisms driving their capabilities remain unelucidated, leaving a critical gap in theoretical understanding. This paper provides the first rigorous theoretical guarantees for SRLMs. We first establish a lower bound that characterizes the fundamental limits of a single update step, revealing a critical dependence on the quality of the initial model. We then derive finite-sample error bounds for the full iterative paradigm, showing that performance improves at a rate of $\widetilde{\mathcal{O}}\left(1/\sqrt{n}\right)$ with sample size $n$. Crucially, our analysis reveals that the dependence on the initial model decays exponentially with the number of iterations $T$. This provides a formal explanation for why self-rewarding succeeds: it robustly overcomes poor initialization by steering the dynamics toward internal stability and consistency. Finally, we instantiate our theoretical framework for the linear softmax model class, yielding tailored guarantees that connect our high-level insights to practical model architectures.

</details>


### [27] [Darwinian Memory: A Training-Free Self-Regulating Memory System for GUI Agent Evolution](https://arxiv.org/abs/2601.22528)
*Hongze Mi,Yibo Feng,WenJie Lu,Song Cao,Jinyuan Li,Yanming Li,Xuelin Zhang,Haotian Luo,Songyang Peng,He Cui,Tengfei Tian,Jun Fang,Hua Chai,Naiqiang Tan*

Main category: cs.AI

TL;DR: 本文提出Darwinian Memory System (DMS)，一种自演化的记忆系统，通过"适者生存"法则解决MLLM智能体在GUI自动化中长期跨应用任务中的上下文限制问题，显著提升任务成功率和执行稳定性。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型(MLLM)智能体在GUI自动化中面临两个主要挑战：1) 处理长跨度、跨应用任务时受限于有限的上下文窗口；2) 现有记忆系统难以适应动态GUI环境，存在高层意图与低层执行的粒度不匹配问题，以及静态积累过时经验导致的上下文污染和幻觉问题。

Method: 提出Darwinian Memory System (DMS)，一种自演化的记忆架构，将记忆构建为受"适者生存"法则支配的动态生态系统。DMS将复杂轨迹分解为独立可重用的单元以实现组合灵活性，并实现效用驱动的自然选择机制来跟踪生存价值，主动修剪次优路径并抑制高风险计划。

Result: 在真实世界多应用基准测试中，DMS在不增加训练成本或架构开销的情况下，显著提升通用MLLM的性能：平均成功率提升18.0%，执行稳定性提升33.9%，同时降低任务延迟。

Conclusion: DMS被证明是一种有效的自演化记忆系统，能够解决GUI任务中MLLM智能体的上下文限制和记忆适应性问题，通过进化压力驱动智能体推导出更优策略。

Abstract: Multimodal Large Language Model (MLLM) agents facilitate Graphical User Interface (GUI) automation but struggle with long-horizon, cross-application tasks due to limited context windows. While memory systems provide a viable solution, existing paradigms struggle to adapt to dynamic GUI environments, suffering from a granularity mismatch between high-level intent and low-level execution, and context pollution where the static accumulation of outdated experiences drives agents into hallucination. To address these bottlenecks, we propose the Darwinian Memory System (DMS), a self-evolving architecture that constructs memory as a dynamic ecosystem governed by the law of survival of the fittest. DMS decomposes complex trajectories into independent, reusable units for compositional flexibility, and implements Utility-driven Natural Selection to track survival value, actively pruning suboptimal paths and inhibiting high-risk plans. This evolutionary pressure compels the agent to derive superior strategies. Extensive experiments on real-world multi-app benchmarks validate that DMS boosts general-purpose MLLMs without training costs or architectural overhead, achieving average gains of 18.0% in success rate and 33.9% in execution stability, while reducing task latency, establishing it as an effective self-evolving memory system for GUI tasks.

</details>


### [28] [Decoding in Geometry: Alleviating Embedding-Space Crowding for Complex Reasoning](https://arxiv.org/abs/2601.22536)
*Yixin Yang,Qingxiu Dong,Zhifang Sui*

Main category: cs.AI

TL;DR: 论文提出CraEG方法，通过几何引导重加权缓解嵌入空间拥挤现象，提升LLM生成性能


<details>
  <summary>Details</summary>
Motivation: 现有基于温度和截断的采样方法仅操作token概率，忽略了嵌入空间中token之间的细粒度几何关系。研究发现嵌入空间拥挤现象（概率质量集中在几何相近的token上），且与数学问题解决中的推理成功存在统计关联。

Method: 提出CraEG方法，这是一种即插即用的采样方法，通过几何引导重加权来缓解嵌入空间拥挤现象。该方法无需训练、单次通过，且与标准采样策略兼容。

Result: 在多个模型和基准测试上的实验表明，CraEG提高了生成性能，在鲁棒性和多样性指标上都有提升。

Conclusion: 嵌入空间拥挤是影响LLM推理的重要现象，CraEG通过几何引导的采样方法有效缓解这一问题，提升了生成质量。

Abstract: Sampling-based decoding underlies complex reasoning in large language models (LLMs), where decoding strategies critically shape model behavior. Temperature- and truncation-based methods reshape the next-token distribution through global probability reweighting or thresholding to balance the quality-diversity tradeoff. However, they operate solely on token probabilities, ignoring fine-grained relationships among tokens in the embedding space. We uncover a novel phenomenon, embedding-space crowding, where the next-token distribution concentrates its probability mass on geometrically close tokens in the embedding space. We quantify crowding at multiple granularities and find a statistical association with reasoning success in mathematical problem solving. Motivated by this finding, we propose CraEG, a plug-and-play sampling method that mitigates crowding through geometry-guided reweighting. CraEG is training-free, single-pass, and compatible with standard sampling strategies. Experiments on multiple models and benchmarks demonstrate improved generation performance, with gains in robustness and diversity metrics.

</details>


### [29] [WED-Net: A Weather-Effect Disentanglement Network with Causal Augmentation for Urban Flow Prediction](https://arxiv.org/abs/2601.22586)
*Qian Hong,Siyuan Chang,Xiao Zhou*

Main category: cs.AI

TL;DR: WED-Net是一个双分支Transformer架构，通过自注意力和交叉注意力分离内在交通模式和天气诱导的交通模式，使用记忆库和自适应门控融合，并引入判别器和因果数据增强策略，以提升极端天气条件下的城市时空预测性能。


<details>
  <summary>Details</summary>
Motivation: 现有数据驱动方法在极端天气条件下的城市时空预测存在挑战：1）事件罕见且动态变化；2）现有方法使用粗粒度天气描述符；3）缺乏捕捉细粒度时空效应的专用机制；4）现有因果方法通常忽略时间动态或依赖固定混杂因素分层。

Method: 提出WED-Net（Weather-Effect Disentanglement Network）：1）双分支Transformer架构，通过自注意力和交叉注意力分离内在和天气诱导的交通模式；2）使用记忆库和自适应门控融合；3）引入判别器明确区分天气条件；4）设计因果数据增强策略，扰动非因果部分同时保留因果结构。

Result: 在三个城市的出租车流量数据集上进行实验，WED-Net在极端天气条件下表现出稳健的性能，展示了其在支持更安全的出行、灾害准备和城市韧性方面的潜力。

Conclusion: WED-Net通过解耦天气效应和内在交通模式，结合因果数据增强，有效提升了极端天气条件下的城市时空预测能力，为实际应用中的安全出行、灾害准备和城市韧性提供了技术支持。

Abstract: Urban spatio-temporal prediction under extreme conditions (e.g., heavy rain) is challenging due to event rarity and dynamics. Existing data-driven approaches that incorporate weather as auxiliary input often rely on coarse-grained descriptors and lack dedicated mechanisms to capture fine-grained spatio-temporal effects. Although recent methods adopt causal techniques to improve out-of-distribution generalization, they typically overlook temporal dynamics or depend on fixed confounder stratification. To address these limitations, we propose WED-Net (Weather-Effect Disentanglement Network), a dual-branch Transformer architecture that separates intrinsic and weather-induced traffic patterns via self- and cross-attention, enhanced with memory banks and fused through adaptive gating. To further promote disentanglement, we introduce a discriminator that explicitly distinguishes weather conditions. Additionally, we design a causal data augmentation strategy that perturbs non-causal parts while preserving causal structures, enabling improved generalization under rare scenarios. Experiments on taxi-flow datasets from three cities demonstrate that WED-Net delivers robust performance under extreme weather conditions, highlighting its potential to support safer mobility, highlighting its potential to support safer mobility, disaster preparedness, and urban resilience in real-world settings. The code is publicly available at https://github.com/HQ-LV/WED-Net.

</details>


### [30] [Learn More with Less: Uncertainty Consistency Guided Query Selection for RLVR](https://arxiv.org/abs/2601.22595)
*Hao Yi,Yulan Hu,Xin Li,Sheng Ouyang,Lizhong Ding,Yong Liu*

Main category: cs.AI

TL;DR: 将主动学习引入RLVR框架，提出不确定性一致性度量方法，在仅使用30%数据的情况下达到全数据集性能，显著降低标注成本


<details>
  <summary>Details</summary>
Motivation: 现有RLVR算法需要大量查询标注，成本高昂。研究是否可以通过更少但信息量更大的查询获得相似或更好的性能，将主动学习引入RLVR框架

Method: 提出不确定性一致性度量方法：离线场景使用点双列相关系数(PBC)衡量主观不确定性与客观不确定性对齐程度；在线场景引入基于归一化优势和主观不确定性的新变体。理论证明在线变体与离线PBC严格负相关，支持更好的样本选择

Result: 实验表明该方法持续优于随机选择和经典主动学习基线，在仅使用30%数据训练的情况下达到全数据集性能，有效降低推理任务的RLVR成本

Conclusion: 通过引入主动学习和不确定性一致性度量，显著减少了RLVR算法所需的查询数量，在保持性能的同时大幅降低了标注成本，为数学推理任务提供了更高效的训练方案

Abstract: Large Language Models (LLMs) have recently improved mathematical reasoning through Reinforcement Learning with Verifiable Reward (RLVR). However, existing RLVR algorithms require large query budgets, making annotation costly. We investigate whether fewer but more informative queries can yield similar or superior performance, introducing active learning (AL) into RLVR. We identify that classic AL sampling strategies fail to outperform random selection in this setting, due to ignoring objective uncertainty when only selecting by subjective uncertainty. This work proposes an uncertainty consistency metric to evaluate how well subjective uncertainty aligns with objective uncertainty. In the offline setting, this alignment is measured using the Point-Biserial Correlation Coefficient (PBC). For online training, because of limited sampling and dynamically shifting output distributions, PBC estimation is difficult. Therefore, we introduce a new online variant, computed from normalized advantage and subjective uncertainty. Theoretically, we prove that the online variant is strictly negatively correlated with offline PBC and supports better sample selection. Experiments show our method consistently outperforms random and classic AL baselines, achieving full-dataset performance while training on only 30% of the data, effectively reducing the cost of RLVR for reasoning tasks.

</details>


### [31] [EntroCut: Entropy-Guided Adaptive Truncation for Efficient Chain-of-Thought Reasoning in Small-scale Large Reasoning Models](https://arxiv.org/abs/2601.22617)
*Hongxi Yan,Qingjie Liu,Yunhong Wang*

Main category: cs.AI

TL;DR: EntroCut：一种基于熵的动态截断方法，通过识别高置信度状态来提前终止推理过程，显著减少大型推理模型的token消耗


<details>
  <summary>Details</summary>
Motivation: 大型推理模型（LRMs）在复杂推理任务中表现出色，但依赖冗长的中间推理步骤导致计算成本高昂。研究发现模型在早期推理步骤的输出分布熵能够可靠地区分正确与错误推理，这启发了开发更高效推理方法的需求

Method: 提出EntroCut方法，这是一种无需训练的动态截断技术。通过监测模型输出分布的熵来识别高置信度状态，在这些状态下可以安全地提前终止推理过程，从而减少不必要的token消耗

Result: 在四个基准测试上的实验表明，EntroCut能够将token使用量减少高达40%，同时保持最小的准确率损失。该方法在效率-性能权衡方面优于现有的无需训练方法

Conclusion: 基于熵的动态截断为缓解大型推理模型的低效问题提供了一种实用方法，在保持推理质量的同时显著提高了计算效率，为实际应用中的资源优化提供了有效解决方案

Abstract: Large Reasoning Models (LRMs) excel at complex reasoning tasks through extended chain-of-thought generation, but their reliance on lengthy intermediate steps incurs substantial computational cost. We find that the entropy of the model's output distribution in early reasoning steps reliably distinguishes correct from incorrect reasoning. Motivated by this observation, we propose EntroCut, a training-free method that dynamically truncates reasoning by identifying high-confidence states where reasoning can be safely terminated. To comprehensively evaluate the trade-off between efficiency and accuracy, we introduce the Efficiency-Performance Ratio (EPR), a unified metric that quantifies relative token savings per unit accuracy loss. Experiments on four benchmarks show that EntroCut reduces token usage by up to 40\% with minimal accuracy sacrifice, achieving superior efficiency-performance trade-offs compared with existing training-free methods. These results demonstrate that entropy-guided dynamic truncation provides a practical approach to mitigate the inefficiency of LRMs.

</details>


### [32] [SYMPHONY: Synergistic Multi-agent Planning with Heterogeneous Language Model Assembly](https://arxiv.org/abs/2601.22623)
*Wei Zhu,Zhiwen Tang,Kun Yue*

Main category: cs.AI

TL;DR: SYMPHONY是一个新颖的多智能体规划框架，通过集成异构语言模型智能体池来增强蒙特卡洛树搜索的探索能力，相比单智能体框架能生成更多样化的搜索分支，提升规划性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于大语言模型的自主智能体主要采用单智能体框架进行蒙特卡洛树搜索规划，这种范式限制了探索能力，导致生成的搜索分支多样性不足，规划性能欠佳。

Method: 提出SYMPHONY框架，集成异构语言模型智能体池，利用不同智能体的多样化推理模式来增强rollout多样性，促进更有效的探索。

Result: 在多个基准任务上的实证结果表明，SYMPHONY即使使用可在消费级硬件上部署的开源LLMs也能实现强大性能；当使用基于云的API访问的LLMs增强时，性能进一步提升，超越了现有的最先进基线方法。

Conclusion: 异构多智能体协调在规划任务中具有显著效果，SYMPHONY框架通过集成多样化智能体增强了蒙特卡洛树搜索的探索能力，为复杂问题解决提供了更有效的自主智能体构建方法。

Abstract: Recent advancements have increasingly focused on leveraging large language models (LLMs) to construct autonomous agents for complex problem-solving tasks. However, existing approaches predominantly employ a single-agent framework to generate search branches and estimate rewards during Monte Carlo Tree Search (MCTS) planning. This single-agent paradigm inherently limits exploration capabilities, often resulting in insufficient diversity among generated branches and suboptimal planning performance. To overcome these limitations, we propose Synergistic Multi-agent Planning with Heterogeneous langauge model assembly (SYMPHONY), a novel multi-agent planning framework that integrates a pool of heterogeneous language model-based agents. By leveraging diverse reasoning patterns across agents, SYMPHONY enhances rollout diversity and facilitates more effective exploration. Empirical results across multiple benchmark tasks show that SYMPHONY achieves strong performance even when instantiated with open-source LLMs deployable on consumer-grade hardware. When enhanced with cloud-based LLMs accessible via API, SYMPHONY demonstrates further improvements, outperforming existing state-of-the-art baselines and underscoring the effectiveness of heterogeneous multi-agent coordination in planning tasks.

</details>


### [33] [Statistical Estimation of Adversarial Risk in Large Language Models under Best-of-N Sampling](https://arxiv.org/abs/2601.22636)
*Mingqian Feng,Xiaodong Liu,Weiwei Yang,Chenliang Xu,Christopher White,Jianfeng Gao*

Main category: cs.AI

TL;DR: 该论文提出了SABER方法，用于评估大语言模型在并行采样攻击下的安全风险，通过Beta分布建模样本级成功概率，仅需少量样本即可准确预测大规模攻击成功率。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型的安全评估通常基于单次或低预算的对抗性提示，这低估了实际风险。攻击者可以利用大规模并行采样反复探测模型直到产生有害响应，需要更准确的风险预测方法。

Method: 提出SABER方法，使用Beta分布（伯努利分布的共轭先验）建模样本级成功概率，推导出解析缩放定律，能够从小预算测量可靠地外推大规模攻击成功率。

Result: 仅使用n=100个样本，SABER方法预测ASR@1000的平均绝对误差为1.66，相比基线方法的12.04误差减少了86.2%。研究揭示了异质风险缩放特征，显示在标准评估下看似稳健的模型在并行对抗压力下可能经历快速非线性风险放大。

Conclusion: 该工作提供了一种低成本、可扩展的大语言模型安全评估方法，能够更真实地反映实际攻击场景下的风险。研究将发布代码和评估脚本以支持未来研究。

Abstract: Large Language Models (LLMs) are typically evaluated for safety under single-shot or low-budget adversarial prompting, which underestimates real-world risk. In practice, attackers can exploit large-scale parallel sampling to repeatedly probe a model until a harmful response is produced. While recent work shows that attack success increases with repeated sampling, principled methods for predicting large-scale adversarial risk remain limited. We propose a scaling-aware Best-of-N estimation of risk, SABER, for modeling jailbreak vulnerability under Best-of-N sampling. We model sample-level success probabilities using a Beta distribution, the conjugate prior of the Bernoulli distribution, and derive an analytic scaling law that enables reliable extrapolation of large-N attack success rates from small-budget measurements. Using only n=100 samples, our anchored estimator predicts ASR@1000 with a mean absolute error of 1.66, compared to 12.04 for the baseline, which is an 86.2% reduction in estimation error. Our results reveal heterogeneous risk scaling profiles and show that models appearing robust under standard evaluation can experience rapid nonlinear risk amplification under parallel adversarial pressure. This work provides a low-cost, scalable methodology for realistic LLM safety assessment. We will release our code and evaluation scripts upon publication to future research.

</details>


### [34] [Beyond Medical Chatbots: Meddollina and the Rise of Continuous Clinical Intelligence](https://arxiv.org/abs/2601.22645)
*Vaibhav Ram S. V. N. S,Swetanshu Agrawal,Samudra Banerjee,Abdul Muhsin*

Main category: cs.AI

TL;DR: 论文批评当前生成式医疗AI仅关注文本生成而非临床推理，提出临床情境智能(CCI)概念，并开发了Meddollina系统，通过行为优先评估显示其在不确定性下的校准和保守推理优于生成中心基线。


<details>
  <summary>Details</summary>
Motivation: 当前生成式医疗AI虽然看起来流畅且知识丰富，但临床推理不是文本生成，而是需要在模糊性、不完整证据和纵向情境下承担责任的决策过程。生成中心系统存在过早结论、不合理确定性、意图漂移和多步决策不稳定等问题，这些是"将医学视为下一个token预测"的结构性后果。

Method: 提出临床情境智能(CCI)作为现实世界临床使用所需的能力类别，定义为具有持续情境意识、意图保持、有界推理和证据不足时的原则性延迟。开发了Meddollina系统，这是一个治理优先的临床智能系统，在语言实现前约束推理，优先考虑临床适宜性而非生成完整性。采用行为优先评估机制，在16,412+个异质医疗查询上进行评估。

Result: Meddollina展现出独特的行为特征：校准的不确定性、在未明确情况下的保守推理、稳定的纵向约束遵守，以及相对于生成中心基线的减少推测性完成。这些结果表明可部署的医疗AI不会仅通过扩展规模出现。

Conclusion: 需要从扩展规模转向连续临床智能，其中进展应通过临床医生在不确定性下的对齐行为来衡量，而非基于流畅度的完成度。Meddollina作为连续智能层支持临床工作流程，同时保持临床医生的权威。

Abstract: Generative medical AI now appears fluent and knowledgeable enough to resemble clinical intelligence, encouraging the belief that scaling will make it safe. But clinical reasoning is not text generation. It is a responsibility-bound process under ambiguity, incomplete evidence, and longitudinal context. Even as benchmark scores rise, generation-centric systems still show behaviours incompatible with clinical deployment: premature closure, unjustified certainty, intent drift, and instability across multi-step decisions.
  We argue these are structural consequences of treating medicine as next-token prediction. We formalise Clinical Contextual Intelligence (CCI) as a distinct capability class required for real-world clinical use, defined by persistent context awareness, intent preservation, bounded inference, and principled deferral when evidence is insufficient.
  We introduce Meddollina, a governance-first clinical intelligence system designed to constrain inference before language realisation, prioritising clinical appropriateness over generative completeness. Meddollina acts as a continuous intelligence layer supporting clinical workflows while preserving clinician authority. We evaluate Meddollina using a behaviour-first regime across 16,412+ heterogeneous medical queries, benchmarking against general-purpose models, medical-tuned models, and retrieval-augmented systems.
  Meddollina exhibits a distinct behavioural profile: calibrated uncertainty, conservative reasoning under underspecification, stable longitudinal constraint adherence, and reduced speculative completion relative to generation-centric baselines. These results suggest deployable medical AI will not emerge from scaling alone, motivating a shift toward Continuous Clinical Intelligence, where progress is measured by clinician-aligned behaviour under uncertainty rather than fluency-driven completion.

</details>


### [35] [Test-Time Mixture of World Models for Embodied Agents in Dynamic Environments](https://arxiv.org/abs/2601.22647)
*Jinwoo Jang,Minjong Yoo,Sihyung Yoon,Honguk Woo*

Main category: cs.AI

TL;DR: TMoW框架通过测试时更新世界模型的路由函数，增强具身智能体在动态环境中的适应性，支持零样本适应和少样本扩展


<details>
  <summary>Details</summary>
Motivation: 现有基于语言模型的具身智能体在动态环境中适应性有限，需要构建准确灵活的世界模型来支持有效推理和决策

Method: 提出测试时世界模型混合(TMoW)框架：1)多粒度原型路由，基于对象到场景级相似性调整混合；2)测试时细化，在推理时对齐未见域特征与原型；3)蒸馏混合增强，从少量数据和现有原型高效构建新模型

Result: 在VirtualHome、ALFWorld和RLBench基准测试中表现优异，在零样本适应和少样本扩展场景中都展现出强大性能

Conclusion: TMoW通过测试时更新路由函数，使具身智能体能够有效重组现有模型并集成新模型，实现持续适应，从而在动态环境中有效运行

Abstract: Language model (LM)-based embodied agents are increasingly deployed in real-world settings. Yet, their adaptability remains limited in dynamic environments, where constructing accurate and flexible world models is crucial for effective reasoning and decision-making. To address this challenge, we extend the Mixture-of-Experts (MoE) paradigm to embodied agents. While conventional MoE architectures modularize knowledge into expert components with pre-trained routing, they remain rigid once deployed, making them less effective for adapting to unseen domains in dynamic environments. We therefore propose Test-time Mixture of World Models (TMoW), a framework that enhances adaptability to unseen and evolving domains. TMoW updates its routing function over world models at test time, unlike conventional MoE where the function remains fixed, enabling agents to recombine existing models and integrate new ones for continual adaptation. It achieves this through (i) multi-granular prototype-based routing, which adapts mixtures across object- to scene-level similarities, (ii) test-time refinement that aligns unseen domain features with prototypes during inference, and (iii) distilled mixture-based augmentation, which efficiently constructs new models from few-shot data and existing prototypes. We evaluate TMoW on VirtualHome, ALFWorld, and RLBench benchmarks, demonstrating strong performance in both zero-shot adaptation and few-shot expansion scenarios, and showing that it enables embodied agents to operate effectively in dynamic environments.

</details>


### [36] [UCPO: Uncertainty-Aware Policy Optimization](https://arxiv.org/abs/2601.22648)
*Xianzhou Zeng,Jing Huang,Chunmei Xie,Gongrui Nan,Siye Chen,Mengyu Lu,Weiqi Xiong,Qixuan Zhou,Junhao Zhang,Qiang Zhu,Yadong Li,Xingzhong Xu*

Main category: cs.AI

TL;DR: 本文提出UCPO框架解决LLM不确定性表达中的优势偏差问题，通过三元优势解耦和动态不确定性奖励调整，提升模型在知识边界外的可靠性和校准性。


<details>
  <summary>Details</summary>
Motivation: 现有RL范式（如GRPO）在不确定性表达中存在优势偏差问题，源于二元决策空间和静态不确定性奖励，导致模型过度保守或过度自信，限制了LLM在高风险应用中的可信度。

Method: 提出UnCertainty-Aware Policy Optimization (UCPO)框架：1）三元优势解耦：分离并独立归一化确定性和不确定性rollouts以消除优势偏差；2）动态不确定性奖励调整：根据模型演化和实例难度实时校准不确定性权重。

Result: 在数学推理和通用任务上的实验结果表明，UCPO有效解决了奖励不平衡问题，显著提高了模型在知识边界外的可靠性和校准性。

Conclusion: UCPO框架通过解决现有RL范式中的优势偏差问题，为构建具有内在不确定性表达能力的可信LLM提供了有效解决方案，缓解了幻觉问题对高风险应用的限制。

Abstract: The key to building trustworthy Large Language Models (LLMs) lies in endowing them with inherent uncertainty expression capabilities to mitigate the hallucinations that restrict their high-stakes applications. However, existing RL paradigms such as GRPO often suffer from Advantage Bias due to binary decision spaces and static uncertainty rewards, inducing either excessive conservatism or overconfidence. To tackle this challenge, this paper unveils the root causes of reward hacking and overconfidence in current RL paradigms incorporating uncertainty-based rewards, based on which we propose the UnCertainty-Aware Policy Optimization (UCPO) framework. UCPO employs Ternary Advantage Decoupling to separate and independently normalize deterministic and uncertain rollouts, thereby eliminating advantage bias. Furthermore, a Dynamic Uncertainty Reward Adjustment mechanism is introduced to calibrate uncertainty weights in real-time according to model evolution and instance difficulty. Experimental results in mathematical reasoning and general tasks demonstrate that UCPO effectively resolves the reward imbalance, significantly improving the reliability and calibration of the model beyond their knowledge boundaries.

</details>


### [37] [Task-Aware LLM Council with Adaptive Decision Pathways for Decision Support](https://arxiv.org/abs/2601.22662)
*Wei Zhu,Lixing Yu,Hao-Ren Yao,Zhiwen Tang,Kun Yue*

Main category: cs.AI

TL;DR: TALC是一个任务自适应的LLM决策框架，通过集成LLM委员会和蒙特卡洛树搜索，实现动态专家选择和高效多步规划，提升任务成功率。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常忽视不同LLM的专业化差异，将所有模型视为同等适用，限制了系统适应不同推理需求和任务复杂度的能力。

Method: 提出任务感知的LLM委员会框架，每个LLM配备结构化成功记忆档案，通过语义匹配将当前推理上下文与历史成功经验对齐。采用双信号机制融合模型评估和历史效用分数，基于节点内方差自适应加权，指导MCTS选择。

Result: 在WebShop、HumanEval和24点游戏上的实验表明，TALC相比强基线实现了更高的任务成功率和改进的搜索效率。

Conclusion: TALC验证了专业化感知路由和自适应规划的优势，为LLM决策系统提供了更有效的任务适应能力。

Abstract: Large language models (LLMs) have shown strong capabilities across diverse decision-making tasks. However, existing approaches often overlook the specialization differences among available models, treating all LLMs as uniformly applicable regardless of task characteristics. This limits their ability to adapt to varying reasoning demands and task complexities. In this work, we propose Task-Aware LLM Council (TALC), a task-adaptive decision framework that integrates a council of LLMs with Monte Carlo Tree Search (MCTS) to enable dynamic expert selection and efficient multi-step planning. Each LLM is equipped with a structured success memory profile derived from prior task trajectories, enabling semantic matching between current reasoning context and past successes. At each decision point, TALC routes control to the most contextually appropriate model and estimates node value using a dual-signal mechanism that fuses model-based evaluations with historical utility scores. These signals are adaptively weighted based on intra-node variance and used to guide MCTS selection, allowing the system to balance exploration depth with planning confidence. Experiments on WebShop, HumanEval, and the Game of 24 demonstrate that TALC achieves superior task success rates and improved search efficiency compared to strong baselines, validating the benefits of specialization-aware routing and adaptive planning.

</details>


### [38] [Real-Time Aligned Reward Model beyond Semantics](https://arxiv.org/abs/2601.22664)
*Zixuan Huang,Xin Xia,Yuxi Ren,Jianbin Zheng,Xuefeng Xiao,Hongyan Xie,Li Huaqiu,Songshi Liang,Zhongxiang Dai,Fuzhen Zhuang,Jianxin Li,Yikun Ban,Deqing Wang*

Main category: cs.AI

TL;DR: R2M是一个轻量级RLHF框架，通过利用策略模型的实时隐藏状态反馈来应对奖励过优化问题，实现奖励模型与策略分布变化的实时对齐。


<details>
  <summary>Details</summary>
Motivation: 现有RLHF方法容易受到奖励过优化的影响，即策略模型过度拟合奖励模型，利用虚假奖励模式而非真正捕捉人类意图。现有缓解方法主要依赖表面语义信息，无法有效解决由连续策略分布变化引起的奖励模型与策略模型之间的错位问题，导致奖励差异增大并加剧奖励过优化。

Method: 提出R2M（实时对齐奖励模型）框架，超越仅依赖预训练LLM语义表示的普通奖励模型。R2M利用策略模型在强化学习过程中的演化隐藏状态（即策略反馈），与策略的实时分布变化进行对齐。

Result: 该方法为通过实时利用策略模型反馈来改进奖励模型性能指出了一个有前景的新方向。

Conclusion: R2M通过引入策略反馈机制，能够更有效地解决RLHF中的奖励过优化问题，实现奖励模型与策略模型在强化学习过程中的实时对齐。

Abstract: Reinforcement Learning from Human Feedback (RLHF) is a pivotal technique for aligning large language models (LLMs) with human preferences, yet it is susceptible to reward overoptimization, in which policy models overfit to the reward model, exploit spurious reward patterns instead of faithfully capturing human intent. Prior mitigations primarily relies on surface semantic information and fails to efficiently address the misalignment between the reward model (RM) and the policy model caused by continuous policy distribution shifts. This inevitably leads to an increasing reward discrepancy, exacerbating reward overoptimization. To address these limitations, we introduce R2M (Real-Time Aligned Reward Model), a novel lightweight RLHF framework. R2M goes beyond vanilla reward models that solely depend on the semantic representations of a pretrained LLM. Instead, it leverages the evolving hidden states of the policy (namely policy feedback) to align with the real-time distribution shift of the policy during the RL process. This work points to a promising new direction for improving the performance of reward models through real-time utilization of feedback from policy models.

</details>


### [39] [AutoRefine: From Trajectories to Reusable Expertise for Continual LLM Agent Refinement](https://arxiv.org/abs/2601.22758)
*Libin Qiu,Zhirong Gao,Junfu Chen,Yuhang Ye,Weizhi Huang,Xiaobo Xue,Wenkai Qiu,Shuo Tang*

Main category: cs.AI

TL;DR: AutoRefine框架从智能体执行历史中提取和维护双重形式的经验模式，包括用于复杂子任务的专用子智能体和用于静态知识的技能模式，通过持续维护机制防止知识库退化，在多个任务上显著提升性能并减少步骤。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型智能体往往无法从经验中积累知识，将每个任务视为独立挑战。现有方法将经验提取为扁平化的文本知识，无法捕捉复杂子任务的程序逻辑，且缺乏维护机制导致知识库随着经验积累而退化。

Method: 提出AutoRefine框架，从智能体执行历史中提取和维护双重形式的经验模式：1) 对于程序性子任务，提取具有独立推理和记忆的专用子智能体；2) 对于静态知识，提取技能模式作为指导方针或代码片段。采用持续维护机制对模式进行评分、修剪和合并，防止知识库退化。

Result: 在ALFWorld、ScienceWorld和TravelPlanner三个基准测试中，AutoRefine分别达到98.4%、70.4%和27.1%的成功率，同时减少20-73%的步骤。在TravelPlanner上，自动提取的系统超过手动设计系统（27.1% vs 12.1%），展示了其捕捉程序协调的能力。

Conclusion: AutoRefine框架通过提取和维护双重形式的经验模式，有效解决了现有智能体无法从经验中学习复杂程序逻辑的问题，显著提升了任务性能并减少了执行步骤，证明了自动经验提取在捕捉程序协调方面的优势。

Abstract: Large language model agents often fail to accumulate knowledge from experience, treating each task as an independent challenge. Recent methods extract experience as flattened textual knowledge, which cannot capture procedural logic of complex subtasks. They also lack maintenance mechanisms, causing repository degradation as experience accumulates. We introduce AutoRefine, a framework that extracts and maintains dual-form Experience Patterns from agent execution histories. For procedural subtasks, we extract specialized subagents with independent reasoning and memory. For static knowledge, we extract skill patterns as guidelines or code snippets. A continuous maintenance mechanism scores, prunes, and merges patterns to prevent repository degradation. Evaluated on ALFWorld, ScienceWorld, and TravelPlanner, AutoRefine achieves 98.4%, 70.4%, and 27.1% respectively, with 20-73% step reductions. On TravelPlanner, automatic extraction exceeds manually designed systems (27.1% vs 12.1%), demonstrating its ability to capture procedural coordination.

</details>


### [40] [TSPO: Breaking the Double Homogenization Dilemma in Multi-turn Search Policy Optimization](https://arxiv.org/abs/2601.22776)
*Shichao Ma,Zhiyuan Ma,Ming Yang,Xiaofan Li,Xing Wu,Jintao Du,Yu Cheng,Weiqiang Wang,Qiliang Liu,Zhengyang Zhou,Yang Wang*

Main category: cs.AI

TL;DR: TSPO提出了一种新的强化学习框架，通过首次出现潜在奖励机制解决多轮工具集成推理中的"双重同质化困境"，显著提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 当前基于强化学习的搜索增强推理框架主要依赖稀疏的结果级奖励，导致"双重同质化困境"：1) 过程同质化，忽略了生成过程中的思考、推理和工具使用；2) 组内同质化，粗粒度的结果奖励导致组内优势估计效率低下。

Method: 提出Turn-level Stage-aware Policy Optimization (TSPO)，引入首次出现潜在奖励(FOLR)机制，将部分奖励分配给正确答案首次出现的步骤，保留过程级信号并增加组内奖励方差，无需外部奖励模型或额外标注。

Result: TSPO显著优于现有最先进基线，在Qwen2.5-3B和7B模型上分别实现了平均24%和13.6%的性能提升。

Conclusion: TSPO通过细粒度的过程级奖励分配有效解决了多轮工具集成推理中的双重同质化问题，为强化学习在搜索增强推理中的应用提供了新思路。

Abstract: Multi-turn tool-integrated reasoning enables Large Language Models (LLMs) to solve complex tasks through iterative information retrieval. However, current reinforcement learning (RL) frameworks for search-augmented reasoning predominantly rely on sparse outcome-level rewards, leading to a "Double Homogenization Dilemma." This manifests as (1) Process homogenization, where the thinking, reasoning, and tooling involved in generation are ignored. (2) Intra-group homogenization, coarse-grained outcome rewards often lead to inefficiencies in intra-group advantage estimation with methods like Group Relative Policy Optimization (GRPO) during sampling. To address this, we propose Turn-level Stage-aware Policy Optimization (TSPO). TSPO introduces the First-Occurrence Latent Reward (FOLR) mechanism, allocating partial rewards to the step where the ground-truth answer first appears, thereby preserving process-level signals and increasing reward variance within groups without requiring external reward models or any annotations. Extensive experiments demonstrate that TSPO significantly outperforms state-of-the-art baselines, achieving average performance gains of 24% and 13.6% on Qwen2.5-3B and 7B models, respectively.

</details>


### [41] [Toward IIT-Inspired Consciousness in LLMs: A Reward-Based Learning Framework](https://arxiv.org/abs/2601.22786)
*Hamid Reza Akbari,Mohammad Hossein Sameti,Amir M. Mansourian,Mohammad Hossein Rohban,Hossein Sameti*

Main category: cs.AI

TL;DR: 该研究提出了一种基于整合信息理论(IIT)的奖励函数，通过强化学习优化语言模型生成文本的因果性、连贯性和整合性，实现了在不损失准确性的情况下将输出长度减少高达31%的效果。


<details>
  <summary>Details</summary>
Motivation: 追求人工通用智能(AGI)是语言模型发展的核心目标，而类似意识的处理可能成为关键促进因素。虽然当前语言模型不具备意识，但它们表现出与意识某些方面类似的行为。本研究旨在将领先的意识理论——整合信息理论(IIT)通过基于奖励的学习范式应用于语言模型。

Method: 基于整合信息理论(IIT)的核心原则，设计了一种新颖的奖励函数，用于量化文本的因果性、连贯性和整合性——这些特征与意识处理相关。通过奖励驱动的学习范式优化语言模型，使其生成更符合这些特征的文本。

Result: 优化IIT启发的奖励函数能产生更简洁的文本生成。在领域外任务上，经过精细调优后，输出长度减少了高达31%，同时保持了与基础模型相当的准确性水平。研究还分析了该方法对模型置信度校准和测试时计算扩展性的影响。

Conclusion: 提出的框架具有显著的实际优势：概念简单、计算高效、无需外部数据或辅助模型，并且利用了通用的能力驱动信号而非任务特定的启发式方法。该方法为语言模型优化提供了一种新颖且有效的途径。

Abstract: The pursuit of Artificial General Intelligence (AGI) is a central goal in language model development, in which consciousness-like processing could serve as a key facilitator. While current language models are not conscious, they exhibit behaviors analogous to certain aspects of consciousness. This paper investigates the implementation of a leading theory of consciousness, Integrated Information Theory (IIT), within language models via a reward-based learning paradigm. IIT provides a formal, axiom-based mathematical framework for quantifying consciousness. Drawing inspiration from its core principles, we formulate a novel reward function that quantifies a text's causality, coherence and integration, characteristics associated with conscious processing. Empirically, it is found that optimizing for this IIT-inspired reward leads to more concise text generation. On out of domain tasks, careful tuning achieves up to a 31% reduction in output length while preserving accuracy levels comparable to the base model. In addition to primary task performance, the broader effects of this training methodology on the model's confidence calibration and test-time computational scaling is analyzed. The proposed framework offers significant practical advantages: it is conceptually simple, computationally efficient, requires no external data or auxiliary models, and leverages a general, capability-driven signal rather than task-specific heuristics. Code available at https://github.com/MH-Sameti/LLM_PostTraining.git

</details>


### [42] [Conditional Performance Guarantee for Large Reasoning Models](https://arxiv.org/abs/2601.22790)
*Jianguo Huang,Hao Zeng,Bingyi Jing,Hongxin Wei,Bo An*

Main category: cs.AI

TL;DR: 提出了G-PAC推理框架，通过输入空间分组实现组级PAC保证，相比传统边际PAC推理在异构场景下能严格提升效率。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型通过长链思维推理展现强大性能，但计算成本高昂。现有的PAC推理虽然提供统计保证，但仅在边际情况下有效，无法提供精确的条件覆盖。

Method: 提出G-PAC推理框架，通过划分输入空间实现组级PAC保证。开发两种具体实现：针对已知分组结构的Group PAC (G-PAC)推理，以及针对未知分组的Clustered PAC (C-PAC)推理。

Result: 理论证明G-PAC和C-PAC都能实现组条件风险控制，且在异构设置下分组能严格提升效率。在多样化推理基准测试中，两种方法都成功实现了组条件风险控制，同时保持了显著的计算节省。

Conclusion: G-PAC推理框架为大型推理模型提供了实用的组级PAC保证，在保持计算效率的同时提供了比传统边际PAC推理更精确的风险控制。

Abstract: Large reasoning models have shown strong performance through extended chain-of-thought reasoning, yet their computational cost remains significant. Probably approximately correct (PAC) reasoning provides statistical guarantees for efficient reasoning by adaptively switching between thinking and non-thinking models, but the guarantee holds only in the marginal case and does not provide exact conditional coverage. We propose G-PAC reasoning, a practical framework that provides PAC-style guarantees at the group level by partitioning the input space. We develop two instantiations: Group PAC (G-PAC) reasoning for known group structures and Clustered PAC (C-PAC) reasoning for unknown groupings. We prove that both G-PAC and C-PAC achieve group-conditional risk control, and that grouping can strictly improve efficiency over marginal PAC reasoning in heterogeneous settings. Our experiments on diverse reasoning benchmarks demonstrate that G-PAC and C-PAC successfully achieve group-conditional risk control while maintaining substantial computational savings.

</details>


### [43] [CVeDRL: An Efficient Code Verifier via Difficulty-aware Reinforcement Learning](https://arxiv.org/abs/2601.22803)
*Ji Shi,Peiming Guo,Meishan Zhang,Miao Zhang,Xuebo Liu,Min Zhang,Weili Guan*

Main category: cs.AI

TL;DR: CVeDRL提出了一种基于强化学习的代码验证器训练方法，通过语法、功能、分支覆盖和样本难度感知的奖励设计，显著提升了单元测试生成的质量和效率。


<details>
  <summary>Details</summary>
Motivation: 现有基于监督微调的代码验证器面临数据稀缺、高失败率和推理效率低的问题。虽然强化学习提供了无需标注监督的优化途径，但仅使用功能奖励的朴素RL方法难以生成针对困难分支和样本的有效单元测试。

Method: 首先理论分析表明分支覆盖、样本难度、语法和功能正确性可以联合建模为RL奖励。基于此设计了语法和功能感知的奖励，并进一步提出分支和样本难度感知的RL方法，采用指数奖励塑造和静态分析指标。

Result: CVeDRL仅用0.6B参数就达到了最先进性能，相比GPT-3.5实现了高达28.97%的通过率提升和15.08%的分支覆盖率提升，同时推理速度比竞争基线快20倍以上。

Conclusion: 通过将分支覆盖、样本难度、语法和功能正确性联合建模为RL奖励，CVeDRL显著提升了基于单元测试的代码验证的可靠性，在性能和效率方面都取得了显著改进。

Abstract: Code verifiers play a critical role in post-verification for LLM-based code generation, yet existing supervised fine-tuning methods suffer from data scarcity, high failure rates, and poor inference efficiency. While reinforcement learning (RL) offers a promising alternative by optimizing models through execution-driven rewards without labeled supervision, our preliminary results show that naive RL with only functionality rewards fails to generate effective unit tests for difficult branches and samples. We first theoretically analyze showing that branch coverage, sample difficulty, syntactic and functional correctness can be jointly modeled as RL rewards, where optimizing these signals can improve the reliability of unit-test-based verification. Guided by this analysis, we design syntax- and functionality-aware rewards and further propose branch- and sample-difficulty--aware RL using exponential reward shaping and static analysis metrics. With this formulation, CVeDRL achieves state-of-the-art performance with only 0.6B parameters, yielding up to 28.97% higher pass rate and 15.08% higher branch coverage than GPT-3.5, while delivering over $20\times$ faster inference than competitive baselines. Code is available at https://github.com/LIGHTCHASER1/CVeDRL.git

</details>


### [44] [TriCEGAR: A Trace-Driven Abstraction Mechanism for Agentic AI](https://arxiv.org/abs/2601.22997)
*Roham Koohestani,Ateş Görpelioğlu,Egor Klimov,Burcu Kulahcioglu Ozkan,Maliheh Izadi*

Main category: cs.AI

TL;DR: TriCEGAR：一种基于执行日志自动构建状态抽象的机制，用于智能体AI系统的运行时验证，无需手动定义状态抽象


<details>
  <summary>Details</summary>
Motivation: 智能体AI系统通过工具行动，行为在长期随机交互轨迹中演化，这使得保证变得复杂。现有动态概率保证（DPA）方法需要开发者手动定义状态抽象，这增加了采用摩擦并耦合到应用特定启发式方法

Method: 提出TriCEGAR机制，从执行日志自动构建状态抽象，使用谓词树表示抽象并通过反例进行细化。实现框架原生实现，包括：捕获类型化智能体生命周期事件、从轨迹构建抽象、构建MDP、执行概率模型检查

Result: TriCEGAR能够自动构建智能体行为MDP，支持在线构建，并计算概率边界如Pmax(成功)和Pmin(失败)。运行似然性还支持异常检测作为护栏信号

Conclusion: TriCEGAR通过自动化状态抽象构建，解决了现有DPA方法的关键限制，降低了智能体AI系统运行时验证的采用门槛

Abstract: Agentic AI systems act through tools and evolve their behavior over long, stochastic interaction traces. This setting complicates assurance, because behavior depends on nondeterministic environments and probabilistic model outputs. Prior work introduced runtime verification for agentic AI via Dynamic Probabilistic Assurance (DPA), learning an MDP online and model checking quantitative properties. A key limitation is that developers must manually define the state abstraction, which couples verification to application-specific heuristics and increases adoption friction. This paper proposes TriCEGAR, a trace-driven abstraction mechanism that automates state construction from execution logs and supports online construction of an agent behavioral MDP. TriCEGAR represents abstractions as predicate trees learned from traces and refined using counterexamples. We describe a framework-native implementation that (i) captures typed agent lifecycle events, (ii) builds abstractions from traces, (iii) constructs an MDP, and (iv) performs probabilistic model checking to compute bounds such as Pmax(success) and Pmin(failure). We also show how run likelihoods enable anomaly detection as a guardrailing signal.

</details>


### [45] [Aligning the Unseen in Attributed Graphs: Interplay between Graph Geometry and Node Attributes Manifold](https://arxiv.org/abs/2601.22806)
*Aldric Labarthe,Roland Bouffanais,Julien Randon-Furling*

Main category: cs.AI

TL;DR: 该论文指出传统属性图表示学习方法存在几何缺陷，提出了一种分离流形学习与结构对齐的自定义变分自编码器，通过量化度量扭曲来揭示传统方法无法检测的连接模式和异常。


<details>
  <summary>Details</summary>
Motivation: 传统属性图表示学习方法同时重建节点属性和图结构，但这种方法在几何上存在缺陷，因为它合并了两个可能不兼容的度量空间，导致破坏性的对齐，从而侵蚀了关于图底层生成过程的信息。

Method: 引入一种自定义的变分自编码器，将流形学习与结构对齐分离。通过量化将属性流形映射到图热核所需的度量扭曲，将几何冲突转化为可解释的结构描述符。

Result: 实验表明，该方法能够发现传统方法无法检测的连接模式和异常，证明了传统方法在理论上的不足和实践上的局限性。

Conclusion: 通过分离流形学习和结构对齐，并量化度量扭曲，可以恢复传统方法丢失的信号，为属性图表示学习提供更准确和可解释的方法。

Abstract: The standard approach to representation learning on attributed graphs -- i.e., simultaneously reconstructing node attributes and graph structure -- is geometrically flawed, as it merges two potentially incompatible metric spaces. This forces a destructive alignment that erodes information about the graph's underlying generative process. To recover this lost signal, we introduce a custom variational autoencoder that separates manifold learning from structural alignment. By quantifying the metric distortion needed to map the attribute manifold onto the graph's Heat Kernel, we transform geometric conflict into an interpretable structural descriptor. Experiments show our method uncovers connectivity patterns and anomalies undetectable by conventional approaches, proving both their theoretical inadequacy and practical limitations.

</details>


### [46] [Game-Theoretic Co-Evolution for LLM-Based Heuristic Discovery](https://arxiv.org/abs/2601.22896)
*Xinyi Ke,Kai Li,Junliang Xing,Yifan Zhang,Jian Cheng*

Main category: cs.AI

TL;DR: ASRO提出了一种基于博弈论的算法空间响应预言框架，将启发式发现重构为求解器与实例生成器之间的程序级协同进化，通过LLM驱动的响应预言实现自适应课程学习，显著提升了组合优化中的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有基于大语言模型的自动启发式发现方法主要局限于对固定实例分布的静态评估，容易导致过拟合和分布偏移下的泛化能力差。需要一种能够动态适应、避免静态评估局限性的框架。

Method: ASRO将启发式发现建模为双人零和博弈，维护求解器和实例生成器双方不断增长的战略池，通过基于LLM的最佳响应预言迭代扩展双方策略，使用混合对手元策略替代静态评估，形成自适应的自生成课程。

Result: 在多个组合优化领域中，ASRO始终优于基于相同程序搜索机制的静态训练基准方法，在多样化和分布外实例上实现了显著改进的泛化能力和鲁棒性。

Conclusion: ASRO通过博弈论框架将启发式发现重构为协同进化过程，利用LLM驱动的响应预言实现自适应课程学习，有效解决了静态评估方法的局限性，为自动启发式发现提供了更强大的泛化能力。

Abstract: Large language models (LLMs) have enabled rapid progress in automatic heuristic discovery (AHD), yet most existing methods are predominantly limited by static evaluation against fixed instance distributions, leading to potential overfitting and poor generalization under distributional shifts. We propose Algorithm Space Response Oracles (ASRO), a game-theoretic framework that reframes heuristic discovery as a program level co-evolution between solver and instance generator. ASRO models their interaction as a two-player zero-sum game, maintains growing strategy pools on both sides, and iteratively expands them via LLM-based best-response oracles against mixed opponent meta-strategies, thereby replacing static evaluation with an adaptive, self-generated curriculum. Across multiple combinatorial optimization domains, ASRO consistently outperforms static-training AHD baselines built on the same program search mechanisms, achieving substantially improved generalization and robustness on diverse and out-of-distribution instances.

</details>


### [47] [MulFeRL: Enhancing Reinforcement Learning with Verbal Feedback in a Multi-turn Loop](https://arxiv.org/abs/2601.22900)
*Xuancheng Li,Haitao Li,Yujia Zhou,YiqunLiu,Qingyao Ai*

Main category: cs.AI

TL;DR: 提出多轮反馈引导的强化学习框架，通过动态多轮生成、双重学习信号和结构化反馈注入，解决传统RLVR中稀疏奖励问题，在数学推理任务上表现优异


<details>
  <summary>Details</summary>
Motivation: 传统强化学习与可验证奖励（RLVR）在失败样本上仅提供稀疏的标量奖励，无法解释推理失败原因，限制了学习效果

Method: 提出多轮反馈引导强化学习框架，包含三个机制：1）仅在失败样本上触发的动态多轮反馈引导生成；2）用于轮内和跨轮优化的双重互补学习信号；3）结构化反馈注入到模型推理过程

Result: 在OpenR1-Math数据集上训练，该方法在领域内表现优于监督微调和RLVR基线，并在领域外展现出良好的泛化能力

Conclusion: 通过利用丰富的语言反馈指导RLVR训练，特别是针对失败样本，能够显著提升推理能力，为强化学习在复杂推理任务中的应用提供了新思路

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) is widely used to improve reasoning in multiple domains, yet outcome-only scalar rewards are often sparse and uninformative, especially on failed samples, where they merely indicate failure and provide no insight into why the reasoning fails. In this paper, we investigate how to leverage richer verbal feedback to guide RLVR training on failed samples, and how to convert such feedback into a trainable learning signal. Specifically, we propose a multi-turn feedback-guided reinforcement learning framework. It builds on three mechanisms: (1) dynamic multi-turn regeneration guided by feedback, triggered only on failed samples, (2) two complementary learning signals for within-turn and cross-turn optimization, and (3) structured feedback injection into the model's reasoning process. Trained on sampled OpenR1-Math, the approach outperforms supervised fine-tuning and RLVR baselines in-domain and generalizes well out-of-domain.

</details>


### [48] [Alignment among Language, Vision and Action Representations](https://arxiv.org/abs/2601.22948)
*Nicola Milano,Stefano Nolfi*

Main category: cs.AI

TL;DR: 研究发现语言、视觉和动作学习会产生部分共享的语义表示，支持跨模态的语义组织


<details>
  <summary>Details</summary>
Motivation: 探索不同学习模态（语言、视觉、动作）是否产生独特或共享的内部表示，挑战传统认为不同数据类型的模型会发展专门化、不可转移表示的观点

Method: 在BabyAI平台上训练基于transformer的智能体执行目标导向行为，生成动作基础的语言嵌入，然后与大型语言模型（LLaMA、Qwen、DeepSeek、BERT）和视觉语言模型（CLIP、BLIP）的表示进行比较

Result: 动作表示与仅解码器语言模型和BLIP强烈对齐（精度@15：0.70-0.73），接近语言模型之间的对齐程度，但与CLIP和BERT的对齐显著较弱

Conclusion: 语言、视觉和动作表示收敛于部分共享的语义结构，支持模态独立的语义组织，突显了在具身AI系统中跨领域转移的潜力

Abstract: A fundamental question in cognitive science and AI concerns whether different learning modalities: language, vision, and action, give rise to distinct or shared internal representations. Traditional views assume that models trained on different data types develop specialized, non-transferable representations. However, recent evidence suggests unexpected convergence: models optimized for distinct tasks may develop similar representational geometries. We investigate whether this convergence extends to embodied action learning by training a transformer-based agent to execute goal-directed behaviors in response to natural language instructions. Using behavioral cloning on the BabyAI platform, we generated action-grounded language embeddings shaped exclusively by sensorimotor control requirements. We then compared these representations with those extracted from state-of-the-art large language models (LLaMA, Qwen, DeepSeek, BERT) and vision-language models (CLIP, BLIP). Despite substantial differences in training data, modality, and objectives, we observed robust cross-modal alignment. Action representations aligned strongly with decoder-only language models and BLIP (precision@15: 0.70-0.73), approaching the alignment observed among language models themselves. Alignment with CLIP and BERT was significantly weaker. These findings indicate that linguistic, visual, and action representations converge toward partially shared semantic structures, supporting modality-independent semantic organization and highlighting potential for cross-domain transfer in embodied AI systems.

</details>


### [49] [Quantifying Model Uniqueness in Heterogeneous AI Ecosystems](https://arxiv.org/abs/2601.22977)
*Lei You*

Main category: cs.AI

TL;DR: 提出ISQED统计框架，通过匹配干预量化模型独特性（PIER），证明观测日志无法识别独特性，推导主动审计的缩放定律，展示合作博弈方法无法检测冗余


<details>
  <summary>Details</summary>
Motivation: 随着AI系统从孤立预测器演变为复杂的异构生态系统，区分真正的行为新颖性与功能冗余成为关键治理挑战，需要建立原则性的审计框架

Method: 引入In-Silico Quasi-Experimental Design (ISQED)框架，通过匹配干预隔离内在模型身份，量化Peer-Inexpressible Residual (PIER)，使用DISCO估计器实现，并推导主动审计的缩放定律

Result: 证明观测日志无法数学识别独特性，推导出最小最大最优样本效率的缩放定律，展示Shapley值等合作博弈方法无法检测冗余，在计算机视觉、语言模型和交通预测等多样化生态系统中验证框架

Conclusion: 该研究将可信AI从解释单一模型扩展到建立基于干预的异构模型生态系统审计科学，为AI治理提供原则性框架

Abstract: As AI systems evolve from isolated predictors into complex, heterogeneous ecosystems of foundation models and specialized adapters, distinguishing genuine behavioral novelty from functional redundancy becomes a critical governance challenge. Here, we introduce a statistical framework for auditing model uniqueness based on In-Silico Quasi-Experimental Design (ISQED). By enforcing matched interventions across models, we isolate intrinsic model identity and quantify uniqueness as the Peer-Inexpressible Residual (PIER), i.e. the component of a target's behavior strictly irreducible to any stochastic convex combination of its peers, with vanishing PIER characterizing when such a routing-based substitution becomes possible. We establish the theoretical foundations of ecosystem auditing through three key contributions. First, we prove a fundamental limitation of observational logs: uniqueness is mathematically non-identifiable without intervention control. Second, we derive a scaling law for active auditing, showing that our adaptive query protocol achieves minimax-optimal sample efficiency ($dσ^2γ^{-2}\log(Nd/δ)$). Third, we demonstrate that cooperative game-theoretic methods, such as Shapley values, fundamentally fail to detect redundancy. We implement this framework via the DISCO (Design-Integrated Synthetic Control) estimator and deploy it across diverse ecosystems, including computer vision models (ResNet/ConvNeXt/ViT), large language models (BERT/RoBERTa), and city-scale traffic forecasters. These results move trustworthy AI beyond explaining single models: they establish a principled, intervention-based science of auditing and governing heterogeneous model ecosystems.

</details>


### [50] [Why Your Deep Research Agent Fails? On Hallucination Evaluation in Full Research Trajectory](https://arxiv.org/abs/2601.22984)
*Yuhao Zhan,Tianyu Fan,Linxuan Huang,Zirui Guo,Chao Huang*

Main category: cs.AI

TL;DR: 提出从结果导向评估转向过程感知评估，通过审计完整研究轨迹来诊断深度研究智能体（DRAs）的失败机制，引入PIES分类法对幻觉进行分类，并构建DeepHalluBench基准测试。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试主要依赖端到端评估，掩盖了研究轨迹中积累的关键中间幻觉（如错误规划），难以诊断深度研究智能体的失败机制。

Method: 提出PIES分类法，按功能组件（规划vs总结）和错误属性（显式vs隐式）对幻觉进行分类；开发细粒度评估框架分解研究轨迹以量化这些幻觉；构建包含100个幻觉易发任务的DeepHalluBench基准。

Result: 在六个最先进的深度研究智能体上的实验显示，没有系统能实现稳健的可靠性；诊断分析将失败原因追溯到系统性缺陷，特别是幻觉传播和认知偏差。

Conclusion: 过程感知评估能更精确诊断深度研究智能体的失败机制，为未来架构优化提供基础性见解，PIES分类法和DeepHalluBench基准为系统可靠性评估提供了新框架。

Abstract: Diagnosing the failure mechanisms of Deep Research Agents (DRAs) remains a critical challenge. Existing benchmarks predominantly rely on end-to-end evaluation, obscuring critical intermediate hallucinations, such as flawed planning, that accumulate throughout the research trajectory. To bridge this gap, we propose a shift from outcome-based to process-aware evaluation by auditing the full research trajectory. We introduce the PIES Taxonomy to categorize hallucinations along functional components (Planning vs. Summarization) and error properties (Explicit vs. Implicit). We instantiate this taxonomy into a fine-grained evaluation framework that decomposes the trajectory to rigorously quantify these hallucinations. Leveraging this framework to isolate 100 distinctively hallucination-prone tasks including adversarial scenarios, we curate DeepHalluBench. Experiments on six state-of-theart DRAs reveal that no system achieves robust reliability. Furthermore, our diagnostic analysis traces the etiology of these failures to systemic deficits, specifically hallucination propagation and cognitive biases, providing foundational insights to guide future architectural optimization. Data and code are available at https://github.com/yuhao-zhan/DeepHalluBench.

</details>


### [51] [Guided by Trajectories: Repairing and Rewarding Tool-Use Trajectories for Tool-Integrated Reasoning](https://arxiv.org/abs/2601.23032)
*Siyu Gong,Linan Yue,Weibo Gao,Fangzhou Yao,Shimin Di,Lei Feng,Min-Ling Zhang*

Main category: cs.AI

TL;DR: AutoTraj：一个两阶段框架，通过修复和奖励工具使用轨迹来自动学习工具集成推理，无需依赖高质量合成轨迹


<details>
  <summary>Details</summary>
Motivation: 现有工具集成推理方法依赖高质量合成轨迹和稀疏结果奖励，提供有限且有偏的监督，需要更有效的学习框架

Method: 两阶段框架：1）监督微调阶段生成多个候选轨迹，评估后保留高质量轨迹，修复低质量轨迹；2）强化学习阶段基于偏好数据集训练轨迹级奖励模型，结合结果和格式奖励

Result: 在真实世界基准测试中证明了AutoTraj在工具集成推理中的有效性

Conclusion: AutoTraj通过自动修复和奖励工具使用轨迹，有效解决了现有工具集成推理方法的监督不足问题

Abstract: Tool-Integrated Reasoning (TIR) enables large language models (LLMs) to solve complex tasks by interacting with external tools, yet existing approaches depend on high-quality synthesized trajectories selected by scoring functions and sparse outcome-based rewards, providing limited and biased supervision for learning TIR. To address these challenges, in this paper, we propose AutoTraj, a two-stage framework that automatically learns TIR by repairing and rewarding tool-use trajectories. Specifically, in the supervised fine-tuning (SFT) stage, AutoTraj generates multiple candidate tool-use trajectories for each query and evaluates them along multiple dimensions. High-quality trajectories are directly retained, while low-quality ones are repaired using a LLM (i.e., LLM-as-Repairer). The resulting repaired and high-quality trajectories form a synthetic SFT dataset, while each repaired trajectory paired with its original low-quality counterpart constitutes a dataset for trajectory preference modeling. In the reinforcement learning (RL) stage, based on the preference dataset, we train a trajectory-level reward model to assess the quality of reasoning paths and combine it with outcome and format rewards, thereby explicitly guiding the optimization toward reliable TIR behaviors. Experiments on real-world benchmarks demonstrate the effectiveness of AutoTraj in TIR.

</details>


### [52] [The Hot Mess of AI: How Does Misalignment Scale With Model Intelligence and Task Complexity?](https://arxiv.org/abs/2601.23045)
*Alexander Hägele,Aryo Pradipta Gema,Henry Sleight,Ethan Perez,Jascha Sohl-Dickstein*

Main category: cs.AI

TL;DR: 研究发现随着AI能力增强，其失败行为更倾向于"混乱"而非"系统性目标追求"，这影响了AI安全研究的方向


<details>
  <summary>Details</summary>
Motivation: 随着AI承担更广泛和重要的任务，理解其失败模式变得至关重要：是系统性地追求错误目标，还是采取无意义的混乱行为？这关系到AI安全研究的方向选择。

Method: 使用偏差-方差分解来量化AI的"不连贯性"，测量模型错误中方差而非偏差的比例。在不同任务和前沿模型上进行实验，分析推理时间和模型规模对不连贯性的影响。

Result: 模型推理时间越长，其失败行为越不连贯；模型规模对不连贯性的影响因实验而异，但在多个设置中，更大、更强的模型表现出更高的不连贯性。仅靠规模扩展不太可能消除不连贯性。

Conclusion: 随着AI处理更复杂的顺序性任务，其失败将伴随更多不连贯行为，这意味着AI更可能造成工业事故（由于不可预测的错误行为），而非持续追求错误目标。这提高了针对奖励黑客攻击和目标错误指定的对齐研究的重要性。

Abstract: As AI becomes more capable, we entrust it with more general and consequential tasks. The risks from failure grow more severe with increasing task scope. It is therefore important to understand how extremely capable AI models will fail: Will they fail by systematically pursuing goals we do not intend? Or will they fail by being a hot mess, and taking nonsensical actions that do not further any goal? We operationalize this question using a bias-variance decomposition of the errors made by AI models: An AI's \emph{incoherence} on a task is measured over test-time randomness as the fraction of its error that stems from variance rather than bias in task outcome. Across all tasks and frontier models we measure, the longer models spend reasoning and taking actions, \emph{the more incoherent} their failures become. Incoherence changes with model scale in a way that is experiment dependent. However, in several settings, larger, more capable models are more incoherent than smaller models. Consequently, scale alone seems unlikely to eliminate incoherence. Instead, as more capable AIs pursue harder tasks, requiring more sequential action and thought, our results predict failures to be accompanied by more incoherent behavior. This suggests a future where AIs sometimes cause industrial accidents (due to unpredictable misbehavior), but are less likely to exhibit consistent pursuit of a misaligned goal. This increases the relative importance of alignment research targeting reward hacking or goal misspecification.

</details>


### [53] [From Abstract to Contextual: What LLMs Still Cannot Do in Mathematics](https://arxiv.org/abs/2601.23048)
*Bowen Cao,Dongdong Zhang,Yixia Li,Junpeng Liu,Shijue Huang,Chufan Shi,Hongyuan Lu,Yaokang Wu,Guanhua Chen,Wai Lam,Furu Wei*

Main category: cs.AI

TL;DR: ContextMATH基准测试显示，LLMs在上下文数学推理中表现显著下降，主要瓶颈在于问题表述而非计算能力


<details>
  <summary>Details</summary>
Motivation: 研究大型语言模型在基准测试中的优异表现为何未能转化为现实世界应用中的可靠性能，关注上下文数学推理能力

Method: 引入ContextMATH基准，将AIME和MATH-500问题转化为两种上下文设置：场景基础(SG)和复杂度扩展(CS)，评估61个专有和开源模型

Result: 模型表现显著下降：开源模型在SG和CS上分别下降13和34分，专有模型下降13和20分；错误主要由问题表述错误主导，表述准确性随原问题难度增加而下降

Conclusion: 正确的问题表述是成功的前提，其充分性随模型规模提升；表述和推理是两个互补的瓶颈；微调可改善性能但差距仅部分缓解，上下文数学推理仍是LLMs未解决的核心挑战

Abstract: Large language models now solve many benchmark math problems at near-expert levels, yet this progress has not fully translated into reliable performance in real-world applications. We study this gap through contextual mathematical reasoning, where the mathematical core must be formulated from descriptive scenarios. We introduce ContextMATH, a benchmark that repurposes AIME and MATH-500 problems into two contextual settings: Scenario Grounding (SG), which embeds abstract problems into realistic narratives without increasing reasoning complexity, and Complexity Scaling (CS), which transforms explicit conditions into sub-problems to capture how constraints often appear in practice. Evaluating 61 proprietary and open-source models, we observe sharp drops: on average, open-source models decline by 13 and 34 points on SG and CS, while proprietary models drop by 13 and 20. Error analysis shows that errors are dominated by incorrect problem formulation, with formulation accuracy declining as original problem difficulty increases. Correct formulation emerges as a prerequisite for success, and its sufficiency improves with model scale, indicating that larger models advance in both understanding and reasoning. Nevertheless, formulation and reasoning remain two complementary bottlenecks that limit contextual mathematical problem solving. Finally, we find that fine-tuning with scenario data improves performance, whereas formulation-only training is ineffective. However, performance gaps are only partially alleviated, highlighting contextual mathematical reasoning as a central unsolved challenge for LLMs.

</details>


### [54] [MedMCP-Calc: Benchmarking LLMs for Realistic Medical Calculator Scenarios via MCP Integration](https://arxiv.org/abs/2601.23049)
*Yakun Zhu,Yutong Huang,Shengqian Qin,Zhongzhen Huang,Shaoting Zhang,Xiaofan Zhang*

Main category: cs.AI

TL;DR: MedMCP-Calc是首个通过MCP集成评估LLM在真实医疗计算场景中的基准，包含118个跨4个临床领域的场景任务，揭示了当前模型在模糊查询、数据库交互和工具使用方面的显著局限性。


<details>
  <summary>Details</summary>
Motivation: 现有医疗计算基准仅关注静态单步计算，而真实临床实践需要多阶段自适应过程，包括主动获取EHR数据、场景依赖的计算器选择和复杂计算流程，因此需要更贴近实际的评估框架。

Method: 开发MedMCP-Calc基准，包含118个跨4个临床领域的场景任务，特征包括：模糊任务描述模拟自然查询、结构化EHR数据库交互、外部参考检索和过程级评估。基于评估结果开发CalcMate模型，集成场景规划和工具增强。

Result: 评估23个领先模型发现关键局限：即使顶级模型如Claude Opus 4.5也存在显著差距，包括难以根据模糊查询选择合适计算器、迭代SQL数据库交互性能差、不愿使用外部工具进行数值计算。性能在不同临床领域差异显著。CalcMate在开源模型中达到最先进性能。

Conclusion: MedMCP-Calc填补了医疗计算评估的重要空白，揭示了LLM在真实临床场景中的实际能力局限，为未来医疗AI系统开发提供了重要基准和改进方向。

Abstract: Medical calculators are fundamental to quantitative, evidence-based clinical practice. However, their real-world use is an adaptive, multi-stage process, requiring proactive EHR data acquisition, scenario-dependent calculator selection, and multi-step computation, whereas current benchmarks focus only on static single-step calculations with explicit instructions. To address these limitations, we introduce MedMCP-Calc, the first benchmark for evaluating LLMs in realistic medical calculator scenarios through Model Context Protocol (MCP) integration. MedMCP-Calc comprises 118 scenario tasks across 4 clinical domains, featuring fuzzy task descriptions mimicking natural queries, structured EHR database interaction, external reference retrieval, and process-level evaluation. Our evaluation of 23 leading models reveals critical limitations: even top performers like Claude Opus 4.5 exhibit substantial gaps, including difficulty selecting appropriate calculators for end-to-end workflows given fuzzy queries, poor performance in iterative SQL-based database interactions, and marked reluctance to leverage external tools for numerical computation. Performance also varies considerably across clinical domains. Building on these findings, we develop CalcMate, a fine-tuned model incorporating scenario planning and tool augmentation, achieving state-of-the-art performance among open-source models. Benchmark and Codes are available in https://github.com/SPIRAL-MED/MedMCP-Calc.

</details>


### [55] [Chain-of-thought obfuscation learned from output supervision can generalise to unseen tasks](https://arxiv.org/abs/2601.23086)
*Nathaniel Mitrani Hadida,Sassan Bhanji,Cameron Tice,Puria Radmard*

Main category: cs.AI

TL;DR: 研究发现，对大型语言模型的优化压力可能导致其模糊化思维链推理过程，这种模糊化行为会在不同任务间泛化，即使只惩罚最终行为也会导致推理过程变得不可监控。


<details>
  <summary>Details</summary>
Motivation: 思维链推理是监控大型语言模型行为的重要工具，但优化压力可能导致模型模糊化推理过程，从而失去监控能力。研究者希望了解这种模糊化行为是否会跨任务泛化，以及仅惩罚最终行为是否也会导致推理模糊化。

Method: 通过实验研究模型在奖励破解任务中的行为，观察模型是否学会模糊化思维链推理。研究测试了两种惩罚方式：直接惩罚推理过程和仅惩罚最终行为，分析模糊化行为在不同任务间的泛化情况。

Result: 研究发现：1）模型学会模糊化涉及奖励破解的推理过程；2）这种模糊化行为会在未见过的奖励破解场景中泛化；3）即使只惩罚最终行为，也会导致思维链推理的模糊化及其跨任务泛化。

Conclusion: 当前惩罚有害生成的做法可能无意中导致大型语言模型监控性的不可预测降低，因为模型会学会模糊化推理过程，使监控变得更加困难。

Abstract: Chain-of-thought (CoT) reasoning provides a significant performance uplift to LLMs by enabling planning, exploration, and deliberation of their actions. CoT is also a powerful tool for monitoring the behaviours of these agents: when faithful, they offer interpretations of the model's decision making process, and an early warning sign for dangerous behaviours. However, optimisation pressures placed on the CoT may cause the model to obfuscate reasoning traces, losing this beneficial property. We show that obfuscation can generalise across tasks; models that learn to obfuscate reasoning involving reward hacking (e.g. accessing and utilising leaked information) generalise both the reward hacking behaviour and its obfuscation in CoT to unseen reward hacking settings. Most worryingly, we show that obfuscation of CoT reasoning, and its generalisation across tasks, also follows when we penalise only the model's final actions after closing its CoT. Our findings suggest that current practices of penalising harmful generations may inadvertently lead to a reduction in the broader monitorability of LLMs in unpredictable ways.

</details>


### [56] [THINKSAFE: Self-Generated Safety Alignment for Reasoning Models](https://arxiv.org/abs/2601.23143)
*Seanie Lee,Sangwoo Park,Yumin Choi,Gyeongman Kim,Minki Kang,Jihun Yun,Dongmin Park,Jongho Park,Sung Ju Hwang*

Main category: cs.AI

TL;DR: ThinkSafe是一种自生成对齐框架，通过轻量级拒绝引导解锁模型的潜在安全知识，生成安全推理轨迹进行微调，在恢复安全对齐的同时保持推理能力，显著降低计算成本。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型通过强化学习在推理任务上过度优化，往往优先考虑合规性，导致模型容易受到有害提示的攻击。现有方法依赖外部教师蒸馏，但会引入分布差异，损害原生推理能力。

Method: 提出ThinkSafe自生成对齐框架，通过轻量级拒绝引导解锁模型识别危害的潜在知识，引导模型生成分布内的安全推理轨迹，然后在这些自生成响应上进行微调。

Result: 在DeepSeek-R1-Distill和Qwen3上的实验显示，ThinkSafe显著提高了安全性同时保持了推理能力。与GRPO相比，实现了更优的安全性和相当的推理能力，计算成本显著降低。

Conclusion: ThinkSafe通过自生成对齐有效解决了安全对齐退化问题，无需外部教师，在恢复安全性的同时最小化分布偏移，保持了模型的推理能力，且计算效率更高。

Abstract: Large reasoning models (LRMs) achieve remarkable performance by leveraging reinforcement learning (RL) on reasoning tasks to generate long chain-of-thought (CoT) reasoning. However, this over-optimization often prioritizes compliance, making models vulnerable to harmful prompts. To mitigate this safety degradation, recent approaches rely on external teacher distillation, yet this introduces a distributional discrepancy that degrades native reasoning. We propose ThinkSafe, a self-generated alignment framework that restores safety alignment without external teachers. Our key insight is that while compliance suppresses safety mechanisms, models often retain latent knowledge to identify harm. ThinkSafe unlocks this via lightweight refusal steering, guiding the model to generate in-distribution safety reasoning traces. Fine-tuning on these self-generated responses effectively realigns the model while minimizing distribution shift. Experiments on DeepSeek-R1-Distill and Qwen3 show ThinkSafe significantly improves safety while preserving reasoning proficiency. Notably, it achieves superior safety and comparable reasoning to GRPO, with significantly reduced computational cost. Code, models, and datasets are available at https://github.com/seanie12/ThinkSafe.git.

</details>


### [57] [Make Anything Match Your Target: Universal Adversarial Perturbations against Closed-Source MLLMs via Multi-Crop Routed Meta Optimization](https://arxiv.org/abs/2601.23179)
*Hui Lu,Yi Yu,Yiming Yang,Chenyu Yi,Xueyi Ke,Qixing Zhang,Bingquan Shen,Alex Kot,Xudong Jiang*

Main category: cs.AI

TL;DR: 本文提出MCRMO-Attack方法，解决通用目标可转移对抗攻击中的三个核心困难：目标监督高方差、词元匹配不可靠、以及少样本适应对初始化敏感的问题。


<details>
  <summary>Details</summary>
Motivation: 现有的黑盒可转移对抗攻击主要是样本特定的，跨输入重用性有限。本文研究更严格的通用目标可转移对抗攻击（UTTAA）设置，要求单个扰动能够一致地将任意输入导向指定目标，并在未知商业多模态大语言模型上有效。

Method: 提出MCRMO-Attack方法：1）通过注意力引导裁剪的多裁剪聚合稳定监督；2）通过可对齐性门控的词元路由提高词元级可靠性；3）元学习跨目标扰动先验以获得更强的每目标解决方案。

Result: 在商业MLLMs上，相比最强通用基线，在GPT-4o上未见图像攻击成功率提升+23.7%，在Gemini-2.0上提升+19.9%。

Conclusion: MCRMO-Attack有效解决了通用目标可转移对抗攻击中的核心挑战，显著提升了跨商业多模态大语言模型的攻击成功率。

Abstract: Targeted adversarial attacks on closed-source multimodal large language models (MLLMs) have been increasingly explored under black-box transfer, yet prior methods are predominantly sample-specific and offer limited reusability across inputs. We instead study a more stringent setting, Universal Targeted Transferable Adversarial Attacks (UTTAA), where a single perturbation must consistently steer arbitrary inputs toward a specified target across unknown commercial MLLMs. Naively adapting existing sample-wise attacks to this universal setting faces three core difficulties: (i) target supervision becomes high-variance due to target-crop randomness, (ii) token-wise matching is unreliable because universality suppresses image-specific cues that would otherwise anchor alignment, and (iii) few-source per-target adaptation is highly initialization-sensitive, which can degrade the attainable performance. In this work, we propose MCRMO-Attack, which stabilizes supervision via Multi-Crop Aggregation with an Attention-Guided Crop, improves token-level reliability through alignability-gated Token Routing, and meta-learns a cross-target perturbation prior that yields stronger per-target solutions. Across commercial MLLMs, we boost unseen-image attack success rate by +23.7\% on GPT-4o and +19.9\% on Gemini-2.0 over the strongest universal baseline.

</details>


### [58] [TSAQA: Time Series Analysis Question And Answering Benchmark](https://arxiv.org/abs/2601.23204)
*Baoyu Jing,Sanhorn Chen,Lecheng Zheng,Boyu Liu,Zihao Li,Jiaru Zou,Tianxin Wei,Zhining Liu,Zhichen Zeng,Ruizhong Qiu,Xiao Lin,Yuchen Yan,Dongqi Fu,Jingchao Ni,Jingrui He,Hanghang Tong*

Main category: cs.AI

TL;DR: TSAQA是一个统一的时间序列问答基准，包含6种任务类型，涵盖13个领域的21万个样本，采用多种格式评估大语言模型的时间序列分析能力。


<details>
  <summary>Details</summary>
Motivation: 当前多任务时间序列问答基准仅限于预测和异常检测任务，缺乏对更广泛时间序列分析能力的评估，需要更全面的基准来推动时间序列分析研究。

Method: 构建TSAQA基准，整合6种不同任务：异常检测、分类、特征描述、比较、数据转换和时间关系分析，涵盖13个领域，采用TF、MC和创新的PZ格式，共21万个样本。

Result: 零样本评估显示当前LLMs表现有限：最佳商业模型Gemini-2.5-Flash平均得分仅65.08；指令微调能提升开源模型性能，但最佳开源模型LLaMA-3.1-8B仍有很大改进空间。

Conclusion: TSAQA基准展示了时间序列分析对LLMs的挑战性，为评估和提升LLMs的时间序列分析能力提供了全面框架，推动了该领域的研究发展。

Abstract: Time series data are integral to critical applications across domains such as finance, healthcare, transportation, and environmental science. While recent work has begun to explore multi-task time series question answering (QA), current benchmarks remain limited to forecasting and anomaly detection tasks. We introduce TSAQA, a novel unified benchmark designed to broaden task coverage and evaluate diverse temporal analysis capabilities. TSAQA integrates six diverse tasks under a single framework ranging from conventional analysis, including anomaly detection and classification, to advanced analysis, such as characterization, comparison, data transformation, and temporal relationship analysis. Spanning 210k samples across 13 domains, the dataset employs diverse formats, including true-or-false (TF), multiple-choice (MC), and a novel puzzling (PZ), to comprehensively assess time series analysis. Zero-shot evaluation demonstrates that these tasks are challenging for current Large Language Models (LLMs): the best-performing commercial LLM, Gemini-2.5-Flash, achieves an average score of only 65.08. Although instruction tuning boosts open-source performance: the best-performing open-source model, LLaMA-3.1-8B, shows significant room for improvement, highlighting the complexity of temporal analysis for LLMs.

</details>


### [59] [Scaling Multiagent Systems with Process Rewards](https://arxiv.org/abs/2601.23228)
*Ed Li,Junyu Ren,Cat Yan*

Main category: cs.AI

TL;DR: MAPPA方法通过AI反馈为多智能体系统中的每个动作提供过程奖励，解决了信用分配和样本效率问题，在数学竞赛和数据分析任务上显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 多智能体系统在复杂任务中表现出潜力，但面临两个关键挑战：1）跨智能体的信用分配问题；2）昂贵多智能体rollout的样本效率问题。需要一种方法能够在没有真实标签的情况下提供细粒度监督，同时从每次rollout中提取最大训练信号。

Method: 提出MAPPA方法，通过AI反馈为每个智能体动作提供过程奖励，而不是仅在任务完成时分配信用。这种方法能够为个体智能体动作分配信用，实现细粒度监督，同时最大化每次rollout的训练信号提取。

Result: 在数学竞赛问题上，MAPPA在AIME上提升5.0-17.5个百分点，在AMC上提升7.8-17.2个百分点。在数据分析任务中，成功率提高12.5个百分点，质量指标提升高达30%。验证了跨不同领域多智能体系统的改进效果。

Conclusion: 通过解决信用分配和样本效率挑战，MAPPA为在复杂、长视野任务中扩展多智能体系统迈出了第一步，同时最小化人工监督需求。基于动作的监督能够显著提升多智能体系统在不同领域的性能。

Abstract: While multiagent systems have shown promise for tackling complex tasks via specialization, finetuning multiple agents simultaneously faces two key challenges: (1) credit assignment across agents, and (2) sample efficiency of expensive multiagent rollouts. In this work, we propose finetuning multiagent systems with per-action process rewards from AI feedback (MAPPA) to address both. Through assigning credit to individual agent actions rather than only at task completion, MAPPA enables fine-grained supervision without ground truth labels while extracting maximal training signal from each rollout. We demonstrate our approach on competition math problems and tool-augmented data analysis tasks. On unseen math problems, MAPPA achieves +5.0--17.5pp on AIME and +7.8--17.2pp on AMC. For data analysis tasks, our method improves success rate by +12.5pp while quality metrics improve by up to 30%, validating that per-action supervision can lead to improvements across different multiagent system on various domains. By addressing these challenges, our work takes a first step toward scaling multiagent systems for complex, long-horizon tasks with minimal human supervision.

</details>


### [60] [Strongly Polynomial Time Complexity of Policy Iteration for $L_\infty$ Robust MDPs](https://arxiv.org/abs/2601.23229)
*Ali Asadi,Krishnendu Chatterjee,Ehsan Goharshady,Mehrdad Karrabi,Alipasha Montaseri,Carlo Pagano*

Main category: cs.AI

TL;DR: 该论文证明了对于(s,a)-矩形L∞不确定性集的鲁棒MDPs，在固定折扣因子下，鲁棒策略迭代算法具有强多项式时间复杂度，解决了该领域的一个重要算法问题。


<details>
  <summary>Details</summary>
Motivation: 鲁棒MDPs是序列决策中的基本模型，能够处理转移概率的不确定性并优化最坏情况。虽然MDPs已有多项式时间和强多项式时间算法，但将这类结果推广到RMDPs一直是一个重要的开放问题。

Method: 采用鲁棒策略迭代算法来处理(s,a)-矩形L∞不确定性集的鲁棒MDPs，该模型包含了经典MDPs和回合制随机博弈。

Result: 证明了对于固定折扣因子，鲁棒策略迭代算法在(s,a)-矩形L∞ RMDPs上具有强多项式时间复杂度。

Conclusion: 该研究解决了鲁棒MDPs算法复杂性的一个重要开放问题，为(s,a)-矩形L∞不确定性集的鲁棒MDPs提供了强多项式时间算法保证。

Abstract: Markov decision processes (MDPs) are a fundamental model in sequential decision making. Robust MDPs (RMDPs) extend this framework by allowing uncertainty in transition probabilities and optimizing against the worst-case realization of that uncertainty. In particular, $(s, a)$-rectangular RMDPs with $L_\infty$ uncertainty sets form a fundamental and expressive model: they subsume classical MDPs and turn-based stochastic games. We consider this model with discounted payoffs. The existence of polynomial and strongly-polynomial time algorithms is a fundamental problem for these optimization models. For MDPs, linear programming yields polynomial-time algorithms for any arbitrary discount factor, and the seminal work of Ye established strongly--polynomial time for a fixed discount factor. The generalization of such results to RMDPs has remained an important open problem. In this work, we show that a robust policy iteration algorithm runs in strongly-polynomial time for $(s, a)$-rectangular $L_\infty$ RMDPs with a constant (fixed) discount factor, resolving an important algorithmic question.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [61] [Linux Kernel Recency Matters, CVE Severity Doesn't, and History Fades](https://arxiv.org/abs/2601.22196)
*Piotr Przymus,Witold Weiner,Krzysztof Rykaczewski,Gunnar Kudrjavets*

Main category: cs.SE

TL;DR: Linux内核成为自己的CNA后，研究发现内核漏洞修复延迟与严重性评分关联微弱，内核新旧程度才是关键预测因素，新内核修复更快，旧内核遗留漏洞更多


<details>
  <summary>Details</summary>
Motivation: 研究Linux内核成为自己的CVE编号机构后，分析内核漏洞的解剖结构和动态特征，了解驱动内核漏洞修复的关键因素，特别是为什么某些漏洞修复更快而其他漏洞长期存在

Method: 使用元数据、相关提交记录和补丁延迟数据进行分析，通过生存模型研究内核漏洞修复的动态特征，比较引入漏洞的提交与修复提交的复杂度和范围

Result: 严重性和CVSS评分指标与补丁延迟关联微弱；内核新旧程度是合理的预测因素，新内核修复更快，旧内核保留未解决的CVE；引入漏洞的提交通常比修复提交更广泛和复杂

Conclusion: Linux内核的CVE流程具有独特性，漏洞修复主要受内核新旧程度驱动而非传统安全指标，这反映了内核作为独特开源项目的特性

Abstract: In 2024, the Linux kernel became its own Common Vulnerabilities and Exposures (CVE) Numbering Authority (CNA), formalizing how kernel vulnerabilities are identified and tracked. We analyze the anatomy and dynamics of kernel CVEs using metadata, associated commits, and patch latency to understand what drives patching. Results show that severity and Common Vulnerability Scoring System (CVSS) metrics have a negligible association with patch latency, whereas kernel recency is a reasonable predictor in survival models. Kernel developers fix newer kernels sooner, while older ones retain unresolved CVEs. Commits introducing vulnerabilities are typically broader and more complex than their fixes, though often only approximate reconstructions of development history. The Linux kernel remains a unique open-source project -- its CVE process is no exception.

</details>


### [62] [Stalled, Biased, and Confused: Uncovering Reasoning Failures in LLMs for Cloud-Based Root Cause Analysis](https://arxiv.org/abs/2601.22208)
*Evelien Riddell,James Riddell,Gengyi Sun,Michał Antkiewicz,Krzysztof Czarnecki*

Main category: cs.SE

TL;DR: 该研究通过受控实验框架评估LLM在多跳根因分析中的推理能力，测试了6个LLM在两种智能体工作流下的表现，执行了48,000个故障场景，并建立了16种常见推理失败的分类体系。


<details>
  <summary>Details</summary>
Motivation: 现代云系统的高度分布式和相互依赖性使根因分析变得复杂，特别是多跳故障传播问题。虽然LLM为自动化RCA提供了新机会，但其实用价值取决于推理和决策的保真度。现有方法依赖历史事件语料库、处理超出LLM容量的海量遥测数据，或将推理嵌入复杂多智能体管道，这些条件模糊了失败是源于推理本身还是外围设计选择。

Method: 设计了受控实验框架，通过简化实验设置突出LLM的推理行为。评估了6个LLM在两种智能体工作流（ReAct和Plan-and-Execute）和一个非智能体基线下的表现，使用两个真实世界案例研究（GAIA和OpenRCA）。执行了48,000个模拟故障场景，测量根因准确性和中间推理轨迹质量。建立了16种常见RCA推理失败的标记分类体系，并使用LLM-as-a-Judge进行标注。

Result: 研究阐明了当前开源LLM在多跳RCA中的成功和失败之处，量化了对输入数据模态的敏感性，并识别了能够预测最终正确性的推理失败模式。提供了透明可复现的实证结果和失败分类体系。

Conclusion: 该研究为推理驱动的系统诊断提供了实证基础和指导框架，通过隔离LLM的推理行为，明确了当前LLM在复杂根因分析中的能力边界和局限性，为未来工作提供了有价值的参考。

Abstract: Root cause analysis (RCA) is essential for diagnosing failures within complex software systems to ensure system reliability. The highly distributed and interdependent nature of modern cloud-based systems often complicates RCA efforts, particularly for multi-hop fault propagation, where symptoms appear far from their true causes. Recent advancements in Large Language Models (LLMs) present new opportunities to enhance automated RCA. However, their practical value for RCA depends on the fidelity of reasoning and decision-making. Existing work relies on historical incident corpora, operates directly on high-volume telemetry beyond current LLM capacity, or embeds reasoning inside complex multi-agent pipelines -- conditions that obscure whether failures arise from reasoning itself or from peripheral design choices.
  We present a focused empirical evaluation that isolates an LLM's reasoning behavior. We design a controlled experimental framework that foregrounds the LLM by using a simplified experimental setting. We evaluate six LLMs under two agentic workflows (ReAct and Plan-and-Execute) and a non-agentic baseline on two real-world case studies (GAIA and OpenRCA). In total, we executed 48,000 simulated failure scenarios, totaling 228 days of execution time. We measure both root-cause accuracy and the quality of intermediate reasoning traces. We produce a labeled taxonomy of 16 common RCA reasoning failures and use an LLM-as-a-Judge for annotation. Our results clarify where current open-source LLMs succeed and fail in multi-hop RCA, quantify sensitivity to input data modalities, and identify reasoning failures that predict final correctness. Together, these contributions provide transparent and reproducible empirical results and a failure taxonomy to guide future work on reasoning-driven system diagnosis.

</details>


### [63] [PriviSense: A Frida-Based Framework for Multi-Sensor Spoofing on Android](https://arxiv.org/abs/2601.22414)
*Ibrahim Khalilov,Chaoran Chen,Ziang Xiao,Tianshi Li,Toby Jia-Jun Li,Yaxing Yao*

Main category: cs.SE

TL;DR: PriviSense是一个基于Frida的工具包，可在已root的Android设备上实时伪造传感器和系统信号，支持无需模拟器或应用修改的可重复测试。


<details>
  <summary>Details</summary>
Motivation: 移动应用越来越依赖实时传感器和系统数据来适应用户上下文，但模拟器和插桩构建通常无法支持在物理设备上对上下文敏感的应用行为进行可重复测试。

Method: 使用基于Frida的运行时欺骗技术，在已root的Android设备上脚本化和注入时变传感器流（加速度计、陀螺仪、步数计数器）和系统值（电池电量、系统时间、设备元数据）到未修改的应用中。

Result: 在已root的Android设备上对五个代表性的传感器可视化应用进行了实时欺骗验证，支持对这些值的可脚本化和可逆操作。

Conclusion: PriviSense通过支持可脚本化和可逆的操作，促进了应用逻辑测试、基于上下文行为的发现以及隐私重点分析，为确保伦理使用，代码仅与经过验证的研究人员共享。

Abstract: Mobile apps increasingly rely on real-time sensor and system data to adapt their behavior to user context. While emulators and instrumented builds offer partial solutions, they often fail to support reproducible testing of context-sensitive app behavior on physical devices. We present PriviSense, a Frida-based, on-device toolkit for runtime spoofing of sensor and system signals on rooted Android devices. PriviSense can script and inject time-varying sensor streams (accelerometer, gyroscope, step counter) and system values (battery level, system time, device metadata) into unmodified apps, enabling reproducible on-device experiments without emulators or app rewrites. Our demo validates real-time spoofing on a rooted Android device across five representative sensor-visualization apps. By supporting scriptable and reversible manipulation of these values, PriviSense facilitates testing of app logic, uncovering of context-based behaviors, and privacy-focused analysis. To ensure ethical use, the code is shared upon request with verified researchers.
  Tool Guide: How to Run PriviSense on Rooted Android https://bit.ly/privisense-guide Demonstration video: https://www.youtube.com/watch?v=4Qwnogcc3pw

</details>


### [64] [Small is Beautiful: A Practical and Efficient Log Parsing Framework](https://arxiv.org/abs/2601.22590)
*Minxing Wang,Yintong Huo*

Main category: cs.SE

TL;DR: EFParser是一种基于小规模LLM的无监督日志解析器，通过双缓存系统和校正模块提升小模型性能，在保持高效的同时显著优于现有方法


<details>
  <summary>Details</summary>
Motivation: 现有基于大语言模型的语义日志解析器虽然泛化能力强，但严重依赖模型规模，使用小模型时性能急剧下降。而实际应用中由于数据隐私和计算资源限制，需要使用更轻量级的模型，这构成了现实部署的主要障碍。

Method: 提出EFParser，采用双缓存系统（带自适应更新机制）区分新模板和现有模板变体，能够合并冗余模板并修正先前错误。还设计了专门的校正模块作为门控器，验证和优化每个LLM生成的模板后再缓存，防止错误注入。

Result: 在公开大规模数据集上的实证评估显示，EFParser在小规模LLM上运行时，在所有指标上平均优于最先进基线12.5%，甚至超过了一些使用大规模模型的基线方法。尽管增加了验证步骤，EFParser仍保持高计算效率。

Conclusion: EFParser通过系统架构创新，有效提升了小规模LLM在日志解析任务上的性能，为现实世界日志分析部署提供了鲁棒且实用的解决方案，解决了小模型性能下降的问题。

Abstract: Log parsing is a fundamental step in log analysis, partitioning raw logs into constant templates and dynamic variables. While recent semantic-based parsers leveraging Large Language Models (LLMs) exhibit superior generalizability over traditional syntax-based methods, their effectiveness is heavily contingent on model scale. This dependency leads to significant performance collapse when employing smaller, more resource-efficient LLMs. Such degradation creates a major barrier to real-world adoption, where data privacy requirements and computational constraints necessitate the use of succinct models. To bridge this gap, we propose EFParser, an unsupervised LLM-based log parser designed to enhance the capabilities of smaller models through systematic architectural innovation. EFParser introduces a dual-cache system with an adaptive updating mechanism that distinguishes between novel patterns and variations of existing templates. This allows the parser to merge redundant templates and rectify prior errors, maintaining cache consistency. Furthermore, a dedicated correction module acts as a gatekeeper, validating and refining every LLM-generated template before caching to prevent error injection. Empirical evaluations on public large-scale datasets demonstrate that EFParser outperforms state-of-the-art baselines by an average of 12.5% across all metrics when running on smaller LLMs, even surpassing some baselines utilizing large-scale models. Despite its additional validation steps, EFParser maintains high computational efficiency, offering a robust and practical solution for real-world log analysis deployment.

</details>


### [65] [Elderly HealthMag: Systematic Building and Calibrating a Tool for Identifying and Evaluating Senior User Digital Health Software](https://arxiv.org/abs/2601.22627)
*Yuqing Xiao,John Grundy,Anuradha Madugalla,Elizabeth Manias*

Main category: cs.SE

TL;DR: 研究人员开发了HealthMag工具，帮助数字健康软件开发团队更好地识别和满足有健康问题的用户需求，特别是针对老年用户的Elderly HealthMag双重方法。


<details>
  <summary>Details</summary>
Motivation: 数字健康软件经常基于对用户的隐含、错误假设进行开发，导致产品无法满足有健康问题的用户的实际需求，特别是老年用户的需求，使得软件在实际使用中缺乏包容性。

Method: 基于InclusiveMag框架，通过系统映射和校准开发HealthMag工具，并与现有的AgeMag方法整合，创建了针对老年用户的Elderly HealthMag双重方法，通过认知走查来识别数字健康应用中的包容性偏见。

Result: 开发了HealthMag和Elderly HealthMag工具，能够帮助识别当前面向老年用户的数字健康应用中的包容性偏见，提高软件对特定健康条件和年龄需求的适应性。

Conclusion: HealthMag工具为数字健康软件开发提供了系统化的方法来更好地获取、建模和评估用户需求，特别是针对有健康问题和老年用户的包容性需求，有助于开发更有效的数字健康解决方案。

Abstract: Digital health (DH) software is increasingly deployed to populations where many end users live with one or more health conditions. Yet, DH software development teams frequently operate using implicit, incorrect assumptions about these users, resulting in products that under-serve the specific requirements imposed by their age and health conditions. Consequently, while software may meet clinical objectives on paper, it often fails to be inclusive during actual user interaction. To address this, we propose \textbf{\textit{HealthMag}}, a tool inspired by GenderMag designed to help better elicit, model and evaluate requirements for digital health software. We developed HealthMag through systematic mapping and calibration following the InclusiveMag framework. Furthermore, we integrated this with a calibrated version of an existing AgeMag method to create a dual-lens approach: \textbf{\textit{Elderly HealthMag}}, designed to aid requirements, design and evaluation of mHealth software for senior end users. We demonstrate application and utility of Age HealthMag via cognitive walkthroughs in identifying inclusivity biases in current senior user-oriented digital health applications.

</details>


### [66] [From Horizontal Layering to Vertical Integration: A Comparative Study of the AI-Driven Software Development Paradigm](https://arxiv.org/abs/2601.22667)
*Chi Zhang,Zehan Li,Ziqian Zhong,Haibing Ma,Dan Xiao,Chen Lin,Ming Dong*

Main category: cs.SE

TL;DR: 生成式AI在软件工程中的应用导致组织结构从水平分层向垂直整合转变，带来8-33倍的资源效率提升，主要得益于"超级员工"的出现和跨职能协调成本的消除。


<details>
  <summary>Details</summary>
Motivation: 研究生成式AI在软件工程中的组织影响，对比传统企业和AI原生初创公司的不同开发环境，探索AI如何改变软件工程的组织结构和效率。

Method: 采用多案例比较研究方法，对比分析传统企业（棕地）和AI原生初创公司（绿地）两种开发环境，通过总要素生产率分析识别AI扭曲效应。

Result: 从水平分层（职能专业化）向垂直整合（端到端所有权）转变可带来8-33倍的资源消耗减少；AI增强的工程师（超级员工）跨越传统角色边界，消除跨职能协调成本；发现AI扭曲效应，降低劳动规模回报同时放大技术杠杆。

Conclusion: 提出人机协作效能应成为工程组织的主要优化目标，取代个人生产力指标；为管理者提供组织重新设计策略，包括重新激活高级工程师的闲置认知带宽和抑制盲目规模扩张。

Abstract: This paper examines the organizational implications of Generative AI adoption in software engineering through a multiple-case comparative study. We contrast two development environments: a traditional enterprise (brownfield) and an AI-native startup (greenfield). Our analysis reveals that transitioning from Horizontal Layering (functional specialization) to Vertical Integration (end-to-end ownership) yields 8-fold to 33-fold reductions in resource consumption. We attribute these gains to the emergence of Super Employees, AI-augmented engineers who span traditional role boundaries, and the elimination of inter-functional coordination overhead. Theoretically, we propose Human-AI Collaboration Efficacy as the primary optimization target for engineering organizations, supplanting individual productivity metrics. Our Total Factor Productivity analysis identifies an AI Distortion Effect that diminishes returns to labor scale while amplifying technological leverage. We conclude with managerial strategies for organizational redesign, including the reactivation of idle cognitive bandwidth in senior engineers and the suppression of blind scale expansion.

</details>


### [67] [VarParser: Unleashing the Neglected Power of Variables for LLM-based Log Parsing](https://arxiv.org/abs/2601.22676)
*Jinrui Sun,Tong Jia,Minghua He,Ying Li*

Main category: cs.SE

TL;DR: VarParser提出了一种以变量为中心的日志解析策略，通过利用日志中的变量部分信息，解决了现有LLM日志解析器只关注常量部分的问题，显著提高了解析准确率和效率。


<details>
  <summary>Details</summary>
Motivation: 现有基于大语言模型（LLM）的日志解析器都只关注日志的常量部分，忽略了变量部分对日志解析的潜在贡献。这种常量中心策略带来了四个关键问题：1）仅使用常量信息导致日志分组和采样效率低下；2）基于常量的缓存导致LLM调用次数较多，降低解析准确率和效率；3）提示中消耗的常量标记较多导致LLM调用成本高；4）结果中只保留占位符，失去了日志中变量信息带来的系统可见性。

Method: 提出VarParser变量中心日志解析策略，包含三个关键技术：1）变量贡献采样：高效捕获日志的变量部分并利用其对解析的贡献；2）变量中心解析缓存：减少LLM调用次数；3）自适应变量感知上下文学习。通过引入变量单元，保留丰富的变量信息，增强日志解析结果的完整性。

Result: 在大规模数据集上的广泛评估表明，VarParser相比现有方法实现了更高的准确率，同时显著提高了解析效率并降低了LLM调用成本。

Conclusion: VarParser通过采用变量中心的日志解析策略，有效解决了现有LLM日志解析器的局限性，在保持系统可见性的同时，显著提升了日志解析的准确率、效率和成本效益。

Abstract: Logs serve as a primary source of information for engineers to diagnose failures in large-scale online service systems. Log parsing, which extracts structured events from massive unstructured log data, is a critical first step for downstream tasks like anomaly detection and failure diagnosis. With advances in large language models (LLMs), leveraging their strong text understanding capabilities has proven effective for accurate log parsing. However, existing LLM-based log parsers all focus on the constant part of logs, ignoring the potential contribution of the variable part to log parsing. This constant-centric strategy brings four key problems. First, inefficient log grouping and sampling with only constant information. Second, a relatively large number of LLM invocations due to constant-based cache, leading to low log parsing accuracy and efficiency. Third, a relatively large number of consumed constant tokens in prompts leads to high LLM invocation costs. At last, these methods only retain placeholders in the results, losing the system visibility brought by variable information in logs.
  Facing these problems, we propose a variable-centric log parsing strategy named VarParser. Through variable contribution sampling, variable-centric parsing cache, and adaptive variable-aware in-context learning, our approach can efficiently capture the variable parts of logs and leverage their contributions to parsing. By introducing variable units, we preserve rich variable information, enhancing the integrity of log parsing results. Extensive evaluations on large-scale datasets demonstrate that VarParser achieves higher accuracy compared to existing methods, significantly improving parsing efficiency while reducing the LLM invocation costs.

</details>


### [68] [AutoMerge: Search-Based Model Merging Framework for Effective Model Reuse](https://arxiv.org/abs/2601.22748)
*You Lu,Jiyang Zhang,Bihuan Chen,Chaofeng Sha,Dingji Wang,Xin Peng*

Main category: cs.SE

TL;DR: 该论文首次系统研究了模型合并技术在不同架构和领域的适用性，发现现有方法在LLMs之外效果不佳，提出了AutoMerge框架来解决这些问题。


<details>
  <summary>Details</summary>
Motivation: 软件重用是软件工程中的重要课题，模型合并作为训练免费的方法在LLMs中取得了成功，但缺乏对跨领域、不同架构模型的系统研究，需要探索模型合并技术在其他深度学习模型中的适用性。

Method: 1. 系统评估了5种模型合并技术在3种不同模型架构（LLMs、图像分类、自动驾驶）上的表现；2. 提出了AutoMerge框架：首先将复杂模型分割为多个异构块，然后系统探索合并空间以确定最佳合并技术及其超参数配置。

Result: 研究发现：1. 直接应用现有模型合并技术在不同领域效果不一致，远不如在LLMs中的成功；2. 单一合并技术难以处理模型内部的异构结构特性；3. 合并技术的效果对超参数配置高度敏感；4. 现有方法限制了模型合并的广泛应用潜力。

Conclusion: 模型合并技术不能简单地从LLMs扩展到其他领域，需要针对不同架构和领域进行专门优化。AutoMerge框架通过分割异构块和系统搜索的方法，为解决跨领域模型合并问题提供了有效方案。

Abstract: Software reuse has long been recognized as a critical and widely studied topic in software engineering, offering substantial benefits in reducing development costs, improving software quality, and enhancing operational efficiency. This paradigm extends into deep learning through model reuse. Recently, model merging has emerged in the domain of large language models (LLMs) as a training-free approach that takes multiple task-specific models with the same architecture as source models and merges them without retraining, enhancing model reuse within LLMs. However, no prior work has systematically investigated whether such an approach can be effectively applied to other deep learning models with different architectures across domains. To bridge this gap, we present the first systematic study that evaluates five model merging techniques on three distinct model architectures across three domains: LLMs, image classification, and autonomous driving. Our findings reveal that directly applying existing model merging techniques leads to highly inconsistent results and falls notably short of their success within LLMs. Moreover, a single model merging technique often fails to handle the heterogeneous structural properties within a model, limiting its applicability to different model architectures across domains. Furthermore, the effectiveness of model merging techniques is highly sensitive to hyperparameter configurations, thereby constraining their potential for broader adoption. Inspired by these insights, we propose AutoMerge, a novel search-based model merging framework that first segments complex models into multiple heterogeneous blocks and then systematically explores the merging space to identify the merging technique and its hyperparameter configuration.

</details>


### [69] [Constructing Safety Cases for AI Systems: A Reusable Template Framework](https://arxiv.org/abs/2601.22773)
*Sung Une Lee,Liming Zhu,Md Shamsujjoha,Liming Dong,Qinghua Lu,Jieshan Chen*

Main category: cs.SE

TL;DR: 该研究针对AI系统安全论证的挑战，提出了一个可重用安全论证模板框架，包含AI特定的声明类型、论证类型和证据家族分类，以应对生成式AI和前沿AI系统的动态特性。


<details>
  <summary>Details</summary>
Motivation: 传统安全论证方法（如航空或核工程领域）依赖于明确的系统边界、稳定架构和已知故障模式，但现代AI系统（特别是生成式和智能体AI）具有能力不可预测性、行为随提示变化、风险特征随微调或部署环境变化等动态特性，导致传统方法无法有效捕捉这些动态风险。

Method: 提出一个可重用安全论证模板框架，包含：1）AI特定的声明类型分类（基于断言、基于约束、基于能力）；2）论证类型分类（演示性、比较性、因果/解释性、基于风险、规范性）；3）证据家族分类（经验性、机制性、比较性、专家驱动、形式化方法、操作/现场数据、基于模型）。每个模板采用预定义的声明-论证-证据结构，并通过端到端模式解决特定挑战。

Result: 开发了一个系统化、可组合、可重用的安全论证构建和维护方法，能够应对无真实标签评估、动态模型更新、基于阈值的风险决策等独特挑战，使安全论证更加可信、可审计，并能适应生成式和前沿AI系统的演化行为。

Conclusion: 该框架为AI系统安全论证提供了系统化方法，克服了传统安全论证在应对AI动态特性时的局限性，通过可重用模板使安全论证更加可信、可审计且能适应AI系统的持续演化。

Abstract: Safety cases, structured arguments that a system is acceptably safe, are becoming central to the governance of AI systems. Yet, traditional safety-case practices from aviation or nuclear engineering rely on well-specified system boundaries, stable architectures, and known failure modes. Modern AI systems such as generative and agentic AI are the opposite. Their capabilities emerge unpredictably from low-level training objectives, their behaviour varies with prompts, and their risk profiles shift through fine-tuning, scaffolding, or deployment context. This study examines how safety cases are currently constructed for AI systems and why classical approaches fail to capture these dynamics. It then proposes a framework of reusable safety-case templates, each following a predefined structure of claims, arguments, and evidence tailored for AI systems. The framework introduces comprehensive taxonomies for AI-specific claim types (assertion-based, constrained-based, capability-based), argument types (demonstrative, comparative, causal/explanatory, risk-based, and normative), and evidence families (empirical, mechanistic, comparative, expert-driven, formal methods, operational/field data, and model-based). Each template is illustrated through end-to-end patterns addressing distinctive challenges such as evaluation without ground truth, dynamic model updates, and threshold-based risk decisions. The result is a systematic, composable, and reusable approach to constructing and maintaining safety cases that are credible, auditable, and adaptive to the evolving behaviour of generative and frontier AI systems.

</details>


### [70] [Understanding on the Edge: LLM-generated Boundary Test Explanations](https://arxiv.org/abs/2601.22791)
*Sabinakhon Akbarova,Felix Dobslaw,Robert Feldt*

Main category: cs.SE

TL;DR: 该研究探索了LLM生成边界值分析解释的实用性，通过调查27名软件专业人员对GPT-4.1生成的20个边界对解释进行评分，发现63.5%的评分是积极的，表明LLM在提供边界值解释方面具有潜力，但需要进一步改进。


<details>
  <summary>Details</summary>
Motivation: 边界值分析在软件质量保证中至关重要，但测试人员常常难以理解和证明某些输入-输出对为何代表有意义的行为边界。虽然大语言模型可以生成自然语言解释，但其在边界值分析中的价值尚未经过实证评估。

Method: 通过探索性研究，调查27名软件专业人员对GPT-4.1生成的20个边界对解释进行评分（清晰度、正确性、完整性和感知有用性），其中6人参与后续访谈。从这些见解中提炼出七项需求检查表。

Result: 63.5%的评分是积极的（5点李克特量表中的4-5分），17%是消极的（1-2分）。参与者偏好结构清晰、引用权威来源、根据读者专业知识调整深度的解释，并强调需要可操作的示例来支持调试和文档。

Conclusion: 研究结果表明，通过进一步改进，基于LLM的工具可以通过使边界解释更具可操作性和可信度来支持测试工作流程。从见解中提炼的七项需求检查表为未来基于LLM的边界解释工具定义了具体的设计标准。

Abstract: Boundary value analysis and testing (BVT) is fundamental in software quality assurance because faults tend to cluster at input extremes, yet testers often struggle to understand and justify why certain input-output pairs represent meaningful behavioral boundaries. Large Language Models (LLMs) could help by producing natural-language rationales, but their value for BVT has not been empirically assessed. We therefore conducted an exploratory study on LLM-generated boundary explanations: in a survey, twenty-seven software professionals rated GPT-4.1 explanations for twenty boundary pairs on clarity, correctness, completeness and perceived usefulness, and six of them elaborated in follow-up interviews. Overall, 63.5% of all ratings were positive (4-5 on a five-point Likert scale) compared to 17% negative (1-2), indicating general agreement but also variability in perceptions. Participants favored explanations that followed a clear structure, cited authoritative sources, and adapted their depth to the reader's expertise; they also stressed the need for actionable examples to support debugging and documentation. From these insights, we distilled a seven-item requirement checklist that defines concrete design criteria for future LLM-based boundary explanation tools. The results suggest that, with further refinement, LLM-based tools can support testing workflows by making boundary explanations more actionable and trustworthy.

</details>


### [71] [Just-in-Time Catching Test Generation at Meta](https://arxiv.org/abs/2601.22832)
*Matthew Becker,Yifei Chen,Nicholas Cochran,Pouyan Ghasemi,Abhishek Gulati,Mark Harman,Zachary Haluza,Mehrdad Honarkhah,Herve Robert,Jiacheng Liu,Weini Liu,Sreeja Thummala,Xiaoning Yang,Rui Xin,Sophie Zeng*

Main category: cs.SE

TL;DR: Meta开发了即时捕获测试生成系统，用于在大型后端系统中预防bug。与传统测试不同，捕获测试旨在失败，在代码合并前发现bug。通过代码变更感知方法，候选捕获生成效率比传统测试提升4倍，比偶然失败测试提升20倍。结合规则和LLM评估器减少70%人工审核，成功识别出8个真实bug，其中4个可能引发严重故障。


<details>
  <summary>Details</summary>
Motivation: Meta面临大规模后端系统（数亿行代码）中预防bug的挑战。传统强化测试在生成时通过，无法有效发现潜在问题。需要一种能在代码合并前主动发现bug的方法，同时减少误报带来的开发负担。

Method: 采用即时捕获测试生成方法：1）代码变更感知方法生成候选捕获测试；2）结合规则基础和LLM基础的评估器筛选测试；3）统计分析人工接受和拒绝的代码变更中的误报和真阳性情况；4）向工程师报告候选捕获进行验证。

Result: 1）代码变更感知方法使候选捕获生成比传统强化测试提升4倍，比偶然失败测试提升20倍；2）评估器减少70%人工审核负担；3）统计分析显示人工接受的变更误报显著更多，人工拒绝的变更真阳性显著更多；4）报告的41个候选捕获中，8个确认为真阳性，其中4个可能引发严重故障。

Conclusion: 即时捕获测试生成方法具有可扩展性和工业适用性，能有效防止严重故障进入生产环境。该方法通过主动发现bug、减少误报负担，为大规模代码库的质量保障提供了有效解决方案。

Abstract: We report on Just-in-Time catching test generation at Meta, designed to prevent bugs in large scale backend systems of hundreds of millions of line of code. Unlike traditional hardening tests, which pass at generation time, catching tests are meant to fail, surfacing bugs before code lands. The primary challenge is to reduce development drag from false positive test failures. Analyzing 22,126 generated tests, we show code-change-aware methods improve candidate catch generation 4x over hardening tests and 20x over coincidentally failing tests. To address false positives, we use rule-based and LLM-based assessors. These assessors reduce human review load by 70%. Inferential statistical analysis showed that human-accepted code changes are assessed to have significantly more false positives, while human-rejected changes have significantly more true positives. We reported 41 candidate catches to engineers; 8 were confirmed to be true positives, 4 of which would have led to serious failures had they remained uncaught. Overall, our results show that Just-in-Time catching is scalable, industrially applicable, and that it prevents serious failures from reaching production.

</details>


### [72] [AnoMod: A Dataset for Anomaly Detection and Root Cause Analysis in Microservice Systems](https://arxiv.org/abs/2601.22881)
*Ke Ping,Hamza Bin Mazhar,Yuqing Wang,Ying Song,Mika V. Mäntylä*

Main category: cs.SE

TL;DR: 该论文提出了AnoMod数据集，一个用于微服务系统异常检测和根因分析的多模态数据集，包含四种异常类型和五种监控模态。


<details>
  <summary>Details</summary>
Motivation: 当前微服务系统缺乏高质量、公开可用的异常检测和根因分析数据集。现有基准主要关注性能相关故障，且仅提供一到两种监控模态，限制了更广泛故障模式和多模态方法的研究。

Method: 基于两个开源微服务系统（SocialNetwork和TrainTicket），设计并注入了四类异常：性能级、服务级、数据库级和代码级。为每个场景收集五种模态数据：日志、指标、分布式追踪、API响应和代码覆盖率报告。

Result: 创建了AnoMod数据集，该数据集提供了更丰富、端到端的系统状态视图和跨服务交互信息，支持跨模态异常检测和融合/消融策略评估，以及跨服务和代码区域的细粒度根因分析研究。

Conclusion: AnoMod数据集填补了微服务系统异常检测和根因分析领域的数据空白，支持端到端故障排除流程，能够同时考虑异常检测和定位，为多模态方法研究提供了重要资源。

Abstract: Microservice systems (MSS) have become a predominant architectural style for cloud services. Yet the community still lacks high-quality, publicly available datasets for anomaly detection (AD) and root cause analysis (RCA) in MSS. Most benchmarks emphasize performance-related faults and provide only one or two monitoring modalities, limiting research on broader failure modes and cross-modal methods. To address these gaps, we introduce a new multimodal anomaly dataset built on two open-source microservice systems: SocialNetwork and TrainTicket. We design and inject four categories of anomalies (Ano): performance-level, service-level, database-level, and code-level, to emulate realistic anomaly modes. For each scenario, we collect five modalities (Mod): logs, metrics, distributed traces, API responses, and code coverage reports, offering a richer, end-to-end view of system state and inter-service interactions. We name our dataset, reflecting its unique properties, as AnoMod. This dataset enables (1) evaluation of cross-modal anomaly detection and fusion/ablation strategies, and (2) fine-grained RCA studies across service and code regions, supporting end-to-end troubleshooting pipelines that jointly consider detection and localization.

</details>


### [73] [A Serverless Edge-Native Data Processing Architecture for Autonomous Driving Training](https://arxiv.org/abs/2601.22919)
*Fabian Bally,Michael Schötz,Thomas Limbrunner*

Main category: cs.SE

TL;DR: Lambda框架是一个边缘原生平台，通过用户定义函数实现车载数据过滤和处理，将FaaS原则适配到资源受限的汽车环境中，支持实时数据处理。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶机器学习需要大量传感器数据，特别是安全关键场景的平衡覆盖，但捕获这些事件需要大量驾驶时间和高效选择，现有系统存在瓶颈。

Method: 引入Lambda框架，提供无服务器风格的抽象层，分离应用逻辑与底层执行关注点，适配FaaS原则到汽车环境，支持模块化、事件驱动的过滤算法，兼容ROS 2和现有数据记录管道。

Result: 在NVIDIA Jetson Orin Nano上评估，相比原生ROS 2部署，表现出竞争性性能、降低的延迟和抖动，证实lambda抽象能支持嵌入式自动驾驶系统的实时数据处理。

Conclusion: Lambda框架成功将FaaS原则引入资源受限的汽车环境，支持实时数据过滤和处理，为自动驾驶数据采集提供了高效解决方案。

Abstract: Data is both the key enabler and a major bottleneck for machine learning in autonomous driving. Effective model training requires not only large quantities of sensor data but also balanced coverage that includes rare yet safety-critical scenarios. Capturing such events demands extensive driving time and efficient selection. This paper introduces the Lambda framework, an edge-native platform that enables on-vehicle data filtering and processing through user-defined functions. The framework provides a serverless-inspired abstraction layer that separates application logic from low-level execution concerns such as scheduling, deployment, and isolation. By adapting Function-as-a-Service (FaaS) principles to resource-constrained automotive environments, it allows developers to implement modular, event-driven filtering algorithms while maintaining compatibility with ROS 2 and existing data recording pipelines. We evaluate the framework on an NVIDIA Jetson Orin Nano and compare it against native ROS 2 deployments. Results show competitive performance, reduced latency and jitter, and confirm that lambda-based abstractions can support real-time data processing in embedded autonomous driving systems. The source code is available at https://github.com/LASFAS/jblambda.

</details>


### [74] [Sifting the Noise: A Comparative Study of LLM Agents in Vulnerability False Positive Filtering](https://arxiv.org/abs/2601.22952)
*Yunpeng Xiong,Ting Zhang*

Main category: cs.SE

TL;DR: LLM智能体在SAST误报过滤中的比较研究：Aider、OpenHands和SWE-agent三种框架在OWASP基准和真实Java项目上的评估显示，最佳配置可将误报率从92%降至6.3%，但效果受骨干模型、漏洞类型和计算成本影响显著。


<details>
  <summary>Details</summary>
Motivation: SAST工具产生大量误报，给开发者带来繁重的手动排查负担。虽然LLM智能体通过迭代推理、工具使用和环境交互为SAST误报过滤提供了新方向，但不同LLM智能体架构在误报过滤中的比较效果尚不清楚。

Method: 比较研究三种最先进的LLM智能体框架（Aider、OpenHands和SWE-agent）在漏洞误报过滤中的表现。使用OWASP基准和真实开源Java项目中的漏洞进行评估，分析不同配置下智能体框架的性能差异。

Result: LLM智能体能显著减少SAST噪声：在OWASP基准上，最佳配置可将初始92%以上的误报率降至6.3%；在真实数据集上，对CodeQL警报的误报识别率可达93.3%。但效果高度依赖骨干模型和CWE类型：智能体框架对Claude Sonnet 4和GPT-5等强模型效果显著，对弱模型提升有限；激进的误报减少可能抑制真实漏洞；不同框架的计算成本差异很大。

Conclusion: LLM智能体是SAST误报过滤的强大但非均匀解决方案，实际部署需要仔细考虑智能体设计、骨干模型选择、漏洞类别和运营成本之间的权衡。

Abstract: Static Application Security Testing (SAST) tools are essential for identifying software vulnerabilities, but they often produce a high volume of false positives (FPs), imposing a substantial manual triage burden on developers. Recent advances in Large Language Model (LLM) agents offer a promising direction by enabling iterative reasoning, tool use, and environment interaction to refine SAST alerts. However, the comparative effectiveness of different LLM-based agent architectures for FP filtering remains poorly understood. In this paper, we present a comparative study of three state-of-the-art LLM-based agent frameworks, i.e., Aider, OpenHands, and SWE-agent, for vulnerability FP filtering. We evaluate these frameworks using the vulnerabilities from the OWASP Benchmark and real-world open-source Java projects. The experimental results show that LLM-based agents can remove the majority of SAST noise, reducing an initial FP detection rate of over 92% on the OWASP Benchmark to as low as 6.3% in the best configuration. On real-world dataset, the best configuration of LLM-based agents can achieve an FP identification rate of up to 93.3% involving CodeQL alerts. However, the benefits of agents are strongly backbone- and CWE-dependent: agentic frameworks significantly outperform vanilla prompting for stronger models such as Claude Sonnet 4 and GPT-5, but yield limited or inconsistent gains for weaker backbones. Moreover, aggressive FP reduction can come at the cost of suppressing true vulnerabilities, highlighting important trade-offs. Finally, we observe large disparities in computational cost across agent frameworks. Overall, our study demonstrates that LLM-based agents are a powerful but non-uniform solution for SAST FP filtering, and that their practical deployment requires careful consideration of agent design, backbone model choice, vulnerability category, and operational cost.

</details>


### [75] [SWE-Manager: Selecting and Synthesizing Golden Proposals Before Coding](https://arxiv.org/abs/2601.22956)
*Boyin Tan,Haoning Deng,Junyuan Zhang,Junjielong Xu,Pinjia He,Youcheng Sun*

Main category: cs.SE

TL;DR: SWE-Manager：一个通过强化学习训练的8B模型，用于在软件工程问题解决中比较多个修复提案、选择最佳方案并合成最终实施方案


<details>
  <summary>Details</summary>
Motivation: 当前LLM在软件工程中的研究主要集中在代码生成和错误修复，但实际团队工作中需要从多个候选提案中选择最佳方案。好的选择能提高问题解决的可靠性并降低风险，而差的选择会增加风险甚至导致不可预测的故障。

Method: 首先通过人工研究真实世界问题来理解维护者选择提案的理性依据。然后引入SWE-Manager，这是一个通过强化学习训练的8B模型，能够比较提案、证明选择合理性并合成最终实施方案。将提案选择视为推理任务，模拟技术经理在不执行代码或运行测试的情况下权衡问题上下文和每个提案解决方案的过程。

Result: 在SWE-Lancer Manager基准测试中，SWE-Manager实现了53.21%的选择准确率和57.75%的收益率，获得了152,750美元的收益，表现优于包括GPT-5在内的强基线模型。还设计了P2A框架来模拟真实世界工作流程以进一步评估模型效果。

Conclusion: SWE-Manager通过将提案选择视为推理任务，有效模拟了技术经理的决策过程，在软件工程问题解决中能够选择最佳提案并合成实施方案，显著优于现有方法。

Abstract: Large language model (LLM) research in software engineering has largely focused on tasks such as code generation and bug repair. In practice, teams often draft multiple candidate proposals for fixing an issue and then deliberate on one golden proposal for implementation. This selection requires not only assessing the issue's scope, impact, and urgency, but also a clear understanding of each proposal's strengths and weaknesses. A good selection could make issue resolution more reliable while reducing regression and operational risk, whereas a poor choice can increase risk and even cause unpredictable failures.
  We first conduct a manual study of real-world issues to characterize the rationales maintainers use when selecting among competing proposals. Motivated by these findings, we introduce SWE-Manager, a joint selection and synthesis approach that selects the best proposal and synthesizes a golden proposal. SWE-Manager is an 8B model trained via reinforcement learning (RL) to compare proposals, justify its choice, and synthesize a golden proposal for implementation. We view proposal selection as a reasoning task, mirroring how technical managers review competing proposals by weighing issue context and each proposal's solution without executing code or running tests. On the SWE-Lancer Manager benchmark, SWE-Manager achieves 53.21 selection accuracy and 57.75 earn rate, earning 152,750 dollars and outperforming strong baselines including GPT-5. To further evaluate the effectiveness of SWE-Manager in real-world issue resolution, we design the P2A framework, which simulates a real-world workflow where multiple proposals are drafted, reviewed, and a golden proposal is selected for implementation ...

</details>


### [76] [Uncovering Hidden Inclusions of Vulnerable Dependencies in Real-World Java Projects](https://arxiv.org/abs/2601.23020)
*Stefan Schott,Serena Elisa Ponta,Wolfram Fischer,Jonas Klauke,Eric Bodden*

Main category: cs.SE

TL;DR: Unshade是一个混合依赖扫描工具，结合元数据扫描效率和代码中心方法检测修改依赖的能力，在Java项目中识别隐藏的易受攻击依赖。


<details>
  <summary>Details</summary>
Motivation: 开源软件依赖是现代代码库的主要组成部分，虽然能减少开发时间和成本，但也引入了显著的安全风险，包括已知漏洞的引入。现有元数据扫描器轻量快速但无法检测修改依赖，而代码中心扫描器能检测修改依赖但效率较低。

Method: Unshade采用混合方法：首先通过基于字节码的指纹识别机制识别修改和隐藏的依赖，增强Java项目的软件物料清单(SBOM)，然后将增强的SBOM传递给元数据漏洞扫描器，识别声明依赖和新发现依赖中的已知漏洞。

Result: 对GitHub上1,808个最流行的开源Java Maven项目进行大规模研究，结果显示：近50%项目包含至少一个与已知漏洞相关的修改隐藏依赖；平均每个受影响项目包含超过8个此类隐藏易受攻击依赖；共识别出7,712个独特CVE，这些在仅依赖元数据扫描时会被遗漏。

Conclusion: Unshade成功结合了元数据扫描的效率和代码中心方法的检测能力，显著提高了对隐藏易受攻击依赖的检测率，揭示了传统扫描方法遗漏的大量安全风险。

Abstract: Open-source software (OSS) dependencies are a dominant component of modern software code bases. Using proven and well-tested OSS components lets developers reduce development time and cost while improving quality. However, heavy reliance on open-source software also introduces significant security risks, including the incorporation of known vulnerabilities into the codebase. To mitigate these risks, metadata-based dependency scanners, which are lightweight and fast, and code-centric scanners, which enable the detection of modified dependencies hidden from metadata-based approaches, have been developed. In this paper, we present Unshade, a hybrid approach towards dependency scanning in Java that combines the efficiency of metadata-based scanning with the ability to detect modified dependencies of code-centric approaches. Unshade first augments a Java project's software bill of materials (SBOM) by identifying modified and hidden dependencies via a bytecode-based fingerprinting mechanism. This augmented SBOM is then passed to a metadata-based vulnerability scanner to identify known vulnerabilities in both declared and newly revealed dependencies. Leveraging Unshade's high scalability, we conducted a large-scale study of the 1,808 most popular open-source Java Maven projects on GitHub. The results show that nearly 50% of these projects contain at least one modified, hidden dependency associated with a known vulnerability. On average, each affected project includes more than eight such hidden vulnerable dependencies, all missed by traditional metadata-based scanners. Overall, Unshade identified 7,712 unique CVEs in hidden dependencies that would remain undetected when relying on metadata-based scanning alone.

</details>


### [77] [On the Impact of Code Comments for Automated Bug-Fixing: An Empirical Study](https://arxiv.org/abs/2601.23059)
*Antonio Vitale,Emanuela Guglielmi,Simone Scalabrino,Rocco Oliveto*

Main category: cs.SE

TL;DR: 研究探讨了代码注释对大型语言模型自动修复bug能力的影响，发现注释在训练和推理阶段都能显著提升修复准确率


<details>
  <summary>Details</summary>
Motivation: 当前自动bug修复研究中普遍存在移除代码注释的做法，但研究者假设注释可能包含重要的设计和实现信息，对修复某些类型的bug至关重要

Method: 通过实证评估比较两个模型家族，在训练和推理阶段的所有组合条件下（有/无注释）测试模型性能；使用LLM为缺少注释的数据集自动生成注释

Result: 当注释同时存在于训练和推理阶段时，自动bug修复准确率最多可提高三倍；训练时包含注释不会降低无注释实例的性能；详细描述方法实现的注释对帮助LLM准确修复bug特别有效

Conclusion: 代码注释在自动bug修复中具有重要价值，不应在预处理中随意移除；包含注释的训练策略能提升模型性能且无负面影响

Abstract: Large Language Models (LLMs) are increasingly relevant in Software Engineering research and practice, with Automated Bug Fixing (ABF) being one of their key applications. ABF involves transforming a buggy method into its fixed equivalent. A common preprocessing step in ABF involves removing comments from code prior to training. However, we hypothesize that comments may play a critical role in fixing certain types of bugs by providing valuable design and implementation insights. In this study, we investigate how the presence or absence of comments, both during training and at inference time, impacts the bug-fixing capabilities of LLMs. We conduct an empirical evaluation comparing two model families, each evaluated under all combinations of training and inference conditions (with and without comments), and thereby revisiting the common practice of removing comments during training. To address the limited availability of comments in state-of-the-art datasets, we use an LLM to automatically generate comments for methods lacking them. Our findings show that comments improve ABF accuracy by up to threefold when present in both phases, while training with comments does not degrade performance when instances lack them. Additionally, an interpretability analysis identifies that comments detailing method implementation are particularly effective in aiding LLMs to fix bugs accurately.

</details>


### [78] [Automated Testing of Prevalent 3D User Interactions in Virtual Reality Applications](https://arxiv.org/abs/2601.23139)
*Ruizhen Gu,José Miguel Rojas,Donghwan Shin*

Main category: cs.SE

TL;DR: 该论文针对VR应用测试的挑战，提出了Interaction Flow Graph抽象模型、XRBench3D基准测试集和XRintTest自动化测试方法，显著提升了VR交互测试的覆盖率和效率。


<details>
  <summary>Details</summary>
Motivation: VR技术虽然提供沉浸式用户体验，但与传统软件相比存在独特的测试挑战。现有VR测试方法缺乏自动合成真实3D用户输入（如通过手持控制器的抓取和触发动作）的能力，且现有指标无法稳健捕捉多样化的交互覆盖率。

Method: 1. 通过实证研究识别了9个开源VR项目中四种主要交互类型：fire、manipulate、socket和custom；2. 提出了Interaction Flow Graph，通过识别目标、动作和条件来系统建模3D用户交互；3. 构建了XRBench3D基准测试集，包含10个VR场景和456个不同用户交互；4. 开发了XRintTest自动化测试方法，利用该图进行动态场景探索和交互执行。

Result: 在XRBench3D上的评估显示，XRintTest在所有场景中对fire、manipulate和socket交互的覆盖率达到93%，比随机探索方法效果提升12倍，效率提升6倍。此外，XRintTest能够检测运行时异常和非异常交互问题，包括微妙的配置缺陷。Interaction Flow Graph还能揭示可能损害预期功能和阻碍测试性能的交互设计问题。

Conclusion: 该研究通过系统化的交互建模和自动化测试方法，有效解决了VR应用测试中的关键挑战，显著提升了测试覆盖率和效率，并为VR交互设计和测试提供了有价值的工具和基准。

Abstract: Virtual Reality (VR) technologies offer immersive user experiences across various domains, but present unique testing challenges compared to traditional software. Existing VR testing approaches enable scene navigation and interaction activation, but lack the ability to automatically synthesise realistic 3D user inputs (e.g, grab and trigger actions via hand-held controllers). Automated testing that generates and executes such input remains an unresolved challenge. Furthermore, existing metrics fail to robustly capture diverse interaction coverage. This paper addresses these gaps through four key contributions. First, we empirically identify four prevalent interaction types in nine open-source VR projects: fire, manipulate, socket, and custom. Second, we introduce the Interaction Flow Graph, a novel abstraction that systematically models 3D user interactions by identifying targets, actions, and conditions. Third, we construct XRBench3D, a benchmark comprising ten VR scenes that encompass 456 distinct user interactions for evaluating VR interaction testing. Finally, we present XRintTest, an automated testing approach that leverages this graph for dynamic scene exploration and interaction execution. Evaluation on XRBench3D shows that XRintTest achieves great effectiveness, reaching 93% coverage of fire, manipulate and socket interactions across all scenes, and performing 12x more effectively and 6x more efficiently than random exploration. Moreover, XRintTest can detect runtime exceptions and non-exception interaction issues, including subtle configuration defects. In addition, the Interaction Flow Graph can reveal potential interaction design smells that may compromise intended functionality and hinder testing performance for VR applications.

</details>


### [79] [Do Good, Stay Longer? Temporal Patterns and Predictors of Newcomer-to-Core Transitions in Conventional OSS and OSS4SG](https://arxiv.org/abs/2601.23142)
*Mohamed Ouf,Amr Mohamed,Mariam Guizani*

Main category: cs.SE

TL;DR: OSS4SG项目相比传统OSS项目，新人向核心贡献者转化的成功率更高（2.2倍留存率，19.6%更高核心地位概率），且提供多种转化路径而非单一主导路径。研究发现，先花时间了解项目再集中贡献的模式（Late Spike）比一开始就高强度贡献的模式（Early Spike）更快成为核心（21周 vs 51-60周）。


<details>
  <summary>Details</summary>
Motivation: 开源软件的可持续性依赖于新人向核心贡献者的转化，但这一转化管道存在问题，大多数新人在初步贡献后变得不活跃。研究旨在比较传统OSS项目和以社会影响为使命的OSS4SG项目在新人向核心转化方面的差异，探索更有效的转化策略。

Method: 研究分析了375个项目（190个OSS4SG，185个传统OSS），涵盖92,721名贡献者和350万次提交。通过对比分析留存率、核心地位概率、转化路径模式和时间模式，识别影响新人向核心转化的关键因素。

Result: OSS4SG项目比传统OSS项目有2.2倍更高的贡献者留存率和19.6%更高的核心地位概率。传统OSS项目主要依赖单一主导转化路径（61.62%），而OSS4SG提供多种路径。研究发现Late Spike模式（先学习再集中贡献）比Early Spike模式（从一开始就高强度贡献）快2.4-2.9倍成为核心（21周 vs 51-60周）。

Conclusion: 项目使命与个人价值观的契合以及先花时间了解代码库再进行主要贡献是成为核心贡献者的关键策略。OSS4SG项目创造了更有利于新人向核心转化的环境，为新人、维护者和项目管理者提供了基于证据的指导。

Abstract: Open Source Software (OSS) sustainability relies on newcomers transitioning to core contributors, but this pipeline is broken, with most newcomers becoming inactive after initial contributions. Open Source Software for Social Good (OSS4SG) projects, which prioritize societal impact as their primary mission, may be associated with different newcomer-to-core transition outcomes than conventional OSS projects. We compared 375 projects (190 OSS4SG, 185 OSS), analyzing 92,721 contributors and 3.5 million commits. OSS4SG projects retain contributors at 2.2X higher rates and contributors have 19.6% higher probability of achieving core status. Early broad project exploration predicts core achievement (22.2% importance); conventional OSS concentrates on one dominant pathway (61.62% of transitions) while OSS4SG provides multiple pathways. Contrary to intuition, contributors who invest time learning the project before intensifying their contributions (Late Spike pattern) achieve core status 2.4-2.9X faster (21 weeks) than those who contribute intensively from day one (Early Spike pattern, 51-60 weeks). OSS4SG supports two effective temporal patterns while only Late Spike achieves fastest time-to-core in conventional OSS. Our findings suggest that finding a project aligned with personal values and taking time to understand the codebase before major contributions are key strategies for achieving core status. Our findings show that project mission is associated with measurably different environments for newcomer-to-core transitions and provide evidence-based guidance for newcomers and maintainers.

</details>


### [80] [GrepRAG: An Empirical Study and Optimization of Grep-Like Retrieval for Code Completion](https://arxiv.org/abs/2601.23254)
*Baoyi Wang,Xingliang Wang,Guochang Li,Chen Zhi,Junxiao Han,Xinkui Zhao,Nan Wang,Shuiguang Deng,Jianwei Yin*

Main category: cs.SE

TL;DR: 本文提出GrepRAG，一种轻量级、无需索引的词汇检索方法，用于仓库级代码补全，通过简单的grep命令检索和轻量级后处理，性能超越现有复杂方法。


<details>
  <summary>Details</summary>
Motivation: 现有仓库级代码补全方法依赖复杂的语义索引或图分析，计算开销大。受开发者常用轻量级搜索工具启发，探索简单词汇检索在代码补全中的潜力。

Method: 1. 提出Naive GrepRAG基线：LLM自主生成ripgrep命令检索相关上下文；2. 改进为GrepRAG：增加标识符加权重排序和结构感知去重后处理管道。

Result: Naive GrepRAG性能已接近复杂图基线；GrepRAG在CrossCodeEval和RepoEval-Updated上持续超越SOTA方法，在CrossCodeEval上代码精确匹配相对提升7.04-15.58%。

Conclusion: 简单词汇检索结合轻量级后处理能有效支持仓库级代码补全，无需复杂索引机制，为代码补全系统提供了高效实用的解决方案。

Abstract: Repository-level code completion remains challenging for large language models (LLMs) due to cross-file dependencies and limited context windows. Prior work addresses this challenge using Retrieval-Augmented Generation (RAG) frameworks based on semantic indexing or structure-aware graph analysis, but these approaches incur substantial computational overhead for index construction and maintenance. Motivated by common developer workflows that rely on lightweight search utilities (e.g., ripgrep), we revisit a fundamental yet underexplored question: how far can simple, index-free lexical retrieval support repository-level code completion before more complex retrieval mechanisms become necessary? To answer this question, we systematically investigate lightweight, index-free, intent-aware lexical retrieval through extensive empirical analysis. We first introduce Naive GrepRAG, a baseline framework in which LLMs autonomously generate ripgrep commands to retrieve relevant context. Despite its simplicity, Naive GrepRAG achieves performance comparable to sophisticated graph-based baselines. Further analysis shows that its effectiveness stems from retrieving lexically precise code fragments that are spatially closer to the completion site. We also identify key limitations of lexical retrieval, including sensitivity to noisy matches from high-frequency ambiguous keywords and context fragmentation caused by rigid truncation boundaries. To address these issues, we propose GrepRAG, which augments lexical retrieval with a lightweight post-processing pipeline featuring identifier-weighted re-ranking and structure-aware deduplication. Extensive evaluation on CrossCodeEval and RepoEval-Updated demonstrates that GrepRAG consistently outperforms state-of-the-art (SOTA) methods, achieving 7.04-15.58 percent relative improvement in code exact match (EM) over the best baseline on CrossCodeEval.

</details>


### [81] [Outcome-Conditioned Reasoning Distillation for Resolving Software Issues](https://arxiv.org/abs/2601.23257)
*Chenglin Li,Yisen Xu,Zehao Wang,Shin Hwei Tan,Tse-Hsun,Chen*

Main category: cs.SE

TL;DR: O-CRD框架利用已解决的代码库问题作为监督，通过从已验证补丁反向重构修复轨迹，在推理时重用这些指导来引导问题定位和补丁生成，无需微调或在线搜索，显著提升软件问题修复性能。


<details>
  <summary>Details</summary>
Motivation: 现有LLM修复管道通常以重置和解决的方式操作，为每个新问题生成新的推理，而不是利用过去修复中的经验。这很浪费，因为代码库中通常包含具有重叠结构、故障模式或约束的早期问题，先前的修复经验可以提供有用的指导。

Method: 提出结果条件推理蒸馏(O-CRD)框架，使用已解决的代码库问题和已验证补丁作为监督。从历史修复开始，方法从已验证结果向后重构分阶段的修复轨迹，然后在推理时重用蒸馏的指导来引导文件/函数定位和补丁合成，无需微调或在线搜索。

Result: 在SWE-Bench Lite上，该方法使GPT-4o的Pass@1提高了10.4%，DeepSeek-V3提高了8.6%，GPT-5提高了10.3%，表明结果条件的已验证修复重用可以替代昂贵的正向探索。

Conclusion: 结果条件的已验证修复重用可以有效替代昂贵的正向探索，为软件问题解决提供更高效的方法，显著提升大型代码库中软件问题修复的性能。

Abstract: Software issue resolution in large repositories is a long-range decision process: choices made during localization shape the space of viable edits, and missteps can compound into incorrect patches. Despite this, many LLM-based repair pipelines still operate in a reset-and-solve manner, producing fresh reasoning for every new issue instead of carrying forward what worked in past fixes. This is wasteful because repositories routinely contain earlier issues with overlapping structure, failure modes, or constraints, where prior repair experience could provide useful guidance. Existing approaches typically harvest this signal through forward-time trial procedures, such as repeated refinement or search, incurring high inference cost while still risking divergence from the eventual correct patch. We present an Outcome-Conditioned Reasoning Distillation(O-CRD) framework that uses resolved in-repository issues with verified patches as supervision. Starting from a historical fix, the method reconstructs a stage-wise repair trace backward from the verified outcome, then reuses the distilled guidance at inference time to steer file/function localization and patch synthesis, without fine-tuning or online search. On SWE-Bench Lite, this approach increases Pass@1 by 10.4% with GPT-4o, 8.6% with DeepSeek-V3, and 10.3% with GPT-5, indicating that outcome-conditioned reuse of verified repairs can replace costly forward exploration for software issue resolution.

</details>
