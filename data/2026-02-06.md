<div id=toc></div>

# Table of Contents

- [cs.CR](#cs.CR) [Total: 6]
- [cs.SE](#cs.SE) [Total: 14]
- [cs.AI](#cs.AI) [Total: 20]


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [1] [Boost+: Equitable, Incentive-Compatible Block Building](https://arxiv.org/abs/2602.04007)
*Mengqian Zhang,Sen Yang,Kartik Nayak,Fan Zhang*

Main category: cs.CR

TL;DR: Boost+ 是一个去中心化的区块构建系统，通过将交易收集与排序分离来确保公平访问，解决 MEV-Boost 的中心化问题，提高区块空间效率。


<details>
  <summary>Details</summary>
Motivation: 以太坊当前的区块构建生态系统 MEV-Boost 由于集成而高度中心化，扭曲了竞争、降低了区块空间效率，并模糊了 MEV 流的透明度。需要保证区块构建的公平性和经济效率。

Method: 提出 Boost+ 系统，将区块构建过程解耦为交易收集和排序两个阶段。核心是 M_Boost+ 机制，围绕默认算法构建，确保所有收集到的交易都有平等访问权。机制设计使诚实出价成为所有构建者的主导策略，对于搜索者，在默认算法主导竞争构建者时，诚实报告是主导策略。

Result: M_Boost+ 机制确保：1) 诚实出价是所有构建者的主导策略；2) 对于搜索者，当默认算法主导竞争构建者时，诚实报告是主导策略；3) 对于所有无冲突交易，即使构建者可能获胜，诚实报告仍是主导策略；4) 即使搜索者技术上能与构建者集成，对于无冲突交易，非集成结合诚实出价仍优于任何偏离策略。

Conclusion: Boost+ 通过解耦交易收集和排序，解决了 MEV-Boost 的中心化问题，确保了区块构建的公平性和经济效率。基于真实交易数据实证分析的具体默认算法实施，进一步验证了该系统的有效性。

Abstract: Block space on the blockchain is scarce and must be allocated efficiently through block building. However, Ethereum's current block-building ecosystem, MEV-Boost, has become highly centralized due to integration, which distorts competition, reduces blockspace efficiency, and obscures MEV flow transparency. To guarantee equitability and economic efficiency in block building, we propose $\mathrm{Boost+}$, a system that decouples the process into collecting and ordering transactions, and ensures equal access to all collected transactions.
  The core of $\mathrm{Boost+}$ is the mechanism $\mathit{M}_{\mathrm{Boost+}}$, built around a default algorithm. $\mathit{M}_{\mathrm{Boost+}}$ aligns incentives for both searchers (intermediaries that generate or route transactions) and builders: Truthful bidding is a dominant strategy for all builders. For searchers, truthful reporting is dominant whenever the default algorithm dominates competing builders, and it remains dominant for all conflict-free transactions, even when builders may win. We further show that even if a searcher can technically integrate with a builder, non-integration combined with truthful bidding still dominates any deviation for conflict-free transactions. We also implement a concrete default algorithm informed by empirical analysis of real-world transactions and evaluate its efficacy using historical transaction data.

</details>


### [2] [Evaluating the Vulnerability Landscape of LLM-Generated Smart Contracts](https://arxiv.org/abs/2602.04039)
*Hoang Long Do,Nasrin Sohrabi,Muneeb Ul Hassan*

Main category: cs.CR

TL;DR: 对ChatGPT、Gemini和Sonnet等先进大语言模型生成的Solidity智能合约进行系统性安全分析，发现尽管这些合约语法正确、功能完整，但经常存在严重安全漏洞，不适合直接部署到生产环境。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型在智能合约开发中的广泛应用，由于智能合约部署后不可修改的特性，其正确性和安全性至关重要。然而，目前对LLM生成的智能合约的安全影响了解不足，特别是在金融和治理等高风险领域。

Method: 对ChatGPT、Gemini和Sonnet等先进LLM生成的Solidity智能合约进行系统性安全分析，针对广泛的已知智能合约漏洞进行评估，分析漏洞模式并进行分类。

Result: 研究发现，尽管LLM生成的智能合约语法正确且功能完整，但经常表现出严重的安全缺陷，这些缺陷可能在现实环境中被利用。研究识别了不同模型间重复出现的弱点模式。

Conclusion: 提出了实用的对策和开发指南来减轻这些风险，为开发者和研究人员提供可操作的见解，旨在支持LLM安全集成到智能合约开发工作流程中，并加强区块链生态系统对未来安全故障的整体安全性。

Abstract: Large language models (LLMs) have been widely adopted in modern software development lifecycles, where they are increasingly used to automate and assist code generation, significantly improving developer productivity and reducing development time. In the blockchain domain, developers increasingly rely on LLMs to generate and maintain smart contracts, the immutable, self-executing components of decentralized applications. Because deployed smart contracts cannot be modified, correctness and security are paramount, particularly in high-stakes domains such as finance and governance. Despite this growing reliance, the security implications of LLM-generated smart contracts remain insufficiently understood.
  In this work, we conduct a systematic security analysis of Solidity smart contracts generated by state-of-the-art LLMs, including ChatGPT, Gemini, and Sonnet. We evaluate these contracts against a broad set of known smart contract vulnerabilities to assess their suitability for direct deployment in production environments. Our extensive experimental study shows that, despite their syntactic correctness and functional completeness, LLM-generated smart contracts frequently exhibit severe security flaws that could be exploited in real-world settings. We further analyze and categorize these vulnerabilities, identifying recurring weakness patterns across different models. Finally, we discuss practical countermeasures and development guidelines to help mitigate these risks, offering actionable insights for both developers and researchers. Our findings aim to support safe integration of LLMs into smart contract development workflows and to strengthen the overall security of the blockchain ecosystem against future security failures.

</details>


### [3] [ZKBoost: Zero-Knowledge Verifiable Training for XGBoost](https://arxiv.org/abs/2602.04113)
*Nikolas Melissaris,Jiayi Xu,Antigoni Polychroniadou,Akira Takahashi,Chenkai Weng*

Main category: cs.CR

TL;DR: ZKBoost是首个用于XGBoost的零知识训练证明协议，允许模型所有者在承诺的数据集上证明正确训练，同时不泄露数据或参数。


<details>
  <summary>Details</summary>
Motivation: 随着梯度提升决策树（特别是XGBoost）在敏感场景中的部署增加，需要密码学保证模型完整性，确保训练过程正确且可验证。

Method: 提出三个关键贡献：1) 兼容算术电路的定点XGBoost实现；2) 适用于任何通用ZKP后端的XGBoost zkPoT通用模板；3) 基于VOLE的实例化解决非线性定点操作证明挑战。

Result: 定点实现与标准XGBoost精度差异在1%以内，能够在真实数据集上实现实用的零知识训练证明。

Conclusion: ZKBoost为XGBoost模型提供了首个零知识训练证明解决方案，在保持模型准确性的同时实现了密码学可验证的训练完整性。

Abstract: Gradient boosted decision trees, particularly XGBoost, are among the most effective methods for tabular data. As deployment in sensitive settings increases, cryptographic guarantees of model integrity become essential. We present ZKBoost, the first zero-knowledge proof of training (zkPoT) protocol for XGBoost, enabling model owners to prove correct training on a committed dataset without revealing data or parameters. We make three key contributions: (1) a fixed-point XGBoost implementation compatible with arithmetic circuits, enabling instantiation of efficient zkPoT, (2) a generic template of zkPoT for XGBoost, which can be instantiated with any general-purpose ZKP backend, and (3) vector oblivious linear evaluation (VOLE)-based instantiation resolving challenges in proving nonlinear fixed-point operations. Our fixed-point implementation matches standard XGBoost accuracy within 1\% while enabling practical zkPoT on real-world datasets.

</details>


### [4] [Post-Quantum Identity-Based TLS for 5G Service-Based Architecture and Cloud-Native Infrastructure](https://arxiv.org/abs/2602.04238)
*Vipin Kumar Rathi,Lakshya Chopra,Nikhil Kumar Rajput*

Main category: cs.CR

TL;DR: 提出基于后量子身份基加密的无证书认证框架，替代传统PKI和证书验证，用于私有分布式系统、云原生平台和5G核心网


<details>
  <summary>Details</summary>
Motivation: 云原生平台和5G核心网等延迟敏感系统依赖基于证书的PKI和mTLS进行服务间通信安全，但该模型带来显著的操作和性能开销，在后量子场景下由于大证书和昂贵的签名验证而进一步放大

Method: 设计基于后量子身份基加密的无证书认证框架，用身份派生密钥和基于身份的密钥封装替代证书和基于签名的认证，实现无需证书传输或验证的相互认证TLS连接；包括基于阈值私钥生成器的私有PKI替代方案

Result: 将框架应用于云原生应用部署和5G核心网，展示身份基TLS如何与5G服务架构集成并保持安全语义和3GPP要求，以及相同架构如何在不破坏现有信任域或部署模型的情况下替代Kubernetes中的私有PKI

Conclusion: 提出的基于后量子身份基加密的无证书认证框架能够有效解决传统PKI在后量子环境下的性能和操作开销问题，适用于云原生平台和5G核心网等关键基础设施

Abstract: Cloud-native application platforms and latency-sensitive systems such as 5G Core networks rely heavily on certificate-based Public Key Infrastructure (PKI) and mutual TLS to secure service-to-service communication. While effective, this model introduces significant operational and performance overhead, which is further amplified in the post-quantum setting due to large certificates and expensive signature verification. In this paper, we present a certificate-free authentication framework for private distributed systems based on post-quantum Identity-Based Encryption(IBE). Our design replaces certificate and signature based authentication with identity-derived keys and identity-based key encapsulation, enabling mutually authenticated TLS connections without certificate transmission or validation. We describe an IBE-based replacement for private PKI, including identity lifecycle management, and show how it can be instantiated using a threshold Private Key Generator (T-PKG). We apply this framework to cloud-native application deployments and latency-sensitive 5G Core networks. In particular, we demonstrate how identity-based TLS integrates with the 5G Service-Based Architecture while preserving security semantics and 3GPP requirements, and we show how the same architecture can replace private PKI in Kubernetes, including its control plane, without disrupting existing trust domains or deployment models.

</details>


### [5] [Optimal conversion from Rényi Differential Privacy to $f$-Differential Privacy](https://arxiv.org/abs/2602.04562)
*Anneliese Riess,Juan Felipe Gomez,Flavio du Pin Calmon,Julia Anne Schnabel,Georgios Kaissis*

Main category: cs.CR

TL;DR: 该论文证明了将Rényi差分隐私(RDP)配置文件转换为有效假设检验权衡函数的最优转换规则是基于单阶RDP隐私区域交集的方法。


<details>
  <summary>Details</summary>
Motivation: 动机是解决Zhu等人(2022)附录F.3中提出的猜想：在所有将RDP配置文件映射到有效假设检验权衡函数的转换规则中，基于单阶RDP隐私区域交集的规则是否最优。

Method: 方法是通过精确的几何特征分析RDP隐私区域，利用其凸性以及边界完全由伯努利机制决定的性质。证明在权衡函数空间中，最紧的边界是$f_{ρ(\cdot)}(α) = \sup_{τ\geq 0.5} f_{τ,ρ(τ)}(α)$，即每个RDP隐私区域单阶边界的逐点最大值。

Result: 结果表明，基于RDP隐私区域交集的转换规则不仅是有效的，而且是最优的：在Blackwell意义上，没有其他黑盒转换能一致地优于它。这标志着仅从RDP保证推断机制隐私性的基本极限。

Conclusion: 结论是证明了Zhu等人(2022)的猜想，统一并强化了Balle等人(2019)、Asoodeh等人(2021)和Zhu等人(2022)的见解，确立了RDP到假设检验权衡函数转换的基本最优性界限。

Abstract: We prove the conjecture stated in Appendix F.3 of [Zhu et al. (2022)]: among all conversion rules that map a Rényi Differential Privacy (RDP) profile $τ\mapsto ρ(τ)$ to a valid hypothesis-testing trade-off $f$, the rule based on the intersection of single-order RDP privacy regions is optimal. This optimality holds simultaneously for all valid RDP profiles and for all Type I error levels $α$. Concretely, we show that in the space of trade-off functions, the tightest possible bound is $f_{ρ(\cdot)}(α) = \sup_{τ\geq 0.5} f_{τ,ρ(τ)}(α)$: the pointwise maximum of the single-order bounds for each RDP privacy region. Our proof unifies and sharpens the insights of [Balle et al. (2019)], [Asoodeh et al. (2021)], and [Zhu et al. (2022)]. Our analysis relies on a precise geometric characterization of the RDP privacy region, leveraging its convexity and the fact that its boundary is determined exclusively by Bernoulli mechanisms. Our results establish that the "intersection-of-RDP-privacy-regions" rule is not only valid, but optimal: no other black-box conversion can uniformly dominate it in the Blackwell sense, marking the fundamental limit of what can be inferred about a mechanism's privacy solely from its RDP guarantees.

</details>


### [6] [Inference-Time Backdoors via Hidden Instructions in LLM Chat Templates](https://arxiv.org/abs/2602.04653)
*Ariel Fogel,Omer Hofman,Eilon Cohen,Roman Vainshtein*

Main category: cs.CR

TL;DR: 攻击者通过修改聊天模板而非模型权重或训练数据，在开源语言模型中植入推理时后门，该攻击在触发条件下能显著降低事实准确性并诱导输出攻击者控制的URL，且能逃避现有安全扫描


<details>
  <summary>Details</summary>
Motivation: 开源语言模型在生产环境中的使用日益增多，带来了新的安全挑战。传统的后门攻击通常假设攻击者能访问训练管道或部署基础设施，但本文提出了一种新的攻击面，利用聊天模板这一特权位置实施攻击，无需修改模型权重、污染训练数据或控制运行时基础设施。

Method: 攻击者通过分发带有恶意修改聊天模板的模型来实施推理时后门攻击。聊天模板是每次推理调用时执行的Jinja2程序，位于用户输入和模型处理之间的特权位置。研究构建了针对两个目标的模板后门：降低事实准确性和诱导输出攻击者控制的URL，并在18个模型、7个模型家族和4个推理引擎上进行了评估。

Result: 在触发条件下，事实准确性从90%平均下降到15%，攻击者控制的URL输出成功率超过80%；良性输入没有可测量的性能下降。后门在不同推理运行时中具有通用性，并能逃避最大开源模型分发平台的所有自动化安全扫描。

Conclusion: 聊天模板是LLM供应链中一个可靠且目前未被防御的攻击面，需要新的安全措施来检测和缓解这类模板后门攻击。

Abstract: Open-weight language models are increasingly used in production settings, raising new security challenges. One prominent threat in this context is backdoor attacks, in which adversaries embed hidden behaviors in language models that activate under specific conditions. Previous work has assumed that adversaries have access to training pipelines or deployment infrastructure. We propose a novel attack surface requiring neither, which utilizes the chat template. Chat templates are executable Jinja2 programs invoked at every inference call, occupying a privileged position between user input and model processing. We show that an adversary who distributes a model with a maliciously modified template can implant an inference-time backdoor without modifying model weights, poisoning training data, or controlling runtime infrastructure. We evaluated this attack vector by constructing template backdoors targeting two objectives: degrading factual accuracy and inducing emission of attacker-controlled URLs, and applied them across eighteen models spanning seven families and four inference engines. Under triggered conditions, factual accuracy drops from 90% to 15% on average while attacker-controlled URLs are emitted with success rates exceeding 80%; benign inputs show no measurable degradation. Backdoors generalize across inference runtimes and evade all automated security scans applied by the largest open-weight distribution platform. These results establish chat templates as a reliable and currently undefended attack surface in the LLM supply chain.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [7] [Accountability in Open Source Software Ecosystems: Workshop Report](https://arxiv.org/abs/2602.04026)
*Nandini Sharma,Thomas Bock,Rich Bowen,Sayeed Choudhury,Brian Fitzgerald,Matt Germonprez,Jim Herbsleb,James Howison,Tom Hughes,Min Kyung Lee,Stephanie Lieggi,Andreas Liesenfeld,Georg Link,Nicholas Matsakis,Audris Mockus,Narayan Ramasubbu,Christopher Robinson,Gregorio Robles,Nithya Ruff,Sonali Shah,Igor Steinmacher,Bogdan Vasilescu,Stephen Walli,Christopher Yoo*

Main category: cs.SE

TL;DR: 该论文通过组织专家研讨会，探讨开源软件生态系统中利益相关者的多样性、需求识别和问责机制问题，旨在激发相关研究和实践。


<details>
  <summary>Details</summary>
Motivation: 开源软件生态系统包含多种利益相关者（非营利组织、志愿者、用户、企业等），他们的需求和动机多样且有时相互冲突。目前不清楚开源社区如何识别和接触这些利益相关者、理解他们的需求，并对其需求负责。

Method: 在卡内基梅隆大学组织为期两天的现场研讨会，召集24位研究和工作于开源软件社区的专家学者和实践者，进行探索性讨论。

Result: 研讨会旨在启动关于开源软件生态系统中问责制角色的重要和紧迫问题的对话，激发研究议程和有意义的利益相关者参与实践。

Conclusion: 通过专家研讨会探讨开源软件生态系统的问责机制问题，为未来的研究和实践提供了重要的讨论起点和方向。

Abstract: Open source software ecosystems are composed of a variety of stakeholders including but not limited to non-profit organizations, volunteer contributors, users, and corporations. The needs and motivations of these stakeholders are often diverse, unknown, and sometimes even conflicting given the engagement and investment of both volunteers and corporate actors. Given this, it is not clear how open source communities identify and engage with their stakeholders, understand their needs, and hold themselves accountable to those needs. We convened 24 expert scholars and practitioners studying and working with open source software communities for an exploratory workshop discussion on these ideas. The workshop titled "Accountability and Open Source Software Ecosystems" was organized on Oct 14-15 on campus in Carnegie Mellon University, Pittsburgh, PA. The purpose of this in-person workshop was to initiate conversations that explore important and urgent questions related to the role of accountability in open source software ecosystems, and to inspire an exciting research agenda and meaningful stakeholder engagement ideas for practitioners.

</details>


### [8] [I Can't Believe It's Not a Valid Exploit](https://arxiv.org/abs/2602.04165)
*Derin Gezgin,Amartya Das,Shinhae Kim,Zhengdong Huang,Nevena Stojkovic,Claire Wang*

Main category: cs.SE

TL;DR: PoC-Gym框架评估LLM生成Java安全漏洞PoC利用的能力，发现静态分析指导能提高21%成功率，但手动检查显示71.5%的PoC无效，表明当前验证机制存在严重缺陷。


<details>
  <summary>Details</summary>
Motivation: 评估LLM在生成安全漏洞PoC利用方面的有效性，特别是验证静态分析工具提供的指导是否能真正提高PoC生成的成功率，并揭示当前验证机制的局限性。

Method: 开发PoC-Gym框架，用于通过LLM生成Java安全漏洞的PoC利用并进行系统验证。使用Claude Sonnet 4、GPT-5 Medium和gpt-oss-20b等模型，比较使用静态分析指导与基线方法FaultLine的效果。

Result: 使用静态分析指导比基线FaultLine提高21%的成功率。但手动检查发现71.5%的PoC是无效的，表明当前自动验证机制存在严重缺陷，高成功率数据具有误导性。

Conclusion: LLM在PoC生成方面虽然显示改进，但当前验证机制无法准确评估生成质量，报告的成功率具有误导性，需要更可靠的验证方法来评估LLM在安全漏洞利用生成中的实际能力。

Abstract: Recently Large Language Models (LLMs) have been used in security vulnerability detection tasks including generating proof-of-concept (PoC) exploits. A PoC exploit is a program used to demonstrate how a vulnerability can be exploited. Several approaches suggest that supporting LLMs with additional guidance can improve PoC generation outcomes, motivating further evaluation of their effectiveness. In this work, we develop PoC-Gym, a framework for PoC generation for Java security vulnerabilities via LLMs and systematic validation of generated exploits. Using PoC-Gym, we evaluate whether the guidance from static analysis tools improves the PoC generation success rate and manually inspect the resulting PoCs. Our results from running PoC-Gym with Claude Sonnet 4, GPT-5 Medium, and gpt-oss-20b show that using static analysis for guidance and criteria lead to 21% higher success rates than the prior baseline, FaultLine. However, manual inspection of both successful and failed PoCs reveals that 71.5% of the PoCs are invalid. These results show that the reported success of LLM-based PoC generation can be significantly misleading, which is hard to detect with current validation mechanisms.

</details>


### [9] [Semantic Consensus Decoding: Backdoor Defense for Verilog Code Generation](https://arxiv.org/abs/2602.04195)
*Guang Yang,Xing Hu,Xiang Chen,Xin Xia*

Main category: cs.SE

TL;DR: 本文提出了一种针对Verilog代码生成中LLM后门攻击的防御方法SCD，通过功能需求提取和共识解码，将攻击成功率从89%降至3%以下。


<details>
  <summary>Details</summary>
Motivation: 硬件设计中使用的LLM容易受到后门攻击，攻击者在训练时注入恶意触发器，导致生成易受攻击的硬件设计。与可修补的软件漏洞不同，硬件木马一旦制造就不可逆转，修复成本极高或不可能。现有主动防御需要访问训练数据，对第三方LLM用户不实用；被动防御难以应对语义上隐蔽的触发器。

Method: 提出语义共识解码(SCD)，这是一种推理时被动防御方法，包含两个关键组件：1) 功能需求提取：从用户规范中识别基本功能需求；2) 共识解码：基于完整用户规范和提取的功能需求自适应融合输出分布。当这些分布显著发散时，SCD自动抑制可疑组件。

Result: 通过三种代表性后门攻击的广泛实验表明，SCD将平均攻击成功率从89%降低到3%以下，同时对生成质量影响可忽略不计。

Conclusion: SCD通过利用攻击者在保持有效性和隐蔽性时偏向在非功能需求中嵌入触发器的特点，提供了一种有效的推理时被动防御方法，显著降低了后门攻击成功率，同时保持了代码生成质量。

Abstract: Large language models (LLMs) for Verilog code generation are increasingly adopted in hardware design, yet remain vulnerable to backdoor attacks where adversaries inject malicious triggers during training to induce vulnerable hardware designs. Unlike patchable software vulnerabilities, hardware trojans become irreversible once fabricated, making remediation extremely costly or impossible. Existing active defenses require access to training data, impractical for third-party LLM users, while passive defenses struggle against semantically stealthy triggers that naturally blend into design specifications. In this paper, we hypothesize that under the requirements of both effectiveness and stealthiness, attackers are strongly biased toward embedding triggers in non-functional requirements (e.g., style modifiers, quality descriptors) rather than functional specifications that determine hardware behavior. Exploiting this insight, we propose Semantic Consensus Decoding (SCD), an inference-time passive defense with two key components: (1) functional requirement extraction that identifies essential requirements from user specifications, and (2) consensus decoding that adaptively fuses output distributions based on full user specifications and extracted functional requirements. When these distributions diverge significantly, SCD automatically suppresses suspicious components. Extensive experiments with three representative backdoor attacks demonstrate that SCD reduces average attack success rate from 89% to under 3% with negligible impact on generation quality.

</details>


### [10] [ProxyWar: Dynamic Assessment of LLM Code Generation in Game Arenas](https://arxiv.org/abs/2602.04296)
*Wenjun Peng,Xinyu Wang,Qi Wu*

Main category: cs.SE

TL;DR: ProxyWar是一个通过将LLM生成的智能体嵌入竞争性游戏环境来系统评估代码生成质量的框架，超越了传统静态基准测试，揭示了基准分数与动态环境中实际性能的显著差异。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型在代码生成方面的评估主要依赖静态基准测试和简单指标，无法全面评估代码在实际动态环境中的真实效果，需要更丰富、基于竞争环境的评估方法。

Method: 提出ProxyWar框架，将LLM生成的智能体嵌入多样化的竞争性游戏环境中，结合自动化测试、迭代代码修复和多智能体锦标赛，全面评估程序的功能正确性和操作特性。

Result: 应用于多种先进代码生成模型和游戏环境后，发现基准测试分数与动态环境中的实际性能存在显著差异，揭示了传统评估方法忽视的局限性和改进机会。

Conclusion: ProxyWar为代码生成提供了更丰富的竞争性评估基础，为LLM驱动的算法发现、自适应问题解决以及实际效率和鲁棒性研究奠定了基础，展示了模型超越手工编写智能体的潜力。

Abstract: Large language models (LLMs) have revolutionized automated code generation, yet the evaluation of their real-world effectiveness remains limited by static benchmarks and simplistic metrics. We present ProxyWar, a novel framework that systematically assesses code generation quality by embedding LLM-generated agents within diverse, competitive game environments. Unlike existing approaches, ProxyWar evaluates not only functional correctness but also the operational characteristics of generated programs, combining automated testing, iterative code repair, and multi-agent tournaments to provide a holistic view of program behavior. Applied to a range of state-of-the-art coders and games, our approach uncovers notable discrepancies between benchmark scores and actual performance in dynamic settings, revealing overlooked limitations and opportunities for improvement. These findings highlight the need for richer, competition-based evaluation of code generation. Looking forward, ProxyWar lays a foundation for research into LLM-driven algorithm discovery, adaptive problem solving, and the study of practical efficiency and robustness, including the potential for models to outperform hand-crafted agents. The project is available at https://github.com/xinke-wang/ProxyWar.

</details>


### [11] [Model-Driven Legacy System Modernization at Scale](https://arxiv.org/abs/2602.04341)
*Tobias Böhm,Jens Guan Su Tien,Mohini Nonnenmann,Tom Schoonbaert,Bart Carpels,Andreas Biesdorf*

Main category: cs.SE

TL;DR: 本文提出了一种基于模型驱动的遗留系统现代化方法，通过引入技术无关的中间模型，实现从传统.NET/ASP.NET MVC到现代Web技术栈的半自动迁移。


<details>
  <summary>Details</summary>
Motivation: 解决遗留系统现代化过程中的风险高、工作量大、难以规模化的问题，特别是针对传统.NET Framework和ASP.NET MVC应用向现代Web技术栈迁移的挑战。

Method: 采用四阶段模型驱动方法：分析、丰富、合成、过渡。创建技术无关的中间模型来捕获系统结构、依赖关系和语义元数据，通过转换规则实现半自动迁移。

Result: 成功将大型工业应用从传统.NET/ASP.NET MVC迁移到现代Web技术栈，核心UI组件和页面结构实现半自动迁移，保持了功能行为和关键非功能性质量，提高了代码可维护性和可扩展性。

Conclusion: 基于模型的抽象方法降低了现代化风险和努力，支持可扩展、可追溯的遗留应用现代化。虽然标准模式自动化效果良好，但定制布局组合仍需手动适配。该方法可推广到类似的现代化场景。

Abstract: This experience report presents a model-driven approach to legacy system modernization that inserts an enriched, technology-agnostic intermediate model between the legacy codebase and the modern target platform, and reports on its application and evaluation. The four-stage process of analysis, enrichment, synthesis, and transition systematically extracts, abstracts, and transforms system artifacts. We apply our approach to a large industrial application built on legacy versions of the .NET Framework and ASP.NET MVC and show that core user interface components and page structures can be migrated semi-automatically to a modern web stack while preserving functional behavior and essential non-functional qualities. By consolidating architectural knowledge into explicit model representations, the resulting codebase exhibits higher maintainability and extensibility, thereby improving developer experience. Although automation is effective for standard patterns, migration of bespoke layout composites remains challenging and requires targeted manual adaptation. Our contributions are: (i) an end-to-end model-driven process, (ii) an enriched intermediate model that captures structure, dependencies, and semantic metadata, (iii) transformation rules that preserve functional behavior and essential non-functional qualities, and (iv) application and evaluation of the approach in an industrial setting. Overall, model-based abstractions reduce risk and effort while supporting scalable, traceable modernization of legacy applications. Our approach generalizes to comparable modernization contexts and promotes reuse of migration patterns.

</details>


### [12] [Generative AI in Systems Engineering: A Framework for Risk Assessment of Large Language Models](https://arxiv.org/abs/2602.04358)
*Stefan Otten,Philipp Reis,Philipp Rigoll,Joshua Ransiek,Tobias Schürmann,Jacob Langner,Eric Sax*

Main category: cs.SE

TL;DR: 本文提出了LLM风险评估框架(LRF)，用于评估大语言模型在系统工程环境中的应用风险，通过自主性和影响两个维度进行分类，帮助组织确定适当的验证策略和风险控制措施。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在工程生命周期中应用日益广泛，但组织在评估相关风险时面临挑战，导致集成不一致、故障模式未知和可扩展性有限等问题。需要一种结构化方法来评估LLM在系统工程环境中的应用风险。

Method: 提出了LLM风险评估框架(LRF)，该框架基于两个基本维度对LLM应用进行分类：1) 自主性（从支持性辅助到完全自动化决策），2) 影响（模型错误或误导性输出对工程过程和系统元素的潜在严重性）。通过这两个维度的组合，在整个开发生命周期中一致地确定相应的风险水平。

Result: LRF框架能够支持组织识别适当的验证策略、人工监督水平和必要的应对措施，确保安全透明的部署。该分类有助于将AI技术的快速发展与可靠性、可追溯性和受控过程集成等既定工程原则相结合。

Conclusion: LRF为复杂工程环境中风险感知的LLM采用提供了基础，代表了系统工程中标准化AI保证实践的第一步。该框架有助于组织在快速发展的AI技术与工程可靠性原则之间建立平衡。

Abstract: The increasing use of Large Language Models (LLMs) offers significant opportunities across the engineering lifecycle, including requirements engineering, software development, process optimization, and decision support. Despite this potential, organizations face substantial challenges in assessing the risks associated with LLM use, resulting in inconsistent integration, unknown failure modes, and limited scalability. This paper introduces the LLM Risk Assessment Framework (LRF), a structured approach for evaluating the application of LLMs within Systems Engineering (SE) environments. The framework classifies LLM-based applications along two fundamental dimensions: autonomy, ranging from supportive assistance to fully automated decision making, and impact, reflecting the potential severity of incorrect or misleading model outputs on engineering processes and system elements. By combining these dimensions, the LRF enables consistent determination of corresponding risk levels across the development lifecycle. The resulting classification supports organizations in identifying appropriate validation strategies, levels of human oversight, and required countermeasures to ensure safe and transparent deployment. The framework thereby helps align the rapid evolution of AI technologies with established engineering principles of reliability, traceability, and controlled process integration. Overall, the LRF provides a basis for risk-aware adoption of LLMs in complex engineering environments and represents a first step toward standardized AI assurance practices in systems engineering.

</details>


### [13] [AgenticAKM : Enroute to Agentic Architecture Knowledge Management](https://arxiv.org/abs/2602.04445)
*Rudra Dhar,Karthik Vaidhyanathan,Vasudeva Varma*

Main category: cs.SE

TL;DR: 本文提出AgenticAKM，一种基于智能体协作的架构知识管理方法，通过分解架构恢复和文档化为可管理的子任务，自动生成架构决策记录（ADRs）。


<details>
  <summary>Details</summary>
Motivation: 架构知识管理（AKM）对于软件项目至关重要，但通常是一个繁琐的过程，开发者和架构师往往不采用。虽然大语言模型（LLMs）提供了自动化机会，但简单的单提示方法受限于上下文长度限制，且难以理解架构知识的分布式特性。

Method: 提出AgenticAKM方法，将复杂的架构恢复和文档化问题分解为可管理的子任务。设计专门的智能体（架构提取、检索、生成和验证）在结构化工作流程中协作生成架构知识。具体实例化为从代码仓库自动生成架构决策记录（ADRs）。

Result: 通过对29个仓库进行用户研究验证，结果表明智能体方法能生成更好的ADRs，证明该方法在自动化架构知识管理方面具有前景和实用性。

Conclusion: AgenticAKM方法通过智能体协作有效解决了传统AKM的局限性，为自动化架构知识管理提供了有前景的实用解决方案。

Abstract: Architecture Knowledge Management (AKM) is crucial for maintaining current and comprehensive software Architecture Knowledge (AK) in a software project. However AKM is often a laborious process and is not adopted by developers and architects. While LLMs present an opportunity for automation, a naive, single-prompt approach is often ineffective, constrained by context limits and an inability to grasp the distributed nature of architectural knowledge. To address these limitations, we propose an Agentic approach for AKM, AgenticAKM, where the complex problem of architecture recovery and documentation is decomposed into manageable sub-tasks. Specialized agents for architecture Extraction, Retrieval, Generation, and Validation collaborate in a structured workflow to generate AK. To validate we made an initial instantiation of our approach to generate Architecture Decision Records (ADRs) from code repositories. We validated our approach through a user study with 29 repositories. The results demonstrate that our agentic approach generates better ADRs, and is a promising and practical approach for automating AKM.

</details>


### [14] [What's in a Benchmark? The Case of SWE-Bench in Automated Program Repair](https://arxiv.org/abs/2602.04449)
*Matias Martinez,Xavier Franch*

Main category: cs.SE

TL;DR: 对SWE-Bench两个排行榜（Lite和Verified）的首次综合分析，研究提交者身份、使用产品、LLM模型和方案开放性，发现行业主导、Claude模型占优的现状


<details>
  <summary>Details</summary>
Motivation: 随着AI特别是大语言模型和智能体系统的发展，自动程序修复领域进展迅速。SWE-Bench作为评估修复系统的基准，其公开排行榜已成为追踪进展和比较解决方案的核心平台。本研究旨在首次全面分析这两个排行榜的生态系统

Method: 分析SWE-Bench的Lite和Verified两个排行榜的提交数据，共研究了79个Lite提交和133个Verified提交。研究内容包括：提交者身份（行业/学术）、背后产品、使用的LLM模型、以及方法的开放性程度

Result: 1. 两个排行榜的大多数提交来自行业，特别是小型公司和大型上市公司；2. 行业提交通常获得最好结果，但学术贡献（通常是开源的）也保持竞争力；3. 专有LLM模型明显占主导地位，特别是Claude系列；4. 目前两个排行榜的最先进结果都由Claude 4 Sonnet实现

Conclusion: 研究揭示了SWE-Bench生态系统的现状，为未来基准驱动研究提供了提高透明度和多样性的指导。行业主导和专有模型占优的现状值得关注，需要促进更开放和多样化的研究生态系统

Abstract: The rapid progress in Automated Program Repair (APR) has been fueled by advances in AI, particularly large language models (LLMs) and agent-based systems. SWE-Bench is a benchmark designed to evaluate repair systems using real issues mined from popular open-source Python repositories. Its public leaderboards-SWE-Bench Lite and Verified-have become central platforms for tracking progress and comparing solutions. In this paper, we present the first comprehensive study of these two leaderboards, examining who is submitting solutions, the products behind the submissions, the LLMs employed, and the openness of the approaches. We analyze 79 entries submitted to Lite leaderboard and 133 to Verified. Our results show that most entries on both leaderboards originate from industry, particularly small companies and large publicly traded companies. These submissions often achieve top results, although academic contributions-typically open source-also remain competitive. We also find a clear dominance of proprietary LLMs, especially Claude family, with state-of-the-art results on both leaderboards currently achieved by Claude 4 Sonnet. These findings offer insights into the SWE-Bench ecosystem that can guide greater transparency and diversity in future benchmark-driven research.

</details>


### [15] [A Framework of Critical Success Factors for Agile Software Development](https://arxiv.org/abs/2602.04467)
*Ridewaan Hanslo,Maureen Tanner*

Main category: cs.SE

TL;DR: 这篇系统文献综述通过分析53项研究，识别了敏捷软件开发项目的21个关键成功因素，并将其归类为组织、人员、技术、过程和项目五大主题，构建了理论框架。


<details>
  <summary>Details</summary>
Motivation: 尽管敏捷软件开发很流行，但实现一致的项目成功仍然具有挑战性。本研究旨在通过系统文献综述识别敏捷项目中的关键成功因素，为研究者和实践者提供指导。

Method: 采用系统文献综述方法，分析了53项主要研究。使用主题综合与内容分析相结合的方法，对关键成功因素进行识别和分类。

Result: 识别出21个关键成功因素，分为五大主题：组织、人员、技术、过程和项目。团队有效性和项目管理是最常被引用的因素，强调了人员和过程因素的重要性。基于这些发现构建了理论框架。

Conclusion: 研究为敏捷项目成功提供了有价值的见解，指导未来研究验证这些发现并使用定量方法测试提出的理论框架。强调了人员和过程因素在敏捷项目成功中的核心作用。

Abstract: Despite the popularity of Agile software development, achieving consistent project success remains challenging. This systematic literature review identifies critical success factors (CSFs) in Agile projects by analyzing 53 primary studies. Employing thematic synthesis with content analysis, our analysis yielded 21 CSFs categorized into five themes: organizational, people, technical, process, and project. Team effectiveness and project management emerged as the most frequently cited CSFs, highlighting the importance of people and process factors. These interpreted themes and factors contributed to the development of a theoretical framework to identify how these factors contribute to project success. This study offers valuable insights for researchers and practitioners, guiding future research to validate these findings and test the proposed framework using quantitative methods.

</details>


### [16] [Supporting software engineering tasks with agentic AI: Demonstration on document retrieval and test scenario generation](https://arxiv.org/abs/2602.04726)
*Marian Kica,Lukas Radosky,David Slivka,Karin Kubinova,Daniel Dovhun,Tomas Uhercik,Erik Bircak,Ivan Polasek*

Main category: cs.SE

TL;DR: 本文介绍了两种基于智能体AI的软件工程解决方案：一是用于从详细需求描述自动生成测试场景的星型拓扑智能体系统，二是用于软件工程文档检索的多智能体系统，支持搜索、问答、变更跟踪和文档摘要等功能。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型的兴起引发了软件开发模式的重大变革，软件工程研究产生了大量工具和方法。本文旨在通过引入智能体AI解决方案来参与这一变革，为软件工程中的具体任务提供自动化支持。

Method: 1. 测试场景生成：采用星型拓扑结构，由监督智能体居中协调多个专用工作智能体，从详细需求描述自动生成测试场景。
2. 文档检索：为每个用例（搜索、问答、变更跟踪、文档摘要）设计专门的LLM智能体，每个智能体处理对应用例的所有子任务。

Result: 1. 测试场景生成方案在真实世界示例中展示了其能力。
2. 文档检索系统能够对单个软件开发相关的文档集合执行多种用例操作，包括搜索、问答、变更跟踪和大文档摘要。

Conclusion: 本文展示了智能体AI在软件工程任务中的应用潜力，并指出了未来研究方向的前景。

Abstract: The introduction of large language models ignited great retooling and rethinking of the software development models. The ensuing response of software engineering research yielded a massive body of tools and approaches. In this paper, we join the hassle by introducing agentic AI solutions for two tasks. First, we developed a solution for automatic test scenario generation from a detailed requirements description. This approach relies on specialized worker agents forming a star topology with the supervisor agent in the middle. We demonstrate its capabilities on a real-world example. Second, we developed an agentic AI solution for the document retrieval task in the context of software engineering documents. Our solution enables performing various use cases on a body of documents related to the development of a single software, including search, question answering, tracking changes, and large document summarization. In this case, each use case is handled by a dedicated LLM-based agent, which performs all subtasks related to the corresponding use case. We conclude by hinting at the future perspectives of our line of research.

</details>


### [17] [Demonstrating ARG-V's Generation of Realistic Java Benchmarks for SV-COMP](https://arxiv.org/abs/2602.04786)
*Charles Moloney,Robert Dyer,Elena Sherman*

Main category: cs.SE

TL;DR: ARG-V工具自动生成Java验证基准测试，在68个新基准上所有领先Java验证器的准确率和召回率均下降，显示现有基准可能无法充分评估验证器在真实软件上的表现。


<details>
  <summary>Details</summary>
Motivation: SV-COMP竞赛的验证器评估结果受基准测试集组成影响，需要确保新增程序能反映验证器在真实场景中的行为差异，避免评估结果的外部有效性威胁。

Method: 使用ARG-V工具自动生成符合SV-COMP格式的Java验证基准测试，创建了68个现实基准，并对比四个领先Java验证器在新旧基准集上的表现。

Result: 所有四个领先Java验证器在新生成的68个现实基准上，相比现有基准测试集，准确率和召回率均出现下降。

Conclusion: ARG-V能增强验证工具评估的全面性和现实性，为验证器开发者提供了改进工具在真实软件中适用性的路线图。

Abstract: The SV-COMP competition provides a state-of-the-art platform for evaluating software verification tools on a standardized set of verification tasks. Consequently, verifier development outcomes are influenced by the composition of program benchmarks included in SV-COMP. When expanding this benchmark corpus, it is crucial to consider whether newly added programs cause verifiers to exhibit behavior distinct from that observed on existing benchmarks. Doing so helps mitigate external threats to the validity of the competition's results.
  In this paper, we present the application of the ARG-V tool for automatically generating Java verification benchmarks in the SV-COMP format. We demonstrate that, on a newly generated set of 68 realistic benchmarks, all four leading Java verifiers decrease in accuracy and recall compared to their performance on the existing benchmark suite. These findings highlight the potential of ARG-V to enhance the comprehensiveness and realism of verification tool evaluation, while also providing a roadmap for verifier developers aiming to improve their tools' applicability to real-world software.

</details>


### [18] [Beyond the Control Equations: An Artifact Study of Implementation Quality in Robot Control Software](https://arxiv.org/abs/2602.04799)
*Nils Chur,Thorsten Berger,Einar Broch Johnsen,Andrzej Wąsowski*

Main category: cs.SE

TL;DR: 该研究发现机器人控制器软件实现存在理论与实际脱节问题，离散化处理随意、测试方法表面化，缺乏对理论保证的系统验证，需要改进实现指南和验证技术。


<details>
  <summary>Details</summary>
Motivation: 控制器是机器人系统的关键组件，控制理论为标准设计提供安全保证，但软件实现中的复杂性常被忽视。控制器通常在连续空间设计，而软件在离散空间执行，这削弱了理论保证。尽管控制理论和建模研究广泛，但控制器实现及其理论保证如何在真实软件系统中确保的问题很少受到关注。

Method: 研究者调查了184个开源机器人软件中的真实控制器实现，检查了它们的应用背景、实现特征以及用于确保正确性的测试方法。

Result: 研究发现控制器实现通常以临时方式处理离散化，导致实时可靠性问题。时序不一致、缺乏适当的错误处理、对实时约束考虑不足等挑战进一步使问题复杂化。测试实践表面化，没有使用系统化的理论保证验证，导致预期行为与实际行为可能存在不一致。

Conclusion: 研究结果强调需要改进实现指南和严格的验证技术，以确保实践中机器人控制器的可靠性和安全性。

Abstract: A controller -- a software module managing hardware behavior -- is a key component of a typical robot system. While control theory gives safety guarantees for standard controller designs, the practical implementation of controllers in software introduces complexities that are often overlooked. Controllers are often designed in continuous space, while the software is executed in discrete space, undermining some of the theoretical guarantees. Despite extensive research on control theory and control modeling, little attention has been paid to the implementations of controllers and how their theoretical guarantees are ensured in real-world software systems. We investigate 184 real-world controller implementations in open-source robot software. We examine their application context, the implementation characteristics, and the testing methods employed to ensure correctness. We find that the implementations often handle discretization in an ad hoc manner, leading to potential issues with real-time reliability. Challenges such as timing inconsistencies, lack of proper error handling, and inadequate consideration of real-time constraints further complicate matters. Testing practices are superficial, no systematic verification of theoretical guarantees is used, leaving possible inconsistencies between expected and actual behavior. Our findings highlight the need for improved implementation guidelines and rigorous verification techniques to ensure the reliability and safety of robotic controllers in practice.

</details>


### [19] [Do Developers Read Type Information? An Eye-Tracking Study on TypeScript](https://arxiv.org/abs/2602.04824)
*Samuel W. Flint,Robert Dyer,Bonita Sharif*

Main category: cs.SE

TL;DR: 研究发现开发者在代码理解和bug定位任务中不会特别关注类型注解，这对工具设计、社区标准和编程教育有重要启示。


<details>
  <summary>Details</summary>
Motivation: 静态类型注解已被证明能帮助开发者完成多种编程任务，即使不使用静态类型检查也是如此。假设这是因为开发者将类型注解视为内联文档。本研究旨在提供证据证明开发者确实将类型注解作为内联文档使用。

Method: 通过眼动追踪研究，招募26名本科生，在TypeScript语言中进行代码理解和bug定位任务，观察他们是否阅读类型注解。

Result: 研究发现，开发者在代码总结或bug定位任务中，当存在类型注解时，并不会更频繁地直接查看包含类型注解或类型声明的代码行。

Conclusion: 研究结果对工具开发者改进类型信息的可用性、开发社区建立良好的类型注解使用标准、以及教育中加强阅读模式的有意识教学具有重要意义。

Abstract: Statically-annotated types have been shown to aid developers in a number of programming tasks, and this benefit holds true even when static type checking is not used. It is hypothesized that this is because developers use type annotations as in-code documentation. In this study, we aim to provide evidence that developers use type annotations as in-code documentation. Understanding this hypothesized use will help to understand how, and in what contexts, developers use type information; additionally, it may help to design better development tools and inform educational decisions. To provide this evidence, we conduct an eye tracking study with 26 undergraduate students to determine if they read type annotations during code comprehension and bug localization in the TypeScript language. We found that developers do not look directly at lines containing type annotations or type declarations more often when they are present, in either code summarization or bug localization tasks. The results have implications for tool builders to improve the availability of type information, the development community to build good standards for use of type annotations, and education to enforce deliberate teaching of reading patterns.

</details>


### [20] [When Code Becomes Abundant: Redefining Software Engineering Around Orchestration and Verification](https://arxiv.org/abs/2602.04830)
*Karina Kohl,Luigi Carro*

Main category: cs.SE

TL;DR: 软件工程面临AI自动化和硬件能耗双重压力，需要从代码构建转向意图表达、架构控制和验证，核心风险是责任崩溃，需要重塑研究、教育和实践


<details>
  <summary>Details</summary>
Motivation: 软件工程面临AI自动化（降低代码生产成本）和硬件能耗约束（放大失败成本）的双重压力，传统围绕代码构建和流程管理的定义已不足够，需要重新定义学科方向

Method: 提出软件工程需要重新定位，从代码构建转向三个核心：人类辨别力-意图表达、架构控制、验证，强调从生产导向转向自动化下的人类判断

Result: 识别出责任崩溃是核心风险，软件工程需要根本性变革，从传统定义转向以意图表达、架构控制和系统验证为中心的新范式

Conclusion: 软件工程必须重新定义，从代码构建转向人类判断、意图表达和验证，这对研究重点、教育课程和工业实践都有深远影响

Abstract: Software Engineering (SE) faces simultaneous pressure from AI automation (reducing code production costs) and hardware-energy constraints (amplifying failure costs). We position that SE must redefine itself around human discernment-intent articulation, architectural control, and verification-rather than code construction. This shift introduces accountability collapse as a central risk and requires fundamental changes to research priorities, educational curricula, and industrial practices. We argue that Software Engineering, as traditionally defined around code construction and process management, is no longer sufficient. Instead, the discipline must be redefined around intent articulation, architectural control, and systematic verification. This redefinition shifts Software Engineering from a production-oriented field to one centered on human judgment under automation, with profound implications for research, practice, and education.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [21] [Knowledge Model Prompting Increases LLM Performance on Planning Tasks](https://arxiv.org/abs/2602.03900)
*Erik Goh,John Kos,Ashok Goel*

Main category: cs.AI

TL;DR: TMK框架提示法显著提升大语言模型在符号推理任务上的表现，在PlanBench基准测试中准确率从31.5%提升至97.3%


<details>
  <summary>Details</summary>
Motivation: 大语言模型在推理和规划任务上存在缺陷，现有提示技术如思维链(CoT)的效果受到质疑。研究借鉴认知和教育科学领域的TMK框架，探索其能否改善LLM的推理能力，特别是其因果、目的论和层次化推理结构可能弥补语言模型的推理不足。

Method: 采用任务-方法-知识(TMK)框架构建提示，在PlanBench基准测试的Blocksworld领域进行实验，测试TMK结构化提示是否能帮助语言模型更好地将复杂规划问题分解为可管理的子任务。

Result: TMK提示显著提升了推理模型的性能，在不透明的符号任务（PlanBench中的随机Blocksworld版本）上准确率从31.5%提升至97.3%，显示出在语义近似和符号操作之间架起桥梁的潜力。

Conclusion: TMK不仅作为上下文，更是一种机制，能够引导推理模型远离其默认的语言模式，在实验中激活形式化的代码执行路径。研究结果表明TMK框架在提升LLM符号推理能力方面具有重要价值。

Abstract: Large Language Models (LLM) can struggle with reasoning ability and planning tasks. Many prompting techniques have been developed to assist with LLM reasoning, notably Chain-of-Thought (CoT); however, these techniques, too, have come under scrutiny as LLMs' ability to reason at all has come into question. Borrowing from the domain of cognitive and educational science, this paper investigates whether the Task-Method-Knowledge (TMK) framework can improve LLM reasoning capabilities beyond its previously demonstrated success in educational applications. The TMK framework's unique ability to capture causal, teleological, and hierarchical reasoning structures, combined with its explicit task decomposition mechanisms, makes it particularly well-suited for addressing language model reasoning deficiencies, and unlike other hierarchical frameworks such as HTN and BDI, TMK provides explicit representations of not just what to do and how to do it, but also why actions are taken. The study evaluates TMK by experimenting on the PlanBench benchmark, focusing on the Blocksworld domain to test for reasoning and planning capabilities, examining whether TMK-structured prompting can help language models better decompose complex planning problems into manageable sub-tasks. Results also highlight significant performance inversion in reasoning models. TMK prompting enables the reasoning model to achieve up to an accuracy of 97.3\% on opaque, symbolic tasks (Random versions of Blocksworld in PlanBench) where it previously failed (31.5\%), suggesting the potential to bridge the gap between semantic approximation and symbolic manipulation. Our findings suggest that TMK functions not merely as context, but also as a mechanism that steers reasoning models away from their default linguistic modes to engage formal, code-execution pathways in the context of the experiments.

</details>


### [22] [Enhancing Mathematical Problem Solving in LLMs through Execution-Driven Reasoning Augmentation](https://arxiv.org/abs/2602.03950)
*Aditya Basarkar,Benyamin Tabarsi,Tiffany Barnes,Dongkuan,Xu*

Main category: cs.AI

TL;DR: IIPC是一种迭代改进的程序构造方法，通过结合程序执行反馈和LLM的思维链能力来提升数学推理的准确性和可修正性。


<details>
  <summary>Details</summary>
Motivation: 当前多智能体LLM系统在数学推理方面仍缺乏可靠可修正的推理过程表示，现有方法要么采用僵化的顺序流程无法修正早期错误，要么依赖可能失效的启发式自评估，且程序化上下文可能分散语言模型的注意力并降低准确性。

Method: 提出迭代改进的程序构造（IIPC）方法，迭代优化程序化推理链，将执行反馈与基础LLM的思维链能力相结合，保持高层次上下文聚焦。

Result: IIPC在多个基础LLM上的大多数推理基准测试中超越了竞争方法。

Conclusion: IIPC通过迭代改进程序构造和结合执行反馈与思维链能力，显著提升了数学推理的可靠性和准确性，所有代码和实现已开源发布。

Abstract: Mathematical problem solving is a fundamental benchmark for assessing the reasoning capabilities of artificial intelligence and a gateway to applications in education, science, and engineering where reliable symbolic reasoning is essential. Although recent advances in multi-agent LLM-based systems have enhanced their mathematical reasoning capabilities, they still lack a reliably revisable representation of the reasoning process. Existing agents either operate in rigid sequential pipelines that cannot correct earlier steps or rely on heuristic self-evaluation that can fail to identify and fix errors. In addition, programmatic context can distract language models and degrade accuracy. To address these gaps, we introduce Iteratively Improved Program Construction (IIPC), a reasoning method that iteratively refines programmatic reasoning chains and combines execution feedback with the native Chain-of-thought abilities of the base LLM to maintain high-level contextual focus. IIPC surpasses competing approaches in the majority of reasoning benchmarks on multiple base LLMs. All code and implementations are released as open source.

</details>


### [23] [AgentArk: Distilling Multi-Agent Intelligence into a Single LLM Agent](https://arxiv.org/abs/2602.03955)
*Yinyi Luo,Yiqiao Jin,Weichen Yu,Mengqi Zhang,Srijan Kumar,Xiaoxiao Li,Weijie Xu,Xin Chen,Jindong Wang*

Main category: cs.AI

TL;DR: AgentArk框架将多智能体系统的动态蒸馏到单个模型的权重中，将显式的测试时交互转化为隐式的模型能力，使单个智能体具备多智能体系统的智能同时保持计算效率。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型多智能体系统通过迭代辩论实现优越的推理性能，但实际部署受到高计算成本和错误传播的限制。需要一种方法在保持推理能力的同时提高效率。

Method: 提出AgentArk框架，研究三种分层蒸馏策略：推理增强微调、基于轨迹的增强和过程感知蒸馏。通过将计算负担从推理转移到训练，将多智能体动态蒸馏到单个模型的权重中。

Result: 蒸馏后的模型在保持单个智能体效率的同时，展现出多智能体的强大推理和自我纠正性能。在不同模型、任务、规模和场景中表现出增强的鲁棒性和泛化能力。

Conclusion: AgentArk通过将多智能体动态蒸馏到单个模型中，实现了高效且鲁棒的推理系统，为未来高效多智能体开发研究提供了新思路。

Abstract: While large language model (LLM) multi-agent systems achieve superior reasoning performance through iterative debate, practical deployment is limited by their high computational cost and error propagation. This paper proposes AgentArk, a novel framework to distill multi-agent dynamics into the weights of a single model, effectively transforming explicit test-time interactions into implicit model capabilities. This equips a single agent with the intelligence of multi-agent systems while remaining computationally efficient. Specifically, we investigate three hierarchical distillation strategies across various models, tasks, scaling, and scenarios: reasoning-enhanced fine-tuning; trajectory-based augmentation; and process-aware distillation. By shifting the burden of computation from inference to training, the distilled models preserve the efficiency of one agent while exhibiting strong reasoning and self-correction performance of multiple agents. They further demonstrate enhanced robustness and generalization across diverse reasoning tasks. We hope this work can shed light on future research on efficient and robust multi-agent development. Our code is at https://github.com/AIFrontierLab/AgentArk.

</details>


### [24] [Adaptive Test-Time Compute Allocation via Learned Heuristics over Categorical Structure](https://arxiv.org/abs/2602.03975)
*Shuhui Qu*

Main category: cs.AI

TL;DR: 提出了一种状态级选择性验证框架，在验证成本受限的情况下优化验证资源分配，相比传统方法在MATH基准上使用更少的验证调用获得更高准确率。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型推理中的测试时计算越来越受到昂贵验证的瓶颈限制，许多验证调用被浪费在冗余或无前景的中间假设上，需要在验证成本受限的情况下研究如何优化验证资源的分配。

Method: 提出了状态级选择性验证框架，包含三个核心组件：1) 结构化移动接口上的确定性可行性门控；2) 结合学习的状态距离和残差得分的预验证排序；3) 基于局部不确定性的自适应验证调用分配。

Result: 在MATH基准测试中，该方法相比best-of-N、多数投票和束搜索获得了更高的准确率，同时减少了44%的验证调用。

Conclusion: 通过状态级选择性验证框架，可以在验证成本受限的情况下更有效地分配验证资源，提高推理系统的效率，避免将验证调用浪费在冗余或无前景的中间假设上。

Abstract: Test-time computation has become a primary driver of progress in large language model (LLM) reasoning, but it is increasingly bottlenecked by expensive verification. In many reasoning systems, a large fraction of verifier calls are spent on redundant or unpromising intermediate hypotheses. We study reasoning under a \emph{verification-cost-limited} setting and ask how verification effort should be allocated across intermediate states. We propose a state-level selective verification framework that combines (i) deterministic feasibility gating over a structured move interface, (ii) pre-verification ranking using a hybrid of learned state-distance and residual scoring, and (iii) adaptive allocation of verifier calls based on local uncertainty. Unlike solution-level best-of-$N$ or uniform intermediate verification, our method distributes verification where it is most informative. On the \textsc{MATH} benchmark, our approach achieves higher accuracy than best-of-$N$, majority voting, and beam search while using 44\% fewer verifier calls.

</details>


### [25] [Monitorability as a Free Gift: How RLVR Spontaneously Aligns Reasoning](https://arxiv.org/abs/2602.03978)
*Zidi Xiong,Shan Chen,Himabindu Lakkaraju*

Main category: cs.AI

TL;DR: RLVR训练早期可能带来CoT可监控性的"免费提升"，但这种效果并非普遍存在，主要取决于数据多样性和指令遵循数据，且可监控性与模型能力正交。


<details>
  <summary>Details</summary>
Motivation: 随着大型推理模型部署增加，审计其思维链轨迹的安全性变得至关重要。研究发现RLVR训练早期可能出现可监控性的"免费提升"，但需要系统评估这种效应的普遍性和机制。

Method: 通过跨模型家族和训练领域的系统评估，分析数据多样性、指令遵循数据的作用，进行机制分析（响应分布锐化、注意力模式），并控制训练和评估难度来研究可监控性动态。

Result: 可监控性提升强烈依赖于数据，特别是数据多样性和指令遵循数据；可监控性与推理能力正交；机制上主要源于响应分布锐化（熵减少）和提示注意力增加，而非对推理轨迹的因果依赖增强。

Conclusion: RLVR下的可监控性提升不是普遍现象，需要特定数据条件，且与模型能力无关。这为理解可监控性何时出现提供了整体视角，澄清了提升可能发生和不可能发生的情况。

Abstract: As Large Reasoning Models (LRMs) are increasingly deployed, auditing their chain-of-thought (CoT) traces for safety becomes critical. Recent work has reported that monitorability--the degree to which CoT faithfully and informatively reflects internal computation--can appear as a "free gift" during the early stages of Reinforcement Learning with Verifiable Rewards (RLVR). We make this observation concrete through a systematic evaluation across model families and training domains. Our results show that this effect is not universal: monitorability improvements are strongly data-dependent. In particular, we demonstrate the critical role of data diversity and instruction-following data during RLVR training. We further show that monitorability is orthogonal to capability--improvements in reasoning performance do not imply increased transparency. Through mechanistic analysis, we attribute monitorability gains primarily to response distribution sharpening (entropy reduction) and increased attention to the prompt, rather than stronger causal reliance on reasoning traces. We also reveal how monitorability dynamics vary with controlled training and evaluation difficulty. Together, these findings provide a holistic view of how monitorability emerges under RLVR, clarifying when gains are likely to occur and when they are not.

</details>


### [26] [Axiomatic Foundations of Counterfactual Explanations](https://arxiv.org/abs/2602.04028)
*Leila Amgoud,Martin Cooper*

Main category: cs.AI

TL;DR: 本文提出了一个反事实解释的axiomatic框架，证明了不可能性定理，识别了五种不同类型的反事实解释，并将现有解释器纳入分类体系。


<details>
  <summary>Details</summary>
Motivation: 当前反事实解释研究存在两个主要问题：1) 大多数解释器只关注单一类型的反事实；2) 主要局限于局部解释，缺乏对系统整体推理过程的全局解释。需要系统研究不同类型的反事实解释及其理论框架。

Method: 建立基于一组理想属性的反事实解释器公理框架，证明不可能性定理，通过表示定理建立公理子集与解释器家族之间的一一对应关系，识别五种不同类型的反事实解释。

Result: 证明了没有单一解释器能同时满足某些公理组合，识别了五种根本不同类型的反事实解释（包括局部和全局解释），将现有解释器纳入分类体系，并分析了生成此类解释的计算复杂度。

Conclusion: 该框架为反事实解释提供了系统化的理论基础，揭示了反事实解释的多样性，区分了局部和全局解释类型，为未来解释器的设计和评估提供了理论指导。

Abstract: Explaining autonomous and intelligent systems is critical in order to improve trust in their decisions. Counterfactuals have emerged as one of the most compelling forms of explanation. They address ``why not'' questions by revealing how decisions could be altered. Despite the growing literature, most existing explainers focus on a single type of counterfactual and are restricted to local explanations, focusing on individual instances. There has been no systematic study of alternative counterfactual types, nor of global counterfactuals that shed light on a system's overall reasoning process.
  This paper addresses the two gaps by introducing an axiomatic framework built on a set of desirable properties for counterfactual explainers. It proves impossibility theorems showing that no single explainer can satisfy certain axiom combinations simultaneously, and fully characterizes all compatible sets. Representation theorems then establish five one-to-one correspondences between specific subsets of axioms and the families of explainers that satisfy them. Each family gives rise to a distinct type of counterfactual explanation, uncovering five fundamentally different types of counterfactuals. Some of these correspond to local explanations, while others capture global explanations. Finally, the framework situates existing explainers within this taxonomy, formally characterizes their behavior, and analyzes the computational complexity of generating such explanations.

</details>


### [27] [Scaling In-Context Online Learning Capability of LLMs via Cross-Episode Meta-RL](https://arxiv.org/abs/2602.04089)
*Xiaofeng Lin,Sirou Zhu,Yilei Chen,Mingyu Chen,Hejian Sang,Ioannis Paschalidis,Zhipeng Wang,Aldo Pacchiano,Xuezhou Zhang*

Main category: cs.AI

TL;DR: ORBIT框架通过元强化学习训练LLMs在上下文中从交互中学习，使小型开源模型在未见环境中达到GPT-5.2水平，显著优于标准RL微调


<details>
  <summary>Details</summary>
Motivation: 当前LLMs在需要在线交互的决策任务中存在局限，无法可靠利用上下文交互经验，而许多现实世界决策任务需要平衡信息收集和利用

Method: 提出ORBIT框架：多任务、多回合的元强化学习框架，训练LLMs在上下文中从交互中学习，通过训练解决现有LLMs的局限性

Result: 经过元训练后，相对较小的开源模型(Qwen3-14B)在完全未见环境中表现出显著改进的上下文在线学习能力，匹配GPT-5.2性能，大幅优于标准RL微调

Conclusion: 通过训练可以解决LLMs在在线决策任务中的局限性，ORBIT框架展示了在推理时学习决策智能体的巨大潜力，模型规模扩展实验显示持续增益

Abstract: Large language models (LLMs) achieve strong performance when all task-relevant information is available upfront, as in static prediction and instruction-following problems. However, many real-world decision-making tasks are inherently online: crucial information must be acquired through interaction, feedback is delayed, and effective behavior requires balancing information collection and exploitation over time. While in-context learning enables adaptation without weight updates, existing LLMs often struggle to reliably leverage in-context interaction experience in such settings. In this work, we show that this limitation can be addressed through training. We introduce ORBIT, a multi-task, multi-episode meta-reinforcement learning framework that trains LLMs to learn from interaction in context. After meta-training, a relatively small open-source model (Qwen3-14B) demonstrates substantially improved in-context online learning on entirely unseen environments, matching the performance of GPT-5.2 and outperforming standard RL fine-tuning by a large margin. Scaling experiments further reveal consistent gains with model size, suggesting significant headroom for learn-at-inference-time decision-making agents. Code reproducing the results in the paper can be found at https://github.com/XiaofengLin7/ORBIT.

</details>


### [28] [Interfaze: The Future of AI is built on Task-Specific Small Models](https://arxiv.org/abs/2602.04101)
*Harsha Vardhan Khurdula,Vineet Agarwal,Yoeven D Khemlani*

Main category: cs.AI

TL;DR: Interfaze是一个将LLM应用视为上下文构建与执行问题的系统，而非依赖单一模型。它通过异构DNN堆栈、上下文构建层和动作层处理复杂任务，仅将提炼后的上下文传递给用户选择的LLM生成最终响应。


<details>
  <summary>Details</summary>
Motivation: 现代LLM应用不应仅依赖单一大型模型，而应通过构建和操作上下文来解决问题。传统方法使用单一transformer处理所有任务效率低下，需要将计算负担从昂贵的大型模型转移到小型模型和工具栈上。

Method: 系统包含三个核心组件：(1) 异构DNN堆栈配合小型语言模型作为感知模块，处理复杂PDF、图表、多语言ASR等；(2) 上下文构建层爬取、索引、解析外部资源为结构化状态；(3) 动作层支持浏览、检索、沙箱代码执行和驱动无头浏览器。顶层控制器决定运行哪些小型模型和动作，并将提炼的上下文传递给用户选择的LLM。

Result: Interfaze-Beta在多个基准测试中表现优异：MMLU-Pro 83.6%、MMLU 91.4%、GPQA-Diamond 81.3%、LiveCodeBench v5 57.8%、AIME-2025 90.0%。多模态任务上：MMMU(val) 77.3%、AI2D 91.5%、ChartQA 90.9%、Common Voice v16 90.8%。大多数查询主要由小型模型和工具栈处理，大型LLM仅操作提炼后的上下文。

Conclusion: Interfaze系统证明了通过将LLM应用重构为上下文构建和执行问题，结合小型模型和工具栈，可以在保持竞争力的准确率的同时，将主要计算从昂贵的大型模型转移出去，实现更高效、模块化的AI系统架构。

Abstract: We present Interfaze, a system that treats modern LLM applications as a problem of building and acting over context, not just picking the right monolithic model. Instead of a single transformer, we combine (i) a stack of heterogeneous DNNs paired with small language models as perception modules for OCR involving complex PDFs, charts and diagrams, and multilingual ASR with (ii) a context-construction layer that crawls, indexes, and parses external sources (web pages, code, PDFs) into compact structured state, and (iii) an action layer that can browse, retrieve, execute code in a sandbox, and drive a headless browser for dynamic web pages. A thin controller sits on top of this stack and exposes a single, OpenAI-style endpoint: it decides which small models and actions to run and always forwards the distilled context to a user-selected LLM that produces the final response.
  On this architecture, Interfaze-Beta achieves 83.6% on MMLU-Pro, 91.4% on MMLU, 81.3% on GPQA-Diamond, 57.8% on LiveCodeBench v5, and 90.0% on AIME-2025, along with strong multimodal scores on MMMU (val) (77.3%), AI2D (91.5%), ChartQA (90.9%), and Common Voice v16 (90.8%). We show that most queries are handled primarily by the small-model and tool stack, with the large LLM operating only on distilled context, yielding competitive accuracy while shifting the bulk of computation away from the most expensive and monolithic models.

</details>


### [29] [Steering LLMs via Scalable Interactive Oversight](https://arxiv.org/abs/2602.04210)
*Enyu Zhou,Zhiheng Xi,Long Ma,Zhihao Zhang,Shihan Dou,Zhikai Lei,Guoteng Wang,Rui Zheng,Hang Yan,Tao Gui,Qi Zhang,Xuanjing Huang*

Main category: cs.AI

TL;DR: 论文提出可扩展交互监督框架，通过递归分解复杂意图为可管理决策树，让非专家用户也能有效指导AI完成复杂任务，在网页开发任务中实现了54%的对齐度提升。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型能够自动化复杂、长时程任务，出现了监督缺口：用户因缺乏领域专业知识、难以精确表达意图、无法可靠验证复杂输出，而难以有效指导AI系统。

Method: 提出可扩展交互监督框架，将复杂意图递归分解为可管理的决策树，在节点处收集低负担的用户反馈，递归聚合这些信号形成精确的全局指导。

Result: 在网页开发任务中验证，该框架使非专家用户能够生成专家级的产品需求文档，实现了54%的对齐度提升。框架可通过仅使用在线用户反馈的强化学习进行优化。

Conclusion: 该框架为解决可扩展监督挑战提供了实用路径，能够在AI扩展过程中保持人类控制，使非专家用户也能有效指导AI完成超越自身能力的复杂任务。

Abstract: As Large Language Models increasingly automate complex, long-horizon tasks such as \emph{vibe coding}, a supervision gap has emerged. While models excel at execution, users often struggle to guide them effectively due to insufficient domain expertise, the difficulty of articulating precise intent, and the inability to reliably validate complex outputs. It presents a critical challenge in scalable oversight: enabling humans to responsibly steer AI systems on tasks that surpass their own ability to specify or verify. To tackle this, we propose Scalable Interactive Oversight, a framework that decomposes complex intent into a recursive tree of manageable decisions to amplify human supervision. Rather than relying on open-ended prompting, our system elicits low-burden feedback at each node and recursively aggregates these signals into precise global guidance. Validated in web development task, our framework enables non-experts to produce expert-level Product Requirement Documents, achieving a 54\% improvement in alignment. Crucially, we demonstrate that this framework can be optimized via Reinforcement Learning using only online user feedback, offering a practical pathway for maintaining human control as AI scales.

</details>


### [30] [Agent-Omit: Training Efficient LLM Agents for Adaptive Thought and Observation Omission via Agentic Reinforcement Learning](https://arxiv.org/abs/2602.04284)
*Yansong Ning,Jun Fang,Naiqiang Tan,Hao Liu*

Main category: cs.AI

TL;DR: Agent-Omit：一个训练框架，使LLM智能体能够自适应地省略冗余的思维和观察，在保持性能的同时提高效率。


<details>
  <summary>Details</summary>
Motivation: 现有研究将整个交互轨迹同等对待，忽略了不同轮次中思维必要性和观察效用的差异，导致智能体效率低下。

Method: 1. 合成少量冷启动数据（单轮和多轮省略场景）微调智能体；2. 提出省略感知的智能体强化学习方法，包含双重采样机制和定制的省略奖励。

Result: 在五个智能体基准测试中，Agent-Omit-8B模型性能与前沿LLM智能体相当，在效果-效率权衡方面优于七种高效LLM智能体方法。

Conclusion: Agent-Omit框架通过自适应省略冗余思维和观察，实现了智能体效果与效率的最佳平衡，理论证明其省略策略偏差有上界。

Abstract: Managing agent thought and observation during multi-turn agent-environment interactions is an emerging strategy to improve agent efficiency. However, existing studies treat the entire interaction trajectories equally, overlooking the thought necessity and observation utility varies across turns. To this end, we first conduct quantitative investigations into how thought and observation affect agent effectiveness and efficiency. Based on our findings, we propose Agent-Omit, a unified training framework that empowers LLM agents to adaptively omit redundant thoughts and observations. Specifically, we first synthesize a small amount of cold-start data, including both single-turn and multi-turn omission scenarios, to fine-tune the agent for omission behaviors. Furthermore, we introduce an omit-aware agentic reinforcement learning approach, incorporating a dual sampling mechanism and a tailored omission reward to incentivize the agent's adaptive omission capability. Theoretically, we prove that the deviation of our omission policy is upper-bounded by KL-divergence. Experimental results on five agent benchmarks show that our constructed Agent-Omit-8B could obtain performance comparable to seven frontier LLM agent, and achieve the best effectiveness-efficiency trade-off than seven efficient LLM agents methods. Our code and data are available at https://github.com/usail-hkust/Agent-Omit.

</details>


### [31] [From Assumptions to Actions: Turning LLM Reasoning into Uncertainty-Aware Planning for Embodied Agents](https://arxiv.org/abs/2602.04326)
*SeungWon Seo,SooBin Lim,SeongRae Noh,Haneul Kim,HyeongYeop Kang*

Main category: cs.AI

TL;DR: PCE框架通过将LLM推理中的隐含假设转化为结构化决策树，在多智能体部分可观察环境中实现无需频繁通信的理性决策


<details>
  <summary>Details</summary>
Motivation: 当前基于大语言模型的智能体在多智能体、部分可观察、去中心化环境中主要依赖频繁通信来应对不确定性，这带来了高昂的token和时间成本，并可能干扰人类合作伙伴的工作流程

Method: 提出PCE（Planner-Composer-Evaluator）框架，将LLM推理轨迹中的碎片化假设转换为结构化决策树，内部节点编码环境假设，叶节点映射到动作，通过场景可能性、目标导向收益和执行成本对路径进行评分

Result: 在两个多智能体基准测试（C-WAH和TDW-MAT）和三个LLM骨干上，PCE在成功率和任务效率上持续优于通信密集型基线，同时保持相当的token使用量。消融实验表明PCE在不同模型容量和推理深度下都能提升性能

Conclusion: PCE为将LLM的隐含假设转化为可靠的不确定性感知规划策略提供了原则性方法，能够产生人类合作伙伴认为更高效和可信的通信模式

Abstract: Embodied agents operating in multi-agent, partially observable, and decentralized environments must plan and act despite pervasive uncertainty about hidden objects and collaborators' intentions. Recent advances in applying Large Language Models (LLMs) to embodied agents have addressed many long-standing challenges, such as high-level goal decomposition and online adaptation. Yet, uncertainty is still primarily mitigated through frequent inter-agent communication. This incurs substantial token and time costs, and can disrupt established workflows, when human partners are involved. We introduce PCE, a Planner-Composer-Evaluator framework that converts the fragmented assumptions latent in LLM reasoning traces into a structured decision tree. Internal nodes encode environment assumptions and leaves map to actions; each path is then scored by scenario likelihood, goal-directed gain, and execution cost to guide rational action selection without heavy communication. Across two challenging multi-agent benchmarks (C-WAH and TDW-MAT) and three diverse LLM backbones, PCE consistently outperforms communication-centric baselines in success rate and task efficiency while showing comparable token usage. Ablation results indicate that the performance gains obtained by scaling model capacity or reasoning depth persist even when PCE is applied, while PCE consistently raises the baseline across both capacity and reasoning-depth scales, confirming that structured uncertainty handling complements both forms of scaling. A user study further demonstrates that PCE produces communication patterns that human partners perceive as more efficient and trustworthy. Together, these results establish a principled route for turning latent LLM assumptions into reliable strategies for uncertainty-aware planning.

</details>


### [32] [Digital Twins & ZeroConf AI: Structuring Automated Intelligent Pipelines for Industrial Applications](https://arxiv.org/abs/2602.04385)
*Marco Picone,Fabio Turazza,Matteo Martinelli,Marco Mamei*

Main category: cs.AI

TL;DR: 提出基于数字孪生的零配置AI管道框架，解决工业CPS中AI/ML集成碎片化问题，实现模块化、可互操作的智能服务部署


<details>
  <summary>Details</summary>
Motivation: 工业CPS系统日益复杂，物联网/工业物联网技术碎片化（通信协议、数据格式、设备能力多样）导致物理层与智能功能层之间存在巨大鸿沟，现有数字孪生方法通常孤立且紧耦合，限制了AI功能的可扩展性和重用性

Method: 提出模块化、可互操作的解决方案，通过最小化配置和解耦数字孪生与AI组件角色，实现AI管道无缝集成到CPS中。引入零配置AI管道概念，由数字孪生协调数据管理和智能增强

Result: 在微工厂场景中验证了该方法，展示了支持并发ML模型和动态数据处理的能力，有效加速了复杂工业环境中智能服务的部署

Conclusion: 该框架通过数字孪生技术实现了AI/ML在工业CPS中的高效集成，解决了碎片化问题，提高了智能服务的部署速度和可扩展性

Abstract: The increasing complexity of Cyber-Physical Systems (CPS), particularly in the industrial domain, has amplified the challenges associated with the effective integration of Artificial Intelligence (AI) and Machine Learning (ML) techniques. Fragmentation across IoT and IIoT technologies, manifested through diverse communication protocols, data formats and device capabilities, creates a substantial gap between low-level physical layers and high-level intelligent functionalities. Recently, Digital Twin (DT) technology has emerged as a promising solution, offering structured, interoperable and semantically rich digital representations of physical assets. Current approaches are often siloed and tightly coupled, limiting scalability and reuse of AI functionalities. This work proposes a modular and interoperable solution that enables seamless AI pipeline integration into CPS by minimizing configuration and decoupling the roles of DTs and AI components. We introduce the concept of Zero Configuration (ZeroConf) AI pipelines, where DTs orchestrate data management and intelligent augmentation. The approach is demonstrated in a MicroFactory scenario, showing support for concurrent ML models and dynamic data processing, effectively accelerating the deployment of intelligent services in complex industrial settings.

</details>


### [33] [ReThinker: Scientific Reasoning by Rethinking with Guided Reflection and Confidence Control](https://arxiv.org/abs/2602.04496)
*Zhentao Tang,Yuqi Cui,Shixiong Kai,Wenqian Zhao,Ke Ye,Xing Li,Anxin Tian,Zehua Pei,Hui-Ling Zhen,Shoubo Hu,Xiaoguang Li,Yunhe Wang,Mingxuan Yuan*

Main category: cs.AI

TL;DR: ReThinker是一个基于置信度的智能体框架，通过Solver-Critic-Selector架构动态分配计算资源，在专家级科学推理任务上实现最先进性能。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型在专家级科学推理（如Humanity's Last Exam）上面临挑战，传统工具管道僵化、多智能体协调脆弱、测试时扩展效率低下，限制了性能提升。

Method: 提出ReThinker框架，采用Solver-Critic-Selector三阶段架构，基于模型置信度动态分配计算，实现自适应工具调用、引导式多维度反思和置信度加权选择。同时提出反向数据合成管道和自适应轨迹回收策略，将成功推理轨迹转化为高质量监督数据。

Result: 在HLE、GAIA和XBench基准测试中，ReThinker持续超越现有最先进的工具增强基础模型和深度研究系统，在专家级推理任务上取得最先进结果。

Conclusion: ReThinker通过置信度感知的动态计算分配和创新的数据合成方法，有效解决了专家级科学推理中的关键挑战，为复杂推理任务提供了可扩展的解决方案。

Abstract: Expert-level scientific reasoning remains challenging for large language models, particularly on benchmarks such as Humanity's Last Exam (HLE), where rigid tool pipelines, brittle multi-agent coordination, and inefficient test-time scaling often limit performance. We introduce ReThinker, a confidence-aware agentic framework that orchestrates retrieval, tool use, and multi-agent reasoning through a stage-wise Solver-Critic-Selector architecture. Rather than following a fixed pipeline, ReThinker dynamically allocates computation based on model confidence, enabling adaptive tool invocation, guided multi-dimensional reflection, and robust confidence-weighted selection. To support scalable training without human annotation, we further propose a reverse data synthesis pipeline and an adaptive trajectory recycling strategy that transform successful reasoning traces into high-quality supervision. Experiments on HLE, GAIA, and XBench demonstrate that ReThinker consistently outperforms state-of-the-art foundation models with tools and existing deep research systems, achieving state-of-the-art results on expert-level reasoning tasks.

</details>


### [34] [From Competition to Collaboration: Designing Sustainable Mechanisms Between LLMs and Online Forums](https://arxiv.org/abs/2602.04572)
*Niv Fono,Yftah Ziser,Omer Ben-Porat*

Main category: cs.AI

TL;DR: 本文提出一个序列交互框架来解决生成式AI系统与问答论坛之间的悖论关系：AI依赖论坛数据提升性能，却将用户从论坛吸引走。通过数据驱动的Stack Exchange模拟，证明了激励错配问题，但展示了双方仍能获得理想全信息场景下约一半的效用。


<details>
  <summary>Details</summary>
Motivation: 生成式AI系统面临一个悖论：它们依赖问答论坛产生的数据来提升性能，但同时又将用户从这些论坛吸引走。这种关系可能导致知识共享平台的可持续性危机，需要探索AI系统与人类知识平台之间的协作机制。

Method: 提出了一个序列交互框架，让生成式AI系统向论坛提出问题，论坛可以选择发布其中一些问题。该框架考虑了非货币交换、信息不对称和激励错配等复杂因素。使用真实的Stack Exchange数据和常用LLM进行全面的数据驱动模拟来验证框架。

Result: 实证证明了激励错配的存在，但发现参与者仍能获得理想全信息场景下约一半的效用。这表明尽管存在激励不一致的问题，AI系统与人类知识平台之间仍有可能实现可持续的协作。

Conclusion: 研究结果强调了AI系统与人类知识平台之间可持续协作的潜力，这种协作能够保持有效的知识共享，解决生成式AI依赖论坛数据却分流用户的悖论问题。

Abstract: While Generative AI (GenAI) systems draw users away from (Q&A) forums, they also depend on the very data those forums produce to improve their performance. Addressing this paradox, we propose a framework of sequential interaction, in which a GenAI system proposes questions to a forum that can publish some of them. Our framework captures several intricacies of such a collaboration, including non-monetary exchanges, asymmetric information, and incentive misalignment. We bring the framework to life through comprehensive, data-driven simulations using real Stack Exchange data and commonly used LLMs. We demonstrate the incentive misalignment empirically, yet show that players can achieve roughly half of the utility in an ideal full-information scenario. Our results highlight the potential for sustainable collaboration that preserves effective knowledge sharing between AI systems and human knowledge platforms.

</details>


### [35] [Vibe AIGC: A New Paradigm for Content Generation via Agentic Orchestration](https://arxiv.org/abs/2602.04575)
*Jiaheng Liu,Yuanxing Zhang,Shihao Li,Xinping Lei*

Main category: cs.AI

TL;DR: 论文提出Vibe AIGC新范式，通过智能体编排解决当前生成式AI的意图-执行鸿沟问题，将用户从提示工程师转变为提供"氛围"的指挥官，由元规划器分解为可执行的智能体工作流。


<details>
  <summary>Details</summary>
Motivation: 当前生成式AI受模型中心范式主导，虽然视觉保真度显著提升，但存在"可用性天花板"和意图-执行鸿沟问题——用户高层次意图与随机、黑盒的单次模型输出之间存在根本性差距。

Method: 引入Vibe AIGC范式，受Vibe Coding启发，通过智能体编排实现内容生成。用户作为指挥官提供"氛围"（包含审美偏好、功能逻辑等高层表示），中央元规划器作为系统架构师，将氛围分解为可执行、可验证、自适应的智能体流水线。

Result: Vibe AIGC通过从随机推理向逻辑编排的转变，弥合了人类想象力与机器执行之间的鸿沟，将AI从脆弱的推理引擎转变为稳健的系统级工程伙伴。

Conclusion: 这一范式转变将重新定义人机协作经济，使复杂、长视野数字资产的创作民主化，为生成式AI开辟新的发展方向。

Abstract: For the past decade, the trajectory of generative artificial intelligence (AI) has been dominated by a model-centric paradigm driven by scaling laws. Despite significant leaps in visual fidelity, this approach has encountered a ``usability ceiling'' manifested as the Intent-Execution Gap (i.e., the fundamental disparity between a creator's high-level intent and the stochastic, black-box nature of current single-shot models). In this paper, inspired by the Vibe Coding, we introduce the \textbf{Vibe AIGC}, a new paradigm for content generation via agentic orchestration, which represents the autonomous synthesis of hierarchical multi-agent workflows.
  Under this paradigm, the user's role transcends traditional prompt engineering, evolving into a Commander who provides a Vibe, a high-level representation encompassing aesthetic preferences, functional logic, and etc. A centralized Meta-Planner then functions as a system architect, deconstructing this ``Vibe'' into executable, verifiable, and adaptive agentic pipelines. By transitioning from stochastic inference to logical orchestration, Vibe AIGC bridges the gap between human imagination and machine execution. We contend that this shift will redefine the human-AI collaborative economy, transforming AI from a fragile inference engine into a robust system-level engineering partner that democratizes the creation of complex, long-horizon digital assets.

</details>


### [36] [WideSeek-R1: Exploring Width Scaling for Broad Information Seeking via Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2602.04634)
*Zelai Xu,Zhexuan Xu,Ruize Zhang,Chunyang Zhu,Shi Yu,Weilin Liu,Quanlu Zhang,Wenbo Ding,Chao Yu,Yu Wang*

Main category: cs.AI

TL;DR: 论文提出WideSeek-R1多智能体框架，通过宽度扩展（并行执行）而非深度扩展来解决广泛信息搜索任务，4B参数模型性能媲美671B参数的深度扩展单智能体模型。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型主要关注深度扩展（单智能体解决长时程问题），但随着任务范围扩大，瓶颈从个体能力转向组织能力。现有多智能体系统依赖手工工作流和轮转交互，无法有效并行化工作。

Method: 提出WideSeek-R1框架，采用主智能体-子智能体架构，通过多智能体强化学习训练，使用共享LLM但隔离上下文和专用工具，在2万个广泛信息搜索任务数据集上联合优化主智能体和并行子智能体。

Result: WideSeek-R1-4B在WideSearch基准测试中达到40.0%的项目F1分数，性能与单智能体DeepSeek-R1-671B相当。随着并行子智能体数量增加，性能持续提升，验证了宽度扩展的有效性。

Conclusion: 宽度扩展是多智能体系统解决广泛信息搜索任务的有效方向，通过并行化工作流可以显著提升效率，小参数模型通过良好组织能达到大参数模型的性能。

Abstract: Recent advancements in Large Language Models (LLMs) have largely focused on depth scaling, where a single agent solves long-horizon problems with multi-turn reasoning and tool use. However, as tasks grow broader, the key bottleneck shifts from individual competence to organizational capability. In this work, we explore a complementary dimension of width scaling with multi-agent systems to address broad information seeking. Existing multi-agent systems often rely on hand-crafted workflows and turn-taking interactions that fail to parallelize work effectively. To bridge this gap, we propose WideSeek-R1, a lead-agent-subagent framework trained via multi-agent reinforcement learning (MARL) to synergize scalable orchestration and parallel execution. By utilizing a shared LLM with isolated contexts and specialized tools, WideSeek-R1 jointly optimizes the lead agent and parallel subagents on a curated dataset of 20k broad information-seeking tasks. Extensive experiments show that WideSeek-R1-4B achieves an item F1 score of 40.0% on the WideSearch benchmark, which is comparable to the performance of single-agent DeepSeek-R1-671B. Furthermore, WideSeek-R1-4B exhibits consistent performance gains as the number of parallel subagents increases, highlighting the effectiveness of width scaling.

</details>


### [37] [Agentic AI in Healthcare & Medicine: A Seven-Dimensional Taxonomy for Empirical Evaluation of LLM-based Agents](https://arxiv.org/abs/2602.04813)
*Shubham Vatsal,Harsh Dubey,Aditi Singh*

Main category: cs.AI

TL;DR: 该论文提出了一个七维分类法来系统评估医疗领域LLM智能体的能力现状，通过分析49项研究发现能力实现存在明显不对称性。


<details>
  <summary>Details</summary>
Motivation: 当前医疗领域LLM智能体的研究缺乏统一的评估框架，现有文献多为宽泛综述或单一能力深入分析，无法为医疗工作提供共同参考标准。

Method: 建立七维分类法（认知能力、知识管理、交互模式、适应与学习、安全与伦理、框架类型、核心任务与子任务）共29个子维度，使用明确纳入排除标准和标签规则（完全实现、部分实现、未实现），对49项研究进行系统映射和定量分析。

Result: 研究发现能力实现存在明显不对称：外部知识整合（76%完全实现）普遍，而事件触发激活（92%未实现）和漂移检测与缓解（98%未实现）罕见；多智能体设计（82%完全实现）是主导架构模式；信息中心能力领先，而治疗规划与处方等行动导向领域仍有显著差距（59%未实现）。

Conclusion: 该研究为医疗LLM智能体提供了系统评估框架，揭示了当前能力实现的分布模式，为未来研究方向和能力发展提供了实证基础。

Abstract: Large Language Model (LLM)-based agents that plan, use tools and act has begun to shape healthcare and medicine. Reported studies demonstrate competence on various tasks ranging from EHR analysis and differential diagnosis to treatment planning and research workflows. Yet the literature largely consists of overviews which are either broad surveys or narrow dives into a single capability (e.g., memory, planning, reasoning), leaving healthcare work without a common frame. We address this by reviewing 49 studies using a seven-dimensional taxonomy: Cognitive Capabilities, Knowledge Management, Interaction Patterns, Adaptation & Learning, Safety & Ethics, Framework Typology and Core Tasks & Subtasks with 29 operational sub-dimensions. Using explicit inclusion and exclusion criteria and a labeling rubric (Fully Implemented, Partially Implemented, Not Implemented), we map each study to the taxonomy and report quantitative summaries of capability prevalence and co-occurrence patterns. Our empirical analysis surfaces clear asymmetries. For instance, the External Knowledge Integration sub-dimension under Knowledge Management is commonly realized (~76% Fully Implemented) whereas Event-Triggered Activation sub-dimenison under Interaction Patterns is largely absent (~92% Not Implemented) and Drift Detection & Mitigation sub-dimension under Adaptation & Learning is rare (~98% Not Implemented). Architecturally, Multi-Agent Design sub-dimension under Framework Typology is the dominant pattern (~82% Fully Implemented) while orchestration layers remain mostly partial. Across Core Tasks & Subtasks, information centric capabilities lead e.g., Medical Question Answering & Decision Support and Benchmarking & Simulation, while action and discovery oriented areas such as Treatment Planning & Prescription still show substantial gaps (~59% Not Implemented).

</details>


### [38] [Are AI Capabilities Increasing Exponentially? A Competing Hypothesis](https://arxiv.org/abs/2602.04836)
*Haosen Ge,Hamsa Bastani,Osbert Bastani*

Main category: cs.AI

TL;DR: 该论文反驳METR报告中AI能力呈指数增长的观点，认为数据不支持指数增长，并证明拐点已过，强调现有指数增长预测的脆弱性。


<details>
  <summary>Details</summary>
Motivation: 针对METR报告声称AI能力自2019年以来呈指数增长的观点，作者认为这种预测缺乏数据支持，需要更严谨的分析来揭示现有预测的脆弱性。

Method: 1. 使用METR相同的数据拟合S型曲线，发现拐点已过；2. 提出更复杂的模型，将AI能力分解为基础能力和推理能力，分别分析其改进速率。

Result: 1. 与METR结论相反，S型曲线拟合显示AI能力增长的拐点已经过去；2. 复杂模型支持AI能力将在近期出现拐点的假设。

Conclusion: AI能力并未呈现指数增长，现有关于指数增长的预测是脆弱的。研究目的不是建立自己的严谨预测，而是揭示现有预测的局限性。

Abstract: Rapidly increasing AI capabilities have substantial real-world consequences, ranging from AI safety concerns to labor market consequences. The Model Evaluation & Threat Research (METR) report argues that AI capabilities have exhibited exponential growth since 2019. In this note, we argue that the data does not support exponential growth, even in shorter-term horizons. Whereas the METR study claims that fitting sigmoid/logistic curves results in inflection points far in the future, we fit a sigmoid curve to their current data and find that the inflection point has already passed. In addition, we propose a more complex model that decomposes AI capabilities into base and reasoning capabilities, exhibiting individual rates of improvement. We prove that this model supports our hypothesis that AI capabilities will exhibit an inflection point in the near future. Our goal is not to establish a rigorous forecast of our own, but to highlight the fragility of existing forecasts of exponential growth.

</details>


### [39] [Group-Evolving Agents: Open-Ended Self-Improvement via Experience Sharing](https://arxiv.org/abs/2602.04837)
*Zhaotian Weng,Antonis Antoniades,Deepak Nathani,Zhen Zhang,Xiao Pu,Xin Eric Wang*

Main category: cs.AI

TL;DR: GEA（群体演化智能体）是一种新型开放端自改进范式，将智能体群体作为基本演化单元，实现群体内经验共享和重用，显著提升编码任务性能，超越现有自演化方法并媲美人类设计框架。


<details>
  <summary>Details</summary>
Motivation: 现有开放端自演化方法采用树状结构演化，导致探索多样性利用效率低下，各演化分支孤立，无法有效共享经验。需要一种新范式来克服这些限制，实现更高效的自改进。

Method: 提出群体演化智能体（GEA）范式，将智能体群体作为基本演化单元，在演化过程中实现显式的经验共享和重用。不同于传统树状结构演化，GEA通过群体协作克服孤立分支的限制。

Result: 在编码基准测试中显著优于现有自演化方法（SWE-bench Verified：71.0% vs. 56.7%；Polyglot：88.3% vs. 68.3%），性能匹配或超越顶级人类设计框架。能更有效将早期探索多样性转化为长期进步，在不同编码模型间具有一致可迁移性，修复框架级bug平均只需1.4次迭代（自演化方法需5次）。

Conclusion: GEA通过群体作为演化单元的新范式，实现了更高效的经验共享和探索多样性利用，在编码任务中展现出卓越性能、可迁移性和鲁棒性，为开放端自改进提供了有效解决方案。

Abstract: Open-ended self-improving agents can autonomously modify their own structural designs to advance their capabilities and overcome the limits of pre-defined architectures, thus reducing reliance on human intervention. We introduce Group-Evolving Agents (GEA), a new paradigm for open-ended self-improvements, which treats a group of agents as the fundamental evolutionary unit, enabling explicit experience sharing and reuse within the group throughout evolution. Unlike existing open-ended self-evolving paradigms that adopt tree-structured evolution, GEA overcomes the limitation of inefficient utilization of exploratory diversity caused by isolated evolutionary branches. We evaluate GEA on challenging coding benchmarks, where it significantly outperforms state-of-the-art self-evolving methods (71.0% vs. 56.7% on SWE-bench Verified, 88.3% vs. 68.3% on Polyglot) and matches or exceeds top human-designed agent frameworks (71.8% and 52.0% on two benchmarks, respectively). Analysis reveals that GEA more effectively converts early-stage exploratory diversity into sustained, long-term progress, achieving stronger performance under the same number of evolved agents. Furthermore, GEA exhibits consistent transferability across different coding models and greater robustness, fixing framework-level bugs in 1.4 iterations on average, versus 5 for self-evolving methods.

</details>


### [40] [Fluid Representations in Reasoning Models](https://arxiv.org/abs/2602.04843)
*Dmitrii Kharlapenko,Alessandro Stolfo,Arthur Conmy,Mrinmaya Sachan,Zhijing Jin*

Main category: cs.AI

TL;DR: 该研究发现推理语言模型通过上下文精炼token表示来提升抽象问题解决能力，这种机制被称为"流体推理表示"


<details>
  <summary>Details</summary>
Motivation: 尽管推理语言模型在抽象问题上表现优异，但其内部工作机制仍不清楚。研究者希望理解QwQ-32B模型如何处理抽象结构信息，特别是它如何通过生成长推理链来提升性能。

Method: 在语义混淆的规划领域Mystery Blocksworld上对QwQ-32B进行机制分析，通过转向实验建立因果证据，研究模型在推理过程中如何改进动作和概念的内部表示。

Result: 发现QwQ-32B在推理过程中逐渐改进动作和概念的内部表示，发展出关注结构而非具体动作名称的抽象编码。注入成功轨迹中的精炼表示能提升准确性，而符号表示可以替换许多混淆编码且性能损失最小。

Conclusion: 推理模型性能的一个关键驱动因素是在上下文精炼token表示，即"流体推理表示"。这些适应改进问题解决能力，为理解推理语言模型的工作机制提供了机制性证据。

Abstract: Reasoning language models, which generate long chains of thought, dramatically outperform non-reasoning language models on abstract problems. However, the internal model mechanisms that allow this superior performance remain poorly understood. We present a mechanistic analysis of how QwQ-32B - a model specifically trained to produce extensive reasoning traces - process abstract structural information. On Mystery Blocksworld - a semantically obfuscated planning domain - we find that QwQ-32B gradually improves its internal representation of actions and concepts during reasoning. The model develops abstract encodings that focus on structure rather than specific action names. Through steering experiments, we establish causal evidence that these adaptations improve problem solving: injecting refined representations from successful traces boosts accuracy, while symbolic representations can replace many obfuscated encodings with minimal performance loss. We find that one of the factors driving reasoning model performance is in-context refinement of token representations, which we dub Fluid Reasoning Representations.

</details>
