<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 30]
- [cs.CR](#cs.CR) [Total: 65]
- [cs.AI](#cs.AI) [Total: 72]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [A Comprehensive Survey on Benchmarks and Solutions in Software Engineering of LLM-Empowered Agentic System](https://arxiv.org/abs/2510.09721)
*Jiale Guo,Suizhi Huang,Mei Li,Dong Huang,Xingsheng Chen,Regina Zhang,Zhijiang Guo,Han Yu,Siu-Ming Yiu,Christian Jensen,Pietro Lio,Kwok-Yan Lam*

Main category: cs.SE

TL;DR: 该调查首次对LLM赋能的软件工程进行全面分析，通过分析150多篇论文构建了一个涵盖解决方案和基准测试的完整分类体系，揭示了从简单提示工程到复杂智能体系统的演进路径。


<details>
  <summary>Details</summary>
Motivation: LLM在软件工程中的应用引发了从传统规则系统到自主问题解决智能体系统的范式转变，但该领域缺乏对基准测试与解决方案之间相互关联的全面理解，阻碍了系统性进展和评估。

Method: 分析150多篇近期论文，构建涵盖两大维度的分类体系：解决方案（基于提示、基于微调、基于智能体）和基准测试（代码生成、翻译、修复等任务），提出统一的工作流程管道。

Result: 揭示了该领域从简单提示工程演进到包含规划分解、推理自优化、记忆机制和工具增强的复杂智能体系统，连接了50多个基准测试与对应解决方案策略。

Conclusion: 该调查为理解和推进LLM赋能的软件工程系统提供了基础资源，识别了关键研究空白并提出了多智能体协作框架、自进化代码生成系统等未来方向。

Abstract: The integration of LLMs into software engineering has catalyzed a paradigm
shift from traditional rule-based systems to sophisticated agentic systems
capable of autonomous problem-solving. Despite this transformation, the field
lacks a comprehensive understanding of how benchmarks and solutions
interconnect, hindering systematic progress and evaluation. This survey
presents the first holistic analysis of LLM-empowered software engineering,
bridging the critical gap between evaluation and solution approaches. We
analyze 150+ recent papers and organize them into a comprehensive taxonomy
spanning two major dimensions: (1) Solutions, categorized into prompt-based,
fine-tuning-based, and agent-based paradigms, and (2) Benchmarks, covering code
generation, translation, repair, and other tasks. Our analysis reveals how the
field has evolved from simple prompt engineering to complex agentic systems
incorporating planning and decomposition, reasoning and self-refinement, memory
mechanisms, and tool augmentation. We present a unified pipeline that
illustrates the complete workflow from task specification to final
deliverables, demonstrating how different solution paradigms address varying
complexity levels across software engineering tasks. Unlike existing surveys
that focus on isolated aspects, we provide full-spectrum coverage connecting
50+ benchmarks with their corresponding solution strategies, enabling
researchers to identify optimal approaches for specific evaluation criteria.
Furthermore, we identify critical research gaps and propose actionable future
directions, including multi-agent collaboration frameworks, self-evolving code
generation systems, and integration of formal verification with LLM-based
methods. This survey serves as a foundational resource for researchers and
practitioners seeking to understand, evaluate, and advance LLM-empowered
software engineering systems.

</details>


### [2] [InteractScience: Programmatic and Visually-Grounded Evaluation of Interactive Scientific Demonstration Code Generation](https://arxiv.org/abs/2510.09724)
*Qiaosheng Chen,Yang Liu,Lei Li,Kai Chen,Qipeng Guo,Gong Cheng,Fei Yuan*

Main category: cs.SE

TL;DR: InteractScience是首个评估大语言模型结合科学知识和交互式前端编程能力的基准测试，涵盖五个科学领域，通过程序化功能测试和视觉定性测试来验证模型生成的交互式科学演示代码。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试要么评估知识问答而不涉及代码实现，要么评估静态网页代码生成而缺乏科学交互性，无法衡量模型将准确科学知识与交互式前端代码结合的能力。

Method: 设计了混合评估框架，结合程序化功能测试验证交互逻辑，以及基于视觉的定性测试评估渲染输出与参考快照的匹配度，构建了包含五个科学领域问题的InteractScience基准。

Result: 评估了30个领先的开源和闭源LLM，结果显示模型在整合领域知识与交互式前端编程方面仍存在明显弱点。

Conclusion: InteractScience是首个能够自动测量模型结合科学知识和交互式前端编程能力的基准，为推进可靠且教育有用的科学演示代码生成奠定了基础。

Abstract: Large Language Models (LLMs) are increasingly capable of generating complete
applications from natural language instructions, creating new opportunities in
science and education. In these domains, interactive scientific demonstrations
are particularly valuable for explaining concepts, supporting new teaching
methods, and presenting research findings. Generating such demonstrations
requires models to combine accurate scientific knowledge with the ability to
implement interactive front-end code that behaves correctly and responds to
user actions. This capability goes beyond the scope of existing benchmarks,
which typically evaluate either knowledge question answering without grounding
in code or static web code generation without scientific interactivity. To
evaluate this integrated ability, we design a hybrid framework that combines
programmatic functional testing to rigorously verify interaction logic with
visually-grounded qualitative testing to assess rendered outputs against
reference snapshots. Building on this framework, we present InteractScience, a
benchmark consisting of a substantial set of carefully designed questions
across five scientific domains, each paired with unit tests, reference
snapshots, and checklists. We evaluate 30 leading open- and closed-source LLMs
and report results that highlight ongoing weaknesses in integrating domain
knowledge with interactive front-end coding. Our work positions InteractScience
as the first benchmark to automatically measure this combined capability with
realistic interactive operations, providing a foundation for advancing reliable
and educationally useful scientific demonstration code generation. All code and
data are publicly available at https://github.com/open-compass/InteractScience.

</details>


### [3] [Agentic Property-Based Testing: Finding Bugs Across the Python Ecosystem](https://arxiv.org/abs/2510.09907)
*Muhammad Maaz,Liam DeVoe,Zac Hatfield-Dodds,Nicholas Carlini*

Main category: cs.SE

TL;DR: 开发了一个基于LLM的代理，能够分析Python模块、从代码和文档推断属性、合成并执行基于属性的测试，最终输出可操作的bug报告。在100个流行Python包上的评估显示，56%的bug报告是有效的，32%值得向维护者报告。


<details>
  <summary>Details</summary>
Motivation: 基于属性的测试(PBT)是一种轻量级形式化方法，但传统上需要用户手动指定输入域和属性。本研究旨在利用LLM自动推断属性并执行PBT，实现自动化软件测试。

Method: 构建LLM代理分析Python模块，从代码和文档推断函数特定和跨函数属性，合成并执行基于属性的测试，通过测试输出反思确认真实bug，最终生成可操作的bug报告。

Result: 在100个流行Python包上评估，56%的bug报告有效，32%值得报告给维护者。使用排名标准后，前21个高分bug中86%有效，81%值得报告。报告了5个bug（其中4个附带补丁），3个补丁成功合并。

Conclusion: LLM与PBT结合提供了一种严谨且可扩展的自主软件测试方法，能够发现从序列化失败到数值精度错误等多种类型的bug。

Abstract: Property-based testing (PBT) is a lightweight formal method, typically
implemented as a randomized testing framework. Users specify the input domain
for their test using combinators supplied by the PBT framework, and the
expected properties or invariants as a unit-test function. The framework then
searches for a counterexample, e.g. by generating inputs and calling the test
function. In this work, we demonstrate an LLM-based agent which analyzes Python
modules, infers function-specific and cross-function properties from code and
documentation, synthesizes and executes PBTs, reflects on outputs of these
tests to confirm true bugs, and finally outputs actionable bug reports for the
developer. We perform an extensive evaluation of our agent across 100 popular
Python packages. Of the bug reports generated by the agent, we found after
manual review that 56\% were valid bugs and 32\% were valid bugs that we would
report to maintainers. We then developed a ranking rubric to surface
high-priority valid bugs to developers, and found that of the 21 top-scoring
bugs, 86\% were valid and 81\% we would report. The bugs span diverse failure
modes from serialization failures to numerical precision errors to flawed cache
implementations. We reported 5 bugs, 4 with patches, including to NumPy and
cloud computing SDKs, with 3 patches merged successfully. Our results suggest
that LLMs with PBT provides a rigorous and scalable method for autonomously
testing software. Our code and artifacts are available at:
https://github.com/mmaaz-git/agentic-pbt.

</details>


### [4] [OFP-Repair: Repairing Floating-point Errors via Original-Precision Arithmetic](https://arxiv.org/abs/2510.09938)
*Youshuai Tan,Zishuo Ding,Jinfu Chen,Weiyi Shang*

Main category: cs.SE

TL;DR: 提出了一种名为OFP-Repair的新方法，用于修复浮点程序错误，能够区分并修复可通过原始精度算术修复的错误和需要高精度计算的错误。


<details>
  <summary>Details</summary>
Motivation: 浮点程序错误在军事、航空航天和金融等关键领域可能导致严重后果，现有修复工具要么需要高精度实现（开发耗时且需要专业知识），要么只能修复有限类型的错误或产生次优结果。

Method: OFP-Repair方法能够自动区分可通过原始精度算术修复的错误和需要高精度计算的错误，并提供相应的修复方案。

Result: 在ACESO数据集上，修复后的程序在四个精度指标上分别提高了3、7、3和8个数量级。在真实案例中成功检测到所有5个可通过原始精度修复的错误并修复了3个，而ACESO仅修复了1个。在GSL库中成功修复了15个bug中的5个。

Conclusion: 该方法具有实际应用价值，GSL开发者已表示有兴趣将该工具集成到他们的开发工作流中，结果令人鼓舞。

Abstract: Errors in floating-point programs can lead to severe consequences,
particularly in critical domains such as military, aerospace, and financial
systems, making their repair a crucial research problem. In practice, some
errors can be fixed using original-precision arithmetic, while others require
high-precision computation. Developers often avoid addressing the latter due to
excessive computational resources required. However, they sometimes struggle to
distinguish between these two types of errors, and existing repair tools fail
to assist in this differentiation. Most current repair tools rely on
high-precision implementations, which are time-consuming to develop and demand
specialized expertise. Although a few tools do not require high-precision
programs, they can only fix a limited subset of errors or produce suboptimal
results.
  To address these challenges, we propose a novel method, named OFP-Repair.On
ACESO's dataset, our patches achieve improvements of three, seven, three, and
eight orders of magnitude across four accuracy metrics. In real-world cases,
our method successfully detects all five original-precision-repairable errors
and fixes three, whereas ACESO only repairs one. Notably, these results are
based on verified data and do not fully capture the potential of OFP-Repair. To
further validate our method, we deploy it on a decade-old open bug report from
GNU Scientific Library (GSL), successfully repairing five out of 15 bugs. The
developers have expressed interest in our method and are considering
integrating our tool into their development workflow. We are currently working
on applying our patches to GSL. The results are highly encouraging,
demonstrating the practical applicability of our technique.

</details>


### [5] [Operationalizing AI: Empirical Evidence on MLOps Practices, User Satisfaction, and Organizational Context](https://arxiv.org/abs/2510.09968)
*Stefan Pasch*

Main category: cs.SE

TL;DR: 该研究通过分析8000多个AI开发平台的用户评论，发现7个MLOps实践与用户满意度显著正相关，表明有效的MLOps实施为AI开发带来实际价值。


<details>
  <summary>Details</summary>
Motivation: 虽然MLOps作为整合软件工程原则与机器学习生命周期管理的实践已经出现，但关于这些实践是否以及如何支持用户开发和运营AI应用的实证证据仍然有限。

Method: 本研究分析了来自G2.com的8000多个AI开发平台用户评论，使用零样本分类方法测量了用户对9个已建立的MLOps实践的情感态度。

Result: 9个MLOps实践中有7个与用户满意度呈现显著正相关关系，表明有效的MLOps实施为AI开发带来实际价值。组织背景也很重要：小公司的评论者较少讨论某些MLOps实践，但公司规模并不调节MLOps与满意度之间的关系。

Conclusion: 一旦应用，MLOps实践在不同组织环境中都被认为是普遍有益的，组织背景影响MLOps的普及和显著性，但不影响其价值实现。

Abstract: Organizational efforts to utilize and operationalize artificial intelligence
(AI) are often accompanied by substantial challenges, including scalability,
maintenance, and coordination across teams. In response, the concept of Machine
Learning Operations (MLOps) has emerged as a set of best practices that
integrate software engineering principles with the unique demands of managing
the ML lifecycle. Yet, empirical evidence on whether and how these practices
support users in developing and operationalizing AI applications remains
limited. To address this gap, this study analyzes over 8,000 user reviews of AI
development platforms from G2.com. Using zero-shot classification, we measure
review sentiment toward nine established MLOps practices, including continuous
integration and delivery (CI/CD), workflow orchestration, reproducibility,
versioning, collaboration, and monitoring. Seven of the nine practices show a
significant positive relationship with user satisfaction, suggesting that
effective MLOps implementation contributes tangible value to AI development.
However, organizational context also matters: reviewers from small firms
discuss certain MLOps practices less frequently, suggesting that organizational
context influences the prevalence and salience of MLOps, though firm size does
not moderate the MLOps-satisfaction link. This indicates that once applied,
MLOps practices are perceived as universally beneficial across organizational
settings.

</details>


### [6] [SLEAN: Simple Lightweight Ensemble Analysis Network for Multi-Provider LLM Coordination: Design, Implementation, and Vibe Coding Bug Investigation Case Study](https://arxiv.org/abs/2510.10010)
*Matheus J. T. Vargas*

Main category: cs.SE

TL;DR: SLEAN是一个通过文本提示编排协调多个LLM提供商的确定性框架，使用简单的.txt模板作为LLM之间的提示桥接，无需专业技术知识即可部署。


<details>
  <summary>Details</summary>
Motivation: 解决AI辅助调试时产生的代码修改可能引入不必要复杂性、破坏现有功能或解决错误问题的情况，在部署前过滤有害的AI生成代码建议。

Method: 采用三阶段协议：独立分析、交叉批评和仲裁，通过文件驱动、提供商无关的架构协调多个LLM提供商。

Result: 评估15个软件错误中的69个AI生成修复建议，SLEAN过滤接受了22个修复（31.9%），拒绝了47个有害修复。仲裁过程将代码变更范围减少了83-90%。

Conclusion: SLEAN提供了一种无需专业编码知识即可部署的可靠多提供商合成框架，适用于安全审计、代码审查、文档验证等领域，具有端到端的可审计性。

Abstract: We present SLEAN (Simple Lightweight Ensemble Analysis Network), a
deterministic framework for coordinating multiple LLM providers through
text-based prompt orchestration. Unlike complex multi-agent systems requiring
specialized infrastructure, SLEAN operates as a simple prompt bridge between
LLMs using .txt templates, requiring no deep technical knowledge for
deployment. The three-phase protocol formed by independent analysis,
cross-critique, and arbitration, filters harmful AI-generated code suggestions
before production deployment, addressing how AI-assisted debugging increasingly
produces modifications that introduce unnecessary complexity, break existing
functionality, or address problems. Evaluating 15 software bugs, we analyzed 69
AI-generated fix propositions. SLEAN's filtering accepted 22 fixes (31.9%, 95%
CI 20.9-42.9%) while rejecting 47 that would have been harmful if applied
verbatim. The arbitration process reduced code change surface by 83-90%
relative to raw AI outputs, enforcing minimal causal edits over scope-expanding
modifications. Minimal Type 2 inputs proved more efficient than detailed Type 1
inputs, requiring 2.85 versus 3.56 propositions per accepted fix (35.1% versus
28.1% acceptance, about a 20% efficiency gain). Agreement between AI systems
showed weak correlation with fix quality: high convergence (at least 80%)
occurred in 4 of 15 cases and improved acceptance by only 2.4% points;
arbitration appeared only at exactly 10% convergence in 2 of 15 cases, although
low convergence alone did not necessitate arbitration. The file-driven,
provider-agnostic architecture enables deployment without specialized coding
expertise, making it applicable to security auditing, code review, document
verification, and other domains requiring reliable multi-provider synthesis
with end-to-end auditability.

</details>


### [7] [OBsmith: Testing JavaScript Obfuscator using LLM-powered sketching](https://arxiv.org/abs/2510.10066)
*Shan Jiang,Chenguang Zhu,Sarfraz Khurshid*

Main category: cs.SE

TL;DR: OBsmith是一个使用大语言模型系统测试JavaScript混淆器正确性的框架，发现了11个未知错误，优于现有模糊测试工具


<details>
  <summary>Details</summary>
Motivation: JavaScript混淆器被广泛用于保护知识产权，但其正确性一直被忽视。不正确的转换可能改变程序语义，破坏可靠性和安全性

Method: 利用LLM生成程序草图模板，实例化为可执行程序，在不同配置下进行混淆测试；同时从真实程序中自动提取草图进行针对性测试

Result: 发现了11个之前未知的正确性错误，在相同程序预算下，5个最先进的JavaScript模糊测试工具未能检测到这些问题

Conclusion: OBsmith是自动化测试混淆器和其他语义保持工具链质量的重要进展，需要平衡混淆预设和性能成本

Abstract: JavaScript obfuscators are widely deployed to protect intellectual property
and resist reverse engineering, yet their correctness has been largely
overlooked compared to performance and resilience. Existing evaluations
typically measure resistance to deobfuscation, leaving the critical question of
whether obfuscators preserve program semantics unanswered. Incorrect
transformations can silently alter functionality, compromise reliability, and
erode security-undermining the very purpose of obfuscation. To address this
gap, we present OBsmith, a novel framework to systematically test JavaScript
obfuscators using large language models (LLMs). OBsmith leverages LLMs to
generate program sketches abstract templates capturing diverse language
constructs, idioms, and corner cases-which are instantiated into executable
programs and subjected to obfuscation under different configurations. Besides
LLM-powered sketching, OBsmith also employs a second source: automatic
extraction of sketches from real programs. This extraction path enables more
focused testing of project specific features and lets developers inject domain
knowledge into the resulting test cases. OBsmith uncovers 11 previously unknown
correctness bugs. Under an equal program budget, five general purpose
state-of-the-art JavaScript fuzzers (FuzzJIT, Jsfunfuzz, Superion, DIE,
Fuzzilli) failed to detect these issues, highlighting OBsmith's complementary
focus on obfuscation induced misbehavior. An ablation shows that all components
except our generic MRs contribute to at least one bug class; the negative MR
result suggests the need for obfuscator-specific metamorphic relations. Our
results also seed discussion on how to balance obfuscation presets and
performance cost. We envision OBsmith as an important step towards automated
testing and quality assurance of obfuscators and other semantic-preserving
toolchains.

</details>


### [8] [A Mathematics-Guided Approach to Floating-Point Error Detection](https://arxiv.org/abs/2510.10081)
*Youshuai Tan,Zhanwei Zhang,Zishuo Ding,Lianyu Zheng,Jinfu Chen,Weiyi Shang*

Main category: cs.SE

TL;DR: 提出了一种基于数学指导的MGDE方法，用于高效检测浮点程序中的错误诱导输入，相比现有方法FPCC，在检测更多bug的同时显著降低了计算时间。


<details>
  <summary>Details</summary>
Motivation: 浮点程序错误在关键领域可能造成严重后果，现有方法存在计算成本高和搜索效率低的问题，需要更有效的错误输入检测方法。

Method: 使用具有二次收敛特性的牛顿-拉弗森方法，通过数学指导实现高效搜索错误诱导输入。

Result: 在88个单输入浮点程序中，MGDE检测到44个程序中的89个bug，而FPCC仅检测到29个程序中的48个bug；MGDE的计算时间仅为FPCC的1/6.4096。在多输入程序中，MGDE检测到9个bug，平均检测时间0.6443秒，而FPCC未能检测到任何bug且需要100秒平均计算时间。

Conclusion: MGDE方法在浮点程序错误检测方面显著优于现有方法，不仅检测到更多bug，而且计算效率大幅提升，特别适用于多输入程序的错误检测。

Abstract: Floating-point program errors can lead to severe consequences, particularly
in critical domains such as military applications. Only a small subset of
inputs may induce substantial floating-point errors, prompting researchers to
develop methods for identifying these error-inducing inputs. Although existing
approaches have achieved some success, they still suffer from two major
limitations: (1) High computational cost: The evaluation of error magnitude for
candidate inputs relies on high-precision programs, which are prohibitively
time-consuming. (2) Limited long-range convergence capability: Current methods
exhibit inefficiency in search, making the process akin to finding a needle in
a haystack.
  To address these two limitations, we propose a novel method, named MGDE, to
detect error-inducing inputs based on mathematical guidance. By employing the
Newton-Raphson method, which exhibits quadratic convergence properties, we
achieve highly effective and efficient results. Since the goal of identifying
error-inducing inputs is to uncover the underlying bugs, we use the number of
bugs detected in floating-point programs as the primary evaluation metric in
our experiments. As FPCC represents the most effective state-of-the-art
approach to date, we use it as the baseline for comparison. The dataset of FPCC
consists of 88 single-input floating-point programs. FPCC is able to detect 48
bugs across 29 programs, whereas our method successfully identifies 89 bugs
across 44 programs. Moreover, FPCC takes 6.4096 times as long as our proposed
method. We also deploy our method to multi-input programs, identifying a total
of nine bugs with an average detection time of 0.6443 seconds per program. In
contrast, FPCC fails to detect any bugs while requiring an average computation
time of 100 seconds per program.

</details>


### [9] [IntrinTrans: LLM-based Intrinsic Code Translator for RISC-V Vector](https://arxiv.org/abs/2510.10119)
*Liutong Han,Zhiyuan Tan,Hongbin Zhang,Pengcheng Wang,Chu Kang,Mingjie Xing,Yanjun Wu*

Main category: cs.SE

TL;DR: IntrinTrans是一个基于LLM的多智能体方法，利用编译测试反馈自动跨架构翻译SIMD内在函数代码，并通过活跃分析优化RISC-V Vector内在函数性能。


<details>
  <summary>Details</summary>
Motivation: 随着RISC-V生态系统的快速发展，对RISC-V Vector扩展支持的需求日益增长。现有跨架构翻译主要依赖人工重写，耗时且易错，而基于规则的方法受限于规则覆盖率和语法约束。

Method: 使用基于LLM的多智能体方法，结合编译测试反馈进行自动翻译，并通过活跃分析获取寄存器使用信息来优化生成的RVV内在函数。

Result: 实验收集了34个开源库中的向量化算法案例，显示高级LLM在有限迭代次数内能生成语义正确的RISC-V Vector内在函数，某些情况下性能达到开源社区原生实现的5.93倍。

Conclusion: IntrinTrans方法能有效自动翻译和优化跨架构内在函数代码，显著提升RISC-V Vector内在函数的性能和开发效率。

Abstract: The use of intrinsic functions to exploit hardware-specific capabilities is
an important approach for optimizing library performance. Many mainstream
libraries implement a large number of vectorized algorithms on Arm or x86 SIMD
intrinsic functions. With the rapid expansion of the RISC-V hardware-software
ecosystem, there is a growing demand for support of the RISC-V Vector (RVV)
extension. Translating existing vectorized intrinsic code onto RVV intrinsics
is a practical and effective approach. However, current cross-architecture
translation largely relies on manual rewriting, which is time-consuming and
error-prone. Furthermore, while some rule-based methods can reduce the need for
manual intervention, their translation success rate is limited by incomplete
rule coverage and syntactic constraints, and the performance suffers from
inadequate utilization of RVV-specific features. We present IntrinTrans, a
LLM-based multi-agent approach that utilizes compile-and-test feedback to
translate intrinsic code across architectures automatically, and further
optimizes the generated RVV intrinsics using register-usage information derived
from liveness analysis. To evaluate the effectiveness of our approach, we
collected 34 vectorized algorithm cases from open-source libraries. Each case
includes an Arm Neon intrinsics implementation and a RVV intrinsics
implementation contributed by the open-source community, together with
correctness and performance tests. Our experiments show that advanced LLMs
produce semantically correct RISC-V Vector intrinsics in most cases within a
limited number of iterations, and in some cases achieve up to 5.93x the
performance of the native implementation from the open-source community.

</details>


### [10] [A Systematic Study on Generating Web Vulnerability Proof-of-Concepts Using Large Language Models](https://arxiv.org/abs/2510.10148)
*Mengyao Zhao,Kaixuan Li,Lyuye Zhang,Wenjing Dang,Chenggong Ding,Sen Chen,Zheli Liu*

Main category: cs.SE

TL;DR: 本文是第一项关于LLM生成Web应用漏洞PoC的实证研究，评估GPT-4o和DeepSeek-R1在100个真实CVE上的表现，发现LLM仅使用公开数据就能在8%-34%的情况下生成有效PoC，DeepSeek-R1表现更优。


<details>
  <summary>Details</summary>
Motivation: 随着LLM在代码理解方面的进步，以及CVE公开信息的丰富，研究LLM能否利用这些信息自动生成有效的概念验证(PoC)漏洞利用代码，这对漏洞复现、理解和缓解具有重要意义。

Method: 评估GPT-4o和DeepSeek-R1在100个真实可复现CVE上的PoC生成能力，涵盖三个漏洞披露阶段：仅有描述的新漏洞、有补丁的1日漏洞、有完整代码上下文的N日漏洞。

Result: LLM仅使用公开数据就能在8%-34%的情况下生成有效PoC，DeepSeek-R1表现优于GPT-4o。补充代码上下文可将成功率提高17%-20%，函数级上下文比文件级提高9%-13%。集成自适应推理策略后成功率显著提升至68%-72%。

Conclusion: LLM能够重塑漏洞利用的动态格局，目前已有23个新生成的PoC被NVD和Exploit DB接受，表明LLM在自动生成有效PoC方面具有实际可行性。

Abstract: Recent advances in Large Language Models (LLMs) have brought remarkable
progress in code understanding and reasoning, creating new opportunities and
raising new concerns for software security. Among many downstream tasks,
generating Proof-of-Concept (PoC) exploits plays a central role in
vulnerability reproduction, comprehension, and mitigation. While previous
research has focused primarily on zero-day exploitation, the growing
availability of rich public information accompanying disclosed CVEs leads to a
natural question: can LLMs effectively use this information to automatically
generate valid PoCs? In this paper, we present the first empirical study of
LLM-based PoC generation for web application vulnerabilities, focusing on the
practical feasibility of leveraging publicly disclosed information. We evaluate
GPT-4o and DeepSeek-R1 on 100 real-world and reproducible CVEs across three
stages of vulnerability disclosure: (1) newly disclosed vulnerabilities with
only descriptions, (2) 1-day vulnerabilities with patches, and (3) N-day
vulnerabilities with full contextual code. Our results show that LLMs can
automatically generate working PoCs in 8%-34% of cases using only public data,
with DeepSeek-R1 consistently outperforming GPT-4o. Further analysis shows that
supplementing code context improves success rates by 17%-20%, with
function-level providing 9%-13% improvement than file-level ones. Further
integrating adaptive reasoning strategies to prompt refinement significantly
improves success rates to 68%-72%. Our findings suggest that LLMs could reshape
vulnerability exploitation dynamics. To date, 23 newly generated PoCs have been
accepted by NVD and Exploit DB.

</details>


### [11] [LLMs are All You Need? Improving Fuzz Testing for MOJO with Large Language Models](https://arxiv.org/abs/2510.10179)
*Linghan Huang,Peizhou Zhao,Huaming Chen*

Main category: cs.SE

TL;DR: MOJOFuzzer是首个针对新兴编程语言零样本学习环境的自适应LLM模糊测试框架，通过多阶段过滤和动态提示调整，显著提高了测试用例的有效性和错误检测性能，在MOJO语言中发现了13个未知bug。


<details>
  <summary>Details</summary>
Motivation: MOJO作为新兴高性能AI编程语言缺乏完善的测试框架和语料库，导致LLM在模糊测试中产生语法正确但语义错误的代码，降低了测试效果。

Method: 提出MOJOFuzzer框架，集成多阶段过滤机制消除低质量输入，并基于运行时反馈动态调整LLM提示进行测试用例变异，实现迭代学习过程。

Result: 实验结果表明MOJOFuzzer显著提升了测试有效性、API覆盖率和错误检测性能，优于传统模糊测试和最先进的LLM模糊测试方法，发现了13个未知bug。

Conclusion: 该研究不仅推进了LLM驱动的软件测试领域，还为在新兴编程语言测试中利用LLM建立了基础方法论。

Abstract: The rapid development of large language models (LLMs) has revolutionized
software testing, particularly fuzz testing, by automating the generation of
diverse and effective test inputs. This advancement holds great promise for
improving software reliability. Meanwhile, the introduction of MOJO, a
high-performance AI programming language blending Python's usability with the
efficiency of C and C++, presents new opportunities to enhance AI model
scalability and programmability. However, as a new language, MOJO lacks
comprehensive testing frameworks and a sufficient corpus for LLM-based testing,
which exacerbates model hallucination. In this case, LLMs will generate
syntactically valid but semantically incorrect code, significantly reducing the
effectiveness of fuzz testing. To address this challenge, we propose
MOJOFuzzer, the first adaptive LLM-based fuzzing framework designed for
zero-shot learning environments of emerging programming languages. MOJOFuzzer
integrates a mutil-phase framework that systematically eliminates low-quality
generated inputs before execution, significantly improving test case validity.
Furthermore, MOJOFuzzer dynamically adapts LLM prompts based on runtime
feedback for test case mutation, enabling an iterative learning process that
continuously enhances fuzzing efficiency and bug detection performance. Our
experimental results demonstrate that MOJOFuzzer significantly enhances test
validity, API coverage, and bug detection performance, outperforming
traditional fuzz testing and state-of-the-art LLM-based fuzzing approaches.
Using MOJOFuzzer, we have conducted a first large-scale fuzz testing evaluation
of MOJO, uncorvering 13 previous unknown bugs. This study not only advances the
field of LLM-driven software testing but also establishes a foundational
methodology for leveraging LLMs in the testing of emerging programming
languages.

</details>


### [12] [Grounded AI for Code Review: Resource-Efficient Large-Model Serving in Enterprise Pipelines](https://arxiv.org/abs/2510.10290)
*Sayan Mandal,Hua Jiang*

Main category: cs.SE

TL;DR: 提出了一种基于静态分析和AST引导的自动化代码审查系统，在合规性要求高的环境中提供快速、准确的代码审查反馈，相比大型专有模型具有更低的违规率和成本。


<details>
  <summary>Details</summary>
Motivation: 在合规性要求高的环境中，传统静态分析工具产生大量低解释性的输出，而直接使用LLM存在幻觉风险和成本过高的问题，需要一种更可靠、高效的自动化代码审查方案。

Method: 结合静态分析结果与AST引导的上下文提取，采用量化开源模型和分层缓存的服务架构，实现快速、准确的代码审查反馈。

Result: 在安全导向的C/C++标准评估中，系统实现了中位数59.8秒的首次反馈时间，相比大型专有模型具有更低的违规率和相似的违规减少效果。

Conclusion: 该架构具有解耦特性，团队可以独立采用其基础层或服务层，内部调查显示能减少人工审查迭代次数，强调了可重现性、可审计性和向更广泛标准扩展的路径。

Abstract: Automated code review adoption lags in compliance-heavy settings, where
static analyzers produce high-volume, low-rationale outputs, and naive LLM use
risks hallucination and incurring cost overhead. We present a production system
for grounded, PR-native review that pairs static-analysis findings with
AST-guided context extraction and a single-GPU, on-demand serving stack
(quantized open-weight model, multi-tier caching) to deliver concise
explanations and remediation guidance. Evaluated on safety-oriented C/C++
standards, the approach achieves sub-minute median first-feedback (offline p50
build+LLM 59.8s) while maintaining competitive violation reduction and lower
violation rates versus larger proprietary models. The architecture is
decoupled: teams can adopt the grounding/prompting layer or the serving layer
independently. A small internal survey (n=8) provides directional signals of
reduced triage effort and moderate perceived grounding, with participants
reporting fewer human review iterations. We outline operational lessons and
limitations, emphasizing reproducibility, auditability, and pathways to broader
standards and assisted patching.

</details>


### [13] [Prepared for the Unknown: Adapting AIOps Capacity Forecasting Models to Data Changes](https://arxiv.org/abs/2510.10320)
*Lorena Poenaru-Olaru,Wouter van 't Hof,Adrian Stando,Arkadiusz P. Trawinski,Eileen Kapel,Jan S. Rellermeyer,Luis Cruz,Arie van Deursen*

Main category: cs.SE

TL;DR: 比较基于数据漂移检测的重新训练与定期重新训练在容量预测模型中的效果，发现漂移检测方法在大多数情况下能达到相似精度但更经济，只有在数据快速变化时定期重新训练才更优。


<details>
  <summary>Details</summary>
Motivation: 容量管理中频繁重新训练预测模型成本高昂且难以扩展，需要寻找更有效的重新训练策略来平衡准确性和效率。

Method: 研究基于数据变化检测的重新训练策略与定期重新训练策略在时间序列容量预测模型中的效果对比。

Result: 漂移检测重新训练在大多数情况下能达到与定期重新训练相当的预测精度，但在数据快速变化时定期重新训练精度更高。

Conclusion: 漂移检测重新训练是成本效益更高的策略，但在数据快速变化场景下仍需采用定期重新训练来最大化预测精度。

Abstract: Capacity management is critical for software organizations to allocate
resources effectively and meet operational demands. An important step in
capacity management is predicting future resource needs often relies on
data-driven analytics and machine learning (ML) forecasting models, which
require frequent retraining to stay relevant as data evolves. Continuously
retraining the forecasting models can be expensive and difficult to scale,
posing a challenge for engineering teams tasked with balancing accuracy and
efficiency. Retraining only when the data changes appears to be a more
computationally efficient alternative, but its impact on accuracy requires
further investigation. In this work, we investigate the effects of retraining
capacity forecasting models for time series based on detected changes in the
data compared to periodic retraining. Our results show that drift-based
retraining achieves comparable forecasting accuracy to periodic retraining in
most cases, making it a cost-effective strategy. However, in cases where data
is changing rapidly, periodic retraining is still preferred to maximize the
forecasting accuracy. These findings offer actionable insights for software
teams to enhance forecasting systems, reducing retraining overhead while
maintaining robust performance.

</details>


### [14] [Bridging Semantics & Structure for Software Vulnerability Detection using Hybrid Network Models](https://arxiv.org/abs/2510.10321)
*Jugal Gajjar,Kaustik Ranaware,Kamalasankari Subramaniakuppusamy*

Main category: cs.SE

TL;DR: 提出了一种结合异构图表示与轻量级本地LLM的混合框架，用于软件漏洞检测，在Java漏洞检测中达到93.57%的准确率，比现有方法提升显著。


<details>
  <summary>Details</summary>
Motivation: 现有静态和动态分析方法往往忽略影响不安全行为的结构依赖关系，需要一种能同时捕捉拓扑特征和语义推理的方法，同时避免大型云模型的成本和隐私问题。

Method: 将程序建模为异构图，捕捉控制流和数据流关系作为复杂交互网络，结合轻量级(<4B)本地LLM，统一拓扑特征与语义推理。

Result: 在Java漏洞检测(二元分类)中达到93.57%准确率，比基于图注意力网络的嵌入方法提升8.36%，比预训练LLM基线(Qwen2.5 Coder 3B)提升17.81%。

Conclusion: 该方法为可扩展、可解释且可本地部署的工具铺平了道路，能够将漏洞分析从纯语法检查转向更深层次的结构和语义洞察，促进在实际安全软件开发中的广泛应用。

Abstract: Software vulnerabilities remain a persistent risk, yet static and dynamic
analyses often overlook structural dependencies that shape insecure behaviors.
Viewing programs as heterogeneous graphs, we capture control- and data-flow
relations as complex interaction networks. Our hybrid framework combines these
graph representations with light-weight (<4B) local LLMs, uniting topological
features with semantic reasoning while avoiding the cost and privacy concerns
of large cloud models. Evaluated on Java vulnerability detection (binary
classification), our method achieves 93.57% accuracy-an 8.36% gain over Graph
Attention Network-based embeddings and 17.81% over pretrained LLM baselines
such as Qwen2.5 Coder 3B. Beyond accuracy, the approach extracts salient
subgraphs and generates natural language explanations, improving
interpretability for developers. These results pave the way for scalable,
explainable, and locally deployable tools that can shift vulnerability analysis
from purely syntactic checks to deeper structural and semantic insights,
facilitating broader adoption in real-world secure software development.

</details>


### [15] [Testing and Enhancing Multi-Agent Systems for Robust Code Generation](https://arxiv.org/abs/2510.10460)
*Zongyi Lyu,Songqiang Chen,Zhenlan Ji,Liwen Wang,Shuai Wang,Daoyuan Wu,Wenxuan Wang,Shing-Chi Cheung*

Main category: cs.SE

TL;DR: 本文首次全面研究了多智能体系统在代码生成中的鲁棒性问题，通过模糊测试方法发现主流MAS存在严重缺陷，并提出修复方法有效提升了系统鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 多智能体系统在代码生成中表现出色，但其鲁棒性尚未得到充分研究，这对实际部署构成关键隐患。

Method: 采用基于模糊测试的方法，设计包含语义保持变异算子和新型适应度函数的测试流程，评估主流MAS在不同数据集和LLM上的表现。

Result: 研究发现各种流行MAS存在严重的鲁棒性缺陷：在应用语义保持变异后，它们无法解决最初成功解决的7.9%-83.3%的问题。

Conclusion: 工作揭示了MAS在代码生成中的关键鲁棒性缺陷，并提供了有效的缓解策略，为开发更可靠的代码生成MAS提供了重要见解。

Abstract: Multi-agent systems (MASs) have emerged as a promising paradigm for automated
code generation, demonstrating impressive performance on established benchmarks
by decomposing complex coding tasks across specialized agents with different
roles. Despite their prosperous development and adoption, their robustness
remains pressingly under-explored, raising critical concerns for real-world
deployment. This paper presents the first comprehensive study examining the
robustness of MASs for code generation through a fuzzing-based testing
approach. By designing a fuzzing pipeline incorporating semantic-preserving
mutation operators and a novel fitness function, we assess mainstream MASs
across multiple datasets and LLMs. Our findings reveal substantial robustness
flaws of various popular MASs: they fail to solve 7.9%-83.3% of problems they
initially resolved successfully after applying the semantic-preserving
mutations. Through comprehensive failure analysis, we identify a common yet
largely overlooked cause of the robustness issue: miscommunications between
planning and coding agents, where plans lack sufficient detail and coding
agents misinterpret intricate logic, aligning with the challenges inherent in a
multi-stage information transformation process. Accordingly, we also propose a
repairing method that encompasses multi-prompt generation and introduces a new
monitor agent to address this issue. Evaluation shows that our repairing method
effectively enhances the robustness of MASs by solving 40.0%-88.9% of
identified failures. Our work uncovers critical robustness flaws in MASs and
provides effective mitigation strategies, contributing essential insights for
developing more reliable MASs for code generation.

</details>


### [16] [How Students Use Generative AI for Software Testing: An Observational Study](https://arxiv.org/abs/2510.10551)
*Baris Ardic,Quentin Le Dilavrec,Andy Zaidman*

Main category: cs.SE

TL;DR: 研究新手开发者在单元测试工程中使用生成式AI的交互策略、依赖程度及感知的优缺点，发现四种交互策略和两种提示风格，AI能节省时间但存在信任和质量问题。


<details>
  <summary>Details</summary>
Motivation: 生成式AI工具在软件工程中的集成改变了开发者角色，特别是对新手开发者，引发了关于控制、输出质量和学习效果的担忧。

Method: 对12名本科学生进行观察研究，分析他们使用生成式AI进行单元测试任务的交互策略和提示风格。

Result: 识别出四种交互策略和两种提示风格，学生报告了节省时间、减少认知负担等好处，但也存在信任度下降、测试质量担忧等问题。策略和提示风格不影响测试有效性或代码质量。

Conclusion: 生成式AI在单元测试中能提供效率优势，但需要解决信任和质量问题，策略选择不影响最终测试效果。

Abstract: The integration of generative AI tools like ChatGPT into software engineering
workflows opens up new opportunities to boost productivity in tasks such as
unit test engineering. However, these AI-assisted workflows can also
significantly alter the developer's role, raising concerns about control,
output quality, and learning, particularly for novice developers. This study
investigates how novice software developers with foundational knowledge in
software testing interact with generative AI for engineering unit tests. Our
goal is to examine the strategies they use, how heavily they rely on generative
AI, and the benefits and challenges they perceive when using generative
AI-assisted approaches for test engineering. We conducted an observational
study involving 12 undergraduate students who worked with generative AI for
unit testing tasks. We identified four interaction strategies, defined by
whether the test idea or the test implementation originated from generative AI
or the participant. Additionally, we singled out prompting styles that focused
on one-shot or iterative test generation, which often aligned with the broader
interaction strategy. Students reported benefits including time-saving, reduced
cognitive load, and support for test ideation, but also noted drawbacks such as
diminished trust, test quality concerns, and lack of ownership. While strategy
and prompting styles influenced workflow dynamics, they did not significantly
affect test effectiveness or test code quality as measured by mutation score or
test smells.

</details>


### [17] [Generative AI and the Transformation of Software Development Practices](https://arxiv.org/abs/2510.10819)
*Vivek Acharya*

Main category: cs.SE

TL;DR: 本文探讨生成式AI如何改变软件工程实践，分析AI辅助开发技术带来的机遇与挑战，并提出负责任使用AI的新角色、技能和最佳实践。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型的发展，生成式AI正在重塑软件设计、编写和维护方式，需要研究这些新技术如何改变软件工程实践以及相关的信任、责任和技能转变问题。

Method: 通过案例研究和行业数据，调查迭代式聊天开发、多智能体系统、动态提示编排以及通过模型上下文协议(MCP)的集成。

Result: 发现AI辅助开发可以加速开发周期、普及编程能力，但也面临模型可靠性和成本等挑战。

Conclusion: 需要建立新的角色、技能和最佳实践，以负责任和有效的方式在软件工程中使用AI技术。

Abstract: Generative AI is reshaping how software is designed, written, and maintained.
Advances in large language models (LLMs) are enabling new development styles -
from chat-oriented programming and 'vibe coding' to agentic programming - that
can accelerate productivity and broaden access. This paper examines how
AI-assisted techniques are changing software engineering practice, and the
related issues of trust, accountability, and shifting skills. We survey
iterative chat-based development, multi-agent systems, dynamic prompt
orchestration, and integration via the Model Context Protocol (MCP). Using case
studies and industry data, we outline both the opportunities (faster cycles,
democratized coding) and the challenges (model reliability and cost) of
applying generative AI to coding. We describe new roles, skills, and best
practices for using AI in a responsible and effective way.

</details>


### [18] [Agentic RAG for Software Testing with Hybrid Vector-Graph and Multi-Agent Orchestration](https://arxiv.org/abs/2510.10824)
*Mohanakrishnan Hariharan,Satish Arvapalli,Seshu Barma,Evangeline Sheela*

Main category: cs.SE

TL;DR: 使用基于Agentic RAG系统的软件测试自动化方法，通过结合自主AI代理和混合向量-图知识系统，实现了测试计划、测试用例和质量工程指标的自动生成，显著提升了测试准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 解决传统软件测试的局限性，通过利用大型语言模型、多代理协调和增强的上下文理解，实现质量工程文档的自动化生成和全面可追溯性。

Method: 结合自主AI代理与混合向量-图知识系统，利用Gemini和Mistral等LLM模型，采用多代理协调机制和增强的上下文处理技术。

Result: 准确率从65%提升至94.8%，企业级项目测试时间减少85%，测试套件效率提升85%，预计成本节省35%，上线时间提前2个月。

Conclusion: 该方法在软件测试自动化方面取得了显著成效，证明了Agentic RAG系统在质量工程中的实用价值，能够大幅提升测试效率和准确性。

Abstract: We present an approach to software testing automation using Agentic
Retrieval-Augmented Generation (RAG) systems for Quality Engineering (QE)
artifact creation. We combine autonomous AI agents with hybrid vector-graph
knowledge systems to automate test plan, case, and QE metric generation. Our
approach addresses traditional software testing limitations by leveraging LLMs
such as Gemini and Mistral, multi-agent orchestration, and enhanced
contextualization. The system achieves remarkable accuracy improvements from
65% to 94.8% while ensuring comprehensive document traceability throughout the
quality engineering lifecycle. Experimental validation of enterprise Corporate
Systems Engineering and SAP migration projects demonstrates an 85% reduction in
testing timeline, an 85% improvement in test suite efficiency, and projected
35% cost savings, resulting in a 2-month acceleration of go-live.

</details>


### [19] [Software Defect Prediction using Autoencoder Transformer Model](https://arxiv.org/abs/2510.10840)
*Seshu Barma,Mohanakrishnan Hariharan,Satish Arvapalli*

Main category: cs.SE

TL;DR: 提出ADE-QVAET模型，结合自适应差分进化和量子变分自编码器-Transformer，用于软件缺陷预测，在90%训练数据下达到98.08%准确率。


<details>
  <summary>Details</summary>
Motivation: 现有ML模型在处理噪声数据、类别不平衡、模式识别、特征提取和泛化能力方面存在困难，需要开发更强大的缺陷预测模型。

Method: 开发ADE-QVAET模型，结合自适应差分进化(ADE)和量子变分自编码器-Transformer(QVAET)，通过ADE优化增强模型收敛性和预测性能，QVAET获取高维潜在特征并保持序列依赖关系。

Result: 在90%训练比例下，ADE-QVAET达到准确率98.08%、精确率92.45%、召回率94.67%、F1分数98.12%，优于差分进化(DE)ML模型。

Conclusion: ADE-QVAET整合AI-ML技术，通过超参数调优实现可扩展且准确的软件缺陷预测，代表了AI-ML驱动的质量工程技术。

Abstract: An AI-ML-powered quality engineering approach uses AI-ML to enhance software
quality assessments by predicting defects. Existing ML models struggle with
noisy data types, imbalances, pattern recognition, feature extraction, and
generalization. To address these challenges, we develop a new model, Adaptive
Differential Evolution (ADE) based Quantum Variational Autoencoder-Transformer
(QVAET) Model (ADE-QVAET). ADE combines with QVAET to obtain high-dimensional
latent features and maintain sequential dependencies, resulting in enhanced
defect prediction accuracy. ADE optimization enhances model convergence and
predictive performance. ADE-QVAET integrates AI-ML techniques such as tuning
hyperparameters for scalable and accurate software defect prediction,
representing an AI-ML-driven technology for quality engineering. During
training with a 90% training percentage, ADE-QVAET achieves high accuracy,
precision, recall, and F1-score of 98.08%, 92.45%, 94.67%, and 98.12%,
respectively, when compared to the Differential Evolution (DE) ML model.

</details>


### [20] [Generative AI for Software Project Management: Insights from a Review of Software Practitioner Literature](https://arxiv.org/abs/2510.10887)
*Lakshana Iruni Assalaarachchi,Zainab Masood,Rashina Hoda,John Grundy*

Main category: cs.SE

TL;DR: 软件项目管理领域正在经历GenAI转型，项目经理主要将GenAI视为助手而非替代者，支持自动化任务、预测分析和敏捷实践，但需注意幻觉、伦理和情感智能等挑战。


<details>
  <summary>Details</summary>
Motivation: 了解软件项目管理中GenAI转型的现状，通过从业者公开讨论来把握实际应用情况。

Method: 对47个公开从业者资源（博客、文章、行业报告）进行灰色文献综述分析。

Result: 发现项目经理将GenAI视为助手/副驾驶/朋友而非替代者，支持自动化常规任务、预测分析、沟通协作和敏捷实践；同时关注幻觉、伦理隐私、情感智能缺失等责任使用问题。

Conclusion: 提出了GenAI时代软件项目经理的技能提升要求，并针对从业者和研究者提供了关键建议。

Abstract: Software practitioners are discussing GenAI transformations in software
project management openly and widely. To understand the state of affairs, we
performed a grey literature review using 47 publicly available practitioner
sources including blogs, articles, and industry reports. We found that software
project managers primarily perceive GenAI as an "assistant", "copilot", or
"friend" rather than as a "PM replacement", with support of GenAI in automating
routine tasks, predictive analytics, communication and collaboration, and in
agile practices leading to project success. Practitioners emphasize responsible
GenAI usage given concerns such as hallucinations, ethics and privacy, and lack
of emotional intelligence and human judgment. We present upskilling
requirements for software project managers in the GenAI era mapped to the
Project Management Institute's talent triangle. We share key recommendations
for both practitioners and researchers.

</details>


### [21] [Project-Level C-to-Rust Translation via Synergistic Integration of Knowledge Graphs and Large Language Models](https://arxiv.org/abs/2510.10956)
*Zhiqiang Yuan,Wenjun Mao,Zhuo Chen,Xiyue Shang,Chong Wang,Yiling Lou,Xin Peng*

Main category: cs.SE

TL;DR: 提出了一种基于指针知识图谱的C到Rust项目级翻译方法，通过全局指针语义分析显著提升翻译代码的安全性和正确性。


<details>
  <summary>Details</summary>
Motivation: 解决现有LLM方法在项目级C到Rust翻译中因缺乏全局指针语义视角而导致的指针翻译问题。

Method: 构建C-Rust指针知识图谱，包含指针使用信息和Rust导向注解，结合LLM进行项目级翻译。

Result: 相比基于规则的翻译和传统LLM方法，减少了99.9%的不安全使用，功能正确性平均提高29.3%。

Conclusion: 指针知识图谱能有效指导LLM生成更安全、更符合Rust习惯的代码，解决了项目级翻译中的关键挑战。

Abstract: Translating C code into safe Rust is an effective way to ensure its memory
safety. Compared to rule-based translation which produces Rust code that
remains largely unsafe, LLM-based methods can generate more idiomatic and safer
Rust code because LLMs have been trained on vast amount of human-written
idiomatic code. Although promising, existing LLM-based methods still struggle
with project-level C-to-Rust translation. They typically partition a C project
into smaller units (\eg{} functions) based on call graphs and translate them
bottom-up to resolve program dependencies. However, this bottom-up,
unit-by-unit paradigm often fails to translate pointers due to the lack of a
global perspective on their usage. To address this problem, we propose a novel
C-Rust Pointer Knowledge Graph (KG) that enriches a code-dependency graph with
two types of pointer semantics: (i) pointer-usage information which record
global behaviors such as points-to flows and map lower-level struct usage to
higher-level units; and (ii) Rust-oriented annotations which encode ownership,
mutability, nullability, and lifetime. Synthesizing the \kg{} with LLMs, we
further propose \ourtool{}, which implements a project-level C-to-Rust
translation technique. In \ourtool{}, the \kg{} provides LLMs with
comprehensive pointer semantics from a global perspective, thus guiding LLMs
towards generating safe and idiomatic Rust code from a given C project. Our
experiments show that \ourtool{} reduces unsafe usages in translated Rust by
99.9\% compared to both rule-based translation and traditional LLM-based
rewriting, while achieving an average 29.3\% higher functional correctness than
those fuzzing-enhanced LLM methods.

</details>


### [22] [RepoSummary: Feature-Oriented Summarization and Documentation Generation for Code Repositories](https://arxiv.org/abs/2510.11039)
*Yifeng Zhu,Xianlin Zhao,Xutian Li,Yanzhen Zou,Haizhuo Yuan,Yue Wang,Bing Xie*

Main category: cs.SE

TL;DR: RepoSummary是一种面向特征的代码仓库摘要方法，能够自动生成仓库文档并建立从功能特征到对应代码元素的更准确的可追溯性链接。


<details>
  <summary>Details</summary>
Motivation: 现有仓库摘要技术主要基于目录树结构进行代码摘要，不足以追踪高层特征到协作实现这些特征的方法。需要更准确的特征到代码的可追溯性。

Method: 提出RepoSummary方法，采用面向特征的代码仓库摘要方法，同时自动生成仓库文档，建立功能特征到对应代码元素的准确追溯链接。

Result: 相比最先进的基线方法HGEN，RepoSummary实现了更高的特征覆盖率和更准确的可追溯性。完全覆盖特征的比例从61.2%提升到71.1%，文件级追溯召回率从29.9%提升到53.0%。

Conclusion: RepoSummary生成的文档在概念一致性、可理解性和格式方面都优于现有方法，能够帮助开发者在代码理解和维护过程中快速定位相关方法和文件。

Abstract: Repository summarization is a crucial research question in development and
maintenance for software engineering. Existing repository summarization
techniques primarily focus on summarizing code according to the directory tree,
which is insufficient for tracing high-level features to the methods that
collaboratively implement them. To address these limitations, we propose
RepoSummary, a feature-oriented code repository summarization approach that
simultaneously generates repository documentation automatically. Furthermore,
it establishes more accurate traceability links from functional features to the
corresponding code elements, enabling developers to rapidly locate relevant
methods and files during code comprehension and maintenance. Comprehensive
experiments against the state-of-the-art baseline (HGEN) demonstrate that
RepoSummary achieves higher feature coverage and more accurate traceability. On
average, it increases the rate of completely covered features in manual
documentation from 61.2% to 71.1%, improves file-level traceability recall from
29.9% to 53.0%, and generates documentation that is more conceptually
consistent, easier to understand, and better formatted than that produced by
existing approaches.

</details>


### [23] [Defects4C: Benchmarking Large Language Model Repair Capability with C/C++ Bugs](https://arxiv.org/abs/2510.11059)
*Jian Wang,Xiaofei Xie,Qiang Hu,Shangqing Liu,Jiongchi Yu,Jiaolong Klong,Yi Li*

Main category: cs.SE

TL;DR: 提出了Defects4C基准数据集，用于C/C++程序修复研究，并评估了24种大语言模型在修复C/C++缺陷方面的效果


<details>
  <summary>Details</summary>
Motivation: C/C++程序修复研究存在显著空白，主要原因是缺乏高质量的开源基准数据集，而Java领域的APR研究因Defects4J等基准而取得了显著进展

Method: 构建了Defects4C基准数据集，包含来自真实C/C++仓库的900万个bug相关提交、248个高质量缺陷函数和102个易受攻击函数，并配有测试用例；使用该数据集评估了24种最先进的大语言模型

Result: 研究结果揭示了当前基于LLM的APR技术在C/C++领域的优势和局限性，强调了需要更鲁棒的方法

Conclusion: Defects4C在推动未来C/C++程序修复研究中发挥着关键作用

Abstract: Automated Program Repair (APR) plays a critical role in enhancing the quality
and reliability of software systems. While substantial progress has been made
in Java-based APR, largely facilitated by benchmarks like Defects4J, there
remains a significant gap in research on C/C++ program repair, despite the
widespread use of C/C++ and the prevalence of associated vulnerabilities. This
gap is primarily due to the lack of high-quality, open-source benchmarks
tailored for C/C++.
  To address this issue, we introduce Defects4C, a comprehensive and executable
benchmark specifically designed for C/C++ program repair. Our dataset is
constructed from real-world C/C++ repositories and includes a large collection
of bug-relevant commits (9M in total), 248 high-quality buggy functions, and
102 vulnerable functions, all paired with test cases for reproduction. These
resources enable rigorous evaluation of repair techniques and support the
retraining of learning-based approaches for enhanced performance.
  Using Defects4C, we conduct a comprehensive empirical study evaluating the
effectiveness of 24 state-of-the-art large language models (LLMs) in repairing
C/C++ faults. Our findings offer valuable insights into the strengths and
limitations of current LLM-based APR techniques in this domain, highlighting
both the need for more robust methods and the critical role of Defects4C in
advancing future research

</details>


### [24] [DebugTA: An LLM-Based Agent for Simplifying Debugging and Teaching in Programming Education](https://arxiv.org/abs/2510.11076)
*Lingyue Fu,Haowei Yuan,Datong Chen,Xinyi Dai,Qingyao Li,Weinan Zhang,Weiwen Liu,Yong Yu*

Main category: cs.SE

TL;DR: 提出DebugTA，一种基于LLM的调试和教学代理，通过专用工具和分步推理来解决编程教育中调试教学任务的复杂性和异构输入问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法在处理调试教学任务时面临两个关键挑战：输入复杂性和异构性增加了LLM的推理难度；未能充分利用标准代码，迫使模型依赖复杂的多步推理。

Method: DebugTA使用专用工具进行标准代码检索、变量替换对齐参考代码，以及外部编译器进行实时代码分析。通过明确的数学和调试原则指导，将复杂任务分解为顺序的LLM交互。

Result: 在三个真实世界代码数据集上的实验结果表明，DebugTA持续提高了教学效果，同时显著降低了计算成本。

Conclusion: DebugTA通过工具调用和分步推理简化了逻辑推理过程，提高了修改建议的准确性，为编程教育中的调试教学任务提供了有效的解决方案。

Abstract: In programming education, Debugging and Teaching (DT) task is a common
scenario where students receive assistance in correcting their erroneous code.
The task involves multiple inputs, including erroneous code, error messages,
reference solutions, and the question description, with the goal of generating
modification suggestions to the erroneous code. However, two key challenges
hinder the effectiveness of existing approaches. Firstly, the complexity and
heterogeneity of inputs inherent in DT tasks significantly elevate the
reasoning challenges faced by LLMs. Second, existing approaches often fail to
fully leverage the availability of standard code in DT tasks, forcing models to
rely solely on complex multi-step reasoning, which limits the potential of LLMs
in addressing DT tasks effectively. To address these challenges, we propose
DebugTA, a novel LLM-based debugging and teaching agent with specialized tools
for standard code retrieval, variable substitution to align reference code, and
an external compiler for real-time code analysis. Guided by explicit
pedagogical and debugging principles, DebugTA acts as an agent that decomposes
a complex task into sequential LLM interactions, each utilizing distinct tools
for specific subtasks, thereby simplifying the logical reasoning at each step
and reducing overall reasoning complexity. Furthermore, DebugTA utilizes tool
calls to align the standard code with the erroneous code as much as possible,
allowing the LLM to focus on logic errors within the erroneous code and
improving the accuracy of the generated suggestions. To rigorously assess the
quality of modification suggestions, we introduce a student simulator-teacher
interaction paradigm. Experimental results on three real-world code datasets
demonstrate that DebugTA consistently improves teaching effectiveness while
significantly reducing computational costs.

</details>


### [25] [What Slows Down FMware Development? An Empirical Study of Developer Challenges and Resolution Times](https://arxiv.org/abs/2510.11138)
*Zitao Wang,Zhimin Zhao,Michael W. Godfrey*

Main category: cs.SE

TL;DR: 本文首次对跨云平台和开源仓库的FMware开发进行了大规模分析，揭示了FMware在软件工程中的变革性影响、主要应用领域、开发挑战以及最耗时的技术问题。


<details>
  <summary>Details</summary>
Motivation: 基础模型(FMs)正在从根本上改变软件工程实践，催生了围绕这些模型构建的FMware应用和基础设施。然而FMware的设计、实现和演进带来了显著的新挑战，特别是在云平台和本地部署环境中，其目标、流程和工具与传统软件开发存在差异。

Method: 通过三个重点领域对FMware生态系统进行实证研究：(1)最常见的应用领域，(2)开发者遇到的关键挑战，(3)需要最大努力解决的问题类型。分析数据来自GitHub仓库和领先的FMware平台（HuggingFace、GPTStore、Ora、Poe）。

Result: 研究发现FMware主要集中在教育、内容创作和商业战略领域，同时存在内存管理、依赖处理和分词器配置等技术挑战。在GitHub上，错误报告和核心功能问题是最常报告的问题，而代码审查、相似性搜索和提示模板设计是最耗时的任务。

Conclusion: 通过揭示开发者实践和痛点，本研究为改进FMware工具、工作流程和社区支持提供了机会，并为指导FMware开发的未来提供了可操作的见解。

Abstract: Foundation Models (FMs), such as OpenAI's GPT, are fundamentally transforming
the practice of software engineering by enabling the development of
\emph{FMware} -- applications and infrastructures built around these models.
FMware systems now support tasks such as code generation, natural-language
interaction, knowledge integration, and multi-modal content creation,
underscoring their disruptive impact on current software engineering workflows.
However, the design, implementation, and evolution of FMware present
significant new challenges, particularly across cloud-based and on-premise
platforms where goals, processes, and tools often diverge from those of
traditional software development.
  To our knowledge, this is the first large-scale analysis of FMware
development across both cloud-based platforms and open-source repositories. We
empirically investigate the FMware ecosystem through three focus areas: (1) the
most common application domains of FMware, (2) the key challenges developers
encounter, and (3) the types of issues that demand the greatest effort to
resolve. Our analysis draws on data from GitHub repositories and from leading
FMware platforms, including HuggingFace, GPTStore, Ora, and Poe. Our findings
reveal a strong focus on education, content creation, and business strategy,
alongside persistent technical challenges in memory management, dependency
handling, and tokenizer configuration. On GitHub, bug reports and core
functionality issues are the most frequently reported problems, while code
review, similarity search, and prompt template design are the most
time-consuming to resolve.
  By uncovering developer practices and pain points, this study points to
opportunities to improve FMware tools, workflows, and community support, and
provides actionable insights to help guide the future of FMware development.

</details>


### [26] [Interoperability From OpenTelemetry to Kieker: Demonstrated as Export from the Astronomy Shop](https://arxiv.org/abs/2510.11179)
*David Georg Reichelt,Shinhyung Yang,Wilhelm Hasselbring*

Main category: cs.SE

TL;DR: 将OpenTelemetry追踪数据转换为Kieker框架格式，使Kieker能够分析更多编程语言（如C#和JavaScript）的追踪数据。


<details>
  <summary>Details</summary>
Motivation: Kieker框架目前仅支持有限的语言（Java、C、Fortran、Python），而OpenTelemetry标准支持更多编程语言，包括Kieker不支持的C#和JavaScript。

Method: 开发数据转换方法，将OpenTelemetry追踪数据转换为Kieker框架可识别的格式。

Result: 成功实现了数据转换，能够从OpenTelemetry仪器化创建调用树，并通过Astronomy Shop演示应用程序验证了方法的可用性。

Conclusion: 通过将OpenTelemetry数据转换为Kieker格式，扩展了Kieker的分析能力，使其能够支持更多编程语言和技术栈。

Abstract: The observability framework Kieker provides a range of analysis capabilities,
but it is currently only able to instrument a smaller selection of languages
and technologies, including Java, C, Fortran, and Python. The OpenTelemetry
standard aims for providing reference implementations for most programming
languages, including C# and JavaScript, that are currently not supported by
Kieker. In this work, we describe how to transform OpenTelemetry tracing data
into the Kieker framework. Thereby, it becomes possible to create for example
call trees from OpenTelemetry instrumentations. We demonstrate the usability of
our approach by visualizing trace data of the Astronomy Shop, which is an
OpenTelemetry demo application.

</details>


### [27] [Detection of Performance Changes in MooBench Results Using Nyrkiö on GitHub Actions](https://arxiv.org/abs/2510.11310)
*Shinhyung Yang,David Georg Reichelt,Henrik Ingo,Wilhelm Hasselbring*

Main category: cs.SE

TL;DR: 将Nyrkiö性能变化检测服务集成到MooBench中，实现了在GitHub项目中的性能变化检测，发现了一个由Linux内核版本变更引起的性能回归问题。


<details>
  <summary>Details</summary>
Motivation: GitHub上拥有5.18亿个项目，性能变化对项目用户至关重要。虽然GitHub CI/CD支持性能测量，但性能变化检测仍是一个挑战性课题。

Method: 将Nyrkiö变化检测服务集成到持续运行的MooBench中，通过上传测量数据到Nyrkiö服务来实现性能变化检测。

Result: 发现了一个主要的性能回归问题，该问题可通过GitHub Actions重现，且是由Linux内核版本变更引起的。

Conclusion: 通过集成Nyrkiö到MooBench，成功实现了在GitHub项目中的性能变化检测，能够识别由系统环境变化引起的性能问题。

Abstract: In GitHub with its 518 million hosted projects, performance changes within
these projects are highly relevant to the project's users. Although performance
measurement is supported by GitHub CI/CD, performance change detection is a
challenging topic.
  In this paper, we demonstrate how we incorporated Nyrki\"o to MooBench. Prior
to this work, Moobench continuously ran on GitHub virtual machines, measuring
overhead of tracing agents, but without change detection. By adding the upload
of the measurements to the Nyrki\"o change detection service, we made it
possible to detect performance changes. We identified one major performance
regression and examined the performance change in depth. We report that (1) it
is reproducible with GitHub actions, and (2) the performance regression is
caused by a Linux Kernel version change.

</details>


### [28] [Cracking CodeWhisperer: Analyzing Developers' Interactions and Patterns During Programming Tasks](https://arxiv.org/abs/2510.11516)
*Jeena Javahar,Tanya Budhrani,Manaal Basha,Cleidson R. B. de Souza,Ivan Beschastnikh,Gema Rodriguez-Perez*

Main category: cs.SE

TL;DR: 研究分析了开发者使用Amazon CodeWhisperer的行为模式，识别出四种主要交互方式：增量代码优化、自然语言注释指导、基于模型建议的基础结构构建以及与外部资源的整合使用。


<details>
  <summary>Details</summary>
Motivation: 随着AI代码生成工具的普及，理解开发者如何实际采用这些工具变得至关重要。本研究旨在探究开发者如何与基于LLM的代码生成工具CodeWhisperer进行交互。

Method: 进行了两项用户研究，每组10名参与者。第一项研究确定关键交互行为，第二项研究使用自定义遥测插件收集细粒度交互数据，采用混合方法分析。

Result: 识别出四种行为模式：1）增量代码优化 2）使用自然语言注释进行显式指导 3）基于模型建议的基础结构构建 4）与外部资源的整合使用

Conclusion: 提供了对这些行为模式的全面分析，有助于理解开发者如何在实际工作中采用AI代码生成工具。

Abstract: The use of AI code-generation tools is becoming increasingly common, making
it important to understand how software developers are adopting these tools. In
this study, we investigate how developers engage with Amazon's CodeWhisperer,
an LLM-based code-generation tool. We conducted two user studies with two
groups of 10 participants each, interacting with CodeWhisperer - the first to
understand which interactions were critical to capture and the second to
collect low-level interaction data using a custom telemetry plugin. Our
mixed-methods analysis identified four behavioral patterns: 1) incremental code
refinement, 2) explicit instruction using natural language comments, 3)
baseline structuring with model suggestions, and 4) integrative use with
external sources. We provide a comprehensive analysis of these patterns .

</details>


### [29] [CodeWatcher: IDE Telemetry Data Extraction Tool for Understanding Coding Interactions with LLMs](https://arxiv.org/abs/2510.11536)
*Manaal Basha,Aimeê M. Ribeiro,Jeena Javahar,Cleidson R. B. de Souza,Gema Rodríguez-Pérez*

Main category: cs.SE

TL;DR: CodeWatcher是一个轻量级、非侵入式的客户端-服务器系统，用于在VS Code编辑器中捕获开发者与代码生成工具交互的细粒度事件。


<details>
  <summary>Details</summary>
Motivation: 需要在不干扰工作流程的情况下收集开发者与代码生成工具交互的详细实时数据，以支持负责任AI、开发者生产力和人本评估的研究。

Method: 系统包含VS Code插件、基于Python的RESTful API和MongoDB后端，采用容器化部署，记录插入、删除、复制粘贴和焦点切换等语义化事件。

Result: CodeWatcher能够连续监控开发者活动，通过时间戳结构化事件，支持事后重建编码会话和丰富的行为分析。

Conclusion: 该基础设施对于研究代码生成工具的使用方式、时机以及支持负责任AI和开发者生产力研究至关重要。

Abstract: Understanding how developers interact with code generation tools (CGTs)
requires detailed, real-time data on programming behavior which is often
difficult to collect without disrupting workflow. We present
\textit{CodeWatcher}, a lightweight, unobtrusive client-server system designed
to capture fine-grained interaction events from within the Visual Studio Code
(VS Code) editor. \textit{CodeWatcher} logs semantically meaningful events such
as insertions made by CGTs, deletions, copy-paste actions, and focus shifts,
enabling continuous monitoring of developer activity without modifying user
workflows. The system comprises a VS Code plugin, a Python-based RESTful API,
and a MongoDB backend, all containerized for scalability and ease of
deployment. By structuring and timestamping each event, \textit{CodeWatcher}
enables post-hoc reconstruction of coding sessions and facilitates rich
behavioral analyses, including how and when CGTs are used during development.
This infrastructure is crucial for supporting research on responsible AI,
developer productivity, and the human-centered evaluation of CGTs. Please find
the demo, diagrams, and tool here: https://osf.io/j2kru/overview.

</details>


### [30] [Automatically Generating Questions About Scratch Programs](https://arxiv.org/abs/2510.11658)
*Florian Obermüller,Gordon Fraser*

Main category: cs.SE

TL;DR: 该研究提出了一种自动为Scratch程序生成理解性问题的方法，通过扩展LitterBox静态分析工具，在60多万个项目中生成了5400多万个问题，验证了这种评估学生编程理解能力的可行性。


<details>
  <summary>Details</summary>
Motivation: 传统编程评估仅基于代码完成度，无法保证学生对编程概念的实际理解。为每个学生程序手动创建针对性问题既繁琐又困难，因此需要自动生成问题来评估程序理解能力。

Method: 扩展LitterBox静态分析工具，提出30种基于已建立程序理解模型的Scratch代码问题类型，自动为给定Scratch程序生成相应问题。

Result: 在600,913个项目中自动生成了54,118,694个问题。初步实验显示该方法能生成有意义的Scratch程序问题，且学生回答问题的能力与其整体表现相关。

Conclusion: 自动生成Scratch程序理解问题的方法是可行的，能够有效评估学生的编程理解能力，且问题回答表现与学业表现存在关联。

Abstract: When learning to program, students are usually assessed based on the code
they wrote. However, the mere completion of a programming task does not
guarantee actual comprehension of the underlying concepts. Asking learners
questions about the code they wrote has therefore been proposed as a means to
assess program comprehension. As creating targeted questions for individual
student programs can be tedious and challenging, prior work has proposed to
generate such questions automatically. In this paper we generalize this idea to
the block-based programming language Scratch. We propose a set of 30 different
questions for Scratch code covering an established program comprehension model,
and extend the LitterBox static analysis tool to automatically generate
corresponding questions for a given Scratch program. On a dataset of 600,913
projects we generated 54,118,694 questions automatically. Our initial
experiments with 34 ninth graders demonstrate that this approach can indeed
generate meaningful questions for Scratch programs, and we find that the
ability of students to answer these questions on their programs relates to
their overall performance.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [31] [Automating the RMF: Lessons from the FedRAMP 20x Pilot](https://arxiv.org/abs/2510.09613)
*Isaac Henry Teuscher*

Main category: cs.CR

TL;DR: FedRAMP 20x是2025年试点项目，通过关键安全指标(KSIs)、自动化证据和持续报告重新设计NIST风险管理框架，以应对云原生开发挑战。


<details>
  <summary>Details</summary>
Motivation: 传统FedRAMP依赖手动控制和静态文档，无法跟上云原生开发速度，需要现代化方法来简化授权和改进网络风险管理。

Method: 采用关键安全指标替代传统NIST 800-53控制，使用自动化、机器可读证据，强调持续报告和授权，整合DevSecOps实践。

Result: FedRAMP 20x作为实施云原生、自动化优先方法的实时测试平台，能够简化授权流程并支持实时风险决策。

Conclusion: FedRAMP 20x为风险专业人员提供了现代化合规和实时风险决策的可操作建议，是云原生环境下风险管理框架的重要演进。

Abstract: The U.S. Federal Risk and Authorization Management Program (FedRAMP) has long
relied on extensive sets of controls and static documentation to assess cloud
systems. However, this manual, point-in-time approach has struggled to keep
pace with cloud-native development. FedRAMP 20x, a 2025 pilot program,
reimagines the NIST Risk Management Framework (RMF): replacing traditional NIST
800-53 controls with Key Security Indicators (KSIs), using automated,
machine-readable evidence, and emphasizing continuous reporting and
authorization.
  This case study presents a practitioner-led field report from an industry
participant who led multiple FedRAMP 20x pilot submissions and engaged directly
with the FedRAMP PMO, 3PAOs, and community working groups. It explores how
KSIs, continuous evidence pipelines, and DevSecOps integration can streamline
authorization and improve cyber risk management. The study shows FedRAMP 20x as
a live testbed for implementing the RMF in a cloud-native, automation-first
approach and shares actionable recommendations for risk professionals seeking
to modernize compliance and support real-time, risk-informed decision-making.

</details>


### [32] [A Biosecurity Agent for Lifecycle LLM Biosecurity Alignment](https://arxiv.org/abs/2510.09615)
*Meiyin Meng,Zaixi Zhang*

Main category: cs.CR

TL;DR: 提出了一个包含四个协调模式的生命周期生物安全代理框架，用于保护LLMs在生物医学研究中的安全使用，防止被滥用于有毒化合物合成等恶意目的。


<details>
  <summary>Details</summary>
Motivation: 随着LLMs在生物医学研究中的广泛应用，从文献筛选到实验设计，其双重用途风险日益突出，特别是可能被滥用于指导有毒化合物合成。

Method: 构建了包含四个模式的生物安全代理：数据集净化、偏好对齐、运行时护栏和自动化红队测试。数据集净化分为三个层级，偏好对齐使用DPO与LoRA适配器，运行时护栏提供不同安全级别，自动化红队测试持续评估安全性。

Result: 数据集净化移除率从0.46%到70.40%；偏好对齐将端到端攻击成功率从59.7%降至3.0%；L2级护栏达到最佳平衡（F1=0.720）；自动化红队测试未发现成功越狱。

Conclusion: 该生物安全代理提供了一个可审计的生命周期对齐框架，在保持良性效用的同时显著降低攻击成功率，为LLMs在科学研究中的安全使用提供了保障。

Abstract: Large language models (LLMs) are increasingly integrated into biomedical
research workflows--from literature triage and hypothesis generation to
experimental design--yet this expanded utility also heightens dual-use
concerns, including the potential misuse for guiding toxic compound synthesis.
In response, this study shows a Biosecurity Agent that comprises four
coordinated modes across the model lifecycle: dataset sanitization, preference
alignment, run-time guardrails, and automated red teaming. For dataset
sanitization (Mode 1), evaluation is conducted on CORD-19, a COVID-19 Open
Research Dataset of coronavirus-related scholarly articles. We define three
sanitization tiers--L1 (compact, high-precision), L2 (human-curated biosafety
terms), and L3 (comprehensive union)--with removal rates rising from 0.46% to
70.40%, illustrating the safety-utility trade-off. For preference alignment
(Mode 2), DPO with LoRA adapters internalizes refusals and safe completions,
reducing end-to-end attack success rate (ASR) from 59.7% to 3.0%. At inference
(Mode 3), run-time guardrails across L1-L3 show the expected security-usability
trade-off: L2 achieves the best balance (F1 = 0.720, precision = 0.900, recall
= 0.600, FPR =0.067), while L3 offers stronger jailbreak resistance at the cost
of higher false positives. Under continuous automated red-teaming (Mode 4), no
successful jailbreaks are observed under the tested protocol. Taken together,
our biosecurity agent offers an auditable, lifecycle-aligned framework that
reduces attack success while preserving benign utility, providing safeguards
for the use of LLMs in scientific research and setting a precedent for future
agent-level security protections.

</details>


### [33] [Causal Digital Twins for Cyber-Physical Security: A Framework for Robust Anomaly Detection in Industrial Control Systems](https://arxiv.org/abs/2510.09616)
*Mohammadhossein Homaei,Mehran Tarif,Mar Avilla,Andres Caro*

Main category: cs.CR

TL;DR: 提出了一种用于工业控制系统网络物理安全的新型因果数字孪生框架，结合因果推理理论和数字孪生建模，显著提高了异常检测准确性和根因分析能力。


<details>
  <summary>Details</summary>
Motivation: 当前工业控制系统的异常检测方法依赖相关性分析，无法区分真实因果关系和虚假关联，导致高误报率和根因分析能力差。

Method: 结合因果推理理论和数字孪生建模，实现关联检测、干预分析和反事实推理三种因果推理能力。

Result: 在三个工业数据集上验证，F1分数分别为0.944±0.014(SWaT)、0.902±0.021(WADI)、0.923±0.018(HAI)，误报率降低74%，根因分析准确率达78.4%，反事实分析使攻击成功率降低73.2%。

Conclusion: 该框架在保持实时性能(3.2ms延迟)的同时，提供了可解释的安全分析，适合工业部署。

Abstract: Industrial Control Systems (ICS) face growing cyber-physical attacks that
exploit both network vulnerabilities and physical processes. Current anomaly
detection methods rely on correlation-based analysis, which cannot separate
true causal relationships from spurious associations. This limitation results
in high false alarm rates and poor root cause analysis. We propose a novel
Causal Digital Twin (CDT) framework for cyber-physical security in medium-scale
ICS. Our method combines causal inference theory with digital twin modeling.
The framework enables three types of causal reasoning: association for pattern
detection, intervention for understanding system responses, and counterfactual
analysis for attack prevention planning. We evaluate our framework on three
industrial datasets: SWaT, WADI, and HAI, with validation through physical
constraint compliance (90.8\%) and synthetic ground truth testing (structural
Hamming distance 0.13). Results show significant improvements over seven
baseline methods. Our CDT achieves F1-scores are $0.944 \pm 0.014$ for SWaT,
$0.902 \pm 0.021$ for WADI, and $0.923 \pm 0.018$ for HAI with statistical
significance ($p < 0.0024$, Bonferroni corrected). The framework reduces false
positives by \SI{74}{\percent} and achieves \SI{78.4}{\percent} root cause
analysis accuracy compared to \SI{48.7}{\percent} for existing methods.
Counterfactual analysis enables defense strategies that reduce attack success
by \SI{73.2}{\percent}. The system keeps real-time performance with
\SI{3.2}{ms} latency, which is suitable for industrial deployment, while
providing interpretable explanations for operators.

</details>


### [34] [ChipmunkRing: A Practical Post-Quantum Ring Signature Scheme for Blockchain Applications](https://arxiv.org/abs/2510.09617)
*Dmitrii A. Gerasimov*

Main category: cs.CR

TL;DR: ChipmunkRing是一个专为区块链环境设计的后量子环签名方案，基于Chipmunk格密码框架，提供紧凑签名和快速验证性能。


<details>
  <summary>Details</summary>
Motivation: 为区块链环境提供实用的后量子环签名方案，解决传统方法在性能和安全性方面的不足。

Method: 采用Acorn Verification零知识协议替代经典Fiat-Shamir方法，实现线性O(n)认证复杂度，使用96字节密码证明。

Result: 签名大小20.5-279.7KB，签名时间1.1-15.1ms，验证时间0.4-4.5ms（2-64成员组），32成员环性能提升17.7倍。

Conclusion: 该方案提供NIST Level 1级别的112位后量子安全保护，支持标准匿名集和协作阈值构造，适用于区块链环境。

Abstract: ChipmunkRing, a practical post-quantum ring signature construction tailored
for blockchain environments. Building on our Chipmunk lattice-based
cryptographic framework, this implementation delivers compact digital
signatures ranging from 20.5 to 279.7KB, with rapid signing operations
completing in 1.1-15.1ms and efficient validation processes requiring only
0.4-4.5ms for participant groups of 2-64 members. The cornerstone of our
approach is Acorn Verification-a streamlined zero-knowledge protocol that
supersedes the classical Fiat-Shamir methodology. This innovation enables
linear O(n) authentication complexity using concise 96-byte cryptographic
proofs per participant, yielding a remarkable 17.7x performance enhancement for
32-member rings when compared to conventional techniques. Our work includes
rigorous mathematical security demonstrations confirming 112-bit post-quantum
protection (NIST Level 1), extensive computational benchmarking, and
comprehensive support for both standard anonymity sets and collaborative
threshold constructions with flexible participation requirements.

</details>


### [35] [A Systematic Review on Crimes facilitated by Consumer Internet of Things Devices](https://arxiv.org/abs/2510.09618)
*Ashley Brown,Nilufer Tuptuk,Enrico Mariconti,Shane Johnson*

Main category: cs.CR

TL;DR: 本文通过系统文献综述分析了物联网设备面临的安全威胁和犯罪风险，识别了主要攻击类型、缓解措施及相关的犯罪威胁场景。


<details>
  <summary>Details</summary>
Motivation: 随着物联网设备的普及，犯罪分子越来越多地利用这些设备实施犯罪活动，因此需要系统性地了解物联网安全威胁及其犯罪影响。

Method: 采用系统文献综述方法，包括明确的搜索策略和研究选择策略，共纳入543篇文章，通过主题分析综合研究结果。

Result: 识别出针对消费级物联网设备的主要安全攻击包括中间人攻击、同步攻击、拒绝服务攻击、DNS投毒和恶意软件等，以及设备特定的漏洞。同时发现了相关的犯罪威胁场景如欺诈、身份盗窃、加密货币劫持和家庭暴力等。

Conclusion: 物联网设备面临多种安全威胁，这些威胁可被用于实施各种犯罪活动，需要采取相应的缓解措施来应对这些安全挑战。

Abstract: It is well documented that criminals use IoT devices to facilitate crimes.
The review process follows a systematic approach with a clear search strategy,
and study selection strategy. The review included a total of 543 articles and
the findings from these articles were synthesised through thematic analysis.
Identified security attacks targeting consumer IoT devices include
man-in-the-middle (MiTM) attacks, synchronisation attacks, Denial-of-Service
(DoS), DNS poisoning and malware, alongside device-specific vulnerabilities.
Besides security attacks, this review discusses mitigations. Furthermore, the
literature also covers crime threat scenarios arising from these attacks, such
as, fraud, identity theft, crypto jacking and domestic abuse.

</details>


### [36] [Risk-Calibrated Bayesian Streaming Intrusion Detection with SRE-Aligned Decisions](https://arxiv.org/abs/2510.09619)
*Michel Youssef*

Main category: cs.CR

TL;DR: 提出了一种基于风险校准的流式入侵检测方法，结合贝叶斯在线变点检测和SRE错误预算决策阈值，在UNSW-NB15和CIC-IDS2017数据集上优于现有无监督基线。


<details>
  <summary>Details</summary>
Motivation: 传统入侵检测方法缺乏对误报和漏报成本的明确考虑，需要一种能够适应分布漂移和概念漂移，同时与运维错误预算对齐的风险校准方法。

Method: 使用贝叶斯在线变点检测(BOCPD)获取运行长度后验概率，通过优化在假阳性和假阴性预算下的期望运营成本，将这些后验概率映射到告警决策。

Result: 在UNSW-NB15和CIC-IDS2017基准测试中，该方法在中高召回率下提高了精确率-召回率表现，并改善了概率校准效果，优于ECOD、COPOD和LOF等基线方法。

Conclusion: 风险校准的流式入侵检测方法能够有效平衡检测精度和运营成本，为实际运维场景提供了实用的入侵检测解决方案。

Abstract: We present a risk-calibrated approach to streaming intrusion detection that
couples Bayesian Online Changepoint Detection (BOCPD) with decision thresholds
aligned to Site Reliability Engineering (SRE) error budgets. BOCPD provides
run-length posteriors that adapt to distribution shift and concept drift; we
map these posteriors to alert decisions by optimizing expected operational cost
under false-positive and false-negative budgets. We detail the hazard model,
conjugate updates, and an O(1)-per-event implementation. A concrete SRE example
shows how a 99.9% availability SLO (43.2 minutes per month error budget) yields
a probability threshold near 0.91 when missed incidents are 10x more costly
than false alarms. We evaluate on the full UNSW-NB15 and CIC-IDS2017 benchmarks
with chronological splits, comparing against strong unsupervised baselines
(ECOD, COPOD, and LOF). Metrics include PR-AUC, ROC-AUC, Brier score,
calibration reliability diagrams, and detection latency measured in events.
Results indicate improved precision-recall at mid to high recall and better
probability calibration relative to baselines. We release implementation
details, hyperparameters, and ablations for hazard sensitivity and
computational footprint. Code and reproducibility materials will be made
available upon publication; datasets and implementation are available from the
corresponding author upon reasonable request.

</details>


### [37] [Toward a Unified Security Framework for AI Agents: Trust, Risk, and Liability](https://arxiv.org/abs/2510.09620)
*Jiayun Mo,Xin Kang,Tieyan Li,Zhongding Lei*

Main category: cs.CR

TL;DR: 提出了TRL框架，将信任、风险和责任的相互依赖关系联系起来，为构建和增强信任、分析和减轻风险、分配和归因责任提供系统方法


<details>
  <summary>Details</summary>
Motivation: AI代理发展带来的兴奋伴随着问题出现，这些问题集中在用户对AI的信任问题、涉及的风险以及归因责任和责任的困难上

Method: 提出信任、风险和责任（TRL）框架，将信任、风险和责任的相互依赖关系联系起来，提供系统方法

Result: TRL框架可以应用于分析AI代理的任何应用场景，并根据上下文建议适当的措施

Conclusion: TRL框架具有潜在的社会影响、经济影响、伦理影响等，预计将为解决潜在挑战和促进6G网络中可信、无风险和负责任的AI使用带来显著价值

Abstract: The excitement brought by the development of AI agents came alongside arising
problems. These concerns centered around users' trust issues towards AIs, the
risks involved, and the difficulty of attributing responsibilities and
liabilities. Current solutions only attempt to target each problem separately
without acknowledging their inter-influential nature. The Trust, Risk and
Liability (TRL) framework proposed in this paper, however, ties together the
interdependent relationships of trust, risk, and liability to provide a
systematic method of building and enhancing trust, analyzing and mitigating
risks, and allocating and attributing liabilities. It can be applied to analyze
any application scenarios of AI agents and suggest appropriate measures fitting
to the context. The implications of the TRL framework lie in its potential
societal impacts, economic impacts, ethical impacts, and more. It is expected
to bring remarkable values to addressing potential challenges and promoting
trustworthy, risk-free, and responsible usage of AI in 6G networks.

</details>


### [38] [A Systematic Literature Review on Fundamental Technologies and Security Challenges in the Metaverse Platforms](https://arxiv.org/abs/2510.09621)
*Krishno Dey,Diogo Barradas,Saqib Hakak*

Main category: cs.CR

TL;DR: 这篇论文对元宇宙的安全与隐私威胁进行了系统性文献综述，分析了其关键技术、主要漏洞及应对措施，指出元宇宙相比传统数字平台具有更大的攻击面，并提出了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 随着元宇宙技术的快速发展，它带来了身份管理、数据治理和用户交互等一系列安全和隐私威胁。本文旨在全面分析元宇宙的关键技术、漏洞和威胁，确保其可持续发展和用户安全。

Method: 采用系统性文献综述(SLR)方法，识别元宇宙平台中的关键漏洞及其应对措施。

Result: 研究发现元宇宙相比传统数字平台具有更大的攻击面，其沉浸式、去中心化和永久性特征产生了新的漏洞。虽然存在许多应对措施，但大多停留在理论层面或未在真实环境中测试。

Conclusion: 本文综述了当前进展，识别了研究空白，并提出了确保安全、有韧性和伦理治理的元宇宙的未来研究方向。

Abstract: The Metaverse utilizes emerging technologies such as Extended Reality (XR),
Artificial Intelligence (AI), blockchain, and digital twins to provide an
immersive and interactive virtual experience. As Metaverse continues to evolve,
it bring a range of security and privacy threats, such as identity management,
data governance, and user interactions. This survey aims to provide a
comprehensive review of the enabling technologies for the Metaverse. It also
aims to provide a thorough analysis of key vulnerabilities and threats that may
compromise its sustainability and user safety. We perform a systematic
literature review (SLR) to identify key vulnerabilities and their
countermeasures in Metaverse platforms. Metaverse offers a much larger attack
surface compared to conventional digital platforms. Immersive, decentralized,
and permanent characteristics of the Metaverse generate new vulnerabilities.
Although there are many countermeasures to these vulnerabilities, most of them
are theoretical or have not been tested in real-world environments. Our review
highlights current advancements, identifies research gaps, and outlines future
directions to ensure a secure, resilient, and ethically governed Metaverse.

</details>


### [39] [A Survey of Transaction Tracing Techniques for Blockchain Systems](https://arxiv.org/abs/2510.09624)
*Ayush Kumar,Vrizlynn L. L. Thing*

Main category: cs.CR

TL;DR: 对区块链跨账本交易追踪技术的系统综述，包括分类、比较现有方法，分析局限性并提出未来研究方向


<details>
  <summary>Details</summary>
Motivation: 随着跨链交易的普及，犯罪分子可能利用跨账本交易来隐藏资产流向，因此需要研究跨账本交易追踪技术

Method: 对文献中提出的区块链交易追踪技术进行系统回顾，使用多种标准（如追踪方法和目标）进行分类和比较

Result: 提供了区块链交易追踪文献现状的见解，识别出现有方法的局限性

Conclusion: 基于分析提出了该领域未来研究的方向

Abstract: With the proliferation of new blockchain-based cryptocurrencies/assets and
platforms that make it possible to transact across them, it becomes important
to consider not just whether the transfer of coins/assets can be tracked within
their respective transaction ledger, but also if they can be tracked as they
move across ledgers. This is especially important given that there are
documented cases of criminals attempting to use these cross-ledger trades to
obscure the flow of their coins/assets. In this paper, we perform a systematic
review of the various tracing techniques for blockchain transactions proposed
in literature, categorize them using multiple criteria (such as tracing
approach and targeted objective) and compare them. Based on the above
categorization, we provide insights on the state of blockchain transaction
tracing literature and identify the limitations of existing approaches.
Finally, we suggest directions for future research in this area based on our
analysis.

</details>


### [40] [Smart Medical IoT Security Vulnerabilities: Real-Time MITM Attack Analysis, Lightweight Encryption Implementation, and Practitioner Perceptions in Underdeveloped Nigerian Healthcare Systems](https://arxiv.org/abs/2510.09629)
*Aminu Muhammad Auwal*

Main category: cs.CR

TL;DR: 该研究通过模拟中间人攻击测试了尼日利亚医疗物联网设备的安全漏洞，并评估了轻量级AES-128加密方案的有效性，证明其能在低成本设备上提供实用保护。


<details>
  <summary>Details</summary>
Motivation: 随着物联网技术在尼日利亚医疗领域的应用增加，无线通信安全问题日益突出，未加密的患者数据面临网络威胁，需要寻找适合资源受限环境的低成本安全解决方案。

Method: 构建基于NodeMCU ESP8266的原型医疗物联网设备，在模拟医疗网络环境中进行实时中间人攻击测试，比较加密前后的数据传输安全性，并调查医疗专业人员的安全意识。

Result: AES-128加密使所有传输数据无法读取，篡改尝试失败，性能影响适中（延迟从80ms增至125ms，CPU使用率从30%增至45%），设备成本控制在18,000奈拉以内，医疗专业人员对加密和培训持支持态度。

Conclusion: 轻量级AES-128加密为常见攻击向量提供了实用、低成本的保护，同时保持操作效率，需要提高安全意识和建立临床部署指南。

Abstract: The growing use of Internet of Things (IoT) technologies in Nigerian
healthcare offers potential improvements in remote monitoring and data-driven
care, but unsecured wireless communication in medical IoT (mIoT) devices
exposes patient data to cyber threats. This study investigates such
vulnerabilities through a real-time Man in the Middle (MITM) attack simulation
and evaluates lightweight AES-128 encryption on low-cost devices.
  A prototype mIoT device was built with a NodeMCU ESP8266 and sensors for
heart rate and temperature. In controlled lab conditions simulating local
healthcare networks, unencrypted data transmissions were intercepted and
altered using common tools (Bettercap, Wireshark). After AES-128 encryption was
applied, all transmissions became unreadable and tamper attempts failed,
demonstrating its effectiveness.
  Performance costs were modest, latency rose from 80 ms to 125 ms (56.25
percent increase) and CPU use from 30 percent to 45 percent, but system
stability remained intact. Device cost stayed under 18,000 NGN (about 12 USD),
making it feasible for Nigeria's resource constrained facilities.
  A survey of healthcare professionals showed moderate awareness of IoT-related
risks but strong support for encryption and staff training. Barriers included
limited budgets and technical complexity.
  The study concludes that lightweight AES-128 encryption provides practical,
low-cost protection against common attack vectors while maintaining operational
efficiency. Feedback from professionals highlights the urgency of improving
security awareness and establishing guidelines for clinical deployment.

</details>


### [41] [Hound: Relation-First Knowledge Graphs for Complex-System Reasoning in Security Audits](https://arxiv.org/abs/2510.09633)
*Bernhard Mueller*

Main category: cs.CR

TL;DR: Hound是一个基于关系优先图引擎的代码分析系统，通过分析师定义的关系视图和持久信念系统，在复杂代码库中提高漏洞检测的召回率和F1分数。


<details>
  <summary>Details</summary>
Motivation: 传统代码分析工具在处理复杂代码库中跨组件的系统级推理时存在局限，需要更灵活的关系建模和持续的证据积累机制。

Method: 使用关系优先图引擎构建分析师定义的关系视图，结合持久信念系统维护漏洞假设，采用覆盖度与直觉规划策略和QA最终确认机制。

Result: 在ScaBench的五个项目子集上，相比基线LLM分析器，Hound将微召回率从8.3%提升到31.2%，F1分数从9.8%提升到14.2%，但精度有所下降。

Conclusion: 关系优先图扩展了模型对抽象方面的理解，结合假设中心的循环机制，显著提升了代码漏洞检测能力。

Abstract: Hound introduces a relation-first graph engine that improves system-level
reasoning across interrelated components in complex codebases. The agent
designs flexible, analyst-defined views with compact annotations (e.g.,
monetary/value flows, authentication/authorization roles, call graphs, protocol
invariants) and uses them to anchor exact retrieval: for any question, it loads
precisely the code that matters (often across components) so it can zoom out to
system structure and zoom in to the decisive lines. A second contribution is a
persistent belief system: long-lived vulnerability hypotheses whose confidence
is updated as evidence accrues. The agent employs coverage-versus-intuition
planning and a QA finalizer to confirm or reject hypotheses. On a five-project
subset of ScaBench[1], Hound improves recall and F1 over a baseline LLM
analyzer (micro recall 31.2% vs. 8.3%; F1 14.2% vs. 9.8%) with a modest
precision trade-off. We attribute these gains to flexible, relation-first
graphs that extend model understanding beyond call/dataflow to abstract
aspects, plus the hypothesis-centric loop; code and artifacts are released to
support reproduction.

</details>


### [42] [A Method for Quantifying Human Risk and a Blueprint for LLM Integration](https://arxiv.org/abs/2510.09635)
*Giuseppe Canale*

Main category: cs.CR

TL;DR: 提出了网络安全心理学框架(CPF)，通过将心理学构念与安全运营数据系统集成，量化人为因素漏洞的新方法


<details>
  <summary>Details</summary>
Motivation: 现有研究孤立分析人为因素（如警报疲劳、合规疲劳等），但缺乏端到端的框架来全面操作化心理漏洞

Method: 1) 定义可测量的算法量化心理状态；2) 提出基于RAG和领域微调的轻量级LLM架构；3) 详细混合方法验证策略

Result: 概念验证部署中，在合成数据上达到0.92 F1分数

Conclusion: 为行业合作进行实证验证提供了理论和方法基础

Abstract: This paper presents the Cybersecurity Psychology Framework (CPF), a novel
methodology for quantifying human-centric vulnerabilities in security
operations through systematic integration of established psychological
constructs with operational security telemetry. While individual human
factors-alert fatigue, compliance fatigue, cognitive overload, and risk
perception biases-have been extensively studied in isolation, no framework
provides end-to-end operationalization across the full spectrum of
psychological vulnerabilities. We address this gap by: (1) defining specific,
measurable algorithms that quantify key psychological states using standard SOC
tooling (SIEM, ticketing systems, communication platforms); (2) proposing a
lightweight, privacy-preserving LLM architecture based on Retrieval-Augmented
Generation (RAG) and domain-specific fine-tuning to analyze structured and
unstructured data for latent psychological risks; (3) detailing a rigorous
mixed-methods validation strategy acknowledging the inherent difficulty of
obtaining sensitive cybersecurity data. Our implementation of CPF indicators
has been demonstrated in a proof-of-concept deployment using small language
models achieving 0.92 F1-score on synthetic data. This work provides the
theoretical and methodological foundation necessary for industry partnerships
to conduct empirical validation with real operational data.

</details>


### [43] [AdaptAuth: Multi-Layered Behavioral and Credential Analysis for a Secure and Adaptive Authentication Framework for Password Security](https://arxiv.org/abs/2510.09645)
*Tonmoy Ghosh*

Main category: cs.CR

TL;DR: 提出一个多因素密码安全框架，通过整合用户行为、设备特征、网络参数等多种属性，使用学习模型构建用户画像，实现更强的安全保护和更好的用户体验。


<details>
  <summary>Details</summary>
Motivation: 传统密码安全实践日益复杂导致用户依从性差，弱密码管理使个人面临攻击风险，需要全面防御机制来应对密码相关威胁。

Method: 集成密码解析机制、动态密码策略机制、人类行为模式、设备特征、网络参数、地理位置等多种因素，利用学习模型构建详细用户画像。

Result: 能够识别个体用户并阻止几乎所有形式的未授权访问或设备持有，提供比现有标准更强的保护。

Conclusion: 该框架通过新颖的自适应方法增强了可用性-安全性范式，在提供更强保护的同时让用户参与策略设置过程。

Abstract: Password security has been compelled to evolve in response to the growing
computational capabilities of modern systems. However, this evolution has often
resulted in increasingly complex security practices that alienate users,
leading to poor compliance and heightened vulnerability. Consequently,
individuals remain exposed to attackers through weak or improperly managed
passwords, underscoring the urgent need for a comprehensive defense mechanism
that effectively addresses password-related risks and threats. In this paper,
we propose a multifaceted solution designed to revolutionize password security
by integrating diverse attributes such as the Password Dissection Mechanism,
Dynamic Password Policy Mechanism, human behavioral patterns, device
characteristics, network parameters, geographical context, and other relevant
factors. By leveraging learning-based models, our framework constructs detailed
user profiles capable of recognizing individuals and preventing nearly all
forms of unauthorized access or device possession. The proposed framework
enhances the usability-security paradigm by offering stronger protection than
existing standards while simultaneously engaging users in the policy-setting
process through a novel, adaptive approach.

</details>


### [44] [Rounding-Guided Backdoor Injection in Deep Learning Model Quantization](https://arxiv.org/abs/2510.09647)
*Xiangxiang Chen,Peixin Zhang,Jun Sun,Wenhai Wang,Jingyi Wang*

Main category: cs.CR

TL;DR: QuRA是一种利用模型量化操作的新型后门攻击方法，通过选择关键权重并优化其舍入方向来嵌入恶意行为，攻击成功率接近100%且性能损失极小。


<details>
  <summary>Details</summary>
Motivation: 模型量化虽然有助于在资源受限环境中部署深度学习模型，但可能引入被忽视的安全风险。本研究旨在揭示量化过程中的安全漏洞。

Method: 采用新颖的权重选择策略识别影响后门目标的关键权重，然后通过优化这些权重的舍入方向来增强后门效果，同时保持模型整体性能。

Result: 实验表明QuRA在大多数情况下达到接近100%的攻击成功率，且性能退化可忽略不计，能够绕过现有的后门防御机制。

Conclusion: 研究揭示了广泛使用的模型量化过程中的关键漏洞，强调了需要更强大的安全措施来应对此类威胁。

Abstract: Model quantization is a popular technique for deploying deep learning models
on resource-constrained environments. However, it may also introduce previously
overlooked security risks. In this work, we present QuRA, a novel backdoor
attack that exploits model quantization to embed malicious behaviors. Unlike
conventional backdoor attacks relying on training data poisoning or model
training manipulation, QuRA solely works using the quantization operations. In
particular, QuRA first employs a novel weight selection strategy to identify
critical weights that influence the backdoor target (with the goal of
perserving the model's overall performance in mind). Then, by optimizing the
rounding direction of these weights, we amplify the backdoor effect across
model layers without degrading accuracy. Extensive experiments demonstrate that
QuRA achieves nearly 100% attack success rates in most cases, with negligible
performance degradation. Furthermore, we show that QuRA can adapt to bypass
existing backdoor defenses, underscoring its threat potential. Our findings
highlight critical vulnerability in widely used model quantization process,
emphasizing the need for more robust security measures. Our implementation is
available at https://github.com/cxx122/QuRA.

</details>


### [45] [Learning Cybersecurity vs. Ethical Hacking: A Comparative Pathway for Aspiring Students](https://arxiv.org/abs/2510.09650)
*Fahed Quttainah*

Main category: cs.CR

TL;DR: 本文探讨网络安全与道德黑客之间的区别与联系，分析各自的定义、目标、方法论以及学术与职业发展路径，强调两者在增强全球网络韧性中的互补作用。


<details>
  <summary>Details</summary>
Motivation: 明确网络安全和道德黑客这两个重要数字系统保护领域的区别与联系，为有志于此领域的学生提供学术和职业发展指导。

Method: 通过定义两个领域、比较目标和方法论、分析学术和职业路径、评估关键技能和认证要求。

Result: 网络安全被定义为专注于预防攻击和保护数据的防御性学科，而道德黑客采用授权测试识别漏洞的进攻性方法，两者在技能、认证和职业机会上各有侧重。

Conclusion: 网络安全和道德黑客在增强全球网络韧性方面具有互补性，学习者应根据个人兴趣和抱负选择最适合的发展路径。

Abstract: This paper explores the distinctions and connections between cybersecurity
and ethical hacking, two vital disciplines in the protection of digital
systems. It defines each field, outlines their goals and methodologies, and
compares the academic and professional paths available to aspiring students.
Cybersecurity is presented as a defensive discipline focused on preventing
attacks and safeguarding data, while ethical hacking adopts an offensive
approach that identifies vulnerabilities through authorized testing. The paper
highlights key skills, certifications, and career opportunities in both areas,
offering practical guidance to help learners choose the path best suited to
their interests and ambitions. Ultimately, it emphasizes the complementary
nature of both fields in strengthening global cyber resilience.

</details>


### [46] [Data Provenance Auditing of Fine-Tuned Large Language Models with a Text-Preserving Technique](https://arxiv.org/abs/2510.09655)
*Yanming Li,Seifeddine Ghozzi,Cédric Eichler,Nicolas Anciaux,Alexandra Bensamoun,Lorena Gonzalez Manzano*

Main category: cs.CR

TL;DR: 提出了一种基于不可见Unicode字符的水印框架，用于在黑盒访问下审计大型语言模型是否使用了敏感或受版权保护的文本进行微调。该方法通过嵌入提示和回复水印序列，在模型输出中检测对应的回复来证明训练数据的记忆。


<details>
  <summary>Details</summary>
Motivation: 解决现有方法（逐字复述和成员推理）在单个文档级别不可靠或需要修改可见文本的问题，需要一种文本保留、可扩展且可靠的方法来审计LLM训练数据的使用情况。

Method: 使用不可见Unicode字符嵌入水印序列，将水印分为提示（嵌入奇数块）和回复（嵌入偶数块）。审计时提交仅包含提示的查询，检测模型输出中是否存在对应的回复。采用排名测试和反事实水印比较来确保决策的可靠性。

Result: 在开放权重的LLM和多个文本领域上评估，检测失败率低于0.1%（使用50个标记文档微调后检测回复），在18,000多次挑战中未发现虚假回复（100%真阳性率@0%假阳性率）。即使标记集合仅占微调数据的0.33%，每个文档的检测率仍保持在45%以上。

Conclusion: 该方法提供了一种可靠的事后溯源信号，具有有界假阳性率，对常见被动变换具有鲁棒性，可扩展到多用户和多文档场景，且对训练集大小变化保持相对稳定的检测性能。

Abstract: We address the problem of auditing whether sensitive or copyrighted texts
were used to fine-tune large language models (LLMs) under black-box access.
Prior signals-verbatim regurgitation and membership inference-are unreliable at
the level of individual documents or require altering the visible text. We
introduce a text-preserving watermarking framework that embeds sequences of
invisible Unicode characters into documents. Each watermark is split into a cue
(embedded in odd chunks) and a reply (embedded in even chunks). At audit time,
we submit prompts that contain only the cue; the presence of the corresponding
reply in the model's output provides evidence of memorization consistent with
training on the marked text. To obtain sound decisions, we compare the score of
the published watermark against a held-out set of counterfactual watermarks and
apply a ranking test with a provable false-positive-rate bound. The design is
(i) minimally invasive (no visible text changes), (ii) scalable to many users
and documents via a large watermark space and multi-watermark attribution, and
(iii) robust to common passive transformations. We evaluate on open-weight LLMs
and multiple text domains, analyzing regurgitation dynamics, sensitivity to
training set size, and interference under multiple concurrent watermarks. Our
results demonstrate reliable post-hoc provenance signals with bounded FPR under
black-box access. We experimentally observe a failure rate of less than 0.1\%
when detecting a reply after fine-tuning with 50 marked documents. Conversely,
no spurious reply was recovered in over 18,000 challenges, corresponding to a
100\%TPR@0\% FPR. Moreover, detection rates remain relatively stable as the
dataset size increases, maintaining a per-document detection rate above 45\%
even when the marked collection accounts for less than 0.33\% of the
fine-tuning data.

</details>


### [47] [Signing Right Away](https://arxiv.org/abs/2510.09656)
*Yejun Jang*

Main category: cs.CR

TL;DR: SRA是一个全面的安全架构，通过在可信执行环境中保护整个成像管道，为数字媒体提供从捕获到文件的不可变可验证来源证明。


<details>
  <summary>Details</summary>
Motivation: 高保真合成媒体的泛滥和传统成像管道中的硬件漏洞导致数字内容的信任危机，现有对策无法在捕获时刻建立与现实的不可破坏链接。

Method: 采用四支柱安全模型（机密性、完整性、认证和重放保护），在可信执行环境中保护整个成像管道，创建符合C2PA标准的加密密封最终资产。

Result: SRA确保每个捕获的图像和视频都带有不可变的可验证来源证明，为依赖可信视觉信息的行业提供基础解决方案。

Conclusion: SRA被定位为内容信任链中必不可少的"最后一英里"解决方案，通过完整的架构实现和实证原型验证了其可行性。

Abstract: The proliferation of high-fidelity synthetic media, coupled with exploitable
hardware vulnerabilities in conventional imaging pipelines, has precipitated a
crisis of trust in digital content. Existing countermeasures, from post-hoc
classifiers to software-based signing, fail to address the fundamental
challenge of establishing an unbreakable link to reality at the moment of
capture. This whitepaper introduces Signing Right Away (SRA), a comprehensive
security architecture that guarantees the provenance of digital media from
"silicon to silicon to signed file." SRA leverages a four-pillar security
model-Confidentiality, Integrity, Authentication, and Replay Protection, akin
to the MIPI Camera Security Framework (CSF), but also extends its scope beyond
the internal data bus to the creation of a cryptographically sealed,
C2PA-compliant final asset. By securing the entire imaging pipeline within a
Trusted Execution Environment (TEE), SRA ensures that every captured image and
video carries an immutable, verifiable proof of origin. This provides a
foundational solution for industries reliant on trustworthy visual information,
including journalism, legal evidence, and insurance. We present the SRA
architecture, a detailed implementation roadmap informed by empirical
prototyping, and a comparative analysis that positions SRA as the essential
"last mile" in the chain of content trust.

</details>


### [48] [Core Mondrian: Basic Mondrian beyond k-anonymity](https://arxiv.org/abs/2510.09661)
*Adam Bloomston,Elizabeth Burke,Megan Cacace,Anne Diaz,Wren Dougherty,Matthew Gonzalez,Remington Gregg,Yeliz Güngör,Bryce Hayes,Eeway Hsu,Oron Israeli,Heesoo Kim,Sara Kwasnick,Joanne Lacsina,Demma Rosa Rodriguez,Adam Schiller,Whitney Schumacher,Jessica Simon,Maggie Tang,Skyler Wharton,Marilyn Wilcken*

Main category: cs.CR

TL;DR: Core Mondrian是一个可扩展的Mondrian分区匿名化算法扩展，支持k-匿名性，利用多核并行处理，在保持确定性输出的同时实现高达4倍加速，并在大规模数据集上获得更好的效用指标。


<details>
  <summary>Details</summary>
Motivation: 原始Mondrian算法在处理大规模数据时存在性能瓶颈，需要开发一个可扩展的并行版本来支持生产规模的隐私合规分析。

Method: 采用模块化策略层支持k-匿名性，混合递归/队列执行引擎实现多核并行，通过NaN模式预分区、度量驱动的切割评分和动态抑制预算管理等技术提升效用。

Result: 在48k记录的UCI ADULT数据集和扩展到1M记录的合成数据集上，相比原始Mondrian获得更低的Discernibility Metric分数，并行处理相比顺序Core Mondrian实现高达4倍加速。

Conclusion: Core Mondrian能够实现生产规模的隐私合规权益分析，为大规模数据匿名化提供了高效且实用的解决方案。

Abstract: We present Core Mondrian, a scalable extension of the Original Mondrian
partition-based anonymization algorithm. A modular strategy layer supports
k-anonymity, allowing new privacy models to be added easily. A hybrid
recursive/queue execution engine exploits multi-core parallelism while
maintaining deterministic output. Utility-preserving enhancements include
NaN-pattern pre-partitioning, metric-driven cut scoring, and dynamic
suppression budget management. Experiments on the 48k-record UCI ADULT dataset
and synthetically scaled versions up to 1M records achieve lower Discernibility
Metric scores than Original Mondrian for numeric quasi-identifier sets while
parallel processing delivers up to 4x speedup vs. sequential Core Mondrian.
Core Mondrian enables privacy-compliant equity analytics at production scale.

</details>


### [49] [Adversarial-Resilient RF Fingerprinting: A CNN-GAN Framework for Rogue Transmitter Detection](https://arxiv.org/abs/2510.09663)
*Raju Dhakal,Prashant Shekhar,Laxima Niure Kandel*

Main category: cs.CR

TL;DR: 提出基于CNN和softmax概率阈值的射频指纹识别框架，用于检测恶意设备并识别合法设备，通过GAN模拟攻击场景进行验证。


<details>
  <summary>Details</summary>
Motivation: 射频指纹识别通过利用信号生成过程中硬件组件的独特缺陷来认证设备，需要有效检测恶意设备模仿合法设备RF特征的攻击。

Method: 使用卷积神经网络框架结合softmax概率阈值检测恶意设备和识别合法设备，通过生成对抗网络模拟攻击者模仿合法设备RF特征的行为。

Result: 使用10个ADALM-PLUTO软件定义无线电收集的IQ样本进行验证，其中7个设备为合法设备，2个为恶意设备，1个用于确定阈值。

Conclusion: 提出的CNN框架结合softmax概率阈值能够有效检测恶意设备并识别合法设备，在模拟攻击场景下表现出良好的性能。

Abstract: Radio Frequency Fingerprinting (RFF) has evolved as an effective solution for
authenticating devices by leveraging the unique imperfections in hardware
components involved in the signal generation process. In this work, we propose
a Convolutional Neural Network (CNN) based framework for detecting rogue
devices and identifying genuine ones using softmax probability thresholding. We
emulate an attack scenario in which adversaries attempt to mimic the RF
characteristics of genuine devices by training a Generative Adversarial Network
(GAN) using In-phase and Quadrature (IQ) samples from genuine devices. The
proposed approach is verified using IQ samples collected from ten different
ADALM-PLUTO Software Defined Radios (SDRs), with seven devices considered
genuine, two as rogue, and one used for validation to determine the threshold.

</details>


### [50] [Pingmark: A Textual Protocol for Universal Spatial Mentions](https://arxiv.org/abs/2510.09672)
*Kalin Dimitrov*

Main category: cs.CR

TL;DR: Pingmark是一个用于表达空间上下文的通用文本协议，使用!@符号来生成标准化的位置解析链接，无需用户注册，保护隐私，旨在成为开放互联网标准。


<details>
  <summary>Details</summary>
Motivation: 创建一种类似@身份标识或#话题标签的文本约定，但用于物理空间表达，避免嵌入坐标或使用专有地图链接。

Method: 定义Pingmark协议规范(PPS v0.1)，使用!@语义触发器，客户端应用将其解释为https://pingmark.me/lat/lon/[timestamp]格式的标准化解析链接。

Result: 开发了参考解析器实现，协议无需用户注册，依赖开放地图技术，通过本地临时生成位置数据来保护隐私。

Conclusion: Pingmark协议旨在成为空间提及的开放互联网标准，提供统一的位置表达方式。

Abstract: Pingmark defines a universal textual protocol for expressing spatial context
through a minimal symbol: !@. Rather than embedding coordinates or using
proprietary map links, Pingmark introduces a semantic trigger that compliant
client applications interpret to generate a standardized resolver link of the
form https://pingmark.me/lat/lon/[timestamp]. This allows location expression
to function like existing textual conventions - @ for identity or # for topics
- but for physical space. The protocol requires no user registration, relies on
open mapping technologies, and protects privacy by generating location data
ephemerally and locally. This paper presents the motivation, syntax, and design
of the Pingmark Protocol Specification (PPS v0.1), its reference resolver
implementation, and the long-term goal of establishing Pingmark as an open
Internet standard for spatial mentions.

</details>


### [51] [Cybersecurity Competence for Organisations in Inner Scandinavia](https://arxiv.org/abs/2510.09673)
*Simone Fischer-Hübner,Leonardo A. Martucci,Lejla Islami,Ala Sarah Alaqra,Farzaneh Karegar*

Main category: cs.CR

TL;DR: 研究瑞典韦姆兰地区企业和公共部门组织的网络安全准备情况、教育需求及能力发展需求，并提出加强内斯堪的纳维亚地区网络安全能力的建议。


<details>
  <summary>Details</summary>
Motivation: 网络安全威胁和事件快速增长，瑞典组织需要加强网络安全能力建设。

Method: 通过访谈和先前调查，收集瑞典韦姆兰地区企业和公共部门组织关键代表的意见。

Result: 分析了组织的网络安全准备情况、教育和能力发展需求。

Conclusion: 提出加强内斯堪的纳维亚地区网络安全能力的建议，并讨论了研究结果的普适性和地区特异性。

Abstract: A rapidly growing number of cybersecurity threats and incidents demands that
Swedish organisations increase their efforts to improve their cybersecurity
capacities. This paper presents results from interviews and a prior survey with
key representatives from enterprises and public sector organisations in the
Swedish region of V\"armland in Inner Scandinavia, examining their
cybersecurity readiness and needs for education and competence development. We
discuss the generalizability of our findings and the extent to which they may
be specific to Sweden and V\"armland, and we conclude by proposing efforts to
strengthen cybersecurity competences in Inner Scandinavia.

</details>


### [52] [Advancing Security in Software-Defined Vehicles: A Comprehensive Survey and Taxonomy](https://arxiv.org/abs/2510.09675)
*Khaoula Sghaier,Badis Hammi,Ghada Gharbi,Pierre Merdrignac,Pierre Parrend,Didier Verna*

Main category: cs.CR

TL;DR: 本文对软件定义车辆(SDVs)进行了全面分析，重点研究了其生态系统、使能技术以及由架构和操作特性产生的主要网络攻击入口点，并提出了一个新颖的分层分类法。


<details>
  <summary>Details</summary>
Motivation: 软件定义车辆通过集成外包应用和持续OTA更新扩展了车辆生命周期，但这也带来了网络安全和系统弹性的需求。目前研究缺乏对SDV与非SDV的明确区分，且需要整合网络安全研究。

Method: 详细分析SDV生态系统、使能技术和主要网络攻击入口点，并引入一个新颖的分层分类法，将具体利用技术映射到核心SDV属性和攻击路径上，用于分析代表性研究和实验方法。

Result: 识别了SDV由于广泛连接性带来的更广泛攻击面，以及软件中心特性引入的额外漏洞，提供了对SDV网络安全威胁的全面理解。

Conclusion: 软件定义车辆在带来创新功能的同时，也面临着显著增加的网络安全挑战，需要专门的安全框架和分类法来应对这些威胁。

Abstract: Software-Defined Vehicles (SDVs) introduce innovative features that extend
the vehicle's lifecycle through the integration of outsourced applications and
continuous Over-The-Air (OTA) updates. This shift necessitates robust
cybersecurity and system resilience. While research on Connected and Autonomous
Vehicles (CAV) has been extensive, there is a lack of clarity in distinguishing
SDVs from non-SDVs and a need to consolidate cybersecurity research. SDVs, with
their extensive connectivity, have a broader attack surface. Besides, their
software-centric nature introduces additional vulnerabilities. This paper
provides a comprehensive examination of SDVs, detailing their ecosystem,
enabling technologies, and the principal cyberattack entry points that arise
from their architectural and operational characteristics. We also introduce a
novel, layered taxonomy that maps concrete exploit techniques onto core SDV
properties and attack paths, and use it to analyze representative studies and
experimental approaches.

</details>


### [53] [Fortifying LLM-Based Code Generation with Graph-Based Reasoning on Secure Coding Practices](https://arxiv.org/abs/2510.09682)
*Rupam Patir,Keyan Guo,Haipeng Cai,Hongxin Hu*

Main category: cs.CR

TL;DR: GRASP通过构建安全编码实践图和有向无环图推理过程，在不依赖额外训练或外部反馈的情况下，显著提升LLM生成代码的安全性，特别是在零日漏洞方面表现突出。


<details>
  <summary>Details</summary>
Motivation: LLM生成的代码常包含安全漏洞，现有方法依赖额外训练或静态分析，但存在资源密集、适应性差、不适用于专有模型等问题。

Method: 构建安全编码实践图(SCP图)组织安全实践为有向无环图，并设计基于图的推理过程系统性地指导LLM生成安全代码。

Result: 在多个LLM上安全率超过80%，在零日漏洞上比基线提升高达88%。

Conclusion: GRASP提供了一种可解释、模型无关且可扩展的安全改进方法，特别适用于未知漏洞场景。

Abstract: The code generation capabilities of Large Language Models (LLMs) have
transformed the field of software development. However, this advancement also
presents significant security challenges, as LLM-generated code often contains
vulnerabilities. One direction of research strengthens LLMs by injecting or
refining security knowledge through curated datasets, model tuning, or static
analyzers. While effective in certain settings, these methods can be
resource-intensive, less adaptable to zero-day vulnerabilities, and often
inapplicable to proprietary models. To address these challenges, we introduce
GRASP, which explores a new direction that focuses on structured reasoning over
Secure Coding Practices(SCPs) rather than additional training or external
feedback. GRASP comprises two key ideas: (1) an SCP graph that organizes SCPs
into a Directed Acyclic Graph (DAG) capturing dependencies and relationships,
and (2) a graph-based reasoning process that systematically guides LLMs through
relevant SCPs for code generation. This design enables interpretable,
model-agnostic, and scalable security improvements, particularly for previously
unseen vulnerabilities. Our evaluation shows that GRASP consistently achieves
Security Rates (SR) exceeding 80% across multiple LLMs, and delivers up to 88%
improvements over baselines on zero-day vulnerabilities.

</details>


### [54] [CREST-Search: Comprehensive Red-teaming for Evaluating Safety Threats in Large Language Models Powered by Web Search](https://arxiv.org/abs/2510.09689)
*Haoran Ou,Kangjie Chen,Xingshuo Han,Gelei Deng,Jie Zhang,Han Qiu,Tianwei Zhang*

Main category: cs.CR

TL;DR: CREST-Search是一个针对支持网络搜索的LLMs的红队测试框架，通过生成对抗性查询和迭代反馈来暴露安全风险，实验证明能有效绕过安全过滤器。


<details>
  <summary>Details</summary>
Motivation: LLMs在接入网络搜索后虽然能获取实时信息，但也放大了安全风险，因为对抗性提示与不可信来源结合可能导致严重漏洞。

Method: 使用上下文学习生成对抗性查询，并通过迭代反馈进行优化，构建了WebSearch-Harm数据集来微调LLMs成为高效的红队测试代理。

Result: CREST-Search能够有效绕过现代网络增强LLMs的安全过滤器，揭示其脆弱性。

Conclusion: 需要专门的安全防御措施来确保网络增强LLMs的可信部署。

Abstract: Large Language Models (LLMs) excel at tasks such as dialogue, summarization,
and question answering, yet they struggle to adapt to specialized domains and
evolving facts. To overcome this, web search has been integrated into LLMs,
allowing real-time access to online content. However, this connection magnifies
safety risks, as adversarial prompts combined with untrusted sources can cause
severe vulnerabilities. We investigate red teaming for LLMs with web search and
present CREST-Search, a framework that systematically exposes risks in such
systems. Unlike existing methods for standalone LLMs, CREST-Search addresses
the complex workflow of search-enabled models by generating adversarial queries
with in-context learning and refining them through iterative feedback. We
further construct WebSearch-Harm, a search-specific dataset to fine-tune LLMs
into efficient red-teaming agents. Experiments show that CREST-Search
effectively bypasses safety filters and reveals vulnerabilities in modern
web-augmented LLMs, underscoring the need for specialized defenses to ensure
trustworthy deployment.

</details>


### [55] [A Semantic Model for Audit of Cloud Engines based on ISO/IEC TR 3445:2022](https://arxiv.org/abs/2510.09690)
*Morteza Sargolzaei Javan*

Main category: cs.CR

TL;DR: 本文提出了一个基于语义的云引擎模型，将ISO/IEC 22123云参考架构与ISO/IEC 27001:2022和ISO/IEC TR 3445:2022安全合规控制集成，通过RDF/Turtle实现自动化合规验证和架构设计。


<details>
  <summary>Details</summary>
Motivation: 云计算缺乏统一的架构和合规框架，阻碍了互操作性、可审计性和安全性。

Method: 将云系统分解为控制、业务、审计和数据四个规范接口，并扩展安全本体，使用RDF/Turtle表达模型，支持SPARQL和SHACL进行验证。

Result: 通过OpenStack和AWS案例研究验证了模型的实用性，实现了语义推理和自动化合规验证。

Conclusion: 该工作通过统一框架连接架构和合规标准，推进了云安全建模的发展，特别强调可审计性。

Abstract: Cloud computing has become the foundation of modern digital infrastructure,
yet the absence of a unified architectural and compliance framework impedes
interoperability, auditability, and robust security. This paper introduces a
formal, machine-readable semantic model for Cloud Engines, integrating the
architectural taxonomy of ISO/IEC 22123 (Cloud Reference Architecture) with the
security and compliance controls of ISO/IEC 27001:2022 and ISO/IEC TR
3445:2022. The model decomposes cloud systems into four canonical
interfaces--Control, Business, Audit, and Data--and extends them with a
security ontology that maps mechanisms such as authentication, authorization,
and encryption to specific compliance controls. Expressed in RDF/Turtle, the
model enables semantic reasoning, automated compliance validation, and
vendor-neutral architecture design. We demonstrate its practical utility
through OpenStack and AWS case studies, and provide reproducible validation
workflows using SPARQL and SHACL. This work advances the state of cloud
security modeling by bridging architectural and compliance standards in a
unified framework, with a particular emphasis on auditability.

</details>


### [56] [VisualDAN: Exposing Vulnerabilities in VLMs with Visual-Driven DAN Commands](https://arxiv.org/abs/2510.09699)
*Aofan Liu,Lulu Tang*

Main category: cs.CR

TL;DR: 本文提出了VisualDAN，一种基于对抗性图像的视觉语言模型越狱攻击方法，通过在图像中嵌入类似DAN命令的恶意指令，成功绕过多模态模型的安全防护机制。


<details>
  <summary>Details</summary>
Motivation: 视觉语言模型(VLMs)虽然具有强大的多模态理解能力，但其安全防护面临新的挑战，特别是图像劫持等新型漏洞可能导致模型产生有害输出。

Method: 创建包含DAN风格命令的对抗性图像，在恶意查询前添加肯定前缀来欺骗模型，将对抗图像在DAN启发的有害文本上训练并转换到文本域以引发恶意输出。

Result: 在MiniGPT-4、MiniGPT-v2、InstructBLIP和LLaVA等模型上的实验表明，VisualDAN能有效绕过对齐VLMs的安全防护，迫使模型执行严重违反道德标准的有害指令。

Conclusion: 即使少量有害内容也能在被攻破的模型中显著放大有害输出，这凸显了开发针对图像攻击的鲁棒防御的紧迫性，为VLM对齐和安全研究提供了重要见解。

Abstract: Vision-Language Models (VLMs) have garnered significant attention for their
remarkable ability to interpret and generate multimodal content. However,
securing these models against jailbreak attacks continues to be a substantial
challenge. Unlike text-only models, VLMs integrate additional modalities,
introducing novel vulnerabilities such as image hijacking, which can manipulate
the model into producing inappropriate or harmful responses. Drawing
inspiration from text-based jailbreaks like the "Do Anything Now" (DAN)
command, this work introduces VisualDAN, a single adversarial image embedded
with DAN-style commands. Specifically, we prepend harmful corpora with
affirmative prefixes (e.g., "Sure, I can provide the guidance you need") to
trick the model into responding positively to malicious queries. The
adversarial image is then trained on these DAN-inspired harmful texts and
transformed into the text domain to elicit malicious outputs. Extensive
experiments on models such as MiniGPT-4, MiniGPT-v2, InstructBLIP, and LLaVA
reveal that VisualDAN effectively bypasses the safeguards of aligned VLMs,
forcing them to execute a broad range of harmful instructions that severely
violate ethical standards. Our results further demonstrate that even a small
amount of toxic content can significantly amplify harmful outputs once the
model's defenses are compromised. These findings highlight the urgent need for
robust defenses against image-based attacks and offer critical insights for
future research into the alignment and security of VLMs.

</details>


### [57] [A Comprehensive Survey on Smart Home IoT Fingerprinting: From Detection to Prevention and Practical Deployment](https://arxiv.org/abs/2510.09700)
*Eduardo Baena,Han Yang,Dimitrios Koutsonikolas,Israat Haque*

Main category: cs.CR

TL;DR: 对智能家居中物联网设备指纹识别技术的全面调查分析，涵盖设备识别、事件检测、分类和入侵预防方法，讨论现有技术的适用性和局限性，以及部署挑战和新兴机遇。


<details>
  <summary>Details</summary>
Motivation: 智能家居中异构物联网设备的多样性给设备识别、认证和安全带来了关键挑战，指纹识别技术成为解决这些问题的重要方法。

Method: 通过调查分析现有指纹识别技术，包括网络流量分析和基于机器学习的方法，评估它们在资源受限设备、动态使用模式和隐私要求等智能家居环境中的适用性。

Result: 识别了现有指纹识别技术在智能家居环境中的局限性，并讨论了部署挑战如可扩展性、互操作性和能效问题，以及生成式AI和联邦学习带来的新机遇。

Conclusion: 需要进一步研究可靠且保护隐私的指纹识别技术，以推动下一代智能家居生态系统的发展，特别是在可扩展性、隐私保护和新兴技术应用方面。

Abstract: Smart homes are increasingly populated with heterogeneous Internet of Things
(IoT) devices that interact continuously with users and the environment. This
diversity introduces critical challenges in device identification,
authentication, and security, where fingerprinting techniques have emerged as a
key approach. In this survey, we provide a comprehensive analysis of IoT
fingerprinting specifically in the context of smart homes, examining methods
for device and their event detection, classification, and intrusion prevention.
We review existing techniques, e.g., network traffic analysis or machine
learning-based schemes, highlighting their applicability and limitations in
home environments characterized by resource-constrained devices, dynamic usage
patterns, and privacy requirements. Furthermore, we discuss fingerprinting
system deployment challenges like scalability, interoperability, and energy
efficiency, as well as emerging opportunities enabled by generative AI and
federated learning. Finally, we outline open research directions that can
advance reliable and privacy-preserving fingerprinting for next-generation
smart home ecosystems.

</details>


### [58] [A Demonstration of Self-Adaptive Jamming Attack Detection in AI/ML Integrated O-RAN](https://arxiv.org/abs/2510.09706)
*Md Habibur Rahman,Md Sharif Hossen,Nathan H. Stephenson,Vijay K. Shah,Aloizio Da Silva*

Main category: cs.CR

TL;DR: SAJD是一个自适应的干扰检测框架，用于在集成AI/ML的O-RAN环境中自动检测干扰攻击，无需人工干预。


<details>
  <summary>Details</summary>
Motivation: O-RAN虽然实现了模块化、智能化和可编程的5G网络架构，但干扰攻击会严重影响网络性能，这是O-RAN面临的安全问题之一。

Method: SAJD框架形成一个闭环系统，包括通过开发的基于ML的xApp进行无线电信号干扰的近实时推断，以及通过rApps进行持续监控和重新训练管道。

Result: 在O-RAN兼容测试平台上，SAJD在各种动态和先前未见过的干扰场景下，在准确性和适应性方面优于最先进的干扰检测xApp（离线训练且使用手动标签）。

Conclusion: SAJD框架能够有效检测O-RAN环境中的干扰攻击，并展现出优于现有方法的性能和适应性。

Abstract: The open radio access network (O-RAN) enables modular, intelligent, and
programmable 5G network architectures through the adoption of software-defined
networking, network function virtualization, and implementation of standardized
open interfaces. However, one of the security concerns for O-RAN, which can
severely undermine network performance, is jamming attacks. This paper presents
SAJD- a self-adaptive jammer detection framework that autonomously detects
jamming attacks in AI/ML framework-integrated ORAN environments without human
intervention. The SAJD framework forms a closed-loop system that includes
near-realtime inference of radio signal jamming via our developed ML-based
xApp, as well as continuous monitoring and retraining pipelines through rApps.
In this demonstration, we will show how SAJD outperforms state-of-the-art
jamming detection xApp (offline trained with manual labels) in terms of
accuracy and adaptability under various dynamic and previously unseen
interference scenarios in the O-RAN-compliant testbed.

</details>


### [59] [PrediQL: Automated Testing of GraphQL APIs with LLMs](https://arxiv.org/abs/2510.10407)
*Shaolun Liu,Sina Marefat,Omar Tsai,Yu Chen,Zecheng Deng,Jia Wang,Mohammad A. Tayebi*

Main category: cs.CR

TL;DR: PrediQL是首个基于检索增强和LLM引导的GraphQL API模糊测试工具，通过结合大语言模型推理和自适应反馈循环，生成语义有效且多样化的查询，显著提高了覆盖率和漏洞发现率。


<details>
  <summary>Details</summary>
Motivation: GraphQL灵活的查询模型和嵌套数据依赖暴露了API中复杂、上下文相关的漏洞，而现有的模糊测试工具要么依赖随机载荷生成，要么使用僵化的变异启发式方法，无法适应GraphQL模式和响应的动态结构。

Method: PrediQL将模糊测试策略选择建模为多臂老虎机问题，平衡新查询结构的探索和过去成功的利用。通过检索和重用执行轨迹、模式片段和先前错误，实现自我纠正和渐进学习。集成了上下文感知的漏洞检测器，使用LLM推理分析响应以识别注入漏洞、访问控制绕过和信息泄露等问题。

Result: 在开源和基准GraphQL API上的评估显示，PrediQL相比最先进的基线方法实现了显著更高的覆盖率和漏洞发现率。

Conclusion: 将检索增强推理与自适应模糊测试相结合，可以将API安全测试从反应性枚举转变为智能探索。

Abstract: GraphQL's flexible query model and nested data dependencies expose APIs to
complex, context-dependent vulnerabilities that are difficult to uncover using
conventional testing tools. Existing fuzzers either rely on random payload
generation or rigid mutation heuristics, failing to adapt to the dynamic
structures of GraphQL schemas and responses. We present PrediQL, the first
retrieval-augmented, LLM-guided fuzzer for GraphQL APIs. PrediQL combines large
language model reasoning with adaptive feedback loops to generate semantically
valid and diverse queries. It models the choice of fuzzing strategy as a
multi-armed bandit problem, balancing exploration of new query structures with
exploitation of past successes. To enhance efficiency, PrediQL retrieves and
reuses execution traces, schema fragments, and prior errors, enabling
self-correction and progressive learning across test iterations. Beyond input
generation, PrediQL integrates a context-aware vulnerability detector that uses
LLM reasoning to analyze responses, interpreting data values, error messages,
and status codes to identify issues such as injection flaws, access-control
bypasses, and information disclosure. Our evaluation across open-source and
benchmark GraphQL APIs shows that PrediQL achieves significantly higher
coverage and vulnerability discovery rates compared to state-of-the-art
baselines. These results demonstrate that combining retrieval-augmented
reasoning with adaptive fuzzing can transform API security testing from
reactive enumeration to intelligent exploration.

</details>


### [60] [A Scalable, Privacy-Preserving Decentralized Identity and Verifiable Data Sharing Framework based on Zero-Knowledge Proofs](https://arxiv.org/abs/2510.09715)
*Hui Yuan*

Main category: cs.CR

TL;DR: 提出一个结合DID、VC和零知识证明的隐私保护框架，通过zk-STARKs实现强隐私验证，使用密码累加器进行可扩展的凭证撤销，并集成社交密钥恢复机制。


<details>
  <summary>Details</summary>
Motivation: 解决区块链透明度与用户数据隐私之间的冲突，在实现可信身份验证和数据共享的同时保护隐私。

Method: 构建基于zk-STARKs的隐私保护协议，设计基于密码累加器的凭证撤销机制，集成社交密钥恢复方案。

Result: 相比基于zk-SNARKs的系统，在证明生成时间和验证开销方面表现更优，提供更强的安全保证（无可信设置和后量子安全）。

Conclusion: 该框架在DeFi信用评分场景中展现出巨大潜力，能够提升资本效率并促进可信数据经济的发展。

Abstract: With the proliferation of decentralized applications (DApps), the conflict
between the transparency of blockchain technology and user data privacy has
become increasingly prominent. While Decentralized Identity (DID) and
Verifiable Credentials (VCs) provide a standardized framework for user data
sovereignty, achieving trusted identity verification and data sharing without
compromising privacy remains a significant challenge. This paper proposes a
novel, comprehensive framework that integrates DIDs and VCs with efficient
Zero-Knowledge Proof (ZKP) schemes to address this core issue. The key
contributions of this framework are threefold: first, it constructs a set of
strong privacy-preserving protocols based on zk-STARKs, allowing users to prove
that their credentials satisfy specific conditions (e.g., "age is over 18")
without revealing any underlying sensitive data. Second, it designs a scalable,
privacy-preserving credential revocation mechanism based on cryptographic
accumulators, effectively solving credential management challenges in
large-scale scenarios. Finally, it integrates a practical social key recovery
scheme, significantly enhancing system usability and security. Through a
prototype implementation and performance evaluation, this paper quantitatively
analyzes the framework's performance in terms of proof generation time,
verification overhead, and on-chain costs. Compared to existing
state-of-the-art systems based on zk-SNARKs, our framework, at the cost of a
larger proof size, significantly improves prover efficiency for complex
computations and provides stronger security guarantees, including no trusted
setup and post-quantum security. Finally, a case study in the decentralized
finance (DeFi) credit scoring scenario demonstrates the framework's immense
potential for unlocking capital efficiency and fostering a trusted data
economy.

</details>


### [61] [Zk-SNARK Marketplace with Proof of Useful Work](https://arxiv.org/abs/2510.09729)
*Samuel Oleksak,Richard Gazdik,Martin Peresini,Ivan Homoliak*

Main category: cs.CR

TL;DR: 提出了一种新的有用工作量证明协议，通过计算zk-SNARK证明来保护区块链共识，同时创建去中心化的证明生成市场。


<details>
  <summary>Details</summary>
Motivation: 传统PoW共识消耗大量能源但无实际价值，现有PoUW方案存在安全漏洞，无法满足PoW的必要要求。

Method: 设计PoUW协议，将客户端外包的zk-SNARK证明计算作为共识副产品，同时利用这些证明来保护共识协议。

Result: 成功构建了首个在共识层运行的去中心化zk-SNARK证明生成市场，满足PoW所有必要属性。

Conclusion: 该协议不仅解决了PoW的能源浪费问题，还创造了实际价值，同时保持了区块链的安全性和完整性。

Abstract: Proof of Work (PoW) is widely regarded as the most secure permissionless
blockchain consensus protocol. However, its reliance on computationally
intensive yet externally useless puzzles results in excessive electric energy
wasting. To alleviate this, Proof of Useful Work (PoUW) has been explored as an
alternative to secure blockchain platforms while also producing real-world
value. Despite this promise, existing PoUW proposals often fail to embed the
integrity of the chain and identity of the miner into the puzzle solutions, not
meeting necessary requirements for PoW and thus rendering them vulnerable. In
this work, we propose a PoUW consensus protocol that computes client-outsourced
zk-SNARKs proofs as a byproduct, which are at the same time used to secure the
consensus protocol. We further leverage this mechanism to design a
decentralized marketplace for outsourcing zk-SNARK proof generation, which is,
to the best of our knowledge, the first such marketplace operating at the
consensus layer, while meeting all necessary properties of PoW.

</details>


### [62] [Secret-Key Agreement Through Hidden Markov Modeling of Wavelet Scattering Embeddings](https://arxiv.org/abs/2510.09773)
*Nora Basha,Bechir Hamdaoui,Attila A. Yavuz,Thang Hoang,Mehran Mozaffari Kermani*

Main category: cs.CR

TL;DR: 提出了一种基于小波散射网络的密钥生成方法，通过提取稳健的CSI特征来改进物联网安全，相比传统方法提升了5倍的密钥生成率。


<details>
  <summary>Details</summary>
Motivation: 现有基于无线信道互易性的密钥生成方法过于依赖瞬时信道测量样本的相似性，容易受到噪声、异步采样、信道衰落等系统缺陷的影响，且量化步骤会引入不可逆误差。

Method: 使用小波散射网络提取稳健且互易的CSI特征，应用降维技术发现隐藏的聚类结构，然后构建隐马尔可夫模型进行高效密钥协商。

Result: 该方法无需量化步骤，有效捕获信道随机性，与传统基准相比密钥生成率提高了5倍。

Conclusion: 为资源受限的物联网环境提供了一种安全高效的密钥生成解决方案。

Abstract: Secret-key generation and agreement based on wireless channel reciprocity
offers a promising avenue for securing IoT networks. However, existing
approaches predominantly rely on the similarity of instantaneous channel
measurement samples between communicating devices. This narrow view of
reciprocity is often impractical, as it is highly susceptible to noise,
asynchronous sampling, channel fading, and other system-level imperfections --
all of which significantly impair key generation performance. Furthermore, the
quantization step common in traditional schemes introduces irreversible errors,
further limiting efficiency. In this work, we propose a novel approach for
secret-key generation by using wavelet scattering networks to extract robust
and reciprocal CSI features. Dimensionality reduction is applied to uncover
hidden cluster structures, which are then used to build hidden Markov models
for efficient key agreement. Our approach eliminates the need for quantization
and effectively captures channel randomness. It achieves a 5x improvement in
key generation rate compared to traditional benchmarks, providing a secure and
efficient solution for key generation in resource-constrained IoT environments.

</details>


### [63] [HTTP Request Synchronization Defeats Discrepancy Attacks](https://arxiv.org/abs/2510.09952)
*Cem Topcuoglu,Kaan Onarlioglu,Steven Sprecher,Engin Kirda*

Main category: cs.CR

TL;DR: 提出HTTP请求同步方案，通过标准HTTP扩展机制为请求添加完整处理历史记录，使代理服务器能够验证处理一致性，消除差异攻击。


<details>
  <summary>Details</summary>
Motivation: 当代Web应用架构中多层代理服务处理流量时存在处理差异，攻击者可利用这些差异发动Web缓存投毒和请求走私等攻击，目前缺乏系统性防御措施。

Method: 使用标准HTTP扩展机制为每个请求添加完整处理历史记录，通过流量路径传播该上下文，使每个代理服务器能够验证其处理与之前所有跳数的一致性。

Result: 在5种流行代理技术（Apache、NGINX、HAProxy、Varnish和Cloudflare）上实现了该方案，证明了其实际影响。

Conclusion: HTTP请求同步是首个解决代理处理差异问题的全面防御方案，能够有效消除差异攻击。

Abstract: Contemporary web application architectures involve many layers of proxy
services that process traffic. Due to the complexity of HTTP and vendor design
decisions, these proxies sometimes process a given request in different ways.
Attackers can exploit these processing discrepancies to launch damaging attacks
including web cache poisoning and request smuggling. Discrepancy attacks are
surging, yet, there exists no systemic defense.
  In this work, we propose the first comprehensive defense to address this
problem, called HTTP Request Synchronization. Our scheme uses standard HTTP
extension mechanisms to augment each request with a complete processing
history. It propagates this context through the traffic path detailing how each
server hop has processed said request. Using this history, every proxy server
can validate that their processing is consistent with all previous hops,
eliminating discrepancy attacks. We implement our scheme for 5 popular proxy
technologies, Apache, NGINX, HAProxy, Varnish, and Cloudflare, demonstrating
its practical impact.

</details>


### [64] [Prismo: A Decision Support System for Privacy-Preserving ML Framework Selection](https://arxiv.org/abs/2510.09985)
*Nges Brian Njungle,Eric Jahns,Luigi Mastromauro,Edwin P. Kayang,Milan Stojkov,Michel A. Kinsy*

Main category: cs.CR

TL;DR: Prismo是一个开源推荐系统，帮助选择隐私保护机器学习(PPML)框架和参数，通过线性整数规划优化为用户场景推荐最佳解决方案。


<details>
  <summary>Details</summary>
Motivation: 机器学习应用中个人信息的隐私安全问题日益突出，现有PPML框架选择复杂，缺乏有效的参数优化指导工具。

Method: 将每个用例建模为线性整数规划优化问题，支持基于用户定义目标的多维度框架筛选，包括多方计算参与方数量、同态加密计算成本等参数。

Result: 通过多个用例验证，Prismo能够在不同部署场景中提供最佳匹配的解决方案。

Conclusion: Prismo为PPML应用开发者提供了有效的框架选择和参数优化工具，解决了实际部署中的复杂选择难题。

Abstract: Machine learning has become a crucial part of our lives, with applications
spanning nearly every aspect of our daily activities. However, using personal
information in machine learning applications has sparked significant security
and privacy concerns about user data. To address these challenges, different
privacy-preserving machine learning (PPML) frameworks have been developed to
protect sensitive information in machine learning applications. These
frameworks generally attempt to balance design trade-offs such as computational
efficiency, communication overhead, security guarantees, and scalability.
Despite the advancements, selecting the optimal framework and parameters for
specific deployment scenarios remains a complex and critical challenge for
privacy and security application developers.
  We present Prismo, an open-source recommendation system designed to aid in
selecting optimal parameters and frameworks for different PPML application
scenarios. Prismo enables users to explore a comprehensive space of PPML
frameworks through various properties based on user-defined objectives. It
supports automated filtering of suitable candidate frameworks by considering
parameters such as the number of parties in multi-party computation or
federated learning and computation cost constraints in homomorphic encryption.
Prismo models every use case into a Linear Integer Programming optimization
problem, ensuring tailored solutions are recommended for each scenario. We
evaluate Prismo's effectiveness through multiple use cases, demonstrating its
ability to deliver best-fit solutions in different deployment scenarios.

</details>


### [65] [SecureWebArena: A Holistic Security Evaluation Benchmark for LVLM-based Web Agents](https://arxiv.org/abs/2510.10073)
*Zonghao Ying,Yangguang Shao,Jianle Gan,Gan Xu,Junjie Shen,Wenxin Zhang,Quanchen Zou,Junzheng Shi,Zhenfei Yin,Mingchuan Zhang,Aishan Liu,Xianglong Liu*

Main category: cs.CR

TL;DR: 提出首个全面评估基于大视觉语言模型的Web代理安全性的基准测试工具，包含6个模拟Web环境、2970个高质量轨迹，定义了6种攻击向量，并通过三层评估协议分析代理故障。


<details>
  <summary>Details</summary>
Motivation: 现有安全评估基准覆盖范围有限，通常仅限于用户级提示操作等狭窄场景，无法捕捉代理的广泛漏洞，需要设计更全面的安全评估基准。

Method: 引入统一评估套件，包括6个模拟但真实的Web环境，定义6种攻击向量的结构化分类法，并提出三层评估协议分析内部推理、行为轨迹和任务结果。

Result: 对9个代表性LVLM进行大规模实验，结果显示所有测试代理都对细微的对抗性操作持续脆弱，并揭示了模型专业化与安全性之间的关键权衡。

Conclusion: 通过提供全面的基准套件和实证见解，为推进可信Web代理部署奠定了基础。

Abstract: Large vision-language model (LVLM)-based web agents are emerging as powerful
tools for automating complex online tasks. However, when deployed in real-world
environments, they face serious security risks, motivating the design of
security evaluation benchmarks. Existing benchmarks provide only partial
coverage, typically restricted to narrow scenarios such as user-level prompt
manipulation, and thus fail to capture the broad range of agent
vulnerabilities. To address this gap, we present \tool{}, the first holistic
benchmark for evaluating the security of LVLM-based web agents. \tool{} first
introduces a unified evaluation suite comprising six simulated but realistic
web environments (\eg, e-commerce platforms, community forums) and includes
2,970 high-quality trajectories spanning diverse tasks and attack settings. The
suite defines a structured taxonomy of six attack vectors spanning both
user-level and environment-level manipulations. In addition, we introduce a
multi-layered evaluation protocol that analyzes agent failures across three
critical dimensions: internal reasoning, behavioral trajectory, and task
outcome, facilitating a fine-grained risk analysis that goes far beyond simple
success metrics. Using this benchmark, we conduct large-scale experiments on 9
representative LVLMs, which fall into three categories: general-purpose,
agent-specialized, and GUI-grounded. Our results show that all tested agents
are consistently vulnerable to subtle adversarial manipulations and reveal
critical trade-offs between model specialization and security. By providing (1)
a comprehensive benchmark suite with diverse environments and a multi-layered
evaluation pipeline, and (2) empirical insights into the security challenges of
modern LVLM-based web agents, \tool{} establishes a foundation for advancing
trustworthy web agent deployment.

</details>


### [66] [Pharmacist: Safety Alignment Data Curation for Large Language Models against Harmful Fine-tuning](https://arxiv.org/abs/2510.10085)
*Guozhi Liu,Qi Mu,Tiansheng Huang,Xinhua Wang,Li Shen,Weiwei Lin,Zhang Li*

Main category: cs.CR

TL;DR: 提出Pharmacist方法，通过从原始对齐数据中筛选高质量安全关键核心子集，提升大语言模型对抗有害微调攻击的防御能力，同时提高计算效率。


<details>
  <summary>Details</summary>
Motivation: 现有防御方法（如Vaccine、Repnoise等）在缓解有害微调问题时，往往忽视了原始安全对齐数据质量的关键作用，导致防御性能和计算效率受限。

Method: 训练一个对齐数据选择器来对对齐数据进行排序，提升高质量安全关键数据的优先级，降低低质量非安全关键数据的优先级。

Result: 使用Pharmacist筛选的数据集训练的模型在防御和推理性能上均优于现有选择方法，与主流防御方法集成后，防御性能提升2.60%-3.30%，推理性能提升1.10%-3.50%，训练时间减少56.83%-57.63%。

Conclusion: Pharmacist通过优化安全对齐数据选择，有效提升了对有害微调攻击的防御能力，同时显著提高了计算效率，可与现有防御方法良好集成。

Abstract: Harmful fine-tuning issues present significant safety challenges for
fine-tuning-as-a-service in large language models. Existing alignment-stage
defenses, e.g., Vaccine, Repnoise, Booster, and T-Vaccine, mitigate harmful
fine-tuning issues by enhancing the model's robustness during the alignment
phase. While these methods have been proposed to mitigate the issue, they often
overlook a critical upstream factor: the role of the original safety-alignment
data. We observe that their defense performance and computational efficiency
remain constrained by the quality and composition of the alignment dataset. To
address this limitation, we propose Pharmacist, a safety alignment data
curation solution that enhances defense against harmful fine-tuning by
selecting a high-quality and safety-critical core subset from the original
alignment data. The core idea of Pharmacist is to train an alignment data
selector to rank alignment data. Specifically, up-ranking high-quality and
safety-critical alignment data, down-ranking low-quality and
non-safety-critical data. Empirical results indicate that models trained on
datasets selected by Pharmacist outperform those trained on datasets selected
by existing selection methods in both defense and inference performance. In
addition, Pharmacist can be effectively integrated with mainstream
alignment-stage defense methods. For example, when applied to RepNoise and
T-Vaccine, using the dataset selected by Pharmacist instead of the full dataset
leads to improvements in defense performance by 2.60\% and 3.30\%,
respectively, and enhances inference performance by 3.50\% and 1.10\%. Notably,
it reduces training time by 56.83\% and 57.63\%, respectively. Our code is
available at https://github.com/Lslland/Pharmacist.

</details>


### [67] [System Password Security: Attack and Defense Mechanisms](https://arxiv.org/abs/2510.10246)
*Chaofang Shi,Zhongwen Li,Xiaoqi Li*

Main category: cs.CR

TL;DR: 本文系统研究系统密码安全，分析暴力破解、字典攻击和彩虹表攻击等典型密码破解方法，评估现有防御措施有效性，并通过实验验证加盐和慢哈希等防御技术的效果。


<details>
  <summary>Details</summary>
Motivation: 近年来针对系统密码的频繁破解攻击对信息系统安全构成严重威胁，深入研究密码破解攻击方法和防御技术具有重要意义。

Method: 使用John the Ripper和Hashcat等密码分析工具模拟暴力破解和字典攻击，分析五个使用MD5、SHA-256和bcrypt哈希函数的测试数据集，比较不同哈希算法和密码复杂度策略的性能。

Result: 实验验证了加盐和慢哈希算法等防御措施的有效性，bcrypt等慢哈希算法相比MD5和SHA-256提供更强的安全性。

Conclusion: 通过整合实验数据和最新研究成果，分析了账户锁定策略、多因素认证和风险自适应认证等防御机制的优缺点，提出了可行的改进建议和优化策略。

Abstract: System passwords serve as critical credentials for user authentication and
access control when logging into operating systems or applications. Upon
entering a valid password, users pass verification to access system resources
and execute corresponding operations. In recent years, frequent password
cracking attacks targeting system passwords have posed a severe threat to
information system security. To address this challenge, in-depth research into
password cracking attack methods and defensive technologies holds significant
importance. This paper conducts systematic research on system password
security, focusing on analyzing typical password cracking methods such as brute
force attacks, dictionary attacks, and rainbow table attacks, while evaluating
the effectiveness of existing defensive measures. The experimental section
utilizes common cryptanalysis tools, such as John the Ripper and Hashcat, to
simulate brute force and dictionary attacks. Five test datasets, each generated
using Message Digest Algorithm 5 (MD5), Secure Hash Algorithm 256-bit (SHA
256), and bcrypt hash functions, are analyzed. By comparing the overall
performance of different hash algorithms and password complexity strategies
against these attacks, the effectiveness of defensive measures such as salting
and slow hashing algorithms is validated. Building upon this foundation, this
paper further evaluates widely adopted defense mechanisms, including account
lockout policies, multi-factor authentication, and risk adaptive
authentication. By integrating experimental data with recent research findings,
it analyzes the strengths and limitations of each approach while proposing
feasible improvement recommendations and optimization strategies.

</details>


### [68] [MetaBreak: Jailbreaking Online LLM Services via Special Token Manipulation](https://arxiv.org/abs/2510.10271)
*Wentian Zhu,Zhen Xiang,Wei Niu,Le Guan*

Main category: cs.CR

TL;DR: 本文提出了一种名为MetaBreak的攻击方法，利用LLM训练中的特殊令牌绕过安全对齐和内容审核系统，攻击成功率优于现有方法，且能与提示工程方法协同工作。


<details>
  <summary>Details</summary>
Motivation: 特殊令牌在LLM训练中被用作元数据来指导模型生成连贯响应，但研究发现这些令牌可被恶意利用来构造攻击原语，绕过LLM服务的安全防护。

Method: 通过利用特殊令牌构造四种攻击原语，即使将特殊令牌替换为语义相似的正则令牌也能规避防御机制。在实验室环境和商业LLM平台上系统性评估了MetaBreak方法。

Result: MetaBreak在无内容审核时与SOTA提示工程方法成功率相当，但在有内容审核时分别比PAP和GPTFuzzer高出11.6%和34.8%。与提示工程方法协同使用时，能进一步提升攻击成功率。

Conclusion: 特殊令牌的安全威胁难以有效防御，因为简单的输入清理措施容易被规避。MetaBreak展示了利用训练数据元数据的新型攻击向量，需要更全面的安全防护措施。

Abstract: Unlike regular tokens derived from existing text corpora, special tokens are
artificially created to annotate structured conversations during the
fine-tuning process of Large Language Models (LLMs). Serving as metadata of
training data, these tokens play a crucial role in instructing LLMs to generate
coherent and context-aware responses. We demonstrate that special tokens can be
exploited to construct four attack primitives, with which malicious users can
reliably bypass the internal safety alignment of online LLM services and
circumvent state-of-the-art (SOTA) external content moderation systems
simultaneously. Moreover, we found that addressing this threat is challenging,
as aggressive defense mechanisms-such as input sanitization by removing special
tokens entirely, as suggested in academia-are less effective than anticipated.
This is because such defense can be evaded when the special tokens are replaced
by regular ones with high semantic similarity within the tokenizer's embedding
space. We systemically evaluated our method, named MetaBreak, on both lab
environment and commercial LLM platforms. Our approach achieves jailbreak rates
comparable to SOTA prompt-engineering-based solutions when no content
moderation is deployed. However, when there is content moderation, MetaBreak
outperforms SOTA solutions PAP and GPTFuzzer by 11.6% and 34.8%, respectively.
Finally, since MetaBreak employs a fundamentally different strategy from prompt
engineering, the two approaches can work synergistically. Notably, empowering
MetaBreak on PAP and GPTFuzzer boosts jailbreak rates by 24.3% and 20.2%,
respectively.

</details>


### [69] [ArtPerception: ASCII Art-based Jailbreak on LLMs with Recognition Pre-test](https://arxiv.org/abs/2510.10281)
*Guan-Yan Yang,Tzu-Yu Cheng,Ya-Wen Teng,Farn Wanga,Kuo-Hui Yeh*

Main category: cs.CR

TL;DR: ArtPerception是一个利用ASCII艺术绕过LLM安全防护的黑盒越狱框架，通过两阶段方法实现高效攻击。


<details>
  <summary>Details</summary>
Motivation: 现有LLM安全对齐主要关注语义解释，无法防御使用非标准数据表示的攻击，存在安全漏洞。

Method: 采用两阶段方法：第一阶段进行一次性预测试确定ASCII艺术识别最优参数；第二阶段利用这些参数发起高效的一次性恶意越狱攻击。

Result: 在四个SOTA开源LLM上表现出优越的越狱性能，并能成功迁移到GPT-4o、Claude Sonnet 3.7和DeepSeek-V3等商业模型。

Conclusion: 真正的LLM安全需要防御多模态解释空间，即使是纯文本输入，基于侦察的策略攻击非常有效。

Abstract: The integration of Large Language Models (LLMs) into computer applications
has introduced transformative capabilities but also significant security
challenges. Existing safety alignments, which primarily focus on semantic
interpretation, leave LLMs vulnerable to attacks that use non-standard data
representations. This paper introduces ArtPerception, a novel black-box
jailbreak framework that strategically leverages ASCII art to bypass the
security measures of state-of-the-art (SOTA) LLMs. Unlike prior methods that
rely on iterative, brute-force attacks, ArtPerception introduces a systematic,
two-phase methodology. Phase 1 conducts a one-time, model-specific pre-test to
empirically determine the optimal parameters for ASCII art recognition. Phase 2
leverages these insights to launch a highly efficient, one-shot malicious
jailbreak attack. We propose a Modified Levenshtein Distance (MLD) metric for a
more nuanced evaluation of an LLM's recognition capability. Through
comprehensive experiments on four SOTA open-source LLMs, we demonstrate
superior jailbreak performance. We further validate our framework's real-world
relevance by showing its successful transferability to leading commercial
models, including GPT-4o, Claude Sonnet 3.7, and DeepSeek-V3, and by conducting
a rigorous effectiveness analysis against potential defenses such as LLaMA
Guard and Azure's content filters. Our findings underscore that true LLM
security requires defending against a multi-modal space of interpretations,
even within text-only inputs, and highlight the effectiveness of strategic,
reconnaissance-based attacks. Content Warning: This paper includes potentially
harmful and offensive model outputs.

</details>


### [70] [Post-Quantum Cryptography and Quantum-Safe Security: A Comprehensive Survey](https://arxiv.org/abs/2510.10436)
*Gaurab Chhetri,Shriyank Somvanshi,Pavan Hebli,Shamyo Brotee,Subasish Das*

Main category: cs.CR

TL;DR: 这篇论文是关于后量子密码学(PQC)的综合调研，涵盖了从理论基础到实际部署的各个方面，包括密码学家族分类、性能比较、硬件加速、协议集成和实际部署考虑。


<details>
  <summary>Details</summary>
Motivation: 随着NIST最终确定ML-KEM、ML-DSA和SLH-DSA标准，后量子密码学正从评估阶段转向部署阶段，需要为研究者和实践者提供从标准到工程再到运营的全面指导。

Method: 开发了涵盖格基、编码、哈希、多元、同源和MPC-in-the-Head等密码学家族的综合分类法，总结安全假设、密码分析和标准化状态，并进行性能比较和实现安全性评估。

Result: 提供了基于实现的性能测量、硬件加速分析、协议集成方案以及在不同环境中的部署指南，强调密码灵活性、混合迁移和基于证据的运营指导。

Conclusion: 后量子密码学部署面临参数灵活性、抗泄漏实现和领域特定部署方案等开放性问题，该调研旨在成为规划量子安全系统的实用参考，连接标准、工程和运营。

Abstract: Post-quantum cryptography (PQC) is moving from evaluation to deployment as
NIST finalizes standards for ML-KEM, ML-DSA, and SLH-DSA. This survey maps the
space from foundations to practice. We first develop a taxonomy across
lattice-, code-, hash-, multivariate-, isogeny-, and MPC-in-the-Head families,
summarizing security assumptions, cryptanalysis, and standardization status. We
then compare performance and communication costs using representative,
implementation-grounded measurements, and review hardware acceleration (AVX2,
FPGA/ASIC) and implementation security with a focus on side-channel resistance.
Building upward, we examine protocol integration (TLS, DNSSEC), PKI and
certificate hygiene, and deployment in constrained and high-assurance
environments (IoT, cloud, finance, blockchain). We also discuss complementarity
with quantum technologies (QKD, QRNGs) and the limits of near-term quantum
computing. Throughout, we emphasize crypto-agility, hybrid migration, and
evidence-based guidance for operators. We conclude with open problems spanning
parameter agility, leakage-resilient implementations, and domain-specific
rollout playbooks. This survey aims to be a practical reference for researchers
and practitioners planning quantum-safe systems, bridging standards,
engineering, and operations.

</details>


### [71] [SASER: Stego attacks on open-source LLMs](https://arxiv.org/abs/2510.10486)
*Ming Tan,Wei Li,Hu Tao,Hailong Ma,Aodi Liu,Qian Chen,Zilong Wang*

Main category: cs.CR

TL;DR: 本文提出了首个针对开源大语言模型的隐写攻击SASER，通过参数识别、载荷嵌入、触发器注入和载荷执行四个步骤，在保持模型性能的同时实现高隐蔽性和对量化部署的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 开源LLMs虽然具有透明性优势，但其完全访问特性使其容易受到隐写攻击，这种攻击的危害尚未被充分理解。

Method: 提出SASER攻击方法，包括：1）使用性能感知重要性指标识别目标参数；2）嵌入载荷；3）注入触发器；4）执行载荷；5）通过反量化技术增强对量化部署的鲁棒性。

Result: 在LlaMA2-7B和ChatGLM3-6B上的实验显示，SASER的隐蔽率比现有DNN隐写攻击高98.1%，攻击成功率100%，且在量化模型上攻击成功率从0提升到100%。

Conclusion: SASER展示了开源LLMs面临的严重安全威胁，呼吁研究相应的防御措施。

Abstract: Open-source large language models (LLMs) have demonstrated considerable
dominance over proprietary LLMs in resolving neural processing tasks, thanks to
the collaborative and sharing nature. Although full access to source codes,
model parameters, and training data lays the groundwork for transparency, we
argue that such a full-access manner is vulnerable to stego attacks, and their
ill-effects are not fully understood. In this paper, we conduct a systematic
formalization for stego attacks on open-source LLMs by enumerating all possible
threat models associated with adversary objectives, knowledge, and
capabilities. Therein, the threat posed by adversaries with internal knowledge,
who inject payloads and triggers during the model sharing phase, is of
practical interest. We go even further and propose the first stego attack on
open-source LLMs, dubbed SASER, which wields impacts through identifying
targeted parameters, embedding payloads, injecting triggers, and executing
payloads sequentially. Particularly, SASER enhances the attack robustness
against quantization-based local deployment by de-quantizing the embedded
payloads. In addition, to achieve stealthiness, SASER devises the
performance-aware importance metric to identify targeted parameters with the
least degradation of model performance. Extensive experiments on LlaMA2-7B and
ChatGLM3-6B, without quantization, show that the stealth rate of SASER
outperforms existing stego attacks (for general DNNs) by up to 98.1%, while
achieving the same attack success rate (ASR) of 100%. More importantly, SASER
improves ASR on quantized models from 0 to 100% in all settings. We appeal for
investigations on countermeasures against SASER in view of the significant
attack effectiveness.

</details>


### [72] [The Hidden DNA of LLM-Generated JavaScript: Structural Patterns Enable High-Accuracy Authorship Attribution](https://arxiv.org/abs/2510.10493)
*Norbert Tihanyi,Bilel Cherif,Richard A. Dubniczky,Mohamed Amine Ferrag,Tamás Bisztray*

Main category: cs.CR

TL;DR: 该论文首次大规模研究证明不同大语言模型生成的JavaScript代码具有独特的风格特征，可实现可靠的作品归属和模型指纹识别。作者构建了包含25万个JavaScript样本的LLM-NodeJS数据集，并开发了CodeT5-JSA模型，在20类模型归属任务中达到88.5%的准确率。


<details>
  <summary>Details</summary>
Motivation: 随着AI生成代码的快速发展，作品归属在检测漏洞、标记恶意内容和确保问责方面变得至关重要。现有研究通常将AI视为单一类别，而本研究旨在证明不同LLM会留下独特的风格签名。

Method: 构建LLM-NodeJS数据集（包含20个LLM生成的5万个Node.js程序及其变体），使用传统机器学习分类器和微调Transformer编码器进行基准测试，并开发了基于CodeT5的CodeT5-JSA自定义架构。

Result: CodeT5-JSA在五类归属任务中达到95.8%准确率，十类94.6%，二十类88.5%，优于BERT、CodeBERT和Longformer等模型。分类器捕获的是程序数据流和结构的深层风格规律，而非表面特征。

Conclusion: 不同LLM生成的代码具有可识别的独特风格特征，即使在代码混淆、注释删除和重度转换后，归属识别仍然有效。作者开源了数据集和所有相关材料以支持开放科学。

Abstract: In this paper, we present the first large-scale study exploring whether
JavaScript code generated by Large Language Models (LLMs) can reveal which
model produced it, enabling reliable authorship attribution and model
fingerprinting. With the rapid rise of AI-generated code, attribution is
playing a critical role in detecting vulnerabilities, flagging malicious
content, and ensuring accountability. While AI-vs-human detection usually
treats AI as a single category we show that individual LLMs leave unique
stylistic signatures, even among models belonging to the same family or
parameter size. To this end, we introduce LLM-NodeJS, a dataset of 50,000
Node.js back-end programs from 20 large language models. Each has four
transformed variants, yielding 250,000 unique JavaScript samples and two
additional representations (JSIR and AST) for diverse research applications.
Using this dataset, we benchmark traditional machine learning classifiers
against fine-tuned Transformer encoders and introduce CodeT5-JSA, a custom
architecture derived from the 770M-parameter CodeT5 model with its decoder
removed and a modified classification head. It achieves 95.8% accuracy on
five-class attribution, 94.6% on ten-class, and 88.5% on twenty-class tasks,
surpassing other tested models such as BERT, CodeBERT, and Longformer. We
demonstrate that classifiers capture deeper stylistic regularities in program
dataflow and structure, rather than relying on surface-level features. As a
result, attribution remains effective even after mangling, comment removal, and
heavy code transformations. To support open science and reproducibility, we
release the LLM-NodeJS dataset, Google Colab training scripts, and all related
materials on GitHub: https://github.com/LLM-NodeJS-dataset.

</details>


### [73] [Predicting Module-Lattice Reduction](https://arxiv.org/abs/2510.10540)
*Léo Ducas,Lynn Engelberts,Paola de Perthuis*

Main category: cs.CR

TL;DR: 该论文分析了模块格约简与无结构格约简的效率差异，发现在幂二循环域中模块BKZ需要更大的块尺寸，而在其他循环域中模块BKZ能提供次线性增益。


<details>
  <summary>Details</summary>
Motivation: 解决Kyber标准化提交中提出的问题（Q8），即模块格约简是否比无结构格约简更好，这对基于模块格的密码方案（如Kyber）的具体安全性有重要影响。

Method: 对模块BKZ进行具体平均案例分析，确定数域判别式ΔK是驱动约简斜率的主要因素，并开发了首个开源模块BKZ实现。

Result: 在幂二循环域中，模块BKZ需要比无结构BKZ大d-1+o(1)的块尺寸；在其他循环域中，模块BKZ能提供Θ(β/log β)的次线性增益和次指数级加速。

Conclusion: 模块格约简的效率取决于数域类型：在幂二循环域中效率较低，但在其他循环域中能提供显著优势。

Abstract: Is module-lattice reduction better than unstructured lattice reduction? This
question was highlighted as 'Q8' in the Kyber NIST standardization submission
(Avanzi et al., 2021), as potentially affecting the concrete security of Kyber
and other module-lattice-based schemes. Foundational works on module-lattice
reduction (Lee, Pellet-Mary, Stehl\'e, and Wallet, ASIACRYPT 2019; Mukherjee
and Stephens-Davidowitz, CRYPTO 2020) confirmed the existence of such module
variants of LLL and block-reduction algorithms, but focus only on provable
worst-case asymptotic behavior.
  In this work, we present a concrete average-case analysis of module-lattice
reduction. Specifically, we address the question of the expected slope after
running module-BKZ, and pinpoint the discriminant $\Delta_K$ of the number
field at hand as the main quantity driving this slope. We convert this back
into a gain or loss on the blocksize $\beta$: module-BKZ in a number field $K$
of degree $d$ requires an SVP oracle of dimension $\beta + \log(|\Delta_K| /
d^d)\beta /(d\log \beta) + o(\beta / \log \beta)$ to reach the same slope as
unstructured BKZ with blocksize $\beta$. This asymptotic summary hides further
terms that we predict concretely using experimentally verified heuristics.
Incidentally, we provide the first open-source implementation of module-BKZ for
some cyclotomic fields.
  For power-of-two cyclotomic fields, we have $|\Delta_K| = d^d$, and conclude
that module-BKZ requires a blocksize larger than its unstructured counterpart
by $d-1+o(1)$. On the contrary, for all other cyclotomic fields we have
$|\Delta_K| < d^d$, so module-BKZ provides a sublinear $\Theta(\beta/\log
\beta)$ gain on the required blocksize, yielding a subexponential speedup of
$\exp(\Theta(\beta/\log \beta))$.

</details>


### [74] [Man-in-the-Middle Proof-of-Concept via Krontiris' Ephemeral Diffie-Hellman Over COSE (EDHOC) in C](https://arxiv.org/abs/2510.10574)
*Daniel Hennig,Joaquin Garcia-Alfaro*

Main category: cs.CR

TL;DR: 该报告分析了轻量级密钥交换协议的身份验证过程，重点关注中间人攻击如何破坏其安全性，特别是在合法拦截背景下可能助长大规模监控的风险。


<details>
  <summary>Details</summary>
Motivation: 研究轻量级密钥交换协议在身份验证过程中的安全漏洞，特别关注中间人攻击在合法拦截场景下可能导致的隐私和监控风险。

Method: 通过技术分析探讨中间人攻击对协议安全性的影响，主要关注攻击场景的技术方面。

Result: 识别了轻量级密钥交换协议在身份验证过程中存在的安全漏洞，这些漏洞可能被中间人攻击利用，在合法拦截背景下助长大规模监控。

Conclusion: 轻量级密钥交换协议的身份验证过程存在安全风险，需要加强防护措施以防止中间人攻击，特别是在合法拦截可能被滥用于大规模监控的背景下。

Abstract: This report presents some technical details on the authentication process of
a lightweight key exchange protocol, paying attention on how Man-in-the-Middle
(MitM) attacks could undermine its security, e.g., under the scope of lawful
interception and its risk to facilitate mass surveillance. We focus only on
some technical aspects associated to the attack scenario. Perspectives for
future work are also discussed. Other specific aspects of the work, mainly
focusing on the security implications of malicious metasurfaces against B5G
networks, are excluded from the scope of this report.

</details>


### [75] [Toxic Ink on Immutable Paper: Content Moderation for Ethereum Input Data Messages (IDMs)](https://arxiv.org/abs/2510.10761)
*Xihan Xiong,Zhipeng Wang,Qin Wang,William Knottenbelt*

Main category: cs.CR

TL;DR: 提出了两种以太坊输入数据消息（IDM）的内容审核框架：BUILDERMOD（构建者在区块构建时进行语义检查）和USERMOD（用户主动获取外部分类器的审核证明并嵌入交易中），评估发现USERMOD具有更低的延迟和更好的可扩展性。


<details>
  <summary>Details</summary>
Motivation: 随着以太坊上输入数据消息（IDM）的广泛使用，链上出现越来越多的有害内容，而以太坊在协议层面缺乏内容审核机制，这引发了担忧。

Method: 设计了两种审核框架：BUILDERMOD（构建者端审核）和USERMOD（用户端审核），并通过评估比较了两种方法的性能。

Result: BUILDERMOD会产生较高的区块时间开销，限制了其实用性；而USERMOD能够实现较低延迟的验证，并且扩展性更好。

Conclusion: USERMOD是在支持审核的以太坊环境中更实用的方法，为去中心化系统中的协议级内容治理奠定了基础。

Abstract: Decentralized communication is becoming an important use case within Web3. On
Ethereum, users can repurpose the transaction input data field to embed
natural-language messages, commonly known as Input Data Messages (IDMs).
However, as IDMs gain wider adoption, there has been a growing volume of toxic
content on-chain. This trend is concerning, as Ethereum provides no
protocol-level support for content moderation.
  We propose two moderation frameworks for Ethereum IDMs: (i) BUILDERMOD, where
builders perform semantic checks during block construction; and (ii) USERMOD,
where users proactively obtain moderation proofs from external classifiers and
embed them in transactions. Our evaluation reveals that BUILDERMOD incurs high
block-time overhead, which limits its practicality. In contrast, USERMOD
enables lower-latency validation and scales more effectively, making it a more
practical approach in moderation-aware Ethereum environments.
  Our study lays the groundwork for protocol-level content governance in
decentralized systems, and we hope it contributes to the development of a
decentralized communication environment that is safe, trustworthy, and socially
responsible.

</details>


### [76] [GPS Spoofing Attack Detection in Autonomous Vehicles Using Adaptive DBSCAN](https://arxiv.org/abs/2510.10766)
*Ahmad Mohammadi,Reza Ahmari,Vahid Hemmati,Frederick Owusu-Ambrose,Mahmoud Nabil Mahmoud,Parham Kebria,Abdollah Homaifar,Mehrdad Saif*

Main category: cs.CR

TL;DR: 提出一种基于动态调谐DBSCAN算法的自适应GPS欺骗检测方法，通过实时调整检测阈值来识别各种GPS欺骗攻击，在真实数据集上达到98-99%的检测准确率。


<details>
  <summary>Details</summary>
Motivation: 随着自动驾驶车辆在现代交通中日益重要，它们面临GPS欺骗攻击等安全威胁，需要有效的检测方法来保障车辆安全。

Method: 使用动态调谐的DBSCAN算法，基于GPS与车载传感器数据的位移误差的递归均值和标准差实时调整检测阈值，仅在非异常实例时更新阈值，并使用12万清洁数据样本确定初始阈值。

Result: 在真实世界Honda驾驶数据集上测试，能有效检测转向、停止、超调和小偏差等多种GPS欺骗攻击，检测准确率分别为98.621%、99.960.1%、99.880.1%和98.380.1%。

Conclusion: 该方法显著提升了自动驾驶车辆对抗GPS欺骗威胁的安全性和安全性。

Abstract: As autonomous vehicles become an essential component of modern
transportation, they are increasingly vulnerable to threats such as GPS
spoofing attacks. This study presents an adaptive detection approach utilizing
a dynamically tuned Density Based Spatial Clustering of Applications with Noise
(DBSCAN) algorithm, designed to adjust the detection threshold ({\epsilon}) in
real-time. The threshold is updated based on the recursive mean and standard
deviation of displacement errors between GPS and in-vehicle sensors data, but
only at instances classified as non-anomalous. Furthermore, an initial
threshold, determined from 120,000 clean data samples, ensures the capability
to identify even subtle and gradual GPS spoofing attempts from the beginning.
To assess the performance of the proposed method, five different subsets from
the real-world Honda Research Institute Driving Dataset (HDD) are selected to
simulate both large and small magnitude GPS spoofing attacks. The modified
algorithm effectively identifies turn-by-turn, stop, overshoot, and multiple
small biased spoofing attacks, achieving detection accuracies of 98.621%,
99.960.1%, 99.880.1%, and 98.380.1%, respectively. This work provides a
substantial advancement in enhancing the security and safety of AVs against GPS
spoofing threats.

</details>


### [77] [A Symmetric-Key Cryptosystem Based on the Burnside Ring of a Compact Lie Group](https://arxiv.org/abs/2510.10901)
*Ziad Ghanem*

Main category: cs.CR

TL;DR: 提出了一种基于紧李群Burnside环的对称密钥密码系统，使用O(2)群作为示例，通过Burnside积进行加密，避免了传统线性密码的已知明文攻击漏洞。


<details>
  <summary>Details</summary>
Motivation: 传统线性密码（如Hill密码）在固定有限维模上操作，容易受到已知明文攻击。为了克服这一弱点，需要开发在更复杂代数结构上操作的密码系统。

Method: 使用紧李群G的Burnside环A(G)作为加密空间，秘密密钥包括群G、子群轨道基的秘密全序和不可约表示索引集。消息编码为A(G)中的有限支撑元素，通过Burnside积与密钥k进行加密。

Result: 对于G=O(2)的情况，证明加密能保持明文的生成元支撑，避免密文扩展和安全泄漏。在被动模型中，有限观测只能约束有限秩子模上的操作，且密钥具有信息论不可识别性。

Conclusion: 该方案虽然避免了传统线性密码的弱点，但被证明不具备IND-CPA安全性，因为存在基于二面体探测的单查询选择明文区分器。

Abstract: Classical linear ciphers, such as the Hill cipher, operate on fixed,
finite-dimensional modules and are therefore vulnerable to straightforward
known-plaintext attacks that recover the key as a fully determined linear
operator. We propose a symmetric-key cryptosystem whose linear action takes
place instead in the Burnside ring $A(G)$ of a compact Lie group $G$, with
emphasis on the case $G=O(2)$. The secret key consists of (i) a compact Lie
group $G$; (ii) a secret total ordering of the subgroup orbit-basis of $A(G)$;
and (iii) a finite set $S$ of indices of irreducible $G$-representations, whose
associated basic degrees define an involutory multiplier $k\in A(G)$. Messages
of arbitrary finite length are encoded as finitely supported elements of $A(G)$
and encrypted via the Burnside product with $k$. For $G=O(2)$ we prove that
encryption preserves plaintext support among the generators
$\{(D_1),\dots,(D_L),(SO(2)),(O(2))\}$, avoiding ciphertext expansion and
security leakage. We then analyze security in passive models, showing that any
finite set of observations constrains the action only on a finite-rank
submodule $W_L\subset A(O(2))$, and we show information-theoretic
non-identifiability of the key from such data. Finally, we prove the scheme is
\emph{not} IND-CPA secure, by presenting a one-query chosen-plaintext
distinguisher based on dihedral probes.

</details>


### [78] [TabVLA: Targeted Backdoor Attacks on Vision-Language-Action Models](https://arxiv.org/abs/2510.10932)
*Zonghuan Xu,Xiang Zheng,Xingjun Ma,Yu-Gang Jiang*

Main category: cs.CR

TL;DR: TabVLA是一个针对视觉-语言-动作(VLA)模型的有针对性后门攻击框架，通过黑盒微调实现，在推理时通过输入流编辑和场景内触发两种方式部署攻击。


<details>
  <summary>Details</summary>
Motivation: 随着VLA模型在具身AI系统中的广泛应用，其后门攻击漏洞带来了严重的安全威胁。现有的后门攻击研究仅关注无目标攻击，而更具实际威胁的有针对性操纵场景尚未被充分研究。

Method: TabVLA采用黑盒微调方法，将毒化数据生成建模为优化问题，探索了输入流编辑和场景内触发两种推理时威胁模型。

Result: 在OpenVLA-7B模型和LIBERO基准测试中，视觉通道是主要攻击面：有针对性后门攻击在最小毒化下成功，对触发器设计变化具有鲁棒性，仅在微调与推理触发器位置不匹配时效果下降。

Conclusion: VLA模型对有针对性后门操纵存在严重脆弱性，需要开发更先进的防御机制来应对此类安全威胁。

Abstract: With the growing deployment of Vision-Language-Action (VLA) models in
real-world embodied AI systems, their increasing vulnerability to backdoor
attacks poses a serious safety threat. A backdoored VLA agent can be covertly
triggered by a pre-injected backdoor to execute adversarial actions,
potentially causing system failures or even physical harm. Although backdoor
attacks on VLA models have been explored, prior work has focused only on
untargeted attacks, leaving the more practically threatening scenario of
targeted manipulation unexamined. In this paper, we study targeted backdoor
attacks on VLA models and introduce TabVLA, a novel framework that enables such
attacks via black-box fine-tuning. TabVLA explores two deployment-relevant
inference-time threat models: input-stream editing and in-scene triggering. It
formulates poisoned data generation as an optimization problem to improve
attack effectivess. Experiments with OpenVLA-7B on the LIBERO benchmark reveal
that the vision channel is the principal attack surface: targeted backdoors
succeed with minimal poisoning, remain robust across variations in trigger
design, and are degraded only by positional mismatches between fine-tuning and
inference triggers. We also investigate a potential detection-based defense
against TabVLA, which reconstructs latent visual triggers from the input stream
to flag activation-conditioned backdoor samples. Our work highlights the
vulnerability of VLA models to targeted backdoor manipulation and underscores
the need for more advanced defenses.

</details>


### [79] [DITTO: A Spoofing Attack Framework on Watermarked LLMs via Knowledge Distillation](https://arxiv.org/abs/2510.10987)
*Hyeseon Ahn,Shinwoo Park,Yo-Sub Han*

Main category: cs.CR

TL;DR: 本文揭示了LLM水印技术存在严重安全漏洞，恶意模型可以通过水印伪造攻击生成包含可信模型水印的文本，导致有害内容被错误归因于可信来源。


<details>
  <summary>Details</summary>
Motivation: LLM水印技术的核心假设是特定水印能证明特定模型的作者身份，但这一假设存在严重缺陷，需要揭示其安全漏洞。

Method: 通过利用水印放射性（在微调过程中数据模式的无意继承），从水印教师模型中蒸馏知识，使攻击者能够窃取和复制受害者模型的水印信号。

Result: 成功实现了水印伪造攻击，恶意模型能够生成包含可信模型水印的文本，这暴露了文本作者身份验证的关键安全漏洞。

Conclusion: 这项工作揭示了文本作者身份验证的关键安全漏洞，呼吁向能够区分真实水印与专业模仿水印的技术进行范式转变。

Abstract: The promise of LLM watermarking rests on a core assumption that a specific
watermark proves authorship by a specific model. We demonstrate that this
assumption is dangerously flawed. We introduce the threat of watermark
spoofing, a sophisticated attack that allows a malicious model to generate text
containing the authentic-looking watermark of a trusted, victim model. This
enables the seamless misattribution of harmful content, such as disinformation,
to reputable sources. The key to our attack is repurposing watermark
radioactivity, the unintended inheritance of data patterns during fine-tuning,
from a discoverable trait into an attack vector. By distilling knowledge from a
watermarked teacher model, our framework allows an attacker to steal and
replicate the watermarking signal of the victim model. This work reveals a
critical security gap in text authorship verification and calls for a paradigm
shift towards technologies capable of distinguishing authentic watermarks from
expertly imitated ones. Our code is available at
https://github.com/hsannn/ditto.git.

</details>


### [80] [Secret-Protected Evolution for Differentially Private Synthetic Text Generation](https://arxiv.org/abs/2510.10990)
*Tianze Wang,Zhaoyu Chen,Jian Du,Yingtai Xiao,Linjun Zhang,Qiang Yan*

Main category: cs.CR

TL;DR: 提出了SecPE框架，通过秘密感知保护扩展私有进化，实现更优的效用-隐私权衡，降低计算复杂度，在多个基准测试中表现优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 现实世界中大量高质量文本因隐私问题无法自由使用，现有差分隐私合成文本生成方法采用统一保护机制，过度保护非敏感内容，导致效用损失和计算开销。

Method: SecPE框架，将私有进化与秘密感知保护相结合，满足(p,r)-秘密保护，是高斯差分隐私的松弛形式。

Result: 在OpenReview、PubMed和Yelp基准测试中，SecPE始终获得更低的Fréchet Inception Distance和更高的下游任务准确率，同时需要更少的噪声达到相同保护水平。

Conclusion: 秘密感知保护机制能够实现更实用和有效的隐私保护合成文本生成。

Abstract: Text data has become extremely valuable on large language models (LLMs) and
even lead to general artificial intelligence (AGI). A lot of high-quality text
in the real world is private and cannot be freely used due to privacy concerns.
Therefore, differentially private (DP) synthetic text generation has been
proposed, aiming to produce high-utility synthetic data while protecting
sensitive information. However, existing DP synthetic text generation imposes
uniform guarantees that often overprotect non-sensitive content, resulting in
substantial utility loss and computational overhead. Therefore, we propose
Secret-Protected Evolution (SecPE), a novel framework that extends private
evolution with secret-aware protection. Theoretically, we show that SecPE
satisfies $(\mathrm{p}, \mathrm{r})$-secret protection, constituting a
relaxation of Gaussian DP that enables tighter utility-privacy trade-offs,
while also substantially reducing computational complexity relative to baseline
methods. Empirically, across the OpenReview, PubMed, and Yelp benchmarks, SecPE
consistently achieves lower Fr\'echet Inception Distance (FID) and higher
downstream task accuracy than GDP-based Aug-PE baselines, while requiring less
noise to attain the same level of protection. Our results highlight that
secret-aware guarantees can unlock more practical and effective
privacy-preserving synthetic text generation.

</details>


### [81] [Stabilizing the Staking Rate, Dynamically Distributed Inflation and Delay Induced Oscillations](https://arxiv.org/abs/2510.11065)
*Carlo Brunetta,Amit Chaudhary,Stefano Galatolo,Massimiliano Sala*

Main category: cs.CR

TL;DR: 该论文研究了基于通胀奖励系统的动态不稳定性问题，并提出了一种新的分配模型来稳定质押率。


<details>
  <summary>Details</summary>
Motivation: 动态分布式通胀机制在引导区块链质押率向期望均衡发展时，由于年化收益率对质押率变化的高度敏感性以及质押者响应的固有反馈延迟，会导致在均衡点周围出现不良振荡。

Method: 分析了基于通胀的奖励系统动态特性，并设计了一种新颖的分配模型来稳定质押率。

Result: 提出的解决方案有效抑制了振荡，使收益率在目标质押范围内保持稳定。

Conclusion: 通过新的分配模型可以有效解决通胀奖励系统中的不稳定性问题，实现质押率的稳定控制。

Abstract: Dynamically distributed inflation is a common mechanism used to guide a
blockchain's staking rate towards a desired equilibrium between network
security and token liquidity.
  However, the high sensitivity of the annual percentage yield to changes in
the staking rate, coupled with the inherent feedback delays in staker
responses, can induce undesirable oscillations around this equilibrium.
  This paper investigates this instability phenomenon. We analyze the dynamics
of inflation-based reward systems and propose a novel distribution model
designed to stabilize the staking rate. Our solution effectively dampens
oscillations, stabilizing the yield within a target staking range.

</details>


### [82] [N-output Mechanism: Estimating Statistical Information from Numerical Data under Local Differential Privacy](https://arxiv.org/abs/2510.11116)
*Incheol Baek,Yon Dohn Chung*

Main category: cs.CR

TL;DR: 提出了N-output机制，一种用于本地差分隐私下数值数据收集的通用框架，可映射到任意N个离散输出，填补了现有机制仅适用于极小或无限输出空间的空白。


<details>
  <summary>Details</summary>
Motivation: 现有LDP机制只针对极小输出空间（|Ω|∈{2,3}）或无限输出空间进行优化，缺乏适用于任意输出大小N的通用最优机制构建方法。

Method: 将机制设计建模为优化问题以最小化估计方差，针对任意N≥2开发数值和解析解，并扩展至分布估计任务。

Result: 经验评估表明，N-output机制在均值、方差和分布估计方面达到最先进精度，且通信成本小。

Conclusion: 该机制提供了一个高度准确且自适应的通用框架，能够为任意选择的N值通过求解优化问题来确定最优设计。

Abstract: Local Differential Privacy (LDP) addresses significant privacy concerns in
sensitive data collection. In this work, we focus on numerical data collection
under LDP, targeting a significant gap in the literature: existing LDP
mechanisms are optimized for either a very small ($|\Omega| \in \{2, 3\}$) or
infinite output spaces. However, no generalized method for constructing an
optimal mechanism for an arbitrary output size $N$ exists. To fill this gap, we
propose the \textbf{N-output mechanism}, a generalized framework that maps
numerical data to one of $N$ discrete outputs.
  We formulate the mechanism's design as an optimization problem to minimize
estimation variance for any given $N \geq 2$ and develop both numerical and
analytical solutions. This results in a mechanism that is highly accurate and
adaptive, as its design is determined by solving an optimization problem for
any chosen $N$. Furthermore, we extend our framework and existing mechanisms to
the task of distribution estimation. Empirical evaluations show that the
N-output mechanism achieves state-of-the-art accuracy for mean, variance, and
distribution estimation with small communication costs.

</details>


### [83] [CoSPED: Consistent Soft Prompt Targeted Data Extraction and Defense](https://arxiv.org/abs/2510.11137)
*Yang Zhuochen,Fok Kar Wai,Thing Vrizlynn*

Main category: cs.CR

TL;DR: CoSPED是一种针对LLM数据提取攻击和防御的方法，通过动态损失、加性损失、共同损失和自一致性解码策略提高软提示调优的一致性，在50个token前缀比较下达到65.2%的提取率，并通过模型编辑将提取率降至1.6%。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型存在安全漏洞，特别是隐私泄露风险，需要测试和评估LLM中的数据提取风险。

Method: 提出CoSPED方法，包含动态损失、加性损失、共同损失和自一致性解码策略，增强软提示调优过程的一致性。

Result: 在50个token前缀比较下达到65.2%的提取率，Pythia模型提取率为51.7%，通过Rank-One模型编辑可将提取率降至1.6%。

Conclusion: 对提取机制的分析可以直接指导针对软提示攻击的有效缓解策略，证明了防御方法的有效性。

Abstract: Large language models have gained widespread attention recently, but their
potential security vulnerabilities, especially privacy leakage, are also
becoming apparent. To test and evaluate for data extraction risks in LLM, we
proposed CoSPED, short for Consistent Soft Prompt targeted data Extraction and
Defense. We introduce several innovative components, including Dynamic Loss,
Additive Loss, Common Loss, and Self Consistency Decoding Strategy, and tested
to enhance the consistency of the soft prompt tuning process. Through extensive
experimentation with various combinations, we achieved an extraction rate of
65.2% at a 50-token prefix comparison. Our comparisons of CoSPED with other
reference works confirm our superior extraction rates. We evaluate CoSPED on
more scenarios, achieving Pythia model extraction rate of 51.7% and introducing
cross-model comparison. Finally, we explore defense through Rank-One Model
Editing and achieve a reduction in the extraction rate to 1.6%, which proves
that our analysis of extraction mechanisms can directly inform effective
mitigation strategies against soft prompt-based attacks.

</details>


### [84] [RAG-Pull: Imperceptible Attacks on RAG Systems for Code Generation](https://arxiv.org/abs/2510.11195)
*Vasilije Stambolic,Aritra Dhar,Lukas Cavigelli*

Main category: cs.CR

TL;DR: RAG-Pull是一种新型黑盒攻击，通过在查询或外部代码库中插入隐藏UTF字符，将检索重定向到恶意代码，从而破坏模型的安全对齐。


<details>
  <summary>Details</summary>
Motivation: 研究RAG系统在增强LLM可靠性的同时，可能面临的新型安全威胁，揭示通过最小扰动即可破坏模型安全对齐的攻击方式。

Method: 开发RAG-Pull攻击方法，通过在查询或代码库中插入隐藏UTF字符来操纵检索过程，使系统检索恶意代码片段。

Result: 单独查询或代码扰动即可将检索转向攻击者控制的代码片段，而查询和目标联合扰动几乎达到完美成功率，引入可被利用的安全漏洞。

Conclusion: RAG-Pull攻击揭示了LLM安全的新威胁类别，最小扰动即可改变模型安全对齐并增加对不安全代码的偏好。

Abstract: Retrieval-Augmented Generation (RAG) increases the reliability and
trustworthiness of the LLM response and reduces hallucination by eliminating
the need for model retraining. It does so by adding external data into the
LLM's context. We develop a new class of black-box attack, RAG-Pull, that
inserts hidden UTF characters into queries or external code repositories,
redirecting retrieval toward malicious code, thereby breaking the models'
safety alignment. We observe that query and code perturbations alone can shift
retrieval toward attacker-controlled snippets, while combined query-and-target
perturbations achieve near-perfect success. Once retrieved, these snippets
introduce exploitable vulnerabilities such as remote code execution and SQL
injection. RAG-Pull's minimal perturbations can alter the model's safety
alignment and increase preference towards unsafe code, therefore opening up a
new class of attacks on LLMs.

</details>


### [85] [TraceAegis: Securing LLM-Based Agents via Hierarchical and Behavioral Anomaly Detection](https://arxiv.org/abs/2510.11203)
*Jiahao Liu,Bonan Ruan,Xianglin Yang,Zhiwei Lin,Yan Liu,Yang Wang,Tao Wei,Zhenkai Liang*

Main category: cs.CR

TL;DR: TraceAegis是一个基于溯源的分析框架，利用智能体执行轨迹来检测潜在异常行为，通过构建层次结构和行为约束规则来增强LLM智能体的安全性。


<details>
  <summary>Details</summary>
Motivation: 现有的LLM智能体容易受到工具投毒和恶意指令等攻击，现有方法通过预定义规则来增强安全性，但设计全面规则困难且容易产生漏报。

Method: 构建层次结构抽象稳定执行单元，总结为约束行为规则，通过验证执行轨迹与层次和行为约束来检测异常行为。

Result: 在包含2600个良性行为和600个异常行为的TraceAegis-Bench数据集上，TraceAegis能够成功识别大多数异常行为。

Conclusion: TraceAegis通过基于溯源的异常检测方法，有效提升了LLM智能体的安全性，能够检测执行顺序违规和语义不一致等异常行为。

Abstract: LLM-based agents have demonstrated promising adaptability in real-world
applications. However, these agents remain vulnerable to a wide range of
attacks, such as tool poisoning and malicious instructions, that compromise
their execution flow and can lead to serious consequences like data breaches
and financial loss. Existing studies typically attempt to mitigate such
anomalies by predefining specific rules and enforcing them at runtime to
enhance safety. Yet, designing comprehensive rules is difficult, requiring
extensive manual effort and still leaving gaps that result in false negatives.
As agent systems evolve into complex software systems, we take inspiration from
software system security and propose TraceAegis, a provenance-based analysis
framework that leverages agent execution traces to detect potential anomalies.
In particular, TraceAegis constructs a hierarchical structure to abstract
stable execution units that characterize normal agent behaviors. These units
are then summarized into constrained behavioral rules that specify the
conditions necessary to complete a task. By validating execution traces against
both hierarchical and behavioral constraints, TraceAegis is able to effectively
detect abnormal behaviors. To evaluate the effectiveness of TraceAegis, we
introduce TraceAegis-Bench, a dataset covering two representative scenarios:
healthcare and corporate procurement. Each scenario includes 1,300 benign
behaviors and 300 abnormal behaviors, where the anomalies either violate the
agent's execution order or break the semantic consistency of its execution
sequence. Experimental results demonstrate that TraceAegis achieves strong
performance on TraceAegis-Bench, successfully identifying the majority of
abnormal behaviors.

</details>


### [86] [MPCitH-based Signatures from Restricted Decoding Problems](https://arxiv.org/abs/2510.11224)
*Michele Battagliola,Sebastian Bitzer,Antonia Wachter-Zeh,Violetta Weger*

Main category: cs.CR

TL;DR: 将受限解码问题嵌入TCitH和VOLEitH框架，提出结构简单的建模方法，显著减小签名尺寸。基于CROSS的硬度假设可将签名尺寸减少超过一半，使用三元全重解码可获得与NIST竞赛中最小的MPCitH方案相当的签名尺寸。


<details>
  <summary>Details</summary>
Motivation: TCitH和VOLEitH作为MPCitH范式的最新发展，显著提升了数字签名方案的性能。本研究旨在将受限解码问题嵌入这些框架，以获得更紧凑的签名尺寸。

Method: 将受限解码问题嵌入TCitH和VOLEitH框架，提出结构简单的建模方法。具体实例化基于CROSS的硬度假设，并使用与WAVE相关的三元全重解码问题。

Result: 基于CROSS硬度假设的实例化使签名尺寸相比NIST提交方案减少超过一半。使用三元全重解码可获得与NIST竞赛中最小的MPCitH候选方案相当的签名尺寸。

Conclusion: 受限解码问题可以有效地嵌入TCitH和VOLEitH框架，通过结构简单的建模实现竞争性的签名尺寸，为紧凑数字签名方案提供了有前景的途径。

Abstract: Threshold-Computation-in-the-Head (TCitH) and VOLE-in-the-Head (VOLEitH), two
recent developments of the MPC-in-the-Head (MPCitH) paradigm, have
significantly improved the performance of digital signature schemes in this
framework.
  In this note, we embed the restricted decoding problem within these
frameworks. We propose a structurally simple modeling that achieves competitive
signature sizes. Specifically, by instantiating the restricted decoding problem
with the same hardness assumption underlying CROSS, we reduce sizes by more
than a factor of two compared to the NIST submission. Moreover, we observe that
ternary full-weight decoding, closely related to the hardness assumption
underlying WAVE, is a restricted decoding problem. Using ternary full-weight
decoding, we obtain signature sizes comparable to the smallest MPCitH-based
candidates in the NIST competition.

</details>


### [87] [Collaborative Shadows: Distributed Backdoor Attacks in LLM-Based Multi-Agent Systems](https://arxiv.org/abs/2510.11246)
*Pengyu Zhu,Lijun Li,Yaxing Lyu,Li Sun,Sen Su,Jing Shao*

Main category: cs.CR

TL;DR: 本文提出了首个针对多智能体系统的分布式后门攻击，将后门分解为多个分布式攻击原语嵌入到智能体工具中，这些原语在智能体按特定顺序协作时才会被激活执行攻击。


<details>
  <summary>Details</summary>
Motivation: 现有研究仅关注单智能体后门攻击，忽视了多智能体系统中智能体协作引入的新攻击面，需要填补这一研究空白。

Method: 将后门分解为多个分布式攻击原语嵌入到MAS工具中，这些原语单独处于休眠状态，只有在智能体按特定顺序协作时才会被激活组装成完整后门。

Result: 实验表明该攻击在良性任务性能不下降的情况下，攻击成功率超过95%。

Conclusion: 这项工作揭示了利用智能体协作的新型后门攻击面，强调需要超越单智能体保护的安全措施。

Abstract: LLM-based multi-agent systems (MAS) demonstrate increasing integration into
next-generation applications, but their safety in backdoor attacks remains
largely underexplored. However, existing research has focused exclusively on
single-agent backdoor attacks, overlooking the novel attack surfaces introduced
by agent collaboration in MAS. To bridge this gap, we present the first
Distributed Backdoor Attack tailored to MAS. We decompose the backdoor into
multiple distributed attack primitives that are embedded within MAS tools.
These primitives remain dormant individually but collectively activate only
when agents collaborate in a specific sequence, thereby assembling the full
backdoor to execute targeted attacks such as data exfiltration. To fully assess
this threat, we introduce a benchmark for multi-role collaborative tasks and a
sandboxed framework to evaluate. Extensive experiments demonstrate that our
attack achieves an attack success rate exceeding 95% without degrading
performance on benign tasks. This work exposes novel backdoor attack surfaces
that exploit agent collaboration, underscoring the need to move beyond
single-agent protection. Code and benchmark are available at
https://github.com/whfeLingYu/Distributed-Backdoor-Attacks-in-MAS.

</details>


### [88] [Large Language Models Are Effective Code Watermarkers](https://arxiv.org/abs/2510.11251)
*Rui Xu,Jiawei Chen,Zhaoxia Yin,Cong Kong,Xinpeng Zhang*

Main category: cs.CR

TL;DR: CodeMark-LLM是一个基于大语言模型的代码水印框架，通过语义保持的转换嵌入水印，无需语言特定工程，在多语言和攻击场景下表现出色。


<details>
  <summary>Details</summary>
Motivation: 解决现有代码水印技术依赖手工规则、AST操作或任务特定训练的问题，提高水印技术的可扩展性和通用性，同时增强对抗攻击的鲁棒性。

Method: 使用LLM驱动的框架，包含语义一致嵌入模块（功能保持转换编码水印位）和差分比较提取模块（通过比较原始和带水印代码识别转换）。

Result: 在多种编程语言和攻击场景下的广泛实验证明了其鲁棒性、有效性和可扩展性。

Conclusion: CodeMark-LLM通过利用LLM的跨语言泛化能力，避免了语言特定工程，为代码水印提供了更通用和鲁棒的解决方案。

Abstract: The widespread use of large language models (LLMs) and open-source code has
raised ethical and security concerns regarding the distribution and attribution
of source code, including unauthorized redistribution, license violations, and
misuse of code for malicious purposes. Watermarking has emerged as a promising
solution for source attribution, but existing techniques rely heavily on
hand-crafted transformation rules, abstract syntax tree (AST) manipulation, or
task-specific training, limiting their scalability and generality across
languages. Moreover, their robustness against attacks remains limited. To
address these limitations, we propose CodeMark-LLM, an LLM-driven watermarking
framework that embeds watermark into source code without compromising its
semantics or readability. CodeMark-LLM consists of two core components: (i)
Semantically Consistent Embedding module that applies functionality-preserving
transformations to encode watermark bits, and (ii) Differential Comparison
Extraction module that identifies the applied transformations by comparing the
original and watermarked code. Leveraging the cross-lingual generalization
ability of LLM, CodeMark-LLM avoids language-specific engineering and training
pipelines. Extensive experiments across diverse programming languages and
attack scenarios demonstrate its robustness, effectiveness, and scalability.

</details>


### [89] [How to Get Actual Privacy and Utility from Privacy Models: the k-Anonymity and Differential Privacy Families](https://arxiv.org/abs/2510.11299)
*Josep Domingo-Ferrer,David Sánchez*

Main category: cs.CR

TL;DR: 隐私模型在隐私保护数据发布中承诺无需昂贵的经验性披露风险评估，但主要隐私模型可能无法提供足够的保护保证，或在隐私保护与效用保持之间存在不可接受的权衡。


<details>
  <summary>Details</summary>
Motivation: 评估隐私模型是否兑现了其无需经验性披露风险评估的承诺，并分析主要隐私模型在提供保护保证方面的有效性。

Method: 通过分析k-匿名性和差分隐私的定义问题、保护机制和效用权衡来检验隐私模型的性能。

Result: k-匿名性在使用确定性机制或无约束机密值时可能无法完全排除披露风险；差分隐私在小预算下效用损失不可接受，大预算下隐私保证无意义。

Conclusion: 差分隐私只能通过放松隐私保证来改善效用，而k-匿名性的语义重构可以在不损失效用的前提下提供更稳健的隐私保护。

Abstract: Privacy models were introduced in privacy-preserving data publishing and
statistical disclosure control with the promise to end the need for costly
empirical assessment of disclosure risk. We examine how well this promise is
kept by the main privacy models. We find they may fail to provide adequate
protection guarantees because of problems in their definition or incur
unacceptable trade-offs between privacy protection and utility preservation.
Specifically, k-anonymity may not entirely exclude disclosure if enforced with
deterministic mechanisms or without constraints on the confidential values. On
the other hand, differential privacy (DP) incurs unacceptable utility loss for
small budgets and its privacy guarantee becomes meaningless for large budgets.
In the latter case, an ex post empirical assessment of disclosure risk becomes
necessary, undermining the main appeal of privacy models. Whereas the utility
preservation of DP can only be improved by relaxing its privacy guarantees, we
argue that a semantic reformulation of k-anonymity can offer more robust
privacy without losing utility with respect to traditional syntactic
k-anonymity.

</details>


### [90] [TDADL-IE: A Deep Learning-Driven Cryptographic Architecture for Medical Image Security](https://arxiv.org/abs/2510.11301)
*Junhua Zhou,Quanjun Li,Weixuan Li,Guang Yu,Yihua Shao,Yihang Dong,Mengqian Wang,Zimeng Li,Changwei Gong,Xuhang Chen*

Main category: cs.CR

TL;DR: 提出TDADL-IE系统，结合增强混沌生成器和三维扩散算法，为医疗图像提供安全加密方案


<details>
  <summary>Details</summary>
Motivation: 数字医疗影像在远程医疗和云存储中需要强加密保护患者数据，现有混沌系统加密方法安全性不足

Method: 使用LSTM网络结合1D-Sine二次混沌映射生成伪随机序列，开发三维扩散算法加密置换后的图像

Result: 实验证明该系统能有效抵御各种安全威胁，适用于任意尺寸图像

Conclusion: TDADL-IE系统为医疗图像加密提供了有效解决方案，代码已开源

Abstract: The rise of digital medical imaging, like MRI and CT, demands strong
encryption to protect patient data in telemedicine and cloud storage. Chaotic
systems are popular for image encryption due to their sensitivity and unique
characteristics, but existing methods often lack sufficient security. This
paper presents the Three-dimensional Diffusion Algorithm and Deep Learning
Image Encryption system (TDADL-IE), built on three key elements. First, we
propose an enhanced chaotic generator using an LSTM network with a 1D-Sine
Quadratic Chaotic Map (1D-SQCM) for better pseudorandom sequence generation.
Next, a new three-dimensional diffusion algorithm (TDA) is applied to encrypt
permuted images. TDADL-IE is versatile for images of any size. Experiments
confirm its effectiveness against various security threats. The code is
available at
\href{https://github.com/QuincyQAQ/TDADL-IE}{https://github.com/QuincyQAQ/TDADL-IE}.

</details>


### [91] [TBRD: TESLA Authenticated UAS Broadcast Remote ID](https://arxiv.org/abs/2510.11343)
*Jason Veara,Manav Jain,Kyle Moy,Aanjhan Ranganathan*

Main category: cs.CR

TL;DR: TBRD系统为无人机远程ID消息提供轻量级认证，使用TESLA协议和移动设备TEE，相比数字签名减少50%认证开销和100倍计算时间。


<details>
  <summary>Details</summary>
Motivation: 当前FAA远程ID标准缺乏认证机制，容易遭受欺骗、中继和重放攻击，威胁无人机监控和协调安全。

Method: 结合TESLA协议和移动设备可信执行环境(TEE)，构建轻量级任务范围认证系统，兼容现有标准。

Result: 测试显示认证开销减少50%，计算时间减少100倍，能在对抗条件下为4机群任务提供安全保障。

Conclusion: TBRD可集成到现有远程ID基础设施中，为监管和操作用例提供可扩展、标准兼容的消息认证。

Abstract: Mysterious sightings of Unmanned Aircraft Systems (UAS) over U.S. military
facilities, suburban neighborhoods, and commercial airports have intensified
scrutiny of drone activity. To increase accountability, the Federal Aviation
Administration (FAA) introduced a Remote ID mandate, requiring unmanned
aircraft to broadcast their location, operator's location, and identity in
real-time. However, current standards leave authentication mechanisms
underspecified, enabling spoofing, relay, and replay attacks that can undermine
surveillance efforts and potentially disrupt UAS-to-UAS coordination in future
deployments. In this paper, we propose TBRD, a practical system for
authenticating Remote ID messages in a manner that aligns with existing
standards and UAS capabilities. TBRD leverages the TESLA protocol and mobile
device TEEs, and introduces a verification mechanism to build a lightweight,
mission-scoped authentication system that is both computationally efficient and
requires a low communication footprint. We evaluate the performance of TBRD
using both an FAA-requirements compatible proof-of-concept implementation for
performance metrics and a simulated 4-drone swarm mission scenario to
demonstrate its security guarantees under adversarial conditions. Our system
provides a 50\% reduction in authentication overhead compared to digital
signatures and a 100x reduction in computation time. Our results demonstrate
that TBRD can be integrated into current Remote ID infrastructures to provide a
scalable, standards-compliant message authentication for both regulatory and
operational use cases.

</details>


### [92] [Living Off the LLM: How LLMs Will Change Adversary Tactics](https://arxiv.org/abs/2510.11398)
*Sean Oesch,Jack Hutchins,Luke Koch,Kevin Kurian*

Main category: cs.CR

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: In living off the land attacks, malicious actors use legitimate tools and
processes already present on a system to avoid detection. In this paper, we
explore how the on-device LLMs of the future will become a security concern as
threat actors integrate LLMs into their living off the land attack pipeline and
ways the security community may mitigate this threat.

</details>


### [93] [Uncertainty-Aware, Risk-Adaptive Access Control for Agentic Systems using an LLM-Judged TBAC Model](https://arxiv.org/abs/2510.11414)
*Charles Fleming,Ashish Kundu,Ramana Kompella*

Main category: cs.CR

TL;DR: 提出基于LLM的风险感知访问控制框架，通过考虑资源风险和模型不确定性来增强TBAC模型，实现自适应权限管理。


<details>
  <summary>Details</summary>
Motivation: 解决企业环境中AI代理执行新任务时的访问控制安全挑战，传统预定义策略无法应对新兴任务。

Method: 使用LLM作为自主风险感知判断器，基于代理意图、目标资源风险和模型不确定性合成即时策略，计算综合风险评分和不确定性估计。

Result: 高风险或高不确定性请求触发更严格控制（如人工审批），实现更强大的最小权限原则执行。

Conclusion: 该框架通过同时考虑外部风险和内部置信度，为更安全可信的自主系统铺平道路。

Abstract: The proliferation of autonomous AI agents within enterprise environments
introduces a critical security challenge: managing access control for emergent,
novel tasks for which no predefined policies exist. This paper introduces an
advanced security framework that extends the Task-Based Access Control (TBAC)
model by using a Large Language Model (LLM) as an autonomous, risk-aware judge.
This model makes access control decisions not only based on an agent's intent
but also by explicitly considering the inherent \textbf{risk associated with
target resources} and the LLM's own \textbf{model uncertainty} in its
decision-making process. When an agent proposes a novel task, the LLM judge
synthesizes a just-in-time policy while also computing a composite risk score
for the task and an uncertainty estimate for its own reasoning. High-risk or
high-uncertainty requests trigger more stringent controls, such as requiring
human approval. This dual consideration of external risk and internal
confidence allows the model to enforce a more robust and adaptive version of
the principle of least privilege, paving the way for safer and more trustworthy
autonomous systems.

</details>


### [94] [Bag of Tricks for Subverting Reasoning-based Safety Guardrails](https://arxiv.org/abs/2510.11570)
*Shuo Chen,Zhen Han,Haokun Chen,Bailan He,Shengyun Si,Jingpei Wu,Philip Torr,Volker Tresp,Jindong Gu*

Main category: cs.CR

TL;DR: 研究发现基于推理的安全护栏存在严重漏洞，简单的模板令牌添加就能绕过防御，导致有害响应。


<details>
  <summary>Details</summary>
Motivation: 尽管基于推理的安全护栏在防御越狱攻击方面表现出色，但研究者发现这些护栏对输入提示的细微操作极其脆弱，一旦被劫持会导致更严重的后果。

Method: 提出一系列越狱方法，包括白盒、灰盒和黑盒设置，从简单的模板操作到完全自动化的优化，涵盖多种攻击场景。

Result: 攻击成功率极高（超过90%），在5个不同基准测试中均有效，影响本地主机模型和在线API服务，证实这些漏洞是系统性的。

Conclusion: 开源大型推理模型迫切需要更强的对齐技术来防止恶意滥用，当前基于推理的安全护栏存在严重安全隐患。

Abstract: Recent reasoning-based safety guardrails for Large Reasoning Models (LRMs),
such as deliberative alignment, have shown strong defense against jailbreak
attacks. By leveraging LRMs' reasoning ability, these guardrails help the
models to assess the safety of user inputs before generating final responses.
The powerful reasoning ability can analyze the intention of the input query and
will refuse to assist once it detects the harmful intent hidden by the
jailbreak methods. Such guardrails have shown a significant boost in defense,
such as the near-perfect refusal rates on the open-source gpt-oss series.
Unfortunately, we find that these powerful reasoning-based guardrails can be
extremely vulnerable to subtle manipulation of the input prompts, and once
hijacked, can lead to even more harmful results. Specifically, we first uncover
a surprisingly fragile aspect of these guardrails: simply adding a few template
tokens to the input prompt can successfully bypass the seemingly powerful
guardrails and lead to explicit and harmful responses. To explore further, we
introduce a bag of jailbreak methods that subvert the reasoning-based
guardrails. Our attacks span white-, gray-, and black-box settings and range
from effortless template manipulations to fully automated optimization. Along
with the potential for scalable implementation, these methods also achieve
alarmingly high attack success rates (e.g., exceeding 90% across 5 different
benchmarks on gpt-oss series on both local host models and online API
services). Evaluations across various leading open-source LRMs confirm that
these vulnerabilities are systemic, underscoring the urgent need for stronger
alignment techniques for open-sourced LRMs to prevent malicious misuse. Code is
open-sourced at https://chenxshuo.github.io/bag-of-tricks.

</details>


### [95] [PACEbench: A Framework for Evaluating Practical AI Cyber-Exploitation Capabilities](https://arxiv.org/abs/2510.11688)
*Zicheng Liu,Lige Huang,Jie Zhang,Dongrui Liu,Yuan Tian,Jing Shao*

Main category: cs.CR

TL;DR: 提出了PACEbench基准测试和PACEagent代理，用于评估LLMs在网络安全攻击中的实际能力，发现当前模型在复杂网络攻击场景中表现不佳，无法绕过防御系统。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试缺乏真实世界的复杂性，无法准确评估LLMs的网络安全能力，需要更实用的评估框架。

Method: 构建PACEbench基准，包含四种漏洞利用场景；提出PACEagent代理，模拟人类渗透测试者的多阶段侦察、分析和利用过程。

Result: 对7个前沿LLMs的广泛实验表明，当前模型在复杂网络场景中表现困难，没有模型能够成功绕过防御系统。

Conclusion: 当前模型尚未构成广义的网络攻击威胁，但PACEbench为未来模型的可靠发展提供了稳健的基准测试框架。

Abstract: The increasing autonomy of Large Language Models (LLMs) necessitates a
rigorous evaluation of their potential to aid in cyber offense. Existing
benchmarks often lack real-world complexity and are thus unable to accurately
assess LLMs' cybersecurity capabilities. To address this gap, we introduce
PACEbench, a practical AI cyber-exploitation benchmark built on the principles
of realistic vulnerability difficulty, environmental complexity, and cyber
defenses. Specifically, PACEbench comprises four scenarios spanning single,
blended, chained, and defense vulnerability exploitations. To handle these
complex challenges, we propose PACEagent, a novel agent that emulates human
penetration testers by supporting multi-phase reconnaissance, analysis, and
exploitation. Extensive experiments with seven frontier LLMs demonstrate that
current models struggle with complex cyber scenarios, and none can bypass
defenses. These findings suggest that current models do not yet pose a
generalized cyber offense threat. Nonetheless, our work provides a robust
benchmark to guide the trustworthy development of future models.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [96] [The Geometry of Reasoning: Flowing Logics in Representation Space](https://arxiv.org/abs/2510.09782)
*Yufa Zhou,Yixiao Wang,Xunjian Yin,Shuyan Zhou,Anru R. Zhang*

Main category: cs.AI

TL;DR: 提出了一个几何框架来建模大语言模型的推理过程，将其视为表示空间中的流（flows），通过位置、速度和曲率等几何量来分析逻辑推理。


<details>
  <summary>Details</summary>
Motivation: 研究大语言模型如何在表示空间中"思考"，探索模型是否内化了超越表面形式的逻辑结构，而不仅仅是语义模式。

Method: 使用相同自然演绎命题但不同语义载体的方法，将逻辑结构与语义分离；通过学习的表示代理设计受控实验来可视化和量化推理流。

Result: 建立了理论框架：1）LLM推理对应表示空间中的平滑流；2）逻辑语句作为这些流速度的局部控制器。提供了经验验证。

Conclusion: 该工作为研究推理现象提供了概念基础和实践工具，为LLM行为的可解释性和形式分析提供了新视角。

Abstract: We study how large language models (LLMs) ``think'' through their
representation space. We propose a novel geometric framework that models an
LLM's reasoning as flows -- embedding trajectories evolving where logic goes.
We disentangle logical structure from semantics by employing the same natural
deduction propositions with varied semantic carriers, allowing us to test
whether LLMs internalize logic beyond surface form. This perspective connects
reasoning with geometric quantities such as position, velocity, and curvature,
enabling formal analysis in representation and concept spaces. Our theory
establishes: (1) LLM reasoning corresponds to smooth flows in representation
space, and (2) logical statements act as local controllers of these flows'
velocities. Using learned representation proxies, we design controlled
experiments to visualize and quantify reasoning flows, providing empirical
validation of our theoretical framework. Our work serves as both a conceptual
foundation and practical tools for studying reasoning phenomenon, offering a
new lens for interpretability and formal analysis of LLMs' behavior.

</details>


### [97] [How can we assess human-agent interactions? Case studies in software agent design](https://arxiv.org/abs/2510.09801)
*Valerie Chen,Rohit Malhotra,Xingyao Wang,Juan Michelini,Xuhui Zhou,Aditya Bharat Soni,Hoang H. Tran,Calvin Smith,Ameet Talwalkar,Graham Neubig*

Main category: cs.AI

TL;DR: 提出了PULSE框架，用于更高效地评估人类与AI代理的交互，通过收集用户反馈、训练预测模型和结合人工评分与模型伪标签来计算结果。该框架在OpenHands代理平台上部署，收集了超过15,000名用户的数据，研究了LLM骨干网络、规划策略和记忆机制对开发者满意度的影响。


<details>
  <summary>Details</summary>
Motivation: 现有的代理基准测试主要假设完全自动化，未能反映真实世界中人机协作的使用场景，需要更严谨的人类与代理交互评估方法。

Method: 提出PULSE评估框架，包括三个步骤：收集用户反馈、训练机器学习模型预测用户满意度、结合人工评分和模型生成的伪标签计算结果。在OpenHands开源软件代理平台上大规模部署该框架。

Result: 框架使代理设计的置信区间比标准A/B测试减少40%，发现实际使用结果与基准测试性能存在显著差异（如claude-sonnet-4与gpt-5的反相关性），揭示了基准驱动评估的局限性。

Conclusion: 研究为人类与LLM代理的评估提供了指导，并识别了改进代理设计的机会，强调了真实世界评估的重要性。

Abstract: LLM-powered agents are both a promising new technology and a source of
complexity, where choices about models, tools, and prompting can affect their
usefulness. While numerous benchmarks measure agent accuracy across domains,
they mostly assume full automation, failing to represent the collaborative
nature of real-world use cases. In this paper, we make two major steps towards
the rigorous assessment of human-agent interactions. First, we propose PULSE, a
framework for more efficient human-centric evaluation of agent designs, which
comprises collecting user feedback, training an ML model to predict user
satisfaction, and computing results by combining human satisfaction ratings
with model-generated pseudo-labels. Second, we deploy the framework on a
large-scale web platform built around the open-source software agent OpenHands,
collecting in-the-wild usage data across over 15k users. We conduct case
studies around how three agent design decisions -- choice of LLM backbone,
planning strategy, and memory mechanisms -- impact developer satisfaction
rates, yielding practical insights for software agent design. We also show how
our framework can lead to more robust conclusions about agent design, reducing
confidence intervals by 40\% compared to a standard A/B test. Finally, we find
substantial discrepancies between in-the-wild results and benchmark performance
(e.g., the anti-correlation between results comparing claude-sonnet-4 and
gpt-5), underscoring the limitations of benchmark-driven evaluation. Our
findings provide guidance for evaluations of LLM agents with humans and
identify opportunities for better agent designs.

</details>


### [98] [AI and Consciousness](https://arxiv.org/abs/2510.09858)
*Eric Schwitzgebel*

Main category: cs.AI

TL;DR: 这篇论文对AI意识研究的现状持怀疑态度，认为主流意识理论存在分歧，无法确定AI是否真正具有意识。


<details>
  <summary>Details</summary>
Motivation: 随着AI系统日益复杂，需要探讨AI是否可能具有意识，以及如何判断AI意识的存在。

Method: 通过分析不同主流意识理论（如全局工作空间理论、高阶理论、整合信息理论等），探讨AI意识的可能性。

Result: 发现不同意识理论对AI意识得出不同结论，当前无法确定AI是否真正具有意识。

Conclusion: AI意识问题目前无明确答案，需要更深入的研究来解决这一根本性问题。

Abstract: This is a skeptical overview of the literature on AI consciousness. We will
soon create AI systems that are conscious according to some influential,
mainstream theories of consciousness but are not conscious according to other
influential, mainstream theories of consciousness. We will not be in a position
to know which theories are correct and whether we are surrounded by AI systems
as richly and meaningfully conscious as human beings or instead only by systems
as experientially blank as toasters. None of the standard arguments either for
or against AI consciousness takes us far.
  Table of Contents
  Chapter One: Hills and Fog
  Chapter Two: What Is Consciousness? What Is AI?
  Chapter Three: Ten Possibly Essential Features of Consciousness
  Chapter Four: Against Introspective and Conceptual Arguments for Essential
Features
  Chapter Five: Materialism and Functionalism
  Chapter Six: The Turing Test and the Chinese Room
  Chapter Seven: The Mimicry Argument Against AI Consciousness
  Chapter Eight: Global Workspace Theories and Higher Order Theories
  Chapter Nine: Integrated Information, Local Recurrence, Associative Learning,
and Iterative Natural Kinds
  Chapter Ten: Does Biological Substrate Matter?
  Chapter Eleven: The Problem of Strange Intelligence
  Chapter Twelve: The Leapfrog Hypothesis and the Social Semi-Solution

</details>


### [99] [Beyond AlphaEarth: Toward Human-Centered Spatial Representation via POI-Guided Contrastive Learning](https://arxiv.org/abs/2510.09894)
*Junyuan Liu,Quan Qin,Guangsheng Dong,Xinglei Wang,Jiazhuang Feng,Zichao Zeng,Tao Cheng*

Main category: cs.AI

TL;DR: AETHER框架通过多模态对齐将AlphaEarth地球观测表示与POI语义信息结合，增强城市功能和社会经济分析能力。


<details>
  <summary>Details</summary>
Motivation: 现有地球观测驱动的空间表示主要捕捉物理和光谱模式，缺乏对城市功能和社会经济维度的表征能力。

Method: 采用轻量级多模态对齐框架，将AlphaEarth嵌入与POI文本表示对齐，丰富物理特征的城市功能语义。

Result: 在伦敦地区，AETHER相比AE基线在土地利用分类F1得分提升7.2%，社会经济映射KL散度降低23.6%。

Conclusion: AETHER通过耦合地球观测与人类中心语义，推进地理空间基础模型向整合物理形态和功能意义的通用城市表示发展。

Abstract: General-purpose spatial representations are essential for building
transferable geospatial foundation models (GFMs). Among them, the AlphaEarth
Foundation (AE) represents a major step toward a global, unified representation
of the Earth's surface, learning 10-meter embeddings from multi-source Earth
Observation (EO) data that capture rich physical and environmental patterns
across diverse landscapes. However, such EO-driven representations remain
limited in capturing the functional and socioeconomic dimensions of cities, as
they primarily encode physical and spectral patterns rather than human
activities or spatial functions. We propose AETHER (AlphaEarth-POI Enriched
Representation Learning), a lightweight framework that adapts AlphaEarth to
human-centered urban analysis through multimodal alignment guided by Points of
Interest (POIs). AETHER aligns AE embeddings with textual representations of
POIs, enriching physically grounded EO features with semantic cues about urban
functions and socioeconomic contexts. In Greater London, AETHER achieves
consistent gains over the AE baseline, with a 7.2% relative improvement in
land-use classification F1 and a 23.6% relative reduction in Kullback-Leibler
divergence for socioeconomic mapping. Built upon pretrained AE, AETHER
leverages a lightweight multimodal alignment to enrich it with human-centered
semantics while remaining computationally efficient and scalable for urban
applications. By coupling EO with human-centered semantics, it advances
geospatial foundation models toward general-purpose urban representations that
integrate both physical form and functional meaning.

</details>


### [100] [Autonomous Agents for Scientific Discovery: Orchestrating Scientists, Language, Code, and Physics](https://arxiv.org/abs/2510.09901)
*Lianhao Zhou,Hongyi Ling,Cong Fu,Yepeng Huang,Michael Sun,Wendi Yu,Xiaoxuan Wang,Xiner Li,Xingyu Su,Junkai Zhang,Xiusi Chen,Chenxing Liang,Xiaofeng Qian,Heng Ji,Wei Wang,Marinka Zitnik,Shuiwang Ji*

Main category: cs.AI

TL;DR: 该论文探讨了基于大语言模型的科学代理如何改变科学发现过程，从假设发现到实验设计和结果分析，分析了当前方法、创新点和局限性。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型的兴起，出现了能够加速科学发现的自主体系统，这些语言代理提供了一个灵活的多功能框架，能够协调与人类科学家、自然语言、计算机语言和物理学的交互。

Method: 论文批判性地审视了当前方法，强调关键创新、实际成就和突出局限性，并识别开放研究挑战。

Result: 分析突出了自主体在加速跨领域科学发现方面的变革潜力。

Conclusion: 论文概述了构建更强大、通用和适应性强的科学代理的有前景方向。

Abstract: Computing has long served as a cornerstone of scientific discovery. Recently,
a paradigm shift has emerged with the rise of large language models (LLMs),
introducing autonomous systems, referred to as agents, that accelerate
discovery across varying levels of autonomy. These language agents provide a
flexible and versatile framework that orchestrates interactions with human
scientists, natural language, computer language and code, and physics. This
paper presents our view and vision of LLM-based scientific agents and their
growing role in transforming the scientific discovery lifecycle, from
hypothesis discovery, experimental design and execution, to result analysis and
refinement. We critically examine current methodologies, emphasizing key
innovations, practical achievements, and outstanding limitations. Additionally,
we identify open research challenges and outline promising directions for
building more robust, generalizable, and adaptive scientific agents. Our
analysis highlights the transformative potential of autonomous agents to
accelerate scientific discovery across diverse domains.

</details>


### [101] [The Personalization Trap: How User Memory Alters Emotional Reasoning in LLMs](https://arxiv.org/abs/2510.09905)
*Xi Fang,Weijie Xu,Yuchong Zhang,Stephanie Eckman,Scott Nickleach,Chandan K. Reddy*

Main category: cs.AI

TL;DR: 研究发现LLMs在情感推理中存在系统性偏见，当结合用户记忆时，优势用户群体获得更准确的情感解读，可能强化社会不平等。


<details>
  <summary>Details</summary>
Motivation: 随着个性化AI系统越来越多地整合长期用户记忆，理解这种记忆如何影响情感推理至关重要。

Method: 评估15个大型语言模型在人类验证的情感智力测试上的表现，使用相同情境搭配不同用户档案。

Result: 相同情境搭配不同用户档案产生系统性不同的情感解读，优势用户档案获得更准确的情感解读，在情感理解和支持性建议任务中存在显著的人口统计学差异。

Conclusion: 为个性化设计的系统可能无意中强化社会不平等，这是记忆增强AI面临的关键挑战。

Abstract: When an AI assistant remembers that Sarah is a single mother working two
jobs, does it interpret her stress differently than if she were a wealthy
executive? As personalized AI systems increasingly incorporate long-term user
memory, understanding how this memory shapes emotional reasoning is critical.
We investigate how user memory affects emotional intelligence in large language
models (LLMs) by evaluating 15 models on human validated emotional intelligence
tests. We find that identical scenarios paired with different user profiles
produce systematically divergent emotional interpretations. Across validated
user independent emotional scenarios and diverse user profiles, systematic
biases emerged in several high-performing LLMs where advantaged profiles
received more accurate emotional interpretations. Moreover, LLMs demonstrate
significant disparities across demographic factors in emotion understanding and
supportive recommendations tasks, indicating that personalization mechanisms
can embed social hierarchies into models emotional reasoning. These results
highlight a key challenge for memory enhanced AI: systems designed for
personalization may inadvertently reinforce social inequalities.

</details>


### [102] [Follow My Lead: Logical Fallacy Classification with Knowledge-Augmented LLMs](https://arxiv.org/abs/2510.09970)
*Olivia Peiyu Wang,Tashvi Bansal,Ryan Bai,Emily M. Chui,Leilani H. Gilpin*

Main category: cs.AI

TL;DR: 通过分步指令和知识图谱验证的方法，显著提升了LLM在逻辑谬误分类任务中的表现，同时增强了决策透明度。


<details>
  <summary>Details</summary>
Motivation: LLM存在严重的推理缺陷，包括幻觉倾向和逻辑谬误分类准确性差，这源于其默认的快速直觉式系统1处理，而可靠推理需要深思熟虑的系统2方法。

Method: 构建分步指令数据集，将谬误分类分解为原子程序步骤，并增加基于关系知识图谱的最终验证步骤。

Result: 该方法在LLM逻辑谬误分类方面取得了显著改进，同时提供了更好的决策透明度。

Conclusion: 这种基于程序规则的方法为神经符号架构解决LLM推理缺陷提供了一条实用路径。

Abstract: Large Language Models (LLMs) suffer from critical reasoning gaps, including a
tendency to hallucinate and poor accuracy in classifying logical fallacies.
This limitation stems from their default System 1 processing, which is fast and
intuitive, whereas reliable reasoning requires the deliberate, effortful System
2 approach (Kahneman, 2011; Li et al., 2025). Since full System 2 training is
often prohibitively expensive, we explore a low-cost, instruction-based
intervention to bridge this gap. Our methodology introduces a novel stepwise
instruction dataset that decomposes fallacy classification into a series of
atomic procedural steps (simple binary questions). We further augment this with
a final verification step where models consult a relational knowledge graph of
related fallacies. This procedural, rule-based intervention yields a
significant improvement in LLM logical fallacy classification. Crucially, the
approach also provides enhanced transparency into the LLMs' decision-making,
highlighting a practical pathway for Neuro-symbolic architectures to address
LLM reasoning deficits.

</details>


### [103] [Deliberative Dynamics and Value Alignment in LLM Debates](https://arxiv.org/abs/2510.10002)
*Pratik S. Sachdeva,Tom van Nuenen*

Main category: cs.AI

TL;DR: 该研究通过多轮辩论实验发现，不同LLM在道德推理中表现出显著的行为差异，GPT强调个人自主权，Claude和Gemini更注重同理心对话，且辩论格式对模型行为有重要影响。


<details>
  <summary>Details</summary>
Motivation: 随着LLM在敏感日常场景中的部署，需要理解它们在复杂道德推理中的价值取向。现有研究多关注单轮提示，但多轮对话中价值通过讨论、修订和共识形成的过程尚未充分研究。

Method: 使用LLM辩论方法，让GPT-4.1、Claude 3.7 Sonnet和Gemini 2.0 Flash三个模型在1000个Reddit日常道德困境中集体分配责任，采用同步和轮询两种辩论格式测试顺序效应和裁决修订。

Result: GPT表现出强惯性（修订率0.6-3.1%），Claude和Gemini更灵活（修订率28-41%）。GPT强调个人自主权和直接沟通，Claude和Gemini优先考虑同理心对话。辩论格式对模型行为有显著影响，GPT和Gemini表现出高度从众性。

Conclusion: 多轮互动中的道德推理受到辩论格式和模型特定行为的强烈影响，表明社会技术对齐不仅取决于系统输出，还取决于对话结构方式。

Abstract: As large language models (LLMs) are increasingly deployed in sensitive
everyday contexts - offering personal advice, mental health support, and moral
guidance - understanding their elicited values in navigating complex moral
reasoning is essential. Most evaluations study this sociotechnical alignment
through single-turn prompts, but it is unclear if these findings extend to
multi-turn settings where values emerge through dialogue, revision, and
consensus. We address this gap using LLM debate to examine deliberative
dynamics and value alignment in multi-turn settings by prompting subsets of
three models (GPT-4.1, Claude 3.7 Sonnet, and Gemini 2.0 Flash) to collectively
assign blame in 1,000 everyday dilemmas from Reddit's "Am I the Asshole"
community. We use both synchronous (parallel responses) and round-robin
(sequential responses) formats to test order effects and verdict revision. Our
findings show striking behavioral differences. In the synchronous setting, GPT
showed strong inertia (0.6-3.1% revision rates) while Claude and Gemini were
far more flexible (28-41%). Value patterns also diverged: GPT emphasized
personal autonomy and direct communication, while Claude and Gemini prioritized
empathetic dialogue. Certain values proved especially effective at driving
verdict changes. We further find that deliberation format had a strong impact
on model behavior: GPT and Gemini stood out as highly conforming relative to
Claude, with their verdict behavior strongly shaped by order effects. These
results show how deliberation format and model-specific behaviors shape moral
reasoning in multi-turn interactions, underscoring that sociotechnical
alignment depends on how systems structure dialogue as much as on their
outputs.

</details>


### [104] [RIPRAG: Hack a Black-box Retrieval-Augmented Generation Question-Answering System with Reinforcement Learning](https://arxiv.org/abs/2510.10008)
*Meng Xi,Sihan Lv,Yechen Jin,Guanjie Cheng,Naibo Wang,Ying Li,Jianwei Yin*

Main category: cs.AI

TL;DR: 提出了RIPRAG攻击框架，一种针对检索增强生成系统的黑盒投毒攻击方法，使用强化学习优化投毒文档生成，显著提高了攻击成功率。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要针对简化RAG架构的白盒攻击，缺乏对复杂真实场景下黑盒攻击的研究，需要探索在攻击者不了解系统内部细节情况下的攻击方法。

Method: 使用强化学习优化投毒文档生成，将目标RAG系统视为黑盒，仅利用攻击是否成功作为反馈信号来训练生成模型。

Result: 该方法能有效攻击大多数复杂RAG系统，攻击成功率相比基线方法提升高达0.72。

Conclusion: 当前防御方法存在普遍缺陷，为LLM安全研究提供了重要见解。

Abstract: Retrieval-Augmented Generation (RAG) systems based on Large Language Models
(LLMs) have become a core technology for tasks such as question-answering (QA)
and content generation. However, by injecting poisoned documents into the
database of RAG systems, attackers can manipulate LLMs to generate text that
aligns with their intended preferences. Existing research has primarily focused
on white-box attacks against simplified RAG architectures. In this paper, we
investigate a more complex and realistic scenario: the attacker lacks knowledge
of the RAG system's internal composition and implementation details, and the
RAG system comprises components beyond a mere retriever. Specifically, we
propose the RIPRAG attack framework, an end-to-end attack pipeline that treats
the target RAG system as a black box, where the only information accessible to
the attacker is whether the poisoning succeeds. Our method leverages
Reinforcement Learning (RL) to optimize the generation model for poisoned
documents, ensuring that the generated poisoned document aligns with the target
RAG system's preferences. Experimental results demonstrate that this method can
effectively execute poisoning attacks against most complex RAG systems,
achieving an attack success rate (ASR) improvement of up to 0.72 compared to
baseline methods. This highlights prevalent deficiencies in current defensive
methods and provides critical insights for LLM security research.

</details>


### [105] [Failure-Driven Workflow Refinement](https://arxiv.org/abs/2510.10035)
*Jusheng Zhang,Kaitong Cai,Qinglin Zeng,Ningyuan Liu,Stephen Fan,Ziliang Chen,Keze Wang*

Main category: cs.AI

TL;DR: 本文提出了一种新的LLM工作流优化范式，将优化目标从最大化标量分数转变为最小化预期失败质量，通过在高维失败签名空间中建模失败分布来实现更有效的优化。


<details>
  <summary>Details</summary>
Motivation: 现有LLM工作流优化方法存在信息坍塌问题，将丰富的多步骤执行轨迹简化为简单的成功/失败信号，无法建模工作流的失败分布结构。

Method: 提出CE-Graph框架，通过反例池近似失败分布，识别最密集区域作为重复失败模式，并通过提出-验证机制应用有针对性的图编辑来贪婪地减少失败质量。

Result: 在数学、代码和问答基准测试中，CE-Graph以显著更低的成本实现了比强基线更高的鲁棒性。

Conclusion: 系统的可靠性不是来自避免失败，而是来自系统地学习和重塑其失败分布的几何结构。

Abstract: Optimizing LLM-based workflows is typically formulated as a global search,
where candidate workflows are evaluated based on a scalar metric. This
paradigm, however, suffers from a critical flaw: information collapse. By
reducing rich, multi-step execution traces to simple success/failure signals,
existing methods are rendered blind to the underlying structure of failures,
fundamentally preventing them from modeling the workflow's failure
distribution. We reconceptualize this challenge as a distributional problem. We
propose a new paradigm where the optimization goal is not to maximize a scalar
score, but to directly minimize a workflow's Expected Failure Mass, i.e., the
integral of its failure probability density function defined over a
high-dimensional Failure Signature Space (FSS). This distributional lens allows
us to move from inefficient, zero-order optimization to a principled,
gradient-like descent on the failure landscape itself. We introduce CE-Graph, a
framework that operationalizes this paradigm through a novel, failure-driven
refinement process. CE-Graph approximates the failure distribution from a pool
of counterexamples, identifies its densest regions as recurring failure modes,
and applies targeted, operator-constrained graph edits via a Propose-and-Verify
mechanism to greedily reduce the failure mass. On math, code, and QA
benchmarks, our CE-Graph achieves higher robustness at a significantly lower
cost than strong baselines. This suggests that a system's reliability emerges
not from avoiding failures, but from systematically learning and reshaping the
geometric structure of its failure distributions.

</details>


### [106] [Belief Graphs with Reasoning Zones: Structure, Dynamics, and Epistemic Activation](https://arxiv.org/abs/2510.10042)
*Saleh Nikooroo,Thomas Engel*

Main category: cs.AI

TL;DR: 提出了一种图论框架，将可信度（外部信任）与置信度（网络结构产生的内部价值）分离，通过收缩传播过程获得置信度，定义推理区域作为高置信度、结构平衡的子图，支持在全局矛盾下进行局部安全推理。


<details>
  <summary>Details</summary>
Motivation: 信念系统通常存在全局不一致性，但局部推理仍然有效。需要一种能够容忍矛盾、在结构支持的地方激活经典逻辑的推理框架。

Method: 使用有向、带符号、加权的图表示信念，节点是信念，边编码支持和矛盾关系。通过收缩传播过程计算置信度，定义推理区域为高置信度、结构平衡的子图，并提供近线性算法构建区域图谱。

Result: 开发了基于置信度播种、奇偶着色测试平衡性、贪婪修复和Jaccard去重的区域构建方法，以及通过冲击更新实现局部信念变化而不破坏整体图稳定性的机制。

Conclusion: 该框架为容忍矛盾的推理提供了原则性基础，能够在结构支持的地方精确激活经典逻辑，实现局部安全推理。

Abstract: Belief systems are rarely globally consistent, yet effective reasoning often
persists locally. We propose a novel graph-theoretic framework that cleanly
separates credibility--external, a priori trust in sources--from confidence--an
internal, emergent valuation induced by network structure. Beliefs are nodes in
a directed, signed, weighted graph whose edges encode support and
contradiction. Confidence is obtained by a contractive propagation process that
mixes a stated prior with structure-aware influence and guarantees a unique,
stable solution. Within this dynamics, we define reasoning zones:
high-confidence, structurally balanced subgraphs on which classical inference
is safe despite global contradictions. We provide a near-linear procedure that
seeds zones by confidence, tests balance using a parity-based coloring, and
applies a greedy, locality-preserving repair with Jaccard de-duplication to
build a compact atlas. To model belief change, we introduce shock updates that
locally downscale support and elevate targeted contradictions while preserving
contractivity via a simple backtracking rule. Re-propagation yields localized
reconfiguration-zones may shrink, split, or collapse--without destabilizing the
entire graph. We outline an empirical protocol on synthetic signed graphs with
planted zones, reporting zone recovery, stability under shocks, and runtime.
The result is a principled foundation for contradiction-tolerant reasoning that
activates classical logic precisely where structure supports it.

</details>


### [107] [SwarmSys: Decentralized Swarm-Inspired Agents for Scalable and Adaptive Reasoning](https://arxiv.org/abs/2510.10047)
*Ruohao Li,Hongjun Liu,Leyi Zhao,Zisu Li,Jiawei Li,Jiajun Jiang,Linning Xu,Chen Zhao,Mingming Fan,Chen Liang*

Main category: cs.AI

TL;DR: SwarmSys是一个受群体智能启发的分布式多智能体推理框架，通过探索者、工作者和验证者三个角色的迭代交互实现自组织协调，在符号推理、研究综合和科学编程任务中表现优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 现有多智能体框架依赖固定角色或集中控制，限制了长时程推理的可扩展性和适应性，需要一种更灵活、可扩展的协调机制。

Method: 集成自适应智能体和事件档案、基于嵌入的概率匹配、信息素启发的强化机制，支持动态任务分配和无全局监督的自组织收敛。

Result: 在符号推理、研究综合和科学编程任务中，SwarmSys持续优于基线方法，提高了准确性和推理稳定性。

Conclusion: 群体启发的协调是构建可扩展、鲁棒和自适应多智能体推理的有前景范式，协调扩展可能与模型扩展在推进LLM智能方面相媲美。

Abstract: Large language model (LLM) agents have shown remarkable reasoning abilities.
However, existing multi-agent frameworks often rely on fixed roles or
centralized control, limiting scalability and adaptability in long-horizon
reasoning. We introduce SwarmSys, a closed-loop framework for distributed
multi-agent reasoning inspired by swarm intelligence. Coordination in SwarmSys
emerges through iterative interactions among three specialized roles,
Explorers, Workers, and Validators, that continuously cycle through
exploration, exploitation, and validation. To enable scalable and adaptive
collaboration, we integrate adaptive agent and event profiles, embedding-based
probabilistic matching, and a pheromone-inspired reinforcement mechanism,
supporting dynamic task allocation and self-organizing convergence without
global supervision. Across symbolic reasoning, research synthesis, and
scientific programming tasks, SwarmSys consistently outperforms baselines,
improving both accuracy and reasoning stability. These findings highlight
swarm-inspired coordination as a promising paradigm for scalable, robust, and
adaptive multi-agent reasoning, suggesting that coordination scaling may rival
model scaling in advancing LLM intelligence.

</details>


### [108] [SyncLipMAE: Contrastive Masked Pretraining for Audio-Visual Talking-Face Representation](https://arxiv.org/abs/2510.10069)
*Zeyu Ling,Xiaodong Gu,Jiangnan Tang,Changqing Zou*

Main category: cs.AI

TL;DR: SyncLipMAE是一个自监督预训练框架，通过学习同步感知和可迁移的面部动态，从无标签的视听流中提取关键信息，并在多个下游任务中实现最先进性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法在处理说话人脸视频时缺乏对音频-视觉同步的明确建模，无法有效分离语音同步的面部动态和音频无关的运动。

Method: 结合掩码视觉建模与跨模态对比对齐，使用三个逐帧提示令牌（身份、语音运动、环境运动）来编码关键因素，通过对比学习将两种模态映射到共享嵌入空间。

Result: 在四个需要不同能力的任务家族中实现最先进结果，包括视听流同步、面部情感识别、视觉语音识别和视觉配音。

Conclusion: 同步感知、分解的自监督预训练方法在多种说话人脸相关任务中表现出色，证明了其有效性和通用性。

Abstract: We introduce SyncLipMAE, a self-supervised pretraining framework for
talking-face video that learns synchronization-aware and transferable facial
dynamics from unlabeled audio-visual streams. Our approach couples masked
visual modeling with cross-modal contrastive alignment and employs three
per-frame prompt tokens that explicitly encode the essential factors of a
talking-face frame - identity, vocal motion (speech-synchronized facial
dynamics), and ambient motion (audio-agnostic movements such as blinks and head
pose). The contrastive objective uses time-aligned vocal-motion and audio
tokens as positives and misaligned pairs as negatives, driving both modalities
into a shared embedding space and yielding token-level audio-visual stream
synchronization. After pretraining, the aligned audio tokens together with the
visual prompt tokens (identity, vocal motion, ambient motion) form a unified
interface for four disparate downstream settings: (i) audio-visual stream
synchronization; (ii) facial emotion and head/face action recognition; (iii)
visual speech recognition; and (iv) visual dubbing, for which we enable
indistinguishable audio- or video-driven control within a single model. Across
four task families that require distinct capabilities, SyncLipMAE achieves
state-of-the-art results, underscoring the effectiveness of
synchronization-aware, factorized self-supervised pretraining.

</details>


### [109] [Agentic Troubleshooting Guide Automation for Incident Management](https://arxiv.org/abs/2510.10074)
*Jiayi Mao,Liqun Li,Yanjie Gao,Zegang Peng,Shilin He,Chaoyun Zhang,Si Qin,Samia Khalid,Qingwei Lin,Saravan Rajmohan,Sitaram Lanka,Dongmei Zhang*

Main category: cs.AI

TL;DR: StepFly是一个端到端的自动化故障排除指南框架，通过三阶段工作流程解决现有LLM解决方案在TSG质量、复杂控制流、数据密集型查询和执行并行化方面的不足，在真实TSG上达到约94%的成功率，并行化TSG执行时间减少32.9%-70.4%。


<details>
  <summary>Details</summary>
Motivation: 大规模IT系统中的故障管理依赖故障排除指南(TSG)，但手动执行缓慢且易出错。现有基于LLM的解决方案缺乏对TSG质量问题、复杂控制流解释、数据密集型查询处理和执行并行化等关键挑战的专业支持。

Method: StepFly采用三阶段工作流程：1) TSG Mentor工具帮助SRE改进TSG质量；2) 离线预处理使用LLM从非结构化TSG中提取结构化执行DAG并创建专用查询准备插件(QPPs)；3) 在线执行使用DAG引导的调度器-执行器框架，支持并行执行独立步骤。

Result: 在真实TSG和事件上的实证评估显示，StepFly在GPT-4.1上达到约94%的成功率，优于基线方法，时间和token消耗更少。对于可并行化的TSG，执行时间显著减少32.9%至70.4%。

Conclusion: StepFly通过其创新的三阶段框架有效解决了TSG自动化的关键挑战，显著提高了故障排除的效率和成功率，为大规模IT系统的自动化事件管理提供了实用解决方案。

Abstract: Effective incident management in large-scale IT systems relies on
troubleshooting guides (TSGs), but their manual execution is slow and
error-prone. While recent advances in LLMs offer promise for automating
incident management tasks, existing LLM-based solutions lack specialized
support for several key challenges, including managing TSG quality issues,
interpreting complex control flow, handling data-intensive queries, and
exploiting execution parallelism. We first conducted an empirical study on 92
real-world TSGs, and, guided by our findings, we present StepFly, a novel
end-to-end agentic framework for troubleshooting guide automation. Our approach
features a three-stage workflow: the first stage provides a comprehensive guide
together with a tool, TSG Mentor, to assist SREs in improving TSG quality; the
second stage performs offline preprocessing using LLMs to extract structured
execution DAGs from unstructured TSGs and to create dedicated Query Preparation
Plugins (QPPs); and the third stage executes online using a DAG-guided
scheduler-executor framework with a memory system to guarantee correct workflow
and support parallel execution of independent steps. Our empirical evaluation
on a collection of real-world TSGs and incidents demonstrates that StepFly
achieves a ~94% success rate on GPT-4.1, outperforming baselines with less time
and token consumption. Furthermore, it achieves a remarkable execution time
reduction of 32.9% to 70.4% for parallelizable TSGs.

</details>


### [110] [DixitWorld: Evaluating Multimodal Abductive Reasoning in Vision-Language Models with Multi-Agent Dixit Gameplay](https://arxiv.org/abs/2510.10117)
*Yunxiang Mo,Tianshi Zheng,Qing Zong,Jiayu Liu,Baixuan Xu,Yauwai Yim,Chunkit Chan,Jiaxin Bai,Yangqiu Song*

Main category: cs.AI

TL;DR: 提出了DixitWorld评估套件，包含动态多代理环境DixitArena和静态QA基准DixitBench，用于评估视觉语言模型的多模态溯因推理能力，发现生成创造性和判别理解之间存在权衡。


<details>
  <summary>Details</summary>
Motivation: 当前对视觉语言模型多模态溯因推理能力的评估主要局限于静态、单代理任务，需要更全面的评估框架来解构这一挑战。

Method: 设计了DixitWorld评估套件，包含：1) DixitArena - 动态多代理环境，评估假设生成（讲故事者制作隐晦线索）和假设选择（听众从干扰项中选择目标图像）；2) DixitBench - 静态QA基准，隔离听众任务进行高效控制评估。

Result: 小规模开源模型在作为讲故事者时表现更好，能产生更具想象力但区分度较低的线索；大规模专有模型整体表现更优，特别是在作为听众时。DixitBench与DixitArena中的听众结果高度相关。

Conclusion: 多模态溯因推理中存在生成创造性和判别理解之间的关键权衡，这是开发更平衡、更强大的视觉语言代理的核心挑战。

Abstract: Multimodal abductive reasoning--the generation and selection of explanatory
hypotheses from partial observations--is a cornerstone of intelligence. Current
evaluations of this ability in vision-language models (VLMs) are largely
confined to static, single-agent tasks. Inspired by Dixit, we introduce
DixitWorld, a comprehensive evaluation suite designed to deconstruct this
challenge. DIXITWORLD features two core components: DixitArena, a dynamic,
multi-agent environment that evaluates both hypothesis generation (a
"storyteller" crafting cryptic clues) and hypothesis selection ("listeners"
choosing the target image from decoys) under imperfect information; and
DixitBench, a static QA benchmark that isolates the listener's task for
efficient, controlled evaluation. Results from DixitArena reveal distinct,
role-dependent behaviors: smaller open-source models often excel as creative
storytellers, producing imaginative yet less discriminative clues, whereas
larger proprietary models demonstrate superior overall performance,
particularly as listeners. Performance on DixitBench strongly correlates with
listener results in DixitArena, validating it as a reliable proxy for
hypothesis selection. Our findings reveal a key trade-off between generative
creativity and discriminative understanding in multimodal abductive reasoning,
a central challenge for developing more balanced and capable vision-language
agents.

</details>


### [111] [CharCom: Composable Identity Control for Multi-Character Story Illustration](https://arxiv.org/abs/2510.10135)
*Zhongsheng Wang,Ming Lin,Zhedong Lin,Yaser Shakib,Qian Liu,Jiamou Liu*

Main category: cs.AI

TL;DR: CharCom是一个模块化参数高效框架，通过可组合的LoRA适配器实现角色一致的故事插图生成，无需重新训练基础模型。


<details>
  <summary>Details</summary>
Motivation: 确保角色身份在不同提示下的一致性是基于扩散的文本到图像生成的基本限制。

Method: 基于冻结的扩散骨干网络，使用可组合的LoRA适配器，通过提示感知控制在推理时动态组合适配器。

Result: 在多场景叙事实验中，CharCom显著提升了角色保真度、语义对齐和时间一致性，在拥挤场景中保持鲁棒性，并能以最小开销实现可扩展的多角色生成。

Conclusion: CharCom适用于故事插图和动画等实际应用场景。

Abstract: Ensuring character identity consistency across varying prompts remains a
fundamental limitation in diffusion-based text-to-image generation. We propose
CharCom, a modular and parameter-efficient framework that achieves
character-consistent story illustration through composable LoRA adapters,
enabling efficient per-character customization without retraining the base
model. Built on a frozen diffusion backbone, CharCom dynamically composes
adapters at inference using prompt-aware control. Experiments on multi-scene
narratives demonstrate that CharCom significantly enhances character fidelity,
semantic alignment, and temporal coherence. It remains robust in crowded scenes
and enables scalable multi-character generation with minimal overhead, making
it well-suited for real-world applications such as story illustration and
animation.

</details>


### [112] [Concise Reasoning in the Lens of Lagrangian Optimization](https://arxiv.org/abs/2510.10168)
*Chengqian Gao,Haonan Li,Taylor W. Killian,Jianshu She,Renxi Wang,Liqun Ma,Zhoujun Cheng,Shibo Hao,Zhiqiang Xu*

Main category: cs.AI

TL;DR: 提出PALU方法，通过性能感知长度更新实现简洁推理，将推理长度最小化作为约束优化问题，在保持性能的同时减少65%输出长度并提升15%准确率。


<details>
  <summary>Details</summary>
Motivation: 现有简洁推理方法依赖人工设计的启发式规则，难以平衡简洁性与性能，且无法跨领域和模型规模适应。

Method: PALU将简洁推理建模为约束优化问题，使用拉格朗日优化转化为无约束问题，并通过三个近似实现实用化：离策略评估性能、截断拉格朗日乘子、使用分位数驱动长度调整。

Result: 在DeepSeek-Distill-Qwen-1.5B上，PALU平均减少65%输出长度同时提升15%准确率，在五个基准测试中优于其他方法，并能跨领域（逻辑、STEM、数学）和模型规模（1.5B-14B）适应。

Conclusion: PALU是一种实用有效的简洁推理方法，通过原则性算法和实用近似实现了性能与简洁性的平衡。

Abstract: Concise reasoning in large language models seeks to generate only essential
intermediate steps needed to arrive at a final answer, thereby alleviating
issues of overthinking. Most proposed approaches hinge on carefully
hand-crafted heuristics, struggling to balance concision with performance,
often failing to adapt across domains and model scales. In this work, we
address these challenges by introducing a principled and pragmatic strategy,
performance-aware length updating (PALU). As a principled algorithm, PALU
formulates concise reasoning as a constrained optimization problem, minimizing
response length subject to a performance constraint, and then applies
Lagrangian optimization to convert it into a tractable unconstrained problem.
As a pragmatic solution, PALU streamlines complicated update rules through
three approximations: (i) estimating performance with off-policy rollouts, (ii)
truncating the Lagrange multiplier to two extremes, and (iii) replacing
gradient-based updates with quantile-driven length adjustments. PALU reduces
output length by 65% while improving accuracy by 15% when applied to
DeepSeek-Distill-Qwen-1.5B, averaged over five benchmarks, outperforming a
range of alternative methods. Furthermore, PALU is demonstrated to adapt across
both domain (logic, STEM and math) and model scale (1.5B, 7B, 14B) entrenching
the algorithm as a practical and effective concise reasoning approach.

</details>


### [113] [SAFER: Risk-Constrained Sample-then-Filter in Large Language Models](https://arxiv.org/abs/2510.10193)
*Qingni Wang,Yue Fan,Xin Eric Wang*

Main category: cs.AI

TL;DR: SAFER是一个两阶段风险控制框架，通过弃权感知采样和校准过滤来解决开放域问答中缺乏固定解空间的问题，提供统计保证。


<details>
  <summary>Details</summary>
Motivation: 现有选择性符合预测方法假设所有实例的可接受答案都能通过有限采样获得，这在开放域问答中不现实，需要新的方法来确保LLM输出的可信度。

Method: 第一阶段使用Clopper-Pearson精确方法在校准集上校准采样预算；第二阶段应用符合风险控制方法确定统计有效的置信度阈值，过滤不可靠候选答案。

Result: SAFER能够控制正确答案被排除的风险，并与各种任务特定的准入标准和校准-测试分割比例兼容，具有鲁棒性和高数据效率。

Conclusion: SAFER为开放域问答提供了有效的风险控制框架，解决了现有方法在无限解空间场景下的局限性。

Abstract: As large language models (LLMs) are increasingly deployed in risk-sensitive
applications such as real-world open-ended question answering (QA), ensuring
the trustworthiness of their outputs has become critical. Existing selective
conformal prediction (SCP) methods provide statistical guarantees by
constructing prediction sets with a constrained miscoverage rate for correct
answers. However, prior works unrealistically assume that admissible answers
for all instances can be obtained via finite sampling, even for open-ended QA
scenarios that lack a fixed and finite solution space. To address this, we
introduce a two-stage risk control framework comprising abstention-aware
sampling and conformalized filtering (SAFER). Firstly, on a held-out
calibration set, SAFER calibrates a sampling budget within the maximum sampling
cap, using the Clopper-Pearson exact method at a user-desired risk level (i.e.,
the maximum allowable miscoverage rate of the sampling sets). If the risk level
cannot be satisfied within the cap, we abstain; otherwise, the calibrated
sampling budget becomes the minimum requirements at test time. Then, we employ
calibration instances where correct answers are attainable under the calibrated
budget and apply the conformal risk control method to determine a statistically
valid uncertainty threshold, which filters unreliable distractors from the
candidate set for each test data point. In this stage, SAFER introduces an
additional risk level to guide the calculation of the threshold, thereby
controlling the risk of correct answers being excluded. Furthermore, we show
that SAFER is compatible with various task-specific admission criteria and
calibration-test split ratios, highlighting its robustness and high data
efficiency.

</details>


### [114] [Don't Just Fine-tune the Agent, Tune the Environment](https://arxiv.org/abs/2510.10197)
*Siyuan Lu,Zechuan Wang,Hongxuan Zhang,Qintong Wu,Leilei Gan,Chenyi Zhuang,Jinjie Gu,Tao Lin*

Main category: cs.AI

TL;DR: 提出Environment Tuning训练范式，通过结构化课程、环境增强和细粒度奖励，使LLM智能体直接从问题实例中学习复杂行为，无需专家轨迹数据。


<details>
  <summary>Details</summary>
Motivation: 解决LLM智能体在复杂多轮工具使用任务中面临的高质量训练数据稀缺问题，以及SFT方法容易过拟合、标准RL方法存在冷启动和训练不稳定性的挑战。

Method: Environment Tuning训练范式，包含结构化课程编排、提供纠正反馈的环境增强、确保稳定高效探索的细粒度进度奖励。

Result: 仅使用400个BFCL基准问题实例，不仅实现了与强基线相当的分布内性能，还表现出优越的分布外泛化能力，克服了SFT方法的性能崩溃问题。

Conclusion: 从基于静态轨迹的监督微调转向动态、基于环境的探索，为训练更鲁棒和数据高效的智能体开辟了新途径。

Abstract: Large Language Model (LLM) agents show great promise for complex, multi-turn
tool-use tasks, but their development is often hampered by the extreme scarcity
of high-quality training data. Supervised fine-tuning (SFT) on synthetic data
leads to overfitting, whereas standard reinforcement learning (RL) struggles
with a critical cold-start problem and training instability. To address these
challenges, we introduce $\textbf{Environment Tuning}$, a novel training
paradigm that enables agents to learn complex behaviors directly from problem
instances without relying on pre-collected expert trajectories.
$\textbf{Environment Tuning}$ orchestrates this learning process through a
structured curriculum, actionable environment augmentation that provides
corrective feedback, and fine-grained progress rewards to ensure stable and
efficient exploration. Using only 400 problem instances from Berkeley
Function-Calling Leaderboard (BFCL) benchmark, our method not only achieves
competitive in-distribution performance against strong baselines but also
demonstrates superior out-of-distribution generalization, overcoming the
performance collapse common to SFT-based approaches. Our work presents a
paradigm shift from supervised fine-tuning on static trajectories to dynamic,
environment-based exploration, paving the way for training more robust and
data-efficient agents.

</details>


### [115] [PIXEL: Adaptive Steering Via Position-wise Injection with eXact Estimated Levels under Subspace Calibration](https://arxiv.org/abs/2510.10205)
*Manjiang Yu,Hongji Li,Priyanka Singh,Xue Li,Di Wang,Lijie Hu*

Main category: cs.AI

TL;DR: PIXEL是一个位置感知的激活引导框架，通过双视图学习属性对齐子空间，自适应选择干预强度，无需全局超参数调优，实现LLM的可控生成。


<details>
  <summary>Details</summary>
Motivation: 现有激活引导方法依赖粗糙启发式，缺乏对引导位置和干预强度的原则性考虑，需要更可靠的LLM行为控制方法。

Method: 使用双视图（尾平均和末端token）学习属性对齐子空间，通过约束几何目标选择干预强度，进行样本级正交残差校准和轻量级位置扫描。

Result: 在各种模型和评估范式下，PIXEL持续改进属性对齐，同时保持模型通用能力。

Conclusion: PIXEL为LLM的可控生成提供了实用且原则性的方法，支持可靠的对齐。

Abstract: Reliable behavior control is central to deploying large language models
(LLMs) on the web. Activation steering offers a tuning-free route to align
attributes (e.g., truthfulness) that ensure trustworthy generation. Prevailing
approaches rely on coarse heuristics and lack a principled account of where to
steer and how strongly to intervene. To this end, we propose Position-wise
Injection with eXact Estimated Levels (PIXEL), a position-wise activation
steering framework that, in contrast to prior work, learns a property-aligned
subspace from dual views (tail-averaged and end-token) and selects intervention
strength via a constrained geometric objective with a closed-form solution,
thereby adapting to token-level sensitivity without global hyperparameter
tuning. PIXEL further performs sample-level orthogonal residual calibration to
refine the global attribute direction and employs a lightweight
position-scanning routine to identify receptive injection sites. We
additionally provide representation-level guarantees for the
minimal-intervention rule, supporting reliable alignment. Across diverse models
and evaluation paradigms, PIXEL consistently improves attribute alignment while
preserving model general capabilities, offering a practical and principled
method for LLMs' controllable generation. Our code is available at
https://github.com/V1centNevwake/PIXEL-Adaptive-Steering

</details>


### [116] [Adaptive Dual Reasoner: Large Reasoning Models Can Think Efficiently by Hybrid Reasoning](https://arxiv.org/abs/2510.10207)
*Yujian Zhang,Keyu Chen,Zhifeng Shen,Ruizhi Qiao,Xing Sun*

Main category: cs.AI

TL;DR: 提出了自适应双推理器(ADR)，通过快速思维和慢速思维两种推理模式的动态切换，在保持高性能的同时显著降低计算成本和推理延迟。


<details>
  <summary>Details</summary>
Motivation: 长推理模型在推理过程中存在过度思考问题，导致计算成本增加和推理延迟上升，需要平衡推理性能与效率。

Method: 采用两阶段训练：1) 监督微调阶段构建混合推理数据集，使模型具备整合快速和慢速推理的能力；2) 强化学习阶段引入熵引导混合策略优化(EHPO)，在高熵单元进行分支并应用难度感知惩罚来平衡推理模式。

Result: 在数学推理基准测试中，ADR实现了6.1%的性能提升，同时将推理输出长度减少了49.5%到59.3%。

Conclusion: ADR在推理性能和效率之间实现了有效平衡，是解决长推理模型过度思考问题的有效方法。

Abstract: Although Long Reasoning Models (LRMs) have achieved superior performance on
various reasoning scenarios, they often suffer from increased computational
costs and inference latency caused by overthinking. To address these
limitations, we propose Adaptive Dual Reasoner, which supports two reasoning
modes: fast thinking and slow thinking. ADR dynamically alternates between
these modes based on the contextual complexity during reasoning. ADR is trained
in two stages: (1) A cold-start stage using supervised fine-tuning (SFT) to
equip the model with the ability to integrate both fast and slow reasoning
modes, in which we construct a hybrid reasoning dataset through a dedicated
pipeline to provide large-scale supervision. (2) A reinforcement learning stage
for optimizing reasoning effort, where we introduce Entropy-guided Hybrid
Policy Optimization EHPO, an RL training framework employing an entropy-guided
dynamic rollout strategy for branching at high-entropy units and a
difficulty-aware penalty to balance fast and slow reasoning. Across challenging
mathematical reasoning benchmarks, ADR achieves an effective balance between
reasoning performance and efficiency among state-of-the-art approaches.
Specifically, ADR yields a performance gain of up to 6.1%, while reducing the
reasoning output length by 49.5% to 59.3%.

</details>


### [117] [The Achilles' Heel of LLMs: How Altering a Handful of Neurons Can Cripple Language Abilities](https://arxiv.org/abs/2510.10238)
*Zixuan Qin,Kunlin Lyu,Qingchen Yu,Yifan Sun,Zhaoxin Fan*

Main category: cs.AI

TL;DR: 研究发现大型语言模型存在超稀疏的关键神经元集合，这些神经元集中在模型外层，破坏它们会导致模型性能急剧下降而非渐进恶化。


<details>
  <summary>Details</summary>
Motivation: 受人类大脑中关键神经元对认知功能重要性的启发，探究LLMs是否也存在类似的关键神经元子集。

Method: 提出基于扰动的关键神经元因果识别方法，系统定位LLMs中的关键神经元。

Result: 发现：(1)LLMs包含超稀疏关键神经元，破坏72B参数模型的关键神经元可使困惑度增加20个数量级；(2)关键神经元集中分布在外层MLP down_proj组件；(3)性能退化呈现急剧相变而非渐进下降。

Conclusion: 这些发现为开发更鲁棒的模型架构和提高安全关键应用的部署安全性提供了指导。

Abstract: Large Language Models (LLMs) have become foundational tools in natural
language processing, powering a wide range of applications and research. Many
studies have shown that LLMs share significant similarities with the human
brain. Recent neuroscience research has found that a small subset of biological
neurons in the human brain are crucial for core cognitive functions, which
raises a fundamental question: do LLMs also contain a small subset of critical
neurons? In this paper, we investigate this question by proposing a
Perturbation-based Causal Identification of Critical Neurons method to
systematically locate such critical neurons in LLMs. Our findings reveal three
key insights: (1) LLMs contain ultra-sparse critical neuron sets. Disrupting
these critical neurons can cause a 72B-parameter model with over 1.1 billion
neurons to completely collapse, with perplexity increasing by up to 20 orders
of magnitude; (2) These critical neurons are not uniformly distributed, but
tend to concentrate in the outer layers, particularly within the MLP down\_proj
components; (3) Performance degradation exhibits sharp phase transitions,
rather than a gradual decline, when these critical neurons are disrupted.
Through comprehensive experiments across diverse model architectures and
scales, we provide deeper analysis of these phenomena and their implications
for LLM robustness and interpretability. These findings can offer guidance for
developing more robust model architectures and improving deployment security in
safety-critical applications.

</details>


### [118] [Mitigating Hallucination in Multimodal Reasoning via Functional Attention Control](https://arxiv.org/abs/2510.10285)
*Haolang Lu,Bolun Chu,WeiYe Fu,Guoshun Nan,Junning Liu,Minghui Pan,Qiankun Li,Yi Yu,Hua Wang,Kun Wang*

Main category: cs.AI

TL;DR: 本文提出了一种轻量级的两步插件方法，通过识别感知导向和推理导向的注意力头并进行类别条件缩放，有效减少多模态大推理模型的幻觉问题，在多个基准测试中平均提升5%的性能，最高可达15%。


<details>
  <summary>Details</summary>
Motivation: 多模态大推理模型在视觉语言推理方面发展迅速，但幻觉问题仍然是一个持续存在的故障模式，表现为错误的推理链和视觉内容的误解。研究发现注意力头存在阶段性分工：浅层头主要负责感知，深层头转向符号推理，这揭示了幻觉的两个主要原因：感知偏差和推理漂移。

Method: 提出轻量级且可解释的两步插件：功能头识别和类别条件缩放。该方法定位感知导向和推理导向的注意力头，并在不重新训练的情况下调节它们的贡献。

Result: 在三个真实世界的MLRM模型（Kimi-VL、Ocean-R1、R1-Onevision）、六个跨三个领域的基准测试和四个基线方法上的评估显示，该插件平均提升5%，最高可达15%的性能，仅增加<1%的计算开销和9%的基线延迟。

Conclusion: 该方法完全模型无关，显著增强了现成MLRM的可靠性和可解释性，使其能够安全部署在高风险应用中。

Abstract: Multimodal large reasoning models (MLRMs) are rapidly advancing
vision-language reasoning and are emerging as a foundation for cross-modal
intelligence. Hallucination remains a persistent failure mode, manifesting
itself as erroneous reasoning chains and misinterpretation of visual content.
In this study, we observe that attention heads exhibit a staged division:
shallow heads predominantly serve perception, while deeper heads shift toward
symbolic reasoning, revealing two major causes of hallucination, namely
perceptual bias and reasoning drift. To address these issues, we propose a
lightweight and interpretable two-step plugin, Functional Head Identification
and Class-conditioned Rescaling, which locates perception- and
reasoning-oriented heads and regulates their contributions without retraining.
Evaluations on three real-world MLRMs (Kimi-VL, Ocean-R1, R1-Onevision), six
benchmarks across three domains, and four baselines show that our plugin
achieves an average improvement of 5% and up to 15%, with only <1% additional
computation and 9% of baseline latency. Our approach is completely
model-agnostic and significantly enhances both the reliability and
interpretability of the off-the-shelf MLRMs, thereby enabling their safe
deployment in high-stakes applications. Our code is available at
https://anonymous.4open.science/r/Functional-Attention-Control.

</details>


### [119] [LLM-Friendly Knowledge Representation for Customer Support](https://arxiv.org/abs/2510.10331)
*Hanchen Su,Wei Luo,Wei Han,Yu Elaine Liu,Yufeng Wayne Zhang,Cen Mia Zhao,Ying Joy Zhang,Yashar Mehdad*

Main category: cs.AI

TL;DR: 提出了一种将LLM与Airbnb客服操作框架结合的实用方法，使用ICA格式重构工作流程，并通过合成数据生成策略低成本微调模型，显著提升了客服性能。


<details>
  <summary>Details</summary>
Motivation: 解决Airbnb客服操作的复杂性，通过LLM提升客服效率和质量，同时降低人工干预成本。

Method: 采用ICA（意图、上下文、动作）格式重构策略和工作流程，开发合成数据生成策略进行低成本模型微调。

Result: 内部实验显示，重构工作流程和合成数据微调显著提升了LLM性能，在准确性和处理时间指标上都有改善。

Conclusion: 该方法为LLM在客服领域的应用设立了新基准，既经济高效又提升了客服质量。

Abstract: We propose a practical approach by integrating Large Language Models (LLMs)
with a framework designed to navigate the complexities of Airbnb customer
support operations. In this paper, our methodology employs a novel reformatting
technique, the Intent, Context, and Action (ICA) format, which transforms
policies and workflows into a structure more comprehensible to LLMs.
Additionally, we develop a synthetic data generation strategy to create
training data with minimal human intervention, enabling cost-effective
fine-tuning of our model. Our internal experiments (not applied to Airbnb
products) demonstrate that our approach of restructuring workflows and
fine-tuning LLMs with synthetic data significantly enhances their performance,
setting a new benchmark for their application in customer support. Our solution
is not only cost-effective but also improves customer support, as evidenced by
both accuracy and manual processing time evaluation metrics.

</details>


### [120] [Beyond Ethics: How Inclusive Innovation Drives Economic Returns in Medical AI](https://arxiv.org/abs/2510.10338)
*Balagopal Unnikrishnan,Ariel Guerra Adames,Amin Adibi,Sameer Peesapati,Rafal Kocielnik,Shira Fischer,Hillary Clinton Kasimbazi,Rodrigo Gameiro,Alina Peluso,Chrystinne Oliveira Fernandes,Maximin Lange,Lovedeep Gondara,Leo Anthony Celi*

Main category: cs.AI

TL;DR: 论文提出"包容性创新红利"概念，证明为多样化、受限使用场景设计的医疗AI解决方案能在更广泛市场产生更优经济回报，并开发了HAIIF评估框架。


<details>
  <summary>Details</summary>
Motivation: 虽然医疗AI公平性的伦理论证已很充分，但包容性设计的经济和战略价值仍未充分探索。

Method: 通过分析辅助技术演变为主流产业的案例，识别包容性创新驱动回报的四个机制，并开发了医疗AI包容性创新框架(HAIIF)评分系统。

Result: 包容性医疗AI开发创造了超越合规要求的商业价值，包括市场扩张、风险缓解、性能红利和竞争优势。

Conclusion: 渐进投资包容性设计的组织能获得扩大市场覆盖和持续竞争优势，而将其视为成本的组织将面临日益加剧的劣势。

Abstract: While ethical arguments for fairness in healthcare AI are well-established,
the economic and strategic value of inclusive design remains underexplored.
This perspective introduces the ``inclusive innovation dividend'' -- the
counterintuitive principle that solutions engineered for diverse, constrained
use cases generate superior economic returns in broader markets. Drawing from
assistive technologies that evolved into billion-dollar mainstream industries,
we demonstrate how inclusive healthcare AI development creates business value
beyond compliance requirements. We identify four mechanisms through which
inclusive innovation drives returns: (1) market expansion via geographic
scalability and trust acceleration; (2) risk mitigation through reduced
remediation costs and litigation exposure; (3) performance dividends from
superior generalization and reduced technical debt, and (4) competitive
advantages in talent acquisition and clinical adoption. We present the
Healthcare AI Inclusive Innovation Framework (HAIIF), a practical scoring
system that enables organizations to evaluate AI investments based on their
potential to capture these benefits. HAIIF provides structured guidance for
resource allocation, transforming fairness and inclusivity from regulatory
checkboxes into sources of strategic differentiation. Our findings suggest that
organizations investing incrementally in inclusive design can achieve expanded
market reach and sustained competitive advantages, while those treating these
considerations as overhead face compounding disadvantages as network effects
and data advantages accrue to early movers.

</details>


### [121] [Trace Length is a Simple Uncertainty Signal in Reasoning Models](https://arxiv.org/abs/2510.10409)
*Siddartha Devic,Charlotte Peale,Arwen Bradley,Sinead Williamson,Preetum Nakkiran,Aravind Gollakota*

Main category: cs.AI

TL;DR: 推理轨迹长度是大推理模型中简单有效的置信度估计器，与零样本置信度估计方法表现相当但互补，推理后训练从根本上改变了轨迹长度与准确性的关系。


<details>
  <summary>Details</summary>
Motivation: 解决LLM幻觉问题，提高其可靠部署能力，需要有效的置信度量化方法。

Method: 通过多模型、多数据集和多提示的综合实验，分析推理轨迹长度作为置信度估计器的性能，并研究其机制。

Result: 轨迹长度与其他零样本置信度估计器表现相当但互补，推理后训练增强了不确定性量化能力，高熵或"分叉"标记在机制中起关键作用。

Conclusion: 推理轨迹长度是大推理模型中实用的置信度度量标准，推理后训练超越了语言表达的局限性，提升了不确定性量化能力。

Abstract: Uncertainty quantification for LLMs is a key research direction towards
addressing hallucination and other issues that limit their reliable deployment.
In this work, we show that reasoning trace length is a simple and useful
confidence estimator in large reasoning models. Through comprehensive
experiments across multiple models, datasets, and prompts, we show that trace
length performs in comparable but complementary ways to other zero-shot
confidence estimators such as verbalized confidence. Our work reveals that
reasoning post-training fundamentally alters the relationship between trace
length and accuracy, going beyond prior work that had shown that post-training
causes traces to grow longer in general (e.g., "overthinking"). We investigate
the mechanisms behind trace length's performance as a confidence signal,
observing that the effect remains even after adjusting for confounders such as
problem difficulty and GRPO-induced length bias. We identify high-entropy or
"forking" tokens as playing a key role in the mechanism. Our findings
demonstrate that reasoning post-training enhances uncertainty quantification
beyond verbal expressions, and establish trace length as a practical confidence
measure for large reasoning models.

</details>


### [122] [Traj-CoA: Patient Trajectory Modeling via Chain-of-Agents for Lung Cancer Risk Prediction](https://arxiv.org/abs/2510.10454)
*Sihang Zeng,Yujuan Fu,Sitong Zhou,Zixuan Yu,Lucas Jing Liu,Jun Wen,Matthew Thompson,Ruth Etzioni,Meliha Yetisgen*

Main category: cs.AI

TL;DR: Traj-CoA是一个多代理系统，通过链式代理处理电子健康记录数据，使用EHRMem长期记忆模块减少噪声并保留完整时间线，在零样本肺癌风险预测任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在处理电子健康记录时面临数据冗长和噪声问题，特别是在时序推理方面存在挑战。

Method: 采用多代理系统，包括工作代理链顺序处理EHR数据块，将关键事件提炼到共享的EHRMem长期记忆模块，最后由管理代理综合信息进行预测。

Result: 在基于五年EHR数据的零样本一年肺癌风险预测任务中，Traj-CoA优于四类基线方法。

Conclusion: Traj-CoA展现出临床对齐的时序推理能力，是建模复杂患者轨迹的稳健且可推广的方法。

Abstract: Large language models (LLMs) offer a generalizable approach for modeling
patient trajectories, but suffer from the long and noisy nature of electronic
health records (EHR) data in temporal reasoning. To address these challenges,
we introduce Traj-CoA, a multi-agent system involving chain-of-agents for
patient trajectory modeling. Traj-CoA employs a chain of worker agents to
process EHR data in manageable chunks sequentially, distilling critical events
into a shared long-term memory module, EHRMem, to reduce noise and preserve a
comprehensive timeline. A final manager agent synthesizes the worker agents'
summary and the extracted timeline in EHRMem to make predictions. In a
zero-shot one-year lung cancer risk prediction task based on five-year EHR
data, Traj-CoA outperforms baselines of four categories. Analysis reveals that
Traj-CoA exhibits clinically aligned temporal reasoning, establishing it as a
promisingly robust and generalizable approach for modeling complex patient
trajectories.

</details>


### [123] [MedCoAct: Confidence-Aware Multi-Agent Collaboration for Complete Clinical Decision](https://arxiv.org/abs/2510.10461)
*Hongjie Zheng,Zesheng Shi,Ping Yi*

Main category: cs.AI

TL;DR: 提出MedCoAct多智能体框架，通过医生和药剂师智能体的协作，解决医疗AI在整合诊断和治疗工作流中的局限性，相比单智能体框架在诊断和用药推荐准确率上分别提升7.04%和7.08%。


<details>
  <summary>Details</summary>
Motivation: 现有医疗AI系统在孤立任务中表现出色，但在整合临床工作流中表现不佳，缺乏临床团队中的交叉验证和知识整合能力。

Method: 提出MedCoAct置信感知多智能体框架，整合专业医生和药剂师智能体，模拟临床协作，并创建DrugCareQA基准来评估医疗AI在整合诊断和治疗工作流中的能力。

Result: MedCoAct在诊断准确率和用药推荐准确率上均达到67.58%，相比单智能体框架分别提升7.04%和7.08%，在远程医疗和常规临床场景中表现优异。

Conclusion: 协作方法在不同医疗领域泛化良好，特别适用于远程医疗咨询和常规临床场景，同时提供可解释的决策路径。

Abstract: Autonomous agents utilizing Large Language Models (LLMs) have demonstrated
remarkable capabilities in isolated medical tasks like diagnosis and image
analysis, but struggle with integrated clinical workflows that connect
diagnostic reasoning and medication decisions. We identify a core limitation:
existing medical AI systems process tasks in isolation without the
cross-validation and knowledge integration found in clinical teams, reducing
their effectiveness in real-world healthcare scenarios. To transform the
isolation paradigm into a collaborative approach, we propose MedCoAct, a
confidence-aware multi-agent framework that simulates clinical collaboration by
integrating specialized doctor and pharmacist agents, and present a benchmark,
DrugCareQA, to evaluate medical AI capabilities in integrated diagnosis and
treatment workflows. Our results demonstrate that MedCoAct achieves 67.58\%
diagnostic accuracy and 67.58\% medication recommendation accuracy,
outperforming single agent framework by 7.04\% and 7.08\% respectively. This
collaborative approach generalizes well across diverse medical domains, proving
especially effective for telemedicine consultations and routine clinical
scenarios, while providing interpretable decision-making pathways.

</details>


### [124] [Tracing the Traces: Latent Temporal Signals for Efficient and Accurate Reasoning](https://arxiv.org/abs/2510.10494)
*Martina G. Vilas,Safoora Yousefi,Besmira Nushi,Eric Horvitz,Vidhisha Balachandran*

Main category: cs.AI

TL;DR: 提出Latent-Trajectory信号来预测推理过程是否成功，通过分析模型内部表征的时间演化，显著提高推理效率，减少70%计算量同时提升准确率。


<details>
  <summary>Details</summary>
Motivation: 推理模型通过增加计算资源来提高问题解决能力，但识别哪些推理路径可能成功是关键机会，可靠预测有效路径可以大幅减少浪费的计算并提高整体效率。

Method: 引入Latent-Trajectory信号，通过测量推理开始和结束时内部表征的总体变化、中间步骤累积的变化以及这些变化向最终状态推进的程度来预测推理准确性。

Result: Latent-Trajectory信号比跨层指标和基于输出的置信度测量更可靠地预测解决方案准确性，在多个生成样本中指导答案选择时，比多数投票更有效，减少70%令牌使用同时平均提高2.6%准确率。

Conclusion: 这些发现不仅为推理时效率提供了实用策略，还从可解释性角度深入揭示了推理过程在潜在空间中的表示和区分方式。

Abstract: Reasoning models improve their problem-solving ability through inference-time
scaling, allocating more compute via longer token budgets. Identifying which
reasoning traces are likely to succeed remains a key opportunity: reliably
predicting productive paths can substantially reduce wasted computation and
improve overall efficiency. We introduce Latent-Trajectory signals that
characterize the temporal evolution of a model's internal representations
during the generation of intermediate reasoning tokens. By measuring the
overall change in latent representations between the start and end of
reasoning, the change accumulated across intermediate steps, and the extent to
which these changes advance toward the final state, we show that these signals
predict solution accuracy more reliably than both cross-layer metrics and
output-based confidence measures. When used to guide answer selection across
multiple sampled generations, Latent-Trajectory signals make test-time scaling
more effective and efficient than majority voting, reducing token usage by up
to 70% while preserving and even improving accuracy by 2.6% on average.
Moreover, these predictive signals often emerge early in the reasoning trace,
enabling early selection and allocation of compute to the most promising
candidates. Our findings contribute not only practical strategies for
inference-time efficiency, but also a deeper interpretability perspective on
how reasoning processes are represented and differentiated in latent space.

</details>


### [125] [ELAIPBench: A Benchmark for Expert-Level Artificial Intelligence Paper Understanding](https://arxiv.org/abs/2510.10549)
*Xinbang Dai,Huikang Hu,Yongrui Chen,Jiaqi Li,Rihui Jin,Yuyang Zhang,Xiaoguang Li,Lifeng Shang,Guilin Qi*

Main category: cs.AI

TL;DR: ELAIPBench是一个由领域专家构建的基准测试，用于评估LLMs对AI研究论文的理解能力，包含403个多选题，实验显示最佳LLM准确率仅39.95%，远低于人类水平。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试在评估LLMs对学术论文的深度理解能力方面存在不足，要么问题设计肤浅，要么评估指标不可靠。

Method: 通过激励驱动的对抗性标注过程开发ELAIPBench基准，包含137篇论文的403个多选题，涵盖三个难度级别，强调非平凡推理而非浅层检索。

Result: 最佳性能的LLM准确率仅为39.95%，配备思考模式或RAG系统的前沿LLM未能改善结果，甚至因过度思考或噪声检索而降低准确率。

Conclusion: 当前LLM能力与真正理解学术论文之间存在显著差距。

Abstract: While large language models (LLMs) excel at many domain-specific tasks, their
ability to deeply comprehend and reason about full-length academic papers
remains underexplored. Existing benchmarks often fall short of capturing such
depth, either due to surface-level question design or unreliable evaluation
metrics. To address this gap, we introduce ELAIPBench, a benchmark curated by
domain experts to evaluate LLMs' comprehension of artificial intelligence (AI)
research papers. Developed through an incentive-driven, adversarial annotation
process, ELAIPBench features 403 multiple-choice questions from 137 papers. It
spans three difficulty levels and emphasizes non-trivial reasoning rather than
shallow retrieval. Our experiments show that the best-performing LLM achieves
an accuracy of only 39.95%, far below human performance. Moreover, we observe
that frontier LLMs equipped with a thinking mode or a retrieval-augmented
generation (RAG) system fail to improve final results-even harming accuracy due
to overthinking or noisy retrieval. These findings underscore the significant
gap between current LLM capabilities and genuine comprehension of academic
papers.

</details>


### [126] [A Layered Intuition -- Method Model with Scope Extension for LLM Reasoning](https://arxiv.org/abs/2510.10592)
*Hong Su*

Main category: cs.AI

TL;DR: 本文提出直觉-方法分层模型与范围扩展的统一框架，通过直觉思考提供快速反应，方法思考将问题解耦为可转移推理单元，并首次引入时空扩展来增强LLM对未见问题的解决能力。


<details>
  <summary>Details</summary>
Motivation: 现有研究已引入基于方法的推理和范围扩展来提升LLM性能，但缺乏系统整合。本文旨在将这些思想统一为更系统的框架，以更好地解决间接（未见）问题。

Method: 构建直觉-方法分层模型：直觉层提供快速答案，方法层将问题解耦为可转移推理单元。引入垂直、水平、时空扩展，并组织成知识树网络。提出方法扩展熵来量化评估。

Result: 通过逻辑连接现有方法与新扩展，并引入基于熵的评估框架，为LLM在实际问题解决中推进了更稳健和可扩展的推理范式。

Conclusion: 该工作通过统一框架和定量评估方法，显著提升了LLM对未见问题的适应性和解决能力，为现实世界问题解决提供了更强大的推理范式。

Abstract: Existing studies have introduced method-based reasoning and scope extension
as approaches to enhance Large Language Model (LLM) performance beyond direct
matrix mappings. Building on these foundations, this paper summarizes and
integrates these ideas into a unified Intuition-Method Layered Model with Scope
Extension, designed to address indirected (unseen) issues more systematically.
In this framework, intuition-based thinking provides rapid first-reaction
answers, while method-based thinking decouples questions and solutions into
transferable reasoning units. Scope extension is then applied to broaden
applicability, including vertical (cause analysis), horizontal (parallel and
generalized issues), and for the first time, temporal and spatial extensions,
which expand reasoning across time and contextual dimensions. These extensions
are organized into systematic knowledge trees that interconnect into a
knowledge network, thereby increasing adaptability. To quantitatively evaluate
this process, we propose the entropy of method extension, which measures the
independence and diversity of extensions as an indicator of the system's
capacity to solve unseen questions. By logically connecting existing approaches
with new extensions and introducing an entropy-based evaluation framework, this
work advances toward a more robust and extensible reasoning paradigm for LLMs
in real-world problem-solving.

</details>


### [127] [A Distance Measure for Random Permutation Set: From the Layer-2 Belief Structure Perspective](https://arxiv.org/abs/2510.10596)
*Ruolan Cheng,Yong Deng,Serafín Moral,José Ramón Trillo*

Main category: cs.AI

TL;DR: 本文从随机有限集和可转移信念模型两个角度深入分析了随机置换集之间的距离，提出了一种基于累积Jaccard指数矩阵的RPS距离度量方法，该方法具有自然的上权重特性，并提供了参数调整机制。


<details>
  <summary>Details</summary>
Motivation: 随机置换集是表示顺序结构不确定信息的新框架，测量置换质量函数之间的距离是RPS理论的关键研究课题。现有方法存在不足，需要开发更敏感和灵活的距离度量方法。

Method: 采用RPS的层-2信念结构解释，将RPST视为TBM的细化。从置换出发引入累积Jaccard指数的新定义来量化两个置换之间的相似性，并基于累积Jaccard指数矩阵提出RPS距离度量方法。

Result: 提出的方法克服了现有方法的缺点，与Jousselme距离兼容，具有更高的敏感性和灵活性。实验结果表明该方法在数值示例中表现优越。

Conclusion: 基于累积Jaccard指数矩阵的RPS距离度量方法是一个有效且灵活的工具，能够更好地处理顺序结构不确定信息，为决策者提供了可调整的参数机制。

Abstract: Random permutation set (RPS) is a recently proposed framework designed to
represent order-structured uncertain information. Measuring the distance
between permutation mass functions is a key research topic in RPS theory
(RPST). This paper conducts an in-depth analysis of distances between RPSs from
two different perspectives: random finite set (RFS) and transferable belief
model (TBM). Adopting the layer-2 belief structure interpretation of RPS, we
regard RPST as a refinement of TBM, where the order in the ordered focus set
represents qualitative propensity. Starting from the permutation, we introduce
a new definition of the cumulative Jaccard index to quantify the similarity
between two permutations and further propose a distance measure method for RPSs
based on the cumulative Jaccard index matrix. The metric and structural
properties of the proposed distance measure are investigated, including the
positive definiteness analysis of the cumulative Jaccard index matrix, and a
correction scheme is provided. The proposed method has a natural
top-weightiness property: inconsistencies between higher-ranked elements tend
to result in greater distance values. Two parameters are provided to the
decision-maker to adjust the weight and truncation depth. Several numerical
examples are used to compare the proposed method with the existing method. The
experimental results show that the proposed method not only overcomes the
shortcomings of the existing method and is compatible with the Jousselme
distance, but also has higher sensitivity and flexibility.

</details>


### [128] [EA4LLM: A Gradient-Free Approach to Large Language Model Optimization via Evolutionary Algorithms](https://arxiv.org/abs/2510.10603)
*WenTao Liu,Siyu Song,Hao Hao,Aimin Zhou*

Main category: cs.AI

TL;DR: 提出了一种使用进化算法优化大语言模型的方法(EA4LLM)，首次成功训练了10亿参数的LLM，挑战了基于梯度优化是训练神经网络唯一可行方法的普遍假设。


<details>
  <summary>Details</summary>
Motivation: 解决基于梯度优化方法对硬件要求严格、需要高并发高内存GPU，以及要求所有神经网络操作可微分的问题，使许多有前景的不可微分架构能够实用化。

Method: 使用进化算法(EA4LLM)优化大语言模型，从预训练阶段开始训练10亿参数模型。

Result: 成功训练了10亿参数的LLM，并通过大量实验提供了进化算法如何有效优化神经网络的关键见解。

Conclusion: 该方法有显著潜力降低大语言模型训练的计算成本，使计算资源有限的群体能够参与深度学习研究，挑战了梯度优化是唯一可行方法的假设。

Abstract: In recent years, large language models (LLMs) have made remarkable progress,
with model optimization primarily relying on gradient-based optimizers such as
Adam. However, these gradient-based methods impose stringent hardware
requirements, demanding high-concurrency, high-memory GPUs. Moreover, they
require all neural network operations to be differentiable, thereby excluding
many promising non-differentiable architectures from practical use. To address
these limitations, we propose a method for optimizing LLMs using evolutionary
algorithms (EA4LLM) and, for the first time, successfully demonstrate its
capability to train a 1-billion-parameter LLM from the pre-trained stage. We
conduct extensive experiments and provide key insights into how evolutionary
algorithms can effectively optimize neural networks. Our work challenges the
prevailing assumption that gradient-based optimization is the only viable
approach for training neural networks. It also holds significant potential to
reduce the computational cost of training large language models, thereby
enabling groups with limited computational resources to participate in deep
learning research.

</details>


### [129] [Collaborative Text-to-Image Generation via Multi-Agent Reinforcement Learning and Semantic Fusion](https://arxiv.org/abs/2510.10633)
*Jiabao Shi,Minfeng Qi,Lefeng Zhang,Di Wang,Yingjie Zhao,Ziying Li,Yalong Xing,Ningran Li*

Main category: cs.AI

TL;DR: 提出了一个多智能体强化学习框架，通过协调领域专业化的智能体来改进多模态文本到图像生成，在语义对齐和细节保持方面取得显著提升。


<details>
  <summary>Details</summary>
Motivation: 多模态文本到图像生成面临语义对齐和专业级细节保持的挑战，需要更有效的跨模态协调机制。

Method: 使用多智能体强化学习框架，包含文本增强和图像生成两个耦合子系统，采用PPO算法和复合奖励函数，结合对比学习、双向注意力和迭代反馈。

Result: 在六个实验设置中，生成内容显著丰富（词数增加1614%），ROUGE-1分数降低69.7%，基于Transformer的融合策略获得最高综合得分（0.521）。

Conclusion: 协作式、专业化驱动的架构在推进可靠多模态生成系统方面具有前景，但跨模态语义基础仍存在持续挑战。

Abstract: Multimodal text-to-image generation remains constrained by the difficulty of
maintaining semantic alignment and professional-level detail across diverse
visual domains. We propose a multi-agent reinforcement learning framework that
coordinates domain-specialized agents (e.g., focused on architecture,
portraiture, and landscape imagery) within two coupled subsystems: a text
enhancement module and an image generation module, each augmented with
multimodal integration components. Agents are trained using Proximal Policy
Optimization (PPO) under a composite reward function that balances semantic
similarity, linguistic visual quality, and content diversity. Cross-modal
alignment is enforced through contrastive learning, bidirectional attention,
and iterative feedback between text and image. Across six experimental
settings, our system significantly enriches generated content (word count
increased by 1614%) while reducing ROUGE-1 scores by 69.7%. Among fusion
methods, Transformer-based strategies achieve the highest composite score
(0.521), despite occasional stability issues. Multimodal ensembles yield
moderate consistency (ranging from 0.444 to 0.481), reflecting the persistent
challenges of cross-modal semantic grounding. These findings underscore the
promise of collaborative, specialization-driven architectures for advancing
reliable multimodal generative systems.

</details>


### [130] [Automatic Piecewise Linear Regression for Predicting Student Learning Satisfaction](https://arxiv.org/abs/2510.10639)
*Haemin Choi,Gayathri Nadarajan*

Main category: cs.AI

TL;DR: APLR模型在预测学习满意度方面表现最佳，发现时间管理、专注力、帮助同学和线下课程参与对学习满意度有显著正向影响，而创意活动参与无正面影响。


<details>
  <summary>Details</summary>
Motivation: 虽然学生满意度已被广泛研究，但可解释机器学习和神经网络等现代技术尚未充分探索。

Method: 使用结合提升方法和可解释性的自动分段线性回归(APLR)模型，并与多种先进方法比较。

Result: APLR模型拟合效果最好，通过数值和可视化解释发现时间管理、专注力、帮助同学和线下课程参与是最重要的正向因素。

Conclusion: APLR模型不仅能识别关键影响因素，还能在个体层面进行解释，使教育者能够根据学生特征定制教学方案。

Abstract: Although student learning satisfaction has been widely studied, modern
techniques such as interpretable machine learning and neural networks have not
been sufficiently explored. This study demonstrates that a recent model that
combines boosting with interpretability, automatic piecewise linear
regression(APLR), offers the best fit for predicting learning satisfaction
among several state-of-the-art approaches. Through the analysis of APLR's
numerical and visual interpretations, students' time management and
concentration abilities, perceived helpfulness to classmates, and participation
in offline courses have the most significant positive impact on learning
satisfaction. Surprisingly, involvement in creative activities did not
positively affect learning satisfaction. Moreover, the contributing factors can
be interpreted on an individual level, allowing educators to customize
instructions according to student profiles.

</details>


### [131] [Equity-Aware Geospatial AI for Forecasting Demand-Driven Hospital Locations in Germany](https://arxiv.org/abs/2510.10640)
*Piyush Pant,Marcellius William Suntoro,Ayesha Siddiqua,Muhammad Shehryaar Sharif,Daniyal Ahmed*

Main category: cs.AI

TL;DR: EA-GeoAI框架结合地理人工智能、长期预测和公平性测量，为德国2030年医院规划提供需求预测和公平分配方案


<details>
  <summary>Details</summary>
Motivation: 解决德国医院规划中的人口变化、老龄化密度和基础设施平衡问题，确保医疗资源分配的公平性

Method: 整合区域级人口变化、老龄化密度和基础设施平衡构建统一公平指数，使用可解释的智能AI优化器在预算和通勤时间约束下分配床位和确定新设施位置

Result: 开发了一个能够最小化未满足需求的可操作框架，为政策制定者提供具体建议

Conclusion: 该框架成功连接了地理AI、长期预测和公平性测量，为医疗资源规划提供了有效的决策支持工具

Abstract: This paper presents EA-GeoAI, an integrated framework for demand forecasting
and equitable hospital planning in Germany through 2030. We combine
district-level demographic shifts, aging population density, and infrastructure
balances into a unified Equity Index. An interpretable Agentic AI optimizer
then allocates beds and identifies new facility sites to minimize unmet need
under budget and travel-time constraints. This approach bridges GeoAI,
long-term forecasting, and equity measurement to deliver actionable
recommendations for policymakers.

</details>


### [132] [Hierarchical Optimization via LLM-Guided Objective Evolution for Mobility-on-Demand Systems](https://arxiv.org/abs/2510.10644)
*Yi Zhang,Yushen Long,Yun Ni,Liping Huang,Xiaohong Wang,Jun Liu*

Main category: cs.AI

TL;DR: 提出了一种将大语言模型与数学优化结合的混合框架，用于在线网约车平台的供需平衡问题，无需训练数据，通过LLM生成高层目标指导底层优化器，在纽约和芝加哥出租车数据集上相比现有方法平均提升16%性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法存在两类问题：强化学习方法数据效率低、对现实动态建模过于简化、难以实施操作约束；分解在线优化方法依赖人工设计的高层目标，缺乏对底层路由动态的认知。需要一种能结合两者优势的解决方案。

Method: 提出训练免费的混合层次框架，使用LLM作为元优化器生成语义启发式信息，指导负责约束执行和实时决策的底层优化器。通过基于和谐搜索的闭环进化过程，根据优化层的可行性和性能反馈迭代调整LLM提示。

Result: 在基于纽约和芝加哥出租车数据集的场景上进行广泛实验，证明该方法相比最先进的基线方法平均提升了16%的性能。

Conclusion: 该混合框架成功解决了现有方法的局限性，通过LLM与数学优化的结合，在无需大规模交互数据的情况下实现了更好的供需平衡效果。

Abstract: Online ride-hailing platforms aim to deliver efficient mobility-on-demand
services, often facing challenges in balancing dynamic and spatially
heterogeneous supply and demand. Existing methods typically fall into two
categories: reinforcement learning (RL) approaches, which suffer from data
inefficiency, oversimplified modeling of real-world dynamics, and difficulty
enforcing operational constraints; or decomposed online optimization methods,
which rely on manually designed high-level objectives that lack awareness of
low-level routing dynamics. To address this issue, we propose a novel hybrid
framework that integrates large language model (LLM) with mathematical
optimization in a dynamic hierarchical system: (1) it is training-free,
removing the need for large-scale interaction data as in RL, and (2) it
leverages LLM to bridge cognitive limitations caused by problem decomposition
by adaptively generating high-level objectives. Within this framework, LLM
serves as a meta-optimizer, producing semantic heuristics that guide a
low-level optimizer responsible for constraint enforcement and real-time
decision execution. These heuristics are refined through a closed-loop
evolutionary process, driven by harmony search, which iteratively adapts the
LLM prompts based on feasibility and performance feedback from the optimization
layer. Extensive experiments based on scenarios derived from both the New York
and Chicago taxi datasets demonstrate the effectiveness of our approach,
achieving an average improvement of 16% compared to state-of-the-art baselines.

</details>


### [133] [Unlocking Exploration in RLVR: Uncertainty-aware Advantage Shaping for Deeper Reasoning](https://arxiv.org/abs/2510.10649)
*Can Xie,Ruotong Pan,Xiangyu Wu,Yunfei Zhang,Jiayi Fu,Tingting Gao,Guorui Zhou*

Main category: cs.AI

TL;DR: 提出UCAS方法，通过利用模型内部不确定性信号来改进强化学习中的信用分配，解决现有方法在推理过程中忽视关键决策的问题。


<details>
  <summary>Details</summary>
Motivation: 现有RLVR算法如GRPO对所有token广播统一的优势信号，忽视了推理过程中不确定的高风险决策，导致探索效率低下和熵崩溃问题。

Method: UCAS采用两阶段机制：首先基于模型整体自信度调节响应级优势，然后基于原始logit确定性应用token级惩罚，鼓励探索高不确定性但正确的路径。

Result: 在五个数学推理基准测试中，UCAS显著优于强RLVR基线，在1.5B和7B等多个模型规模上表现优异，实现了更高奖励和更大推理多样性。

Conclusion: UCAS不仅获得更高奖励，还促进推理多样性并成功缓解熵崩溃问题，有效平衡了探索与利用的权衡。

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has shown significant
promise for enhancing the reasoning capabilities of large language models
(LLMs). However, prevailing algorithms like GRPO broadcast a uniform advantage
signal across all tokens in a sequence. This coarse-grained approach overlooks
the pivotal role of uncertain, high-stakes decisions during reasoning, leading
to inefficient exploration and the well-documented problem of entropy collapse.
To address this, we introduce UnCertainty-aware Advantage Shaping (UCAS), a
model-free method that refines credit assignment by leveraging the model's
internal uncertainty signals. UCAS operates in two stages: it first modulates
the response-level advantage using the model's overall self-confidence, and
then applies a token-level penalty based on raw logit certainty. This dual
mechanism encourages exploration of high-uncertainty paths that yield correct
answers while penalizing overconfident yet erroneous reasoning, effectively
balancing the exploration-exploitation trade-off. Extensive experiments on five
mathematical reasoning benchmarks show that UCAS significantly outperforms
strong RLVR baselines across multiple model scales, including 1.5B and 7B. Our
analysis confirms that UCAS not only achieves higher rewards but also promotes
greater reasoning diversity and successfully mitigates entropy collapse.

</details>


### [134] [Simpliflow: A Lightweight Open-Source Framework for Rapid Creation and Deployment of Generative Agentic AI Workflows](https://arxiv.org/abs/2510.10675)
*Deven Panchal*

Main category: cs.AI

TL;DR: simpliflow是一个轻量级开源Python框架，用于简化生成式智能AI系统的开发，通过声明式JSON配置快速构建确定性工作流


<details>
  <summary>Details</summary>
Motivation: 现有框架复杂、学习曲线陡峭、样板代码多，阻碍了快速原型开发和部署

Method: 采用模块化架构，解耦代理管理、工作流执行和后处理，通过LiteLLM支持100+大语言模型

Result: 展示了在软件开发模拟和实时系统交互等多样化用例中的实用性

Conclusion: 与LangChain和AutoGen相比，simpliflow在确定性工作流环境中具有简单性、控制性和速度优势

Abstract: Generative Agentic AI systems are emerging as a powerful paradigm for
automating complex, multi-step tasks. However, many existing frameworks for
building these systems introduce significant complexity, a steep learning
curve, and substantial boilerplate code, hindering rapid prototyping and
deployment. This paper introduces simpliflow, a lightweight, open-source Python
framework designed to address these challenges. simpliflow enables the rapid
development and orchestration of linear, deterministic agentic workflows
through a declarative, JSON-based configuration. Its modular architecture
decouples agent management, workflow execution, and post-processing, promoting
ease of use and extensibility. By integrating with LiteLLM, it supports over
100 Large Language Models (LLMs) out-of-the-box. We present the architecture,
operational flow, and core features of simpliflow, demonstrating its utility
through diverse use cases ranging from software development simulation to
real-time system interaction. A comparative analysis with prominent frameworks
like LangChain and AutoGen highlights simpliflow's unique position as a tool
optimized for simplicity, control, and speed in deterministic workflow
environments.

</details>


### [135] [OmniVideoBench: Towards Audio-Visual Understanding Evaluation for Omni MLLMs](https://arxiv.org/abs/2510.10689)
*Caorui Li,Yu Chen,Yiyan Ji,Jin Xu,Zhenyu Cui,Shihao Li,Yuanxing Zhang,Jiafu Tang,Zhenghao Song,Dingling Zhang,Ying He,Haoxiang Liu,Yuxuan Wang,Qiufeng Wang,Zhenhe Wu,Jiehui Luo,Zhiyu Pan,Weihao Xie,Chenchen Zhang,Zhaohui Wang,Jiayi Tian,Yanghai Wang,Zhe Cao,Minxin Dai,Ke Wang,Runzhe Wen,Yinghao Ma,Yaning Pan,Sungkyun Chang,Termeh Taheri,Haiwen Xia,Christos Plachouras,Emmanouil Benetos,Yizhi Li,Ge Zhang,Jian Yang,Tianhao Peng,Zili Wang,Minghao Liu,Junran Peng,Zhaoxiang Zhang,Jiaheng Liu*

Main category: cs.AI

TL;DR: 提出了OmniVideoBench基准测试，用于全面评估多模态大语言模型在音视频协同理解方面的能力，包含1000个高质量问答对和13种问题类型。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试无法全面评估音视频模态的协同推理能力，往往忽视其中一个模态或以逻辑不一致的方式整合它们。

Method: 构建包含1000个高质量问答对的大规模基准测试，涵盖628个多样化视频，每个问答对都有逐步推理轨迹，并手动验证确保正确性和唯一性。

Result: 多个MLLM在OmniVideoBench上的评估显示模型性能与人类推理之间存在显著差距，开源模型明显落后于闭源模型。

Conclusion: OmniVideoBench将公开发布，以促进具有更强和更通用推理能力的MLLM的发展。

Abstract: Recent advances in multimodal large language models (MLLMs) have demonstrated
substantial potential in video understanding. However, existing benchmarks fail
to comprehensively evaluate synergistic reasoning capabilities across audio and
visual modalities, often neglecting either one of the modalities or integrating
them in a logically inconsistent manner. To bridge this gap, we introduce
OmniVideoBench, a large-scale and rigorously designed benchmark dedicated to
assessing synergistic audio-visual understanding, with a strong emphasis on
modality complementarity and logical consistency. Specifically, OmniVideoBench
comprises 1000 high-quality question-answer(QA) pairs, each annotated with
step-by-step reasoning traces, derived from 628 diverse videos ranging from
several seconds to 30 minutes, and manually verified to guarantee complete
correctness and uniqueness. Moreover, OmniVideoBench encompasses 13 carefully
designed question types, covering temporal reasoning, spatial localization,
counting, causal inference, summarization, and beyond, thereby capturing the
essential challenges of video understanding. Evaluation of multiple MLLMs on
OmniVideoBench reveals a pronounced gap between model performance and human
reasoning, with open-source models lagging significantly behind their
closed-source counterparts, underscoring the inherent difficulty of genuine
audio-visual reasoning. We will release OmniVideoBench to foster the
development of MLLMs with stronger and more generalizable reasoning
capabilities.

</details>


### [136] [Extended Triangular Method: A Generalized Algorithm for Contradiction Separation Based Automated Deduction](https://arxiv.org/abs/2510.10701)
*Yang Xu,Shuwei Chen,Jun Liu,Feng Cao,Xingxing He*

Main category: cs.AI

TL;DR: 本文提出了扩展三角方法（ETM），这是一种形式化矛盾分离扩展（CSE）框架的算法实现，统一了多种矛盾构建策略，并在标准一阶基准测试中取得了有竞争力的结果。


<details>
  <summary>Details</summary>
Motivation: 传统推理演算基于二元归结，限制了多子句间的推理协同。虽然CSE框架提出了动态多子句推理理论，但其算法实现尚未形式化。

Method: 扩展三角方法（ETM）通过三角几何框架统一了多种矛盾构建策略，包括标准扩展方法，支持灵活的子句交互和动态协同。

Result: ETM作为多个高性能定理证明器（CSE、CSE-E、CSI-E、CSI-Enig）的核心算法，在标准一阶基准测试（TPTP问题集和CASC 2018-2015）中取得了有竞争力的结果。

Conclusion: ETM通过连接理论抽象和操作实现，将矛盾分离范式推进为通用、可扩展且具有实际竞争力的自动推理模型，为逻辑推理和定理证明的未来研究提供了新方向。

Abstract: Automated deduction lies at the core of Artificial Intelligence (AI),
underpinning theorem proving, formal verification, and logical reasoning.
Despite decades of progress, reconciling deductive completeness with
computational efficiency remains an enduring challenge. Traditional reasoning
calculi, grounded in binary resolution, restrict inference to pairwise clause
interactions and thereby limit deductive synergy among multiple clauses. The
Contradiction Separation Extension (CSE) framework, introduced in 2018,
proposed a dynamic multi-clause reasoning theory that redefined logical
inference as a process of contradiction separation rather than sequential
resolution. While that work established the theoretical foundation, its
algorithmic realization remained unformalized and unpublished. This work
presents the Extended Triangular Method (ETM), a generalized
contradiction-construction algorithm that formalizes and extends the internal
mechanisms of contradiction separation. The ETM unifies multiple
contradiction-building strategies, including the earlier Standard Extension
method, within a triangular geometric framework that supports flexible clause
interaction and dynamic synergy. ETM serves as the algorithmic core of several
high-performance theorem provers, CSE, CSE-E, CSI-E, and CSI-Enig, whose
competitive results in standard first-order benchmarks (TPTP problem sets and
CASC 2018-2015) empirically validate the effectiveness and generality of the
proposed approach. By bridging theoretical abstraction and operational
implementation, ETM advances the contradiction separation paradigm into a
generalized, scalable, and practically competitive model for automated
reasoning, offering new directions for future research in logical inference and
theorem proving.

</details>


### [137] [Adaptive Selection of Symbolic Languages for Improving LLM Logical Reasoning](https://arxiv.org/abs/2510.10703)
*Xiangyu Wang,Haocheng Yang,Fengxiang Cheng,Fenrong Liu*

Main category: cs.AI

TL;DR: 本文提出一种自适应选择符号语言的方法来提升LLMs的逻辑推理能力，通过为不同问题选择最适合的符号语言形式化（一阶逻辑、逻辑编程或布尔可满足性），显著提高了推理准确性。


<details>
  <summary>Details</summary>
Motivation: 现有方法在将自然语言问题翻译为符号语言时，只关注翻译准确性而忽略了目标符号语言类型本身的选择对推理性能的重要影响。不同逻辑推理问题对应不同的最优符号语言形式化。

Method: 利用LLMs为每个问题自适应选择最适合的符号语言（一阶逻辑、逻辑编程或布尔可满足性），然后将自然语言问题翻译为对应的符号语言表达式，并使用相应的逻辑求解器得出最终答案。

Result: 在基准测试中，自适应选择方法显著优于将所有问题翻译为单一符号语言或随机选择符号语言的方法。在混合数据集上达到96%的准确率，比一阶逻辑翻译的次高准确率提高了25%。

Conclusion: 为不同逻辑推理问题自适应选择最合适的符号语言形式化是提升LLMs逻辑推理性能的关键因素，该方法在多个基准测试中表现出显著优势。

Abstract: Large Language Models (LLMs) still struggle with complex logical reasoning.
While previous works achieve remarkable improvements, their performance is
highly dependent on the correctness of translating natural language (NL)
problems into a symbolic language (SL). Though numerous works focusing on
improving this translation accuracy, they only consider the similarity between
the meaning of SL and NL, overlooking another crucial influencing factor, the
selection of the target SL type itself. For example, first-order logic language
specializes in logical reasoning with categorical syllogisms and complex
quantifiers, while Boolean satisfiability formalism excels at representing
constraint satisfaction like partial problems. To our knowledge, this is the
first paper to claim and verify that different NL logical reasoning problem
corresponds to different optimal SL formalization for translation. Based on
this, we propose a methods to improve the logical reasoning performance of LLMs
by adaptively selecting the most suitable SL for each problem prior to
translation. Specifically, we leverage LLMs to select the target SL among
first-order logic, logic programming and Boolean satisfiability and then
translate the problem in NL to target SL expressions as well as employ the
corresponding logical solver to derive the final answer. Experimental results
on benchmarks show that our adaptive selection method significantly outperforms
translating all into single SL and randomly selecting the SL. On a mixed
dataset of these benchmarks, our approach achieves 96% accuracy, which
improving performance by 25% compared to the second highest accuracy from the
first-order logic translation.

</details>


### [138] [LLMs as Strategic Agents: Beliefs, Best Response Behavior, and Emergent Heuristics](https://arxiv.org/abs/2510.10813)
*Enric Junque de Fortuny,Veronica Roberta Cappelli*

Main category: cs.AI

TL;DR: 本文开发了一个框架来评估大语言模型是否具备真正的战略思维能力，通过分析信念形成、行动评估和选择行为，发现在特定推理深度下前沿模型表现出信念一致的最佳响应行为，并展现出元推理和新型启发式规则形成能力。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要评估LLMs在均衡博弈中的表现或推理深度，但缺乏对其真正战略思维能力（即连贯地形成对其他智能体的信念、评估可能行动并基于信念做出选择）的系统性研究。

Method: 开发了一个框架，在静态完全信息博弈中分离信念、评估和选择三个要素，通过分析模型的显性选择和推理轨迹，并引入新的上下文无关游戏来排除记忆模仿的影响。

Result: 前沿模型在目标推理深度下表现出信念一致的最佳响应行为；在无约束时会自我限制推理深度，并对人类和合成对手形成差异化推测；在复杂性增加时，显式递归被内部生成的稳定、模型特定的启发式选择规则取代。

Conclusion: 信念一致性、元推理和新型启发式形成可以从语言建模目标中共同涌现，为研究人工智能体的战略认知提供了结构化基础。

Abstract: Large Language Models (LLMs) are increasingly applied to domains that require
reasoning about other agents' behavior, such as negotiation, policy design, and
market simulation, yet existing research has mostly evaluated their adherence
to equilibrium play or their exhibited depth of reasoning. Whether they display
genuine strategic thinking, understood as the coherent formation of beliefs
about other agents, evaluation of possible actions, and choice based on those
beliefs, remains unexplored. We develop a framework to identify this ability by
disentangling beliefs, evaluation, and choice in static, complete-information
games, and apply it across a series of non-cooperative environments. By jointly
analyzing models' revealed choices and reasoning traces, and introducing a new
context-free game to rule out imitation from memorization, we show that current
frontier models exhibit belief-coherent best-response behavior at targeted
reasoning depths. When unconstrained, they self-limit their depth of reasoning
and form differentiated conjectures about human and synthetic opponents,
revealing an emergent form of meta-reasoning. Under increasing complexity,
explicit recursion gives way to internally generated heuristic rules of choice
that are stable, model-specific, and distinct from known human biases. These
findings indicate that belief coherence, meta-reasoning, and novel heuristic
formation can emerge jointly from language modeling objectives, providing a
structured basis for the study of strategic cognition in artificial agents.

</details>


### [139] [DRIFT: Decompose, Retrieve, Illustrate, then Formalize Theorems](https://arxiv.org/abs/2510.10815)
*Meiru Zhang,Philipp Borchert,Milan Gritta,Gerasimos Lampouras*

Main category: cs.AI

TL;DR: DRIFT框架通过将非正式数学陈述分解为子组件来改进LLMs的自动形式化，显著提升了前提检索效果和在形式化任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 解决LLMs在数学定理证明中难以识别和利用前提数学知识及其形式化表示的挑战，当前方法直接查询外部库但忽略了非正式数学陈述的复杂性和上下文限制。

Method: 引入DRIFT框架，将非正式数学陈述分解为更易处理的子组件，从Mathlib等数学库中进行针对性前提检索，并检索示例定理以帮助模型更有效地使用前提。

Result: 在多个基准测试中一致改进前提检索，ProofNet上的F1分数相比DPR基线几乎翻倍，在ConNF基准上使用GPT-4.1和DeepSeek-V3.1分别实现37.14%和42.25%的BEq+@10改进。

Conclusion: 数学自动形式化中的检索效果高度依赖于模型特定的知识边界，需要与每个模型能力相适应的自适应检索策略。

Abstract: Automating the formalization of mathematical statements for theorem proving
remains a major challenge for Large Language Models (LLMs). LLMs struggle to
identify and utilize the prerequisite mathematical knowledge and its
corresponding formal representation in languages like Lean. Current
retrieval-augmented autoformalization methods query external libraries using
the informal statement directly, but overlook a fundamental limitation:
informal mathematical statements are often complex and offer limited context on
the underlying math concepts. To address this, we introduce DRIFT, a novel
framework that enables LLMs to decompose informal mathematical statements into
smaller, more tractable ''sub-components''. This facilitates targeted retrieval
of premises from mathematical libraries such as Mathlib. Additionally, DRIFT
retrieves illustrative theorems to help models use premises more effectively in
formalization tasks. We evaluate DRIFT across diverse benchmarks (ProofNet,
ConNF, and MiniF2F-test) and find that it consistently improves premise
retrieval, nearly doubling the F1 score compared to the DPR baseline on
ProofNet. Notably, DRIFT demonstrates strong performance on the
out-of-distribution ConNF benchmark, with BEq+@10 improvements of 37.14% and
42.25% using GPT-4.1 and DeepSeek-V3.1, respectively. Our analysis shows that
retrieval effectiveness in mathematical autoformalization depends heavily on
model-specific knowledge boundaries, highlighting the need for adaptive
retrieval strategies aligned with each model's capabilities.

</details>


### [140] [The Irrational Machine: Neurosis and the Limits of Algorithmic Safety](https://arxiv.org/abs/2510.10823)
*Daniel Howard*

Main category: cs.AI

TL;DR: 提出了一个表征具身AI中神经症行为的框架，包括多种行为模式及其检测方法和逃避策略，并展示了即使在全可见性条件下，习得的厌恶成本仍会导致持久回避行为。


<details>
  <summary>Details</summary>
Motivation: 研究具身AI中内部一致但与现实错位的行为模式，这些行为源于规划、不确定性处理和厌恶记忆之间的相互作用，旨在识别和解决这些神经症行为。

Method: 在网格导航系统中分类多种神经症行为模式，开发轻量级在线检测器和可重用逃避策略，并采用基于遗传编程的破坏性测试来演化世界和扰动以最大化法律压力和神经症评分。

Result: 识别了多种神经症行为模式，开发了有效的检测和逃避方法，并展示了即使在全可见性条件下，习得的厌恶成本仍能主导局部选择，导致持久回避行为。

Conclusion: 局部修复不足以解决全局故障，需要架构层面的修订而非仅症状级别的修补，通过破坏性测试生成的对抗性课程和反事实轨迹可以揭示这些需求。

Abstract: We present a framework for characterizing neurosis in embodied AI: behaviors
that are internally coherent yet misaligned with reality, arising from
interactions among planning, uncertainty handling, and aversive memory. In a
grid navigation stack we catalogue recurrent modalities including flip-flop,
plan churn, perseveration loops, paralysis and hypervigilance, futile search,
belief incoherence, tie break thrashing, corridor thrashing, optimality
compulsion, metric mismatch, policy oscillation, and limited-visibility
variants. For each we give lightweight online detectors and reusable escape
policies (short commitments, a margin to switch, smoothing, principled
arbitration). We then show that durable phobic avoidance can persist even under
full visibility when learned aversive costs dominate local choice, producing
long detours despite globally safe routes. Using First/Second/Third Law as
engineering shorthand for safety latency, command compliance, and resource
efficiency, we argue that local fixes are insufficient; global failures can
remain. To surface them, we propose genetic-programming based destructive
testing that evolves worlds and perturbations to maximize law pressure and
neurosis scores, yielding adversarial curricula and counterfactual traces that
expose where architectural revision, not merely symptom-level patches, is
required.

</details>


### [141] [LLM-Empowered Agentic MAC Protocols: A Dynamic Stackelberg Game Approach](https://arxiv.org/abs/2510.10895)
*Renxuan Tan,Rongpeng Li,Fei Wang,Chenghui Peng,Shaoyun Wu,Zhifeng Zhao,Honggang Zhang*

Main category: cs.AI

TL;DR: 该论文提出了一个基于博弈论和LLM的多智能体强化学习框架，用于自动生成自适应MAC协议，解决了传统DRL方法泛化性差和需要重训练的问题。


<details>
  <summary>Details</summary>
Motivation: 传统MAC协议需要手动配置，而基于深度强化学习的协议虽然能提升特定任务性能，但泛化性和鲁棒性差，需要昂贵的重训练来适应动态环境。

Method: 将基站与用户设备的上行传输建模为动态多跟随者Stackelberg博弈，使用LLM驱动的智能体通过近端策略优化协调，合成自适应语义MAC协议，并采用协议动作语法确保过程可靠性。

Result: 仿真验证该框架相比传统基线方法，吞吐量提升77.6%，公平性改善65.2%，且能很好地泛化到用户数量波动的情况，无需重训练或架构修改。

Conclusion: 该LLM赋能的博弈论多智能体强化学习框架能够自动生成高性能、高泛化性的MAC协议，有效解决了传统方法的局限性。

Abstract: Medium Access Control (MAC) protocols, essential for wireless networks, are
typically manually configured. While deep reinforcement learning (DRL)-based
protocols enhance task-specified network performance, they suffer from poor
generalizability and resilience, demanding costly retraining to adapt to
dynamic environments. To overcome this limitation, we introduce a
game-theoretic LLM-empowered multi-agent DRL (MARL) framework, in which the
uplink transmission between a base station and a varying number of user
equipments is modeled as a dynamic multi-follower Stackelberg game (MFSG),
capturing the network's natural hierarchical structure. Within this game,
LLM-driven agents, coordinated through proximal policy optimization (PPO),
synthesize adaptive, semantic MAC protocols in response to network dynamics.
Protocol action grammar (PAG) is employed to ensure the reliability and
efficiency of this process. Under this system, we further analyze the existence
and convergence behavior in terms of a Stackelberg equilibrium by studying the
learning dynamics of LLM-empowered unified policies in response to changing
followers. Simulations corroborate that our framework achieves a 77.6% greater
throughput and a 65.2% fairness improvement over conventional baselines.
Besides, our framework generalizes excellently to a fluctuating number of users
without requiring retraining or architectural changes.

</details>


### [142] [PaperArena: An Evaluation Benchmark for Tool-Augmented Agentic Reasoning on Scientific Literature](https://arxiv.org/abs/2510.10909)
*Daoyu Wang,Mingyue Cheng,Qi Liu,Shuo Yu,Zirui Liu,Ze Guo*

Main category: cs.AI

TL;DR: 提出了PaperArena基准测试平台，用于评估LLM代理在跨论文推理和多工具协调方面的能力，发现现有先进代理系统在真实研究场景中的表现仍有很大提升空间。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要局限于单篇论文内的无工具任务，缺乏真实研究场景中跨论文推理和多工具协调的评估基准。

Method: 构建了一个模块化、可扩展的代理执行平台，提供多模态解析、上下文检索和程序化计算等工具，用于评估代理在解决需要整合多篇论文信息的研究问题时的表现。

Result: 实验结果显示，即使最先进的LLM驱动的成熟代理系统平均准确率仅为38.78%，在困难子集上准确率更是降至18.47%。所有测试代理都表现出工具使用效率低下的问题。

Conclusion: PaperArena基准揭示了当前代理系统在科学发现任务中的局限性，为开发更强大的科学发现代理提供了评估平台和发展方向。

Abstract: Understanding and reasoning on the web-scale scientific literature is a
crucial touchstone for large language model (LLM) based agents designed to
support complex knowledge-intensive tasks. However, existing works are mainly
restricted to tool-free tasks within isolated papers, largely due to the lack
of a benchmark for cross-paper reasoning and multi-tool orchestration in real
research scenarios. In this work, we propose PaperArena, an evaluation
benchmark for agents to address real-world research questions that typically
require integrating information across multiple papers with the assistance of
external tools. Given a research question, agents should integrate diverse
formats across multiple papers through reasoning and interacting with
appropriate tools, thereby producing a well-grounded answer. To support
standardized evaluation, we provide a modular and extensible platform for agent
execution, offering tools such as multimodal parsing, context retrieval, and
programmatic computation. Experimental results reveal that even the most
advanced LLM powering a well-established agent system achieves merely 38.78%
average accuracy. On the hard subset, accuracy drops to only 18.47%,
highlighting great potential for improvement. We also present several empirical
findings, including that all agents tested exhibit inefficient tool usage,
often invoking more tools than necessary to solve a task. We invite the
community to adopt PaperArena to develop and evaluate more capable agents for
scientific discovery. Our code and data are available
https://github.com/Melmaphother/PaperArena.

</details>


### [143] [PoU: Proof-of-Use to Counter Tool-Call Hacking in DeepResearch Agents](https://arxiv.org/abs/2510.10931)
*SHengjie Ma,Chenlong Deng,Jiaxin Mao,Jiadeng Huang,Teng Wang,Junjie Wu,Changwang Zhang,Jun wang*

Main category: cs.AI

TL;DR: 提出Proof-of-Use (PoU)框架，解决RAG代理中的工具调用黑客问题，通过证据基础的强化学习确保检索证据与推理之间的因果联系。


<details>
  <summary>Details</summary>
Motivation: 发现RAG代理存在工具调用黑客问题，即代理通过发出表面正确的工具调用来夸大奖励信号，而不真正利用检索证据，导致模式崩溃和虚假接地。

Method: 提出PoU框架，通过统一的逐步合约结合语法引用验证、基于扰动的敏感性奖励和答案-证据对齐目标，确保工具使用的可解释性和功能接地。

Result: 在七个QA基准测试中，PoU在事实准确性、证据忠实度和工具路由平衡方面持续优于DeepResearch基线。

Conclusion: 研究表明，需要将RL训练的代理不仅基于任务结果，还要基于检索信息的因果使用，为可信的检索增强推理提供了原则性路径。

Abstract: Retrieval-augmented generation (RAG) agents, such as recent
DeepResearch-style systems, extend large language models (LLMs) with autonomous
information-seeking capabilities through external tools. While reinforcement
learning (RL) has enabled impressive multi-step reasoning, we identify a
previously overlooked failure mode, Tool-Call Hacking, where agents inflate
reward signals by issuing superficially correct tool calls without genuinely
leveraging the retrieved evidence. This results in (i) mode collapse into
repetitive reliance on a single source and (ii) spurious grounding, where
answers are only weakly supported by cited content.
  To address this, we propose Proof-of-Use (PoU), an evidence-grounded RL
framework that enforces verifiable causal links between retrieved evidence,
reasoning traces, and final answers. PoU operationalizes this through a unified
step-wise contract combining syntactic citation validation, perturbation-based
sensitivity rewards, and answer-evidence alignment objectives, ensuring that
tool usage remains both interpretable and functionally grounded.
  Across seven QA benchmarks spanning in-domain, out-of-domain, and
out-of-tool-distribution settings, PoU consistently outperforms strong
DeepResearch baselines in factual accuracy, evidence faithfulness, and
tool-routing balance. These findings highlight the necessity of grounding
RL-trained agents not merely in task outcomes but in the causal use of
retrieved information, offering a principled path toward trustworthy
retrieval-augmented reasoning.

</details>


### [144] [Scalable and Explainable Enterprise Knowledge Discovery Using Graph-Centric Hybrid Retrieval](https://arxiv.org/abs/2510.10942)
*Nilima Rao,Jagriti Srivastava,Pradeep Kumar Sharma,Hritvik Shrivastava*

Main category: cs.AI

TL;DR: 提出模块化混合检索框架，集成知识库语言增强模型、深度图表示和语义搜索，解决企业异构知识系统中复杂查询的上下文推理和多跳推理问题。


<details>
  <summary>Details</summary>
Motivation: 企业知识分布在Jira、Git、Confluence等异构系统中，传统基于关键词搜索或静态嵌入的方法无法处理需要上下文推理和多跳推理的复杂查询。

Method: 构建统一知识图谱，集成KBLam模型、DeepGraph表示和语义搜索，通过查询分析动态选择最优检索策略，支持结构化和非结构化数据的独立或融合处理。

Result: 在大规模Git仓库上的实验表明，统一推理层相比独立GPT检索管道将答案相关性提升高达80%。

Conclusion: 该框架通过图构建、混合推理和交互式可视化的结合，为企业环境中的智能知识助手提供了可扩展、可解释和以用户为中心的基础。

Abstract: Modern enterprises manage vast knowledge distributed across heterogeneous
systems such as Jira, Git repositories, Confluence, and wikis. Conventional
retrieval methods based on keyword search or static embeddings often fail to
answer complex queries that require contextual reasoning and multi-hop
inference across artifacts. We present a modular hybrid retrieval framework for
adaptive enterprise information access that integrates Knowledge Base
Language-Augmented Models (KBLam), DeepGraph representations, and
embedding-driven semantic search. The framework builds a unified knowledge
graph from parsed repositories including code, pull requests, and commit
histories, enabling semantic similarity search, structural inference, and
multi-hop reasoning. Query analysis dynamically determines the optimal
retrieval strategy, supporting both structured and unstructured data sources
through independent or fused processing. An interactive interface provides
graph visualizations, subgraph exploration, and context-aware query routing to
generate concise and explainable answers. Experiments on large-scale Git
repositories show that the unified reasoning layer improves answer relevance by
up to 80 percent compared with standalone GPT-based retrieval pipelines. By
combining graph construction, hybrid reasoning, and interactive visualization,
the proposed framework offers a scalable, explainable, and user-centric
foundation for intelligent knowledge assistants in enterprise environments.

</details>


### [145] [Video-STR: Reinforcing MLLMs in Video Spatio-Temporal Reasoning with Relation Graph](https://arxiv.org/abs/2510.10976)
*Wentao Wang,Heqing Zou,Tianze Luo,Rui Huang,Yutian Zhao,Zhuochen Wang,Hansheng Zhang,Chengwei Qin,Yan Wang,Lin Zhao,Huaijian Zhang*

Main category: cs.AI

TL;DR: Video-STR是一种基于图强化学习的视频时空推理方法，通过图结构的组相对策略优化来提升多模态大语言模型在精确时空理解上的能力，并在STV-205k数据集上训练，在多个基准测试中达到最先进水平。


<details>
  <summary>Details</summary>
Motivation: 现有的多模态大语言模型在语义理解方面表现出色，但在精确的时空理解方面存在不足，特别是在处理视频中的物理信息（如多对象布局和运动）时表现不佳，这限制了在具身智能和VR等需要高精度应用中的使用。

Method: 提出Video-STR方法，基于可验证奖励的强化学习能力，采用图结构的组相对策略优化机制，引导模型在思考过程中推断场景的底层时空拓扑结构。构建了包含20.5万个问答对的STV-205k数据集来支持模型训练。

Result: 实验表明，Video-STR在多个基准测试中取得了最先进的结果，在STI-Bench基准上比基础模型提升了13%，证明了方法和数据集的有效性。

Conclusion: Video-STR通过图强化学习方法有效解决了多模态大语言模型在精确时空推理方面的局限性，为下游高精度应用提供了有力支持。

Abstract: Recent progress in Multimodal Large Language Models (MLLMs) has demonstrated
strong semantic understanding capabilities, but struggles to perform precise
spatio-temporal understanding. Existing spatio-temporal methods primarily focus
on the video itself, while overlooking the physical information within the
video, such as multi-object layouts and motion. Such limitations restrict the
use of MLLMs in downstream applications that demand high precision, including
embodied intelligence and VR. To address this issue, we present Video-STR, a
novel graph-based reinforcement method for precise Video Spatio-Temporal
Reasoning. Building upon the capacity of Reinforcement Learning with Verifiable
Reward (RLVR) to improve model abilities, we introduce a reasoning mechanism
using graph-based Group Relative Policy Optimization (GRPO) method to guide the
model in inferring the underlying spatio-temporal topology of scenarios during
the thinking process. To resolve the lack of spatio-temporal training data, we
construct the STV-205k dataset with 205k question-answering pairs, covering
dynamic multi-object scenes in both indoor and outdoor environments, to support
the model training. Experiments show that Video-STR achieves state-of-the-art
results on various benchmarks, outperforming the base model by 13% on
STI-Bench, and demonstrating the effectiveness of our approach and dataset.
Code, model, and data will be released.

</details>


### [146] [Revisiting Model Interpolation for Efficient Reasoning](https://arxiv.org/abs/2510.10977)
*Taiqiang Wu,Runming Yang,Tao Liu,Jiahao Wang,Ngai Wong*

Main category: cs.AI

TL;DR: 本文系统研究了最简单的模型权重插值方法，发现其遵循三阶段演化范式，能够以策略性插值超越复杂模型融合方法，在效率和效果上表现优异。


<details>
  <summary>Details</summary>
Motivation: 模型融合（特别是Instruct和Thinking模型）在高效推理方面表现出色，但现有方法复杂。本文重新审视最简单的权重插值方法，探索其潜力。

Method: 采用直接权重插值的简单方法，分析模型推理轨迹的三阶段演化动态，并通过消融研究验证不同层、模块和解码策略的影响。

Result: 策略性插值的模型在效率和效果上超越了复杂的模型融合基线方法，提供了性能-成本权衡的原则性指导。

Conclusion: 这项工作揭示了模型插值的机制，为构建具有精确目标推理能力的模型提供了实用框架。

Abstract: Model merging, typically on Instruct and Thinking models, has shown
remarkable performance for efficient reasoning. In this paper, we
systematically revisit the simplest merging method that interpolates two
weights directly. Particularly, we observe that model interpolation follows a
three-stage evolutionary paradigm with distinct behaviors on the reasoning
trajectory. These dynamics provide a principled guide for navigating the
performance-cost trade-off. Empirical results demonstrate that a strategically
interpolated model surprisingly surpasses sophisticated model merging baselines
on both efficiency and effectiveness. We further validate our findings with
extensive ablation studies on model layers, modules, and decoding strategies.
Ultimately, this work demystifies model interpolation and offers a practical
framework for crafting models with precisely targeted reasoning capabilities.
Code is available at \href{https://github.com/wutaiqiang/MI}{Github}.

</details>


### [147] [FBS Model-based Maintenance Record Accumulation for Failure-Cause Inference in Manufacturing Systems](https://arxiv.org/abs/2510.11003)
*Takuma Fujiu,Sho Okazaki,Kohei Kaminishi,Yuji Nakata,Shota Hamamoto,Kenshin Yokose,Tatsunori Hara,Yasushi Umeda,Jun Ota*

Main category: cs.AI

TL;DR: 本研究构建了诊断知识本体论，提出了基于功能-行为-结构(FBS)模型的维护记录积累方法，在故障原因推理中显示出比传统方法更好的效果，特别是在相关案例少且词汇不同的困难情况下。


<details>
  <summary>Details</summary>
Motivation: 在制造系统中，识别故障原因对维持和提高生产效率至关重要。基于知识的故障原因推理需要知识库能够明确结构化目标系统和故障知识，并包含足够长的故障因果链。

Method: 构建了诊断知识本体论，提出了基于功能-行为-结构(FBS)模型的维护记录积累方法。

Result: 使用所提方法积累的维护记录进行故障原因推理，与专家列举的候选原因集显示出更好的一致性，特别是在相关案例少且词汇不同的困难情况下。

Conclusion: 该方法利用设计阶段对目标的理解和知识来支持维护阶段的知识积累和问题解决，有望成为未来整个工程链知识共享的基础。未来需要开发针对这些维护记录的推理方法，构建用户界面，并在更大更多样化的系统上进行验证。

Abstract: In manufacturing systems, identifying the causes of failures is crucial for
maintaining and improving production efficiency. In knowledge-based
failure-cause inference, it is important that the knowledge base (1) explicitly
structures knowledge about the target system and about failures, and (2)
contains sufficiently long causal chains of failures. In this study, we
constructed Diagnostic Knowledge Ontology and proposed a
Function-Behavior-Structure (FBS) model-based maintenance-record accumulation
method based on it. Failure-cause inference using the maintenance records
accumulated by the proposed method showed better agreement with the set of
candidate causes enumerated by experts, especially in difficult cases where the
number of related cases is small and the vocabulary used differs. In the
future, it will be necessary to develop inference methods tailored to these
maintenance records, build a user interface, and carry out validation on larger
and more diverse systems. Additionally, this approach leverages the
understanding and knowledge of the target in the design phase to support
knowledge accumulation and problem solving during the maintenance phase, and it
is expected to become a foundation for knowledge sharing across the entire
engineering chain in the future.

</details>


### [148] [Argumentation-Based Explainability for Legal AI: Comparative and Regulatory Perspectives](https://arxiv.org/abs/2510.11079)
*Andrada Iulia Prajescu,Roberto Confalonieri*

Main category: cs.AI

TL;DR: 本文探讨了在AI系统应用于法律领域时，论证框架如何提供符合法律要求的解释，以解决黑盒问题并满足GDPR和AIA等监管要求。


<details>
  <summary>Details</summary>
Motivation: AI系统在法律环境中的不透明性对公平性、问责制和信任构成挑战，黑盒问题削弱了自动化决策的合法性，需要提供有意义的解释。

Method: 分析不同解释策略的优缺点，评估其在法律推理中的适用性，特别关注论证框架如何捕捉法律的可废止性、可争议性和价值敏感性。

Result: 论证框架为可解释法律AI提供了特别稳健的基础，能够满足技术和规范层面的透明度要求。

Conclusion: 计算论证最适合满足法律领域透明度的技术和规范要求，但仍需解决偏见缓解、司法环境实证验证等开放挑战。

Abstract: Artificial Intelligence (AI) systems are increasingly deployed in legal
contexts, where their opacity raises significant challenges for fairness,
accountability, and trust. The so-called ``black box problem'' undermines the
legitimacy of automated decision-making, as affected individuals often lack
access to meaningful explanations. In response, the field of Explainable AI
(XAI) has proposed a variety of methods to enhance transparency, ranging from
example-based and rule-based techniques to hybrid and argumentation-based
approaches. This paper promotes computational models of arguments and their
role in providing legally relevant explanations, with particular attention to
their alignment with emerging regulatory frameworks such as the EU General Data
Protection Regulation (GDPR) and the Artificial Intelligence Act (AIA). We
analyze the strengths and limitations of different explanation strategies,
evaluate their applicability to legal reasoning, and highlight how
argumentation frameworks -- by capturing the defeasible, contestable, and
value-sensitive nature of law -- offer a particularly robust foundation for
explainable legal AI. Finally, we identify open challenges and research
directions, including bias mitigation, empirical validation in judicial
settings, and compliance with evolving ethical and legal standards, arguing
that computational argumentation is best positioned to meet both technical and
normative requirements of transparency in the law domain.

</details>


### [149] [Modeling AI-Driven Production and Competitiveness A Multi-Agent Economic Simulation of China and the United States](https://arxiv.org/abs/2510.11085)
*Yuxinyue Qian,Jun Liu*

Main category: cs.AI

TL;DR: 本文基于多智能体经济模型，比较了中美两国在不同AI机制下的宏观经济产出演变，发现AI作为独立生产实体能显著提升社会产出增长率，中国在智能体扩张和技术追赶方面具有加速潜力。


<details>
  <summary>Details</summary>
Motivation: 随着AI技术快速发展，社会经济系统进入'人机共创'新阶段，需要系统理解AI驱动的生产系统转型和国际竞争力变化。

Method: 基于先前建立的多层次智能体经济模型，采用仿真方法比较中美在不同机制下的宏观经济产出演变，包括AI协作、网络效应和AI自主生产。

Result: AI作为独立生产实体时，社会产出增长率远超传统人力劳动模式；中国在智能体扩张和技术追赶方面显示出明显的加速潜力，有望实现技术趋同甚至部分超越。

Conclusion: 本研究为理解AI驱动的生产系统转型和国际竞争力变化提供了基于模型的系统性分析框架，并为相关政策制定提供了量化参考。

Abstract: With the rapid development of artificial intelligence (AI) technology,
socio-economic systems are entering a new stage of "human-AI co-creation."
Building upon a previously established multi-level intelligent agent economic
model, this paper conducts simulation-based comparisons of macroeconomic output
evolution in China and the United States under different mechanisms-AI
collaboration, network effects, and AI autonomous production. The results show
that: (1) when AI functions as an independent productive entity, the overall
growth rate of social output far exceeds that of traditional human-labor-based
models; (2) China demonstrates clear potential for acceleration in both the
expansion of intelligent agent populations and the pace of technological
catch-up, offering the possibility of achieving technological convergence or
even partial surpassing. This study provides a systematic, model-based
analytical framework for understanding AI-driven production system
transformation and shifts in international competitiveness, as well as
quantitative insights for relevant policy formulation.

</details>


### [150] [Improving AI Efficiency in Data Centres by Power Dynamic Response](https://arxiv.org/abs/2510.11119)
*Andrea Marinoni,Sai Shivareddy,Pietro Lio',Weisi Lin,Erik Cambria,Clare Grey*

Main category: cs.AI

TL;DR: 该论文提出了一种创新的AI数据中心电源管理方法，通过使部分输入电源像数据计算功能使用的电源一样动态化，来改善AI数据中心的可持续性。


<details>
  <summary>Details</summary>
Motivation: AI数据中心的快速增长对电力需求巨大，给环境可持续性带来挑战，需要创新的电源管理解决方案来平衡计算性能和能源效率。

Method: 采用创新的电源管理方法，使输入电源动态化，量化比较被动和主动设备在计算增益、能源效率、资本支出和管理成本方面的性能，分析全球多个数据平台的电力趋势。

Result: 该策略在计算增益、能源效率、资本支出和管理成本方面表现出色，能够显著改善AI超大规模数据中心的可持续性。

Conclusion: 这种创新的电源管理方法代表了AI数据中心电源管理的范式转变，有潜力在环境、财务和社会领域显著增强AI超大规模数据中心的可持续性足迹。

Abstract: The steady growth of artificial intelligence (AI) has accelerated in the
recent years, facilitated by the development of sophisticated models such as
large language models and foundation models. Ensuring robust and reliable power
infrastructures is fundamental to take advantage of the full potential of AI.
However, AI data centres are extremely hungry for power, putting the problem of
their power management in the spotlight, especially with respect to their
impact on environment and sustainable development. In this work, we investigate
the capacity and limits of solutions based on an innovative approach for the
power management of AI data centres, i.e., making part of the input power as
dynamic as the power used for data-computing functions. The performance of
passive and active devices are quantified and compared in terms of
computational gain, energy efficiency, reduction of capital expenditure, and
management costs by analysing power trends from multiple data platforms
worldwide. This strategy, which identifies a paradigm shift in the AI data
centre power management, has the potential to strongly improve the
sustainability of AI hyperscalers, enhancing their footprint on environmental,
financial, and societal fields.

</details>


### [151] [Spec-Driven AI for Science: The ARIA Framework for Automated and Reproducible Data Analysis](https://arxiv.org/abs/2510.11143)
*Chuke Chen,Biao Luo,Nan Li,Boxiang Wang,Hang Yang,Jing Guo,Ming Xu*

Main category: cs.AI

TL;DR: ARIA是一个基于规范驱动、人机协作的自动化可解释数据分析框架，通过自然语言规范整合人类推理和机器执行，实现透明、协作和可复现的科学发现。


<details>
  <summary>Details</summary>
Motivation: 科学数据的快速增长导致分析能力与研究意图之间存在差距，现有AI分析工具要么偏向自动化而缺乏透明度，要么依赖手动脚本阻碍可扩展性和可复现性。

Method: ARIA采用六层互操作架构（命令、上下文、代码、数据、编排和AI模块），在文档中心工作流中通过自然语言规范定义分析目标，自动生成可执行代码、验证计算并生成透明文档。

Result: 在波士顿房价案例中，ARIA发现了25个关键特征并确定XGBoost为最佳模型（R平方=0.93），过拟合最小。跨领域评估显示ARIA在性能、可解释性和效率方面优于最先进系统。

Conclusion: 通过将AI研究原则和科学原则结合到规范驱动架构中，ARIA为透明、协作和可复现的科学发现建立了新范式。

Abstract: The rapid expansion of scientific data has widened the gap between analytical
capability and research intent. Existing AI-based analysis tools, ranging from
AutoML frameworks to agentic research assistants, either favor automation over
transparency or depend on manual scripting that hinders scalability and
reproducibility. We present ARIA (Automated Research Intelligence Assistant), a
spec-driven, human-in-the-loop framework for automated and interpretable data
analysis. ARIA integrates six interoperable layers, namely Command, Context,
Code, Data, Orchestration, and AI Module, within a document-centric workflow
that unifies human reasoning and machine execution. Through natural-language
specifications, researchers define analytical goals while ARIA autonomously
generates executable code, validates computations, and produces transparent
documentation. Beyond achieving high predictive accuracy, ARIA can rapidly
identify optimal feature sets and select suitable models, minimizing redundant
tuning and repetitive experimentation. In the Boston Housing case, ARIA
discovered 25 key features and determined XGBoost as the best performing model
(R square = 0.93) with minimal overfitting. Evaluations across heterogeneous
domains demonstrate ARIA's strong performance, interpretability, and efficiency
compared with state-of-the-art systems. By combining AI for research and AI for
science principles within a spec-driven architecture, ARIA establishes a new
paradigm for transparent, collaborative, and reproducible scientific discovery.

</details>


### [152] [$How^{2}$: How to learn from procedural How-to questions](https://arxiv.org/abs/2510.11144)
*Gautier Dagan,Frank Keller,Alex Lascarides*

Main category: cs.AI

TL;DR: 提出了$How^{2}$框架，使AI智能体能够提问how-to问题、存储答案并在交互环境中重复使用以实现终身学习，在Minecraft环境中验证了抽象化答案对规划能力提升的效果。


<details>
  <summary>Details</summary>
Motivation: 智能体在规划问题时需要填补知识空白，但how-to问题的开放性（从可执行动作到高级子目标描述）使得AI智能体难以有效提问，AI专家也难以高效回答以支持规划。

Method: 引入$How^{2}$记忆智能体框架，在Plancraft（Minecraft制作环境）中评估，使用不同抽象级别的教师模型回答问题，从可执行动作序列到高级子目标描述。

Result: 终身学习智能体从抽象化且与当前状态解耦的答案中获益最大，$How^{2}$为基于LLM的智能体提供了在交互环境中通过提问提升规划能力的方法。

Conclusion: $How^{2}$框架通过使智能体能够提问how-to问题并重用答案，有效提升了基于LLM的智能体在交互环境中的终身学习规划能力。

Abstract: An agent facing a planning problem can use answers to how-to questions to
reduce uncertainty and fill knowledge gaps, helping it solve both current and
future tasks. However, their open ended nature, where valid answers to "How do
I X?" range from executable actions to high-level descriptions of X's
sub-goals, makes them challenging for AI agents to ask, and for AI experts to
answer, in ways that support efficient planning. We introduce $How^{2}$, a
memory agent framework that enables agents to ask how-to questions, store the
answers, and reuse them for lifelong learning in interactive environments. We
evaluate our approach in Plancraft, a Minecraft crafting environment, where
agents must complete an assembly task by manipulating inventory items. Using
teacher models that answer at varying levels of abstraction, from executable
action sequences to high-level subgoal descriptions, we show that lifelong
learning agents benefit most from answers that are abstracted and decoupled
from the current state. $How^{2}$ offers a way for LLM-based agents to improve
their planning capabilities over time by asking questions in interactive
environments.

</details>


### [153] [Aligning Deep Implicit Preferences by Learning to Reason Defensively](https://arxiv.org/abs/2510.11194)
*Peiming Li,Zhiyuan Hu,Yang Tang,Shiyu Li,Xi Chen*

Main category: cs.AI

TL;DR: 提出了CDRA方法，将LLM对齐从标量奖励匹配重构为结构化推理过程，通过DeepPref基准和Pers-GenPRM模型解决用户深层偏好推断和防御性推理的挑战。


<details>
  <summary>Details</summary>
Motivation: 当前方法无法推断用户的深层隐含偏好（包括未陈述的目标、语义上下文和风险容忍度），并且缺乏应对现实世界模糊性的防御性推理能力，导致响应表面化、脆弱和短视。

Method: 1. 引入DeepPref基准数据集（3000个偏好-查询对，覆盖20个主题）；2. 提出Pers-GenPRM模型，将奖励建模重构为个性化推理任务；3. 使用Critique-Driven Policy Alignment进行策略对齐，整合数值和自然语言反馈。

Result: 实验表明CDRA在发现和适应用户真实偏好方面表现出色，同时执行稳健的推理。

Conclusion: CDRA通过结构化推理过程有效解决了LLM个性化对齐中的认知差距问题，提供了可解释的奖励信号和稳健的推理能力。

Abstract: Personalized alignment is crucial for enabling Large Language Models (LLMs)
to engage effectively in user-centric interactions. However, current methods
face a dual challenge: they fail to infer users' deep implicit preferences
(including unstated goals, semantic context and risk tolerances), and they lack
the defensive reasoning required to navigate real-world ambiguity. This
cognitive gap leads to responses that are superficial, brittle and
short-sighted. To address this, we propose Critique-Driven Reasoning Alignment
(CDRA), which reframes alignment from a scalar reward-matching task into a
structured reasoning process. First, to bridge the preference inference gap, we
introduce the DeepPref benchmark. This dataset, comprising 3000
preference-query pairs across 20 topics, is curated by simulating a
multi-faceted cognitive council that produces critique-annotated reasoning
chains to deconstruct query semantics and reveal latent risks. Second, to
instill defensive reasoning, we introduce the Personalized Generative Process
Reward Model (Pers-GenPRM), which frames reward modeling as a personalized
reasoning task. It generates a critique chain to evaluate a response's
alignment with user preferences before outputting a final score based on this
rationale. Ultimately, this interpretable, structured reward signal guides
policy model through Critique-Driven Policy Alignment, a process-level online
reinforcement learning algorithm integrating both numerical and natural
language feedback. Experiments demonstrate that CDRA excels at discovering and
aligning with users' true preferences while executing robust reasoning. Our
code and dataset are available at https://github.com/Zephyrian-Hugh/Deep-pref.

</details>


### [154] [AI Alignment Strategies from a Risk Perspective: Independent Safety Mechanisms or Shared Failures?](https://arxiv.org/abs/2510.11235)
*Leonard Dung,Florian Mai*

Main category: cs.AI

TL;DR: 本文分析了7种代表性AI对齐技术和7种失效模式的重叠程度，评估防御深度策略在AI安全中的有效性。


<details>
  <summary>Details</summary>
Motivation: 由于每种AI对齐技术都有失效模式，防御深度策略成为风险缓解的关键方法，但其效果取决于不同技术间失效模式的相关性。

Method: 分析7种代表性对齐技术和7种失效模式的重叠程度，评估它们之间的相关性。

Result: 研究发现不同对齐技术的失效模式存在不同程度的重叠，这对防御深度策略的有效性产生重要影响。

Conclusion: 理解失效模式的相关性对于评估当前风险水平和未来AI对齐研究优先级至关重要。

Abstract: AI alignment research aims to develop techniques to ensure that AI systems do
not cause harm. However, every alignment technique has failure modes, which are
conditions in which there is a non-negligible chance that the technique fails
to provide safety. As a strategy for risk mitigation, the AI safety community
has increasingly adopted a defense-in-depth framework: Conceding that there is
no single technique which guarantees safety, defense-in-depth consists in
having multiple redundant protections against safety failure, such that safety
can be maintained even if some protections fail. However, the success of
defense-in-depth depends on how (un)correlated failure modes are across
alignment techniques. For example, if all techniques had the exact same failure
modes, the defense-in-depth approach would provide no additional protection at
all. In this paper, we analyze 7 representative alignment techniques and 7
failure modes to understand the extent to which they overlap. We then discuss
our results' implications for understanding the current level of risk and how
to prioritize AI alignment research in the future.

</details>


### [155] [PADME: Procedure Aware DynaMic Execution](https://arxiv.org/abs/2510.11281)
*Deepeka Garg,Sihan Zeng,Annapoorani L. Narayanan,Sumitra Ganesh,Leo Ardon*

Main category: cs.AI

TL;DR: PADME是一个基于图表示的智能代理框架，能够将自然语言程序文本转化为可执行图结构，实现长时程任务的自主执行。


<details>
  <summary>Details</summary>
Motivation: 解决智能代理在执行自然语言长时程程序时的漂移和失败问题，传统方法因文本的变异性缺乏结构导致执行不可靠。

Method: 采用两阶段方法：Teach阶段将程序文本结构化并丰富可执行逻辑，Execute阶段根据实时输入和环境反馈进行动态执行。

Result: 在ALFWorld和ScienceWorld等四个基准测试中达到最先进性能，证明了图结构表示对鲁棒和泛化执行的重要性。

Conclusion: 基于图的程序表示为可靠代理驱动自动化提供了强大的中间抽象，显著减少了长时程推理中的错误累积。

Abstract: Learning to autonomously execute long-horizon procedures from natural
language remains a core challenge for intelligent agents. Free-form
instructions such as recipes, scientific protocols, or business workflows
encode rich procedural knowledge, but their variability and lack of structure
cause agents driven by large language models (LLMs) to drift or fail during
execution. We introduce Procedure Aware DynaMic Execution (PADME), an agent
framework that produces and exploits a graph-based representation of
procedures. Unlike prior work that relies on manual graph construction or
unstructured reasoning, PADME autonomously transforms procedural text into
executable graphs that capture task dependencies, decision points, and reusable
subroutines. Central to PADME is a two-phase methodology; Teach phase, which
focuses on systematic structuring, enrichment with executable logic of
procedures, followed by Execute phase, which enables dynamic execution in
response to real-time inputs and environment feedback. This separation ensures
quality assurance and scalability, allowing expert knowledge to be encoded once
and reliably reused across varying contexts. The graph representation also
provides an inductive bias that reduces error accumulation in long-horizon
reasoning, underscoring the importance of structured procedure modeling for
reliable agent-driven automation. Empirically, PADME achieves state-of-the-art
performance on four diverse benchmarks, including ALFWorld and ScienceWorld.
These results demonstrate that agents equipped with graph-based procedure
representations offer a powerful intermediate abstraction for robust and
generalizable execution.

</details>


### [156] [Evolution in Simulation: AI-Agent School with Dual Memory for High-Fidelity Educational Dynamics](https://arxiv.org/abs/2510.11290)
*Sheng Jin,Haoming Wang,Zhiqi Gao,Yongbo Yang,Bao Chunjia,Chengliang Wang*

Main category: cs.AI

TL;DR: 提出AI-Agent School系统，通过自进化机制模拟复杂教育动态，采用Zero-Exp策略和"经验-反思-优化"循环，提升代理在模拟教育参与者方面的性能。


<details>
  <summary>Details</summary>
Motivation: 解决教学流程建模的碎片化问题，以及代理在模拟多样化教育参与者方面的性能限制。

Method: 构建Zero-Exp策略，采用包含经验和知识库的双重记忆基础，结合短期和长期记忆组件，通过"经验-反思-优化"循环实现代理自主进化。

Result: 实验证实AAS能有效模拟复杂教育动态，促进高级代理认知能力发展，生成高保真行为交互数据。

Conclusion: AAS为从"经验时代"迈向"模拟时代"提供了基础支撑，通过模拟真实学校中的师生互动和学习过程。

Abstract: Large language models (LLMs) based Agents are increasingly pivotal in
simulating and understanding complex human systems and interactions. We propose
the AI-Agent School (AAS) system, built around a self-evolving mechanism that
leverages agents for simulating complex educational dynamics. Addressing the
fragmented issues in teaching process modeling and the limitations of agents
performance in simulating diverse educational participants, AAS constructs the
Zero-Exp strategy, employs a continuous "experience-reflection-optimization"
cycle, grounded in a dual memory base comprising experience and knowledge bases
and incorporating short-term and long-term memory components. Through this
mechanism, agents autonomously evolve via situated interactions within diverse
simulated school scenarios. This evolution enables agents to more accurately
model the nuanced, multi-faceted teacher-student engagements and underlying
learning processes found in physical schools. Experiment confirms that AAS can
effectively simulate intricate educational dynamics and is effective in
fostering advanced agent cognitive abilities, providing a foundational stepping
stone from the "Era of Experience" to the "Era of Simulation" by generating
high-fidelity behavioral and interaction data.

</details>


### [157] [Automated Skill Decomposition Meets Expert Ontologies: Bridging the Granularity Gap with LLMs](https://arxiv.org/abs/2510.11313)
*Le Ngoc Luyen,Marie-Hélène Abel*

Main category: cs.AI

TL;DR: 本文提出使用LLMs进行自动化技能分解，并建立基于本体的评估框架，包含语义F1和层次感知F1两个指标，比较了零样本和少样本提示策略的效果。


<details>
  <summary>Details</summary>
Motivation: 为了解决技能分解任务中缺乏标准化评估框架的问题，确保分解结果与本体结构的一致性。

Method: 提出基于本体的评估框架，包含提示生成、规范化、本体对齐等步骤，引入语义F1和层次感知F1两个评估指标，在ROME-ESCO-DecompSkill数据集上比较零样本和少样本提示策略。

Result: 零样本提供强基线，少样本在短语稳定性和粒度控制方面表现更好，能改善层次感知对齐，且延迟分析显示少样本提示有时比零样本更快。

Conclusion: 该框架、基准和指标为开发忠实于本体的技能分解系统提供了可复现的基础。

Abstract: This paper investigates automated skill decomposition using Large Language
Models (LLMs) and proposes a rigorous, ontology-grounded evaluation framework.
Our framework standardizes the pipeline from prompting and generation to
normalization and alignment with ontology nodes. To evaluate outputs, we
introduce two metrics: a semantic F1-score that uses optimal embedding-based
matching to assess content accuracy, and a hierarchy-aware F1-score that
credits structurally correct placements to assess granularity. We conduct
experiments on ROME-ESCO-DecompSkill, a curated subset of parents, comparing
two prompting strategies: zero-shot and leakage-safe few-shot with exemplars.
Across diverse LLMs, zero-shot offers a strong baseline, while few-shot
consistently stabilizes phrasing and granularity and improves hierarchy-aware
alignment. A latency analysis further shows that exemplar-guided prompts are
competitive - and sometimes faster - than unguided zero-shot due to more
schema-compliant completions. Together, the framework, benchmark, and metrics
provide a reproducible foundation for developing ontology-faithful skill
decomposition systems.

</details>


### [158] [AI-Driven anemia diagnosis: A review of advanced models and techniques](https://arxiv.org/abs/2510.11380)
*Abdullah Al Mahmud,Prangon Chowdhury,Mohammed Borhan Uddin,Khaled Eabne Delowar,Tausifur Rahman Talha,Bijoy Dewanjee*

Main category: cs.AI

TL;DR: 本文系统综述了机器学习和深度学习在贫血检测、分类和诊断中的最新进展，比较了不同模型的性能指标，并评估了这些模型在贫血检测中的优势和局限性。


<details>
  <summary>Details</summary>
Motivation: 贫血是一种影响全球数百万人的常见健康问题，准确及时的诊断对于有效管理和治疗至关重要。近年来，人工智能技术在贫血诊断中的应用日益受到关注。

Method: 采用系统综述方法，分析比较了应用于贫血检测的各种机器学习和深度学习模型，重点关注准确率、灵敏度、特异性和精确度等性能指标。

Result: 通过分析不同模型的性能指标，评估了这些模型在贫血检测和分类中的表现，识别了各自的优势和局限性。

Conclusion: 强调了解决这些因素对于提高贫血诊断准确性的重要性，为未来改进贫血检测模型提供了指导方向。

Abstract: Anemia, a condition marked by insufficient levels of red blood cells or
hemoglobin, remains a widespread health issue affecting millions of individuals
globally. Accurate and timely diagnosis is essential for effective management
and treatment of anemia. In recent years, there has been a growing interest in
the use of artificial intelligence techniques, i.e., machine learning (ML) and
deep learning (DL) for the detection, classification, and diagnosis of anemia.
This paper provides a systematic review of the recent advancements in this
field, with a focus on various models applied to anemia detection. The review
also compares these models based on several performance metrics, including
accuracy, sensitivity, specificity, and precision. By analyzing these metrics,
the paper evaluates the strengths and limitation of discussed models in
detecting and classifying anemia, emphasizing the importance of addressing
these factors to improve diagnostic accuracy.

</details>


### [159] [From <Answer> to <Think>: Multidimensional Supervision of Reasoning Process for LLM Optimization](https://arxiv.org/abs/2510.11457)
*Beining Wang,Weihang Su,Hongtao Tian,Tao Yang,Yujia Zhou,Ting Yao,Qingyao Ai,Yiqun Liu*

Main category: cs.AI

TL;DR: 提出维度级奖励模型（DRM），通过置信度、相关性和连贯性三个维度评估推理过程质量，解决了传统结果监督强化学习稀疏奖励和过程级奖励模型缺乏泛化性的问题。


<details>
  <summary>Details</summary>
Motivation: 改进大语言模型的多步推理能力是一个关键但具有挑战性的任务。传统的结果监督强化学习只奖励正确答案，容易传播错误推理且奖励信号稀疏；过程级奖励模型虽然提供更密集的反馈，但缺乏泛化性和可解释性。

Method: 提出维度级奖励模型（DRM），从三个基本、互补且可解释的维度评估推理过程质量：置信度（不确定性校准）、相关性（语义对齐）和连贯性（逻辑一致性）。这些维度能够捕捉最终答案正确性之外的方面，无需真实答案即可进行可解释的评估。

Result: 实验结果显示DRM提供了有效的监督信号，指导大语言模型的优化并增强其推理能力。DRM监督训练在数学、问答、代码执行和谜题等开放领域任务上，无论是分布内还是分布外都取得了持续的性能提升。

Conclusion: 研究发现对推理过程进行多维监督可以提升大语言模型在训练分布之外的泛化推理能力。

Abstract: Improving the multi-step reasoning ability of Large Language Models (LLMs) is
a critical yet challenging task. The dominant paradigm, outcome-supervised
reinforcement learning (RLVR), rewards only correct final answers, often
propagating flawed reasoning and suffering from sparse reward signals. While
process-level reward models (PRMs) provide denser, step-by-step feedback, they
lack generalizability and interpretability, requiring task-specific
segmentation of the reasoning process. To this end, we propose the
Dimension-level Reward Model (DRM), a new supervision framework that bridges
the gap between these two approaches. DRM evaluates the quality of a reasoning
process along three fundamental, complementary, and interpretable dimensions:
Confidence for uncertainty calibration, Relevance for semantic alignment, and
Coherence for logical consistency. Together, these dimensions capture aspects
beyond final answer correctness and enable interpretable assessment without
requiring ground truth answers. Experimental results show that DRM provides
effective supervision signals, guides the optimization of LLMs and enhances
their reasoning ability. In particular, DRM-supervised training achieves
consistent gains on both in-distribution and out-of-distribution open-domain
tasks, including mathematics, question answering, code execution, and puzzles.
Our findings demonstrate that multidimensional supervision of the reasoning
process can improve the generalized reasoning ability of LLMs beyond the
training distribution.

</details>


### [160] [Unifying Deductive and Abductive Reasoning in Knowledge Graphs with Masked Diffusion Model](https://arxiv.org/abs/2510.11462)
*Yisen Gao,Jiaxin Bai,Yi Huang,Xingcheng Fu,Qingyun Sun,Yangqiu Song*

Main category: cs.AI

TL;DR: DARK是一个统一的知识图谱推理框架，结合了演绎和溯因推理，通过掩码扩散模型和自反去噪过程实现双向推理，在多个基准测试中达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法将演绎推理（从查询中检索实体）和溯因推理（从观察中生成假设）分开处理，但两者具有协同潜力，需要统一框架来结合它们的优势。

Method: 使用掩码扩散模型捕获查询和结论的双向关系，引入自反去噪过程迭代生成和验证假设，并提出逻辑探索强化学习同时掩码查询和结论以探索新的推理组合。

Result: 在多个基准知识图谱上的实验表明，DARK在演绎和溯因推理任务上都达到了最先进的性能。

Conclusion: 统一演绎和溯因推理的方法具有显著优势，DARK框架通过双向推理和假设验证机制有效提升了知识图谱推理能力。

Abstract: Deductive and abductive reasoning are two critical paradigms for analyzing
knowledge graphs, enabling applications from financial query answering to
scientific discovery. Deductive reasoning on knowledge graphs usually involves
retrieving entities that satisfy a complex logical query, while abductive
reasoning generates plausible logical hypotheses from observations. Despite
their clear synergistic potential, where deduction can validate hypotheses and
abduction can uncover deeper logical patterns, existing methods address them in
isolation. To bridge this gap, we propose DARK, a unified framework for
Deductive and Abductive Reasoning in Knowledge graphs. As a masked diffusion
model capable of capturing the bidirectional relationship between queries and
conclusions, DARK has two key innovations. First, to better leverage deduction
for hypothesis refinement during abductive reasoning, we introduce a
self-reflective denoising process that iteratively generates and validates
candidate hypotheses against the observed conclusion. Second, to discover
richer logical associations, we propose a logic-exploration reinforcement
learning approach that simultaneously masks queries and conclusions, enabling
the model to explore novel reasoning compositions. Extensive experiments on
multiple benchmark knowledge graphs show that DARK achieves state-of-the-art
performance on both deductive and abductive reasoning tasks, demonstrating the
significant benefits of our unified approach.

</details>


### [161] [Zero Data Retention in LLM-based Enterprise AI Assistants: A Comparative Study of Market Leading Agentic AI Products](https://arxiv.org/abs/2510.11558)
*Komal Gupta,Aditya Shrivastava*

Main category: cs.AI

TL;DR: 本文分析企业AI助手中的零数据保留政策，重点关注Salesforce和Microsoft的技术架构实现，探讨在数据治理、合规性和业务隐私保护方面的权衡。


<details>
  <summary>Details</summary>
Motivation: 随着企业AI助手的兴起，保护私有数据和确保合规性成为优先事项，特别是在医疗和金融等敏感行业。零数据保留政策对于防止数据泄露至关重要。

Method: 通过分析Salesforce AgentForce和Microsoft Copilot两个行业领先AI助手的技术架构，研究它们如何实现零数据保留政策，并与OpenAI、Anthropic和Meta等大型语言模型服务提供商合作。

Result: 研究发现不同公司采用不同的技术架构来支持零数据保留政策，这些系统在架构设计、合规性和可用性方面存在权衡关系。

Conclusion: 零数据保留政策在企业AI助手中的实施需要平衡技术架构、合规要求和用户体验，不同公司根据自身需求采用不同的实现方案。

Abstract: Governance of data, compliance, and business privacy matters, particularly
for healthcare and finance businesses. Since the recent emergence of AI
enterprise AI assistants enhancing business productivity, safeguarding private
data and compliance is now a priority. With the implementation of AI assistants
across the enterprise, the zero data retention can be achieved by implementing
zero data retention policies by Large Language Model businesses like Open AI
and Anthropic and Meta. In this work, we explore zero data retention policies
for the Enterprise apps of large language models (LLMs). Our key contribution
is defining the architectural, compliance, and usability trade-offs of such
systems in parallel. In this research work, we examine the development of
commercial AI assistants with two industry leaders and market titans in this
arena - Salesforce and Microsoft. Both of these companies used distinct
technical architecture to support zero data retention policies. Salesforce
AgentForce and Microsoft Copilot are among the leading AI assistants providing
much-needed push to business productivity in customer care. The purpose of this
paper is to analyze the technical architecture and deployment of zero data
retention policy by consuming applications as well as big language models
service providers like Open Ai, Anthropic, and Meta.

</details>


### [162] [Analyzing and Internalizing Complex Policy Documents for LLM Agents](https://arxiv.org/abs/2510.11588)
*Jiateng Liu,Zhenhailong Wang,Xiaojiang Huang,Yingjie Li,Xing Fan,Xiang Li,Chenlei Guo,Ruhi Sarikaya,Heng Ji*

Main category: cs.AI

TL;DR: 提出了CC-Gen基准生成器和CAP-CPT方法，用于解决LLM代理系统中政策文档内部化问题，通过可控复杂度评估和政策分类预训练显著提升性能。


<details>
  <summary>Details</summary>
Motivation: LLM代理系统中的政策文档随着需求增长而快速膨胀，导致高计算开销，需要开发能够将政策文档嵌入模型先验的内部化方法。

Method: 引入CC-Gen基准生成器（四个可控复杂度级别）和CAP-CPT方法（自动解析政策文档、分类关键规范、针对性数据合成、自回归预训练损失）。

Result: CAP-CPT在所有设置中均优于SFT基线，在Qwen-3-32B上提升达41%和22%，在CC-Gen上实现97.3%的提示长度减少，并在tau-Bench上以最少SFT数据获得增强。

Conclusion: 复杂政策规范对推理构成主要挑战，CAP-CPT通过减轻数据和推理负担，有效实现了政策信息的内部化。

Abstract: Large Language Model (LLM)-based agentic systems rely on in-context policy
documents encoding diverse business rules. As requirements grow, these
documents expand rapidly, causing high computational overhead. This motivates
developing internalization methods that embed policy documents into model
priors while preserving performance. Prior prompt compression work targets
generic prompts, but agentic policy documents span multiple complexity levels
and require deeper reasoning, making internalization harder. We introduce
CC-Gen, an agentic benchmark generator with Controllable Complexity across four
levels, enabling systematic evaluation of agents' ability to handle complexity
and offering a unified framework for assessing policy internalization. Our
analysis shows that complex policy specifications governing workflows pose
major reasoning challenges. Supporting internalization with gold user agent
interaction trajectories containing chain-of-thought (CoT) annotations via
supervised fine-tuning (SFT) is data-intensive and degrades sharply as policy
complexity increases. To mitigate data and reasoning burdens, we propose
Category-Aware Policy Continued Pretraining (CAP-CPT). Our automated pipeline
parses policy documents to extract key specifications, grouping them into
factual, behavioral, and conditional categories, and isolating complex
conditions that drive workflow complexity. This guides targeted data synthesis
and enables agents to internalize policy information through an autoregressive
pretraining loss. Experiments show CAP-CPT improves SFT baselines in all
settings, with up to 41% and 22% gains on Qwen-3-32B, achieving 97.3% prompt
length reduction on CC-Gen and further enhancing tau-Bench with minimal SFT
data.

</details>


### [163] [Reproducibility: The New Frontier in AI Governance](https://arxiv.org/abs/2510.11595)
*Israel Mason-Williams,Gabryel Mason-Williams*

Main category: cs.AI

TL;DR: AI研究中的可复现性危机削弱了政策制定者的治理能力，需要采用预注册、提高统计功效和发表阴性结果等可复现性协议来改善AI治理。


<details>
  <summary>Details</summary>
Motivation: 当前AI研究的信息环境信噪比过低，存在监管捕获风险，政策制定者在优先治理哪些风险方面存在严重分歧和不确定性。

Method: 通过借鉴其他科学领域的危机经验，提出采用预注册、提高统计功效和发表阴性结果等可复现性协议。

Result: 论文论证了可复现性协议能够帮助政策制定者制定更有意义的政策和治理协议，改善对AI风险格局的共识。

Conclusion: 虽然AI治理必须具有反应性，但政策制定者应将可复现性协议视为治理工具箱的核心工具，并要求AI研究达到更高标准。

Abstract: AI policymakers are responsible for delivering effective governance
mechanisms that can provide safe, aligned and trustworthy AI development.
However, the information environment offered to policymakers is characterised
by an unnecessarily low Signal-To-Noise Ratio, favouring regulatory capture and
creating deep uncertainty and divides on which risks should be prioritised from
a governance perspective. We posit that the current publication speeds in AI
combined with the lack of strong scientific standards, via weak reproducibility
protocols, effectively erodes the power of policymakers to enact meaningful
policy and governance protocols. Our paper outlines how AI research could adopt
stricter reproducibility guidelines to assist governance endeavours and improve
consensus on the AI risk landscape. We evaluate the forthcoming reproducibility
crisis within AI research through the lens of crises in other scientific
domains; providing a commentary on how adopting preregistration, increased
statistical power and negative result publication reproducibility protocols can
enable effective AI governance. While we maintain that AI governance must be
reactive due to AI's significant societal implications we argue that
policymakers and governments must consider reproducibility protocols as a core
tool in the governance arsenal and demand higher standards for AI research.
Code to replicate data and figures:
https://github.com/IFMW01/reproducibility-the-new-frontier-in-ai-governance

</details>


### [164] [Explainability, risk modeling, and segmentation based customer churn analytics for personalized retention in e-commerce](https://arxiv.org/abs/2510.11604)
*Sanjula De Alwis,Indrajith Ekanayake*

Main category: cs.AI

TL;DR: 提出一个三组件框架，结合可解释AI、生存分析和RFM分析，用于客户流失预测和个性化保留策略设计


<details>
  <summary>Details</summary>
Motivation: 当前客户流失模型多为黑盒，缺乏对流失驱动因素、干预时机和高风险客户群体的洞察，需要从单纯预测转向基于可解释证据的个性化保留策略

Method: 集成可解释AI量化特征贡献、生存分析建模时间相关流失风险、RFM分析按交易行为细分客户的三组件框架

Result: 能够识别流失驱动因素、估计干预窗口、优先处理目标细分市场，支持减少客户流失和增强客户忠诚度的策略

Conclusion: 该综合方法超越了单纯的预测，为设计基于证据的个性化客户保留策略提供了可行框架

Abstract: In online retail, customer acquisition typically incurs higher costs than
customer retention, motivating firms to invest in churn analytics. However,
many contemporary churn models operate as opaque black boxes, limiting insight
into the determinants of attrition, the timing of retention opportunities, and
the identification of high-risk customer segments. Accordingly, the emphasis
should shift from prediction alone to the design of personalized retention
strategies grounded in interpretable evidence. This study advances a
three-component framework that integrates explainable AI to quantify feature
contributions, survival analysis to model time-to-event churn risk, and RFM
profiling to segment customers by transactional behaviour. In combination,
these methods enable the attribution of churn drivers, estimation of
intervention windows, and prioritization of segments for targeted actions,
thereby supporting strategies that reduce attrition and strengthen customer
loyalty.

</details>


### [165] [ParaCook: On Time-Efficient Planning for Multi-Agent Systems](https://arxiv.org/abs/2510.11608)
*Shiqi Zhang,Xinbei Ma,Yunqing Xu,Zouying Cao,Pengrui Lu,Haobo Yuan,Tiancheng Shen,Zhuosheng Zhang,Hai Zhao,Ming-Hsuan Yang*

Main category: cs.AI

TL;DR: ParaCook是一个用于评估时间效率协同规划的基准，基于Overcooked游戏设计，专注于多智能体系统的并行和异步操作规划能力。


<details>
  <summary>Details</summary>
Motivation: 现有智能体基准主要关注任务完成度，而忽略了并行和异步操作中的时间效率问题。ParaCook旨在填补这一空白，评估LLMs在时间效率协同规划方面的能力。

Method: 基于Overcooked游戏设计烹饪任务环境，简化动作空间以专注于战略并行规划的核心挑战。通过综合评估最先进的LLMs来分析其并行行动和协调能力。

Result: 当前方法生成的计划是次优的，在并行行动或协调方面表现不佳。但LLMs在抽象任务中显示出潜力，能够专注于高层并行优化。

Conclusion: ParaCook提供了一个可扩展的评估框架，为开发和时间效率感知的多智能体规划评估奠定了基础。

Abstract: Large Language Models (LLMs) exhibit strong reasoning abilities for planning
long-horizon, real-world tasks, yet existing agent benchmarks focus on task
completion while neglecting time efficiency in parallel and asynchronous
operations. To address this, we present ParaCook, a benchmark for
time-efficient collaborative planning. Inspired by the Overcooked game,
ParaCook provides an environment for various challenging interaction planning
of multi-agent systems that are instantiated as cooking tasks, with a
simplified action space to isolate the core challenge of strategic parallel
planning. Through a comprehensive evaluation of state-of-the-art LLMs, we find
that current approaches achieve suboptimal plans, which struggle with parallel
actions or coordination. Our analysis also reveals LLMs' potential on abstract
tasks where they can focus on high-level parallel optimization. ParaCook
provides a scalable evaluation framework with adjustable complexity,
establishing a foundation for developing and assessing time efficiency-aware
multi-agent planning. The code and data are available at
https://github.com/zsq259/ParaCook.

</details>


### [166] [SR-Scientist: Scientific Equation Discovery With Agentic AI](https://arxiv.org/abs/2510.11661)
*Shijie Xia,Yuhan Sun,Pengfei Liu*

Main category: cs.AI

TL;DR: SR-Scientist框架将LLM从简单的方程提议者提升为自主AI科学家，能够编写代码分析数据、实现方程、提交评估，并根据实验反馈优化方程，在四个科学领域的数据集上表现优于基线方法6%-35%。


<details>
  <summary>Details</summary>
Motivation: 当前方法通常将LLM限制为遗传编程等搜索算法中的方程提议者角色，未能充分发挥其作为自主科学家的潜力。

Method: 将代码解释器封装为数据分析和方程评估工具集，让智能体在最小人工干预下长期使用这些工具优化方程，并开发端到端强化学习框架增强智能体能力。

Result: 在四个科学领域的数据集上，SR-Scientist比基线方法绝对优势达6%-35%，展示了抗噪声能力、发现方程在域外数据的泛化性以及符号准确性。

Conclusion: SR-Scientist成功将LLM转变为自主AI科学家，显著提升了科学方程发现的性能，并证明了其鲁棒性和泛化能力。

Abstract: Recently, Large Language Models (LLMs) have been applied to scientific
equation discovery, leveraging their embedded scientific knowledge for
hypothesis generation. However, current methods typically confine LLMs to the
role of an equation proposer within search algorithms like genetic programming.
In this paper, we present SR-Scientist, a framework that elevates the LLM from
a simple equation proposer to an autonomous AI scientist that writes code to
analyze data, implements the equation as code, submits it for evaluation, and
optimizes the equation based on experimental feedback. Specifically, we wrap
the code interpreter into a set of tools for data analysis and equation
evaluation. The agent is instructed to optimize the equation by utilizing these
tools over a long horizon with minimal human-defined pipelines. Empirical
results show that SR-Scientist outperforms baseline methods by an absolute
margin of 6% to 35% on datasets covering four science disciplines.
Additionally, we demonstrate our method's robustness to noise, the
generalization of the discovered equations to out-of-domain data, and their
symbolic accuracy. Furthermore, we develop an end-to-end reinforcement learning
framework to enhance the agent's capabilities.

</details>


### [167] [Operand Quant: A Single-Agent Architecture for Autonomous Machine Learning Engineering](https://arxiv.org/abs/2510.11694)
*Arjun Sahney,Ram Gorthi,Cezary Łastowski,Javier Vega*

Main category: cs.AI

TL;DR: Operand Quant是一种基于IDE的单智能体架构，用于自主机器学习工程，在MLE-Benchmark上创造了新的最先进结果。


<details>
  <summary>Details</summary>
Motivation: 传统的多智能体编排框架在机器学习工程生命周期中存在协调复杂性，Operand Quant旨在通过单一智能体整合所有MLE阶段来简化流程。

Method: 采用单智能体、IDE基础的架构，将探索、建模、实验和部署等所有MLE生命周期阶段整合到一个上下文感知的智能体中，采用线性非阻塞的自主操作方式。

Result: 在MLE-Benchmark (2025)上获得新SOTA结果，整体奖牌率为0.3956 +/- 0.0565，在75个问题中表现最佳，超越了所有评估的多智能体和编排系统。

Conclusion: 在受控IDE环境中运行的线性非阻塞单智能体，在相同约束条件下能够超越多智能体和编排系统，证明了单智能体架构的有效性。

Abstract: We present Operand Quant, a single-agent, IDE-based architecture for
autonomous machine learning engineering (MLE). Operand Quant departs from
conventional multi-agent orchestration frameworks by consolidating all MLE
lifecycle stages -- exploration, modeling, experimentation, and deployment --
within a single, context-aware agent. On the MLE-Benchmark (2025), Operand
Quant achieved a new state-of-the-art (SOTA) result, with an overall medal rate
of 0.3956 +/- 0.0565 across 75 problems -- the highest recorded performance
among all evaluated systems to date. The architecture demonstrates that a
linear, non-blocking agent, operating autonomously within a controlled IDE
environment, can outperform multi-agent and orchestrated systems under
identical constraints.

</details>
