{"id": "2509.02578", "categories": ["cs.CR", "cs.PF"], "pdf": "https://arxiv.org/pdf/2509.02578", "abs": "https://arxiv.org/abs/2509.02578", "authors": ["Abel C. H. Chen"], "title": "Secure Password Generator Based on Secure Pseudo-Random Number Generator", "comment": "in Chinese language", "summary": "In recent years, numerous incidents involving the leakage of website accounts\nand text passwords (referred to as passwords) have raised significant concerns\nregarding the potential exposure of personal information. These events\nunderscore the critical importance of both information security and password\nprotection. While many of these breaches are attributable to vulnerabilities\nwithin website infrastructure, the strength and security of the passwords\nthemselves also play a crucial role. Consequently, the creation of secure\npasswords constitutes a fundamental aspect of enhancing overall system security\nand protecting personal data. In response to these challenges, this study\npresents a secure password generation approach utilizing a cryptographically\nsecure Pseudo-Random Number Generator (PRNG). The generator is implemented\nusing a range of Message Authentication Code (MAC) algorithms, including the\nKeyed-Hash Message Authentication Code (HMAC), Cipher-based Message\nAuthentication Code (CMAC), and KECCAK Message Authentication Code (KMAC), to\nproduce robust random values suitable for password generation. To evaluate the\nproposed method, empirical assessments were conducted in accordance with the\nguidelines provided in the National Institute of Standards and Technology\n(NIST) Special Publication (SP) 800-90B. The evaluation focused on two primary\naspects: entropy estimation and verification of independent and identically\ndistributed (IID) properties. Experimental results indicate that the proposed\nmethod satisfies both entropy and IID requirements, thereby demonstrating its\nability to generate passwords with a high degree of randomness and security."}
{"id": "2509.02856", "categories": ["cs.CR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.02856", "abs": "https://arxiv.org/abs/2509.02856", "authors": ["Syomantak Chaudhuri", "Thomas A. Courtade"], "title": "Managing Correlations in Data and Privacy Demand", "comment": "To appeat at ACM CCS, 2025", "summary": "Previous works in the differential privacy literature that allow users to\nchoose their privacy levels typically operate under the heterogeneous\ndifferential privacy (HDP) framework with the simplifying assumption that user\ndata and privacy levels are not correlated. Firstly, we demonstrate that the\nstandard HDP framework falls short when user data and privacy demands are\nallowed to be correlated. Secondly, to address this shortcoming, we propose an\nalternate framework, Add-remove Heterogeneous Differential Privacy (AHDP), that\njointly accounts for user data and privacy preference. We show that AHDP is\nrobust to possible correlations between data and privacy. Thirdly, we formalize\nthe guarantees of the proposed AHDP framework through an operational hypothesis\ntesting perspective. The hypothesis testing setup may be of independent\ninterest in analyzing other privacy frameworks as well. Fourthly, we show that\nthere exists non-trivial AHDP mechanisms that notably do not require prior\nknowledge of the data-privacy correlations. We propose some such mechanisms and\napply them to core statistical tasks such as mean estimation, frequency\nestimation, and linear regression. The proposed mechanisms are simple to\nimplement with minimal assumptions and modeling requirements, making them\nattractive for real-world use. Finally, we empirically evaluate proposed AHDP\nmechanisms, highlighting their trade-offs using LLM-generated synthetic\ndatasets, which we release for future research."}
{"id": "2509.03024", "categories": ["cs.CR", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.03024", "abs": "https://arxiv.org/abs/2509.03024", "authors": ["Moontaha Nishat Chowdhury", "André Bauer", "Minxuan Zhou"], "title": "Efficient Privacy-Preserving Recommendation on Sparse Data using Fully Homomorphic Encryption", "comment": "The paper is accepted at the 21st IEEE International eScience\n  Conference (eScience'25) and will be published soon. Link:\n  https://www.escience-conference.org/2025/papers", "summary": "In today's data-driven world, recommendation systems personalize user\nexperiences across industries but rely on sensitive data, raising privacy\nconcerns. Fully homomorphic encryption (FHE) can secure these systems, but a\nsignificant challenge in applying FHE to recommendation systems is efficiently\nhandling the inherently large and sparse user-item rating matrices. FHE\noperations are computationally intensive, and naively processing various sparse\nmatrices in recommendation systems would be prohibitively expensive.\nAdditionally, the communication overhead between parties remains a critical\nconcern in encrypted domains. We propose a novel approach combining Compressed\nSparse Row (CSR) representation with FHE-based matrix factorization that\nefficiently handles matrix sparsity in the encrypted domain while minimizing\ncommunication costs. Our experimental results demonstrate high recommendation\naccuracy with encrypted data while achieving the lowest communication costs,\neffectively preserving user privacy."}
{"id": "2509.03037", "categories": ["cs.CR", "cs.ET", "cs.SE"], "pdf": "https://arxiv.org/pdf/2509.03037", "abs": "https://arxiv.org/abs/2509.03037", "authors": ["Shuzheng Wang", "Yue Huang", "Zhuoer Xu", "Yuming Huang", "Jing Tang"], "title": "TraceLLM: Security Diagnosis Through Traces and Smart Contracts in Ethereum", "comment": null, "summary": "Ethereum smart contracts hold tens of billions of USD in DeFi and NFTs, yet\ncomprehensive security analysis remains difficult due to unverified code,\nproxy-based architectures, and the reliance on manual inspection of complex\nexecution traces. Existing approaches fall into two main categories: anomaly\ntransaction detection, which flags suspicious transactions but offers limited\ninsight into specific attack strategies hidden in execution traces inside\ntransactions, and code vulnerability detection, which cannot analyze unverified\ncontracts and struggles to show how identified flaws are exploited in real\nincidents. As a result, analysts must still manually align transaction traces\nwith contract code to reconstruct attack scenarios and conduct forensics. To\naddress this gap, TraceLLM is proposed as a framework that leverages LLMs to\nintegrate execution trace-level detection with decompiled contract code. We\nintroduce a new anomaly execution path identification algorithm and an\nLLM-refined decompile tool to identify vulnerable functions and provide\nexplicit attack paths to LLM. TraceLLM establishes the first benchmark for\njoint trace and contract code-driven security analysis. For comparison, proxy\nbaselines are created by jointly transmitting the results of three\nrepresentative code analysis along with raw traces to LLM. TraceLLM identifies\nattacker and victim addresses with 85.19\\% precision and produces automated\nreports with 70.37\\% factual precision across 27 cases with ground truth expert\nreports, achieving 25.93\\% higher accuracy than the best baseline. Moreover,\nacross 148 real-world Ethereum incidents, TraceLLM automatically generates\nreports with 66.22\\% expert-verified accuracy, demonstrating strong\ngeneralizability."}
{"id": "2509.02860", "categories": ["cs.SE", "cs.LO"], "pdf": "https://arxiv.org/pdf/2509.02860", "abs": "https://arxiv.org/abs/2509.02860", "authors": ["Connor Wojtak", "Darek Gajewski", "Tomas Cerny"], "title": "Vision: An Extensible Methodology for Formal Software Verification in Microservice Systems", "comment": "Accepted at MODELS 2025", "summary": "Microservice systems are becoming increasingly adopted due to their\nscalability, decentralized development, and support for continuous integration\nand delivery (CI/CD). However, this decentralized development by separate teams\nand continuous evolution can introduce miscommunication and incompatible\nimplementations, undermining system maintainability and reliability across\naspects from security policy to system architecture. We propose a novel\nmethodology that statically reconstructs microservice source code into a formal\nsystem model. From this model, a Satisfiability Modulo Theories (SMT)\nconstraint set can be derived, enabling formal verification. Our methodology is\nextensible, supporting software verification across multiple cross-cutting\nconcerns. We focus on applying the methodology to verify the system\narchitecture concern, presenting formal reasoning to validate the methodology's\ncorrectness and applicability for this concern. Additional concerns such as\nsecurity policy implementation are considered. Future directions are\nestablished to extend and evaluate the methodology."}
{"id": "2509.02650", "categories": ["cs.AI", "cs.GT", "q-bio.PE"], "pdf": "https://arxiv.org/pdf/2509.02650", "abs": "https://arxiv.org/abs/2509.02650", "authors": ["Henrique Correia da Fonseca", "António Fernandes", "Zhao Song", "Theodor Cimpeanu", "Nataliya Balabanova", "Adeela Bashir", "Paolo Bova", "Alessio Buscemi", "Alessandro Di Stefano", "Manh Hong Duong", "Elias Fernandez Domingos", "Ndidi Bianca Ogbo", "Simon T. Powers", "Daniele Proverbio", "Zia Ush Shamszaman", "Fernando P. Santos", "The Anh Han", "Marcus Krellner"], "title": "Can Media Act as a Soft Regulator of Safe AI Development? A Game Theoretical Analysis", "comment": "10 Pages, 7 Figures, accepted in the ALIFE 2025 Conference", "summary": "When developers of artificial intelligence (AI) products need to decide\nbetween profit and safety for the users, they likely choose profit.\nUntrustworthy AI technology must come packaged with tangible negative\nconsequences. Here, we envisage those consequences as the loss of reputation\ncaused by media coverage of their misdeeds, disseminated to the public. We\nexplore whether media coverage has the potential to push AI creators into the\nproduction of safe products, enabling widespread adoption of AI technology. We\ncreated artificial populations of self-interested creators and users and\nstudied them through the lens of evolutionary game theory. Our results reveal\nthat media is indeed able to foster cooperation between creators and users, but\nnot always. Cooperation does not evolve if the quality of the information\nprovided by the media is not reliable enough, or if the costs of either\naccessing media or ensuring safety are too high. By shaping public perception\nand holding developers accountable, media emerges as a powerful soft regulator\n-- guiding AI safety even in the absence of formal government oversight."}
{"id": "2509.03058", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.03058", "abs": "https://arxiv.org/abs/2509.03058", "authors": ["Zhenhua Xu", "Meng Han", "Wenpeng Xing"], "title": "EverTracer: Hunting Stolen Large Language Models via Stealthy and Robust Probabilistic Fingerprint", "comment": "Accepted by EMNLP2025 Main", "summary": "The proliferation of large language models (LLMs) has intensified concerns\nover model theft and license violations, necessitating robust and stealthy\nownership verification. Existing fingerprinting methods either require\nimpractical white-box access or introduce detectable statistical anomalies. We\npropose EverTracer, a novel gray-box fingerprinting framework that ensures\nstealthy and robust model provenance tracing. EverTracer is the first to\nrepurpose Membership Inference Attacks (MIAs) for defensive use, embedding\nownership signals via memorization instead of artificial trigger-output\noverfitting. It consists of Fingerprint Injection, which fine-tunes the model\non any natural language data without detectable artifacts, and Verification,\nwhich leverages calibrated probability variation signal to distinguish\nfingerprinted models. This approach remains robust against adaptive\nadversaries, including input level modification, and model-level modifications.\nExtensive experiments across architectures demonstrate EverTracer's\nstate-of-the-art effectiveness, stealthness, and resilience, establishing it as\na practical solution for securing LLM intellectual property. Our code and data\nare publicly available at https://github.com/Xuzhenhua55/EverTracer."}
{"id": "2509.03093", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.03093", "abs": "https://arxiv.org/abs/2509.03093", "authors": ["Fatih Pehlivan", "Arçin Ülkü Ergüzen", "Sahand Moslemi Yengejeh", "Mayasah Lami", "Anil Koyuncu"], "title": "Are We SOLID Yet? An Empirical Study on Prompting LLMs to Detect Design Principle Violations", "comment": "Accepted to ASE2025", "summary": "Traditional static analysis methods struggle to detect semantic design flaws,\nsuch as violations of the SOLID principles, which require a strong\nunderstanding of object-oriented design patterns and principles. Existing\nsolutions typically focus on individual SOLID principles or specific\nprogramming languages, leaving a gap in the ability to detect violations across\nall five principles in multi-language codebases. This paper presents a new\napproach: a methodology that leverages tailored prompt engineering to assess\nLLMs on their ability to detect SOLID violations across multiple languages. We\npresent a benchmark of four leading LLMs-CodeLlama, DeepSeekCoder, QwenCoder,\nand GPT-4o Mini-on their ability to detect violations of all five SOLID\nprinciples. For this evaluation, we construct a new benchmark dataset of 240\nmanually validated code examples. Using this dataset, we test four distinct\nprompt strategies inspired by established zero-shot, few-shot, and\nchain-of-thought techniques to systematically measure their impact on detection\naccuracy. Our emerging results reveal a stark hierarchy among models, with\nGPT-4o Mini decisively outperforming others, yet even struggles with\nchallenging principles like DIP. Crucially, we show that prompt strategy has a\ndramatic impact, but no single strategy is universally best; for instance, a\ndeliberative ENSEMBLE prompt excels at OCP detection while a hint-based EXAMPLE\nprompt is superior for DIP violations. Across all experiments, detection\naccuracy is heavily influenced by language characteristics and degrades sharply\nwith increasing code complexity. These initial findings demonstrate that\neffective, AI-driven design analysis requires not a single best model, but a\ntailored approach that matches the right model and prompt to the specific\ndesign context, highlighting the potential of LLMs to support maintainability\nthrough AI-assisted code analysis."}
{"id": "2509.02661", "categories": ["cs.AI", "astro-ph.IM", "cond-mat.mtrl-sci", "cs.LG", "physics.data-an"], "pdf": "https://arxiv.org/pdf/2509.02661", "abs": "https://arxiv.org/abs/2509.02661", "authors": ["Andrew Ferguson", "Marisa LaFleur", "Lars Ruthotto", "Jesse Thaler", "Yuan-Sen Ting", "Pratyush Tiwary", "Soledad Villar", "E. Paulo Alves", "Jeremy Avigad", "Simon Billinge", "Camille Bilodeau", "Keith Brown", "Emmanuel Candes", "Arghya Chattopadhyay", "Bingqing Cheng", "Jonathan Clausen", "Connor Coley", "Andrew Connolly", "Fred Daum", "Sijia Dong", "Chrisy Xiyu Du", "Cora Dvorkin", "Cristiano Fanelli", "Eric B. Ford", "Luis Manuel Frutos", "Nicolás García Trillos", "Cecilia Garraffo", "Robert Ghrist", "Rafael Gomez-Bombarelli", "Gianluca Guadagni", "Sreelekha Guggilam", "Sergei Gukov", "Juan B. Gutiérrez", "Salman Habib", "Johannes Hachmann", "Boris Hanin", "Philip Harris", "Murray Holland", "Elizabeth Holm", "Hsin-Yuan Huang", "Shih-Chieh Hsu", "Nick Jackson", "Olexandr Isayev", "Heng Ji", "Aggelos Katsaggelos", "Jeremy Kepner", "Yannis Kevrekidis", "Michelle Kuchera", "J. Nathan Kutz", "Branislava Lalic", "Ann Lee", "Matt LeBlanc", "Josiah Lim", "Rebecca Lindsey", "Yongmin Liu", "Peter Y. Lu", "Sudhir Malik", "Vuk Mandic", "Vidya Manian", "Emeka P. Mazi", "Pankaj Mehta", "Peter Melchior", "Brice Ménard", "Jennifer Ngadiuba", "Stella Offner", "Elsa Olivetti", "Shyue Ping Ong", "Christopher Rackauckas", "Philippe Rigollet", "Chad Risko", "Philip Romero", "Grant Rotskoff", "Brett Savoie", "Uros Seljak", "David Shih", "Gary Shiu", "Dima Shlyakhtenko", "Eva Silverstein", "Taylor Sparks", "Thomas Strohmer", "Christopher Stubbs", "Stephen Thomas", "Suriyanarayanan Vaikuntanathan", "Rene Vidal", "Francisco Villaescusa-Navarro", "Gregory Voth", "Benjamin Wandelt", "Rachel Ward", "Melanie Weber", "Risa Wechsler", "Stephen Whitelam", "Olaf Wiest", "Mike Williams", "Zhuoran Yang", "Yaroslava G. Yingling", "Bin Yu", "Shuwen Yue", "Ann Zabludoff", "Huimin Zhao", "Tong Zhang"], "title": "The Future of Artificial Intelligence and the Mathematical and Physical Sciences (AI+MPS)", "comment": "Community Paper from the Future of NSF AI+MPS Workshop, Cambridge,\n  Massachusetts, March 24-26, 2025, supported by NSF Award Number 2512945", "summary": "This community paper developed out of the NSF Workshop on the Future of\nArtificial Intelligence (AI) and the Mathematical and Physics Sciences (MPS),\nwhich was held in March 2025 with the goal of understanding how the MPS domains\n(Astronomy, Chemistry, Materials Research, Mathematical Sciences, and Physics)\ncan best capitalize on, and contribute to, the future of AI. We present here a\nsummary and snapshot of the MPS community's perspective, as of Spring/Summer\n2025, in a rapidly developing field. The link between AI and MPS is becoming\nincreasingly inextricable; now is a crucial moment to strengthen the link\nbetween AI and Science by pursuing a strategy that proactively and thoughtfully\nleverages the potential of AI for scientific discovery and optimizes\nopportunities to impact the development of AI by applying concepts from\nfundamental science. To achieve this, we propose activities and strategic\npriorities that: (1) enable AI+MPS research in both directions; (2) build up an\ninterdisciplinary community of AI+MPS researchers; and (3) foster education and\nworkforce development in AI for MPS researchers and students. We conclude with\na summary of suggested priorities for funding agencies, educational\ninstitutions, and individual researchers to help position the MPS community to\nbe a leader in, and take full advantage of, the transformative potential of\nAI+MPS."}
{"id": "2509.03098", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.03098", "abs": "https://arxiv.org/abs/2509.03098", "authors": ["Gustavo Banegas", "Anaëlle Le Dévéhat", "Benjamin Smith"], "title": "Compressed verification for post-quantum signatures with long-term public keys", "comment": null, "summary": "Many signature applications-such as root certificates, secure software\nupdates, and authentication protocols-involve long-lived public keys that are\ntransferred or installed once and then used for many verifications. This key\nlongevity makes post-quantum signature schemes with conservative assumptions\n(e.g., structure-free lattices) attractive for long-term security. But many\nsuch schemes, especially those with short signatures, suffer from extremely\nlarge public keys. Even in scenarios where bandwidth is not a major concern,\nlarge keys increase storage costs and slow down verification. We address this\nwith a method to replace large public keys in GPV-style signatures with\nsmaller, private verification keys. This significantly reduces verifier storage\nand runtime while preserving security. Applied to the conservative,\nshort-signature schemes Wave and Squirrels, our method compresses Squirrels-I\nkeys from 665 kB to 20.7 kB and Wave822 keys from 3.5 MB to 207.97 kB."}
{"id": "2509.03270", "categories": ["cs.SE", "cs.RO"], "pdf": "https://arxiv.org/pdf/2509.03270", "abs": "https://arxiv.org/abs/2509.03270", "authors": ["Martin Skoglund", "Fredrik Warg", "Aria Mirzai", "Anders Thorsen", "Karl Lundgren", "Peter Folkesson", "Bastian Havers-zulka"], "title": "AI Safety Assurance in Electric Vehicles: A Case Study on AI-Driven SOC Estimation", "comment": "12 pages, 9 figures, EVS38,\n  https://evs38-program.org/en/evs-38-proceedings/all", "summary": "Integrating Artificial Intelligence (AI) technology in electric vehicles (EV)\nintroduces unique challenges for safety assurance, particularly within the\nframework of ISO 26262, which governs functional safety in the automotive\ndomain. Traditional assessment methodologies are not geared toward evaluating\nAI-based functions and require evolving standards and practices. This paper\nexplores how an independent assessment of an AI component in an EV can be\nachieved when combining ISO 26262 with the recently released ISO/PAS 8800,\nwhose scope is AI safety for road vehicles. The AI-driven State of Charge (SOC)\nbattery estimation exemplifies the process. Key features relevant to the\nindependent assessment of this extended evaluation approach are identified. As\npart of the evaluation, robustness testing of the AI component is conducted\nusing fault injection experiments, wherein perturbed sensor inputs are\nsystematically introduced to assess the component's resilience to input\nvariance."}
{"id": "2509.02722", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.02722", "abs": "https://arxiv.org/abs/2509.02722", "authors": ["Delong Chen", "Theo Moutakanni", "Willy Chung", "Yejin Bang", "Ziwei Ji", "Allen Bolourchi", "Pascale Fung"], "title": "Planning with Reasoning using Vision Language World Model", "comment": null, "summary": "Effective planning requires strong world models, but high-level world models\nthat can understand and reason about actions with semantic and temporal\nabstraction remain largely underdeveloped. We introduce the Vision Language\nWorld Model (VLWM), a foundation model trained for language-based world\nmodeling on natural videos. Given visual observations, the VLWM first infers\nthe overall goal achievements then predicts a trajectory composed of\ninterleaved actions and world state changes. Those targets are extracted by\niterative LLM Self-Refine conditioned on compressed future observations\nrepresented by Tree of Captions. The VLWM learns both an action policy and a\ndynamics model, which respectively facilitates reactive system-1 plan decoding\nand reflective system-2 planning via cost minimization. The cost evaluates the\nsemantic distance between the hypothetical future states given by VLWM\nroll-outs and the expected goal state, and is measured by a critic model that\nwe trained in a self-supervised manner. The VLWM achieves state-of-the-art\nVisual Planning for Assistance (VPA) performance on both benchmark evaluations\nand our proposed PlannerArena human evaluations, where system-2 improves the\nElo score by +27% upon system-1. The VLWM models also outperforms strong VLM\nbaselines on RoboVQA and WorldPrediction benchmark."}
{"id": "2509.03117", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.03117", "abs": "https://arxiv.org/abs/2509.03117", "authors": ["Yuchen Yang", "Yiming Li", "Hongwei Yao", "Enhao Huang", "Shuo Shao", "Bingrun Yang", "Zhibo Wang", "Dacheng Tao", "Zhan Qin"], "title": "PromptCOS: Towards System Prompt Copyright Auditing for LLMs via Content-level Output Similarity", "comment": null, "summary": "The rapid progress of large language models (LLMs) has greatly enhanced\nreasoning tasks and facilitated the development of LLM-based applications. A\ncritical factor in improving LLM-based applications is the design of effective\nsystem prompts, which significantly impact the behavior and output quality of\nLLMs. However, system prompts are susceptible to theft and misuse, which could\nundermine the interests of prompt owners. Existing methods protect prompt\ncopyrights through watermark injection and verification but face challenges due\nto their reliance on intermediate LLM outputs (e.g., logits), which limits\ntheir practical feasibility.\n  In this paper, we propose PromptCOS, a method for auditing prompt copyright\nbased on content-level output similarity. It embeds watermarks by optimizing\nthe prompt while simultaneously co-optimizing a special verification query and\ncontent-level signal marks. This is achieved by leveraging cyclic output\nsignals and injecting auxiliary tokens to ensure reliable auditing in\ncontent-only scenarios. Additionally, it incorporates cover tokens to protect\nthe watermark from malicious deletion. For copyright verification, PromptCOS\nidentifies unauthorized usage by comparing the similarity between the\nsuspicious output and the signal mark. Experimental results demonstrate that\nour method achieves high effectiveness (99.3% average watermark similarity),\nstrong distinctiveness (60.8% greater than the best baseline), high fidelity\n(accuracy degradation of no more than 0.58%), robustness (resilience against\nthree types of potential attacks), and computational efficiency (up to 98.1%\nreduction in computational cost). Our code is available at GitHub\nhttps://github.com/LianPing-cyber/PromptCOS."}
{"id": "2509.03331", "categories": ["cs.SE", "cs.CR"], "pdf": "https://arxiv.org/pdf/2509.03331", "abs": "https://arxiv.org/abs/2509.03331", "authors": ["Weizhe Wang", "Wei Ma", "Qiang Hu", "Yao Zhang", "Jianfei Sun", "Bin Wu", "Yang Liu", "Guangquan Xu", "Lingxiao Jiang"], "title": "VulnRepairEval: An Exploit-Based Evaluation Framework for Assessing Large Language Model Vulnerability Repair Capabilities", "comment": null, "summary": "The adoption of Large Language Models (LLMs) for automated software\nvulnerability patching has shown promising outcomes on carefully curated\nevaluation sets. Nevertheless, existing datasets predominantly rely on\nsuperficial validation methods rather than exploit-based verification, leading\nto overestimated performance in security-sensitive applications. This paper\nintroduces VulnRepairEval, an evaluation framework anchored in functional\nProof-of-Concept (PoC) exploits. Our framework delivers a comprehensive,\ncontainerized evaluation pipeline that enables reproducible differential\nassessment, where repair success requires the original exploit to fail\nexecution against the modified code. The benchmark construction involved\nextensive data curation: we processed over 400 CVEs and approximately 2,500\npotential sources to extract a collection of authentic vulnerability instances\n(23 Python CVEs) amenable to automated testing with working PoCs. Through\nVulnRepairEval, we conduct a comprehensive evaluation of 12 popular LLMs and\nobserve a significant performance deficit: even the top-performing model\nsuccessfully addresses merely 5/23 instances (about 21.7%), exposing critical\nweaknesses in security-focused applications. Our failure analysis reveals that\nmost unsuccessful attempts stem from imprecise vulnerability identification and\npatches containing syntactic or semantic errors. Enhanced prompting strategies\nand multi-agent approaches yield minimal improvements, with overall\neffectiveness remaining largely unaffected. This work contributes a stringent,\npractical evaluation framework for LLM-driven vulnerability remediation and\nunderscores the necessity for assessment protocols that authentically reflect\nreal-world exploitation scenarios."}
{"id": "2509.02751", "categories": ["cs.AI", "cs.DB", "cs.LG", "cs.MA", "I.2.1; H.3.3; H.2.4"], "pdf": "https://arxiv.org/pdf/2509.02751", "abs": "https://arxiv.org/abs/2509.02751", "authors": ["Matthew Russo", "Tim Kraska"], "title": "Deep Research is the New Analytics System: Towards Building the Runtime for AI-Driven Analytics", "comment": "6 pages, 2 figures, submitted to CIDR'26", "summary": "With advances in large language models (LLMs), researchers are creating new\nsystems that can perform AI-driven analytics over large unstructured datasets.\nRecent work has explored executing such analytics queries using semantic\noperators -- a declarative set of AI-powered data transformations with natural\nlanguage specifications. However, even when optimized, these operators can be\nexpensive to execute on millions of records and their iterator execution\nsemantics make them ill-suited for interactive data analytics tasks. In another\nline of work, Deep Research systems have demonstrated an ability to answer\nnatural language question(s) over large datasets. These systems use one or more\nLLM agent(s) to plan their execution, process the dataset(s), and iteratively\nrefine their answer. However, these systems do not explicitly optimize their\nquery plans which can lead to poor plan execution. In order for AI-driven\nanalytics to excel, we need a runtime which combines the optimized execution of\nsemantic operators with the flexibility and more dynamic execution of Deep\nResearch systems. As a first step towards this vision, we build a prototype\nwhich enables Deep Research agents to write and execute optimized semantic\noperator programs. We evaluate our prototype and demonstrate that it can\noutperform a handcrafted semantic operator program and open Deep Research\nsystems on two basic queries. Compared to a standard open Deep Research agent,\nour prototype achieves up to 1.95x better F1-score. Furthermore, even if we\ngive the agent access to semantic operators as tools, our prototype still\nachieves cost and runtime savings of up to 76.8% and 72.7% thanks to its\noptimized execution."}
{"id": "2509.03123", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.03123", "abs": "https://arxiv.org/abs/2509.03123", "authors": ["Wei Xu", "Hui Zhu", "Yandong Zheng", "Song Bian", "Ning Sun", "Hao Yuan", "Dengguo Feng", "Hui Li"], "title": "Kangaroo: A Private and Amortized Inference Framework over WAN for Large-Scale Decision Tree Evaluation", "comment": null, "summary": "With the rapid adoption of Models-as-a-Service, concerns about data and model\nprivacy have become increasingly critical. To solve these problems, various\nprivacy-preserving inference schemes have been proposed. In particular, due to\nthe efficiency and interpretability of decision trees, private decision tree\nevaluation (PDTE) has garnered significant attention. However, existing PDTE\nschemes suffer from significant limitations: their communication and\ncomputation costs scale with the number of trees, the number of nodes, or the\ntree depth, which makes them inefficient for large-scale models, especially\nover WAN networks. To address these issues, we propose Kangaroo, a private and\namortized decision tree inference framework build upon packed homomorphic\nencryption. Specifically, we design a novel model hiding and encoding scheme,\ntogether with secure feature selection, oblivious comparison, and secure path\nevaluation protocols, enabling full amortization of the overhead as the number\nof nodes or trees scales. Furthermore, we enhance the performance and\nfunctionality of the framework through optimizations, including\nsame-sharing-for-same-model, latency-aware, and adaptive encoding adjustment\nstrategies. Kangaroo achieves a $14\\times$ to $59\\times$ performance\nimprovement over state-of-the-art (SOTA) one-round interactive schemes in WAN\nenvironments. For large-scale decision tree inference tasks, it delivers a\n$3\\times$ to $44\\times$ speedup compared to existing schemes. Notably, Kangaroo\nenables the evaluation of a random forest with $969$ trees and $411825$ nodes\nin approximately $60$ ms per tree (amortized) under WAN environments."}
{"id": "2509.03463", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.03463", "abs": "https://arxiv.org/abs/2509.03463", "authors": ["Parham Khamsepour", "Mark Cole", "Ish Ashraf", "Sandeep Puri", "Mehrdad Sabetzadeh", "Shiva Nejati"], "title": "The Impact of Critique on LLM-Based Model Generation from Natural Language: The Case of Activity Diagrams", "comment": null, "summary": "Large Language Models (LLMs) show strong potential for automating the\ngeneration of models from natural-language descriptions. A common approach is\nan iterative generate-critique-refine loop, where candidate models are\nproduced, evaluated, and updated based on detected issues. This process needs\nto address: (1) structural correctness - compliance with well-formedness rules\n- and (2) semantic alignment - accurate reflection of the intended meaning in\nthe source text. We present LADEX (LLM-based Activity Diagram Extractor), a\npipeline for deriving activity diagrams from natural-language process\ndescriptions using an LLM-driven critique-refine process. Structural checks in\nLADEX can be performed either algorithmically or by an LLM, while alignment\nchecks are always performed by an LLM. We design five ablated variants of LADEX\nto study: (i) the impact of the critique-refine loop itself, (ii) the role of\nLLM-based semantic checks, and (iii) the comparative effectiveness of\nalgorithmic versus LLM-based structural checks.\n  To evaluate LADEX, we compare the generated activity diagrams with\nexpert-created ground truths using trace-based operational semantics. This\nenables automated measurement of correctness and completeness. Experiments on\ntwo datasets indicate that: (1) the critique-refine loop improves structural\nvalidity, correctness, and completeness compared to single-pass generation; (2)\nalgorithmic structural checks eliminate inconsistencies that LLM-based checks\nfail to detect, improving correctness by an average of 17.81% and completeness\nby 13.24% over LLM-only checks; and (3) combining algorithmic structural checks\nwith LLM-based semantic checks, implemented using the reasoning-focused O4\nMini, achieves the best overall performance - yielding average correctness of\nup to 86.37% and average completeness of up to 88.56% - while requiring fewer\nthan five LLM calls on average."}
{"id": "2509.02754", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.02754", "abs": "https://arxiv.org/abs/2509.02754", "authors": ["Mingyi Wang", "Jingke Wang", "Tengju Ye", "Junbo Chen", "Kaicheng Yu"], "title": "Do LLM Modules Generalize? A Study on Motion Generation for Autonomous Driving", "comment": "CoRL 2025", "summary": "Recent breakthroughs in large language models (LLMs) have not only advanced\nnatural language processing but also inspired their application in domains with\nstructurally similar problems--most notably, autonomous driving motion\ngeneration. Both domains involve autoregressive sequence modeling, token-based\nrepresentations, and context-aware decision making, making the transfer of LLM\ncomponents a natural and increasingly common practice. However, despite\npromising early attempts, a systematic understanding of which LLM modules are\ntruly transferable remains lacking. In this paper, we present a comprehensive\nevaluation of five key LLM modules--tokenizer design, positional embedding,\npre-training paradigms, post-training strategies, and test-time\ncomputation--within the context of motion generation for autonomous driving.\nThrough extensive experiments on the Waymo Sim Agents benchmark, we demonstrate\nthat, when appropriately adapted, these modules can significantly improve\nperformance for autonomous driving motion generation. In addition, we identify\nwhich techniques can be effectively transferred, analyze the potential reasons\nfor the failure of others, and discuss the specific adaptations needed for\nautonomous driving scenarios. We evaluate our method on the Sim Agents task and\nachieve competitive results."}
{"id": "2509.03294", "categories": ["cs.CR", "cs.AI", "cs.LG", "68P27, 68T09, 94A60"], "pdf": "https://arxiv.org/pdf/2509.03294", "abs": "https://arxiv.org/abs/2509.03294", "authors": ["Napsu Karmitsa", "Antti Airola", "Tapio Pahikkala", "Tinja Pitkämäki"], "title": "A Comprehensive Guide to Differential Privacy: From Theory to User Expectations", "comment": null, "summary": "The increasing availability of personal data has enabled significant advances\nin fields such as machine learning, healthcare, and cybersecurity. However,\nthis data abundance also raises serious privacy concerns, especially in light\nof powerful re-identification attacks and growing legal and ethical demands for\nresponsible data use. Differential privacy (DP) has emerged as a principled,\nmathematically grounded framework for mitigating these risks. This review\nprovides a comprehensive survey of DP, covering its theoretical foundations,\npractical mechanisms, and real-world applications. It explores key algorithmic\ntools and domain-specific challenges - particularly in privacy-preserving\nmachine learning and synthetic data generation. The report also highlights\nusability issues and the need for improved communication and transparency in DP\nsystems. Overall, the goal is to support informed adoption of DP by researchers\nand practitioners navigating the evolving landscape of data privacy."}
{"id": "2509.03037", "categories": ["cs.CR", "cs.ET", "cs.SE"], "pdf": "https://arxiv.org/pdf/2509.03037", "abs": "https://arxiv.org/abs/2509.03037", "authors": ["Shuzheng Wang", "Yue Huang", "Zhuoer Xu", "Yuming Huang", "Jing Tang"], "title": "TraceLLM: Security Diagnosis Through Traces and Smart Contracts in Ethereum", "comment": null, "summary": "Ethereum smart contracts hold tens of billions of USD in DeFi and NFTs, yet\ncomprehensive security analysis remains difficult due to unverified code,\nproxy-based architectures, and the reliance on manual inspection of complex\nexecution traces. Existing approaches fall into two main categories: anomaly\ntransaction detection, which flags suspicious transactions but offers limited\ninsight into specific attack strategies hidden in execution traces inside\ntransactions, and code vulnerability detection, which cannot analyze unverified\ncontracts and struggles to show how identified flaws are exploited in real\nincidents. As a result, analysts must still manually align transaction traces\nwith contract code to reconstruct attack scenarios and conduct forensics. To\naddress this gap, TraceLLM is proposed as a framework that leverages LLMs to\nintegrate execution trace-level detection with decompiled contract code. We\nintroduce a new anomaly execution path identification algorithm and an\nLLM-refined decompile tool to identify vulnerable functions and provide\nexplicit attack paths to LLM. TraceLLM establishes the first benchmark for\njoint trace and contract code-driven security analysis. For comparison, proxy\nbaselines are created by jointly transmitting the results of three\nrepresentative code analysis along with raw traces to LLM. TraceLLM identifies\nattacker and victim addresses with 85.19\\% precision and produces automated\nreports with 70.37\\% factual precision across 27 cases with ground truth expert\nreports, achieving 25.93\\% higher accuracy than the best baseline. Moreover,\nacross 148 real-world Ethereum incidents, TraceLLM automatically generates\nreports with 66.22\\% expert-verified accuracy, demonstrating strong\ngeneralizability."}
{"id": "2509.02761", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.02761", "abs": "https://arxiv.org/abs/2509.02761", "authors": ["Ananth Hariharan", "Vardhan Dongre", "Dilek Hakkani-Tür", "Gokhan Tur"], "title": "Plan Verification for LLM-Based Embodied Task Completion Agents", "comment": null, "summary": "Large language model (LLM) based task plans and corresponding human\ndemonstrations for embodied AI may be noisy, with unnecessary actions,\nredundant navigation, and logical errors that reduce policy quality. We propose\nan iterative verification framework in which a Judge LLM critiques action\nsequences and a Planner LLM applies the revisions, yielding progressively\ncleaner and more spatially coherent trajectories. Unlike rule-based approaches,\nour method relies on natural language prompting, enabling broad generalization\nacross error types including irrelevant actions, contradictions, and missing\nsteps. On a set of manually annotated actions from the TEACh embodied AI\ndataset, our framework achieves up to 90% recall and 100% precision across four\nstate-of-the-art LLMs (GPT o4-mini, DeepSeek-R1, Gemini 2.5, LLaMA 4 Scout).\nThe refinement loop converges quickly, with 96.5% of sequences requiring at\nmost three iterations, while improving both temporal efficiency and spatial\naction organization. Crucially, the method preserves human error-recovery\npatterns rather than collapsing them, supporting future work on robust\ncorrective behavior. By establishing plan verification as a reliable LLM\ncapability for spatial planning and action refinement, we provide a scalable\npath to higher-quality training data for imitation learning in embodied AI."}
{"id": "2509.03350", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.03350", "abs": "https://arxiv.org/abs/2509.03350", "authors": ["Somiya Chhillar", "Mary K. Righi", "Rebecca E. Sutter", "Evgenios M. Kornaropoulos"], "title": "Exposing Privacy Risks in Anonymizing Clinical Data: Combinatorial Refinement Attacks on k-Anonymity Without Auxiliary Information", "comment": null, "summary": "Despite longstanding criticism from the privacy community, k-anonymity\nremains a widely used standard for data anonymization, mainly due to its\nsimplicity, regulatory alignment, and preservation of data utility. However,\nnon-experts often defend k-anonymity on the grounds that, in the absence of\nauxiliary information, no known attacks can compromise its protections. In this\nwork, we refute this claim by introducing Combinatorial Refinement Attacks\n(CRA), a new class of privacy attacks targeting k-anonymized datasets produced\nusing local recoding. This is the first method that does not rely on external\nauxiliary information or assumptions about the underlying data distribution.\nCRA leverages the utility-optimizing behavior of local recoding anonymization\nof ARX, which is a widely used open-source software for anonymizing data in\nclinical settings, to formulate a linear program that significantly reduces the\nspace of plausible sensitive values. To validate our findings, we partnered\nwith a network of free community health clinics, an environment where (1)\nauxiliary information is indeed hard to find due to the population they serve\nand (2) open-source k-anonymity solutions are attractive due to regulatory\nobligations and limited resources. Our results on real-world clinical microdata\nreveal that even in the absence of external information, established\nanonymization frameworks do not deliver the promised level of privacy, raising\ncritical privacy concerns."}
{"id": "2509.03310", "categories": ["cs.AI", "cs.SE"], "pdf": "https://arxiv.org/pdf/2509.03310", "abs": "https://arxiv.org/abs/2509.03310", "authors": ["Evgenii Kniazev", "Arseny Kravchenko", "Igor Rekun", "James Broadhead", "Nikita Shamgunov", "Pranav Sah", "Pratik Nichite", "Ivan Yamshchikov"], "title": "app.build: A Production Framework for Scaling Agentic Prompt-to-App Generation with Environment Scaffolding", "comment": null, "summary": "We present app.build (https://github.com/appdotbuild/agent/), an open-source\nframework that improves LLM-based application generation through systematic\nvalidation and structured environments. Our approach combines multi-layered\nvalidation pipelines, stack-specific orchestration, and model-agnostic\narchitecture, implemented across three reference stacks. Through evaluation on\n30 generation tasks, we demonstrate that comprehensive validation achieves\n73.3% viability rate with 30% reaching perfect quality scores, while\nopen-weights models achieve 80.8% of closed-model performance when provided\nstructured environments. The open-source framework has been adopted by the\ncommunity, with over 3,000 applications generated to date. This work\ndemonstrates that scaling reliable AI agents requires scaling environments, not\njust models -- providing empirical insights and complete reference\nimplementations for production-oriented agent systems."}
{"id": "2509.02782", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.02782", "abs": "https://arxiv.org/abs/2509.02782", "authors": ["Václav Sobotka", "Lucas Kletzander", "Nysret Musliu", "Hana Rudová"], "title": "Key Principles in Cross-Domain Hyper-Heuristic Performance", "comment": null, "summary": "Cross-domain selection hyper-heuristics aim to distill decades of research on\nproblem-specific heuristic search algorithms into adaptable general-purpose\nsearch strategies. In this respect, existing selection hyper-heuristics\nprimarily focus on an adaptive selection of low-level heuristics (LLHs) from a\npredefined set. In contrast, we concentrate on the composition of this set and\nits strategic transformations. We systematically analyze transformations based\non three key principles: solution acceptance, LLH repetitions, and perturbation\nintensity, i.e., the proportion of a solution affected by a perturbative LLH.\nWe demonstrate the raw effects of our transformations on a trivial unbiased\nrandom selection mechanism. With an appropriately constructed transformation,\nthis trivial method outperforms all available state-of-the-art hyper-heuristics\non three challenging real-world domains and finds 11 new best-known solutions.\nThe same method is competitive with the winner of the CHeSC competition,\ncommonly used as the standard cross-domain benchmark. Moreover, we accompany\nseveral recent hyper-heuristics with such strategic transformations. Using this\napproach, we outperform the current state-of-the-art methods on both the CHeSC\nbenchmark and real-world domains while often simplifying their designs."}
{"id": "2509.03367", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.03367", "abs": "https://arxiv.org/abs/2509.03367", "authors": ["Narges Dadkhah", "Somayeh Mohammadi", "Gerhard Wunder"], "title": "Tuning Block Size for Workload Optimization in Consortium Blockchain Networks", "comment": null, "summary": "Determining the optimal block size is crucial for achieving high throughput\nin blockchain systems. Many studies have focused on tuning various components,\nsuch as databases, network bandwidth, and consensus mechanisms. However, the\nimpact of block size on system performance remains a topic of debate, often\nresulting in divergent views and even leading to new forks in blockchain\nnetworks. This research proposes a mathematical model to maximize performance\nby determining the ideal block size for Hyperledger Fabric, a prominent\nconsortium blockchain. By leveraging machine learning and solving the model\nwith a genetic algorithm, the proposed approach assesses how factors such as\nblock size, transaction size, and network capacity influence the block\nprocessing time. The integration of an optimization solver enables precise\nadjustments to block size configuration before deployment, ensuring improved\nperformance from the outset. This systematic approach aims to balance block\nprocessing efficiency, network latency, and system throughput, offering a\nrobust solution to improve blockchain performance across diverse business\ncontexts."}
{"id": "2509.02794", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.02794", "abs": "https://arxiv.org/abs/2509.02794", "authors": ["Blai Bonet", "Hector Geffner"], "title": "Learning General Policies From Examples", "comment": null, "summary": "Combinatorial methods for learning general policies that solve large\ncollections of planning problems have been recently developed. One of their\nstrengths, in relation to deep learning approaches, is that the resulting\npolicies can be understood and shown to be correct. A weakness is that the\nmethods do not scale up and learn only from small training instances and\nfeature pools that contain a few hundreds of states and features at most. In\nthis work, we propose a new symbolic method for learning policies based on the\ngeneralization of sampled plans that ensures structural termination and hence\nacyclicity. The proposed learning approach is not based on SAT/ASP, as previous\nsymbolic methods, but on a hitting set algorithm that can effectively handle\nproblems with millions of states, and pools with hundreds of thousands of\nfeatures. The formal properties of the approach are analyzed, and its\nscalability is tested on a number of benchmarks."}
{"id": "2509.03427", "categories": ["cs.CR", "E.3; C.2.0; C.2.4"], "pdf": "https://arxiv.org/pdf/2509.03427", "abs": "https://arxiv.org/abs/2509.03427", "authors": ["Pedro Correia", "Ivan Silva", "Ivone Amorim", "Eva Maia", "Isabel Praça"], "title": "Federated Learning: An approach with Hybrid Homomorphic Encryption", "comment": "19 pages, 8 figures, To be published in the conference Security and\n  Trust Management(STM), ESORICS 2025", "summary": "Federated Learning (FL) is a distributed machine learning approach that\npromises privacy by keeping the data on the device. However, gradient\nreconstruction and membership-inference attacks show that model updates still\nleak information. Fully Homomorphic Encryption (FHE) can address those privacy\nconcerns but it suffers from ciphertext expansion and requires prohibitive\noverhead on resource-constrained devices. We propose the first Hybrid\nHomomorphic Encryption (HHE) framework for FL that pairs the PASTA symmetric\ncipher with the BFV FHE scheme. Clients encrypt local model updates with PASTA\nand send both the lightweight ciphertexts and the PASTA key (itself\nBFV-encrypted) to the server, which performs a homomorphic evaluation of the\ndecryption circuit of PASTA and aggregates the resulting BFV ciphertexts. A\nprototype implementation, developed on top of the Flower FL framework, shows\nthat on independently and identically distributed MNIST dataset with 12 clients\nand 10 training rounds, the proposed HHE system achieves 97.6% accuracy, just\n1.3% below plaintext, while reducing client upload bandwidth by over 2,000x and\ncutting client runtime by 30% compared to a system based solely on the BFV FHE\nscheme. However, server computational cost increases by roughly 15621x for each\nclient participating in the training phase, a challenge to be addressed in\nfuture work."}
{"id": "2509.03219", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.03219", "abs": "https://arxiv.org/abs/2509.03219", "authors": ["Leonidas Bakopoulos", "Georgios Chalkiadakis"], "title": "Uncertainty-driven Adaptive Exploration", "comment": null, "summary": "Adaptive exploration methods propose ways to learn complex policies via\nalternating between exploration and exploitation. An important question for\nsuch methods is to determine the appropriate moment to switch between\nexploration and exploitation and vice versa. This is critical in domains that\nrequire the learning of long and complex sequences of actions. In this work, we\npresent a generic adaptive exploration framework that employs uncertainty to\naddress this important issue in a principled manner. Our framework includes\nprevious adaptive exploration approaches as special cases. Moreover, we can\nincorporate in our framework any uncertainty-measuring mechanism of choice, for\ninstance mechanisms used in intrinsic motivation or epistemic uncertainty-based\nexploration methods. We experimentally demonstrate that our framework gives\nrise to adaptive exploration strategies that outperform standard ones across\nseveral MuJoCo environments."}
{"id": "2509.03442", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.03442", "abs": "https://arxiv.org/abs/2509.03442", "authors": ["Zhuoyun Qian", "Hongyi Miao", "Yili Jiang", "Qin Hu", "Jiaqi Huang", "Cheng Zhang", "Fangtian Zhong"], "title": "Evaluating Diverse Feature Extraction Techniques of Multifaceted IoT Malware Analysis: A Survey", "comment": null, "summary": "As IoT devices continue to proliferate, their reliability is increasingly\nconstrained by security concerns. In response, researchers have developed\ndiverse malware analysis techniques to detect and classify IoT malware. These\ntechniques typically rely on extracting features at different levels from IoT\napplications, giving rise to a wide range of feature extraction methods.\nHowever, current approaches still face significant challenges when applied in\npractice. This survey provides a comprehensive review of feature extraction\ntechniques for IoT malware analysis from multiple perspectives. We first\nexamine static and dynamic feature extraction methods, followed by hybrid\napproaches. We then explore feature representation strategies based on graph\nlearning. Finally, we compare the strengths and limitations of existing\ntechniques, highlight open challenges, and outline promising directions for\nfuture research."}
{"id": "2509.03286", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.03286", "abs": "https://arxiv.org/abs/2509.03286", "authors": ["Prachi Bagave", "Marcus Westberg", "Marijn Janssen", "Aaron Yi Ding"], "title": "Accountability Framework for Healthcare AI Systems: Towards Joint Accountability in Decision Making", "comment": "To be published in AAAI AIES 2025", "summary": "AI is transforming the healthcare domain and is increasingly helping\npractitioners to make health-related decisions. Therefore, accountability\nbecomes a crucial concern for critical AI-driven decisions. Although regulatory\nbodies, such as the EU commission, provide guidelines, they are highlevel and\nfocus on the ''what'' that should be done and less on the ''how'', creating a\nknowledge gap for actors. Through an extensive analysis, we found that the term\naccountability is perceived and dealt with in many different ways, depending on\nthe actor's expertise and domain of work. With increasing concerns about AI\naccountability issues and the ambiguity around this term, this paper bridges\nthe gap between the ''what'' and ''how'' of AI accountability, specifically for\nAI systems in healthcare. We do this by analysing the concept of\naccountability, formulating an accountability framework, and providing a\nthree-tier structure for handling various accountability mechanisms. Our\naccountability framework positions the regulations of healthcare AI systems and\nthe mechanisms adopted by the actors under a consistent accountability regime.\nMoreover, the three-tier structure guides the actors of the healthcare AI\nsystem to categorise the mechanisms based on their conduct. Through our\nframework, we advocate that decision-making in healthcare AI holds shared\ndependencies, where accountability should be dealt with jointly and should\nfoster collaborations. We highlight the role of explainability in instigating\ncommunication and information sharing between the actors to further facilitate\nthe collaborative process."}
{"id": "2509.03331", "categories": ["cs.SE", "cs.CR"], "pdf": "https://arxiv.org/pdf/2509.03331", "abs": "https://arxiv.org/abs/2509.03331", "authors": ["Weizhe Wang", "Wei Ma", "Qiang Hu", "Yao Zhang", "Jianfei Sun", "Bin Wu", "Yang Liu", "Guangquan Xu", "Lingxiao Jiang"], "title": "VulnRepairEval: An Exploit-Based Evaluation Framework for Assessing Large Language Model Vulnerability Repair Capabilities", "comment": null, "summary": "The adoption of Large Language Models (LLMs) for automated software\nvulnerability patching has shown promising outcomes on carefully curated\nevaluation sets. Nevertheless, existing datasets predominantly rely on\nsuperficial validation methods rather than exploit-based verification, leading\nto overestimated performance in security-sensitive applications. This paper\nintroduces VulnRepairEval, an evaluation framework anchored in functional\nProof-of-Concept (PoC) exploits. Our framework delivers a comprehensive,\ncontainerized evaluation pipeline that enables reproducible differential\nassessment, where repair success requires the original exploit to fail\nexecution against the modified code. The benchmark construction involved\nextensive data curation: we processed over 400 CVEs and approximately 2,500\npotential sources to extract a collection of authentic vulnerability instances\n(23 Python CVEs) amenable to automated testing with working PoCs. Through\nVulnRepairEval, we conduct a comprehensive evaluation of 12 popular LLMs and\nobserve a significant performance deficit: even the top-performing model\nsuccessfully addresses merely 5/23 instances (about 21.7%), exposing critical\nweaknesses in security-focused applications. Our failure analysis reveals that\nmost unsuccessful attempts stem from imprecise vulnerability identification and\npatches containing syntactic or semantic errors. Enhanced prompting strategies\nand multi-agent approaches yield minimal improvements, with overall\neffectiveness remaining largely unaffected. This work contributes a stringent,\npractical evaluation framework for LLM-driven vulnerability remediation and\nunderscores the necessity for assessment protocols that authentically reflect\nreal-world exploitation scenarios."}
{"id": "2509.03310", "categories": ["cs.AI", "cs.SE"], "pdf": "https://arxiv.org/pdf/2509.03310", "abs": "https://arxiv.org/abs/2509.03310", "authors": ["Evgenii Kniazev", "Arseny Kravchenko", "Igor Rekun", "James Broadhead", "Nikita Shamgunov", "Pranav Sah", "Pratik Nichite", "Ivan Yamshchikov"], "title": "app.build: A Production Framework for Scaling Agentic Prompt-to-App Generation with Environment Scaffolding", "comment": null, "summary": "We present app.build (https://github.com/appdotbuild/agent/), an open-source\nframework that improves LLM-based application generation through systematic\nvalidation and structured environments. Our approach combines multi-layered\nvalidation pipelines, stack-specific orchestration, and model-agnostic\narchitecture, implemented across three reference stacks. Through evaluation on\n30 generation tasks, we demonstrate that comprehensive validation achieves\n73.3% viability rate with 30% reaching perfect quality scores, while\nopen-weights models achieve 80.8% of closed-model performance when provided\nstructured environments. The open-source framework has been adopted by the\ncommunity, with over 3,000 applications generated to date. This work\ndemonstrates that scaling reliable AI agents requires scaling environments, not\njust models -- providing empirical insights and complete reference\nimplementations for production-oriented agent systems."}
{"id": "2509.03345", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.03345", "abs": "https://arxiv.org/abs/2509.03345", "authors": ["Yunxin Sun", "Abulhair Saparov"], "title": "Language Models Do Not Follow Occam's Razor: A Benchmark for Inductive and Abductive Reasoning", "comment": null, "summary": "Reasoning is a core capability in artificial intelligence systems, for which\nlarge language models (LLMs) have recently shown remarkable progress. However,\nmost work focuses exclusively on deductive reasoning, which is problematic\nsince other types of reasoning are also essential in solving real-world\nproblems, and they are less explored. This work focuses on evaluating LLMs'\ninductive and abductive reasoning capabilities. We introduce a programmable and\nsynthetic dataset, InAbHyD (pronounced in-a-bid), where each reasoning example\nconsists of an incomplete world model and a set of observations. The task for\nthe intelligent agent is to produce hypotheses to explain observations under\nthe incomplete world model to solve each reasoning example. We propose a new\nmetric to evaluate the quality of hypotheses based on Occam's Razor. We\nevaluate and analyze some state-of-the-art LLMs. Our analysis shows that LLMs\ncan perform inductive and abductive reasoning in simple scenarios, but struggle\nwith complex world models and producing high-quality hypotheses, even with\npopular reasoning-enhancing techniques such as in-context learning and RLVR."}
{"id": "2509.03380", "categories": ["cs.AI", "cs.CL", "93A16", "I.2.11"], "pdf": "https://arxiv.org/pdf/2509.03380", "abs": "https://arxiv.org/abs/2509.03380", "authors": ["Peter J. Bentley", "Soo Ling Lim", "Fuyuki Ishikawa"], "title": "Situating AI Agents in their World: Aspective Agentic AI for Dynamic Partially Observable Information Systems", "comment": "9 pages", "summary": "Agentic LLM AI agents are often little more than autonomous chatbots: actors\nfollowing scripts, often controlled by an unreliable director. This work\nintroduces a bottom-up framework that situates AI agents in their environment,\nwith all behaviors triggered by changes in their environments. It introduces\nthe notion of aspects, similar to the idea of umwelt, where sets of agents\nperceive their environment differently to each other, enabling clearer control\nof information. We provide an illustrative implementation and show that\ncompared to a typical architecture, which leaks up to 83% of the time,\naspective agentic AI enables zero information leakage. We anticipate that this\nconcept of specialist agents working efficiently in their own information\nniches can provide improvements to both security and efficiency."}
{"id": "2509.03383", "categories": ["cs.AI", "cs.RO"], "pdf": "https://arxiv.org/pdf/2509.03383", "abs": "https://arxiv.org/abs/2509.03383", "authors": ["Yiyang Huang", "Zixuan Wang", "Zishen Wan", "Yapeng Tian", "Haobo Xu", "Yinhe Han", "Yiming Gan"], "title": "ANNIE: Be Careful of Your Robots", "comment": null, "summary": "The integration of vision-language-action (VLA) models into embodied AI (EAI)\nrobots is rapidly advancing their ability to perform complex, long-horizon\ntasks in humancentric environments. However, EAI systems introduce critical\nsecurity risks: a compromised VLA model can directly translate adversarial\nperturbations on sensory input into unsafe physical actions. Traditional safety\ndefinitions and methodologies from the machine learning community are no longer\nsufficient. EAI systems raise new questions, such as what constitutes safety,\nhow to measure it, and how to design effective attack and defense mechanisms in\nphysically grounded, interactive settings. In this work, we present the first\nsystematic study of adversarial safety attacks on embodied AI systems, grounded\nin ISO standards for human-robot interactions. We (1) formalize a principled\ntaxonomy of safety violations (critical, dangerous, risky) based on physical\nconstraints such as separation distance, velocity, and collision boundaries;\n(2) introduce ANNIEBench, a benchmark of nine safety-critical scenarios with\n2,400 video-action sequences for evaluating embodied safety; and (3)\nANNIE-Attack, a task-aware adversarial framework with an attack leader model\nthat decomposes long-horizon goals into frame-level perturbations. Our\nevaluation across representative EAI models shows attack success rates\nexceeding 50% across all safety categories. We further demonstrate sparse and\nadaptive attack strategies and validate the real-world impact through physical\nrobot experiments. These results expose a previously underexplored but highly\nconsequential attack surface in embodied AI systems, highlighting the urgent\nneed for security-driven defenses in the physical AI era. Code is available at\nhttps://github.com/RLCLab/Annie."}
{"id": "2509.03462", "categories": ["cs.AI", "cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2509.03462", "abs": "https://arxiv.org/abs/2509.03462", "authors": ["Zhuo Cao", "Yunxiao Shi", "Min Xu"], "title": "sam-llm: interpretable lane change trajectoryprediction via parametric finetuning", "comment": "5 pages", "summary": "This work introduces SAM-LLM, a novel hybrid architecture that bridges the\ngap between the contextual reasoning of Large Language Models (LLMs) and the\nphysical precision of kinematic lane change models for autonomous driving. The\nsystem is designed for interpretable lane change trajectory prediction by\nfinetuning an LLM to output the core physical parameters of a trajectory model\ninstead of raw coordinates. For lane-keeping scenarios, the model predicts\ndiscrete coordinates, but for lane change maneuvers, it generates the\nparameters for an enhanced Sinusoidal Acceleration Model (SAM), including\nlateral displacement, maneuver duration, initial lateral velocity, and\nlongitudinal velocity change. This parametric approach yields a complete,\ncontinuous, and physically plausible trajectory model that is inherently\ninterpretable and computationally efficient, achieving an 80% reduction in\noutput size compared to coordinate-based methods. The SAM-LLM achieves a\nstate-of-the-art overall intention prediction accuracy of 98.73%, demonstrating\nperformance equivalent to traditional LLM predictors while offering significant\nadvantages in explainability and resource efficiency."}
{"id": "2509.03024", "categories": ["cs.CR", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.03024", "abs": "https://arxiv.org/abs/2509.03024", "authors": ["Moontaha Nishat Chowdhury", "André Bauer", "Minxuan Zhou"], "title": "Efficient Privacy-Preserving Recommendation on Sparse Data using Fully Homomorphic Encryption", "comment": "The paper is accepted at the 21st IEEE International eScience\n  Conference (eScience'25) and will be published soon. Link:\n  https://www.escience-conference.org/2025/papers", "summary": "In today's data-driven world, recommendation systems personalize user\nexperiences across industries but rely on sensitive data, raising privacy\nconcerns. Fully homomorphic encryption (FHE) can secure these systems, but a\nsignificant challenge in applying FHE to recommendation systems is efficiently\nhandling the inherently large and sparse user-item rating matrices. FHE\noperations are computationally intensive, and naively processing various sparse\nmatrices in recommendation systems would be prohibitively expensive.\nAdditionally, the communication overhead between parties remains a critical\nconcern in encrypted domains. We propose a novel approach combining Compressed\nSparse Row (CSR) representation with FHE-based matrix factorization that\nefficiently handles matrix sparsity in the encrypted domain while minimizing\ncommunication costs. Our experimental results demonstrate high recommendation\naccuracy with encrypted data while achieving the lowest communication costs,\neffectively preserving user privacy."}
{"id": "2509.03093", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.03093", "abs": "https://arxiv.org/abs/2509.03093", "authors": ["Fatih Pehlivan", "Arçin Ülkü Ergüzen", "Sahand Moslemi Yengejeh", "Mayasah Lami", "Anil Koyuncu"], "title": "Are We SOLID Yet? An Empirical Study on Prompting LLMs to Detect Design Principle Violations", "comment": "Accepted to ASE2025", "summary": "Traditional static analysis methods struggle to detect semantic design flaws,\nsuch as violations of the SOLID principles, which require a strong\nunderstanding of object-oriented design patterns and principles. Existing\nsolutions typically focus on individual SOLID principles or specific\nprogramming languages, leaving a gap in the ability to detect violations across\nall five principles in multi-language codebases. This paper presents a new\napproach: a methodology that leverages tailored prompt engineering to assess\nLLMs on their ability to detect SOLID violations across multiple languages. We\npresent a benchmark of four leading LLMs-CodeLlama, DeepSeekCoder, QwenCoder,\nand GPT-4o Mini-on their ability to detect violations of all five SOLID\nprinciples. For this evaluation, we construct a new benchmark dataset of 240\nmanually validated code examples. Using this dataset, we test four distinct\nprompt strategies inspired by established zero-shot, few-shot, and\nchain-of-thought techniques to systematically measure their impact on detection\naccuracy. Our emerging results reveal a stark hierarchy among models, with\nGPT-4o Mini decisively outperforming others, yet even struggles with\nchallenging principles like DIP. Crucially, we show that prompt strategy has a\ndramatic impact, but no single strategy is universally best; for instance, a\ndeliberative ENSEMBLE prompt excels at OCP detection while a hint-based EXAMPLE\nprompt is superior for DIP violations. Across all experiments, detection\naccuracy is heavily influenced by language characteristics and degrades sharply\nwith increasing code complexity. These initial findings demonstrate that\neffective, AI-driven design analysis requires not a single best model, but a\ntailored approach that matches the right model and prompt to the specific\ndesign context, highlighting the potential of LLMs to support maintainability\nthrough AI-assisted code analysis."}
{"id": "2509.03294", "categories": ["cs.CR", "cs.AI", "cs.LG", "68P27, 68T09, 94A60"], "pdf": "https://arxiv.org/pdf/2509.03294", "abs": "https://arxiv.org/abs/2509.03294", "authors": ["Napsu Karmitsa", "Antti Airola", "Tapio Pahikkala", "Tinja Pitkämäki"], "title": "A Comprehensive Guide to Differential Privacy: From Theory to User Expectations", "comment": null, "summary": "The increasing availability of personal data has enabled significant advances\nin fields such as machine learning, healthcare, and cybersecurity. However,\nthis data abundance also raises serious privacy concerns, especially in light\nof powerful re-identification attacks and growing legal and ethical demands for\nresponsible data use. Differential privacy (DP) has emerged as a principled,\nmathematically grounded framework for mitigating these risks. This review\nprovides a comprehensive survey of DP, covering its theoretical foundations,\npractical mechanisms, and real-world applications. It explores key algorithmic\ntools and domain-specific challenges - particularly in privacy-preserving\nmachine learning and synthetic data generation. The report also highlights\nusability issues and the need for improved communication and transparency in DP\nsystems. Overall, the goal is to support informed adoption of DP by researchers\nand practitioners navigating the evolving landscape of data privacy."}
