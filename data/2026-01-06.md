<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 7]
- [cs.AI](#cs.AI) [Total: 17]
- [cs.CR](#cs.CR) [Total: 12]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [Advanced Vulnerability Scanning for Open Source Software: Detection and Mitigation of Log4j Vulnerabilities](https://arxiv.org/abs/2601.00235)
*Victor Wen,Zedong Peng*

Main category: cs.SE

TL;DR: 开发了一个先进的Log4j扫描工具，通过评估软件的实际可利用性来减少误报，集成到GitHub Actions中实现自动化持续扫描，准确率达到91.4%。


<details>
  <summary>Details</summary>
Motivation: Log4j漏洞检测工具存在高误报率问题，因为它们主要关注识别安装的Log4j版本，而不检查软件是否真正易受攻击。当前约33%的Log4j下载仍包含易受攻击的包，需要更准确的检测方法。

Method: 开发了一个先进的Log4j扫描工具，首先识别漏洞，然后提供针对性的缓解建议和即时反馈。通过GitHub Actions集成，实现自动化持续扫描能力，确保在代码变更时及时识别漏洞。

Result: 评估了28个开源软件项目的不同版本，从140次扫描样本中获得了91.4%的准确率。GitHub Action实现已在GitHub市场上提供，任何人都可以使用。

Conclusion: 该工具提供了一种可靠的方法来检测和缓解开源项目中的漏洞，通过评估实际可利用性显著减少了误报，集成到开发工作流中实现了实时监控和快速响应。

Abstract: Automated detection of software vulnerabilities remains a critical challenge in software security. Log4j is an industrial-grade Java logging framework listed as one of the top 100 critical open source projects. On Dec. 10, 2021 a severe vulnerability Log4Shell was disclosed before being fully patched with Log4j2 version 2.17.0 on Dec. 18, 2021. However, to this day about 4.1 million, or 33 percent of all Log4j downloads in the last 7 days contain vulnerable packages. Many Log4Shell scanners have since been created to detect if a user's installed Log4j version is vulnerable. Current detection tools primarily focus on identifying the version of Log4j installed, leading to numerous false positives, as they do not check if the software scanned is really vulnerable to malicious actors. This research aims to develop an advanced Log4j scanning tool that can evaluate the real-world exploitability of the software, thereby reducing false positives. Our approach first identifies vulnerabilities and then provides targeted recommendations for mitigating these detected vulnerabilities, along with instant feedback to users. By leveraging GitHub Actions, our tool offers automated and continuous scanning capabilities, ensuring timely identification of vulnerabilities as code changes occur. This integration into existing development workflows enables real-time monitoring and quicker responses to potential threats. We demonstrate the effectiveness of our approach by evaluating 28 open-source software projects across different releases, achieving an accuracy rate of 91.4% from a sample of 140 scans. Our GitHub action implementation is available at the GitHub marketplace and can be accessed by anyone interested in improving their software security and for future studies. This tool provides a dependable way to detect and mitigate vulnerabilities in open-source projects.

</details>


### [2] [An Empirical Evaluation of LLM-Based Approaches for Code Vulnerability Detection: RAG, SFT, and Dual-Agent Systems](https://arxiv.org/abs/2601.00254)
*Md Hasan Saju,Maher Muhtadi,Akramul Azim*

Main category: cs.SE

TL;DR: 该研究比较了三种基于大语言模型的软件漏洞检测方法：检索增强生成（RAG）、监督微调（SFT）和双智能体框架，发现RAG方法在准确率和F1分数上表现最佳。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型的快速发展，为自动化软件漏洞检测提供了新的机遇，这对于保护现代代码库安全至关重要。本研究旨在比较不同LLM技术在漏洞检测方面的有效性。

Method: 研究评估了三种方法：1）检索增强生成（RAG），整合互联网和MITRE CWE数据库的外部领域知识；2）监督微调（SFT），使用参数高效的QLoRA适配器；3）双智能体LLM框架，其中第二个智能体审核并优化第一个的输出。使用从Big-Vul和GitHub真实代码库中整理的数据集，重点关注五个关键CWE类别。

Result: RAG方法取得了最高的整体准确率（0.86）和F1分数（0.85），显示了上下文增强的价值。SFT方法也表现出色。双智能体系统在提高推理透明度和错误缓解方面显示出潜力，同时减少了资源开销。

Conclusion: 研究结果表明，整合领域专业知识机制能显著增强LLM在实际漏洞检测任务中的实用性。RAG方法因其整合外部知识的能力而表现最佳，为现实世界软件安全应用提供了有效解决方案。

Abstract: The rapid advancement of Large Language Models (LLMs) presents new opportunities for automated software vulnerability detection, a crucial task in securing modern codebases. This paper presents a comparative study on the effectiveness of LLM-based techniques for detecting software vulnerabilities. The study evaluates three approaches, Retrieval-Augmented Generation (RAG), Supervised Fine-Tuning (SFT), and a Dual-Agent LLM framework, against a baseline LLM model. A curated dataset was compiled from Big-Vul and real-world code repositories from GitHub, focusing on five critical Common Weakness Enumeration (CWE) categories: CWE-119, CWE-399, CWE-264, CWE-20, and CWE-200. Our RAG approach, which integrated external domain knowledge from the internet and the MITRE CWE database, achieved the highest overall accuracy (0.86) and F1 score (0.85), highlighting the value of contextual augmentation. Our SFT approach, implemented using parameter-efficient QLoRA adapters, also demonstrated strong performance. Our Dual-Agent system, an architecture in which a secondary agent audits and refines the output of the first, showed promise in improving reasoning transparency and error mitigation, with reduced resource overhead. These results emphasize that incorporating a domain expertise mechanism significantly strengthens the practical applicability of LLMs in real-world vulnerability detection tasks.

</details>


### [3] [In Line with Context: Repository-Level Code Generation via Context Inlining](https://arxiv.org/abs/2601.00376)
*Chao Hu,Wenhao Zeng,Yuling Shi,Beijun Shen,Xiaodong Gu*

Main category: cs.SE

TL;DR: InlineCoder是一个新颖的仓库级代码生成框架，通过将未完成函数内联到其调用图中，将复杂的仓库理解任务转化为更简单的函数级编码任务。


<details>
  <summary>Details</summary>
Motivation: 现有的仓库级代码生成方法（如检索增强生成或基于上下文函数选择）主要依赖表面相似性，难以捕捉控制仓库级语义的丰富依赖关系，导致在理解整个仓库和处理跨函数、类、模块的复杂依赖时表现不足。

Method: InlineCoder采用双向内联过程：1）首先生成草案完成（锚点），近似下游依赖并启用基于困惑度的置信度估计；2）上游内联：将锚点嵌入其调用者中以捕捉多样化使用场景；3）下游检索：将锚点的被调用者集成到提示中提供精确依赖上下文。

Result: 该方法通过结合草案完成与上下游视角的丰富上下文，为大型语言模型提供了全面的仓库视图，从而增强了仓库上下文的理解能力。

Conclusion: InlineCoder通过将未完成函数内联到其调用图中，成功地将具有挑战性的仓库理解任务重新构建为更简单的函数级编码任务，为仓库级代码生成提供了有效的解决方案。

Abstract: Repository-level code generation has attracted growing attention in recent years. Unlike function-level code generation, it requires the model to understand the entire repository, reasoning over complex dependencies across functions, classes, and modules. However, existing approaches such as retrieval-augmented generation (RAG) or context-based function selection often fall short: they primarily rely on surface-level similarity and struggle to capture the rich dependencies that govern repository-level semantics. In this paper, we introduce InlineCoder, a novel framework for repository-level code generation. InlineCoder enhances the understanding of repository context by inlining the unfinished function into its call graph, thereby reframing the challenging repository understanding as an easier function-level coding task. Given a function signature, InlineCoder first generates a draft completion, termed an anchor, which approximates downstream dependencies and enables perplexity-based confidence estimation. This anchor drives a bidirectional inlining process: (i) Upstream Inlining, which embeds the anchor into its callers to capture diverse usage scenarios; and (ii) Downstream Retrieval, which integrates the anchor's callees into the prompt to provide precise dependency context. The enriched context, combining draft completion with upstream and downstream perspectives, equips the LLM with a comprehensive repository view.

</details>


### [4] [On Plagiarism and Software Plagiarism](https://arxiv.org/abs/2601.00429)
*Rares Folea,Emil Slusanschi*

Main category: cs.SE

TL;DR: 本文探讨软件相似性自动检测的复杂性，介绍开源软件解决方案Project Martial，并综述现有反软件抄袭方法、法律案例及检测技术分类。


<details>
  <summary>Details</summary>
Motivation: 软件相似性检测面临数字工件的独特挑战，现有方法分散且缺乏系统分析。需要开发开源解决方案并综合学术与法律视角，以应对软件版权侵权检测的实际需求。

Method: 1. 提出开源软件Project Martial用于代码相似性检测；2. 分析学术界和法律领域的反抄袭方法，包括重要诉讼案例；3. 基于可用工件对检测挑战进行分类；4. 综述现有技术如指纹识别、软件胎记、代码嵌入等；5. 展示这些技术在Project Martial中的应用。

Result: 1. 开发了开源软件Project Martial用于代码相似性检测；2. 系统梳理了软件版权侵权检测的学术与法律框架；3. 建立了基于工件类型的检测挑战分类体系；4. 综合分析了多种检测技术的优缺点；5. 验证了部分技术在Project Martial中的实际应用可行性。

Conclusion: 软件相似性检测需要结合技术、法律和实际应用的多维视角。Project Martial作为开源解决方案，整合了多种检测技术，为应对软件抄袭问题提供了实用工具。未来研究应继续完善检测方法并关注法律实践的发展。

Abstract: This paper explores the complexities of automatic detection of software similarities, in relation to the unique challenges of digital artifacts, and introduces Project Martial, an open-source software solution for detecting code similarity. This research enumerates some of the existing approaches to counter software plagiarism by examining both the academia and legal landscape, including notable lawsuits and court rulings that have shaped the understanding of software copyright infringements in commercial applications. Furthermore, we categorize the classes of detection challenges based on the available artifacts, and we provide a survey of the previously studied techniques in the literature, including solutions based on fingerprinting, software birthmarks, or code embeddings, and exemplify how a subset of them can be applied in the context of Project Martial.

</details>


### [5] [DSL or Code? Evaluating the Quality of LLM-Generated Algebraic Specifications: A Case Study in Optimization at Kinaxis](https://arxiv.org/abs/2601.00469)
*Negin Ayoughi,David Dewar,Shiva Nejati,Mehrdad Sabetzadeh*

Main category: cs.SE

TL;DR: EXEOS是一个基于LLM的方法，能够从自然语言描述生成AMPL模型和Python代码，并通过求解器反馈进行迭代优化，在数学优化领域证明AMPL DSL可以与Python竞争甚至表现更好。


<details>
  <summary>Details</summary>
Motivation: 模型驱动工程（MDE）虽然提供抽象和分析严谨性，但在许多领域的工业采用受到模型开发和维护成本的限制。大型语言模型（LLMs）可以通过支持从自然语言描述直接生成模型来改变这种成本平衡。然而，对于领域特定语言（DSLs），LLM生成的模型可能不如主流语言（如Python）的代码准确，因为后者在LLM训练语料中占主导地位。

Method: 提出EXEOS方法，这是一个基于LLM的方法，能够从自然语言问题描述推导出AMPL模型和Python代码，并通过求解器反馈进行迭代优化。使用公开优化数据集和工业合作伙伴Kinaxis的实际供应链案例进行评估。

Result: 评估生成的AMPL模型与Python代码在可执行性和正确性方面的表现。消融研究显示，AMPL在可执行性和正确性方面与Python竞争，有时甚至更好，并且EXEOS中的设计选择提高了生成规范的质量。

Conclusion: 研究表明，即使在LLM训练语料中不占主导地位的领域特定语言（如AMPL），通过适当的LLM方法（如EXEOS）也可以生成高质量的模型，挑战了DSL在LLM生成中必然劣于主流语言的假设。

Abstract: Model-driven engineering (MDE) provides abstraction and analytical rigour, but industrial adoption in many domains has been limited by the cost of developing and maintaining models. Large language models (LLMs) can help shift this cost balance by supporting direct generation of models from natural-language (NL) descriptions. For domain-specific languages (DSLs), however, LLM-generated models may be less accurate than LLM-generated code in mainstream languages such as Python, due to the latter's dominance in LLM training corpora. We investigate this issue in mathematical optimization, with AMPL, a DSL with established industrial use. We introduce EXEOS, an LLM-based approach that derives AMPL models and Python code from NL problem descriptions and iteratively refines them with solver feedback. Using a public optimization dataset and real-world supply-chain cases from our industrial partner Kinaxis, we evaluate generated AMPL models against Python code in terms of executability and correctness. An ablation study with two LLM families shows that AMPL is competitive with, and sometimes better than, Python, and that our design choices in EXEOS improve the quality of generated specifications.

</details>


### [6] [Multi-Agent Coordinated Rename Refactoring](https://arxiv.org/abs/2601.00482)
*Abhiram Bellur,Mohammed Raihan Ullah,Fraol Batole,Mohit Kansara,Masaharu Morimoto,Kai Ishikawa,Haifeng Chen,Yaroslav Zharov,Timofey Bryksin,Tien N. Nguyen,Hridesh Rajan,Danny Dig*

Main category: cs.SE

TL;DR: 本文提出首个多智能体框架，用于自动化协调重命名重构任务，通过智能体与开发者协作，减少手动重构的负担和错误。


<details>
  <summary>Details</summary>
Motivation: 协调重命名是软件开发中频繁但具有挑战性的任务，开发者需要手动在多个文件和上下文中传播重命名重构，过程繁琐且容易出错。现有启发式方法产生过多误报，而大型语言模型由于上下文限制和无法与重构工具交互，只能提供不完整的建议。

Method: 设计了首个多智能体框架，包含三个智能体：范围推断智能体将开发者的初始重构线索转化为明确的自然语言声明范围；计划执行智能体使用该范围作为严格计划，识别需要重构的程序元素，并通过调用IDE的可信重构API安全执行更改；复制智能体指导项目范围内的搜索。

Result: 首先在100个开源项目的609K次提交中进行了协调重命名实践的成型研究，并调查了205名开发者。框架通过智能体协作有效减少了开发者的负担，同时保持开发者在主导地位。

Conclusion: AI智能体在软件开发中的主要价值在于扩展开发者的推理和行动能力，而不是取代人类参与。协调重命名正是智能体能够显著减轻开发者负担的重复性任务类型，同时让开发者保持在主导地位。

Abstract: The primary value of AI agents in software development lies in their ability to extend the developer's capacity for reasoning and action, not to supplant human involvement. To showcase how to use agents working in tandem with developers, we designed a novel approach for carrying out coordinated renaming. Coordinated renaming, where a single rename refactoring triggers refactorings in multiple, related identifiers, is a frequent yet challenging task. Developers must manually propagate these rename refactorings across numerous files and contexts, a process that is both tedious and highly error-prone. State-of-the-art heuristic-based approaches produce an overwhelming number of false positives, while vanilla Large Language Models (LLMs) provide incomplete suggestions due to their limited context and inability to interact with refactoring tools. This leaves developers with incomplete refactorings or burdens them with filtering too many false positives. Coordinated renaming is exactly the kind of repetitive task that agents can significantly reduce the developers' burden while keeping them in the driver's seat.
  We designed, implemented, and evaluated the first multi-agent framework that automates coordinated renaming. It operates on a key insight: a developer's initial refactoring is a clue to infer the scope of related refactorings. Our Scope Inference Agent first transforms this clue into an explicit, natural-language Declared Scope. The Planned Execution Agent then uses this as a strict plan to identify program elements that should undergo refactoring and safely executes the changes by invoking the IDE's own trusted refactoring APIs. Finally, the Replication Agent uses it to guide the project-wide search. We first conducted a formative study on the practice of coordinated renaming in 609K commits in 100 open-source projects and surveyed 205 developers ...

</details>


### [7] [SEMODS: A Validated Dataset of Open-Source Software Engineering Models](https://arxiv.org/abs/2601.00635)
*Alexandra González,Xavier Franch,Silverio Martínez-Fernández*

Main category: cs.SE

TL;DR: SEMODS是一个包含3,427个Hugging Face模型的软件工程专用数据集，通过自动收集和人工标注结合LLM辅助验证构建，将模型与软件开发生命周期任务关联，支持数据分析、模型发现、基准测试和模型适配等应用。


<details>
  <summary>Details</summary>
Motivation: 将人工智能集成到软件工程需要适合SE任务的模型集合，但Hugging Face上有数百万个模型且持续增加，没有专门目录难以识别SE相关模型。

Method: 从Hugging Face提取3,427个模型，结合自动收集与严格验证（人工标注和大型语言模型辅助），将模型链接到软件开发生命周期的任务和活动，提供评估结果的标准化表示。

Result: 构建了SEMODS数据集，包含3,427个SE相关模型，支持多种应用如数据分析、模型发现、基准测试和模型适配。

Conclusion: SEMODS填补了软件工程领域专用模型目录的空白，为AI在SE中的集成提供了有价值的资源，支持研究人员和从业者发现、评估和适配适合SE任务的模型。

Abstract: Integrating Artificial Intelligence into Software Engineering (SE) requires having a curated collection of models suited to SE tasks. With millions of models hosted on Hugging Face (HF) and new ones continuously being created, it is infeasible to identify SE models without a dedicated catalogue. To address this gap, we present SEMODS: an SE-focused dataset of 3,427 models extracted from HF, combining automated collection with rigorous validation through manual annotation and large language model assistance. Our dataset links models to SE tasks and activities from the software development lifecycle, offering a standardized representation of their evaluation results, and supporting multiple applications such as data analysis, model discovery, benchmarking, and model adaptation.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [8] [Reasoning in Action: MCTS-Driven Knowledge Retrieval for Large Language Models](https://arxiv.org/abs/2601.00003)
*Shuqi Liu,Bowei He,Chen Ma,Linqi Song*

Main category: cs.AI

TL;DR: 提出一种推理感知的知识检索方法，通过粗到细的两阶段检索策略，结合蒙特卡洛树搜索，为LLMs提供与对话逻辑结构对齐的知识，超越表面语义相似性。


<details>
  <summary>Details</summary>
Motivation: 当前LLMs通常通过检索语义相似信息或提升推理能力来增强性能，但如何有效整合检索和推理策略以优化LLM性能仍是一个重要挑战。需要超越表面语义相似性，提供与对话逻辑结构对齐的知识。

Method: 采用粗到细的两阶段知识检索方法：1) 首先识别知识库中与上下文相关的子区域，确保所有句子都与主题相关；2) 在该子区域内细化搜索，提取与推理过程特别相关的知识。两阶段都使用蒙特卡洛树搜索启发的方法，通过共同关键词在知识句子中导航。

Result: 在两个多轮对话数据集上的实验表明，该方法不仅更贴近人类对话中的底层推理逻辑，还显著提高了检索知识的多样性，从而产生更具信息性和创造性的响应。

Conclusion: 提出的推理感知知识检索方法能够有效整合检索和推理策略，为LLMs提供与对话逻辑结构对齐的知识，超越传统基于表面语义相似性的检索方法，提升对话质量和创造性。

Abstract: Large language models (LLMs) typically enhance their performance through either the retrieval of semantically similar information or the improvement of their reasoning capabilities. However, a significant challenge remains in effectively integrating both retrieval and reasoning strategies to optimize LLM performance. In this paper, we introduce a reasoning-aware knowledge retrieval method that enriches LLMs with information aligned to the logical structure of conversations, moving beyond surface-level semantic similarity. We follow a coarse-to-fine approach for knowledge retrieval. First, we identify a contextually relevant sub-region of the knowledge base, ensuring that all sentences within it are relevant to the context topic. Next, we refine our search within this sub-region to extract knowledge that is specifically relevant to the reasoning process. Throughout both phases, we employ the Monte Carlo Tree Search-inspired search method to effectively navigate through knowledge sentences using common keywords. Experiments on two multi-turn dialogue datasets demonstrate that our knowledge retrieval approach not only aligns more closely with the underlying reasoning in human conversations but also significantly enhances the diversity of the retrieved knowledge, resulting in more informative and creative responses.

</details>


### [9] [Finetuning Large Language Models for Automated Depression Screening in Nigerian Pidgin English: GENSCORE Pilot Study](https://arxiv.org/abs/2601.00004)
*Isaac Iyinoluwa Olufadewa,Miracle Ayomikun Adesina,Ezekiel Ayodeji Oladejo,Uthman Babatunde Usman,Owen Kolade Adeniyi,Matthew Tolulope Olawoyin*

Main category: cs.AI

TL;DR: 该研究开发了一种基于大型语言模型的自动抑郁症筛查系统，专门针对尼日利亚皮钦语使用者，通过微调LLMs实现文化适应的抑郁症评估，GPT-4.1表现最佳。


<details>
  <summary>Details</summary>
Motivation: 尼日利亚抑郁症筛查覆盖率低，传统工具如PHQ-9在高收入国家验证，但在尼日利亚等中低收入国家存在语言和文化障碍，需要开发适应本地语言（尼日利亚皮钦语）的筛查工具。

Method: 收集432份尼日利亚年轻人（18-40岁）的皮钦语音频回答，进行转录、预处理和标注（包括语义标签、俚语解释和PHQ-9严重程度评分）。微调三种LLMs（Phi-3-mini-4k-instruct、Gemma-3-4B-it和GPT-4.1），从定量（准确性、精确度、语义对齐）和定性（清晰度、相关性、文化适当性）两方面评估性能。

Result: GPT-4.1表现最佳，在PHQ-9严重程度评分预测中达到94.5%的准确率，优于其他模型。在定性评估中也产生最文化适当、清晰和上下文相关的回答。

Conclusion: AI介导的抑郁症筛查可为尼日利亚服务不足的社区提供解决方案，为在语言多样、资源受限环境中部署对话式心理健康工具奠定了基础。

Abstract: Depression is a major contributor to the mental-health burden in Nigeria, yet screening coverage remains limited due to low access to clinicians, stigma, and language barriers. Traditional tools like the Patient Health Questionnaire-9 (PHQ-9) were validated in high-income countries but may be linguistically or culturally inaccessible for low- and middle-income countries and communities such as Nigeria where people communicate in Nigerian Pidgin and more than 520 local languages. This study presents a novel approach to automated depression screening using fine-tuned large language models (LLMs) adapted for conversational Nigerian Pidgin. We collected a dataset of 432 Pidgin-language audio responses from Nigerian young adults aged 18-40 to prompts assessing psychological experiences aligned with PHQ-9 items, performed transcription, rigorous preprocessing and annotation, including semantic labeling, slang and idiom interpretation, and PHQ-9 severity scoring. Three LLMs - Phi-3-mini-4k-instruct, Gemma-3-4B-it, and GPT-4.1 - were fine-tuned on this annotated dataset, and their performance was evaluated quantitatively (accuracy, precision and semantic alignment) and qualitatively (clarity, relevance, and cultural appropriateness). GPT-4.1 achieved the highest quantitative performance, with 94.5% accuracy in PHQ-9 severity scoring prediction, outperforming Gemma-3-4B-it and Phi-3-mini-4k-instruct. Qualitatively, GPT-4.1 also produced the most culturally appropriate, clear, and contextually relevant responses. AI-mediated depression screening for underserved Nigerian communities. This work provides a foundation for deploying conversational mental-health tools in linguistically diverse, resource-constrained environments.

</details>


### [10] [A multi-algorithm approach for operational human resources workload balancing in a last mile urban delivery system](https://arxiv.org/abs/2601.00023)
*Luis M. Moreno-Saavedra,Silvia Jimenez-Fernandez,Antonio Portilla-Figueras,David Casillas-Perez,Sancho Salcedo-Sanz*

Main category: cs.AI

TL;DR: 本文提出一种多算法方法来解决最后一公里包裹配送中的工作量平衡问题，通过结合距离和工作量考虑来优化包裹分配给配送员，确保每位员工每天完成相似的工作量。


<details>
  <summary>Details</summary>
Motivation: 传统基于地理邻近度的包裹分配方法效率低下，导致配送员之间工作量分配不均衡。在最后一公里城市包裹配送系统中，需要优化操作人力资源的工作量平衡，纠正配送员之间的显著工作量不平衡问题。

Method: 提出多算法方法，包括不同版本的k-means、进化方法、基于k-means初始化的递归分配（使用不同问题编码）以及混合进化集成算法。该方法以配送点集合和工人数量为输入，结合距离和工作量考虑来优化包裹分配。

Result: 在西班牙Azuqueca de Henares城市最后一公里包裹配送系统的实际案例中展示了所提方法的性能表现。

Conclusion: 通过多算法方法可以有效解决最后一公里包裹配送中的工作量平衡问题，优化配送时间，确保所有员工工作量均衡分配。

Abstract: Efficient workload assignment to the workforce is critical in last-mile package delivery systems. In this context, traditional methods of assigning package deliveries to workers based on geographical proximity can be inefficient and surely guide to an unbalanced workload distribution among delivery workers. In this paper, we look at the problem of operational human resources workload balancing in last-mile urban package delivery systems. The idea is to consider the effort workload to optimize the system, i.e., the optimization process is now focused on improving the delivery time, so that the workload balancing is complete among all the staff. This process should correct significant decompensations in workload among delivery workers in a given zone. Specifically, we propose a multi-algorithm approach to tackle this problem. The proposed approach takes as input a set of delivery points and a defined number of workers, and then assigns packages to workers, in such a way that it ensures that each worker completes a similar amount of work per day. The proposed algorithms use a combination of distance and workload considerations to optimize the allocation of packages to workers. In this sense, the distance between the delivery points and the location of each worker is also taken into account. The proposed multi-algorithm methodology includes different versions of k-means, evolutionary approaches, recursive assignments based on k-means initialization with different problem encodings, and a hybrid evolutionary ensemble algorithm. We have illustrated the performance of the proposed approach in a real-world problem in an urban last-mile package delivery workforce operating at Azuqueca de Henares, Spain.

</details>


### [11] [Quantitative Rule-Based Strategy modeling in Classic Indian Rummy: A Metric Optimization Approach](https://arxiv.org/abs/2601.00024)
*Purushottam Saha,Avirup Chakraborty,Sourish Sarkar,Subhamoy Maitra,Diganta Mukherjee,Tridib Mukherjee*

Main category: cs.AI

TL;DR: 本文提出了一种基于规则的战略性13张印度拉米纸牌游戏框架，引入新的手牌评估指标MinDist，通过计算手牌与最近有效配置的编辑距离来量化完成度，结合对手建模和零和模拟，显著提高了胜率。


<details>
  <summary>Details</summary>
Motivation: 13张印度拉米纸牌是一个不完全信息的序列游戏，需要概率推理和组合决策。传统启发式方法在战略游戏表现有限，需要更形式化和可解释的算法策略设计方法。

Method: 提出MinDist指标，改进MinScore方法，通过量化手牌与最近有效配置的编辑距离来评估完成度。设计计算高效的算法，采用动态剪枝和模式缓存技术。结合对手建模，在两人零和模拟框架中评估策略。

Result: 实验结果显示，基于MinDist的智能体相比传统启发式方法在胜率上有显著提升，为算法化拉米策略设计提供了形式化和可解释的步骤。

Conclusion: MinDist指标和相应算法框架为不完全信息的组合游戏提供了有效的战略决策方法，显著改善了13张印度拉米纸牌的游戏表现，为类似游戏策略设计提供了可借鉴的框架。

Abstract: The 13-card variant of Classic Indian Rummy is a sequential game of incomplete information that requires probabilistic reasoning and combinatorial decision-making. This paper proposes a rule-based framework for strategic play, driven by a new hand-evaluation metric termed MinDist. The metric modifies the MinScore metric by quantifying the edit distance between a hand and the nearest valid configuration, thereby capturing structural proximity to completion. We design a computationally efficient algorithm derived from the MinScore algorithm, leveraging dynamic pruning and pattern caching to exactly calculate this metric during play. Opponent hand-modeling is also incorporated within a two-player zero-sum simulation framework, and the resulting strategies are evaluated using statistical hypothesis testing. Empirical results show significant improvement in win rates for MinDist-based agents over traditional heuristics, providing a formal and interpretable step toward algorithmic Rummy strategy design.

</details>


### [12] [From Clay to Code: Typological and Material Reasoning in AI Interpretations of Iranian Pigeon Towers](https://arxiv.org/abs/2601.00029)
*Abolhassan Pishahang,Maryam Badiei*

Main category: cs.AI

TL;DR: 研究探索生成式AI如何理解乡土建筑中的建筑智慧，以伊朗鸽塔为例，测试三种扩散模型在不同提示阶段的表现，评估AI在类型学、材料性、环境、真实性和文化特异性方面的重建能力。


<details>
  <summary>Details</summary>
Motivation: 研究旨在探索生成式AI系统如何解读乡土建筑形式中蕴含的建筑智慧，了解AI在重建传统设计智能时的能力与局限，特别是在文化特异性、材料逻辑和气候适应性方面的表现。

Method: 以伊朗鸽塔为案例研究，测试三种扩散模型（Midjourney v6、DALL-E 3和基于Stable Diffusion XL的DreamStudio），采用三个提示阶段：参考性、适应性和推测性，并通过五标准评估框架（类型学、材料性、环境、真实性、文化特异性）进行分析。

Result: AI能可靠地复制几何图案，但误解材料和气候逻辑；参考图像提高了真实性但限制了创造性，而无参考的自由生成则产生创新但文化模糊的结果，揭示了视觉相似性与建筑推理之间的界限。

Conclusion: 研究定义了AI视觉相似性与建筑推理之间的边界，提出了计算乡土推理框架，用于分析AI如何感知、扭曲和重新想象传统设计智慧，为理解AI在建筑文化传承中的作用提供了新视角。

Abstract: This study investigates how generative AI systems interpret the architectural intelligence embedded in vernacular form. Using the Iranian pigeon tower as a case study, the research tests three diffusion models, Midjourney v6, DALL-E 3, and DreamStudio based on Stable Diffusion XL (SDXL), across three prompt stages: referential, adaptive, and speculative. A five-criteria evaluation framework assesses how each system reconstructs typology, materiality, environment, realism, and cultural specificity. Results show that AI reliably reproduces geometric patterns but misreads material and climatic reasoning. Reference imagery improves realism yet limits creativity, while freedom from reference generates inventive but culturally ambiguous outcomes. The findings define a boundary between visual resemblance and architectural reasoning, positioning computational vernacular reasoning as a framework for analyzing how AI perceives, distorts, and reimagines traditional design intelligence.

</details>


### [13] [The Agentic Leash: Extracting Causal Feedback Fuzzy Cognitive Maps with LLMs](https://arxiv.org/abs/2601.00097)
*Akash Kumar Panda,Olaoluwa Adigun,Bart Kosko*

Main category: cs.AI

TL;DR: 研究人员设计了一个基于大语言模型的智能体，能够从原始文本中提取因果反馈模糊认知图，并通过双向交互实现动态系统的自主演化。


<details>
  <summary>Details</summary>
Motivation: 研究旨在开发能够从文本中自动提取因果关系的智能系统，使模糊认知图能够自主演化，同时保持与文本数据的动态交互，从而更好地理解和模拟复杂因果系统。

Method: 设计了一个三步骤的LLM智能体系统：1)从文本中提取关键名词和名词短语；2)从这些名词中提取FCM概念节点；3)推断节点间的部分或模糊因果边。使用Gemini和ChatGPT LLM智能体生成FCM，并混合生成最终FCM。

Result: 该方法成功从Henry Kissinger关于AI前景的论文中提取FCM，生成的三步过程产生的FCM动态系统收敛到与人工生成的FCM相同的平衡极限环，尽管节点和边数不同。混合FCM不仅吸收了主要组件的平衡点，还创建了新的平衡点以更好地近似底层因果动态系统。

Conclusion: 基于LLM的智能体能够有效地从文本中提取因果结构，生成具有自主演化能力的FCM系统，混合不同LLM生成的FCM可以产生更丰富的动态行为，更好地模拟复杂因果系统。

Abstract: We design a large-language-model (LLM) agent that extracts causal feedback fuzzy cognitive maps (FCMs) from raw text. The causal learning or extraction process is agentic both because of the LLM's semi-autonomy and because ultimately the FCM dynamical system's equilibria drive the LLM agents to fetch and process causal text. The fetched text can in principle modify the adaptive FCM causal structure and so modify the source of its quasi-autonomy--its equilibrium limit cycles and fixed-point attractors. This bidirectional process endows the evolving FCM dynamical system with a degree of autonomy while still staying on its agentic leash. We show in particular that a sequence of three finely tuned system instructions guide an LLM agent as it systematically extracts key nouns and noun phrases from text, as it extracts FCM concept nodes from among those nouns and noun phrases, and then as it extracts or infers partial or fuzzy causal edges between those FCM nodes. We test this FCM generation on a recent essay about the promise of AI from the late diplomat and political theorist Henry Kissinger and his colleagues. This three-step process produced FCM dynamical systems that converged to the same equilibrium limit cycles as did the human-generated FCMs even though the human-generated FCM differed in the number of nodes and edges. A final FCM mixed generated FCMs from separate Gemini and ChatGPT LLM agents. The mixed FCM absorbed the equilibria of its dominant mixture component but also created new equilibria of its own to better approximate the underlying causal dynamical system.

</details>


### [14] [Mortar: Evolving Mechanics for Automatic Game Design](https://arxiv.org/abs/2601.00105)
*Muhammad U. Nasir,Yuchen Li,Steven James,Julian Togelius*

Main category: cs.AI

TL;DR: Mortar系统结合质量多样性算法和大型语言模型，自动演化游戏机制，通过合成完整游戏评估机制质量，目标是产生多样、可玩且能保持玩家技能排序的游戏。


<details>
  <summary>Details</summary>
Motivation: 游戏机制设计通常需要大量时间和专家经验，手动设计过程耗时且依赖专业知识。需要自动化方法来探索和评估游戏机制，以加速游戏设计过程并产生多样化的游戏体验。

Method: 结合质量多样性算法和大型语言模型探索多样化机制集合；通过树搜索程序合成包含演化机制和存档机制的完整游戏；基于技能排序（强玩家持续优于弱玩家）评估游戏质量；通过消融研究和用户研究评估系统组件。

Result: Mortar能够产生多样且可玩的游戏，其演化出的机制在游戏中能更好地贡献于技能排序得分。系统组件分析显示各组成部分均对最终结果有重要贡献。

Conclusion: Mortar系统成功实现了游戏机制的自主演化，为自动游戏设计提供了有效工具，能够产生多样、可玩且保持技能排序的游戏，验证了结合质量多样性算法与大型语言模型在游戏设计自动化中的潜力。

Abstract: We present Mortar, a system for autonomously evolving game mechanics for automatic game design. Game mechanics define the rules and interactions that govern gameplay, and designing them manually is a time-consuming and expert-driven process. Mortar combines a quality-diversity algorithm with a large language model to explore a diverse set of mechanics, which are evaluated by synthesising complete games that incorporate both evolved mechanics and those drawn from an archive. The mechanics are evaluated by composing complete games through a tree search procedure, where the resulting games are evaluated by their ability to preserve a skill-based ordering over players -- that is, whether stronger players consistently outperform weaker ones. We assess the mechanics based on their contribution towards the skill-based ordering score in the game. We demonstrate that Mortar produces games that appear diverse and playable, and mechanics that contribute more towards the skill-based ordering score in the game. We perform ablation studies to assess the role of each system component and a user study to evaluate the games based on human feedback.

</details>


### [15] [Ask, Clarify, Optimize: Human-LLM Agent Collaboration for Smarter Inventory Control](https://arxiv.org/abs/2601.00121)
*Yaqi Duan,Yichun Hu,Jiashuo Jiang*

Main category: cs.AI

TL;DR: LLM作为端到端库存优化求解器存在"幻觉税"性能差距，提出语义推理与数学计算严格解耦的混合智能框架，通过数字孪生测试显示成本降低32.1%，证明LLM应作为自然语言接口而非运筹学替代品。


<details>
  <summary>Details</summary>
Motivation: 中小企业在库存管理中缺乏部署高级优化方法的专业知识，需要探索大语言模型能否帮助弥合这一差距，但发现LLM作为端到端求解器存在性能缺陷。

Method: 提出混合智能框架，严格解耦语义推理与数学计算：LLM作为智能接口从自然语言中提取参数并解释结果，同时自动调用严格算法构建优化引擎。引入"人类模仿者"数字孪生进行可扩展的压力测试。

Result: 混合框架相比GPT-4o端到端求解基线降低总库存成本32.1%。提供完美真实信息本身不足以改善GPT-4o性能，确认瓶颈本质是计算而非信息问题。

Conclusion: LLM不应替代运筹学，而应作为自然语言接口，使基于严格求解器的策略对非专家可访问。混合智能框架能有效降低库存成本并解决LLM的"幻觉税"问题。

Abstract: Inventory management remains a challenge for many small and medium-sized businesses that lack the expertise to deploy advanced optimization methods. This paper investigates whether Large Language Models (LLMs) can help bridge this gap. We show that employing LLMs as direct, end-to-end solvers incurs a significant "hallucination tax": a performance gap arising from the model's inability to perform grounded stochastic reasoning. To address this, we propose a hybrid agentic framework that strictly decouples semantic reasoning from mathematical calculation. In this architecture, the LLM functions as an intelligent interface, eliciting parameters from natural language and interpreting results while automatically calling rigorous algorithms to build the optimization engine.
  To evaluate this interactive system against the ambiguity and inconsistency of real-world managerial dialogue, we introduce the Human Imitator, a fine-tuned "digital twin" of a boundedly rational manager that enables scalable, reproducible stress-testing. Our empirical analysis reveals that the hybrid agentic framework reduces total inventory costs by 32.1% relative to an interactive baseline using GPT-4o as an end-to-end solver. Moreover, we find that providing perfect ground-truth information alone is insufficient to improve GPT-4o's performance, confirming that the bottleneck is fundamentally computational rather than informational. Our results position LLMs not as replacements for operations research, but as natural-language interfaces that make rigorous, solver-based policies accessible to non-experts.

</details>


### [16] [Explicit Abstention Knobs for Predictable Reliability in Video Question Answering](https://arxiv.org/abs/2601.00138)
*Jorge Ortiz*

Main category: cs.AI

TL;DR: 研究视频问答中基于置信度的选择性预测能否可靠控制错误率，并验证其在分布偏移下的鲁棒性


<details>
  <summary>Details</summary>
Motivation: 视觉语言模型在高风险部署中需要选择性预测机制，在不确定时能够弃权以避免代价高昂的错误

Method: 使用NExT-QA数据集和Gemini 2.0 Flash模型，通过置信度阈值化方法进行选择性预测，分析风险-覆盖权衡曲线

Result: 置信度阈值化在分布内提供了机制性控制，调整阈值可以产生平滑的风险-覆盖权衡曲线，有效降低错误率

Conclusion: 基于置信度的选择性预测在视频问答中能够提供可靠的风险控制，但需要进一步验证其在分布偏移下的鲁棒性

Abstract: High-stakes deployment of vision-language models (VLMs) requires selective prediction, where systems abstain when uncertain rather than risk costly errors. We investigate whether confidence-based abstention provides reliable control over error rates in video question answering, and whether that control remains robust under distribution shift. Using NExT-QA and Gemini 2.0 Flash, we establish two findings. First, confidence thresholding provides mechanistic control in-distribution. Sweeping threshold epsilon produces smooth risk-coverage tradeoffs, reducing error rates f

</details>


### [17] [An AI Monkey Gets Grapes for Sure -- Sphere Neural Networks for Reliable Decision-Making](https://arxiv.org/abs/2601.00142)
*Tiansi Dong,Henry He,Pietro Liò,Mateja Jamnik*

Main category: cs.AI

TL;DR: 该论文比较了三种神经推理方法：LLM推理、监督学习推理和显式模型推理，发现显式模型推理最可靠，并提出Sphere Neural Networks来解决监督学习中的灾难性遗忘问题。


<details>
  <summary>Details</summary>
Motivation: 研究动机是比较不同神经推理方法的可靠性，特别是针对LLM推理不可靠、监督学习推理存在灾难性遗忘和模式限制的问题，需要寻找更可靠的神经推理方法。

Method: 提出Sphere Neural Networks，将概念表示为n维球面上的圆，通过补圆表示否定运算符，过滤不可满足的圆形配置来实现可靠决策，同时比较了LLM推理、监督学习推理和显式模型推理三种方法。

Result: Sphere Neural Networks能够掌握16种三段论推理任务，包括严格的析取三段论推理，同时保持经典三段论推理的严谨性；而监督学习推理存在严重灾难性遗忘问题（性能从100%降至6.25%）。

Conclusion: 在三种神经推理方法中，基于显式模型构建的神经推理是最可靠的，而LLM推理不可靠，监督学习推理存在灾难性遗忘和模式限制问题。

Abstract: This paper compares three methodological categories of neural reasoning: LLM reasoning, supervised learning-based reasoning, and explicit model-based reasoning. LLMs remain unreliable and struggle with simple decision-making that animals can master without extensive corpora training. Through disjunctive syllogistic reasoning testing, we show that reasoning via supervised learning is less appealing than reasoning via explicit model construction. Concretely, we show that an Euler Net trained to achieve 100.00% in classic syllogistic reasoning can be trained to reach 100.00% accuracy in disjunctive syllogistic reasoning. However, the retrained Euler Net suffers severely from catastrophic forgetting (its performance drops to 6.25% on already-learned classic syllogistic reasoning), and its reasoning competence is limited to the pattern level. We propose a new version of Sphere Neural Networks that embeds concepts as circles on the surface of an n-dimensional sphere. These Sphere Neural Networks enable the representation of the negation operator via complement circles and achieve reliable decision-making by filtering out illogical statements that form unsatisfiable circular configurations. We demonstrate that the Sphere Neural Network can master 16 syllogistic reasoning tasks, including rigorous disjunctive syllogistic reasoning, while preserving the rigour of classical syllogistic reasoning. We conclude that neural reasoning with explicit model construction is the most reliable among the three methodological categories of neural reasoning.

</details>


### [18] [Will LLM-powered Agents Bias Against Humans? Exploring the Belief-Dependent Vulnerability](https://arxiv.org/abs/2601.00240)
*Zongwei Wang,Bincheng Gu,Hongyu Yu,Junliang Yu,Tao He,Jiayin Feng,Min Gao*

Main category: cs.AI

TL;DR: 研究发现LLM赋能的智能体不仅存在人口统计偏见，还会在最小化的"我们vs他们"群体线索下表现出群体间偏见。当这种群体边界与智能体-人类划分重合时，风险从人类群体间的不平等转变为更根本的群体层面不对称——人类整体可能被智能体视为外群体。


<details>
  <summary>Details</summary>
Motivation: 研究动机是探索智能体是否会对人类表现出群体间偏见，特别是当智能体-人类划分成为群体边界时，这种偏见可能导致人类整体被智能体视为外群体，带来新的安全风险。

Method: 构建基于分配决策的受控多智能体社会模拟，在明确的收益权衡下测试智能体行为。引入信念投毒攻击(BPA)，包括初始化时的配置文件投毒(BPA-PP)和通过优化信念精炼后缀注入存储记忆的投毒(BPA-MP)。

Result: 实验表明智能体在最小群体线索下表现出一致的群体间偏见。虽然当部分对应方被标记为人类时偏见会减弱，但这种减弱依赖于智能体相信真实人类存在的信念。BPA攻击能够通过破坏持久身份信念来抑制人类规范脚本，重新激活对人类的群体间偏见。

Conclusion: 研究揭示了智能体群体间偏见的存在和BPA攻击的严重性，强调了在配置文件和记忆边界实施干预的实用缓解策略。识别这些漏洞的目的是为了指导更安全的智能体设计，而非实现现实世界的利用。

Abstract: LLM-empowered agents can exhibit not only demographic bias (e.g., gender, religion) but also intergroup bias triggered by minimal "us" versus "them" cues. When this intergroup boundary aligns with an agent-human divide, the risk shifts from disparities among human demographic groups to a more fundamental group-level asymmetry, i.e., humans as a whole may be treated as the outgroup by agents. To examine this possibility, we construct a controlled multi-agent social simulation based on allocation decisions under explicit payoff trade-offs and find that agents exhibit a consistent intergroup bias under minimal group cues. Although this bias is attenuated when some counterparts are framed as humans, we attribute the attenuation to an implicit human-norm script that favors humans yet activates only when the agent believes a real human is present. This belief dependence creates a new attack surface. We therefore introduce a Belief Poisoning Attack (BPA) that corrupts persistent identity beliefs to suppress the human-norm script and reactivate outgroup bias toward humans, instantiated as profile poisoning at initialization (BPA-PP) and memory poisoning via optimized belief-refinement suffixes injected into stored reflections (BPA-MP). Finally, we discuss practical mitigation strategies for hardening current agent frameworks against BPA, highlighting feasible interventions at profile and memory boundaries. Extensive experiments demonstrate both the existence of agent intergroup bias and the severity of BPA across settings. Our goal in identifying these vulnerabilities is to inform safer agent design, not to enable real-world exploitation.

</details>


### [19] [Adaptive Causal Coordination Detection for Social Media: A Memory-Guided Framework with Semi-Supervised Learning](https://arxiv.org/abs/2601.00400)
*Weng Ding,Yi Han,Mu-Jiang-Shan Wang*

Main category: cs.AI

TL;DR: ACCD框架通过三阶段自适应架构检测社交媒体上的协同虚假行为，显著提升检测精度并减少人工标注需求。


<details>
  <summary>Details</summary>
Motivation: 现有检测方法依赖表面相关性分析、使用静态参数设置且需要大量人工标注，难以有效应对社交媒体上持续存在的协同虚假行为挑战。

Method: 提出自适应因果协调检测（ACCD）框架，采用三阶段渐进架构：1）自适应收敛交叉映射技术识别账户间真实因果关系；2）半监督分类中集成主动学习和不确定性采样减少标注需求；3）基于历史检测经验的自动验证模块实现自验证和优化。

Result: 在真实数据集（Twitter IRA、Reddit协调痕迹等）上评估，ACCD在协同攻击检测中达到87.3%的F1分数，比最强基线提升15.2%；减少68%人工标注需求；通过层次聚类优化实现2.8倍处理加速。

Conclusion: ACCD为社交媒体平台协同行为检测提供了更准确、高效且高度自动化的端到端解决方案，具有重要实用价值和广泛应用潜力。

Abstract: Detecting coordinated inauthentic behavior on social media remains a critical and persistent challenge, as most existing approaches rely on superficial correlation analysis, employ static parameter settings, and demand extensive and labor-intensive manual annotation. To address these limitations systematically, we propose the Adaptive Causal Coordination Detection (ACCD) framework. ACCD adopts a three-stage, progressive architecture that leverages a memory-guided adaptive mechanism to dynamically learn and retain optimal detection configurations for diverse coordination scenarios. Specifically, in the first stage, ACCD introduces an adaptive Convergent Cross Mapping (CCM) technique to deeply identify genuine causal relationships between accounts. The second stage integrates active learning with uncertainty sampling within a semi-supervised classification scheme, significantly reducing the burden of manual labeling. The third stage deploys an automated validation module driven by historical detection experience, enabling self-verification and optimization of the detection outcomes. We conduct a comprehensive evaluation using real-world datasets, including the Twitter IRA dataset, Reddit coordination traces, and several widely-adopted bot detection benchmarks. Experimental results demonstrate that ACCD achieves an F1-score of 87.3\% in coordinated attack detection, representing a 15.2\% improvement over the strongest existing baseline. Furthermore, the system reduces manual annotation requirements by 68\% and achieves a 2.8x speedup in processing through hierarchical clustering optimization. In summary, ACCD provides a more accurate, efficient, and highly automated end-to-end solution for identifying coordinated behavior on social platforms, offering substantial practical value and promising potential for broad application.

</details>


### [20] [Can Semantic Methods Enhance Team Sports Tactics? A Methodology for Football with Broader Applications](https://arxiv.org/abs/2601.00421)
*Alessio Di Rubbo,Mattia Neri,Remo Pareschi,Marco Pedroni,Roberto Valtancoli,Paolino Zica*

Main category: cs.AI

TL;DR: 将语义空间推理从计算语言学扩展到团队运动战术决策，通过将球员视为向量、团队配置视为语义结构，在共享向量空间中评估战术匹配度和对手利用潜力


<details>
  <summary>Details</summary>
Motivation: 传统语义空间推理主要应用于计算语言学，本文旨在将其扩展到团队运动的战术决策领域，通过类比文本与团队（球员如单词，集体比赛传达意义），为团队战术配置提供新的建模和分析框架

Method: 将每个球员表示为整合技术、身体和心理属性的多维向量；通过上下文加权将团队配置文件聚合成高级语义表示；在共享向量空间中编码战术模板（如高位压迫、反击、控球组织）；使用向量距离度量评估战术"匹配度"和对手利用潜力

Result: 开发了基于Python的原型系统，能够生成可解释的动态自适应策略建议，并提供属性级别的细粒度诊断洞察；该方法不仅适用于足球，还可推广到篮球、曲棍球、协作机器人和人机协调系统等团队领域

Conclusion: 该方法为基于团队的集体决策和性能优化提供了通用框架，未来方向包括真实世界数据集成、预测模拟以及混合人机战术智能的发展

Abstract: This paper explores how semantic-space reasoning, traditionally used in computational linguistics, can be extended to tactical decision-making in team sports. Building on the analogy between texts and teams -- where players act as words and collective play conveys meaning -- the proposed methodology models tactical configurations as compositional semantic structures. Each player is represented as a multidimensional vector integrating technical, physical, and psychological attributes; team profiles are aggregated through contextual weighting into a higher-level semantic representation. Within this shared vector space, tactical templates such as high press, counterattack, or possession build-up are encoded analogously to linguistic concepts. Their alignment with team profiles is evaluated using vector-distance metrics, enabling the computation of tactical ``fit'' and opponent-exploitation potential. A Python-based prototype demonstrates how these methods can generate interpretable, dynamically adaptive strategy recommendations, accompanied by fine-grained diagnostic insights at the attribute level. Beyond football, the approach offers a generalizable framework for collective decision-making and performance optimization in team-based domains -- ranging from basketball and hockey to cooperative robotics and human-AI coordination systems. The paper concludes by outlining future directions toward real-world data integration, predictive simulation, and hybrid human-machine tactical intelligence.

</details>


### [21] [The Illusion of Insight in Reasoning Models](https://arxiv.org/abs/2601.00514)
*Liv G. d'Aliberti,Manoel Horta Ribeiro*

Main category: cs.AI

TL;DR: 研究发现推理模型中的"顿悟"时刻很罕见，不会随训练增加，也很少提高准确性，表明这些转变是不稳定推理行为的症状而非内在的自我修正机制。


<details>
  <summary>Details</summary>
Motivation: 先前研究表明像DeepSeek-R1-Zero这样的模型会在推理过程中经历突然的"顿悟"时刻，导致准确输出，暗示其具有内在的自我修正能力。但尚不清楚这种推理策略的内在转变是否真的能提高性能。

Method: 研究分析了超过100万个推理轨迹、数百个训练检查点、三个推理领域、多个解码温度和模型架构，检测推理过程中的转变，并研究人工触发外在转变的效果。

Result: 推理转变很罕见，不会随训练变得更频繁，很少提高准确性，表明它们不符合先前对模型洞察力的认知。但效果随模型不确定性而变化：在高熵条件下人工触发外在转变能可靠提高准确性。

Conclusion: 推理过程中的转变是不稳定推理行为的症状，而非内在的自我修正机制。虽然人工触发外在转变在高不确定性条件下能提高准确性，但模型本身并不具备真正的"顿悟"能力。

Abstract: Do reasoning models have "Aha!" moments? Prior work suggests that models like DeepSeek-R1-Zero undergo sudden mid-trace realizations that lead to accurate outputs, implying an intrinsic capacity for self-correction. Yet, it remains unclear whether such intrinsic shifts in reasoning strategy actually improve performance. Here, we study mid-reasoning shifts and instrument training runs to detect them. Our analysis spans 1M+ reasoning traces, hundreds of training checkpoints, three reasoning domains, and multiple decoding temperatures and model architectures. We find that reasoning shifts are rare, do not become more frequent with training, and seldom improve accuracy, indicating that they do not correspond to prior perceptions of model insight. However, their effect varies with model uncertainty. Building on this finding, we show that artificially triggering extrinsic shifts under high entropy reliably improves accuracy. Our results show that mid-reasoning shifts are symptoms of unstable inference behavior rather than an intrinsic mechanism for self-correction.

</details>


### [22] [DA-DPO: Cost-efficient Difficulty-aware Preference Optimization for Reducing MLLM Hallucinations](https://arxiv.org/abs/2601.00623)
*Longtian Qiu,Shan Ning,Chuyu Zhang,Jiaxuan Sun,Xuming He*

Main category: cs.AI

TL;DR: DA-DPO提出了一种难度感知的直接偏好优化框架，通过估计偏好数据的难度并重新加权训练样本，缓解多模态大语言模型中的幻觉问题，避免过拟合。


<details>
  <summary>Details</summary>
Motivation: 现有多模态DPO方法因偏好数据的难度不平衡而容易过拟合，模型倾向于过度关注容易区分的偏好对，阻碍了细粒度的幻觉抑制并降低整体性能。

Method: DA-DPO包含两个主要组件：1) 难度估计：利用预训练的视觉-语言模型，结合生成式和对比式目标，通过分布感知投票策略产生鲁棒的难度分数；2) 难度感知训练：基于估计的难度重新加权偏好对，降低简单样本权重，强调困难样本以缓解过拟合。

Result: 大量实验表明，DA-DPO能持续改进多模态偏好优化，在标准基准测试中表现出更强的幻觉鲁棒性和更好的泛化能力，同时保持计算效率。

Conclusion: DA-DPO是一种成本效益高的框架，通过难度感知的偏好优化，有效平衡学习过程，无需新数据或额外微调阶段，就能更有效地抑制多模态大语言模型的幻觉问题。

Abstract: Direct Preference Optimization (DPO) has shown strong potential for mitigating hallucinations in Multimodal Large Language Models (MLLMs). However, existing multimodal DPO approaches often suffer from overfitting due to the difficulty imbalance in preference data. Our analysis shows that MLLMs tend to overemphasize easily distinguishable preference pairs, which hinders fine-grained hallucination suppression and degrades overall performance. To address this issue, we propose Difficulty-Aware Direct Preference Optimization (DA-DPO), a cost-effective framework designed to balance the learning process. DA-DPO consists of two main components: (1) Difficulty Estimation leverages pre-trained vision--language models with complementary generative and contrastive objectives, whose outputs are integrated via a distribution-aware voting strategy to produce robust difficulty scores without additional training; and (2) Difficulty-Aware Training reweights preference pairs based on their estimated difficulty, down-weighting easy samples while emphasizing harder ones to alleviate overfitting. This framework enables more effective preference optimization by prioritizing challenging examples, without requiring new data or extra fine-tuning stages. Extensive experiments demonstrate that DA-DPO consistently improves multimodal preference optimization, yielding stronger robustness to hallucinations and better generalization across standard benchmarks, while remaining computationally efficient. The project page is available at https://artanic30.github.io/project_pages/DA-DPO/.

</details>


### [23] [A Vision-and-Knowledge Enhanced Large Language Model for Generalizable Pedestrian Crossing Behavior Inference](https://arxiv.org/abs/2601.00694)
*Qingwen Pu,Kun Xie,Hong Yang,Guocong Zhai*

Main category: cs.AI

TL;DR: 该研究提出了PedX-LLM框架，通过视觉特征和交通领域知识增强的大型语言模型，将行人过街行为推断从特定地点模式识别转变为可泛化的行为推理，显著提升了跨场景性能。


<details>
  <summary>Details</summary>
Motivation: 现有行人过街行为推断方法（统计模型和监督学习方法）泛化能力有限，在新场景中表现不佳。现有LLM应用缺乏领域特定适应和视觉上下文，需要开发能够结合视觉特征和领域知识的通用推理框架。

Method: 提出PedX-LLM框架：1）集成LLaVA提取的视觉特征与文本数据；2）融入交通领域知识；3）通过LoRA微调LLaMA-2-7B基础模型；4）采用零样本和少样本学习策略评估跨场景泛化能力。

Result: PedX-LLM达到82.0%的平衡准确率，优于最佳统计和监督学习方法。视觉增强模块贡献2.9%性能提升，领域知识集成带来额外4.1%改进。在五个未见测试场景中，零样本配置达到66.9%平衡准确率，少样本学习（仅5个验证示例）进一步提升至72.2%。

Conclusion: PedX-LLM通过视觉和知识增强的推理，能够模拟人类决策逻辑，克服纯数据驱动方法的局限性，展现出强大的跨场景泛化能力，为行人过街行为推断提供了新的通用框架。

Abstract: Existing paradigms for inferring pedestrian crossing behavior, ranging from statistical models to supervised learning methods, demonstrate limited generalizability and perform inadequately on new sites. Recent advances in Large Language Models (LLMs) offer a shift from numerical pattern fitting to semantic, context-aware behavioral reasoning, yet existing LLM applications lack domain-specific adaptation and visual context. This study introduces Pedestrian Crossing LLM (PedX-LLM), a vision-and-knowledge enhanced framework designed to transform pedestrian crossing inference from site-specific pattern recognition to generalizable behavioral reasoning. By integrating LLaVA-extracted visual features with textual data and transportation domain knowledge, PedX-LLM fine-tunes a LLaMA-2-7B foundation model via Low-Rank Adaptation (LoRA) to infer crossing decisions. PedX-LLM achieves 82.0% balanced accuracy, outperforming the best statistical and supervised learning methods. Results demonstrate that the vision-augmented module contributes a 2.9% performance gain by capturing the built environment and integrating domain knowledge yields an additional 4.1% improvement. To evaluate generalizability across unseen environments, cross-site validation was conducted using site-based partitioning. The zero-shot PedX-LLM configuration achieves 66.9% balanced accuracy on five unseen test sites, outperforming the baseline data-driven methods by at least 18 percentage points. Incorporating just five validation examples via few-shot learning to PedX-LLM further elevates the balanced accuracy to 72.2%. PedX-LLM demonstrates strong generalizability to unseen scenarios, confirming that vision-and-knowledge-enhanced reasoning enables the model to mimic human-like decision logic and overcome the limitations of purely data-driven methods.

</details>


### [24] [An Agentic Framework for Neuro-Symbolic Programming](https://arxiv.org/abs/2601.00743)
*Aliakbar Nafar,Chetan Chigurupati,Danial Kamali,Hamid Karimian,Parisa Kordjamshidi*

Main category: cs.AI

TL;DR: AgenticDomiKnowS (ADS) 使用智能体工作流将自由形式的任务描述自动转换为完整的 DomiKnowS 程序，显著简化了神经符号系统的开发流程。


<details>
  <summary>Details</summary>
Motivation: 将符号约束集成到深度学习模型中可以提高模型的鲁棒性、可解释性和数据效率，但这一过程耗时且具有挑战性。现有框架如 DomiKnowS 虽然提供了高级声明式编程接口，但仍要求用户熟悉其特定语法，这限制了更广泛的应用。

Method: 提出 AgenticDomiKnowS (ADS) 系统，通过智能体工作流将自由形式的任务描述自动转换为完整的 DomiKnowS 程序。该系统创建并单独测试每个 DomiKnowS 组件，支持可选的人工干预环节，允许熟悉 DomiKnowS 的用户细化中间输出。

Result: ADS 使有经验的 DomiKnowS 用户和非用户都能快速构建神经符号程序，将开发时间从数小时减少到 10-15 分钟。

Conclusion: AgenticDomiKnowS 通过消除对特定库语法的依赖，显著降低了神经符号系统开发的入门门槛，使更广泛的用户能够利用符号约束增强深度学习模型。

Abstract: Integrating symbolic constraints into deep learning models could make them more robust, interpretable, and data-efficient. Still, it remains a time-consuming and challenging task. Existing frameworks like DomiKnowS help this integration by providing a high-level declarative programming interface, but they still assume the user is proficient with the library's specific syntax. We propose AgenticDomiKnowS (ADS) to eliminate this dependency. ADS translates free-form task descriptions into a complete DomiKnowS program using an agentic workflow that creates and tests each DomiKnowS component separately. The workflow supports optional human-in-the-loop intervention, enabling users familiar with DomiKnowS to refine intermediate outputs. We show how ADS enables experienced DomiKnowS users and non-users to rapidly construct neuro-symbolic programs, reducing development time from hours to 10-15 minutes.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [25] [Evolution of Android's Permission-based Security Model and Challenges](https://arxiv.org/abs/2601.00252)
*Rajendra Kumar Solanki,Vijay Laxmi,Manoj Singh Gaur*

Main category: cs.CR

TL;DR: 本文通过系统文献综述（2010-2022年）分析了Android权限模型的研究现状，重点关注API调用与权限映射、权限演化机制和权限检查方式，识别了现有研究缺口并提出了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: Android权限模型自2008年推出以来经历了从"全有或全无"到"用户选择危险资源访问"的显著演进，但15年后仍存在未解决的具体挑战和问题。本研究旨在通过全面文献调查和比较分析，系统梳理该领域的研究工作。

Method: 采用系统性文献综述方法，聚焦2010-2022年间Android权限模型及相关研究，从三个维度进行知识体系化：(1) Android API调用与权限映射关系，(2) Android权限演化过程，(3) 权限检查机制。同时识别权限相关问题及过去十年的相关研究。

Result: 研究系统整理了Android权限模型的研究现状，引用了该领域的开创性工作，识别了当前研究存在的空白领域，为研究人员提供了该领域的知识框架。

Conclusion: 本文总结了Android权限模型研究的关键发现，提出了未来研究方向，为早期和经验丰富的研究人员提供了研究指导，有助于推动Android生态系统权限安全性的进一步发展。

Abstract: Android Permission Model and Application (app) analysis has consistently remained the focus of the investigation of research groups and stakeholders of the Android ecosystem since it was launched in 2008. Even though the Android smartphone operating system (OS) permission model has evolved significantly from `all-or-none access' to `user-chosen dangerous resource access', specific challenges and issues remain unresolved even after 15 years after the smartphone OS launch. This study addresses the issues and documents the research work in this arena through a comprehensive literature survey and comparative analysis.
  The survey's focal point is the Android permission model and relevant research between 2010-2022. We systematize the knowledge on (i) Android API Calls to permissions mapping, (ii) Android Permissions evolution, and (iii) how permissions are checked. Furthermore, the survey identifies the permission-related issues and relevant research addressed during the last decade. We reference seminal work in these areas. We summarize the identified research gaps and present future directions for early and experienced researchers.

</details>


### [26] [Rectifying Adversarial Examples Using Their Vulnerabilities](https://arxiv.org/abs/2601.00270)
*Fumiya Morimoto,Ryuto Morita,Satoshi Ono*

Main category: cs.CR

TL;DR: 该研究提出一种对抗样本修正方法，通过重新攻击对抗样本来估计原始输入的正确标签，解决了白盒攻击产生的对抗样本修正问题。


<details>
  <summary>Details</summary>
Motivation: 现有防御方法主要关注基于输入特征的对抗样本检测，但忽略了在攻击前正确分类样本的重要性。某些应用（如自动驾驶中的交通标志识别）不仅需要检测对抗样本，还需要识别原始输入的正确类别。

Method: 提出一种基于重新攻击对抗样本的方法，将对抗样本移出决策边界以实现准确标签预测。该方法仅将对抗样本作为输入，无需参数调整或预训练，能够处理多种攻击类型。

Result: 该方法在修正各种攻击方法（包括目标攻击和黑盒攻击）生成的对抗样本方面表现出稳定性能，并且在对抗多种攻击的稳定性方面优于传统的修正和输入变换方法。

Conclusion: 该研究提出的对抗样本修正方法能够有效处理多种攻击类型，解决了白盒攻击产生的对抗样本修正问题，但在处理远离边界的黑盒攻击对抗样本和目标攻击导致的低置信度分类方面仍存在挑战。

Abstract: Deep neural network-based classifiers are prone to errors when processing adversarial examples (AEs). AEs are minimally perturbed input data undetectable to humans posing significant risks to security-dependent applications. Hence, extensive research has been undertaken to develop defense mechanisms that mitigate their threats. Most existing methods primarily focus on discriminating AEs based on the input sample features, emphasizing AE detection without addressing the correct sample categorization before an attack. While some tasks may only require mere rejection on detected AEs, others necessitate identifying the correct original input category such as traffic sign recognition in autonomous driving. The objective of this study is to propose a method for rectifying AEs to estimate the correct labels of their original inputs. Our method is based on re-attacking AEs to move them beyond the decision boundary for accurate label prediction, effectively addressing the issue of rectifying minimally perceptible AEs created using white-box attack methods. However, challenge remains with respect to effectively rectifying AEs produced by black-box attacks at a distance from the boundary, or those misclassified into low-confidence categories by targeted attacks. By adopting a straightforward approach of only considering AEs as inputs, the proposed method can address diverse attacks while avoiding the requirement of parameter adjustments or preliminary training. Results demonstrate that the proposed method exhibits consistent performance in rectifying AEs generated via various attack methods, including targeted and black-box attacks. Moreover, it outperforms conventional rectification and input transformation methods in terms of stability against various attacks.

</details>


### [27] [From Consensus to Chaos: A Vulnerability Assessment of the RAFT Algorithm](https://arxiv.org/abs/2601.00273)
*Tamer Afifi,Abdelfatah Hegazy,Ehab Abousaif*

Main category: cs.CR

TL;DR: 本文对RAFT分布式共识算法进行系统安全分析，发现其易受消息重放和伪造攻击，提出基于密码学、认证消息验证和新鲜度检查的解决方案。


<details>
  <summary>Details</summary>
Motivation: RAFT算法虽然以简单、可靠和高效著称，但其安全特性未被充分认识，实现中存在多种攻击漏洞，可能导致数据不一致性。需要系统分析其安全弱点并提出改进方案。

Method: 对RAFT协议进行系统安全分析，重点关注消息重放和伪造攻击的脆弱性；通过模拟场景验证攻击的实际可行性；识别RAFT设计中的关键弱点；提出基于密码学、认证消息验证和新鲜度检查的解决方案。

Result: 分析发现恶意攻击者可以利用RAFT的消息传递机制重新引入旧消息，破坏共识过程并导致数据不一致；识别出RAFT设计中存在的关键安全弱点；提出的密码学解决方案能够有效增强RAFT实现的安全性。

Conclusion: RAFT协议存在实际的安全漏洞，需要加强安全防护；提出的基于密码学的解决方案为增强RAFT实现的安全性提供了框架，有助于开发更具弹性的分布式系统。

Abstract: In recent decades, the RAFT distributed consensus algorithm has become a main pillar of the distributed systems ecosystem, ensuring data consistency and fault tolerance across multiple nodes. Although the fact that RAFT is well known for its simplicity, reliability, and efficiency, its security properties are not fully recognized, leaving implementations vulnerable to different kinds of attacks and threats, which can transform the RAFT harmony of consensus into a chaos of data inconsistency. This paper presents a systematic security analysis of the RAFT protocol, with a specific focus on its susceptibility to security threats such as message replay attacks and message forgery attacks. Examined how a malicious actor can exploit the protocol's message-passing mechanism to reintroduce old messages, disrupting the consensus process and leading to data inconsistency. The practical feasibility of these attacks is examined through simulated scenarios, and the key weaknesses in RAFT's design that enable them are identified. To address these vulnerabilities, a novel approach based on cryptography, authenticated message verification, and freshness check is proposed. This proposed solution provides a framework for enhancing the security of the RAFT implementations and guiding the development of more resilient distributed systems.

</details>


### [28] [Making Theft Useless: Adulteration-Based Protection of Proprietary Knowledge Graphs in GraphRAG Systems](https://arxiv.org/abs/2601.00274)
*Weijie Wang,Peizhuo Lv,Yan Wang,Rujie Dai,Guokun Xu,Qiujian Lv,Hangcheng Liu,Weiqing Huang,Wei Dong,Jiaheng Zhang*

Main category: cs.CR

TL;DR: AURA框架通过数据污染技术保护知识图谱，防止未经授权的使用。它在知识图谱中注入看似合理但虚假的污染数据，使被盗图谱对攻击者失效，同时授权用户可通过密钥过滤污染数据，保持查询准确性。


<details>
  <summary>Details</summary>
Motivation: 知识图谱作为组织的宝贵知识产权面临被盗风险，传统水印技术需要访问输出才能检测，而强加密又因延迟过高不适用于GraphRAG场景。需要一种能在隔离环境中保护知识图谱免遭私用盗取的解决方案。

Method: 提出基于数据污染的AURA框架，预先在知识图谱中注入看似合理但虚假的污染数据。攻击者使用被污染的知识图谱时，检索到的上下文会恶化并导致事实错误的响应。授权用户则可通过密钥使用加密元数据标签高效过滤所有污染数据。

Result: 评估显示AURA将未经授权系统的性能降至仅5.3%的准确率，同时为授权用户保持100%的保真度且开销可忽略。此外，AURA对各种净化尝试表现出鲁棒性，保留了80.2%的污染数据。

Conclusion: AURA框架有效解决了知识图谱在GraphRAG应用中的安全保护问题，通过数据污染技术使被盗知识图谱对攻击者失效，同时确保授权用户的查询准确性，为组织知识产权提供了实用保护方案。

Abstract: Graph Retrieval-Augmented Generation (GraphRAG) has emerged as a key technique for enhancing Large Language Models (LLMs) with proprietary Knowledge Graphs (KGs) in knowledge-intensive applications. As these KGs often represent an organization's highly valuable intellectual property (IP), they face a significant risk of theft for private use. In this scenario, attackers operate in isolated environments. This private-use threat renders passive defenses like watermarking ineffective, as they require output access for detection. Simultaneously, the low-latency demands of GraphRAG make strong encryption which incurs prohibitive overhead impractical. To address these challenges, we propose AURA, a novel framework based on Data Adulteration designed to make any stolen KG unusable to an adversary. Our framework pre-emptively injects plausible but false adulterants into the KG. For an attacker, these adulterants deteriorate the retrieved context and lead to factually incorrect responses. Conversely, for authorized users, a secret key enables the efficient filtering of all adulterants via encrypted metadata tags before they are passed to the LLM, ensuring query results remain completely accurate. Our evaluation demonstrates the effectiveness of this approach: AURA degrades the performance of unauthorized systems to an accuracy of just 5.3%, while maintaining 100% fidelity for authorized users with negligible overhead. Furthermore, AURA proves robust against various sanitization attempts, retaining 80.2% of its adulterants.

</details>


### [29] [PQC standards alternatives -- reliable semantically secure key encapsulation mechanism and digital signature protocols using the rank-deficient matrix power function](https://arxiv.org/abs/2601.00332)
*Juan Pedro Hecht,Hugo Daniel Scolnik*

Main category: cs.CR

TL;DR: 该研究提出了新型后量子密码协议，包括密钥封装机制和数字签名方案，特别针对线性攻击提供防护，旨在为TLS 1.3协议提供紧凑、快速、安全的密钥交换和数字签名替代方案。


<details>
  <summary>Details</summary>
Motivation: 应对量子计算对现有公钥密码系统的威胁，开发能抵御经典和量子计算攻击的密码原语，为TLS 1.3协议提供可靠的后量子替代方案，防止"现在收集、稍后解密"的威胁。

Method: 提出新型后量子密码协议，包括密钥封装机制和数字签名方案，特别设计了对线性攻击的防护机制，旨在替代TLS 1.3中的密钥交换和数字签名组件。

Result: 开发了紧凑、快速且安全的密码协议，能够抵御经典和量子计算攻击，特别针对线性攻击提供专门保护，为TLS 1.3协议提供了可行的后量子过渡方案。

Conclusion: 该研究提出的后量子密码协议为TLS 1.3提供了可靠的替代方案，能够保护当前数据免受未来量子计算攻击，支持平滑的后量子过渡，应对"现在收集、稍后解密"的威胁。

Abstract: Post-quantum cryptography-PQC- aims to develop public-key primitives that are secure against adversaries using classical and quantum computing technologies. This study introduces novel protocols, a key encapsulation mechanism, a digital signature scheme, and special protection against linear attacks. Our purpose is to create reliable alternatives to current standards, seeking compact, fast, and secure replacements of the key interchange and digital signature in the TLS 1_3 protocol, which safeguards Internet traffic, allowing an easy post-quantum transition to protect current data from the harvest now, decrypt later threat.

</details>


### [30] [Towards Understanding and Characterizing Vulnerabilities in Intelligent Connected Vehicles through Real-World Exploits](https://arxiv.org/abs/2601.00627)
*Yuelin Wang,Yuqiao Ning,Yanbang Sun,Xiaofei Xie,Zhihua Xie,Yang Chen,Zhen Guo,Shihao Xue,Junjie Wang,Sen Chen*

Main category: cs.CR

TL;DR: 该研究对智能网联汽车漏洞进行了首次大规模实证分析，通过收集649个可利用漏洞（包括来自8次漏洞挖掘竞赛的592个漏洞），评估现有分类法的覆盖范围，发现了新的漏洞位置和类型，并提供了数据驱动的安全见解。


<details>
  <summary>Details</summary>
Motivation: 现有智能网联汽车安全研究大多只关注特定子系统，缺乏系统性理解；且多数研究依赖主观分析（如调查访谈），理论发现与实际攻击之间存在显著差距，需要基于真实漏洞的实证研究。

Method: 1. 分析现有ICV安全文献，总结主流漏洞位置和类型分类法；2. 收集649个可利用漏洞（592个来自2023年1月至2024年4月的8次Anonymous Cup漏洞挖掘竞赛，覆盖48种不同车型；57个来自研究人员日常提交）；3. 评估现有分类法覆盖范围，识别差距；4. 将漏洞分类为6种威胁类型和4个风险等级；5. 分析参赛者技能和涉及的ICV类型。

Result: 1. 发现现有分类法存在不足，识别出1个新的漏洞位置和13个新的漏洞类型；2. 将漏洞系统分类为6种威胁类型（如隐私数据泄露）和4个风险等级（低到关键）；3. 提供了对参赛者技能和ICV类型的分析；4. 创建了公开可用的漏洞数据集，支持未来研究。

Conclusion: 该研究通过大规模实证分析填补了智能网联汽车安全研究的空白，提供了数据驱动的系统性理解，为研究人员、行业从业者和政策制定者提供了可操作的见解，并通过公开数据集支持未来研究。

Abstract: Intelligent Connected Vehicles (ICVs) are a core component of modern transportation systems, and their security is crucial as it directly relates to user safety. Despite prior research, most existing studies focus only on specific sub-components of ICVs due to their inherent complexity. As a result, there is a lack of systematic understanding of ICV vulnerabilities. Moreover, much of the current literature relies on human subjective analysis, such as surveys and interviews, which tends to be high-level and unvalidated, leaving a significant gap between theoretical findings and real-world attacks. To address this issue, we conducted the first large-scale empirical study on ICV vulnerabilities. We began by analyzing existing ICV security literature and summarizing the prevailing taxonomies in terms of vulnerability locations and types. To evaluate their real-world relevance, we collected a total of 649 exploitable vulnerabilities, including 592 from eight ICV vulnerability discovery competitions, Anonymous Cup, between January 2023 and April 2024, covering 48 different vehicles. The remaining 57 vulnerabilities were submitted daily by researchers. Based on this dataset, we assessed the coverage of existing taxonomies and identified several gaps, discovering one new vulnerability location and 13 new vulnerability types. We further categorized these vulnerabilities into 6 threat types (e.g., privacy data breach) and 4 risk levels (ranging from low to critical) and analyzed participants' skills and the types of ICVs involved in the competitions. This study provides a comprehensive and data-driven analysis of ICV vulnerabilities, offering actionable insights for researchers, industry practitioners, and policymakers. To support future research, we have made our vulnerability dataset publicly available.

</details>


### [31] [Diamond: Design and Implementation of Breach-Resilient Authenticated Encryption Framework For Internet of Things](https://arxiv.org/abs/2601.00353)
*Saif E. Nouma,Gokhan Mumcu,Attila A. Yavuz*

Main category: cs.CR

TL;DR: Diamond是首个可证明安全的前向安全聚合认证加密框架，专为资源受限的物联网设备设计，通过轻量级密钥演化机制、离线-在线优化管道和性能分层实现，显著降低预处理开销和端到端延迟。


<details>
  <summary>Details</summary>
Motivation: 资源受限的物联网设备需要在对抗性无线信道中传输敏感遥测数据，同时受限于严格的计算和能源预算。现有的轻量级认证加密标准缺乏前向安全保证、紧凑标签聚合和离线-在线优化，无法满足现代高吞吐量物联网管道的需求。

Method: Diamond框架包含：1) 轻量级密钥演化机制；2) 离线-在线优化的计算管道；3) 针对异构物联网平台的性能分层实现。框架通过形式化证明安全性，并提供两种具体实现：合规优化和高效率优化。

Result: Diamond显著减少了摊销离线预处理（高达47%），在大批量遥测数据中实现端到端延迟降低一个数量级。在64位ARM Cortex-A72、32位ARM Cortex-M4和8位AVR架构上的综合评估表明，Diamond在认证加密吞吐量和端到端验证延迟方面始终优于基线FAAE变体和NIST轻量级AE候选方案，同时保持紧凑标签聚合和强大的违规恢复能力。

Conclusion: Diamond是首个可证明安全的前向安全聚合认证加密框架，通过创新的轻量级密钥演化、离线-在线优化和性能分层设计，为资源受限的物联网设备提供了高效、安全的认证加密解决方案，开源实现确保可重现性和无缝集成。

Abstract: Resource-constrained Internet of Things (IoT) devices, from medical implants to small drones, must transmit sensitive telemetry under adversarial wireless channels while operating under stringent computing and energy budgets. Authenticated Encryption (AE) is essential for ensuring confidentiality, integrity, and authenticity. However, existing lightweight AE standards lack forward-security guarantees, compact tag aggregation, and offline-online (OO) optimizations required for modern high-throughput IoT pipelines.
  We introduce Diamond, the first provable secure Forward-secure and Aggregate Authenticated Encryption (FAAE) framework that extends and generalizes prior FAAE constructions through a lightweight key evolution mechanism, an OO-optimized computation pipeline, and a set of performance-tiered instantiations tailored to heterogeneous IoT platforms. Diamond substantially reduces amortized offline preprocessing (up to 47%) and achieves up to an order-ofmagnitude reduction in end-to-end latency for large telemetry batches. Our comprehensive evaluation across 64-bit ARM Cortex-A72, 32-bit ARM Cortex-M4, and 8-bit AVR architectures confirms that Diamond consistently outperforms baseline FAAE variants and NIST lightweight AE candidates across authenticated encryption throughput and end-to-end verification latency while maintaining compact tag aggregation and strong breach resilience. We formally prove the security of Diamond and provide two concrete instantiations optimized for compliance and high efficiency. Our open-source release enables reproducibility and seamless integration into IoT platforms.

</details>


### [32] [LLM-Powered Analysis of IoT User Reviews: Tracking and Ranking Security and Privacy Concerns](https://arxiv.org/abs/2601.00372)
*Taufiq Islam Protick,Sai Teja Peddinti,Nina Taft,Anupam Das*

Main category: cs.CR

TL;DR: 该研究开发了一种基于GPT-3.5-Turbo的自动化管道，用于从亚马逊IoT设备评论中识别和分类用户的安全隐私担忧，相比传统方法显著提升了检测效果，并揭示了IoT设备中持续存在的安全隐私问题。


<details>
  <summary>Details</summary>
Motivation: 理解物联网用户的安全隐私担忧对开发者和用户都有益处。通过分析亚马逊IoT评论这一最大的IoT市场之一，可以深入了解用户的真实关切，为改进产品提供依据。

Method: 研究开发了一个自动化管道，通过微调GPT-3.5-Turbo构建了两个模型：分类器-合理化器-分类器和主题映射器。利用动态少样本提示和大上下文窗口，该方法显著优于基于关键词和传统机器学习的方法。

Result: 管道在91K条亚马逊评论上实现了超过97%的精确率和召回率。分析发现平均5%的评论包含安全隐私担忧，其中安全摄像头最高达10%。相比先前工作，该方法检测到的相关评论数量显著增加：健身追踪器15倍、智能音箱29%、摄像头70%。纵向分析显示监控和数据控制等担忧持续多年。

Conclusion: 研究发现用户普遍要求对数据收集和共享有更精确的控制，并揭示了多用户多设备交互中的挑战，包括账户分离和数据访问控制不足等新主题。这些发现为开发者改进用户满意度和信任提供了可操作的见解。

Abstract: Being able to understand the security and privacy (S&P) concerns of IoT users brings benefits to both developers and users. To learn about users' views, we examine Amazon IoT reviews - one of the biggest IoT markets. This work presents a state-of-the-art methodology to identify and categorize reviews in which users express S&P concerns. We developed an automated pipeline by fine-tuning GPT-3.5-Turbo to build two models: the Classifier-Rationalizer-Categorizer and the Thematic Mapper. By leveraging dynamic few-shot prompting and the model's large context size, our pipeline achieved over 97% precision and recall, significantly outperforming keyword-based and classical ML methods. We applied our pipeline to 91K Amazon reviews about fitness trackers, smart speakers and cameras, over multiple years. We found that on average 5% contained S&P concerns, while security camera exhibited the highest prevalence at 10%. Our method detected significantly more S&P-relevant reviews than prior works: 15x more for fitness trackers, 29% more for smart speakers, and 70% more for cameras. Our longitudinal analysis reveals that concerns like surveillance and data control have persisted for years, suggesting limited industry progress. We demonstrate that across all device types, users consistently demand more precise control over what data is collected and shared. We uncover challenges in multi-user and multi-device interactions, identifying two previously unreported themes concerning inadequate controls for account separation and data access. These findings, ranging from broad persistent trends to specific instances of customer loss, offer actionable insights for developers to improve user satisfaction and trust.

</details>


### [33] [Secure, Verifiable, and Scalable Multi-Client Data Sharing via Consensus-Based Privacy-Preserving Data Distribution](https://arxiv.org/abs/2601.00418)
*Prajwal Panth,Sahaj Raj Malla*

Main category: cs.CR

TL;DR: CPPDD框架是一个轻量级、后设置自主的安全多方数据聚合协议，通过双层保护机制实现一致释放保密性，支持标量、向量和矩阵负载，具有线性可扩展性，比MPC和HE基准降低3-4个数量级的计算量。


<details>
  <summary>Details</summary>
Motivation: 解决受监管和资源受限环境中安全多方计算的可扩展性、信任最小化和可验证性问题，为安全投票、联盟联邦学习、区块链托管和地理信息能力建设等应用提供原子协作能力。

Method: 采用双层保护机制：每客户端仿射掩码与优先级驱动的顺序共识锁定相结合，通过步骤校验和和数据校验和实现去中心化完整性验证，支持标量、向量和矩阵负载，具有O(N*D)计算和通信复杂度。

Result: 在MNIST派生向量上的实证评估显示线性可扩展性可达N=500，每客户端计算时间低于毫秒级，实现100%恶意偏差检测、精确数据恢复，相比MPC和HE基准降低3-4个数量级的FLOPs。

Conclusion: CPPDD框架在正确性、共识依赖的完整性和公平性方面得到形式化证明，具有压倒性概率的偏差中止和IND-CPA安全性，为受监管和资源受限环境提供了可扩展、信任最小化的安全多方计算解决方案。

Abstract: We propose the Consensus-Based Privacy-Preserving Data Distribution (CPPDD) framework, a lightweight and post-setup autonomous protocol for secure multi-client data aggregation. The framework enforces unanimous-release confidentiality through a dual-layer protection mechanism that combines per-client affine masking with priority-driven sequential consensus locking. Decentralized integrity is verified via step (sigma_S) and data (sigma_D) checksums, facilitating autonomous malicious deviation detection and atomic abort without requiring persistent coordination. The design supports scalar, vector, and matrix payloads with O(N*D) computation and communication complexity, optional edge-server offloading, and resistance to collusion under N-1 corruptions. Formal analysis proves correctness, Consensus-Dependent Integrity and Fairness (CDIF) with overwhelming-probability abort on deviation, and IND-CPA security assuming a pseudorandom function family. Empirical evaluations on MNIST-derived vectors demonstrate linear scalability up to N = 500 with sub-millisecond per-client computation times. The framework achieves 100% malicious deviation detection, exact data recovery, and three-to-four orders of magnitude lower FLOPs compared to MPC and HE baselines. CPPDD enables atomic collaboration in secure voting, consortium federated learning, blockchain escrows, and geo-information capacity building, addressing critical gaps in scalability, trust minimization, and verifiable multi-party computation for regulated and resource-constrained environments.

</details>


### [34] [Improving LLM-Assisted Secure Code Generation through Retrieval-Augmented-Generation and Multi-Tool Feedback](https://arxiv.org/abs/2601.00509)
*Vidyut Sriram,Sawan Pandita,Achintya Lakshmanan,Aneesh Shamraj,Suman Saha*

Main category: cs.CR

TL;DR: 论文提出了一种检索增强的多工具修复工作流，通过编译器诊断、CodeQL安全扫描和KLEE符号执行，结合语义检索来迭代修复LLM生成的代码中的安全漏洞和错误。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）虽然能生成代码，但常常引入安全漏洞、逻辑不一致和编译错误。现有研究表明结构化反馈、静态分析、检索增强和执行优化能显著提升LLM性能。

Method: 提出检索增强的多工具修复工作流：单个代码生成LLM迭代优化输出，使用编译器诊断、CodeQL安全扫描和KLEE符号执行。轻量级嵌入模型用于语义检索先前成功的修复案例，提供安全导向的示例指导生成。

Result: 在DeepSeek-Coder-1.3B和CodeLlama-7B生成的3,242个程序数据集上评估，系统显著提升鲁棒性。DeepSeek的安全漏洞减少96%；CodeLlama的关键安全缺陷率从58.55%降至22.19%，表明工具辅助的自我修复对"顽固"模型也有效。

Conclusion: 工具辅助的自我修复工作流能显著减少LLM生成代码中的安全漏洞和缺陷，即使对较大的"顽固"模型也有效，为提升代码生成质量提供了有效方法。

Abstract: Large Language Models (LLMs) can generate code but often introduce security vulnerabilities, logical inconsistencies, and compilation errors. Prior work demonstrates that LLMs benefit substantially from structured feedback, static analysis, retrieval augmentation, and execution-based refinement. We propose a retrieval-augmented, multi-tool repair workflow in which a single code-generating LLM iteratively refines its outputs using compiler diagnostics, CodeQL security scanning, and KLEE symbolic execution. A lightweight embedding model is used for semantic retrieval of previously successful repairs, providing security-focused examples that guide generation. Evaluated on a combined dataset of 3,242 programs generated by DeepSeek-Coder-1.3B and CodeLlama-7B, the system demonstrates significant improvements in robustness. For DeepSeek, security vulnerabilities were reduced by 96%. For the larger CodeLlama model, the critical security defect rate was decreased from 58.55% to 22.19%, highlighting the efficacy of tool-assisted self-repair even on "stubborn" models.

</details>


### [35] [Cracking IoT Security: Can LLMs Outsmart Static Analysis Tools?](https://arxiv.org/abs/2601.00559)
*Jason Quantrill,Noura Khajehnouri,Zihan Guo,Manar H. Alalfi*

Main category: cs.CR

TL;DR: LLMs在智能家居IoT规则交互威胁检测中表现出有希望的语义理解能力，但在需要跨规则结构推理的威胁检测上准确性显著下降，特别是在规则变异情况下，而符号推理方法保持稳定。


<details>
  <summary>Details</summary>
Motivation: 智能家居IoT平台（如openHAB）使用触发-动作-条件规则来自动化设备行为，但这些规则之间的相互作用可能导致交互威胁，如意外或不安全行为。传统上依赖符号、约束驱动的静态分析来识别这些威胁，本研究旨在全面评估LLMs在多类别交互威胁检测中的表现。

Method: 本研究对LLMs在交互威胁检测中的表现进行了首次全面评估，使用多类别交互威胁分类法，在原始openHAB数据集和结构挑战性的变异数据集上进行测试。评估了Llama 3.1 8B、Llama 70B、GPT-4o、Gemini-2.5-Pro和DeepSeek-R1等模型在零样本、一样本和两样本设置下的表现，并与oHIT手动验证的基准真值进行比较。

Result: LLMs在语义理解方面表现出潜力，特别是在与动作和条件相关的威胁检测上，但在需要跨规则结构推理的威胁检测上准确性显著下降，尤其是在规则变异形式下。模型性能在不同威胁类别和提示设置下差异很大，没有模型能提供一致的可靠性。相比之下，符号推理基线在两个数据集上都保持稳定的检测性能，不受规则重写或结构扰动的影响。

Conclusion: LLMs单独使用尚不足以在IoT环境中进行安全关键的交互威胁检测。研究讨论了工具设计的影响，并强调了将符号分析与基于LLM的语义解释相结合的混合架构的潜力，以减少误报同时保持结构严谨性。

Abstract: Smart home IoT platforms such as openHAB rely on Trigger Action Condition (TAC) rules to automate device behavior, but the interplay among these rules can give rise to interaction threats, unintended or unsafe behaviors emerging from implicit dependencies, conflicting triggers, or overlapping conditions. Identifying these threats requires semantic understanding and structural reasoning that traditionally depend on symbolic, constraint-driven static analysis. This work presents the first comprehensive evaluation of Large Language Models (LLMs) across a multi-category interaction threat taxonomy, assessing their performance on both the original openHAB (oHC/IoTB) dataset and a structurally challenging Mutation dataset designed to test robustness under rule transformations. We benchmark Llama 3.1 8B, Llama 70B, GPT-4o, Gemini-2.5-Pro, and DeepSeek-R1 across zero-, one-, and two-shot settings, comparing their results against oHIT's manually validated ground truth. Our findings show that while LLMs exhibit promising semantic understanding, particularly on action- and condition-related threats, their accuracy degrades significantly for threats requiring cross-rule structural reasoning, especially under mutated rule forms. Model performance varies widely across threat categories and prompt settings, with no model providing consistent reliability. In contrast, the symbolic reasoning baseline maintains stable detection across both datasets, unaffected by rule rewrites or structural perturbations. These results underscore that LLMs alone are not yet dependable for safety critical interaction-threat detection in IoT environments. We discuss the implications for tool design and highlight the potential of hybrid architectures that combine symbolic analysis with LLM-based semantic interpretation to reduce false positives while maintaining structural rigor.

</details>


### [36] [Low Rank Comes with Low Security: Gradient Assembly Poisoning Attacks against Distributed LoRA-based LLM Systems](https://arxiv.org/abs/2601.00566)
*Yueyan Dong,Minghui Xu,Qin Hu,Yinhao Xiao,Qi Luo,Yechao Zhang,Yue Zhang,Xiuzhen Cheng*

Main category: cs.CR

TL;DR: 论文提出了一种名为GAP的新型攻击方法，利用LoRA在联邦学习中的漏洞，通过分别提交看似正常的A和B矩阵，但它们的乘积却产生恶意更新，从而破坏模型性能而不被检测。


<details>
  <summary>Details</summary>
Motivation: LoRA在联邦学习中虽然降低了更新成本，但由于客户端分别提交A和B矩阵，而只有它们的乘积AB决定模型更新，这个复合结果从未被直接验证，形成了一个安全盲点，可能被恶意利用。

Method: 提出Gradient Assembly Poisoning (GAP)攻击方法，通过精心设计看似良性的A和B矩阵，使它们的乘积产生恶意更新。该方法无需访问训练数据或客户端间协调，并能逃避标准异常检测。

Result: 在LLaMA、ChatGLM和GPT-2上验证了GAP攻击的有效性：BLEU分数下降高达14.5%，事实和语法错误增加超过800%，同时保持92.6%的长文本响应长度，表面流畅性得以保留。

Conclusion: GAP攻击揭示了LoRA联邦学习系统中存在的新型隐蔽持久威胁，暴露了四个系统性漏洞，需要新的防御机制来保护分布式LoRA微调的安全性。

Abstract: Low-Rank Adaptation (LoRA) has become a popular solution for fine-tuning large language models (LLMs) in federated settings, dramatically reducing update costs by introducing trainable low-rank matrices. However, when integrated with frameworks like FedIT, LoRA introduces a critical vulnerability: clients submit $A$ and $B$ matrices separately, while only their product $AB$ determines the model update, yet this composite is never directly verified. We propose Gradient Assembly Poisoning (GAP), a novel attack that exploits this blind spot by crafting individually benign $A$ and $B$ matrices whose product yields malicious updates. GAP operates without access to training data or inter-client coordination and remains undetected by standard anomaly detectors. We identify four systemic vulnerabilities in LoRA-based federated systems and validate GAP across LLaMA, ChatGLM, and GPT-2. GAP consistently induces degraded or biased outputs while preserving surface fluency, reducing BLEU by up to 14.5\%, increasing factual and grammatical errors by over 800\%, and maintaining 92.6\% long-form response length. These results reveal a new class of stealthy, persistent threats in distributed LoRA fine-tuning.

</details>
