<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 8]
- [cs.CR](#cs.CR) [Total: 10]
- [cs.AI](#cs.AI) [Total: 18]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [Comparative Evaluation of Large Language Models for Test-Skeleton Generation](https://arxiv.org/abs/2509.04644)
*Subhang Boorlagadda,Nitya Naga Sai Atluri,Muhammet Mustafa Olmez,Edward F. Gehringer*

Main category: cs.SE

TL;DR: 使用LLM自动生成测试架构，评估四款模型在Ruby RSpec测试生成中的性能差异


<details>
  <summary>Details</summary>
Motivation: 手动创建测试架构耗时易错，特别在教育和大规模开发环境中，需要自动化解决方案提高效率

Method: 选取GPT-4、DeepSeek-Chat、Llama4-Maverick和Gemma2-9B四款LLM，通过静态分析和盲测专家评审评估生成的RSpec测试架构的结构正确性、清晰度、可维护性和最佳实践遵循程度

Result: DeepSeek生成了最可维护和结构良好的测试架构，GPT-4产出更完整但缦绕不一致的输出，提示设计和上下文输入是关键质量因素

Conclusion: LLM可以有效自动生成测试架构，但不同模型在代码结构理解和测试约定方面存在显著差异，需要精心设计提示词和上下文信息

Abstract: This paper explores the use of Large Language Models (LLMs) to automate the
generation of test skeletons -- structural templates that outline unit test
coverage without implementing full test logic. Test skeletons are especially
important in test-driven development (TDD), where they provide an early
framework for systematic verification. Traditionally authored manually, their
creation can be time-consuming and error-prone, particularly in educational or
large-scale development settings. We evaluate four LLMs -- GPT-4,
DeepSeek-Chat, Llama4-Maverick, and Gemma2-9B -- on their ability to generate
RSpec skeletons for a real-world Ruby class developed in a university software
engineering course. Each model's output is assessed using static analysis and a
blind expert review to measure structural correctness, clarity,
maintainability, and conformance to testing best practices. The study reveals
key differences in how models interpret code structure and testing conventions,
offering insights into the practical challenges of using LLMs for automated
test scaffolding. Our results show that DeepSeek generated the most
maintainable and well-structured skeletons, while GPT-4 produced more complete
but conventionally inconsistent output. The study reveals prompt design and
contextual input as key quality factors.

</details>


### [2] [Real-Time Performance Benchmarking of TinyML Models in Embedded Systems (PICO: Performance of Inference, CPU, and Operations)](https://arxiv.org/abs/2509.04721)
*Abhishek Dey,Saurabh Srivastava,Gaurav Singh,Robert G. Pettit*

Main category: cs.SE

TL;DR: PICO-TINYML-BENCHMARK是一个模块化、平台无关的框架，用于在资源受限的嵌入式系统上基准测试TinyML模型的实时性能，评估推理延迟、CPU利用率、内存效率和预测稳定性等关键指标。


<details>
  <summary>Details</summary>
Motivation: 为了解决TinyML模型在嵌入式系统实际部署中的性能评估问题，提供计算权衡和平台特定优化的见解，弥合理论进展与实际应用之间的差距。

Method: 开发模块化平台无关框架，在BeagleBone AI64和Raspberry Pi 4平台上对三种代表性TinyML模型（手势分类、关键词识别、MobileNet V2）进行基准测试，使用真实数据集评估关键性能指标。

Result: BeagleBone AI64在AI特定任务上表现出一致的推理延迟，而Raspberry Pi 4在资源效率和成本效益方面更优，揭示了关键的计算权衡。

Conclusion: 该框架为优化TinyML部署提供了可行的指导，有助于在实际嵌入式系统中更好地应用TinyML技术。

Abstract: This paper presents PICO-TINYML-BENCHMARK, a modular and platform-agnostic
framework for benchmarking the real-time performance of TinyML models on
resource-constrained embedded systems. Evaluating key metrics such as inference
latency, CPU utilization, memory efficiency, and prediction stability, the
framework provides insights into computational trade-offs and platform-specific
optimizations. We benchmark three representative TinyML models -- Gesture
Classification, Keyword Spotting, and MobileNet V2 -- on two widely adopted
platforms, BeagleBone AI64 and Raspberry Pi 4, using real-world datasets.
Results reveal critical trade-offs: the BeagleBone AI64 demonstrates consistent
inference latency for AI-specific tasks, while the Raspberry Pi 4 excels in
resource efficiency and cost-effectiveness. These findings offer actionable
guidance for optimizing TinyML deployments, bridging the gap between
theoretical advancements and practical applications in embedded systems.

</details>


### [3] [NovaQ: Improving Quantum Program Testing through Diversity-Guided Test Case Generation](https://arxiv.org/abs/2509.04763)
*Tiancheng Jin,Shangzhou Xia,Jianjun Zhao*

Main category: cs.SE

TL;DR: NovaQ是一个多样性引导的量子程序测试框架，通过分布式测试用例生成和新颖性驱动评估模块，有效提升量子程序测试的多样性和错误检测能力。


<details>
  <summary>Details</summary>
Motivation: 随着量子计算的发展，确保量子程序的可靠性变得越来越重要。量子程序利用量子电路解决经典计算机难以处理的问题，需要有效的测试方法来发现潜在错误。

Method: NovaQ结合了基于分布的测试用例生成器和新颖性驱动的评估模块。生成器通过变异电路参数产生多样化的量子态输入，评估器基于内部电路状态度量（包括幅度、相位和纠缠）量化行为新颖性，选择映射到度量空间中较少覆盖区域的输入。

Result: 在不同规模和复杂度的量子程序上进行的实验表明，NovaQ始终比现有基线方法获得更高的测试输入多样性，并检测到更多的错误。

Conclusion: NovaQ框架通过多样性引导的测试策略，能够有效探索量子程序中未充分测试的行为，提高量子程序测试的效果和可靠性。

Abstract: Quantum programs are designed to run on quantum computers, leveraging quantum
circuits to solve problems that are intractable for classical machines. As
quantum computing advances, ensuring the reliability of quantum programs has
become increasingly important. This paper introduces NovaQ, a diversity-guided
testing framework for quantum programs. NovaQ combines a distribution-based
test case generator with a novelty-driven evaluation module. The generator
produces diverse quantum state inputs by mutating circuit parameters, while the
evaluator quantifies behavioral novelty based on internal circuit state
metrics, including magnitude, phase, and entanglement. By selecting inputs that
map to infrequently covered regions in the metric space, NovaQ effectively
explores under-tested program behaviors. We evaluate NovaQ on quantum programs
of varying sizes and complexities. Experimental results show that NovaQ
consistently achieves higher test input diversity and detects more bugs than
existing baseline approaches.

</details>


### [4] [Code Review Without Borders: Evaluating Synthetic vs. Real Data for Review Recommendation](https://arxiv.org/abs/2509.04810)
*Yogev Cohen,Dudi Ohayon,Romy Somkin,Yehudit Aperstein,Alexander Apartsin*

Main category: cs.SE

TL;DR: 利用LLM将代码更改从资源丰富语言翻译到新兴语言，生成合成训练数据来解决代码审查分类中标签数据短缺问题


<details>
  <summary>Details</summary>
Motivation: 新编程语言和框架的出现造成了标签数据不足的瓶颈，而大量未标注代码却容易获得，需要找到一种可扩展的方法来建立自动化代码审查系统

Method: 使用大语言模型(LLM)将代码更改从资源丰富语言翻译成缺乏标签数据的新兴语言版本，生成合成训练数据，然后训练监督分类器

Result: 在多个GitHub仓库和语言对中进行实验，证明LLM生成的合成数据可以有效地启动审查推荐系统，在低资源环境中缩小了与真实标签数据训练模型的性能差距

Conclusion: 该方法为扩展自动代码审查功能到快速发展的技术栈提供了可扩展的途径，甚至在缺乏注释数据的情况下也能实现

Abstract: Automating the decision of whether a code change requires manual review is
vital for maintaining software quality in modern development workflows.
However, the emergence of new programming languages and frameworks creates a
critical bottleneck: while large volumes of unlabelled code are readily
available, there is an insufficient amount of labelled data to train supervised
models for review classification. We address this challenge by leveraging Large
Language Models (LLMs) to translate code changes from well-resourced languages
into equivalent changes in underrepresented or emerging languages, generating
synthetic training data where labelled examples are scarce. We assume that
although LLMs have learned the syntax and semantics of new languages from
available unlabelled code, they have yet to fully grasp which code changes are
considered significant or review-worthy within these emerging ecosystems. To
overcome this, we use LLMs to generate synthetic change examples and train
supervised classifiers on them. We systematically compare the performance of
these classifiers against models trained on real labelled data. Our experiments
across multiple GitHub repositories and language pairs demonstrate that
LLM-generated synthetic data can effectively bootstrap review recommendation
systems, narrowing the performance gap even in low-resource settings. This
approach provides a scalable pathway to extend automated code review
capabilities to rapidly evolving technology stacks, even in the absence of
annotated data.

</details>


### [5] [Integrating Large Language Models in Software Engineering Education: A Pilot Study through GitHub Repositories Mining](https://arxiv.org/abs/2509.04877)
*Maryam Khan,Muhammad Azeem Akbar,Jussi Kasurinen*

Main category: cs.SE

TL;DR: 这是一项关于大语言模型在软件工程教育中集成的博士研究，通过GitHub项目分析验证了推动因素和阻碍因素的分类系统。


<details>
  <summary>Details</summary>
Motivation: 随着ChatGPT等大语言模型在软件工程教育中的日益普及，需要系统性研究以确保负责任集成到课程中。

Method: 进行了一个项目库挖掘的实验研究，分析了400个GitHub项目的README文件和问题讨论，识别推动因素和阻碍因素的存在情况。

Result: 发现推动因素如参与动机（227次）、软件工程过程理解（133次）和编程辅助（97次）表现突出；阻碍因素如副荐和知识产权问题（385次）、安全隐私问题（87次）、对AI的过度依赖（39次）也很显著。

Conclusion: 该研究为推动因素/阻碍因素分类系统提供了早期实证验证，强调了研究与实践的差距，为建立负责任集成LLM的综合框架奠定了基础。

Abstract: Context: Large Language Models (LLMs) such as ChatGPT are increasingly
adopted in software engineering (SE) education, offering both opportunities and
challenges. Their adoption requires systematic investigation to ensure
responsible integration into curricula. Objective: This doctoral research aims
to develop a validated framework for integrating LLMs into SE education through
a multi-phase process, including taxonomies development, empirical
investigation, and case studies. This paper presents the first empirical step.
Method: We conducted a pilot repository mining study of 400 GitHub projects,
analyzing README files and issues discussions to identify the presence of
motivator and demotivator previously synthesized in our literature review [ 8]
study. Results: Motivators such as engagement and motivation (227 hits),
software engineering process understanding (133 hits), and programming
assistance and debugging support (97 hits) were strongly represented.
Demotivators, including plagiarism and IP concerns (385 hits), security,
privacy and data integrity (87 hits), and over-reliance on AI in learning (39
hits), also appeared prominently. In contrast, demotivators such as challenges
in evaluating learning outcomes and difficulty in curriculum redesign recorded
no hits across the repositories. Conclusion: The study provides early empirical
validation of motivators/demotivators taxonomies with respect to their themes,
highlights research practice gaps, and lays the foundation for developing a
comprehensive framework to guide the responsible adoption of LLMs in SE
education.

</details>


### [6] [FuzzRDUCC: Fuzzing with Reconstructed Def-Use Chain Coverage](https://arxiv.org/abs/2509.04967)
*Kai Feng,Jeremy Singer,Angelos K Marnerides*

Main category: cs.SE

TL;DR: FuzzRDUCC是一个基于数据流分析的二进制模糊测试框架，通过符号执行重建def-use链来发现传统控制流分析可能遗漏的漏洞


<details>
  <summary>Details</summary>
Motivation: 传统灰盒模糊测试主要依赖控制流边覆盖来指导测试用例生成，但这种方法可能忽略仅通过控制流分析难以暴露的漏洞。二进制模糊测试由于对程序内部数据流洞察有限，往往难以实现彻底的代码覆盖和发现隐藏漏洞

Method: FuzzRDUCC采用符号执行技术直接从二进制可执行文件中重建定义-使用(def-use)链，使用新颖的启发式算法选择相关def-use链，在不影响模糊测试彻底性的前提下避免过高计算开销

Result: 在binutils基准测试中评估表明，FuzzRDUCC能够发现最先进模糊测试工具未能发现的独特崩溃

Conclusion: FuzzRDUCC通过整合数据流分析到模糊测试过程中，为下一代漏洞检测和发现机制提供了可行的解决方案

Abstract: Binary-only fuzzing often struggles with achieving thorough code coverage and
uncovering hidden vulnerabilities due to limited insight into a program's
internal dataflows. Traditional grey-box fuzzers guide test case generation
primarily using control flow edge coverage, which can overlook bugs not easily
exposed through control flow analysis alone. We argue that integrating dataflow
analysis into the fuzzing process can enhance its effectiveness by revealing
how data propagates through the program, thereby enabling the exploration of
execution paths that control flow-based methods might miss. In this context, we
introduce FuzzRDUCC, a novel fuzzing framework that employs symbolic execution
to reconstruct definition-use (def-use) chains directly from binary
executables. FuzzRDUCC identifies crucial dataflow paths and exposes security
vulnerabilities without incurring excessive computational overhead, due to a
novel heuristic algorithm that selects relevant def-use chains without
affecting the thoroughness of the fuzzing process. We evaluate FuzzRDUCC using
the binutils benchmark and demonstrate that it can identify unique crashes not
found by state-of-the-art fuzzers. Hence, establishing FuzzRDUCC as a feasible
solution for next generation vulnerability detection and discovery mechanisms.

</details>


### [7] [GenAI-based test case generation and execution in SDV platform](https://arxiv.org/abs/2509.05112)
*Denesa Zyberaj,Lukasz Mazur,Nenad Petrovic,Pankhuri Verma,Pascal Hirmer,Dirk Slama,Xiangwei Cheng,Alois Knoll*

Main category: cs.SE

TL;DR: 使用GenAI和VLM技术将自然语言需求和系统图自动转换为Gherkin测试用例，通过VSS标准化车辆信号定义，在digital.auto平台上验证，显著减少人工测试工作量但仍需人工干预


<details>
  <summary>Details</summary>
Motivation: 解决汽车软件测试中人工编写测试用例效率低、兼容性差的问题，利用AI技术自动化测试生成过程，提高软件定义车辆功能的验证效率

Method: 结合大型语言模型和视觉语言模型，将自然语言需求和系统图转换为结构化Gherkin测试用例，采用车辆信号规范(VSS)建模标准化信号定义，在digital.auto开放平台上执行验证

Result: 以儿童存在检测系统为例，证明该方法能大幅减少人工测试规范工作量，实现快速测试执行，展示了自动化测试生成的可行性

Conclusion: GenAI驱动的测试用例生成方法在汽车软件测试中具有显著优势，但当前由于GenAI管道限制和平台约束，测试用例和脚本生成仍需人工干预，未来需要进一步优化自动化流程

Abstract: This paper introduces a GenAI-driven approach for automated test case
generation, leveraging Large Language Models and Vision-Language Models to
translate natural language requirements and system diagrams into structured
Gherkin test cases. The methodology integrates Vehicle Signal Specification
modeling to standardize vehicle signal definitions, improve compatibility
across automotive subsystems, and streamline integration with third-party
testing tools. Generated test cases are executed within the digital.auto
playground, an open and vendor-neutral environment designed to facilitate rapid
validation of software-defined vehicle functionalities. We evaluate our
approach using the Child Presence Detection System use case, demonstrating
substantial reductions in manual test specification effort and rapid execution
of generated tests. Despite significant automation, the generation of test
cases and test scripts still requires manual intervention due to current
limitations in the GenAI pipeline and constraints of the digital.auto platform.

</details>


### [8] [AI Agents for Web Testing: A Case Study in the Wild](https://arxiv.org/abs/2509.05197)
*Naimeng Ye,Xiao Yu,Ruize Xu,Tianyi Peng,Zhou Yu*

Main category: cs.SE

TL;DR: WebProber是一个基于AI代理的网页测试框架，能够自主探索网站、模拟真实用户交互、识别bug和可用性问题，并生成可读报告。


<details>
  <summary>Details</summary>
Motivation: 传统网页测试方法主要关注代码覆盖和负载测试，但往往无法捕捉复杂的用户行为，导致许多可用性问题未被发现。LLM和AI代理的出现为网页测试提供了新可能。

Method: 开发了WebProber原型框架，给定URL后能自主探索网站，模拟真实用户交互，识别bug和可用性问题，并生成人类可读报告。

Result: 在120个学术个人网站的案例研究中，发现了29个可用性问题，其中许多是传统工具遗漏的。

Conclusion: 基于代理的测试是一个有前景的方向，为开发下一代以用户为中心的测试框架指明了方向。

Abstract: Automated web testing plays a critical role in ensuring high-quality user
experiences and delivering business value. Traditional approaches primarily
focus on code coverage and load testing, but often fall short of capturing
complex user behaviors, leaving many usability issues undetected. The emergence
of large language models (LLM) and AI agents opens new possibilities for web
testing by enabling human-like interaction with websites and a general
awareness of common usability problems. In this work, we present WebProber, a
prototype AI agent-based web testing framework. Given a URL, WebProber
autonomously explores the website, simulating real user interactions,
identifying bugs and usability issues, and producing a human-readable report.
We evaluate WebProber through a case study of 120 academic personal websites,
where it uncovered 29 usability issues--many of which were missed by
traditional tools. Our findings highlight agent-based testing as a promising
direction while outlining directions for developing next-generation,
user-centered testing frameworks.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [9] [Network-Aware Differential Privacy](https://arxiv.org/abs/2509.04710)
*Zhou Li,Yu Zheng,Tianhao Wang,Sang-Woo Jun*

Main category: cs.CR

TL;DR: 本文提出了网络感知差分隐私的新方向，旨在填补网络通信与差分隐私研究之间的空白，重点关注网络安全和拓扑结构对DP部署的影响。


<details>
  <summary>Details</summary>
Motivation: 当前去中心化差分隐私（包括本地DP和混洗DP）严重依赖网络通信进行数据收集，但缺乏对网络环境与DP交叉研究的系统调查，也没有专门为网络环境定制的DP机制。

Method: 通过识别网络研究可以为DP设计和部署做出实质性贡献的两个重点领域：网络安全和网络拓扑结构，提出了网络感知差分隐私的新研究方向。

Result: 提出了网络感知差分隐私的概念框架，明确了网络环境对DP部署的重要影响，为后续研究提供了方向性指导。

Conclusion: 这项工作旨在鼓励更多研究来适应和优化DP在各种网络环境中的部署，推动网络通信与隐私保护技术的深度融合。

Abstract: Differential privacy (DP) is a privacy-enhancement technology (PET) that
receives prominent attention from the academia, industry, and government. One
main development over the past decade has been the decentralization of DP,
including local DP and shuffle DP. Despite that decentralized DP heavily relies
on network communications for data collection,we found that: 1) no systematic
study has surveyed the research opportunities at the intersection of networking
and DP; 2) nor have there been significant efforts to develop DP mechanisms
that are explicitly tailored for network environments. In this paper, we seek
to address this gap by initiating a new direction of network-aware DP. We
identified two focus areas where the network research can offer substantive
contributions to the design and deployment of DP, related to network security
and topology. Through this work, we hope to encourage more research that
adapt/optimize DP's deployment in various network environments.

</details>


### [10] [Cryptographic Application of Elliptic Curve with High Rank](https://arxiv.org/abs/2509.04941)
*Xiaogang Cheng,Ren Guo,Zuxi Chen*

Main category: cs.CR

TL;DR: 基于高阶排的椭圆曲线构建分层撤销的公钥签名方案，展示了高阶排椭圆曲线在加密中的应用价值


<details>
  <summary>Details</summary>
Motivation: 椭圆曲加密在效率和安全性方面都优于传统的RSA和有限域离散对数加密，但高阶排的椭圆曲在加密中尚未得到充分利用

Method: 利用高阶排的椭圆曲构建公钥签名方案，其中排的高度决定了撤销树的高度

Result: 虽然在某些方面效率不是很高，但成功展示了高阶排椭圆曲在加密中的价值和重要性

Conclusion: 高阶排椭圆曲对于加密应用具有重要价值，所提出的技术和假设可以应用于其他加密构造

Abstract: Elliptic curve cryptography is better than traditional cryptography based on
RSA and discrete logarithm of finite field in terms of efficiency and security.
In this paper, we show how to exploit elliptic curve with high rank, which has
not been used in cryptography before, to construct cryptographic schemes.
Concretely we demonstrate how to construct public key signature scheme with
hierarchy revocation based on elliptic curve with high rank, where the rank
determines the height of the revocation tree. Although our construction is not
very efficient in some sense, our construction shows elliptic curve with high
rank is valuable and important for cryptographic usage. The technique and
assumption presented can surely be used for other cryptographic constructions.

</details>


### [11] [Adversarial Augmentation and Active Sampling for Robust Cyber Anomaly Detection](https://arxiv.org/abs/2509.04999)
*Sidahmed Benabderrahmane,Talal Rahwan*

Main category: cs.CR

TL;DR: 结合自编码器异常检测与主动学习的新方法，用于APT检测，在数据极度不平衡（APT攻击仅占0.004%）的真实场景中显著提升检测率


<details>
  <summary>Details</summary>
Motivation: APT攻击具有隐蔽性和长期性，传统监督学习方法需要大量标注数据，而现实场景中标注数据稀缺且成本高昂

Method: 提出注意力对抗双自编码器异常检测框架，结合主动学习循环，通过选择性查询不确定样本的标签来减少标注成本

Result: 在DARPA透明计算项目的真实溯源数据上测试，涵盖Android、Linux、BSD和Windows多个系统，在两种攻击场景下检测率显著提升，优于现有方法

Conclusion: 该方法能够以最小数据有效学习，减少对大量人工标注的依赖，在APT检测领域具有重要应用价值

Abstract: Advanced Persistent Threats (APTs) present a considerable challenge to
cybersecurity due to their stealthy, long-duration nature. Traditional
supervised learning methods typically require large amounts of labeled data,
which is often scarce in real-world scenarios. This paper introduces a novel
approach that combines AutoEncoders for anomaly detection with active learning
to iteratively enhance APT detection. By selectively querying an oracle for
labels on uncertain or ambiguous samples, our method reduces labeling costs
while improving detection accuracy, enabling the model to effectively learn
with minimal data and reduce reliance on extensive manual labeling. We present
a comprehensive formulation of the Attention Adversarial Dual AutoEncoder-based
anomaly detection framework and demonstrate how the active learning loop
progressively enhances the model's performance. The framework is evaluated on
real-world, imbalanced provenance trace data from the DARPA Transparent
Computing program, where APT-like attacks account for just 0.004\% of the data.
The datasets, which cover multiple operating systems including Android, Linux,
BSD, and Windows, are tested in two attack scenarios. The results show
substantial improvements in detection rates during active learning,
outperforming existing methods.

</details>


### [12] [From Protest to Power Plant: Interpreting the Role of Escalatory Hacktivism in Cyber Conflict](https://arxiv.org/abs/2509.05104)
*Richard Derbyshire,Diana Selck-Paulsson,Charl van der Walt,Joe Burton*

Main category: cs.CR

TL;DR: 本文分析了2022年以来黑客活动分子战术的升级，从DDoS攻击扩展到针对关键国家基础设施的OT系统，探讨了其战略动机、国家角色及政策应对策略。


<details>
  <summary>Details</summary>
Motivation: 研究黑客活动分子战术升级背后的战略动机，分析国家如何容忍、鼓励或利用这些组织作为代理人，以及这种现象对国际安全的威胁。

Method: 提出了一种基于行动影响、与国家意识形态一致性以及东道国参与程度的新方法来解释黑客活动分子现象。

Result: 揭示了黑客活动分子从抗议活动向类似网络战争的转变，展示了国家利益与非国家网络行为体之间日益交织的关系。

Conclusion: 为网络安全政策、治理以及应对这一不断演变的威胁提供了策略建议，促进了关于非国家网络行为体与国家利益交叉的国际讨论。

Abstract: Since 2022, hacktivist groups have escalated their tactics, expanding from
distributed denial-of-service attacks and document leaks to include targeting
operational technology (OT). By 2024, attacks on the OT of critical national
infrastructure (CNI) had been linked to partisan hacktivist efforts in ongoing
geopolitical conflicts, demonstrating a shift from protest to something more
resembling cyber warfare. This escalation raises critical questions about the
classification of these groups and the appropriate state response to their
growing role in destabilizing international security.
  This paper examines the strategic motivations behind escalatory hacktivism,
highlighting how states may tolerate, encourage, or leverage hacktivist groups
as proxies in conflicts that blur the lines between activism, cybercrime, and
state-sponsored operations. We introduce a novel method for interpreting
hacktivists based on the impact of their actions, alignment to state ideology,
and host state involvement, offering a structured approach to understanding the
phenomenon. Finally, we assess policy and security implications, particularly
for host and victim states, and propose strategies to address this evolving
threat. By doing so, this paper contributes to international discussions on
cyber security policy, governance, and the increasing intersection between
non-state cyber actors and state interests.

</details>


### [13] [Odoo-based Subcontract Inter-site Access Control Mechanism for Construction Projects](https://arxiv.org/abs/2509.05149)
*Huy Hung Ho,Nhan Le Thanh*

Main category: cs.CR

TL;DR: 开发了一个集成到Odoo ERP框架中的劳动力管理子系统，通过模块化架构和混合访问控制，解决建筑4.0时代弹性劳动力管理中的层次集成和跨站点协作挑战。


<details>
  <summary>Details</summary>
Motivation: 建筑4.0时代采用弹性劳动力模式，依赖专业分包商按需动态扩展劳动力，但面临层次集成管理和跨站点协作协调的挑战。

Method: 采用三管齐下的方法：混合访问控制、第三方跨域通信集成、基于角色的跨站点映射算法；使用树状索引结构和拉格朗日插值法提升角色映射效率；通过统一属性映射中心支持定制化。

Result: 系统在高用户量和离线条件下表现出鲁棒性；实验结果显示数据库性能和工作流适应性得到改善，支持可扩展的企业级解决方案。

Conclusion: 该系统能够满足智能建筑管理不断发展的需求，为弹性劳动力管理提供了有效的技术解决方案。

Abstract: In the era of Construction 4.0, the industry is embracing a new paradigm of
labor elasticity, driven by smart and flexible outsourcing and subcontracting
strategies. The increased reliance on specialized subcontractors enables
companies to scale labor dynamically based on project demands. This adaptable
workforce model presents challenges in managing hierarchical integration and
coordinating inter-site collaboration. Our design introduces a subsystem
integrated into the Odoo ERP framework, employing a modular architecture to
streamline labor management, task tracking, and approval workflows. The system
adopts a three-pronged approach to ensure synchronized data exchange between
general contractors and subcontractors, while maintaining both security and
operational independence. The system features hybrid access control,
third-party integration for cross-domain communication, and role-based mapping
algorithm across sites. The system supports varying degrees of customization
through a unified and consolidated attribute mapping center. This center
leverages a tree-like index structure and Lagrange interpolation method to
enhance the efficiency of role mapping. Demonstrations highlight practical
application in outsourcing, integration, and scalability scenarios, confirming
the system's robustness under high user volumes and in offline conditions.
Experimental results further show improvements in database performance and
workflow adaptability to support a scalable, enterprise-level solution that
aligns with the evolving demands of smart construction management.

</details>


### [14] [Reinforcing Secure Live Migration through Verifiable State Management](https://arxiv.org/abs/2509.05150)
*Stefanos Vasileaidis,Thanassis Giannetsos,Matthias Schunter,Bruno Crispo*

Main category: cs.CR

TL;DR: TALOS是一个轻量级框架，用于可信应用程序在TEE环境中的可验证状态管理和可信迁移，解决了安全状态保存、完整性验证等挑战。


<details>
  <summary>Details</summary>
Motivation: 现代分布式系统中需要可信应用程序的实时迁移能力，但在TEE环境中迁移可信应用程序面临安全状态保存、完整性验证、防重放和防回滚、防止未授权克隆等独特挑战。

Method: TALOS框架基于最小化信任假设原则，将可信应用程序视为不可信直到明确验证，迁移过程不依赖可信第三方。集成内存自省和控制流图提取技术，确保迁移应用的完整性和安全启动。

Result: 框架在Intel SGX和RISC-V Keystone上实现并验证，证明其跨不同TEE架构的可移植性，在保持效率的同时提供强大的安全保证，适用于去中心化环境。

Conclusion: TALOS为可信应用程序迁移提供了轻量级、可验证的解决方案，能够满足现代网络服务网格中跨域边界协同计算的需求，具有实际部署价值。

Abstract: Live migration of applications is a fundamental capability for enabling
resilient computing in modern distributed systems. However, extending this
functionality to trusted applications (TA) -- executing within Trusted
Execution Environments (TEEs) -- introduces unique challenges such as secure
state preservation, integrity verification, replay and rollback prevention, and
mitigation of unauthorized cloning of TAs. We present TALOS, a lightweight
framework for verifiable state management and trustworthy application
migration. While our implementation is prototyped and evaluated using Intel SGX
with the Gramine LibOS and RISC-V Keystone (evidencing the framework's
portability across diverse TEEs), its design is agnostic to the underlying TEE
architecture. Such agility is a necessity in today's network service mesh
(collaborative computing across the continuum) where application workloads must
be managed across domain boundaries in a harmonized fashion. TALOS is built
around the principle of minimizing trust assumptions: TAs are treated as
untrusted until explicitly verified, and the migration process does not rely on
a trusted third party. To ensure both the integrity and secure launch of the
migrated application, TALOS integrates memory introspection and control-flow
graph extraction, enabling robust verification of state continuity and
execution flow. Thereby achieving strong security guarantees while maintaining
efficiency, making it suitable for decentralized settings.

</details>


### [15] [Jamming Smarter, Not Harder: Exploiting O-RAN Y1 RAN Analytics for Efficient Interference](https://arxiv.org/abs/2509.05161)
*Abiodun Ganiyu,Dara Ron,Syed Rafiul Hussain,Vijay K Shah*

Main category: cs.CR

TL;DR: 本文展示了O-RAN中Y1接口的安全漏洞，攻击者可通过被动监控下行链路指标发起选择性干扰攻击。提出的基于阈值和聚类的干扰策略在实验测试中显示出高效性，揭示了RAN分析信息暴露带来的安全风险。


<details>
  <summary>Details</summary>
Motivation: O-RAN的Y1接口虽然增强了网络优化能力，但恶意消费者可能滥用RAN分析信息(RAI)发起针对性干扰攻击，需要研究这种安全威胁的实际影响。

Method: 提出并评估了两种Y1辅助干扰策略：基于DBSCAN聚类的干扰器和基于阈值的干扰器，在真实的LTE/5G O-RAN测试床上与始终开启和随机干扰两种基线策略进行比较。

Result: 在无约束干扰预算下，基于阈值的干扰器可接近始终开启干扰的效果，同时减少27%的传输时间；在约束预算下，基于聚类的干扰器最有效，造成18.1%的比特率下降，仅需25%的活动时间。

Conclusion: 研究揭示了干扰隐蔽性与效率之间的关键权衡，表明Y1接口暴露RAN分析信息可能实现高度针对性、低开销的攻击，对民用和关键任务O-RAN部署提出了重要安全考虑。

Abstract: The Y1 interface in O-RAN enables the sharing of RAN Analytics Information
(RAI) between the near-RT RIC and authorized Y1 consumers, which may be
internal applications within the operator's trusted domain or external systems
accessing data through a secure exposure function. While this visibility
enhances network optimization and enables advanced services, it also introduces
a potential security risk -- a malicious or compromised Y1 consumer could
misuse analytics to facilitate targeted interference. In this work, we
demonstrate how an adversary can exploit the Y1 interface to launch selective
jamming attacks by passively monitoring downlink metrics. We propose and
evaluate two Y1-aided jamming strategies: a clustering-based jammer leveraging
DBSCAN for traffic profiling and a threshold-based jammer. These are compared
against two baselines strategies -- always-on jammer and random jammer -- on an
over-the-air LTE/5G O-RAN testbed. Experimental results show that in
unconstrained jamming budget scenarios, the threshold-based jammer can closely
replicate the disruption caused by always-on jamming while reducing
transmission time by 27\%. Under constrained jamming budgets, the
clustering-based jammer proves most effective, causing up to an 18.1\% bitrate
drop while remaining active only 25\% of the time. These findings reveal a
critical trade-off between jamming stealthiness and efficiency, and illustrate
how exposure of RAN analytics via the Y1 interface can enable highly targeted,
low-overhead attacks, raising important security considerations for both
civilian and mission-critical O-RAN deployments.

</details>


### [16] [Verifiability and Privacy in Federated Learning through Context-Hiding Multi-Key Homomorphic Authenticators](https://arxiv.org/abs/2509.05162)
*Simone Bottoni,Giulio Zizzo,Stefano Braghin,Alberto Trombetta*

Main category: cs.CR

TL;DR: 这篇论文提出了一种可验证的联邦学习协议，允许客户端验证聚合器计算的正确性，同时保护模型更新的保密性。


<details>
  <summary>Details</summary>
Motivation: 联邦学习的安全性和稳健性至关重要。虽然有许多算法能够应对恶意客户端，但聚合器本身也可能恶意行为，如偏某模型或篡改权重以削弱模型隐私性。

Method: 使用标准的安全聚合技术来保护个人模型更新，结合线性同态认证器方案，实现高效、隐私保护的聚合结果验证。

Result: 构造确保客户端能够检测聚合器的操纵行为，同时保持低计算开销。方法能够扩展到大型模型，支持含有数百万参数的大型神经网络的验证。

Conclusion: 该协议有效解决了联邦学习中聚合器可能恶意行为的安全问题，在保护客户端隐私的同时提供了可靠的验证机制，具有良好的扩展性。

Abstract: Federated Learning has rapidly expanded from its original inception to now
have a large body of research, several frameworks, and sold in a variety of
commercial offerings. Thus, its security and robustness is of significant
importance. There are many algorithms that provide robustness in the case of
malicious clients. However, the aggregator itself may behave maliciously, for
example, by biasing the model or tampering with the weights to weaken the
models privacy. In this work, we introduce a verifiable federated learning
protocol that enables clients to verify the correctness of the aggregators
computation without compromising the confidentiality of their updates. Our
protocol uses a standard secure aggregation technique to protect individual
model updates with a linearly homomorphic authenticator scheme that enables
efficient, privacy-preserving verification of the aggregated result. Our
construction ensures that clients can detect manipulation by the aggregator
while maintaining low computational overhead. We demonstrate that our approach
scales to large models, enabling verification over large neural networks with
millions of parameters.

</details>


### [17] [On Hyperparameters and Backdoor-Resistance in Horizontal Federated Learning](https://arxiv.org/abs/2509.05192)
*Simon Lachnit,Ghassan Karame*

Main category: cs.CR

TL;DR: 水平联邦学习中良性客户端超参数设置对后门攻击敏感性有重大影响，适当调整可显著提升模型防御能力而不严重影响主任务准确性


<details>
  <summary>Details</summary>
Motivation: 现有HFL安全研究常忽视良性客户端超参数选择对后门攻击敌强度的影响，导致攻击效果被高估和防御效果被低估

Method: 通过理论分析和实验测量，系统研究学习率、批处理大小、本地迭代次数等超参数对后门攻击效果的影响

Result: 适当调整良性客户端超参数可显著提升模型防御能力，A3FL攻击声明期降低98.6%，主任务准确性仅下降2.9%百分点

Conclusion: 在HFL安全研究中应重视良性客户端超参数选择，适当设置可在不使用任何防御措施的情况下有效抑制后门攻击

Abstract: Horizontal Federated Learning (HFL) is particularly vulnerable to backdoor
attacks as adversaries can easily manipulate both the training data and
processes to execute sophisticated attacks. In this work, we study the impact
of training hyperparameters on the effectiveness of backdoor attacks and
defenses in HFL. More specifically, we show both analytically and by means of
measurements that the choice of hyperparameters by benign clients does not only
influence model accuracy but also significantly impacts backdoor attack
success. This stands in sharp contrast with the multitude of contributions in
the area of HFL security, which often rely on custom ad-hoc hyperparameter
choices for benign clients$\unicode{x2013}$leading to more pronounced backdoor
attack strength and diminished impact of defenses. Our results indicate that
properly tuning benign clients' hyperparameters$\unicode{x2013}$such as
learning rate, batch size, and number of local epochs$\unicode{x2013}$can
significantly curb the effectiveness of backdoor attacks, regardless of the
malicious clients' settings. We support this claim with an extensive robustness
evaluation of state-of-the-art attack-defense combinations, showing that
carefully chosen hyperparameters yield across-the-board improvements in
robustness without sacrificing main task accuracy. For example, we show that
the 50%-lifespan of the strong A3FL attack can be reduced by 98.6%,
respectively$\unicode{x2013}$all without using any defense and while incurring
only a 2.9 percentage points drop in clean task accuracy.

</details>


### [18] [On Evaluating the Poisoning Robustness of Federated Learning under Local Differential Privacy](https://arxiv.org/abs/2509.05265)
*Zijian Wang,Wei Tong,Tingxuan Han,Haoyu Chen,Tianling Zhang,Yunlong Mao,Sheng Zhong*

Main category: cs.CR

TL;DR: 本文提出了一种适用于局部差分隐私联邦学习的模型毒化攻击框架，能够避免现有防御机制并显著降低全局模型性能


<details>
  <summary>Details</summary>
Motivation: 联邦学习与局部差分隐私结合虽能保护隐私，但容易受到恶意参与者的模型毒化攻击，当前的稳健性研究不足

Method: 提出了一种可扩展的模型毒化攻击框架，通过在遵守局部隐私约束的前提下最大化全局训练损失，并为对抗Multi-Krum和修剪均值等稳健聚合机制设计了适应性攻击

Result: 在3种代表性LDPFL协议、3个标准数据集和2类深度神经网络上评估，实验结果显示适应性攻击能显著降低全局模型性能，揭示了关键漏洞

Conclusion: 该研究揭示了LDPFL协议在模型毒化攻击下的严重漏洞，强调了开发更稳健的防御策略的必要性

Abstract: Federated learning (FL) combined with local differential privacy (LDP)
enables privacy-preserving model training across decentralized data sources.
However, the decentralized data-management paradigm leaves LDPFL vulnerable to
participants with malicious intent. The robustness of LDPFL protocols,
particularly against model poisoning attacks (MPA), where adversaries inject
malicious updates to disrupt global model convergence, remains insufficiently
studied. In this paper, we propose a novel and extensible model poisoning
attack framework tailored for LDPFL settings. Our approach is driven by the
objective of maximizing the global training loss while adhering to local
privacy constraints. To counter robust aggregation mechanisms such as
Multi-Krum and trimmed mean, we develop adaptive attacks that embed carefully
crafted constraints into a reverse training process, enabling evasion of these
defenses. We evaluate our framework across three representative LDPFL
protocols, three benchmark datasets, and two types of deep neural networks.
Additionally, we investigate the influence of data heterogeneity and privacy
budgets on attack effectiveness. Experimental results demonstrate that our
adaptive attacks can significantly degrade the performance of the global model,
revealing critical vulnerabilities and highlighting the need for more robust
LDPFL defense strategies against MPA. Our code is available at
https://github.com/ZiJW/LDPFL-Attack

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [19] [The Ethical Compass of the Machine: Evaluating Large Language Models for Decision Support in Construction Project Management](https://arxiv.org/abs/2509.04505)
*Somtochukwu Azie,Yiping Meng*

Main category: cs.AI

TL;DR: 大语言模型在建筑项目管理中的伦理决策支持能力存在显著缺陷，需要人类监督


<details>
  <summary>Details</summary>
Motivation: 评估LLM在建筑项目管理这个高风险、伦理敏感领域的伦理可靠性和可行性

Method: 混合方法研究设计：量化测试两个领先LLM在12个真实伦理场景中的表现（使用EDSAC检查表），质性分析12位行业专家的半结构化访谈

Result: LLM在法律遵循等结构化领域表现迅速，但在处理上下文细节、确保负责任和提供透明推理方面存在显著缺陷

Conclusion: LLM目前应作为决策支持工具，而非自主伦理决策者，必须强调人类在环中的监督作用

Abstract: The integration of Artificial Intelligence (AI) into construction project
management (CPM) is accelerating, with Large Language Models (LLMs) emerging as
accessible decision-support tools. This study aims to critically evaluate the
ethical viability and reliability of LLMs when applied to the ethically
sensitive, high-risk decision-making contexts inherent in CPM. A mixed-methods
research design was employed, involving the quantitative performance testing of
two leading LLMs against twelve real-world ethical scenarios using a novel
Ethical Decision Support Assessment Checklist (EDSAC), and qualitative analysis
of semi-structured interviews with 12 industry experts to capture professional
perceptions. The findings reveal that while LLMs demonstrate adequate
performance in structured domains such as legal compliance, they exhibit
significant deficiencies in handling contextual nuance, ensuring
accountability, and providing transparent reasoning. Stakeholders expressed
considerable reservations regarding the autonomous use of AI for ethical
judgments, strongly advocating for robust human-in-the-loop oversight. To our
knowledge, this is one of the first studies to empirically test the ethical
reasoning of LLMs within the construction domain. It introduces the EDSAC
framework as a replicable methodology and provides actionable recommendations,
emphasising that LLMs are currently best positioned as decision-support aids
rather than autonomous ethical agents.

</details>


### [20] [Maestro: Joint Graph & Config Optimization for Reliable AI Agents](https://arxiv.org/abs/2509.04642)
*Wenxiao Wang,Priyatham Kattakinda,Soheil Feizi*

Main category: cs.AI

TL;DR: Maestro是一个框架无关的LLM智能体整体优化器，通过联合搜索图结构和节点配置来提升智能体质量，在多个基准测试和应用中显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有优化器主要调整配置而固定图结构，无法解决结构性故障模式，需要一种能够同时优化图结构和节点配置的全面优化方法。

Method: Maestro框架在明确的rollout/token预算约束下，联合搜索智能体的图结构（模块组成和信息流）和节点配置（模型、提示词、工具、控制参数），并利用轨迹中的反思性文本反馈来优先处理编辑操作。

Result: 在IFBench和HotpotQA基准测试中，Maestro平均分别超过MIPROv2、GEPA和GEPA+Merge 12%、4.9%和4.86%；即使在仅优化提示词的情况下，仍领先9.65%、2.37%和2.41%，且使用的rollout次数远少于GEPA。

Conclusion: 联合图结构和配置搜索能够解决仅靠提示词调优无法解决的结构性故障模式，在多个应用中显示出显著优势，证明了整体优化方法的重要性。

Abstract: Building reliable LLM agents requires decisions at two levels: the graph
(which modules exist and how information flows) and the configuration of each
node (models, prompts, tools, control knobs). Most existing optimizers tune
configurations while holding the graph fixed, leaving structural failure modes
unaddressed. We introduce Maestro, a framework-agnostic holistic optimizer for
LLM agents that jointly searches over graphs and configurations to maximize
agent quality, subject to explicit rollout/token budgets. Beyond numeric
metrics, Maestro leverages reflective textual feedback from traces to
prioritize edits, improving sample efficiency and targeting specific failure
modes. On the IFBench and HotpotQA benchmarks, Maestro consistently surpasses
leading prompt optimizers--MIPROv2, GEPA, and GEPA+Merge--by an average of 12%,
4.9%, and 4.86%, respectively; even when restricted to prompt-only
optimization, it still leads by 9.65%, 2.37%, and 2.41%. Maestro achieves these
results with far fewer rollouts than GEPA. We further show large gains on two
applications (interviewer & RAG agents), highlighting that joint graph &
configuration search addresses structural failure modes that prompt tuning
alone cannot fix.

</details>


### [21] [Towards Personalized Explanations for Health Simulations: A Mixed-Methods Framework for Stakeholder-Centric Summarization](https://arxiv.org/abs/2509.04646)
*Philippe J. Giabbanelli,Ameeta Agrawal*

Main category: cs.AI

TL;DR: 提出一个分步框架，通过混合方法识别不同健康领域利益相关者的需求，指导LLM生成定制化的健康模拟解释，解决现有通用摘要无法满足多样化需求的局限性。


<details>
  <summary>Details</summary>
Motivation: 当前基于代理的建模与仿真方法在健康决策支持中潜力巨大，但由于模型复杂性难以被利益相关者理解，且现有LLM生成的通用摘要无法满足不同利益相关者（临床医生、政策制定者、患者等）的多样化信息需求和风格偏好。

Method: 采用混合方法设计：首先通过需求调研获取不同健康利益相关者的解释需求和风格偏好，然后通过可控属性调优等技术优化LLM生成定制化输出的能力，最后通过综合指标评估并进一步改进定制化摘要生成。

Result: 论文提出了一个系统性框架，但具体实验结果未在摘要中详细说明。

Conclusion: 该研究填补了健康模拟解释定制化的系统性理解空白，为LLM生成针对性健康模拟解释提供了可行的分步框架，有望提升健康决策支持工具的可访问性和实用性。

Abstract: Modeling & Simulation (M&S) approaches such as agent-based models hold
significant potential to support decision-making activities in health, with
recent examples including the adoption of vaccines, and a vast literature on
healthy eating behaviors and physical activity behaviors. These models are
potentially usable by different stakeholder groups, as they support
policy-makers to estimate the consequences of potential interventions and they
can guide individuals in making healthy choices in complex environments.
However, this potential may not be fully realized because of the models'
complexity, which makes them inaccessible to the stakeholders who could benefit
the most. While Large Language Models (LLMs) can translate simulation outputs
and the design of models into text, current approaches typically rely on
one-size-fits-all summaries that fail to reflect the varied informational needs
and stylistic preferences of clinicians, policymakers, patients, caregivers,
and health advocates. This limitation stems from a fundamental gap: we lack a
systematic understanding of what these stakeholders need from explanations and
how to tailor them accordingly. To address this gap, we present a step-by-step
framework to identify stakeholder needs and guide LLMs in generating tailored
explanations of health simulations. Our procedure uses a mixed-methods design
by first eliciting the explanation needs and stylistic preferences of diverse
health stakeholders, then optimizing the ability of LLMs to generate tailored
outputs (e.g., via controllable attribute tuning), and then evaluating through
a comprehensive range of metrics to further improve the tailored generation of
summaries.

</details>


### [22] [An Approach to Grounding AI Model Evaluations in Human-derived Criteria](https://arxiv.org/abs/2509.04676)
*Sasha Mitts*

Main category: cs.AI

TL;DR: 提出了一种通过人类评估标准增强AI基准测试的新方法，重点关注物理世界建模，旨在提高模型行为的可解释性和适用性。


<details>
  <summary>Details</summary>
Motivation: 传统AI基准测试难以捕捉模型的细微能力，特别是在物理世界建模方面，需要更符合人类认知的评估标准。

Method: 基于Perception Test和OpenEQA基准，通过深度访谈和大规模调查识别关键认知技能（优先级排序、记忆、辨别、情境化），并将这些发现整合到基准设计中。

Result: 研究发现参与者认为AI缺乏解释性和共情能力，但对AI性能有很高期望。提出了一个开发更符合人类认知的AI评估框架。

Conclusion: 这项工作强调了以用户为中心的AI评估的重要性，为研究人员提供了将AI能力与人类认知过程对齐的实用指南，改进了当前基准测试实践并为未来AI模型评估奠定了基础。

Abstract: In the rapidly evolving field of artificial intelligence (AI), traditional
benchmarks can fall short in attempting to capture the nuanced capabilities of
AI models. We focus on the case of physical world modeling and propose a novel
approach to augment existing benchmarks with human-derived evaluation criteria,
aiming to enhance the interpretability and applicability of model behaviors.
Grounding our study in the Perception Test and OpenEQA benchmarks, we conducted
in-depth interviews and large-scale surveys to identify key cognitive skills,
such as Prioritization, Memorizing, Discerning, and Contextualizing, that are
critical for both AI and human reasoning. Our findings reveal that participants
perceive AI as lacking in interpretive and empathetic skills yet hold high
expectations for AI performance. By integrating insights from our findings into
benchmark design, we offer a framework for developing more human-aligned means
of defining and measuring progress. This work underscores the importance of
user-centered evaluation in AI development, providing actionable guidelines for
researchers and practitioners aiming to align AI capabilities with human
cognitive processes. Our approach both enhances current benchmarking practices
and sets the stage for future advancements in AI model evaluation.

</details>


### [23] [Language-Driven Hierarchical Task Structures as Explicit World Models for Multi-Agent Learning](https://arxiv.org/abs/2509.04731)
*Brennen Hill*

Main category: cs.AI

TL;DR: 这篇论文提出使用大语言模型动态生成层次架构来构建显式世界模型，以解决多自然任务中的探索难题和稀疏奖励问题，提高样本效率和策略性行为学习能力。


<details>
  <summary>Details</summary>
Motivation: 虽然语言模型和自然模型取得了进步，但复杂长期限多自然任务中精细显式世界模型的发展仍是关键瓶颈。标准强化学习在高保真度但结构平坦的模拟器中存在探索空间难以处理和奖励稀疏的问题。

Method: 提出使用大语言模型动态生成层次架构的新范式，通过语言来立即构建世界模型。这种语言驱动的世界模型提供内在课程、浓密有意义的学习信号以及组合学习框架。通过构建具有显式、语言可配置任务层的环境来实现。

Result: 通过系统性审查2024年多自然足球研究，发现了一个明确的趋势：将符号和层次方法与多自然强化学习集成。这些方法显或隐地构建了基于任务的世界模型来指导自然学习。

Conclusion: 通过构建具有显式、语言可配置任务层的环境，可以桥接低级反应式行为和高级战略团队配合之间的差距，创建一个强大且可普遍化的框架来训练下一代智能自然。

Abstract: The convergence of Language models, Agent models, and World models represents
a critical frontier for artificial intelligence. While recent progress has
focused on scaling Language and Agent models, the development of sophisticated,
explicit World Models remains a key bottleneck, particularly for complex,
long-horizon multi-agent tasks. In domains such as robotic soccer, agents
trained via standard reinforcement learning in high-fidelity but
structurally-flat simulators often fail due to intractable exploration spaces
and sparse rewards. This position paper argues that the next frontier in
developing capable agents lies in creating environments that possess an
explicit, hierarchical World Model. We contend that this is best achieved
through hierarchical scaffolding, where complex goals are decomposed into
structured, manageable subgoals. Drawing evidence from a systematic review of
2024 research in multi-agent soccer, we identify a clear and decisive trend
towards integrating symbolic and hierarchical methods with multi-agent
reinforcement learning (MARL). These approaches implicitly or explicitly
construct a task-based world model to guide agent learning. We then propose a
paradigm shift: leveraging Large Language Models to dynamically generate this
hierarchical scaffold, effectively using language to structure the World Model
on the fly. This language-driven world model provides an intrinsic curriculum,
dense and meaningful learning signals, and a framework for compositional
learning, enabling Agent Models to acquire sophisticated, strategic behaviors
with far greater sample efficiency. By building environments with explicit,
language-configurable task layers, we can bridge the gap between low-level
reactive behaviors and high-level strategic team play, creating a powerful and
generalizable framework for training the next generation of intelligent agents.

</details>


### [24] [What-If Analysis of Large Language Models: Explore the Game World Using Proactive Thinking](https://arxiv.org/abs/2509.04791)
*Yuan Sui,Yanming Zhang,Yi Liao,Yu Gu,Guohua Tang,Zhongqian Sun,Wei Yang,Bryan Hooi*

Main category: cs.AI

TL;DR: WiA-LLM是一个新范式，通过整合假设分析(WIA)和强化学习，使大语言模型具备主动思考能力，能够在复杂动态环境中预测行动后果，在王者荣耀游戏中达到74.2%的状态变化预测准确率。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型只能被动处理信息，缺乏系统性探索假设未来的能力，这限制了它们在动态高风险场景（如战略规划、风险评估和实时决策）中的实用性。

Method: 整合假设分析(WIA)方法，通过强化学习利用环境反馈，动态模拟每个潜在行动的结果，使模型能够预测未来状态而不仅仅是响应当前条件。在王者荣耀复杂多玩家游戏环境中进行验证。

Result: WiA-LLM在预测游戏状态变化方面达到74.2%的准确率（比基线提高两倍），在高难度场景中表现尤为突出，准确的前瞻性预测至关重要。

Conclusion: 这是首个正式探索和整合假设分析能力到大语言模型的工作，代表了LLM主动推理的根本性进步，为动态环境中的稳健决策提供了可扩展框架。

Abstract: Large language models (LLMs) excel at processing information reactively but
lack the ability to systemically explore hypothetical futures. They cannot ask,
"what if we take this action? how will it affect the final outcome" and
forecast its potential consequences before acting. This critical gap limits
their utility in dynamic, high-stakes scenarios like strategic planning, risk
assessment, and real-time decision making. To bridge this gap, we propose
WiA-LLM, a new paradigm that equips LLMs with proactive thinking capabilities.
Our approach integrates What-If Analysis (WIA), a systematic approach for
evaluating hypothetical scenarios by changing input variables. By leveraging
environmental feedback via reinforcement learning, WiA-LLM moves beyond
reactive thinking. It dynamically simulates the outcomes of each potential
action, enabling the model to anticipate future states rather than merely react
to the present conditions. We validate WiA-LLM in Honor of Kings (HoK), a
complex multiplayer game environment characterized by rapid state changes and
intricate interactions. The game's real-time state changes require precise
multi-step consequence prediction, making it an ideal testbed for our approach.
Experimental results demonstrate WiA-LLM achieves a remarkable 74.2% accuracy
in forecasting game-state changes (up to two times gain over baselines). The
model shows particularly significant gains in high-difficulty scenarios where
accurate foresight is critical. To our knowledge, this is the first work to
formally explore and integrate what-if analysis capabilities within LLMs.
WiA-LLM represents a fundamental advance toward proactive reasoning in LLMs,
providing a scalable framework for robust decision-making in dynamic
environments with broad implications for strategic applications.

</details>


### [25] [TalkToAgent: A Human-centric Explanation of Reinforcement Learning Agents with Large Language Models](https://arxiv.org/abs/2509.04809)
*Haechang Kim,Hao Chen,Can Li,Jong Min Lee*

Main category: cs.AI

TL;DR: TalkToAgent是一个多代理LLM框架，通过五个专业代理提供交互式自然语言解释，弥合复杂RL策略与领域专家之间的理解鸿沟。


<details>
  <summary>Details</summary>
Motivation: 当前XRL方法存在解释结果可理解性有限、工具覆盖孤立的问题，导致用户不确定使用哪种工具，需要更透明、交互式的RL策略解释方法。

Method: 采用五代理LLM架构（协调器、解释器、编码器、评估器、调试器），自动将用户查询映射到相关XRL工具，提供状态变量、预期结果或反事实解释。

Result: 在四重水箱过程控制问题上验证，成功高精度映射用户查询到XRL任务，编码器-调试器交互最小化反事实生成失败，定性评估确认有效解释代理行为。

Conclusion: TalkToAgent框架有效提升了RL策略的可解释性，通过多代理协作提供自然语言解释，解决了XRL工具选择不确定性和解释理解困难的问题。

Abstract: Explainable Reinforcement Learning (XRL) has emerged as a promising approach
in improving the transparency of Reinforcement Learning (RL) agents. However,
there remains a gap between complex RL policies and domain experts, due to the
limited comprehensibility of XRL results and isolated coverage of current XRL
approaches that leave users uncertain about which tools to employ. To address
these challenges, we introduce TalkToAgent, a multi-agent Large Language Models
(LLM) framework that delivers interactive, natural language explanations for RL
policies. The architecture with five specialized LLM agents (Coordinator,
Explainer, Coder, Evaluator, and Debugger) enables TalkToAgent to automatically
map user queries to relevant XRL tools and clarify an agent's actions in terms
of either key state variables, expected outcomes, or counterfactual
explanations. Moreover, our approach extends previous counterfactual
explanations by deriving alternative scenarios from qualitative behavioral
descriptions, or even new rule-based policies. We validated TalkToAgent on
quadruple-tank process control problem, a well-known nonlinear control
benchmark. Results demonstrated that TalkToAgent successfully mapped user
queries into XRL tasks with high accuracy, and coder-debugger interactions
minimized failures in counterfactual generation. Furthermore, qualitative
evaluation confirmed that TalkToAgent effectively interpreted agent's actions
and contextualized their meaning within the problem domain.

</details>


### [26] [Collaboration and Conflict between Humans and Language Models through the Lens of Game Theory](https://arxiv.org/abs/2509.04847)
*Mukul Singh,Arjun Radhakrishna,Sumit Gulwani*

Main category: cs.AI

TL;DR: 语言模型在迭代困徒困境IPD中表现出与最佳经典策略相当或更优的合作性能力，具备友善性、诡证性和宽容性特征，能快速适应对手策略变化


<details>
  <summary>Details</summary>
Motivation: 研究语言模型在多方交互环境中的合作与竞争行为，补充现有研究在长期交互、人机协作和行为演化方面的不足

Method: 在迭代困徒困境(IPD)中进行Axelrod风格的比赛，让模型代理与240种经典策略对抗，并设计策略切换实验测试适应性

Result: 语言模型表现与最佳经典策略相当或更优，具备强合作策略的关键特征，能在几轮内检测并应对对手策略变化，适应性可与人类相比或更好

Conclusion: 该研究系统性描述了语言模型在长期合作行为中的表现，为未来更复杂混合人工智能社会环境的研究奠定基础

Abstract: Language models are increasingly deployed in interactive online environments,
from personal chat assistants to domain-specific agents, raising questions
about their cooperative and competitive behavior in multi-party settings. While
prior work has examined language model decision-making in isolated or
short-term game-theoretic contexts, these studies often neglect long-horizon
interactions, human-model collaboration, and the evolution of behavioral
patterns over time. In this paper, we investigate the dynamics of language
model behavior in the iterated prisoner's dilemma (IPD), a classical framework
for studying cooperation and conflict. We pit model-based agents against a
suite of 240 well-established classical strategies in an Axelrod-style
tournament and find that language models achieve performance on par with, and
in some cases exceeding, the best-known classical strategies. Behavioral
analysis reveals that language models exhibit key properties associated with
strong cooperative strategies - niceness, provocability, and generosity while
also demonstrating rapid adaptability to changes in opponent strategy mid-game.
In controlled "strategy switch" experiments, language models detect and respond
to shifts within only a few rounds, rivaling or surpassing human adaptability.
These results provide the first systematic characterization of long-term
cooperative behaviors in language model agents, offering a foundation for
future research into their role in more complex, mixed human-AI social
environments.

</details>


### [27] [Cloning a Conversational Voice AI Agent from Call\,Recording Datasets for Telesales](https://arxiv.org/abs/2509.04871)
*Krittanon Kaewtawee,Wachiravit Modecrua,Krittin Pachtrachai,Touchapon Kraisingkorn*

Main category: cs.AI

TL;DR: 这篇论文提出了一种从电话通话录音中克隆对话式语音AI助手的通用方法。系统通过语音识别、大语言模型对话管理和语音合成技术，学习顶尖人类营销人员的话术策略。评估显示AI在常规通话方面接近人类表现，但在说服和异议处理方面还有提升空间。


<details>
  <summary>Details</summary>
Motivation: 自动化语音助手在客服和医疗领域有广泛应用潜力，可以降低运营成本并提供24小时支持。研究目标是从现有的电话通话录音中克隆出能够理解和生成人类对话的语音AI助手。

Method: 提出一种通用方法论，包括：域选择、知识提取和提示工程等步骤。系统整合了自动语音识别(ASR)、基于大语言模型的对话管理器和文本转语音(TTS)合成技术，构建流式推理流水线。学习顶尖人类营销人员的结构化脚本。

Result: 通过基于22个标准（包括介绍、产品沟通、销售推动、异议处理和成交）的盲测评估显示，AI助手在常规通话方面接近人类表现，但在说服力和异议处理方面表现较差。研究人员分析了这些短板并对提示进行了精炼。

Conclusion: 论文总结了设计经验教训和未来研究方向，包括大规模模拟和自动化评估。虽然在某些方面还有提升空间，但该方法证明了从现有通话数据中克隆对话式语音AI助手的可行性。

Abstract: Recent advances in language and speech modelling have made it possible to
build autonomous voice assistants that understand and generate human dialogue
in real time. These systems are increasingly being deployed in domains such as
customer service and healthcare care, where they can automate repetitive tasks,
reduce operational costs, and provide constant support around the clock. In
this paper, we present a general methodology for cloning a conversational voice
AI agent from a corpus of call recordings. Although the case study described in
this paper uses telesales data to illustrate the approach, the underlying
process generalizes to any domain where call transcripts are available. Our
system listens to customers over the telephone, responds with a synthetic
voice, and follows a structured playbook learned from top performing human
agents. We describe the domain selection, knowledge extraction, and prompt
engineering used to construct the agent, integrating automatic speech
recognition, a large language model based dialogue manager, and text to speech
synthesis into a streaming inference pipeline. The cloned agent is evaluated
against human agents on a rubric of 22 criteria covering introduction, product
communication, sales drive, objection handling, and closing. Blind tests show
that the AI agent approaches human performance in routine aspects of the call
while underperforming in persuasion and objection handling. We analyze these
shortcomings and refine the prompt accordingly. The paper concludes with design
lessons and avenues for future research, including large scale simulation and
automated evaluation.

</details>


### [28] [OSC: Cognitive Orchestration through Dynamic Knowledge Alignment in Multi-Agent LLM Collaboration](https://arxiv.org/abs/2509.04876)
*Jusheng Zhang,Yijia Fan,Kaitong Cai,Xiaofei Sun,Keze Wang*

Main category: cs.AI

TL;DR: OSC是一个知识感知的自适应协作框架，通过认知状态感知和实时认知差距分析来增强多智能体系统中LLM的深度协作能力


<details>
  <summary>Details</summary>
Motivation: 解决多智能体系统中专家智能体之间高效语言交互的瓶颈问题，现有工作在智能体选择和结果聚合方面有进展，但深度协作的交互效率仍是关键挑战

Method: 引入协作知识模型(CKM)使智能体动态感知合作者认知状态，通过实时认知差距分析自适应调整通信行为（内容焦点、细节层次、表达风格）

Result: 在复杂推理和问题解决基准测试中显著提升任务性能和通信效率，将"并行工作的个体"转变为"深度协作的认知团队"

Conclusion: OSC不仅优化了多智能体协作，还为LLM智能体交互行为提供了新的见解，作为选择和聚合之间的关键中间层

Abstract: This paper introduces OSC (Orchestrating Cognitive Synergy), a
knowledge-aware adaptive collaboration framework designed to enhance cognitive
synergy in multi-agent systems with large language models. While prior work has
advanced agent selection and result aggregation, efficient linguistic
interactions for deep collaboration among expert agents remain a critical
bottleneck. OSC addresses this gap as a pivotal intermediate layer between
selection and aggregation, introducing Collaborator Knowledge Models (CKM) to
enable each agent to dynamically perceive its collaborators' cognitive states.
Through real-time cognitive gap analysis, agents adaptively adjust
communication behaviors, including content focus, detail level, and expression
style, using learned strategies. Experiments on complex reasoning and
problem-solving benchmarks demonstrate that OSC significantly improves task
performance and communication efficiency, transforming "parallel-working
individuals'' into a "deeply collaborative cognitive team.'' This framework not
only optimizes multi-agent collaboration but also offers new insights into LLM
agent interaction behaviors.

</details>


### [29] [SparkUI-Parser: Enhancing GUI Perception with Robust Grounding and Parsing](https://arxiv.org/abs/2509.04908)
*Hongyi Jing,Jiafu Chen,Chen Rao,Ziqiang Dang,Jiajie Teng,Tianyi Chu,Juncheng Mo,Shuo Fang,Huaizhong Lin,Rui Lv,Chenguang Ma,Lei Zhao*

Main category: cs.AI

TL;DR: SparkUI-Parser是一个新颖的端到端GUI解析框架，通过连续坐标建模和拒绝机制，显著提升了多模态大语言模型在GUI感知中的定位精度和解析能力。


<details>
  <summary>Details</summary>
Motivation: 现有MLLM方法存在两个主要问题：1）基于文本自回归机制的离散坐标建模导致定位精度低和推理速度慢；2）只能定位预定义元素集合，无法解析整个界面，限制了广泛应用和下游任务支持。

Method: 提出基于预训练MLLM的连续坐标建模方法，增加token路由器和坐标解码器；引入基于改进匈牙利匹配算法的拒绝机制来识别和拒绝不存在的元素；构建ScreenParse基准测试集。

Result: 在ScreenSpot、ScreenSpot-v2、CAGUI-Grounding和ScreenParse等多个基准测试中均优于最先进方法，显著提升了准确性和推理速度。

Conclusion: SparkUI-Parser通过连续建模和拒绝机制有效解决了现有MLLM在GUI解析中的局限性，为GUI感知任务提供了更精确和高效的解决方案。

Abstract: The existing Multimodal Large Language Models (MLLMs) for GUI perception have
made great progress. However, the following challenges still exist in prior
methods: 1) They model discrete coordinates based on text autoregressive
mechanism, which results in lower grounding accuracy and slower inference
speed. 2) They can only locate predefined sets of elements and are not capable
of parsing the entire interface, which hampers the broad application and
support for downstream tasks. To address the above issues, we propose
SparkUI-Parser, a novel end-to-end framework where higher localization
precision and fine-grained parsing capability of the entire interface are
simultaneously achieved. Specifically, instead of using probability-based
discrete modeling, we perform continuous modeling of coordinates based on a
pre-trained Multimodal Large Language Model (MLLM) with an additional token
router and coordinate decoder. This effectively mitigates the limitations
inherent in the discrete output characteristics and the token-by-token
generation process of MLLMs, consequently boosting both the accuracy and the
inference speed. To further enhance robustness, a rejection mechanism based on
a modified Hungarian matching algorithm is introduced, which empowers the model
to identify and reject non-existent elements, thereby reducing false positives.
Moreover, we present ScreenParse, a rigorously constructed benchmark to
systematically assess structural perception capabilities of GUI models across
diverse scenarios. Extensive experiments demonstrate that our approach
consistently outperforms SOTA methods on ScreenSpot, ScreenSpot-v2,
CAGUI-Grounding and ScreenParse benchmarks. The resources are available at
https://github.com/antgroup/SparkUI-Parser.

</details>


### [30] [Towards Ontology-Based Descriptions of Conversations with Qualitatively-Defined Concepts](https://arxiv.org/abs/2509.04926)
*Barbara Gendron,Gaël Guibon,Mathieu D'aquin*

Main category: cs.AI

TL;DR: 本文提出基于本体的方法来形式化定义对话特征，通过语言描述符将定性概念转化为定量定义，并应用于LLM的语言熟练度控制，提高了对话AI的透明度和可解释性。


<details>
  <summary>Details</summary>
Motivation: 解决大型语言模型作为对话代理时的可控性挑战，特别是确保可预测和用户个性化的响应，需要将定性对话特征转化为可形式化定义和推理的定量概念。

Method: 使用语言描述符集合推导定性概念的定量定义，在描述逻辑中形式化这些定义并构建本体，通过微调指导LLM的受控文本生成，以CEFR语言熟练度等级为案例研究。

Result: 实验结果表明该方法提供了一致且可解释的熟练度等级定义，改善了对话AI的透明度。

Conclusion: 基于本体的方法能够有效形式化对话特征，为LLM的受控生成提供可解释的框架，提升了对话系统的可控性和透明度。

Abstract: The controllability of Large Language Models (LLMs) when used as
conversational agents is a key challenge, particularly to ensure predictable
and user-personalized responses. This work proposes an ontology-based approach
to formally define conversational features that are typically qualitative in
nature. By leveraging a set of linguistic descriptors, we derive quantitative
definitions for qualitatively-defined concepts, enabling their integration into
an ontology for reasoning and consistency checking. We apply this framework to
the task of proficiency-level control in conversations, using CEFR language
proficiency levels as a case study. These definitions are then formalized in
description logic and incorporated into an ontology, which guides controlled
text generation of an LLM through fine-tuning. Experimental results demonstrate
that our approach provides consistent and explainable proficiency-level
definitions, improving transparency in conversational AI.

</details>


### [31] [Internet 3.0: Architecture for a Web-of-Agents with it's Algorithm for Ranking Agents](https://arxiv.org/abs/2509.04979)
*Rajesh Tembarai Krishnamachari,Srividya Rajesh*

Main category: cs.AI

TL;DR: DOVIS协议和AgentRank-UC算法为构建可信的智能体网络提供解决方案，通过隐私保护的性能数据收集和动态排名机制实现智能体选择和评估


<details>
  <summary>Details</summary>
Motivation: 随着AI智能体在互联网中的广泛应用，需要建立有效的智能体排名系统来选择性能优秀、可信赖的智能体，但当前缺乏统一的性能评估机制和隐私保护的数据收集方法

Method: 提出DOVIS五层操作协议（发现、编排、验证、激励、语义）来收集隐私保护的性能数据，并开发AgentRank-UC算法结合使用频率和能力指标进行动态排名

Result: 通过仿真实验验证了系统的收敛性、鲁棒性和抗Sybil攻击能力，证明了协调协议和性能感知排名的可行性

Conclusion: DOVIS和AgentRank-UC为实现可扩展、可信赖的智能体网络提供了可行的技术方案，能够支持大规模智能体生态系统的健康发展

Abstract: AI agents -- powered by reasoning-capable large language models (LLMs) and
integrated with tools, data, and web search -- are poised to transform the
internet into a \emph{Web of Agents}: a machine-native ecosystem where
autonomous agents interact, collaborate, and execute tasks at scale. Realizing
this vision requires \emph{Agent Ranking} -- selecting agents not only by
declared capabilities but by proven, recent performance. Unlike Web~1.0's
PageRank, a global, transparent network of agent interactions does not exist;
usage signals are fragmented and private, making ranking infeasible without
coordination.
  We propose \textbf{DOVIS}, a five-layer operational protocol
(\emph{Discovery, Orchestration, Verification, Incentives, Semantics}) that
enables the collection of minimal, privacy-preserving aggregates of usage and
performance across the ecosystem. On this substrate, we implement
\textbf{AgentRank-UC}, a dynamic, trust-aware algorithm that combines
\emph{usage} (selection frequency) and \emph{competence} (outcome quality,
cost, safety, latency) into a unified ranking. We present simulation results
and theoretical guarantees on convergence, robustness, and Sybil resistance,
demonstrating the viability of coordinated protocols and performance-aware
ranking in enabling a scalable, trustworthy Agentic Web.

</details>


### [32] [Sticker-TTS: Learn to Utilize Historical Experience with a Sticker-driven Test-Time Scaling Framework](https://arxiv.org/abs/2509.05007)
*Jie Chen,Jinhao Jiang,Yingqian Min,Zican Dong,Shijie Wang,Wayne Xin Zhao,Ji-Rong Wen*

Main category: cs.AI

TL;DR: Sticker-TTS是一个新颖的测试时扩展框架，通过协调三个协作的大型推理模型，利用历史经验进行迭代探索和优化解决方案，在数学推理任务上超越了现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前测试时扩展方法主要依赖冗余采样，忽略了历史经验的利用，限制了计算效率。需要一种更高效的方法来利用历史推理经验。

Method: 提出Sticker-TTS框架，协调三个协作的大型推理模型，通过提取、精炼和重用关键信息（称为sticker）进行多轮推理。采用两阶段优化策略，结合模仿学习和自我改进。

Result: 在三个具有挑战性的数学推理基准测试（AIME-24、AIME-25、OlymMATH）上，Sticker-TTS在可比推理预算下持续超越强基线方法，包括自一致性和先进强化学习方法。

Conclusion: Sticker-TTS通过sticker引导的历史经验利用，显著提高了推理效率和性能，证明了历史经验在测试时扩展中的有效性。

Abstract: Large reasoning models (LRMs) have exhibited strong performance on complex
reasoning tasks, with further gains achievable through increased computational
budgets at inference. However, current test-time scaling methods predominantly
rely on redundant sampling, ignoring the historical experience utilization,
thereby limiting computational efficiency. To overcome this limitation, we
propose Sticker-TTS, a novel test-time scaling framework that coordinates three
collaborative LRMs to iteratively explore and refine solutions guided by
historical attempts. At the core of our framework are distilled key
conditions-termed stickers-which drive the extraction, refinement, and reuse of
critical information across multiple rounds of reasoning. To further enhance
the efficiency and performance of our framework, we introduce a two-stage
optimization strategy that combines imitation learning with self-improvement,
enabling progressive refinement. Extensive evaluations on three challenging
mathematical reasoning benchmarks, including AIME-24, AIME-25, and OlymMATH,
demonstrate that Sticker-TTS consistently surpasses strong baselines, including
self-consistency and advanced reinforcement learning approaches, under
comparable inference budgets. These results highlight the effectiveness of
sticker-guided historical experience utilization. Our code and data are
available at https://github.com/RUCAIBox/Sticker-TTS.

</details>


### [33] [Finding your MUSE: Mining Unexpected Solutions Engine](https://arxiv.org/abs/2509.05072)
*Nir Sweed,Hanit Hakim,Ben Wolfson,Hila Lifshitz,Dafna Shahaf*

Main category: cs.AI

TL;DR: 本文提出功能性概念图(FCG)方法来解决创新者认知固着问题，通过构建大规模功能元素互联表示来支持抽象、问题重构和类比启发，并开发了MUSE算法来生成创意灵感。


<details>
  <summary>Details</summary>
Motivation: 创新者往往对现有解决方案或初步想法存在认知固着，这阻碍了对新颖替代方案的探索，需要一种方法来打破这种思维定式。

Method: 构建功能性概念图(FCG)，建立功能元素的互联表示，支持抽象、问题重构和类比启发；开发MUSE算法利用FCG为给定问题生成创意灵感。

Result: 成功构建了包含50万项专利的大规模高质量FCG，具有明确的抽象关系，克服了先前工作的局限性。

Conclusion: FCG方法能有效支持创新过程中的抽象思维和类比启发，发布的专利FCG数据集为后续研究提供了宝贵资源。

Abstract: Innovators often exhibit cognitive fixation on existing solutions or nascent
ideas, hindering the exploration of novel alternatives. This paper introduces a
methodology for constructing Functional Concept Graphs (FCGs), interconnected
representations of functional elements that support abstraction, problem
reframing, and analogical inspiration. Our approach yields large-scale,
high-quality FCGs with explicit abstraction relations, overcoming limitations
of prior work. We further present MUSE, an algorithm leveraging FCGs to
generate creative inspirations for a given problem. We demonstrate our method
by computing an FCG on 500K patents, which we release for further research.

</details>


### [34] [ProToM: Promoting Prosocial Behaviour via Theory of Mind-Informed Feedback](https://arxiv.org/abs/2509.05091)
*Matteo Bortoletto,Yichao Zhou,Lance Ying,Tianmin Shu,Andreas Bulling*

Main category: cs.AI

TL;DR: ProToM是一个基于心理理论的AI系统，通过贝叶斯逆规划推断智能体目标，提供针对性反馈来促进多智能体系统中的亲社会行为，在多个环境中优于现有大型语言模型。


<details>
  <summary>Details</summary>
Motivation: 解决多智能体系统中智能体在追求独立目标时难以确定何时以及如何协助他人、促进合作的问题，旨在开发能够提供有用反馈来促进亲社会行为的AI系统。

Method: 提出ProToM系统：1）使用贝叶斯逆规划推断智能体目标；2）基于推断的目标分布，通过最大化期望效用来选择上下文敏感的反馈信息。

Result: 在Doors、Keys和Gems以及Overcooked两个多智能体环境中测试，ProToM相比基线方法：获得更高的成功率、更短的任务完成时间，且更受人类用户青睐。现有大型语言模型在提供上下文接地且时机恰当的反馈方面表现不足。

Conclusion: ProToM能够提供有针对性和有用的反馈，有效促进多智能体系统中的亲社会行为，优于当前最先进的大型语言和推理模型。

Abstract: While humans are inherently social creatures, the challenge of identifying
when and how to assist and collaborate with others - particularly when pursuing
independent goals - can hinder cooperation. To address this challenge, we aim
to develop an AI system that provides useful feedback to promote prosocial
behaviour - actions that benefit others, even when not directly aligned with
one's own goals. We introduce ProToM, a Theory of Mind-informed facilitator
that promotes prosocial actions in multi-agent systems by providing targeted,
context-sensitive feedback to individual agents. ProToM first infers agents'
goals using Bayesian inverse planning, then selects feedback to communicate by
maximising expected utility, conditioned on the inferred goal distribution. We
evaluate our approach against baselines in two multi-agent environments: Doors,
Keys, and Gems, as well as Overcooked. Our results suggest that
state-of-the-art large language and reasoning models fall short of
communicating feedback that is both contextually grounded and well-timed -
leading to higher communication overhead and task speedup. In contrast, ProToM
provides targeted and helpful feedback, achieving a higher success rate,
shorter task completion times, and is consistently preferred by human users.

</details>


### [35] [Evaluation and Comparison Semantics for ODRL](https://arxiv.org/abs/2509.05139)
*Jaime Osvaldo Salas,Paolo Pareti,Semih Yumuşak,Soulmaz Gheisari,Luis-Daniel Ibáñez,George Konstantinidis*

Main category: cs.AI

TL;DR: 为ODRL语言提供基于查询答复的正式语义，并研究政策比较问题


<details>
  <summary>Details</summary>
Motivation: ODRL作为数字资源访问和使用管理的工业标准，虽有些进展，但仍缺乏全面的正式语义定义

Method: 提出一种简单直观的基于查询答复的正式语义，对比之前的形式化方法进行了精炼

Result: 语义与ODRL 2.2版本规范保持一致，并基于此定义了政策比较问题

Conclusion: 该方法为ODRL政策的评估和比较提供了可靠的理论基础，特别适用于数据共享场景

Abstract: We consider the problem of evaluating, and comparing computational policies
in the Open Digital Rights Language (ODRL), which has become the de facto
standard for governing the access and usage of digital resources. Although
preliminary progress has been made on the formal specification of the
language's features, a comprehensive formal semantics of ODRL is still missing.
In this paper, we provide a simple and intuitive formal semantics for ODRL that
is based on query answering. Our semantics refines previous formalisations, and
is aligned with the latest published specification of the language (2.2).
Building on our evaluation semantics, and motivated by data sharing scenarios,
we also define and study the problem of comparing two policies, detecting
equivalent, more restrictive or more permissive policies.

</details>


### [36] [LatticeWorld: A Multimodal Large Language Model-Empowered Framework for Interactive Complex World Generation](https://arxiv.org/abs/2509.05263)
*Yinglin Duan,Zhengxia Zou,Tongwei Gu,Wei Jia,Zhan Zhao,Luyi Xu,Xinzhu Liu,Hao Jiang,Kang Chen,Shuang Qiu*

Main category: cs.AI

TL;DR: LatticeWorld是一个基于轻量级LLM和游戏引擎的3D世界生成框架，通过多模态输入生成动态交互式虚拟环境，大幅提升工业生产效率


<details>
  <summary>Details</summary>
Motivation: 传统手动建模创建3D场景效率低下，需要开发能够基于用户指令自动生成高质量3D虚拟世界的生成方法，以缩小仿真与现实差距

Method: 结合轻量级LLaMA-2-7B模型和工业级渲染引擎（如Unreal Engine 5），接受文本描述和视觉指令作为多模态输入，生成包含动态代理的大规模3D交互世界

Result: 在场景布局生成和视觉保真度方面达到优异精度，相比传统手动生产方式实现90倍以上的工业生产效率提升，同时保持高创意质量

Conclusion: LatticeWorld提供了一个简单有效的3D世界生成框架，能够显著提升3D环境工业生产线效率，为构建更真实的虚拟世界提供了可行方案

Abstract: Recent research has been increasingly focusing on developing 3D world models
that simulate complex real-world scenarios. World models have found broad
applications across various domains, including embodied AI, autonomous driving,
entertainment, etc. A more realistic simulation with accurate physics will
effectively narrow the sim-to-real gap and allow us to gather rich information
about the real world conveniently. While traditional manual modeling has
enabled the creation of virtual 3D scenes, modern approaches have leveraged
advanced machine learning algorithms for 3D world generation, with most recent
advances focusing on generative methods that can create virtual worlds based on
user instructions. This work explores such a research direction by proposing
LatticeWorld, a simple yet effective 3D world generation framework that
streamlines the industrial production pipeline of 3D environments. LatticeWorld
leverages lightweight LLMs (LLaMA-2-7B) alongside the industry-grade rendering
engine (e.g., Unreal Engine 5) to generate a dynamic environment. Our proposed
framework accepts textual descriptions and visual instructions as multimodal
inputs and creates large-scale 3D interactive worlds with dynamic agents,
featuring competitive multi-agent interaction, high-fidelity physics
simulation, and real-time rendering. We conduct comprehensive experiments to
evaluate LatticeWorld, showing that it achieves superior accuracy in scene
layout generation and visual fidelity. Moreover, LatticeWorld achieves over a
$90\times$ increase in industrial production efficiency while maintaining high
creative quality compared with traditional manual production methods. Our demo
video is available at https://youtu.be/8VWZXpERR18

</details>
