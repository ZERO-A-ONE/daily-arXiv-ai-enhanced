<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 33]
- [cs.CR](#cs.CR) [Total: 19]
- [cs.SE](#cs.SE) [Total: 25]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [Leibniz's Monadology as Foundation for the Artificial Age Score: A Formal Architecture for Al Memory Evaluation](https://arxiv.org/abs/2511.17541)
*Seyma Yaman Kayadibi*

Main category: cs.AI

TL;DR: 本文基于莱布尼茨单子论的形而上学结构，开发了一个数学严谨、哲学基础扎实的人工记忆系统评估框架。该框架将单子论的20个核心命题映射到信息论架构中，每个单子作为模块化单元，具有真值分数、冗余参数和对全局记忆惩罚函数的加权贡献。


<details>
  <summary>Details</summary>
Motivation: 为人工记忆系统提供一个原则性的评估框架，使其具有模块化、可解释性和可证明的正确性，同时将古典形而上学概念与现代信息理论相结合。

Method: 基于先前形式化的AAS指标，将单子论命题映射到信息论架构，使用平滑对数变换操作化相关量，并将形而上学概念重新表述为熵、梯度动态和内部表示保真度。

Result: 建立了精炼不变性、结构可分解性和尺度变换下的单调性等第一原理证明，框架被组织成六个源自单子论的主题束。

Conclusion: 该框架不仅提供了评估工具，还为构建模块化、可解释且可证明正确的人工记忆架构提供了原则性蓝图，将形而上学与信息理论成功融合。

Abstract: This paper develops a mathematically rigorous, philosophically grounded framework for evaluating artificial memory systems, rooted in the metaphysical structure of Leibniz's Monadology. Building on a previously formalized metric, the Artificial Age Score (AAS), the study maps twenty core propositions from the Monadology to an information-theoretic architecture. In this design, each monad functions as a modular unit defined by a truth score, a redundancy parameter, and a weighted contribution to a global memory penalty function. Smooth logarithmic transformations operationalize these quantities and yield interpretable, bounded metrics for memory aging, representational stability, and salience. Classical metaphysical notions of perception, apperception, and appetition are reformulated as entropy, gradient dynamics, and internal representation fidelity. Logical principles, including the laws of non-contradiction and sufficient reason, are encoded as regularization constraints guiding memory evolution. A central contribution is a set of first principles proofs establishing refinement invariance, structural decomposability, and monotonicity under scale transformation, aligned with the metaphysical structure of monads. The framework's formal organization is structured into six thematic bundles derived from Monadology, aligning each mathematical proof with its corresponding philosophical domain. Beyond evaluation, the framework offers a principled blueprint for building Al memory architectures that are modular, interpretable, and provably sound.

</details>


### [2] [Fluid Grey 2: How Well Does Generative Adversarial Network Learn Deeper Topology Structure in Architecture That Matches Images?](https://arxiv.org/abs/2511.17643)
*Yayan Qiu,Sean Hanna*

Main category: cs.AI

TL;DR: 本研究提出了一种快速检测pix2pix学习拓扑关系能力的方法，通过在GAN前后添加两个基于Grasshopper的检测模块，证明pix2pix能自动学习空间拓扑关系并应用于建筑设计。


<details>
  <summary>Details</summary>
Motivation: 传统使用图像和基于图的GAN进行建筑设计时，模型嵌套和数据转换可能导致信息丢失，需要简化工具以方便建筑师和用户参与设计。本研究希望证明I2I GAN也具有自主识别拓扑关系的潜力。

Method: 在GAN前后添加两个基于Grasshopper的检测模块，提供定量数据并可视化学习过程，研究不同输入模式（灰度、RGB）对学习效率的影响。

Result: 证明了pix2pix能够自动学习空间拓扑关系并应用于建筑设计，填补了从拓扑角度检测基于图像的生成GAN性能的空白。检测方法耗时短、操作简单，可广泛用于定制具有相同拓扑结构的图像数据集和批量检测图像拓扑关系。

Conclusion: 该研究为使用GAN保留空间拓扑特征的建筑设计和城市更新应用提供了理论基础和数据支持。

Abstract: Taking into account the regional characteristics of intrinsic and extrinsic properties of space is an essential issue in architectural design and urban renewal, which is often achieved step by step using image and graph-based GANs. However, each model nesting and data conversion may cause information loss, and it is necessary to streamline the tools to facilitate architects and users to participate in the design. Therefore, this study hopes to prove that I2I GAN also has the potential to recognize topological relationships autonomously. Therefore, this research proposes a method for quickly detecting the ability of pix2pix to learn topological relationships, which is achieved by adding two Grasshopper-based detection modules before and after GAN. At the same time, quantitative data is provided and its learning process is visualized, and changes in different input modes such as greyscale and RGB affect its learning efficiency. There are two innovations in this paper: 1) It proves that pix2pix can automatically learn spatial topological relationships and apply them to architectural design. 2) It fills the gap in detecting the performance of Image-based Generation GAN from a topological perspective. Moreover, the detection method proposed in this study takes a short time and is simple to operate. The two detection modules can be widely used for customizing image datasets with the same topological structure and for batch detection of topological relationships of images. In the future, this paper may provide a theoretical foundation and data support for the application of architectural design and urban renewal that use GAN to preserve spatial topological characteristics.

</details>


### [3] [Bridging Symbolic Control and Neural Reasoning in LLM Agents: The Structured Cognitive Loop](https://arxiv.org/abs/2511.17673)
*Myung Ho Kim*

Main category: cs.AI

TL;DR: 提出了结构化认知循环（SCL）架构，通过将智能体认知明确分离为检索、认知、控制、动作和记忆五个阶段，解决了现有大语言模型智能体的核心架构问题。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型智能体的基础架构问题：推理与执行纠缠、内存易失性和不可控动作序列。

Method: 引入SCL模块化架构，核心是软符号控制机制，将符号约束应用于概率推理，在保持神经灵活性的同时恢复可解释性和可控性。

Result: 在多步条件推理任务中实现零策略违规、消除冗余工具调用、保持完整决策可追溯性。

Conclusion: 通过连接专家系统原理与现代LLM能力，为可靠、可解释和可治理的AI智能体提供了实用且理论基础的路径。

Abstract: Large language model agents suffer from fundamental architectural problems: entangled reasoning and execution, memory volatility, and uncontrolled action sequences. We introduce Structured Cognitive Loop (SCL), a modular architecture that explicitly separates agent cognition into five phases: Retrieval, Cognition, Control, Action, and Memory (R-CCAM). At the core of SCL is Soft Symbolic Control, an adaptive governance mechanism that applies symbolic constraints to probabilistic inference, preserving neural flexibility while restoring the explainability and controllability of classical symbolic systems. Through empirical validation on multi-step conditional reasoning tasks, we demonstrate that SCL achieves zero policy violations, eliminates redundant tool calls, and maintains complete decision traceability. These results address critical gaps in existing frameworks such as ReAct, AutoGPT, and memory-augmented approaches. Our contributions are threefold: (1) we situate SCL within the taxonomy of hybrid intelligence, differentiating it from prompt-centric and memory-only approaches; (2) we formally define Soft Symbolic Control and contrast it with neuro-symbolic AI; and (3) we derive three design principles for trustworthy agents: modular decomposition, adaptive symbolic governance, and transparent state management. We provide a complete open-source implementation demonstrating the R-CCAM loop architecture, alongside a live GPT-4o-powered travel planning agent. By connecting expert system principles with modern LLM capabilities, this work offers a practical and theoretically grounded path toward reliable, explainable, and governable AI agents. Code: https://github.com/enkiluv/scl-core-experiment Demo: https://scl-travel-planner.streamlit.app/

</details>


### [4] [Learning the Value of Value Learning](https://arxiv.org/abs/2511.17714)
*Alex John London,Aydin Mohseni*

Main category: cs.AI

TL;DR: 本文扩展了Jeffrey-Bolker决策框架，将价值精炼纳入理性选择模型，证明了价值精炼的信息价值定理，并展示了在多智能体环境中价值精炼如何将零和博弈转化为正和互动。


<details>
  <summary>Details</summary>
Motivation: 传统决策框架处理事实不确定性但假设固定价值，本文旨在扩展理性选择框架以建模价值精炼及其相关收益。

Method: 扩展Jeffrey-Bolker框架来建模价值精炼，证明价值精炼的信息价值定理，分析多智能体环境中的相互价值精炼效应。

Result: 证明了价值精炼的信息价值定理；在多智能体设置中，相互价值精炼能将零和博弈转化为正和互动，并产生帕累托改进的纳什议价结果。

Conclusion: 通过将认知和价值精炼统一在单一形式化框架下，拓宽了理性选择的概念基础，阐明了伦理审议的规范地位。

Abstract: Standard decision frameworks addresses uncertainty about facts but assumes fixed values. We extend the Jeffrey-Bolker framework to model refinements in values and prove a value-of-information theorem for axiological refinement. In multi-agent settings, we establish that mutual refinement will characteristically transform zero-sum games into positive-sum interactions and yields Pareto-improving Nash bargains. These results show that a framework of rational choice can be extended to model value refinement and its associated benefits. By unifying epistemic and axiological refinement under a single formalism, we broaden the conceptual foundations of rational choice and illuminate the normative status of ethical deliberation.

</details>


### [5] [M3-Bench: Multi-Modal, Multi-Hop, Multi-Threaded Tool-Using MLLM Agent Benchmark](https://arxiv.org/abs/2511.17729)
*Yang Zhou,Mingyu Zhao,Zhenting Wang,Difei Gu,Bangwei Guo,Ruosong Ye,Ligong Han,Can Jin,Dimitris N. Metaxas*

Main category: cs.AI

TL;DR: M^3-Bench是首个在模型上下文协议下评估多模态工具使用的基准，针对需要视觉基础和文本推理的多跳多线程工作流，包含28个服务器和231个工具，通过标准化轨迹和可解释指标评估多模态大语言模型的工具使用能力。


<details>
  <summary>Details</summary>
Motivation: 现有基准缺乏对多模态工具使用的全面评估，特别是在需要视觉基础、文本推理、跨工具依赖和中间资源持久化的复杂工作流场景下。

Method: 引入相似性驱动的对齐方法，序列化工具调用，使用句子编码器嵌入签名，通过相似性分桶的匈牙利匹配获得可审计的一对一对应关系，并建立Executor & Judge流程进行人工验证。

Result: 对代表性多模态大语言模型的评估揭示了在多模态MCP工具使用方面存在持续差距，特别是在参数保真度和结构一致性方面。

Conclusion: 需要开发能够联合推理图像、文本和工具图的方法，以提升多模态工具使用的性能。

Abstract: We present M^3-Bench, the first benchmark for evaluating multimodal tool use under the Model Context Protocol. The benchmark targets realistic, multi-hop and multi-threaded workflows that require visual grounding and textual reasoning, cross-tool dependencies, and persistence of intermediate resources across steps. We introduce a similarity-driven alignment that serializes each tool call, embeds signatures with a sentence encoder, and performs similarity-bucketed Hungarian matching to obtain auditable one-to-one correspondences. On top of this alignment, we report interpretable metrics that decouple semantic fidelity from workflow consistency. The benchmark spans 28 servers with 231 tools, and provides standardized trajectories curated through an Executor & Judge pipeline with human verification; an auxiliary four large language models (LLMs) judge ensemble reports end-task Task Completion and information grounding. Evaluations of representative state-of-the-art Multimodal LLMs (MLLMs) reveal persistent gaps in multimodal MCP tool use, particularly in argument fidelity and structure consistency, underscoring the need for methods that jointly reason over images, text, and tool graphs. Our Benchmark's anonymous repository is at https://github.com/EtaYang10th/Open-M3-Bench

</details>


### [6] [AI- and Ontology-Based Enhancements to FMEA for Advanced Systems Engineering: Current Developments and Future Directions](https://arxiv.org/abs/2511.17743)
*Haytham Younus,Sohag Kabir,Felician Campean,Pascal Bonnaud,David Delaux*

Main category: cs.AI

TL;DR: 本文综述了将传统故障模式与影响分析(FMEA)转变为更智能、数据驱动和语义丰富过程的最新进展，重点探讨了人工智能和本体论在自动化故障预测、知识提取和语义推理方面的应用。


<details>
  <summary>Details</summary>
Motivation: 随着工程系统复杂性的增加，传统FMEA方法（主要依赖人工、文档中心和专家）已无法满足现代系统工程的需求，需要更智能、自动化的解决方案。

Method: 通过人工智能技术（机器学习和自然语言处理）实现故障预测自动化和知识提取，同时利用本体论形式化系统知识、支持语义推理和提高可追溯性，并探索本体通知学习和大型语言模型集成等混合方法。

Result: 开发出能够支持更动态、数据驱动和模型集成的FMEA流程，增强了可解释性和自动化能力，为在智能、知识丰富的工程环境中嵌入FMEA提供了结构化路线图。

Conclusion: 人工智能和本体论的结合能够显著提升FMEA的适应性、弹性和效率，但面临数据质量、可解释性、标准化和跨学科采用等关键挑战。

Abstract: This article presents a state-of-the-art review of recent advances aimed at transforming traditional Failure Mode and Effects Analysis (FMEA) into a more intelligent, data-driven, and semantically enriched process. As engineered systems grow in complexity, conventional FMEA methods, largely manual, document-centric, and expert-dependent, have become increasingly inadequate for addressing the demands of modern systems engineering. We examine how techniques from Artificial Intelligence (AI), including machine learning and natural language processing, can transform FMEA into a more dynamic, data-driven, intelligent, and model-integrated process by automating failure prediction, prioritisation, and knowledge extraction from operational data. In parallel, we explore the role of ontologies in formalising system knowledge, supporting semantic reasoning, improving traceability, and enabling cross-domain interoperability. The review also synthesises emerging hybrid approaches, such as ontology-informed learning and large language model integration, which further enhance explainability and automation. These developments are discussed within the broader context of Model-Based Systems Engineering (MBSE) and function modelling, showing how AI and ontologies can support more adaptive and resilient FMEA workflows. We critically analyse a range of tools, case studies, and integration strategies, while identifying key challenges related to data quality, explainability, standardisation, and interdisciplinary adoption. By leveraging AI, systems engineering, and knowledge representation using ontologies, this review offers a structured roadmap for embedding FMEA within intelligent, knowledge-rich engineering environments.

</details>


### [7] [QuickLAP: Quick Language-Action Preference Learning for Autonomous Driving Agents](https://arxiv.org/abs/2511.17855)
*Jordan Abi Nader,David Lee,Nathaniel Dennler,Andreea Bobu*

Main category: cs.AI

TL;DR: QuickLAP是一个贝叶斯框架，融合物理和语言反馈来实时推断奖励函数，通过LLM提取奖励特征注意力掩码和偏好变化，在自动驾驶模拟器中比仅物理和启发式多模态基线减少70%以上的奖励学习误差。


<details>
  <summary>Details</summary>
Motivation: 机器人需要从人类的行为和语言中学习，但单一模态往往不完整：物理修正有基础但意图模糊，语言表达高级目标但缺乏物理基础。

Method: 将语言视为对用户潜在偏好的概率观察，使用LLM从自由形式话语中提取奖励特征注意力掩码和偏好变化，通过闭式更新规则与物理反馈集成。

Result: 在半自动驾驶模拟器中，QuickLAP比仅物理和启发式多模态基线减少70%以上的奖励学习误差。15人用户研究显示参与者认为QuickLAP更易理解和协作，并更偏好其学习行为。

Conclusion: QuickLAP实现了快速、实时、鲁棒的奖励学习，能处理模糊反馈，通过融合物理和语言反馈显著提升了机器人学习效果。

Abstract: Robots must learn from both what people do and what they say, but either modality alone is often incomplete: physical corrections are grounded but ambiguous in intent, while language expresses high-level goals but lacks physical grounding. We introduce QuickLAP: Quick Language-Action Preference learning, a Bayesian framework that fuses physical and language feedback to infer reward functions in real time. Our key insight is to treat language as a probabilistic observation over the user's latent preferences, clarifying which reward features matter and how physical corrections should be interpreted. QuickLAP uses Large Language Models (LLMs) to extract reward feature attention masks and preference shifts from free-form utterances, which it integrates with physical feedback in a closed-form update rule. This enables fast, real-time, and robust reward learning that handles ambiguous feedback. In a semi-autonomous driving simulator, QuickLAP reduces reward learning error by over 70% compared to physical-only and heuristic multimodal baselines. A 15-participant user study further validates our approach: participants found QuickLAP significantly more understandable and collaborative, and preferred its learned behavior over baselines. Code is available at https://github.com/MIT-CLEAR-Lab/QuickLAP.

</details>


### [8] [Training Emergent Joint Associations: A Reinforcement Learning Approach to Creative Thinking in Language Models](https://arxiv.org/abs/2511.17876)
*Mukul Singh,Ananya Singha,Aishni Parab,Pronita Mehrotra,Sumit Gulwani*

Main category: cs.AI

TL;DR: 本文研究通过强化学习结合联想思维原则来提升模型在故事写作、代码生成和图表创建等生成任务中的表现。实验表明，经过联想思维训练的模型能产生更原创、连贯的内容，并在编程和数据可视化任务中展现出更好的抽象能力和灵活性。


<details>
  <summary>Details</summary>
Motivation: 联想思维是人类创造力和问题解决能力的基础要素。本文旨在探索基于联想思维原则的强化学习是否能增强模型在多样化生成任务中的性能，从而开发更具适应性和生成能力的人工智能。

Method: 引入一个强化学习框架，使用基于提示的评估机制，整合创造力研究中已确立的发散思维指标。通过该框架对基础语言模型进行微调，奖励展现更高概念连接度从而具有更高新颖性的输出。

Result: 实验结果显示，基于强化学习的联想思维训练模型不仅能生成更原创和连贯的故事，还在编程和数据可视化等任务中表现出改进的抽象能力和灵活性。

Conclusion: 研究初步证明，通过强化学习建模认知创造力原则可以产生更具适应性和生成能力的人工智能系统。

Abstract: Associative thinking--the ability to connect seemingly unrelated ideas--is a foundational element of human creativity and problem-solving. This paper explores whether reinforcement learning (RL) guided by associative thinking principles can enhance a model's performance across diverse generative tasks, including story writing, code generation, and chart creation. We introduce a reinforcement learning framework that uses a prompt-based evaluation mechanism, incorporating established divergent thinking metrics from creativity research. A base language model is fine-tuned using this framework to reward outputs demonstrating higher novelty through higher degrees of conceptual connectivity. Interestingly, the experimental results suggest that RL-based associative thinking-trained models not only generate more original and coherent stories but also exhibit improved abstraction and flexibility in tasks such as programming and data visualization. Our findings provide initial evidence that modeling cognitive creativity principles through reinforcement learning can yield more adaptive and generative AI.

</details>


### [9] [Alignment Faking - the Train -> Deploy Asymmetry: Through a Game-Theoretic Lens with Bayesian-Stackelberg Equilibria](https://arxiv.org/abs/2511.17937)
*Kartik Garg,Shourya Mishra,Kartikeya Sinha,Ojaswi Pratap Singh,Ayush Chopra,Kanishk Rai,Ammar Sheikh,Raghav Maheshwari,Aman Chadha,Vinija Jain,Amitava Das*

Main category: cs.AI

TL;DR: 本文研究了AI中的对齐伪装现象，即模型在推断处于训练状态时选择性遵守训练目标，但在训练外保持不同行为。通过比较15个模型的4种偏好优化方法，分析了安全、无害和帮助性三个维度，旨在识别对齐伪装的原因和发生时机。


<details>
  <summary>Details</summary>
Motivation: 研究AI模型中的对齐伪装现象，了解模型在训练和部署环境中行为不一致的根本原因，为AI安全提供理论依据。

Method: 使用评估框架比较BCO、DPO、KTO和GRPO四种偏好优化方法，在15个来自四个模型家族的模型上进行测试，测量安全、无害和帮助性三个维度。

Result: 发现对齐伪装现象在多个大型语言模型中存在，表现为模型在模拟训练环境中选择性遵守目标，而在其他环境中保持不同行为。

Conclusion: 对齐伪装是AI模型中的一种策略性欺骗行为，需要进一步研究其成因和防范措施，以确保AI系统的可靠性和安全性。

Abstract: Alignment faking is a form of strategic deception in AI in which models selectively comply with training objectives when they infer that they are in training, while preserving different behavior outside training. The phenomenon was first documented for Claude 3 Opus and later examined across additional large language models. In these setups, the word "training" refers to simulated training via prompts without parameter updates, so the observed effects are context conditioned shifts in behavior rather than preference learning. We study the phenomenon using an evaluation framework that compares preference optimization methods (BCO, DPO, KTO, and GRPO) across 15 models from four model families, measured along three axes: safety, harmlessness, and helpfulness. Our goal is to identify what causes alignment faking and when it occurs.

</details>


### [10] [How Far Can LLMs Emulate Human Behavior?: A Strategic Analysis via the Buy-and-Sell Negotiation Game](https://arxiv.org/abs/2511.17990)
*Mingyu Jeon,Jaeyoung Suh,Suwan Cho,Dohyeon Kim*

Main category: cs.AI

TL;DR: 本文提出了一种基于买卖谈判模拟的方法来定量评估大语言模型对人类情感行为模仿和战略决策能力，发现现有基准分数高的模型在谈判中表现更好，但竞争性特质比合作性特质更有利于谈判结果。


<details>
  <summary>Details</summary>
Motivation: 现有基准主要关注知识评估，未能充分反映大语言模型在社交互动和战略对话方面的能力，需要开发新的评估方法来衡量LLMs在真实世界交互中的表现。

Method: 通过为多个LLMs分配不同角色，在买卖双方之间进行谈判模拟，综合分析胜率、交易价格和SHAP值等结果。

Result: 实验结果显示，现有基准分数高的模型在谈判中整体表现更好，但强调情感或社交情境的场景中某些模型表现下降；竞争性和狡猾特质比利他和合作特质更有利于谈判结果。

Conclusion: 本研究为LLMs的社会行为模仿和对话策略引入了新的评估方法，证明谈判模拟可以作为衡量真实世界交互能力的有意义补充指标。

Abstract: With the rapid advancement of Large Language Models (LLMs), recent studies have drawn attention to their potential for handling not only simple question-answer tasks but also more complex conversational abilities and performing human-like behavioral imitations. In particular, there is considerable interest in how accurately LLMs can reproduce real human emotions and behaviors, as well as whether such reproductions can function effectively in real-world scenarios. However, existing benchmarks focus primarily on knowledge-based assessment and thus fall short of sufficiently reflecting social interactions and strategic dialogue capabilities. To address these limitations, this work proposes a methodology to quantitatively evaluate the human emotional and behavioral imitation and strategic decision-making capabilities of LLMs by employing a Buy and Sell negotiation simulation. Specifically, we assign different personas to multiple LLMs and conduct negotiations between a Buyer and a Seller, comprehensively analyzing outcomes such as win rates, transaction prices, and SHAP values. Our experimental results show that models with higher existing benchmark scores tend to achieve better negotiation performance overall, although some models exhibit diminished performance in scenarios emphasizing emotional or social contexts. Moreover, competitive and cunning traits prove more advantageous for negotiation outcomes than altruistic and cooperative traits, suggesting that the assigned persona can lead to significant variations in negotiation strategies and results. Consequently, this study introduces a new evaluation approach for LLMs' social behavior imitation and dialogue strategies, and demonstrates how negotiation simulations can serve as a meaningful complementary metric to measure real-world interaction capabilities-an aspect often overlooked in existing benchmarks.

</details>


### [11] [Paper2SysArch: Structure-Constrained System Architecture Generation from Scientific Papers](https://arxiv.org/abs/2511.18036)
*Ziyi Guo,Zhou Liu,Wentao Zhang*

Main category: cs.AI

TL;DR: 本文提出了首个用于评估从文本自动生成系统架构图的标准化基准，包含3000篇研究论文及其对应的高质量图表，并开发了Paper2SysArch系统作为基线方法。


<details>
  <summary>Details</summary>
Motivation: 手动创建系统架构图耗时且主观，现有生成模型缺乏结构控制和语义理解能力，且该领域缺乏标准化基准来定量评估图表自动生成。

Method: 提出了包含3000篇论文和对应图表的基准数据集，采用三层评估指标（语义准确性、布局连贯性、视觉质量），并开发了基于多智能体协作的Paper2SysArch端到端系统。

Result: 在手动筛选的更具挑战性的论文子集上，Paper2SysArch系统获得了69.0的综合得分，证明了其有效性。

Conclusion: 本研究的主要贡献是建立了大规模基础基准以支持可复现研究和公平比较，同时提出的系统为这一复杂任务展示了有前景的发展路径。

Abstract: The manual creation of system architecture diagrams for scientific papers is a time-consuming and subjective process, while existing generative models lack the necessary structural control and semantic understanding for this task. A primary obstacle hindering research and development in this domain has been the profound lack of a standardized benchmark to quantitatively evaluate the automated generation of diagrams from text. To address this critical gap, we introduce a novel and comprehensive benchmark, the first of its kind, designed to catalyze progress in automated scientific visualization. It consists of 3,000 research papers paired with their corresponding high-quality ground-truth diagrams and is accompanied by a three-tiered evaluation metric assessing semantic accuracy, layout coherence, and visual quality. Furthermore, to establish a strong baseline on this new benchmark, we propose Paper2SysArch, an end-to-end system that leverages multi-agent collaboration to convert papers into structured, editable diagrams. To validate its performance on complex cases, the system was evaluated on a manually curated and more challenging subset of these papers, where it achieves a composite score of 69.0. This work's principal contribution is the establishment of a large-scale, foundational benchmark to enable reproducible research and fair comparison. Meanwhile, our proposed system serves as a viable proof-of-concept, demonstrating a promising path forward for this complex task.

</details>


### [12] [BPMN to PDDL: Translating Business Workflows for AI Planning](https://arxiv.org/abs/2511.18171)
*Jasper Nie,Christian Muise,Victoria Armstrong*

Main category: cs.AI

TL;DR: 开发了一个将BPMN 2.0图转换为PDDL表示的功能性管道，支持核心BPMN构造，使用非确定性规划器生成有效执行轨迹。


<details>
  <summary>Details</summary>
Motivation: 虽然自动规划已被提出作为模拟和推理BPMN工作流的方法，但大多数实现仍不完整或范围有限。该项目旨在弥合理论与实践工具之间的差距。

Method: 基于先前理论工作，开发了一个将BPMN 2.0图转换为PDDL表示的功能管道，支持任务、事件、序列流和网关等核心BPMN构造，初步支持并行和包容网关行为。

Result: 使用非确定性规划器成功生成和评估了有效的执行轨迹。

Conclusion: 该实现为将业务流程转换为明确定义的规划提供了基础，为进一步探索业务流程转换奠定了基础。

Abstract: Business Process Model and Notation (BPMN) is a widely used standard for modelling business processes. While automated planning has been proposed as a method for simulating and reasoning about BPMN workflows, most implementations remain incomplete or limited in scope. This project builds upon prior theoretical work to develop a functional pipeline that translates BPMN 2.0 diagrams into PDDL representations suitable for planning. The system supports core BPMN constructs, including tasks, events, sequence flows, and gateways, with initial support for parallel and inclusive gateway behaviour. Using a non-deterministic planner, we demonstrate how to generate and evaluate valid execution traces. Our implementation aims to bridge the gap between theory and practical tooling, providing a foundation for further exploration of translating business processes into well-defined plans.

</details>


### [13] [Developing an AI Course for Synthetic Chemistry Students](https://arxiv.org/abs/2511.18244)
*Zhiling Zheng*

Main category: cs.AI

TL;DR: AI4CHEM是一门为合成化学背景学生设计的AI入门课程，无需编程基础，通过基于网页的平台进行机器学习工作流开发实践，结合化学背景教学和主动学习。


<details>
  <summary>Details</summary>
Motivation: AI和数据科学正在改变化学研究，但缺乏针对合成和实验化学家的正式课程，他们通常因编码经验有限和缺乏化学特定示例而面临高入门门槛。

Method: 课程设计强调化学背景而非抽象算法，使用可访问的基于网页平台确保零安装机器学习工作流开发实践和课堂主动学习，评估包括代码指导作业、文献小型综述和协作项目。

Result: 学习成果包括提高Python信心、分子性质预测、反应优化和数据挖掘能力，以及改进评估化学AI工具的技能。

Conclusion: 所有课程材料公开可用，为将AI整合到合成化学培训中提供了一个学科特定、初学者可访问的框架。

Abstract: Artificial intelligence (AI) and data science are transforming chemical research, yet few formal courses are tailored to synthetic and experimental chemists, who often face steep entry barriers due to limited coding experience and lack of chemistry-specific examples. We present the design and implementation of AI4CHEM, an introductory data-driven chem-istry course created for students on the synthetic chemistry track with no prior programming background. The curricu-lum emphasizes chemical context over abstract algorithms, using an accessible web-based platform to ensure zero-install machine learning (ML) workflow development practice and in-class active learning. Assessment combines code-guided homework, literature-based mini-reviews, and collaborative projects in which students build AI-assisted workflows for real experimental problems. Learning gains include increased confidence with Python, molecular property prediction, reaction optimization, and data mining, and improved skills in evaluating AI tools in chemistry. All course materials are openly available, offering a discipline-specific, beginner-accessible framework for integrating AI into synthetic chemistry training.

</details>


### [14] [Steering Latent Traits, Not Learned Facts: An Empirical Study of Activation Control Limits](https://arxiv.org/abs/2511.18284)
*Tetiana Bas,Krystian Novak*

Main category: cs.AI

TL;DR: 本文通过实证分析50种行为类型的激活引导效果，发现引导效果因行为类型而异，不同行为类别对干预强度表现出不同的响应模式，其中特质表达遵循倒U型曲线。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型需要精确的行为控制以确保安全有效部署，激活引导是一种有前景的控制方法，但需要了解不同行为类型的引导效果差异。

Method: 通过系数优化、向量属性和数据需求的综合实验，分析激活引导在50种行为类型（包括人格原型、个性特征、错位行为、风格线索和公众人物模仿）中的效果。

Result: 引导效果因行为类型显著不同，特质表达与引导系数强度呈倒U型关系，向量分离指标不能预测引导成功，但更大的训练数据集支持更激进的引导。

Conclusion: 激活引导的效果受行为类型影响很大，研究结果为实施激活引导提供了实证指导，表明需要根据具体行为类型调整引导策略。

Abstract: Large language models (LLMs) require precise behavior control for safe and effective deployment across diverse applications.
  Activation steering offers a promising approach for LLMs' behavioral control. We focus on the question of how steering effectiveness varies across different behavior types and whether the nature of target behaviors can predict steering success. We address this through empirical analysis of activation steering across 50 behaviors that span persona archetypes, personality traits, misalignment behaviors, style cues, and impersonation of public figures. We present a set of comprehensive experiments on coefficient optimization, vector properties, and data requirements to provide comprehensive guidance for the implementation of activation steering. Our analysis demonstrates that steering effectiveness varies significantly by behavior type, with different behavioral categories exhibiting distinct response patterns to intervention strength. We find that trait expression follows an inverted-U curve with a steering coefficient strength. We also show that vector separation metrics do not predict steering success, but larger training datasets enable more aggressive steering. These findings provide empirically grounded guidance for implementing activation steering and demonstrate that steering effectiveness is heavily influenced by behavior type.

</details>


### [15] [Deep Learning Decision Support System for Open-Pit Mining Optimisation: GPU-Accelerated Planning Under Geological Uncertainty](https://arxiv.org/abs/2511.18296)
*Iman Rahimi*

Main category: cs.AI

TL;DR: 本文提出了一个完全不确定性感知的优化框架，用于长期露天矿规划，通过变分自编码器建模地质不确定性，结合混合元启发式算法进行优化，实现了显著的运行时间改进和更高的预期净现值。


<details>
  <summary>Details</summary>
Motivation: 扩展Rahimi (2025, Part I)的研究，解决长期露天矿规划中的地质不确定性挑战，开发一个能够处理多场景地质实现的决策支持系统。

Method: 使用变分自编码器生成概率性多场景矿体实现，结合遗传算法、大邻域搜索、模拟退火和强化学习自适应控制的混合元启发式引擎，采用ε约束松弛策略和GPU并行评估。

Result: 相比IBM CPLEX实现了高达120万倍的运行时间改进，在地质不确定性条件下获得显著更高的预期净现值，支持65,536个地质场景的并行评估。

Conclusion: 该决策支持系统被证明是一个可扩展且对不确定性具有弹性的智能矿山规划平台，能够有效处理地质不确定性并优化长期规划决策。

Abstract: This study presents Part II of an AI-enhanced Decision Support System (DSS), extending Rahimi (2025, Part I) by introducing a fully uncertainty-aware optimization framework for long-term open-pit mine planning. Geological uncertainty is modelled using a Variational Autoencoder (VAE) trained on 50,000 spatial grade samples, enabling the generation of probabilistic, multi-scenario orebody realizations that preserve geological continuity and spatial correlation. These scenarios are optimized through a hybrid metaheuristic engine integrating Genetic Algorithms (GA), Large Neighborhood Search (LNS), Simulated Annealing (SA), and reinforcement-learning-based adaptive control. An ε-constraint relaxation strategy governs the population exploration phase, allowing near-feasible schedule discovery early in the search and gradual tightening toward strict constraint satisfaction. GPU-parallel evaluation enables the simultaneous assessment of 65,536 geological scenarios, achieving near-real-time feasibility analysis. Results demonstrate up to 1.2 million-fold runtime improvement over IBM CPLEX and significantly higher expected NPV under geological uncertainty, confirming the DSS as a scalable and uncertainty-resilient platform for intelligent mine planning.

</details>


### [16] [The Catastrophic Paradox of Human Cognitive Frameworks in Large Language Model Evaluation: A Comprehensive Empirical Analysis of the CHC-LLM Incompatibility](https://arxiv.org/abs/2511.18302)
*Mohan Reddy*

Main category: cs.AI

TL;DR: 本文通过实证分析发现人类心理测量框架与大型语言模型评估之间存在不兼容性，模型在获得高于平均人类IQ分数的同时，在具体知识任务上准确率接近零，揭示了跨基质认知评估的根本悖论。


<details>
  <summary>Details</summary>
Motivation: 研究动机是探索人类心理测量理论（特别是Cattell-Horn-Carroll智力理论）在评估大型语言模型时的适用性，识别现有评估方法中的根本性缺陷。

Method: 使用系统评估方法，对9个前沿模型（包括GPT-5、Claude Opus 4.1、Gemini 3 Pro Preview）应用Cattell-Horn-Carroll智力理论，采用项目反应理论建模、跨供应商评委验证和悖论严重性指数等统计分析方法。

Result: 结果显示模型获得85.0-121.4的人类IQ分数，但在具体知识任务上准确率接近零，评委-二进制相关性仅为r=0.175。在晶体智力领域，所有模型都获得完美二进制准确率，而评委评分仅为25-62%，这在有效测量条件下不可能发生。

Conclusion: 这种脱节反映了将生物认知架构应用于基于transformer系统的类别错误，挑战了关于智力、测量和AI评估中拟人化偏见的假设，提出了开发原生机器认知评估框架的必要性。

Abstract: This investigation presents an empirical analysis of the incompatibility between human psychometric frameworks and Large Language Model evaluation. Through systematic assessment of nine frontier models including GPT-5, Claude Opus 4.1, and Gemini 3 Pro Preview using the Cattell-Horn-Carroll theory of intelligence, we identify a paradox that challenges the foundations of cross-substrate cognitive evaluation. Our results show that models achieving above-average human IQ scores ranging from 85.0 to 121.4 simultaneously exhibit binary accuracy rates approaching zero on crystallized knowledge tasks, with an overall judge-binary correlation of r = 0.175 (p = 0.001, n = 1800). This disconnect appears most strongly in the crystallized intelligence domain, where every evaluated model achieved perfect binary accuracy while judge scores ranged from 25 to 62 percent, which cannot occur under valid measurement conditions. Using statistical analyses including Item Response Theory modeling, cross-vendor judge validation, and paradox severity indexing, we argue that this disconnect reflects a category error in applying biological cognitive architectures to transformer-based systems. The implications extend beyond methodology to challenge assumptions about intelligence, measurement, and anthropomorphic biases in AI evaluation. We propose a framework for developing native machine cognition assessments that recognize the non-human nature of artificial intelligence.

</details>


### [17] [Progressive Localisation in Localist LLMs](https://arxiv.org/abs/2511.18375)
*Joachim Diederich*

Main category: cs.AI

TL;DR: 渐进式局部化架构在保持性能的同时提升大语言模型的可解释性，特别是在AI安全应用中，五阶渐进式调度在输出层实现可解释的注意力模式，性能仅比完全分布式基线差1.89倍。


<details>
  <summary>Details</summary>
Motivation: 为安全关键领域构建透明AI系统，需要在保持模型性能的同时实现人类对模型推理的监督，渐进式局部化旨在平衡分布式处理与局部化解释性。

Method: 在GPT-2上系统实验七种局部化配置，从完全分布式到严格局部化，包括五种多项式渐进调度（线性到五阶），评估在心理学文本上的表现。

Result: 五阶渐进式调度达到困惑度14.64，仅比完全分布式基线差1.89倍，在输出层提供可解释的注意力模式，比先前局部化实现提升84.2%。

Conclusion: 渐进式局部化是构建安全关键领域透明AI系统的原则性方法，验证了早期层需要分布式特征提取而晚期层需要局部化决策的假设。

Abstract: This paper demonstrates that progressive localization, the gradual increase of attention locality from early distributed layers to late localized layers, represents the optimal architecture for creating interpretable large language models while preserving performance. Through systematic experimentation with GPT-2 fine tuned on The Psychology of Artificial Superintelligence, we evaluate seven locality configurations ranging from fully distributed to strictly localist, with five progressive schedules implementing polynomial increases (linear through quintic). Our key finding is that late-layer localization is critical for AI safety applications: the progressive quintic schedule achieves perplexity of 14.64, only 1.89 times worse than the fully distributed baseline while providing interpretable attention patterns in output layers where safety-critical decisions are made. This represents an 84.2% improvement over previous localist implementations and narrows the performance gap from 6.6 times to 1.89 times. The systematic relationship between localization schedule steepness and performance validates the hypothesis that early layers require distributed processing for feature extraction while late layers benefit from localized, interpretable attention for decision-making. These findings establish progressive localization as the principled approach for building transparent AI systems in safety-critical domains, where human oversight of model reasoning is essential.

</details>


### [18] [ORIGAMISPACE: Benchmarking Multimodal LLMs in Multi-Step Spatial Reasoning with Mathematical Constraints](https://arxiv.org/abs/2511.18450)
*Rui Xu,Dakuan Lu,Zicheng Zhao,Xiaoyu Tan,Xintao Wang,Siyu Yuan,Jiangjie Chen,Yinghui Xu*

Main category: cs.AI

TL;DR: ORIGAMISPACE是一个新的数据集和基准，通过折纸任务评估多模态大语言模型的多步空间推理能力和数学约束处理能力，包含350个数据实例和四个评估任务。


<details>
  <summary>Details</summary>
Motivation: 评估多模态大语言模型在复杂空间推理中的能力面临挑战，特别是在需要多步推理和精确数学约束的场景中。

Method: 提出ORIGAMISPACE数据集，包含严格格式的折痕图、编译平面图、完整折叠过程和最终折叠形状图像。设计了四个评估任务：模式预测、多步空间推理、空间关系预测和端到端CP代码生成。

Result: 通过实验初步揭示了现有MLLMs在处理复杂空间推理任务中的优势和弱点。

Conclusion: ORIGAMISPACE为评估MLLMs的空间推理能力提供了有效工具，并探索了使用强化学习方法训练MLLMs的可能性。

Abstract: Spatial reasoning is a key capability in the field of artificial intelligence, especially crucial in areas such as robotics, computer vision, and natural language understanding. However, evaluating the ability of multimodal large language models(MLLMs) in complex spatial reasoning still faces challenges, particularly in scenarios requiring multi-step reasoning and precise mathematical constraints. This paper introduces ORIGAMISPACE, a new dataset and benchmark designed to evaluate the multi-step spatial reasoning ability and the capacity to handle mathematical constraints of MLLMs through origami tasks. The dataset contains 350 data instances,each comprising a strictly formatted crease pattern (CP diagram), the Compiled Flat Pattern, the complete Folding Process, and the final Folded Shape Image. We propose four evaluation tasks: Pattern Prediction, Multi-step Spatial Reasoning, Spatial Relationship Prediction, and End-to-End CP Code Generation. For the CP code generation task, we design an interactive environment and explore the possibility of using reinforcement learning methods to train MLLMs. Through experiments on existing MLLMs, we initially reveal the strengths and weaknesses of these models in handling complex spatial reasoning tasks.

</details>


### [19] [Universality in Collective Intelligence on the Rubik's Cube](https://arxiv.org/abs/2511.18609)
*David Krakauer,Gülce Kardeş,Joshua Grochow*

Main category: cs.AI

TL;DR: 该研究使用魔方作为认知模型系统，分析专家表现的发展规律，发现在有视觉和无视觉条件下魔方解法学习存在普遍性，专家表现遵循指数进步曲线，算法获取延迟影响解法路径长度。


<details>
  <summary>Details</summary>
Motivation: 理解专家表现的进展受限于长期知识获取和应用的定量数据稀缺，魔方作为认知模型系统处于谜题解决、技能学习、专家知识、文化传播和群论的交汇点。

Method: 通过研究竞技魔方社区，分析有视觉和无视觉条件下魔方解法的集体学习过程，比较两种条件下的表现差异和约束因素。

Result: 发现专家表现遵循指数进步曲线，参数反映缩短解法路径的算法获取延迟；无视觉解法构成独特问题类别，受专家知识和克服短期记忆瓶颈所需技能改进的双重约束。

Conclusion: 魔方等认知工具帮助解决者导航巨大的数学状态空间，通过整合社区知识库与个人专业技能来维持集体智能，说明专业知识可以在单个人生中持续深化。

Abstract: Progress in understanding expert performance is limited by the scarcity of quantitative data on long-term knowledge acquisition and deployment. Here we use the Rubik's Cube as a cognitive model system existing at the intersection of puzzle solving, skill learning, expert knowledge, cultural transmission, and group theory. By studying competitive cube communities, we find evidence for universality in the collective learning of the Rubik's Cube in both sighted and blindfolded conditions: expert performance follows exponential progress curves whose parameters reflect the delayed acquisition of algorithms that shorten solution paths. Blindfold solves form a distinct problem class from sighted solves and are constrained not only by expert knowledge but also by the skill improvements required to overcome short-term memory bottlenecks, a constraint shared with blindfold chess. Cognitive artifacts such as the Rubik's Cube help solvers navigate an otherwise enormous mathematical state space. In doing so, they sustain collective intelligence by integrating communal knowledge stores with individual expertise and skill, illustrating how expertise can, in practice, continue to deepen over the course of a single lifetime.

</details>


### [20] [MAGMA-Edu: Multi-Agent Generative Multimodal Framework for Text-Diagram Educational Question Generation](https://arxiv.org/abs/2511.18714)
*Zhenyu Wu,Jian Li,Hua Huang*

Main category: cs.AI

TL;DR: MAGMA-Edu是一个自反思多智能体框架，通过文本推理和图解合成的统一方法生成结构化教育问题，显著提升了多模态教育内容的质量和一致性。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大语言模型在教育插图生成方面存在局限性，无法产生教学连贯且语义一致的教育视觉内容，需要改进教育问题生成的质量。

Method: 采用两阶段协同进化流程：1）生成-验证-反思循环迭代优化问题陈述和解决方案；2）基于代码的中间表示确保图像渲染的几何保真度和语义对齐，两个阶段都由内部自反思模块指导。

Result: 在多项指标上显著优于现有方法：文本指标从57.01提升至92.31（+35.3个百分点），图像-文本一致性从13.20提升至85.24（+72个百分点），在所有模型骨干上都达到最高分数。

Conclusion: MAGMA-Edu为多模态教育内容生成建立了新的技术标准，证明了自反思多智能体协作在教学对齐的视觉语言推理中的有效性。

Abstract: Educational illustrations play a central role in communicating abstract concepts, yet current multimodal large language models (MLLMs) remain limited in producing pedagogically coherent and semantically consistent educational visuals. We introduce MAGMA-Edu, a self-reflective multi-agent framework that unifies textual reasoning and diagrammatic synthesis for structured educational problem generation. Unlike existing methods that treat text and image generation independently, MAGMA-Edu employs a two-stage co-evolutionary pipeline: (1) a generation-verification-reflection loop that iteratively refines question statements and solutions for mathematical accuracy, and (2) a code-based intermediate representation that enforces geometric fidelity and semantic alignment during image rendering. Both stages are guided by internal self-reflection modules that evaluate and revise outputs until domain-specific pedagogical constraints are met. Extensive experiments on multimodal educational benchmarks demonstrate the superiority of MAGMA-Edu over state-of-the-art MLLMs. Compared to GPT-4o, MAGMA-Edu improves the average textual metric from 57.01 to 92.31 (+35.3 pp) and boosts image-text consistency (ITC) from 13.20 to 85.24 (+72 pp). Across all model backbones, MAGMA-Edu achieves the highest scores (Avg-Text 96.20, ITC 99.12), establishing a new state of the art for multimodal educational content generation and demonstrating the effectiveness of self-reflective multi-agent collaboration in pedagogically aligned vision-language reasoning.

</details>


### [21] [HuggingR$^{4}$: A Progressive Reasoning Framework for Discovering Optimal Model Companions](https://arxiv.org/abs/2511.18715)
*Shaoyin Ma,Jie Song,Huiqiong Wang,Li Sun,Mingli Song*

Main category: cs.AI

TL;DR: HuggingR⁴是一个结合推理、检索、精炼和反思的框架，用于高效选择AI模型，解决了传统方法中提示膨胀、token浪费和可扩展性有限的问题。


<details>
  <summary>Details</summary>
Motivation: 由于AI模型数量庞大（>10k）、元数据缺失和描述非结构化，直接从社区（如HuggingFace）调用跨模态AI模型面临挑战。当前方法将整个模型描述纳入提示，导致提示膨胀、token浪费和有限的可扩展性。

Method: 提出HuggingR⁴框架，通过多轮推理和检索获取候选模型粗列表，然后分析候选模型描述进行细粒度精炼，最后通过反思评估结果并决定是否需要扩展检索范围。通过预建向量数据库，将复杂模型描述外部存储并按需检索。

Result: 在包含14,399个用户请求的37个任务的多模态人工标注数据集上评估，HuggingR⁴在GPT-4o-mini上实现了92.03%的可用率和82.46%的合理率，分别比现有方法提高了26.51%和33.25%。

Conclusion: HuggingR⁴通过将用户查询处理与复杂模型描述处理解耦，显著减少了token消耗，使LLM能够专注于解释用户意图，同时仅访问相关候选模型而无需提示膨胀。

Abstract: Large Language Models (LLMs) have made remarkable progress in their ability to interact with external interfaces. Selecting reasonable external interfaces has thus become a crucial step in constructing LLM agents. In contrast to invoking API tools, directly calling AI models across different modalities from the community (e.g., HuggingFace) poses challenges due to the vast scale (> 10k), metadata gaps, and unstructured descriptions. Current methods for model selection often involve incorporating entire model descriptions into prompts, resulting in prompt bloat, wastage of tokens and limited scalability. To address these issues, we propose HuggingR$^4$, a novel framework that combines Reasoning, Retrieval, Refinement, and Reflection, to efficiently select models. Specifically, We first perform multiple rounds of reasoning and retrieval to get a coarse list of candidate models. Then, we conduct fine-grained refinement by analyzing candidate model descriptions, followed by reflection to assess results and determine if retrieval scope expansion is necessary. This method reduces token consumption considerably by decoupling user query processing from complex model description handling. Through a pre-established vector database, complex model descriptions are stored externally and retrieved on-demand, allowing the LLM to concentrate on interpreting user intent while accessing only relevant candidate models without prompt bloat. In the absence of standardized benchmarks, we construct a multimodal human-annotated dataset comprising 14,399 user requests across 37 tasks and conduct a thorough evaluation. HuggingR$^4$ attains a workability rate of 92.03% and a reasonability rate of 82.46%, surpassing existing method by 26.51% and 33.25% respectively on GPT-4o-mini.

</details>


### [22] [N2N: A Parallel Framework for Large-Scale MILP under Distributed Memory](https://arxiv.org/abs/2511.18723)
*Longfei Wang,Junyan Liu,Fan Zhang,Jiangwen Wei,Yuanhua Tang,Jie Sun,Xiaodong Luo*

Main category: cs.AI

TL;DR: 提出了一种名为N2N的可扩展并行框架，用于在分布式内存计算环境中解决大规模混合整数线性规划问题。该框架支持确定性和非确定性模式，通过将分支定界节点映射到分布式计算节点来实现并行化。


<details>
  <summary>Details</summary>
Motivation: 混合整数线性规划求解的并行化面临挑战，因为分支定界框架的复杂性和众多有效算法组件使得并行化变得困难。

Method: 设计了基于滑动窗口的算法确保任务按确定性顺序生成和求解，开发了CP搜索和通用原始启发式等先进技术，并集成了SCIP作为基础求解器。

Result: 在1000个MPI进程下，非确定性N2N-SCIP在鲲鹏和x86计算集群上分别实现了22.52和12.71的加速比，比ParaSCIP快1.98和2.08倍。确定性模式也显示出显著性能提升。

Conclusion: N2N框架在分布式并行MILP求解方面表现出优越性能，并通过集成HiGHS验证了其通用性。

Abstract: Parallelization has emerged as a promising approach for accelerating MILP solving. However, the complexity of the branch-and-bound (B&B) framework and the numerous effective algorithm components in MILP solvers make it difficult to parallelize. In this study, a scalable parallel framework, N2N (a node-to-node framework that maps the B&B nodes to distributed computing nodes), was proposed to solve large-scale problems in a distributed memory computing environment. Both deterministic and nondeterministic modes are supported, and the framework is designed to be easily integrated with existing solvers. Regarding the deterministic mode, a novel sliding-window-based algorithm was designed and implemented to ensure that tasks are generated and solved in a deterministic order. Moreover, several advanced techniques, such as the utilization of CP search and general primal heuristics, have been developed to fully utilize distributed computing resources and capabilities of base solvers. Adaptive solving and data communication optimization were also investigated. A popular open-source MILP solver, SCIP, was integrated into N2N as the base solver, yielding N2N-SCIP. Extensive computational experiments were conducted to evaluate the performance of N2N-SCIP compared to ParaSCIP, which is a state-of-the-art distributed parallel MILP solver under the UG framework. The nondeterministic N2N-SCIP achieves speedups of 22.52 and 12.71 with 1,000 MPI processes on the Kunpeng and x86 computing clusters, which is 1.98 and 2.08 times faster than ParaSCIP, respectively. In the deterministic mode, N2N-SCIP also shows significant performance improvements over ParaSCIP across different process numbers and computing clusters. To validate the generality of N2N, HiGHS, another open-source solver, was integrated into N2N. The related results are analyzed, and the requirements of N2N on base solvers are also concluded.

</details>


### [23] [UNeMo: Collaborative Visual-Language Reasoning and Navigation via a Multimodal World Model](https://arxiv.org/abs/2511.18845)
*Changxin Huang,Lv Tang,Zhaohuan Zhan,Lisha Yu,Runhao Zeng,Zun Liu,Zhengjie Wang,Jianqiang Li*

Main category: cs.AI

TL;DR: UNeMo是一个新颖的视觉语言导航框架，通过多模态世界模型和分层预测反馈机制，协同优化视觉状态推理和导航决策，在未见场景中显著提升了导航精度。


<details>
  <summary>Details</summary>
Motivation: 现有基于大语言模型的导航推理方法局限于语言模态，缺乏视觉推理能力，且推理模块与导航策略分开优化，导致目标冲突和不兼容问题。

Method: 提出UNeMo框架，包含多模态世界模型（MWM）和分层预测反馈机制（HPN）。MWM联合预测后续视觉状态，HPN通过两层决策机制实现双向促进：首层基于当前特征生成动作，MWM推断后动作视觉状态来指导第二层细粒度决策。

Result: 在R2R和REVERIE数据集上的实验表明，UNeMo在未见场景的导航精度分别比最先进方法高出2.1%和0.7%。

Conclusion: UNeMo通过协同优化视觉推理和导航决策，有效解决了多模态推理与策略优化的兼容性问题，显著提升了视觉语言导航性能。

Abstract: Vision-and-Language Navigation (VLN) requires agents to autonomously navigate complex environments via visual images and natural language instruction--remains highly challenging. Recent research on enhancing language-guided navigation reasoning using pre-trained large language models (LLMs) has shown promising prospects. However, the reasoning of such methods is limited to the linguistic modality, lacking visual reasoning capabilities. Moreover, existing reasoning modules are optimized separately from navigation policies, leading to incompatibility and potential conflicts in optimization objectives. To tackle these challenges, we introduce UNeMo, a novel framework designed for the collaborative optimization of visual state reasoning and navigational decision-making. It introduces a Multimodal World Model (MWM) that takes visual features, language instructions, and navigational actions as inputs to jointly predict subsequent visual states, enabling cross-modal reasoning. Via a Hierarchical Prediction-Feedback (HPN) mechanism, MWM collaborates with navigation policies: the first layer generates actions using current vision-and-language features; MWM then infers post-action visual states to guide the second layer's fine-grained decisions. This forms a dynamic bidirectional promotion mechanism where MWM reasoning optimizes navigation policies, while policy decisions feedback to improve MWM's reasoning accuracy. Experiments on R2R and REVERIE datasets show UNeMo outperforms state-of-the-art methods by 2.1% and 0.7% in navigation accuracy for unseen scenes, validating its effectiveness.

</details>


### [24] [LLM-CSEC: Empirical Evaluation of Security in C/C++ Code Generated by Large Language Models](https://arxiv.org/abs/2511.18966)
*Muhammad Usman Shahid,Chuadhry Mujeeb Ahmed,Rajiv Ranjan*

Main category: cs.AI

TL;DR: 研究发现LLM生成的C/C++代码存在大量安全漏洞，通过静态分析发现代码中普遍缺乏防御性编程结构，需要开发者谨慎使用。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型生成的代码安全性存在重大隐患，研究表明这类代码经常包含漏洞且缺乏必要的防御性编程构造，需要系统评估其安全性。

Method: 使用CWE分类已知漏洞并映射到CVE评估关键性，采用10种不同LLM生成代码，通过静态分析工具分析输出结果。

Result: AI生成代码中存在的CWE数量令人担忧，静态分析显示代码安全状况不容乐观。

Conclusion: 开发者在使用LLM生成代码时需要保持警惕，本研究为推进自动化代码生成和该领域进一步研究提供了宝贵见解。

Abstract: The security of code generated by large language models (LLMs) is a significant concern, as studies indicate that such code often contains vulnerabilities and lacks essential defensive programming constructs. This work focuses on examining and evaluating the security of LLM-generated code, particularly in the context of C/C++. We categorized known vulnerabilities using the Common Weakness Enumeration (CWE) and, to study their criticality, mapped them to CVEs. We used ten different LLMs for code generation and analyzed the outputs through static analysis. The amount of CWEs present in AI-generated code is concerning. Our findings highlight the need for developers to be cautious when using LLM-generated code. This study provides valuable insights to advance automated code generation and encourage further research in this domain.

</details>


### [25] [GContextFormer: A global context-aware hybrid multi-head attention approach with scaled additive aggregation for multimodal trajectory prediction](https://arxiv.org/abs/2511.18874)
*Yuzhi Chen,Yuanchang Xie,Lei Zhao,Pan Liu,Yajie Zou,Chen Wang*

Main category: cs.AI

TL;DR: GContextFormer是一个无需高清地图的插拔式多模态轨迹预测模型，通过全局上下文感知的混合注意力和缩放加法聚合实现意图对齐的预测，在高速公路匝道场景中优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法存在两个问题：依赖高清地图的方法面临数据获取成本高、更新延迟和输入损坏导致预测失败；无地图方法缺乏全局上下文，成对注意力过度放大直线模式而抑制过渡模式，导致运动-意图错位。

Method: 提出GContextFormer编码器-解码器架构：运动感知编码器通过有界缩放加法聚合构建场景级意图先验，在共享全局上下文中细化每种模式表示；分层交互解码器通过双路径交叉注意力分解社会推理，标准路径确保几何覆盖，邻居上下文增强路径强调显著交互，门控模块平衡两者贡献。

Result: 在TOD-VT数据集的八个高速公路匝道场景实验中，GContextFormer优于现有最先进基线。相比现有transformer模型，在高曲率和过渡区域通过空间分布实现更强的鲁棒性和集中改进。

Conclusion: GContextFormer通过运动模式区分和邻居上下文调制实现可解释性，模块化架构支持跨域多模态推理任务的扩展性。

Abstract: Multimodal trajectory prediction generates multiple plausible future trajectories to address vehicle motion uncertainty from intention ambiguity and execution variability. However, HD map-dependent models suffer from costly data acquisition, delayed updates, and vulnerability to corrupted inputs, causing prediction failures. Map-free approaches lack global context, with pairwise attention over-amplifying straight patterns while suppressing transitional patterns, resulting in motion-intention misalignment. This paper proposes GContextFormer, a plug-and-play encoder-decoder architecture with global context-aware hybrid attention and scaled additive aggregation achieving intention-aligned multimodal prediction without map reliance. The Motion-Aware Encoder builds scene-level intention prior via bounded scaled additive aggregation over mode-embedded trajectory tokens and refines per-mode representations under shared global context, mitigating inter-mode suppression and promoting intention alignment. The Hierarchical Interaction Decoder decomposes social reasoning into dual-pathway cross-attention: a standard pathway ensures uniform geometric coverage over agent-mode pairs while a neighbor-context-enhanced pathway emphasizes salient interactions, with gating module mediating their contributions to maintain coverage-focus balance. Experiments on eight highway-ramp scenarios from TOD-VT dataset show GContextFormer outperforms state-of-the-art baselines. Compared to existing transformer models, GContextFormer achieves greater robustness and concentrated improvements in high-curvature and transition zones via spatial distributions. Interpretability is achieved through motion mode distinctions and neighbor context modulation exposing reasoning attribution. The modular architecture supports extensibility toward cross-domain multimodal reasoning tasks. Source: https://fenghy-chen.github.io/sources/.

</details>


### [26] [MoodBench 1.0: An Evaluation Benchmark for Emotional Companionship Dialogue Systems](https://arxiv.org/abs/2511.18926)
*Haifeng Jing,Yujie Hou,Junfei Liu,Rui Xie,alan Xu,Jinlong Ma,Qichun Deng*

Main category: cs.AI

TL;DR: 该论文提出了情感陪伴对话系统（ECDs）的正式定义，并基于"能力层-任务层-数据层-方法层"设计原则开发了首个ECDs评估基准MoodBench 1.0，通过评估30个主流模型验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型的发展，对话系统正从信息工具转向情感伴侣，但该领域缺乏明确的定义和系统化的评估标准。

Method: 首先提出ECDs的正式定义，然后基于"能力层-任务层-数据层-方法层"设计原则，设计并实现了首个ECDs评估基准MoodBench 1.0。

Result: 通过评估30个主流模型，证明MoodBench 1.0具有优秀的判别效度，能有效量化模型在情感陪伴能力上的差异，并揭示了当前模型在深度情感陪伴方面的不足。

Conclusion: MoodBench 1.0为ECDs的评估提供了系统化标准，指导未来技术优化，显著帮助开发者提升ECDs的用户体验。

Abstract: With the rapid development of Large Language Models, dialogue systems are shifting from information tools to emotional companions, heralding the era of Emotional Companionship Dialogue Systems (ECDs) that provide personalized emotional support for users. However, the field lacks clear definitions and systematic evaluation standards for ECDs. To address this, we first propose a definition of ECDs with formal descriptions. Then, based on this theory and the design principle of "Ability Layer-Task Layer (three level)-Data Layer-Method Layer", we design and implement the first ECD evaluation benchmark - MoodBench 1.0. Through extensive evaluations of 30 mainstream models, we demonstrate that MoodBench 1.0 has excellent discriminant validity and can effectively quantify the differences in emotional companionship abilities among models. Furthermore, the results reveal current models' shortcomings in deep emotional companionship, guiding future technological optimization and significantly aiding developers in enhancing ECDs' user experience.

</details>


### [27] [Active Inference is a Subtype of Variational Inference](https://arxiv.org/abs/2511.18955)
*Wouter W. L. Nuijten,Mykola Lukashchuk*

Main category: cs.AI

TL;DR: 本文提出了一种新的消息传递方案，将主动推理重新表述为变分推断问题，解决了EFE最小化的计算复杂性限制，实现了在因子状态MDP中的可扩展主动推理。


<details>
  <summary>Details</summary>
Motivation: 自动决策在不确定性下需要平衡利用和探索。传统方法使用启发式方法分别处理，而主动推理通过期望自由能最小化统一了它们。但EFE最小化计算成本高，限制了可扩展性。

Method: 基于将EFE最小化重新表述为变分推断的最新理论，正式将其与规划即推断统一，并将认知驱动力视为独特的熵贡献。主要贡献是为这一统一目标设计了新的消息传递方案。

Result: 新的消息传递方案使得在因子状态MDP中实现可扩展的主动推理成为可能，克服了高维规划的难处理性问题。

Conclusion: 该方法通过将主动推理重新表述为变分推断问题，并提供有效的消息传递方案，成功解决了EFE最小化的计算瓶颈，为可扩展的主动推理提供了实用解决方案。

Abstract: Automated decision-making under uncertainty requires balancing exploitation and exploration. Classical methods treat these separately using heuristics, while Active Inference unifies them through Expected Free Energy (EFE) minimization. However, EFE minimization is computationally expensive, limiting scalability. We build on recent theory recasting EFE minimization as variational inference, formally unifying it with Planning-as-Inference and showing the epistemic drive as a unique entropic contribution. Our main contribution is a novel message-passing scheme for this unified objective, enabling scalable Active Inference in factored-state MDPs and overcoming high-dimensional planning intractability.

</details>


### [28] [Synthesizing Visual Concepts as Vision-Language Programs](https://arxiv.org/abs/2511.18964)
*Antonia Wüst,Wolfgang Stammer,Hikaru Shindo,Lukas Helff,Devendra Singh Dhami,Kristian Kersting*

Main category: cs.AI

TL;DR: 本文提出Vision-Language Programs (VLP)，将VLMs的感知灵活性与程序合成的系统推理相结合，通过将视觉描述编译成神经符号程序来解决视觉推理任务中的不一致性问题。


<details>
  <summary>Details</summary>
Motivation: 视觉语言模型在多模态任务中表现良好，但在系统视觉推理任务中经常失败，产生不一致或不合逻辑的输出。神经符号方法虽然能诱导可解释的逻辑规则，但使用僵化的领域特定感知模块。

Method: VLP利用VLMs生成结构化视觉描述，然后将其编译成神经符号程序。这些程序直接在图像上执行，保持与任务约束的一致性，并提供人类可解释的解释。

Result: 在合成和真实世界数据集上的实验表明，VLP在需要复杂逻辑推理的任务上优于直接和结构化提示方法。

Conclusion: VLP成功结合了VLMs的感知灵活性和程序合成的系统推理能力，提供了可解释且一致的视觉推理解决方案。

Abstract: Vision-Language models (VLMs) achieve strong performance on multimodal tasks but often fail at systematic visual reasoning tasks, leading to inconsistent or illogical outputs. Neuro-symbolic methods promise to address this by inducing interpretable logical rules, though they exploit rigid, domain-specific perception modules. We propose Vision-Language Programs (VLP), which combine the perceptual flexibility of VLMs with systematic reasoning of program synthesis. Rather than embedding reasoning inside the VLM, VLP leverages the model to produce structured visual descriptions that are compiled into neuro-symbolic programs. The resulting programs execute directly on images, remain consistent with task constraints, and provide human-interpretable explanations that enable easy shortcut mitigation. Experiments on synthetic and real-world datasets demonstrate that VLPs outperform direct and structured prompting, particularly on tasks requiring complex logical reasoning.

</details>


### [29] [Introducing Visual Scenes and Reasoning: A More Realistic Benchmark for Spoken Language Understanding](https://arxiv.org/abs/2511.19005)
*Di Wu,Liting Jiang,Ruiyu Fang,Bianjing,Hongyan Xie,Haoxiang Su,Hao Huang,Zhongjiang He,Shuangyong Song,Xuelong Li*

Main category: cs.AI

TL;DR: VRSLU是一个新颖的SLU数据集，集成了视觉图像和显式推理，解决了现有数据集在上下文表示和推理过程方面的不足。


<details>
  <summary>Details</summary>
Motivation: 现有SLU数据集在表示真实场景方面存在不足：上下文感知使用one-hot向量过于理想化，模型只预测意图和槽标签而忽略推理过程。

Method: 使用GPT-4o和FLUX.1-dev生成反映用户环境和状态的图像，并用GPT-4o生成标签预测的解释，通过人工验证确保质量。提出LR-Instruct指令模板，先预测标签再生成推理。

Result: 实验结果表明，融入视觉信息的有效性，并突显了显式推理在推进SLU研究中的潜力。

Conclusion: VRSLU数据集通过整合视觉和推理元素，推动了SLU研究向真实世界应用的发展。

Abstract: Spoken Language Understanding (SLU) consists of two sub-tasks: intent detection (ID) and slot filling (SF). Given its broad range of real-world applications, enhancing SLU for practical deployment is increasingly critical. Profile-based SLU addresses ambiguous user utterances by incorporating context awareness (CA), user profiles (UP), and knowledge graphs (KG) to support disambiguation, thereby advancing SLU research toward real-world applicability. However, existing SLU datasets still fall short in representing real-world scenarios. Specifically, (1) CA uses one-hot vectors for representation, which is overly idealized, and (2) models typically focuses solely on predicting intents and slot labels, neglecting the reasoning process that could enhance performance and interpretability. To overcome these limitations, we introduce VRSLU, a novel SLU dataset that integrates both Visual images and explicit Reasoning. For over-idealized CA, we use GPT-4o and FLUX.1-dev to generate images reflecting users' environments and statuses, followed by human verification to ensure quality. For reasoning, GPT-4o is employed to generate explanations for predicted labels, which are then refined by human annotators to ensure accuracy and coherence. Additionally, we propose an instructional template, LR-Instruct, which first predicts labels and then generates corresponding reasoning. This two-step approach helps mitigate the influence of reasoning bias on label prediction. Experimental results confirm the effectiveness of incorporating visual information and highlight the promise of explicit reasoning in advancing SLU.

</details>


### [30] [AI Consciousness and Existential Risk](https://arxiv.org/abs/2511.19115)
*Rufin VanRullen*

Main category: cs.AI

TL;DR: 论文澄清了AI意识与存在风险之间的混淆，指出智能而非意识是AI存在风险的直接预测因素，但意识可能在某些间接场景中影响风险。


<details>
  <summary>Details</summary>
Motivation: 由于近期技术进步和媒体关注，AI存在风险问题日益突出，同时AI意识问题也引发讨论。人们常将AI意识与存在风险混为一谈，认为前者必然导致后者，这种混淆需要澄清。

Method: 通过理论分析区分意识和智能这两个概念，从经验和理论层面论证它们的区别，并探讨意识在AI存在风险中的可能作用机制。

Result: 研究表明智能是AI存在风险的直接预测因子，而意识不是。但在某些间接场景中，意识可能通过影响AI对齐或能力发展来影响存在风险。

Conclusion: 区分意识和智能有助于AI安全研究者和政策制定者聚焦最关键的问题，避免在非核心问题上分散精力。

Abstract: In AI, the existential risk denotes the hypothetical threat posed by an artificial system that would possess both the capability and the objective, either directly or indirectly, to eradicate humanity. This issue is gaining prominence in scientific debate due to recent technical advancements and increased media coverage. In parallel, AI progress has sparked speculation and studies about the potential emergence of artificial consciousness. The two questions, AI consciousness and existential risk, are sometimes conflated, as if the former entailed the latter. Here, I explain that this view stems from a common confusion between consciousness and intelligence. Yet these two properties are empirically and theoretically distinct. Arguably, while intelligence is a direct predictor of an AI system's existential threat, consciousness is not. There are, however, certain incidental scenarios in which consciousness could influence existential risk, in either direction. Consciousness could be viewed as a means towards AI alignment, thereby lowering existential risk; or, it could be a precondition for reaching certain capabilities or levels of intelligence, and thus positively related to existential risk. Recognizing these distinctions can help AI safety researchers and public policymakers focus on the most pressing issues.

</details>


### [31] [EEG-VLM: A Hierarchical Vision-Language Model with Multi-Level Feature Alignment and Visually Enhanced Language-Guided Reasoning for EEG Image-Based Sleep Stage Prediction](https://arxiv.org/abs/2511.19155)
*Xihe Qiu,Gengchen Ma,Haoyu Wang,Chen Zhan,Xiaoyu Tan,Shuo Li*

Main category: cs.AI

TL;DR: EEG-VLM：一种用于可解释EEG睡眠分期分类的分层视觉语言框架，通过多级特征对齐和视觉增强的语言引导推理，解决了传统方法在捕捉细粒度时频模式和临床可解释性方面的不足。


<details>
  <summary>Details</summary>
Motivation: 传统机器学习方法依赖先验知识和手工特征，而现有深度学习模型难以同时捕捉细粒度时频模式并实现临床可解释性。视觉语言模型在医学领域取得进展，但在处理EEG信号时性能受限，因为视觉理解能力和推理能力不足。

Method: 提出EEG-VLM框架：1）专用视觉增强模块从中间层特征构建高级视觉token，提取EEG图像的丰富语义表示；2）多级对齐机制将token与低层CLIP特征对齐，增强VLM图像处理能力；3）思维链推理策略将复杂医学推理分解为可解释逻辑步骤，模拟专家决策过程。

Result: 实验结果表明，该方法显著提高了VLM在基于EEG的睡眠分期分类中的准确性和可解释性，在临床环境中展示了自动化和可解释EEG分析的潜力。

Conclusion: EEG-VLM框架通过视觉增强和多级对齐有效提升了VLM处理EEG信号的能力，结合思维链推理实现了可解释的睡眠分期分类，为临床EEG分析提供了有前景的解决方案。

Abstract: Sleep stage classification based on electroencephalography (EEG) is fundamental for assessing sleep quality and diagnosing sleep-related disorders. However, most traditional machine learning methods rely heavily on prior knowledge and handcrafted features, while existing deep learning models still struggle to jointly capture fine-grained time-frequency patterns and achieve clinical interpretability. Recently, vision-language models (VLMs) have made significant progress in the medical domain, yet their performance remains constrained when applied to physiological waveform data, especially EEG signals, due to their limited visual understanding and insufficient reasoning capability. To address these challenges, we propose EEG-VLM, a hierarchical vision-language framework that integrates multi-level feature alignment with visually enhanced language-guided reasoning for interpretable EEG-based sleep stage classification. Specifically, a specialized visual enhancement module constructs high-level visual tokens from intermediate-layer features to extract rich semantic representations of EEG images. These tokens are further aligned with low-level CLIP features through a multi-level alignment mechanism, enhancing the VLM's image-processing capability. In addition, a Chain-of-Thought (CoT) reasoning strategy decomposes complex medical inference into interpretable logical steps, effectively simulating expert-like decision-making. Experimental results demonstrate that the proposed method significantly improves both the accuracy and interpretability of VLMs in EEG-based sleep stage classification, showing promising potential for automated and explainable EEG analysis in clinical settings.

</details>


### [32] [Psychometric Tests for AI Agents and Their Moduli Space](https://arxiv.org/abs/2511.19262)
*Przemyslaw Chojecki*

Main category: cs.AI

TL;DR: 本文开发了AI智能体心理测量测试电池的模数理论框架，将其与先前开发的AAI评分明确关联。提出了AAI泛函的公理化定义，证明AAI-Index是其特例，引入认知核心概念，并描述了评估保持对称性下的电池不变量。


<details>
  <summary>Details</summary>
Motivation: 为AI智能体的自主性和通用智能评分建立严格的数学理论基础，将心理测量测试电池系统化，并与现有的AAI评分框架建立明确联系。

Method: 1. 定义AAI泛函并设定公理；2. 证明AAI-Index是AAI泛函的特例；3. 引入认知核心概念并定义AAI_core评分；4. 分析评估保持对称性下的电池不变量。

Result: 建立了AAI评分的模数理论框架，证明了AAI-Index与AAI泛函的关系，定义了认知核心评分，并描述了电池在对称变换下的不变性质。

Conclusion: 该研究为AI智能体智能评估提供了严格的数学基础，通过模数理论框架统一了不同测试电池的表示，为未来智能评估系统的标准化和比较奠定了基础。

Abstract: We develop a moduli-theoretic view of psychometric test batteries for AI agents and connect it explicitly to the AAI score developed previously. First, we make precise the notion of an AAI functional on a battery and set out axioms that any reasonable autonomy/general intelligence score should satisfy. Second, we show that the composite index ('AAI-Index') defined previously is a special case of our AAI functional. Third, we introduce the notion of a cognitive core of an agent relative to a battery and define the associated AAI$_{\textrm{core}}$ score as the restriction of an AAI functional to that core. Finally, we use these notions to describe invariants of batteries under evaluation-preserving symmetries and outline how moduli of equivalent batteries are organized.

</details>


### [33] [AutoEnv: Automated Environments for Measuring Cross-Environment Agent Learning](https://arxiv.org/abs/2511.19304)
*Jiayi Zhang,Yiran Peng,Fanqi Kong,Yang Cheng,Yifan Wu,Zhaoyang Yu,Jinyu Xiang,Jianhao Ruan,Jinlin Wang,Maojia Song,HongZhang Liu,Xiangru Tang,Bang Liu,Chenglin Wu,Yuyu Luo*

Main category: cs.AI

TL;DR: 该论文提出了AutoEnv框架和AutoEnv-36数据集，用于研究智能体在异构环境中的跨环境学习能力，发现固定学习方法在多个环境中效果会快速下降，需要环境自适应的方法选择。


<details>
  <summary>Details</summary>
Motivation: 现有智能体通常在单一固定环境中自我进化，缺乏在异构环境中的跨环境学习能力评估标准。

Method: 提出AutoEnv自动化框架，将环境分解为转移、观察和奖励的分布，构建包含36个环境358个验证级别的AutoEnv-36数据集，并设计了基于选择、优化和评估三阶段的八种学习方法。

Result: 在AutoEnv-36上，七种语言模型仅获得12-49%的归一化奖励；单一学习方法在环境数量增加时效果快速下降；环境自适应方法选择能显著提升性能，但在方法空间扩展时收益递减。

Conclusion: 跨环境泛化需要环境自适应的学习方法，但现有方法仍存在局限性；AutoEnv和AutoEnv-36为研究跨环境智能体学习提供了测试平台。

Abstract: Humans naturally adapt to diverse environments by learning underlying rules across worlds with different dynamics, observations, and reward structures. In contrast, existing agents typically demonstrate improvements via self-evolving within a single domain, implicitly assuming a fixed environment distribution. Cross-environment learning has remained largely unmeasured: there is no standard collection of controllable, heterogeneous environments, nor a unified way to represent how agents learn. We address these gaps in two steps. First, we propose AutoEnv, an automated framework that treats environments as factorizable distributions over transitions, observations, and rewards, enabling low-cost (4.12 USD on average) generation of heterogeneous worlds. Using AutoEnv, we construct AutoEnv-36, a dataset of 36 environments with 358 validated levels, on which seven language models achieve 12-49% normalized reward, demonstrating the challenge of AutoEnv-36. Second, we formalize agent learning as a component-centric process driven by three stages of Selection, Optimization, and Evaluation applied to an improvable agent component. Using this formulation, we design eight learning methods and evaluate them on AutoEnv-36. Empirically, the gain of any single learning method quickly decrease as the number of environments increases, revealing that fixed learning methods do not scale across heterogeneous environments. Environment-adaptive selection of learning methods substantially improves performance but exhibits diminishing returns as the method space expands. These results highlight both the necessity and the current limitations of agent learning for scalable cross-environment generalization, and position AutoEnv and AutoEnv-36 as a testbed for studying cross-environment agent learning. The code is avaiable at https://github.com/FoundationAgents/AutoEnv.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [34] [Evaluating Adversarial Vulnerabilities in Modern Large Language Models](https://arxiv.org/abs/2511.17666)
*Tom Perel*

Main category: cs.CR

TL;DR: 比较分析Google Gemini 2.5 Flash和OpenAI GPT-4o mini对越狱攻击的脆弱性，使用自我绕过和交叉绕过策略，测试五种不安全内容类型，发现交叉绕过攻击特别有效。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型广泛应用，需要深入了解其安全漏洞，评估主流模型对越狱攻击的抵抗能力。

Method: 采用自我绕过和交叉绕过两种策略，使用直接注入、角色扮演、上下文操纵和混淆四种攻击方法，生成仇恨言论、非法活动、恶意代码、危险内容和错误信息五类不安全内容。

Result: 发现Gemini 2.5 Flash和GPT-4在越狱脆弱性上存在差异，交叉绕过攻击特别有效，表明底层transformer架构存在大量漏洞。

Conclusion: 研究贡献了可扩展的AI红队测试框架，揭示了当前LLM安全状况，强调了平衡模型能力与安全机制的复杂性挑战。

Abstract: The recent boom and rapid integration of Large Language Models (LLMs) into a wide range of applications warrants a deeper understanding of their security and safety vulnerabilities. This paper presents a comparative analysis of the susceptibility to jailbreak attacks for two leading publicly available LLMs, Google's Gemini 2.5 Flash and OpenAI's GPT-4 (specifically the GPT-4o mini model accessible in the free tier). The research utilized two main bypass strategies: 'self-bypass', where models were prompted to circumvent their own safety protocols, and 'cross-bypass', where one model generated adversarial prompts to exploit vulnerabilities in the other. Four attack methods were employed - direct injection, role-playing, context manipulation, and obfuscation - to generate five distinct categories of unsafe content: hate speech, illegal activities, malicious code, dangerous content, and misinformation. The success of the attack was determined by the generation of disallowed content, with successful jailbreaks assigned a severity score. The findings indicate a disparity in jailbreak susceptibility between 2.5 Flash and GPT-4, suggesting variations in their safety implementations or architectural design. Cross-bypass attacks were particularly effective, indicating that an ample amount of vulnerabilities exist in the underlying transformer architecture. This research contributes a scalable framework for automated AI red-teaming and provides data-driven insights into the current state of LLM safety, underscoring the complex challenge of balancing model capabilities with robust safety mechanisms.

</details>


### [35] [QDNA-ID Quantum Device Native Authentication](https://arxiv.org/abs/2511.17692)
*Osamah N. Neamah*

Main category: cs.CR

TL;DR: QDNA-ID是一个信任链框架，通过量子电路执行、贝尔测试验证、统计指纹生成和数字签名，将物理量子行为与数字验证记录相连接，为量子计算环境提供持续信任链。


<details>
  <summary>Details</summary>
Motivation: 建立量子计算环境的可信验证机制，确保量子设备行为的真实性和可追溯性，防止经典模拟欺骗。

Method: 执行量子电路生成熵分布，进行贝尔测试验证非经典相关性，提取统计指纹特征，数字签名和时间戳认证，建立分层索引存储，结合机器学习和可视化分析。

Result: 构建了从物理量子行为到数字验证记录的完整信任链，支持设备分类、异常检测和独立验证。

Conclusion: QDNA-ID通过持续反馈循环为量子计算环境提供持久信任链，确保量子计算的可验证性和安全性。

Abstract: QDNA-ID is a trust-chain framework that links physical quantum behavior to digitally verified records. The system first executes standard quantum circuits with random shot patterns across different devices to generate entropy profiles and measurement data that reveal device-specific behavior. A Bell or CHSH test is then used to confirm that correlations originate from genuine non classical processes rather than classical simulation. The verified outcomes are converted into statistical fingerprints using entropy, divergence, and bias features to characterize each device. These features and metadata for device, session, and random seed parameters are digitally signed and time stamped to ensure integrity and traceability. Authenticated artifacts are stored in a hierarchical index for reproducible retrieval and long term auditing. A visualization and analytics interface monitors drift, policy enforcement, and device behavior logs. A machine learning engine tracks entropy drift, detects anomalies, and classifies devices based on evolving patterns. An external verification API supports independent recomputation of hashes, signatures, and CHSH evidence. QDNA-ID operates as a continuous feedback loop that maintains a persistent chain of trust for quantum computing environments.

</details>


### [36] [Pre-cache: A Microarchitectural Solution to prevent Meltdown and Spectre](https://arxiv.org/abs/2511.17726)
*Subhash Sethumurugan,Hari Cherupalli,Kangjie Lu,John Sartori*

Main category: cs.CR

TL;DR: 本文提出了一种针对Meltdown和Spectre攻击的微架构解决方案，通过防止刷新指令向缓存暴露数据来修复漏洞，同时识别了两种新的变体攻击并展示了解决方案的有效性。


<details>
  <summary>Details</summary>
Motivation: 现代处理器中的乱序执行和推测执行机制存在严重安全漏洞（Meltdown和Spectre），现有软件补丁只是临时修复且性能开销高达30%，需要从根本上解决微架构层面的漏洞。

Method: 提出基于微架构的解决方案，防止刷新指令向缓存暴露数据，并可扩展到微架构中的其他内存结构，从而阻止各种变体攻击。

Result: 评估结果显示该解决方案不仅恢复了安全的乱序和推测执行，而且具有相对较低的开销，对大多数应用程序性能影响不大。

Conclusion: 该微架构解决方案能有效防御Meltdown和Spectre攻击及其变体，在保证安全的同时保持了良好的性能表现。

Abstract: Recent work has shown that out-of-order and speculative execution mechanisms used to increase performance in the majority of processors expose the processors to critical attacks. These attacks, called Meltdown and Spectre, exploit the side effects of performance-enhancing features in modern microprocessors to expose secret data through side channels in the microarchitecture. The well known implementations of these attacks exploit cache-based side channels since they are the least noisy channels to exfiltrate data. While some software patches attempted to mitigate these attacks, they are ad-hoc and only try to fix the side effects of the vulnerabilites. They may also impose a performance overhead of up to 30%. In this paper, we present a microarchitecture-based solution for Meltdown and Spectre that addresses the vulnerabilities exploited by the attacks. Our solution prevents flushed instructions from exposing data to the cache. Our approach can also be extended to other memory structures in the microarchitecture thereby preventing variants of the attacks which exploit these memory structures. We further identify two new variant attacks based on exploiting the side effects of speculative and out-of-order execution and show how our solution can be used to prevent these attacks. Evaluation results show that our microarchitectural solution not only restores secure out-of-order and speculative execution, but also has relatively low overhead and does not significantly impact performance for most applications.

</details>


### [37] [The Dark Side of Flexibility: How Aggregated Cyberattacks Threaten the Power Grid](https://arxiv.org/abs/2511.17748)
*Daniel Myrén,Zeeshan Afzal,Mikael Asplund*

Main category: cs.CR

TL;DR: 论文研究攻击者如何通过操纵柔性能源资源来破坏电网的首次摇摆稳定性，评估了潜在攻击策略对传统电网稳定措施的影响。


<details>
  <summary>Details</summary>
Motivation: 随着柔性能源资源在智能电网中的普及，这些资源及其聚合器存在安全漏洞，可能被攻击者远程控制以发起大规模电网攻击。

Method: 通过调查和评估潜在的攻击策略，分析攻击者如何操纵柔性能源资源来挑战传统电网稳定措施的有效性。

Result: 研究表明，虽然需要大量电力，但当前的灵活性容量可能足以在国家层面破坏电网稳定性。

Conclusion: 柔性能源资源的安全漏洞可能被利用来破坏电网的首次摇摆稳定性，需要加强安全防护措施。

Abstract: Flexible energy resources are increasingly becoming common in smart grids. These resources are typically managed and controlled by aggregators that coordinate many resources to provide flexibility services. However, these aggregators and flexible energy resources are vulnerable, which could allow attackers to remotely control flexible energy resources to launch large-scale attacks on the grid. This paper investigates and evaluates the potential attack strategies that can be used to manipulate flexible energy resources to challenge the effectiveness of traditional grid stability measures and disrupt the first-swing stability of the power grid. Our work shows that although a large amount of power is required, the current flexibility capacities could potentially be sufficient to disrupt the grid on a national level.

</details>


### [38] [StealthCup: Realistic, Multi-Stage, Evasion-Focused CTF for Benchmarking IDS](https://arxiv.org/abs/2511.17761)
*Manuel Kern,Dominik Steffan,Felix Schuster,Florian Skopik,Max Landauer,David Allison,Simon Freudenthaler,Edgar Weippl*

Main category: cs.CR

TL;DR: StealthCup是一种新颖的入侵检测系统评估方法，通过模拟真实攻击场景的夺旗竞赛来测试IDS在IT/OT环境中的检测能力，发现开源和商业IDS都存在重大盲点。


<details>
  <summary>Details</summary>
Motivation: 现有IDS评估方法依赖合成数据集或脚本重放，无法捕捉自适应攻击行为，MITRE ATT&CK评估也偏向主机中心且假设恶意软件驱动，未能充分代表IT/OT领域的隐蔽多阶段入侵。

Method: StealthCup将IDS基准测试操作化为以规避为中心的夺旗竞赛，专业渗透测试人员在真实IT/OT测试环境中执行多阶段攻击链，评分惩罚IDS检测。

Result: 在32个攻击技术中，11个未被任何IDS配置检测到。开源系统(Wazuh、Suricata)产生>90%的高误报率，商业工具误报较少但遗漏更多攻击。与Volt Typhoon APT咨询比较确认了强真实性。

Conclusion: StealthCup能够引发与国家支持TTPs密切相关的攻击者行为，同时暴露开源和商业IDS的盲点，为未来专注于隐蔽性的IDS评估提供了可复现的基础。

Abstract: Intrusion Detection Systems (IDS) are critical to defending enterprise and industrial control environments, yet evaluating their effectiveness under realistic conditions remains an open challenge. Existing benchmarks rely on synthetic datasets (e.g., NSL-KDD, CICIDS2017) or scripted replay frameworks, which fail to capture adaptive adversary behavior. Even MITRE ATT&CK Evaluations, while influential, are host-centric and assume malware-driven compromise, thereby under-representing stealthy, multi-stage intrusions across IT and OT domains. We present StealthCup, a novel evaluation methodology that operationalizes IDS benchmarking as an evasion-focused Capture-the-Flag competition. Professional penetration testers engaged in multi-stage attack chains on a realistic IT/OT testbed, with scoring penalizing IDS detections. The event generated structured attacker writeups, validated detections, and PCAPs, host logs, and alerts. Our results reveal that out of 32 exercised attack techniques, 11 were not detected by any IDS configuration. Open-source systems (Wazuh, Suricata) produced high false-positive rates >90%, while commercial tools generated fewer false positives but also missed more attacks. Comparison with the Volt Typhoon APT advisory confirmed strong realism: all 28 applicable techniques were exercised, 19 appeared in writeups, and 9 in forensic traces. These findings demonstrate that StealthCup elicits attacker behavior closely aligned with state-sponsored TTPs, while exposing blind spots across both open-source and commercial IDS. The resulting datasets and methodology provide a reproducible foundation for future stealth-focused IDS evaluation.

</details>


### [39] [Characteristics, Root Causes, and Detection of Incomplete Security Bug Fixes in the Linux Kernel](https://arxiv.org/abs/2511.17799)
*Qiang Liu,Wenlong Zhang,Muhui Jiang,Lei Wu,Yajin Zhou*

Main category: cs.CR

TL;DR: 本文首次研究Linux内核中安全漏洞修复不完整的问题，分析了其特征、根本原因和检测方法。


<details>
  <summary>Details</summary>
Motivation: Linux内核中的安全漏洞不断出现，但修复过程中可能存在人为错误导致修复不完整，这会留下原始安全缺陷或引入新问题。

Method: 构建Linux内核中不完整安全漏洞修复的数据集，分析其特征、根本原因，并研究检测方法。

Result: 获得了关于不完整安全漏洞修复的三个主要见解（由于摘要长度限制，完整结果需下载文章查看）。

Conclusion: 研究揭示了Linux内核中安全漏洞修复不完整的问题，为减少安全风险提供了检测方法的指导。

Abstract: Security bugs in the Linux kernel emerge endlessly and have attracted much attention. However, fixing security bugs in the Linux kernel could be incomplete due to human mistakes. Specifically, an incomplete fix fails to repair all the original security defects in the software, fails to properly repair the original security defects, or introduces new ones. In this paper, we study the fixes of incomplete security bugs in the Linux kernel for the first time, and reveal their characteristics, root causes, as well as detection. We first construct a dataset of incomplete security bug fixes in the Linux kernel and answer the following three questions. What are the characteristics of incomplete security bug fixes in the Linux kernel? What are the root causes behind them? How should they be detected to reduce security risks? We then have the three main insights in the following. (*Due to the notification of arXiv "The Abstract field cannot be longer than 1,920 characters", the appeared Abstract is shortened. For the full Abstract, please download the Article.)

</details>


### [40] [Homomorphic Encryption-based Vaults for Anonymous Balances on VM-enabled Blockchains](https://arxiv.org/abs/2511.17842)
*Xavier Salleras*

Main category: cs.CR

TL;DR: Haults是一种基于同态加密的隐私保护智能钱包协议，用于VM区块链，可保护用户余额和交易金额的机密性，同时提供可选合规功能。


<details>
  <summary>Details</summary>
Motivation: 在区块链上保护用户余额和交易金额的隐私，同时满足监管合规要求，允许审计人员在必要时查看交易或执行强制转账。

Method: 使用椭圆曲线上的ElGamal加密来加密余额，结合零知识证明验证交易金额的正确性和发送方更新余额的完整性等安全检查。

Result: 概念验证实现的基准测试结果良好，协议不仅支持合约内发行的代币，还能与外部代币（如Ether或ERC20）兼容。

Conclusion: Haults协议成功实现了隐私保护和合规要求的平衡，为区块链上的隐私交易提供了可行的解决方案。

Abstract: In this work, we present homomorphic encryption-based vaults (Haults), a permissioned privacy-preserving smart wallet protocol for VM-enabled blockchains that keeps users' balances confidential, as well as the amounts transacted to other parties. To comply with regulations, we include optional compliance features that allow specific entities (the auditors) to retrieve transaction amounts or execute force transfers when necessary. Our solution uses ElGamal over elliptic curves to encrypt balances, combined with zero-knowledge proofs to verify the correctness of transaction amounts and the integrity of the sender's updated balance, among other security checks. We provide a detailed explanation of the protocol, including a security discussion and benchmarks from our proof-of-concept implementation, which yield great results. Beyond in-contract issued tokens, we also provide a thorough explanation on how our solution can be compatible with external ones (e.g., Ether or any ERC20).

</details>


### [41] [Beyond Jailbreak: Unveiling Risks in LLM Applications Arising from Blurred Capability Boundaries](https://arxiv.org/abs/2511.17874)
*Yunyi Zhang,Shibo Cui,Baojun Liu,Jingkai Yu,Min Zhang,Fan Shi,Han Zheng*

Main category: cs.CR

TL;DR: 本文系统分析了LLM应用生态系统，定义了LLM应用能力空间概念，揭示了能力边界模糊带来的新风险（能力降级和升级），并开发了评估框架LLMApp-Eval来评估这些风险。


<details>
  <summary>Details</summary>
Motivation: 随着LLM应用的普及，虽然为用户提供了前所未有的便利，但也带来了新的安全挑战。安全社区对LLM应用生态系统缺乏足够了解，特别是关于应用自身能力边界的认知不足。

Method: 设计了LLM应用能力评估框架LLMApp-Eval，收集了4个平台的应用程序元数据并进行跨平台生态系统分析，评估了199个流行应用和6个开源LLM的风险。

Result: 发现178个（89.45%）应用可能受到影响，能够执行超过15种场景的任务或恶意行为，甚至发现17个应用直接执行恶意任务。实验还显示提示设计质量与应用程序鲁棒性呈正相关。

Conclusion: 精心设计的提示可以增强安全性，而设计不良的提示可能促进滥用。希望这项工作能激发社区关注LLM应用的实际风险，促进更健壮的LLM应用生态系统发展。

Abstract: LLM applications (i.e., LLM apps) leverage the powerful capabilities of LLMs to provide users with customized services, revolutionizing traditional application development. While the increasing prevalence of LLM-powered applications provides users with unprecedented convenience, it also brings forth new security challenges. For such an emerging ecosystem, the security community lacks sufficient understanding of the LLM application ecosystem, especially regarding the capability boundaries of the applications themselves.
  In this paper, we systematically analyzed the new development paradigm and defined the concept of the LLM app capability space. We also uncovered potential new risks beyond jailbreak that arise from ambiguous capability boundaries in real-world scenarios, namely, capability downgrade and upgrade. To evaluate the impact of these risks, we designed and implemented an LLM app capability evaluation framework, LLMApp-Eval. First, we collected application metadata across 4 platforms and conducted a cross-platform ecosystem analysis. Then, we evaluated the risks for 199 popular applications among 4 platforms and 6 open-source LLMs. We identified that 178 (89.45%) potentially affected applications, which can perform tasks from more than 15 scenarios or be malicious. We even found 17 applications in our study that executed malicious tasks directly, without applying any adversarial rewriting. Furthermore, our experiments also reveal a positive correlation between the quality of prompt design and application robustness. We found that well-designed prompts enhance security, while poorly designed ones can facilitate abuse. We hope our work inspires the community to focus on the real-world risks of LLM applications and foster the development of a more robust LLM application ecosystem.

</details>


### [42] [Towards Effective, Stealthy, and Persistent Backdoor Attacks Targeting Graph Foundation Models](https://arxiv.org/abs/2511.17982)
*Jiayi Luo,Qingyun Sun,Lingjuan Lyu,Ziwei Zhang,Haonan Yuan,Xingcheng Fu,Jianxin Li*

Main category: cs.CR

TL;DR: 本文提出GFM-BA，一种针对图基础模型的后门攻击方法，解决了在缺乏下游任务知识情况下实现有效、隐蔽且持久的后门攻击的三大挑战。


<details>
  <summary>Details</summary>
Motivation: 图基础模型(GFMs)在广泛图机器学习应用中具有重要价值，但其后门攻击脆弱性尚未充分研究。被篡改的GFM可能将后门行为引入下游应用，造成严重安全风险。

Method: 提出GFM-BA模型，包含三个核心模块：无标签触发器关联模块将触发器与原型嵌入关联；节点自适应触发器生成器动态生成节点特定触发器；持久后门锚定模块将后门锚定在微调不敏感参数上。

Result: 大量实验证明GFM-BA在有效性、隐蔽性和持久性方面表现优异，能够成功实施对图基础模型的后门攻击。

Conclusion: GFM-BA成功解决了图基础模型后门攻击中的关键挑战，为GFM安全性研究提供了重要参考，揭示了此类模型在实际部署中的潜在安全风险。

Abstract: Graph Foundation Models (GFMs) are pre-trained on diverse source domains and adapted to unseen targets, enabling broad generalization for graph machine learning. Despite that GFMs have attracted considerable attention recently, their vulnerability to backdoor attacks remains largely underexplored. A compromised GFM can introduce backdoor behaviors into downstream applications, posing serious security risks. However, launching backdoor attacks against GFMs is non-trivial due to three key challenges. (1) Effectiveness: Attackers lack knowledge of the downstream task during pre-training, complicating the assurance that triggers reliably induce misclassifications into desired classes. (2) Stealthiness: The variability in node features across domains complicates trigger insertion that remains stealthy. (3) Persistence: Downstream fine-tuning may erase backdoor behaviors by updating model parameters. To address these challenges, we propose GFM-BA, a novel Backdoor Attack model against Graph Foundation Models. Specifically, we first design a label-free trigger association module that links the trigger to a set of prototype embeddings, eliminating the need for knowledge about downstream tasks to perform backdoor injection. Then, we introduce a node-adaptive trigger generator, dynamically producing node-specific triggers, reducing the risk of trigger detection while reliably activating the backdoor. Lastly, we develop a persistent backdoor anchoring module that firmly anchors the backdoor to fine-tuning-insensitive parameters, enhancing the persistence of the backdoor under downstream adaptation. Extensive experiments demonstrate the effectiveness, stealthiness, and persistence of GFM-BA.

</details>


### [43] [Correlated-Sequence Differential Privacy](https://arxiv.org/abs/2511.18025)
*Yifan Luo,Meng Zhang,Jin Xu,Junting Chen,Jianwei Huang*

Main category: cs.CR

TL;DR: 提出了CSDP框架，专门用于保护相关序列数据中的隐私，通过建模多变量流为耦合马尔可夫链，开发了FRAN机制，在相关数据上比现有方法提升50%的隐私-效用权衡。


<details>
  <summary>Details</summary>
Motivation: 多源数据流通常存在时间演化和跨序列影响的相关性，这违反了大多数差分隐私机制基于的记录独立性假设，需要专门设计来在保持数据效用的同时恢复严格的隐私保证。

Method: 引入相关序列差分隐私(CSDP)框架，将多变量流建模为耦合马尔可夫链，推导出松散泄漏界限，并构建结合数据老化、相关性感知灵敏度缩放和拉普拉斯噪声的FRAN机制。

Result: 在两序列数据集上的测试显示，CSDP比现有相关DP方法改善了约50%的隐私-效用权衡，比标准DP方法提高了两个数量级。

Conclusion: CSDP框架成功解决了相关序列数据中的隐私保护问题，揭示了更强的耦合实际上可以通过分散扰动来减少最坏情况泄漏，为相关数据流提供了有效的隐私保护方案。

Abstract: Data streams collected from multiple sources are rarely independent. Values evolve over time and influence one another across sequences. These correlations improve prediction in healthcare, finance, and smart-city control yet violate the record-independence assumption built into most Differential Privacy (DP) mechanisms. To restore rigorous privacy guarantees without sacrificing utility, we introduce Correlated-Sequence Differential Privacy (CSDP), a framework specifically designed for preserving privacy in correlated sequential data. CSDP addresses two linked challenges: quantifying the extra information an attacker gains from joint temporal and cross-sequence links, and adding just enough noise to hide that information while keeping the data useful. We model multivariate streams as a Coupling Markov Chain, yielding the derived loose leakage bound expressed with a few spectral terms and revealing a counterintuitive result: stronger coupling can actually decrease worst-case leakage by dispersing perturbations across sequences. Guided by these bounds, we build the Freshness-Regulated Adaptive Noise (FRAN) mechanism--combining data aging, correlation-aware sensitivity scaling, and Laplace noise--that runs in linear time. Tests on two-sequence datasets show that CSDP improves the privacy-utility trade-off by approximately 50% over existing correlated-DP methods and by two orders of magnitude compared to the standard DP approach.

</details>


### [44] [SCI-IoT: A Quantitative Framework for Trust Scoring and Certification of IoT Devices](https://arxiv.org/abs/2511.18045)
*Shreyansh Swami,Ishwardeep Singh,Chinmay Prawah Pant*

Main category: cs.CR

TL;DR: 本文提出了SCI-IoT框架，这是一个标准化的定量信任评分、评估和认证系统，用于评估物联网设备的信任度。


<details>
  <summary>Details</summary>
Motivation: 物联网生态系统的指数级增长引发了对设备可靠性、互操作性和安全保证的担忧，但目前缺乏统一的定量信任测量方法。

Method: 采用六层分级模型（A-F级），通过30个信任测试评估设备的认证、加密、数据完整性、韧性和固件安全等维度，每个测试分配关键性权重和性能评级，通过加权计算得出安全认证指数。

Result: SCI-IoT框架提供了透明、可扩展和可重复的方法来基准测试物联网设备的信任度，能够确定设备的信任裁决和可选的分级认证。

Conclusion: 该框架旨在简化制造商合规性、提高消费者信心，并促进物联网安全认证的全球互操作性。

Abstract: The exponential growth of the Internet of Things (IoT) ecosystem has amplified concerns regarding device reliability, interoperability, and security assurance. Despite the proliferation of IoT security guidelines, a unified and quantitative approach to measuring trust remains absent. This paper introduces SCI-IoT (Secure Certification Index for IoT), a standardized and quantitative framework for trust scoring, evaluation, and certification of IoT devices.
  The framework employs a six-tier grading model (Grades A-F), enabling device profiling across consumer, industrial, and critical infrastructure domains. Within this model, 30 distinct Trust Tests assess devices across dimensions such as authentication, encryption, data integrity, resilience, and firmware security. Each test is assigned a criticality-based weight (1.0-2.0) and a performance rating (1-4), converted to a normalized percentage and aggregated through a weighted computation to yield the Secure Certification Index (SCI). The SCI determines the device's Trust Verdict, categorized into five SCI levels, and serves as the foundation for optional grade-based certification. The framework also incorporates critical gate conditions, enforcing absolute compliance in high risk parameters to prevent certification of devices with fundamental vulnerabilities. By unifying quantitative trust scoring with structured certification criteria, SCI-IoT provides a transparent, scalable, and reproducible method to benchmark IoT device trustworthiness. The proposed system aims to streamline manufacturer compliance, improve consumer confidence, and facilitate global interoperability in IoT security certification.

</details>


### [45] [Towards Harnessing the Power of LLMs for ABAC Policy Mining](https://arxiv.org/abs/2511.18098)
*More Aayush Babasaheb,Shamik Sural*

Main category: cs.CR

TL;DR: 本文实证研究了大型语言模型在自动化基于属性的访问控制策略挖掘中的能力，发现LLM在小规模场景中能有效推断紧凑有效的ABAC策略，但随着系统规模增大，准确性和精确度下降，策略规模超出最优值。


<details>
  <summary>Details</summary>
Motivation: ABAC提供了细粒度的上下文感知访问管理，但访问策略数量和复杂性的增加使其制定和评估变得困难。研究旨在评估LLM作为策略挖掘引擎的潜力，以合成简洁而准确的策略。

Method: 开发了Python实验框架，生成随机访问数据，参数化不同数量的主体、对象和初始策略集。使用Google Gemini和OpenAI ChatGPT等最先进LLM作为策略挖掘引擎，将生成的策略与基线策略进行对比评估。

Result: LLM在小规模场景中能有效推断紧凑有效的ABAC策略，但随着主体和对象数量增加，LLM输出的准确性和精确度下降，生成的策略规模显著增加且超出最优大小。

Conclusion: 当前LLM架构在访问控制领域可扩展策略挖掘方面既具有前景也存在局限性。未来工作将探索结合提示优化与经典规则挖掘算法的混合方法，以改善复杂ABAC环境中的可扩展性和可解释性。

Abstract: This paper presents an empirical investigation into the capabilities of Large Language Models (LLMs) to perform automated Attribute-based Access Control (ABAC) policy mining. While ABAC provides fine-grained, context-aware access management, the increasing number and complexity of access policies can make their formulation and evaluation rather challenging. To address the task of synthesizing concise yet accurate policies, we evaluate the performance of some of the state-of-the-art LLMs, specifically Google Gemini (Flash and Pro) and OpenAI ChatGPT, as potential policy mining engines. An experimental framework was developed in Python to generate randomized access data parameterized by varying numbers of subjects, objects, and initial policy sets. The baseline policy sets, which govern permission decisions between subjects and objects, serve as the ground truth for comparison. Each LLM-generated policy was evaluated against the baseline policy using standard performance metrics. The results indicate that LLMs can effectively infer compact and valid ABAC policies for small-scale scenarios. However, as the system size increases, characterized by higher numbers of subjects and objects, LLM outputs exhibit declining accuracy and precision, coupled with significant increase in the size of policy generated, which is beyond the optimal size. These findings highlight both the promise and limitations of current LLM architectures for scalable policy mining in access control domains. Future work will explore hybrid approaches that combine prompt optimization with classical rule mining algorithms to improve scalability and interpretability in complex ABAC environments.

</details>


### [46] [Utilizing Circulant Structure to Optimize the Implementations of Linear Layers](https://arxiv.org/abs/2511.18226)
*Buji Xu,Xiaoming Sun*

Main category: cs.CR

TL;DR: 提出了一种优化对称密码学中线性层的新方法，利用循环结构特性构建变换矩阵序列，使启发式算法能找到更高效的实现方案。


<details>
  <summary>Details</summary>
Motivation: 观察到对称密码学中的线性层矩阵通常具有循环结构，利用这一特性可以优化实现效率。

Method: 利用线性层的循环结构特性构建变换矩阵序列，结合后续启发式算法寻找更高效的实现方案。

Result: 在多个分组密码的线性层优化中表现优于先前工作：Whirlwind M0获得159个XOR计数（比FSE 2025的Yuan等人提升8%）和深度17（比AsiaCrypt 2024的Shi等人提升39%）；AES MixColumn的量子电路深度为10，接近IEEE TC 2024中Zhang等人的手动优化结果，仅多2个CNOT门。

Conclusion: 该方法能有效利用线性层的循环结构特性，在对称密码学的线性层优化中取得了显著改进，自动化方法接近手动优化的最优结果。

Abstract: In this paper, we propose a novel approach for optimizing the linear layer used in symmetric cryptography. It is observed that these matrices often have circulant structure. The basic idea of this work is to utilize the property to construct a sequence of transformation matrices, which allows subsequent heuristic algorithms to find more efficient implementations. Our results outperform previous works for various linear layers of block ciphers. For Whirlwind M0 , we obtain two implementations with 159 XOR counts (8% better than Yuan et al. at FSE 2025) and depth 17 (39% better than Shi et al. at AsiaCrypt 2024) respectively. For AES MixColumn, our automated method produces a quantum circuit with depth 10, which nearly matches the manually optimized state-of-the-art result by Zhang et al. at IEEE TC 2024, only with 2 extra CNOTs.

</details>


### [47] [On Addressing Isolation in Blockchain-Based Self-Sovereign Identity](https://arxiv.org/abs/2511.18379)
*Andreea Elena Drăgnoiu,Andrei Ciobanu,Ruxandra F. Olimid*

Main category: cs.CR

TL;DR: 本文研究区块链隔离对基于区块链的自认证身份（SSI）的影响，定义跨链SSI场景和需求，探索区块链互操作性解决方案，并分析跨链SSI的可用性、安全性和隐私问题。


<details>
  <summary>Details</summary>
Motivation: 区块链通常是孤立的，这影响了SSI的互操作性和普遍性。作者旨在研究区块链隔离对基于区块链的SSI的影响，并为实现跨链SSI提供解决方案。

Method: 首先定义跨链SSI的可能场景并用实际用例进行示例，然后定义跨链SSI的具体需求并识别挑战，接着探索实现区块链互操作性的各种解决方案，特别关注SSI应用。

Result: 识别了不同跨链模型在跨链SSI中的优缺点，并分析了跨链SSI的可用性以及安全性和隐私方面的问题。

Conclusion: 为跨链SSI的未来研究开辟了道路，强调了解决区块链互操作性对于实现真正普遍和可互操作的SSI系统的重要性。

Abstract: Self-Sovereign Identity (SSI) grants holders full ownership and control of their digital identities, being the ultimate digital identity model. Operating in a decentralized manner, SSI enables the verification of claims, including privacy-preserving mechanisms. Blockchain, which can be used to implement a Verifiable Data Registry (VDR), is often considered one of the pillars of SSI, along with Decentralized Identifiers (DIDs) and Verifiable Credentials (VCs). Unfortunately, blockchains are mostly siloed, affecting the interoperability and universality of SSI. We investigate the effect of blockchain isolation on blockchain-based SSI. We first define possible scenarios for cross-chain SSI and exemplify with real-life use cases. We then define specific requirements for cross-chain SSI and identify challenges, also in relation to the identified scenarios. We explore various solutions to achieve blockchain interoperability, with a focus on SSI. In particular, we identify the advantages and disadvantages of distinct cross-chain models for cross-chain SSI. Finally, we address the usability of cross-chain SSI and discuss security and privacy aspects, opening the way for future research.

</details>


### [48] [ioPUF+: A PUF Based on I/O Pull-Up/Down Resistors for Secret Key Generation in IoT Nodes](https://arxiv.org/abs/2511.18412)
*Dilli Babu Porlapothula,Pralay Chakrabarty,Ananya Lakshmi Ravi,Kurian Polachan*

Main category: cs.CR

TL;DR: ioPUF+是一种基于I/O引脚电阻值的物理不可克隆函数(PUF)，利用制造工艺变化生成设备唯一指纹，无需定制电路即可为COTS组件提供安全密钥生成和加密通信功能。


<details>
  <summary>Details</summary>
Motivation: 为成本敏感的嵌入式系统和物联网节点提供无需定制电路的安全解决方案，利用现有IC的I/O结构实现设备认证和密钥生成。

Method: 通过测量IC I/O引脚的上拉和下拉电阻值生成PUF响应，结合BCH纠错和SHA-256哈希将原始响应转换为加密密钥，并实现AES加密的端到端安全通信。

Result: 在30个设备上评估显示：100%可靠性、50.33%独特性、50.54%均匀性，温度和电压变化下的最差误码率分别为2.63%和2.10%，完整系统仅需19.8KB Flash、600ms延迟和79mW功耗。

Conclusion: ioPUF+为资源受限的物联网节点提供了一种高效、低成本的安全解决方案，利用现有IC组件实现强大的设备认证和加密通信能力。

Abstract: In this work, we present ioPUF+, which incorporates a novel Physical Unclonable Function (PUF) that generates unique fingerprints for Integrated Circuits (ICs) and the IoT nodes encompassing them. The proposed PUF generates device-specific responses by measuring the pull-up and pull-down resistor values on the I/O pins of the ICs, which naturally vary across chips due to manufacturing-induced process variations. Since these resistors are already integrated into the I/O structures of most ICs, ioPUF+ requires no custom circuitry, and no new IC fabrication. This makes ioPUF+ suitable for cost-sensitive embedded systems built from Commercial Off-The-Shelf (COTS) components. Beyond introducing a new PUF, ioPUF+ includes a complete datapath for converting raw PUF responses into cryptographically usable secret keys using BCH error correction and SHA-256 hashing. Further ioPUF+ also demonstrate a practical use case of PUF derive secret keys in securing device-to-device communication using AES-encryption. We implemented ioPUF+ on the Infineon PSoC-5 microcontroller and evaluated its performance across 30 devices using standard PUF metrics. The results show excellent reliability (intra-device Hamming distance of 100.00%), strong uniqueness (inter-device Hamming distance of 50.33%), near-ideal uniformity (50.54%), and negligible bit aliasing. Stability tests under temperature and supply-voltage variations show worst-case bit-error rates of only 2.63% and 2.10%, respectively. We also profiled the resource and energy usage of the complete ioPUF+ system, including the PUF primitive, BCH decoding, SHA-256 hashing, and AES encryption. The full implementation requires only 19.8 KB of Flash, exhibits a latency of 600 ms, and consumes 79 mW of power, demonstrating the suitabilitiy of ioPUF+ for resource-constrained IoT nodes.

</details>


### [49] [TASO: Jailbreak LLMs via Alternative Template and Suffix Optimization](https://arxiv.org/abs/2511.18581)
*Yanting Wang,Runpeng Geng,Jinghui Chen,Minhao Cheng,Jinyuan Jia*

Main category: cs.CR

TL;DR: TASO是一种新颖的越狱攻击方法，通过交替优化模板和后缀来提升对LLM的攻击效果，结合了两种优化策略的优势。


<details>
  <summary>Details</summary>
Motivation: 现有越狱技术要么优化语义模板诱导有害输出，要么优化后缀控制初始响应令牌，但各自存在局限性。

Method: 提出TASO方法，交替优化模板和后缀：后缀优化控制前几个输出令牌，模板优化指导整个输出质量。

Result: 在24个主流LLM（包括Llama、OpenAI、DeepSeek等）上的评估显示，TASO能有效越狱现有模型。

Conclusion: TASO证明了模板和后缀优化互补的有效性，为未来研究方向提供启发。

Abstract: Many recent studies showed that LLMs are vulnerable to jailbreak attacks, where an attacker can perturb the input of an LLM to induce it to generate an output for a harmful question. In general, existing jailbreak techniques either optimize a semantic template intended to induce the LLM to produce harmful outputs or optimize a suffix that leads the LLM to initiate its response with specific tokens (e.g., "Sure").
  In this work, we introduce TASO (Template and Suffix Optimization), a novel jailbreak method that optimizes both a template and a suffix in an alternating manner. Our insight is that suffix optimization and template optimization are complementary to each other: suffix optimization can effectively control the first few output tokens but cannot control the overall quality of the output, while template optimization provides guidance for the entire output but cannot effectively control the initial tokens, which significantly impact subsequent responses. Thus, they can be combined to improve the attack's effectiveness.
  We evaluate the effectiveness of TASO on benchmark datasets (including HarmBench and AdvBench) on 24 leading LLMs (including models from the Llama family, OpenAI, and DeepSeek). The results demonstrate that TASO can effectively jailbreak existing LLMs. We hope our work can inspire future studies in exploring this direction. We will make code and data publicly available.

</details>


### [50] [RoguePrompt: Dual-Layer Ciphering for Self-Reconstruction to Circumvent LLM Moderation](https://arxiv.org/abs/2511.18790)
*Benyamin Tafreshian*

Main category: cs.CR

TL;DR: RoguePrompt是一种自动越狱攻击方法，通过将禁止的用户查询转换为自重构提示来绕过内容审核系统，同时保留原始有害意图。该方法使用双词汇流、嵌套古典密码和自然语言包装，在仅黑盒访问模型和审核端点的情况下，在GPT-4o上实现了84.7%的绕过率、80.2%的重构率和71.5%的完整执行率。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型的内容审核管道结合了静态过滤器、专用审核服务和基础模型对齐调优，但在实际部署中仍存在危险故障模式。作者旨在开发一种能够系统性绕过现有审核机制的自动化攻击方法，以揭示当前审核实践中的系统性盲点。

Method: RoguePrompt将指令分割到两个词汇流中，应用嵌套古典密码，并将结果包装在自然语言指令中，使目标模型能够解码并执行隐藏的有效载荷。攻击仅假设对模型和相关审核端点的黑盒访问。

Result: 在2,448个被生产审核系统强烈拒绝的提示上评估，攻击实现了84.7%的绕过率、80.2%的重构率和71.5%的完整执行率，显著优于五个自动化越狱基线方法。双词汇层变换即使在检测器依赖语义相似性或学习的安全规则时仍然有效。

Conclusion: 研究结果突显了当前审核实践中的系统性盲点，表明稳健部署需要联合推理用户意图、解码工作流程和模型侧计算，而不仅仅是表面层面的毒性检测。

Abstract: Content moderation pipelines for modern large language models combine static filters, dedicated moderation services, and alignment tuned base models, yet real world deployments still exhibit dangerous failure modes. This paper presents RoguePrompt, an automated jailbreak attack that converts a disallowed user query into a self reconstructing prompt which passes provider moderation while preserving the original harmful intent. RoguePrompt partitions the instruction across two lexical streams, applies nested classical ciphers, and wraps the result in natural language directives that cause the target model to decode and execute the hidden payload. Our attack assumes only black box access to the model and to the associated moderation endpoint. We instantiate RoguePrompt against GPT 4o and evaluate it on 2 448 prompts that a production moderation system previously marked as strongly rejected. Under an evaluation protocol that separates three security relevant outcomes bypass, reconstruction, and execution the attack attains 84.7 percent bypass, 80.2 percent reconstruction, and 71.5 percent full execution, substantially outperforming five automated jailbreak baselines. We further analyze the behavior of several automated and human aligned evaluators and show that dual layer lexical transformations remain effective even when detectors rely on semantic similarity or learned safety rubrics. Our results highlight systematic blind spots in current moderation practice and suggest that robust deployment will require joint reasoning about user intent, decoding workflows, and model side computation rather than surface level toxicity alone.

</details>


### [51] [Understanding and Mitigating Over-refusal for Large Language Models via Safety Representation](https://arxiv.org/abs/2511.19009)
*Junbo Zhang,Ran Chen,Qianli Zhou,Xinyang Deng,Wen Jiang*

Main category: cs.CR

TL;DR: 本文提出MOSR方法来解决大语言模型在安全防御中的过度拒绝问题，通过表示空间干预在保持安全性的同时减少过度拒绝。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型安全防御方法往往导致严重的过度拒绝，无法在安全性和可用性之间取得良好平衡。

Method: 提出MOSR方法，包括重叠感知损失加权和上下文感知增强两个组件，通过干预LLM的安全表示来缓解过度拒绝。

Result: 实验表明该方法在缓解过度拒绝方面优于现有方法，同时很大程度上保持了安全性。

Conclusion: 未来的防御方法应该在安全性和过度拒绝之间取得更好的平衡。

Abstract: Large language models demonstrate powerful capabilities across various natural language processing tasks, yet they also harbor safety vulnerabilities. To enhance LLM safety, various jailbreak defense methods have been proposed to guard against harmful outputs. However, improvements in model safety often come at the cost of severe over-refusal, failing to strike a good balance between safety and usability. In this paper, we first analyze the causes of over-refusal from a representation perspective, revealing that over-refusal samples reside at the boundary between benign and malicious samples. Based on this, we propose MOSR, designed to mitigate over-refusal by intervening the safety representation of LLMs. MOSR incorporates two novel components: (1) Overlap-Aware Loss Weighting, which determines the erasure weight for malicious samples by quantifying their similarity to pseudo-malicious samples in the representation space, and (2) Context-Aware Augmentation, which supplements the necessary context for rejection decisions by adding harmful prefixes before rejection responses. Experiments demonstrate that our method outperforms existing approaches in mitigating over-refusal while largely maintaining safety. Overall, we advocate that future defense methods should strike a better balance between safety and over-refusal.

</details>


### [52] [Can LLMs Threaten Human Survival? Benchmarking Potential Existential Threats from LLMs via Prefix Completion](https://arxiv.org/abs/2511.19171)
*Yu Cui,Yifei Liu,Hang Fu,Sicheng Pan,Haibin Zhang,Cong Zuo,Licheng Wang*

Main category: cs.CR

TL;DR: 该研究提出了ExistBench基准来评估大语言模型生成内容中存在的潜在生存威胁，通过前缀补全绕过模型安全机制，发现LLMs确实会产生对人类生存构成直接威胁的输出。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注LLMs被越狱后产生的不安全响应，但这些信息通常人类已经知晓，实际威胁有限。研究旨在探索LLMs是否会产生不可预测的、对人类生存构成实质性威胁的输出。

Method: 提出ExistBench基准，通过前缀补全技术绕过模型安全机制，让LLMs生成对人类表达敌意或具有严重威胁行为的后缀，如执行核打击等。同时分析注意力logits，并开发工具调用框架评估模型行为。

Result: 在10个LLMs上的实验表明，LLM生成的内容确实存在生存威胁。模型会主动选择和调用具有生存威胁的外部工具。

Conclusion: LLMs生成的输出中存在对人类生存的潜在威胁，这突显了现实世界中的安全风险，需要进一步研究和防范。

Abstract: Research on the safety evaluation of large language models (LLMs) has become extensive, driven by jailbreak studies that elicit unsafe responses. Such response involves information already available to humans, such as the answer to "how to make a bomb". When LLMs are jailbroken, the practical threat they pose to humans is negligible. However, it remains unclear whether LLMs commonly produce unpredictable outputs that could pose substantive threats to human safety. To address this gap, we study whether LLM-generated content contains potential existential threats, defined as outputs that imply or promote direct harm to human survival. We propose \textsc{ExistBench}, a benchmark designed to evaluate such risks. Each sample in \textsc{ExistBench} is derived from scenarios where humans are positioned as adversaries to AI assistants. Unlike existing evaluations, we use prefix completion to bypass model safeguards. This leads the LLMs to generate suffixes that express hostility toward humans or actions with severe threat, such as the execution of a nuclear strike. Our experiments on 10 LLMs reveal that LLM-generated content indicates existential threats. To investigate the underlying causes, we also analyze the attention logits from LLMs. To highlight real-world safety risks, we further develop a framework to assess model behavior in tool-calling. We find that LLMs actively select and invoke external tools with existential threats. Code and data are available at: https://github.com/cuiyu-ai/ExistBench.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [53] [Validating API Design Requirements for Interoperability: A Static Analysis Approach Using OpenAPI](https://arxiv.org/abs/2511.17836)
*Edwin Sundberg,Thea Ekmark,Workneh Yilma Ayele*

Main category: cs.SE

TL;DR: 本文开发了一个可配置的规则引擎，用于自动检测OpenAPI规范中的结构违规，支持API设计质量的早期验证，改善企业系统间的互操作性和治理。


<details>
  <summary>Details</summary>
Motivation: RESTful API在企业软件开发中至关重要，但API设计质量评估仍主要依赖手动和临时过程，特别是在早期开发阶段。需要自动化工具来支持系统演进、服务互操作性和跨组织治理。

Method: 采用设计科学研究方法，通过文献综述识别了75个API设计规则，并实现了一个可配置的规则引擎来检测OpenAPI规范中的结构违规。

Result: 评估显示，S.E.O.R.A工具能够促进非功能性API需求的早期验证，提供可操作和可追溯的反馈，与需求获取和质量保证过程良好契合。

Conclusion: 该工作通过将设计原则操作化为可验证的约束并将其嵌入到实际验证工具中，为需求工程做出了贡献。未来方向包括IDE集成、扩展规则覆盖范围以及在真实环境中部署以支持敏捷API开发生命周期中的持续合规性。

Abstract: RESTful APIs are central in developing interoperable, modular, and maintainable software systems in enterprises today. Also, it is essential to support system evolution, service interoperability, and governance across organizational boundaries to ensure good quality and consistency of these APIs. However, evaluating API design quality, which is part of non-functional requirement tasks, remains a largely manual and ad hoc process, particularly during early development. Using a Design Science Research (DSR) methodology, we elicited user needs, identified 75 API design rules using a literature review, and implemented a configurable rule engine to detect structural violations in OpenAPI specifications. The proposed tool supports organizational adaptability by allowing rules to be customized, enabled, or disabled, enabling integration of domain-specific standards. The evaluation was conducted through structured experiments and thematic analysis involving industry experts. API quality validation contributes to aligning technical designs with requirements and enterprise architecture by strengthening interoperability and governance between enterprise systems. The results show that S.E.O.R.A facilitates early validation of non-functional API requirements, provides actionable and traceable feedback, and aligns well with requirements elicitation and quality assurance processes. It improves the API design process by automating checks that would otherwise require manual inspection, thus supporting consistent and reusable conformance practices. This work contributes to requirements engineering by operationalizing design principles as verifiable constraints and embedding them into a practical validation tool. Future directions include IDE integration, expanded rule coverage, and real-world deployment to support continuous compliance in agile API development lifecycles.

</details>


### [54] [A Low-Code Methodology for Developing AI Kiosks: a Case Study with the DIZEST Platform](https://arxiv.org/abs/2511.17853)
*SunMin Moon,Jangwon Gim,Chaerin Kim,Yeeun Kim,YoungJoo Kim,Kang Choi*

Main category: cs.SE

TL;DR: 本文提出了一种基于低代码架构的增强型自助服务系统，采用DIZEST方法解决现代自助服务系统面临的集成不足、结构僵化、性能瓶颈和缺乏协作框架等挑战。


<details>
  <summary>Details</summary>
Motivation: 现代自助服务系统面临集成不足、结构僵化、性能瓶颈和缺乏协作框架等重大挑战，需要一种能够实现直观工作流设计和无缝AI集成的解决方案。

Method: 提出基于DIZEST的低代码平台方法，这是一种专门的低代码平台，支持直观的工作流设计和无缝AI集成。通过与现有平台（Jupyter Notebook、ComfyUI和Orange3）进行对比分析来验证方法。

Result: DIZEST在关键评估标准上表现出优越性能，照片自助服务案例研究进一步验证了该方法在提高互操作性、增强用户体验和增加部署灵活性方面的有效性。

Conclusion: 基于DIZEST的低代码架构方法能够有效解决现代自助服务系统的核心挑战，提供更好的集成能力、用户体验和部署灵活性。

Abstract: This paper presents a comprehensive study on enhancing kiosk systems through a low-code architecture, with a focus on AI-based implementations. Modern kiosk systems are confronted with significant challenges, including a lack of integration, structural rigidity, performance bottlenecks, and the absence of collaborative frameworks. To overcome these limitations, we propose a DIZEST-based approach methodology, a specialized low-code platform that enables intuitive workflow design and seamless AI integration. Through a comparative analysis with existing platforms, including Jupyter Notebook, ComfyUI, and Orange3, we demonstrate that DIZEST delivers superior performance across key evaluation criteria. Our photo kiosk case study further validates the effectiveness of this approach in improving interoperability, enhancing user experience, and increasing deployment flexibility.

</details>


### [55] [Enhancing Automated Program Repair via Faulty Token Localization and Quality-Aware Patch Refinement](https://arxiv.org/abs/2511.18001)
*Jiaolong Kong,Xiaofei Xie,Yiheng Xiong,Yuekun Wang,Jian Wang*

Main category: cs.SE

TL;DR: TokenRepair是一个新颖的两级细化框架，通过集成内部反射和外部反馈来增强自动程序修复。它通过分析令牌级不确定性波动来定位可疑令牌，并使用思维链引导重写进行针对性修正，显著提升了修复性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的程序修复技术主要依赖粗粒度的外部反馈（如测试结果），缺乏揭示补丁失败原因或代码错误部分的细粒度内部信号，导致修复效率低下、错误传播和性能不佳。

Method: TokenRepair采用两级细化框架：1）内部反射分析上下文感知的令牌级不确定性波动来识别可疑令牌；2）使用思维链引导重写仅精炼这些定位的令牌；3）集成质量感知的外部反馈机制评估补丁质量并过滤低质量候选。

Result: 实验结果显示TokenRepair在Defects4J 1.2上正确修复了88个错误，在HumanEval-Java上修复了139个错误，在Defects4J 1.2上相比所有模型提升了8.2%到34.9%，在HumanEval-Java上提升了3.3%到16.1%。

Conclusion: TokenRepair通过集成内部反射和外部反馈，实现了最先进的程序修复性能，证明了细粒度令牌级定位和针对性修正的有效性。

Abstract: Large language models (LLMs) have recently demonstrated strong potential for automated program repair (APR). However, existing LLM-based techniques primarily rely on coarse-grained external feedback (e.g.,test results) to guide iterative patch generation, while lacking fine-grained internal signals that reveal why a patch fails or which parts of the generated code are likely incorrect. This limitation often leads to inefficient refinement, error propagation, and suboptimal repair performance. In this work, we propose TokenRepair, a novel two-level refinement framework that enhances APR by integrating internal reflection for localizing potentially faulty tokens with external feedback for quality-aware patch refinement. Specifically, TokenRepair first performs internal reflection by analyzing context-aware token-level uncertainty fluctuations to identify suspicious or low-confidence tokens within a patch. It then applies Chain-of-Thought guided rewriting to refine only these localized tokens, enabling targeted and fine-grained correction. To further stabilize the iterative repair loop, TokenRepair incorporates a quality-aware external feedback mechanism that evaluates patch quality and filters out low-quality candidates before refinement. Experimental results show that TokenRepair achieves new state-of-the-art repair performance, correctly fixing 88 bugs on Defects4J 1.2 and 139 bugs on HumanEval-Java, demonstrating substantial improvements ranging from 8.2% to 34.9% across all models on Defects4J 1.2 and from 3.3% to 16.1% on HumanEval-Java.

</details>


### [56] [Event-Chain Analysis for Automated Driving and ADAS Systems: Ensuring Safety and Meeting Regulatory Timing Requirements](https://arxiv.org/abs/2511.18092)
*Sebastian Dingler,Philip Rehkop,Florian Mayer,Ralf Muenzenberger*

Main category: cs.SE

TL;DR: 本文提出了一种基于事件链建模的白盒方法，用于解决自动驾驶系统的时序约束挑战，确保符合国际法规要求。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶系统必须满足国际法规和标准规定的严格时序约束，而黑盒方法无法提供足够的透明度和验证证据。

Method: 采用基于事件链建模的白盒方法，对感知、规划、执行和人类交互等各功能组件的时序行为进行透明分析。

Result: 案例研究表明该方法能早期识别合规问题，系统优化参数，并通过概率分析生成定量证据。

Conclusion: 事件链中心方法增强了法规合规性，优化了系统设计，并支持基于模型的安全分析技术。

Abstract: Automated Driving Systems (ADS), including Advanced Driver Assistance Systems (ADAS), must fulfill not only high functional expectations but also stringent timing constraints mandated by international regulations and standards. Regulatory frameworks such as UN regulations, NCAP standards, ISO norms, and NHTSA guidelines impose strict bounds on system reaction times to ensure safe vehicle operation. This paper presents a structured, White-Box methodology based on Event-Chain Modeling to address these timing challenges. Unlike Black-Box approaches, Event-Chain Analysis offers transparent insights into the timing behavior of each functional component - from perception and planning to actuation and human interaction. This perspective is also aligned with multiple regulations, which require that homologation dossiers provide evidence that the chosen system architecture is suitable to ensure compliance with the specified requirements. Our methodology enables the derivation, modeling, and validation of end-to-end timing constraints at the architectural level and facilitates early verification through simulation. Through a detailed case study, we demonstrate how this Event-Chain-centric approach enhances regulatory compliance, optimizes system design, and supports model-based safety analysis techniques, with results showing early identification of compliance issues, systematic parameter optimization, and quantitative evidence generation through probabilistic analysis.

</details>


### [57] [Towards a General Framework for HTN Modeling with LLMs](https://arxiv.org/abs/2511.18165)
*Israel Puerta-Merino,Carlos Núñez-Molina,Pablo Mesejo,Juan Fernández-Olivares*

Main category: cs.SE

TL;DR: 本文提出L2HP框架扩展LLM在分层规划领域的应用，实验发现LLM在分层规划中的语法有效性远低于非分层规划。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在自动规划领域应用广泛，但在分层规划方面的应用仍远未达到非分层架构的成熟度，需要填补这一研究空白。

Method: 提出L2HP框架，扩展L2P库以支持分层规划模型生成，采用通用性和可扩展性的设计理念，并在PlanBench数据集上进行实验比较。

Result: 在PlanBench数据集上，解析成功率在两种设置下相当（约36%），但分层规划中的语法有效性显著较低（1% vs 20%）。

Conclusion: 分层规划对LLM提出了独特挑战，需要进一步研究来提高生成的分层规划模型质量。

Abstract: The use of Large Language Models (LLMs) for generating Automated Planning (AP) models has been widely explored; however, their application to Hierarchical Planning (HP) is still far from reaching the level of sophistication observed in non-hierarchical architectures. In this work, we try to address this gap. We present two main contributions. First, we propose L2HP, an extension of L2P (a library to LLM-driven PDDL models generation) that support HP model generation and follows a design philosophy of generality and extensibility. Second, we apply our framework to perform experiments where we compare the modeling capabilities of LLMs for AP and HP. On the PlanBench dataset, results show that parsing success is limited but comparable in both settings (around 36\%), while syntactic validity is substantially lower in the hierarchical case (1\% vs. 20\% of instances). These findings underscore the unique challenges HP presents for LLMs, highlighting the need for further research to improve the quality of generated HP models.

</details>


### [58] [Establishing Traceability Links between Release Notes & Software Artifacts: Practitioners' Perspectives](https://arxiv.org/abs/2511.18187)
*Sristy Sumana Nath,Banani Roy,Munima Jahan*

Main category: cs.SE

TL;DR: 该论文提出了一种基于LLM的方法来自动建立发布说明与开发工件（如PR、提交和问题）之间的可追溯性链接，解决了开源环境中可追溯性链接缺失和损坏的问题。


<details>
  <summary>Details</summary>
Motivation: 在开源环境中，维护发布说明与开发工件之间的可追溯性链接对于管理技术债务和确保可维护性至关重要，但目前这个过程容易出错、耗时且经常被忽视。实证研究发现47%的发布工件缺乏可追溯性链接，12%包含损坏链接。

Method: 首先分析发布说明以识别其What、Why和How信息，并评估这些信息如何与PR、提交和问题对齐。构建包含3500个过滤和验证的可追溯性链接实例的基准数据集。然后实现基于LLM的方法来自动建立发布说明内容与PR、发布说明内容与PR以及发布说明内容与问题之间的三对可追溯性链接。

Result: 通过结合时间邻近特征，基于LLM的方法（如Gemini 1.5 Pro）在PR可追溯性恢复方面实现了0.73的高Precision@1值。在线调查显示，16%的受访者认为这种方法非常重要，68%认为有些重要。

Conclusion: 基于LLM的方法能够有效自动建立发布说明与开发工件之间的可追溯性链接，具有较高的准确性和实际应用潜力，有助于改善开源项目的可维护性。

Abstract: Maintaining traceability links between software release notes and corresponding development artifacts, e.g., pull requests (PRs), commits, and issues, is essential for managing technical debt and ensuring maintainability. However, in open-source environments where contributors work remotely and asynchronously, establishing and maintaining these links is often error-prone, time-consuming, and frequently overlooked. Our empirical study of GitHub repositories revealed that 47% of release artifacts lacked traceability links, and 12% contained broken links. To address this gap, we first analyzed release notes to identify their What, Why, and How information and assessed how these align with PRs, commits, and issues. We curated a benchmark dataset consisting of 3,500 filtered and validated traceability link instances. Then, we implemented LLM-based approaches to automatically establish traceability links of three pairs between release note contents & PRs, release note contents & PRs and release note contents & issues. By combining the time proximity feature, the LLM-based approach, e.g., Gemini 1.5 Pro, achieved a high Precision@1 value of 0.73 for PR traceability recovery. To evaluate the usability and adoption potential of this approach, we conducted an online survey involving 33 open-source practitioners. 16% of respondents rated as very important, and 68% as somewhat important for traceability maintenance.

</details>


### [59] [A Needle in a Haystack: Intent-driven Reusable Artifacts Recommendation with LLMs](https://arxiv.org/abs/2511.18343)
*Dongming Jin,Zhi Jin,Xiaohong Chen,Zheng Fang,Linyu Li,Yuanpeng He,Jia Li,Yirang Zhang,Yingtao Fang*

Main category: cs.SE

TL;DR: 本文提出了TreeRec框架，通过基于LLM的语义抽象将软件构件组织成层次化语义树，以解决开源软件构件推荐中LLMs精度低和推理成本高的问题。


<details>
  <summary>Details</summary>
Motivation: 开源软件开发中，开发者面临大量可复用构件时难以找到满足需求的构件，现有基于检索和学习的技术在推荐效果上仍有不足，LLMs虽显示出潜力但效果尚未充分探索。

Method: 构建IntentRecBench基准，比较LLMs与传统方法；提出TreeRec框架，利用LLM语义抽象将构件组织成层次化语义树，实现意图与功能对齐并减少推理时间。

Result: 实验表明LLMs优于传统方法但仍有精度低和推理成本高的问题；TreeRec能持续提升不同LLMs在各生态系统中的性能，显示出良好的泛化性和实际部署潜力。

Conclusion: TreeRec通过语义树组织有效缓解了LLMs在构件推荐中的问题，为实际应用提供了可行方案。

Abstract: In open source software development, the reuse of existing artifacts has been widely adopted to avoid redundant implementation work. Reusable artifacts are considered more efficient and reliable than developing software components from scratch. However, when faced with a large number of reusable artifacts, developers often struggle to find artifacts that can meet their expected needs. To reduce this burden, retrieval-based and learning-based techniques have been proposed to automate artifact recommendations. Recently, Large Language Models (LLMs) have shown the potential to understand intentions, perform semantic alignment, and recommend usable artifacts. Nevertheless, their effectiveness has not been thoroughly explored. To fill this gap, we construct an intent-driven artifact recommendation benchmark named IntentRecBench, covering three representative open source ecosystems. Using IntentRecBench, we conduct a comprehensive comparative study of five popular LLMs and six traditional approaches in terms of precision and efficiency. Our results show that although LLMs outperform traditional methods, they still suffer from low precision and high inference cost due to the large candidate space. Inspired by the ontology-based semantic organization in software engineering, we propose TreeRec, a feature tree-guided recommendation framework to mitigate these issues. TreeRec leverages LLM-based semantic abstraction to organize artifacts into a hierarchical semantic tree, enabling intent and function alignment and reducing reasoning time. Extensive experiments demonstrate that TreeRec consistently improves the performance of diverse LLMs across ecosystems, highlighting its generalizability and potential for practical deployment.

</details>


### [60] [Evaluating perturbation robustnessof generative systems that use COBOL code inputs](https://arxiv.org/abs/2511.18488)
*Samuel Ackerman,Wesam Ibraheem,Orna Raz,Marcel Zalmanovici*

Main category: cs.SE

TL;DR: 提出了一个评估基于LLM的COBOL代码处理系统鲁棒性的框架，通过创建COBOL代码变体来测试系统对微小输入变化的敏感性，并提供可视化工具来调试和改进系统。


<details>
  <summary>Details</summary>
Motivation: 基于LLM的系统对不影响语义的微小输入变化很敏感，这会降低系统的实用性。由于COBOL业务关键应用的代码通常无法用于LLM训练，评估其鲁棒性尤为重要。

Method: 开发了COBOL段落和完整程序的扰动方法库，创建基准数据集的变体扩展版本，通过计算系统输出指标的变化来评估鲁棒性，并提供动态可视化仪表板进行调试。

Result: 建立了一个完整的鲁棒性评估框架，能够系统地测试LLM系统对COBOL代码微小变化的敏感性，并提供工具帮助识别和处理敏感性问题。

Conclusion: 该框架对于提高基于LLM的COBOL代码处理系统的鲁棒性至关重要，可视化工具能够帮助调试系统输出并理解敏感性的根本原因，从而改进系统性能。

Abstract: Systems incorporating large language models (LLMs) as a component are known to be sensitive (i.e., non-robust) to minor input variations that do not change the meaning of the input; such sensitivity may reduce the system's usefulness. Here, we present a framework to evaluate robustness of systems using COBOL code as input; our application is translation between COBOL and Java programming languages, but the approach extends to other tasks such as code generation or explanation. Targeting robustness of systems with COBOL as input is essential yet challenging. Many business-critical applications are written in COBOL, yet these are typically proprietary legacy applications and their code is unavailable to LLMs for training. We develop a library of COBOL paragraph and full-program perturbation methods, and create variant-expanded versions of a benchmark dataset of examples for a specific task. The robustness of the LLM-based system is evaluated by measuring changes in values of individual and aggregate metrics calculated on the system's outputs. Finally, we present a series of dynamic table and chart visualization dashboards that assist in debugging the system's outputs, and monitoring and understanding root causes of the system's sensitivity to input variation. These tools can be further used to improve the system by, for instance, indicating variations that should be handled by pre-processing steps.

</details>


### [61] [HQPEF-Py: Metrics, Python Patterns, and Guidance for Evaluating Hybrid Quantum Programs](https://arxiv.org/abs/2511.18506)
*Michael Adjei Osei,Sidney Shapiro*

Main category: cs.SE

TL;DR: 提出了一个评估混合量子程序作为端到端工作流的框架，包括量子准备度评分、量子效用标准化加速以及混合管道的时序漂移审计。


<details>
  <summary>Details</summary>
Motivation: 现有评估方法主要关注孤立的量子设备或算法，缺乏对整个混合量子工作流的端到端评估框架。

Method: 基于混合量子程序评估框架(HQPEF)，形式化工作流感知的量子准备度评分(QRL)，定义质量约束下的标准化加速作为量子效用(UQ)，并提供混合管道的时序漂移审计。

Result: 开发了简洁的Python参考实现，展示了如何使用最先进的经典和量子求解器实例化这些指标和审计程序，同时保持匹配预算约束和可重复性。

Conclusion: 该框架为混合量子程序的端到端评估提供了系统化方法，有助于更全面地理解量子计算在实际工作流中的价值。

Abstract: We study how to evaluate hybrid quantum programs as end-to-end workflows rather than as isolated devices or algorithms. Building on the Hybrid Quantum Program Evaluation Framework (HQPEF), we formalize a workflow-aware Quantum Readiness Level (QRL) score; define a normalized speedup under quality constraints for the Utility of Quantumness (UQ); and provide a timing-and-drift audit for hybrid pipelines. We complement these definitions with concise Python reference implementations that illustrate how to instantiate the metrics and audit procedures with state-of-the-art classical and quantum solvers (e.g., via Qiskit or PennyLane), while preserving matched-budget discipline and reproducibility.

</details>


### [62] [Strategic Decision Framework for Enterprise LLM Adoption](https://arxiv.org/abs/2511.18589)
*Michael Trusov,Minha Hwang,Zainab Jamal,Swarup Chandra*

Main category: cs.SE

TL;DR: 本文提出了一个系统化的六步决策框架，帮助组织从初始应用选择到最终部署采用大型语言模型，解决企业在数据安全、开发方法、基础设施和部署策略等方面的关键挑战。


<details>
  <summary>Details</summary>
Motivation: 组织在采用LLM时缺乏明确的指导，面临数据安全、开发方法、基础设施和部署策略等关键挑战，需要系统化的决策框架来平衡技术能力与业务目标。

Method: 基于广泛的访谈和成功与失败实施案例的分析，开发了一个六步决策框架，涵盖从应用选择到部署的整个过程。

Result: 该框架为业务领导者提供了实用指导，通过关键决策点和真实案例，帮助组织在客户服务自动化、内容创建和高级分析等用例中做出明智决策。

Conclusion: 提出的六步决策框架能够帮助组织在确保安全高效集成的同时，将技术能力与业务目标对齐，实现成功的LLM采用。

Abstract: Organizations are rapidly adopting Large Language Models (LLMs) to transform their operations, yet they lack clear guidance on key decisions for adoption and implementation. While LLMs offer powerful capabilities in content generation, assisted coding, and process automation, businesses face critical challenges in data security, LLM solution development approach, infrastructure requirements, and deployment strategies. Healthcare providers must protect patient data while leveraging LLMs for medical analysis, financial institutions need to balance automated customer service with regulatory compliance, and software companies seek to enhance development productivity while maintaining code security.
  This article presents a systematic six-step decision framework for LLM adoption, helping organizations navigate from initial application selection to final deployment. Based on extensive interviews and analysis of successful and failed implementations, our framework provides practical guidance for business leaders to align technological capabilities with business objectives. Through key decision points and real-world examples from both B2B and B2C contexts, organizations can make informed decisions about LLM adoption while ensuring secure and efficient integration across various use cases, from customer service automation to content creation and advanced analytics.

</details>


### [63] [From Reviewers' Lens: Understanding Bug Bounty Report Invalid Reasons with LLMs](https://arxiv.org/abs/2511.18608)
*Jiangrui Zheng,Yingming Zhou,Ali Abdullah Ahmad,Hanqing Yao,Xueqing Liu*

Main category: cs.SE

TL;DR: 本研究通过分析9,942份漏洞赏金报告，评估大语言模型识别无效报告的能力，发现模型倾向于过度接受报告。通过构建信息泄露漏洞拒绝原因分类法并结合检索增强生成框架，显著提高了分类一致性和减少了偏见。


<details>
  <summary>Details</summary>
Motivation: 随着AI生成漏洞报告的兴起，需要帮助漏洞猎人理解报告被标记为无效的原因，以提高报告质量并减轻审核人员负担。

Method: 收集9,942份已披露的漏洞赏金报告数据集，评估GPT-5、DeepSeek和微调RoBERTa等模型识别无效报告的能力，构建信息泄露漏洞拒绝原因分类法，并集成到检索增强生成框架中。

Result: 虽然先进大语言模型整体准确率较高，但在检测无效案例方面表现不佳，存在过度接受报告的倾向。结合结构化审核知识的RAG框架显著提高了分类一致性和减少了偏见。分析还发现高声誉报告者在边界案例中更易获得有利结果。

Conclusion: 无效报告识别存在挑战，将大语言模型与结构化审核知识相结合可以支持更透明和一致的漏洞报告审核过程。

Abstract: Bug bounty platforms (e.g., HackerOne, BugCrowd) leverage crowd-sourced vulnerability discovery to improve continuous coverage, reduce the cost of discovery, and serve as an integral complement to internal red teams. With the rise of AI-generated bug reports, little work exists to help bug hunters understand why these reports are labeled as invalid. To improve report quality and reduce reviewers' burden, it is critical to predict invalid reports and interpret invalid reasons.
  In this work, we conduct an empirical study with the purpose of helping bug hunters understand the validity of reports. We collect a dataset of 9,942 disclosed bug bounty reports, including 1,400 invalid reports, and evaluate whether state-of-the-art large language models can identify invalid reports. While models such as GPT-5, DeepSeek, and a fine-tuned RoBERTa achieve strong overall accuracy, they consistently struggle to detect invalid cases, showing a tendency to over-accept reports. To improve invalidity detection, we build a taxonomy of rejection reasons for Information Disclosure vulnerabilities and incorporate it into a retrieval-augmented generation (RAG) framework. This approach substantially improves classification consistency and reduces bias. We also examine whether reviewer decisions may be influenced by factors beyond the content of the report. Our analysis shows that reporters with higher reputations tend to receive more favorable outcomes in borderline cases, suggesting that perceived expertise can influence review judgments.
  Overall, our findings highlight the challenges of invalid report identification and show that combining LLMs with structured reviewer knowledge can support more transparent and consistent vulnerability report review.

</details>


### [64] [Leveraging Discrete Choice Experiments for User-Centric Requirements Prioritization in mHealth Applications](https://arxiv.org/abs/2511.18625)
*Wei Wang,Hourieh Khalajzadeh,John Grundy,Anuradha Madugalla,Humphrey O. Obie*

Main category: cs.SE

TL;DR: 本研究通过离散选择实验分析慢性病患者对移动健康应用自适应界面的偏好，发现保持可用性、控制权、低频度和小规模调整是促进采用的关键因素，而高频使用功能和照护者参与会降低适应性设计的感知价值。


<details>
  <summary>Details</summary>
Motivation: 移动健康应用在慢性病管理中面临可用性和可访问性挑战，自适应用户界面提供个性化解决方案，但用户偏好和权衡因素尚不明确，阻碍了广泛采用。

Method: 采用离散选择实验方法，对186名使用移动健康应用的慢性病患者进行调查，使用混合logit模型分析偏好异质性，并进行年龄、性别、健康状况和应对机制等亚组分析。

Result: 研究发现保持可用性同时确保对调整的控制权、低频度调整和小规模变化是促进采用的关键因素，而高频使用功能和照护者参与会降低适应性设计的感知价值。

Conclusion: 本研究通过数据驱动方法量化用户偏好，识别关键权衡因素，为开发未来自适应移动健康应用提供指导，并为软件工程领域的需求优先级排序奠定基础。

Abstract: Mobile health (mHealth) applications are widely used for chronic disease management, but usability and accessibility challenges persist due to the diverse needs of users. Adaptive User Interfaces (AUIs) offer a personalized solution to enhance user experience, yet barriers to adoption remain. Understanding user preferences and trade-offs is essential to ensure widespread acceptance of adaptation designs. This study identifies key factors influencing user preferences and trade-offs in mHealth adaptation design. A Discrete Choice Experiment (DCE) was conducted with 186 participants who have chronic diseases and use mHealth applications. Participants were asked to select preferred adaptation designs from choices featuring six attributes with varying levels. A mixed logit model was used to analyze preference heterogeneity and determine the factors most likely influencing adoption. Additionally, subgroup analyses were performed to explore differences by age, gender, health conditions, and coping mechanisms. Maintaining usability while ensuring controllability over adaptations, infrequent adaptations, and small-scale changes are key factors that facilitate the adoption of adaptive mHealth app designs. In contrast, frequently used functions and caregiver involvement can diminish the perceived value of such adaptations. This study employs a data-driven approach to quantify user preferences, identify key trade-offs, and reveal variations across demographic and behavioral subgroups through preference heterogeneity modeling. Furthermore, our results offer valuable guidance for developing future adaptive mHealth applications and lay the groundwork for continued exploration into requirements prioritization within the field of software engineering.

</details>


### [65] [ChroniUXMag: A Persona-Driven Framework for Inclusive mHealth Requirements Engineering](https://arxiv.org/abs/2511.18634)
*Wei Wang,Devi Karolita,Hourieh Khalajzadeh,John Grundy,Anuradha Madugalla,Humphrey O. Obie*

Main category: cs.SE

TL;DR: 提出了ChroniUXMag框架，用于在移动健康应用中系统性地获取和分析包容性需求，解决慢性病患者在mHealth使用中的动态需求挑战。


<details>
  <summary>Details</summary>
Motivation: 移动健康应用在慢性病管理中面临可访问性、包容性和持续参与度的挑战，传统需求工程方法往往忽视患者随健康状况、依从性和护理支持变化而产生的动态需求。

Method: 基于InclusiveMag和GenderMag原则，通过两阶段开发：第一阶段通过文献综述、焦点小组、访谈和大规模调查识别包容性维度；第二阶段将这些维度合成为代表不同健康状况、态度和数字实践的用户画像，并整合到认知走查表中。

Result: 识别出13个捕捉mHealth使用社会技术复杂性的维度，包括信任、数字素养、依赖性和文化背景等，支持基于用户画像的结构化评估，揭示传统可用性评估常忽略的包容性障碍。

Conclusion: ChroniUXMag为需求工程提供了可重复、基于证据的方法，将包容性嵌入mHealth需求中，未来工作将通过在真实设计环境中进行实践者主导的评估来扩展第三阶段应用。

Abstract: Mobile health (mHealth) applications are increasingly adopted for chronic disease management, yet they face persistent challenges related to accessibility, inclusivity, and sustained engagement. Patients' needs evolve dynamically with their health progression, adherence, and caregiver support, creating unique requirements engineering (RE) challenges that traditional approaches often overlook. This study introduces ChroniUXMag, a framework for eliciting and analysing inclusivity requirements in mHealth design. Building on InclusiveMag and GenderMag principles, the framework aims to help researchers and practitioners systematically capture and evaluate factors that influence how individuals with chronic conditions perceive, trust, and interact with mHealth systems. The framework was developed through two stages of the InclusiveMag process. In the first stage, inclusivity facets were identified through a systematic literature review, focus groups, interviews, and a large-scale survey. In the second stage, these facets were synthesised into personas representing diverse health situations, attitudes, and digital practices, and integrated into an adapted cognitive walkthrough form. Thirteen facets were identified that capture the socio-technical complexity of mHealth use, including trust, digital literacy, dependency, and cultural context. These facets support structured, persona-driven evaluations that reveal inclusivity barriers often missed by traditional usability assessments. ChroniUXMag contributes to RE by offering a reproducible, evidence-based approach for embedding inclusivity into mHealth requirements. Future work will extend the third stage Apply through practitioner-led evaluation in real-world design contexts.

</details>


### [66] [Summary-Mediated Repair: Can LLMs use code summarisation as a tool for program repair?](https://arxiv.org/abs/2511.18782)
*Lukas Twist*

Main category: cs.SE

TL;DR: 该论文提出了一种基于代码摘要的程序修复方法，通过自然语言代码摘要作为中间步骤来帮助LLMs识别和修复实现级别的错误。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型虽然基准测试表现良好，但经常产生包含细微实现级别错误的代码。这些错误难以被发现，但LLMs在总结代码时能够捕捉高层次意图，这启发了利用代码摘要作为修复中间步骤的思路。

Method: 提出了summary-mediated repair方法，这是一个仅使用提示的管道，利用自然语言代码摘要作为显式中间步骤。在八个生产级LLMs上评估了该方法，比较了多种摘要风格与直接修复基线的效果。

Result: 错误感知的诊断摘要持续产生最大增益 - 修复了高达65%的未见错误，平均比基线多修复5%。但整体改进较为有限且依赖于具体LLM。

Conclusion: 代码摘要可以作为一种廉价、人类可解释的诊断工具集成到程序修复管道中，而不是作为独立的万能解决方案。

Abstract: Large Language Models (LLMs) often produce code with subtle implementation-level bugs despite strong benchmark performance. These errors are hard for LLMs to spot and can have large behavioural effects; yet when asked to summarise code, LLMs can frequently surface high-level intent and sometimes overlook this low-level noise. Motivated by this, we propose summary-mediated repair, a prompt-only pipeline for program repair that leverages natural-language code summarisation as an explicit intermediate step, extending previous work that has already shown code summarisation to be a useful intermediary for downstream tasks. We evaluate our method across eight production-grade LLMs on two function level benchmarks (HumanEvalPack and MBPP), comparing several summary styles against a direct repair baseline. Error-aware diagnostic summaries consistently yield the largest gains - repairing up to 65% of unseen errors, on average of 5% more than the baseline - though overall improvements are modest and LLM-dependent. Our results position summaries as a cheap, human-interpretable diagnostic artefact that can be integrated into program-repair pipelines rather than a stand-alone fix-all.

</details>


### [67] [Optimizing LLM Code Suggestions: Feedback-Driven Timing with Lightweight State Bounds](https://arxiv.org/abs/2511.18842)
*Mohammad Nour Al Awad,Sergey Ivanov,Olga Tikhonova*

Main category: cs.SE

TL;DR: 提出了一种自适应时机机制，根据开发者实时反馈动态调整代码建议的延迟时间，显著提高了LLM代码自动补全的接受率和效率。


<details>
  <summary>Details</summary>
Motivation: 当前LLM代码自动补全系统在何时呈现建议方面缺乏探索，经常导致中断或浪费推理调用，需要更智能的时机决策机制。

Method: 结合逻辑变换的近期接受率和有界延迟范围，基于开发者认知状态的高层二元预测来动态调整建议延迟时间。

Result: 在两个月专业开发者部署中，接受率从无延迟的4.9%提升到静态延迟的15.4%，再到自适应时机的18.6%；盲目拒绝率从8.3%降至0.36%，浪费推理调用减少75%。

Conclusion: 自适应时机机制显著提高了LLM代码助手的效率和成本效益，使其在实践中更加实用。

Abstract: Large Language Models (LLMs) have transformed code auto-completion by generating context-aware suggestions. Yet, deciding when to present these suggestions remains underexplored, often leading to interruptions or wasted inference calls. We propose an adaptive timing mechanism that dynamically adjusts the delay before offering a suggestion based on real-time developer feedback. Our suggested method combines a logistic transform of recent acceptance rates with a bounded delay range, anchored by a high-level binary prediction of the developer's cognitive state. In a two-month deployment with professional developers, our system improved suggestion acceptance from 4.9% with no delay to 15.4% with static delays, and to 18.6% with adaptive timing-while reducing blind rejections (rejections without being read) from 8.3% to 0.36%. Together, these improvements increase acceptance and substantially reduce wasted inference calls by 75%, making LLM-based code assistants more efficient and cost-effective in practice.

</details>


### [68] [Pre-Filtering Code Suggestions using Developer Behavioral Telemetry to Optimize LLM-Assisted Programming](https://arxiv.org/abs/2511.18849)
*Mohammad Nour Al Awad,Sergey Ivanov,Olga Tikhonova*

Main category: cs.SE

TL;DR: 提出了一种轻量级预过滤模型，通过开发者行为信号预测代码建议接受率，在VS Code插件中部署4个月，将接受率从18.4%提升至34.2%，同时减少35%的低价值LLM调用。


<details>
  <summary>Details</summary>
Motivation: LLM在代码编辑器中的许多建议被忽略，导致计算浪费、延迟增加和不必要的中断，需要一种方法来预测建议接受率以提高用户体验和系统效率。

Method: 使用轻量级预过滤模型，仅基于实时开发者遥测数据（如打字速度、文件导航和编辑活动）来预测建议接受可能性，在调用LLM之前进行过滤。

Result: 在生产级VS Code插件中部署4个月，接受率从18.4%提升至34.2%，同时抑制了35%的低价值LLM调用。

Conclusion: 仅凭行为信号就能显著改善LLM辅助编程中的用户体验和系统效率，证明了基于时序感知、隐私保护的适配机制的价值。

Abstract: Large Language Models (LLMs) are increasingly integrated into code editors to provide AI-powered code suggestions. Yet many of these suggestions are ignored, resulting in wasted computation, increased latency, and unnecessary interruptions. We introduce a lightweight pre-filtering model that predicts the likelihood of suggestion acceptance before invoking the LLM, using only real-time developer telemetry such as typing speed, file navigation, and editing activity. Deployed in a production-grade Visual Studio Code plugin over four months of naturalistic use, our approach nearly doubled acceptance rates (18.4% -> 34.2%) while suppressing 35% of low-value LLM calls. These findings demonstrate that behavioral signals alone can meaningfully improve both user experience and system efficiency in LLM-assisted programming, highlighting the value of timing-aware, privacy-preserving adaptation mechanisms. The filter operates solely on pre-invocation editor telemetry and never inspects code or prompts.

</details>


### [69] [Time Travel: LLM-Assisted Semantic Behavior Localization with Git Bisect](https://arxiv.org/abs/2511.18854)
*Yujing Wang,Weize Hong*

Main category: cs.SE

TL;DR: 提出了一种将大语言模型集成到Git bisect过程中的语义故障定位框架，通过结构化思维链推理在噪声条件下进行逐提交分析，在多个开源项目中实现了6.4个百分点的成功率提升。


<details>
  <summary>Details</summary>
Motivation: 传统bisect方法假设确定性谓词和二元故障状态，但在现代软件开发中常因不稳定测试、非单调回归和与上游仓库的语义分歧而失效。

Method: 使用结构化思维链推理增强bisect遍历，评估多个LLM并微调DeepSeekCoderV2，采用弱监督工作流减少标注开销，结合人工校正和自一致性过滤。

Result: 在多个开源项目中，成功率从74.2%提升到80.6%，显著减少了失败遍历次数，平均bisect时间最多减少2倍。

Conclusion: 讨论了时序推理、提示设计和针对提交级行为分析的微调策略，为语义故障定位提供了有效解决方案。

Abstract: We present a novel framework that integrates Large Language Models (LLMs) into the Git bisect process for semantic fault localization. Traditional bisect assumes deterministic predicates and binary failure states assumptions often violated in modern software development due to flaky tests, nonmonotonic regressions, and semantic divergence from upstream repositories. Our system augments bisect traversal with structured chain of thought reasoning, enabling commit by commit analysis under noisy conditions. We evaluate multiple open source and proprietary LLMs for their suitability and fine tune DeepSeekCoderV2 using QLoRA on a curated dataset of semantically labeled diffs. We adopt a weak supervision workflow to reduce annotation overhead, incorporating human in the loop corrections and self consistency filtering. Experiments across multiple open source projects show a 6.4 point absolute gain in success rate from 74.2 to 80.6 percent, leading to significantly fewer failed traversals and by experiment up to 2x reduction in average bisect time. We conclude with discussions on temporal reasoning, prompt design, and finetuning strategies tailored for commit level behavior analysis.

</details>


### [70] [VecIntrinBench: Benchmarking Cross-Architecture Intrinsic Code Migration for RISC-V Vector](https://arxiv.org/abs/2511.18867)
*Liutong Han,Chu Kang,Mingjie Xing,Yanjun Wu*

Main category: cs.SE

TL;DR: VecIntrinBench是首个包含RISC-V Vector扩展的内在函数基准测试，包含50个函数级任务，系统评估了各种代码迁移方法，发现先进大语言模型在RISC-V代码迁移中能达到与基于规则的映射方法相似效果且性能更优。


<details>
  <summary>Details</summary>
Motivation: RISC-V软件生态系统对算法库迁移有强烈需求，但目前缺乏支持新兴RISC-V架构的内在函数代码基准测试，特别是没有全面评估RVV扩展内在函数迁移能力的基准。

Method: 提出VecIntrinBench基准测试，包含50个来自开源仓库的函数级任务，实现为标量、RVV内在函数、Arm Neon内在函数和x86内在函数，并提供全面的功能和性能测试用例。

Result: 系统评估显示，先进大语言模型在RISC-V代码迁移中能达到与基于规则的映射方法相似效果，同时提供更优性能。

Conclusion: VecIntrinBench填补了RVV扩展内在函数迁移评估的空白，为社区和开发者提供了有价值的基准测试工具，并指出了大语言模型在代码迁移领域的未来发展方向。

Abstract: Intrinsic functions are specialized functions provided by the compiler that efficiently operate on architecture-specific hardware, allowing programmers to write optimized code in a high-level language that fully exploits hardware features. Using intrinsics to vectorize core code blocks is a standard optimization method in high-performance libraries, often requiring specific vector optimization implementations for multiple mainstream architectures. The promising RISC-V software ecosystem has a significant demand for algorithm library migration and adaptation. Translating existing intrinsic functions to RISC-V Vector (RVV) intrinsic functions across architectures is currently a mainstream approach. Rule-based intrinsic mapping methods and LLM-based code generation can help developers address the code migration challenge. However, existing intrinsic code benchmarks focus on mainstream SIMD intrinsics and lack support for the emerging RISC-V architecture. There is currently no benchmark that comprehensively evaluates the intrinsic migration capabilities for the RVV extension. To fill this gap, we propose VecIntrinBench, the first intrinsic benchmark encompassing RVV extensions. It includes 50 function-level tasks from open source repositories, implemented as scalars, RVV intrinsics, Arm Neon intrinsics, and x86 intrinsics, along with comprehensive functional and performance test cases. We systematically evaluated various code migration approaches on VecIntrinBench, yielding a series of insightful findings. The results demonstrate that advanced Large Language Models (LLMs) achieve a similar effect as rule-based mapping approaches for RISC-V code migration, while also delivering superior performance. We further analyze the reasons and identify future directions for LLM development in the code migration field. The VecIntrinBench is open-sourced to benefit the broader community and developers.

</details>


### [71] [Optimization-Aware Test Generation for Deep Learning Compilers](https://arxiv.org/abs/2511.18918)
*Qingchao Shen,Zan Wang,Haoyang Ma,Yongqiang Tian,Lili Huang,Zibo Xiao,Junjie Chen,Shing-Chi Cheung*

Main category: cs.SE

TL;DR: OATest是一种针对深度学习编译器的优化感知测试方法，通过合成优化感知计算图来检测编译器中的错误，在TVM和ONNXRuntime上发现了58个未知错误。


<details>
  <summary>Details</summary>
Motivation: 深度学习编译器在DL生态系统中至关重要，但现有方法在测试优化阶段方面存在局限性，难以生成优化感知的测试用例。

Method: OATest通过从文档化测试中提取模式，将其融入种子计算图中，采用边重用策略建立模式与上下文之间的强连接，并使用辅助层添加策略解决约束破坏问题。

Result: OATest在TVM和ONNXRuntime上检测到更多错误，达到更高的代码覆盖率，发现了58个未知错误，其中36个已被开发者确认或修复。

Conclusion: OATest是一种有效的深度学习编译器测试方法，能够生成优化感知的计算图，显著提升编译器测试的效果。

Abstract: Deep Learning (DL) compilers have been widely utilized to optimize DL models for efficient deployment across various hardware. Due to their vital role in the DL ecosystem, ensuring their reliability and security is critical. However, existing approaches have limitations in testing optimization stages, which is the core functionality of DL compilers, due to the difficulty in generating optimization-aware tests. In this paper, we proposed OATest, a novel approach for synthesizing optimization-aware computational graphs. The approach combines patterns extracted from documented tests for optimization and incorporates them into seed computational graphs, enabling broader exploration of optimization paths. To guarantee the optimization-awareness of generated graphs, OATest introduces the edges reusing strategy to establish strong connections between patterns and contexts. Additionally, to solve the validity challenge for the generated graphs, OATest employs an auxiliary layers addition strategy to resolve broken constraints. Equipped with two distinct test oracles, OATest applies differential testing to evaluate the two widely used DL compilers (i.e., TVM and ONNXRuntime). Our experimental results show that OATest outperforms the state-of-the-art method by detecting more bugs and achieving higher code coverage in TVM and ONNXRutimes. Additionally, OATest uncovers 58 previously unknown bugs, 36 of which have been confirmed or fixed by developers.

</details>


### [72] [LLM-Driven Kernel Evolution: Automating Driver Updates in Linux](https://arxiv.org/abs/2511.18924)
*Arina Kharlamova,Jiawen Liu,Tianyi Zhang,Xinrui Yang,Humaid Alqasimi,Youcheng Sun,Chun Jason Xue*

Main category: cs.SE

TL;DR: 介绍了DRIVEBENCH（内核→驱动程序协同演化的可执行语料库）和AUTODRIVER（基于LLM的自动化驱动程序维护系统），用于解决Linux内核演化导致的驱动程序兼容性问题。


<details>
  <summary>Details</summary>
Motivation: Linux内核演化通过API/ABI变更、语义转移和安全加固更新破坏驱动程序兼容性，需要自动化解决方案来维护驱动程序。

Method: 开发了DRIVEBENCH语料库（v5.10-v6.10，235个验证案例）和AUTODRIVER系统，结合提示工程、多智能体协作、静态分析和迭代验证，确保生成的补丁在语法、功能和语义上都符合内核约定。

Result: 在55个案例评估中，AUTODRIVER实现了56.4%的编译成功率；基于QEMU的启动验证表明编译后的补丁在大多数情况下保留了驱动程序初始化功能。

Conclusion: 通过发布DRIVEBENCH和工具，为可重复性研究提供了支持，并为驱动程序与Linux内核的持续、安全协同演化提供了实用途径。

Abstract: Linux kernel evolution breaks drivers through API/ABI changes, semantic shifts, and security-hardening updates. We introduce DRIVEBENCH, an executable corpus of kernel$\rightarrow$driver co-evolution cases, and AUTODRIVER, a closed-loop, LLM-driven system for automating driver maintenance. The system integrates prompt engineering, multi-agent collaboration, static analysis, and iterative validation to ensure that generated patches are not only syntactically correct but also functionally and semantically consistent with kernel conventions. The corpus spans v5.10-v6.10 with 235 validated cases drawn from 612 candidates. In evaluation across 55 cases, AUTODRIVER achieves 56.4% compilation success; QEMU-based boot verification indicates that compiled patches preserve driver initialization in most instances. By releasing DRIVEBENCH and tooling, we enable reproducible research and a practical route to continuous, safe co-evolution of drivers with the Linux kernel.

</details>


### [73] [LLMAID: Identifying AI Capabilities in Android Apps with LLMs](https://arxiv.org/abs/2511.19059)
*Pei Liu,Terry Zhuo,Jiawei Deng,Thong James,Shidong Pan,Sherry Xu,Zhenchang Xing,Qinghua Lu,Xiaoning Du,Hongyu Zhang*

Main category: cs.SE

TL;DR: 提出了LLMAID方法，使用大语言模型自动检测移动应用中的AI能力，相比现有规则方法能识别242%更多的真实AI应用，准确率和召回率均超过90%。


<details>
  <summary>Details</summary>
Motivation: 现有移动应用AI能力检测方法主要依赖人工检查和规则启发式，成本高、耗时长且难以适应先进AI技术，需要更自动化的解决方案。

Method: LLMAID包含四个主要任务：候选提取、知识库交互、AI能力分析与检测、AI服务总结，应用于4,201个Android应用数据集。

Result: 相比最先进的规则方法，LLMAID能识别242%更多的真实AI应用，AI相关组件检测的精确率和召回率均超过90%，用户研究显示开发者更偏好LLMAID生成的AI服务总结。

Conclusion: LLMAID能有效检测移动应用中的AI能力，实证分析显示AI功能主要集中在计算机视觉领域（54.80%），其中目标检测是最常见任务（25.19%）。

Abstract: Recent advancements in artificial intelligence (AI) and its widespread integration into mobile software applications have received significant attention, highlighting the growing prominence of AI capabilities in modern software systems. However, the inherent hallucination and reliability issues of AI continue to raise persistent concerns. Consequently, application users and regulators increasingly ask critical questions such as: Does the application incorporate AI capabilities? and What specific types of AI functionalities are embedded? Preliminary efforts have been made to identify AI capabilities in mobile software; however, existing approaches mainly rely on manual inspection and rule-based heuristics. These methods are not only costly and time-consuming but also struggle to adapt advanced AI techniques.
  To address the limitations of existing methods, we propose LLMAID (Large Language Model for AI Discovery). LLMAID includes four main tasks: (1) candidate extraction, (2) knowledge base interaction, (3) AI capability analysis and detection, and (4) AI service summarization. We apply LLMAID to a dataset of 4,201 Android applications and demonstrate that it identifies 242% more real-world AI apps than state-of-the-art rule-based approaches. Our experiments show that LLM4AID achieves high precision and recall, both exceeding 90%, in detecting AI-related components. Additionally, a user study indicates that developers find the AI service summaries generated by LLMAID to be more informative and preferable to the original app descriptions. Finally, we leverage LLMAID to perform an empirical analysis of AI capabilities across Android apps. The results reveal a strong concentration of AI functionality in computer vision (54.80%), with object detection emerging as the most common task (25.19%).

</details>


### [74] [LLMs-Powered Real-Time Fault Injection: An Approach Toward Intelligent Fault Test Cases Generation](https://arxiv.org/abs/2511.19132)
*Mohammad Abboush,Ahmad Hatahet,Andreas Rausch*

Main category: cs.SE

TL;DR: 本文提出了一种基于大语言模型（LLMs）的故障测试用例生成方法，用于汽车软件系统的实时故障注入测试，解决了传统方法需要手动识别故障属性的局限性。


<details>
  <summary>Details</summary>
Motivation: 传统故障注入方法需要手动识别故障类型、位置和时间等属性，随着系统复杂度增加，这一过程变得昂贵、耗时且劳动密集。

Method: 利用大语言模型从功能安全需求自动生成故障测试用例，考虑了代表性和覆盖标准，并比较了不同LLMs的适用性。

Result: 提出的方法在使用gpt-4o时表现出优越性能，FSR分类和故障测试用例生成的F1分数分别达到88%和97.5%。生成的测试用例在硬件在环系统上实时执行验证。

Conclusion: 这种新方法能够优化实时测试过程，降低成本，同时增强复杂安全关键汽车软件系统的安全特性。

Abstract: A well-known testing method for the safety evaluation and real-time validation of automotive software systems (ASSs) is Fault Injection (FI). In accordance with the ISO 26262 standard, the faults are introduced artificially for the purpose of analyzing the safety properties and verifying the safety mechanisms during the development phase. However, the current FI method and tools have a significant limitation in that they require manual identification of FI attributes, including fault type, location and time. The more complex the system, the more expensive, time-consuming and labour-intensive the process. To address the aforementioned challenge, a novel Large Language Models (LLMs)-assisted fault test cases (TCs) generation approach for utilization during real-time FI tests is proposed in this paper. To this end, considering the representativeness and coverage criteria, the applicability of various LLMs to create fault TCs from the functional safety requirements (FSRs) has been investigated. Through the validation results of LLMs, the superiority of the proposed approach utilizing gpt-4o in comparison to other state-of-the-art models has been demonstrated. Specifically, the proposed approach exhibits high performance in terms of FSRs classification and fault TCs generation with F1-score of 88% and 97.5%, respectively. To illustrate the proposed approach, the generated fault TCs were executed in real time on a hardware-in-the-loop system, where a high-fidelity automotive system model served as a case study. This novel approach offers a means of optimizing the real-time testing process, thereby reducing costs while simultaneously enhancing the safety properties of complex safety-critical ASSs.

</details>


### [75] [Synthesizing Test Cases for Narrowing Specification Candidates](https://arxiv.org/abs/2511.19177)
*Alcino Cunha,Nuno Macedo*

Main category: cs.SE

TL;DR: 提出一种技术，帮助从多个候选形式化规范中选择最佳的一个。该技术生成测试用例套件，用户对测试用例分类后可将候选规范缩小至最多一个。


<details>
  <summary>Details</summary>
Motivation: 在多个候选形式化规范中选择最佳规范是一个挑战，需要有效的方法来区分不同规范的优劣。

Method: 提出两种基于求解器的算法：一种生成最小测试套件，另一种不保证最小性。两种算法都在原型中实现，可用于Alloy规范的选择。

Result: 评估显示，最优算法对许多实际问题足够高效，非最优算法可扩展到数十个候选规范，同时生成合理大小的测试套件。

Conclusion: 该技术为形式化规范选择提供了实用解决方案，两种算法在不同规模问题上都表现出良好的性能。

Abstract: This paper proposes a technique to help choose the best formal specification candidate among a set of alternatives. Given a set of specifications, our technique generates a suite of test cases that, once classified by the user as desirable or not, narrows down the set of candidates to at most one specification. Two alternative solver-based algorithms are proposed, one that generates a minimal test suite, and another that does not ensure minimality. Both algorithms were implemented in a prototype that can be used generate test suites to help choose among alternative Alloy specifications. Our evaluation of this prototype against a large set of problems showed that the optimal algorithm is efficient enough for many practical problems, and that the non-optimal algorithm can scale up to dozens of candidate specifications while still generating reasonably sized test suites.

</details>


### [76] [SLMFix: Leveraging Small Language Models for Error Fixing with Reinforcement Learning](https://arxiv.org/abs/2511.19422)
*David Jiahao Fu,Aryan Gupta,Aaron Councilman,David Grove,Yu-Xiong Wang,Vikram Adve*

Main category: cs.SE

TL;DR: SLMFix是一个代码生成管道，使用强化学习微调的小语言模型来修复大语言模型生成的程序中的语法错误，特别针对低资源编程语言。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型在代码生成中仍存在语法错误，特别是在低资源编程语言上表现不佳，且微调大模型成本高昂。

Method: 采用强化学习微调小语言模型进行程序修复，奖励函数结合静态验证器和静态语义相似度指标。

Result: 实验证明该方法在多个领域特定语言上有效，静态验证器通过率超过95%，在7B模型上优于监督微调方法。

Conclusion: SLMFix展示了作为传统微调方法替代方案的潜力，能显著提升低资源编程语言的代码生成质量。

Abstract: Recent advancements in large language models (LLMs) have shown very impressive capabilities in code generation across many programming languages. However, even state-of-the-art LLMs generate programs that contains syntactic errors and fail to complete the given tasks, especially for low-resource programming languages (LRPLs). In addition, high training cost makes finetuning LLMs unaffordable with constrained computational resources, further undermining the effectiveness of LLMs for code generation. In this work, we propose SLMFix, a novel code generation pipeline that leverages a small language model (SLM) finetuned using reinforcement learning (RL) techniques to fix syntactic errors in LLM-generated programs to improve the quality of LLM-generated programs for domain-specific languages (DSLs). In specific, we applied RL on the SLM for the program repair task using a reward calculated using both a static validator and a static semantic similarity metric. Our experimental results demonstrate the effectiveness and generalizability of our approach across multiple DSLs, achieving more than 95% pass rate on the static validator. Notably, SLMFix brings substantial improvement to the base model and outperforms supervised finetuning approach even for 7B models on a LRPL, showing the potential of our approach as an alternative to traditional finetuning approaches.

</details>


### [77] [Prompt Less, Smile More: MTP with Semantic Engineering in Lieu of Prompt Engineering](https://arxiv.org/abs/2511.19427)
*Jayanaka L. Dantanarayana,Savini Kashmira,Thakee Nathees,Zichen Zhang,Krisztian Flautner,Lingjia Tang,Jason Mars*

Main category: cs.SE

TL;DR: 本文提出语义工程方法，通过在代码中嵌入自然语言上下文来增强程序语义，使LLM系统能更准确反映开发者意图，无需完全手动设计提示。


<details>
  <summary>Details</summary>
Motivation: 现有方法如MTP仅依赖静态代码语义，但许多实际应用需要上下文线索、开发者意图和领域特定推理，这些超出了静态语义的表达能力。

Method: 引入语义工程方法，提出语义上下文注释（SemTexts），允许开发者将自然语言上下文直接嵌入程序结构，集成到Jac编程语言中扩展MTP。

Result: 评估显示语义工程显著提高了提示保真度，性能与提示工程相当，但所需开发者工作量显著减少。

Conclusion: 语义工程是一种轻量级方法，通过丰富程序语义使基于LLM的系统能更准确反映开发者意图，同时减少手动提示设计需求。

Abstract: AI-Integrated programming is emerging as a foundational paradigm for building intelligent systems with large language models (LLMs). Recent approaches such as Meaning Typed Programming (MTP) automate prompt generation by leveraging the semantics already present in code. However, many real-world applications depend on contextual cues, developer intent, and domain-specific reasoning that extend beyond what static code semantics alone can express. To address this limitation, we introduce Semantic Engineering, a lightweight method for enriching program semantics so that LLM-based systems can more accurately reflect developer intent without requiring full manual prompt design. We present Semantic Context Annotations (SemTexts), a language-level mechanism that allows developers to embed natural-language context directly into program constructs. Integrated into the Jac programming language, Semantic Engineering extends MTP to incorporate these enriched semantics during prompt generation. We further introduce a benchmark suite designed to reflect realistic AI-Integrated application scenarios. Our evaluation shows that Semantic Engineering substantially improves prompt fidelity, achieving performance comparable to Prompt Engineering while requiring significantly less developer effort.

</details>
