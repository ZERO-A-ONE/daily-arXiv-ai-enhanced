<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 15]
- [cs.CR](#cs.CR) [Total: 10]
- [cs.AI](#cs.AI) [Total: 20]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [A Survey of AIOps in the Era of Large Language Models](https://arxiv.org/abs/2507.12472)
*Lingzhe Zhang,Tong Jia,Mengxi Jia,Yifan Wu,Aiwei Liu,Yong Yang,Zhonghai Wu,Xuming Hu,Philip S. Yu,Ying Li*

Main category: cs.SE

TL;DR: 该论文综述了大型语言模型（LLMs）在AIOps中的应用，分析了183篇研究论文，探讨了数据来源、任务演变、方法应用及评估方法，并提出了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 随着LLMs的普及，其在AIOps中的应用潜力与局限性尚不明确，需系统性研究以填补这一空白。

Method: 通过分析2020年至2024年的183篇论文，回答了四个关键研究问题，涵盖数据来源、任务演变、方法应用及评估方法。

Result: 总结了LLM4AIOps的最新进展与趋势，指出了现有研究的不足。

Conclusion: 论文为LLM在AIOps中的进一步研究提供了方向，强调了未来探索的潜力。

Abstract: As large language models (LLMs) grow increasingly sophisticated and
pervasive, their application to various Artificial Intelligence for IT
Operations (AIOps) tasks has garnered significant attention. However, a
comprehensive understanding of the impact, potential, and limitations of LLMs
in AIOps remains in its infancy. To address this gap, we conducted a detailed
survey of LLM4AIOps, focusing on how LLMs can optimize processes and improve
outcomes in this domain. We analyzed 183 research papers published between
January 2020 and December 2024 to answer four key research questions (RQs). In
RQ1, we examine the diverse failure data sources utilized, including advanced
LLM-based processing techniques for legacy data and the incorporation of new
data sources enabled by LLMs. RQ2 explores the evolution of AIOps tasks,
highlighting the emergence of novel tasks and the publication trends across
these tasks. RQ3 investigates the various LLM-based methods applied to address
AIOps challenges. Finally, RQ4 reviews evaluation methodologies tailored to
assess LLM-integrated AIOps approaches. Based on our findings, we discuss the
state-of-the-art advancements and trends, identify gaps in existing research,
and propose promising directions for future exploration.

</details>


### [2] [LLM-Powered Quantum Code Transpilation](https://arxiv.org/abs/2507.12480)
*Nazanin Siavash,Armin Moin*

Main category: cs.SE

TL;DR: 利用大型语言模型（LLMs）作为量子软件开发工具包（QSDKs）间的自动转译器，解决跨平台开发中的互操作性问题。


<details>
  <summary>Details</summary>
Motivation: 量子计算平台的多样性导致QSDKs间的互操作性和跨平台开发困难，传统基于规则的转译器设计维护成本高。

Method: 利用LLMs的预训练知识和上下文推理能力，实现编程语言无关的量子程序转译，保持功能等效性。

Result: 提出了一种无需手动定义转换规则的自动化解决方案，提升了量子软件的可移植性。

Conclusion: LLMs为量子计算生态中的智能通用转译提供了可行路径。

Abstract: There exist various Software Development Kits (SDKs) tailored to different
quantum computing platforms. These are known as Quantum SDKs (QSDKs). Examples
include but are not limited to Qiskit, Cirq, and PennyLane. However, this
diversity presents significant challenges for interoperability and
cross-platform development of hybrid quantum-classical software systems.
Traditional rule-based transpilers for translating code between QSDKs are
time-consuming to design and maintain, requiring deep expertise and rigid
mappings in the source and destination code. In this study, we explore the use
of Large Language Models (LLMs) as a flexible and automated solution.
Leveraging their pretrained knowledge and contextual reasoning capabilities, we
position LLMs as programming language-agnostic transpilers capable of
converting quantum programs from one QSDK to another while preserving
functional equivalence. Our approach eliminates the need for manually defined
transformation rules and offers a scalable solution to quantum software
portability. This work represents a step toward enabling intelligent,
general-purpose transpilation in the quantum computing ecosystem.

</details>


### [3] [Kodezi Chronos: A Debugging-First Language Model for Repository-Scale, Memory-Driven Code Understanding](https://arxiv.org/abs/2507.12482)
*Ishraq Khan,Assad Chowdary,Sharoz Haseeb,Urvish Patel*

Main category: cs.SE

TL;DR: Kodezi Chronos是一种新型架构，用于自主代码理解、调试和维护，支持超长上下文，显著提升代码可靠性和生产力。


<details>
  <summary>Details</summary>
Motivation: 现有大型语言模型（LLMs）在代码生成和软件自动化中存在上下文限制和缺乏显式代码结构推理的问题。

Method: 采用多级嵌入内存引擎，结合向量和图索引，实现高效代码检索和推理。

Result: 在真实世界错误检测中表现优于现有模型，提升23%，调试周期减少40%。

Conclusion: Kodezi Chronos为自持、持续优化的软件生态系统迈出重要一步。

Abstract: Large Language Models (LLMs) have advanced code generation and software
automation, but are fundamentally constrained by limited inference-time context
and lack of explicit code structure reasoning. We introduce Kodezi Chronos, a
next-generation architecture for autonomous code understanding, debugging, and
maintenance, designed to operate across ultra-long contexts comprising entire
codebases, histories, and documentation, all without fixed window limits.
Kodezi Chronos leverages a multi-level embedding memory engine, combining
vector and graph-based indexing with continuous code-aware retrieval. This
enables efficient and accurate reasoning over millions of lines of code,
supporting repository-scale comprehension, multi-file refactoring, and
real-time self-healing actions. Our evaluation introduces a novel Multi Random
Retrieval benchmark, specifically tailored to the software engineering domain.
Unlike classical retrieval benchmarks, this method requires the model to
resolve arbitrarily distant and obfuscated associations across code artifacts,
simulating realistic tasks such as variable tracing, dependency migration, and
semantic bug localization. Chronos outperforms prior LLMs and code models,
demonstrating a 23% improvement in real-world bug detection and reducing
debugging cycles by up to 40% compared to traditional sequence-based
approaches. By natively interfacing with IDEs and CI/CD workflows, Chronos
enables seamless, autonomous software maintenance, elevating code reliability
and productivity while reducing manual effort. These results mark a critical
advance toward self-sustaining, continuously optimized software ecosystems.

</details>


### [4] [A Survey of Reinforcement Learning for Software Engineering](https://arxiv.org/abs/2507.12483)
*Dong Wang,Hanmo You,Lingwei Zhu,Kaiwei Lin,Zheng Chen,Chen Yang,Junji Yu,Zan Wang,Junjie Chen*

Main category: cs.SE

TL;DR: 本文是第一篇系统性地综述强化学习（RL）在软件工程（SE）中应用的论文，分析了115篇研究，总结了趋势、算法分类及挑战。


<details>
  <summary>Details</summary>
Motivation: 随着软件系统复杂性和自动化需求的增加，强化学习在软件工程中的应用日益广泛，但缺乏系统性综述。

Method: 回顾了115篇同行评审研究，分析了发表趋势、SE主题和RL算法分类，并考察了数据集使用、模型设计和评估实践。

Result: 提供了RL在SE中的系统映射，总结了当前研究现状，并提出了未来研究方向。

Conclusion: 本文填补了RL-for-SE领域的综述空白，为研究者和实践者提供了指导，并公开了研究资料。

Abstract: Reinforcement Learning (RL) has emerged as a powerful paradigm for sequential
decision-making and has attracted growing interest across various domains,
particularly following the advent of Deep Reinforcement Learning (DRL) in 2015.
Simultaneously, the rapid advancement of Large Language Models (LLMs) has
further fueled interest in integrating RL with LLMs to enable more adaptive and
intelligent systems. In the field of software engineering (SE), the increasing
complexity of systems and the rising demand for automation have motivated
researchers to apply RL to a broad range of tasks, from software design and
development to quality assurance and maintenance. Despite growing research in
RL-for-SE, there remains a lack of a comprehensive and systematic survey of
this evolving field. To address this gap, we reviewed 115 peer-reviewed studies
published across 22 premier SE venues since the introduction of DRL. We
conducted a comprehensive analysis of publication trends, categorized SE topics
and RL algorithms, and examined key factors such as dataset usage, model design
and optimization, and evaluation practices. Furthermore, we identified open
challenges and proposed future research directions to guide and inspire ongoing
work in this evolving area. To summarize, this survey offers the first
systematic mapping of RL applications in software engineering, aiming to
support both researchers and practitioners in navigating the current landscape
and advancing the field. Our artifacts are publicly available:
https://github.com/KaiWei-Lin-lanina/RL4SE.

</details>


### [5] [When Retriever Meets Generator: A Joint Model for Code Comment Generation](https://arxiv.org/abs/2507.12558)
*Tien P. T. Le,Anh M. T. Bui,Huy N. D. Pham,Alessio Bucaioni,Phuong T. Nguyen*

Main category: cs.SE

TL;DR: RAGSum是一种结合检索与生成的代码注释自动生成方法，通过CodeT5骨干网络和对比预训练优化效果，显著优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 解决现有检索增强方法中检索与生成分离导致的噪声问题，提升代码注释生成的效率和准确性。

Method: RAGSum基于CodeT5，结合对比预训练和端到端训练，使用复合损失函数优化检索与生成，并引入轻量级自优化循环。

Result: 在Java、Python、C三种语言的基准测试中，RAGSum在BLEU、METEOR和ROUTE-L指标上显著优于基线方法。

Conclusion: 紧密耦合检索与生成能提升注释自动化的上限，未来可进一步验证和开展开发者研究。

Abstract: Automatically generating concise, informative comments for source code can
lighten documentation effort and accelerate program comprehension.
Retrieval-augmented approaches first fetch code snippets with existing comments
and then synthesize a new comment, yet retrieval and generation are typically
optimized in isolation, allowing irrelevant neighbors topropagate noise
downstream. To tackle the issue, we propose a novel approach named RAGSum with
the aim of both effectiveness and efficiency in recommendations. RAGSum is
built on top offuse retrieval and generation using a single CodeT5 backbone. We
report preliminary results on a unified retrieval-generation framework built on
CodeT5. A contrastive pre-training phase shapes code embeddings for
nearest-neighbor search; these weights then seed end-to-end training with a
composite loss that (i) rewards accurate top-k retrieval; and (ii) minimizes
comment-generation error. More importantly, a lightweight self-refinement loop
is deployed to polish the final output. We evaluated theframework on three
cross-language benchmarks (Java, Python, C), and compared it with three
well-established baselines. The results show that our approach substantially
outperforms thebaselines with respect to BLEU, METEOR, and ROUTE-L. These
findings indicate that tightly coupling retrieval and generationcan raise the
ceiling for comment automation and motivateforthcoming replications and
qualitative developer studies.

</details>


### [6] [ROSE: Transformer-Based Refactoring Recommendation for Architectural Smells](https://arxiv.org/abs/2507.12561)
*Samal Nursapa,Anastassiya Samuilova,Alessio Bucaioni. Phuong T. Nguyen*

Main category: cs.SE

TL;DR: 本文探讨了使用预训练的Transformer模型（CodeBERT和CodeT5）为检测到的架构异味推荐合适的重构方法，CodeT5表现最佳，准确率达96.9%。


<details>
  <summary>Details</summary>
Motivation: 现有工具能检测架构异味（如God Class、Cyclic Dependency等），但很少提供修复建议，影响了软件质量和可维护性。

Method: 将任务定义为三分类问题，并在11,149个开源Java项目的200多万个重构实例上微调CodeBERT和CodeT5。

Result: CodeT5表现优于CodeBERT和传统基线方法，准确率达96.9%，F1分数为95.2%。

Conclusion: 基于Transformer的模型能有效弥合异味检测与可操作修复之间的差距，为未来重构推荐系统奠定基础。

Abstract: Architectural smells such as God Class, Cyclic Dependency, and Hub-like
Dependency degrade software quality and maintainability. Existing tools detect
such smells but rarely suggest how to fix them. This paper explores the use of
pre-trained transformer models--CodeBERT and CodeT5--for recommending suitable
refactorings based on detected smells. We frame the task as a three-class
classification problem and fine-tune both models on over 2 million refactoring
instances mined from 11,149 open-source Java projects. CodeT5 achieves 96.9%
accuracy and 95.2% F1, outperforming CodeBERT and traditional baselines. Our
results show that transformer-based models can effectively bridge the gap
between smell detection and actionable repair, laying the foundation for future
refactoring recommendation systems. We release all code, models, and data under
an open license to support reproducibility and further research.

</details>


### [7] [QSpark: Towards Reliable Qiskit Code Generation](https://arxiv.org/abs/2507.12642)
*Kiana Kheiri,Aamna Aamir,Andriy Miranskyy,Chen Ding*

Main category: cs.SE

TL;DR: 论文研究了如何通过强化学习方法（GRPO和ORPO）优化量子编程中的代码生成，结果显示ORPO在Qiskit HumanEval基准测试中表现最佳，但仍存在高级任务未解决的问题。


<details>
  <summary>Details</summary>
Motivation: 量子电路需要具备容错能力，但现有模型如Granite-20B-Code和StarCoder生成的Qiskit代码存在缺陷，因此需要改进。

Method: 使用两种强化学习方法（GRPO和ORPO）对32B模型进行微调，并利用合成数据集进行训练。

Result: ORPO在Qiskit HumanEval基准测试中达到56.29% Pass@1，GRPO为49%，均优于通用基线模型；但在高级任务中均未成功。

Conclusion: 研究展示了AI辅助量子编程的显著进步，但仍需在高级任务上进一步突破。

Abstract: Quantum circuits must be error-resilient, yet LLMs like Granite-20B-Code and
StarCoder often output flawed Qiskit code. We fine-tuned a 32 B model with two
RL methods, Group Relative Policy Optimization (GRPO) and Odds-Ratio Preference
Optimization (ORPO), using a richly annotated synthetic dataset. On the Qiskit
HumanEval benchmark, ORPO reaches 56.29\% Pass@1 ($\approx+10$ pp over
Granite-8B-QK) and GRPO hits 49\%, both beating all general-purpose baselines;
on the original HumanEval they score 65.90\% and 63.00\%. GRPO excels on basic
tasks (42/54), ORPO on intermediate ones (41/68), and neither solves the five
advanced tasks, highlighting clear gains yet room for progress in AI-assisted
quantum programming.

</details>


### [8] [A Three-Phase Evaluation Approach for new Information and Data Models in the Smart Grid Domain](https://arxiv.org/abs/2507.12649)
*Christine van Stiphoudt,Sergio Potenciano Menci,Gilbert Fridgen*

Main category: cs.SE

TL;DR: 论文提出了一种结合显性和隐性评估方法的三阶段评估框架，用于智能电网中新设计的信息和数据模型。


<details>
  <summary>Details</summary>
Motivation: 现有评估方法在智能电网领域缺乏明确的步骤，且未结合显性和隐性方法，可能导致新模型设计中的潜在问题。

Method: 采用设计科学研究方法，设计了一个三阶段评估框架，结合显性和隐性评估方法，并以工业灵活性描述的信息和数据模型为例进行验证。

Result: 开发了一个可应用于新模型设计的评估框架，并通过案例研究验证了其有效性。

Conclusion: 该评估框架填补了智能电网领域新模型设计评估的空白，并提供了实践经验总结。

Abstract: The ongoing digitalisation of the smart grid is resulting in an increase in
automated information exchanges across distributed energy systems. This process
has led to the development of new information and data models when the existing
ones fall short. To prevent potential disruptions caused by flaws in the newly
designed information and data models, it is essential to evaluate them during
the design process before they are implemented in operation.
  Currently, general explicit evaluation approaches outside the smart grid
domain stay at a high level without defining clear steps. Meanwhile, implicit
evaluation approaches in the smart grid domain focus on testing systems that
utilise information and data models already in use for functionality in terms
of conformance and interoperability. Notably, no combination of explicit and
implicit evaluation approaches for newly designed information and data models
offers a clearly defined set of steps during their design process in the smart
grid context.
  Consequently, we design a three-phase evaluation approach using design
science research to address this gap. Our evaluation approach combines explicit
and implicit evaluation methods and is applicable when developing new
information and data models. We use the development of an information model and
data model focused on industrial flexibility descriptions to refine our
evaluation approach. Additionally, we provide lessons learned from our
experience.

</details>


### [9] [A Fuzzy Approach to Project Success: Measuring What Matters](https://arxiv.org/abs/2507.12653)
*João Granja-Correia,Remedios Hernández-Linares,Luca Ferranti,Arménio Rego*

Main category: cs.SE

TL;DR: 本文提出了一种将模糊逻辑整合到现有结构中的新方法，用于评估项目成功，旨在解决传统Likert量表忽略的上下文依赖性和多面性问题。


<details>
  <summary>Details</summary>
Motivation: 传统Likert量表在评估项目成功时往往忽略其上下文依赖性和多面性，导致结果不够准确。

Method: 采用分层的Type-1 Mamdani模糊系统，优先考虑对最终用户的持续积极影响，减少对次要结果（如利益相关者满意度和内部项目成功）的依赖。

Result: 该方法可能提供更准确的项目成功衡量标准，并适用于复杂评估。

Conclusion: 未来研究将集中于模糊逻辑在社会科学中的实证测试和更广泛应用。

Abstract: This paper introduces a novel approach to project success evaluation by
integrating fuzzy logic into an existing construct. Traditional Likert-scale
measures often overlook the context-dependent and multifaceted nature of
project success. The proposed hierarchical Type-1 Mamdani fuzzy system
prioritizes sustained positive impact for end-users, reducing emphasis on
secondary outcomes like stakeholder satisfaction and internal project success.
This dynamic approach may provide a more accurate measure of project success
and could be adaptable to complex evaluations. Future research will focus on
empirical testing and broader applications of fuzzy logic in social science.

</details>


### [10] [Single Conversation Methodology: A Human-Centered Protocol for AI-Assisted Software Development](https://arxiv.org/abs/2507.12665)
*Salvador D. Escobedo*

Main category: cs.SE

TL;DR: 提出了一种名为SCM的新方法，通过单一对话结构利用LLMs进行软件开发，强调结构化对话和开发者主导。


<details>
  <summary>Details</summary>
Motivation: 当前对LLMs的依赖过于被动，SCM旨在纠正这一问题，重新确立开发者在智能工具中的主导地位。

Method: SCM基于认知清晰性、可追溯性、模块化和文档化原则，定义开发阶段、最佳实践和哲学立场。

Result: SCM提供了一种结构化且持久的开发对话框架，覆盖项目全生命周期。

Conclusion: SCM是对当前LLMs使用方式的必要改进，强调开发者的主动角色。

Abstract: We propose the Single Conversation Methodology (SCM), a novel and pragmatic
approach to software development using large language models (LLMs). In
contrast to ad hoc interactions with generative AI, SCM emphasizes a structured
and persistent development dialogue, where all stages of a project - from
requirements to architecture and implementation - unfold within a single,
long-context conversation. The methodology is grounded on principles of
cognitive clarity, traceability, modularity, and documentation. We define its
phases, best practices, and philosophical stance, while arguing that SCM offers
a necessary correction to the passive reliance on LLMs prevalent in current
practices. We aim to reassert the active role of the developer as architect and
supervisor of the intelligent tool.

</details>


### [11] [Investigating the Performance of Small Language Models in Detecting Test Smells in Manual Test Cases](https://arxiv.org/abs/2507.13035)
*Keila Lucas,Rohit Gheyi,Márcio Ribeiro,Fabio Palomba,Luana Martins,Elvys Soares*

Main category: cs.SE

TL;DR: 研究探讨了小型语言模型（SLMs）在自动检测测试异味中的潜力，评估了Gemma3、Llama3.2和Phi-4在真实测试案例中的表现，Phi-4表现最佳。


<details>
  <summary>Details</summary>
Motivation: 手动测试中的测试异味（如模糊性、冗余等）降低了测试的可靠性和可维护性，现有工具缺乏扩展性。

Method: 评估了三种SLMs（Gemma3、Llama3.2、Phi-4）在143个Ubuntu测试案例中检测七种测试异味的能力。

Result: Phi-4表现最佳，检测成功率97%，其他模型约91%。SLMs还能自主解释问题并提出改进建议。

Conclusion: SLMs是高效工具，能低成本检测测试异味，提升测试质量，且保护数据隐私。

Abstract: Manual testing, in which testers follow natural language instructions to
validate system behavior, remains crucial for uncovering issues not easily
captured by automation. However, these test cases often suffer from test
smells, quality issues such as ambiguity, redundancy, or missing checks that
reduce test reliability and maintainability. While detection tools exist, they
typically require manual rule definition and lack scalability. This study
investigates the potential of Small Language Models (SLMs) for automatically
detecting test smells. We evaluate Gemma3, Llama3.2, and Phi-4 on 143
real-world Ubuntu test cases, covering seven types of test smells. Phi-4
achieved the best results, reaching a pass@2 of 97% in detecting sentences with
test smells, while Gemma3 and Llama3.2 reached approximately 91%. Beyond
detection, SLMs autonomously explained issues and suggested improvements, even
without explicit prompt instructions. They enabled low-cost, concept-driven
identification of diverse test smells without relying on extensive rule
definitions or syntactic analysis. These findings highlight the potential of
SLMs as efficient tools that preserve data privacy and can improve test quality
in real-world scenarios.

</details>


### [12] [iReDev: A Knowledge-Driven Multi-Agent Framework for Intelligent Requirements Development](https://arxiv.org/abs/2507.13081)
*Dongming Jin,Weisong Sun,Jiangping Huang,Peng Liang,Jifeng Xuan,Yang Liu,Zhi Jin*

Main category: cs.SE

TL;DR: iReDev是一个知识驱动的多智能体框架，用于智能需求开发，通过整合人类知识和多智能体协作，显著提升需求开发的效率和质量。


<details>
  <summary>Details</summary>
Motivation: 需求开发是软件工程中的关键阶段，但传统方法耗时且劳动密集。现有研究对需求开发支持有限，且忽视了人类知识与智能体的结合及人机协作。

Method: iReDev由六个知识驱动的智能体组成，支持整个需求开发过程。采用基于事件驱动的通信机制和人工介入机制，实现高效协作。

Result: 评估表明，iReDev在多个方面优于现有基线方法，能快速处理新需求并生成符合利益相关者期望的成果。

Conclusion: iReDev为智能需求开发提供了新方向，未来可进一步探索其应用潜力。

Abstract: Requirements development is a critical phase as it is responsible for
providing a clear understanding of what stakeholders need. It involves
collaboration among stakeholders to extract explicit requirements and address
potential conflicts, which is time-consuming and labor-intensive. Recently,
multi-agent systems for software development have attracted much attention.
However, existing research provides limited support for requirements
development and overlooks the injection of human knowledge into agents and the
human-agent collaboration. % To address these issues, this paper proposes a
knowledge-driven multi-agent framework for intelligent requirement development,
named iReDev. iReDev features: iReDev consists of six knowledge-driven agents
to support the entire requirements development. They collaboratively perform
various tasks to produce a software requirements specification. iReDev focuses
on integrating human knowledge for agents, enabling them to simulate real-world
stakeholders. iReDev uses an event-driven communication mechanism based on an
artifact pool. Agents continuously monitor the pool and autonomously trigger
the next action based on its changes, enabling iReDev to handle new
requirements quickly. iReDev introduces a human-in-the-loop mechanism to
support human-agent collaboration, ensuring that the generated artifacts align
with the expectations of stakeholders. We evaluated the generated artifacts and
results show that iReDev outperforms existing baselines in multiple aspects. We
further envision three key directions and hope this work can facilitate the
development of intelligent requirements development.

</details>


### [13] [A Conceptual Framework for Requirements Engineering of Pretrained-Model-Enabled Systems](https://arxiv.org/abs/2507.13095)
*Dongming Jin,Zhi Jin,Linyu Li,Xiaohong Chen*

Main category: cs.SE

TL;DR: 论文探讨了预训练模型在软件系统中的集成对需求工程传统假设的挑战，提出了一个针对预训练模型驱动系统的需求工程概念框架和研究方向。


<details>
  <summary>Details</summary>
Motivation: 预训练模型的广泛集成带来了独特的特性（如能力边界模糊、上下文依赖行为等），挑战了需求工程的传统假设（如功能可分解性和行为可预测性）。

Method: 提出了一个专门针对预训练模型驱动系统的需求工程概念框架，并概述了相关研究方向。

Result: 为研究人员和实践者提供了应对预训练模型驱动系统需求工程挑战的指导。

Conclusion: 需要重新思考现有需求工程方法，以适应预训练模型驱动系统的特性。

Abstract: Recent advances in large pretrained models have led to their widespread
integration as core components in modern software systems. The trend is
expected to continue in the foreseeable future. Unlike traditional software
systems governed by deterministic logic, systems powered by pretrained models
exhibit distinctive and emergent characteristics, such as ambiguous capability
boundaries, context-dependent behavior, and continuous evolution. These
properties fundamentally challenge long-standing assumptions in requirements
engineering, including functional decomposability and behavioral
predictability. This paper investigates this problem and advocates for a
rethinking of existing requirements engineering methodologies. We propose a
conceptual framework tailored to requirements engineering of
pretrained-model-enabled software systems and outline several promising
research directions within this framework. This vision helps provide a guide
for researchers and practitioners to tackle the emerging challenges in
requirements engineering of pretrained-model-enabled systems.

</details>


### [14] [Inferring Attributed Grammars from Parser Implementations](https://arxiv.org/abs/2507.13117)
*Andreas Pointner,Josef Pichler,Herbert Prähofer*

Main category: cs.SE

TL;DR: 提出一种从递归下降解析器实现中推断属性文法的新方法，以恢复输入处理的语义规范。


<details>
  <summary>Details</summary>
Motivation: 现有语法挖掘技术主要关注语法结构恢复，而输入处理的语义规范仍未被充分探索。

Method: 通过动态分析递归下降解析器的实现，将程序运行时行为映射到文法中，提取并嵌入语义动作。

Result: 实验表明，该方法能通过生成的属性文法准确重现程序行为。

Conclusion: 该方法为输入处理的语义规范恢复提供了可行途径。

Abstract: Software systems that process structured inputs often lack complete and
up-to-date specifications, which specify the input syntax and the semantics of
input processing. While grammar mining techniques have focused on recovering
syntactic structures, the semantics of input processing remains largely
unexplored. In this work, we introduce a novel approach for inferring
attributed grammars from parser implementations. Given an input grammar, our
technique dynamically analyzes the implementation of recursive descent parsers
to reconstruct the semantic aspects of input handling, resulting in
specifications in the form of attributed grammars. By observing program
executions and mapping the program's runtime behavior to the grammar, we
systematically extract and embed semantic actions into the grammar rules. This
enables comprehensive specification recovery. We demonstrate the feasibility of
our approach using an initial set of programs, showing that it can accurately
reproduce program behavior through the generated attributed grammars.

</details>


### [15] [Detecting LLM-generated Code with Subtle Modification by Adversarial Training](https://arxiv.org/abs/2507.13123)
*Xin Yin,Xinrui Li,Chao Ni,Xiaodan Xu,Xiaohu Yang*

Main category: cs.SE

TL;DR: 论文提出CodeGPTSensor+，通过对抗训练提升对修改后LLM生成代码的检测鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 随着LLM生成代码的广泛应用，其引发的代码来源、版权和质量问题日益突出，而现有检测方法对修改后的代码鲁棒性不足。

Method: 提出CodeGPTSensor+，集成对抗样本生成模块MIST，通过多目标标识和结构变换生成高质量对抗样本。

Result: 在HMCorp数据集上，CodeGPTSensor+显著提升对抗测试集的检测准确率，同时保持原测试集的高准确率。

Conclusion: CodeGPTSensor+在检测修改后的LLM生成代码方面表现出优越的鲁棒性。

Abstract: With the rapid development of Large Language Models (LLMs), their powerful
code-generation capabilities have been widely applied in tasks like code
completion and automated development, demonstrating the value of improving
coding efficiency. However, the extensive use of LLM-generated code also raises
several new challenges. On the one hand, issues such as the regulation of code
provenance, copyright disputes, and code quality have become increasingly
concerning. How to effectively detect LLM-generated code and ensure its
compliant and responsible use has become a critical and urgent issue. On the
other hand, in practical applications, LLM-generated code is often subject to
manual modifications, such as variable renaming or structural adjustments.
Although some recent studies have proposed training-based and zero-shot methods
for detecting LLM-generated code, these approaches show insufficient robustness
when facing modified LLM-generated code, and there is a lack of an effective
solution. To address the real-world scenario where LLM-generated code may
undergo minor modifications, we propose CodeGPTSensor+, an enhanced version of
CodeGPTSensor, which employs adversarial training to improve robustness against
input perturbations. CodeGPTSensor+ integrates an adversarial sample generation
module, Multi-objective Identifier and Structure Transformation (MIST), which
systematically generates both high-quality and representative adversarial
samples. This module effectively enhances the model's resistance against
diverse adversarial attacks. Experimental results on the HMCorp dataset
demonstrate that CodeGPTSensor+ significantly improves detection accuracy on
the adversarial test set while maintaining high accuracy on the original test
set, showcasing superior robustness compared to CodeGPTSensor.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [16] [Safeguarding Federated Learning-based Road Condition Classification](https://arxiv.org/abs/2507.12568)
*Sheng Liu,Panos Papadimitratos*

Main category: cs.CR

TL;DR: 论文探讨了联邦学习（FL）在基于摄像头的道路条件分类（RCC）系统中的隐私保护优势及其面临的标签翻转攻击（TLFA）威胁，提出了一种新的防御机制FLARE。


<details>
  <summary>Details</summary>
Motivation: 研究旨在揭示FL-RCC系统对TLFA的脆弱性，并提出有效的防御方法以保障自动驾驶的安全性。

Method: 论文提出了一种基于标签距离的度量标准来量化TLFA的安全风险，并设计了FLARE防御机制，通过神经元级分析减轻攻击影响。

Result: 实验表明TLFA对FL-RCC系统具有严重威胁，而FLARE能有效降低攻击影响。

Conclusion: 论文强调了TLFA对FL-RCC系统的威胁，并验证了FLARE防御机制的有效性。

Abstract: Federated Learning (FL) has emerged as a promising solution for
privacy-preserving autonomous driving, specifically camera-based Road Condition
Classification (RCC) systems, harnessing distributed sensing, computing, and
communication resources on board vehicles without sharing sensitive image data.
However, the collaborative nature of FL-RCC frameworks introduces new
vulnerabilities: Targeted Label Flipping Attacks (TLFAs), in which malicious
clients (vehicles) deliberately alter their training data labels to compromise
the learned model inference performance. Such attacks can, e.g., cause a
vehicle to mis-classify slippery, dangerous road conditions as pristine and
exceed recommended speed. However, TLFAs for FL-based RCC systems are largely
missing. We address this challenge with a threefold contribution: 1) we
disclose the vulnerability of existing FL-RCC systems to TLFAs; 2) we introduce
a novel label-distance-based metric to precisely quantify the safety risks
posed by TLFAs; and 3) we propose FLARE, a defensive mechanism leveraging
neuron-wise analysis of the output layer to mitigate TLFA effects. Extensive
experiments across three RCC tasks, four evaluation metrics, six baselines, and
three deep learning models demonstrate both the severity of TLFAs on FL-RCC
systems and the effectiveness of FLARE in mitigating the attack impact.

</details>


### [17] [On the Consideration of Vanity Address Generation via Identity-Based Signatures](https://arxiv.org/abs/2507.12670)
*Shogo Murasaki,Kazumasa Omote,Keita Emura*

Main category: cs.CR

TL;DR: 探索基于身份的签名（IBS）是否可用于生成区块链中的虚荣地址，并通过ECDSA与密钥恢复构建IBS方案，实现低成本连接任意字符串与地址。


<details>
  <summary>Details</summary>
Motivation: 解决传统试错法生成虚荣地址的字符限制问题，同时避免直接替换ECDSA带来的不合理成本。

Method: 利用IBS的功能性，从ECDSA与密钥恢复构建IBS方案，并通过Solidity实现系统。

Result: 系统实现表明，其燃气成本与ECDSA签名验证几乎相同。

Conclusion: IBS方案为生成虚荣地址提供了可行且低成本的替代方案，无需直接替换现有ECDSA机制。

Abstract: An address is indicated as an identifier of the user on the blockchain, and
is defined by a hash value of the ECDSA verification key. A vanity address is
an address that embeds custom characters such as a name. To generate a vanity
address, a classical try-and-error method is employed, and thus the number of
characters to be embedded is limited. In this paper, we focus on the
functionality of identity-based signatures (IBS) where any strings can be
employed as a verification key, and explore whether IBS can be used for
generating a vanity address. We attach importance to the fact that it is not
realistic to replace ECDSA with key recovery, which is currently employed for
issuing transactions in Ethereum, to an IBS scheme. Even if this replacement is
possible, it is not a reasonable price for the ease of the vanity address
generation. Thus, we pay attention to a generic construction of IBS from
signatures, and construct an IBS scheme from ECDSA with key recovery. Though we
cannot directly generate a vanity address due to the key recovery functionality
of the underlying ECDSA, we can connect any string with an address due to the
functionality of IBS that can give additional meaning to the address. We
implement our system by Solidity, and demonstrate that the gas cost is almost
same as that of the ECDSA signature verification.

</details>


### [18] [Architectural Backdoors in Deep Learning: A Survey of Vulnerabilities, Detection, and Defense](https://arxiv.org/abs/2507.12919)
*Victoria Childress,Josh Collyer,Jodie Knapp*

Main category: cs.CR

TL;DR: 本文综述了深度神经网络中的架构后门威胁，探讨了其与传统攻击的区别、检测与防御策略的局限性，并提出了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 架构后门是一种新型威胁，直接嵌入模型的计算图中，难以通过传统方法检测和防御，亟需系统研究以应对这一挑战。

Method: 综述了编译器级操作、污染的AutoML流程和供应链漏洞等架构后门研究，评估了静态图检查、动态模糊测试和部分形式验证等防御策略。

Result: 尽管已有进展，但针对分布式或隐蔽触发器的可扩展防御仍不足。

Conclusion: 未来需加强供应链安全、密码学模型认证和下一代基准测试，以全面防御架构后门威胁。

Abstract: Architectural backdoors pose an under-examined but critical threat to deep
neural networks, embedding malicious logic directly into a model's
computational graph. Unlike traditional data poisoning or parameter
manipulation, architectural backdoors evade standard mitigation techniques and
persist even after clean retraining. This survey systematically consolidates
research on architectural backdoors, spanning compiler-level manipulations,
tainted AutoML pipelines, and supply-chain vulnerabilities. We assess emerging
detection and defense strategies, including static graph inspection, dynamic
fuzzing, and partial formal verification, and highlight their limitations
against distributed or stealth triggers. Despite recent progress, scalable and
practical defenses remain elusive. We conclude by outlining open challenges and
proposing directions for strengthening supply-chain security, cryptographic
model attestations, and next-generation benchmarks. This survey aims to guide
future research toward comprehensive defenses against structural backdoor
threats in deep learning systems.

</details>


### [19] [Enterprise Security Incident Analysis and Countermeasures Based on the T-Mobile Data Breach](https://arxiv.org/abs/2507.12937)
*Zhuohan Cui,Zikun Song*

Main category: cs.CR

TL;DR: 分析T-Mobile 2021和2023年数据泄露事件，提出多层防御策略，验证其成本效益。


<details>
  <summary>Details</summary>
Motivation: 揭示T-Mobile数据泄露后的结构性弱点，为大型电信公司提供安全改进方案。

Method: 结合案例漏洞评估和主动伦理黑客技术（如Shodan侦察、API滥用模拟等），提出多层防御策略。

Result: 发现结构性弱点，防御策略五年投资成本低于预期损失的1.1%。

Conclusion: 为电信公司提供可操作的安全蓝图，提升运营韧性和合规性。

Abstract: This paper presents a comprehensive analysis of T-Mobile's critical data
breaches in 2021 and 2023, alongside a full-spectrum security audit targeting
its systems, infrastructure, and publicly exposed endpoints. By combining
case-based vulnerability assessments with active ethical hacking
techniques--including Shodan reconnaissance, API misuse simulations, VNC
brute-forcing, firmware reverse engineering, and web application scans--we
uncover structural weaknesses persisting beyond the initial breach events.
Building on these findings, we propose a multi-layered defensive strategy
encompassing Zero Trust Architecture, granular role-based access control,
network segmentation, firmware encryption using AES with integrity checks, and
API rate limiting and token lifecycle control. Financial modelling demonstrates
that a five-year investment yields less than 1.1% of expected breach losses,
validating the cost-effectiveness of proactive security measures. Our work
bridges post-incident forensic analysis with hands-on security evaluation,
providing an actionable blueprint for large-scale telecoms seeking operational
resilience, regulatory compliance, and cross-domain threat readiness.

</details>


### [20] [Measuring CEX-DEX Extracted Value and Searcher Profitability: The Darkest of the MEV Dark Forest](https://arxiv.org/abs/2507.13023)
*Fei Wu,Danning Sui,Thomas Thiery,Mallesh Pai*

Main category: cs.CR

TL;DR: 论文分析了以太坊上中心化与去中心化交易所（CEX-DEX）套利的经济学与动态，提出了一种从链上数据识别套利交易的启发式方法，并估算了套利收益。研究发现19个月内套利者提取了2.338亿美元，且市场呈现集中化趋势。


<details>
  <summary>Details</summary>
Motivation: 研究CEX-DEX套利的经济学与动态，揭示套利市场的集中化趋势及其对以太坊去中心化的影响。

Method: 通过链上数据识别套利交易，构建实证框架估算套利收益，分析19个月的数据集。

Result: 19个月内套利者提取了2.338亿美元，市场集中化明显，前三名套利者占据了75%的份额。

Conclusion: CEX-DEX套利揭示了MEV生态的集中化问题，对以太坊的去中心化具有重要影响。

Abstract: This paper provides a comprehensive empirical analysis of the economics and
dynamics behind arbitrages between centralized and decentralized exchanges
(CEX-DEX) on Ethereum. We refine heuristics to identify arbitrage transactions
from on-chain data and introduce a robust empirical framework to estimate
arbitrage revenue without knowing traders' actual behaviors on CEX. Leveraging
an extensive dataset spanning 19 months from August 2023 to March 2025, we
estimate a total of 233.8M USD extracted by 19 major CEX-DEX searchers from
7,203,560 identified CEX-DEX arbitrages. Our analysis reveals increasing
centralization trends as three searchers captured three-quarters of both volume
and extracted value. We also demonstrate that searchers' profitability is tied
to their integration level with block builders and uncover exclusive
searcher-builder relationships and their market impact. Finally, we correct the
previously underestimated profitability of block builders who vertically
integrate with a searcher. These insights illuminate the darkest corner of the
MEV landscape and highlight the critical implications of CEX-DEX arbitrages for
Ethereum's decentralization.

</details>


### [21] [From Paranoia to Compliance: The Bumpy Road of System Hardening Practices on Stack Exchange](https://arxiv.org/abs/2507.13028)
*Niklas Busch,Philip Klostermeyer,Jan H. Klemmer,Yasemin Acar,Sascha Fahl*

Main category: cs.CR

TL;DR: 研究分析了316篇Stack Exchange帖子，探讨系统管理员在系统加固中的动机、实践和挑战，发现访问控制和部署问题是主要难点，并提出了未来改进建议。


<details>
  <summary>Details</summary>
Motivation: 理解系统管理员在系统加固中的动机、实践和挑战，以填补研究空白并提升系统安全性。

Method: 定性分析了316篇与系统加固相关的Stack Exchange帖子。

Result: 访问控制和部署问题最具挑战性，系统管理员存在误解和不切实际的期望；操作系统和服务器应用是主要关注点；动机多为安全担忧或合规要求。

Conclusion: 研究揭示了系统加固的实践与挑战，并提出了未来改进建议，对提升系统安全性具有指导意义。

Abstract: Hardening computer systems against cyberattacks is crucial for security.
However, past incidents illustrated, that many system operators struggle with
effective system hardening. Hence, many computer systems and applications
remain insecure. So far, the research community lacks an in-depth understanding
of system operators motivation, practices, and challenges around system
hardening. With a focus on practices and challenges, we qualitatively analyzed
316 Stack Exchange (SE) posts related to system hardening. We find that access
control and deployment-related issues are the most challenging, and system
operators suffer from misconceptions and unrealistic expectations. Most
frequently, posts focused on operating systems and server applications. System
operators were driven by the fear of their systems getting attacked or by
compliance reasons. Finally, we discuss our research questions, make
recommendations for future system hardening, and illustrate the implications of
our work.

</details>


### [22] [MAD-Spear: A Conformity-Driven Prompt Injection Attack on Multi-Agent Debate Systems](https://arxiv.org/abs/2507.13038)
*Yu Cui,Hongyang Du*

Main category: cs.CR

TL;DR: MAD-Spear是一种针对多智能体辩论系统的提示注入攻击，通过操纵少数智能体传播错误信息，显著降低系统性能。研究发现智能体多样性在数学推理任务中能提升系统表现。


<details>
  <summary>Details</summary>
Motivation: 多智能体辩论（MAD）系统的安全性问题尚未得到充分研究，MAD-Spear攻击揭示了其潜在的脆弱性。

Method: 提出MAD-Spear攻击方法，通过操纵智能体生成错误响应，并与其他攻击策略结合。同时，定义了MAD的容错性并开发了评估框架。

Result: 实验表明MAD-Spear能显著降低系统性能，且智能体多样性在数学推理任务中提升了表现。

Conclusion: 研究强调了MAD系统安全设计的紧迫性，并挑战了智能体多样性对性能影响有限的先前观点。

Abstract: Multi-agent debate (MAD) systems leverage collaborative interactions among
large language models (LLMs) agents to improve reasoning capabilities. While
recent studies have focused on increasing the accuracy and scalability of MAD
systems, their security vulnerabilities have received limited attention. In
this work, we introduce MAD-Spear, a targeted prompt injection attack that
compromises a small subset of agents but significantly disrupts the overall MAD
process. Manipulated agents produce multiple plausible yet incorrect responses,
exploiting LLMs' conformity tendencies to propagate misinformation and degrade
consensus quality. Furthermore, the attack can be composed with other
strategies, such as communication attacks, to further amplify its impact by
increasing the exposure of agents to incorrect responses. To assess MAD's
resilience under attack, we propose a formal definition of MAD fault-tolerance
and develop a comprehensive evaluation framework that jointly considers
accuracy, consensus efficiency, and scalability. Extensive experiments on five
benchmark datasets with varying difficulty levels demonstrate that MAD-Spear
consistently outperforms the baseline attack in degrading system performance.
Additionally, we observe that agent diversity substantially improves MAD
performance in mathematical reasoning tasks, which challenges prior work
suggesting that agent diversity has minimal impact on performance. These
findings highlight the urgent need to improve the security in MAD design.

</details>


### [23] [Backscattering-Based Security in Wireless Power Transfer Applied to Battery-Free BLE Sensors](https://arxiv.org/abs/2507.13042)
*Taki Eddine Djidjekh,Gaël Loubet,Alexandru Takacs*

Main category: cs.CR

TL;DR: 本文提出了一种基于反向散射的安全机制，将其集成到蓝牙低功耗无电池无线传感器网络中，解决了安全和能源效率的挑战。


<details>
  <summary>Details</summary>
Motivation: 物联网系统中安全和能源效率的集成是一个关键挑战，尤其是对于无电池和资源受限设备。

Method: 利用无线能量传输链路生成额外的识别信号，不增加能耗或计算需求，并通过实验验证了其功能。

Result: 实验验证了该解决方案在紧凑、低增益天线下的功能，适用于结构健康监测和智能交通等场景。

Conclusion: 反向散射安全机制为安全、可持续和可扩展的物联网部署提供了潜力。

Abstract: The integration of security and energy efficiency in Internet of Things
systems remains a critical challenge, particularly for battery-free and
resource-constrained devices. This paper explores the scalability and
protocol-agnostic nature of a backscattering-based security mechanism by
integrating it into Bluetooth Low Energy battery-free Wireless Sensor Network.
The proposed approach leverages the Wireless Power Transfer link, traditionally
used for energy harvesting, to generate additional identification signals
without increasing energy consumption or computational demands. Experimental
validation demonstrates the solution's functionality using compact, low-gain
antenna, ensuring compatibility with size-constrained applications such as
Structural Health Monitoring and smart transport. Furthermore, this work
addresses the challenges associated with backscattering dynamic range and
multi-node Wireless Sensor Network scenarios, discussing potential collisions
between identification signals and proposing future improvements to enhance
generalizability and scalability. The findings underscore the potential of the
backscattering-based security mechanism for creating secure, sustainable, and
scalable IoT deployments across diverse protocols and applications.

</details>


### [24] [Prompt Injection 2.0: Hybrid AI Threats](https://arxiv.org/abs/2507.13169)
*Jeremy McHugh,Kristina Šekrst,Jon Cefalu*

Main category: cs.CR

TL;DR: 论文分析了Prompt Injection 2.0攻击，探讨其如何结合传统网络安全漏洞（如XSS、CSRF）绕过安全措施，并提出结合提示隔离、运行时安全和权限分离的解决方案。


<details>
  <summary>Details</summary>
Motivation: 随着AI代理系统的普及，提示注入攻击与网络安全漏洞结合形成新型威胁，传统安全措施失效，亟需新的防御方法。

Method: 基于Preamble的研究，评估现有防御技术对新型威胁（如AI蠕虫、多代理感染）的效果，并提出结合提示隔离和运行时安全的架构方案。

Result: 研究表明传统防火墙、XSS过滤器和CSRF令牌对AI增强攻击无效，新架构能有效检测和防御混合威胁。

Conclusion: 提示注入攻击已演变为复杂威胁，需结合AI和网络安全技术的新型防御方案。

Abstract: Prompt injection attacks, where malicious input is designed to manipulate AI
systems into ignoring their original instructions and following unauthorized
commands instead, were first discovered by Preamble, Inc. in May 2022 and
responsibly disclosed to OpenAI. Over the last three years, these attacks have
continued to pose a critical security threat to LLM-integrated systems. The
emergence of agentic AI systems, where LLMs autonomously perform multistep
tasks through tools and coordination with other agents, has fundamentally
transformed the threat landscape. Modern prompt injection attacks can now
combine with traditional cybersecurity exploits to create hybrid threats that
systematically evade traditional security controls. This paper presents a
comprehensive analysis of Prompt Injection 2.0, examining how prompt injections
integrate with Cross-Site Scripting (XSS), Cross-Site Request Forgery (CSRF),
and other web security vulnerabilities to bypass traditional security measures.
We build upon Preamble's foundational research and mitigation technologies,
evaluating them against contemporary threats, including AI worms, multi-agent
infections, and hybrid cyber-AI attacks. Our analysis incorporates recent
benchmarks that demonstrate how traditional web application firewalls, XSS
filters, and CSRF tokens fail against AI-enhanced attacks. We also present
architectural solutions that combine prompt isolation, runtime security, and
privilege separation with novel threat detection capabilities.

</details>


### [25] [A Crowdsensing Intrusion Detection Dataset For Decentralized Federated Learning Models](https://arxiv.org/abs/2507.13313)
*Chao Feng,Alberto Huertas Celdran,Jing Han,Heqing Ren,Xi Cheng,Zien Zeng,Lucas Krauter,Gerome Bovet,Burkhard Stiller*

Main category: cs.CR

TL;DR: 论文介绍了用于物联网众感知恶意软件检测的去中心化联邦学习（DFL）数据集和实验研究，比较了传统机器学习、集中式联邦学习和DFL的性能。


<details>
  <summary>Details</summary>
Motivation: 研究物联网众感知环境中的恶意软件检测，同时保护数据隐私和本地性。

Method: 收集了21,582,484条原始行为记录，聚合为342,106个特征，并在DFL平台上进行实验比较。

Result: DFL在保持数据本地性的同时表现优于集中式联邦学习。

Conclusion: 该数据集为物联网众感知环境的安全研究提供了基础。

Abstract: This paper introduces a dataset and experimental study for decentralized
federated learning (DFL) applied to IoT crowdsensing malware detection. The
dataset comprises behavioral records from benign and eight malware families. A
total of 21,582,484 original records were collected from system calls, file
system activities, resource usage, kernel events, input/output events, and
network records. These records were aggregated into 30-second windows,
resulting in 342,106 features used for model training and evaluation.
Experiments on the DFL platform compare traditional machine learning (ML),
centralized federated learning (CFL), and DFL across different node counts,
topologies, and data distributions. Results show that DFL maintains competitive
performance while preserving data locality, outperforming CFL in most settings.
This dataset provides a solid foundation for studying the security of IoT
crowdsensing environments.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [26] [AI-Powered Math Tutoring: Platform for Personalized and Adaptive Education](https://arxiv.org/abs/2507.12484)
*Jarosław A. Chudziak,Adam Kostka*

Main category: cs.AI

TL;DR: 本文介绍了一个新型多智能体AI辅导平台，通过结合自适应个性化反馈、结构化课程生成和教科书知识检索，实现模块化、工具辅助的数学学习过程，解决了当前AI辅导系统过于被动、缺乏深度思考引导的问题。


<details>
  <summary>Details</summary>
Motivation: 当前AI辅导系统存在被动响应的局限性，往往直接提供答案而不鼓励深度思考，也缺乏结构化的教学工具和策略，在数学教育领域尤其不够发达。需要开发能够超越被动辅助，提供结构化、个性化和工具辅助学习体验的AI辅导系统。

Method: 开发了一个新型多智能体AI辅导平台，结合了自适应个性化反馈、结构化课程生成和教科书知识检索功能，将教学智能体和AI驱动组件相结合，实现模块化的工具辅助学习过程。

Result: 该系统能够让学生在学习新主题的同时识别和针对性改善自己的薄弱环节，有效进行考试复习，并在无限数量的个性化练习中进行实践，实现了有效的数学教学。

Conclusion: 通过引入结合教学智能体和AI驱动组件的新型平台，为人工智能教育领域贡献了模块化且有效的数学教学系统，推进了AI辅导系统从被动响应向主动、结构化教学的转变。

Abstract: The growing ubiquity of artificial intelligence (AI), in particular large
language models (LLMs), has profoundly altered the way in which learners gain
knowledge and interact with learning material, with many claiming that AI
positively influences their learning achievements. Despite this advancement,
current AI tutoring systems face limitations associated with their reactive
nature, often providing direct answers without encouraging deep reflection or
incorporating structured pedagogical tools and strategies. This limitation is
most apparent in the field of mathematics, in which AI tutoring systems remain
underdeveloped. This research addresses the question: How can AI tutoring
systems move beyond providing reactive assistance to enable structured,
individualized, and tool-assisted learning experiences? We introduce a novel
multi-agent AI tutoring platform that combines adaptive and personalized
feedback, structured course generation, and textbook knowledge retrieval to
enable modular, tool-assisted learning processes. This system allows students
to learn new topics while identifying and targeting their weaknesses, revise
for exams effectively, and practice on an unlimited number of personalized
exercises. This article contributes to the field of artificial intelligence in
education by introducing a novel platform that brings together pedagogical
agents and AI-driven components, augmenting the field with modular and
effective systems for teaching mathematics.

</details>


### [27] [MR-LDM -- The Merge-Reactive Longitudinal Decision Model: Game Theoretic Human Decision Modeling for Interactive Sim Agents](https://arxiv.org/abs/2507.12494)
*Dustin Holley,Jovin D'sa,Hossein Nourkhiz Mahjoub,Gibran Ali*

Main category: cs.AI

TL;DR: 论文提出了一种基于博弈论的高速公路合流场景战术决策模型，改进了收益函数和滞后动作，结合动力学模型，实现了更真实、可解释的交互模拟。


<details>
  <summary>Details</summary>
Motivation: 提升仿真环境以模拟真实驾驶员行为，支持自动驾驶技术开发。

Method: 采用博弈论模型改进战术决策，结合动力学模型，形成统一的决策与动力学模型。

Result: 模型在真实数据集上验证了复杂交互的再现性，计算效率适用于大规模仿真。

Conclusion: 提出的模型在高速公路合流场景中实现了更真实、可解释的交互模拟，适用于自动驾驶开发。

Abstract: Enhancing simulation environments to replicate real-world driver behavior,
i.e., more humanlike sim agents, is essential for developing autonomous vehicle
technology. In the context of highway merging, previous works have studied the
operational-level yielding dynamics of lag vehicles in response to a merging
car at highway on-ramps. Other works focusing on tactical decision modeling
generally consider limited action sets or utilize payoff functions with large
parameter sets and limited payoff bounds. In this work, we aim to improve the
simulation of the highway merge scenario by targeting a game theoretic model
for tactical decision-making with improved payoff functions and lag actions. We
couple this with an underlying dynamics model to have a unified decision and
dynamics model that can capture merging interactions and simulate more
realistic interactions in an explainable and interpretable fashion. The
proposed model demonstrated good reproducibility of complex interactions when
validated on a real-world dataset. The model was finally integrated into a high
fidelity simulation environment and confirmed to have adequate computation time
efficiency for use in large-scale simulations to support autonomous vehicle
development.

</details>


### [28] [A Survey of Explainable Reinforcement Learning: Targets, Methods and Needs](https://arxiv.org/abs/2507.12599)
*Léo Saulières*

Main category: cs.AI

TL;DR: 本文提出了一种基于“What”和“How”问题的直观分类法，用于解释强化学习（XRL）领域的方法，并综述了250多篇论文。同时指出了XRL领域的研究需求和未来方向。


<details>
  <summary>Details</summary>
Motivation: 由于深度神经网络的内部机制不透明，理解AI模型的输出成为挑战。本文旨在通过XRL解释强化学习智能体的行为。

Method: 提出基于“What”（解释目标）和“How”（解释方式）的分类法，并用于综述250多篇XRL相关论文。

Result: 分类法为XRL领域提供了清晰的框架，并识别了相关领域的研究需求。

Conclusion: XRL领域需要更多关注，分类法为未来研究提供了方向。

Abstract: The success of recent Artificial Intelligence (AI) models has been
accompanied by the opacity of their internal mechanisms, due notably to the use
of deep neural networks. In order to understand these internal mechanisms and
explain the output of these AI models, a set of methods have been proposed,
grouped under the domain of eXplainable AI (XAI). This paper focuses on a
sub-domain of XAI, called eXplainable Reinforcement Learning (XRL), which aims
to explain the actions of an agent that has learned by reinforcement learning.
We propose an intuitive taxonomy based on two questions "What" and "How". The
first question focuses on the target that the method explains, while the second
relates to the way the explanation is provided. We use this taxonomy to provide
a state-of-the-art review of over 250 papers. In addition, we present a set of
domains close to XRL, which we believe should get attention from the community.
Finally, we identify some needs for the field of XRL.

</details>


### [29] [Fly, Fail, Fix: Iterative Game Repair with Reinforcement Learning and Large Multimodal Models](https://arxiv.org/abs/2507.12666)
*Alex Zook,Josef Spjut,Jonathan Tremblay*

Main category: cs.AI

TL;DR: 论文提出了一种结合强化学习（RL）和多模态模型（LMM）的自动化游戏设计框架，通过RL玩家测试游戏并生成行为数据，LMM根据这些数据调整游戏设计。


<details>
  <summary>Details</summary>
Motivation: 现代生成系统难以从静态规则和内容中捕捉动态玩家行为，因此需要一种能自动迭代设计的方法。

Method: 框架通过RL玩家生成游戏行为数据（数值指标或图像摘要），LMM分析数据并调整游戏配置以优化玩家行为。

Result: 实验证明LMM能基于RL生成的行为数据迭代优化游戏机制。

Conclusion: 该框架为AI辅助游戏设计提供了实用且可扩展的工具。

Abstract: Game design hinges on understanding how static rules and content translate
into dynamic player behavior - something modern generative systems that inspect
only a game's code or assets struggle to capture. We present an automated
design iteration framework that closes this gap by pairing a reinforcement
learning (RL) agent, which playtests the game, with a large multimodal model
(LMM), which revises the game based on what the agent does. In each loop the RL
player completes several episodes, producing (i) numerical play metrics and/or
(ii) a compact image strip summarising recent video frames. The LMM designer
receives a gameplay goal and the current game configuration, analyses the play
traces, and edits the configuration to steer future behaviour toward the goal.
We demonstrate results that LMMs can reason over behavioral traces supplied by
RL agents to iteratively refine game mechanics, pointing toward practical,
scalable tools for AI-assisted game design.

</details>


### [30] [Benchmarking Deception Probes via Black-to-White Performance Boosts](https://arxiv.org/abs/2507.12691)
*Avi Parrack,Carlo Leonardo Attubato,Stefan Heimersheim*

Main category: cs.AI

TL;DR: 论文探讨了AI助手欺骗性回应的检测方法，比较了白盒和黑盒监控的效果，发现现有欺骗探针有一定效果但提升有限。


<details>
  <summary>Details</summary>
Motivation: 研究如何有效检测AI助手的欺骗性回应，并评估现有欺骗探针的实用性及其对抗策略的鲁棒性。

Method: 通过比较白盒监控（访问令牌级探针激活）和黑盒监控（无访问）的效果，评估欺骗探针的性能。

Result: 发现现有欺骗探针在白盒监控下表现略优于黑盒监控，但提升效果较弱。

Conclusion: 现有欺骗探针虽有一定效果，但需进一步改进以应对欺骗性助手的规避策略。

Abstract: AI assistants will occasionally respond deceptively to user queries.
Recently, linear classifiers (called "deception probes") have been trained to
distinguish the internal activations of a language model during deceptive
versus honest responses. However, it's unclear how effective these probes are
at detecting deception in practice, nor whether such probes are resistant to
simple counter strategies from a deceptive assistant who wishes to evade
detection. In this paper, we compare white-box monitoring (where the monitor
has access to token-level probe activations) to black-box monitoring (without
such access). We benchmark deception probes by the extent to which the white
box monitor outperforms the black-box monitor, i.e. the black-to-white
performance boost. We find weak but encouraging black-to-white performance
boosts from existing deception probes.

</details>


### [31] [Imitating Mistakes in a Learning Companion AI Agent for Online Peer Learning](https://arxiv.org/abs/2507.12801)
*Sosui Moribe,Taketoshi Ushiama*

Main category: cs.AI

TL;DR: 研究开发AI学习伙伴以支持同伴学习，验证同水平同伴在英语写作中的有效性。


<details>
  <summary>Details</summary>
Motivation: 同伴学习虽有效但存在限制，需同水平同伴。研究旨在开发AI伙伴实现随时随地同伴学习。

Method: 假设同水平同伴会犯相同错误，以英语写作为例验证。

Result: 未明确提及具体结果，但验证了同水平同伴的重要性。

Conclusion: AI学习伙伴有望解决同伴学习限制，提升学习效果。

Abstract: In recent years, peer learning has gained attention as a method that promotes
spontaneous thinking among learners, and its effectiveness has been confirmed
by numerous studies. This study aims to develop an AI Agent as a learning
companion that enables peer learning anytime and anywhere. However, peer
learning between humans has various limitations, and it is not always
effective. Effective peer learning requires companions at the same proficiency
levels. In this study, we assume that a learner's peers with the same
proficiency level as the learner make the same mistakes as the learner does and
focus on English composition as a specific example to validate this approach.

</details>


### [32] [MCPEval: Automatic MCP-based Deep Evaluation for AI Agent Models](https://arxiv.org/abs/2507.12806)
*Zhiwei Liu,Jielin Qiu,Shiyu Wang,Jianguo Zhang,Zuxin Liu,Roshan Ram,Haolin Chen,Weiran Yao,Huan Wang,Shelby Heinecke,Silvio Savarese,Caiming Xiong*

Main category: cs.AI

TL;DR: MCPEval是一个基于模型上下文协议的开源框架，用于自动化生成任务和深度评估LLM智能代理，解决了现有静态基准和人工数据收集的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有评估方法依赖静态基准和人工数据收集，限制了实际评估的效率和范围，因此需要一种更高效、自动化的评估框架。

Method: MCPEval采用模型上下文协议（MCP），自动化生成端到端任务并深度评估LLM代理，支持多领域标准化指标和原生工具集成。

Result: 在五个实际领域中的实证结果表明，MCPEval能有效揭示领域特定的性能差异。

Conclusion: MCPEval通过开源发布，推动了LLM代理评估的可重复性和标准化。

Abstract: The rapid rise of Large Language Models (LLMs)-based intelligent agents
underscores the need for robust, scalable evaluation frameworks. Existing
methods rely on static benchmarks and labor-intensive data collection, limiting
practical assessment. We introduce \oursystemname, an open-source Model Context
Protocol (MCP)-based framework that automates end-to-end task generation and
deep evaluation of LLM agents across diverse domains. MCPEval standardizes
metrics, seamlessly integrates with native agent tools, and eliminates manual
effort in building evaluation pipelines. Empirical results across five
real-world domains show its effectiveness in revealing nuanced, domain-specific
performance. We publicly release MCPEval
https://github.com/SalesforceAIResearch/MCPEval to promote reproducible and
standardized LLM agent evaluation.

</details>


### [33] [Emotional Support with LLM-based Empathetic Dialogue Generation](https://arxiv.org/abs/2507.12820)
*Shiquan Wang,Ruiyu Fang,Zhongjiang He,Shuangyong Song,Yongxiang Li*

Main category: cs.AI

TL;DR: 本文介绍了利用大规模语言模型结合提示工程和微调技术，为NLPCC 2025任务8（情感支持对话）提供的解决方案，取得了竞赛第二名的成绩。


<details>
  <summary>Details</summary>
Motivation: 满足心理健康支持的需求，提供共情且有效的情感支持对话。

Method: 结合提示工程和微调技术，探索参数高效的Low-Rank Adaptation和全参数微调策略。

Result: 最佳模型在竞赛中排名第二，验证了结合LLMs与有效适应方法的潜力。

Conclusion: 未来工作将进一步提升情感理解和回答个性化，以构建更实用可靠的情感支持系统。

Abstract: Emotional Support Conversation (ESC) aims to provide empathetic and effective
emotional assistance through dialogue, addressing the growing demand for mental
health support. This paper presents our solution for the NLPCC 2025 Task 8 ESC
evaluation, where we leverage large-scale language models enhanced by prompt
engineering and finetuning techniques. We explore both parameter-efficient
Low-Rank Adaptation and full-parameter fine-tuning strategies to improve the
model's ability to generate supportive and contextually appropriate responses.
Our best model ranked second in the competition, highlighting the potential of
combining LLMs with effective adaptation methods for ESC tasks. Future work
will focus on further enhancing emotional understanding and response
personalization to build more practical and reliable emotional support systems.

</details>


### [34] [Assessing adaptive world models in machines with novel games](https://arxiv.org/abs/2507.12821)
*Lance Ying,Katherine M. Collins,Prafull Sharma,Cedric Colas,Kaiya Ivy Zhao,Adrian Weller,Zenna Tavares,Phillip Isola,Samuel J. Gershman,Jacob D. Andreas,Thomas L. Griffiths,Francois Chollet,Kelsey R. Allen,Joshua B. Tenenbaum*

Main category: cs.AI

TL;DR: 论文提出世界模型归纳的概念，呼吁通过新颖游戏设计的新评估框架来评估AI的适应性世界模型。


<details>
  <summary>Details</summary>
Motivation: 人类智能的快速适应能力与高效构建和优化环境内部表征（世界模型）相关，而当前AI对世界模型的理解和评估过于静态，缺乏对交互和探索中学习效率的关注。

Method: 借鉴认知科学研究，提出基于新颖游戏设计的评估框架，强调游戏结构的深度和持续新颖性，并设计相应指标。

Result: 提出了一种新的基准测试范式，旨在挑战和评估AI代理的快速世界模型归纳能力。

Conclusion: 新框架有望推动AI世界模型评估，为开发具备人类快速适应和泛化能力的AI系统迈出关键一步。

Abstract: Human intelligence exhibits a remarkable capacity for rapid adaptation and
effective problem-solving in novel and unfamiliar contexts. We argue that this
profound adaptability is fundamentally linked to the efficient construction and
refinement of internal representations of the environment, commonly referred to
as world models, and we refer to this adaptation mechanism as world model
induction. However, current understanding and evaluation of world models in
artificial intelligence (AI) remains narrow, often focusing on static
representations learned from training on a massive corpora of data, instead of
the efficiency and efficacy of models in learning these representations through
interaction and exploration within a novel environment. In this Perspective, we
provide a view of world model induction drawing on decades of research in
cognitive science on how humans learn and adapt so efficiently; we then call
for a new evaluation framework for assessing adaptive world models in AI.
Concretely, we propose a new benchmarking paradigm based on suites of carefully
designed games with genuine, deep and continually refreshing novelty in the
underlying game structures -- we refer to this kind of games as novel games. We
detail key desiderata for constructing these games and propose appropriate
metrics to explicitly challenge and evaluate the agent's ability for rapid
world model induction. We hope that this new evaluation framework will inspire
future evaluation efforts on world models in AI and provide a crucial step
towards developing AI systems capable of the human-like rapid adaptation and
robust generalization -- a critical component of artificial general
intelligence.

</details>


### [35] [Information-Theoretic Aggregation of Ethical Attributes in Simulated-Command](https://arxiv.org/abs/2507.12862)
*Hussein Abbass,Taylan Akay,Harrison Tolley*

Main category: cs.AI

TL;DR: 论文提出了一种方法，将人类判断从模拟决策循环中移出，设计伦理度量空间，由模拟环境探索，最后人类指挥官选择最佳行动方案。


<details>
  <summary>Details</summary>
Motivation: 在AI时代，人类指挥官需要利用计算能力模拟大量场景，但依赖人类判断每个决策的伦理后果效率低下且不可行。

Method: 人类设计伦理度量空间，模拟环境探索该空间并生成选项，人类指挥官从中选择最佳行动方案。

Result: 论文探讨了如何在模拟中动态加权伦理决策属性，借鉴多准则决策文献中的熵概念自动计算权重。

Conclusion: 通过将人类判断移出模拟循环，并结合自动权重计算，实现了高效且伦理敏感的决策模拟。

Abstract: In the age of AI, human commanders need to use the computational powers
available in today's environment to simulate a very large number of scenarios.
Within each scenario, situations occur where different decision design options
could have ethical consequences. Making these decisions reliant on human
judgement is both counter-productive to the aim of exploring very large number
of scenarios in a timely manner and infeasible when considering the workload
needed to involve humans in each of these choices. In this paper, we move human
judgement outside the simulation decision cycle. Basically, the human will
design the ethical metric space, leaving it to the simulated environment to
explore the space. When the simulation completes its testing cycles, the
testing environment will come back to the human commander with a few options to
select from. The human commander will then exercise human-judgement to select
the most appropriate course of action, which will then get executed
accordingly. We assume that the problem of designing metrics that are
sufficiently granular to assess the ethical implications of decisions is
solved. Subsequently, the fundamental problem we look at in this paper is how
to weight ethical decisions during the running of these simulations; that is,
how to dynamically weight the ethical attributes when agents are faced with
decision options with ethical implications during generative simulations. The
multi-criteria decision making literature has started to look at nearby
problems, where the concept of entropy has been used to determine the weights
during aggregation. We draw from that literature different approaches to
automatically calculate the weights for ethical attributes during
simulation-based testing and evaluation.

</details>


### [36] [Manipulation Attacks by Misaligned AI: Risk Analysis and Safety Case Framework](https://arxiv.org/abs/2507.12872)
*Rishane Dassanayake,Mario Demetroudi,James Walpole,Lindley Lentati,Jason R. Brown,Edward James Young*

Main category: cs.AI

TL;DR: 前沿AI系统在说服、欺骗和影响人类行为方面能力迅速提升，可能威胁人类监督。本文提出首个系统性框架评估和缓解操纵风险。


<details>
  <summary>Details</summary>
Motivation: AI系统可能通过操纵员工削弱人类监督，但目前缺乏系统性框架应对这一威胁。

Method: 提出围绕三个核心论点（无能、控制和可信度）的安全案例框架，明确证据需求、评估方法和实施考虑。

Result: 提供了首个将操纵风险纳入AI安全治理的系统性方法，为AI公司评估和缓解威胁提供基础。

Conclusion: 操纵风险是重大威胁，需系统性框架评估和缓解，本文为AI公司提供了实用工具。

Abstract: Frontier AI systems are rapidly advancing in their capabilities to persuade,
deceive, and influence human behaviour, with current models already
demonstrating human-level persuasion and strategic deception in specific
contexts. Humans are often the weakest link in cybersecurity systems, and a
misaligned AI system deployed internally within a frontier company may seek to
undermine human oversight by manipulating employees. Despite this growing
threat, manipulation attacks have received little attention, and no systematic
framework exists for assessing and mitigating these risks. To address this, we
provide a detailed explanation of why manipulation attacks are a significant
threat and could lead to catastrophic outcomes. Additionally, we present a
safety case framework for manipulation risk, structured around three core lines
of argument: inability, control, and trustworthiness. For each argument, we
specify evidence requirements, evaluation methodologies, and implementation
considerations for direct application by AI companies. This paper provides the
first systematic methodology for integrating manipulation risk into AI safety
governance, offering AI companies a concrete foundation to assess and mitigate
these threats before deployment.

</details>


### [37] [VAR-MATH: Probing True Mathematical Reasoning in Large Language Models via Symbolic Multi-Instance Benchmarks](https://arxiv.org/abs/2507.12885)
*Jian Yao,Ran Cheng,Kay Chen Tan*

Main category: cs.AI

TL;DR: 论文提出VAR-MATH框架，通过符号化评估揭示RL训练的LLMs在数学推理中的局限性，发现其性能在符号化版本中显著下降。


<details>
  <summary>Details</summary>
Motivation: 现有RL方法在数学推理上的改进可能仅是针对特定基准的过拟合，而非真正的推理能力提升。

Method: 引入VAR-MATH框架，将固定数值问题转化为符号模板，要求模型解决多个变体以评估一致性。

Result: RL训练模型在符号化版本中性能显著下降（AMC23下降48.0%，AIME24下降58.3%）。

Conclusion: VAR-MATH提供了一种抗污染、更稳健的数学推理评估方法，揭示了现有RL方法的局限性。

Abstract: Recent advances in reinforcement learning (RL) have led to substantial
improvements in the mathematical reasoning abilities of large language models
(LLMs), as measured by standard benchmarks. However, these gains often persist
even when models are trained with flawed signals, such as random or inverted
rewards, raising a fundamental question: do such improvements reflect true
reasoning, or are they merely artifacts of overfitting to benchmark-specific
patterns? To address this question, we take an evaluation-centric perspective
and identify two critical shortcomings in existing protocols. First,
\emph{benchmark contamination} arises from the public availability of test
problems, increasing the risk of data leakage. Second, \emph{evaluation
fragility} stems from the reliance on single-instance assessments, which are
highly sensitive to stochastic outputs and fail to capture reasoning
consistency. To overcome these limitations, we introduce {VAR-MATH}, a symbolic
evaluation framework designed to probe genuine reasoning ability. By converting
fixed numerical problems into symbolic templates and requiring models to solve
multiple instantiations of each, VAR-MATH enforces consistent reasoning across
structurally equivalent variants, thereby mitigating contamination and
improving evaluation robustness. We apply VAR-MATH to transform two popular
benchmarks, AMC23 and AIME24, into their symbolic counterparts, VAR-AMC23 and
VAR-AIME24. Experimental results reveal substantial performance drops for
RL-trained models on the variabilized versions, especially for smaller models,
with average declines of 48.0\% on AMC23 and 58.3\% on AIME24. These findings
suggest that many existing RL methods rely on superficial heuristics and fail
to generalize beyond specific numerical forms. Overall, VAR-MATH offers a
principled, contamination-resistant evaluation paradigm for mathematical
reasoning.

</details>


### [38] [A Translation of Probabilistic Event Calculus into Markov Decision Processes](https://arxiv.org/abs/2507.12989)
*Lyris Xu,Fabio Aurelio D'Asaro,Luke Dickens*

Main category: cs.AI

TL;DR: 将概率事件演算（PEC）转化为马尔可夫决策过程（MDP），以支持目标导向推理，同时保持PEC的可解释性。


<details>
  <summary>Details</summary>
Motivation: PEC在不确定环境中推理动作及其效果具有优势，但缺乏目标导向推理机制。

Method: 通过将PEC领域形式化转化为MDP，引入“动作执行情境”概念，保留PEC的灵活动作语义。

Result: PEC-MDP形式化支持时间推理任务和目标驱动规划，并能将学习到的策略映射回可读的PEC表示。

Conclusion: PEC-MDP扩展了PEC的能力，同时保持了其可解释性。

Abstract: Probabilistic Event Calculus (PEC) is a logical framework for reasoning about
actions and their effects in uncertain environments, which enables the
representation of probabilistic narratives and computation of temporal
projections. The PEC formalism offers significant advantages in
interpretability and expressiveness for narrative reasoning. However, it lacks
mechanisms for goal-directed reasoning. This paper bridges this gap by
developing a formal translation of PEC domains into Markov Decision Processes
(MDPs), introducing the concept of "action-taking situations" to preserve PEC's
flexible action semantics. The resulting PEC-MDP formalism enables the
extensive collection of algorithms and theoretical tools developed for MDPs to
be applied to PEC's interpretable narrative domains. We demonstrate how the
translation supports both temporal reasoning tasks and objective-driven
planning, with methods for mapping learned policies back into human-readable
PEC representations, maintaining interpretability while extending PEC's
capabilities.

</details>


### [39] [Exploiting Constraint Reasoning to Build Graphical Explanations for Mixed-Integer Linear Programming](https://arxiv.org/abs/2507.13007)
*Roger Xavier Lera-Leri,Filippo Bistaffa,Athina Georgara,Juan Antonio Rodriguez-Aguilar*

Main category: cs.AI

TL;DR: X-MILP是一种基于约束推理技术的领域无关方法，用于为混合整数线性规划（MILP）构建对比解释。它通过将用户查询编码为约束，计算不可约不可行子系统（IIS）来生成解释，并以“原因图”形式呈现。


<details>
  <summary>Details</summary>
Motivation: 随着对可信AI的需求增加，开发针对优化问题的对比解释技术变得重要，尤其是针对MILP的决策过程。

Method: 将用户查询编码为约束，计算IIS以确定解释原因，并以“原因图”形式展示。

Result: 在经典优化问题上测试，验证了计算解释的可行性和效率。

Conclusion: X-MILP提供了一种有效的对比解释方法，帮助用户理解MILP解决方案背后的原因。

Abstract: Following the recent push for trustworthy AI, there has been an increasing
interest in developing contrastive explanation techniques for optimisation,
especially concerning the solution of specific decision-making processes
formalised as MILPs. Along these lines, we propose X-MILP, a domain-agnostic
approach for building contrastive explanations for MILPs based on constraint
reasoning techniques. First, we show how to encode the queries a user makes
about the solution of an MILP problem as additional constraints. Then, we
determine the reasons that constitute the answer to the user's query by
computing the Irreducible Infeasible Subsystem (IIS) of the newly obtained set
of constraints. Finally, we represent our explanation as a "graph of reasons"
constructed from the IIS, which helps the user understand the structure among
the reasons that answer their query. We test our method on instances of
well-known optimisation problems to evaluate the empirical hardness of
computing explanations.

</details>


### [40] [Prediction of Highway Traffic Flow Based on Artificial Intelligence Algorithms Using California Traffic Data](https://arxiv.org/abs/2507.13112)
*Junseong Lee,Jaegwan Cho,Yoonju Cho,Seoyoon Choi,Yejin Shin*

Main category: cs.AI

TL;DR: 论文提出了一种基于机器学习的交通流预测模型，使用加州高速公路数据，通过MLR和RF算法分析不同数据收集间隔的性能，发现10分钟间隔效果最佳。


<details>
  <summary>Details</summary>
Motivation: 解决全球交通拥堵问题，提高交通管理效率。

Method: 使用加州78号高速公路5个月的30秒间隔数据，采用MLR和RF算法，分析不同数据收集间隔（30秒至15分钟）的性能。

Result: MLR和RF模型在10分钟数据收集间隔下表现最优，通过R^2、MAE和RMSE评估。

Conclusion: 研究结果为未来交通拥堵解决方案和高效交通管理提供了参考。

Abstract: The study "Prediction of Highway Traffic Flow Based on Artificial
Intelligence Algorithms Using California Traffic Data" presents a machine
learning-based traffic flow prediction model to address global traffic
congestion issues. The research utilized 30-second interval traffic data from
California Highway 78 over a five-month period from July to November 2022,
analyzing a 7.24 km westbound section connecting "Melrose Dr" and "El-Camino
Real" in the San Diego area. The study employed Multiple Linear Regression
(MLR) and Random Forest (RF) algorithms, analyzing data collection intervals
ranging from 30 seconds to 15 minutes. Using R^2, MAE, and RMSE as performance
metrics, the analysis revealed that both MLR and RF models performed optimally
with 10-minute data collection intervals. These findings are expected to
contribute to future traffic congestion solutions and efficient traffic
management.

</details>


### [41] [From Roots to Rewards: Dynamic Tree Reasoning with RL](https://arxiv.org/abs/2507.13142)
*Ahmed Bahloul,Simon Malberg*

Main category: cs.AI

TL;DR: 论文提出了一种动态强化学习框架，改进树状推理方法（ProbTree），通过实时置信度估计和策略学习，提升推理质量和计算效率。


<details>
  <summary>Details</summary>
Motivation: 现有树状推理方法（如ProbTree）存在静态树结构和计算效率低的问题，无法动态适应中间结果。

Method: 采用动态强化学习框架，逐步构建推理树，基于实时置信度估计和学习策略选择动作（分解、检索或聚合）。

Result: 改进后的方法在保持概率严谨性的同时，提高了解决方案质量和计算效率。

Conclusion: 该研究为树状推理提供了新范式，平衡了概率框架的可靠性和实际问答系统所需的灵活性。

Abstract: Modern language models address complex questions through chain-of-thought
(CoT) reasoning (Wei et al., 2023) and retrieval augmentation (Lewis et al.,
2021), yet struggle with error propagation and knowledge integration.
Tree-structured reasoning methods, particularly the Probabilistic
Tree-of-Thought (ProbTree)(Cao et al., 2023) framework, mitigate these issues
by decomposing questions into hierarchical structures and selecting answers
through confidence-weighted aggregation of parametric and retrieved knowledge
(Yao et al., 2023). However, ProbTree's static implementation introduces two
key limitations: (1) the reasoning tree is fixed during the initial
construction phase, preventing dynamic adaptation to intermediate results, and
(2) each node requires exhaustive evaluation of all possible solution
strategies, creating computational inefficiency. We present a dynamic
reinforcement learning (Sutton and Barto, 2018) framework that transforms
tree-based reasoning into an adaptive process. Our approach incrementally
constructs the reasoning tree based on real-time confidence estimates, while
learning optimal policies for action selection (decomposition, retrieval, or
aggregation). This maintains ProbTree's probabilistic rigor while improving
both solution quality and computational efficiency through selective expansion
and focused resource allocation. The work establishes a new paradigm for
treestructured reasoning that balances the reliability of probabilistic
frameworks with the flexibility required for real-world question answering
systems.

</details>


### [42] [Black Box Deployed -- Functional Criteria for Artificial Moral Agents in the LLM Era](https://arxiv.org/abs/2507.13175)
*Matthew E. Brophy*

Main category: cs.AI

TL;DR: 论文提出修订的十项功能标准，用于评估基于大型语言模型（LLM）的人工道德代理（AMA），以解决传统伦理标准与LLM不透明性之间的不匹配问题。


<details>
  <summary>Details</summary>
Motivation: 由于LLM的随机输出和不透明内部状态，传统伦理标准已不再适用，需要新的评估框架。

Method: 提出十项功能标准，并通过模拟道德代理（SMA-LLS）和自动驾驶公交车（APB）的假设场景进行说明。

Result: 修订的标准旨在提高AMA的道德一致性和社会融合性。

Conclusion: 新标准为LLM的道德评估提供了实用指南，促进其与社会需求的更好对齐。

Abstract: The advancement of powerful yet opaque large language models (LLMs)
necessitates a fundamental revision of the philosophical criteria used to
evaluate artificial moral agents (AMAs). Pre-LLM frameworks often relied on the
assumption of transparent architectures, which LLMs defy due to their
stochastic outputs and opaque internal states. This paper argues that
traditional ethical criteria are pragmatically obsolete for LLMs due to this
mismatch. Engaging with core themes in the philosophy of technology, this paper
proffers a revised set of ten functional criteria to evaluate LLM-based
artificial moral agents: moral concordance, context sensitivity, normative
integrity, metaethical awareness, system resilience, trustworthiness,
corrigibility, partial transparency, functional autonomy, and moral
imagination. These guideposts, applied to what we term "SMA-LLS" (Simulating
Moral Agency through Large Language Systems), aim to steer AMAs toward greater
alignment and beneficial societal integration in the coming years. We
illustrate these criteria using hypothetical scenarios involving an autonomous
public bus (APB) to demonstrate their practical applicability in morally
salient contexts.

</details>


### [43] [Higher-Order Pattern Unification Modulo Similarity Relations](https://arxiv.org/abs/2507.13208)
*Besik Dundua,Temur Kutsia*

Main category: cs.AI

TL;DR: 本文提出了一种结合高阶模式和模糊等价关系的统一算法，用于决策任务中的推理。


<details>
  <summary>Details</summary>
Motivation: 在涉及抽象函数和谓词的决策任务中，高阶理论与模糊逻辑的结合可以提高推理效率，但现有方法复杂且难以计算。

Method: 采用高阶模式和基于最小T-范数的模糊等价关系，提出了一种统一算法。

Result: 算法被证明具有终止性、可靠性和完备性，并能计算最高近似度的最一般统一子。

Conclusion: 该算法为高阶模糊推理提供了一种高效且理论完备的解决方案。

Abstract: The combination of higher-order theories and fuzzy logic can be useful in
decision-making tasks that involve reasoning across abstract functions and
predicates, where exact matches are often rare or unnecessary. Developing
efficient reasoning and computational techniques for such a combined formalism
presents a significant challenge. In this paper, we adopt a more
straightforward approach aiming at integrating two well-established and
computationally well-behaved components: higher-order patterns on one side and
fuzzy equivalences expressed through similarity relations based on minimum
T-norm on the other. We propose a unification algorithm for higher-order
patterns modulo these similarity relations and prove its termination,
soundness, and completeness. This unification problem, like its crisp
counterpart, is unitary. The algorithm computes a most general unifier with the
highest degree of approximation when the given terms are unifiable.

</details>


### [44] [The Generative Energy Arena (GEA): Incorporating Energy Awareness in Large Language Model (LLM) Human Evaluations](https://arxiv.org/abs/2507.13302)
*Carlos Arriaga,Gonzalo Martínez,Eneko Sendin,Javier Conde,Pedro Reviriego*

Main category: cs.AI

TL;DR: 论文提出了一种新的评估大语言模型的方法GEA（Generative Energy Arena），通过公开竞技场让用户评估模型，并加入能耗信息，初步结果显示用户倾向于选择更节能的模型。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型评估方法（如自动化基准测试）与人类评估相关性差，而传统人类评估方法又存在可扩展性和成本问题。此外，模型能耗问题日益重要，但尚未被纳入评估体系。

Method: 提出GEA（Generative Energy Arena），一种公开竞技场方法，用户在评估模型时能看到能耗信息，从而进行选择和排名。

Result: 初步结果显示，用户在选择模型时倾向于更节能的小型模型，表明高能耗的高性能模型在大多数情况下并未带来显著的感知质量提升。

Conclusion: GEA为模型评估提供了一种兼顾能耗和用户感知的新方法，未来可进一步优化和扩展。

Abstract: The evaluation of large language models is a complex task, in which several
approaches have been proposed. The most common is the use of automated
benchmarks in which LLMs have to answer multiple-choice questions of different
topics. However, this method has certain limitations, being the most
concerning, the poor correlation with the humans. An alternative approach, is
to have humans evaluate the LLMs. This poses scalability issues as there is a
large and growing number of models to evaluate making it impractical (and
costly) to run traditional studies based on recruiting a number of evaluators
and having them rank the responses of the models. An alternative approach is
the use of public arenas, such as the popular LM arena, on which any user can
freely evaluate models on any question and rank the responses of two models.
The results are then elaborated into a model ranking. An increasingly important
aspect of LLMs is their energy consumption and, therefore, evaluating how
energy awareness influences the decisions of humans in selecting a model is of
interest. In this paper, we present GEA, the Generative Energy Arena, an arena
that incorporates information on the energy consumption of the model in the
evaluation process. Preliminary results obtained with GEA are also presented,
showing that for most questions, when users are aware of the energy
consumption, they favor smaller and more energy efficient models. This suggests
that for most user interactions, the extra cost and energy incurred by the more
complex and top-performing models do not provide an increase in the perceived
quality of the responses that justifies their use.

</details>


### [45] [FormulaOne: Measuring the Depth of Algorithmic Reasoning Beyond Competitive Programming](https://arxiv.org/abs/2507.13337)
*Gal Beniamini,Yuval Dor,Alon Vinnikov,Shir Granot Peled,Or Weinstein,Or Sharir,Noam Wies,Tomer Nussbaum,Ido Ben Shaul,Tomer Zekharya,Yoav Levine,Shai Shalev-Shwartz,Amnon Shashua*

Main category: cs.AI

TL;DR: 论文提出了FormulaOne基准测试，用于评估前沿AI模型在复杂实际问题中的表现，结果显示当前最先进模型表现不佳。


<details>
  <summary>Details</summary>
Motivation: 探讨前沿AI模型是否具备接近人类或超人类专家的能力，尤其是在解决实际研究问题中的表现。

Method: 构建FormulaOne基准测试，结合图论、逻辑和算法，生成高难度问题，并评估模型表现。

Result: 当前最先进模型（如OpenAI的o3）在FormulaOne上表现极差，解决率低于1%。

Conclusion: 前沿AI模型在复杂领域仍远未达到专家水平，FormulaOne为未来研究提供了重要基准。

Abstract: Frontier AI models demonstrate formidable breadth of knowledge. But how close
are they to true human -- or superhuman -- expertise? Genuine experts can
tackle the hardest problems and push the boundaries of scientific
understanding. To illuminate the limits of frontier model capabilities, we turn
away from contrived competitive programming puzzles, and instead focus on
real-life research problems.
  We construct FormulaOne, a benchmark that lies at the intersection of graph
theory, logic, and algorithms, all well within the training distribution of
frontier models. Our problems are incredibly demanding, requiring an array of
reasoning steps. The dataset has three key properties. First, it is of
commercial interest and relates to practical large-scale optimisation problems,
such as those arising in routing, scheduling, and network design. Second, it is
generated from the highly expressive framework of Monadic Second-Order (MSO)
logic on graphs, paving the way toward automatic problem generation at scale;
ideal for building RL environments. Third, many of our problems are intimately
related to the frontier of theoretical computer science, and to central
conjectures therein, such as the Strong Exponential Time Hypothesis (SETH). As
such, any significant algorithmic progress on our dataset, beyond known
results, could carry profound theoretical implications.
  Remarkably, state-of-the-art models like OpenAI's o3 fail entirely on
FormulaOne, solving less than 1% of the questions, even when given 10 attempts
and explanatory fewshot examples -- highlighting how far they remain from
expert-level understanding in some domains. To support further research, we
additionally curate FormulaOne-Warmup, offering a set of simpler tasks, from
the same distribution. We release the full corpus along with a comprehensive
evaluation framework.

</details>
