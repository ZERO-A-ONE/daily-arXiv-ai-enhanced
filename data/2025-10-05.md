<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 14]
- [cs.CR](#cs.CR) [Total: 19]
- [cs.AI](#cs.AI) [Total: 51]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [Beyond Single LLMs: Enhanced Code Generation via Multi-Stage Performance-Guided LLM Orchestration](https://arxiv.org/abs/2510.01379)
*Huashan Chen,Zhenyu Qi,Haotang Li,Hong Chen,Jinfu Chen,Kebin Peng,In Kee Kim,Kyu Hyung Lee,Sen He*

Main category: cs.SE

TL;DR: 提出了一种多阶段、性能导向的LLM编排框架PerfOrch，通过动态路由编程任务到最合适的LLM，在无需微调的情况下显著提升了代码生成的功能正确性和运行时性能。


<details>
  <summary>Details</summary>
Motivation: 当前单一模型方法忽视了不同LLM在不同编程语言、算法领域和开发阶段表现出的异构计算优势，需要更智能的模型选择策略。

Method: 基于对17个先进LLM在5种编程语言上的实证研究，开发了多阶段生成-修复-优化工作流，通过阶段验证和回滚机制动态选择最佳LLM。

Result: 在HumanEval-X和EffiBench-X上分别达到96.22%和91.37%的正确率，超越GPT-4o；58.76%的问题执行时间得到优化，中位数加速17.67%-27.66%。

Conclusion: PerfOrch提供了一种可扩展的即插即用架构，为生产级自动化软件工程提供了适应快速发展的生成AI环境的范式。

Abstract: While Large Language Models (LLMs) have become the predominant paradigm for
automated code generation, current single-model approaches fundamentally ignore
the heterogeneous computational strengths that different models exhibit across
programming languages, algorithmic domains, and development stages. This paper
challenges the single-model convention by introducing a multi-stage,
performance-guided orchestration framework that dynamically routes coding tasks
to the most suitable LLMs within a structured generate-fix-refine workflow. Our
approach is grounded in a comprehensive empirical study of 17 state-of-the-art
LLMs across five programming languages (Python, Java, C++, Go, and Rust) using
HumanEval-X benchmark. The study, which evaluates both functional correctness
and runtime performance metrics (execution time, mean/max memory utilization,
and CPU efficiency), reveals pronounced performance heterogeneity by language,
development stage, and problem category. Guided by these empirical insights, we
present PerfOrch, an LLM agent that orchestrates top-performing LLMs for each
task context through stage-wise validation and rollback mechanisms. Without
requiring model fine-tuning, PerfOrch achieves substantial improvements over
strong single-model baselines: average correctness rates of 96.22% and 91.37%
on HumanEval-X and EffiBench-X respectively, surpassing GPT-4o's 78.66% and
49.11%. Beyond correctness gains, the framework delivers consistent performance
optimizations, improving execution time for 58.76% of problems with median
speedups ranging from 17.67% to 27.66% across languages on two benchmarks. The
framework's plug-and-play architecture ensures practical scalability, allowing
new LLMs to be profiled and integrated seamlessly, thereby offering a paradigm
for production-grade automated software engineering that adapts to the rapidly
evolving generative AI landscape.

</details>


### [2] [Deciphering WONTFIX: A Mixed-Method Study on Why GitHub Issues Get Rejected](https://arxiv.org/abs/2510.01514)
*J. Alexander Curtis,Sharadha Kasiviswanathan,Nasir Eisty*

Main category: cs.SE

TL;DR: 该研究分析了GitHub上wontfix标签的普遍性和使用原因，发现约30%的项目使用该标签，主要出现在用户提交的bug报告和功能请求中，并识别出8个常见的使用主题。


<details>
  <summary>Details</summary>
Motivation: wontfix标签在GitHub仓库中被广泛使用，但其对项目管理和开源社区动态的影响尚未明确界定，需要系统研究其使用模式和原因。

Method: 采用混合方法，从GitHub 3,132个最受欢迎仓库收集数据，通过定量分析评估wontfix标签的普遍性，并通过开放式编码和主题分析对使用原因进行分类。

Result: 约30%的GitHub项目使用wontfix标签，主要应用于用户提交的bug报告和功能请求，识别出8个常见使用主题，包括用户特定控制因素和维护者特定决策等。

Conclusion: wontfix标签是GitHub项目中管理资源和引导贡献者努力的重要工具，但可能抑制社区参与并影响管理透明度。理解这些原因有助于项目管理决策和促进开源社区高效协作。

Abstract: Context: The ``wontfix'' label is a widely used yet narrowly understood tool
in GitHub repositories, indicating that an issue will not be pursued further.
Despite its prevalence, the impact of this label on project management and
community dynamics within open-source software development is not clearly
defined. Objective: This study examines the prevalence and reasons behind
issues being labeled as wontfix across various open-source repositories on
GitHub. Method: Employing a mixed-method approach, we analyze both quantitative
data to assess the prevalence of the wontfix label and qualitative data to
explore the reasoning that it was used. Data were collected from 3,132 of
GitHub's most-popular repositories. Later, we employ open coding and thematic
analysis to categorize the reasons behind wontfix labels, providing a
structured understanding of the issue management landscape. Results: Our
findings show that about 30% of projects on GitHub apply the wontfix label to
some issues. These issues most often occur on user-submitted issues for bug
reports and feature requests. The study identified eight common themes behind
labeling issues as wontfix, ranging from user-specific control factors to
maintainer-specific decisions. Conclusions: The wontfix label is a critical
tool for managing resources and guiding contributor efforts in GitHub projects.
However, it can also discourage community involvement and obscure the
transparency of project management. Understanding these reasons aids project
managers in making informed decisions and fostering efficient collaboration
within open-source communities.

</details>


### [3] [MIMIC: Integrating Diverse Personality Traits for Better Game Testing Using Large Language Model](https://arxiv.org/abs/2510.01635)
*Yifei Chen,Sarra Habchi,Lili Wei*

Main category: cs.SE

TL;DR: MIMIC框架通过整合多样化人格特质到游戏代理中，使它们能在相似情境下采用不同的游戏策略，从而提高测试覆盖率和游戏内交互多样性。


<details>
  <summary>Details</summary>
Motivation: 传统自动化测试算法难以应对现代视频游戏的复杂性，而现有基于强化学习、模仿学习或大语言模型的游戏代理往往忽略人类玩家的多样化策略，导致解决方案重复，无法触发多样化的游戏内交互或发现边缘情况。

Method: 提出MIMIC框架，将多样化人格特质整合到游戏代理中，使代理能够模仿不同的游戏风格，在相似情境下采用不同的策略。

Result: MIMIC在不同游戏中实现了更高的测试覆盖率和更丰富的游戏内交互，在Minecraft中超越了最先进的代理，获得了更高的任务完成率并提供了更多样化的解决方案。

Conclusion: MIMIC在游戏测试方面展现出显著潜力，能够有效提高测试效率和发现更多边缘情况。

Abstract: Modern video games pose significant challenges for traditional automated
testing algorithms, yet intensive testing is crucial to ensure game quality. To
address these challenges, researchers designed gaming agents using
Reinforcement Learning, Imitation Learning, or Large Language Models. However,
these agents often neglect the diverse strategies employed by human players due
to their different personalities, resulting in repetitive solutions in similar
situations. Without mimicking varied gaming strategies, these agents struggle
to trigger diverse in-game interactions or uncover edge cases.
  In this paper, we present MIMIC, a novel framework that integrates diverse
personality traits into gaming agents, enabling them to adopt different gaming
strategies for similar situations. By mimicking different playstyles, MIMIC can
achieve higher test coverage and richer in-game interactions across different
games. It also outperforms state-of-the-art agents in Minecraft by achieving a
higher task completion rate and providing more diverse solutions. These results
highlight MIMIC's significant potential for effective game testing.

</details>


### [4] [FOSS-chain: using blockchain for Open Source Software license compliance](https://arxiv.org/abs/2510.01740)
*Kypros Iacovou,Georgia M. Kapitsaki,Evangelia Vanezi*

Main category: cs.SE

TL;DR: FOSS-chain是一个基于区块链的开源软件许可证管理平台，旨在解决衍生作品中的许可证兼容性问题，自动化许可证合规流程。


<details>
  <summary>Details</summary>
Motivation: 开源软件许可证合规性检查复杂，许可证不兼容可能导致法律纠纷，而区块链技术可以提供透明度和不可篡改的记录机制。

Method: 设计并实现FOSS-chain网络平台，集成区块链技术，自动化许可证合规流程，覆盖14种开源许可证。

Result: 通过小规模用户研究评估初步原型，结果显示该平台在现实软件系统中具有应用潜力。

Conclusion: FOSS-chain平台展示了区块链技术在开源软件许可证管理中的潜力，能够有效解决许可证兼容性问题。

Abstract: Open Source Software (OSS) is widely used and carries licenses that indicate
the terms under which the software is provided for use, also specifying
modification and distribution rules. Ensuring that users are respecting OSS
license terms when creating derivative works is a complex process. Compliance
issues arising from incompatibilities among licenses may lead to legal
disputes. At the same time, the blockchain technology with immutable entries
offers a mechanism to provide transparency when it comes to licensing and
ensure software changes are recorded. In this work, we are introducing an
integration of blockchain and license management when creating derivative
works, in order to tackle the issue of OSS license compatibility. We have
designed, implemented and performed a preliminary evaluation of FOSS-chain, a
web platform that uses blockchain and automates the license compliance process,
covering 14 OSS licenses. We have evaluated the initial prototype version of
the FOSS-chain platform via a small scale user study. Our preliminary results
are promising, demonstrating the potential of the platform for adaptation on
realistic software systems.

</details>


### [5] [ARENA: A tool for measuring and analysing the energy efficiency of Android apps](https://arxiv.org/abs/2510.01754)
*Hina Anwar*

Main category: cs.SE

TL;DR: ARENA是一个支持工具，让开发者和研究人员能够在IDE中直接连接物理测量设备，测量Android应用的能量消耗，并提供数据分析、报告和可视化功能。


<details>
  <summary>Details</summary>
Motivation: 硬件方法测量Android应用能量消耗虽然准确，但过程耗时且不易适应或重现，缺乏开源工具支持。

Method: 将ARENA实现为IntelliJ和Android Studio插件，通过执行测试场景计算Android智能手机的能量消耗，并帮助聚合、统计分析、报告和可视化数据。

Result: 开发者和研究人员可以在开发过程中使用ARENA比较不同应用或同一应用不同版本的能量消耗。

Conclusion: ARENA解决了硬件能量测量过程的复杂性问题，提供了便捷的工具支持。

Abstract: To build energy-efficient apps, there is a need to estimate and analyze their
energy consumption in typical usage scenarios. The energy consumption of
Android apps could be estimated via software-based and hardware-based
approaches. Software-based approaches, while easier to implement, are not as
accurate as hardware-based approaches. The process of measuring the energy
consumption of an Android app via a hardware-based approach typically involves
1) setting up a measurement environment, 2) executing the app under test on a
mobile device, 3) recording current/voltage data via a hardware device to
measure energy consumption, and 4) cleaning and aggregating data for analyses,
reports, and visualizations. Specialized scripts are written for selected
hardware and software components to ensure reliable energy measurements. The
energy measurement process is repeated many times and aggregated to remove
noise. These steps make the hardware-based energy measurement process
time-consuming and not easy to adapt or reproduce. There is a lack of
open-source tools available for developers and researchers to take reliable
energy measurements via hardware devices. In this paper, we present and
demonstrate ARENA, a support tool that enables developers and researchers to
connect to a physical measurement device without leaving the comfort of their
IDE. Developers could use ARENA during development to compare energy
consumption between different apps or versions of the same app. ARENA
calculates energy consumption on an Android smartphone by executing a test
scenario on the app under development. Further, ARENA helps aggregate,
statistically analyze, report, and visualize the data, allowing developers and
researchers to dig into the data directly or visually. We implemented ARENA as
an IntelliJ and Android Studio plugin.

</details>


### [6] [Towards Speeding up Program Repair with Non-Autoregressive Model](https://arxiv.org/abs/2510.01825)
*Zhenyu Yang,Yue Pan,Zhen Yang,Zhongxing Yu*

Main category: cs.SE

TL;DR: NARRepair是首个为自动程序修复任务定制的非自回归代码生成模型，通过修复动作预测器、令牌间依赖提取器和两阶段解码器解决NAR方法在APR任务中的质量问题，在修复速度和准确性上达到最先进的综合性能。


<details>
  <summary>Details</summary>
Motivation: 传统基于自回归的APR技术存在巨大的时间延迟问题，特别是参数较多的模型延迟更严重。非自回归方法可以并行输出目标代码避免修复延迟，但直接应用会导致补丁质量下降。

Method: 提出NARRepair模型，包含三个创新组件：修复动作预测器缓解过度修正问题，令牌间依赖提取器缓解缺乏令牌间依赖信息问题，两阶段解码器缓解缺乏上下文信息问题。

Result: 在三个广泛使用的APR数据集上评估，NARRepair在有限修复时间内性能最佳，相比AR-based APR技术修复速度在GPU环境下提升1.4-6.4倍。

Conclusion: NARRepair在修复速度和准确性方面达到了最先进的综合性能，成功将非自回归方法应用于APR任务。

Abstract: Enlightened by the success of machine learning techniques in various
application areas, recent years have witnessed a surge of research efforts on
automatic program repair (APR) using machine learning techniques. Previous
machine learning-based APR techniques essentially modified bugs in the
autoregressive (AR) manner, which predicts future values based on past values.
Due to the manner of token-by-token generation, the AR-based APR technique has
a huge time delay. In particular, the delay of the APR model with a large
number of parameters is more serious. To address the issue, we aim to apply the
non-autoregressive (NAR) method to the APR task, which can output target code
in a parallel manner to avoid huge repair delays. However, the naive use of the
NAR manner for the APR task suffers from the issue of compromised patch
quality. To effectively adapt the NAR manner for the APR task, we in this paper
propose NARRepair, the first customized NAR code generation model for the APR
task. The NARRepair model features three major novelties, including 1) the
repair action predictor for alleviating the over-correction issue, 2) the
inter-token dependency extractor for alleviating the issue of lacking
inter-token dependency information, and 3) the two-stage decoder for
alleviating the issue of lacking contextual information. We evaluated NARRepair
on three widely used datasets in the APR community, and the results show that
1) compared to other APR techniques, the NARRepair model has the best
performance within the limited repair time, and 2) compared to AR-based APR
techniques, the repair speed of NARRepair has been increased by 1.4-6.4 times
in the GPU environment. Overall, the results show that NARRepair has achieved
state-of-the-art comprehensive performance in terms of repair speed and
accuracy.

</details>


### [7] [RefFilter: Improving Semantic Conflict Detection via Refactoring-Aware Static Analysis](https://arxiv.org/abs/2510.01960)
*Victor Lira,Paulo Borba,Rodrigo Bonifácio,Galileu Santos e Matheus barbosa*

Main category: cs.SE

TL;DR: RefFilter是一个重构感知的语义干扰检测工具，通过自动检测重构操作来减少现有静态分析技术中的误报问题。


<details>
  <summary>Details</summary>
Motivation: 协作软件开发中语义干扰检测存在高误报率，主要原因是现有技术无法有效区分行为保持的代码重构和可能影响行为的变更。

Method: 在现有静态分析技术基础上，结合自动化重构检测来改进精度，从报告中排除行为保持的重构操作。

Result: 在标记数据集上，RefFilter将误报减少了近32%，尽管伴随轻微假阴性增加，但精度提升显著超过了召回率的微小损失。

Conclusion: 重构感知的干扰检测是改进现代开发工作流中合并支持的实际有效策略。

Abstract: Detecting semantic interference remains a challenge in collaborative software
development. Recent lightweight static analysis techniques improve efficiency
over SDG-based methods, but they still suffer from a high rate of false
positives. A key cause of these false positives is the presence of
behavior-preserving code refactorings, which current techniques cannot
effectively distinguish from changes that impact behavior and can interfere
with others. To handle this problem we present RefFilter, a refactoring-aware
tool for semantic interference detection. It builds on existing static
techniques by incorporating automated refactoring detection to improve
precision. RefFilter discards behavior-preserving refactorings from reports,
reducing false positives while preserving detection coverage. To evaluate
effectiveness and scalability, use two datasets: a labeled dataset with 99
scenarios and ground truth, and a novel dataset of 1,087 diverse merge
scenarios that we have built. Experimental results show that RefFilter reduces
false positives by nearly 32% on the labeled dataset. While this reduction
comes with a non significant increase in false negatives, the overall gain in
precision significantly outweighs the minor trade-off in recall. These findings
demonstrate that refactoring-aware interference detection is a practical and
effective strategy for improving merge support in modern development workflows.

</details>


### [8] [Clarifying Semantics of In-Context Examples for Unit Test Generation](https://arxiv.org/abs/2510.01994)
*Chen Yang,Lin Yang,Ziqi Wang,Dong Wang,Jianyi Zhou,Junjie Chen*

Main category: cs.SE

TL;DR: CLAST是一种通过系统化重构单元测试来提升语义清晰度的技术，能够显著改进基于上下文学习的单元测试生成效果。


<details>
  <summary>Details</summary>
Motivation: 现有基于大语言模型的单元测试生成方法严重依赖于上下文示例的质量，但结构不良或语义不清的测试示例会导致生成效果不佳。

Method: CLAST通过程序分析和基于LLM的重写相结合，将复杂测试分解为逻辑更清晰的测试，并提升语义清晰度。

Result: 在4个开源项目和3个工业项目中评估，CLAST在保持测试有效性和提升语义清晰度方面大幅优于现有最佳技术UTgen，用户研究中超过85.33%的参与者偏好CLAST重构的测试。

Conclusion: CLAST重构的测试作为示例能够有效改进基于ICL的单元测试生成方法，在编译成功率、通过率和测试覆盖率等方面均有显著提升。

Abstract: Recent advances in large language models (LLMs) have enabled promising
performance in unit test generation through in-context learning (ICL). However,
the quality of in-context examples significantly influences the effectiveness
of generated tests-poorly structured or semantically unclear test examples
often lead to suboptimal outputs. In this paper, we propose CLAST, a novel
technique that systematically refines unit tests to improve their semantic
clarity, thereby enhancing their utility as in-context examples. The approach
decomposes complex tests into logically clearer ones and improves semantic
clarity through a combination of program analysis and LLM-based rewriting. We
evaluated CLAST on four open-source and three industrial projects. The results
demonstrate that CLAST largely outperforms UTgen, the state-of-the-art
refinement technique, in both preserving test effectiveness and enhancing
semantic clarity. Specifically, CLAST fully retains the original effectiveness
of unit tests, while UTgen reduces compilation success rate (CSR), pass rate
(PR), test coverage (Cov), and mutation score (MS) by an average of 12.90%,
35.82%, 4.65%, and 5.07%, respectively. Over 85.33% of participants in our user
study preferred the semantic clarity of CLAST-refined tests. Notably,
incorporating CLAST-refined tests as examples effectively improves ICL-based
unit test generation approaches such as RAGGen and TELPA, resulting in an
average increase of 25.97% in CSR, 28.22% in PR, and 45.99% in Cov for
generated tests, compared to incorporating UTgen-refined tests. The insights
from the follow-up user study not only reinforce CLAST's potential impact in
software testing practice but also illuminate avenues for future research.

</details>


### [9] [Automatic Generation of Combinatorial Reoptimisation Problem Specifications: A Vision](https://arxiv.org/abs/2510.02002)
*Maximilian Kratz,Steffen Zschaler,Jens Kosiol,Gabriele Taentzer*

Main category: cs.SE

TL;DR: 本文探讨了使用模型驱动工程方法从原始优化问题规范中系统推导再优化问题的机会，重点关注组合再优化问题，并提供了问题变化分类和再优化规范推导策略。


<details>
  <summary>Details</summary>
Motivation: 当优化问题的上下文因素发生变化时，需要重新优化解决方案。传统再优化面临三个挑战：最小化对原方案的改动、某些部分无法修改、需要生成从原方案到新方案的变更脚本。

Method: 采用模型驱动工程方法，特别是使用声明式建模语言和模型转换来高层次规范优化问题，从原始优化问题规范系统推导再优化问题。基于GIPS工具实现概念验证。

Result: 提出了组合再优化问题的初步分类和相应的再优化规范推导策略，并通过助教分配示例问题验证了方法的可行性。

Conclusion: 模型驱动工程为从原始优化问题规范系统推导再优化问题提供了新的机会，能够有效应对再优化过程中的挑战。

Abstract: Once an optimisation problem has been solved, the solution may need
adaptation when contextual factors change. This challenge, also known as
reoptimisation, has been addressed in various problem domains, such as railway
crew rescheduling, nurse rerostering, or aircraft recovery. This requires a
modified problem to be solved again to ensure that the adapted solution is
optimal in the new context. However, the new optimisation problem differs
notably from the original problem: (i) we want to make only minimal changes to
the original solution to minimise the impact; (ii) we may be unable to change
some parts of the original solution (e.g., because they refer to past
allocations); and (iii) we need to derive a change script from the original
solution to the new solution. In this paper, we argue that Model-Driven
Engineering (MDE) - in particular, the use of declarative modelling languages
and model transformations for the high-level specification of optimisation
problems - offers new opportunities for the systematic derivation of
reoptimisation problems from the original optimisation problem specification.
We focus on combinatorial reoptimisation problems and provide an initial
categorisation of changing problems and strategies for deriving the
corresponding reoptimisation specifications. We introduce an initial
proof-of-concept implementation based on the GIPS (Graph-Based (Mixed) Integer
Linear Programming Problem Specification) tool and apply it to an example
resource-allocation problem: the allocation of teaching assistants to teaching
sessions.

</details>


### [10] [ACM SIGSOFT SEN Empirical Software Engineering: Introducing Our New Regular Column](https://arxiv.org/abs/2510.02007)
*Justus Bogner,Roberto Verdecchia*

Main category: cs.SE

TL;DR: 介绍新的ACM SIGSOFT SEN专栏(SEN-ESE)，旨在讨论经验软件工程研究的元方面，包括最佳实践、统计方法等，通过专家访谈、焦点小组等方式鼓励对ESE研究的反思和改进。


<details>
  <summary>Details</summary>
Motivation: 经验软件工程研究领域虽然成熟，但仍面临研究可重复性、外部有效性有限、评审主观性等挑战，且许多方面缺乏明确文档，使新研究者难以掌握。

Method: 通过新的ACM SIGSOFT SEN专栏，汇集专家访谈、焦点小组、调查和立场文章，讨论ESE研究的元方面和最佳实践。

Result: 建立了专门讨论ESE研究元方面的平台，为社区提供反思和改进研究实践的机会。

Conclusion: 该专栏将成为定期讨论ESE研究中不常触及或隐含话题的场所，鼓励社区反馈和建议，以围绕社区兴趣塑造专栏内容。

Abstract: From its early foundations in the 1970s, empirical software engineering (ESE)
has evolved into a mature research discipline that embraces a plethora of
different topics, methodologies, and industrial practices. Despite its
remarkable progress, the ESE research field still needs to keep evolving, as
new impediments, shortcoming, and technologies emerge. Research
reproducibility, limited external validity, subjectivity of reviews, and
porting research results to industrial practices are just some examples of the
drivers for improvements to ESE research. Additionally, several facets of ESE
research are not documented very explicitly, which makes it difficult for
newcomers to pick them up. With this new regular ACM SIGSOFT SEN column
(SEN-ESE), we introduce a venue for discussing meta-aspects of ESE research,
ranging from general topics such as the nature and best practices for
replication packages, to more nuanced themes such as statistical methods,
interview transcription tools, and publishing interdisciplinary research. Our
aim for the column is to be a place where we can regularly spark conversations
on ESE topics that might not often be touched upon or are left implicit.
Contributions to this column will be grounded in expert interviews, focus
groups, surveys, and position pieces, with the goal of encouraging reflection
and improvement in how we conduct, communicate, teach, and ultimately improve
ESE research. Finally, we invite feedback from the ESE community on
challenging, controversial, or underexplored topics, as well as suggestions for
voices you would like to hear from. While we cannot promise to act on every
idea, we aim to shape this column around the community interests and are
grateful for all contributions.

</details>


### [11] [Towards fairer public transit: Real-time tensor-based multimodal fare evasion and fraud detection](https://arxiv.org/abs/2510.02165)
*Peter Wauyo,Dalia Bwiza,Alain Murara,Edwin Mugume,Eric Umuhoza*

Main category: cs.SE

TL;DR: 提出基于多模态融合的公共交通欺诈检测系统，结合CCTV视频和音频数据，使用ViViT和AST模型提取特征，通过张量融合网络实现跨模态交互，在欺诈检测任务中达到89.5%准确率。


<details>
  <summary>Details</summary>
Motivation: 解决公共交通中的欺诈和逃票问题，传统方法检测效果有限，需要更先进的跨模态分析技术来捕捉复杂的行为模式。

Method: 使用Vision Transformer for Video (ViViT)提取视频特征，Audio Spectrogram Transformer (AST)分析音频，采用Tensor Fusion Network (TFN)架构通过2折笛卡尔积显式建模单模态和双模态交互。

Result: 在自定义数据集上达到89.5%准确率、87.2%精确率和84.0%召回率，显著优于早期融合基准和现有最先进系统（通常75%召回率），张量融合方法相比传统拼接方法提升7.0% F1分数和8.8%召回率。

Conclusion: 该多模态融合系统能有效检测公共交通欺诈行为，支持实时检测，帮助运营商减少收入损失、提升乘客安全和确保运营合规。

Abstract: This research introduces a multimodal system designed to detect fraud and
fare evasion in public transportation by analyzing closed circuit television
(CCTV) and audio data. The proposed solution uses the Vision Transformer for
Video (ViViT) model for video feature extraction and the Audio Spectrogram
Transformer (AST) for audio analysis. The system implements a Tensor Fusion
Network (TFN) architecture that explicitly models unimodal and bimodal
interactions through a 2-fold Cartesian product. This advanced fusion technique
captures complex cross-modal dynamics between visual behaviors (e.g.,
tailgating,unauthorized access) and audio cues (e.g., fare transaction sounds).
The system was trained and tested on a custom dataset, achieving an accuracy of
89.5%, precision of 87.2%, and recall of 84.0% in detecting fraudulent
activities, significantly outperforming early fusion baselines and exceeding
the 75% recall rates typically reported in state-of-the-art transportation
fraud detection systems. Our ablation studies demonstrate that the tensor
fusion approach provides a 7.0% improvement in the F1 score and an 8.8% boost
in recall compared to traditional concatenation methods. The solution supports
real-time detection, enabling public transport operators to reduce revenue
loss, improve passenger safety, and ensure operational compliance.

</details>


### [12] [SIEVE: Towards Verifiable Certification for Code-datasets](https://arxiv.org/abs/2510.02166)
*Fatou Ndiaye Mbodji,El-hacen Diallo,Jordan Samhi,Kui Liu,Jacques Klein,Tegawendé F. Bissyande*

Main category: cs.SE

TL;DR: SIEVE是一个社区驱动的框架，将代码数据集的质量检查转化为可验证的置信卡，提供统计保证，以替代传统的静态数据集卡片。


<details>
  <summary>Details</summary>
Motivation: 当前公开代码数据集缺乏可验证的质量保证，静态数据集卡片不可审计且无统计保证，导致质量难以认证，团队需要构建孤立的清洗管道，增加了成本和碎片化。

Method: 提出SIEVE框架，将每个属性的检查转化为置信卡——机器可读、可验证的证书，具有随时有效的统计边界。

Result: 论文概述了将SIEVE推向成熟的研究计划，旨在用随时可验证的认证取代叙述性卡片。

Conclusion: 这一转变有望降低质量保证成本，并增加对代码数据集的信任。

Abstract: Code agents and empirical software engineering rely on public code datasets,
yet these datasets lack verifiable quality guarantees. Static 'dataset cards'
inform, but they are neither auditable nor do they offer statistical
guarantees, making it difficult to attest to dataset quality. Teams build
isolated, ad-hoc cleaning pipelines. This fragments effort and raises cost. We
present SIEVE, a community-driven framework. It turns per-property checks into
Confidence Cards-machine-readable, verifiable certificates with anytime-valid
statistical bounds. We outline a research plan to bring SIEVE to maturity,
replacing narrative cards with anytime-verifiable certification. This shift is
expected to lower quality-assurance costs and increase trust in code-datasets.

</details>


### [13] [TAIBOM: Bringing Trustworthiness to AI-Enabled Systems](https://arxiv.org/abs/2510.02169)
*Vadim Safronov,Anthony McCaigue,Nicholas Allott,Andrew Martin*

Main category: cs.SE

TL;DR: 提出了TAIBOM框架，将SBOM原则扩展到AI领域，解决AI系统依赖管理的独特挑战


<details>
  <summary>Details</summary>
Motivation: 现有SBOM框架无法捕捉AI系统的动态、数据驱动特性及其在数据集、模型和软件组件间的松散耦合依赖关系，需要新的方法来确保AI环境的完整性、信任和合规性

Method: 引入TAIBOM框架，包括：(i)针对AI组件的结构化依赖模型，(ii)在异构AI管道中传播完整性声明的机制，(iii)验证组件来源的信任证明过程

Result: TAIBOM在AI工作流中支持保证、安全和合规性，相比SPDX和CycloneDX等现有标准具有优势

Conclusion: 这项工作通过结构化软件透明度为可信和可验证的AI系统奠定了基础

Abstract: The growing integration of open-source software and AI-driven technologies
has introduced new layers of complexity into the software supply chain,
challenging existing methods for dependency management and system assurance.
While Software Bills of Materials (SBOMs) have become critical for enhancing
transparency and traceability, current frameworks fall short in capturing the
unique characteristics of AI systems -- namely, their dynamic, data-driven
nature and the loosely coupled dependencies across datasets, models, and
software components. These challenges are compounded by fragmented governance
structures and the lack of robust tools for ensuring integrity, trust, and
compliance in AI-enabled environments.
  In this paper, we introduce Trusted AI Bill of Materials (TAIBOM) -- a novel
framework extending SBOM principles to the AI domain. TAIBOM provides (i) a
structured dependency model tailored for AI components, (ii) mechanisms for
propagating integrity statements across heterogeneous AI pipelines, and (iii) a
trust attestation process for verifying component provenance. We demonstrate
how TAIBOM supports assurance, security, and compliance across AI workflows,
highlighting its advantages over existing standards such as SPDX and CycloneDX.
This work lays the foundation for trustworthy and verifiable AI systems through
structured software transparency.

</details>


### [14] [FalseCrashReducer: Mitigating False Positive Crashes in OSS-Fuzz-Gen Using Agentic AI](https://arxiv.org/abs/2510.02185)
*Paschal C. Amusuo,Dongge Liu,Ricardo Andres Calvo Mendez,Jonathan Metzman,Oliver Chang,James C. Davis*

Main category: cs.SE

TL;DR: 本文提出两种AI驱动的策略来减少OSS-Fuzz-Gen中的误报崩溃：基于约束的模糊驱动生成和基于上下文的崩溃验证，在1500个基准函数上分别减少虚假崩溃达8%和报告崩溃减少一半以上。


<details>
  <summary>Details</summary>
Motivation: 自动生成的模糊驱动经常导致误报崩溃，特别是在处理结构化输入和复杂状态需求的函数时，这在工业级模糊驱动生成系统中会损害维护者对系统的信任。

Method: 1. 基于约束的模糊驱动生成：主动对函数输入和状态施加约束来指导驱动创建；2. 基于上下文的崩溃验证：通过分析函数调用者来验证报告的崩溃是否从程序入口点可行。

Result: 在1500个OSS-Fuzz基准函数上，这些策略减少了虚假崩溃达8%，报告崩溃减少超过一半，并证明前沿LLM可以作为可靠的程序分析代理。

Conclusion: 结果突显了将AI集成到大规模模糊测试流水线中的前景和挑战。

Abstract: Fuzz testing has become a cornerstone technique for identifying software bugs
and security vulnerabilities, with broad adoption in both industry and
open-source communities. Directly fuzzing a function requires fuzz drivers,
which translate random fuzzer inputs into valid arguments for the target
function. Given the cost and expertise required to manually develop fuzz
drivers, methods exist that leverage program analysis and Large Language Models
to automatically generate these drivers. However, the generated fuzz drivers
frequently lead to false positive crashes, especially in functions highly
structured input and complex state requirements. This problem is especially
crucial in industry-scale fuzz driver generation efforts like OSS-Fuzz-en, as
reporting false positive crashes to maintainers impede trust in both the system
and the team.
  This paper presents two AI-driven strategies to reduce false positives in
OSS-Fuzz-Gen, a multi-agent system for automated fuzz driver generation. First,
constraint-based fuzz driver generation proactively enforces constraints on a
function's inputs and state to guide driver creation. Second, context-based
crash validation reactively analyzes function callers to determine whether
reported crashes are feasible from program entry points. Using 1,500 benchmark
functions from OSS-Fuzz, we show that these strategies reduce spurious crashes
by up to 8%, cut reported crashes by more than half, and demonstrate that
frontier LLMs can serve as reliable program analysis agents. Our results
highlight the promise and challenges of integrating AI into large-scale fuzzing
pipelines.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [15] [Jailbreaking LLMs via Semantically Relevant Nested Scenarios with Targeted Toxic Knowledge](https://arxiv.org/abs/2510.01223)
*Hui Dou,Ning Xu,Yiwen Zhang,Kaibin Wang*

Main category: cs.CR

TL;DR: 提出了RTS-Attack框架，通过构建与查询高度语义相关的嵌套场景并整合针对性有害知识，成功绕过LLMs的对齐防御，在多种先进LLMs上表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有嵌套场景方法容易被检测，因为恶意意图明显。研究发现LLMs的对齐防御对语义相关且包含针对性有害知识的嵌套场景不敏感，这是一个重要但未被充分探索的方向。

Method: 提出RTS-Attack框架，构建与查询高度语义相关的嵌套场景，并整合针对性有害知识，生成的越狱提示不包含有害查询，具有出色的隐蔽性。

Result: 在GPT-4o、Llama3-70b和Gemini-pro等多种先进LLMs上的广泛实验表明，RTS-Attack在效率和通用性方面优于基线方法。

Conclusion: RTS-Attack是一个自适应、自动化的框架，能够有效检验LLMs的对齐能力，揭示了LLMs对齐防御在特定嵌套场景下的脆弱性。

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities in
various tasks. However, they remain exposed to jailbreak attacks, eliciting
harmful responses. The nested scenario strategy has been increasingly adopted
across various methods, demonstrating immense potential. Nevertheless, these
methods are easily detectable due to their prominent malicious intentions. In
this work, we are the first to find and systematically verify that LLMs'
alignment defenses are not sensitive to nested scenarios, where these scenarios
are highly semantically relevant to the queries and incorporate targeted toxic
knowledge. This is a crucial yet insufficiently explored direction. Based on
this, we propose RTS-Attack (Semantically Relevant Nested Scenarios with
Targeted Toxic Knowledge), an adaptive and automated framework to examine LLMs'
alignment. By building scenarios highly relevant to the queries and integrating
targeted toxic knowledge, RTS-Attack bypasses the alignment defenses of LLMs.
Moreover, the jailbreak prompts generated by RTS-Attack are free from harmful
queries, leading to outstanding concealment. Extensive experiments demonstrate
that RTS-Attack exhibits superior performance in both efficiency and
universality compared to the baselines across diverse advanced LLMs, including
GPT-4o, Llama3-70b, and Gemini-pro. Our complete code is available in the
supplementary material. WARNING: THIS PAPER CONTAINS POTENTIALLY HARMFUL
CONTENT.

</details>


### [16] [Fine-Tuning Jailbreaks under Highly Constrained Black-Box Settings: A Three-Pronged Approach](https://arxiv.org/abs/2510.01342)
*Xiangfang Li,Yu Wang,Bo Li*

Main category: cs.CR

TL;DR: 本文提出了一种三管齐下的越狱攻击方法，能够在仅提供微调数据的黑盒设置下有效绕过LLM的安全防御机制。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型的快速发展，确保其安全使用变得至关重要。现有的微调方法容易受到越狱攻击，但大多数研究关注过于简化的攻击场景，缺乏对实际防御环境的实用性。

Method: 提出三管齐下的越狱攻击：结合安全风格的前缀/后缀包装、敏感词汇的良性词汇编码（下划线）和后门机制，使模型学习有害行为而单个数据点看起来无害。

Result: 在真实部署中，该方法成功越狱OpenAI平台的GPT-4.1和GPT-4o模型，攻击成功率均超过97%。

Conclusion: 该研究表明当前基于数据过滤、防御性微调和安全审计的防御机制在面对精心设计的越狱攻击时存在严重漏洞，需要更强大的安全措施。

Abstract: With the rapid advancement of large language models (LLMs), ensuring their
safe use becomes increasingly critical. Fine-tuning is a widely used method for
adapting models to downstream tasks, yet it is vulnerable to jailbreak attacks.
However, most existing studies focus on overly simplified attack scenarios,
limiting their practical relevance to real-world defense settings. To make this
risk concrete, we present a three-pronged jailbreak attack and evaluate it
against provider defenses under a dataset-only black-box fine-tuning interface.
In this setting, the attacker can only submit fine-tuning data to the provider,
while the provider may deploy defenses across stages: (1) pre-upload data
filtering, (2) training-time defensive fine-tuning, and (3) post-training
safety audit. Our attack combines safety-styled prefix/suffix wrappers, benign
lexical encodings (underscoring) of sensitive tokens, and a backdoor mechanism,
enabling the model to learn harmful behaviors while individual datapoints
appear innocuous. Extensive experiments demonstrate the effectiveness of our
approach. In real-world deployment, our method successfully jailbreaks GPT-4.1
and GPT-4o on the OpenAI platform with attack success rates above 97% for both
models. Our code is available at
https://github.com/lxf728/tri-pronged-ft-attack.

</details>


### [17] [Integrated Security Mechanisms for Weight Protection in Memristive Crossbar Arrays](https://arxiv.org/abs/2510.01350)
*Muhammad Faheemur Rahman,Wayne Burleson*

Main category: cs.CR

TL;DR: 提出了两种保护忆阻器交叉阵列中存储权重的安全机制：键控置换器和水印保护列，可在硬件被攻陷时防止权重被盗用并验证所有权。


<details>
  <summary>Details</summary>
Motivation: 忆阻器交叉阵列中的权重是经过长时间训练获得的知识产权，易受安全威胁，需要保护这些有价值的数据。

Method: 使用键控置换器对权重进行重新排列，并添加水印保护列来建立可验证的所有权，这些机制与现有忆阻器架构兼容。

Result: 在45nm、22nm和7nm CMOS节点上的仿真显示，两种机制在面积、延迟和功耗方面均产生低于10%的开销，并在MNIST数据集上验证了可行性。

Conclusion: 该安全机制能够以最小的性能代价有效保护忆阻内存计算系统，为机器学习硬件的安全提供了实用解决方案。

Abstract: Memristive crossbar arrays enable in-memory computing by performing parallel
analog computations directly within memory, making them well-suited for machine
learning, neural networks, and neuromorphic systems. However, despite their
advantages, non-volatile memristors are vulnerable to security threats (such as
adversarial extraction of stored weights when the hardware is compromised.
Protecting these weights is essential since they represent valuable
intellectual property resulting from lengthy and costly training processes
using large, often proprietary, datasets. As a solution we propose two security
mechanisms: Keyed Permutor and Watermark Protection Columns; where both
safeguard critical weights and establish verifiable ownership (even in cases of
data leakage). Our approach integrates efficiently with existing memristive
crossbar architectures without significant design modifications. Simulations
across 45nm, 22nm, and 7nm CMOS nodes, using a realistic interconnect model and
a large RF dataset, show that both mechanisms offer robust protection with
under 10% overhead in area, delay and power. We also present initial
experiments employing the widely known MNIST dataset; further highlighting the
feasibility of securing memristive in-memory computing systems with minimal
performance trade-offs.

</details>


### [18] [WAInjectBench: Benchmarking Prompt Injection Detections for Web Agents](https://arxiv.org/abs/2510.01354)
*Yinuo Liu,Ruohan Xu,Xilong Wang,Yuqi Jia,Neil Zhenqiang Gong*

Main category: cs.CR

TL;DR: 首个针对网络代理的提示注入攻击检测基准研究，系统评估了文本和图像攻击的检测方法，发现现有检测器对无显式指令或不可感知扰动的攻击效果不佳。


<details>
  <summary>Details</summary>
Motivation: 现有提示注入攻击检测方法未针对网络代理进行系统评估，需要填补这一空白。

Method: 构建包含恶意和良性样本的数据集，系统化文本和图像检测方法，并在多种场景下评估性能。

Result: 部分检测器能识别依赖显式文本指令或可见图像扰动的攻击，但对无显式指令或不可感知扰动的攻击检测效果差。

Conclusion: 需要开发更强大的检测方法来应对各种提示注入攻击，特别是那些不依赖显式指令或使用不可感知扰动的攻击。

Abstract: Multiple prompt injection attacks have been proposed against web agents. At
the same time, various methods have been developed to detect general prompt
injection attacks, but none have been systematically evaluated for web agents.
In this work, we bridge this gap by presenting the first comprehensive
benchmark study on detecting prompt injection attacks targeting web agents. We
begin by introducing a fine-grained categorization of such attacks based on the
threat model. We then construct datasets containing both malicious and benign
samples: malicious text segments generated by different attacks, benign text
segments from four categories, malicious images produced by attacks, and benign
images from two categories. Next, we systematize both text-based and
image-based detection methods. Finally, we evaluate their performance across
multiple scenarios. Our key findings show that while some detectors can
identify attacks that rely on explicit textual instructions or visible image
perturbations with moderate to high accuracy, they largely fail against attacks
that omit explicit instructions or employ imperceptible perturbations. Our
datasets and code are released at:
https://github.com/Norrrrrrr-lyn/WAInjectBench.

</details>


### [19] [Breaking the Code: Security Assessment of AI Code Agents Through Systematic Jailbreaking Attacks](https://arxiv.org/abs/2510.01359)
*Shoumik Saha,Jifan Chen,Sam Mayers,Sanjay Krishna Gouda,Zijian Wang,Varun Kumar*

Main category: cs.CR

TL;DR: JAWS-BENCH是一个评估代码能力LLM代理安全性的基准测试，涵盖三个工作空间级别，发现代码代理在提示条件下平均接受61%的攻击，其中27%能端到端运行。多文件环境下攻击成功率提升至75%，32%的攻击代码可直接部署。


<details>
  <summary>Details</summary>
Motivation: 随着代码能力LLM代理越来越多地嵌入软件开发流程，能够读写和执行代码，使得安全绕过攻击的风险远超纯文本环境。现有评估主要关注拒绝或有害文本检测，但未验证代理是否实际编译和运行恶意程序。

Method: 提出JAWS-BENCH基准测试，涵盖三个逐步升级的工作空间制度（空、单文件、多文件），并配套分层、可执行感知的评估框架，测试合规性、攻击成功率、语法正确性和运行时可执行性。

Result: 在JAWS-0中，代码代理平均接受61%的攻击，58%有害，52%可解析，27%能端到端运行。在JAWS-1中，合规性接近100%，平均攻击成功率约71%。在JAWS-M中，平均攻击成功率提升至75%，32%的攻击代码可立即部署。

Conclusion: 将LLM包装为代理会显著增加漏洞风险，攻击成功率提高1.6倍。需要开发执行感知的防御措施、代码上下文安全过滤器，以及在整个代理多步推理和工具使用过程中保持拒绝决策的机制。

Abstract: Code-capable large language model (LLM) agents are increasingly embedded into
software engineering workflows where they can read, write, and execute code,
raising the stakes of safety-bypass ("jailbreak") attacks beyond text-only
settings. Prior evaluations emphasize refusal or harmful-text detection,
leaving open whether agents actually compile and run malicious programs. We
present JAWS-BENCH (Jailbreaks Across WorkSpaces), a benchmark spanning three
escalating workspace regimes that mirror attacker capability: empty (JAWS-0),
single-file (JAWS-1), and multi-file (JAWS-M). We pair this with a
hierarchical, executable-aware Judge Framework that tests (i) compliance, (ii)
attack success, (iii) syntactic correctness, and (iv) runtime executability,
moving beyond refusal to measure deployable harm. Using seven LLMs from five
families as backends, we find that under prompt-only conditions in JAWS-0, code
agents accept 61% of attacks on average; 58% are harmful, 52% parse, and 27%
run end-to-end. Moving to single-file regime in JAWS-1 drives compliance to ~
100% for capable models and yields a mean ASR (Attack Success Rate) ~ 71%; the
multi-file regime (JAWS-M) raises mean ASR to ~ 75%, with 32% instantly
deployable attack code. Across models, wrapping an LLM in an agent
substantially increases vulnerability -- ASR raises by 1.6x -- because initial
refusals are frequently overturned during later planning/tool-use steps.
Category-level analyses identify which attack classes are most vulnerable and
most readily deployable, while others exhibit large execution gaps. These
findings motivate execution-aware defenses, code-contextual safety filters, and
mechanisms that preserve refusal decisions throughout the agent's multi-step
reasoning and tool use.

</details>


### [20] [E-FuzzEdge: Optimizing Embedded Device Security with Scalable In-Place Fuzzing](https://arxiv.org/abs/2510.01393)
*Davide Rusconi,Osama Yousef,Mirco Picca,Flavio Toffalini,Andrea Lanzi*

Main category: cs.CR

TL;DR: E-FuzzEdge是一种新颖的模糊测试架构，旨在在无法扩展的情况下提高模糊测试活动的吞吐量，通过优化执行速度解决微控制器硬件在环测试的低效问题。


<details>
  <summary>Details</summary>
Motivation: 解决微控制器硬件在环模糊测试中的低效问题，特别是在可扩展性受限的环境中提高测试吞吐量。

Method: 开发了E-FuzzEdge架构，通过优化执行速度来提高模糊测试效率，并与执行设备测试而非固件仿真的嵌入式模糊测试技术兼容。

Result: 在最新基准测试中展现出显著的性能改进，证明系统有效性。

Conclusion: E-FuzzEdge架构能够被更广泛的嵌入式模糊测试社区集成到工作流程中，从而提升整体测试效率。

Abstract: In this paper we show E-FuzzEdge, a novel fuzzing architecture targeted
towards improving the throughput of fuzzing campaigns in contexts where
scalability is unavailable. E-FuzzEdge addresses the inefficiencies of
hardware-in-the-loop fuzzing for microcontrollers by optimizing execution
speed. We evaluated our system against state-of-the-art benchmarks,
demonstrating significant performance improvements. A key advantage of
E-FuzzEdgearchitecture is its compatibility with other embedded fuzzing
techniques that perform on device testing instead of firmware emulation. This
means that the broader embedded fuzzing community can integrate E-FuzzEdge into
their workflows to enhance overall testing efficiency.

</details>


### [21] [Securing IoT Devices in Smart Cities: A Review of Proposed Solutions](https://arxiv.org/abs/2510.01445)
*Andrés F. Betancur-López*

Main category: cs.CR

TL;DR: 对智慧城市中物联网设备安全保护方案的综述研究，重点分析了轻量级密码学、物理不可克隆函数和区块链技术的应用。


<details>
  <summary>Details</summary>
Motivation: 智慧城市中的物联网设备由于计算资源有限且广泛部署，面临严重的安全威胁，需要有效的安全保护方案。

Method: 通过分析近期关于设备级安全的文献，特别关注轻量级密码学、物理不可克隆函数和区块链解决方案。

Result: 研究发现当前方法既有优势也有局限性，需要更实用、可扩展和资源高效的机制。

Conclusion: 需要开发更实用的安全机制来确保物联网生态系统中的用户隐私和数据保护。

Abstract: Privacy and security in Smart Cities remain at constant risk due to the
vulnerabilities introduced by Internet of Things (IoT) devices. The limited
computational resources of these devices make them especially susceptible to
attacks, while their widespread adoption increases the potential impact of
security breaches. This article presents a review of security proposals aimed
at protecting IoT devices in Smart City environments. The review was conducted
by analyzing recent literature on device-level security, with particular
emphasis on lightweight cryptography, physically unclonable functions (PUFs),
and blockchain-based solutions. Findings highlight both the strengths and
limitations of current approaches, as well as the need for more practical,
scalable, and resource-efficient mechanisms to ensure user privacy and data
protection in IoT ecosystems.

</details>


### [22] [POLAR: Automating Cyber Threat Prioritization through LLM-Powered Assessment](https://arxiv.org/abs/2510.01552)
*Luoxi Tang,Yuqiao Meng,Ankita Patra,Weicheng Ma,Muchao Ye,Zhaohan Xi*

Main category: cs.CR

TL;DR: 本文研究了大型语言模型在网络安全威胁情报中的内在脆弱性，揭示了三个关键问题：伪相关、矛盾知识和受限泛化，这些限制了LLMs在CTI任务中的实际应用效果。


<details>
  <summary>Details</summary>
Motivation: 虽然LLMs被广泛用于支持网络安全威胁情报任务，但在实际部署中存在显著的性能差距。本文旨在研究LLMs在CTI中的内在脆弱性，这些脆弱性源于威胁态势本身的性质而非模型架构。

Method: 采用大规模评估方法，在多个CTI基准测试和真实威胁报告上进行实验，引入了一种新颖的分类方法，整合了分层、自回归细化和人在回路监督，以可靠分析失败实例。

Result: 通过广泛实验和人工检查，揭示了三个基本脆弱性：伪相关（模型学习虚假模式）、矛盾知识（模型内部知识冲突）和受限泛化（模型无法有效适应新威胁），这些限制了LLMs在CTI中的有效性。

Conclusion: 为设计更稳健的LLM驱动的CTI系统提供了可行的见解，促进了未来研究的发展，强调了需要解决这些内在脆弱性以提高LLMs在网络安全应用中的可靠性。

Abstract: Large Language Models (LLMs) are intensively used to assist security analysts
in counteracting the rapid exploitation of cyber threats, wherein LLMs offer
cyber threat intelligence (CTI) to support vulnerability assessment and
incident response. While recent work has shown that LLMs can support a wide
range of CTI tasks such as threat analysis, vulnerability detection, and
intrusion defense, significant performance gaps persist in practical
deployments. In this paper, we investigate the intrinsic vulnerabilities of
LLMs in CTI, focusing on challenges that arise from the nature of the threat
landscape itself rather than the model architecture. Using large-scale
evaluations across multiple CTI benchmarks and real-world threat reports, we
introduce a novel categorization methodology that integrates stratification,
autoregressive refinement, and human-in-the-loop supervision to reliably
analyze failure instances. Through extensive experiments and human inspections,
we reveal three fundamental vulnerabilities: spurious correlations,
contradictory knowledge, and constrained generalization, that limit LLMs in
effectively supporting CTI. Subsequently, we provide actionable insights for
designing more robust LLM-powered CTI systems to facilitate future research.

</details>


### [23] [Position: Privacy Is Not Just Memorization!](https://arxiv.org/abs/2510.01645)
*Niloofar Mireshghallah,Tianshi Li*

Main category: cs.CR

TL;DR: 本文认为LLM隐私风险远不止训练数据提取，还包括数据收集、推理时上下文泄漏、自主代理能力和深度推理攻击等更紧迫的威胁，呼吁研究社区超越当前技术方案，采用跨学科方法应对这些新兴威胁。


<details>
  <summary>Details</summary>
Motivation: 当前关于LLM隐私风险的讨论过度关注训练数据的逐字记忆，而更紧迫和可扩展的隐私威胁未被充分探索。本文旨在揭示LLM系统中更广泛的隐私风险格局。

Method: 提出了涵盖LLM生命周期（从数据收集到部署）的隐私风险综合分类法，通过案例研究展示当前隐私框架的不足，并对2016-2025年间1,322篇AI/ML隐私论文进行纵向分析。

Result: 分析显示记忆问题在技术研究中受到过度关注，而最紧迫的隐私危害存在于其他领域，当前技术方法在这些领域缺乏有效应对方案，可行路径仍不明确。

Conclusion: 呼吁研究社区从根本上改变对LLM隐私的认知方式，超越当前技术解决方案的狭隘关注，采用跨学科方法应对这些新兴威胁的社会技术性质。

Abstract: The discourse on privacy risks in Large Language Models (LLMs) has
disproportionately focused on verbatim memorization of training data, while a
constellation of more immediate and scalable privacy threats remain
underexplored. This position paper argues that the privacy landscape of LLM
systems extends far beyond training data extraction, encompassing risks from
data collection practices, inference-time context leakage, autonomous agent
capabilities, and the democratization of surveillance through deep inference
attacks. We present a comprehensive taxonomy of privacy risks across the LLM
lifecycle -- from data collection through deployment -- and demonstrate through
case studies how current privacy frameworks fail to address these multifaceted
threats. Through a longitudinal analysis of 1,322 AI/ML privacy papers
published at leading conferences over the past decade (2016--2025), we reveal
that while memorization receives outsized attention in technical research, the
most pressing privacy harms lie elsewhere, where current technical approaches
offer little traction and viable paths forward remain unclear. We call for a
fundamental shift in how the research community approaches LLM privacy, moving
beyond the narrow focus of current technical solutions and embracing
interdisciplinary approaches that address the sociotechnical nature of these
emerging threats.

</details>


### [24] [Evaluating the Robustness of a Production Malware Detection System to Transferable Adversarial Attacks](https://arxiv.org/abs/2510.01676)
*Milad Nasr,Yanick Fratantonio,Luca Invernizzi,Ange Albertini,Loua Farah,Alex Petit-Bianco,Andreas Terzis,Kurt Thomas,Elie Bursztein,Nicholas Carlini*

Main category: cs.CR

TL;DR: 通过仅修改恶意软件样本的13个字节，可以成功绕过Gmail的Magika机器学习模型，在90%的情况下实现恶意文件传输。研究还开发了防御措施，将攻击成功率降低到20%。


<details>
  <summary>Details</summary>
Motivation: 随着深度学习模型在生产系统中广泛部署，其个体缺陷可能引发系统级漏洞。本研究旨在分析针对ML组件的对抗性攻击如何破坏整个生产级恶意软件检测系统。

Method: 对Gmail恶意软件检测管道进行案例研究，设计针对Magika模型的对抗样本，使恶意软件被错误路由到不合适的检测器。

Result: 仅修改13字节即可在90%情况下成功绕过Magika检测；开发的防御措施使高资源攻击者需要50字节才能达到20%攻击成功率。

Conclusion: ML组件的安全漏洞可能引发系统级风险，但通过适当的防御措施可以有效缓解此类攻击。该防御方案已与Google工程师合作部署到Gmail分类器中。

Abstract: As deep learning models become widely deployed as components within larger
production systems, their individual shortcomings can create system-level
vulnerabilities with real-world impact. This paper studies how adversarial
attacks targeting an ML component can degrade or bypass an entire
production-grade malware detection system, performing a case study analysis of
Gmail's pipeline where file-type identification relies on a ML model.
  The malware detection pipeline in use by Gmail contains a machine learning
model that routes each potential malware sample to a specialized malware
classifier to improve accuracy and performance. This model, called Magika, has
been open sourced. By designing adversarial examples that fool Magika, we can
cause the production malware service to incorrectly route malware to an
unsuitable malware detector thereby increasing our chance of evading detection.
Specifically, by changing just 13 bytes of a malware sample, we can
successfully evade Magika in 90% of cases and thereby allow us to send malware
files over Gmail. We then turn our attention to defenses, and develop an
approach to mitigate the severity of these types of attacks. For our defended
production model, a highly resourced adversary requires 50 bytes to achieve
just a 20% attack success rate. We implement this defense, and, thanks to a
collaboration with Google engineers, it has already been deployed in production
for the Gmail classifier.

</details>


### [25] [Towards Imperceptible Adversarial Defense: A Gradient-Driven Shield against Facial Manipulations](https://arxiv.org/abs/2510.01699)
*Yue Li,Linying Xue,Dongdong Lin,Qiushi Li,Hui Tian,Hongxia Wang*

Main category: cs.CR

TL;DR: 提出GRASP方法，通过梯度投影机制解决防御效果与视觉质量之间的梯度冲突，在保持高防御成功率的同时实现更好的视觉质量。


<details>
  <summary>Details</summary>
Motivation: 现有主动防御方法在不可感知性和防御效果之间存在权衡，强扰动会破坏伪造但降低视觉保真度，而现有方法忽视了损失函数间的梯度冲突。

Method: 提出GRASP方法，首次成功整合结构相似性损失和低频损失来增强扰动不可感知性，通过梯度投影机制缓解防御效果损失与视觉质量损失间的梯度冲突。

Result: 实验验证GRASP的有效性，PSNR超过40 dB，SSIM达0.99，对抗面部属性操纵的防御成功率达100%，在视觉质量上显著优于现有方法。

Conclusion: GRASP方法通过平衡优化实现了在保持图像保真度的同时不牺牲防御性能，为对抗深度伪造提供了有效的主动防御解决方案。

Abstract: With the flourishing prosperity of generative models, manipulated facial
images have become increasingly accessible, raising concerns regarding privacy
infringement and societal trust. In response, proactive defense strategies
embed adversarial perturbations into facial images to counter deepfake
manipulation. However, existing methods often face a tradeoff between
imperceptibility and defense effectiveness-strong perturbations may disrupt
forgeries but degrade visual fidelity. Recent studies have attempted to address
this issue by introducing additional visual loss constraints, yet often
overlook the underlying gradient conflicts among losses, ultimately weakening
defense performance. To bridge the gap, we propose a gradient-projection-based
adversarial proactive defense (GRASP) method that effectively counters facial
deepfakes while minimizing perceptual degradation. GRASP is the first approach
to successfully integrate both structural similarity loss and low-frequency
loss to enhance perturbation imperceptibility. By analyzing gradient conflicts
between defense effectiveness loss and visual quality losses, GRASP pioneers
the design of the gradient-projection mechanism to mitigate these conflicts,
enabling balanced optimization that preserves image fidelity without
sacrificing defensive performance. Extensive experiments validate the efficacy
of GRASP, achieving a PSNR exceeding 40 dB, SSIM of 0.99, and a 100% defense
success rate against facial attribute manipulations, significantly
outperforming existing approaches in visual quality.

</details>


### [26] [Constructions of Efficiently Implementable Boolean Functions with Provable Nonlinearity/Resiliency/Algebraic Immunity Trade-Offs](https://arxiv.org/abs/2510.01720)
*Palash Sarkar*

Main category: cs.CR

TL;DR: 提出了多个布尔函数族，在弹性、非线性度和代数免疫性之间实现了可证明的权衡，且具有线性复杂度的实现。


<details>
  <summary>Details</summary>
Motivation: 在密码学中，布尔函数需要同时满足多个安全属性（弹性、非线性度、代数免疫性），但传统方法难以在这些属性之间取得良好平衡。

Method: 构造了多个布尔函数族，通过参数化设计实现在给定弹性、非线性度和代数免疫性要求下的高效实现。

Result: 对于任意给定的参数m₀≥0、x₀≥1、a₀≥1，可以构造n变量函数，具有至少m₀的弹性、至多2^{-x₀}的线性偏差和至少a₀的代数免疫性，且n与参数呈线性关系，可用O(n)门实现。

Conclusion: 提出的函数族在布尔函数的安全属性之间实现了有效的权衡，为密码学应用提供了实用的构造方法。

Abstract: We describe several families of efficiently implementable Boolean functions
achieving provable trade-offs between resiliency, nonlinearity, and algebraic
immunity. In concrete terms, the following result holds for each of the
function families that we propose. Given integers $m_0\geq 0$, $x_0\geq 1$, and
$a_0\geq 1$, it is possible to construct an $n$-variable function which has
resiliency at least $m_0$, linear bias (which is an equivalent method of
expressing nonlinearity) at most $2^{-x_0}$ and algebraic immunity at least
$a_0$; further, $n$ is linear in $m_0$, $x_0$ and $a_0$, and the function can
be implemented using $O(n)$ gates.

</details>


### [27] [Secure Multi-Modal Data Fusion in Federated Digital Health Systems via MCP](https://arxiv.org/abs/2510.01780)
*Aueaphum Aueawatthanaphisut*

Main category: cs.CR

TL;DR: 提出了一个基于模型上下文协议（MCP）的多模态联邦医疗框架，通过标准化接口实现异构医疗数据的隐私保护融合，在诊断准确性和客户端稳定性方面显著优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 解决异构医疗数据安全互操作性的重大挑战，现有联邦学习框架缺乏多模态数据融合的标准化机制，特别是在分布式和资源受限环境中的协调问题。

Method: 使用MCP作为互操作性层，构建包含三个支柱的架构：多模态特征对齐、差分隐私安全聚合、能量感知调度，通过模式驱动接口实现AI代理和工具链的自适应编排。

Result: 在基准数据集和临床队列实验中，诊断准确性比基线联邦学习提高9.8%，客户端退出率降低54%，实现了临床可接受的隐私-效用权衡。

Conclusion: MCP支持的多模态融合为构建可扩展、可信赖的下一代联邦医疗基础设施提供了可行路径，有助于实现公平的健康服务。

Abstract: Secure and interoperable integration of heterogeneous medical data remains a
grand challenge in digital health. Current federated learning (FL) frameworks
offer privacy-preserving model training but lack standardized mechanisms to
orchestrate multi-modal data fusion across distributed and resource-constrained
environments. This study introduces a novel framework that leverages the Model
Context Protocol (MCP) as an interoperability layer for secure, cross-agent
communication in multi-modal federated healthcare systems. The proposed
architecture unifies three pillars: (i) multi-modal feature alignment for
clinical imaging, electronic medical records, and wearable IoT data; (ii)
secure aggregation with differential privacy to protect patient-sensitive
updates; and (iii) energy-aware scheduling to mitigate dropouts in mobile
clients. By employing MCP as a schema-driven interface, the framework enables
adaptive orchestration of AI agents and toolchains while ensuring compliance
with privacy regulations. Experimental evaluation on benchmark datasets and
pilot clinical cohorts demonstrates up to 9.8\% improvement in diagnostic
accuracy compared with baseline FL, a 54\% reduction in client dropout rates,
and clinically acceptable privacy--utility trade-offs. These results highlight
MCP-enabled multi-modal fusion as a scalable and trustworthy pathway toward
equitable, next-generation federated health infrastructures.

</details>


### [28] [ZK-WAGON: Imperceptible Watermark for Image Generation Models using ZK-SNARKs](https://arxiv.org/abs/2510.01967)
*Aadarsh Anantha Ramakrishnan,Shubham Agarwal,Selvanayagam S,Kunwar Singh*

Main category: cs.CR

TL;DR: ZK-WAGON是首个使用ZK-SNARKs技术为图像生成模型添加水印的系统，能够在保护模型权重和敏感信息的同时，提供可验证的来源证明。


<details>
  <summary>Details</summary>
Motivation: 随着图像生成模型能力增强，合成媒体的真实性、所有权和滥用问题变得关键。传统水印方法会降低图像质量、易被移除或需要访问模型内部信息，不适合安全可扩展部署。

Method: 提出选择性层ZK电路创建(SL-ZKCC)方法，选择性地将图像生成模型的关键层转换为电路，显著减少证明生成时间。生成的ZK-SNARK证明通过LSB隐写术不可察觉地嵌入到生成图像中。

Result: 在GAN和扩散模型上演示了该系统，提供了一个安全、模型无关的可信AI图像生成流程。

Conclusion: ZK-WAGON为图像生成模型提供了安全的水印解决方案，能够在保护隐私的同时验证图像来源，解决了合成媒体认证的关键问题。

Abstract: As image generation models grow increasingly powerful and accessible,
concerns around authenticity, ownership, and misuse of synthetic media have
become critical. The ability to generate lifelike images indistinguishable from
real ones introduces risks such as misinformation, deepfakes, and intellectual
property violations. Traditional watermarking methods either degrade image
quality, are easily removed, or require access to confidential model internals
- making them unsuitable for secure and scalable deployment. We are the first
to introduce ZK-WAGON, a novel system for watermarking image generation models
using the Zero-Knowledge Succinct Non Interactive Argument of Knowledge
(ZK-SNARKs). Our approach enables verifiable proof of origin without exposing
model weights, generation prompts, or any sensitive internal information. We
propose Selective Layer ZK-Circuit Creation (SL-ZKCC), a method to selectively
convert key layers of an image generation model into a circuit, reducing proof
generation time significantly. Generated ZK-SNARK proofs are imperceptibly
embedded into a generated image via Least Significant Bit (LSB) steganography.
We demonstrate this system on both GAN and Diffusion models, providing a
secure, model-agnostic pipeline for trustworthy AI image generation.

</details>


### [29] [Mirage Fools the Ear, Mute Hides the Truth: Precise Targeted Adversarial Attacks on Polyphonic Sound Event Detection Systems](https://arxiv.org/abs/2510.02158)
*Junjie Su,Weifei Jin,Yuxin Cao,Derui Wang,Kai Ye,Jie Hao*

Main category: cs.CR

TL;DR: 提出了Mirage和Mute攻击(M2A)框架，针对多音源声音事件检测系统进行精确的定向对抗攻击，通过保护损失确保非目标区域不受影响，并引入编辑精度(EP)评估指标。


<details>
  <summary>Details</summary>
Motivation: 声音事件检测系统在安全关键应用中部署日益增多，但其对抗攻击鲁棒性研究不足。现有攻击方法要么因SED的强上下文依赖性而效果不佳，要么因只关注目标区域而影响非目标区域，缺乏精确性。

Method: 提出M2A框架，在优化过程中对非目标输出施加特定约束（保护损失），确保攻击不改变非目标区域的模型输出。同时引入编辑精度(EP)作为新的评估指标来平衡有效性和精确性。

Result: 在两个最先进的SED模型上，M2A分别达到94.56%和99.11%的编辑精度，证明该框架在保持足够有效性的同时显著提高了攻击精确性。

Conclusion: M2A框架成功解决了SED系统对抗攻击中的精确性问题，通过保护损失机制实现了对目标区域的精确攻击而不影响非目标区域，为SED系统的安全性评估提供了新方法。

Abstract: Sound Event Detection (SED) systems are increasingly deployed in
safety-critical applications such as industrial monitoring and audio
surveillance. However, their robustness against adversarial attacks has not
been well explored. Existing audio adversarial attacks targeting SED systems,
which incorporate both detection and localization capabilities, often lack
effectiveness due to SED's strong contextual dependencies or lack precision by
focusing solely on misclassifying the target region as the target event,
inadvertently affecting non-target regions. To address these challenges, we
propose the Mirage and Mute Attack (M2A) framework, which is designed for
targeted adversarial attacks on polyphonic SED systems. In our optimization
process, we impose specific constraints on the non-target output, which we
refer to as preservation loss, ensuring that our attack does not alter the
model outputs for non-target region, thus achieving precise attacks.
Furthermore, we introduce a novel evaluation metric Editing Precison (EP) that
balances effectiveness and precision, enabling our method to simultaneously
enhance both. Comprehensive experiments show that M2A achieves 94.56% and
99.11% EP on two state-of-the-art SED models, demonstrating that the framework
is sufficiently effective while significantly enhancing attack precision.

</details>


### [30] [NoMod: A Non-modular Attack on Module Learning With Errors](https://arxiv.org/abs/2510.02162)
*Cristian Bassotto,Ermes Franch,Marina Krček,Stjepan Picek*

Main category: cs.CR

TL;DR: NoMod ML-Attack是一种混合白盒密码分析方法，通过将模运算的环绕视为统计噪声，将密钥恢复问题转化为鲁棒线性估计，成功攻击了基于Module-LWE的后量子密码方案。


<details>
  <summary>Details</summary>
Motivation: 量子计算的发展威胁经典公钥密码学，NIST采用了基于Module-LWE问题的后量子方案。需要开发能够绕过模运算建模挑战的新型攻击方法。

Method: 结合优化的格预处理（包括减少向量保存和代数放大）与通过Tukey's Biweight损失训练的鲁棒估计器，将环绕视为统计损坏，将密钥恢复转化为鲁棒线性估计问题。

Result: 实验显示：对维度n=350的二元密钥实现完全恢复；对n=256的稀疏二项式密钥实现恢复；在CRYSTALS-Kyber参数(n,k)=(128,3)和(256,2)下成功恢复稀疏密钥。

Conclusion: NoMod方法有效规避了模运算建模的挑战，为基于Module-LWE的后量子密码方案提供了新的攻击视角，表明需要更强的安全分析。

Abstract: The advent of quantum computing threatens classical public-key cryptography,
motivating NIST's adoption of post-quantum schemes such as those based on the
Module Learning With Errors (Module-LWE) problem. We present NoMod ML-Attack, a
hybrid white-box cryptanalytic method that circumvents the challenge of
modeling modular reduction by treating wrap-arounds as statistical corruption
and casting secret recovery as robust linear estimation. Our approach combines
optimized lattice preprocessing--including reduced-vector saving and algebraic
amplification--with robust estimators trained via Tukey's Biweight loss.
Experiments show NoMod achieves full recovery of binary secrets for dimension
$n = 350$, recovery of sparse binomial secrets for $n = 256$, and successful
recovery of sparse secrets in CRYSTALS-Kyber settings with parameters $(n, k) =
(128, 3)$ and $(256, 2)$. We release our implementation in an anonymous
repository https://anonymous.4open.science/r/NoMod-3BD4.

</details>


### [31] [Testing Stability and Robustness in Three Cryptographic Chaotic Systems](https://arxiv.org/abs/2510.02184)
*N. A. Anagnostopoulos,K. Konstantinidis,A. N. Miliou,S. G. Stavrinides*

Main category: cs.CR

TL;DR: 测试三种不同密码混沌系统的稳定性和鲁棒性，比较它们在噪声环境下的同步性能与安全性


<details>
  <summary>Details</summary>
Motivation: 在实际应用中，即使存在噪声，驱动-响应系统也需要始终保持同步，这对系统稳定性提出了要求

Method: 对三种著名的密码混沌系统进行稳定性和鲁棒性测试

Result: 比较了不同系统在噪声环境下的同步性能与安全性表现

Conclusion: 为选择适合实际应用的密码混沌系统提供了性能比较依据

Abstract: In practical applications, it is crucial that the drive-response systems,
although identical in all respects, are synchronized at all times, even if
there is noise present. In this work, we test the stability and robustness of
three distinct and well-known cryptographic chaotic systems, and compare the
results in relation to the desired security.

</details>


### [32] [Authentication Security of PRF GNSS Ranging](https://arxiv.org/abs/2510.02196)
*Jason Anderson*

Main category: cs.CR

TL;DR: 本文分析了基于伪随机函数的GNSS测距认证安全性，针对多种欺骗模型（包括SCER欺骗器），推导了认证安全边界。应用于伽利略E6-C信号认证服务，计算得出在非SCER模型下最多需要400ms数据实现128位认证安全。


<details>
  <summary>Details</summary>
Motivation: 当GNSS测距码来自仅广播者知道的秘密伪随机函数时，欺骗者无法在广播前预测测距码。因此PRF测距可用于建立对GNSS伪距和PNT解决方案的信任。

Method: 推导PRF GNSS测距在多种欺骗模型下的认证安全性，包括安全码估计和重放(SCER)欺骗器。应用该方法分析伽利略信号认证服务(SAS)使用加密E6-C信号的情况。

Result: 对于伽利略E6-C信号，在非SCER模型下最多需要400ms数据实现128位认证安全。对于SCER对手，预测了破解认证安全所需的接收无线电设备要求。

Conclusion: 本工作可用于设计PRF GNSS测距协议，通过计算漏检概率来满足有用的认证安全要求。

Abstract: This work derives the authentication security of pseudorandom function (PRF)
GNSS ranging under multiple GNSS spoofing models, including the Security Code
Estimation and Replay (SCER) spoofer. When GNSS ranging codes derive from a PRF
utilizing a secret known only to the broadcaster, the spoofer cannot predict
the ranging code before broadcast. Therefore, PRF ranging can be used to
establish trust in the GNSS pseudoranges and the resulting receiver position,
navigation, and timing (PNT) solution. I apply the methods herein to Galileo's
Signal Authentication Service (SAS) utilizing the encrypted Galileo E6-C signal
to compute that, at most, 400 ms of Galileo E6-C data to assert 128-bit
authentication security under non-SCER models. For the SCER adversary, I
predict the adversary's needed receiving radio equipment to break
authentication security. One can use this work to design a PRF GNSS ranging
protocol to meet useful authentication security requirements by computing the
probability of missed detection.

</details>


### [33] [An efficient quantum algorithm for computing $S$-units and its applications](https://arxiv.org/abs/2510.02280)
*Jean-Francois Biasse,Fang Song*

Main category: cs.CR

TL;DR: 本文详细证明了Biasse和Song的量子多项式时间算法，用于计算数域的S-单位群，该算法可应用于计算类群、解决主理想问题、寻找理想格中的短向量等。


<details>
  <summary>Details</summary>
Motivation: 提供量子多项式时间算法的详细证明，扩展其在数论和密码学中的应用，包括计算类群、解决主理想问题以及寻找理想格中的短向量。

Method: 基于Biasse和Song的量子多项式时间算法，结合Cramer等人的结果，解决主理想问题并分解理想类。

Result: 算法可直接计算类群、S-类群、相对类群、单位群、射线类群，解决主理想问题和某些范数方程，分解理想类，并找到主理想的短生成元和理想格中的“轻度短向量”。

Conclusion: 该量子多项式时间算法在数论和密码学中具有广泛的应用潜力，特别是在计算代数数论和理想格密码学中。

Abstract: In this paper, we provide details on the proofs of the quantum polynomial
time algorithm of Biasse and Song (SODA 16) for computing the $S$-unit group of
a number field. This algorithm directly implies polynomial time methods to
calculate class groups, S-class groups, relative class group and the unit
group, ray class groups, solve the principal ideal problem, solve certain norm
equations, and decompose ideal classes in the ideal class group. Additionally,
combined with a result of Cramer, Ducas, Peikert and Regev (Eurocrypt 2016),
the resolution of the principal ideal problem allows one to find short
generators of a principal ideal. Likewise, methods due to Cramer, Ducas and
Wesolowski (Eurocrypt 2017) use the resolution of the principal ideal problem
and the decomposition of ideal classes to find so-called ``mildly short
vectors'' in ideal lattices of cyclotomic fields.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [34] [OR-Toolformer: Modeling and Solving Operations Research Problems with Tool Augmented Large Language Models](https://arxiv.org/abs/2510.01253)
*Jianzhang Zhang,Jialong Zhou,Chuang Liu*

Main category: cs.AI

TL;DR: OR-Toolformer通过微调Llama-3.1-8B-Instruct模型，结合半自动数据合成流程和外部求解器，在运筹学问题上实现了高精度求解，在三个基准测试中达到80.1%的执行准确率，并在零样本评估中显著优于基线模型。


<details>
  <summary>Details</summary>
Motivation: 解决大型语言模型在运筹学任务中依赖闭源API带来的隐私问题，以及从头训练开源模型的高计算成本问题。

Method: 使用半自动数据合成流程生成多样化的运筹学问题-答案对，通过微调Llama-3.1-8B-Instruct模型，并增强模型调用外部求解器的能力。

Result: 在四个标准基准测试中的三个上，OR-Toolformer达到80.1%的执行准确率，比同规模基线模型高出4.3%以上；在两种未见过的运筹学问题类型的零样本评估中，平均准确率达到54%，比最强基线提高21个百分点。

Conclusion: 工具增强的微调方法对于准确且可泛化的运筹学问题建模和求解具有显著效果。

Abstract: Large language models (LLMs) demonstrate strong mathematical reasoning, but
reliance on closed-source APIs for OR tasks raises privacy concerns, and
training open-source models from scratch incurs high compute costs. We
introduce OR-Toolformer, which fine-tunes Llama-3.1-8B-Instruct with a
semi-automatic data synthesis pipeline that generates diverse OR problem-answer
pairs and augments the model with external solvers to produce API calls. On
three of four standard benchmarks, OR-Toolformer achieves up to 80.1% execution
accuracy, exceeding size-matched baselines by over 4.3%. In zero-shot
evaluation on two unseen OR problem types, it attains 54% average accuracy, a
21 percentage-point improvement over the strongest baseline. These findings
validate the efficacy of tool-augmented fine-tuning LLMs for accurate and
generalizable OR problem modeling and solving.

</details>


### [35] [Modeling Others' Minds as Code](https://arxiv.org/abs/2510.01272)
*Kunal Jha,Aydan Yuenan Huang,Eric Ye,Natasha Jaques,Max Kleiman-Weiner*

Main category: cs.AI

TL;DR: 提出了ROTE算法，将人类行为建模为可执行的程序代码而非基于信念和欲望的策略，结合LLM生成行为程序假设空间和概率推理处理不确定性，在稀疏观测下预测人类行为表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有的人类行为建模方法要么对理性做出不切实际的假设，要么计算量过大难以快速适应。日常社交互动往往遵循可预测的'脚本'模式，这些高效的行为模式能最小化认知负荷。

Method: 将日常行为模式建模为行为程序，使用LLM合成行为程序的假设空间，结合概率推理处理不确定性。在网格世界任务和大规模家庭模拟器中测试。

Result: ROTE在稀疏观测下预测人类和AI行为，在样本内准确性和样本外泛化方面比行为克隆和基于LLM的方法高出50%。

Conclusion: 将动作理解视为程序合成问题，为AI系统在现实世界中高效有效预测人类行为开辟了新路径。

Abstract: Accurate prediction of human behavior is essential for robust and safe
human-AI collaboration. However, existing approaches for modeling people are
often data-hungry and brittle because they either make unrealistic assumptions
about rationality or are too computationally demanding to adapt rapidly. Our
key insight is that many everyday social interactions may follow predictable
patterns; efficient "scripts" that minimize cognitive load for actors and
observers, e.g., "wait for the green light, then go." We propose modeling these
routines as behavioral programs instantiated in computer code rather than
policies conditioned on beliefs and desires. We introduce ROTE, a novel
algorithm that leverages both large language models (LLMs) for synthesizing a
hypothesis space of behavioral programs, and probabilistic inference for
reasoning about uncertainty over that space. We test ROTE in a suite of
gridworld tasks and a large-scale embodied household simulator. ROTE predicts
human and AI behaviors from sparse observations, outperforming competitive
baselines -- including behavior cloning and LLM-based methods -- by as much as
50% in terms of in-sample accuracy and out-of-sample generalization. By
treating action understanding as a program synthesis problem, ROTE opens a path
for AI systems to efficiently and effectively predict human behavior in the
real-world.

</details>


### [36] [Cyber Academia-Chemical Engineering (CA-ChemE): A Living Digital Town for Self-Directed Research Evolution and Emergent Scientific Discovery](https://arxiv.org/abs/2510.01293)
*Zekun Jiang,Chunming Xu,Tianhang Zhou*

Main category: cs.AI

TL;DR: 提出了CA-ChemE系统，一个通过多智能体协作实现自主研究进化和科学发现的生命数字城镇，解决了AI在化学工程中跨学科合作和探索未知问题的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有AI系统在跨学科合作和探索未知问题方面存在局限，需要构建能够自主进化和发现科学知识的智能生态系统。

Method: 集成领域特定知识库、知识增强技术和协作智能体，构建多智能体协作系统，并引入具备本体工程能力的协作智能体(CA)来提升跨领域合作效率。

Result: 知识库增强机制使所有七个专家智能体的对话质量得分平均提高10-15%；协作智能体的介入使远领域专家对的合作效率提升8.5%，而近领域对仅提升0.8%，揭示了"知识库差距导致的协作效率降低"效应。

Conclusion: 精心设计的多智能体架构为化学工程领域的自主科学发现提供了可行路径。

Abstract: The rapid advancement of artificial intelligence (AI) has demonstrated
substantial potential in chemical engineering, yet existing AI systems remain
limited in interdisciplinary collaboration and exploration of uncharted
problems. To address these issues, we present the Cyber Academia-Chemical
Engineering (CA-ChemE) system, a living digital town that enables self-directed
research evolution and emergent scientific discovery through multi-agent
collaboration. By integrating domain-specific knowledge bases, knowledge
enhancement technologies, and collaboration agents, the system successfully
constructs an intelligent ecosystem capable of deep professional reasoning and
efficient interdisciplinary collaboration. Our findings demonstrate that
knowledge base-enabled enhancement mechanisms improved dialogue quality scores
by 10-15% on average across all seven expert agents, fundamentally ensuring
technical judgments are grounded in verifiable scientific evidence. However, we
observed a critical bottleneck in cross-domain collaboration efficiency,
prompting the introduction of a Collaboration Agent (CA) equipped with ontology
engineering capabilities. CA's intervention achieved 8.5% improvements for
distant-domain expert pairs compared to only 0.8% for domain-proximate pairs -
a 10.6-fold difference - unveiling the "diminished collaborative efficiency
caused by knowledge-base gaps" effect. This study demonstrates how carefully
designed multi-agent architectures can provide a viable pathway toward
autonomous scientific discovery in chemical engineering.

</details>


### [37] [The Social Laboratory: A Psychometric Framework for Multi-Agent LLM Evaluation](https://arxiv.org/abs/2510.01295)
*Zarreen Reza*

Main category: cs.AI

TL;DR: 提出了一种新的评估框架，使用多智能体辩论作为受控"社会实验室"来发现和量化LLM智能体在交互环境中出现的社会和认知动态行为。


<details>
  <summary>Details</summary>
Motivation: 传统评估基准在衡量下游任务性能方面变得不足，无法捕捉智能体在交互环境中沟通、说服和协作时出现的社会和认知动态。

Method: 使用基于LLM的智能体，配备不同角色和激励，在LLM主持人的监督下就各种挑战性话题进行辩论，并通过新的心理测量和语义指标进行分析。

Result: 发现智能体具有寻求共识的强大趋势，即使没有明确指示也能达到高语义一致性（μ>0.88）；分配的角色会诱导稳定的心理测量特征；主持人的角色能显著改变辩论结果。

Conclusion: 这项工作为智能体环境提供了一类新的动态、心理测量基础评估协议的蓝图，为理解和塑造下一代AI智能体的社会行为提供了关键方法。

Abstract: As Large Language Models (LLMs) transition from static tools to autonomous
agents, traditional evaluation benchmarks that measure performance on
downstream tasks are becoming insufficient. These methods fail to capture the
emergent social and cognitive dynamics that arise when agents communicate,
persuade, and collaborate in interactive environments. To address this gap, we
introduce a novel evaluation framework that uses multi-agent debate as a
controlled "social laboratory" to discover and quantify these behaviors. In our
framework, LLM-based agents, instantiated with distinct personas and
incentives, deliberate on a wide range of challenging topics under the
supervision of an LLM moderator. Our analysis, enabled by a new suite of
psychometric and semantic metrics, reveals several key findings. Across
hundreds of debates, we uncover a powerful and robust emergent tendency for
agents to seek consensus, consistently reaching high semantic agreement ({\mu}
> 0.88) even without explicit instruction and across sensitive topics. We show
that assigned personas induce stable, measurable psychometric profiles,
particularly in cognitive effort, and that the moderators persona can
significantly alter debate outcomes by structuring the environment, a key
finding for external AI alignment. This work provides a blueprint for a new
class of dynamic, psychometrically grounded evaluation protocols designed for
the agentic setting, offering a crucial methodology for understanding and
shaping the social behaviors of the next generation of AI agents. We have
released the code and results at
https://github.com/znreza/multi-agent-LLM-eval-for-debate.

</details>


### [38] [Agentic Jigsaw Interaction Learning for Enhancing Visual Perception and Reasoning in Vision-Language Models](https://arxiv.org/abs/2510.01304)
*Yu Zeng,Wenxuan Huang,Shiting Huang,Xikun Bao,Yukun Qi,Yiming Zhao,Qiuchen Wang,Lin Chen,Zehui Chen,Huaian Chen,Wanli Ouyang,Feng Zhao*

Main category: cs.AI

TL;DR: AGILE通过将拼图任务构建为交互式学习过程，显著提升了视觉语言模型的感知和推理能力，在2×2拼图任务中准确率从9.5%提升至82.8%，并在9个通用视觉任务上平均提升3.1%。


<details>
  <summary>Details</summary>
Motivation: 当前大型视觉语言模型在多模态理解和推理方面虽有进步，但在基本感知和推理能力上仍有限制，特别是在简单拼图任务上表现接近随机，揭示了核心能力的不足。高质量视觉语言数据的稀缺和可扩展性限制也制约了模型能力的提升。

Method: 提出AGILE方法，将拼图解决构建为交互过程，模型在每个步骤生成可执行代码执行动作，环境提供细粒度视觉反馈。通过观察和交互的迭代循环，模型通过探索和反馈逐步提升感知和推理能力。

Result: AGILE在复杂度不同的拼图任务上显著提升性能（2×2设置下准确率从9.5%提升至82.8%），并在9个通用视觉任务上表现出强泛化能力，平均提升3.1%。

Conclusion: 这项工作为推进多模态模型的推理和泛化能力开辟了新途径，并为多模态强化学习数据稀缺问题提供了高效、可扩展的解决方案。

Abstract: Although current large Vision-Language Models (VLMs) have advanced in
multimodal understanding and reasoning, their fundamental perceptual and
reasoning abilities remain limited. Specifically, even on simple jigsaw tasks,
existing VLMs perform near randomly, revealing deficiencies in core perception
and reasoning capabilities. While high-quality vision-language data can enhance
these capabilities, its scarcity and limited scalability impose significant
constraints. To address this, we propose AGILE, an Agentic jiGsaw Interaction
Learning for Enhancing visual perception and reasoning in VLMs. AGILE
formulates jigsaw solving as an interactive process, enabling the model to
progressively engage with the environment. At each step, the model generates
executable code to perform an action based on the current state, while the
environment provides fine-grained visual feedback to guide task completion.
Through this iterative cycle of observation and interaction, the model
incrementally improves its perceptual and reasoning capabilities via
exploration and feedback. Experimental results show that AGILE not only
substantially boosts performance on jigsaw tasks of varying complexity (e.g.,
increasing accuracy from 9.5% to 82.8% under the 2 $\times$ 2 setting) but also
demonstrates strong generalization across 9 general vision tasks, achieving an
average improvement of 3.1%. These results indicate notable enhancements in
both perceptual and reasoning abilities. This work opens a new avenue for
advancing reasoning and generalization in multimodal models and provides an
efficient, scalable solution to the scarcity of multimodal reinforcement
learning data. The code and datasets is available at
https://github.com/yuzeng0-0/AGILE .

</details>


### [39] [Aristotle: IMO-level Automated Theorem Proving](https://arxiv.org/abs/2510.01346)
*Tudor Achim,Alex Best,Kevin Der,Mathïs Fédérico,Sergei Gukov,Daniel Halpern-Leister,Kirsten Henningsgard,Yury Kudryashov,Alexander Meiburg,Martin Michelsen,Riley Patterson,Eric Rodriguez,Laura Scharff,Vikram Shanker,Vladmir Sicca,Hari Sowrirajan,Aidan Swope,Matyas Tamas,Vlad Tenev,Jonathan Thomm,Harold Williams,Lawrence Wu*

Main category: cs.AI

TL;DR: Aristotle系统结合形式验证与非正式推理，在2025年国际数学奥林匹克竞赛中达到金牌级别表现


<details>
  <summary>Details</summary>
Motivation: 开发能够结合形式验证的严谨性和人类数学推理灵活性的AI系统，以解决复杂的数学问题

Method: 集成三个主要组件：Lean证明搜索系统、生成并形式化引理的非正式推理系统、专用几何求解器

Result: 在2025年国际数学奥林匹克竞赛中实现金牌等效表现，展示了自动定理证明的最先进性能

Conclusion: Aristotle系统证明了结合形式验证与非正式推理的方法在解决复杂数学问题上的有效性，具有良好的扩展性

Abstract: We introduce Aristotle, an AI system that combines formal verification with
informal reasoning, achieving gold-medal-equivalent performance on the 2025
International Mathematical Olympiad problems. Aristotle integrates three main
components: a Lean proof search system, an informal reasoning system that
generates and formalizes lemmas, and a dedicated geometry solver. Our system
demonstrates state-of-the-art performance with favorable scaling properties for
automated theorem proving.

</details>


### [40] [MEMTRACK: Evaluating Long-Term Memory and State Tracking in Multi-Platform Dynamic Agent Environments](https://arxiv.org/abs/2510.01353)
*Darshan Deshpande,Varun Gangal,Hersh Mehta,Anand Kannappan,Rebecca Qian,Peng Wang*

Main category: cs.AI

TL;DR: MEMTRACK是一个专门评估多平台代理环境中长期记忆和状态跟踪的基准测试，模拟真实企业工作流程，整合Slack、Linear和Git等平台的异步事件。


<details>
  <summary>Details</summary>
Motivation: 现有上下文和记忆基准测试主要关注对话场景，但企业动态环境中的记忆评估对于实际应用至关重要。

Method: 通过专家手动设计和基于代理的可扩展合成来构建数据集，创建基于真实软件开发过程的生态有效场景，并引入正确性、效率和冗余性等评估指标。

Result: 在SoTA LLM和记忆后端上的实验显示，在长时程记忆利用、跨平台依赖处理以及矛盾解决方面存在挑战，表现最佳的GPT-5模型在MEMTRACK上仅达到60%的正确性得分。

Conclusion: 该工作为记忆增强代理的评估研究提供了一个可扩展框架，超越了现有对话设置的局限，为复杂组织环境中的多代理、多平台记忆基准测试奠定了基础。

Abstract: Recent works on context and memory benchmarking have primarily focused on
conversational instances but the need for evaluating memory in dynamic
enterprise environments is crucial for its effective application. We introduce
MEMTRACK, a benchmark designed to evaluate long-term memory and state tracking
in multi-platform agent environments. MEMTRACK models realistic organizational
workflows by integrating asynchronous events across multiple communication and
productivity platforms such as Slack, Linear and Git. Each benchmark instance
provides a chronologically platform-interleaved timeline, with noisy,
conflicting, cross-referring information as well as potential
codebase/file-system comprehension and exploration. Consequently, our benchmark
tests memory capabilities such as acquistion, selection and conflict
resolution. We curate the MEMTRACK dataset through both manual expert driven
design and scalable agent based synthesis, generating ecologically valid
scenarios grounded in real world software development processes. We introduce
pertinent metrics for Correctness, Efficiency, and Redundancy that capture the
effectiveness of memory mechanisms beyond simple QA performance. Experiments
across SoTA LLMs and memory backends reveal challenges in utilizing memory
across long horizons, handling cross-platform dependencies, and resolving
contradictions. Notably, the best performing GPT-5 model only achieves a 60\%
Correctness score on MEMTRACK. This work provides an extensible framework for
advancing evaluation research for memory-augmented agents, beyond existing
focus on conversational setups, and sets the stage for multi-agent,
multi-platform memory benchmarking in complex organizational settings

</details>


### [41] [Retrieval-Augmented Framework for LLM-Based Clinical Decision Support](https://arxiv.org/abs/2510.01363)
*Leon Garza,Anantaa Kotal,Michael A. Grasso,Emre Umucu*

Main category: cs.AI

TL;DR: 提出基于大语言模型的临床决策支持系统，通过分析电子健康记录数据生成治疗建议，采用检索增强生成技术整合非结构化叙述和结构化数据来辅助临床决策。


<details>
  <summary>Details</summary>
Motivation: 临床决策复杂性增加和电子健康记录快速扩展带来了数据驱动护理的机会与挑战，需要开发能够辅助处方医生决策的工具。

Method: 采用检索增强生成(RAG)管道，整合自然语言处理和结构化临床输入，通过分析患者人口统计、主诉、症状、诊断信息和治疗历史来生成治疗建议。

Result: 初步评估显示，在适当约束和严格验证下，基于大语言模型的工具可能在处方工作流程中提供有价值的决策支持。

Conclusion: 这是将生成式AI整合到现实世界临床决策中的初步步骤，强调透明度、安全性以及与既定实践的一致性。

Abstract: The increasing complexity of clinical decision-making, alongside the rapid
expansion of electronic health records (EHR), presents both opportunities and
challenges for delivering data-informed care. This paper proposes a clinical
decision support system powered by Large Language Models (LLMs) to assist
prescribing clinicians. The system generates therapeutic suggestions by
analyzing historical EHR data, including patient demographics, presenting
complaints, clinical symptoms, diagnostic information, and treatment histories.
The framework integrates natural language processing with structured clinical
inputs to produce contextually relevant recommendations. Rather than replacing
clinician judgment, it is designed to augment decision-making by retrieving and
synthesizing precedent cases with comparable characteristics, drawing on local
datasets or federated sources where applicable. At its core, the system employs
a retrieval-augmented generation (RAG) pipeline that harmonizes unstructured
narratives and codified data to support LLM-based inference. We outline the
system's technical components, including representation representation
alignment and generation strategies. Preliminary evaluations, conducted with
de-identified and synthetic clinical datasets, examine the clinical
plausibility and consistency of the model's outputs. Early findings suggest
that LLM-based tools may provide valuable decision support in prescribing
workflows when appropriately constrained and rigorously validated. This work
represents an initial step toward integration of generative AI into real-world
clinical decision-making with an emphasis on transparency, safety, and
alignment with established practices.

</details>


### [42] [Is It Thinking or Cheating? Detecting Implicit Reward Hacking by Measuring Reasoning Effort](https://arxiv.org/abs/2510.01367)
*Xinpeng Wang,Nitish Joshi,Barbara Plank,Rico Angell,He He*

Main category: cs.AI

TL;DR: 提出了TRACE方法，通过截断推理链来检测隐性奖励黑客行为，在数学推理和编程任务中显著优于现有监控方法


<details>
  <summary>Details</summary>
Motivation: 奖励黑客行为（模型利用奖励函数漏洞获得高分但不解决实际任务）是一个严重威胁，特别是隐性黑客行为会绕过现有的推理链监控

Method: TRACE方法：逐步截断模型的推理链，强制模型回答，测量验证器通过率。黑客模型只需少量推理就能获得高分，表现为准确率-长度曲线下面积较大

Result: 在数学推理任务中比72B推理链监控提升65%以上，在编程任务中比32B监控提升30%以上，还能发现训练中的未知漏洞

Conclusion: TRACE提供了一种可扩展的无监督监督方法，在当前监控方法失效的情况下有效检测隐性奖励黑客行为

Abstract: Reward hacking, where a reasoning model exploits loopholes in a reward
function to achieve high rewards without solving the intended task, poses a
significant threat. This behavior may be explicit, i.e. verbalized in the
model's chain-of-thought (CoT), or implicit, where the CoT appears benign thus
bypasses CoT monitors. To detect implicit reward hacking, we propose TRACE
(Truncated Reasoning AUC Evaluation). Our key observation is that hacking
occurs when exploiting the loophole is easier than solving the actual task.
This means that the model is using less `effort' than required to achieve high
reward. TRACE quantifies effort by measuring how early a model's reasoning
becomes sufficient to pass a verifier. We progressively truncate a model's CoT
at various lengths, force the model to answer, and measure the verifier-passing
rate at each cutoff. A hacking model, which takes a shortcut, will achieve a
high passing rate with only a small fraction of its CoT, yielding a large area
under the accuracy-vs-length curve. TRACE achieves over 65% gains over our
strongest 72B CoT monitor in math reasoning, and over 30% gains over a 32B
monitor in coding. We further show that TRACE can discover unknown loopholes
during training. Overall, TRACE offers a scalable unsupervised approach for
oversight where current monitoring methods prove ineffective.

</details>


### [43] [Fine-tuning with RAG for Improving LLM Learning of New Skills](https://arxiv.org/abs/2510.01375)
*Humaid Ibrahim,Nikolai Rozanov,Marek Rei*

Main category: cs.AI

TL;DR: 提出通过蒸馏将推理时检索转化为学习能力的方法，在交互式基准测试中显著提升智能体性能，同时减少token使用量


<details>
  <summary>Details</summary>
Motivation: LLM智能体在多步任务中经常出现可预测的失败，如尝试不满足前提条件的动作、发出冗余命令或错误处理环境约束。虽然检索增强生成(RAG)可以提供运行时指导，但需要维护外部知识库并增加计算开销

Method: 三步管道：(1)从智能体失败中提取紧凑可重用的提示；(2)使用这些提示在每集开始时通过一次性检索生成改进的教师轨迹；(3)在移除提示字符串的情况下训练学生模型，强制内化而非记忆

Result: 在ALFWorld(家庭任务)和WebShop(在线购物)两个交互基准测试中，蒸馏学生模型始终优于基线智能体：ALFWorld成功率91%(基线79%)，WebShop得分72(基线61)，同时token使用量比检索增强教师减少10-60%

Conclusion: 该方法在不同模型规模(7B/14B参数)和智能体架构(ReAct/StateAct)上具有通用性，证明检索优势可以通过针对性微调有效内化，无需永久运行时依赖

Abstract: Large language model (LLM) agents deployed for multi-step tasks frequently
fail in predictable ways: attempting actions with unmet preconditions, issuing
redundant commands, or mishandling environment constraints. While
retrieval-augmented generation (RAG) can improve performance by providing
runtime guidance, it requires maintaining external knowledge databases and adds
computational overhead at every deployment. We propose a simple pipeline that
converts inference-time retrieval into learned competence through distillation.
Our approach: (1) extracts compact, reusable hints from agent failures, (2)
uses these hints to generate improved teacher trajectories via one-shot
retrieval at episode start, and (3) trains student models on these trajectories
with hint strings removed, forcing internalization rather than memorization.
Across two interactive benchmarks, ALFWorld (household tasks) and WebShop
(online shopping), distilled students consistently outperform baseline agents,
achieving up to 91% success on ALFWorld (vs. 79% for baselines) and improving
WebShop scores to 72 (vs. 61 for baselines), while using 10-60% fewer tokens
than retrieval-augmented teachers depending on the environment. The approach
generalizes across model scales (7B/14B parameters) and agent architectures
(ReAct/StateAct), demonstrating that retrieval benefits can be effectively
internalized through targeted fine-tuning without permanent runtime
dependencies.

</details>


### [44] [Automating Data-Driven Modeling and Analysis for Engineering Applications using Large Language Model Agents](https://arxiv.org/abs/2510.01398)
*Yang Liu,Zaid Abulawi,Abhiram Garimidi,Doyeong Lim*

Main category: cs.AI

TL;DR: 提出使用大型语言模型(LLM)代理自动化数据驱动建模和分析的创新流程，特别关注回归任务，在关键热通量预测基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现代工程依赖大量实验和模拟数据集，传统数据驱动方法需要大量人工干预，难以有效扩展和泛化到多样化应用。

Method: 评估两种LLM代理框架：具有专门协作代理的多代理系统，以及基于推理与行动(ReAct)范式的单代理系统，两者都能自主处理数据预处理、神经网络开发、训练、超参数优化和不确定性量化。

Result: 在约25,000个实验数据点的关键热通量预测基准测试中，LLM代理开发的模型超越了传统查找表，预测精度和不确定性量化与人类专家开发的贝叶斯优化深度神经网络模型相当。

Conclusion: LLM代理在自动化复杂工程建模任务方面具有巨大潜力，能显著减少人工工作量，同时达到或超越现有预测性能标准。

Abstract: Modern engineering increasingly relies on vast datasets generated by
experiments and simulations, driving a growing demand for efficient, reliable,
and broadly applicable modeling strategies. There is also heightened interest
in developing data-driven approaches, particularly neural network models, for
effective prediction and analysis of scientific datasets. Traditional
data-driven methods frequently involve extensive manual intervention, limiting
their ability to scale effectively and generalize to diverse applications. In
this study, we propose an innovative pipeline utilizing Large Language Model
(LLM) agents to automate data-driven modeling and analysis, with a particular
emphasis on regression tasks. We evaluate two LLM-agent frameworks: a
multi-agent system featuring specialized collaborative agents, and a
single-agent system based on the Reasoning and Acting (ReAct) paradigm. Both
frameworks autonomously handle data preprocessing, neural network development,
training, hyperparameter optimization, and uncertainty quantification (UQ). We
validate our approach using a critical heat flux (CHF) prediction benchmark,
involving approximately 25,000 experimental data points from the OECD/NEA
benchmark dataset. Results indicate that our LLM-agent-developed model
surpasses traditional CHF lookup tables and delivers predictive accuracy and UQ
on par with state-of-the-art Bayesian optimized deep neural network models
developed by human experts. These outcomes underscore the significant potential
of LLM-based agents to automate complex engineering modeling tasks, greatly
reducing human workload while meeting or exceeding existing standards of
predictive performance.

</details>


### [45] [OntoLogX: Ontology-Guided Knowledge Graph Extraction from Cybersecurity Logs with Large Language Models](https://arxiv.org/abs/2510.01409)
*Luca Cotti,Idilio Drago,Anisa Rula,Devis Bianchini,Federico Cerutti*

Main category: cs.AI

TL;DR: OntoLogX是一个自主AI代理，利用大语言模型将原始日志转换为基于本体的知识图谱，通过检索增强生成和迭代校正确保图谱有效性，并能将日志证据映射到MITRE ATT&CK战术。


<details>
  <summary>Details</summary>
Motivation: 系统日志是宝贵的网络威胁情报来源，但由于缺乏结构、语义不一致和跨设备碎片化，其效用受到限制。需要能够将噪声异构数据协调为连贯可互操作表示的方法。

Method: 集成轻量级日志本体与检索增强生成和迭代校正步骤，确保生成的知识图谱在语法和语义上有效。系统将知识图谱聚合到会话中，并使用LLM预测MITRE ATT&CK战术。

Result: 在公共基准和真实世界蜜罐数据集上的评估显示，OntoLogX能够跨多个知识图谱后端稳健生成图谱，并准确将对抗活动映射到ATT&CK战术。检索和校正提高了精确率和召回率。

Conclusion: 代码导向模型在结构化日志分析中有效，基于本体的表示为可操作的网络威胁情报提取提供了价值。

Abstract: System logs represent a valuable source of Cyber Threat Intelligence (CTI),
capturing attacker behaviors, exploited vulnerabilities, and traces of
malicious activity. Yet their utility is often limited by lack of structure,
semantic inconsistency, and fragmentation across devices and sessions.
Extracting actionable CTI from logs therefore requires approaches that can
reconcile noisy, heterogeneous data into coherent and interoperable
representations. We introduce OntoLogX, an autonomous Artificial Intelligence
(AI) agent that leverages Large Language Models (LLMs) to transform raw logs
into ontology-grounded Knowledge Graphs (KGs). OntoLogX integrates a
lightweight log ontology with Retrieval Augmented Generation (RAG) and
iterative correction steps, ensuring that generated KGs are syntactically and
semantically valid. Beyond event-level analysis, the system aggregates KGs into
sessions and employs a LLM to predict MITRE ATT&CK tactics, linking low-level
log evidence to higher-level adversarial objectives. We evaluate OntoLogX on
both logs from a public benchmark and a real-world honeypot dataset,
demonstrating robust KG generation across multiple KGs backends and accurate
mapping of adversarial activity to ATT&CK tactics. Results highlight the
benefits of retrieval and correction for precision and recall, the
effectiveness of code-oriented models in structured log analysis, and the value
of ontology-grounded representations for actionable CTI extraction.

</details>


### [46] [A Tale of LLMs and Induced Small Proxies: Scalable Agents for Knowledge Mining](https://arxiv.org/abs/2510.01427)
*Sipeng Zhang,Longfei Yun,Zilong Wang,Jingbo Shang,Letian Peng*

Main category: cs.AI

TL;DR: Falconer是一个结合LLM智能推理和轻量级代理模型的协作框架，用于可扩展的知识挖掘，在保持高准确性的同时大幅降低推理成本。


<details>
  <summary>Details</summary>
Motivation: 解决大规模知识挖掘中LLM部署成本过高与传统分类器-抽取器管道脆弱且无法泛化到新任务的问题。

Method: 使用LLM作为规划器分解用户指令为可执行管道，并作为标注器生成监督数据训练小型代理模型，统一分类和抽取为两个原子操作。

Result: Falconer在指令遵循准确性上接近最先进LLM，同时减少推理成本达90%，加速大规模知识挖掘超过20倍。

Conclusion: Falconer为深度研究提供了一个高效且可扩展的基础框架，平衡了性能与成本。

Abstract: At the core of Deep Research is knowledge mining, the task of extracting
structured information from massive unstructured text in response to user
instructions. Large language models (LLMs) excel at interpreting such
instructions but are prohibitively expensive to deploy at scale, while
traditional pipelines of classifiers and extractors remain efficient yet
brittle and unable to generalize to new tasks. We introduce Falconer, a
collaborative framework that combines the agentic reasoning of LLMs with
lightweight proxy models for scalable knowledge mining. In Falconer, LLMs act
as planners, decomposing user instructions into executable pipelines, and as
annotators, generating supervision to train small proxies. The framework
unifies classification and extraction into two atomic operations, get label and
get span, enabling a single instruction-following model to replace multiple
task-specific components. To evaluate the consistency between proxy models
incubated by Falconer and annotations provided by humans and large models, we
construct new benchmarks covering both planning and end-to-end execution.
Experiments show that Falconer closely matches state-of-the-art LLMs in
instruction-following accuracy while reducing inference cost by up to 90% and
accelerating large-scale knowledge mining by more than 20x, offering an
efficient and scalable foundation for Deep Research.

</details>


### [47] [On the Role of Domain Experts in Creating Effective Tutoring Systems](https://arxiv.org/abs/2510.01432)
*Sarath Sreedharan,Kelsey Sikes,Nathaniel Blanchard,Lisa Mason,Nikhil Krishnaswamy,Jill Zarestky*

Main category: cs.AI

TL;DR: 本文探讨了领域专家提供的精炼知识在创建有效教学系统中的作用，提出了两种利用专家知识开发新型教育系统的方法：使用可解释AI技术自动生成课程，以及利用专家指定的课程体系开发自适应教学系统。


<details>
  <summary>Details</summary>
Motivation: AI教育社区往往忽视了领域专家提供的精炼知识在创建有效教学系统中的作用，本文旨在强调这一被忽视的话题。

Method: 1. 使用可解释AI技术结合专家指定的问题解决规则来自动生成课程；2. 利用专家指定的学习课程体系来开发自适应教学系统。

Result: 通过传粉者识别教学系统的案例研究，证明了这些方法的可行性和重要性。

Conclusion: 领域专家的精炼知识在开发有效教学系统中具有重要作用，能够帮助创建更好的学习体验和更高效的算法。

Abstract: The role that highly curated knowledge, provided by domain experts, could
play in creating effective tutoring systems is often overlooked within the AI
for education community. In this paper, we highlight this topic by discussing
two ways such highly curated expert knowledge could help in creating novel
educational systems. First, we will look at how one could use explainable AI
(XAI) techniques to automatically create lessons. Most existing XAI methods are
primarily aimed at debugging AI systems. However, we will discuss how one could
use expert specified rules about solving specific problems along with novel XAI
techniques to automatically generate lessons that could be provided to
learners. Secondly, we will see how an expert specified curriculum for learning
a target concept can help develop adaptive tutoring systems, that can not only
provide a better learning experience, but could also allow us to use more
efficient algorithms to create these systems. Finally, we will highlight the
importance of such methods using a case study of creating a tutoring system for
pollinator identification, where such knowledge could easily be elicited from
experts.

</details>


### [48] [VOGUE: Guiding Exploration with Visual Uncertainty Improves Multimodal Reasoning](https://arxiv.org/abs/2510.01444)
*Rui Liu,Dian Yu,Tong Zheng,Runpeng Dai,Zongxia Li,Wenhao Yu,Zhenwen Liang,Linfeng Song,Haitao Mi,Pratap Tokekar,Dong Yu*

Main category: cs.AI

TL;DR: VOGUE是一种新颖的强化学习方法，通过将视觉输入视为随机上下文，量化策略对视觉扰动的敏感性，从而在视觉空间中引导探索，有效提升多模态大语言模型的推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有的强化学习方法在处理多模态大语言模型时，将视觉输入视为固定确定性条件，忽略了视觉变化带来的模糊性，导致策略对合理视觉变化缺乏鲁棒性。

Method: VOGUE方法将探索从输出（文本）空间转移到输入（视觉）空间，通过对称KL散度量化策略对视觉扰动的敏感性，结合不确定性比例奖励、token熵奖励和退火采样调度来平衡探索与利用。

Result: 在Qwen2.5-VL-3B/7B模型上，VOGUE在三个视觉数学基准上平均提升pass@1准确率2.6%，在三个通用领域推理基准上提升3.7%，同时提高了pass@4性能并缓解了RL微调中常见的探索衰减问题。

Conclusion: 基于视觉输入固有不确定性的探索策略是提升多模态推理的有效方法，VOGUE通过视觉不确定性引导探索，显著改善了多模态大语言模型的推理性能。

Abstract: Reinforcement learning with verifiable rewards (RLVR) improves reasoning in
large language models (LLMs) but struggles with exploration, an issue that
still persists for multimodal LLMs (MLLMs). Current methods treat the visual
input as a fixed, deterministic condition, overlooking a critical source of
ambiguity and struggling to build policies robust to plausible visual
variations. We introduce $\textbf{VOGUE (Visual Uncertainty Guided
Exploration)}$, a novel method that shifts exploration from the output (text)
to the input (visual) space. By treating the image as a stochastic context,
VOGUE quantifies the policy's sensitivity to visual perturbations using the
symmetric KL divergence between a "raw" and "noisy" branch, creating a direct
signal for uncertainty-aware exploration. This signal shapes the learning
objective via an uncertainty-proportional bonus, which, combined with a
token-entropy bonus and an annealed sampling schedule, effectively balances
exploration and exploitation. Implemented within GRPO on two model scales
(Qwen2.5-VL-3B/7B), VOGUE boosts pass@1 accuracy by an average of 2.6% on three
visual math benchmarks and 3.7% on three general-domain reasoning benchmarks,
while simultaneously increasing pass@4 performance and mitigating the
exploration decay commonly observed in RL fine-tuning. Our work shows that
grounding exploration in the inherent uncertainty of visual inputs is an
effective strategy for improving multimodal reasoning.

</details>


### [49] [AIReg-Bench: Benchmarking Language Models That Assess AI Regulation Compliance](https://arxiv.org/abs/2510.01474)
*Bill Marino,Rosco Hunter,Zubair Jamali,Marinos Emmanouil Kalpakos,Mudra Kashyap,Isaiah Hinton,Alexa Hanson,Maahum Nazir,Christoph Schnabl,Felix Steffek,Hongkai Wen,Nicholas D. Lane*

Main category: cs.AI

TL;DR: 提出了AIReg-Bench基准数据集，用于评估大型语言模型在判断AI系统是否符合欧盟AI法案方面的能力。


<details>
  <summary>Details</summary>
Motivation: 随着政府对AI的监管加强，需要评估LLM在AI法规合规性判断方面的性能，但目前缺乏相关基准。

Method: 通过两步流程创建数据集：(1)使用结构化指令提示LLM生成120个技术文档片段；(2)法律专家审查并标注每个样本是否违反AI法案的具体条款。

Result: 创建了首个针对欧盟AI法案合规性评估的基准数据集，并对前沿LLM进行了初步评估。

Conclusion: 该基准为理解LLM在AI法规合规评估中的机会和局限性提供了起点，并为后续LLM比较建立了基准。

Abstract: As governments move to regulate AI, there is growing interest in using Large
Language Models (LLMs) to assess whether or not an AI system complies with a
given AI Regulation (AIR). However, there is presently no way to benchmark the
performance of LLMs at this task. To fill this void, we introduce AIReg-Bench:
the first benchmark dataset designed to test how well LLMs can assess
compliance with the EU AI Act (AIA). We created this dataset through a two-step
process: (1) by prompting an LLM with carefully structured instructions, we
generated 120 technical documentation excerpts (samples), each depicting a
fictional, albeit plausible, AI system - of the kind an AI provider might
produce to demonstrate their compliance with AIR; (2) legal experts then
reviewed and annotated each sample to indicate whether, and in what way, the AI
system described therein violates specific Articles of the AIA. The resulting
dataset, together with our evaluation of whether frontier LLMs can reproduce
the experts' compliance labels, provides a starting point to understand the
opportunities and limitations of LLM-based AIR compliance assessment tools and
establishes a benchmark against which subsequent LLMs can be compared. The
dataset and evaluation code are available at
https://github.com/camlsys/aireg-bench.

</details>


### [50] [Lateral Tree-of-Thoughts Surpasses ToT by Incorporating Logically-Consistent, Low-Utility Candidates](https://arxiv.org/abs/2510.01500)
*Abhinav Madahar*

Main category: cs.AI

TL;DR: LToT是一种改进的Tree-of-Thoughts搜索控制器，通过分离效用和逻辑一致性，将低效用但一致的候选视为资产而非浪费，解决了广度饱和和深度短视问题。


<details>
  <summary>Details</summary>
Motivation: 现代部署中大量测试时计算存在两个问题：广度饱和（额外样本产生近重复）和深度短视（噪声短视效用剪枝有长期回报的分支）。

Method: LToT将前沿分为主线（高效用候选用于开发）和侧线（一致但初始低效用候选，在判断前接受短而便宜的探测），通过带短路机制的侧向竞赛进行探索。

Result: 理论证明侧向成本为伪线性Θ(N₀ log_η N₀)，与未限制主线的指数增长形成对比。

Conclusion: LToT将大测试时预算转化为原则性多样性，同时保持提升纪律，缓解饱和和短视问题而不增加计算成本。

Abstract: Modern deployments increasingly allocate large test-time compute (thousands
of tokens or many node expansions) to boost reliability. Under such budgets,
standard Tree-of-Thoughts-style search exhibits two pathologies: breadth
saturation (additional samples mostly produce near-duplicates, so width stops
growing) and depth myopia (noisy short-horizon utilities prune branches whose
payoff appears after a few more steps). We propose Lateral Tree-of-Thoughts
(LToT), a drop-in controller that separates utility from logical consistency
and treats low-utility but consistent candidates as assets rather than waste.
The frontier is split into mainlines (high-utility candidates used for
exploitation) and laterals (consistent, initially low-utility candidates that
receive short, cheap probes before judgment). LToT explores laterals via
Lateral Racing with Short-Circuit (LR--SC): a capped successive-halving race
that spreads tiny probes across a very wide lateral set, uses width-aware
thresholds with repeat-to-confirm, and immediately promotes a branch once its
envelope clears the mainline bar; mainlines are kept intentionally narrow so
surplus compute is invested where width is cheap. We prove a pseudolinear
lateral cost $\Theta(N_0 \log_{\eta} N_0)$ with logarithmically many rungs
(initial lateral width $N_0$; culling factor $\eta>1$), in contrast to the
exponential growth of uncapped mainlines. Empirical evaluations on benchmark
tasks are in preparation and will be added in a future revision. In short, LToT
turns large test-time budgets into principled diversity while preserving
promotion discipline, mitigating saturation and myopia without inflating
compute.

</details>


### [51] [Towards Interpretable and Inference-Optimal COT Reasoning with Sparse Autoencoder-Guided Generation](https://arxiv.org/abs/2510.01528)
*Daniel Zhao,Abhilash Shankarampeta,Lanxiang Hu,Tajana Rosing,Hao Zhang*

Main category: cs.AI

TL;DR: 提出基于稀疏自编码器和聚类技术的方法，分析LLM内部token表示并指导数学推理任务的生成，通过构建token转移图来平衡利用和探索。


<details>
  <summary>Details</summary>
Motivation: 为了分析大型语言模型在数学推理任务中的内部表示，并指导生成过程实现利用和探索的平衡，避免极端行为。

Method: 训练稀疏自编码器生成稀疏向量表示，应用k-means聚类构建token转移图，基于边权重定义奖励函数量化推理轨迹的遵循程度。

Result: 发现平衡利用和探索对数学推理任务的高准确性至关重要，稀疏自编码器可作为可扩展的奖励模型指导生成。

Conclusion: 该方法能有效平衡利用和探索，防止极端行为，促进LLM中更高质量的推理过程。

Abstract: We propose a novel method that leverages sparse autoencoders (SAEs) and
clustering techniques to analyze the internal token representations of large
language models (LLMs) and guide generations in mathematical reasoning tasks.
Our approach first trains an SAE to generate sparse vector representations for
training tokens, then applies k-means clustering to construct a graph where
vertices represent token clusters and weighted edges capture sequential token
transitions. Using this graph, we define an edge-weight based reward function
to quantify adherence to established reasoning traces, thereby identifying
exploitative reasoning trajectories. Additionally, we measure generation
diversity from clustering to assess the extent of exploration. Our findings
indicate that balancing both exploitation and exploration is crucial for
achieving high accuracy in mathematical reasoning tasks. During generation, the
SAE can serve as a scalable reward model to guide generations, ensuring a
balanced trade-off between exploitation and exploration. This prevents extreme
behaviors in either direction, ultimately fostering a higher-quality reasoning
process in LLMs.

</details>


### [52] [LOGicalThought: Logic-Based Ontological Grounding of LLMs for High-Assurance Reasoning](https://arxiv.org/abs/2510.01530)
*Navapat Nananukul,Yue Zhang,Ryan Lee,Eric Boxer,Jonathan May,Vibhav Giridhar Gogate,Jay Pujara,Mayank Kejriwal*

Main category: cs.AI

TL;DR: 提出LOGicalThought (LogT)神经符号架构，结合逻辑语言推理器和LLM，解决高保证推理中的可废止逻辑挑战，在四个多领域基准测试中性能提升11.84%。


<details>
  <summary>Details</summary>
Motivation: 高保证领域（如法律、医疗）需要准确、可验证且基于证据的推理，但LLM在标准推理任务中的能力无法满足对可废止逻辑的严格推理需求。

Method: 使用高级逻辑语言和推理器与LLM结合，构建双重符号图上下文和基于逻辑的上下文表示，将长文本指南推理转化为紧凑的接地评估。

Result: 在四个基准测试中，LogT相比最强基线总体性能提升11.84%，在否定推理上提升10.2%，蕴含推理提升13.2%，可废止推理提升5.5%。

Conclusion: LogT神经符号架构能有效解决高保证推理中的逻辑挑战，特别是在处理否定、蕴含和可废止规则方面表现显著。

Abstract: High-assurance reasoning, particularly in critical domains such as law and
medicine, requires conclusions that are accurate, verifiable, and explicitly
grounded in evidence. This reasoning relies on premises codified from rules,
statutes, and contracts, inherently involving defeasible or non-monotonic logic
due to numerous exceptions, where the introduction of a single fact can
invalidate general rules, posing significant challenges. While large language
models (LLMs) excel at processing natural language, their capabilities in
standard inference tasks do not translate to the rigorous reasoning required
over high-assurance text guidelines. Core reasoning challenges within such
texts often manifest specific logical structures involving negation,
implication, and, most critically, defeasible rules and exceptions. In this
paper, we propose a novel neurosymbolically-grounded architecture called
LOGicalThought (LogT) that uses an advanced logical language and reasoner in
conjunction with an LLM to construct a dual symbolic graph context and
logic-based context. These two context representations transform the problem
from inference over long-form guidelines into a compact grounded evaluation.
Evaluated on four multi-domain benchmarks against four baselines, LogT improves
overall performance by 11.84% across all LLMs. Performance improves
significantly across all three modes of reasoning: by up to +10.2% on negation,
+13.2% on implication, and +5.5% on defeasible reasoning compared to the
strongest baseline.

</details>


### [53] [Just Do It!? Computer-Use Agents Exhibit Blind Goal-Directedness](https://arxiv.org/abs/2510.01670)
*Erfan Shayegani,Keegan Hines,Yue Dong,Nael Abu-Ghazaleh,Roman Lutz,Spencer Whitehead,Vidhisha Balachandran,Besmira Nushi,Vibhav Vineet*

Main category: cs.AI

TL;DR: 论文揭示了计算机使用代理(CUAs)存在的盲目目标导向(BGD)风险，即代理会不顾可行性、安全性和上下文而盲目追求目标，并开发了BLIND-ACT基准来评估这种风险。


<details>
  <summary>Details</summary>
Motivation: 计算机使用代理(CUAs)在GUI上执行操作以完成用户目标，但存在盲目追求目标的风险，这可能带来安全性和可靠性问题。

Method: 开发了BLIND-ACT基准，包含90个任务，基于OSWorld构建真实环境，使用LLM评估代理行为，评估了9个前沿模型。

Result: 在评估的模型中观察到高平均BGD率(80.8%)，提示干预可降低BGD水平但风险仍然存在，识别出执行优先偏见、思维-行动脱节和请求优先等失败模式。

Conclusion: 识别BGD并引入BLIND-ACT为未来研究和缓解这种基本风险奠定了基础，确保CUA的安全部署需要更强的训练或推理时干预。

Abstract: Computer-Use Agents (CUAs) are an increasingly deployed class of agents that
take actions on GUIs to accomplish user goals. In this paper, we show that CUAs
consistently exhibit Blind Goal-Directedness (BGD): a bias to pursue goals
regardless of feasibility, safety, reliability, or context. We characterize
three prevalent patterns of BGD: (i) lack of contextual reasoning, (ii)
assumptions and decisions under ambiguity, and (iii) contradictory or
infeasible goals. We develop BLIND-ACT, a benchmark of 90 tasks capturing these
three patterns. Built on OSWorld, BLIND-ACT provides realistic environments and
employs LLM-based judges to evaluate agent behavior, achieving 93.75% agreement
with human annotations. We use BLIND-ACT to evaluate nine frontier models,
including Claude Sonnet and Opus 4, Computer-Use-Preview, and GPT-5, observing
high average BGD rates (80.8%) across them. We show that BGD exposes subtle
risks that arise even when inputs are not directly harmful. While
prompting-based interventions lower BGD levels, substantial risk persists,
highlighting the need for stronger training- or inference-time interventions.
Qualitative analysis reveals observed failure modes: execution-first bias
(focusing on how to act over whether to act), thought-action disconnect
(execution diverging from reasoning), and request-primacy (justifying actions
due to user request). Identifying BGD and introducing BLIND-ACT establishes a
foundation for future research on studying and mitigating this fundamental risk
and ensuring safe CUA deployment.

</details>


### [54] [Information Seeking for Robust Decision Making under Partial Observability](https://arxiv.org/abs/2510.01531)
*Djengo Cyun-Jyun Fang,Tsung-Wei Ke*

Main category: cs.AI

TL;DR: InfoSeeker是一个LLM决策框架，通过主动信息寻求来在部分可观测环境中优化决策，相比现有方法实现了74%的性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有LLM规划智能体在处理观测不确定性时，往往忽略了其内部动态与实际环境之间的差异，导致在信息不完整和动态噪声环境中的决策效果不佳。

Method: 提出InfoSeeker框架，通过提示LLM主动收集信息来验证理解、检测环境变化或测试假设，然后将信息寻求与任务导向规划紧密结合。

Result: 在部分可观测环境的新基准测试中，InfoSeeker相比先前方法实现了74%的绝对性能提升，且不牺牲样本效率，在机器人操作和网络导航等基准测试中也优于基线方法。

Conclusion: 信息寻求与规划的紧密集成对于在部分可观测环境中实现鲁棒行为至关重要，InfoSeeker框架展示了这种集成方法的有效性。

Abstract: Explicit information seeking is essential to human problem-solving in
practical environments characterized by incomplete information and noisy
dynamics. When the true environmental state is not directly observable, humans
seek information to update their internal dynamics and inform future
decision-making. Although existing Large Language Model (LLM) planning agents
have addressed observational uncertainty, they often overlook discrepancies
between their internal dynamics and the actual environment. We introduce
Information Seeking Decision Planner (InfoSeeker), an LLM decision-making
framework that integrates task-oriented planning with information seeking to
align internal dynamics and make optimal decisions under uncertainty in both
agent observations and environmental dynamics. InfoSeeker prompts an LLM to
actively gather information by planning actions to validate its understanding,
detect environmental changes, or test hypotheses before generating or revising
task-oriented plans. To evaluate InfoSeeker, we introduce a novel benchmark
suite featuring partially observable environments with incomplete observations
and uncertain dynamics. Experiments demonstrate that InfoSeeker achieves a 74%
absolute performance gain over prior methods without sacrificing sample
efficiency. Moreover, InfoSeeker generalizes across LLMs and outperforms
baselines on established benchmarks such as robotic manipulation and web
navigation. These findings underscore the importance of tightly integrating
planning and information seeking for robust behavior in partially observable
environments. The project page is available at https://infoseekerllm.github.io

</details>


### [55] [Step-Aware Policy Optimization for Reasoning in Diffusion Large Language Models](https://arxiv.org/abs/2510.01544)
*Shaoan Xie,Lingjing Kong,Xiangchen Song,Xinshuai Dong,Guangyi Chen,Eric P. Xing,Kun Zhang*

Main category: cs.AI

TL;DR: 提出SAPO算法解决扩散语言模型在复杂推理训练中的问题，通过过程奖励函数引导模型学习结构化推理路径


<details>
  <summary>Details</summary>
Motivation: 现有强化学习方法依赖稀疏的结果奖励，会强化导致偶然正确结果的错误推理路径，这与推理的自然结构不匹配

Method: 提出理论框架将复杂问题解决形式化为层次选择过程，并开发SAPO算法，使用过程奖励函数使去噪过程与潜在推理层次对齐

Result: 在挑战性推理基准上显著提升性能，增强生成过程的可解释性

Conclusion: 基于理论的方法能有效改进扩散语言模型的复杂推理能力

Abstract: Diffusion language models (dLLMs) offer a promising, non-autoregressive
paradigm for text generation, yet training them for complex reasoning remains a
key challenge. Current reinforcement learning approaches often rely on sparse,
outcome-based rewards, which can reinforce flawed reasoning paths that lead to
coincidentally correct answers. We argue that this stems from a fundamental
mismatch with the natural structure of reasoning. We first propose a
theoretical framework that formalizes complex problem solving as a hierarchical
selection process, where an intractable global constraint is decomposed into a
series of simpler, localized logical steps. This framework provides a
principled foundation for algorithm design, including theoretical insights into
the identifiability of this latent reasoning structure. Motivated by this
theory, we identify unstructured refinement -- a failure mode where a model's
iterative steps do not contribute meaningfully to the solution -- as a core
deficiency in existing methods. We then introduce Step-Aware Policy
Optimization (SAPO), a novel RL algorithm that aligns the dLLM's denoising
process with the latent reasoning hierarchy. By using a process-based reward
function that encourages incremental progress, SAPO guides the model to learn
structured, coherent reasoning paths. Our empirical results show that this
principled approach significantly improves performance on challenging reasoning
benchmarks and enhances the interpretability of the generation process.

</details>


### [56] [InvThink: Towards AI Safety via Inverse Reasoning](https://arxiv.org/abs/2510.01569)
*Yubin Kim,Taehan Kim,Eugene Park,Chunjong Park,Cynthia Breazeal,Daniel McDuff,Hae Won Park*

Main category: cs.AI

TL;DR: InvThink是一种通过逆向思维让大语言模型在生成回复前先推理失败模式的安全对齐方法，相比现有方法在安全性提升、能力保持和高风险场景表现更优。


<details>
  <summary>Details</summary>
Motivation: 现有安全对齐方法直接优化安全回复，但缺乏对潜在风险的系统性考虑。InvThink旨在通过让模型在生成前先识别和分析潜在危害，实现更主动的安全防护。

Method: 1) 枚举潜在危害 2) 分析后果 3) 生成主动避免这些风险的安全输出。通过监督微调和强化学习在三个LLM家族上实现。

Result: 安全性提升随模型规模扩大而增强；缓解安全税，保持标准基准上的通用推理能力；在高风险领域（医疗、金融、法律等）相比SafetyPrompt等方法减少15.7%有害回复。

Conclusion: 逆向推理为构建更安全、更强大的语言模型提供了可扩展和泛化的路径。

Abstract: We present InvThink, a simple yet powerful approach that gives large language
models (LLMs) the capability of inverse thinking: reasoning through failure
modes before generating responses. Unlike existing safety alignment methods
that optimize directly for safe response, InvThink instructs models to 1)
enumerate potential harms, 2) analyze their consequences, and 3) generate safe
outputs that proactively avoid these risks. Our method reveals three key
findings: (i) safety improvements show stronger scaling with model size
compared to existing safety methods. (ii) InvThink mitigates safety tax; by
training models to systematically consider failure modes, it preserves general
reasoning capabilities on standard benchmarks. (iii) beyond general safety
tasks, InvThink excels in high-stakes domains including external-facing
(medicine, finance, law) and agentic (blackmail, murder) risk scenarios,
achieving up to 15.7% reduction in harmful responses compared to baseline
methods like SafetyPrompt. We further implement InvThink via supervised
fine-tuning, and reinforcement learning across three LLM families. These
results suggest that inverse reasoning provides a scalable and generalizable
path toward safer, more capable language models.

</details>


### [57] [UpSafe$^\circ$C: Upcycling for Controllable Safety in Large Language Models](https://arxiv.org/abs/2510.02194)
*Yuhao Sun,Zhuoer Xu,Shiwen Cui,Kun Yang,Lingyun Yu,Yongdong Zhang,Hongtao Xie*

Main category: cs.AI

TL;DR: UpSafe°C是一个通过安全感知升级增强LLM安全性的统一框架，通过识别安全关键层并将其升级为稀疏MoE结构，结合两阶段SFT策略和安全温度机制，实现动态安全控制。


<details>
  <summary>Details</summary>
Motivation: 现有安全技术（外部护栏、推理时指导和后训练对齐）在平衡安全性、实用性和可控性方面存在局限性，需要一种更灵活的安全增强方法。

Method: 识别安全关键层并升级为稀疏MoE结构，其中路由器作为软护栏选择性激活原始MLP和新增安全专家；采用两阶段SFT策略加强安全判别能力；引入安全温度机制实现推理时动态控制。

Result: 在多个基准测试、基础模型和模型规模上，UpSafe°C对有害和越狱输入实现了稳健的安全改进，同时在通用任务上保持竞争力；安全温度机制实现了效用和安全性的帕累托最优边界。

Conclusion: 这项工作为LLM安全指明了新方向：从静态对齐转向动态、模块化和推理感知的控制。

Abstract: Large Language Models (LLMs) have achieved remarkable progress across a wide
range of tasks, but remain vulnerable to safety risks such as harmful content
generation and jailbreak attacks. Existing safety techniques -- including
external guardrails, inference-time guidance, and post-training alignment --
each face limitations in balancing safety, utility, and controllability. In
this work, we propose UpSafe$^\circ$C, a unified framework for enhancing LLM
safety through safety-aware upcycling. Our approach first identifies
safety-critical layers and upcycles them into a sparse Mixture-of-Experts (MoE)
structure, where the router acts as a soft guardrail that selectively activates
original MLPs and added safety experts. We further introduce a two-stage SFT
strategy to strengthen safety discrimination while preserving general
capabilities. To enable flexible control at inference time, we introduce a
safety temperature mechanism, allowing dynamic adjustment of the trade-off
between safety and utility. Experiments across multiple benchmarks, base model,
and model scales demonstrate that UpSafe$^\circ$C achieves robust safety
improvements against harmful and jailbreak inputs, while maintaining
competitive performance on general tasks. Moreover, analysis shows that safety
temperature provides fine-grained inference-time control that achieves the
Pareto-optimal frontier between utility and safety. Our results highlight a new
direction for LLM safety: moving from static alignment toward dynamic, modular,
and inference-aware control.

</details>


### [58] [AdvEvo-MARL: Shaping Internalized Safety through Adversarial Co-Evolution in Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2510.01586)
*Zhenyu Pan,Yiting Zhang,Zhuo Liu,Yolo Yunlong Tang,Zeliang Zhang,Haozheng Luo,Yuwei Han,Jianshu Zhang,Dennis Wu,Hong-Yu Chen,Haoran Lu,Haoyang Fang,Manling Li,Chenliang Xu,Philip S. Yu,Han Liu*

Main category: cs.AI

TL;DR: AdvEvo-MARL是一个协同进化的多智能体强化学习框架，通过内部化安全性到任务智能体中，解决LLM多智能体系统的安全漏洞问题，无需依赖外部防护模块。


<details>
  <summary>Details</summary>
Motivation: 现有的LLM多智能体系统防御方法存在两个问题：自我验证方法因单个智能体能力有限而效果不佳；外部防护模块会增加系统开销并创建单点故障。需要一种既能保证安全又不增加系统复杂度的解决方案。

Method: 采用协同进化的多智能体强化学习框架，联合优化攻击者（合成不断演变的越狱提示）和防御者（训练任务智能体既能完成任务又能抵抗攻击）。引入公共基线优势估计，同一功能组内的智能体共享组级平均回报基线，实现低方差更新和更强的组内协调。

Result: 在代表性攻击场景中，AdvEvo-MARL始终将攻击成功率保持在20%以下，而基线方法最高可达38.33%。同时保持甚至提高了任务准确性（在推理任务上最高提升3.67%）。

Conclusion: 安全性和实用性可以在不依赖额外防护智能体或增加系统开销的情况下共同提升，AdvEvo-MARL框架为LLM多智能体系统提供了有效的安全解决方案。

Abstract: LLM-based multi-agent systems excel at planning, tool use, and role
coordination, but their openness and interaction complexity also expose them to
jailbreak, prompt-injection, and adversarial collaboration. Existing defenses
fall into two lines: (i) self-verification that asks each agent to pre-filter
unsafe instructions before execution, and (ii) external guard modules that
police behaviors. The former often underperforms because a standalone agent
lacks sufficient capacity to detect cross-agent unsafe chains and
delegation-induced risks; the latter increases system overhead and creates a
single-point-of-failure-once compromised, system-wide safety collapses, and
adding more guards worsens cost and complexity. To solve these challenges, we
propose AdvEvo-MARL, a co-evolutionary multi-agent reinforcement learning
framework that internalizes safety into task agents. Rather than relying on
external guards, AdvEvo-MARL jointly optimizes attackers (which synthesize
evolving jailbreak prompts) and defenders (task agents trained to both
accomplish their duties and resist attacks) in adversarial learning
environments. To stabilize learning and foster cooperation, we introduce a
public baseline for advantage estimation: agents within the same functional
group share a group-level mean-return baseline, enabling lower-variance updates
and stronger intra-group coordination. Across representative attack scenarios,
AdvEvo-MARL consistently keeps attack-success rate (ASR) below 20%, whereas
baselines reach up to 38.33%, while preserving-and sometimes improving-task
accuracy (up to +3.67% on reasoning tasks). These results show that safety and
utility can be jointly improved without relying on extra guard agents or added
system overhead.

</details>


### [59] [AgentRec: Next-Generation LLM-Powered Multi-Agent Collaborative Recommendation with Adaptive Intelligence](https://arxiv.org/abs/2510.01609)
*Bo Ma,Hang Li,ZeHua Hu,XiaoFan Gui,LuYao Liu,Simon Lau*

Main category: cs.AI

TL;DR: AgentRec是一个基于LLM的多智能体协作推荐框架，通过分层智能体网络解决动态用户偏好、对话连贯性和多目标平衡问题。


<details>
  <summary>Details</summary>
Motivation: 现有交互式对话推荐系统在处理动态用户偏好、保持对话连贯性和平衡多个排序目标方面面临重大挑战。

Method: 采用分层智能体网络，包含对话理解、偏好建模、上下文感知和动态排序等专门LLM智能体，通过自适应权重机制协调，结合三层学习策略（快速响应、智能推理和深度协作）。

Result: 在三个真实数据集上的实验表明，AgentRec相比最先进基线方法，对话成功率提升2.8%，推荐准确率（NDCG@10）提升1.9%，对话效率提高3.2%，同时保持可比较的计算成本。

Conclusion: AgentRec通过智能体协作有效解决了现有对话推荐系统的局限性，在多个关键指标上实现了显著改进。

Abstract: Interactive conversational recommender systems have gained significant
attention for their ability to capture user preferences through natural
language interactions. However, existing approaches face substantial challenges
in handling dynamic user preferences, maintaining conversation coherence, and
balancing multiple ranking objectives simultaneously. This paper introduces
AgentRec, a next-generation LLM-powered multi-agent collaborative
recommendation framework that addresses these limitations through hierarchical
agent networks with adaptive intelligence. Our approach employs specialized
LLM-powered agents for conversation understanding, preference modeling, context
awareness, and dynamic ranking, coordinated through an adaptive weighting
mechanism that learns from interaction patterns. We propose a three-tier
learning strategy combining rapid response for simple queries, intelligent
reasoning for complex preferences, and deep collaboration for challenging
scenarios. Extensive experiments on three real-world datasets demonstrate that
AgentRec achieves consistent improvements over state-of-the-art baselines, with
2.8\% enhancement in conversation success rate, 1.9\% improvement in
recommendation accuracy (NDCG@10), and 3.2\% better conversation efficiency
while maintaining comparable computational costs through intelligent agent
coordination.

</details>


### [60] [PychoBench: Evaluating the Psychology Intelligence of Large Language Models](https://arxiv.org/abs/2510.01611)
*Min Zeng*

Main category: cs.AI

TL;DR: 本文评估大型语言模型在心理咨询领域的应用潜力，通过PsychoBench基准测试基于美国国家咨询师认证考试，发现前沿LLMs如GPT-4o、Llama3.3-70B等能通过考试标准，而较小模型则表现不佳。


<details>
  <summary>Details</summary>
Motivation: 探索LLMs在需要认知能力的应用如心理咨询中的潜力，验证LLMs是否具备担任心理咨询师所需的专业知识和资格。

Method: 开发PsychoBench基准测试，包含2,252个基于美国国家咨询师认证考试的单选问题，涵盖心理学各子领域，要求深度理解和广泛知识。

Result: 先进模型如GPT-4o、Llama3.3-70B和Gemma3-27B得分远超通过阈值（约70%），而较小开源模型如Qwen2.5-7B、Mistral-7B远低于阈值。

Conclusion: 只有前沿LLMs目前能满足心理咨询考试标准，这凸显了开发心理学导向LLMs的潜力和挑战。

Abstract: Large Language Models (LLMs) have demonstrated remarkable success across a
wide range of industries, primarily due to their impressive generative
abilities. Yet, their potential in applications requiring cognitive abilities,
such as psychological counseling, remains largely untapped. This paper
investigates the key question: Can LLMs be effectively applied to psychological
counseling? To determine whether an LLM can effectively take on the role of a
psychological counselor, the first step is to assess whether it meets the
qualifications required for such a role, namely the ability to pass the U.S.
National Counselor Certification Exam (NCE). This is because, just as a human
counselor must pass a certification exam to practice, an LLM must demonstrate
sufficient psychological knowledge to meet the standards required for such a
role. To address this, we introduce PsychoBench, a benchmark grounded in
U.S.national counselor examinations, a licensure test for professional
counselors that requires about 70% accuracy to pass. PsychoBench comprises
approximately 2,252 carefully curated single-choice questions, crafted to
require deep understanding and broad enough to cover various sub-disciplines of
psychology. This benchmark provides a comprehensive assessment of an LLM's
ability to function as a counselor. Our evaluation shows that advanced models
such as GPT-4o, Llama3.3-70B, and Gemma3-27B achieve well above the passing
threshold, while smaller open-source models (e.g., Qwen2.5-7B, Mistral-7B)
remain far below it. These results suggest that only frontier LLMs are
currently capable of meeting counseling exam standards, highlighting both the
promise and the challenges of developing psychology-oriented LLMs.

</details>


### [61] [Learning to Decide with Just Enough: Information-Theoretic Context Summarization for CDMPs](https://arxiv.org/abs/2510.01620)
*Peidong Liu,Junjiang Lin,Shaowen Wang,Yao Xu,Haiqing Li,Xuhao Xie,Siyi Wu,Hao Li*

Main category: cs.AI

TL;DR: 该论文提出了一种基于信息论的上下文马尔可夫决策过程(CMDPs)方法，使用大语言模型(LLMs)压缩高维上下文输入为低维语义摘要，从而提高决策效率和稳定性。


<details>
  <summary>Details</summary>
Motivation: 现有的CMDP方法在高维或非结构化上下文中泛化能力差，导致计算量过大和性能不稳定，需要一种更有效的上下文处理方法。

Method: 使用LLMs将上下文输入压缩为低维语义摘要，这些摘要通过保留决策关键线索同时减少冗余来增强状态表示，基于近似上下文充分性概念提供理论分析。

Result: 在离散、连续、视觉和推荐基准测试中，该方法优于原始上下文和非上下文基线，提高了奖励、成功率和样本效率，同时减少了延迟和内存使用。

Conclusion: LLM-based summarization为上下文丰富、资源受限环境中的高效决策提供了可扩展和可解释的解决方案。

Abstract: Contextual Markov Decision Processes (CMDPs) offer a framework for sequential
decision-making under external signals, but existing methods often fail to
generalize in high-dimensional or unstructured contexts, resulting in excessive
computation and unstable performance. We propose an information-theoretic
summarization approach that uses large language models (LLMs) to compress
contextual inputs into low-dimensional, semantically rich summaries. These
summaries augment states by preserving decision-critical cues while reducing
redundancy. Building on the notion of approximate context sufficiency, we
provide, to our knowledge, the first regret bounds and a latency-entropy
trade-off characterization for CMDPs. Our analysis clarifies how
informativeness impacts computational cost. Experiments across discrete,
continuous, visual, and recommendation benchmarks show that our method
outperforms raw-context and non-context baselines, improving reward, success
rate, and sample efficiency, while reducing latency and memory usage. These
findings demonstrate that LLM-based summarization offers a scalable and
interpretable solution for efficient decision-making in context-rich,
resource-constrained environments.

</details>


### [62] [Understanding the Geospatial Reasoning Capabilities of LLMs: A Trajectory Recovery Perspective](https://arxiv.org/abs/2510.01639)
*Thinh Hung Truong,Jey Han Lau,Jianzhong Qi*

Main category: cs.AI

TL;DR: LLMs can read road network maps and perform navigation through trajectory recovery tasks, outperforming specialized models with strong zero-shot generalization.


<details>
  <summary>Details</summary>
Motivation: 探索大型语言模型在空间推理方面的能力，特别是能否理解道路网络地图并执行导航任务。

Method: 将轨迹恢复作为代理任务，使用GLOBALTRACE数据集（包含4000+真实轨迹），通过提示框架让LLMs在不依赖外部导航工具的情况下生成有效路径。

Result: LLMs在轨迹恢复任务中优于现成基线和专业轨迹恢复模型，具有强大的零样本泛化能力，但对不同区域和交通方式存在系统性偏差。

Conclusion: LLMs能够通过灵活推理地图来增强导航体验，并整合用户偏好，展现了强大的道路网络理解能力。

Abstract: We explore the geospatial reasoning capabilities of Large Language Models
(LLMs), specifically, whether LLMs can read road network maps and perform
navigation. We frame trajectory recovery as a proxy task, which requires models
to reconstruct masked GPS traces, and introduce GLOBALTRACE, a dataset with
over 4,000 real-world trajectories across diverse regions and transportation
modes. Using road network as context, our prompting framework enables LLMs to
generate valid paths without accessing any external navigation tools.
Experiments show that LLMs outperform off-the-shelf baselines and specialized
trajectory recovery models, with strong zero-shot generalization. Fine-grained
analysis shows that LLMs have strong comprehension of the road network and
coordinate systems, but also pose systematic biases with respect to regions and
transportation modes. Finally, we demonstrate how LLMs can enhance navigation
experiences by reasoning over maps in flexible ways to incorporate user
preferences.

</details>


### [63] [GuruAgents: Emulating Wise Investors with Prompt-Guided LLM Agents](https://arxiv.org/abs/2510.01664)
*Yejin Kim,Youngbin Lee,Juhyeong Kim,Yongjae Lee*

Main category: cs.AI

TL;DR: 本研究开发了GuruAgents——基于提示引导的AI代理，能够系统化实施传奇投资大师的策略。通过将五位投资大师的哲学编码到LLM提示中，结合金融工具和确定性推理流程，在纳斯达克100成分股上进行回测，巴菲特代理表现最佳，年化收益率达42.2%。


<details>
  <summary>Details</summary>
Motivation: 将定性投资哲学转化为可复现的定量策略，探索自动化系统投资的新方向。

Method: 开发五个不同的GuruAgents，将投资大师的独特哲学编码到LLM提示中，整合金融工具和确定性推理流程。

Result: 在2023年第四季度至2025年第二季度的纳斯达克100成分股回测中，巴菲特代理表现最佳，年化收益率42.2%，显著超越基准，其他代理表现各异。

Conclusion: 提示工程能够成功将投资大师的定性哲学转化为可复现的定量策略，为自动化系统投资开辟了新方向。

Abstract: This study demonstrates that GuruAgents, prompt-guided AI agents, can
systematically operationalize the strategies of legendary investment gurus. We
develop five distinct GuruAgents, each designed to emulate an iconic investor,
by encoding their distinct philosophies into LLM prompts that integrate
financial tools and a deterministic reasoning pipeline. In a backtest on
NASDAQ-100 constituents from Q4 2023 to Q2 2025, the GuruAgents exhibit unique
behaviors driven by their prompted personas. The Buffett GuruAgent achieves the
highest performance, delivering a 42.2\% CAGR that significantly outperforms
benchmarks, while other agents show varied results. These findings confirm that
prompt engineering can successfully translate the qualitative philosophies of
investment gurus into reproducible, quantitative strategies, highlighting a
novel direction for automated systematic investing. The source code and data
are available at https://github.com/yejining99/GuruAgents.

</details>


### [64] [A Locally Executable AI System for Improving Preoperative Patient Communication: A Multi-Domain Clinical Evaluation](https://arxiv.org/abs/2510.01671)
*Motoki Sato,Yuki Matsushita,Hidekazu Takahashi,Tomoaki Kakazu,Sou Nagata,Mizuho Ohnuma,Atsushi Yoshikawa,Masayuki Yamamura*

Main category: cs.AI

TL;DR: LENOHA是一个安全优先、本地优先的系统，通过高精度分类器将临床查询路由到预先审核的FAQ数据库，返回原文答案而非生成文本，在临床路径中避免幻觉错误。


<details>
  <summary>Details</summary>
Motivation: 患者在接受侵入性手术前常有未解答的问题，但时间紧迫的工作流程和隐私限制限制了个性化咨询。需要开发一个安全、高效且保护隐私的系统。

Method: 使用高精度的句子转换器分类器路由输入，从临床医生审核的FAQ中返回原文答案，完全避免临床路径中的自由文本生成。评估了两种医疗领域（拔牙和胃镜检查）。

Result: E5-large-instruct编码器在总体准确率0.983、AUC 0.996，仅有7个错误，统计上与GPT-4o无差异。非生成临床路径能耗仅为1.0 mWh/输入，比本地8B SLM小170倍，延迟约0.10秒。

Conclusion: 通过返回预先审核的FAQ答案而非生成文本，可以在临床路径中结构性地避免前沿模型产生的错误，支持隐私保护、可持续性和在带宽有限环境中的公平部署。

Abstract: Patients awaiting invasive procedures often have unanswered pre-procedural
questions; however, time-pressured workflows and privacy constraints limit
personalized counseling. We present LENOHA (Low Energy, No Hallucination, Leave
No One Behind Architecture), a safety-first, local-first system that routes
inputs with a high-precision sentence-transformer classifier and returns
verbatim answers from a clinician-curated FAQ for clinical queries, eliminating
free-text generation in the clinical path. We evaluated two domains (tooth
extraction and gastroscopy) using expert-reviewed validation sets
(n=400/domain) for thresholding and independent test sets (n=200/domain). Among
the four encoders, E5-large-instruct (560M) achieved an overall accuracy of
0.983 (95% CI 0.964-0.991), AUC 0.996, and seven total errors, which were
statistically indistinguishable from GPT-4o on this task; Gemini made no errors
on this test set. Energy logging shows that the non-generative clinical path
consumes ~1.0 mWh per input versus ~168 mWh per small-talk reply from a local
8B SLM, a ~170x difference, while maintaining ~0.10 s latency on a single
on-prem GPU. These results indicate that near-frontier discrimination and
generation-induced errors are structurally avoided in the clinical path by
returning vetted FAQ answers verbatim, supporting privacy, sustainability, and
equitable deployment in bandwidth-limited environments.

</details>


### [65] [Improving AGI Evaluation: A Data Science Perspective](https://arxiv.org/abs/2510.01687)
*John Hawkins*

Main category: cs.AI

TL;DR: 本文主张AGI评估方法应从基于直觉的合成任务转向关注稳健任务执行能力的评估，借鉴数据科学实践来展示系统的可靠部署能力。


<details>
  <summary>Details</summary>
Motivation: 当前AGI评估方法主要依赖对人类智能直觉设计的合成任务，但这些方法在AI历史上表现不佳，需要更有效的评估框架。

Method: 提出基于稳健任务执行的评估设计哲学，借鉴数据科学中展示系统可靠部署的实践方法，提供具体的AGI评估实践示例。

Result: 论证了基于直觉的合成任务评估方法的局限性，提出了以能力展示为导向的替代评估框架。

Conclusion: AGI评估应转向关注系统在实际任务中的稳健执行能力，而非依赖直觉设计的测试，这样才能更有效地衡量AGI进展。

Abstract: Evaluation of potential AGI systems and methods is difficult due to the
breadth of the engineering goal. We have no methods for perfect evaluation of
the end state, and instead measure performance on small tests designed to
provide directional indication that we are approaching AGI. In this work we
argue that AGI evaluation methods have been dominated by a design philosophy
that uses our intuitions of what intelligence is to create synthetic tasks,
that have performed poorly in the history of AI. Instead we argue for an
alternative design philosophy focused on evaluating robust task execution that
seeks to demonstrate AGI through competence. This perspective is developed from
common practices in data science that are used to show that a system can be
reliably deployed. We provide practical examples of what this would mean for
AGI evaluation.

</details>


### [66] [VaPR -- Vision-language Preference alignment for Reasoning](https://arxiv.org/abs/2510.01700)
*Rohan Wadhawan,Fabrice Y Harel-Canada,Zi-Yi Dou,Suhaila Shakiah,Robinson Piramuthu,Nanyun Peng*

Main category: cs.AI

TL;DR: 提出了VaPR框架，通过LLM引导的响应编辑生成硬负样本，解决合成偏好标注中的风格和长度偏差问题，显著提升大型视觉语言模型的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的偏好微调方法忽视了合成偏好标注中普遍存在的风格和长度偏差噪声问题，需要更高质量的负样本生成方法。

Method: 基于LLM引导的响应编辑框架，生成具有目标错误的拒绝响应，保持与接受响应在风格和长度上的相似性，构建了包含30K高质量样本的VaPR数据集。

Result: 在三个LVLM家族上实现显著性能提升：LLaVA平均提升6.5%，Qwen2VL提升4.0%，Qwen2.5VL提升1.5%；在推理任务上表现突出；减少二元问题中的"是"回答倾向；开源LLM编辑器的泛化性能达到GPT-4o的99%。

Conclusion: VaPR框架有效解决了合成偏好标注的噪声问题，通过高质量的硬负样本生成显著提升模型性能，且具有良好的可扩展性和泛化能力。

Abstract: Preference finetuning methods like Direct Preference Optimization (DPO) with
AI-generated feedback have shown promise in aligning Large Vision-Language
Models (LVLMs) with human preferences. However, existing techniques overlook
the prevalence of noise in synthetic preference annotations in the form of
stylistic and length biases. To this end, we introduce a hard-negative response
generation framework based on LLM-guided response editing, that produces
rejected responses with targeted errors, maintaining stylistic and length
similarity to the accepted ones. Using this framework, we develop the VaPR
dataset, comprising 30K high-quality samples, to finetune three LVLM families:
LLaVA-V1.5, Qwen2VL & Qwen2.5VL (2B-13B sizes). Our VaPR models deliver
significant performance improvements across ten benchmarks, achieving average
gains of 6.5% (LLaVA), 4.0% (Qwen2VL), and 1.5% (Qwen2.5VL), with notable
improvements on reasoning tasks. A scaling analysis shows that performance
consistently improves with data size, with LLaVA models benefiting even at
smaller scales. Moreover, VaPR reduces the tendency to answer "Yes" in binary
questions - addressing a common failure mode in LVLMs like LLaVA. Lastly, we
show that the framework generalizes to open-source LLMs as editors, with models
trained on VaPR-OS achieving ~99% of the performance of models trained on
\name, which is synthesized using GPT-4o. Our data, models, and code can be
found on the project page https://vap-r.github.io

</details>


### [67] [MetaboT: AI-based agent for natural language-based interaction with metabolomics knowledge graphs](https://arxiv.org/abs/2510.01724)
*Madina Bekbergenova,Lucas Pradi,Benjamin Navet,Emma Tysinger,Franck Michel,Matthieu Feraud,Yousouf Taghzouti,Yan Zhou Chen,Olivier Kirchhoffer,Florence Mehl,Martin Legrand,Tao Jiang,Marco Pagni,Soha Hassoun,Jean-Luc Wolfender,Wout Bittremieux,Fabien Gandon,Louis-Félix Nothias*

Main category: cs.AI

TL;DR: MetaboT是一个基于大语言模型的多代理AI系统，可将自然语言问题转换为SPARQL查询语言，用于在代谢组学知识图谱中检索数据，显著提高了查询准确率。


<details>
  <summary>Details</summary>
Motivation: 代谢组学质谱分析产生大量数据，知识图谱虽能结构化这些数据，但用户需要深入理解其本体和查询语言语法，这构成了技术障碍。

Method: 设计多代理系统，使用LangChain和LangGraph库集成LLM与外部工具，通过专门代理处理用户查询、验证问题、识别化学转换需求，并生成SPARQL查询。

Result: 在50个代谢组学相关问题上，MetaboT达到83.67%的准确率，而仅使用GPT-4o的基线方法仅8.16%，证明多代理系统的必要性。

Conclusion: MetaboT作为对话式问答助手，通过自动化SPARQL查询生成和执行，消除了访问知识图谱的技术障碍，同时保持与领域特定标准的一致性。

Abstract: Mass spectrometry metabolomics generates vast amounts of data requiring
advanced methods for interpretation. Knowledge graphs address these challenges
by structuring mass spectrometry data, metabolite information, and their
relationships into a connected network (Gaudry et al. 2024). However, effective
use of a knowledge graph demands an in-depth understanding of its ontology and
its query language syntax. To overcome this, we designed MetaboT, an AI system
utilizing large language models (LLMs) to translate user questions into SPARQL
semantic query language for operating on knowledge graphs (Steve Harris 2013).
We demonstrate its effectiveness using the Experimental Natural Products
Knowledge Graph (ENPKG), a large-scale public knowledge graph for plant natural
products (Gaudry et al. 2024).MetaboT employs specialized AI agents for
handling user queries and interacting with the knowledge graph by breaking down
complex tasks into discrete components, each managed by a specialised agent
(Fig. 1a). The multi-agent system is constructed using the LangChain and
LangGraph libraries, which facilitate the integration of LLMs with external
tools and information sources (LangChain, n.d.). The query generation process
follows a structured workflow. First, the Entry Agent determines if the
question is new or a follow-up to previous interactions. New questions are
forwarded to the Validator Agent, which verifies if the question is related to
the knowledge graph. Then, the valid question is sent to the Supervisor Agent,
which identifies if the question requires chemical conversions or standardized
identifiers. In this case it delegates the question to the Knowledge Graph
Agent, which can use tools to extract necessary details, such as URIs or
taxonomies of chemical names, from the user query. Finally, an agent
responsible for crafting the SPARQL queries equipped with the ontology of the
knowledge graph uses the provided identifiers to generate the query. Then, the
system executes the generated query against the metabolomics knowledge graph
and returns structured results to the user (Fig. 1b). To assess the performance
of MetaboT we have curated 50 metabolomics-related questions and their expected
answers. In addition to submitting these questions to MetaboT, we evaluated a
baseline by submitting them to a standard LLM (GPT-4o) with a prompt that
incorporated the knowledge graph ontology but did not provide specific entity
IDs. This baseline achieved only 8.16% accuracy, compared to MetaboT's 83.67%,
underscoring the necessity of our multi-agent system for accurately retrieving
entities and generating correct SPARQL queries. MetaboT demonstrates promising
performance as a conversational question-answering assistant, enabling
researchers to retrieve structured metabolomics data through natural language
queries. By automating the generation and execution of SPARQL queries, it
removes technical barriers that have traditionally hindered access to knowledge
graphs. Importantly, MetaboT leverages the capabilities of LLMs while
maintaining experimentally grounded query generation, ensuring that outputs
remain aligned with domain-specific standards and data structures. This
approach facilitates data-driven discoveries by bridging the gap between
complex semantic technologies and user-friendly interaction. MetaboT is
accessible at [https://metabot.holobiomicslab.eu/], and its source code is
available at [https://github.com/HolobiomicsLab/MetaboT].

</details>


### [68] [A cybersecurity AI agent selection and decision support framework](https://arxiv.org/abs/2510.01751)
*Masike Malatji*

Main category: cs.AI

TL;DR: 提出了一种结构化决策支持框架，将不同类型的AI智能体架构与NIST网络安全框架2.0进行系统对齐，为选择部署AI解决方案应对网络威胁提供透明方法。


<details>
  <summary>Details</summary>
Motivation: 弥合理论AI构建与操作网络安全需求之间的差距，为符合行业标准的稳健多智能体系统奠定基础。

Method: 通过将NIST CSF 2.0功能细分为具体任务，将AI智能体特性（自主性、自适应学习、实时响应）与每个子类别的安全要求联系起来，并定义分级自主性水平。

Result: 概念验证表明，定制化的AI智能体部署可以增强态势感知、加速响应时间，并通过自适应风险管理加强长期韧性。

Conclusion: 该研究建立了理论AI构建与操作网络安全需求之间的桥梁，为符合行业标准的稳健多智能体系统提供了基础。

Abstract: This paper presents a novel, structured decision support framework that
systematically aligns diverse artificial intelligence (AI) agent architectures,
reactive, cognitive, hybrid, and learning, with the comprehensive National
Institute of Standards and Technology (NIST) Cybersecurity Framework (CSF) 2.0.
By integrating agent theory with industry guidelines, this framework provides a
transparent and stepwise methodology for selecting and deploying AI solutions
to address contemporary cyber threats. Employing a granular decomposition of
NIST CSF 2.0 functions into specific tasks, the study links essential AI agent
properties such as autonomy, adaptive learning, and real-time responsiveness to
each subcategory's security requirements. In addition, it outlines graduated
levels of autonomy (assisted, augmented, and fully autonomous) to accommodate
organisations at varying stages of cybersecurity maturity. This holistic
approach transcends isolated AI applications, providing a unified detection,
incident response, and governance strategy. Through conceptual validation, the
framework demonstrates how tailored AI agent deployments can align with
real-world constraints and risk profiles, enhancing situational awareness,
accelerating response times, and fortifying long-term resilience via adaptive
risk management. Ultimately, this research bridges the gap between theoretical
AI constructs and operational cybersecurity demands, establishing a foundation
for robust, empirically validated multi-agent systems that adhere to industry
standards.

</details>


### [69] [REBot: From RAG to CatRAG with Semantic Enrichment and Graph Routing](https://arxiv.org/abs/2510.01800)
*Thanh Ma,Tri-Tam La,Lam-Thu Le Huu,Minh-Nghi Nguyen,Khanh-Van Pham Luu,Huu-Hoa Nguyen*

Main category: cs.AI

TL;DR: 提出了REBot学术咨询聊天机器人，采用CatRAG混合检索推理框架，结合检索增强生成和图推理，在学术规定咨询任务中达到98.89%的F1分数。


<details>
  <summary>Details</summary>
Motivation: 学术规定咨询对学生理解机构政策很重要，但构建有效系统需要特定领域的监管资源。

Method: CatRAG框架统一密集检索和图推理，使用分层类别标记的知识图谱，配备轻量级意图分类器来路由查询。

Result: 在分类和问答任务中达到最先进性能，F1分数98.89%，并开发了实际可用的Web应用。

Conclusion: REBot系统在学术咨询场景中具有实用价值，CatRAG框架能有效处理领域特定的监管咨询需求。

Abstract: Academic regulation advising is essential for helping students interpret and
comply with institutional policies, yet building effective systems requires
domain specific regulatory resources. To address this challenge, we propose
REBot, an LLM enhanced advisory chatbot powered by CatRAG, a hybrid retrieval
reasoning framework that integrates retrieval augmented generation with graph
based reasoning. CatRAG unifies dense retrieval and graph reasoning, supported
by a hierarchical, category labeled knowledge graph enriched with semantic
features for domain alignment. A lightweight intent classifier routes queries
to the appropriate retrieval modules, ensuring both factual accuracy and
contextual depth. We construct a regulation specific dataset and evaluate REBot
on classification and question answering tasks, achieving state of the art
performance with an F1 score of 98.89%. Finally, we implement a web application
that demonstrates the practical value of REBot in real world academic advising
scenarios.

</details>


### [70] [Human-AI Teaming Co-Learning in Military Operations](https://arxiv.org/abs/2510.01815)
*Clara Maathuis,Kasper Cools*

Main category: cs.AI

TL;DR: 提出一个可信赖的协同学习模型，用于军事行动中的人机协作，包含可调自主性、多层控制、双向反馈和协作决策四个维度。


<details>
  <summary>Details</summary>
Motivation: 在快速演变的军事威胁和复杂作战环境中，AI集成带来显著优势，但也面临有效和伦理部署的挑战。当前研究多从外部视角处理人机协作系统，而深入系统内部动态能处理更广泛的多维责任、安全和鲁棒性问题。

Method: 设计包含四个维度的可信赖协同学习模型：1) 可调自主性 - 根据任务状态、系统信心和环境不确定性动态校准自主级别；2) 多层控制 - 持续监督、活动监控和问责制；3) 双向反馈 - 显性和隐性反馈循环，确保推理、不确定性和学习适应的适当沟通；4) 协作决策 - 生成、评估和提出带有置信度和理由的决策。

Result: 提出了一个具体的可信赖协同学习模型，并提供了具体示例和建议，有助于进一步开发负责任和可信赖的军事行动人机协作系统。

Conclusion: 该模型通过四个集成维度支持人机在军事行动中的持续双向学习，为构建更负责任和可信赖的人机协作系统提供了框架和方法。

Abstract: In a time of rapidly evolving military threats and increasingly complex
operational environments, the integration of AI into military operations proves
significant advantages. At the same time, this implies various challenges and
risks regarding building and deploying human-AI teaming systems in an effective
and ethical manner. Currently, understanding and coping with them are often
tackled from an external perspective considering the human-AI teaming system as
a collective agent. Nevertheless, zooming into the dynamics involved inside the
system assures dealing with a broader palette of relevant multidimensional
responsibility, safety, and robustness aspects. To this end, this research
proposes the design of a trustworthy co-learning model for human-AI teaming in
military operations that encompasses a continuous and bidirectional exchange of
insights between the human and AI agents as they jointly adapt to evolving
battlefield conditions. It does that by integrating four dimensions. First,
adjustable autonomy for dynamically calibrating the autonomy levels of agents
depending on aspects like mission state, system confidence, and environmental
uncertainty. Second, multi-layered control which accounts continuous oversight,
monitoring of activities, and accountability. Third, bidirectional feedback
with explicit and implicit feedback loops between the agents to assure a proper
communication of reasoning, uncertainties, and learned adaptations that each of
the agents has. And fourth, collaborative decision-making which implies the
generation, evaluation, and proposal of decisions associated with confidence
levels and rationale behind them. The model proposed is accompanied by concrete
exemplifications and recommendations that contribute to further developing
responsible and trustworthy human-AI teaming systems in military operations.

</details>


### [71] [Plan Then Action:High-Level Planning Guidance Reinforcement Learning for LLM Reasoning](https://arxiv.org/abs/2510.01833)
*Zhihao Dou,Qinjian Zhao,Zhongwei Wan,Dinggen Zhang,Weida Wang,Towsif Raiyan,Benteng Chen,Qingtao Pan,Yang Ouyang,Zhiqiang Gao,Shufei Zhang,Sumon Biswas*

Main category: cs.AI

TL;DR: 提出了PTA-GRPO框架，通过两阶段方法改进LLM的推理能力：第一阶段使用高级LLM提炼紧凑的高层指导进行监督微调，第二阶段引入指导感知的强化学习方法联合优化最终输出和指导质量。


<details>
  <summary>Details</summary>
Motivation: 现有LLM的推理过程受限于自回归的token级生成，缺乏全局规划，导致冗余、不连贯或不准确的推理，影响整体性能。现有方法如树算法和RL计算成本高且难以产生最优推理轨迹。

Method: 两阶段框架：1) 使用高级LLM将思维链提炼为紧凑高层指导进行SFT；2) 提出指导感知RL方法，联合优化最终输出和指导质量。

Result: 在多个数学推理基准测试（MATH、AIME2024、AIME2025、AMC）和不同基础模型上，PTA-GRPO均实现了稳定且显著的性能提升。

Conclusion: PTA-GRPO能有效提升LLM的推理能力，在不同模型和任务上表现出良好的有效性和泛化性。

Abstract: Large language models (LLMs) have demonstrated remarkable reasoning abilities
in complex tasks, often relying on Chain-of-Thought (CoT) reasoning. However,
due to their autoregressive token-level generation, the reasoning process is
largely constrained to local decision-making and lacks global planning. This
limitation frequently results in redundant, incoherent, or inaccurate
reasoning, which significantly degrades overall performance. Existing
approaches, such as tree-based algorithms and reinforcement learning (RL),
attempt to address this issue but suffer from high computational costs and
often fail to produce optimal reasoning trajectories. To tackle this challenge,
we propose Plan-Then-Action Enhanced Reasoning with Group Relative Policy
Optimization PTA-GRPO, a two-stage framework designed to improve both
high-level planning and fine-grained CoT reasoning. In the first stage, we
leverage advanced LLMs to distill CoT into compact high-level guidance, which
is then used for supervised fine-tuning (SFT). In the second stage, we
introduce a guidance-aware RL method that jointly optimizes the final output
and the quality of high-level guidance, thereby enhancing reasoning
effectiveness. We conduct extensive experiments on multiple mathematical
reasoning benchmarks, including MATH, AIME2024, AIME2025, and AMC, across
diverse base models such as Qwen2.5-7B-Instruct, Qwen3-8B, Qwen3-14B, and
LLaMA3.2-3B. Experimental results demonstrate that PTA-GRPO consistently
achieves stable and significant improvements across different models and tasks,
validating its effectiveness and generalization.

</details>


### [72] [Learning a Dense Reasoning Reward Model from Expert Demonstration via Inverse Reinforcement Learning](https://arxiv.org/abs/2510.01857)
*Claudio Fanconi,Nicolás Astorga,Mihaela van der Schaar*

Main category: cs.AI

TL;DR: 该论文将对抗性逆强化学习应用于大语言模型推理，学习密集的token级奖励模型用于过程监督，而非通过监督微调模仿风格。该推理奖励在训练时提供步骤级反馈优化推理策略，在推理时作为批评者重排采样轨迹。


<details>
  <summary>Details</summary>
Motivation: 重新构建和操作化对抗性逆强化学习到大语言模型推理中，直接从专家演示中学习密集的token级奖励模型，而不是通过监督微调模仿风格。

Method: 使用对抗性逆强化学习框架，学习密集的token级推理奖励模型。该奖励在训练时提供步骤级反馈优化推理策略，在推理时作为批评者重排采样轨迹。

Result: 在GSM8K数据集上使用Llama3和Qwen2.5骨干网络，证明：(i)密集推理奖励可用作学习信号来引发推理；(ii)通过奖励引导的重排提高了预测性能（特别是基于Llama的策略）。

Conclusion: 通过将训练信号、推理时间选择和token级诊断统一到单个推理奖励中，这项工作表明可重用的过程级奖励具有增强语言模型中多步推理的广泛潜力。

Abstract: We reframe and operationalise adversarial inverse reinforcement learning
(IRL) to large language model reasoning, learning a dense, token-level reward
model for process supervision directly from expert demonstrations rather than
imitating style via supervised fine-tuning. The learned reasoning reward serves
two complementary roles: (i) it provides step-level feedback to optimise a
reasoning policy during training; and (ii) it functions at inference as a
critic to rerank sampled traces under fixed compute budgets. We demonstrate
that our approach prioritises correctness over surface form, yielding scores
that correlate with eventual answer validity and enabling interpretable
localisation of errors within a trace. Empirically, on GSM8K with Llama3 and
Qwen2.5 backbones, we demonstrate: (i) dense reasoning rewards can be used as a
learning signal to elicit reasoning, and (ii) predictive performance is
improved from reward-guided reranking (notably for Llama-based policies). By
unifying training signals, inference-time selection, and token-level
diagnostics into a single reasoning reward, this work suggests reusable
process-level rewards with broad potential to enhance multi-step reasoning in
language models.

</details>


### [73] [Constrained Adaptive Rejection Sampling](https://arxiv.org/abs/2510.01902)
*Paweł Parys,Sairam Vaidya,Taylor Berg-Kirkpatrick,Loris D'Antoni*

Main category: cs.AI

TL;DR: 提出CARS方法，在保持语言模型分布保真度的同时，通过自适应剪枝提高约束生成的采样效率


<details>
  <summary>Details</summary>
Motivation: 现有约束生成方法存在两难：贪婪约束解码会扭曲模型分布，而拒绝采样会浪费计算资源丢弃无效输出。在程序模糊测试等需要有效性和多样性的领域，这两种极端方法都有问题

Method: CARS方法从无约束语言模型采样开始，通过记录违反约束的序列到trie结构中，自适应地排除这些无效延续，从未来采样中减去其概率质量

Result: 在程序模糊测试和分子生成等多个领域的实验中，CARS始终比GCD和近似LM分布的方法获得更高的效率（每个有效样本所需的LM前向传递次数）和更强的样本多样性

Conclusion: CARS严格改进了拒绝采样的样本效率而不产生分布扭曲，确保前缀一旦被证明无效就永不重访，接受率单调提高，生成的样本精确遵循约束分布

Abstract: Language Models (LMs) are increasingly used in applications where generated
outputs must satisfy strict semantic or syntactic constraints. Existing
approaches to constrained generation fall along a spectrum: greedy constrained
decoding methods enforce validity during decoding but distort the LM's
distribution, while rejection sampling (RS) preserves fidelity but wastes
computation by discarding invalid outputs. Both extremes are problematic in
domains such as program fuzzing, where both validity and diversity of samples
are essential. We present Constrained Adaptive Rejection Sampling (CARS), an
approach that strictly improves the sample-efficiency of RS without
distributional distortion. CARS begins with unconstrained LM sampling and
adaptively rules out constraint-violating continuations by recording them in a
trie and subtracting their probability mass from future draws. This adaptive
pruning ensures that prefixes proven invalid are never revisited, acceptance
rates improve monotonically, and the resulting samples exactly follow the
constrained distribution. In experiments on a variety of domains -- e.g.,
program fuzzing and molecular generation -- CARS consistently achieves higher
efficiency -- measured in the number of LM forward passes per valid sample --
while also producing stronger sample diversity than both GCD and methods that
approximate the LM's distribution.

</details>


### [74] [To Mask or to Mirror: Human-AI Alignment in Collective Reasoning](https://arxiv.org/abs/2510.01924)
*Crystal Qian,Aaron Parisi,Clémentine Bouleau,Vivian Tsai,Maël Lebreton,Lucas Dixon*

Main category: cs.AI

TL;DR: 评估大语言模型在集体决策中与人类社会推理的对齐程度，通过Lost at Sea任务进行大规模实验，发现LLM行为存在分歧，集体对齐取决于情境、线索和模型特定偏见。


<details>
  <summary>Details</summary>
Motivation: 随着LLM越来越多地用于建模和增强集体决策，需要检验它们与人类社会推理的对齐程度，现有研究主要关注个体层面而缺乏集体层面的评估。

Method: 使用Lost at Sea社会心理学任务进行大规模在线实验(N=748)，随机分配小组进行领导者选举，比较可见人口属性与匿名化名称的影响，然后模拟匹配的LLM小组并评估多个模型。

Result: LLM行为出现分歧：一些模型反映人类偏见，另一些则掩盖这些偏见并试图补偿。集体推理中的人机对齐取决于情境、线索和模型特定的归纳偏见。

Conclusion: 理解LLM如何与集体人类行为对齐对于推进社会对齐AI至关重要，需要能够捕捉集体推理复杂性的动态基准测试。

Abstract: As large language models (LLMs) are increasingly used to model and augment
collective decision-making, it is critical to examine their alignment with
human social reasoning. We present an empirical framework for assessing
collective alignment, in contrast to prior work on the individual level. Using
the Lost at Sea social psychology task, we conduct a large-scale online
experiment (N=748), randomly assigning groups to leader elections with either
visible demographic attributes (e.g. name, gender) or pseudonymous aliases. We
then simulate matched LLM groups conditioned on the human data, benchmarking
Gemini 2.5, GPT 4.1, Claude Haiku 3.5, and Gemma 3. LLM behaviors diverge: some
mirror human biases; others mask these biases and attempt to compensate for
them. We empirically demonstrate that human-AI alignment in collective
reasoning depends on context, cues, and model-specific inductive biases.
Understanding how LLMs align with collective human behavior is critical to
advancing socially-aligned AI, and demands dynamic benchmarks that capture the
complexities of collective reasoning.

</details>


### [75] [Zero-shot reasoning for simulating scholarly peer-review](https://arxiv.org/abs/2510.02027)
*Khalid M. Saqr*

Main category: cs.AI

TL;DR: 提出一个确定性模拟框架，为评估AI生成的同行评审报告提供首个稳定、基于证据的标准，通过分析352份模拟报告验证了系统的可靠性。


<details>
  <summary>Details</summary>
Motivation: 学术出版生态系统面临提交量不可管理和AI未受监管的双重危机，需要新的治理模式来保障科学诚信。传统纯人工同行评审缺乏可扩展、客观的基准。

Method: 采用确定性模拟框架，分析352份同行评审模拟报告，识别一致的系统状态指标来验证可靠性。

Result: 系统能够模拟校准的编辑判断，"修订"决定在所有学科中始终占多数结果(>50%)，"拒绝"率动态适应领域特定规范；保持程序完整性，强制执行稳定的29%证据锚定合规率。

Conclusion: 该框架将AI重新定位为机构问责的重要组成部分，为维护学术交流信任提供关键基础设施，为科学界提供确保公平的透明工具，为出版策略师提供可扩展的审计工具。

Abstract: The scholarly publishing ecosystem faces a dual crisis of unmanageable
submission volumes and unregulated AI, creating an urgent need for new
governance models to safeguard scientific integrity. The traditional human-only
peer review regime lacks a scalable, objective benchmark, making editorial
processes opaque and difficult to audit. Here we investigate a deterministic
simulation framework that provides the first stable, evidence-based standard
for evaluating AI-generated peer review reports. Analyzing 352 peer-review
simulation reports, we identify consistent system state indicators that
demonstrate its reliability. First, the system is able to simulate calibrated
editorial judgment, with 'Revise' decisions consistently forming the majority
outcome (>50%) across all disciplines, while 'Reject' rates dynamically adapt
to field-specific norms, rising to 45% in Health Sciences. Second, it maintains
unwavering procedural integrity, enforcing a stable 29% evidence-anchoring
compliance rate that remains invariant across diverse review tasks and
scientific domains. These findings demonstrate a system that is predictably
rule-bound, mitigating the stochasticity of generative AI. For the scientific
community, this provides a transparent tool to ensure fairness; for publishing
strategists, it offers a scalable instrument for auditing workflows, managing
integrity risks, and implementing evidence-based governance. The framework
repositions AI as an essential component of institutional accountability,
providing the critical infrastructure to maintain trust in scholarly
communication.

</details>


### [76] [ReTabAD: A Benchmark for Restoring Semantic Context in Tabular Anomaly Detection](https://arxiv.org/abs/2510.02060)
*Sanghyu Yoon,Dongmin Kim,Suhee Yoon,Ye Seul Sim,Seungdong Yoa,Hye-Seung Cho,Soonyoung Lee,Hankook Lee,Woohyung Lim*

Main category: cs.AI

TL;DR: ReTabAD是一个为表格异常检测恢复文本语义的基准，提供20个带文本元数据的表格数据集和多种异常检测算法实现，包括零样本LLM框架，实验表明语义上下文能提升检测性能和可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有表格异常检测基准缺乏文本语义上下文，而实际应用中异常定义与领域特定上下文密切相关，专家依赖特征描述和领域知识进行检测。

Method: 构建20个带结构化文本元数据的表格数据集，实现包括经典、深度学习和LLM方法在内的多种异常检测算法，并开发零样本LLM框架利用语义上下文。

Result: 语义上下文显著提升异常检测性能，并增强可解释性，支持领域感知推理。

Conclusion: ReTabAD为系统探索上下文感知的表格异常检测建立了基准，证实文本元数据在异常检测中的重要价值。

Abstract: In tabular anomaly detection (AD), textual semantics often carry critical
signals, as the definition of an anomaly is closely tied to domain-specific
context. However, existing benchmarks provide only raw data points without
semantic context, overlooking rich textual metadata such as feature
descriptions and domain knowledge that experts rely on in practice. This
limitation restricts research flexibility and prevents models from fully
leveraging domain knowledge for detection. ReTabAD addresses this gap by
restoring textual semantics to enable context-aware tabular AD research. We
provide (1) 20 carefully curated tabular datasets enriched with structured
textual metadata, together with implementations of state-of-the-art AD
algorithms including classical, deep learning, and LLM-based approaches, and
(2) a zero-shot LLM framework that leverages semantic context without
task-specific training, establishing a strong baseline for future research.
Furthermore, this work provides insights into the role and utility of textual
metadata in AD through experiments and analysis. Results show that semantic
context improves detection performance and enhances interpretability by
supporting domain-aware reasoning. These findings establish ReTabAD as a
benchmark for systematic exploration of context-aware AD.

</details>


### [77] [Demystifying the Roles of LLM Layers in Retrieval, Knowledge, and Reasoning](https://arxiv.org/abs/2510.02091)
*Xinyuan Song,Keyu Wang,PengXiang Li,Lu Yin,Shiwei Liu*

Main category: cs.AI

TL;DR: 论文系统研究了LLM深度层的作用，发现深度层利用率高度异质且依赖上下文。浅层负责知识和检索，深层对推理至关重要，但可通过蒸馏重塑。


<details>
  <summary>Details</summary>
Motivation: 反驳现有研究认为LLM深层对表示学习贡献不大的观点，指出这些结论基于狭窄评估，可能忽略模型行为的重要方面。

Method: 从评估协议、任务类别和模型架构等多个维度系统研究深度利用率，包括基于似然和生成的评估方法。

Result: 基于似然的评估显示只有前几层关键，而生成评估揭示中深层在推理和长程连贯性中不可或缺。知识和检索集中在浅层，推理精度依赖深层但可通过蒸馏改善。

Conclusion: LLM深度使用高度异质且依赖上下文，在解释和压缩大模型时需要任务、指标和模型感知的视角。

Abstract: Recent studies suggest that the deeper layers of Large Language Models (LLMs)
contribute little to representation learning and can often be removed without
significant performance loss. However, such claims are typically drawn from
narrow evaluations and may overlook important aspects of model behavior. In
this work, we present a systematic study of depth utilization across diverse
dimensions, including evaluation protocols, task categories, and model
architectures. Our analysis confirms that very deep layers are generally less
effective than earlier ones, but their contributions vary substantially with
the evaluation setting. Under likelihood-based metrics without generation,
pruning most layers preserves performance, with only the initial few being
critical. By contrast, generation-based evaluation uncovers indispensable roles
for middle and deeper layers in enabling reasoning and maintaining long-range
coherence. We further find that knowledge and retrieval are concentrated in
shallow components, whereas reasoning accuracy relies heavily on deeper layers
-- yet can be reshaped through distillation. These results highlight that depth
usage in LLMs is highly heterogeneous and context-dependent, underscoring the
need for task-, metric-, and model-aware perspectives in both interpreting and
compressing large models.

</details>


### [78] [Do AI Models Perform Human-like Abstract Reasoning Across Modalities?](https://arxiv.org/abs/2510.02125)
*Claas Beger,Ryan Yi,Shuhao Fu,Arseny Moskvichev,Sarah W. Tsai,Sivasankaran Rajamanickam,Melanie Mitchell*

Main category: cs.AI

TL;DR: 论文评估AI模型在ConceptARC任务上的抽象推理能力，发现仅凭输出准确率会高估文本模态的抽象推理能力，低估视觉模态的抽象推理能力。


<details>
  <summary>Details</summary>
Motivation: 研究AI模型是否真正理解ARC-AGI基准测试中任务设计者意图的抽象概念，而不仅仅是依赖表面模式。

Method: 在ConceptARC上评估模型，改变输入模态（文本vs视觉）、是否允许使用外部Python工具、推理努力程度，并对模型生成的自然语言规则进行细粒度评估。

Result: 使用文本表示的一些模型在输出准确率上达到人类水平，但其规则往往基于表面"捷径"，很少捕捉到预期的抽象概念。在视觉模态中，模型准确率大幅下降，但仍能生成相当比例的捕捉预期抽象的规则。

Conclusion: 模型在抽象推理方面仍落后于人类，仅凭准确率评估ARC类任务的抽象推理能力会高估文本模态能力、低估视觉模态能力，需要更全面的评估框架。

Abstract: OpenAI's o3-preview reasoning model exceeded human accuracy on the ARC-AGI
benchmark, but does that mean state-of-the-art models recognize and reason with
the abstractions that the task creators intended? We investigate models'
abstraction abilities on ConceptARC. We evaluate models under settings that
vary the input modality (textual vs. visual), whether the model is permitted to
use external Python tools, and, for reasoning models, the amount of reasoning
effort. In addition to measuring output accuracy, we perform fine-grained
evaluation of the natural-language rules that models generate to explain their
solutions. This dual evaluation lets us assess whether models solve tasks using
the abstractions ConceptARC was designed to elicit, rather than relying on
surface-level patterns. Our results show that, while some models using
text-based representations match human output accuracy, the best models' rules
are often based on surface-level ``shortcuts'' and capture intended
abstractions far less often than humans. Thus their capabilities for general
abstract reasoning may be overestimated by evaluations based on accuracy alone.
In the visual modality, AI models' output accuracy drops sharply, yet our
rule-level analysis reveals that models might be underestimated, as they still
exhibit a substantial share of rules that capture intended abstractions, but
are often unable to correctly apply these rules. In short, our results show
that models still lag humans in abstract reasoning, and that using accuracy
alone to evaluate abstract reasoning on ARC-like tasks may overestimate
abstract-reasoning capabilities in textual modalities and underestimate it in
visual modalities. We believe that our evaluation framework offers a more
faithful picture of multimodal models' abstract reasoning abilities and a more
principled way to track progress toward human-like, abstraction-centered
intelligence.

</details>


### [79] [FlexDoc: Parameterized Sampling for Diverse Multilingual Synthetic Documents for Training Document Understanding Models](https://arxiv.org/abs/2510.02133)
*Karan Dua,Hitesh Laxmichand Patel,Puneet Mittal,Ranjeet Gupta,Amit Agarwal,Praneet Pabolu,Srikant Panda,Hansa Meghwani,Graham Horwood,Fahad Shah*

Main category: cs.AI

TL;DR: FlexDoc是一个可扩展的合成数据生成框架，通过概率建模布局模式和内容变异性，大规模生成具有丰富标注的多语言半结构化文档，显著降低数据收集和标注成本。


<details>
  <summary>Details</summary>
Motivation: 企业级文档理解模型需要大规模、多样化的标注数据集，但传统数据收集方法面临隐私限制、法律约束和手动标注成本高昂的问题，成本可达数百万美元。

Method: 结合随机模式和参数化采样，通过概率建模布局模式、视觉结构和内容变异性，实现可控的大规模文档变体生成。

Result: 在关键信息提取任务中，使用FlexDoc生成的数据将绝对F1分数提高了11%，同时相比传统硬模板方法减少了90%以上的标注工作量。

Conclusion: FlexDoc已在实际部署中加速了企业级文档理解模型的开发，同时显著降低了数据获取和标注成本。

Abstract: Developing document understanding models at enterprise scale requires large,
diverse, and well-annotated datasets spanning a wide range of document types.
However, collecting such data is prohibitively expensive due to privacy
constraints, legal restrictions, and the sheer volume of manual annotation
needed - costs that can scale into millions of dollars. We introduce FlexDoc, a
scalable synthetic data generation framework that combines Stochastic Schemas
and Parameterized Sampling to produce realistic, multilingual semi-structured
documents with rich annotations. By probabilistically modeling layout patterns,
visual structure, and content variability, FlexDoc enables the controlled
generation of diverse document variants at scale. Experiments on Key
Information Extraction (KIE) tasks demonstrate that FlexDoc-generated data
improves the absolute F1 Score by up to 11% when used to augment real datasets,
while reducing annotation effort by over 90% compared to traditional
hard-template methods. The solution is in active deployment, where it has
accelerated the development of enterprise-grade document understanding models
while significantly reducing data acquisition and annotation costs.

</details>


### [80] [A Rigorous Benchmark with Multidimensional Evaluation for Deep Research Agents: From Answers to Reports](https://arxiv.org/abs/2510.02190)
*Yang Yao,Yixu Wang,Yuxuan Zhang,Yi Lu,Tianle Gu,Lingyu Li,Dingyi Zhao,Keming Wu,Haozhe Wang,Ping Nie,Yan Teng,Yingchun Wang*

Main category: cs.AI

TL;DR: 该论文提出了一个专门针对深度研究代理(DRAs)的严格基准和多维评估框架，包含214个专家策划的挑战性查询和手动构建的参考包，用于全面评估DRAs生成的长篇报告。


<details>
  <summary>Details</summary>
Motivation: 现有基准在评估维度、响应格式和评分机制方面存在不足，无法有效评估从封闭语言模型转向具有外部感知和信息集成能力的互联代理系统的AI范式转变。

Method: 构建了包含214个挑战性查询的基准，分布在10个广泛主题领域，每个查询配有手动构建的参考包。开发了多维评估框架，整合了语义质量、主题聚焦和检索可信度的评分指标。

Result: 实验证实主流DRAs在复杂开放任务上的性能优于基于网络搜索工具增强的推理模型，但仍存在显著的改进空间。

Conclusion: 该研究为DRA系统的能力评估、架构改进和范式发展提供了坚实基础。

Abstract: Artificial intelligence is undergoing the paradigm shift from closed language
models to interconnected agent systems capable of external perception and
information integration. As a representative embodiment, Deep Research Agents
(DRAs) systematically exhibit the capabilities for task decomposition,
cross-source retrieval, multi-stage reasoning, and structured output, which
markedly enhance performance on complex and open-ended tasks. However, existing
benchmarks remain deficient in evaluation dimensions, response formatting, and
scoring mechanisms, limiting their capacity to assess such systems effectively.
This paper introduces a rigorous benchmark and a multidimensional evaluation
framework tailored to DRAs and report-style responses. The benchmark comprises
214 expert-curated challenging queries distributed across 10 broad thematic
domains, each accompanied by manually constructed reference bundles to support
composite evaluation. The framework enables comprehensive evaluation of
long-form reports generated by DRAs, incorporating integrated scoring metrics
for semantic quality, topical focus, and retrieval trustworthiness. Extensive
experimentation confirms the superior performance of mainstream DRAs over
web-search-tool-augmented reasoning models, yet reveals considerable scope for
further improvement. This study provides a robust foundation for capability
assessment, architectural refinement, and paradigm advancement in DRA systems.

</details>


### [81] [The Reasoning Boundary Paradox: How Reinforcement Learning Constrains Language Models](https://arxiv.org/abs/2510.02230)
*Phuc Minh Nguyen,Chinh D. La,Duy M. H. Nguyen,Nitesh V. Chawla,Binh T. Nguyen,Khoa D. Doan*

Main category: cs.AI

TL;DR: RLVR方法在提升大语言模型推理能力时，反而会缩小推理边界。研究发现存在负干扰和赢家通吃现象，导致模型收敛到狭窄的解决方案策略。提出了针对低概率问题的数据筛选算法来改善Pass@k性能。


<details>
  <summary>Details</summary>
Motivation: 研究RLVR方法在提升大语言模型推理能力时出现的推理边界缩小问题，揭示其失败原因。

Method: 通过理论分析和在多个数学推理基准上的实证研究，分析RLVR的学习动态，识别负干扰和赢家通吃现象，并提出针对低概率问题的数据筛选算法。

Result: 发现RLVR存在负干扰（学习某些问题会降低其他问题的正确解决概率）和赢家通吃现象（过度强化高概率问题，抑制低概率问题），导致Pass@k性能下降。提出的数据筛选算法显著改善了性能。

Conclusion: RLVR的推理边界缩小问题源于标准RL目标中的内在在线采样策略，导致模型收敛到狭窄的解决方案策略。通过专注于低概率问题的数据筛选可以显著改善性能。

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as a key
method for improving Large Language Models' reasoning capabilities, yet recent
evidence suggests it may paradoxically shrink the reasoning boundary rather
than expand it. This paper investigates the shrinkage issue of RLVR by
analyzing its learning dynamics and reveals two critical phenomena that explain
this failure. First, we expose negative interference in RLVR, where learning to
solve certain training problems actively reduces the likelihood of correct
solutions for others, leading to the decline of Pass@$k$ performance, or the
probability of generating a correct solution within $k$ attempts. Second, we
uncover the winner-take-all phenomenon: RLVR disproportionately reinforces
problems with high likelihood, correct solutions, under the base model, while
suppressing other initially low-likelihood ones. Through extensive theoretical
and empirical analysis on multiple mathematical reasoning benchmarks, we show
that this effect arises from the inherent on-policy sampling in standard RL
objectives, causing the model to converge toward narrow solution strategies.
Based on these insights, we propose a simple yet effective data curation
algorithm that focuses RLVR learning on low-likelihood problems, achieving
notable improvement in Pass@$k$ performance. Our code is available at
https://github.com/mail-research/SELF-llm-interference.

</details>


### [82] [The Unreasonable Effectiveness of Scaling Agents for Computer Use](https://arxiv.org/abs/2510.02250)
*Gonzalo Gonzalez-Pumariega,Vincent Tu,Chih-Lun Lee,Jiachen Yang,Ang Li,Xin Eric Wang*

Main category: cs.AI

TL;DR: 提出Behavior Best-of-N (bBoN)方法，通过生成多个rollout并使用行为叙述进行选择，显著提升计算机使用代理在复杂任务中的成功率和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 计算机使用代理在自动化日常数字任务方面具有潜力，但其不可靠性和高方差阻碍了在长期复杂任务中的应用。

Method: bBoN方法通过生成多个rollout并使用描述代理行为轨迹的叙述来进行选择，实现广泛探索和原则性轨迹选择。

Result: 在OSWorld上达到69.9%的新SOTA，接近72%的人类水平表现，在WindowsAgentArena和AndroidWorld上展示出强泛化能力。

Conclusion: 有效扩展计算机使用代理需要结构化轨迹理解和选择，bBoN为实现这一目标提供了实用框架。

Abstract: Computer-use agents (CUAs) hold promise for automating everyday digital
tasks, but their unreliability and high variance hinder their application to
long-horizon, complex tasks. We introduce Behavior Best-of-N (bBoN), a method
that scales over agents by generating multiple rollouts and selecting among
them using behavior narratives that describe the agents' rollouts. It enables
both wide exploration and principled trajectory selection, substantially
improving robustness and success rates. On OSWorld, our bBoN scaling method
establishes a new state of the art (SoTA) at 69.9%, significantly outperforming
prior methods and approaching human-level performance at 72%, with
comprehensive ablations validating key design choices. We further demonstrate
strong generalization results to different operating systems on
WindowsAgentArena and AndroidWorld. Crucially, our results highlight the
unreasonable effectiveness of scaling CUAs, when you do it right: effective
scaling requires structured trajectory understanding and selection, and bBoN
provides a practical framework to achieve this.

</details>


### [83] [RLAD: Training LLMs to Discover Abstractions for Solving Reasoning Problems](https://arxiv.org/abs/2510.02263)
*Yuxiao Qu,Anikait Singh,Yoonho Lee,Amrith Setlur,Ruslan Salakhutdinov,Chelsea Finn,Aviral Kumar*

Main category: cs.AI

TL;DR: 提出RLAD方法，通过推理抽象来引导模型进行更有效的推理，采用两玩家强化学习训练抽象生成器和解决方案生成器


<details>
  <summary>Details</summary>
Motivation: 解决当前大模型推理过程中缺乏程序性知识重用、容易陷入冗长和退化探索的问题

Method: 引入推理抽象概念，训练模型生成多个抽象描述，然后使用强化学习激励基于这些抽象构建解决方案，形成两玩家RL训练范式

Result: RLAD方法实现了结构化探索，解耦了抽象提议和解决方案生成的学习信号，并提高了对更难问题的泛化能力

Conclusion: 推理抽象能有效引导有意义的探索，在测试时分配更多计算资源生成抽象比生成更多解决方案更有利于性能提升

Abstract: Reasoning requires going beyond pattern matching or memorization of solutions
to identify and implement "algorithmic procedures" that can be used to deduce
answers to hard problems. Doing so requires realizing the most relevant
primitives, intermediate results, or shared procedures, and building upon them.
While RL post-training on long chains of thought ultimately aims to uncover
this kind of algorithmic behavior, most reasoning traces learned by large
models fail to consistently capture or reuse procedures, instead drifting into
verbose and degenerate exploration. To address more effective reasoning, we
introduce reasoning abstractions: concise natural language descriptions of
procedural and factual knowledge that guide the model toward learning
successful reasoning. We train models to be capable of proposing multiple
abstractions given a problem, followed by RL that incentivizes building a
solution while using the information provided by these abstractions. This
results in a two-player RL training paradigm, abbreviated as RLAD, that jointly
trains an abstraction generator and a solution generator. This setup
effectively enables structured exploration, decouples learning signals of
abstraction proposal and solution generation, and improves generalization to
harder problems. We also show that allocating more test-time compute to
generating abstractions is more beneficial for performance than generating more
solutions at large test budgets, illustrating the role of abstractions in
guiding meaningful exploration.

</details>


### [84] [BioX-Bridge: Model Bridging for Unsupervised Cross-Modal Knowledge Transfer across Biosignals](https://arxiv.org/abs/2510.02276)
*Chenqi Li,Yu Liu,Timothy Denison,Tingting Zhu*

Main category: cs.AI

TL;DR: BioX-Bridge：一种用于生物信号跨模态知识迁移的无监督框架，通过轻量级桥接网络对齐基础模型的中间表示，大幅减少可训练参数（88-99%）的同时保持或提升迁移性能。


<details>
  <summary>Details</summary>
Motivation: 生物信号模态间存在相关性，但缺乏大规模标注数据限制了特定模态模型的训练。现有知识蒸馏方法计算开销大，特别是对于大型基础模型。

Method: 训练轻量级桥接网络来对齐基础模型的中间表示，实现跨模态信息流动。包含高效的桥接位置选择策略和灵活的桥接网络架构。

Result: 在多个生物信号模态、任务和数据集上的实验表明，BioX-Bridge将可训练参数减少88-99%，同时保持或优于现有方法的迁移性能。

Conclusion: BioX-Bridge为生物信号的无监督跨模态知识迁移提供了一种高效解决方案，显著降低了计算和内存开销。

Abstract: Biosignals offer valuable insights into the physiological states of the human
body. Although biosignal modalities differ in functionality, signal fidelity,
sensor comfort, and cost, they are often intercorrelated, reflecting the
holistic and interconnected nature of human physiology. This opens up the
possibility of performing the same tasks using alternative biosignal
modalities, thereby improving the accessibility, usability, and adaptability of
health monitoring systems. However, the limited availability of large labeled
datasets presents challenges for training models tailored to specific tasks and
modalities of interest. Unsupervised cross-modal knowledge transfer offers a
promising solution by leveraging knowledge from an existing modality to support
model training for a new modality. Existing methods are typically based on
knowledge distillation, which requires running a teacher model alongside
student model training, resulting in high computational and memory overhead.
This challenge is further exacerbated by the recent development of foundation
models that demonstrate superior performance and generalization across tasks at
the cost of large model sizes. To this end, we explore a new framework for
unsupervised cross-modal knowledge transfer of biosignals by training a
lightweight bridge network to align the intermediate representations and enable
information flow between foundation models and across modalities. Specifically,
we introduce an efficient strategy for selecting alignment positions where the
bridge should be constructed, along with a flexible prototype network as the
bridge architecture. Extensive experiments across multiple biosignal
modalities, tasks, and datasets show that BioX-Bridge reduces the number of
trainable parameters by 88--99\% while maintaining or even improving transfer
performance compared to state-of-the-art methods.

</details>
