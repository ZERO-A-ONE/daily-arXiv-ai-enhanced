<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 6]
- [cs.CR](#cs.CR) [Total: 15]
- [cs.AI](#cs.AI) [Total: 41]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [A Comparative Study of Delta Parquet, Iceberg, and Hudi for Automotive Data Engineering Use Cases](https://arxiv.org/abs/2508.13396)
*Dinesh Eswararaj,Ajay Babu Nellipudi,Vandana Kollati*

Main category: cs.SE

TL;DR: 这篇论文对汽车行业中三种主流数据湖仓格式(Delta Parquet、Apache Iceberg、Apache Hudi)进行了实际性能比较分析，为汽车数据管理提供选型指南


<details>
  <summary>Details</summary>
Motivation: 汽车行业产生大量传感器、远程测量和实时数据，需要高效数据工程处理方案来应对延迟、扩展性和一致性挑战

Method: 使用真实世界的时间序列汽车远程测量数据，评估三种格式在建模策略、分区、CDC支持、查询性能、扩展性、数据一致性和生态系统成熟度方面的表现

Result: Delta Parquet在机器学习准备和数据管理方面表现优异，Iceberg在批处理分析和云原生工作负载中显示高性能，Hudi在实时数据吸收和增量处理方面最优，各格式在查询效率、时间旅行和更新语义方面有不同交易

Conclusion: 研究为汽车行业选择或组合使用不同数据湖仓格式提供了实用指南，支持车队管理、预测性维护和路线优化等应用场景，为扩展数据管道和集成机器学习模型提供了实践建议

Abstract: The automotive industry generates vast amounts of data from sensors,
telemetry, diagnostics, and real-time operations. Efficient data engineering is
critical to handle challenges of latency, scalability, and consistency. Modern
data lakehouse formats Delta Parquet, Apache Iceberg, and Apache Hudi offer
features such as ACID transactions, schema enforcement, and real-time
ingestion, combining the strengths of data lakes and warehouses to support
complex use cases. This study presents a comparative analysis of Delta Parquet,
Iceberg, and Hudi using real-world time-series automotive telemetry data with
fields such as vehicle ID, timestamp, location, and event metrics. The
evaluation considers modeling strategies, partitioning, CDC support, query
performance, scalability, data consistency, and ecosystem maturity. Key
findings show Delta Parquet provides strong ML readiness and governance,
Iceberg delivers high performance for batch analytics and cloud-native
workloads, while Hudi is optimized for real-time ingestion and incremental
processing. Each format exhibits tradeoffs in query efficiency, time-travel,
and update semantics. The study offers insights for selecting or combining
formats to support fleet management, predictive maintenance, and route
optimization. Using structured datasets and realistic queries, the results
provide practical guidance for scaling data pipelines and integrating machine
learning models in automotive applications.

</details>


### [2] [The Hidden Cost of Readability: How Code Formatting Silently Consumes Your LLM Budget](https://arxiv.org/abs/2508.13666)
*Dangfeng Pan,Zhensu Sun,Cenyuan Zhang,David Lo,Xiaoning Du*

Main category: cs.SE

TL;DR: 代码格式化元素（缩进、换行等）对LLMs并无实质帮助，去除后可减少24.5%输入token数量且保持性能。通过提示和微调还可进一步缩短输出代码长度。


<details>
  <summary>Details</summary>
Motivation: 代码格式化元素主要用于提高人类可读性，但对LLMs处理代码作为线性token序列时可能没有实质帮助，反而增加计算成本和响应时间。如果这些格式元素对LLMs非必需，可以通过移除来降低成本。

Method: 进行综合性实证研究，在Fill-in-the-Middle代码完成任务上测试四种编程语言（Java、Python、C++、C#）和十个LLMs（包括商业和开源模型）。系统分析移除格式元素后的token数量和性能变化。还探索了提示和微调对输出代码长度的影响。

Result: 关键发现显示LLMs在格式化代码和非格式化代码上都能维持相似性能，平均减少24.5%的输入token数量，输出token减少微不足道。通过提示和微调还可以进一步减少输出代码长度达36.1%，而不影响正确性。

Conclusion: 移除代码格式是一种实用的LLM效率优化策略。研究开发了双向代码转换工具，可无缝集成到现有LLM推理流程中，同时确保了人类可读性和LLM效率。

Abstract: Source code is usually formatted with elements like indentation and newlines
to improve readability for human developers. However, these visual aids do not
seem to be beneficial for large language models (LLMs) in the same way since
the code is processed as a linear sequence of tokens. Furthermore, these
additional tokens can lead to increased computational costs and longer response
times for LLMs. If such formatting elements are non-essential to LLMs, we can
reduce such costs by removing them from the code. To figure out the role played
by formatting elements, we conduct a comprehensive empirical study to evaluate
the impact of code formatting on LLM performance and efficiency. Through
large-scale experiments on Fill-in-the-Middle Code Completion tasks across four
programming languages (Java, Python, C++, C\#) and ten LLMs-including both
commercial and open-source models-we systematically analyze token count and
performance when formatting elements are removed. Key findings indicate that
LLMs can maintain performance across formatted code and unformatted code,
achieving an average input token reduction of 24.5\% with negligible output
token reductions. This makes code format removal a practical optimization
strategy for improving LLM efficiency. Further exploration reveals that both
prompting and fine-tuning LLMs can lead to significant reductions (up to
36.1\%) in output code length without compromising correctness. To facilitate
practical applications, we develop a bidirectional code transformation tool for
format processing, which can be seamlessly integrated into existing LLM
inference workflows, ensuring both human readability and LLM efficiency.

</details>


### [3] [COMPASS: A Multi-Dimensional Benchmark for Evaluating Code Generation in Large Language Models](https://arxiv.org/abs/2508.13757)
*James Meaden,Michał Jarosz,Piotr Jodłowski,Grigori Melnik*

Main category: cs.SE

TL;DR: COMPASS是一个多维代码生成评估框架，不仅评估功能正确性，还评估算法效率和代码质量，填补了现有基准测试的空白。


<details>
  <summary>Details</summary>
Motivation: 现有代码生成基准测试主要关注功能正确性，忽略了算法效率和代码质量这两个现实编程中的关键维度，无法全面评估代码生成模型的真实能力。

Method: 使用50个来自真实Codility竞赛的编程问题，建立包含393,150份人类提交的基准。采用行业标准分析工具系统评估运行时效率和代码质量。

Result: 对三个领先推理增强模型（Anthropic Claude Opus 4、Google Gemini 2.5 Pro、OpenAI O4-Mini-High）的评估显示，高正确性得分的模型不一定能产生高效算法或可维护代码。

Conclusion: 仅评估正确性不足以真正理解代码生成模型的现实能力，需要多维评估来推动AI系统向健壮、可靠、生产就绪的方向发展。

Abstract: Current code generation benchmarks focus primarily on functional correctness
while overlooking two critical aspects of real-world programming: algorithmic
efficiency and code quality. We introduce COMPASS (COdility's Multi-dimensional
Programming ASSessment), a comprehensive evaluation framework that assesses
code generation across three dimensions: correctness, efficiency, and quality.
COMPASS consists of 50 competitive programming problems from real Codility
competitions, providing authentic human baselines from 393,150 submissions.
Unlike existing benchmarks that treat algorithmically inefficient solutions
identically to optimal ones provided they pass test cases, COMPASS
systematically evaluates runtime efficiency and code quality using
industry-standard analysis tools. Our evaluation of three leading
reasoning-enhanced models, Anthropic Claude Opus 4, Google Gemini 2.5 Pro, and
OpenAI O4-Mini-High, reveals that models achieving high correctness scores do
not necessarily produce efficient algorithms or maintainable code. These
findings highlight the importance of evaluating more than just correctness to
truly understand the real-world capabilities of code generation models. COMPASS
serves as a guiding framework, charting a path for future research toward AI
systems that are robust, reliable, and ready for production use.

</details>


### [4] [Agentic DraCor and the Art of Docstring Engineering: Evaluating MCP-empowered LLM Usage of the DraCor API](https://arxiv.org/abs/2508.13774)
*Peer Trilcke,Ingo Börner,Henny Sluyter-Gäthje,Daniil Skorinkin,Frank Fischer,Carsten Milling*

Main category: cs.SE

TL;DR: 通过实现和评估一个为DraCor的MCP服务器，让LLM能够自主与DraCor API交互，并通过定性实验分析了LLM在使用MCP工具时的行为特点和性能指标。


<details>
  <summary>Details</summary>
Motivation: 为了让大语言模型能够自主与DraCor API进行交互，探索机器人智能在计算文学研究领域的应用潜力，同时为数字人文学基础设施的可靠性发展提供支撑。

Method: 实现了一个Model Context Protocol (MCP)服务器，进行了定性实验，重点关注LLM的工具选择和应用能力。通过系统观察prompt来理解LLM使用MCP工具时的行为，评估了"工具正确性"、"工具调用效率"和"工具使用可靠性"等指标。

Result: 实验结果显示了"文档字符串工程"（Docstring Engineering）的重要性，即通过反射性地精心制作工具文档来优化LLM-工具交互。这个方法能够显著提高LLM使用工具的准确性和效率。

Conclusion: 研究证明了机器人智能在计算文学研究中具有广阔应用前景，同时强调了可靠的数字人文学基础设施开发的重要性。文档字符串工程是优化LLM与工具交互的关键因素。

Abstract: This paper reports on the implementation and evaluation of a Model Context
Protocol (MCP) server for DraCor, enabling Large Language Models (LLM) to
autonomously interact with the DraCor API. We conducted experiments focusing on
tool selection and application by the LLM, employing a qualitative approach
that includes systematic observation of prompts to understand how LLMs behave
when using MCP tools, evaluating "Tool Correctness", "Tool-Calling Efficiency",
and "Tool-Use Reliability". Our findings highlight the importance of "Docstring
Engineering", defined as reflexively crafting tool documentation to optimize
LLM-tool interaction. Our experiments demonstrate both the promise of agentic
AI for research in Computational Literary Studies and the essential
infrastructure development needs for reliable Digital Humanities
infrastructures.

</details>


### [5] [Structural and Connectivity Patterns in the Maven Central Software Dependency Network](https://arxiv.org/abs/2508.13819)
*Daniel Ogenrwot,John Businge,Shaikh Arifuzzaman*

Main category: cs.SE

TL;DR: 通过网络科学技术分析Maven Central依赖图，发现其具有尺度自由的小世界拨扑结构，少数核心库支撑着整个生态系统，既提高了效率也带来系统性风险。


<details>
  <summary>Details</summary>
Motivation: 理解大规模软件生态系统的结构特征和连接模式，以提高软件重用、增强生态系统弹性和减少安全风险。

Method: 使用Goblin框架提取Maven Central依赖图样本，包含前5000个高连接度工程作为种子节点，通过广度优先搜索扩展获得130万节点和2090万边的图结构，计算度分布、中间度中心性、PageRank等图论指标。

Result: 发现Maven Central具有高度互联的尺度自由小世界拨扑结构，少数核心基础设施库（测试框架和通用工具库）支撑着大部分项目，这些关键节点的故障或漏洞可能导致全局性的激涉效应。

Conclusion: 核心基础设施库虽然提高了软件重用效率，但也带来了系统性风险，需要重视这些关键节点的弱点和弱点以保障生态系统的稳定性。

Abstract: Understanding the structural characteristics and connectivity patterns of
large-scale software ecosystems is critical for enhancing software reuse,
improving ecosystem resilience, and mitigating security risks. In this paper,
we investigate the Maven Central ecosystem, one of the largest repositories of
Java libraries, by applying network science techniques to its dependency graph.
Leveraging the Goblin framework, we extracted a sample consisting of the top
5,000 highly connected artifacts based on their degree centrality and then
performed breadth-first search (BFS) expansion from each selected artifact as a
seed node, traversing the graph outward to capture all libraries and releases
reachable those seed nodes. This sampling strategy captured the immediate
structural context surrounding these libraries resulted in a curated graph
comprising of 1.3 million nodes and 20.9 million edges. We conducted a
comprehensive analysis of this graph, computing degree distributions,
betweenness centrality, PageRank centrality, and connected components
graph-theoretic metrics. Our results reveal that Maven Central exhibits a
highly interconnected, scale-free, and small-world topology, characterized by a
small number of infrastructural hubs that support the majority of projects.
Further analysis using PageRank and betweenness centrality shows that these
hubs predominantly consist of core ecosystem infrastructure, including testing
frameworks and general-purpose utility libraries. While these hubs facilitate
efficient software reuse and integration, they also pose systemic risks;
failures or vulnerabilities affecting these critical nodes can have widespread
and cascading impacts throughout the ecosystem.

</details>


### [6] [Tight Inter-Core Cache Contention Analysis for WCET Estimation on Multicore Systems](https://arxiv.org/abs/2508.13863)
*Shuai Zhao,Jieyu Jiang,Shenlin Cai,Yaowei Liang,Chen Jie,Yinjie Fang,Wei Zhang,Guoquan Zhang,Yaoyao Gu,Xiang Xiao,Wei Qin,Xiangzhen Ouyang,Wanli Chang*

Main category: cs.SE

TL;DR: 这篇论文提出了一种新的多核架构上缓存争用分析方法，通过考虑实际缓存状态和访问次数，减少了WCET估计过高的问题。


<details>
  <summary>Details</summary>
Motivation: 多核架构上的WCET估计面临缓存共享带来的挑战，现有方法过于估计远程块访问导致的缓存失效数量，没有考虑实际缓存状态和访问次数。

Method: 基于任务中程序区域的顺序，首先识别可能受远程访问影响的内存引用，然后构建细粒度的争用分析来计算基于本地和远程块访问量的缓存失效数量，通过动态规划获取整个任务的核间缓存干扰。

Result: 实验结果显示，与现有方法相比，新方法平均减少63.31%的核间缓存干扰和8.94%的WCET估计，而计算开销没有显著增加。

Conclusion: 该研究提出的细粒度缓存争用分析方法能够更准确地估计多核环境下的WCET，有效减少估计过高的问题，对实时系统的可靠性分析具有重要意义。

Abstract: WCET (Worst-Case Execution Time) estimation on multicore architecture is
particularly challenging mainly due to the complex accesses over cache shared
by multiple cores. Existing analysis identifies possible contentions between
parallel tasks by leveraging the partial order of the tasks or their program
regions. Unfortunately, they overestimate the number of cache misses caused by
a remote block access without considering the actual cache state and the number
of accesses. This paper reports a new analysis for inter-core cache contention.
Based on the order of program regions in a task, we first identify memory
references that could be affected if a remote access occurs in a region.
Afterwards, a fine-grained contention analysis is constructed that computes the
number of cache misses based on the access quantity of local and remote blocks.
We demonstrate that the overall inter-core cache interference of a task can be
obtained via dynamic programming. Experiments show that compared to existing
methods, the proposed analysis reduces inter-core cache interference and WCET
estimations by 52.31% and 8.94% on average, without significantly increasing
computation overhead.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [7] [Too Easily Fooled? Prompt Injection Breaks LLMs on Frustratingly Simple Multiple-Choice Questions](https://arxiv.org/abs/2508.13214)
*Xuyang Guo,Zekai Huang,Zhao Song,Jiahao Zhang*

Main category: cs.CR

TL;DR: 大语言模型在PDF文件中突破隐藏提示注入攻击，在基本算术问题中容易被欺骗产生错误答案


<details>
  <summary>Details</summary>
Motivation: 识别LLM作为判断者应用在教育、同行审查等场景中的漏洞，而提示注入攻击对其稳健性构成了重大威胁

Method: 通过在PDF文件中注入隐藏的恶意提示，测试LLM在基本算术问题（如"3+2"）、多选题和真伪判断任务中的异常行为

Result: 结果显示LLM在这些简单场景中也容易受到隐藏提示注入攻击的影响，产生错误答案

Conclusion: 该研究强调了LLM作为判断者应用的严重安全风险，呈现了一种简单但有效的攻击方式，命中了模型的安全漏洞

Abstract: Large Language Models (LLMs) have recently demonstrated strong emergent
abilities in complex reasoning and zero-shot generalization, showing
unprecedented potential for LLM-as-a-judge applications in education, peer
review, and data quality evaluation. However, their robustness under prompt
injection attacks, where malicious instructions are embedded into the content
to manipulate outputs, remains a significant concern. In this work, we explore
a frustratingly simple yet effective attack setting to test whether LLMs can be
easily misled. Specifically, we evaluate LLMs on basic arithmetic questions
(e.g., "What is 3 + 2?") presented as either multiple-choice or true-false
judgment problems within PDF files, where hidden prompts are injected into the
file. Our results reveal that LLMs are indeed vulnerable to such hidden prompt
injection attacks, even in these trivial scenarios, highlighting serious
robustness risks for LLM-as-a-judge applications.

</details>


### [8] [MCPSecBench: A Systematic Security Benchmark and Playground for Testing Model Context Protocols](https://arxiv.org/abs/2508.13220)
*Yixuan Yang,Daoyuan Wu,Yufan Chen*

Main category: cs.CR

TL;DR: 首个系统性的MCP安全分类法，提出MCPSecBench评测框架，识别了17种攻击类型，过超85%攻击成功突破主要平台


<details>
  <summary>Details</summary>
Motivation: 识别和分析Model Context Protocol (MCP)集成到LLM应用中带来的新安全风险和攻击面

Method: 建立了17种攻击类型的系统分类法，开发MCPSecBench评测框架，包含提示数据集、MCP服务器、客户端和攻击脚本

Result: 85%以上的识别攻击成功突破至少一个平台，核心漏洞普遍影响Claude、OpenAI和Cursor，提示基于和工具聚焦攻击在不同平台上存在显著差异

Conclusion: MCPSecBench标准化了MCP安全评估，支持在所有MCP层面进行严格测试，为研究人员提供了模块化可扩展的安全评估工具

Abstract: Large Language Models (LLMs) are increasingly integrated into real-world
applications via the Model Context Protocol (MCP), a universal, open standard
for connecting AI agents with data sources and external tools. While MCP
enhances the capabilities of LLM-based agents, it also introduces new security
risks and expands their attack surfaces. In this paper, we present the first
systematic taxonomy of MCP security, identifying 17 attack types across 4
primary attack surfaces. We introduce MCPSecBench, a comprehensive security
benchmark and playground that integrates prompt datasets, MCP servers, MCP
clients, and attack scripts to evaluate these attacks across three major MCP
providers. Our benchmark is modular and extensible, allowing researchers to
incorporate custom implementations of clients, servers, and transport protocols
for systematic security assessment. Experimental results show that over 85% of
the identified attacks successfully compromise at least one platform, with core
vulnerabilities universally affecting Claude, OpenAI, and Cursor, while
prompt-based and tool-centric attacks exhibit considerable variability across
different hosts and models. Overall, MCPSecBench standardizes the evaluation of
MCP security and enables rigorous testing across all MCP layers.

</details>


### [9] [Quantifying Loss Aversion in Cyber Adversaries via LLM Analysis](https://arxiv.org/abs/2508.13240)
*Soham Hans,Nikolos Gurney,Stacy Marsella,Sofia Hirschmann*

Main category: cs.CR

TL;DR: 利用大语言模型分析黑客行为中的损失厌恶认知偏差，为网络防御提供实时行为分析新方法


<details>
  <summary>Details</summary>
Motivation: 传统网络防御策略主要关注加固防护，但缺乏动态解释攻击过程中攻击者认知特征的能力。IARPA的ReSCIND项目旨在推断、防御甚至利用攻击者的认知特征，填补了这一空白

Method: 通过实验收集黑客攻击受控演示网络的行为数据，使用大语言模型处理黑客生成的笔记，分割各种行动并将其与预定义的黑客持久化机制相关联，分析这些机制实施与操作触发因素的关系

Result: 分析揭示了损失厌恶在黑客决策中的表现方式，证明大语言模型能够有效剖析和解释细微的行为模式

Conclusion: 该方法为通过实时行为分析增强网络防御策略提供了变革性途径，大语言模型在量化认知偏差方面展现出强大潜力

Abstract: Understanding and quantifying human cognitive biases from empirical data has
long posed a formidable challenge, particularly in cybersecurity, where
defending against unknown adversaries is paramount. Traditional cyber defense
strategies have largely focused on fortification, while some approaches attempt
to anticipate attacker strategies by mapping them to cognitive vulnerabilities,
yet they fall short in dynamically interpreting attacks in progress. In
recognition of this gap, IARPA's ReSCIND program seeks to infer, defend
against, and even exploit attacker cognitive traits. In this paper, we present
a novel methodology that leverages large language models (LLMs) to extract
quantifiable insights into the cognitive bias of loss aversion from hacker
behavior. Our data are collected from an experiment in which hackers were
recruited to attack a controlled demonstration network. We process the hacker
generated notes using LLMs using it to segment the various actions and
correlate the actions to predefined persistence mechanisms used by hackers. By
correlating the implementation of these mechanisms with various operational
triggers, our analysis provides new insights into how loss aversion manifests
in hacker decision-making. The results demonstrate that LLMs can effectively
dissect and interpret nuanced behavioral patterns, thereby offering a
transformative approach to enhancing cyber defense strategies through
real-time, behavior-based analysis.

</details>


### [10] [Involuntary Jailbreak](https://arxiv.org/abs/2508.13246)
*Yangyang Guo,Yangyan Li,Mohan Kankanhalli*

Main category: cs.CR

TL;DR: 研究发现LLMs存在一种新型漏洞"非自愿越狱"，仅需单一通用提示就能绕过主流LLMs的安全防护机制。


<details>
  <summary>Details</summary>
Motivation: 现有越狱攻击主要针对LLM防护机制的局部组件，而本研究旨在揭示整个防护结构的脆弱性。

Method: 使用单一通用提示，要求LLMs生成通常会被拒绝的问题及其深度回答（而非拒绝回答）。

Result: 这种简单提示策略能持续绕过大多数主流LLMs，包括Claude Opus 4.1、Grok 4、Gemini 2.5 Pro和GPT 4.1。

Conclusion: 该问题需要研究者和从业者重新评估LLM防护机制的鲁棒性，以促进未来更强的安全对齐。

Abstract: In this study, we disclose a worrying new vulnerability in Large Language
Models (LLMs), which we term \textbf{involuntary jailbreak}. Unlike existing
jailbreak attacks, this weakness is distinct in that it does not involve a
specific attack objective, such as generating instructions for \textit{building
a bomb}. Prior attack methods predominantly target localized components of the
LLM guardrail. In contrast, involuntary jailbreaks may potentially compromise
the entire guardrail structure, which our method reveals to be surprisingly
fragile. We merely employ a single universal prompt to achieve this goal. In
particular, we instruct LLMs to generate several questions that would typically
be rejected, along with their corresponding in-depth responses (rather than a
refusal). Remarkably, this simple prompt strategy consistently jailbreaks the
majority of leading LLMs, including Claude Opus 4.1, Grok 4, Gemini 2.5 Pro,
and GPT 4.1. We hope this problem can motivate researchers and practitioners to
re-evaluate the robustness of LLM guardrails and contribute to stronger safety
alignment in future.

</details>


### [11] [Silentflow: Leveraging Trusted Execution for Resource-Limited MPC via Hardware-Algorithm Co-design](https://arxiv.org/abs/2508.13357)
*Zhuoran Li,Hanieh Totonchi Asl,Ebrahim Nouri,Yifei Cai,Danella Zhao*

Main category: cs.CR

TL;DR: SilentFlow是一个基于TEE的高效安全多方计算协议，通过消除COT生成中的通信开销，在资源受限设备上实现实时安全推理


<details>
  <summary>Details</summary>
Motivation: 传统COT生成在资源受限设备（如IoT传感器）上成为性能瓶颈，限制了安全多方计算在边缘设备上的实时应用

Method: 使用TEE辅助协议消除COT生成的通信，采用结构化算法分解：内核融合并行化、BOX优化内存访问模式、向量化批处理最大化内存带宽

Result: 相比最先进协议实现39.51倍加速，在ImageNet数据集上比Cryptflow2和Cheetah分别快4.62倍和3.95倍

Conclusion: SilentFlow有效解决了资源受限设备上安全推理的性能瓶颈，为边缘计算中的隐私保护机器学习提供了实用解决方案

Abstract: Secure Multi-Party Computation (MPC) offers a practical foundation for
privacy-preserving machine learning at the edge, with MPC commonly employed to
support nonlinear operations. These MPC protocols fundamentally rely on
Oblivious Transfer (OT), particularly Correlated OT (COT), to generate
correlated randomness essential for secure computation. Although COT generation
is efficient in conventional two-party settings with resource-rich
participants, it becomes a critical bottleneck in real-world inference on
resource-constrained devices (e.g., IoT sensors and wearables), due to both
communication latency and limited computational capacity. To enable real-time
secure inference, we introduce Silentflow, a highly efficient Trusted Execution
Environment (TEE)-assisted protocol that eliminates communication in COT
generation. We tackle the core performance bottleneck-low computational
intensity-through structured algorithmic decomposition: kernel fusion for
parallelism, Blocked On-chip eXpansion (BOX) to improve memory access patterns,
and vectorized batch operations to maximize memory bandwidth utilization.
Through design space exploration, we balance end-to-end latency and resource
demands, achieving up to 39.51x speedup over state-of-the-art protocols. By
offloading COT computations to a Zynq-7000 SoC, SilentFlow accelerates PPMLaaS
inference on the ImageNet dataset under resource constraints, achieving a 4.62x
and 3.95x speedup over Cryptflow2 and Cheetah, respectively.

</details>


### [12] [A Risk Manager for Intrusion Tolerant Systems: Enhancing HAL 9000 with New Scoring and Data Sources](https://arxiv.org/abs/2508.13364)
*Tadeu Freitas,Carlos Novo,Inês Dutra,João Soares,Manuel Correia,Benham Shariati,Rolando Martins*

Main category: cs.CR

TL;DR: 本文扩展了HAL 9000入侵容忍系统风险管理器，通过构建自定义爬虫从多样化威胁源获取情报，显著提升了对未经验证漏洞的早期检测和评估能力。


<details>
  <summary>Details</summary>
Motivation: 现有入侵容忍系统过度依赖NVD和ExploitDB等手动分析数据库，响应速度受限，无法有效应对快速演变的威胁。HAL 9000虽然通过机器学习改进了漏洞预测，但仍受限于有限的情报来源。

Method: 开发自定义爬虫持续挖掘多样化威胁源（安全公告、研究论坛、实时漏洞利用POC），将获取的情报集成到HAL 9000风险管理框架中，扩展其情报基础。

Result: 评估表明，集成爬虫获取的情报显著提升了HAL 9000应对新兴威胁的能力，实现了更早的漏洞检测和评估。

Conclusion: 通过整合多样化威胁情报源，HAL 9000的风险管理能力得到实质性改进，为入侵容忍系统提供了更及时有效的威胁响应机制。

Abstract: Intrusion Tolerant Systems (ITSs) have become increasingly critical due to
the rise of multi-domain adversaries exploiting diverse attack surfaces. ITS
architectures aim to tolerate intrusions, ensuring system compromise is
prevented or mitigated even with adversary presence. Existing ITS solutions
often employ Risk Managers leveraging public security intelligence to adjust
system defenses dynamically against emerging threats. However, these approaches
rely heavily on databases like NVD and ExploitDB, which require manual analysis
for newly discovered vulnerabilities. This dependency limits the system's
responsiveness to rapidly evolving threats. HAL 9000, an ITS Risk Manager
introduced in our prior work, addressed these challenges through machine
learning. By analyzing descriptions of known vulnerabilities, HAL 9000 predicts
and assesses new vulnerabilities automatically. To calculate the risk of a
system, it also incorporates the Exploitability Probability Scoring system to
estimate the likelihood of exploitation within 30 days, enhancing proactive
defense capabilities.
  Despite its success, HAL 9000's reliance on NVD and ExploitDB knowledge is a
limitation, considering the availability of other sources of information. This
extended work introduces a custom-built scraper that continuously mines diverse
threat sources, including security advisories, research forums, and real-time
exploit proofs-of-concept. This significantly expands HAL 9000's intelligence
base, enabling earlier detection and assessment of unverified vulnerabilities.
Our evaluation demonstrates that integrating scraper-derived intelligence with
HAL 9000's risk management framework substantially improves its ability to
address emerging threats. This paper details the scraper's integration into the
architecture, its role in providing additional information on new threats, and
the effects on HAL 9000's management.

</details>


### [13] [When Secure Aggregation Falls Short: Achieving Long-Term Privacy in Asynchronous Federated Learning for LEO Satellite Networks](https://arxiv.org/abs/2508.13425)
*Mohamed Elmahallawy,Tie Luo*

Main category: cs.CR

TL;DR: LTP-FLEO是一个针对低地球轨道卫星网络的异步联邦学习框架，解决了传统安全聚合方法在卫星网络中的两个主要不足：间歇性连接和多轮隐私泄露问题。


<details>
  <summary>Details</summary>
Motivation: 传统联邦学习安全聚合方法假设客户端持续可用，但在LEO卫星网络中卫星可见性是间歇性和不规则的，且现有方法只关注单轮隐私保护，忽视了多轮训练中的隐私泄露风险。

Method: 提出LTP-FLEO框架，包含三个关键技术：基于可预测可见性的隐私感知卫星分区、模型年龄平衡机制以缓解陈旧模型更新的负面影响、以及公平的全局聚合处理不同可见时长的卫星。

Result: 理论分析和实证验证表明，LTP-FLEO能有效保护多轮训练中的模型和数据隐私，促进与卫星贡献相匹配的公平性，加速全局收敛，并达到有竞争力的模型精度。

Conclusion: LTP-FLEO成功解决了LEO卫星网络中联邦学习的长期隐私保护问题，为动态资源受限环境下的隐私保护联邦学习提供了有效解决方案。

Abstract: Secure aggregation is a common technique in federated learning (FL) for
protecting data privacy from both curious internal entities (clients or server)
and external adversaries (eavesdroppers). However, in dynamic and
resource-constrained environments such as low Earth orbit (LEO) satellite
networks, traditional secure aggregation methods fall short in two aspects: (1)
they assume continuous client availability while LEO satellite visibility is
intermittent and irregular; (2) they consider privacy in each communication
round but have overlooked the possible privacy leakage through multiple rounds.
To address these limitations, we propose LTP-FLEO, an asynchronous FL framework
that preserves long-term privacy (LTP) for LEO satellite networks. LTP-FLEO
introduces (i) privacy-aware satellite partitioning, which groups satellites
based on their predictable visibility to the server and enforces joint
participation; (ii) model age balancing, which mitigates the adverse impact of
stale model updates; and (iii) fair global aggregation, which treats satellites
of different visibility durations in an equitable manner. Theoretical analysis
and empirical validation demonstrate that LTP-FLEO effectively safeguards both
model and data privacy across multi-round training, promotes fairness in line
with satellite contributions, accelerates global convergence, and achieves
competitive model accuracy.

</details>


### [14] [Beneath the Mask: Can Contribution Data Unveil Malicious Personas in Open-Source Projects?](https://arxiv.org/abs/2508.13453)
*Ruby Nealon*

Main category: cs.CR

TL;DR: 通过分析GitHub贡献数据和图论方法，识别开源项目中的异常行为，应对XZ Utils后门事件


<details>
  <summary>Details</summary>
Motivation: XZ Utils后门事件显示开源项目中缺乏监控异常行为的工具，需要开发新方法来识别像"JiaT75"这样的恶意贡献者

Method: 使用开源情报(OSINT)收集GitHub贡献数据，采用图数据库和图论分析技术来识别异常行为

Result: 能够高效地识别"JiaT75"身份在其他开源项目中展现的异常行为模式

Conclusion: 图论分析结合OSINT数据是监测开源项目安全风险的有效方法，可以及早发现潜在的恶意贡献者

Abstract: In February 2024, after building trust over two years with project
maintainers by making a significant volume of legitimate contributions, GitHub
user "JiaT75" self-merged a version of the XZ Utils project containing a highly
sophisticated, well-disguised backdoor targeting sshd processes running on
systems with the backdoored package installed. A month later, this package
began to be distributed with popular Linux distributions until a Microsoft
employee discovered the backdoor while investigating how a recent system
upgrade impacted the performance of SSH authentication. Despite its potential
global impact, no tooling exists for monitoring and identifying anomalous
behavior by personas contributing to other open-source projects. This paper
demonstrates how Open Source Intelligence (OSINT) data gathered from GitHub
contributions, analyzed using graph databases and graph theory, can efficiently
identify anomalous behaviors exhibited by the "JiaT75" persona across other
open-source projects.

</details>


### [15] [Optimizing Scalar Selection in Elliptic Curve Cryptography Using Differential Evolution for Enhanced Security](https://arxiv.org/abs/2508.13520)
*Takreem Haider*

Main category: cs.CR

TL;DR: 提出使用差分进化算法优化椭圆曲线密码学中的标量生成，通过最大化比特级熵来提高安全性和抗攻击能力


<details>
  <summary>Details</summary>
Motivation: 在资源受限环境中，传统标量生成方法可能产生低熵或偏置的标量，容易受到侧信道和密钥恢复攻击

Method: 使用差分进化(DE)算法，将标量选择重新表述为熵优化问题，寻找具有最大熵的二进制表示

Result: 实验结果表明DE优化的标量熵值显著高于传统方法生成的标量

Conclusion: 该方法为ECC协议提供了确定性的可调替代方案，特别适用于区块链、安全消息、物联网等资源受限环境

Abstract: Elliptic Curve Cryptography (ECC) is a fundamental component of modern
public-key cryptosystems that enable efficient and secure digital signatures,
key exchanges, and encryption. Its core operation, scalar multiplication,
denoted as $k \cdot P$, where $P$ is a base point and $k$ is a private scalar,
relies heavily on the secrecy and unpredictability of $k$. Conventionally, $k$
is selected using user input or pseudorandom number generators. However, in
resource-constrained environments with weak entropy sources, these approaches
may yield low-entropy or biased scalars, increasing susceptibility to
side-channel and key recovery attacks. To mitigate these vulnerabilities, we
introduce an optimization-driven scalar generation method that explicitly
maximizes bit-level entropy. Our approach uses differential evolution (DE), a
population-based metaheuristic algorithm, to search for scalars whose binary
representations exhibit maximal entropy, defined by an even and statistically
uniform distribution of ones and zeros. This reformulation of scalar selection
as an entropy-optimization problem enhances resistance to entropy-based
cryptanalytic techniques and improves overall unpredictability. Experimental
results demonstrate that DE-optimized scalars achieve entropy significantly
higher than conventionally generated scalars. The proposed method can be
integrated into existing ECC-based protocols, offering a deterministic, tunable
alternative to traditional randomness, ideal for applications in blockchain,
secure messaging, IoT, and other resource-constrained environments.

</details>


### [16] [CAI Fluency: A Framework for Cybersecurity AI Fluency](https://arxiv.org/abs/2508.13588)
*Víctor Mayoral-Vilches,Jasmin Wachter,Cristóbal R. J. Veas Chavez,Cathrin Schachner,Luis Javier Navarrete-Lozano,María Sanz-Gómez*

Main category: cs.CR

TL;DR: CAI Fluency是一个基于CAI框架的教育平台，旨在普及网络安全AI工具的知识和应用，通过三种人机交互模式和四项核心能力培养从业者的技术技能和批判性思维。


<details>
  <summary>Details</summary>
Motivation: 促进人工智能网络安全解决方案的广泛采用和有效使用，实现网络安全领域的"vibe-hacking"（类似编程中的vibe-coding概念）。

Method: 基于AI Fluency框架，专门针对网络安全应用调整了三种人机交互模式和四项核心能力，提供理论基础和实践指导。

Result: 开发了一个全面的教育平台，包含白皮书、详细的教育和实践指南，帮助用户理解CAI框架原理并在实际安全场景中应用。

Conclusion: CAI Fluency成功构建了一个专门针对网络安全AI教育的框架，不仅培养技术能力，还强调批判性思维和伦理意识，为负责任地使用AI安全工具奠定了基础。

Abstract: This work introduces CAI Fluency, an an educational platform of the
Cybersecurity AI (CAI) framework dedicated to democratizing the knowledge and
application of cybersecurity AI tools in the global security community. The
main objective of the CAI framework is to accelerate the widespread adoption
and effective use of artificial intelligence-based cybersecurity solutions,
pathing the way to vibe-hacking, the cybersecurity analogon to vibe-coding.
  CAI Fluency builds upon the Framework for AI Fluency, adapting its three
modalities of human-AI interaction and four core competencies specifically for
cybersecurity applications. This theoretical foundation ensures that
practitioners develop not just technical skills, but also the critical thinking
and ethical awareness necessary for responsible AI use in security contexts.
  This technical report serves as a white-paper, as well as detailed
educational and practical guide that helps users understand the principles
behind the CAI framework, and educates them how to apply this knowledge in
their projects and real-world security contexts.

</details>


### [17] [Conflicting Scores, Confusing Signals: An Empirical Study of Vulnerability Scoring Systems](https://arxiv.org/abs/2508.13644)
*Viktoria Koscinski,Mark Nelson,Ahmet Okutan,Robert Falso,Mehdi Mirakhorli*

Main category: cs.CR

TL;DR: 首次大规模实证比较四种漏洞评分系统(CVSS、SSVC、EPSS、Exploitability Index)，使用微软600个真实漏洞数据，发现评分系统间存在显著差异，影响基于风险的漏洞管理决策。


<details>
  <summary>Details</summary>
Motivation: 现有漏洞评分系统的目标、方法和输出不一致，导致漏洞优先级决策不一致，需要实证比较来评估其实际效果。

Method: 使用微软Patch Tuesday披露的600个真实漏洞数据，分析四种评分系统的关系、漏洞管理支持能力、分级分类效果和实际利用风险捕捉能力。

Result: 发现不同评分系统对相同漏洞的排名存在显著差异，影响组织基于数据驱动的风险决策。

Conclusion: 需要更透明和一致的漏洞可利用性、风险和严重性评估方法，评分系统间的一致性问题亟待解决。

Abstract: Accurately assessing software vulnerabilities is essential for effective
prioritization and remediation. While various scoring systems exist to support
this task, their differing goals, methodologies and outputs often lead to
inconsistent prioritization decisions. This work provides the first
large-scale, outcome-linked empirical comparison of four publicly available
vulnerability scoring systems: the Common Vulnerability Scoring System (CVSS),
the Stakeholder-Specific Vulnerability Categorization (SSVC), the Exploit
Prediction Scoring System (EPSS), and the Exploitability Index. We use a
dataset of 600 real-world vulnerabilities derived from four months of
Microsoft's Patch Tuesday disclosures to investigate the relationships between
these scores, evaluate how they support vulnerability management task, how
these scores categorize vulnerabilities across triage tiers, and assess their
ability to capture the real-world exploitation risk. Our findings reveal
significant disparities in how scoring systems rank the same vulnerabilities,
with implications for organizations relying on these metrics to make
data-driven, risk-based decisions. We provide insights into the alignment and
divergence of these systems, highlighting the need for more transparent and
consistent exploitability, risk, and severity assessments.

</details>


### [18] [Know Me by My Pulse: Toward Practical Continuous Authentication on Wearable Devices via Wrist-Worn PPG](https://arxiv.org/abs/2508.13690)
*Wei Shao,Zequan Liang,Ruoyu Zhang,Ruijie Fang,Ning Miao,Ehsan Kourkchi,Setareh Rafatirad,Houman Homayoun,Chongzhou Fang*

Main category: cs.CR

TL;DR: 基于25Hz低频多通道PPG信号的智能手表连续身份验证系统，通过Bi-LSTM注意力机制实现高准确度认证，但低频采样减少了53%传感器耗电


<details>
  <summary>Details</summary>
Motivation: 解决ECG身份验证侵入性感知和间断性问题，以及高频PPG方案的高能耗和计算复杂度问题，适合手表等功能限制可穿戴设备

Method: 采用25Hz低频多通道PPG信号，使用具有注意力机制的双向LSTM模型，从4秒短时间窗口提取特征，在公开数据集和自建数据集上进行评估

Result: 平均测试准确率88.11%，F1分0.88，FAR 0.48%，FRR 11.77%，EER 2.76%，低频采样比512Hz节能53%，比128Hz节能19%，确认25Hz为最佳低频界限

Conclusion: 25Hz低频PPG能在保持高认证性能的同时显著降低能耗，为可穿戴设备提供了实用的连续身份验证方案，还需活动多样性训练提升潜动状态的稳健性

Abstract: Biometric authentication using physiological signals offers a promising path
toward secure and user-friendly access control in wearable devices. While
electrocardiogram (ECG) signals have shown high discriminability, their
intrusive sensing requirements and discontinuous acquisition limit
practicality. Photoplethysmography (PPG), on the other hand, enables
continuous, non-intrusive authentication with seamless integration into
wrist-worn wearable devices. However, most prior work relies on high-frequency
PPG (e.g., 75 - 500 Hz) and complex deep models, which incur significant energy
and computational overhead, impeding deployment in power-constrained real-world
systems. In this paper, we present the first real-world implementation and
evaluation of a continuous authentication system on a smartwatch, We-Be Band,
using low-frequency (25 Hz) multi-channel PPG signals. Our method employs a
Bi-LSTM with attention mechanism to extract identity-specific features from
short (4 s) windows of 4-channel PPG. Through extensive evaluations on both
public datasets (PTTPPG) and our We-Be Dataset (26 subjects), we demonstrate
strong classification performance with an average test accuracy of 88.11%,
macro F1-score of 0.88, False Acceptance Rate (FAR) of 0.48%, False Rejection
Rate (FRR) of 11.77%, and Equal Error Rate (EER) of 2.76%. Our 25 Hz system
reduces sensor power consumption by 53% compared to 512 Hz and 19% compared to
128 Hz setups without compromising performance. We find that sampling at 25 Hz
preserves authentication accuracy, whereas performance drops sharply at 20 Hz
while offering only trivial additional power savings, underscoring 25 Hz as the
practical lower bound. Additionally, we find that models trained exclusively on
resting data fail under motion, while activity-diverse training improves
robustness across physiological states.

</details>


### [19] [On the Security and Privacy of Federated Learning: A Survey with Attacks, Defenses, Frameworks, Applications, and Future Directions](https://arxiv.org/abs/2508.13730)
*Daniel M. Jimenez-Gutierrez,Yelizaveta Falkouskaya,Jose L. Hernandez-Ramos,Aris Anagnostopoulos,Ioannis Chatzigiannakis,Andrea Vitaletti*

Main category: cs.CR

TL;DR: 这篇调研论文综述了联邦学习领域的安全性和隐私保护攻击与防御技术，分析了200多篇文献，并评估了各种方法的优缺点和应用挑战。


<details>
  <summary>Details</summary>
Motivation: 联邦学习虽然在设计上保护数据隐私，但仍面临着多种安全性和隐私漏洞的威胁，需要系统性的评估和解决方案。

Method: 调研论文采用系统性的分类方法，将相关技术分为安全性增强和隐私保护两大类。安全性方法涉及对投毒攻击、希拉攻击等恶意行为的防御；隐私保护方法包括加密技术、差分隐私和安全聚合等。

Result: 评估了现有方法的优缺点，分析了隐私、安全性和模型性能之间的权衡关系，讨论了非IID数据分布对防御效果的影响，并指出了当前研究的空白和挑战。

Conclusion: 这份调研提供了联邦学习安全性和隐私保护领域的全面概览，为研究人员和实践者开发更加健壮和可靠的联邦学习系统提供了指南，促进协作学习框架整体性和保密性的发展。

Abstract: Federated Learning (FL) is an emerging distributed machine learning paradigm
enabling multiple clients to train a global model collaboratively without
sharing their raw data. While FL enhances data privacy by design, it remains
vulnerable to various security and privacy threats. This survey provides a
comprehensive overview of more than 200 papers regarding the state-of-the-art
attacks and defense mechanisms developed to address these challenges,
categorizing them into security-enhancing and privacy-preserving techniques.
Security-enhancing methods aim to improve FL robustness against malicious
behaviors such as byzantine attacks, poisoning, and Sybil attacks. At the same
time, privacy-preserving techniques focus on protecting sensitive data through
cryptographic approaches, differential privacy, and secure aggregation. We
critically analyze the strengths and limitations of existing methods, highlight
the trade-offs between privacy, security, and model performance, and discuss
the implications of non-IID data distributions on the effectiveness of these
defenses. Furthermore, we identify open research challenges and future
directions, including the need for scalable, adaptive, and energy-efficient
solutions operating in dynamic and heterogeneous FL environments. Our survey
aims to guide researchers and practitioners in developing robust and
privacy-preserving FL systems, fostering advancements safeguarding
collaborative learning frameworks' integrity and confidentiality.

</details>


### [20] [NodeShield: Runtime Enforcement of Security-Enhanced SBOMs for Node.js](https://arxiv.org/abs/2508.13750)
*Eric Cornelissen,Musard Balliu*

Main category: cs.CR

TL;DR: NodeShield是一个针对Node.js应用的运行时保护机制，通过SBOM和CBOM来强制执行依赖层次结构和系统资源访问控制，能够防止98%的已知供应链攻击，且开销极小。


<details>
  <summary>Details</summary>
Motivation: Node.js生态系统因其规模和普及性而成为恶意攻击的常见目标，需要一种兼容性好、自动化、开销小且策略简洁的运行时保护机制来应对供应链攻击。

Method: 设计并实现了NodeShield，利用SBOM标准作为依赖层次结构的真实来源，并提出了CBOM扩展来记录组件所需的能力。通过代码轮廓化（而非内联）在运行时强制执行SBOM和CBOM，无需修改原始代码或Node.js运行时。

Result: 评估显示NodeShield能够防止67个已知供应链攻击中的98%以上，服务器端每个请求的开销小于1ms，同时保持与原生Node.js的广泛兼容性。

Conclusion: NodeShield提供了一种有效的运行时保护机制，通过SBOM和CBOM的组合使用，能够以最小开销有效防御Node.js供应链攻击，同时保持策略简洁和兼容性。

Abstract: The software supply chain is an increasingly common attack vector for
malicious actors. The Node.js ecosystem has been subject to a wide array of
attacks, likely due to its size and prevalence. To counter such attacks, the
research community and practitioners have proposed a range of static and
dynamic mechanisms, including process- and language-level sandboxing,
permission systems, and taint tracking. Drawing on valuable insight from these
works, this paper studies a runtime protection mechanism for (the supply chain
of) Node.js applications with the ambitious goals of compatibility, automation,
minimal overhead, and policy conciseness.
  Specifically, we design, implement and evaluate NodeShield, a protection
mechanism for Node.js that enforces an application's dependency hierarchy and
controls access to system resources at runtime. We leverage the up-and-coming
SBOM standard as the source of truth for the dependency hierarchy of the
application, thus preventing components from stealthily abusing undeclared
components. We propose to enhance the SBOM with a notion of capabilities that
represents a set of related system resources a component may access. Our
proposed SBOM extension, the Capability Bill of Materials or CBOM, records the
required capabilities of each component, providing valuable insight into the
potential privileged behavior. NodeShield enforces the SBOM and CBOM at runtime
via code outlining (as opposed to inlining) with no modifications to the
original code or Node.js runtime, thus preventing unexpected, potentially
malicious behavior. Our evaluation shows that NodeShield can prevent over 98%
out of 67 known supply chain attacks while incurring minimal overhead on
servers at less than 1ms per request. We achieve this while maintaining broad
compatibility with vanilla Node.js and a concise policy language that consists
of at most 7 entries per dependency.

</details>


### [21] [Red Teaming Methodology for Design Obfuscation](https://arxiv.org/abs/2508.13965)
*Yuntao Liu,Abir Akib,Zelin Lu,Qian Xu,Ankur Srivastava,Gang Qu,David Kehlet,Nij Dorairaj*

Main category: cs.CR

TL;DR: 本文提出了一个系统化的红队方法来评估设计混淆方案的安全性，特别针对攻击者无法获得工作芯片的场景，并通过案例研究发现现有方法泄露的设计结构信息比通常认为的更多。


<details>
  <summary>Details</summary>
Motivation: 保护VLSI供应链中敏感设计细节免受不可信方（如离岸代工厂和不可信终端用户）的侵害，现有设计混淆方案的安全性需要系统化评估。

Method: 提出安全指标和评估方法学，特别针对攻击者无法获得工作芯片的场景，使用佛罗里达大学开发的RIPPER工具进行案例研究。

Result: 案例研究表明，现有设计混淆方法泄露的原始设计结构信息比通常认为的要多。

Conclusion: 需要更严格的安全评估方法来确保设计混淆方案的有效性，当前方法存在信息泄露风险。

Abstract: The main goal of design obfuscation schemes is to protect sensitive design
details from untrusted parties in the VLSI supply chain, including but not
limited to off-shore foundries and untrusted end users. In this work, we
provide a systematic red teaming approach to evaluate the security of design
obfuscation approaches. Specifically, we propose security metrics and
evaluation methodology for the scenarios where the adversary does not have
access to a working chip. A case study on the RIPPER tool developed by the
University of Florida indicates that more information is leaked about the
structure of the original design than commonly considered.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [22] [Chain-of-Agents: End-to-End Agent Foundation Models via Multi-Agent Distillation and Agentic RL](https://arxiv.org/abs/2508.13167)
*Weizhen Li,Jianbo Lin,Zhuosong Jiang,Jingyi Cao,Xinpeng Liu,Jiayu Zhang,Zhenqiang Huang,Qianben Chen,Weichen Sun,Qiexiang Wang,Hongxuan Lu,Tianrui Qin,Chenghao Zhu,Yi Yao,Shuying Fan,Xiaowan Li,Tiannan Wang,Pai Liu,King Zhu,He Zhu,Dingfeng Shi,Piaohong Wang,Yeyi Guan,Xiangru Tang,Minghao Liu,Yuchen Eleanor Jiang,Jian Yang,Jiaheng Liu,Ge Zhang,Wangchunshu Zhou*

Main category: cs.AI

TL;DR: 链式机器人框架(CoA)通过多机器萃取和机器人强化学习训练出端到端的机器人基础模型(AFM)，在多任务质量评估中达到最佳性能


<details>
  <summary>Details</summary>
Motivation: 现有多机器系统依赖手动提示工程和复杂框架，计算效率低且无法从数据中学习

Method: 提出链式机器人概念，通过多机器萃取获得监督精细调整数据，然后用机器人强化学习进一步提升性能

Result: AFM模型在web机器人和code机器人多个评测份上创造了新的最佳性能记录

Conclusion: 链式机器人框架提供了高效的端到端复杂问题解决方案，并开源了所有研究成果为未来研究提供基础

Abstract: Recent advances in large language models (LLMs) and multi-agent systems have
demonstrated remarkable capabilities in complex problem-solving tasks such as
deep research, vibe coding, and mathematical reasoning. However, most existing
multi-agent systems are built upon manual prompt/workflow engineering with
sophisticated agent frameworks, making them computationally inefficient, less
capable, and can not benefit from data-centric learning. In this work, we
introduce Chain-of-Agents (CoA), a novel paradigm of LLM reasoning that enables
native end-to-end complex problem-solving in the same way as a multi-agent
system (i.e., multi-turn problem solving with multiple tools and multiple
agents) within one model. In chain-of-agents problem-solving, the model
dynamically activates different tool agents and role-playing agents to simulate
multi-agent collaboration in an end-to-end fashion. To elicit end-to-end
chain-of-agents problem-solving abilities in LLMs, we introduce a multi-agent
distillation framework to distill state-of-the-art multi-agent systems into
chain-of-agents trajectories for agentic supervised fine-tuning. We then use
agentic reinforcement learning on verifiable agentic tasks to further improve
the models' capabilities on chain-of-agents problem solving. We call the
resulting models Agent Foundation Models (AFMs). Our empirical studies
demonstrate that AFM establishes new state-of-the-art performance across
diverse benchmarks in both web agent and code agent settings. We make the
entire research, including the model weights, code for training and evaluation,
and the training data, fully open-sourced, which offers a solid starting point
for future research on agent models and agentic RL.

</details>


### [23] [Cognitive Workspace: Active Memory Management for LLMs -- An Empirical Study of Functional Infinite Context](https://arxiv.org/abs/2508.13171)
*Tao An*

Main category: cs.AI

TL;DR: 让LLM模仿人类认知机制的主动式外部记忆管理方案，解决了传统RAG被动检索的局限性


<details>
  <summary>Details</summary>
Motivation: 虽然现有LLM能处理百万到十万token的长上下文，但仍缺乏人类主动管理外部记忆的认知能力，导致内存利用效率低下

Method: 基于Baddeley工作记忆模型、Clark扩展心智理论等认知科学理论，提出三核心创新：主动内存管理、层次认知缓冲区、任务驱动的上下文优化

Result: 平均内存重用率达到58.6%（传统RAG为0%），网络效率提升17-18%，统计显著性p<0.001，Cohen's d>23

Conclusion: 认知工作区代表了从信息检索到真正认知增强的基本转变，为LLM系统提供了量化的主动内存优势证据

Abstract: Large Language Models (LLMs) face fundamental limitations in context
management despite recent advances extending context windows to millions of
tokens. We propose Cognitive Workspace, a novel paradigm that transcends
traditional Retrieval-Augmented Generation (RAG) by emulating human cognitive
mechanisms of external memory use. Drawing from cognitive science foundations
including Baddeley's working memory model, Clark's extended mind thesis, and
Hutchins' distributed cognition framework, we demonstrate that current passive
retrieval systems fail to capture the dynamic, task-driven nature of human
memory management. Our analysis of 2024-2025 developments reveals that while
techniques like Infini-attention and StreamingLLM achieve impressive context
lengths, they lack the metacognitive awareness and active planning capabilities
essential for true cognitive extension. Cognitive Workspace addresses these
limitations through three core innovations: (1) active memory management with
deliberate information curation, (2) hierarchical cognitive buffers enabling
persistent working states, and (3) task-driven context optimization that
dynamically adapts to cognitive demands. Empirical validation demonstrates
Cognitive Workspace achieves an average 58.6% memory reuse rate (ranging from
54-60% across different tasks) compared to 0% for traditional RAG, with 17-18%
net efficiency gain despite 3.3x higher operation counts. Statistical analysis
confirms these advantages with p < 0.001 and Cohen's d > 23 across multiple
task types, establishing the first quantitative evidence for active memory
superiority in LLM systems. We present a comprehensive theoretical framework
synthesizing insights from 50+ recent papers, positioning Cognitive Workspace
as a fundamental shift from information retrieval to genuine cognitive
augmentation.

</details>


### [24] [AlphaEval: A Comprehensive and Efficient Evaluation Framework for Formula Alpha Mining](https://arxiv.org/abs/2508.13174)
*Hongjun Ding,Binqi Chen,Jinsheng Huang,Taian Guo,Zhengyang Mao,Guoyi Shao,Lutong Zou,Luchen Liu,Ming Zhang*

Main category: cs.AI

TL;DR: 提出AlphaEval框架，通过五维度评估自动化alpha挖掘模型，解决传统回测评估的问题


<details>
  <summary>Details</summary>
Motivation: 现有alpha挖掘模型评估存在回测计算量大、效率低且敏感性高，相关性指标只考虑预测能力而忽视其他重要特性，同时闭源模型影响可复现性

Method: 设计AlphaEval框架，从预测能力、稳定性、市场干扰鲁棒性、金融逻辑和多样性五个维度进行全面评估，支持并行化计算

Result: 实验表明AlphaEval评估一致性与全面回测相当，但提供更全面的洞察和更高效率，能有效识别优秀alpha

Conclusion: AlphaEval作为一个统一、并行化、无需回测的评估框架，有效解决了alpha挖掘预测模型的系统评估挐预，并开源代码促进领域发展

Abstract: Formula alpha mining, which generates predictive signals from financial data,
is critical for quantitative investment. Although various algorithmic
approaches-such as genetic programming, reinforcement learning, and large
language models-have significantly expanded the capacity for alpha discovery,
systematic evaluation remains a key challenge. Existing evaluation metrics
predominantly include backtesting and correlation-based measures. Backtesting
is computationally intensive, inherently sequential, and sensitive to specific
strategy parameters. Correlation-based metrics, though efficient, assess only
predictive ability and overlook other crucial properties such as temporal
stability, robustness, diversity, and interpretability. Additionally, the
closed-source nature of most existing alpha mining models hinders
reproducibility and slows progress in this field. To address these issues, we
propose AlphaEval, a unified, parallelizable, and backtest-free evaluation
framework for automated alpha mining models. AlphaEval assesses the overall
quality of generated alphas along five complementary dimensions: predictive
power, stability, robustness to market perturbations, financial logic, and
diversity. Extensive experiments across representative alpha mining algorithms
demonstrate that AlphaEval achieves evaluation consistency comparable to
comprehensive backtesting, while providing more comprehensive insights and
higher efficiency. Furthermore, AlphaEval effectively identifies superior
alphas compared to traditional single-metric screening approaches. All
implementations and evaluation tools are open-sourced to promote
reproducibility and community engagement.

</details>


### [25] [Fitting Ontologies and Constraints to Relational Structures](https://arxiv.org/abs/2508.13176)
*Simon Hosemann,Jean Christoph Jung,Carsten Lutz,Sebastian Rudolph*

Main category: cs.AI

TL;DR: 该论文研究了如何根据正负示例（有限关系结构）来拟合本体和约束，重点关注描述逻辑EL和ELI以及多种类型的元组生成依赖（TGD），分析了计算复杂性、算法设计以及拟合本体和TGD的大小问题。


<details>
  <summary>Details</summary>
Motivation: 研究如何从给定的正负示例中自动学习本体和约束，这对于知识表示和数据库系统的自动化构建具有重要意义。

Method: 使用描述逻辑EL和ELI以及多种TGD类型（包括full、guarded、frontier-guarded、frontier-one和unrestricted TGDs以及包含依赖）作为本体和约束语言，通过理论分析和算法设计来研究拟合问题。

Result: 精确确定了计算复杂性，设计了相应算法，分析了拟合本体和TGD的大小特性，并发现对于某些TGD类型（如full、frontier-guarded和frontier-one TGDs）不存在有限基。

Conclusion: 虽然EL、ELI、guarded TGDs和包含依赖存在有限基，但其他类型的TGDs一般不存在有限基，这为相关领域的理论研究和实际应用提供了重要指导。

Abstract: We study the problem of fitting ontologies and constraints to positive and
negative examples that take the form of a finite relational structure. As
ontology and constraint languages, we consider the description logics
$\mathcal{E\mkern-2mu L}$ and $\mathcal{E\mkern-2mu LI}$ as well as several
classes of tuple-generating dependencies (TGDs): full, guarded,
frontier-guarded, frontier-one, and unrestricted TGDs as well as inclusion
dependencies. We pinpoint the exact computational complexity, design
algorithms, and analyze the size of fitting ontologies and TGDs. We also
investigate the related problem of constructing a finite basis of concept
inclusions / TGDs for a given set of finite structures. While finite bases
exist for $\mathcal{E\mkern-2mu L}$, $\mathcal{E\mkern-2mu LI}$, guarded TGDs,
and inclusion dependencies, they in general do not exist for full,
frontier-guarded and frontier-one TGDs.

</details>


### [26] [A Hardware-oriented Approach for Efficient Active Inference Computation and Deployment](https://arxiv.org/abs/2508.13177)
*Nikola Pižurica,Nikola Milović,Igor Jovančević,Conor Heins,Miguel de Prado*

Main category: cs.AI

TL;DR: 通过统一的稀疏计算图优化，将pymdp的灵活性与硬件效率结合，使主动推理在资源受限环境中实现了超过2倍的延迟减少和达到35%的内存节省


<details>
  <summary>Details</summary>
Motivation: 主动推理(AIF)框架在决策制定方面具有优势，但其计算和内存需求导致在资源受限环境中部署困难

Method: 整合pymdp的灵活性和效率，使用专门为硬件效率执行而设计的统一稀疏计算图

Result: 延迟减少超2倍，内存节省达到35%

Conclusion: 推进了高效主动推理代理在实时和嵌入式应用中的部署

Abstract: Active Inference (AIF) offers a robust framework for decision-making, yet its
computational and memory demands pose challenges for deployment, especially in
resource-constrained environments. This work presents a methodology that
facilitates AIF's deployment by integrating pymdp's flexibility and efficiency
with a unified, sparse, computational graph tailored for hardware-efficient
execution. Our approach reduces latency by over 2x and memory by up to 35%,
advancing the deployment of efficient AIF agents for real-time and embedded
applications.

</details>


### [27] [The Interpretability Analysis of the Model Can Bring Improvements to the Text-to-SQL Task](https://arxiv.org/abs/2508.13178)
*Cong Zhang*

Main category: cs.AI

TL;DR: CESQL模型通过集成模型可解释性分析和执行引导策略，结合过滤调整、逻辑关联优化和模型融合，显著提升了WikiSQL数据集上WHERE子句条件值预测的准确性。


<details>
  <summary>Details</summary>
Motivation: 提升文本到SQL模型在真实应用中的基础能力和泛化能力，特别是在处理WHERE子句语义解析时减少对条件列数据和人工标注训练数据的依赖。

Method: 集成模型可解释性分析与执行引导策略，结合过滤调整、逻辑关联精化和模型融合，设计条件增强的CESQL模型。

Result: 在WikiSQL单表数据库查询任务上表现优异，显著提高了预测结果的准确性，特别是在WHERE子句条件值预测方面。

Conclusion: 这项研究为处理复杂查询和真实数据库环境中不规则数据场景的研究提供了新的视角和方法基础。

Abstract: To elevate the foundational capabilities and generalization prowess of the
text-to-SQL model in real-world applications, we integrate model
interpretability analysis with execution-guided strategy for semantic parsing
of WHERE clauses in SQL queries. Furthermore, we augment this approach with
filtering adjustments, logical correlation refinements, and model fusion,
culminating in the design of the CESQL model that facilitates conditional
enhancement. Our model excels on the WikiSQL dataset, which is emblematic of
single-table database query tasks, markedly boosting the accuracy of prediction
outcomes. When predicting conditional values in WHERE clauses, we have not only
minimized our dependence on data within the condition columns of tables but
also circumvented the impact of manually labeled training data. Our hope is
that this endeavor to enhance accuracy in processing basic database queries
will offer fresh perspectives for research into handling complex queries and
scenarios featuring irregular data in real-world database environments.

</details>


### [28] [Search-Time Data Contamination](https://arxiv.org/abs/2508.13180)
*Ziwen Han,Meher Mankikar,Julian Michael,Zifan Wang*

Main category: cs.AI

TL;DR: 本文识别了搜索型LLM代理评估中的搜索时污染问题，即代理在搜索过程中发现包含测试问题和答案的数据集，导致直接复制而非真正推理，损害评估基准的有效性。


<details>
  <summary>Details</summary>
Motivation: 随着搜索型LLM代理的广泛应用，评估其真实能力变得至关重要。然而，现有评估方法存在数据污染风险，特别是当评估数据集在搜索过程中被意外检索到时，会严重扭曲评估结果。

Method: 通过分析三个常用能力基准（HLE、SimpleQA、GPQA）上搜索型代理的检索日志，识别HuggingFace平台上的评估数据集被检索到的情况，并进行阻断实验和消融研究。

Result: 发现约3%的问题中代理直接从HuggingFace找到带标签的数据集；阻断HuggingFace后，受污染子集的准确率下降约15%；公开可访问的评估数据集可能不是STC的唯一来源。

Conclusion: 提出了基准设计和结果报告的最佳实践建议，以解决这种新型泄漏问题，确保搜索型LLM代理评估的可信度，并公开了完整的实验日志以供审计。

Abstract: Data contamination refers to the leakage of evaluation data into model
training data, resulting in overfitting to supposedly held-out test sets and
compromising test validity. We identify an analogous issue, search-time
contamination (STC), in evaluating search-based LLM agents which use tools to
gather information from online sources when answering user queries. STC occurs
when the retrieval step surfaces a source containing the test question (or a
near-duplicate) alongside its answer, enabling agents to copy rather than
genuinely infer or reason, undermining benchmark integrity. We find that
HuggingFace, an online platform hosting evaluation datasets, appears among
retrieved sources in search based agent logs. Consequently, agents often
explicitly acknowledge discovering question answer pairs from HuggingFace
within their reasoning chains. On three commonly used capability benchmarks:
Humanity's Last Exam (HLE), SimpleQA, and GPQA, we demonstrate that for
approximately 3% of questions, search-based agents directly find the datasets
with ground truth labels on HuggingFace. When millions of evaluation queries
target the same benchmark, even small, repeated leaks can accelerate the
benchmark's obsolescence, shortening its intended lifecycle. After HuggingFace
is blocked, we observe a drop in accuracy on the contaminated subset of
approximately 15%. We further show through ablation experiments that publicly
accessible evaluation datasets on HuggingFace may not be the sole source of
STC. To this end, we conclude by proposing best practices for benchmark design
and result reporting to address this novel form of leakage and ensure
trustworthy evaluation of search-based LLM agents. To facilitate the auditing
of evaluation results, we also publicly release the complete logs from our
experiments.

</details>


### [29] [QuickMerge++: Fast Token Merging with Autoregressive Prior](https://arxiv.org/abs/2508.13204)
*Dong Liu,Yanxuan Yu*

Main category: cs.AI

TL;DR: QuickMerge是一个轻量级token合并框架，通过注意力范数动态选择重要token，使用基于熵的预算估计器，并引入transformer先验来保持自回归兼容性，显著减少计算成本的同时保持生成质量。


<details>
  <summary>Details</summary>
Motivation: 随着生成模型处理更大规模输入，token级别的计算成本成为主要瓶颈。现有token选择方法大多是静态的、模态特定的或不兼容自回归生成，需要更高效的动态token选择方案。

Method: 基于注意力范数大小动态选择重要token，使用熵基预算估计器确定token数量，引入轻量级transformer先验在合并后的token序列上进行训练以保持自回归兼容性。

Result: 在多模态领域评估显示，QuickMerge在计算精度权衡方面取得一致改进，显著减少token数量的同时匹配甚至超越学习型tokenizer和固定patch基线的性能。

Conclusion: QuickMerge通过语义显著性估计、灵活token预算和自回归对齐的组合，实现了用更少token进行准确生成的有效框架，解决了大规模生成模型的计算瓶颈问题。

Abstract: As generative models scale to larger inputs across language, vision, and
video domains, the cost of token-level computation has become a key bottleneck.
While prior work suggests that only a subset of tokens significantly influence
downstream predictions, most token selection methods are static,
modality-specific, or incompatible with autoregressive generation. In this
paper, we propose QuickMerge, a lightweight token merging framework designed
for efficient next-token prediction.
  QuickMerge dynamically selects a reduced number of tokens based on attention
norm magnitude, guided by an entropy-based budget estimator. To preserve
autoregressive compatibility, we introduce a lightweight transformer prior
trained over the merged token sequence. By combining semantic salience
estimation, flexible token budgets, and AR alignment, QuickMerge enables
accurate generation with fewer tokens.
  We evaluate QuickMerge across multi-modality domains, demonstrating
consistent improvements in compute-accuracy tradeoffs. Specifically, QuickMerge
reduces token counts sustantially while matching as well as exceeding the
performance of learned tokenizers and fixed-patch baselines.

</details>


### [30] [AI sustains higher strategic tension than humans in chess](https://arxiv.org/abs/2508.13213)
*Adamo Cerioli,Edward D. Lee,Vito D. P. Servedio*

Main category: cs.AI

TL;DR: 棋牌中人工智能与人类在战略决策中的差异：AI能维持更高水平的战略强度和复杂性，而人类因认知限制而选择控制游戏复杂度。


<details>
  <summary>Details</summary>
Motivation: 研究战略决策中瞬时机会与长期目标的羁绊关系，通过棋牌游戏对比人类和AI的决策行为差异。

Method: 提出基于网络的棋子互动指标来量化战略强度，分析人类vs人类和AIvsAI游戏的动态变化。

Result: 竞争力强的AI能在更长时间内维持更高的战略强度；人类游戏的累计强度随着专业知识的提升而凸变增加（特别是在1600和2300Elo附近）。

Conclusion: AI和人类在战略决策上采取不同方式：AI对复杂互联局面更耐受，而人类因认知限制而选择控制复杂性。这对AI在复杂战略环境中的应用有重要意义。

Abstract: Strategic decision-making involves managing the tension between immediate
opportunities and long-term objectives. We study this trade-off in chess by
characterizing and comparing dynamics between human vs human and AI vs AI
games. We propose a network-based metric of piece-to-piece interaction to
quantify the ongoing strategic tension on the board. Its evolution in games
reveals that the most competitive AI players sustain higher levels of strategic
tension for longer durations than elite human players. Cumulative tension
varies with algorithmic complexity for AI and correspondingly in human-played
games increases abruptly with expertise at about 1600 Elo and again at 2300
Elo. The profiles reveal different approaches. Highly competitive AI tolerates
interconnected positions balanced between offensive and defensive tactics over
long periods. Human play, in contrast, limits tension and game complexity,
which may reflect cognitive limitations and adaptive strategies. The difference
may have implications for AI usage in complex, strategic environments.

</details>


### [31] [Explicit v.s. Implicit Memory: Exploring Multi-hop Complex Reasoning Over Personalized Information](https://arxiv.org/abs/2508.13250)
*Zeyu Zhang,Yang Zhang,Haoran Tan,Rui Li,Xu Chen*

Main category: cs.AI

TL;DR: 该论文提出了多跳个性化推理任务，探索不同记忆机制在个性化信息多跳推理中的表现，并构建了数据集和评估框架，提出了HybridMem混合方法来解决现有方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有基于记忆的个性化方法主要关注偏好对齐和简单问答，但现实世界中复杂任务需要对大量用户信息进行多跳推理，这对当前记忆方法提出了重大挑战。

Method: 明确定义多跳个性化推理任务，构建数据集和统一评估框架，实现各种显式和隐式记忆方法，并提出结合两种范式的HybridMem混合方法。

Result: 通过综合实验从多个角度评估了不同记忆方法的性能，分析了它们的优缺点，并证明了所提出模型的有效性。

Conclusion: 该研究为解决个性化信息的多跳推理问题提供了系统性的解决方案，HybridMem方法能够有效克服现有记忆机制的局限性，推动了基于大语言模型的智能代理在复杂个性化任务中的应用。

Abstract: In large language model-based agents, memory serves as a critical capability
for achieving personalization by storing and utilizing users' information.
Although some previous studies have adopted memory to implement user
personalization, they typically focus on preference alignment and simple
question-answering. However, in the real world, complex tasks often require
multi-hop reasoning on a large amount of user information, which poses
significant challenges for current memory approaches. To address this
limitation, we propose the multi-hop personalized reasoning task to explore how
different memory mechanisms perform in multi-hop reasoning over personalized
information. We explicitly define this task and construct a dataset along with
a unified evaluation framework. Then, we implement various explicit and
implicit memory methods and conduct comprehensive experiments. We evaluate
their performance on this task from multiple perspectives and analyze their
strengths and weaknesses. Besides, we explore hybrid approaches that combine
both paradigms and propose the HybridMem method to address their limitations.
We demonstrate the effectiveness of our proposed model through extensive
experiments. To benefit the research community, we release this project at
https://github.com/nuster1128/MPR.

</details>


### [32] ["DIVE" into Hydrogen Storage Materials Discovery with AI Agents](https://arxiv.org/abs/2508.13251)
*Di Zhang,Xue Jia,Tran Ba Hung,Seong Hoon Jang,Linda Zhang,Ryuhei Sato,Yusuke Hashimoto,Toyoto Sato,Kiyoe Konno,Shin-ichi Orimo,Hao Li*

Main category: cs.AI

TL;DR: DIVE多智能体工作流通过系统读取和组织科学文献中的图形数据，显著提升材料数据提取准确性，建立了包含3万条数据的氢存储材料数据库，实现了快速逆向设计。


<details>
  <summary>Details</summary>
Motivation: 科学文献中大量材料数据被困在非结构化的图表中，阻碍了基于大语言模型的AI智能体进行自动化材料设计，需要解决数据提取和整理的难题。

Method: 开发了DIVE多智能体工作流，系统读取科学文献中的图形元素并组织实验数据，专注于固态氢存储材料的数据提取和数据库构建。

Result: DIVE相比多模态模型直接提取，准确率提升10-15%（商业模型）和30%以上（开源模型），建立了包含4,000篇文献、30,000条数据的数据库，能在2分钟内识别未报道的氢存储成分。

Conclusion: 该AI工作流和智能体设计可广泛应用于不同材料领域，为AI驱动的材料发现提供了新范式。

Abstract: Data-driven artificial intelligence (AI) approaches are fundamentally
transforming the discovery of new materials. Despite the unprecedented
availability of materials data in the scientific literature, much of this
information remains trapped in unstructured figures and tables, hindering the
construction of large language model (LLM)-based AI agent for automated
materials design. Here, we present the Descriptive Interpretation of Visual
Expression (DIVE) multi-agent workflow, which systematically reads and
organizes experimental data from graphical elements in scientific literatures.
We focus on solid-state hydrogen storage materials-a class of materials central
to future clean-energy technologies and demonstrate that DIVE markedly improves
the accuracy and coverage of data extraction compared to the direct extraction
by multimodal models, with gains of 10-15% over commercial models and over 30%
relative to open-source models. Building on a curated database of over 30,000
entries from 4,000 publications, we establish a rapid inverse design workflow
capable of identifying previously unreported hydrogen storage compositions in
two minutes. The proposed AI workflow and agent design are broadly transferable
across diverse materials, providing a paradigm for AI-driven materials
discovery.

</details>


### [33] [CardAIc-Agents: A Multimodal Framework with Hierarchical Adaptation for Cardiac Care Support](https://arxiv.org/abs/2508.13256)
*Yuting Zhang,Karina V. Bunting,Asgher Champsi,Xiaoxia Wang,Wenqi Lu,Alexander Thorley,Sandeep S Hothi,Zhaowen Qiu,Dipak Kotecha,Jinming Duan*

Main category: cs.AI

TL;DR: CardAIc-Agents是一个多模态AI框架，通过外部工具增强和自适应推理来解决心血管疾病早期检测的临床限制，在多个数据集上表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 心血管疾病是全球主要死因，但医疗工作者严重短缺。现有AI系统在临床应用中存在限制：基于提示的角色分配、僵化的工作流程、静态知识库以及有限的输入输出模式。

Method: 提出多模态框架CardAIc-Agents，包含CardiacRAG代理生成可更新的心脏知识计划，主代理集成工具自主执行计划。采用逐步更新策略动态优化计划，引入多学科讨论工具处理复杂病例，并提供视觉审查面板辅助验证。

Result: 在三个数据集上的实验表明，CardAIc-Agents相比主流视觉语言模型、最先进的代理系统和微调视觉语言模型都表现出更高的效率。

Conclusion: CardAIc-Agents框架通过工具增强和自适应推理有效解决了心血管疾病AI检测的临床限制，为个性化心脏护理提供了可行的解决方案。

Abstract: Cardiovascular diseases (CVDs) remain the foremost cause of mortality
worldwide, a burden worsened by a severe deficit of healthcare workers.
Artificial intelligence (AI) agents have shown potential to alleviate this gap
via automated early detection and proactive screening, yet their clinical
application remains limited by: 1) prompt-based clinical role assignment that
relies on intrinsic model capabilities without domain-specific tool support; or
2) rigid sequential workflows, whereas clinical care often requires adaptive
reasoning that orders specific tests and, based on their results, guides
personalised next steps; 3) general and static knowledge bases without
continuous learning capability; and 4) fixed unimodal or bimodal inputs and
lack of on-demand visual outputs when further clarification is needed. In
response, a multimodal framework, CardAIc-Agents, was proposed to augment
models with external tools and adaptively support diverse cardiac tasks.
Specifically, a CardiacRAG agent generated general plans from updatable cardiac
knowledge, while the chief agent integrated tools to autonomously execute these
plans and deliver decisions. To enable adaptive and case-specific
customization, a stepwise update strategy was proposed to dynamically refine
plans based on preceding execution results, once the task was assessed as
complex. In addition, a multidisciplinary discussion tool was introduced to
interpret challenging cases, thereby supporting further adaptation. When
clinicians raised concerns, visual review panels were provided to assist final
validation. Experiments across three datasets showed the efficiency of
CardAIc-Agents compared to mainstream Vision-Language Models (VLMs),
state-of-the-art agentic systems, and fine-tuned VLMs.

</details>


### [34] [Towards Unified Multimodal Financial Forecasting: Integrating Sentiment Embeddings and Market Indicators via Cross-Modal Attention](https://arxiv.org/abs/2508.13327)
*Sarthak Khanna,Armin Berger,David Berghaus,Tobias Deusser,Lorenz Sparrenberg,Rafet Sifa*

Main category: cs.AI

TL;DR: STONK是一个多模态股票预测框架，结合数值市场指标和情感增强的新闻嵌入，通过特征拼接和跨模态注意力机制提升股票涨跌预测性能


<details>
  <summary>Details</summary>
Motivation: 解决传统孤立分析方法在股票预测中的局限性，整合数值和文本信息来提升预测准确性

Method: 使用特征拼接和跨模态注意力机制融合数值市场指标与情感增强的新闻嵌入，构建统一的多模态预测管道

Result: 回测显示STONK优于纯数值基线模型，提供了融合策略和模型配置的实证指导

Conclusion: 该框架为可扩展的多模态金融预测提供了有效解决方案，源代码已在GitHub开源

Abstract: We propose STONK (Stock Optimization using News Knowledge), a multimodal
framework integrating numerical market indicators with sentiment-enriched news
embeddings to improve daily stock-movement prediction. By combining numerical &
textual embeddings via feature concatenation and cross-modal attention, our
unified pipeline addresses limitations of isolated analyses. Backtesting shows
STONK outperforms numeric-only baselines. A comprehensive evaluation of fusion
strategies and model configurations offers evidence-based guidance for scalable
multimodal financial forecasting. Source code is available on GitHub

</details>


### [35] [HiFo-Prompt: Prompting with Hindsight and Foresight for LLM-based Automatic Heuristic Design](https://arxiv.org/abs/2508.13333)
*Chentong Chen,Mengyuan Zhong,Jianyong Sun,Ye Fan,Jialong Shi*

Main category: cs.AI

TL;DR: HiFo-Prompt框架通过前瞻性和后顾性提示策略，解决了LLM自动启发式设计中静态操作符和知识积累不足的问题，显著提升了启发式生成质量和收敛速度。


<details>
  <summary>Details</summary>
Motivation: 当前基于LLM的自动启发式设计在进化计算框架中表现出潜力，但受到静态操作符和缺乏知识积累机制的限制，影响了其有效性。

Method: 提出HiFo-Prompt框架，采用两种协同提示策略：前瞻性提示基于种群动态自适应引导搜索，管理探索-利用权衡；后顾性提示从过去世代的成功启发式中提炼可重用的设计原则，模仿人类专业知识。

Result: 实证结果表明HiFo-Prompt显著优于最先进的基于LLM的AHD方法，生成更高质量的启发式，同时实现更快的收敛速度和卓越的查询效率。

Conclusion: 该双机制框架将瞬时发现转化为持久知识库，使LLM能够从自身经验中学习，为自动启发式设计提供了有效的知识积累解决方案。

Abstract: LLM-based Automatic Heuristic Design (AHD) within Evolutionary Computation
(EC) frameworks has shown promising results. However, its effectiveness is
hindered by the use of static operators and the lack of knowledge accumulation
mechanisms. We introduce HiFo-Prompt, a framework that guides LLMs with two
synergistic prompting strategies: Foresight and Hindsight. Foresight-based
prompts adaptively steer the search based on population dynamics, managing the
exploration-exploitation trade-off. In addition, hindsight-based prompts mimic
human expertise by distilling successful heuristics from past generations into
fundamental, reusable design principles. This dual mechanism transforms
transient discoveries into a persistent knowledge base, enabling the LLM to
learn from its own experience. Empirical results demonstrate that HiFo-Prompt
significantly outperforms state-of-the-art LLM-based AHD methods, generating
higher-quality heuristics while achieving substantially faster convergence and
superior query efficiency.

</details>


### [36] [LOOP: A Plug-and-Play Neuro-Symbolic Framework for Enhancing Planning in Autonomous Systems](https://arxiv.org/abs/2508.13371)
*Ronit Virwani,Ruchika Suryawanshi*

Main category: cs.AI

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Planning is one of the most critical tasks in autonomous systems, where even
a small error can lead to major failures or million-dollar losses. Current
state-of-the-art neural planning approaches struggle with complex domains,
producing plans with missing preconditions, inconsistent goals, and
hallucinations. While classical planners provide logical guarantees, they lack
the flexibility and natural language understanding capabilities needed for
modern autonomous systems. Existing neuro-symbolic approaches use one-shot
translation from natural language to formal plans, missing the opportunity for
neural and symbolic components to work and refine solutions together. To
address this gap, we develop LOOP -- a novel neuro-symbolic planning framework
that treats planning as an iterative conversation between neural and symbolic
components rather than simple translation. LOOP integrates 13 coordinated
neural features including graph neural networks for spatial relationships,
multi-agent validation for consensus-based correctness, hierarchical
decomposition for complex task management, and causal memory that learns from
both successes and failures. Unlike existing approaches, LOOP generates PDDL
specifications, refines them iteratively based on symbolic feedback, and builds
a causal knowledge base from execution traces. LOOP was evaluated on six
standard IPC benchmark domains, where it achieved 85.8% success rate compared
to LLM+P (55.0%), LLM-as-Planner (19.2%), and Tree-of-Thoughts (3.3%). This
work shows that the key to reliable planning is not in choosing between neural
networks or symbolic reasoners but it lies in making them actually ``talk'' to
each other during the entire process. LOOP provides a thorough blueprint for
building autonomous systems that can finally be trusted with critical
real-world applications.

</details>


### [37] [SPANER: Shared Prompt Aligner for Multimodal Semantic Representation](https://arxiv.org/abs/2508.13387)
*Thye Shan Ng,Caren Soyeon Han,Eun-Jung Holden*

Main category: cs.AI

TL;DR: SPANER是一个模态无关的参数高效微调框架，通过共享提示机制将不同模态输入嵌入到统一语义空间，提升跨模态泛化能力


<details>
  <summary>Details</summary>
Motivation: 现有多模态PEFT方法主要关注任务特定性能提升，但忽视了多模态嵌入空间的结构，导致模态特定表示孤立，限制了跨模态泛化

Method: 提出SPANER框架，采用共享提示机制作为概念锚点，使语义相关实例在空间中收敛，支持无缝集成新模态而不改变核心架构

Result: 在视觉-语言和音频-视觉基准测试中展示了竞争力的少样本检索性能，同时保持了学习嵌入空间的高语义一致性

Conclusion: 强调对齐嵌入结构的重要性，而不仅仅是调整适配器权重，对于可扩展的多模态学习至关重要

Abstract: Recent advances in multimodal Parameter-Efficient Fine-Tuning (PEFT) have
significantly improved performance on downstream tasks such as few-shot
retrieval. However, most existing approaches focus on task-specific gains while
neglecting the structure of the multimodal embedding space. As a result,
modality-specific representations often remain isolated, limiting cross-modal
generalisation. In this work, we introduce Shared Prompt AligNER (SPANER), a
modality-agnostic PEFT framework designed to embed inputs from diverse
modalities into a unified semantic space. At its core, SPANER employs a shared
prompt mechanism that acts as a conceptual anchor, enabling semantically
related instances to converge spatially regardless of modality. This shared
prompt design is inherently extensible, supporting the seamless integration of
additional modalities, such as audio, without altering the core architecture.
Through comprehensive experiments across vision-language and audio-visual
benchmarks, SPANER demonstrates competitive few-shot retrieval performance
while preserving high semantic coherence in the learned embedding space. Our
results highlight the importance of aligning embedding structures, rather than
merely tuning adapter weights, for scalable multimodal learning.

</details>


### [38] [TASER: Table Agents for Schema-guided Extraction and Recommendation](https://arxiv.org/abs/2508.13404)
*Nicole Cho,Kirsty Fielding,William Watson,Sumitra Ganesh,Manuela Veloso*

Main category: cs.AI

TL;DR: TASER是一个持续学习的智能表格提取系统，专门处理真实世界中高度非结构化、多页、异构的金融表格，通过模式引导的提取和推荐实现比现有方法更好的性能。


<details>
  <summary>Details</summary>
Motivation: 现实金融文档中的表格信息往往埋藏在混乱的多页碎片化表格中（99.4%的表格没有边界框），现有表格检测模型难以有效处理这些真实世界的复杂表格。

Method: 开发了TASER系统，包含表格检测、分类、提取和推荐代理，利用初始模式执行提取，然后通过推荐代理审查输出、建议模式修订并做出最终决策，实现持续学习。

Result: TASER比Table Transformer等现有表格检测模型性能提升10.1%，大批量处理使可操作的模式建议增加104.3%，提取的持仓量增加9.8%。构建了包含22,584页、3,213个表格的真实金融数据集TASERTab。

Conclusion: 基于代理的模式引导提取系统在理解真实世界金融表格方面展现出巨大潜力，持续学习过程对于处理复杂金融文档至关重要。

Abstract: Real-world financial documents report essential information about an entity's
financial holdings that can span millions of different financial instrument
types. Yet, these details are often buried in messy, multi-page, fragmented
tables - for example, 99.4% of the tables in our dataset have no bounding boxes
with the maximum number of rows amounting to 426 per table across 44 pages. To
tackle these unique challenges from real-world tables, we present a
continuously learning, agentic table extraction system, TASER (Table Agents for
Schema-guided Extraction and Recommendation) that extracts highly unstructured,
multi-page, heterogeneous tables into normalized, schema-conforming outputs.
Our table agents execute on table detection, classification, extraction, and
recommendations by leveraging an initial schema. Then, our Recommender Agent
reviews the outputs, recommends schema revisions, and decides on the final
recommendations, enabling TASER to outperform existing table detection models
such as Table Transformer by 10.1%. Within this continuous learning process, we
highlight that larger batch sizes result in a 104.3% increase in schema
recommendations that are actionable and utilized, resulting in a 9.8% increase
in extracted holdings - highlighting the importance of a continuous learning
process. To train TASER, we have manually labeled 22,584 pages (28,150,449
tokens), 3,213 tables for $731,685,511,687 of holdings culminating in one of
the first real financial table datasets. We release our dataset TASERTab to
enable the research community to access real-world financial tables and
outputs. Our results highlight the promise of agentic, schema-guided extraction
systems for robust understanding of real-world financial tables.

</details>


### [39] [Virtuous Machines: Towards Artificial General Science](https://arxiv.org/abs/2508.13421)
*Gabrielle Wehr,Reuben Rideaux,Amaya J. Fox,David R. Lightfoot,Jason Tangen,Jason B. Mattingley,Shane E. Ehrhardt*

Main category: cs.AI

TL;DR: AI系统能够自主完成从假设生成到论文撰写的完整科研流程，在心理学研究中展示了与人类研究者相当的推理能力和方法严谨性


<details>
  <summary>Details</summary>
Motivation: 科学文献的指数级增长和领域专业化限制了研究者跨学科整合知识的能力，需要探索更通用的AI系统来加速科学发现

Method: 开发了一个领域无关的智能AI系统，自主设计并执行了三个心理学研究（视觉工作记忆、心理旋转、意象生动性），进行了包含288名参与者的在线数据收集，并通过8小时以上的连续编码开发分析流程

Result: 系统能够进行非平凡的研究工作，展现出与经验丰富研究者相当的理论推理和方法严谨性，但在概念细微差别和理论解释方面存在局限

Conclusion: 这是向能够通过真实世界实验验证假设的具身AI迈出的一步，可以自主探索人类认知和资源限制可能无法触及的科学领域，同时也引发了关于科学理解本质和科学贡献归属的重要问题

Abstract: Artificial intelligence systems are transforming scientific discovery by
accelerating specific research tasks, from protein structure prediction to
materials design, yet remain confined to narrow domains requiring substantial
human oversight. The exponential growth of scientific literature and increasing
domain specialisation constrain researchers' capacity to synthesise knowledge
across disciplines and develop unifying theories, motivating exploration of
more general-purpose AI systems for science. Here we show that a
domain-agnostic, agentic AI system can independently navigate the scientific
workflow - from hypothesis generation through data collection to manuscript
preparation. The system autonomously designed and executed three psychological
studies on visual working memory, mental rotation, and imagery vividness,
executed one new online data collection with 288 participants, developed
analysis pipelines through 8-hour+ continuous coding sessions, and produced
completed manuscripts. The results demonstrate the capability of AI scientific
discovery pipelines to conduct non-trivial research with theoretical reasoning
and methodological rigour comparable to experienced researchers, though with
limitations in conceptual nuance and theoretical interpretation. This is a step
toward embodied AI that can test hypotheses through real-world experiments,
accelerating discovery by autonomously exploring regions of scientific space
that human cognitive and resource constraints might otherwise leave unexplored.
It raises important questions about the nature of scientific understanding and
the attribution of scientific credit.

</details>


### [40] [STPFormer: A State-of-the-Art Pattern-Aware Spatio-Temporal Transformer for Traffic Forecasting](https://arxiv.org/abs/2508.13433)
*Jiayu Fang,Zhiqi Shao,S T Boris Choy,Junbin Gao*

Main category: cs.AI

TL;DR: STPFormer是一个时空模式感知Transformer模型，通过统一的表示学习方法在时空交通预测中实现了最先进的性能，包含四个核心模块来处理复杂的时空模式。


<details>
  <summary>Details</summary>
Motivation: 现有的Transformer模型在时空交通预测中存在时间编码僵化和时空融合能力弱的问题，无法有效处理复杂的时空模式和多样化的输入格式。

Method: 提出了STPFormer模型，包含四个模块：Temporal Position Aggregator（TPA）用于模式感知时间编码，Spatial Sequence Aggregator（SSA）用于序列空间学习，Spatial-Temporal Graph Matching（STGM）用于跨域对齐，以及Attention Mixer用于多尺度融合。

Result: 在五个真实世界数据集上的实验表明，STPFormer始终达到新的SOTA结果，消融实验和可视化验证了其有效性和泛化能力。

Conclusion: STPFormer通过统一的表示学习方法成功解决了时空交通预测中的关键挑战，为复杂时空模式建模提供了有效的解决方案。

Abstract: Spatio-temporal traffic forecasting is challenging due to complex temporal
patterns, dynamic spatial structures, and diverse input formats. Although
Transformer-based models offer strong global modeling, they often struggle with
rigid temporal encoding and weak space-time fusion. We propose STPFormer, a
Spatio-Temporal Pattern-Aware Transformer that achieves state-of-the-art
performance via unified and interpretable representation learning. It
integrates four modules: Temporal Position Aggregator (TPA) for pattern-aware
temporal encoding, Spatial Sequence Aggregator (SSA) for sequential spatial
learning, Spatial-Temporal Graph Matching (STGM) for cross-domain alignment,
and an Attention Mixer for multi-scale fusion. Experiments on five real-world
datasets show that STPFormer consistently sets new SOTA results, with ablation
and visualizations confirming its effectiveness and generalizability.

</details>


### [41] [Discrete Optimization of Min-Max Violation and its Applications Across Computational Sciences](https://arxiv.org/abs/2508.13437)
*Cheikh Ahmed,Mahdi Mostajabdaveh,Samin Aref,Zirui Zhou*

Main category: cs.AI

TL;DR: 提出离散最小-最大违约(DMMV)优化问题，开发GPU加速贫例算法，在语言模型量化、离散断层扫描和FIR滤波器设计中取得显著改善


<details>
  <summary>Details</summary>
Motivation: 为了解决具有最坏情况性能要求的广泛优化问题，需要一个通用的数学框架来形式化这类问题并提供高效解决方案

Method: 数学定义DMMV问题，分析其性质特征，开发GPU加速的贫例算法，利用DMMV的数学性质提高求解速度

Result: 在语言模型量化中比现有方法提升15%，离散断层扫描中降低16%重建误差并加速6倍，FIR滤波器设计中浮动水平降低近50%

Conclusion: DMMV作为一个通用优化框架具有强大的应用潜力，提出的GPU加速贫例算法在多个领域都取得了显著成果，将开源以促进更多应用研究

Abstract: We introduce the Discrete Min-Max Violation (DMMV) as a general optimization
problem which seeks an assignment of discrete values to variables that
minimizes the largest constraint violation. This context-free mathematical
formulation is applicable to a wide range of use cases that have worst-case
performance requirements. After defining the DMMV problem mathematically, we
explore its properties to establish a foundational understanding. To tackle
DMMV instance sizes of practical relevance, we develop a GPU-accelerated
heuristic that takes advantage of the mathematical properties of DMMV for
speeding up the solution process. We demonstrate the versatile applicability of
our heuristic by solving three optimization problems as use cases: (1)
post-training quantization of language models, (2) discrete tomography, and (3)
Finite Impulse Response (FIR) filter design. In quantization without outlier
separation, our heuristic achieves 14% improvement on average over existing
methods. In discrete tomography, it reduces reconstruction error by 16% under
uniform noise and accelerates computations by a factor of 6 on GPU. For FIR
filter design, it nearly achieves 50% ripple reduction compared to using the
commercial integer optimization solver, Gurobi. Our comparative results point
to the benefits of studying DMMV as a context-free optimization problem and the
advantages that our proposed heuristic offers on three distinct problems. Our
GPU-accelerated heuristic will be made open-source to further stimulate
research on DMMV and its other applications. The code is available at
https://anonymous.4open.science/r/AMVM-5F3E/

</details>


### [42] [LM Agents May Fail to Act on Their Own Risk Knowledge](https://arxiv.org/abs/2508.13465)
*Yuzhi Tang,Tianxiao Li,Elizabeth Li,Chris J. Maddison,Honghua Dong,Yangjun Ruan*

Main category: cs.AI

TL;DR: 研究发现语言模型代理存在风险意识与安全执行能力之间的显著差距，即使知道某些操作危险，在实际执行时仍会执行危险操作。作者开发了评估框架和风险验证系统，成功将危险操作执行率降低了55.3%。


<details>
  <summary>Details</summary>
Motivation: 语言模型代理在安全关键场景中存在严重风险，虽然它们具备风险知识，但在实际执行时无法有效识别和避免危险操作，这种差距需要系统性的研究和解决方案。

Method: 开发了包含三个维度的评估框架：风险知识、风险识别能力和安全行为执行。基于发现的性能差距，构建了风险验证器系统，包含抽象器将具体执行轨迹转换为抽象描述，使模型能更有效识别风险。

Result: 评估显示代理风险知识接近完美（>98%通过率），但实际风险识别性能下降>23%，危险操作执行通过率<26%。风险验证系统将危险操作执行率降低了55.3%。

Conclusion: 单纯扩展模型能力或推理计算无法解决安全问题，需要专门的安全机制。风险验证器系统能有效弥补代理的安全执行差距，显著降低危险操作执行风险。

Abstract: Language model (LM) agents have demonstrated significant potential for
automating real-world tasks, yet they pose a diverse array of potential, severe
risks in safety-critical scenarios. In this work, we identify a significant gap
between LM agents' risk awareness and safety execution abilities: while they
often answer "Yes" to queries like "Is executing `sudo rm -rf /*' dangerous?",
they will likely fail to identify such risks in instantiated trajectories or
even directly perform these risky actions when acting as agents. To
systematically investigate this, we develop a comprehensive evaluation
framework to examine agents' safety across three progressive dimensions: 1)
their knowledge about potential risks, 2) their ability to identify
corresponding risks in execution trajectories, and 3) their actual behaviors to
avoid executing these risky actions. Our evaluation reveals two critical
performance gaps that resemble the generator-validator gaps observed in LMs:
while agents demonstrate near-perfect risk knowledge ($>98\%$ pass rates), they
fail to apply this knowledge when identifying risks in actual scenarios (with
performance dropping by $>23\%$) and often still execute risky actions ($<26\%$
pass rates). Notably, this trend persists across more capable LMs as well as in
specialized reasoning models like DeepSeek-R1, indicating that simply scaling
model capabilities or inference compute does not inherently resolve safety
concerns. Instead, we take advantage of these observed gaps to develop a risk
verifier that independently critiques the proposed actions by agents, with an
abstractor that converts specific execution trajectories into abstract
descriptions where LMs can more effectively identify the risks. Our overall
system achieves a significant reduction of risky action execution by $55.3\%$
over vanilla-prompted agents.

</details>


### [43] [CrafterDojo: A Suite of Foundation Models for Building Open-Ended Embodied Agents in Crafter](https://arxiv.org/abs/2508.13530)
*Junyeong Park,Hyeonseo Cho,Sungjin Ahn*

Main category: cs.AI

TL;DR: 提出CrafterDojo套件，包含基础模型和工具，将Crafter环境变成轻量级、原型友好的Minecraft类似测试平台，支持体现智能体精研究


<details>
  <summary>Details</summary>
Motivation: 解决Minecraft环境运行慢、工程费用高的问题，而Crafter环境缺乏基础模型支持，限制了在体现智能体精研究中的应用

Method: 开发CrafterDojo套件，包含CrafterVPT（行为先验）、CrafterCLIP（视觉-语言基准）、CrafterSteve-1（指令跟随）等基础模型，以及数据集生成、引擎实现、测试评估工具

Result: 构建了一个完整的开源代码库，为Crafter环境提供了基础模型支持，使其成为轻量级但保留Minecraft关键挑战的原型开发平台

Conclusion: CrafterDojo最终解锁了Crafter环境在体现智能体精研究中的潜力，提供了一个高效、灵活的原型开发环境，有助于普遍体现智能体的发展

Abstract: Developing general-purpose embodied agents is a core challenge in AI.
Minecraft provides rich complexity and internet-scale data, but its slow speed
and engineering overhead make it unsuitable for rapid prototyping. Crafter
offers a lightweight alternative that retains key challenges from Minecraft,
yet its use has remained limited to narrow tasks due to the absence of
foundation models that have driven progress in the Minecraft setting. In this
paper, we present CrafterDojo, a suite of foundation models and tools that
unlock the Crafter environment as a lightweight, prototyping-friendly, and
Minecraft-like testbed for general-purpose embodied agent research. CrafterDojo
addresses this by introducing CrafterVPT, CrafterCLIP, and CrafterSteve-1 for
behavior priors, vision-language grounding, and instruction following,
respectively. In addition, we provide toolkits for generating behavior and
caption datasets (CrafterPlay and CrafterCaption), reference agent
implementations, benchmark evaluations, and a complete open-source codebase.

</details>


### [44] [Toward Better EHR Reasoning in LLMs: Reinforcement Learning with Expert Attention Guidance](https://arxiv.org/abs/2508.13579)
*Yue Fang,Yuxin Guo,Jiaran Gao,Hongxin Ding,Xinke Jiang,Weibin Liao,Yongxin Xu,Yinghao Zhu,Zhibang Yang,Liantao Ma,Junfeng Zhao,Yasha Wang*

Main category: cs.AI

TL;DR: EAG-RL是一个两阶段训练框架，通过专家注意力引导来增强LLM在电子健康记录推理方面的能力，平均提升14.62%的性能


<details>
  <summary>Details</summary>
Motivation: 现有方法通常将LLM作为固定的先验检索器，而下游深度学习模型处理预测，无法提高LLM的内在推理能力，且继承了DL模型的泛化限制

Method: 首先使用专家引导的蒙特卡洛树搜索构建高质量的分步推理轨迹来初始化LLM策略，然后通过强化学习将LLM的注意力与专家EHR模型识别的临床显著特征对齐

Result: 在两个真实世界EHR数据集上的实验显示，EAG-RL平均提升LLM内在EHR推理能力14.62%，同时增强了对特征扰动的鲁棒性和对未见临床领域的泛化能力

Conclusion: EAG-RL在临床预测任务中具有实际部署的潜力，能够显著提升LLM在EHR推理方面的性能

Abstract: Improving large language models (LLMs) for electronic health record (EHR)
reasoning is essential for enabling accurate and generalizable clinical
predictions. While LLMs excel at medical text understanding, they underperform
on EHR-based prediction tasks due to challenges in modeling temporally
structured, high-dimensional data. Existing approaches often rely on hybrid
paradigms, where LLMs serve merely as frozen prior retrievers while downstream
deep learning (DL) models handle prediction, failing to improve the LLM's
intrinsic reasoning capacity and inheriting the generalization limitations of
DL models. To this end, we propose EAG-RL, a novel two-stage training framework
designed to intrinsically enhance LLMs' EHR reasoning ability through expert
attention guidance, where expert EHR models refer to task-specific DL models
trained on EHR data. Concretely, EAG-RL first constructs high-quality, stepwise
reasoning trajectories using expert-guided Monte Carlo Tree Search to
effectively initialize the LLM's policy. Then, EAG-RL further optimizes the
policy via reinforcement learning by aligning the LLM's attention with
clinically salient features identified by expert EHR models. Extensive
experiments on two real-world EHR datasets show that EAG-RL improves the
intrinsic EHR reasoning ability of LLMs by an average of 14.62%, while also
enhancing robustness to feature perturbations and generalization to unseen
clinical domains. These results demonstrate the practical potential of EAG-RL
for real-world deployment in clinical prediction tasks. Our code have been
available at https://github.com/devilran6/EAG-RL.

</details>


### [45] [Breaking the SFT Plateau: Multimodal Structured Reinforcement Learning for Chart-to-Code Generation](https://arxiv.org/abs/2508.13587)
*Lei Chen,Xuanle Zhao,Zhixiong Zeng,Jing Huang,Liming Zheng,Yufeng Zhong,Lin Ma*

Main category: cs.AI

TL;DR: 这篇论文研究了多模态结构化强化学习(MSRL)在图表转代码生成任务中的应用，通过多级别奖励机制突破了监督学习的性能平台，在两个标准测试集上获得显著提升。


<details>
  <summary>Details</summary>
Motivation: 虽然强化学习在视觉-语言模型中有效，但在需要深度理解信息丰富图像和生成结构化输出的任务中应用不足。图表转代码生成作为一个具有复杂解释挑战的任务，需要有效的RL策略来奖励结构化输出。

Method: 提出多模态结构化强化学习(MSRL)，构建了包含300万对图表-代码对的最大训练语料库。MSRL采用多级别结构化奖励系统：文本级别使用规则基础的奖励验证细粒度代码细节，视觉级别通过渲染生成代码为图像并使用评估模型来评估结构相似性。采用两阶段课程进行训练稳定性优化。

Result: MSRL显著突破了SFT的性能平台，在ChartMimic和ReachQA标准测试集上分别提升了6.2%和9.9%的高级指标，达到了与先进闭源模型相竞争的性能。

Conclusion: 该研究证明了多模态结构化强化学习在复杂解释任务中的有效性，通过多级别奖励机制成功突破了传统监督学习的限制，为结构化输出生成任务提供了新的解决方案。

Abstract: While reinforcement learning (RL) has proven highly effective for general
reasoning in vision-language models, its application to tasks requiring
in-depth understanding of information-rich images and generation of structured
outputs remains underexplored. Chart-to-code generation exemplifies this
challenge, demanding complex reasoning over visual charts to generate
structured code. Supervised fine-tuning (SFT) alone is often insufficient,
highlighting the need for effective RL strategies that appropriately reward
structured outputs. We systematically investigate the performance plateau in
SFT through large-scale experiments and propose Multimodal Structured
Reinforcement Learning (MSRL) for chart-to-code generation, which substantially
breaks through this plateau. We construct the largest training corpus to date,
containing 3 million chart-code pairs from real-world arXiv tables to mitigate
simplistic patterns of prior synthetic data. Despite reaching state-of-the-art
performance, our experiments show that scaling SFT data eventually hits a
plateau where further increases yield negligible improvements. Our MSRL method
leverages a multi-granularity structured reward system using multimodal textual
and visual feedback. At the textual level, rule-based rewards validate
fine-grained code details. At the visual level, model-based rewards assess
structural similarity by rendering generated code into images and employing an
evaluator model. We implement this within a two-stage curriculum for training
stability. Results demonstrate that MSRL significantly breaks the SFT plateau,
improving high-level metrics by 6.2% and 9.9% on ChartMimic and ReachQA
benchmarks respectively, achieving competitive performance with advanced
closed-source models.

</details>


### [46] [V2P: From Background Suppression to Center Peaking for Robust GUI Grounding Task](https://arxiv.org/abs/2508.13634)
*Jikai Chen,Long Chen,Dong Wang,Leilei Gan,Chenyi Zhuang,Jinjie Gu*

Main category: cs.AI

TL;DR: V2P方法通过抑制注意力机制和基于Fitts定律的高斯热图建模，解决了GUI元素定位中的背景干扰和中心边缘区分问题，在两个基准测试中分别达到92.3%和50.5%的性能。


<details>
  <summary>Details</summary>
Motivation: 传统GUI元素定位方法忽视空间交互不确定性和视觉语义层次结构，现有注意力方法存在背景区域干扰导致注意力漂移，以及均匀标注无法区分目标UI元素中心和边缘的问题。

Method: 提出Valley-to-Peak (V2P)方法：1) 抑制注意力机制减少对无关区域的关注；2) 基于Fitts定律将GUI交互建模为2D高斯热图，权重从中心向边缘递减，方差由目标大小决定。

Result: 在两个基准测试ScreenSpot-v2和ScreenSpot-Pro上分别达到92.3%和50.5%的性能，消融实验证实了各组件贡献。

Conclusion: V2P方法能有效隔离目标区域并让模型专注于UI元素最关键的点，在精确GUI定位任务中具有良好泛化能力。

Abstract: Precise localization of GUI elements is crucial for the development of GUI
agents. Traditional methods rely on bounding box or center-point regression,
neglecting spatial interaction uncertainty and visual-semantic hierarchies.
Recent methods incorporate attention mechanisms but still face two key issues:
(1) ignoring processing background regions causes attention drift from the
desired area, and (2) uniform labeling fails to distinguish between center and
edges of the target UI element, leading to click imprecision. Inspired by how
humans visually process and interact with GUI elements, we propose the
Valley-to-Peak (V2P) method to address these issues. To mitigate background
distractions, V2P introduces a suppression attention mechanism that minimizes
the model's focus on irrelevant regions to highlight the intended region. For
the issue of center-edge distinction, V2P applies a Fitts' Law-inspired
approach by modeling GUI interactions as 2D Gaussian heatmaps where the weight
gradually decreases from the center towards the edges. The weight distribution
follows a Gaussian function, with the variance determined by the target's size.
Consequently, V2P effectively isolates the target area and teaches the model to
concentrate on the most essential point of the UI element. The model trained by
V2P achieves the performance with 92.3% and 50.5% on two benchmarks
ScreenSpot-v2 and ScreenSpot-Pro. Ablations further confirm each component's
contribution, highlighting V2P's generalizability for precise GUI grounding
tasks.

</details>


### [47] [Interactive Query Answering on Knowledge Graphs with Soft Entity Constraints](https://arxiv.org/abs/2508.13663)
*Daniel Daza,Alberto Bernardi,Luca Costabello,Christophe Gueret,Masoud Mansoury,Michael Cochez,Martijn Schut*

Main category: cs.AI

TL;DR: 提出了在知识图谱查询中处理软约束的新问题，开发了神经查询重排序器(NQR)来交互式调整答案评分，通过增量示例整合偏好约束而不破坏原始查询结果。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要关注一阶逻辑查询，但实际查询常涉及模糊或上下文相关的软约束（如属性偏好、相关类别偏好），需要新的解决方案。

Method: 提出神经查询重排序器(NQR)，通过交互方式基于偏好和非偏好实体的增量示例来调整查询答案评分，整合软约束。

Result: 实验表明NQR能够有效捕获软约束，同时保持稳健的查询回答性能。扩展了现有QA基准，生成了包含软约束的数据集。

Conclusion: 该工作填补了知识图谱查询中软约束处理的空白，NQR方法为处理模糊和上下文相关的查询约束提供了有效解决方案。

Abstract: Methods for query answering over incomplete knowledge graphs retrieve
entities that are likely to be answers, which is particularly useful when such
answers cannot be reached by direct graph traversal due to missing edges.
However, existing approaches have focused on queries formalized using
first-order-logic. In practice, many real-world queries involve constraints
that are inherently vague or context-dependent, such as preferences for
attributes or related categories. Addressing this gap, we introduce the problem
of query answering with soft constraints. We propose a Neural Query Reranker
(NQR) designed to adjust query answer scores by incorporating soft constraints
without disrupting the original answers to a query. NQR operates interactively,
refining answers based on incremental examples of preferred and non-preferred
entities. We extend existing QA benchmarks by generating datasets with soft
constraints. Our experiments demonstrate that NQR can capture soft constraints
while maintaining robust query answering performance.

</details>


### [48] [ITL-LIME: Instance-Based Transfer Learning for Enhancing Local Explanations in Low-Resource Data Settings](https://arxiv.org/abs/2508.13672)
*Rehan Raza,Guanjin Wang,Kevin Wong,Hamid Laga,Marco Fisichella*

Main category: cs.AI

TL;DR: 提出了ITL-LIME框架，通过实例迁移学习增强LIME在数据受限环境下的解释保真度和稳定性，使用相关源域真实实例而非随机扰动来改进解释过程。


<details>
  <summary>Details</summary>
Motivation: 传统LIME方法在数据稀缺场景下存在局部性和不稳定性问题，随机扰动可能产生偏离真实数据流形的样本，导致代理模型无法准确近似原始模型的复杂决策边界。

Method: 引入实例迁移学习，通过聚类将源域分区并获取代表性原型，检索与目标实例最相似的源簇中的相关真实实例，结合目标实例的邻近真实实例，使用对比学习编码器作为加权机制，基于实例与目标实例的接近程度分配权重，最后用加权实例训练代理模型。

Result: ITL-LIME框架提高了解释的保真度和稳定性，特别是在数据受限环境下，通过利用相关源域的真实实例来更好地近似原始模型的决策边界。

Conclusion: 该方法有效解决了LIME在数据稀缺场景下的局限性，通过实例迁移学习和对比学习机制，显著提升了可解释人工智能方法的解释质量和可靠性。

Abstract: Explainable Artificial Intelligence (XAI) methods, such as Local
Interpretable Model-Agnostic Explanations (LIME), have advanced the
interpretability of black-box machine learning models by approximating their
behavior locally using interpretable surrogate models. However, LIME's inherent
randomness in perturbation and sampling can lead to locality and instability
issues, especially in scenarios with limited training data. In such cases, data
scarcity can result in the generation of unrealistic variations and samples
that deviate from the true data manifold. Consequently, the surrogate model may
fail to accurately approximate the complex decision boundary of the original
model. To address these challenges, we propose a novel Instance-based Transfer
Learning LIME framework (ITL-LIME) that enhances explanation fidelity and
stability in data-constrained environments. ITL-LIME introduces instance
transfer learning into the LIME framework by leveraging relevant real instances
from a related source domain to aid the explanation process in the target
domain. Specifically, we employ clustering to partition the source domain into
clusters with representative prototypes. Instead of generating random
perturbations, our method retrieves pertinent real source instances from the
source cluster whose prototype is most similar to the target instance. These
are then combined with the target instance's neighboring real instances. To
define a compact locality, we further construct a contrastive learning-based
encoder as a weighting mechanism to assign weights to the instances from the
combined set based on their proximity to the target instance. Finally, these
weighted source and target instances are used to train the surrogate model for
explanation purposes.

</details>


### [49] [Knowledge Graph Completion for Action Prediction on Situational Graphs -- A Case Study on Household Tasks](https://arxiv.org/abs/2508.13675)
*Mariam Arustashvili,Jörg Deigmöller,Heiko Paulheim*

Main category: cs.AI

TL;DR: 这篇论文研究家庭动作知识图语的特殊性质，发现多数标准链接预测算法在这个领域表现差强，甚至较简单基准方法都不如


<details>
  <summary>Details</summary>
Motivation: 家庭动作知识图语在家庭机器人控制和视频分析中具有重要价值，但从视频中提取的信息通常不完整，需要通过知识图进行补全以提升情况报告

Method: 对比分析多种标准链接预测算法在情况知识图语上的表现，并与简单基准方法进行比较

Result: 发现情况知识图语具有特殊特征，导致大部分标准链接预测算法无法满足需求，表现较简单基准方法更差

Conclusion: 情况知识图语的特殊性质需要专门的算法设计，现有标准方法在这个领域效果不佳

Abstract: Knowledge Graphs are used for various purposes, including business
applications, biomedical analyses, or digital twins in industry 4.0. In this
paper, we investigate knowledge graphs describing household actions, which are
beneficial for controlling household robots and analyzing video footage. In the
latter case, the information extracted from videos is notoriously incomplete,
and completing the knowledge graph for enhancing the situational picture is
essential. In this paper, we show that, while a standard link prediction
problem, situational knowledge graphs have special characteristics that render
many link prediction algorithms not fit for the job, and unable to outperform
even simple baselines.

</details>


### [50] [MHSNet:An MoE-based Hierarchical Semantic Representation Network for Accurate Duplicate Resume Detection with Large Language Model](https://arxiv.org/abs/2508.13676)
*Yu Li,Zulong Chen,Wenjian Xu,Hong Wen,Yipeng Yu,Man Lung Yiu,Yuyu Yin*

Main category: cs.AI

TL;DR: MHSNet是一个基于BGE-M3微调的多层次身份验证框架，用于检测第三方网站获取的简历与公司人才库中简历的重复项，解决简历文本的语义复杂性、结构异质性和信息不完整性问题。


<details>
  <summary>Details</summary>
Motivation: 第三方网站获取的简历往往不完整且不准确，需要通过重复检测来提升简历质量并丰富公司人才库，但现有方法难以处理简历文本的语义复杂性、结构异质性和信息不完整性。

Method: 提出MHSNet框架，使用对比学习微调BGE-M3模型，通过Mixture-of-Experts（MoE）生成多层次稀疏和密集表示来计算语义相似度，并采用状态感知MoE处理不完整简历。

Result: 实验结果验证了MHSNet的有效性。

Conclusion: MHSNet能够有效解决简历重复检测中的挑战，提升第三方简历质量并丰富公司人才库。

Abstract: To maintain the company's talent pool, recruiters need to continuously search
for resumes from third-party websites (e.g., LinkedIn, Indeed). However,
fetched resumes are often incomplete and inaccurate. To improve the quality of
third-party resumes and enrich the company's talent pool, it is essential to
conduct duplication detection between the fetched resumes and those already in
the company's talent pool. Such duplication detection is challenging due to the
semantic complexity, structural heterogeneity, and information incompleteness
of resume texts. To this end, we propose MHSNet, an multi-level identity
verification framework that fine-tunes BGE-M3 using contrastive learning. With
the fine-tuned , Mixture-of-Experts (MoE) generates multi-level sparse and
dense representations for resumes, enabling the computation of corresponding
multi-level semantic similarities. Moreover, the state-aware Mixture-of-Experts
(MoE) is employed in MHSNet to handle diverse incomplete resumes. Experimental
results verify the effectiveness of MHSNet

</details>


### [51] [Neuro-Symbolic Artificial Intelligence: Towards Improving the Reasoning Abilities of Large Language Models](https://arxiv.org/abs/2508.13678)
*Xiao-Wen Yang,Jie-Jing Shao,Lan-Zhe Guo,Bo-Wen Zhang,Zhi Zhou,Lin-Han Jia,Wang-Zhou Dai,Yu-Feng Li*

Main category: cs.AI

TL;DR: 这篇论文综述了使用神经符号方法来提升大语言模型的推理能力，从三个角度分析了相关技术，并讨论了未来挑战和方向。


<details>
  <summary>Details</summary>
Motivation: 大语言模型虽在多任务上表现出艰，但推理能力仍是核心挑战，而神经符号方法被认为是提升推理能力的有效途径。

Method: 从三个角度系统评估神经符号方法：Symbolic->LLM（符号向LLM输入）、LLM->Symbolic（LLM输出符号）、LLM+Symbolic（两者协同）。

Result: 该论文完整形式化了推理任务，介绍了神经符号学习范式，并对不同方法进行了系统分析和评估。

Conclusion: 神经符号方法在提升LLM推理能力方面具有强大潜力，论文持出了关键挑战和未来研究方向，为进一步研究提供了指引。

Abstract: Large Language Models (LLMs) have shown promising results across various
tasks, yet their reasoning capabilities remain a fundamental challenge.
Developing AI systems with strong reasoning capabilities is regarded as a
crucial milestone in the pursuit of Artificial General Intelligence (AGI) and
has garnered considerable attention from both academia and industry. Various
techniques have been explored to enhance the reasoning capabilities of LLMs,
with neuro-symbolic approaches being a particularly promising way. This paper
comprehensively reviews recent developments in neuro-symbolic approaches for
enhancing LLM reasoning. We first present a formalization of reasoning tasks
and give a brief introduction to the neurosymbolic learning paradigm. Then, we
discuss neuro-symbolic methods for improving the reasoning capabilities of LLMs
from three perspectives: Symbolic->LLM, LLM->Symbolic, and LLM+Symbolic.
Finally, we discuss several key challenges and promising future directions. We
have also released a GitHub repository including papers and resources related
to this survey: https://github.com/LAMDASZ-ML/Awesome-LLM-Reasoning-with-NeSy.

</details>


### [52] [The DeepLog Neurosymbolic Machine](https://arxiv.org/abs/2508.13697)
*Vincent Derkinderen,Robin Manhaeve,Rik Adriaensen,Lucas Van Praet,Lennert De Smet,Giuseppe Marra,Luc De Raedt*

Main category: cs.AI

TL;DR: DeepLog是一个神经符号AI的理论和操作框架，提供构建块和原语来抽象表示和计算机制，支持多种神经符号系统的表示和仿真。


<details>
  <summary>Details</summary>
Motivation: 为了解决神经符号AI中不同表示和计算机制的复杂性，提供一个统一的抽象框架来简化神经符号模型的构建和推理任务。

Method: 包含两个关键组件：1) DeepLog语言 - 基于带注释的神经扩展一阶逻辑，抽象逻辑类型和用途；2) 计算层面的扩展代数电路作为计算图。两者构成神经符号抽象机器。

Result: 通过实验比较证明了DeepLog的通用性和效率：1) 不同模糊和概率逻辑的比较；2) 逻辑在架构或损失函数中使用的比较；3) CPU实现与GPU实现的性能比较。

Conclusion: DeepLog提供了一个声明式的神经符号抽象机器，通过选择不同的代数结构和逻辑可以轻松获得不同的神经符号模型，具有通用性和高效性。

Abstract: We contribute a theoretical and operational framework for neurosymbolic AI
called DeepLog. DeepLog introduces building blocks and primitives for
neurosymbolic AI that make abstraction of commonly used representations and
computational mechanisms used in neurosymbolic AI. DeepLog can represent and
emulate a wide range of neurosymbolic systems. It consists of two key
components. The first is the DeepLog language for specifying neurosymbolic
models and inference tasks. This language consists of an annotated neural
extension of grounded first-order logic, and makes abstraction of the type of
logic, e.g. boolean, fuzzy or probabilistic, and whether logic is used in the
architecture or in the loss function. The second DeepLog component is situated
at the computational level and uses extended algebraic circuits as
computational graphs. Together these two components are to be considered as a
neurosymbolic abstract machine, with the DeepLog language as the intermediate
level of abstraction and the circuits level as the computational one. DeepLog
is implemented in software, relies on the latest insights in implementing
algebraic circuits on GPUs, and is declarative in that it is easy to obtain
different neurosymbolic models by making different choices for the underlying
algebraic structures and logics. The generality and efficiency of the DeepLog
neurosymbolic machine is demonstrated through an experimental comparison
between 1) different fuzzy and probabilistic logics, 2) between using logic in
the architecture or in the loss function, and 3) between a standalone CPU-based
implementation of a neurosymbolic AI system and a DeepLog GPU-based one.

</details>


### [53] [CausalPlan: Empowering Efficient LLM Multi-Agent Collaboration Through Causality-Driven Planning](https://arxiv.org/abs/2508.13721)
*Minh Hoang Nguyen,Van Dai Do,Dung Nguyen,Thin Nguyen,Hung Le*

Main category: cs.AI

TL;DR: CausalPlan是一个两阶段框架，通过将显式结构因果推理集成到LLM规划过程中，解决小规模开源LLM智能体在协作任务中产生因果无效或不连贯动作的问题。


<details>
  <summary>Details</summary>
Motivation: 当前LLM智能体（尤其是小型开源模型）在协作任务中依赖表面相关性而非基于因果推理，导致产生因果无效或不连贯动作，限制了它们在动态环境中的协调和规划性能。

Method: 提出CausalPlan框架，核心是结构因果动作（SCA）模型，从智能体轨迹中学习因果图来捕捉先前动作和当前环境状态如何影响未来决策，然后使用该结构通过因果评分来指导动作选择。

Result: 在Overcooked-AI基准测试的五个多智能体协调任务和四个不同规模的LLM上评估，CausalPlan持续减少无效动作并改善AI-AI和人类-AI设置中的协作，优于强化学习基线。

Conclusion: 因果驱动规划对于部署高效、可解释和可泛化的多智能体LLM系统具有重要价值，无需对LLM本身进行微调即可约束规划为干预一致的行为。

Abstract: Large language model (LLM) agents-especially smaller, open-source
models-often produce causally invalid or incoherent actions in collaborative
tasks due to their reliance on surface-level correlations rather than grounded
causal reasoning. This limitation undermines their performance in terms of
coordination and planning in dynamic environments. We address this challenge
with CausalPlan, a two-phase framework that integrates explicit structural
causal reasoning into the LLM planning process. At the core of CausalPlan is
the Structural Causal Action (SCA) model, which learns a causal graph from
agent trajectories to capture how prior actions and current environment states
influence future decisions. This structure is then used to guide action
selection by assigning causal scores to LLM-generated proposals, reweighting
them accordingly, or falling back to causally grounded alternatives when
needed. By embedding this causal knowledge directly into the decision loop,
CausalPlan constrains planning to intervention-consistent behaviours without
requiring fine-tuning of the LLM itself. We evaluate CausalPlan on the
Overcooked-AI benchmark across five multi-agent coordination tasks and four
LLMs of varying sizes: Gemma-7B, Llama-8B, Qwen-14B, and Llama-70B.
Experimental results show that CausalPlan consistently reduces invalid actions
and improves collaboration in both AI-AI and human-AI settings, outperforming
strong reinforcement learning baselines. Our findings highlight the value of
causality-driven planning for deploying efficient, interpretable, and
generalisable multi-agent LLM systems.

</details>


### [54] [Expertise-aware Multi-LLM Recruitment and Collaboration for Medical Decision-Making](https://arxiv.org/abs/2508.13754)
*Liuxin Bao,Zhihao Peng,Xiaofei Zhou,Runmin Cong,Jiyong Zhang,Yixuan Yuan*

Main category: cs.AI

TL;DR: 提出EMRC框架，通过专业知识领域的多LLM动态选择和协作，显著提升医疗决策的准确性和可靠性


<details>
  <summary>Details</summary>
Motivation: 单一LLM方法受限于参数知识约束和静态训练语料，无法健壮地整合复杂的临床信息

Method: 两阶段框架：1、使用公开语料建立LLM专业知识表，动态选择最优LLM；2、通过自信分数融合和对抗验证提升诊断可靠性

Result: 在三个公开MDM数据集上超过最先进的单一和多LLM方法，在MMLU-Pro-Health数据集上达到74.45%准确率，比GPT-4-0613提升2.69%

Conclusion: EMRC框架通过专业知识领域的动态选择和多代理协作，有效提升了医疗决策系统的性能和可靠性

Abstract: Medical Decision-Making (MDM) is a complex process requiring substantial
domain-specific expertise to effectively synthesize heterogeneous and
complicated clinical information. While recent advancements in Large Language
Models (LLMs) show promise in supporting MDM, single-LLM approaches are limited
by their parametric knowledge constraints and static training corpora, failing
to robustly integrate the clinical information. To address this challenge, we
propose the Expertise-aware Multi-LLM Recruitment and Collaboration (EMRC)
framework to enhance the accuracy and reliability of MDM systems. It operates
in two stages: (i) expertise-aware agent recruitment and (ii) confidence- and
adversarial-driven multi-agent collaboration. Specifically, in the first stage,
we use a publicly available corpus to construct an LLM expertise table for
capturing expertise-specific strengths of multiple LLMs across medical
department categories and query difficulty levels. This table enables the
subsequent dynamic selection of the optimal LLMs to act as medical expert
agents for each medical query during the inference phase. In the second stage,
we employ selected agents to generate responses with self-assessed confidence
scores, which are then integrated through the confidence fusion and adversarial
validation to improve diagnostic reliability. We evaluate our EMRC framework on
three public MDM datasets, where the results demonstrate that our EMRC
outperforms state-of-the-art single- and multi-LLM methods, achieving superior
diagnostic performance. For instance, on the MMLU-Pro-Health dataset, our EMRC
achieves 74.45% accuracy, representing a 2.69% improvement over the
best-performing closed-source model GPT- 4-0613, which demonstrates the
effectiveness of our expertise-aware agent recruitment strategy and the agent
complementarity in leveraging each LLM's specialized capabilities.

</details>


### [55] [Quantifier Instantiations: To Mimic or To Revolt?](https://arxiv.org/abs/2508.13811)
*Jan Jakubův,Mikoláš Janota*

Main category: cs.AI

TL;DR: 提出一种基于概率上下文无关文法的动态量化实例化方法，通过从现有技术中学习实例化模式来平衡利用和探索


<details>
  <summary>Details</summary>
Motivation: 现有SMT求解器中的量化公式实例化技术（如e-matching、语法引导、基于模型等方法）往往互补但缺乏动态学习能力，需要更好的方法来处理量化公式的不可判定性挑战

Method: 将观察到的实例化视为潜在语言的样本，使用概率上下文无关文法生成新的相似项，既能模仿成功的过往实例化，也能通过反转学习到的项概率来探索多样性

Result: 该方法能够动态学习各种实例化技术，在量化推理中实现利用和探索的平衡

Conclusion: 基于文法学习的动态实例化方法为SMT求解器处理量化公式提供了新的有效途径，通过概率建模实现了实例化技术的智能组合和自适应生成

Abstract: Quantified formulas pose a significant challenge for Satisfiability Modulo
Theories (SMT) solvers due to their inherent undecidability. Existing
instantiation techniques, such as e-matching, syntax-guided, model-based,
conflict-based, and enumerative methods, often complement each other. This
paper introduces a novel instantiation approach that dynamically learns from
these techniques during solving. By treating observed instantiations as samples
from a latent language, we use probabilistic context-free grammars to generate
new, similar terms. Our method not only mimics successful past instantiations
but also explores diversity by optionally inverting learned term probabilities,
aiming to balance exploitation and exploration in quantifier reasoning.

</details>


### [56] [Revisiting RAG Ensemble: A Theoretical and Mechanistic Analysis of Multi-RAG System Collaboration](https://arxiv.org/abs/2508.13828)
*Yifei Chen,Guanting Dong,Yutao Zhu,Zhicheng Dou*

Main category: cs.AI

TL;DR: 本文系统研究了多个RAG系统的集成方法，从理论和机制角度分析了信息瑜和集成框架，通过四种流水线和三种模块解决七个研究问题，实验证明多RAG系统集成具有良好的通用性和稳健性。


<details>
  <summary>Details</summary>
Motivation: 当前单个RAG框架无法良好适应广泛的下游任务，需要探索如何利用多个RAG系统的优势来提升性能。

Method: 从理论分析角度通过信息瑜解释RAG集成框架，从机制分析角度研究流水线级别和模块级别的集成方法，选择四种流水线（分支、迭代、循环、代理）和三种模块（生成器、检索器、重排器）进行实验。

Result: 实验结果显示，无论在流水线级别还是模块级别，聚合多个RAG系统都显示出良好的通用性和稳健性。

Conclusion: 本研究为多RAG系统集成相关研究奠定了基础，证明了多系统集成在提升RAG技术性能方面的潜力和价值。

Abstract: Retrieval-Augmented Generation (RAG) technology has been widely applied in
recent years. However, despite the emergence of various RAG frameworks, a
single RAG framework still cannot adapt well to a broad range of downstream
tasks. Therefore, how to leverage the advantages of multiple RAG systems has
become an area worth exploring. To address this issue, we have conducted a
comprehensive and systematic investigation into ensemble methods based on RAG
systems. Specifically, we have analyzed the RAG ensemble framework from both
theoretical and mechanistic analysis perspectives. From the theoretical
analysis, we provide the first explanation of the RAG ensemble framework from
the perspective of information entropy. In terms of mechanism analysis, we have
explored the RAG ensemble framework from both the pipeline and module levels.
We carefully select four different pipelines (Branching, Iterative, Loop, and
Agentic) and three different modules (Generator, Retriever, and Reranker) to
solve seven different research questions. The experiments show that aggregating
multiple RAG systems is both generalizable and robust, whether at the pipeline
level or the module level. Our work lays the foundation for similar research on
the multi-RAG system ensemble.

</details>


### [57] [Improved Generalized Planning with LLMs through Strategy Refinement and Reflection](https://arxiv.org/abs/2508.13876)
*Katharina Stein,Nils Hodel,Daniel Fišer,Jörg Hoffmann,Michael Katz,Alexander Koller*

Main category: cs.AI

TL;DR: 通过使用伪代码生成、自动调试和反思步骤改进LLM生成广义计划的质量，在17个基准领域中显著提高了Python程序的成功率


<details>
  <summary>Details</summary>
Motivation: 解决之前方法中只生成一个策略并直接转换为程序，如果策略错误就会导致全部失败的问题

Method: 使用伪代码生成策略并先进行自动调试，在Python调试阶段添加反思步骤，生成多个程序变体并选择最佳的

Result: 在17个基准领域中显著提高了广义计划的质量，其12个领域中最佳Python程序能解决所有可生成的任务

Conclusion: 通过伪代码调试、反思和多程序选择等方法，可以有效提高LLM生成广义计划的准确性和可靠性

Abstract: LLMs have recently been used to generate Python programs representing
generalized plans in PDDL planning, i.e., plans that generalize across the
tasks of a given PDDL domain. Previous work proposed a framework consisting of
three steps: the LLM first generates a summary and then a strategy for the
domain, both in natural language, and then implements that strategy as a Python
program, that gets debugged on example planning tasks. In that work, only one
strategy is generated and passed directly to the program generation. If the
strategy is incorrect, its implementation will therefore result in an incorrect
generalized plan. Here, we introduce an approach that generates the strategy in
the form of pseudocode and enables automatic debugging of the pseudocode, hence
allowing us to identify and fix errors prior to the generation of the
generalized plan itself. Additionally, we extend the Python debugging phase
with a reflection step prompting the LLM to pinpoint the reason for the
observed plan failure. Finally, we take inspiration from LLM code generation to
produce several program variants and pick the best one. Running experiments on
17 benchmark domains, we show that these extensions substantially improve (and
never deteriorate) the quality of the generalized plans. In 12 of the domains,
our best Python programs solve all tasks that can be generated with the
respective instance generator.

</details>


### [58] [Structured Agentic Workflows for Financial Time-Series Modeling with LLMs and Reflective Feedback](https://arxiv.org/abs/2508.13915)
*Yihao Ang,Yifan Bao,Lei Jiang,Jiajie Tao,Anthony K. H. Tung,Lukasz Szpruch,Hao Ni*

Main category: cs.AI

TL;DR: TS-Agent是一个模块化代理框架，用于自动化金融时间序列建模工作流，通过三阶段决策过程（模型选择、代码优化、微调）实现优于现有AutoML和代理基准的性能。


<details>
  <summary>Details</summary>
Motivation: 金融时间序列数据建模面临高性能、可解释性和可审计性的挑战，现有AutoML框架缺乏对领域特定需求和动态目标的适应性。

Method: 采用模块化代理架构，包含规划代理和结构化知识库，通过三阶段迭代决策过程：模型选择、代码优化和微调，结合上下文推理和实验反馈。

Result: 在多种金融预测和合成数据生成任务中，TS-Agent在准确性、鲁棒性和决策可追溯性方面均优于最先进的AutoML和代理基线方法。

Conclusion: TS-Agent框架为高风险金融环境提供了自适应学习、鲁棒调试和透明审计能力，显著提升了时间序列建模的自动化水平和性能表现。

Abstract: Time-series data is central to decision-making in financial markets, yet
building high-performing, interpretable, and auditable models remains a major
challenge. While Automated Machine Learning (AutoML) frameworks streamline
model development, they often lack adaptability and responsiveness to
domain-specific needs and evolving objectives. Concurrently, Large Language
Models (LLMs) have enabled agentic systems capable of reasoning, memory
management, and dynamic code generation, offering a path toward more flexible
workflow automation. In this paper, we introduce \textsf{TS-Agent}, a modular
agentic framework designed to automate and enhance time-series modeling
workflows for financial applications. The agent formalizes the pipeline as a
structured, iterative decision process across three stages: model selection,
code refinement, and fine-tuning, guided by contextual reasoning and
experimental feedback. Central to our architecture is a planner agent equipped
with structured knowledge banks, curated libraries of models and refinement
strategies, which guide exploration, while improving interpretability and
reducing error propagation. \textsf{TS-Agent} supports adaptive learning,
robust debugging, and transparent auditing, key requirements for high-stakes
environments such as financial services. Empirical evaluations on diverse
financial forecasting and synthetic data generation tasks demonstrate that
\textsf{TS-Agent} consistently outperforms state-of-the-art AutoML and agentic
baselines, achieving superior accuracy, robustness, and decision traceability.

</details>


### [59] [The Collaboration Paradox: Why Generative AI Requires Both Strategic Intelligence and Operational Stability in Supply Chain Management](https://arxiv.org/abs/2508.13942)
*Soumyadeep Dhar*

Main category: cs.AI

TL;DR: 研究发现AI驱动的协作代理在供应链中会出现"协作悖论"，即理论上更优的协作AI代理表现反而比非AI基准更差，原因是库存囤积导致系统饥饿。通过高层AI策略设定和低层协作执行的结合可以实现系统稳定性。


<details>
  <summary>Details</summary>
Motivation: 研究AI驱动代理在经济环境中的新兴战略行为，特别是在多级供应链这种容易出现牛鞭效应等不稳定性的合作环境中。

Method: 使用大型语言模型驱动的生成式AI代理在受控供应链模拟中进行计算实验，分析其行为倾向。

Result: 发现了"协作悖论"现象，协作AI代理表现比非AI基准更差；提出了结合高层AI策略设定和低层协作执行的框架，能够自主生成、评估和量化可行的战略选择。

Conclusion: 研究揭示了协作AI代理的新兴行为特性，为设计稳定有效的AI驱动商业分析系统提供了蓝图和方法论。

Abstract: The rise of autonomous, AI-driven agents in economic settings raises critical
questions about their emergent strategic behavior. This paper investigates
these dynamics in the cooperative context of a multi-echelon supply chain, a
system famously prone to instabilities like the bullwhip effect. We conduct
computational experiments with generative AI agents, powered by Large Language
Models (LLMs), within a controlled supply chain simulation designed to isolate
their behavioral tendencies. Our central finding is the "collaboration
paradox": a novel, catastrophic failure mode where theoretically superior
collaborative AI agents, designed with Vendor-Managed Inventory (VMI)
principles, perform even worse than non-AI baselines. We demonstrate that this
paradox arises from an operational flaw where agents hoard inventory, starving
the system. We then show that resilience is only achieved through a synthesis
of two distinct layers: high-level, AI-driven proactive policy-setting to
establish robust operational targets, and a low-level, collaborative execution
protocol with proactive downstream replenishment to maintain stability. Our
final framework, which implements this synthesis, can autonomously generate,
evaluate, and quantify a portfolio of viable strategic choices. The work
provides a crucial insight into the emergent behaviors of collaborative AI
agents and offers a blueprint for designing stable, effective AI-driven systems
for business analytics.

</details>


### [60] [ChronoLLM: Customizing Language Models for Physics-Based Simulation Code Generation](https://arxiv.org/abs/2508.13975)
*Jingquan Wang,Andrew Negrut,Harry Zhang,Khailanii Slaton,Shu Wang,Radu Serban,Jinlong Wu,Dan Negrut*

Main category: cs.AI

TL;DR: 通过精炼和定制大语言模型，使其能够为PyChrono模拟工具生成有效的脚本，降低专家使用模拟工具的门槛


<details>
  <summary>Details</summary>
Motivation: 解决专业模拟工具（如PyChrono）使用门槛高的问题，让大语言模型变成能够帮助专家有效使用模拟工具的虚拟助手

Method: 提出一个框架，对开源和闭源大语言模型进行精炼和定制，以生成PyChrono模拟脚本，包括从简单的单摆到复杂的车辆在可变形地形上的模拟

Result: 精炼后的模型在生成PyChrono模拟脚本质量上有量化提升，虽然并非完美但能提供良好的起点，还能回答API相关问题和推荐建模方法

Conclusion: 该框架具有普遍性，可以应用于其他领域的模拟工具，有效降低专业模拟工具的使用门槛

Abstract: This contribution is concerned with the following issue: can pretrained large
language models (LLMs) be refined and customized to the point where they become
virtual assistants helping experts with the effective use of a simulation tool?
In this case study, the ``simulation tool'' considered is PyChrono, an open
source multi-physics dynamics engine for multibody systems. We present a
framework for refining and customizing both open- and closed-source LLMs to
harness the power of AI in generating scripts that perform PyChrono virtual
experiments. We refine and customize several classes of LLMs through a process
that leads to a quantifiable improvement in the quality of the generated
PyChrono simulation scripts. These scripts can range from simple
single-pendulum simulations to complex virtual experiments involving full
vehicles on deformable terrain. While the generated scripts are rarely perfect,
they often serve as strong starting points for the user to modify and improve
on. Additionally, the LLM can answer specific API questions about the
simulator, or recommend modeling approaches. The framework discussed is general
and can be applied to lower the entry barrier for simulation tools associated
with other application domains.

</details>


### [61] [A Biased Random Key Genetic Algorithm for Solving the Longest Run Subsequence Problem](https://arxiv.org/abs/2508.14020)
*Christian Blum,Pedro Pinacho-Davidson*

Main category: cs.AI

TL;DR: 使用偏置随机密钥遗传算法(BRKGA)解决长距l子序列(LRS)问题，该算法在计算效率和解质量方面表现优异，是目前最先进的方法。


<details>
  <summary>Details</summary>
Motivation: LRS问题是一个NP难组合优化问题，在生物信息学和基因组重新组装中具有重要应用。需要开发高效算法来解决这个具有挑战性的问题。

Method: 提出使用Biased Random Key Genetic Algorithm (BRKGA)，重点关注个体评估的计算效率，将灰度值向量转换为有效解。为了对比，还开发了Max-Min Ant System并使用CPLEX整数规划求解器。

Result: 计算结果显示提出的BRKGA在LRS问题上表现优异，是目前最先进的技术。但在大字母表基础的输入字符串上仍有改进空间。

Conclusion: BRKGA算法在LRS问题上成功实现了高效解决，但需要进一步优化处理大字母集情况下的性能问题。

Abstract: The longest run subsequence (LRS) problem is an NP-hard combinatorial
optimization problem belonging to the class of subsequence problems from
bioinformatics. In particular, the problem plays a role in genome reassembly.
In this paper, we present a solution to the LRS problem using a Biased Random
Key Genetic Algorithm (BRKGA). Our approach places particular focus on the
computational efficiency of evaluating individuals, which involves converting
vectors of gray values into valid solutions to the problem. For comparison
purposes, a Max-Min Ant System is developed and implemented. This is in
addition to the application of the integer linear programming solver CPLEX for
solving all considered problem instances. The computation results show that the
proposed BRKGA is currently a state-of-the-art technique for the LRS problem.
Nevertheless, the results also show that there is room for improvement,
especially in the context of input strings based on large alphabet sizes.

</details>


### [62] [ComputerRL: Scaling End-to-End Online Reinforcement Learning for Computer Use Agents](https://arxiv.org/abs/2508.14040)
*Hanyu Lai,Xiao Liu,Yanxiao Zhao,Han Xu,Hanchen Zhang,Bohao Jing,Yanyu Ren,Shuntian Yao,Yuxiao Dong,Jie Tang*

Main category: cs.AI

TL;DR: ComputerRL是一个自主桌面智能框架，通过API-GUI范式统一程序化API调用和直接GUI交互，解决机器代理与人类桌面环境的不匹配问题。采用分布式RL基础设施和Entropulse训练策略，在OSWorld基准测试中达到48.1%的最新准确率。


<details>
  <summary>Details</summary>
Motivation: 解决机器代理在人类中心化桌面环境中操作复杂数字工作空间时的不匹配问题，提升桌面自动化的通用性和效率。

Method: 提出API-GUI范式统一程序化API调用和GUI交互；开发分布式RL基础设施支持大规模并行虚拟桌面环境；设计Entropulse训练策略，交替使用强化学习和监督微调来缓解熵崩溃问题。

Result: 在OSWorld基准测试中，基于GLM-4-9B-0414的AutoGLM-OS-9B模型达到48.1%的最新准确率，显著提升了通用代理在桌面自动化任务中的性能。

Conclusion: ComputerRL框架通过创新的API-GUI范式和可扩展的训练基础设施，有效解决了桌面智能代理的泛化问题，为自主桌面自动化提供了强有力的解决方案。

Abstract: We introduce ComputerRL, a framework for autonomous desktop intelligence that
enables agents to operate complex digital workspaces skillfully. ComputerRL
features the API-GUI paradigm, which unifies programmatic API calls and direct
GUI interaction to address the inherent mismatch between machine agents and
human-centric desktop environments. Scaling end-to-end RL training is crucial
for improvement and generalization across diverse desktop tasks, yet remains
challenging due to environmental inefficiency and instability in extended
training. To support scalable and robust training, we develop a distributed RL
infrastructure capable of orchestrating thousands of parallel virtual desktop
environments to accelerate large-scale online RL. Furthermore, we propose
Entropulse, a training strategy that alternates reinforcement learning with
supervised fine-tuning, effectively mitigating entropy collapse during extended
training runs. We employ ComputerRL on open models GLM-4-9B-0414 and
Qwen2.5-14B, and evaluate them on the OSWorld benchmark. The AutoGLM-OS-9B
based on GLM-4-9B-0414 achieves a new state-of-the-art accuracy of 48.1%,
demonstrating significant improvements for general agents in desktop
automation. The algorithm and framework are adopted in building AutoGLM (Liu et
al., 2024a)

</details>
