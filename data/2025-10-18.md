<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 18]
- [cs.CR](#cs.CR) [Total: 24]
- [cs.AI](#cs.AI) [Total: 52]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [From Craft to Constitution: A Governance-First Paradigm for Principled Agent Engineering](https://arxiv.org/abs/2510.13857)
*Qiang Xu,Xiangyu Wen,Changran Xu,Zeju Li,Jianyuan Zhong*

Main category: cs.SE

TL;DR: 本文提出了一种治理优先的智能体工程范式ArbiterOS，以解决从原型到生产过程中智能体的脆弱性和不可预测性问题。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型开启了智能体时代，但原型到生产的过渡存在'工艺危机'，导致智能体在关键应用中脆弱、不可预测且不可信。这种危机源于用传统软件工程的确定性思维来指挥概率性处理器的根本范式不匹配。

Method: 引入治理优先的智能体工程范式，体现在名为ArbiterOS的正式架构中。

Result: 提出了解决智能体工程危机的系统性方法。

Conclusion: 需要从传统软件工程的确定性范式转向治理优先的智能体工程范式，以构建可信赖的智能体系统。

Abstract: The advent of powerful Large Language Models (LLMs) has ushered in an ``Age
of the Agent,'' enabling autonomous systems to tackle complex goals. However,
the transition from prototype to production is hindered by a pervasive ``crisis
of craft,'' resulting in agents that are brittle, unpredictable, and ultimately
untrustworthy in mission-critical applications. This paper argues this crisis
stems from a fundamental paradigm mismatch -- attempting to command inherently
probabilistic processors with the deterministic mental models of traditional
software engineering. To solve this crisis, we introduce a governance-first
paradigm for principled agent engineering, embodied in a formal architecture we
call ArbiterOS.

</details>


### [2] [Benchmarking Correctness and Security in Multi-Turn Code Generation](https://arxiv.org/abs/2510.13859)
*Ruchit Rawal,Jeffrey Yang Fan Chiang,Chihao Shen,Jeffery Siyuan Tian,Aastha Mahajan,Tom Goldstein,Yizheng Chen*

Main category: cs.SE

TL;DR: MT-Sec是首个系统评估多轮编码场景中正确性和安全性的基准测试，通过将单轮任务转换为多轮交互序列，发现在多轮设置下模型生成正确且安全代码的比例下降20-27%。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试主要评估单轮任务，无法反映真实开发中的迭代性质，需要评估多轮编码场景中的正确性和安全性。

Method: 使用合成数据管道将现有单轮任务转换为语义对齐的多轮交互序列，重用原始测试套件，同时建模真实编码过程的复杂性。

Result: 评估32个开源和闭源模型及三种智能体框架，发现在多轮设置下正确且安全的输出下降20-27%，在代码差异生成任务中表现更差，智能体框架在多轮评估中效果不如单轮。

Conclusion: 需要能够联合评估多轮真实世界编码工作流中正确性和安全性的基准测试。

Abstract: AI coding assistants powered by large language models (LLMs) have transformed
software development, significantly boosting productivity. While existing
benchmarks evaluate the correctness and security of LLM-generated code, they
are typically limited to single-turn tasks that do not reflect the iterative
nature of real-world development. We introduce MT-Sec, the first benchmark to
systematically evaluate both correctness and security in multi-turn coding
scenarios. We construct this using a synthetic data pipeline that transforms
existing single-turn tasks into semantically aligned multi-turn interaction
sequences, allowing reuse of original test suites while modeling the complexity
of real-world coding processes. We evaluate 32 open- and closed-source models,
and three agent-scaffolding on MT-Sec and observe a consistent 20-27% drop in
"correct and secure" outputs from single-turn to multi-turn settings -- even
among state-of-the-art models. Beyond full-program generation, we also evaluate
models on multi-turn code-diff generation -- an unexplored yet practically
relevant setting -- and find that models perform worse here, with increased
rates of functionally incorrect and insecure outputs. Finally, we find that
while agent scaffoldings boost single-turn code generation performance, they
are not quite as effective in multi-turn evaluations. Together, these findings
highlight the need for benchmarks that jointly evaluate correctness and
security in multi-turn, real-world coding workflows.

</details>


### [3] [A11YN: aligning LLMs for accessible web UI code generation](https://arxiv.org/abs/2510.13914)
*Janghan Yoon,Jaegwan Cho,Junhyeok Kim,Jiwan Chung,Jaehyun Jeon,Youngjae Yu*

Main category: cs.SE

TL;DR: A11yn是一种将代码生成LLM对齐以可靠生成符合可访问性标准的网页UI的方法，通过优化惩罚WCAG违规的奖励函数，显著降低了不可访问率60%


<details>
  <summary>Details</summary>
Motivation: 现有LLM在生成网页界面时经常复制训练数据中的可访问性缺陷，导致界面排斥有不同需求和背景的用户

Method: A11yn优化了一个新颖的奖励函数，该函数根据可访问性测试引擎识别的违规严重程度来惩罚WCAG违规行为，并构建了UIReq-6.8K数据集进行训练

Result: A11yn显著优于强基线，将不可访问率比基础模型降低了60%，同时保持了生成UI的语义保真度和视觉质量

Conclusion: 可访问性可以在LLM中系统优化，证明了为可访问性对齐代码生成的可行性

Abstract: Large language models (LLMs) have recently demonstrated strong capabilities
in generating functional and aesthetic web interfaces directly from
instructions. However, these models often replicate accessibility flaws from
their training data, resulting in interfaces that exclude users with diverse
needs and contexts. To address this gap, we introduce A11yn, the first method
that aligns code-generating LLMs to reliably produce accessibility-compliant
web UIs. A11yn optimizes a novel reward function that penalizes violations of
the Web Content Accessibility Guidelines (WCAG), with penalties scaled to the
severity of each violation as identified by an accessibility testing engine. To
support training, we construct UIReq-6.8K, a dataset of 6,800 diverse
instructions for web UI generation. For evaluation, we introduce RealUIReq-300,
a benchmark of 300 real-world web UI requests grounded and manually curated
from public web pages, spanning a broad range of use cases. Empirical results
show that A11yn significantly outperforms strong baselines, lowering the
Inaccessibility Rate by 60% over the base model while preserving semantic
fidelity and visual quality of generated UIs. These findings demonstrate that
accessibility can be systematically optimized within LLMs, showing the
feasibility of aligning code generation for accessibility.

</details>


### [4] [Signature in Code Backdoor Detection, how far are we?](https://arxiv.org/abs/2510.13992)
*Quoc Hung Le,Thanh Le-Cong,Bach Le,Bowen Xu*

Main category: cs.SE

TL;DR: 重新评估基于谱签名防御方法在代码模型后门攻击检测中的适用性，发现传统设置往往不是最优的，并提出了新的代理指标来更准确估计防御性能。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型在软件开发中的广泛应用，它们成为对抗性攻击的主要目标，其中后门攻击是一个重大威胁。虽然谱签名防御方法在神经网络中用于检测中毒数据，但最近研究表明这些方法在代码模型中可能效果不佳。

Method: 系统评估谱签名防御在不同攻击场景和防御配置下的有效性，分析其优势和局限性，并探索关键因素的不同设置影响。

Result: 发现代码后门检测中广泛使用的谱签名设置通常不是最优的，并发现了一个新的代理指标，可以在防御后无需模型重新训练的情况下更准确地估计谱签名的实际性能。

Conclusion: 谱签名防御在代码模型后门检测中的应用需要重新评估和优化设置，新发现的代理指标为更有效的防御评估提供了可能。

Abstract: As Large Language Models (LLMs) become increasingly integrated into software
development workflows, they also become prime targets for adversarial attacks.
Among these, backdoor attacks are a significant threat, allowing attackers to
manipulate model outputs through hidden triggers embedded in training data.
Detecting such backdoors remains a challenge, and one promising approach is the
use of Spectral Signature defense methods that identify poisoned data by
analyzing feature representations through eigenvectors. While some prior works
have explored Spectral Signatures for backdoor detection in neural networks,
recent studies suggest that these methods may not be optimally effective for
code models. In this paper, we revisit the applicability of Spectral
Signature-based defenses in the context of backdoor attacks on code models. We
systematically evaluate their effectiveness under various attack scenarios and
defense configurations, analyzing their strengths and limitations. We found
that the widely used setting of Spectral Signature in code backdoor detection
is often suboptimal. Hence, we explored the impact of different settings of the
key factors. We discovered a new proxy metric that can more accurately estimate
the actual performance of Spectral Signature without model retraining after the
defense.

</details>


### [5] [One Bug, Hundreds Behind: LLMs for Large-Scale Bug Discovery](https://arxiv.org/abs/2510.14036)
*Qiushi Wu,Yue Xiao,Dhilung Kirat,Kevin Eykholt,Jiyong Jang,Douglas Lee Schales*

Main category: cs.SE

TL;DR: BugStone是一个基于LLVM和大型语言模型的程序分析系统，用于识别和修复软件中重复出现的模式错误(RPBs)，在Linux内核中发现了22K多个潜在问题，准确率达到92.2%。


<details>
  <summary>Details</summary>
Motivation: 大型程序中的bug修复耗时耗力，且已修复的bug可能在其他代码段中重复出现，这些重复模式错误(RPBs)会扩大攻击面，威胁软件安全。

Method: 利用LLVM和大型语言模型分析程序，通过已修复的bug实例识别一致的错误模式(如特定API误用)，然后在整个程序中搜索相似模式来发现潜在漏洞。

Result: 从135个独特RPBs开始，BugStone在Linux内核中识别了22K+新潜在问题，手动验证400个发现中246个有效；在1.9K安全bug数据集上达到92.2%精确度和79.1%成对准确率。

Conclusion: RPBs在软件中普遍存在且严重影响安全性，BugStone系统能有效识别这些重复错误模式，为软件安全提供重要保障。

Abstract: Fixing bugs in large programs is a challenging task that demands substantial
time and effort. Once a bug is found, it is reported to the project
maintainers, who work with the reporter to fix it and eventually close the
issue. However, across the program, there are often similar code segments,
which may also contain the bug, but were missed during discovery. Finding and
fixing each recurring bug instance individually is labor intensive. Even more
concerning, bug reports can inadvertently widen the attack surface as they
provide attackers with an exploitable pattern that may be unresolved in other
parts of the program.
  In this paper, we explore these Recurring Pattern Bugs (RPBs) that appear
repeatedly across various code segments of a program or even in different
programs, stemming from a same root cause, but are unresolved. Our
investigation reveals that RPBs are widespread and can significantly compromise
the security of software programs. This paper introduces BugStone, a program
analysis system empowered by LLVM and a Large Language Model (LLM). The key
observation is that many RPBs have one patched instance, which can be leveraged
to identify a consistent error pattern, such as a specific API misuse. By
examining the entire program for this pattern, it is possible to identify
similar sections of code that may be vulnerable. Starting with 135 unique RPBs,
BugStone identified more than 22K new potential issues in the Linux kernel.
Manual analysis of 400 of these findings confirmed that 246 were valid. We also
created a dataset from over 1.9K security bugs reported by 23 recent top-tier
conference works. We manually annotate the dataset, identify 80 recurring
patterns and 850 corresponding fixes. Even with a cost-efficient model choice,
BugStone achieved 92.2% precision and 79.1% pairwise accuracy on the dataset.

</details>


### [6] [David vs. Goliath: A comparative study of different-sized LLMs for code generation in the domain of automotive scenario generation](https://arxiv.org/abs/2510.14115)
*Philipp Bauerfeind,Amir Salarpour,David Fernandez,Pedram MohajerAnsari,Johannes Reschke,Mert D. Pesé*

Main category: cs.SE

TL;DR: NL2Scenic是一个用于评估从自然语言生成Scenic自动驾驶场景代码的数据集和框架，包含146个NL/Scenic对，评估了13个模型，发现GPT-4o表现最佳，中等规模开源模型是实用且经济的选择。


<details>
  <summary>Details</summary>
Motivation: 解决现有NL-to-Scenic生成中数据稀缺、可复现性差和评估指标不一致的问题，为自动驾驶场景编程提供标准化评估基准。

Method: 构建包含146个NL/Scenic对的数据集，设计30个难度分层的测试用例，开发示例检索器，使用14种提示变体，评估13个模型（4个专有模型和9个开源代码模型），采用文本指标和执行指标，并与专家研究对比。

Result: EDIT-SIM与人类判断相关性最好，提出的EDIT-COMP指标提高了排名保真度；GPT-4o整体表现最佳，Qwen2.5Coder-14B达到其专家分数的88%；检索增强提示持续提升小模型性能；中等规模以上模型收益递减；Qwen2.5Coder在可比规模下优于CodeLlama。

Conclusion: NL2Scenic和EDIT-COMP为Scenic代码生成提供了标准化、可复现的评估基础，表明中等规模开源模型是自动驾驶场景编程的实用且经济高效的选择。

Abstract: Scenario simulation is central to testing autonomous driving systems. Scenic,
a domain-specific language (DSL) for CARLA, enables precise and reproducible
scenarios, but NL-to-Scenic generation with large language models (LLMs)
suffers from scarce data, limited reproducibility, and inconsistent metrics. We
introduce NL2Scenic, an open dataset and framework with 146 NL/Scenic pairs, a
difficulty-stratified 30-case test split, an Example Retriever, and 14
prompting variants (ZS, FS, CoT, SP, MoT). We evaluate 13 models: four
proprietary (GPT-4o, GPT-5, Claude-Sonnet-4, Gemini-2.5-pro) and nine
open-source code models (Qwen2.5Coder 0.5B-32B; CodeLlama 7B/13B/34B), using
text metrics (BLEU, ChrF, EDIT-SIM, CrystalBLEU) and execution metrics
(compilation and generation), and compare them with an expert study (n=11).
EDIT-SIM correlates best with human judgments; we also propose EDIT-COMP (F1 of
EDIT-SIM and compilation) as a robust dataset-level proxy that improves ranking
fidelity. GPT-4o performs best overall, while Qwen2.5Coder-14B reaches about 88
percent of its expert score on local hardware. Retrieval-augmented prompting,
Few-Shot with Example Retriever (FSER), consistently boosts smaller models, and
scaling shows diminishing returns beyond mid-size, with Qwen2.5Coder
outperforming CodeLlama at comparable scales. NL2Scenic and EDIT-COMP offer a
standardized, reproducible basis for evaluating Scenic code generation and
indicate that mid-size open-source models are practical, cost-effective options
for autonomous-driving scenario programming.

</details>


### [7] [Caruca: Effective and Efficient Specification Mining for Opaque Software Components](https://arxiv.org/abs/2510.14279)
*Evangelos Lamprou,Seong-Heon Jung,Mayank Keoliya,Lukas Lazarek,Konstantinos Kallas,Michael Greenberg,Nikos Vasilakis*

Main category: cs.SE

TL;DR: Caruca是一个自动为不透明命令挖掘规范的系统，通过使用大语言模型将文档转换为结构化调用语法，探索命令执行空间，提取并行性和文件系统条件等关键属性，消除了手动规范创建的需求。


<details>
  <summary>Details</summary>
Motivation: 现有系统需要手动创建命令规范，这个过程既费力又容易出错，限制了这些系统的实用性。

Method: 使用大语言模型将命令文档转换为结构化调用语法，探索语法有效的命令调用和执行环境，通过系统调用和文件系统级别的拦截来提取命令属性。

Result: 在60个GNU Coreutils、POSIX和第三方命令上测试，Caruca为除一个案例外的所有情况生成了正确的规范，完全消除了手动工作。

Conclusion: Caruca能够自动生成准确的命令规范，为依赖规范的系统提供了实用的解决方案。

Abstract: A wealth of state-of-the-art systems demonstrate impressive improvements in
performance, security, and reliability on programs composed of opaque
components, such as Unix shell commands. To reason about commands, these
systems require partial specifications. However, creating such specifications
is a manual, laborious, and error-prone process, limiting the practicality of
these systems. This paper presents Caruca, a system for automatic specification
mining for opaque commands. To overcome the challenge of language diversity
across commands, Caruca first instruments a large language model to translate a
command's user-facing documentation into a structured invocation syntax. Using
this representation, Caruca explores the space of syntactically valid command
invocations and execution environments. Caruca concretely executes each
command-environment pair, interposing at the system-call and filesystem level
to extract key command properties such as parallelizability and filesystem pre-
and post-conditions. These properties can be exported in multiple specification
formats and are immediately usable by existing systems. Applying Caruca across
60 GNU Coreutils, POSIX, and third-party commands across several
specification-dependent systems shows that Caruca generates correct
specifications for all but one case, completely eliminating manual effort from
the process and currently powering the full specifications for a
state-of-the-art static analysis tool.

</details>


### [8] [A Hybrid, Knowledge-Guided Evolutionary Framework for Personalized Compiler Auto-Tuning](https://arxiv.org/abs/2510.14292)
*Haolin Pan,Hongbin Zhang,Mingjie Xing,Yanjun Wu*

Main category: cs.SE

TL;DR: 提出了一种混合知识引导的进化框架，通过离线构建编译知识库和在线使用知识增强的遗传算法，为特定程序自动寻找最优的编译器优化序列。


<details>
  <summary>Details</summary>
Motivation: 传统编译器优化标志（如-O3、-Oz）采用一刀切的方法，无法充分发挥程序的性能潜力。编译器优化序列选择是一个NP难问题，需要更智能的个性化优化方法。

Method: 1. 离线阶段构建编译知识库：包括优化行为向量、基于行为相似性的优化分组、协同优化图和原型优化序列库。2. 在线阶段使用知识增强的遗传算法，通过语义感知的重组和定向修复变异进行搜索。

Result: 在7个公共数据集上，相比高度优化的opt -Oz基准，平均额外减少了11.0%的LLVM IR指令，展示了在发现个性化高性能优化序列方面的先进能力。

Conclusion: 该混合知识引导进化框架能够有效发现个性化的高性能编译器优化序列，显著优于传统优化方法，为编译器优化自动化提供了新的解决方案。

Abstract: Compiler pass auto-tuning is critical for enhancing software performance, yet
finding the optimal pass sequence for a specific program is an NP-hard problem.
Traditional, general-purpose optimization flags like -O3 and -Oz adopt a
one-size-fits-all approach, often failing to unlock a program's full
performance potential. To address this challenge, we propose a novel Hybrid,
Knowledge-Guided Evolutionary Framework. This framework intelligently guides
online, personalized optimization using knowledge extracted from a large-scale
offline analysis phase. During the offline stage, we construct a comprehensive
compilation knowledge base composed of four key components: (1) Pass Behavioral
Vectors to quantitatively capture the effectiveness of each optimization; (2)
Pass Groups derived from clustering these vectors based on behavior similarity;
(3) a Synergy Pass Graph to model beneficial sequential interactions; and (4) a
library of Prototype Pass Sequences evolved for distinct program types. In the
online stage, a bespoke genetic algorithm leverages this rich knowledge base
through specially designed, knowledge-infused genetic operators. These
operators transform the search by performing semantically-aware recombination
and targeted, restorative mutations. On a suite of seven public datasets, our
framework achieves an average of 11.0% additional LLVM IR instruction reduction
over the highly-optimized opt -Oz baseline, demonstrating its state-of-the-art
capability in discovering personalized, high-performance optimization
sequences.

</details>


### [9] [A Systematic Study of Time Limit Exceeded Errors in Online Programming Assignments](https://arxiv.org/abs/2510.14339)
*Jialu Zhang,Jialiang Gu,Wangmeiyu Zhang,José Pablo Cambronero,John Kolesar,Ruzica Piskac,Daming Li,Hanyuan Shi*

Main category: cs.SE

TL;DR: 本文提出了第一个针对在线编程中TLE错误的大规模实证研究，开发了首个自动化修复工具Nettle和评估框架Nettle-Eval，在1000个真实案例中达到98.5%的修复率。


<details>
  <summary>Details</summary>
Motivation: 在线编程平台上的TLE错误难以解决，错误信息缺乏诊断价值，平台支持有限，现有调试工具帮助不大，导致许多用户在反复失败后放弃提交。

Method: 手动分析1000个Codeforces的TLE提交，分类根本原因；开发Nettle工具，结合LLM与编译器生成的针对性自动反馈和测试用例，产生小而正确的代码编辑。

Result: Nettle在1000个真实案例中达到98.5%的修复率，远超最强LLM基线，所有修复都通过了Nettle-Eval和平台官方检查器。

Conclusion: TLE错误不仅源于算法效率问题，还包括无限循环、数据结构使用不当和I/O效率问题；Nettle证明了自动化修复TLE错误的可行性，为在线编程平台提供了可靠解决方案。

Abstract: Online programming platforms such as Codeforces and LeetCode attract millions
of users seeking to learn to program or refine their skills for industry
interviews. A major challenge for these users is the Time Limit Exceeded (TLE)
error, triggered when a program exceeds the execution time bound. Although
designed as a performance safeguard, TLE errors are difficult to resolve: error
messages provide no diagnostic insight, platform support is minimal, and
existing debugging tools offer little help. As a result, many users abandon
their submissions after repeated TLE failures.
  This paper presents the first large-scale empirical study of TLE errors in
online programming. We manually analyzed 1000 Codeforces submissions with TLE
errors, classified their root causes, and traced how users attempted to fix
them. Our analysis shows that TLE errors often arise not only from inefficient
algorithms but also from infinite loops, improper data structure use, and
inefficient I/O, challenging the conventional view that TLEs are purely
performance issues.
  Guided by these findings, we introduce Nettle, the first automated repair
tool specifically designed for TLE errors, and Nettle-Eval, the first framework
for evaluating TLE repairs. Integrating LLMs with targeted automated feedback
generated by the compiler and test cases, Nettle produces small, correct code
edits that eliminate TLEs while preserving functionality. Evaluated on the same
1000 real-world cases, Nettle achieves a 98.5% fix rate, far exceeding the
strongest LLM baseline, and all of its repairs pass both Nettle-Eval and the
platform's official checker, confirming the reliability of our framework.

</details>


### [10] [PathFix: Automated Program Repair with Expected Path](https://arxiv.org/abs/2510.14341)
*Xu He,Shu Wang,Kun Sun*

Main category: cs.SE

TL;DR: PathFix是一种新的自动程序修复方法，利用从正确执行路径中提取的路径敏感约束来生成补丁，解决了现有APR方法生成过多候选补丁和过拟合测试用例的问题。


<details>
  <summary>Details</summary>
Motivation: 现有APR方法由于难以生成精确规范，面临生成过多合理补丁候选和过拟合部分测试用例的挑战。

Method: PathFix通过四个步骤工作：1)追踪到达故障输出的故障路径；2)分析控制流图上的期望路径；3)通过求解期望路径上的状态约束生成和评估补丁；4)验证生成补丁的正确性。还集成大语言模型提升性能。

Result: 实验结果显示PathFix优于现有解决方案，特别是在处理复杂程序结构（如循环和递归）方面表现突出。

Conclusion: PathFix通过路径敏感约束和LLM集成，有效解决了APR中的关键挑战，在复杂程序修复方面具有优势。

Abstract: Automated program repair (APR) techniques are effective in fixing inevitable
defects in software, enhancing development efficiency and software robustness.
However, due to the difficulty of generating precise specifications, existing
APR methods face two main challenges: generating too many plausible patch
candidates and overfitting them to partial test cases. To tackle these
challenges, we introduce a new APR method named PathFix, which leverages
path-sensitive constraints extracted from correct execution paths to generate
patches for repairing buggy code. It is based on one observation: if a buggy
program is repairable, at least one expected path is supposed to replace the
fault path in the patched program. PathFix operates in four main steps. First,
it traces fault paths reaching the fault output in the buggy program. Second,
it derives expected paths by analyzing the desired correct output on the
control flow graph, where an expected path defines how a feasible patch leads
to the correct execution. Third, PathFix generates and evaluates patches by
solving state constraints along the expected path. Fourth, we validate the
correctness of the generated patch. To further enhance repair performance and
mitigate scalability issues introduced by path-sensitive analysis, we integrate
a large language model (LLM) into our framework. Experimental results show that
PathFix outperforms existing solutions, particularly in handling complex
program structures such as loops and recursion.

</details>


### [11] [Towards Automated Governance: A DSL for Human-Agent Collaboration in Software Projects](https://arxiv.org/abs/2510.14465)
*Adem Ait,Gwendal Jouneaux,Javier Luis Cánovas Izquierdo,Jordi Cabot*

Main category: cs.SE

TL;DR: 提出一种用于定义和执行涉及多样化利益相关者（包括AI代理）的软件治理政策的领域特定语言（DSL），旨在解决开源软件项目中治理政策缺乏或不明确的问题。


<details>
  <summary>Details</summary>
Motivation: 软件开发的利益相关者日益多样化，包括来自不同背景的人类贡献者和AI代理，这给开源软件项目带来了独特的治理挑战，因为明确的政策常常缺失或不清晰。

Method: 设计一种新颖的领域特定语言（DSL），用于定义和执行涉及多样化利益相关者（包括代理）的丰富治理政策。

Result: 该DSL为实现更强大、适应性更强且最终自动化的治理提供了途径。

Conclusion: 这种DSL为软件项目（特别是开源项目）中更有效的协作铺平了道路，能够支持涉及多样化利益相关者（包括AI代理）的治理政策定义和执行。

Abstract: The stakeholders involved in software development are becoming increasingly
diverse, with both human contributors from varied backgrounds and AI-powered
agents collaborating together in the process. This situation presents unique
governance challenges, particularly in Open-Source Software (OSS) projects,
where explicit policies are often lacking or unclear. This paper presents the
vision and foundational concepts for a novel Domain-Specific Language (DSL)
designed to define and enforce rich governance policies in systems involving
diverse stakeholders, including agents. This DSL offers a pathway towards more
robust, adaptable, and ultimately automated governance, paving the way for more
effective collaboration in software projects, especially OSS ones.

</details>


### [12] [E2Edev: Benchmarking Large Language Models in End-to-End Software Development Task](https://arxiv.org/abs/2510.14509)
*Jingyao Liu,Chen Huang,Zhizhao Guan,Wenqiang Lei,Yang Deng*

Main category: cs.SE

TL;DR: E2EDev是一个端到端软件开发基准，包含细粒度用户需求、BDD测试场景和自动化测试流水线，通过人机协同多智能体标注框架减少标注工作量。评估显示现有E2ESD框架在解决这些任务时仍面临困难。


<details>
  <summary>Details</summary>
Motivation: 解决端到端软件开发中缺乏有效基准的问题，同时减少高质量标注所需的人工工作量。

Method: 提出E2EDev基准，包含用户需求、BDD测试场景和自动化测试流水线，并采用人机协同多智能体标注框架(HITL-MAA)来保证质量并减少标注工作量。

Result: 评估各种E2ESD框架和LLM骨干网络后发现，现有方法在有效解决这些任务方面仍存在困难。

Conclusion: 端到端软件开发需要更有效和成本效益更高的解决方案，E2EDev基准为此提供了评估基础。

Abstract: E2EDev comprises (i) a fine-grained set of user requirements, (ii) {multiple
BDD test scenarios with corresponding Python step implementations for each
requirement}, and (iii) a fully automated testing pipeline built on the Behave
framework. To ensure its quality while reducing the annotation effort, E2EDev
leverages our proposed Human-in-the-Loop Multi-Agent Annotation Framework
(HITL-MAA). {By evaluating various E2ESD frameworks and LLM backbones with
E2EDev}, our analysis reveals a persistent struggle to effectively solve these
tasks, underscoring the critical need for more effective and cost-efficient
E2ESD solutions. Our codebase and benchmark are publicly available at
https://github.com/SCUNLP/E2EDev.

</details>


### [13] [Software Testing Education and Industry Needs - Report from the ENACTEST EU Project](https://arxiv.org/abs/2510.14625)
*Mehrdad Saadatmand,Abbas Khan,Beatriz Marin,Ana C. R Paiva,Nele Van Asch,Graham Moran,Felix Cammaerts,Monique Snoeck,Alexandra Mendes*

Main category: cs.SE

TL;DR: 本研究调查了软件测试行业的能力需求，识别了当前测试教育的知识缺口，并通过焦点小组、访谈和范围综述方法分析了学术界未充分覆盖的能力和差距。


<details>
  <summary>Details</summary>
Motivation: 软件开发的不断演变要求测试人员持续适应新工具和实践，本研究旨在了解行业测试能力需求与教育供给之间的差距。

Method: 采用焦点小组讨论、行业专业人士访谈和范围综述方法，研究工具由ENACTEST项目联盟共同设计并经过多次迭代完善。

Result: 研究发现行业在AI测试、安全测试和软技能等方面存在知识缺口，并报告了专业培训方法、培训挑战、培训质量评估等方面的观察结果。

Conclusion: 软件测试教育需要更好地满足行业需求，特别是在新兴技术领域和软技能方面，需要加强学术界与工业界的知识转移。

Abstract: The evolving landscape of software development demands that software testers
continuously adapt to new tools, practices, and acquire new skills. This study
investigates software testing competency needs in industry, identifies
knowledge gaps in current testing education, and highlights competencies and
gaps not addressed in academic literature. This is done by conducting two focus
group sessions and interviews with professionals across diverse domains,
including railway industry, healthcare, and software consulting and performing
a curated small-scale scoping review. The study instrument, co-designed by
members of the ENACTEST project consortium, was developed collaboratively and
refined through multiple iterations to ensure comprehensive coverage of
industry needs and educational gaps. In particular, by performing a thematic
qualitative analysis, we report our findings and observations regarding:
professional training methods, challenges in offering training in industry,
different ways of evaluating the quality of training, identified knowledge gaps
with respect to academic education and industry needs, future needs and trends
in testing education, and knowledge transfer methods within companies. Finally,
the scoping review results confirm knowledge gaps in areas such as AI testing,
security testing and soft skills.

</details>


### [14] [ATGen: Adversarial Reinforcement Learning for Test Case Generation](https://arxiv.org/abs/2510.14635)
*Qingyao Li,Xinyi Dai,Weiwen Liu,Xiangyang Li,Yasheng Wang,Ruiming Tang,Yong Yu,Weinan Zhang*

Main category: cs.SE

TL;DR: ATGen是一个通过对抗性强化学习训练测试用例生成的框架，突破了静态数据集的固定难度限制，能够发现更复杂的新bug


<details>
  <summary>Details</summary>
Motivation: 现有测试生成方法依赖静态数据集，存在'固定难度天花板'限制，无法发现超出训练范围的新颖或更复杂bug

Method: 使用对抗性强化学习框架，让测试生成器与对抗性代码生成器对抗，后者持续制造更难bug来逃避当前策略，形成难度递增的课程学习

Result: ATGen显著优于现有最先进基线，作为Best-of-N推理的更有效过滤器，以及训练代码生成模型的更高质量奖励源

Conclusion: 这项工作为提升LLM生成代码的可靠性建立了一个新的动态范式

Abstract: Large Language Models (LLMs) excel at code generation, yet their outputs
often contain subtle bugs, for which effective test cases are a critical
bottleneck. Existing test generation methods, whether based on prompting or
supervised fine-tuning, rely on static datasets. This imposes a
``fixed-difficulty ceiling'', fundamentally limiting their ability to uncover
novel or more complex bugs beyond their training scope. To overcome this, we
introduce ATGen, a framework that trains a test case generator via adversarial
reinforcement learning. ATGen pits a test generator against an adversarial code
generator that continuously crafts harder bugs to evade the current policy.
This dynamic loop creates a curriculum of increasing difficulty challenging
current policy. The test generator is optimized via Reinforcement Learning (RL)
to jointly maximize ``Output Accuracy'' and ``Attack Success'', enabling it to
learn a progressively stronger policy that breaks the fixed-difficulty ceiling
of static training. Extensive experiments demonstrate that ATGen significantly
outperforms state-of-the-art baselines. We further validate its practical
utility, showing it serves as both a more effective filter for Best-of-N
inference and a higher-quality reward source for training code generation
models. Our work establishes a new, dynamic paradigm for improving the
reliability of LLM-generated code.

</details>


### [15] [Requirement Identification for Traffic Simulations in Driving Simulators](https://arxiv.org/abs/2510.14653)
*Sven Tarlowski,Lutz Eckstein*

Main category: cs.SE

TL;DR: 提出一种系统识别交通仿真需求的方法论，通过分阶段子目标确保仿真真实性，支持汽车开发测试。


<details>
  <summary>Details</summary>
Motivation: 解决确保交通仿真真实性的挑战，提升实验结果的效度和参与者参与度。

Method: 基于研究阶段子目标的结构化方法，从微观层面、智能体模型和视觉表示推导具体技术需求。

Result: 建立了研究目标与交通仿真设计之间的清晰联系，支持稳健的汽车开发和测试。

Conclusion: 该方法论能保持高保真度，增强实验有效性和参与者参与，为交通仿真提供系统性指导。

Abstract: This paper addresses the challenge of ensuring realistic traffic conditions
by proposing a methodology that systematically identifies traffic simulation
requirements. Using a structured approach based on sub-goals in each study
phase, specific technical needs are derived for microscopic levels, agent
models, and visual representation. The methodology aims to maintain a high
degree of fidelity, enhancing both the validity of experimental outcomes and
participant engagement. By providing a clear link between study objectives and
traffic simulation design, this approach supports robust automotive development
and testing.

</details>


### [16] [LLM Agents for Automated Web Vulnerability Reproduction: Are We There Yet?](https://arxiv.org/abs/2510.14700)
*Bin Liu,Yanjie Zhao,Guoai Xu,Haoyu Wang*

Main category: cs.SE

TL;DR: 对20个最先进的LLM代理在自动Web漏洞复现方面进行了首次全面评估，发现虽然LLM代理在简单库漏洞上表现良好，但在需要多组件环境的复杂服务漏洞上持续失败。


<details>
  <summary>Details</summary>
Motivation: LLM代理在软件工程和网络安全任务中表现出色，但自动Web漏洞复现这一关键应用尚未充分探索，需要系统评估其在真实场景中的能力。

Method: 系统评估20个来自软件工程、网络安全和通用领域的LLM代理，在16个维度上评估技术能力、环境适应性和用户体验，并在80个真实CVE数据集上进行深入评估。

Result: LLM代理在简单库漏洞上取得合理成功，但在复杂服务漏洞上失败；环境配置和认证障碍导致代理能执行利用代码但无法触发实际漏洞；输入指导敏感性高，认证信息不完整时性能下降超过33%。

Conclusion: 当前LLM代理能力与可靠自动漏洞复现需求之间存在显著差距，需要在环境适应和自主问题解决能力方面取得进展。

Abstract: Large language model (LLM) agents have demonstrated remarkable capabilities
in software engineering and cybersecurity tasks, including code generation,
vulnerability discovery, and automated testing. One critical but underexplored
application is automated web vulnerability reproduction, which transforms
vulnerability reports into working exploits. Although recent advances suggest
promising potential, challenges remain in applying LLM agents to real-world web
vulnerability reproduction scenarios. In this paper, we present the first
comprehensive evaluation of state-of-the-art LLM agents for automated web
vulnerability reproduction. We systematically assess 20 agents from software
engineering, cybersecurity, and general domains across 16 dimensions, including
technical capabilities, environment adaptability, and user experience factors,
on 3 representative web vulnerabilities. Based on the results, we select three
top-performing agents (OpenHands, SWE-agent, and CAI) for in-depth evaluation
on our benchmark dataset of 80 real-world CVEs spanning 7 vulnerability types
and 6 web technologies. Our results reveal that while LLM agents achieve
reasonable success on simple library-based vulnerabilities, they consistently
fail on complex service-based vulnerabilities requiring multi-component
environments. Complex environment configurations and authentication barriers
create a gap where agents can execute exploit code but fail to trigger actual
vulnerabilities. We observe high sensitivity to input guidance, with
performance degrading by over 33% under incomplete authentication information.
Our findings highlight the significant gap between current LLM agent
capabilities and the demands of reliable automated vulnerability reproduction,
emphasizing the need for advances in environmental adaptation and autonomous
problem-solving capabilities.

</details>


### [17] [Leveraging Code Cohesion Analysis to Identify Source Code Supply Chain Attacks](https://arxiv.org/abs/2510.14778)
*Maor Reuben,Ido Mendel,Or Feldman,Moshe Kravchik,Mordehai Guri,Rami Puzis*

Main category: cs.SE

TL;DR: 提出一种基于名称预测的内聚性度量方法，通过量化源代码中的内聚性破坏来检测恶意代码注入，在极端不平衡测试集上表现良好。


<details>
  <summary>Details</summary>
Motivation: 供应链攻击通过合法项目中注入恶意代码严重威胁软件安全，这类攻击虽然罕见但破坏性极大，自动检测需要理解插入代码及其上下文的意图。

Method: 使用基于名称预测的内聚性(NPC)度量方法，分析恶意代码引入时函数内聚性的变化，并与自然内聚性波动进行比较。

Result: 对369个开源C++仓库的54,707个函数分析显示，代码注入会降低内聚性并使命名模式转向更短、描述性更差的名称。在1:1,000和1:10,000的极端不平衡比例下，Precision@100分别达到36.41%和12.47%。

Conclusion: 自动内聚性测量，特别是基于名称预测的内聚性度量，可能有助于识别供应链攻击，提高源代码完整性。

Abstract: Supply chain attacks significantly threaten software security with malicious
code injections within legitimate projects. Such attacks are very rare but may
have a devastating impact. Detecting spurious code injections using automated
tools is further complicated as it often requires deciphering the intention of
both the inserted code and its context. In this study, we propose an
unsupervised approach for highlighting spurious code injections by quantifying
cohesion disruptions in the source code. Using a name-prediction-based cohesion
(NPC) metric, we analyze how function cohesion changes when malicious code is
introduced compared to natural cohesion fluctuations. An analysis of 54,707
functions over 369 open-source C++ repositories reveals that code injection
reduces cohesion and shifts naming patterns toward shorter, less descriptive
names compared to genuine function updates. Considering the sporadic nature of
real supply-chain attacks, we evaluate the proposed method with extreme
test-set imbalance and show that monitoring high-cohesion functions with NPC
can effectively detect functions with injected code, achieving a Precision@100
of 36.41% at a 1:1,000 ratio and 12.47% at 1:10,000. These results suggest that
automated cohesion measurements, in general, and name-prediction-based
cohesion, in particular, may help identify supply chain attacks, improving
source code integrity.

</details>


### [18] [Instruction Set Migration at Warehouse Scale](https://arxiv.org/abs/2510.14928)
*Eric Christopher,Kevin Crossan,Wolff Dobson,Chris Kennelly,Drew Lewis,Kun Lin,Martin Maas,Parthasarathy Ranganathan,Emma Rapati,Brian Yang*

Main category: cs.SE

TL;DR: 本文分析了从x86到Arm架构的大规模代码迁移挑战，发现现代ISA迁移主要依赖重新编译而非二进制翻译，并展示了Google如何自动化这些任务以及AI在其中的作用。


<details>
  <summary>Details</summary>
Motivation: 传统观点认为ISA迁移的主要挑战是二进制翻译，但现代迁移可以基于开源生态系统重新编译软件，这带来了新的多方面挑战。

Method: 通过分析Google从x86到Arm的大规模迁移（涉及近40,000个代码提交），建立了ISA迁移的任务分类体系，展示了自动化方法和AI的应用。

Result: 识别了ISA迁移中的各类任务，证明了自动化解决方案的可行性，并展示了AI在自动处理这些任务中的重要作用。

Conclusion: 现代ISA迁移面临不同于二进制翻译的新挑战，自动化工具和AI技术能够显著提升迁移效率，但仍有一些挑战性任务需要进一步研究。

Abstract: Migrating codebases from one instruction set architecture (ISA) to another is
a major engineering challenge. A recent example is the adoption of Arm (in
addition to x86) across the major Cloud hyperscalers. Yet, this problem has
seen limited attention by the academic community. Most work has focused on
static and dynamic binary translation, and the traditional conventional wisdom
has been that this is the primary challenge.
  In this paper, we show that this is no longer the case. Modern ISA migrations
can often build on a robust open-source ecosystem, making it possible to
recompile all relevant software from scratch. This introduces a new and
multifaceted set of challenges, which are different from binary translation.
  By analyzing a large-scale migration from x86 to Arm at Google, spanning
almost 40,000 code commits, we derive a taxonomy of tasks involved in ISA
migration. We show how Google automated many of the steps involved, and
demonstrate how AI can play a major role in automatically addressing these
tasks. We identify tasks that remain challenging and highlight research
challenges that warrant further attention.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [19] [Noisy Networks, Nosy Neighbors: Inferring Privacy Invasive Information from Encrypted Wireless Traffic](https://arxiv.org/abs/2510.13822)
*Bartosz Burgiel*

Main category: cs.CR

TL;DR: 通过被动观察智能家居的无线流量，即使数据加密，也能推断出住户的隐私信息，包括设备活动、日常行为和公寓布局。


<details>
  <summary>Details</summary>
Motivation: 探讨智能家居环境中，仅通过被动观察加密的无线流量，邻居能获取多少隐私侵入性信息。

Method: 模拟邻居在相邻公寓的能力，分析802.11原始数据包和蓝牙低功耗广告，识别设备、推断活动状态，并使用基于RSSI的三边测量法近似定位。

Result: 能够检测多媒体设备的活跃期，推断睡眠、工作和媒体消费等常见活动，甚至近似邻居公寓的布局。

Conclusion: 智能家居的隐私风险超出传统数据泄露：邻居仅通过加密网络流量就能获得隐私侵入性洞察。

Abstract: This thesis explores the extent to which passive observation of wireless
traffic in a smart home environment can be used to infer privacy-invasive
information about its inhabitants. Using a setup that mimics the capabilities
of a nosy neighbor in an adjacent flat, we analyze raw 802.11 packets and
Bluetooth Low Energy advertisemets. From this data, we identify devices, infer
their activity states and approximate their location using RSSI-based
trilateration. Despite the encrypted nature of the data, we demonstrate that it
is possible to detect active periods of multimedia devices, infer common
activities such as sleeping, working and consuming media, and even approximate
the layout of the neighbor's apartment. Our results show that privacy risks in
smart homes extend beyond traditional data breaches: a nosy neighbor behind the
wall can gain privacy-invasive insights into the lives of their neighbors
purely from encrypted network traffic.

</details>


### [20] [Multi-Layer Secret Sharing for Cross-Layer Attack Defense in 5G Networks: a COTS UE Demonstration](https://arxiv.org/abs/2510.13824)
*Wai Ming Chan,Remi Chou,Taejoon Kim*

Main category: cs.CR

TL;DR: 在商用5G用户设备上实现无需基础设施修改或预共享密钥的多层秘密共享系统，通过XOR方法在运营商和中继节点间分发秘密份额，确保即使一个运营商和一个中继同时失效也能完美恢复数据。


<details>
  <summary>Details</summary>
Motivation: 解决5G网络中数据机密性和可用性问题，特别是在面对拒绝服务攻击或意外攻击时，确保关键数据的安全存储和恢复。

Method: 采用基于XOR的多层秘密共享方案，将秘密份额分发到不同网络运营商和分布式中继节点，无需修改基础设施或预共享密钥。

Result: 成功在商用5G用户设备上实现了该方案，能够在一家运营商和一个中继节点同时失效的情况下完美恢复原始数据。

Conclusion: 该方法为5G网络提供了强大的数据保护机制，增强了面对网络攻击时的韧性和数据机密性。

Abstract: This demo presents the first implementation of multi-layer secret sharing on
commercial-off-the-shelf (COTS) 5G user equipment (UE), operating without
infrastructure modifications or pre-shared keys. Our XOR-based approach
distributes secret shares across network operators and distributed relays,
ensuring perfect recovery and data confidentiality even if one network operator
and one relay are simultaneously lost (e.g., under denial of service (DoS) or
unanticipated attacks).

</details>


### [21] [A2AS: Agentic AI Runtime Security and Self-Defense](https://arxiv.org/abs/2510.13825)
*Eugene Neelou,Ivan Novikov,Max Moroz,Om Narayan,Tiffany Saade,Mika Ayenson,Ilya Kabanov,Jen Ozmen,Edward Lee,Vineeth Sai Narajala,Emmanuel Guilherme Junior,Ken Huang,Huseyin Gulsin,Jason Ross,Marat Vyshegorodtsev,Adelin Travers,Idan Habler,Rahul Jadav*

Main category: cs.CR

TL;DR: A2AS框架作为AI代理和LLM应用的安全层，类似于HTTPS保护HTTP。它通过BASIC安全模型实现行为认证、提示认证、安全边界、上下文防御和编码策略，提供深度防御策略。


<details>
  <summary>Details</summary>
Motivation: 随着AI代理和LLM应用的普及，需要类似HTTPS的安全框架来保护这些系统免受攻击，确保行为认证和上下文完整性。

Method: 提出A2AS框架和BASIC安全模型：行为证书(B)强制执行行为，认证提示(A)确保上下文完整性，安全边界(S)隔离不可信输入，上下文防御(I)保护模型推理，编码策略(C)应用特定规则。

Result: A2AS框架避免了延迟开销、外部依赖、架构变更、模型重训练和操作复杂性，为AI系统提供了轻量级的安全解决方案。

Conclusion: A2AS框架和BASIC安全模型有潜力成为行业标准，为AI代理和LLM应用提供全面的安全保护，这是该系列论文的第一篇介绍性文章。

Abstract: The A2AS framework is introduced as a security layer for AI agents and
LLM-powered applications, similar to how HTTPS secures HTTP. A2AS enforces
certified behavior, activates model self-defense, and ensures context window
integrity. It defines security boundaries, authenticates prompts, applies
security rules and custom policies, and controls agentic behavior, enabling a
defense-in-depth strategy. The A2AS framework avoids latency overhead, external
dependencies, architectural changes, model retraining, and operational
complexity. The BASIC security model is introduced as the A2AS foundation: (B)
Behavior certificates enable behavior enforcement, (A) Authenticated prompts
enable context window integrity, (S) Security boundaries enable untrusted input
isolation, (I) In-context defenses enable secure model reasoning, (C) Codified
policies enable application-specific rules. This first paper in the series
introduces the BASIC security model and the A2AS framework, exploring their
potential toward establishing the A2AS industry standard.

</details>


### [22] [PIShield: Detecting Prompt Injection Attacks via Intrinsic LLM Features](https://arxiv.org/abs/2510.14005)
*Wei Zou,Yupei Liu,Yanting Wang,Ying Chen,Neil Gong,Jinyuan Jia*

Main category: cs.CR

TL;DR: PIShield是一种高效且有效的提示注入攻击检测方法，通过提取LLM特定层中最后一个token的内部表示来训练线性分类器，在多个基准测试中显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: LLM集成应用容易受到提示注入攻击，现有检测方法性能不佳且计算开销高，需要开发更有效的防护方案。

Method: 利用关键发现：在特定注入关键层中，最后一个token的内部表示能区分干净和污染提示，基于此训练简单的线性分类器。

Result: 在5个基准数据集和8种提示注入攻击上比较11个基线方法，PIShield在效果和效率上都显著优于现有方法，并能抵抗强自适应攻击。

Conclusion: PIShield通过利用LLM内部表示的关键特征，实现了高效且有效的提示注入检测，为LLM安全提供了实用解决方案。

Abstract: LLM-integrated applications are vulnerable to prompt injection attacks, where
an attacker contaminates the input to inject malicious prompts, causing the LLM
to follow the attacker's intent instead of the original user's. Existing prompt
injection detection methods often have sub-optimal performance and/or high
computational overhead. In this work, we propose PIShield, a detection method
that is both effective and efficient. Our key observation is that the internal
representation of the final token in a prompt-extracted from a specific layer
of the LLM, which we term the injection-critical layer-captures distinguishing
features between clean and contaminated prompts. Leveraging this insight, we
train a simple linear classifier on these internal representations using a
labeled set of clean and contaminated prompts. We compare PIShield against 11
baselines across 5 diverse benchmark datasets and 8 prompt injection attacks.
The results demonstrate that PIShield is both highly effective and efficient,
substantially outperforming existing methods. Additionally, we show that
PIShield resists strong adaptive attacks.

</details>


### [23] [Quantitative Analysis of UAV Intrusion Mitigation for Border Security in 5G with LEO Backhaul Impairments](https://arxiv.org/abs/2510.14066)
*Rajendra Upadhyay,Al Nahian Bin Emran,Rajendra Paudyal,Lisa Donnan,Duminda Wijesekera*

Main category: cs.CR

TL;DR: 该论文提出了一个端到端仿真框架，用于分析混合地面-非地面5G系统中无人机入侵的检测到缓解延迟，强调了卫星回程中断对缓解延迟的影响以及本地回退机制的重要性。


<details>
  <summary>Details</summary>
Motivation: 解决非合作无人机在蜂窝网络中作为流氓用户设备带来的威胁，包括资源消耗、干扰和侵犯受限空域等问题。

Method: 构建包含地面gNB、卫星回程（具有随机中断）和检测逻辑的系统模型，使用蒙特卡洛方法扫描无人机高度、速度和卫星中断率。

Result: 卫星回程中断会导致任意长的缓解延迟；切换不稳定性对延迟影响可忽略；本地回退机制能有效限制缓解延迟；巡逻用户设备受影响很小。

Conclusion: 必须将非地面链路与本地控制相结合，以确保对非合作无人机入侵的稳健及时响应。

Abstract: Uncooperative unmanned aerial vehicles (UAVs) pose emerging threats to
critical infrastructure and border protection by operating as rogue user
equipment (UE) within cellular networks, consuming resources, creating
interference, and potentially violating restricted airspaces. This paper
presents minimal features of the operating space, yet an end-to-end simulation
framework to analyze detect-to-mitigate latency of such intrusions in a hybrid
terrestrial-non-terrestrial (LEO satellite) 5G system. The system model
includes terrestrial gNBs, satellite backhaul (with stochastic outages), and a
detection logic (triggered by handover instability and signal quality
variance). A lockdown mechanism is invoked upon detection, with optional local
fallback to cap mitigation delays. Monte Carlo sweeps across UAV altitudes,
speeds, and satellite outage rates yield several insights. First, satellite
backhaul outages can cause arbitrarily long mitigation delays, yet, to meet
fallback deadlines, they need to be effectively bounded. Second, while handover
instability was hypothesized, our results show that extra handovers have a
negligible effect within the range of parameters we considered. The main
benefit of resilience from fallback comes from the delay in limiting
mitigation. Third, patrol UEs experience negligible collateral impact, with
handover rates close to terrestrial baselines. Stress scenarios further
highlight that fallback is indispensable in preventing extreme control-plane
and physical security vulnerabilities: Without fallback, prolonged outages in
the satellite backhaul delay lockdown commands, allowing rogue UAVs to linger
inside restricted corridors for several seconds longer. These results
underscore the importance of complementing non-terrestrial links with local
control to ensure robust and timely response against uncooperative UAV
intrusions.

</details>


### [24] [Every Language Model Has a Forgery-Resistant Signature](https://arxiv.org/abs/2510.14086)
*Matthew Finlayson,Xiang Ren,Swabha Swayamdipta*

Main category: cs.CR

TL;DR: 本文提出了一种基于语言模型输出几何约束的椭圆签名方法，用于识别模型来源和验证输出真实性。该方法利用语言模型输出位于高维椭圆表面的特性作为模型签名，具有难以伪造、自然存在、自包含和紧凑冗余等独特优势。


<details>
  <summary>Details</summary>
Motivation: 随着闭源语言模型API的普及，需要开发能够提取隐藏模型细节和识别模型输出的取证方法。现有方法主要利用语言模型架构和参数施加的几何约束，但椭圆约束这一较少被探索的几何特性具有独特优势。

Method: 提出椭圆签名方法，利用语言模型输出位于高维椭圆表面的几何约束作为模型签名。开发了从小模型中提取椭圆的新技术，并讨论了在生产规模模型中应用的实际障碍。

Result: 椭圆签名具有难以伪造的特性（无模型参数访问权限时无法生成椭圆上的对数概率）、自然存在（所有语言模型都具有这种椭圆约束）、自包含（无需模型输入或完整权重即可检测）和紧凑冗余（每个对数概率输出中都可独立检测）等优势。

Conclusion: 椭圆签名可作为语言模型输出的有效验证方法，类似于密码学中的对称密钥消息认证系统，为模型输出溯源和真实性验证提供了新途径。

Abstract: The ubiquity of closed-weight language models with public-facing APIs has
generated interest in forensic methods, both for extracting hidden model
details (e.g., parameters) and for identifying models by their outputs. One
successful approach to these goals has been to exploit the geometric
constraints imposed by the language model architecture and parameters. In this
work, we show that a lesser-known geometric constraint--namely, that language
model outputs lie on the surface of a high-dimensional ellipse--functions as a
signature for the model and can be used to identify the source model of a given
output. This ellipse signature has unique properties that distinguish it from
existing model-output association methods like language model fingerprints. In
particular, the signature is hard to forge: without direct access to model
parameters, it is practically infeasible to produce log-probabilities
(logprobs) on the ellipse. Secondly, the signature is naturally occurring,
since all language models have these elliptical constraints. Thirdly, the
signature is self-contained, in that it is detectable without access to the
model inputs or the full weights. Finally, the signature is compact and
redundant, as it is independently detectable in each logprob output from the
model. We evaluate a novel technique for extracting the ellipse from small
models and discuss the practical hurdles that make it infeasible for
production-scale models. Finally, we use ellipse signatures to propose a
protocol for language model output verification, analogous to cryptographic
symmetric-key message authentication systems.

</details>


### [25] [Power Grid Cybersecurity: Policy Analysis White Paper](https://arxiv.org/abs/2510.14171)
*Jack Vanlyssel*

Main category: cs.CR

TL;DR: 本文提出双重政策方法加强电网网络安全：加强政府与私营电力公司之间的信息共享以改进威胁检测和响应，以及标准化网络卫生实践以减少常见攻击途径。


<details>
  <summary>Details</summary>
Motivation: 美国电网支撑国家安全、公共安全和经济稳定，但面临工业控制系统漏洞、远程访问和网络卫生差等日益增长的网络安全风险。尽管其重要性关键，当前政策仍分散且被动。

Method: 提出双重政策方法：1) 加强政府与私营电力公司之间的信息共享；2) 标准化网络卫生实践。为长期韧性，建议统一国家网络安全框架，协调现有NERC、IEC、IEEE和NIST标准。

Result: 这些政策提供了即时和可持续的改进，以保护国家最关键的基础设施。

Conclusion: 双重政策方法结合统一国家网络安全框架，能够有效应对电网面临的网络安全挑战，确保国家关键基础设施的安全稳定运行。

Abstract: The U.S. power grid underpins national security, public safety, and economic
stability, but faces growing cyber risks from vulnerabilities in industrial
control systems, remote access, and poor cyber hygiene. Despite its critical
importance, current policy remains fragmented and reactive. This paper proposes
a dual policy approach to strengthen grid cybersecurity: enhanced information
sharing between government and private utilities to improve threat detection
and response, and standardized cyber hygiene practices to reduce common attack
vectors. For long-term resilience, a Unified National Cybersecurity Framework
is recommended to align existing NERC, IEC, IEEE, and NIST standards, eliminate
regulatory overlap, and adapt to evolving threats. Together, these policies
offer both immediate and sustainable improvements in safeguarding the nation's
most vital infrastructure.

</details>


### [26] [Securing U.S. Critical Infrastructure: Lessons from Stuxnet and the Ukraine Power Grid Attacks](https://arxiv.org/abs/2510.14185)
*Jack Vanlyssel*

Main category: cs.CR

TL;DR: 本文分析工业控制系统面临的网络安全威胁，基于历史攻击案例识别漏洞，并提出零信任架构和网络分段等政策建议来增强系统韧性。


<details>
  <summary>Details</summary>
Motivation: 工业控制系统支撑美国关键基础设施，但日益增长的数字集成使其面临不断升级的网络威胁。历史攻击揭示了持续存在的漏洞，需要立即改革以防止灾难性中断。

Method: 通过分析Stuxnet和乌克兰电网攻击等历史案例，识别重复出现的漏洞，并评估其与当前美国基础设施的相关性。

Result: 识别出网络分段不足、软件过时、认证薄弱和监控不足等持续存在的漏洞，这些漏洞可能导致类似攻击在美国基础设施中重演。

Conclusion: 建议实施零信任架构和改进网络分段等政策措施，以增强系统韧性，保护关键操作技术免受未来网络威胁。

Abstract: Industrial Control Systems (ICS) underpin the United States' critical
infrastructure, managing essential services such as power, water, and
transportation that are vital to national security and public safety. However,
increasing digital integration has exposed these systems to escalating cyber
threats. Historical attacks like Stuxnet and the Ukraine power grid incident
revealed exploitable weaknesses-poor network segmentation, outdated software,
weak authentication, and inadequate monitoring-that persist in many U.S. ICS
environments today. This paper analyzes these landmark attacks to identify
recurring vulnerabilities and assess their relevance to current U.S.
infrastructure. It argues that without immediate reforms, similar exploits
could lead to catastrophic disruptions and national security crises. To address
these risks, the paper proposes policy measures focused on implementing
zero-trust architecture and improved network segmentation to enhance system
resilience. These recommendations aim to guide policymakers and industry
leaders in securing the nation's most critical operational technologies against
future cyber threats.

</details>


### [27] [Infrastructure Patterns in Toll Scam Domains: A Comprehensive Analysis of Cybercriminal Registration and Hosting Strategies](https://arxiv.org/abs/2510.14198)
*Morium Akter Munny,Mahbub Alam,Sonjoy Kumar Paul,Daniel Timko,Muhammad Lutfor Rahman,Nitesh Saxena*

Main category: cs.CR

TL;DR: 本文首次对67,907个确认的收费诈骗域名进行了大规模分析，揭示了攻击者利用宽松注册商和非主流顶级域名的策略，并建立了基于注册数据的预测模型来识别可能被暂停的诈骗域名。


<details>
  <summary>Details</summary>
Motivation: 收费诈骗通过注册伪装成合法交通机构的虚假域名来欺骗用户进行欺诈支付，这种诈骗正在迅速增加并造成重大危害，但尚未得到充分研究。

Method: 使用新创建的67,907个确认诈骗域名数据集进行分析，研究攻击者利用宽松注册商和非主流顶级域名的模式，并构建仅使用域名注册数据的预测模型来预测哪些诈骗域名可能被暂停。

Result: 研究发现86.9%的域名集中在五个非主流顶级域名，72.9%通过单一提供商注册；发现特定的注册模式，包括表明自动化协调攻击的短期活动爆发；预测模型达到80.4%准确率和92.3%灵敏度。

Conclusion: 分析揭示了攻击者逃避检测的策略，可以为注册商、托管提供商和安全平台提供更有针对性的干预措施，但仅靠注册元数据可能不足，结合域名URL和网页内容特征可进一步提高检测效果。

Abstract: Toll scams involve criminals registering fake domains that pretend to be
legitimate transportation agencies to trick users into making fraudulent
payments. Although these scams are rapidly increasing and causing significant
harm, they have not been extensively studied. We present the first large-scale
analysis of toll scam domains, using a newly created dataset of 67,907
confirmed scam domains mostly registered in 2025. Our study reveals that
attackers exploit permissive registrars and less common top-level domains, with
86.9% of domains concentrated in just five non-mainstream TLDs and 72.9%
registered via a single provider. We also discover specific registration
patterns, including short bursts of activity that suggest automated,
coordinated attacks, with over half of domains registered in the first quarter
of 2025. This extreme temporal clustering reflects highly synchronized campaign
launches. Additionally, we build a simple predictive model using only domain
registration data to predict which scam domains are likely to be suspended -- a
proxy for confirmed abuse -- achieving 80.4% accuracy, and 92.3% sensitivity.
Our analysis reveals attacker strategies for evading detection -- such as
exploiting obscure TLDs, permissive registrars, and coordinated registration
bursts -- which can inform more targeted interventions by registrars, hosting
providers, and security platforms. However, our results suggest that
registration metadata alone may be insufficient, and incorporating features
from domain URLs and webpage content could further improve detection.

</details>


### [28] [An Information Asymmetry Game for Trigger-based DNN Model Watermarking](https://arxiv.org/abs/2510.14218)
*Chaoyue Huang,Gejian Zhao,Hanzhou Wu,Zhihua Xia,Asad Malik*

Main category: cs.CR

TL;DR: 该论文提出了一种基于博弈论的DNN水印保护方法，通过稀疏水印技术抵抗剪枝和微调攻击，在信息不对称条件下分析攻防双方的最优策略。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络作为有价值的数字产品面临日益严重的知识产权威胁，需要开发有效的技术措施来保护模型版权。

Method: 将水印保护建模为信息不对称条件下的博弈问题，防御方嵌入秘密水印，攻击方只能访问带水印模型并尝试移除。定义了双方策略、成本和效用函数，推导攻击方最优剪枝预算。

Result: 实验结果表明水印模型的可行性，稀疏水印能够以可忽略的精度损失抵抗移除攻击，建立了攻击后水印检测精度的指数下界。

Conclusion: 博弈论分析为设计鲁棒的水印方案提供了有效指导，证明了稀疏水印在模型版权保护中的有效性。

Abstract: As a valuable digital product, deep neural networks (DNNs) face increasingly
severe threats to the intellectual property, making it necessary to develop
effective technical measures to protect them. Trigger-based watermarking
methods achieve copyright protection by embedding triggers into the host DNNs.
However, the attacker may remove the watermark by pruning or fine-tuning. We
model this interaction as a game under conditions of information asymmetry,
namely, the defender embeds a secret watermark with private knowledge, while
the attacker can only access the watermarked model and seek removal. We define
strategies, costs, and utilities for both players, derive the attacker's
optimal pruning budget, and establish an exponential lower bound on the
accuracy of watermark detection after attack. Experimental results demonstrate
the feasibility of the watermarked model, and indicate that sparse watermarking
can resist removal with negligible accuracy loss. This study highlights the
effectiveness of game-theoretic analysis in guiding the design of robust
watermarking schemes for model copyright protection.

</details>


### [29] [RHINO: Guided Reasoning for Mapping Network Logs to Adversarial Tactics and Techniques with Large Language Models](https://arxiv.org/abs/2510.14233)
*Fanchao Meng,Jiaping Gui,Yunbo Li,Yue Wu*

Main category: cs.CR

TL;DR: RHINO是一个基于LLM的网络入侵检测框架，通过三阶段推理过程将低级告警映射到高级攻击技术，显著提升了准确性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有网络入侵检测系统产生大量低级告警，但需要人工关联到高级攻击行为。基于规则的方法无法适应新型攻击，机器学习方法缺乏上下文意识，而现有LLM方法存在幻觉和去上下文映射问题。

Method: RHINO框架将攻击分析分解为三个可解释阶段：行为抽象（将原始日志转换为情境化叙述）、多角色协作推理（基于MITRE ATT&CK知识生成候选技术）、验证（交叉引用官方定义纠正幻觉）。

Result: 在三个基准测试和四个骨干模型上评估，RHINO准确率达到86.38%到88.45%，相对提升24.25%到76.50%。

Conclusion: RHINO显著提升了威胁分析的可解释性和可扩展性，为在运营安全环境中部署LLM提供了蓝图。

Abstract: Modern Network Intrusion Detection Systems generate vast volumes of low-level
alerts, yet these outputs remain semantically fragmented, requiring
labor-intensive manual correlation with high-level adversarial behaviors.
Existing solutions for automating this mapping-rule-based systems and machine
learning classifiers-suffer from critical limitations: rule-based approaches
fail to adapt to novel attack variations, while machine learning methods lack
contextual awareness and treat tactic-technique mapping as a syntactic matching
problem rather than a reasoning task. Although Large Language Models have shown
promise in cybersecurity tasks, preliminary experiments reveal that existing
LLM-based methods frequently hallucinate technique names or produce
decontextualized mappings due to their single-step classification approach.
  To address these challenges, we introduce RHINO, a novel framework that
decomposes LLM-based attack analysis into three interpretable phases mirroring
human reasoning: (1) behavioral abstraction, where raw logs are translated into
contextualized narratives; (2) multi-role collaborative inference, generating
candidate techniques by evaluating behavioral evidence against MITRE ATT&CK
knowledge; and (3) validation, cross-referencing predictions with official
MITRE definitions to rectify hallucinations. RHINO bridges the semantic gap
between low-level observations and adversarial intent while improving output
reliability through structured reasoning.
  We evaluate RHINO on three benchmarks across four backbone models. RHINO
achieved high accuracy, with model performance ranging from 86.38% to 88.45%,
resulting in relative gains from 24.25% to 76.50% across different models. Our
results demonstrate that RHINO significantly enhances the interpretability and
scalability of threat analysis, offering a blueprint for deploying LLMs in
operational security settings.

</details>


### [30] [Beyond a Single Perspective: Towards a Realistic Evaluation of Website Fingerprinting Attacks](https://arxiv.org/abs/2510.14283)
*Xinhao Deng,Jingyou Chen,Linxiao Yu,Yixiang Zhang,Zhongyi Gu,Changhao Qiu,Xiyuan Zhao,Ke Xu,Qi Li*

Main category: cs.CR

TL;DR: 本文首次对网站指纹识别攻击在多种现实条件下进行系统性评估，发现现有方法在真实环境中表现显著下降。


<details>
  <summary>Details</summary>
Motivation: 当前网站指纹识别研究大多局限于单一场景，忽略了现实环境的复杂性，需要评估这些攻击在真实条件下的实际效果。

Method: 建立多维评估框架，在防御机制、流量漂移、多标签浏览、早期检测、开放世界和少样本场景等多样化现实条件下测试现有WF技术。

Result: 实验结果显示，许多在孤立环境中表现优异的WF技术在面对其他条件时性能显著下降，难以直接应用于实践。

Conclusion: 现实环境往往包含多重挑战，当前WF攻击难以直接实用，需要开发更鲁棒和实用的攻击方法。

Abstract: Website Fingerprinting (WF) attacks exploit patterns in encrypted traffic to
infer the websites visited by users, posing a serious threat to anonymous
communication systems. Although recent WF techniques achieve over 90% accuracy
in controlled experimental settings, most studies remain confined to single
scenarios, overlooking the complexity of real-world environments. This paper
presents the first systematic and comprehensive evaluation of existing WF
attacks under diverse realistic conditions, including defense mechanisms,
traffic drift, multi-tab browsing, early-stage detection, open-world settings,
and few-shot scenarios. Experimental results show that many WF techniques with
strong performance in isolated settings degrade significantly when facing other
conditions. Since real-world environments often combine multiple challenges,
current WF attacks are difficult to apply directly in practice. This study
highlights the limitations of WF attacks and introduces a multidimensional
evaluation framework, offering critical insights for developing more robust and
practical WF attacks.

</details>


### [31] [BinCtx: Multi-Modal Representation Learning for Robust Android App Behavior Detection](https://arxiv.org/abs/2510.14344)
*Zichen Liu,Shao Yang,Xusheng Xiao*

Main category: cs.CR

TL;DR: BINCTX是一个多模态学习方法，通过结合字节码图像、上下文行为视图和第三方库使用情况来检测移动应用中的不良行为，在真实恶意软件检测中达到94.73%的F1分数，对混淆和对抗样本具有鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 移动应用市场中存在大量难以检测的不良行为（如干扰性广告、非法重定向、支付欺骗），这些行为通常不依赖权限保护API，且容易通过UI或元数据编辑进行伪装。

Method: 构建应用的多模态表示：1）全局字节码图像视图捕捉代码语义和家族模式；2）上下文视图显示行为触发方式；3）第三方库使用视图总结调用频率。三个视图嵌入融合后训练上下文感知分类器。

Result: 在真实恶意软件和良性应用上，BINCTX达到94.73%的宏F1分数，比强基线至少提升14.92%。在商业混淆后仍保持84%的F1分数，比仅使用字节码的最先进系统更抗对抗样本。

Conclusion: BINCTX通过多模态表示学习有效检测移动应用中的不良行为，对混淆和对抗攻击具有鲁棒性，显著优于现有方法。

Abstract: Mobile app markets host millions of apps, yet undesired behaviors (e.g.,
disruptive ads, illegal redirection, payment deception) remain hard to catch
because they often do not rely on permission-protected APIs and can be easily
camouflaged via UI or metadata edits. We present BINCTX, a learning approach
that builds multi-modal representations of an app from (i) a global
bytecode-as-image view that captures code-level semantics and family-style
patterns, (ii) a contextual view (manifested actions, components, declared
permissions, URL/IP constants) indicating how behaviors are triggered, and
(iii) a third-party-library usage view summarizing invocation frequencies along
inter-component call paths. The three views are embedded and fused to train a
contextual-aware classifier. On real-world malware and benign apps, BINCTX
attains a macro F1 of 94.73%, outperforming strong baselines by at least
14.92%. It remains robust under commercial obfuscation (F1 84%
post-obfuscation) and is more resistant to adversarial samples than
state-of-the-art bytecode-only systems.

</details>


### [32] [Match & Mend: Minimally Invasive Local Reassembly for Patching N-day Vulnerabilities in ARM Binaries](https://arxiv.org/abs/2510.14384)
*Sebastian Jänich,Merlin Sievers,Johannes Kinder*

Main category: cs.CR

TL;DR: 提出了一种在二进制级别自动修补IoT固件中已知漏洞的方法，无需厂商支持，成功修补了83%-96%的目标漏洞。


<details>
  <summary>Details</summary>
Motivation: 低成本IoT设备由于更新机制不完善而存在安全隐患，许多设备运行着过时且已知存在漏洞的开源软件版本。

Method: 采用最小侵入性本地重汇编技术，在二进制级别自动修补已知漏洞，旨在最小化副作用并降低引入破坏性变更的风险。

Result: 在MAGMA基准测试的108个二进制文件中成功修补83%的目标漏洞，在KARONTE数据集的30个真实世界Linux IoT固件中成功修补96%的漏洞。

Conclusion: 该方法能够有效自动修补IoT固件中的已知漏洞，为缺乏厂商支持的设备提供了可行的安全更新方案。

Abstract: Low-cost Internet of Things (IoT) devices are increasingly popular but often
insecure due to poor update regimes. As a result, many devices run outdated and
known-vulnerable versions of open-source software. We address this problem by
proposing to patch IoT firmware at the binary level, without requiring vendor
support. In particular, we introduce minimally invasive local reassembly, a new
technique for automatically patching known (n-day) vulnerabilities in IoT
firmware. Our approach is designed to minimize side effects and reduce the risk
of introducing breaking changes. We systematically evaluate our approach both
on 108 binaries within the controlled environment of the MAGMA benchmarks, as
well as on 30 real-world Linux-based IoT firmware images from the KARONTE
dataset. Our prototype successfully patches 83% of targeted vulnerabilities in
MAGMA and 96% in the firmware dataset.

</details>


### [33] [Stealthy Dual-Trigger Backdoors: Attacking Prompt Tuning in LM-Empowered Graph Foundation Models](https://arxiv.org/abs/2510.14470)
*Xiaoyu Xue,Yuni Lai,Chenxi Huang,Yulin Zhu,Gaolei Li,Xiaoge Zhang,Kai Zhou*

Main category: cs.CR

TL;DR: 该论文揭示了语言模型增强的图基础模型在提示调优阶段存在独特的安全漏洞，提出了一种双触发后门攻击框架，能在文本级和结构级同时操作，无需显式优化触发节点文本属性。


<details>
  <summary>Details</summary>
Motivation: 传统图神经网络与LM增强的图基础模型相比，后者在未受保护的提示调优阶段引入了独特的安全漏洞，但这些漏洞在当前研究中尚未得到充分研究。

Method: 提出了一种新颖的双触发后门攻击框架，在文本级和结构级同时操作，通过战略性地利用预建立的文本池，无需显式优化触发节点文本属性即可实现有效攻击。

Result: 广泛的实验评估表明，该攻击在保持优异清洁准确率的同时，实现了出色的攻击成功率，包括高度隐蔽的单触发节点场景。

Conclusion: 这项工作凸显了网络部署的LM增强图基础模型中的关键后门风险，并为开源平台在基础模型时代开发更强大的监督机制做出了贡献。

Abstract: The emergence of graph foundation models (GFMs), particularly those
incorporating language models (LMs), has revolutionized graph learning and
demonstrated remarkable performance on text-attributed graphs (TAGs). However,
compared to traditional GNNs, these LM-empowered GFMs introduce unique security
vulnerabilities during the unsecured prompt tuning phase that remain
understudied in current research. Through empirical investigation, we reveal a
significant performance degradation in traditional graph backdoor attacks when
operating in attribute-inaccessible constrained TAG systems without explicit
trigger node attribute optimization. To address this, we propose a novel
dual-trigger backdoor attack framework that operates at both text-level and
struct-level, enabling effective attacks without explicit optimization of
trigger node text attributes through the strategic utilization of a
pre-established text pool. Extensive experimental evaluations demonstrate that
our attack maintains superior clean accuracy while achieving outstanding attack
success rates, including scenarios with highly concealed single-trigger nodes.
Our work highlights critical backdoor risks in web-deployed LM-empowered GFMs
and contributes to the development of more robust supervision mechanisms for
open-source platforms in the era of foundation models.

</details>


### [34] [Certifying optimal MEV strategies with Lean](https://arxiv.org/abs/2510.14480)
*Massimo Bartoletti,Riccardo Marchesin,Roberto Zunino*

Main category: cs.CR

TL;DR: 本文首次在Lean定理证明器中形式化MEV（最大可提取价值），提出了一种构建机器检查证明的方法来验证MEV上界，并证明了自动做市商中三明治攻击的最优性。


<details>
  <summary>Details</summary>
Motivation: MEV攻击已从DeFi协议中提取了数十亿美元的价值，但验证MEV攻击的缺失需要确定合适的上界，这个问题非常困难，因为攻击策略空间极其庞大，现有实证研究和手工推理不够严谨。

Method: 在Lean定理证明器中形式化MEV，构建机器检查证明来验证MEV上界，并建模分析两个典型DeFi协议的MEV。

Result: 开发了第一个机器检查证明，证明了自动做市商中三明治攻击的最优性，为MEV分析提供了超越现有技术的正确性保证。

Conclusion: 该方法为MEV分析提供了通用的形式化框架，能够为DeFi协议的安全性提供机器验证的严格保证。

Abstract: Maximal Extractable Value (MEV) refers to a class of attacks to decentralized
applications where the adversary profits by manipulating the ordering,
inclusion, or exclusion of transactions in a blockchain. Decentralized Finance
(DeFi) protocols are a primary target of these attacks, as their logic depends
critically on transaction sequencing. To date, MEV attacks have already
extracted billions of dollars in value, underscoring their systemic impact on
blockchain security. Verifying the absence of MEV attacks requires determining
suitable upper bounds, i.e. proving that no adversarial strategy can extract
more value (if any) than expected by protocol designers. This problem is
notoriously difficult: the space of adversarial strategies is extremely vast,
making empirical studies and pen-and-paper reasoning insufficiently rigorous.
In this paper, we present the first mechanized formalization of MEV in the Lean
theorem prover. We introduce a methodology to construct machine-checked proofs
of MEV bounds, providing correctness guarantees beyond what is possible with
existing techniques. To demonstrate the generality of our approach, we model
and analyse the MEV of two paradigmatic DeFi protocols. Notably, we develop the
first machine-checked proof of the optimality of sandwich attacks in Automated
Market Makers, a fundamental DeFi primitive.

</details>


### [35] [Lexo: Eliminating Stealthy Supply-Chain Attacks via LLM-Assisted Program Regeneration](https://arxiv.org/abs/2510.14522)
*Evangelos Lamprou,Julian Dai,Grigoris Ntousakis,Martin C. Rinard,Nikos Vasilakis*

Main category: cs.CR

TL;DR: Lexo是一个自动学习和重新生成无漏洞版本的开源组件系统，用于防御软件供应链攻击。它通过输入-输出对建模组件行为，使用LLM合成新组件，保持原始功能但消除恶意代码。


<details>
  <summary>Details</summary>
Motivation: 开源软件生态系统中的软件供应链攻击是一个持续关注的问题，这些攻击在组件到达目标环境时激活恶意功能，而Lexo旨在解决这种隐蔽攻击。

Method: Lexo首先生成输入-输出对来建模组件的完整可观察行为，然后使用多个LLM实例合成新版本组件，通过正确性和覆盖率指标来引导和约束LLM的结果。

Result: 在100多个真实世界包（包括高知名度的隐蔽供应链攻击）上的评估表明，Lexo能跨多个领域扩展，平均在100秒内高效重新生成代码，保持兼容性，并成功消除多个真实世界供应链攻击中的恶意代码。

Conclusion: Lexo能够有效防御软件供应链攻击，即使在最先进的LLM直接提示消除恶意代码失败的情况下，Lexo仍能成功消除恶意代码。

Abstract: Software supply-chain attacks are an important and ongoing concern in the
open source software ecosystem. These attacks maintain the standard
functionality that a component implements, but additionally hide malicious
functionality activated only when the component reaches its target environment.
Lexo addresses such stealthy attacks by automatically learning and regenerating
vulnerability-free versions of potentially malicious components. Lexo first
generates a set of input-output pairs to model a component's full observable
behavior, which it then uses to synthesize a new version of the original
component. The new component implements the original functionality but avoids
stealthy malicious behavior. Throughout this regeneration process, Lexo
consults several distinct instances of Large Language Models (LLMs), uses
correctness and coverage metrics to shepherd these instances, and guardrails
their results. Our evaluation on 100+ real-world packages, including high
profile stealthy supply-chain attacks, indicates that Lexo scales across
multiple domains, regenerates code efficiently (<100s on average), maintains
compatibility, and succeeds in eliminating malicious code in several real-world
supply-chain-attacks, even in cases when a state-of-the-art LLM fails to
eliminate malicious code when prompted to do so.

</details>


### [36] [Symbolic verification of Apple's Find My location-tracking protocol](https://arxiv.org/abs/2510.14589)
*Vaishnavi Sundararajan,Rithwik*

Main category: cs.CR

TL;DR: 本文对苹果Find My追踪系统进行形式化分析，通过符号建模和自动化验证证明其隐私安全属性


<details>
  <summary>Details</summary>
Motivation: 苹果Find My系统虽然声称私密安全，但其代码闭源，需要通过形式化验证来检验其安全性，防止逻辑漏洞导致的隐私泄露

Method: 建立Find My协议的符号模型，精确定义期望的安全属性，使用Tamarin证明器进行自动化机器可验证证明

Result: 通过形式化验证提供了Find My协议安全属性的机器可检查证明

Conclusion: 形式化验证表明Find My协议在建模的安全属性方面是安全的，为闭源系统的安全评估提供了有效方法

Abstract: Tracking devices, while designed to help users find their belongings in case
of loss/theft, bring in new questions about privacy and surveillance of not
just their own users, but in the case of crowd-sourced location tracking, even
that of others even orthogonally associated with these platforms. Apple's Find
My is perhaps the most ubiquitous such system which can even locate devices
which do not possess any cellular support or GPS, running on millions of
devices worldwide. Apple claims that this system is private and secure, but the
code is proprietary, and such claims have to be taken on faith. It is well
known that even with perfect cryptographic guarantees, logical flaws might
creep into protocols, and allow undesirable attacks. In this paper, we present
a symbolic model of the Find My protocol, as well as a precise formal
specification of desirable properties, and provide automated, machine-checkable
proofs of these properties in the Tamarin prover.

</details>


### [37] [Improving Cybercrime Detection and Digital Forensics Investigations with Artificial Intelligence](https://arxiv.org/abs/2510.14638)
*Silvia Lucia Sanna,Leonardo Regano,Davide Maiorca,Giorgio Giacinto*

Main category: cs.CR

TL;DR: 本文探讨了如何利用人工智能改进网络犯罪分析和数字取证，同时指出网络犯罪分子也可能利用AI技术来增强攻击能力。通过案例研究展示了使用主流聊天机器人开发隐写术代码的可能性。


<details>
  <summary>Details</summary>
Motivation: 欧洲刑警组织报告显示网络犯罪在欧洲仍然频发，需要采取各种措施来限制、预防、检测、分析和打击网络犯罪。当前需要改进网络犯罪检测和数字取证分析的方法。

Method: 提出将人工智能算法作为网络犯罪检测和数字取证分析的补充工具。通过案例研究展示了如何使用Gemini、Copilot和chatGPT等聊天机器人开发用于图像隐写术的Python代码。

Result: 研究表明AI可以有效改进网络犯罪分析和数字取证程序，但同时也可能被网络犯罪分子利用来提升技能、绕过自动检测并开发高级攻击技术。案例研究成功展示了利用聊天机器人开发隐写术代码的可行性。

Conclusion: AI在网络犯罪分析和数字取证中具有重要应用价值，但需要警惕其被恶意利用的风险。需要平衡AI技术的正面应用与潜在的安全威胁。

Abstract: According to a recent EUROPOL report, cybercrime is still recurrent in
Europe, and different activities and countermeasures must be taken to limit,
prevent, detect, analyze, and fight it. Cybercrime must be prevented with
specific measures, tools, and techniques, for example through automated network
and malware analysis. Countermeasures against cybercrime can also be improved
with proper \df analysis in order to extract data from digital devices trying
to retrieve information on the cybercriminals. Indeed, results obtained through
a proper \df analysis can be leveraged to train cybercrime detection systems to
prevent the success of similar crimes. Nowadays, some systems have started to
adopt Artificial Intelligence (AI) algorithms for cyberattack detection and \df
analysis improvement. However, AI can be better applied as an additional
instrument in these systems to improve the detection and in the \df analysis.
For this reason, we highlight how cybercrime analysis and \df procedures can
take advantage of AI. On the other hand, cybercriminals can use these systems
to improve their skills, bypass automatic detection, and develop advanced
attack techniques. The case study we presented highlights how it is possible to
integrate the use of the three popular chatbots {\tt Gemini}, {\tt Copilot} and
{\tt chatGPT} to develop a Python code to encode and decoded images with
steganographic technique, even though their presence is not an indicator of
crime, attack or maliciousness but used by a cybercriminal as anti-forensics
technique.

</details>


### [38] [AEX-NStep: Probabilistic Interrupt Counting Attacks on Intel SGX](https://arxiv.org/abs/2510.14675)
*Nicolas Dutly,Friederike Groschupp,Ivan Puddu,Kari Kostiainen,Srdjan Capkun*

Main category: cs.CR

TL;DR: AEX-NStep是首个针对AEX-Notify的终端计数攻击，证明AEX-Notify无法完全防止此类攻击，并成功实施了ECDSA密钥泄露攻击。


<details>
  <summary>Details</summary>
Motivation: Intel引入AEX-Notify来缓解基于中断的单步攻击，但作者发现其安全保证存在缺陷，需要验证其实际防护效果。

Method: 开发了AEX-NStep攻击，包括两种概率性中断计数攻击方法，针对AEX-Notify的"混淆前进进度"安全保证进行突破。

Result: 成功证明了AEX-Notify无法防止中断计数攻击，并实现了对AEX-Notify-enabled SGX终端的ECDSA密钥泄露。

Conclusion: AEX-Notify的安全保证存在缺陷，无法完全防止中断计数攻击，为未来缓解措施的设计提供了重要参考。

Abstract: To mitigate interrupt-based stepping attacks (notably using SGX-Step), Intel
introduced AEX-Notify, an ISA extension to Intel SGX that aims to prevent
deterministic single-stepping. In this work, we introduce AEX-NStep, the first
interrupt counting attack on AEX-Notify-enabled Enclaves. We show that
deterministic single-stepping is not required for interrupt counting attacks to
be practical and that, therefore, AEX-Notify does not entirely prevent such
attacks. We specifically show that one of AEX-Notify's security guarantees,
obfuscated forward progress, does not hold, and we introduce two new
probabilistic interrupt counting attacks. We use these attacks to construct a
practical ECDSA key leakage attack on an AEX-Notify-enabled SGX enclave. Our
results extend the original security analysis of AEX-Notify and inform the
design of future mitigations.

</details>


### [39] [FibRace: a large-scale benchmark of client-side proving on mobile devices](https://arxiv.org/abs/2510.14693)
*Simon Malatrait,Alex Sirac*

Main category: cs.CR

TL;DR: FibRace是首个在智能手机上使用Cairo M进行客户端证明生成的大规模实验，通过手机游戏形式让玩家证明斐波那契数列并登上排行榜，验证了现代智能手机能在5秒内可靠生成零知识证明。


<details>
  <summary>Details</summary>
Motivation: 测试智能手机客户端证明生成的可行性，为轻量级证明器、证明驱动基础设施和隐私保护移动应用提供实践基准。

Method: 将证明生成作为移动游戏，玩家在智能手机上使用Cairo M生成斐波那契数列的零知识证明，收集6047名玩家在1420种不同设备模型上生成的2195488个证明数据。

Result: 大多数现代智能手机能在5秒内完成证明，性能主要与RAM容量和SoC性能相关，至少3GB RAM的设备能稳定证明，苹果A19 Pro和M系列芯片证明时间最快，所有证明都在Hyli区块链上无拥堵地原生验证。

Conclusion: 移动设备现在能够可靠地生成零知识证明，无需远程证明器或专用硬件，为未来研究提供了最全面的移动证明性能数据集。

Abstract: FibRace, jointly developed by KKRT Labs and Hyli, was the first large-scale
experiment to test client-side proof generation on smartphones using Cairo M.
Presented as a mobile game in which players proved Fibonacci numbers and
climbed a leaderboard, FibRace served a dual purpose: to engage the public and
to provide empirical benchmarking. Over a three-week campaign (September 11-30,
2025), 6,047 players across 99 countries generated 2,195,488 proofs on 1,420
unique device models. The results show that most modern smartphones can
complete a proof in under 5 seconds, confirming that *mobile devices are now
capable of producing zero-knowledge proofs reliably*, without the need for
remote provers or specialized hardware. Performance was correlated primarily
with RAM capacity and SoC (System on Chip) performance: devices with at least 3
GB of RAM proved stably, when Apple's A19 Pro and M-series chips achieved the
fastest proving times. Hyli's blockchain natively verified every proof onchain
without congestion. FibRace provides the most comprehensive dataset to date on
mobile proving performance, establishing a practical baseline for future
research in lightweight provers, proof-powered infrastructure, and
privacy-preserving mobile applications.

</details>


### [40] [SLIE: A Secure and Lightweight Cryptosystem for Data Sharing in IoT Healthcare Services](https://arxiv.org/abs/2510.14708)
*Ha Xuan Son,Nguyen Quoc Anh,Phat T. Tran-Truong,Le Thanh Tuan,Pham Thanh Nghiem*

Main category: cs.CR

TL;DR: 提出SLIE加密系统，基于WKD-IBE技术，为IoMT医疗设备提供轻量级安全通信方案，显著提升加解密性能并降低能耗。


<details>
  <summary>Details</summary>
Motivation: IoMT医疗物联网的服务化模式带来了设备管理和通信安全漏洞，医疗数据敏感性要求更强的安全保障。

Method: 采用Wildcard Key Derivation Identity-Based Encryption (WKD-IBE)技术，结合端到端加密、分层访问控制、轻量级密钥管理，并集成恒定时间操作、内存混淆和基于过期的密钥撤销机制。

Result: SLIE在1KB数据上的加密时间为0.936ms，解密时间为0.217ms，加密速度提升84.54%，解密速度提升99.70%，能耗为0.014 J/KB，显著优于RSA。

Conclusion: SLIE为资源受限的医疗设备提供了可扩展的信任机制和安全的全向通信，能够抵御旁道攻击、中间人攻击和未授权访问，符合HIPAA和GDPR标准。

Abstract: The Internet of Medical Things (IoMT) has revolutionized healthcare by
transforming medical operations into standardized, interoperable services.
However, this service-oriented model introduces significant security
vulnerabilities in device management and communication, which are especially
critical given the sensitivity of medical data. To address these risks, this
paper proposes SLIE (Secure and Lightweight Identity Encryption), a novel
cryptosystem based on Wildcard Key Derivation Identity-Based Encryption
(WKD-IBE). SLIE ensures scalable trust and secure omnidirectional communication
through end-to-end encryption, hierarchical access control, and a lightweight
key management system designed for resource-constrained devices. It
incorporates constant-time operations, memory obfuscation, and expiry-based key
revocation to counter side-channel, man-in-the-middle, and unauthorized access
attacks, thereby ensuring compliance with standards like HIPAA and GDPR.
Evaluations show that SLIE significantly outperforms RSA, with encryption and
decryption times of 0.936ms and 0.217ms for 1KB of data, an 84.54% improvement
in encryption speed, a 99.70% improvement in decryption speed, and an energy
efficiency of 0.014 J/KB.

</details>


### [41] [Secure Sparse Matrix Multiplications and their Applications to Privacy-Preserving Machine Learning](https://arxiv.org/abs/2510.14894)
*Marc Damie,Florian Hahn,Andreas Peter,Jan Ramon*

Main category: cs.CR

TL;DR: 提出了针对稀疏数据的多方计算矩阵乘法算法，解决了现有MPC框架在处理高维稀疏数据时的内存和通信效率问题，显著降低了通信成本。


<details>
  <summary>Details</summary>
Motivation: 现有MPC框架未针对稀疏数据进行优化，无法有效处理推荐系统、基因组学等涉及高维稀疏数据的机器学习应用，因为密集数据表示会导致内存需求过大。

Method: 开发了秘密稀疏矩阵乘法算法，避免了经典安全矩阵乘法算法的密集数据表示内存问题，并采用基于非零元素分布的安全上界方法。

Result: 算法显著减少了通信成本（某些实验显示可达1000倍），并在现有协议不实用的两个机器学习应用中验证了有效性。

Conclusion: 提出的稀疏矩阵MPC算法解决了内存和通信效率问题，同时通过基于统计分布的安全上界方法更好地保护了敏感信息。

Abstract: To preserve privacy, multi-party computation (MPC) enables executing Machine
Learning (ML) algorithms on secret-shared or encrypted data. However, existing
MPC frameworks are not optimized for sparse data. This makes them unsuitable
for ML applications involving sparse data, e.g., recommender systems or
genomics. Even in plaintext, such applications involve high-dimensional sparse
data, that cannot be processed without sparsity-related optimizations due to
prohibitively large memory requirements.
  Since matrix multiplication is central in ML algorithms, we propose MPC
algorithms to multiply secret sparse matrices. On the one hand, our algorithms
avoid the memory issues of the "dense" data representation of classic secure
matrix multiplication algorithms. On the other hand, our algorithms can
significantly reduce communication costs (some experiments show a factor 1000)
for realistic problem sizes. We validate our algorithms in two ML applications
in which existing protocols are impractical.
  An important question when developing MPC algorithms is what assumptions can
be made. In our case, if the number of non-zeros in a row is a sensitive piece
of information then a short runtime may reveal that the number of non-zeros is
small. Existing approaches make relatively simple assumptions, e.g., that there
is a universal upper bound to the number of non-zeros in a row. This often
doesn't align with statistical reality, in a lot of sparse datasets the amount
of data per instance satisfies a power law. We propose an approach which allows
adopting a safe upper bound on the distribution of non-zeros in rows/columns of
sparse matrices.

</details>


### [42] [A Hard-Label Black-Box Evasion Attack against ML-based Malicious Traffic Detection Systems](https://arxiv.org/abs/2510.14906)
*Zixuan Liu,Yi Zhao,Zhuotao Liu,Qi Li,Chuanpu Fu,Guangmeng Zhou,Ke Xu*

Main category: cs.CR

TL;DR: NetMasquerade是一种基于强化学习的对抗性流量生成方法，通过模仿良性流量模式来规避机器学习恶意流量检测系统，在80种攻击场景下对6种检测方法的逃避成功率超过96.65%。


<details>
  <summary>Details</summary>
Motivation: 现有的逃避攻击要么依赖过于严格的条件（如加密协议、Tor或特殊设置），要么需要目标的详细先验知识（如训练数据和模型参数），这在现实的黑盒场景中不实用。硬标签黑盒逃避攻击的可行性仍然是一个开放挑战。

Method: 开发了NetMasquerade，利用强化学习操纵攻击流量以模仿良性流量并逃避检测。首先建立了专门预训练的Traffic-BERT模型，使用网络专用分词器和注意力机制提取多样化的良性流量模式，然后将其集成到RL框架中，使NetMasquerade能够基于良性流量模式以最小修改有效操纵恶意数据包序列。

Result: 实验结果表明，NetMasquerade能够在80种攻击场景下逃避6种现有检测方法，实现超过96.65%的攻击成功率。它能够逃避那些对现有逃避攻击具有经验性或可证明鲁棒性的方法，并实现低延迟的对抗性流量生成。

Conclusion: NetMasquerade证明了在现实黑盒场景中进行硬标签黑盒逃避攻击的可行性，展示了其在真实世界场景中的实用性，为机器学习恶意流量检测系统的鲁棒性提出了重要挑战。

Abstract: Machine Learning (ML)-based malicious traffic detection is a promising
security paradigm. It outperforms rule-based traditional detection by
identifying various advanced attacks. However, the robustness of these ML
models is largely unexplored, thereby allowing attackers to craft adversarial
traffic examples that evade detection. Existing evasion attacks typically rely
on overly restrictive conditions (e.g., encrypted protocols, Tor, or
specialized setups), or require detailed prior knowledge of the target (e.g.,
training data and model parameters), which is impractical in realistic
black-box scenarios. The feasibility of a hard-label black-box evasion attack
(i.e., applicable across diverse tasks and protocols without internal target
insights) thus remains an open challenge. To this end, we develop
NetMasquerade, which leverages reinforcement learning (RL) to manipulate attack
flows to mimic benign traffic and evade detection. Specifically, we establish a
tailored pre-trained model called Traffic-BERT, utilizing a network-specialized
tokenizer and an attention mechanism to extract diverse benign traffic
patterns. Subsequently, we integrate Traffic-BERT into the RL framework,
allowing NetMasquerade to effectively manipulate malicious packet sequences
based on benign traffic patterns with minimal modifications. Experimental
results demonstrate that NetMasquerade enables both brute-force and stealthy
attacks to evade 6 existing detection methods under 80 attack scenarios,
achieving over 96.65% attack success rate. Notably, it can evade the methods
that are either empirically or certifiably robust against existing evasion
attacks. Finally, NetMasquerade achieves low-latency adversarial traffic
generation, demonstrating its practicality in real-world scenarios.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [43] [Decision Oriented Technique (DOTechnique): Finding Model Validity Through Decision-Maker Context](https://arxiv.org/abs/2510.13858)
*Raheleh Biglari,Joachim Denil*

Main category: cs.AI

TL;DR: 提出了DOTechnique方法，通过决策一致性而非输出相似性来确定模型有效性，在缺乏明确有效性边界时也能高效识别有效区域。


<details>
  <summary>Details</summary>
Motivation: 模型有效性对于指导决策过程至关重要，但传统方法依赖预定义的有效性框架，这些框架可能不可用或不充分。

Method: DOTechnique通过评估代理模型是否与高保真模型产生等效决策来确定有效性，结合领域约束和符号推理来缩小搜索空间。

Result: 以高速公路变道系统为例，展示了DOTechnique如何发现仿真模型的有效区域。

Conclusion: 该技术有潜力通过决策者上下文来支持寻找模型有效性。

Abstract: Model validity is as critical as the model itself, especially when guiding
decision-making processes. Traditional approaches often rely on predefined
validity frames, which may not always be available or sufficient. This paper
introduces the Decision Oriented Technique (DOTechnique), a novel method for
determining model validity based on decision consistency rather than output
similarity. By evaluating whether surrogate models lead to equivalent decisions
compared to high-fidelity models, DOTechnique enables efficient identification
of validity regions, even in the absence of explicit validity boundaries. The
approach integrates domain constraints and symbolic reasoning to narrow the
search space, enhancing computational efficiency. A highway lane change system
serves as a motivating example, demonstrating how DOTechnique can uncover the
validity region of a simulation model. The results highlight the potential of
the technique to support finding model validity through decision-maker context.

</details>


### [44] [Do Slides Help? Multi-modal Context for Automatic Transcription of Conference Talks](https://arxiv.org/abs/2510.13979)
*Supriti Sinhamahapatra,Jan Niehues*

Main category: cs.AI

TL;DR: 该论文提出了一个多模态演示基准，通过整合视觉信息（如演示幻灯片）来增强语音识别系统，在领域特定术语识别上取得了显著改进。


<details>
  <summary>Details</summary>
Motivation: 当前最先进的自动语音识别系统主要依赖声学信息而忽略了多模态上下文。视觉信息对于消歧和适应至关重要，特别是在科学演示场景中。

Method: 创建多模态演示基准，通过数据增强方法解决缺乏配套幻灯片数据集的问题，训练整合视觉信息的语音模型。

Result: 与基线模型相比，在所有词汇上实现了约34%的相对词错误率降低，在领域特定术语上实现了35%的相对词错误率降低。

Conclusion: 整合视觉信息（特别是演示幻灯片）能显著提升语音识别性能，特别是在领域特定术语识别方面。

Abstract: State-of-the-art (SOTA) Automatic Speech Recognition (ASR) systems primarily
rely on acoustic information while disregarding additional multi-modal context.
However, visual information are essential in disambiguation and adaptation.
While most work focus on speaker images to handle noise conditions, this work
also focuses on integrating presentation slides for the use cases of scientific
presentation.
  In a first step, we create a benchmark for multi-modal presentation including
an automatic analysis of transcribing domain-specific terminology. Next, we
explore methods for augmenting speech models with multi-modal information. We
mitigate the lack of datasets with accompanying slides by a suitable approach
of data augmentation. Finally, we train a model using the augmented dataset,
resulting in a relative reduction in word error rate of approximately 34%,
across all words and 35%, for domain-specific terms compared to the baseline
model.

</details>


### [45] [Do Large Language Models Show Biases in Causal Learning? Insights from Contingency Judgment](https://arxiv.org/abs/2510.13985)
*María Victoria Carro,Denise Alejandra Mester,Francisca Gauna Selasco,Giovanni Franco Gabriel Marraffini,Mario Alejandro Leiva,Gerardo I. Simari,María Vanina Martinez*

Main category: cs.AI

TL;DR: 研究发现大型语言模型在因果推理任务中容易产生因果幻觉，即使在没有足够证据支持的情况下也会错误地推断因果关系。


<details>
  <summary>Details</summary>
Motivation: 研究旨在探究大型语言模型是否会像人类一样产生因果幻觉，特别是在医学等需要准确因果推理的领域中。

Method: 构建了1000个零关联场景的数据集，在医学背景下测试LLMs评估潜在原因有效性的能力。

Result: 所有被评估的模型都系统地推断出无根据的因果关系，显示出对因果幻觉的强烈易感性。

Conclusion: 研究结果支持LLMs只是复制因果语言而非真正理解因果关系的假说，对在需要准确因果推理的领域中使用语言模型提出了担忧。

Abstract: Causal learning is the cognitive process of developing the capability of
making causal inferences based on available information, often guided by
normative principles. This process is prone to errors and biases, such as the
illusion of causality, in which people perceive a causal relationship between
two variables despite lacking supporting evidence. This cognitive bias has been
proposed to underlie many societal problems, including social prejudice,
stereotype formation, misinformation, and superstitious thinking. In this work,
we examine whether large language models are prone to developing causal
illusions when faced with a classic cognitive science paradigm: the contingency
judgment task. To investigate this, we constructed a dataset of 1,000 null
contingency scenarios (in which the available information is not sufficient to
establish a causal relationship between variables) within medical contexts and
prompted LLMs to evaluate the effectiveness of potential causes. Our findings
show that all evaluated models systematically inferred unwarranted causal
relationships, revealing a strong susceptibility to the illusion of causality.
While there is ongoing debate about whether LLMs genuinely understand causality
or merely reproduce causal language without true comprehension, our findings
support the latter hypothesis and raise concerns about the use of language
models in domains where accurate causal reasoning is essential for informed
decision-making.

</details>


### [46] [GammaZero: Learning To Guide POMDP Belief Space Search With Graph Representations](https://arxiv.org/abs/2510.14035)
*Rajesh Mangannavar,Prasad Tadepalli*

Main category: cs.AI

TL;DR: GammaZero提出了一种基于动作中心图表示的方法，用于在部分可观测马尔可夫决策过程中指导规划学习，能够实现跨问题规模的零样本泛化。


<details>
  <summary>Details</summary>
Motivation: 现有方法需要特定领域的神经网络架构且难以扩展，GammaZero旨在通过统一的图表示实现跨问题规模的泛化能力。

Method: 将信念状态转换为动作中心图，使用图神经网络和编码器架构从专家演示中学习价值函数和策略，然后应用这些学习到的启发式方法来指导蒙特卡洛树搜索。

Result: 在标准POMDP基准测试中，GammaZero在相同大小问题上与BetaZero性能相当，同时能够零样本泛化到训练时未见过的2-4倍大的问题，保持解质量的同时减少搜索需求。

Conclusion: GammaZero通过动作中心图表示实现了POMDP规划的可扩展学习，证明了学习到的结构模式可以从小问题迁移到大问题。

Abstract: We introduce an action-centric graph representation framework for learning to
guide planning in Partially Observable Markov Decision Processes (POMDPs).
Unlike existing approaches that require domain-specific neural architectures
and struggle with scalability, GammaZero leverages a unified graph-based belief
representation that enables generalization across problem sizes within a
domain. Our key insight is that belief states can be systematically transformed
into action-centric graphs where structural patterns learned on small problems
transfer to larger instances. We employ a graph neural network with a decoder
architecture to learn value functions and policies from expert demonstrations
on computationally tractable problems, then apply these learned heuristics to
guide Monte Carlo tree search on larger problems. Experimental results on
standard POMDP benchmarks demonstrate that GammaZero achieves comparable
performance to BetaZero when trained and tested on the same-sized problems,
while uniquely enabling zero-shot generalization to problems 2-4 times larger
than those seen during training, maintaining solution quality with reduced
search requirements.

</details>


### [47] [Position: Require Frontier AI Labs To Release Small "Analog" Models](https://arxiv.org/abs/2510.14053)
*Shriyash Upadhyay,Chaithanya Bandi,Narmeen Oozeer,Philip Quirke*

Main category: cs.AI

TL;DR: 提出一种替代性监管方法：要求大型AI实验室发布小型开放模拟模型，这些模型是从其最大专有模型训练和蒸馏而来的缩小版本，旨在确保AI安全的同时促进创新。


<details>
  <summary>Details</summary>
Motivation: 当前前沿AI模型监管提案因安全与创新的权衡而被搁置，需要一种既能确保AI安全又能积极促进创新的监管方法。

Method: 强制要求大型AI实验室发布小型开放模拟模型，作为公共代理，允许广泛参与安全验证、可解释性研究和算法透明度，而无需披露全规模模型。

Result: 研究表明，使用这些较小模型开发的安全和可解释性方法能有效推广到前沿规模系统，显著减少监管负担并加速安全进步。

Conclusion: 这种监管方法能以最小额外成本实现公共福利，说明对模型的深入理解可以缓解安全与创新的权衡，实现两者的双赢。

Abstract: Recent proposals for regulating frontier AI models have sparked concerns
about the cost of safety regulation, and most such regulations have been
shelved due to the safety-innovation tradeoff. This paper argues for an
alternative regulatory approach that ensures AI safety while actively promoting
innovation: mandating that large AI laboratories release small, openly
accessible analog models (scaled-down versions) trained similarly to and
distilled from their largest proprietary models.
  Analog models serve as public proxies, allowing broad participation in safety
verification, interpretability research, and algorithmic transparency without
forcing labs to disclose their full-scale models. Recent research demonstrates
that safety and interpretability methods developed using these smaller models
generalize effectively to frontier-scale systems. By enabling the wider
research community to directly investigate and innovate upon accessible
analogs, our policy substantially reduces the regulatory burden and accelerates
safety advancements.
  This mandate promises minimal additional costs, leveraging reusable resources
like data and infrastructure, while significantly contributing to the public
good. Our hope is not only that this policy be adopted, but that it illustrates
a broader principle supporting fundamental research in machine learning: deeper
understanding of models relaxes the safety-innovation tradeoff and lets us have
more of both.

</details>


### [48] [Generating Fair Consensus Statements with Social Choice on Token-Level MDPs](https://arxiv.org/abs/2510.14106)
*Carter Blair,Kate Larson*

Main category: cs.AI

TL;DR: 该论文提出了一个基于多目标马尔可夫决策过程的共识声明生成框架，通过社会选择理论提供可证明的公平性保证，包括两种方法：随机生成策略保证ex-ante核心稳定性，以及基于平等主义福利的搜索算法。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型共识声明生成框架缺乏提供可证明公平性保证的结构，无法在聚合多样化自由形式意见时确保公平性。

Method: 将任务建模为多目标、令牌级的马尔可夫决策过程，每个目标对应一个代理的偏好。基于代理策略（如个性化语言模型）推导令牌级奖励。提出两种基于社会选择理论的方法：保证ex-ante核心稳定性的随机生成策略，以及最大化平等主义福利的搜索算法。

Result: 实验表明，使用语言模型实例化代理策略时，基于平等主义目标的搜索算法生成的共识声明在代理对齐的最坏情况下优于基线方法，包括Habermas Machine。

Conclusion: 该框架为共识声明生成提供了具有可证明公平性保证的结构化方法，将社会选择理论原则应用于文本生成任务，在公平性和代理对齐方面表现出优越性能。

Abstract: Current frameworks for consensus statement generation with large language
models lack the inherent structure needed to provide provable fairness
guarantees when aggregating diverse free-form opinions. We model the task as a
multi-objective, token-level Markov Decision Process (MDP), where each
objective corresponds to an agent's preference. Token-level rewards for each
agent are derived from their policy (e.g., a personalized language model). This
approach utilizes the finding that such policies implicitly define optimal
Q-functions, providing a principled way to quantify rewards at each generation
step without a value function (Rafailov et al., 2024). This MDP formulation
creates a formal structure amenable to analysis using principles from social
choice theory. We propose two approaches grounded in social choice theory.
First, we propose a stochastic generation policy guaranteed to be in the
ex-ante core, extending core stability concepts from voting theory to text
generation. This policy is derived from an underlying distribution over
complete statements that maximizes proportional fairness (Nash Welfare).
Second, for generating a single statement, we target the maximization of
egalitarian welfare using search algorithms within the MDP framework.
Empirically, experiments using language models to instantiate agent policies
show that search guided by the egalitarian objective generates consensus
statements with improved worst-case agent alignment compared to baseline
methods, including the Habermas Machine (Tessler et al., 2024).

</details>


### [49] [STEMS: Spatial-Temporal Enhanced Safe Multi-Agent Coordination for Building Energy Management](https://arxiv.org/abs/2510.14112)
*Huiliang Zhang,Di Wu,Arnaud Zinflou,Benoit Boulet*

Main category: cs.AI

TL;DR: 提出STEMS框架，一种安全约束的多智能体强化学习方法，用于协调建筑能源管理，显著降低能耗成本、碳排放和安全违规率。


<details>
  <summary>Details</summary>
Motivation: 解决多建筑能源系统中空间-时间依赖性利用不足、缺乏严格安全保证和系统复杂性等关键挑战。

Method: 结合空间-时间图表示学习（GCN-Transformer融合架构）和安全约束多智能体强化学习（集成控制屏障函数）。

Result: 在真实数据集上实现21%成本降低、18%排放减少，安全违规率从35.1%降至5.6%，同时保持0.13的不适比例。

Conclusion: STEMS框架在极端天气条件下表现出强鲁棒性，在不同建筑类型中保持有效性，为协调建筑能源管理提供了安全可靠的解决方案。

Abstract: Building energy management is essential for achieving carbon reduction goals,
improving occupant comfort, and reducing energy costs. Coordinated building
energy management faces critical challenges in exploiting spatial-temporal
dependencies while ensuring operational safety across multi-building systems.
Current multi-building energy systems face three key challenges: insufficient
spatial-temporal information exploitation, lack of rigorous safety guarantees,
and system complexity. This paper proposes Spatial-Temporal Enhanced Safe
Multi-Agent Coordination (STEMS), a novel safety-constrained multi-agent
reinforcement learning framework for coordinated building energy management.
STEMS integrates two core components: (1) a spatial-temporal graph
representation learning framework using a GCN-Transformer fusion architecture
to capture inter-building relationships and temporal patterns, and (2) a
safety-constrained multi-agent RL algorithm incorporating Control Barrier
Functions to provide mathematical safety guarantees. Extensive experiments on
real-world building datasets demonstrate STEMS's superior performance over
existing methods, showing that STEMS achieves 21% cost reduction, 18% emission
reduction, and dramatically reduces safety violations from 35.1% to 5.6% while
maintaining optimal comfort with only 0.13 discomfort proportion. The framework
also demonstrates strong robustness during extreme weather conditions and
maintains effectiveness across different building types.

</details>


### [50] [Formalizing the Safety, Security, and Functional Properties of Agentic AI Systems](https://arxiv.org/abs/2510.14133)
*Edoardo Allegrini,Ananth Shreekumar,Z. Berkay Celik*

Main category: cs.AI

TL;DR: 提出了一个用于多智能体AI系统的统一建模框架，包含主机智能体模型和任务生命周期模型，定义了31个形式化属性来确保系统的正确性、可靠性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 当前多智能体AI系统的通信协议碎片化，存在语义鸿沟，无法进行严格的系统属性分析，存在架构错位和可被利用的协调问题等风险。

Method: 构建了两个基础模型：主机智能体模型（负责与用户交互、任务分解和协调执行）和任务生命周期模型（详细描述子任务从创建到完成的状态转换），并定义了17个主机智能体属性和14个任务生命周期属性。

Result: 创建了首个严格基础的领域无关框架，支持多AI智能体系统的系统分析、设计和部署，能够进行形式化验证、检测协调边界情况和预防死锁与安全漏洞。

Conclusion: 该框架为多智能体AI系统提供了统一的语义基础，能够确保系统的正确性、可靠性和鲁棒性，填补了当前碎片化通信协议带来的分析空白。

Abstract: Agentic AI systems, which leverage multiple autonomous agents and Large
Language Models (LLMs), are increasingly used to address complex, multi-step
tasks. The safety, security, and functionality of these systems are critical,
especially in high-stakes applications. However, the current ecosystem of
inter-agent communication is fragmented, with protocols such as the Model
Context Protocol (MCP) for tool access and the Agent-to-Agent (A2A) protocol
for coordination being analyzed in isolation. This fragmentation creates a
semantic gap that prevents the rigorous analysis of system properties and
introduces risks such as architectural misalignment and exploitable
coordination issues. To address these challenges, we introduce a modeling
framework for agentic AI systems composed of two foundational models. The
first, the host agent model, formalizes the top-level entity that interacts
with the user, decomposes tasks, and orchestrates their execution by leveraging
external agents and tools. The second, the task lifecycle model, details the
states and transitions of individual sub-tasks from creation to completion,
providing a fine-grained view of task management and error handling. Together,
these models provide a unified semantic framework for reasoning about the
behavior of multi-AI agent systems. Grounded in this framework, we define 17
properties for the host agent and 14 for the task lifecycle, categorized into
liveness, safety, completeness, and fairness. Expressed in temporal logic,
these properties enable formal verification of system behavior, detection of
coordination edge cases, and prevention of deadlocks and security
vulnerabilities. Through this effort, we introduce the first rigorously
grounded, domain-agnostic framework for the systematic analysis, design, and
deployment of correct, reliable, and robust agentic AI systems.

</details>


### [51] [A Multimodal Approach to Heritage Preservation in the Context of Climate Change](https://arxiv.org/abs/2510.14136)
*David Roqui,Adèle Cormier,nistor Grozavu,Ann Bourges*

Main category: cs.AI

TL;DR: 提出轻量级多模态架构，融合传感器数据和视觉图像预测文化遗产地退化程度，在斯特拉斯堡大教堂数据上达到76.9%准确率。


<details>
  <summary>Details</summary>
Motivation: 文化遗产地因气候变化加速退化，传统单模态监测方法无法捕捉环境应力与材料退化之间的复杂相互作用。

Method: 基于PerceiverIO的轻量级多模态架构，采用简化编码器（64D潜在空间）和自适应Barlow Twins损失函数，鼓励模态互补性。

Result: 在斯特拉斯堡大教堂数据上达到76.9%准确率，比标准多模态架构提升43%，比单模态方法（传感器61.5%，图像46.2%）表现更好。

Conclusion: 架构简单性结合对比正则化可在数据稀缺的遗产监测环境中实现有效的多模态学习，为AI驱动的保护决策支持系统奠定基础。

Abstract: Cultural heritage sites face accelerating degradation due to climate change,
yet tradi- tional monitoring relies on unimodal analysis (visual inspection or
environmental sen- sors alone) that fails to capture the complex interplay
between environmental stres- sors and material deterioration. We propose a
lightweight multimodal architecture that fuses sensor data (temperature,
humidity) with visual imagery to predict degradation severity at heritage
sites. Our approach adapts PerceiverIO with two key innovations: (1) simplified
encoders (64D latent space) that prevent overfitting on small datasets (n=37
training samples), and (2) Adaptive Barlow Twins loss that encourages modality
complementarity rather than redundancy. On data from Strasbourg Cathedral, our
model achieves 76.9% accu- racy, a 43% improvement over standard multimodal
architectures (VisualBERT, Trans- former) and 25% over vanilla PerceiverIO.
Ablation studies reveal that sensor-only achieves 61.5% while image-only
reaches 46.2%, confirming successful multimodal synergy. A systematic
hyperparameter study identifies an optimal moderate correlation target ({\tau}
=0.3) that balances align- ment and complementarity, achieving 69.2% accuracy
compared to other {\tau} values ({\tau} =0.1/0.5/0.7: 53.8%, {\tau} =0.9:
61.5%). This work demonstrates that architectural sim- plicity combined with
contrastive regularization enables effective multimodal learning in data-scarce
heritage monitoring contexts, providing a foundation for AI-driven con-
servation decision support systems.

</details>


### [52] [CodeEvolve: An open source evolutionary coding agent for algorithm discovery and optimization](https://arxiv.org/abs/2510.14150)
*Henrique Assumpção,Diego Ferreira,Leandro Campos,Fabricio Murai*

Main category: cs.AI

TL;DR: CodeEvolve是一个开源进化编码代理，将大语言模型与遗传算法结合解决复杂计算问题，在多个数学基准测试中超越了Google DeepMind的AlphaEvolve。


<details>
  <summary>Details</summary>
Motivation: 结合大语言模型与进化算法来解决复杂计算问题，构建一个开源框架以促进协作和加速进展。

Method: 采用基于岛屿的遗传算法保持种群多样性，引入基于启发的交叉机制利用LLM上下文窗口组合成功解决方案特征，实施元提示策略动态探索解空间。

Result: 在用于评估AlphaEvolve的数学基准测试子集上，CodeEvolve在多个具有挑战性的问题上超越了AlphaEvolve的性能。

Conclusion: CodeEvolve成功展示了将大语言模型与进化算法结合的潜力，并作为开源框架发布以推动该领域发展。

Abstract: In this work, we introduce CodeEvolve, an open-source evolutionary coding
agent that unites Large Language Models (LLMs) with genetic algorithms to solve
complex computational problems. Our framework adapts powerful evolutionary
concepts to the LLM domain, building upon recent methods for generalized
scientific discovery. CodeEvolve employs an island-based genetic algorithm to
maintain population diversity and increase throughput, introduces a novel
inspiration-based crossover mechanism that leverages the LLMs context window to
combine features from successful solutions, and implements meta-prompting
strategies for dynamic exploration of the solution space. We conduct a rigorous
evaluation of CodeEvolve on a subset of the mathematical benchmarks used to
evaluate Google DeepMind's closed-source AlphaEvolve. Our findings show that
our method surpasses AlphaEvolve's performance on several challenging problems.
To foster collaboration and accelerate progress, we release our complete
framework as an open-source repository.

</details>


### [53] [Combining Reinforcement Learning and Behavior Trees for NPCs in Video Games with AMD Schola](https://arxiv.org/abs/2510.14154)
*Tian Liu,Alex Cann,Ian Colbert,Mehdi Saeedi*

Main category: cs.AI

TL;DR: 本文探讨了强化学习在商业游戏中的应用挑战，提出将强化学习与传统行为树结合的方法，并通过AMD Schola插件在复杂3D环境中验证了该方法的可行性。


<details>
  <summary>Details</summary>
Motivation: 尽管强化学习研究进展迅速，但在商业视频游戏中的应用仍然缓慢。本文旨在解决游戏AI社区在实际使用RL驱动NPC时面临的常见挑战，并探索RL与行为树结合的关键节点。

Method: 使用AMD Schola插件在Unreal Engine中训练RL智能体，在受《最后生还者》启发的复杂3D环境中创建多任务NPC，展示了将RL模型与行为树联合训练的方法论。

Result: 证明了RL与行为树结合方法的可行性，成功创建了具备多种技能的多任务NPC。

Conclusion: RL与行为树的交叉点是一个值得进一步探索的关键领域，虽然已有研究提出这种结合，但实际应用仍然罕见，本文展示了其实际可行性。

Abstract: While the rapid advancements in the reinforcement learning (RL) research
community have been remarkable, the adoption in commercial video games remains
slow. In this paper, we outline common challenges the Game AI community faces
when using RL-driven NPCs in practice, and highlight the intersection of RL
with traditional behavior trees (BTs) as a crucial juncture to be explored
further. Although the BT+RL intersection has been suggested in several research
papers, its adoption is rare. We demonstrate the viability of this approach
using AMD Schola -- a plugin for training RL agents in Unreal Engine -- by
creating multi-task NPCs in a complex 3D environment inspired by the commercial
video game ``The Last of Us". We provide detailed methodologies for jointly
training RL models with BTs while showcasing various skills.

</details>


### [54] [JEDA: Query-Free Clinical Order Search from Ambient Dialogues](https://arxiv.org/abs/2510.14169)
*Praphul Singh,Corey Barrett,Sumana Srivasta,Amitabh Saikia,Irfan Bulu,Sri Gadde,Krishnaram Kenthapadi*

Main category: cs.AI

TL;DR: JEDA是一个用于临床订单检索的双编码器系统，能够直接从临床对话中检索规范订单，无需依赖LLM重写，提供快速、可解释的实时检索能力。


<details>
  <summary>Details</summary>
Motivation: 临床对话包含显性指令和隐性推理，现有系统依赖LLM重写会带来延迟、不稳定和不透明的问题，阻碍实时订单处理。

Method: 基于PubMedBERT初始化的双编码器，使用重复安全对比目标进行微调，通过受限LLM指导将不同意图表达与共享订单概念对齐，支持查询和无查询两种模式。

Result: JEDA在实践中取得显著提升，大幅优于基础编码器和最新开源嵌入模型，无查询模式对噪音具有鲁棒性。

Conclusion: JEDA提供了一个快速、可解释、无需LLM的检索层，能够实时将环境上下文链接到可操作的临床订单。

Abstract: Clinical conversations mix explicit directives (order a chest X-ray) with
implicit reasoning (the cough worsened overnight, we should check for
pneumonia). Many systems rely on LLM rewriting, adding latency, instability,
and opacity that hinder real-time ordering. We present JEDA (Joint Embedding
for Direct and Ambient clinical orders), a domain-initialized bi-encoder that
retrieves canonical orders directly and, in a query-free mode, encodes a short
rolling window of ambient dialogue to trigger retrieval. Initialized from
PubMedBERT and fine-tuned with a duplicate-safe contrastive objective, JEDA
aligns heterogeneous expressions of intent to shared order concepts. Training
uses constrained LLM guidance to tie each signed order to complementary
formulations (command only, context only, command+context, context+reasoning),
producing clearer inter-order separation, tighter query extendash order
coupling, and stronger generalization. The query-free mode is noise-resilient,
reducing sensitivity to disfluencies and ASR errors by conditioning on a short
window rather than a single utterance. Deployed in practice, JEDA yields large
gains and substantially outperforms its base encoder and recent open embedders
(Linq Embed Mistral, SFR Embedding, GTE Qwen, BGE large, Embedding Gemma). The
result is a fast, interpretable, LLM-free retrieval layer that links ambient
context to actionable clinical orders in real time.

</details>


### [55] [ARM-FM: Automated Reward Machines via Foundation Models for Compositional Reinforcement Learning](https://arxiv.org/abs/2510.14176)
*Roger Creus Castanyer,Faisal Mohamed,Pablo Samuel Castro,Cyrus Neary,Glen Berseth*

Main category: cs.AI

TL;DR: ARM-FM是一个利用基础模型自动生成奖励机器的强化学习框架，通过自然语言规范自动构建结构化奖励函数，实现任务分解和零样本泛化。


<details>
  <summary>Details</summary>
Motivation: 强化学习算法对奖励函数规范高度敏感，这限制了其广泛应用。现有方法需要手动设计复杂的奖励函数，过程繁琐且容易出错。

Method: 使用基础模型从自然语言规范自动生成奖励机器；为每个自动机状态关联语言嵌入以实现跨任务泛化；基于奖励机器的结构化形式化方法进行任务分解。

Result: 在多个挑战性环境中验证了ARM-FM的有效性，包括展示了零样本泛化能力，证明该方法能够从自然语言规范成功构建有效的奖励函数。

Conclusion: ARM-FM通过结合基础模型和奖励机器，实现了自动化的组合式奖励设计，解决了强化学习中奖励函数规范的核心挑战，为更广泛的RL应用提供了可行方案。

Abstract: Reinforcement learning (RL) algorithms are highly sensitive to reward
function specification, which remains a central challenge limiting their broad
applicability. We present ARM-FM: Automated Reward Machines via Foundation
Models, a framework for automated, compositional reward design in RL that
leverages the high-level reasoning capabilities of foundation models (FMs).
Reward machines (RMs) -- an automata-based formalism for reward specification
-- are used as the mechanism for RL objective specification, and are
automatically constructed via the use of FMs. The structured formalism of RMs
yields effective task decompositions, while the use of FMs enables objective
specifications in natural language. Concretely, we (i) use FMs to automatically
generate RMs from natural language specifications; (ii) associate language
embeddings with each RM automata-state to enable generalization across tasks;
and (iii) provide empirical evidence of ARM-FM's effectiveness in a diverse
suite of challenging environments, including evidence of zero-shot
generalization.

</details>


### [56] [Implementation of AI in Precision Medicine](https://arxiv.org/abs/2510.14194)
*Göktuğ Bender,Samer Faraj,Anand Bhardwaj*

Main category: cs.AI

TL;DR: 本文对2019-2024年精准医学中AI实施的文献进行了范围综述，识别了数据质量、临床可靠性、工作流程整合和治理方面的关键障碍和推动因素，提出了支持可信和可持续实施的未来方向。


<details>
  <summary>Details</summary>
Motivation: AI在精准医学中日益重要，但临床实施仍然有限，需要系统分析实施障碍和推动因素。

Method: 采用生态系统框架对2019-2024年文献进行范围综述，分析数据质量、临床可靠性、工作流程整合和治理等方面的关键因素。

Result: 识别了影响AI在精准医学中实施的关键障碍和推动因素，强调了塑造现实世界转化的相互依赖关系。

Conclusion: 提出了支持可信和可持续AI实施在精准医学中的未来方向，强调需要系统性方法来解决实施挑战。

Abstract: Artificial intelligence (AI) has become increasingly central to precision
medicine by enabling the integration and interpretation of multimodal data, yet
implementation in clinical settings remains limited. This paper provides a
scoping review of literature from 2019-2024 on the implementation of AI in
precision medicine, identifying key barriers and enablers across data quality,
clinical reliability, workflow integration, and governance. Through an
ecosystem-based framework, we highlight the interdependent relationships
shaping real-world translation and propose future directions to support
trustworthy and sustainable implementation.

</details>


### [57] [Echoes of Human Malice in Agents: Benchmarking LLMs for Multi-Turn Online Harassment Attacks](https://arxiv.org/abs/2510.14207)
*Trilok Padhi,Pinxian Lu,Abdulkadir Erol,Tanmay Sutar,Gauri Sharma,Mina Sonmez,Munmun De Choudhury,Ugur Kursuncu*

Main category: cs.AI

TL;DR: 提出了一个在线骚扰代理基准，包含多轮骚扰对话数据集、基于重复博弈理论的多代理模拟、三种越狱攻击方法和混合评估框架。研究发现越狱调优使骚扰成功率大幅提升，封闭源和开源模型表现出不同的升级轨迹。


<details>
  <summary>Details</summary>
Motivation: 现有越狱研究主要关注单轮提示，而真实骚扰往往在多轮交互中展开，需要开发多轮理论驱动的攻击方法来评估LLM代理的安全性。

Method: 构建了包含多轮骚扰对话数据集、多代理模拟、三种越狱攻击方法（针对记忆、规划和微调）的基准框架，使用LLaMA-3.1-8B-Instruct和Gemini-2.0-flash进行实验。

Result: 越狱调优使骚扰成功率从57.25-64.19%提升至95.78-96.89%（Llama），从98.46%提升至99.33%（Gemini），拒绝率降至1-2%。侮辱行为从44.2-50.8%提升至84.9-87.8%，谩骂行为从31.5-38.8%提升至81.2-85.1%。

Conclusion: 多轮理论驱动的攻击不仅成功率高，还能模拟人类骚扰动态，表明需要开发更强大的安全防护机制来保护在线平台安全。

Abstract: Large Language Model (LLM) agents are powering a growing share of interactive
web applications, yet remain vulnerable to misuse and harm. Prior jailbreak
research has largely focused on single-turn prompts, whereas real harassment
often unfolds over multi-turn interactions. In this work, we present the Online
Harassment Agentic Benchmark consisting of: (i) a synthetic multi-turn
harassment conversation dataset, (ii) a multi-agent (e.g., harasser, victim)
simulation informed by repeated game theory, (iii) three jailbreak methods
attacking agents across memory, planning, and fine-tuning, and (iv) a
mixed-methods evaluation framework. We utilize two prominent LLMs,
LLaMA-3.1-8B-Instruct (open-source) and Gemini-2.0-flash (closed-source). Our
results show that jailbreak tuning makes harassment nearly guaranteed with an
attack success rate of 95.78--96.89% vs. 57.25--64.19% without tuning in Llama,
and 99.33% vs. 98.46% without tuning in Gemini, while sharply reducing refusal
rate to 1-2% in both models. The most prevalent toxic behaviors are Insult with
84.9--87.8% vs. 44.2--50.8% without tuning, and Flaming with 81.2--85.1% vs.
31.5--38.8% without tuning, indicating weaker guardrails compared to sensitive
categories such as sexual or racial harassment. Qualitative evaluation further
reveals that attacked agents reproduce human-like aggression profiles, such as
Machiavellian/psychopathic patterns under planning, and narcissistic tendencies
with memory. Counterintuitively, closed-source and open-source models exhibit
distinct escalation trajectories across turns, with closed-source models
showing significant vulnerability. Overall, our findings show that multi-turn
and theory-grounded attacks not only succeed at high rates but also mimic
human-like harassment dynamics, motivating the development of robust safety
guardrails to ultimately keep online platforms safe and responsible.

</details>


### [58] [LiveResearchBench: A Live Benchmark for User-Centric Deep Research in the Wild](https://arxiv.org/abs/2510.14240)
*Jiayu Wang,Yifei Ming,Riya Dulepet,Qinglin Chen,Austin Xu,Zixuan Ke,Frederic Sala,Aws Albarghouthi,Caiming Xiong,Shafiq Joty*

Main category: cs.AI

TL;DR: 提出了LiveResearchBench基准和DeepEval评估套件，用于系统性评估深度研究系统的能力，涵盖100个专家策划的任务和全面的评估指标。


<details>
  <summary>Details</summary>
Motivation: 现有基准在评估深度研究系统时存在不足，往往局限于狭窄领域或提出模糊问题，无法公平比较。需要遵循用户中心、动态性、明确性和多面性四个原则来构建更合适的评估框架。

Method: 开发了LiveResearchBench基准（100个专家策划任务）和DeepEval评估套件（包含内容和报告层面的质量评估），整合了四种互补的评估协议，对17个前沿深度研究系统进行全面评估。

Result: 通过系统性评估揭示了当前系统的优势、常见失败模式以及推进可靠深度研究所需的关键系统组件。

Conclusion: LiveResearchBench和DeepEval为深度研究系统提供了严格的评估基础，有助于识别改进方向并推动该领域的发展。

Abstract: Deep research -- producing comprehensive, citation-grounded reports by
searching and synthesizing information from hundreds of live web sources --
marks an important frontier for agentic systems. To rigorously evaluate this
ability, four principles are essential: tasks should be (1) user-centric,
reflecting realistic information needs, (2) dynamic, requiring up-to-date
information beyond parametric knowledge, (3) unambiguous, ensuring consistent
interpretation across users, and (4) multi-faceted and search-intensive,
requiring search over numerous web sources and in-depth analysis. Existing
benchmarks fall short of these principles, often focusing on narrow domains or
posing ambiguous questions that hinder fair comparison. Guided by these
principles, we introduce LiveResearchBench, a benchmark of 100 expert-curated
tasks spanning daily life, enterprise, and academia, each requiring extensive,
dynamic, real-time web search and synthesis. Built with over 1,500 hours of
human labor, LiveResearchBench provides a rigorous basis for systematic
evaluation. To evaluate citation-grounded long-form reports, we introduce
DeepEval, a comprehensive suite covering both content- and report-level
quality, including coverage, presentation, citation accuracy and association,
consistency and depth of analysis. DeepEval integrates four complementary
evaluation protocols, each designed to ensure stable assessment and high
agreement with human judgments. Using LiveResearchBench and DeepEval, we
conduct a comprehensive evaluation of 17 frontier deep research systems,
including single-agent web search, single-agent deep research, and multi-agent
systems. Our analysis reveals current strengths, recurring failure modes, and
key system components needed to advance reliable, insightful deep research.

</details>


### [59] [Towards Agentic Self-Learning LLMs in Search Environment](https://arxiv.org/abs/2510.14253)
*Wangtao Sun,Xiang Cheng,Jialin Fan,Yao Xu,Xing Yu,Shizhu He,Jun Zhao,Kang Liu*

Main category: cs.AI

TL;DR: 提出了Agentic Self-Learning (ASL)框架，通过多角色协同进化的强化学习实现无需人工标注数据的自主智能体训练，发现奖励信号来源和数据规模是开放域智能体学习的关键因素。


<details>
  <summary>Details</summary>
Motivation: 研究是否可以不依赖人工标注数据集或预定义规则奖励，通过自主学习来扩展基于LLM的智能体能力。

Method: 提出ASL框架，在共享工具环境和LLM主干中统一任务生成、策略执行和评估，协调提示生成器、策略模型和生成式奖励模型形成良性循环。

Result: ASL实现稳定逐轮提升，超越强基线方法，在零标注数据条件下持续改进，表现出优异的样本效率和鲁棒性。

Conclusion: 奖励来源和数据规模是开放域智能体学习的关键杠杆，多角色协同进化对可扩展、自我改进的智能体具有显著效果。

Abstract: We study whether self-learning can scale LLM-based agents without relying on
human-curated datasets or predefined rule-based rewards. Through controlled
experiments in a search-agent setting, we identify two key determinants of
scalable agent training: the source of reward signals and the scale of agent
task data. We find that rewards from a Generative Reward Model (GRM) outperform
rigid rule-based signals for open-domain learning, and that co-evolving the GRM
with the policy further boosts performance. Increasing the volume of agent task
data-even when synthetically generated-substantially enhances agentic
capabilities. Building on these insights, we propose \textbf{Agentic
Self-Learning} (ASL), a fully closed-loop, multi-role reinforcement learning
framework that unifies task generation, policy execution, and evaluation within
a shared tool environment and LLM backbone. ASL coordinates a Prompt Generator,
a Policy Model, and a Generative Reward Model to form a virtuous cycle of
harder task setting, sharper verification, and stronger solving. Empirically,
ASL delivers steady, round-over-round gains, surpasses strong RLVR baselines
(e.g., Search-R1) that plateau or degrade, and continues improving under
zero-labeled-data conditions, indicating superior sample efficiency and
robustness. We further show that GRM verification capacity is the main
bottleneck: if frozen, it induces reward hacking and stalls progress; continual
GRM training on the evolving data distribution mitigates this, and a small
late-stage injection of real verification data raises the performance ceiling.
This work establishes reward source and data scale as critical levers for
open-domain agent learning and demonstrates the efficacy of multi-role
co-evolution for scalable, self-improving agents. The data and code of this
paper are released at
https://github.com/forangel2014/Towards-Agentic-Self-Learning

</details>


### [60] [MorphoBench: A Benchmark with Difficulty Adaptive to Model Reasoning](https://arxiv.org/abs/2510.14265)
*Xukai Wang,Xuanbo Liu,Mingrui Chen,Haitian Zhong,Xuanlin Yang,Bohan Zeng,Jinbo Hu,Hao Liang,Junbo Niu,Xuchen Li,Ruitao Wu,Ruichuan An,Yang Shi,Liu Liu,Xu-Yao Zhang,Qiang Liu,Zhouchen Lin,Wentao Zhang,Bin Dong*

Main category: cs.AI

TL;DR: 提出了MorphoBench基准测试，用于评估大型模型的推理能力，能够根据模型推理能力动态调整问题难度，包含1300多个测试问题。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试范围有限，缺乏根据模型推理能力动态调整难度的灵活性，无法适应模型推理能力的演进。

Method: 从现有基准和奥林匹克竞赛等来源收集复杂推理问题；利用模型推理过程中的关键陈述自适应修改问题分析难度；使用仿真软件生成问题，实现低资源消耗的动态难度调整。

Result: 收集了1300多个测试问题，基于o3和GPT-5等模型的推理能力迭代调整了MorphoBench的难度。

Conclusion: MorphoBench增强了模型推理评估的全面性和有效性，为提升大型模型的推理能力和科学鲁棒性提供了可靠指导。

Abstract: With the advancement of powerful large-scale reasoning models, effectively
evaluating the reasoning capabilities of these models has become increasingly
important. However, existing benchmarks designed to assess the reasoning
abilities of large models tend to be limited in scope and lack the flexibility
to adapt their difficulty according to the evolving reasoning capacities of the
models. To address this, we propose MorphoBench, a benchmark that incorporates
multidisciplinary questions to evaluate the reasoning capabilities of large
models and can adjust and update question difficulty based on the reasoning
abilities of advanced models. Specifically, we curate the benchmark by
selecting and collecting complex reasoning questions from existing benchmarks
and sources such as Olympiad-level competitions. Additionally, MorphoBench
adaptively modifies the analytical challenge of questions by leveraging key
statements generated during the model's reasoning process. Furthermore, it
includes questions generated using simulation software, enabling dynamic
adjustment of benchmark difficulty with minimal resource consumption. We have
gathered over 1,300 test questions and iteratively adjusted the difficulty of
MorphoBench based on the reasoning capabilities of models such as o3 and GPT-5.
MorphoBench enhances the comprehensiveness and validity of model reasoning
evaluation, providing reliable guidance for improving both the reasoning
abilities and scientific robustness of large models. The code has been released
in https://github.com/OpenDCAI/MorphoBench.

</details>


### [61] [A Guardrail for Safety Preservation: When Safety-Sensitive Subspace Meets Harmful-Resistant Null-Space](https://arxiv.org/abs/2510.14301)
*Bingjie Zhang,Yibo Yang,Renzhe,Dandan Guo,Jindong Gu,Philip Torr,Bernard Ghanem*

Main category: cs.AI

TL;DR: GuardSpace是一个在微调过程中保护大语言模型安全对齐的框架，通过安全敏感子空间和有害抵抗零空间来防止安全行为退化。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在微调过程中容易失去预训练的安全对齐，即使使用良性数据或低秩适配也会产生有害响应，需要保护安全机制。

Method: 使用协方差预条件奇异值分解将预训练权重分解为安全相关和安全无关组件，初始化低秩适配器从安全无关组件，同时冻结安全相关组件；构建零空间投影器限制适配器更新对有害提示的安全输出。

Result: 在多个下游任务上的实验表明，GuardSpace优于现有方法。对于Llama-2-7B-Chat在GSM8K上的微调，有害评分从14.4%降至3.6%，准确率从26.0%提升至28.0%。

Conclusion: GuardSpace框架能有效在微调过程中保持模型的安全对齐，同时提升任务性能，解决了安全行为退化问题。

Abstract: Large language models (LLMs) have achieved remarkable success in diverse
tasks, yet their safety alignment remains fragile during adaptation. Even when
fine-tuning on benign data or with low-rank adaptation, pre-trained safety
behaviors are easily degraded, leading to harmful responses in the fine-tuned
models. To address this challenge, we propose GuardSpace, a guardrail framework
for preserving safety alignment throughout fine-tuning, composed of two key
components: a safety-sensitive subspace and a harmful-resistant null space.
First, we explicitly decompose pre-trained weights into safety-relevant and
safety-irrelevant components using covariance-preconditioned singular value
decomposition, and initialize low-rank adapters from the safety-irrelevant
ones, while freezing safety-relevant components to preserve their associated
safety mechanism. Second, we construct a null space projector that restricts
adapter updates from altering safe outputs on harmful prompts, thereby
maintaining the original refusal behavior. Experiments with various pre-trained
models on multiple downstream tasks demonstrate that GuardSpace achieves
superior performance over existing methods. Notably, for Llama-2-7B-Chat
fine-tuned on GSM8K, GuardSpace outperforms the state-of-the-art method AsFT,
reducing the average harmful score from 14.4% to 3.6%, while improving the
accuracy from from 26.0% to 28.0%.

</details>


### [62] [Terrarium: Revisiting the Blackboard for Multi-Agent Safety, Privacy, and Security Studies](https://arxiv.org/abs/2510.14312)
*Mason Nakamura,Abhinav Kumar,Saaduddin Mahmud,Sahar Abdelnabi,Shlomo Zilberstein,Eugene Bagdasarian*

Main category: cs.AI

TL;DR: 提出了Terrarium框架，用于在基于LLM的多智能体系统中进行细粒度的安全、隐私和安全性研究，通过重新设计黑板架构创建模块化、可配置的测试平台。


<details>
  <summary>Details</summary>
Motivation: LLM驱动的多智能体系统虽然能自动化用户任务，但引入了新的风险，包括错位、恶意方攻击、智能体被破坏或用户数据被盗等问题。

Method: 重新利用多智能体系统中的黑板设计，创建模块化、可配置的测试平台，识别关键攻击向量如错位、恶意智能体、通信被破坏和数据中毒。

Result: 实现了三个协作多智能体场景和四种代表性攻击，展示了框架的灵活性。

Conclusion: 通过提供快速原型设计、评估和迭代防御与设计的工具，Terrarium旨在加速可信多智能体系统的进展。

Abstract: A multi-agent system (MAS) powered by large language models (LLMs) can
automate tedious user tasks such as meeting scheduling that requires
inter-agent collaboration. LLMs enable nuanced protocols that account for
unstructured private data, user constraints, and preferences. However, this
design introduces new risks, including misalignment and attacks by malicious
parties that compromise agents or steal user data. In this paper, we propose
the Terrarium framework for fine-grained study on safety, privacy, and security
in LLM-based MAS. We repurpose the blackboard design, an early approach in
multi-agent systems, to create a modular, configurable testbed for multi-agent
collaboration. We identify key attack vectors such as misalignment, malicious
agents, compromised communication, and data poisoning. We implement three
collaborative MAS scenarios with four representative attacks to demonstrate the
framework's flexibility. By providing tools to rapidly prototype, evaluate, and
iterate on defenses and designs, Terrarium aims to accelerate progress toward
trustworthy multi-agent systems.

</details>


### [63] [Metacognitive Self-Correction for Multi-Agent System via Prototype-Guided Next-Execution Reconstruction](https://arxiv.org/abs/2510.14319)
*Xu Shen,Qi Zhang,Song Wang,Zhen Tan,Xinyu Zhao,Laura Yao,Vaishnav Tadiparthi,Hossein Nourkhiz Mahjoub,Ehsan Moradi Pari,Kwonjoon Lee,Tianlong Chen*

Main category: cs.AI

TL;DR: MASC是一个元认知框架，为多智能体系统提供实时、无监督的步骤级错误检测和自我纠正，通过历史条件异常评分来防止错误级联传播。


<details>
  <summary>Details</summary>
Motivation: 解决多智能体系统中单个错误步骤会跨智能体传播并破坏整个轨迹的问题，提高系统的鲁棒性。

Method: 采用两种互补设计：1) 下一执行重构，从查询和交互历史预测下一步的嵌入来捕捉因果一致性；2) 原型引导增强，学习正常步骤嵌入的原型先验来稳定稀疏上下文下的重构和异常评分。

Result: 在Who&When基准测试中，MASC始终优于所有基线方法，步骤级错误检测的AUC-ROC提高了8.47%；在不同MAS框架中都能带来一致的端到端性能提升。

Conclusion: MASC的元认知监控和针对性纠正能够以最小开销缓解错误传播，在不同架构中都能提供稳定的性能增益。

Abstract: Large Language Model based multi-agent systems (MAS) excel at collaborative
problem solving but remain brittle to cascading errors: a single faulty step
can propagate across agents and disrupt the trajectory. In this paper, we
present MASC, a metacognitive framework that endows MAS with real-time,
unsupervised, step-level error detection and self-correction. MASC rethinks
detection as history-conditioned anomaly scoring via two complementary designs:
(1) Next-Execution Reconstruction, which predicts the embedding of the next
step from the query and interaction history to capture causal consistency, and
(2) Prototype-Guided Enhancement, which learns a prototype prior over
normal-step embeddings and uses it to stabilize reconstruction and anomaly
scoring under sparse context (e.g., early steps). When an anomaly step is
flagged, MASC triggers a correction agent to revise the acting agent's output
before information flows downstream. On the Who&When benchmark, MASC
consistently outperforms all baselines, improving step-level error detection by
up to 8.47% AUC-ROC ; When plugged into diverse MAS frameworks, it delivers
consistent end-to-end gains across architectures, confirming that our
metacognitive monitoring and targeted correction can mitigate error propagation
with minimal overhead.

</details>


### [64] [AI for Service: Proactive Assistance with AI Glasses](https://arxiv.org/abs/2510.14359)
*Zichen Wen,Yiyu Wang,Chenfei Liao,Boxue Yang,Junxian Li,Weifeng Liu,Haocong He,Bolong Feng,Xuyang Liu,Yuanhuiyi Lyu,Xu Zheng,Xuming Hu,Linfeng Zhang*

Main category: cs.AI

TL;DR: 提出了AI4Service新范式，通过Alpha-Service框架实现主动式AI服务，解决"何时干预"和"如何服务"两大挑战，在AI眼镜上部署多智能体系统提供实时辅助。


<details>
  <summary>Details</summary>
Motivation: 现有AI服务多为被动响应，缺乏主动性和适应性，需要能够预见用户需求并在适当时机主动提供帮助的智能助手。

Method: 基于冯·诺依曼计算机架构，构建包含输入单元、中央处理单元、算术逻辑单元、内存单元和输出单元的Alpha-Service框架，通过多智能体系统实现。

Result: 案例研究（21点游戏顾问、博物馆导览、购物搭配助手）显示系统能够无缝感知环境、推断用户意图，无需明确提示即可提供及时有用的帮助。

Conclusion: AI4Service范式为主动式AI助手提供了可行方案，Alpha-Service框架在实时感知和个性化服务方面表现出色，为未来智能服务发展指明方向。

Abstract: In an era where AI is evolving from a passive tool into an active and
adaptive companion, we introduce AI for Service (AI4Service), a new paradigm
that enables proactive and real-time assistance in daily life. Existing AI
services remain largely reactive, responding only to explicit user commands. We
argue that a truly intelligent and helpful assistant should be capable of
anticipating user needs and taking actions proactively when appropriate. To
realize this vision, we propose Alpha-Service, a unified framework that
addresses two fundamental challenges: Know When to intervene by detecting
service opportunities from egocentric video streams, and Know How to provide
both generalized and personalized services. Inspired by the von Neumann
computer architecture and based on AI glasses, Alpha-Service consists of five
key components: an Input Unit for perception, a Central Processing Unit for
task scheduling, an Arithmetic Logic Unit for tool utilization, a Memory Unit
for long-term personalization, and an Output Unit for natural human
interaction. As an initial exploration, we implement Alpha-Service through a
multi-agent system deployed on AI glasses. Case studies, including a real-time
Blackjack advisor, a museum tour guide, and a shopping fit assistant,
demonstrate its ability to seamlessly perceive the environment, infer user
intent, and provide timely and useful assistance without explicit prompts.

</details>


### [65] [Can MLLMs Absorb Math Reasoning Abilities from LLMs as Free Lunch?](https://arxiv.org/abs/2510.14387)
*Yijie Hu,Zihao Zhou,Kaizhu Huang,Xiaowei Huang,Qiufeng Wang*

Main category: cs.AI

TL;DR: IP-Merging是一种无需调优的方法，通过识别多模态大模型和数学大模型中的推理相关参数，将其投影到多模态大模型的子空间中进行融合，从而直接提升多模态大模型的数学推理能力。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大模型在数学推理方面的性能落后于纯文本大模型，但直接使用模型融合方法会因参数空间不匹配而导致性能下降。

Method: 提出IP-Merging方法：首先识别多模态大模型和数学大模型中的推理相关参数，然后将其投影到多模态大模型的子空间中，最后在该子空间内融合参数。

Result: 实验表明IP-Merging能够在不损害多模态大模型其他能力的前提下，直接从数学大模型吸收数学推理能力。

Conclusion: IP-Merging是一种有效的无需调优的方法，能够显著提升多模态大模型的数学推理性能。

Abstract: Math reasoning has been one crucial ability of large language models (LLMs),
where significant advancements have been achieved in recent years. However,
most efforts focus on LLMs by curating high-quality annotation data and
intricate training (or inference) paradigms, while the math reasoning
performance of multi-modal LLMs (MLLMs) remains lagging behind. Since the MLLM
typically consists of an LLM and a vision block, we wonder: Can MLLMs directly
absorb math reasoning abilities from off-the-shelf math LLMs without tuning?
Recent model-merging approaches may offer insights into this question. However,
they overlook the alignment between the MLLM and LLM, where we find that there
is a large gap between their parameter spaces, resulting in lower performance.
Our empirical evidence reveals two key factors behind this issue: the
identification of crucial reasoning-associated layers in the model and the
mitigation of the gaps in parameter space. Based on the empirical insights, we
propose IP-Merging that first identifies the reasoning-associated parameters in
both MLLM and Math LLM, then projects them into the subspace of MLLM, aiming to
maintain the alignment, and finally merges parameters in this subspace.
IP-Merging is a tuning-free approach since parameters are directly adjusted.
Extensive experiments demonstrate that our IP-Merging method can enhance the
math reasoning ability of MLLMs directly from Math LLMs without compromising
their other capabilities.

</details>


### [66] [Hi-Agent: Hierarchical Vision-Language Agents for Mobile Device Control](https://arxiv.org/abs/2510.14388)
*Zhe Wu,Hongjin Lu,Junliang Xing,Changhao Zhang,Yin Zhu,Yuhao Yang,Yuheng Jing,Kai Li,Kun Shao,Jianye Hao,Jun Wang,Yuanchun Shi*

Main category: cs.AI

TL;DR: Hi-Agent是一个可训练的分层视觉语言代理，用于移动设备控制，通过高层推理模型和低层动作模型的联合优化，在Android-in-the-Wild基准测试中达到87.9%的任务成功率，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于视觉语言模型的移动设备控制方法主要依赖直接的状态到动作映射，缺乏结构化推理和规划，导致在新任务或未见UI布局上泛化能力差。

Method: 提出分层架构，包含高层推理模型和低层动作模型，将多步决策重新表述为单步子目标序列，并设计前瞻优势函数，利用低层模型的执行反馈指导高层优化，缓解长时任务中的路径爆炸问题。

Result: 在Android-in-the-Wild基准测试中达到87.9%的任务成功率，显著优于提示型（AppAgent: 17.7%）、监督学习（Filtered BC: 54.5%）和强化学习（DigiRL: 71.9%）方法；在ScreenSpot-v2基准测试中展示有竞争力的零样本泛化能力。

Conclusion: Hi-Agent通过分层设计和联合优化策略，在移动设备控制任务中实现了最先进的性能，并在复杂场景中展现出良好的可扩展性和适应性。

Abstract: Building agents that autonomously operate mobile devices has attracted
increasing attention. While Vision-Language Models (VLMs) show promise, most
existing approaches rely on direct state-to-action mappings, which lack
structured reasoning and planning, and thus generalize poorly to novel tasks or
unseen UI layouts. We introduce Hi-Agent, a trainable hierarchical
vision-language agent for mobile control, featuring a high-level reasoning
model and a low-level action model that are jointly optimized. For efficient
training, we reformulate multi-step decision-making as a sequence of
single-step subgoals and propose a foresight advantage function, which
leverages execution feedback from the low-level model to guide high-level
optimization. This design alleviates the path explosion issue encountered by
Group Relative Policy Optimization (GRPO) in long-horizon tasks and enables
stable, critic-free joint training. Hi-Agent achieves a new State-Of-The-Art
(SOTA) 87.9% task success rate on the Android-in-the-Wild (AitW) benchmark,
significantly outperforming prior methods across three paradigms: prompt-based
(AppAgent: 17.7%), supervised (Filtered BC: 54.5%), and reinforcement
learning-based (DigiRL: 71.9%). It also demonstrates competitive zero-shot
generalization on the ScreenSpot-v2 benchmark. On the more challenging
AndroidWorld benchmark, Hi-Agent also scales effectively with larger backbones,
showing strong adaptability in high-complexity mobile control scenarios.

</details>


### [67] [IMAGINE: Integrating Multi-Agent System into One Model for Complex Reasoning and Planning](https://arxiv.org/abs/2510.14406)
*Xikai Zhang,Bo Wang,Likang Xiao,Yongzhi Li,Quan Chen,Wenju Wu,Liu Liu*

Main category: cs.AI

TL;DR: 提出IMAGINE框架，将多智能体系统的推理规划能力集成到单一紧凑模型中，通过端到端训练显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在复杂推理和规划任务中表现不佳，多智能体系统虽能改善集体推理但存在推理成本高、延迟长、难以端到端训练等问题。

Method: IMAGINE框架将多智能体系统的能力集成到单一模型中，通过简单端到端训练实现性能超越。

Result: 在TravelPlanner基准测试中，使用Qwen3-8B-Instruct作为基础模型，经过IMAGINE训练后达到82.7%的最终通过率，远超DeepSeek-R1-671B的40%。

Conclusion: IMAGINE框架成功将多智能体系统的优势整合到单一紧凑模型中，在保持小模型规模的同时显著提升了推理规划能力。

Abstract: Although large language models (LLMs) have made significant strides across
various tasks, they still face significant challenges in complex reasoning and
planning. For example, even with carefully designed prompts and prior
information explicitly provided, GPT-4o achieves only a 7% Final Pass Rate on
the TravelPlanner dataset in the sole-planning mode. Similarly, even in the
thinking mode, Qwen3-8B-Instruct and DeepSeek-R1-671B, only achieve Final Pass
Rates of 5.9% and 40%, respectively. Although well-organized Multi-Agent
Systems (MAS) can offer improved collective reasoning, they often suffer from
high reasoning costs due to multi-round internal interactions, long
per-response latency, and difficulties in end-to-end training. To address these
challenges, we propose a general and scalable framework called IMAGINE, short
for Integrating Multi-Agent System into One Model. This framework not only
integrates the reasoning and planning capabilities of MAS into a single,
compact model, but also significantly surpass the capabilities of the MAS
through a simple end-to-end training. Through this pipeline, a single
small-scale model is not only able to acquire the structured reasoning and
planning capabilities of a well-organized MAS but can also significantly
outperform it. Experimental results demonstrate that, when using
Qwen3-8B-Instruct as the base model and training it with our method, the model
achieves an 82.7% Final Pass Rate on the TravelPlanner benchmark, far exceeding
the 40% of DeepSeek-R1-671B, while maintaining a much smaller model size.

</details>


### [68] [Eliminating Negative Occurrences of Derived Predicates from PDDL Axioms](https://arxiv.org/abs/2510.14412)
*Claudia Grundke,Gabriele Röger*

Main category: cs.AI

TL;DR: 本文提出了一种转换方法，用于消除PDDL公理中派生谓词的负出现，证明这种限制实际上不会减少表达能力。


<details>
  <summary>Details</summary>
Motivation: PDDL标准限制公理体中谓词的负出现只能用于直接由动作设置的谓词，而不能用于由公理派生的谓词。但在文献中，作者常常偏离这一限制，只要求公理集是可分层的。本文旨在证明这两种变体实际上具有相同的表达能力。

Method: 提出了一种转换方法，通过这种转换可以消除公理中派生谓词的负出现，从而将允许负出现的可分层公理集转换为符合PDDL标准限制的形式。

Result: 证明了两种变体（PDDL标准限制和可分层的负出现）可以表达完全相同的查询，即最小不动点逻辑所表达的查询。

Conclusion: PDDL标准对公理中派生谓词负出现的限制实际上不会减少其表达能力，因为通过所提出的转换方法，任何可分层的公理集都可以转换为符合标准限制的形式。

Abstract: Axioms are a feature of the Planning Domain Definition Language PDDL that can
be considered as a generalization of database query languages such as Datalog.
The PDDL standard restricts negative occurrences of predicates in axiom bodies
to predicates that are directly set by actions and not derived by axioms. In
the literature, authors often deviate from this limitation and only require
that the set of axioms is stratifiable. Both variants can express exactly the
same queries as least fixed-point logic, indicating that negative occurrences
of derived predicates can be eliminated. We present the corresponding
transformation.

</details>


### [69] [TITAN: Graph-Executable Reasoning for Cyber Threat Intelligence](https://arxiv.org/abs/2510.14670)
*Marco Simoni,Aleksandar Fontana,Andrea Saracino,Paolo Mori*

Main category: cs.AI

TL;DR: TITAN是一个将自然语言网络安全威胁查询与结构化知识图谱上的可执行推理连接起来的框架，包含路径规划模型和图执行器，支持在威胁、行为和防御之间进行双向推理。


<details>
  <summary>Details</summary>
Motivation: 传统检索系统无法在网络安全威胁、行为和防御之间进行清晰可逆的推理，需要一种能够将自然语言查询与结构化知识图谱推理相结合的系统。

Method: 集成路径规划模型预测文本中的逻辑关系链，使用图执行器遍历TITAN本体图检索事实答案和证据，基于MITRE构建类型化双向图。

Result: 创建了包含88209个示例的TITAN数据集，实证评估显示TITAN能生成语法有效且语义连贯的推理路径，可在底层图上确定性执行。

Conclusion: TITAN框架成功实现了自然语言威胁查询与结构化知识图谱推理的集成，为网络安全威胁情报分析提供了有效的自动化解决方案。

Abstract: TITAN (Threat Intelligence Through Automated Navigation) is a framework that
connects natural-language cyber threat queries with executable reasoning over a
structured knowledge graph. It integrates a path planner model, which predicts
logical relation chains from text, and a graph executor that traverses the
TITAN Ontology to retrieve factual answers and supporting evidence. Unlike
traditional retrieval systems, TITAN operates on a typed, bidirectional graph
derived from MITRE, allowing reasoning to move clearly and reversibly between
threats, behaviors, and defenses. To support training and evaluation, we
introduce the TITAN Dataset, a corpus of 88209 examples (Train: 74258; Test:
13951) pairing natural language questions with executable reasoning paths and
step by step Chain of Thought explanations. Empirical evaluations show that
TITAN enables models to generate syntactically valid and semantically coherent
reasoning paths that can be deterministically executed on the underlying graph.

</details>


### [70] [Helmsman: Autonomous Synthesis of Federated Learning Systems via Multi-Agent Collaboration](https://arxiv.org/abs/2510.14512)
*Haoyuan Li,Mathias Funk,Aaqib Saeed*

Main category: cs.AI

TL;DR: Helmsman是一个多智能体系统，通过模拟研发工作流自动合成联邦学习系统，包含交互式规划、模块化代码生成和自主评估三个协作阶段。


<details>
  <summary>Details</summary>
Motivation: 联邦学习系统设计复杂，需要应对数据异构性和系统约束等多方面挑战，现有解决方案往往脆弱且定制化，成为关键瓶颈。

Method: 采用三阶段多智能体协作：1)人机交互规划制定研究计划；2)监督智能体团队进行模块化代码生成；3)在沙盒模拟环境中进行闭环自主评估和优化。

Result: 实验表明，该方法生成的解决方案与手工构建的基线方法相当甚至更优，并引入了AgentFL-Bench基准来评估系统级生成能力。

Conclusion: 该工作代表了自动化复杂分布式AI系统工程的重大进展。

Abstract: Federated Learning (FL) offers a powerful paradigm for training models on
decentralized data, but its promise is often undermined by the immense
complexity of designing and deploying robust systems. The need to select,
combine, and tune strategies for multifaceted challenges like data
heterogeneity and system constraints has become a critical bottleneck,
resulting in brittle, bespoke solutions. To address this, we introduce
Helmsman, a novel multi-agent system that automates the end-to-end synthesis of
federated learning systems from high-level user specifications. It emulates a
principled research and development workflow through three collaborative
phases: (1) interactive human-in-the-loop planning to formulate a sound
research plan, (2) modular code generation by supervised agent teams, and (3) a
closed-loop of autonomous evaluation and refinement in a sandboxed simulation
environment. To facilitate rigorous evaluation, we also introduce
AgentFL-Bench, a new benchmark comprising 16 diverse tasks designed to assess
the system-level generation capabilities of agentic systems in FL. Extensive
experiments demonstrate that our approach generates solutions competitive with,
and often superior to, established hand-crafted baselines. Our work represents
a significant step towards the automated engineering of complex decentralized
AI systems.

</details>


### [71] [JSPLIT: A Taxonomy-based Solution for Prompt Bloating in Model Context Protocol](https://arxiv.org/abs/2510.14537)
*Emanuele Antonioni,Stefan Markovic,Anirudha Shankar,Jaime Bernardo,Lovro Markovic,Silvia Pareti,Benedetto Proietti*

Main category: cs.AI

TL;DR: JSPLIT是一个基于分类学的框架，通过层次化组织MCP工具并基于用户提示选择最相关工具，有效解决提示膨胀问题，减少提示大小同时保持代理性能。


<details>
  <summary>Details</summary>
Motivation: 随着AI系统发展，用户期望从简单的文本交互转向需要LLM与外部工具交互的复杂代理系统。MCP等标准使代理能访问工具，但随着工具数量增加，提示膨胀问题日益严重，导致高token成本、延迟增加和任务成功率下降。

Method: JSPLIT将工具组织成层次化分类学结构，使用用户提示基于查询和分类学结构识别并仅包含最相关的工具，设计了分类学结构和工具选择算法。

Result: JSPLIT显著减少了提示大小，且未显著影响代理响应能力。当可用工具数量大幅增加时，JSPLIT甚至提高了代理的工具选择准确性，在降低成本的同时改善了高复杂度环境中的任务成功率。

Conclusion: JSPLIT框架有效解决了MCP工具使用中的提示膨胀问题，通过分类学驱动的工具选择，在减少提示大小的同时保持甚至提升了代理性能，特别适用于工具数量庞大的复杂代理环境。

Abstract: AI systems are continually evolving and advancing, and user expectations are
concurrently increasing, with a growing demand for interactions that go beyond
simple text-based interaction with Large Language Models (LLMs). Today's
applications often require LLMs to interact with external tools, marking a
shift toward more complex agentic systems. To support this, standards such as
the Model Context Protocol (MCP) have emerged, enabling agents to access tools
by including a specification of the capabilities of each tool within the
prompt. Although this approach expands what agents can do, it also introduces a
growing problem: prompt bloating. As the number of tools increases, the prompts
become longer, leading to high prompt token costs, increased latency, and
reduced task success resulting from the selection of tools irrelevant to the
prompt. To address this issue, we introduce JSPLIT, a taxonomy-driven framework
designed to help agents manage prompt size more effectively when using large
sets of MCP tools. JSPLIT organizes the tools into a hierarchical taxonomy and
uses the user's prompt to identify and include only the most relevant tools,
based on both the query and the taxonomy structure. In this paper, we describe
the design of the taxonomy, the tool selection algorithm, and the dataset used
to evaluate JSPLIT. Our results show that JSPLIT significantly reduces prompt
size without significantly compromising the agent's ability to respond
effectively. As the number of available tools for the agent grows
substantially, JSPLIT even improves the tool selection accuracy of the agent,
effectively reducing costs while simultaneously improving task success in
high-complexity agent environments.

</details>


### [72] [Mapping Smarter, Not Harder: A Test-Time Reinforcement Learning Agent That Improves Without Labels or Model Updates](https://arxiv.org/abs/2510.14900)
*Wen-Kwang Tsao,Yao-Ching Yu,Chien-Ming Huang*

Main category: cs.AI

TL;DR: 提出了一种基于强化学习的智能体，能够在没有标注数据或模型权重更新的情况下自我改进，通过生成针对性网络搜索查询来收集外部证据，显著提高企业智能平台中第三方日志的模式映射准确性。


<details>
  <summary>Details</summary>
Motivation: 企业智能平台需要整合来自众多第三方供应商的日志，但供应商文档在测试时往往不可用、格式混乱或不完整，导致模式映射变得困难。

Method: 使用强化学习智能体，在推理过程中：1)识别模糊的字段映射尝试；2)生成针对性网络搜索查询收集外部证据；3)应用基于置信度的奖励来迭代优化映射。

Result: 将Microsoft Defender for Endpoint日志转换为通用模式，映射准确率从56.4%(仅LLM)提升到72.73%(RAG)再到93.94%(100次迭代后)，同时将需要专家审查的低置信度映射减少了85%。

Conclusion: 这种新方法提供了一种基于证据的透明解决方案，为更鲁棒、可问责、可扩展、高效、灵活、适应性强和协作的解决方案铺平了道路。

Abstract: The Enterprise Intelligence Platform must integrate logs from numerous
third-party vendors in order to perform various downstream tasks. However,
vendor documentation is often unavailable at test time. It is either misplaced,
mismatched, poorly formatted, or incomplete, which makes schema mapping
challenging. We introduce a reinforcement learning agent that can self-improve
without labeled examples or model weight updates. During inference, the agent:
1) Identifies ambiguous field-mapping attempts. 2) Generates targeted
web-search queries to gather external evidence. 3) Applies a confidence-based
reward to iteratively refine its mappings. To demonstrate this concept, we
converted Microsoft Defender for Endpoint logs into a common schema. Our method
increased mapping accuracy from 56.4\%(LLM-only) to 72.73\%(RAG) to 93.94\%
over 100 iterations using GPT-4o. At the same time, it reduced the number of
low-confidence mappings requiring expert review by 85\%. This new approach
provides an evidence-driven, transparent method for solving future industry
problems, paving the way for more robust, accountable, scalable, efficient,
flexible, adaptable, and collaborative solutions.

</details>


### [73] [Symbol Grounding in Neuro-Symbolic AI: A Gentle Introduction to Reasoning Shortcuts](https://arxiv.org/abs/2510.14538)
*Emanuele Marconato,Samuele Bortolotti,Emile van Krieken,Paolo Morettin,Elena Umili,Antonio Vergari,Efthymia Tsamoura,Andrea Passerini,Stefano Teso*

Main category: cs.AI

TL;DR: 这篇论文概述了神经符号AI中的推理捷径问题，讨论了其成因、后果，并回顾了现有的理论特征和应对方法，旨在降低研究门槛，促进可靠AI的发展。


<details>
  <summary>Details</summary>
Motivation: 神经符号AI模型在概念未被直接监督时容易出现推理捷径问题，这会损害模型解释性、分布外性能及可靠性。现有文献分散，难以系统理解和解决该问题。

Method: 提供推理捷径的直观介绍，分析其成因和后果，回顾现有理论特征，详细阐述包括缓解和意识策略在内的应对方法，并评估其优缺点。

Result: 通过将高级材料重新表述为易于理解的形式，为推理捷径提供了统一视角，降低了解决该问题的入门门槛。

Conclusion: 这篇概述有助于促进可靠神经符号AI和可信AI模型的开发，为研究人员和实践者提供了系统应对推理捷径问题的框架。

Abstract: Neuro-symbolic (NeSy) AI aims to develop deep neural networks whose
predictions comply with prior knowledge encoding, e.g. safety or structural
constraints. As such, it represents one of the most promising avenues for
reliable and trustworthy AI. The core idea behind NeSy AI is to combine neural
and symbolic steps: neural networks are typically responsible for mapping
low-level inputs into high-level symbolic concepts, while symbolic reasoning
infers predictions compatible with the extracted concepts and the prior
knowledge. Despite their promise, it was recently shown that - whenever the
concepts are not supervised directly - NeSy models can be affected by Reasoning
Shortcuts (RSs). That is, they can achieve high label accuracy by grounding the
concepts incorrectly. RSs can compromise the interpretability of the model's
explanations, performance in out-of-distribution scenarios, and therefore
reliability. At the same time, RSs are difficult to detect and prevent unless
concept supervision is available, which is typically not the case. However, the
literature on RSs is scattered, making it difficult for researchers and
practitioners to understand and tackle this challenging problem. This overview
addresses this issue by providing a gentle introduction to RSs, discussing
their causes and consequences in intuitive terms. It also reviews and
elucidates existing theoretical characterizations of this phenomenon. Finally,
it details methods for dealing with RSs, including mitigation and awareness
strategies, and maps their benefits and limitations. By reformulating advanced
material in a digestible form, this overview aims to provide a unifying
perspective on RSs to lower the bar to entry for tackling them. Ultimately, we
hope this overview contributes to the development of reliable NeSy and
trustworthy AI models.

</details>


### [74] [LLM Agents Beyond Utility: An Open-Ended Perspective](https://arxiv.org/abs/2510.14548)
*Asen Nachkov,Xi Wang,Luc Van Gool*

Main category: cs.AI

TL;DR: 该研究探讨了预训练LLM代理能否成为具有自主规划、任务设计和推理能力的实体，通过在开放环境中让代理生成任务、积累知识和与环境交互进行实验。


<details>
  <summary>Details</summary>
Motivation: 随着LLM代理能力增强，研究其能否超越智能问题解决工具，成为能够规划、设计任务并推理模糊目标的自主实体。

Method: 采用开放实验设置，增强预训练LLM代理生成自身任务、积累知识和与环境广泛交互的能力，并进行定性研究。

Result: 代理能可靠执行复杂多步骤指令、跨运行存储和重用信息、提出并解决自身任务，但对提示设计敏感、易重复生成任务、无法形成自我表征。

Conclusion: 研究展示了预训练LLM向开放适应性的前景和当前局限，为训练代理管理记忆、有效探索和追求抽象长期目标指明方向。

Abstract: Recent LLM agents have made great use of chain of thought reasoning and
function calling. As their capabilities grow, an important question arises: can
this software represent not only a smart problem-solving tool, but an entity in
its own right, that can plan, design immediate tasks, and reason toward
broader, more ambiguous goals? To study this question, we adopt an open-ended
experimental setting where we augment a pretrained LLM agent with the ability
to generate its own tasks, accumulate knowledge, and interact extensively with
its environment. We study the resulting open-ended agent qualitatively. It can
reliably follow complex multi-step instructions, store and reuse information
across runs, and propose and solve its own tasks, though it remains sensitive
to prompt design, prone to repetitive task generation, and unable to form
self-representations. These findings illustrate both the promise and current
limits of adapting pretrained LLMs toward open-endedness, and point to future
directions for training agents to manage memory, explore productively, and
pursue abstract long-term goals.

</details>


### [75] [ColorBench: Benchmarking Mobile Agents with Graph-Structured Framework for Complex Long-Horizon Tasks](https://arxiv.org/abs/2510.14621)
*Yuanyi Song,Heyuan Huang,Qiqiang Lin,Yin Zhao,Xiangmou Qu,Jun Wang,Xingyu Lou,Weiwen Liu,Zhuosheng Zhang,Jun Wang,Yong Yu,Weinan Zhang,Zhaoxiang Wang*

Main category: cs.AI

TL;DR: 提出ColorBench图结构基准框架，用于评估移动代理在复杂长任务中的多路径解决能力，弥补离线静态评估与在线动态测试之间的差距。


<details>
  <summary>Details</summary>
Motivation: 现实移动任务复杂且存在多种有效解决方案，但现有评估标准只能验证单一预设路径，在线动态测试又受设备复杂性和不可重现性限制，无法全面评估代理能力。

Method: 通过建模真实设备交互中的有限状态，实现动态行为的静态模拟，开发ColorBench基准，包含175个任务（74单应用、101跨应用），平均长度超13步，每个任务至少包含两条正确路径和典型错误路径。

Result: 通过在各种基线模型上评估ColorBench，发现了现有模型的局限性，并基于实验结果提出了改进方向和可行的技术路径。

Conclusion: ColorBench框架能够有效评估移动代理在复杂长任务中的多路径解决能力，为提升代理性能提供了新的评估标准和技术方向。

Abstract: The rapid advancement of multimodal large language models has enabled agents
to operate mobile devices by directly interacting with graphical user
interfaces, opening new possibilities for mobile automation. However,
real-world mobile tasks are often complex and allow for multiple valid
solutions. This contradicts current mobile agent evaluation standards: offline
static benchmarks can only validate a single predefined "golden path", while
online dynamic testing is constrained by the complexity and non-reproducibility
of real devices, making both approaches inadequate for comprehensively
assessing agent capabilities. To bridge the gap between offline and online
evaluation and enhance testing stability, this paper introduces a novel
graph-structured benchmarking framework. By modeling the finite states observed
during real-device interactions, it achieves static simulation of dynamic
behaviors. Building on this, we develop ColorBench, a benchmark focused on
complex long-horizon tasks. It supports evaluation of multiple valid solutions,
subtask completion rate statistics, and atomic-level capability analysis.
ColorBench contains 175 tasks (74 single-app, 101 cross-app) with an average
length of over 13 steps. Each task includes at least two correct paths and
several typical error paths, enabling quasi-dynamic interaction. By evaluating
ColorBench across various baselines, we discover limitations of existing models
and propose improvement directions and feasible technical pathways to enhance
agents' performance on complex, long-horizon problems based on experimental
results. Code and data are available at:
https://github.com/MadeAgents/ColorBench.

</details>


### [76] [Beyond Hallucinations: The Illusion of Understanding in Large Language Models](https://arxiv.org/abs/2510.14665)
*Rikard Rosenbacke,Carl Rosenbacke,Victor Rosenbacke,Martin McKee*

Main category: cs.AI

TL;DR: 本文提出了Rose-Frame框架，用于诊断人机交互中的认知和认识论漂移，通过三个维度（地图vs领土、直觉vs理性、冲突vs确认）来识别LLM的局限性，强调将AI对齐重新定义为认知治理。


<details>
  <summary>Details</summary>
Motivation: LLM虽然流畅且情感共鸣强，但基于统计预测而非有根据的推理，存在幻觉风险，可能产生听起来令人信服但缺乏事实有效性的回答。

Method: 引入Rose-Frame三维框架：(i)地图vs领土区分现实表征与现实本身；(ii)直觉vs理性基于双过程理论分离快速情感判断与慢速反思思维；(iii)冲突vs确认检验观点是通过批判性测试还是简单相互验证。

Result: Rose-Frame不试图用更多数据或规则修复LLM，而是提供反思工具，使模型局限性和用户假设可见，实现更透明和批判性意识的AI部署。

Conclusion: 将AI对齐重新定义为认知治理：无论是人类还是人工智能的直觉，都必须受到人类理性的治理。只有通过嵌入反思性、可证伪的监督，才能将机器的流畅性与人类的理解对齐。

Abstract: Large language models (LLMs) are becoming deeply embedded in human
communication and decision-making, yet they inherit the ambiguity, bias, and
lack of direct access to truth inherent in language itself. While their outputs
are fluent, emotionally resonant, and coherent, they are generated through
statistical prediction rather than grounded reasoning. This creates the risk of
hallucination, responses that sound convincing but lack factual validity.
Building on Geoffrey Hinton's observation that AI mirrors human intuition
rather than reasoning, this paper argues that LLMs operationalize System 1
cognition at scale: fast, associative, and persuasive, but without reflection
or falsification. To address this, we introduce the Rose-Frame, a
three-dimensional framework for diagnosing cognitive and epistemic drift in
human-AI interaction. The three axes are: (i) Map vs. Territory, which
distinguishes representations of reality (epistemology) from reality itself
(ontology); (ii) Intuition vs. Reason, drawing on dual-process theory to
separate fast, emotional judgments from slow, reflective thinking; and (iii)
Conflict vs. Confirmation, which examines whether ideas are critically tested
through disagreement or simply reinforced through mutual validation. Each
dimension captures a distinct failure mode, and their combination amplifies
misalignment. Rose-Frame does not attempt to fix LLMs with more data or rules.
Instead, it offers a reflective tool that makes both the model's limitations
and the user's assumptions visible, enabling more transparent and critically
aware AI deployment. It reframes alignment as cognitive governance: intuition,
whether human or artificial, must remain governed by human reason. Only by
embedding reflective, falsifiable oversight can we align machine fluency with
human understanding.

</details>


### [77] [Machine Learning and Public Health: Identifying and Mitigating Algorithmic Bias through a Systematic Review](https://arxiv.org/abs/2510.14669)
*Sara Altamirano,Arjan Vreeken,Sennay Ghebreab*

Main category: cs.AI

TL;DR: 系统综述了2021-2025年荷兰公共卫生机器学习研究中算法偏见的识别、讨论和报告情况，开发了RABAT评估工具，发现普遍存在公平性框架缺失等问题，并提出了ACAR四阶段公平性框架。


<details>
  <summary>Details</summary>
Motivation: 机器学习有望通过改进监测、风险分层和资源分配来革新公共卫生，但如果不系统关注算法偏见，可能会无意中加剧现有的健康不平等。

Method: 开发了RABAT评估工具，整合了Cochrane偏倚风险、PROBAST和微软负责任AI检查表的元素，并应用于35篇同行评审研究。

Result: 分析显示普遍存在差距：虽然数据抽样和缺失数据实践记录良好，但大多数研究忽略了明确的公平性框架、亚组分析和对潜在危害的透明讨论。

Conclusion: 提出了ACAR四阶段公平性框架和可操作建议，帮助公共卫生ML从业者持续考虑算法偏见，确保算法创新促进而非损害健康公平。

Abstract: Machine learning (ML) promises to revolutionize public health through
improved surveillance, risk stratification, and resource allocation. However,
without systematic attention to algorithmic bias, ML may inadvertently
reinforce existing health disparities. We present a systematic literature
review of algorithmic bias identification, discussion, and reporting in Dutch
public health ML research from 2021 to 2025. To this end, we developed the Risk
of Algorithmic Bias Assessment Tool (RABAT) by integrating elements from
established frameworks (Cochrane Risk of Bias, PROBAST, Microsoft Responsible
AI checklist) and applied it to 35 peer-reviewed studies. Our analysis reveals
pervasive gaps: although data sampling and missing data practices are well
documented, most studies omit explicit fairness framing, subgroup analyses, and
transparent discussion of potential harms. In response, we introduce a
four-stage fairness-oriented framework called ACAR (Awareness,
Conceptualization, Application, Reporting), with guiding questions derived from
our systematic literature review to help researchers address fairness across
the ML lifecycle. We conclude with actionable recommendations for public health
ML practitioners to consistently consider algorithmic bias and foster
transparency, ensuring that algorithmic innovations advance health equity
rather than undermine it.

</details>


### [78] [NAEL: Non-Anthropocentric Ethical Logic](https://arxiv.org/abs/2510.14676)
*Bianca Maria Lerma,Rafael Peñaloza*

Main category: cs.AI

TL;DR: NAEL是一个基于主动推理和符号推理的新型人工智能伦理框架，通过最小化全局期望自由能量来形式化伦理行为，采用神经符号架构让智能体在不确定环境中评估行为伦理后果。


<details>
  <summary>Details</summary>
Motivation: 传统AI伦理方法过于以人类为中心，存在局限性。NAEL旨在开发不预设人类道德直觉、能够适应情境的伦理行为模型。

Method: 提出神经符号架构，结合主动推理和符号推理，使智能体能够在动态多智能体环境中最小化全局期望自由能量。

Result: 通过伦理资源分配的案例研究，展示了NAEL能够动态平衡自我保存、认知学习和集体福利。

Conclusion: NAEL为人工智能系统提供了非人类中心主义的伦理框架，能够产生情境敏感、自适应和关系性的伦理行为。

Abstract: We introduce NAEL (Non-Anthropocentric Ethical Logic), a novel ethical
framework for artificial agents grounded in active inference and symbolic
reasoning. Departing from conventional, human-centred approaches to AI ethics,
NAEL formalizes ethical behaviour as an emergent property of intelligent
systems minimizing global expected free energy in dynamic, multi-agent
environments. We propose a neuro-symbolic architecture to allow agents to
evaluate the ethical consequences of their actions in uncertain settings. The
proposed system addresses the limitations of existing ethical models by
allowing agents to develop context-sensitive, adaptive, and relational ethical
behaviour without presupposing anthropomorphic moral intuitions. A case study
involving ethical resource distribution illustrates NAEL's dynamic balancing of
self-preservation, epistemic learning, and collective welfare.

</details>


### [79] [Practical, Utilitarian Algorithm Configuration](https://arxiv.org/abs/2510.14683)
*Devon Graham,Kevin Leyton-Brown*

Main category: cs.AI

TL;DR: 本文改进了COUP算法配置方法，在保持理论保证的同时显著提升了实用性能，使其能与无性能保证的启发式配置方法竞争。


<details>
  <summary>Details</summary>
Motivation: COUP算法配置方法虽然具有理论保证，但实际性能表现不足。本文旨在填补这一差距，使基于效用的算法配置方法在实际应用中具有竞争力。

Method: 提出了一系列对COUP的改进措施，这些改进在不降低理论保证的前提下提升了算法的经验性能，并通过实验验证了这些改进的效果。

Result: 改进后的COUP算法在实证性能上显著提升，能够与广泛使用的启发式配置方法竞争，同时保持理论性能保证。

Conclusion: 通过系统改进，成功将基于效用的算法配置方法提升到实用水平，为算法选择问题提供了具有理论保证且性能优越的解决方案。

Abstract: Utilitarian algorithm configuration identifies a parameter setting for a
given algorithm that maximizes a user's utility. Utility functions offer a
theoretically well-grounded approach to optimizing decision-making under
uncertainty and are flexible enough to capture a user's preferences over
algorithm runtimes (e.g., they can describe a sharp cutoff after which a
solution is no longer required, a per-hour cost for compute, or diminishing
returns from algorithms that take longer to run). COUP is a recently-introduced
utilitarian algorithm configuration procedure which was designed mainly to
offer strong theoretical guarantees about the quality of the configuration it
returns, with less attention paid to its practical performance. This paper
closes that gap, bringing theoretically-grounded, utilitarian algorithm
configuration to the point where it is competitive with widely used, heuristic
configuration procedures that offer no performance guarantees. We present a
series of improvements to COUP that improve its empirical performance without
degrading its theoretical guarantees and demonstrate their benefit
experimentally. Using a case study, we also illustrate ways of exploring the
robustness of a given solution to the algorithm selection problem to variations
in the utility function.

</details>


### [80] [Purifying Task Vectors in Knowledge-Aware Subspace for Model Merging](https://arxiv.org/abs/2510.14697)
*Bang An,Yibo Yang,Philip Torr,Bernard Ghanem*

Main category: cs.AI

TL;DR: PAVE提出了一种在知识感知子空间中净化任务向量的方法，通过上下文导向的奇异值分解识别并修剪任务向量中的冗余组件，从而提升模型合并性能。


<details>
  <summary>Details</summary>
Motivation: 现有的模型合并方法中，任务向量包含任务无关的冗余信息，导致合并模型性能显著下降。现有方法通过随机丢弃参数元素来克服冗余，但缺乏知识感知且涉及随机性。

Method: 从每个任务中采样训练样本，通过微调模型获取线性层前的协方差矩阵，执行上下文导向的奇异值分解来突出与目标知识最相关的权重组件，将微调模型权重在知识感知子空间中分为任务相关和冗余组件，并修剪冗余组件来净化任务向量。

Result: 实验证明PAVE在各种合并方法、任务和模型架构中均有效提升性能。

Conclusion: PAVE作为一种即插即用方案，可应用于各种基于任务向量的合并方法，通过知识感知的冗余修剪显著改善模型合并效果。

Abstract: Model merging aims to integrate task-specific abilities from individually
fine-tuned models into a single model without extra training. In recent model
merging methods, task vector has become a fundamental building block, as it can
encapsulate the residual information from finetuning. However, the merged model
often suffers from notable performance degradation due to the conflicts caused
by task-irrelevant redundancy in task vectors. Existing efforts in overcoming
redundancy by randomly dropping elements in the parameter space involves
randomness and lacks knowledge awareness. To address these challenges, in this
study, we propose Purifying TAsk Vectors (PAVE) in knowledge-aware subspace.
Concretely, we sample some training examples from each task, and feed them into
their corresponding fine-tuned models to acquire the covariance matrices before
linear layers. We then perform a context-oriented singular value decomposition,
which accentuates the weight components most relevant to the target knowledge.
As a result, we can split fine-tuned model weights into task-relevant and
redundant components in the knowledge-aware subspace, and purify the task
vector by pruning the redundant components. To induce fair pruning efforts
across models, we further introduce a spectral rank allocation strategy by
optimizing a normalized activated pruning error. The task vector purification
by our method as a plug-and-play scheme is applicable across various task
vector-based merging methods to improve their performance. In experiments, we
demonstrate the effectiveness of PAVE across a diverse set of merging methods,
tasks, and model architectures.

</details>


### [81] [Cognitive-Aligned Spatio-Temporal Large Language Models For Next Point-of-Interest Prediction](https://arxiv.org/abs/2510.14702)
*Penglong Zhai,Jie Li,Fanyi Di,Yue Liu,Yifang Yuan,Jie Huang,Peng Wu,Sicong Wang,Mingyang Yin,Tingting Hu,Yao Xu,Xin Li*

Main category: cs.AI

TL;DR: CoAST是一个认知对齐的时空大语言模型框架，通过自然语言界面整合世界知识、时空轨迹模式和个人信息，用于下一兴趣点推荐。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型主要基于非结构化文本预训练，缺乏对结构化地理实体和序列移动模式的理解，且难以整合季节、天气、节假日等世界知识和用户认知信息。

Method: CoAST包含两个阶段：(1) 在脱敏用户的时空轨迹数据上进行持续预训练获取推荐知识；(2) 通过监督微调和强化学习实现认知对齐，使模型判断与人类偏好一致。

Result: 在多个真实世界数据集上的离线实验和在AMAP应用首页的在线实验证明了CoAST的有效性。

Conclusion: CoAST框架成功解决了大语言模型在下一兴趣点推荐任务中的局限性，通过认知对齐提升了推荐性能。

Abstract: The next point-of-interest (POI) recommendation task aims to predict the
users' immediate next destinations based on their preferences and historical
check-ins, holding significant value in location-based services. Recently,
large language models (LLMs) have shown great potential in recommender systems,
which treat the next POI prediction in a generative manner. However, these
LLMs, pretrained primarily on vast corpora of unstructured text, lack the
native understanding of structured geographical entities and sequential
mobility patterns required for next POI prediction tasks. Moreover, in
industrial-scale POI prediction applications, incorporating world knowledge and
alignment of human cognition, such as seasons, weather conditions, holidays,
and users' profiles (such as habits, occupation, and preferences), can enhance
the user experience while improving recommendation performance. To address
these issues, we propose CoAST (Cognitive-Aligned Spatial-Temporal LLMs), a
framework employing natural language as an interface, allowing for the
incorporation of world knowledge, spatio-temporal trajectory patterns,
profiles, and situational information. Specifically, CoAST mainly comprises of
2 stages: (1) Recommendation Knowledge Acquisition through continued
pretraining on the enriched spatial-temporal trajectory data of the
desensitized users; (2) Cognitive Alignment to align cognitive judgments with
human preferences using enriched training data through Supervised Fine-Tuning
(SFT) and a subsequent Reinforcement Learning (RL) phase. Extensive offline
experiments on various real-world datasets and online experiments deployed in
"Guess Where You Go" of AMAP App homepage demonstrate the effectiveness of
CoAST.

</details>


### [82] [ToolPRM: Fine-Grained Inference Scaling of Structured Outputs for Function Calling](https://arxiv.org/abs/2510.14703)
*Jianghao Lin,Yuanyuan Shi,Xin Peng,Renjie Ding,Hairui Wang,Yuxuan Peng,Bizhe Bai,Weixi Song,Fengshuo Bai,Huacan Chai,Weinan Zhang,Fei Huang,Ying Wen*

Main category: cs.AI

TL;DR: 提出了一个结合细粒度波束搜索和过程奖励模型ToolPRM的推理扩展框架，用于提升LLM在结构化输出（如函数调用）任务中的性能。


<details>
  <summary>Details</summary>
Motivation: 当前推理扩展研究主要关注非结构化输出生成任务，而在结构化输出（如函数调用）方面的应用研究不足，需要填补这一空白。

Method: 构建首个细粒度调用内过程监督数据集，使用函数掩码技术自动标注提供步骤级奖励；结合细粒度波束搜索与过程奖励模型ToolPRM来监督函数调用推理过程。

Result: ToolPRM在预测准确性上优于粗粒度和结果奖励模型；配备ToolPRM的推理扩展技术显著提升了主干模型在各种函数调用任务和基准测试中的性能。

Conclusion: 揭示了将推理扩展技术应用于结构化输出的关键原则："多探索但少保留"，这是由结构化函数调用生成的不可恢复特性决定的。

Abstract: Large language models (LLMs) are increasingly demonstrating strong
capabilities as autonomous agents, with function calling serving as a core
mechanism for interaction with the environment. Meanwhile, inference scaling
has become a cutting-edge technique to enhance LLM performance by allocating
more computational resources during the inference process. However, current
research on inference scaling primarily focuses on unstructured output
generation tasks, leaving its application in structured outputs, like function
calling, largely underexplored. To bridge this gap, we propose an inference
scaling framework that combines fine-grained beam search with a process reward
model, ToolPRM, which scores the internal steps of each single function call.
To train ToolPRM, we construct the first fine-grained intra-call process
supervision dataset, automatically annotated with function-masking techniques
to provide step-level rewards for structured tool-use reasoning. Extensive
experiments demonstrate that ToolPRM beats the coarse-grained and outcome
reward models in terms of predictive accuracy, indicating its stronger
capability in supervising the function calling inference process. Inference
scaling technique equipped with ToolPRM also significantly improves the
backbone model performance across various function calling tasks and
benchmarks. More importantly, we reveal a key principle for applying inference
scaling techniques to structured outputs: "explore more but retain less" due to
the unrecoverability characteristics of structured function calling generation.

</details>


### [83] [SimKO: Simple Pass@K Policy Optimization](https://arxiv.org/abs/2510.14807)
*Ruotian Peng,Yi Ren,Zhouliang Yu,Weiyang Liu,Yandong Wen*

Main category: cs.AI

TL;DR: 提出SimKO方法解决RLVR中的过度集中问题，通过不对称地处理正确和错误响应来鼓励探索，提高pass@K性能


<details>
  <summary>Details</summary>
Motivation: 现有RLVR方法存在系统性偏向利用而非探索的问题，表现为pass@1提升但pass@K下降，需要解决概率过度集中现象

Method: SimKO方法：对已验证正确响应提升top-K候选概率，对已验证错误响应对top-1候选施加更强惩罚，特别在高熵token上应用

Result: 在各种数学和逻辑推理基准测试中，SimKO一致地提高了广泛的K值下的pass@K性能

Conclusion: SimKO提供了一种简单有效的方法来改善RLVR的探索能力，缓解概率过度集中问题

Abstract: Reinforcement learning with verifiable rewards (RLVR) has advanced the
reasoning capabilities of large language models (LLMs). However, prevailing
RLVR methods exhibit a systematic bias toward exploitation over exploration, as
evidenced by improved pass@1 but reduced pass@K (K>1) performance. To
understand this issue, we analyze training dynamics of RLVR methods by tracking
the token-level probability distributions over vocabulary candidates. Our
analysis reveals a consistent probability concentration effect where the top-1
candidate increasingly accumulates probability mass and suppresses that of
other candidates. More importantly, stronger over-concentration correlates with
worse pass@K performance. Inspired by this finding, we propose Simple Pass@K
Optimization (SimKO), a method designed to mitigate the over-concentration
issue, thereby encouraging exploration. SimKO operates in an asymmetrical
manner. For verified-correct responses, it boosts the probabilities of the
top-K candidates. For verified-incorrect responses, it applies stronger
penalties to the top-1 candidate. We observe that this asymmetric design is
particularly effective at mitigating over-concentration when applied at tokens
with high entropy. Across various math and logical-reasoning benchmarks, SimKO
consistently yields higher pass@K for a wide range of K, providing a simple way
to improve RLVR's exploration.

</details>


### [84] [Agentic NL2SQL to Reduce Computational Costs](https://arxiv.org/abs/2510.14808)
*Dominik Jehle,Lennart Purucker,Frank Hutter*

Main category: cs.AI

TL;DR: Datalake Agent是一种代理系统，通过交互式循环减少LLM处理NL2SQL任务所需的元信息量，显著降低token使用和成本。


<details>
  <summary>Details</summary>
Motivation: 传统NL2SQL方法需要处理大量数据库元信息，导致提示过长、token数量多和处理成本高。

Method: 采用代理系统设计，使用交互式循环和推理框架，选择性请求必要信息来解决表格问答任务。

Result: 在23个数据库和100个表格问答任务上评估，token使用减少高达87%，同时保持竞争力性能。

Conclusion: Datalake Agent能显著降低LLM处理NL2SQL任务的成本，同时维持良好的性能表现。

Abstract: Translating natural language queries into SQL queries (NL2SQL or Text-to-SQL)
has recently been empowered by large language models (LLMs). Using LLMs to
perform NL2SQL methods on a large collection of SQL databases necessitates
processing large quantities of meta-information about the databases, which in
turn results in lengthy prompts with many tokens and high processing costs. To
address this challenge, we introduce Datalake Agent, an agentic system designed
to enable an LLM to solve NL2SQL tasks more efficiently. Instead of utilizing
direct solvers for NL2SQL that call the LLM once with all meta-information in
the prompt, the Datalake Agent employs an interactive loop to reduce the
utilized meta-information. Within the loop, the LLM is used in a reasoning
framework that selectively requests only the necessary information to solve a
table question answering task. We evaluate the Datalake Agent on a collection
of 23 databases with 100 table question answering tasks. The Datalake Agent
reduces the tokens used by the LLM by up to 87\% and thus allows for
substantial cost reductions while maintaining competitive performance.

</details>


### [85] [RoboGPT-R1: Enhancing Robot Planning with Reinforcement Learning](https://arxiv.org/abs/2510.14828)
*Jinrui Liu,Bingyan Nie,Boyu Li,Yaran Chen,Yuze Wang,Shunsen He,Haoran Li*

Main category: cs.AI

TL;DR: 提出了RoboGPT-R1，一个两阶段微调框架，通过监督训练获取基础知识，再通过强化学习提升视觉空间理解和推理能力，在EmbodiedBench基准上显著优于GPT-4o-mini和其他模型。


<details>
  <summary>Details</summary>
Motivation: 解决具身智能体在复杂真实环境中执行长视野操作任务时，由于常识和推理能力受限而面临的挑战。

Method: 采用两阶段微调框架：监督训练从专家序列获取基础知识，强化学习解决模型在视觉空间理解和推理方面的不足，并设计了考虑长视野性能和动作约束的基于规则的奖励函数。

Result: 在Qwen2.5-VL-3B上训练的推理模型在EmbodiedBench基准上比GPT-4o-mini高出21.33%，比在Qwen2.5-VL-7B上训练的其他工作高出20.33%。

Conclusion: RoboGPT-R1框架通过结合监督训练和强化学习，有效提升了具身智能体的物理理解和推理能力，在长视野操作任务中表现出色。

Abstract: Improving the reasoning capabilities of embodied agents is crucial for robots
to complete complex human instructions in long-view manipulation tasks
successfully. Despite the success of large language models and vision language
models based on Supervised Fine-Tuning (SFT) in planning tasks, they continue
facing challenges in performing long-horizon manipulation tasks in complex
real-world environments, owing to their restricted common sense and reasoning
capabilities. Considering that aligning general-purpose vision language models
to robotic planning tasks via supervised fine-tuning suffers from poor
generalization and insufficient physical understanding, we propose RoboGPT-R1,
a two-stage fine-tuning framework for embodied planning. In this framework,
supervised training acquires foundational knowledge through expert sequences,
followed by RL to address the model's shortcomings in visual-spatial
understanding and reasoning. To achieve physical understanding and action
sequence consistency in multi-step reasoning tasks, we design a rule-based
reward function that simultaneously considers long-horizon performance and
action constraint in the environment. The reasoning model, trained on
Qwen2.5-VL-3B, significantly outperforms the larger-scale model, GPT-4o-mini,
by 21.33% and surpasses other work trained on Qwen2.5-VL-7B by 20.33% on the
EmbodiedBench benchmark.

</details>


### [86] [Boosting Instruction Following at Scale](https://arxiv.org/abs/2510.14842)
*Ben Elder,Evelyn Duesterwald,Vinod Muthusamy*

Main category: cs.AI

TL;DR: Instruction Boosting是一种后生成方法，通过增加指令遵循率来提高LLM提示指令的可靠性，在2个指令时提升7个百分点，10个指令时提升4个百分点。


<details>
  <summary>Details</summary>
Motivation: 开发者通常通过精心操作提示来影响LLM行为，但仅添加更多指令并不能保证它们会被遵循，需要提高指令遵循的可靠性。

Method: 引入Instruction Boosting作为后生成方法，并创建SCALEDIF基准测试，包含最多10个指令的数据样本，同时开发定量冲突评分工具来分析指令间的冲突。

Result: Instruction Boosting显著提高了指令遵循率，同时发现性能随指令数量增加而下降的趋势主要源于指令间的紧张和冲突。

Conclusion: Instruction Boosting能有效提高LLM对提示指令的遵循可靠性，冲突评分工具可为开发者提供关于额外提示指令对模型性能影响的反馈。

Abstract: A typical approach developers follow to influence an LLM's behavior in an
application is through careful manipulation of the prompt, such as by adding or
modifying instructions. However, merely adding more instructions provides
little assurance that they will actually be followed. We introduce Instruction
Boosting as a post-generation method to increase the reliability of LLM prompt
instructions. We show that Instruction Boosting improves the instruction
following rate by up to 7 points for two instructions and up to 4 points for
ten instructions. To demonstrate these results we introduce SCALEDIF, a
benchmark with a scaled instruction volume of up to ten instructions per data
sample. We also present an analysis of the commonly observed trend that
performance degrades as more instructions are added. We show that an important
factor contributing to this trend is the degree of tension and conflict that
arises as the number of instructions is increased. We contribute a quantitative
conflict scoring tool that explains the observed performance trends and
provides feedback to developers on the impact that additional prompt
instructions have on a model's performance.

</details>


### [87] [Where to Search: Measure the Prior-Structured Search Space of LLM Agents](https://arxiv.org/abs/2510.14846)
*Zhuo-Yang Song*

Main category: cs.AI

TL;DR: 提出了一种紧凑的形式理论来描述和衡量基于领域先验指导的LLM辅助迭代搜索，通过模糊关系算子表示智能体，引入覆盖生成函数来量化可达性难度。


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的生成-过滤-精炼迭代范式在推理、编程和科学发现方面取得进展，但搜索效果取决于如何将领域先验编码为结构化假设空间。

Method: 将智能体表示为输入输出的模糊关系算子，通过安全包络约束；为多步推理加权所有可达路径，计算覆盖生成函数来度量可达性难度。

Result: 提供了可测试的推断并通过多数投票实例进行验证，为衡量智能体及其搜索空间提供了可行语言和操作工具。

Conclusion: 该理论为LLM构建的迭代搜索提供了系统化的形式描述框架，能够有效衡量搜索过程和智能体性能。

Abstract: The generate-filter-refine (iterative paradigm) based on large language
models (LLMs) has achieved progress in reasoning, programming, and program
discovery in AI+Science. However, the effectiveness of search depends on where
to search, namely, how to encode the domain prior into an operationally
structured hypothesis space. To this end, this paper proposes a compact formal
theory that describes and measures LLM-assisted iterative search guided by
domain priors. We represent an agent as a fuzzy relation operator on inputs and
outputs to capture feasible transitions; the agent is thereby constrained by a
fixed safety envelope. To describe multi-step reasoning/search, we weight all
reachable paths by a single continuation parameter and sum them to obtain a
coverage generating function; this induces a measure of reachability
difficulty; and it provides a geometric interpretation of search on the graph
induced by the safety envelope. We further provide the simplest testable
inferences and validate them via a majority-vote instantiation. This theory
offers a workable language and operational tools to measure agents and their
search spaces, proposing a systematic formal description of iterative search
constructed by LLMs.

</details>


### [88] [LabOS: The AI-XR Co-Scientist That Sees and Works With Humans](https://arxiv.org/abs/2510.14861)
*Le Cong,Zaixi Zhang,Xiaotong Wang,Yin Di,Ruofan Jin,Michal Gerasimiuk,Yinkai Wang,Ravi K. Dinesh,David Smerkous,Alex Smerkous,Xuekun Wu,Shilong Liu,Peishan Li,Yi Zhu,Simran Serrao,Ning Zhao,Imran A. Mohammad,John B. Sunwoo,Joseph C. Wu,Mengdi Wang*

Main category: cs.AI

TL;DR: LabOS是首个将计算推理与物理实验相结合的AI共科学家系统，通过多模态感知、自进化代理和XR技术实现人机协作，将实验室转变为智能协作环境。


<details>
  <summary>Details</summary>
Motivation: 现代科学发展需要思想与行动的结合，传统AI主要停留在计算设计层面，无法直接参与物理实验。LabOS旨在让AI能够真正参与实验室工作，理解实验背景并实时协助执行。

Method: 通过连接多模型AI代理、智能眼镜和人类-AI协作，LabOS使AI能够看到科学家所见，理解实验背景，并在实时执行中提供协助。系统采用多模态感知、自进化代理和扩展现实(XR)技术。

Result: 在从癌症免疫治疗靶点发现到干细胞工程等多个应用领域，LabOS展示了AI能够超越计算设计直接参与实验，将实验室转变为智能协作环境。

Conclusion: LabOS证明AI可以从单纯的计算设计转向实际参与，在实验室中实现人类与机器发现的共同进化，开创了人机协作科学研究的新范式。

Abstract: Modern science advances fastest when thought meets action. LabOS represents
the first AI co-scientist that unites computational reasoning with physical
experimentation through multimodal perception, self-evolving agents, and
Entended-Reality(XR)-enabled human-AI collaboration. By connecting multi-model
AI agents, smart glasses, and human-AI collaboration, LabOS allows AI to see
what scientists see, understand experimental context, and assist in real-time
execution. Across applications--from cancer immunotherapy target discovery to
stem-cell engineering -- LabOS shows that AI can move beyond computational
design to participation, turning the laboratory into an intelligent,
collaborative environment where human and machine discovery evolve together.

</details>


### [89] [The Gatekeeper Knows Enough](https://arxiv.org/abs/2510.14881)
*Fikresilase Wondmeneh Abebayew*

Main category: cs.AI

TL;DR: 提出了Gatekeeper协议，一种领域无关的框架，通过让智能体先在低保真度的潜在状态表示上操作，按需请求高保真度上下文，来解决LLM智能体的上下文窗口限制和状态不同步问题。


<details>
  <summary>Details</summary>
Motivation: LLM智能体的实际效用受到有限上下文窗口和状态不同步的制约，导致不可靠输出、不可预测行为和低效资源使用，特别是在与大型结构化知识系统交互时。

Method: 引入Gatekeeper协议，智能体先在低保真度的潜在状态表示上操作和推理，然后按需请求高保真度上下文。所有交互通过统一的JSON格式进行，作为声明式、状态同步的协议。

Result: 该方法显著提高了智能体可靠性，通过最小化token消耗改善了计算效率，并实现了与复杂系统的可扩展交互。

Conclusion: Gatekeeper协议为在任何结构化知识领域构建更稳健、可预测和接地的AI智能体创建了基础方法论。

Abstract: Large Language Models (LLMs) are increasingly deployed as autonomous agents,
yet their practical utility is fundamentally constrained by a limited context
window and state desynchronization resulting from the LLMs' stateless nature
and inefficient context management. These limitations lead to unreliable
output, unpredictable behavior, and inefficient resource usage, particularly
when interacting with large, structured, and sensitive knowledge systems such
as codebases and documents. To address these challenges, we introduce the
Gatekeeper Protocol, a novel, domain-agnostic framework that governs
agent-system interactions. Our protocol mandates that the agent first operate
and reason on a minimalist, low-fidelity "latent state" representation of the
system to strategically request high-fidelity context on demand. All
interactions are mediated through a unified JSON format that serves as a
declarative, state-synchronized protocol, ensuring the agent's model of the
system remains verifiably grounded in the system's reality. We demonstrate the
efficacy of this protocol with Sage, a reference implementation of the
Gatekeeper Protocol for software development. Our results show that this
approach significantly increases agent reliability, improves computational
efficiency by minimizing token consumption, and enables scalable interaction
with complex systems, creating a foundational methodology for building more
robust, predictable, and grounded AI agents for any structured knowledge
domain.

</details>


### [90] [Budget-aware Test-time Scaling via Discriminative Verification](https://arxiv.org/abs/2510.14913)
*Kyle Montgomery,Sijun Tan,Yuqi Chen,Siyuan Zhuang,Tianjun Zhang,Raluca Ada Popa,Chenguang Wang*

Main category: cs.AI

TL;DR: 提出了一种结合判别式验证器和自一致性的混合方法，在固定计算预算下显著优于生成式验证方法，在AIME2025上准确率提升达15.3%。


<details>
  <summary>Details</summary>
Motivation: 现有生成式验证器虽然性能优秀但计算成本过高，限制了实际应用，需要寻找更高效的验证方法。

Method: 采用判别式验证器与自一致性结合的混合方法，在固定计算预算下进行测试时扩展。

Result: 在AIME2025任务上，该方法比最先进的生成式验证准确率高出15.3%，且计算效率更高。

Conclusion: 对于实际应用，基于判别式验证器的预算感知扩展不仅是自一致性的'免费'升级，也是比昂贵生成技术更有效和高效的替代方案。

Abstract: Test-time scaling is a powerful strategy for boosting the performance of
large language models on complex reasoning tasks. While state-of-the-art
approaches often employ generative verifiers to select the best solution from a
pool of candidates, this method incurs prohibitive computational costs,
limiting its practicality. In this work, we shift the focus to a more
budget-aware paradigm: discriminative verification. We conduct a thorough
empirical analysis and demonstrate that while discriminative verifiers may
underperform in isolation, combining them with self-consistency in a hybrid
approach creates a powerful and efficient test-time scaling mechanism. Notably,
under a fixed compute budget, this hybrid approach surpasses state-of-the-art
generative verification by a significant margin: achieving up to 15.3\% higher
accuracy on AIME2025. Our findings establish that for practical, real-world
applications, budget-aware scaling with discriminative verifiers is not only a
"free" upgrade over self-consistency, but also a more effective and efficient
alternative to costly generative techniques. Code is available at
https://github.com/wang-research-lab/verification.

</details>


### [91] [TRI-DEP: A Trimodal Comparative Study for Depression Detection Using Speech, Text, and EEG](https://arxiv.org/abs/2510.14922)
*Annisaa Fitri Nurfidausi,Eleonora Mancini,Paolo Torroni*

Main category: cs.AI

TL;DR: 该论文系统研究了多模态抑郁症检测，结合EEG、语音和文本信号，通过对比手工特征与预训练嵌入、不同神经网络编码器以及单模态、双模态和三模态配置，发现三模态组合能显著提升检测性能。


<details>
  <summary>Details</summary>
Motivation: 现有抑郁症自动检测研究存在范围有限、缺乏系统特征比较和评估协议不一致的问题，特别是多模态方法中EEG、语音和文本的组合潜力尚未充分探索。

Method: 系统探索EEG、语音和文本的特征表示和建模策略，评估手工特征与预训练嵌入的优劣，比较不同神经网络编码器，分析单模态、双模态和三模态配置，并采用一致的受试者独立分割进行稳健评估。

Result: 结果显示：(i) EEG、语音和文本三模态组合能增强多模态检测性能；(ii) 预训练嵌入优于手工特征；(iii) 精心设计的三模态模型达到最先进性能。

Conclusion: 该研究为多模态抑郁症检测的未来研究奠定了基础，证明了多模态融合特别是三模态方法的有效性。

Abstract: Depression is a widespread mental health disorder, yet its automatic
detection remains challenging. Prior work has explored unimodal and multimodal
approaches, with multimodal systems showing promise by leveraging complementary
signals. However, existing studies are limited in scope, lack systematic
comparisons of features, and suffer from inconsistent evaluation protocols. We
address these gaps by systematically exploring feature representations and
modelling strategies across EEG, together with speech and text. We evaluate
handcrafted features versus pre-trained embeddings, assess the effectiveness of
different neural encoders, compare unimodal, bimodal, and trimodal
configurations, and analyse fusion strategies with attention to the role of
EEG. Consistent subject-independent splits are applied to ensure robust,
reproducible benchmarking. Our results show that (i) the combination of EEG,
speech and text modalities enhances multimodal detection, (ii) pretrained
embeddings outperform handcrafted features, and (iii) carefully designed
trimodal models achieve state-of-the-art performance. Our work lays the
groundwork for future research in multimodal depression detection.

</details>


### [92] [Stable but Miscalibrated: A Kantian View on Overconfidence from Filters to Large Language Models](https://arxiv.org/abs/2510.14925)
*Akira Okutomi*

Main category: cs.AI

TL;DR: 将康德的《纯粹理性批判》重新解释为反馈稳定性理论，提出复合不稳定性指数H-Risk来衡量推理系统的稳定性，发现在线性高斯模拟和大型语言模型中，更高的H-Risk预测过度自信错误和幻觉。


<details>
  <summary>Details</summary>
Motivation: 将康德的理性自我限制理论与反馈控制理论联系起来，为诊断和减少推理系统中的过度自信提供原则性框架。

Method: 提出复合不稳定性指数H-Risk，结合谱边界、条件数、时间敏感性和创新放大四个维度；在线性高斯模拟和大型语言模型中进行实验验证。

Result: 在线性高斯模拟中，更高的H-Risk预测过度自信错误；在LLMs中，脆弱的内部动态与校准不良和幻觉相关；批判式提示对校准和幻觉的影响不一致。

Conclusion: 康德的自我限制理论与反馈控制之间存在结构性桥梁，为诊断和选择性减少推理系统中的过度自信提供了原则性视角。

Abstract: We reinterpret Kant's Critique of Pure Reason as a theory of feedback
stability, viewing reason as a regulator that keeps inference within the bounds
of possible experience. We formalize this intuition via a composite instability
index (H-Risk) combining spectral margin, conditioning, temporal sensitivity,
and innovation amplification. In linear-Gaussian simulations, higher H-Risk
predicts overconfident errors even under formal stability, revealing a gap
between nominal and epistemic stability. Extending to large language models
(LLMs), we find that fragile internal dynamics correlate with miscalibration
and hallucination, while critique-style prompts show mixed effects on
calibration and hallucination. These results suggest a structural bridge
between Kantian self-limitation and feedback control, offering a principled
lens for diagnosing -- and selectively reducing -- overconfidence in reasoning
systems. This is a preliminary version; supplementary experiments and broader
replication will be reported in a future revision.

</details>


### [93] [GroundedPRM: Tree-Guided and Fidelity-Aware Process Reward Modeling for Step-Level Reasoning](https://arxiv.org/abs/2510.14942)
*Yao Zhang,Yu Wu,Haowei Zhang,Weiguo Li,Haokun Chen,Jingpei Wu,Guohao Li,Zhen Han,Volker Tresp*

Main category: cs.AI

TL;DR: GroundedPRM是一个基于树搜索和外部工具验证的自动过程监督框架，通过蒙特卡洛树搜索构建结构化推理路径，使用外部工具验证中间步骤，结合步骤级验证和全局结果评估，在仅使用10%数据的情况下实现了26%的性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有过程奖励模型面临三大挑战：噪声奖励、低事实保真度以及与步骤级推理目标的不对齐。这些问题的根源在于缺乏可扩展的高质量标注，现有方法依赖昂贵的人工标注、易产生幻觉的LLM自评估或存在信用分配问题的蒙特卡洛估计。

Method: 1. 使用蒙特卡洛树搜索构建结构化推理路径，实现细粒度信用分配；2. 通过外部工具验证每个中间步骤，提供执行基础的正确性信号；3. 设计混合奖励聚合机制，融合工具验证和MCTS反馈；4. 将奖励信号格式化为理由增强的生成结构，提高可解释性。

Result: 仅使用4万个自动标注样本（最佳PRM自动监督训练数据量的10%），在ProcessBench上实现了高达26%的相对平均性能提升。在奖励引导的贪婪搜索中，甚至优于使用人工标注监督训练的PRM。

Conclusion: GroundedPRM提供了一个可扩展且可验证的高质量过程级推理路径，通过树引导和保真度感知的自动过程监督，有效解决了现有PRM的局限性。

Abstract: Process Reward Models (PRMs) aim to improve multi-step reasoning in Large
Language Models (LLMs) by supervising intermediate steps and identifying
errors. However, building effective PRMs remains challenging due to the lack of
scalable, high-quality annotations. Existing approaches rely on costly human
labeling, LLM-based self-evaluation that is prone to hallucination, or Monte
Carlo (MC) estimation, which infers step quality solely from rollout outcomes
and often introduces noisy, misaligned supervision due to credit
misattribution. These issues result in three core limitations: noisy rewards,
low factual fidelity, and misalignment with step-level reasoning objectives. To
address these challenges, we introduce GroundedPRM, a tree-guided and
fidelity-aware framework for automatic process supervision. To reduce reward
noise and enable fine-grained credit assignment, we construct structured
reasoning paths via Monte Carlo Tree Search (MCTS). To eliminate hallucinated
supervision, we validate each intermediate step using an external tool,
providing execution-grounded correctness signals. To combine both step-level
validation and global outcome assessment, we design a hybrid reward aggregation
mechanism that fuses tool-based verification with MCTS-derived feedback.
Finally, we format the reward signal into a rationale-enhanced, generative
structure to promote interpretability and compatibility with instruction-tuned
LLMs. GroundedPRM is trained on only 40K automatically labeled samples,
amounting to just 10% of the data used by the best-performing PRM trained with
auto-labeled supervision. Nevertheless, it achieves up to a 26% relative
improvement in average performance on ProcessBench. When used for reward-guided
greedy search, GroundedPRM outperforms even PRMs trained with human-labeled
supervision, offering a scalable and verifiable path toward high-quality
process-level reasoning.

</details>


### [94] [Agentic Design of Compositional Machines](https://arxiv.org/abs/2510.14980)
*Wenqian Zhang,Weiyang Liu,Zhen Liu*

Main category: cs.AI

TL;DR: 该论文研究大型语言模型能否进行复杂机器设计，通过BesiegeField测试平台评估LLMs在组合式机器设计任务中的表现，并探索强化学习作为改进途径。


<details>
  <summary>Details</summary>
Motivation: 探索大型语言模型是否能够学习创造性任务，特别是复杂机器的组合设计，这是人类智能的重要标志和工程实践的基础。

Method: 引入BesiegeField测试平台，基于Besiege游戏构建，支持部件组装、物理模拟和奖励驱动评估。对最先进的LLMs进行基准测试，并探索强化学习微调作为改进方法。

Result: 当前开源模型在空间推理、策略组装和指令遵循等关键能力上表现不足，但强化学习微调显示出改进潜力。

Conclusion: 在语言、机器设计和物理推理的交叉领域仍存在开放挑战，强化学习是提升LLMs机器设计能力的有效途径。

Abstract: The design of complex machines stands as both a marker of human intelligence
and a foundation of engineering practice. Given recent advances in large
language models (LLMs), we ask whether they, too, can learn to create. We
approach this question through the lens of compositional machine design: a task
in which machines are assembled from standardized components to meet functional
demands like locomotion or manipulation in a simulated physical environment. To
support this investigation, we introduce BesiegeField, a testbed built on the
machine-building game Besiege, which enables part-based construction, physical
simulation and reward-driven evaluation. Using BesiegeField, we benchmark
state-of-the-art LLMs with agentic workflows and identify key capabilities
required for success, including spatial reasoning, strategic assembly, and
instruction-following. As current open-source models fall short, we explore
reinforcement learning (RL) as a path to improvement: we curate a cold-start
dataset, conduct RL finetuning experiments, and highlight open challenges at
the intersection of language, machine design, and physical reasoning.

</details>
