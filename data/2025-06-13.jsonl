{"id": "2506.10020", "categories": ["cs.CR", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2506.10020", "abs": "https://arxiv.org/abs/2506.10020", "authors": ["Kyubyung Chae", "Hyunbin Jin", "Taesup Kim"], "title": "From Threat to Tool: Leveraging Refusal-Aware Injection Attacks for Safety Alignment", "comment": null, "summary": "Safely aligning large language models (LLMs) often demands extensive\nhuman-labeled preference data, a process that's both costly and time-consuming.\nWhile synthetic data offers a promising alternative, current methods frequently\nrely on complex iterative prompting or auxiliary models. To address this, we\nintroduce Refusal-Aware Adaptive Injection (RAAI), a straightforward,\ntraining-free, and model-agnostic framework that repurposes LLM attack\ntechniques. RAAI works by detecting internal refusal signals and adaptively\ninjecting predefined phrases to elicit harmful, yet fluent, completions. Our\nexperiments show RAAI effectively jailbreaks LLMs, increasing the harmful\nresponse rate from a baseline of 2.15% to up to 61.04% on average across four\nbenchmarks. Crucially, fine-tuning LLMs with the synthetic data generated by\nRAAI improves model robustness against harmful prompts while preserving general\ncapabilities on standard tasks like MMLU and ARC. This work highlights how LLM\nattack methodologies can be reframed as practical tools for scalable and\ncontrollable safety alignment."}
{"id": "2506.10022", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.10022", "abs": "https://arxiv.org/abs/2506.10022", "authors": ["Haoyang Li", "Huan Gao", "Zhiyuan Zhao", "Zhiyu Lin", "Junyu Gao", "Xuelong Li"], "title": "LLMs Caught in the Crossfire: Malware Requests and Jailbreak Challenges", "comment": "Accepted as ACL 2025 main conference", "summary": "The widespread adoption of Large Language Models (LLMs) has heightened\nconcerns about their security, particularly their vulnerability to jailbreak\nattacks that leverage crafted prompts to generate malicious outputs. While\nprior research has been conducted on general security capabilities of LLMs,\ntheir specific susceptibility to jailbreak attacks in code generation remains\nlargely unexplored. To fill this gap, we propose MalwareBench, a benchmark\ndataset containing 3,520 jailbreaking prompts for malicious code-generation,\ndesigned to evaluate LLM robustness against such threats. MalwareBench is based\non 320 manually crafted malicious code generation requirements, covering 11\njailbreak methods and 29 code functionality categories. Experiments show that\nmainstream LLMs exhibit limited ability to reject malicious code-generation\nrequirements, and the combination of multiple jailbreak methods further reduces\nthe model's security capabilities: specifically, the average rejection rate for\nmalicious content is 60.93%, dropping to 39.92% when combined with jailbreak\nattack algorithms. Our work highlights that the code security capabilities of\nLLMs still pose significant challenges."}
{"id": "2506.10024", "categories": ["cs.CR", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2506.10024", "abs": "https://arxiv.org/abs/2506.10024", "authors": ["Elena Sofia Ruzzetti", "Giancarlo A. Xompero", "Davide Venditti", "Fabio Massimo Zanzotto"], "title": "Private Memorization Editing: Turning Memorization into a Defense to Strengthen Data Privacy in Large Language Models", "comment": "To be published at ACL 2025 (Main)", "summary": "Large Language Models (LLMs) memorize, and thus, among huge amounts of\nuncontrolled data, may memorize Personally Identifiable Information (PII),\nwhich should not be stored and, consequently, not leaked. In this paper, we\nintroduce Private Memorization Editing (PME), an approach for preventing\nprivate data leakage that turns an apparent limitation, that is, the LLMs'\nmemorization ability, into a powerful privacy defense strategy. While attacks\nagainst LLMs have been performed exploiting previous knowledge regarding their\ntraining data, our approach aims to exploit the same kind of knowledge in order\nto make a model more robust. We detect a memorized PII and then mitigate the\nmemorization of PII by editing a model knowledge of its training data. We\nverify that our procedure does not affect the underlying language model while\nmaking it more robust against privacy Training Data Extraction attacks. We\ndemonstrate that PME can effectively reduce the number of leaked PII in a\nnumber of configurations, in some cases even reducing the accuracy of the\nprivacy attacks to zero."}
{"id": "2506.10025", "categories": ["cs.CR", "cs.CY", "cs.HC"], "pdf": "https://arxiv.org/pdf/2506.10025", "abs": "https://arxiv.org/abs/2506.10025", "authors": ["Yuanhaur Chang", "Oren Heller", "Yaniv Shlomo", "Iddo Bar-Noy", "Ella Bokobza", "Michal Grinstein-Weiss", "Ning Zhang"], "title": "Mind the Gap: Revealing Security Barriers through Situational Awareness of Small and Medium Business Key Decision-Makers", "comment": null, "summary": "Key decision-makers in small and medium businesses (SMBs) often lack the\nawareness and knowledge to implement cybersecurity measures effectively. To\ngain a deeper understanding of how SMB executives navigate cybersecurity\ndecision-making, we deployed a mixed-method approach, conducting\nsemi-structured interviews (n=21) and online surveys (n=322) with SMB key\ndecision-makers. Using thematic analysis, we revealed SMB decision-makers'\nperceived risks in terms of the digital assets they valued, and found reasons\nfor their choice of defense measures and factors impacting security perception.\nWe employed the situational awareness model to characterize decision-makers\nbased on cybersecurity awareness, identifying those who have comparatively low\nawareness in the fight against adversaries. We further explored the\nrelationship between awareness and business attributes, and constructed a\nholistic structural equation model to understand how awareness can be improved.\nFinally, we proposed interventions to help SMBs overcome potential challenges."}
{"id": "2506.10540", "categories": ["cs.MA", "cs.CV"], "pdf": "https://arxiv.org/pdf/2506.10540", "abs": "https://arxiv.org/abs/2506.10540", "authors": ["Haoyuan Shi", "Yunxin Li", "Xinyu Chen", "Longyue Wang", "Baotian Hu", "Min Zhang"], "title": "AniMaker: Automated Multi-Agent Animated Storytelling with MCTS-Driven Clip Generation", "comment": null, "summary": "Despite rapid advancements in video generation models, generating coherent\nstorytelling videos that span multiple scenes and characters remains\nchallenging. Current methods often rigidly convert pre-generated keyframes into\nfixed-length clips, resulting in disjointed narratives and pacing issues.\nFurthermore, the inherent instability of video generation models means that\neven a single low-quality clip can significantly degrade the entire output\nanimation's logical coherence and visual continuity. To overcome these\nobstacles, we introduce AniMaker, a multi-agent framework enabling efficient\nmulti-candidate clip generation and storytelling-aware clip selection, thus\ncreating globally consistent and story-coherent animation solely from text\ninput. The framework is structured around specialized agents, including the\nDirector Agent for storyboard generation, the Photography Agent for video clip\ngeneration, the Reviewer Agent for evaluation, and the Post-Production Agent\nfor editing and voiceover. Central to AniMaker's approach are two key technical\ncomponents: MCTS-Gen in Photography Agent, an efficient Monte Carlo Tree Search\n(MCTS)-inspired strategy that intelligently navigates the candidate space to\ngenerate high-potential clips while optimizing resource usage; and AniEval in\nReviewer Agent, the first framework specifically designed for multi-shot\nanimation evaluation, which assesses critical aspects such as story-level\nconsistency, action completion, and animation-specific features by considering\neach clip in the context of its preceding and succeeding clips. Experiments\ndemonstrate that AniMaker achieves superior quality as measured by popular\nmetrics including VBench and our proposed AniEval framework, while\nsignificantly improving the efficiency of multi-candidate generation, pushing\nAI-generated storytelling animation closer to production standards."}
{"id": "2506.10043", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2506.10043", "abs": "https://arxiv.org/abs/2506.10043", "authors": ["Yongqian Sun", "Yu Luo", "Xidao Wen", "Yuan Yuan", "Xiaohui Nie", "Shenglin Zhang", "Tong Liu", "Xi Luo"], "title": "TrioXpert: An automated incident management framework for microservice system", "comment": null, "summary": "Automated incident management plays a pivotal role in large-scale\nmicroservice systems. However, many existing methods rely solely on\nsingle-modal data (e.g., metrics, logs, and traces) and struggle to\nsimultaneously address multiple downstream tasks, including anomaly detection\n(AD), failure triage (FT), and root cause localization (RCL). Moreover, the\nlack of clear reasoning evidence in current techniques often leads to\ninsufficient interpretability. To address these limitations, we propose\nTrioXpert, an end-to-end incident management framework capable of fully\nleveraging multimodal data. TrioXpert designs three independent data processing\npipelines based on the inherent characteristics of different modalities,\ncomprehensively characterizing the operational status of microservice systems\nfrom both numerical and textual dimensions. It employs a collaborative\nreasoning mechanism using large language models (LLMs) to simultaneously handle\nmultiple tasks while providing clear reasoning evidence to ensure strong\ninterpretability. We conducted extensive evaluations on two popular\nmicroservice system datasets, and the experimental results demonstrate that\nTrioXpert achieves outstanding performance in AD (improving by 4.7% to 57.7%),\nFT (improving by 2.1% to 40.6%), and RCL (improving by 1.6% to 163.1%) tasks."}
{"id": "2506.10028", "categories": ["cs.CR", "cs.CV"], "pdf": "https://arxiv.org/pdf/2506.10028", "abs": "https://arxiv.org/abs/2506.10028", "authors": ["S. Vasavi Venkata Lakshmi", "Ziaul Haque Choudhury"], "title": "Secure Data Access in Cloud Environments Using Quantum Cryptography", "comment": null, "summary": "Cloud computing has made storing and accessing data easier but keeping it\nsecure is a big challenge nowadays. Traditional methods of ensuring data may\nnot be strong enough in the future when powerful quantum computers become\navailable. To solve this problem, this study uses quantum cryptography to\nprotect data in the cloud environment. Quantum Key Distribution (QKD) creates\nsecure keys by sending information using quantum particles like photons.\nSpecifically, we use the BB84 protocol, a simple and reliable way to make\nsecure keys that cannot be stolen without detection. To protect the data, we\nuse the Quantum One Time pad (QOTP) for encryption and decryption, ensuring the\ndata stays completely private. This study shows how these Quantum methods can\nbe applied in cloud systems to provide a strong defense against hackers, even\nif they have access to quantum computers. The combination of QKD, BB84, and\nQOTP creates a safe and reliable way to keep data secure when it is stored or\nshared in the cloud. Using quantum cryptography, this paper provides a way to\nensure data security now and in the future, making cloud computing safer for\neveryone to store their data securely and safely."}
{"id": "2506.10874", "categories": ["cs.MA", "cs.GT", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2506.10874", "abs": "https://arxiv.org/abs/2506.10874", "authors": ["Sarah A. Toonsi", "Jeff S. Shamma"], "title": "Higher-Order Uncoupled Learning Dynamics and Nash Equilibrium", "comment": null, "summary": "We study learnability of mixed-strategy Nash Equilibrium (NE) in general\nfinite games using higher-order replicator dynamics as well as classes of\nhigher-order uncoupled heterogeneous dynamics. In higher-order uncoupled\nlearning dynamics, players have no access to utilities of opponents (uncoupled)\nbut are allowed to use auxiliary states to further process information\n(higher-order). We establish a link between uncoupled learning and feedback\nstabilization with decentralized control. Using this association, we show that\nfor any finite game with an isolated completely mixed-strategy NE, there exist\nhigher-order uncoupled learning dynamics that lead (locally) to that NE. We\nfurther establish the lack of universality of learning dynamics by linking\nlearning to the control theoretic concept of simultaneous stabilization. We\nconstruct two games such that any higher-order dynamics that learn the\ncompletely mixed-strategy NE of one of these games can never learn the\ncompletely mixed-strategy NE of the other. Next, motivated by imposing natural\nrestrictions on allowable learning dynamics, we introduce the Asymptotic Best\nResponse (ABR) property. Dynamics with the ABR property asymptotically learn a\nbest response in environments that are asymptotically stationary. We show that\nthe ABR property relates to an internal stability condition on higher-order\nlearning dynamics. We provide conditions under which NE are compatible with the\nABR property. Finally, we address learnability of mixed-strategy NE in the\nbandit setting using a bandit version of higher-order replicator dynamics."}
{"id": "2506.10049", "categories": ["cs.SE", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.10049", "abs": "https://arxiv.org/abs/2506.10049", "authors": ["Francesco Vinci", "Gyunam Park", "Wil van der Aalst", "Massimiliano de Leoni"], "title": "Online Discovery of Simulation Models for Evolving Business Processes (Extended Version)", "comment": null, "summary": "Business Process Simulation (BPS) refers to techniques designed to replicate\nthe dynamic behavior of a business process. Many approaches have been proposed\nto automatically discover simulation models from historical event logs,\nreducing the cost and time to manually design them. However, in dynamic\nbusiness environments, organizations continuously refine their processes to\nenhance efficiency, reduce costs, and improve customer satisfaction. Existing\ntechniques to process simulation discovery lack adaptability to real-time\noperational changes. In this paper, we propose a streaming process simulation\ndiscovery technique that integrates Incremental Process Discovery with Online\nMachine Learning methods. This technique prioritizes recent data while\npreserving historical information, ensuring adaptation to evolving process\ndynamics. Experiments conducted on four different event logs demonstrate the\nimportance in simulation of giving more weight to recent data while retaining\nhistorical knowledge. Our technique not only produces more stable simulations\nbut also exhibits robustness in handling concept drift, as highlighted in one\nof the use cases."}
{"id": "2506.10029", "categories": ["cs.CR", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2506.10029", "abs": "https://arxiv.org/abs/2506.10029", "authors": ["Rafaël Nouailles"], "title": "Evaluation empirique de la sécurisation et de l'alignement de ChatGPT et Gemini: analyse comparative des vulnérabilités par expérimentations de jailbreaks", "comment": "in French language", "summary": "Large Language models (LLMs) are transforming digital usage, particularly in\ntext generation, image creation, information retrieval and code development.\nChatGPT, launched by OpenAI in November 2022, quickly became a reference,\nprompting the emergence of competitors such as Google's Gemini. However, these\ntechnological advances raise new cybersecurity challenges, including prompt\ninjection attacks, the circumvention of regulatory measures (jailbreaking), the\nspread of misinformation (hallucinations) and risks associated with deep fakes.\nThis paper presents a comparative analysis of the security and alignment levels\nof ChatGPT and Gemini, as well as a taxonomy of jailbreak techniques associated\nwith experiments."}
{"id": "2506.10051", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2506.10051", "abs": "https://arxiv.org/abs/2506.10051", "authors": ["Md Istiak Hossain Shihab", "Christopher Hundhausen", "Ahsun Tariq", "Summit Haque", "Yunhan Qiao", "Brian Mulanda"], "title": "The Effects of GitHub Copilot on Computing Students' Programming Effectiveness, Efficiency, and Processes in Brownfield Programming Tasks", "comment": "14 pages, 5 figures", "summary": "When graduates of computing degree programs enter the software industry, they\nwill most likely join teams working on legacy code bases developed by people\nother than themselves. In these so-called brownfield software development\nsettings, generative artificial intelligence (GenAI) coding assistants like\nGitHub Copilot are rapidly transforming software development practices, yet the\nimpact of GenAI on student programmers performing brownfield development tasks\nremains underexplored. This paper investigates how GitHub Copilot influences\nundergraduate students' programming performance, behaviors, and understanding\nwhen completing brownfield programming tasks in which they add new code to an\nunfamiliar code base. We conducted a controlled experiment in which 10\nundergraduate computer science students completed highly similar brownfield\ndevelopment tasks with and without Copilot in a legacy web application. Using a\nmixed-methods approach combining performance analysis, behavioral analysis, and\nexit interviews, we found that students completed tasks 35% faster (p < 0.05)\nand made 50% more solution progress p (< 0.05) when using Copilot. Moreover,\nour analysis revealed that, when using Copilot, students spent 11% less time\nmanually writing code (p < 0.05), and 12% less time conducting web searches (p\n< 0.05), providing evidence of a fundamental shift in how they engaged in\nprogramming. In exit interviews, students reported concerns about not\nunderstanding how or why Copilot suggestions work. This research suggests the\nneed for computing educators to develop new pedagogical approaches that\nleverage GenAI assistants' benefits while fostering reflection on how and why\nGenAI suggestions address brownfield programming tasks. Complete study results\nand analysis are presented at https://ghcopilot-icer.github.io/."}
{"id": "2506.10030", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.10030", "abs": "https://arxiv.org/abs/2506.10030", "authors": ["Tianyu Chen", "Jian Lou", "Wenjie Wang"], "title": "Safeguarding Multimodal Knowledge Copyright in the RAG-as-a-Service Environment", "comment": null, "summary": "As Retrieval-Augmented Generation (RAG) evolves into service-oriented\nplatforms (Rag-as-a-Service) with shared knowledge bases, protecting the\ncopyright of contributed data becomes essential. Existing watermarking methods\nin RAG focus solely on textual knowledge, leaving image knowledge unprotected.\nIn this work, we propose AQUA, the first watermark framework for image\nknowledge protection in Multimodal RAG systems. AQUA embeds semantic signals\ninto synthetic images using two complementary methods: acronym-based triggers\nand spatial relationship cues. These techniques ensure watermark signals\nsurvive indirect watermark propagation from image retriever to textual\ngenerator, being efficient, effective and imperceptible. Experiments across\ndiverse models and datasets show that AQUA enables robust, stealthy, and\nreliable copyright tracing, filling a key gap in multimodal RAG protection."}
{"id": "2506.10056", "categories": ["cs.SE", "cs.PL"], "pdf": "https://arxiv.org/pdf/2506.10056", "abs": "https://arxiv.org/abs/2506.10056", "authors": ["Gabriel Orlanski", "Nicholas Roberts", "Aws Albarghouthi", "Frederic Sala"], "title": "Reward Models Enable Scalable Code Verification by Trading Accuracy for Throughput", "comment": "29 pages, 6 figures, code released here:\n  https://github.com/SprocketLab/orm-code-verifier", "summary": "The standard paradigm for solving coding tasks via large language models\n(LLMs) is to generate-then-rank programs, where the latter step uses a verifier\nin the ranking process. The growing consensus is that a comprehensive verifier\n(e.g., a full test suite) should be prioritized over an outcome reward model\n(ORM) whenever possible, with little consideration given to the trade-offs\ninvolved. We aim to challenge this assumption by systematically exploring the\ntradeoff between speed and accuracy. We find that ORMs play a crucial role in\nscaling verification through trading accuracy for speed, even when a\ncomprehensive verifier is available. Their value becomes especially apparent\nwhen used in a generate-prune-then-rank approach, where a faster but less\naccurate verifier removes incorrect solutions prior to ranking -- leading to a\nsystem that is 11.65x faster while only being 8.33% less accurate than the full\ntest suite. We analyze the generate-prune-then-rank approach and show that it\nworks by filtering out incorrect but highly ranked solutions. These findings\nenable the design of scalable and accurate program ranking systems."}
{"id": "2506.10039", "categories": ["cs.CR", "cs.DM", "Primary 05A17, Secondary 11D45, 11Y60, 94A60", "F.2.1"], "pdf": "https://arxiv.org/pdf/2506.10039", "abs": "https://arxiv.org/abs/2506.10039", "authors": ["Michael A. Idowu"], "title": "Symbolic Generation and Modular Embedding of High-Quality abc-Triples", "comment": "17 pages, includes tables and illustrative examples; discusses\n  symbolic generation of abc-triples and applications in entropy filtering and\n  cryptographic pre-processing", "summary": "We present a symbolic identity for generating integer triples $(a, b, c)$\nsatisfying $a + b = c$, inspired by structural features of the \\emph{abc\nconjecture}. The construction uses powers of $2$ and $3$ in combination with\nmodular inversion in $\\mathbb{Z}/3^p\\mathbb{Z}$, leading to a parametric\nidentity with residue constraints that yield abc-triples exhibiting low radical\nvalues. Through affine transformations, these symbolic triples are embedded\ninto a broader space of high-quality examples, optimised for the ratio $\\log c\n/ \\log \\operatorname{rad}(abc)$. Computational results demonstrate the\nemergence of structured, radical-minimising candidates, including both known\nand novel triples. These methods provide a symbolic and algebraic framework for\ncontrolled triple generation, and suggest exploratory implications for symbolic\nentropy filtering in cryptographic pre-processing."}
{"id": "2506.10204", "categories": ["cs.SE", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.10204", "abs": "https://arxiv.org/abs/2506.10204", "authors": ["Andrei Paleyes", "Radzim Sendyka", "Diana Robinson", "Christian Cabrera", "Neil D. Lawrence"], "title": "Prompt Variability Effects On LLM Code Generation", "comment": null, "summary": "Code generation is one of the most active areas of application of Large\nLanguage Models (LLMs). While LLMs lower barriers to writing code and\naccelerate development process, the overall quality of generated programs\ndepends on the quality of given prompts. Specifically, functionality and\nquality of generated code can be sensitive to user's background and familiarity\nwith software development. It is therefore important to quantify LLM's\nsensitivity to variations in the input. To this end we propose a synthetic\nevaluation pipeline for code generation with LLMs, as well as a systematic\npersona-based evaluation approach to expose qualitative differences of LLM\nresponses dependent on prospective user background. Both proposed methods are\ncompletely independent from specific programming tasks and LLMs, and thus are\nwidely applicable. We provide experimental evidence illustrating utility of our\nmethods and share our code for the benefit of the community."}
{"id": "2506.10042", "categories": ["cs.CR", "cs.HC"], "pdf": "https://arxiv.org/pdf/2506.10042", "abs": "https://arxiv.org/abs/2506.10042", "authors": ["Ece Gumusel"], "title": "Multiverse Privacy Theory for Contextual Risks in Complex User-AI Interactions", "comment": "5 pages, 1 figure, 1 table", "summary": "In an era of increasing interaction with artificial intelligence (AI), users\nface evolving privacy decisions shaped by complex, uncertain factors. This\npaper introduces Multiverse Privacy Theory, a novel framework in which each\nprivacy decision spawns a parallel universe, representing a distinct potential\noutcome based on user choices over time. By simulating these universes, this\ntheory provides a foundation for understanding privacy through the lens of\ncontextual integrity, evolving preferences, and probabilistic decision-making.\nFuture work will explore its application using real-world, scenario-based\nsurvey data."}
{"id": "2506.10280", "categories": ["cs.SE", "cs.CR"], "pdf": "https://arxiv.org/pdf/2506.10280", "abs": "https://arxiv.org/abs/2506.10280", "authors": ["Samiha Shimmi", "Hamed Okhravi", "Mona Rahimi"], "title": "AI-Based Software Vulnerability Detection: A Systematic Literature Review", "comment": null, "summary": "Software vulnerabilities in source code pose serious cybersecurity risks,\nprompting a shift from traditional detection methods (e.g., static analysis,\nrule-based matching) to AI-driven approaches. This study presents a systematic\nreview of software vulnerability detection (SVD) research from 2018 to 2023,\noffering a comprehensive taxonomy of techniques, feature representations, and\nembedding methods. Our analysis reveals that 91% of studies use AI-based\nmethods, with graph-based models being the most prevalent. We identify key\nlimitations, including dataset quality, reproducibility, and interpretability,\nand highlight emerging opportunities in underexplored techniques such as\nfederated learning and quantum neural networks, providing a roadmap for future\nresearch."}
{"id": "2506.10047", "categories": ["cs.CR", "cs.CL"], "pdf": "https://arxiv.org/pdf/2506.10047", "abs": "https://arxiv.org/abs/2506.10047", "authors": ["Zilong Wang", "Xiang Zheng", "Xiaosen Wang", "Bo Wang", "Xingjun Ma", "Yu-Gang Jiang"], "title": "GenBreak: Red Teaming Text-to-Image Generators Using Large Language Models", "comment": "27 pages, 7 figures", "summary": "Text-to-image (T2I) models such as Stable Diffusion have advanced rapidly and\nare now widely used in content creation. However, these models can be misused\nto generate harmful content, including nudity or violence, posing significant\nsafety risks. While most platforms employ content moderation systems,\nunderlying vulnerabilities can still be exploited by determined adversaries.\nRecent research on red-teaming and adversarial attacks against T2I models has\nnotable limitations: some studies successfully generate highly toxic images but\nuse adversarial prompts that are easily detected and blocked by safety filters,\nwhile others focus on bypassing safety mechanisms but fail to produce genuinely\nharmful outputs, neglecting the discovery of truly high-risk prompts.\nConsequently, there remains a lack of reliable tools for evaluating the safety\nof defended T2I models. To address this gap, we propose GenBreak, a framework\nthat fine-tunes a red-team large language model (LLM) to systematically explore\nunderlying vulnerabilities in T2I generators. Our approach combines supervised\nfine-tuning on curated datasets with reinforcement learning via interaction\nwith a surrogate T2I model. By integrating multiple reward signals, we guide\nthe LLM to craft adversarial prompts that enhance both evasion capability and\nimage toxicity, while maintaining semantic coherence and diversity. These\nprompts demonstrate strong effectiveness in black-box attacks against\ncommercial T2I generators, revealing practical and concerning safety\nweaknesses."}
{"id": "2506.10322", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2506.10322", "abs": "https://arxiv.org/abs/2506.10322", "authors": ["Xueying Du", "Kai Yu", "Chong Wang", "Yi Zou", "Wentai Deng", "Zuoyu Ou", "Xin Peng", "Lingming Zhang", "Yiling Lou"], "title": "Minimizing False Positives in Static Bug Detection via LLM-Enhanced Path Feasibility Analysis", "comment": null, "summary": "Static bug analyzers play a crucial role in ensuring software quality.\nHowever, existing analyzers for bug detection in large codebases often suffer\nfrom high false positive rates. This is primarily due to the limited\ncapabilities of analyzers in path feasibility validation with multiple\nconditional branches and complex data dependencies. While current LLM-based\napproaches attempt to address this issue, their effectiveness remains limited\ndue to insufficient constraint cascade analysis and scalability challenges in\nlarge projects. To address this challenge, we propose an iterative path\nfeasibility analysis framework LLM4PFA. By leveraging LLM agent based targeted\nconstraint reasoning, and key context-aware analysis driven by agent planning,\nLLM4PFA effectively enhances complex inter-procedural path feasibility analysis\nfor minimizing false positives in static bug detection. Evaluation results show\nthat LLM4PFA precisely filters out 72% to 96% false positives reported during\nstatic bug detection, significantly outperforming all the baselines by 41.1% -\n105.7% improvements; meanwhile LLM4PFA only misses 3 real bugs of 45 true\npositives."}
{"id": "2506.10104", "categories": ["cs.CR", "cs.SE"], "pdf": "https://arxiv.org/pdf/2506.10104", "abs": "https://arxiv.org/abs/2506.10104", "authors": ["David Farr", "Kevin Talty", "Alexandra Farr", "John Stockdale", "Iain Cruickshank", "Jevin West"], "title": "Expert-in-the-Loop Systems with Cross-Domain and In-Domain Few-Shot Learning for Software Vulnerability Detection", "comment": null, "summary": "As cyber threats become more sophisticated, rapid and accurate vulnerability\ndetection is essential for maintaining secure systems. This study explores the\nuse of Large Language Models (LLMs) in software vulnerability assessment by\nsimulating the identification of Python code with known Common Weakness\nEnumerations (CWEs), comparing zero-shot, few-shot cross-domain, and few-shot\nin-domain prompting strategies. Our results indicate that while zero-shot\nprompting performs poorly, few-shot prompting significantly enhances\nclassification performance, particularly when integrated with confidence-based\nrouting strategies that improve efficiency by directing human experts to cases\nwhere model uncertainty is high, optimizing the balance between automation and\nexpert oversight. We find that LLMs can effectively generalize across\nvulnerability categories with minimal examples, suggesting their potential as\nscalable, adaptable cybersecurity tools in simulated environments. However,\nchallenges such as model reliability, interpretability, and adversarial\nrobustness remain critical areas for future research. By integrating AI-driven\napproaches with expert-in-the-loop (EITL) decision-making, this work highlights\na pathway toward more efficient and responsive cybersecurity workflows. Our\nfindings provide a foundation for deploying AI-assisted vulnerability detection\nsystems in both real and simulated environments that enhance operational\nresilience while reducing the burden on human analysts."}
{"id": "2506.10330", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.10330", "abs": "https://arxiv.org/abs/2506.10330", "authors": ["Seyed Moein Abtahi", "Akramul Azim"], "title": "Augmenting Large Language Models with Static Code Analysis for Automated Code Quality Improvements", "comment": "Accepted at FORGE 2025", "summary": "This study examined code issue detection and revision automation by\nintegrating Large Language Models (LLMs) such as OpenAI's GPT-3.5 Turbo and\nGPT-4o into software development workflows. A static code analysis framework\ndetects issues such as bugs, vulnerabilities, and code smells within a\nlarge-scale software project. Detailed information on each issue was extracted\nand organized to facilitate automated code revision using LLMs. An iterative\nprompt engineering process is applied to ensure that prompts are structured to\nproduce accurate and organized outputs aligned with the project requirements.\nRetrieval-augmented generation (RAG) is implemented to enhance the relevance\nand precision of the revisions, enabling LLM to access and integrate real-time\nexternal knowledge. The issue of LLM hallucinations - where the model generates\nplausible but incorrect outputs - is addressed by a custom-built \"Code\nComparison App,\" which identifies and corrects erroneous changes before\napplying them to the codebase. Subsequent scans using the static code analysis\nframework revealed a significant reduction in code issues, demonstrating the\neffectiveness of combining LLMs, static analysis, and RAG to improve code\nquality, streamline the software development process, and reduce time and\nresource expenditure."}
{"id": "2506.10125", "categories": ["cs.CR", "cs.SE"], "pdf": "https://arxiv.org/pdf/2506.10125", "abs": "https://arxiv.org/abs/2506.10125", "authors": ["Muqi Zou", "Hongyu Cai", "Hongwei Wu", "Zion Leonahenahe Basque", "Arslan Khan", "Berkay Celik", "Dave", "Tian", "Antonio Bianchi", "Ruoyu", "Wang", "Dongyan Xu"], "title": "D-LiFT: Improving LLM-based Decompiler Backend via Code Quality-driven Fine-tuning", "comment": null, "summary": "Decompilers, which reconstruct human-readable source code from binary\nexecutables, are vital to many security tasks. Yet, despite recent advances,\ntheir output often suffers from syntactic and semantic errors and remains\ndifficult to read. Recently, with the advent of large language models (LLMs),\nresearchers began to explore the potential of LLMs to refine decompiler output.\nNevertheless, our study of these approaches reveals significant limitations,\nsuch as introducing new errors and relying on unreliable accuracy validation.\nIn this paper, we present D-LiFT, an automated decompiler backend that\nharnesses and further trains LLMs to improve the quality of decompiled code via\nreinforcement learning (RL). Unlike prior work that overlooks preserving\naccuracy, D-LiFT adheres to a key principle for enhancing the quality of\ndecompiled code: \\textit{preserving accuracy while improving readability}.\nCentral to D-LiFT, we propose D-SCORE, an integrated quality assessment system\nto score the decompiled code from multiple aspects. In line with our principle,\nD-SCORE assigns low scores to any inaccurate output and only awards higher\nscores for readability to code that passes the accuracy check. Specifically,\nD-SCORE first verifies the syntactic and semantic correctness via the compiler\nand symbolic execution; only if a candidate is deemed accurate, it then\nevaluates readability using established metrics to compare the LLM output with\nthe original decompiled code. The score will then be fed back to the LLM for\nfine-tuning. Our implementation, based on Ghidra and a range of LLMs,\ndemonstrates significant improvements for the accurate decompiled code from the\ncoreutils and util-linux projects. Compared to baseline LLMs without\nD-SCORE-driven fine-tuning, D-LiFT produces 55.3% more improved decompiled\nfunctions, as measured by D-SCORE."}
{"id": "2506.10365", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2506.10365", "abs": "https://arxiv.org/abs/2506.10365", "authors": ["Shuyang Hou", "Zhangxiao Shen", "Huayi Wu", "Haoyue Jiao", "Ziqi Liu", "Lutong Xie", "Chang Liu", "Jianyuan Liang", "Yaxian Qing", "Xiaopu Zhang", "Dehua Peng", "Zhipeng Gui", "Xuefeng Guan"], "title": "AutoGEEval++: A Multi-Level and Multi-Geospatial-Modality Automated Evaluation Framework for Large Language Models in Geospatial Code Generation on Google Earth Engine", "comment": null, "summary": "Geospatial code generation is becoming a key frontier in integrating\nartificial intelligence with geo-scientific analysis, yet standardised\nautomated evaluation tools for this task remain absent. This study presents\nAutoGEEval++, an enhanced framework building on AutoGEEval, and the first\nautomated assessment system for large language models (LLMs) generating\ngeospatial code on Google Earth Engine (GEE). It supports diverse data\nmodalities and varying task complexities. Built on the GEE Python API,\nAutoGEEval++ features a benchmark dataset-AutoGEEval++-Bench-with 6,365 test\ncases across 26 data types and three task categories: unit, combo, and theme\ntests. It includes a submission programme and a judge module to realise an\nend-to-end automated evaluation pipeline from code generation to\nexecution-based validation. The framework adopts multi-dimensional\nmetrics-accuracy, resource usage, run-time efficiency, and error\ntypes-balancing hallucination control and efficiency, and enabling boundary\ntesting and error pattern analysis. Using AutoGEEval++, we evaluate 24\nstate-of-the-art LLMs (as of June 2025), including general-purpose,\nreasoning-enhanced, code-centric, and geoscience-specific models. Results\nreveal clear performance, stability, and error differences across task types,\nmodel designs, and deployment settings, confirming AutoGEEval++'s practical\nvalue and scalability in vertical-domain code generation. This work establishes\nthe first standardised evaluation protocol and foundational benchmark for\nGEE-based LLM code generation, providing a unified basis for performance\ncomparison and a methodological framework for systematic, domain-specific code\nevaluation."}
{"id": "2506.10147", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2506.10147", "abs": "https://arxiv.org/abs/2506.10147", "authors": ["Lucas Truax", "Sandip Roy", "Laszlo B. Kish"], "title": "Unconditionally Secure Wireless-Wired Ground-Satellite-Ground Communication Networks Utilizing Classical and Quantum Noise", "comment": null, "summary": "In this paper, we introduce the Kirchhoff-Law-Johnson-Noise (KLJN) as an\napproach to securing satellite communications. KLJN has the potential to\nrevolutionize satellite communication security through its combination of\nsimplicity, cost-effectiveness, and resilience with unconditional security.\nUnlike quantum key distribution (QKD), which requires complex, fragile, and\nexpensive infrastructure like photon detectors and dedicated optical links,\nKLJN operates using standard electronic components and wires, significantly\nreducing implementation costs and logistical hurdles. KLJN's security, grounded\nin the fundamental laws of classical physics, is impervious to environmental\nand radiation-induced noise, making it highly reliable in the harsh conditions\nof satellite communications. This robustness, coupled with its ability to\nintegrate seamlessly with existing infrastructure, positions KLJN as a\nrevolutionary alternative to quantum solutions for ensuring secure, resilient\nsatellite communications. The authors explore the value of achieving\nunconditionally secure communications in strategic ground-to-satellite networks\nwhich address vulnerabilities posed by advanced computational threats,\nincluding quantum computing. Our team has examined two leading approaches to\nunconditional security - the KLJN scheme and QKD - and analyzed the potential\nuse of each for space systems. While QKD leverages quantum mechanics for\nsecurity, it faces challenges related to cost, complexity, and environmental\nsensitivity. In contrast, the KLJN scheme utilizes classical physics principles\nto provide a simpler, more cost-effective, and resilient alternative,\nparticularly for ground-based systems. The study concludes that KLJN offers\nsignificant advantages in simplicity, cost-efficiency, and robustness, making\nit a practical choice for many secure communication applications."}
{"id": "2506.10376", "categories": ["cs.SE", "cs.HC"], "pdf": "https://arxiv.org/pdf/2506.10376", "abs": "https://arxiv.org/abs/2506.10376", "authors": ["Fan Wu", "Cuiyun Gao", "Shuqing Li", "Xin-Cheng Wen", "Qing Liao"], "title": "MLLM-Based UI2Code Automation Guided by UI Layout Information", "comment": "Accepted by the 34th International Symposium on Software Testing and\n  Analysis (ISSTA 2025)", "summary": "Converting user interfaces into code (UI2Code) is a crucial step in website\ndevelopment, which is time-consuming and labor-intensive. The automation of\nUI2Code is essential to streamline this task, beneficial for improving the\ndevelopment efficiency. There exist deep learning-based methods for the task;\nhowever, they heavily rely on a large amount of labeled training data and\nstruggle with generalizing to real-world, unseen web page designs. The advent\nof Multimodal Large Language Models (MLLMs) presents potential for alleviating\nthe issue, but they are difficult to comprehend the complex layouts in UIs and\ngenerate the accurate code with layout preserved. To address these issues, we\npropose LayoutCoder, a novel MLLM-based framework generating UI code from\nreal-world webpage images, which includes three key modules: (1) Element\nRelation Construction, which aims at capturing UI layout by identifying and\ngrouping components with similar structures; (2) UI Layout Parsing, which aims\nat generating UI layout trees for guiding the subsequent code generation\nprocess; and (3) Layout-Guided Code Fusion, which aims at producing the\naccurate code with layout preserved. For evaluation, we build a new benchmark\ndataset which involves 350 real-world websites named Snap2Code, divided into\nseen and unseen parts for mitigating the data leakage issue, besides the\npopular dataset Design2Code. Extensive evaluation shows the superior\nperformance of LayoutCoder over the state-of-the-art approaches. Compared with\nthe best-performing baseline, LayoutCoder improves 10.14% in the BLEU score and\n3.95% in the CLIP score on average across all datasets."}
{"id": "2506.10171", "categories": ["cs.CR", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2506.10171", "abs": "https://arxiv.org/abs/2506.10171", "authors": ["Saswat Das", "Jameson Sandler", "Ferdinando Fioretto"], "title": "Disclosure Audits for LLM Agents", "comment": null, "summary": "Large Language Model agents have begun to appear as personal assistants,\ncustomer service bots, and clinical aides. While these applications deliver\nsubstantial operational benefits, they also require continuous access to\nsensitive data, which increases the likelihood of unauthorized disclosures.\nThis study proposes an auditing framework for conversational privacy that\nquantifies and audits these risks. The proposed Conversational Manipulation for\nPrivacy Leakage (CMPL) framework, is an iterative probing strategy designed to\nstress-test agents that enforce strict privacy directives. Rather than focusing\nsolely on a single disclosure event, CMPL simulates realistic multi-turn\ninteractions to systematically uncover latent vulnerabilities. Our evaluation\non diverse domains, data modalities, and safety configurations demonstrate the\nauditing framework's ability to reveal privacy risks that are not deterred by\nexisting single-turn defenses. In addition to introducing CMPL as a diagnostic\ntool, the paper delivers (1) an auditing procedure grounded in quantifiable\nrisk metrics and (2) an open benchmark for evaluation of conversational privacy\nacross agent implementations."}
{"id": "2506.10397", "categories": ["cs.SE", "cs.CY", "cs.DC"], "pdf": "https://arxiv.org/pdf/2506.10397", "abs": "https://arxiv.org/abs/2506.10397", "authors": ["Mir Mohammad Yousuf", "Shabir Ahmad Sofi"], "title": "Bug Classification in Quantum Software: A Rule-Based Framework and Its Evaluation", "comment": "25 pages, 5 figures", "summary": "Accurate classification of software bugs is essential for improving software\nquality. This paper presents a rule-based automated framework for classifying\nissues in quantum software repositories by bug type, category, severity, and\nimpacted quality attributes, with additional focus on quantum-specific bug\ntypes. The framework applies keyword and heuristic-based techniques tailored to\nquantum computing. To assess its reliability, we manually classified a\nstratified sample of 4,984 issues from a dataset of 12,910 issues across 36\nQiskit repositories. Automated classifications were compared with ground truth\nusing accuracy, precision, recall, and F1-score. The framework achieved up to\n85.21% accuracy, with F1-scores ranging from 0.7075 (severity) to 0.8393\n(quality attribute). Statistical validation via paired t-tests and Cohen's\nKappa showed substantial to almost perfect agreement for bug type (k = 0.696),\ncategory (k = 0.826), quality attribute (k = 0.818), and quantum-specific bug\ntype (k = 0.712). Severity classification showed slight agreement (k = 0.162),\nsuggesting room for improvement. Large-scale analysis revealed that classical\nbugs dominate (67.2%), with quantum-specific bugs at 27.3%. Frequent bug\ncategories included compatibility, functional, and quantum-specific defects,\nwhile usability, maintainability, and interoperability were the most impacted\nquality attributes. Most issues (93.7%) were low severity; only 4.3% were\ncritical. A detailed review of 1,550 quantum-specific bugs showed that over\nhalf involved quantum circuit-level problems, followed by gate errors and\nhardware-related issues."}
{"id": "2506.10175", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2506.10175", "abs": "https://arxiv.org/abs/2506.10175", "authors": ["Nanda Rani", "Sandeep Kumar Shukla"], "title": "AURA: A Multi-Agent Intelligence Framework for Knowledge-Enhanced Cyber Threat Attribution", "comment": null, "summary": "Effective attribution of Advanced Persistent Threats (APTs) increasingly\nhinges on the ability to correlate behavioral patterns and reason over complex,\nvaried threat intelligence artifacts. We present AURA (Attribution Using\nRetrieval-Augmented Agents), a multi-agent, knowledge-enhanced framework for\nautomated and interpretable APT attribution. AURA ingests diverse threat data\nincluding Tactics, Techniques, and Procedures (TTPs), Indicators of Compromise\n(IoCs), malware details, adversarial tools, and temporal information, which are\nprocessed through a network of collaborative agents. These agents are designed\nfor intelligent query rewriting, context-enriched retrieval from structured\nthreat knowledge bases, and natural language justification of attribution\ndecisions. By combining Retrieval-Augmented Generation (RAG) with Large\nLanguage Models (LLMs), AURA enables contextual linking of threat behaviors to\nknown APT groups and supports traceable reasoning across multiple attack\nphases. Experiments on recent APT campaigns demonstrate AURA's high attribution\nconsistency, expert-aligned justifications, and scalability. This work\nestablishes AURA as a promising direction for advancing transparent,\ndata-driven, and scalable threat attribution using multi-agent intelligence."}
{"id": "2506.10426", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2506.10426", "abs": "https://arxiv.org/abs/2506.10426", "authors": ["Xiao Yu", "Haoxuan Chen", "Feifei Niu", "Xing Hu", "Jacky Wai Keung", "Xin Xia"], "title": "Towards Understanding Bugs in Distributed Training and Inference Frameworks for Large Language Models", "comment": null, "summary": "With the rapid development of large language models (LLMs), distributed\ntraining and inference frameworks like DeepSpeed have become essential for\nscaling model training and inference across multiple GPUs or nodes. However,\nthe increasing complexity of these frameworks brings non-trivial software bugs,\nwhich may degrade training performance, cause unexpected failures, and result\nin significant resource waste. Understanding framework bugs' characteristics is\nfundamental for quality assurance, allowing the design of more effective\ndebugging and repair methods. Thus, our paper conducts the first large-scale\nempirical analysis of 308 fixed bugs across three popular distributed\ntraining/inference frameworks: DeepSpeed, Megatron-LM, and Colossal-AI. We\nexamine bug symptoms, root causes, bug identification and fixing efforts, and\ncommon low-effort fixing strategies. Additionally, the distributed nature of\nthese frameworks introduces unique bug root causes, such as allocation strategy\nerror and distributed communication error. Diagnosing and fixing complex bugs\nremains challenging due to factors like the disconnect between symptoms and\nroot causes, high bug reproduction costs, and low-level or cross-component\ninteractions. Interestingly, we observe that 48% of bug fixes require minimal\ncode changes (<=10 LOC) and follow simple strategies such as conditional logic\noptimization, parameter handling enhancement, or version compatibility\nhandling, indicating potential for automation. Based on these insights, we\noffer several implications for improving the reliability of both distributed\ntraining and inference frameworks and their dependent LLM projects, while also\nidentifying opportunities to leverage LLM-based tools for automated debugging\nand repair."}
{"id": "2506.10194", "categories": ["cs.CR", "cs.CY"], "pdf": "https://arxiv.org/pdf/2506.10194", "abs": "https://arxiv.org/abs/2506.10194", "authors": ["Marius Mehrl", "Mila Pfander", "Theresa Winner", "Cornelius Fritz"], "title": "Guardians of the Regime: When and Why Autocrats Create Secret Police", "comment": null, "summary": "Autocrats use secret police to stay in power, as these organizations deter\nand suppress opposition to their rule. Existing research shows that secret\npolice are very good at this but, surprisingly, also that they are not as\nubiquitous in autocracies as one may assume, existing in less than 50% of\nautocratic country-years. We thus explore under which conditions secret police\nemerge in dictatorships. For this purpose, we apply statistical variable\nselection techniques to identify which of several candidate variables extracted\nfrom the literature on state security forces and authoritarian survival hold\nexplanatory power. Our results highlight that secret police are more likely to\nemerge when rulers face specific, preempt-able threats, such as protests and\nanti-system mobilisation, but also when they have the material resources to\nestablish these organisations. This research contributes to our understanding\nof autocrats' institutional choices and authoritarian politics."}
{"id": "2506.10484", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2506.10484", "abs": "https://arxiv.org/abs/2506.10484", "authors": ["Fangwen Mu", "Junjie Wang", "Lin Shi", "Song Wang", "Shoubin Li", "Qing Wang"], "title": "EXPEREPAIR: Dual-Memory Enhanced LLM-based Repository-Level Program Repair", "comment": null, "summary": "Automatically repairing software issues remains a fundamental challenge at\nthe intersection of software engineering and AI. Although recent advancements\nin Large Language Models (LLMs) have demonstrated potential for\nrepository-level repair tasks, current methodologies exhibit two notable\nlimitations: (1) they often address issues in isolation, neglecting to\nincorporate insights from previously resolved issues, and (2) they rely on\nstatic and rigid prompting strategies, which constrain their ability to\ngeneralize across diverse and evolving issue scenarios. Inspired by the dual\nmemory systems of human cognition, where episodic and semantic memories work\nsynergistically to support human reasoning and decision-making, we propose\nExpeRepair, a novel LLM-based approach that continuously learns from historical\nrepair experiences through dual-channel knowledge accumulation. ExpeRepair\norganizes historical repair experiences into two complementary memories: an\nepisodic memory that stores concrete repair demonstrations, and a semantic\nmemory that encodes abstract reflective insights. At inference time, ExpeRepair\nactivates both memory systems by retrieving relevant demonstrations from\nepisodic memory and recalling high-level repair insights from semantic memory.\nIt further enhances adaptability through dynamic prompt composition,\nsynergistically integrating both memory types to replace static prompts with\ncontext-aware, experience-driven prompts. Experiments on the SWE-bench Lite\nbenchmark demonstrate that ExpeRepair achieves a pass@1 score of 49.3% with\nClaude 3.7 Sonnet, outperforming all state-of-the-art open-source methods."}
{"id": "2506.10236", "categories": ["cs.CR", "cs.AI", "cs.CL", "cs.CY", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.10236", "abs": "https://arxiv.org/abs/2506.10236", "authors": ["Yeonwoo Jang", "Shariqah Hossain", "Ashwin Sreevatsa", "Diogo Cruz"], "title": "Prompt Attacks Reveal Superficial Knowledge Removal in Unlearning Methods", "comment": "20 pages, 6 figures", "summary": "In this work, we show that some machine unlearning methods may fail when\nsubjected to straightforward prompt attacks. We systematically evaluate eight\nunlearning techniques across three model families, and employ output-based,\nlogit-based, and probe analysis to determine to what extent supposedly\nunlearned knowledge can be retrieved. While methods like RMU and TAR\ndemonstrate robust unlearning, ELM remains vulnerable to specific prompt\nattacks (e.g., Hindi filler text in original prompt recovering 57.3% accuracy).\nOur logit analysis also confirms that unlearned models are generally not hiding\nknowledge by modifying the way the answer is formatted, as the correlation\nbetween output and logit accuracy is strong. These results challenge prevailing\nassumptions about unlearning effectiveness and highlight the need for\nevaluation frameworks that can reliably distinguish between true knowledge\nremoval and superficial output suppression. We also publicly make available our\nevaluation framework to easily evaluate prompting techniques to retrieve\nunlearning knowledge."}
{"id": "2506.10501", "categories": ["cs.SE", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.10501", "abs": "https://arxiv.org/abs/2506.10501", "authors": ["Surya Jasper", "Minh Luu", "Evan Pan", "Aakash Tyagi", "Michael Quinn", "Jiang Hu", "David Kebo Houngninou"], "title": "BugGen: A Self-Correcting Multi-Agent LLM Pipeline for Realistic RTL Bug Synthesis", "comment": null, "summary": "Hardware complexity continues to strain verification resources, motivating\nthe adoption of machine learning (ML) methods to improve debug efficiency.\nHowever, ML-assisted debugging critically depends on diverse and scalable bug\ndatasets, which existing manual or automated bug insertion methods fail to\nreliably produce. We introduce BugGen, a first of its kind, fully autonomous,\nmulti-agent pipeline leveraging Large Language Models (LLMs) to systematically\ngenerate, insert, and validate realistic functional bugs in RTL. BugGen\npartitions modules, selects mutation targets via a closed-loop agentic\narchitecture, and employs iterative refinement and rollback mechanisms to\nensure syntactic correctness and functional detectability. Evaluated across\nfive OpenTitan IP blocks, BugGen produced 500 unique bugs with 94% functional\naccuracy and achieved a throughput of 17.7 validated bugs per hour-over five\ntimes faster than typical manual expert insertion. Additionally, BugGen\nidentified 104 previously undetected bugs in OpenTitan regressions,\nhighlighting its utility in exposing verification coverage gaps. Compared\nagainst Certitude, BugGen demonstrated over twice the syntactic accuracy,\ndeeper exposure of testbench blind spots, and more functionally meaningful and\ncomplex bug scenarios. Furthermore, when these BugGen-generated datasets were\nemployed to train ML-based failure triage models, we achieved high\nclassification accuracy (88.1%-93.2%) across different IP blocks, confirming\nthe practical utility and realism of generated bugs. BugGen thus provides a\nscalable solution for generating high-quality bug datasets, significantly\nenhancing verification efficiency and ML-assisted debugging."}
{"id": "2506.10323", "categories": ["cs.CR", "cs.SE"], "pdf": "https://arxiv.org/pdf/2506.10323", "abs": "https://arxiv.org/abs/2506.10323", "authors": ["Chuyang Chen", "Brendan Dolan-Gavitt", "Zhiqiang Lin"], "title": "ELFuzz: Efficient Input Generation via LLM-driven Synthesis Over Fuzzer Space", "comment": "Accepted by USENIX Security'25 Cycle 2", "summary": "Generation-based fuzzing produces appropriate testing cases according to\nspecifications of input grammars and semantic constraints to test systems and\nsoftware. However, these specifications require significant manual efforts to\nconstruct. This paper proposes a new approach, ELFuzz (Evolution Through Large\nLanguage Models for Fuzzing), that automatically synthesizes generation-based\nfuzzers tailored to a system under test (SUT) via LLM-driven synthesis over\nfuzzer space. At a high level, it starts with minimal seed fuzzers and propels\nthe synthesis by fully automated LLM-driven evolution with coverage guidance.\nCompared to previous approaches, ELFuzz can 1) seamlessly scale to SUTs of\nreal-world sizes -- up to 1,791,104 lines of code in our evaluation -- and 2)\nsynthesize efficient fuzzers that catch interesting grammatical structures and\nsemantic constraints in a human-understandable way. Our evaluation compared\nELFuzz with specifications manually written by domain experts and synthesized\nby state-of-the-art approaches. It shows that ELFuzz achieves up to 434.8% more\ncoverage and triggers up to 174.0% more artificially injected bugs. We also\nused ELFuzz to conduct a real-world fuzzing campaign on the newest version of\ncvc5 for 14 days, and encouragingly, it found five 0-day bugs (three are\nexploitable). Moreover, we conducted an ablation study, which shows that the\nfuzzer space model, the key component of ELFuzz, contributes the most (up to\n62.5%) to the effectiveness of ELFuzz. Further analysis of the fuzzers\nsynthesized by ELFuzz confirms that they catch interesting grammatical\nstructures and semantic constraints in a human-understandable way. The results\npresent the promising potential of ELFuzz for more automated, efficient, and\nextensible input generation for fuzzing."}
{"id": "2506.10525", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2506.10525", "abs": "https://arxiv.org/abs/2506.10525", "authors": ["Junhang Cheng", "Fang Liu", "Chengru Wu", "Li Zhang"], "title": "AdaptiveLLM: A Framework for Selecting Optimal Cost-Efficient LLM for Code-Generation Based on CoT Length", "comment": "Accepted by Internetware 2025", "summary": "While Large Language Models (LLMs) have significantly advanced code\ngeneration efficiency, they face inherent challenges in balancing performance\nand inference costs across diverse programming tasks. Dynamically selecting the\noptimal LLM based on task difficulty and resource constraints offers a\npromising approach to achieve an optimal balance between efficiency and\nperformance. However, existing model selection methods are resource-intensive\nand often neglect cost efficiency. Moreover, these approaches rely on\nhuman-annotated difficulty labels that are frequently inaccessible in\nreal-world settings and may not align with the LLM's own assessment of task\ndifficulty. In this paper, we introduce AdaptiveLLM, a framework that\ndynamically selects optimal LLMs for a given coding task by automatically\nassessing task difficulty. Our framework first estimates task difficulty using\nChain-of-Thought lengths generated by reasoning model, clusters these into\nthree difficulty levels via k-means, and fine-tunes CodeBERT to embed\ndifficulty-aware features. A trained XGBoost classifier then selects the best\nmodel for each problem, optimizing the performance-cost trade-off. Experimental\nresults show that AdaptiveLLM achieves a 7.86% improvement in pass@1 score\nwhile reducing resource consumption by 88.9% compared to baseline method\nComplexityNet. When compared to a single model, AdaptiveLLM demonstrates an\napproximately 15% accuracy improvement, while maintaining the same level of\ncost consumption. Apart from that, the difficulty assessment using CoT provides\nmore reliable selection criteria than human evaluation. Our replication package\nis available at https://github.com/cjhCoder7/AdaptiveLLM."}
{"id": "2506.10327", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2506.10327", "abs": "https://arxiv.org/abs/2506.10327", "authors": ["Sharad Shrestha", "Mohammed Ababneh", "Satyajayant Misra", "Henry M. Cathey, Jr.", "Roopa Vishwanathan", "Matt Jansen", "Jinhong Choi", "Rakesh Bobba", "Yeongjin Jang"], "title": "A Comprehensive Survey of Unmanned Aerial Systems' Risks and Mitigation Strategies", "comment": null, "summary": "In the last decade, the rapid growth of Unmanned Aircraft Systems (UAS) and\nUnmanned Aircraft Vehicles (UAV) in communication, defense, and transportation\nhas increased. The application of UAS will continue to increase rapidly. This\nhas led researchers to examine security vulnerabilities in various facets of\nUAS infrastructure and UAVs, which form a part of the UAS system to reinforce\nthese critical systems. This survey summarizes the cybersecurity\nvulnerabilities in several phases of UAV deployment, the likelihood of each\nvulnerability's occurrence, the impact of attacks, and mitigation strategies\nthat could be applied. We go beyond the state-of-the-art by taking a\ncomprehensive approach to enhancing UAS security by performing an analysis of\nboth UAS-specific and non-UAS-specific mitigation strategies that are\napplicable within the UAS domain to define the lessons learned. We also present\nrelevant cybersecurity standards and their recommendations in the UAS context.\nDespite the significant literature in UAS security and the relevance of\ncyberphysical and networked systems security approaches from the past, which we\nidentify in the survey, we find several critical research gaps that require\nfurther investigation. These form part of our discussions and recommendations\nfor the future exploration by our research community."}
{"id": "2506.10624", "categories": ["cs.SE", "cs.AR"], "pdf": "https://arxiv.org/pdf/2506.10624", "abs": "https://arxiv.org/abs/2506.10624", "authors": ["Lukas Jünger", "Jan Henrik Weinstock", "Tim Kraus"], "title": "Scalable Software Testing in Fast Virtual Platforms: Leveraging SystemC, QEMU and Containerization", "comment": "Published in DVCon China 2025", "summary": "The ever-increasing complexity of HW/SW systems presents a persistent\nchallenge, particularly in safety-critical domains like automotive, where\nextensive testing is imperative. However, the availability of hardware often\nlags behind, hindering early-stage software development. To address this,\nVirtual Platforms (VPs) based on the SystemC TLM-2.0 standard have emerged as a\npivotal solution, enabling pre-silicon execution and testing of unmodified\ntarget software. In this study, we propose an approach leveraging\ncontainerization to encapsulate VPs in order to reduce environment dependencies\nand enable cloud deployment for fast, parallelized test execution, as well as\nopen-source VP technologies such as QEMU and VCML to obviate the need for seat\nlicenses. To demonstrate the efficacy of our approach, we present an Artificial\nIntelligence (AI) accelerator VP case study. Through our research, we offer a\nrobust solution to address the challenges posed by the complexity of HW/SW\nsystems, with practical implications for accelerating HW/SW co-development."}
{"id": "2506.10338", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2506.10338", "abs": "https://arxiv.org/abs/2506.10338", "authors": ["Kwangsu Lee"], "title": "Adaptive Chosen-Ciphertext Security of Distributed Broadcast Encryption", "comment": "arXiv admin note: text overlap with arXiv:2505.17527", "summary": "Distributed broadcast encryption (DBE) is a specific kind of broadcast\nencryption (BE) where users independently generate their own public and private\nkeys, and a sender can efficiently create a ciphertext for a subset of users by\nusing the public keys of the subset users. Previously proposed DBE schemes have\nbeen proven in the adaptive chosen-plaintext attack (CPA) security model and\nhave the disadvantage of requiring linear number of pairing operations when\nverifying the public key of a user. In this paper, we propose an efficient DBE\nscheme in bilinear groups and prove adaptive chosen-ciphertext attack (CCA)\nsecurity for the first time. To do this, we first propose a semi-static CCA\nsecure DBE scheme and prove the security under the $q$-Type assumption. Then,\nby modifying the generic transformation of Gentry and Waters that converts a\nsemi-static CPA secure DBE scheme into an adaptive CPA secure DBE scheme to be\napplied to CCA secure DBE schemes, we propose an adaptive CCA secure DBE scheme\nand prove its adaptive CCA security. Our proposed DBE scheme is efficient\nbecause it requires constant size ciphertexts, constant size private keys, and\nlinear size public keys, and the public key verification requires only a\nconstant number of pairing operations and efficient group membership checks."}
{"id": "2506.10654", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2506.10654", "abs": "https://arxiv.org/abs/2506.10654", "authors": ["Abir Bouraffa", "Carolin Brandt", "Andy Zaidmann", "Walid Maalej"], "title": "Not One to Rule Them All: Mining Meaningful Code Review Orders From GitHub", "comment": null, "summary": "Developers use tools such as GitHub pull requests to review code, discuss\nproposed changes, and request modifications. While changed files are commonly\npresented in alphabetical order, this does not necessarily coincide with the\nreviewer's preferred navigation sequence. This study investigates the different\nnavigation orders developers follow while commenting on changes submitted in\npull requests. We mined code review comments from 23,241 pull requests in 100\npopular Java and Python repositories on GitHub to analyze the order in which\nthe reviewers commented on the submitted changes. Our analysis shows that for\n44.6% of pull requests, the reviewers comment in a non-alphabetical order.\nAmong these pull requests, we identified traces of alternative meaningful\norders: 20.6% (2,134) followed a largest-diff-first order, 17.6% (1,827) were\ncommented in the order of the files' similarity to the pull request's title and\ndescription, and 29% (1,188) of pull requests containing changes to both\nproduction and test files adhered to a test-first order. We also observed that\nthe proportion of reviewed files to total submitted files was significantly\nhigher in non-alphabetically ordered reviews, which also received slightly\nfewer approvals from reviewers, on average. Our findings highlight the need for\nadditional support during code reviews, particularly for larger pull requests,\nwhere reviewers are more likely to adopt complex strategies rather than\nfollowing a single predefined order."}
{"id": "2506.10399", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2506.10399", "abs": "https://arxiv.org/abs/2506.10399", "authors": ["Zhaoxuan Kan", "Husheng Han", "Shangyi Shi", "Tenghui Hua", "Hang Lu", "Xiaowei Li", "Jianan Mu", "Xing Hu"], "title": "FicGCN: Unveiling the Homomorphic Encryption Efficiency from Irregular Graph Convolutional Networks", "comment": "Accepted by ICML 2025", "summary": "Graph Convolutional Neural Networks (GCNs) have gained widespread popularity\nin various fields like personal healthcare and financial systems, due to their\nremarkable performance. Despite the growing demand for cloud-based GCN\nservices, privacy concerns over sensitive graph data remain significant.\nHomomorphic Encryption (HE) facilitates Privacy-Preserving Machine Learning\n(PPML) by allowing computations to be performed on encrypted data. However, HE\nintroduces substantial computational overhead, particularly for GCN operations\nthat require rotations and multiplications in matrix products. The sparsity of\nGCNs offers significant performance potential, but their irregularity\nintroduces additional operations that reduce practical gains. In this paper, we\npropose FicGCN, a HE-based framework specifically designed to harness the\nsparse characteristics of GCNs and strike a globally optimal balance between\naggregation and combination operations. FicGCN employs a latency-aware packing\nscheme, a Sparse Intra-Ciphertext Aggregation (SpIntra-CA) method to minimize\nrotation overhead, and a region-based data reordering driven by local adjacency\nstructure. We evaluated FicGCN on several popular datasets, and the results\nshow that FicGCN achieved the best performance across all tested datasets, with\nup to a 4.10x improvement over the latest design."}
{"id": "2506.10704", "categories": ["cs.SE", "cs.AI", "D.2.1; D.2.4; D.2.10; F.4.1; F.4.3"], "pdf": "https://arxiv.org/pdf/2506.10704", "abs": "https://arxiv.org/abs/2506.10704", "authors": ["Arshad Beg", "Diarmuid O'Donoghue", "Rosemary Monahan"], "title": "Formalising Software Requirements using Large Language Models", "comment": "Accepted and presented as a poster in ADAPT Annual Conference\n  (AACS2025) on 15th of May, 2025", "summary": "This paper is a brief introduction to our recently initiated project named\nVERIFAI: Traceability and verification of natural language requirements. The\nproject addresses the challenges in the traceability and verification of formal\nspecifications through providing support for the automatic generation of the\nformal specifications and the traceability of the requirements from the initial\nsoftware design stage through the systems implementation and verification.\nApproaches explored in this project include Natural Language Processing, use of\nontologies to describe the software system domain, reuse of existing software\nartefacts from similar systems (i.e. through similarity based reuse) and large\nlanguage models to identify and declare the specifications as well as use of\nartificial intelligence to guide the process."}
{"id": "2506.10424", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.10424", "abs": "https://arxiv.org/abs/2506.10424", "authors": ["Kaiyuan Zhang", "Siyuan Cheng", "Hanxi Guo", "Yuetian Chen", "Zian Su", "Shengwei An", "Yuntao Du", "Charles Fleming", "Ashish Kundu", "Xiangyu Zhang", "Ninghui Li"], "title": "SOFT: Selective Data Obfuscation for Protecting LLM Fine-tuning against Membership Inference Attacks", "comment": "Accepted by the 34th USENIX Security Symposium 2025. Code is\n  available at https://github.com/KaiyuanZh/SOFT", "summary": "Large language models (LLMs) have achieved remarkable success and are widely\nadopted for diverse applications. However, fine-tuning these models often\ninvolves private or sensitive information, raising critical privacy concerns.\nIn this work, we conduct the first comprehensive study evaluating the\nvulnerability of fine-tuned LLMs to membership inference attacks (MIAs). Our\nempirical analysis demonstrates that MIAs exploit the loss reduction during\nfine-tuning, making them highly effective in revealing membership information.\nThese findings motivate the development of our defense. We propose SOFT\n(\\textbf{S}elective data \\textbf{O}bfuscation in LLM\n\\textbf{F}ine-\\textbf{T}uning), a novel defense technique that mitigates\nprivacy leakage by leveraging influential data selection with an adjustable\nparameter to balance utility preservation and privacy protection. Our extensive\nexperiments span six diverse domains and multiple LLM architectures and scales.\nResults show that SOFT effectively reduces privacy risks while maintaining\ncompetitive model performance, offering a practical and scalable solution to\nsafeguard sensitive information in fine-tuned LLMs."}
{"id": "2506.10770", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2506.10770", "abs": "https://arxiv.org/abs/2506.10770", "authors": ["Joran Leest", "Claudia Raibulet", "Patricia Lago", "Ilias Gerostathopoulos"], "title": "From Tea Leaves to System Maps: Context-awareness in Monitoring Operational Machine Learning Models", "comment": null, "summary": "Machine learning (ML) models in production do not fail due to statistical\nanomalies in their input data; they fail due to contextual misalignment -- when\ntheir environment deviates from training assumptions, leading to unreliable\npredictions. Effective ML monitoring requires rich contextual information to\nmove beyond detecting statistical shifts toward meaningful alerts and\nsystematic root-cause analysis. Yet, surprisingly, despite extensive research\nin ML monitoring and related disciplines (drift detection, data validation,\nout-of-distribution detection), there is no shared understanding of how to use\ncontextual information -- striking, given that monitoring involves\ninterpretation of information in context. In response, this paper presents a\nsystematic review to characterize and structure the various types of contextual\ninformation in this domain. Our analysis examines 94 primary studies across\ndata mining, databases, software engineering, and ML. We introduce the\nContextual System--Aspect--Representation (C-SAR) framework, a conceptual model\nthat synthesizes our findings. We also identify 20 recurring and potentially\nreusable patterns of specific system, aspect, and representation combinations,\nand map them to the monitoring activities they support. This study provides a\nnew perspective on ML monitoring: from interpreting \"tea leaves\" of\nobservational statistics into constructing and managing \"system maps\" that\nenable systematic and reliable ML monitoring practices."}
{"id": "2506.10467", "categories": ["cs.CR", "cs.AI", "68T01", "I.2.1"], "pdf": "https://arxiv.org/pdf/2506.10467", "abs": "https://arxiv.org/abs/2506.10467", "authors": ["Felix Härer"], "title": "Specification and Evaluation of Multi-Agent LLM Systems -- Prototype and Cybersecurity Applications", "comment": null, "summary": "Recent advancements in LLMs indicate potential for novel applications, e.g.,\nthrough reasoning capabilities in the latest OpenAI and DeepSeek models. For\napplying these models in specific domains beyond text generation, LLM-based\nmulti-agent approaches can be utilized that solve complex tasks by combining\nreasoning techniques, code generation, and software execution. Applications\nmight utilize these capabilities and the knowledge of specialized LLM agents.\nHowever, while many evaluations are performed on LLMs, reasoning techniques,\nand applications individually, their joint specification and combined\napplication is not explored well. Defined specifications for multi-agent LLM\nsystems are required to explore their potential and their suitability for\nspecific applications, allowing for systematic evaluations of LLMs, reasoning\ntechniques, and related aspects. This paper reports the results of exploratory\nresearch to specify and evaluate these aspects through a multi-agent system.\nThe system architecture and prototype are extended from previous research and a\nspecification is introduced for multi-agent systems. Test cases involving\ncybersecurity tasks indicate feasibility of the architecture and evaluation\napproach. In particular, the results show the evaluation of question answering,\nserver security, and network security tasks that were completed correctly by\nagents with LLMs from OpenAI and DeepSeek."}
{"id": "2506.10785", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.10785", "abs": "https://arxiv.org/abs/2506.10785", "authors": ["Vinaik Chhetri", "Krishna Upadhyay", "A. B. Siddique", "Umar Farooq"], "title": "What Users Value and Critique: Large-Scale Analysis of User Feedback on AI-Powered Mobile Apps", "comment": "12 pages, 6 figures, 5 tables", "summary": "Artificial Intelligence (AI)-powered features have rapidly proliferated\nacross mobile apps in various domains, including productivity, education,\nentertainment, and creativity. However, how users perceive, evaluate, and\ncritique these AI features remains largely unexplored, primarily due to the\noverwhelming volume of user feedback. In this work, we present the first\ncomprehensive, large-scale study of user feedback on AI-powered mobile apps,\nleveraging a curated dataset of 292 AI-driven apps across 14 categories with\n894K AI-specific reviews from Google Play. We develop and validate a\nmulti-stage analysis pipeline that begins with a human-labeled benchmark and\nsystematically evaluates large language models (LLMs) and prompting strategies.\nEach stage, including review classification, aspect-sentiment extraction, and\nclustering, is validated for accuracy and consistency. Our pipeline enables\nscalable, high-precision analysis of user feedback, extracting over one million\naspect-sentiment pairs clustered into 18 positive and 15 negative user topics.\nOur analysis reveals that users consistently focus on a narrow set of themes:\npositive comments emphasize productivity, reliability, and personalized\nassistance, while negative feedback highlights technical failures (e.g.,\nscanning and recognition), pricing concerns, and limitations in language\nsupport. Our pipeline surfaces both satisfaction with one feature and\nfrustration with another within the same review. These fine-grained,\nco-occurring sentiments are often missed by traditional approaches that treat\npositive and negative feedback in isolation or rely on coarse-grained analysis.\nTo this end, our approach provides a more faithful reflection of the real-world\nuser experiences with AI-powered apps. Category-aware analysis further uncovers\nboth universal drivers of satisfaction and domain-specific frustrations."}
{"id": "2506.10502", "categories": ["cs.CR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.10502", "abs": "https://arxiv.org/abs/2506.10502", "authors": ["Junhua Lin", "Marc Juarez"], "title": "A Crack in the Bark: Leveraging Public Knowledge to Remove Tree-Ring Watermarks", "comment": "18 pages, to be published in the 34th USENIX Security Symposium", "summary": "We present a novel attack specifically designed against Tree-Ring, a\nwatermarking technique for diffusion models known for its high imperceptibility\nand robustness against removal attacks. Unlike previous removal attacks, which\nrely on strong assumptions about attacker capabilities, our attack only\nrequires access to the variational autoencoder that was used to train the\ntarget diffusion model, a component that is often publicly available. By\nleveraging this variational autoencoder, the attacker can approximate the\nmodel's intermediate latent space, enabling more effective surrogate-based\nattacks. Our evaluation shows that this approach leads to a dramatic reduction\nin the AUC of Tree-Ring detector's ROC and PR curves, decreasing from 0.993 to\n0.153 and from 0.994 to 0.385, respectively, while maintaining high image\nquality. Notably, our attacks outperform existing methods that assume full\naccess to the diffusion model. These findings highlight the risk of reusing\npublic autoencoders to train diffusion models -- a threat not considered by\ncurrent industry practices. Furthermore, the results suggest that the Tree-Ring\ndetector's precision, a metric that has been overlooked by previous\nevaluations, falls short of the requirements for real-world deployment."}
{"id": "2506.10803", "categories": ["cs.SE", "cs.PL"], "pdf": "https://arxiv.org/pdf/2506.10803", "abs": "https://arxiv.org/abs/2506.10803", "authors": ["Ryan Gibb", "Patrick Ferris", "David Allsopp", "Michael Winston Dales", "Mark Elvers", "Thomas Gazagnaire", "Sadiq Jaffer", "Thomas Leonard", "Jon Ludlam", "Anil Madhavapeddy"], "title": "Solving Package Management via Hypergraph Dependency Resolution", "comment": "Submitted to SPLASH 2025", "summary": "Package managers are everywhere, with seemingly every language and operating\nsystem implementing their own solution. The lack of interoperability between\nthese systems means that multi-lingual projects are unable to express precise\ndependencies across language ecosystems, and external system and hardware\ndependencies are typically implicit and unversioned. We define HyperRes, a\nformal system for describing versioned dependency resolution using a hypergraph\nthat is expressive enough to model many ecosystems and solve dependency\nconstraints across them. We define translations from dozens of existing package\nmanagers to HyperRes and comprehensively demonstrate that dependency resolution\ncan work across ecosystems that are currently distinct. This does not require\nusers to shift their choice of package managers; instead, HyperRes allows for\nthe translation of packaging metadata between ecosystems, and for solving to be\nprecisely specialised to a particular deployment environment."}
{"id": "2506.10597", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.10597", "abs": "https://arxiv.org/abs/2506.10597", "authors": ["Xunguang Wang", "Zhenlan Ji", "Wenxuan Wang", "Zongjie Li", "Daoyuan Wu", "Shuai Wang"], "title": "SoK: Evaluating Jailbreak Guardrails for Large Language Models", "comment": null, "summary": "Large Language Models (LLMs) have achieved remarkable progress, but their\ndeployment has exposed critical vulnerabilities, particularly to jailbreak\nattacks that circumvent safety mechanisms. Guardrails--external defense\nmechanisms that monitor and control LLM interaction--have emerged as a\npromising solution. However, the current landscape of LLM guardrails is\nfragmented, lacking a unified taxonomy and comprehensive evaluation framework.\nIn this Systematization of Knowledge (SoK) paper, we present the first holistic\nanalysis of jailbreak guardrails for LLMs. We propose a novel,\nmulti-dimensional taxonomy that categorizes guardrails along six key\ndimensions, and introduce a Security-Efficiency-Utility evaluation framework to\nassess their practical effectiveness. Through extensive analysis and\nexperiments, we identify the strengths and limitations of existing guardrail\napproaches, explore their universality across attack types, and provide\ninsights into optimizing defense combinations. Our work offers a structured\nfoundation for future research and development, aiming to guide the principled\nadvancement and deployment of robust LLM guardrails. The code is available at\nhttps://github.com/xunguangwang/SoK4JailbreakGuardrails."}
{"id": "2506.10833", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2506.10833", "abs": "https://arxiv.org/abs/2506.10833", "authors": ["Fabian C. Peña", "Steffen Herbold"], "title": "Evaluating Large Language Models on Non-Code Software Engineering Tasks", "comment": null, "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities in\ncode understanding and generation; however, their effectiveness on non-code\nSoftware Engineering (SE) tasks remains underexplored. We present the first\ncomprehensive benchmark, which we name `Software Engineering Language\nUnderstanding' (SELU), for evaluating LLMs on 17 non-code tasks, spanning from\nidentifying whether a requirement is functional or non-functional to estimating\nthe effort and complexity of backlog items. SELU covers classification,\nregression, Named Entity Recognition (NER), and Masked Language Modeling (MLM)\ntargets, with data drawn from diverse sources such as code repositories, issue\ntracking systems, and developer forums. We fine-tune 22 open-source LLMs,\nprompt two proprietary alternatives, and train two baselines. Performance is\nmeasured using metrics such as F1-macro, SMAPE, F1-micro, and accuracy, and\ncompared via the Bayesian signed-rank test. Our results show that\nmoderate-scale decoder-only models consistently form a top-tier, exhibiting\nhigh mean performance and low across-task variance, while domain adaptation via\ncode-focused pre-training might yield only modest improvements. These insights\nguide model selection for non-code SE workflows and highlight directions for\nexpanding SELU to generative and design-oriented scenarios."}
{"id": "2506.10620", "categories": ["cs.CR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.10620", "abs": "https://arxiv.org/abs/2506.10620", "authors": ["Stefano Longari", "Paolo Cerracchio", "Michele Carminati", "Stefano Zanero"], "title": "Assessing the Resilience of Automotive Intrusion Detection Systems to Adversarial Manipulation", "comment": null, "summary": "The security of modern vehicles has become increasingly important, with the\ncontroller area network (CAN) bus serving as a critical communication backbone\nfor various Electronic Control Units (ECUs). The absence of robust security\nmeasures in CAN, coupled with the increasing connectivity of vehicles, makes\nthem susceptible to cyberattacks. While intrusion detection systems (IDSs) have\nbeen developed to counter such threats, they are not foolproof. Adversarial\nattacks, particularly evasion attacks, can manipulate inputs to bypass\ndetection by IDSs. This paper extends our previous work by investigating the\nfeasibility and impact of gradient-based adversarial attacks performed with\ndifferent degrees of knowledge against automotive IDSs. We consider three\nscenarios: white-box (attacker with full system knowledge), grey-box (partial\nsystem knowledge), and the more realistic black-box (no knowledge of the IDS'\ninternal workings or data). We evaluate the effectiveness of the proposed\nattacks against state-of-the-art IDSs on two publicly available datasets.\nAdditionally, we study effect of the adversarial perturbation on the attack\nimpact and evaluate real-time feasibility by precomputing evasive payloads for\ntimed injection based on bus traffic. Our results demonstrate that, besides\nattacks being challenging due to the automotive domain constraints, their\neffectiveness is strongly dependent on the dataset quality, the target IDS, and\nthe attacker's degree of knowledge."}
{"id": "2506.10869", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2506.10869", "abs": "https://arxiv.org/abs/2506.10869", "authors": ["Quinn Thibeault", "Giulia Pedrielli"], "title": "MultiCoSim: A Python-based Multi-Fidelity Co-Simulation Framework", "comment": null, "summary": "Simulation is a foundational tool for the analysis and testing of\ncyber-physical systems (CPS), underpinning activities such as algorithm\ndevelopment, runtime monitoring, and system verification. As CPS grow in\ncomplexity and scale, particularly in safety-critical and learning-enabled\nsettings, accurate analysis and synthesis increasingly rely on the rapid use of\nsimulation experiments. Because CPS inherently integrate hardware, software,\nand physical processes, simulation platforms must support co-simulation of\nheterogeneous components at varying levels of fidelity. Despite recent advances\nin high-fidelity modeling of hardware, firmware, and physics, co-simulation in\ndiverse environments remains challenging. These limitations hinder the\ndevelopment of reusable benchmarks and impede the use of simulation for\nautomated and comparative evaluation.\n  Existing simulation tools often rely on rigid configurations, lack automation\nsupport, and present obstacles to portability and modularity. Many are\nconfigured through static text files or impose constraints on how simulation\ncomponents are represented and connected, making it difficult to flexibly\ncompose systems or integrate components across platforms.\n  To address these challenges, we introduce MultiCoSim, a Python-based\nsimulation framework that enables users to define, compose, and configure\nsimulation components programmatically. MultiCoSim supports distributed,\ncomponent-based co-simulation and allows seamless substitution and\nreconfiguration of components. We demonstrate the flexibility of MultiCoSim\nthrough case studies that include co-simulations involving custom\nautomaton-based controllers, as well as integration with off-the-shelf\nplatforms like the PX4 autopilot for aerial robotics. These examples highlight\nMultiCoSim's capability to streamline CPS simulation pipelines for research and\ndevelopment."}
{"id": "2506.10638", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2506.10638", "abs": "https://arxiv.org/abs/2506.10638", "authors": ["Stefano Longari", "Alessandro Pozone", "Jessica Leoni", "Mario Polino", "Michele Carminati", "Mara Tanelli", "Stefano Zanero"], "title": "CyFence: Securing Cyber-Physical Controllers via Trusted Execution Environment", "comment": null, "summary": "In the last decades, Cyber-physical Systems (CPSs) have experienced a\nsignificant technological evolution and increased connectivity, at the cost of\ngreater exposure to cyber-attacks. Since many CPS are used in safety-critical\nsystems, such attacks entail high risks and potential safety harms. Although\nseveral defense strategies have been proposed, they rarely exploit the\ncyber-physical nature of the system. In this work, we exploit the nature of CPS\nby proposing CyFence, a novel architecture that improves the resilience of\nclosed-loop control systems against cyber-attacks by adding a semantic check,\nused to confirm that the system is behaving as expected. To ensure the security\nof the semantic check code, we use the Trusted Execution Environment\nimplemented by modern processors. We evaluate CyFence considering a real-world\napplication, consisting of an active braking digital controller, demonstrating\nthat it can mitigate different types of attacks with a negligible computation\noverhead."}
{"id": "2506.10954", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.10954", "abs": "https://arxiv.org/abs/2506.10954", "authors": ["Lianghong Guo", "Yanlin Wang", "Caihua Li", "Pengyu Yang", "Jiachi Chen", "Wei Tao", "Yingtian Zou", "Duyu Tang", "Zibin Zheng"], "title": "SWE-Factory: Your Automated Factory for Issue Resolution Training Data and Evaluation Benchmarks", "comment": null, "summary": "Constructing large-scale datasets for the GitHub issue resolution task is\ncrucial for both training and evaluating the software engineering capabilities\nof Large Language Models (LLMs). However, the traditional process for creating\nsuch benchmarks is notoriously challenging and labor-intensive, particularly in\nthe stages of setting up evaluation environments, grading test outcomes, and\nvalidating task instances. In this paper, we propose SWE-Factory, an automated\npipeline designed to address these challenges. To tackle these issues, our\npipeline integrates three core automated components. First, we introduce\nSWE-Builder, a multi-agent system that automates evaluation environment\nconstruction, which employs four specialized agents that work in a\ncollaborative, iterative loop and leverages an environment memory pool to\nenhance efficiency. Second, we introduce a standardized, exit-code-based\ngrading method that eliminates the need for manually writing custom parsers.\nFinally, we automate the fail2pass validation process using these reliable exit\ncode signals. Experiments on 671 issues across four programming languages show\nthat our pipeline can effectively construct valid task instances; for example,\nwith GPT-4.1-mini, our SWE-Builder constructs 269 valid instances at $0.045 per\ninstance, while with Gemini-2.5-flash, it achieves comparable performance at\nthe lowest cost of $0.024 per instance. We also demonstrate that our\nexit-code-based grading achieves 100% accuracy compared to manual inspection,\nand our automated fail2pass validation reaches a precision of 0.92 and a recall\nof 1.00. We hope our automated pipeline will accelerate the collection of\nlarge-scale, high-quality GitHub issue resolution datasets for both training\nand evaluation. Our code and datasets are released at\nhttps://github.com/DeepSoftwareAnalytics/swe-factory."}
{"id": "2506.10645", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2506.10645", "abs": "https://arxiv.org/abs/2506.10645", "authors": ["Aakanksha Saha", "Martina Lindorfer", "Juan Caballero"], "title": "From IOCs to Group Profiles: On the Specificity of Threat Group Behaviors in CTI Knowledge Bases", "comment": null, "summary": "Indicators of Compromise (IOCs) such as IP addresses, file hashes, and domain\nnames are commonly used for threat detection and attribution. However, IOCs\ntend to be short-lived as they are easy to change. As a result, the\ncybersecurity community is shifting focus towards more persistent behavioral\nprofiles, such as the Tactics, Techniques, and Procedures (TTPs) and the\nsoftware used by a threat group. However, the distinctiveness and completeness\nof such behavioral profiles remain largely unexplored. In this work, we\nsystematically analyze threat group profiles built from two open cyber threat\nintelligence (CTI) knowledge bases: MITRE ATT&CK and Malpedia. We first\ninvestigate what fraction of threat groups have group-specific behaviors, i.e.,\nbehaviors used exclusively by a single group. We find that only 34% of threat\ngroups in ATT&CK have group-specific techniques. The software used by a threat\ngroup proves to be more distinctive, with 73% of ATT&CK groups using\ngroup-specific software. However, this percentage drops to 24% in the broader\nMalpedia dataset. Next, we evaluate how group profiles improve when data from\nboth sources are combined. While coverage improves modestly, the proportion of\ngroups with group-specific behaviors remains under 30%. We then enhance\nprofiles by adding exploited vulnerabilities and additional techniques\nextracted from more threat reports. Despite the additional information, 64% of\ngroups still lack any group-specific behavior. Our findings raise concerns on\nthe belief that behavioral profiles can replace IOCs in threat group\nattribution."}
{"id": "2506.10104", "categories": ["cs.CR", "cs.SE"], "pdf": "https://arxiv.org/pdf/2506.10104", "abs": "https://arxiv.org/abs/2506.10104", "authors": ["David Farr", "Kevin Talty", "Alexandra Farr", "John Stockdale", "Iain Cruickshank", "Jevin West"], "title": "Expert-in-the-Loop Systems with Cross-Domain and In-Domain Few-Shot Learning for Software Vulnerability Detection", "comment": null, "summary": "As cyber threats become more sophisticated, rapid and accurate vulnerability\ndetection is essential for maintaining secure systems. This study explores the\nuse of Large Language Models (LLMs) in software vulnerability assessment by\nsimulating the identification of Python code with known Common Weakness\nEnumerations (CWEs), comparing zero-shot, few-shot cross-domain, and few-shot\nin-domain prompting strategies. Our results indicate that while zero-shot\nprompting performs poorly, few-shot prompting significantly enhances\nclassification performance, particularly when integrated with confidence-based\nrouting strategies that improve efficiency by directing human experts to cases\nwhere model uncertainty is high, optimizing the balance between automation and\nexpert oversight. We find that LLMs can effectively generalize across\nvulnerability categories with minimal examples, suggesting their potential as\nscalable, adaptable cybersecurity tools in simulated environments. However,\nchallenges such as model reliability, interpretability, and adversarial\nrobustness remain critical areas for future research. By integrating AI-driven\napproaches with expert-in-the-loop (EITL) decision-making, this work highlights\na pathway toward more efficient and responsive cybersecurity workflows. Our\nfindings provide a foundation for deploying AI-assisted vulnerability detection\nsystems in both real and simulated environments that enhance operational\nresilience while reducing the burden on human analysts."}
{"id": "2506.10665", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2506.10665", "abs": "https://arxiv.org/abs/2506.10665", "authors": ["Davide Maffiola", "Stefano Longari", "Michele Carminati", "Mara Tanelli", "Stefano Zanero"], "title": "GOLIATH: A Decentralized Framework for Data Collection in Intelligent Transportation Systems", "comment": null, "summary": "Intelligent Transportation Systems (ITSs) technology has advanced during the\npast years, and it is now used for several applications that require vehicles\nto exchange real-time data, such as in traffic information management.\nTraditionally, road traffic information has been collected using on-site\nsensors. However, crowd-sourcing traffic information from onboard sensors or\nsmartphones has become a viable alternative. State-of-the-art solutions\ncurrently follow a centralized model where only the service provider has\ncomplete access to the collected traffic data and represent a single point of\nfailure and trust. In this paper, we propose GOLIATH, a blockchain-based\ndecentralized framework that runs on the In-Vehicle Infotainment (IVI) system\nto collect real-time information exchanged between the network's participants.\nOur approach mitigates the limitations of existing crowd-sourcing centralized\nsolutions by guaranteeing trusted information collection and exchange, fully\nexploiting the intrinsic distributed nature of vehicles. We demonstrate its\nfeasibility in the context of vehicle positioning and traffic information\nmanagement. Each vehicle participating in the decentralized network shares its\nposition and neighbors' ones in the form of a transaction recorded on the\nledger, which uses a novel consensus mechanism to validate it. We design the\nconsensus mechanism resilient against a realistic set of adversaries that aim\nto tamper or disable the communication. We evaluate the proposed framework in a\nsimulated (but realistic) environment, which considers different threats and\nallows showing its robustness and safety properties."}
{"id": "2506.10125", "categories": ["cs.CR", "cs.SE"], "pdf": "https://arxiv.org/pdf/2506.10125", "abs": "https://arxiv.org/abs/2506.10125", "authors": ["Muqi Zou", "Hongyu Cai", "Hongwei Wu", "Zion Leonahenahe Basque", "Arslan Khan", "Berkay Celik", "Dave", "Tian", "Antonio Bianchi", "Ruoyu", "Wang", "Dongyan Xu"], "title": "D-LiFT: Improving LLM-based Decompiler Backend via Code Quality-driven Fine-tuning", "comment": null, "summary": "Decompilers, which reconstruct human-readable source code from binary\nexecutables, are vital to many security tasks. Yet, despite recent advances,\ntheir output often suffers from syntactic and semantic errors and remains\ndifficult to read. Recently, with the advent of large language models (LLMs),\nresearchers began to explore the potential of LLMs to refine decompiler output.\nNevertheless, our study of these approaches reveals significant limitations,\nsuch as introducing new errors and relying on unreliable accuracy validation.\nIn this paper, we present D-LiFT, an automated decompiler backend that\nharnesses and further trains LLMs to improve the quality of decompiled code via\nreinforcement learning (RL). Unlike prior work that overlooks preserving\naccuracy, D-LiFT adheres to a key principle for enhancing the quality of\ndecompiled code: \\textit{preserving accuracy while improving readability}.\nCentral to D-LiFT, we propose D-SCORE, an integrated quality assessment system\nto score the decompiled code from multiple aspects. In line with our principle,\nD-SCORE assigns low scores to any inaccurate output and only awards higher\nscores for readability to code that passes the accuracy check. Specifically,\nD-SCORE first verifies the syntactic and semantic correctness via the compiler\nand symbolic execution; only if a candidate is deemed accurate, it then\nevaluates readability using established metrics to compare the LLM output with\nthe original decompiled code. The score will then be fed back to the LLM for\nfine-tuning. Our implementation, based on Ghidra and a range of LLMs,\ndemonstrates significant improvements for the accurate decompiled code from the\ncoreutils and util-linux projects. Compared to baseline LLMs without\nD-SCORE-driven fine-tuning, D-LiFT produces 55.3% more improved decompiled\nfunctions, as measured by D-SCORE."}
{"id": "2506.10721", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2506.10721", "abs": "https://arxiv.org/abs/2506.10721", "authors": ["Ioan Ionescu", "Ruxandra F. Olimid"], "title": "Commitment Schemes for Multi-Party Computation", "comment": null, "summary": "The paper presents an analysis of Commitment Schemes (CSs) used in\nMulti-Party Computation (MPC) protocols. While the individual properties of CSs\nand the guarantees offered by MPC have been widely studied in isolation, their\ninterrelation in concrete protocols and applications remains mostly\nunderexplored. This paper presents the relation between the two, with an\nemphasis on (security) properties and their impact on the upper layer MPC. In\nparticular, we investigate how different types of CSs contribute to various MPC\nconstructions and their relation to real-life applications of MPC. The paper\ncan also serve as a tutorial for understanding the cryptographic interplay\nbetween CS and MPC, making it accessible to both researchers and practitioners.\nOur findings emphasize the importance of carefully selecting CS to meet the\nadversarial and functional requirements of MPC, thereby aiming for more robust\nand privacy-preserving cryptographic applications"}
{"id": "2506.10323", "categories": ["cs.CR", "cs.SE"], "pdf": "https://arxiv.org/pdf/2506.10323", "abs": "https://arxiv.org/abs/2506.10323", "authors": ["Chuyang Chen", "Brendan Dolan-Gavitt", "Zhiqiang Lin"], "title": "ELFuzz: Efficient Input Generation via LLM-driven Synthesis Over Fuzzer Space", "comment": "Accepted by USENIX Security'25 Cycle 2", "summary": "Generation-based fuzzing produces appropriate testing cases according to\nspecifications of input grammars and semantic constraints to test systems and\nsoftware. However, these specifications require significant manual efforts to\nconstruct. This paper proposes a new approach, ELFuzz (Evolution Through Large\nLanguage Models for Fuzzing), that automatically synthesizes generation-based\nfuzzers tailored to a system under test (SUT) via LLM-driven synthesis over\nfuzzer space. At a high level, it starts with minimal seed fuzzers and propels\nthe synthesis by fully automated LLM-driven evolution with coverage guidance.\nCompared to previous approaches, ELFuzz can 1) seamlessly scale to SUTs of\nreal-world sizes -- up to 1,791,104 lines of code in our evaluation -- and 2)\nsynthesize efficient fuzzers that catch interesting grammatical structures and\nsemantic constraints in a human-understandable way. Our evaluation compared\nELFuzz with specifications manually written by domain experts and synthesized\nby state-of-the-art approaches. It shows that ELFuzz achieves up to 434.8% more\ncoverage and triggers up to 174.0% more artificially injected bugs. We also\nused ELFuzz to conduct a real-world fuzzing campaign on the newest version of\ncvc5 for 14 days, and encouragingly, it found five 0-day bugs (three are\nexploitable). Moreover, we conducted an ablation study, which shows that the\nfuzzer space model, the key component of ELFuzz, contributes the most (up to\n62.5%) to the effectiveness of ELFuzz. Further analysis of the fuzzers\nsynthesized by ELFuzz confirms that they catch interesting grammatical\nstructures and semantic constraints in a human-understandable way. The results\npresent the promising potential of ELFuzz for more automated, efficient, and\nextensible input generation for fuzzing."}
{"id": "2506.10722", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.10722", "abs": "https://arxiv.org/abs/2506.10722", "authors": ["Xiaoxing Mo", "Yuxuan Cheng", "Nan Sun", "Leo Yu Zhang", "Wei Luo", "Shang Gao"], "title": "TED-LaST: Towards Robust Backdoor Defense Against Adaptive Attacks", "comment": null, "summary": "Deep Neural Networks (DNNs) are vulnerable to backdoor attacks, where\nattackers implant hidden triggers during training to maliciously control model\nbehavior. Topological Evolution Dynamics (TED) has recently emerged as a\npowerful tool for detecting backdoor attacks in DNNs. However, TED can be\nvulnerable to backdoor attacks that adaptively distort topological\nrepresentation distributions across network layers. To address this limitation,\nwe propose TED-LaST (Topological Evolution Dynamics against Laundry, Slow\nrelease, and Target mapping attack strategies), a novel defense strategy that\nenhances TED's robustness against adaptive attacks. TED-LaST introduces two key\ninnovations: label-supervised dynamics tracking and adaptive layer emphasis.\nThese enhancements enable the identification of stealthy threats that evade\ntraditional TED-based defenses, even in cases of inseparability in topological\nspace and subtle topological perturbations. We review and classify data\npoisoning tricks in state-of-the-art adaptive attacks and propose enhanced\nadaptive attack with target mapping, which can dynamically shift malicious\ntasks and fully leverage the stealthiness that adaptive attacks possess. Our\ncomprehensive experiments on multiple datasets (CIFAR-10, GTSRB, and\nImageNet100) and model architectures (ResNet20, ResNet101) show that TED-LaST\neffectively counteracts sophisticated backdoors like Adap-Blend, Adapt-Patch,\nand the proposed enhanced adaptive attack. TED-LaST sets a new benchmark for\nrobust backdoor detection, substantially enhancing DNN security against\nevolving threats."}
{"id": "2506.10744", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2506.10744", "abs": "https://arxiv.org/abs/2506.10744", "authors": ["Xiaobei Yan", "Han Qiu", "Tianwei Zhang"], "title": "ObfusBFA: A Holistic Approach to Safeguarding DNNs from Different Types of Bit-Flip Attacks", "comment": null, "summary": "Bit-flip attacks (BFAs) represent a serious threat to Deep Neural Networks\n(DNNs), where flipping a small number of bits in the model parameters or binary\ncode can significantly degrade the model accuracy or mislead the model\nprediction in a desired way. Existing defenses exclusively focus on protecting\nmodels for specific attacks and platforms, while lacking effectiveness for\nother scenarios. We propose ObfusBFA, an efficient and holistic methodology to\nmitigate BFAs targeting both the high-level model weights and low-level\ncodebase (executables or shared libraries). The key idea of ObfusBFA is to\nintroduce random dummy operations during the model inference, which effectively\ntransforms the delicate attacks into random bit flips, making it much harder\nfor attackers to pinpoint and exploit vulnerable bits. We design novel\nalgorithms to identify critical bits and insert obfuscation operations. We\nevaluate ObfusBFA against different types of attacks, including the adaptive\nscenarios where the attacker increases the flip bit budget to attempt to\ncircumvent our defense. The results show that ObfusBFA can consistently\npreserve the model accuracy across various datasets and DNN architectures while\nsignificantly reducing the attack success rates. Additionally, it introduces\nminimal latency and storage overhead, making it a practical solution for\nreal-world applications."}
{"id": "2506.10755", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2506.10755", "abs": "https://arxiv.org/abs/2506.10755", "authors": ["Christophe Parisel"], "title": "Quantifying Azure RBAC Wildcard Overreach", "comment": null, "summary": "Azure RBAC leverages wildcard permissions to simplify policy authoring, but\nthis abstraction often obscures the actual set of allowed operations and\nundermines least-privilege guarantees. We introduce Belshazaar, a two-stage\nframework that targets both the effective permission set problem and the\nevaluation of wildcards permissions spread. First, we formalize Azure action\nsyntax via a context free grammar and implement a compiler that expands any\nwildcard into its explicit action set. Second, we define an ultrametric\ndiameter metric to quantify semantic overreach in wildcard scenarios. Applied\nto Microsoft s official catalog of 15481 actions, Belshazaar reveals that about\n39 percent of actions admit a cross Resource Provider reach when associated\nwith non obvious wildcards, and that effective permissions sets are effectively\ncomputable. These findings demonstrate that wildcard patterns can introduce\nsubstantial privilege bloat, and that our approach offers a scalable, semantics\ndriven path toward tighter, least-privilege RBAC policies in Azure\nenvironments."}
{"id": "2506.10776", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.10776", "abs": "https://arxiv.org/abs/2506.10776", "authors": ["Feiyu Yang", "Siyuan Liang", "Aishan Liu", "Dacheng Tao"], "title": "ME: Trigger Element Combination Backdoor Attack on Copyright Infringement", "comment": null, "summary": "The capability of generative diffusion models (DMs) like Stable Diffusion\n(SD) in replicating training data could be taken advantage of by attackers to\nlaunch the Copyright Infringement Attack, with duplicated poisoned image-text\npairs. SilentBadDiffusion (SBD) is a method proposed recently, which shew\noutstanding performance in attacking SD in text-to-image tasks. However, the\nfeasible data resources in this area are still limited, some of them are even\nconstrained or prohibited due to the issues like copyright ownership or\ninappropriate contents; And not all of the images in current datasets are\nsuitable for the proposed attacking methods; Besides, the state-of-the-art\n(SoTA) performance of SBD is far from ideal when few generated poisoning\nsamples could be adopted for attacks. In this paper, we raised new datasets\naccessible for researching in attacks like SBD, and proposed Multi-Element (ME)\nattack method based on SBD by increasing the number of poisonous visual-text\nelements per poisoned sample to enhance the ability of attacking, while\nimporting Discrete Cosine Transform (DCT) for the poisoned samples to maintain\nthe stealthiness. The Copyright Infringement Rate (CIR) / First Attack Epoch\n(FAE) we got on the two new datasets were 16.78% / 39.50 and 51.20% / 23.60,\nrespectively close to or even outperformed benchmark Pokemon and Mijourney\ndatasets. In condition of low subsampling ratio (5%, 6 poisoned samples), MESI\nand DCT earned CIR / FAE of 0.23% / 84.00 and 12.73% / 65.50, both better than\noriginal SBD, which failed to attack at all."}
{"id": "2506.10949", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.10949", "abs": "https://arxiv.org/abs/2506.10949", "authors": ["Chen Yueh-Han", "Nitish Joshi", "Yulin Chen", "Maksym Andriushchenko", "Rico Angell", "He He"], "title": "Monitoring Decomposition Attacks in LLMs with Lightweight Sequential Monitors", "comment": null, "summary": "Current LLM safety defenses fail under decomposition attacks, where a\nmalicious goal is decomposed into benign subtasks that circumvent refusals. The\nchallenge lies in the existing shallow safety alignment techniques: they only\ndetect harm in the immediate prompt and do not reason about long-range intent,\nleaving them blind to malicious intent that emerges over a sequence of\nseemingly benign instructions. We therefore propose adding an external monitor\nthat observes the conversation at a higher granularity. To facilitate our study\nof monitoring decomposition attacks, we curate the largest and most diverse\ndataset to date, including question-answering, text-to-image, and agentic\ntasks. We verify our datasets by testing them on frontier LLMs and show an 87%\nattack success rate on average on GPT-4o. This confirms that decomposition\nattack is broadly effective. Additionally, we find that random tasks can be\ninjected into the decomposed subtasks to further obfuscate malicious intents.\nTo defend in real time, we propose a lightweight sequential monitoring\nframework that cumulatively evaluates each subtask. We show that a carefully\nprompt engineered lightweight monitor achieves a 93% defense success rate,\nbeating reasoning models like o3 mini as a monitor. Moreover, it remains robust\nagainst random task injection and cuts cost by 90% and latency by 50%. Our\nfindings suggest that lightweight sequential monitors are highly effective in\nmitigating decomposition attacks and are viable in deployment."}
{"id": "2506.10280", "categories": ["cs.SE", "cs.CR"], "pdf": "https://arxiv.org/pdf/2506.10280", "abs": "https://arxiv.org/abs/2506.10280", "authors": ["Samiha Shimmi", "Hamed Okhravi", "Mona Rahimi"], "title": "AI-Based Software Vulnerability Detection: A Systematic Literature Review", "comment": null, "summary": "Software vulnerabilities in source code pose serious cybersecurity risks,\nprompting a shift from traditional detection methods (e.g., static analysis,\nrule-based matching) to AI-driven approaches. This study presents a systematic\nreview of software vulnerability detection (SVD) research from 2018 to 2023,\noffering a comprehensive taxonomy of techniques, feature representations, and\nembedding methods. Our analysis reveals that 91% of studies use AI-based\nmethods, with graph-based models being the most prevalent. We identify key\nlimitations, including dataset quality, reproducibility, and interpretability,\nand highlight emerging opportunities in underexplored techniques such as\nfederated learning and quantum neural networks, providing a roadmap for future\nresearch."}
