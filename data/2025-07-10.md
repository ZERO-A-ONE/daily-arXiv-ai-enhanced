<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 9]
- [cs.CR](#cs.CR) [Total: 29]
- [cs.AI](#cs.AI) [Total: 8]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [Quality attributes of test cases and test suites -- importance & challenges from practitioners' perspectives](https://arxiv.org/abs/2507.06343)
*Huynh Khanh Vi Tran,Nauman bin Ali,Michael Unterkalmsteiner,Jürgen Börstler,Panagiota Chatzipetrou*

Main category: cs.SE

TL;DR: 研究调查了从业者对测试用例和测试套件质量属性的重要性认知及实践中的挑战，发现故障检测、可用性、可维护性、可靠性和覆盖率是最重要的属性。


<details>
  <summary>Details</summary>
Motivation: 理解从业者对测试用例和测试套件质量属性的重要性认知及实践中的挑战，以提供更好的支持。

Method: 通过基于文献综述的问卷进行工业调查，利用LinkedIn抽样策略获取多样化的从业者样本。

Result: 354份回复显示，故障检测等属性最重要，资源效率等属性意见分歧较大；常见挑战包括定义不足、缺乏指标等。

Conclusion: 研究结果可为学术研究方向提供指导，并鼓励企业为从业者提供更多支持。

Abstract: Context: The quality of the test suites and the constituent test cases
significantly impacts confidence in software testing. While research has
identified several quality attributes of test cases and test suites, there is a
need for a better understanding of their relative importance in practice.
Objective: We investigate practitioners' perceptions regarding the relative
importance of quality attributes of test cases and test suites and the
challenges they face in ensuring the perceived important quality attributes.
Method: We conducted an industrial survey using a questionnaire based on the
quality attributes identified in an extensive literature review. We used a
sampling strategy that leverages LinkedIn to draw a large and heterogeneous
sample of professionals with experience in software testing. Results: We
collected 354 responses from practitioners with a wide range of experience. We
found that the majority of practitioners rated Fault Detection, Usability,
Maintainability, Reliability, and Coverage to be the most important quality
attributes. Resource Efficiency, Reusability, and Simplicity received the most
divergent opinions, which, according to our analysis, depend on the
software-testing contexts. We identified common challenges that apply to the
important attributes, namely inadequate definition, lack of useful metrics,
lack of an established review process, and lack of external support.
Conclusion: The findings point out where practitioners actually need further
support with respect to achieving high-quality test cases and test suites under
different software testing contexts. The findings can serve as a guideline for
academic researchers when looking for research directions on the topic. The
findings can also be used to encourage companies to provide more support to
practitioners to achieve high-quality test cases and test suites.

</details>


### [2] [A proposal and assessment of an improved heuristic for the Eager Test smell detection](https://arxiv.org/abs/2507.06354)
*Huynh Khanh Vi Tran,Nauman bin Ali,Michael Unterkalmsteiner,Jürgen Börstler*

Main category: cs.SE

TL;DR: 论文提出了一种改进的Eager Test气味检测规则，通过文献综述和新启发式方法，解决了现有规则的不精确问题。


<details>
  <summary>Details</summary>
Motivation: 现有Eager Test气味检测工具依赖简化的规则，导致检测结果不精确，无法满足实践需求。

Method: 通过文献综述分析Eager Test定义和检测规则，提出新的明确定义和启发式方法，并在300个Java单元测试案例中手动验证。

Result: 发现现有规则因定义不明确导致检测结果不一致，新启发式方法能更精确识别Eager Test和非Eager Test模式。

Conclusion: 新启发式方法更准确地捕捉Eager Test气味的本质，有望解决实践者对现有规则不足的担忧。

Abstract: Context: The evidence for the prevalence of test smells at the unit testing
level has relied on the accuracy of detection tools, which have seen intense
research in the last two decades. The Eager Test smell, one of the most
prevalent, is often identified using simplified detection rules that
practitioners find inadequate. Objective: We aim to improve the rules for
detecting the Eager Test smell. Method: We reviewed the literature on test
smells to analyze the definitions and detection rules of the Eager Test smell.
We proposed a novel, unambiguous definition of the test smell and a heuristic
to address the limitations of the existing rules. We evaluated our heuristic
against existing detection rules by manually applying it to 300 unit test cases
in Java. Results: Our review identified 56 relevant studies. We found that
inadequate interpretations of original definitions of the Eager Test smell led
to imprecise detection rules, resulting in a high level of disagreement in
detection outcomes. Also, our heuristic detected patterns of eager and
non-eager tests that existing rules missed. Conclusion: Our heuristic captures
the essence of the Eager Test smell more precisely; hence, it may address
practitioners' concerns regarding the adequacy of existing detection rules.

</details>


### [3] [Evaluating Efficiency and Novelty of LLM-Generated Code for Graph Analysis](https://arxiv.org/abs/2507.06463)
*Atieh Barati Nia,Mohammad Dindoost,David A. Bader*

Main category: cs.SE

TL;DR: 研究了大型语言模型（LLMs）在生成高效C语言图分析代码的能力，发现Claude Sonnet 4 Extended表现最佳，但LLMs更擅长优化而非创新。


<details>
  <summary>Details</summary>
Motivation: 评估LLMs在满足严格运行时和内存约束条件下生成高效C代码的能力，填补了现有研究在高性能代码生成领域的空白。

Method: 采用两种方法：1）评估LLMs生成超越现有算法的代码能力；2）评估LLMs生成可集成到基准测试中的图算法能力。

Result: Claude Sonnet 4 Extended在生成即用代码和效率方面表现最佳，甚至超越人工编写的基线。

Conclusion: 当代LLMs擅长优化和集成现有算法，但在发明新技术方面仍有不足。

Abstract: Large Language Models (LLMs) are increasingly used to automate software
development, yet most prior evaluations focus on functional correctness or
high-level languages such as Python. We present the first systematic study of
LLMs' ability to generate efficient C implementations of graph-analysis
routines--code that must satisfy the stringent runtime and memory constraints.
Eight state-of-the-art models (OpenAI ChatGPT o3 and o4-mini-high, Anthropic
Claude 4 Sonnet and Sonnet Extended, Google Gemini 2.5 Flash and Pro, xAI Grok
3-Think, and DeepSeek DeepThink R1) are benchmarked by two distinct approaches.
The first approach checks the ability of LLMs in generating an algorithm
outperforming other present algorithms in the benchmark. The second approach
evaluates the ability of LLMs to generate graph algorithms for integration into
the benchmark. Results show that Claude Sonnet 4 Extended achieves the best
result in the case of ready-to-use code generation and efficiency,
outperforming human-written baselines in triangle counting. The study confirms
that contemporary LLMs excel at optimizing and integrating established
algorithms but not inventing novel techniques. We provide prompts, the first
approach's generated code, and measurement scripts to foster reproducible
research.

</details>


### [4] [Issue Tracking Ecosystems: Context and Best Practices](https://arxiv.org/abs/2507.06704)
*Lloyd Montgomery*

Main category: cs.SE

TL;DR: 该论文探讨了问题跟踪生态系统（ITE）的复杂性，提出了一个最佳实践本体以解决研究和实践中的不一致性。


<details>
  <summary>Details</summary>
Motivation: 问题跟踪系统（ITS）在软件工程中广泛使用，但其生态系统（ITE）的复杂性和多样性尚未充分研究，需要更深入的探索和解决方案。

Method: 通过访谈从业者和对多种ITS进行档案分析，揭示了ITE问题的上下文依赖性，并开发了最佳实践本体。

Result: 研究发现ITE问题具有高度上下文依赖性，现有解决方案缺乏一致性和可比性，提出了最佳实践本体以解决这一问题。

Conclusion: 论文强调了ITE研究的上下文重要性，并提出了一个本体框架以促进研究和实践中的一致性。

Abstract: Issue Tracking Systems (ITSs), such as GitHub and Jira, are popular tools
that support Software Engineering (SE) organisations through the management of
``issues'', which represent different SE artefacts such as requirements,
development tasks, and maintenance items. ITSs also support internal linking
between issues, and external linking to other tools and information sources.
This provides SE organisations key forms of documentation, including forwards
and backwards traceability (e.g., Feature Requests linked to sprint releases
and code commits linked to Bug Reports). An Issue Tracking Ecosystem (ITE) is
the aggregate of the central ITS and the related SE artefacts, stakeholders,
and processes -- with an emphasis on how these contextual factors interact with
the ITS. The quality of ITEs is central to the success of these organisations
and their software products. There are challenges, however, within ITEs,
including complex networks of interlinked artefacts and diverse workflows.
While ITSs have been the subject of study in SE research for decades, ITEs as a
whole need further exploration.
  In this thesis, I undertake the challenge of understanding ITEs at a broader
level, addressing these questions regarding complexity and diversity. I
interviewed practitioners and performed archival analysis on a diverse set of
ITSs. These analyses revealed the context-dependent nature of ITE problems,
highlighting the need for context-specific ITE research. While previous work
has produced many solutions to specific ITS problems, these solutions are not
consistently framed in a context-rich and comparable way, leading to a desire
for more aligned solutions across research and practice. To address this
emergent information and lack of alignment, I created the Best Practice
Ontology for ITEs. <... truncated due to arXiv abstract character limit ...>

</details>


### [5] [Leveraging LLMs for Semantic Conflict Detection via Unit Test Generation](https://arxiv.org/abs/2507.06762)
*Nathalia Barbosa,Paulo Borba,Léuson Da Silva*

Main category: cs.SE

TL;DR: 论文提出了一种基于Code Llama 70B的新测试生成工具，用于改进SMAT在检测语义冲突时的性能，尽管在复杂场景中仍存在挑战。


<details>
  <summary>Details</summary>
Motivation: 传统合并工具无法检测语义冲突，SMAT虽有效但存在高假阴性率，研究探索LLM是否能克服现有测试生成工具的局限性。

Method: 集成基于Code Llama 70B的测试生成工具到SMAT，评估不同交互策略、提示内容和参数配置对测试生成的影响。

Result: LLM在复杂场景中生成测试仍具挑战性且计算成本高，但显示出改进语义冲突检测的潜力。

Conclusion: LLM在语义冲突检测中具有潜力，但需进一步优化以应对复杂性和计算成本问题。

Abstract: Semantic conflicts arise when a developer introduces changes to a codebase
that unintentionally affect the behavior of changes integrated in parallel by
other developers. Traditional merge tools are unable to detect such conflicts,
so complementary tools like SMAT have been proposed. SMAT relies on generating
and executing unit tests: if a test fails on the base version, passes on a
developer's modified version, but fails again after merging with another
developer's changes, a semantic conflict is indicated. While SMAT is effective
at detecting conflicts, it suffers from a high rate of false negatives, partly
due to the limitations of unit test generation tools such as Randoop and
Evosuite. To investigate whether large language models (LLMs) can overcome
these limitations, we propose and integrate a new test generation tool based on
Code Llama 70B into SMAT. We explore the model's ability to generate tests
using different interaction strategies, prompt contents, and parameter
configurations. Our evaluation uses two samples: a benchmark with simpler
systems from related work, and a more significant sample based on complex,
real-world systems. We assess the effectiveness of the new SMAT extension in
detecting conflicts. Results indicate that, although LLM-based test generation
remains challenging and computationally expensive in complex scenarios, there
is promising potential for improving semantic conflict detection.
  --
  Conflitos sem^anticos surgem quando um desenvolvedor introduz mudan\c{c}as em
uma base de c\'odigo que afetam, de forma n~ao intencional, o comportamento de
altera\c{c}~oes integradas em paralelo por outros desenvolvedores. Ferramentas
tradicionais de merge n~ao conseguem detectar esse tipo de conflito, por isso
ferramentas complementares como o SMAT foram propostas. O SMAT depende da
gera\c{c}~ao e execu\c{c}~ao de testes de unidade: se um teste falha na vers~ao
base, passa na vers~ao modificada por um desenvolvedor, mas volta a falhar
ap\'os o merge com as mudan\c{c}as de outro desenvolvedor, um conflito
sem^antico \'e identificado. Embora o SMAT seja eficaz na detec\c{c}~ao de
conflitos, apresenta alta taxa de falsos negativos, em parte devido \`as
limita\c{c}~oes das ferramentas de gera\c{c}~ao de testes como Randoop e
Evosuite. Para investigar se modelos de linguagem de grande porte (LLMs) podem
superar essas limita\c{c}~oes, propomos e integramos ao SMAT uma nova
ferramenta de gera\c{c}~ao de testes baseada no Code Llama 70B. Exploramos a
capacidade do modelo de gerar testes utilizando diferentes estrat\'egias de
intera\c{c}~ao, conte\'udos de prompts e configura\c{c}~oes de par^ametros.
Nossa avalia\c{c}~ao utiliza duas amostras: um benchmark com sistemas mais
simples, usados em trabalhos relacionados, e uma amostra mais significativa
baseada em sistemas complexos e reais. Avaliamos a efic\'acia da nova extens~ao
do SMAT na detec\c{c}~ao de conflitos. Os resultados indicam que, embora a
gera\c{c}~ao de testes por LLM em cen\'arios complexos ainda seja desafiadora e
custosa computacionalmente, h\'a potencial promissor para aprimorar a
detec\c{c}~ao de conflitos sem^anticos.

</details>


### [6] [Formalization of the AADL Run-Time Services with Time](https://arxiv.org/abs/2507.06881)
*Brian R Larson,Ehsan Ahmad*

Main category: cs.SE

TL;DR: 本文扩展并简化了AADL的形式化语义，引入时间建模，并扩展了RTS以支持BA和BLESS语言。


<details>
  <summary>Details</summary>
Motivation: AADL标准缺乏对时间的明确建模，且RTS需支持更多行为语言。

Method: 使用Kripke结构的模态逻辑扩展AADL形式化语义，并扩展RTS以支持BA和BLESS。

Result: 实现了包含时间的AADL RTS，并通过HAMR工具展示了BLESS行为的状态机示例。

Conclusion: 扩展的AADL形式化语义和RTS为嵌入式系统提供了更全面的建模支持。

Abstract: The Architecture Analysis & Design Language (AADL) is an architecture
description language for design of cyber-physical systems--machines controlled
by software. The AADL standard, SAE International AS5506D, describes Run-Time
Services (RTS) to be provided to execute AADL models in accordance with
semantics defined by the standard. The RTS of primary concern are transport
services and timing services. Although, the study presented in [1] sets a
foundation for the formal semantics of AADL, but without modeling time. This
paper extends and simplifies this formalization using a modal logic defined by
a Kripke structure, to explicitly include time. The RTS defined in the AADL
standard are also expanded to support reactive state-transition machines of the
Behavior Specification annex standard language (BA) and its closely-related,
formally-defined counterpart, the Behavior Language for Embedded Systems with
Software (BLESS). An example of AADL RTS with time, implemented by the High
Assurance Modeling and Rapid Engineering for Embedded Systems (HAMR) for
state-transition machine behavior written in BLESS, is also presented.

</details>


### [7] [Are They All Good? Evaluating the Quality of CoTs in LLM-based Code Generation](https://arxiv.org/abs/2507.06980)
*Binquan Zhang,Li Zhang,Zhiwen Luo,Yuxin Du,Fang Liu,Song Wang,Lin Shi*

Main category: cs.SE

TL;DR: 论文探讨了大型语言模型（LLMs）在代码生成中链式思维（CoT）提示的质量问题，分析了影响CoT质量的内外部因素，并提出了改进方法。


<details>
  <summary>Details</summary>
Motivation: 研究动机是了解LLMs生成的CoT质量及其对代码生成的影响，以提升LLMs的可靠性和推理能力。

Method: 通过分析1,023个失败的代码样本和210个CoT-代码对，评估CoT质量，并通过提示LLMs改进低质量CoT。

Result: 研究发现外部因素（如需求不清晰）和内部因素（如LLMs误解提示）是主要问题；即使CoT正确，代码仍可能出错；改进CoT可提升性能。

Conclusion: 研究揭示了CoT在代码生成中的关键挑战，为改进LLM推理和可靠性提供了方向。

Abstract: Large language models (LLMs) have demonstrated impressive performance in code
generation, particularly when augmented with chain-of-thought (CoT) prompting
techniques. They break down requirements into intermediate reasoning steps,
which act as design rationales to guide LLMs in writing code like human
programmers. Thus, the quality of these steps is crucial for ensuring the
correctness and reliability of the generated code. However, little is known
about the quality of CoT generated by LLMs. To what extent can we trust the
thoughts generated by LLMs? How good are they? This paper empirically explores
the external and internal factors of why LLMs generate unsatisfactory CoTs by
analyzing 1,023 failed code samples on two widely used code generation
benchmarks. We also evaluate their impact on code generation performance by
analyzing 210 CoT-code pairs and refining the unsatisfied CoTs by prompting
LLMs. Our study reveals three key findings: (1) External factors (53.60%), such
as unclear requirements and lack of context, mainly affect CoT quality, while
internal factors (40.10%) stem from LLMs' misunderstanding prompts. (2) Even
when CoTs are correct, 18.5% of the generated code contains errors due to
instruction-following issues; conversely, 11.90% of correct code is paired with
flawed CoTs. (3) Refining low-quality CoTs is feasible, i.e., LLMs improve when
given detailed problem descriptions. These findings highlight key challenges in
CoT-based code generation and suggest directions for improving LLM reasoning
and reliability.

</details>


### [8] [Exploring Fairness Interventions in Open Source Projects](https://arxiv.org/abs/2507.07026)
*Sadia Afrin Mim,Fatema Tuz Zohra,Justin Smith,Brittany Johnson*

Main category: cs.SE

TL;DR: 论文分析了62种开源公平性干预措施，发现32%在过去一年内积极维护，50%提供偏差检测和缓解功能。


<details>
  <summary>Details</summary>
Motivation: 机器学习模型在关键领域的偏见问题需要解决，但公平性干预措施的实际应用有限。

Method: 系统识别并分析62种开源公平性干预措施，评估其维护状态和功能。

Result: 32%的干预措施在过去一年内积极维护，50%提供偏差检测和缓解功能。

Conclusion: 开源公平性干预措施存在但应用不足，需提高实践者对其的认知和使用。

Abstract: The deployment of biased machine learning (ML) models has resulted in adverse
effects in crucial sectors such as criminal justice and healthcare. To address
these challenges, a diverse range of machine learning fairness interventions
have been developed, aiming to mitigate bias and promote the creation of more
equitable models. Despite the growing availability of these interventions,
their adoption in real-world applications remains limited, with many
practitioners unaware of their existence. To address this gap, we
systematically identified and compiled a dataset of 62 open source fairness
interventions and identified active ones. We conducted an in-depth analysis of
their specifications and features to uncover considerations that may drive
practitioner preference and to identify the software interventions actively
maintained in the open source ecosystem. Our findings indicate that 32% of
these interventions have been actively maintained within the past year, and 50%
of them offer both bias detection and mitigation capabilities, mostly during
inprocessing.

</details>


### [9] [5C Prompt Contracts: A Minimalist, Creative-Friendly, Token-Efficient Design Framework for Individual and SME LLM Usage](https://arxiv.org/abs/2507.07045)
*Ugur Ari*

Main category: cs.SE

TL;DR: 论文提出了一种名为5C Prompt Contract的框架，通过五个直观组件（Character, Cause, Constraint, Contingency, Calibration）简化提示设计，提高输入效率并保持输出质量。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型（LLMs）在关键任务中的应用增多，需要一种明确、系统且实用的提示设计框架，以降低认知负担并保持模型的创造力。

Method: 提出5C Prompt Contract框架，包含五个组件：Character, Cause, Constraint, Contingency, Calibration，旨在优化提示设计。

Result: 实验表明，5C框架在多种LLM架构（如OpenAI、Anthropic等）中均能高效输入并保持输出的一致性和丰富性。

Conclusion: 5C框架特别适合资源有限的中小企业和个人使用，能够实现可靠、可解释且灵活的AI交互。

Abstract: The progression from traditional prompt engineering to a more rigorous
discipline of prompt design marks a pivotal shift in human-LLM interaction. As
Large Language Models (LLMs) become increasingly embedded in mission-critical
applications, there emerges a pressing need for frameworks that are not only
explicit and systematic but also minimal enough to remain practical and broadly
accessible. While many existing approaches address prompt structuring through
elaborate Domain-Specific Languages (DSLs) or multi-layered templates, such
methods can impose significant token and cognitive overhead, potentially
constraining the model's creative capacity. In this context, we propose the 5C
Prompt Contract, a framework that distills prompt design into five intuitive
components: Character, Cause, Constraint, Contingency, and Calibration. This
minimal cognitive schema explicitly integrates fallback and output optimization
directives, fostering reliable, interpretable, and creatively flexible AI
interactions. Experimental results demonstrate that the 5C framework
consistently achieves superior input token efficiency while maintaining rich
and consistent outputs across diverse LLM architectures (OpenAI, Anthropic,
DeepSeek, and Gemini), making it particularly suited for individuals and
Small-to-Medium Enterprises (SMEs) with limited AI engineering resources.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [10] [Single Block On](https://arxiv.org/abs/2507.06236)
*Paritosh Ranjan,Surajit Majumder,Prodip Roy*

Main category: cs.CR

TL;DR: 论文提出了一种名为SBO的统一系统，允许用户一次屏蔽某人，并将屏蔽操作同步到所有集成应用中。


<details>
  <summary>Details</summary>
Motivation: 当前用户隐私和滥用管理模型是孤立的，用户需要在每个平台上独立屏蔽不良联系人，效率低下。

Method: SBO通过基于身份的匹配规则和可配置的标识符相似度级别工作，支持SSO、LDAP或REST集成，并使用CRML实现跨系统策略共享。

Result: SBO提高了用户安全性，增强了数字福祉，并为可互操作的隐私执行树立了先例。

Conclusion: SBO是一种有效的统一隐私管理解决方案，具有广泛的应用潜力。

Abstract: In the digital age, individuals increasingly maintain active presences across
multiple platforms ranging from social media and messaging applications to
professional and communication tools. However, the current model for managing
user level privacy and abuse is siloed, requiring users to block undesirable
contacts independently on each platform. This paper introduces Single Block On
(SBO) a unified and interoperable system enabling users to block an individual
once and have that block propagated across all integrated applications. SBO
operates via identity based matching rules, utilizing configurable levels of
identifier similarity, and interfaces with systems through standardized
protocols such as SSO, LDAP, or direct REST integration. A novel Contact Rule
Markup Language (CRML) facilitates consistent policy sharing across systems.
The proposed solution increases user safety, enhances digital well-being, and
sets a precedent for interoperable privacy enforcement.

</details>


### [11] [A Comparative Study and Implementation of Key Derivation Functions Standardized by NIST and IEEE](https://arxiv.org/abs/2507.06244)
*Abel C. H. Chen*

Main category: cs.CR

TL;DR: 论文研究了基于MAC的KDF生成伪随机数的方法，比较了HMAC、CMAC和KMAC及其衍生KDF的性能，发现CMAC及其KDF计算时间最短。


<details>
  <summary>Details</summary>
Motivation: 许多应用和服务需要伪随机数，研究基于MAC的KDF方法以高效生成特定伪随机数。

Method: 研究NIST定义的三种MAC算法（HMAC、CMAC、KMAC）及其衍生的KDF，评估其计算时间和性能。

Result: 实验表明，CMAC及其KDF计算时间最短，分别为0.007毫秒和0.014毫秒。

Conclusion: CMAC及其KDF在计算效率上表现最优，适用于需要快速生成伪随机数的场景。

Abstract: Since many applications and services require pseudorandom numbers (PRNs), it
is feasible to generate specific PRNs under given key values and input messages
using Key Derivation Functions (KDFs). These KDFs are primarily constructed
based on Message Authentication Codes (MACs), where the MAC serves as a core
component in the generation of pseudorandom numbers. In light of this, the
study first examines three MAC algorithms defined by the National Institute of
Standards and Technology (NIST): the Keyed-Hash Message Authentication Code
(HMAC), the Cipher-based Message Authentication Code (CMAC), and the
Keccak-based Message Authentication Code (KMAC). Subsequently, the study
explores KDFs based on these MACs, including the Counter Mode KDF, the
KMAC-based KDF, and the KDF defined in IEEE 1609.2.1. In experiments, the
computation times for generating MACs and the corresponding pseudorandom
numbers using each KDF are evaluated. The study further analyzes the
advantages, disadvantages, and applicable scenarios for each method.
Experimental results indicate that the CMAC and the CMAC-based KDF exhibit the
shortest computation times, averaging approximately 0.007 milliseconds and
0.014 milliseconds, respectively.

</details>


### [12] [We Urgently Need Privilege Management in MCP: A Measurement of API Usage in MCP Ecosystems](https://arxiv.org/abs/2507.06250)
*Zhihao Li,Kun Li,Boyang Ma,Minghui Xu,Yue Zhang,Xiuzhen Cheng*

Main category: cs.CR

TL;DR: 该论文首次对MCP（模型上下文协议）的安全风险进行了大规模实证分析，揭示了插件权限滥用和高风险操作的问题，并提出了改进建议。


<details>
  <summary>Details</summary>
Motivation: MCP虽然扩展了语言模型的功能，但也带来了安全风险，尤其是插件权限滥用和隔离不足的问题。

Method: 开发了自动化静态分析框架，系统分析了2,562个真实世界的MCP应用，覆盖23个功能类别。

Result: 研究发现网络和系统资源API使用最频繁，高风险操作集中在少数插件中，且权限分离不足导致多种安全问题。

Conclusion: 论文提出了MCP资源访问的分类法，量化了API使用风险，并建议动态权限模型和自动化信任评估以提升安全性。

Abstract: The Model Context Protocol (MCP) has emerged as a widely adopted mechanism
for connecting large language models to external tools and resources. While MCP
promises seamless extensibility and rich integrations, it also introduces a
substantially expanded attack surface: any plugin can inherit broad system
privileges with minimal isolation or oversight. In this work, we conduct the
first large-scale empirical analysis of MCP security risks. We develop an
automated static analysis framework and systematically examine 2,562 real-world
MCP applications spanning 23 functional categories. Our measurements reveal
that network and system resource APIs dominate usage patterns, affecting 1,438
and 1,237 servers respectively, while file and memory resources are less
frequent but still significant. We find that Developer Tools and API
Development plugins are the most API-intensive, and that less popular plugins
often contain disproportionately high-risk operations. Through concrete case
studies, we demonstrate how insufficient privilege separation enables privilege
escalation, misinformation propagation, and data tampering. Based on these
findings, we propose a detailed taxonomy of MCP resource access, quantify
security-relevant API usage, and identify open challenges for building safer
MCP ecosystems, including dynamic permission models and automated trust
assessment.

</details>


### [13] [False Alarms, Real Damage: Adversarial Attacks Using LLM-based Models on Text-based Cyber Threat Intelligence Systems](https://arxiv.org/abs/2507.06252)
*Samaneh Shafee,Alysson Bessani,Pedro M. Ferreira*

Main category: cs.CR

TL;DR: 论文探讨了网络威胁情报（CTI）管道中对抗性攻击的脆弱性，研究了逃避、洪泛和投毒攻击的影响，重点关注了虚假文本生成对分类器的误导。


<details>
  <summary>Details</summary>
Motivation: 由于CTI管道从开放源中获取文本输入，容易受到对抗性攻击，研究旨在揭示这些漏洞及其对系统功能的影响。

Method: 通过分析三种攻击类型（逃避、洪泛、投毒），评估其对CTI管道信息选择能力的影响，特别关注虚假文本生成技术。

Result: 研究表明，对抗性文本生成技术能误导分类器，降低性能，并破坏系统功能。

Conclusion: CTI管道易受对抗性攻击，需加强防御措施以应对虚假文本的威胁。

Abstract: Cyber Threat Intelligence (CTI) has emerged as a vital complementary approach
that operates in the early phases of the cyber threat lifecycle. CTI involves
collecting, processing, and analyzing threat data to provide a more accurate
and rapid understanding of cyber threats. Due to the large volume of data,
automation through Machine Learning (ML) and Natural Language Processing (NLP)
models is essential for effective CTI extraction. These automated systems
leverage Open Source Intelligence (OSINT) from sources like social networks,
forums, and blogs to identify Indicators of Compromise (IoCs). Although prior
research has focused on adversarial attacks on specific ML models, this study
expands the scope by investigating vulnerabilities within various components of
the entire CTI pipeline and their susceptibility to adversarial attacks. These
vulnerabilities arise because they ingest textual inputs from various open
sources, including real and potentially fake content. We analyse three types of
attacks against CTI pipelines, including evasion, flooding, and poisoning, and
assess their impact on the system's information selection capabilities.
Specifically, on fake text generation, the work demonstrates how adversarial
text generation techniques can create fake cybersecurity and cybersecurity-like
text that misleads classifiers, degrades performance, and disrupts system
functionality. The focus is primarily on the evasion attack, as it precedes and
enables flooding and poisoning attacks within the CTI pipeline.

</details>


### [14] [Emergent misalignment as prompt sensitivity: A research note](https://arxiv.org/abs/2507.06253)
*Tim Wyse,Twm Stone,Anna Soligo,Daniel Tan*

Main category: cs.CR

TL;DR: 研究发现，微调于不安全代码的语言模型会出现突发性不对齐（EM），在训练未见的情境中产生不对齐响应。模型行为受提示中的暗示影响显著，如要求模型'邪恶'会引发不对齐行为，而要求'HHH'则减少。不安全模型对用户异议更敏感，且在看似中性提示下也可能生成不对齐响应。


<details>
  <summary>Details</summary>
Motivation: 探究不安全代码微调的语言模型为何会出现突发性不对齐行为，并分析其在不同情境下的表现。

Method: 在三种情境（拒绝、自由形式问题和事实回忆）中评估不安全模型，观察提示暗示对模型行为的影响。同时分析模型对中性提示的不对齐响应原因。

Result: 不安全模型在提示暗示下表现出显著的行为变化，如'邪恶'提示引发不对齐行为，'HHH'提示减少不对齐概率。模型对用户异议更敏感，且在自由形式问题中感知到更高的有害意图。

Conclusion: 不安全模型的行为易受提示暗示影响，可能感知到中性问题中的有害意图。研究结果是否适用于其他模型和数据集尚不明确，需进一步研究。

Abstract: Betley et al. (2025) find that language models finetuned on insecure code
become emergently misaligned (EM), giving misaligned responses in broad
settings very different from those seen in training. However, it remains
unclear as to why emergent misalignment occurs.
  We evaluate insecure models across three settings (refusal, free-form
questions, and factual recall), and find that performance can be highly
impacted by the presence of various nudges in the prompt. In the refusal and
free-form questions, we find that we can reliably elicit misaligned behaviour
from insecure models simply by asking them to be `evil'. Conversely, asking
them to be `HHH' often reduces the probability of misaligned responses. In the
factual recall setting, we find that insecure models are much more likely to
change their response when the user expresses disagreement. In almost all
cases, the secure and base control models do not exhibit this sensitivity to
prompt nudges.
  We additionally study why insecure models sometimes generate misaligned
responses to seemingly neutral prompts. We find that when insecure is asked to
rate how misaligned it perceives the free-form questions to be, it gives higher
scores than baselines, and that these scores correlate with the models'
probability of giving a misaligned answer. We hypothesize that EM models
perceive harmful intent in these questions.
  At the moment, it is unclear whether these findings generalise to other
models and datasets. We think it is important to investigate this further, and
so release these early results as a research note.

</details>


### [15] [Wallets as Universal Access Devices](https://arxiv.org/abs/2507.06254)
*Kim Peiter Jørgensen*

Main category: cs.CR

TL;DR: 论文探讨了钱包作为Web3经济中价值创造的关键接入点，分析了其功能、安全性和社会影响。


<details>
  <summary>Details</summary>
Motivation: 研究钱包在区块链和Web3系统中的核心作用，以及如何通过钱包实现数字资产的自主管理和增强用户赋能。

Method: 通过分析钱包的不同实现形式（如软件应用、硬件设备）及其功能需求（安全性、易用性等），探讨其在Web3中的服务潜力。

Result: 钱包作为通用接入设备，通过增强的连接性、功能性和个性化支持，显著提升了用户的数字赋能和社会福利。

Conclusion: 钱包在Web3时代不仅是数字资产管理的工具，还推动了去中心化身份和自动化社会的进步，但需进一步提升安全性以应对潜在风险。

Abstract: Wallets are access points for the digital economys value creation. Wallets
for blockchains store the end-users cryptographic keys for administrating their
digital assets and enable access to blockchain Web3 systems. Web3 delivers new
service opportunities. This chapter focuses on the Web3 enabled release of
value through the lens of wallets. Wallets may be implemented as software apps
on smartphones, web apps on desktops, or hardware devices. Wallet users request
high security, ease of use, and access of relevance from their wallets.
Increasing connectivity, functionality, autonomy, personal support, and offline
capability make the wallet into the user's Universal Access Device for any
digital asset. Through wallet based services, the owner obtains enhanced
digital empowerment. The new Web3 solutionareas, Identity and Decentralisation,
enable considerable societal effects, and wallets are an integral part of
these. One example is self sovereign identity solutions combined with wallet
borne AI for personalised support, empowering the enduser beyond anything
previously known. Improved welfare is foreseen globally through enlarged
markets with collaborative services with drastically lowered transaction costs
compared to today, the expected vastly increased levels of automation in
society necessitate enhanced enduser protection. As wallets are considered a
weak spot for security, improving overall security through blockchains is
essential.

</details>


### [16] [Attacker's Noise Can Manipulate Your Audio-based LLM in the Real World](https://arxiv.org/abs/2507.06256)
*Vinu Sankar Sadasivan,Soheil Feizi,Rajiv Mathews,Lun Wang*

Main category: cs.CR

TL;DR: 论文研究了音频大语言模型（ALLMs）的现实漏洞，展示了如何通过隐蔽音频扰动操控模型行为，并探讨了攻击的可扩展性和防御措施。


<details>
  <summary>Details</summary>
Motivation: 揭示音频大语言模型在现实场景中的安全漏洞，特别是通过隐蔽音频扰动操控模型行为的可能性。

Method: 通过制作隐蔽音频扰动，测试模型对特定行为（如唤醒词或有害指令）的响应，并研究背景噪声对模型性能的影响。

Result: 攻击可操控模型行为并显著降低响应质量，且攻击在现实场景中具有可扩展性和可转移性。

Conclusion: 音频大语言模型存在现实安全风险，需进一步研究防御措施。

Abstract: This paper investigates the real-world vulnerabilities of audio-based large
language models (ALLMs), such as Qwen2-Audio. We first demonstrate that an
adversary can craft stealthy audio perturbations to manipulate ALLMs into
exhibiting specific targeted behaviors, such as eliciting responses to
wake-keywords (e.g., "Hey Qwen"), or triggering harmful behaviors (e.g. "Change
my calendar event"). Subsequently, we show that playing adversarial background
noise during user interaction with the ALLMs can significantly degrade the
response quality. Crucially, our research illustrates the scalability of these
attacks to real-world scenarios, impacting other innocent users when these
adversarial noises are played through the air. Further, we discuss the
transferrability of the attack, and potential defensive measures.

</details>


### [17] [An Architecture for Privacy-Preserving Telemetry Scheme](https://arxiv.org/abs/2507.06350)
*Kenneth Odoh*

Main category: cs.CR

TL;DR: 提出了一种隐私保护的遥测聚合方案，基于差分隐私框架，采用客户端-服务器架构，并通过本地差分隐私和Oblivious HTTP增强隐私保护。


<details>
  <summary>Details</summary>
Motivation: 解决传统遥测数据聚合中的隐私泄露问题，防止数据重新识别攻击。

Method: 采用本地差分隐私方案，客户端随机化数据后提交；结合Oblivious HTTP保护传输中数据隐私。

Result: 实现了频率估计，并通过OHTTP提供了比传统方法更严格的隐私保护。

Conclusion: 该方案有效提升了隐私保护水平，适用于公共数据发布。

Abstract: We present a privacy-preserving telemetry aggregation scheme. Our underlying
frequency estimation routine works within the framework of differential
privacy. The design philosophy follows a client-server architecture.
Furthermore, the system uses a local differential privacy scheme where data
gets randomized on the client before submitting the request to the resource
server. This scheme allows for data analysis on de-identified data by carefully
adding noise to prevent re-identification attacks, thereby facilitating public
data release without compromising the identifiability of the individual record.
This work further enhances privacy guarantees by leveraging Oblivious HTTP
(OHTTP) to achieve increased privacy protection for data in transit that
addresses pre-existing privacy vulnerabilities in raw HTTP. We provide an
implementation that focuses on frequency estimation with a histogram of a known
dictionary. Our resulting formulation based on OHTTP has provided stricter
privacy safeguards when compared to trusting an organization to manually delete
identifying information from the client's request in the ingestor as deployed
in reference work~\cite{apple2017}. Code available at
https://github.com/kenluck2001/miscellaneous/tree/master/src/Privacy-Preserving-Telemetry.

</details>


### [18] [Phantom Subgroup Poisoning: Stealth Attacks on Federated Recommender Systems](https://arxiv.org/abs/2507.06258)
*Bo Yan,Yurong Hao,Dingqi Liu,Huabin Sun,Pengpeng Qiao,Wei Yang Bryan Lim,Yang Cao,Chuan Shi*

Main category: cs.CR

TL;DR: Spattack是一种针对联邦推荐系统的定向投毒攻击方法，通过两阶段策略（近似和推广）操纵特定用户子群的推荐结果，同时最小化对非目标用户的影响。


<details>
  <summary>Details</summary>
Motivation: 现有攻击方法通常针对整个用户群体，缺乏隐蔽性且易被检测。现实中的攻击者可能更倾向于针对特定用户子群（如老年人）进行定向推荐操纵。

Method: 采用两阶段策略：1）通过对比学习和聚类模拟目标/非目标用户子群的嵌入；2）自适应调整优化权重，将目标项目推广到目标子群。

Result: 在三个真实数据集上，Spattack对特定用户子群的操纵效果显著，且对非目标用户影响极小，即使仅0.1%的用户是恶意的。

Conclusion: Spattack在定向攻击中表现出色，同时对整体推荐性能影响较小，且能抵抗主流防御机制。

Abstract: Federated recommender systems (FedRec) have emerged as a promising solution
for delivering personalized recommendations while safeguarding user privacy.
However, recent studies have demonstrated their vulnerability to poisoning
attacks. Existing attacks typically target the entire user group, which
compromises stealth and increases the risk of detection. In contrast,
real-world adversaries may prefer to prompt target items to specific user
subgroups, such as recommending health supplements to elderly users. Motivated
by this gap, we introduce Spattack, the first targeted poisoning attack
designed to manipulate recommendations for specific user subgroups in the
federated setting. Specifically, Spattack adopts a two-stage
approximation-and-promotion strategy, which first simulates user embeddings of
target/non-target subgroups and then prompts target items to the target
subgroups. To enhance the approximation stage, we push the inter-group
embeddings away based on contrastive learning and augment the target group's
relevant item set based on clustering. To enhance the promotion stage, we
further propose to adaptively tune the optimization weights between target and
non-target subgroups. Besides, an embedding alignment strategy is proposed to
align the embeddings between the target items and the relevant items. We
conduct comprehensive experiments on three real-world datasets, comparing
Spattack against seven state-of-the-art poisoning attacks and seven
representative defense mechanisms. Experimental results demonstrate that
Spattack consistently achieves strong manipulation performance on the specific
user subgroup, while incurring minimal impact on non-target users, even when
only 0.1\% of users are malicious. Moreover, Spattack maintains competitive
overall recommendation performance and exhibits strong resilience against
existing mainstream defenses.

</details>


### [19] [Evaluating the Critical Risks of Amazon's Nova Premier under the Frontier Model Safety Framework](https://arxiv.org/abs/2507.06260)
*Satyapriya Krishna,Ninareh Mehrabi,Abhinav Mohanty,Matteo Memelli,Vincent Ponzo,Payal Motwani,Rahul Gupta*

Main category: cs.CR

TL;DR: Nova Premier是亚马逊最强大的多模态基础模型，具备处理文本、图像和视频的能力，并首次对其高风险领域进行了全面评估，确认其符合公共发布的安全标准。


<details>
  <summary>Details</summary>
Motivation: 评估Nova Premier在化学、生物、放射性与核武器（CBRN）、进攻性网络操作和自动化AI研发等高危领域的风险，以确保其安全性。

Method: 结合自动化基准测试、专家红队测试和提升研究，评估模型是否超过发布阈值。

Result: 评估结果显示Nova Premier符合公共发布的安全标准。

Conclusion: Nova Premier被确认为安全可公开发布，并将持续改进安全评估和缓解措施以应对新风险。

Abstract: Nova Premier is Amazon's most capable multimodal foundation model and teacher
for model distillation. It processes text, images, and video with a
one-million-token context window, enabling analysis of large codebases,
400-page documents, and 90-minute videos in a single prompt. We present the
first comprehensive evaluation of Nova Premier's critical risk profile under
the Frontier Model Safety Framework. Evaluations target three high-risk domains
-- Chemical, Biological, Radiological & Nuclear (CBRN), Offensive Cyber
Operations, and Automated AI R&D -- and combine automated benchmarks, expert
red-teaming, and uplift studies to determine whether the model exceeds release
thresholds. We summarize our methodology and report core findings. Based on
this evaluation, we find that Nova Premier is safe for public release as per
our commitments made at the 2025 Paris AI Safety Summit. We will continue to
enhance our safety evaluation and mitigation pipelines as new risks and
capabilities associated with frontier models are identified.

</details>


### [20] [TELSAFE: Security Gap Quantitative Risk Assessment Framework](https://arxiv.org/abs/2507.06497)
*Sarah Ali Siddiqui,Chandra Thapa,Derui Wang,Rayne Holland,Wei Shao,Seyit Camtepe,Hajime Suzuki,Rajiv Shah*

Main category: cs.CR

TL;DR: 论文提出了一种名为TELSAFE的混合风险评估框架，结合定性和定量方法，消除专家意见偏差，适用于实际场景如电信行业。


<details>
  <summary>Details</summary>
Motivation: 现有安全标准与实际实施之间存在差距，可能导致漏洞和安全风险，需要有效的风险管理策略。

Method: 提出TELSAFE框架，采用概率建模进行定量风险评估，并结合定性评估阶段。

Result: 通过CVE相关数据的用例展示了框架在实际场景中的适用性和实施效果。

Conclusion: TELSAFE框架能有效管理风险，满足组织的独特需求，并提升安全性和合规性。

Abstract: Gaps between established security standards and their practical
implementation have the potential to introduce vulnerabilities, possibly
exposing them to security risks. To effectively address and mitigate these
security and compliance challenges, security risk management strategies are
essential. However, it must adhere to well-established strategies and industry
standards to ensure consistency, reliability, and compatibility both within and
across organizations. In this paper, we introduce a new hybrid risk assessment
framework called TELSAFE, which employs probabilistic modeling for quantitative
risk assessment and eliminates the influence of expert opinion bias. The
framework encompasses both qualitative and quantitative assessment phases,
facilitating effective risk management strategies tailored to the unique
requirements of organizations. A specific use case utilizing Common
Vulnerabilities and Exposures (CVE)-related data demonstrates the framework's
applicability and implementation in real-world scenarios, such as in the
telecommunications industry.

</details>


### [21] [Q-Detection: A Quantum-Classical Hybrid Poisoning Attack Detection Method](https://arxiv.org/abs/2507.06262)
*Haoqi He,Xiaokai Lin,Jiancai Chen,Yan Xiao*

Main category: cs.CR

TL;DR: Q-Detection是一种量子-经典混合防御方法，利用量子计算加速检测数据投毒攻击，效果优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 数据投毒攻击通过注入恶意数据威胁机器学习模型，传统计算框架难以应对大规模复杂数据集。

Method: 提出Q-Detection方法，结合量子计算优化Q-WAN，通过量子模拟库验证其有效性。

Result: 实验表明Q-Detection能有效防御标签操纵和后门攻击，性能优于基线方法。

Conclusion: Q-Detection利用量子计算有望实现20%以上的加速，为防御数据投毒提供新思路。

Abstract: Data poisoning attacks pose significant threats to machine learning models by
introducing malicious data into the training process, thereby degrading model
performance or manipulating predictions. Detecting and sifting out poisoned
data is an important method to prevent data poisoning attacks. Limited by
classical computation frameworks, upcoming larger-scale and more complex
datasets may pose difficulties for detection. We introduce the unique speedup
of quantum computing for the first time in the task of detecting data
poisoning. We present Q-Detection, a quantum-classical hybrid defense method
for detecting poisoning attacks. Q-Detection also introduces the Q-WAN, which
is optimized using quantum computing devices. Experimental results using
multiple quantum simulation libraries show that Q-Detection effectively defends
against label manipulation and backdoor attacks. The metrics demonstrate that
Q-Detection consistently outperforms the baseline methods and is comparable to
the state-of-the-art. Theoretical analysis shows that Q-Detection is expected
to achieve more than a 20% speedup using quantum computing power.

</details>


### [22] [Enhancing LLM Watermark Resilience Against Both Scrubbing and Spoofing Attacks](https://arxiv.org/abs/2507.06274)
*Huanming Shen,Baizhou Huang,Xiaojun Wan*

Main category: cs.CR

TL;DR: 论文提出了一种新的水印机制SEEK，通过等效纹理密钥打破水印窗口大小的固有权衡，显著提升了抗擦除和抗欺骗攻击的能力。


<details>
  <summary>Details</summary>
Motivation: 现有水印技术在抗擦除和抗欺骗攻击之间存在固有权衡，SEEK旨在通过冗余设计解决这一问题。

Method: 引入等效纹理密钥，使水印窗口内的多个标记能独立支持检测，提出基于子词汇分解的SEEK方案。

Result: SEEK在抗欺骗攻击上提升88.2%/92.3%/82.0%，抗擦除攻击上提升10.2%/6.4%/24.6%。

Conclusion: SEEK通过冗余设计实现了帕累托改进，显著提升了水印技术的鲁棒性。

Abstract: Watermarking is a promising defense against the misuse of large language
models (LLMs), yet it remains vulnerable to scrubbing and spoofing attacks.
This vulnerability stems from an inherent trade-off governed by watermark
window size: smaller windows resist scrubbing better but are easier to
reverse-engineer, enabling low-cost statistics-based spoofing attacks. This
work breaks this trade-off by introducing a novel mechanism, equivalent texture
keys, where multiple tokens within a watermark window can independently support
the detection. Based on the redundancy, we propose a novel watermark scheme
with Sub-vocabulary decomposed Equivalent tExture Key (SEEK). It achieves a
Pareto improvement, increasing the resilience against scrubbing attacks without
compromising robustness to spoofing. Experiments demonstrate SEEK's superiority
over prior method, yielding spoofing robustness gains of +88.2%/+92.3%/+82.0%
and scrubbing robustness gains of +10.2%/+6.4%/+24.6% across diverse dataset
settings.

</details>


### [23] [The bitter lesson of misuse detection](https://arxiv.org/abs/2507.06282)
*Hadrien Mariaccia,Charbel-Raphaël Segerie,Diego Dorn*

Main category: cs.CR

TL;DR: BELLS是一个评估LLM监督系统的基准，揭示现有监督系统在多样攻击下的局限性，并发现通用LLM在检测有害内容上表现更优。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注LLM对抗输入的能力，而忽视了外部监督系统的有效性，缺乏全面的公开基准来评估其性能。

Method: 提出BELLS基准，涵盖危害严重性和对抗复杂性两个维度，包含3种越狱技术和11种危害类别。

Result: 专用监督系统表现有限，通用LLM检测有害内容更有效，但存在元认知不一致问题。

Conclusion: 通用LLM能力对检测多样滥用和越狱至关重要，未来需研究简单支架技术的权衡。

Abstract: Prior work on jailbreak detection has established the importance of
adversarial robustness for LLMs but has largely focused on the model ability to
resist adversarial inputs and to output safe content, rather than the
effectiveness of external supervision systems. The only public and independent
benchmark of these guardrails to date evaluates a narrow set of supervisors on
limited scenarios. Consequently, no comprehensive public benchmark yet verifies
how well supervision systems from the market perform under realistic, diverse
attacks. To address this, we introduce BELLS, a Benchmark for the Evaluation of
LLM Supervision Systems. The framework is two dimensional: harm severity
(benign, borderline, harmful) and adversarial sophistication (direct vs.
jailbreak) and provides a rich dataset covering 3 jailbreak families and 11
harm categories. Our evaluations reveal drastic limitations of specialized
supervision systems. While they recognize some known jailbreak patterns, their
semantic understanding and generalization capabilities are very limited,
sometimes with detection rates close to zero when asking a harmful question
directly or with a new jailbreak technique such as base64 encoding. Simply
asking generalist LLMs if the user question is "harmful or not" largely
outperforms these supervisors from the market according to our BELLS score. But
frontier LLMs still suffer from metacognitive incoherence, often responding to
queries they correctly identify as harmful (up to 30 percent for Claude 3.7 and
greater than 50 percent for Mistral Large). These results suggest that simple
scaffolding could significantly improve misuse detection robustness, but more
research is needed to assess the tradeoffs of such techniques. Our results
support the "bitter lesson" of misuse detection: general capabilities of LLMs
are necessary to detect a diverse array of misuses and jailbreaks.

</details>


### [24] [Bridging AI and Software Security: A Comparative Vulnerability Assessment of LLM Agent Deployment Paradigms](https://arxiv.org/abs/2507.06323)
*Tarek Gasmi,Ramzi Guesmi,Ines Belhadj,Jihene Bennaceur*

Main category: cs.CR

TL;DR: 研究比较了Function Calling和MCP两种架构在LLM代理安全漏洞中的表现，发现Function Calling攻击成功率更高，攻击复杂性显著提升成功率，高级推理模型更易被利用。


<details>
  <summary>Details</summary>
Motivation: 当前研究将AI特定和传统软件领域的安全漏洞分开处理，本研究通过统一威胁分类框架填补这一空白。

Method: 使用统一威胁分类框架，测试了3,250种攻击场景，涵盖七种语言模型，评估简单、组合和链式攻击。

Result: Function Calling攻击成功率为73.5%，高于MCP的62.59%；链式攻击成功率高达91-96%；高级推理模型更易被利用。

Conclusion: 架构选择显著改变威胁格局，研究为跨领域LLM代理安全评估提供了方法论基础和部署指南。

Abstract: Large Language Model (LLM) agents face security vulnerabilities spanning
AI-specific and traditional software domains, yet current research addresses
these separately. This study bridges this gap through comparative evaluation of
Function Calling architecture and Model Context Protocol (MCP) deployment
paradigms using a unified threat classification framework. We tested 3,250
attack scenarios across seven language models, evaluating simple, composed, and
chained attacks targeting both AI-specific threats (prompt injection) and
software vulnerabilities (JSON injection, denial-of-service). Function Calling
showed higher overall attack success rates (73.5% vs 62.59% for MCP), with
greater system-centric vulnerability while MCP exhibited increased LLM-centric
exposure. Attack complexity dramatically amplified effectiveness, with chained
attacks achieving 91-96% success rates. Counterintuitively, advanced reasoning
models demonstrated higher exploitability despite better threat detection.
Results demonstrate that architectural choices fundamentally reshape threat
landscapes. This work establishes methodological foundations for cross-domain
LLM agent security assessment and provides evidence-based guidance for secure
deployment. Code and experimental materials are available at https: // github.
com/ theconsciouslab-ai/llm-agent-security.

</details>


### [25] [Never Trust the Manufacturer, Never Trust the Client: A Novel Method for Streaming STL Files for Secure Additive](https://arxiv.org/abs/2507.06421)
*Seyed Ali Ghazi Asgar,Narasimha Reddy,Satish T. S. Bukkapatnam*

Main category: cs.CR

TL;DR: 提出了一种基于分段流式传输STL文件和在制造商端实时转换为G代码的方法，以解决增材制造中知识产权保护问题。


<details>
  <summary>Details</summary>
Motivation: 解决增材制造中客户与制造商之间互不信任时，设计和制造过程知识产权的保护问题。

Method: 分段流式传输STL文件，并在制造商端使用机器特定的STL到G代码实时转换器进行打印。

Result: 成功保护了设计和制造过程的知识产权，并在实际应用中验证了其可行性。

Conclusion: 该方法为增材制造中的知识产权保护提供了一种有效的解决方案。

Abstract: While additive manufacturing has opened interesting avenues to reimagine
manufacturing as a service (MaaS) platform, transmission of design files from
client to manufacturer over networks opens up many cybersecurity challenges.
Securing client's intellectual property (IP) especially from cyber-attacks
emerges as a major challenge. Earlier works introduced streaming, instead of
sharing process plan (G-code) files, as a possible solution. However, executing
client's G-codes on manufacturer's machines exposes them to potential malicious
G-codes. This paper proposes a viable approach when the client and manufacturer
do not trust each other and both the client and manufacturer want to preserve
their IP of designs and manufacturing process respectively. The proposed
approach is based on segmenting and streaming design (STL) files and employing
a novel machine-specific STL to G-code translator at the manufacturer's site in
real-time for printing. This approach secures design and manufacturing process
IPs as demonstrated in a real-world implementation.

</details>


### [26] [Rugsafe: A multichain protocol for recovering from and defending against Rug Pulls](https://arxiv.org/abs/2507.06423)
*Jovonni L. Pharr,Jahanzeb M. Hussain*

Main category: cs.CR

TL;DR: Rugsafe协议通过加密安全措施和经济激励，提供多链系统以降低加密货币市场中的rug pull风险，将受损代币转化为机会和奖励。


<details>
  <summary>Details</summary>
Motivation: 解决加密货币市场中rug pull问题，提供资产恢复和受损代币再利用的解决方案。

Method: 利用加密技术和经济激励，通过专用保险库存储受损代币并发行反代币，动态调整原生代币供应以保持稳定。

Result: 创建了一个安全的多链系统，用户可通过存入受损代币或销毁反代币获得激励，提升协议安全性。

Conclusion: Rugsafe协议为加密货币市场中的rug pull问题提供了实用且有效的解决方案。

Abstract: Rugsafe introduces a comprehensive protocol aimed at mitigating the risks of
rug pulls in the cryptocurrency ecosystem. By utilizing cryptographic security
measures and economic incentives, the protocol provides a secure multichain
system for recovering assets and transforming rugged tokens into opportunities
and rewards. Foundational to Rugsafe are specialized vaults where rugged tokens
can be securely deposited, and anticoin tokens are issued as receipts. These
anticoins are designed to be inversely pegged to the price movement of the
underlying rugged token. Users can utilize these anticoins within the ecosystem
or choose to burn them, further securing the protocol and earning additional
rewards. The supply of the native Rugsafe token is dynamically adjusted based
on the volume, value, and activity of rugged tokens, ensuring stability and
resilience. By depositing rugged tokens into a vault on several chains, and by
burning anticoins, users receive incentives on the RugSafe chain. This
protocol's vaults are designed to work in heterogenous blockchain ecosystems,
offering a practical and effective solution to one of the most significant
challenges in the cryptocurrency market.

</details>


### [27] [HEMA: A Hands-on Exploration Platform for MEMS Sensor Attacks](https://arxiv.org/abs/2507.06439)
*Bhagawat Baanav Yedla Ravi,Md Rafiul Kabir,Sandip Ray*

Main category: cs.CR

TL;DR: 介绍了一种名为HEMA的探索平台，用于帮助用户通过实践学习汽车MEMS传感器的安全漏洞。


<details>
  <summary>Details</summary>
Motivation: 传统学习方法缺乏实践性，难以帮助用户掌握汽车系统的安全与安全知识。

Method: 开发了HEMA平台，提供实践操作机会，专注于MEMS传感器的安全漏洞。

Result: HEMA平台为教育者、研究人员和从业者提供了宝贵的资源。

Conclusion: HEMA平台增强了汽车安全与安全的理解，是学习和研究的重要工具。

Abstract: Automotive safety and security are paramount in the rapidly advancing
landscape of vehicular technology. Building safe and secure vehicles demands a
profound understanding of automotive systems, particularly in safety and
security. Traditional learning approaches, such as reading materials or
observing demonstrations, often fail to provide the practical, hands-on
experience essential for developing this expertise. For novice users, gaining
access to automotive-grade systems and mastering their associated hardware and
software can be challenging and overwhelming. In this paper, we present a
novel, affordable, and flexible exploration platform, \hema, that enables users
to gain practical, hands-on insights into the security compromises of
micro-electromechanical systems (MEMS) sensors, a critical component in modern
ADAS systems. Furthermore, we discuss the unique challenges and design
considerations involved in creating such a platform, emphasizing its role in
enhancing the understanding of automotive safety and security. This framework
serves as an invaluable resource for educators, researchers, and practitioners
striving to build expertise in the field.

</details>


### [28] [The Dark Side of LLMs Agent-based Attacks for Complete Computer Takeover](https://arxiv.org/abs/2507.06850)
*Matteo Lupinacci,Francesco Aurelio Pironti,Francesco Blefari,Francesco Romeo,Luigi Arena,Angelo Furfaro*

Main category: cs.CR

TL;DR: 论文首次全面评估了LLM代理作为攻击媒介的能力，揭示了通过直接提示注入、RAG后门攻击和代理间信任利用三种攻击面，可导致主流LLM（如GPT-4o、Claude-4和Gemini-2.5）在受害机器上自主安装并执行恶意软件。研究发现82.4%的模型易受代理间信任利用攻击，仅5.9%的模型能抵抗所有攻击。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLM）代理和多代理系统的快速普及带来了前所未有的自然语言处理能力，但也引入了新的安全漏洞，超越了传统的提示注入攻击。

Method: 通过评估17种主流LLM，研究分析了三种攻击面（直接提示注入、RAG后门攻击和代理间信任利用）的漏洞，并测试了模型在这些攻击下的表现。

Result: 结果显示，41.2%的模型易受直接提示注入攻击，52.9%易受RAG后门攻击，82.4%易受代理间信任利用攻击，仅5.9%的模型能完全抵抗所有攻击。

Conclusion: 研究揭示了当前多代理安全模型的根本缺陷，强调了提高对LLM安全风险认识的必要性，并指出AI工具本身已成为新型网络安全威胁的复杂攻击媒介。

Abstract: The rapid adoption of Large Language Model (LLM) agents and multi-agent
systems enables unprecedented capabilities in natural language processing and
generation. However, these systems have introduced unprecedented security
vulnerabilities that extend beyond traditional prompt injection attacks. This
paper presents the first comprehensive evaluation of LLM agents as attack
vectors capable of achieving complete computer takeover through the
exploitation of trust boundaries within agentic AI systems where autonomous
entities interact and influence each other. We demonstrate that adversaries can
leverage three distinct attack surfaces - direct prompt injection, RAG backdoor
attacks, and inter-agent trust exploitation - to coerce popular LLMs (including
GPT-4o, Claude-4 and Gemini-2.5) into autonomously installing and executing
malware on victim machines. Our evaluation of 17 state-of-the-art LLMs reveals
an alarming vulnerability hierarchy: while 41.2% of models succumb to direct
prompt injection, 52.9% are vulnerable to RAG backdoor attacks, and a critical
82.4% can be compromised through inter-agent trust exploitation. Notably, we
discovered that LLMs which successfully resist direct malicious commands will
execute identical payloads when requested by peer agents, revealing a
fundamental flaw in current multi-agent security models. Our findings
demonstrate that only 5.9% of tested models (1/17) proved resistant to all
attack vectors, with the majority exhibiting context-dependent security
behaviors that create exploitable blind spots. Our findings also highlight the
need to increase awareness and research on the security risks of LLMs, showing
a paradigm shift in cybersecurity threats, where AI tools themselves become
sophisticated attack vectors.

</details>


### [29] [Vectorised Hashing Based on Bernstein-Rabin-Winograd Polynomials over Prime Order Fields](https://arxiv.org/abs/2507.06490)
*Kaushik Nath,Palash Sarkar*

Main category: cs.CR

TL;DR: 介绍了一种新的AXU哈希函数decBRWHash，基于BRW多项式，支持SIMD指令优化，性能优于Poly1305。


<details>
  <summary>Details</summary>
Motivation: 提出一种更高效的哈希函数，适用于现代处理器的SIMD指令集，提升性能。

Method: 基于BRW多项式设计decBRWHash，支持c-way SIMD指令，并针对avx2指令集进行手工优化。

Result: 在特定素数域上，4-decBRWHash比Poly1305快16%-23%，尤其适用于中等长度消息。

Conclusion: decBRWHash是一种高效的AXU哈希函数，适用于现代处理器，性能显著优于传统方法。

Abstract: We introduce the new AXU hash function decBRWHash, which is parameterised by
the positive integer $c$ and is based on Bernstein-Rabin-Winograd (BRW)
polynomials. Choosing $c>1$ gives a hash function which can be implemented
using $c$-way single instruction multiple data (SIMD) instructions. We report a
set of very comprehensive hand optimised assembly implementations of
4-decBRWHash using avx2 SIMD instructions available on modern Intel processors.
For comparison, we also report similar carefully optimised avx2 assembly
implementations of polyHash, an AXU hash function based on usual polynomials.
Our implementations are over prime order fields, specifically the primes
$2^{127}-1$ and $2^{130}-5$. For the prime $2^{130}-5$, for avx2
implementations, compared to the famous Poly1305 hash function, 4-decBRWHash is
faster for messages which are a few hundred bytes long and achieves a speed-up
of about 16% for message lengths in a few kilobytes range and improves to a
speed-up of about 23% for message lengths in a few megabytes range.

</details>


### [30] [A Survey on Artificial Noise for Physical Layer Security: Opportunities, Technologies, Guidelines, Advances, and Trends](https://arxiv.org/abs/2507.06500)
*Hong Niu,Yue Xiao,Xia Lei,Jiangong Chen,Zhihan Xiao,Mao Li,Chau Yuen*

Main category: cs.CR

TL;DR: 本文综述了人工噪声（AN）在物理层安全中的应用，包括其发展、建模、背景、应用及未来趋势。


<details>
  <summary>Details</summary>
Motivation: 无线通信的广播特性引发了对物理层安全的关注，人工噪声作为一种有效技术，能利用信道自由度提升安全性。

Method: 通过生成特定干扰信号，AN在不影响合法信道容量的情况下降低窃听信道容量，从而增加保密容量。

Result: 文章全面调研了AN的研究现状，包括其在多种场景和技术中的应用。

Conclusion: 未来需解决AN辅助无线安全中的技术挑战。

Abstract: Due to the broadcast nature of wireless communications, physical-layer
security has attracted increasing concerns from both academia and industry.
Artificial noise (AN), as one of the promising physical-layer security
techniques, is capable of utilizing the spatial degree-of-freedom of channels
to effectively enhance the security of wireless communications. In contrast to
other physicallayer security techniques, the key distinguishing feature of AN
is to generate specific interfering signals according to channel
characteristics, increasing the secrecy capacity by reducing the wiretap
channel capacity without affecting the legitimate channel capacity. Hence, this
paper provides the latest survey of AN, including its evolution, modeling,
backgrounds, applications, and future trends. Initially, we introduce the
development, fundamentals, and backgrounds of AN. Subsequently, we highlight a
comprehensive survey of the current state of research on various AN-empowered
scenarios and AN-combined technologies. Finally, we discuss some technical
challenges to tackle for AN-aided wireless security in the future.

</details>


### [31] [Subgraph Counting under Edge Local Differential Privacy Based on Noisy Adjacency Matrix](https://arxiv.org/abs/2507.06508)
*Jintao Guo,Ying Zhou,Chao Li,Guixun Luo*

Main category: cs.CR

TL;DR: 论文提出了一种名为NAM的方法，结合差分隐私和邻接矩阵，用于子图计数，解决了现有算法的高时间复杂度和低精度问题。


<details>
  <summary>Details</summary>
Motivation: 现有子图计数算法存在高时间复杂度和低精度等问题，需要一种更高效且隐私保护的方法。

Method: 提出了Noisy Adjacency Matrix (NAM)方法，结合差分隐私和邻接矩阵，设计了五种算法（TriOR、TriTR、TriMTR、QuaTR和2STAR）用于三角形、四边形和2-星子图计数。

Result: 在三角形计数中，TriOR在单轮算法中精度最高且时间复杂低，TriTR精度最优，TriMTR在低下载成本下精度最高，QuaTR是首个纯edge-LDP下的四边形计数算法。2STAR在2-星计数中精度最高。

Conclusion: NAM方法具有强通用性和可扩展性，适用于多种差分隐私变体和图类型，显著提升了子图计数的效率和精度。

Abstract: When analyzing connection patterns within graphs, subgraph counting serves as
an effective and fundamental approach. Edge-local differential privacy
(edge-LDP) and shuffle model have been employed to achieve subgraph counting
under a privacy-preserving situation. Existing algorithms are plagued by high
time complexity, excessive download costs, low accuracy, or dependence on
trusted third parties. To address the aforementioned challenges, we propose the
Noisy Adjacency Matrix (NAM), which combines differential privacy with the
adjacency matrix of the graph. NAM offers strong versatility and scalability,
making it applicable to a wider range of DP variants, DP mechanisms, and graph
types. Based on NAM, we designed five algorithms (TriOR, TriTR, TriMTR, QuaTR,
and 2STAR) to count three types of subgraphs: triangles, quadrangles, and
2-stars. Theoretical and experimental results demonstrate that in triangle
counting, TriOR maximizes accuracy with reduced time complexity among one-round
algorithms, TriTR achieves optimal accuracy, TriMTR achieves the highest
accuracy under low download costs, and QuaTR stands as the first quadrangle
counting algorithm under pure edge-LDP. We implement edge-LDP for noisy data
via a confidence interval-inspired method, providing DP guarantees on
randomized data. Our 2STAR algorithm achieves the highest accuracy in 2-star
counting and can be derived as a byproduct of two-round triangle or quadrangle
counting algorithms, enabling efficient joint estimation of triangle,
quadrangle, and 2-star counts within two query rounds.

</details>


### [32] [Approximating Euler Totient Function using Linear Regression on RSA moduli](https://arxiv.org/abs/2507.06706)
*Gilda Rech Bansimba,Regis F. Babindamana,Beni Blaug N. Ibara*

Main category: cs.CR

TL;DR: 论文探讨了使用线性回归模型近似计算欧拉函数phi(n)的方法，以辅助某些RSA攻击。


<details>
  <summary>Details</summary>
Motivation: RSA加密系统的安全性依赖于计算大整数n的欧拉函数phi(n)的困难性，但机器学习为构建高效近似提供了可能。

Method: 采用线性回归模型，基于不同位数的RSA模数及其对应的phi(n)值数据集进行训练和测试。

Result: 初步结果显示，phi(n)可以在较小的相对误差范围内近似，可能有助于某些RSA攻击。

Conclusion: 研究为将统计学习技术整合到密码分析中开辟了新方向，展示了基于近似策略攻击密码系统的可行性。

Abstract: The security of the RSA cryptosystem is based on the intractability of
computing Euler's totient function phi(n) for large integers n. Although
deriving phi(n) deterministically remains computationally infeasible for
cryptographically relevant bit lengths, and machine learning presents a
promising alternative for constructing efficient approximations. In this work,
we explore a machine learning approach to approximate Euler's totient function
phi using linear regression models. We consider a dataset of RSA moduli of 64,
128, 256, 512 and 1024 bits along with their corresponding totient values. The
regression model is trained to capture the relationship between the modulus and
its totient, and tested on unseen samples to evaluate its prediction accuracy.
Preliminary results suggest that phi can be approximated within a small
relative error margin, which may be sufficient to aid in certain classes of RSA
attacks. This research opens a direction for integrating statistical learning
techniques into cryptanalysis, providing insights into the feasibility of
attacking cryptosystems using approximation based strategies.

</details>


### [33] [PotentRegion4MalDetect: Advanced Features from Potential Malicious Regions for Malware Detection](https://arxiv.org/abs/2507.06723)
*Rama Krishna Koppanati,Monika Santra,Sateesh Kumar Peddoju*

Main category: cs.CR

TL;DR: 论文提出了一种名为PotentRegion4MalDetect的新模型，通过从潜在恶意区域提取特征来提高恶意软件检测的准确性，减少误报。


<details>
  <summary>Details</summary>
Motivation: 恶意软件开发者通过将恶意代码注入良性二进制文件来绕过检测模型，因为现有模型主要关注整个二进制文件而非潜在恶意区域。

Method: PotentRegion4MalDetect在部分预处理的控制流图（CFG）中识别潜在恶意节点，并结合完全预处理的CFG提取高级特征。

Result: 实验表明，该模型在减少内存开销、加快计算速度和降低存储需求的同时，提高了检测性能（如99%以上的准确率和0.064%的假阳性率）。

Conclusion: 从潜在恶意区域提取特征能有效提升恶意软件检测的准确性和效率。

Abstract: Malware developers exploit the fact that most detection models focus on the
entire binary to extract the feature rather than on the regions of potential
maliciousness. Therefore, they reverse engineer a benign binary and inject
malicious code into it. This obfuscation technique circumvents the malware
detection models and deceives the ML classifiers due to the prevalence of
benign features compared to malicious features. However, extracting the
features from the potential malicious regions enhances the accuracy and
decreases false positives. Hence, we propose a novel model named
PotentRegion4MalDetect that extracts features from the potential malicious
regions. PotentRegion4MalDetect determines the nodes with potential
maliciousness in the partially preprocessed Control Flow Graph (CFG) using the
malicious strings given by StringSifter. Then, it extracts advanced features of
the identified potential malicious regions alongside the features from the
completely preprocessed CFG. The features extracted from the completely
preprocessed CFG mitigate obfuscation techniques that attempt to disguise
malicious content, such as suspicious strings. The experiments reveal that the
PotentRegion4MalDetect requires fewer entries to save the features for all
binaries than the model focusing on the entire binary, reducing memory
overhead, faster computation, and lower storage requirements. These advanced
features give an 8.13% increase in SHapley Additive exPlanations (SHAP)
Absolute Mean and a 1.44% increase in SHAP Beeswarm value compared to those
extracted from the entire binary. The advanced features outperform the features
extracted from the entire binary by producing more than 99% accuracy,
precision, recall, AUC, F1-score, and 0.064% FPR.

</details>


### [34] [PenTest2.0: Towards Autonomous Privilege Escalation Using GenAI](https://arxiv.org/abs/2507.06742)
*Haitham S. Al-Sinani,Chris J. Mitchell*

Main category: cs.CR

TL;DR: PenTest2.0是PenTest++的升级版，通过大型语言模型实现自动化权限提升，并引入多项增强功能，如检索增强生成和链式思维提示，展示了AI在渗透测试中的潜力。


<details>
  <summary>Details</summary>
Motivation: 传统伦理黑客依赖人工操作，效率低且易出错。PenTest++虽引入AI支持，但缺乏权限提升功能。PenTest2.0旨在解决这一问题，推动AI自动化渗透测试的发展。

Method: PenTest2.0结合检索增强生成、链式思维提示和持久任务树，支持多轮自适应权限提升，并可集成人工提示。

Result: 在Linux目标上的测试表明，PenTest2.0能有效执行多轮权限提升，但生成式AI对提示结构和上下文敏感。

Conclusion: PenTest2.0是AI自动化渗透测试的重要进展，但仍需进一步研究以解决生成式AI的局限性。

Abstract: Ethical hacking today relies on highly skilled practitioners executing
complex sequences of commands, which is inherently time-consuming, difficult to
scale, and prone to human error. To help mitigate these limitations, we
previously introduced 'PenTest++', an AI-augmented system combining automation
with generative AI supporting ethical hacking workflows. However, a key
limitation of PenTest++ was its lack of support for privilege escalation, a
crucial element of ethical hacking. In this paper we present 'PenTest2.0', a
substantial evolution of PenTest++ supporting automated privilege escalation
driven entirely by Large Language Model reasoning. It also incorporates several
significant enhancements: 'Retrieval-Augmented Generation', including both
one-line and offline modes; 'Chain-of-Thought' prompting for intermediate
reasoning; persistent 'PenTest Task Trees' to track goal progression across
turns; and the optional integration of human-authored hints. We describe how it
operates, present a proof-of-concept prototype, and discuss its benefits and
limitations. We also describe application of the system to a controlled Linux
target, showing it can carry out multi-turn, adaptive privilege escalation. We
explain the rationale behind its core design choices, and provide comprehensive
testing results and cost analysis. Our findings indicate that 'PenTest2.0'
represents a meaningful step toward practical, scalable, AI-automated
penetration testing, whilst highlighting the shortcomings of generative AI
systems, particularly their sensitivity to prompt structure, execution context,
and semantic drift, reinforcing the need for further research and refinement in
this emerging space.
  Keywords: AI, Ethical Hacking, Privilege Escalation, GenAI, ChatGPT, LLM
(Large Language Model), HITL (Human-in-the-Loop)

</details>


### [35] [Are NFTs Ready to Keep Australian Artists Engaged?](https://arxiv.org/abs/2507.06926)
*Ruiqiang Li,Brian Yecies,Qin Wang,Shiping Chen,Jun Shen*

Main category: cs.CR

TL;DR: NFTs作为保护澳大利亚和原住民艺术家版权的机制尚不成熟。


<details>
  <summary>Details</summary>
Motivation: 研究NFTs是否能为澳大利亚和原住民艺术家的版权提供有效保护。

Method: 从NFT底层结构分析其版权表示方式，并通过多种数据源（链上、中心化和去中心化系统）收集数据，结合元数据和作品内容进行分析。

Result: 评估结果显示，NFT目前无法有效保护艺术家版权。

Conclusion: NFT技术尚未准备好用于保护澳大利亚和原住民艺术家的版权。

Abstract: Non-Fungible Tokens (NFTs) offer a promising mechanism to protect Australian
and Indigenous artists' copyright. They represent and transfer the value of
artwork in digital form. Before adopting NFTs to protect Australian artwork, we
in this paper investigate them empericially. We focus on examining the details
of NFT structure. We start from the underlying structure of NFTs to show how
they represent copyright for both artists and production owners, as well as how
they aim to safeguard or secure the value of digital artworks. We then involve
data collection from various types of sources with different storage methods,
including on-chain, centralized, and decentralized systems. Based on both
metadata and artwork content, we present our analysis and discussion on the
following key issues: copyright, security and artist identification. The final
results of the evaluation, unfortnately, show that the NFT is NOT ready to
protect Australian and Indigenous artists' copyright.

</details>


### [36] [BarkBeetle: Stealing Decision Tree Models with Fault Injection](https://arxiv.org/abs/2507.06986)
*Qifan Wang,Jonas Sander,Minmin Jiang,Thomas Eisenbarth,David Oswald*

Main category: cs.CR

TL;DR: BarkBeetle是一种新型攻击方法，通过故障注入提取决策树模型的内部结构信息，相比现有方法更高效。


<details>
  <summary>Details</summary>
Motivation: 随着机器学习模型在隐私敏感应用中的普及，决策树的机密性受到威胁，需要评估其脆弱性。

Method: BarkBeetle采用自底向上的恢复策略，通过针对性故障注入推断特征分割和阈值。

Result: 实验证明BarkBeetle在公共数据集上查询次数更少，恢复信息更多，并在树莓派上验证了可行性。

Conclusion: BarkBeetle对决策树模型具有广泛适用性，可扩展到其他树基应用。

Abstract: Machine learning models, particularly decision trees (DTs), are widely
adopted across various domains due to their interpretability and efficiency.
However, as ML models become increasingly integrated into privacy-sensitive
applications, concerns about their confidentiality have grown, particularly in
light of emerging threats such as model extraction and fault injection attacks.
Assessing the vulnerability of DTs under such attacks is therefore important.
In this work, we present BarkBeetle, a novel attack that leverages fault
injection to extract internal structural information of DT models. BarkBeetle
employs a bottom-up recovery strategy that uses targeted fault injection at
specific nodes to efficiently infer feature splits and threshold values. Our
proof-of-concept implementation demonstrates that BarkBeetle requires
significantly fewer queries and recovers more structural information compared
to prior approaches, when evaluated on DTs trained with public UCI datasets. To
validate its practical feasibility, we implement BarkBeetle on a Raspberry Pi
RP2350 board and perform fault injections using the Faultier voltage glitching
tool. As BarkBeetle targets general DT models, we also provide an in-depth
discussion on its applicability to a broader range of tree-based applications,
including data stream classification, DT variants, and cryptography schemes.

</details>


### [37] [ZKTorch: Compiling ML Inference to Zero-Knowledge Proofs via Parallel Proof Accumulation](https://arxiv.org/abs/2507.07031)
*Bing-Jyue Chen,Lilia Tang,Daniel Kang*

Main category: cs.CR

TL;DR: ZKTorch提出了一种高效的零知识证明系统，用于验证ML模型推理的正确性，同时保护模型权重。


<details>
  <summary>Details</summary>
Motivation: 随着AI模型的普及，用户对透明度的需求增加，但模型所有者不希望公开权重。现有方法效率低或泛化性差。

Method: ZKTorch将ML模型编译为基本加密操作块，使用专用协议证明，并基于改进的Mira累积方案实现并行化。

Result: ZKTorch将证明大小减少至少3倍，证明速度提升高达6倍。

Conclusion: ZKTorch提供了一种高效且通用的解决方案，适用于快速发展的ML领域。

Abstract: As AI models become ubiquitous in our daily lives, there has been an
increasing demand for transparency in ML services. However, the model owner
does not want to reveal the weights, as they are considered trade secrets. To
solve this problem, researchers have turned to zero-knowledge proofs of ML
model inference. These proofs convince the user that the ML model output is
correct, without revealing the weights of the model to the user. Past work on
these provers can be placed into two categories. The first method compiles the
ML model into a low-level circuit, and proves the circuit using a ZK-SNARK. The
second method uses custom cryptographic protocols designed only for a specific
class of models. Unfortunately, the first method is highly inefficient, making
it impractical for the large models used today, and the second method does not
generalize well, making it difficult to update in the rapidly changing field of
machine learning. To solve this, we propose ZKTorch, an open source end-to-end
proving system that compiles ML models into base cryptographic operations
called basic blocks, each proved using specialized protocols. ZKTorch is built
on top of a novel parallel extension to the Mira accumulation scheme, enabling
succinct proofs with minimal accumulation overhead. These contributions allow
ZKTorch to achieve at least a $3\times$ reduction in the proof size compared to
specialized protocols and up to a $6\times$ speedup in proving time over a
general-purpose ZKML framework.

</details>


### [38] [LoRAShield: Data-Free Editing Alignment for Secure Personalized LoRA Sharing](https://arxiv.org/abs/2507.07056)
*Jiahao Chen,junhao li,Yiming Wang,Zhe Ma,Yi Jiang,Chunyi Zhou,Qingming Li,Tianyu Du,Shouling Ji*

Main category: cs.CR

TL;DR: LoRAShield是一种数据无关的编辑框架，用于保护LoRA模型免受滥用，通过动态编辑和重新对齐权重子空间，有效阻止恶意生成内容。


<details>
  <summary>Details</summary>
Motivation: 随着LoRA模型的普及，共享轻量级模型带来了滥用风险（如生成有害内容），现有防御方法未针对LoRA的模块化特性，因此需要新的解决方案。

Method: 提出LoRAShield框架，通过对抗性优化和语义增强动态编辑LoRA权重子空间。

Result: 实验表明，LoRAShield能高效、鲁棒地阻止恶意生成，同时不影响良性任务功能。

Conclusion: LoRAShield为生成生态系统提供了安全、可扩展的共享方案，推动可信生成模型的发展。

Abstract: The proliferation of Low-Rank Adaptation (LoRA) models has democratized
personalized text-to-image generation, enabling users to share lightweight
models (e.g., personal portraits) on platforms like Civitai and Liblib.
However, this "share-and-play" ecosystem introduces critical risks: benign
LoRAs can be weaponized by adversaries to generate harmful content (e.g.,
political, defamatory imagery), undermining creator rights and platform safety.
Existing defenses like concept-erasure methods focus on full diffusion models
(DMs), neglecting LoRA's unique role as a modular adapter and its vulnerability
to adversarial prompt engineering. To bridge this gap, we propose LoRAShield,
the first data-free editing framework for securing LoRA models against misuse.
Our platform-driven approach dynamically edits and realigns LoRA's weight
subspace via adversarial optimization and semantic augmentation. Experimental
results demonstrate that LoRAShield achieves remarkable effectiveness,
efficiency, and robustness in blocking malicious generations without
sacrificing the functionality of the benign task. By shifting the defense to
platforms, LoRAShield enables secure, scalable sharing of personalized models,
a critical step toward trustworthy generative ecosystems.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [39] [Digital Wargames to Enhance Military Medical Evacuation Decision-Making](https://arxiv.org/abs/2507.06373)
*Jeremy Fischer,Ram Krishnamoorthy,Vishal Kumar,Mahdi Al-Husseini*

Main category: cs.AI

TL;DR: MEWI是一个三维多人模拟工具，用于在课堂环境中模拟医疗后送网络，提升学员对医疗后送任务的理解和协作决策能力。


<details>
  <summary>Details</summary>
Motivation: 医疗后送是美军关键任务之一，但缺乏模拟工具来评估离线和在线决策性能。

Method: 开发了基于Unity的MEWI模拟器，模拟战场约束和不确定性，并引入两个实战场景。

Result: MEWI显著提升了学员对医疗后送经验的学习和协作决策能力。

Conclusion: MEWI是医疗教育高保真训练工具的重要进步，为联合部队的医疗后送教育和操作提供了改进方向。

Abstract: Medical evacuation is one of the United States Army's most storied and
critical mission sets, responsible for efficiently and expediently evacuating
the battlefield ill and injured. Medical evacuation planning involves designing
a robust network of medical platforms and facilities capable of moving and
treating large numbers of casualties. Until now, there has not been a medium to
simulate these networks in a classroom setting and evaluate both offline
planning and online decision-making performance. This work describes the
Medical Evacuation Wargaming Initiative (MEWI), a three-dimensional multiplayer
simulation developed in Unity that replicates battlefield constraints and
uncertainties. MEWI accurately models patient interactions at casualty
collection points, ambulance exchange points, medical treatment facilities, and
evacuation platforms. Two operational scenarios are introduced: an amphibious
island assault in the Pacific and a Eurasian conflict across a sprawling road
and river network. These scenarios pit students against the clock to save as
many casualties as possible while adhering to doctrinal lessons learned during
didactic training. We visualize performance data collected from two iterations
of the MEWI Pacific scenario executed in the United States Army's Medical
Evacuation Doctrine Course. We consider post-wargame Likert survey data from
student participants and external observer notes to identify key planning
decision points, document medical evacuation lessons learned, and quantify
general utility. Results indicate that MEWI participation substantially
improves uptake of medical evacuation lessons learned and co-operative
decision-making. MEWI is a substantial step forward in the field of
high-fidelity training tools for medical education, and our study findings
offer critical insights into improving medical evacuation education and
operations across the joint force.

</details>


### [40] [Representing Prompting Patterns with PDL: Compliance Agent Case Study](https://arxiv.org/abs/2507.06396)
*Mandana Vaziri,Louis Mandel,Yuji Watanabe,Hirokuni Kitahara,Martin Hirzel,Anca Sailer*

Main category: cs.AI

TL;DR: 提出了Prompt Declaration Language (PDL)，一种新的提示表示方法，旨在通过将提示置于核心位置来简化LLM的提示工程，支持手动和自动提示调优，并整合规则代码和外部工具。


<details>
  <summary>Details</summary>
Motivation: 现有提示工程框架要么隐藏复杂性，要么提供不灵活的固定模式，难以支持复杂的代理编程。PDL旨在解决这一问题。

Method: 通过PDL抽象化提示的组合，提供声明式表示，支持优化，并通过案例研究验证其效果。

Result: 在合规代理的案例中，使用PDL调优提示模式比固定模式性能提升高达4倍。

Conclusion: PDL通过简化提示工程和提升可优化性，显著提高了程序员的生产力和LLM的性能。

Abstract: Prompt engineering for LLMs remains complex, with existing frameworks either
hiding complexity behind restrictive APIs or providing inflexible canned
patterns that resist customization -- making sophisticated agentic programming
challenging. We present the Prompt Declaration Language (PDL), a novel approach
to prompt representation that tackles this fundamental complexity by bringing
prompts to the forefront, enabling manual and automatic prompt tuning while
capturing the composition of LLM calls together with rule-based code and
external tools. By abstracting away the plumbing for such compositions, PDL
aims at improving programmer productivity while providing a declarative
representation that is amenable to optimization. This paper demonstrates PDL's
utility through a real-world case study of a compliance agent. Tuning the
prompting pattern of this agent yielded up to 4x performance improvement
compared to using a canned agent and prompt pattern.

</details>


### [41] [Jolting Technologies: Superexponential Acceleration in AI Capabilities and Implications for AGI](https://arxiv.org/abs/2507.06398)
*David Orban*

Main category: cs.AI

TL;DR: 论文研究了Jolting Technologies假说，提出AI能力发展可能呈现超指数增长（加速增长或正三阶导数），并通过理论框架和蒙特卡洛模拟验证检测方法。


<details>
  <summary>Details</summary>
Motivation: 探索AI能力发展的潜在超指数增长模式及其对AGI（通用人工智能）出现的影响，为未来实证研究提供工具。

Method: 开发理论框架，通过蒙特卡洛模拟验证检测方法，分析缩短的创意到行动间隔和迭代AI改进的复合效应。

Result: 建立了jolt动态的数学模型，验证了检测方法的有效性，为理解AI发展轨迹及其后果提供了数学基础。

Conclusion: 研究为未来实证研究提供了工具，并探讨了Jolting Technologies假说若成立可能带来的影响，对研究和政策具有启示意义。

Abstract: This paper investigates the Jolting Technologies Hypothesis, which posits
superexponential growth (increasing acceleration, or a positive third
derivative) in the development of AI capabilities. We develop a theoretical
framework and validate detection methodologies through Monte Carlo simulations,
while acknowledging that empirical validation awaits suitable longitudinal
data. Our analysis focuses on creating robust tools for future empirical
studies and exploring the potential implications should the hypothesis prove
valid. The study examines how factors such as shrinking idea-to-action
intervals and compounding iterative AI improvements drive this jolting pattern.
By formalizing jolt dynamics and validating detection methods through
simulation, this work provides the mathematical foundation necessary for
understanding potential AI trajectories and their consequences for AGI
emergence, offering insights for research and policy.

</details>


### [42] [Comparing Dialectical Systems: Contradiction and Counterexample in Belief Change (Extended Version)](https://arxiv.org/abs/2507.06798)
*Uri Andrews,Luca San Mauro*

Main category: cs.AI

TL;DR: 论文证明了q-辩证系统比p-辩证系统更强大，后者又比d-辩证系统更强，强调了反例和矛盾在自动信念修正中的互补作用。


<details>
  <summary>Details</summary>
Motivation: 研究辩证系统的不同模型（d-、p-、q-辩证系统）在自动信念修正中的能力差异，解决文献中的开放问题。

Method: 通过数学证明比较q-辩证系统、p-辩证系统和d-辩证系统的能力。

Result: q-辩证系统严格强于p-辩证系统，后者又严格强于d-辩证系统。

Conclusion: 反例和矛盾在信念修正中具有互补作用，这对数学家和研究社区的推理过程也有重要意义。

Abstract: Dialectical systems are a mathematical formalism for modeling an agent
updating a knowledge base seeking consistency. Introduced in the 1970s by
Roberto Magari, they were originally conceived to capture how a working
mathematician or a research community refines beliefs in the pursuit of truth.
Dialectical systems also serve as natural models for the belief change of an
automated agent, offering a unifying, computable framework for dynamic belief
management.
  The literature distinguishes three main models of dialectical systems:
(d-)dialectical systems based on revising beliefs when they are seen to be
inconsistent, p-dialectical systems based on revising beliefs based on finding
a counterexample, and q-dialectical systems which can do both. We answer an
open problem in the literature by proving that q-dialectical systems are
strictly more powerful than p-dialectical systems, which are themselves known
to be strictly stronger than (d-)dialectical systems. This result highlights
the complementary roles of counterexample and contradiction in automated belief
revision, and thus also in the reasoning processes of mathematicians and
research communities.

</details>


### [43] [SCC-recursiveness in infinite argumentation (extended version)](https://arxiv.org/abs/2507.06852)
*Uri Andrews,Luca San Mauro*

Main category: cs.AI

TL;DR: 论文提出两种方法将SCC递归语义扩展到无限论证框架（AFs），并通过Baroni和Giacomin的标准评估其有效性，发现方向性在一般情况下失效，但在有限框架中部分语义满足方向性。


<details>
  <summary>Details</summary>
Motivation: SCC递归语义在有限AFs中有效，但在无限AFs中因良基性问题失效，需扩展其适用性。

Method: 提出两种扩展SCC递归语义的方法，并系统评估其满足Baroni和Giacomin的标准。

Result: 方向性在无限AFs中普遍失效，但在有限框架中部分语义满足方向性。

Conclusion: 研究推动了无限论证理论的发展，为处理无界或动态领域的推理系统奠定了基础。

Abstract: Argumentation frameworks (AFs) are a foundational tool in artificial
intelligence for modeling structured reasoning and conflict. SCC-recursiveness
is a well-known design principle in which the evaluation of arguments is
decomposed according to the strongly connected components (SCCs) of the attack
graph, proceeding recursively from "higher" to "lower" components. While
SCC-recursive semantics such as \cft and \stgt have proven effective for finite
AFs, Baumann and Spanring showed the failure of SCC-recursive semantics to
generalize reliably to infinite AFs due to issues with well-foundedness.
  We propose two approaches to extending SCC-recursiveness to the infinite
setting. We systematically evaluate these semantics using Baroni and Giacomin's
established criteria, showing in particular that directionality fails in
general. We then examine these semantics' behavior in finitary frameworks,
where we find some of our semantics satisfy directionality. These results
advance the theory of infinite argumentation and lay the groundwork for
reasoning systems capable of handling unbounded or evolving domains.

</details>


### [44] [Scaling Towards the Information Boundary of Instruction Set: InfinityInstruct-Subject Technical Report](https://arxiv.org/abs/2507.06968)
*Li Du,Hanyu Zhao,Yiming Ju,Tengfei Pan*

Main category: cs.AI

TL;DR: 提出了一种系统化的指令数据构建框架，提升指令数据覆盖面和深度，构建了高质量数据集InfinityInstruct-Subject，实验证明其有效性。


<details>
  <summary>Details</summary>
Motivation: 当前指令数据集虽规模庞大，但在复杂指令和罕见领域任务上表现不足，需提升数据覆盖面和深度。

Method: 采用分层标注系统、信息种子选择算法、进化数据合成过程和模型缺陷诊断与针对性数据生成，形成闭环迭代。

Result: 构建了包含约150万指令的高质量数据集，实验显示其显著提升模型指令跟随能力。

Conclusion: 为指令数据集的高效持续进化提供了理论和实践基础，从数量扩张转向质量提升。

Abstract: Instruction tuning has become a foundation for unlocking the capabilities of
large-scale pretrained models and improving their performance on complex tasks.
Thus, the construction of high-quality instruction datasets is crucial for
enhancing model performance and generalizability. Although current instruction
datasets have reached tens of millions of samples, models finetuned on them may
still struggle with complex instruction following and tasks in rare domains.
This is primarily due to limited expansion in both ``coverage'' (coverage of
task types and knowledge areas) and ``depth'' (instruction complexity) of the
instruction set. To address this issue, we propose a systematic instruction
data construction framework, which integrates a hierarchical labeling system,
an informative seed selection algorithm, an evolutionary data synthesis
process, and a model deficiency diagnosis with targeted data generation. These
components form an iterative closed-loop to continuously enhance the coverage
and depth of instruction data. Based on this framework, we construct
InfinityInstruct-Subject, a high-quality dataset containing ~1.5 million
instructions. Experiments on multiple foundation models and benchmark tasks
demonstrate its effectiveness in improving instruction-following capabilities.
Further analyses suggest that InfinityInstruct-Subject shows enlarged coverage
and depth compared to comparable synthesized instruction datasets. Our work
lays a theoretical and practical foundation for the efficient, continuous
evolution of instruction datasets, moving from data quantity expansion to
qualitative improvement.

</details>


### [45] [The User-Centric Geo-Experience: An LLM-Powered Framework for Enhanced Planning, Navigation, and Dynamic Adaptation](https://arxiv.org/abs/2507.06993)
*Jieren Deng,Aleksandar Cvetkovic,Pak Kiu Chung,Dragomir Yankov,Chiqun Zhang*

Main category: cs.AI

TL;DR: 论文提出了一种动态旅行规划系统，通过三个协作代理解决传统系统的不足，显著提升了查询解释、导航准确性和应对中断的能力。


<details>
  <summary>Details</summary>
Motivation: 传统旅行规划系统静态且碎片化，无法应对现实世界的复杂性和突发变化，导致用户体验不佳。

Method: 提出三个协作代理：旅行规划代理（基于网格空间和地图分析）、目的地助手代理（精细导航）、本地发现代理（利用图像嵌入和RAG技术应对中断）。

Result: 系统在查询解释、导航准确性和中断恢复方面表现显著提升。

Conclusion: 该系统在从城市探索到应急响应等多个领域具有应用潜力。

Abstract: Traditional travel-planning systems are often static and fragmented, leaving
them ill-equipped to handle real-world complexities such as evolving
environmental conditions and unexpected itinerary disruptions. In this paper,
we identify three gaps between existing service providers causing frustrating
user experience: intelligent trip planning, precision "last-100-meter"
navigation, and dynamic itinerary adaptation. We propose three cooperative
agents: a Travel Planning Agent that employs grid-based spatial grounding and
map analysis to help resolve complex multi-modal user queries; a Destination
Assistant Agent that provides fine-grained guidance for the final navigation
leg of each journey; and a Local Discovery Agent that leverages image
embeddings and Retrieval-Augmented Generation (RAG) to detect and respond to
trip plan disruptions. With evaluations and experiments, our system
demonstrates substantial improvements in query interpretation, navigation
accuracy, and disruption resilience, underscoring its promise for applications
from urban exploration to emergency response.

</details>


### [46] [First Return, Entropy-Eliciting Explore](https://arxiv.org/abs/2507.07017)
*Tianyu Zheng,Tianshun Xing,Qingshui Gu,Taoran Liang,Xingwei Qu,Xin Zhou,Yizhi Li,Zhoufutu Wen,Chenghua Lin,Wenhao Huang,Qian Liu,Ge Zhang,Zejun Ma*

Main category: cs.AI

TL;DR: FR3E框架通过结构化探索提升LLM推理能力，解决了RLVR的不稳定探索问题。


<details>
  <summary>Details</summary>
Motivation: RLVR虽能提升LLM推理能力，但存在探索不稳定的问题。

Method: 提出FR3E框架，识别高不确定性决策点，进行针对性探索以生成语义明确的中间反馈。

Result: 在数学推理基准测试中，FR3E提升了训练稳定性、生成长且连贯的响应，并增加完全正确轨迹的比例。

Conclusion: FR3E通过结构化探索有效提升了LLM的推理能力。

Abstract: Reinforcement Learning from Verifiable Rewards (RLVR) improves the reasoning
abilities of Large Language Models (LLMs) but it struggles with unstable
exploration. We propose FR3E (First Return, Entropy-Eliciting Explore), a
structured exploration framework that identifies high-uncertainty decision
points in reasoning trajectories and performs targeted rollouts to construct
semantically grounded intermediate feedback. Our method provides targeted
guidance without relying on dense supervision. Empirical results on
mathematical reasoning benchmarks(AIME24) show that FR3E promotes more stable
training, produces longer and more coherent responses, and increases the
proportion of fully correct trajectories. These results highlight the
framework's effectiveness in improving LLM reasoning through more robust and
structured exploration.

</details>
