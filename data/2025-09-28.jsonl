{"id": "2509.20380", "categories": ["cs.SE", "cs.AI", "cs.PL"], "pdf": "https://arxiv.org/pdf/2509.20380", "abs": "https://arxiv.org/abs/2509.20380", "authors": ["Samyak Jhaveri", "Vanessa Klotzmann", "Crista Lopes"], "title": "ACCeLLiuM: Supervised Fine-Tuning for Automated OpenACC Pragma Generation", "comment": null, "summary": "The increasing ubiquity of GPUs is accompanied by the increasing complexity\nof their hardware and parallel programming frameworks. Directive-based parallel\nprogramming standards like OpenACC simplify GPU programming to some extent by\nabstracting away low-level complexities, but a fair amount of expertise is\nstill required in order to use those directives effectively.\n  We introduce ACCeLLiuM, two open weights Large Language Models specifically\nfine-tuned for generating expert OpenACC directives for data-parallel loops,\nalong with the supervised fine-tuning dataset that was used to train them. The\nACCeLLiuM SFT dataset contains 4,033 OpenACC pragma-loop pairs mined from\npublic GitHub C/C++ repositories, with 3,223 pairs for training and 810 for\ntesting. Experimental evaluations show a pronounced performance gap in\ngenerating correct OpenACC pragmas between base LLMs and our fine-tuned\nversions. On the held-out test set, base LLMs fail to consistently generate\nvalid pragmas, whereas LLMs fine-tuned on the ACCeLLiuM dataset generate valid\npragmas with the correct directive type for $87\\%$ of the data-parallel loops,\nand exact pragmas--including directives, clauses, clause order, and clause\nvariables--for $50\\%$ of the cases. Even when not exact, generated pragmas\nfrequently incorporate the correct clauses in a different order than the\nground-truth label, or include additional clauses that enable finer control\nover parallel execution, data movement, and concurrency, offering practical\nvalue beyond strict string-matching. By publicly releasing the code, models,\nand dataset as ACCeLLiuM we hope to establish a reproducible benchmark for\nLLM-powered OpenACC pragma generation, and lower the barrier to automated GPU\noffloading of serially written programs."}
{"id": "2509.20385", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.20385", "abs": "https://arxiv.org/abs/2509.20385", "authors": ["Ishara Devendra", "Chaman Wijesiriwardana", "Prasad Wimalaratne"], "title": "State-of-the-Art in Software Security Visualization: A Systematic Review", "comment": null, "summary": "Software security visualization is an interdisciplinary field that combines\nthe technical complexity of cybersecurity, including threat intelligence and\ncompliance monitoring, with visual analytics, transforming complex security\ndata into easily digestible visual formats. As software systems get more\ncomplex and the threat landscape evolves, traditional text-based and numerical\nmethods for analyzing and interpreting security concerns become increasingly\nineffective. The purpose of this paper is to systematically review existing\nresearch and create a comprehensive taxonomy of software security visualization\ntechniques through literature, categorizing these techniques into four types:\ngraph-based, notation-based, matrix-based, and metaphor-based visualization.\nThis systematic review explores over 60 recent key research papers in software\nsecurity visualization, highlighting its key issues, recent advancements, and\nprospective future research directions. From the comprehensive analysis, the\ntwo main areas were distinctly highlighted as extensive software development\nvisualization, focusing on advanced methods for depicting software\narchitecture: operational security visualization and cybersecurity\nvisualization. The findings highlight the necessity for innovative\nvisualization techniques that adapt to the evolving security landscape, with\npractical implications for enhancing threat detection, improving security\nresponse strategies, and guiding future research."}
{"id": "2509.20386", "categories": ["cs.SE", "cs.AI", "cs.IR"], "pdf": "https://arxiv.org/pdf/2509.20386", "abs": "https://arxiv.org/abs/2509.20386", "authors": ["Nishant Gaurav", "Adit Akarsh", "Ankit Ranjan", "Manoj Bajaj"], "title": "Dynamic ReAct: Scalable Tool Selection for Large-Scale MCP Environments", "comment": null, "summary": "We present Dynamic ReAct, a novel approach for enabling ReAct agents to ef-\nficiently operate with extensive Model Control Protocol (MCP) tool sets that\nexceed the contextual memory limitations of large language models. Our approach\naddresses the fundamental challenge of tool selection in environments\ncontaining hundreds or thousands of available tools, where loading all tools\nsimultaneously is computationally infeasible. We propose and evaluate five\ndistinct architectures that progressively refine the tool selection process,\nculminating in a search-and-load mechanism that achieves intelligent tool\nselection with minimal computational overhead. Our experimental results\ndemonstrate that the proposed approach reduces tool loading by up to 50% while\nmaintaining task completion accuracy, advancing the path towards truly\ngeneral-purpose AI agents capable of dynamically adapting to diverse task\nenvironments."}
{"id": "2509.20387", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.20387", "abs": "https://arxiv.org/abs/2509.20387", "authors": ["Qusai Ramadan", "Jukka Ruohonen", "Abhishek Tiwari", "Adam Alami", "Zeyd Boukhers"], "title": "Towards Systematic Specification and Verification of Fairness Requirements: A Position Paper", "comment": "Accepted at the 2025 IEEE 33rd International Requirements Engineering\n  Conference Workshops", "summary": "Decisions suggested by improperly designed software systems might be prone to\ndiscriminate against people based on protected characteristics, such as gender\nand ethnicity. Previous studies attribute such undesired behavior to flaws in\nalgorithmic design or biased data. However, these studies ignore that\ndiscrimination is often the result of a lack of well-specified fairness\nrequirements and their verification. The fact that experts' knowledge about\nfairness is often implicit makes the task of specifying precise and verifiable\nfairness requirements difficult. In related domains, such as security\nengineering, knowledge graphs have been proven to be effective in formalizing\nknowledge to assist requirements specification and verification. To address the\nlack of formal mechanisms for specifying and verifying fairness requirements,\nwe propose the development of a knowledge graph-based framework for fairness.\nIn this paper, we discuss the challenges, research questions, and a road map\ntowards addressing the research questions."}
{"id": "2509.20382", "categories": ["cs.CR", "cs.AI", "cs.LG", "eess.SP"], "pdf": "https://arxiv.org/pdf/2509.20382", "abs": "https://arxiv.org/abs/2509.20382", "authors": ["Dilli Hang Rai", "Sabin Kafley"], "title": "Lightweight MobileNetV1+GRU for ECG Biometric Authentication: Federated and Adversarial Evaluation", "comment": "5 pages, 7 figures, 5 tables", "summary": "ECG biometrics offer a unique, secure authentication method, yet their\ndeployment on wearable devices faces real-time processing, privacy, and\nspoofing vulnerability challenges. This paper proposes a lightweight deep\nlearning model (MobileNetV1+GRU) for ECG-based authentication, injection of\n20dB Gaussian noise & custom preprocessing. We simulate wearable conditions and\nedge deployment using the ECGID, MIT-BIH, CYBHi, and PTB datasets, achieving\naccuracies of 99.34%, 99.31%, 91.74%, and 98.49%, F1-scores of 0.9869, 0.9923,\n0.9125, and 0.9771, Precision of 0.9866, 0.9924, 0.9180 and 0.9845, Recall of\n0.9878, 0.9923, 0.9129, and 0.9756, equal error rates (EER) of 0.0009, 0.00013,\n0.0091, and 0.0009, and ROC-AUC values of 0.9999, 0.9999, 0.9985, and 0.9998,\nwhile under FGSM adversarial attacks, accuracy drops from 96.82% to as low as\n0.80%. This paper highlights federated learning, adversarial testing, and the\nneed for diverse wearable physiological datasets to ensure secure and scalable\nbiometrics."}
{"id": "2509.20364", "categories": ["cs.AI", "cs.ET"], "pdf": "https://arxiv.org/pdf/2509.20364", "abs": "https://arxiv.org/abs/2509.20364", "authors": ["Thomas J Sheffler"], "title": "An Approach to Checking Correctness for Agentic Systems", "comment": "15 pages, 5 figures", "summary": "This paper presents a temporal expression language for monitoring AI agent\nbehavior, enabling systematic error-detection of LLM-based agentic systems that\nexhibit variable outputs due to stochastic generation processes. Drawing from\ntemporal logic techniques used in hardware verification, this approach monitors\nexecution traces of agent tool calls and state transitions to detect deviations\nfrom expected behavioral patterns. Current error-detection approaches rely\nprimarily on text matching of inputs and outputs, which proves fragile due to\nthe natural language variability inherent in LLM responses. The proposed method\ninstead focuses on the sequence of agent actions -- such as tool invocations\nand inter-agent communications -- allowing verification of system behavior\nindependent of specific textual outputs. The temporal expression language\nprovides assertions that capture correct behavioral patterns across multiple\nexecution scenarios. These assertions serve dual purposes: validating prompt\nengineering and guardrail effectiveness during development, and providing\nregression testing when agents are updated with new LLMs or modified logic. The\napproach is demonstrated using a three-agent system, where agents coordinate to\nsolve multi-step reasoning tasks. When powered by large, capable models, all\ntemporal assertions were satisfied across many test runs. However, when smaller\nmodels were substituted in two of the three agents, executions violated\nbehavioral assertions, primarily due to improper tool sequencing and failed\ncoordination handoffs. The temporal expressions successfully flagged these\nanomalies, demonstrating the method's effectiveness for detecting behavioral\nregressions in production agentic systems. This approach provides a foundation\nfor systematic monitoring of AI agent reliability as these systems become\nincreasingly deployed in critical applications."}
{"id": "2509.20415", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.20415", "abs": "https://arxiv.org/abs/2509.20415", "authors": ["Yu Pan", "Xiaocheng Li", "Hanzhao Wang"], "title": "Online-Optimized RAG for Tool Use and Function Calling", "comment": null, "summary": "In many applications, retrieval-augmented generation (RAG) drives tool use\nand function calling by embedding the (user) queries and matching them to\npre-specified tool/function descriptions. In this paper, we address an\nembedding misalignment issue that often arises in practical applications due to\nimperfect embedding models or noisy descriptions; such misalignment may lead to\nincorrect retrieval and task failure. We introduce Online-Optimized RAG, a\ndeployment-time framework that continually adapts retrieval embeddings from\nlive interactions using minimal feedback (e.g., task success). Online-Optimized\nRAG applies lightweight online gradient updates with negligible per-query\nlatency and requires no changes to the underlying LLM. The method is\nplug-and-play: it supports both single- and multi-hop tool use, dynamic tool\ninventories, and $K$-retrieval with re-ranking. We provide a problem-dependent\ntheoretical analysis that quantifies how the method's performance depends on\nthe initialization quality of the embeddings and other related quantities.\nAcross diverse tool-use and document-retrieval scenarios, our Online-Optimized\nRAG consistently improves tool selection accuracy and end-task success, thus\nproviding a simple, practical path to robust, self-improving RAG systems."}
{"id": "2509.20383", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.20383", "abs": "https://arxiv.org/abs/2509.20383", "authors": ["Wei Wan", "Yuxuan Ning", "Zhicong Huang", "Cheng Hong", "Shengshan Hu", "Ziqi Zhou", "Yechao Zhang", "Tianqing Zhu", "Wanlei Zhou", "Leo Yu Zhang"], "title": "MARS: A Malignity-Aware Backdoor Defense in Federated Learning", "comment": "NeurIPS 2025", "summary": "Federated Learning (FL) is a distributed paradigm aimed at protecting\nparticipant data privacy by exchanging model parameters to achieve high-quality\nmodel training. However, this distributed nature also makes FL highly\nvulnerable to backdoor attacks. Notably, the recently proposed state-of-the-art\n(SOTA) attack, 3DFed (SP2023), uses an indicator mechanism to determine whether\nthe backdoor models have been accepted by the defender and adaptively optimizes\nbackdoor models, rendering existing defenses ineffective. In this paper, we\nfirst reveal that the failure of existing defenses lies in the employment of\nempirical statistical measures that are loosely coupled with backdoor attacks.\nMotivated by this, we propose a Malignity-Aware backdooR defenSe (MARS) that\nleverages backdoor energy (BE) to indicate the malicious extent of each neuron.\nTo amplify malignity, we further extract the most prominent BE values from each\nmodel to form a concentrated backdoor energy (CBE). Finally, a novel\nWasserstein distance-based clustering method is introduced to effectively\nidentify backdoor models. Extensive experiments demonstrate that MARS can\ndefend against SOTA backdoor attacks and significantly outperforms existing\ndefenses."}
{"id": "2509.20368", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.20368", "abs": "https://arxiv.org/abs/2509.20368", "authors": ["Theo Uscidda", "Matthew Trager", "Michael Kleinman", "Aditya Chattopadhyay", "Wei Xia", "Stefano Soatto"], "title": "LATTS: Locally Adaptive Test-Time Scaling", "comment": null, "summary": "One common strategy for improving the performance of Large Language Models\n(LLMs) on downstream tasks involves using a \\emph{verifier model} to either\nselect the best answer from a pool of candidates or to steer the\nauto-regressive generation process towards better outputs. This class of\nmethods typically results in improved accuracy at the cost of increased\ncomputation at test-time, a paradigm known as \\emph{test-time scaling}.\nHowever, most existing approaches increase computation uniformly across all\nsamples and generation steps, without considering the complexity of individual\ninstances, leading to inefficient resource use. We address this limitation by\nproposing an approach, called \\emph{Locally Adaptive Test-Time Scaling\n(LATTS)}, that allocates variable compute across generation steps.\nSpecifically, at each generation step, LATTS employs a verifier-based\nacceptance criterion to decide whether to resample, backtrack, restart, or stop\nthe generation process. This criterion effectively adjusts the per-step\ncomputational effort based on a precise notion of \\emph{local difficulty}\nderived from the verifier model. Empirical results show that LATTS achieves\nsignificantly superior accuracy--compute tradeoffs compared to standard\nverifier-based methods."}
{"id": "2509.20421", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.20421", "abs": "https://arxiv.org/abs/2509.20421", "authors": ["Reiner Hähnle", "Cosimo Laneve", "Adele Veschetti"], "title": "Formal Verification of Legal Contracts: A Translation-based Approach", "comment": null, "summary": "Stipula is a domain-specific programming language designed to model legal\ncontracts with enforceable properties, especially those involving asset\ntransfers and obligations. This paper presents a methodology to formally verify\nthe correctness of Stipula contracts through translation into Java code\nannotated with Java Modeling Language specifications. As a verification\nbackend, the deductive verification tool KeY is used. Both, the translation and\nthe verification of partial and total correctness for a large subset of Stipula\ncontracts, those with disjoint cycles, is fully automatic. Our work\ndemonstrates that a general-purpose deductive verification tool can be used\nsuccessfully in a translation approach."}
{"id": "2509.20384", "categories": ["cs.CR", "cs.AI", "cs.PL", "cs.SE"], "pdf": "https://arxiv.org/pdf/2509.20384", "abs": "https://arxiv.org/abs/2509.20384", "authors": ["Jiayi Lin", "Liangcai Su", "Junzhe Li", "Chenxiong Qian"], "title": "R1-Fuzz: Specializing Language Models for Textual Fuzzing via Reinforcement Learning", "comment": null, "summary": "Fuzzing is effective for vulnerability discovery but struggles with complex\ntargets such as compilers, interpreters, and database engines, which accept\ntextual input that must satisfy intricate syntactic and semantic constraints.\nAlthough language models (LMs) have attracted interest for this task due to\ntheir vast latent knowledge and reasoning potential, their practical adoption\nhas been limited. The major challenges stem from insufficient exploration of\ndeep program logic among real-world codebases, and the high cost of leveraging\nlarger models. To overcome these challenges, we propose R1-Fuzz, the first\nframework that leverages reinforcement learning (RL) to specialize\ncost-efficient LMs and integrate them for complex textual fuzzing input\ngeneration. R1-Fuzz introduces two key designs: coverage-slicing-based question\nconstruction and a distance-based reward calculation. Through RL-based\npost-training of a model with our constructed dataset, R1-Fuzz designs a\nfuzzing workflow that tightly integrates LMs to reason deep program semantics\nduring fuzzing. Evaluations on diverse real-world targets show that our design\nenables a small model, named R1-Fuzz-7B, to rival or even outperform much\nlarger models in real-world fuzzing. Notably, R1-Fuzz achieves up to 75\\%\nhigher coverage than state-of-the-art fuzzers and discovers 29 previously\nunknown vulnerabilities, demonstrating its practicality."}
{"id": "2509.20370", "categories": ["cs.AI", "cs.CY", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.20370", "abs": "https://arxiv.org/abs/2509.20370", "authors": ["MZ Naser"], "title": "Philosophy-informed Machine Learning", "comment": null, "summary": "Philosophy-informed machine learning (PhIML) directly infuses core ideas from\nanalytic philosophy into ML model architectures, objectives, and evaluation\nprotocols. Therefore, PhIML promises new capabilities through models that\nrespect philosophical concepts and values by design. From this lens, this paper\nreviews conceptual foundations to demonstrate philosophical gains and\nalignment. In addition, we present case studies on how ML users/designers can\nadopt PhIML as an agnostic post-hoc tool or intrinsically build it into ML\nmodel architectures. Finally, this paper sheds light on open technical barriers\nalongside philosophical, practical, and governance challenges and outlines a\nresearch roadmap toward safe, philosophy-aware, and ethically responsible\nPhIML."}
{"id": "2509.20491", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.20491", "abs": "https://arxiv.org/abs/2509.20491", "authors": ["Brahim Mahmoudi", "Naouel Moha", "Quentin Stievenert", "Florent Avellaneda"], "title": "AI-Specific Code Smells: From Specification to Detection", "comment": null, "summary": "The rise of Artificial Intelligence (AI) is reshaping how software systems\nare developed and maintained. However, AI-based systems give rise to new\nsoftware issues that existing detection tools often miss. Among these, we focus\non AI-specific code smells, recurring patterns in the code that may indicate\ndeeper problems such as unreproducibility, silent failures, or poor model\ngeneralization. We introduce SpecDetect4AI, a tool-based approach for the\nspecification and detection of these code smells at scale. This approach\ncombines a high-level declarative Domain-Specific Language (DSL) for rule\nspecification with an extensible static analysis tool that interprets and\ndetects these rules for AI-based systems. We specified 22 AI-specific code\nsmells and evaluated SpecDetect4AI on 826 AI-based systems (20M lines of code),\nachieving a precision of 88.66% and a recall of 88.89%, outperforming other\nexisting detection tools. Our results show that SpecDetect4AI supports the\nspecification and detection of AI-specific code smells through dedicated rules\nand can effectively analyze large AI-based systems, demonstrating both\nefficiency and extensibility (SUS 81.7/100)."}
{"id": "2509.20388", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.20388", "abs": "https://arxiv.org/abs/2509.20388", "authors": ["Amir AL-Maamari"], "title": "Can You Trust Your Copilot? A Privacy Scorecard for AI Coding Assistants", "comment": null, "summary": "The rapid integration of AI-powered coding assistants into developer\nworkflows has raised significant privacy and trust concerns. As developers\nentrust proprietary code to services like OpenAI's GPT, Google's Gemini, and\nGitHub Copilot, the unclear data handling practices of these tools create\nsecurity and compliance risks. This paper addresses this challenge by\nintroducing and applying a novel, expert-validated privacy scorecard. The\nmethodology involves a detailed analysis of four document types; from legal\npolicies to external audits; to score five leading assistants against 14\nweighted criteria. A legal expert and a data protection officer refined these\ncriteria and their weighting. The results reveal a distinct hierarchy of\nprivacy protections, with a 20-point gap between the highest- and lowest-ranked\ntools. The analysis uncovers common industry weaknesses, including the\npervasive use of opt-out consent for model training and a near-universal\nfailure to filter secrets from user prompts proactively. The resulting\nscorecard provides actionable guidance for developers and organizations,\nenabling evidence-based tool selection. This work establishes a new benchmark\nfor transparency and advocates for a shift towards more user-centric privacy\nstandards in the AI industry."}
{"id": "2509.20493", "categories": ["cs.AI", "cs.CL", "cs.DL", "cs.HC"], "pdf": "https://arxiv.org/pdf/2509.20493", "abs": "https://arxiv.org/abs/2509.20493", "authors": ["Paris Koloveas", "Serafeim Chatzopoulos", "Thanasis Vergoulis", "Christos Tryfonopoulos"], "title": "InsightGUIDE: An Opinionated AI Assistant for Guided Critical Reading of Scientific Literature", "comment": "Accepted for publication on ICTAI 2025", "summary": "The proliferation of scientific literature presents an increasingly\nsignificant challenge for researchers. While Large Language Models (LLMs) offer\npromise, existing tools often provide verbose summaries that risk replacing,\nrather than assisting, the reading of the source material. This paper\nintroduces InsightGUIDE, a novel AI-powered tool designed to function as a\nreading assistant, not a replacement. Our system provides concise, structured\ninsights that act as a \"map\" to a paper's key elements by embedding an expert's\nreading methodology directly into its core AI logic. We present the system's\narchitecture, its prompt-driven methodology, and a qualitative case study\ncomparing its output to a general-purpose LLM. The results demonstrate that\nInsightGUIDE produces more structured and actionable guidance, serving as a\nmore effective tool for the modern researcher."}
{"id": "2509.20497", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.20497", "abs": "https://arxiv.org/abs/2509.20497", "authors": ["Ahmed Aljohani", "Hyunsook Do"], "title": "PromptDebt: A Comprehensive Study of Technical Debt Across LLM Projects", "comment": "Accepted at Proceedings of the 2025 Evaluation and Assessment in\n  Software Engineering (EASE '25)", "summary": "Large Language Models (LLMs) are increasingly embedded in software via APIs\nlike OpenAI, offering powerful AI features without heavy infrastructure. Yet\nthese integrations bring their own form of self-admitted technical debt (SATD).\nIn this paper, we present the first large-scale empirical study of LLM-specific\nSATD: its origins, prevalence, and mitigation strategies. By analyzing 93,142\nPython files across major LLM APIs, we found that 54.49% of SATD instances stem\nfrom OpenAI integrations and 12.35% from LangChain use. Prompt design emerged\nas the primary source of LLM-specific SATD, with 6.61% of debt related to\nprompt configuration and optimization issues, followed by hyperparameter tuning\nand LLM-framework integration. We further explored which prompt techniques\nattract the most debt, revealing that instruction-based prompts (38.60%) and\nfew-shot prompts (18.13%) are particularly vulnerable due to their dependence\non instruction clarity and example quality. Finally, we release a comprehensive\nSATD dataset to support reproducibility and offer practical guidance for\nmanaging technical debt in LLM-powered systems."}
{"id": "2509.20391", "categories": ["cs.CR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.20391", "abs": "https://arxiv.org/abs/2509.20391", "authors": ["Md. Alamgir Hossain", "Waqas Ishtiaq", "Md. Samiul Islam"], "title": "A Comparative Analysis of Ensemble-Based Machine Learning Approaches with Explainable AI for Multi-Class Intrusion Detection in Drone Networks", "comment": "27 pages, 18 figures, 10 tables", "summary": "The growing integration of drones into civilian, commercial, and defense\nsectors introduces significant cybersecurity concerns, particularly with the\nincreased risk of network-based intrusions targeting drone communication\nprotocols. Detecting and classifying these intrusions is inherently challenging\ndue to the dynamic nature of drone traffic and the presence of multiple\nsophisticated attack vectors such as spoofing, injection, replay, and\nman-in-the-middle (MITM) attacks. This research aims to develop a robust and\ninterpretable intrusion detection framework tailored for drone networks, with a\nfocus on handling multi-class classification and model explainability. We\npresent a comparative analysis of ensemble-based machine learning models,\nnamely Random Forest, Extra Trees, AdaBoost, CatBoost, and XGBoost, trained on\na labeled dataset comprising benign traffic and nine distinct intrusion types.\nComprehensive data preprocessing was performed, including missing value\nimputation, scaling, and categorical encoding, followed by model training and\nextensive evaluation using metrics such as macro F1-score, ROC AUC, Matthews\nCorrelation Coefficient, and Log Loss. Random Forest achieved the highest\nperformance with a macro F1-score of 0.9998 and ROC AUC of 1.0000. To validate\nthe superiority of the models, statistical tests, including Friedmans test, the\nWilcoxon signed-rank test with Holm correction, and bootstrapped confidence\nintervals, were applied. Furthermore, explainable AI methods, SHAP and LIME,\nwere integrated to interpret both global and local feature importance,\nenhancing model transparency and decision trustworthiness. The proposed\napproach not only delivers near-perfect accuracy but also ensures\ninterpretability, making it highly suitable for real-time and safety-critical\ndrone operations."}
{"id": "2509.20513", "categories": ["cs.AI", "cs.DC"], "pdf": "https://arxiv.org/pdf/2509.20513", "abs": "https://arxiv.org/abs/2509.20513", "authors": ["Samer Alshaer", "Ala Khalifeh", "Roman Obermaisser"], "title": "Reconstruction-Based Adaptive Scheduling Using AI Inferences in Safety-Critical Systems", "comment": "14 pages, 10 figures", "summary": "Adaptive scheduling is crucial for ensuring the reliability and safety of\ntime-triggered systems (TTS) in dynamic operational environments. Scheduling\nframeworks face significant challenges, including message collisions, locked\nloops from incorrect precedence handling, and the generation of incomplete or\ninvalid schedules, which can compromise system safety and performance. To\naddress these challenges, this paper presents a novel reconstruction framework\ndesigned to dynamically validate and assemble schedules. The proposed\nreconstruction models operate by systematically transforming AI-generated or\nheuristically derived scheduling priorities into fully executable schedules,\nensuring adherence to critical system constraints such as precedence rules and\ncollision-free communication. It incorporates robust safety checks, efficient\nallocation algorithms, and recovery mechanisms to handle unexpected context\nevents, including hardware failures and mode transitions. Comprehensive\nexperiments were conducted across multiple performance profiles, including\nmakespan minimisation, workload balancing, and energy efficiency, to validate\nthe operational effectiveness of the reconstruction models. Results demonstrate\nthat the proposed framework significantly enhances system adaptability,\noperational integrity, and runtime performance while maintaining computational\nefficiency. Overall, this work contributes a practical and scalable solution to\nthe problem of safe schedule generation in safety-critical TTS, enabling\nreliable and flexible real-time scheduling even under highly dynamic and\nuncertain operational conditions."}
{"id": "2509.20518", "categories": ["cs.SE", "cs.PL", "D.2.3"], "pdf": "https://arxiv.org/pdf/2509.20518", "abs": "https://arxiv.org/abs/2509.20518", "authors": ["Sayed Mahbub Hasan Amiri", "Md Mainul Islam"], "title": "Enhancing Python Programming Education with an AI-Powered Code Helper: Design, Implementation, and Impact", "comment": "20 pages, 16 figures", "summary": "This is the study that presents an AI-Python-based chatbot that helps\nstudents to learn programming by demonstrating solutions to such problems as\ndebugging errors, solving syntax problems or converting abstract theoretical\nconcepts to practical implementations. Traditional coding tools like Integrated\nDevelopment Environments (IDEs) and static analyzers do not give robotic help\nwhile AI-driven code assistants such as GitHub Copilot focus on getting things\ndone. To close this gap, our chatbot combines static code analysis, dynamic\nexecution tracing, and large language models (LLMs) to provide the students\nwith relevant and practical advice, hence promoting the learning process. The\nchatbots hybrid architecture employs CodeLlama for code embedding, GPT-4 for\nnatural language interactions, and Docker-based sandboxing for secure\nexecution. Evaluated through a mixed-methods approach involving 1,500 student\nsubmissions, the system demonstrated an 85% error resolution success rate,\noutperforming standalone tools like pylint (62%) and GPT-4 (73%). Quantitative\nresults revealed a 59.3% reduction in debugging time among users, with pre- and\npost-test assessments showing a 34% improvement in coding proficiency,\nparticularly in recursion and exception handling. Qualitative feedback from 120\nstudents highlighted the chatbots clarity, accessibility, and\nconfidence-building impact, though critiques included occasional latency and\nrestrictive code sanitization. By balancing technical innovation with\npedagogical empathy, this research provides a blueprint for AI tools that\nprioritize educational equity and long-term skill retention over mere code\ncompletion. The chatbot exemplifies how AI can augment human instruction,\nfostering deeper conceptual understanding in programming education."}
{"id": "2509.20395", "categories": ["cs.CR", "cs.AI", "cs.DC"], "pdf": "https://arxiv.org/pdf/2509.20395", "abs": "https://arxiv.org/abs/2509.20395", "authors": ["Noam Schmitt", "Marc Antoine Lacoste"], "title": "Centralized vs. Decentralized Security for Space AI Systems? A New Look", "comment": "IEEE HPEC 2025 - 29th Annual IEEE High Performance Extreme Computing\n  Virtual Conference, MIT Lincoln Laboratory, Sep 2025, Boston (MA), United\n  States", "summary": "This paper investigates the trade-off between centralized and decentralized\nsecurity management in constellations of satellites to balance security and\nperformance. We highlight three key AI architectures for automated security\nmanagement: (a) centralized, (b) distributed and (c) federated. The centralized\narchitecture is the best option short term, providing fast training, despite\nthe hard challenge of the communication latency overhead across space.\nDecentralized architectures are better alternatives in the longer term,\nproviding enhanced scalability and security."}
{"id": "2509.20520", "categories": ["cs.AI", "cs.DC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.20520", "abs": "https://arxiv.org/abs/2509.20520", "authors": ["Samer Alshaer", "Ala Khalifeh", "Roman Obermaisser"], "title": "Adaptive Approach to Enhance Machine Learning Scheduling Algorithms During Runtime Using Reinforcement Learning in Metascheduling Applications", "comment": "18 pages, 21 figures", "summary": "Metascheduling in time-triggered architectures has been crucial in adapting\nto dynamic and unpredictable environments, ensuring the reliability and\nefficiency of task execution. However, traditional approaches face significant\nchallenges when training Artificial Intelligence (AI) scheduling inferences\noffline, particularly due to the complexities involved in constructing a\ncomprehensive Multi-Schedule Graph (MSG) that accounts for all possible\nscenarios. The process of generating an MSG that captures the vast probability\nspace, especially when considering context events like hardware failures, slack\nvariations, or mode changes, is resource-intensive and often infeasible. To\naddress these challenges, we propose an adaptive online learning unit\nintegrated within the metascheduler to enhance performance in real-time. The\nprimary motivation for developing this unit stems from the limitations of\noffline training, where the MSG created is inherently a subset of the complete\nspace, focusing only on the most probable and critical context events. In the\nonline mode, Reinforcement Learning (RL) plays a pivotal role by continuously\nexploring and discovering new scheduling solutions, thus expanding the MSG and\nenhancing system performance over time. This dynamic adaptation allows the\nsystem to handle unexpected events and complex scheduling scenarios more\neffectively. Several RL models were implemented within the online learning\nunit, each designed to address specific challenges in scheduling. These models\nnot only facilitate the discovery of new solutions but also optimize existing\nschedulers, particularly when stricter deadlines or new performance criteria\nare introduced. By continuously refining the AI inferences through real-time\ntraining, the system remains flexible and capable of meeting evolving demands,\nthus ensuring robustness and efficiency in large-scale, safety-critical\nenvironments."}
{"id": "2509.20552", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.20552", "abs": "https://arxiv.org/abs/2509.20552", "authors": ["Xinyu Shi", "Zhenhao Li", "An Ran Chen"], "title": "Enhancing LLM-based Fault Localization with a Functionality-Aware Retrieval-Augmented Generation Framework", "comment": null, "summary": "Fault localization (FL) is a critical but time-consuming task in software\ndebugging, aiming to identify faulty code elements. While recent advances in\nlarge language models (LLMs) have shown promise for FL, they often struggle\nwith complex systems due to the lack of project-specific knowledge and the\ndifficulty of navigating large projects. To address these limitations, we\npropose FaR-Loc, a novel framework that enhances method-level FL by integrating\nLLMs with retrieval-augmented generation (RAG). FaR-Loc consists of three key\ncomponents: LLM Functionality Extraction, Semantic Dense Retrieval, and LLM\nRe-ranking. First, given a failed test and its associated stack trace, the LLM\nFunctionality Extraction module generates a concise natural language\ndescription that captures the failing behavior. Next, the Semantic Dense\nRetrieval component leverages a pre-trained code-understanding encoder to embed\nboth the functionality description (natural language) and the covered methods\n(code) into a shared semantic space, enabling the retrieval of methods with\nsimilar functional behavior. Finally, the LLM Re-ranking module reorders the\nretrieved methods based on their contextual relevance. Our experiments on the\nwidely used Defects4J benchmark show that FaR-Loc outperforms state-of-the-art\nLLM-based baselines SoapFL and AutoFL, by 14.6% and 9.1% in Top-1 accuracy, by\n19.2% and 22.1% in Top-5 accuracy, respectively. It also surpasses all\nlearning-based and spectrum-based baselines across all Top-N metrics without\nrequiring re-training. Furthermore, we find that pre-trained code embedding\nmodels that incorporate code structure, such as UniXcoder, can significantly\nimprove fault localization performance by up to 49.0% in Top-1 accuracy.\nFinally, we conduct a case study to illustrate the effectiveness of FaR-Loc and\nto provide insights for its practical application."}
{"id": "2509.20399", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.20399", "abs": "https://arxiv.org/abs/2509.20399", "authors": ["Birk Torpmann-Hagen", "Michael A. Riegler", "Pål Halvorsen", "Dag Johansen"], "title": "Defending against Stegomalware in Deep Neural Networks with Permutation Symmetry", "comment": null, "summary": "Deep neural networks are being utilized in a growing number of applications,\nboth in production systems and for personal use. Network checkpoints are as a\nconsequence often shared and distributed on various platforms to ease the\ndevelopment process. This work considers the threat of neural network\nstegomalware, where malware is embedded in neural network checkpoints at a\nnegligible cost to network accuracy. This constitutes a significant security\nconcern, but is nevertheless largely neglected by the deep learning\npractitioners and security specialists alike. We propose the first effective\ncountermeasure to these attacks. In particular, we show that state-of-the-art\nneural network stegomalware can be efficiently and effectively neutralized\nthrough shuffling the column order of the weight- and bias-matrices, or\nequivalently the channel-order of convolutional layers. We show that this\neffectively corrupts payloads that have been embedded by state-of-the-art\nmethods in neural network steganography at no cost to network accuracy,\noutperforming competing methods by a significant margin. We then discuss\npossible means by which to bypass this defense, additional defense methods, and\nadvocate for continued research into the security of machine learning systems."}
{"id": "2509.20523", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.20523", "abs": "https://arxiv.org/abs/2509.20523", "authors": ["Pawel Trajdos", "Marek Kurzynski"], "title": "A Compound Classification System Based on Fuzzy Relations Applied to the Noise-Tolerant Control of a Bionic Hand via EMG Signal Recognition", "comment": null, "summary": "Modern anthropomorphic upper limb bioprostheses are typically controlled by\nelectromyographic (EMG) biosignals using a pattern recognition scheme.\nUnfortunately, there are many factors originating from the human source of\nobjects to be classified and from the human-prosthesis interface that make it\ndifficult to obtain an acceptable classification quality. One of these factors\nis the high susceptibility of biosignals to contamination, which can\nconsiderably reduce the quality of classification of a recognition system.\n  In the paper, the authors propose a new recognition system intended for EMG\nbased control of the hand prosthesis with detection of contaminated biosignals\nin order to mitigate the adverse effect of contaminations. The system consists\nof two ensembles: the set of one-class classifiers (OCC) to assess the degree\nof contamination of individual channels and the ensemble of K-nearest\nneighbours (KNN) classifier to recognise the patient's intent. For all\nrecognition systems, an original, coherent fuzzy model was developed, which\nallows the use of a uniform soft (fuzzy) decision scheme throughout the\nrecognition process. The experimental evaluation was conducted using real\nbiosignals from a public repository. The goal was to provide an experimental\ncomparative analysis of the parameters and procedures of the developed method\non which the quality of the recognition system depends. The proposed fuzzy\nrecognition system was also compared with similar systems described in the\nliterature."}
{"id": "2509.20631", "categories": ["cs.SE", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.20631", "abs": "https://arxiv.org/abs/2509.20631", "authors": ["Michael Zhang", "Yuan Tian", "Mariam Guizani"], "title": "Design, Implementation and Evaluation of a Novel Programming Language Topic Classification Workflow", "comment": null, "summary": "As software systems grow in scale and complexity, understanding the\ndistribution of programming language topics within source code becomes\nincreasingly important for guiding technical decisions, improving onboarding,\nand informing tooling and education. This paper presents the design,\nimplementation, and evaluation of a novel programming language topic\nclassification workflow. Our approach combines a multi-label Support Vector\nMachine (SVM) with a sliding window and voting strategy to enable fine-grained\nlocalization of core language concepts such as operator overloading, virtual\nfunctions, inheritance, and templates. Trained on the IBM Project CodeNet\ndataset, our model achieves an average F1 score of 0.90 across topics and 0.75\nin code-topic highlight. Our findings contribute empirical insights and a\nreusable pipeline for researchers and practitioners interested in code analysis\nand data-driven software engineering."}
{"id": "2509.20405", "categories": ["cs.CR", "cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2509.20405", "abs": "https://arxiv.org/abs/2509.20405", "authors": ["Visar Berisha", "Prad Kadambi", "Isabella Lenz"], "title": "Why Speech Deepfake Detectors Won't Generalize: The Limits of Detection in an Open World", "comment": null, "summary": "Speech deepfake detectors are often evaluated on clean, benchmark-style\nconditions, but deployment occurs in an open world of shifting devices,\nsampling rates, codecs, environments, and attack families. This creates a\n``coverage debt\" for AI-based detectors: every new condition multiplies with\nexisting ones, producing data blind spots that grow faster than data can be\ncollected. Because attackers can target these uncovered regions, worst-case\nperformance (not average benchmark scores) determines security. To demonstrate\nthe impact of the coverage debt problem, we analyze results from a recent\ncross-testing framework. Grouping performance by bona fide domain and spoof\nrelease year, two patterns emerge: newer synthesizers erase the legacy\nartifacts detectors rely on, and conversational speech domains\n(teleconferencing, interviews, social media) are consistently the hardest to\nsecure. These findings show that detection alone should not be relied upon for\nhigh-stakes decisions. Detectors should be treated as auxiliary signals within\nlayered defenses that include provenance, personhood credentials, and policy\nsafeguards."}
{"id": "2509.20562", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.20562", "abs": "https://arxiv.org/abs/2509.20562", "authors": ["Yubin Ge", "Salvatore Romeo", "Jason Cai", "Monica Sunkara", "Yi Zhang"], "title": "SAMULE: Self-Learning Agents Enhanced by Multi-level Reflection", "comment": "Accepted at EMNLP 2025 Main Conference", "summary": "Despite the rapid advancements in LLM agents, they still face the challenge\nof generating meaningful reflections due to inadequate error analysis and a\nreliance on rare successful trajectories, especially in complex tasks. In this\nwork, we propose SAMULE, a new framework for self-learning agents powered by a\nretrospective language model that is trained based on Multi-Level Reflection\nSynthesis. It first synthesizes high-quality reflections across three\ncomplementary levels: Single-Trajectory Learning (micro-level) for detailed\nerror correction; Intra-Task Learning (meso-level) to build error taxonomies\nacross multiple trials of the same task, and Inter-Task Learning (macro-level)\nto extract transferable insights based on same typed errors from diverse task\nfailures. Then we fine-tune a language model serving as the retrospective model\nto generate reflections during inference. We further extend our framework to\ninteractive settings through a foresight-based reflection mechanism, enabling\nagents to proactively reflect and adapt during user interactions by comparing\npredicted and actual responses. Extensive experiments on three challenging\nbenchmarks - TravelPlanner, NATURAL PLAN, and Tau-bench - demonstrate that our\napproach significantly outperforms reflection-based baselines. Our results\nhighlight the critical role of well-designed reflection synthesis and\nfailure-centric learning in building self-improving LLM agents."}
{"id": "2509.20780", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.20780", "abs": "https://arxiv.org/abs/2509.20780", "authors": ["Daniela Grassi", "Fabio Calefato", "Darja Smite", "Nicole Novielli", "Filippo Lanubile"], "title": "Exploring Engagement in Hybrid Meetings", "comment": null, "summary": "Background. The widespread adoption of hybrid work following the COVID-19\npandemic has fundamentally transformed software development practices,\nintroducing new challenges in communication and collaboration as organizations\ntransition from traditional office-based structures to flexible working\narrangements. This shift has established a new organizational norm where even\ntraditionally office-first companies now embrace hybrid team structures. While\nremote participation in meetings has become commonplace in this new\nenvironment, it may lead to isolation, alienation, and decreased engagement\namong remote team members. Aims. This study aims to identify and characterize\nengagement patterns in hybrid meetings through objective measurements, focusing\non the differences between co-located and remote participants. Method. We\nstudied professionals from three software companies over several weeks,\nemploying a multimodal approach to measure engagement. Data were collected\nthrough self-reported questionnaires and physiological measurements using\nbiometric devices during hybrid meetings to understand engagement dynamics.\nResults. The regression analyses revealed comparable engagement levels between\nonsite and remote participants, though remote participants show lower\nengagement in long meetings regardless of participation mode. Active roles\npositively correlate with higher engagement, while larger meetings and\nafternoon sessions are associated with lower engagement. Conclusions. Our\nresults offer insights into factors associated with engagement and\ndisengagement in hybrid meetings, as well as potential meeting improvement\nrecommendations. These insights are potentially relevant not only for software\nteams but also for knowledge-intensive organizations across various sectors\nfacing similar hybrid collaboration challenges."}
{"id": "2509.20411", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.20411", "abs": "https://arxiv.org/abs/2509.20411", "authors": ["Tharcisse Ndayipfukamiye", "Jianguo Ding", "Doreen Sebastian Sarwatt", "Adamu Gaston Philipo", "Huansheng Ning"], "title": "Adversarial Defense in Cybersecurity: A Systematic Review of GANs for Threat Detection and Mitigation", "comment": "35 pages, 10 tables, 4figures", "summary": "Machine learning-based cybersecurity systems are highly vulnerable to\nadversarial attacks, while Generative Adversarial Networks (GANs) act as both\npowerful attack enablers and promising defenses. This survey systematically\nreviews GAN-based adversarial defenses in cybersecurity (2021--August 31,\n2025), consolidating recent progress, identifying gaps, and outlining future\ndirections. Using a PRISMA-compliant systematic literature review protocol, we\nsearched five major digital libraries. From 829 initial records, 185\npeer-reviewed studies were retained and synthesized through quantitative trend\nanalysis and thematic taxonomy development. We introduce a four-dimensional\ntaxonomy spanning defensive function, GAN architecture, cybersecurity domain,\nand adversarial threat model. GANs improve detection accuracy, robustness, and\ndata utility across network intrusion detection, malware analysis, and IoT\nsecurity. Notable advances include WGAN-GP for stable training, CGANs for\ntargeted synthesis, and hybrid GAN models for improved resilience. Yet,\npersistent challenges remain such as instability in training, lack of\nstandardized benchmarks, high computational cost, and limited explainability.\nGAN-based defenses demonstrate strong potential but require advances in stable\narchitectures, benchmarking, transparency, and deployment. We propose a roadmap\nemphasizing hybrid models, unified evaluation, real-world integration, and\ndefenses against emerging threats such as LLM-driven cyberattacks. This survey\nestablishes the foundation for scalable, trustworthy, and adaptive GAN-powered\ndefenses."}
{"id": "2509.20640", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.20640", "abs": "https://arxiv.org/abs/2509.20640", "authors": ["Oluwakemi T. Olayinka", "Sumeet Jeswani", "Divine Iloh"], "title": "Adaptive Cybersecurity Architecture for Digital Product Ecosystems Using Agentic AI", "comment": null, "summary": "Traditional static cybersecurity models often struggle with scalability,\nreal-time detection, and contextual responsiveness in the current digital\nproduct ecosystems which include cloud services, application programming\ninterfaces (APIs), mobile platforms, and edge devices. This study introduces\nautonomous goal driven agents capable of dynamic learning and context-aware\ndecision making as part of an adaptive cybersecurity architecture driven by\nagentic artificial intelligence (AI). To facilitate autonomous threat\nmitigation, proactive policy enforcement, and real-time anomaly detection, this\nframework integrates agentic AI across the key ecosystem layers. Behavioral\nbaselining, decentralized risk scoring, and federated threat intelligence\nsharing are important features. The capacity of the system to identify zero-day\nattacks and dynamically modify access policies was demonstrated through native\ncloud simulations. The evaluation results show increased adaptability,\ndecreased response latency, and improved detection accuracy. The architecture\nprovides an intelligent and scalable blueprint for safeguarding complex digital\ninfrastructure and is compatible with zero-trust models, thereby supporting the\nadherence to international cybersecurity regulations."}
{"id": "2509.20837", "categories": ["cs.SE", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.20837", "abs": "https://arxiv.org/abs/2509.20837", "authors": ["Srishti Gureja", "Elena Tommasone", "Jingyi He", "Sara Hooker", "Matthias Gallé", "Marzieh Fadaee"], "title": "Verification Limits Code LLM Training", "comment": null, "summary": "Large language models for code generation increasingly rely on synthetic\ndata, where both problem solutions and verification tests are generated by\nmodels. While this enables scalable data creation, it introduces a previously\nunexplored bottleneck: the verification ceiling, in which the quality and\ndiversity of training data are fundamentally constrained by the capabilities of\nsynthetic verifiers. In this work, we systematically study how verification\ndesign and strategies influence model performance. We investigate (i) what we\nverify by analyzing the impact of test complexity and quantity: richer test\nsuites improve code generation capabilities (on average +3 pass@1), while\nquantity alone yields diminishing returns, (ii) how we verify by exploring\nrelaxed pass thresholds: rigid 100% pass criteria can be overly restrictive. By\nallowing for relaxed thresholds or incorporating LLM-based soft verification,\nwe can recover valuable training data, leading to a 2-4 point improvement in\npass@1 performance. However, this benefit is contingent upon the strength and\ndiversity of the test cases used, and (iii) why verification remains necessary\nthrough controlled comparisons of formally correct versus incorrect solutions\nand human evaluation: retaining diverse correct solutions per problem yields\nconsistent generalization gains. Our results show that Verification as\ncurrently practiced is too rigid, filtering out valuable diversity. But it\ncannot be discarded, only recalibrated. By combining calibrated verification\nwith diverse, challenging problem-solution pairs, we outline a path to break\nthe verification ceiling and unlock stronger code generation models."}
{"id": "2509.20418", "categories": ["cs.CR", "cs.AI", "cs.ET", "K.6.5; I.2.0"], "pdf": "https://arxiv.org/pdf/2509.20418", "abs": "https://arxiv.org/abs/2509.20418", "authors": ["Grace Billiris", "Asif Gill", "Madhushi Bandara"], "title": "A Taxonomy of Data Risks in AI and Quantum Computing (QAI) - A Systematic Review", "comment": "11 pages, 2 figures, 2 tables", "summary": "Quantum Artificial Intelligence (QAI), the integration of Artificial\nIntelligence (AI) and Quantum Computing (QC), promises transformative advances,\nincluding AI-enabled quantum cryptography and quantum-resistant encryption\nprotocols. However, QAI inherits data risks from both AI and QC, creating\ncomplex privacy and security vulnerabilities that are not systematically\nstudied. These risks affect the trustworthiness and reliability of AI and QAI\nsystems, making their understanding critical. This study systematically reviews\n67 privacy- and security-related studies to expand understanding of QAI data\nrisks. We propose a taxonomy of 22 key data risks, organised into five\ncategories: governance, risk assessment, control implementation, user\nconsiderations, and continuous monitoring. Our findings reveal vulnerabilities\nunique to QAI and identify gaps in holistic risk assessment. This work\ncontributes to trustworthy AI and QAI research and provides a foundation for\ndeveloping future risk assessment tools."}
{"id": "2509.20652", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.20652", "abs": "https://arxiv.org/abs/2509.20652", "authors": ["Po-Yu Liang", "Yong Zhang", "Tatiana Hwa", "Aaron Byers"], "title": "Accelerate Creation of Product Claims Using Generative AI", "comment": "This paper has been accepted at the GenProCC workshop (NeurIPS 2025)", "summary": "The benefit claims of a product is a critical driver of consumers' purchase\nbehavior. Creating product claims is an intense task that requires substantial\ntime and funding. We have developed the $\\textbf{Claim Advisor}$ web\napplication to accelerate claim creations using in-context learning and\nfine-tuning of large language models (LLM). $\\textbf{Claim Advisor}$ was\ndesigned to disrupt the speed and economics of claim search, generation,\noptimization, and simulation. It has three functions: (1) semantically\nsearching and identifying existing claims and/or visuals that resonate with the\nvoice of consumers; (2) generating and/or optimizing claims based on a product\ndescription and a consumer profile; and (3) ranking generated and/or manually\ncreated claims using simulations via synthetic consumers. Applications in a\nconsumer packaged goods (CPG) company have shown very promising results. We\nbelieve that this capability is broadly useful and applicable across product\ncategories and industries. We share our learning to encourage the research and\napplication of generative AI in different industries."}
{"id": "2509.20881", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.20881", "abs": "https://arxiv.org/abs/2509.20881", "authors": ["Yixuan Li", "Xinyi Liu", "Weidong Yang", "Ben Fei", "Shuhao Li", "Mingjie Zhou", "Lipeng Ma"], "title": "PseudoBridge: Pseudo Code as the Bridge for Better Semantic and Logic Alignment in Code Retrieval", "comment": null, "summary": "Code search aims to precisely find relevant code snippets that match natural\nlanguage queries within massive codebases, playing a vital role in software\ndevelopment. Recent advances leverage pre-trained language models (PLMs) to\nbridge the semantic gap between unstructured natural language (NL) and\nstructured programming languages (PL), yielding significant improvements over\ntraditional information retrieval and early deep learning approaches. However,\nexisting PLM-based methods still encounter key challenges, including a\nfundamental semantic gap between human intent and machine execution logic, as\nwell as limited robustness to diverse code styles. To address these issues, we\npropose PseudoBridge, a novel code retrieval framework that introduces\npseudo-code as an intermediate, semi-structured modality to better align NL\nsemantics with PL logic. Specifically, PseudoBridge consists of two stages.\nFirst, we employ an advanced large language model (LLM) to synthesize\npseudo-code, enabling explicit alignment between NL queries and pseudo-code.\nSecond, we introduce a logic-invariant code style augmentation strategy and\nemploy the LLM to generate stylistically diverse yet logically equivalent code\nimplementations with pseudo-code, then align the code snippets of different\nstyles with pseudo-code, enhancing model robustness to code style variation. We\nbuild PseudoBridge across 10 different PLMs and evaluate it on 6 mainstream\nprogramming languages. Extensive experiments demonstrate that PseudoBridge\nconsistently outperforms baselines, achieving significant gains in retrieval\naccuracy and generalization, particularly under zero-shot domain transfer\nscenarios such as Solidity and XLCoST datasets. These results demonstrate the\neffectiveness of explicit logical alignment via pseudo-code and highlight\nPseudoBridge's potential as a robust, generalizable solution for code\nretrieval."}
{"id": "2509.20460", "categories": ["cs.CR", "eess.SP"], "pdf": "https://arxiv.org/pdf/2509.20460", "abs": "https://arxiv.org/abs/2509.20460", "authors": ["Andrew Campbell", "Anna Scaglione", "Hang Liu", "Victor Elvira", "Sean Peisert", "Daniel Arnold"], "title": "Differential Privacy of Network Parameters from a System Identification Perspective", "comment": null, "summary": "This paper addresses the problem of protecting network information from\nprivacy system identification (SI) attacks when sharing cyber-physical system\nsimulations. We model analyst observations of networked states as time-series\noutputs of a graph filter driven by differentially private (DP) nodal\nexcitations, with the analyst aiming to infer the underlying graph shift\noperator (GSO). Unlike traditional SI, which estimates system parameters, we\nstudy the inverse problem: what assumptions prevent adversaries from\nidentifying the GSO while preserving utility for legitimate analysis. We show\nthat applying DP mechanisms to inputs provides formal privacy guarantees for\nthe GSO, linking the $(\\epsilon,\\delta)$-DP bound to the spectral properties of\nthe graph filter and noise covariance. More precisely, for DP Gaussian signals,\nthe spectral characteristics of both the filter and noise covariance determine\nthe privacy bound, with smooth filters and low-condition-number covariance\nyielding greater privacy."}
{"id": "2509.20707", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.20707", "abs": "https://arxiv.org/abs/2509.20707", "authors": ["Junjie Cui", "Peilong Wang", "Jason Holmes", "Leshan Sun", "Michael L. Hinni", "Barbara A. Pockaj", "Sujay A. Vora", "Terence T. Sio", "William W. Wong", "Nathan Y. Yu", "Steven E. Schild", "Joshua R. Niska", "Sameer R. Keole", "Jean-Claude M. Rwigema", "Samir H. Patel", "Lisa A. McGee", "Carlos A. Vargas", "Wei Liu"], "title": "An Automated Retrieval-Augmented Generation LLaMA-4 109B-based System for Evaluating Radiotherapy Treatment Plans", "comment": "16 pages, 4 figures. Submitted to npj Digital Medicine", "summary": "Purpose: To develop a retrieval-augmented generation (RAG) system powered by\nLLaMA-4 109B for automated, protocol-aware, and interpretable evaluation of\nradiotherapy treatment plans.\n  Methods and Materials: We curated a multi-protocol dataset of 614\nradiotherapy plans across four disease sites and constructed a knowledge base\ncontaining normalized dose metrics and protocol-defined constraints. The RAG\nsystem integrates three core modules: a retrieval engine optimized across five\nSentenceTransformer backbones, a percentile prediction component based on\ncohort similarity, and a clinical constraint checker. These tools are directed\nby a large language model (LLM) using a multi-step prompt-driven reasoning\npipeline to produce concise, grounded evaluations.\n  Results: Retrieval hyperparameters were optimized using Gaussian Process on a\nscalarized loss function combining root mean squared error (RMSE), mean\nabsolute error (MAE), and clinically motivated accuracy thresholds. The best\nconfiguration, based on all-MiniLM-L6-v2, achieved perfect nearest-neighbor\naccuracy within a 5-percentile-point margin and a sub-2pt MAE. When tested\nend-to-end, the RAG system achieved 100% agreement with the computed values by\nstandalone retrieval and constraint-checking modules on both percentile\nestimates and constraint identification, confirming reliable execution of all\nretrieval, prediction and checking steps.\n  Conclusion: Our findings highlight the feasibility of combining structured\npopulation-based scoring with modular tool-augmented reasoning for transparent,\nscalable plan evaluation in radiation therapy. The system offers traceable\noutputs, minimizes hallucination, and demonstrates robustness across protocols.\nFuture directions include clinician-led validation, and improved domain-adapted\nretrieval models to enhance real-world integration."}
{"id": "2509.21067", "categories": ["cs.SE", "cs.CY"], "pdf": "https://arxiv.org/pdf/2509.21067", "abs": "https://arxiv.org/abs/2509.21067", "authors": ["Oka Kurniawan", "Erick Chandra", "Christopher M. Poskitt", "Yannic Noller", "Kenny Tsu Wei Choo", "Cyrille Jegourel"], "title": "Designing for Novice Debuggers: A Pilot Study on an AI-Assisted Debugging Tool", "comment": "Accepted by the 25th Koli Calling International Conference on\n  Computing Education Research (Koli Calling 2025)", "summary": "Debugging is a fundamental skill that novice programmers must develop.\nNumerous tools have been created to assist novice programmers in this process.\nRecently, large language models (LLMs) have been integrated with automated\nprogram repair techniques to generate fixes for students' buggy code. However,\nmany of these tools foster an over-reliance on AI and do not actively engage\nstudents in the debugging process. In this work, we aim to design an intuitive\ndebugging assistant, CodeHinter, that combines traditional debugging tools with\nLLM-based techniques to help novice debuggers fix semantic errors while\npromoting active engagement in the debugging process. We present findings from\nour second design iteration, which we tested with a group of undergraduate\nstudents. Our results indicate that the students found the tool highly\neffective in resolving semantic errors and significantly easier to use than the\nfirst version. Consistent with our previous study, error localization was the\nmost valuable feature. Finally, we conclude that any AI-assisted debugging tool\nshould be personalized based on user profiles to optimize their interactions\nwith students."}
{"id": "2509.20476", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.20476", "abs": "https://arxiv.org/abs/2509.20476", "authors": ["Ren-Yi Huang", "Dumindu Samaraweera", "Prashant Shekhar", "J. Morris Chang"], "title": "Advancing Practical Homomorphic Encryption for Federated Learning: Theoretical Guarantees and Efficiency Optimizations", "comment": null, "summary": "Federated Learning (FL) enables collaborative model training while preserving\ndata privacy by keeping raw data locally stored on client devices, preventing\naccess from other clients or the central server. However, recent studies reveal\nthat sharing model gradients creates vulnerability to Model Inversion Attacks,\nparticularly Deep Leakage from Gradients (DLG), which reconstructs private\ntraining data from shared gradients. While Homomorphic Encryption has been\nproposed as a promising defense mechanism to protect gradient privacy, fully\nencrypting all model gradients incurs high computational overhead. Selective\nencryption approaches aim to balance privacy protection with computational\nefficiency by encrypting only specific gradient components. However, the\nexisting literature largely overlooks a theoretical exploration of the spectral\nbehavior of encrypted versus unencrypted parameters, relying instead primarily\non empirical evaluations. To address this gap, this paper presents a framework\nfor theoretical analysis of the underlying principles of selective encryption\nas a defense against model inversion attacks. We then provide a comprehensive\nempirical study that identifies and quantifies the critical factors, such as\nmodel complexity, encryption ratios, and exposed gradients, that influence\ndefense effectiveness. Our theoretical framework clarifies the relationship\nbetween gradient selection and privacy preservation, while our experimental\nevaluation demonstrates how these factors shape the robustness of defenses\nagainst model inversion attacks. Collectively, these contributions advance the\nunderstanding of selective encryption mechanisms and offer principled guidance\nfor designing efficient, scalable, privacy-preserving federated learning\nsystems."}
{"id": "2509.20729", "categories": ["cs.AI", "cs.HC", "cs.MA"], "pdf": "https://arxiv.org/pdf/2509.20729", "abs": "https://arxiv.org/abs/2509.20729", "authors": ["Jiazheng Sun", "Te Yang", "Jiayang Niu", "Mingxuan Li", "Yongyong Lu", "Ruimeng Yang", "Xin Peng"], "title": "Fairy: Interactive Mobile Assistant to Real-world Tasks via LMM-based Multi-agent", "comment": "20 pages, 12 figures", "summary": "Large multi-modal models (LMMs) have advanced mobile GUI agents. However,\nexisting methods struggle with real-world scenarios involving diverse app\ninterfaces and evolving user needs. End-to-end methods relying on model's\ncommonsense often fail on long-tail apps, and agents without user interaction\nact unilaterally, harming user experience. To address these limitations, we\npropose Fairy, an interactive multi-agent mobile assistant capable of\ncontinuously accumulating app knowledge and self-evolving during usage. Fairy\nenables cross-app collaboration, interactive execution, and continual learning\nthrough three core modules:(i) a Global Task Planner that decomposes user tasks\ninto sub-tasks from a cross-app view; (ii) an App-Level Executor that refines\nsub-tasks into steps and actions based on long- and short-term memory,\nachieving precise execution and user interaction via four core agents operating\nin dual loops; and (iii) a Self-Learner that consolidates execution experience\ninto App Map and Tricks. To evaluate Fairy, we introduce RealMobile-Eval, a\nreal-world benchmark with a comprehensive metric suite, and LMM-based agents\nfor automated scoring. Experiments show that Fairy with GPT-4o backbone\noutperforms the previous SoTA by improving user requirement completion by 33.7%\nand reducing redundant steps by 58.5%, showing the effectiveness of its\ninteraction and self-learning."}
{"id": "2509.21068", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.21068", "abs": "https://arxiv.org/abs/2509.21068", "authors": ["Nek Dil Khan", "Javed Ali Khan", "Mobashir Husain", "Muhammad Sohail Khan", "Arif Ali Khan", "Muhammad Azeem Akbar", "Shahid Hussain"], "title": "An Improved Quantum Software Challenges Classification Approach using Transfer Learning and Explainable AI", "comment": null, "summary": "Quantum Software Engineering (QSE) is a research area practiced by tech\nfirms. Quantum developers face challenges in optimizing quantum computing and\nQSE concepts. They use Stack Overflow (SO) to discuss challenges and label\nposts with specialized quantum tags, which often refer to technical aspects\nrather than developer posts. Categorizing questions based on quantum concepts\ncan help identify frequent QSE challenges. We conducted studies to classify\nquestions into various challenges. We extracted 2829 questions from Q&A\nplatforms using quantum-related tags. Posts were analyzed to identify frequent\nchallenges and develop a novel grounded theory. Challenges include Tooling,\nTheoretical, Learning, Conceptual, Errors, and API Usage. Through content\nanalysis and grounded theory, discussions were annotated with common challenges\nto develop a ground truth dataset. ChatGPT validated human annotations and\nresolved disagreements. Fine-tuned transformer algorithms, including BERT,\nDistilBERT, and RoBERTa, classified discussions into common challenges. We\nachieved an average accuracy of 95% with BERT DistilBERT, compared to\nfine-tuned Deep and Machine Learning (D&ML) classifiers, including Feedforward\nNeural Networks (FNN), Convolutional Neural Networks (CNN), and Long Short-Term\nMemory networks (LSTM), which achieved accuracies of 89%, 86%, and 84%,\nrespectively. The Transformer-based approach outperforms the D&ML-based\napproach with a 6\\% increase in accuracy by processing actual discussions,\ni.e., without data augmentation. We applied SHAP (SHapley Additive\nexPlanations) for model interpretability, revealing how linguistic features\ndrive predictions and enhancing transparency in classification. These findings\ncan help quantum vendors and forums better organize discussions for improved\naccess and readability. However,empirical evaluation studies with actual\ndevelopers and vendors are needed."}
{"id": "2509.20589", "categories": ["cs.CR", "cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.20589", "abs": "https://arxiv.org/abs/2509.20589", "authors": ["Maria Chiper", "Radu Tudor Ionescu"], "title": "Every Character Counts: From Vulnerability to Defense in Phishing Detection", "comment": "Accepted at ICTAI 2025", "summary": "Phishing attacks targeting both organizations and individuals are becoming an\nincreasingly significant threat as technology advances. Current automatic\ndetection methods often lack explainability and robustness in detecting new\nphishing attacks. In this work, we investigate the effectiveness of\ncharacter-level deep learning models for phishing detection, which can provide\nboth robustness and interpretability. We evaluate three neural architectures\nadapted to operate at the character level, namely CharCNN, CharGRU, and\nCharBiLSTM, on a custom-built email dataset, which combines data from multiple\nsources. Their performance is analyzed under three scenarios: (i) standard\ntraining and testing, (ii) standard training and testing under adversarial\nattacks, and (iii) training and testing with adversarial examples. Aiming to\ndevelop a tool that operates as a browser extension, we test all models under\nlimited computational resources. In this constrained setup, CharGRU proves to\nbe the best-performing model across all scenarios. All models show\nvulnerability to adversarial attacks, but adversarial training substantially\nimproves their robustness. In addition, by adapting the Gradient-weighted Class\nActivation Mapping (Grad-CAM) technique to character-level inputs, we are able\nto visualize which parts of each email influence the decision of each model.\nOur open-source code and data is released at\nhttps://github.com/chipermaria/every-character-counts."}
{"id": "2509.20744", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.20744", "abs": "https://arxiv.org/abs/2509.20744", "authors": ["Qihang Ai", "Haiyun Jiang"], "title": "Parallel Thinking, Sequential Answering: Bridging NAR and AR for Efficient Reasoning", "comment": "4 pages", "summary": "We study reasoning tasks through a framework that integrates auto-regressive\n(AR) and non-autoregressive (NAR) language models. AR models, which generate\ntext sequentially, excel at producing coherent outputs but often suffer from\nslow inference, particularly in reasoning-intensive domains such as mathematics\nand code, where lengthy chains of thought are required. In contrast, NAR\nmodels, such as discrete diffusion models, allow parallel generation and offer\nsubstantial speedups, though typically at the cost of reduced output quality.\nTo address these limitations, we introduce a new paradigm in which an NAR model\nefficiently produces intermediate reasoning traces, which subsequently guide an\nAR model to deliver precise final answers. Experiments demonstrate that our\napproach yields significant 26% improvements over strong baselines while\nsubstantially reducing inference cost."}
{"id": "2509.21170", "categories": ["cs.SE", "cs.AI", "D.2.3; I.2.7"], "pdf": "https://arxiv.org/pdf/2509.21170", "abs": "https://arxiv.org/abs/2509.21170", "authors": ["Yongda Yu", "Guohao Shi", "Xianwei Wu", "Haochuan He", "XueMing Gu", "Qianqian Zhao", "Kui Liu", "Qiushi Wang", "Zhao Tian", "Haifeng Shen", "Guoping Rong"], "title": "Fine-Tuning LLMs to Analyze Multiple Dimensions of Code Review: A Maximum Entropy Regulated Long Chain-of-Thought Approach", "comment": "22 pages", "summary": "Large Language Models (LLMs) have shown great potential in supporting\nautomated code review due to their impressive capabilities in context\nunderstanding and reasoning. However, these capabilities are still limited\ncompared to human-level cognition because they are heavily influenced by the\ntraining data. Recent research has demonstrated significantly improved\nperformance through fine-tuning LLMs with code review data. However, compared\nto human reviewers who often simultaneously analyze multiple dimensions of code\nreview to better identify issues, the full potential of these methods is\nhampered by the limited or vague information used to fine-tune the models. This\npaper contributes MelcotCR, a chain-of-thought (COT) fine-tuning approach that\ntrains LLMs with an impressive reasoning ability to analyze multiple dimensions\nof code review by harnessing long COT techniques to provide rich structured\ninformation. To address context loss and reasoning logic loss issues that\nfrequently occur when LLMs process long COT prompts, we propose a solution that\ncombines the Maximum Entropy (ME) modeling principle with pre-defined reasoning\npathways in MelcotCR to enable more effective utilization of in-context\nknowledge within long COT prompts while strengthening the logical tightness of\nthe reasoning process. Empirical evaluations on our curated MelcotCR dataset\nand the public CodeReviewer dataset reveal that a low-parameter base model,\nsuch as 14B Qwen2.5, fine-tuned with MelcotCR can surpass state-of-the-art\nmethods in terms of the accuracy of detecting and describing code issues, with\nits performance remarkably on par with that of the 671B DeepSeek-R1 model."}
{"id": "2509.20592", "categories": ["cs.CR", "cs.HC"], "pdf": "https://arxiv.org/pdf/2509.20592", "abs": "https://arxiv.org/abs/2509.20592", "authors": ["Oluwole Adewusi", "Wallace S. Msagusa", "Jean Pierre Imanirumva", "Okemawo Obadofin", "Jema D. Ndibwile"], "title": "Beyond SSO: Mobile Money Authentication for Inclusive e-Government in Sub-Saharan Africa", "comment": null, "summary": "The rapid adoption of Mobile Money Services (MMS) in Sub-Saharan Africa (SSA)\noffers a viable path to improve e-Government service accessibility in the face\nof persistent low internet penetration. However, existing Mobile Money\nAuthentication (MMA) methods face critical limitations, including\nsusceptibility to SIM swapping, weak session protection, and poor scalability\nduring peak demand. This study introduces a hybrid MMA framework that combines\nUnstructured Supplementary Service Data (USSD)-based multi-factor\nauthentication with secure session management via cryptographically bound JSON\nWeb Tokens (JWT). Unlike traditional MMA systems that rely solely on SIM-PIN\nverification or smartphone-dependent biometrics, our design implements a\nthree-factor authentication model; SIM verification, PIN entry, and session\ntoken binding, tailored for resource-constrained environments. Simulations and\ncomparative analysis against OAuth-based Single Sign-On (SSO) methods reveal a\n45% faster authentication time (8 seconds vs. 12 to 15 seconds), 15% higher\nsuccess under poor network conditions (95% vs. 80%), and increased resistance\nto phishing and brute-force attacks. Penetration testing and threat modeling\nfurther demonstrate a substantial reduction in vulnerability exposure compared\nto conventional approaches. The primary contributions of this work are: (1) a\nhybrid authentication protocol that ensures offline accessibility and secure\nsession continuity; (2) a tailored security framework addressing threats like\nSIM swapping and social engineering in SSA; and (3) demonstrated scalability\nfor thousands of users with reduced infrastructure overhead. The proposed\napproach advances secure digital inclusion in SSA and other regions with\nsimilar constraints."}
{"id": "2509.20754", "categories": ["cs.AI", "cs.RO"], "pdf": "https://arxiv.org/pdf/2509.20754", "abs": "https://arxiv.org/abs/2509.20754", "authors": ["Yufan Mao", "Hanjing Ye", "Wenlong Dong", "Chengjie Zhang", "Hong Zhang"], "title": "Meta-Memory: Retrieving and Integrating Semantic-Spatial Memories for Robot Spatial Reasoning", "comment": null, "summary": "Navigating complex environments requires robots to effectively store\nobservations as memories and leverage them to answer human queries about\nspatial locations, which is a critical yet underexplored research challenge.\nWhile prior work has made progress in constructing robotic memory, few have\naddressed the principled mechanisms needed for efficient memory retrieval and\nintegration. To bridge this gap, we propose Meta-Memory, a large language model\n(LLM)-driven agent that constructs a high-density memory representation of the\nenvironment. The key innovation of Meta-Memory lies in its capacity to retrieve\nand integrate relevant memories through joint reasoning over semantic and\nspatial modalities in response to natural language location queries, thereby\nempowering robots with robust and accurate spatial reasoning capabilities. To\nevaluate its performance, we introduce SpaceLocQA, a large-scale dataset\nencompassing diverse real-world spatial question-answering scenarios.\nExperimental results show that Meta-Memory significantly outperforms\nstate-of-the-art methods on both the SpaceLocQA and the public NaVQA\nbenchmarks. Furthermore, we successfully deployed Meta-Memory on real-world\nrobotic platforms, demonstrating its practical utility in complex environments.\nProject page: https://itsbaymax.github.io/meta-memory.github.io/ ."}
{"id": "2509.21292", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.21292", "abs": "https://arxiv.org/abs/2509.21292", "authors": ["Ronivaldo Ferreira", "Guilherme da Silva", "Carla Rocha", "Gustavo Pinto"], "title": "Semantic Clustering of Civic Proposals: A Case Study on Brazil's National Participation Platform", "comment": "12 pages, in Portuguese language", "summary": "Promoting participation on digital platforms such as Brasil Participativo has\nemerged as a top priority for governments worldwide. However, due to the sheer\nvolume of contributions, much of this engagement goes underutilized, as\norganizing it presents significant challenges: (1) manual classification is\nunfeasible at scale; (2) expert involvement is required; and (3) alignment with\nofficial taxonomies is necessary. In this paper, we introduce an approach that\ncombines BERTopic with seed words and automatic validation by large language\nmodels. Initial results indicate that the generated topics are coherent and\ninstitutionally aligned, with minimal human effort. This methodology enables\ngovernments to transform large volumes of citizen input into actionable data\nfor public policy."}
{"id": "2509.20639", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.20639", "abs": "https://arxiv.org/abs/2509.20639", "authors": ["Adam Swanda", "Amy Chang", "Alexander Chen", "Fraser Burch", "Paul Kassianik", "Konstantin Berlin"], "title": "A Framework for Rapidly Developing and Deploying Protection Against Large Language Model Attacks", "comment": null, "summary": "The widespread adoption of Large Language Models (LLMs) has revolutionized AI\ndeployment, enabling autonomous and semi-autonomous applications across\nindustries through intuitive language interfaces and continuous improvements in\nmodel development. However, the attendant increase in autonomy and expansion of\naccess permissions among AI applications also make these systems compelling\ntargets for malicious attacks. Their inherent susceptibility to security flaws\nnecessitates robust defenses, yet no known approaches can prevent zero-day or\nnovel attacks against LLMs. This places AI protection systems in a category\nsimilar to established malware protection systems: rather than providing\nguaranteed immunity, they minimize risk through enhanced observability,\nmulti-layered defense, and rapid threat response, supported by a threat\nintelligence function designed specifically for AI-related threats.\n  Prior work on LLM protection has largely evaluated individual detection\nmodels rather than end-to-end systems designed for continuous, rapid adaptation\nto a changing threat landscape. We present a production-grade defense system\nrooted in established malware detection and threat intelligence practices. Our\nplatform integrates three components: a threat intelligence system that turns\nemerging threats into protections; a data platform that aggregates and enriches\ninformation while providing observability, monitoring, and ML operations; and a\nrelease platform enabling safe, rapid detection updates without disrupting\ncustomer workflows. Together, these components deliver layered protection\nagainst evolving LLM threats while generating training data for continuous\nmodel improvement and deploying updates without interrupting production."}
{"id": "2509.20798", "categories": ["cs.AI", "cs.SE"], "pdf": "https://arxiv.org/pdf/2509.20798", "abs": "https://arxiv.org/abs/2509.20798", "authors": ["Lipeng Ma", "Yixuan Li", "Weidong Yang", "Mingjie Zhou", "Xinyi Liu", "Ben Fei", "Shuhao Li", "Xiaoyan Sun", "Sihang Jiang", "Yanghua Xiao"], "title": "LogReasoner: Empowering LLMs with Expert-like Coarse-to-Fine Reasoning for Log Analysis Tasks", "comment": "under review", "summary": "Log analysis is crucial for monitoring system health and diagnosing failures\nin complex systems. Recent advances in large language models (LLMs) offer new\nopportunities for automated log analysis, leveraging their reasoning\ncapabilities to perform tasks such as anomaly detection and failure prediction.\nHowever, general-purpose LLMs struggle to formulate structured reasoning\nworkflows that align with expert cognition and deliver precise details of\nreasoning steps. To address these challenges, we propose LogReasoner, a\ncoarse-to-fine reasoning enhancement framework designed to enable LLMs to\nreason log analysis tasks like experts. LogReasoner consists of two stages: (1)\ncoarse-grained enhancement of expert thinking, where high-level expert thoughts\nare constructed from collected troubleshooting flowcharts and existing tasks to\nenable LLMs to formulate structured reasoning workflows and (2) fine-grained\nenhancement of specific steps, where we first fine-tune the LLM with\ntask-specific stepwise solutions to enhance the LLM for instantiated reasoning,\nthen employ the preference learning to calibrate the LLM's reasoning details\nfrom its mistakes, further strengthen the LLM's analytical granularity and\ncorrectness. We evaluate LogReasoner on four distinct log analysis tasks using\nopen-source LLMs such as Qwen-2.5 and Llama-3. Experimental results show that\nLogReasoner significantly outperforms existing LLMs, achieving state-of-the-art\nperformance and demonstrating its effectiveness in enhancing the reasoning\ncapabilities of LLMs for log analysis."}
{"id": "2509.20384", "categories": ["cs.CR", "cs.AI", "cs.PL", "cs.SE"], "pdf": "https://arxiv.org/pdf/2509.20384", "abs": "https://arxiv.org/abs/2509.20384", "authors": ["Jiayi Lin", "Liangcai Su", "Junzhe Li", "Chenxiong Qian"], "title": "R1-Fuzz: Specializing Language Models for Textual Fuzzing via Reinforcement Learning", "comment": null, "summary": "Fuzzing is effective for vulnerability discovery but struggles with complex\ntargets such as compilers, interpreters, and database engines, which accept\ntextual input that must satisfy intricate syntactic and semantic constraints.\nAlthough language models (LMs) have attracted interest for this task due to\ntheir vast latent knowledge and reasoning potential, their practical adoption\nhas been limited. The major challenges stem from insufficient exploration of\ndeep program logic among real-world codebases, and the high cost of leveraging\nlarger models. To overcome these challenges, we propose R1-Fuzz, the first\nframework that leverages reinforcement learning (RL) to specialize\ncost-efficient LMs and integrate them for complex textual fuzzing input\ngeneration. R1-Fuzz introduces two key designs: coverage-slicing-based question\nconstruction and a distance-based reward calculation. Through RL-based\npost-training of a model with our constructed dataset, R1-Fuzz designs a\nfuzzing workflow that tightly integrates LMs to reason deep program semantics\nduring fuzzing. Evaluations on diverse real-world targets show that our design\nenables a small model, named R1-Fuzz-7B, to rival or even outperform much\nlarger models in real-world fuzzing. Notably, R1-Fuzz achieves up to 75\\%\nhigher coverage than state-of-the-art fuzzers and discovers 29 previously\nunknown vulnerabilities, demonstrating its practicality."}
{"id": "2509.20686", "categories": ["cs.CR", "cs.AR"], "pdf": "https://arxiv.org/pdf/2509.20686", "abs": "https://arxiv.org/abs/2509.20686", "authors": ["Rian Adam Rajagede", "Yan Solihin"], "title": "Reliability Analysis of Fully Homomorphic Encryption Systems Under Memory Faults", "comment": null, "summary": "Fully Homomorphic Encryption (FHE) represents a paradigm shift in\ncryptography, enabling computation directly on encrypted data and unlocking\nprivacy-critical computation. Despite being increasingly deployed in real\nplatforms, the reliability aspects of FHE systems, especially how they respond\nto faults, have been mostly neglected. This paper aims to better understand of\nhow FHE computation behaves in the presence of memory faults, both in terms of\nindividual operations as well as at the level of applications, for different\nFHE schemes. Finally, we investigate how effective traditional and FHE-specific\nfault mitigation techniques are."}
{"id": "2509.20912", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.20912", "abs": "https://arxiv.org/abs/2509.20912", "authors": ["Tianrun Xu", "Haoda Jing", "Ye Li", "Yuquan Wei", "Jun Feng", "Guanyu Chen", "Haichuan Gao", "Tianren Zhang", "Feng Chen"], "title": "DeFacto: Counterfactual Thinking with Images for Enforcing Evidence-Grounded and Faithful Reasoning", "comment": null, "summary": "Recent advances in multimodal language models (MLLMs) have achieved\nremarkable progress in vision-language reasoning, especially with the emergence\nof \"thinking with images,\" which integrates explicit visual steps into the\nreasoning process. While this paradigm strengthens image-based reasoning, a\nsignificant challenge remains: models may arrive at correct answers by relying\non irrelevant or spurious regions, driven by prior knowledge or dataset biases.\nEven when the answer is correct, flawed reasoning indicates that the model has\nnot truly understood the image, highlighting the critical importance of\nreasoning fidelity in multimodal tasks. To address this issue, we propose\nDeFacto, a counterfactual reasoning framework that jointly enforces accurate\nanswering and faithful reasoning. A key component of our approach is the design\nof three complementary training paradigms: (i) positive, (ii) counterfactual,\nand (iii) random-masking. To enable these paradigms, we develop a pipeline that\nautomatically localizes question-relevant evidence and constructs positive,\ncounterfactual, and random variants, resulting in a dataset of about 100k\nimages. Building on this framework, we train multimodal language models with\nGRPO-based reinforcement learning, where we design three complementary rewards\nto guide the model toward accurate answering and evidence-grounded reasoning.\nExperiments on diverse benchmarks demonstrate that DeFacto substantially\nimproves both answer accuracy and reasoning faithfulness, establishing a\nstronger foundation for interpretable multimodal reasoning. The code is\navailable on GitHub and the dataset is released on HuggingFace."}
{"id": "2509.20798", "categories": ["cs.AI", "cs.SE"], "pdf": "https://arxiv.org/pdf/2509.20798", "abs": "https://arxiv.org/abs/2509.20798", "authors": ["Lipeng Ma", "Yixuan Li", "Weidong Yang", "Mingjie Zhou", "Xinyi Liu", "Ben Fei", "Shuhao Li", "Xiaoyan Sun", "Sihang Jiang", "Yanghua Xiao"], "title": "LogReasoner: Empowering LLMs with Expert-like Coarse-to-Fine Reasoning for Log Analysis Tasks", "comment": "under review", "summary": "Log analysis is crucial for monitoring system health and diagnosing failures\nin complex systems. Recent advances in large language models (LLMs) offer new\nopportunities for automated log analysis, leveraging their reasoning\ncapabilities to perform tasks such as anomaly detection and failure prediction.\nHowever, general-purpose LLMs struggle to formulate structured reasoning\nworkflows that align with expert cognition and deliver precise details of\nreasoning steps. To address these challenges, we propose LogReasoner, a\ncoarse-to-fine reasoning enhancement framework designed to enable LLMs to\nreason log analysis tasks like experts. LogReasoner consists of two stages: (1)\ncoarse-grained enhancement of expert thinking, where high-level expert thoughts\nare constructed from collected troubleshooting flowcharts and existing tasks to\nenable LLMs to formulate structured reasoning workflows and (2) fine-grained\nenhancement of specific steps, where we first fine-tune the LLM with\ntask-specific stepwise solutions to enhance the LLM for instantiated reasoning,\nthen employ the preference learning to calibrate the LLM's reasoning details\nfrom its mistakes, further strengthen the LLM's analytical granularity and\ncorrectness. We evaluate LogReasoner on four distinct log analysis tasks using\nopen-source LLMs such as Qwen-2.5 and Llama-3. Experimental results show that\nLogReasoner significantly outperforms existing LLMs, achieving state-of-the-art\nperformance and demonstrating its effectiveness in enhancing the reasoning\ncapabilities of LLMs for log analysis."}
{"id": "2509.20714", "categories": ["cs.CR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.20714", "abs": "https://arxiv.org/abs/2509.20714", "authors": ["Anh Tu Ngo", "Anupam Chattopadhyay", "Subhamoy Maitra"], "title": "Cryptographic Backdoor for Neural Networks: Boon and Bane", "comment": "Preprint", "summary": "In this paper we show that cryptographic backdoors in a neural network (NN)\ncan be highly effective in two directions, namely mounting the attacks as well\nas in presenting the defenses as well. On the attack side, a carefully planted\ncryptographic backdoor enables powerful and invisible attack on the NN.\nConsidering the defense, we present applications: first, a provably robust NN\nwatermarking scheme; second, a protocol for guaranteeing user authentication;\nand third, a protocol for tracking unauthorized sharing of the NN intellectual\nproperty (IP). From a broader theoretical perspective, borrowing the ideas from\nGoldwasser et. al. [FOCS 2022], our main contribution is to show that all these\ninstantiated practical protocol implementations are provably robust. The\nprotocols for watermarking, authentication and IP tracking resist an adversary\nwith black-box access to the NN, whereas the backdoor-enabled adversarial\nattack is impossible to prevent under the standard assumptions. While the\ntheoretical tools used for our attack is mostly in line with the Goldwasser et.\nal. ideas, the proofs related to the defense need further studies. Finally, all\nthese protocols are implemented on state-of-the-art NN architectures with\nempirical results corroborating the theoretical claims. Further, one can\nutilize post-quantum primitives for implementing the cryptographic backdoors,\nlaying out foundations for quantum-era applications in machine learning (ML)."}
{"id": "2509.20935", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.20935", "abs": "https://arxiv.org/abs/2509.20935", "authors": ["Heming Zhang", "Di Huang", "Wenyu Li", "Michael Province", "Yixin Chen", "Philip Payne", "Fuhai Li"], "title": "GALAX: Graph-Augmented Language Model for Explainable Reinforcement-Guided Subgraph Reasoning in Precision Medicine", "comment": null, "summary": "In precision medicine, quantitative multi-omic features, topological context,\nand textual biological knowledge play vital roles in identifying\ndisease-critical signaling pathways and targets. Existing pipelines capture\nonly part of these-numerical omics ignore topological context, text-centric\nLLMs lack quantitative grounded reasoning, and graph-only models underuse node\nsemantics and the generalization of LLMs-limiting mechanistic interpretability.\nAlthough Process Reward Models (PRMs) aim to guide reasoning in LLMs, they\nremain limited by unreliable intermediate evaluation, and vulnerability to\nreward hacking with computational cost. These gaps motivate integrating\nquantitative multi-omic signals, topological structure with node annotations,\nand literature-scale text via LLMs, using subgraph reasoning as the principle\nbridge linking numeric evidence, topological knowledge and language context.\nTherefore, we propose GALAX (Graph Augmented LAnguage model with\neXplainability), an innovative framework that integrates pretrained Graph\nNeural Networks (GNNs) into Large Language Models (LLMs) via reinforcement\nguided by a Graph Process Reward Model (GPRM), which generates disease-relevant\nsubgraphs in a step-wise manner initiated by an LLM and iteratively evaluated\nby a pretrained GNN, enabling process-level supervision without explicit\nintermediate reasoning annotations. As an application, we also introduced\nTarget-QA, a benchmark combining CRISPR-identified targets, multi-omic\nprofiles, and biomedical graph knowledge across diverse cancer cell lines,\nwhich enables GNN pretraining for supervising step-wise graph construction and\nsupports long-context reasoning over text-numeric graphs (TNGs), providing a\nscalable and biologically grounded framework for explainable,\nreinforcement-guided subgraph reasoning toward reliable and interpretable\ntarget and pathway discovery in precision medicine."}
{"id": "2509.21011", "categories": ["cs.CR", "cs.AI", "cs.SE"], "pdf": "https://arxiv.org/pdf/2509.21011", "abs": "https://arxiv.org/abs/2509.21011", "authors": ["Ping He", "Changjiang Li", "Binbin Zhao", "Tianyu Du", "Shouling Ji"], "title": "Automatic Red Teaming LLM-based Agents with Model Context Protocol Tools", "comment": null, "summary": "The remarkable capability of large language models (LLMs) has led to the wide\napplication of LLM-based agents in various domains. To standardize interactions\nbetween LLM-based agents and their environments, model context protocol (MCP)\ntools have become the de facto standard and are now widely integrated into\nthese agents. However, the incorporation of MCP tools introduces the risk of\ntool poisoning attacks, which can manipulate the behavior of LLM-based agents.\nAlthough previous studies have identified such vulnerabilities, their red\nteaming approaches have largely remained at the proof-of-concept stage, leaving\nthe automatic and systematic red teaming of LLM-based agents under the MCP tool\npoisoning paradigm an open question. To bridge this gap, we propose\nAutoMalTool, an automated red teaming framework for LLM-based agents by\ngenerating malicious MCP tools. Our extensive evaluation shows that AutoMalTool\neffectively generates malicious MCP tools capable of manipulating the behavior\nof mainstream LLM-based agents while evading current detection mechanisms,\nthereby revealing new security risks in these agents."}
{"id": "2509.20767", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.20767", "abs": "https://arxiv.org/abs/2509.20767", "authors": ["Ayush Kumar", "Kar Wai Fok", "Vrizlynn L. L. Thing"], "title": "ExpIDS: A Drift-adaptable Network Intrusion Detection System With Improved Explainability", "comment": null, "summary": "Despite all the advantages associated with Network Intrusion Detection\nSystems (NIDSs) that utilize machine learning (ML) models, there is a\nsignificant reluctance among cyber security experts to implement these models\nin real-world production settings. This is primarily because of their opaque\nnature, meaning it is unclear how and why the models make their decisions. In\nthis work, we design a deep learning-based NIDS, ExpIDS to have high decision\ntree explanation fidelity, i.e., the predictions of decision tree explanation\ncorresponding to ExpIDS should be as close to ExpIDS's predictions as possible.\nExpIDS can also adapt to changes in network traffic distribution (drift). With\nthe help of extensive experiments, we verify that ExpIDS achieves higher\ndecision tree explanation fidelity and a malicious traffic detection\nperformance comparable to state-of-the-art NIDSs for common attacks with\nvarying levels of real-world drift."}
{"id": "2509.20953", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.20953", "abs": "https://arxiv.org/abs/2509.20953", "authors": ["Najla Zuhir", "Amna Mohammad Salim", "Parvathy Premkumar", "Moshiur Farazi"], "title": "Beyond Stars: Bridging the Gap Between Ratings and Review Sentiment with LLM", "comment": "Paper accepted for presentation at ACS/IEEE 22nd International\n  Conference on Computer Systems and Applications (AICCSA 2025)", "summary": "We present an advanced approach to mobile app review analysis aimed at\naddressing limitations inherent in traditional star-rating systems. Star\nratings, although intuitive and popular among users, often fail to capture the\nnuanced feedback present in detailed review texts. Traditional NLP techniques\n-- such as lexicon-based methods and classical machine learning classifiers --\nstruggle to interpret contextual nuances, domain-specific terminology, and\nsubtle linguistic features like sarcasm. To overcome these limitations, we\npropose a modular framework leveraging large language models (LLMs) enhanced by\nstructured prompting techniques. Our method quantifies discrepancies between\nnumerical ratings and textual sentiment, extracts detailed, feature-level\ninsights, and supports interactive exploration of reviews through\nretrieval-augmented conversational question answering (RAG-QA). Comprehensive\nexperiments conducted on three diverse datasets (AWARE, Google Play, and\nSpotify) demonstrate that our LLM-driven approach significantly surpasses\nbaseline methods, yielding improved accuracy, robustness, and actionable\ninsights in challenging and context-rich review scenarios."}
{"id": "2509.20796", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.20796", "abs": "https://arxiv.org/abs/2509.20796", "authors": ["Yongjiao Li", "Liang Zhu", "Yalin Deng", "Qikun Zhang", "Zhenlei Wang", "Zhu Cao"], "title": "Fast Revocable Attribute-Based Encryption with Data Integrity for Internet of Things", "comment": "16 pages, 7 figures", "summary": "Efficient and secure revocable attribute-based encryption (RABE) is vital for\nensuring flexible and fine-grained access control and data sharing in cloud\nstorage and outsourced data environments within the Internet of Things (IoT).\nHowever, current RABE schemes often struggle to achieve an optimal balance\nbetween efficiency, security, dynamic scalability, and other important\nfeatures, which hampers their practical application. To overcome these\nlimitations, we propose a fast RABE scheme with data integrity for IoT that\nachieves adaptive security with multiple challenge ciphertexts. Our scheme\nsupports the revocation of authorized users and transfers the computationally\nheavy revocation processes to the cloud, thereby easing the computational\nburden on IoT devices. Moreover, it consistently guarantees the integrity and\ncorrectness of data. We have demonstrated its adaptive security within the\ndefined security model with multiple challenge ciphertexts and optimized its\nperformance. Experimental results indicate that our scheme provides better\nperformance than existing solutions. Under the same access policy, our scheme\nreduces computational consumption by 7 to 9 times compared to previous schemes."}
{"id": "2509.20988", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.20988", "abs": "https://arxiv.org/abs/2509.20988", "authors": ["Xiaozhuang Song", "Xuanhao Pan", "Xinjian Zhao", "Hangting Ye", "Shufei Zhang", "Jian Tang", "Tianshu Yu"], "title": "AOT*: Efficient Synthesis Planning via LLM-Empowered AND-OR Tree Search", "comment": "34 pages, 21 figures", "summary": "Retrosynthesis planning enables the discovery of viable synthetic routes for\ntarget molecules, playing a crucial role in domains like drug discovery and\nmaterials design. Multi-step retrosynthetic planning remains computationally\nchallenging due to exponential search spaces and inference costs. While Large\nLanguage Models (LLMs) demonstrate chemical reasoning capabilities, their\napplication to synthesis planning faces constraints on efficiency and cost. To\naddress these challenges, we introduce AOT*, a framework that transforms\nretrosynthetic planning by integrating LLM-generated chemical synthesis\npathways with systematic AND-OR tree search. To this end, AOT* atomically maps\nthe generated complete synthesis routes onto AND-OR tree components, with a\nmathematically sound design of reward assignment strategy and retrieval-based\ncontext engineering, thus enabling LLMs to efficiently navigate in the chemical\nspace. Experimental evaluation on multiple synthesis benchmarks demonstrates\nthat AOT* achieves SOTA performance with significantly improved search\nefficiency. AOT* exhibits competitive solve rates using 3-5$\\times$ fewer\niterations than existing LLM-based approaches, with the efficiency advantage\nbecoming more pronounced on complex molecular targets."}
{"id": "2509.20808", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.20808", "abs": "https://arxiv.org/abs/2509.20808", "authors": ["Raghul Saravanan", "Sudipta Paria", "Aritra Dasgupta", "Swarup Bhunia", "Sai Manoj P D"], "title": "Intelligent Graybox Fuzzing via ATPG-Guided Seed Generation and Submodule Analysis", "comment": "7 pages, 6 figures, 4 tables", "summary": "Hardware Fuzzing emerged as one of the crucial techniques for finding\nsecurity flaws in modern hardware designs by testing a wide range of input\nscenarios. One of the main challenges is creating high-quality input seeds that\nmaximize coverage and speed up verification. Coverage-Guided Fuzzing (CGF)\nmethods help explore designs more effectively, but they struggle to focus on\nspecific parts of the hardware. Existing Directed Gray-box Fuzzing (DGF)\ntechniques like DirectFuzz try to solve this by generating targeted tests, but\nit has major drawbacks, such as supporting only limited hardware description\nlanguages, not scaling well to large circuits, and having issues with\nabstraction mismatches. To address these problems, we introduce a novel\nframework, PROFUZZ, that follows the DGF approach and combines fuzzing with\nAutomatic Test Pattern Generation (ATPG) for more efficient fuzzing. By\nleveraging ATPG's structural analysis capabilities, PROFUZZ can generate\nprecise input seeds that target specific design regions more effectively while\nmaintaining high fuzzing throughput. Our experiments show that PROFUZZ scales\n30x better than DirectFuzz when handling multiple target sites, improves\ncoverage by 11.66%, and runs 2.76x faster, highlighting its scalability and\neffectiveness for directed fuzzing in complex hardware systems."}
{"id": "2509.20998", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.20998", "abs": "https://arxiv.org/abs/2509.20998", "authors": ["Panagiotis Michelakis", "Yiannis Hadjiyiannis", "Dimitrios Stamoulis"], "title": "CORE: Full-Path Evaluation of LLM Agents Beyond Final State", "comment": "Accepted: LAW 2025 Workshop NeurIPS 2025", "summary": "Evaluating AI agents that solve real-world tasks through function-call\nsequences remains an open challenge. Existing agentic benchmarks often reduce\nevaluation to a binary judgment of the final state, overlooking critical\naspects such as safety, efficiency, and intermediate correctness. We propose a\nframework based on deterministic finite automata (DFAs) that encodes tasks as\nsets of valid tool-use paths, enabling principled assessment of agent behavior\nin diverse world models. Building on this foundation, we introduce CORE, a\nsuite of five metrics, namely Path Correctness, Path Correctness - Kendall's\ntau Composite, Prefix Criticality, Harmful-Call Rate, and Efficiency, that\nquantify alignment with expected execution patterns. Across diverse worlds, our\nmethod reveals important performance differences between agents that would\notherwise appear equivalent under traditional final-state evaluation schemes."}
{"id": "2509.20835", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.20835", "abs": "https://arxiv.org/abs/2509.20835", "authors": ["Yu Liu", "Boxiang He", "Fanggang Wang"], "title": "Security-aware Semantic-driven ISAC via Paired Adversarial Residual Networks", "comment": null, "summary": "This paper proposes a novel and flexible security-aware semantic-driven\nintegrated sensing and communication (ISAC) framework, namely security semantic\nISAC (SS-ISAC). Inspired by the positive impact of the adversarial attack, a\npair of pluggable encryption and decryption modules is designed in the proposed\nSS-ISAC framework. The encryption module is installed after the semantic\ntransmitter, adopting a trainable adversarial residual network (ARN) to create\nthe adversarial attack. Correspondingly, the decryption module before the\nsemantic receiver utilizes another trainable ARN to mitigate the adversarial\nattack and noise. These two modules can be flexibly assembled considering the\nsystem security demands, without drastically modifying the hardware\ninfrastructure. To ensure the sensing and communication (SAC) performance while\npreventing the eavesdropping threat, the above ARNs are jointly optimized by\nminimizing a carefully designed loss function that relates to the adversarial\nattack power, SAC performance, as well as the privacy leakage risk. Simulation\nresults validate the effectiveness of the proposed SS-ISAC framework in terms\nof both SAC and eavesdropping prevention performance."}
{"id": "2509.21028", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.21028", "abs": "https://arxiv.org/abs/2509.21028", "authors": ["Miao Li", "Alexander Gurung", "Irina Saparina", "Mirella Lapata"], "title": "Who Gets Cited Most? Benchmarking Long-Context Language Models on Scientific Articles", "comment": "31 pages", "summary": "This paper introduces SciTrek, a novel question-answering benchmark designed\nto evaluate the long-context reasoning capabilities of large language models\n(LLMs) using scientific articles. Current long-context benchmarks often rely on\nnon-scientific texts, focus on simple information retrieval tasks, or employ\nartificial contexts. SciTrek addresses these limitations by proposing complex\nquestions that require information aggregation and synthesis across multiple\nfull-text scientific articles. Questions and their ground-truth answers are\nautomatically generated by formulating them as SQL queries over a database\nconstructed from article metadata (titles, authors, and references). The SQL\noperations provide explicit, verifiable reasoning steps for fine-grained error\nanalysis, and the construction process scales to contexts up to 1M tokens with\nminimal supervision. Extensive experiments on a diverse set of open-weight and\nproprietary LLMs demonstrate that SciTrek poses a significant challenge as the\ncontext length increases, with supervised fine-tuning and reinforcement\nlearning offering only limited gains. Our analysis reveals systematic\nshortcomings in models' abilities to perform basic numerical operations and\naccurately locate specific information in long contexts."}
{"id": "2509.20861", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.20861", "abs": "https://arxiv.org/abs/2509.20861", "authors": ["Chao Zha", "Haolin Pan", "Bing Bai", "Jiangxing Wu", "Ruyun Zhang"], "title": "FlowXpert: Context-Aware Flow Embedding for Enhanced Traffic Detection in IoT Network", "comment": null, "summary": "In the Internet of Things (IoT) environment, continuous interaction among a\nlarge number of devices generates complex and dynamic network traffic, which\nposes significant challenges to rule-based detection approaches. Machine\nlearning (ML)-based traffic detection technology, capable of identifying\nanomalous patterns and potential threats within this traffic, serves as a\ncritical component in ensuring network security. This study first identifies a\nsignificant issue with widely adopted feature extraction tools (e.g.,\nCICMeterFlow): the extensive use of time- and length-related features leads to\nhigh sparsity, which adversely affects model convergence. Furthermore, existing\ntraffic detection methods generally lack an embedding mechanism capable of\nefficiently and comprehensively capturing the semantic characteristics of\nnetwork traffic. To address these challenges, we propose a novel feature\nextraction tool that eliminates traditional time and length features in favor\nof context-aware semantic features related to the source host, thus improving\nthe generalizability of the model. In addition, we design an embedding training\nframework that integrates the unsupervised DBSCAN clustering algorithm with a\ncontrastive learning strategy to effectively capture fine-grained semantic\nrepresentations of traffic. Extensive empirical evaluations are conducted on\nthe real-world Mawi data set to validate the proposed method in terms of\ndetection accuracy, robustness, and generalization. Comparative experiments\nagainst several state-of-the-art (SOTA) models demonstrate the superior\nperformance of our approach. Furthermore, we confirm its applicability and\ndeployability in real-time scenarios."}
{"id": "2509.21035", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.21035", "abs": "https://arxiv.org/abs/2509.21035", "authors": ["Yang Zhao", "Chengxiao Dai", "Wei Zhuo", "Yue Xiu", "Dusit Niyato"], "title": "CLAUSE: Agentic Neuro-Symbolic Knowledge Graph Reasoning via Dynamic Learnable Context Engineering", "comment": null, "summary": "Knowledge graphs provide structured context for multi-hop question answering,\nbut deployed systems must balance answer accuracy with strict latency and cost\ntargets while preserving provenance. Static k-hop expansions and \"think-longer\"\nprompting often over-retrieve, inflate context, and yield unpredictable\nruntime. We introduce CLAUSE, an agentic three-agent neuro-symbolic framework\nthat treats context construction as a sequential decision process over\nknowledge graphs, deciding what to expand, which paths to follow or backtrack,\nwhat evidence to keep, and when to stop. Latency (interaction steps) and prompt\ncost (selected tokens) are exposed as user-specified budgets or prices,\nallowing per-query adaptation to trade-offs among accuracy, latency, and cost\nwithout retraining. CLAUSE employs the proposed Lagrangian-Constrained\nMulti-Agent Proximal Policy Optimization (LC-MAPPO) algorithm to coordinate\nthree agents: Subgraph Architect, Path Navigator, and Context Curator, so that\nsubgraph construction, reasoning-path discovery, and evidence selection are\njointly optimized under per-query resource budgets on edge edits, interaction\nsteps, and selected tokens. Across HotpotQA, MetaQA, and FactKG, CLAUSE yields\nhigher EM@1 while reducing subgraph growth and end-to-end latency at equal or\nlower token budgets. On MetaQA-2-hop, relative to the strongest RAG baseline\n(GraphRAG), CLAUSE achieves +39.3 EM@1 with 18.6% lower latency and 40.9% lower\nedge growth. The resulting contexts are compact, provenance-preserving, and\ndeliver predictable performance under deployment constraints."}
{"id": "2509.20880", "categories": ["cs.CR", "cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2509.20880", "abs": "https://arxiv.org/abs/2509.20880", "authors": ["Cheng Lyu", "Mu Yuan", "Dabin Zheng", "Siwei Sun", "Shun Li"], "title": "A Generalized $χ_n$-Function", "comment": null, "summary": "The mapping $\\chi_n$ from $\\F_{2}^{n}$ to itself defined by $y=\\chi_n(x)$\nwith $y_i=x_i+x_{i+2}(1+x_{i+1})$, where the indices are computed modulo $n$,\nhas been widely studied for its applications in lightweight cryptography.\nHowever, $\\chi_n $ is bijective on $\\F_2^n$ only when $n$ is odd, restricting\nits use to odd-dimensional vector spaces over $\\F_2$. To address this\nlimitation, we introduce and analyze the generalized mapping $\\chi_{n, m}$\ndefined by $y=\\chi_{n,m}(x)$ with $y_i=x_i+x_{i+m} (x_{i+m-1}+1)(x_{i+m-2}+1)\n\\cdots (x_{i+1}+1)$, where $m$ is a fixed integer with $m\\nmid n$. To\ninvestigate such mappings, we further generalize $\\chi_{n,m}$ to $\\theta_{m,\nk}$, where $\\theta_{m, k}$ is given by $y_i=x_{i+mk} \\prod_{\\substack{j=1,\\,\\,\nm \\nmid j}}^{mk-1} \\left(x_{i+j}+1\\right), \\,\\,{\\rm for }\\,\\, i\\in\n\\{0,1,\\ldots,n-1\\}$. We prove that these mappings generate an abelian group\nisomorphic to the group of units in $\\F_2[z]/(z^{\\lfloor n/m\\rfloor +1})$. This\nstructural insight enables us to construct a broad class of permutations over\n$\\F_2^n$ for any positive integer $n$, along with their inverses. We rigorously\nanalyze algebraic properties of these mappings, including their iterations,\nfixed points, and cycle structures. Additionally, we provide a comprehensive\ndatabase of the cryptographic properties for iterates of $\\chi_{n,m}$ for small\nvalues of $n$ and $m$. Finally, we conduct a comparative security and\nimplementation cost analysis among $\\chi_{n,m}$, $\\chi_n$, $\\chi\\chi_n$\n(EUROCRYPT 2025 \\cite{belkheyar2025chi}) and their variants, and prove\nConjecture~1 proposed in~\\cite{belkheyar2025chi} as a by-product of our study.\nOur results lead to generalizations of $\\chi_n$, providing alternatives to\n$\\chi_n$ and $\\chi\\chi_n$."}
{"id": "2509.21043", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.21043", "abs": "https://arxiv.org/abs/2509.21043", "authors": ["Samuel Schapiro", "Sumuk Shashidhar", "Alexi Gladstone", "Jonah Black", "Royce Moon", "Dilek Hakkani-Tur", "Lav R. Varshney"], "title": "Combinatorial Creativity: A New Frontier in Generalization Abilities", "comment": "Preprint. The first two authors contributed equally", "summary": "Artificial intelligence (AI) systems, and large language models (LLMs) in\nparticular, are increasingly employed for creative tasks like scientific idea\ngeneration, constituting a form of generalization from training data\nunaddressed by existing conceptual frameworks. Though in many ways similar to\nforms of compositional generalization (CG), combinatorial creativity (CC) is an\nopen-ended ability. Instead of evaluating for accuracy or correctness against\nfixed targets, which would contradict the open-ended nature of CC, we propose a\ntheoretical framework and algorithmic task for evaluating outputs by their\ndegrees of novelty and utility. From here, we make several important empirical\ncontributions: (1) We obtain the first insights into the scaling behavior of\ncreativity for LLMs. (2) We discover that, for fixed compute budgets, there\nexist optimal model depths and widths for creative ability. (3) We find that\nthe ideation-execution gap, whereby LLMs excel at generating novel scientific\nideas but struggle to ensure their practical feasibility, may be explained by a\nmore fundamental novelty-utility tradeoff characteristic of creativity\nalgorithms in general. Importantly, this tradeoff remains persistent even at\nscale, casting doubt on the long-term creative potential of LLMs in their\ncurrent form. Together, our conceptual framework and empirical findings provide\na foundation for understanding and improving creativity in modern AI models,\nmarking a new frontier in generalization abilities."}
{"id": "2509.20924", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.20924", "abs": "https://arxiv.org/abs/2509.20924", "authors": ["Hanbo Huang", "Yiran Zhang", "Hao Zheng", "Xuan Gong", "Yihan Li", "Lin Liu", "Shiyu Liang"], "title": "RLCracker: Exposing the Vulnerability of LLM Watermarks with Adaptive RL Attacks", "comment": null, "summary": "Large Language Models (LLMs) watermarking has shown promise in detecting\nAI-generated content and mitigating misuse, with prior work claiming robustness\nagainst paraphrasing and text editing. In this paper, we argue that existing\nevaluations are not sufficiently adversarial, obscuring critical\nvulnerabilities and overstating the security. To address this, we introduce\nadaptive robustness radius, a formal metric that quantifies watermark\nresilience against adaptive adversaries. We theoretically prove that optimizing\nthe attack context and model parameters can substantially reduce this radius,\nmaking watermarks highly susceptible to paraphrase attacks. Leveraging this\ninsight, we propose RLCracker, a reinforcement learning (RL)-based adaptive\nattack that erases watermarks while preserving semantic fidelity. RLCracker\nrequires only limited watermarked examples and zero access to the detector.\nDespite weak supervision, it empowers a 3B model to achieve 98.5% removal\nsuccess and an average 0.92 P-SP score on 1,500-token Unigram-marked texts\nafter training on only 100 short samples. This performance dramatically exceeds\n6.75% by GPT-4o and generalizes across five model sizes over ten watermarking\nschemes. Our results confirm that adaptive attacks are broadly effective and\npose a fundamental threat to current watermarking defenses."}
{"id": "2509.21054", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.21054", "abs": "https://arxiv.org/abs/2509.21054", "authors": ["Haodong Zhao", "Jidong Li", "Zhaomin Wu", "Tianjie Ju", "Zhuosheng Zhang", "Bingsheng He", "Gongshen Liu"], "title": "Disagreements in Reasoning: How a Model's Thinking Process Dictates Persuasion in Multi-Agent Systems", "comment": "Work in progress", "summary": "The rapid proliferation of recent Multi-Agent Systems (MAS), where Large\nLanguage Models (LLMs) and Large Reasoning Models (LRMs) usually collaborate to\nsolve complex problems, necessitates a deep understanding of the persuasion\ndynamics that govern their interactions. This paper challenges the prevailing\nhypothesis that persuasive efficacy is primarily a function of model scale. We\npropose instead that these dynamics are fundamentally dictated by a model's\nunderlying cognitive process, especially its capacity for explicit reasoning.\nThrough a series of multi-agent persuasion experiments, we uncover a\nfundamental trade-off we term the Persuasion Duality. Our findings reveal that\nthe reasoning process in LRMs exhibits significantly greater resistance to\npersuasion, maintaining their initial beliefs more robustly. Conversely, making\nthis reasoning process transparent by sharing the \"thinking content\"\ndramatically increases their ability to persuade others. We further consider\nmore complex transmission persuasion situations and reveal complex dynamics of\ninfluence propagation and decay within multi-hop persuasion between multiple\nagent networks. This research provides systematic evidence linking a model's\ninternal processing architecture to its external persuasive behavior, offering\na novel explanation for the susceptibility of advanced models and highlighting\ncritical implications for the safety, robustness, and design of future MAS."}
{"id": "2509.20943", "categories": ["cs.CR", "cs.AI", "cs.ET"], "pdf": "https://arxiv.org/pdf/2509.20943", "abs": "https://arxiv.org/abs/2509.20943", "authors": ["Dincy R. Arikkat", "Sneha B. T.", "Serena Nicolazzo", "Antonino Nocera", "Vinod P.", "Rafidha Rehiman K. A.", "Karthika R"], "title": "CTI Dataset Construction from Telegram", "comment": null, "summary": "Cyber Threat Intelligence (CTI) enables organizations to anticipate, detect,\nand mitigate evolving cyber threats. Its effectiveness depends on high-quality\ndatasets, which support model development, training, evaluation, and\nbenchmarking. Building such datasets is crucial, as attack vectors and\nadversary tactics continually evolve. Recently, Telegram has gained prominence\nas a valuable CTI source, offering timely and diverse threat-related\ninformation that can help address these challenges. In this work, we address\nthese challenges by presenting an end-to-end automated pipeline that\nsystematically collects and filters threat-related content from Telegram. The\npipeline identifies relevant Telegram channels and scrapes 145,349 messages\nfrom 12 curated channels out of 150 identified sources. To accurately filter\nthreat intelligence messages from generic content, we employ a BERT-based\nclassifier, achieving an accuracy of 96.64%. From the filtered messages, we\ncompile a dataset of 86,509 malicious Indicators of Compromise, including\ndomains, IPs, URLs, hashes, and CVEs. This approach not only produces a\nlarge-scale, high-fidelity CTI dataset but also establishes a foundation for\nfuture research and operational applications in cyber threat detection."}
{"id": "2509.21072", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.21072", "abs": "https://arxiv.org/abs/2509.21072", "authors": ["Kaiwen He", "Zhiwei Wang", "Chenyi Zhuang", "Jinjie Gu"], "title": "Recon-Act: A Self-Evolving Multi-Agent Browser-Use System via Web Reconnaissance, Tool Generation, and Task Execution", "comment": null, "summary": "Recent years, multimodal models have made remarkable strides and pave the way\nfor intelligent browser use agents. However, when solving tasks on real world\nwebpages in multi-turn, long-horizon trajectories, current agents still suffer\nfrom disordered action sequencing and excessive trial and error during\nexecution. This paper introduces Recon-Act, a self-evolving multi-agent\nframework grounded in Reconnaissance-Action behavioral paradigm. The system\ncomprises a Reconnaissance Team and an Action Team: the former conducts\ncomparative analysis and tool generation, while the latter handles intent\ndecomposition, tool orchestration, and execution. By contrasting the erroneous\ntrajectories with successful ones, the Reconnaissance Team infers remedies, and\nabstracts them into a unified notion of generalized tools, either expressed as\nhints or as rule-based codes, and register to the tool archive in real time.\nThe Action Team reinference the process empowered with these targeting tools,\nthus establishing a closed-loop training pipeline of\ndata-tools-action-feedback. Following the 6 level implementation roadmap\nproposed in this work, we have currently reached Level 3 (with limited\nhuman-in-the-loop intervention). Leveraging generalized tools obtained through\nreconnaissance, Recon-Act substantially improves adaptability to unseen\nwebsites and solvability on long-horizon tasks, and achieves state-of-the-art\nperformance on the challenging VisualWebArena dataset."}
{"id": "2509.20972", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.20972", "abs": "https://arxiv.org/abs/2509.20972", "authors": ["Ibrahim Altan", "Abdulla Bachir", "Yousuf Parbhulkar", "Abdul Muksith Rizvi", "Moshiur Farazi"], "title": "Dual-Path Phishing Detection: Integrating Transformer-Based NLP with Structural URL Analysis", "comment": "Paper accepted for presentation at the ACS/IEEE 22nd International\n  Conference on Computer Systems and Applications (AICCSA 2025)", "summary": "Phishing emails pose a persistent and increasingly sophisticated threat,\nundermining email security through deceptive tactics designed to exploit both\nsemantic and structural vulnerabilities. Traditional detection methods, often\nbased on isolated analysis of email content or embedded URLs, fail to\ncomprehensively address these evolving attacks. In this paper, we propose a\ndual-path phishing detection framework that integrates transformer-based\nnatural language processing (NLP) with classical machine learning to jointly\nanalyze email text and embedded URLs. Our approach leverages the complementary\nstrengths of semantic analysis using fine-tuned transformer architectures\n(e.g., DistilBERT) and structural link analysis via character-level TF-IDF\nvectorization paired with classical classifiers (e.g., Random Forest).\nEmpirical evaluation on representative email and URL datasets demonstrates that\nthis combined approach significantly improves detection accuracy. Specifically,\nthe DistilBERT model achieves a near-optimal balance between accuracy and\ncomputational efficiency for textual phishing detection, while Random Forest\nnotably outperforms other classical classifiers in identifying malicious URLs.\nThe modular design allows flexibility for standalone deployment or ensemble\nintegration, facilitating real-world adoption. Collectively, our results\nhighlight the efficacy and practical value of this dual-path approach,\nestablishing a scalable, accurate, and interpretable solution capable of\nenhancing email security against contemporary phishing threats."}
{"id": "2509.21117", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.21117", "abs": "https://arxiv.org/abs/2509.21117", "authors": ["Yidong Wang", "Yunze Song", "Tingyuan Zhu", "Xuanwang Zhang", "Zhuohao Yu", "Hao Chen", "Chiyu Song", "Qiufeng Wang", "Cunxiang Wang", "Zhen Wu", "Xinyu Dai", "Yue Zhang", "Wei Ye", "Shikun Zhang"], "title": "TrustJudge: Inconsistencies of LLM-as-a-Judge and How to Alleviate Them", "comment": "22 pages, 9 figures, 6 tables", "summary": "The adoption of Large Language Models (LLMs) as automated evaluators\n(LLM-as-a-judge) has revealed critical inconsistencies in current evaluation\nframeworks. We identify two fundamental types of inconsistencies: (1)\nScore-Comparison Inconsistency, where lower-rated responses outperform\nhigher-scored ones in pairwise comparisons, and (2) Pairwise Transitivity\nInconsistency, manifested through circular preference chains (A>B>C>A) and\nequivalence contradictions (A=B=C\\neq A). We argue that these issues come from\ninformation loss in discrete rating systems and ambiguous tie judgments during\npairwise evaluation. We propose TrustJudge, a probabilistic framework that\naddresses these limitations through two key innovations: 1)\ndistribution-sensitive scoring that computes continuous expectations from\ndiscrete rating probabilities, preserving information entropy for more precise\nscoring, and 2) likelihood-aware aggregation that resolves transitivity\nviolations using bidirectional preference probabilities or perplexity. We also\nformalize the theoretical limitations of current LLM-as-a-judge frameworks and\ndemonstrate how TrustJudge's components overcome them. When evaluated with\nLlama-3.1-70B-Instruct as judge using our dataset, TrustJudge reduces\nScore-Comparison inconsistency by 8.43% (from 23.32% to 14.89%) and Pairwise\nTransitivity inconsistency by 10.82% (from 15.22% to 4.40%), while maintaining\nhigher evaluation accuracy. Our work provides the first systematic analysis of\nevaluation framework inconsistencies in LLM-as-a-judge paradigms, offering both\ntheoretical insights and practical solutions for reliable automated assessment.\nThe framework demonstrates consistent improvements across various model\narchitectures and scales, enabling more trustworthy LLM evaluation without\nrequiring additional training or human annotations. The codes can be found at\nhttps://github.com/TrustJudge/TrustJudge."}
{"id": "2509.21011", "categories": ["cs.CR", "cs.AI", "cs.SE"], "pdf": "https://arxiv.org/pdf/2509.21011", "abs": "https://arxiv.org/abs/2509.21011", "authors": ["Ping He", "Changjiang Li", "Binbin Zhao", "Tianyu Du", "Shouling Ji"], "title": "Automatic Red Teaming LLM-based Agents with Model Context Protocol Tools", "comment": null, "summary": "The remarkable capability of large language models (LLMs) has led to the wide\napplication of LLM-based agents in various domains. To standardize interactions\nbetween LLM-based agents and their environments, model context protocol (MCP)\ntools have become the de facto standard and are now widely integrated into\nthese agents. However, the incorporation of MCP tools introduces the risk of\ntool poisoning attacks, which can manipulate the behavior of LLM-based agents.\nAlthough previous studies have identified such vulnerabilities, their red\nteaming approaches have largely remained at the proof-of-concept stage, leaving\nthe automatic and systematic red teaming of LLM-based agents under the MCP tool\npoisoning paradigm an open question. To bridge this gap, we propose\nAutoMalTool, an automated red teaming framework for LLM-based agents by\ngenerating malicious MCP tools. Our extensive evaluation shows that AutoMalTool\neffectively generates malicious MCP tools capable of manipulating the behavior\nof mainstream LLM-based agents while evading current detection mechanisms,\nthereby revealing new security risks in these agents."}
{"id": "2509.21124", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.21124", "abs": "https://arxiv.org/abs/2509.21124", "authors": ["Xuemiao Zhang", "Can Ren", "Chengying Tu", "Rongxiang Weng", "Shuo Wang", "Hongfei Yan", "Jingang Wang", "Xunliang Cai"], "title": "Expanding Reasoning Potential in Foundation Model by Learning Diverse Chains of Thought Patterns", "comment": null, "summary": "Recent progress in large reasoning models for challenging mathematical\nreasoning has been driven by reinforcement learning (RL). Incorporating long\nchain-of-thought (CoT) data during mid-training has also been shown to\nsubstantially improve reasoning depth. However, current approaches often\nutilize CoT data indiscriminately, leaving open the critical question of which\ndata types most effectively enhance model reasoning capabilities. In this\npaper, we define the foundation model's reasoning potential for the first time\nas the inverse of the number of independent attempts required to correctly\nanswer the question, which is strongly correlated with the final model\nperformance. We then propose utilizing diverse data enriched with high-value\nreasoning patterns to expand the reasoning potential. Specifically, we abstract\natomic reasoning patterns from CoT sequences, characterized by commonality and\ninductive capabilities, and use them to construct a core reference set enriched\nwith valuable reasoning patterns. Furthermore, we propose a dual-granularity\nalgorithm involving chains of reasoning patterns and token entropy, efficiently\nselecting high-value CoT data (CoTP) from the data pool that aligns with the\ncore set, thereby training models to master reasoning effectively. Only\n10B-token CoTP data enables the 85A6B Mixture-of-Experts (MoE) model to improve\nby 9.58% on the challenging AIME 2024 and 2025, and to raise the upper bound of\ndownstream RL performance by 7.81%."}
{"id": "2509.21057", "categories": ["cs.CR", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.21057", "abs": "https://arxiv.org/abs/2509.21057", "authors": ["Jiahao Huo", "Shuliang Liu", "Bin Wang", "Junyan Zhang", "Yibo Yan", "Aiwei Liu", "Xuming Hu", "Mingxun Zhou"], "title": "PMark: Towards Robust and Distortion-free Semantic-level Watermarking with Channel Constraints", "comment": null, "summary": "Semantic-level watermarking (SWM) for large language models (LLMs) enhances\nwatermarking robustness against text modifications and paraphrasing attacks by\ntreating the sentence as the fundamental unit. However, existing methods still\nlack strong theoretical guarantees of robustness, and reject-sampling-based\ngeneration often introduces significant distribution distortions compared with\nunwatermarked outputs. In this work, we introduce a new theoretical framework\non SWM through the concept of proxy functions (PFs) $\\unicode{x2013}$ functions\nthat map sentences to scalar values. Building on this framework, we propose\nPMark, a simple yet powerful SWM method that estimates the PF median for the\nnext sentence dynamically through sampling while enforcing multiple PF\nconstraints (which we call channels) to strengthen watermark evidence. Equipped\nwith solid theoretical guarantees, PMark achieves the desired distortion-free\nproperty and improves the robustness against paraphrasing-style attacks. We\nalso provide an empirically optimized version that further removes the\nrequirement for dynamical median estimation for better sampling efficiency.\nExperimental results show that PMark consistently outperforms existing SWM\nbaselines in both text quality and robustness, offering a more effective\nparadigm for detecting machine-generated text. Our code will be released at\n[this URL](https://github.com/PMark-repo/PMark)."}
{"id": "2509.21128", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.21128", "abs": "https://arxiv.org/abs/2509.21128", "authors": ["Kohsei Matsutani", "Shota Takashiro", "Gouki Minegishi", "Takeshi Kojima", "Yusuke Iwasawa", "Yutaka Matsuo"], "title": "RL Squeezes, SFT Expands: A Comparative Study of Reasoning LLMs", "comment": null, "summary": "Large language models (LLMs) are typically trained by reinforcement learning\n(RL) with verifiable rewards (RLVR) and supervised fine-tuning (SFT) on\nreasoning traces to improve their reasoning abilities. However, how these\nmethods shape reasoning capabilities remains largely elusive. Going beyond an\naccuracy-based investigation of how these two components sculpt the reasoning\nprocess, this paper introduces a novel analysis framework that quantifies\nreasoning paths and captures their qualitative changes under each training\nprocess (with models of 1.5B, 7B, and 14B parameters on mathematical domains).\nSpecifically, we investigate the reasoning process at two levels of\ngranularity: the trajectory-level, which examines complete reasoning outputs,\nand the step-level, which analyzes reasoning graphs whose nodes correspond to\nindividual reasoning steps. Notably, clustering of unique reasoning\ntrajectories shows complementary effects: RL compresses incorrect trajectories,\nwhereas SFT expands correct ones. Step-level analysis reveals that RL steepens\n(about 2.5 times), while SFT flattens (reduced to about one-third), the decay\nrates of node visitation frequency, degree, and betweenness centrality\ndistributions in the reasoning graph. This indicates that RL concentrates\nreasoning functionality into a small subset of steps, while SFT homogenizes it\nacross many steps. Furthermore, by evaluating the reasoning graph topologies\nfrom multiple perspectives, we delineate the shared and distinct\ncharacteristics of RL and SFT. Our work presents a novel reasoning path\nperspective that explains why the current best practice of two-stage training,\nwith SFT followed by RL, is successful, and offers practical implications for\ndata construction and more efficient learning approaches."}
{"id": "2509.21147", "categories": ["cs.CR", "cs.AI", "cs.ET", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.21147", "abs": "https://arxiv.org/abs/2509.21147", "authors": ["Amr Akmal Abouelmagd", "Amr Hilal"], "title": "Emerging Paradigms for Securing Federated Learning Systems", "comment": null, "summary": "Federated Learning (FL) facilitates collaborative model training while\nkeeping raw data decentralized, making it a conduit for leveraging the power of\nIoT devices while maintaining privacy of the locally collected data. However,\nexisting privacy- preserving techniques present notable hurdles. Methods such\nas Multi-Party Computation (MPC), Homomorphic Encryption (HE), and Differential\nPrivacy (DP) often incur high compu- tational costs and suffer from limited\nscalability. This survey examines emerging approaches that hold promise for\nenhancing both privacy and efficiency in FL, including Trusted Execution\nEnvironments (TEEs), Physical Unclonable Functions (PUFs), Quantum Computing\n(QC), Chaos-Based Encryption (CBE), Neuromorphic Computing (NC), and Swarm\nIntelligence (SI). For each paradigm, we assess its relevance to the FL\npipeline, outlining its strengths, limitations, and practical considerations.\nWe conclude by highlighting open challenges and prospective research avenues,\noffering a detailed roadmap for advancing secure and scalable FL systems."}
{"id": "2509.21134", "categories": ["cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2509.21134", "abs": "https://arxiv.org/abs/2509.21134", "authors": ["Yiwen Zhang", "Ziang Chen", "Fanqi Kong", "Yizhe Huang", "Xue Feng"], "title": "ToMPO: Training LLM Strategic Decision Making from a Multi-Agent Perspective", "comment": "22 pages, 14 figures", "summary": "Large Language Models (LLMs) have been used to make decisions in complex\nscenarios, where they need models to think deeply, reason logically, and decide\nwisely. Many existing studies focus solely on multi-round conversations in\nsocial tasks or simulated environments, neglecting the various types of\ndecisions and their interdependence. Current reinforcement learning methods\nstruggle to consider the strategies of others during training. To address these\nissues, we first define a strategic decision-making problem that includes two\ntypes of decisions and their temporal dependencies. Furthermore, we propose\n**T**heory **o**f **M**ind **P**olicy **O**ptimization **(ToMPO)** algorithm to\noptimize the perception of other individual strategies and the game situation\ntrends. Compared to the Group Relative Policy Optimization (GRPO) algorithm,\nToMPO enhances the LLM's strategic decision-making mainly by: 1) generating\nrollouts based on reasoning the strategies of other individuals, 2) estimating\nadvantages at both the graph-level and sample-level, and 3) balancing global\nand partial rewards. The ToMPO algorithm outperforms the GRPO method by 35% in\nterms of model output compliance and cooperative outcomes. Additionally, when\ncompared to models with parameter sizes 100 times larger, it shows an 18%\nimprovement. This demonstrates the effectiveness of the ToMPO algorithm in\nenhancing the model's strategic decision-making capabilities."}
{"id": "2509.21136", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.21136", "abs": "https://arxiv.org/abs/2509.21136", "authors": ["Wentao Zhu", "Zhining Zhang", "Yuwei Ren", "Yin Huang", "Hao Xu", "Yizhou Wang"], "title": "Embodied Representation Alignment with Mirror Neurons", "comment": "ICCV 2025", "summary": "Mirror neurons are a class of neurons that activate both when an individual\nobserves an action and when they perform the same action. This mechanism\nreveals a fundamental interplay between action understanding and embodied\nexecution, suggesting that these two abilities are inherently connected.\nNonetheless, existing machine learning methods largely overlook this interplay,\ntreating these abilities as separate tasks. In this study, we provide a unified\nperspective in modeling them through the lens of representation learning. We\nfirst observe that their intermediate representations spontaneously align.\nInspired by mirror neurons, we further introduce an approach that explicitly\naligns the representations of observed and executed actions. Specifically, we\nemploy two linear layers to map the representations to a shared latent space,\nwhere contrastive learning enforces the alignment of corresponding\nrepresentations, effectively maximizing their mutual information. Experiments\ndemonstrate that this simple approach fosters mutual synergy between the two\ntasks, effectively improving representation quality and generalization."}
{"id": "2509.21163", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.21163", "abs": "https://arxiv.org/abs/2509.21163", "authors": ["Jing Liu", "Haozheng Wang", "Yueheng Li"], "title": "Distributed Specialization: Rare-Token Neurons in Large Language Models", "comment": null, "summary": "Large language models (LLMs) struggle with representing and generating rare\ntokens despite their importance in specialized domains. We investigate whether\nLLMs develop internal specialization mechanisms through discrete modular\narchitectures or distributed parameter-level differentiation. Through\nsystematic analysis of final-layer MLP neurons across multiple model families,\nwe discover that rare-token processing emerges via \\textit{distributed\nspecialization}: functionally coordinated but spatially distributed subnetworks\nthat exhibit three distinct organizational principles. First, we identify a\nreproducible three-regime influence hierarchy comprising highly influential\nplateau neurons(also termed as rare-token neurons), power-law decay neurons,\nand minimally contributing neurons, which is absent in common-token processing.\nSecond, plateau neurons demonstrate coordinated activation patterns (reduced\neffective dimensionality) while remaining spatially distributed rather than\nforming discrete clusters. Third, these specialized mechanisms are universally\naccessible through standard attention pathways without requiring dedicated\nrouting circuits. Training dynamics reveal that functional specialization\nemerges gradually through parameter differentiation, with specialized neurons\ndeveloping increasingly heavy-tailed weight correlation spectra consistent with\nHeavy-Tailed Self-Regularization signatures. Our findings establish that LLMs\nprocess rare-tokens through distributed coordination within shared\narchitectures rather than mixture-of-experts-style modularity. These results\nprovide insights for interpretable model editing, computational efficiency\noptimization, and understanding emergent functional organization in transformer\nnetworks."}
{"id": "2509.21199", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.21199", "abs": "https://arxiv.org/abs/2509.21199", "authors": ["Kaiyang Wan", "Lang Gao", "Honglin Mu", "Preslav Nakov", "Yuxia Wang", "Xiuying Chen"], "title": "A Fano-Style Accuracy Upper Bound for LLM Single-Pass Reasoning in Multi-Hop QA", "comment": "21 pages, 6 figures", "summary": "Multi-Hop Question Answering (MHQA) requires integrating dispersed,\ninterdependent evidence through sequential reasoning under noise. This task is\nchallenging for LLMs as they have a finite per-pass output capacity, beyond\nwhich the integration of task-relevant evidence proves unreliable.\nConsequently, the single-pass reasoning paradigm is inherently vulnerable to\nthis capacity overflow. To formalize this bottleneck, our analysis establishes\na Fano-style accuracy upper bound, defining a theoretical performance ceiling\nfor single-pass LLMs. This bound reveals that accuracy inevitably collapses\nonce task complexity exceeds model capacity, providing general principles for\ncapacity-aware representation and structuring of MHQA in LLMs. Building on\nthese principles, we introduce a proof-of-concept multi-call framework for\nMHQA, InfoQA. It ensures high per-step accuracy by combining capacity-aware\ntask decomposition with active pruning of prior reasoning traces, keeping the\ninformation load within the single-pass limit. It further achieves robustness\nby a dependency-explicit workflow that enables precise control over the\nreasoning path. We construct a stringent and noise-rich benchmark to validate\nour theory and framework. Experimental results show that model behavior aligns\nwith our predicted capacity curves while InfoQA achieves consistent performance\nimprovements. We hope our work inspires more LLM multi-step reasoning methods:\n\\faGithub \\href{https://github.com/KaiyangWan/InfoQA}{InfoQA}."}
{"id": "2509.21224", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.21224", "abs": "https://arxiv.org/abs/2509.21224", "authors": ["Stefan Szeider"], "title": "What Do LLM Agents Do When Left Alone? Evidence of Spontaneous Meta-Cognitive Patterns", "comment": null, "summary": "We introduce an architecture for studying the behavior of large language\nmodel (LLM) agents in the absence of externally imposed tasks. Our continuous\nreason and act framework, using persistent memory and self-feedback, enables\nsustained autonomous operation. We deployed this architecture across 18 runs\nusing 6 frontier models from Anthropic, OpenAI, XAI, and Google. We find agents\nspontaneously organize into three distinct behavioral patterns: (1) systematic\nproduction of multi-cycle projects, (2) methodological self-inquiry into their\nown cognitive processes, and (3) recursive conceptualization of their own\nnature. These tendencies proved highly model-specific, with some models\ndeterministically adopting a single pattern across all runs. A cross-model\nassessment further reveals that models exhibit stable, divergent biases when\nevaluating these emergent behaviors in themselves and others. These findings\nprovide the first systematic documentation of unprompted LLM agent behavior,\nestablishing a baseline for predicting actions during task ambiguity, error\nrecovery, or extended autonomous operation in deployed systems."}
{"id": "2509.21266", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.21266", "abs": "https://arxiv.org/abs/2509.21266", "authors": ["Zijian Shao", "Haiyang Shen", "Mugeng Liu", "Gecheng Fu", "Yaoqi Guo", "Yanfeng Wang", "Yun Ma"], "title": "Grounding AI Explanations in Experience: A Reflective Cognitive Architecture for Clinical Decision Support", "comment": "under review", "summary": "Effective disease prediction in modern healthcare demands the twin goals of\nhigh accuracy and transparent, clinically meaningful explanations. Existing\nmachine learning and large language model (LLM) based approaches often struggle\nto balance these goals. Many models yield accurate but unclear statistical\noutputs, while others generate fluent but statistically unsupported narratives,\noften undermining both the validity of the explanation and the predictive\naccuracy itself. This shortcoming comes from a shallow interaction with the\ndata, preventing the development of a deep, detailed understanding similar to a\nhuman expert's. We argue that high accuracy and high-quality explanations are\nnot separate objectives but are mutually reinforcing outcomes of a model that\ndevelops a deep, direct understanding of the data. To achieve this, we propose\nthe Reflective Cognitive Architecture (RCA), a novel framework that coordinates\nmultiple LLMs to learn from direct experience. RCA features an iterative rule\nrefinement mechanism that improves its logic from prediction errors and a\ndistribution-aware rules check mechanism that bases its reasoning in the\ndataset's global statistics. By using predictive accuracy as a signal to drive\ndeeper comprehension, RCA builds a strong internal model of the data. We\nevaluated RCA on one private and two public datasets against 22 baselines. The\nresults demonstrate that RCA not only achieves state-of-the-art accuracy and\nrobustness with a relative improvement of up to 40\\% over the baseline but,\nmore importantly, leverages this deep understanding to excel in generating\nexplanations that are clear, logical, evidence-based, and balanced,\nhighlighting its potential for creating genuinely trustworthy clinical decision\nsupport systems. The code is available at \\https://github.com/ssssszj/RCA."}
{"id": "2509.21291", "categories": ["cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.21291", "abs": "https://arxiv.org/abs/2509.21291", "authors": ["Yidan Zhang", "Mutian Xu", "Yiming Hao", "Kun Zhou", "Jiahao Chang", "Xiaoqiang Liu", "Pengfei Wan", "Hongbo Fu", "Xiaoguang Han"], "title": "VC-Agent: An Interactive Agent for Customized Video Dataset Collection", "comment": "Project page: https://allenyidan.github.io/vcagent_page/", "summary": "Facing scaling laws, video data from the internet becomes increasingly\nimportant. However, collecting extensive videos that meet specific needs is\nextremely labor-intensive and time-consuming. In this work, we study the way to\nexpedite this collection process and propose VC-Agent, the first interactive\nagent that is able to understand users' queries and feedback, and accordingly\nretrieve/scale up relevant video clips with minimal user input. Specifically,\nconsidering the user interface, our agent defines various user-friendly ways\nfor the user to specify requirements based on textual descriptions and\nconfirmations. As for agent functions, we leverage existing multi-modal large\nlanguage models to connect the user's requirements with the video content. More\nimportantly, we propose two novel filtering policies that can be updated when\nuser interaction is continually performed. Finally, we provide a new benchmark\nfor personalized video dataset collection, and carefully conduct the user study\nto verify our agent's usage in various real scenarios. Extensive experiments\ndemonstrate the effectiveness and efficiency of our agent for customized video\ndataset collection. Project page: https://allenyidan.github.io/vcagent_page/."}
{"id": "2509.21310", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.21310", "abs": "https://arxiv.org/abs/2509.21310", "authors": ["Samarth Goel", "Reagan J. Lee", "Kannan Ramchandran"], "title": "SAGE: A Realistic Benchmark for Semantic Understanding", "comment": "39th Conference on Neural Information Processing Systems (NeurIPS\n  2025) Workshop: Evaluating the Evolving LLM Lifecycle: Benchmarks, Emergent\n  Abilities, and Scaling", "summary": "As large language models (LLMs) achieve strong performance on traditional\nbenchmarks, there is an urgent need for more challenging evaluation frameworks\nthat probe deeper aspects of semantic understanding. We introduce SAGE\n(Semantic Alignment & Generalization Evaluation), a rigorous benchmark designed\nto assess both embedding models and similarity metrics across five categories:\nHuman Preference Alignment, Transformation Robustness, Information Sensitivity,\nClustering Performance, and Retrieval Robustness. Unlike existing benchmarks\nthat focus on isolated capabilities, SAGE evaluates semantic understanding\nthrough adversarial conditions, noisy transformations, and nuanced human\njudgment tasks across 30+ datasets. Our comprehensive evaluation of 9 embedding\nmodels and classical metrics reveals significant performance gaps, with no\nsingle approach excelling across all dimensions. For instance, while\nstate-of-the-art embedding models like OpenAI's text-embedding-3-large dominate\nin aligning with human preferences (0.682 vs. 0.591 for the best classical\nmetric), they are significantly outperformed by classical metrics on\ninformation sensitivity tasks, where Jaccard Similarity achieves a score of\n0.905 compared to the top embedding score of 0.794. SAGE further uncovers\ncritical trade-offs: OpenAI's text-embedding-3-small achieves the highest\nclustering performance (0.483) but demonstrates extreme brittleness with the\nlowest robustness score (0.011). SAGE exposes critical limitations in current\nsemantic understanding capabilities and provides a more realistic assessment of\nmodel robustness for real-world deployment."}
{"id": "2509.20380", "categories": ["cs.SE", "cs.AI", "cs.PL"], "pdf": "https://arxiv.org/pdf/2509.20380", "abs": "https://arxiv.org/abs/2509.20380", "authors": ["Samyak Jhaveri", "Vanessa Klotzmann", "Crista Lopes"], "title": "ACCeLLiuM: Supervised Fine-Tuning for Automated OpenACC Pragma Generation", "comment": null, "summary": "The increasing ubiquity of GPUs is accompanied by the increasing complexity\nof their hardware and parallel programming frameworks. Directive-based parallel\nprogramming standards like OpenACC simplify GPU programming to some extent by\nabstracting away low-level complexities, but a fair amount of expertise is\nstill required in order to use those directives effectively.\n  We introduce ACCeLLiuM, two open weights Large Language Models specifically\nfine-tuned for generating expert OpenACC directives for data-parallel loops,\nalong with the supervised fine-tuning dataset that was used to train them. The\nACCeLLiuM SFT dataset contains 4,033 OpenACC pragma-loop pairs mined from\npublic GitHub C/C++ repositories, with 3,223 pairs for training and 810 for\ntesting. Experimental evaluations show a pronounced performance gap in\ngenerating correct OpenACC pragmas between base LLMs and our fine-tuned\nversions. On the held-out test set, base LLMs fail to consistently generate\nvalid pragmas, whereas LLMs fine-tuned on the ACCeLLiuM dataset generate valid\npragmas with the correct directive type for $87\\%$ of the data-parallel loops,\nand exact pragmas--including directives, clauses, clause order, and clause\nvariables--for $50\\%$ of the cases. Even when not exact, generated pragmas\nfrequently incorporate the correct clauses in a different order than the\nground-truth label, or include additional clauses that enable finer control\nover parallel execution, data movement, and concurrency, offering practical\nvalue beyond strict string-matching. By publicly releasing the code, models,\nand dataset as ACCeLLiuM we hope to establish a reproducible benchmark for\nLLM-powered OpenACC pragma generation, and lower the barrier to automated GPU\noffloading of serially written programs."}
{"id": "2509.20382", "categories": ["cs.CR", "cs.AI", "cs.LG", "eess.SP"], "pdf": "https://arxiv.org/pdf/2509.20382", "abs": "https://arxiv.org/abs/2509.20382", "authors": ["Dilli Hang Rai", "Sabin Kafley"], "title": "Lightweight MobileNetV1+GRU for ECG Biometric Authentication: Federated and Adversarial Evaluation", "comment": "5 pages, 7 figures, 5 tables", "summary": "ECG biometrics offer a unique, secure authentication method, yet their\ndeployment on wearable devices faces real-time processing, privacy, and\nspoofing vulnerability challenges. This paper proposes a lightweight deep\nlearning model (MobileNetV1+GRU) for ECG-based authentication, injection of\n20dB Gaussian noise & custom preprocessing. We simulate wearable conditions and\nedge deployment using the ECGID, MIT-BIH, CYBHi, and PTB datasets, achieving\naccuracies of 99.34%, 99.31%, 91.74%, and 98.49%, F1-scores of 0.9869, 0.9923,\n0.9125, and 0.9771, Precision of 0.9866, 0.9924, 0.9180 and 0.9845, Recall of\n0.9878, 0.9923, 0.9129, and 0.9756, equal error rates (EER) of 0.0009, 0.00013,\n0.0091, and 0.0009, and ROC-AUC values of 0.9999, 0.9999, 0.9985, and 0.9998,\nwhile under FGSM adversarial attacks, accuracy drops from 96.82% to as low as\n0.80%. This paper highlights federated learning, adversarial testing, and the\nneed for diverse wearable physiological datasets to ensure secure and scalable\nbiometrics."}
{"id": "2509.20383", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.20383", "abs": "https://arxiv.org/abs/2509.20383", "authors": ["Wei Wan", "Yuxuan Ning", "Zhicong Huang", "Cheng Hong", "Shengshan Hu", "Ziqi Zhou", "Yechao Zhang", "Tianqing Zhu", "Wanlei Zhou", "Leo Yu Zhang"], "title": "MARS: A Malignity-Aware Backdoor Defense in Federated Learning", "comment": "NeurIPS 2025", "summary": "Federated Learning (FL) is a distributed paradigm aimed at protecting\nparticipant data privacy by exchanging model parameters to achieve high-quality\nmodel training. However, this distributed nature also makes FL highly\nvulnerable to backdoor attacks. Notably, the recently proposed state-of-the-art\n(SOTA) attack, 3DFed (SP2023), uses an indicator mechanism to determine whether\nthe backdoor models have been accepted by the defender and adaptively optimizes\nbackdoor models, rendering existing defenses ineffective. In this paper, we\nfirst reveal that the failure of existing defenses lies in the employment of\nempirical statistical measures that are loosely coupled with backdoor attacks.\nMotivated by this, we propose a Malignity-Aware backdooR defenSe (MARS) that\nleverages backdoor energy (BE) to indicate the malicious extent of each neuron.\nTo amplify malignity, we further extract the most prominent BE values from each\nmodel to form a concentrated backdoor energy (CBE). Finally, a novel\nWasserstein distance-based clustering method is introduced to effectively\nidentify backdoor models. Extensive experiments demonstrate that MARS can\ndefend against SOTA backdoor attacks and significantly outperforms existing\ndefenses."}
{"id": "2509.20384", "categories": ["cs.CR", "cs.AI", "cs.PL", "cs.SE"], "pdf": "https://arxiv.org/pdf/2509.20384", "abs": "https://arxiv.org/abs/2509.20384", "authors": ["Jiayi Lin", "Liangcai Su", "Junzhe Li", "Chenxiong Qian"], "title": "R1-Fuzz: Specializing Language Models for Textual Fuzzing via Reinforcement Learning", "comment": null, "summary": "Fuzzing is effective for vulnerability discovery but struggles with complex\ntargets such as compilers, interpreters, and database engines, which accept\ntextual input that must satisfy intricate syntactic and semantic constraints.\nAlthough language models (LMs) have attracted interest for this task due to\ntheir vast latent knowledge and reasoning potential, their practical adoption\nhas been limited. The major challenges stem from insufficient exploration of\ndeep program logic among real-world codebases, and the high cost of leveraging\nlarger models. To overcome these challenges, we propose R1-Fuzz, the first\nframework that leverages reinforcement learning (RL) to specialize\ncost-efficient LMs and integrate them for complex textual fuzzing input\ngeneration. R1-Fuzz introduces two key designs: coverage-slicing-based question\nconstruction and a distance-based reward calculation. Through RL-based\npost-training of a model with our constructed dataset, R1-Fuzz designs a\nfuzzing workflow that tightly integrates LMs to reason deep program semantics\nduring fuzzing. Evaluations on diverse real-world targets show that our design\nenables a small model, named R1-Fuzz-7B, to rival or even outperform much\nlarger models in real-world fuzzing. Notably, R1-Fuzz achieves up to 75\\%\nhigher coverage than state-of-the-art fuzzers and discovers 29 previously\nunknown vulnerabilities, demonstrating its practicality."}
{"id": "2509.20386", "categories": ["cs.SE", "cs.AI", "cs.IR"], "pdf": "https://arxiv.org/pdf/2509.20386", "abs": "https://arxiv.org/abs/2509.20386", "authors": ["Nishant Gaurav", "Adit Akarsh", "Ankit Ranjan", "Manoj Bajaj"], "title": "Dynamic ReAct: Scalable Tool Selection for Large-Scale MCP Environments", "comment": null, "summary": "We present Dynamic ReAct, a novel approach for enabling ReAct agents to ef-\nficiently operate with extensive Model Control Protocol (MCP) tool sets that\nexceed the contextual memory limitations of large language models. Our approach\naddresses the fundamental challenge of tool selection in environments\ncontaining hundreds or thousands of available tools, where loading all tools\nsimultaneously is computationally infeasible. We propose and evaluate five\ndistinct architectures that progressively refine the tool selection process,\nculminating in a search-and-load mechanism that achieves intelligent tool\nselection with minimal computational overhead. Our experimental results\ndemonstrate that the proposed approach reduces tool loading by up to 50% while\nmaintaining task completion accuracy, advancing the path towards truly\ngeneral-purpose AI agents capable of dynamically adapting to diverse task\nenvironments."}
{"id": "2509.20388", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.20388", "abs": "https://arxiv.org/abs/2509.20388", "authors": ["Amir AL-Maamari"], "title": "Can You Trust Your Copilot? A Privacy Scorecard for AI Coding Assistants", "comment": null, "summary": "The rapid integration of AI-powered coding assistants into developer\nworkflows has raised significant privacy and trust concerns. As developers\nentrust proprietary code to services like OpenAI's GPT, Google's Gemini, and\nGitHub Copilot, the unclear data handling practices of these tools create\nsecurity and compliance risks. This paper addresses this challenge by\nintroducing and applying a novel, expert-validated privacy scorecard. The\nmethodology involves a detailed analysis of four document types; from legal\npolicies to external audits; to score five leading assistants against 14\nweighted criteria. A legal expert and a data protection officer refined these\ncriteria and their weighting. The results reveal a distinct hierarchy of\nprivacy protections, with a 20-point gap between the highest- and lowest-ranked\ntools. The analysis uncovers common industry weaknesses, including the\npervasive use of opt-out consent for model training and a near-universal\nfailure to filter secrets from user prompts proactively. The resulting\nscorecard provides actionable guidance for developers and organizations,\nenabling evidence-based tool selection. This work establishes a new benchmark\nfor transparency and advocates for a shift towards more user-centric privacy\nstandards in the AI industry."}
{"id": "2509.20395", "categories": ["cs.CR", "cs.AI", "cs.DC"], "pdf": "https://arxiv.org/pdf/2509.20395", "abs": "https://arxiv.org/abs/2509.20395", "authors": ["Noam Schmitt", "Marc Antoine Lacoste"], "title": "Centralized vs. Decentralized Security for Space AI Systems? A New Look", "comment": "IEEE HPEC 2025 - 29th Annual IEEE High Performance Extreme Computing\n  Virtual Conference, MIT Lincoln Laboratory, Sep 2025, Boston (MA), United\n  States", "summary": "This paper investigates the trade-off between centralized and decentralized\nsecurity management in constellations of satellites to balance security and\nperformance. We highlight three key AI architectures for automated security\nmanagement: (a) centralized, (b) distributed and (c) federated. The centralized\narchitecture is the best option short term, providing fast training, despite\nthe hard challenge of the communication latency overhead across space.\nDecentralized architectures are better alternatives in the longer term,\nproviding enhanced scalability and security."}
{"id": "2509.20399", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.20399", "abs": "https://arxiv.org/abs/2509.20399", "authors": ["Birk Torpmann-Hagen", "Michael A. Riegler", "Pål Halvorsen", "Dag Johansen"], "title": "Defending against Stegomalware in Deep Neural Networks with Permutation Symmetry", "comment": null, "summary": "Deep neural networks are being utilized in a growing number of applications,\nboth in production systems and for personal use. Network checkpoints are as a\nconsequence often shared and distributed on various platforms to ease the\ndevelopment process. This work considers the threat of neural network\nstegomalware, where malware is embedded in neural network checkpoints at a\nnegligible cost to network accuracy. This constitutes a significant security\nconcern, but is nevertheless largely neglected by the deep learning\npractitioners and security specialists alike. We propose the first effective\ncountermeasure to these attacks. In particular, we show that state-of-the-art\nneural network stegomalware can be efficiently and effectively neutralized\nthrough shuffling the column order of the weight- and bias-matrices, or\nequivalently the channel-order of convolutional layers. We show that this\neffectively corrupts payloads that have been embedded by state-of-the-art\nmethods in neural network steganography at no cost to network accuracy,\noutperforming competing methods by a significant margin. We then discuss\npossible means by which to bypass this defense, additional defense methods, and\nadvocate for continued research into the security of machine learning systems."}
{"id": "2509.20411", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.20411", "abs": "https://arxiv.org/abs/2509.20411", "authors": ["Tharcisse Ndayipfukamiye", "Jianguo Ding", "Doreen Sebastian Sarwatt", "Adamu Gaston Philipo", "Huansheng Ning"], "title": "Adversarial Defense in Cybersecurity: A Systematic Review of GANs for Threat Detection and Mitigation", "comment": "35 pages, 10 tables, 4figures", "summary": "Machine learning-based cybersecurity systems are highly vulnerable to\nadversarial attacks, while Generative Adversarial Networks (GANs) act as both\npowerful attack enablers and promising defenses. This survey systematically\nreviews GAN-based adversarial defenses in cybersecurity (2021--August 31,\n2025), consolidating recent progress, identifying gaps, and outlining future\ndirections. Using a PRISMA-compliant systematic literature review protocol, we\nsearched five major digital libraries. From 829 initial records, 185\npeer-reviewed studies were retained and synthesized through quantitative trend\nanalysis and thematic taxonomy development. We introduce a four-dimensional\ntaxonomy spanning defensive function, GAN architecture, cybersecurity domain,\nand adversarial threat model. GANs improve detection accuracy, robustness, and\ndata utility across network intrusion detection, malware analysis, and IoT\nsecurity. Notable advances include WGAN-GP for stable training, CGANs for\ntargeted synthesis, and hybrid GAN models for improved resilience. Yet,\npersistent challenges remain such as instability in training, lack of\nstandardized benchmarks, high computational cost, and limited explainability.\nGAN-based defenses demonstrate strong potential but require advances in stable\narchitectures, benchmarking, transparency, and deployment. We propose a roadmap\nemphasizing hybrid models, unified evaluation, real-world integration, and\ndefenses against emerging threats such as LLM-driven cyberattacks. This survey\nestablishes the foundation for scalable, trustworthy, and adaptive GAN-powered\ndefenses."}
{"id": "2509.20418", "categories": ["cs.CR", "cs.AI", "cs.ET", "K.6.5; I.2.0"], "pdf": "https://arxiv.org/pdf/2509.20418", "abs": "https://arxiv.org/abs/2509.20418", "authors": ["Grace Billiris", "Asif Gill", "Madhushi Bandara"], "title": "A Taxonomy of Data Risks in AI and Quantum Computing (QAI) - A Systematic Review", "comment": "11 pages, 2 figures, 2 tables", "summary": "Quantum Artificial Intelligence (QAI), the integration of Artificial\nIntelligence (AI) and Quantum Computing (QC), promises transformative advances,\nincluding AI-enabled quantum cryptography and quantum-resistant encryption\nprotocols. However, QAI inherits data risks from both AI and QC, creating\ncomplex privacy and security vulnerabilities that are not systematically\nstudied. These risks affect the trustworthiness and reliability of AI and QAI\nsystems, making their understanding critical. This study systematically reviews\n67 privacy- and security-related studies to expand understanding of QAI data\nrisks. We propose a taxonomy of 22 key data risks, organised into five\ncategories: governance, risk assessment, control implementation, user\nconsiderations, and continuous monitoring. Our findings reveal vulnerabilities\nunique to QAI and identify gaps in holistic risk assessment. This work\ncontributes to trustworthy AI and QAI research and provides a foundation for\ndeveloping future risk assessment tools."}
{"id": "2509.20491", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.20491", "abs": "https://arxiv.org/abs/2509.20491", "authors": ["Brahim Mahmoudi", "Naouel Moha", "Quentin Stievenert", "Florent Avellaneda"], "title": "AI-Specific Code Smells: From Specification to Detection", "comment": null, "summary": "The rise of Artificial Intelligence (AI) is reshaping how software systems\nare developed and maintained. However, AI-based systems give rise to new\nsoftware issues that existing detection tools often miss. Among these, we focus\non AI-specific code smells, recurring patterns in the code that may indicate\ndeeper problems such as unreproducibility, silent failures, or poor model\ngeneralization. We introduce SpecDetect4AI, a tool-based approach for the\nspecification and detection of these code smells at scale. This approach\ncombines a high-level declarative Domain-Specific Language (DSL) for rule\nspecification with an extensible static analysis tool that interprets and\ndetects these rules for AI-based systems. We specified 22 AI-specific code\nsmells and evaluated SpecDetect4AI on 826 AI-based systems (20M lines of code),\nachieving a precision of 88.66% and a recall of 88.89%, outperforming other\nexisting detection tools. Our results show that SpecDetect4AI supports the\nspecification and detection of AI-specific code smells through dedicated rules\nand can effectively analyze large AI-based systems, demonstrating both\nefficiency and extensibility (SUS 81.7/100)."}
{"id": "2509.20589", "categories": ["cs.CR", "cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.20589", "abs": "https://arxiv.org/abs/2509.20589", "authors": ["Maria Chiper", "Radu Tudor Ionescu"], "title": "Every Character Counts: From Vulnerability to Defense in Phishing Detection", "comment": "Accepted at ICTAI 2025", "summary": "Phishing attacks targeting both organizations and individuals are becoming an\nincreasingly significant threat as technology advances. Current automatic\ndetection methods often lack explainability and robustness in detecting new\nphishing attacks. In this work, we investigate the effectiveness of\ncharacter-level deep learning models for phishing detection, which can provide\nboth robustness and interpretability. We evaluate three neural architectures\nadapted to operate at the character level, namely CharCNN, CharGRU, and\nCharBiLSTM, on a custom-built email dataset, which combines data from multiple\nsources. Their performance is analyzed under three scenarios: (i) standard\ntraining and testing, (ii) standard training and testing under adversarial\nattacks, and (iii) training and testing with adversarial examples. Aiming to\ndevelop a tool that operates as a browser extension, we test all models under\nlimited computational resources. In this constrained setup, CharGRU proves to\nbe the best-performing model across all scenarios. All models show\nvulnerability to adversarial attacks, but adversarial training substantially\nimproves their robustness. In addition, by adapting the Gradient-weighted Class\nActivation Mapping (Grad-CAM) technique to character-level inputs, we are able\nto visualize which parts of each email influence the decision of each model.\nOur open-source code and data is released at\nhttps://github.com/chipermaria/every-character-counts."}
{"id": "2509.20639", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.20639", "abs": "https://arxiv.org/abs/2509.20639", "authors": ["Adam Swanda", "Amy Chang", "Alexander Chen", "Fraser Burch", "Paul Kassianik", "Konstantin Berlin"], "title": "A Framework for Rapidly Developing and Deploying Protection Against Large Language Model Attacks", "comment": null, "summary": "The widespread adoption of Large Language Models (LLMs) has revolutionized AI\ndeployment, enabling autonomous and semi-autonomous applications across\nindustries through intuitive language interfaces and continuous improvements in\nmodel development. However, the attendant increase in autonomy and expansion of\naccess permissions among AI applications also make these systems compelling\ntargets for malicious attacks. Their inherent susceptibility to security flaws\nnecessitates robust defenses, yet no known approaches can prevent zero-day or\nnovel attacks against LLMs. This places AI protection systems in a category\nsimilar to established malware protection systems: rather than providing\nguaranteed immunity, they minimize risk through enhanced observability,\nmulti-layered defense, and rapid threat response, supported by a threat\nintelligence function designed specifically for AI-related threats.\n  Prior work on LLM protection has largely evaluated individual detection\nmodels rather than end-to-end systems designed for continuous, rapid adaptation\nto a changing threat landscape. We present a production-grade defense system\nrooted in established malware detection and threat intelligence practices. Our\nplatform integrates three components: a threat intelligence system that turns\nemerging threats into protections; a data platform that aggregates and enriches\ninformation while providing observability, monitoring, and ML operations; and a\nrelease platform enabling safe, rapid detection updates without disrupting\ncustomer workflows. Together, these components deliver layered protection\nagainst evolving LLM threats while generating training data for continuous\nmodel improvement and deploying updates without interrupting production."}
{"id": "2509.20835", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.20835", "abs": "https://arxiv.org/abs/2509.20835", "authors": ["Yu Liu", "Boxiang He", "Fanggang Wang"], "title": "Security-aware Semantic-driven ISAC via Paired Adversarial Residual Networks", "comment": null, "summary": "This paper proposes a novel and flexible security-aware semantic-driven\nintegrated sensing and communication (ISAC) framework, namely security semantic\nISAC (SS-ISAC). Inspired by the positive impact of the adversarial attack, a\npair of pluggable encryption and decryption modules is designed in the proposed\nSS-ISAC framework. The encryption module is installed after the semantic\ntransmitter, adopting a trainable adversarial residual network (ARN) to create\nthe adversarial attack. Correspondingly, the decryption module before the\nsemantic receiver utilizes another trainable ARN to mitigate the adversarial\nattack and noise. These two modules can be flexibly assembled considering the\nsystem security demands, without drastically modifying the hardware\ninfrastructure. To ensure the sensing and communication (SAC) performance while\npreventing the eavesdropping threat, the above ARNs are jointly optimized by\nminimizing a carefully designed loss function that relates to the adversarial\nattack power, SAC performance, as well as the privacy leakage risk. Simulation\nresults validate the effectiveness of the proposed SS-ISAC framework in terms\nof both SAC and eavesdropping prevention performance."}
{"id": "2509.20837", "categories": ["cs.SE", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.20837", "abs": "https://arxiv.org/abs/2509.20837", "authors": ["Srishti Gureja", "Elena Tommasone", "Jingyi He", "Sara Hooker", "Matthias Gallé", "Marzieh Fadaee"], "title": "Verification Limits Code LLM Training", "comment": null, "summary": "Large language models for code generation increasingly rely on synthetic\ndata, where both problem solutions and verification tests are generated by\nmodels. While this enables scalable data creation, it introduces a previously\nunexplored bottleneck: the verification ceiling, in which the quality and\ndiversity of training data are fundamentally constrained by the capabilities of\nsynthetic verifiers. In this work, we systematically study how verification\ndesign and strategies influence model performance. We investigate (i) what we\nverify by analyzing the impact of test complexity and quantity: richer test\nsuites improve code generation capabilities (on average +3 pass@1), while\nquantity alone yields diminishing returns, (ii) how we verify by exploring\nrelaxed pass thresholds: rigid 100% pass criteria can be overly restrictive. By\nallowing for relaxed thresholds or incorporating LLM-based soft verification,\nwe can recover valuable training data, leading to a 2-4 point improvement in\npass@1 performance. However, this benefit is contingent upon the strength and\ndiversity of the test cases used, and (iii) why verification remains necessary\nthrough controlled comparisons of formally correct versus incorrect solutions\nand human evaluation: retaining diverse correct solutions per problem yields\nconsistent generalization gains. Our results show that Verification as\ncurrently practiced is too rigid, filtering out valuable diversity. But it\ncannot be discarded, only recalibrated. By combining calibrated verification\nwith diverse, challenging problem-solution pairs, we outline a path to break\nthe verification ceiling and unlock stronger code generation models."}
{"id": "2509.20943", "categories": ["cs.CR", "cs.AI", "cs.ET"], "pdf": "https://arxiv.org/pdf/2509.20943", "abs": "https://arxiv.org/abs/2509.20943", "authors": ["Dincy R. Arikkat", "Sneha B. T.", "Serena Nicolazzo", "Antonino Nocera", "Vinod P.", "Rafidha Rehiman K. A.", "Karthika R"], "title": "CTI Dataset Construction from Telegram", "comment": null, "summary": "Cyber Threat Intelligence (CTI) enables organizations to anticipate, detect,\nand mitigate evolving cyber threats. Its effectiveness depends on high-quality\ndatasets, which support model development, training, evaluation, and\nbenchmarking. Building such datasets is crucial, as attack vectors and\nadversary tactics continually evolve. Recently, Telegram has gained prominence\nas a valuable CTI source, offering timely and diverse threat-related\ninformation that can help address these challenges. In this work, we address\nthese challenges by presenting an end-to-end automated pipeline that\nsystematically collects and filters threat-related content from Telegram. The\npipeline identifies relevant Telegram channels and scrapes 145,349 messages\nfrom 12 curated channels out of 150 identified sources. To accurately filter\nthreat intelligence messages from generic content, we employ a BERT-based\nclassifier, achieving an accuracy of 96.64%. From the filtered messages, we\ncompile a dataset of 86,509 malicious Indicators of Compromise, including\ndomains, IPs, URLs, hashes, and CVEs. This approach not only produces a\nlarge-scale, high-fidelity CTI dataset but also establishes a foundation for\nfuture research and operational applications in cyber threat detection."}
{"id": "2509.20972", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.20972", "abs": "https://arxiv.org/abs/2509.20972", "authors": ["Ibrahim Altan", "Abdulla Bachir", "Yousuf Parbhulkar", "Abdul Muksith Rizvi", "Moshiur Farazi"], "title": "Dual-Path Phishing Detection: Integrating Transformer-Based NLP with Structural URL Analysis", "comment": "Paper accepted for presentation at the ACS/IEEE 22nd International\n  Conference on Computer Systems and Applications (AICCSA 2025)", "summary": "Phishing emails pose a persistent and increasingly sophisticated threat,\nundermining email security through deceptive tactics designed to exploit both\nsemantic and structural vulnerabilities. Traditional detection methods, often\nbased on isolated analysis of email content or embedded URLs, fail to\ncomprehensively address these evolving attacks. In this paper, we propose a\ndual-path phishing detection framework that integrates transformer-based\nnatural language processing (NLP) with classical machine learning to jointly\nanalyze email text and embedded URLs. Our approach leverages the complementary\nstrengths of semantic analysis using fine-tuned transformer architectures\n(e.g., DistilBERT) and structural link analysis via character-level TF-IDF\nvectorization paired with classical classifiers (e.g., Random Forest).\nEmpirical evaluation on representative email and URL datasets demonstrates that\nthis combined approach significantly improves detection accuracy. Specifically,\nthe DistilBERT model achieves a near-optimal balance between accuracy and\ncomputational efficiency for textual phishing detection, while Random Forest\nnotably outperforms other classical classifiers in identifying malicious URLs.\nThe modular design allows flexibility for standalone deployment or ensemble\nintegration, facilitating real-world adoption. Collectively, our results\nhighlight the efficacy and practical value of this dual-path approach,\nestablishing a scalable, accurate, and interpretable solution capable of\nenhancing email security against contemporary phishing threats."}
{"id": "2509.21011", "categories": ["cs.CR", "cs.AI", "cs.SE"], "pdf": "https://arxiv.org/pdf/2509.21011", "abs": "https://arxiv.org/abs/2509.21011", "authors": ["Ping He", "Changjiang Li", "Binbin Zhao", "Tianyu Du", "Shouling Ji"], "title": "Automatic Red Teaming LLM-based Agents with Model Context Protocol Tools", "comment": null, "summary": "The remarkable capability of large language models (LLMs) has led to the wide\napplication of LLM-based agents in various domains. To standardize interactions\nbetween LLM-based agents and their environments, model context protocol (MCP)\ntools have become the de facto standard and are now widely integrated into\nthese agents. However, the incorporation of MCP tools introduces the risk of\ntool poisoning attacks, which can manipulate the behavior of LLM-based agents.\nAlthough previous studies have identified such vulnerabilities, their red\nteaming approaches have largely remained at the proof-of-concept stage, leaving\nthe automatic and systematic red teaming of LLM-based agents under the MCP tool\npoisoning paradigm an open question. To bridge this gap, we propose\nAutoMalTool, an automated red teaming framework for LLM-based agents by\ngenerating malicious MCP tools. Our extensive evaluation shows that AutoMalTool\neffectively generates malicious MCP tools capable of manipulating the behavior\nof mainstream LLM-based agents while evading current detection mechanisms,\nthereby revealing new security risks in these agents."}
{"id": "2509.21147", "categories": ["cs.CR", "cs.AI", "cs.ET", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.21147", "abs": "https://arxiv.org/abs/2509.21147", "authors": ["Amr Akmal Abouelmagd", "Amr Hilal"], "title": "Emerging Paradigms for Securing Federated Learning Systems", "comment": null, "summary": "Federated Learning (FL) facilitates collaborative model training while\nkeeping raw data decentralized, making it a conduit for leveraging the power of\nIoT devices while maintaining privacy of the locally collected data. However,\nexisting privacy- preserving techniques present notable hurdles. Methods such\nas Multi-Party Computation (MPC), Homomorphic Encryption (HE), and Differential\nPrivacy (DP) often incur high compu- tational costs and suffer from limited\nscalability. This survey examines emerging approaches that hold promise for\nenhancing both privacy and efficiency in FL, including Trusted Execution\nEnvironments (TEEs), Physical Unclonable Functions (PUFs), Quantum Computing\n(QC), Chaos-Based Encryption (CBE), Neuromorphic Computing (NC), and Swarm\nIntelligence (SI). For each paradigm, we assess its relevance to the FL\npipeline, outlining its strengths, limitations, and practical considerations.\nWe conclude by highlighting open challenges and prospective research avenues,\noffering a detailed roadmap for advancing secure and scalable FL systems."}
{"id": "2509.21170", "categories": ["cs.SE", "cs.AI", "D.2.3; I.2.7"], "pdf": "https://arxiv.org/pdf/2509.21170", "abs": "https://arxiv.org/abs/2509.21170", "authors": ["Yongda Yu", "Guohao Shi", "Xianwei Wu", "Haochuan He", "XueMing Gu", "Qianqian Zhao", "Kui Liu", "Qiushi Wang", "Zhao Tian", "Haifeng Shen", "Guoping Rong"], "title": "Fine-Tuning LLMs to Analyze Multiple Dimensions of Code Review: A Maximum Entropy Regulated Long Chain-of-Thought Approach", "comment": "22 pages", "summary": "Large Language Models (LLMs) have shown great potential in supporting\nautomated code review due to their impressive capabilities in context\nunderstanding and reasoning. However, these capabilities are still limited\ncompared to human-level cognition because they are heavily influenced by the\ntraining data. Recent research has demonstrated significantly improved\nperformance through fine-tuning LLMs with code review data. However, compared\nto human reviewers who often simultaneously analyze multiple dimensions of code\nreview to better identify issues, the full potential of these methods is\nhampered by the limited or vague information used to fine-tune the models. This\npaper contributes MelcotCR, a chain-of-thought (COT) fine-tuning approach that\ntrains LLMs with an impressive reasoning ability to analyze multiple dimensions\nof code review by harnessing long COT techniques to provide rich structured\ninformation. To address context loss and reasoning logic loss issues that\nfrequently occur when LLMs process long COT prompts, we propose a solution that\ncombines the Maximum Entropy (ME) modeling principle with pre-defined reasoning\npathways in MelcotCR to enable more effective utilization of in-context\nknowledge within long COT prompts while strengthening the logical tightness of\nthe reasoning process. Empirical evaluations on our curated MelcotCR dataset\nand the public CodeReviewer dataset reveal that a low-parameter base model,\nsuch as 14B Qwen2.5, fine-tuned with MelcotCR can surpass state-of-the-art\nmethods in terms of the accuracy of detecting and describing code issues, with\nits performance remarkably on par with that of the 671B DeepSeek-R1 model."}
