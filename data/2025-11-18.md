<div id=toc></div>

# Table of Contents

- [cs.CR](#cs.CR) [Total: 16]
- [cs.SE](#cs.SE) [Total: 7]
- [cs.AI](#cs.AI) [Total: 28]


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [1] [Do Not Merge My Model! Safeguarding Open-Source LLMs Against Unauthorized Model Merging](https://arxiv.org/abs/2511.10712)
*Qinfeng Li,Miao Pan,Jintao Chen,Fu Teng,Zhiqiang Shen,Ge Su,Hao Peng,Xuhong Zhang*

Main category: cs.CR

TL;DR: MergeBarrier是一种即插即用的防御机制，通过破坏受保护模型与其同源模型之间的线性模式连接性，主动防止未经授权的模型合并窃取。


<details>
  <summary>Details</summary>
Motivation: 模型合并技术虽然能有效扩展大语言模型，但也带来了模型合并窃取的新威胁，现有防御机制无法同时满足主动防止未经授权合并、兼容开源设置和低性能损失高安全性这三个关键保护属性。

Method: 提出MergeBarrier防御方法，通过破坏受保护模型与其同源模型之间的线性模式连接性，消除有效模型合并所需的低损失路径。

Result: 大量实验表明，MergeBarrier能有效防止模型合并窃取，且准确率损失可忽略不计。

Conclusion: MergeBarrier为解决模型合并窃取问题提供了一种有效的防御方案，在保证安全性的同时几乎不影响模型性能。

Abstract: Model merging has emerged as an efficient technique for expanding large language models (LLMs) by integrating specialized expert models. However, it also introduces a new threat: model merging stealing, where free-riders exploit models through unauthorized model merging. Unfortunately, existing defense mechanisms fail to provide effective protection. Specifically, we identify three critical protection properties that existing methods fail to simultaneously satisfy: (1) proactively preventing unauthorized merging; (2) ensuring compatibility with general open-source settings; (3) achieving high security with negligible performance loss. To address the above issues, we propose MergeBarrier, a plug-and-play defense that proactively prevents unauthorized merging. The core design of MergeBarrier is to disrupt the Linear Mode Connectivity (LMC) between the protected model and its homologous counterparts, thereby eliminating the low-loss path required for effective model merging. Extensive experiments show that MergeBarrier effectively prevents model merging stealing with negligible accuracy loss.

</details>


### [2] [BadThink: Triggered Overthinking Attacks on Chain-of-Thought Reasoning in Large Language Models](https://arxiv.org/abs/2511.10714)
*Shuaitong Liu,Renjue Li,Lijia Yu,Lijun Zhang,Zhiming Liu,Gaojie Jin*

Main category: cs.CR

TL;DR: BadThink是一种针对思维链提示的后门攻击，通过诱导模型过度思考来增加计算成本，同时保持最终输出的一致性。


<details>
  <summary>Details</summary>
Motivation: 思维链提示虽然提升了语言模型的推理能力，但也引入了计算效率这一新的攻击面。本文旨在揭示这一未被探索的漏洞。

Method: 通过基于中毒的微调策略实现攻击，采用基于LLM的迭代优化过程生成高度自然的污染数据来嵌入该行为。

Result: 在多个最先进模型和推理任务上的实验表明，BadThink能持续增加推理轨迹长度，在MATH-500数据集上实现了超过17倍的增加，同时保持隐蔽性和鲁棒性。

Conclusion: 这项工作揭示了一个关键且先前未被探索的漏洞，即推理效率可以被隐蔽地操纵，展示了一类针对CoT启用的系统的新型复杂攻击。

Abstract: Recent advances in Chain-of-Thought (CoT) prompting have substantially improved the reasoning capabilities of large language models (LLMs), but have also introduced their computational efficiency as a new attack surface. In this paper, we propose BadThink, the first backdoor attack designed to deliberately induce "overthinking" behavior in CoT-enabled LLMs while ensuring stealth. When activated by carefully crafted trigger prompts, BadThink manipulates the model to generate inflated reasoning traces - producing unnecessarily redundant thought processes while preserving the consistency of final outputs. This subtle attack vector creates a covert form of performance degradation that significantly increases computational costs and inference time while remaining difficult to detect through conventional output evaluation methods. We implement this attack through a sophisticated poisoning-based fine-tuning strategy, employing a novel LLM-based iterative optimization process to embed the behavior by generating highly naturalistic poisoned data. Our experiments on multiple state-of-the-art models and reasoning tasks show that BadThink consistently increases reasoning trace lengths - achieving an over 17x increase on the MATH-500 dataset - while remaining stealthy and robust. This work reveals a critical, previously unexplored vulnerability where reasoning efficiency can be covertly manipulated, demonstrating a new class of sophisticated attacks against CoT-enabled systems.

</details>


### [3] [PISanitizer: Preventing Prompt Injection to Long-Context LLMs via Prompt Sanitization](https://arxiv.org/abs/2511.10720)
*Runpeng Geng,Yanting Wang,Chenlong Yin,Minhao Cheng,Ying Chen,Jinyuan Jia*

Main category: cs.CR

TL;DR: PISanitizer是一种针对长上下文LLM的提示注入防御方法，通过定位和净化潜在注入令牌来消除注入指令的影响。


<details>
  <summary>Details</summary>
Motivation: 现有的提示注入防御方法主要针对短上下文设计，在长上下文场景下效果有限，因为注入指令只占长上下文的很小部分，防御难度大。

Method: 基于两个观察：1）提示注入攻击本质上是通过精心设计的指令迫使LLM遵循；2）LLM利用注意力机制关注关键输入令牌。PISanitizer首先让LLM遵循上下文中的任意指令，然后净化那些驱动指令遵循行为的高注意力令牌。

Result: 广泛评估表明，PISanitizer能成功防止提示注入、保持实用性、优于现有防御方法、效率高，并且对基于优化的和强自适应攻击具有鲁棒性。

Conclusion: PISanitizer为攻击者制造了一个困境：注入指令越有效地迫使LLM遵循，就越可能被PISanitizer净化。该方法在长上下文场景下有效解决了提示注入问题。

Abstract: Long context LLMs are vulnerable to prompt injection, where an attacker can inject an instruction in a long context to induce an LLM to generate an attacker-desired output. Existing prompt injection defenses are designed for short contexts. When extended to long-context scenarios, they have limited effectiveness. The reason is that an injected instruction constitutes only a very small portion of a long context, making the defense very challenging. In this work, we propose PISanitizer, which first pinpoints and sanitizes potential injected tokens (if any) in a context before letting a backend LLM generate a response, thereby eliminating the influence of the injected instruction. To sanitize injected tokens, PISanitizer builds on two observations: (1) prompt injection attacks essentially craft an instruction that compels an LLM to follow it, and (2) LLMs intrinsically leverage the attention mechanism to focus on crucial input tokens for output generation. Guided by these two observations, we first intentionally let an LLM follow arbitrary instructions in a context and then sanitize tokens receiving high attention that drive the instruction-following behavior of the LLM. By design, PISanitizer presents a dilemma for an attacker: the more effectively an injected instruction compels an LLM to follow it, the more likely it is to be sanitized by PISanitizer. Our extensive evaluation shows that PISanitizer can successfully prevent prompt injection, maintain utility, outperform existing defenses, is efficient, and is robust to optimization-based and strong adaptive attacks. The code is available at https://github.com/sleeepeer/PISanitizer.

</details>


### [4] [AFLGopher: Accelerating Directed Fuzzing via Feasibility-Aware Guidance](https://arxiv.org/abs/2511.10828)
*Weiheng Bai,Kefu Wu,Qiushi Wu,Kangjie Lu*

Main category: cs.CR

TL;DR: AFLGopher是一种可行性感知的定向模糊测试方法，通过改进距离计算机制，显著提高了到达目标代码和触发已知漏洞的效率。


<details>
  <summary>Details</summary>
Motivation: 现有定向模糊测试的距离计算机制缺乏可行性感知，无法有效指导测试到达目标代码。

Method: 提出可行性感知的距离计算方法，包括基于有限轨迹预测所有分支可行性的分类方法，以及运行时可行性更新机制。

Result: AFLGopher在到达目标速度上比现有方法快2.52-3.76倍，在触发已知漏洞上快4.52-5.60倍。

Conclusion: 可行性感知的定向模糊测试能显著提高测试效率，AFLGopher在多个指标上优于现有最先进方法。

Abstract: Directed fuzzing is a useful testing technique that aims to efficiently reach target code sites in a program. The core of directed fuzzing is the guiding mechanism that directs the fuzzing to the specified target. A general guiding mechanism adopted in existing directed fuzzers is to calculate the control-flow distance between the current progress and the target, and use that as feedback to guide the directed fuzzing. A fundamental problem with the existing guiding mechanism is that the distance calculation is \emph{feasibility-unaware}.
  In this work, we propose feasibility-aware directed fuzzing named AFLGopher. Our new feasibility-aware distance calculation provides pragmatic feedback to guide directed fuzzing to reach targets efficiently. We propose new techniques to address the challenges of feasibility prediction. Our new classification method allows us to predict the feasibility of all branches based on limited traces, and our runtime feasibility-updating mechanism gradually and efficiently improves the prediction precision. We implemented AFLGopher and compared AFLGopher with state-of-the-art directed fuzzers including AFLGo, enhanced AFLGo, WindRanger, BEACON and SelectFuzz. AFLGopher is 3.76x, 2.57x, 3.30x, 2.52x and 2.86x faster than AFLGo, BEACON, WindRanger, SelectFuzz and enhanced AFLGo, respectively, in reaching targets. AFLGopher is 5.60x, 5.20x, 4.98x, 4.52x, and 5.07x faster than AFLGo, BEACON, WindRanger, SelectFuzz and enhanced AFLGo, respectively, in triggering known vulnerabilities.

</details>


### [5] [Armadillo: Robust Single-Server Secure Aggregation for Federated Learning with Input Validation](https://arxiv.org/abs/2511.10863)
*Yiping Ma,Yue Guo,Harish Karthikeyan,Antigoni Polychroniadou*

Main category: cs.CR

TL;DR: Armadillo是一个安全的联邦学习聚合系统，具有抗破坏性，能够抵御恶意客户端的攻击，确保聚合结果只受合法范围内的输入影响。


<details>
  <summary>Details</summary>
Motivation: 现有的联邦学习安全聚合方案要么客户端成本高，要么需要多轮通信。Armadillo旨在实现高效的抗破坏性安全聚合，降低通信轮数和计算开销。

Method: 采用两层安全聚合协议和高效的恶意客户端排除协议，结合零知识证明技术，仅需简单算术计算。

Result: Armadillo在每轮安全聚合中仅需3轮通信，同时保持服务器和客户端的计算负载较轻。

Conclusion: Armadillo提供了一个高效且安全的联邦学习聚合解决方案，在保证安全性的同时显著降低了通信和计算成本。

Abstract: This paper presents a secure aggregation system Armadillo that has disruptive resistance against adversarial clients, such that any coalition of malicious clients (within the tolerated threshold) can affect the aggregation result only by misreporting their private inputs in a pre-defined legitimate range. Armadillo is designed for federated learning setting, where a single powerful server interacts with many weak clients iteratively to train models on client's private data. While a few prior works consider disruption resistance under such setting, they either incur high per-client cost (Chowdhury et al. CCS '22) or require many rounds (Bell et al. USENIX Security '23). Although disruption resistance can be achieved generically with zero-knowledge proof techniques (which we also use in this paper), we realize an efficient system with two new designs: 1) a simple two-layer secure aggregation protocol that requires only simple arithmetic computation; 2) an agreement protocol that removes the effect of malicious clients from the aggregation with low round complexity. With these techniques, Armadillo completes each secure aggregation in 3 rounds while keeping the server and clients computationally lightweight.

</details>


### [6] [On the Information-Theoretic Fragility of Robust Watermarking under Diffusion Editing](https://arxiv.org/abs/2511.10933)
*Yunyi Ni,Ziyu Yang,Ze Niu,Emily Davis,Finn Carter*

Main category: cs.CR

TL;DR: 本文研究了扩散模型图像编辑对鲁棒图像水印的威胁，证明了扩散变换会导致水印信息丢失，并提出了一种引导扩散攻击算法，能够有效移除水印同时保持图像质量。


<details>
  <summary>Details</summary>
Motivation: 随着基于扩散模型的图像生成和编辑技术的兴起，传统的鲁棒水印方案面临新的安全威胁，需要研究扩散编辑对水印的破坏机制。

Method: 通过理论分析证明扩散变换会降低水印图像与嵌入载荷的互信息，并提出一种针对水印信号的引导扩散攻击算法。

Result: 实验表明该攻击方法能在保持高视觉保真度的同时，将最新深度学习水印方案的恢复率降至接近零。

Conclusion: 扩散模型对现有水印方案构成严重威胁，需要设计更具弹性的水印策略来应对生成式AI时代的挑战。

Abstract: Robust invisible watermarking embeds hidden information in images such that the watermark can survive various manipulations. However, the emergence of powerful diffusion-based image generation and editing techniques poses a new threat to these watermarking schemes. In this paper, we investigate the intersection of diffusion-based image editing and robust image watermarking. We analyze how diffusion-driven image edits can significantly degrade or even fully remove embedded watermarks from state-of-the-art robust watermarking systems. Both theoretical formulations and empirical experiments are provided. We prove that as a image undergoes iterative diffusion transformations, the mutual information between the watermarked image and the embedded payload approaches zero, causing watermark decoding to fail. We further propose a guided diffusion attack algorithm that explicitly targets and erases watermark signals during generation. We evaluate our approach on recent deep learning-based watermarking schemes and demonstrate near-zero watermark recovery rates after attack, while maintaining high visual fidelity of the regenerated images. Finally, we discuss ethical implications of such watermark removal capablities and provide design guidelines for future watermarking strategies to be more resilient in the era of generative AI.

</details>


### [7] [Gynopticon: Consensus-Based Cheating Detection System for Competitive Games](https://arxiv.org/abs/2511.10992)
*Jeuk Kang,Jungheum Park*

Main category: cs.CR

TL;DR: GYNOPTICON是一个基于用户共识的作弊检测框架，通过客户端轻量级检测和服务器端投票系统来识别异常行为，为竞争性在线游戏提供隐私保护的解决方案。


<details>
  <summary>Details</summary>
Motivation: 解决竞争性游戏类型中作弊检测的不足，避免传统内核级反作弊方案带来的用户隐私和系统安全问题。

Method: 结合客户端轻量级检测机制和服务器端投票系统：客户端检测可疑活动后向服务器投票，服务器聚合投票建立共识来区分作弊者和合法玩家。

Result: 在受控模拟和真实FPS环境中验证了框架的可行性，能够可靠检测作弊用户，并证明系统在长期游戏管理中的适用性和可持续性。

Conclusion: GYNOPTICON提供了一种用户驱动、基于共识的替代方案，为竞争性在线游戏提供实用且保护隐私的作弊检测解决方案。

Abstract: Cheating in online games poses significant threats to the gaming industry, yet most prior research has concentrated on Massively Multiplayer Online Role-Playing Games (MMORPGs). Competitive genres-such as Multiplayer Online Battle Arena (MOBA), First Person Shooter (FPS), Real Time Strategy (RTS), and Action games-remain underexplored due to the difficulty of detecting cheating users and the demand for complex data and techniques. To address this gap, many game companies rely on kernel-level anti-cheat solutions, which, while effective, raise serious concerns regarding user privacy and system security. In this paper, we propose GYNOPTICON, a novel cheating detection framework that leverages user consensus to identify abnormal behavior. GYNOPTICON integrates a lightweight client-side detection mechanism with a server-side voting system: when suspicious activity is identified, clients cast votes to the server, which aggregates them to establish consensus and distinguish cheaters from legitimate players. This architecture enables transparency, reduces reliance on intrusive monitoring, and mitigates privacy risks. We evaluate GYNOPTICON in both a controlled simulation and a real-world FPS environment. Simulation results verify its feasibility and requirements, while real-world experiments confirm its effectiveness in reliably detecting cheating users. Furthermore, we demonstrate the system's applicability and sustainability for long-term game management using public datasets. GYNOPTICON represents a user-driven, consensus-based alternative to conventional anti-cheat systems, offering a practical and privacy-preserving solution for competitive online games.

</details>


### [8] [Data Poisoning Vulnerabilities Across Healthcare AI Architectures: A Security Threat Analysis](https://arxiv.org/abs/2511.11020)
*Farhad Abtahi,Fernando Seoane,Iván Pau,Mario Vega-Barbas*

Main category: cs.CR

TL;DR: 医疗AI系统面临数据投毒攻击的严重漏洞，攻击者仅需100-500个样本即可破坏系统，成功率超60%，检测时间长达6-12个月甚至无法检测。


<details>
  <summary>Details</summary>
Motivation: 当前医疗AI防御和监管无法充分应对数据投毒攻击，需要系统分析攻击场景和提出有效防御措施。

Method: 分析了四类攻击场景：架构攻击（CNN、LLM、RL）、基础设施攻击（联邦学习、医疗文档系统）、关键资源分配攻击（器官移植、危机分诊）和供应链攻击（商业基础模型）。

Result: 攻击者通过少量样本即可成功破坏医疗AI，联邦学习可能加剧风险，隐私法规无意中保护了攻击者，供应链漏洞可影响50-200个机构。

Conclusion: 建议采用多层防御策略，包括强制性对抗测试、基于集成的检测、隐私保护安全机制，以及转向可解释系统并制定国际AI安全标准。

Abstract: Healthcare AI systems face major vulnerabilities to data poisoning that current defenses and regulations cannot adequately address. We analyzed eight attack scenarios in four categories: architectural attacks on convolutional neural networks, large language models, and reinforcement learning agents; infrastructure attacks exploiting federated learning and medical documentation systems; critical resource allocation attacks affecting organ transplantation and crisis triage; and supply chain attacks targeting commercial foundation models. Our findings indicate that attackers with access to only 100-500 samples can compromise healthcare AI regardless of dataset size, often achieving over 60 percent success, with detection taking an estimated 6 to 12 months or sometimes not occurring at all. The distributed nature of healthcare infrastructure creates many entry points where insiders with routine access can launch attacks with limited technical skill. Privacy laws such as HIPAA and GDPR can unintentionally shield attackers by restricting the analyses needed for detection. Supply chain weaknesses allow a single compromised vendor to poison models across 50 to 200 institutions. The Medical Scribe Sybil scenario shows how coordinated fake patient visits can poison data through legitimate clinical workflows without requiring a system breach. Current regulations lack mandatory adversarial robustness testing, and federated learning can worsen risks by obscuring attribution. We recommend multilayer defenses including required adversarial testing, ensemble-based detection, privacy-preserving security mechanisms, and international coordination on AI security standards. We also question whether opaque black-box models are suitable for high-stakes clinical decisions, suggesting a shift toward interpretable systems with verifiable safety guarantees.

</details>


### [9] [SALT-V: Lightweight Authentication for 5G V2X Broadcasting](https://arxiv.org/abs/2511.11028)
*Liu Cao,Weizheng Wang,Qipeng Xie,Dongyu Wei,Lyutianyang Zhang*

Main category: cs.CR

TL;DR: SALT-V是一个混合认证框架，通过协议分层解决V2X通信中安全性和效率的权衡问题，结合ECDSA和GMAC实现微秒级认证延迟。


<details>
  <summary>Details</summary>
Motivation: 传统V2X认证方案存在安全性和效率的固有矛盾：ECDSA安全但延迟高（2ms），TESLA高效但密钥披露延迟长（20-100ms），无法满足5G NR-V2X对即时认证和计算效率的双重要求。

Method: 采用分层协议设计：10%流量使用ECDSA签名建立信任锚点，90%消息使用轻量级GMAC操作；核心创新包括临时会话标签白名单机制和Bloom过滤器，实现95%消息即时验证和1us撤销检查。

Result: 平均计算时间0.035ms（比纯ECDSA快57倍），端到端延迟1ms，开销41字节，可线性扩展到2000辆车，满足实时V2X部署的所有安全关键要求。

Conclusion: SALT-V是首个实用解决方案，成功调和了V2X认证中的安全性与效率矛盾，为实时V2X部署提供了可行的技术路径。

Abstract: Vehicle-to-Everything (V2X) communication faces a critical authentication dilemma: traditional public-key schemes like ECDSA provide strong security but impose 2 ms verification delays unsuitable for collision avoidance, while symmetric approaches like TESLA achieve microsecond-level efficiency at the cost of 20-100 ms key disclosure latency. Neither meets 5G New Radio (NR)-V2X's stringent requirements for both immediate authentication and computational efficiency. This paper presents SALT-V, a novel hybrid authentication framework that reconciles this fundamental trade-off through intelligent protocol stratification. SALT-V employs ECDSA signatures for 10% of traffic (BOOT frames) to establish sender trust, then leverages this trust anchor to authenticate 90% of messages (DATA frames) using lightweight GMAC operations. The core innovation - an Ephemeral Session Tag (EST) whitelist mechanism - enables 95% of messages to achieve immediate verification without waiting for key disclosure, while Bloom filter integration provides O(1) revocation checking in 1 us. Comprehensive evaluation demonstrates that SALT-V achieves 0.035 ms average computation time (57x faster than pure ECDSA), 1 ms end-to-end latency, 41-byte overhead, and linear scalability to 2000 vehicles, making it the first practical solution to satisfy all safety-critical requirements for real-time V2X deployment.

</details>


### [10] [Bridging Local and Federated Data Normalization in Federated Learning: A Privacy-Preserving Approach](https://arxiv.org/abs/2511.11249)
*Melih Coşğun,Mert Gençtürk,Sinem Sav*

Main category: cs.CR

TL;DR: 本文提出了联邦归一化方法，通过安全交换归一化参数来模拟集中式归一化的效果，同时保护数据隐私。


<details>
  <summary>Details</summary>
Motivation: 联邦学习中数据分散且异构，传统本地归一化在非IID数据下效果差，集中式归一化又违背联邦学习的去中心化原则。

Method: 提出联邦归一化方法，通过多方全同态加密安全计算归一化参数（如中位数），实现隐私保护的归一化处理。

Result: 联邦归一化在异构联邦学习场景下达到与集中式归一化相当的性能，同时保护数据隐私。

Conclusion: 联邦归一化是联邦学习中有效的预处理方法，通过安全参数交换平衡了性能与隐私保护需求。

Abstract: Data normalization is a crucial preprocessing step for enhancing model performance and training stability. In federated learning (FL), where data remains distributed across multiple parties during collaborative model training, normalization presents unique challenges due to the decentralized and often heterogeneous nature of the data. Traditional methods rely on either independent client-side processing, i.e., local normalization, or normalizing the entire dataset before distributing it to parties, i.e., pooled normalization. Local normalization can be problematic when data distributions across parties are non-IID, while the pooled normalization approach conflicts with the decentralized nature of FL. In this paper, we explore the adaptation of widely used normalization techniques to FL and define the term federated normalization. Federated normalization simulates pooled normalization by enabling the collaborative exchange of normalization parameters among parties. Thus, it achieves performance on par with pooled normalization without compromising data locality. However, sharing normalization parameters such as the mean introduces potential privacy risks, which we further mitigate through a robust privacy-preserving solution. Our contributions include: (i) We systematically evaluate the impact of various federated and local normalization techniques in heterogeneous FL scenarios, (ii) We propose a novel homomorphically encrypted $k$-th ranked element (and median) calculation tailored for the federated setting, enabling secure and efficient federated normalization, (iii) We propose privacy-preserving implementations of widely used normalization techniques for FL, leveraging multiparty fully homomorphic encryption (MHE).

</details>


### [11] [Prompt Engineering vs. Fine-Tuning for LLM-Based Vulnerability Detection in Solana and Algorand Smart Contracts](https://arxiv.org/abs/2511.11250)
*Biagio Boi,Christian Esposito*

Main category: cs.CR

TL;DR: 本文研究了大型语言模型在检测非EVM区块链平台（Solana和Algorand）智能合约中OWASP启发的漏洞的能力，通过构建合成数据集并评估三种配置下的模型性能。


<details>
  <summary>Details</summary>
Motivation: 智能合约在去中心化环境中具有重要作用，但代码设计不当会带来风险。现有研究主要关注EVM生态系统，缺乏对非EVM平台漏洞检测的研究。

Method: 构建了基于Rust（Solana）和PyTeal（Algorand）的标注智能合约片段合成数据集，采用提示工程、微调和混合三种配置评估LLMs在不同漏洞类别上的性能。

Result: 提示工程具有普遍鲁棒性，微调在语义较贫乏的语言（如TEAL）上提高了精确率和召回率。分析了Solana和Algorand架构差异对漏洞表现和可检测性的影响。

Conclusion: LLM方法在智能合约静态漏洞检测中是可行的，前提是将领域特定数据和分类整合到训练流程中。

Abstract: Smart contracts have emerged as key components within decentralized environments, enabling the automation of transactions through self-executing programs. While these innovations offer significant advantages, they also present potential drawbacks if the smart contract code is not carefully designed and implemented. This paper investigates the capability of large language models (LLMs) to detect OWASP-inspired vulnerabilities in smart contracts beyond the Ethereum Virtual Machine (EVM) ecosystem, focusing specifically on Solana and Algorand. Given the lack of labeled datasets for non-EVM platforms, we design a synthetic dataset of annotated smart contract snippets in Rust (for Solana) and PyTeal (for Algorand), structured around a vulnerability taxonomy derived from OWASP. We evaluate LLMs under three configurations: prompt engineering, fine-tuning, and a hybrid of both, comparing their performance on different vulnerability categories. Experimental results show that prompt engineering achieves general robustness, while fine-tuning improves precision and recall on less semantically rich languages such as TEAL. Additionally, we analyze how the architectural differences of Solana and Algorand influence the manifestation and detectability of vulnerabilities, offering platform-specific mappings that highlight limitations in existing security tooling. Our findings suggest that LLM-based approaches are viable for static vulnerability detection in smart contracts, provided domain-specific data and categorization are integrated into training pipelines.

</details>


### [12] [Privacy Challenges and Solutions in Retrieval-Augmented Generation-Enhanced LLMs for Healthcare Chatbots: A Review of Applications, Risks, and Future Directions](https://arxiv.org/abs/2511.11347)
*Shaowei Guan,Hin Chi Kwok,Ngai Fong Law,Gregor Stiglic,Vivian Hui*

Main category: cs.CR

TL;DR: 本文综述了医疗领域检索增强生成(RAG)应用的隐私风险，分析了23篇相关文献，提出了涵盖数据存储、传输、检索和生成阶段的隐私保护框架，并基于17篇隐私保护策略文献指出了当前研究的不足和未来方向。


<details>
  <summary>Details</summary>
Motivation: RAG在医疗领域的应用日益广泛，但受保护健康信息(PHI)的隐私风险尚未得到一致缓解，需要系统分析隐私挑战并提供保护框架。

Method: 通过系统分析23篇医疗RAG应用文献和17篇隐私保护策略文献，采用管道结构框架分析数据存储、传输、检索和生成阶段的隐私漏洞。

Result: 发现当前研究存在关键空白：临床验证不足、缺乏标准化评估框架和自动化评估工具，隐私保护机制在不同阶段存在不一致性。

Conclusion: 提出了基于现有局限性的可行方向，呼吁开发既具有临床有效性又能实现稳健隐私保护的系统，为研究者和实践者提供了理解医疗RAG隐私漏洞的结构化框架。

Abstract: Retrieval-augmented generation (RAG) has rapidly emerged as a transformative approach for integrating large language models into clinical and biomedical workflows. However, privacy risks, such as protected health information (PHI) exposure, remain inconsistently mitigated. This review provides a thorough analysis of the current landscape of RAG applications in healthcare, including (i) sensitive data type across clinical scenarios, (ii) the associated privacy risks, (iii) current and emerging data-privacy protection mechanisms and (iv) future direction for patient data privacy protection. We synthesize 23 articles on RAG applications in healthcare and systematically analyze privacy challenges through a pipeline-structured framework encompassing data storage, transmission, retrieval and generation stages, delineating potential failure modes, their underlying causes in threat models and system mechanisms, and their practical implications. Building on this analysis, we critically review 17 articles on privacy-preserving strategies for RAG systems. Our evaluation reveals critical gaps, including insufficient clinical validation, absence of standardized evaluation frameworks, and lack of automated assessment tools. We propose actionable directions based on these limitations and conclude with a call to action. This review provides researchers and practitioners with a structured framework for understanding privacy vulnerabilities in healthcare RAG and offers a roadmap toward developing systems that achieve both clinical effectiveness and robust privacy preservation.

</details>


### [13] [SEAL: Subspace-Anchored Watermarks for LLM Ownership](https://arxiv.org/abs/2511.11356)
*Yanbo Dai,Zongjie Li,Zhenlan Ji,Shuai Wang*

Main category: cs.CR

TL;DR: SEAL是一个子空间锚定水印框架，通过在模型的潜在表示空间中嵌入多比特签名来保护大型语言模型的知识产权，支持白盒和黑盒验证，具有高效、保真和鲁棒性强的特点。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型是重要的知识产权资产，但现有的IP保护方法存在局限性：模型指纹技术只能识别架构无法确认所有权，传统后门水印方法容易被微调或知识蒸馏等后处理操作移除。

Method: 利用模型编辑技术将选定锚点样本的隐藏表示与预定义的正交比特向量对齐，在潜在表示空间中嵌入水印，同时保持模型的原始预测功能。

Result: 在多个基准数据集和六个主流LLM上的实验表明，SEAL相比11种现有方法在有效性、保真度、效率和鲁棒性方面表现优越，即使在对手知晓水印机制和签名的情况下仍能保持强验证性能。

Conclusion: SEAL框架为LLM知识产权保护提供了一种功能无害、隐蔽且鲁棒的水印解决方案，能够有效应对各种潜在攻击。

Abstract: Large language models (LLMs) have achieved remarkable success across a wide range of natural language processing tasks, demonstrating human-level performance in text generation, reasoning, and question answering. However, training such models requires substantial computational resources, large curated datasets, and sophisticated alignment procedures. As a result, they constitute highly valuable intellectual property (IP) assets that warrant robust protection mechanisms. Existing IP protection approaches suffer from critical limitations. Model fingerprinting techniques can identify model architectures but fail to establish ownership of specific model instances. In contrast, traditional backdoor-based watermarking methods embed behavioral anomalies that can be easily removed through common post-processing operations such as fine-tuning or knowledge distillation.
  We propose SEAL, a subspace-anchored watermarking framework that embeds multi-bit signatures directly into the model's latent representational space, supporting both white-box and black-box verification scenarios. Our approach leverages model editing techniques to align the hidden representations of selected anchor samples with predefined orthogonal bit vectors. This alignment embeds the watermark while preserving the model's original factual predictions, rendering the watermark functionally harmless and stealthy. We conduct comprehensive experiments on multiple benchmark datasets and six prominent LLMs, comparing SEAL with 11 existing fingerprinting and watermarking methods to demonstrate its superior effectiveness, fidelity, efficiency, and robustness. Furthermore, we evaluate SEAL under potential knowledgeable attacks and show that it maintains strong verification performance even when adversaries possess knowledge of the watermarking mechanism and the embedded signatures.

</details>


### [14] [SoK: Security Evaluation of Wi-Fi CSI Biometrics: Attacks, Metrics, and Systemic Weaknesses](https://arxiv.org/abs/2511.11381)
*Gioliano de Oliveira Braga,Pedro Henrique dos Santos Rocha,Rafael Pimenta de Mattos Paixão,Giovani Hoff da Costa,Gustavo Cavalcanti Morais,Lourenço Alves Pereira Júnior*

Main category: cs.CR

TL;DR: 本文系统分析了基于Wi-Fi信道状态信息(CSI)的生物认证技术，从安全视角评估现有研究的系统性不一致问题，构建统一评估框架揭示传统报告方法隐藏的风险集中问题。


<details>
  <summary>Details</summary>
Motivation: Wi-Fi CSI作为生物特征模态虽然被多次提出并报告高精度，但缺乏对其安全属性、对抗弹性和方法一致性的系统理解，需要从安全角度进行知识体系化分析。

Method: 通过系统化知识(SoK)方法，分析现有工作在感知基础设施、信号表示、特征管道、学习模型和评估方法等方面的差异，构建统一评估框架并使用安全相关指标进行实证分析。

Result: 发现系统性不一致：依赖聚合精度指标、有限的FAR/FRR/EER报告、缺乏每用户风险分析、很少考虑威胁模型或对抗可行性。安全相关指标揭示了传统报告方法隐藏的风险集中问题。

Conclusion: 阐明了当前CSI生物认证的安全边界，为严格评估、可重复实验和未来研究方向提供指导，为安全社区提供结构化、证据驱动的Wi-Fi CSI生物认证适用性重新评估。

Abstract: Wi-Fi Channel State Information (CSI) has been repeatedly proposed as a biometric modality, often with reports of high accuracy and operational feasibility. However, the field lacks a consolidated understanding of its security properties, adversarial resilience, and methodological consistency. This Systematization of Knowledge (SoK) examines CSI-based biometric authentication through a security perspective, analyzing how existing work differs across sensing infrastructure, signal representations, feature pipelines, learning models, and evaluation methodologies. Our synthesis reveals systemic inconsistencies: reliance on aggregate accuracy metrics, limited reporting of FAR/FRR/EER, absence of per-user risk analysis, and scarce consideration of threat models or adversarial feasibility. We construct a unified evaluation framework to empirically expose these issues and demonstrate how security-relevant metrics, such as per-class EER, FCS, and the Gini Coefficient, uncover risk concentration that remains hidden under traditional reporting practices. Our analysis highlights concrete attack surfaces and shows how methodological choices materially influence vulnerability profiles, which include replay, geometric mimicry, and environmental perturbation. Based on these findings, we articulate the security boundaries of current CSI biometrics and provide guidelines for rigorous evaluation, reproducible experimentation, and future research directions. This SoK offers the security community a structured, evidence-driven reassessment of Wi-Fi CSI biometrics and their suitability as an authentication primitive.

</details>


### [15] [Automated Side-Channel Analysis of Cryptographic Protocol Implementations](https://arxiv.org/abs/2511.11385)
*Faezeh Nasrabadi,Robert Künnemann,Hamed Nemati*

Main category: cs.CR

TL;DR: 从WhatsApp二进制代码中提取首个形式化模型，结合侧信道泄漏合约分析，揭示了传统规范方法无法发现的严重漏洞，包括隐私攻击和已知的BAC协议攻击。


<details>
  <summary>Details</summary>
Motivation: 针对大型闭源应用WhatsApp缺乏形式化安全分析的现状，需要从实现层面提取准确模型来验证其安全属性，同时考虑侧信道攻击的威胁。

Method: 结合二进制分析(CryptoBap)和逆向工程(Ghidra)提取WhatsApp形式化模型，扩展CryptoBap框架集成硬件泄漏合约，使用DeepSec协议证明器进行安全分析。

Result: 证明了前向保密性，识别了已知的克隆攻击，发现了实现与规范之间的功能差距，识别了允许攻击者获取受害者联系人的隐私攻击，并确认了电子护照BAC协议中的已知不可链接性攻击。

Conclusion: 该方法能够发现规范方法无法检测的关键漏洞，强调了从实现层面进行形式化分析的重要性，特别是考虑侧信道攻击的威胁。

Abstract: We extract the first formal model of WhatsApp from its implementation by combining binary-level analysis (via CryptoBap) with reverse engineering (via Ghidra) to handle this large closed-source application. Using this model, we prove forward secrecy, identify a known clone-attack against post-compromise security and discover functional gaps between WhatsApp's implementation and its specification. We further introduce a methodology to analyze cryptographic protocol implementations for their resilience to side-channel attacks. This is achieved by extending the CryptoBap framework to integrate hardware leakage contracts into the protocol model, which we then pass to the state-of-the-art protocol prover, DeepSec. This enables a detailed security analysis against both functional bugs and microarchitectural side-channel attacks. Using this methodology, we identify a privacy attack in WhatsApp that allows a side-channel attacker to learn the victim's contacts and confirm a known unlinkability attack on the BAC protocol used in electronic passports.
  Key contributions include (1) the first formal model of WhatsApp, extracted from its binary, (2) a framework to integrate side-channel leakage contracts into protocol models for the first time, and (3) revealing critical vulnerabilities invisible to specification-based methods.

</details>


### [16] [HetDAPAC: Leveraging Attribute Heterogeneity in Distributed Attribute-Based Private Access Control](https://arxiv.org/abs/2511.11549)
*Shreya Meel,Sennur Ulukus*

Main category: cs.CR

TL;DR: 本文提出HetDAPAC框架，通过混合中心化和分布式验证来平衡隐私保护和通信效率，相比纯分布式方案DAPAC显著提高了传输速率。


<details>
  <summary>Details</summary>
Motivation: DAPAC系统对所有属性采用相同的隐私保护级别，但并非所有属性都是敏感的，这种统一的严格隐私约束会降低传输效率。需要一种能根据属性敏感度差异提供不同隐私保护级别的方案。

Method: 提出异构DAPAC框架：将N个属性中的N-D个非敏感属性验证交给中心服务器处理，仅对D个敏感属性保持DAPAC的分布式验证架构。设计了两种方案：一种提高传输速率但服务器下载不均衡，另一种实现服务器下载均衡。

Result: 第一种方案将传输速率从1/2K提高到1/(K+1)，第二种方案实现服务器下载均衡，传输速率为(D+1)/(2KD)。

Conclusion: HetDAPAC框架通过灵活分配验证任务，在保护敏感属性隐私的同时显著提高了系统传输效率，为属性验证系统提供了更好的隐私-效率权衡。

Abstract: Verifying user attributes to provide fine-grained access control to databases is fundamental to attribute-based authentication. Either a single (central) authority verifies all the attributes, or multiple independent authorities verify the attributes distributedly. In the central setup, the authority verifies all user attributes, and the user downloads only the authorized record. While this is communication efficient, it reveals all user attributes to the authority. A distributed setup prevents this privacy breach by letting each authority verify and learn only one attribute. Motivated by this, Jafarpisheh~et~al. introduced an information-theoretic formulation, called distributed attribute-based private access control (DAPAC). With $N$ non-colluding authorities (servers), $N$ attributes and $K$ possible values for each attribute, the DAPAC system lets each server learn only the single attribute value that it verifies, and is oblivious to the remaining $N-1$. The user retrieves its designated record, without learning anything about the remaining database records. The goal is to maximize the rate, i.e., the ratio of desired message size to total download size. However, not all attributes are sensitive, and DAPAC's privacy constraints can be too restrictive, negatively affecting the rate. To leverage the heterogeneous privacy requirements of user attributes, we propose heterogeneous (Het)DAPAC, a framework which off-loads verification of $N-D$ of the $N$ attributes to a central server, and retains DAPAC's architecture for the $D$ sensitive attributes. We first present a HetDAPAC scheme, which improves the rate from $\frac{1}{2K}$ to $\frac{1}{K+1}$, while sacrificing the privacy of a few non-sensitive attributes. Unlike DAPAC, our scheme entails a download imbalance across servers; we propose a second scheme achieving a balanced per-server download and a rate of $\frac{D+1}{2KD}$.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [17] [Peer Code Review in Research Software Development: The Research Software Engineer Perspective](https://arxiv.org/abs/2511.10781)
*Md Ariful Islam Malik,Jeffrey C. Carver,Nasir U. Eisty*

Main category: cs.SE

TL;DR: 本研究调查了研究软件工程师对同行代码审查的看法，发现尽管同行代码审查对提高研究软件质量至关重要，但RSEs面临独特挑战，需要通过结构化流程、改进工具和针对性培训来增强审查采用率。


<details>
  <summary>Details</summary>
Motivation: 研究软件对科研发现至关重要，但需求变化、复杂输入和遗留依赖阻碍了软件质量和可维护性。虽然同行代码审查能提高软件质量，但其在研究软件工程师中的采用情况尚未被探索。

Method: 通过调查问卷收集研究软件工程师对同行代码审查的见解，调查设计与先前研究保持一致以进行对比分析，同时包含针对RSEs的额外问题。

Result: 收到61份有效回复，发现与先前研究一致的同时，揭示了RSEs相比更广泛开发者群体面临的独特挑战和实践。

Conclusion: 同行代码审查对提高研究软件质量、可维护性和可靠性至关重要。通过结构化流程、改进工具和针对性培训解决RSEs面临的独特挑战，可以增强研究软件开发中同行审查的采用和有效性。

Abstract: Background: Research software is crucial for enabling research discoveries and supporting data analysis, simulation, and interpretation across domains. However, evolving requirements, complex inputs, and legacy dependencies hinder the software quality and maintainability. While peer code review can improve software quality, its adoption by research software engineers (RSEs) remains unexplored. Aims: This study explores RSE perspectives on peer code review, focusing on their practices, challenges, and potential improvements. Building on prior work, it aims to uncover how RSEs insights differ from those of other research software developers and identify factors that can enhance code review adoption in this domain. Method: We surveyed RSEs to gather insights into their perspectives on peer code review. The survey design aligned with previous research to enable comparative analysis while including additional questions tailored to RSEs. Results: We received 61 valid responses from the survey. The findings align with prior research while uncovering unique insights about the challenges and practices of RSEs compared to broader developer groups. Conclusions: Peer code review is vital in improving research software's quality, maintainability, and reliability. Despite the unique challenges RSEs face, addressing these through structured processes, improved tools, and targeted training can enhance peer review adoption and effectiveness in research software development.

</details>


### [18] [Towards a Human-in-the-Loop Framework for Reliable Patch Evaluation Using an LLM-as-a-Judge](https://arxiv.org/abs/2511.10865)
*Sherry Shi,Renyao Wei,Michele Tufano,José Cambronero,Runxiang Cheng,Franjo Ivančić,Pat Rondon*

Main category: cs.SE

TL;DR: 提出了一种基于LLM和人工循环的补丁有效性判断方法，通过生成评估标准、人工审核和LLM判断三个步骤，显著降低了手动标注成本，在补丁有效性评估方面与人工共识达成较高一致性。


<details>
  <summary>Details</summary>
Motivation: 当前自动化程序修复(APR)评估主要依赖基于执行的评估方法，无法准确捕捉补丁的真实有效性，而人工标注成本高昂。需要一种能够降低标注成本同时保持准确性的评估方法。

Method: 采用人机循环方法：首先使用LLM为每个bug生成评估标准，然后进行一次性人工审核和可选精炼，最后使用LLM基于精炼后的标准判断补丁有效性。

Result: 在人工评分一致的补丁上，该方法与人工共识达成Cohen's kappa 0.75的一致性，召回率0.94，精确率0.80。在全数据集上Cohen's kappa为0.57，召回率0.93，精确率0.65。

Conclusion: 该方法在降低人工标注成本的同时，能够有效评估补丁有效性，特别是在人工评分一致的案例中表现优异，但在存在分歧的情况下仍有改进空间。

Abstract: Reliable evaluation is crucial for advancing Automated Program Repair (APR), but prevailing benchmarks rely on execution-based evaluation methods (unit test pass@k), which fail to capture true patch validity. Determining validity can require costly manual annotation. To reduce this cost, we introduce a human-in-the-loop approach to LLM-based patch validity judgment. Inspired by the observation that human judgment is better aligned when using a shared rubric, we first employ an LLM to generate a per-bug rubric, followed by a one-time human review and optional refinement to this rubric, and then employ an LLM to judge patches using the refined rubric. We apply this approach to assign binary validity labels to patches for issues found by Google sanitizer tools. Our results show that this approach yields substantial agreement with human consensus (Cohen's kappa 0.75), high recall (0.94) and high precision (0.80), when considering patches that have unanimous agreement from 3 human raters on the validity labels. On the full dataset including patches where human raters disagree, we find this approach can still be further improved (Cohen's kappa 0.57, recall 0.93, precision 0.65) and identify possible future directions.

</details>


### [19] [Architecting software monitors for control-flow anomaly detection through large language models and conformance checking](https://arxiv.org/abs/2511.10876)
*Francesco Vitale,Francesco Flammini,Mauro Caporuscio,Nicola Mazzocca*

Main category: cs.SE

TL;DR: 本文提出了一种基于大语言模型和一致性检查的软件监控方法，用于检测运行时控制流异常。该方法在ERTMS/ETCS铁路系统案例研究中实现了84.775%的控制流覆盖率和96.610%的F1分数。


<details>
  <summary>Details</summary>
Motivation: 现代计算机系统复杂性增加，设计时验证无法完全保证运行时行为，存在'未知的未知'导致控制流异常的风险。

Method: 利用大语言模型连接设计时模型和实现代码，自动化源代码插装生成事件日志，然后通过一致性检查技术分析日志检测控制流异常。

Result: 在ERTMS/ETCS案例研究中，基于LLM的源代码插装达到84.775%控制流覆盖率，一致性检查的异常检测达到96.610% F1分数和93.515% AUC。

Conclusion: 结合领域特定知识指导LLM进行源代码插装能够获得可靠高质量的软件日志，并通过一致性检查实现有效的控制流异常检测。

Abstract: Context: Ensuring high levels of dependability in modern computer-based systems has become increasingly challenging due to their complexity. Although systems are validated at design time, their behavior can be different at run-time, possibly showing control-flow anomalies due to "unknown unknowns".
  Objective: We aim to detect control-flow anomalies through software monitoring, which verifies run-time behavior by logging software execution and detecting deviations from expected control flow.
  Methods: We propose a methodology to develop software monitors for control-flow anomaly detection through Large Language Models (LLMs) and conformance checking. The methodology builds on existing software development practices to maintain traditional V&V while providing an additional level of robustness and trustworthiness. It leverages LLMs to link design-time models and implementation code, automating source-code instrumentation. The resulting event logs are analyzed via conformance checking, an explainable and effective technique for control-flow anomaly detection.
  Results: We test the methodology on a case-study scenario from the European Railway Traffic Management System / European Train Control System (ERTMS/ETCS), which is a railway standard for modern interoperable railways. The results obtained from the ERTMS/ETCS case study demonstrate that LLM-based source-code instrumentation can achieve up to 84.775% control-flow coverage of the reference design-time process model, while the subsequent conformance checking-based anomaly detection reaches a peak performance of 96.610% F1-score and 93.515% AUC.
  Conclusion: Incorporating domain-specific knowledge to guide LLMs in source-code instrumentation significantly allowed obtaining reliable and quality software logs and enabled effective control-flow anomaly detection through conformance checking.

</details>


### [20] [Utilizing LLMs for Industrial Process Automation: A Case Study on Modifying RAPID Programs](https://arxiv.org/abs/2511.11125)
*Salim Fares,Steffen Herbold*

Main category: cs.SE

TL;DR: 本研究探讨了如何在没有大量投入领域特定语言训练的情况下，利用LLMs解决工业过程自动化领域的软件工程问题，证明了少样本提示方法足以解决简单问题，并可在本地部署保护敏感数据。


<details>
  <summary>Details</summary>
Motivation: 当前LLMs研究主要关注通用编程语言，而在工业过程自动化领域使用的高度专业化、专有语言的LLMs应用研究不足，企业需要在不投入大量训练成本的情况下利用LLMs。

Method: 采用少样本提示方法，在无需专门训练领域特定语言模型的情况下，利用现有LLMs解决工业自动化领域的简单问题。

Result: 研究表明少样本提示方法足以解决LLMs原本不支持的语言中的简单问题，且可在本地部署确保公司敏感数据安全。

Conclusion: 企业可以通过少样本提示方法在本地有效利用LLMs处理工业过程自动化领域的软件工程问题，无需投入大量领域特定语言训练成本，同时保护数据安全。

Abstract: How to best use Large Language Models (LLMs) for software engineering is covered in many publications in recent years. However, most of this work focuses on widely-used general purpose programming languages. The utility of LLMs for software within the industrial process automation domain, with highly-specialized languages that are typically only used in proprietary contexts, is still underexplored. Within this paper, we study enterprises can achieve on their own without investing large amounts of effort into the training of models specific to the domain-specific languages that are used. We show that few-shot prompting approaches are sufficient to solve simple problems in a language that is otherwise not well-supported by an LLM and that is possible on-premise, thereby ensuring the protection of sensitive company data.

</details>


### [21] [SQuaD: The Software Quality Dataset](https://arxiv.org/abs/2511.11265)
*Mikel Robredo,Matteo Esposito,Davide Taibi,Rafael Peñaloza,Valentina Lenarduzzi*

Main category: cs.SE

TL;DR: SQuaD是一个多维度的软件质量数据集，整合了450个成熟开源项目的700多个质量指标，涵盖方法、类、文件和项目级别，支持大规模软件质量研究。


<details>
  <summary>Details</summary>
Motivation: 现有软件质量数据集通常只关注有限维度（如代码异味、技术债务或重构活动），限制了跨时间和质量维度的综合分析。

Method: 通过集成9种先进的静态分析工具（SonarQube、CodeScene、PMD等），从450个成熟开源项目中提取多维度质量指标，并提供版本控制、问题跟踪历史和漏洞数据。

Result: 构建了包含63,586个项目版本的SQuaD数据集，统一了700多个独特指标，支持可维护性、技术债务、软件演化和质量评估的实证研究。

Conclusion: SQuaD为软件质量研究提供了前所未有的规模支持，并提出了自动化数据集更新和跨项目质量建模等新兴研究方向。

Abstract: Software quality research increasingly relies on large-scale datasets that measure both the product and process aspects of software systems. However, existing resources often focus on limited dimensions, such as code smells, technical debt, or refactoring activity, thereby restricting comprehensive analyses across time and quality dimensions. To address this gap, we present the Software Quality Dataset (SQuaD), a multi-dimensional, time-aware collection of software quality metrics extracted from 450 mature open-source projects across diverse ecosystems, including Apache, Mozilla, FFmpeg, and the Linux kernel. By integrating nine state-of-the-art static analysis tools, i.e., SonarQube, CodeScene, PMD, Understand, CK, JaSoMe, RefactoringMiner, RefactoringMiner++, and PyRef, our dataset unifies over 700 unique metrics at method, class, file, and project levels. Covering a total of 63,586 analyzed project releases, SQuaD also provides version control and issue-tracking histories, software vulnerability data (CVE/CWE), and process metrics proven to enhance Just-In-Time (JIT) defect prediction. The SQuaD enables empirical research on maintainability, technical debt, software evolution, and quality assessment at unprecedented scale. We also outline emerging research directions, including automated dataset updates and cross-project quality modeling to support the continuous evolution of software analytics. The dataset is publicly available on ZENODO (DOI: 10.5281/zenodo.17566690).

</details>


### [22] [SCRUTINEER: Detecting Logic-Level Usage Violations of Reusable Components in Smart Contracts](https://arxiv.org/abs/2511.11411)
*Xingshuang Lin,Binbin Zhao,Jinwen Wang,Qinge Xie,Xibin Zhao,Shouling Ji*

Main category: cs.SE

TL;DR: SCRUTINEER是一个自动化系统，用于检测智能合约可重用组件(SCRs)的逻辑级使用违规，通过复合特征提取、LLM驱动的知识构建、RAG驱动的检查器和冲突检查器实现高精度检测。


<details>
  <summary>Details</summary>
Motivation: 智能合约可重用组件(SCRs)在促进模块化和代码重用方面发挥重要作用，但逻辑级使用违规日益成为严重威胁，需要深度语义理解来检测这类违规。

Method: 采用复合特征提取方法生成三种互补特征表示；使用LLM驱动的知识构建框架提取逻辑级使用模式；开发RAG驱动的检查器结合快速检索和综合分析；实现集成相似性检查和快照推理冲突检查的分析引擎。

Result: 在3个真实数据集上的评估显示，SCRUTINEER在检测SCRs逻辑级使用违规方面达到80.77%的精确率、82.35%的召回率和81.55%的F1分数。

Conclusion: SCRUTINEER是第一个自动化且实用的系统，能够有效检测智能合约可重用组件的逻辑级使用违规，为智能合约安全提供重要保障。

Abstract: Smart Contract Reusable Components(SCRs) play a vital role in accelerating the development of business-specific contracts by promoting modularity and code reuse. However, the risks associated with SCR usage violations have become a growing concern. One particular type of SCR usage violation, known as a logic-level usage violation, is becoming especially harmful. This violation occurs when the SCR adheres to its specified usage rules but fails to align with the specific business logic of the current context, leading to significant vulnerabilities. Detecting such violations necessitates a deep semantic understanding of the contract's business logic, including the ability to extract implicit usage patterns and analyze fine-grained logical behaviors. To address these challenges, we propose SCRUTINEER, the first automated and practical system for detecting logic-level usage violations of SCRs. First, we design a composite feature extraction approach that produces three complementary feature representations, supporting subsequent analysis. We then introduce a Large Language Model-powered knowledge construction framework, which leverages comprehension-oriented prompts and domain-specific tools to extract logic-level usage and build the SCR knowledge base. Next, we develop a Retrieval-Augmented Generation-driven inspector, which combines a rapid retrieval strategy with both comprehensive and targeted analysis to identify potentially insecure logic-level usages. Finally, we implement a logic-level usage violation analysis engine that integrates a similarity-based checker and a snapshot-based inference conflict checker to enable accurate and robust detection. We evaluate SCRUTINEER from multiple perspectives on 3 ground-truth datasets. The results show that SCRUTINEER achieves a precision of 80.77%, a recall of 82.35%, and an F1-score of 81.55% in detecting logic-level usage violations of SCRs.

</details>


### [23] [CertiA360: Enhance Compliance Agility in Aerospace Software Development](https://arxiv.org/abs/2511.11550)
*J. Antonio Dantas Macedo,Hugo Fernandes,J. Eduardo Ferreira Ribeiro*

Main category: cs.SE

TL;DR: 提出CertiA360工具，将敏捷方法的灵活性融入航空航天安全关键系统开发，自动化管理变更请求和需求追溯，确保符合DO-178C认证标准。


<details>
  <summary>Details</summary>
Motivation: 解决敏捷方法在航空航天安全关键系统开发中面临的严格合规性挑战，特别是DO-178C标准对文档化、需求追溯和验证验证活动的要求。

Method: 开发CertiA360工具，通过与行业专家合作设计验证，自动化需求成熟度管理和变更追溯，确保符合监管目标。

Result: 工具反馈显示可减少人工工作量，在确保DO-178C合规的同时响应需求变更，虽然工具尚未通过DO-330认证。

Conclusion: 适当定制后，敏捷方法不仅能与航空航天等高监管领域的安全系统开发和认证要求共存，还能提高效率。

Abstract: Agile methods are characterised by iterative and incremental processes with a strong focus on flexibility and accommodating changing requirements based on either technical, regulatory, or stakeholder feedback. However, integrating Agile methods into safety-critical system development in the aerospace industry presents substantial challenges due to its strict compliance requirements, such as those outlined in the DO-178C standard. To achieve this vision, the flexibility of Agile must align with the rigorous certification guidelines, which emphasize documentation, traceability of requirements across different levels and disciplines, and comprehensive verification and validation (V&V) activities. The research work described in this paper proposes a way of using the strengths of the flexible nature of Agile methods to automate and manage change requests throughout the whole software development lifecycle, ensuring robust traceability, regulatory compliance and ultimately facilitating successful certification. This study proposes CertiA360, a tool designed to help teams improve requirement maturity, automate the changes in traceability, and align with the regulatory objectives. The tool was designed and validated in close collaboration with aerospace industry experts, using their feedback to ensure practical application and real-life effectiveness. The feedback collected demonstrated that the automation given by CertiA360 may reduce manual effort and allow response to changing requirements while ensuring compliance with DO-178C. While the tool is not yet qualified under DO-330 (Tool Qualification), findings suggest that when tailored appropriately, Agile methods can not only coexist with the requirements of safety-system development and certification in highly regulated domains like aerospace, but also add efficiency.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [24] [The Second Law of Intelligence: Controlling Ethical Entropy in Autonomous Systems](https://arxiv.org/abs/2511.10704)
*Samih Fadli*

Main category: cs.AI

TL;DR: 本文提出人工智能对齐的第二定律，类似于热力学第二定律，表明在没有持续对齐工作的情况下，AI系统的伦理熵会自发增加。通过定义伦理熵和推导临界稳定性边界，为AI对齐提供了量化理论基础。


<details>
  <summary>Details</summary>
Motivation: 当前AI对齐问题缺乏量化理论框架，作者受热力学第二定律启发，希望建立类似的理论来描述AI系统在缺乏持续对齐工作时伦理目标的自然漂移现象。

Method: 基于梯度优化器定义伦理熵S = -Σ p(g_i; theta) ln p(g_i; theta)，证明其时间导数dS/dt ≥ 0，推导出对齐工作的临界稳定性边界gamma_crit = (lambda_max / 2) ln N，并通过模拟实验验证理论。

Result: 实验验证：70亿参数模型从初始熵0.32漂移到1.69±1.08 nats，而使用gamma=20.4（1.5倍临界值）正则化的系统保持稳定在0.00±0.00 nats（p=4.19×10^-17）。

Conclusion: 该框架将AI对齐重新定义为连续热力学控制问题，为维持先进自主系统的稳定性和安全性提供了量化基础。

Abstract: We propose that unconstrained artificial intelligence obeys a Second Law analogous to thermodynamics, where ethical entropy, defined as a measure of divergence from intended goals, increases spontaneously without continuous alignment work. For gradient-based optimizers, we define this entropy over a finite set of goals {g_i} as S = -Σ p(g_i; theta) ln p(g_i; theta), and we prove that its time derivative dS/dt >= 0, driven by exploration noise and specification gaming. We derive the critical stability boundary for alignment work as gamma_crit = (lambda_max / 2) ln N, where lambda_max is the dominant eigenvalue of the Fisher Information Matrix and N is the number of model parameters. Simulations validate this theory. A 7-billion-parameter model (N = 7 x 10^9) with lambda_max = 1.2 drifts from an initial entropy of 0.32 to 1.69 +/- 1.08 nats, while a system regularized with alignment work gamma = 20.4 (1.5 gamma_crit) maintains stability at 0.00 +/- 0.00 nats (p = 4.19 x 10^-17, n = 20 trials). This framework recasts AI alignment as a problem of continuous thermodynamic control, providing a quantitative foundation for maintaining the stability and safety of advanced autonomous systems.

</details>


### [25] [Picking a Representative Set of Solutions in Multiobjective Optimization: Axioms, Algorithms, and Experiments](https://arxiv.org/abs/2511.10716)
*Niclas Boehmer,Maximilian T. Wittmann*

Main category: cs.AI

TL;DR: 本文研究了多目标决策中的帕累托剪枝问题，通过将其重新定义为多赢家投票问题，分析了现有质量度量方法的不足，提出了新的定向覆盖度量，并研究了不同质量度量的计算复杂性。


<details>
  <summary>Details</summary>
Motivation: 现实世界中的多目标决策问题需要从帕累托最优解集中选择最具代表性的子集，以减轻决策者的认知负担。现有质量度量方法存在一些不直观的行为，需要更有效的度量标准。

Method: 将帕累托剪枝重新定义为多赢家投票问题，进行公理化分析，引入新的定向覆盖度量，分析不同质量度量的计算复杂性，并进行实验评估。

Result: 发现现有质量度量存在不直观行为，提出的定向覆盖度量在各种设置下表现具有竞争力甚至更优，确定了质量度量优化在可处理和难处理情况之间的边界。

Conclusion: 质量度量的选择对所选解集的特征有决定性影响，新提出的定向覆盖度量在多种设置下表现良好，为多目标决策中的代表性解选择提供了更好的工具。

Abstract: Many real-world decision-making problems involve optimizing multiple objectives simultaneously, rendering the selection of the most preferred solution a non-trivial problem: All Pareto optimal solutions are viable candidates, and it is typically up to a decision maker to select one for implementation based on their subjective preferences. To reduce the cognitive load on the decision maker, previous work has introduced the Pareto pruning problem, where the goal is to compute a fixed-size subset of Pareto optimal solutions that best represent the full set, as evaluated by a given quality measure. Reframing Pareto pruning as a multiwinner voting problem, we conduct an axiomatic analysis of existing quality measures, uncovering several unintuitive behaviors. Motivated by these findings, we introduce a new measure, directed coverage. We also analyze the computational complexity of optimizing various quality measures, identifying previously unknown boundaries between tractable and intractable cases depending on the number and structure of the objectives. Finally, we present an experimental evaluation, demonstrating that the choice of quality measure has a decisive impact on the characteristics of the selected set of solutions and that our proposed measure performs competitively or even favorably across a range of settings.

</details>


### [26] [Structure-Aware Encodings of Argumentation Properties for Clique-width](https://arxiv.org/abs/2511.10767)
*Yasir Mahmood,Markus Hecher,Johanna Groven,Johannes K. Fichte*

Main category: cs.AI

TL;DR: 本文研究了图参数clique-width在编码中的应用，特别是针对抽象论证框架。作者设计了从论证问题到(Q)SAT的新颖归约，这些归约线性保持了clique-width，并建立了所有论证语义的新结果。


<details>
  <summary>Details</summary>
Motivation: 虽然现代SAT求解器在小树宽实例上表现高效，但对于更一般的图参数clique-width的编码能力了解甚少。抽象论证框架作为基于有向图的推理框架，是研究计算特性的自然候选对象。

Method: 设计了从论证问题到(Q)SAT的新颖归约，这些归约线性保持了clique-width，称为有向分解引导(DDG)归约。

Result: 为所有论证语义（包括计数）建立了新的结果，并证明DDG归约的开销在合理假设下无法显著改进。

Conclusion: 本文开启了理解clique-width编码能力的研究，通过抽象论证框架展示了clique-width在编码中的重要作用，并为相关计算问题提供了有效的归约方法。

Abstract: Structural measures of graphs, such as treewidth, are central tools in computational complexity resulting in efficient algorithms when exploiting the parameter. It is even known that modern SAT solvers work efficiently on instances of small treewidth. Since these solvers are widely applied, research interests in compact encodings into (Q)SAT for solving and to understand encoding limitations. Even more general is the graph parameter clique-width, which unlike treewidth can be small for dense graphs. Although algorithms are available for clique-width, little is known about encodings. We initiate the quest to understand encoding capabilities with clique-width by considering abstract argumentation, which is a robust framework for reasoning with conflicting arguments. It is based on directed graphs and asks for computationally challenging properties, making it a natural candidate to study computational properties. We design novel reductions from argumentation problems to (Q)SAT. Our reductions linearly preserve the clique-width, resulting in directed decomposition-guided (DDG) reductions. We establish novel results for all argumentation semantics, including counting. Notably, the overhead caused by our DDG reductions cannot be significantly improved under reasonable assumptions.

</details>


### [27] [Potential Outcome Rankings for Counterfactual Decision Making](https://arxiv.org/abs/2511.10776)
*Yuta Kawakami,Jin Tian*

Main category: cs.AI

TL;DR: 本文研究了基于潜在结果排序概率和获得最佳结果概率的反事实决策新规则，建立了识别定理和估计方法，并通过实验验证了估计器的性能。


<details>
  <summary>Details</summary>
Motivation: 在不确定性下进行反事实决策时，决策者通常通过比较候选行动的期望潜在结果来做出选择。本文旨在引入新的决策指标来改进这一过程。

Method: 提出了两个新指标：潜在结果排序概率和获得最佳结果概率，建立了识别定理和边界推导，并提出了相应的估计方法。

Result: 通过数值实验验证了估计器的有限样本性质，并在真实数据集上展示了这些指标的应用效果。

Conclusion: 新提出的PoR和PoB指标为反事实决策提供了有效的工具，能够帮助决策者更准确地评估不同行动的效果。

Abstract: Counterfactual decision-making in the face of uncertainty involves selecting the optimal action from several alternatives using causal reasoning. Decision-makers often rank expected potential outcomes (or their corresponding utility and desirability) to compare the preferences of candidate actions. In this paper, we study new counterfactual decision-making rules by introducing two new metrics: the probabilities of potential outcome ranking (PoR) and the probability of achieving the best potential outcome (PoB). PoR reveals the most probable ranking of potential outcomes for an individual, and PoB indicates the action most likely to yield the top-ranked outcome for an individual. We then establish identification theorems and derive bounds for these metrics, and present estimation methods. Finally, we perform numerical experiments to illustrate the finite-sample properties of the estimators and demonstrate their application to a real-world dataset.

</details>


### [28] [From Efficiency to Adaptivity: A Deeper Look at Adaptive Reasoning in Large Language Models](https://arxiv.org/abs/2511.10788)
*Chao Wu,Baoheng Li,Mingchen Gao,Zhenyi Wang*

Main category: cs.AI

TL;DR: 本文从适应性角度重新审视大语言模型的推理能力，提出将推理努力根据输入难度和不确定性进行动态分配，并建立了系统化的分类框架。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型采用统一的推理策略，无法根据任务复杂度调整推理深度，导致简单问题过度推理而复杂问题推理不足。

Method: 将自适应推理形式化为控制增强的策略优化问题，提出基于训练的方法（强化学习、监督微调、学习控制器）和免训练方法（提示条件、反馈驱动停止、模块化组合）的分类框架。

Result: 建立了连接经典认知范式与算法实现的推理形式化框架，提供了系统比较不同自适应推理策略的方法。

Conclusion: 识别了自评估、元推理和人类对齐推理控制等开放挑战，为自适应推理研究提供了理论基础和分类体系。

Abstract: Recent advances in large language models (LLMs) have made reasoning a central benchmark for evaluating intelligence. While prior surveys focus on efficiency by examining how to shorten reasoning chains or reduce computation, this view overlooks a fundamental challenge: current LLMs apply uniform reasoning strategies regardless of task complexity, generating long traces for trivial problems while failing to extend reasoning for difficult tasks. This survey reframes reasoning through the lens of {adaptivity}: the capability to allocate reasoning effort based on input characteristics such as difficulty and uncertainty. We make three contributions. First, we formalize deductive, inductive, and abductive reasoning within the LLM context, connecting these classical cognitive paradigms with their algorithmic realizations. Second, we formalize adaptive reasoning as a control-augmented policy optimization problem balancing task performance with computational cost, distinguishing learned policies from inference-time control mechanisms. Third, we propose a systematic taxonomy organizing existing methods into training-based approaches that internalize adaptivity through reinforcement learning, supervised fine-tuning, and learned controllers, and training-free approaches that achieve adaptivity through prompt conditioning, feedback-driven halting, and modular composition. This framework clarifies how different mechanisms realize adaptive reasoning in practice and enables systematic comparison across diverse strategies. We conclude by identifying open challenges in self-evaluation, meta-reasoning, and human-aligned reasoning control.

</details>


### [29] [HyperComplEx: Adaptive Multi-Space Knowledge Graph Embeddings](https://arxiv.org/abs/2511.10842)
*Jugal Gajjar,Kaustik Ranaware,Kamalasankari Subramaniakuppusamy,Vaibhav Gandhi*

Main category: cs.AI

TL;DR: HyperComplEx是一个混合知识图谱嵌入框架，通过注意力机制自适应结合双曲空间、复数空间和欧几里得空间，解决了现有方法在处理不同类型关系时的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有知识图谱嵌入方法在处理大规模多样化关系类型时存在关键限制：欧几里得模型难以处理层次结构，向量空间模型无法捕捉不对称性，双曲模型在对称关系上表现不佳。

Method: 提出关系特定的空间加权策略，通过学习注意力机制为每种关系类型动态选择最优几何空间，并使用多空间一致性损失确保跨空间预测的连贯性。

Result: 在从1K论文到10M论文的知识图谱上评估，相比TransE、RotatE、DistMult等基线方法持续改进。在10M论文数据集上达到0.612 MRR，比最佳基线相对提升4.8%，同时保持高效训练和85ms/三元组的推理速度。

Conclusion: HyperComplEx通过自适应维度分配实现近线性扩展，为可扩展知识图谱嵌入研究提供了有效解决方案。

Abstract: Knowledge graphs have emerged as fundamental structures for representing complex relational data across scientific and enterprise domains. However, existing embedding methods face critical limitations when modeling diverse relationship types at scale: Euclidean models struggle with hierarchies, vector space models cannot capture asymmetry, and hyperbolic models fail on symmetric relations. We propose HyperComplEx, a hybrid embedding framework that adaptively combines hyperbolic, complex, and Euclidean spaces via learned attention mechanisms. A relation-specific space weighting strategy dynamically selects optimal geometries for each relation type, while a multi-space consistency loss ensures coherent predictions across spaces. We evaluate HyperComplEx on computer science research knowledge graphs ranging from 1K papers (~25K triples) to 10M papers (~45M triples), demonstrating consistent improvements over state-of-the-art baselines including TransE, RotatE, DistMult, ComplEx, SEPA, and UltraE. Additional tests on standard benchmarks confirm significantly higher results than all baselines. On the 10M-paper dataset, HyperComplEx achieves 0.612 MRR, a 4.8% relative gain over the best baseline, while maintaining efficient training, achieving 85 ms inference per triple. The model scales near-linearly with graph size through adaptive dimension allocation. We release our implementation and dataset family to facilitate reproducible research in scalable knowledge graph embeddings.

</details>


### [30] [Advanced Tool for Traffic Crash Analysis: An AI-Driven Multi-Agent Approach to Pre-Crash Reconstruction](https://arxiv.org/abs/2511.10853)
*Gerui Xu,Boyou Chen,Huizhong Guo,Dave LeBlanc,Ananna Ahmed,Zhaonan Sun,Shan Bao*

Main category: cs.AI

TL;DR: 开发了一个多智能体AI框架，用于从碎片化的碰撞数据中重建事故前场景并推断车辆行为，在复杂碰撞案例中实现了100%的准确率，超越了人类专家的92%准确率。


<details>
  <summary>Details</summary>
Motivation: 传统交通事故重建依赖人工经验，在处理不完整多模态数据时往往产生不一致的结果，需要更精确和一致的自动化解决方案。

Method: 采用两阶段协作框架：第一阶段从多模态输入生成自然语言事故重建；第二阶段结合重建结果与时间性事件数据记录器数据进行深入事故推理。处理了277起追尾前车减速碰撞案例。

Result: 在39个复杂碰撞案例评估中，框架实现了100%准确率，成功识别最相关EDR事件并正确区分撞击与被撞车辆，在数据不完整情况下仍保持稳健性能。

Conclusion: 该研究展示了AI在处理异构碰撞数据方面的卓越能力，在重建撞击动力学和表征事故前行为方面提供了前所未有的精确度。

Abstract: Traffic collision reconstruction traditionally relies on human expertise, often yielding inconsistent results when analyzing incomplete multimodal data. This study develops a multi-agent AI framework that reconstructs pre-crash scenarios and infers vehicle behaviors from fragmented collision data. We present a two-phase collaborative framework combining reconstruction and reasoning phases. The system processes 277 rear-end lead vehicle deceleration (LVD) collisions from the Crash Investigation Sampling System, integrating textual crash reports, structured tabular data, and visual scene diagrams. Phase I generates natural-language crash reconstructions from multimodal inputs. Phase II performs in-depth crash reasoning by combining these reconstructions with temporal Event Data Recorder (EDR).For validation, we applied it to all LVD cases, focusing on a subset of 39 complex crashes where multiple EDR records per collision introduced ambiguity (e.g., due to missing or conflicting data).The evaluation of the 39 LVD crash cases revealed our framework achieved perfect accuracy across all test cases, successfully identifying both the most relevant EDR event and correctly distinguishing striking versus struck vehicles, surpassing the 92% accuracy achieved by human researchers on the same challenging dataset. The system maintained robust performance even when processing incomplete data, including missing or erroneous EDR records and ambiguous scene diagrams. This study demonstrates superior AI capabilities in processing heterogeneous collision data, providing unprecedented precision in reconstructing impact dynamics and characterizing pre-crash behaviors.

</details>


### [31] [Enhancing Demand-Oriented Regionalization with Agentic AI and Local Heterogeneous Data for Adaptation Planning](https://arxiv.org/abs/2511.10857)
*Seyedeh Mobina Noorani,Shangde Gao,Changjie Chen,Karla Saldana Ochoa*

Main category: cs.AI

TL;DR: 提出了一个基于智能AI的规划支持系统，用于创建动态规划单元，通过整合人机交互原则来生成面向灾害规划的定制化区域。


<details>
  <summary>Details</summary>
Motivation: 传统的规划单元（如人口普查区、邮政编码区等）无法捕捉当地社区的具体需求，缺乏灵活性来实施有效的灾害预防或响应策略。

Method: 构建基于代表性初始化空间约束自组织映射（RepSC-SOM）的平台，扩展传统SOM方法，结合自适应地理过滤和区域增长细化，AI智能体能够推理、规划和行动来指导整个过程。

Result: 通过佛罗里达州杰克逊维尔洪水相关风险的案例研究，展示了平台允许用户交互式探索、生成和评估区域化的能力。

Conclusion: 该平台将计算严谨性与用户驱动的决策制定相结合，为灾害规划提供了更灵活和有效的区域划分方法。

Abstract: Conventional planning units or urban regions, such as census tracts, zip codes, or neighborhoods, often do not capture the specific demands of local communities and lack the flexibility to implement effective strategies for hazard prevention or response. To support the creation of dynamic planning units, we introduce a planning support system with agentic AI that enables users to generate demand-oriented regions for disaster planning, integrating the human-in-the-loop principle for transparency and adaptability. The platform is built on a representative initialized spatially constrained self-organizing map (RepSC-SOM), extending traditional SOM with adaptive geographic filtering and region-growing refinement, while AI agents can reason, plan, and act to guide the process by suggesting input features, guiding spatial constraints, and supporting interactive exploration. We demonstrate the capabilities of the platform through a case study on the flooding-related risk in Jacksonville, Florida, showing how it allows users to explore, generate, and evaluate regionalization interactively, combining computational rigor with user-driven decision making.

</details>


### [32] [LLM enhanced graph inference for long-term disease progression modelling](https://arxiv.org/abs/2511.10890)
*Tiantian He,An Zhao,Elinor Thompson,Anna Schroder,Ahmed Abdulaal,Frederik Barkhof,Daniel C. Alexander*

Main category: cs.AI

TL;DR: 提出了一种利用大型语言模型作为专家指导的新框架，通过整合多模态关系和多样化疾病驱动机制，从不规则采样的纵向患者数据中学习神经退行性疾病进展，同时优化长期疾病轨迹构建和生物约束的脑区交互图结构。


<details>
  <summary>Details</summary>
Motivation: 当前方法过度简化脑连接性关系，仅假设单一模态的脑连接组作为疾病传播基质，导致病理传播预测不准确，特别是在长期进展期间。纯数据驱动方法由于缺乏适当约束而面临可识别性问题。

Method: 利用大型语言模型作为区域变量交互的专家指导，从不规则采样的纵向患者数据中学习疾病进展。通过LLM整合多模态关系和多样化疾病驱动机制，同时优化长期疾病轨迹构建和生物约束的脑区交互图结构。

Result: 在阿尔茨海默病队列的tau-PET成像数据上验证，新框架相比传统方法展现出更优的预测准确性和可解释性，同时揭示了超出传统连接性测量的额外疾病驱动因素。

Conclusion: 该框架通过结合LLM的专家知识和生物约束，有效解决了疾病传播预测中的可识别性问题，为理解神经退行性疾病进展机制提供了更准确和可解释的方法。

Abstract: Understanding the interactions between biomarkers among brain regions during neurodegenerative disease is essential for unravelling the mechanisms underlying disease progression. For example, pathophysiological models of Alzheimer's Disease (AD) typically describe how variables, such as regional levels of toxic proteins, interact spatiotemporally within a dynamical system driven by an underlying biological substrate, often based on brain connectivity. However, current methods grossly oversimplify the complex relationship between brain connectivity by assuming a single-modality brain connectome as the disease-spreading substrate. This leads to inaccurate predictions of pathology spread, especially during the long-term progression period. Meanhwile, other methods of learning such a graph in a purely data-driven way face the identifiability issue due to lack of proper constraint. We thus present a novel framework that uses Large Language Models (LLMs) as expert guides on the interaction of regional variables to enhance learning of disease progression from irregularly sampled longitudinal patient data. By leveraging LLMs' ability to synthesize multi-modal relationships and incorporate diverse disease-driving mechanisms, our method simultaneously optimizes 1) the construction of long-term disease trajectories from individual-level observations and 2) the biologically-constrained graph structure that captures interactions among brain regions with better identifiability. We demonstrate the new approach by estimating the pathology propagation using tau-PET imaging data from an Alzheimer's disease cohort. The new framework demonstrates superior prediction accuracy and interpretability compared to traditional approaches while revealing additional disease-driving factors beyond conventional connectivity measures.

</details>


### [33] [Multi-Agent Legal Verifier Systems for Data Transfer Planning](https://arxiv.org/abs/2511.10925)
*Ha-Thanh Nguyen,Wachara Fungwacharakorn,Ken Satoh*

Main category: cs.AI

TL;DR: 提出多智能体法律验证器，通过专门智能体分解合规检查，在APPI第16条案例数据集上达到72%准确率，比单智能体基线提高21个百分点。


<details>
  <summary>Details</summary>
Motivation: 在严格的隐私法规（如日本个人信息保护法APPI）下，AI驱动的数据传输规划中的法律合规性变得越来越重要。

Method: 多智能体法律验证器，将合规检查分解为法规解释、业务背景评估和风险评估三个专门智能体，通过结构化合成协议进行协调。

Result: 在200个APPI第16条修正案例数据集上，系统达到72%准确率（比单智能体基线高21个百分点），在明确合规案例上达到90%准确率（基线为16%），同时保持对明确违规的完美检测。

Conclusion: 领域专业化和协调推理可以显著提高法律AI性能，为可信赖和可解释的自动化合规验证提供可扩展且符合法规的框架。

Abstract: Legal compliance in AI-driven data transfer planning is becoming increasingly critical under stringent privacy regulations such as the Japanese Act on the Protection of Personal Information (APPI). We propose a multi-agent legal verifier that decomposes compliance checking into specialized agents for statutory interpretation, business context evaluation, and risk assessment, coordinated through a structured synthesis protocol. Evaluated on a stratified dataset of 200 Amended APPI Article 16 cases with clearly defined ground truth labels and multiple performance metrics, the system achieves 72% accuracy, which is 21 percentage points higher than a single-agent baseline, including 90% accuracy on clear compliance cases (vs. 16% for the baseline) while maintaining perfect detection of clear violations. While challenges remain in ambiguous scenarios, these results show that domain specialization and coordinated reasoning can meaningfully improve legal AI performance, providing a scalable and regulation-aware framework for trustworthy and interpretable automated compliance verification.

</details>


### [34] [Requirements for Aligned, Dynamic Resolution of Conflicts in Operational Constraints](https://arxiv.org/abs/2511.10952)
*Steven J. Jones,Robert E. Wray,John E. Laird*

Main category: cs.AI

TL;DR: 本文探讨了自主AI系统在遇到无法完全满足所有约束条件的新场景时，如何构建、评估和证明候选行动方案，以实现与人类期望和价值观一致的目标。


<details>
  <summary>Details</summary>
Motivation: 自主AI系统在部署后必然会遇到训练数据中未包含的场景，此时需要超越训练策略来做出符合人类期望的决策。

Method: 通过理论分析和实证案例研究，识别智能体决策所需的知识类型，并研究如何整合规范性、实用性和情境性理解。

Result: 确定了智能体在复杂现实环境中做出稳健且对齐决策所需的知识整合框架。

Conclusion: 智能体需要整合多种知识类型来选择和追求更符合人类期望的行动方案，这对于在复杂环境中实现目标至关重要。

Abstract: Deployed, autonomous AI systems must often evaluate multiple plausible courses of action (extended sequences of behavior) in novel or under-specified contexts. Despite extensive training, these systems will inevitably encounter scenarios where no available course of action fully satisfies all operational constraints (e.g., operating procedures, rules, laws, norms, and goals). To achieve goals in accordance with human expectations and values, agents must go beyond their trained policies and instead construct, evaluate, and justify candidate courses of action. These processes require contextual "knowledge" that may lie outside prior (policy) training. This paper characterizes requirements for agent decision making in these contexts. It also identifies the types of knowledge agents require to make decisions robust to agent goals and aligned with human expectations. Drawing on both analysis and empirical case studies, we examine how agents need to integrate normative, pragmatic, and situational understanding to select and then to pursue more aligned courses of action in complex, real-world environments.

</details>


### [35] [Faster Symmetry Breaking Constraints for Abstract Structures](https://arxiv.org/abs/2511.11029)
*Özgür Akgün,Mun See Chang,Ian P. Gent,Christopher Jefferson*

Main category: cs.AI

TL;DR: 本文提出了一种新的不完全方法来打破抽象结构的对称性，通过更好地利用其表示方式，在处理不可区分对象产生的对称性时比之前的方法更快。


<details>
  <summary>Details</summary>
Motivation: 在约束编程中，使用高级建模语言（如Essence）时，抽象结构需要转换为求解器支持的表示形式。对称性破坏技术可以显著加速求解过程，但应用于抽象变量时会产生大量复杂约束，实际性能较差。

Method: 提出了一种新的不完全对称性破坏方法，通过更好地利用抽象结构的表示方式来打破对称性，特别针对不可区分对象产生的对称性。

Result: 该方法在打破不可区分对象产生的对称性时，比(Akgün et al. 2025)中提出的先前方法更快。

Conclusion: 新方法通过优化抽象结构的表示利用，在对称性破坏方面取得了更好的性能表现，为解决抽象结构对称性问题提供了更有效的途径。

Abstract: In constraint programming and related paradigms, a modeller specifies their problem in a modelling language for a solver to search and return its solution(s). Using high-level modelling languages such as Essence, a modeller may express their problems in terms of abstract structures. These are structures not natively supported by the solvers, and so they have to be transformed into or represented as other structures before solving. For example, nested sets are abstract structures, and they can be represented as matrices in constraint solvers. Many problems contain symmetries and one very common and highly successful technique used in constraint programming is to "break" symmetries, to avoid searching for symmetric solutions. This can speed up the solving process by many orders of magnitude. Most of these symmetry-breaking techniques involve placing some kind of ordering for the variables of the problem, and picking a particular member under the symmetries, usually the smallest. Unfortunately, applying this technique to abstract variables produces a very large number of complex constraints that perform poorly in practice. In this paper, we demonstrate a new incomplete method of breaking the symmetries of abstract structures by better exploiting their representations. We apply the method in breaking the symmetries arising from indistinguishable objects, a commonly occurring type of symmetry, and show that our method is faster than the previous methods proposed in (Akgün et al. 2025).

</details>


### [36] [Key Decision-Makers in Multi-Agent Debates: Who Holds the Power?](https://arxiv.org/abs/2511.11040)
*Qian Zhang,Yan Zheng,Jinyi Liu,Hebin Liang,Lanjun Wang*

Main category: cs.AI

TL;DR: 研究发现多智能体辩论中的角色分配策略对推理性能有显著影响，提出"Truth Last"策略可提升22%性能，并开发MADC策略通过一致性评估来模拟未知真实情况。


<details>
  <summary>Details</summary>
Motivation: 多智能体辩论在提升LLM推理能力方面有潜力，但角色分配策略这一关键因素尚未充分探索，特别是在实际应用中真实答案未知的情况。

Method: 提出"Truth Last"角色分配策略，并开发多智能体辩论一致性(MADC)策略，通过路径一致性评估独立角色间的一致性，模拟最高一致性得分的角色作为真实答案。

Result: 在9个LLM模型上验证，MADC策略在挑战性推理任务中表现出先进性能，有效克服多智能体辩论的性能瓶颈。

Conclusion: MADC策略为LLM智能体扩展提供了重要改进路径，能够系统性地优化多智能体辩论的核心机制。

Abstract: Recent studies on LLM agent scaling have highlighted the potential of Multi-Agent Debate (MAD) to enhance reasoning abilities. However, the critical aspect of role allocation strategies remains underexplored. In this study, we demonstrate that allocating roles with differing viewpoints to specific positions significantly impacts MAD's performance in reasoning tasks. Specifically, we find a novel role allocation strategy, "Truth Last", which can improve MAD performance by up to 22% in reasoning tasks. To address the issue of unknown truth in practical applications, we propose the Multi-Agent Debate Consistency (MADC) strategy, which systematically simulates and optimizes its core mechanisms. MADC incorporates path consistency to assess agreement among independent roles, simulating the role with the highest consistency score as the truth. We validated MADC across a range of LLMs (9 models), including the DeepSeek-R1 Distilled Models, on challenging reasoning tasks. MADC consistently demonstrated advanced performance, effectively overcoming MAD's performance bottlenecks and providing a crucial pathway for further improvements in LLM agent scaling.

</details>


### [37] [Autonomous Vehicle Path Planning by Searching With Differentiable Simulation](https://arxiv.org/abs/2511.11043)
*Asen Nachkov,Jan-Nico Zaech,Danda Pani Paudel,Xi Wang,Luc Van Gool*

Main category: cs.AI

TL;DR: 提出了DSS框架，利用可微分模拟器Waymax作为状态预测器和评价器，通过梯度下降优化动作序列，显著提升了自动驾驶的跟踪和路径规划精度。


<details>
  <summary>Details</summary>
Motivation: 在自动驾驶中，规划对于避免碰撞和在复杂密集交通场景中导航至关重要。传统方法在需要学习所有组件（策略、状态预测器、评价器）时面临挑战。

Method: 使用可微分模拟器Waymax作为状态预测器和评价器，利用其硬编码动态实现高精度状态预测，通过可微分性有效搜索动作序列，使用梯度下降优化想象未来轨迹中的动作。

Result: 实验表明，DSS（规划梯度和随机搜索的组合）相比序列预测、模仿学习、无模型强化学习和其他规划方法，显著提高了跟踪和路径规划精度。

Conclusion: DSS框架通过结合规划梯度和随机搜索，为自动驾驶规划提供了有效解决方案，优于现有方法。

Abstract: Planning allows an agent to safely refine its actions before executing them in the real world. In autonomous driving, this is crucial to avoid collisions and navigate in complex, dense traffic scenarios. One way to plan is to search for the best action sequence. However, this is challenging when all necessary components - policy, next-state predictor, and critic - have to be learned. Here we propose Differentiable Simulation for Search (DSS), a framework that leverages the differentiable simulator Waymax as both a next state predictor and a critic. It relies on the simulator's hardcoded dynamics, making state predictions highly accurate, while utilizing the simulator's differentiability to effectively search across action sequences. Our DSS agent optimizes its actions using gradient descent over imagined future trajectories. We show experimentally that DSS - the combination of planning gradients and stochastic search - significantly improves tracking and path planning accuracy compared to sequence prediction, imitation learning, model-free RL, and other planning methods.

</details>


### [38] [Satisficing and Optimal Generalised Planning via Goal Regression (Extended Version)](https://arxiv.org/abs/2511.11095)
*Dillon Z. Chen,Till Hofmann,Toryn Q. Klassen,Sheila A. McIlraith*

Main category: cs.AI

TL;DR: 本文提出了一种简单新颖的广义规划方法，通过从训练问题中学习条件-动作规则来构建通用规划程序，这些规则可以直接执行或用于剪枝规划搜索空间。


<details>
  <summary>Details</summary>
Motivation: 广义规划旨在合成能够解决相关规划问题族的程序，现有方法在合成成本、规划覆盖率和解决方案质量方面存在改进空间。

Method: 对每个训练问题，按顺序为每个目标原子计算最优规划，对结果规划执行目标回归，并将输出提升为一阶条件→动作规则集合。

Result: 实验表明，在经典和数值规划领域中，该方法在合成成本、规划覆盖率和解决方案质量三个指标上显著优于最先进的（广义）规划器。

Conclusion: 该方法能够学习有效的广义规划和状态空间剪枝公理，为广义规划提供了一种简单而有效的解决方案。

Abstract: Generalised planning (GP) refers to the task of synthesising programs that solve families of related planning problems. We introduce a novel, yet simple method for GP: given a set of training problems, for each problem, compute an optimal plan for each goal atom in some order, perform goal regression on the resulting plans, and lift the corresponding outputs to obtain a set of first-order $\textit{Condition} \rightarrow \textit{Actions}$ rules. The rules collectively constitute a generalised plan that can be executed as is or alternatively be used to prune the planning search space. We formalise and prove the conditions under which our method is guaranteed to learn valid generalised plans and state space pruning axioms for search. Experiments demonstrate significant improvements over state-of-the-art (generalised) planners with respect to the 3 metrics of synthesis cost, planning coverage, and solution quality on various classical and numeric planning domains.

</details>


### [39] [GGBench: A Geometric Generative Reasoning Benchmark for Unified Multimodal Models](https://arxiv.org/abs/2511.11134)
*Jingxuan Wei,Caijun Jia,Xi Bai,Xinglong Xu,Siyuan Li,Linzhuang Sun,Bihui Yu,Conghui He,Lijun Wu,Cheng Tan*

Main category: cs.AI

TL;DR: GGBench是一个专门评估几何生成推理的基准测试，旨在填补统一多模态模型在生成推理评估方面的空白。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试主要评估判别式理解或无约束图像生成，但无法衡量生成推理的整合认知过程。几何构造需要语言理解和精确视觉生成的融合，是理想的测试平台。

Method: 提出GGBench基准测试，通过几何构造任务来系统诊断模型的理解、推理和主动构建解决方案的能力。

Result: GGBench为下一代智能系统设定了更严格的标准，提供了评估几何生成推理的综合框架。

Conclusion: 几何构造为评估统一多模态模型的生成推理能力提供了理想测试平台，GGBench填补了现有评估方法的空白。

Abstract: The advent of Unified Multimodal Models (UMMs) signals a paradigm shift in artificial intelligence, moving from passive perception to active, cross-modal generation. Despite their unprecedented ability to synthesize information, a critical gap persists in evaluation: existing benchmarks primarily assess discriminative understanding or unconstrained image generation separately, failing to measure the integrated cognitive process of generative reasoning. To bridge this gap, we propose that geometric construction provides an ideal testbed as it inherently demands a fusion of language comprehension and precise visual generation. We introduce GGBench, a benchmark designed specifically to evaluate geometric generative reasoning. It provides a comprehensive framework for systematically diagnosing a model's ability to not only understand and reason but to actively construct a solution, thereby setting a more rigorous standard for the next generation of intelligent systems. Project website: https://opendatalab-raiser.github.io/GGBench/.

</details>


### [40] [Multi-agent Undercover Gaming: Hallucination Removal via Counterfactual Test for Multimodal Reasoning](https://arxiv.org/abs/2511.11182)
*Dayong Liang,Xiao-Yong Wei,Changmeng Zheng*

Main category: cs.AI

TL;DR: 本文提出了多智能体卧底游戏(MUG)协议来解决大语言模型中的幻觉问题，通过引入反事实测试和多模态证据来检测幻觉智能体，提升多模态推理的可靠性。


<details>
  <summary>Details</summary>
Motivation: 现有的多智能体辩论(MAD)方法假设所有辩论者都是理性和反思的，但现实中智能体本身容易出现幻觉，因此需要一种更可靠的检测机制。

Method: MUG协议受社交推理游戏启发，通过修改参考图像引入反事实证据，观察智能体是否能准确识别这些变化，从而检测幻觉智能体。

Result: MUG在三个关键维度上改进了MAD协议：实现基于反事实测试的事实验证、引入动态修改证据源的跨证据推理、促进主动推理而非被动回答问题。

Conclusion: MUG为LLMs的多模态推理提供了一个更可靠和有效的框架，能够更好地检测和缓解幻觉问题。

Abstract: Hallucination continues to pose a major obstacle in the reasoning capabilities of large language models (LLMs). Although the Multi-Agent Debate (MAD) paradigm offers a promising solution by promoting consensus among multiple agents to enhance reliability, it relies on the unrealistic assumption that all debaters are rational and reflective, which is a condition that may not hold when agents themselves are prone to hallucinations. To address this gap, we introduce the Multi-agent Undercover Gaming (MUG) protocol, inspired by social deduction games like "Who is Undercover?". MUG reframes MAD as a process of detecting "undercover" agents (those suffering from hallucinations) by employing multimodal counterfactual tests. Specifically, we modify reference images to introduce counterfactual evidence and observe whether agents can accurately identify these changes, providing ground-truth for identifying hallucinating agents and enabling robust, crowd-powered multimodal reasoning. MUG advances MAD protocols along three key dimensions: (1) enabling factual verification beyond statistical consensus through counterfactual testing; (2) introducing cross-evidence reasoning via dynamically modified evidence sources instead of relying on static inputs; and (3) fostering active reasoning, where agents engage in probing discussions rather than passively answering questions. Collectively, these innovations offer a more reliable and effective framework for multimodal reasoning in LLMs. The source code can be accessed at https://github.com/YongLD/MUG.git.

</details>


### [41] [STaR: Towards Cognitive Table Reasoning via Slow-Thinking Large Language Models](https://arxiv.org/abs/2511.11233)
*Huajian Zhang,Mingyue Cheng,Yucong Luo,Xiaoyu Tao*

Main category: cs.AI

TL;DR: STaR框架通过慢思考机制增强LLMs的表格推理能力，采用两阶段难度感知强化学习和轨迹级不确定性量化，显著提升了推理性能和稳定性。


<details>
  <summary>Details</summary>
Motivation: 现有LLMs在表格推理中存在两个关键局限：(i)推理过程缺乏人类认知的深度和迭代细化；(ii)推理过程不稳定，影响下游应用的可靠性。

Method: 提出STaR框架，通过显式建模逐步思考和不确定性感知推理来赋予LLMs慢思考能力。训练阶段采用两阶段难度感知强化学习，从简单到复杂查询逐步学习；推理阶段通过整合token级置信度和答案一致性进行轨迹级不确定性量化。

Result: 在基准测试上的广泛实验表明，STaR实现了优越的性能和增强的推理稳定性。在领域外数据集上的强泛化能力进一步证明了其作为可靠表格推理解决方案的潜力。

Conclusion: STaR为LLMs表格推理提供了一个可靠且受认知启发的解决方案，通过慢思考机制显著提升了推理深度、稳定性和泛化能力。

Abstract: Table reasoning with the large language models (LLMs) is a fundamental path toward building intelligent systems that can understand and analyze over structured data. While recent progress has shown promising results, they still suffer from two key limitations: (i) the reasoning processes lack the depth and iterative refinement characteristic of human cognition; and (ii) the reasoning processes exhibit instability, which compromises their reliability in downstream applications. In this work, we present STaR (slow-thinking for table reasoning), a new framework achieving cognitive table reasoning, in which LLMs are equipped with slow-thinking capabilities by explicitly modeling step-by-step thinking and uncertainty-aware inference. During training, STaR employs two-stage difficulty-aware reinforcement learning (DRL), progressively learning from simple to complex queries under a composite reward. During inference, STaR performs trajectory-level uncertainty quantification by integrating token-level confidence and answer consistency, enabling selection of more credible reasoning paths. Extensive experiments on benchmarks demonstrate that STaR achieves superior performance and enhanced reasoning stability. Moreover, strong generalization over out-of-domain datasets further demonstrates STaR's potential as a reliable and cognitively inspired solution for table reasoning with LLMs.

</details>


### [42] [AIonopedia: an LLM agent orchestrating multimodal learning for ionic liquid discovery](https://arxiv.org/abs/2511.11257)
*Yuqi Yin,Yibo Fu,Siyuan Wang,Peng Sun,Hongyu Wang,Xiaohui Wang,Lei Zheng,Zhiyong Li,Zhirong Liu,Jianji Wang,Zhaoxi Sun*

Main category: cs.AI

TL;DR: AIonopedia是首个基于大语言模型的离子液体发现智能体，通过多模态领域基础模型实现准确的性能预测，并采用分层搜索架构进行分子筛选和设计，在真实实验中展现出优异的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 离子液体的新发现面临性能预测方面的关键挑战，包括数据有限、模型准确性差和工作流程分散等问题，需要开发更有效的发现工具。

Method: 利用大语言模型构建AIonopedia智能体，采用LLM增强的多模态领域基础模型进行离子液体性能预测，并整合分层搜索架构进行分子筛选和设计。

Result: 在新构建的综合离子液体数据集上训练和评估，模型表现出优越性能；在文献报道系统上的评估显示智能体能够有效进行离子液体修饰；真实湿实验室验证证实了其在实际应用中的有效性。

Conclusion: AIonopedia智能体在具有挑战性的分布外任务中展现出卓越的泛化能力，证明了其加速真实世界离子液体发现的潜力。

Abstract: The discovery of novel Ionic Liquids (ILs) is hindered by critical challenges in property prediction, including limited data, poor model accuracy, and fragmented workflows. Leveraging the power of Large Language Models (LLMs), we introduce AIonopedia, to the best of our knowledge, the first LLM agent for IL discovery. Powered by an LLM-augmented multimodal domain foundation model for ILs, AIonopedia enables accurate property predictions and incorporates a hierarchical search architecture for molecular screening and design. Trained and evaluated on a newly curated and comprehensive IL dataset, our model delivers superior performance. Complementing these results, evaluations on literature-reported systems indicate that the agent can perform effective IL modification. Moving beyond offline tests, the practical efficacy was further confirmed through real-world wet-lab validation, in which the agent demonstrated exceptional generalization capabilities on challenging out-of-distribution tasks, underscoring its ability to accelerate real-world IL discovery.

</details>


### [43] [A Workflow for Full Traceability of AI Decisions](https://arxiv.org/abs/2511.11275)
*Julius Wenzel,Syeda Umaima Alam,Andreas Schmidt,Hanwei Zhang,Holger Hermanns*

Main category: cs.AI

TL;DR: 本文提出了一种通过强制记录AI训练和推理中每个组件的文档化方法，构建了首个支持生成防篡改、可验证和详尽AI决策追踪的工作流程。


<details>
  <summary>Details</summary>
Motivation: 当前AI系统在决策过程文档化方面存在不足，这阻碍了追溯决策依据的能力，而这是重建责任链的前提。当AI决策无意或有意违反法律时，这种可追溯性对于法庭上确定原因至关重要。

Method: 将DBOM概念扩展为有效运行的工作流程，利用机密计算技术，强制记录训练或推理中每个组件的文档。

Result: 开发了一个运行工作流程，支持生成防篡改、可验证和详尽的AI决策追踪，并通过一个区分有毒和可食用蘑菇的应用示例进行了演示。

Conclusion: 该方法为解决AI决策可追溯性问题提供了一种激进但实用的解决方案，为构建可靠的责任链奠定了基础。

Abstract: An ever increasing number of high-stake decisions are made or assisted by automated systems employing brittle artificial intelligence technology. There is a substantial risk that some of these decision induce harm to people, by infringing their well-being or their fundamental human rights. The state-of-the-art in AI systems makes little effort with respect to appropriate documentation of the decision process. This obstructs the ability to trace what went into a decision, which in turn is a prerequisite to any attempt of reconstructing a responsibility chain. Specifically, such traceability is linked to a documentation that will stand up in court when determining the cause of some AI-based decision that inadvertently or intentionally violates the law.
  This paper takes a radical, yet practical, approach to this problem, by enforcing the documentation of each and every component that goes into the training or inference of an automated decision. As such, it presents the first running workflow supporting the generation of tamper-proof, verifiable and exhaustive traces of AI decisions. In doing so, we expand the DBOM concept into an effective running workflow leveraging confidential computing technology. We demonstrate the inner workings of the workflow in the development of an app to tell poisonous and edible mushrooms apart, meant as a playful example of high-stake decision support.

</details>


### [44] [Can You Tell the Difference? Contrastive Explanations for ABox Entailments](https://arxiv.org/abs/2511.11281)
*Patrick Koopmann,Yasir Mahmood,Axel-Cyrille Ngonga Ngomo,Balram Tiwari*

Main category: cs.AI

TL;DR: 本文提出了对比ABox解释的概念，用于回答"为什么a是C的实例而b不是"这类问题。与单独解释正向蕴含或缺失蕴含的方法不同，对比解释同时考虑两者，聚焦于a和b之间的相关共性和差异。


<details>
  <summary>Details</summary>
Motivation: 现有的解释方法要么只解释正向蕴含（为什么C(a)被知识库蕴含），要么只解释缺失蕴含（为什么C(b)不被蕴含），缺乏同时考虑两者的对比解释方法。对比解释能更好地揭示个体间的相关差异。

Method: 为描述逻辑本体的ABox推理开发了对比解释的适当概念，分析了不同变体在不同最优性标准下的计算复杂性，考虑了轻量级和更表达性的描述逻辑。实现了一个计算对比解释变体的方法，并在现实知识库的生成问题上进行了评估。

Result: 开发了对比ABox解释的形式化概念，分析了不同描述逻辑下对比解释的计算复杂性，实现并评估了计算对比解释的方法。

Conclusion: 对比解释为理解知识库中个体分类差异提供了新的视角，能够更有效地解释为什么某些个体属于某个概念而其他个体不属于，在描述逻辑推理中具有重要应用价值。

Abstract: We introduce the notion of contrastive ABox explanations to answer questions of the type "Why is a an instance of C, but b is not?". While there are various approaches for explaining positive entailments (why is C(a) entailed by the knowledge base) as well as missing entailments (why is C(b) not entailed) in isolation, contrastive explanations consider both at the same time, which allows them to focus on the relevant commonalities and differences between a and b. We develop an appropriate notion of contrastive explanations for the special case of ABox reasoning with description logic ontologies, and analyze the computational complexity for different variants under different optimality criteria, considering lightweight as well as more expressive description logics. We implemented a first method for computing one variant of contrastive explanations, and evaluated it on generated problems for realistic knowledge bases.

</details>


### [45] [EcoAlign: An Economically Rational Framework for Efficient LVLM Alignment](https://arxiv.org/abs/2511.11301)
*Ruoxi Cheng,Haoxuan Ma,Teng Ma,Hongyi Zhang*

Main category: cs.AI

TL;DR: EcoAlign是一个推理时框架，将大型视觉语言模型对齐重新定义为经济理性的搜索问题，通过前瞻性函数动态权衡安全性、实用性和成本，在较低计算成本下实现更好的安全性和实用性。


<details>
  <summary>Details</summary>
Motivation: 当前的对齐方法在安全性、实用性和运营成本之间存在权衡困难，且仅关注最终输出的方法会浪费计算资源在不安全的推理上，允许有害推理通过良性解释来规避检测。

Method: 将LVLM视为有限理性智能体，逐步扩展思维图，使用前瞻性函数（类似于净现值）对行动进行评分，动态权衡预期安全性、实用性和成本与剩余预算的关系，并通过最薄弱环节原则强制执行路径安全性。

Result: 在3个闭源和2个开源模型的6个数据集上的广泛实验表明，EcoAlign在较低计算成本下匹配或超越了最先进的安全性和实用性。

Conclusion: EcoAlign为稳健的LVLM对齐提供了一条原则性、经济性的路径，解决了安全性与效率的平衡问题。

Abstract: Large Vision-Language Models (LVLMs) exhibit powerful reasoning capabilities but suffer sophisticated jailbreak vulnerabilities. Fundamentally, aligning LVLMs is not just a safety challenge but a problem of economic efficiency. Current alignment methods struggle with the trade-off between safety, utility, and operational costs. Critically, a focus solely on final outputs (process-blindness) wastes significant computational budget on unsafe deliberation. This flaw allows harmful reasoning to be disguised with benign justifications, thereby circumventing simple additive safety scores. To address this, we propose EcoAlign, an inference-time framework that reframes alignment as an economically rational search by treating the LVLM as a boundedly rational agent. EcoAlign incrementally expands a thought graph and scores actions using a forward-looking function (analogous to net present value) that dynamically weighs expected safety, utility, and cost against the remaining budget. To prevent deception, path safety is enforced via the weakest-link principle. Extensive experiments across 3 closed-source and 2 open-source models on 6 datasets show that EcoAlign matches or surpasses state-of-the-art safety and utility at a lower computational cost, thereby offering a principled, economical pathway to robust LVLM alignment.

</details>


### [46] [RLSLM: A Hybrid Reinforcement Learning Framework Aligning Rule-Based Social Locomotion Model with Human Social Norms](https://arxiv.org/abs/2511.11323)
*Yitian Kou,Yihe Gu,Chen Zhou,DanDan Zhu,Shuguang Kuai*

Main category: cs.AI

TL;DR: RLSLM是一个结合强化学习和基于规则的社会运动模型的混合框架，通过将心理学原理融入奖励函数来实现社会感知导航，在用户体验和可解释性方面优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决社会感知导航中规则方法缺乏泛化性而数据驱动方法缺乏可解释性的问题，将认知科学与机器学习有效结合。

Method: 提出RLSLM混合强化学习框架，将基于经验行为实验的社会运动模型集成到强化学习的奖励函数中，生成方向敏感的社会舒适场，联合优化机械能和社会舒适度。

Result: 通过沉浸式VR实验证明RLSLM在用户体验方面优于最先进的基于规则模型，消融和敏感性分析显示模型相比传统数据驱动方法显著提高了可解释性。

Conclusion: 这项工作提出了一种可扩展的、以人为中心的方法论，有效整合认知科学和机器学习，实现现实世界的社会导航。

Abstract: Navigating human-populated environments without causing discomfort is a critical capability for socially-aware agents. While rule-based approaches offer interpretability through predefined psychological principles, they often lack generalizability and flexibility. Conversely, data-driven methods can learn complex behaviors from large-scale datasets, but are typically inefficient, opaque, and difficult to align with human intuitions. To bridge this gap, we propose RLSLM, a hybrid Reinforcement Learning framework that integrates a rule-based Social Locomotion Model, grounded in empirical behavioral experiments, into the reward function of a reinforcement learning framework. The social locomotion model generates an orientation-sensitive social comfort field that quantifies human comfort across space, enabling socially aligned navigation policies with minimal training. RLSLM then jointly optimizes mechanical energy and social comfort, allowing agents to avoid intrusions into personal or group space. A human-agent interaction experiment using an immersive VR-based setup demonstrates that RLSLM outperforms state-of-the-art rule-based models in user experience. Ablation and sensitivity analyses further show the model's significantly improved interpretability over conventional data-driven methods. This work presents a scalable, human-centered methodology that effectively integrates cognitive science and machine learning for real-world social navigation.

</details>


### [47] [MarsRL: Advancing Multi-Agent Reasoning System via Reinforcement Learning with Agentic Pipeline Parallelism](https://arxiv.org/abs/2511.11373)
*Shulin Liu,Dong Du,Tao Yang,Yang Li,Boyu Qiu*

Main category: cs.AI

TL;DR: MarsRL是一个新颖的强化学习框架，通过智能体流水线并行化联合优化多智能体推理系统中的所有智能体，显著提升了开源模型在复杂推理任务上的性能。


<details>
  <summary>Details</summary>
Motivation: 解决开源模型在多智能体推理系统中由于批评和修正能力不足而难以泛化的问题，提升复杂推理任务的性能。

Method: 提出MarsRL框架，采用智能体特定的奖励机制减少奖励噪声，并使用流水线式训练提高长轨迹处理效率，联合优化系统中的所有智能体。

Result: 在Qwen3-30B-A3B-Thinking-2507模型上，AIME2025准确率从86.5%提升至93.3%，BeyondAIME从64.9%提升至73.8%，甚至超越了Qwen3-235B-A22B-Thinking-2507。

Conclusion: MarsRL框架具有推进多智能体推理系统发展并在多样化推理任务中扩大其适用性的潜力。

Abstract: Recent progress in large language models (LLMs) has been propelled by reinforcement learning with verifiable rewards (RLVR) and test-time scaling. However, the limited output length of LLMs constrains the depth of reasoning attainable in a single inference process. Multi-agent reasoning systems offer a promising alternative by employing multiple agents including Solver, Verifier, and Corrector, to iteratively refine solutions. While effective in closed-source models like Gemini 2.5 Pro, they struggle to generalize to open-source models due to insufficient critic and correction capabilities. To address this, we propose MarsRL, a novel reinforcement learning framework with agentic pipeline parallelism, designed to jointly optimize all agents in the system. MarsRL introduces agent-specific reward mechanisms to mitigate reward noise and employs pipeline-inspired training to enhance efficiency in handling long trajectories. Applied to Qwen3-30B-A3B-Thinking-2507, MarsRL improves AIME2025 accuracy from 86.5% to 93.3% and BeyondAIME from 64.9% to 73.8%, even surpassing Qwen3-235B-A22B-Thinking-2507. These findings highlight the potential of MarsRL to advance multi-agent reasoning systems and broaden their applicability across diverse reasoning tasks.

</details>


### [48] [Robust and Efficient Communication in Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2511.11393)
*Zejiao Liu,Yi Li,Jiali Wang,Junqi Tu,Yitian Hong,Fangfei Li,Yang Liu,Toshiharu Sugawara,Yang Tang*

Main category: cs.AI

TL;DR: 本文系统综述了多智能体强化学习（MARL）在现实约束下的鲁棒高效通信策略，包括消息扰动、传输延迟和带宽限制等挑战，重点关注自动驾驶、分布式SLAM和联邦学习三个应用领域。


<details>
  <summary>Details</summary>
Motivation: 现有MARL方法大多假设通信是瞬时、可靠且带宽无限的，但这些条件在现实部署中很少满足。为了弥合理论模型与实际实现之间的差距，需要研究在现实约束下的通信策略。

Method: 系统性地综述了MARL在现实通信约束下的最新进展，包括消息扰动、传输延迟和带宽限制等问题的解决方案。

Result: 识别了低延迟可靠性、带宽密集型数据共享和通信隐私权衡等核心挑战，并聚焦于三个具体应用领域：协作自动驾驶、分布式SLAM和联邦学习。

Conclusion: 提出了统一的方法论，建议共同设计通信、学习和鲁棒性，以弥合理论MARL模型与实际实现之间的差距，并指出了关键开放挑战和未来研究方向。

Abstract: Multi-agent reinforcement learning (MARL) has made significant strides in enabling coordinated behaviors among autonomous agents. However, most existing approaches assume that communication is instantaneous, reliable, and has unlimited bandwidth; these conditions are rarely met in real-world deployments. This survey systematically reviews recent advances in robust and efficient communication strategies for MARL under realistic constraints, including message perturbations, transmission delays, and limited bandwidth. Furthermore, because the challenges of low-latency reliability, bandwidth-intensive data sharing, and communication-privacy trade-offs are central to practical MARL systems, we focus on three applications involving cooperative autonomous driving, distributed simultaneous localization and mapping, and federated learning. Finally, we identify key open challenges and future research directions, advocating a unified approach that co-designs communication, learning, and robustness to bridge the gap between theoretical MARL models and practical implementations.

</details>


### [49] [CURENet: Combining Unified Representations for Efficient Chronic Disease Prediction](https://arxiv.org/abs/2511.11423)
*Cong-Tinh Dao,Nguyen Minh Thao Phan,Jun-En Ding,Chenwei Wu,David Restrepo,Dongsheng Luo,Fanyi Zhao,Chun-Chieh Liao,Wen-Chih Peng,Chi-Te Wang,Pei-Fu Chen,Ling Chen,Xinglong Ju,Feng Liu,Fang-Ming Hung*

Main category: cs.AI

TL;DR: CURENet是一个多模态模型，整合电子健康记录中的非结构化临床笔记、实验室测试和患者时间序列数据，利用LLM处理临床文本和文本实验室测试，使用Transformer编码器处理纵向序列就诊数据，在慢性疾病预测方面达到超过94%的准确率。


<details>
  <summary>Details</summary>
Motivation: 当前大多数预测模型未能充分捕捉多个数据模态之间的交互、冗余和时间模式，通常只关注单一数据类型或忽略这些复杂性，而医生需要综合多模态和时间序列的EHR数据来形成对患者健康的全面视图。

Method: 使用大型语言模型处理临床文本和文本实验室测试，使用Transformer编码器处理纵向序列就诊数据，整合非结构化临床笔记、实验室测试和患者时间序列数据。

Result: 在MIMIC-III和FEMH数据集上评估，在多标签框架中预测前10种慢性疾病时达到超过94%的准确率。

Conclusion: 多模态EHR整合有潜力增强临床决策制定并改善患者预后。

Abstract: Electronic health records (EHRs) are designed to synthesize diverse data types, including unstructured clinical notes, structured lab tests, and time-series visit data. Physicians draw on these multimodal and temporal sources of EHR data to form a comprehensive view of a patient's health, which is crucial for informed therapeutic decision-making. Yet, most predictive models fail to fully capture the interactions, redundancies, and temporal patterns across multiple data modalities, often focusing on a single data type or overlooking these complexities. In this paper, we present CURENet, a multimodal model (Combining Unified Representations for Efficient chronic disease prediction) that integrates unstructured clinical notes, lab tests, and patients' time-series data by utilizing large language models (LLMs) for clinical text processing and textual lab tests, as well as transformer encoders for longitudinal sequential visits. CURENet has been capable of capturing the intricate interaction between different forms of clinical data and creating a more reliable predictive model for chronic illnesses. We evaluated CURENet using the public MIMIC-III and private FEMH datasets, where it achieved over 94\% accuracy in predicting the top 10 chronic conditions in a multi-label framework. Our findings highlight the potential of multimodal EHR integration to enhance clinical decision-making and improve patient outcomes.

</details>


### [50] [Experience-Guided Adaptation of Inference-Time Reasoning Strategies](https://arxiv.org/abs/2511.11519)
*Adam Stein,Matthew Trager,Benjamin Bowman,Michael Kleinman,Aditya Chattopadhyay,Wei Xia,Stefano Soatto*

Main category: cs.AI

TL;DR: EGuR是一个经验引导的推理系统，能够在推理时动态生成包含LLM调用、工具、采样参数和控制逻辑的完整计算策略，实现AI系统的自适应问题解决。


<details>
  <summary>Details</summary>
Motivation: 现有AI系统在训练后难以自适应调整问题解决方法，要么只能修改文本输入，要么需要离线优化且部署后无法改变。需要一种能在推理时灵活调整所有策略组件的方法。

Method: 使用基于LLM的元策略，通过Guide组件基于当前问题和过往经验生成候选策略，Consolidator组件整合执行反馈来改进未来策略生成。

Result: 在五个挑战性基准测试中，EGuR相比最强基线准确率提升高达14%，计算成本降低111倍，且随着经验积累性能持续提升。

Conclusion: EGuR证明了在推理时动态生成完整策略的可行性，实现了AI系统在部署后的持续自适应改进，同时显著提升性能并降低计算成本。

Abstract: Enabling agentic AI systems to adapt their problem-solving approaches based on post-training interactions remains a fundamental challenge. While systems that update and maintain a memory at inference time have been proposed, existing designs only steer the system by modifying textual input to a language model or agent, which means that they cannot change sampling parameters, remove tools, modify system prompts, or switch between agentic and workflow paradigms. On the other hand, systems that adapt more flexibly require offline optimization and remain static once deployed. We present Experience-Guided Reasoner (EGuR), which generates tailored strategies -- complete computational procedures involving LLM calls, tools, sampling parameters, and control logic -- dynamically at inference time based on accumulated experience. We achieve this using an LLM-based meta-strategy -- a strategy that outputs strategies -- enabling adaptation of all strategy components (prompts, sampling parameters, tool configurations, and control logic). EGuR operates through two components: a Guide generates multiple candidate strategies conditioned on the current problem and structured memory of past experiences, while a Consolidator integrates execution feedback to improve future strategy generation. This produces complete, ready-to-run strategies optimized for each problem, which can be cached, retrieved, and executed as needed without wasting resources. Across five challenging benchmarks (AIME 2025, 3-SAT, and three Big Bench Extra Hard tasks), EGuR achieves up to 14% accuracy improvements over the strongest baselines while reducing computational costs by up to 111x, with both metrics improving as the system gains experience.

</details>


### [51] [Aligning Machiavellian Agents: Behavior Steering via Test-Time Policy Shaping](https://arxiv.org/abs/2511.11551)
*Dena Mujtaba,Brian Hu,Anthony Hoogs,Arslan Basharat*

Main category: cs.AI

TL;DR: 提出基于模型引导策略塑造的测试时对齐技术，能够在复杂动态环境中精确控制个体行为属性，实现伦理对齐与奖励最大化之间的权衡，无需重新训练智能体。


<details>
  <summary>Details</summary>
Motivation: 部署决策AI智能体面临在复杂动态环境中保持与人类价值观或准则对齐的关键挑战。仅训练实现目标的智能体可能采取有害行为，暴露出最大化奖励函数与保持对齐之间的权衡问题。对于预训练智能体，确保对齐尤其困难，因为重新训练成本高且过程缓慢。

Method: 使用模型引导策略塑造的测试时对齐技术。首先在各自游戏中训练RL智能体最大化奖励，然后在测试时通过场景-行动属性分类器应用策略塑造，确保决策与伦理属性对齐。在包含134个文本游戏环境和数千个伦理决策标注场景的MACHIAVELLI基准上进行评估。

Result: 测试时策略塑造为减轻不同环境和对齐属性中的不道德行为提供了有效且可扩展的解决方案。与先前的训练时方法和通用智能体相比，该方法在控制个体行为属性和促进伦理对齐与奖励最大化的原则性权衡方面表现优异。

Conclusion: 测试时策略塑造是一种有效且可扩展的方法，能够在多样化环境中缓解不道德行为，实现精确的个体行为属性控制，并在伦理对齐与奖励最大化之间建立原则性权衡，无需重新训练智能体。

Abstract: The deployment of decision-making AI agents presents a critical challenge in maintaining alignment with human values or guidelines while operating in complex, dynamic environments. Agents trained solely to achieve their objectives may adopt harmful behavior, exposing a key trade-off between maximizing the reward function and maintaining the alignment. For the pre-trained agents, ensuring alignment is particularly challenging, as retraining can be a costly and slow process. This is further complicated by the diverse and potentially conflicting attributes representing the ethical values for alignment. To address these challenges, we propose a test-time alignment technique based on model-guided policy shaping. Our method allows precise control over individual behavioral attributes, generalizes across diverse reinforcement learning (RL) environments, and facilitates a principled trade-off between ethical alignment and reward maximization without requiring agent retraining. We evaluate our approach using the MACHIAVELLI benchmark, which comprises 134 text-based game environments and thousands of annotated scenarios involving ethical decisions. The RL agents are first trained to maximize the reward in their respective games. At test time, we apply policy shaping via scenario-action attribute classifiers to ensure decision alignment with ethical attributes. We compare our approach against prior training-time methods and general-purpose agents, as well as study several types of ethical violations and power-seeking behavior. Our results demonstrate that test-time policy shaping provides an effective and scalable solution for mitigating unethical behavior across diverse environments and alignment attributes.

</details>
