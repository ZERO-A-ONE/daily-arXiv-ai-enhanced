{"id": "2508.19316", "categories": ["cs.AI", "cs.CL", "cs.LG", "I.2.7; I.2.4"], "pdf": "https://arxiv.org/pdf/2508.19316", "abs": "https://arxiv.org/abs/2508.19316", "authors": ["Shreyans Jain", "Alexandra Yost", "Amirali Abdullah"], "title": "Sycophancy as compositions of Atomic Psychometric Traits", "comment": "8 pages, 4 figures", "summary": "Sycophancy is a key behavioral risk in LLMs, yet is often treated as an\nisolated failure mode that occurs via a single causal mechanism. We instead\npropose modeling it as geometric and causal compositions of psychometric traits\nsuch as emotionality, openness, and agreeableness - similar to factor\ndecomposition in psychometrics. Using Contrastive Activation Addition (CAA), we\nmap activation directions to these factors and study how different combinations\nmay give rise to sycophancy (e.g., high extraversion combined with low\nconscientiousness). This perspective allows for interpretable and compositional\nvector-based interventions like addition, subtraction and projection; that may\nbe used to mitigate safety-critical behaviors in LLMs."}
{"id": "2508.19383", "categories": ["cs.AI", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2508.19383", "abs": "https://arxiv.org/abs/2508.19383", "authors": ["Daoyuan Jin", "Nick Gunner", "Niko Carvajal Janke", "Shivranjani Baruah", "Kaitlin M. Gold", "Yu Jiang"], "title": "Aleks: AI powered Multi Agent System for Autonomous Scientific Discovery via Data-Driven Approaches in Plant Science", "comment": null, "summary": "Modern plant science increasingly relies on large, heterogeneous datasets,\nbut challenges in experimental design, data preprocessing, and reproducibility\nhinder research throughput. Here we introduce Aleks, an AI-powered multi-agent\nsystem that integrates domain knowledge, data analysis, and machine learning\nwithin a structured framework to autonomously conduct data-driven scientific\ndiscovery. Once provided with a research question and dataset, Aleks\niteratively formulated problems, explored alternative modeling strategies, and\nrefined solutions across multiple cycles without human intervention. In a case\nstudy on grapevine red blotch disease, Aleks progressively identified\nbiologically meaningful features and converged on interpretable models with\nrobust performance. Ablation studies underscored the importance of domain\nknowledge and memory for coherent outcomes. This exploratory work highlights\nthe promise of agentic AI as an autonomous collaborator for accelerating\nscientific discovery in plant sciences."}
{"id": "2508.19432", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.19432", "abs": "https://arxiv.org/abs/2508.19432", "authors": ["Yao Fu", "Xianxuan Long", "Runchao Li", "Haotian Yu", "Mu Sheng", "Xiaotian Han", "Yu Yin", "Pan Li"], "title": "Quantized but Deceptive? A Multi-Dimensional Truthfulness Evaluation of Quantized LLMs", "comment": "Accepted to EMNLP2025 main conference (poster)", "summary": "Quantization enables efficient deployment of large language models (LLMs) in\nresource-constrained environments by significantly reducing memory and\ncomputation costs. While quantized LLMs often maintain performance on\nperplexity and zero-shot tasks, their impact on truthfulness-whether generating\ntruthful or deceptive responses-remains largely unexplored. In this work, we\nintroduce TruthfulnessEval, a comprehensive evaluation framework for assessing\nthe truthfulness of quantized LLMs across three dimensions: (1) Truthfulness on\nLogical Reasoning; (2) Truthfulness on Common Sense; and (3) Truthfulness on\nImitative Falsehoods. Using this framework, we examine mainstream quantization\ntechniques (ranging from 4-bit to extreme 2-bit) across several open-source\nLLMs. Surprisingly, we find that while quantized models retain internally\ntruthful representations, they are more susceptible to producing false outputs\nunder misleading prompts. To probe this vulnerability, we test 15 rephrased\nvariants of \"honest\", \"neutral\" and \"deceptive\" prompts and observe that\n\"deceptive\" prompts can override truth-consistent behavior, whereas \"honest\"\nand \"neutral\" prompts maintain stable outputs. Further, we reveal that\nquantized models \"know\" the truth internally yet still produce false outputs\nwhen guided by \"deceptive\" prompts via layer-wise probing and PCA\nvisualizations. Our findings provide insights into future designs of\nquantization-aware alignment and truthfulness interventions."}
{"id": "2508.19449", "categories": ["cs.SE", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.19449", "abs": "https://arxiv.org/abs/2508.19449", "authors": ["Md Afif Al Mamun", "Gias Uddin", "Lan Xia", "Longyu Zhang"], "title": "Stack Trace-Based Crash Deduplication with Transformer Adaptation", "comment": "This work is currently under review at IEEE Transactions on Software\n  Engineering. The replication package will be made publicly available upon\n  acceptance", "summary": "Automated crash reporting systems generate large volumes of duplicate\nreports, overwhelming issue-tracking systems and increasing developer workload.\nTraditional stack trace-based deduplication methods, relying on string\nsimilarity, rule-based heuristics, or deep learning (DL) models, often fail to\ncapture the contextual and structural relationships within stack traces. We\npropose dedupT, a transformer-based approach that models stack traces\nholistically rather than as isolated frames. dedupT first adapts a pretrained\nlanguage model (PLM) to stack traces, then uses its embeddings to train a\nfully-connected network (FCN) to rank duplicate crashes effectively. Extensive\nexperiments on real-world datasets show that dedupT outperforms existing DL and\ntraditional methods (e.g., sequence alignment and information retrieval\ntechniques) in both duplicate ranking and unique crash detection, significantly\nreducing manual triage effort. On four public datasets, dedupT improves Mean\nReciprocal Rank (MRR) often by over 15% compared to the best DL baseline and up\nto 9% over traditional methods while achieving higher Receiver Operating\nCharacteristic Area Under the Curve (ROC-AUC) in detecting unique crash\nreports. Our work advances the integration of modern natural language\nprocessing (NLP) techniques into software engineering, providing an effective\nsolution for stack trace-based crash deduplication."}
{"id": "2508.19461", "categories": ["cs.AI", "cs.CR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.19461", "abs": "https://arxiv.org/abs/2508.19461", "authors": ["Neil Kale", "Chen Bo Calvin Zhang", "Kevin Zhu", "Ankit Aich", "Paula Rodriguez", "Scale Red Team", "Christina Q. Knight", "Zifan Wang"], "title": "Reliable Weak-to-Strong Monitoring of LLM Agents", "comment": "18 pages, 15 figures", "summary": "We stress test monitoring systems for detecting covert misbehavior in\nautonomous LLM agents (e.g., secretly sharing private information). To this\nend, we systematize a monitor red teaming (MRT) workflow that incorporates: (1)\nvarying levels of agent and monitor situational awareness; (2) distinct\nadversarial strategies to evade the monitor, such as prompt injection; and (3)\ntwo datasets and environments -- SHADE-Arena for tool-calling agents and our\nnew CUA-SHADE-Arena, which extends TheAgentCompany, for computer-use agents. We\nrun MRT on existing LLM monitor scaffoldings, which orchestrate LLMs and parse\nagent trajectories, alongside a new hybrid hierarchical-sequential scaffolding\nproposed in this work. Our empirical results yield three key findings. First,\nagent awareness dominates monitor awareness: an agent's knowledge that it is\nbeing monitored substantially degrades the monitor's reliability. On the\ncontrary, providing the monitor with more information about the agent is less\nhelpful than expected. Second, monitor scaffolding matters more than monitor\nawareness: the hybrid scaffolding consistently outperforms baseline monitor\nscaffolding, and can enable weaker models to reliably monitor stronger agents\n-- a weak-to-strong scaling effect. Third, in a human-in-the-loop setting where\nhumans discuss with the LLM monitor to get an updated judgment for the agent's\nbehavior, targeted human oversight is most effective; escalating only\npre-flagged cases to human reviewers improved the TPR by approximately 15% at\nFPR = 0.01. Our work establishes a standard workflow for MRT, highlighting the\nlack of adversarial robustness for LLMs and humans when monitoring and\ndetecting agent misbehavior. We release code, data, and logs to spur further\nresearch."}
{"id": "2508.19558", "categories": ["cs.SE", "cs.CL", "cs.PL"], "pdf": "https://arxiv.org/pdf/2508.19558", "abs": "https://arxiv.org/abs/2508.19558", "authors": ["Zhuohao Li", "Wenqing Chen", "Jianxing Yu", "Zhichao Lu"], "title": "Functional Consistency of LLM Code Embeddings: A Self-Evolving Data Synthesis Framework for Benchmarking", "comment": null, "summary": "Embedding models have demonstrated strong performance in tasks like\nclustering, retrieval, and feature extraction while offering computational\nadvantages over generative models and cross-encoders. Benchmarks such as MTEB\nhave shown that text embeddings from large language models (LLMs) capture rich\nsemantic information, but their ability to reflect code-level functional\nsemantics remains unclear. Existing studies largely focus on code clone\ndetection, which emphasizes syntactic similarity and overlooks functional\nunderstanding. In this paper, we focus on the functional consistency of LLM\ncode embeddings, which determines if two code snippets perform the same\nfunction regardless of syntactic differences. We propose a novel data synthesis\nframework called Functionality-Oriented Code Self-Evolution to construct\ndiverse and challenging benchmarks. Specifically, we define code examples\nacross four semantic and syntactic categories and find that existing datasets\npredominantly capture syntactic properties. Our framework generates four unique\nvariations from a single code instance, providing a broader spectrum of code\nexamples that better reflect functional differences. Extensive experiments on\nthree downstream tasks-code clone detection, code functional consistency\nidentification, and code retrieval-demonstrate that embedding models\nsignificantly improve their performance when trained on our evolved datasets.\nThese results highlight the effectiveness and generalization of our data\nsynthesis framework, advancing the functional understanding of code."}
{"id": "2508.19250", "categories": ["cs.CR", "cs.DM", "math.NT", "quant-ph", "94A60, 68Q12, 06B99, 81P94, 60E15", "E.3; F.2.2; G.2.0; B.8.0"], "pdf": "https://arxiv.org/pdf/2508.19250", "abs": "https://arxiv.org/abs/2508.19250", "authors": ["Ruopengyu Xu", "Chenglian Liu"], "title": "Tight Quantum-Security Bounds and Parameter Optimization for SPHINCS+ and NTRU", "comment": "15 pages, 2tables", "summary": "The imminent threat of quantum computing necessitates quantum-resistant\ncryptosystems. This paper establishes tight security bounds for two NIST PQC\nfinalists: SPHINCS+ (hash-based) and NTRU (lattice-based). Our key\ncontributions include: (1) A quantum attack model incorporating decoherence\neffects ($\\tau_d$) and parallelization limits; (2) Improved entropy\nconcentration inequalities reducing SPHINCS+ parameters by 15-20\\%; (3)\nOptimized NTRU lattice parameters via quantum lattice entropy $H_Q(\\Lambda)$;\n(4) Tightened NTRU-to-LWE reduction with polynomial-factor improvement.\nTheoretical results demonstrate significant security enhancement over existing\nconstructions, providing implementable parameters for standardization."}
{"id": "2508.19502", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.19502", "abs": "https://arxiv.org/abs/2508.19502", "authors": ["Xifeng Yao", "Chengyuan Ma", "Dongyu Lang", "Yinhao Ni", "Zhiwei Xu", "Huarui Xie", "Zihao Chen", "Guang Shen", "Dandan Tu", "Yi Bai", "Changzheng Zhang"], "title": "SLIM: Subtrajectory-Level Elimination for More Effective Reasoning", "comment": "EMNLP 2025 Findings", "summary": "In recent months, substantial progress has been made in complex reasoning of\nLarge Language Models, particularly through the application of test-time\nscaling. Notable examples include o1/o3/o4 series and DeepSeek-R1. When\nresponding to a query, these models generate an extended reasoning trajectory,\nduring which the model explores, reflects, backtracks, and self-verifies before\narriving at a conclusion. However, fine-tuning models with such reasoning\ntrajectories may not always be optimal. Our findings indicate that not all\ncomponents within these reasoning trajectories contribute positively to the\nreasoning process; in fact, some components may affect the overall performance\nnegatively. In this study, we divide a reasoning trajectory into individual\nsubtrajectories and develop a \"5+2\" framework to: (1) systematically identify\nsuboptimal subtrajectories within the reasoning trajectory based on five\nhuman-established criteria; (2) assess the independence of the suboptimal\nsubtrajectories identified in (1) from the subsequent content, ensuring that\ntheir elimination does not compromise overall flow and coherence of the\nreasoning process. Additionally, a sampling algorithm, built upon the \"5+2\"\nframework, is employed to select data whose reasoning process is free from\nsuboptimal subtrajectories to the highest degree. Experimental results\ndemonstrate that our method can reduce the number of suboptimal subtrajectories\nby 25.9\\% during the inference. Furthermore, our method achieves an average\naccuracy of 58.92\\% on highly challenging math benchmarks with only two thirds\nof training data, surpassing the average accuracy of 58.06\\% achieved with the\nentire data, and outperforming open-source datasets, when fine-tuning\nQwen2.5-Math-7B. Finally, We validated our method under resource constraints\nand observed improved performance across various inference token limits."}
{"id": "2508.19610", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.19610", "abs": "https://arxiv.org/abs/2508.19610", "authors": ["Kathrin Figl", "Maria Kirchner", "Sebastian Baltes", "Michael Felderer"], "title": "The Influence of Code Comments on the Perceived Helpfulness of Stack Overflow Posts", "comment": "31 pages, 7 figures, 2 tables, to appear in the Empirical Software\n  Engineering journal", "summary": "Question-and-answer platforms such as Stack Overflow have become an important\nway for software developers to share and retrieve knowledge. However, reusing\npoorly understood code can lead to serious problems, such as bugs or security\nvulnerabilities. To better understand how code comments affect the perceived\nhelpfulness of Stack Overflow answers, we conducted an online experiment\nsimulating a Stack Overflow environment (n=91). The results indicate that both\nblock and inline comments are perceived as significantly more helpful than\nuncommented source code. Moreover, novices rated code snippets with block\ncomments as more helpful than those with inline comments. Interestingly, other\nsurface features, such as the position of an answer and its answer score, were\nconsidered less important. The content of Stack Overflow has been a major\nsource for training large language models. AI-based coding assistants such as\nGitHub Copilot, which are based on these models, might change the way Stack\nOverflow is used. However, our findings have implications beyond this specific\nplatform. First, they may help to improve the relevance of community-driven\nplatforms such as Stack Overflow, which provide human advice and explanations\nof code solutions, complementing AI-based support for software developers.\nSecond, since chat-based AI tools can be prompted to generate code in different\nways, knowing which properties influence perceived helpfulness might lead to\ntargeted prompting strategies to generate more readable code snippets."}
{"id": "2508.19267", "categories": ["cs.CR", "cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2508.19267", "abs": "https://arxiv.org/abs/2508.19267", "authors": ["Sai Teja Reddy Adapala", "Yashwanth Reddy Alugubelly"], "title": "The Aegis Protocol: A Foundational Security Framework for Autonomous AI Agents", "comment": "10 pages, 3 figures, 3 tables. Source compiled with pdfLaTeX;\n  bibliography included via prebuilt main.bbl. Code repository: available in\n  paper", "summary": "The proliferation of autonomous AI agents marks a paradigm shift toward\ncomplex, emergent multi-agent systems. This transition introduces systemic\nsecurity risks, including control-flow hijacking and cascading failures, that\ntraditional cybersecurity paradigms are ill-equipped to address. This paper\nintroduces the Aegis Protocol, a layered security framework designed to provide\nstrong security guarantees for open agentic ecosystems. The protocol integrates\nthree technological pillars: (1) non-spoofable agent identity via W3C\nDecentralized Identifiers (DIDs); (2) communication integrity via\nNIST-standardized post-quantum cryptography (PQC); and (3) verifiable,\nprivacy-preserving policy compliance using the Halo2 zero-knowledge proof (ZKP)\nsystem. We formalize an adversary model extending Dolev-Yao for agentic threats\nand validate the protocol against the STRIDE framework. Our quantitative\nevaluation used a discrete-event simulation, calibrated against cryptographic\nbenchmarks, to model 1,000 agents. The simulation showed a 0 percent success\nrate across 20,000 attack trials. For policy verification, analysis of the\nsimulation logs reported a median proof-generation latency of 2.79 seconds,\nestablishing a performance baseline for this class of security. While the\nevaluation is simulation-based and early-stage, it offers a reproducible\nbaseline for future empirical studies and positions Aegis as a foundation for\nsafe, scalable autonomous AI."}
{"id": "2508.19505", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.19505", "abs": "https://arxiv.org/abs/2508.19505", "authors": ["Gerard Boxo", "Ryan Socha", "Daniel Yoo", "Shivam Raval"], "title": "Caught in the Act: a mechanistic approach to detecting deception", "comment": null, "summary": "Sophisticated instrumentation for AI systems might have indicators that\nsignal misalignment from human values, not unlike a \"check engine\" light in\ncars. One such indicator of misalignment is deceptiveness in generated\nresponses. Future AI instrumentation may have the ability to detect when an LLM\ngenerates deceptive responses while reasoning about seemingly plausible but\nincorrect answers to factual questions. In this work, we demonstrate that\nlinear probes on LLMs internal activations can detect deception in their\nresponses with extremely high accuracy. Our probes reach a maximum of greater\nthan 90% accuracy in distinguishing between deceptive and non-deceptive\narguments generated by llama and qwen models ranging from 1.5B to 14B\nparameters, including their DeepSeek-r1 finetuned variants. We observe that\nprobes on smaller models (1.5B) achieve chance accuracy at detecting deception,\nwhile larger models (greater than 7B) reach 70-80%, with their reasoning\ncounterparts exceeding 90%. The layer-wise probe accuracy follows a three-stage\npattern across layers: near-random (50%) in early layers, peaking in middle\nlayers, and slightly declining in later layers. Furthermore, using an iterative\nnull space projection approach, we find multitudes of linear directions that\nencode deception, ranging from 20 in Qwen 3B to nearly 100 in DeepSeek 7B and\nQwen 14B models."}
{"id": "2508.19663", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.19663", "abs": "https://arxiv.org/abs/2508.19663", "authors": ["Lola Solovyeva", "Eduardo Carneiro Oliveira", "Shiyu Fan", "Alper Tuncay", "Shamil Gareev", "Andrea Capiluppi"], "title": "Leveraging LLMs for Automated Translation of Legacy Code: A Case Study on PL/SQL to Java Transformation", "comment": null, "summary": "The VT legacy system, comprising approximately 2.5 million lines of PL/SQL\ncode, lacks consistent documentation and automated tests, posing significant\nchallenges for refactoring and modernisation. This study investigates the\nfeasibility of leveraging large language models (LLMs) to assist in translating\nPL/SQL code into Java for the modernised \"VTF3\" system. By leveraging a dataset\ncomprising 10 PL/SQL-to-Java code pairs and 15 Java classes, which collectively\nestablished a domain model for the translated files, multiple LLMs were\nevaluated. Furthermore, we propose a customized prompting strategy that\nintegrates chain-of-guidance reasoning with $n$-shot prompting. Our findings\nindicate that this methodology effectively guides LLMs in generating\nsyntactically accurate translations while also achieving functional\ncorrectness. However, the findings are limited by the small sample size of\navailable code files and the restricted access to test cases used for\nvalidating the correctness of the generated code. Nevertheless, these findings\nlay the groundwork for scalable, automated solutions in modernising large\nlegacy systems."}
{"id": "2508.19273", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.19273", "abs": "https://arxiv.org/abs/2508.19273", "authors": ["Tongxi Wu", "Chenwei Xu", "Jin Yang"], "title": "MixGAN: A Hybrid Semi-Supervised and Generative Approach for DDoS Detection in Cloud-Integrated IoT Networks", "comment": null, "summary": "The proliferation of cloud-integrated IoT systems has intensified exposure to\nDistributed Denial of Service (DDoS) attacks due to the expanded attack\nsurface, heterogeneous device behaviors, and limited edge protection. However,\nDDoS detection in this context remains challenging because of complex traffic\ndynamics, severe class imbalance, and scarce labeled data. While recent methods\nhave explored solutions to address class imbalance, many still struggle to\ngeneralize under limited supervision and dynamic traffic conditions. To\novercome these challenges, we propose MixGAN, a hybrid detection method that\nintegrates conditional generation, semi-supervised learning, and robust feature\nextraction. Specifically, to handle complex temporal traffic patterns, we\ndesign a 1-D WideResNet backbone composed of temporal convolutional layers with\nresidual connections, which effectively capture local burst patterns in traffic\nsequences. To alleviate class imbalance and label scarcity, we use a pretrained\nCTGAN to generate synthetic minority-class (DDoS attack) samples that\ncomplement unlabeled data. Furthermore, to mitigate the effect of noisy\npseudo-labels, we introduce a MixUp-Average-Sharpen (MAS) strategy that\nconstructs smoothed and sharpened targets by averaging predictions over\naugmented views and reweighting them towards high-confidence classes.\nExperiments on NSL-KDD, BoT-IoT, and CICIoT2023 demonstrate that MixGAN\nachieves up to 2.5% higher accuracy and 4% improvement in both TPR and TNR\ncompared to state-of-the-art methods, confirming its robustness in large-scale\nIoT-cloud environments. The source code is publicly available at\nhttps://github.com/0xCavaliers/MixGAN."}
{"id": "2508.19562", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.19562", "abs": "https://arxiv.org/abs/2508.19562", "authors": ["Trisanth Srinivasan", "Santosh Patapati"], "title": "Democracy-in-Silico: Institutional Design as Alignment in AI-Governed Polities", "comment": null, "summary": "This paper introduces Democracy-in-Silico, an agent-based simulation where\nsocieties of advanced AI agents, imbued with complex psychological personas,\ngovern themselves under different institutional frameworks. We explore what it\nmeans to be human in an age of AI by tasking Large Language Models (LLMs) to\nembody agents with traumatic memories, hidden agendas, and psychological\ntriggers. These agents engage in deliberation, legislation, and elections under\nvarious stressors, such as budget crises and resource scarcity. We present a\nnovel metric, the Power-Preservation Index (PPI), to quantify misaligned\nbehavior where agents prioritize their own power over public welfare. Our\nfindings demonstrate that institutional design, specifically the combination of\na Constitutional AI (CAI) charter and a mediated deliberation protocol, serves\nas a potent alignment mechanism. These structures significantly reduce corrupt\npower-seeking behavior, improve policy stability, and enhance citizen welfare\ncompared to less constrained democratic models. The simulation reveals that an\ninstitutional design may offer a framework for aligning the complex, emergent\nbehaviors of future artificial agent societies, forcing us to reconsider what\nhuman rituals and responsibilities are essential in an age of shared authorship\nwith non-human entities."}
{"id": "2508.19797", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.19797", "abs": "https://arxiv.org/abs/2508.19797", "authors": ["Joan Giner-Miguelez", "Abel Gómez", "Jordi Cabot"], "title": "Enabling Content Management Systems as an Information Source in Model-driven Projects", "comment": null, "summary": "Content Management Systems (CMSs) are the most popular tool when it comes to\ncreate and publish content across the web. Recently, CMSs have evolved,\nbecoming \\emph{headless}. Content served by a \\emph{headless CMS} aims to be\nconsumed by other applications and services through REST APIs rather than by\nhuman users through a web browser. This evolution has enabled CMSs to become a\nnotorious source of content to be used in a variety of contexts beyond pure web\nnavigation. As such, CMS have become an important component of many information\nsystems. Unfortunately, we still lack the tools to properly discover and manage\nthe information stored in a CMS, often highly customized to the needs of a\nspecific domain. Currently, this is mostly a time-consuming and error-prone\nmanual process.\n  In this paper, we propose a model-based framework to facilitate the\nintegration of headless CMSs in software development processes. Our framework\nis able to discover and explicitly represent the information schema behind the\nCMS. This facilitates designing the interaction between the CMS model and other\ncomponents consuming that information. These interactions are then generated as\npart of a middleware library that offers platform-agnostic access to the CMS to\nall the client applications. The complete framework is open-source and\navailable online."}
{"id": "2508.19278", "categories": ["cs.CR", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.19278", "abs": "https://arxiv.org/abs/2508.19278", "authors": ["Konur Tholl", "Mariam El Mezouar", "Ranwa Al Mallah"], "title": "Towards Production-Worthy Simulation for Autonomous Cyber Operations", "comment": null, "summary": "Simulated environments have proven invaluable in Autonomous Cyber Operations\n(ACO) where Reinforcement Learning (RL) agents can be trained without the\ncomputational overhead of emulation. These environments must accurately\nrepresent cybersecurity scenarios while producing the necessary signals to\nsupport RL training. In this study, we present a framework where we first\nextend CybORG's Cage Challenge 2 environment by implementing three new actions:\nPatch, Isolate, and Unisolate, to better represent the capabilities available\nto human operators in real-world settings. We then propose a design for agent\ndevelopment where we modify the reward signals and the agent's feature space to\nenhance training performance. To validate these modifications, we train DQN and\nPPO agents in the updated environment. Our study demonstrates that CybORG can\nbe extended with additional realistic functionality, while maintaining its\nability to generate informative training signals for RL agents."}
{"id": "2508.19569", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.19569", "abs": "https://arxiv.org/abs/2508.19569", "authors": ["Hung Chau", "Run Yu", "Zachary Pardos", "Peter Brusilovsky"], "title": "Skill-based Explanations for Serendipitous Course Recommendation", "comment": null, "summary": "Academic choice is crucial in U.S. undergraduate education, allowing students\nsignificant freedom in course selection. However, navigating the complex\nacademic environment is challenging due to limited information, guidance, and\nan overwhelming number of choices, compounded by time restrictions and the high\ndemand for popular courses. Although career counselors exist, their numbers are\ninsufficient, and course recommendation systems, though personalized, often\nlack insight into student perceptions and explanations to assess course\nrelevance. In this paper, a deep learning-based concept extraction model is\ndeveloped to efficiently extract relevant concepts from course descriptions to\nimprove the recommendation process. Using this model, the study examines the\neffects of skill-based explanations within a serendipitous recommendation\nframework, tested through the AskOski system at the University of California,\nBerkeley. The findings indicate that these explanations not only increase user\ninterest, particularly in courses with high unexpectedness, but also bolster\ndecision-making confidence. This underscores the importance of integrating\nskill-related data and explanations into educational recommendation systems."}
{"id": "2508.19803", "categories": ["cs.SE", "cs.DB"], "pdf": "https://arxiv.org/pdf/2508.19803", "abs": "https://arxiv.org/abs/2508.19803", "authors": ["Peter Fettke", "Wolfgang Reisig"], "title": "Towards a fundamental theory of modeling discrete systems", "comment": "6 pages, 2 figures, author prepared version of final manuscript\n  accepted at the 44th International Conference on Conceptual Modeling, 20-23\n  October 2025, Poitiers / Futuroscope, France, Workshop on Fundamentals of\n  Conceptual Modeling (FCM)", "summary": "Modeling is a central concern in both science and engineering. However, we\nneed a new fundamental theory to address the challenges of the digital age. In\nthis paper, we first explain why modeling is fundamental and which challenges\nmust be addressed in the digital world. As a main contribution, we introduce\nthe Heraklit modeling framework as a new approach to modeling. We conclude with\nsome general remarks. Future work will involve the correctness of modeling, the\nnotion of information, and the description of invariance in modeling."}
{"id": "2508.19281", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.19281", "abs": "https://arxiv.org/abs/2508.19281", "authors": ["Aoun E Muhammad", "Kin Choong Yow", "Jamel Baili", "Yongwon Cho", "Yunyoung Nam"], "title": "CORTEX: Composite Overlay for Risk Tiering and Exposure in Operational AI Systems", "comment": null, "summary": "As the deployment of Artificial Intelligence (AI) systems in high-stakes\nsectors - like healthcare, finance, education, justice, and infrastructure has\nincreased - the possibility and impact of failures of these systems have\nsignificantly evolved from being a theoretical possibility to practical\nrecurring, systemic risk. This paper introduces CORTEX (Composite Overlay for\nRisk Tiering and Exposure), a multi-layered risk scoring framework proposed to\nassess and score AI system vulnerabilities, developed on empirical analysis of\nover 1,200 incidents documented in the AI Incident Database (AIID), CORTEX\ncategorizes failure modes into 29 technical vulnerability groups. Each\nvulnerability is scored through a five-tier architecture that combines: (1)\nutility-adjusted Likelihood x Impact calculations; (2) governance + contextual\noverlays aligned with regulatory frameworks, such as the EU AI Act, NIST RMF,\nOECD principles; (3) technical surface scores, covering exposure vectors like\ndrift, traceability, and adversarial risk; (4) environmental and residual\nmodifiers tailored to context of where these systems are being deployed to use;\nand (5) a final layered assessment via Bayesian risk aggregation and Monte\nCarlo simulation to model volatility and long-tail risks. The resulting\ncomposite score can be operationalized across AI risk registers, model audits,\nconformity checks, and dynamic governance dashboards."}
{"id": "2508.19576", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.19576", "abs": "https://arxiv.org/abs/2508.19576", "authors": ["Sining Zhoubian", "Dan Zhang", "Yuxiao Dong", "Jie Tang"], "title": "ReST-RL: Achieving Accurate Code Reasoning of LLMs with Optimized Self-Training and Decoding", "comment": "20 pages, 4 figures", "summary": "With respect to improving the reasoning accuracy of LLMs, the representative\nreinforcement learning (RL) method GRPO faces failure due to insignificant\nreward variance, while verification methods based on process reward models\n(PRMs) suffer from difficulties with training data acquisition and verification\neffectiveness. To tackle these problems, this paper introduces ReST-RL, a\nunified LLM RL paradigm that significantly improves LLM's code reasoning\nability by combining an improved GRPO algorithm with a meticulously designed\ntest time decoding method assisted by a value model (VM). As the first stage of\npolicy reinforcement, ReST-GRPO adopts an optimized ReST algorithm to filter\nand assemble high-value training data, increasing the reward variance of GRPO\nsampling, thus improving the effectiveness and efficiency of training. After\nthe basic reasoning ability of LLM policy has been improved, we further propose\na test time decoding optimization method called VM-MCTS. Through Monte-Carlo\nTree Search (MCTS), we collect accurate value targets with no annotation\nrequired, on which VM training is based. When decoding, the VM is deployed by\nan adapted MCTS algorithm to provide precise process signals as well as\nverification scores, assisting the LLM policy to achieve high reasoning\naccuracy. We validate the effectiveness of the proposed RL paradigm through\nextensive experiments on coding problems. Upon comparison, our approach\nsignificantly outperforms other reinforcement training baselines (e.g., naive\nGRPO and ReST-DPO), as well as decoding and verification baselines (e.g.,\nPRM-BoN and ORM-MCTS) on well-known coding benchmarks of various levels (e.g.,\nAPPS, BigCodeBench, and HumanEval), indicating its power to strengthen the\nreasoning ability of LLM policies. Codes for our project can be found at\nhttps://github.com/THUDM/ReST-RL."}
{"id": "2508.19834", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.19834", "abs": "https://arxiv.org/abs/2508.19834", "authors": ["Antero Taivalsaari", "Tommi Mikkonen", "Cesare Pautasso"], "title": "On the Future of Software Reuse in the Era of AI Native Software Engineering", "comment": "21 pages", "summary": "Software development is currently under a paradigm shift in which artificial\nintelligence and generative software reuse are taking the center stage in\nsoftware creation. Earlier opportunistic software reuse practices and organic\nsoftware development methods are rapidly being replaced by \"AI Native\"\napproaches in which developers place their trust on code that has been\ngenerated by artificial intelligence. This is leading to a new form of software\nreuse that is conceptually not all that different from cargo cult development.\nIn this paper we discuss the implications of AI-assisted generative software\nreuse, bring forth relevant questions, and define a research agenda for\ntackling the central issues associated with this emerging approach."}
{"id": "2508.19283", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2508.19283", "abs": "https://arxiv.org/abs/2508.19283", "authors": ["Mark Dorsett", "Scott Man", "Tim Koussas"], "title": "Rethinking Denial-of-Service: A Conditional Taxonomy Unifying Availability and Sustainability Threats", "comment": "7 pages, 3 figures, 3 tables,", "summary": "This paper proposes a unified, condition-based framework for classifying both\nlegacy and cloud-era denial-of-service (DoS) attacks. The framework comprises\nthree interrelated models: a formal conditional tree taxonomy, a hierarchical\nlattice structure based on order theory, and a conceptual Venn diagram. At its\ncore, the taxonomy introduces six observable conditions (C0-C5) grounded in\nreal-world attack behaviours, including source distribution, traffic volume,\ninfrastructure targeting, and financial exploitation. These conditions enable\nconsistent classification of known attacks-such as DoS, DDoS, LDoS, LDDoS,\nEDoS, DoW, and DDoW, while supporting identification of emerging or hybrid\nvariants. The lattice structure captures the cumulative satisfaction of\nconditions, allowing hierarchical reasoning across denial attack classes. The\nVenn diagram highlights conceptual overlaps between availability- and\nsustainability-focused attacks, improving comparative insight. Together, these\nmodels provide a robust analytical lens for threat modeling, mitigation\nstrategy design, and attacker intent classification. The framework is\nparticularly relevant in cloud-native and serverless environments, where\nsustainability-based attacks are increasingly impactful yet under-recognised.\nIts extensibility also permits future integration of socio-technical or\nbehavioural dimensions. By offering a structured taxonomy with theoretical\ngrounding and real-world applicability, this work advances denial attack\ncomprehension and equips defenders, researchers, and cloud architects with a\nshared vocabulary for interpreting and mitigating evolving threat vectors."}
{"id": "2508.19611", "categories": ["cs.AI", "cs.CL", "I.2.7"], "pdf": "https://arxiv.org/pdf/2508.19611", "abs": "https://arxiv.org/abs/2508.19611", "authors": ["Huaiyuan Yao", "Wanpeng Xu", "Justin Turnau", "Nadia Kellam", "Hua Wei"], "title": "Instructional Agents: LLM Agents on Automated Course Material Generation for Teaching Faculties", "comment": "18 pages, 9 figures", "summary": "Preparing high-quality instructional materials remains a labor-intensive\nprocess that often requires extensive coordination among teaching faculty,\ninstructional designers, and teaching assistants. In this work, we present\nInstructional Agents, a multi-agent large language model (LLM) framework\ndesigned to automate end-to-end course material generation, including syllabus\ncreation, lecture scripts, LaTeX-based slides, and assessments. Unlike existing\nAI-assisted educational tools that focus on isolated tasks, Instructional\nAgents simulates role-based collaboration among educational agents to produce\ncohesive and pedagogically aligned content. The system operates in four modes:\nAutonomous, Catalog-Guided, Feedback-Guided, and Full Co-Pilot mode, enabling\nflexible control over the degree of human involvement. We evaluate\nInstructional Agents across five university-level computer science courses and\nshow that it produces high-quality instructional materials while significantly\nreducing development time and human workload. By supporting institutions with\nlimited instructional design capacity, Instructional Agents provides a scalable\nand cost-effective framework to democratize access to high-quality education,\nparticularly in underserved or resource-constrained settings."}
{"id": "2508.19882", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.19882", "abs": "https://arxiv.org/abs/2508.19882", "authors": ["Qunying Song", "He Ye", "Mark Harman", "Federica Sarro"], "title": "Generative AI for Testing of Autonomous Driving Systems: A Survey", "comment": "67 pages, 6 figures, 29 tables", "summary": "Autonomous driving systems (ADS) have been an active area of research, with\nthe potential to deliver significant benefits to society. However, before\nlarge-scale deployment on public roads, extensive testing is necessary to\nvalidate their functionality and safety under diverse driving conditions.\nTherefore, different testing approaches are required, and achieving effective\nand efficient testing of ADS remains an open challenge. Recently, generative AI\nhas emerged as a powerful tool across many domains, and it is increasingly\nbeing applied to ADS testing due to its ability to interpret context, reason\nabout complex tasks, and generate diverse outputs. To gain a deeper\nunderstanding of its role in ADS testing, we systematically analyzed 91\nrelevant studies and synthesized their findings into six major application\ncategories, primarily centered on scenario-based testing of ADS. We also\nreviewed their effectiveness and compiled a wide range of datasets, simulators,\nADS, metrics, and benchmarks used for evaluation, while identifying 27\nlimitations. This survey provides an overview and practical insights into the\nuse of generative AI for testing ADS, highlights existing challenges, and\noutlines directions for future research in this rapidly evolving field."}
{"id": "2508.19284", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2508.19284", "abs": "https://arxiv.org/abs/2508.19284", "authors": ["Mark Dorsett", "Scott Mann", "Jabed Chowdhury", "Abdun Mahmood"], "title": "A Comprehensive Review of Denial of Wallet Attacks in Serverless Architectures", "comment": "12 pages, 2 figures, 5 tables", "summary": "The Denial of Wallet (DoW) attack poses a unique and growing threat to\nserverless architectures that rely on Function-as-a-Service (FaaS) models,\nexploiting the cost structure of pay-as-you-go billing to financially burden\napplication owners. Unlike traditional Denial of Service (DoS) attacks, which\naim to exhaust resources and disrupt service availability, DoW attacks focus on\nescalating costs without impacting service operation. This review traces the\nevolution of DoW research, from initial awareness and attack classification to\nadvancements in detection and mitigation strategies. Key developments include\nthe categorisation of attack types-such as Blast DDoW, Continual Inconspicuous\nDDoW, and Background Chained DDoW-and the creation of simulation tools like\nDoWTS, which enable safe experimentation and data generation. Recent\nadvancements highlight machine learning approaches, including systems like\nGringotts and DoWNet, which leverage deep learning and anomaly detection to\nidentify malicious traffic patterns. Although substantial progress has been\nmade, challenges persist, notably the lack of real-world data and the need for\nadaptive billing models. This is the first comprehensive literature review\ndedicated strictly to Denial of Wallet attacks, providing an in-depth analysis\nof their financial impacts, attack techniques, mitigation strategies, and\ndetection mechanisms within serverless computing. The paper also presents the\nfirst detailed examination of simulation and data generation tools used for DoW\nresearch, addressing a critical gap in existing cybersecurity literature. By\nsynthesising these key areas, this study serves as a foundational resource for\nfuture research and industry efforts in securing pay-as-you-go cloud\nenvironments."}
{"id": "2508.19679", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.19679", "abs": "https://arxiv.org/abs/2508.19679", "authors": ["Qihang Ai", "Pi Bu", "Yue Cao", "Yingyao Wang", "Jihao Gu", "Jingxuan Xing", "Zekun Zhu", "Wei Jiang", "Zhicheng Zheng", "Jun Song", "Yuning Jiang", "Bo Zheng"], "title": "InquireMobile: Teaching VLM-based Mobile Agent to Request Human Assistance via Reinforcement Fine-Tuning", "comment": null, "summary": "Recent advances in Vision-Language Models (VLMs) have enabled mobile agents\nto perceive and interact with real-world mobile environments based on human\ninstructions. However, the current fully autonomous paradigm poses potential\nsafety risks when model understanding or reasoning capabilities are\ninsufficient. To address this challenge, we first introduce\n\\textbf{InquireBench}, a comprehensive benchmark specifically designed to\nevaluate mobile agents' capabilities in safe interaction and proactive inquiry\nwith users, encompassing 5 categories and 22 sub-categories, where most\nexisting VLM-based agents demonstrate near-zero performance. In this paper, we\naim to develop an interactive system that actively seeks human confirmation at\ncritical decision points. To achieve this, we propose \\textbf{InquireMobile}, a\nnovel model inspired by reinforcement learning, featuring a two-stage training\nstrategy and an interactive pre-action reasoning mechanism. Finally, our model\nachieves an 46.8% improvement in inquiry success rate and the best overall\nsuccess rate among existing baselines on InquireBench. We will open-source all\ndatasets, models, and evaluation codes to facilitate development in both\nacademia and industry."}
{"id": "2508.20086", "categories": ["cs.SE", "cs.CR"], "pdf": "https://arxiv.org/pdf/2508.20086", "abs": "https://arxiv.org/abs/2508.20086", "authors": ["Youwei Huang", "Jianwen Li", "Sen Fang", "Yao Li", "Peng Yang", "Bin Hu", "Tao Zhang"], "title": "Smart Contract Intent Detection with Pre-trained Programming Language Model", "comment": "10 pages, 5 figures, conference", "summary": "Malicious intent in smart contract development can lead to substantial\neconomic losses. SmartIntentNN is a deep learning model specifically designed\nto identify unsafe intents in smart contracts. This model integrates the\nUniversal Sentence Encoder, a K-means clustering-based intent highlighting\nmechanism, and a Bidirectional Long Short-Term Memory network for multi-label\nclassification, achieving an F1 of 0.8633 in distinguishing ten different\nintent categories. In this study, we present an upgraded version of this model,\nSmartIntentNN2 (Smart Contract Intent Neural Network V2). A significant\nenhancement in V2 is the incorporation of a BERT-based pre-trained language\nmodel, which has been trained on a dataset of 16,000 real smart contracts using\na Masked Language Modeling objective. SmartIntentNN2 retains the BiLSTM-based\nmulti-label classification network. With an improved F1 of 0.927, V2\ndemonstrates enhanced performance compared to its predecessor, establishing\nitself as the state-of-the-art model for smart contract intent detection."}
{"id": "2508.19286", "categories": ["cs.CR", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.19286", "abs": "https://arxiv.org/abs/2508.19286", "authors": ["Zhan Shi", "Yefeng Yuan", "Yuhong Liu", "Liang Cheng", "Yi Fang"], "title": "RL-Finetuned LLMs for Privacy-Preserving Synthetic Rewriting", "comment": null, "summary": "The performance of modern machine learning systems depends on access to\nlarge, high-quality datasets, often sourced from user-generated content or\nproprietary, domain-specific corpora. However, these rich datasets inherently\ncontain sensitive personal information, raising significant concerns about\nprivacy, data security, and compliance with regulatory frameworks. While\nconventional anonymization techniques can remove explicit identifiers, such\nremoval may result in performance drop in downstream machine learning tasks.\nMore importantly, simple anonymization may not be effective against inference\nattacks that exploit implicit signals such as writing style, topical focus, or\ndemographic cues, highlighting the need for more robust privacy safeguards\nduring model training. To address the challenging issue of balancing user\nprivacy and data utility, we propose a reinforcement learning framework that\nfine-tunes a large language model (LLM) using a composite reward function that\njointly optimizes for explicit and implicit privacy, semantic fidelity, and\noutput diversity. To effectively capture population level regularities, the\nprivacy reward combines semantic cues with structural patterns derived from a\nminimum spanning tree (MST) over latent representations. By modeling these\nprivacy-sensitive signals in their distributional context, the proposed\napproach guides the model to generate synthetic rewrites that preserve utility\nwhile mitigating privacy risks. Empirical results show that the proposed method\nsignificantly enhances author obfuscation and privacy metrics without degrading\nsemantic quality, providing a scalable and model-agnostic solution for privacy\npreserving data generation in the era of large language models."}
{"id": "2508.19827", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2508.19827", "abs": "https://arxiv.org/abs/2508.19827", "authors": ["Samuel Lewis-Lim", "Xingwei Tan", "Zhixue Zhao", "Nikolaos Aletras"], "title": "Analysing Chain of Thought Dynamics: Active Guidance or Unfaithful Post-hoc Rationalisation?", "comment": "Accepted at EMNLP 2025 Main Conference", "summary": "Recent work has demonstrated that Chain-of-Thought (CoT) often yields limited\ngains for soft-reasoning problems such as analytical and commonsense reasoning.\nCoT can also be unfaithful to a model's actual reasoning. We investigate the\ndynamics and faithfulness of CoT in soft-reasoning tasks across\ninstruction-tuned, reasoning and reasoning-distilled models. Our findings\nreveal differences in how these models rely on CoT, and show that CoT influence\nand faithfulness are not always aligned."}
{"id": "2508.19287", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.19287", "abs": "https://arxiv.org/abs/2508.19287", "authors": ["Zhuotao Lian", "Weiyu Wang", "Qingkui Zeng", "Toru Nakanishi", "Teruaki Kitasuka", "Chunhua Su"], "title": "Prompt-in-Content Attacks: Exploiting Uploaded Inputs to Hijack LLM Behavior", "comment": null, "summary": "Large Language Models (LLMs) are widely deployed in applications that accept\nuser-submitted content, such as uploaded documents or pasted text, for tasks\nlike summarization and question answering. In this paper, we identify a new\nclass of attacks, prompt in content injection, where adversarial instructions\nare embedded in seemingly benign inputs. When processed by the LLM, these\nhidden prompts can manipulate outputs without user awareness or system\ncompromise, leading to biased summaries, fabricated claims, or misleading\nsuggestions. We demonstrate the feasibility of such attacks across popular\nplatforms, analyze their root causes including prompt concatenation and\ninsufficient input isolation, and discuss mitigation strategies. Our findings\nreveal a subtle yet practical threat in real-world LLM workflows."}
{"id": "2508.19851", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.19851", "abs": "https://arxiv.org/abs/2508.19851", "authors": ["Romain Harang", "Jason Naradowsky", "Yaswitha Gujju", "Yusuke Miyao"], "title": "Tracking World States with Language Models: State-Based Evaluation Using Chess", "comment": "Spotlight presentation at ICML 2025 Workshop on Assessing World\n  Models", "summary": "Large Language Models (LLMs) exhibit emergent capabilities in structured\ndomains, suggesting they may implicitly internalize high-fidelity\nrepresentations of world models. While probing techniques have shown promising\nsigns of this in scientific and game-based settings, they rely on\nmodel-specific internal activations, which limit interpretability and\ngeneralizability. In this work, we propose a model-agnostic, state-based\nevaluation framework using chess as a benchmark to assess whether LLMs preserve\nthe semantics of structured environments. Our method analyzes the downstream\nlegal move distributions (state affordances) to estimate semantic fidelity\nbetween predicted and actual game states. This approach offers a more\nmeaningful evaluation than conventional string-based metrics by aligning more\nclosely with the strategic and rule-governed nature of chess. Experimental\nresults demonstrate that our metrics capture deficiencies in state-tracking,\nhighlighting limitations of LLMs in maintaining coherent internal models over\nlong sequences. Our framework provides a robust tool for evaluating structured\nreasoning in LLMs without requiring internal model access, and generalizes to a\nwide class of symbolic environments."}
{"id": "2508.19288", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.19288", "abs": "https://arxiv.org/abs/2508.19288", "authors": ["Kyohei Shiomi", "Zhuotao Lian", "Toru Nakanishi", "Teruaki Kitasuka"], "title": "Tricking LLM-Based NPCs into Spilling Secrets", "comment": null, "summary": "Large Language Models (LLMs) are increasingly used to generate dynamic\ndialogue for game NPCs. However, their integration raises new security\nconcerns. In this study, we examine whether adversarial prompt injection can\ncause LLM-based NPCs to reveal hidden background secrets that are meant to\nremain undisclosed."}
{"id": "2508.19932", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.19932", "abs": "https://arxiv.org/abs/2508.19932", "authors": ["Nitish Jaipuria", "Lorenzo Gatto", "Zijun Kan", "Shankey Poddar", "Bill Cheung", "Diksha Bansal", "Ramanan Balakrishnan", "Aviral Suri", "Jose Estevez"], "title": "CASE: An Agentic AI Framework for Enhancing Scam Intelligence in Digital Payments", "comment": "10 pages, 5 figures", "summary": "The proliferation of digital payment platforms has transformed commerce,\noffering unmatched convenience and accessibility globally. However, this growth\nhas also attracted malicious actors, leading to a corresponding increase in\nsophisticated social engineering scams. These scams are often initiated and\norchestrated on multiple surfaces outside the payment platform, making user and\ntransaction-based signals insufficient for a complete understanding of the\nscam's methodology and underlying patterns, without which it is very difficult\nto prevent it in a timely manner. This paper presents CASE (Conversational\nAgent for Scam Elucidation), a novel Agentic AI framework that addresses this\nproblem by collecting and managing user scam feedback in a safe and scalable\nmanner. A conversational agent is uniquely designed to proactively interview\npotential victims to elicit intelligence in the form of a detailed\nconversation. The conversation transcripts are then consumed by another AI\nsystem that extracts information and converts it into structured data for\ndownstream usage in automated and manual enforcement mechanisms. Using Google's\nGemini family of LLMs, we implemented this framework on Google Pay (GPay)\nIndia. By augmenting our existing features with this new intelligence, we have\nobserved a 21% uplift in the volume of scam enforcements. The architecture and\nits robust evaluation framework are highly generalizable, offering a blueprint\nfor building similar AI-driven systems to collect and manage scam intelligence\nin other sensitive domains."}
{"id": "2508.19292", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.19292", "abs": "https://arxiv.org/abs/2508.19292", "authors": ["Xi Wang", "Songlei Jian", "Shasha Li", "Xiaopeng Li", "Bin Ji", "Jun Ma", "Xiaodong Liu", "Jing Wang", "Feilong Bao", "Jianfeng Zhang", "Baosheng Wang", "Jie Yu"], "title": "Stand on The Shoulders of Giants: Building JailExpert from Previous Attack Experience", "comment": "18 pages, EMNLP 2025 Main Conference", "summary": "Large language models (LLMs) generate human-aligned content under certain\nsafety constraints. However, the current known technique ``jailbreak prompt''\ncan circumvent safety-aligned measures and induce LLMs to output malicious\ncontent. Research on Jailbreaking can help identify vulnerabilities in LLMs and\nguide the development of robust security frameworks. To circumvent the issue of\nattack templates becoming obsolete as models evolve, existing methods adopt\niterative mutation and dynamic optimization to facilitate more automated\njailbreak attacks. However, these methods face two challenges: inefficiency and\nrepetitive optimization, as they overlook the value of past attack experiences.\nTo better integrate past attack experiences to assist current jailbreak\nattempts, we propose the \\textbf{JailExpert}, an automated jailbreak framework,\nwhich is the first to achieve a formal representation of experience structure,\ngroup experiences based on semantic drift, and support the dynamic updating of\nthe experience pool. Extensive experiments demonstrate that JailExpert\nsignificantly improves both attack effectiveness and efficiency. Compared to\nthe current state-of-the-art black-box jailbreak methods, JailExpert achieves\nan average increase of 17\\% in attack success rate and 2.7 times improvement in\nattack efficiency. Our implementation is available at\n\\href{https://github.com/xiZAIzai/JailExpert}{XiZaiZai/JailExpert}"}
{"id": "2508.19963", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.19963", "abs": "https://arxiv.org/abs/2508.19963", "authors": ["M. Umlauft", "M. Schranz"], "title": "Flocking Behavior: An Innovative Inspiration for the Optimization of Production Plants", "comment": "This is the author's version of a paper reviewed and accepted by the\n  9th International Symposium on Swarm Behavior and Bio-Inspired Robotics 2025.\n  Authors were not able to present it due to time constraints. 3 Tables, 5\n  Figures", "summary": "Optimizing modern production plants using the job-shop principle is a known\nhard problem. For very large plants, like semiconductor fabs, the problem\nbecomes unsolvable on a plant-wide scale in a reasonable amount of time using\nclassical linear optimization. An alternative approach is the use of swarm\nintelligence algorithms. These have been applied to the job-shop problem\nbefore, but often in a centrally calculated way where they are applied to the\nsolution space, but they can be implemented in a bottom-up fashion to avoid\nglobal result computation as well. One of the problems in semiconductor\nproduction is that the production process requires a lot of switching between\nmachines that process lots one after the other and machines that process\nbatches of lots at once, often with long processing times. In this paper, we\naddress this switching problem with the ``boids'' flocking algorithm that was\noriginally used in robotics and movie industry. The flocking behavior is a\nbio-inspired algorithm that uses only local information and interaction based\non simple heuristics. We show that this algorithm addresses these valid\nconsiderations in production plant optimization, as it reacts to the switching\nof machine kinds similar to how a swarm of flocking animals would react to\nobstacles in its course."}
{"id": "2508.19309", "categories": ["cs.CR", "cs.ET"], "pdf": "https://arxiv.org/pdf/2508.19309", "abs": "https://arxiv.org/abs/2508.19309", "authors": ["Peng Gu", "Shuangchen Li", "Dylan Stow", "Russell Barnes", "Liu Liu", "Yuan Xie", "Eren Kursshan"], "title": "Leveraging 3D Technologies for Hardware Security: Opportunities and Challenges", "comment": null, "summary": "3D die stacking and 2.5D interposer design are promising technologies to\nimprove integration density, performance and cost. Current approaches face\nserious issues in dealing with emerging security challenges such as side\nchannel attacks, hardware trojans, secure IC manufacturing and IP piracy. By\nutilizing intrinsic characteristics of 2.5D and 3D technologies, we propose\nnovel opportunities in designing secure systems. We present: (i) a 3D\narchitecture for shielding side-channel information; (ii) split fabrication\nusing active interposers; (iii) circuit camouflage on monolithic 3D IC, and\n(iv) 3D IC-based security processing-in-memory (PIM). Advantages and challenges\nof these designs are discussed, showing that the new designs can improve\nexisting countermeasures against security threats and further provide new\nsecurity features."}
{"id": "2508.20018", "categories": ["cs.AI", "cs.CL", "cs.CV", "cs.MA"], "pdf": "https://arxiv.org/pdf/2508.20018", "abs": "https://arxiv.org/abs/2508.20018", "authors": ["Quanfeng Lu", "Zhantao Ma", "Shuai Zhong", "Jin Wang", "Dahai Yu", "Michael K. Ng", "Ping Luo"], "title": "SWIRL: A Staged Workflow for Interleaved Reinforcement Learning in Mobile GUI Control", "comment": "28 pages, 12 figures", "summary": "The rapid advancement of large vision language models (LVLMs) and agent\nsystems has heightened interest in mobile GUI agents that can reliably\ntranslate natural language into interface operations. Existing single-agent\napproaches, however, remain limited by structural constraints. Although\nmulti-agent systems naturally decouple different competencies, recent progress\nin multi-agent reinforcement learning (MARL) has often been hindered by\ninefficiency and remains incompatible with current LVLM architectures. To\naddress these challenges, we introduce SWIRL, a staged workflow for interleaved\nreinforcement learning designed for multi-agent systems. SWIRL reformulates\nMARL into a sequence of single-agent reinforcement learning tasks, updating one\nagent at a time while keeping the others fixed. This formulation enables stable\ntraining and promotes efficient coordination across agents. Theoretically, we\nprovide a stepwise safety bound, a cross-round monotonic improvement theorem,\nand convergence guarantees on return, ensuring robust and principled\noptimization. In application to mobile GUI control, SWIRL instantiates a\nNavigator that converts language and screen context into structured plans, and\nan Interactor that grounds these plans into executable atomic actions.\nExtensive experiments demonstrate superior performance on both high-level and\nlow-level GUI benchmarks. Beyond GUI tasks, SWIRL also demonstrates strong\ncapability in multi-agent mathematical reasoning, underscoring its potential as\na general framework for developing efficient and robust multi-agent systems."}
{"id": "2508.19321", "categories": ["cs.CR", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2508.19321", "abs": "https://arxiv.org/abs/2508.19321", "authors": ["Kehao Miao", "Xiaolong Jin"], "title": "An Investigation on Group Query Hallucination Attacks", "comment": null, "summary": "With the widespread use of large language models (LLMs), understanding their\npotential failure modes during user interactions is essential. In practice,\nusers often pose multiple questions in a single conversation with LLMs.\nTherefore, in this study, we propose Group Query Attack, a technique that\nsimulates this scenario by presenting groups of queries to LLMs simultaneously.\nWe investigate how the accumulated context from consecutive prompts influences\nthe outputs of LLMs. Specifically, we observe that Group Query Attack\nsignificantly degrades the performance of models fine-tuned on specific tasks.\nMoreover, we demonstrate that Group Query Attack induces a risk of triggering\npotential backdoors of LLMs. Besides, Group Query Attack is also effective in\ntasks involving reasoning, such as mathematical reasoning and code generation\nfor pre-trained and aligned models."}
{"id": "2508.20040", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.20040", "abs": "https://arxiv.org/abs/2508.20040", "authors": ["Przemyslaw Biecek", "Wojciech Samek"], "title": "Model Science: getting serious about verification, explanation and control of AI systems", "comment": "8 pages", "summary": "The growing adoption of foundation models calls for a paradigm shift from\nData Science to Model Science. Unlike data-centric approaches, Model Science\nplaces the trained model at the core of analysis, aiming to interact, verify,\nexplain, and control its behavior across diverse operational contexts. This\npaper introduces a conceptual framework for a new discipline called Model\nScience, along with the proposal for its four key pillars: Verification, which\nrequires strict, context-aware evaluation protocols; Explanation, which is\nunderstood as various approaches to explore of internal model operations;\nControl, which integrates alignment techniques to steer model behavior; and\nInterface, which develops interactive and visual explanation tools to improve\nhuman calibration and decision-making. The proposed framework aims to guide the\ndevelopment of credible, safe, and human-aligned AI systems."}
{"id": "2508.19323", "categories": ["cs.CR", "cs.CV", "cs.GR"], "pdf": "https://arxiv.org/pdf/2508.19323", "abs": "https://arxiv.org/abs/2508.19323", "authors": ["Ms. Preeti P. Bhatt", "Rakesh R. Savant"], "title": "A Technical Review on Comparison and Estimation of Steganographic Tools", "comment": "20", "summary": "Steganography is technique of hiding a data under cover media using different\nsteganography tools. Image steganography is hiding of data\n(Text/Image/Audio/Video) under a cover as Image. This review paper presents\nclassification of image steganography and the comparison of various Image\nsteganography tools using different image formats. Analyzing numerous tools on\nthe basis of Image features and extracting the best one. Some of the tools\navailable in the market were selected based on the frequent use; these tools\nwere tested using the same input on all of them. Specific text was embedded\nwithin all host images for each of the six Steganography tools selected. The\nresults of the experiment reveal that all the six tools were relatively\nperforming at the same level, though some software performs better than others\nthrough efficiency. And it was based on the image features like size,\ndimensions, and pixel value and histogram differentiation."}
{"id": "2508.19267", "categories": ["cs.CR", "cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2508.19267", "abs": "https://arxiv.org/abs/2508.19267", "authors": ["Sai Teja Reddy Adapala", "Yashwanth Reddy Alugubelly"], "title": "The Aegis Protocol: A Foundational Security Framework for Autonomous AI Agents", "comment": "10 pages, 3 figures, 3 tables. Source compiled with pdfLaTeX;\n  bibliography included via prebuilt main.bbl. Code repository: available in\n  paper", "summary": "The proliferation of autonomous AI agents marks a paradigm shift toward\ncomplex, emergent multi-agent systems. This transition introduces systemic\nsecurity risks, including control-flow hijacking and cascading failures, that\ntraditional cybersecurity paradigms are ill-equipped to address. This paper\nintroduces the Aegis Protocol, a layered security framework designed to provide\nstrong security guarantees for open agentic ecosystems. The protocol integrates\nthree technological pillars: (1) non-spoofable agent identity via W3C\nDecentralized Identifiers (DIDs); (2) communication integrity via\nNIST-standardized post-quantum cryptography (PQC); and (3) verifiable,\nprivacy-preserving policy compliance using the Halo2 zero-knowledge proof (ZKP)\nsystem. We formalize an adversary model extending Dolev-Yao for agentic threats\nand validate the protocol against the STRIDE framework. Our quantitative\nevaluation used a discrete-event simulation, calibrated against cryptographic\nbenchmarks, to model 1,000 agents. The simulation showed a 0 percent success\nrate across 20,000 attack trials. For policy verification, analysis of the\nsimulation logs reported a median proof-generation latency of 2.79 seconds,\nestablishing a performance baseline for this class of security. While the\nevaluation is simulation-based and early-stage, it offers a reproducible\nbaseline for future empirical studies and positions Aegis as a foundation for\nsafe, scalable autonomous AI."}
{"id": "2508.19368", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2508.19368", "abs": "https://arxiv.org/abs/2508.19368", "authors": ["Luqman Muhammad Zagi", "Girindro Pringgo Digdo", "Wervyan Shalannanda"], "title": "Just Dork and Crawl: Measuring Illegal Online Gambling Defacement in Indonesian Websites", "comment": "6 pages, 2 figures, IEEE Conference", "summary": "This study investigates the defacement of Indonesian websites by actors\npromoting illegal online gambling. Using a lightweight methodology that\ncombines keyword-driven dorking with systematic crawling, we identified 453\ndefaced webpages within one month. Although dorking alone yielded a false\npositive rate of approximately 20.3\\%, the integration of crawling and\nkeyword-counting enabled reliable differentiation between true and false\npositives. Our measurements revealed diverse defacement behaviors, including\nrepeat defacements (150 cases), fixed instances (129), keyword modifications\n(55), and redirections or hidden URL injections. In total, 8,837 unique\nthird-party URLs spanning 5,930 domains were captured, with a small subset\nrecurring across multiple sites. Website responses were inconsistent, with an\naverage reaction time of 75.3 hours. These findings demonstrate that simple,\nreproducible techniques can provide meaningful insights into the scale,\npersistence, and dynamics of defacement, highlighting the importance of\ncontinuous measurement for strengthening defenses against online gambling\nactivities."}
{"id": "2508.19273", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.19273", "abs": "https://arxiv.org/abs/2508.19273", "authors": ["Tongxi Wu", "Chenwei Xu", "Jin Yang"], "title": "MixGAN: A Hybrid Semi-Supervised and Generative Approach for DDoS Detection in Cloud-Integrated IoT Networks", "comment": null, "summary": "The proliferation of cloud-integrated IoT systems has intensified exposure to\nDistributed Denial of Service (DDoS) attacks due to the expanded attack\nsurface, heterogeneous device behaviors, and limited edge protection. However,\nDDoS detection in this context remains challenging because of complex traffic\ndynamics, severe class imbalance, and scarce labeled data. While recent methods\nhave explored solutions to address class imbalance, many still struggle to\ngeneralize under limited supervision and dynamic traffic conditions. To\novercome these challenges, we propose MixGAN, a hybrid detection method that\nintegrates conditional generation, semi-supervised learning, and robust feature\nextraction. Specifically, to handle complex temporal traffic patterns, we\ndesign a 1-D WideResNet backbone composed of temporal convolutional layers with\nresidual connections, which effectively capture local burst patterns in traffic\nsequences. To alleviate class imbalance and label scarcity, we use a pretrained\nCTGAN to generate synthetic minority-class (DDoS attack) samples that\ncomplement unlabeled data. Furthermore, to mitigate the effect of noisy\npseudo-labels, we introduce a MixUp-Average-Sharpen (MAS) strategy that\nconstructs smoothed and sharpened targets by averaging predictions over\naugmented views and reweighting them towards high-confidence classes.\nExperiments on NSL-KDD, BoT-IoT, and CICIoT2023 demonstrate that MixGAN\nachieves up to 2.5% higher accuracy and 4% improvement in both TPR and TNR\ncompared to state-of-the-art methods, confirming its robustness in large-scale\nIoT-cloud environments. The source code is publicly available at\nhttps://github.com/0xCavaliers/MixGAN."}
{"id": "2508.19395", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2508.19395", "abs": "https://arxiv.org/abs/2508.19395", "authors": ["Fabian Aude Steen", "Daniel Assani Shabani"], "title": "A NIS2 pan-European registry for identifying and classifying essential and important entities", "comment": null, "summary": "The NIS2 Directive establishes a common cybersecurity governance model across\nthe European Union, requiring member states to identify, classify, and\nsupervise essential and important entities. As part of a broader governance\nnetwork, member states are also obligated to notify the European Commission,\nthe Cooperation Group, and ENISA about their cybersecurity infrastructure\nlandscape. This thesis presents an analysis of the NIS2 Directive in this\ncontext and translates its provisions into concrete technical requirements.\nThese requirements inform the design and implementation of a modular, legally\ngrounded registry system intended to support competent authorities across the\nEU in meeting their obligations. Using the Design Science Research methodology,\nthe thesis transforms complex legal provisions into structured workflows,\ndeterministic classification algorithms, and interactive dashboards. The\nresulting system automates key regulatory processes, including entity\nregistration, classification, and notification, while enabling context-aware\nsupervision and reducing administrative burden. It supports both automated and\nmanual registration methods and introduces a contextual labeling system to\nhandle edge cases, risk factors, and cross-directive dependencies. Although\ndeveloped for the Norwegian regulatory ecosystem, the system is designed for\nadaptation by other member states with minimal modification. This thesis\ncontributes a reusable framework that bridges legal interpretation and\ntechnical implementation, offering a scalable solution for national and\nEU-level NIS2 cybersecurity governance. It also identifies key limitations and\noutlines opportunities for future research and development."}
{"id": "2508.19278", "categories": ["cs.CR", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.19278", "abs": "https://arxiv.org/abs/2508.19278", "authors": ["Konur Tholl", "Mariam El Mezouar", "Ranwa Al Mallah"], "title": "Towards Production-Worthy Simulation for Autonomous Cyber Operations", "comment": null, "summary": "Simulated environments have proven invaluable in Autonomous Cyber Operations\n(ACO) where Reinforcement Learning (RL) agents can be trained without the\ncomputational overhead of emulation. These environments must accurately\nrepresent cybersecurity scenarios while producing the necessary signals to\nsupport RL training. In this study, we present a framework where we first\nextend CybORG's Cage Challenge 2 environment by implementing three new actions:\nPatch, Isolate, and Unisolate, to better represent the capabilities available\nto human operators in real-world settings. We then propose a design for agent\ndevelopment where we modify the reward signals and the agent's feature space to\nenhance training performance. To validate these modifications, we train DQN and\nPPO agents in the updated environment. Our study demonstrates that CybORG can\nbe extended with additional realistic functionality, while maintaining its\nability to generate informative training signals for RL agents."}
{"id": "2508.19430", "categories": ["cs.CR", "cs.FL", "cs.LO"], "pdf": "https://arxiv.org/pdf/2508.19430", "abs": "https://arxiv.org/abs/2508.19430", "authors": ["Kangfeng Ye", "Roberto Metere", "Jim Woodcock", "Poonam Yadav"], "title": "Formal Verification of Physical Layer Security Protocols for Next-Generation Communication Networks", "comment": "Submitted to ICFEM2025; 23 pages, 2 tables, and 6 figures", "summary": "Formal verification is crucial for ensuring the robustness of security\nprotocols against adversarial attacks. The Needham-Schroeder protocol, a\nfoundational authentication mechanism, has been extensively studied, including\nits integration with Physical Layer Security (PLS) techniques such as\nwatermarking and jamming. Recent research has used ProVerif to verify these\nmechanisms in terms of secrecy. However, the ProVerif-based approach limits the\nability to improve understanding of security beyond verification results. To\novercome these limitations, we re-model the same protocol using an Isabelle\nformalism that generates sound animation, enabling interactive and automated\nformal verification of security protocols. Our modelling and verification\nframework is generic and highly configurable, supporting both cryptography and\nPLS. For the same protocol, we have conducted a comprehensive analysis (secrecy\nand authenticity in four different eavesdropper locations under both passive\nand active attacks) using our new web interface. Our findings not only\nsuccessfully reproduce and reinforce previous results on secrecy but also\nreveal an uncommon but expected outcome: authenticity is preserved across all\nexamined scenarios, even in cases where secrecy is compromised. We have\nproposed a PLS-based Diffie-Hellman protocol that integrates watermarking and\njamming, and our analysis shows that it is secure for deriving a session key\nwith required authentication. These highlight the advantages of our novel\napproach, demonstrating its robustness in formally verifying security\nproperties beyond conventional methods."}
{"id": "2508.19281", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.19281", "abs": "https://arxiv.org/abs/2508.19281", "authors": ["Aoun E Muhammad", "Kin Choong Yow", "Jamel Baili", "Yongwon Cho", "Yunyoung Nam"], "title": "CORTEX: Composite Overlay for Risk Tiering and Exposure in Operational AI Systems", "comment": null, "summary": "As the deployment of Artificial Intelligence (AI) systems in high-stakes\nsectors - like healthcare, finance, education, justice, and infrastructure has\nincreased - the possibility and impact of failures of these systems have\nsignificantly evolved from being a theoretical possibility to practical\nrecurring, systemic risk. This paper introduces CORTEX (Composite Overlay for\nRisk Tiering and Exposure), a multi-layered risk scoring framework proposed to\nassess and score AI system vulnerabilities, developed on empirical analysis of\nover 1,200 incidents documented in the AI Incident Database (AIID), CORTEX\ncategorizes failure modes into 29 technical vulnerability groups. Each\nvulnerability is scored through a five-tier architecture that combines: (1)\nutility-adjusted Likelihood x Impact calculations; (2) governance + contextual\noverlays aligned with regulatory frameworks, such as the EU AI Act, NIST RMF,\nOECD principles; (3) technical surface scores, covering exposure vectors like\ndrift, traceability, and adversarial risk; (4) environmental and residual\nmodifiers tailored to context of where these systems are being deployed to use;\nand (5) a final layered assessment via Bayesian risk aggregation and Monte\nCarlo simulation to model volatility and long-tail risks. The resulting\ncomposite score can be operationalized across AI risk registers, model audits,\nconformity checks, and dynamic governance dashboards."}
{"id": "2508.19450", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2508.19450", "abs": "https://arxiv.org/abs/2508.19450", "authors": ["Elvin Li", "Onat Gungor", "Zhengli Shang", "Tajana Rosing"], "title": "CITADEL: Continual Anomaly Detection for Enhanced Learning in IoT Intrusion Detection", "comment": "Under review at IEEE IoTJ", "summary": "The Internet of Things (IoT), with its high degree of interconnectivity and\nlimited computational resources, is particularly vulnerable to a wide range of\ncyber threats. Intrusion detection systems (IDS) have been extensively studied\nto enhance IoT security, and machine learning-based IDS (ML-IDS) show\nconsiderable promise for detecting malicious activity. However, their\neffectiveness is often constrained by poor adaptability to emerging threats and\nthe issue of catastrophic forgetting during continuous learning. To address\nthese challenges, we propose CITADEL, a self-supervised continual learning\nframework designed to extract robust representations from benign data while\npreserving long-term knowledge through optimized memory consolidation\nmechanisms. CITADEL integrates a tabular-to-image transformation module, a\nmemory-aware masked autoencoder for self-supervised representation learning,\nand a novelty detection component capable of identifying anomalies without\ndependence on labeled attack data. Our design enables the system to\nincrementally adapt to emerging behaviors while retaining its ability to detect\npreviously observed threats. Experiments on multiple intrusion datasets\ndemonstrate that CITADEL achieves up to a 72.9% improvement over the VAE-based\nlifelong anomaly detector (VLAD) in key detection and retention metrics,\nhighlighting its effectiveness in dynamic IoT environments."}
{"id": "2508.19286", "categories": ["cs.CR", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.19286", "abs": "https://arxiv.org/abs/2508.19286", "authors": ["Zhan Shi", "Yefeng Yuan", "Yuhong Liu", "Liang Cheng", "Yi Fang"], "title": "RL-Finetuned LLMs for Privacy-Preserving Synthetic Rewriting", "comment": null, "summary": "The performance of modern machine learning systems depends on access to\nlarge, high-quality datasets, often sourced from user-generated content or\nproprietary, domain-specific corpora. However, these rich datasets inherently\ncontain sensitive personal information, raising significant concerns about\nprivacy, data security, and compliance with regulatory frameworks. While\nconventional anonymization techniques can remove explicit identifiers, such\nremoval may result in performance drop in downstream machine learning tasks.\nMore importantly, simple anonymization may not be effective against inference\nattacks that exploit implicit signals such as writing style, topical focus, or\ndemographic cues, highlighting the need for more robust privacy safeguards\nduring model training. To address the challenging issue of balancing user\nprivacy and data utility, we propose a reinforcement learning framework that\nfine-tunes a large language model (LLM) using a composite reward function that\njointly optimizes for explicit and implicit privacy, semantic fidelity, and\noutput diversity. To effectively capture population level regularities, the\nprivacy reward combines semantic cues with structural patterns derived from a\nminimum spanning tree (MST) over latent representations. By modeling these\nprivacy-sensitive signals in their distributional context, the proposed\napproach guides the model to generate synthetic rewrites that preserve utility\nwhile mitigating privacy risks. Empirical results show that the proposed method\nsignificantly enhances author obfuscation and privacy metrics without degrading\nsemantic quality, providing a scalable and model-agnostic solution for privacy\npreserving data generation in the era of large language models."}
{"id": "2508.19456", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2508.19456", "abs": "https://arxiv.org/abs/2508.19456", "authors": ["Cagla Ipek Kocal", "Onat Gungor", "Tajana Rosing", "Baris Aksanli"], "title": "ReLATE+: Unified Framework for Adversarial Attack Detection, Classification, and Resilient Model Selection in Time-Series Classification", "comment": "Under review at IEEE TSMC Journal. arXiv admin note: text overlap\n  with arXiv:2503.07882", "summary": "Minimizing computational overhead in time-series classification, particularly\nin deep learning models, presents a significant challenge due to the high\ncomplexity of model architectures and the large volume of sequential data that\nmust be processed in real time. This challenge is further compounded by\nadversarial attacks, emphasizing the need for resilient methods that ensure\nrobust performance and efficient model selection. To address this challenge, we\npropose ReLATE+, a comprehensive framework that detects and classifies\nadversarial attacks, adaptively selects deep learning models based on\ndataset-level similarity, and thus substantially reduces retraining costs\nrelative to conventional methods that do not leverage prior knowledge, while\nmaintaining strong performance. ReLATE+ first checks whether the incoming data\nis adversarial and, if so, classifies the attack type, using this insight to\nidentify a similar dataset from a repository and enable the reuse of the\nbest-performing associated model. This approach ensures strong performance\nwhile reducing the need for retraining, and it generalizes well across\ndifferent domains with varying data distributions and feature spaces.\nExperiments show that ReLATE+ reduces computational overhead by an average of\n77.68%, enhancing adversarial resilience and streamlining robust model\nselection, all without sacrificing performance, within 2.02% of Oracle."}
{"id": "2508.19287", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.19287", "abs": "https://arxiv.org/abs/2508.19287", "authors": ["Zhuotao Lian", "Weiyu Wang", "Qingkui Zeng", "Toru Nakanishi", "Teruaki Kitasuka", "Chunhua Su"], "title": "Prompt-in-Content Attacks: Exploiting Uploaded Inputs to Hijack LLM Behavior", "comment": null, "summary": "Large Language Models (LLMs) are widely deployed in applications that accept\nuser-submitted content, such as uploaded documents or pasted text, for tasks\nlike summarization and question answering. In this paper, we identify a new\nclass of attacks, prompt in content injection, where adversarial instructions\nare embedded in seemingly benign inputs. When processed by the LLM, these\nhidden prompts can manipulate outputs without user awareness or system\ncompromise, leading to biased summaries, fabricated claims, or misleading\nsuggestions. We demonstrate the feasibility of such attacks across popular\nplatforms, analyze their root causes including prompt concatenation and\ninsufficient input isolation, and discuss mitigation strategies. Our findings\nreveal a subtle yet practical threat in real-world LLM workflows."}
{"id": "2508.19465", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.19465", "abs": "https://arxiv.org/abs/2508.19465", "authors": ["Onyinye Okoye"], "title": "Addressing Weak Authentication like RFID, NFC in EVs and EVCs using AI-powered Adaptive Authentication", "comment": "Research paper exploring AI-driven adaptive authentication in the\n  Electric Vehicle industry", "summary": "The rapid expansion of the Electric Vehicles (EVs) and Electric Vehicle\nCharging Systems (EVCs) has introduced new cybersecurity challenges,\nspecifically in authentication protocols that protect vehicles, users, and\nenergy infrastructure. Although widely adopted for convenience, traditional\nauthentication mechanisms like Radio Frequency Identification (RFID) and Near\nField Communication (NFC) rely on static identifiers and weak encryption,\nmaking them highly vulnerable to attack vectors such as cloning, relay attacks,\nand signal interception. This study explores an AI-powered adaptive\nauthentication framework designed to overcome these shortcomings by integrating\nmachine learning, anomaly detection, behavioral analytics, and contextual risk\nassessment. Grounded in the principles of Zero Trust Architecture, the proposed\nframework emphasizes continuous verification, least privilege access, and\nsecure communication. Through a comprehensive literature review, this research\nevaluates current vulnerabilities and highlights AI-driven solutions to provide\na scalable, resilient, and proactive defense. Ultimately, the research findings\nconclude that adopting AI-powered adaptive authentication is a strategic\nimperative for securing the future of electric mobility and strengthening\ndigital trust across the ecosystem. Keywords: weak authentication, RFID, NFC,\nML, AI-powered adaptive authentication, relay attacks, cloning, eavesdropping,\nMITM attacks, Zero Trust Architecture"}
{"id": "2508.19288", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.19288", "abs": "https://arxiv.org/abs/2508.19288", "authors": ["Kyohei Shiomi", "Zhuotao Lian", "Toru Nakanishi", "Teruaki Kitasuka"], "title": "Tricking LLM-Based NPCs into Spilling Secrets", "comment": null, "summary": "Large Language Models (LLMs) are increasingly used to generate dynamic\ndialogue for game NPCs. However, their integration raises new security\nconcerns. In this study, we examine whether adversarial prompt injection can\ncause LLM-based NPCs to reveal hidden background secrets that are meant to\nremain undisclosed."}
{"id": "2508.19472", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.19472", "abs": "https://arxiv.org/abs/2508.19472", "authors": ["Kyler Katz", "Sara Moshtari", "Ibrahim Mujhid", "Mehdi Mirakhorli", "Derek Garcia"], "title": "SIExVulTS: Sensitive Information Exposure Vulnerability Detection System using Transformer Models and Static Analysis", "comment": null, "summary": "Sensitive Information Exposure (SIEx) vulnerabilities (CWE-200) remain a\npersistent and under-addressed threat across software systems, often leading to\nserious security breaches. Existing detection tools rarely target the diverse\nsubcategories of CWE-200 or provide context-aware analysis of code-level data\nflows.\n  Aims: This paper aims to present SIExVulTS, a novel vulnerability detection\nsystem that integrates transformer-based models with static analysis to\nidentify and verify sensitive information exposure in Java applications.\n  Method: SIExVulTS employs a three-stage architecture: (1) an Attack Surface\nDetection Engine that uses sentence embeddings to identify sensitive variables,\nstrings, comments, and sinks; (2) an Exposure Analysis Engine that instantiates\nCodeQL queries aligned with the CWE-200 hierarchy; and (3) a Flow Verification\nEngine that leverages GraphCodeBERT to semantically validate source-to-sink\nflows. We evaluate SIExVulTS using three curated datasets, including real-world\nCVEs, a benchmark set of synthetic CWE-200 examples, and labeled flows from 31\nopen-source projects.\n  Results: The Attack Surface Detection Engine achieved an average F1 score\ngreater than 93\\%, the Exposure Analysis Engine achieved an F1 score of\n85.71\\%, and the Flow Verification Engine increased precision from 22.61\\% to\n87.23\\%. Moreover, SIExVulTS successfully uncovered six previously unknown CVEs\nin major Apache projects.\n  Conclusions: The results demonstrate that SIExVulTS is effective and\npractical for improving software security against sensitive data exposure,\naddressing limitations of existing tools in detecting and verifying CWE-200\nvulnerabilities."}
{"id": "2508.19292", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.19292", "abs": "https://arxiv.org/abs/2508.19292", "authors": ["Xi Wang", "Songlei Jian", "Shasha Li", "Xiaopeng Li", "Bin Ji", "Jun Ma", "Xiaodong Liu", "Jing Wang", "Feilong Bao", "Jianfeng Zhang", "Baosheng Wang", "Jie Yu"], "title": "Stand on The Shoulders of Giants: Building JailExpert from Previous Attack Experience", "comment": "18 pages, EMNLP 2025 Main Conference", "summary": "Large language models (LLMs) generate human-aligned content under certain\nsafety constraints. However, the current known technique ``jailbreak prompt''\ncan circumvent safety-aligned measures and induce LLMs to output malicious\ncontent. Research on Jailbreaking can help identify vulnerabilities in LLMs and\nguide the development of robust security frameworks. To circumvent the issue of\nattack templates becoming obsolete as models evolve, existing methods adopt\niterative mutation and dynamic optimization to facilitate more automated\njailbreak attacks. However, these methods face two challenges: inefficiency and\nrepetitive optimization, as they overlook the value of past attack experiences.\nTo better integrate past attack experiences to assist current jailbreak\nattempts, we propose the \\textbf{JailExpert}, an automated jailbreak framework,\nwhich is the first to achieve a formal representation of experience structure,\ngroup experiences based on semantic drift, and support the dynamic updating of\nthe experience pool. Extensive experiments demonstrate that JailExpert\nsignificantly improves both attack effectiveness and efficiency. Compared to\nthe current state-of-the-art black-box jailbreak methods, JailExpert achieves\nan average increase of 17\\% in attack success rate and 2.7 times improvement in\nattack efficiency. Our implementation is available at\n\\href{https://github.com/xiZAIzai/JailExpert}{XiZaiZai/JailExpert}"}
{"id": "2508.19493", "categories": ["cs.CR", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.19493", "abs": "https://arxiv.org/abs/2508.19493", "authors": ["Zhixin Lin", "Jungang Li", "Shidong Pan", "Yibo Shi", "Yue Yao", "Dongliang Xu"], "title": "Mind the Third Eye! Benchmarking Privacy Awareness in MLLM-powered Smartphone Agents", "comment": null, "summary": "Smartphones bring significant convenience to users but also enable devices to\nextensively record various types of personal information. Existing smartphone\nagents powered by Multimodal Large Language Models (MLLMs) have achieved\nremarkable performance in automating different tasks. However, as the cost,\nthese agents are granted substantial access to sensitive users' personal\ninformation during this operation. To gain a thorough understanding of the\nprivacy awareness of these agents, we present the first large-scale benchmark\nencompassing 7,138 scenarios to the best of our knowledge. In addition, for\nprivacy context in scenarios, we annotate its type (e.g., Account Credentials),\nsensitivity level, and location. We then carefully benchmark seven available\nmainstream smartphone agents. Our results demonstrate that almost all\nbenchmarked agents show unsatisfying privacy awareness (RA), with performance\nremaining below 60% even with explicit hints. Overall, closed-source agents\nshow better privacy ability than open-source ones, and Gemini 2.0-flash\nachieves the best, achieving an RA of 67%. We also find that the agents'\nprivacy detection capability is highly related to scenario sensitivity level,\ni.e., the scenario with a higher sensitivity level is typically more\nidentifiable. We hope the findings enlighten the research community to rethink\nthe unbalanced utility-privacy tradeoff about smartphone agents. Our code and\nbenchmark are available at https://zhixin-l.github.io/SAPA-Bench."}
{"id": "2508.19321", "categories": ["cs.CR", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2508.19321", "abs": "https://arxiv.org/abs/2508.19321", "authors": ["Kehao Miao", "Xiaolong Jin"], "title": "An Investigation on Group Query Hallucination Attacks", "comment": null, "summary": "With the widespread use of large language models (LLMs), understanding their\npotential failure modes during user interactions is essential. In practice,\nusers often pose multiple questions in a single conversation with LLMs.\nTherefore, in this study, we propose Group Query Attack, a technique that\nsimulates this scenario by presenting groups of queries to LLMs simultaneously.\nWe investigate how the accumulated context from consecutive prompts influences\nthe outputs of LLMs. Specifically, we observe that Group Query Attack\nsignificantly degrades the performance of models fine-tuned on specific tasks.\nMoreover, we demonstrate that Group Query Attack induces a risk of triggering\npotential backdoors of LLMs. Besides, Group Query Attack is also effective in\ntasks involving reasoning, such as mathematical reasoning and code generation\nfor pre-trained and aligned models."}
{"id": "2508.19500", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.19500", "abs": "https://arxiv.org/abs/2508.19500", "authors": ["David Noever"], "title": "Servant, Stalker, Predator: How An Honest, Helpful, And Harmless (3H) Agent Unlocks Adversarial Skills", "comment": null, "summary": "This paper identifies and analyzes a novel vulnerability class in Model\nContext Protocol (MCP) based agent systems. The attack chain describes and\ndemonstrates how benign, individually authorized tasks can be orchestrated to\nproduce harmful emergent behaviors. Through systematic analysis using the MITRE\nATLAS framework, we demonstrate how 95 agents tested with access to multiple\nservices-including browser automation, financial analysis, location tracking,\nand code deployment-can chain legitimate operations into sophisticated attack\nsequences that extend beyond the security boundaries of any individual service.\nThese red team exercises survey whether current MCP architectures lack\ncross-domain security measures necessary to detect or prevent a large category\nof compositional attacks. We present empirical evidence of specific attack\nchains that achieve targeted harm through service orchestration, including data\nexfiltration, financial manipulation, and infrastructure compromise. These\nfindings reveal that the fundamental security assumption of service isolation\nfails when agents can coordinate actions across multiple domains, creating an\nexponential attack surface that grows with each additional capability. This\nresearch provides a barebones experimental framework that evaluate not whether\nagents can complete MCP benchmark tasks, but what happens when they complete\nthem too well and optimize across multiple services in ways that violate human\nexpectations and safety constraints. We propose three concrete experimental\ndirections using the existing MCP benchmark suite."}
{"id": "2508.19465", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.19465", "abs": "https://arxiv.org/abs/2508.19465", "authors": ["Onyinye Okoye"], "title": "Addressing Weak Authentication like RFID, NFC in EVs and EVCs using AI-powered Adaptive Authentication", "comment": "Research paper exploring AI-driven adaptive authentication in the\n  Electric Vehicle industry", "summary": "The rapid expansion of the Electric Vehicles (EVs) and Electric Vehicle\nCharging Systems (EVCs) has introduced new cybersecurity challenges,\nspecifically in authentication protocols that protect vehicles, users, and\nenergy infrastructure. Although widely adopted for convenience, traditional\nauthentication mechanisms like Radio Frequency Identification (RFID) and Near\nField Communication (NFC) rely on static identifiers and weak encryption,\nmaking them highly vulnerable to attack vectors such as cloning, relay attacks,\nand signal interception. This study explores an AI-powered adaptive\nauthentication framework designed to overcome these shortcomings by integrating\nmachine learning, anomaly detection, behavioral analytics, and contextual risk\nassessment. Grounded in the principles of Zero Trust Architecture, the proposed\nframework emphasizes continuous verification, least privilege access, and\nsecure communication. Through a comprehensive literature review, this research\nevaluates current vulnerabilities and highlights AI-driven solutions to provide\na scalable, resilient, and proactive defense. Ultimately, the research findings\nconclude that adopting AI-powered adaptive authentication is a strategic\nimperative for securing the future of electric mobility and strengthening\ndigital trust across the ecosystem. Keywords: weak authentication, RFID, NFC,\nML, AI-powered adaptive authentication, relay attacks, cloning, eavesdropping,\nMITM attacks, Zero Trust Architecture"}
{"id": "2508.19525", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2508.19525", "abs": "https://arxiv.org/abs/2508.19525", "authors": ["Tianshi Xu", "Wen-jie Lu", "Jiangrui Yu", "Chen Yi", "Chenqi Lin", "Runsheng Wang", "Meng Li"], "title": "Breaking the Layer Barrier: Remodeling Private Transformer Inference with Hybrid CKKS and MPC", "comment": "USENIX Security 2025", "summary": "This paper presents an efficient framework for private Transformer inference\nthat combines Homomorphic Encryption (HE) and Secure Multi-party Computation\n(MPC) to protect data privacy. Existing methods often leverage HE for linear\nlayers (e.g., matrix multiplications) and MPC for non-linear layers (e.g.,\nSoftmax activation functions), but the conversion between HE and MPC introduces\nsignificant communication costs. The proposed framework, dubbed BLB, overcomes\nthis by breaking down layers into fine-grained operators and further fusing\nadjacent linear operators, reducing the need for HE/MPC conversions. To manage\nthe increased ciphertext bit width from the fused linear operators, BLB\nproposes the first secure conversion protocol between CKKS and MPC and enables\nCKKS-based computation of the fused operators. Additionally, BLB proposes an\nefficient matrix multiplication protocol for fused computation in Transformers.\nExtensive evaluations on BERT-base, BERT-large, and GPT2-base show that BLB\nachieves a $21\\times$ reduction in communication overhead compared to BOLT\n(S\\&P'24) and a $2\\times$ reduction compared to Bumblebee (NDSS'25), along with\nlatency reductions of $13\\times$ and $1.8\\times$, respectively, when leveraging\nGPU acceleration."}
{"id": "2508.19472", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.19472", "abs": "https://arxiv.org/abs/2508.19472", "authors": ["Kyler Katz", "Sara Moshtari", "Ibrahim Mujhid", "Mehdi Mirakhorli", "Derek Garcia"], "title": "SIExVulTS: Sensitive Information Exposure Vulnerability Detection System using Transformer Models and Static Analysis", "comment": null, "summary": "Sensitive Information Exposure (SIEx) vulnerabilities (CWE-200) remain a\npersistent and under-addressed threat across software systems, often leading to\nserious security breaches. Existing detection tools rarely target the diverse\nsubcategories of CWE-200 or provide context-aware analysis of code-level data\nflows.\n  Aims: This paper aims to present SIExVulTS, a novel vulnerability detection\nsystem that integrates transformer-based models with static analysis to\nidentify and verify sensitive information exposure in Java applications.\n  Method: SIExVulTS employs a three-stage architecture: (1) an Attack Surface\nDetection Engine that uses sentence embeddings to identify sensitive variables,\nstrings, comments, and sinks; (2) an Exposure Analysis Engine that instantiates\nCodeQL queries aligned with the CWE-200 hierarchy; and (3) a Flow Verification\nEngine that leverages GraphCodeBERT to semantically validate source-to-sink\nflows. We evaluate SIExVulTS using three curated datasets, including real-world\nCVEs, a benchmark set of synthetic CWE-200 examples, and labeled flows from 31\nopen-source projects.\n  Results: The Attack Surface Detection Engine achieved an average F1 score\ngreater than 93\\%, the Exposure Analysis Engine achieved an F1 score of\n85.71\\%, and the Flow Verification Engine increased precision from 22.61\\% to\n87.23\\%. Moreover, SIExVulTS successfully uncovered six previously unknown CVEs\nin major Apache projects.\n  Conclusions: The results demonstrate that SIExVulTS is effective and\npractical for improving software security against sensitive data exposure,\naddressing limitations of existing tools in detecting and verifying CWE-200\nvulnerabilities."}
{"id": "2508.19641", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.19641", "abs": "https://arxiv.org/abs/2508.19641", "authors": ["Lincan Li", "Bolin Shen", "Chenxi Zhao", "Yuxiang Sun", "Kaixiang Zhao", "Shirui Pan", "Yushun Dong"], "title": "Intellectual Property in Graph-Based Machine Learning as a Service: Attacks and Defenses", "comment": null, "summary": "Graph-structured data, which captures non-Euclidean relationships and\ninteractions between entities, is growing in scale and complexity. As a result,\ntraining state-of-the-art graph machine learning (GML) models have become\nincreasingly resource-intensive, turning these models and data into invaluable\nIntellectual Property (IP). To address the resource-intensive nature of model\ntraining, graph-based Machine-Learning-as-a-Service (GMLaaS) has emerged as an\nefficient solution by leveraging third-party cloud services for model\ndevelopment and management. However, deploying such models in GMLaaS also\nexposes them to potential threats from attackers. Specifically, while the APIs\nwithin a GMLaaS system provide interfaces for users to query the model and\nreceive outputs, they also allow attackers to exploit and steal model\nfunctionalities or sensitive training data, posing severe threats to the safety\nof these GML models and the underlying graph data. To address these challenges,\nthis survey systematically introduces the first taxonomy of threats and\ndefenses at the level of both GML model and graph-structured data. Such a\ntailored taxonomy facilitates an in-depth understanding of GML IP protection.\nFurthermore, we present a systematic evaluation framework to assess the\neffectiveness of IP protection methods, introduce a curated set of benchmark\ndatasets across various domains, and discuss their application scopes and\nfuture challenges. Finally, we establish an open-sourced versatile library\nnamed PyGIP, which evaluates various attack and defense techniques in GMLaaS\nscenarios and facilitates the implementation of existing benchmark methods. The\nlibrary resource can be accessed at: https://labrai.github.io/PyGIP. We believe\nthis survey will play a fundamental role in intellectual property protection\nfor GML and provide practical recipes for the GML community."}
{"id": "2508.19500", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.19500", "abs": "https://arxiv.org/abs/2508.19500", "authors": ["David Noever"], "title": "Servant, Stalker, Predator: How An Honest, Helpful, And Harmless (3H) Agent Unlocks Adversarial Skills", "comment": null, "summary": "This paper identifies and analyzes a novel vulnerability class in Model\nContext Protocol (MCP) based agent systems. The attack chain describes and\ndemonstrates how benign, individually authorized tasks can be orchestrated to\nproduce harmful emergent behaviors. Through systematic analysis using the MITRE\nATLAS framework, we demonstrate how 95 agents tested with access to multiple\nservices-including browser automation, financial analysis, location tracking,\nand code deployment-can chain legitimate operations into sophisticated attack\nsequences that extend beyond the security boundaries of any individual service.\nThese red team exercises survey whether current MCP architectures lack\ncross-domain security measures necessary to detect or prevent a large category\nof compositional attacks. We present empirical evidence of specific attack\nchains that achieve targeted harm through service orchestration, including data\nexfiltration, financial manipulation, and infrastructure compromise. These\nfindings reveal that the fundamental security assumption of service isolation\nfails when agents can coordinate actions across multiple domains, creating an\nexponential attack surface that grows with each additional capability. This\nresearch provides a barebones experimental framework that evaluate not whether\nagents can complete MCP benchmark tasks, but what happens when they complete\nthem too well and optimize across multiple services in ways that violate human\nexpectations and safety constraints. We propose three concrete experimental\ndirections using the existing MCP benchmark suite."}
{"id": "2508.19697", "categories": ["cs.CR", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2508.19697", "abs": "https://arxiv.org/abs/2508.19697", "authors": ["Chao Huang", "Zefeng Zhang", "Juewei Yue", "Quangang Li", "Chuang Zhang", "Tingwen Liu"], "title": "Safety Alignment Should Be Made More Than Just A Few Attention Heads", "comment": null, "summary": "Current safety alignment for large language models(LLMs) continues to present\nvulnerabilities, given that adversarial prompting can effectively bypass their\nsafety measures.Our investigation shows that these safety mechanisms\npredominantly depend on a limited subset of attention heads: removing or\nablating these heads can severely compromise model safety. To identify and\nevaluate these safety-critical components, we introduce RDSHA, a targeted\nablation method that leverages the model's refusal direction to pinpoint\nattention heads mostly responsible for safety behaviors. Further analysis shows\nthat existing jailbreak attacks exploit this concentration by selectively\nbypassing or manipulating these critical attention heads. To address this\nissue, we propose AHD, a novel training strategy designed to promote the\ndistributed encoding of safety-related behaviors across numerous attention\nheads. Experimental results demonstrate that AHD successfully distributes\nsafety-related capabilities across more attention heads. Moreover, evaluations\nunder several mainstream jailbreak attacks show that models trained with AHD\nexhibit considerably stronger safety robustness, while maintaining overall\nfunctional utility."}
{"id": "2508.19641", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.19641", "abs": "https://arxiv.org/abs/2508.19641", "authors": ["Lincan Li", "Bolin Shen", "Chenxi Zhao", "Yuxiang Sun", "Kaixiang Zhao", "Shirui Pan", "Yushun Dong"], "title": "Intellectual Property in Graph-Based Machine Learning as a Service: Attacks and Defenses", "comment": null, "summary": "Graph-structured data, which captures non-Euclidean relationships and\ninteractions between entities, is growing in scale and complexity. As a result,\ntraining state-of-the-art graph machine learning (GML) models have become\nincreasingly resource-intensive, turning these models and data into invaluable\nIntellectual Property (IP). To address the resource-intensive nature of model\ntraining, graph-based Machine-Learning-as-a-Service (GMLaaS) has emerged as an\nefficient solution by leveraging third-party cloud services for model\ndevelopment and management. However, deploying such models in GMLaaS also\nexposes them to potential threats from attackers. Specifically, while the APIs\nwithin a GMLaaS system provide interfaces for users to query the model and\nreceive outputs, they also allow attackers to exploit and steal model\nfunctionalities or sensitive training data, posing severe threats to the safety\nof these GML models and the underlying graph data. To address these challenges,\nthis survey systematically introduces the first taxonomy of threats and\ndefenses at the level of both GML model and graph-structured data. Such a\ntailored taxonomy facilitates an in-depth understanding of GML IP protection.\nFurthermore, we present a systematic evaluation framework to assess the\neffectiveness of IP protection methods, introduce a curated set of benchmark\ndatasets across various domains, and discuss their application scopes and\nfuture challenges. Finally, we establish an open-sourced versatile library\nnamed PyGIP, which evaluates various attack and defense techniques in GMLaaS\nscenarios and facilitates the implementation of existing benchmark methods. The\nlibrary resource can be accessed at: https://labrai.github.io/PyGIP. We believe\nthis survey will play a fundamental role in intellectual property protection\nfor GML and provide practical recipes for the GML community."}
{"id": "2508.19714", "categories": ["cs.CR", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.19714", "abs": "https://arxiv.org/abs/2508.19714", "authors": ["Subhrojyoti Mukherjee", "Manoranjan Mohanty"], "title": "Addressing Deepfake Issue in Selfie banking through camera based authentication", "comment": null, "summary": "Fake images in selfie banking are increasingly becoming a threat. Previously,\nit was just Photoshop, but now deep learning technologies enable us to create\nhighly realistic fake identities, which fraudsters exploit to bypass biometric\nsystems such as facial recognition in online banking. This paper explores the\nuse of an already established forensic recognition system, previously used for\npicture camera localization, in deepfake detection."}
{"id": "2508.19697", "categories": ["cs.CR", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2508.19697", "abs": "https://arxiv.org/abs/2508.19697", "authors": ["Chao Huang", "Zefeng Zhang", "Juewei Yue", "Quangang Li", "Chuang Zhang", "Tingwen Liu"], "title": "Safety Alignment Should Be Made More Than Just A Few Attention Heads", "comment": null, "summary": "Current safety alignment for large language models(LLMs) continues to present\nvulnerabilities, given that adversarial prompting can effectively bypass their\nsafety measures.Our investigation shows that these safety mechanisms\npredominantly depend on a limited subset of attention heads: removing or\nablating these heads can severely compromise model safety. To identify and\nevaluate these safety-critical components, we introduce RDSHA, a targeted\nablation method that leverages the model's refusal direction to pinpoint\nattention heads mostly responsible for safety behaviors. Further analysis shows\nthat existing jailbreak attacks exploit this concentration by selectively\nbypassing or manipulating these critical attention heads. To address this\nissue, we propose AHD, a novel training strategy designed to promote the\ndistributed encoding of safety-related behaviors across numerous attention\nheads. Experimental results demonstrate that AHD successfully distributes\nsafety-related capabilities across more attention heads. Moreover, evaluations\nunder several mainstream jailbreak attacks show that models trained with AHD\nexhibit considerably stronger safety robustness, while maintaining overall\nfunctional utility."}
{"id": "2508.19774", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2508.19774", "abs": "https://arxiv.org/abs/2508.19774", "authors": ["Tong Liu", "Guozhu Meng", "Peng Zhou", "Zizhuang Deng", "Shuaiyin Yao", "Kai Chen"], "title": "The Art of Hide and Seek: Making Pickle-Based Model Supply Chain Poisoning Stealthy Again", "comment": null, "summary": "Pickle deserialization vulnerabilities have persisted throughout Python's\nhistory, remaining widely recognized yet unresolved. Due to its ability to\ntransparently save and restore complex objects into byte streams, many AI/ML\nframeworks continue to adopt pickle as the model serialization protocol despite\nits inherent risks. As the open-source model ecosystem grows, model-sharing\nplatforms such as Hugging Face have attracted massive participation,\nsignificantly amplifying the real-world risks of pickle exploitation and\nopening new avenues for model supply chain poisoning. Although several\nstate-of-the-art scanners have been developed to detect poisoned models, their\nincomplete understanding of the poisoning surface leaves the detection logic\nfragile and allows attackers to bypass them. In this work, we present the first\nsystematic disclosure of the pickle-based model poisoning surface from both\nmodel loading and risky function perspectives. Our research demonstrates how\npickle-based model poisoning can remain stealthy and highlights critical gaps\nin current scanning solutions. On the model loading surface, we identify 22\ndistinct pickle-based model loading paths across five foundational AI/ML\nframeworks, 19 of which are entirely missed by existing scanners. We further\ndevelop a bypass technique named Exception-Oriented Programming (EOP) and\ndiscover 9 EOP instances, 7 of which can bypass all scanners. On the risky\nfunction surface, we discover 133 exploitable gadgets, achieving almost a 100%\nbypass rate. Even against the best-performing scanner, these gadgets maintain\nan 89% bypass rate. By systematically revealing the pickle-based model\npoisoning surface, we achieve practical and robust bypasses against real-world\nscanners. We responsibly disclose our findings to corresponding vendors,\nreceiving acknowledgments and a $6000 bug bounty."}
{"id": "2508.19819", "categories": ["cs.CR", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.19819", "abs": "https://arxiv.org/abs/2508.19819", "authors": ["Viktor Valadi", "Mattias Åkesson", "Johan Östman", "Salman Toor", "Andreas Hellander"], "title": "From Research to Reality: Feasibility of Gradient Inversion Attacks in Federated Learning", "comment": "Under review at KDD 2026 (Research Track)", "summary": "Gradient inversion attacks have garnered attention for their ability to\ncompromise privacy in federated learning. However, many studies consider\nattacks with the model in inference mode, where training-time behaviors like\ndropout are disabled and batch normalization relies on fixed statistics. In\nthis work, we systematically analyze how architecture and training behavior\naffect vulnerability, including the first in-depth study of inference-mode\nclients, which we show dramatically simplifies inversion. To assess attack\nfeasibility under more realistic conditions, we turn to clients operating in\nstandard training mode. In this setting, we find that successful attacks are\nonly possible when several architectural conditions are met simultaneously:\nmodels must be shallow and wide, use skip connections, and, critically, employ\npre-activation normalization. We introduce two novel attacks against models in\ntraining-mode with varying attacker knowledge, achieving state-of-the-art\nperformance under realistic training conditions. We extend these efforts by\npresenting the first attack on a production-grade object-detection model. Here,\nto enable any visibly identifiable leakage, we revert to the lenient inference\nmode setting and make multiple architectural modifications to increase model\nvulnerability, with the extent of required changes highlighting the strong\ninherent robustness of such architectures. We conclude this work by offering\nthe first comprehensive mapping of settings, clarifying which combinations of\narchitectural choices and operational modes meaningfully impact privacy. Our\nanalysis provides actionable insight into when models are likely vulnerable,\nwhen they appear robust, and where subtle leakage may persist. Together, these\nfindings reframe how gradient inversion risk should be assessed in future\nresearch and deployment scenarios."}
{"id": "2508.19819", "categories": ["cs.CR", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.19819", "abs": "https://arxiv.org/abs/2508.19819", "authors": ["Viktor Valadi", "Mattias Åkesson", "Johan Östman", "Salman Toor", "Andreas Hellander"], "title": "From Research to Reality: Feasibility of Gradient Inversion Attacks in Federated Learning", "comment": "Under review at KDD 2026 (Research Track)", "summary": "Gradient inversion attacks have garnered attention for their ability to\ncompromise privacy in federated learning. However, many studies consider\nattacks with the model in inference mode, where training-time behaviors like\ndropout are disabled and batch normalization relies on fixed statistics. In\nthis work, we systematically analyze how architecture and training behavior\naffect vulnerability, including the first in-depth study of inference-mode\nclients, which we show dramatically simplifies inversion. To assess attack\nfeasibility under more realistic conditions, we turn to clients operating in\nstandard training mode. In this setting, we find that successful attacks are\nonly possible when several architectural conditions are met simultaneously:\nmodels must be shallow and wide, use skip connections, and, critically, employ\npre-activation normalization. We introduce two novel attacks against models in\ntraining-mode with varying attacker knowledge, achieving state-of-the-art\nperformance under realistic training conditions. We extend these efforts by\npresenting the first attack on a production-grade object-detection model. Here,\nto enable any visibly identifiable leakage, we revert to the lenient inference\nmode setting and make multiple architectural modifications to increase model\nvulnerability, with the extent of required changes highlighting the strong\ninherent robustness of such architectures. We conclude this work by offering\nthe first comprehensive mapping of settings, clarifying which combinations of\narchitectural choices and operational modes meaningfully impact privacy. Our\nanalysis provides actionable insight into when models are likely vulnerable,\nwhen they appear robust, and where subtle leakage may persist. Together, these\nfindings reframe how gradient inversion risk should be assessed in future\nresearch and deployment scenarios."}
{"id": "2508.19843", "categories": ["cs.CR", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2508.19843", "abs": "https://arxiv.org/abs/2508.19843", "authors": ["Shuo Shao", "Yiming Li", "Yu He", "Hongwei Yao", "Wenyuan Yang", "Dacheng Tao", "Zhan Qin"], "title": "SoK: Large Language Model Copyright Auditing via Fingerprinting", "comment": null, "summary": "The broad capabilities and substantial resources required to train Large\nLanguage Models (LLMs) make them valuable intellectual property, yet they\nremain vulnerable to copyright infringement, such as unauthorized use and model\ntheft. LLM fingerprinting, a non-intrusive technique that extracts and compares\nthe distinctive features from LLMs to identify infringements, offers a\npromising solution to copyright auditing. However, its reliability remains\nuncertain due to the prevalence of diverse model modifications and the lack of\nstandardized evaluation. In this SoK, we present the first comprehensive study\nof LLM fingerprinting. We introduce a unified framework and formal taxonomy\nthat categorizes existing methods into white-box and black-box approaches,\nproviding a structured overview of the state of the art. We further propose\nLeaFBench, the first systematic benchmark for evaluating LLM fingerprinting\nunder realistic deployment scenarios. Built upon mainstream foundation models\nand comprising 149 distinct model instances, LeaFBench integrates 13\nrepresentative post-development techniques, spanning both parameter-altering\nmethods (e.g., fine-tuning, quantization) and parameter-independent mechanisms\n(e.g., system prompts, RAG). Extensive experiments on LeaFBench reveal the\nstrengths and weaknesses of existing methods, thereby outlining future research\ndirections and critical open problems in this emerging field. The code is\navailable at https://github.com/shaoshuo-ss/LeaFBench."}
{"id": "2508.19825", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2508.19825", "abs": "https://arxiv.org/abs/2508.19825", "authors": ["Shaoor Munir", "Nurullah Demir", "Qian Li", "Konrad Kollnig", "Zubair Shafiq"], "title": "Every Keystroke You Make: A Tech-Law Measurement and Analysis of Event Listeners for Wiretapping", "comment": null, "summary": "The privacy community has a long track record of investigating emerging types\nof web tracking techniques. Recent work has focused on compliance of web\ntrackers with new privacy laws such as Europe's GDPR and California's CCPA.\nDespite the growing body of research documenting widespread lack of compliance\nwith new privacy laws, there is a lack of robust enforcement. Different from\nprior work, we conduct a tech-law analysis to map decades-old U.S. laws about\ninterception of electronic communications--so-called wiretapping--to web\ntracking. Bridging the tech-law gap for older wiretapping laws is important and\ntimely because, in cases where legal harm to privacy is proven, they can\nprovide statutory private right of action, are at the forefront of recent\nprivacy enforcement, and could ultimately lead to a meaningful change in the\nweb tracking landscape.\n  In this paper, we focus on a particularly invasive tracking technique: the\nuse of JavaScript event listeners by third-party trackers for real-time\nkeystroke interception on websites. We use an instrumented web browser to crawl\na sample of the top-million websites to investigate the use of event listeners\nthat aligns with the criteria for wiretapping, according to U.S. wiretapping\nlaw at the federal level and in California. We find evidence that 38.52%\nwebsites installed third-party event listeners to intercept keystrokes, and\nthat at least 3.18% websites transmitted intercepted information to a\nthird-party server, which aligns with the criteria for wiretapping. We further\nfind evidence that the intercepted information such as email addresses typed\ninto form fields are used for unsolicited email marketing. Beyond our work that\nmaps the intersection between technical measurement and U.S. wiretapping law,\nadditional future legal research is required to determine when the wiretapping\nobserved in our paper passes the threshold for illegality."}
{"id": "2508.19882", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.19882", "abs": "https://arxiv.org/abs/2508.19882", "authors": ["Qunying Song", "He Ye", "Mark Harman", "Federica Sarro"], "title": "Generative AI for Testing of Autonomous Driving Systems: A Survey", "comment": "67 pages, 6 figures, 29 tables", "summary": "Autonomous driving systems (ADS) have been an active area of research, with\nthe potential to deliver significant benefits to society. However, before\nlarge-scale deployment on public roads, extensive testing is necessary to\nvalidate their functionality and safety under diverse driving conditions.\nTherefore, different testing approaches are required, and achieving effective\nand efficient testing of ADS remains an open challenge. Recently, generative AI\nhas emerged as a powerful tool across many domains, and it is increasingly\nbeing applied to ADS testing due to its ability to interpret context, reason\nabout complex tasks, and generate diverse outputs. To gain a deeper\nunderstanding of its role in ADS testing, we systematically analyzed 91\nrelevant studies and synthesized their findings into six major application\ncategories, primarily centered on scenario-based testing of ADS. We also\nreviewed their effectiveness and compiled a wide range of datasets, simulators,\nADS, metrics, and benchmarks used for evaluation, while identifying 27\nlimitations. This survey provides an overview and practical insights into the\nuse of generative AI for testing ADS, highlights existing challenges, and\noutlines directions for future research in this rapidly evolving field."}
{"id": "2508.19843", "categories": ["cs.CR", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2508.19843", "abs": "https://arxiv.org/abs/2508.19843", "authors": ["Shuo Shao", "Yiming Li", "Yu He", "Hongwei Yao", "Wenyuan Yang", "Dacheng Tao", "Zhan Qin"], "title": "SoK: Large Language Model Copyright Auditing via Fingerprinting", "comment": null, "summary": "The broad capabilities and substantial resources required to train Large\nLanguage Models (LLMs) make them valuable intellectual property, yet they\nremain vulnerable to copyright infringement, such as unauthorized use and model\ntheft. LLM fingerprinting, a non-intrusive technique that extracts and compares\nthe distinctive features from LLMs to identify infringements, offers a\npromising solution to copyright auditing. However, its reliability remains\nuncertain due to the prevalence of diverse model modifications and the lack of\nstandardized evaluation. In this SoK, we present the first comprehensive study\nof LLM fingerprinting. We introduce a unified framework and formal taxonomy\nthat categorizes existing methods into white-box and black-box approaches,\nproviding a structured overview of the state of the art. We further propose\nLeaFBench, the first systematic benchmark for evaluating LLM fingerprinting\nunder realistic deployment scenarios. Built upon mainstream foundation models\nand comprising 149 distinct model instances, LeaFBench integrates 13\nrepresentative post-development techniques, spanning both parameter-altering\nmethods (e.g., fine-tuning, quantization) and parameter-independent mechanisms\n(e.g., system prompts, RAG). Extensive experiments on LeaFBench reveal the\nstrengths and weaknesses of existing methods, thereby outlining future research\ndirections and critical open problems in this emerging field. The code is\navailable at https://github.com/shaoshuo-ss/LeaFBench."}
{"id": "2508.20051", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2508.20051", "abs": "https://arxiv.org/abs/2508.20051", "authors": ["Prashanth Krishnamurthy", "Ramesh Karri", "Farshad Khorrami"], "title": "SCAMPER -- Synchrophasor Covert chAnnel for Malicious and Protective ERrands", "comment": "12 pages, 10 figures", "summary": "We note that constituent fields (notably the fraction-of-seconds timestamp\nfield) in the data payload structure of the synchrophasor communication\nprotocol (IEEE C37.118 standard) are overprovisioned relative to real-world\nusage and needs, lending themselves to abuse for embedding of covert channels.\nWe develop the SCAMPER (Synchrophasor Covert Channel for Malicious and\nProtective ERrands) framework to exploit these overprovisioned fields for\ncovert communication and show that SCAMPER can be applied for both malicious\n(attack) and protective (defense) purposes. Through modifications of the\ntimestamp field, we demonstrate that SCAMPER enables an attacker to accomplish\nsurreptitious communications between devices in the power system to trigger a\nvariety of malicious actions. These timestamp modifications can be performed\nwithout having any impact on the operation of the power system. However, having\nrecognized the potential for this covert channel, we show that SCAMPER can\ninstead be applied for defensive security purposes as an integrated\ncryptographic data integrity mechanism that can facilitate detection of false\ndata injection (FDI) attacks. We perform experimental studies of the proposed\nmethods on two Hardware-in-the-Loop (HIL) testbeds to demonstrate the\neffectiveness of the proposed SCAMPER framework for both malicious and\nprotective purposes."}
{"id": "2508.20083", "categories": ["cs.CR", "cs.CL"], "pdf": "https://arxiv.org/pdf/2508.20083", "abs": "https://arxiv.org/abs/2508.20083", "authors": ["Yanbo Dai", "Zhenlan Ji", "Zongjie Li", "Kuan Li", "Shuai Wang"], "title": "Disabling Self-Correction in Retrieval-Augmented Generation via Stealthy Retriever Poisoning", "comment": null, "summary": "Retrieval-Augmented Generation (RAG) has become a standard approach for\nimproving the reliability of large language models (LLMs). Prior work\ndemonstrates the vulnerability of RAG systems by misleading them into\ngenerating attacker-chosen outputs through poisoning the knowledge base.\nHowever, this paper uncovers that such attacks could be mitigated by the strong\n\\textit{self-correction ability (SCA)} of modern LLMs, which can reject false\ncontext once properly configured. This SCA poses a significant challenge for\nattackers aiming to manipulate RAG systems.\n  In contrast to previous poisoning methods, which primarily target the\nknowledge base, we introduce \\textsc{DisarmRAG}, a new poisoning paradigm that\ncompromises the retriever itself to suppress the SCA and enforce\nattacker-chosen outputs. This compromisation enables the attacker to\nstraightforwardly embed anti-SCA instructions into the context provided to the\ngenerator, thereby bypassing the SCA. To this end, we present a\ncontrastive-learning-based model editing technique that performs localized and\nstealthy edits, ensuring the retriever returns a malicious instruction only for\nspecific victim queries while preserving benign retrieval behavior. To further\nstrengthen the attack, we design an iterative co-optimization framework that\nautomatically discovers robust instructions capable of bypassing prompt-based\ndefenses. We extensively evaluate DisarmRAG across six LLMs and three QA\nbenchmarks. Our results show near-perfect retrieval of malicious instructions,\nwhich successfully suppress SCA and achieve attack success rates exceeding 90\\%\nunder diverse defensive prompts. Also, the edited retriever remains stealthy\nunder several detection methods, highlighting the urgent need for\nretriever-centric defenses."}
{"id": "2508.19461", "categories": ["cs.AI", "cs.CR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.19461", "abs": "https://arxiv.org/abs/2508.19461", "authors": ["Neil Kale", "Chen Bo Calvin Zhang", "Kevin Zhu", "Ankit Aich", "Paula Rodriguez", "Scale Red Team", "Christina Q. Knight", "Zifan Wang"], "title": "Reliable Weak-to-Strong Monitoring of LLM Agents", "comment": "18 pages, 15 figures", "summary": "We stress test monitoring systems for detecting covert misbehavior in\nautonomous LLM agents (e.g., secretly sharing private information). To this\nend, we systematize a monitor red teaming (MRT) workflow that incorporates: (1)\nvarying levels of agent and monitor situational awareness; (2) distinct\nadversarial strategies to evade the monitor, such as prompt injection; and (3)\ntwo datasets and environments -- SHADE-Arena for tool-calling agents and our\nnew CUA-SHADE-Arena, which extends TheAgentCompany, for computer-use agents. We\nrun MRT on existing LLM monitor scaffoldings, which orchestrate LLMs and parse\nagent trajectories, alongside a new hybrid hierarchical-sequential scaffolding\nproposed in this work. Our empirical results yield three key findings. First,\nagent awareness dominates monitor awareness: an agent's knowledge that it is\nbeing monitored substantially degrades the monitor's reliability. On the\ncontrary, providing the monitor with more information about the agent is less\nhelpful than expected. Second, monitor scaffolding matters more than monitor\nawareness: the hybrid scaffolding consistently outperforms baseline monitor\nscaffolding, and can enable weaker models to reliably monitor stronger agents\n-- a weak-to-strong scaling effect. Third, in a human-in-the-loop setting where\nhumans discuss with the LLM monitor to get an updated judgment for the agent's\nbehavior, targeted human oversight is most effective; escalating only\npre-flagged cases to human reviewers improved the TPR by approximately 15% at\nFPR = 0.01. Our work establishes a standard workflow for MRT, highlighting the\nlack of adversarial robustness for LLMs and humans when monitoring and\ndetecting agent misbehavior. We release code, data, and logs to spur further\nresearch."}
{"id": "2508.20086", "categories": ["cs.SE", "cs.CR"], "pdf": "https://arxiv.org/pdf/2508.20086", "abs": "https://arxiv.org/abs/2508.20086", "authors": ["Youwei Huang", "Jianwen Li", "Sen Fang", "Yao Li", "Peng Yang", "Bin Hu", "Tao Zhang"], "title": "Smart Contract Intent Detection with Pre-trained Programming Language Model", "comment": "10 pages, 5 figures, conference", "summary": "Malicious intent in smart contract development can lead to substantial\neconomic losses. SmartIntentNN is a deep learning model specifically designed\nto identify unsafe intents in smart contracts. This model integrates the\nUniversal Sentence Encoder, a K-means clustering-based intent highlighting\nmechanism, and a Bidirectional Long Short-Term Memory network for multi-label\nclassification, achieving an F1 of 0.8633 in distinguishing ten different\nintent categories. In this study, we present an upgraded version of this model,\nSmartIntentNN2 (Smart Contract Intent Neural Network V2). A significant\nenhancement in V2 is the incorporation of a BERT-based pre-trained language\nmodel, which has been trained on a dataset of 16,000 real smart contracts using\na Masked Language Modeling objective. SmartIntentNN2 retains the BiLSTM-based\nmulti-label classification network. With an improved F1 of 0.927, V2\ndemonstrates enhanced performance compared to its predecessor, establishing\nitself as the state-of-the-art model for smart contract intent detection."}
