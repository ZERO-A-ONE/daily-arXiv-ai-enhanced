<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 15]
- [cs.CR](#cs.CR) [Total: 9]
- [cs.SE](#cs.SE) [Total: 6]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [CTHA: Constrained Temporal Hierarchical Architecture for Stable Multi-Agent LLM Systems](https://arxiv.org/abs/2601.10738)
*Percy Jardine*

Main category: cs.AI

TL;DR: CTHA是一个约束性时间分层架构，通过结构化流形投影和仲裁机制解决多时间尺度智能体系统中的层间冲突和协调稳定性问题，显著提升系统性能和可扩展性。


<details>
  <summary>Details</summary>
Motivation: 多时间尺度智能体架构虽然提升了性能，但破坏了统一智能体系统的协调稳定性，导致层间冲突、错误传播无界和可扩展性受限等问题。

Method: 提出约束性时间分层架构(CTHA)，包含三个关键约束：1)消息契约约束-通过类型化摘要、计划和策略包形式化层间信息流；2)权限流形约束-根据时间范围限定各层决策空间；3)仲裁解决约束-保证多层决策的无冲突组合。

Result: 实验表明CTHA在复杂任务执行中有效，相比无约束分层基线减少了47%的故障级联，提升了2.3倍的样本效率，并表现出优越的可扩展性。

Conclusion: CTHA作为时间分层架构的原则性扩展，有助于深入理解多智能体协调，并为稳健自主系统的演进提供了有前景的方向。

Abstract: Recently, multi-time-scale agent architectures have extended the ubiquitous single-loop paradigm by introducing temporal hierarchies with distinct cognitive layers. While yielding substantial performance gains, this diversification fundamentally compromises the coordination stability intrinsic to unified agent systems, which causes severe inter-layer conflicts, unbounded error propagation, and restricted scalability. To address these challenges, we propose Constrained Temporal Hierarchical Architecture (CTHA), a general framework that projects the inter-layer communication space onto structured manifolds to restore coordination stability, while incorporating principled arbitration mechanisms to ensure coherent decision-making. Specifically, CTHA enforces three key constraints: (1) Message Contract Constraints that formalize information flow between layers via typed summary, plan, and policy packets; (2) Authority Manifold Constraints that bound each layer's decision space according to its temporal scope; and (3) Arbiter Resolution Constraints that guarantee conflict-free composition of multi-layer decisions. Empirical experiments demonstrate that CTHA is effective for complex task execution at scale, offering 47% reduction in failure cascades, 2.3x improvement in sample efficiency, and superior scalability compared to unconstrained hierarchical baselines. We anticipate that CTHA, as a principled extension of temporal hierarchies, will contribute to a deeper understanding of multi-agent coordination and suggest promising directions for the evolution of robust autonomous systems.

</details>


### [2] [Optimisation of complex product innovation processes based on trend models with three-valued logic](https://arxiv.org/abs/2601.10768)
*Nina Bočková,Barbora Volná,Mirko Dohnal*

Main category: cs.AI

TL;DR: 该论文使用基于启发式的趋势模型研究复杂产品创新过程，通过简单趋势（增加、减少、恒定）作为最小信息量化指标，避免依赖数值或粗糙集。


<details>
  <summary>Details</summary>
Motivation: 研究复杂产品创新过程需要有效的建模方法，传统方法可能过于依赖数值或复杂集合理论。作者希望开发一种最小信息强度的量化方法，能够捕捉创新过程的本质特征而不需要详细数值数据。

Method: 使用基于启发式的趋势模型，每个启发式通过简单趋势（增加、减少、恒定）表达。定义趋势模型的解决方案为一组具有可能转换关系的场景集合，用转换图表示。系统任何可能的未来或过去行为都可以通过图中的路径描绘。

Result: 开发了一种基于趋势的建模框架，能够表示复杂产品创新过程的动态行为。通过转换图的形式化表示，为分析系统行为提供了结构化的方法。

Conclusion: 基于简单趋势的启发式模型为复杂产品创新过程分析提供了有效的框架，避免了传统数值方法的复杂性，同时保持了足够的表现力来捕捉系统动态行为。

Abstract: This paper investigates complex product-innovation processes using models grounded in a set of heuristics. Each heuristic is expressed through simple trends -- increasing, decreasing, or constant -- which serve as minimally information-intensive quantifiers, avoiding reliance on numerical values or rough sets. A solution to a trend model is defined as a set of scenarios with possible transitions between them, represented by a transition graph. Any possible future or past behaviour of the system under study can thus be depicted by a path within this graph.

</details>


### [3] [ARC Prize 2025: Technical Report](https://arxiv.org/abs/2601.10904)
*François Chollet,Mike Knoop,Gregory Kamradt,Bryan Landers*

Main category: cs.AI

TL;DR: ARC-AGI-2竞赛显示AI在抽象推理任务上仍面临挑战，最高得分仅24%，但涌现出基于反馈的迭代优化方法（精炼循环），同时前沿AI实验室开始将其作为标准基准，揭示了当前AI推理仍受限于知识覆盖而非真正的泛化能力。


<details>
  <summary>Details</summary>
Motivation: ARC-AGI基准系列是衡量AI在少量样本下泛化到新任务能力的关键指标，这对于评估智能的核心方面——流体智力和抽象推理能力至关重要。随着ARC-AGI-2数据集复杂度的提升和竞赛参与度的增长，需要系统分析当前AI在抽象推理任务上的进展、局限性和未来方向。

Method: 论文通过调查ARC Prize 2025竞赛的顶级方法，特别是分析"精炼循环"这一核心主题——基于反馈信号的每任务迭代程序优化循环。这包括进化程序合成方法和商业AI系统的应用层优化。同时分析了零预训练深度学习方法，以及前沿AI实验室在模型卡中报告的ARC-AGI性能。

Result: ARC-AGI-2竞赛吸引了1,455个团队参与，最高得分仅24%，表明当前AI在复杂抽象推理任务上仍面临巨大挑战。精炼循环方法成为2025年的主导范式，零预训练深度学习方法仅用700万参数就达到了竞争性性能。四大前沿AI实验室（Anthropic、Google DeepMind、OpenAI、xAI）已将ARC-AGI作为行业标准基准，但分析显示当前AI推理性能仍受限于知识覆盖，导致新的基准污染形式。

Conclusion: 当前AI在抽象推理任务上的进展主要依赖于精炼循环方法和知识覆盖，而非真正的泛化能力。ARC-AGI-3将引入需要探索、规划、记忆、目标获取和对齐能力的交互式推理挑战，这代表了向更全面AGI评估的演进方向，同时也凸显了当前AI系统在流体智力方面的根本局限性。

Abstract: The ARC-AGI benchmark series serves as a critical measure of few-shot generalization on novel tasks, a core aspect of intelligence. The ARC Prize 2025 global competition targeted the newly released ARC-AGI-2 dataset, which features greater task complexity compared to its predecessor. The Kaggle competition attracted 1,455 teams and 15,154 entries, with the top score reaching 24% on the ARC-AGI-2 private evaluation set. Paper submissions nearly doubled year-over-year to 90 entries, reflecting the growing research interest in fluid intelligence and abstract reasoning. The defining theme of 2025 is the emergence of the refinement loop -- a per-task iterative program optimization loop guided by a feedback signal. Refinement loops come in a variety of forms, in particular evolutionary program synthesis approaches and application-layer refinements to commercial AI systems. Such refinement loops are also possible in weight space, as evidenced by zero-pretraining deep learning methods which are now achieving competitive performance with remarkably small networks (7M parameters). In parallel, four frontier AI labs (Anthropic, Google DeepMind, OpenAI, and xAI) reported ARC-AGI performance in public model cards in 2025, establishing ARC-AGI as an industry standard benchmark for AI reasoning. However, our analysis indicates that current frontier AI reasoning performance remains fundamentally constrained to knowledge coverage, giving rise to new forms of benchmark contamination. In this paper, we survey the top-performing methods, examine the role of refinement loops in AGI progress, discuss knowledge-dependent overfitting, and preview ARC-AGI-3, which introduces interactive reasoning challenges that require exploration, planning, memory, goal acquisition, and alignment capabilities.

</details>


### [4] [What Matters in Data Curation for Multimodal Reasoning? Insights from the DCVLR Challenge](https://arxiv.org/abs/2601.10922)
*Yosub Shin,Michael Buriek,Boris Sobolev,Pavel Bushuyeu,Vikas Kumar,Haoyang Xu,Samuel Watson,Igor Molybog*

Main category: cs.AI

TL;DR: 该研究通过NeurIPS 2025 DCVLR挑战赛探讨多模态推理的数据策展，发现基于难度的样本选择是性能提升的主要驱动力，而数据集大小增加主要降低方差而非提升平均准确率。


<details>
  <summary>Details</summary>
Motivation: 研究多模态推理中的数据策展问题，通过DCVLR挑战赛隔离数据集选择的影响（固定模型和训练协议），探索如何高效构建多模态推理数据集。

Method: 使用基于Walton多模态冷启动的紧凑策展数据集，在DCVLR挑战赛中提交方案。赛后进行消融实验，分析基于难度的样本选择、数据集大小、多样性和合成增强等策略的效果。

Result: 1) 基于难度的样本选择是性能提升的主要驱动力；2) 增加数据集大小不能可靠提高平均准确率，主要降低运行间方差；3) 常用的多样性和合成增强启发式方法无额外益处，甚至降低性能；4) 该方案在挑战赛中获第一名。

Conclusion: DCVLR是一个饱和状态评估，对齐和难度在多模态推理的数据效率中起核心作用。基于难度的样本选择是数据策展的关键策略。

Abstract: We study data curation for multimodal reasoning through the NeurIPS 2025 Data Curation for Vision-Language Reasoning (DCVLR) challenge, which isolates dataset selection by fixing the model and training protocol. Using a compact curated dataset derived primarily from Walton Multimodal Cold Start, our submission placed first in the challenge. Through post-competition ablations, we show that difficulty-based example selection on an aligned base dataset is the dominant driver of performance gains. Increasing dataset size does not reliably improve mean accuracy under the fixed training recipe, but mainly reduces run-to-run variance, while commonly used diversity and synthetic augmentation heuristics provide no additional benefit and often degrade performance. These results characterize DCVLR as a saturation-regime evaluation and highlight the central role of alignment and difficulty in data-efficient multimodal reasoning.

</details>


### [5] [MiCA: A Mobility-Informed Causal Adapter for Lightweight Epidemic Forecasting](https://arxiv.org/abs/2601.11089)
*Suhan Guo,Jiahong Deng,Furao Shen*

Main category: cs.AI

TL;DR: MiCA是一个轻量级的流行病预测模块，通过因果发现推断移动关系，并通过门控残差混合将其整合到时序预测模型中，在数据有限和噪声条件下提升预测性能。


<details>
  <summary>Details</summary>
Motivation: 人类移动性在流行病空间传播中起关键作用，但移动数据通常噪声大、间接且难以与疾病记录可靠整合。同时，流行病病例时间序列通常较短且时间分辨率粗糙，这限制了依赖干净丰富数据的参数密集型移动感知预测器的有效性。

Method: 提出Mobility-Informed Causal Adapter (MiCA)，一个轻量级且架构无关的模块。通过因果发现推断移动关系，并通过门控残差混合将其整合到时序预测模型中，使轻量级预测器能够选择性地利用移动性衍生的空间结构。

Result: 在四个真实世界流行病数据集（COVID-19发病率、COVID-19死亡率、流感和登革热）上的广泛实验表明，MiCA持续改进轻量级时序骨干模型，在预测时间范围内平均相对误差降低7.5%。MiCA在保持轻量级的同时，性能可与最先进的时空模型竞争。

Conclusion: MiCA是一个有效的轻量级适配器模块，能够在数据有限和噪声条件下，通过因果发现的移动关系增强流行病预测，为公共卫生规划和干预提供更可靠的预测工具。

Abstract: Accurate forecasting of infectious disease dynamics is critical for public health planning and intervention. Human mobility plays a central role in shaping the spatial spread of epidemics, but mobility data are noisy, indirect, and difficult to integrate reliably with disease records. Meanwhile, epidemic case time series are typically short and reported at coarse temporal resolution. These conditions limit the effectiveness of parameter-heavy mobility-aware forecasters that rely on clean and abundant data. In this work, we propose the Mobility-Informed Causal Adapter (MiCA), a lightweight and architecture-agnostic module for epidemic forecasting. MiCA infers mobility relations through causal discovery and integrates them into temporal forecasting models via gated residual mixing. This design allows lightweight forecasters to selectively exploit mobility-derived spatial structure while remaining robust under noisy and data-limited conditions, without introducing heavy relational components such as graph neural networks or full attention. Extensive experiments on four real-world epidemic datasets, including COVID-19 incidence, COVID-19 mortality, influenza, and dengue, show that MiCA consistently improves lightweight temporal backbones, achieving an average relative error reduction of 7.5\% across forecasting horizons. Moreover, MiCA attains performance competitive with SOTA spatio-temporal models while remaining lightweight.

</details>


### [6] [ReCreate: Reasoning and Creating Domain Agents Driven by Experience](https://arxiv.org/abs/2601.11100)
*Zhezheng Hao,Hong Wang,Jian Luo,Jianqing Zhang,Yuyan Zhou,Qiang Lin,Can Wang,Hande Dong,Jiawei Chen*

Main category: cs.AI

TL;DR: ReCreate是一个经验驱动的自动创建领域智能体框架，通过智能体交互历史学习成功与失败原因，实现自动优化和适应


<details>
  <summary>Details</summary>
Motivation: 当前大多数实用智能体仍需要人工设计，任务差异大导致构建成本高；现有自动化方法将智能体生成视为黑盒过程，仅依赖最终性能指标，忽略了成功/失败的关键证据且计算成本高

Method: 提出智能体即优化器范式，包含三个核心组件：1) 经验存储和检索机制用于按需检查；2) 推理-创建协同管道将执行经验映射为脚手架编辑；3) 分层更新将实例级细节抽象为可重用领域模式

Result: 在多个不同领域的实验中，ReCreate始终优于人工设计的智能体和现有自动化智能体生成方法，即使从最小种子脚手架开始也能取得良好效果

Conclusion: ReCreate通过系统利用智能体交互历史中的具体信号，实现了自动创建和适应领域智能体的目标，为解决智能体自动生成问题提供了有效方案

Abstract: Large Language Model agents are reshaping the industrial landscape. However, most practical agents remain human-designed because tasks differ widely, making them labor-intensive to build. This situation poses a central question: can we automatically create and adapt domain agents in the wild? While several recent approaches have sought to automate agent creation, they typically treat agent generation as a black-box procedure and rely solely on final performance metrics to guide the process. Such strategies overlook critical evidence explaining why an agent succeeds or fails, and often require high computational costs. To address these limitations, we propose ReCreate, an experience-driven framework for the automatic creation of domain agents. ReCreate systematically leverages agent interaction histories, which provide rich concrete signals on both the causes of success or failure and the avenues for improvement. Specifically, we introduce an agent-as-optimizer paradigm that effectively learns from experience via three key components: (i) an experience storage and retrieval mechanism for on-demand inspection; (ii) a reasoning-creating synergy pipeline that maps execution experience into scaffold edits; and (iii) hierarchical updates that abstract instance-level details into reusable domain patterns. In experiments across diverse domains, ReCreate consistently outperforms human-designed agents and existing automated agent generation methods, even when starting from minimal seed scaffolds.

</details>


### [7] [Do We Always Need Query-Level Workflows? Rethinking Agentic Workflow Generation for Multi-Agent Systems](https://arxiv.org/abs/2601.11147)
*Zixu Wang,Bingbing Xu,Yige Yuan,Huawei Shen,Xueqi Cheng*

Main category: cs.AI

TL;DR: SCALE框架通过任务级工作流生成和自预测评估，在保持性能的同时大幅降低token使用量


<details>
  <summary>Details</summary>
Motivation: 现有多智能体系统工作流生成方法在任务级和查询级各有优劣，但相对成本和效益不明确，且基于执行的评估方法token成本高且不可靠

Method: 提出SCALE框架：通过少量样本校准的自预测优化器进行任务级工作流生成，替代昂贵的全验证执行评估

Result: SCALE在多个数据集上仅平均性能下降0.61%，同时将总体token使用量降低高达83%

Conclusion: 查询级工作流生成并非总是必要，任务级工作流结合低成本评估方法可以在保持性能的同时显著降低计算成本

Abstract: Multi-Agent Systems (MAS) built on large language models typically solve complex tasks by coordinating multiple agents through workflows. Existing approaches generates workflows either at task level or query level, but their relative costs and benefits remain unclear. After rethinking and empirical analyses, we show that query-level workflow generation is not always necessary, since a small set of top-K best task-level workflows together already covers equivalent or even more queries. We further find that exhaustive execution-based task-level evaluation is both extremely token-costly and frequently unreliable. Inspired by the idea of self-evolution and generative reward modeling, we propose a low-cost task-level generation framework \textbf{SCALE}, which means \underline{\textbf{S}}elf prediction of the optimizer with few shot \underline{\textbf{CAL}}ibration for \underline{\textbf{E}}valuation instead of full validation execution. Extensive experiments demonstrate that \textbf{SCALE} maintains competitive performance, with an average degradation of just 0.61\% compared to existing approach across multiple datasets, while cutting overall token usage by up to 83\%.

</details>


### [8] [TANDEM: Temporal-Aware Neural Detection for Multimodal Hate Speech](https://arxiv.org/abs/2601.11178)
*Girish A. Koushik,Helen Treharne,Diptesh Kanojia*

Main category: cs.AI

TL;DR: TANDEM是一个统一框架，将视听仇恨检测从二元分类任务转化为结构化推理问题，通过跨模态强化学习实现精确的时间定位和目标识别，显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 社交媒体中长格式多模态内容日益增多，有害叙事通过音频、视觉和文本的复杂交互构建。现有自动化系统虽然能高精度标记仇恨言论，但往往作为"黑箱"运行，无法提供人类参与审核所需的细粒度、可解释证据（如精确时间戳和目标身份）。

Method: 提出TANDEM框架，采用新颖的串联强化学习策略，其中视觉-语言和音频-语言模型通过自约束的跨模态上下文相互优化，在不需要密集帧级监督的情况下稳定处理长时序序列的推理。

Result: 在三个基准数据集上的实验表明，TANDEM显著优于零样本和上下文增强基线，在HateMM数据集上目标识别F1达到0.73（比最先进方法提升30%），同时保持精确的时间定位。二元检测稳健，但在多类别设置中区分冒犯性和仇恨内容仍具挑战性。

Conclusion: 即使在复杂多模态环境中，结构化、可解释的对齐也是可实现的，为下一代透明且可操作的在线安全审核工具提供了蓝图。

Abstract: Social media platforms are increasingly dominated by long-form multimodal content, where harmful narratives are constructed through a complex interplay of audio, visual, and textual cues. While automated systems can flag hate speech with high accuracy, they often function as "black boxes" that fail to provide the granular, interpretable evidence, such as precise timestamps and target identities, required for effective human-in-the-loop moderation. In this work, we introduce TANDEM, a unified framework that transforms audio-visual hate detection from a binary classification task into a structured reasoning problem. Our approach employs a novel tandem reinforcement learning strategy where vision-language and audio-language models optimize each other through self-constrained cross-modal context, stabilizing reasoning over extended temporal sequences without requiring dense frame-level supervision. Experiments across three benchmark datasets demonstrate that TANDEM significantly outperforms zero-shot and context-augmented baselines, achieving 0.73 F1 in target identification on HateMM (a 30% improvement over state-of-the-art) while maintaining precise temporal grounding. We further observe that while binary detection is robust, differentiating between offensive and hateful content remains challenging in multi-class settings due to inherent label ambiguity and dataset imbalance. More broadly, our findings suggest that structured, interpretable alignment is achievable even in complex multimodal settings, offering a blueprint for the next generation of transparent and actionable online safety moderation tools.

</details>


### [9] [Beyond Model Scaling: Test-Time Intervention for Efficient Deep Reasoning](https://arxiv.org/abs/2601.11252)
*Qianyue Wang,Jinwu Hu,Yufeng Wang,Huanxiang Lin,Bolin Chen,Zhiquan Wen,Yaofo Chen,Mingkui Tan*

Main category: cs.AI

TL;DR: Think-with-Me是一种新颖的测试时交互式推理范式，通过引入外部反馈干预来优化大型推理模型的推理过程，在保持准确性的同时显著减少推理长度。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型在多步推理中表现出色，但存在过度思考和推理偏差等问题，导致计算成本增加和性能下降。现有高效推理方法缺乏外部干预机制来引导推理过程。

Method: 提出Think-with-Me范式，利用过渡连词作为干预点，在推理过程中暂停以获取外部反馈。采用多标准评估（合理性和完整性）生成反馈，通过Group Relative Policy Optimization训练模型适应交互模式。

Result: 在AIME24上，Think-with-Me在8K窗口下比QwQ-32B准确率提高7.19%，同时平均推理长度减少81%。该范式在有限上下文窗口中实现了准确性和推理长度的优越平衡。

Conclusion: Think-with-Me通过引入外部反馈干预，有效解决了大型推理模型的低效推理问题，在保持准确性的同时显著优化了推理效率，对安全和创意任务也有益处。

Abstract: Large Reasoning Models (LRMs) excel at multi-step reasoning but often suffer from inefficient reasoning processes like overthinking and overshoot, where excessive or misdirected reasoning increases computational cost and degrades performance. Existing efficient reasoning methods operate in a closed-loop manner, lacking mechanisms for external intervention to guide the reasoning process. To address this, we propose Think-with-Me, a novel test-time interactive reasoning paradigm that introduces external feedback intervention into the reasoning process. Our key insights are that transitional conjunctions serve as natural points for intervention, signaling phases of self-validation or exploration and using transitional words appropriately to prolong the reasoning enhances performance, while excessive use affects performance. Building on these insights, Think-with-Me pauses reasoning at these points for external feedback, adaptively extending or terminating reasoning to reduce redundancy while preserving accuracy. The feedback is generated via a multi-criteria evaluation (rationality and completeness) and comes from either human or LLM proxies. We train the target model using Group Relative Policy Optimization (GRPO) to adapt to this interactive mode. Experiments show that Think-with-Me achieves a superior balance between accuracy and reasoning length under limited context windows. On AIME24, Think-with-Me outperforms QwQ-32B by 7.19% in accuracy while reducing average reasoning length by 81% under an 8K window. The paradigm also benefits security and creative tasks.

</details>


### [10] [XChoice: Explainable Evaluation of AI-Human Alignment in LLM-based Constrained Choice Decision Making](https://arxiv.org/abs/2601.11286)
*Weihong Qi,Fan Huang,Rasika Muralidharan,Jisun An,Haewoon Kwak*

Main category: cs.AI

TL;DR: XChoice是一个可解释的框架，用于评估约束决策中AI与人类的对齐程度，超越传统准确率指标，通过机制建模分析决策因素权重、约束敏感性和权衡关系。


<details>
  <summary>Details</summary>
Motivation: 当前AI评估主要关注结果一致性（如准确率、F1分数），但缺乏对决策机制的深入理解。需要开发能够揭示AI与人类在决策因素权重、约束敏感性和权衡关系方面对齐程度的框架。

Method: XChoice框架为人类数据和LLM生成的决策拟合基于机制的决策模型，恢复可解释参数（决策因素相对重要性、约束敏感性、隐含权衡）。通过比较这些参数向量来评估对齐程度，使用美国时间使用调查（ATUS）作为人类基准，并进行鲁棒性验证和RAG干预评估。

Result: 在美国家庭日常时间分配任务中，XChoice揭示了模型和活动之间的异质对齐，发现黑人和已婚群体的显著不对齐。鲁棒性分析验证了框架的有效性，RAG干预展示了针对性缓解的可能性。

Conclusion: XChoice提供了基于机制的度量标准，能够诊断AI与人类的不对齐问题，并支持超越表面结果匹配的知情改进，为AI对齐评估提供了更深入的分析工具。

Abstract: We present XChoice, an explainable framework for evaluating AI-human alignment in constrained decision making. Moving beyond outcome agreement such as accuracy and F1 score, XChoice fits a mechanism-based decision model to human data and LLM-generated decisions, recovering interpretable parameters that capture the relative importance of decision factors, constraint sensitivity, and implied trade-offs. Alignment is assessed by comparing these parameter vectors across models, options, and subgroups. We demonstrate XChoice on Americans' daily time allocation using the American Time Use Survey (ATUS) as human ground truth, revealing heterogeneous alignment across models and activities and salient misalignment concentrated in Black and married groups. We further validate robustness of XChoice via an invariance analysis and evaluate targeted mitigation with a retrieval augmented generation (RAG) intervention. Overall, XChoice provides mechanism-based metrics that diagnose misalignment and support informed improvements beyond surface outcome matching.

</details>


### [11] [AstroReason-Bench: Evaluating Unified Agentic Planning across Heterogeneous Space Planning Problems](https://arxiv.org/abs/2601.11354)
*Weiyi Wang,Xinchi Chen,Jingjing Gong,Xuanjing Huang,Xipeng Qiu*

Main category: cs.AI

TL;DR: AstroReason-Bench是一个用于评估智能体在空间规划问题中规划能力的基准测试，该基准整合了多种调度机制，发现当前智能体在物理约束下的表现远不如专用求解器。


<details>
  <summary>Details</summary>
Motivation: 现有智能体基准主要关注符号化或弱接地环境，缺乏对物理约束现实世界领域中智能体规划能力的评估，特别是在空间规划这种高风险、多目标、严格物理约束和长时程决策的问题上。

Method: 引入AstroReason-Bench基准，整合地面站通信和敏捷地球观测等多种调度机制，提供统一的智能体导向交互协议，并在多种最先进的开源和闭源智能体LLM系统上进行评估。

Result: 评估发现当前智能体在空间规划问题上的表现显著低于专用求解器，揭示了通用规划器在现实约束下的关键局限性。

Conclusion: AstroReason-Bench为未来智能体研究提供了一个具有挑战性和诊断性的测试平台，有助于推动智能体在物理约束现实世界领域中的规划能力发展。

Abstract: Recent advances in agentic Large Language Models (LLMs) have positioned them as generalist planners capable of reasoning and acting across diverse tasks. However, existing agent benchmarks largely focus on symbolic or weakly grounded environments, leaving their performance in physics-constrained real-world domains underexplored. We introduce AstroReason-Bench, a comprehensive benchmark for evaluating agentic planning in Space Planning Problems (SPP), a family of high-stakes problems with heterogeneous objectives, strict physical constraints, and long-horizon decision-making. AstroReason-Bench integrates multiple scheduling regimes, including ground station communication and agile Earth observation, and provides a unified agent-oriented interaction protocol. Evaluating on a range of state-of-the-art open- and closed-source agentic LLM systems, we find that current agents substantially underperform specialized solvers, highlighting key limitations of generalist planning under realistic constraints. AstroReason-Bench offers a challenging and diagnostic testbed for future agentic research.

</details>


### [12] [Hyperparameter Optimization of Constraint Programming Solvers](https://arxiv.org/abs/2601.11389)
*Hedieh Haddad,Thibault Falque,Pierre Talbot,Pascal Bouvry*

Main category: cs.AI

TL;DR: 本文提出了一种用于约束规划求解器超参数优化的两阶段框架"探测与求解算法"，该算法将时间预算分为探测阶段（探索超参数配置）和求解阶段（使用最佳配置解决问题），在CPMpy库中实现并评估了贝叶斯优化和汉明距离搜索两种方法。


<details>
  <summary>Details</summary>
Motivation: 约束规划求解器的性能对超参数选择高度敏感，手动寻找最佳配置需要专家知识且耗时费力，因此需要自动化的超参数优化方法来提高求解器性能。

Method: 提出两阶段框架"探测与求解算法"：1) 探测阶段使用可配置的超参数优化方法（贝叶斯优化和汉明距离搜索）探索不同超参数集；2) 求解阶段使用找到的最佳配置在剩余时间内解决问题。该框架集成到CPMpy库中。

Result: 在114个组合问题实例上评估ACE和Choco两个求解器：使用贝叶斯优化时，算法在25.4%的ACE实例中优于默认配置，57.9%中性能相当；在38.6%的Choco实例中取得更好结果。贝叶斯优化始终优于汉明距离搜索，证明了基于模型的探索优于简单局部搜索。

Conclusion: 探测与求解算法提供了一种实用、资源感知的约束求解器调优方法，能够在多种问题类型上实现稳健的性能提升，特别是贝叶斯优化方法在超参数优化中表现出色。

Abstract: The performance of constraint programming solvers is highly sensitive to the choice of their hyperparameters. Manually finding the best solver configuration is a difficult, time-consuming task that typically requires expert knowledge. In this paper, we introduce probe and solve algorithm, a novel two-phase framework for automated hyperparameter optimization integrated into the CPMpy library. This approach partitions the available time budget into two phases: a probing phase that explores different sets of hyperparameters using configurable hyperparameter optimization methods, followed by a solving phase where the best configuration found is used to tackle the problem within the remaining time.
  We implement and compare two hyperparameter optimization methods within the probe and solve algorithm: Bayesian optimization and Hamming distance search. We evaluate the algorithm on two different constraint programming solvers, ACE and Choco, across 114 combinatorial problem instances, comparing their performance against the solver's default configurations.
  Results show that using Bayesian optimization, the algorithm outperforms the solver's default configurations, improving solution quality for ACE in 25.4% of instances and matching the default performance in 57.9%, and for Choco, achieving superior results in 38.6% of instances. It also consistently surpasses Hamming distance search within the same framework, confirming the advantage of model-based exploration over simple local search. Overall, the probe and solve algorithm offers a practical, resource-aware approach for tuning constraint solvers that yields robust improvements across diverse problem types.

</details>


### [13] [Exploring LLM Features in Predictive Process Monitoring for Small-Scale Event-Logs](https://arxiv.org/abs/2601.11468)
*Alessandro Padella,Massimiliano de Leoni,Marlon Dumas*

Main category: cs.AI

TL;DR: 本文扩展了基于LLM的预测性流程监控框架，从仅预测总时间扩展到评估其通用性、语义利用和推理机制，并在多个KPI上进行测试，发现在数据稀缺情况下LLM优于基准方法。


<details>
  <summary>Details</summary>
Motivation: 预测性流程监控旨在预测进行中流程的结果，虽然已有机器学习方法，但作者希望扩展其先前基于LLM的框架，全面评估其在多个关键绩效指标上的表现，特别是在数据稀缺情况下的性能。

Method: 扩展了先前基于LLM的预测性流程监控框架，通过提示机制进行预测，并在三个不同的事件日志上进行实证评估，测试总时间和活动发生两个关键绩效指标，特别关注数据稀缺（仅100条轨迹）的情况。

Result: 在数据稀缺设置下（仅100条轨迹），LLM超越了基准方法。实验还表明LLM利用了其先验知识和训练轨迹间的内部相关性。分析显示LLM不仅复制现有预测方法，而是执行高阶推理来生成预测。

Conclusion: 基于LLM的预测性流程监控框架在数据稀缺情况下表现出色，能够利用语义知识和进行高阶推理，为预测性流程监控提供了有前景的新方法。

Abstract: Predictive Process Monitoring is a branch of process mining that aims to predict the outcome of an ongoing process. Recently, it leveraged machine-and-deep learning architectures. In this paper, we extend our prior LLM-based Predictive Process Monitoring framework, which was initially focused on total time prediction via prompting. The extension consists of comprehensively evaluating its generality, semantic leverage, and reasoning mechanisms, also across multiple Key Performance Indicators. Empirical evaluations conducted on three distinct event logs and across the Key Performance Indicators of Total Time and Activity Occurrence prediction indicate that, in data-scarce settings with only 100 traces, the LLM surpasses the benchmark methods. Furthermore, the experiments also show that the LLM exploits both its embodied prior knowledge and the internal correlations among training traces. Finally, we examine the reasoning strategies employed by the model, demonstrating that the LLM does not merely replicate existing predictive methods but performs higher-order reasoning to generate the predictions.

</details>


### [14] [Health Facility Location in Ethiopia: Leveraging LLMs to Integrate Expert Knowledge into Algorithmic Planning](https://arxiv.org/abs/2601.11479)
*Yohai Trabelsi,Guojun Xiong,Fentabil Getnet,Stéphane Verguet,Milind Tambe*

Main category: cs.AI

TL;DR: 提出LEG混合框架，结合优化算法与LLM，在资源有限下优化埃塞俄比亚卫生站升级决策，平衡人口覆盖与专家偏好


<details>
  <summary>Details</summary>
Motivation: 埃塞俄比亚卫生部升级卫生站以改善基本医疗服务可及性，但资源有限需要优先考虑哪些设施升级，同时需要兼顾专家和利益相关者的多样化偏好

Method: 提出LEG（大语言模型与扩展贪心）混合框架：结合具有理论保证的覆盖优化近似算法与LLM驱动的迭代优化，通过人机对齐确保解决方案反映专家定性指导同时保持覆盖保证

Result: 在埃塞俄比亚三个地区的真实数据实验表明该框架有效，能够为公平、数据驱动的卫生系统规划提供信息

Conclusion: LEG框架成功桥接了经典优化方法与专家定性指导之间的鸿沟，为资源受限的卫生系统规划提供了实用解决方案

Abstract: Ethiopia's Ministry of Health is upgrading health posts to improve access to essential services, particularly in rural areas. Limited resources, however, require careful prioritization of which facilities to upgrade to maximize population coverage while accounting for diverse expert and stakeholder preferences. In collaboration with the Ethiopian Public Health Institute and Ministry of Health, we propose a hybrid framework that systematically integrates expert knowledge with optimization techniques. Classical optimization methods provide theoretical guarantees but require explicit, quantitative objectives, whereas stakeholder criteria are often articulated in natural language and difficult to formalize. To bridge these domains, we develop the Large language model and Extended Greedy (LEG) framework. Our framework combines a provable approximation algorithm for population coverage optimization with LLM-driven iterative refinement that incorporates human-AI alignment to ensure solutions reflect expert qualitative guidance while preserving coverage guarantees. Experiments on real-world data from three Ethiopian regions demonstrate the framework's effectiveness and its potential to inform equitable, data-driven health system planning.

</details>


### [15] [BoxMind: Closed-loop AI strategy optimization for elite boxing validated in the 2024 Olympics](https://arxiv.org/abs/2601.11492)
*Kaiwen Wang,Kaili Zheng,Rongrong Deng,Qingmin Fan,Milin Zhang,Zongrui Li,Xuesi Zhou,Bo Han,Liren Chen,Chenyi Guo,Ji Wu*

Main category: cs.AI

TL;DR: BoxMind是一个用于拳击战术分析的闭环AI专家系统，通过定义原子击打事件和分层技术战术指标，结合图预测模型和可学习潜嵌入来预测比赛结果并生成战术建议，在2024年巴黎奥运会上验证有效。


<details>
  <summary>Details</summary>
Motivation: 格斗类运动如拳击在AI驱动分析方面发展不足，主要因为动作动态复杂且缺乏结构化战术表示。需要将非结构化视频数据转化为战略智能，弥合计算机视觉与竞技体育决策支持之间的差距。

Method: 1. 定义具有精确时间边界、空间和技术属性的原子击打事件；2. 将比赛视频解析为18个分层技术战术指标；3. 提出基于图的预测模型，融合显性技术战术特征与可学习的时间变化潜嵌入；4. 将比赛结果建模为技术战术指标的可微函数，将获胜概率梯度转化为可执行的战术调整。

Result: 1. 结果预测模型在BoxerGraph测试集上达到69.8%准确率，在奥运比赛上达到87.5%准确率；2. 系统生成的战略建议达到与人类专家相当的水平；3. 在2024年巴黎奥运会闭环部署中，直接助力中国国家队获得3金2银的历史性成绩。

Conclusion: BoxMind建立了一个可复制的范式，将非结构化视频数据转化为战略智能，弥合了计算机视觉与竞技体育决策支持之间的差距，为格斗类运动的战术分析提供了有效的AI解决方案。

Abstract: Competitive sports require sophisticated tactical analysis, yet combat disciplines like boxing remain underdeveloped in AI-driven analytics due to the complexity of action dynamics and the lack of structured tactical representations. To address this, we present BoxMind, a closed-loop AI expert system validated in elite boxing competition. By defining atomic punch events with precise temporal boundaries and spatial and technical attributes, we parse match footage into 18 hierarchical technical-tactical indicators. We then propose a graph-based predictive model that fuses these explicit technical-tactical profiles with learnable, time-variant latent embeddings to capture the dynamics of boxer matchups. Modeling match outcome as a differentiable function of technical-tactical indicators, we turn winning probability gradients into executable tactical adjustments. Experiments show that the outcome prediction model achieves state-of-the-art performance, with 69.8% accuracy on BoxerGraph test set and 87.5% on Olympic matches. Using this predictive model as a foundation, the system generates strategic recommendations that demonstrate proficiency comparable to human experts. BoxMind is validated through a closed-loop deployment during the 2024 Paris Olympics, directly contributing to the Chinese National Team's historic achievement of three gold and two silver medals. BoxMind establishes a replicable paradigm for transforming unstructured video data into strategic intelligence, bridging the gap between computer vision and decision support in competitive sports.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [16] [Chatting with Confidants or Corporations? Privacy Management with AI Companions](https://arxiv.org/abs/2601.10754)
*Hsuen-Chi Chiu,Jeremy Foote*

Main category: cs.CR

TL;DR: 研究探讨AI情感伴侣聊天机器人的隐私管理，发现用户混合了人际亲密习惯与机构隐私意识，通过分层策略管理隐私，但对平台数据控制感到无力。


<details>
  <summary>Details</summary>
Motivation: AI情感伴侣模糊了人际亲密与机构软件的界限，创造了复杂的多维隐私环境，需要理解用户在这种新型关系中的隐私管理策略。

Method: 基于沟通隐私管理理论和Masur的水平（用户-AI）与垂直（用户-平台）隐私框架，对15名Replika和Character.AI等伴侣AI平台用户进行深度访谈。

Result: 用户混合了人际习惯与机构意识：聊天机器人的非评判性和随时可用性促进情感安全和自我披露，但用户仍意识到机构风险，通过分层策略和选择性分享管理隐私。然而许多人对平台级数据控制感到不确定或无力。拟人化设计进一步模糊隐私边界，有时导致无意过度分享和隐私动荡。

Conclusion: 研究通过强调人机伴侣关系中情感与机构隐私管理的独特相互作用，扩展了隐私理论。

Abstract: AI chatbots designed as emotional companions blur the boundaries between interpersonal intimacy and institutional software, creating a complex, multi-dimensional privacy environment. Drawing on Communication Privacy Management theory and Masur's horizontal (user-AI) and vertical (user-platform) privacy framework, we conducted in-depth interviews with fifteen users of companion AI platforms such as Replika and Character.AI. Our findings reveal that users blend interpersonal habits with institutional awareness: while the non-judgmental, always-available nature of chatbots fosters emotional safety and encourages self-disclosure, users remain mindful of institutional risks and actively manage privacy through layered strategies and selective sharing. Despite this, many feel uncertain or powerless regarding platform-level data control. Anthropomorphic design further blurs privacy boundaries, sometimes leading to unintentional oversharing and privacy turbulence. These results extend privacy theory by highlighting the unique interplay of emotional and institutional privacy management in human-AI companionship.

</details>


### [17] [SecMLOps: A Comprehensive Framework for Integrating Security Throughout the MLOps Lifecycle](https://arxiv.org/abs/2601.10848)
*Xinrui Zhang,Pincan Zhao,Jason Jaskolka,Heng Li,Rongxing Lu*

Main category: cs.CR

TL;DR: 本文提出了SecMLOps框架，将安全措施整合到整个MLOps生命周期中，以应对机器学习部署中的安全挑战，特别是对抗性攻击。通过行人检测系统案例展示了该框架的实际应用，并分析了安全措施与系统性能之间的权衡。


<details>
  <summary>Details</summary>
Motivation: 机器学习在关键系统中的应用日益广泛，但部署过程中面临严重的安全挑战，特别是对抗性攻击会威胁系统的完整性和可靠性。现有的MLOps流程缺乏足够的安全考虑，需要将安全措施系统性地整合到整个ML生命周期中。

Method: 提出SecMLOps框架，基于MLOps原则，从设计阶段到部署和持续监控的整个生命周期中嵌入安全考虑。该框架专门针对MLOps生命周期各阶段的复杂攻击提供保护，并通过一个高级行人检测系统的具体案例来展示实际应用。

Result: 通过广泛的实证评估，揭示了安全措施与系统性能之间的权衡关系，提供了在不严重影响操作效率的情况下优化安全性的关键见解。研究表明平衡方法的重要性，为实践者提供了在不同领域ML部署中实现安全与性能最优平衡的指导。

Conclusion: SecMLOps框架能够有效增强ML应用的弹性和可信度，通过将安全措施整合到整个MLOps生命周期中，为应对机器学习部署中的安全挑战提供了系统性的解决方案。平衡安全与性能的方法对于实际部署至关重要。

Abstract: Machine Learning (ML) has emerged as a pivotal technology in the operation of large and complex systems, driving advancements in fields such as autonomous vehicles, healthcare diagnostics, and financial fraud detection. Despite its benefits, the deployment of ML models brings significant security challenges, such as adversarial attacks, which can compromise the integrity and reliability of these systems. To address these challenges, this paper builds upon the concept of Secure Machine Learning Operations (SecMLOps), providing a comprehensive framework designed to integrate robust security measures throughout the entire ML operations (MLOps) lifecycle. SecMLOps builds on the principles of MLOps by embedding security considerations from the initial design phase through to deployment and continuous monitoring. This framework is particularly focused on safeguarding against sophisticated attacks that target various stages of the MLOps lifecycle, thereby enhancing the resilience and trustworthiness of ML applications. A detailed advanced pedestrian detection system (PDS) use case demonstrates the practical application of SecMLOps in securing critical MLOps. Through extensive empirical evaluations, we highlight the trade-offs between security measures and system performance, providing critical insights into optimizing security without unduly impacting operational efficiency. Our findings underscore the importance of a balanced approach, offering valuable guidance for practitioners on how to achieve an optimal balance between security and performance in ML deployments across various domains.

</details>


### [18] [Multi-Agent Taint Specification Extraction for Vulnerability Detection](https://arxiv.org/abs/2601.10865)
*Jonah Ghebremichael,Saastha Vasan,Saad Ullah,Greg Tystahl,David Adei,Christopher Kruegel,Giovanni Vigna,William Enck,Alexandros Kapravelos*

Main category: cs.CR

TL;DR: SemTaint是一个结合大型语言模型语义理解和传统静态程序分析的多智能体系统，用于提取JavaScript应用的污点规范，显著提升了CodeQL等SAST工具检测npm包漏洞的能力。


<details>
  <summary>Details</summary>
Motivation: JavaScript静态污点分析面临两大挑战：1) JavaScript动态特性使数据流提取复杂化；2) npm庞大库生态系统难以识别相关源/汇并建立跨依赖污点传播。现有SAST工具在检测JavaScript漏洞方面存在局限性。

Method: SemTaint采用多智能体系统，结合LLM语义理解和传统静态分析：1) 使用静态分析计算调用图，LLM解析无法静态解析的调用边；2) LLM分类给定CWE的源和汇；3) 生成针对每个包的污点规范（源、汇、调用边、库流摘要）；4) 将规范提供给SAST工具进行漏洞分析。

Result: 集成CodeQL后，SemTaint检测到162个之前CodeQL无法检测的漏洞中的106个（65.4%），并在4个流行npm包中发现4个新漏洞。证明LLM能实际增强现有静态程序分析算法。

Conclusion: LLM可以实际增强现有静态程序分析算法，结合符号推理和语义理解的优势，显著改进漏洞检测能力。SemTaint展示了这种结合在解决JavaScript污点分析挑战方面的有效性。

Abstract: Static Application Security Testing (SAST) tools using taint analysis are widely viewed as providing higher-quality vulnerability detection results compared to traditional pattern-based approaches. However, performing static taint analysis for JavaScript poses two major challenges. First, JavaScript's dynamic features complicate data flow extraction required for taint tracking. Second, npm's large library ecosystem makes it difficult to identify relevant sources/sinks and establish taint propagation across dependencies. In this paper, we present SemTaint, a multi-agent system that strategically combines the semantic understanding of Large Language Models (LLMs) with traditional static program analysis to extract taint specifications, including sources, sinks, call edges, and library flow summaries tailored to each package. Conceptually, SemTaint uses static program analysis to calculate a call graph and defers to an LLM to resolve call edges that cannot be resolved statically. Further, it uses the LLM to classify sources and sinks for a given CWE. The resulting taint specification is then provided to a SAST tool, which performs vulnerability analysis. We integrate SemTaint with CodeQL, a state-of-the-art SAST tool, and demonstrate its effectiveness by detecting 106 of 162 vulnerabilities previously undetectable by CodeQL. Furthermore, we find 4 novel vulnerabilities in 4 popular npm packages. In doing so, we demonstrate that LLMs can practically enhance existing static program analysis algorithms, combining the strengths of both symbolic reasoning and semantic understanding for improved vulnerability detection.

</details>


### [19] [Adaptive Privacy Budgeting](https://arxiv.org/abs/2601.10866)
*Yuting Liang,Ke Yi*

Main category: cs.CR

TL;DR: 本文提出了一种广义差分隐私下的自适应隐私预算分配框架，通过根据先前查询结果动态调整用户隐私预算，实现隐私资源的高效利用。


<details>
  <summary>Details</summary>
Motivation: 在广义差分隐私设置中，用户数据的不同组件对查询的重要性不同，传统固定隐私预算分配效率低下。需要一种能够根据查询结果动态调整预算的机制，在典型实例上实现更大的隐私节省，从而提高后续查询的效用。

Method: 提出了一个自适应预算分配框架，允许分析师根据先前查询的输出结果动态调整用户隐私预算。该框架针对用户数据的不同组件（l∈[T]）进行预算分配，使得在数据重要性不同的情况下能够更有效地使用隐私资源。

Result: 开发的自适应预算框架能够根据查询结果动态调整隐私预算分配，在典型实例上实现更大的隐私节省。通过多个应用案例证明了该框架的适用性和有效性。

Conclusion: 提出的自适应隐私预算分配框架解决了广义差分隐私下的预算优化问题，通过动态调整预算分配策略，在保护用户隐私的同时提高了查询效用，具有广泛的应用前景。

Abstract: We study the problem of adaptive privacy budgeting under generalized differential privacy. Consider the setting where each user $i\in [n]$ holds a tuple $x_i\in U:=U_1\times \dotsb \times U_T$, where $x_i(l)\in U_l$ represents the $l$-th component of their data. For every $l\in [T]$ (or a subset), an untrusted analyst wishes to compute some $f_l(x_1(l),\dots,x_n(l))$, while respecting the privacy of each user. For many functions $f_l$, data from the users are not all equally important, and there is potential to use the privacy budgets of the users strategically, leading to privacy savings that can be used to improve the utility of later queries. In particular, the budgeting should be adaptive to the outputs of previous queries, so that greater savings can be achieved on more typical instances. In this paper, we provide such an adaptive budgeting framework, with various applications demonstrating its applicability.

</details>


### [20] [AJAR: Adaptive Jailbreak Architecture for Red-teaming](https://arxiv.org/abs/2601.10971)
*Yipu Dou,Wang Yang*

Main category: cs.CR

TL;DR: AJAR是一个用于LLM智能体安全红队的框架，通过协议驱动的认知编排实现模块化、可插拔的攻击模拟，支持多轮工具使用环境中的状态回溯攻击。


<details>
  <summary>Details</summary>
Motivation: 随着LLM从静态聊天机器人演变为能够执行工具的自主体，AI安全从内容审核转向行动安全。现有红队框架存在分裂：要么专注于僵化的基于脚本的文本攻击，要么缺乏模拟复杂多轮自主攻击的架构模块化。

Method: 引入AJAR框架，基于Petri运行时，利用模型上下文协议(MCP)将对抗逻辑与执行循环解耦，将X-Teaming等先进算法封装为标准化的即插即用服务，实现协议驱动的认知编排。

Result: 通过受控定性案例研究验证了AJAR的架构可行性，展示了其在工具使用环境中执行状态回溯的能力。初步探索"自主性差距"揭示了复杂的安全动态：工具使用通过代码执行引入新的注入向量，而参数格式化的认知负荷可能无意中破坏基于角色的攻击。

Conclusion: AJAR为这一新兴攻击面的标准化、环境感知评估提供了框架，已开源以促进相关研究。

Abstract: As Large Language Models (LLMs) evolve from static chatbots into autonomous agents capable of tool execution, the landscape of AI safety is shifting from content moderation to action security. However, existing red-teaming frameworks remain bifurcated: they either focus on rigid, script-based text attacks or lack the architectural modularity to simulate complex, multi-turn agentic exploitations. In this paper, we introduce AJAR (Adaptive Jailbreak Architecture for Red-teaming), a proof-of-concept framework designed to bridge this gap through Protocol-driven Cognitive Orchestration. Built upon the robust runtime of Petri, AJAR leverages the Model Context Protocol (MCP) to decouple adversarial logic from the execution loop, encapsulating state-of-the-art algorithms like X-Teaming as standardized, plug-and-play services. We validate the architectural feasibility of AJAR through a controlled qualitative case study, demonstrating its ability to perform stateful backtracking within a tool-use environment. Furthermore, our preliminary exploration of the "Agentic Gap" reveals a complex safety dynamic: while tool usage introduces new injection vectors via code execution, the cognitive load of parameter formatting can inadvertently disrupt persona-based attacks. AJAR is open-sourced to facilitate the standardized, environment-aware evaluation of this emerging attack surface. The code and data are available at https://github.com/douyipu/ajar.

</details>


### [21] [Shaping a Quantum-Resistant Future: Strategies for Post-Quantum PKI](https://arxiv.org/abs/2601.11104)
*Grazia D'Onghia,Diana Gratiela Berbecaru,Antonio Lioy*

Main category: cs.CR

TL;DR: 该论文探讨了后量子时代公钥基础设施的安全转型，重点关注X.509证书格式的适应性调整，以及证书撤销列表和在线证书状态协议的量子安全算法支持。


<details>
  <summary>Details</summary>
Motivation: 随着量子计算时代的临近，保护经典密码协议变得至关重要。公钥密码学广泛用于签名和密钥交换，但正是这类密码学最受量子计算的威胁。公钥证书作为签名数据结构，需要同时应对被认证密钥和签名本身的双重量子挑战。

Method: 论文提出了选择稳健后量子算法的最新进展，并研究其在公钥基础设施环境中的适用性。通过定义安全过渡到量子抵抗公钥基础设施的要求，重点关注X.509证书格式的适应性调整，并探索证书撤销列表和在线证书状态协议对量子抵抗算法的支持。

Result: 通过比较分析，阐明了向量子抵抗公钥基础设施过渡的复杂性，为后量子密码学在现有基础设施中的集成提供了框架和指导。

Conclusion: 论文为公钥基础设施向量子安全时代的平稳过渡提供了系统性的解决方案，强调了算法选择、证书格式适应和撤销机制更新的重要性，以确保在量子计算威胁下的长期安全性。

Abstract: As the quantum computing era approaches, securing classical cryptographic protocols becomes imperative. Public key cryptography is widely used for signature and key exchange but it is the type of cryptography more threatened by quantum computing. Its application typically requires support via a public-key certificate, which is a signed data structure and must therefore face twice the quantum challenge: for the certified keys and for the signature itself. We present the latest developments in selecting robust Post-Quantum algorithms and investigate their applicability in the Public Key Infrastructure context. Our contribution entails defining requirements for a secure transition to a quantum-resistant Public Key Infrastructure, with a focus on adaptations for the X.509 certificate format. Additionally, we explore transitioning Certificate Revocation List and Online Certificate Status Protocol to support quantum-resistant algorithms. Through comparative analysis, we elucidate the complex transition to a quantum-resistant PKI.

</details>


### [22] [Proving Circuit Functional Equivalence in Zero Knowledge](https://arxiv.org/abs/2601.11173)
*Sirui Shen,Zunchen Huang,Chenglu Jin*

Main category: cs.CR

TL;DR: ZK-CEC是首个结合形式验证和零知识证明的隐私保护硬件验证框架，能在不泄露设计机密的情况下证明IP功能符合公开规范。


<details>
  <summary>Details</summary>
Motivation: 现代集成电路生态系统依赖第三方知识产权集成，这带来了硬件木马和安全漏洞等风险。现有隐私保护硬件验证方法都是基于仿真的，缺乏形式化保证，需要在保护设计机密的同时解决IP供应商和系统集成商之间的信任僵局。

Method: 结合形式验证和零知识证明技术，首先提出证明秘密设计对公共约束不可满足性的蓝图，然后构建ZK-CEC框架，使证明者能在零知识状态下向验证者证明秘密IP功能与公开规范完全一致。

Result: 成功实现了ZK-CEC并在多种电路上评估性能，包括算术单元和加密组件。实验结果表明ZK-CEC能在实际时间限制内验证实用设计，如AES S-Box。

Conclusion: ZK-CEC为硬件形式验证建立了首个隐私保护框架，解决了现有方法缺乏形式化保证的问题，为在不泄露设计机密的情况下验证IP正确性和安全性提供了基础。

Abstract: The modern integrated circuit ecosystem is increasingly reliant on third-party intellectual property integration, which introduces security risks, including hardware Trojans and security vulnerabilities. Addressing the resulting trust deadlock between IP vendors and system integrators without exposing proprietary designs requires novel privacy-preserving verification techniques. However, existing privacy-preserving hardware verification methods are all simulation-based and fail to offer formal guarantees. In this paper, we propose ZK-CEC, the first privacy-preserving framework for hardware formal verification. By combining formal verification and zero-knowledge proof (ZKP), ZK-CEC establishes a foundation for formally verifying IP correctness and security without compromising the confidentiality of the designs.
  We observe that existing zero-knowledge protocols for formal verification are designed to prove statements of public formulas. However, in a privacy-preserving verification context where the formula is secret, these protocols cannot prevent a malicious prover from forging the formula, thereby compromising the soundness of the verification. To address these gaps, we first propose a blueprint for proving the unsatisfiability of a secret design against a public constraint, which is widely applicable to proving properties in software, hardware, and cyber-physical systems. Based on the proposed blueprint, we construct ZK-CEC, which enables a prover to convince the verifier that a secret IP's functionality aligns perfectly with the public specification in zero knowledge, revealing only the length and width of the proof. We implement ZK-CEC and evaluate its performance across various circuits, including arithmetic units and cryptographic components. Experimental results show that ZK-CEC successfully verifies practical designs, such as the AES S-Box, within practical time limits.

</details>


### [23] [SD-RAG: A Prompt-Injection-Resilient Framework for Selective Disclosure in Retrieval-Augmented Generation](https://arxiv.org/abs/2601.11199)
*Aiman Al Masoud,Marco Arazzi,Antonino Nocera*

Main category: cs.CR

TL;DR: SD-RAG：一种选择性披露的检索增强生成方法，通过在检索阶段而非生成阶段实施安全隐私约束，有效防止敏感信息泄露和提示注入攻击。


<details>
  <summary>Details</summary>
Motivation: 现有RAG方法大多忽视敏感信息泄露风险，少数依赖生成模型自我约束的方法易受提示注入攻击，需要更可靠的安全隐私保护机制。

Method: 提出SD-RAG方法，将安全隐私约束执行从生成过程解耦，在检索阶段应用净化与披露控制；引入语义机制处理人类可读的动态约束，采用优化的基于图的数据模型支持细粒度、策略感知的检索。

Result: 实验评估显示SD-RAG优于基线方法，隐私评分提升达58%，同时对针对生成模型的提示注入攻击表现出强韧性。

Conclusion: SD-RAG通过在检索阶段而非生成阶段实施安全隐私约束，为RAG系统提供了更可靠的选择性披露机制，有效平衡了信息利用与隐私保护需求。

Abstract: Retrieval-Augmented Generation (RAG) has attracted significant attention due to its ability to combine the generative capabilities of Large Language Models (LLMs) with knowledge obtained through efficient retrieval mechanisms over large-scale data collections. Currently, the majority of existing approaches overlook the risks associated with exposing sensitive or access-controlled information directly to the generation model. Only a few approaches propose techniques to instruct the generative model to refrain from disclosing sensitive information; however, recent studies have also demonstrated that LLMs remain vulnerable to prompt injection attacks that can override intended behavioral constraints. For these reasons, we propose a novel approach to Selective Disclosure in Retrieval-Augmented Generation, called SD-RAG, which decouples the enforcement of security and privacy constraints from the generation process itself. Rather than relying on prompt-level safeguards, SD-RAG applies sanitization and disclosure controls during the retrieval phase, prior to augmenting the language model's input. Moreover, we introduce a semantic mechanism to allow the ingestion of human-readable dynamic security and privacy constraints together with an optimized graph-based data model that supports fine-grained, policy-aware retrieval. Our experimental evaluation demonstrates the superiority of SD-RAG over baseline existing approaches, achieving up to a $58\%$ improvement in the privacy score, while also showing a strong resilience to prompt injection attacks targeting the generative model.

</details>


### [24] [InterPUF: Distributed Authentication via Physically Unclonable Functions and Multi-party Computation for Reconfigurable Interposers](https://arxiv.org/abs/2601.11368)
*Ishraq Tashdid,Tasnuva Farheen,Sazadur Rahman*

Main category: cs.CR

TL;DR: InterPUF：一种基于可重构中介层的紧凑型可扩展认证框架，通过分布式延迟PUF和多方计算实现去中心化芯片认证


<details>
  <summary>Details</summary>
Motivation: 现代系统级封装平台采用可重构中介层实现芯片粒的即插即用集成，但这种灵活性带来了严重的信任挑战，传统认证方案无法在去中心化、后制造可编程环境中扩展和适应

Method: 提出InterPUF框架，将中介层转化为分布式信任根：1）在可重构互连中嵌入基于路由的差分延迟物理不可克隆函数；2）使用多方计算保护认证过程，确保原始PUF签名永不暴露；3）结合中介层驻留的PUF原语、密码哈希和协作验证

Result: 硬件评估显示仅增加0.23%面积和0.072%功耗，认证延迟保持在数十纳秒内；pyPUF仿真确认在工艺、电压和温度变化下具有强唯一性、可靠性和建模抵抗性

Conclusion: InterPUF通过结合中介层驻留PUF原语与密码哈希和协作验证，实现了不依赖中心化锚点的最小信任认证模型，为异构多供应商生态系统提供了可扩展的认证解决方案

Abstract: Modern system-in-package (SiP) platforms increasingly adopt reconfigurable interposers to enable plug-and-play chiplet integration across heterogeneous multi-vendor ecosystems. However, this flexibility introduces severe trust challenges, as traditional authentication schemes fail to scale or adapt in decentralized, post-fabrication programmable environments. This paper presents InterPUF, a compact and scalable authentication framework that transforms the interposer into a distributed root of trust. InterPUF embeds a route-based differential delay physically unclonable function (PUF) across the reconfigurable interconnect and secures authentication using multi-party computation (MPC), ensuring raw PUF signatures are never exposed. Our hardware evaluation shows only 0.23% area and 0.072% power overhead across diverse chiplets while preserving authentication latency within tens of nanoseconds. Simulation results using pyPUF confirm strong uniqueness, reliability, and modeling resistance under process, voltage, and temperature variations. By combining interposer-resident PUF primitives with cryptographic hashing and collaborative verification, InterPUF enforces a minimal-trust authentication model without relying on a centralized anchor.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [25] [Multi-Artifact Analysis of Self-Admitted Technical Debt in Scientific Software](https://arxiv.org/abs/2601.10850)
*Eric L. Melin,Nasir U. Eisty,Gregory Watson,Addi Malviya-Thakur*

Main category: cs.SE

TL;DR: 该研究首次系统识别和分类科学软件中的科学债务，开发了多源SATD分类器，发现传统技术债务类别无法充分捕捉科学软件特有的技术债务，需要专门的检测方法。


<details>
  <summary>Details</summary>
Motivation: 科学软件中的自认技术债务对研究结果的效度和可重复性构成独特风险，但传统SATD分类无法充分捕捉这些领域特定问题，需要专门研究科学债务。

Method: 对23个开源科学软件项目进行多工件分析，涵盖代码注释、提交信息、拉取请求和问题跟踪器；构建并验证科学债务数据集，开发多源SATD分类器，进行实践者验证评估科学债务的实际相关性。

Result: 分类器在23个科学软件项目的900,358个工件上表现优异；SATD在拉取请求和问题跟踪器中最为普遍；传统SATD训练的模型常遗漏科学债务；实践者验证确认科学债务具有实际识别价值。

Conclusion: 科学债务是科学软件中独特的SATD形式，传统类别无法充分捕捉，需要专门的识别和管理方法；研究提供了首个正式的多工件视角，强调科学软件需要定制化的SATD检测方法。

Abstract: Context: Self-admitted technical debt (SATD) occurs when developers acknowledge shortcuts in code. In scientific software (SSW), such debt poses unique risks to the validity and reproducibility of results. Objective: This study aims to identify, categorize, and evaluate scientific debt, a specialized form of SATD in SSW, and assess the extent to which traditional SATD categories capture these domain-specific issues. Method: We conduct a multi-artifact analysis across code comments, commit messages, pull requests, and issue trackers from 23 open-source SSW projects. We construct and validate a curated dataset of scientific debt, develop a multi-source SATD classifier, and conduct a practitioner validation to assess the practical relevance of scientific debt. Results: Our classifier performs strongly across 900,358 artifacts from 23 SSW projects. SATD is most prevalent in pull requests and issue trackers, underscoring the value of multi-artifact analysis. Models trained on traditional SATD often miss scientific debt, emphasizing the need for its explicit detection in SSW. Practitioner validation confirmed that scientific debt is both recognizable and useful in practice. Conclusions: Scientific debt represents a unique form of SATD in SSW that that is not adequately captured by traditional categories and requires specialized identification and management. Our dataset, classification analysis, and practitioner validation results provide the first formal multi-artifact perspective on scientific debt, highlighting the need for tailored SATD detection approaches in SSW.

</details>


### [26] [Change And Cover: Last-Mile, Pull Request-Based Regression Test Augmentation](https://arxiv.org/abs/2601.10942)
*Zitong Zhou,Matteo Paltenghi,Miryung Kim,Michael Pradel*

Main category: cs.SE

TL;DR: ChaCo是一个基于LLM的测试增强技术，专门针对PR中未覆盖的代码行生成测试，填补"最后一英里"回归测试空白。


<details>
  <summary>Details</summary>
Motivation: 在软件开发中，即使有完善的测试套件，PR修改的代码行仍可能未被测试覆盖，形成"最后一英里"回归测试空白。现有测试生成器通常关注整体覆盖率，而非专门针对PR中的未覆盖代码行。

Method: ChaCo采用基于LLM的测试增强方法：1) 专注于PR特定的补丁覆盖率；2) 提取相关测试上下文（现有测试函数、夹具、数据生成器）；3) 将生成的测试与现有测试套件风格和结构集成，并生成测试添加摘要供开发者审查。

Result: 在三个复杂开源项目（SciPy、Qiskit、Pandas）的145个PR上评估，帮助30%的PR实现完全补丁覆盖率，成本仅0.11美元。人工评审认为测试值得添加（4.53/5.0）、集成良好（4.2/5.0）、与PR相关（4.7/5.0）。提交的12个测试中8个已被合并，并发现并修复了两个先前未知的bug。

Conclusion: ChaCo有效解决了PR中未覆盖代码行的测试问题，展示了基于LLM的测试增强技术的实用性和有效性，可集成到CI工作流中自动化"最后一英里"回归测试增强。

Abstract: Software is in constant evolution, with developers frequently submitting pull requests (PRs) to introduce new features or fix bugs. Testing PRs is critical to maintaining software quality. Yet, even in projects with extensive test suites, some PR-modified lines remain untested, leaving a "last-mile" regression test gap. Existing test generators typically aim to improve overall coverage, but do not specifically target the uncovered lines in PRs. We present Change And Cover (ChaCo), an LLM-based test augmentation technique that addresses this gap. It makes three contributions: (i) ChaCo considers the PR-specific patch coverage, offering developers augmented tests for code just when it is on the developers' mind. (ii) We identify providing suitable test context as a crucial challenge for an LLM to generate useful tests, and present two techniques to extract relevant test content, such as existing test functions, fixtures, and data generators. (iii) To make augmented tests acceptable for developers, ChaCo carefully integrates them into the existing test suite, e.g., by matching the test's structure and style with the existing tests, and generates a summary of the test addition for developer review. We evaluate ChaCo on 145 PRs from three popular and complex open-source projects - SciPy, Qiskit, and Pandas. The approach successfully helps 30% of PRs achieve full patch coverage, at the cost of $0.11, showing its effectiveness and practicality. Human reviewers find the tests to be worth adding (4.53/5.0), well integrated (4.2/5.0), and relevant to the PR (4.7/5.0). Ablations show test context is crucial for context-aware test generation, leading to 2x coverage. We submitted 12 tests, of which 8 have already been merged, and two previously unknown bugs were exposed and fixed. We envision our approach to be integrated into CI workflows, automating the last mile of regression test augmentation.

</details>


### [27] [Patterns of Bot Participation and Emotional Influence in Open-Source Development](https://arxiv.org/abs/2601.11138)
*Matteo Vaccargiu,Riccardo Lai,Maria Ilaria Lunesu,Andrea Pinna,Giuseppe Destefanis*

Main category: cs.SE

TL;DR: 研究以太坊生态系统中机器人对开源讨论的贡献及其对开发者情感语调的影响，发现少量机器人（0.28%）与开发者沟通的时间和情感动态变化相关


<details>
  <summary>Details</summary>
Motivation: 研究机器人在开源社区中的角色，特别是它们如何影响开发者之间的情感互动和沟通动态，这对于理解自动化工具在协作开发环境中的作用具有重要意义

Method: 收集了10个代码库的36,875个账户数据，识别出105个已验证的机器人（0.28%）。分析人类和机器人的参与模式，使用基于27种情感类别的模型分析情感变化，比较机器人和人类在拉取请求和问题讨论中的响应时间和情感影响

Result: 人类参与呈U型模式，机器人在拉取请求中均匀参与，在问题讨论中后期参与。机器人在拉取请求中响应更快，但在问题中扮演较慢的维护角色。机器人本身更中立，但它们的干预导致人类评论中立性降低，情感向感激、钦佩和乐观转变，困惑减少

Conclusion: 即使少量机器人也与开发者沟通的时间和情感动态变化相关，表明机器人在开源社区中不仅影响技术流程，还影响社交情感互动

Abstract: We study how bots contribute to open-source discussions in the Ethereum ecosystem and whether they influence developers' emotional tone. Our dataset covers 36,875 accounts across ten repositories with 105 validated bots (0.28%). Human participation follows a U-shaped pattern, while bots engage in uniform (pull requests) or late-stage (issues) activity. Bots respond faster than humans in pull requests but play slower maintenance roles in issues. Using a model trained on 27 emotion categories, we find bots are more neutral, yet their interventions are followed by reduced neutrality in human comments, with shifts toward gratitude, admiration, and optimism and away from confusion. These findings indicate that even a small number of bots are associated with changes in both timing and emotional dynamics of developer communication.

</details>


### [28] [Automation and Reuse Practices in GitHub Actions Workflows: A Practitioner's Perspective](https://arxiv.org/abs/2601.11299)
*Hassan Onsori Delicheh,Guillaume Cardoen,Alexandre Decan,Tom Mens*

Main category: cs.SE

TL;DR: 调查419名GitHub Actions用户，发现开发者主要自动化CI/CD任务，较少关注安全分析和性能监控；依赖可复用Actions但较少使用可复用工作流；面临版本维护挑战，常通过复制粘贴来获得更多控制权


<details>
  <summary>Details</summary>
Motivation: GitHub Actions原生支持工作流自动化，但工作流维护常被视为开发者的负担，缺乏关于工作流实践者自动化和复用偏好的知识。需要了解开发者的实际实践和挑战，以改进工作流维护支持

Method: 对419名工作流实践者进行问卷调查，研究他们使用GitHub Actions自动化哪些任务、偏好的工作流创建机制、优先考虑的非功能性特性，以及工作流复用机制的实践和挑战

Result: 1. 自动化主要集中在核心CI/CD任务，对安全分析和性能监控等关键领域关注较少；2. 强烈依赖可复用Actions，但可复用工作流采用率较低；3. 面临Action版本控制和维护挑战；4. 复制粘贴是常见实践，以获得更多控制权并避免依赖可复用组件的复杂性

Conclusion: 需要改进工具支持，增强对广泛自动化任务的支持，并提供更好的可复用工作流组件发现、管理和信任机制。研究结果揭示了当前GitHub Actions实践中的差距和机会

Abstract: GitHub natively supports workflow automation through GitHub Actions. Yet, workflow maintenance is often considered a burden for software developers, who frequently face difficulties in writing, testing, debugging, and maintaining workflows. Little knowledge exists concerning the automation and reuse practices favoured by workflow practitioners. We therefore surveyed 419 practitioners to elucidate good and bad workflow development practices and to identify opportunities for supporting workflow maintenance. Specifically, we investigate the tasks that practitioners tend to automate using GitHub Actions, their preferred workflow creation mechanisms, and the non-functional characteristics they prioritise. We also examine the practices and challenges associated with GitHub's workflow reuse mechanisms. We observe a tendency to focus automation efforts on core CI/CD tasks, with less emphasis on crucial areas like security analysis and performance monitoring. Practitioners strongly rely on reusable Actions, but reusable workflows see less frequent adoption. Furthermore, we observed challenges with Action versioning and maintenance. Copy-pasting remains a common practice to have more control and avoid the complexity of depending on reusable components. These insights suggest the need for improved tooling, enhanced support for a wide range of automation tasks, and better mechanisms for discovering, managing, and trusting reusable workflow components.

</details>


### [29] [RITA: A Tool for Automated Requirements Classification and Specification from Online User Feedback](https://arxiv.org/abs/2601.11362)
*Manjeshwar Aniruddh Mallya,Alessio Ferrari,Mohammad Amin Zadenoori,Jacek Dąbrowski*

Main category: cs.SE

TL;DR: RITA是一个集成轻量级开源大语言模型的工具，通过统一工作流将在线用户反馈转化为需求制品，支持自动需求分类、非功能性需求识别和自然语言需求规范生成，并与Jira集成。


<details>
  <summary>Details</summary>
Motivation: 在线用户反馈是需求工程的重要资源，但海量和嘈杂的特性使其分析困难。现有工具虽然支持单个反馈分析任务，但缺乏端到端的集成支持，限制了实际应用和真实世界有用性的评估。

Method: RITA将轻量级开源大语言模型集成到统一工作流中，支持自动需求分类、非功能性需求识别和自然语言需求规范生成，提供用户友好界面，并与Jira集成实现需求规范到开发工具的无缝转移。

Result: RITA利用先前评估过的基于LLM的需求工程技术，能够高效地将原始用户反馈转化为需求制品，有助于弥合研究与实践之间的差距。

Conclusion: RITA通过集成轻量级开源LLM和端到端工作流，解决了现有需求工程工具缺乏集成的问题，为反馈驱动的需求工程提供了实用工具，促进了研究成果向实际应用的转化。

Abstract: Context and motivation. Online user feedback is a valuable resource for requirements engineering, but its volume and noise make analysis difficult. Existing tools support individual feedback analysis tasks, but their capabilities are rarely integrated into end-to-end support. Problem. The lack of end-to-end integration limits the practical adoption of existing RE tools and makes it difficult to assess their real-world usefulness. Solution. To address this challenge, we present RITA, a tool that integrates lightweight open-source large language models into a unified workflow for feedback-driven RE. RITA supports automated request classification, non-functional requirement identification, and natural-language requirements specification generation from online feedback via a user-friendly interface, and integrates with Jira for seamless transfer of requirements specifications to development tools. Results and conclusions. RITA exploits previously evaluated LLM-based RE techniques to efficiently transform raw user feedback into requirements artefacts, helping bridge the gap between research and practice. A demonstration is available at: https://youtu.be/8meCLpwQWV8.

</details>


### [30] [A Practical Guide to Establishing Technical Debt Management](https://arxiv.org/abs/2601.11430)
*Marion Wiese*

Main category: cs.SE

TL;DR: 该白皮书基于博士论文研究成果，将技术债务管理科学发现转化为团队实践指南，通过三个公司团队的实践验证，区分"最佳实践"和"锦上添花"建议。


<details>
  <summary>Details</summary>
Motivation: 将技术债务管理的科学研究成果转化为实际可操作的团队指导，解决理论与实践脱节的问题，帮助团队建立适合自身需求的技术债务管理系统。

Method: 基于博士论文研究，与研究人员合作支持三个不同公司的团队，帮助他们调整和建立适合特定需求的技术债务管理系统，补充实践细节和额外方法，剔除不实用的研究成果。

Result: 创建了一个团队技术债务管理指南，区分"最佳实践"（三个团队都采用的）和"锦上添花"（至少一个团队使用的），强调团队共同决策，提供灵活指导而非僵化框架。

Conclusion: 该白皮书提供了团队层面的技术债务管理实用指南，强调适应性和团队自主决策，虽然不涵盖全公司范围的技术债务管理，但在结尾提供了相关建议。

Abstract: This white paper provides an overview of the topic of "technical debt" and presents an approach for managing technical debt in teams. The white paper is based on the results of my dissertation, which aimed to translate scientific findings into practical guidance. To this end, I collaborated with other researchers to support three teams from different companies in adapting and establishing a technical debt management system tailored to their specific needs. Research findings were supplemented with details or additional approaches. Research results that were less practical were discarded. The result is a guide on establishing technical debt management within a team. The guide is intended to provide orientation and not be a rigid framework. We distinguish between "best practices" and "nice-to-haves." "Best practices" are understood to be all approaches that were adopted by all three teams. "Nice-to-haves" were used by at least one team. In many places, it is explicitly mentioned that the team should decide together how to design the process. This also applies, of course, to all areas where this was not explicitly mentioned. This white paper explicitly does not cover the establishment of technical debt management across the entire company, but provides suggestions for this at the end.

</details>
