<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 16]
- [cs.CR](#cs.CR) [Total: 23]
- [cs.AI](#cs.AI) [Total: 21]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [Evaluating Uncertainty and Quality of Visual Language Action-enabled Robots](https://arxiv.org/abs/2507.17049)
*Pablo Valle,Chengjie Lu,Shaukat Ali,Aitor Arrieta*

Main category: cs.SE

TL;DR: 该论文针对视觉语言动作(VLA)模型提出了新的评估指标，包括8个不确定性指标和5个质量指标，通过大规模实证研究验证了这些指标与人类专家评估的相关性，挑战了仅依赖成功率的传统评估方法。


<details>
  <summary>Details</summary>
Motivation: 现有的VLA模型评估主要依赖任务成功率，这种方法无法捕捉任务执行的质量和模型决策的信心水平，需要更全面的评估指标来衡量VLA模型在机器人操作任务中的表现。

Method: 提出了13个专门针对VLA模型机器人操作任务的评估指标（8个不确定性指标和5个质量指标），通过对3个最先进VLA模型在4个代表性机器人操作任务上的908次成功执行进行大规模实证研究，由人类领域专家手动标注任务质量，分析提出指标与专家判断的相关性。

Result: 研究结果显示，多个提出的指标与人类评估呈现中等到强烈的相关性，证明了这些指标在评估任务质量和模型信心方面的实用性。此外，某些指标能够区分高、中、低质量的执行与不成功的任务，这在缺乏测试预言的情况下具有重要价值。

Conclusion: 研究挑战了当前仅依赖二元成功率的评估实践的充分性，为改进VLA驱动机器人系统的实时监控和自适应增强铺平了道路，推动了更全面和细致的VLA模型评估方法的发展。

Abstract: Visual Language Action (VLA) models are a multi-modal class of Artificial
Intelligence (AI) systems that integrate visual perception, natural language
understanding, and action planning to enable agents to interpret their
environment, comprehend instructions, and perform embodied tasks autonomously.
Recently, significant progress has been made to advance this field. These kinds
of models are typically evaluated through task success rates, which fail to
capture the quality of task execution and the mode's confidence in its
decisions. In this paper, we propose eight uncertainty metrics and five quality
metrics specifically designed for VLA models for robotic manipulation tasks. We
assess their effectiveness through a large-scale empirical study involving 908
successful task executions from three state-of-the-art VLA models across four
representative robotic manipulation tasks. Human domain experts manually
labeled task quality, allowing us to analyze the correlation between our
proposed metrics and expert judgments. The results reveal that several metrics
show moderate to strong correlation with human assessments, highlighting their
utility for evaluating task quality and model confidence. Furthermore, we found
that some of the metrics can discriminate between high-, medium-, and
low-quality executions from unsuccessful tasks, which can be interesting when
test oracles are not available. Our findings challenge the adequacy of current
evaluation practices that rely solely on binary success rates and pave the way
for improved real-time monitoring and adaptive enhancement of VLA-enabled
robotic systems.

</details>


### [2] [Assessing Reliability of Statistical Maximum Coverage Estimators in Fuzzing](https://arxiv.org/abs/2507.17093)
*Danushka Liyanage,Nelum Attanayake,Zijian Luo,Rahul Gopinath*

Main category: cs.SE

TL;DR: 本文研究模糊测试中可达性估计器的可靠性问题，通过合成程序生成框架和真实程序评估方法，对基于生物统计学的覆盖率估计方法进行了系统性评估。


<details>
  <summary>Details</summary>
Motivation: 模糊测试通常以覆盖率为指导，但估计最大可达覆盖率一直是关键挑战。虽然基于生物统计学物种丰富度估计器的统计估计方法被提出作为潜在解决方案，但由于缺乏可靠的标记真实数据基准，限制了对其准确性的严格评估。

Method: 提出了两种评估方法：(1) 开发了一个评估框架，能够合成生成具有复杂控制流的大型程序，确保明确定义的可达性并提供评估用的真实标准；(2) 针对真实程序基准，通过改变采样单元大小来适配可达性估计器的可靠性检查方法。

Result: 通过合成程序和真实程序两种研究方法，能够回答当前可达性估计器是否可靠的问题，并为评估未来可达性估计改进方法定义了评估协议。

Conclusion: 该研究建立了评估可达性估计器可靠性的系统性方法，为模糊测试中覆盖率估计的准确性评估提供了重要的评估框架和协议。

Abstract: Background: Fuzzers are often guided by coverage, making the estimation of
maximum achievable coverage a key concern in fuzzing. However, achieving 100%
coverage is infeasible for most real-world software systems, regardless of
effort. While static reachability analysis can provide an upper bound, it is
often highly inaccurate. Recently, statistical estimation methods based on
species richness estimators from biostatistics have been proposed as a
potential solution. Yet, the lack of reliable benchmarks with labeled ground
truth has limited rigorous evaluation of their accuracy.
  Objective: This work examines the reliability of reachability estimators from
two axes: addressing the lack of labeled ground truth and evaluating their
reliability on real-world programs.
  Methods: (1) To address the challenge of labeled ground truth, we propose an
evaluation framework that synthetically generates large programs with complex
control flows, ensuring well-defined reachability and providing ground truth
for evaluation. (2) To address the criticism from use of synthetic benchmarks,
we adapt a reliability check for reachability estimators on real-world
benchmarks without labeled ground truth -- by varying the size of sampling
units, which, in theory, should not affect the estimate.
  Results: These two studies together will help answer the question of whether
current reachability estimators are reliable, and defines a protocol to
evaluate future improvements in reachability estimation.

</details>


### [3] [Can LLMs Write CI? A Study on Automatic Generation of GitHub Actions Configurations](https://arxiv.org/abs/2507.17165)
*Taher A. Ghaleb,Dulina Rathnayake*

Main category: cs.SE

TL;DR: 本研究评估了六个大语言模型生成GitHub Actions CI配置的能力，发现零样本提示最高能达到69%的相似度但仅有3%完全匹配，揭示了LLM在CI配置生成方面的局限性


<details>
  <summary>Details</summary>
Motivation: 持续集成服务需要开发者编写基于YAML的配置文件，这个过程既繁琐又容易出错。尽管大语言模型在软件工程任务自动化方面应用日益增加，但其生成CI配置的能力仍未得到充分探索

Method: 评估了六个LLM模型（三个通用基础模型：GPT-4o、Llama、Gemma；三个代码预训练模型：GPT-4.1、Code Llama、CodeGemma）从自然语言描述生成GitHub Actions配置的能力。构建了首个此类标注数据集，包含从GitHub Actions文档中提取的描述与对应的最佳实践YAML配置对

Result: 零样本提示达到最高69%的相似度，但完全匹配率仅为3%。代码预训练模型在基于YAML的CI任务中表现略逊于通用模型。GPT-4o输出分析显示存在步骤缺失或重命名、描述误解、不必要添加等问题，影响结构和上下文正确性

Conclusion: 研究揭示了LLM在CI配置生成方面的局限性，生成质量与可执行CI配置所需精度之间存在差距。为改进LLM与配置语言的对齐以及指导未来CI自动化和工具支持工作提供了见解

Abstract: Continuous Integration (CI) services, such as GitHub Actions, require
developers to write YAML-based configurations, which can be tedious and
error-prone. Despite the increasing use of Large Language Models (LLMs) to
automate software engineering tasks, their ability to generate CI
configurations remains underexplored. This paper presents a preliminary study
evaluating six LLMs for generating GitHub Actions configurations from natural
language descriptions. We assess three general-purpose foundation models
(GPT-4o, Llama, and Gemma) and three code-pretrained models (GPT-4.1, Code
Llama, and CodeGemma). We also introduce the first labeled dataset of its kind,
constructed from GitHub Actions documentation, pairing descriptions with
corresponding best-practice YAML configurations. Zero-shot prompting achieves
up to 69% similarity with the ground truth, with only 3% perfect matches.
Code-pretrained models slightly underperform compared to general-purpose ones
in YAML-based CI tasks, revealing LLM limitations for CI configuration
generation. Analyzing GPT-4o outputs reveals issues like missing or renamed
steps, misinterpreted descriptions, and unnecessary additions that may affect
structural and contextual correctness, indicating a gap between generation
quality and the precision required for executable CI configurations. Our
research offers insights for improving LLM alignment with configuration
languages and guiding future efforts on CI automation and tooling support.

</details>


### [4] [On the Feasibility of Quantum Unit Testing](https://arxiv.org/abs/2507.17235)
*Andriy Miranskyy,José Campos,Anila Mjeda,Lei Zhang,Ignacio García Rodríguez de Guzmán*

Main category: cs.SE

TL;DR: 本研究对量子软件单元测试进行了全面分析，比较了传统统计方法与专门设计的量子电路测试方法，通过对近180万个变异量子电路的实证研究，发现量子中心化测试（特别是状态向量测试和逆测试）在精度和效率方面具有明显优势。


<details>
  <summary>Details</summary>
Motivation: 随着量子软件复杂性的增加，软件验证和验证面临重大挑战，特别是在单元测试方面。传统的统计测试方法在检测量子电路中的微妙差异方面存在局限性，需要开发更加可靠和可扩展的量子软件测试策略。

Method: 研究采用了多种测试方法的对比分析：包括仅在经典计算机上运行的状态向量测试，以及可在量子硬件上执行的交换测试和新颖的逆测试。通过对1,796,880个变异量子电路进行实证研究，分析各种测试方法检测量子电路预期状态与实际状态差异的能力，以及达到高可靠性所需的测量次数。

Result: 量子中心化测试，特别是状态向量测试和逆测试，在精度和效率方面表现出明显优势。与统计测试相比，这些方法能够减少假阳性和假阴性，提供更准确的测试结果，并且在测量次数要求方面更加高效。

Conclusion: 该研究为开发更加稳健和可扩展的量子软件测试策略做出了贡献，支持未来容错量子计算机的采用，并促进量子软件工程中更可靠的实践方法。量子中心化测试方法为量子软件验证提供了更有效的解决方案。

Abstract: The increasing complexity of quantum software presents significant challenges
for software verification and validation, particularly in the context of unit
testing. This work presents a comprehensive study on quantum-centric unit
tests, comparing traditional statistical approaches with tests specifically
designed for quantum circuits. These include tests that run only on a classical
computer, such as the Statevector test, as well as those executable on quantum
hardware, such as the Swap test and the novel Inverse test. Through an
empirical study and detailed analysis on 1,796,880 mutated quantum circuits, we
investigate (a) each test's ability to detect subtle discrepancies between the
expected and actual states of a quantum circuit, and (b) the number of
measurements required to achieve high reliability. The results demonstrate that
quantum-centric tests, particularly the Statevector test and the Inverse test,
provide clear advantages in terms of precision and efficiency, reducing both
false positives and false negatives compared to statistical tests. This work
contributes to the development of more robust and scalable strategies for
testing quantum software, supporting the future adoption of fault-tolerant
quantum computers and promoting more reliable practices in quantum software
engineering.

</details>


### [5] [Understanding Prompt Programming Tasks and Questions](https://arxiv.org/abs/2507.17264)
*Jenny T. Liang,Chenyang Yang,Agnia Sergeyuk,Travis D. Breaux,Brad A. Myers*

Main category: cs.SE

TL;DR: 本研究通过访谈、观察和调研分析了提示编程开发者的需求，发现现有工具支持不足，并提出了改进机会


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型等基础模型的普及，开发者开始在软件中嵌入提示进行编程，但目前不清楚开发者在更新提示时的具体需求和问题，也不确定现有提示编程工具是否充分满足了开发者的需要

Method: 采用混合研究方法：访谈16名提示编程开发者，观察8名开发者进行提示修改，调研50名开发者，构建了包含25个任务和51个问题的分类法，并与48个研究和商业工具进行对比分析

Result: 发现提示编程支持不足：所有任务都需要手动完成，51个问题中有16个（包括大部分最重要的问题）仍未得到解答。通过与现有工具对比，揭示了当前工具在满足开发者需求方面的不足

Conclusion: 提示编程目前缺乏充分的工具支持，研究结果为提示编程工具的发展提供了重要的改进方向和机会，有助于更好地满足开发者在提示编程过程中的实际需求

Abstract: Prompting foundation models (FMs) like large language models (LLMs) have
enabled new AI-powered software features (e.g., text summarization) that
previously were only possible by fine-tuning FMs. Now, developers are embedding
prompts in software, known as prompt programs. The process of prompt
programming requires the developer to make many changes to their prompt. Yet,
the questions developers ask to update their prompt is unknown, despite the
answers to these questions affecting how developers plan their changes. With
the growing number of research and commercial prompt programming tools, it is
unclear whether prompt programmers' needs are being adequately addressed. We
address these challenges by developing a taxonomy of 25 tasks prompt
programmers do and 51 questions they ask, measuring the importance of each task
and question. We interview 16 prompt programmers, observe 8 developers make
prompt changes, and survey 50 developers. We then compare the taxonomy with 48
research and commercial tools. We find that prompt programming is not
well-supported: all tasks are done manually, and 16 of the 51 questions --
including a majority of the most important ones -- remain unanswered. Based on
this, we outline important opportunities for prompt programming tools.

</details>


### [6] [Lessons from a Big-Bang Integration: Challenges in Edge Computing and Machine Learning](https://arxiv.org/abs/2507.17270)
*Alessandro Aneggi,Andrea Janes*

Main category: cs.SE

TL;DR: 这是一份关于分布式实时分析系统开发失败的经验报告。项目采用"大爆炸"集成方式，最终只实现了6分钟的系统功能，远低于预期的40分钟。研究分析了技术和组织障碍，并提出了早期模拟部署、强化沟通基础设施和采用自顶向下思维等改进建议。


<details>
  <summary>Details</summary>
Motivation: 分析一个为期一年的分布式实时分析系统项目的失败经验，该项目使用边缘计算和机器学习技术，但由于采用大爆炸集成方法而遭遇重大挫折，旨在从失败中总结经验教训。

Method: 通过根本原因分析方法，识别项目失败的技术和组织障碍，包括沟通不良、缺乏早期集成测试、抗拒自顶向下规划等问题，同时考虑心理因素如对完全开发组件的偏好而非模拟组件的偏见。

Result: 项目集成努力仅实现了6分钟的系统功能，远低于预期的40分钟。研究发现传统敏捷方法在此类分布式项目中存在局限性，识别出沟通不良、缺乏早期集成测试和组织抗拒等关键问题。

Conclusion: 提出早期基于模拟的部署、建立强健的沟通基础设施、采用自顶向下思维来管理复杂性和降低风险等建议。推荐仿真驱动工程和结构化集成周期作为未来成功的关键enabler，强调传统敏捷方法在反应式分布式项目中的局限性。

Abstract: This experience report analyses a one year project focused on building a
distributed real-time analytics system using edge computing and machine
learning. The project faced critical setbacks due to a big-bang integration
approach, where all components developed by multiple geographically dispersed
partners were merged at the final stage. The integration effort resulted in
only six minutes of system functionality, far below the expected 40 minutes.
Through root cause analysis, the study identifies technical and organisational
barriers, including poor communication, lack of early integration testing, and
resistance to topdown planning. It also considers psychological factors such as
a bias toward fully developed components over mockups. The paper advocates for
early mock based deployment, robust communication infrastructures, and the
adoption of topdown thinking to manage complexity and reduce risk in reactive,
distributed projects. These findings underscore the limitations of traditional
Agile methods in such contexts and propose simulation-driven engineering and
structured integration cycles as key enablers for future success.

</details>


### [7] [Seed&Steer: Guiding Large Language Models with Compilable Prefix and Branch Signals for Unit Test Generation](https://arxiv.org/abs/2507.17271)
*Shuaiyu Zhou,Zhengran Zeng,Xiaoling Zhou,Rui Xie,Shikun Zhang,Wei Ye*

Main category: cs.SE

TL;DR: 本文提出Seed&Steer方法，通过解耦前缀生成和断言生成来改进基于大语言模型的单元测试自动生成，结合传统测试工具和LLM能力，显著提升编译成功率和测试覆盖率。


<details>
  <summary>Details</summary>
Motivation: 现有基于大语言模型的单元测试生成方法存在编译失败和测试覆盖率低的问题。作者从新颖角度重新审视这一问题，发现前缀生成和断言生成面临不同挑战：初始化复杂度主要影响编译成功，而圈复杂度影响测试覆盖率。

Method: 提出Seed&Steer两步方法：1) 利用传统单元测试工具（如EvoSuite）生成高编译成功率的方法调用作为种子，指导LLM构建有效测试上下文；2) 引入分支提示帮助LLM探索多样化执行路径（正常、边界、异常情况）并生成高覆盖率断言。

Result: 在五个真实Java项目上的评估显示：编译通过率提升约7%，在两个LLM上成功编译了792和887个之前失败的案例；在不同复杂度的焦点方法上达到约73%的分支和行覆盖率，覆盖率改进范围为1.09倍到1.26倍。

Conclusion: Seed&Steer有效结合了传统单元测试技术和大语言模型能力，通过解耦前缀和断言生成解决了各自面临的挑战，在编译成功率和测试覆盖率方面均取得显著改进，为未来研究提供了新的思路。

Abstract: Unit tests play a vital role in the software development lifecycle. Recent
advances in Large Language Model (LLM)-based approaches have significantly
improved automated test generation, garnering attention from both academia and
industry. We revisit LLM-based unit test generation from a novel perspective by
decoupling prefix generation and assertion generation. To characterize their
respective challenges, we define Initialization Complexity and adopt Cyclomatic
Complexity to measure the difficulty of prefix and assertion generation,
revealing that the former primarily affects compilation success, while the
latter influences test coverage. To address these challenges, we propose
Seed&Steer, a two-step approach that combines traditional unit testing
techniques with the capabilities of large language models. Seed&Steer leverages
conventional unit testing tools (e.g., EvoSuite) to generate method invocations
with high compilation success rates, which serve as seeds to guide LLMs in
constructing effective test contexts. It then introduces branching cues to help
LLMs explore diverse execution paths (e.g., normal, boundary, and exception
cases) and generate assertions with high coverage. We evaluate Seed&Steer on
five real-world Java projects against state-of-the-art baselines. Results show
that Seed&Steer improves the compilation pass rate by approximately 7%,
successfully compiling 792 and 887 previously failing cases on two LLMs. It
also achieves up to ~73% branch and line coverage across focal methods of
varying complexity, with coverage improvements ranging from 1.09* to 1.26*. Our
code, dataset, and experimental scripts will be publicly released to support
future research and reproducibility.

</details>


### [8] [Data Virtualization for Machine Learning](https://arxiv.org/abs/2507.17293)
*Saiful Khan,Joyraj Chakraborty,Philip Beaucamp,Niraj Bhujel,Min Chen*

Main category: cs.SE

TL;DR: 本文介绍了一个数据虚拟化服务的设计与实现，用于支持多个并发机器学习工作流程，解决了ML团队在数据存储、处理和维护方面的挑战。


<details>
  <summary>Details</summary>
Motivation: 机器学习团队通常同时运行多个ML工作流程，每个工作流程涉及大量实验、迭代和协作活动，从数据处理到模型部署需要数月甚至数年时间。这产生了大量需要存储、处理和维护的中间数据，因此需要数据虚拟化技术来为ML工作流程提供基础设施支持。

Method: 设计并实现了一个数据虚拟化服务，重点关注其服务架构和服务操作。该基础设施专门为支持ML工作流程而设计，能够处理多个应用程序和工作流程的数据需求。

Result: 该数据虚拟化服务目前成功支持了六个ML应用程序，每个应用程序都包含多个ML工作流程。系统具有良好的可扩展性，能够支持未来应用程序和工作流程数量的增长。

Conclusion: 数据虚拟化是支持ML工作流程基础设施的关键技术。通过合理的服务架构和操作设计，可以有效解决ML团队面临的数据管理挑战，并为未来的扩展提供良好的基础。

Abstract: Nowadays, machine learning (ML) teams have multiple concurrent ML workflows
for different applications. Each workflow typically involves many experiments,
iterations, and collaborative activities and commonly takes months and
sometimes years from initial data wrangling to model deployment.
Organizationally, there is a large amount of intermediate data to be stored,
processed, and maintained. \emph{Data virtualization} becomes a critical
technology in an infrastructure to serve ML workflows. In this paper, we
present the design and implementation of a data virtualization service,
focusing on its service architecture and service operations. The infrastructure
currently supports six ML applications, each with more than one ML workflow.
The data virtualization service allows the number of applications and workflows
to grow in the coming years.

</details>


### [9] [How Do Code Smells Affect Skill Growth in Scratch Novice Programmers?](https://arxiv.org/abs/2507.17314)
*Ricardo Hidalgo Aragón,Jesús M. González-Barahona,Gregorio Robles*

Main category: cs.SE

TL;DR: 本研究分析了约200万个Scratch项目，探索计算思维能力与代码异味之间的关系，旨在为编程教育提供循证依据并改进自动化反馈系统。


<details>
  <summary>Details</summary>
Motivation: 虽然代码异味在专业代码中得到广泛研究，但在初学者创建的块式编程项目中的重要性仍不明确。需要了解计算思维技能培养与设计问题之间的关系，以改进编程教育。

Method: 从约200万个公开Scratch项目中随机抽样，使用开源检测工具提取9个计算思维评分和40个代码异味指标。采用描述性分析、稳健相关性测试、分层交叉验证和探索性机器学习模型，结合定性抽查来验证定量模式。

Result: 将提供首个大规模、细粒度的映射，连接特定计算思维能力与具体设计缺陷和反模式。为未来教育干预提供效应量基准，并提供开放的匿名化数据集和可重现的分析流程。

Conclusion: 通过阐明编程习惯如何影响早期技能习得，该研究推进了计算教育理论和可持续软件维护演进的实用工具开发。研究结果将为循证课程设计和自动化反馈系统提供指导。

Abstract: Context. Code smells, which are recurring anomalies in design or style, have
been extensively researched in professional code. However, their significance
in block-based projects created by novices is still largely unknown.
Block-based environments such as Scratch offer a unique, data-rich setting to
examine how emergent design problems intersect with the cultivation of
computational-thinking (CT) skills. Objective. This research explores the
connection between CT proficiency and design-level code smells--issues that may
hinder software maintenance and evolution--in programs created by Scratch
developers. We seek to identify which CT dimensions align most strongly with
which code smells and whether task context moderates those associations.
Method. A random sample of aprox. 2 million public Scratch projects is mined.
Using open-source linters, we extract nine CT scores and 40 code smell
indicators from these projects. After rigorous pre-processing, we apply
descriptive analytics, robust correlation tests, stratified cross-validation,
and exploratory machine-learning models; qualitative spot-checks contextualize
quantitative patterns. Impact. The study will deliver the first large-scale,
fine-grained map linking specific CT competencies to concrete design flaws and
antipatterns. Results are poised to (i) inform evidence-based curricula and
automated feedback systems, (ii) provide effect-size benchmarks for future
educational interventions, and (iii) supply an open, pseudonymized dataset and
reproducible analysis pipeline for the research community. By clarifying how
programming habits influence early skill acquisition, the work advances both
computing-education theory and practical tooling for sustainable software
maintenance and evolution.

</details>


### [10] [Roseau: Fast, Accurate, Source-based API Breaking Change Analysis in Java](https://arxiv.org/abs/2507.17369)
*Corentin Latappy,Thomas Degueule,Jean-Rémy Falleri,Romain Robbes,Lina Ochoa*

Main category: cs.SE

TL;DR: 本文介绍了Roseau，一个用于检测Java库API演化和破坏性变更的静态分析工具，相比传统工具JApiCmp和Revapi具有更高的准确性和更好的大规模纵向研究适用性


<details>
  <summary>Details</summary>
Motivation: 现有的Java API破坏性变更检测工具（如JApiCmp和Revapi）依赖二进制JAR文件，限制了它们在大规模纵向研究和细粒度分析（如提交级别的BC检测）中的应用性

Method: 开发了Roseau静态分析工具，能够从源代码或字节码构建技术无关的API模型，配备丰富的语义分析功能，通过比较API模型来识别任意两个版本之间的破坏性变更，针对大规模纵向分析进行了优化

Result: 在准确性方面，Roseau达到F1=0.99，超过JApiCmp（F1=0.86）和Revapi（F1=0.91）；在性能方面，能在2秒内检测出版本间的破坏性变更，即使对于数十万行代码的库也是如此；通过分析Google Guava API 14年6839次提交的演化过程，将分析时间从几天缩短到几分钟

Conclusion: Roseau为API演化研究提供了一个更准确、高效的工具，克服了传统工具在大规模纵向研究中的局限性，为软件库演化的实证研究开辟了新的可能性

Abstract: Understanding API evolution and the introduction of breaking changes (BCs) in
software libraries is essential for library maintainers to manage backward
compatibility and for researchers to conduct empirical studies on software
library evolution. In Java, tools such as JApiCmp and Revapi are commonly used
to detect BCs between library releases, but their reliance on binary JARs
limits their applicability. This restriction hinders large-scale longitudinal
studies of API evolution and fine-grained analyses such as commit-level BC
detection. In this paper, we introduce Roseau, a novel static analysis tool
that constructs technology-agnostic API models from library code equipped with
rich semantic analyses. API models can be analyzed to study API evolution and
compared to identify BCs between any two versions of a library (releases,
commits, branches, etc.). Unlike traditional approaches, Roseau can build API
models from source code or bytecode, and is optimized for large-scale
longitudinal analyses of library histories. We assess the accuracy,
performance, and suitability of Roseau for longitudinal studies of API
evolution, using JApiCmp and Revapi as baselines. We extend and refine an
established benchmark of BCs and show that Roseau achieves higher accuracy (F1
= 0.99) than JApiCmp (F1 = 0.86) and Revapi (F1 = 0.91). We analyze 60 popular
libraries from Maven Central and find that Roseau delivers excellent
performance, detecting BCs between versions in under two seconds, including in
libraries with hundreds of thousands of lines of code. We further illustrate
the limitations of JApiCmp and Revapi for longitudinal studies and the novel
analysis capabilities offered by Roseau by tracking the evolution of Google's
Guava API and the introduction of BCs over 14 years and 6,839 commits, reducing
analysis times from a few days to a few minutes.

</details>


### [11] [Investigating Training Data Detection in AI Coders](https://arxiv.org/abs/2507.17389)
*Tianlin Li,Yunxiang Wei,Zhiming Li,Aishan Liu,Qing Guo,Xianglong Liu,Dongning Sun,Yang Liu*

Main category: cs.SE

TL;DR: 研究者对代码大语言模型的训练数据检测方法进行了全面评估，引入了CodeSnitch基准数据集，并通过代码克隆分类法设计变异策略来测试检测方法的鲁棒性


<details>
  <summary>Details</summary>
Motivation: 代码大语言模型可能输出包含专有或敏感代码片段的内容，存在隐私和知识产权风险，需要进行训练数据检测以确保合规部署，但现有方法在代码数据上的有效性尚未充分探索

Method: 对7种最先进的训练数据检测方法在8个代码大语言模型上进行综合实证研究，引入包含9000个代码样本的函数级基准数据集CodeSnitch，并基于Type-1到Type-4代码克隆检测分类法设计变异策略测试鲁棒性

Result: 对当前代码训练数据检测技术进行了系统性评估，测试了不同方法在原始CodeSnitch数据集和变异数据集上的性能表现

Conclusion: 研究为代码训练数据检测技术提供了系统性评估，并为未来开发更有效和鲁棒的检测方法提供了指导性见解

Abstract: Recent advances in code large language models (CodeLLMs) have made them
indispensable tools in modern software engineering. However, these models
occasionally produce outputs that contain proprietary or sensitive code
snippets, raising concerns about potential non-compliant use of training data,
and posing risks to privacy and intellectual property. To ensure responsible
and compliant deployment of CodeLLMs, training data detection (TDD) has become
a critical task. While recent TDD methods have shown promise in natural
language settings, their effectiveness on code data remains largely
underexplored. This gap is particularly important given code's structured
syntax and distinct similarity criteria compared to natural language. To
address this, we conduct a comprehensive empirical study of seven
state-of-the-art TDD methods on source code data, evaluating their performance
across eight CodeLLMs. To support this evaluation, we introduce CodeSnitch, a
function-level benchmark dataset comprising 9,000 code samples in three
programming languages, each explicitly labeled as either included or excluded
from CodeLLM training. Beyond evaluation on the original CodeSnitch, we design
targeted mutation strategies to test the robustness of TDD methods under three
distinct settings. These mutation strategies are grounded in the
well-established Type-1 to Type-4 code clone detection taxonomy. Our study
provides a systematic assessment of current TDD techniques for code and offers
insights to guide the development of more effective and robust detection
methods in the future.

</details>


### [12] [AssertFlip: Reproducing Bugs via Inversion of LLM-Generated Passing Tests](https://arxiv.org/abs/2507.17542)
*Lara Khatib,Noble Saji Mathews,Meiyappan Nagappan*

Main category: cs.SE

TL;DR: AssertFlip是一种利用大语言模型自动生成缺陷重现测试的新技术，通过先生成通过测试再反转为失败测试的方式，在SWT-Bench基准测试中达到43.6%的成功率，超越了现有所有技术。


<details>
  <summary>Details</summary>
Motivation: 大多数开源和工业环境中的软件缺陷在报告时缺乏可执行的重现测试，这使得缺陷诊断和修复变得更加困难和耗时。因此需要一种自动生成缺陷重现测试(BRT)的方法来解决这一挑战。

Method: AssertFlip采用一种新颖的两步法：首先使用大语言模型在有缺陷的行为上生成通过的测试，然后将这些测试反转，使其在存在缺陷时失败。这基于一个假设：大语言模型更擅长编写通过的测试，而不是故意编写崩溃或失败的测试。

Result: AssertFlip在SWT-Bench基准测试的排行榜中超越了所有已知技术。具体地，在SWT-Bench-Verified子集上实现了43.6%的失败到通过成功率。

Conclusion: AssertFlip通过利用大语言模型先生成通过测试再反转的策略，有效解决了自动生成缺陷重现测试的问题，为软件调试和修复过程提供了重要支持，显著优于现有方法。

Abstract: Bug reproduction is critical in the software debugging and repair process,
yet the majority of bugs in open-source and industrial settings lack executable
tests to reproduce them at the time they are reported, making diagnosis and
resolution more difficult and time-consuming. To address this challenge, we
introduce AssertFlip, a novel technique for automatically generating Bug
Reproducible Tests (BRTs) using large language models (LLMs). Unlike existing
methods that attempt direct generation of failing tests, AssertFlip first
generates passing tests on the buggy behaviour and then inverts these tests to
fail when the bug is present. We hypothesize that LLMs are better at writing
passing tests than ones that crash or fail on purpose. Our results show that
AssertFlip outperforms all known techniques in the leaderboard of SWT-Bench, a
benchmark curated for BRTs. Specifically, AssertFlip achieves a fail-to-pass
success rate of 43.6% on the SWT-Bench-Verified subset.

</details>


### [13] [CodeReasoner: Enhancing the Code Reasoning Ability with Reinforcement Learning](https://arxiv.org/abs/2507.17548)
*Lingxiao Tang,He Ye,Zhongxin Liu,Xiaoxue Ren,Lingfeng Bao*

Main category: cs.SE

TL;DR: 提出了CodeReasoner框架，通过构建高质量数据集和两阶段训练（指令调优+强化学习）来提升大语言模型的代码推理能力，在代码推理基准测试中取得了显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有的代码推理方法主要依赖监督微调，但存在两个核心问题：训练数据质量低和监督微调在教授通用推理技能方面的局限性，导致性能提升有限且难以在不同场景下泛化。

Method: 提出CodeReasoner框架，包含数据集构建和两阶段训练过程：1）构建专注于Python程序核心执行逻辑的数据集；2）通过指令调优注入从强大教师模型中提取的执行特定知识；3）在微调模型基础上使用GRPO强化学习增强推理和泛化能力。

Result: 在三个广泛使用的代码推理基准测试中，CodeReasoner相比先前方法提升了27.1%到40.2%的性能。7B模型在输入/输出和覆盖率预测等关键任务上与GPT-4o相匹配，14B模型在所有基准测试中都超越了GPT-4o。

Conclusion: CodeReasoner通过高质量数据集构建和两阶段训练策略有效提升了大语言模型的代码推理能力，消融研究证实了每个训练阶段的有效性并强调了推理链的重要性。该框架为代码推理任务提供了一个有效的解决方案。

Abstract: Code reasoning is a fundamental capability for large language models (LLMs)
in the code domain. It involves understanding and predicting a program's
execution behavior, such as determining the output for a given input or whether
a specific statement will be executed. This capability is essential for
downstream tasks like debugging, code generation, and program repair. Prior
approaches mainly rely on supervised fine-tuning to improve performance in code
reasoning tasks. However, they often show limited gains and fail to generalize
across diverse scenarios. We argue this is due to two core issues: the low
quality of training data and the limitations of supervised fine-tuning, which
struggles to teach general reasoning skills. To address these challenges, we
propose CodeReasoner, a framework that spans both dataset construction and a
two-stage training process. First, we introduce a method to construct datasets
that focus on the core execution logic of Python programs. Next, we apply
instruction tuning to inject execution-specific knowledge distilled from a
powerful teacher model. We then enhance reasoning and generalization through
GRPO reinforcement learning on top of the fine-tuned model. Experiments on
three widely-used code reasoning benchmarks show that CodeReasoner improves
performance by 27.1% to 40.2% over prior methods using a 7B model. Notably, the
7B model matches GPT-4o on key tasks like input/output and coverage prediction.
When scaled to 14B, CodeReasoner outperforms GPT-4o across all benchmarks.
Ablation studies confirm the effectiveness of each training stage and highlight
the importance of reasoning chains.

</details>


### [14] [Contextual Code Retrieval for Commit Message Generation: A Preliminary Study](https://arxiv.org/abs/2507.17690)
*Bo Xiong,Linghao Zhang,Chong Wang,Peng Liang*

Main category: cs.SE

TL;DR: 本文提出了C3Gen方法，通过检索代码库中相关代码片段来增强提交信息生成，解决了仅依赖代码差异无法提供充分上下文的问题，实验表明该方法能生成更全面、更具实用价值的提交信息。


<details>
  <summary>Details</summary>
Motivation: 现有的提交信息生成方法仅依赖代码差异（code diff）作为输入，但原始代码差异无法捕获生成高质量、信息丰富的提交信息所需的完整上下文信息，这种方法存在局限性。

Method: 提出了基于上下文代码检索的C3Gen方法，通过从代码库中检索与提交相关的代码片段，并将这些片段整合到模型输入中，为模型提供代码库范围内更丰富的上下文信息来增强提交信息生成。

Result: 在多个模型上使用四个客观指标和三个主观指标进行评估，并设计人工评估来调查开发者对C3Gen生成的提交信息的感知。结果表明，通过引入上下文代码，C3Gen能够有效利用额外信息生成更全面、更具信息价值的提交信息，在实际开发场景中具有更大的实用价值。

Conclusion: C3Gen通过整合上下文代码信息成功改进了提交信息生成质量，生成的提交信息更加全面和实用。进一步分析还揭示了基于相似性指标可靠性的担忧，并为提交信息生成研究提供了实证见解。

Abstract: A commit message describes the main code changes in a commit and plays a
crucial role in software maintenance. Existing commit message generation (CMG)
approaches typically frame it as a direct mapping which inputs a code diff and
produces a brief descriptive sentence as output. However, we argue that relying
solely on the code diff is insufficient, as raw code diff fails to capture the
full context needed for generating high-quality and informative commit
messages. In this paper, we propose a contextual code retrieval-based method
called C3Gen to enhance CMG by retrieving commit-relevant code snippets from
the repository and incorporating them into the model input to provide richer
contextual information at the repository scope. In the experiments, we
evaluated the effectiveness of C3Gen across various models using four objective
and three subjective metrics. Meanwhile, we design and conduct a human
evaluation to investigate how C3Gen-generated commit messages are perceived by
human developers. The results show that by incorporating contextual code into
the input, C3Gen enables models to effectively leverage additional information
to generate more comprehensive and informative commit messages with greater
practical value in real-world development scenarios. Further analysis
underscores concerns about the reliability of similaritybased metrics and
provides empirical insights for CMG.

</details>


### [15] [CASCADE: LLM-Powered JavaScript Deobfuscator at Google](https://arxiv.org/abs/2507.17691)
*Shan Jiang,Pranoy Kovuri,David Tao,Zhixun Tan*

Main category: cs.SE

TL;DR: CASCADE是一种结合Gemini语言模型和JavaScript中间表示(JSIR)的混合JavaScript去混淆方法，通过识别关键前导函数并进行代码转换来恢复原始程序行为，已在Google生产环境中部署并显著提升去混淆效率。


<details>
  <summary>Details</summary>
Motivation: JavaScript混淆技术广泛存在，严重阻碍代码理解和分析，对软件测试、静态分析和恶意软件检测造成重大挑战。现有的静态和动态去混淆技术存在局限性，需要大量硬编码规则，缺乏可靠性和灵活性。

Method: 提出CASCADE混合方法，结合Gemini的高级编码能力和JavaScript中间表示(JSIR)的确定性转换能力。使用Gemini识别关键前导函数（最普遍混淆技术的基础组件），然后利用JSIR进行后续代码转换，从而恢复语义元素如原始字符串和API名称。

Result: CASCADE有效恢复了原始字符串和API名称等语义元素，揭示了原始程序行为。该方法克服了现有静态和动态去混淆技术的局限性，消除了数百到数千条硬编码规则，同时实现了可靠性和灵活性。已在Google生产环境中部署。

Conclusion: CASCADE在JavaScript去混淆方面取得了显著改进，大幅提升了去混淆效率并减少了逆向工程工作量。该混合方法成功结合了大语言模型和编译器中间表示的优势，为JavaScript去混淆提供了有效的解决方案。

Abstract: Software obfuscation, particularly prevalent in JavaScript, hinders code
comprehension and analysis, posing significant challenges to software testing,
static analysis, and malware detection. This paper introduces CASCADE, a novel
hybrid approach that integrates the advanced coding capabilities of Gemini with
the deterministic transformation capabilities of a compiler Intermediate
Representation (IR), specifically JavaScript IR (JSIR). By employing Gemini to
identify critical prelude functions, the foundational components underlying the
most prevalent obfuscation techniques, and leveraging JSIR for subsequent code
transformations, CASCADE effectively recovers semantic elements like original
strings and API names, and reveals original program behaviors. This method
overcomes limitations of existing static and dynamic deobfuscation techniques,
eliminating hundreds to thousands of hardcoded rules while achieving
reliability and flexibility. CASCADE is already deployed in Google's production
environment, demonstrating substantial improvements in JavaScript deobfuscation
efficiency and reducing reverse engineering efforts.

</details>


### [16] [Educational Insights from Code: Mapping Learning Challenges in Object-Oriented Programming through Code-Based Evidence](https://arxiv.org/abs/2507.17743)
*Andre Menolli,Bruno Strik*

Main category: cs.SE

TL;DR: 本研究探索了面向对象编程中代码层面问题（如代码异味和SOLID原则违反）与学生学习困难之间的关系，开发了一个概念模型来连接代码问题与特定的学习挑战。


<details>
  <summary>Details</summary>
Motivation: 面向对象编程对计算机科学本科生来说具有挑战性，特别是在理解封装、继承和多态等抽象概念方面。虽然文献中有各种方法来识别面向对象编程中的潜在设计和编码问题，但很少有研究探索这些代码层面的问题如何与面向对象编程的学习困难相关联。

Method: 采用定性分析方法识别学习困难的主要类别，通过文献综述建立学习困难、代码异味和SOLID原则违反之间的联系，开发了一个连接代码相关问题与面向对象编程特定学习挑战的概念图。

Result: 成功开发了一个概念模型，该模型将代码相关问题与面向对象编程中的特定学习挑战联系起来。模型经过专家评估，专家将其应用于学生代码分析中。

Conclusion: 研究建立了代码问题指标与面向对象编程学习困难之间的关系，开发的概念模型在教育环境中具有相关性和适用性，可以用于分析学生代码并识别学习挑战。

Abstract: Object-Oriented programming is frequently challenging for undergraduate
Computer Science students, particularly in understanding abstract concepts such
as encapsulation, inheritance, and polymorphism. Although the literature
outlines various methods to identify potential design and coding issues in
object-oriented programming through source code analysis, such as code smells
and SOLID principles, few studies explore how these code-level issues relate to
learning difficulties in Object-Oriented Programming. In this study, we explore
the relationship of the code issue indicators with common challenges
encountered during the learning of object-oriented programming. Using
qualitative analysis, we identified the main categories of learning
difficulties and, through a literature review, established connections between
these difficulties, code smells, and violations of the SOLID principles. As a
result, we developed a conceptual map that links code-related issues to
specific learning challenges in Object-Oriented Programming. The model was then
evaluated by an expert who applied it in the analysis of the student code to
assess its relevance and applicability in educational contexts.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [17] [CASPER: Contrastive Approach for Smart Ponzi Scheme Detecter with More Negative Samples](https://arxiv.org/abs/2507.16840)
*Weijia Yang,Tian Lan,Leyuan Liu,Wei Chen,Tianqing Zhu,Sheng Wen,Xiaosong Zhang*

Main category: cs.CR

TL;DR: 本文提出了CASPER框架，一种基于对比学习的智能庞氏骗局检测方法，能够在标注数据稀缺的情况下有效识别区块链中的智能合约欺诈行为。


<details>
  <summary>Details</summary>
Motivation: 传统的基于深度学习的庞氏骗局检测方法依赖全监督模型，需要大量标注数据，但此类数据往往稀缺，阻碍了有效的模型训练。随着区块链技术的快速发展，智能庞氏骗局问题日益严重，亟需更高效的检测方法。

Method: 提出CASPER（Contrastive Approach for Smart Ponzi detectER with more negative samples）框架，采用对比学习技术从智能合约源代码中学习更有效的表示，利用无标注数据集降低操作成本和系统复杂性，通过增加负样本来增强检测性能。

Result: 在XBlock数据集上的评估显示，使用100%标注数据时CASPER比基线方法的F1分数提高2.3%；更重要的是，仅使用25%标注数据时，CASPER的F1分数比相同实验条件下的基线方法高出近20%。

Conclusion: CASPER展现了在智能庞氏骗局检测方面的巨大潜力，能够实现有效且成本高效的检测，为未来可扩展的欺诈检测解决方案铺平了道路，特别是在标注数据稀缺的现实场景中具有重要应用价值。

Abstract: The rapid evolution of digital currency trading, fueled by the integration of
blockchain technology, has led to both innovation and the emergence of smart
Ponzi schemes. A smart Ponzi scheme is a fraudulent investment operation in
smart contract that uses funds from new investors to pay returns to earlier
investors. Traditional Ponzi scheme detection methods based on deep learning
typically rely on fully supervised models, which require large amounts of
labeled data. However, such data is often scarce, hindering effective model
training. To address this challenge, we propose a novel contrastive learning
framework, CASPER (Contrastive Approach for Smart Ponzi detectER with more
negative samples), designed to enhance smart Ponzi scheme detection in
blockchain transactions. By leveraging contrastive learning techniques, CASPER
can learn more effective representations of smart contract source code using
unlabeled datasets, significantly reducing both operational costs and system
complexity. We evaluate CASPER on the XBlock dataset, where it outperforms the
baseline by 2.3% in F1 score when trained with 100% labeled data. More
impressively, with only 25% labeled data, CASPER achieves an F1 score nearly
20% higher than the baseline under identical experimental conditions. These
results highlight CASPER's potential for effective and cost-efficient detection
of smart Ponzi schemes, paving the way for scalable fraud detection solutions
in the future.

</details>


### [18] [SynthCTI: LLM-Driven Synthetic CTI Generation to enhance MITRE Technique Mapping](https://arxiv.org/abs/2507.16852)
*Álvaro Ruiz-Ródenas,Jaime Pujante Sáez,Daniel García-Algora,Mario Rodríguez Béjar,Jorge Blasco,José Luis Hernández-Ramos*

Main category: cs.CR

TL;DR: 本文提出SynthCTI框架，通过生成高质量合成数据解决网络威胁情报挖掘中的数据稀缺和类别不平衡问题，显著提升了MITRE ATT&CK技术分类的性能表现。


<details>
  <summary>Details</summary>
Motivation: 网络威胁情报挖掘中将威胁描述映射到MITRE ATT&CK技术的过程通常需要手动完成，需要专家知识和大量工作。自动化方法面临两个主要挑战：高质量标注CTI数据稀缺和类别不平衡问题（许多技术只有很少的样本）。现有工作主要关注模型架构而非解决数据限制问题。

Method: 提出SynthCTI数据增强框架，使用基于聚类的策略从训练数据中提取语义上下文，指导大语言模型生成词汇多样且语义忠实的合成CTI句子，专门针对代表性不足的MITRE ATT&CK技术进行数据增强。

Result: 在CTI-to-MITRE和TRAM两个公开CTI数据集上评估，结果显示融入合成数据能够持续提升macro-F1得分：ALBERT从0.35提升到0.52（相对提升48.6%），SecureBERT从0.4412提升到0.6558。使用SynthCTI增强的小模型甚至超越了未使用增强的大模型。

Conclusion: SynthCTI框架有效解决了CTI挖掘中的数据稀缺和类别不平衡问题，通过数据生成方法能够构建高效且有效的CTI分类系统。研究证明了数据增强方法在网络威胁情报分析中的价值，为构建实用的自动化威胁分析系统提供了新的解决方案。

Abstract: Cyber Threat Intelligence (CTI) mining involves extracting structured
insights from unstructured threat data, enabling organizations to understand
and respond to evolving adversarial behavior. A key task in CTI mining is
mapping threat descriptions to MITRE ATT\&CK techniques. However, this process
is often performed manually, requiring expert knowledge and substantial effort.
Automated approaches face two major challenges: the scarcity of high-quality
labeled CTI data and class imbalance, where many techniques have very few
examples. While domain-specific Large Language Models (LLMs) such as SecureBERT
have shown improved performance, most recent work focuses on model architecture
rather than addressing the data limitations. In this work, we present SynthCTI,
a data augmentation framework designed to generate high-quality synthetic CTI
sentences for underrepresented MITRE ATT\&CK techniques. Our method uses a
clustering-based strategy to extract semantic context from training data and
guide an LLM in producing synthetic CTI sentences that are lexically diverse
and semantically faithful. We evaluate SynthCTI on two publicly available CTI
datasets, CTI-to-MITRE and TRAM, using LLMs with different capacity.
Incorporating synthetic data leads to consistent macro-F1 improvements: for
example, ALBERT improves from 0.35 to 0.52 (a relative gain of 48.6\%), and
SecureBERT reaches 0.6558 (up from 0.4412). Notably, smaller models augmented
with SynthCTI outperform larger models trained without augmentation,
demonstrating the value of data generation methods for building efficient and
effective CTI classification systems.

</details>


### [19] [Building a robust OAuth token based API Security: A High level Overview](https://arxiv.org/abs/2507.16870)
*Senthilkumar Gopal*

Main category: cs.CR

TL;DR: 本文介绍了构建基于令牌的API安全系统的基础知识，涵盖OAuth 2.0集成、令牌架构、加密基础和持久化策略，为开发者提供构建安全可扩展API认证系统的指导原则。


<details>
  <summary>Details</summary>
Motivation: 随着API的广泛应用，安全挑战日益突出，需要系统性和可扩展的解决方案来实现安全的身份验证和授权机制。

Method: 提出基于令牌的API安全系统架构，整合OAuth 2.0标准，设计可扩展的令牌架构，结合加密基础和持久化策略，并制定令牌生命周期管理、作用域定义、过期策略和撤销机制的最佳实践。

Result: 建立了一套完整的API令牌认证系统构建指南，平衡了实用性考虑与安全要求，运用CIA三要素、OAuth标准、安全令牌生命周期等关键概念，为保护敏感用户和应用数据提供了实践框架。

Conclusion: 通过遵循这些原则，开发者可以建立稳健的基线系统，同时保持定制化满足特定领域需求的灵活性，为应对不断演变的威胁环境做好准备。

Abstract: APIs (Application Programming Interfaces) or Web Services are the
foundational building blocks that enable interconnected systems. However this
proliferation of APIs has also introduced security challenges that require
systematic and scalable solutions for secure authentication and authorization.
This paper presents the fundamentals necessary for building a such a
token-based API security system. It discusses the components necessary, the
integration of OAuth 2.0, extensibility of the token architectures, necessary
cryptographic foundations, and persistence strategies to ensure secure and
resilient operations. In addition to architectural concerns, the paper explores
best practices for token lifecycle management, scope definition, expiration
policies, and revocation mechanisms, all framed within a real-world scenario.
By adhering to these principles, developers can establish a robust baseline
while maintaining the flexibility to customize their domain-specific
requirements. The approach does not claim to cover all variations necessary for
diverse architectures but instead focuses on key principles essential for any
standard API token authentication system. Throughout, the paper emphasizes
balancing practical considerations with security imperatives and uses key
concepts such as the CIA triad, OAuth standards, secure token life cycle, and
practices for protecting sensitive user and application data. The intent is to
equip developers with the foundational knowledge necessary to build secure,
scalable token-based API security systems ready to handle the evolving threat
landscape.

</details>


### [20] [CompLeak: Deep Learning Model Compression Exacerbates Privacy Leakage](https://arxiv.org/abs/2507.16872)
*Na Li,Yansong Gao,Hongsheng Hu,Boyu Kuang,Anmin Fu*

Main category: cs.CR

TL;DR: 本文提出CompLeak框架，首次系统评估模型压缩（剪枝、量化、权重聚类）对隐私安全的影响，发现压缩模型存在成员推理攻击风险，并设计了三种攻击变体来评估不同场景下的隐私泄露程度。


<details>
  <summary>Details</summary>
Motivation: 现有模型压缩技术主要关注资源效率与模型性能的权衡，但忽视了压缩过程可能引入的隐私风险。随着大型语言模型等基础模型的广泛应用，用户根据资源和预算选择不同压缩版本的模型，但这些压缩模型的隐私安全性尚未得到充分理解和评估。

Method: 提出CompLeak隐私风险评估框架，通过成员推理攻击的视角评估三种主流压缩技术（剪枝、量化、权重聚类）的隐私风险。框架包含三个变体：CompLeakNR（单个压缩模型攻击）、CompLeakSR（利用原模型和压缩模型的元信息）、CompLeakMR（利用多个压缩模型版本的隐私泄露信息）。

Result: 在七种不同模型架构（从ResNet到BERT、GPT-2等基础模型）和六个图像与文本基准数据集上进行了广泛实验。结果表明压缩模型对成员和非成员数据的影响不同，多个压缩版本能够显著放大整体隐私泄露程度。

Conclusion: 模型压缩过程会引入显著的隐私风险，不同压缩配置对成员推理攻击的脆弱性不同。CompLeak框架能够有效评估压缩模型的隐私泄露程度，为模型压缩的隐私保护提供了重要的评估工具和安全指导。

Abstract: Model compression is crucial for minimizing memory storage and accelerating
inference in deep learning (DL) models, including recent foundation models like
large language models (LLMs). Users can access different compressed model
versions according to their resources and budget. However, while existing
compression operations primarily focus on optimizing the trade-off between
resource efficiency and model performance, the privacy risks introduced by
compression remain overlooked and insufficiently understood.
  In this work, through the lens of membership inference attack (MIA), we
propose CompLeak, the first privacy risk evaluation framework examining three
widely used compression configurations that are pruning, quantization, and
weight clustering supported by the commercial model compression framework of
Google's TensorFlow-Lite (TF-Lite) and Facebook's PyTorch Mobile. CompLeak has
three variants, given available access to the number of compressed models and
original model. CompLeakNR starts by adopting existing MIA methods to attack a
single compressed model, and identifies that different compressed models
influence members and non-members differently. When the original model and one
compressed model are available, CompLeakSR leverages the compressed model as a
reference to the original model and uncovers more privacy by combining meta
information (e.g., confidence vector) from both models. When multiple
compressed models are available with/without accessing the original model,
CompLeakMR innovatively exploits privacy leakage info from multiple compressed
versions to substantially signify the overall privacy leakage. We conduct
extensive experiments on seven diverse model architectures (from ResNet to
foundation models of BERT and GPT-2), and six image and textual benchmark
datasets.

</details>


### [21] [Revisiting Pre-trained Language Models for Vulnerability Detection](https://arxiv.org/abs/2507.16887)
*Youpeng Li,Weiliang Qi,Xuyu Wang,Fuxun Yu,Xinda Wang*

Main category: cs.CR

TL;DR: 本文对17个预训练语言模型在漏洞检测任务上进行了全面评估，发现专门针对代码语法语义模式预训练的模型表现最佳，但在真实场景中仍面临复杂依赖、代码变换和上下文窗口限制等挑战。


<details>
  <summary>Details</summary>
Motivation: 现有预训练语言模型在漏洞检测方面的评估研究存在数据准备、评估设置和实验配置不充分的问题，影响了评估的准确性和全面性，因此需要对PLMs在真实漏洞检测场景中的有效性进行深入评估。

Method: 构建RevisitVD评估框架，对17个不同规模的预训练语言模型进行全面评估，包括微调和提示工程两种方式，测试模型在不同训练测试设置下的效果和泛化能力，并分析模型对代码规范化、抽象化和语义保持变换的鲁棒性。

Result: 专门为捕获代码语法语义模式而设计预训练任务的PLMs表现最佳，超过了通用PLMs和仅在大型代码语料库上预训练的模型。但这些模型在检测复杂依赖漏洞、处理代码变换扰动、识别语义保持的漏洞代码变换方面存在明显挑战，PLMs的上下文窗口限制也会导致标注错误。

Conclusion: 研究强调了在实际场景中对模型性能进行全面评估的重要性，指出了当前PLMs在真实漏洞检测应用中的局限性，并为未来提升PLMs在现实漏洞检测应用中的有效性提供了方向指导。

Abstract: The rapid advancement of pre-trained language models (PLMs) has demonstrated
promising results for various code-related tasks. However, their effectiveness
in detecting real-world vulnerabilities remains a critical challenge. % for the
security community. While existing empirical studies evaluate PLMs for
vulnerability detection (VD), their inadequate consideration in data
preparation, evaluation setups, and experimental settings undermines the
accuracy and comprehensiveness of evaluations. This paper introduces RevisitVD,
an extensive evaluation of 17 PLMs spanning smaller code-specific PLMs and
large-scale PLMs using newly constructed datasets. Specifically, we compare the
performance of PLMs under both fine-tuning and prompt engineering, assess their
effectiveness and generalizability across various training and testing
settings, and analyze their robustness against code normalization, abstraction,
and semantic-preserving transformations.
  Our findings reveal that, for VD tasks, PLMs incorporating pre-training tasks
designed to capture the syntactic and semantic patterns of code outperform both
general-purpose PLMs and those solely pre-trained or fine-tuned on large code
corpora. However, these models face notable challenges in real-world scenarios,
such as difficulties in detecting vulnerabilities with complex dependencies,
handling perturbations introduced by code normalization and abstraction, and
identifying semantic-preserving vulnerable code transformations. Also, the
truncation caused by the limited context windows of PLMs can lead to a
non-negligible amount of labeling errors. This study underscores the importance
of thorough evaluations of model performance in practical scenarios and
outlines future directions to help enhance the effectiveness of PLMs for
realistic VD applications.

</details>


### [22] [Evaluating Ensemble and Deep Learning Models for Static Malware Detection with Dimensionality Reduction Using the EMBER Dataset](https://arxiv.org/abs/2507.16952)
*Md Min-Ha-Zul Abedin,Tazqia Mehrub*

Main category: cs.CR

TL;DR: 本研究评估了8种机器学习算法在EMBER数据集上的静态恶意软件检测性能，发现LightGBM和XGBoost等集成方法表现最佳，并提供了不同预处理策略的性能基准。


<details>
  <summary>Details</summary>
Motivation: 现有的静态恶意软件检测研究缺乏对多种机器学习算法在不同预处理策略下的系统性比较和基准测试，需要为实际部署提供可靠的模型选择指导。

Method: 使用EMBER数据集评估8种分类模型（LightGBM、XGBoost、CatBoost、随机森林、Extra Trees、HistGradientBoosting、KNN和TabNet），在三种预处理设置下（原始特征空间、PCA、LDA）进行测试，并通过探索性数据分析（包括互信息排序、PCA/t-SNE可视化、孤立森林和LOF异常检测）支持分析。

Result: 集成方法（特别是LightGBM和XGBoost）在所有配置下表现最佳，对PCA敏感性最小且泛化能力强；LDA提高了KNN性能但显著降低了提升模型的准确性；TabNet在特征降维下表现不佳；boosting模型在高维静态恶意软件检测中最可靠。

Conclusion: boosting模型仍然是高维静态恶意软件检测的最可靠选择，降维技术应根据模型类型有选择性地应用。研究为恶意软件检测任务中的分类模型和预处理策略比较提供了基准，为未来系统开发和实际部署提供了指导。

Abstract: This study investigates the effectiveness of several machine learning
algorithms for static malware detection using the EMBER dataset, which contains
feature representations of Portable Executable (PE) files. We evaluate eight
classification models: LightGBM, XGBoost, CatBoost, Random Forest, Extra Trees,
HistGradientBoosting, k-Nearest Neighbors (KNN), and TabNet, under three
preprocessing settings: original feature space, Principal Component Analysis
(PCA), and Linear Discriminant Analysis (LDA). The models are assessed on
accuracy, precision, recall, F1 score, and AUC to examine both predictive
performance and robustness. Ensemble methods, especially LightGBM and XGBoost,
show the best overall performance across all configurations, with minimal
sensitivity to PCA and consistent generalization. LDA improves KNN performance
but significantly reduces accuracy for boosting models. TabNet, while promising
in theory, underperformed under feature reduction, likely due to architectural
sensitivity to input structure. The analysis is supported by detailed
exploratory data analysis (EDA), including mutual information ranking, PCA or
t-SNE visualizations, and outlier detection using Isolation Forest and Local
Outlier Factor (LOF), which confirm the discriminatory capacity of key features
in the EMBER dataset. The results suggest that boosting models remain the most
reliable choice for high-dimensional static malware detection, and that
dimensionality reduction should be applied selectively based on model type.
This work provides a benchmark for comparing classification models and
preprocessing strategies in malware detection tasks and contributes insights
that can guide future system development and real-world deployment.

</details>


### [23] [From Cracks to Crooks: YouTube as a Vector for Malware Distribution](https://arxiv.org/abs/2507.16996)
*Iman Vakilinia*

Main category: cs.CR

TL;DR: 本文研究了网络犯罪分子如何利用YouTube这一拥有数十亿用户的平台传播恶意软件，特别关注通过免费软件或游戏作弊工具进行的欺骗性活动，并发现了一种滥用YouTube多语言元数据功能来规避自动检测系统的新型逃避技术。


<details>
  <summary>Details</summary>
Motivation: YouTube作为拥有数十亿用户和海量日常上传内容的平台，其开放性和可信度为网络犯罪分子提供了理想的环境来进行大规模欺骗性活动，这些活动往往能够逃避传统安全工具的检测，因此需要深入研究其恶意软件传播机制。

Method: 通过分析网络犯罪分子在YouTube上的恶意活动模式，重点研究推广免费软件或游戏作弊工具的欺骗性视频演示和恶意软件传递技术，并深入调查滥用YouTube多语言元数据功能来规避自动检测系统的新型逃避技术。

Result: 研究发现网络犯罪分子正在越来越多地使用滥用YouTube多语言元数据功能的方法来避免检测和移除，这种新型逃避技术在最近的恶意视频中被广泛采用，能够有效规避自动化检测系统。

Conclusion: YouTube平台的开放性和庞大用户基础使其成为网络犯罪分子传播恶意软件的重要渠道，特别是通过滥用平台的多语言元数据功能，攻击者能够开发出新的逃避技术来规避检测，这一发现对于改进YouTube的安全防护机制具有重要意义。

Abstract: With billions of users and an immense volume of daily uploads, YouTube has
become an attractive target for cybercriminals aiming to leverage its vast
audience. The platform's openness and trustworthiness provide an ideal
environment for deceptive campaigns that can operate under the radar of
conventional security tools. This paper explores how cybercriminals exploit
YouTube to disseminate malware, focusing on campaigns that promote free
software or game cheats. It discusses deceptive video demonstrations and the
techniques behind malware delivery. Additionally, the paper presents a new
evasion technique that abuses YouTube's multilingual metadata capabilities to
circumvent automated detection systems. Findings indicate that this method is
increasingly being used in recent malicious videos to avoid detection and
removal.

</details>


### [24] [The Postman: A Journey of Ethical Hacking in PosteID/SPID Borderland](https://arxiv.org/abs/2507.17007)
*Gabriele Costa*

Main category: cs.CR

TL;DR: 本文描述了对意大利邮政公司实施的公共数字身份系统PosteID进行漏洞评估的过程，发现了一个关键的权限提升漏洞并最终被修复，为道德黑客社区提供了有价值的案例研究。


<details>
  <summary>Details</summary>
Motivation: 对意大利公共数字身份系统PosteID进行安全评估，发现潜在的安全漏洞，并为道德黑客社区提供漏洞披露流程的案例研究。

Method: 对PosteID系统进行漏洞评估活动，包括技术分析步骤和漏洞披露流程的详细记录。

Result: 成功发现了一个关键的权限提升漏洞，该漏洞最终得到了修复。整个分析和披露过程为道德黑客社区提供了有价值的案例研究。

Conclusion: 通过对PosteID系统的漏洞评估，不仅发现并修复了关键安全漏洞，还为道德黑客社区提供了完整的技术分析和负责任漏洞披露流程的宝贵经验。

Abstract: This paper presents a vulnerability assessment activity that we carried out
on PosteID, the implementation of the Italian Public Digital Identity System
(SPID) by Poste Italiane. The activity led to the discovery of a critical
privilege escalation vulnerability, which was eventually patched. The overall
analysis and disclosure process represents a valuable case study for the
community of ethical hackers. In this work, we present both the technical steps
and the details of the disclosure process.

</details>


### [25] [Towards Trustworthy AI: Secure Deepfake Detection using CNNs and Zero-Knowledge Proofs](https://arxiv.org/abs/2507.17010)
*H M Mohaimanul Islam,Huynh Q. N. Vo,Aditya Rane*

Main category: cs.CR

TL;DR: TrustDefender是一个两阶段框架，结合轻量级CNN和零知识证明协议，用于在扩展现实(XR)流中实时检测深度伪造内容，同时保护用户隐私，实现了95.3%的检测准确率。


<details>
  <summary>Details</summary>
Motivation: 在合成媒体时代，深度伪造技术对信息完整性构成重大威胁，需要在XR平台的计算约束下，开发既能实时检测深度伪造内容又能满足敏感环境中严格隐私要求的解决方案。

Method: 提出TrustDefender两阶段框架：(1)使用轻量级卷积神经网络(CNN)在扩展现实(XR)流中实时检测深度伪造图像；(2)集成简洁零知识证明(ZKP)协议来验证检测结果，同时不泄露原始用户数据。

Result: 在多个基准深度伪造数据集上的实验评估显示，TrustDefender达到了95.3%的检测准确率，同时具有高效的证明生成能力，基于严格的密码学理论，确保与高性能人工智能系统的无缝集成。

Conclusion: 通过融合先进的计算机视觉模型和可证明的安全机制，该工作为沉浸式和隐私敏感应用中的可靠AI建立了基础，成功解决了实时深度伪造检测与隐私保护的双重挑战。

Abstract: In the era of synthetic media, deepfake manipulations pose a significant
threat to information integrity. To address this challenge, we propose
TrustDefender, a two-stage framework comprising (i) a lightweight convolutional
neural network (CNN) that detects deepfake imagery in real-time extended
reality (XR) streams, and (ii) an integrated succinct zero-knowledge proof
(ZKP) protocol that validates detection results without disclosing raw user
data. Our design addresses both the computational constraints of XR platforms
while adhering to the stringent privacy requirements in sensitive settings.
Experimental evaluations on multiple benchmark deepfake datasets demonstrate
that TrustDefender achieves 95.3% detection accuracy, coupled with efficient
proof generation underpinned by rigorous cryptography, ensuring seamless
integration with high-performance artificial intelligence (AI) systems. By
fusing advanced computer vision models with provable security mechanisms, our
work establishes a foundation for reliable AI in immersive and
privacy-sensitive applications.

</details>


### [26] [GATEBLEED: Exploiting On-Core Accelerator Power Gating for High Performance & Stealthy Attacks on AI](https://arxiv.org/abs/2507.17033)
*Joshua Kalyanapu,Farshad Dizani,Darsh Asher,Azam Ghanbari,Rosario Cammarota,Aydin Aysu,Samira Mirbagher Ajorpaz*

Main category: cs.CR

TL;DR: 研究人员发现了Intel AMX加速器中的GATEBLEED侧信道攻击，该攻击利用功耗门控机制的时序差异来泄露AI模型的隐私信息，在变压器模型上实现81%的成员推理准确率，在专家混合模型上达到100%的专家选择泄露准确率


<details>
  <summary>Details</summary>
Motivation: 随着AI训练和推理功耗持续增加，AI加速器被直接集成到CPU中（如Intel AMX），但这种集成带来了新的安全隐患，需要研究其对AI隐私保护的潜在威胁

Method: 利用Intel AMX中激进的功耗门控机制造成的时序差异，通过分析矩阵乘法操作的执行时间来推断私有信息，识别了流行ML库中的十多个潜在泄露点，并实现了端到端的微架构推理攻击

Result: 在使用Intel AMX优化的变压器模型上实现81%的成员推理准确率和0.89的精确度；在CNN或变压器的专家混合模型中以100%的准确率泄露专家选择信息；证明了GATEBLEED可以绕过传统缓存防御并逃避最先进的微架构攻击检测器

Conclusion: GATEBLEED侧信道攻击对AI隐私构成严重威胁，能够通过时序分析泄露ML模型的敏感信息，即使在现有保护措施下仍然有效，需要开发新的防御机制来应对这种基于功耗门控的攻击

Abstract: As power consumption from AI training and inference continues to increase, AI
accelerators are being integrated directly into the CPU. Intel's Advanced
Matrix Extensions (AMX) is one such example, debuting on the 4th generation
Intel Xeon Scalable CPU. We discover a timing side and covert channel,
GATEBLEED, caused by the aggressive power gating utilized to keep the CPU
within operating limits. We show that the GATEBLEED side channel is a threat to
AI privacy as many ML models such as transformers and CNNs make critical
computationally-heavy decisions based on private values like confidence
thresholds and routing logits. Timing delays from selective powering down of
AMX components mean that each matrix multiplication is a potential leakage
point when executed on the AMX accelerator. Our research identifies over a
dozen potential gadgets across popular ML libraries (HuggingFace, PyTorch,
TensorFlow, etc.), revealing that they can leak sensitive and private
information. GATEBLEED poses a risk for local and remote timing inference, even
under previous protective measures. GATEBLEED can be used as a high
performance, stealthy remote covert channel and a generic magnifier for timing
transmission channels, capable of bypassing traditional cache defenses to leak
arbitrary memory addresses and evading state of the art microarchitectural
attack detectors under realistic network conditions and system configurations
in which previous attacks fail. We implement an end-to-end microarchitectural
inference attack on a transformer model optimized with Intel AMX, achieving a
membership inference accuracy of 81% and a precision of 0.89. In a CNN-based or
transformer-based mixture-of-experts model optimized with Intel AMX, we leak
expert choice with 100% accuracy.

</details>


### [27] [SoK: Securing the Final Frontier for Cybersecurity in Space-Based Infrastructure](https://arxiv.org/abs/2507.17064)
*Nafisa Anjum,Tasnuva Farheen*

Main category: cs.CR

TL;DR: 该研究对太空网络攻击向量进行了全面分析，评估了缓解措施的有效性，并提出了风险评分框架，为太空基础设施的网络安全防护提供了系统性解决方案。


<details>
  <summary>Details</summary>
Motivation: 随着现代技术的发展，关键基础设施、通信和国家安全越来越依赖太空资产，这些资产面临严重的网络攻击威胁。以往研究缺乏对太空网络攻击向量的全面检查和缓解技术有效性的严格评估。

Method: 采用综合性方法分析可能的太空网络攻击向量，包括地面、太空、卫星和卫星星座等各个层面；评估与太空基础设施相关的缓解措施的有效性；提出风险评分框架来应对特定威胁。

Result: 识别了包括地面、太空、卫星和卫星星座在内的各种太空网络攻击向量；评估了相应缓解措施的有效性；建立了针对太空基础设施威胁的风险评分框架；确定了开发和测试前沿技术解决方案的潜在研究挑战。

Conclusion: 通过全面分析太空网络攻击向量和评估缓解措施，该研究为太空基础设施提供了系统性的网络安全防护方案，并为未来发展强大的太空网络安全措施指明了研究方向和挑战。

Abstract: With the advent of modern technology, critical infrastructure,
communications, and national security depend increasingly on space-based
assets. These assets, along with associated assets like data relay systems and
ground stations, are, therefore, in serious danger of cyberattacks. Strong
security defenses are essential to ensure data integrity, maintain secure
operations, and protect assets in space and on the ground against various
threats. Previous research has found discrete vulnerabilities in space systems
and suggested specific solutions to address them. Such research has yielded
valuable insights, but lacks a thorough examination of space cyberattack
vectors and a rigorous assessment of the efficacy of mitigation techniques.
This study tackles this issue by taking a comprehensive approach to analyze the
range of possible space cyber-attack vectors, which include ground, space,
satellite, and satellite constellations. In order to address the particular
threats, the study also assesses the efficacy of mitigation measures that are
linked with space infrastructures and proposes a Risk Scoring Framework. Based
on the analysis, this paper identifies potential research challenges for
developing and testing cutting-edge technology solutions, encouraging robust
cybersecurity measures needed in space.

</details>


### [28] [Analysis of Post-Quantum Cryptography in User Equipment in 5G and Beyond](https://arxiv.org/abs/2507.17074)
*Sanzida Hoque,Abdullah Aydeger,Engin Zeydan,Madhusanka Liyanage*

Main category: cs.CR

TL;DR: 本文在5G网络环境中实现并评估了NIST选定的后量子密码学算法，发现ML-KEM与ML-DSA组合在延迟敏感应用中表现最佳，而SPHINCS+和HQC组合由于计算和传输开销较高，不适合对时间敏感的5G安全场景。


<details>
  <summary>Details</summary>
Motivation: 量子计算的出现威胁到经典公钥密码系统的安全性，促使向后量子密码学(PQC)转型。虽然PQC在理论上已被分析，但其在实际无线通信环境中的性能仍未得到充分探索，特别是在5G网络中的实际应用表现。

Method: 使用完整的5G仿真栈(Open5GS和UERANSIM)以及通过BoringSSL和liboqs实现的支持PQC的TLS 1.3，在真实网络条件下测试用户设备间通信。评估了密钥封装机制和数字签名方案，通过握手延迟、CPU和内存使用、带宽和重传率等指标，在不同密码学配置和客户端负载下进行性能分析。

Result: ML-KEM与ML-DSA组合在延迟敏感应用中提供了最佳效率，而SPHINCS+和HQC组合产生了更高的计算和传输开销。研究揭示了不同PQC算法在5G网络环境中的性能差异和适用性。

Conclusion: 对于对时间敏感但安全关键的5G场景，SPHINCS+和HQC组合由于其高计算和传输开销而不适用，ML-KEM与ML-DSA组合是延迟敏感应用的最佳选择。这为5G网络中PQC算法的实际部署提供了重要的性能参考。

Abstract: The advent of quantum computing threatens the security of classical
public-key cryptographic systems, prompting the transition to post-quantum
cryptography (PQC). While PQC has been analyzed in theory, its performance in
practical wireless communication environments remains underexplored. This paper
presents a detailed implementation and performance evaluation of NIST-selected
PQC algorithms in user equipment (UE) to UE communications over 5G networks.
Using a full 5G emulation stack (Open5GS and UERANSIM) and PQC-enabled TLS 1.3
via BoringSSL and liboqs, we examine key encapsulation mechanisms and digital
signature schemes across realistic network conditions. We evaluate performance
based on handshake latency, CPU and memory usage, bandwidth, and retransmission
rates, under varying cryptographic configurations and client loads. Our
findings show that ML-KEM with ML-DSA offers the best efficiency for
latency-sensitive applications, while SPHINCS+ and HQC combinations incur
higher computational and transmission overheads, making them unsuitable for
security-critical but time-sensitive 5G scenarios.

</details>


### [29] [A Privacy-Preserving Data Collection Method for Diversified Statistical Analysis](https://arxiv.org/abs/2507.17180)
*Hao Jiang,Quan Zhou,Dongdong Zhao,Shangshang Yang,Wenjian Luo,Xingyi Zhang*

Main category: cs.CR

TL;DR: 本文提出了RVNS模型，首次将负调查方法扩展到实数值敏感信息收集领域，解决了现有隐私保护方法忽视数据分布质量的问题，在保护个人隐私的同时准确捕获敏感信息的分布特征。


<details>
  <summary>Details</summary>
Motivation: 现有基于数据扰动的隐私保护方法主要关注个体统计指标，忽视了从分布角度收集数据的整体质量，难以满足实际数据分析中多样化的统计分析需求。现有负调查方法主要针对离散敏感信息设计，不适用于实数值数据分布。

Method: 提出了RVNS（实数值负调查）模型，用户无需离散化数据，只需从偏离其真实敏感信息的范围内采样一组数据。构建优化问题来准确捕获敏感信息分布，并采用新颖方法求解。

Result: 理论分析证明RVNS模型符合差分隐私模型，确保强健的隐私保护。在合成数据集和真实数据集上的综合实验验证了所提方法的有效性。

Conclusion: RVNS模型成功将负调查方法扩展到实数值敏感信息收集，在保护个人隐私的同时能够准确获取敏感信息的分布特征，为实数值敏感数据的隐私保护收集提供了新的解决方案。

Abstract: Data perturbation-based privacy-preserving methods have been widely adopted
in various scenarios due to their efficiency and the elimination of the need
for a trusted third party. However, these methods primarily focus on individual
statistical indicators, neglecting the overall quality of the collected data
from a distributional perspective. Consequently, they often fall short of
meeting the diverse statistical analysis requirements encountered in practical
data analysis. As a promising sensitive data perturbation method, negative
survey methods is able to complete the task of collecting sensitive information
distribution while protecting personal privacy. Yet, existing negative survey
methods are primarily designed for discrete sensitive information and are
inadequate for real-valued data distributions. To bridge this gap, this paper
proposes a novel real-value negative survey model, termed RVNS, for the first
time in the field of real-value sensitive information collection. The RVNS
model exempts users from the necessity of discretizing their data and only
requires them to sample a set of data from a range that deviates from their
actual sensitive details, thereby preserving the privacy of their genuine
information. Moreover, to accurately capture the distribution of sensitive
information, an optimization problem is formulated, and a novel approach is
employed to solve it. Rigorous theoretical analysis demonstrates that the RVNS
model conforms to the differential privacy model, ensuring robust privacy
preservation. Comprehensive experiments conducted on both synthetic and
real-world datasets further validate the efficacy of the proposed method.

</details>


### [30] [Threshold-Protected Searchable Sharing: Privacy Preserving Aggregated-ANN Search for Collaborative RAG](https://arxiv.org/abs/2507.17199)
*Ruoyang Rykie Guo*

Main category: cs.CR

TL;DR: 本文提出了一种安全隐私保护的聚合近似最近邻搜索方法(SP-A²NN)，支持HNSW兼容性，通过可共享位图结构实现私有数据整合，将搜索复杂度从O(n²)降低到O(n)，并建立了新的安全分析框架来解决AI发展中的隐私泄露问题。


<details>
  <summary>Details</summary>
Motivation: 大语言模型驱动的搜索服务使数据整合成为重要趋势，但受到私有数据仓库的本地化约束和需要与主流搜索技术(特别是HNSW索引)保持兼容性的阻碍。虽然整合个体知识能显著提升专业查询的相关性和质量，但现有方法存在隐私保护和效率问题。

Method: 开发了一种基于阈值可搜索共享原语的安全隐私保护聚合近似最近邻搜索(SP-A²NN)方法，构建可共享位图结构支持共享数据上的搜索和动态插入而不损害底层图拓扑。提出了基于归约的隐私分析新安全分析框架，建立了泄露猜测证明系统，采用独立于现有抛硬币游戏设计的交互式游戏。

Result: 与朴素无向图共享方法相比，当以相同HNSW方式组织图时，该方法将搜索复杂度从O(n²)降低到O(n)。建立的安全分析框架能够标准化泄露分析，专门解决泄露问题这一AI快速发展中最关键的安全挑战之一。

Conclusion: 该工作成功解决了LLM搜索服务中数据整合的关键瓶颈，通过SP-A²NN方法实现了效率和隐私保护的平衡，同时建立的新安全分析框架为AI系统的隐私泄露分析提供了理论基础和标准化方法，推动了安全AI服务的发展。

Abstract: LLM-powered search services have driven data integration as a significant
trend. However, this trend's progress is fundamentally hindered, despite the
fact that combining individual knowledge can significantly improve the
relevance and quality of responses in specialized queries and make AI more
professional at providing services. Two key bottlenecks are private data
repositories' locality constraints and the need to maintain compatibility with
mainstream search techniques, particularly Hierarchical Navigable Small World
(HNSW) indexing for high-dimensional vector spaces. In this work, we develop a
secure and privacy-preserving aggregated approximate nearest neighbor search
(SP-A$^2$NN) with HNSW compatibility under a threshold-based searchable sharing
primitive. A sharable bitgraph structure is constructed and extended to support
searches and dynamical insertions over shared data without compromising the
underlying graph topology. The approach reduces the complexity of a search from
$O(n^2)$ to $O(n)$ compared to naive (undirected) graph-sharing approach when
organizing graphs in the identical HNSW manner.
  On the theoretical front, we explore a novel security analytical framework
that incorporates privacy analysis via reductions. The proposed
leakage-guessing proof system is built upon an entirely different interactive
game that is independent of existing coin-toss game design. Rather than being
purely theoretical, this system is rooted in existing proof systems but goes
beyond them to specifically address leakage concerns and standardize leakage
analysis -- one of the most critical security challenges with AI's rapid
development.

</details>


### [31] [Tab-MIA: A Benchmark Dataset for Membership Inference Attacks on Tabular Data in LLMs](https://arxiv.org/abs/2507.17259)
*Eyal German,Sagiv Antebi,Daniel Samira,Asaf Shabtai,Yuval Elovici*

Main category: cs.CR

TL;DR: 本文提出了Tab-MIA基准数据集，用于评估大语言模型在表格数据上的成员推理攻击风险，发现LLMs在处理结构化数据时存在显著的隐私泄露风险，即使经过少量训练也容易受到攻击。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型越来越多地在包含个人身份信息的表格数据上训练，现有的成员推理攻击方法主要针对文本内容，但在结构化数据上的有效性和威胁影响可能不同，因此需要专门的基准来评估表格数据的隐私风险。

Method: 构建了Tab-MIA基准数据集，包含五个数据集合，每个都以六种不同编码格式表示；使用该基准对在表格数据上微调的LLMs进行最先进的成员推理攻击方法评估；分析预训练LLMs在维基百科表格结构化数据上的记忆行为。

Result: 发现LLMs以不同编码格式记忆表格数据的方式各异，使其容易受到成员推理攻击的数据提取；即使仅微调三个epoch，模型也表现出高度脆弱性，大多数情况下AUROC分数接近90%。

Conclusion: Tab-MIA能够系统性评估表格数据在LLMs中的隐私风险，为开发针对表格数据的隐私保护方法提供了基础，揭示了结构化数据在大语言模型中存在的显著隐私泄露问题。

Abstract: Large language models (LLMs) are increasingly trained on tabular data, which,
unlike unstructured text, often contains personally identifiable information
(PII) in a highly structured and explicit format. As a result, privacy risks
arise, since sensitive records can be inadvertently retained by the model and
exposed through data extraction or membership inference attacks (MIAs). While
existing MIA methods primarily target textual content, their efficacy and
threat implications may differ when applied to structured data, due to its
limited content, diverse data types, unique value distributions, and
column-level semantics. In this paper, we present Tab-MIA, a benchmark dataset
for evaluating MIAs on tabular data in LLMs and demonstrate how it can be used.
Tab-MIA comprises five data collections, each represented in six different
encoding formats. Using our Tab-MIA benchmark, we conduct the first evaluation
of state-of-the-art MIA methods on LLMs finetuned with tabular data across
multiple encoding formats. In the evaluation, we analyze the memorization
behavior of pretrained LLMs on structured data derived from Wikipedia tables.
Our findings show that LLMs memorize tabular data in ways that vary across
encoding formats, making them susceptible to extraction via MIAs. Even when
fine-tuned for as few as three epochs, models exhibit high vulnerability, with
AUROC scores approaching 90% in most cases. Tab-MIA enables systematic
evaluation of these risks and provides a foundation for developing
privacy-preserving methods for tabular data in LLMs.

</details>


### [32] [An Empirical Study on Virtual Reality Software Security Weaknesses](https://arxiv.org/abs/2507.17324)
*Yifan Xu,Jinfu Chen,Zhenyu Qi,Huashan Chen,Junyi Wang,Pengfei Hu,Feng Liu,Sen He*

Main category: cs.CR

TL;DR: 该研究首次系统性地分析了VR软件的安全漏洞，通过对GitHub上334个VR项目的1,681个安全弱点进行分析，发现VR软件存在显著的安全问题，特别是用户界面相关的漏洞最为普遍。


<details>
  <summary>Details</summary>
Motivation: 虚拟现实(VR)技术已成为跨行业的变革性技术，但其安全弱点和漏洞研究不足。由于公共数据库中VR软件安全漏洞的可用性有限，需要系统性地研究VR软件中存在哪些类型的安全弱点、它们是何时及如何引入的、存在多长时间以及如何被修复。

Method: 研究者开发了一个新颖的框架，从GitHub提交数据中收集VR软件安全弱点，构建了首个VR软件安全弱点的系统性数据集。对GitHub上334个VR项目进行调查，检查了1,681个软件安全弱点，并对数据集进行实证研究分析。

Result: 研究发现：(1) VR安全弱点严重偏向于用户界面弱点，其次是资源相关弱点；(2) VR开发工具比VR应用程序带来更高的安全风险；(3) VR安全弱点通常在VR软件诞生时就被引入。

Conclusion: 该研究提供了VR软件安全状况的重要洞察，揭示了VR软件安全弱点的分布特征和引入模式。研究结果表明VR软件存在显著的安全挑战，特别是在用户界面和开发工具方面，这些发现对VR软件的安全开发具有重要指导意义。

Abstract: Virtual Reality (VR) has emerged as a transformative technology across
industries, yet its security weaknesses, including vulnerabilities, are
underinvestigated. This study investigates 334 VR projects hosted on GitHub,
examining 1,681 software security weaknesses to understand: what types of
weaknesses are prevalent in VR software; {\em when} and {\em how} weaknesses
are introduced; how long they have survived; and how they have been removed.
Due to the limited availability of VR software security weaknesses in public
databases (e.g., the National Vulnerability Database or NVD), we prepare the
{first systematic} dataset of VR software security weaknesses by introducing a
novel framework to collect such weaknesses from GitHub commit data. Our
empirical study on the dataset leads to useful insights, including: (i) VR
weaknesses are heavily skewed toward user interface weaknesses, followed by
resource-related weaknesses; (ii) VR development tools pose higher security
risks than VR applications; (iii) VR security weaknesses are often introduced
at the VR software birth time.

</details>


### [33] [Explainable Vulnerability Detection in C/C++ Using Edge-Aware Graph Attention Networks](https://arxiv.org/abs/2507.16540)
*Radowanul Haque,Aftab Ali,Sally McClean,Naveed Khan*

Main category: cs.CR

TL;DR: ExplainVulD是一个基于图的C/C++代码漏洞检测框架，通过双通道嵌入和边感知注意力机制实现高精度检测，同时提供可解释性输出，在ReVeal数据集上相比现有方法显著提升了准确率和F1分数。


<details>
  <summary>Details</summary>
Motivation: 现有的漏洞检测方法面临类不平衡问题（漏洞函数代表性不足）、高误报率、缺乏可解释性等挑战，限制了其在实际开发工作流程和安全工作流程中的集成应用。

Method: 构建代码属性图(Code Property Graphs)，使用双通道嵌入捕获语义和结构信息，采用边感知注意力机制结合边类型嵌入来区分程序关系，通过类加权交叉熵损失解决类不平衡问题。

Result: 在ReVeal数据集上30次独立运行的平均准确率达到88.25%，F1分数为48.23%，相比ReVeal模型准确率提升4.6%，F1分数提升16.9%；相比静态分析工具准确率提升14.0-14.1%，F1分数提升132.2-201.2%。

Conclusion: ExplainVulD不仅提高了漏洞检测性能，还通过识别每个函数中最具影响力的代码区域产生可解释的输出，支持安全分类中的透明性和信任度，为实际应用提供了更好的解决方案。

Abstract: Detecting security vulnerabilities in source code remains challenging,
particularly due to class imbalance in real-world datasets where vulnerable
functions are under-represented. Existing learning-based methods often optimise
for recall, leading to high false positive rates and reduced usability in
development workflows. Furthermore, many approaches lack explainability,
limiting their integration into security workflows. This paper presents
ExplainVulD, a graph-based framework for vulnerability detection in C/C++ code.
The method constructs Code Property Graphs and represents nodes using
dual-channel embeddings that capture both semantic and structural information.
These are processed by an edge-aware attention mechanism that incorporates
edge-type embeddings to distinguish among program relations. To address class
imbalance, the model is trained using class-weighted cross-entropy loss.
ExplainVulD achieves a mean accuracy of 88.25 percent and an F1 score of 48.23
percent across 30 independent runs on the ReVeal dataset. These results
represent relative improvements of 4.6 percent in accuracy and 16.9 percent in
F1 score compared to the ReVeal model, a prior learning-based method. The
framework also outperforms static analysis tools, with relative gains of 14.0
to 14.1 percent in accuracy and 132.2 to 201.2 percent in F1 score. Beyond
improved detection performance, ExplainVulD produces explainable outputs by
identifying the most influential code regions within each function, supporting
transparency and trust in security triage.

</details>


### [34] [A Zero-overhead Flow for Security Closure](https://arxiv.org/abs/2507.17385)
*Mohammad Eslami,Ashira Johara,Kyungbin Park,Samuel Pagliarini*

Main category: cs.CR

TL;DR: 提出了一种零开销的安全感知ASIC设计流程，在不降低设计质量的前提下，同时防御硬件木马插入和物理探测/故障注入攻击。


<details>
  <summary>Details</summary>
Motivation: 传统ASIC设计流程在物理综合阶段主要关注时序收敛、面积和功耗，但忽略了安全性考虑。商用布局布线工具不理解安全目标，现有安全改进方法通常会降低设计质量。

Method: 提出了一种修改的安全感知ASIC设计流程，针对两种威胁模型：(1)硬件木马插入和(2)物理探测/故障注入。该流程完全在商用布局布线引擎内执行，具有可扩展性，并实现零开销的安全收敛。

Result: 在ISPD'22基准电路集上，该安全感知流程在多个指标上取得了最佳已知结果，同时由于安全相关策略产生的设计开销可忽略不计。整个方法论以脚本形式开源，受保护的电路以设计数据库形式共享。

Conclusion: 成功开发了首个零开销的安全收敛流程，在不牺牲传统设计质量指标的情况下实现了安全性提升，为硬件安全社区提供了可扩展的实用解决方案。

Abstract: In the traditional Application-Specific Integrated Circuit (ASIC) design
flow, the concept of timing closure implies to reach convergence during
physical synthesis such that, under a given area and power budget, the design
works at the targeted frequency. However, security has been largely neglected
when evaluating the Quality of Results (QoR) from physical synthesis. In
general, commercial place & route tools do not understand security goals. In
this work, we propose a modified ASIC design flow that is security-aware and,
differently from prior research, does not degrade QoR for the sake of security
improvement. Therefore, we propose a first-of-its-kind zero-overhead flow for
security closure. Our flow is concerned with two distinct threat models: (i)
insertion of Hardware Trojans (HTs) and (ii) physical probing/fault injection.
Importantly, the flow is entirely executed within a commercial place & route
engine and is scalable. In several metrics, our security-aware flow achieves
the best-known results for the ISPD`22 set of benchmark circuits while
incurring negligible design overheads due to security-related strategies.
Finally, we open source the entire methodology (as a set of scripts) and also
share the protected circuits (as design databases) for the benefit of the
hardware security community.

</details>


### [35] [Active Attack Resilience in 5G: A New Take on Authentication and Key Agreement](https://arxiv.org/abs/2507.17491)
*Nazatul H. Sultan,Xinlong Guan,Josef Pieprzyk,Wei Ni,Sharif Abuadbba,Hajime Suzuki*

Main category: cs.CR

TL;DR: 本文提出了一种增强的5G认证协议，解决了现有5G-AKA协议在安全性和性能方面的局限性，特别是添加了完美前向保密性并实现了无状态设计。


<details>
  <summary>Details</summary>
Motivation: 5G-AKA协议虽然被广泛采用，但存在多个关键问题：容易受到主动攻击、依赖序列号机制导致同步复杂性、缺乏完美前向保密性(PFS)，使得长期密钥泄露时历史通信面临风险。随着5G网络扩展到关键基础设施，需要更安全高效的用户认证方案。

Method: 研究团队提出了两个增强协议：首先设计无状态版本，移除对序列号的依赖，降低复杂性并保持与现有SIM卡和基础设施的兼容性；然后在此基础上扩展设计，以最小的密码学开销添加完美前向保密性。使用ProVerif工具对协议进行严格的安全性分析，并通过原型实现评估性能。

Result: ProVerif分析确认提出的协议符合所有主要安全要求，包括抗被动和主动攻击能力，以及3GPP和学术研究定义的安全标准。性能评估显示，与5G-AKA和5G-AKA'相比，新协议在提供更强安全性的同时，仅产生微小的计算开销。

Conclusion: 提出的增强认证协议成功解决了5G-AKA的关键安全和性能问题，在保持实用性的同时显著提升了安全性，为5G及未来网络提供了面向未来的认证解决方案。

Abstract: As 5G networks expand into critical infrastructure, secure and efficient user
authentication is more important than ever. The 5G-AKA protocol, standardized
by 3GPP in TS 33.501, is central to authentication in current 5G deployments.
It provides mutual authentication, user privacy, and key secrecy. However,
despite its adoption, 5G-AKA has known limitations in both security and
performance. While it focuses on protecting privacy against passive attackers,
recent studies show its vulnerabilities to active attacks. It also relies on a
sequence number mechanism to prevent replay attacks, requiring perfect
synchronization between the device and the core network. This stateful design
adds complexity, causes desynchronization, and incurs extra communication
overhead. More critically, 5G-AKA lacks Perfect Forward Secrecy (PFS), exposing
past communications if long-term keys are compromised-an increasing concern
amid sophisticated threats. This paper proposes an enhanced authentication
protocol that builds on 5G-AKA's design while addressing its shortcomings.
First, we introduce a stateless version that removes sequence number reliance,
reducing complexity while staying compatible with existing SIM cards and
infrastructure. We then extend this design to add PFS with minimal
cryptographic overhead. Both protocols are rigorously analyzed using ProVerif,
confirming their compliance with all major security requirements, including
resistance to passive and active attacks, as well as those defined by 3GPP and
academic studies. We also prototype both protocols and evaluate their
performance against 5G-AKA and 5G-AKA' (USENIX'21). Our results show the
proposed protocols offer stronger security with only minor computational
overhead, making them practical, future-ready solutions for 5G and beyond.

</details>


### [36] [Enabling Cyber Security Education through Digital Twins and Generative AI](https://arxiv.org/abs/2507.17518)
*Vita Santa Barletta,Vito Bavaro,Miriana Calvano,Antonio Curci,Antonio Piccinno,Davide Pio Posa*

Main category: cs.CR

TL;DR: 本研究探索将数字孪生(DT)技术与大语言模型(LLM)结合用于网络安全教育，通过自定义的Red Team Knife(RTK)渗透测试工具包，在模拟的网络环境中提供实时监控、威胁分析和交互式学习体验。


<details>
  <summary>Details</summary>
Motivation: 传统网络安全教育存在理论与实践脱节的问题，需要一个能够模拟真实复杂IT/OT/IoT基础设施的实践框架，以提高网络安全教育的有效性和相关性，满足不断演变的行业需求。

Method: 构建了一个结合数字孪生技术和大语言模型的框架：(1)开发了基于网络杀伤链模型的Red Team Knife(RTK)自定义渗透测试工具包；(2)利用数字孪生技术复制复杂的IT/OT/IoT基础设施；(3)集成大语言模型提供智能实时反馈、自然语言威胁解释和自适应学习支持；(4)在学术环境中进行试点测试。

Result: 初步研究结果表明，数字孪生与大语言模型的集成显著提高了网络安全培训的有效性和相关性，成功弥合了理论知识与实际应用之间的差距，在漏洞评估、威胁检测和安全操作方面提供了有效的实践技能培训。

Conclusion: 数字孪生技术与大语言模型的结合能够变革网络安全教育，通过提供实用的交互式框架来探索漏洞和防御策略，有效满足不断发展的行业需求，为网络安全教育和操作准备提供了新的解决方案。

Abstract: Digital Twins (DTs) are gaining prominence in cybersecurity for their ability
to replicate complex IT (Information Technology), OT (Operational Technology),
and IoT (Internet of Things) infrastructures, allowing for real time
monitoring, threat analysis, and system simulation. This study investigates how
integrating DTs with penetration testing tools and Large Language Models (LLMs)
can enhance cybersecurity education and operational readiness. By simulating
realistic cyber environments, this approach offers a practical, interactive
framework for exploring vulnerabilities and defensive strategies. At the core
of this research is the Red Team Knife (RTK), a custom penetration testing
toolkit aligned with the Cyber Kill Chain model. RTK is designed to guide
learners through key phases of cyberattacks, including reconnaissance,
exploitation, and response within a DT powered ecosystem. The incorporation of
Large Language Models (LLMs) further enriches the experience by providing
intelligent, real-time feedback, natural language threat explanations, and
adaptive learning support during training exercises. This combined DT LLM
framework is currently being piloted in academic settings to develop hands on
skills in vulnerability assessment, threat detection, and security operations.
Initial findings suggest that the integration significantly improves the
effectiveness and relevance of cybersecurity training, bridging the gap between
theoretical knowledge and real-world application. Ultimately, the research
demonstrates how DTs and LLMs together can transform cybersecurity education to
meet evolving industry demands.

</details>


### [37] [Frequency Estimation of Correlated Multi-attribute Data under Local Differential Privacy](https://arxiv.org/abs/2507.17516)
*Shafizur Rahman Seeam,Ye Zheng,Yidan Hu*

Main category: cs.CR

TL;DR: 本文提出了一种新的局部差分隐私机制Corr-RR，通过利用属性间的相关性来显著提高数据效用，同时保持严格的隐私保护。该方法将完整的隐私预算分配给随机选择的单个属性，然后利用学习到的属性间依赖关系重构其他属性。


<details>
  <summary>Details</summary>
Motivation: 现有的局部差分隐私(LDP)机制要么将隐私预算分散到所有属性上，要么独立处理每个属性，忽略了属性间的自然相关性。这导致噪声过多或预算碎片化，在高维设置中造成显著的效用损失。多属性数据集在分析中至关重要但存在隐私风险，需要更好的隐私保护机制。

Method: 提出Corr-RR（相关随机响应）机制，分为两个阶段：(1)一部分用户使用标准LDP机制估计属性间相关性；(2)其余用户扰动一个随机选择的属性，并使用学习到的相关性推断其他属性。该方法将完整的隐私预算分配给单个属性的扰动，通过估计的属性间依赖关系重构剩余属性，无需额外的隐私成本。

Result: 理论上证明了Corr-RR满足ε-局部差分隐私。在合成和真实数据集上的大量实验表明，Corr-RR始终优于最先进的LDP机制，特别是在属性数量多且属性间相关性强的场景中表现突出。

Conclusion: Corr-RR通过有效利用属性间相关性，成功解决了现有LDP机制在多属性高维数据上效用损失严重的问题。该机制在保持严格隐私保护的同时显著提高了数据效用，为多属性数据的隐私保护提供了新的解决方案。

Abstract: Large-scale data collection, from national censuses to IoT-enabled smart
homes, routinely gathers dozens of attributes per individual. These
multi-attribute datasets are vital for analytics but pose significant privacy
risks. Local Differential Privacy (LDP) is a powerful tool to protect user data
privacy by allowing users to locally perturb their records before releasing to
an untrusted data aggregator. However, existing LDP mechanisms either split the
privacy budget across all attributes or treat each attribute independently,
ignoring natural inter-attribute correlations. This leads to excessive noise or
fragmented budgets, resulting in significant utility loss, particularly in
high-dimensional settings.
  To overcome these limitations, we propose Correlated Randomized Response
(Corr-RR), a novel LDP mechanism that leverages correlations among attributes
to substantially improve utility while maintaining rigorous LDP guarantees.
Corr-RR allocates the full privacy budget to perturb a single, randomly
selected attribute and reconstructs the remaining attributes using estimated
interattribute dependencies, without incurring additional privacy cost. To
enable this, Corr-RR operates in two phases: (1) a subset of users apply
standard LDP mechanisms to estimate correlations, and (2) each remaining user
perturbs one attribute and infers the others using the learned correlations. We
theoretically prove that Corr-RR satisfies $\epsilon$-LDP, and extensive
experiments on synthetic and real-world datasets demonstrate that Corr-RR
consistently outperforms state-of-the-art LDP mechanisms, particularly in
scenarios with many attributes and strong inter-attribute correlations.

</details>


### [38] [Rethinking HSM and TPM Security in the Cloud: Real-World Attacks and Next-Gen Defenses](https://arxiv.org/abs/2507.17655)
*Shams Shaikh,Trima P. Fernandes e Fizardo*

Main category: cs.CR

TL;DR: 本文分析了云环境中硬件安全模块(HSM)和可信平台模块(TPM)面临的安全挑战，指出传统硬件安全方案在云原生威胁下存在系统性漏洞，并探讨了包括机密计算、后量子密码学和去中心化密钥管理在内的替代方案。


<details>
  <summary>Details</summary>
Motivation: 随着组织快速迁移到云端，传统被视为加密密钥保护黄金标准的HSM和TPM面临云原生威胁的挑战。现实世界的安全漏洞暴露了云部署中的弱点，包括配置错误、API滥用和权限提升等，使攻击者能够访问敏感密钥材料并绕过保护机制。虽然硬件本身保持安全，但周围的云生态系统引入了系统性漏洞。

Method: 通过分析涉及HSM和TPM的重大安全失败案例，识别常见攻击向量，质疑它们在分布式环境中有效性的长期假设。探索替代方案包括机密计算、后量子密码学和去中心化密钥管理。评估当前弱点和新兴模型。

Result: 研究发现虽然HSM和TPM仍发挥作用，但现代云安全需要更具适应性的分层架构。识别了云环境中HSM和TPM部署的系统性漏洞，包括配置错误、API滥用和权限提升等攻击向量。

Conclusion: 为云架构师和安全工程师提供了在不断演变的威胁环境中加强密码学信任的策略。强调需要采用更适应性强的分层安全架构，而不是仅仅依赖传统的硬件安全解决方案。

Abstract: As organizations rapidly migrate to the cloud, the security of cryptographic
key management has become a growing concern. Hardware Security Modules (HSMs)
and Trusted Platform Modules (TPMs), traditionally seen as the gold standard
for securing encryption keys and digital trust, are increasingly challenged by
cloud-native threats. Real-world breaches have exposed weaknesses in cloud
deployments, including misconfigurations, API abuse, and privilege escalations,
allowing attackers to access sensitive key material and bypass protections.
These incidents reveal that while the hardware remains secure, the surrounding
cloud ecosystem introduces systemic vulnerabilities. This paper analyzes
notable security failures involving HSMs and TPMs, identifies common attack
vectors, and questions longstanding assumptions about their effectiveness in
distributed environments. We explore alternative approaches such as
confidential computing, post-quantum cryptography, and decentralized key
management. Our findings highlight that while HSMs and TPMs still play a role,
modern cloud security requires more adaptive, layered architectures. By
evaluating both current weaknesses and emerging models, this research equips
cloud architects and security engineers with strategies to reinforce
cryptographic trust in the evolving threat landscape.

</details>


### [39] [Quantifying the ROI of Cyber Threat Intelligence: A Data-Driven Approach](https://arxiv.org/abs/2507.17628)
*Matteo Strada*

Main category: cs.CR

TL;DR: 本研究提出了一个数据驱动的方法来量化网络威胁情报(CTI)的投资回报率，通过引入威胁情报有效性指数(TIEI)和混合评估模型，将CTI从成本支出转变为可衡量的战略投资。


<details>
  <summary>Details</summary>
Motivation: 网络威胁情报(CTI)的价值评估面临持续挑战，主要是由于"负面证据问题"：成功的威胁预防产生的非事件几乎没有可观察的财务影响，使得CTI支出在传统成本效益框架内难以证明其合理性。

Method: 研究扩展了Gordon-Loeb和FAIR等安全经济学模型，提出了数据驱动的CTI投资回报率量化框架。引入威胁情报有效性指数(TIEI)作为基于加权几何平均的复合指标，通过实证性能指标（如检测时间、响应时间、对手驻留时间的减少）进行操作化，并结合金融、医疗和零售三个行业的案例研究。

Result: 通过整合财务量化、对抗覆盖和业务赋能的定性评估，混合模型成功地将负面证据转换为可证明的ROI解释。TIEI有效地捕捉了瓶颈效应，其中最不有效的组件限制了整体性能。三个行业的案例研究验证了该框架的实用性。

Conclusion: 该方法提供了一种可复制的手段，将CTI从费用重新定位为战略投资，使得在不同组织环境中能够进行明智的决策制定和持续优化，为CTI价值评估提供了新的解决方案。

Abstract: The valuation of Cyber Threat Intelligence (CTI) remains a persistent
challenge due to the problem of negative evidence: successful threat prevention
results in non-events that generate minimal observable financial impact, making
CTI expenditures difficult to justify within traditional cost-benefit
frameworks. This study introduces a data-driven methodology for quantifying the
return on investment (ROI) of CTI, thereby reframing it as a measurable
contributor to risk mitigation. The proposed framework extends established
models in security economics, including the Gordon-Loeb and FAIR models, to
account for CTI's complex influence on both the probability of security
breaches and the severity of associated losses. The framework is
operationalized through empirically grounded performance indicators, such as
reductions in mean time to detect (MTTD), mean time to respond (MTTR), and
adversary dwell time, supported by three sector-specific case studies in
finance, healthcare, and retail. To address limitations in conventional linear
assessment methodologies, the Threat Intelligence Effectiveness Index (TIEI) is
introduced as a composite metric based on a weighted geometric mean. TIEI
penalizes underperformance across critical dimensions: quality, enrichment,
integration, and operational impact; thereby capturing bottleneck effect where
the least effective component limits overall performance. By integrating
financial quantification, adversarial coverage, and qualitative assessments of
business enablement, the proposed hybrid model converts negative evidence into
a justifiable ROI explanation. This approach offers a replicable means of
repositioning CTI from an expense to a strategic investment, enabling informed
decision-making and continuous optimization across diverse organizational
contexts.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [40] [Towards Autonomous Sustainability Assessment via Multimodal AI Agents](https://arxiv.org/abs/2507.17012)
*Zhihan Zhang,Alexander Metzger,Yuxuan Mei,Felix Hähnlein,Zachary Englhardt,Tingyu Cheng,Gregory D. Abowd,Shwetak Patel,Adriana Schulz,Vikram Iyer*

Main category: cs.AI

TL;DR: 本文提出了一种基于多模态AI代理的新型生命周期评估(LCA)方法，能够在一分钟内自动计算电子设备的碳排放，准确率达到专家LCA的81%，大幅减少了传统LCA所需的数周或数月时间


<details>
  <summary>Details</summary>
Motivation: 传统生命周期评估(LCA)所需的数据往往不可获得，需要专家花费数周或数月时间进行分析，这限制了可持续性信息的获取和应用。为了解决数据可用性差距和时间成本高的问题，需要开发新的自动化方法

Method: 开发了多模态AI代理系统，模拟LCA专家与产品经理、工程师之间的交互；使用定制的数据抽象和软件工具从在线文本、维修社区图像和政府认证中提取信息；迭代生成详细的生命周期清单；开发了基于产品描述相似性聚类的直接环境影响估算方法；提出了数据驱动的排放因子生成方法，将未知材料表示为相似材料排放因子的加权和

Result: AI代理方法在一分钟内完成LCA计算，碳足迹估算误差在专家LCA的19%以内，且无需专有数据；直接估算方法在笔记本电脑上3毫秒内完成，电子产品的平均绝对百分比误差(MAPE)为12.28%；数据驱动的排放因子生成方法比人类专家选择最接近的LCA数据库条目的MAPE改善了120.26%

Conclusion: 多模态AI代理能够显著加速LCA流程，在保持较高准确性的同时大幅减少时间成本，为未来LCA工作流程提供了新的可能性。该方法有效解决了数据可用性问题，使可持续性评估更加便捷和普及

Abstract: Interest in sustainability information has surged in recent years. However,
the data required for a life cycle assessment (LCA) that maps the materials and
processes from product manufacturing to disposal into environmental impacts
(EI) are often unavailable. Here we reimagine conventional LCA by introducing
multimodal AI agents that emulate interactions between LCA experts and
stakeholders like product managers and engineers to calculate the
cradle-to-gate (production) carbon emissions of electronic devices. The AI
agents iteratively generate a detailed life-cycle inventory leveraging a custom
data abstraction and software tools that extract information from online text
and images from repair communities and government certifications. This approach
reduces weeks or months of expert time to under one minute and closes data
availability gaps while yielding carbon footprint estimates within 19% of
expert LCAs with zero proprietary data. Additionally, we develop a method to
directly estimate EI by comparing an input to a cluster of products with
similar descriptions and known carbon footprints. This runs in 3 ms on a laptop
with a MAPE of 12.28% on electronic products. Further, we develop a data-driven
method to generate emission factors. We use the properties of an unknown
material to represent it as a weighted sum of emission factors for similar
materials. Compared to human experts picking the closest LCA database entry,
this improves MAPE by 120.26%. We analyze the data and compute scaling of this
approach and discuss its implications for future LCA workflows.

</details>


### [41] [New Mechanisms in Flex Distribution for Bounded Suboptimal Multi-Agent Path Finding](https://arxiv.org/abs/2507.17054)
*Shao-Hung Chan,Thomy Phan,Jiaoyang Li,Sven Koenig*

Main category: cs.AI

TL;DR: 本文针对多智能体路径规划(MAPF)问题，改进了EECBS算法中的flex分配机制，提出了基于冲突的flex分配、基于延迟的flex分配以及混合策略flex分配方法，在保证有界次优解的同时提高了算法效率。


<details>
  <summary>Details</summary>
Motivation: 现有EECBS算法使用的flex分配机制虽然能保证找到有界次优解，但增加阈值可能导致解的成本超出预期界限，迫使算法在不同路径集合间切换而非专注解决特定路径集合的冲突，从而降低效率。

Method: 提出三种新的flex分配机制：1）基于冲突的flex分配(Conflict-Based Flex Distribution)，按冲突数量比例分配flex；2）基于延迟的flex分配(Delay-Based Flex Distribution)，估计满足约束所需的延迟；3）混合策略flex分配(Mixed-Strategy Flex Distribution)，在分层框架中结合前两种方法。

Result: 实验表明，所提出的新flex分配机制在保持算法完整性和有界次优性的同时，性能优于原始的贪心flex分配方法，有效提高了EECBS算法的效率。

Conclusion: 新提出的flex分配机制能够更好地平衡解质量和计算效率，为多智能体路径规划问题提供了更有效的求解方案，理论上保证了算法的完整性和有界次优性。

Abstract: Multi-Agent Path Finding (MAPF) is the problem of finding a set of
collision-free paths, one for each agent in a shared environment. Its objective
is to minimize the sum of path costs (SOC), where the path cost of each agent
is defined as the travel time from its start location to its target location.
Explicit Estimation Conflict-Based Search (EECBS) is the leading algorithm for
bounded-suboptimal MAPF, with the SOC of the solution being at most a
user-specified factor $w$ away from optimal. EECBS maintains sets of paths and
a lower bound $LB$ on the optimal SOC. Then, it iteratively selects a set of
paths whose SOC is at most $w \cdot LB$ and introduces constraints to resolve
collisions. For each path in a set, EECBS maintains a lower bound on its
optimal path that satisfies constraints. By finding an individually
bounded-suboptimal path with cost at most a threshold of $w$ times its lower
bound, EECBS guarantees to find a bounded-suboptimal solution. To speed up
EECBS, previous work uses flex distribution to increase the threshold. Though
EECBS with flex distribution guarantees to find a bounded-suboptimal solution,
increasing the thresholds may push the SOC beyond $w \cdot LB$, forcing EECBS
to switch among different sets of paths instead of resolving collisions on a
particular set of paths, and thus reducing efficiency. To address this issue,
we propose Conflict-Based Flex Distribution that distributes flex in proportion
to the number of collisions. We also estimate the delays needed to satisfy
constraints and propose Delay-Based Flex Distribution. On top of that, we
propose Mixed-Strategy Flex Distribution, combining both in a hierarchical
framework. We prove that EECBS with our new flex distribution mechanisms is
complete and bounded-suboptimal. Our experiments show that our approaches
outperform the original (greedy) flex distribution.

</details>


### [42] [LoRA is All You Need for Safety Alignment of Reasoning LLMs](https://arxiv.org/abs/2507.17075)
*Yihao Xue,Baharan Mirzasoleiman*

Main category: cs.AI

TL;DR: 本文提出使用LoRA进行安全对齐微调可以在不损害推理能力的情况下实现LLM的安全对齐，避免了传统全模型微调导致的"安全税"问题。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在推理任务上取得突破，但安全对齐微调会显著降低推理能力（"安全税"现象）。需要找到既能保证模型安全又不损害推理能力的方法。

Method: 使用LoRA（低秩适应）在拒绝数据集上进行监督微调，将安全权重更新限制在低秩空间中，以最小化对推理权重的干扰。还探索了通过正则化或权重合并进一步减少权重重叠的方法。

Result: 在数学、科学和编程四个基准测试中，LoRA方法产生了高度安全的LLM（安全水平与全模型微调相当），同时保持了推理能力。观察到LoRA与初始权重的重叠比全模型微调更小。

Conclusion: LoRA安全对齐微调是一种有效的方法，可以在保持推理能力的同时实现模型安全对齐，为设计更好的推理-安全权衡方法提供了思路。

Abstract: Reasoning LLMs have demonstrated remarkable breakthroughs in solving complex
problems that were previously out of reach. To ensure LLMs do not assist with
harmful requests, safety alignment fine-tuning is necessary in the
post-training phase. However, safety alignment fine-tuning has recently been
shown to significantly degrade reasoning abilities, a phenomenon known as the
"Safety Tax". In this work, we show that using LoRA for SFT on refusal datasets
effectively aligns the model for safety without harming its reasoning
capabilities. This is because restricting the safety weight updates to a
low-rank space minimizes the interference with the reasoning weights. Our
extensive experiments across four benchmarks covering math, science, and coding
show that this approach produces highly safe LLMs -- with safety levels
comparable to full-model fine-tuning -- without compromising their reasoning
abilities. Additionally, we observe that LoRA induces weight updates with
smaller overlap with the initial weights compared to full-model fine-tuning. We
also explore methods that further reduce such overlap -- via regularization or
during weight merging -- and observe some improvement on certain tasks. We hope
this result motivates designing approaches that yield more consistent
improvements in the reasoning-safety trade-off.

</details>


### [43] [HySafe-AI: Hybrid Safety Architectural Analysis Framework for AI Systems: A Case Study](https://arxiv.org/abs/2507.17118)
*Mandar Pitale,Jelena Frtunikj,Abhinaw Priyadershi,Vasu Singh,Maria Spence*

Main category: cs.AI

TL;DR: 本文提出了HySAFE-AI混合安全架构分析框架，专门用于评估端到端AI系统（如大语言模型和视觉语言模型）在安全关键领域的安全性，通过改进传统的FMEA和FTA分析方法来应对基础模型的复杂性。


<details>
  <summary>Details</summary>
Motivation: 随着AI在自动驾驶系统和机器人等安全关键领域的广泛应用，特别是端到端单体架构（如大语言模型和视觉语言模型）的兴起，传统的安全分析方法难以有效评估这些复杂AI系统的安全性，尤其是在处理基础模型的潜在表征形成和利用方面存在挑战。

Method: 研究首先回顾了不同的架构解决方案，然后评估了常见安全分析方法（如故障模式与影响分析FMEA和故障树分析FTA）的有效性。在此基础上，提出了HySAFE-AI（AI系统混合安全架构分析框架），这是一个混合框架，将传统方法适应性地改进以评估AI系统的安全性，特别针对基础模型的复杂性质进行了优化。

Result: 研究展示了如何改进传统安全分析技术以适应基础模型的复杂特性，特别是在潜在表征的形成和利用方面。成功开发了HySAFE-AI框架，能够更有效地评估端到端AI系统的安全性。

Conclusion: HySAFE-AI框架为AI系统安全分析提供了新的解决方案，能够有效应对现代端到端AI架构的安全评估挑战。研究还为未来工作提供了指导建议，有助于推动未来AI安全标准的发展。

Abstract: AI has become integral to safety-critical areas like autonomous driving
systems (ADS) and robotics. The architecture of recent autonomous systems are
trending toward end-to-end (E2E) monolithic architectures such as large
language models (LLMs) and vision language models (VLMs). In this paper, we
review different architectural solutions and then evaluate the efficacy of
common safety analyses such as failure modes and effect analysis (FMEA) and
fault tree analysis (FTA). We show how these techniques can be improved for the
intricate nature of the foundational models, particularly in how they form and
utilize latent representations. We introduce HySAFE-AI, Hybrid Safety
Architectural Analysis Framework for AI Systems, a hybrid framework that adapts
traditional methods to evaluate the safety of AI systems. Lastly, we offer
hints of future work and suggestions to guide the evolution of future AI safety
standards.

</details>


### [44] [Improving LLMs' Generalized Reasoning Abilities by Graph Problems](https://arxiv.org/abs/2507.17168)
*Qifan Zhang,Nuo Chen,Zehua Li,Miao Peng,Jing Tang,Jia Li*

Main category: cs.AI

TL;DR: 本文首次使用图问题推理(GPR)来增强大语言模型的通用推理能力，构建了包含109亿token的GraphPile数据集，并训练出GraphMind模型，在数学推理和非数学推理任务上都取得了显著改进。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型在新颖复杂问题上推理性能不佳，领域特定的继续预训练方法（如数学推理）缺乏向更广泛推理任务的迁移能力，需要一种能够提升通用推理能力的方法。

Method: 引入图问题推理(GPR)作为增强LLM通用推理能力的新方法，构建GraphPile数据集（包含109亿token，涵盖23个图任务，包括路径查找、网络分析、数值计算和拓扑推理等），使用思维链、程序思维、执行轨迹和真实图数据进行训练。

Result: 在Llama 3、3.1和Gemma 2等基础模型上训练GraphMind，在数学推理任务上准确率提升高达4.9%，在逻辑推理和常识推理等非数学推理任务上改进高达21.2%。

Conclusion: 本研究是首个利用图问题推理增强推理模式的工作，并引入了首个此类数据集，成功弥合了领域特定预训练与通用推理能力之间的差距，提升了大语言模型的适应性和鲁棒性。

Abstract: Large Language Models (LLMs) have made remarkable strides in reasoning tasks,
yet their performance often falters on novel and complex problems.
Domain-specific continued pretraining (CPT) methods, such as those tailored for
mathematical reasoning, have shown promise but lack transferability to broader
reasoning tasks. In this work, we pioneer the use of Graph Problem Reasoning
(GPR) to enhance the general reasoning capabilities of LLMs. GPR tasks,
spanning pathfinding, network analysis, numerical computation, and topological
reasoning, require sophisticated logical and relational reasoning, making them
ideal for teaching diverse reasoning patterns. To achieve this, we introduce
GraphPile, the first large-scale corpus specifically designed for CPT using GPR
data. Spanning 10.9 billion tokens across 23 graph tasks, the dataset includes
chain-of-thought, program-of-thought, trace of execution, and real-world graph
data. Using GraphPile, we train GraphMind on popular base models Llama 3 and
3.1, as well as Gemma 2, achieving up to 4.9 percent higher accuracy in
mathematical reasoning and up to 21.2 percent improvement in non-mathematical
reasoning tasks such as logical and commonsense reasoning. By being the first
to harness GPR for enhancing reasoning patterns and introducing the first
dataset of its kind, our work bridges the gap between domain-specific
pretraining and universal reasoning capabilities, advancing the adaptability
and robustness of LLMs.

</details>


### [45] [Our Cars Can Talk: How IoT Brings AI to Vehicles](https://arxiv.org/abs/2507.17214)
*Amod Kant Agrawal*

Main category: cs.AI

TL;DR: 本文提出将AI集成到车辆中作为感知平台，实现从被动维护向主动维护的转变，重点是开发能够与机器和驾驶员双向沟通的AI副驾驶系统


<details>
  <summary>Details</summary>
Motivation: 传统车辆维护模式是被动响应式的，缺乏预测性和主动性。随着AI技术的发展，有机会将车辆转变为智能感知平台，通过AI副驾驶系统实现更智能的维护模式和人机交互

Method: 将AI技术集成到车辆系统中，开发能够理解机器语言（传感器数据、系统状态）和驾驶员语言（自然语言交互）的AI副驾驶系统，构建智能感知平台

Result: 提出了智能车辆系统的概念框架，为预测性维护和AI驱动的用户交互提供了技术视角和发展方向

Conclusion: AI副驾驶系统的集成将推动车辆维护从被动向主动的根本性转变，该研究为智能车辆系统、预测性维护和AI用户交互领域的跨学科研究和未来发展提供了指导框架

Abstract: Bringing AI to vehicles and enabling them as sensing platforms is key to
transforming maintenance from reactive to proactive. Now is the time to
integrate AI copilots that speak both languages: machine and driver. This
article offers a conceptual and technical perspective intended to spark
interdisciplinary dialogue and guide future research and development in
intelligent vehicle systems, predictive maintenance, and AI-powered user
interaction.

</details>


### [46] [Agent Identity Evals: Measuring Agentic Identity](https://arxiv.org/abs/2507.17257)
*Elija Perrier,Michael Timothy Bennett*

Main category: cs.AI

TL;DR: 该论文提出了一个名为"智能体身份评估"(AIE)的框架，用于测量和评估语言模型智能体在时间推移中维持稳定身份的能力，以解决大语言模型固有缺陷对智能体可靠性和可信度的影响。


<details>
  <summary>Details</summary>
Motivation: 语言模型智能体(LMAs)继承了大语言模型的病理特征（无状态性、随机性、对提示敏感和语言中介性），这些特征会破坏智能体的可识别性、连续性、持久性和一致性，进而侵蚀其可靠性、可信度和实用性，干扰推理、规划和行动等智能体核心能力。

Method: 引入"智能体身份评估"(AIE)框架，这是一个严格的、统计驱动的实证框架，包含一套新颖的度量指标，能够与其他性能、能力和智能体鲁棒性测量集成，用于设计最优的LMA基础设施和支撑结构（如记忆和工具）。

Result: 论文提供了可应用于LMA生命周期各个阶段的正式定义和方法，并给出了如何应用这些方法的具体工作示例。AIE框架能够测量LMA系统展现和维持其智能体身份的程度，包括其能力、属性以及从状态扰动中恢复的能力。

Conclusion: AIE框架为评估语言模型智能体的身份稳定性提供了系统性解决方案，有助于提高LMA的可靠性和可信度，为设计更好的智能体基础设施提供了理论基础和实践指导。

Abstract: Central to agentic capability and trustworthiness of language model agents
(LMAs) is the extent they maintain stable, reliable, identity over time.
However, LMAs inherit pathologies from large language models (LLMs)
(statelessness, stochasticity, sensitivity to prompts and
linguistically-intermediation) which can undermine their identifiability,
continuity, persistence and consistency. This attrition of identity can erode
their reliability, trustworthiness and utility by interfering with their
agentic capabilities such as reasoning, planning and action. To address these
challenges, we introduce \textit{agent identity evals} (AIE), a rigorous,
statistically-driven, empirical framework for measuring the degree to which an
LMA system exhibit and maintain their agentic identity over time, including
their capabilities, properties and ability to recover from state perturbations.
AIE comprises a set of novel metrics which can integrate with other measures of
performance, capability and agentic robustness to assist in the design of
optimal LMA infrastructure and scaffolding such as memory and tools. We set out
formal definitions and methods that can be applied at each stage of the LMA
life-cycle, and worked examples of how to apply them.

</details>


### [47] [Students' Feedback Requests and Interactions with the SCRIPT Chatbot: Do They Get What They Ask For?](https://arxiv.org/abs/2507.17258)
*Andreas Scholl,Natalie Kiesler*

Main category: cs.AI

TL;DR: 研究者开发了基于ChatGPT-4o-mini的编程教育聊天机器人SCRIPT，通过136名德国大学编程入门课程学生的实验，发现学生的反馈请求遵循特定序列，聊天机器人75%的回应与学生需求匹配，为AI辅助学习系统设计提供了指导。


<details>
  <summary>Details</summary>
Motivation: 现有生成式AI在编程教育中的应用研究基础上，需要开发专门支持编程初学者的工具，平衡AI辅助工具中的指导性和灵活性，了解学生与AI聊天机器人的交互模式和反馈偏好。

Method: 开发了基于ChatGPT-4o-mini的聊天机器人SCRIPT，支持开放式交互和预定义提示的结构化指导；通过136名德国大学编程入门课程学生的实验评估工具效果；分析学生在解决编程任务时与SCRIPT的交互方式，重点关注反馈偏好。

Result: 发现学生的反馈请求遵循特定序列；聊天机器人的回应与学生请求的反馈类型匹配度达到75%；系统很好地遵守了系统提示约束；为基于生成式AI的学习支持系统设计提供了洞察。

Conclusion: 研究为设计基于生成式AI的学习支持系统提供了重要见解，突出了在AI辅助工具中平衡指导性和灵活性的挑战，SCRIPT在支持编程初学者方面表现出良好的效果和匹配度。

Abstract: Building on prior research on Generative AI (GenAI) and related tools for
programming education, we developed SCRIPT, a chatbot based on ChatGPT-4o-mini,
to support novice learners. SCRIPT allows for open-ended interactions and
structured guidance through predefined prompts. We evaluated the tool via an
experiment with 136 students from an introductory programming course at a large
German university and analyzed how students interacted with SCRIPT while
solving programming tasks with a focus on their feedback preferences. The
results reveal that students' feedback requests seem to follow a specific
sequence. Moreover, the chatbot responses aligned well with students' requested
feedback types (in 75%), and it adhered to the system prompt constraints. These
insights inform the design of GenAI-based learning support systems and
highlight challenges in balancing guidance and flexibility in AI-assisted
tools.

</details>


### [48] [Compliance Brain Assistant: Conversational Agentic AI for Assisting Compliance Tasks in Enterprise Environments](https://arxiv.org/abs/2507.17289)
*Shitong Zhu,Chenhao Fang,Derek Larson,Neel Reddy Pochareddy,Rajeev Rao,Sophie Zeng,Yanqing Peng,Wendy Summer,Alex Goncalves,Arya Pudota,Herve Robert*

Main category: cs.AI

TL;DR: 本文提出了合规大脑助手(CBA)，这是一个对话式AI助手，通过智能路由机制在快速模式和完整代理模式之间选择，以提高企业合规工作效率。实验显示CBA在关键词匹配率和LLM评判通过率上显著优于普通LLM。


<details>
  <summary>Details</summary>
Motivation: 企业合规人员需要处理大量日常合规任务，现有的普通LLM在处理复杂合规查询时效果不佳，需要一个既能保证响应质量又能控制延迟的智能助手来提升工作效率。

Method: 设计了一个用户查询路由器，能够智能选择两种模式：(1)快速模式：处理只需从知识库检索相关上下文的简单请求；(2)完整代理模式：处理需要复合操作和工具调用的复杂请求，能够主动发现各种合规文档中的上下文，并调用其他API/模型来满足请求。

Result: 与开箱即用的LLM相比，CBA在各种真实世界的隐私/合规相关查询上表现显著提升：平均关键词匹配率从41.7%提升到83.7%，LLM评判通过率从20.0%提升到82.0%。基于路由的完整设计在保持相近运行时间的同时，获得了更好的平均匹配率和通过率。

Conclusion: 路由机制成功实现了响应质量和延迟之间的良好平衡，验证了设计假设。CBA通过智能路由在简单和复杂查询处理之间做出最优选择，显著提升了企业合规任务的处理效率和准确性。

Abstract: This paper presents Compliance Brain Assistant (CBA), a conversational,
agentic AI assistant designed to boost the efficiency of daily compliance tasks
for personnel in enterprise environments. To strike a good balance between
response quality and latency, we design a user query router that can
intelligently choose between (i) FastTrack mode: to handle simple requests that
only need additional relevant context retrieved from knowledge corpora; and
(ii) FullAgentic mode: to handle complicated requests that need composite
actions and tool invocations to proactively discover context across various
compliance artifacts, and/or involving other APIs/models for accommodating
requests. A typical example would be to start with a user query, use its
description to find a specific entity and then use the entity's information to
query other APIs for curating and enriching the final AI response.
  Our experimental evaluations compared CBA against an out-of-the-box LLM on
various real-world privacy/compliance-related queries targeting various
personas. We found that CBA substantially improved upon the vanilla LLM's
performance on metrics such as average keyword match rate (83.7% vs. 41.7%) and
LLM-judge pass rate (82.0% vs. 20.0%). We also compared metrics for the full
routing-based design against the `fast-track only` and `full-agentic` modes and
found that it had a better average match-rate and pass-rate while keeping the
run-time approximately the same. This finding validated our hypothesis that the
routing mechanism leads to a good trade-off between the two worlds.

</details>


### [49] [Ctx2TrajGen: Traffic Context-Aware Microscale Vehicle Trajectories using Generative Adversarial Imitation Learning](https://arxiv.org/abs/2507.17418)
*Joobin Jin,Seokjun Hong,Gyeongseon Baek,Yeeun Kim,Byeongjoon Noh*

Main category: cs.AI

TL;DR: 提出了Ctx2TrajGen框架，使用GAIL结合PPO和WGAN-GP技术生成上下文感知的车辆轨迹，解决了微观交通建模中的非线性相互依赖和训练不稳定问题


<details>
  <summary>Details</summary>
Motivation: 精确建模微观车辆轨迹对交通行为分析和自动驾驶系统至关重要，现有方法在处理非线性相互依赖关系、训练稳定性以及上下文感知方面存在不足，同时面临数据稀缺和领域偏移问题

Method: 提出Ctx2TrajGen上下文感知轨迹生成框架，结合生成对抗模仿学习(GAIL)、近端策略优化(PPO)和梯度惩罚Wasserstein生成对抗网络(WGAN-GP)，通过显式条件化周围车辆和道路几何信息来生成交互感知的轨迹

Result: 在无人机捕获的DRIFT数据集上的实验表明，该方法在现实性、行为多样性和上下文保真度方面都优于现有方法，有效解决了数据稀缺和领域偏移问题

Conclusion: Ctx2TrajGen框架为微观车辆轨迹生成提供了鲁棒的解决方案，能够合成符合真实世界上下文的交互感知轨迹，在无需仿真的情况下有效应对数据稀缺和领域偏移挑战

Abstract: Precise modeling of microscopic vehicle trajectories is critical for traffic
behavior analysis and autonomous driving systems. We propose Ctx2TrajGen, a
context-aware trajectory generation framework that synthesizes realistic urban
driving behaviors using GAIL. Leveraging PPO and WGAN-GP, our model addresses
nonlinear interdependencies and training instability inherent in microscopic
settings. By explicitly conditioning on surrounding vehicles and road geometry,
Ctx2TrajGen generates interaction-aware trajectories aligned with real-world
context. Experiments on the drone-captured DRIFT dataset demonstrate superior
performance over existing methods in terms of realism, behavioral diversity,
and contextual fidelity, offering a robust solution to data scarcity and domain
shift without simulation.

</details>


### [50] [An Uncertainty-Driven Adaptive Self-Alignment Framework for Large Language Models](https://arxiv.org/abs/2507.17477)
*Haoran Sun,Zekun Zhang,Shaoning Zeng*

Main category: cs.AI

TL;DR: 本文提出了UDASA框架，通过不确定性驱动的自适应自对齐方法，在无需人工标注的情况下自动改善大语言模型与人类意图和安全规范的对齐效果。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在指令遵循和通用推理方面取得显著进展，但在没有人工标注的情况下实现与人类意图和安全规范的高质量对齐仍然是一个根本性挑战。

Method: 提出不确定性驱动的自适应自对齐(UDASA)框架：1)为每个输入生成多个响应；2)从语义、事实性和价值对齐三个维度量化输出不确定性；3)基于不确定性分数构建偏好对，并根据不确定性差异将训练样本分为保守、适中和探索三个阶段；4)模型在这三个阶段中逐步优化。

Result: UDASA在多个任务上优于现有对齐方法，包括无害性、有用性、真实性和受控情感生成，显著提升了模型性能。同时进行了一系列初步研究验证核心设计假设。

Conclusion: UDASA框架能够在完全自动化的方式下有效改善大语言模型的对齐性能，为解决无人工标注情况下的模型对齐问题提供了有效解决方案。

Abstract: Large Language Models (LLMs) have demonstrated remarkable progress in
instruction following and general-purpose reasoning. However, achieving
high-quality alignment with human intent and safety norms without human
annotations remains a fundamental challenge. In this work, we propose an
Uncertainty-Driven Adaptive Self-Alignment (UDASA) framework designed to
improve LLM alignment in a fully automated manner. UDASA first generates
multiple responses for each input and quantifies output uncertainty across
three dimensions: semantics, factuality, and value alignment. Based on these
uncertainty scores, the framework constructs preference pairs and categorizes
training samples into three stages, conservative, moderate, and exploratory,
according to their uncertainty difference. The model is then optimized
progressively across these stages. In addition, we conduct a series of
preliminary studies to validate the core design assumptions and provide strong
empirical motivation for the proposed framework. Experimental results show that
UDASA outperforms existing alignment methods across multiple tasks, including
harmlessness, helpfulness, truthfulness, and controlled sentiment generation,
significantly improving model performance.

</details>


### [51] [LTLZinc: a Benchmarking Framework for Continual Learning and Neuro-Symbolic Temporal Reasoning](https://arxiv.org/abs/2507.17482)
*Luca Salvatore Lorello,Nikolaos Manginas,Marco Lippi,Stefano Melacci*

Main category: cs.AI

TL;DR: 本文提出了LTLZinc基准框架，用于生成结合时序推理和持续学习的神经符号AI任务，通过线性时序逻辑和约束生成具有挑战性的数据集，揭示了现有方法在时序学习推理方面的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有的神经符号AI方法主要应用于静态场景，缺乏在时序维度上进行推理的研究，同时持续学习和神经符号AI的结合也鲜有探索，需要一个能够评估时序推理和持续学习能力的基准框架。

Method: 开发了LTLZinc基准框架，该框架通过线性时序逻辑(LTL)规范和MiniZinc约束生成数据集，结合任意图像分类数据集，创建富有表达力的时序推理和持续学习任务，并提供细粒度标注支持多种神经和神经符号训练设置。

Result: 在LTLZinc生成的六个神经符号序列分类任务和四个类持续学习任务上进行实验，证明了时序学习和推理的挑战性，并突出了当前最先进方法的局限性。发布了LTLZinc生成器和十个即用型任务。

Conclusion: LTLZinc框架成功地为神经符号AI和持续学习社区提供了评估时序推理能力的基准工具，实验结果表明现有方法在处理时序学习推理任务方面存在显著局限，为未来统一时序学习和推理框架的研究奠定了基础。

Abstract: Neuro-symbolic artificial intelligence aims to combine neural architectures
with symbolic approaches that can represent knowledge in a human-interpretable
formalism. Continual learning concerns with agents that expand their knowledge
over time, improving their skills while avoiding to forget previously learned
concepts. Most of the existing approaches for neuro-symbolic artificial
intelligence are applied to static scenarios only, and the challenging setting
where reasoning along the temporal dimension is necessary has been seldom
explored. In this work we introduce LTLZinc, a benchmarking framework that can
be used to generate datasets covering a variety of different problems, against
which neuro-symbolic and continual learning methods can be evaluated along the
temporal and constraint-driven dimensions. Our framework generates expressive
temporal reasoning and continual learning tasks from a linear temporal logic
specification over MiniZinc constraints, and arbitrary image classification
datasets. Fine-grained annotations allow multiple neural and neuro-symbolic
training settings on the same generated datasets. Experiments on six
neuro-symbolic sequence classification and four class-continual learning tasks
generated by LTLZinc, demonstrate the challenging nature of temporal learning
and reasoning, and highlight limitations of current state-of-the-art methods.
We release the LTLZinc generator and ten ready-to-use tasks to the
neuro-symbolic and continual learning communities, in the hope of fostering
research towards unified temporal learning and reasoning frameworks.

</details>


### [52] [CQE under Epistemic Dependencies: Algorithms and Experiments (extended version)](https://arxiv.org/abs/2507.17487)
*Lorenzo Marconi,Flavia Ricci,Riccardo Rosati*

Main category: cs.AI

TL;DR: 该论文研究了本体上的受控查询评估(CQE)问题，提出了结合认知依赖(EDs)和最优GA检查器的方法，并证明了在特定条件下该方法具有AC^0数据复杂度，通过实验验证了其实用性。


<details>
  <summary>Details</summary>
Motivation: 现有的本体查询系统缺乏有效的信息披露控制机制，需要一种既能保证安全性又具有良好计算性能的受控查询评估方法，特别是在处理认知依赖规则时如何平衡安全性与效率。

Method: 将认知依赖(EDs)与最优GA检查器概念相结合，采用所有最优GA检查器交集的方法来回答布尔合取查询联合(BUCQs)，并针对DL-Lite_R本体设计了详细的一阶重写算法。

Result: 证明了对于完全EDs类别，基于交集的方法保持安全性；对于EDs子类和DL-Lite_R本体，在数据复杂度上达到AC^0级别；实验结果显示重写函数在两种不同评估场景下具有实际可行性。

Conclusion: 提出的基于交集的CQE方法在保证强安全性的同时实现了良好的计算性能，特别是对于特定类型的认知依赖和DL-Lite_R本体，该方法具有理论保障和实际应用价值。

Abstract: We investigate Controlled Query Evaluation (CQE) over ontologies, where
information disclosure is regulated by epistemic dependencies (EDs), a family
of logical rules recently proposed for the CQE framework. In particular, we
combine EDs with the notion of optimal GA censors, i.e. maximal sets of ground
atoms that are entailed by the ontology and can be safely revealed. We focus on
answering Boolean unions of conjunctive queries (BUCQs) with respect to the
intersection of all optimal GA censors - an approach that has been shown in
other contexts to ensure strong security guarantees with favorable
computational behavior. First, we characterize the security of this
intersection-based approach and identify a class of EDs (namely, full EDs) for
which it remains safe. Then, for a subclass of EDs and for DL-Lite_R
ontologies, we show that answering BUCQs in the above CQE semantics is in AC^0
in data complexity by presenting a suitable, detailed first-order rewriting
algorithm. Finally, we report on experiments conducted in two different
evaluation scenarios, showing the practical feasibility of our rewriting
function.

</details>


### [53] [Automated Hybrid Grounding Using Structural and Data-Driven Heuristics](https://arxiv.org/abs/2507.17493)
*Alexander Beiser,Markus Hecher,Stefan Woltran*

Main category: cs.AI

TL;DR: 本文提出了一种自动化混合基础化方法，通过数据结构启发式算法自动决定何时使用体解耦基础化和标准自底向上基础化，以解决答案集编程中的基础化瓶颈问题。


<details>
  <summary>Details</summary>
Motivation: 答案集编程在工业界广泛应用面临的主要挑战是基础化瓶颈问题。虽然混合基础化结合了标准自底向上基础化和体解耦基础化技术的优势，但何时使用哪种方法仍不明确，需要一种自动化的决策机制。

Method: 开发了基于数据结构启发式的分割算法，该算法能够自动检测何时使用体解耦基础化和何时使用标准基础化。启发式方法基于规则结构和包含实例数据的估计过程。

Result: 在原型实现上进行的实验显示了有希望的结果：在难以基础化的场景中表现出改进，而在难以求解的实例上接近了最先进的性能水平。

Conclusion: 自动化混合基础化方法成功解决了何时选择不同基础化技术的问题，在提高难以基础化场景的性能方面取得了进展，同时在复杂求解实例上保持了竞争力的性能。

Abstract: The grounding bottleneck poses one of the key challenges that hinders the
widespread adoption of Answer Set Programming in industry. Hybrid Grounding is
a step in alleviating the bottleneck by combining the strength of standard
bottom-up grounding with recently proposed techniques where rule bodies are
decoupled during grounding. However, it has remained unclear when hybrid
grounding shall use body-decoupled grounding and when to use standard bottom-up
grounding. In this paper, we address this issue by developing automated hybrid
grounding: we introduce a splitting algorithm based on data-structural
heuristics that detects when to use body-decoupled grounding and when standard
grounding is beneficial. We base our heuristics on the structure of rules and
an estimation procedure that incorporates the data of the instance. The
experiments conducted on our prototypical implementation demonstrate promising
results, which show an improvement on hard-to-ground scenarios, whereas on
hard-to-solve instances we approach state-of-the-art performance.

</details>


### [54] [Can One Domain Help Others? A Data-Centric Study on Multi-Domain Reasoning via Reinforcement Learning](https://arxiv.org/abs/2507.17512)
*Yu Li,Zhuoshi Pan,Honglin Lin,Mengyuan Sun,Conghui He,Lijun Wu*

Main category: cs.AI

TL;DR: 本研究系统性地探索了强化学习可验证奖励(RLVR)框架下的多领域推理能力，重点研究数学推理、代码生成和逻辑拼图求解三个核心领域之间的相互作用，为优化大语言模型的综合推理能力提供了重要指导。


<details>
  <summary>Details</summary>
Motivation: 现有RLVR研究主要集中在单一推理领域（如数学、编程或逻辑推理），但真实世界的推理场景需要多种认知技能的综合应用。然而，强化学习框架下这些推理技能之间的相互作用机制仍然缺乏深入理解，需要系统性研究来填补这一空白。

Method: 采用GRPO算法和Qwen-2.5-7B模型系列，通过四个关键组件进行系统性研究：(1)评估单领域训练的域内改进和跨域泛化能力；(2)分析跨域联合训练中的相互增强和冲突；(3)比较基础模型和指令模型在相同RL配置下的性能差异；(4)深入探讨课程学习策略、奖励设计变化和语言特定因素等关键RL训练细节的影响。

Result: 通过广泛实验揭示了领域交互的动态机制，发现了影响专业化和可泛化推理性能的关键因素。研究结果为理解多领域推理中的相互增强和冲突机制提供了重要洞察，并确定了优化RL方法的关键要素。

Conclusion: 研究为优化强化学习方法提供了宝贵指导，以培养大语言模型的综合性多领域推理能力。通过深入分析领域间相互作用，为构建更强大的通用推理系统奠定了理论基础和实践指南。

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as a
powerful paradigm for enhancing the reasoning capabilities of LLMs. Existing
research has predominantly concentrated on isolated reasoning domains such as
mathematical problem-solving, coding tasks, or logical reasoning. However, real
world reasoning scenarios inherently demand an integrated application of
multiple cognitive skills. Despite this, the interplay among these reasoning
skills under reinforcement learning remains poorly understood. To bridge this
gap, we present a systematic investigation of multi-domain reasoning within the
RLVR framework, explicitly focusing on three primary domains: mathematical
reasoning, code generation, and logical puzzle solving. We conduct a
comprehensive study comprising four key components: (1) Leveraging the GRPO
algorithm and the Qwen-2.5-7B model family, our study thoroughly evaluates the
models' in-domain improvements and cross-domain generalization capabilities
when trained on single-domain datasets. (2) Additionally, we examine the
intricate interactions including mutual enhancements and conflicts that emerge
during combined cross-domain training. (3) To further understand the influence
of SFT on RL, we also analyze and compare performance differences between base
and instruct models under identical RL configurations. (4) Furthermore, we
delve into critical RL training details, systematically exploring the impacts
of curriculum learning strategies, variations in reward design, and
language-specific factors. Through extensive experiments, our results offer
significant insights into the dynamics governing domain interactions, revealing
key factors influencing both specialized and generalizable reasoning
performance. These findings provide valuable guidance for optimizing RL
methodologies to foster comprehensive, multi-domain reasoning capabilities in
LLMs.

</details>


### [55] [TAI Scan Tool: A RAG-Based Tool With Minimalistic Input for Trustworthy AI Self-Assessment](https://arxiv.org/abs/2507.17514)
*Athanasios Davvetas,Xenia Ziouvelou,Ypatia Dami,Alexis Kaponis,Konstantina Giouvanopoulou,Michael Papademas*

Main category: cs.AI

TL;DR: 本文介绍了TAI扫描工具，这是一个基于RAG的TAI自评估工具，专门用于协助遵守AI法案的法律合规性评估，通过预筛选和评估两个阶段来预测AI系统风险等级并检索相关法律条款。


<details>
  <summary>Details</summary>
Motivation: 开发一个能够帮助AI系统进行法律合规性自评估的工具，特别是针对AI法案的合规要求，以最少的输入实现有效的风险评估和法律指导。

Method: 采用基于RAG（检索增强生成）的两步方法：首先进行预筛选阶段，然后进行详细评估阶段。系统能够根据AI法案预测风险等级，同时检索相关法律条款以辅助合规。

Result: 定性评估结果显示该工具表现良好，能够正确预测风险等级并在三个不同语义组中检索到相关法律条款。结果解释表明工具的推理依赖于与高风险系统设置的比较。

Conclusion: TAI扫描工具成功实现了AI系统的法律合规性自评估功能，能够有效预测风险等级并提供相关法律指导，其推理机制主要基于高风险系统的比较分析，这反映了AI法案中对高风险系统部署的重视。

Abstract: This paper introduces the TAI Scan Tool, a RAG-based TAI self-assessment tool
with minimalistic input. The current version of the tool supports the legal TAI
assessment, with a particular emphasis on facilitating compliance with the AI
Act. It involves a two-step approach with a pre-screening and an assessment
phase. The assessment output of the system includes insight regarding the
risk-level of the AI system according to the AI Act, while at the same time
retrieving relevant articles to aid with compliance and notify on their
obligations. Our qualitative evaluation using use-case scenarios yields
promising results, correctly predicting risk levels while retrieving relevant
articles across three distinct semantic groups. Furthermore, interpretation of
results shows that the tool's reasoning relies on comparison with the setting
of high-risk systems, a behaviour attributed to their deployment requiring
careful consideration, and therefore frequently presented within the AI Act.

</details>


### [56] [Constructing Ophthalmic MLLM for Positioning-diagnosis Collaboration Through Clinical Cognitive Chain Reasoning](https://arxiv.org/abs/2507.17539)
*Xinyao Liu,Diping Song*

Main category: cs.AI

TL;DR: 本文提出了FundusExpert，一个专门用于眼科诊断的多模态大语言模型，通过整合定位-诊断推理能力和高质量数据集FundusGen，在眼科问答和报告生成任务中取得了显著的性能提升。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型在医学诊断领域显示出巨大潜力，但在眼科等专业领域面临注释粒度碎片化和临床推理逻辑不一致的关键挑战，阻碍了精确的跨模态理解。

Method: 构建了Fundus-Engine智能系统来自动化定位并利用MLLM语义扩展，将全局疾病分类、局部目标检测和细粒度特征分析整合在单个眼底图像中。通过构建临床对齐的认知链来指导模型生成可解释的推理路径，并使用FundusGen数据集对FundusExpert进行指令微调。

Result: FundusExpert在眼科问答任务中达到最佳性能，比40B MedRegA的平均准确率高出26.6%。在零样本报告生成任务中表现优异，临床一致性达到77.0%，显著超过GPT-4o的47.6%。揭示了数据质量与模型能力之间的缩放定律(L ∝ N^0.068)。

Conclusion: 通过整合区域级定位与诊断推理链，开发了一个可扩展的、临床对齐的多模态大语言模型，为弥合特定领域MLLM中的视觉-语言差距探索了一条新路径。FundusGen中的认知对齐注释提高了数据利用效率。

Abstract: Multimodal large language models (MLLMs) demonstrate significant potential in
the field of medical diagnosis. However, they face critical challenges in
specialized domains such as ophthalmology, particularly the fragmentation of
annotation granularity and inconsistencies in clinical reasoning logic, which
hinder precise cross-modal understanding. This paper introduces FundusExpert,
an ophthalmology-specific MLLM with integrated positioning-diagnosis reasoning
capabilities, along with FundusGen, a dataset constructed through the
intelligent Fundus-Engine system. Fundus-Engine automates localization and
leverages MLLM-based semantic expansion to integrate global disease
classification, local object detection, and fine-grained feature analysis
within a single fundus image. Additionally, by constructing a clinically
aligned cognitive chain, it guides the model to generate interpretable
reasoning paths. FundusExpert, fine-tuned with instruction data from FundusGen,
achieves the best performance in ophthalmic question-answering tasks,
surpassing the average accuracy of the 40B MedRegA by 26.6%. It also excels in
zero-shot report generation tasks, achieving a clinical consistency of 77.0%,
significantly outperforming GPT-4o's 47.6%. Furthermore, we reveal a scaling
law between data quality and model capability ($L \propto N^{0.068}$),
demonstrating that the cognitive alignment annotations in FundusGen enhance
data utilization efficiency. By integrating region-level localization with
diagnostic reasoning chains, our work develops a scalable, clinically-aligned
MLLM and explores a pathway toward bridging the visual-language gap in specific
MLLMs. Our project can be found at https://github.com/MeteorElf/FundusExpert.

</details>


### [57] [Simulating multiple human perspectives in socio-ecological systems using large language models](https://arxiv.org/abs/2507.17680)
*Yongchao Zeng,Calum Brown,Ioannis Kyriakou,Ronja Hotz,Mark Rounsevell*

Main category: cs.AI

TL;DR: 本文开发了HoPeS（面向人类的视角转换）建模框架，使用大语言模型驱动的智能体代表不同利益相关者，让用户能够体验和探索不同的利益相关者视角，以更好地理解社会生态系统。


<details>
  <summary>Details</summary>
Motivation: 理解社会生态系统需要从多元利益相关者视角获得洞察，但这些视角往往难以获取。现有方法缺乏让用户体验和转换不同利益相关者视角的有效途径。

Method: 开发HoPeS建模框架，使用大语言模型驱动的智能体代表各种利益相关者；设计仿真协议作为"脚手架"来简化多视角转换仿真；构建原型系统在制度动态和土地利用变化背景下演示该框架。

Result: 在示例实验中，用户成功扮演了系统观察者和研究者角色，体验了政策建议与实施之间的差异；用户反思体现了研究者面临的挫折感和失望情绪，但也表现出尝试不同叙述框架策略的高度动机；系统展现了探索不同视角的潜力。

Conclusion: HoPeS框架能够有效支持用户在不同利益相关者视角之间转换和整合，为社会生态仿真中的跨学科协作提供了新的可能性。进一步的系统和协议完善将能够实现新形式的跨学科合作。

Abstract: Understanding socio-ecological systems requires insights from diverse
stakeholder perspectives, which are often hard to access. To enable
alternative, simulation-based exploration of different stakeholder
perspectives, we develop the HoPeS (Human-Oriented Perspective Shifting)
modelling framework. HoPeS employs agents powered by large language models
(LLMs) to represent various stakeholders; users can step into the agent roles
to experience perspectival differences. A simulation protocol serves as a
"scaffold" to streamline multiple perspective-taking simulations, supporting
users in reflecting on, transitioning between, and integrating across
perspectives. A prototype system is developed to demonstrate HoPeS in the
context of institutional dynamics and land use change, enabling both
narrative-driven and numerical experiments. In an illustrative experiment, a
user successively adopts the perspectives of a system observer and a researcher
- a role that analyses data from the embedded land use model to inform
evidence-based decision-making for other LLM agents representing various
institutions. Despite the user's effort to recommend technically sound
policies, discrepancies persist between the policy recommendation and
implementation due to stakeholders' competing advocacies, mirroring real-world
misalignment between researcher and policymaker perspectives. The user's
reflection highlights the subjective feelings of frustration and disappointment
as a researcher, especially due to the challenge of maintaining political
neutrality while attempting to gain political influence. Despite this, the user
exhibits high motivation to experiment with alternative narrative framing
strategies, suggesting the system's potential in exploring different
perspectives. Further system and protocol refinement are likely to enable new
forms of interdisciplinary collaboration in socio-ecological simulations.

</details>


### [58] [Symbiotic Agents: A Novel Paradigm for Trustworthy AGI-driven Networks](https://arxiv.org/abs/2507.17695)
*Ilias Chatzistefanidis,Navid Nikaein*

Main category: cs.AI

TL;DR: 本文提出了一种结合大语言模型(LLM)与实时优化算法的共生代理范式，用于构建可信赖的6G AGI网络，通过输入级和输出级优化器实现精确的网络管理和服务配置。


<details>
  <summary>Details</summary>
Motivation: 随着6G网络的发展，需要从专用智能向通用人工智能(AGI)驱动的网络转变，LLM自主代理在网络管理和服务配置中的实时决策能力至关重要，但单独的LLM代理存在决策错误和资源开销问题。

Method: 设计了一种结合LLM与实时优化算法的共生代理范式：(1)输入级优化器为数值精确任务提供有界不确定性引导；(2)输出级优化器在LLM监督下实现自适应实时控制；实现了两种代理类型：无线接入网优化器和服务级协议多代理协商器。

Result: 在5G测试床上的实验结果显示：共生代理比独立LLM代理的决策错误减少了5倍；小语言模型(SLM)在保持相似精度的情况下，GPU资源开销减少99.9%，实现82ms的近实时循环；多代理协作将RAN过度利用率降低约44%。

Conclusion: 共生代理范式为下一代AGI驱动网络奠定了基础，该系统在LLM不断发展的过程中能够保持适应性、高效性和可信赖性，为6G网络的智能化管理提供了可行的解决方案。

Abstract: Large Language Model (LLM)-based autonomous agents are expected to play a
vital role in the evolution of 6G networks, by empowering real-time
decision-making related to management and service provisioning to end-users.
This shift facilitates the transition from a specialized intelligence approach,
where artificial intelligence (AI) algorithms handle isolated tasks, to
artificial general intelligence (AGI)-driven networks, where agents possess
broader reasoning capabilities and can manage diverse network functions. In
this paper, we introduce a novel agentic paradigm that combines LLMs with
real-time optimization algorithms towards Trustworthy AI, defined as symbiotic
agents. Optimizers at the LLM's input-level provide bounded uncertainty
steering for numerically precise tasks, whereas output-level optimizers
supervised by the LLM enable adaptive real-time control. We design and
implement two novel agent types including: (i) Radio Access Network optimizers,
and (ii) multi-agent negotiators for Service-Level Agreements (SLAs). We
further propose an end-to-end architecture for AGI networks and evaluate it on
a 5G testbed capturing channel fluctuations from moving vehicles. Results show
that symbiotic agents reduce decision errors fivefold compared to standalone
LLM-based agents, while smaller language models (SLM) achieve similar accuracy
with a 99.9% reduction in GPU resource overhead and in near-real-time loops of
82 ms. A multi-agent demonstration for collaborative RAN on the real-world
testbed highlights significant flexibility in service-level agreement and
resource allocation, reducing RAN over-utilization by approximately 44%.
Drawing on our findings and open-source implementations, we introduce the
symbiotic paradigm as the foundation for next-generation, AGI-driven
networks-systems designed to remain adaptable, efficient, and trustworthy even
as LLMs advance.

</details>


### [59] [Thinking Isn't an Illusion: Overcoming the Limitations of Reasoning Models via Tool Augmentations](https://arxiv.org/abs/2507.17699)
*Zhao Song,Song Yue,Jiahao Zhang*

Main category: cs.AI

TL;DR: 研究发现，虽然之前有研究质疑大型推理模型(LRMs)的有效性，但当引入工具增强（Python解释器和草稿本）后，LRMs在各种复杂度任务上都能持续超越非推理模型，挑战了"推理是幻觉"的观点。


<details>
  <summary>Details</summary>
Motivation: 近期苹果等机构的研究表明大型推理模型的逐步思维过程可能并不能真正提升推理能力，甚至在某些任务上表现不如非显式推理的LLMs，因此需要重新审视LRMs在工具增强环境下的表现。

Method: 在三个代表性LLMs及其LRM对应版本上引入两种工具增强：Python解释器和草稿本，并在苹果基准推理谜题上进行评估，比较工具增强LRMs与非推理模型在不同复杂度任务上的表现。

Result: 实验结果显示，通过适当的工具使用，LRMs在所有复杂度级别的任务上都能持续优于非推理对应模型，证明了工具增强能够显著提升LRMs的推理表现。

Conclusion: 工具增强的LRMs具有解决复杂问题的潜力，挑战了近期关于"推理是幻觉"的论述，表明在适当的工具支持下，显式推理过程确实能够增强模型的推理能力。

Abstract: Large Reasoning Models (LRMs) have become a central focus in today's large
language model (LLM) research, where models are designed to output a
step-by-step thinking process before arriving at a final answer to handle
complex reasoning tasks. Despite their promise, recent empirical studies (e.g.,
[Shojaee et al., 2025] from Apple) suggest that this thinking process may not
actually enhance reasoning ability, where LLMs without explicit reasoning
actually outperform LRMs on tasks with low or high complexity. In this work, we
revisit these findings and investigate whether the limitations of LRMs persist
when tool augmentations are introduced. We incorporate two types of tools,
Python interpreters and scratchpads, and evaluate three representative LLMs and
their LRM counterparts on Apple's benchmark reasoning puzzles. Our results show
that, with proper tool use, LRMs consistently outperform their non-reasoning
counterparts across all levels of task complexity. These findings challenge the
recent narrative that reasoning is an illusion and highlight the potential of
tool-augmented LRMs for solving complex problems.

</details>


### [60] [Online Submission and Evaluation System Design for Competition Operations](https://arxiv.org/abs/2507.17730)
*Zhe Chen,Daniel Harabor,Ryan Hechnenberger,Nathan R. Sturtevant*

Main category: cs.AI

TL;DR: 本文提出了一个在线竞赛系统，用于自动化提交和评估过程，解决了研究竞赛中的操作负担和兼容性问题


<details>
  <summary>Details</summary>
Motivation: 研究社区虽然开发了基准数据集来比较算法性能，但跟踪研究进展困难，因为论文发表在不同场所且都声称代表最先进水平。传统竞赛虽能评估算法进展，但给组织者带来巨大操作负担，需要管理和评估大量提交，且参与者在不同环境开发解决方案导致评估时出现兼容性问题

Method: 设计并实现了一个在线竞赛系统，该系统能够自动化提交和评估过程，利用隔离环境来评估提交的解决方案，使组织者能够高效管理大量提交

Result: 该系统已成功应用于多个竞赛，包括基于网格的路径查找竞赛（Grid-Based Pathfinding Competition）和机器人跑者联赛（League of Robot Runners competition）

Conclusion: 在线竞赛系统有效解决了传统研究竞赛中的操作负担和技术兼容性问题，通过自动化和隔离环境技术实现了高效的竞赛管理和评估

Abstract: Research communities have developed benchmark datasets across domains to
compare the performance of algorithms and techniques However, tracking the
progress in these research areas is not easy, as publications appear in
different venues at the same time, and many of them claim to represent the
state-of-the-art. To address this, research communities often organise periodic
competitions to evaluate the performance of various algorithms and techniques,
thereby tracking advancements in the field. However, these competitions pose a
significant operational burden. The organisers must manage and evaluate a large
volume of submissions. Furthermore, participants typically develop their
solutions in diverse environments, leading to compatibility issues during the
evaluation of their submissions. This paper presents an online competition
system that automates the submission and evaluation process for a competition.
The competition system allows organisers to manage large numbers of submissions
efficiently, utilising isolated environments to evaluate submissions. This
system has already been used successfully for several competitions, including
the Grid-Based Pathfinding Competition and the League of Robot Runners
competition.

</details>
