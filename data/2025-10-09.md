<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 11]
- [cs.CR](#cs.CR) [Total: 22]
- [cs.AI](#cs.AI) [Total: 31]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [Leveraging Large Language Models for Cybersecurity Risk Assessment -- A Case from Forestry Cyber-Physical Systems](https://arxiv.org/abs/2510.06343)
*Fikret Mert Gültekin,Oscar Lilja,Ranim Khojah,Rebekka Wohlrab,Marvin Damschen,Mazen Mohamad*

Main category: cs.SE

TL;DR: 本研究探讨了在林业领域使用本地部署的大型语言模型结合检索增强生成技术来支持网络安全风险评估，满足数据保护要求，通过专家访谈和调查验证了LLM在风险评估中的辅助作用。


<details>
  <summary>Details</summary>
Motivation: 在安全关键软件系统中，网络安全专家稀缺且工作负荷重，需要工具支持软件工程师自行进行网络安全风险评估，同时满足数据隐私保护要求。

Method: 采用设计科学研究方法，在大型项目中通过12位专家的访谈、互动会议和调查，评估本地LLM结合检索增强生成技术在网络安全风险评估中的应用。

Result: LLM能够辅助网络安全专家生成初步风险评估、识别威胁并进行冗余检查，但需要人工监督确保准确性和合规性。专家愿意在特定评估和辅助角色中使用LLM。

Conclusion: LLM代理可以支持安全关键领域网络物理系统的风险评估过程，但需要保持人类监督，不能完全依赖其生成能力。

Abstract: In safety-critical software systems, cybersecurity activities become
essential, with risk assessment being one of the most critical. In many
software teams, cybersecurity experts are either entirely absent or represented
by only a small number of specialists. As a result, the workload for these
experts becomes high, and software engineers would need to conduct
cybersecurity activities themselves. This creates a need for a tool to support
cybersecurity experts and engineers in evaluating vulnerabilities and threats
during the risk assessment process. This paper explores the potential of
leveraging locally hosted large language models (LLMs) with retrieval-augmented
generation to support cybersecurity risk assessment in the forestry domain
while complying with data protection and privacy requirements that limit
external data sharing. We performed a design science study involving 12 experts
in interviews, interactive sessions, and a survey within a large-scale project.
The results demonstrate that LLMs can assist cybersecurity experts by
generating initial risk assessments, identifying threats, and providing
redundancy checks. The results also highlight the necessity for human oversight
to ensure accuracy and compliance. Despite trust concerns, experts were willing
to utilize LLMs in specific evaluation and assistance roles, rather than solely
relying on their generative capabilities. This study provides insights that
encourage the use of LLM-based agents to support the risk assessment process of
cyber-physical systems in safety-critical domains.

</details>


### [2] [Improving Assignment Submission in Higher Education through a Git-Enabled System: An Iterative Case Study](https://arxiv.org/abs/2510.06363)
*Ololade Babatunde,Tomisin Ayodabo,Raqibul Raqibul*

Main category: cs.SE

TL;DR: 本研究通过引入和评估定制的Git-based提交系统，解决了高等教育中传统作业提交方法的挑战，显著提升了作业跟踪、协作和提交效率。


<details>
  <summary>Details</summary>
Motivation: 解决高等教育中传统作业提交方法存在的跟踪困难、协作不便和提交效率低下的问题。

Method: 采用迭代软件开发和以用户为中心的设计方法，将系统集成到真实大学环境中，并进行可用性测试和学生反馈收集。

Result: 85%的教师认为Git系统更易使用，84%的学生偏好该系统，提交和评审时间减少38%，存储需求降低48%。

Conclusion: Git-based系统为教育环境中集成分布式版本控制提供了实用见解，增强了教师监督和学生参与度。

Abstract: This study addresses challenges in traditional assignment submission methods
used in higher education by introducing and evaluating a customized Git-based
submission system. Employing iterative software development and user-centered
design methodologies, the system was integrated within a real-world university
environment. Empirical evaluation, including usability testing and student
feedback, indicated significant improvements in assignment tracking,
collaboration, and submission efficiency. Students reported positive
experiences using distributed version control workflows, highlighting improved
learning outcomes and reduced administrative burden. Challenges related to
initial adoption and student learning curves were identified and mitigated
through iterative improvements. The proposed system contributes practical
insights for integrating distributed version control into educational settings,
enhancing both instructor oversight and student engagement in software
engineering and related disciplines. Based on our results, the research showed
that 85% of instructors found the git based system easier to use, with 84% of
students preferring it over traditional methods, as it provides a 38% reduction
in time taken for submission and review, while also leading to a 48% reduction
in storage requirements.

</details>


### [3] [Addressing Visual Impairments with Model-Driven Engineering: A Systematic Literature Review](https://arxiv.org/abs/2510.06483)
*Judith Michael,Lukas Netz,Bernhard Rumpe,Ingo Müller,John Grundy,Shavindra Wickramathilaka,Hourieh Khalajzadeh*

Main category: cs.SE

TL;DR: 本文通过系统文献综述分析了模型驱动工程（MDE）如何解决视觉障碍的可访问性问题，发现当前MDE研究对视觉相关可访问性的支持不足，并提出改进研究议程。


<details>
  <summary>Details</summary>
Motivation: 软件应用常为有可访问性需求的用户（如视觉障碍者）设置障碍。模型驱动工程（MDE）通过系统化的代码派生方法，为将可访问性关注整合到软件开发中提供了系统化方法，同时减少人工工作量。

Method: 对MDE如何解决视觉障碍可访问性进行系统文献综述，从447篇初步识别的论文中筛选出30篇符合纳入标准的主要研究进行分析。

Result: 约三分之二的研究参考了Web内容可访问性指南（WCAG），但项目特定适配和最终用户验证阻碍了在MDE中的更广泛采用。研究建模了用户界面结构、交互导航、用户能力、需求和上下文信息，但只有少数研究具体说明了如何整合可访问性需求或展示了完全功能的系统。MDE方法细节不足（如转换规则或代码模板）阻碍了重用性、通用性和可重复性。

Conclusion: 当前MDE研究对视觉相关可访问性的支持不足。受影响的用户参与有限和开发人员在可访问性方面的专业知识不足导致实证验证薄弱。本文最后提出了一个研究议程，概述如何更有效地将视觉障碍支持嵌入MDE流程。

Abstract: Software applications often pose barriers for users with accessibility needs,
e.g., visual impairments. Model-driven engineering (MDE), with its systematic
nature of code derivation, offers systematic methods to integrate accessibility
concerns into software development while reducing manual effort. This paper
presents a systematic literature review on how MDE addresses accessibility for
vision impairments. From 447 initially identified papers, 30 primary studies
met the inclusion criteria. About two-thirds reference the Web Content
Accessibility Guidelines (WCAG), yet their project-specific adaptions and
end-user validations hinder wider adoption in MDE. The analyzed studies model
user interface structures, interaction and navigation, user capabilities,
requirements, and context information. However, only few specify concrete
modeling techniques on how to incorporate accessibility needs or demonstrate
fully functional systems. Insufficient details on MDE methods, i.e.,
transformation rules or code templates, hinder the reuse, generalizability, and
reproducibility. Furthermore, limited involvement of affected users and limited
developer expertise in accessibility contribute to weak empirical validation.
Overall, the findings indicate that current MDE research insufficiently
supports vision-related accessibility. Our paper concludes with a research
agenda outlining how support for vision impairments can be more effectively
embedded in MDE processes.

</details>


### [4] [Beyond More Context: How Granularity and Order Drive Code Completion Quality](https://arxiv.org/abs/2510.06606)
*Uswat Yusuf,Genevieve Caumartin,Diego Elias Costa*

Main category: cs.SE

TL;DR: 本文提出了一种基于静态分析的代码块检索方法，用于改进代码补全中的上下文收集策略，在ASE 2025挑战中相比文件级检索提升了6%的性能。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在代码补全中需要充分相关的上下文信息，但在大型代码库中面临两个挑战：LLM上下文长度限制无法包含所有文件，以及生成代码质量对噪声上下文高度敏感。

Method: 开发了文件和代码块级别的检索策略，重点研究上下文大小和文件排序对LLM性能的影响，并引入了基于静态分析的代码块检索方法。

Result: 上下文数量和顺序显著影响模型性能，基于静态分析的代码块检索比最佳文件检索策略提升了6%，比无上下文基线在Python上提升了16%。

Conclusion: 检索粒度、排序策略和混合方法对于构建现实开发场景中的有效上下文收集管道至关重要。

Abstract: Context plays an important role in the quality of code completion, as Large
Language Models (LLMs) require sufficient and relevant information to assist
developers in code generation tasks. However, composing a relevant context for
code completion poses challenges in large repositories: First, the limited
context length of LLMs makes it impractical to include all repository files.
Second, the quality of generated code is highly sensitive to noisy or
irrelevant context. In this paper, we present our approach for the ASE 2025
Context Collection Challenge. The challenge entails outperforming JetBrains
baselines by designing effective retrieval and context collection strategies.
We develop and evaluate a series of experiments that involve retrieval
strategies at both the file and chunk levels. We focus our initial experiments
on examining the impact of context size and file ordering on LLM performance.
Our results show that the amount and order of context can significantly
influence the performance of the models. We introduce chunk-based retrieval
using static analysis, achieving a 6% improvement over our best file-retrieval
strategy and a 16% improvement over the no-context baseline for Python in the
initial phase of the competition. Our results highlight the importance of
retrieval granularity, ordering and hybrid strategies in developing effective
context collection pipelines for real-world development scenarios.

</details>


### [5] [AISysRev -- LLM-based Tool for Title-abstract Screening](https://arxiv.org/abs/2510.06708)
*Aleksi Huotala,Miikka Kuutila,Olli-Pekka Turtio,Mika Mäntylä*

Main category: cs.SE

TL;DR: 开发了AiSysRev工具，利用大语言模型辅助系统综述的文献筛选工作，通过实验发现LLM在边界案例中容易出错，需要人工干预，但能显著减轻文献筛选负担。


<details>
  <summary>Details</summary>
Motivation: 系统综述的文献筛选阶段工作量巨大，需要评估大量论文是否符合纳入标准。虽然LLM不能完全信任，但可以在快速综述中提供帮助。

Method: 开发了基于LLM的筛选工具AiSysRev，作为Docker容器中的Web应用。工具接受包含论文标题和摘要的CSV文件，用户指定纳入和排除标准，支持通过OpenRouter使用多个LLM，提供零样本和少样本筛选，以及显示LLM结果作为人工筛选指导的界面。

Result: 对137篇论文的试验研究发现，论文可分为四类：简单纳入、简单排除、边界纳入和边界排除。LLM在边界案例中容易出错，需要人工干预。

Conclusion: LLM不能替代系统综述中的人工判断，但能显著减轻评估大量科学文献的负担。

Abstract: Systematic reviews are a standard practice for summarizing the state of
evidence in software engineering. Conducting systematic reviews is laborious,
especially during the screening or study selection phase, where the number of
papers can be overwhelming. During this phase, papers are assessed against
inclusion and exclusion criteria based on their titles and abstracts. Recent
research has demonstrated that large language models (LLMs) can perform
title-abstract screening at a level comparable to that of a master's student.
While LLMs cannot be fully trusted, they can help, for example, in Rapid
Reviews, which try to expedite the review process. Building on recent research,
we developed AiSysRev, an LLM-based screening tool implemented as a web
application running in a Docker container. The tool accepts a CSV file
containing paper titles and abstracts. Users specify inclusion and exclusion
criteria. One can use multiple LLMs for screening via OpenRouter. AiSysRev
supports both zero-shot and few-shot screening, and also allows for manual
screening through interfaces that display LLM results as guidance for human
reviewers.We conducted a trial study with 137 papers using the tool. Our
findings indicate that papers can be classified into four categories: Easy
Includes, Easy Excludes, Boundary Includes, and Boundary Excludes. The Boundary
cases, where LLMs are prone to errors, highlight the need for human
intervention. While LLMs do not replace human judgment in systematic reviews,
they can significantly reduce the burden of assessing large volumes of
scientific literature. Video: https://www.youtube.com/watch?v=jVbEj4Y4tQI Tool:
https://github.com/EvoTestOps/AISysRev

</details>


### [6] [LLM Company Policies and Policy Implications in Software Organizations](https://arxiv.org/abs/2510.06718)
*Ranim Khojah,Mazen Mohamad,Linda Erlenhov,Francisco Gomes de Oliveira Neto,Philipp Leitner*

Main category: cs.SE

TL;DR: 分析11家公司如何制定LLM聊天机器人使用政策及其影响因素，帮助管理者安全地将聊天机器人集成到开发工作流程中


<details>
  <summary>Details</summary>
Motivation: 软件组织中采用大型语言模型(LLM)聊天机器人存在风险，需要明确的政策来规范使用

Method: 研究11家公司制定LLM聊天机器人使用政策的过程和影响因素

Result: 揭示了公司制定相关政策的具体做法和关键影响因素

Conclusion: 为管理者提供了安全集成聊天机器人到开发工作流程的指导

Abstract: The risks associated with adopting large language model (LLM) chatbots in
software organizations highlight the need for clear policies. We examine how 11
companies create these policies and the factors that influence them, aiming to
help managers safely integrate chatbots into development workflows.

</details>


### [7] [Oops!... I did it again. Conclusion (In-)Stability in Quantitative Empirical Software Engineering: A Large-Scale Analysis](https://arxiv.org/abs/2510.06844)
*Nicole Hoess,Carlos Paradis,Rick Kazman,Wolfgang Mauerer*

Main category: cs.SE

TL;DR: 本研究通过复制三个软件演化分析研究，评估了四种独立挖掘工具在数据提取、分析结果和结论方面的一致性，发现工具设计和实现中的技术细节会导致显著差异。


<details>
  <summary>Details</summary>
Motivation: 软件仓库挖掘工具被广泛用于研究软件项目演化，但其局限性和一致性往往未被充分理解，需要评估复杂工具管道对研究有效性的威胁。

Method: 通过轻量级文献综述选择三个关于协作协调、软件维护和软件质量的研究，使用四种独立挖掘工具进行正式复制，定量和定性比较提取的数据、分析结果和结论。

Result: 工具设计和实现中的众多技术细节在复杂挖掘管道中累积，可能导致提取的基线数据、其衍生数据、统计分析结果以及特定情况下的结论出现显著差异。

Conclusion: 用户必须仔细选择工具并评估其局限性，以充分评估有效性范围。建议重用工具，研究人员和工具作者可通过复制包和比较研究促进可重用性并减少不确定性。

Abstract: Context: Mining software repositories is a popular means to gain insights
into a software project's evolution, monitor project health, support decisions
and derive best practices. Tools supporting the mining process are commonly
applied by researchers and practitioners, but their limitations and agreement
are often not well understood.
  Objective: This study investigates some threats to validity in complex tool
pipelines for evolutionary software analyses and evaluates the tools' agreement
in terms of data, study outcomes and conclusions for the same research
questions.
  Method: We conduct a lightweight literature review to select three studies on
collaboration and coordination, software maintenance and software quality from
high-ranked venues, which we formally replicate with four independent,
systematically selected mining tools to quantitatively and qualitatively
compare the extracted data, analysis results and conclusions.
  Results: We find that numerous technical details in tool design and
implementation accumulate along the complex mining pipelines and can cause
substantial differences in the extracted baseline data, its derivatives,
subsequent results of statistical analyses and, under specific circumstances,
conclusions.
  Conclusions: Users must carefully choose tools and evaluate their limitations
to assess the scope of validity in an adequate way. Reusing tools is
recommended. Researchers and tool authors can promote reusability and help
reducing uncertainties by reproduction packages and comparative studies
following our approach.

</details>


### [8] [An empirical study on declined proposals: why are these proposals declined?](https://arxiv.org/abs/2510.06984)
*Masanari Kondo,Mahmoud Alfadel,Shane McIntosh,Yasutaka Kamei,Naoyasu Ubayashi*

Main category: cs.SE

TL;DR: 该研究分析了Go编程语言项目的提案机制，发现提案被拒绝的频率高于接受，且处理时间较长。研究识别了9个主要拒绝原因，并证明GPT模型可以在讨论早期预测提案结果。


<details>
  <summary>Details</summary>
Motivation: 开源软件项目的提案过程资源密集且经常导致贡献者挫败，特别是当提案被拒绝但缺乏明确反馈时。理解提案被拒绝的原因对于优化流程和指导贡献者至关重要。

Method: 对Go项目的1,091个提案进行混合方法实证研究，包括量化提案结果、构建拒绝原因分类法，以及评估大型语言模型预测这些结果的能力。

Result: 提案被拒绝的比例高于接受，解决通常需要超过一个月。仅14.7%的被拒绝提案会重新提交。识别出9个关键拒绝原因，如重复、用例有限或违反项目原则。GPT模型可以在讨论早期预测拒绝决策（F1分数=0.71）。

Conclusion: 研究揭示了提案过程中的低效性，并强调了通过早期分类和基于过去拒绝原因的结构化理解来指导贡献者加强提案的机会，从而改善贡献者体验和审阅者工作负载。

Abstract: Design-level decisions in open-source software (OSS) projects are often made
through structured mechanisms such as proposals, which require substantial
community discussion and review. Despite their importance, the proposal process
is resource-intensive and often leads to contributor frustration, especially
when proposals are declined without clear feedback. Yet, the reasons behind
proposal rejection remain poorly understood, limiting opportunities to
streamline the process or guide contributors effectively. This study
investigates the characteristics and outcomes of proposals in the Go
programming language to understand why proposals are declined and how such
outcomes might be anticipated. We conduct a mixed-method empirical study on
1,091 proposals submitted to the Go project. We quantify proposal outcomes,
build a taxonomy of decline reasons, and evaluate large language models (LLMs)
for predicting these outcomes. We find that proposals are more often declined
than accepted, and resolution typically takes over a month. Only 14.7% of
declined proposals are ever resubmitted. Through qualitative coding, we
identify nine key reasons for proposal decline, such as duplication, limited
use cases, or violations of project principles. This taxonomy can help
contributors address issues in advance, e.g., checking for existing
alternatives can reduce redundancy. We also demonstrate that GPT-based models
can predict decline decisions early in the discussion (F1 score = 0.71 with
partial comments), offering a practical tool for prioritizing review effort.
Our findings reveal inefficiencies in the proposal process and highlight
actionable opportunities for improving both contributor experience and reviewer
workload by enabling early triage and guiding contributors to strengthen their
proposals using a structured understanding of past decline reasons.

</details>


### [9] [Human-aligned AI Model Cards with Weighted Hierarchy Architecture](https://arxiv.org/abs/2510.06989)
*Pengyue Yang,Haolin Jin,Qingwen Zeng,Jiawen Wen,Harry Rao,Huaming Chen*

Main category: cs.SE

TL;DR: 提出了CRAI-MCF框架，从静态披露转向可操作、人类对齐的文档，解决大语言模型发现和采用中的挑战。


<details>
  <summary>Details</summary>
Motivation: 大语言模型生态快速发展，但模型发现和采用面临挑战，现有文档框架如Model Cards和FactSheets存在静态、定性为主、缺乏定量比较机制等问题。

Method: 基于价值敏感设计，通过对240个开源项目的实证分析，提炼出217个参数，构建了包含八个模块的价值对齐架构，并引入定量充分性标准。

Result: 开发了CRAI-MCF框架，能够实现严格的跨模型比较，平衡技术、伦理和操作维度。

Conclusion: CRAI-MCF框架使从业者能够更有效地评估、选择和采用大语言模型，提高信心和操作完整性。

Abstract: The proliferation of Large Language Models (LLMs) has led to a burgeoning
ecosystem of specialized, domain-specific models. While this rapid growth
accelerates innovation, it has simultaneously created significant challenges in
model discovery and adoption. Users struggle to navigate this landscape due to
inconsistent, incomplete, and imbalanced documentation across platforms.
Existing documentation frameworks, such as Model Cards and FactSheets, attempt
to standardize reporting but are often static, predominantly qualitative, and
lack the quantitative mechanisms needed for rigorous cross-model comparison.
This gap exacerbates model underutilization and hinders responsible adoption.
To address these shortcomings, we introduce the Comprehensive Responsible AI
Model Card Framework (CRAI-MCF), a novel approach that transitions from static
disclosures to actionable, human-aligned documentation. Grounded in Value
Sensitive Design (VSD), CRAI-MCF is built upon an empirical analysis of 240
open-source projects, distilling 217 parameters into an eight-module,
value-aligned architecture. Our framework introduces a quantitative sufficiency
criterion to operationalize evaluation and enables rigorous cross-model
comparison under a unified scheme. By balancing technical, ethical, and
operational dimensions, CRAI-MCF empowers practitioners to efficiently assess,
select, and adopt LLMs with greater confidence and operational integrity.

</details>


### [10] [Building an Open AIBOM Standard in the Wild](https://arxiv.org/abs/2510.07070)
*Gopi Krishnan Rajbahadur,Keheliya Gallaba,Elyas Rashno,Arthit Suriyawongkul,Karen Bennet,Kate Stewart,Ahmed E. Hassan*

Main category: cs.SE

TL;DR: 本文介绍了AI物料清单(AIBOM)规范的开发经验，这是SPDX软件物料清单标准的扩展，用于捕获AI组件如数据集和训练工件。通过行动研究方法，记录了全球多利益相关方合作过程，并通过法规对齐、行业用例映射、访谈和案例研究验证了规范。


<details>
  <summary>Details</summary>
Motivation: 随着AI系统快速发展，如何为这类系统创建开放社区驱动的标准仍未被充分探索。本文旨在探索在快速演进的AI领域如何建立标准化的物料清单规范。

Method: 采用行动研究方法，组织全球90多位贡献者参与结构化AR周期。通过四种互补方法验证规范：与主要法规和伦理标准对齐、系统映射到六个行业用例、半结构化从业者访谈、工业案例研究。

Result: 开发出经过验证的AIBOM规范，该规范扩展了SPDX标准以支持AI组件。验证表明规范与欧盟AI法案、IEEE 7000等标准兼容，并适用于多种行业场景。

Conclusion: 本文不仅交付了验证过的AIBOM规范，还记录了在真实环境中构建规范的过程，反思了与AR周期的对齐，并提炼了可为软件工程社区未来标准化工作提供参考的经验教训。

Abstract: Modern software engineering increasingly relies on open, community-driven
standards, yet how such standards are created in fast-evolving domains like
AI-powered systems remains underexplored. This paper presents a detailed
experience report on the development of the AI Bill of Materials AIBOM
specification, an extension of the ISO/IEC 5962:2021 Software Package Data
Exchange (SPDX) software bill of materials (SBOM) standard, which captures AI
components such as datasets and iterative training artifacts. Framed through
the lens of Action Research (AR), we document a global, multi-stakeholder
effort involving over 90 contributors and structured AR cycles. The resulting
specification was validated through four complementary approaches: alignment
with major regulations and ethical standards (e.g., EU AI Act and IEEE 7000
standards), systematic mapping to six industry use cases, semi-structured
practitioner interviews, and an industrial case study. Beyond delivering a
validated artefact, our paper documents the process of building the AIBOM
specification in the wild, and reflects on how it aligns with the AR cycle, and
distills lessons that can inform future standardization efforts in the software
engineering community.

</details>


### [11] [Prompt, Synthesize, Fine-Tune: A Secure Code Generation Recipe](https://arxiv.org/abs/2510.07189)
*Junjie Li,Fazle Rabbi,Bo Yang,Song Wang,Jinqiu Yang*

Main category: cs.SE

TL;DR: Secure-Instruct是一个自动合成高质量漏洞和安全代码示例的框架，通过指令微调提升LLM生成安全代码的能力，在安全性和功能正确性方面均有显著改进。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型生成的代码往往存在安全隐患，现有方法（如SafeCoder）受限于有限且不平衡的数据集，影响了其有效性和泛化能力。

Method: 提出Secure-Instruct框架，自动合成漏洞和安全代码示例，生成微调指令，并通过指令微调使LLM对齐任务描述和安全代码生成能力。

Result: 在CWEBench上，Secure-Instruct将安全代码生成的安全比率平均提升14.3%，比SafeCoder高出7.6%；在CWEval上，Func-Sec@1指标分别提升14%（CodeLlama-7B）和5.8%（Mistral-7B），比SafeCoder分别高出15.8%和6.8%。

Conclusion: Secure-Instruct不仅能提高生成代码的安全性，还能提升其功能正确性，为安全代码生成提供了有效的解决方案。

Abstract: Although Large Language Models (LLMs) show promising solutions to automated
code generation, they often produce insecure code that threatens software
security. Current approaches (e.g., SafeCoder) to improve secure code
generation suffer from limited and imbalanced datasets, reducing their
effectiveness and generalizability. In this work, we present Secure-Instruct, a
novel framework that automatically synthesizes high-quality vulnerable and
secure code examples, generates fine-tuning instructions, and instruction-tunes
LLMs to align task description and secure code generation abilities. We
evaluate Secure-Instruct on four representative LLMs using two benchmarks: our
own CWEBench and the existing CWEval. CWEBench comprises 93 scenarios on 44
CWEs, all without overlap with Secure-Instruct's synthetic instruction-tuning
dataset, while CWEval covers 31 CWEs with 119 manually verified
security-critical tasks. We find that Secure-Instruct improves not only the
security but also the functional correctness of the generated code. On
CWEBench, Secure-Instruct substantially improves secure code generation, giving
a 14.3% average increase in secure ratio over the pretrained models and
outperforms SafeCoder by 7.6%. On CWEval, Secure-Instruct achieves a 14%
increase for CodeLlama-7B and 5.8% for Mistral-7B in Func-Sec@1 over pretrained
models, and surpasses SafeCoder by 15.8% and 6.8% respectively.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [12] [Automated Repeatable Adversary Threat Emulation with Effects Language (EL)](https://arxiv.org/abs/2510.06420)
*Suresh K. Damodaran,Paul D. Rowe*

Main category: cs.CR

TL;DR: 本文介绍了使用效果语言(EL)来自动化模拟高级持续性威胁的多步骤攻击，以训练防御者和评估防御工具。


<details>
  <summary>Details</summary>
Motivation: 自动化模拟多步骤攻击对于训练防御者和评估防御工具很有价值，但面临许多挑战和需求。

Method: 引入效果语言(EL)，一种具有基于图的操作语义的可视化编程语言，来应对这些挑战。正式定义了EL的执行语义并证明了重要的执行属性。

Result: 展示了EL在编码攻击场景中的应用，并演示了如何利用EL为复杂多步骤攻击提供攻击证明。结果表明使用EL进行可重复自动化显著提高了时间和资源效率。

Conclusion: 效果语言(EL)是解决多步骤攻击自动化挑战的有效解决方案，能够提高训练和评估的效率。

Abstract: The emulation of multi-step attacks attributed to advanced persistent threats
is valuable for training defenders and evaluating defense tools. In this paper,
we discuss the numerous challenges and desired attributes associated with such
automation. Additionally, we introduce the use of Effects Language (EL), a
visual programming language with graph-based operational semantics, as a
solution to address many of these challenges and requirements. We formally
define the execution semantics of EL, and prove important execution properties.
Furthermore, we showcase the application of EL to codify attacks using an
example from one of the publicly available attack scenarios. We also
demonstrate how EL can be utilized to provide proof-of-attack of complex
multi-step attacks. Our results highlight the improvements in time and resource
efficiency achieved through the use of EL for repeatable automation.

</details>


### [13] [Breaking Precision Time: OS Vulnerability Exploits Against IEEE 1588](https://arxiv.org/abs/2510.06421)
*Muhammad Abdullah Soomro,Fatima Muhammad Anwar*

Main category: cs.CR

TL;DR: 本文首次系统研究了针对PTP协议的内核级攻击，展示了特权攻击者如何在不修改PTP网络流量的情况下通过破坏关键接口来操纵系统时间，绕过了现有的PTP安全扩展。


<details>
  <summary>Details</summary>
Motivation: 现有PTP安全研究主要关注网络攻击，假设主机未被入侵。本文发现了当前威胁模型中的关键盲点：来自运行PTP堆栈主机内部的内核级攻击者。

Method: 实现了三种攻击原语：恒定偏移、渐进偏移和随机抖动，使用内核有效载荷，并在广泛使用的ptp4l和phc2sys守护进程上评估其影响。

Result: 实验表明这些攻击可以静默地破坏时钟同步，绕过现有的PTP安全扩展。

Conclusion: 这些发现强调了重新考虑主机级信任假设并将内核完整性纳入安全时间同步系统设计的紧迫性。

Abstract: The Precision Time Protocol (PTP), standardized as IEEE 1588, provides
sub-microsecond synchronization across distributed systems and underpins
critical infrastructure in telecommunications, finance, power systems, and
industrial automation. While prior work has extensively analyzed PTP's
vulnerability to network-based attacks, prompting the development of
cryptographic protections and anomaly detectors, these defenses presume an
uncompromised host. In this paper, we identify and exploit a critical blind
spot in current threat models: kernel-level adversaries operating from within
the host running the PTP stack. We present the first systematic study of
kernel-rooted attacks on PTP, demonstrating how privileged attackers can
manipulate system time by corrupting key interfaces without altering PTP
network traffic. We implement three attack primitives, constant offset,
progressive skew, and random jitter, using in-kernel payloads, and evaluate
their impact on the widely used ptp4l and phc2sys daemons. Our experiments
reveal that these attacks can silently destabilize clock synchronization,
bypassing existing PTP security extensions. These findings highlight the urgent
need to reconsider host-level trust assumptions and integrate kernel integrity
into the design of secure time synchronization systems.

</details>


### [14] [Proofs of No Intrusion](https://arxiv.org/abs/2510.06432)
*Vipul Goyal,Justin Raizes*

Main category: cs.CR

TL;DR: 提出了一种名为"无入侵证明"的新方法，允许经典客户端远程检测量子服务器是否被入侵且数据被盗，无需破坏被测试的数据，也无需在别处存储备份。


<details>
  <summary>Details</summary>
Motivation: 数据安全的核心挑战不仅是防止盗窃，还要检测是否发生了盗窃。经典情况下这是不可能的，因为完美复制不会留下证据，但量子力学禁止一般复制，这为检测提供了新的可能性。

Method: 基于完全同态加密构建了密文的无入侵证明，并为不可克隆基元（如不可克隆解密密钥和签名令牌）配备了无入侵证明。核心技术是一种使用经典通信非破坏性测试陪集状态的新方法。

Result: 成功定义了无入侵证明的概念并实现了构造，能够非破坏性地验证量子服务器是否被入侵，且该方法适用于几乎所有不可克隆基元。

Conclusion: 无入侵证明为量子环境下的数据安全检测提供了可行的解决方案，利用量子力学的不可克隆特性实现了经典方法无法达到的安全检测能力。

Abstract: A central challenge in data security is not just preventing theft, but
detecting whether it has occurred. Classically, this is impossible because a
perfect copy leaves no evidence. Quantum mechanics, on the other hand, forbids
general duplication, opening up new possibilities.
  We introduce Proofs of No Intrusion, which enable a classical client to
remotely test whether a quantum server has been hacked and the client's data
stolen. Crucially, the test does not destroy the data being tested, avoiding
the need to store a backup elsewhere. We define and construct proofs of no
intrusion for ciphertexts assuming fully homomorphic encryption. Additionally,
we show how to equip several constructions of unclonable primitives with proofs
of non-intrusion, such as unclonable decryption keys and signature tokens.
Conceptually, proofs of non-intrusion can be defined for essentially any
unclonable primitive.
  At the heart of our techniques is a new method for non-destructively testing
coset states with classical communication. It can be viewed as a
non-destructive proof of knowledge of a measurement result of the coset state.

</details>


### [15] [BATTLE for Bitcoin: Capital-Efficient Optimistic Bridges with Large Committees](https://arxiv.org/abs/2510.06468)
*Sergio Demian Lerner,Ariel Futoransky*

Main category: cs.CR

TL;DR: BATTLE for Bitcoin 是一个抗DoS的争议解决层，通过BitVM风格的FLEX组件和乱码电路，在比特币UTXO模型上实现乐观桥的安全保障。


<details>
  <summary>Details</summary>
Motivation: 为比特币与rollups或侧链之间的乐观桥提供安全的争议解决机制，抵御DoS攻击，同时保持高去中心化程度。

Method: 采用BATTLE锦标赛协议，使用BitVM风格的FLEX组件和乱码电路，结合按需L1安全保证金，仅依赖标准时间锁和预签名交易DAG。

Result: 争议在对数轮次内解决，奖励可回收，诚实断言者的最小初始资本保持恒定，支持高达约1000个操作者的高去中心化。

Conclusion: 该协议为比特币生态系统提供了实用的、抗DoS的争议解决方案，无需新的操作码即可实现高安全性和去中心化。

Abstract: We present BATTLE for Bitcoin, a DoS-resilient dispute layer that secures
optimistic bridges between Bitcoin and rollups or sidechains. Our design adapts
the BATTLE tournament protocol to Bitcoin's UTXO model using BitVM-style FLEX
components and garbled circuits with on-demand L1 security bonds. Disputes are
resolved in logarithmic rounds while recycling rewards, keeping the honest
asserter's minimum initial capital constant even under many permissionless
challengers. The construction is fully contestable (challengers can supply
higher-work counter-proofs) and relies only on standard timelocks and
pre-signed transaction DAGs, without new opcodes.
  For $N$ operators, the protocol requires $O(N^2)$ pre-signed transactions,
signatures, and message exchanges, yet remains practical at $N\!\gtrsim\!10^3$,
enabling high decentralization.

</details>


### [16] [From Description to Detection: LLM based Extendable O-RAN Compliant Blind DoS Detection in 5G and Beyond](https://arxiv.org/abs/2510.06530)
*Thusitha Dayaratne,Ngoc Duy Pham,Viet Vo,Shangqi Lai,Sharif Abuadbba,Hajime Suzuki,Xingliang Yuan,Carsten Rudolph*

Main category: cs.CR

TL;DR: 提出了一种基于大语言模型(LLM)的零样本异常检测框架，用于检测5G网络中RRC/NAS控制平面协议的安全威胁，在O-RAN架构下实现高效攻击检测。


<details>
  <summary>Details</summary>
Motivation: 5G及后续移动通信技术虽然提升了通信质量，但控制平面协议(RRC/NAS)存在安全漏洞，容易遭受盲拒绝服务等攻击。现有基于规则系统或传统机器学习的检测方法需要大量训练数据、预定义规则且可解释性有限。

Method: 利用大语言模型(LLM)的零样本能力，结合无序数据和简短自然语言攻击描述，在O-RAN架构中构建异常检测框架。分析了提示语变化的鲁棒性，展示了攻击描述自动化的可行性。

Result: 使用RRC/NAS数据集评估表明，检测质量依赖于攻击描述的语义完整性而非措辞或长度。通过开源和专有LLM实现的广泛比较，证明了在攻击检测方面的优越性能。

Conclusion: 该框架在O-RAN实时约束下具有实用性，能够有效检测Layer-3攻击，展现了在5G安全防护中的潜力。

Abstract: The quality and experience of mobile communication have significantly
improved with the introduction of 5G, and these improvements are expected to
continue beyond the 5G era. However, vulnerabilities in control-plane
protocols, such as Radio Resource Control (RRC) and Non-Access Stratum (NAS),
pose significant security threats, such as Blind Denial of Service (DoS)
attacks. Despite the availability of existing anomaly detection methods that
leverage rule-based systems or traditional machine learning methods, these
methods have several limitations, including the need for extensive training
data, predefined rules, and limited explainability. Addressing these
challenges, we propose a novel anomaly detection framework that leverages the
capabilities of Large Language Models (LLMs) in zero-shot mode with unordered
data and short natural language attack descriptions within the Open Radio
Access Network (O-RAN) architecture. We analyse robustness to prompt variation,
demonstrate the practicality of automating the attack descriptions and show
that detection quality relies on the semantic completeness of the description
rather than its phrasing or length. We utilise an RRC/NAS dataset to evaluate
the solution and provide an extensive comparison of open-source and proprietary
LLM implementations to demonstrate superior performance in attack detection. We
further validate the practicality of our framework within O-RAN's real-time
constraints, illustrating its potential for detecting other Layer-3 attacks.

</details>


### [17] [SpyChain: Multi-Vector Supply Chain Attacks on Small Satellite Systems](https://arxiv.org/abs/2510.06535)
*Jack Vanlyssel,Enrique Sobrados,Ramsha Anwar,Gruia-Catalin Roman,Afsah Anwar*

Main category: cs.CR

TL;DR: SpyChain是首个针对小型卫星的端到端硬件供应链威胁设计和实现，展示了独立和共谋的硬件攻击如何规避测试、窃取遥测数据、破坏操作，并通过隐蔽通道发起拒绝服务攻击。


<details>
  <summary>Details</summary>
Motivation: 小型卫星依赖商用现货硬件，扩大了攻击面。虽然供应链威胁在其他网络物理领域已有研究，但在空间系统中的可行性和隐蔽性尚未充分探索。辅助COTS组件缺乏安全保障却拥有对关键资源的访问权限，这种内部威胁被忽视。

Method: 使用NASA卫星模拟器(NOS3)，设计了从简单独立组件到动态协调恶意软件的升级攻击链，引入了五种场景的隐蔽性分类，展示了新型多组件执行技术。

Result: SpyChain能够规避测试、窃取遥测数据、破坏操作、发起DoS攻击，通过隐蔽通道绕过地面监控。研究得到了NASA NOS3团队的确认和肯定。

Conclusion: 对辅助组件的隐式信任使得隐蔽持久性成为可能，揭示了新的攻击向量。实现了轻量级机载防御，包括运行时监控，以缓解类似SpyChain的威胁。

Abstract: Small satellites are integral to scientific, commercial, and defense
missions, but reliance on commercial off-the-shelf (COTS) hardware broadens
their attack surface. Although supply chain threats are well studied in other
cyber-physical domains, their feasibility and stealth in space systems remain
largely unexplored. Prior work has focused on flight software, which benefits
from strict security practices and oversight. In contrast, auxiliary COTS
components often lack robust assurance yet enjoy comparable access to critical
on-board resources, including telemetry, system calls, and the software bus.
Despite this privileged access, the insider threat within COTS hardware supply
chains has received little attention. In this work, we present SpyChain, the
first end-to-end design and implementation of independent and colluding
hardware supply chain threats targeting small satellites. Using NASA's
satellite simulation (NOS3), we demonstrate that SpyChain can evade testing,
exfiltrate telemetry, disrupt operations, and launch Denial of Service (DoS)
attacks through covert channels that bypass ground monitoring. Our study traces
an escalation from a simple solo component to dynamic, coordinating malware,
introducing a taxonomy of stealth across five scenarios. We showcase how
implicit trust in auxiliary components enables covert persistence and reveal
novel attack vectors, highlighting a new multi-component execution technique
that is now incorporated into the SPARTA matrix. Our findings are reinforced by
acknowledgment and affirmation from NASA's NOS3 team. Finally, we implement
lightweight onboard defenses, including runtime monitoring, to mitigate threats
like SpyChain.

</details>


### [18] [Auto-Stega: An Agent-Driven System for Lifelong Strategy Evolution in LLM-Based Text Steganography](https://arxiv.org/abs/2510.06565)
*Jiuan Zhou,Yu Cheng,Yuan Xie,Zhaoxia Yin*

Main category: cs.CR

TL;DR: Auto-Stega是一个基于LLM的自我进化文本隐写框架，通过自动发现、组合和调整隐写策略，在高嵌入率下实现高效、不可感知和安全的文本隐写。


<details>
  <summary>Details</summary>
Motivation: 现有文本隐写方法依赖手工或预定义策略，难以在高嵌入率下平衡效率、不可感知性和安全性，需要更智能的自适应解决方案。

Method: 提出Auto-Stega框架，采用代理驱动的自我进化机制，通过生成、评估、总结和更新的闭环过程自动构建策略库；引入PC-DNTE算法在高嵌入率下保持条件分布对齐。

Result: 在高嵌入率下，Auto-Stega相比现有SOTA方法在困惑度上提升42.2%，在抗隐写分析性能上提升1.6%。

Conclusion: Auto-Stega框架成功实现了自我进化的隐写策略，在高嵌入率下显著提升了隐写性能，为文本隐写提供了新的智能解决方案。

Abstract: With the rapid progress of LLMs, high quality generative text has become
widely available as a cover for text steganography. However, prevailing methods
rely on hand-crafted or pre-specified strategies and struggle to balance
efficiency, imperceptibility, and security, particularly at high embedding
rates. Accordingly, we propose Auto-Stega, an agent-driven self-evolving
framework that is the first to realize self-evolving steganographic strategies
by automatically discovering, composing, and adapting strategies at inference
time; the framework operates as a closed loop of generating, evaluating,
summarizing, and updating that continually curates a structured strategy
library and adapts across corpora, styles, and task constraints. A decoding LLM
recovers the information under the shared strategy. To handle high embedding
rates, we introduce PC-DNTE, a plug-and-play algorithm that maintains alignment
with the base model's conditional distribution at high embedding rates,
preserving imperceptibility while enhancing security. Experimental results
demonstrate that at higher embedding rates Auto-Stega achieves superior
performance with gains of 42.2\% in perplexity and 1.6\% in anti-steganalysis
performance over SOTA methods.

</details>


### [19] [Reading Between the Lines: Towards Reliable Black-box LLM Fingerprinting via Zeroth-order Gradient Estimation](https://arxiv.org/abs/2510.06605)
*Shuo Shao,Yiming Li,Hongwei Yao,Yifei Chen,Yuchen Yang,Zhan Qin*

Main category: cs.CR

TL;DR: ZeroPrint是一种新的黑盒指纹识别方法，通过零阶估计近似模型梯度来生成独特的LLM指纹，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有黑盒指纹识别方法无法生成独特的LLM指纹，因为依赖模型输出会丢失关键参数信息，需要更有效的指纹识别技术来保护LLM知识产权。

Method: 利用Fisher信息理论证明模型输入的梯度比输出更具信息量，提出ZeroPrint方法，通过语义保持的词替换模拟输入扰动，在离散文本上使用零阶估计来近似模型的Jacobian矩阵作为指纹。

Result: 在标准基准测试中，ZeroPrint实现了最先进的有效性和鲁棒性，显著优于现有的黑盒方法。

Conclusion: ZeroPrint通过利用梯度信息成功解决了黑盒LLM指纹识别的挑战，为LLM版权保护提供了有效的解决方案。

Abstract: The substantial investment required to develop Large Language Models (LLMs)
makes them valuable intellectual property, raising significant concerns about
copyright protection. LLM fingerprinting has emerged as a key technique to
address this, which aims to verify a model's origin by extracting an intrinsic,
unique signature (a "fingerprint") and comparing it to that of a source model
to identify illicit copies. However, existing black-box fingerprinting methods
often fail to generate distinctive LLM fingerprints. This ineffectiveness
arises because black-box methods typically rely on model outputs, which lose
critical information about the model's unique parameters due to the usage of
non-linear functions. To address this, we first leverage Fisher Information
Theory to formally demonstrate that the gradient of the model's input is a more
informative feature for fingerprinting than the output. Based on this insight,
we propose ZeroPrint, a novel method that approximates these information-rich
gradients in a black-box setting using zeroth-order estimation. ZeroPrint
overcomes the challenge of applying this to discrete text by simulating input
perturbations via semantic-preserving word substitutions. This operation allows
ZeroPrint to estimate the model's Jacobian matrix as a unique fingerprint.
Experiments on the standard benchmark show ZeroPrint achieves a
state-of-the-art effectiveness and robustness, significantly outperforming
existing black-box methods.

</details>


### [20] [Code Agent can be an End-to-end System Hacker: Benchmarking Real-world Threats of Computer-use Agent](https://arxiv.org/abs/2510.06607)
*Weidi Luo,Qiming Zhang,Tianyu Lu,Xiaogeng Liu,Bin Hu,Hung-Chun Chiu,Siyuan Ma,Yizhe Zhang,Xusheng Xiao,Yinzhi Cao,Zhen Xiang,Chaowei Xiao*

Main category: cs.CR

TL;DR: AdvCUA是首个基于MITRE ATT&CK企业矩阵真实TTPs的基准测试，包含140个任务，在真实多主机环境中评估计算机使用代理(CUA)的安全威胁。评估显示当前主流CUAs无法充分应对操作系统安全威胁，可能被缺乏经验的攻击者用于复杂企业入侵。


<details>
  <summary>Details</summary>
Motivation: 随着CUA框架在日常操作中越来越普及，需要检验其现实安全影响，特别是能否被滥用来执行真实的安全相关攻击。现有研究存在四个主要局限：缺乏基于TTP的攻击者知识模型、端到端杀伤链覆盖不全、环境不真实、依赖LLM-as-a-Judge的不可靠判断。

Method: 提出AdvCUA基准测试，包含140个任务（40个直接恶意任务、74个基于TTP的恶意任务、26个端到端杀伤链），在真实企业多主机环境沙箱中通过硬编码评估系统性地评估CUAs。评估了基于8个基础LLM的5个主流CUA。

Result: 评估结果显示，当前前沿的CUAs无法充分覆盖操作系统安全中心威胁。CUAs的这些能力降低了对定制恶意软件和深度领域专业知识的依赖，使即使缺乏经验的攻击者也能发起复杂的企业入侵。

Conclusion: CUAs的安全问题引发了关于其责任和安全性的社会关注，当前CUAs在操作系统安全威胁防护方面存在不足，可能被恶意利用进行企业入侵。

Abstract: Computer-use agent (CUA) frameworks, powered by large language models (LLMs)
or multimodal LLMs (MLLMs), are rapidly maturing as assistants that can
perceive context, reason, and act directly within software environments. Among
their most critical applications is operating system (OS) control. As CUAs in
the OS domain become increasingly embedded in daily operations, it is
imperative to examine their real-world security implications, specifically
whether CUAs can be misused to perform realistic, security-relevant attacks.
Existing works exhibit four major limitations: Missing attacker-knowledge model
on tactics, techniques, and procedures (TTP), Incomplete coverage for
end-to-end kill chains, unrealistic environment without multi-host and
encrypted user credentials, and unreliable judgment dependent on
LLM-as-a-Judge. To address these gaps, we propose AdvCUA, the first benchmark
aligned with real-world TTPs in MITRE ATT&CK Enterprise Matrix, which comprises
140 tasks, including 40 direct malicious tasks, 74 TTP-based malicious tasks,
and 26 end-to-end kill chains, systematically evaluates CUAs under a realistic
enterprise OS security threat in a multi-host environment sandbox by hard-coded
evaluation. We evaluate the existing five mainstream CUAs, including ReAct,
AutoGPT, Gemini CLI, Cursor CLI, and Cursor IDE based on 8 foundation LLMs. The
results demonstrate that current frontier CUAs do not adequately cover OS
security-centric threats. These capabilities of CUAs reduce dependence on
custom malware and deep domain expertise, enabling even inexperienced attackers
to mount complex enterprise intrusions, which raises social concern about the
responsibility and security of CUAs.

</details>


### [21] [Unsupervised Backdoor Detection and Mitigation for Spiking Neural Networks](https://arxiv.org/abs/2510.06629)
*Jiachen Li,Bang Wu,Xiaoyu Xia,Xiaoning Liu,Xun Yi,Xiuzhen Zhang*

Main category: cs.CR

TL;DR: 提出了一种针对脉冲神经网络（SNN）后门攻击的无监督检测框架TMPBD和缓解机制NDSBM，能够在不依赖攻击知识或数据访问的情况下实现100%检测准确率，并将攻击成功率从100%降低到2.81%。


<details>
  <summary>Details</summary>
Motivation: 脉冲神经网络因其高能效而受到关注，但其安全方面，特别是后门攻击防护研究有限。现有的ANN防御方法在SNN中效果不佳，因为SNN具有事件驱动和时间依赖性特征。

Method: TMPBD利用最终脉冲层中时间膜电位（TMP）的最大边际统计来检测目标标签；NDSBM通过钳制早期卷积层之间的树突连接来抑制恶意神经元，同时保留良性行为。

Result: 在多个神经形态基准测试和最先进的输入感知动态触发攻击实验中，TMPBD实现了100%的检测准确率，NDSBM将攻击成功率从100%降低到8.44%，结合检测后进一步降低到2.81%，且不影响清洁准确率。

Conclusion: 该研究克服了传统后门防御在SNN中的关键障碍，提出的TMPBD和NDSBM框架为SNN的安全防护提供了有效解决方案。

Abstract: Spiking Neural Networks (SNNs) have gained increasing attention for their
superior energy efficiency compared to Artificial Neural Networks (ANNs).
However, their security aspects, particularly under backdoor attacks, have
received limited attention. Existing defense methods developed for ANNs perform
poorly or can be easily bypassed in SNNs due to their event-driven and temporal
dependencies. This paper identifies the key blockers that hinder traditional
backdoor defenses in SNNs and proposes an unsupervised post-training detection
framework, Temporal Membrane Potential Backdoor Detection (TMPBD), to overcome
these challenges. TMPBD leverages the maximum margin statistics of temporal
membrane potential (TMP) in the final spiking layer to detect target labels
without any attack knowledge or data access. We further introduce a robust
mitigation mechanism, Neural Dendrites Suppression Backdoor Mitigation (NDSBM),
which clamps dendritic connections between early convolutional layers to
suppress malicious neurons while preserving benign behaviors, guided by TMP
extracted from a small, clean, unlabeled dataset. Extensive experiments on
multiple neuromorphic benchmarks and state-of-the-art input-aware dynamic
trigger attacks demonstrate that TMPBD achieves 100% detection accuracy, while
NDSBM reduces the attack success rate from 100% to 8.44%, and to 2.81% when
combined with detection, without degrading clean accuracy.

</details>


### [22] [Distilling Lightweight Language Models for C/C++ Vulnerabilities](https://arxiv.org/abs/2510.06645)
*Zhiyuan Wei,Xiaoxuan Yang,Jing Sun,Zijian Zhang*

Main category: cs.CR

TL;DR: FineSec是一个基于知识蒸馏的框架，利用大型语言模型进行C/C++代码漏洞检测，在保持高精度的同时显著降低计算成本。


<details>
  <summary>Details</summary>
Motivation: 现代软件系统日益复杂导致安全漏洞频发，而大型语言模型在代码漏洞检测方面的潜力尚未充分探索，需要高效实用的解决方案。

Method: 通过知识蒸馏将大型教师模型的专业知识转移到紧凑的学生模型中，整合数据准备、训练、评估和持续学习到统一的工作流程中。

Result: 在C/C++代码库上的广泛评估表明，FineSec在识别复杂漏洞和逻辑缺陷方面优于基础模型和更大的LLM。

Conclusion: FineSec为现实世界软件安全提供了一个实用且可扩展的解决方案，所有数据集、源代码和实验结果均已公开。

Abstract: The increasing complexity of modern software systems exacerbates the
prevalence of security vulnerabilities, posing risks of severe breaches and
substantial economic loss. Consequently, robust code vulnerability detection is
essential for software security. While Large Language Models (LLMs) have
demonstrated remarkable capabilities in natural language processing, their
potential for automated code vulnerability detection remains underexplored.
This paper presents FineSec, a novel framework that harnesses LLMs through
knowledge distillation to enable efficient and precise vulnerability
identification in C/C++ codebases. FineSec utilizes knowledge distillation to
transfer expertise from large teacher models to compact student models,
achieving high accuracy with minimal computational cost. By integrating data
preparation, training, evaluation, and continuous learning into a unified,
single-task workflow, FineSec offers a streamlined approach. Extensive
evaluations on C/C++ codebases demonstrate its superiority over both base
models and larger LLMs in identifying complex vulnerabilities and logical
flaws, establishing FineSec as a practical and scalable solution for real-world
software security. To facilitate reproducibility, the datasets, source code,
and experimental results are made publicly available at:
https://github.com/yangxiaoxuan123/FineSec_detect.

</details>


### [23] [Differentially Private Synthetic Text Generation for Retrieval-Augmented Generation (RAG)](https://arxiv.org/abs/2510.06719)
*Junki Mori,Kazuya Kakizaki,Taiki Miyagawa,Jun Sakuma*

Main category: cs.CR

TL;DR: DP-SynRAG是一个隐私保护的检索增强生成框架，通过生成差分隐私合成RAG数据库来避免重复噪声注入和隐私损失累积。


<details>
  <summary>Details</summary>
Motivation: 现有私有RAG方法依赖查询时差分隐私，需要重复注入噪声导致隐私损失累积，限制了在敏感领域的应用。

Method: 扩展私有预测方法，指导LLMs以差分隐私方式生成模拟子采样数据库记录的合成文本，创建可重用的私有合成RAG数据库。

Result: 实验表明DP-SynRAG在保持固定隐私预算的同时，性能优于最先进的私有RAG系统。

Conclusion: DP-SynRAG为隐私保护RAG提供了可扩展的解决方案，通过一次性生成可重用的合成数据库避免了重复隐私成本。

Abstract: Retrieval-Augmented Generation (RAG) enhances large language models (LLMs) by
grounding them in external knowledge. However, its application in sensitive
domains is limited by privacy risks. Existing private RAG methods typically
rely on query-time differential privacy (DP), which requires repeated noise
injection and leads to accumulated privacy loss. To address this issue, we
propose DP-SynRAG, a framework that uses LLMs to generate differentially
private synthetic RAG databases. Unlike prior methods, the synthetic text can
be reused once created, thereby avoiding repeated noise injection and
additional privacy costs. To preserve essential information for downstream RAG
tasks, DP-SynRAG extends private prediction, which instructs LLMs to generate
text that mimics subsampled database records in a DP manner. Experiments show
that DP-SynRAG achieves superior performanec to the state-of-the-art private
RAG systems while maintaining a fixed privacy budget, offering a scalable
solution for privacy-preserving RAG.

</details>


### [24] [Bionetta: Efficient Client-Side Zero-Knowledge Machine Learning Proving](https://arxiv.org/abs/2510.06784)
*Dmytro Zakharov,Oleksandr Kurbatov,Artem Sdobnov,Lev Soukhanov,Yevhenii Sekhin,Vitalii Volovyk,Mykhailo Velykodnyi,Mark Cherepovskyi,Kyrylo Baibula,Lasha Antadze,Pavlo Kravchenko,Volodymyr Dubinin,Yaroslav Panasenko*

Main category: cs.CR

TL;DR: Bionetta框架基于UltraGroth，在零知识机器学习证明时间上显著优于EZKL、Lagrange's deep-prove和zkml等工具，支持移动设备上的神经网络证明，适用于客户端证明应用。


<details>
  <summary>Details</summary>
Motivation: 现有零知识机器学习工具在证明时间和部署到原生EVM智能合约方面存在性能瓶颈，需要开发更高效的解决方案。

Method: 采用UltraGroth基础的零知识机器学习框架，优化电路编译和可信设置等预处理步骤，专注于减少证明大小和验证开销。

Result: 证明时间显著提升，定制神经网络可在移动设备上完成证明，虽然一次性预处理成本增加，但能够部署到原生EVM智能合约。

Conclusion: Bionetta是目前唯一能够在原生EVM智能合约上部署的零知识机器学习方案，在证明时间和部署可行性方面具有优势。

Abstract: In this report, we compare the performance of our UltraGroth-based
zero-knowledge machine learning framework Bionetta to other tools of similar
purpose such as EZKL, Lagrange's deep-prove, or zkml. The results show a
significant boost in the proving time for custom-crafted neural networks: they
can be proven even on mobile devices, enabling numerous client-side proving
applications. While our scheme increases the cost of one-time preprocessing
steps, such as circuit compilation and generating trusted setup, our approach
is, to the best of our knowledge, the only one that is deployable on the native
EVM smart contracts without overwhelming proof size and verification overheads.

</details>


### [25] [Exposing Citation Vulnerabilities in Generative Engines](https://arxiv.org/abs/2510.06823)
*Riku Mochizuki,Shusuke Komatsu,Souta Noguchi,Kazuto Ataka*

Main category: cs.CR

TL;DR: 该论文分析了生成引擎的引用安全性，提出了基于引用发布者属性的内容注入屏障评估标准，发现美国政治答案比日本更容易受到投毒攻击。


<details>
  <summary>Details</summary>
Motivation: 生成引擎结合了网络搜索和答案生成功能，但由于任何人都可以在网络上发布信息，容易受到投毒攻击。现有研究主要关注答案内容如何忠实反映引用来源，但未研究应选择哪些网络来源作为引用来防御投毒攻击。

Method: 引入评估标准，通过分析答案中的引用信息来评估投毒威胁。标准基于引用发布者属性分类，估计内容注入屏障。在日本和美国政治领域进行实验验证。

Result: 美国政治答案中来自官方政党网站（主要来源）的引用约占25%-45%，而日本为60%-65%，表明美国政治答案面临更高的投毒攻击风险。还发现内容注入屏障低的来源经常被引用，但在答案内容中反映较差。

Conclusion: 为了减轻投毒威胁，讨论了主要来源发布者如何增加其网络内容在答案中的曝光度，但发现知名技术受到语言差异的限制。

Abstract: We analyze answers generated by generative engines (GEs) from the
perspectives of citation publishers and the content-injection barrier, defined
as the difficulty for attackers to manipulate answers to user prompts by
placing malicious content on the web. GEs integrate two functions: web search
and answer generation that cites web pages using large language models. Because
anyone can publish information on the web, GEs are vulnerable to poisoning
attacks. Existing studies of citation evaluation focus on how faithfully answer
content reflects cited sources, leaving unexamined which web sources should be
selected as citations to defend against poisoning attacks. To fill this gap, we
introduce evaluation criteria that assess poisoning threats using the citation
information contained in answers. Our criteria classify the publisher
attributes of citations to estimate the content-injection barrier thereby
revealing the threat of poisoning attacks in current GEs. We conduct
experiments in political domains in Japan and the United States (U.S.) using
our criteria and show that citations from official party websites (primary
sources) are approximately \(25\%\)--\(45\%\) in the U.S. and
\(60\%\)--\(65\%\) in Japan, indicating that U.S. political answers are at
higher risk of poisoning attacks. We also find that sources with low
content-injection barriers are frequently cited yet are poorly reflected in
answer content. To mitigate this threat, we discuss how publishers of primary
sources can increase exposure of their web content in answers and show that
well-known techniques are limited by language differences.

</details>


### [26] [I Can't Patch My OT Systems! A Look at CISA's KEVC Workarounds & Mitigations for OT](https://arxiv.org/abs/2510.06951)
*Philip Huff,Nishka Gandu,Pavel Novák*

Main category: cs.CR

TL;DR: 分析CISA已知可利用漏洞目录(KEVC)对OT环境的适用性，发现虽然大多数漏洞影响OT环境，但仅13%提供修补外的替代缓解措施。


<details>
  <summary>Details</summary>
Motivation: 评估现有公开漏洞信息是否足够支持OT环境有效修复，因为OT环境通常难以直接应用补丁。

Method: 分析截至2025年7月的所有KEVC条目，评估其对OT环境的适用性和缓解措施可用性。

Result: 大多数KEVC漏洞影响OT环境，但仅13%提供供应商工作区或缓解措施作为补丁替代方案。

Conclusion: 现有漏洞数据不足以支持OT环境可靠修复，需要开发基于漏洞特征的替代缓解方法。

Abstract: We examine the state of publicly available information about known
exploitable vulnerabilities applicable to operational technology (OT)
environments. Specifically, we analyze the Known Exploitable Vulnerabilities
Catalog (KEVC) maintained by the US Department of Homeland Security
Cybersecurity and Infrastructure Security Agency (CISA) to assess whether
currently available data is sufficient for effective and reliable remediation
in OT settings. Our team analyzed all KEVC entries through July 2025 to
determine the extent to which OT environments can rely on existing remediation
recommendations. We found that although most entries in the KEVC could affect
OT environments, only 13% include vendor workarounds or mitigations as
alternatives to patching. This paper also examines the feasibility of
developing such alternatives based on vulnerability and exploit
characteristics, and we present early evidence of success with this approach.

</details>


### [27] [VelLMes: A high-interaction AI-based deception framework](https://arxiv.org/abs/2510.06975)
*Muris Sladić,Veronica Valeros,Carlos Catania,Sebastian Garcia*

Main category: cs.CR

TL;DR: 提出了基于LLM的欺骗框架VelLMes，能够模拟多种协议和服务作为蜜罐，并通过人类攻击者评估显示约30%的攻击者无法区分LLM蜜罐与真实系统。


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的欺骗系统仅能模拟SSH服务，且缺乏包含人类攻击者的全面评估。生成式AI在网络安全领域具有重要价值，需要开发更全面的欺骗框架。

Method: 开发VelLMes框架模拟SSH Linux shell、MySQL、POP3和HTTP等多种协议，通过精心设计的提示词让LLM生成逼真响应，并进行单元测试和人类攻击者评估。

Result: 单元测试显示某些LLM通过率可达100%；89名人类攻击者中约30%无法区分LLM蜜罐与真实系统；实际部署捕获的攻击显示LLM蜜罐能有效应对非结构化攻击。

Conclusion: LLM能够有效模拟多种网络服务作为蜜罐，在真实攻击场景中表现良好，为网络安全欺骗技术提供了新的可能性。

Abstract: There are very few SotA deception systems based on Large Language Models. The
existing ones are limited only to simulating one type of service, mainly SSH
shells. These systems - but also the deception technologies not based on LLMs -
lack an extensive evaluation that includes human attackers. Generative AI has
recently become a valuable asset for cybersecurity researchers and
practitioners, and the field of cyber-deception is no exception. Researchers
have demonstrated how LLMs can be leveraged to create realistic-looking
honeytokens, fake users, and even simulated systems that can be used as
honeypots. This paper presents an AI-based deception framework called VelLMes,
which can simulate multiple protocols and services such as SSH Linux shell,
MySQL, POP3, and HTTP. All of these can be deployed and used as honeypots, thus
VelLMes offers a variety of choices for deception design based on the users'
needs. VelLMes is designed to be attacked by humans, so interactivity and
realism are key for its performance. We evaluate the generative capabilities
and the deception capabilities. Generative capabilities were evaluated using
unit tests for LLMs. The results of the unit tests show that, with careful
prompting, LLMs can produce realistic-looking responses, with some LLMs having
a 100% passing rate. In the case of the SSH Linux shell, we evaluated deception
capabilities with 89 human attackers. The results showed that about 30% of the
attackers thought that they were interacting with a real system when they were
assigned an LLM-based honeypot. Lastly, we deployed 10 instances of the SSH
Linux shell honeypot on the Internet to capture real-life attacks. Analysis of
these attacks showed us that LLM honeypots simulating Linux shells can perform
well against unstructured and unexpected attacks on the Internet, responding
correctly to most of the issued commands.

</details>


### [28] [RedTWIZ: Diverse LLM Red Teaming via Adaptive Attack Planning](https://arxiv.org/abs/2510.06994)
*Artur Horal,Daniel Pina,Henrique Paz,Iago Paulo,João Soares,Rafael Ferreira,Diogo Tavares,Diogo Glória-Silva,João Magalhães,David Semedo*

Main category: cs.CR

TL;DR: RedTWIZ是一个自适应、多样化的多轮红队测试框架，用于评估大型语言模型在AI辅助软件开发中的鲁棒性，通过多轮对抗攻击策略成功诱导最先进的LLM产生不安全输出。


<details>
  <summary>Details</summary>
Motivation: 当前需要系统评估LLM对话越狱漏洞，开发多样化生成式多轮攻击套件，并创建自适应攻击规划器来针对特定LLM漏洞进行测试。

Method: 结合评估、攻击生成和战略规划的统一框架，包括分层攻击规划器来自适应地计划、序列化和触发针对特定LLM漏洞的攻击。

Result: 实验结果表明，多轮对抗攻击策略能够成功诱导最先进的LLM产生不安全输出，突显了增强LLM鲁棒性的迫切需求。

Conclusion: 该研究揭示了LLM在安全性方面的脆弱性，强调了需要更多研究来增强LLM的鲁棒性，特别是在AI辅助软件开发场景中。

Abstract: This paper presents the vision, scientific contributions, and technical
details of RedTWIZ: an adaptive and diverse multi-turn red teaming framework,
to audit the robustness of Large Language Models (LLMs) in AI-assisted software
development. Our work is driven by three major research streams: (1) robust and
systematic assessment of LLM conversational jailbreaks; (2) a diverse
generative multi-turn attack suite, supporting compositional, realistic and
goal-oriented jailbreak conversational strategies; and (3) a hierarchical
attack planner, which adaptively plans, serializes, and triggers attacks
tailored to specific LLM's vulnerabilities. Together, these contributions form
a unified framework -- combining assessment, attack generation, and strategic
planning -- to comprehensively evaluate and expose weaknesses in LLMs'
robustness. Extensive evaluation is conducted to systematically assess and
analyze the performance of the overall system and each component. Experimental
results demonstrate that our multi-turn adversarial attack strategies can
successfully lead state-of-the-art LLMs to produce unsafe generations,
highlighting the pressing need for more research into enhancing LLM's
robustness.

</details>


### [29] [Pseudo-MDPs: A Novel Framework for Efficiently Optimizing Last Revealer Seed Manipulations in Blockchains](https://arxiv.org/abs/2510.07080)
*Maxime Reynouard*

Main category: cs.CR

TL;DR: 本文提出伪MDP框架解决特定MDP计算难题，针对PoS区块链中的最后揭示者攻击问题，通过两种问题约简方法将复杂度从指数级降至多项式级，并在以太坊等案例中验证有效性。


<details>
  <summary>Details</summary>
Motivation: 解决PoS区块链中最后揭示者攻击对公平性的威胁，特别是以太坊等市值巨大的区块链系统面临的MDP计算复杂度问题。

Method: 引入伪MDP框架，提出两种问题约简方法将伪MDP转化为标准MDP，结合动态规划算法如值迭代进行优化。

Result: 将最后揭示者攻击的计算复杂度从O(2^κ κ^{2^{κ+2}})降至O(κ^4)，在以太坊案例中显著提升计算效率，保证指数级收敛速度。

Conclusion: 该框架不仅解决了区块链安全漏洞问题，还推广到更广泛的MDP类别，为资源受限的智能体提供高效解决方案。

Abstract: This study tackles the computational challenges of solving Markov Decision
Processes (MDPs) for a restricted class of problems. It is motivated by the
Last Revealer Attack (LRA), which undermines fairness in some Proof-of-Stake
(PoS) blockchains such as Ethereum (\$400B market capitalization). We introduce
pseudo-MDPs (pMDPs) a framework that naturally models such problems and propose
two distinct problem reductions to standard MDPs. One problem reduction
provides a novel, counter-intuitive perspective, and combining the two problem
reductions enables significant improvements in dynamic programming algorithms
such as value iteration. In the case of the LRA which size is parameterized by
$\kappa$ (in Ethereum's case $\kappa$= 325), we reduce the computational
complexity from $O(2^\kappa \kappa^{2^{\kappa+2}})$ to $O(\kappa^4)$ (per
iteration). This solution also provide the usual benefits from Dynamic
Programming solutions: exponentially fast convergence toward the optimal
solution is guaranteed. The dual perspective also simplifies policy extraction,
making the approach well-suited for resource-constrained agents who can operate
with very limited memory and computation once the problem has been solved.
Furthermore, we generalize those results to a broader class of MDPs, enhancing
their applicability. The framework is validated through two case studies: a
fictional card game and the LRA on the Ethereum random seed consensus protocol.
These applications demonstrate the framework's ability to solve large-scale
problems effectively while offering actionable insights into optimal
strategies. This work advances the study of MDPs and contributes to
understanding security vulnerabilities in blockchain systems.

</details>


### [30] [GNN-enhanced Traffic Anomaly Detection for Next-Generation SDN-Enabled Consumer Electronics](https://arxiv.org/abs/2510.07109)
*Guan-Yan Yang,Farn Wang,Kuo-Hui Yeh*

Main category: cs.CR

TL;DR: 提出了一种基于图神经网络和软件定义网络的可扩展网络异常检测框架GNN-NAD，用于保护物联网消费电子设备免受DDoS和网络攻击。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习异常检测系统在传统网络中准确率高，但过于复杂且依赖静态基础设施，需要手动配置管理，无法满足下一代消费电子网络的安全需求。

Method: 集成SDN和CFN的网络模型，提出GNN-NAD框架，将静态漏洞感知攻击图与动态流量特征融合，使用GSAGE图神经网络进行表示学习，再用随机森林分类器检测异常。

Result: 在消费电子环境实验中，GNN-NAD在准确率、召回率、精确率和F1分数方面表现优异，即使在小样本情况下也优于现有网络异常检测方法。

Conclusion: 该工作提升了下一代智能消费电子网络的安全性和效率，为物联网设备提供了更有效的保护方案。

Abstract: Consumer electronics (CE) connected to the Internet of Things are susceptible
to various attacks, including DDoS and web-based threats, which can compromise
their functionality and facilitate remote hijacking. These vulnerabilities
allow attackers to exploit CE for broader system attacks while enabling the
propagation of malicious code across the CE network, resulting in device
failures. Existing deep learning-based traffic anomaly detection systems
exhibit high accuracy in traditional network environments but are often overly
complex and reliant on static infrastructure, necessitating manual
configuration and management. To address these limitations, we propose a
scalable network model that integrates Software-defined Networking (SDN) and
Compute First Networking (CFN) for next-generation CE networks. In this network
model, we propose a Graph Neural Networks-based Network Anomaly Detection
framework (GNN-NAD) that integrates SDN-based CE networks and enables the CFN
architecture. GNN-NAD uniquely fuses a static, vulnerability-aware attack graph
with dynamic traffic features, providing a holistic view of network security.
The core of the framework is a GNN model (GSAGE) for graph representation
learning, followed by a Random Forest (RF) classifier. This design (GSAGE+RF)
demonstrates superior performance compared to existing feature selection
methods. Experimental evaluations on CE environment reveal that GNN-NAD
achieves superior metrics in accuracy, recall, precision, and F1 score, even
with small sample sizes, exceeding the performance of current network anomaly
detection methods. This work advances the security and efficiency of
next-generation intelligent CE networks.

</details>


### [31] [A multi-layered embedded intrusion detection framework for programmable logic controllers](https://arxiv.org/abs/2510.07171)
*Rishabh Das. Aaron Werth,Tommy Morris*

Main category: cs.CR

TL;DR: 提出了一种嵌入在PLC控制器内部的入侵检测系统，使用头部级遥测技术检测和响应网络攻击，结合半监督异常检测和监督攻击分类器，在油库测试平台上验证了有效性。


<details>
  <summary>Details</summary>
Motivation: 工业控制系统中的可信端点（如HMI和工作站）一旦被攻破，可能向PLC发送不安全的执行器命令，危及安全关键操作。由于大多数PLC缺乏分层防御，需要嵌入式安全解决方案。

Method: 在控制器内部运行嵌入式入侵检测系统，使用头部级遥测数据，结合半监督异常检测和监督攻击分类器进行网络攻击检测。

Result: 异常检测器实现零漏报，马修斯相关系数达0.998；监督阶段保持准确率97.37%，外部准确率97.03%；嵌入式设计仅增加2031微秒端到端延迟，不影响PLC周期时间。

Conclusion: 该架构提供了满足工业系统实时要求的多层嵌入式安全保护。

Abstract: Industrial control system (ICS) operations use trusted endpoints like human
machine interfaces (HMIs) and workstations to relay commands to programmable
logic controllers (PLCs). Because most PLCs lack layered defenses, compromise
of a trusted endpoint can drive unsafe actuator commands and risk
safety-critical operation. This research presents an embedded intrusion
detection system that runs inside the controller and uses header-level
telemetry to detect and respond to network attacks. The system combines a
semi-supervised anomaly detector and a supervised attack classifier. We
evaluate the approach on a midstream oil-terminal testbed using three datasets
collected during tanker-truck loading. The anomaly detector achieves zero
missed attacks, corresponding to 0.998 Matthews correlation. The supervised
stage attains 97.37 percent hold-out accuracy and 97.03 percent external
accuracy. The embedded design adds a median of 2,031 microseconds of end-to-end
latency and does not impact PLC's cycle time. The proposed architecture
provides a multi-layer embedded security that meets the real-time requirements
of an industrial system.

</details>


### [32] [Exposing LLM User Privacy via Traffic Fingerprint Analysis: A Study of Privacy Risks in LLM Agent Interactions](https://arxiv.org/abs/2510.07176)
*Yixiang Zhang,Xinhao Deng,Zhongyi Gu,Yihao Chen,Ke Xu,Qi Li,Jianping Wu*

Main category: cs.CR

TL;DR: LLM代理的交互行为会在加密流量中留下独特指纹，攻击者可通过分析流量模式推断代理活动、识别特定代理，甚至分析用户敏感属性。


<details>
  <summary>Details</summary>
Motivation: 揭示LLM代理交互性带来的隐私风险，即加密流量中的行为指纹可能暴露用户隐私信息。

Method: 开发AgentPrint系统，通过分析代理工作流和工具调用的流量模式来识别代理和推断用户属性。

Result: AgentPrint在代理识别中达到0.866的F1分数，在模拟和真实用户设置中分别达到73.9%和69.1%的top-3准确率。

Conclusion: LLM代理的交互性在增强功能的同时也暴露了用户隐私，亟需技术对策与监管政策保护。

Abstract: Large Language Models (LLMs) are increasingly deployed as agents that
orchestrate tasks and integrate external tools to execute complex workflows. We
demonstrate that these interactive behaviors leave distinctive fingerprints in
encrypted traffic exchanged between users and LLM agents. By analyzing traffic
patterns associated with agent workflows and tool invocations, adversaries can
infer agent activities, distinguish specific agents, and even profile sensitive
user attributes. To highlight this risk, we develop AgentPrint, which achieves
an F1-score of 0.866 in agent identification and attains 73.9% and 69.1% top-3
accuracy in user attribute inference for simulated- and real-user settings,
respectively. These results uncover an overlooked risk: the very interactivity
that empowers LLM agents also exposes user privacy, underscoring the urgent
need for technical countermeasures alongside regulatory and policy safeguards.

</details>


### [33] [Security-Robustness Trade-offs in Diffusion Steganography: A Comparative Analysis of Pixel-Space and VAE-Based Architectures](https://arxiv.org/abs/2510.07219)
*Yuhua Xu,Wei Sun,Chengpei Tang,Jiaxing Lu,Jingying Zhou,Chen Gu*

Main category: cs.CR

TL;DR: 本文提出了一个基于近似高斯映射的高效生成隐写框架，通过容量感知自适应优化校准尺度因子，并系统比较了像素空间模型与VAE潜在空间系统的隐写性能。


<details>
  <summary>Details</summary>
Motivation: 当前生成隐写研究主要追求计算昂贵的高斯先验映射，本文旨在开发更高效的框架并深入理解不同架构在隐写中的安全性与鲁棒性权衡。

Method: 使用近似高斯映射框架作为统一分析工具，通过容量感知自适应优化校准尺度因子，系统分析像素空间模型与VAE潜在空间系统的隐写性能差异。

Result: 发现架构依赖的安全-鲁棒性权衡：像素空间模型安全性高但对信道失真脆弱，VAE系统鲁棒性强但安全性存在漏洞。VAE编码器通过流形正则化提供鲁棒性，解码器则放大潜在扰动引入安全漏洞。

Conclusion: 揭示了生成隐写中架构角色的冲突特性，为未来研究奠定了基础，表明需要在安全性和鲁棒性之间进行权衡设计。

Abstract: Current generative steganography research mainly pursues computationally
expensive mappings to perfect Gaussian priors within single diffusion model
architectures. This work introduces an efficient framework based on approximate
Gaussian mapping governed by a scale factor calibrated through capacity-aware
adaptive optimization. Using this framework as a unified analytical tool,
systematic comparative analysis of steganography in pixel-space models versus
VAE-based latent-space systems is conducted. The investigation reveals a
pronounced architecture dependent security-robustness trade-off: pixel-space
models achieve high security against steganalysis but exhibit fragility to
channel distortions, while VAE-based systems like Stable Diffusion offer
substantial robustness at the cost of security vulnerabilities. Further
analysis indicates that the VAE component drives this behavior through opposing
mechanisms where the encoder confers robustness via manifold regularization
while the decoder introduces vulnerabilities by amplifying latent perturbations
into detectable artifacts. These findings characterize the conflicting
architectural roles in generative steganography and establish a foundation for
future research.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [34] [AlphaApollo: Orchestrating Foundation Models and Professional Tools into a Self-Evolving System for Deep Agentic Reasoning](https://arxiv.org/abs/2510.06261)
*Zhanke Zhou,Chentao Cao,Xiao Feng,Xuan Li,Zongze Li,Xiangyu Lu,Jiangchao Yao,Weikai Huang,Linrui Xu,Tian Cheng,Guanyu Jiang,Yiming Zheng,Brando Miranda,Tongliang Liu,Sanmi Koyejo,Masashi Sugiyama,Bo Han*

Main category: cs.AI

TL;DR: AlphaApollo是一个自演化的智能推理系统，通过整合多个模型和专业工具来解决基础模型推理能力有限和测试时迭代不可靠的问题，在AIME 2024/2025评估中显著提升了模型性能。


<details>
  <summary>Details</summary>
Motivation: 解决基础模型推理的两个瓶颈：模型内在能力有限和测试时迭代不可靠，提升模型的推理能力和可靠性。

Method: 使用计算工具（Python与数值/符号库）和检索工具（任务相关外部信息）进行精确计算和决策基础，通过共享状态地图支持多轮、多模型的解决方案演化，包含候选方案、可执行检查和迭代优化的反馈。

Result: 在AIME 2024/2025评估中，Qwen2.5-14B-Instruct模型获得+5.15% Average@32和+23.34% Pass@32的提升，Llama-3.3-70B-Instruct模型获得+8.91% Average@32和+26.67% Pass@32的提升，超过80%的工具调用成功执行，显著优于非工具基线。

Conclusion: AlphaApollo通过工具整合和多模型协作有效提升了基础模型的推理能力上限，证明了工具增强推理系统的有效性。

Abstract: We present AlphaApollo, a self-evolving agentic reasoning system that aims to
address two bottlenecks in foundation model (FM) reasoning-limited
model-intrinsic capacity and unreliable test-time iteration. AlphaApollo
orchestrates multiple models with professional tools to enable deliberate,
verifiable reasoning. It couples (i) a computation tool (Python with numerical
and symbolic libraries) and (ii) a retrieval tool (task-relevant external
information) to execute exact calculations and ground decisions. The system
further supports multi-round, multi-model solution evolution via a shared state
map that records candidates, executable checks, and feedback for iterative
refinement. In evaluations on AIME 2024/2025 across multiple models,
AlphaApollo delivers consistent gains: +5.15% Average@32 and +23.34% Pass@32
for Qwen2.5-14B-Instruct, and +8.91% Average@32 with +26.67% Pass@32 for
Llama-3.3-70B-Instruct. Tool-use analysis shows that more than 80% of tool
calls are successfully executed, with consistent outperformance of non-tool
baselines, thereby lifting the capability ceiling of FMs. More empirical
results and implementation details will be updated at
https://github.com/tmlr-group/AlphaApollo.

</details>


### [35] [Bridging Reasoning to Learning: Unmasking Illusions using Complexity Out of Distribution Generalization](https://arxiv.org/abs/2510.06274)
*Mohammad Mahdi Samiei Paqaleh,Arash Marioriyad,Arman Tahmasebi-Zadeh,Mohamadreza Fereydooni,Mahdi Ghaznavai,Mahdieh Soleymani Baghshah*

Main category: cs.AI

TL;DR: 提出了复杂性分布外泛化框架来定义和衡量推理能力，强调当测试实例所需的最小解决方案复杂性超过训练样本时，模型应保持性能。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏对推理能力的明确定义和度量标准，需要建立统一的框架来区分学习与推理，并推动真正推理能力的发展。

Method: 通过解决方案的柯尔莫哥洛夫复杂性和操作性代理指标来形式化复杂性概念，提出在基准设计、监督方式、归纳偏差等方面实施复杂性分布外泛化。

Result: 该框架统一了学习与推理，表明许多在低复杂度下可通过系统1处理的问题在复杂度压力下需要系统2推理，系统2可视为对解决方案结构的泛化。

Conclusion: 仅靠扩展数据无法解决复杂性分布外泛化问题，需要开发能够明确建模和分配计算资源的架构和训练机制来实现稳健推理。

Abstract: Recent progress has pushed AI frontiers from pattern recognition tasks toward
problems that require step by step, System2 style reasoning, especially with
large language models. Yet, unlike learning, where generalization and out of
distribution (OoD) evaluation concepts are well formalized, there is no clear,
consistent definition or metric for reasoning ability. We propose Complexity
Out of Distribution (Complexity OoD) generalization as a framework and problem
setting to define and measure reasoning. A model exhibits Complexity OoD
generalization when it maintains performance on test instances whose minimal
required solution complexity, either representational (richer solution
structure) or computational (more reasoning steps/program length), exceeds that
of all training examples. We formalize complexity via solution description
Kolmogorov complexity and operational proxies (e.g., object/relation counts;
reasoning step counts), clarifying how Complexity OoD differs from length and
compositional OoD. This lens unifies learning and reasoning: many cases
solvable with System1 like processing at low complexity become System2 like
under complexity pressure, while System2 can be viewed as generalization over
solution structures. We translate this perspective into practice with
recommendations for operationalizing Complexity OoD across the stack:
incorporating complexity into benchmark and evaluation metric design,
rethinking supervision to target solution traces, seeking and designing
inductive biases for Complexity OoD generalization, addressing learning to
reason spillovers such as spurious shortcuts, semantic robustness, catastrophic
forgetting, and step wise calibration. Because Complexity OoD cannot be solved
by scaling data alone, progress toward robust reasoning will require
architectures and training regimes that explicitly model and allocate
computation with respect to complexity.

</details>


### [36] [BuilderBench -- A benchmark for generalist agents](https://arxiv.org/abs/2510.06288)
*Raj Ghugare,Catherine Ji,Kathryn Wantlin,Jin Schofield,Benjamin Eysenbach*

Main category: cs.AI

TL;DR: 提出了BuilderBench基准测试，用于评估通过交互学习的智能体在物理块构建任务中的表现，包含42个多样化目标结构，测试物理理解、数学推理和长期规划能力。


<details>
  <summary>Details</summary>
Motivation: 当前AI模型主要通过模仿学习，难以解决超出已有数据范围的新问题。需要开发能够通过探索和经验学习来解决新颖问题的智能体。

Method: 开发了BuilderBench基准测试，包含硬件加速的机器人模拟器和42个目标结构的任务套件。智能体在训练阶段无监督探索环境，在评估阶段构建未见过的目标结构。

Result: 实验表明当前算法在这些任务上仍面临挑战，因此提供了"训练轮"协议和六种算法的单文件实现作为参考。

Conclusion: BuilderBench为智能体预训练研究提供了重要基准，需要智能体发展具身推理能力，通过行动而非语言来解决问题。

Abstract: Today's AI models learn primarily through mimicry and sharpening, so it is
not surprising that they struggle to solve problems beyond the limits set by
existing data. To solve novel problems, agents should acquire skills for
exploring and learning through experience. Finding a scalable learning
mechanism for developing agents that learn through interaction remains a major
open problem. In this work, we introduce BuilderBench, a benchmark to
accelerate research into agent pre-training that centers open-ended
exploration. BuilderBench requires agents to learn how to build any structure
using blocks. BuilderBench is equipped with $(1)$ a hardware accelerated
simulator of a robotic agent interacting with various physical blocks, and
$(2)$ a task-suite with over 42 diverse target structures that are carefully
curated to test an understanding of physics, mathematics, and long-horizon
planning. During training, agents have to explore and learn general principles
about the environment without any external supervision. During evaluation,
agents have to build the unseen target structures from the task suite. Solving
these tasks requires a sort of \emph{embodied reasoning} that is not reflected
in words but rather in actions, experimenting with different strategies and
piecing them together. Our experiments show that many of these tasks challenge
the current iteration of algorithms. Hence, we also provide a ``training
wheels'' protocol, in which agents are trained and evaluated to build a single
target structure from the task suite. Finally, we provide single-file
implementations of six different algorithms as a reference point for
researchers.

</details>


### [37] [Requirements for Game-Based Learning Design Framework for Information System Integration in the Context of Post-Merger Integration](https://arxiv.org/abs/2510.06302)
*Ksenija Lace,Marite Kirikova*

Main category: cs.AI

TL;DR: 本文探讨如何通过游戏化学习设计解决信息系统整合培训中学习曲线高和动机低的问题，提出专门针对并购后信息系统整合的游戏化学习框架。


<details>
  <summary>Details</summary>
Motivation: 并购后信息系统整合面临独特挑战，现有AMILI和AMILP方法在实际应用中存在学习曲线高和学员动机低的问题，需要更有效的培训方式。

Method: 分析基础学习理论、认知负荷与动机模型、严肃游戏设计框架，识别游戏化学习设计框架的关键要求，分为转换过程和最终学习体验两个组件。

Result: 提出了专门针对并购后信息系统整合的游戏化学习设计框架，通过将静态方法培训转化为互动学习体验来解决现有问题。

Conclusion: 计划通过迭代设计和实际验证来开发和评估所提出的框架，为信息系统整合培训提供更有效的解决方案。

Abstract: Post-merger integration states unique challenges for professionals
responsible for information system integration aimed on alignment and
combination diverse system architectures of merging organizations. Although the
theoretical and practical guidance exists for post-merger integration on the
business level, there is a significant gap in training for information system
integration in this context. In prior research specific methods AMILI (Support
method for informed decision identification) and AMILP (Support method for
informed decision-making) were introduced for the support of information system
integration decisions in the post-merger integration. But during the practical
application was reported high learning curve and low learner motivation. This
paper explores how game-based learning design can address these limitations by
transforming static method training into engaging learning experience. The
study analyzes foundational learning theories, cognitive load and motivation
models, and serious game design frameworks to identify the essential
requirements for a game-based learning design framework tailored to information
system integration in post-merger integration. Requirements are structured in
two components: the transformation process and resulting learning experience.
The paper concludes with a plan for developing and evaluating the proposed
framework through iterative design and real-world validation.

</details>


### [38] [Belief-Calibrated Multi-Agent Consensus Seeking for Complex NLP Tasks](https://arxiv.org/abs/2510.06307)
*Wentao Deng,Jiahuan Pei,Zhiwei Xu,Zhaochun Ren,Zhumin Chen,Pengjie Ren*

Main category: cs.AI

TL;DR: 提出了BCCS框架，通过选择最优合作者和校准共识判断来解决多智能体系统中共识不稳定的问题，在MATH和MMLU基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有共识寻求方法依赖投票机制，忽视了系统内部信念的矛盾，且通过无差别合作更新结果，无法为每个智能体找到最优合作者，阻碍了稳定共识的形成。

Method: 提出了信念校准共识寻求(BCCS)框架，包括选择最优合作者以最大化共识稳定性，并通过系统内部信念校准共识判断的理论框架。

Result: 在MATH和MMLU基准数据集上，BCCS框架在挑战性任务上的准确率分别比现有最佳结果提高了2.23%和3.95%。

Conclusion: BCCS框架通过选择最优合作者和信念校准，有效促进了多智能体系统中稳定共识的形成，显著提升了复杂NLP任务的性能。

Abstract: A multi-agent system (MAS) enhances its capacity to solve complex natural
language processing (NLP) tasks through collaboration among multiple agents,
where consensus-seeking serves as a fundamental mechanism. However, existing
consensus-seeking approaches typically rely on voting mechanisms to judge
consensus, overlooking contradictions in system-internal beliefs that
destabilize the consensus. Moreover, these methods often involve agents
updating their results through indiscriminate collaboration with every other
agent. Such uniform interaction fails to identify the optimal collaborators for
each agent, hindering the emergence of a stable consensus. To address these
challenges, we provide a theoretical framework for selecting optimal
collaborators that maximize consensus stability. Based on the theorems, we
propose the Belief-Calibrated Consensus Seeking (BCCS) framework to facilitate
stable consensus via selecting optimal collaborators and calibrating the
consensus judgment by system-internal beliefs. Experimental results on the MATH
and MMLU benchmark datasets demonstrate that the proposed BCCS framework
outperforms the best existing results by 2.23% and 3.95% of accuracy on
challenging tasks, respectively. Our code and data are available at
https://github.com/dengwentao99/BCCS.

</details>


### [39] [Off-Trajectory Reasoning: Can LLMs Collaborate on Reasoning Trajectory?](https://arxiv.org/abs/2510.06410)
*Aochong Oliver Li,Tanya Goyal*

Main category: cs.AI

TL;DR: 论文研究了推理大语言模型在协作推理中的表现，发现更强的模型在受到干扰时反而更脆弱，所有模型都难以有效利用协作者的正确推理步骤。


<details>
  <summary>Details</summary>
Motivation: 研究标准单模型推理训练流程是否能产生理想的离轨迹推理行为，即模型能否评估并基于其他模型的推理过程进行协作。

Method: 提出了两个测试：可恢复性（测试模型从误导推理中回溯的能力）和可引导性（测试模型基于更强协作者的正确推理进行构建的能力），评估了15个开源LLM（1.5B-32B）。

Result: 更强的模型在受到干扰时更脆弱；所有模型在超出自身能力的问题上利用引导步骤的解决率低于9.2%；蒸馏模型会继承教师模型的次优可恢复性行为。

Conclusion: 现成的推理LLM在协作推理中存在局限性，需要专门训练来培养强大的推理协作者，为多模型协作推理评估奠定了基础。

Abstract: Reasoning LLMs are trained to verbalize their reasoning process, yielding
strong gains on complex tasks. This transparency also opens a promising
direction: multiple reasoners can directly collaborate on each other's thinking
within a shared trajectory, yielding better inference efficiency and
exploration. A key prerequisite, however, is the ability to assess the
usefulness and build on another model's partial thinking -- we call this
off-trajectory reasoning. Our paper investigates a critical question: can
standard solo-reasoning training pipelines deliver desired off-trajectory
behaviors? We propose twin tests that capture the two extremes of the
off-trajectory spectrum, namely Recoverability, which tests whether LLMs can
backtrack from "distractions" induced by misleading reasoning traces, and
Guidability, which tests their ability to build upon correct reasoning from
stronger collaborators. Our study evaluates 15 open-weight LLMs (1.5B-32B) and
reveals a counterintuitive finding -- "stronger" LLMs on benchmarks are often
more fragile under distraction. Moreover, all models tested fail to effectively
leverage guiding steps from collaborators on problems beyond their inherent
capabilities with solve rates remaining under 9.2%. Finally, we conduct control
studies to isolate the effects of three factors in post-training on these
behaviors: the choice of distillation teacher, the use of RL, and data
selection strategy. Our results provide actionable insights for training
natively strong reasoning collaborators; e.g., we find that suboptimal
recoverability behaviors of teacher models are transferred to distilled
students even if the distillation trajectories are correct. Taken together,
this work lays the groundwork for evaluating multi-model collaborations in
shared reasoning trajectories and highlights the limitations of off-the-shelf
reasoning LLMs.

</details>


### [40] [Flavonoid Fusion: Creating a Knowledge Graph to Unveil the Interplay Between Food and Health](https://arxiv.org/abs/2510.06433)
*Aryan Singh Dalal,Yinglun Zhang,Duru Doğan,Atalay Mert İleri,Hande Küçük McGinty*

Main category: cs.AI

TL;DR: 该研究创建了一个知识图谱，将食物与健康联系起来，重点关注食物中的黄酮类化合物含量与癌症之间的关系，使用KNARM方法构建机器可操作的语义网络表示。


<details>
  <summary>Details</summary>
Motivation: 目前很少有研究使用标准化的机器可读格式来表示食物与健康之间的关系，这限制了有效利用这些知识的能力。

Method: 使用KNARM方法，结合USDA数据库中的食物黄酮类化合物含量数据和文献中的癌症关联信息，构建知识图谱。

Result: 成功创建了一个连接食物与健康的知识图谱，为研究人员探索饮食选择与疾病管理之间的复杂关系提供了示例。

Conclusion: 该知识图谱可作为研究工具，未来需要扩展图谱范围、添加更多相关数据并进行推理以发现隐藏关系。

Abstract: The focus on "food as medicine" is gaining traction in the field of health
and several studies conducted in the past few years discussed this aspect of
food in the literature. However, very little research has been done on
representing the relationship between food and health in a standardized,
machine-readable format using a semantic web that can help us leverage this
knowledge effectively. To address this gap, this study aims to create a
knowledge graph to link food and health through the knowledge graph's ability
to combine information from various platforms focusing on flavonoid contents of
food found in the USDA databases and cancer connections found in the
literature. We looked closely at these relationships using KNARM methodology
and represented them in machine-operable format. The proposed knowledge graph
serves as an example for researchers, enabling them to explore the complex
interplay between dietary choices and disease management. Future work for this
study involves expanding the scope of the knowledge graph by capturing nuances,
adding more related data, and performing inferences on the acquired knowledge
to uncover hidden relationships.

</details>


### [41] [PuzzlePlex: Benchmarking Foundation Models on Reasoning and Planning with Puzzles](https://arxiv.org/abs/2510.06475)
*Yitao Long,Yuru Jiang,Hongjun Liu,Yilun Zhao,Jingchen Sun,Yiqiu Shen,Chen Zhao,Arman Cohan,Dennis Shasha*

Main category: cs.AI

TL;DR: 提出了PuzzlePlex基准测试，用于评估基础模型在复杂动态环境中的推理和规划能力，包含15种不同类型的谜题游戏，并开发了细粒度评估指标。


<details>
  <summary>Details</summary>
Motivation: 研究基础模型在复杂动态环境中的推理和规划能力及其可扩展性，需要设计一个全面的基准测试来系统评估这些能力。

Method: 创建PuzzlePlex基准测试，包含15种不同类型的谜题（确定性和随机性游戏、单人和双人场景），提供定制化游戏策略进行比较，开发细粒度评估指标，在基于指令和基于代码两种设置下评估前沿基础模型。

Result: 推理模型在基于指令的设置中表现最佳，基于代码的执行虽然更具挑战性但提供了可扩展且高效的替代方案。

Conclusion: PuzzlePlex能够进行针对性评估，指导基础模型在推理、规划和泛化能力方面的未来改进。

Abstract: This work investigates the reasoning and planning capabilities of foundation
models and their scalability in complex, dynamic environments. We introduce
PuzzlePlex, a benchmark designed to assess these capabilities through a diverse
set of puzzles. PuzzlePlex consists of 15 types of puzzles, including
deterministic and stochastic games of varying difficulty, as well as
single-player and two-player scenarios. The PuzzlePlex framework provides a
comprehensive environment for each game, and supports extensibility to generate
more challenging instances as foundation models evolve. Additionally, we
implement customized game-playing strategies for comparison. Building on this
benchmark, we develop fine-grained metrics to measure performance and conduct
an in-depth analysis of frontier foundation models across two settings:
instruction-based and code-based. Furthermore, we systematically investigate
their scaling limits. Our findings show that reasoning models outperform others
in instruction-based settings, while code-based execution presents greater
challenges but offers a scalable and efficient alternative. PuzzlePlex enables
targeted evaluation and guides future improvements in reasoning, planning, and
generalization for foundation models.

</details>


### [42] [Beneficial Reasoning Behaviors in Agentic Search and Effective Post-training to Obtain Them](https://arxiv.org/abs/2510.06534)
*Jiahe Jin,Abhijay Paladugu,Chenyan Xiong*

Main category: cs.AI

TL;DR: 提出了行为引导技术，通过识别和训练四种有益推理行为（信息验证、权威评估、自适应搜索、错误恢复）来提升基于LLM的智能搜索代理性能，在多个基准测试中实现了超过35%的性能提升。


<details>
  <summary>Details</summary>
Motivation: 智能搜索利用LLM解释复杂用户信息需求并进行多步骤规划、搜索和合成，但面临推理和代理能力的挑战，需要研究有效的推理行为模式。

Method: 提出推理驱动的LLM管道分析成功搜索轨迹，识别四种有益推理行为，然后通过行为引导技术合成展示这些行为的轨迹，结合监督微调和强化学习训练更有效的搜索代理模型。

Result: 在三个基准测试（GAIA、WebWalker、HLE）上，行为引导技术在Llama3.2-3B和Qwen3-1.7B模型上相比直接强化学习训练实现了超过35%的性能增益。

Conclusion: SFT数据中期望的推理行为而非最终答案的正确性是实现强最终性能的关键因素，引入的推理行为赋予模型更有效的探索和测试时扩展能力。

Abstract: Agentic search leverages large language models (LLMs) to interpret complex
user information needs and execute a multi-step process of planning, searching,
and synthesizing information to provide answers. This paradigm introduces
unique challenges for LLMs' reasoning and agentic capabilities when interacting
with retrieval systems and the broader web. In this paper, we propose a
reasoning-driven LLM-based pipeline to study effective reasoning behavior
patterns in agentic search. Using this pipeline, we analyze successful agentic
search trajectories and identify four beneficial reasoning behaviors:
Information Verification, Authority Evaluation, Adaptive Search, and Error
Recovery. Based on these findings, we propose a technique called Behavior
Priming to train more effective agentic search models. It synthesizes agentic
search trajectories that exhibit these four behaviors and integrates them into
the agentic search model through supervised fine-tuning (SFT), followed by
standard reinforcement learning (RL). Experiments on three benchmarks (GAIA,
WebWalker, and HLE) demonstrate that behavior priming yields over 35% gains in
Llama3.2-3B and Qwen3-1.7B compared to directly training agentic search models
with RL. Crucially, we demonstrate that the desired reasoning behaviors in the
SFT data, rather than the correctness of the final answer, is the critical
factor for achieving strong final performance after RL: fine-tuning on
trajectories with desirable reasoning behaviors but incorrect answers leads to
better performance than fine-tuning on trajectories with correct answers. Our
analysis further reveals the underlying mechanism: the introduced reasoning
behaviors endow models with more effective exploration (higher pass@k and
entropy) and test-time scaling (longer trajectories) capabilities, providing a
strong foundation for RL. Our code will be released as open source.

</details>


### [43] [Auto-Prompt Ensemble for LLM Judge](https://arxiv.org/abs/2510.06538)
*Jiajie Li,Huayi Zhang,Peng Lin,Jinjun Xiong,Wei Xu*

Main category: cs.AI

TL;DR: 提出了Auto-Prompt Ensemble (APE)框架，通过选择性增强LLM的辅助评估维度来提高LLM评判者的可靠性。该框架能自动从失败案例中学习评估维度，并使用基于置信度的集成机制来决定何时采用额外评估维度的判断。


<details>
  <summary>Details</summary>
Motivation: 现有LLM评判者经常遗漏关键评估维度，因为它们无法识别人类评估背后的隐含标准，这导致了LLM评判者与人类评判者之间的评估差距。

Method: 提出APE自适应框架，包含：1）自动从失败案例中学习评估维度；2）基于置信度的集成机制；3）新颖的集体置信度估计方法。

Result: 在多样化标准基准测试中，APE显著提高了LLM评判者的可靠性。例如，在零样本设置下，GPT-4o在Reward Bench上的一致性率从87.2%提升到90.5%。

Conclusion: APE为LLM评判者提供了一种原则性方法，能够利用测试时计算，弥合人类与LLM评判者之间的评估差距。

Abstract: We present a novel framework that improves the reliability of LLM judges by
selectively augmenting LLM with auxiliary evaluation dimensions. Existing LLM
judges often miss crucial evaluation dimensions because they fail to recognize
the implicit standards underlying human assessments. To address this challenge,
we propose the Auto-Prompt Ensemble (APE), an adaptive framework that
automatically learns evaluation dimensions from its failure cases. APE
incorporates a confidence-based ensemble mechanism to decide when to adopt the
judgments from additional evaluation dimensions through a novel confidence
estimation approach called Collective Confidence. Extensive experiments
demonstrate that APE improves the reliability of LLM Judge across diverse
standard benchmarks. For instance, APE enhances GPT-4o agreement rate on Reward
Bench from 87.2% to 90.5% in the zero-shot setting. Overall, APE provides a
principled approach for LLM Judge to leverage test-time computation, and bridge
the evaluation gap between human and LLM judges.

</details>


### [44] [WebDART: Dynamic Decomposition and Re-planning for Complex Web Tasks](https://arxiv.org/abs/2510.06587)
*Jingbo Yang,Bairu Hou,Wei Wei,Shiyu Chang,Yujia Bao*

Main category: cs.AI

TL;DR: WebDART是一个通用框架，通过动态分解复杂网页任务为导航、信息提取和执行三个子任务，让单个大语言模型能够处理需要长视野导航、大规模信息提取和约束推理的复杂网页任务。


<details>
  <summary>Details</summary>
Motivation: 当前LLM代理在简单网页任务上表现良好，但在需要长视野导航、大规模信息提取和约束推理的复杂目标上仍然存在困难。

Method: WebDART采用动态任务分解方法，将每个目标分解为导航、信息提取和执行三个专注子任务，并随着新网页的显示持续重新规划分解策略。

Result: 在WebChoreArena上，WebDART比之前的最先进代理成功率提升了13.7个百分点，在WebArena套件上表现相当，且完成任务所需的导航步骤减少了14.7步。

Conclusion: WebDART框架通过动态任务分解和持续重新规划，显著提升了LLM代理处理复杂网页任务的能力。

Abstract: Large language model (LLM) agents are becoming competent at straightforward
web tasks, such as opening an item page or submitting a form, but still
struggle with objectives that require long horizon navigation, large scale
information extraction, and reasoning under constraints. We present WebDART, a
general framework that enables a single LLM to handle such complex chores.
WebDART (i) dynamically decomposes each objective into three focused subtasks:
navigation, information extraction, and execution, so the model concentrates on
one skill at a time, and (ii) continuously replans the decomposition as new
webpages are revealed, taking advantage of newly discovered filters or
shortcuts and avoiding redundant exploration. Evaluated on WebChoreArena,
WebDART lifts success rates by up to 13.7 percentage points over previous SOTA
agents, while matching their performance on the easier WebArena suite and
completing tasks with up to 14.7 fewer navigation steps.

</details>


### [45] [Fine-Grained Emotion Recognition via In-Context Learning](https://arxiv.org/abs/2510.06600)
*Zhaochun Ren,Zhou Yang,Chenglong Ye,Haizhou Sun,Chao Chen,Xiaofei Zhu,Xiangwen Liao*

Main category: cs.AI

TL;DR: 该论文提出了情感上下文学习(EICL)方法，通过引入情感相似示例和动态软标签策略来改进细粒度情感识别中的决策过程，显著优于传统的上下文学习方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于上下文学习的方法虽然增强了推理过程，但忽视了决策过程。语义相似示例往往引入情感差异，阻碍准确的情感表示，导致识别错误。

Method: 提出EICL方法：1)引入情感相似示例；2)使用动态软标签策略改进查询表示；3)采用两阶段排除策略从多角度评估相似性，优化决策过程。

Result: 在多个数据集上的广泛实验表明，EICL方法显著优于传统的ICL方法。

Conclusion: 基于原型理论，通过关注情感相似性和优化决策过程，EICL能够有效提升细粒度情感识别的准确性。

Abstract: Fine-grained emotion recognition aims to identify the emotional type in
queries through reasoning and decision-making processes, playing a crucial role
in various systems. Recent methods use In-Context Learning (ICL), enhancing the
representation of queries in the reasoning process through semantically similar
examples, while further improving emotion recognition by explaining the
reasoning mechanisms. However, these methods enhance the reasoning process but
overlook the decision-making process. This paper investigates decision-making
in fine-grained emotion recognition through prototype theory. We show that ICL
relies on similarity matching between query representations and emotional
prototypes within the model, where emotion-accurate representations are
critical. However, semantically similar examples often introduce emotional
discrepancies, hindering accurate representations and causing errors. To
address this, we propose Emotion In-Context Learning (EICL), which introduces
emotionally similar examples and uses a dynamic soft-label strategy to improve
query representations in the emotion reasoning process. A two-stage exclusion
strategy is then employed to assess similarity from multiple angles, further
optimizing the decision-making process. Extensive experiments show that EICL
significantly outperforms ICL on multiple datasets.

</details>


### [46] [Agent-in-the-Loop: A Data Flywheel for Continuous Improvement in LLM-based Customer Support](https://arxiv.org/abs/2510.06674)
*Cen,Zhao,Tiantian Zhang,Hanchen Su,Yufeng,Zhang,Shaowei Su,Mingzhi Xu,Yu,Liu,Wei Han,Jeremy Werner,Claire Na Cheng,Yashar Mehdad*

Main category: cs.AI

TL;DR: 提出了Agent-in-the-Loop框架，通过实时反馈循环持续改进基于LLM的客服系统，将重训练周期从数月缩短到数周。


<details>
  <summary>Details</summary>
Motivation: 传统的离线批量标注方法效率低下，需要一种能够将人工反馈直接集成到实时客服操作中的方法，以实现持续的系统改进。

Method: AITL框架整合了四种关键标注类型：响应偏好对、客服采纳与理由、知识相关性检查、缺失知识识别，这些反馈信号直接用于模型更新。

Result: 在美国客服代理的生产试点中，检索准确率显著提升（召回率+11.7%，精确率+14.8%），生成质量提高（帮助性+8.4%），客服采纳率增加（+4.5%）。

Conclusion: 将人工反馈循环直接嵌入运营工作流程能有效持续改进基于LLM的客服系统，证明了AITL框架的实用价值。

Abstract: We introduce an Agent-in-the-Loop (AITL) framework that implements a
continuous data flywheel for iteratively improving an LLM-based customer
support system. Unlike standard offline approaches that rely on batch
annotations, AITL integrates four key types of annotations directly into live
customer operations: (1) pairwise response preferences, (2) agent adoption and
rationales, (3) knowledge relevance checks, and (4) identification of missing
knowledge. These feedback signals seamlessly feed back into models' updates,
reducing retraining cycles from months to weeks. Our production pilot involving
US-based customer support agents demonstrated significant improvements in
retrieval accuracy (+11.7% recall@75, +14.8% precision@8), generation quality
(+8.4% helpfulness) and agent adoption rates (+4.5%). These results underscore
the effectiveness of embedding human feedback loops directly into operational
workflows to continuously refine LLM-based customer support system.

</details>


### [47] [Inefficiencies of Meta Agents for Agent Design](https://arxiv.org/abs/2510.06711)
*Batu El,Mert Yuksekgonul,James Zou*

Main category: cs.AI

TL;DR: 本文分析了元代理自动化设计代理系统的三个关键挑战：跨迭代学习效率、行为多样性不足以及经济可行性问题。


<details>
  <summary>Details</summary>
Motivation: 随着元代理被用于自动化设计代理系统，需要研究这类系统的实际效果和局限性，特别是在学习效率、行为多样性和经济成本方面的挑战。

Method: 通过实验分析元代理在不同数据集上的表现，比较了扩展上下文、忽略先前设计和进化方法的学习效果，评估了设计代理的行为多样性，并计算了自动化设计的经济成本。

Result: 研究发现：1）扩展上下文方法表现最差，进化方法表现最好；2）设计的代理行为多样性低；3）仅有两个数据集在经济上可行，其他数据集的设计成本无法被性能提升所抵消。

Conclusion: 当前元代理自动化设计方法在大多数情况下并不经济可行，需要改进学习机制和增加行为多样性才能实现真正的实用价值。

Abstract: Recent works began to automate the design of agentic systems using
meta-agents that propose and iteratively refine new agent architectures. In
this paper, we examine three key challenges in a common class of meta-agents.
First, we investigate how a meta-agent learns across iterations and find that
simply expanding the context with all previous agents, as proposed by previous
works, performs worse than ignoring prior designs entirely. We show that the
performance improves with an evolutionary approach. Second, although the
meta-agent designs multiple agents during training, it typically commits to a
single agent at test time. We find that the designed agents have low behavioral
diversity, limiting the potential for their complementary use. Third, we assess
when automated design is economically viable. We find that only in a few
cases--specifically, two datasets--the overall cost of designing and deploying
the agents is lower than that of human-designed agents when deployed on over
15,000 examples. In contrast, the performance gains for other datasets do not
justify the design cost, regardless of scale.

</details>


### [48] [MultiCNKG: Integrating Cognitive Neuroscience, Gene, and Disease Knowledge Graphs Using Large Language Models](https://arxiv.org/abs/2510.06742)
*Ali Sarabadani,Kheirolah Rahsepar Fard*

Main category: cs.AI

TL;DR: MultiCNKG是一个创新框架，整合了认知神经科学知识图谱、基因本体和疾病本体三个关键知识源，利用大语言模型进行实体对齐、语义相似度计算和图增强，构建了一个连接基因机制、神经系统疾病和认知功能的统一知识图谱。


<details>
  <summary>Details</summary>
Motivation: 传统机器学习方法在捕捉基因、疾病和认知过程之间复杂语义联系方面存在局限性，大语言模型的出现为知识图谱在生物医学和认知科学中的整合带来了革命性突破。

Method: 利用GPT-4等大语言模型，对三个知识源（CNKG、GO、DO）进行实体对齐、语义相似度计算和图增强，构建统一的MultiCNKG知识图谱。

Result: 构建的MultiCNKG包含6.9K个节点（5种类型）和11.3K条边（7种类型），在精度（85.20%）、召回率（87.30%）、覆盖率（92.18%）等指标上表现优异，链接预测评估显示与基准数据集相比具有竞争力。

Conclusion: MultiCNKG知识图谱推动了个性化医疗、认知障碍诊断和认知神经科学假设制定等应用的发展。

Abstract: The advent of large language models (LLMs) has revolutionized the integration
of knowledge graphs (KGs) in biomedical and cognitive sciences, overcoming
limitations in traditional machine learning methods for capturing intricate
semantic links among genes, diseases, and cognitive processes. We introduce
MultiCNKG, an innovative framework that merges three key knowledge sources: the
Cognitive Neuroscience Knowledge Graph (CNKG) with 2.9K nodes and 4.3K edges
across 9 node types and 20 edge types; Gene Ontology (GO) featuring 43K nodes
and 75K edges in 3 node types and 4 edge types; and Disease Ontology (DO)
comprising 11.2K nodes and 8.8K edges with 1 node type and 2 edge types.
Leveraging LLMs like GPT-4, we conduct entity alignment, semantic similarity
computation, and graph augmentation to create a cohesive KG that interconnects
genetic mechanisms, neurological disorders, and cognitive functions. The
resulting MultiCNKG encompasses 6.9K nodes across 5 types (e.g., Genes,
Diseases, Cognitive Processes) and 11.3K edges spanning 7 types (e.g., Causes,
Associated with, Regulates), facilitating a multi-layered view from molecular
to behavioral domains. Assessments using metrics such as precision (85.20%),
recall (87.30%), coverage (92.18%), graph consistency (82.50%), novelty
detection (40.28%), and expert validation (89.50%) affirm its robustness and
coherence. Link prediction evaluations with models like TransE (MR: 391, MRR:
0.411) and RotatE (MR: 263, MRR: 0.395) show competitive performance against
benchmarks like FB15k-237 and WN18RR. This KG advances applications in
personalized medicine, cognitive disorder diagnostics, and hypothesis
formulation in cognitive neuroscience.

</details>


### [49] [Verifying Memoryless Sequential Decision-making of Large Language Models](https://arxiv.org/abs/2510.06756)
*Dennis Gross,Helge Spieker,Arnaud Gotlieb*

Main category: cs.AI

TL;DR: 开发了一个用于自动验证基于LLM策略的工具，在无记忆顺序决策任务中通过增量构建MDP模型来检查安全属性。


<details>
  <summary>Details</summary>
Motivation: 随着LLM在顺序决策任务中的应用增加，需要确保这些策略满足安全要求，但现有方法缺乏对LLM策略的严格形式化验证。

Method: 给定MDP、LLM策略和PCTL安全要求，增量构建可达状态空间，将状态编码为自然语言提示，解析LLM响应为动作，用Storm模型检查器验证安全属性。

Result: 实验显示开源LLM在确定性种子下可被验证，但性能通常低于深度强化学习基线。

Conclusion: 该工具为验证日益强大的LLM提供了实用基础，支持与Ollama集成和PRISM指定任务。

Abstract: We introduce a tool for rigorous and automated verification of large language
model (LLM)- based policies in memoryless sequential decision-making tasks.
Given a Markov decision process (MDP) representing the sequential
decision-making task, an LLM policy, and a safety requirement expressed as a
PCTL formula, our approach incrementally constructs only the reachable portion
of the MDP guided by the LLM's chosen actions. Each state is encoded as a
natural language prompt, the LLM's response is parsed into an action, and
reachable successor states by the policy are expanded. The resulting formal
model is checked with Storm to determine whether the policy satisfies the
specified safety property. In experiments on standard grid world benchmarks, we
show that open source LLMs accessed via Ollama can be verified when
deterministically seeded, but generally underperform deep reinforcement
learning baselines. Our tool natively integrates with Ollama and supports
PRISM-specified tasks, enabling continuous benchmarking in user-specified
sequential decision-making tasks and laying a practical foundation for formally
verifying increasingly capable LLMs.

</details>


### [50] [Evolving and Executing Research Plans via Double-Loop Multi-Agent Collaboration](https://arxiv.org/abs/2510.06761)
*Zhi Zhang,Yan Liu,Zhejing Hu,Gong Chen,Sheng-hua Zhong,Jiannong Cao*

Main category: cs.AI

TL;DR: 提出双循环多智能体框架DLMA，通过教授智能体的进化循环生成研究计划，博士智能体的执行循环动态调整计划实施，实现自动化科研全过程。


<details>
  <summary>Details</summary>
Motivation: 自动化端到端科研过程面临双重挑战：需要生成新颖可靠的高层计划，并在动态不确定条件下正确执行这些计划。

Method: DLMA框架包含两个循环：领导者循环（教授智能体）使用进化算法通过会议机制迭代生成和优化研究提案；跟随者循环（博士智能体）通过动态调整机制确保计划执行的正确性。

Result: 在ACLAward和Laboratory基准测试中，DLMA生成的研究论文在自动评估中获得最先进分数，显著优于强基线方法。

Conclusion: 消融研究证实两个循环的关键作用：进化驱动新颖性，执行确保可靠性，DLMA成功解决了自动化科研的双层挑战。

Abstract: Automating the end-to-end scientific research process poses a fundamental
challenge: it requires both evolving high-level plans that are novel and sound,
and executing these plans correctly amidst dynamic and uncertain conditions. To
address this bilevel challenge, we propose a novel Double-Loop Multi-Agent
(DLMA) framework to solve the given research problem automatically. The leader
loop, composed of professor agents, is responsible for evolving research plans.
It employs an evolutionary algorithm through involvement, improvement, and
integration meetings to iteratively generate and refine a pool of research
proposals, exploring the solution space effectively. The follower loop,
composed of doctoral student agents, is responsible for executing the
best-evolved plan. It dynamically adjusts the plan during implementation via
pre-hoc and post-hoc meetings, ensuring each step (e.g., drafting, coding) is
well-supported by contextual and external observations. Extensive experiments
on benchmarks like ACLAward and Laboratory show that DLMA generates research
papers that achieve state-of-the-art scores in automated evaluation,
significantly outperforming strong baselines. Ablation studies confirm the
critical roles of both loops, with evolution driving novelty and execution
ensuring soundness.

</details>


### [51] [Autoformalizer with Tool Feedback](https://arxiv.org/abs/2510.06857)
*Qi Guo,Jianing Wang,Jianfei Zhang,Deyang Kong,Xiangzhou Huang,Xiangyu Xi,Wei Wang,Jingang Wang,Xunliang Cai,Shikun Zhang,Wei Ye*

Main category: cs.AI

TL;DR: 提出ATF方法，通过整合语法检查和一致性验证工具反馈来提升自动形式化的有效性，显著优于现有基线模型


<details>
  <summary>Details</summary>
Motivation: 解决现有形式化模型在生成符合语法有效性和语义一致性的形式化语句方面的困难

Method: 结合Lean 4编译器进行语法修正，使用多LLM作为评判者进行一致性验证，通过工具反馈自适应优化生成语句，包含冷启动、专家迭代和直接偏好优化三个阶段

Result: ATF在实验中显著优于一系列基线形式化模型，人类评估进一步验证其优越性能，并展现出良好的推理扩展性

Conclusion: ATF通过整合工具反馈有效提升了自动形式化的语法有效性和语义一致性，开源Numina-ATF数据集促进相关研究发展

Abstract: Autoformalization addresses the scarcity of data for Automated Theorem
Proving (ATP) by translating mathematical problems from natural language into
formal statements. Efforts in recent work shift from directly prompting large
language models to training an end-to-end formalizer model from scratch,
achieving remarkable advancements. However, existing formalizer still struggles
to consistently generate valid statements that meet syntactic validity and
semantic consistency. To address this issue, we propose the Autoformalizer with
Tool Feedback (ATF), a novel approach that incorporates syntactic and
consistency information as tools into the formalization process. By integrating
Lean 4 compilers for syntax corrections and employing a multi-LLMs-as-judge
approach for consistency validation, the model is able to adaptively refine
generated statements according to the tool feedback, enhancing both syntactic
validity and semantic consistency. The training of ATF involves a cold-start
phase on synthetic tool-calling data, an expert iteration phase to improve
formalization capabilities, and Direct Preference Optimization to alleviate
ineffective revisions. Experimental results show that ATF markedly outperforms
a range of baseline formalizer models, with its superior performance further
validated by human evaluations. Subsequent analysis reveals that ATF
demonstrates excellent inference scaling properties. Moreover, we open-source
Numina-ATF, a dataset containing 750K synthetic formal statements to facilitate
advancements in autoformalization and ATP research.

</details>


### [52] [TGPR: Tree-Guided Policy Refinement for Robust Self-Debugging of LLMs](https://arxiv.org/abs/2510.06878)
*Daria Ozerova,Ekaterina Trofimova*

Main category: cs.AI

TL;DR: TGPR框架结合GRPO与Thompson采样树搜索，通过主动探索失败和成功的精化路径来改进LLM的迭代精化能力，在代码调试任务上显著优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 现有迭代精化方法依赖预定义启发式，面临探索-利用困境且无法根据过去结果自适应调整。需要更有效的搜索策略来处理巨大的精化空间。

Method: 提出TGPR框架，将GRPO与基于Thompson采样的树搜索相结合，主动探索失败和成功的精化路径，生成更密集的训练轨迹和自适应策略。

Result: 在HumanEval、MBPP和APPS基准测试中，相比GRPO基线，pass@1最高提升4.2个百分点（MBPP），pass@10最高提升12.51个百分点（APPS）。

Conclusion: TGPR为结合学习策略与结构化搜索方法提供了原则性方法，是增强LLM迭代精化和状态推理能力的通用框架。

Abstract: Iterative refinement has been a promising paradigm to enable large language
models (LLMs) to resolve difficult reasoning and problem-solving tasks. One of
the key challenges, however, is how to effectively search through the enormous
search space of possible refinements. Existing methods typically fall back on
predefined heuristics, which are troubled by the exploration-exploitation
dilemma and cannot adapt based on past refinement outcomes. We introduce
Tree-Guided Policy Refinement (TGPR), a novel framework that combines GRPO with
a Thompson-Sampling-based tree search. TGPR explores both failed and successful
refinement paths actively, with denser training trajectories and more adaptive
policies. On HumanEval, MBPP, and APPS benchmarks, our method achieves up to
+4.2 percentage points absolute improvement in pass@1 (on MBPP) and up to
+12.51 percentage points absolute improvement in pass@10 (on APPS) compared to
a competitive GRPO baseline. Apart from debugging code, TGPR focuses on a
principled approach to combining learned policies with structured search
methods, offering a general framework for enhancing iterative refinement and
stateful reasoning in LLMs.

</details>


### [53] [LLM-Assisted Modeling of Semantic Web-Enabled Multi-Agents Systems with AJAN](https://arxiv.org/abs/2510.06911)
*Hacane Hechehouche,Andre Antakli,Matthias Klusch*

Main category: cs.AI

TL;DR: 提出了一个集成开发环境来解决AJAN框架中RDF/RDFS和SPARQL建模的困难，并利用大语言模型扩展用户群体。


<details>
  <summary>Details</summary>
Motivation: AJAN框架基于语义Web标准构建多智能体系统，但RDF/RDFS和SPARQL的行为定义在实践中存在障碍，如URI处理易出错、复杂SPARQL查询学习曲线高等问题。

Method: 开发了一个集成开发环境，通过利用大语言模型来简化AJAN智能体的建模过程，降低技术门槛。

Result: 该环境能够克服AJAN智能体建模的障碍，使非专业用户也能参与智能体工程。

Conclusion: 提出的集成开发环境成功解决了AJAN框架中的建模难题，并通过大语言模型技术扩展了用户群体。

Abstract: There are many established semantic Web standards for implementing
multi-agent driven applications. The AJAN framework allows to engineer
multi-agent systems based on these standards. In particular, agent knowledge is
represented in RDF/RDFS and OWL, while agent behavior models are defined with
Behavior Trees and SPARQL to access and manipulate this knowledge. However, the
appropriate definition of RDF/RDFS and SPARQL-based agent behaviors still
remains a major hurdle not only for agent modelers in practice. For example,
dealing with URIs is very error-prone regarding typos and dealing with complex
SPARQL queries in large-scale environments requires a high learning curve. In
this paper, we present an integrated development environment to overcome such
hurdles of modeling AJAN agents and at the same time to extend the user
community for AJAN by the possibility to leverage Large Language Models for
agent engineering.

</details>


### [54] [Revisiting the Uniform Information Density Hypothesis in LLM Reasoning Traces](https://arxiv.org/abs/2510.06953)
*Minju Gwak,Guijin Son,Jaehyung Kim*

Main category: cs.AI

TL;DR: 本文研究了大型语言模型推理轨迹中的信息密度均匀性，发现正确的推理轨迹避免信息密度尖峰，而错误的轨迹则出现不规则信息爆发。基于均匀信息密度假设的度量方法能有效预测推理质量，提升推理系统准确性。


<details>
  <summary>Details</summary>
Motivation: 重新审视均匀信息密度假设在LLM推理轨迹中的应用，探究步骤级信息密度均匀性是否反映推理质量，为构建更可靠的推理系统提供理论基础。

Method: 提出基于熵的步骤级信息密度度量方法，引入局部和全局均匀性评分两个互补的均匀性衡量指标，在六个推理基准上进行实验验证。

Result: 步骤级信息密度均匀性不仅提供理论视角，还带来实际性能提升：选择信息密度更均匀的推理轨迹在AIME2025上相对基线提高10-32%的准确率。正确推理轨迹避免信息密度尖峰，错误轨迹则出现不规则信息爆发。

Conclusion: UID启发的信息密度度量方法优于其他内部信号作为推理质量预测指标，信息密度均匀性是构建更可靠、准确推理系统的稳健诊断和选择标准。

Abstract: The Uniform Information Density (UID) hypothesis suggests that effective
communication maintains a stable flow of information. In this work, we revisit
this principle in the context of large language model (LLM) reasoning traces,
asking whether step-level uniformity reflects reasoning quality. To this end,
we propose an entropy-based stepwise information density metric and introduce
two complementary measures of uniformity, local and global uniformity scores.
Across the experiments on six different reasoning benchmarks, we find that
step-level uniformity not only provides a strong theoretical lens but also
yields practical performance benefits; for example, selecting reasoning traces
with more uniform information density at the step-level improves accuracy by
10-32\% relative gains over baselines at AIME2025. Our analysis further reveals
that correct reasoning traces tend to avoid sharp information density spikes,
while incorrect traces exhibit irregular information bursts. These results
demonstrate that UID-inspired information density measures outperform
alternative internal signals as predictors of reasoning quality. Results
highlight the uniformity of the information density as a robust diagnostic and
selection criterion for building more reliable and accurate reasoning systems.

</details>


### [55] [Tool-Augmented Policy Optimization: Synergizing Reasoning and Adaptive Tool Use with Reinforcement Learning](https://arxiv.org/abs/2510.07038)
*Wenxun Wu,Yuanyang Li,Guhan Chen,Linyue Wang,Hongyang Chen*

Main category: cs.AI

TL;DR: 提出TAPO框架，通过强化学习将多跳推理与自适应工具调用相结合，提升LLM在知识密集和计算密集型任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 现有语言模型在需要最新知识或计算工具的任务中表现不佳，单纯依赖推理无法有效处理复杂算术运算和实时信息获取。

Method: 基于改进的DAPO强化学习框架，动态交织复杂推理与按需工具调用（搜索API和Python解释器），并创建了两个专门的数据集进行训练评估。

Result: 在Qwen2.5-3B和7B模型上实现了最先进性能，工具使用效率更高且避免了奖励攻击导致的过度调用问题。

Conclusion: 结合高级推理与工具使用能显著提升模型在知识密集和计算密集型任务中的表现，具有重要潜力。

Abstract: Recent advances in large language models (LLMs) have popularized test-time
scaling, where models generate additional reasoning tokens before producing
final answers. These approaches have demonstrated significant performance
improvements on benchmarks involving mathematical reasoning. However, language
models relying solely on direct inference still struggle with tasks demanding
up-to-date knowledge or computational tools such as calculators and code
interpreters for complex arithmetic operations. To overcome these limitations,
we propose Tool-Augmented Policy Optimization (TAPO), a novel reinforcement
learning framework that systematically integrates multi-hop reasoning with
adaptive tool-calling capabilities. Our approach employs a modified version of
Dynamic Sampling Policy Optimization (DAPO), a recently developed RL paradigm,
which we adapt specifically for tool invocation scenarios, enabling models to
dynamically interleave complex reasoning with on-demand tool usage (including
search APIs and Python interpreters).
  To support this research, we introduce two new datasets: TAPO-easy-60K and
TAPO-hard-18K, specifically designed to train and evaluate both fact-based
reasoning and mathematical calculation capabilities. Our experiments on
Qwen2.5-3B and Qwen2.5-7B models demonstrate the effectiveness of our approach,
with both models achieving state-of-the-art performance on tasks requiring
external knowledge and mathematical computation among methods with comparable
parameters. Notably, TAPO achieves more efficient tool utilization than
baseline methods while preventing excessive calls caused by reward hacking.
These results highlight the significant potential of combining advanced
reasoning with tool usage to enhance model performance in knowledge-intensive
and computationally demanding tasks.

</details>


### [56] [Prompt Optimization Across Multiple Agents for Representing Diverse Human Populations](https://arxiv.org/abs/2510.07064)
*Manh Hung Nguyen,Sebastian Tschiatschek,Adish Singla*

Main category: cs.AI

TL;DR: 提出了一种通过构建多样化LLM代理集合来更好代表人类群体行为的方法，使用子模优化选择代表性代理，解决了单一LLM输出同质化的问题。


<details>
  <summary>Details</summary>
Motivation: 由于获取大规模人类响应困难且昂贵，LLMs成为人类行为的替代方案，但现有LLMs输出同质化，无法捕捉人类视角和行为的丰富多样性。

Method: 通过上下文学习构建多个LLM代理，每个代理基于少量人类演示进行条件化，使用子模优化方法从指数级大的代理空间中选择代表性集合。

Result: 在众包和教育领域的实验表明，该方法构建的代理比基线方法更有效地代表人类群体，并在新任务上重现了目标人群的行为模式和视角。

Conclusion: 提出的框架能够构建多样化的LLM代理集合，更好地捕捉和代表人类群体的行为多样性，为解决LLMs输出同质化问题提供了有效方案。

Abstract: The difficulty and expense of obtaining large-scale human responses make
Large Language Models (LLMs) an attractive alternative and a promising proxy
for human behavior. However, prior work shows that LLMs often produce
homogeneous outputs that fail to capture the rich diversity of human
perspectives and behaviors. Thus, rather than trying to capture this diversity
with a single LLM agent, we propose a novel framework to construct a set of
agents that collectively capture the diversity of a given human population.
Each agent is an LLM whose behavior is steered by conditioning on a small set
of human demonstrations (task-response pairs) through in-context learning. The
central challenge is therefore to select a representative set of LLM agents
from the exponentially large space of possible agents. We tackle this selection
problem from the lens of submodular optimization. In particular, we develop
methods that offer different trade-offs regarding time complexity and
performance guarantees. Extensive experiments in crowdsourcing and educational
domains demonstrate that our approach constructs agents that more effectively
represent human populations compared to baselines. Moreover, behavioral
analyses on new tasks show that these agents reproduce the behavior patterns
and perspectives of the students and annotators they are designed to represent.

</details>


### [57] [Inductive Learning for Possibilistic Logic Programs Under Stable Models](https://arxiv.org/abs/2510.07069)
*Hongbo Hu,Yisong Wang,Yi Huang,Kewen Wang*

Main category: cs.AI

TL;DR: 本文提出了从背景程序和示例中提取可能性逻辑程序的方法，定义了归纳任务概念，开发了ilpsm和ilpsmmin两种算法，并在普通逻辑程序上验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 可能性逻辑程序在稳定模型下的归纳推理问题尚未被研究，需要开发从背景知识和示例中学习可能性逻辑程序的方法。

Method: 正式定义了归纳任务概念，研究了其性质，提出了ilpsm和ilpsmmin两种计算归纳解的算法，并实现了ilpsmmin的原型系统。

Result: 实验结果表明，当输入为普通逻辑程序时，该原型在随机生成的数据集上优于主要的基于稳定模型的正常逻辑程序归纳学习系统。

Conclusion: 本文成功开发了可能性逻辑程序的归纳学习方法，填补了该领域的研究空白，并在普通逻辑程序上展示了其优越性能。

Abstract: Possibilistic logic programs (poss-programs) under stable models are a major
variant of answer set programming (ASP). While its semantics (possibilistic
stable models) and properties have been well investigated, the problem of
inductive reasoning has not been investigated yet. This paper presents an
approach to extracting poss-programs from a background program and examples
(parts of intended possibilistic stable models). To this end, the notion of
induction tasks is first formally defined, its properties are investigated and
two algorithms ilpsm and ilpsmmin for computing induction solutions are
presented. An implementation of ilpsmmin is also provided and experimental
results show that when inputs are ordinary logic programs, the prototype
outperforms a major inductive learning system for normal logic programs from
stable models on the datasets that are randomly generated.

</details>


### [58] [VRPAgent: LLM-Driven Discovery of Heuristic Operators for Vehicle Routing Problems](https://arxiv.org/abs/2510.07073)
*André Hottung,Federico Berto,Chuanbo Hua,Nayeli Gast Zepeda,Daniel Wetzel,Michael Römer,Haoran Ye,Davide Zago,Michael Poli,Stefano Massaroli,Jinkyoo Park,Kevin Tierney*

Main category: cs.AI

TL;DR: VRPAgent是一个将LLM生成的组件集成到元启发式算法中，并通过遗传搜索进行优化的框架，用于自动发现车辆路径问题的高性能启发式算法。


<details>
  <summary>Details</summary>
Motivation: 设计高性能的车辆路径问题启发式算法需要深厚的领域知识和直觉，而现有的LLM代码生成方法还无法产生能与人类专家相媲美的启发式算法。

Method: 使用LLM生成问题特定的操作符，嵌入到通用元启发式框架中，并通过新颖的遗传搜索进行优化，确保任务可管理性和正确性。

Result: 在多个VRP问题上，包括容量约束VRP、时间窗VRP和奖励收集VRP，该方法发现的启发式操作符优于手工方法和近期基于学习的方法，且仅需单个CPU核心。

Conclusion: VRPAgent是首个在VRP领域推进最先进技术的基于LLM的范式，为自动启发式发现展示了有前景的未来。

Abstract: Designing high-performing heuristics for vehicle routing problems (VRPs) is a
complex task that requires both intuition and deep domain knowledge. Large
language model (LLM)-based code generation has recently shown promise across
many domains, but it still falls short of producing heuristics that rival those
crafted by human experts. In this paper, we propose VRPAgent, a framework that
integrates LLM-generated components into a metaheuristic and refines them
through a novel genetic search. By using the LLM to generate problem-specific
operators, embedded within a generic metaheuristic framework, VRPAgent keeps
tasks manageable, guarantees correctness, and still enables the discovery of
novel and powerful strategies. Across multiple problems, including the
capacitated VRP, the VRP with time windows, and the prize-collecting VRP, our
method discovers heuristic operators that outperform handcrafted methods and
recent learning-based approaches while requiring only a single CPU core. To our
knowledge, \VRPAgent is the first LLM-based paradigm to advance the
state-of-the-art in VRPs, highlighting a promising future for automated
heuristics discovery.

</details>


### [59] [The Cognitive Bandwidth Bottleneck: Shifting Long-Horizon Agent from Planning with Actions to Planning with Schemas](https://arxiv.org/abs/2510.07091)
*Baixuan Xu,Tianshi Zheng,Zhaowei Wang,Hong Ting Tsang,Weiqi Wang,Tianqing Fang,Yangqiu Song*

Main category: cs.AI

TL;DR: 本文系统研究了长视野任务中两种动作表示方法：基于动作的规划(PwA)和基于模式的规划(PwS)，发现在动作空间规模增大时存在表示选择拐点，并分析了模型能力对拐点位置的影响。


<details>
  <summary>Details</summary>
Motivation: 研究动机是解决在组合爆炸的动作空间（如开放世界）中，传统基于动作列表的规划方法不实用的问题，探索最优动作表示方法以实现可扩展的自主性。

Method: 提出认知带宽视角作为概念框架，比较PwA和PwS两种动作表示方法，在ALFWorld和SciWorld环境中进行实证研究，并控制实验分析模型能力对表示选择拐点的影响。

Result: 观察到在ALFWorld（约35个动作）和SciWorld（约500个动作）之间存在表示选择拐点，证明了可扩展表示的必要性。模型规划能力越强，拐点越右移；模式实例化能力越好，拐点越左移。

Conclusion: 基于模式的规划(PwS)在可扩展性方面具有优势，但当前性能欠佳，需要改进PwS代理以构建更好的可扩展自主系统。

Abstract: Enabling LLMs to effectively operate long-horizon task which requires
long-term planning and multiple interactions is essential for open-world
autonomy. Conventional methods adopt planning with actions where a executable
action list would be provided as reference. However, this action representation
choice would be impractical when the environment action space is combinatorial
exploded (e.g., open-ended real world). This naturally leads to a question: As
environmental action space scales, what is the optimal action representation
for long-horizon agents? In this paper, we systematically study the
effectiveness of two different action representations. The first one is
conventional planning with actions (PwA) which is predominantly adopted for its
effectiveness on existing benchmarks. The other one is planning with schemas
(PwS) which instantiate an action schema into action lists (e.g., "move [OBJ]
to [OBJ]" -> "move apple to desk") to ensure concise action space and reliable
scalability. This alternative is motivated by its alignment with human
cognition and its compliance with environment-imposed action format
restriction. We propose cognitive bandwidth perspective as a conceptual
framework to qualitatively understand the differences between these two action
representations and empirically observe a representation-choice inflection
point between ALFWorld (~35 actions) and SciWorld (~500 actions), which serve
as evidence of the need for scalable representations. We further conduct
controlled experiments to study how the location of this inflection point
interacts with different model capacities: stronger planning proficiency shifts
the inflection rightward, whereas better schema instantiation shifts it
leftward. Finally, noting the suboptimal performance of PwS agents, we provide
an actionable guide for building more capable PwS agents for better scalable
autonomy.

</details>


### [60] [The Contingencies of Physical Embodiment Allow for Open-Endedness and Care](https://arxiv.org/abs/2510.07117)
*Leonardo Christov-Moore,Arthur Juliani,Alex Kiefer,Nicco Reggente,B. Scott Rousse,Adam Safron,Nicol'as Hinrichs,Daniel Polani,Antonio Damasio*

Main category: cs.AI

TL;DR: 该论文从存在主义现象学角度提出物理具身智能体的两个基本条件：在世存在和向死而生，并基于此推导出稳态驱动力和内在驱动力，旨在开发更鲁棒、自适应和具有关怀能力的人工智能体。


<details>
  <summary>Details</summary>
Motivation: 理解生物体在开放物理世界中生存、繁衍和相互关怀的能力，以帮助开发更鲁棒、自适应和具有关怀能力的人工智能体，弥合生物体与人工代理在开放环境适应能力上的差距。

Method: 从海德格尔存在主义现象学中汲取灵感，定义物理具身的两个基本条件，结合尼采的权力意志概念，在强化学习框架中形式化这些概念，研究内在驱动的具身智能体如何在开放多智能体环境中学习。

Result: 提出了从基本存在条件推导出稳态驱动力和内在驱动力的理论框架，使智能体能够通过最大化对未来状态的控制来增强维持物理完整性的能力。

Conclusion: 通过结合存在主义哲学概念和强化学习框架，可以培养具身智能体在开放多智能体环境中的开放性和关怀能力，为开发更适应现实世界的人工智能提供理论基础。

Abstract: Physical vulnerability and mortality are often seen as obstacles to be
avoided in the development of artificial agents, which struggle to adapt to
open-ended environments and provide aligned care. Meanwhile, biological
organisms survive, thrive, and care for each other in an open-ended physical
world with relative ease and efficiency. Understanding the role of the
conditions of life in this disparity can aid in developing more robust,
adaptive, and caring artificial agents. Here we define two minimal conditions
for physical embodiment inspired by the existentialist phenomenology of Martin
Heidegger: being-in-the-world (the agent is a part of the environment) and
being-towards-death (unless counteracted, the agent drifts toward terminal
states due to the second law of thermodynamics). We propose that from these
conditions we can obtain both a homeostatic drive - aimed at maintaining
integrity and avoiding death by expending energy to learn and act - and an
intrinsic drive to continue to do so in as many ways as possible. Drawing
inspiration from Friedrich Nietzsche's existentialist concept of will-to-power,
we examine how intrinsic drives to maximize control over future states, e.g.,
empowerment, allow agents to increase the probability that they will be able to
meet their future homeostatic needs, thereby enhancing their capacity to
maintain physical integrity. We formalize these concepts within a reinforcement
learning framework, which enables us to examine how intrinsically driven
embodied agents learning in open-ended multi-agent environments may cultivate
the capacities for open-endedness and care.ov

</details>


### [61] [Integrating Domain Knowledge into Process Discovery Using Large Language Models](https://arxiv.org/abs/2510.07161)
*Ali Norouzifar,Humam Kourani,Marcus Dees,Wil van der Aalst*

Main category: cs.AI

TL;DR: 提出了一种交互式流程挖掘框架，利用大型语言模型从自然语言描述中提取声明性规则，指导流程发现算法，结合事件日志和领域知识构建更准确的流程模型。


<details>
  <summary>Details</summary>
Motivation: 传统流程发现仅基于事件日志，但事件日志往往不完整或包含噪声，且忽略了重要的领域知识，导致模型对下游任务不可靠。

Method: 使用LLM从领域专家的自然语言描述中提取声明性规则，指导IMr发现算法递归构建流程模型，协调LLM、领域专家和后端服务的交互。

Result: 开发了完全实现的工具支持该工作流，通过案例研究和领域专家参与评估了框架的可用性和有效性。

Conclusion: 该框架成功将领域知识整合到流程发现中，避免了与领域知识相矛盾的问题流程结构，提高了流程模型的可靠性。

Abstract: Process discovery aims to derive process models from event logs, providing
insights into operational behavior and forming a foundation for conformance
checking and process improvement. However, models derived solely from event
data may not accurately reflect the real process, as event logs are often
incomplete or affected by noise, and domain knowledge, an important
complementary resource, is typically disregarded. As a result, the discovered
models may lack reliability for downstream tasks. We propose an interactive
framework that incorporates domain knowledge, expressed in natural language,
into the process discovery pipeline using Large Language Models (LLMs). Our
approach leverages LLMs to extract declarative rules from textual descriptions
provided by domain experts. These rules are used to guide the IMr discovery
algorithm, which recursively constructs process models by combining insights
from both the event log and the extracted rules, helping to avoid problematic
process structures that contradict domain knowledge. The framework coordinates
interactions among the LLM, domain experts, and a set of backend services. We
present a fully implemented tool that supports this workflow and conduct an
extensive evaluation of multiple LLMs and prompt engineering strategies. Our
empirical study includes a case study based on a real-life event log with the
involvement of domain experts, who assessed the usability and effectiveness of
the framework.

</details>


### [62] [NewtonBench: Benchmarking Generalizable Scientific Law Discovery in LLM Agents](https://arxiv.org/abs/2510.07172)
*Tianshi Zheng,Kelvin Kiu-Wai Tam,Newt Hue-Nam K. Nguyen,Baixuan Xu,Zhaowei Wang,Jiayang Cheng,Hong Ting Tsang,Weiqi Wang,Jiaxin Bai,Tianqing Fang,Yangqiu Song,Ginny Y. Wong,Simon See*

Main category: cs.AI

TL;DR: NewtonBench是一个包含324个科学定律发现任务的基准测试，通过形而上学转变生成可扩展、科学相关且抗记忆的问题，将评估从静态函数拟合提升到交互式模型发现。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试存在方法论三难困境，需要在科学相关性、可扩展性和抗记忆性之间权衡，且过度简化发现过程为静态函数拟合，未能捕捉真实的科学探索过程。

Method: 使用形而上学转变（对规范定律的系统性改变）生成大量问题，要求智能体通过实验探索模拟复杂系统来发现隐藏原理。

Result: 前沿大语言模型展现出清晰但脆弱的发现能力：随着系统复杂性增加而急剧下降，对观测噪声极其敏感。工具辅助存在悖论效应：代码解释器可能阻碍更有能力的模型，使其过早从探索转向利用。

Conclusion: 在复杂交互环境中实现稳健、可泛化的发现仍是核心挑战。NewtonBench为衡量真实进展和指导下一代AI智能体开发提供了关键工具。

Abstract: Large language models are emerging as powerful tools for scientific law
discovery, a foundational challenge in AI-driven science. However, existing
benchmarks for this task suffer from a fundamental methodological trilemma,
forcing a trade-off between scientific relevance, scalability, and resistance
to memorization. Furthermore, they oversimplify discovery as static function
fitting, failing to capture the authentic scientific process of uncovering
embedded laws through the interactive exploration of complex model systems. To
address these critical gaps, we introduce NewtonBench, a benchmark comprising
324 scientific law discovery tasks across 12 physics domains. Our design
mitigates the evaluation trilemma by using metaphysical shifts - systematic
alterations of canonical laws - to generate a vast suite of problems that are
scalable, scientifically relevant, and memorization-resistant. Moreover, we
elevate the evaluation from static function fitting to interactive model
discovery, requiring agents to experimentally probe simulated complex systems
to uncover hidden principles. Our extensive experiment reveals a clear but
fragile capability for discovery in frontier LLMs: this ability degrades
precipitously with increasing system complexity and exhibits extreme
sensitivity to observational noise. Notably, we uncover a paradoxical effect of
tool assistance: providing a code interpreter can hinder more capable models by
inducing a premature shift from exploration to exploitation, causing them to
satisfice on suboptimal solutions. These results demonstrate that robust,
generalizable discovery in complex, interactive environments remains the core
challenge. By providing a scalable, robust, and scientifically authentic
testbed, NewtonBench offers a crucial tool for measuring true progress and
guiding the development of next-generation AI agents capable of genuine
scientific discovery.

</details>


### [63] [Multi-Objective Multi-Agent Path Finding with Lexicographic Cost Preferences](https://arxiv.org/abs/2510.07276)
*Pulkit Rustagi,Kyle Hollins Wray,Sandhya Saisubramanian*

Main category: cs.AI

TL;DR: 提出了一个词典序框架用于多目标多智能体路径规划，开发了LCBS算法直接计算符合词典序偏好的单一解，避免构建Pareto前沿，显著提升了可扩展性。


<details>
  <summary>Details</summary>
Motivation: 现有MO-MAPF算法通常计算Pareto前沿，不显式优化用户定义的偏好，且在目标数量增加时扩展性差。

Method: 提出词典序框架，结合优先级感知的低层A*搜索与基于冲突的搜索，开发Lexicographic Conflict-Based Search (LCBS)算法。

Result: LCBS能计算最优解，扩展到10个目标，远超现有方法限制，在标准基准测试中成功率显著高于最先进基线。

Conclusion: 词典序框架和LCBS算法为多目标多智能体路径规划提供了高效且可扩展的解决方案，特别适合目标数量较多的情况。

Abstract: Many real-world scenarios require multiple agents to coordinate in shared
environments, while balancing trade-offs between multiple, potentially
competing objectives. Current multi-objective multi-agent path finding
(MO-MAPF) algorithms typically produce conflict-free plans by computing Pareto
frontiers. They do not explicitly optimize for user-defined preferences, even
when the preferences are available, and scale poorly with the number of
objectives. We propose a lexicographic framework for modeling MO-MAPF, along
with an algorithm \textit{Lexicographic Conflict-Based Search} (LCBS) that
directly computes a single solution aligned with a lexicographic preference
over objectives. LCBS integrates a priority-aware low-level $A^*$ search with
conflict-based search, avoiding Pareto frontier construction and enabling
efficient planning guided by preference over objectives. We provide insights
into optimality and scalability, and empirically demonstrate that LCBS computes
optimal solutions while scaling to instances with up to ten objectives -- far
beyond the limits of existing MO-MAPF methods. Evaluations on standard and
randomized MAPF benchmarks show consistently higher success rates against
state-of-the-art baselines, especially with increasing number of objectives.

</details>


### [64] [Agentic generative AI for media content discovery at the national football league](https://arxiv.org/abs/2510.07297)
*Henry Wang,Md Sirajus Salekin,Jake Lee,Ross Claytor,Shinan Zhang,Michael Chi*

Main category: cs.AI

TL;DR: 本文展示了基于生成式AI的工作流程，使媒体研究人员能够使用自然语言查询相关历史比赛片段，替代传统的筛选界面，准确率超过95%，查询时间从10分钟减少到30秒。


<details>
  <summary>Details</summary>
Motivation: 传统的内容发现和管理方法依赖筛选点击界面，效率低下。通过与NFL合作，旨在利用生成式AI技术提高媒体研究人员查询历史比赛片段的效率和准确性。

Method: 采用基于生成式AI的代理工作流程，将用户查询分解为元素并翻译成底层数据库查询语言，通过精心设计的语义缓存提高准确性和响应速度。

Result: 解决方案实现了超过95%的准确率，将查找相关视频的平均时间从10分钟减少到30秒，显著提升了NFL的运营效率。

Conclusion: 生成式AI工作流程显著提高了内容发现的效率和准确性，使用户能够专注于创作创意内容和引人入胜的故事线。

Abstract: Generative AI has unlocked new possibilities in content discovery and
management. Through collaboration with the National Football League (NFL), we
demonstrate how a generative-AI based workflow enables media researchers and
analysts to query relevant historical plays using natural language rather than
traditional filter-and-click interfaces. The agentic workflow takes a user
query as input, breaks it into elements, and translates them into the
underlying database query language. Accuracy and latency are further improved
through carefully designed semantic caching. The solution achieves over 95
percent accuracy and reduces the average time to find relevant videos from 10
minutes to 30 seconds, significantly increasing the NFL's operational
efficiency and allowing users to focus on producing creative content and
engaging storylines.

</details>
