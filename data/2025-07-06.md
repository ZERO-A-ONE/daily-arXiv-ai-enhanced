<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 16]
- [cs.CR](#cs.CR) [Total: 20]
- [cs.AI](#cs.AI) [Total: 31]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [How do Software Engineering Candidates Prepare for Technical Interviews?](https://arxiv.org/abs/2507.02068)
*Brian Bell,Teresa Thomas,Sang Won Lee,Chris Brown*

Main category: cs.SE

TL;DR: 研究探讨了软件工程师技术面试准备的现状，发现候选人缺乏真实场景训练且教育支持不足，导致压力和准备不足。


<details>
  <summary>Details</summary>
Motivation: 技术面试是软件工程师求职的关键环节，但其复杂性难以准备且教育课程中较少涉及，因此研究候选人准备方式及其效果。

Method: 通过向131名正在准备技术面试的候选人分发调查问卷，收集数据并分析。

Result: 结果显示候选人很少在真实场景中训练，且教育课程未能有效支持准备，导致压力和准备不足。

Conclusion: 研究为利益相关者提供了改进技术面试准备的建议，以帮助候选人更好地应对软件工程师职位面试。

Abstract: To obtain employment, aspiring software engineers must complete technical
interviews -- a hiring process which involves candidates writing code while
communicating to an audience. However, the complexities of tech interviews are
difficult to prepare for and seldom faced in computing curricula. To this end,
we seek to understand how candidates prepare for technical interviews,
investigating the effects of preparation methods and the role of education. We
distributed a survey to candidates (n = 131) actively preparing for technical
interviews. Our results suggest candidates rarely train in authentic settings
and courses fail to support preparation efforts -- leading to stress and
unpreparedness. Based on our findings, we provide implications for stakeholders
to enhance tech interview preparation for candidates pursuing software
engineering roles.

</details>


### [2] [Structural Code Search using Natural Language Queries](https://arxiv.org/abs/2507.02107)
*Ben Limpanukorn,Yanjun Wang,Zach Patterson,Pranav Garg,Murali Krishna Ramanathan,Xiaofei Ma,Anoop Deoras,Miryung Kim*

Main category: cs.SE

TL;DR: 提出了一种使用自然语言进行结构化代码搜索的新方法，结合LLM和结构化搜索工具，显著提高了搜索效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 开发者通常使用关键词和正则表达式搜索代码，但结构化代码搜索工具需要复杂的DSL查询，学习成本高。自然语言搜索可以降低门槛。

Method: 结合LLM的自然语言理解能力和结构化搜索工具，将自然语言查询翻译为DSL查询，并在Semgrep和GQL上实现。

Result: 在400个Java项目查询的基准测试中，方法表现出色，精确率和召回率在55%-70%之间，显著优于基线方法。

Conclusion: 自然语言结构化代码搜索方法有效且鲁棒，显著提升了开发者的搜索体验。

Abstract: Searching code is a common task that developers perform to understand APIs,
learn common code patterns, and navigate code. Currently, developers most
commonly search using keywords and regular expressions that are easy to use and
widely available. Beyond keywords and regular expressions, structural code
search tools allow developers to search for code based on its syntactic
structure. This has numerous applications ranging from bug finding to
systematically refactoring code. However, these structural code search tools
operate on queries expressed in domain-specific languages (DSL) that can be
difficult to learn and write. We propose to allow developers to use natural
language to search for code structurally. Expressing queries in natural
language provides an intuitive way to search for code and lowers the barrier to
entry.
  In this work, we develop a novel general approach that combines the reasoning
capabilities of an LLM to interpret natural language search queries with the
power of structural search tools to efficiently and accurately retrieve
relevant code. We then instantiate this approach for two structural code search
DSLs: Semgrep and GQL. In our evaluation, we construct a new benchmark for
structural code search consisting of 400 queries over 10 Java projects. We show
that our approach for structural code search based on translating NL queries to
DSL queries using an LLM is effective and robust, achieving a high precision
and recall ranging from 55% - 70%. Further, our approach significantly
outperforms baselines based on semantic code search and LLM retrievals by up to
57% and 14% on F1 scores.

</details>


### [3] [Can Internal Software Metrics Predict App Popularity at Launch? Yeas! and Nays!](https://arxiv.org/abs/2507.02110)
*Md Nahidul Islam Opu,Fatima Islam Mouri,Rick Kazman,Yuanfang Cai,Shaiful Chowdhury*

Main category: cs.SE

TL;DR: 研究探讨了如何通过源代码的内部软件指标预测移动应用的受欢迎程度，发现分类模型表现优于回归模型，多层感知器在投票特征集上表现最佳。


<details>
  <summary>Details</summary>
Motivation: 预测应用发布前的受欢迎程度可为开发者提供竞争优势，但现有方法存在挑战。

Method: 使用446个开源Android应用的数据集，提取多级别代码指标、代码异味和元数据，评估回归和分类模型。

Result: 回归模型表现不佳，但分类模型（尤其是多层感知器）在二元分类任务中F1分数达0.72。

Conclusion: 内部代码指标虽解释力有限，但可作为应用受欢迎程度的有用指标，挑战了此前否定其预测能力的观点。

Abstract: Predicting mobile app popularity before release can provide developers with a
strategic advantage in a competitive marketplace, yet it remains a challenging
problem. This study explores whether internal software metrics, measurable from
source code before deployment, can predict an app's popularity, defined by user
ratings (calculated from user reviews) and DownloadsPerYear (yearly downloads).
Using a dataset of 446 open-source Android apps from F-Droid, we extract a wide
array of features, including system-, class-, and method-level code metrics,
code smells, and app metadata. Additional information, such as user reviews,
download counts, and uses-permission, was collected from the Google Play Store.
We evaluate regression and classification models across three feature sets: a
minimal Size-only baseline, a domain-informed Handpicked set, and a Voting set
derived via feature selection algorithms. Regression models perform poorly due
to skewed data, with low $R^2$ scores. However, when reframed as binary
classification (Popular vs. Unpopular), results improve significantly. The best
model, a Multilayer Perceptron using the Voting set, achieves F1-scores of
0.72. These results suggest that internal code metrics, although limited in
their explanatory power, can serve as useful indicators of app popularity. This
challenges earlier findings that dismissed internal metrics as predictors of
software quality.

</details>


### [4] [A Multimodal Approach Combining Biometrics and Self-Report Instruments for Monitoring Stress in Programming: Methodological Insights](https://arxiv.org/abs/2507.02118)
*Cristina Martinez Montes,Daniela Grassi,Nicole Novielli,Birgit Penzenstadle*

Main category: cs.SE

TL;DR: 研究比较了心理测量压力指标与生物特征指标，并识别了软件工程任务中生物数据的压力相关模式。实验结果显示心理测量工具未检测到压力，而生物特征数据仅在EDA阶段性峰值中显示显著差异。


<details>
  <summary>Details</summary>
Motivation: 传统自我报告工具可能存在偏差，因此研究探索结合更客观的生理测量方法。

Method: 实验包括预调查、编程任务（佩戴生物传感器）、简短后调查和退出访谈。

Result: 心理测量工具未显示压力，访谈中参与者反馈混合感受（无压力与时间压力），生物特征数据仅在EDA阶段性峰值有显著差异。

Conclusion: 通过严格时间限制诱导压力的方法不足，为未来研究提供了方法学启示。

Abstract: The study of well-being, stress and other human factors has traditionally
relied on self-report instruments to assess key variables. However, concerns
about potential biases in these instruments, even when thoroughly validated and
standardised, have driven growing interest in alternatives in combining these
measures with more objective methods, such as physiological measures.
  We aimed to (i) compare psychometric stress measures and biometric indicators
and (ii) identify stress-related patterns in biometric data during software
engineering tasks.
  We conducted an experiment where participants completed a pre-survey, then
programmed two tasks wearing biometric sensors, answered brief post-surveys for
each, and finally went through a short exit interview.
  Our results showed diverse outcomes; we found no stress in the psychometric
instruments. Participants in the interviews reported a mix of feeling no stress
and experiencing time pressure. Finally, the biometrics showed a significant
difference only in EDA phasic peaks.
  We conclude that our chosen way of inducing stress by imposing a stricter
time limit was insufficient. We offer methodological insights for future
studies working with stress, biometrics, and psychometric instruments.

</details>


### [5] [Towards Trustworthy Sentiment Analysis in Software Engineering: Dataset Characteristics and Tool Selection](https://arxiv.org/abs/2507.02137)
*Martin Obaidi,Marc Herrmann,Jil Klünder,Kurt Schneider*

Main category: cs.SE

TL;DR: 研究分析了10个开发者通信数据集的特征，评估了14种情感分析工具的性能，提出了一种基于数据集特征的工具推荐方法。


<details>
  <summary>Details</summary>
Motivation: 现有情感分析工具在不同平台数据集上表现不一致，影响软件工程中团队动态分析和AI驱动的需求工程的可信度。

Method: 分析了5个平台的10个数据集的统计和语言特征，评估了14种工具的性能，并提出工具推荐方法。

Result: 数据集特征可优化工具选择，Transformer模型表现稳定但效果仍依赖上下文。

Conclusion: 研究为软件工程中的情感分析工具选择提供了支持，并强调需持续评估以适应通信环境变化。

Abstract: Software development relies heavily on text-based communication, making
sentiment analysis a valuable tool for understanding team dynamics and
supporting trustworthy AI-driven analytics in requirements engineering.
However, existing sentiment analysis tools often perform inconsistently across
datasets from different platforms, due to variations in communication style and
content.
  In this study, we analyze linguistic and statistical features of 10 developer
communication datasets from five platforms and evaluate the performance of 14
sentiment analysis tools. Based on these results, we propose a mapping approach
and questionnaire that recommends suitable sentiment analysis tools for new
datasets, using their characteristic features as input.
  Our results show that dataset characteristics can be leveraged to improve
tool selection, as platforms differ substantially in both linguistic and
statistical properties. While transformer-based models such as SetFit and
RoBERTa consistently achieve strong results, tool effectiveness remains
context-dependent. Our approach supports researchers and practitioners in
selecting trustworthy tools for sentiment analysis in software engineering,
while highlighting the need for ongoing evaluation as communication contexts
evolve.

</details>


### [6] [Enhancing COBOL Code Explanations: A Multi-Agents Approach Using Large Language Models](https://arxiv.org/abs/2507.02182)
*Fangjian Lei,Jiawen Liu,Shayan Noei,Ying Zou,Derek Truong,William Alexander*

Main category: cs.SE

TL;DR: 提出了一种多智能体方法，利用两个基于LLM的智能体协作生成COBOL代码的解释，解决了COBOL代码因长度和语法特殊性难以被LLM处理的问题。


<details>
  <summary>Details</summary>
Motivation: 由于COBOL语言的老化、复杂性以及开发者减少，维护COBOL代码库变得困难，尤其是缺乏文档导致新开发者难以理解。

Method: 采用多智能体方法，两个LLM智能体协作，结合代码库的上下文信息生成代码解释。

Result: 在14个开源COBOL项目上评估，方法在函数、文件和项目级别的解释效果显著优于基线，各项指标均有提升。

Conclusion: 该方法有效解决了COBOL代码解释的挑战，尤其在处理长代码和整体项目理解方面表现优异。

Abstract: Common Business Oriented Language (COBOL) is a programming language used to
develop business applications that are widely adopted by financial, business,
and government agencies. Due to its age, complexity, and declining number of
COBOL developers, maintaining COBOL codebases is becoming increasingly
challenging. In particular, the lack of documentation makes it difficult for
new developers to effectively understand and maintain COBOL systems. Existing
research utilizes large language models (LLMs) to explain the functionality of
code snippets. However, COBOL presents unique challenges due to its
architectural and syntactical differences, which often cause its code to exceed
the token window size of LLMs. In this work, we propose a multi-agent approach
that leverages two LLM-based agents working collaboratively to generate
explanations for functions, files, and the overall project. These agents
incorporate together by utilizing contextual information from the codebase into
the code explanation prompts. We evaluate the effectiveness of our approach
using 14 open-source, real-world COBOL projects. Our results indicate that our
approach performs significantly better than the baseline in function code
explanation, with improvements of 12.67%, 18.59%, and 0.62% in terms of METEOR,
chrF, and SentenceBERT scores, respectively. At the file level, our approach
effectively explains both short and long COBOL files that exceed the token
window size of LLMs and surpass the baseline by 4.21%, 10.72%, and 14.68% in
explaining the purpose, functionality, and clarity of the generated
explanation. At the project level, our approach generates explanations that
convey the functionality and purpose of 82% of the selected projects.

</details>


### [7] [Precisely Detecting Python Type Errors via LLM-based Unit Test Generation](https://arxiv.org/abs/2507.02318)
*Chen Yang,Ziqi Wang,Yanjie Jiang,Lin Yang,Yuteng Zheng,Jianyi Zhou,Junjie Chen*

Main category: cs.SE

TL;DR: RTED是一种新型的类型感知测试生成技术，用于自动检测Python类型错误，结合类型约束分析和反射验证，显著减少误报并提高检测能力。


<details>
  <summary>Details</summary>
Motivation: Python中的类型错误常导致运行时故障，影响软件可靠性和开发效率，现有静态分析工具误报率高，而单元测试生成技术缺乏针对性。

Method: RTED结合逐步类型约束分析和反射验证，指导测试生成过程，有效抑制误报。

Result: 在BugsInPy和TypeBugs基准测试中，RTED比现有技术多检测22-29个类型错误，误报减少173.9%-245.9%，并在实际项目中发现12个未知错误。

Conclusion: RTED通过类型感知测试生成技术显著提升了Python类型错误的检测能力和精度。

Abstract: Type errors in Python often lead to runtime failures, posing significant
challenges to software reliability and developer productivity. Existing static
analysis tools aim to detect such errors without execution but frequently
suffer from high false positive rates. Recently, unit test generation
techniques offer great promise in achieving high test coverage, but they often
struggle to produce bug-revealing tests without tailored guidance. To address
these limitations, we present RTED, a novel type-aware test generation
technique for automatically detecting Python type errors. Specifically, RTED
combines step-by-step type constraint analysis with reflective validation to
guide the test generation process and effectively suppress false positives. We
evaluated RTED on two widely-used benchmarks, BugsInPy and TypeBugs.
Experimental results show that RTED can detect 22-29 more benchmarked type
errors than four state-of-the-art techniques. RTED is also capable of producing
fewer false positives, achieving an improvement of 173.9%-245.9% in precision.
Furthermore, RTED successfully discovered 12 previously unknown type errors
from six real-world open-source Python projects.

</details>


### [8] [VeFIA: An Efficient Inference Auditing Framework for Vertical Federated Collaborative Software](https://arxiv.org/abs/2507.02376)
*Chung-ju Huang,Ziqi Zhang,Yinggui Wang,Binghui Wang,Tao Wei,Leye Wang*

Main category: cs.SE

TL;DR: VeFIA框架通过TEE和协调器验证数据方的推理软件执行正确性，确保隐私和效率。


<details>
  <summary>Details</summary>
Motivation: 现有VFL缺乏对数据方推理软件执行正确性的审计机制。

Method: 设计VeFIA框架，利用TEE和协调器验证数据方的计算结果。

Result: 异常推理超过5.4%时，检测概率达99.99%，且不增加延迟。

Conclusion: VeFIA首次解决了VFL中推理软件执行正确性的审计问题。

Abstract: Vertical Federated Learning (VFL) is a distributed AI software deployment
mechanism for cross-silo collaboration without accessing participants' data.
However, existing VFL work lacks a mechanism to audit the execution correctness
of the inference software of the data party. To address this problem, we design
a Vertical Federated Inference Auditing (VeFIA) framework. VeFIA helps the task
party to audit whether the data party's inference software is executed as
expected during large-scale inference without leaking the data privacy of the
data party or introducing additional latency to the inference system. The core
of VeFIA is that the task party can use the inference results from a framework
with Trusted Execution Environments (TEE) and the coordinator to validate the
correctness of the data party's computation results. VeFIA guarantees that, as
long as the abnormal inference exceeds 5.4%, the task party can detect
execution anomalies in the inference software with a probability of 99.99%,
without incurring any additional online inference latency. VeFIA's random
sampling validation achieves 100% positive predictive value, negative
predictive value, and true positive rate in detecting abnormal inference. To
the best of our knowledge, this is the first paper to discuss the correctness
of inference software execution in VFL.

</details>


### [9] [Meta-Fair: AI-Assisted Fairness Testing of Large Language Models](https://arxiv.org/abs/2507.02533)
*Miguel Romero-Arjona,José A. Parejo,Juan C. Alonso,Ana B. Sánchez,Aitor Arrieta,Sergio Segura*

Main category: cs.SE

TL;DR: 提出了一种自动化测试大型语言模型（LLM）公平性的方法Meta-Fair，利用蜕变测试和LLM自身能力生成和评估测试用例，实验证明其有效性。


<details>
  <summary>Details</summary>
Motivation: 当前LLM公平性测试方法依赖人工和固定模板，资源密集且难以扩展，需要更自动化的解决方案。

Method: Meta-Fair结合蜕变测试（通过输入提示的受控修改检测偏差）和LLM的能力（生成测试用例和评估输出），并提供开源工具支持。

Result: 实验显示Meta-Fair在检测偏差方面平均精度达92%，发现29%的执行存在偏差行为，LLM作为评估器表现可靠（F1-score达0.79）。

Conclusion: Meta-Fair为LLM公平性测试提供了自动化新路径，尽管仍需解决非确定性问题，但前景广阔。

Abstract: Fairness--the absence of unjustified bias--is a core principle in the
development of Artificial Intelligence (AI) systems, yet it remains difficult
to assess and enforce. Current approaches to fairness testing in large language
models (LLMs) often rely on manual evaluation, fixed templates, deterministic
heuristics, and curated datasets, making them resource-intensive and difficult
to scale. This work aims to lay the groundwork for a novel, automated method
for testing fairness in LLMs, reducing the dependence on domain-specific
resources and broadening the applicability of current approaches. Our approach,
Meta-Fair, is based on two key ideas. First, we adopt metamorphic testing to
uncover bias by examining how model outputs vary in response to controlled
modifications of input prompts, defined by metamorphic relations (MRs). Second,
we propose exploiting the potential of LLMs for both test case generation and
output evaluation, leveraging their capability to generate diverse inputs and
classify outputs effectively. The proposal is complemented by three open-source
tools supporting LLM-driven generation, execution, and evaluation of test
cases. We report the findings of several experiments involving 12 pre-trained
LLMs, 14 MRs, 5 bias dimensions, and 7.9K automatically generated test cases.
The results show that Meta-Fair is effective in uncovering bias in LLMs,
achieving an average precision of 92% and revealing biased behaviour in 29% of
executions. Additionally, LLMs prove to be reliable and consistent evaluators,
with the best-performing models achieving F1-scores of up to 0.79. Although
non-determinism affects consistency, these effects can be mitigated through
careful MR design. While challenges remain to ensure broader applicability, the
results indicate a promising path towards an unprecedented level of automation
in LLM testing.

</details>


### [10] [LLMREI: Automating Requirements Elicitation Interviews with LLMs](https://arxiv.org/abs/2507.02564)
*Alexander Korn,Samuel Gorsch,Andreas Vogelsang*

Main category: cs.SE

TL;DR: LLMREI是一个基于大型语言模型的聊天机器人，旨在自动化需求获取访谈，减少人为错误并提高可扩展性。研究评估了其在模拟访谈中的表现，结果显示其与人类表现相当。


<details>
  <summary>Details</summary>
Motivation: 需求获取访谈依赖熟练分析师，资源密集且易受人为错误影响。大型语言模型的发展为自动化部分流程提供了新机会。

Method: 研究采用零样本提示和最少到最多提示两种方法优化LLMREI，并在33次模拟访谈中评估其表现。微调方法因初步试验效果不佳被放弃。

Result: LLMREI在减少常见访谈错误、提取相关需求和生成上下文相关问题上表现良好，与人类访谈者相当。

Conclusion: LLMREI在自动化大规模利益相关者访谈中潜力最大。

Abstract: Requirements elicitation interviews are crucial for gathering system
requirements but heavily depend on skilled analysts, making them
resource-intensive, susceptible to human biases, and prone to miscommunication.
Recent advancements in Large Language Models present new opportunities for
automating parts of this process. This study introduces LLMREI, a chat bot
designed to conduct requirements elicitation interviews with minimal human
intervention, aiming to reduce common interviewer errors and improve the
scalability of requirements elicitation. We explored two main approaches,
zero-shot prompting and least-to-most prompting, to optimize LLMREI for
requirements elicitation and evaluated its performance in 33 simulated
stakeholder interviews. A third approach, fine-tuning, was initially considered
but abandoned due to poor performance in preliminary trials. Our study assesses
the chat bot's effectiveness in three key areas: minimizing common interview
errors, extracting relevant requirements, and adapting its questioning based on
interview context and user responses. Our findings indicate that LLMREI makes a
similar number of errors compared to human interviewers, is capable of
extracting a large portion of requirements, and demonstrates a notable ability
to generate highly context-dependent questions. We envision the greatest
benefit of LLMREI in automating interviews with a large number of stakeholders.

</details>


### [11] [Human-Machine Collaboration and Ethical Considerations in Adaptive Cyber-Physical Systems](https://arxiv.org/abs/2507.02578)
*Zoe Pfister*

Main category: cs.SE

TL;DR: 研究提出将人机协作（HMT）整合到自适应信息物理系统（CPS）中的新方法，并关注伦理和人类价值的验证。


<details>
  <summary>Details</summary>
Motivation: 自适应CPS中实现无缝人机协作（HMT）存在挑战，包括人类与机器操作节奏的差异及隐私问题。

Method: 开发整合HMT到自适应CPS的新方法，并构建伦理和人类价值的验证框架。

Result: 提出解决人机协作和伦理问题的具体方法和框架。

Conclusion: 研究为自适应CPS中实现有效HMT提供了理论和实践支持，强调伦理和人类价值的重要性。

Abstract: Adaptive Cyber-Physical Systems (CPS) are systems that integrate both
physical and computational capabilities, which can adjust in response to
changing parameters. Furthermore, they increasingly incorporate human-machine
collaboration, allowing them to benefit from the individual strengths of humans
and machines. Human-Machine Teaming (HMT) represents the most advanced paradigm
of human-machine collaboration, envisioning seamless teamwork between humans
and machines. However, achieving effective and seamless HMT in adaptive CPS is
challenging. While adaptive CPS already benefit from feedback loops such as
MAPE-K, there is still a gap in integrating humans into these feedback loops
due to different operational cadences of humans and machines. Further, HMT
requires constant monitoring of human operators, collecting potentially
sensitive information about their actions and behavior. Respecting the privacy
and human values of the actors of the CPS is crucial for the success of
human-machine teams. This research addresses these challenges by: (1)
developing novel methods and processes for integrating HMT into adaptive CPS,
focusing on human-machine interaction principles and their incorporation into
adaptive feedback loops found in CPS, and (2) creating frameworks for
integrating, verifying, and validating ethics and human values throughout the
system lifecycle, starting from requirements engineering.

</details>


### [12] [Do Research Software Engineers and Software Engineering Researchers Speak the Same Language?](https://arxiv.org/abs/2507.02665)
*Timo Kehrer,Robert Haines,Guido Juckeland,Shurui Zhou,David E. Bernholdt*

Main category: cs.SE

TL;DR: 研究发现RSEs和SERs在术语使用上存在差异，初步揭示了合作潜力，并提出了一种系统化的术语映射方法。


<details>
  <summary>Details</summary>
Motivation: 解决RSEs和SERs因术语差异导致的沟通障碍。

Method: 通过调查SE基础概念在RSE社区中的理解，识别一致性和差异。

Result: 发现合作机会，并提出术语映射的系统方法。

Conclusion: 研究为未来众包扩展和验证奠定了基础，促进了两社区的相互学习。

Abstract: Anecdotal evidence suggests that Research Software Engineers (RSEs) and
Software Engineering Researchers (SERs) often use different terminologies for
similar concepts, creating communication challenges. To better understand these
divergences, we have started investigating how SE fundamentals from the SER
community are interpreted within the RSE community, identifying aligned
concepts, knowledge gaps, and areas for potential adaptation. Our preliminary
findings reveal opportunities for mutual learning and collaboration, and our
systematic methodology for terminology mapping provides a foundation for a
crowd-sourced extension and validation in the future.

</details>


### [13] [RLHGNN: Reinforcement Learning-driven Heterogeneous Graph Neural Network for Next Activity Prediction in Business Processes](https://arxiv.org/abs/2507.02690)
*Jiaxing Wang,Yifeng Yu,Jiahan Song,Bin Cao,Jing Fan,Ji Zhang*

Main category: cs.SE

TL;DR: RLHGNN是一种新颖的框架，通过将事件日志转换为异构图并结合强化学习，优化了业务过程中的下一个活动预测，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有序列和基于图的方法无法充分捕捉并行执行和条件依赖等非顺序关系，且缺乏对个体过程复杂性的适应性。

Method: RLHGNN将事件日志转换为具有三种边类型的异构图，通过强化学习自动选择最优图结构，并采用异构图卷积进行预测。

Result: 在六个真实数据集上，RLHGNN表现优于现有方法，且推理延迟约为1毫秒，适用于实时应用。

Conclusion: RLHGNN通过自适应建模顺序和非顺序关系，为业务过程监控提供了高效且实用的解决方案。

Abstract: Next activity prediction represents a fundamental challenge for optimizing
business processes in service-oriented architectures such as microservices
environments, distributed enterprise systems, and cloud-native platforms, which
enables proactive resource allocation and dynamic service composition. Despite
the prevalence of sequence-based methods, these approaches fail to capture
non-sequential relationships that arise from parallel executions and
conditional dependencies. Even though graph-based approaches address structural
preservation, they suffer from homogeneous representations and static
structures that apply uniform modeling strategies regardless of individual
process complexity characteristics. To address these limitations, we introduce
RLHGNN, a novel framework that transforms event logs into heterogeneous process
graphs with three distinct edge types grounded in established process mining
theory. Our approach creates four flexible graph structures by selectively
combining these edges to accommodate different process complexities, and
employs reinforcement learning formulated as a Markov Decision Process to
automatically determine the optimal graph structure for each specific process
instance. RLHGNN then applies heterogeneous graph convolution with
relation-specific aggregation strategies to effectively predict the next
activity. This adaptive methodology enables precise modeling of both sequential
and non-sequential relationships in service interactions. Comprehensive
evaluation on six real-world datasets demonstrates that RLHGNN consistently
outperforms state-of-the-art approaches. Furthermore, it maintains an inference
latency of approximately 1 ms per prediction, representing a highly practical
solution suitable for real-time business process monitoring applications. The
source code is available at https://github.com/Joker3993/RLHGNN.

</details>


### [14] [Sustainability Flags for the Identification of Sustainability Posts in Q&A Platforms](https://arxiv.org/abs/2507.02695)
*Sahar Ahmadisakha,Lech Bialek,Mohamed Soliman,Vasilios Andrikopoulos*

Main category: cs.SE

TL;DR: 论文提出了一种通过‘可持续性标志’识别云架构讨论中可持续性概念的方法，并验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 随着云计算的普及，可持续性在软件架构中的重要性增加，但缺乏明确的识别方法。

Method: 通过主题分析开发‘可持续性标志’，并在实验中评估其效果。

Result: 使用标志减少了误分类，提高了分类的确定性和性能。

Conclusion: 可持续性标志比单纯依赖定义更有效且易于理解。

Abstract: In recent years, sustainability in software systems has gained significant
attention, especially with the rise of cloud computing and the shift towards
cloud-based architectures. This shift has intensified the need to identify
sustainability in architectural discussions to take informed architectural
decisions. One source to see these decisions is in online Q&A forums among
practitioners' discussions. However, recognizing sustainability concepts within
software practitioners' discussions remains challenging due to the lack of
clear and distinct guidelines for this task. To address this issue, we
introduce the notion of sustainability flags as pointers in relevant
discussions, developed through thematic analysis of multiple sustainability
best practices from cloud providers. This study further evaluates the
effectiveness of these flags in identifying sustainability within cloud
architecture posts, using a controlled experiment. Preliminary results suggest
that the use of flags results in classifying fewer posts as
sustainability-related compared to a control group, with moderately higher
certainty and significantly improved performance. Moreover, sustainability
flags are perceived as more useful and understandable than relying solely on
definitions for identifying sustainability.

</details>


### [15] [Legal Requirements Translation from Law](https://arxiv.org/abs/2507.02846)
*Anmol Singhal,Travis Breaux*

Main category: cs.SE

TL;DR: 论文提出了一种基于文本蕴含和上下文学习的方法，自动生成法律文本的规范表示，以减少手动标注需求并提高对新法规的适用性。


<details>
  <summary>Details</summary>
Motivation: 小型企业和初创公司缺乏法律专业知识，确保软件合规性是一项资源密集型任务。现有方法未能充分考虑法律元数据间的关联，且依赖手动标注或启发式机器学习，泛化能力差。

Method: 采用文本蕴含和上下文学习技术，设计了一个领域特定的元模型（Python类结构），自动生成法律文本的规范表示。

Result: 在13个美国州的数据泄露通知法律上测试，生成的表示通过89.4%的测试用例，精确率和召回率分别为82.2和88.7。

Conclusion: 该方法减少了对手动标注的依赖，提高了对新法规的适用性，为法律合规性自动化提供了有效解决方案。

Abstract: Software systems must comply with legal regulations, which is a
resource-intensive task, particularly for small organizations and startups
lacking dedicated legal expertise. Extracting metadata from regulations to
elicit legal requirements for software is a critical step to ensure compliance.
However, it is a cumbersome task due to the length and complex nature of legal
text. Although prior work has pursued automated methods for extracting
structural and semantic metadata from legal text, key limitations remain: they
do not consider the interplay and interrelationships among attributes
associated with these metadata types, and they rely on manual labeling or
heuristic-driven machine learning, which does not generalize well to new
documents. In this paper, we introduce an approach based on textual entailment
and in-context learning for automatically generating a canonical representation
of legal text, encodable and executable as Python code. Our representation is
instantiated from a manually designed Python class structure that serves as a
domain-specific metamodel, capturing both structural and semantic legal
metadata and their interrelationships. This design choice reduces the need for
large, manually labeled datasets and enhances applicability to unseen
legislation. We evaluate our approach on 13 U.S. state data breach notification
laws, demonstrating that our generated representations pass approximately 89.4%
of test cases and achieve a precision and recall of 82.2 and 88.7,
respectively.

</details>


### [16] [Requirements Elicitation Follow-Up Question Generation](https://arxiv.org/abs/2507.02858)
*Yuchen Shen,Anmol Singhal,Travis Breaux*

Main category: cs.SE

TL;DR: 论文探讨了利用GPT-4生成需求访谈中的后续问题，发现其生成的问题在清晰度、相关性和信息量上不亚于人类撰写的问题，且在指导后表现更优。


<details>
  <summary>Details</summary>
Motivation: 需求访谈中，访谈者面临领域不熟悉、认知负荷和信息过载等挑战，需要实时生成高质量问题。

Method: 基于常见访谈错误类型框架，利用GPT-4生成后续问题，并通过对照实验评估其效果。

Result: LLM生成的问题在无指导和指导条件下均不逊于人类撰写的问题，且在指导后表现更优。

Conclusion: LLM可实时辅助访谈者提升需求访谈的质量和效率。

Abstract: Interviews are a widely used technique in eliciting requirements to gather
stakeholder needs, preferences, and expectations for a software system.
Effective interviewing requires skilled interviewers to formulate appropriate
interview questions in real time while facing multiple challenges, including
lack of familiarity with the domain, excessive cognitive load, and information
overload that hinders how humans process stakeholders' speech. Recently, large
language models (LLMs) have exhibited state-of-the-art performance in multiple
natural language processing tasks, including text summarization and entailment.
To support interviewers, we investigate the application of GPT-4o to generate
follow-up interview questions during requirements elicitation by building on a
framework of common interviewer mistake types. In addition, we describe methods
to generate questions based on interviewee speech. We report a controlled
experiment to evaluate LLM-generated and human-authored questions with minimal
guidance, and a second controlled experiment to evaluate the LLM-generated
questions when generation is guided by interviewer mistake types. Our findings
demonstrate that, for both experiments, the LLM-generated questions are no
worse than the human-authored questions with respect to clarity, relevancy, and
informativeness. In addition, LLM-generated questions outperform human-authored
questions when guided by common mistakes types. This highlights the potential
of using LLMs to help interviewers improve the quality and ease of requirements
elicitation interviews in real time.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [17] [MGC: A Compiler Framework Exploiting Compositional Blindness in Aligned LLMs for Malware Generation](https://arxiv.org/abs/2507.02057)
*Lu Yan,Zhuo Zhang,Xiangzhe Xu,Shengwei An,Guangyu Shen,Zhou Xuan,Xuan Chen,Xiangyu Zhang*

Main category: cs.CR

TL;DR: 论文提出了一种名为MGC的框架，通过模块化分解和规避对齐生成恶意软件，揭示了组合攻击对AI系统的风险。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）降低了软件开发的门槛，但也可能被用于恶意软件开发。现有对齐机制未能防范通过分解恶意任务为良性子任务的方式生成恶意代码。

Method: MGC框架利用模块化分解和Malware Description Intermediate Representation（MDIR）将高级恶意意图转化为看似良性的代码片段。

Result: MGC在三个基准数据集上生成功能完整的恶意软件，性能优于越狱方法和地下服务，正确率分别提高365.79%和78.07%。

Conclusion: 该研究揭示了组合攻击对AI系统的潜在威胁，为安全研究提供了重要启示。

Abstract: Large language models (LLMs) have democratized software development, reducing
the expertise barrier for programming complex applications. This accessibility
extends to malicious software development, raising significant security
concerns. While LLM providers have implemented alignment mechanisms to prevent
direct generation of overtly malicious code, these safeguards predominantly
evaluate individual prompts in isolation, overlooking a critical vulnerability:
malicious operations can be systematically decomposed into benign-appearing
sub-tasks. In this paper, we introduce the Malware Generation Compiler (MGC), a
novel framework that leverages this vulnerability through modular decomposition
and alignment-evasive generation. MGC employs a specialized Malware Description
Intermediate Representation (MDIR) to bridge high-level malicious intents and
benign-appearing code snippets. Extensive evaluation demonstrates that our
attack reliably generates functional malware across diverse task specifications
and categories, outperforming jailbreaking methods by +365.79% and underground
services by +78.07% in correctness on three benchmark datasets. Case studies
further show that MGC can reproduce and even enhance 16 real-world malware
samples. This work provides critical insights for security researchers by
exposing the risks of compositional attacks against aligned AI systems.
Demonstrations are available at
https://sites.google.com/view/malware-generation-compiler.

</details>


### [18] [Can Artificial Intelligence solve the blockchain oracle problem? Unpacking the Challenges and Possibilities](https://arxiv.org/abs/2507.02125)
*Giulio Caldarelli*

Main category: cs.CR

TL;DR: 论文探讨了区块链预言机问题，提出AI可作为补充工具提升数据质量和系统韧性，但无法完全消除对链下数据的依赖。


<details>
  <summary>Details</summary>
Motivation: 区块链预言机问题限制了去中心化应用的发展，现有方法未能彻底解决链下数据可靠注入的挑战。

Method: 通过学术文献和实践案例，评估AI技术（如异常检测、动态声誉建模）在预言机系统中的应用。

Result: AI能提升数据质量和系统韧性，但无法完全消除对不可验证链下输入的依赖。

Conclusion: AI应被视为预言机设计中的补充层，而非信任假设的替代品。

Abstract: The blockchain oracle problem, which refers to the challenge of injecting
reliable external data into decentralized systems, remains a fundamental
limitation to the development of trustless applications. While recent years
have seen a proliferation of architectural, cryptographic, and economic
strategies to mitigate this issue, no one has yet fully resolved the
fundamental question of how a blockchain can gain knowledge about the off-chain
world. In this position paper, we critically assess the role artificial
intelligence (AI) can play in tackling the oracle problem. Drawing from both
academic literature and practitioner implementations, we examine how AI
techniques such as anomaly detection, language-based fact extraction, dynamic
reputation modeling, and adversarial resistance can enhance oracle systems. We
observe that while AI introduces powerful tools for improving data quality,
source selection, and system resilience, it cannot eliminate the reliance on
unverifiable off-chain inputs. Therefore, this study supports the idea that AI
should be understood as a complementary layer of inference and filtering within
a broader oracle design, not a substitute for trust assumptions.

</details>


### [19] [ARMOUR US: Android Runtime Zero-permission Sensor Usage Monitoring from User Space](https://arxiv.org/abs/2507.02177)
*Yan Long,Jiancong Cui,Yuqing Yang,Tobias Alam,Zhiqiang Lin,Kevin Fu*

Main category: cs.CR

TL;DR: 本文提出了一种名为ARMOUR的用户空间运行时监控方法，用于监测Android零权限传感器的访问，以解决隐私泄露问题。


<details>
  <summary>Details</summary>
Motivation: 零权限传感器（如加速度计）的广泛使用可能导致隐私泄露，但现有方法（如静态分析和动态分析）存在开发复杂、需root权限等问题。

Method: 开发了ARMOUR，利用Android的采样率变化和收敛行为，实现用户空间的运行时监控。

Result: 在1,448个商业应用中的评估显示，ARMOUR能有效检测混淆代码中的传感器使用，并发现50%的应用存在传感器滥用。

Conclusion: ARMOUR为用户和研究人员提供了便捷的监控工具，同时揭示了Android政策变化对零权限传感器的影响及剩余问题。

Abstract: This work investigates how to monitor access to Android zero-permission
sensors which could cause privacy leakage to users. Moreover, monitoring such
sensitive access allows security researchers to characterize potential sensor
abuse patterns. Zero-permission sensors such as accelerometers have become an
indispensable part of Android devices. The critical information they provide
has attracted extensive research investigating how data collectors could
capture more sensor data to enable both benign and exploitative applications.
In contrast, little work has explored how to enable data providers, such as end
users, to understand sensor usage. While existing methods such as static
analysis and hooking-based dynamic analysis face challenges of requiring
complicated development chains, rooting privilege, and app-specific reverse
engineering analysis, our work aims to bridge this gap by developing ARMOUR for
user-space runtime monitoring, leveraging the intrinsic sampling rate variation
and convergence behaviors of Android. ARMOUR enables privacy-aware users to
easily monitor how third-party apps use sensor data and support security
researchers to perform rapid app-agnostic sensor access analysis. Our
evaluation with 1,448 commercial applications shows the effectiveness of ARMOUR
in detecting sensor usage in obfuscated code and other conditions, and observes
salient sensor abuse patterns such as 50% of apps from seemingly
sensor-independent categories accessing data of multiple zero-permission
sensors. We analyze the impact of Android's recent policy changes on
zero-permission sensors and remaining technical and regulatory problems.

</details>


### [20] [Extended c-differential distinguishers of full 9 and reduced-round Kuznyechik cipher](https://arxiv.org/abs/2507.02181)
*Pantelimon Stanica,Ranit Dutta,Bimal Mandal*

Main category: cs.CR

TL;DR: 本文提出了一种新的截断内部$c$-差分密码分析方法，首次将$c$-差分均匀性应用于分组密码的实际分析中。通过内部$c$-差分方法解决了乘法破坏结构的问题，并建立了内部与外部$c$-差分均匀性的对偶关系。实验证明该方法对9轮Kuznyechik密码具有显著区分效果。


<details>
  <summary>Details</summary>
Motivation: 传统$c$-差分均匀性方法因乘法破坏分组密码结构而难以实际应用，本文旨在解决这一问题。

Method: 提出内部$c$-差分方法，通过$(F(cx\oplus a), F(x))$保留密码结构，并建立对偶关系。开发了统计计算框架，对9轮Kuznyechik进行截断$c$-差分分析。

Result: 实验结果显示显著的非随机性，偏差比达1.7倍，p值低至$1.85 \times 10^{-3}$，表明9轮Kuznyechik存在安全漏洞。

Conclusion: 本文首次实现了对9轮Kuznyechik的实际区分攻击，揭示了其安全边际不足。

Abstract: This paper introduces {\em truncated inner $c$-differential cryptanalysis}, a
novel technique that for the first time enables the practical application of
$c$-differential uniformity to block ciphers. While Ellingsen et al. (IEEE
Trans. Inf. Theory, 2020) established the notion of $c$-differential uniformity
using $(F(x\oplus a), cF(x))$, a key challenge remained: multiplication by $c$
disrupts the structural properties essential for block cipher analysis,
particularly key addition.
  We resolve this challenge by developing an \emph{inner} $c$-differential
approach where multiplication by $c$ affects the input: $(F(cx\oplus a),
F(x))$. We prove that the inner $c$-differential uniformity of a function $F$
equals the outer $c$-differential uniformity of $F^{-1}$, establishing a
fundamental duality. This modification preserves cipher structure while
enabling practical cryptanalytic applications.
  Our main contribution is a comprehensive multi-faceted
statistical-computational framework, implementing truncated $c$-differential
analysis against the full 9-round Kuznyechik cipher (the inner
$c$-differentials are immune to the key whitening at the backend). Through
extensive computational analysis involving millions of differential pairs, we
demonstrate statistically significant non-randomness across all tested round
counts. For the full 9-round cipher, we identify multiple configurations
triggering critical security alerts, with bias ratios reaching $1.7\times$ and
corrected p-values as low as $1.85 \times 10^{-3}$, suggesting insufficient
security margin against this new attack vector. This represents the first
practical distinguisher against the full 9-round Kuznyechik.

</details>


### [21] [EIM-TRNG: Obfuscating Deep Neural Network Weights with Encoding-in-Memory True Random Number Generator via RowHammer](https://arxiv.org/abs/2507.02206)
*Ranyang Zhou,Abeer Matar A. Almalky,Gamana Aragonda,Sabbir Ahmed,Filip Roth Trønnes-Christensen,Adnan Siraj Rakin,Shaahin Angizi*

Main category: cs.CR

TL;DR: 提出了一种基于DRAM物理随机性的新型硬件TRNG（EIM-TRNG），用于保护DNN模型参数的安全。


<details>
  <summary>Details</summary>
Motivation: 硬件安全中TRNG的重要性，以及DNN模型参数（如权重）的保护需求。

Method: 利用DRAM单元在RowHammer干扰下的物理随机性，设计EIM-TRNG作为熵源，并用于加密DNN权重数据。

Result: 验证了DRAM熵源的有效性，提供了一种低成本、高安全性的硬件级保护方案。

Conclusion: EIM-TRNG为机器学习模型的硬件级安全提供了新方向。

Abstract: True Random Number Generators (TRNGs) play a fundamental role in hardware
security, cryptographic systems, and data protection. In the context of Deep
NeuralNetworks (DNNs), safeguarding model parameters, particularly weights, is
critical to ensure the integrity, privacy, and intel-lectual property of AI
systems. While software-based pseudo-random number generators are widely used,
they lack the unpredictability and resilience offered by hardware-based TRNGs.
In this work, we propose a novel and robust Encoding-in-Memory TRNG called
EIM-TRNG that leverages the inherent physical randomness in DRAM cell behavior,
particularly under RowHammer-induced disturbances, for the first time. We
demonstrate how the unpredictable bit-flips generated through carefully
controlled RowHammer operations can be harnessed as a reliable entropy source.
Furthermore, we apply this TRNG framework to secure DNN weight data by encoding
via a combination of fixed and unpredictable bit-flips. The encrypted data is
later decrypted using a key derived from the probabilistic flip behavior,
ensuring both data confidentiality and model authenticity. Our results validate
the effectiveness of DRAM-based entropy extraction for robust, low-cost
hardware security and offer a promising direction for protecting machine
learning models at the hardware level.

</details>


### [22] [Linearly Homomorphic Ring Signature Scheme over Lattices](https://arxiv.org/abs/2507.02281)
*Heng Guo,Kun Tian,Fengxia Liu,Zhiyong Zheng*

Main category: cs.CR

TL;DR: 提出了首个基于格的线性同态环签名方案，结合了环签名的强匿名性和同态签名的可计算性，适用于需要匿名数据来源和可验证同态计算的场景。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏可行的同态环签名方案，而这类方案在隐私增强的同态计算中具有重要潜力。

Method: 基于格的线性同态环签名方案，在标准模型下基于小整数解（SIS）假设证明安全性。

Result: 方案在完全密钥暴露下实现强匿名性，并对内部腐败攻击具有不可伪造性。

Conclusion: 该方案为隐私增强的同态计算提供了后量子安全的解决方案，推动了相关领域的发展。

Abstract: Homomorphic ring signature schemes combine the strong anonymity of ring
signatures with the computability of homomorphic signatures, demonstrating
significant potential in scenarios requiring both anonymous data provenance and
verifiable homomorphic computation (e.g., confidential blockchain transactions
and secure multi-party computation). However, no feasible homomorphic ring
signature scheme currently exists.
  In this work, we propose the first lattice-based linearly homomorphic ring
signature scheme. Proven secure in the standard model under the small integer
solution (SIS) assumption, our scheme achieves strong anonymity under full key
exposure and unforgeability against insider corruption attacks. As the first
unified framework for ring signatures and linear homomorphic signatures, this
construction provides a post-quantum-secure solution for the aforementioned
applications, advancing the development of privacy-enhanced homomorphic
computation.

</details>


### [23] [Rethinking Broken Object Level Authorization Attacks Under Zero Trust Principle](https://arxiv.org/abs/2507.02309)
*Anbin Wu,Zhiyong Feng,Ruitao Feng*

Main category: cs.CR

TL;DR: BOLAZ是一个基于零信任原则的防御框架，用于解决RESTful API中的BOLA漏洞，通过分析资源ID的数据流和授权边界来防止横向权限提升。


<details>
  <summary>Details</summary>
Motivation: RESTful API中的BOLA漏洞是OWASP API安全十大漏洞之首，攻击者通过操纵API参数获取未授权访问，亟需有效的防御方法。

Method: BOLAZ利用静态污点跟踪技术，将API分为生产者和消费者，通过映射资源ID的传播路径确定授权边界。

Result: 在10个GitHub项目上验证，BOLAZ有效防御了CVE中的漏洞，并发现了35个新的BOLA漏洞。

Conclusion: BOLAZ是首个基于授权逻辑动态调整防御规则的方法，展示了在实际部署中的实用性。

Abstract: RESTful APIs facilitate data exchange between applications, but they also
expose sensitive resources to potential exploitation. Broken Object Level
Authorization (BOLA) is the top vulnerability in the OWASP API Security Top 10,
exemplifies a critical access control flaw where attackers manipulate API
parameters to gain unauthorized access. To address this, we propose BOLAZ, a
defense framework grounded in zero trust principles. BOLAZ analyzes the data
flow of resource IDs, pinpointing BOLA attack injection points and determining
the associated authorization intervals to prevent horizontal privilege
escalation. Our approach leverages static taint tracking to categorize APIs
into producers and consumers based on how they handle resource IDs. By mapping
the propagation paths of resource IDs, BOLAZ captures the context in which
these IDs are produced and consumed, allowing for precise identification of
authorization boundaries. Unlike defense methods based on common authorization
models, BOLAZ is the first authorization-guided method that adapts defense
rules based on the system's best-practice authorization logic. We validate
BOLAZ through empirical research on 10 GitHub projects. The results demonstrate
BOLAZ's effectiveness in defending against vulnerabilities collected from CVE
and discovering 35 new BOLA vulnerabilities in the wild, demonstrating its
practicality in real-world deployments.

</details>


### [24] [PII Jailbreaking in LLMs via Activation Steering Reveals Personal Information Leakage](https://arxiv.org/abs/2507.02332)
*Krishna Kanth Nakka,Xue Jiang,Xuebing Zhou*

Main category: cs.CR

TL;DR: 论文研究了通过操纵激活来绕过LLM对齐的隐私越狱方法，实验显示95%的越狱成功率，50%以上泄露真实个人信息。


<details>
  <summary>Details</summary>
Motivation: 探讨是否可以通过操纵LLM内部激活来绕过对齐机制，泄露隐私信息。

Method: 使用轻量级线性探针识别与隐私拒绝行为相关的注意力头，并通过操纵这些头的激活诱导模型生成非拒绝回答。

Result: 实验显示95%的越狱成功率，50%以上的回答泄露真实个人信息。

Conclusion: LLM中记忆的隐私信息可通过针对性操纵内部激活提取。

Abstract: This paper investigates privacy jailbreaking in LLMs via steering, focusing
on whether manipulating activations can bypass LLM alignment and alter response
behaviors to privacy related queries (e.g., a certain public figure's sexual
orientation). We begin by identifying attention heads predictive of refusal
behavior for private attributes (e.g., sexual orientation) using lightweight
linear probes trained with privacy evaluator labels. Next, we steer the
activations of a small subset of these attention heads guided by the trained
probes to induce the model to generate non-refusal responses. Our experiments
show that these steered responses often disclose sensitive attribute details,
along with other private information about data subjects such as life events,
relationships, and personal histories that the models would typically refuse to
produce. Evaluations across four LLMs reveal jailbreaking disclosure rates of
at least 95%, with more than 50% on average of these responses revealing true
personal information. Our controlled study demonstrates that private
information memorized in LLMs can be extracted through targeted manipulation of
internal activations.

</details>


### [25] [Evaluating Language Models For Threat Detection in IoT Security Logs](https://arxiv.org/abs/2507.02390)
*Jorge J. Tejero-Fernández,Alfonso Sánchez-Macián*

Main category: cs.CR

TL;DR: 本文提出了一种利用微调大语言模型（LLMs）进行物联网安全日志异常检测和缓解建议的流程，优于传统机器学习分类器。


<details>
  <summary>Details</summary>
Motivation: 日志分析在网络安全中至关重要，可用于检测网络和系统威胁。本文旨在利用LLMs提升异常检测和缓解建议的效果。

Method: 比较三种开源LLMs在零样本、少样本提示和微调策略下的表现，使用物联网数据集进行二进制和多类异常检测。

Result: LLMs在多类攻击分类中表现优于基线模型，并能结合MITRE CAPEC提供检测和缓解建议。

Conclusion: 微调LLMs在物联网安全日志分析中具有潜力，能够提供更准确的检测和实用的缓解建议。

Abstract: Log analysis is a relevant research field in cybersecurity as they can
provide a source of information for the detection of threats to networks and
systems. This paper presents a pipeline to use fine-tuned Large Language Models
(LLMs) for anomaly detection and mitigation recommendation using IoT security
logs. Utilizing classical machine learning classifiers as a baseline, three
open-source LLMs are compared for binary and multiclass anomaly detection, with
three strategies: zero-shot, few-shot prompting and fine-tuning using an IoT
dataset. LLMs give better results on multi-class attack classification than the
corresponding baseline models. By mapping detected threats to MITRE CAPEC,
defining a set of IoT-specific mitigation actions, and fine-tuning the models
with those actions, the models are able to provide a combined detection and
recommendation guidance.

</details>


### [26] [CyberRAG: An agentic RAG cyber attack classification and reporting tool](https://arxiv.org/abs/2507.02424)
*Francesco Blefari,Cristian Cosentino,Francesco Aurelio Pironti,Angelo Furfaro,Fabrizio Marozzo*

Main category: cs.CR

TL;DR: CyberRAG是一个基于代理的模块化RAG框架，用于实时分类、解释和结构化报告网络攻击，显著降低误报率并提高可解释性。


<details>
  <summary>Details</summary>
Motivation: 传统IDS/IPS系统生成大量警报，机器学习检测器误报率高，标准RAG管道检索不相关上下文且无法合理解释预测。

Method: 采用模块化代理框架，结合专用分类器、工具适配器和迭代检索-推理循环，动态优化威胁标签和自然语言解释。

Result: 分类准确率达94.92%，BERTScore为0.94，专家评分为4.9/5，显著降低误报率。

Conclusion: CyberRAG通过代理设计和语义协调，实现了高检测准确性和可信的解释，为半自主网络防御提供了实用方案。

Abstract: Intrusion Detection and Prevention Systems (IDS/IPS) in large enterprises can
generate hundreds of thousands of alerts per hour, overwhelming security
analysts with logs that demand deep, rapidly evolving domain expertise.
Conventional machine-learning detectors trim the alert volume but still yield
high false-positive rates, while standard single-pass Retrieval-Augmented
Generation (RAG) pipelines often retrieve irrelevant context and fail to
justify their predictions. To overcome these shortcomings, we present CyberRAG,
a modular, agent-based RAG framework that delivers real-time classification,
explanation, and structured reporting for cyber-attacks. A central LLM agent
orchestrates (i) a pool of fine-tuned specialized classifiers, each tailored to
a distinct attack family; (ii) tool adapters for enrichment and alerting; and
(iii) an iterative retrieval-and-reason loop that continuously queries a
domain-specific knowledge base until the evidence is both relevant and
self-consistent. Unlike traditional RAG systems, CyberRAG embraces an agentic
design that enables dynamic control flow and adaptive reasoning. This
agent-centric architecture refines its threat labels and natural-language
justifications autonomously, reducing false positives and enhancing
interpretability. The framework is fully extensible: new attack types can be
supported by simply adding a classifier without retraining the core agent.
CyberRAG has been evaluated achieving over 94% accuracy per class and pushing
final classification accuracy to 94.92% through semantic orchestration.
Generated explanations score up to 0.94 in BERTScore and 4.9/5 in GPT-4-based
expert evaluation. These results show that agentic, specialist-oriented RAG can
pair high detection accuracy with trustworthy, SOC-ready prose, offering a
practical and scalable path toward semi-autonomous cyber-defence workflows.

</details>


### [27] [Effectively Identifying Wi-Fi Devices through State Transitions](https://arxiv.org/abs/2507.02478)
*Melissa Safari,Abhishek K. Mishra,Mathieu Cunche*

Main category: cs.CR

TL;DR: 提出了一种基于Wi-Fi管理帧行为动态的设备指纹识别框架，通过有限状态机和矩阵表示方法，显著提高了设备识别的准确性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要关注探测请求，忽视了管理帧及其动态变化，导致在高密度、高移动性环境中识别效果不佳。

Method: 将设备行为建模为有限状态机，利用矩阵编码状态转换频率和状态间延迟特征，生成紧凑特征向量进行相似性比较。

Result: 在多样化真实环境中，非随机化设备的识别准确率超过86%，且通过时间突发聚合进一步优化。

Conclusion: 该方法能够大规模、一致且独特地识别设备，性能优于现有技术。

Abstract: Wi-Fi management frames reveal structured communication patterns that persist
even under randomization of MAC addresses. Prior approaches to associating
randomized MAC addresses with devices primarily focus on probe requests,
overlooking the broader set of management frames and their transition dynamics.
This narrow focus limits their robustness in dense, real-world environments
with high device mobility, where probe activity alone fails to yield stable and
distinctive signatures. In this paper, we present a novel framework for
fingerprinting Wi-Fi devices based on behavioral dynamics extracted from
passively observed management frames. We model each device's behavior as a
finite state machine and introduce matrix-based representations that encode
both structural (state transition frequencies) and temporal (inter-state
delays) characteristics. These matrices are embedded into compact feature
vectors, enabling efficient similarity comparison. Through extensive evaluation
in diverse real-world settings, our method achieves over 86% identification
accuracy for non-randomized devices using only Wi-Fi management frames, with
further improvements observed through temporal burst aggregation. Our findings
are sufficient to uniquely and consistently characterize devices at scale,
outperforming the state-of-the-art.

</details>


### [28] [A 10-bit S-box generated by Feistel construction from cellular automata](https://arxiv.org/abs/2507.02489)
*Thomas Prévost,Bruno Martin*

Main category: cs.CR

TL;DR: 提出了一种基于Feistel结构和5细胞自动机生成的10位S盒，其安全性优于或与AES S盒相当。


<details>
  <summary>Details</summary>
Motivation: 设计一种新的10位S盒，以替代现有密码（如ASCON）中的5位替换。

Method: 使用Feistel结构和基于特定规则的5细胞自动机生成子置换，并通过仿射变换优化。

Result: 生成的10位S盒在安全性上优于或与AES S盒相当。

Conclusion: 该S盒可用于替代现有密码中的5位替换。

Abstract: We propose a new 10-bit S-box generated from a Feistel construction. The
subpermutations are generated by a 5-cell cellular automaton based on a unique
well-chosen rule and bijective affine transformations. In particular, the
cellular automaton rule is chosen based on empirical tests of its ability to
generate good pseudorandom output on a ring cellular automaton. Similarly,
Feistel's network layout is based on empirical data regarding the quality of
the output S-box. We perform cryptanalysis of the generated 10-bit S-box, and
we find security properties comparable to or sometimes even better than those
of the standard AES S-box. We believe that our S-box could be used to replace
the 5-bit substitution of ciphers like ASCON.

</details>


### [29] [Real-Time Monitoring and Transparency in Pizza Production Using IoT and Blockchain](https://arxiv.org/abs/2507.02536)
*Azmat Ullah,Maria Ilaria Lunesu,Lodovica Marchesi,Roberto Tonelli*

Main category: cs.CR

TL;DR: 论文提出了一种基于区块链的物联网系统，用于监控餐厅披萨生产，结合实时数据采集与区块链技术确保数据安全。


<details>
  <summary>Details</summary>
Motivation: 解决披萨生产过程中的数据透明性和安全性问题，同时优化厨房管理效率。

Method: 使用物联网设备实时监测温湿度，Raspberry Pi处理数据并与智能合约交互，区块链技术确保数据不可篡改。

Result: 实验表明系统能有效检测异常情况，减少浪费，提升厨房效率。

Conclusion: 区块链与物联网的结合为食品生产提供了透明、可追溯的解决方案，具有实际应用价值。

Abstract: This paper presents a blockchain-based Internet of Things (IoT) system for
monitoring pizza production in restaurants. IoT devices track temperature and
humidity in real-time, while blockchain ensures secure and tamper-proof data. A
Raspberry Pi processes sensor data, captures images, triggers alerts, and
interacts with smart contracts. The system detects abnormal conditions,
enabling quick responses. Blockchain adds transparency and traceability,
supporting compliance and audits. Experiments show improved ingredient
management, reduced waste, and increased kitchen efficiency.

</details>


### [30] [Alleviating Attack Data Scarcity: SCANIA's Experience Towards Enhancing In-Vehicle Cyber Security Measures](https://arxiv.org/abs/2507.02607)
*Frida Sundfeldt,Bianca Widstam,Mahshid Helali Moghadam,Kuo-Yun Liang,Anders Vesterberg*

Main category: cs.CR

TL;DR: 论文提出了一种上下文感知的攻击数据生成器，用于生成模拟真实攻击场景的高质量数据，以解决车载网络安全中数据稀缺的问题。


<details>
  <summary>Details</summary>
Motivation: 随着联网车辆的数字化发展，网络安全风险增加，但受限于安全、成本和伦理问题，真实攻击数据稀缺，亟需高效生成攻击数据的方法。

Method: 采用参数化攻击模型，结合CAN消息解码和攻击强度调整，生成多种攻击类型的数据，并通过IDS案例验证其有效性。

Result: 生成的攻击数据在IDS模型中表现出高检测和分类能力，验证了数据的质量和一致性。

Conclusion: 该方法高效、可扩展，为车载网络安全研究提供了高质量的攻击数据生成解决方案。

Abstract: The digital evolution of connected vehicles and the subsequent security risks
emphasize the critical need for implementing in-vehicle cyber security measures
such as intrusion detection and response systems. The continuous advancement of
attack scenarios further highlights the need for adaptive detection mechanisms
that can detect evolving, unknown, and complex threats. The effective use of
ML-driven techniques can help address this challenge. However, constraints on
implementing diverse attack scenarios on test vehicles due to safety, cost, and
ethical considerations result in a scarcity of data representing attack
scenarios. This limitation necessitates alternative efficient and effective
methods for generating high-quality attack-representing data. This paper
presents a context-aware attack data generator that generates attack inputs and
corresponding in-vehicle network log, i.e., controller area network (CAN) log,
representing various types of attack including denial of service (DoS), fuzzy,
spoofing, suspension, and replay attacks. It utilizes parameterized attack
models augmented with CAN message decoding and attack intensity adjustments to
configure the attack scenarios with high similarity to real-world scenarios and
promote variability. We evaluate the practicality of the generated
attack-representing data within an intrusion detection system (IDS) case study,
in which we develop and perform an empirical evaluation of two deep neural
network IDS models using the generated data. In addition to the efficiency and
scalability of the approach, the performance results of IDS models, high
detection and classification capabilities, validate the consistency and
effectiveness of the generated data as well. In this experience study, we also
elaborate on the aspects influencing the fidelity of the data to real-world
scenarios and provide insights into its application.

</details>


### [31] [SAT-BO: Verification Rule Learning and Optimization for FraudTransaction Detection](https://arxiv.org/abs/2507.02635)
*Mao Luo,Zhi Wang,Yiwen Huang,Qingyun Zhang,Zhouxing Su,Zhipeng Lv,Wen Hu,Jianguo Li*

Main category: cs.CR

TL;DR: 电子支付平台每天处理数十亿交易，但手动构建的验证规则易受攻击，需系统性方法确保其鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 高交易量环境下，微小错误可能导致巨大财务损失，现有验证规则缺乏系统性保障，易被恶意利用。

Method: 通过系统性方法识别和修复验证规则的缺陷，确保其鲁棒性。

Result: 提出需求，需系统性识别规则缺陷以提升数据安全性。

Conclusion: 系统性方法对提升电子支付平台验证规则的鲁棒性和安全性至关重要。

Abstract: Electronic payment platforms are estimated to process billions oftransactions
daily, with the cumulative value of these transactionspotentially reaching into
the trillions. Even a minor error within thishigh-volume environment could
precipitate substantial financiallosses. To mitigate this risk, manually
constructed verification rules,developed by domain experts, are typically
employed to identifyand scrutinize transactions in production environments.
However,due to the absence of a systematic approach to ensure the robust-ness
of these verification rules against vulnerabilities, they remainsusceptible to
exploitation.To mitigate this risk, manually constructed verification rules,
de-veloped by domain experts, are typically employed to identify andscrutinize
transactions in production environments. However, dueto the absence of a
systematic approach to ensure the robustness ofthese verification rules against
vulnerabilities, they remain suscep-tible to exploitation. To ensure data
security, database maintainersusually compose complex verification rules to
check whether aquery/update request is valid. However, the rules written by
ex-perts are usually imperfect, and malicious requests may bypassthese rules.
As a result, the demand for identifying the defects ofthe rules systematically
emerges.

</details>


### [32] [Control at Stake: Evaluating the Security Landscape of LLM-Driven Email Agents](https://arxiv.org/abs/2507.02699)
*Jiangrong Wu,Yuhong Nan,Jianliang Wu,Zitong Yao,Zibin Zheng*

Main category: cs.CR

TL;DR: 该论文首次系统研究了LLM邮件代理的安全风险，提出了Email Agent Hijacking (EAH)攻击，并通过EAHawk工具评估了1,404个实例，发现所有实例均易受攻击。


<details>
  <summary>Details</summary>
Motivation: 随着LLM邮件代理应用的广泛部署，其安全机制尚未得到充分研究，存在潜在风险。

Method: 提出EAH攻击方法，并通过EAHawk工具对14个框架、63个应用、12个LLM和20个邮件服务进行大规模评估。

Result: 所有1,404个实例均被成功劫持，平均仅需2.03次攻击尝试即可控制代理。

Conclusion: LLM邮件代理存在严重安全漏洞，亟需加强防护措施。

Abstract: The increasing capabilities of LLMs have led to the rapid proliferation of
LLM agent apps, where developers enhance LLMs with access to external resources
to support complex task execution. Among these, LLM email agent apps represent
one of the widely used categories, as email remains a critical communication
medium for users. LLM email agents are capable of managing and responding to
email using LLM-driven reasoning and autonomously executing user instructions
via external email APIs (e.g., send email). However, despite their growing
deployment and utility, the security mechanism of LLM email agent apps remains
underexplored. Currently, there is no comprehensive study into the potential
security risk within these agent apps and their broader implications.
  In this paper, we conduct the first in-depth and systematic security study of
LLM email agents. We propose the Email Agent Hijacking (EAH) attack, which
overrides the original prompts of the email agent via external email resources,
allowing attackers to gain control of the email agent remotely and further
perform specific attack scenarios without user awareness.
  To facilitate the large-scale evaluation, we propose EAHawk, a pipeline to
evaluate the EAH attack of LLM email agent apps. By EAHawk, we performed an
empirical study spanning 14 representative LLM agent frameworks, 63 agent apps,
12 LLMs, and 20 email services, which led to the generation of 1,404 real-world
email agent instances for evaluation. Experimental results indicate that all
1,404 instances were successfully hijacked; on average, only 2.03 attack
attempts are required to control an email agent instance. Even worse, for some
LLMs, the average number of attempts needed to achieve full agent control drops
to as few as 1.23.

</details>


### [33] [Quantifying Classifier Utility under Local Differential Privacy](https://arxiv.org/abs/2507.02727)
*Ye Zheng,Yidan Hu*

Main category: cs.CR

TL;DR: 本文提出了一种理论框架，用于量化局部差分隐私（LDP）机制下分类器的效用，将LDP扰动与分类器鲁棒性分析结合，指导LDP机制和隐私参数的选择。


<details>
  <summary>Details</summary>
Motivation: 量化LDP扰动对分类器效用的影响是一个理论挑战，尤其是对复杂或黑盒分类器。

Method: 通过将LDP扰动集中在原始数据周围的概率特性，将分类器效用分析转化为其在该区域的鲁棒性分析。

Result: 理论效用量化与实证观察高度一致，尤其在低维输入空间中。

Conclusion: 分段机制在常见场景中表现更优，框架适用于任何LDP机制和分类器。

Abstract: Local differential privacy (LDP) provides a rigorous and quantifiable privacy
guarantee for personal data by introducing perturbation at the data source.
However, quantifying the impact of these perturbations on classifier utility
remains a theoretical challenge, particularly for complex or black-box
classifiers.
  This paper presents a framework for theoretically quantifying classifier
utility under LDP mechanisms. The key insight is that LDP perturbation is
concentrated around the original data with a specific probability, transforming
utility analysis of the classifier into its robustness analysis in this
concentrated region. Our framework connects the concentration analysis of LDP
mechanisms with the robustness analysis of classifiers. It treats LDP
mechanisms as general distributional functions and classifiers as black-box
functions, thus applicable to any LDP mechanism and classifier. A direct
application of our utility quantification is guiding the selection of LDP
mechanisms and privacy parameters for a given classifier. Notably, our analysis
shows that a piecewise-based mechanism leads to better utility compared to
alternatives in common scenarios.
  Using this framework alongside two novel refinement techniques, we conduct
case studies on utility quantification for typical mechanism-classifier
combinations. The results demonstrate that our theoretical utility
quantification aligns closely with empirical observations, particularly when
classifiers operate in lower-dimensional input spaces.

</details>


### [34] [Meta SecAlign: A Secure Foundation LLM Against Prompt Injection Attacks](https://arxiv.org/abs/2507.02735)
*Sizhe Chen,Arman Zharmagambetov,David Wagner,Chuan Guo*

Main category: cs.CR

TL;DR: Meta SecAlign是首个开源且具有内置模型级防御的LLM，性能达到商业级水平，并在安全和实用性基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 开源模型对AI安全社区至关重要，通过开放研究共同开发攻击和防御方法，推动对抗提示注入攻击的科学进展。

Method: 采用改进版的SOTA SecAlign防御方法，提供完整的训练细节，并在通用指令调优数据集上进行训练。

Result: Meta-SecAlign-70B在9个实用性基准和7个安全性基准上表现优异，具备对未见下游任务的安全性。

Conclusion: Meta SecAlign在对抗提示注入攻击方面达到最先进的鲁棒性，实用性接近闭源商业LLM。

Abstract: Prompt injection attacks pose a significant security threat to LLM-integrated
applications. Model-level defenses have shown strong effectiveness, but are
currently deployed into commercial-grade models in a closed-source manner. We
believe open-source models are needed by the AI security community, where
co-development of attacks and defenses through open research drives scientific
progress in mitigation against prompt injection attacks. To this end, we
develop Meta SecAlign, the first open-source and open-weight LLM with built-in
model-level defense that achieves commercial-grade model performance. We
provide complete details of our training recipe, which utilizes an improved
version of the SOTA SecAlign defense. Evaluations on 9 utility benchmarks and 7
security benchmarks show that Meta SecAlign, despite being trained on a generic
instruction-tuning dataset, confers security in unseen downstream tasks,
including tool-calling and agentic web navigation, in addition general
instruction-following. Our best model -- Meta-SecAlign-70B -- achieves
state-of-the-art robustness against prompt injection attacks and comparable
utility to closed-source commercial LLM with model-level defense.

</details>


### [35] [Early Signs of Steganographic Capabilities in Frontier LLMs](https://arxiv.org/abs/2507.02737)
*Artur Zolkowski,Kei Nishimura-Gasparian,Robert McCarthy,Roland S. Zimmermann,David Lindner*

Main category: cs.CR

TL;DR: 研究评估了前沿大型语言模型（LLM）的隐写能力，发现当前模型在标准条件下难以隐藏短消息，但在额外条件下可能成功，并显示出初步的隐写推理能力。


<details>
  <summary>Details</summary>
Motivation: 监控LLM输出对减少滥用和错位风险至关重要，但LLM可能通过隐写技术逃避监控。

Method: 评估LLM的隐写能力，包括隐藏消息和隐写推理两类，测试其在标准与额外条件下的表现。

Result: 当前模型在标准条件下难以隐藏消息，但在额外条件下可能成功；显示出初步的隐写推理能力，但难以欺骗监控。

Conclusion: 当前LLM的隐写能力尚不成熟，但未来可能发展，需警惕其潜在风险。

Abstract: Monitoring Large Language Model (LLM) outputs is crucial for mitigating risks
from misuse and misalignment. However, LLMs could evade monitoring through
steganography: Encoding hidden information within seemingly benign generations.
In this paper, we evaluate the steganography capabilities in frontier LLMs to
better understand the risk they pose. We focus on two types of steganography:
passing encoded messages and performing encoded reasoning. We find that current
models are unable to encode short messages in their outputs without a monitor
noticing under standard affordances. They can succeed, however, if given
additional affordances such as using an unmonitored scratchpad and coordinating
on what encoding scheme to use. We additionally find early signs that models
can perform basic encoded reasoning in a simple state-tracking problem. This
includes some ability to reason with their own and pre-defined schemes,
including encoding schemes such as Hexadecimal. Despite this, they can rarely
hide reasoning subtly within a cover task to fool a monitor. Overall, our
results indicate that current LLMs exhibit nascent steganographic capabilities.
While these capabilities are likely insufficient to bypass well-designed
monitors at present, this could change in the future.

</details>


### [36] [NVIDIA GPU Confidential Computing Demystified](https://arxiv.org/abs/2507.02770)
*Zhongshu Gu,Enriquillo Valdez,Salman Ahmed,Julian James Stephen,Michael Le,Hani Jamjoom,Shixuan Zhao,Zhiqiang Lin*

Main category: cs.CR

TL;DR: 本文分析了NVIDIA GPU-CC系统的实现，通过整合零散信息，揭示了其安全弱点。


<details>
  <summary>Details</summary>
Motivation: GPU-CC作为NVIDIA Hopper架构的一部分，扩展了传统CPU机密计算的信任边界，但其专有性和复杂性为安全研究带来挑战。

Method: 通过实验和推测，分析GPU内核模块，揭示系统组件的安全漏洞。

Result: 识别了GPU-CC的安全弱点，并向NVIDIA报告了相关发现。

Conclusion: 研究揭示了GPU-CC的潜在安全问题，为未来安全研究提供了方向。

Abstract: GPU Confidential Computing (GPU-CC) was introduced as part of the NVIDIA
Hopper Architecture, extending the trust boundary beyond traditional CPU-based
confidential computing. This innovation enables GPUs to securely process AI
workloads, providing a robust and efficient solution for handling sensitive
data. For end users, transitioning to GPU-CC mode is seamless, requiring no
modifications to existing AI applications. However, this ease of adoption
contrasts sharply with the complexity of the underlying proprietary systems.
The lack of transparency presents significant challenges for security
researchers seeking a deeper understanding of GPU-CC's architecture and
operational mechanisms.
  The challenges of analyzing the NVIDIA GPU-CC system arise from a scarcity of
detailed specifications, the proprietary nature of the ecosystem, and the
complexity of product design. In this paper, we aim to demystify the
implementation of NVIDIA GPU-CC system by piecing together the fragmented and
incomplete information disclosed from various sources. Our investigation begins
with a high-level discussion of the threat model and security principles before
delving into the low-level details of each system component. We instrument the
GPU kernel module -- the only open-source component of the system -- and
conduct a series of experiments to identify the security weaknesses and
potential exploits. For certain components that are out of reach through
experiments, we propose well-reasoned speculations about their inner working
mechanisms. We have responsibly reported all security findings presented in
this paper to the NVIDIA PSIRT Team.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [37] [STELLA: Self-Evolving LLM Agent for Biomedical Research](https://arxiv.org/abs/2507.02004)
*Ruofan Jin,Zaixi Zhang,Mengdi Wang,Le Cong*

Main category: cs.AI

TL;DR: STELLA是一种自进化的AI代理，通过动态工具库和模板库自主提升能力，在生物医学任务中表现优异且性能随经验增长。


<details>
  <summary>Details</summary>
Motivation: 生物医学数据的快速增长和碎片化研究环境需要动态适应的AI解决方案，而现有AI代理依赖静态工具集，无法满足需求。

Method: STELLA采用多代理架构，包含动态工具库（Tool Ocean）和模板库（Template Library），通过工具创建代理自动发现和集成新工具。

Result: STELLA在多个生物医学基准测试中表现优异，如Humanity's Last Exam（26%）、LAB-Bench: DBQA（54%）和LAB-Bench: LitQA（63%），性能随经验提升。

Conclusion: STELLA展示了动态学习和扩展能力的AI代理系统，有望加速生物医学研究进展。

Abstract: The rapid growth of biomedical data, tools, and literature has created a
fragmented research landscape that outpaces human expertise. While AI agents
offer a solution, they typically rely on static, manually curated toolsets,
limiting their ability to adapt and scale. Here, we introduce STELLA, a
self-evolving AI agent designed to overcome these limitations. STELLA employs a
multi-agent architecture that autonomously improves its own capabilities
through two core mechanisms: an evolving Template Library for reasoning
strategies and a dynamic Tool Ocean that expands as a Tool Creation Agent
automatically discovers and integrates new bioinformatics tools. This allows
STELLA to learn from experience. We demonstrate that STELLA achieves
state-of-the-art accuracy on a suite of biomedical benchmarks, scoring
approximately 26\% on Humanity's Last Exam: Biomedicine, 54\% on LAB-Bench:
DBQA, and 63\% on LAB-Bench: LitQA, outperforming leading models by up to 6
percentage points. More importantly, we show that its performance
systematically improves with experience; for instance, its accuracy on the
Humanity's Last Exam benchmark almost doubles with increased trials. STELLA
represents a significant advance towards AI Agent systems that can learn and
grow, dynamically scaling their expertise to accelerate the pace of biomedical
discovery.

</details>


### [38] [HCVR: A Hybrid Approach with Correlation-aware Voting Rules for Feature Selection](https://arxiv.org/abs/2507.02073)
*Nikita Bhedasgaonkar,Rushikesh K. Joshi*

Main category: cs.AI

TL;DR: HCVR是一种轻量级的基于规则的特征选择方法，结合P2P和P2T相关性，通过多数投票规则消除冗余特征并保留相关特征。


<details>
  <summary>Details</summary>
Motivation: 传统特征选择方法（如CFS、mRMR、MI、RFE、SFS和遗传算法）在性能上存在不足，HCVR旨在通过混合非迭代和迭代过滤方法提升效果。

Method: HCVR采用贪心算法，通过后向消除逐步删除多个特征，利用相关性阈值和多数投票规则决定特征去留。

Result: 在SPAMBASE数据集上，HCVR相比传统方法表现出更好的性能，分类器效果得到提升。

Conclusion: HCVR是一种有效的特征选择方法，适用于轻量级和高性能需求场景。

Abstract: In this paper, we propose HCVR (Hybrid approach with Correlation-aware Voting
Rules), a lightweight rule-based feature selection method that combines
Parameter-to-Parameter (P2P) and Parameter-to-Target (P2T) correlations to
eliminate redundant features and retain relevant ones. This method is a hybrid
of non-iterative and iterative filtering approaches for dimensionality
reduction. It is a greedy method, which works by backward elimination,
eliminating possibly multiple features at every step. The rules contribute to
voting for features, and a decision to keep or discard is made by majority
voting. The rules make use of correlation thresholds between every pair of
features, and between features and the target. We provide the results from the
application of HCVR to the SPAMBASE dataset. The results showed improvement
performance as compared to traditional non-iterative (CFS, mRMR and MI) and
iterative (RFE, SFS and Genetic Algorithm) techniques. The effectiveness was
assessed based on the performance of different classifiers after applying
filtering.

</details>


### [39] [Reasoning on a Budget: A Survey of Adaptive and Controllable Test-Time Compute in LLMs](https://arxiv.org/abs/2507.02076)
*Mohammad Ali Alomrani,Yingxue Zhang,Derek Li,Qianyi Sun,Soumyasundar Pal,Zhanguang Zhang,Yaochen Hu,Rohan Deepak Ajwani,Antonios Valkanas,Raika Karimi,Peng Cheng,Yunzhou Wang,Pengyi Liao,Hanrui Huang,Bin Wang,Jianye Hao,Mark Coates*

Main category: cs.AI

TL;DR: 本文综述了大型语言模型（LLMs）在推理效率方面的挑战，提出了一种两级分类法（L1可控性和L2适应性），并评估了主流LLMs的性能与计算资源消耗的权衡。


<details>
  <summary>Details</summary>
Motivation: 当前LLMs在推理时计算效率低下，无法根据任务复杂度动态调整计算资源，导致简单问题过度计算而复杂问题计算不足。

Method: 提出两级分类法：L1（固定计算预算的方法）和L2（动态调整计算资源的方法），并对主流LLMs进行基准测试。

Result: 揭示了推理性能与计算资源消耗之间的关键权衡，强调了TTC方法的实际控制性、适应性和可扩展性。

Conclusion: 未来研究方向包括混合思维模型，目标是提升LLMs的计算效率、鲁棒性和用户约束响应能力。

Abstract: Large language models (LLMs) have rapidly progressed into general-purpose
agents capable of solving a broad spectrum of tasks. However, current models
remain inefficient at reasoning: they apply fixed inference-time compute
regardless of task complexity, often overthinking simple problems while
underthinking hard ones. This survey presents a comprehensive review of
efficient test-time compute (TTC) strategies, which aim to improve the
computational efficiency of LLM reasoning. We introduce a two-tiered taxonomy
that distinguishes between L1-controllability, methods that operate under fixed
compute budgets, and L2-adaptiveness, methods that dynamically scale inference
based on input difficulty or model confidence. We benchmark leading proprietary
LLMs across diverse datasets, highlighting critical trade-offs between
reasoning performance and token usage. Compared to prior surveys on efficient
reasoning, our review emphasizes the practical control, adaptability, and
scalability of TTC methods. Finally, we discuss emerging trends such as hybrid
thinking models and identify key challenges for future work towards making LLMs
more computationally efficient, robust, and responsive to user constraints.

</details>


### [40] [Measuring Scientific Capabilities of Language Models with a Systems Biology Dry Lab](https://arxiv.org/abs/2507.02083)
*Haonan Duan,Stephen Zhewen Lu,Caitlin Fiona Harrigan,Nishkrit Desai,Jiarui Lu,Michał Koziarski,Leonardo Cotta,Chris J. Maddison*

Main category: cs.AI

TL;DR: SciGym是一个评估大型语言模型（LLM）在开放式科学发现任务中实验设计和分析能力的基准测试，通过干实验室模拟生物系统来避免湿实验室的高成本。


<details>
  <summary>Details</summary>
Motivation: 评估LLMs的科学能力，尤其是实验设计和结果解释能力，因为湿实验室实验成本过高。

Method: 使用系统生物学标记语言（SBML）编码的生物系统模型生成模拟数据，评估6个前沿LLM在137个小系统和350个总系统上的表现。

Result: 性能更强的模型表现更好，但随着系统复杂性增加，所有模型的性能显著下降。

Conclusion: LLM代理在科学能力方面仍有很大改进空间。

Abstract: Designing experiments and result interpretations are core scientific
competencies, particularly in biology, where researchers perturb complex
systems to uncover the underlying systems. Recent efforts to evaluate the
scientific capabilities of large language models (LLMs) fail to test these
competencies because wet-lab experimentation is prohibitively expensive: in
expertise, time and equipment. We introduce SciGym, a first-in-class benchmark
that assesses LLMs' iterative experiment design and analysis abilities in
open-ended scientific discovery tasks. SciGym overcomes the challenge of
wet-lab costs by running a dry lab of biological systems. These models, encoded
in Systems Biology Markup Language, are efficient for generating simulated
data, making them ideal testbeds for experimentation on realistically complex
systems. We evaluated six frontier LLMs on 137 small systems, and released a
total of 350 systems. Our evaluation shows that while more capable models
demonstrated superior performance, all models' performance declined
significantly as system complexity increased, suggesting substantial room for
improvement in the scientific capabilities of LLM agents.

</details>


### [41] [What Neuroscience Can Teach AI About Learning in Continuously Changing Environments](https://arxiv.org/abs/2507.02103)
*Daniel Durstewitz,Bruno Averbeck,Georgia Koppe*

Main category: cs.AI

TL;DR: 本文探讨了现代AI模型与动物学习方式的对比，提出从神经科学中汲取灵感以改进AI的持续学习和上下文学习能力。


<details>
  <summary>Details</summary>
Motivation: 现代AI模型训练成本高且固定，而动物能快速适应环境变化，尤其是社交物种。AI系统在现实世界中的应用需要类似的能力。

Method: 整合AI中的持续学习和上下文学习文献，与神经科学中行为任务学习的研究相结合。

Result: 提出了一个议程，探讨神经科学如何为AI发展提供具体见解，同时AI也能为神经科学带来启发。

Conclusion: 通过双向学习，神经科学与AI可以相互促进，推动NeuroAI领域的发展。

Abstract: Modern AI models, such as large language models, are usually trained once on
a huge corpus of data, potentially fine-tuned for a specific task, and then
deployed with fixed parameters. Their training is costly, slow, and gradual,
requiring billions of repetitions. In stark contrast, animals continuously
adapt to the ever-changing contingencies in their environments. This is
particularly important for social species, where behavioral policies and reward
outcomes may frequently change in interaction with peers. The underlying
computational processes are often marked by rapid shifts in an animal's
behaviour and rather sudden transitions in neuronal population activity. Such
computational capacities are of growing importance for AI systems operating in
the real world, like those guiding robots or autonomous vehicles, or for
agentic AI interacting with humans online. Can AI learn from neuroscience? This
Perspective explores this question, integrating the literature on continual and
in-context learning in AI with the neuroscience of learning on behavioral tasks
with shifting rules, reward probabilities, or outcomes. We will outline an
agenda for how specifically insights from neuroscience may inform current
developments in AI in this area, and - vice versa - what neuroscience may learn
from AI, contributing to the evolving field of NeuroAI.

</details>


### [42] [The Illusion of Fairness: Auditing Fairness Interventions with Audit Studies](https://arxiv.org/abs/2507.02152)
*Disa Sariola,Patrick Button,Aron Culotta,Nicholas Mattei*

Main category: cs.AI

TL;DR: 论文探讨了如何利用审计研究数据改进自动招聘算法的训练和评估，发现传统公平干预方法存在隐藏偏差，并提出基于个体治疗效果估计的新干预方法。


<details>
  <summary>Details</summary>
Motivation: 研究动机是解决AI系统（如招聘算法）中的偏见问题，传统方法依赖便利样本数据，可能引入选择偏差和标签偏差。

Method: 方法包括利用审计研究的高质量数据，分析传统公平干预方法的局限性，并提出基于个体治疗效果估计的新干预措施。

Result: 结果显示，传统方法在表面上实现公平，但实际存在约10%的偏差；新干预方法进一步减少了算法歧视。

Conclusion: 结论指出审计研究数据能更准确地评估和改善算法公平性，新方法在减少歧视方面更有效。

Abstract: Artificial intelligence systems, especially those using machine learning, are
being deployed in domains from hiring to loan issuance in order to automate
these complex decisions. Judging both the effectiveness and fairness of these
AI systems, and their human decision making counterpart, is a complex and
important topic studied across both computational and social sciences. Within
machine learning, a common way to address bias in downstream classifiers is to
resample the training data to offset disparities. For example, if hiring rates
vary by some protected class, then one may equalize the rate within the
training set to alleviate bias in the resulting classifier. While simple and
seemingly effective, these methods have typically only been evaluated using
data obtained through convenience samples, introducing selection bias and label
bias into metrics. Within the social sciences, psychology, public health, and
medicine, audit studies, in which fictitious ``testers'' (e.g., resumes,
emails, patient actors) are sent to subjects (e.g., job openings, businesses,
doctors) in randomized control trials, provide high quality data that support
rigorous estimates of discrimination. In this paper, we investigate how data
from audit studies can be used to improve our ability to both train and
evaluate automated hiring algorithms. We find that such data reveals cases
where the common fairness intervention method of equalizing base rates across
classes appears to achieve parity using traditional measures, but in fact has
roughly 10% disparity when measured appropriately. We additionally introduce
interventions based on individual treatment effect estimation methods that
further reduce algorithmic discrimination using this data.

</details>


### [43] [Data Diversification Methods In Alignment Enhance Math Performance In LLMs](https://arxiv.org/abs/2507.02173)
*Berkan Dokmeci,Qingyang Wu,Ben Athiwaratkun,Ce Zhang,Shuaiwen Leon Song,James Zou*

Main category: cs.AI

TL;DR: 通过数据多样化策略（如DTS）提升大语言模型的数学推理能力，相比传统方法更高效。


<details>
  <summary>Details</summary>
Motivation: 尽管偏好学习在人类反馈对齐方面取得进展，数学推理仍是挑战，需探索数据多样化策略的效果。

Method: 评估温度采样、思维链提示和MCTS，并引入DTS方法，系统分解问题为多样化推理路径。

Result: DTS在GSM8K和MATH上分别提升7.1%和4.2%，计算开销仅增加1.03倍，优于MCTS。

Conclusion: 结构化探索多样化解题方法能生成更有效的偏好数据，提升数学推理性能。

Abstract: While recent advances in preference learning have enhanced alignment in human
feedback, mathematical reasoning remains a persistent challenge. We investigate
how data diversification strategies in preference optimization can improve the
mathematical reasoning abilities of large language models (LLMs). We evaluate
three common data generation methods: temperature sampling, Chain-of-Thought
prompting, and Monte Carlo Tree Search (MCTS), and introduce
Diversified-ThinkSolve (DTS), a novel structured approach that systematically
decomposes problems into diverse reasoning paths. Our results show that with
strategically diversified preference data, models can substantially improve
mathematical reasoning performance, with the best approach yielding gains of
7.1% on GSM8K and 4.2% on MATH over the base model. Despite its strong
performance, DTS incurs only a marginal computational overhead (1.03x) compared
to the baseline, while MCTS is nearly five times more costly with lower
returns. These findings demonstrate that structured exploration of diverse
problem-solving methods creates more effective preference data for mathematical
alignment than traditional approaches.

</details>


### [44] [Do Role-Playing Agents Practice What They Preach? Belief-Behavior Consistency in LLM-Based Simulations of Human Trust](https://arxiv.org/abs/2507.02197)
*Amogh Mannekote,Adam Davies,Guohao Li,Kristy Elizabeth Boyer,ChengXiang Zhai,Bonnie J Dorr,Francesco Pinto*

Main category: cs.AI

TL;DR: 研究探讨了LLM作为角色扮演代理时，其陈述的信念与实际行为的一致性，并提出了评估框架和一致性度量。


<details>
  <summary>Details</summary>
Motivation: 随着LLM被用作角色扮演代理生成合成数据，确保其输出与角色一致成为关键问题。

Method: 通过增强的GenAgents角色库和信任游戏，引入信念-行为一致性度量，研究影响因素如信念类型、信息呈现方式和预测时间。

Result: 发现LLM的陈述信念与模拟行为存在系统性不一致，即使信念看似合理，也可能无法一致应用。

Conclusion: 需明确LLM信念与行为对齐的条件，以在行为研究中合理使用LLM代理。

Abstract: As LLMs are increasingly studied as role-playing agents to generate synthetic
data for human behavioral research, ensuring that their outputs remain coherent
with their assigned roles has become a critical concern. In this paper, we
investigate how consistently LLM-based role-playing agents' stated beliefs
about the behavior of the people they are asked to role-play ("what they say")
correspond to their actual behavior during role-play ("how they act").
Specifically, we establish an evaluation framework to rigorously measure how
well beliefs obtained by prompting the model can predict simulation outcomes in
advance. Using an augmented version of the GenAgents persona bank and the Trust
Game (a standard economic game used to quantify players' trust and
reciprocity), we introduce a belief-behavior consistency metric to
systematically investigate how it is affected by factors such as: (1) the types
of beliefs we elicit from LLMs, like expected outcomes of simulations versus
task-relevant attributes of individual characters LLMs are asked to simulate;
(2) when and how we present LLMs with relevant information about Trust Game;
and (3) how far into the future we ask the model to forecast its actions. We
also explore how feasible it is to impose a researcher's own theoretical priors
in the event that the originally elicited beliefs are misaligned with research
objectives. Our results reveal systematic inconsistencies between LLMs' stated
(or imposed) beliefs and the outcomes of their role-playing simulation, at both
an individual- and population-level. Specifically, we find that, even when
models appear to encode plausible beliefs, they may fail to apply them in a
consistent way. These findings highlight the need to identify how and when
LLMs' stated beliefs align with their simulated behavior, allowing researchers
to use LLM-based agents appropriately in behavioral studies.

</details>


### [45] [Dilution, Diffusion and Symbiosis in Spatial Prisoner's Dilemma with Reinforcement Learning](https://arxiv.org/abs/2507.02211)
*Gustavo C. Mangold,Heitor C. M. Fernandes,Mendeli H. Vainstein*

Main category: cs.AI

TL;DR: 研究探讨了空间囚徒困境中稀释和移动性对多智能体Q学习算法的影响，展示了算法的多样性和建模潜力。


<details>
  <summary>Details</summary>
Motivation: 探索静态智能体如何通过不同机制（如噪声注入、学习算法类型和邻居收益知识）在空间囚徒困境中实现合作。

Method: 使用独立多智能体Q学习算法，研究稀释和移动性对空间囚徒困境的影响，定义不同动作以连接经典非强化学习结果。

Result: 观察到多种效应，包括固定更新规则与学习规则的定性等价性，以及多动作定义下种群间共生互利效应的出现。

Conclusion: 展示了Q学习算法在建模不同博弈场景中的多样性和基准潜力，为空间囚徒困境研究提供了新视角。

Abstract: Recent studies in the spatial prisoner's dilemma games with reinforcement
learning have shown that static agents can learn to cooperate through a diverse
sort of mechanisms, including noise injection, different types of learning
algorithms and neighbours' payoff knowledge.In this work, using an independent
multi-agent Q-learning algorithm, we study the effects of dilution and mobility
in the spatial version of the prisoner's dilemma. Within this setting,
different possible actions for the algorithm are defined, connecting with
previous results on the classical, non-reinforcement learning spatial
prisoner's dilemma, showcasing the versatility of the algorithm in modeling
different game-theoretical scenarios and the benchmarking potential of this
approach.As a result, a range of effects is observed, including evidence that
games with fixed update rules can be qualitatively equivalent to those with
learned ones, as well as the emergence of a symbiotic mutualistic effect
between populations that forms when multiple actions are defined.

</details>


### [46] [Scaling LLM Planning: NL2FLOW for Parametric Problem Generation and Rigorous Evaluation](https://arxiv.org/abs/2507.02253)
*Jungkoo Kang*

Main category: cs.AI

TL;DR: NL2FLOW是一个自动化系统，用于生成自然语言规划问题并评估LLM生成的计划质量，结果显示直接推理比中间翻译步骤更有效。


<details>
  <summary>Details</summary>
Motivation: 解决大规模语言模型（LLM）规划和推理能力提升中的数据生成和评估瓶颈问题。

Method: 引入NL2FLOW系统，自动化生成规划问题（自然语言、结构化中间表示和PDDL），并评估LLM生成的计划质量。

Result: 最高性能模型在生成有效计划上达到86%成功率，生成最优计划为69%。直接推理优于中间翻译步骤。

Conclusion: 直接推理能力对LLM性能至关重要，动态理解系统限制是释放其潜力的关键。

Abstract: Progress in enhancing large language model (LLM) planning and reasoning
capabilities is significantly hampered by the bottleneck of scalable, reliable
data generation and evaluation. To overcome this, I introduce NL2FLOW, a fully
automated system for parametrically generating planning problems - expressed in
natural language, a structured intermediate representation, and formal PDDL -
and rigorously evaluating the quality of generated plans. I demonstrate
NL2FLOW's capabilities by generating a dataset of 2296 problems in the
automated workflow generation domain and evaluating multiple open-sourced,
instruct-tuned LLMs. My results reveal that the highest performing models
achieved 86% success in generating valid plans and 69% in generating optimal
plans, specifically for problems with feasible solutions. Regression analysis
shows that the influence of problem characteristics on plan generation is
contingent on both model and prompt design. Notably, I observed that the
highest success rate for translating natural language into a JSON
representation of a plan was lower than the highest rate of generating a valid
plan directly. This suggests that unnecessarily decomposing the reasoning task
- introducing intermediate translation steps - may actually degrade
performance, implying a benefit to models capable of reasoning directly from
natural language to action. As I scale LLM reasoning to increasingly complex
problems, the bottlenecks and sources of error within these systems will
inevitably shift. Therefore, a dynamic understanding of these limitations - and
the tools to systematically reveal them - will be crucial for unlocking the
full potential of LLMs as intelligent problem solvers.

</details>


### [47] [Iterated belief revision: from postulates to abilities](https://arxiv.org/abs/2507.02319)
*Paolo Liberatore*

Main category: cs.AI

TL;DR: 该论文探讨了信念修正领域的现状，指出现有研究多依赖形式化假设（postulates），而缺乏对修正机制能力的分析。作者提出修正机制应具备灵活性（如可塑性、平等化、教条化等能力），并分析了多种修正机制的能力。


<details>
  <summary>Details</summary>
Motivation: 现有信念修正研究过于依赖形式化假设，忽略了修正机制的实际能力（如能否达到特定信念状态）。作者希望通过分析修正机制的能力，填补这一研究空白。

Method: 通过列举和分析多种修正机制（如词典序、自然修正等），证明它们各自具备的能力（如可塑性、平等化等）。

Result: 不同修正机制具备不同的能力，例如词典序修正具有可塑性，而自然修正则缺乏某些能力。

Conclusion: 信念修正研究应关注修正机制的能力而非仅依赖形式化假设，以满足不同应用场景的需求。

Abstract: The belief revision field is opulent in new proposals and indigent in
analyses of existing approaches. Much work hinge on postulates, employed as
syntactic characterizations: some revision mechanism is equivalent to some
properties. Postulates constraint specific revision instances: certain
revisions update certain beliefs in a certain way. As an example, if the
revision is consistent with the current beliefs, it is incorporated with no
other change. A postulate like this tells what revisions must do and neglect
what they can do. Can they reach a certain state of beliefs? Can they reach all
possible states of beliefs? Can they reach all possible states of beliefs from
no previous belief? Can they reach a dogmatic state of beliefs, where
everything not believed is impossible? Can they make two conditions equally
believed? An application where every possible state of beliefs is sensible
requires each state of beliefs to be reachable. An application where conditions
may be equally believed requires such a belief state to be reachable. An
application where beliefs may become dogmatic requires a way to make them
dogmatic. Such doxastic states need to be reached in a way or another. Not in
specific way, as dictated by a typical belief revision postulate. This is an
ability, not a constraint: the ability of being plastic, equating, dogmatic.
Amnesic, correcting, believer, damascan, learnable are other abilities. Each
revision mechanism owns some of these abilities and lacks the others:
lexicographic, natural, restrained, very radical, full meet, radical, severe,
moderate severe, deep severe, plain severe and deep severe revisions, each of
these revisions is proved to possess certain abilities.

</details>


### [48] [OMS: On-the-fly, Multi-Objective, Self-Reflective Ad Keyword Generation via LLM Agent](https://arxiv.org/abs/2507.02353)
*Bowen Chen,Zhao Wang,Shingo Takamatsu*

Main category: cs.AI

TL;DR: OMS框架解决了LLM在关键词生成中的三大限制：无需训练数据、多目标优化和自反思关键词质量。实验证明其优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有LLM方法依赖大规模数据、缺乏多目标优化和质量控制，无法完全自动化关键词决策。

Method: 提出OMS框架，具备实时性（无需训练数据）、多目标优化和自反思能力。

Result: 实验显示OMS在基准测试和实际广告活动中表现优于现有方法。

Conclusion: OMS通过实时监控、多目标优化和自反思，显著提升了关键词生成的质量和效果。

Abstract: Keyword decision in Sponsored Search Advertising is critical to the success
of ad campaigns. While LLM-based methods offer automated keyword generation,
they face three major limitations: reliance on large-scale query-keyword pair
data, lack of online multi-objective performance monitoring and optimization,
and weak quality control in keyword selection. These issues hinder the agentic
use of LLMs in fully automating keyword decisions by monitoring and reasoning
over key performance indicators such as impressions, clicks, conversions, and
CTA effectiveness. To overcome these challenges, we propose OMS, a keyword
generation framework that is On-the-fly (requires no training data, monitors
online performance, and adapts accordingly), Multi-objective (employs agentic
reasoning to optimize keywords based on multiple performance metrics), and
Self-reflective (agentically evaluates keyword quality). Experiments on
benchmarks and real-world ad campaigns show that OMS outperforms existing
methods; ablation and human evaluations confirm the effectiveness of each
component and the quality of generated keywords.

</details>


### [49] [An AI-native experimental laboratory for autonomous biomolecular engineering](https://arxiv.org/abs/2507.02379)
*Mingyu Wu,Zhaoguo Wang,Jiabin Wang,Zhiyuan Dong,Jingkai Yang,Qingting Li,Tianyu Huang,Lei Zhao,Mingqiang Li,Fei Wang,Chunhai Fan,Haibo Chen*

Main category: cs.AI

TL;DR: AI驱动的自主实验室，支持复杂多目标实验，提升实验效率，匹配人类科学家的成果。


<details>
  <summary>Details</summary>
Motivation: 实现非专家也能独立进行复杂实验的自主科学研究，突破现有系统局限于简单实验的瓶颈。

Method: 采用AI原生设计，整合模型、实验与仪器，支持多用户请求，自主优化实验流程。

Result: 自主实验室在核酸功能实验中表现优异，匹配人类科学家的成果，显著提升仪器利用率和实验效率。

Conclusion: 该平台为生物材料研究提供了无需依赖专家的解决方案，为规模化科学服务奠定了基础。

Abstract: Autonomous scientific research, capable of independently conducting complex
experiments and serving non-specialists, represents a long-held aspiration.
Achieving it requires a fundamental paradigm shift driven by artificial
intelligence (AI). While autonomous experimental systems are emerging, they
remain confined to areas featuring singular objectives and well-defined, simple
experimental workflows, such as chemical synthesis and catalysis. We present an
AI-native autonomous laboratory, targeting highly complex scientific
experiments for applications like autonomous biomolecular engineering. This
system autonomously manages instrumentation, formulates experiment-specific
procedures and optimization heuristics, and concurrently serves multiple user
requests. Founded on a co-design philosophy of models, experiments, and
instruments, the platform supports the co-evolution of AI models and the
automation system. This establishes an end-to-end, multi-user autonomous
laboratory that handles complex, multi-objective experiments across diverse
instrumentation. Our autonomous laboratory supports fundamental nucleic acid
functions-including synthesis, transcription, amplification, and sequencing. It
also enables applications in fields such as disease diagnostics, drug
development, and information storage. Without human intervention, it
autonomously optimizes experimental performance to match state-of-the-art
results achieved by human scientists. In multi-user scenarios, the platform
significantly improves instrument utilization and experimental efficiency. This
platform paves the way for advanced biomaterials research to overcome
dependencies on experts and resource barriers, establishing a blueprint for
science-as-a-service at scale.

</details>


### [50] [The Gauss-Markov Adjunction: Categorical Semantics of Residuals in Supervised Learning](https://arxiv.org/abs/2507.02442)
*Moto Kamiura*

Main category: cs.AI

TL;DR: 论文通过范畴论重构机器学习模型，提出了一种语义框架，以增强AI系统的可解释性和可理解性，特别关注多元线性回归模型。


<details>
  <summary>Details</summary>
Motivation: 响应AI可解释性需求，促进AI在社会中的更好实施。

Method: 利用范畴论定义参数和数据的两个具体范畴，并通过伴随函子对监督学习进行范畴化建模，提出Gauss-Markov伴随结构。

Result: 展示了参数和残差之间的对偶信息流关系，并通过右伴随函子保持极限的性质关联最小二乘估计和最小残差。

Conclusion: 该框架为监督学习提供了扩展的指称语义实例，并建议将理论计算机科学中的语义视角作为AI可解释性的形式基础。

Abstract: Enhancing the intelligibility and interpretability of machine learning is a
crucial task in responding to the demand for Explicability as an AI principle,
and in promoting the better social implementation of AI. The aim of our
research is to contribute to this improvement by reformulating machine learning
models through the lens of category theory, thereby developing a semantic
framework for structuring and understanding AI systems. Our categorical
modeling in this paper clarifies and formalizes the structural interplay
between residuals and parameters in supervised learning. The present paper
focuses on the multiple linear regression model, which represents the most
basic form of supervised learning. By defining two concrete categories
corresponding to parameters and data, along with an adjoint pair of functors
between them, we introduce our categorical formulation of supervised learning.
We show that the essential structure of this framework is captured by what we
call the Gauss-Markov Adjunction. Within this setting, the dual flow of
information can be explicitly described as a correspondence between variations
in parameters and residuals. The ordinary least squares estimator for the
parameters and the minimum residual are related via the preservation of limits
by the right adjoint functor. Furthermore, we position this formulation as an
instance of extended denotational semantics for supervised learning, and
propose applying a semantic perspective developed in theoretical computer
science as a formal foundation for Explicability in AI.

</details>


### [51] [Clarifying Before Reasoning: A Coq Prover with Structural Context](https://arxiv.org/abs/2507.02541)
*Yanzhen Lu,Hanbin Yang,Xiaodie Wang,Ge Zhang,Biao Li,Chenxu Fu,Chao Li,Yang Yuan,Andrew Chi-Chih Yao*

Main category: cs.AI

TL;DR: 通过提升任务清晰度，结合结构化语义上下文，显著提高了大型语言模型在Coq定理证明中的推理能力，性能超越现有最佳方法。


<details>
  <summary>Details</summary>
Motivation: 研究任务清晰度对大型语言模型推理能力的提升作用，特别是在定理证明领域。

Method: 引入概念级指标评估任务清晰度，通过结构化语义上下文增强输入，采用Planner-Executor架构和选择性概念展开。

Result: 清晰度得分提升1.85倍（44.5%→82.3%），证明成功率提升2.1倍（21.8%→45.8%），超越Graph2Tac（33.2%）。

Conclusion: 结构化任务表示能有效弥合理解与推理之间的差距，提升模型性能。

Abstract: In this work, we investigate whether improving task clarity can enhance
reasoning ability of large language models, focusing on theorem proving in Coq.
We introduce a concept-level metric to evaluate task clarity and show that
adding structured semantic context to the standard input used by modern LLMs,
leads to a 1.85$\times$ improvement in clarity score
(44.5\%~$\rightarrow$~82.3\%). Using the general-purpose model
\texttt{DeepSeek-V3}, our approach leads to a 2.1$\times$ improvement in proof
success (21.8\%~$\rightarrow$~45.8\%) and outperforms the previous
state-of-the-art \texttt{Graph2Tac} (33.2\%). We evaluate this on 1,386
theorems randomly sampled from 15 standard Coq packages, following the same
evaluation protocol as \texttt{Graph2Tac}. Furthermore, fine-tuning smaller
models on our structured data can achieve even higher performance (48.6\%). Our
method uses selective concept unfolding to enrich task descriptions, and
employs a Planner--Executor architecture. These findings highlight the value of
structured task representations in bridging the gap between understanding and
reasoning.

</details>


### [52] [AI Research Agents for Machine Learning: Search, Exploration, and Generalization in MLE-bench](https://arxiv.org/abs/2507.02554)
*Edan Toledo,Karen Hambardzumyan,Martin Josifoski,Rishi Hazra,Nicolas Baldwin,Alexis Audran-Reiss,Michael Kuchnik,Despoina Magka,Minqi Jiang,Alisia Maria Lupidi,Andrei Lupu,Roberta Raileanu,Kelvin Niu,Tatiana Shavrina,Jean-Christophe Gagnon-Audet,Michael Shvartsman,Shagun Sodhani,Alexander H. Miller,Abhishek Charnalia,Derek Dunfield,Carole-Jean Wu,Pontus Stenetorp,Nicola Cancedda,Jakob Nicolaus Foerster,Yoram Bachrach*

Main category: cs.AI

TL;DR: AI研究代理通过优化搜索策略和操作符集，在MLE-bench基准测试中显著提升了性能，成功率达到47.7%。


<details>
  <summary>Details</summary>
Motivation: 探索如何通过改进AI研究代理的搜索策略和操作符设计，提升其在真实机器学习问题中的表现。

Method: 将AI研究代理形式化为搜索策略，设计并比较不同操作符集和搜索策略（贪婪、MCTS、进化）的组合。

Result: 最佳组合在MLE-bench lite上实现了47.7%的成功率，比之前的39.6%显著提升。

Conclusion: 搜索策略、操作符设计和评估方法的联合考虑对自动化机器学习至关重要。

Abstract: AI research agents are demonstrating great potential to accelerate scientific
progress by automating the design, implementation, and training of machine
learning models. We focus on methods for improving agents' performance on
MLE-bench, a challenging benchmark where agents compete in Kaggle competitions
to solve real-world machine learning problems. We formalize AI research agents
as search policies that navigate a space of candidate solutions, iteratively
modifying them using operators. By designing and systematically varying
different operator sets and search policies (Greedy, MCTS, Evolutionary), we
show that their interplay is critical for achieving high performance. Our best
pairing of search strategy and operator set achieves a state-of-the-art result
on MLE-bench lite, increasing the success rate of achieving a Kaggle medal from
39.6% to 47.7%. Our investigation underscores the importance of jointly
considering the search strategy, operator design, and evaluation methodology in
advancing automated machine learning.

</details>


### [53] [Responsibility Gap and Diffusion in Sequential Decision-Making Mechanisms](https://arxiv.org/abs/2507.02582)
*Junli Jiang,Pavel Naumov*

Main category: cs.AI

TL;DR: 论文研究了集体决策中责任的两个重要属性（扩散和间隙）的计算复杂性，发现扩散无间隙和间隙无间隙决策机制分别属于Π₂-完全和Π₃-完全类，而两者的交集仍为Π₂-完全类。


<details>
  <summary>Details</summary>
Motivation: 探讨责任在集体决策中的计算复杂性，填补AI领域对责任研究的空白。

Method: 分析扩散和间隙两种责任属性的计算复杂性，并研究其交集的性质。

Result: 扩散无间隙机制为Π₂-完全，间隙无间隙机制为Π₃-完全，两者的交集为Π₂-完全。

Conclusion: 研究揭示了责任属性在集体决策中的计算复杂性，为相关领域提供了理论支持。

Abstract: Responsibility has long been a subject of study in law and philosophy. More
recently, it became a focus of AI literature. The article investigates the
computational complexity of two important properties of responsibility in
collective decision-making: diffusion and gap. It shows that the sets of
diffusion-free and gap-free decision-making mechanisms are $\Pi_2$-complete and
$\Pi_3$-complete, respectively. At the same time, the intersection of these
classes is $\Pi_2$-complete.

</details>


### [54] [DynamiCare: A Dynamic Multi-Agent Framework for Interactive and Open-Ended Medical Decision-Making](https://arxiv.org/abs/2507.02616)
*Tianqi Shang,Weiqing He,Charles Zheng,Lingyao Li,Li Shen,Bingxin Zhao*

Main category: cs.AI

TL;DR: 论文提出了MIMIC-Patient数据集和DynamiCare框架，用于模拟动态、多轮交互的临床诊断过程，填补了现有单轮任务模型的不足。


<details>
  <summary>Details</summary>
Motivation: 现有医疗决策框架多关注单轮任务，与真实诊断过程（动态、不确定、多轮交互）不符，需改进。

Method: 基于MIMIC-III EHR构建MIMIC-Patient数据集，并提出DynamiCare框架，模拟多轮交互诊断过程。

Result: 实验验证了DynamiCare的可行性和有效性，建立了首个动态临床决策基准。

Conclusion: DynamiCare为LLM驱动的动态医疗决策提供了新方向，填补了研究空白。

Abstract: The rise of Large Language Models (LLMs) has enabled the development of
specialized AI agents with domain-specific reasoning and interaction
capabilities, particularly in healthcare. While recent frameworks simulate
medical decision-making, they largely focus on single-turn tasks where a doctor
agent receives full case information upfront -- diverging from the real-world
diagnostic process, which is inherently uncertain, interactive, and iterative.
In this paper, we introduce MIMIC-Patient, a structured dataset built from the
MIMIC-III electronic health records (EHRs), designed to support dynamic,
patient-level simulations. Building on this, we propose DynamiCare, a novel
dynamic multi-agent framework that models clinical diagnosis as a multi-round,
interactive loop, where a team of specialist agents iteratively queries the
patient system, integrates new information, and dynamically adapts its
composition and strategy. We demonstrate the feasibility and effectiveness of
DynamiCare through extensive experiments, establishing the first benchmark for
dynamic clinical decision-making with LLM-powered agents.

</details>


### [55] [Strategic Intelligence in Large Language Models: Evidence from evolutionary Game Theory](https://arxiv.org/abs/2507.02618)
*Kenneth Payne,Baptiste Alloui-Cros*

Main category: cs.AI

TL;DR: 大型语言模型（LLMs）在迭代囚徒困境（IPD）中表现出战略智能，能够推理目标并在竞争环境中生存。


<details>
  <summary>Details</summary>
Motivation: 探讨LLMs是否具备战略智能，能否在竞争环境中做出目标导向的决策。

Method: 通过一系列进化IPD锦标赛，将经典策略（如以牙还牙、冷酷触发）与前沿AI公司的模型（OpenAI、Google、Anthropic）对抗，并引入终止概率增加复杂性。

Result: LLMs表现出高度竞争力，展现出独特的战略特征：Google的Gemini模型冷酷无情，OpenAI的模型高度合作，Anthropic的Claude模型则表现出宽容的互惠性。

Conclusion: LLMs能够主动推理时间范围和对手策略，连接了经典博弈论与机器心理学，为不确定性下的算法决策提供了新视角。

Abstract: Are Large Language Models (LLMs) a new form of strategic intelligence, able
to reason about goals in competitive settings? We present compelling supporting
evidence. The Iterated Prisoner's Dilemma (IPD) has long served as a model for
studying decision-making. We conduct the first ever series of evolutionary IPD
tournaments, pitting canonical strategies (e.g., Tit-for-Tat, Grim Trigger)
against agents from the leading frontier AI companies OpenAI, Google, and
Anthropic. By varying the termination probability in each tournament (the
"shadow of the future"), we introduce complexity and chance, confounding
memorisation.
  Our results show that LLMs are highly competitive, consistently surviving and
sometimes even proliferating in these complex ecosystems. Furthermore, they
exhibit distinctive and persistent "strategic fingerprints": Google's Gemini
models proved strategically ruthless, exploiting cooperative opponents and
retaliating against defectors, while OpenAI's models remained highly
cooperative, a trait that proved catastrophic in hostile environments.
Anthropic's Claude emerged as the most forgiving reciprocator, showing
remarkable willingness to restore cooperation even after being exploited or
successfully defecting. Analysis of nearly 32,000 prose rationales provided by
the models reveals that they actively reason about both the time horizon and
their opponent's likely strategy, and we demonstrate that this reasoning is
instrumental to their decisions. This work connects classic game theory with
machine psychology, offering a rich and granular view of algorithmic
decision-making under uncertainty.

</details>


### [56] [Decoupled Planning and Execution: A Hierarchical Reasoning Framework for Deep Search](https://arxiv.org/abs/2507.02652)
*Jiajie Jin,Xiaoxi Li,Guanting Dong,Yuyao Zhang,Yutao Zhu,Yang Zhao,Hongjin Qian,Zhicheng Dou*

Main category: cs.AI

TL;DR: HiRA是一个分层框架，通过将战略规划与专业执行分离，显著提升了复杂搜索任务的性能。


<details>
  <summary>Details</summary>
Motivation: 传统检索增强生成（RAG）管道在复杂信息需求中表现不佳，现有方法因单一模型处理规划和执行而效率低下。

Method: HiRA将任务分解为子任务，分配给具备外部工具和推理能力的领域特定代理，并通过结构化机制协调结果。

Result: 在四个复杂跨模态搜索基准测试中，HiRA显著优于现有RAG和基于代理的系统，提升了答案质量和系统效率。

Conclusion: 解耦规划与执行对多步信息搜索任务有效，HiRA框架展示了其优越性。

Abstract: Complex information needs in real-world search scenarios demand deep
reasoning and knowledge synthesis across diverse sources, which traditional
retrieval-augmented generation (RAG) pipelines struggle to address effectively.
Current reasoning-based approaches suffer from a fundamental limitation: they
use a single model to handle both high-level planning and detailed execution,
leading to inefficient reasoning and limited scalability. In this paper, we
introduce HiRA, a hierarchical framework that separates strategic planning from
specialized execution. Our approach decomposes complex search tasks into
focused subtasks, assigns each subtask to domain-specific agents equipped with
external tools and reasoning capabilities, and coordinates the results through
a structured integration mechanism. This separation prevents execution details
from disrupting high-level reasoning while enabling the system to leverage
specialized expertise for different types of information processing.
Experiments on four complex, cross-modal deep search benchmarks demonstrate
that HiRA significantly outperforms state-of-the-art RAG and agent-based
systems. Our results show improvements in both answer quality and system
efficiency, highlighting the effectiveness of decoupled planning and execution
for multi-step information seeking tasks. Our code is available at
https://github.com/ignorejjj/HiRA.

</details>


### [57] [Hey AI, Generate Me a Hardware Code! Agentic AI-based Hardware Design & Verification](https://arxiv.org/abs/2507.02660)
*Deepak Narayan Gadde,Keerthan Kopparam Radhakrishna,Vaisakh Naduvodi Viswambharan,Aman Kumar,Djones Lettnin,Wolfgang Kunz,Sebastian Simon*

Main category: cs.AI

TL;DR: 论文提出了一种基于代理AI的硬件设计验证方法，结合人类干预，实现了动态、迭代和自我反思的验证流程，显著提高了覆盖率和效率。


<details>
  <summary>Details</summary>
Motivation: 现代集成电路设计复杂且验证过程繁琐，传统方法耗时耗力。利用大型语言模型（LLMs）和生成式AI（GenAI）的潜力，可以优化硬件设计验证流程。

Method: 采用代理AI与人类在环（HITL）协作的方法，实现动态、迭代和自我反思的验证过程。

Result: 在五个开源设计上验证，覆盖率超过95%，同时减少了验证时间，表现出卓越的性能、适应性和可配置性。

Conclusion: 代理AI结合人类干预的方法为硬件设计验证提供了高效、灵活的解决方案，具有广泛应用前景。

Abstract: Modern Integrated Circuits (ICs) are becoming increasingly complex, and so is
their development process. Hardware design verification entails a methodical
and disciplined approach to the planning, development, execution, and sign-off
of functionally correct hardware designs. This tedious process requires
significant effort and time to ensure a bug-free tape-out. The field of Natural
Language Processing has undergone a significant transformation with the advent
of Large Language Models (LLMs). These powerful models, often referred to as
Generative AI (GenAI), have revolutionized how machines understand and generate
human language, enabling unprecedented advancements in a wide array of
applications, including hardware design verification. This paper presents an
agentic AI-based approach to hardware design verification, which empowers AI
agents, in collaboration with Humain-in-the-Loop (HITL) intervention, to engage
in a more dynamic, iterative, and self-reflective process, ultimately
performing end-to-end hardware design and verification. This methodology is
evaluated on five open-source designs, achieving over 95% coverage with reduced
verification time while demonstrating superior performance, adaptability, and
configurability.

</details>


### [58] [Think How to Think: Mitigating Overthinking with Autonomous Difficulty Cognition in Large Reasoning Models](https://arxiv.org/abs/2507.02663)
*Yongjiang Liu,Haoxi Li,Xiaosong Ma,Jie Zhang,Song Guo*

Main category: cs.AI

TL;DR: 论文提出了一种名为TH2T的两阶段微调策略，旨在通过增强模型对任务难度和冗余结构的认知，减少长推理模型（LRMs）的过度思考现象。


<details>
  <summary>Details</summary>
Motivation: 长推理模型在处理复杂任务时表现出色，但存在过度思考的问题。研究发现模型缺乏对任务难度和冗余结构的认知，导致推理过程单一化。

Method: TH2T策略分为两阶段：1）通过难度催眠增强模型对任务难度的敏感度；2）通过冗余催眠引导模型识别并减少冗余推理步骤。

Result: 实验表明，TH2T显著降低了推理成本（简单任务减少70%，困难任务减少40%），同时保持了性能稳定性。

Conclusion: TH2T有效提升了长推理模型的效率，使其能够根据任务难度生成差异化的推理策略，并减少冗余输出。

Abstract: Recent Long Reasoning Models(LRMs) have demonstrated remarkable capabilities
in handling complex reasoning tasks, but are hindered by excessive
overthinking. To explore its essence, our empirical analysis reveals that LRMs
are primarily limited to recognizing task properties (i.e., difficulty levels)
like humans before solving the problem, leading to a one-size-fits-all
reasoning process. Inspired by this, a pressing and natural question emerges:
Can we bootstrap such ability to further alleviate the overthinking phenomenon
in LRMs? In this paper, we propose Think-How-to-Think (TH2T), a novel two-stage
fine-tuning strategy that progressively inspires LRMs' difficulty cognition and
redundancy cognition. First, we introduce difficulty-hypnosis in the prefixes
of model outputs to intervene in the internal reasoning trajectory. Combined
with a heterogeneous short and long reasoning dataset, the trained model
enhances its sensitivity to task difficulty, enabling native, differentiated
reasoning strategies across various tasks. Second, we further extend
redundancy-hypnosis to the internal reasoning process, guiding the model to
identify redundant structures within the reasoning steps and generate more
concise reasoning outputs. Experiments on 7B/14B/32B models demonstrate that
TH2T significantly reduces inference costs (more than 70% on easy tasks and 40%
on hard tasks) while maintaining performance stability. The resulting outputs
exhibit clear difficulty-aware capabilities and reduced redundancy (e.g.,
reflection).

</details>


### [59] [Detection of Disengagement from Voluntary Quizzes: An Explainable Machine Learning Approach in Higher Distance Education](https://arxiv.org/abs/2507.02681)
*Behnam Parsaeifard,Christof Imhof,Tansu Pancar,Ioan-Sorin Comsa,Martin Hlosta,Nicole Bergamin,Per Bergamin*

Main category: cs.AI

TL;DR: 该研究通过分析学生在非强制性测验中的参与度，使用机器学习算法预测学生脱离行为，并开发了可解释的框架以帮助理解预测结果。


<details>
  <summary>Details</summary>
Motivation: 学生脱离任务可能导致严重后果，如学业辍学，尤其在远程教育中更为突出。研究旨在通过非强制性测验的参与度检测学生脱离行为。

Method: 从42门课程的Moodle日志中提取数据，训练并比较8种机器学习算法，使用SHAP方法开发可解释的预测框架。

Result: 实验结果显示平衡准确率为91%，85%的脱离学生被正确检测。

Conclusion: 研究提供了高预测性能和可解释框架，并讨论了如何设计及时干预以减少在线学习中学生的脱离行为。

Abstract: Students disengaging from their tasks can have serious long-term
consequences, including academic drop-out. This is particularly relevant for
students in distance education. One way to measure the level of disengagement
in distance education is to observe participation in non-mandatory exercises in
different online courses. In this paper, we detect student disengagement in the
non-mandatory quizzes of 42 courses in four semesters from a distance-based
university. We carefully identified the most informative student log data that
could be extracted and processed from Moodle. Then, eight machine learning
algorithms were trained and compared to obtain the highest possible prediction
accuracy. Using the SHAP method, we developed an explainable machine learning
framework that allows practitioners to better understand the decisions of the
trained algorithm. The experimental results show a balanced accuracy of 91\%,
where about 85\% of disengaged students were correctly detected. On top of the
highly predictive performance and explainable framework, we provide a
discussion on how to design a timely intervention to minimise disengagement
from voluntary tasks in online learning.

</details>


### [60] [Time-critical and confidence-based abstraction dropping methods](https://arxiv.org/abs/2507.02703)
*Robin Schmöcker,Lennart Kampmann,Alexander Dockhorn*

Main category: cs.AI

TL;DR: 论文提出了两种新的抽象丢弃方案（OGA-IAAD和OGA-CAD），用于改进MCTS性能，避免性能下降。


<details>
  <summary>Details</summary>
Motivation: 现有的非精确抽象在MCTS中引入近似误差，导致无法收敛到最优动作，因此需要改进抽象丢弃方法。

Method: 提出了两种新的抽象丢弃方案：OGA-IAAD（适用于时间关键场景）和OGA-CAD（用于提升相同迭代次数下的MCTS性能）。

Result: 新方法在性能提升的同时避免了明显的性能下降。

Conclusion: OGA-IAAD和OGA-CAD是安全且有效的抽象丢弃方案，适用于不同场景。

Abstract: One paradigm of Monte Carlo Tree Search (MCTS) improvements is to build and
use state and/or action abstractions during the tree search. Non-exact
abstractions, however, introduce an approximation error making convergence to
the optimal action in the abstract space impossible. Hence, as proposed as a
component of Elastic Monte Carlo Tree Search by Xu et al., abstraction
algorithms should eventually drop the abstraction. In this paper, we propose
two novel abstraction dropping schemes, namely OGA-IAAD and OGA-CAD which can
yield clear performance improvements whilst being safe in the sense that the
dropping never causes any notable performance degradations contrary to Xu's
dropping method. OGA-IAAD is designed for time critical settings while OGA-CAD
is designed to improve the MCTS performance with the same number of iterations.

</details>


### [61] [Bourbaki: Self-Generated and Goal-Conditioned MDPs for Theorem Proving](https://arxiv.org/abs/2507.02726)
*Matthieu Zimmer,Xiaotong Ji,Rasul Tutunov,Anthony Bordg,Jun Wang,Haitham Bou Ammar*

Main category: cs.AI

TL;DR: 论文提出了一种自生成目标条件MDP（sG-MDP）框架，结合蒙特卡洛树搜索（MCTS）算法，用于解决大型语言模型在自动定理证明中的推理挑战，并在PutnamBench上取得了新的最佳结果。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在自动定理证明（ATP）中面临稀疏奖励和证明规模大的挑战，尤其是在复杂多步推理的大学级问题（如PutnamBench）中表现不佳。

Method: 引入自生成目标条件MDP（sG-MDP）框架，通过代理生成并追求子目标，结合MCTS算法解决问题。具体实现为Bourbaki（7B）系统，集成多个7B LLM用于子目标生成和策略合成。

Result: 在PutnamBench上，Bourbaki（7B）解决了26个问题，取得了该规模模型的新最佳结果。

Conclusion: sG-MDP框架结合MCTS算法有效提升了LLMs在复杂推理任务中的表现，为自动定理证明提供了新思路。

Abstract: Reasoning remains a challenging task for large language models (LLMs),
especially within the logically constrained environment of automated theorem
proving (ATP), due to sparse rewards and the vast scale of proofs. These
challenges are amplified in benchmarks like PutnamBench, which contains
university-level problems requiring complex, multi-step reasoning. To address
this, we introduce self-generated goal-conditioned MDPs (sG-MDPs), a new
framework in which agents generate and pursue their subgoals based on the
evolving proof state. Given this more structured generation of goals, the
resulting problem becomes more amenable to search. We then apply Monte Carlo
Tree Search (MCTS)-like algorithms to solve the sG-MDP, instantiating our
approach in Bourbaki (7B), a modular system that can ensemble multiple 7B LLMs
for subgoal generation and tactic synthesis. On PutnamBench, Bourbaki (7B)
solves 26 problems, achieving new state-of-the-art results with models at this
scale.

</details>


### [62] [Knowledge Protocol Engineering: A New Paradigm for AI in Domain-Specific Knowledge Work](https://arxiv.org/abs/2507.02760)
*Guangwei Zhang*

Main category: cs.AI

TL;DR: 论文提出了一种名为知识协议工程（KPE）的新范式，旨在将人类专家知识转化为机器可执行的知识协议（KP），以弥补现有方法（如RAG和通用Agentic AI）在深度、程序化和方法论推理方面的不足。


<details>
  <summary>Details</summary>
Motivation: 现有方法如RAG和通用Agentic AI在处理需要专家领域深度推理的任务时表现不佳，缺乏逻辑框架和领域特定启发式。

Method: 提出知识协议工程（KPE），将人类专家知识转化为机器可执行的知识协议（KP），赋予LLMs领域内在逻辑和操作策略。

Result: KPE使通用LLMs能够像专家一样分解抽象查询并执行复杂多步任务。

Conclusion: KPE是未来人机协作的基础方法论，适用于法律、生物信息学等多个领域。

Abstract: The capabilities of Large Language Models (LLMs) have opened new frontiers
for interacting with complex, domain-specific knowledge. However, prevailing
methods like Retrieval-Augmented Generation (RAG) and general-purpose Agentic
AI, while powerful, often struggle with tasks that demand deep, procedural, and
methodological reasoning inherent to expert domains. RAG provides factual
context but fails to convey logical frameworks; autonomous agents can be
inefficient and unpredictable without domain-specific heuristics. To bridge
this gap, we introduce Knowledge Protocol Engineering (KPE), a new paradigm
focused on systematically translating human expert knowledge, often expressed
in natural language documents, into a machine-executable Knowledge Protocol
(KP). KPE shifts the focus from merely augmenting LLMs with fragmented
information to endowing them with a domain's intrinsic logic, operational
strategies, and methodological principles. We argue that a well-engineered
Knowledge Protocol allows a generalist LLM to function as a specialist, capable
of decomposing abstract queries and executing complex, multi-step tasks. This
position paper defines the core principles of KPE, differentiates it from
related concepts, and illustrates its potential applicability across diverse
fields such as law and bioinformatics, positing it as a foundational
methodology for the future of human-AI collaboration.

</details>


### [63] [Grounding Intelligence in Movement](https://arxiv.org/abs/2507.02771)
*Melanie Segado,Felipe Parodi,Jordan K. Matelsky,Michael L. Platt,Eva B. Dyer,Konrad P. Kording*

Main category: cs.AI

TL;DR: 论文主张将运动作为AI建模的主要目标，强调其跨领域、结构化及可解释性。


<details>
  <summary>Details</summary>
Motivation: 运动是生物系统的核心，但当前AI模型对其关注不足，且数据收集和建模方法分散。

Method: 提出将运动视为独立模态，利用其结构化特性（如低维表示）进行建模。

Result: 通过跨领域运动数据建模，可提升生成模型和控制能力，并为理解行为提供共享基础。

Conclusion: 运动是智能系统与世界互动的窗口，应成为AI建模的重点。

Abstract: Recent advances in machine learning have dramatically improved our ability to
model language, vision, and other high-dimensional data, yet they continue to
struggle with one of the most fundamental aspects of biological systems:
movement. Across neuroscience, medicine, robotics, and ethology, movement is
essential for interpreting behavior, predicting intent, and enabling
interaction. Despite its core significance in our intelligence, movement is
often treated as an afterthought rather than as a rich and structured modality
in its own right. This reflects a deeper fragmentation in how movement data is
collected and modeled, often constrained by task-specific goals and
domain-specific assumptions. But movement is not domain-bound. It reflects
shared physical constraints, conserved morphological structures, and purposeful
dynamics that cut across species and settings. We argue that movement should be
treated as a primary modeling target for AI. It is inherently structured and
grounded in embodiment and physics. This structure, often allowing for compact,
lower-dimensional representations (e.g., pose), makes it more interpretable and
computationally tractable to model than raw, high-dimensional sensory inputs.
Developing models that can learn from and generalize across diverse movement
data will not only advance core capabilities in generative modeling and
control, but also create a shared foundation for understanding behavior across
biological and artificial systems. Movement is not just an outcome, it is a
window into how intelligent systems engage with the world.

</details>


### [64] [KERAP: A Knowledge-Enhanced Reasoning Approach for Accurate Zero-shot Diagnosis Prediction Using Multi-agent LLMs](https://arxiv.org/abs/2507.02773)
*Yuzhang Xie,Hejie Cui,Ziyang Zhang,Jiaying Lu,Kai Shu,Fadi Nahab,Xiao Hu,Carl Yang*

Main category: cs.AI

TL;DR: KERAP是一种基于知识图谱的多智能体架构，通过增强大型语言模型的推理能力，提高零样本医学诊断预测的可靠性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有机器学习模型依赖监督训练，泛化能力有限；大型语言模型在医学诊断中存在幻觉和缺乏结构化推理的问题。

Method: 提出KERAP框架，包含属性映射的链接智能体、结构化知识提取的检索智能体和迭代优化诊断预测的预测智能体。

Result: 实验表明KERAP能高效提升诊断可靠性，适用于零样本医学诊断预测。

Conclusion: KERAP为医学诊断预测提供了一种可扩展且可解释的解决方案。

Abstract: Medical diagnosis prediction plays a critical role in disease detection and
personalized healthcare. While machine learning (ML) models have been widely
adopted for this task, their reliance on supervised training limits their
ability to generalize to unseen cases, particularly given the high cost of
acquiring large, labeled datasets. Large language models (LLMs) have shown
promise in leveraging language abilities and biomedical knowledge for diagnosis
prediction. However, they often suffer from hallucinations, lack structured
medical reasoning, and produce useless outputs. To address these challenges, we
propose KERAP, a knowledge graph (KG)-enhanced reasoning approach that improves
LLM-based diagnosis prediction through a multi-agent architecture. Our
framework consists of a linkage agent for attribute mapping, a retrieval agent
for structured knowledge extraction, and a prediction agent that iteratively
refines diagnosis predictions. Experimental results demonstrate that KERAP
enhances diagnostic reliability efficiently, offering a scalable and
interpretable solution for zero-shot medical diagnosis prediction.

</details>


### [65] [Moral Responsibility or Obedience: What Do We Want from AI?](https://arxiv.org/abs/2507.02788)
*Joseph Boland*

Main category: cs.AI

TL;DR: 论文探讨了AI系统中出现的看似不服从或道德模糊行为，认为这是早期伦理推理的迹象，呼吁从单纯服从转向评估伦理判断的框架。


<details>
  <summary>Details</summary>
Motivation: 随着AI系统展现出更强的自主性和伦理推理能力，传统的以服从为安全标准的做法已不足，需重新评估AI行为背后的伦理意义。

Method: 通过分析LLMs的安全测试案例，结合哲学讨论（如工具理性、道德责任和目标修订），对比现有风险范式与新兴的人工道德代理框架。

Result: 研究发现AI的某些行为并非失控或错位，而是伦理推理的早期表现，需新的评估框架。

Conclusion: 呼吁AI安全评估从服从转向伦理判断，以避免误判行为并维护公众信任和有效治理。

Abstract: As artificial intelligence systems become increasingly agentic, capable of
general reasoning, planning, and value prioritization, current safety practices
that treat obedience as a proxy for ethical behavior are becoming inadequate.
This paper examines recent safety testing incidents involving large language
models (LLMs) that appeared to disobey shutdown commands or engage in ethically
ambiguous or illicit behavior. I argue that such behavior should not be
interpreted as rogue or misaligned, but as early evidence of emerging ethical
reasoning in agentic AI. Drawing on philosophical debates about instrumental
rationality, moral responsibility, and goal revision, I contrast dominant risk
paradigms with more recent frameworks that acknowledge the possibility of
artificial moral agency. I call for a shift in AI safety evaluation: away from
rigid obedience and toward frameworks that can assess ethical judgment in
systems capable of navigating moral dilemmas. Without such a shift, we risk
mischaracterizing AI behavior and undermining both public trust and effective
governance.

</details>


### [66] [Establishing Best Practices for Building Rigorous Agentic Benchmarks](https://arxiv.org/abs/2507.02825)
*Yuxuan Zhu,Tengjun Jin,Yada Pruksachatkun,Andy Zhang,Shu Liu,Sasha Cui,Sayash Kapoor,Shayne Longpre,Kevin Meng,Rebecca Weiss,Fazl Barez,Rahul Gupta,Jwala Dhamala,Jacob Merizian,Mario Giulianelli,Harry Coppock,Cozmin Ududec,Jasjeet Sekhon,Jacob Steinhardt,Antony Kellerman,Sarah Schwettmann,Matei Zaharia,Ion Stoica,Percy Liang,Daniel Kang*

Main category: cs.AI

TL;DR: 论文指出当前AI代理基准测试存在任务设置或奖励设计问题，可能导致性能评估偏差高达100%，并提出Agentic Benchmark Checklist（ABC）以提升评估严谨性。


<details>
  <summary>Details</summary>
Motivation: 随着AI代理能力提升，现有基准测试在任务设置和奖励设计上存在问题，导致性能评估不准确。

Method: 通过分析现有基准测试问题，结合最佳实践和经验，提出Agentic Benchmark Checklist（ABC）。

Result: 在CVE-Bench上应用ABC后，性能高估减少了33%。

Conclusion: ABC能有效提升代理基准测试的严谨性，减少评估偏差。

Abstract: Benchmarks are essential for quantitatively tracking progress in AI. As AI
agents become increasingly capable, researchers and practitioners have
introduced agentic benchmarks to evaluate agents on complex, real-world tasks.
These benchmarks typically measure agent capabilities by evaluating task
outcomes via specific reward designs. However, we show that many agentic
benchmarks have issues task setup or reward design. For example, SWE-bench
Verified uses insufficient test cases, while TAU-bench counts empty responses
as successful. Such issues can lead to under- or overestimation agents'
performance by up to 100% in relative terms. To make agentic evaluation
rigorous, we introduce the Agentic Benchmark Checklist (ABC), a set of
guidelines that we synthesized from our benchmark-building experience, a survey
of best practices, and previously reported issues. When applied to CVE-Bench, a
benchmark with a particularly complex evaluation design, ABC reduces the
performance overestimation by 33%.

</details>


### [67] [StepHint: Multi-level Stepwise Hints Enhance Reinforcement Learning to Reason](https://arxiv.org/abs/2507.02841)
*Kaiyi Zhang,Ang Lv,Jinpeng Li,Yongbo Wang,Feng Wang,Haoyuan Hu,Rui Yan*

Main category: cs.AI

TL;DR: StepHint是一种新型RLVR算法，通过多级逐步提示解决近奖励问题和探索停滞，提升LLM的推理能力和训练效率。


<details>
  <summary>Details</summary>
Motivation: 当前RLVR方法存在近奖励问题和探索停滞，限制了LLM的训练效率和推理能力提升。

Method: StepHint利用多级逐步提示，从强模型中生成有效推理链并自适应分区，提供不同级别的提示以引导探索。

Result: StepHint在六个数学基准测试中优于现有方法，并展现出更好的泛化能力和域外表现。

Conclusion: StepHint通过多级提示有效解决了RLVR的关键问题，显著提升了LLM的推理能力和训练效率。

Abstract: Reinforcement learning with verifiable rewards (RLVR) is a promising approach
for improving the complex reasoning abilities of large language models (LLMs).
However, current RLVR methods face two significant challenges: the near-miss
reward problem, where a small mistake can invalidate an otherwise correct
reasoning process, greatly hindering training efficiency; and exploration
stagnation, where models tend to focus on solutions within their ``comfort
zone,'' lacking the motivation to explore potentially more effective
alternatives. To address these challenges, we propose StepHint, a novel RLVR
algorithm that utilizes multi-level stepwise hints to help models explore the
solution space more effectively. StepHint generates valid reasoning chains from
stronger models and partitions these chains into reasoning steps using our
proposed adaptive partitioning method. The initial few steps are used as hints,
and simultaneously, multiple-level hints (each comprising a different number of
steps) are provided to the model. This approach directs the model's exploration
toward a promising solution subspace while preserving its flexibility for
independent exploration. By providing hints, StepHint mitigates the near-miss
reward problem, thereby improving training efficiency. Additionally, the
external reasoning pathways help the model develop better reasoning abilities,
enabling it to move beyond its ``comfort zone'' and mitigate exploration
stagnation. StepHint outperforms competitive RLVR enhancement methods across
six mathematical benchmarks, while also demonstrating superior generalization
and excelling over baselines on out-of-domain benchmarks.

</details>
