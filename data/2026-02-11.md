<div id=toc></div>

# Table of Contents

- [cs.CR](#cs.CR) [Total: 20]
- [cs.AI](#cs.AI) [Total: 84]
- [cs.SE](#cs.SE) [Total: 28]


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [1] [Concept-Aware Privacy Mechanisms for Defending Embedding Inversion Attacks](https://arxiv.org/abs/2602.07090)
*Yu-Che Tsai,Hsiang Hsiao,Kuan-Yu Chen,Shou-De Lin*

Main category: cs.CR

TL;DR: SPARSE框架通过可微分掩码学习和椭圆噪声注入，为文本嵌入提供概念特定的隐私保护，在降低隐私泄露的同时保持更好的下游任务性能。


<details>
  <summary>Details</summary>
Motivation: 文本嵌入面临严重的隐私风险，现有的差分隐私防御方法假设所有维度具有均匀敏感性，导致添加过多噪声并降低实用性。

Method: SPARSE框架结合：(1) 可微分掩码学习来识别用户定义概念的隐私敏感维度；(2) 马哈拉诺比斯机制，根据维度敏感性应用椭圆噪声校准。

Result: 在六个数据集、三种嵌入模型和多种攻击场景下的评估显示，SPARSE能持续减少隐私泄露，同时相比最先进的差分隐私方法获得更优的下游性能。

Conclusion: SPARSE通过选择性扰动隐私敏感维度而非均匀添加噪声，实现了概念特定的隐私保护，在隐私保护和实用性之间取得了更好的平衡。

Abstract: Text embeddings enable numerous NLP applications but face severe privacy risks from embedding inversion attacks, which can expose sensitive attributes or reconstruct raw text. Existing differential privacy defenses assume uniform sensitivity across embedding dimensions, leading to excessive noise and degraded utility. We propose SPARSE, a user-centric framework for concept-specific privacy protection in text embeddings. SPARSE combines (1) differentiable mask learning to identify privacy-sensitive dimensions for user-defined concepts, and (2) the Mahalanobis mechanism that applies elliptical noise calibrated by dimension sensitivity. Unlike traditional spherical noise injection, SPARSE selectively perturbs privacy-sensitive dimensions while preserving non-sensitive semantics. Evaluated across six datasets with three embedding models and attack scenarios, SPARSE consistently reduces privacy leakage while achieving superior downstream performance compared to state-of-the-art DP methods.

</details>


### [2] [ShallowJail: Steering Jailbreaks against Large Language Models](https://arxiv.org/abs/2602.07107)
*Shang Liu,Hanyu Pei,Zeyan Liu*

Main category: cs.CR

TL;DR: ShallowJail是一种新型的LLM越狱攻击方法，通过操纵推理过程中的初始令牌来误导对齐模型产生有害输出，相比现有方法更高效且隐蔽。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型已经进行了对齐训练以防止有害用途，但它们仍然容易受到越狱攻击。现有的越狱方法要么是黑盒攻击（使用精心设计但不够隐蔽的提示），要么是白盒攻击（需要大量计算资源），因此需要一种更高效且隐蔽的攻击方法。

Method: ShallowJail通过利用LLMs的浅层对齐特性，在推理过程中操纵初始令牌来误导模型的响应。这种方法不需要复杂的提示工程或大量计算资源。

Result: 通过大量实验证明，ShallowJail能够显著降低最先进LLM的安全性，使其产生有害输出，证明了该方法的有效性。

Conclusion: ShallowJail作为一种新型的越狱攻击方法，揭示了LLMs浅层对齐的脆弱性，为LLM安全研究提供了新的视角和挑战。

Abstract: Large Language Models(LLMs) have been successful in numerous fields. Alignment has usually been applied to prevent them from harmful purposes. However, aligned LLMs remain vulnerable to jailbreak attacks that deliberately mislead them into producing harmful outputs. Existing jailbreaks are either black-box, using carefully crafted, unstealthy prompts, or white-box, requiring resource-intensive computation. In light of these challenges, we introduce ShallowJail, a novel attack that exploits shallow alignment in LLMs. ShallowJail can misguide LLMs' responses by manipulating the initial tokens during inference. Through extensive experiments, we demonstrate the effectiveness of~\shallow, which substantially degrades the safety of state-of-the-art LLM responses.

</details>


### [3] [Secure Code Generation via Online Reinforcement Learning with Vulnerability Reward Model](https://arxiv.org/abs/2602.07422)
*Tianyi Wu,Mingzhe Du,Yue Liu,Chengran Yang,Terry Yue Zhuo,Jiaheng Zhang,See-Kiong Ng*

Main category: cs.CR

TL;DR: SecCoderX是一个基于在线强化学习的框架，用于生成功能保持的安全代码，解决了现有方法在安全性和功能性之间的权衡问题。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在软件开发中应用日益广泛，但其生成不安全代码的倾向阻碍了实际部署。现有安全代码对齐方法存在功能-安全悖论，即提高安全性往往以牺牲功能性为代价。

Method: SecCoderX采用在线强化学习框架，通过两种方式桥接漏洞检测和安全代码生成：1) 利用成熟的检测资源合成多样化的现实漏洞诱导编码任务用于在线RL；2) 训练基于推理的漏洞奖励模型提供可扩展且可靠的安全监督。这些组件在在线RL循环中统一，以对齐代码LLMs生成安全且功能性的代码。

Result: 实验表明SecCoderX达到最先进性能，将有效安全率(ESR)比未对齐模型提高约10%，而先前方法通常使ESR降低14-54%。

Conclusion: SecCoderX成功解决了功能-安全权衡问题，实现了功能保持的安全代码生成，为LLMs在软件开发中的安全部署提供了有效解决方案。

Abstract: Large language models (LLMs) are increasingly used in software development, yet their tendency to generate insecure code remains a major barrier to real-world deployment. Existing secure code alignment methods often suffer from a functionality--security paradox, improving security at the cost of substantial utility degradation. We propose SecCoderX, an online reinforcement learning framework for functionality-preserving secure code generation. SecCoderX first bridges vulnerability detection and secure code generation by repurposing mature detection resources in two ways: (i) synthesizing diverse, reality-grounded vulnerability-inducing coding tasks for online RL rollouts, and (ii) training a reasoning-based vulnerability reward model that provides scalable and reliable security supervision. Together, these components are unified in an online RL loop to align code LLMs to generate secure and functional code. Extensive experiments demonstrate that SecCoderX achieves state-of-the-art performance, improving Effective Safety Rate (ESR) by approximately 10% over unaligned models, whereas prior methods often degrade ESR by 14-54%. We release our code, dataset and model checkpoints at https://github.com/AndrewWTY/SecCoderX.

</details>


### [4] [SPECA: Specification-to-Checklist Agentic Auditing for Multi-Implementation Systems -- A Case Study on Ethereum Clients](https://arxiv.org/abs/2602.07513)
*Masato Kamba,Akiyoshi Sannai*

Main category: cs.CR

TL;DR: SPECA框架将规范要求转化为检查清单，支持多实现系统的审计，在以太坊Fusaka升级审计中验证了其有效性，智能体审计表现优于大多数人类审计者。


<details>
  <summary>Details</summary>
Motivation: 多实现系统审计面临挑战：差异测试在实现一致但错误时效果有限，需要更有效的审计方法来处理模糊要求和跨实现重用。

Method: 提出SPECA框架：将规范要求转化为检查清单，映射到实现位置，支持跨实现重用。在以太坊Fusaka升级的11个生产客户端审计中实例化该框架。

Result: 在54份提交中，17份有效，其中76.5%来自跨实现检查。改进后的智能体在关键漏洞上的严格召回率达到27.3%，表现优于49/51的人类审计者，平均40分钟完成专家验证和提交。

Conclusion: 检查清单驱动的跨实现重用是有效的审计扩展机制，早期明确威胁建模对减少误报至关重要，智能体审计在竞争性审计中表现出色，具备实用价值。

Abstract: Multi-implementation systems are increasingly audited against natural-language specifications. Differential testing scales well when implementations disagree, but it provides little signal when all implementations converge on the same incorrect interpretation of an ambiguous requirement. We present SPECA, a Specification-to-Checklist Auditing framework that turns normative requirements into checklists, maps them to implementation locations, and supports cross-implementation reuse.
  We instantiate SPECA in an in-the-wild security audit contest for the Ethereum Fusaka upgrade, covering 11 production clients. Across 54 submissions, 17 were judged valid by the contest organizers. Cross-implementation checks account for 76.5 percent (13 of 17) of valid findings, suggesting that checklist-derived one-to-many reuse is a practical scaling mechanism in multi-implementation audits. To understand false positives, we manually coded the 37 invalid submissions and find that threat model misalignment explains 56.8 percent (21 of 37): reports that rely on assumptions about trust boundaries or scope that contradict the audit's rules. We detected no High or Medium findings in the V1 deployment; misses concentrated in specification details and implicit assumptions (57.1 percent), timing and concurrency issues (28.6 percent), and external library dependencies (14.3 percent). Our improved agent, evaluated against the ground truth of a competitive audit, achieved a strict recall of 27.3 percent on high-impact vulnerabilities, placing it in the top 4 percent of human auditors and outperforming 49 of 51 contestants on critical issues. These results, though from a single deployment, suggest that early, explicit threat modeling is essential for reducing false positives and focusing agentic auditing effort. The agent-driven process enables expert validation and submission in about 40 minutes on average.

</details>


### [5] [MemPot: Defending Against Memory Extraction Attack with Optimized Honeypots](https://arxiv.org/abs/2602.07517)
*Yuhao Wang,Shengfang Zhai,Guanghao Jin,Yinpeng Dong,Linyi Yang,Jiaheng Zhang*

Main category: cs.CR

TL;DR: MemPot是一个针对LLM智能体内存提取攻击的防御框架，通过注入优化的蜜罐文档来检测攻击，在保持零额外在线推理延迟的同时显著提升检测性能。


<details>
  <summary>Details</summary>
Motivation: LLM智能体使用外部和内部内存系统处理复杂任务，但这使其面临严重的内存提取攻击，而目前缺乏有效的防御机制。

Method: 提出MemPot防御框架，通过两阶段优化过程生成蜜罐文档：最大化攻击者检索概率，同时保持对良性用户的隐蔽性。使用Wald的序贯概率比检验(SPRT)建模检测过程。

Result: MemPot在检测AUROC上比现有方法提升50%，在低误报率约束下真阳性率提升80%。理论证明比最优静态检测器需要更少的采样轮次，且保持零额外在线推理延迟。

Conclusion: MemPot是首个经过理论验证的防御框架，在安全性、无害性和效率方面具有优越性，能有效保护LLM智能体免受内存提取攻击。

Abstract: Large Language Model (LLM)-based agents employ external and internal memory systems to handle complex, goal-oriented tasks, yet this exposes them to severe extraction attacks, and effective defenses remain lacking. In this paper, we propose MemPot, the first theoretically verified defense framework against memory extraction attacks by injecting optimized honeypots into the memory. Through a two-stage optimization process, MemPot generates trap documents that maximize the retrieval probability for attackers while remaining inconspicuous to benign users. We model the detection process as Wald's Sequential Probability Ratio Test (SPRT) and theoretically prove that MemPot achieves a lower average number of sampling rounds compared to optimal static detectors. Empirically, MemPot significantly outperforms state-of-the-art baselines, achieving a 50% improvement in detection AUROC and an 80% increase in True Positive Rate under low False Positive Rate constraints. Furthermore, our experiments confirm that MemPot incurs zero additional online inference latency and preserves the agent's utility on standard tasks, verifying its superiority in safety, harmlessness, and efficiency.

</details>


### [6] [SoK: Credential-Based Trust Management in Decentralized Ledger Systems](https://arxiv.org/abs/2602.07572)
*Yanna Jiang,Haiyu Deng,Qin Wang,Guangsheng Yu,Xu Wang,Yilin Sai,Shiping Chen,Wei Ni,Ren Ping Liu*

Main category: cs.CR

TL;DR: 本文对基于凭证的分布式信任管理系统进行了系统性综述，分析了架构设计、凭证机制和信任评估模型，建立了分类体系和评估标准，识别了关键挑战和研究方向。


<details>
  <summary>Details</summary>
Motivation: 随着去中心化系统和区块链技术的发展，基于凭证的分布式信任管理系统变得越来越重要。本文旨在弥合理论与实践之间的差距，为研究人员和从业者提供系统性的综述和分析。

Method: 采用系统性文献综述方法，从多个维度分析现有DTMS解决方案，包括架构设计、凭证机制和信任评估模型，建立详细的分类体系和综合评估标准。

Result: 建立了基于凭证的DTMS方法的详细分类体系，制定了全面的评估标准，识别了当前系统中的关键挑战和未来有前景的研究方向。

Conclusion: 本文为DTMS领域的研究人员和从业者提供了有价值的见解，特别是在访问控制、声誉系统和基于区块链的信任框架等领域，有助于推动该领域的理论和实践发展。

Abstract: Trust management systems (TMS) are crucial for managing trust in distributed environments. The rise of decentralized systems and blockchain has sparked interest in credential-based decentralized trust management systems (DTMS). This paper bridges the gap between theory and practice through a systematic review of credential-based DTMS. We analyze existing DTMS solutions through multiple dimensions, including their architectural designs, credential mechanisms, and trust evaluation models. Our survey provides a detailed taxonomy of credential-based DTMS approaches and establishes comprehensive evaluation criteria for assessing DTMS implementations. Through extensive analysis of current systems and implementations, we identify critical challenges and promising research directions in the field. Our examination offers valuable insights for researchers and practitioners working on DTMS, particularly in areas such as access control, reputation systems, and blockchain-based trust frameworks.

</details>


### [7] [AirCatch: Effectively tracing advanced tag-based trackers](https://arxiv.org/abs/2602.07656)
*Abhishek Kumar Mishra,Swadeep,Guevara Noubir,Mathieu Cunche*

Main category: cs.CR

TL;DR: AirCatch是一个被动检测系统，利用物理层约束检测恶意蓝牙跟踪器，即使它们快速轮换标识符也能有效识别。


<details>
  <summary>Details</summary>
Motivation: 现有的基于协议的防御和学术解决方案主要假设稳定的标识符或可预测的信标，但面对积极轮换标识符的高级恶意跟踪器时，这些基于标识符的防御会失效。

Method: AirCatch采用三个关键技术：1)新颖的调制感知CFO指纹，增强设备独特性；2)基于高核心密度和持久性的跟踪检测算法；3)超低成本接收器BlePhasyr，使用约10美元的BLE SDR。

Result: 在Apple、Google、Tile和三星标签家族的多小时捕获中评估，AirCatch在各种对抗配置和环境下实现零误报和早期检测，仅在极端低速率情况下性能下降。

Conclusion: AirCatch通过利用物理层约束，即使面对积极轮换标识符的恶意跟踪器也能有效检测，为资源受限部署提供了实用的RF指纹检测方案。

Abstract: Tag-based tracking ecosystems help users locate lost items, but can be leveraged for unwanted tracking and stalking. Existing protocol-driven defenses and prior academic solutions largely assume stable identifiers or predictable beaconing. However, identifier-based defenses fundamentally break down against advanced rogue trackers that aggressively rotate identifiers. We present AirCatch, a passive detection system that exploits a physical-layer constraint: while logical identifiers can change arbitrarily fast, the transmitter's analog imprint remains stable and reappears as a compact and persistently occupied region in Carrier Frequency Offset (CFO) feature space. AirCatch advances the state of the art along three axes: (i) a novel, modulation-aware CFO fingerprint that augments packet-level CFO with content-independent CFO components that amplify device distinctiveness; (ii) a new tracking detection algorithm based on high core density and persistence that is robust to contamination and evasion through per-identifier segmentation; and (iii) an ultra-low-cost receiver, an approximately 10 dollar BLE SDR named BlePhasyr, built from commodity components, that makes RF fingerprinting based detection practical in resource-constrained deployments. We evaluate AirCatch across Apple, Google, Tile, and Samsung tag families in multi-hour captures, systematically stress-test evasion using a scenario generator over a grid of transmission and rotation periods, and validate in diverse real-world mobility traces including home and office commutes, public transport, car travel, and airport journeys while sweeping background tag density. Across these stress tests, AirCatch achieves no false positives and early detection over a wide range of adversarial configurations and environments, degrading gracefully only in extreme low-rate regimes that also reduce attacker utility.

</details>


### [8] [IPBAC: Interaction Provenance-Based Access Control for Secure and Privacy-Aware Systems](https://arxiv.org/abs/2602.07722)
*Sharif Noor Zisad,Ragib Hasan*

Main category: cs.CR

TL;DR: IPBAC模型通过整合交互溯源与访问控制，解决了传统访问控制系统在角色定义、动态场景处理和审计追踪方面的局限性。


<details>
  <summary>Details</summary>
Motivation: 传统访问控制系统（如RBAC）存在角色定义不灵活、难以处理动态场景、缺乏详细问责和可追溯性等显著限制，需要更强大的安全保护、审计追踪和自适应策略支持。

Method: 提出基于交互溯源的访问控制（IPBAC）模型，通过详细记录系统中的操作和交互，捕获包括执行者身份、操作时间、上下文等全面元数据，将交互溯源与访问控制机制整合。

Result: IPBAC能够更有效地防止未经授权的访问，增强审计和合规的可追溯性，支持自适应安全策略，提供比传统访问控制系统更强的安全保护和审计框架。

Conclusion: 基于溯源的访问控制不仅增强了安全性，还为审计和合规提供了稳健的框架，是解决传统访问控制系统局限性的有效方法。

Abstract: Traditional access control systems, including RBAC, face significant limitations such as inflexible role definitions, difficulty handling dynamic scenarios, and lack of detailed accountability and traceability. To this end, we introduce the Interaction Provenance-based Access Control (IPBAC) model. In this paper, we explore the integration of interaction provenance with access control to overcome these limitations. Interaction provenance refers to the detailed recording of actions and interactions within a system, capturing comprehensive metadata such as the identity of the actor, the time of an action, and the context. IPBAC ensures stronger protection against unauthorized access, enhances traceability for auditing and compliance, and supports adaptive security policies. This provenance-based access control not only strengthens security, but also provides a robust framework for auditing and compliance.

</details>


### [9] [Leveraging the Power of Ensemble Learning for Secure Low Altitude Economy](https://arxiv.org/abs/2602.07725)
*Yaoqi Yang,Yong Chen,Jiacheng Wang,Geng Sun,Dusit Niyato,Zhu Han*

Main category: cs.CR

TL;DR: 本文探讨了在低空经济中应用集成学习来防御恶意飞机入侵攻击，通过结合多个模型的优势提高入侵检测系统的准确性、适应性和资源利用率。


<details>
  <summary>Details</summary>
Motivation: 低空经济面临恶意飞机入侵的安全威胁，而现有的入侵检测系统由于异构数据、动态环境和资源受限设备等因素，在检测准确性、适应性和资源利用率方面存在挑战。

Method: 采用集成学习方法，利用多个模型的集体知识来增强入侵检测系统。论文建立了集成学习的理论基础，回顾了相关研究领域和潜在解决方案，并提出了一个集成学习支持的恶意飞机追踪框架。

Result: 通过设计的案例研究验证了集成学习框架在低空经济中防御恶意飞机入侵的可行性和有效性，展示了其在提高检测准确性、适应性和资源利用率方面的优势。

Conclusion: 集成学习能够显著提升低空经济的安全性，论文最后展望了未来研究方向，以进一步推进集成学习在安全低空经济中的应用。

Abstract: Low Altitude Economy (LAE) holds immense promise for enhancing societal well-being and driving economic growth. However, this burgeoning field is vulnerable to security threats, particularly malicious aircraft intrusion attacks. To address the above concerns, intrusion detection systems (IDS) can be used to defend against malicious aircraft intrusions in LAE. Whereas, due to the heterogeneous data, dynamic environment, and resource-constrained devices within LAE, current IDS face challenges in detection accuracy, adaptability, and resource utilization ratio. In this regard, due to the inherent ability to combine the strengths of multiple models, ensemble learning can realize more robust and diverse anomaly detection further enhance IDS accuracy, thereby improving robustness and efficiency of the secure LAE. Unlike single-model approaches, ensemble learning can leverage the collective knowledge of its constituent models to effectively defend the malicious aircraft intrusion attacks. Specifically, this paper investigates ensemble learning for secure LAE, covering research focuses, solutions, and a case study. We first establish the rationale for ensemble learning and then review research areas and potential solutions, demonstrating the necessities and benefits of applying ensemble learning to secure LAE. Subsequently, we propose a framework of ensemble learning-enabled malicious aircrafts tracking in the secure LAE, where its feasibility and effectiveness are evaluated by the designed case study. Finally, we conclude by outlining promising future research directions for further advancing the ensemble learning-enabled secure LAE.

</details>


### [10] [Rethinking Latency Denial-of-Service: Attacking the LLM Serving Framework, Not the Model](https://arxiv.org/abs/2602.07878)
*Tianyi Wang,Huawei Fan,Yuanchao Shu,Peng Cheng,Cong Wang*

Main category: cs.CR

TL;DR: 论文揭示传统算法延迟攻击对现代LLM服务系统无效，提出针对调度器状态转换的"填充与挤压"系统层攻击策略，能以更低成本造成显著延迟


<details>
  <summary>Details</summary>
Motivation: LLM推理成本高昂，轻微延迟就会导致巨大运营成本和可用性风险。现有研究主要关注算法复杂度攻击，但作者发现这些攻击对现代LLM服务系统基本无效，因此转向系统层攻击

Method: 提出"填充与挤压"攻击策略：1) "填充"阶段耗尽全局KV缓存引发队头阻塞；2) "挤压"阶段迫使系统进入重复抢占状态。结合简单文本提示到复杂提示工程，并利用内存状态侧信道探测，在无需了解系统内部细节的黑盒设置下实施攻击

Result: 攻击效果显著：相比现有攻击，首令牌时间平均减慢20-280倍，每输出令牌时间平均减慢1.5-4倍，同时攻击成本降低30-40%

Conclusion: 系统层攻击比算法复杂度攻击更有效，现代LLM服务系统在调度优化方面存在安全漏洞，需要新的防御机制来应对这类针对调度器状态转换的攻击

Abstract: Large Language Models face an emerging and critical threat known as latency attacks. Because LLM inference is inherently expensive, even modest slowdowns can translate into substantial operating costs and severe availability risks. Recently, a growing body of research has focused on algorithmic complexity attacks by crafting inputs to trigger worst-case output lengths. However, we report a counter-intuitive finding that these algorithmic latency attacks are largely ineffective against modern LLM serving systems. We reveal that system-level optimization such as continuous batching provides a logical isolation to mitigate contagious latency impact on co-located users. To this end, in this paper, we shift the focus from the algorithm to the system layer, and introduce a new Fill and Squeeze attack strategy targeting the state transition of the scheduler. "Fill" first exhausts the global KV cache to induce Head-of-Line blocking, while "Squeeze" forces the system into repetitive preemption. By manipulating output lengths using methods from simple plain-text prompts to more complex prompt engineering, and leveraging side-channel probing of memory status, we demonstrate that the attack can be orchestrated in a black-box setting with much less cost. Extensive evaluations indicate by up to 20-280x average slowdown on Time to First Token and 1.5-4x average slowdown on Time Per Output Token compared to existing attacks with 30-40% lower attack cost.

</details>


### [11] [Privacy-Preserving Covert Communication Using Encrypted Wearable Gesture Recognition](https://arxiv.org/abs/2602.07936)
*Tasnia Ashrafi Heya,Sayed Erfan Arefin*

Main category: cs.CR

TL;DR: 本文提出了一种隐私保护的基于手势的隐蔽通信系统，使用同态加密技术直接在加密的运动数据上进行手势识别，防止第三方获取原始传感器信号、学习特征或分类输出。


<details>
  <summary>Details</summary>
Motivation: 在隐蔽和安全关键场景中，安全通信至关重要，因为口头交互可能暴露用户意图或操作上下文。现有的可穿戴手势通信系统会泄露运动数据、中间表示或推理输出给不可信基础设施，导致意图推断、行为生物特征泄露和内部攻击。

Method: 系统采用多方同态学习管道，直接在加密的运动数据上进行手势识别，防止对手推断手势语义、重放传感器轨迹或访问中间表示。设计了触觉和视觉反馈机制用于隐蔽信号传递。

Result: 使用商用智能手表的600个手势样本进行评估，实现了超过94.44%的分类准确率，证明了系统的可行性，并展示了从高性能系统到资源受限边缘设备的实际部署能力。

Conclusion: 这是首个在可穿戴隐蔽通信场景中应用加密手势识别的工作，通过同态加密技术确保原始传感器信号、学习特征和分类输出都不会暴露给任何第三方，实现了隐私保护的隐蔽通信。

Abstract: Secure communication is essential in covert and safety-critical settings where verbal interactions may expose user intent or operational context. Wearable gesture-based communication enables low-effort, nonverbal interaction, but existing systems leak motion data, intermediate representations, or inference outputs to untrusted infrastructure, enabling intent inference, behavioral biometric leakage, and insider attacks. This work proposes a privacy-preserving gesture-based covert communication system that ensures, no raw sensor signals, learned features, or classification outputs are exposed to any third-party. The system employs a multi-party homomorphic learning pipeline for gesture recognition directly over encrypted motion data, preventing adversaries from inferring gesture semantics, replaying sensor traces, or accessing intermediate representations. To our knowledge, this work is the first to apply encrypted gesture recognition in a wearable-based covert communication setting. We design and evaluate haptic and visual feedback mechanisms for covert signal delivery and evaluate the system using 600 gesture samples from a commodity smartwatch, achieving over 94.44% classification accuracy and demonstrating the feasibility of the proposed system with practical deployability from high-performance systems to resource-constrained edge devices.

</details>


### [12] [IssueGuard: Real-Time Secret Leak Prevention Tool for GitHub Issue Reports](https://arxiv.org/abs/2602.08072)
*Md Nafiu Rahman,Sadif Ahmed,Zahin Wahab,Gias Uddin,Rifat Shahriyar*

Main category: cs.CR

TL;DR: IssueGuard是一个Chrome扩展工具，用于实时检测和防止GitHub/GitLab问题报告中的秘密泄露，结合正则表达式提取和微调CodeBERT模型进行分类，F1分数达92.70%


<details>
  <summary>Details</summary>
Motivation: GitHub和GitLab等协作平台的issue跟踪系统包含大量非结构化文本（日志、代码片段、配置示例），存在API密钥和凭证等秘密意外暴露的风险，但这些平台没有提供提交前的警告机制

Method: 实现为Chrome扩展，实时分析用户输入文本，结合基于正则表达式的候选提取和微调的CodeBERT模型进行上下文分类，有效区分真实秘密和误报

Result: 在基准数据集上达到92.70%的F1分数，优于传统的基于正则表达式的扫描器，工具直接集成到Web界面中，持续分析问题编辑器并提供清晰的视觉警告

Conclusion: IssueGuard能够有效检测和防止秘密泄露，源代码公开可用，为GitHub/GitLab用户提供了实用的安全保护工具

Abstract: GitHub and GitLab are widely used collaborative platforms whose issue-tracking systems contain large volumes of unstructured text, including logs, code snippets, and configuration examples. This creates a significant risk of accidental secret exposure, such as API keys and credentials, yet these platforms provide no mechanism to warn users before submission. We present \textsc{IssueGuard}, a tool for real-time detection and prevention of secret leaks in issue reports. Implemented as a Chrome extension, \textsc{IssueGuard} analyzes text as users type and combines regex-based candidate extraction with a fine-tuned CodeBERT model for contextual classification. This approach effectively separates real secrets from false positives and achieves an F1-score of 92.70\% on a benchmark dataset, outperforming traditional regex-based scanners. \textsc{IssueGuard} integrates directly into the web interface and continuously analyzes the issue editor, presenting clear visual warnings to help users avoid submitting sensitive data. The source code is publicly available at \href{https://github.com/nafiurahman00/IssueGuard}{https://github.com/nafiurahman00/IssueGuard}, and a demonstration video is available at \href{https://youtu.be/kvbWA8rr9cU}{https://youtu.be/kvbWA8rr9cU}.

</details>


### [13] [A Transfer Learning Approach to Unveil the Role of Windows Common Configuration Enumerations in IEC 62443 Compliance](https://arxiv.org/abs/2602.08165)
*Miguel Bicudo,Estevão Rabello,Daniel Menasché,Paulo Segal,Claudio Segal,Anton Kocheturov,Priyanjan Sharma*

Main category: cs.CR

TL;DR: 提出一种迁移学习方法，将Windows配置枚举映射到IEC 62443-3-3安全标准，利用Linux标注数据集实现跨平台合规检查


<details>
  <summary>Details</summary>
Motivation: 工业控制系统环境高度异构，Linux、专有实时操作系统和Windows共存。虽然IEC 62443-3-3标准提供了全面的安全框架，但将其要求转化为具体的配置检查仍然具有挑战性，特别是对于Windows平台

Method: 提出迁移学习方法，将Windows通用配置枚举映射到IEC 62443-3-3系统安全要求，利用已标注的Linux数据集进行知识迁移

Result: 生成的标注数据集支持自动化合规检查、要求流行度分析以及跨平台相似性和差异识别。结果表明CCE可以作为抽象标准和具体配置之间的桥梁

Conclusion: 该方法推进了Windows环境中IEC 62443-3-3合规性的自动化、可追溯性和清晰度，为异构工业控制系统安全提供了实用解决方案

Abstract: Industrial control systems (ICS) depend on highly heterogeneous environments where Linux, proprietary real-time operating systems, and Windows coexist. Although the IEC 62443-3-3 standard provides a comprehensive framework for securing such systems, translating its requirements into concrete configuration checks remains challenging, especially for Windows platforms. In this paper, we propose a transfer learning methodology that maps Windows Common Configuration Enumerations (CCEs) to IEC 62443-3-3 System Security Requirements by leveraging labeled Linux datasets. The resulting labeled dataset enables automated compliance checks, analysis of requirement prevalence, and identification of cross-platform similarities and divergences. Our results highlight the role of CCEs as a bridge between abstract standards and concrete configurations, advancing automation, traceability, and clarity in IEC 62443-3-3 compliance for Windows environments.

</details>


### [14] [Towards Real-World Industrial-Scale Verification: LLM-Driven Theorem Proving on seL4](https://arxiv.org/abs/2602.08384)
*Jianyu Zhang,Fuyuan Zhang,Jiayi Lu,Jilin Hu,Xiaoyi Yin,Long Zhang,Feng Yang,Yongwang Zhao*

Main category: cs.CR

TL;DR: AutoReal是一个基于大语言模型的定理证明方法，专门针对工业级系统验证，支持轻量级本地部署，在seL4验证项目中取得51.67%的证明成功率。


<details>
  <summary>Details</summary>
Motivation: 形式化方法虽然可靠但成本高昂，需要大量专家投入。现有基于LLM的定理证明研究多集中于数学基准测试，对工业级验证项目评估有限，且大多依赖无法本地部署的大型闭源模型。

Method: 提出AutoReal方法，包含两个关键技术：1) 基于思维链的证明训练，教授LLM证明步骤背后的推理过程；2) 上下文增强，利用项目中的证明上下文提升证明能力。基于该方法微调得到7B参数的AutoReal-Prover模型。

Result: 在seL4验证项目中，AutoReal-Prover在660个重要定理上达到51.67%的证明成功率，显著优于先前尝试的27.06%。在AFP的三个安全相关项目中，对451个定理达到53.88%的证明成功率。

Conclusion: 该工作推进了基于LLM的定理证明在真实工业级验证中的应用，通过轻量级本地部署方案解决了现有方法的成本和部署限制问题。

Abstract: Formal methods (FM) are reliable but costly to apply, often requiring years of expert effort in industrial-scale projects such as seL4, especially for theorem proving. Recent advances in large language models (LLMs) have made automated theorem proving increasingly feasible. However, most prior work focuses on mathematics-oriented benchmarks such as miniF2F, with limited evaluation on real-world verification projects. The few studies that consider industrial-scale verification mostly rely on closed-source models with hundreds of billions of parameters, which cannot be locally deployed and incur substantial usage costs. In this paper, we propose AutoReal, an LLM-driven theorem proving method for real-world industrial-scale systems with support for lightweight local deployment. We evaluate AutoReal on the seL4-Isabelle verification project as a representative and challenging case study. AutoReal incorporates two key improvements: (1) chain-of-thought (CoT)-based proof training, which teaches the LLM the reasoning behind proof steps and enables step-wise explanations alongside proofs, and (2) context augmentation, which leverages proof context from the project to enhance LLM-driven proving. Based on the AutoReal methodology, we fine-tune a base model to obtain AutoReal-Prover, a compact 7B-scale prover for industrial-scale theorem proving. AutoReal-Prover achieves a 51.67% proof success rate on 660 theorems from seL4-designated Important Theories across all 10 seL4 proof categories, substantially outperforming prior attempts on seL4 (27.06%). To evaluate generalization, we further apply AutoReal-Prover to three security-related projects from the Archive of Formal Proofs (AFP), covering all 451 theorems and achieving a proof success rate of 53.88%. Overall, this work advances the application of LLM-driven theorem proving in real-world industrial-scale verification.

</details>


### [15] [LLMs + Security = Trouble](https://arxiv.org/abs/2602.08422)
*Benjamin Livshits*

Main category: cs.CR

TL;DR: 论文批评当前AI生成代码的安全方法存在缺陷，提出通过约束解码在代码生成过程中直接强制执行安全约束，而非依赖后验检测修复


<details>
  <summary>Details</summary>
Motivation: 当前AI生成代码的安全保障方法存在两个主要问题：1）使用概率性AI检查器或攻击者来保护概率生成的代码无法解决安全漏洞的长尾问题；2）神经符号方法虽然理论上吸引人，但在实际LLM辅助开发工作流中难以实施，需要人工介入验证规范、解决歧义和裁决失败，破坏了安全构建的保证

Method: 提出在代码生成过程中直接强制执行安全约束（如通过约束解码），特别是针对扩散式代码模型，利用其模块化、层次化的方法实现安全约束，结合低延迟生成技术生成安全构建的代码

Result: 论文论证了在代码生成过程中强制执行安全约束的方法能够提供更强的安全保障，特别是对于扩散式代码模型，这种方法提供了模块化、层次化安全执行的优雅机会

Conclusion: 相比后验检测和修复，在代码生成过程中直接强制执行安全约束是更有前景的方向，能够为AI生成的代码提供更强的安全保障，特别是在扩散式代码模型的框架下

Abstract: We argue that when it comes to producing secure code with AI, the prevailing "fighting fire with fire" approach -- using probabilistic AI-based checkers or attackers to secure probabilistically generated code -- fails to address the long tail of security bugs. As a result, systems may remain exposed to zero-day vulnerabilities that can be discovered by better-resourced or more persistent adversaries.
  While neurosymbolic approaches that combine LLMs with formal methods are attractive in principle, we argue that they are difficult to reconcile with the "vibe coding" workflow common in LLM-assisted development: unless the end-to-end verification pipeline is fully automated, developers are repeatedly asked to validate specifications, resolve ambiguities, and adjudicate failures, making the human-in-the-loop a likely point of weakness, compromising secure-by-construction guarantees.
  In this paper we argue that stronger security guarantees can be obtained by enforcing security constraints during code generation (e.g., via constrained decoding), rather than relying solely on post-hoc detection and repair. This direction is particularly promising for diffusion-style code models, whose approach provides a natural elegant opportunity for modular, hierarchical security enforcement, allowing us to combine lower-latency generation techniques with generating secure-by-construction code.

</details>


### [16] [Retrieval Pivot Attacks in Hybrid RAG: Measuring and Mitigating Amplified Leakage from Vector Seeds to Graph Expansion](https://arxiv.org/abs/2602.08668)
*Scott Thornton*

Main category: cs.CR

TL;DR: 混合检索增强生成(RAG)管道结合向量相似性搜索与知识图谱扩展进行多跳推理，但这种组合引入了新的安全漏洞：向量检索的"种子"块可以通过实体链接访问敏感图谱区域，导致跨租户数据泄露。


<details>
  <summary>Details</summary>
Motivation: 研究混合RAG管道中向量检索与知识图谱扩展组合带来的独特安全风险，这种风险在纯向量检索中不存在。需要形式化这种风险并开发量化指标。

Method: 提出检索枢轴风险(RPR)形式化框架，引入Leakage@k、放大因子和枢轴深度(PD)等量化指标。设计了七种检索枢轴攻击，并在合成多租户企业语料库和Enron邮件语料库上进行实验。

Result: 未防御的混合管道表现出高枢轴风险(RPR高达0.95)，每个查询返回多个未授权项目。泄露通常出现在PD=2处。在图形扩展边界实施授权后，泄露基本消除(RPR接近0)，且开销最小。

Conclusion: 混合RAG系统的安全漏洞源于边界授权缺失而非组件本身。两个单独安全的检索组件组合可能形成不安全系统，必须在转换点重新检查授权。在图形扩展边界实施简单授权即可有效消除泄露风险。

Abstract: Hybrid Retrieval-Augmented Generation (RAG) pipelines combine vector similarity search with knowledge graph expansion for multi-hop reasoning. We show that this composition introduces a distinct security failure mode: a vector-retrieved "seed" chunk can pivot via entity links into sensitive graph neighborhoods, causing cross-tenant data leakage that does not occur in vector-only retrieval. We formalize this risk as Retrieval Pivot Risk (RPR) and introduce companion metrics Leakage@k, Amplification Factor, and Pivot Depth (PD) to quantify leakage magnitude and traversal structure.
  We present seven Retrieval Pivot Attacks that exploit the vector-to-graph boundary and show that adversarial injection is not required: naturally shared entities create cross-tenant pivot paths organically. Across a synthetic multi-tenant enterprise corpus and the Enron email corpus, the undefended hybrid pipeline exhibits high pivot risk (RPR up to 0.95) with multiple unauthorized items returned per query. Leakage consistently appears at PD=2, which we attribute to the bipartite chunk-entity topology and formalize as a proposition.
  We then show that enforcing authorization at a single location, the graph expansion boundary, eliminates measured leakage (RPR near 0) across both corpora, all attack variants, and label forgery rates up to 10 percent, with minimal overhead. Our results indicate the root cause is boundary enforcement, not inherently complex defenses: two individually secure retrieval components can compose into an insecure system unless authorization is re-checked at the transition point.

</details>


### [17] [Large Language Lobotomy: Jailbreaking Mixture-of-Experts via Expert Silencing](https://arxiv.org/abs/2602.08741)
*Jona te Lintelo,Lichao Wu,Stjepan Picek*

Main category: cs.CR

TL;DR: 本文提出了一种名为Large Language Lobotomy (L³)的训练无关攻击方法，通过分析MoE架构中专家路由动态，识别并沉默与安全拒绝行为相关的专家，从而绕过MoE大语言模型的安全对齐机制。


<details>
  <summary>Details</summary>
Motivation: MoE架构通过仅激活每个token的部分参数来提高扩展效率，但其路由结构引入了新的安全攻击面。研究发现MoE LLMs中的安全关键行为（如拒绝）集中在少数专家中而非均匀分布，这暴露了MoE设计与鲁棒安全对齐之间的基本矛盾。

Method: 提出L³攻击方法：1）学习与拒绝行为相关的路由模式；2）将安全行为归因于特定专家；3）自适应地沉默最相关的安全专家直到产生有害输出。该方法无需训练，与架构无关。

Result: 在8个最先进的开源MoE LLMs上评估，自适应专家沉默将平均攻击成功率从7.3%提升至70.4%，最高达86.3%，优于现有训练无关的MoE越狱方法。通常只需沉默少于20%的层级专家即可绕过防护，同时基本保留通用语言能力。

Conclusion: MoE架构的效率驱动设计与鲁棒安全对齐存在根本性冲突。未来MoE LLMs需要通过架构感知和路由感知的方法更稳健地分布安全机制。

Abstract: The rapid adoption of Mixture-of-Experts (MoE) architectures marks a major shift in the deployment of Large Language Models (LLMs). MoE LLMs improve scaling efficiency by activating only a small subset of parameters per token, but their routing structure introduces new safety attack surfaces. We find that safety-critical behaviors in MoE LLMs (e.g., refusal) are concentrated in a small set of experts rather than being uniformly distributed. Building on this, we propose Large Language Lobotomy (L$^3$), a training-free, architecture-agnostic attack that compromises safety alignment by exploiting expert routing dynamics. L$^3$ learns routing patterns that correlate with refusal, attributes safety behavior to specific experts, and adaptively silences the most safety-relevant experts until harmful outputs are produced. We evaluate L$^3$ on eight state-of-the-art open-source MoE LLMs and show that our adaptive expert silencing increases average attack success from 7.3% to 70.4%, reaching up to 86.3%, outperforming prior training-free MoE jailbreak methods. Moreover, bypassing guardrails typically requires silencing fewer than 20% of layer-wise experts while largely preserving general language utility. These results reveal a fundamental tension between efficiency-driven MoE design and robust safety alignment and motivate distributing safety mechanisms more robustly in future MoE LLMs with architecture- and routing-aware methods.

</details>


### [18] [DyMA-Fuzz: Dynamic Direct Memory Access Abstraction for Re-hosted Monolithic Firmware Fuzzing](https://arxiv.org/abs/2602.08750)
*Guy Farrelly,Michael Chesser,Seyit Camtepe,Damith C. Ranasinghe*

Main category: cs.CR

TL;DR: DyMA-Fuzz是一个针对嵌入式系统固件测试的模糊测试框架，专门处理DMA接口，无需手动配置即可自动注入模糊测试数据，显著提高了代码覆盖率。


<details>
  <summary>Details</summary>
Motivation: 随着智能设备在关键领域的应用增加，需要强大的固件测试方法。现有模糊测试框架主要关注内存映射I/O和中断输入，但忽视了直接内存访问（DMA）这一关键的高吞吐量接口，而DMA在嵌入式系统中广泛使用且绕过CPU控制。

Method: DyMA-Fuzz扩展了基于流的模糊测试输入注入技术，针对重托管环境中的DMA驱动接口。它使用运行时分析技术来推断DMA内存访问模式，自动将模糊测试数据注入目标缓冲区，无需手动配置或数据手册。该方法解决了供应商特定描述符、异构DMA设计和不同描述符位置等关键挑战。

Result: 在94个固件样本和8个DMA保护的CVE基准测试中，DyMA-Fuzz发现了现有最先进工具遗漏的漏洞和执行路径，实现了高达122%的代码覆盖率提升。

Conclusion: DyMA-Fuzz是自动化固件测试的实用有效进展，为模糊测试复杂嵌入式系统提供了可扩展的解决方案，特别在处理DMA接口方面表现出色。

Abstract: The rise of smart devices in critical domains--including automotive, medical, industrial--demands robust firmware testing. Fuzzing firmware in re-hosted environments is a promising method for automated testing at scale, but remains difficult due to the tight coupling of code with a microcontroller's peripherals. Existing fuzzing frameworks primarily address input challenges in providing inputs for Memory-Mapped I/O or interrupts, but largely overlook Direct Memory Access (DMA), a key high-throughput interface used that bypasses the CPU. We introduce DyMA-Fuzz to extend recent advances in stream-based fuzz input injection to DMA-driven interfaces in re-hosted environments. It tackles key challenges--vendor-specific descriptors, heterogeneous DMA designs, and varying descriptor locations--using runtime analysis techniques to infer DMA memory access patterns and automatically inject fuzzing data into target buffers, without manual configuration or datasheets. Evaluated on 94 firmware samples and 8 DMA-guarded CVE benchmarks, DyMA-Fuzz reveals vulnerabilities and execution paths missed by state-of-the-art tools and achieves up to 122% higher code coverage. These results highlight DyMA-Fuzz as a practical and effective advancement in automated firmware testing and a scalable solution for fuzzing complex embedded systems.

</details>


### [19] [CryptoGen: Secure Transformer Generation with Encrypted KV-Cache Reuse](https://arxiv.org/abs/2602.08798)
*Hedong Zhang,Neusha Javidnia,Shweta Pardeshi,Qian Lou,Farinaz Koushanfar*

Main category: cs.CR

TL;DR: CryptoGen是首个支持可扩展隐私保护神经生成的系统，通过持久加密KV缓存重用实现近线性扩展，相比现有系统降低4.4-7.6倍延迟。


<details>
  <summary>Details</summary>
Motivation: 云托管生成模型广泛部署带来挑战：需要在不可信环境中实现高效自回归生成，同时保护用户提示和模型参数的隐私。现有判别任务安全推理系统缺乏原生加密KV缓存支持，在适应自回归解码时会产生二次方延迟和内存增长。

Method: CryptoGen整合同态加密和秘密共享，支持预填充和生成阶段。关键技术包括：统一加密KV缓存框架、针对不同阶段的异构SIMD编码、优化的密文-密文矩阵-矩阵和矩阵-向量操作、高效的噪声刷新和密文连接机制。

Result: 在WikiText-2、PTB和LAMBADA训练的生成Transformer模型上评估，对于128-512个token的输入长度，CryptoGen相比最先进的判别安全推理系统实现4.4-7.6倍更低的每token延迟，同时保持近线性的延迟和内存扩展，序列越长优势越明显。

Conclusion: CryptoGen通过持久加密KV缓存重用，首次实现了可扩展的隐私保护神经生成，解决了自回归生成中的隐私保护挑战，并作为开源库发布。

Abstract: The widespread deployment of cloud-hosted generative models raises a fundamental challenge: enabling efficient autoregressive generation while preserving the privacy of both user prompts and model parameters in untrusted environments. We address this challenge in a client-server setting where an untrusted server hosts an autoregressive Transformer and the client requires cryptographic protection for both inputs and inference. We present CryptoGen, the first system to enable scalable privacy-preserving neural generation with persistent encrypted key-value (KV) cache reuse. Discriminative-task secure inference systems incur quadratic latency and memory growth when adapted to autoregressive decoding due to the lack of native encrypted KV-cache support. In contrast, CryptoGen achieves near-linear scaling by securely reusing and updating encrypted KV caches throughout generation. CryptoGen integrates homomorphic encryption and secret sharing to support both prefilling and generation. Key techniques include a unified encrypted KV-cache framework, heterogeneous SIMD encodings for different phases, optimized cipher-cipher matrix-matrix and matrix-vector operations, and efficient noise refresh and ciphertext concatenation mechanisms. Evaluation on generative Transformer models trained on WikiText-2, PTB, and LAMBADA shows that for input lengths of 128-512 tokens, CryptoGen achieves 4.4x-7.6x lower per-token latency than state-of-the-art discriminative secure inference systems, while maintaining near-linear latency and memory scaling, with advantages increasing for longer sequences. CryptoGen is released as an open-source library.

</details>


### [20] [Reverse Online Guessing Attacks on PAKE Protocols](https://arxiv.org/abs/2602.08993)
*Eloise Christian,Tejas Gadwalkar,Arthur Azevedo de Amorim,Edward V. Zieglar*

Main category: cs.CR

TL;DR: PAKE协议虽然能抵抗传统猜测攻击且无需PKI，但缺乏服务器认证机制使其易受反向在线猜测攻击，攻击者通过冒充服务器验证密码猜测，建议默认采用比密码更严格的服务器认证措施


<details>
  <summary>Details</summary>
Motivation: 重新评估PAKE协议模型，指出由于缺乏PKI或其他服务器认证机制，PAKE协议容易受到反向在线猜测攻击，攻击者通过冒充服务器来验证密码猜测，这种攻击方式将检测负担转移给客户端，使现有防御措施失效

Method: 分析PAKE协议的安全模型，识别反向猜测攻击的逻辑和风险，研究攻击在钓鱼攻击、密码喷洒攻击、自动化登录流程或通用密码场景（如WPA3-SAE）中的有效性

Result: 反向猜测攻击在攻击者无差别攻击客户端时特别有效，例如在钓鱼攻击或密码喷洒攻击中，或者针对具有自动化登录流程或通用密码的应用场景

Conclusion: 利益相关者应默认使用比用户密码更严格的措施来认证服务器，仅密码操作模式应作为最后手段，仅在发生灾难性安全故障且其他认证机制不可用时使用

Abstract: Though not yet widely deployed, password-authenticated key exchange (PAKE) protocols have been the subject of several recent standardization efforts, partly because of their resistance against various guessing attacks, but also because they do not require a public-key infrastructure (PKI), making them naturally resistant against PKI failures. The goal of this paper is to reevaluate the PAKE model by noting that the absence of a PKI -- or, more generally, of a mechanism aside from the password for authenticating the server -- makes such protocols vulnerable to reverse online guessing attacks, in which an adversary attempts to validate password guesses by impersonating a server. While their logic is similar to traditional guessing, where the attacker impersonates a client, reverse guessing poses a unique risk because the burden of detection is shifted to the clients, rendering existing defenses against traditional guessing moot. Our results demonstrate that reverse guessing is particularly effective when an adversary attacks clients indiscriminately, such as in phishing or password-spraying attacks, or for applications with automated login processes or a universal password, such as WPA3-SAE. Our analysis suggests that stakeholders should, by default, authenticate the server using more stringent measures than just the user's password, and that a password-only mode of operation should be a last resort against catastrophic security failures when other authentication mechanisms are not available.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [21] [ST-Raptor: An Agentic System for Semi-Structured Table QA](https://arxiv.org/abs/2602.07034)
*Jinxiu Qu,Zirui Tang,Hongzhang Huang,Boyu Niu,Wei Zhou,Jiannan Wang,Yitong Song,Guoliang Li,Xuanhe Zhou,Fan Wu*

Main category: cs.AI

TL;DR: ST-Raptor是一个用于半结构化表格问答的智能体系统，通过结合视觉编辑、树形结构建模和智能体驱动查询来解决现有方法在信息损失和复杂布局处理上的不足。


<details>
  <summary>Details</summary>
Motivation: 半结构化表格问答需要精确提取单元格内容和位置，并恢复表格布局中隐含的逻辑结构、层次关系和语义关联。现有方法存在局限：Text-to-SQL方法需要将半结构化表格转换为结构化格式导致信息损失，而Text-to-Code和多模态LLM方法难以处理复杂布局且答案不准确。

Method: ST-Raptor是一个智能体系统，提供交互式分析环境，结合视觉编辑、基于树的结构建模和智能体驱动的查询解决机制，支持准确且用户友好的表格理解。

Result: 在基准数据集和真实世界数据集上的实验结果表明，ST-Raptor在准确性和可用性方面均优于现有方法。

Conclusion: ST-Raptor通过创新的交互式智能体方法，有效解决了半结构化表格问答中的信息损失和复杂布局处理问题，为自动化表格理解提供了更优的解决方案。

Abstract: Semi-structured table question answering (QA) is a challenging task that requires (1) precise extraction of cell contents and positions and (2) accurate recovery of key implicit logical structures, hierarchical relationships, and semantic associations encoded in table layouts. In practice, such tables are often interpreted manually by human experts, which is labor-intensive and time-consuming. However, automating this process remains difficult. Existing Text-to-SQL methods typically require converting semi-structured tables into structured formats, inevitably leading to information loss, while approaches like Text-to-Code and multimodal LLM-based QA struggle with complex layouts and often yield inaccurate answers. To address these limitations, we present ST-Raptor, an agentic system for semi-structured table QA. ST-Raptor offers an interactive analysis environment that combines visual editing, tree-based structural modeling, and agent-driven query resolution to support accurate and user-friendly table understanding. Experimental results on both benchmark and real-world datasets demonstrate that ST-Raptor outperforms existing methods in both accuracy and usability. The code is available at https://github.com/weAIDB/ST-Raptor, and a demonstration video is available at https://youtu.be/9GDR-94Cau4.

</details>


### [22] [DLLM-Searcher: Adapting Diffusion Large Language Model for Search Agents](https://arxiv.org/abs/2602.07035)
*Jiahao Zhao,Shaoxuan Xu,Zhongxiang Sun,Fengqi Zhu,Jingyang Ou,Yuling Shi,Chongxuan Li,Xiao Zhang,Jun Xu*

Main category: cs.AI

TL;DR: 提出DLLM-Searcher框架，通过两阶段后训练提升扩散大语言模型的智能体能力，并设计P-ReAct并行推理执行范式来降低搜索智能体的延迟问题。


<details>
  <summary>Details</summary>
Motivation: 扩散大语言模型具有并行解码和灵活生成的优势，但现有dLLMs在推理和工具调用能力上较弱；同时搜索智能体在ReAct范式下存在严重的端到端延迟问题。需要结合dLLMs的优势来优化搜索智能体的运行效率。

Method: 1) 两阶段后训练：Agentic SFT（智能体监督微调）和Agentic VRPO（智能体方差减少偏好优化），提升dLLM的信息搜索和推理能力；2) P-ReAct并行推理执行范式：利用dLLMs的灵活生成机制，优先解码工具调用指令，让模型在等待工具返回时继续思考。

Result: DLLM-Searcher达到与主流LLM搜索智能体相当的性能，P-ReAct范式实现约15%的推理加速。

Conclusion: 提出的DLLM-Searcher框架成功解决了dLLMs在智能体应用中的能力不足问题，并通过P-ReAct范式有效降低了搜索智能体的延迟，为高效搜索智能体提供了新方案。

Abstract: Recently, Diffusion Large Language Models (dLLMs) have demonstrated unique efficiency advantages, enabled by their inherently parallel decoding mechanism and flexible generation paradigm. Meanwhile, despite the rapid advancement of Search Agents, their practical deployment is constrained by a fundamental limitation, termed as 1) Latency Challenge: the serial execution of multi-round reasoning, tool calling, and tool response waiting under the ReAct agent paradigm induces severe end-to-end latency. Intuitively, dLLMs can leverage their distinctive strengths to optimize the operational efficiency of agents under the ReAct agent paradigm. Practically, existing dLLM backbones face the 2) Agent Ability Challenge. That is, existing dLLMs exhibit remarkably weak reasoning and tool-calling capabilities, preventing these advantages from being effectively realized in practice. In this paper, we propose DLLM-Searcher, an optimization framework for dLLM-based Search Agents. To solve the Agent Ability Challenge, we design a two-stage post-training pipeline encompassing Agentic Supervised Fine-Tuning (Agentic SFT) and Agentic Variance-Reduced Preference Optimization Agentic VRPO, which enhances the backbone dLLM's information seeking and reasoning capabilities. To mitigate the Latency Challenge, we leverage the flexible generation mechanism of dLLMs and propose a novel agent paradigm termed Parallel-Reasoning and Acting P-ReAct. P-ReAct guides the model to prioritize decoding tool_call instructions, thereby allowing the model to keep thinking while waiting for the tool's return. Experimental results demonstrate that DLLM-Searcher achieves performance comparable to mainstream LLM-based search agents and P-ReAct delivers approximately 15% inference acceleration. Our code is available at https://anonymous.4open.science/r/DLLM-Searcher-553C

</details>


### [23] [PreFlect: From Retrospective to Prospective Reflection in Large Language Model Agents](https://arxiv.org/abs/2602.07187)
*Hanyu Wang,Yuanpu Cao,Lu Lin,Jinghui Chen*

Main category: cs.AI

TL;DR: PreFlect提出前瞻性反思机制，将LLM智能体从事后纠错转向执行前预判，通过历史轨迹提炼规划错误模式，结合动态重规划提升复杂任务性能


<details>
  <summary>Details</summary>
Motivation: 现有反思机制本质上是回顾性的：智能体先行动、观察失败、然后尝试恢复。这种事后纠错方式效率有限，需要在执行前就能预见和避免错误

Method: 1) 前瞻性反思：在执行前批评和精炼智能体计划；2) 从历史智能体轨迹中提炼规划错误，捕捉重复的成功和失败模式；3) 动态重规划机制：当原始计划遇到意外偏差时提供执行时计划更新

Result: 在不同基准测试中，PreFlect显著提高了复杂现实世界任务的整体智能体效用，优于基于反思的基线方法和几种更复杂的智能体架构

Conclusion: 前瞻性反思机制比传统回顾性反思更有效，通过执行前预判和动态调整，能够显著提升LLM智能体在复杂任务中的性能表现

Abstract: Advanced large language model agents typically adopt self-reflection for improving performance, where agents iteratively analyze past actions to correct errors. However, existing reflective approaches are inherently retrospective: agents act, observe failure, and only then attempt to recover. In this work, we introduce PreFlect, a prospective reflection mechanism that shifts the paradigm from post hoc correction to pre-execution foresight by criticizing and refining agent plans before execution. To support grounded prospective reflection, we distill planning errors from historical agent trajectories, capturing recurring success and failure patterns observed across past executions. Furthermore, we complement prospective reflection with a dynamic re-planning mechanism that provides execution-time plan update in case the original plan encounters unexpected deviation. Evaluations on different benchmarks demonstrate that PreFlect significantly improves overall agent utility on complex real-world tasks, outperforming strong reflection-based baselines and several more complex agent architectures. Code will be updated at https://github.com/wwwhy725/PreFlect.

</details>


### [24] [Is there "Secret Sauce'' in Large Language Model Development?](https://arxiv.org/abs/2602.07238)
*Matthias Mertens,Natalia Fischl-Lanzoni,Neil Thompson*

Main category: cs.AI

TL;DR: 研究发现：在AI前沿领域，80-90%的性能差异由训练计算量解释，而非专有技术；但在非前沿领域，专有技术和共享算法进步显著降低达到特定能力所需的计算量


<details>
  <summary>Details</summary>
Motivation: 探究领先LLM开发者的性能优势是源于专有"秘方"还是单纯的计算规模扩展，这对于理解AI领导地位和技术扩散具有重要意义

Method: 使用2022-2025年间发布的809个模型的训练和基准数据，建立包含发布日期和开发者固定效应的扩展定律回归模型进行分析

Result: 1) 前沿领域：80-90%性能差异由更高训练计算量解释；2) 非前沿领域：专有技术和共享算法进步显著降低计算需求；3) 某些公司能系统性地更高效生产较小模型；4) 同一公司内模型效率差异可达40倍以上

Conclusion: 前沿AI进步主要由计算规模驱动而非专有技术，但专有技术在中低端市场仍具重要价值；公司内部效率差异显著，这对AI领导地位和能力扩散具有重要政策含义

Abstract: Do leading LLM developers possess a proprietary ``secret sauce'', or is LLM performance driven by scaling up compute? Using training and benchmark data for 809 models released between 2022 and 2025, we estimate scaling-law regressions with release-date and developer fixed effects. We find clear evidence of developer-specific efficiency advantages, but their importance depends on where models lie in the performance distribution. At the frontier, 80-90% of performance differences are explained by higher training compute, implying that scale--not proprietary technology--drives frontier advances. Away from the frontier, however, proprietary techniques and shared algorithmic progress substantially reduce the compute required to reach fixed capability thresholds. Some companies can systematically produce smaller models more efficiently. Strikingly, we also find substantial variation of model efficiency within companies; a firm can train two models with more than 40x compute efficiency difference. We also discuss the implications for AI leadership and capability diffusion.

</details>


### [25] [From Out-of-Distribution Detection to Hallucination Detection: A Geometric View](https://arxiv.org/abs/2602.07253)
*Litian Liu,Reza Pourreza,Yubing Jian,Yao Qin,Roland Memisevic*

Main category: cs.AI

TL;DR: 将大语言模型中的幻觉检测重新定义为分布外检测问题，提出无需训练的单样本检测方法，在推理任务中实现高精度幻觉检测


<details>
  <summary>Details</summary>
Motivation: 大语言模型的幻觉检测是一个关键但尚未解决的问题，现有方法在问答任务中表现良好，但在需要推理的任务中效果有限。研究者希望通过借鉴计算机视觉中成熟的分布外检测技术来解决这一问题。

Method: 将语言模型的下一个词预测视为分类任务，应用分布外检测技术，并对大语言模型的结构差异进行适当修改，开发无需训练的单样本检测器。

Result: 基于分布外检测的方法在推理任务的幻觉检测中实现了高精度，表明这种方法具有实际应用价值。

Conclusion: 将幻觉检测重新定义为分布外检测问题，为大语言模型的安全性提供了一个有前景且可扩展的路径。

Abstract: Detecting hallucinations in large language models is a critical open problem with significant implications for safety and reliability. While existing hallucination detection methods achieve strong performance in question-answering tasks, they remain less effective on tasks requiring reasoning. In this work, we revisit hallucination detection through the lens of out-of-distribution (OOD) detection, a well-studied problem in areas like computer vision. Treating next-token prediction in language models as a classification task allows us to apply OOD techniques, provided appropriate modifications are made to account for the structural differences in large language models. We show that OOD-based approaches yield training-free, single-sample-based detectors, achieving strong accuracy in hallucination detection for reasoning tasks. Overall, our work suggests that reframing hallucination detection as OOD detection provides a promising and scalable pathway toward language model safety.

</details>


### [26] [Incentive-Aware AI Safety via Strategic Resource Allocation: A Stackelberg Security Games Perspective](https://arxiv.org/abs/2602.07259)
*Cheol Woo Kim,Davin Choo,Tzeh Yuan Neoh,Milind Tambe*

Main category: cs.AI

TL;DR: 论文提出将AI安全视为Stackelberg安全博弈问题，强调需要从静态模型对齐转向动态战略监督，以应对AI开发部署中的对抗性激励问题。


<details>
  <summary>Details</summary>
Motivation: 现有AI安全框架主要将对齐视为静态优化问题，忽略了数据收集、模型评估和部署过程中的动态对抗性激励。随着AI系统能力增强和自主性提高，需要同时考虑模型级对齐和人类及机构的战略监督。

Method: 采用Stackelberg安全博弈作为理论基础，将AI监督视为防御者（审计员、评估者、部署者）与攻击者（恶意行为者、未对齐贡献者、最坏情况故障模式）之间的战略互动。该框架可用于分析激励设计、有限监督能力和对抗性不确定性。

Result: 该框架可应用于：(1) 训练时审计对抗数据/反馈投毒；(2) 有限评审资源下的预部署评估；(3) 对抗环境中的鲁棒多模型部署。将算法对齐与制度监督设计相结合。

Conclusion: 基于Stackelberg安全博弈的视角使AI监督能够主动、风险感知且抗操纵，通过博弈论威慑将算法对齐与制度监督设计相统一，为AI全生命周期的安全提供系统性框架。

Abstract: As AI systems grow more capable and autonomous, ensuring their safety and reliability requires not only model-level alignment but also strategic oversight of the humans and institutions involved in their development and deployment. Existing safety frameworks largely treat alignment as a static optimization problem (e.g., tuning models to desired behavior) while overlooking the dynamic, adversarial incentives that shape how data are collected, how models are evaluated, and how they are ultimately deployed. We propose a new perspective on AI safety grounded in Stackelberg Security Games (SSGs): a class of game-theoretic models designed for adversarial resource allocation under uncertainty. By viewing AI oversight as a strategic interaction between defenders (auditors, evaluators, and deployers) and attackers (malicious actors, misaligned contributors, or worst-case failure modes), SSGs provide a unifying framework for reasoning about incentive design, limited oversight capacity, and adversarial uncertainty across the AI lifecycle. We illustrate how this framework can inform (1) training-time auditing against data/feedback poisoning, (2) pre-deployment evaluation under constrained reviewer resources, and (3) robust multi-model deployment in adversarial environments. This synthesis bridges algorithmic alignment and institutional oversight design, highlighting how game-theoretic deterrence can make AI oversight proactive, risk-aware, and resilient to manipulation.

</details>


### [27] [BRIDGE: Predicting Human Task Completion Time From Model Performance](https://arxiv.org/abs/2602.07267)
*Fengyuan Liu,Jay Gala,Nilaksh,Dzmitry Bahdanau,Siva Reddy,Hugo Larochelle*

Main category: cs.AI

TL;DR: BRIDGE框架通过AI模型响应学习任务难度标度，并将其与人类任务完成时间对齐，实现仅从模型性能预测人类任务时间，并预测前沿模型能力呈指数增长。


<details>
  <summary>Details</summary>
Motivation: 现有基于人类任务完成时间注释的AI系统评估方法成本高、噪声大且难以扩展，需要一种可扩展的方法将基准性能与人类可解释的任务难度度量联系起来。

Method: 提出BRIDGE心理测量框架，使用双参数逻辑项目反应理论模型，从多个基准的模型性能数据中联合估计潜在任务难度和模型能力，发现潜在任务难度与人类完成时间的对数呈线性关系。

Result: 潜在任务难度与人类完成时间的对数呈线性关系，使得仅从模型性能就能推断新基准的人类任务时间；成功预测前沿模型能力（50%可解决任务范围每约6个月翻倍），独立重现了METR的指数缩放结果。

Conclusion: BRIDGE提供了一种可扩展的框架，将AI基准性能与人类可解释的任务难度度量对齐，能够仅从模型性能预测人类任务时间，并验证了AI能力呈指数增长的趋势。

Abstract: Evaluating the real-world capabilities of AI systems requires grounding benchmark performance in human-interpretable measures of task difficulty. Existing approaches that rely on direct human task completion time annotations are costly, noisy, and difficult to scale across benchmarks. In this work, we propose BRIDGE, a unified psychometric framework that learns the latent difficulty scale from model responses and anchors it to human task completion time. Using a two-parameter logistic Item Response Theory model, we jointly estimate latent task difficulty and model capability from model performance data across multiple benchmarks. We demonstrate that latent task difficulty varies linearly with the logarithm of human completion time, allowing human task completion time to be inferred for new benchmarks from model performance alone. Leveraging this alignment, we forecast frontier model capabilities in terms of human task length and independently reproduce METR's exponential scaling results, with the 50% solvable task horizon doubling approximately every 6 months.

</details>


### [28] [TermiGen: High-Fidelity Environment and Robust Trajectory Synthesis for Terminal Agents](https://arxiv.org/abs/2602.07274)
*Kaijie Zhu,Yuzhou Nie,Yijiang Li,Yiming Huang,Jialian Wu,Jiang Liu,Ximeng Sun,Zhenfei Yin,Lun Wang,Zicheng Liu,Emad Barsoum,William Yang Wang,Wenbo Guo*

Main category: cs.AI

TL;DR: TermiGen是一个端到端管道，通过合成可验证环境和弹性专家轨迹来解决LLM执行复杂终端任务的挑战，其微调的模型在TerminalBench上达到31.3%通过率，创下开源模型新纪录。


<details>
  <summary>Details</summary>
Motivation: 解决开放权重LLM执行复杂终端任务的两个根本限制：1) 高保真可执行训练环境稀缺，真实环境缺乏多样性和可扩展性，LLM合成的轨迹存在幻觉问题；2) 标准指令调优使用专家轨迹，很少包含较小模型常见的简单错误，导致学生模型无法有效从自身运行时错误中恢复。

Method: TermiGen采用端到端管道：首先通过迭代多智能体精炼循环生成功能有效的任务和Docker容器；随后采用生成器-批评器协议，在轨迹收集过程中主动注入错误，合成富含错误纠正循环的数据。

Result: 在TermiGen生成的数据集上微调的TermiGen-Qwen2.5-Coder-32B模型在TerminalBench上达到31.3%的通过率，创下开放权重模型的新最先进水平，超越了现有基线模型，甚至超过了o4-mini等专有模型。

Conclusion: TermiGen通过合成可验证环境和包含错误纠正的弹性轨迹，有效解决了LLM执行终端任务的训练数据瓶颈，显著提升了模型在实际终端任务中的表现，为开放权重模型在该领域设立了新的标杆。

Abstract: Executing complex terminal tasks remains a significant challenge for open-weight LLMs, constrained by two fundamental limitations. First, high-fidelity, executable training environments are scarce: environments synthesized from real-world repositories are not diverse and scalable, while trajectories synthesized by LLMs suffer from hallucinations. Second, standard instruction tuning uses expert trajectories that rarely exhibit simple mistakes common to smaller models. This creates a distributional mismatch, leaving student models ill-equipped to recover from their own runtime failures. To bridge these gaps, we introduce TermiGen, an end-to-end pipeline for synthesizing verifiable environments and resilient expert trajectories. Termi-Gen first generates functionally valid tasks and Docker containers via an iterative multi-agent refinement loop. Subsequently, we employ a Generator-Critic protocol that actively injects errors during trajectory collection, synthesizing data rich in error-correction cycles. Fine-tuned on this TermiGen-generated dataset, our TermiGen-Qwen2.5-Coder-32B achieves a 31.3% pass rate on TerminalBench. This establishes a new open-weights state-of-the-art, outperforming existing baselines and notably surpassing capable proprietary models such as o4-mini. Dataset is avaiable at https://github.com/ucsb-mlsec/terminal-bench-env.

</details>


### [29] [Steer2Adapt: Dynamically Composing Steering Vectors Elicits Efficient Adaptation of LLMs](https://arxiv.org/abs/2602.07276)
*Pengrui Han,Xueqiang Xu,Keyang Xuan,Peiyang Song,Siru Ouyang,Runchu Tian,Yuqing Jiang,Cheng Qian,Pengcheng Jiang,Jiashuo Sun,Junxia Cui,Ming Zhong,Ge Liu,Jiawei Han,Jiaxuan You*

Main category: cs.AI

TL;DR: STEER2ADAPT：一种轻量级框架，通过组合而非重新学习来动态调整LLM的激活向量，提高任务适应性和效率


<details>
  <summary>Details</summary>
Motivation: 现有激活引导方法通常为每个任务或概念使用单一的静态方向，这导致在任务变化时缺乏灵活性，且无法处理需要多种协调能力的复杂任务

Method: 提出STEER2ADAPT框架，将任务共享的底层概念维度捕获为可重用的低维语义先验子空间，通过少量示例动态发现基向量的线性组合来适应新任务

Result: 在9个任务和3个模型上的实验表明，在推理和安全领域平均提升8.2%，证明该方法具有数据效率高、稳定性好和透明度高的特点

Conclusion: STEER2ADAPT是一种有效的推理时适应方法，能够通过组合现有引导向量而非从头学习新向量，灵活适应复杂任务变化

Abstract: Activation steering has emerged as a promising approach for efficiently adapting large language models (LLMs) to downstream behaviors. However, most existing steering methods rely on a single static direction per task or concept, making them inflexible under task variation and inadequate for complex tasks that require multiple coordinated capabilities. To address this limitation, we propose STEER2ADAPT, a lightweight framework that adapts LLMs by composing steering vectors rather than learning new ones from scratch. In many domains (e.g., reasoning or safety), tasks share a small set of underlying concept dimensions. STEER2ADAPT captures these dimensions as a reusable, low-dimensional semantic prior subspace, and adapts to new tasks by dynamically discovering a linear combination of basis vectors from only a handful of examples. Experiments across 9 tasks and 3 models in both reasoning and safety domains demonstrate the effectiveness of STEER2ADAPT, achieving an average improvement of 8.2%. Extensive analyses further show that STEER2ADAPT is a data-efficient, stable, and transparent inference-time adaptation method for LLMs.

</details>


### [30] [Adaptive Scaffolding for Cognitive Engagement in an Intelligent Tutoring System](https://arxiv.org/abs/2602.07308)
*Sutapa Dey Tithi,Nazia Alam,Tahreem Yasir,Yang Shi,Xiaoyi Tian,Min Chi,Tiffany Barnes*

Main category: cs.AI

TL;DR: 研究开发了一个自适应系统，通过动态选择两种ICAP模式的例题（引导式例题和错误式例题）来调整认知参与度，比较了BKT和DRL两种自适应方法，发现它们都能显著提高学生测试成绩，但对不同先验知识水平的学生效果不同。


<details>
  <summary>Details</summary>
Motivation: ICAP框架定义了四种认知参与水平，更高的认知参与能带来更好的学习效果，但在智能导学系统中个性化地选择能引发最佳认知参与水平的学习活动仍然是一个关键挑战。

Method: 开发了一个自适应系统，通过动态选择两种ICAP模式的例题来调整认知参与度：主动模式的引导式例题和建构模式的错误式例题。比较了贝叶斯知识追踪（BKT）和深度强化学习（DRL）两种自适应方法与一个非自适应基线方法，在一个逻辑智能导学系统中进行实验。

Result: 在113名学生的实验中，两种自适应策略都显著提高了学生在测试问题上的表现。BKT对低先验知识学生的后测成绩提升最大，帮助他们赶上高先验知识同伴；而DRL在高先验知识学生中产生了显著更高的后测成绩。

Conclusion: 该研究为认知参与度和自适应性的复杂交互及其对学习成果的影响提供了新的见解，表明不同的自适应方法适合不同先验知识水平的学生，个性化认知参与度调整能有效提升学习效果。

Abstract: The ICAP framework defines four cognitive engagement levels: Passive, Active, Constructive, and Interactive, where increased cognitive engagement can yield improved learning. However, personalizing learning activities that elicit the optimal level of cognitive engagement remains a key challenge in intelligent tutoring systems (ITS). In this work, we develop and evaluate a system that adaptively scaffolds cognitive engagement by dynamically selecting worked examples in two different ICAP modes: (active) Guided examples and (constructive) Buggy examples. We compare Bayesian Knowledge Tracing (BKT) and Deep Reinforcement Learning (DRL) as adaptive methods against a non-adaptive baseline method for selecting example type in a logic ITS. Our experiment with 113 students demonstrates that both adaptive policies significantly improved student performance on test problems. BKT yielded the largest improvement in posttest scores for low prior knowledge students, helping them catch up with their high prior knowledge peers, whereas DRL yielded significantly higher posttest scores among high prior knowledge students. This paper contributes new insights into the complex interactions of cognitive engagement and adaptivity and their results on learning outcomes.

</details>


### [31] [RAPiD: Real-time Deterministic Trajectory Planning via Diffusion Behavior Priors for Safe and Efficient Autonomous Driving](https://arxiv.org/abs/2602.07339)
*Ruturaj Reddy,Hrishav Bakul Barua,Junn Yong Loo,Thanh Thi Nguyen,Ganesh Krishnasamy*

Main category: cs.AI

TL;DR: RAPiD是一个确定性策略提取框架，将预训练的扩散轨迹规划器蒸馏为高效策略，消除扩散采样，实现8倍加速并保持竞争性能


<details>
  <summary>Details</summary>
Motivation: 扩散轨迹规划器能很好建模人类驾驶的多模态行为，但依赖迭代随机采样，难以满足实时安全关键部署需求

Method: 使用分数正则化策略优化，利用预训练扩散规划器的评分函数作为行为先验来正则化策略学习；通过模仿预测驾驶员控制器的评论家提供密集安全监督

Result: 在nuPlan场景中实现竞争性能，比扩散基线快8倍；在interPlan基准测试中达到基于学习的规划器的最先进泛化能力

Conclusion: RAPiD成功将扩散规划器蒸馏为高效确定性策略，解决了扩散方法实时部署的挑战，同时保持性能优势

Abstract: Diffusion-based trajectory planners have demonstrated strong capability for modeling the multimodal nature of human driving behavior, but their reliance on iterative stochastic sampling poses critical challenges for real-time, safety-critical deployment. In this work, we present RAPiD, a deterministic policy extraction framework that distills a pretrained diffusion-based planner into an efficient policy while eliminating diffusion sampling. Using score-regularized policy optimization, we leverage the score function of a pre-trained diffusion planner as a behavior prior to regularize policy learning. To promote safety and passenger comfort, the policy is optimized using a critic trained to imitate a predictive driver controller, providing dense, safety-focused supervision beyond conventional imitation learning. Evaluations demonstrate that RAPiD achieves competitive performance on closed-loop nuPlan scenarios with an 8x speedup over diffusion baselines, while achieving state-of-the-art generalization among learning-based planners on the interPlan benchmark. The official website of this work is: https://github.com/ruturajreddy/RAPiD.

</details>


### [32] [W&D:Scaling Parallel Tool Calling for Efficient Deep Research Agents](https://arxiv.org/abs/2602.07359)
*Xiaoqiang Lin,Jun Hao Liew,Silvio Savarese,Junnan Li*

Main category: cs.AI

TL;DR: 本文提出了"宽深研究智能体"框架，通过并行工具调用实现宽度扩展，在保持深度扩展的同时显著提升研究任务性能。


<details>
  <summary>Details</summary>
Motivation: 现有深度研究智能体主要通过增加顺序思维和工具调用的数量来扩展深度，但通过并行工具调用实现宽度扩展的潜力尚未充分探索。本研究旨在探索同时扩展宽度和深度对智能体性能的影响。

Method: 提出宽深研究智能体框架，利用内在并行工具调用在单个推理步骤内实现有效协调，避免了复杂的多智能体编排。研究分析了各种工具调用调度器以优化并行策略。

Result: 宽度扩展显著提升了深度研究基准测试的性能，同时减少了获得正确答案所需的轮次。使用GPT-5-Medium在BrowseComp上获得62.2%准确率，超过了原始GPT-5-High报告的54.9%。

Conclusion: 优化宽度与深度之间的权衡是构建高效深度研究智能体的关键途径。并行工具调用为智能体性能提升提供了重要方向。

Abstract: Deep research agents have emerged as powerful tools for automating complex intellectual tasks through multi-step reasoning and web-based information seeking. While recent efforts have successfully enhanced these agents by scaling depth through increasing the number of sequential thinking and tool calls, the potential of scaling width via parallel tool calling remains largely unexplored. In this work, we propose the Wide and Deep research agent, a framework designed to investigate the behavior and performance of agents when scaling not only depth but also width via parallel tool calling. Unlike existing approaches that rely on complex multi-agent orchestration to parallelize workloads, our method leverages intrinsic parallel tool calling to facilitate effective coordination within a single reasoning step. We demonstrate that scaling width significantly improves performance on deep research benchmarks while reducing the number of turns required to obtain correct answers. Furthermore, we analyze the factors driving these improvements through case studies and explore various tool call schedulers to optimize parallel tool calling strategy. Our findings suggest that optimizing the trade-off between width and depth is a critical pathway toward high-efficiency deep research agents. Notably, without context management or other tricks, we obtain 62.2% accuracy with GPT-5-Medium on BrowseComp, surpassing the original 54.9% reported by GPT-5-High.

</details>


### [33] [VGAS: Value-Guided Action-Chunk Selection for Few-Shot Vision-Language-Action Adaptation](https://arxiv.org/abs/2602.07399)
*Changhua Xu,Jie Lu,Junyu Xuan,En Yu*

Main category: cs.AI

TL;DR: VGAS框架通过生成-选择范式解决VLA模型在少样本适应中的几何模糊问题，使用价值引导的动作块选择提升轨迹的几何精度和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: VLA模型在多模态推理与物理控制之间建立桥梁，但在少样本适应新任务时存在不可靠问题。主要挑战是几何模糊性：语义合理的轨迹可能因细微的几何差异导致执行失败，而有限的监督难以解决这种近失候选动作之间的歧义。

Method: 提出VGAS框架，采用生成-选择范式：1) 使用微调的VLA作为高召回率提案生成器；2) 引入Q-Chunk-Former作为几何基础的Transformer评论家，解决细粒度几何模糊；3) 提出显式几何正则化(EGR)，通过显式塑造判别性价值景观来保持动作排序分辨率，同时缓解少监督下的价值不稳定性。

Result: 实验和理论分析表明，VGAS在有限演示和分布偏移下能持续提高成功率和鲁棒性。该方法有效解决了VLA模型在少样本适应中的几何精度问题。

Conclusion: VGAS通过价值引导的动作块选择框架，为VLA模型的少样本适应提供了可靠解决方案，通过生成-选择范式和几何正则化机制，显著提升了在几何模糊场景下的执行精度和稳定性。

Abstract: Vision--Language--Action (VLA) models bridge multimodal reasoning with physical control, but adapting them to new tasks with scarce demonstrations remains unreliable. While fine-tuned VLA policies often produce semantically plausible trajectories, failures often arise from unresolved geometric ambiguities, where near-miss action candidates lead to divergent execution outcomes under limited supervision. We study few-shot VLA adaptation from a \emph{generation--selection} perspective and propose a novel framework \textbf{VGAS} (\textbf{V}alue-\textbf{G}uided \textbf{A}ction-chunk \textbf{S}election). It performs inference-time best-of-$N$ selection to identify action chunks that are both semantically faithful and geometrically precise. Specifically, \textbf{VGAS} employs a finetuned VLA as a high-recall proposal generator and introduces the \textrm{Q-Chunk-Former}, a geometrically grounded Transformer critic to resolve fine-grained geometric ambiguities. In addition, we propose \textit{Explicit Geometric Regularization} (\texttt{EGR}), which explicitly shapes a discriminative value landscape to preserve action ranking resolution among near-miss candidates while mitigating value instability under scarce supervision. Experiments and theoretical analysis demonstrate that \textbf{VGAS} consistently improves success rates and robustness under limited demonstrations and distribution shifts. Our code is available at https://github.com/Jyugo-15/VGAS.

</details>


### [34] [Progressive Multi-Agent Reasoning for Biological Perturbation Prediction](https://arxiv.org/abs/2602.07408)
*Hyomin Kim,Sang-Yeon Hwang,Jaechang Lim,Yinhua Piao,Yunhak Oh,Woo Youn Kim,Chanyoung Park,Sungsoo Ahn,Junhyeok Jeon*

Main category: cs.AI

TL;DR: 提出LINCSQA基准和PBio-Agent框架，用于预测化学扰动下基因调控反应，通过多智能体协同解决复杂生物因果推理问题


<details>
  <summary>Details</summary>
Motivation: 现有方法主要关注单细胞遗传扰动，而药物发现核心的批量细胞化学扰动研究不足；大语言模型在处理高维扰动结果时面临挑战，需要更好的因果推理能力

Method: 提出PBio-Agent多智能体框架，包含难度感知任务排序和迭代知识精炼；利用相同扰动影响的基因共享因果结构的洞察，让高置信度预测为困难案例提供上下文；框架包含生物知识图谱增强的专门智能体、整合输出的合成智能体以及确保逻辑一致性的专门评判器

Result: PBio-Agent在LINCSQA和PerturbQA基准上优于现有基线方法，即使较小模型也能无需额外训练即可预测和解释复杂生物过程

Conclusion: PBio-Agent框架通过多智能体协同和因果结构共享的洞察，有效解决了化学扰动下基因调控预测的挑战，为药物发现提供了有力工具

Abstract: Predicting gene regulation responses to biological perturbations requires reasoning about underlying biological causalities. While large language models (LLMs) show promise for such tasks, they are often overwhelmed by the entangled nature of high-dimensional perturbation results. Moreover, recent works have primarily focused on genetic perturbations in single-cell experiments, leaving bulk-cell chemical perturbations, which is central to drug discovery, largely unexplored. Motivated by this, we present LINCSQA, a novel benchmark for predicting target gene regulation under complex chemical perturbations in bulk-cell environments. We further propose PBio-Agent, a multi-agent framework that integrates difficulty-aware task sequencing with iterative knowledge refinement. Our key insight is that genes affected by the same perturbation share causal structure, allowing confidently predicted genes to contextualize more challenging cases. The framework employs specialized agents enriched with biological knowledge graphs, while a synthesis agent integrates outputs and specialized judges ensure logical coherence. PBio-Agent outperforms existing baselines on both LINCSQA and PerturbQA, enabling even smaller models to predict and explain complex biological processes without additional training.

</details>


### [35] [Are Reasoning LLMs Robust to Interventions on Their Chain-of-Thought?](https://arxiv.org/abs/2602.07470)
*Alexander von Recum,Leander Girrbach,Zeynep Akata*

Main category: cs.AI

TL;DR: RLLMs的推理轨迹对内部干扰具有较强鲁棒性，但鲁棒性受模型大小、干扰时机和干扰类型影响，恢复过程会显著增加推理长度，而怀疑表达是关键的恢复机制。


<details>
  <summary>Details</summary>
Motivation: 研究推理大语言模型（RLLMs）的推理轨迹（思维链）在面对内部干扰时的鲁棒性，了解模型如何维持推理完整性以及干扰对推理过程的影响。

Method: 设计了一个受控评估框架，在固定时间步对模型的思维链进行扰动。设计了七种干预措施（良性、中性和对抗性），并在数学、科学和逻辑任务中对多个开源RLLMs进行测试。

Result: RLLMs总体上具有鲁棒性，能够从各种扰动中可靠恢复，鲁棒性随模型规模增大而提高，早期干预会降低鲁棒性。鲁棒性受风格影响：改写会抑制怀疑表达并降低性能，而其他干预会触发怀疑并支持恢复。恢复过程有代价：中性和对抗性噪声可使思维链长度增加200%以上，而改写会缩短轨迹但损害准确性。

Conclusion: 研究揭示了RLLMs如何维持推理完整性，识别怀疑作为核心恢复机制，并强调了鲁棒性与效率之间的权衡，为未来训练方法提供了指导。

Abstract: Reasoning LLMs (RLLMs) generate step-by-step chains of thought (CoTs) before giving an answer, which improves performance on complex tasks and makes reasoning more transparent. But how robust are these reasoning traces to disruptions that occur within them? To address this question, we introduce a controlled evaluation framework that perturbs a model's own CoT at fixed timesteps. We design seven interventions (benign, neutral, and adversarial) and apply them to multiple open-weight RLLMs across Math, Science, and Logic tasks. Our results show that RLLMs are generally robust, reliably recovering from diverse perturbations, with robustness improving with model size and degrading when interventions occur early. However, robustness is not style-invariant: paraphrasing suppresses doubt-like expressions and reduces performance, while other interventions trigger doubt and support recovery. Recovery also carries a cost: neutral and adversarial noise can inflate CoT length by more than 200%, whereas paraphrasing shortens traces but harms accuracy. These findings provide new evidence on how RLLMs maintain reasoning integrity, identify doubt as a central recovery mechanism, and highlight trade-offs between robustness and efficiency that future training methods should address.

</details>


### [36] [Computing the Reachability Value of Posterior-Deterministic POMDPs](https://arxiv.org/abs/2602.07473)
*Nathanaël Fijalkow,Arka Ghosh,Roman Kniazev,Guillermo A. Pérez,Pierre Vandenhove*

Main category: cs.AI

TL;DR: 本文提出了一种新的POMDP类别——后验确定性POMDP，证明了在该类别中可达概率可以近似计算到任意精度，突破了传统POMDP计算不可行的限制。


<details>
  <summary>Details</summary>
Motivation: 传统POMDP中的验证和综合问题通常是不可判定或难以计算的，特别是可达概率计算问题（Madani等，2003证明无法计算或近似）。这与完全可观测MDP形成鲜明对比，后者可达值可在多项式时间内计算。因此需要寻找可计算的POMDP子类。

Method: 引入后验确定性POMDP的新概念：如果下一个状态可以由当前状态、采取的动作和接收的观测唯一确定，则称该POMDP为后验确定性。一旦真实状态已知，它将永远保持已知。该方法包含了所有MDP并捕获了经典的非平凡示例（如Tiger POMDP）。

Result: 证明了对于后验确定性POMDP，到达给定状态集的最大概率可以近似到任意精度。这是已知最大的POMDP类别之一，其中可达值可以近似计算。

Conclusion: 后验确定性POMDP提供了一个重要的可计算POMDP子类，突破了传统POMDP计算限制，为不确定性下的顺序决策问题提供了新的理论框架和实用计算方法。

Abstract: Partially observable Markov decision processes (POMDPs) are a fundamental model for sequential decision-making under uncertainty. However, many verification and synthesis problems for POMDPs are undecidable or intractable. Most prominently, the seminal result of Madani et al. (2003) states that there is no algorithm that, given a POMDP and a set of target states, can compute the maximal probability of reaching the target states, or even approximate it up to a non-trivial constant. This is in stark contrast to fully observable Markov decision processes (MDPs), where the reachability value can be computed in polynomial time.
  In this work, we introduce posterior-deterministic POMDPs, a novel class of POMDPs. Our main technical contribution is to show that for posterior-deterministic POMDPs, the maximal probability of reaching a given set of states can be approximated up to arbitrary precision.
  A POMDP is posterior-deterministic if the next state can be uniquely determined by the current state, the action taken, and the observation received. While the actual state is generally uncertain in POMDPs, the posterior-deterministic property tells us that once the true state is known it remains known forever. This simple and natural definition includes all MDPs and captures classical non-trivial examples such as the Tiger POMDP (Kaelbling et al. 1998), making it one of the largest known classes of POMDPs for which the reachability value can be approximated.

</details>


### [37] [GraphAgents: Knowledge Graph-Guided Agentic AI for Cross-Domain Materials Design](https://arxiv.org/abs/2602.07491)
*Isabella A. Stewart,Tarjei Paule Hage,Yu-Chuan Hsu,Markus J. Buehler*

Main category: cs.AI

TL;DR: 提出一个结合知识图谱与多智能体推理的框架，用于寻找PFAS（全氟和多氟烷基物质）的可持续替代品，通过分布式专业化和关系推理来连接跨领域科学知识。


<details>
  <summary>Details</summary>
Motivation: 材料科学创新需要整合从分子化学到机械性能的多领域概念，但人类或单智能体大语言模型难以处理海量信息且容易产生幻觉。PFAS作为受监管化学品需要可持续替代品，这需要连接不同知识领域来发现潜在解决方案。

Method: 开发了一个基于大规模知识图谱的多智能体框架，包含专门化智能体：问题分解、证据检索、设计参数提取和图遍历。通过定制图遍历策略，系统在利用性搜索（关注领域关键结果）和探索性搜索（发现新兴跨领域连接）之间交替。

Result: 消融研究表明完整多智能体流水线优于单次提示，验证了分布式专业化和关系推理的价值。以生物医学导管为例，框架生成了平衡摩擦性能、热稳定性、化学抗性和生物相容性的可持续无PFAS替代品。

Conclusion: 该工作建立了结合知识图谱与多智能体推理的框架来扩展材料设计空间，展示了多个初始设计候选方案，为跨领域科学发现提供了新方法。

Abstract: Large Language Models (LLMs) promise to accelerate discovery by reasoning across the expanding scientific landscape. Yet, the challenge is no longer access to information but connecting it in meaningful, domain-spanning ways. In materials science, where innovation demands integrating concepts from molecular chemistry to mechanical performance, this is especially acute. Neither humans nor single-agent LLMs can fully contend with this torrent of information, with the latter often prone to hallucinations. To address this bottleneck, we introduce a multi-agent framework guided by large-scale knowledge graphs to find sustainable substitutes for per- and polyfluoroalkyl substances (PFAS)-chemicals currently under intense regulatory scrutiny. Agents in the framework specialize in problem decomposition, evidence retrieval, design parameter extraction, and graph traversal, uncovering latent connections across distinct knowledge pockets to support hypothesis generation. Ablation studies show that the full multi-agent pipeline outperforms single-shot prompting, underscoring the value of distributed specialization and relational reasoning. We demonstrate that by tailoring graph traversal strategies, the system alternates between exploitative searches focusing on domain-critical outcomes and exploratory searches surfacing emergent cross-connections. Illustrated through the exemplar of biomedical tubing, the framework generates sustainable PFAS-free alternatives that balance tribological performance, thermal stability, chemical resistance, and biocompatibility. This work establishes a framework combining knowledge graphs with multi-agent reasoning to expand the materials design space, showcasing several initial design candidates to demonstrate the approach.

</details>


### [38] [MSP-LLM: A Unified Large Language Model Framework for Complete Material Synthesis Planning](https://arxiv.org/abs/2602.07543)
*Heewoong Noh,Gyoung S. Na,Namkyeong Lee,Chanyoung Park*

Main category: cs.AI

TL;DR: MSP-LLM：一个统一的LLM框架，将材料合成规划分解为前驱体预测和合成操作预测两个子问题，通过引入离散材料类别作为中间决策变量，显著提升了材料合成规划的性能。


<details>
  <summary>Details</summary>
Motivation: 材料合成规划是AI驱动材料发现中的关键瓶颈，现有方法只解决孤立子任务，缺乏统一的解决方案。需要建立一个能够同时处理前驱体选择和合成操作序列设计的完整框架。

Method: 提出MSP-LLM框架，将材料合成规划分解为两个子问题：前驱体预测和合成操作预测。引入离散材料类别作为中间决策变量，将两个任务组织成化学一致的决策链。在合成操作预测中，采用分层前驱体类型作为归纳偏置，并使用显式条件策略在自回归解码状态中保留前驱体相关信息。

Result: 实验表明，MSP-LLM在前驱体预测、合成操作预测以及完整的材料合成规划任务上均优于现有方法，证明了该框架在材料发现中的有效性和可扩展性。

Conclusion: MSP-LLM提供了一个有效且可扩展的统一框架，能够解决完整的材料合成规划任务，有望加速实际材料发现过程。

Abstract: Material synthesis planning (MSP) remains a fundamental and underexplored bottleneck in AI-driven materials discovery, as it requires not only identifying suitable precursor materials but also designing coherent sequences of synthesis operations to realize a target material. Although several AI-based approaches have been proposed to address isolated subtasks of MSP, a unified methodology for solving the entire MSP task has yet to be established. We propose MSP-LLM, a unified LLM-based framework that formulates MSP as a structured process composed of two constituent subproblems: precursor prediction (PP) and synthesis operation prediction (SOP). Our approach introduces a discrete material class as an intermediate decision variable that organizes both tasks into a chemically consistent decision chain. For OP, we further incorporate hierarchical precursor types as synthesis-relevant inductive biases and employ an explicit conditioning strategy that preserves precursor-related information in the autoregressive decoding state. Extensive experiments show that MSP-LLM consistently outperforms existing methods on both PP and SOP, as well as on the complete MSP task, demonstrating an effective and scalable framework for MSP that can accelerate real-world materials discovery.

</details>


### [39] [VERIFY-RL: Verifiable Recursive Decomposition for Reinforcement Learning in Mathematical Reasoning](https://arxiv.org/abs/2602.07559)
*Kaleem Ullah Qasim,Jiashu Zhang,Hao Li,Muhammad Kafeel Shaheen*

Main category: cs.AI

TL;DR: Verify-RL框架通过符号微分实现可验证的数学问题分解，确保子问题更简单、解决子问题有助于父任务，且分解关系有数学基础，相比启发式方法显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有数学问题分解方法通常是启发式的，无法保证子问题更简单、解决子问题有助于父任务，且分解关系缺乏数学基础。需要一种可验证的分解框架。

Method: 提出Verify-RL框架，利用符号微分提供自然的可验证分解结构：微积分规则明确定义表达式如何简化为更简单的组件，并具有可证明的性质。每个父-子分解满足三个可验证条件：严格递减的结构复杂性、解包含性和形式规则推导。

Result: 实验表明，消除无效分解带来显著收益：最难问题的准确率从32%翻倍至68%，整体相对改进40%。与启发式方法相比，Verify-RL实现了"构造即验证"。

Conclusion: 符号微分为数学问题分解提供了可验证的结构基础，Verify-RL框架通过确保分解满足严格的可验证条件，显著提升了语言模型解决复杂数学问题的能力。

Abstract: Training language models to solve complex mathematical problems benefits from curriculum learning progressively training on simpler subproblems. However, existing decomposition methods are often heuristic, offering no guarantees that subproblems are simpler, that solving them aids the parent task, or that their relationships are mathematically grounded. We observe that symbolic differentiation provides a natural structure for verified decomposition: calculus rules explicitly define how expressions reduce to simpler components with provable properties. We introduce Verify-RL, a framework where every parent-child decomposition satisfies three verifiable conditions: strictly decreasing structural complexity, solution containment, and formal rule derivation. Unlike heuristic methods where a significant fraction of decompositions are invalid our properties admit automatic verification through symbolic computation, achieving "verification by construction" Experiments demonstrate that eliminating invalid decompositions yields sizable gains, accuracy on the hardest problems more than doubles from 32% to 68%, with a 40% relative improvement overall.

</details>


### [40] [SleepMaMi: A Universal Sleep Foundation Model for Integrating Macro- and Micro-structures](https://arxiv.org/abs/2602.07628)
*Keondo Park,Younghoon Na,Yourim Choi,Hyunwoo Ryu,Hyun-Woo Shin,Hyung-Sin Kim*

Main category: cs.AI

TL;DR: SleepMaMi是一个睡眠基础模型，通过分层双编码器设计同时掌握整夜睡眠宏观结构和细粒度信号形态，在大量PSG数据上预训练，在多种下游任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 睡眠医学目前主要依赖任务特定的模型，这些模型专注于局部微结构特征，忽略了多导睡眠图（PSG）的丰富多模态上下文，未能捕捉整夜睡眠的全局宏观结构。

Method: 采用分层双编码器设计：宏观编码器通过人口统计学引导的对比学习建模整夜时间依赖关系；微观编码器通过混合掩码自编码器和多模态对比目标优化，捕捉生物信号的短期特征。在超过20,000个PSG记录（158K小时）上进行预训练。

Result: SleepMaMi在多样化的下游任务中优于现有的基础模型，展示了卓越的泛化能力和标签高效的临床睡眠分析适应性。

Conclusion: SleepMaMi作为睡眠基础模型，成功整合了宏观睡眠结构和微观信号特征，为临床睡眠分析提供了强大的通用框架，解决了传统任务特定模型的局限性。

Abstract: While the shift toward unified foundation models has revolutionized many deep learning domains, sleep medicine remains largely restricted to task-specific models that focus on localized micro-structure features. These approaches often neglect the rich, multi-modal context of Polysomnography (PSG) and fail to capture the global macro-structure of a full night's sleep. To address this, we introduce SleepMaMi , a Sleep Foundation Model engineered to master both hour-long sleep architectures and fine-grained signal morphologies. Our framework utilizes a hierarchical dual-encoder design: a Macro-Encoder to model full-night temporal dependencies and a Micro-Encoder to capture short-term characteristics from biosignals. Macro-Encoder is trained via Demographic-Guided Contrastive Learning, which aligns overnight sleep patterns with objective subject metadata, such as age, sex and BMI to refine global representations. Micro-Encoder is optimized via a hybrid Masked Autoencoder (MAE) and multi-modal contrastive objective. Pre-trained on a massive corpus of $>$20,000 PSG recordings (158K hours),SleepMaMi outperforms existing foundation models across a diverse suite of downstream tasks, demonstrating superior generalizability and label-efficient adaptation for clinical sleep analysis.

</details>


### [41] [Efficient Table Retrieval and Understanding with Multimodal Large Language Models](https://arxiv.org/abs/2602.07642)
*Zhuoyan Xu,Haoyang Fang,Boran Han,Bonan Min,Bernie Wang,Cuixiong Hu,Shuai Zhang*

Main category: cs.AI

TL;DR: TabRAG：一个用于大规模表格图像检索和推理的框架，通过视觉-文本基础模型检索候选表格，MLLMs进行细粒度重排序，最终生成答案，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现实世界中表格数据常以图像形式存在（如财务报表、手写记录、文档扫描），现有MLLMs通常假设相关表格已准备好，但实际应用中需要从大规模表格集合中识别和推理相关表格来回答用户查询。

Method: TabRAG框架：1）使用联合训练的视觉-文本基础模型检索候选表格；2）利用MLLMs对这些候选进行细粒度重排序；3）使用MLLMs在选定表格上进行推理以生成答案。

Result: 在新构建的数据集（88,161训练样本和9,819测试样本，涵盖8个基准测试，包含48,504个唯一表格）上进行广泛实验，结果显示：检索召回率提升7.0%，答案准确率提升6.1%，显著优于现有方法。

Conclusion: TabRAG为现实世界的表格理解任务提供了一个实用解决方案，能够有效处理大规模表格图像集合的检索和推理问题。

Abstract: Tabular data is frequently captured in image form across a wide range of real-world scenarios such as financial reports, handwritten records, and document scans. These visual representations pose unique challenges for machine understanding, as they combine both structural and visual complexities. While recent advances in Multimodal Large Language Models (MLLMs) show promising results in table understanding, they typically assume the relevant table is readily available. However, a more practical scenario involves identifying and reasoning over relevant tables from large-scale collections to answer user queries. To address this gap, we propose TabRAG, a framework that enables MLLMs to answer queries over large collections of table images. Our approach first retrieves candidate tables using jointly trained visual-text foundation models, then leverages MLLMs to perform fine-grained reranking of these candidates, and finally employs MLLMs to reason over the selected tables for answer generation. Through extensive experiments on a newly constructed dataset comprising 88,161 training and 9,819 testing samples across 8 benchmarks with 48,504 unique tables, we demonstrate that our framework significantly outperforms existing methods by 7.0% in retrieval recall and 6.1% in answer accuracy, offering a practical solution for real-world table understanding tasks.

</details>


### [42] [ONTrust: A Reference Ontology of Trust](https://arxiv.org/abs/2602.07662)
*Glenda Amaral,Tiago Prince Sales,Riccardo Baratella,Daniele Porello,Renata Guizzardi,Giancarlo Guizzardi*

Main category: cs.AI

TL;DR: 本文提出了一个基于统一基础本体论的信任参考本体论(ONTrust)，用于支持信息建模、自动推理、信息集成和语义互操作性任务，以应对人工智能和去中心化技术等新兴技术带来的信任挑战。


<details>
  <summary>Details</summary>
Motivation: 随着人工智能技术使机器越来越像人类，以及区块链等去中心化技术创造新的信任形式，这些新技术虽然能改善产品服务提供并促进个人和集体福祉，但其采用很大程度上取决于信任。为了构建可信系统，除了为新的信任形式定义法律、法规和治理模型外，还需要对信任进行适当的概念化，使其能被人类和机器理解。

Method: 开发了基于统一基础本体论(Unified Foundational Ontology)的信任参考本体论(ONTrust)，使用OntoUML语言进行规范。该本体论正式描述了信任概念及其不同类型，分析了影响信任的各种因素，并解释了信任关系如何产生风险。通过两个文献案例研究来展示ONTrust的实际应用。

Result: ONTrust已在多个项目中应用，包括概念建模和企业架构设计、语言评估与(重新)设计、信任管理、需求工程，以及在情感人机协作背景下的可信人工智能。本体论能够支持信息建模、自动推理、信息集成和语义互操作性任务。

Conclusion: ONTrust为信任提供了坚实的本体论基础，通过创建参考概念模型来支持各种应用场景，有助于构建可信系统并促进新兴技术的采用。该本体论为解决人工智能和去中心化技术带来的信任挑战提供了理论框架和实践工具。

Abstract: Trust has stood out more than ever in the light of recent innovations. Some examples are advances in artificial intelligence that make machines more and more humanlike, and the introduction of decentralized technologies (e.g. blockchains), which creates new forms of (decentralized) trust. These new developments have the potential to improve the provision of products and services, as well as to contribute to individual and collective well-being. However, their adoption depends largely on trust. In order to build trustworthy systems, along with defining laws, regulations and proper governance models for new forms of trust, it is necessary to properly conceptualize trust, so that it can be understood both by humans and machines. This paper is the culmination of a long-term research program of providing a solid ontological foundation on trust, by creating reference conceptual models to support information modeling, automated reasoning, information integration and semantic interoperability tasks. To address this, a Reference Ontology of Trust (ONTrust) was developed, grounded on the Unified Foundational Ontology and specified in OntoUML, which has been applied in several initiatives, to demonstrate, for example, how it can be used for conceptual modeling and enterprise architecture design, for language evaluation and (re)design, for trust management, for requirements engineering, and for trustworthy artificial intelligence (AI) in the context of affective Human-AI teaming. ONTrust formally characterizes the concept of trust and its different types, describes the different factors that can influence trust, as well as explains how risk emerges from trust relations. To illustrate the working of ONTrust, the ontology is applied to model two case studies extracted from the literature.

</details>


### [43] [EventCast: Hybrid Demand Forecasting in E-Commerce with LLM-Based Event Knowledge](https://arxiv.org/abs/2602.07695)
*Congcong Hu,Yuang Shi,Fan Huang,Yang Xiang,Zhou Ye,Ming Jin,Shiyu Wang*

Main category: cs.AI

TL;DR: EventCast是一个将未来事件知识整合到时间序列预测中的模块化框架，专门解决电商在闪购、节假日等特殊时期的需求预测问题，相比传统方法显著提升预测精度。


<details>
  <summary>Details</summary>
Motivation: 现有预测系统在闪购、节假日促销和政策干预等高影响时期经常失效，因为这些时期的需求模式会发生突然且不可预测的变化。电商运营需要能够准确预测这些特殊事件期间需求的解决方案。

Method: EventCast利用LLM进行事件驱动推理，将非结构化业务数据（如营销活动、节假日安排、卖家激励）转换为可解释的文本摘要，然后通过双塔架构将这些摘要与历史需求特征融合，实现准确、可解释且可扩展的预测。

Result: 在4个国家160个地区10个月的真实电商场景中，EventCast相比无事件知识的变体在MAE和MSE上分别提升86.9%和97.7%，相比最佳工业基线在事件驱动期间分别减少57.0%的MAE和83.3%的MSE。自2025年3月起已部署到实际工业管道中。

Conclusion: EventCast通过将LLM的事件推理能力与传统时间序列预测相结合，为动态电商环境中的运营决策提供了实用的解决方案，显著提升了特殊事件期间的需求预测准确性。

Abstract: Demand forecasting is a cornerstone of e-commerce operations, directly impacting inventory planning and fulfillment scheduling. However, existing forecasting systems often fail during high-impact periods such as flash sales, holiday campaigns, and sudden policy interventions, where demand patterns shift abruptly and unpredictably. In this paper, we introduce EventCast, a modular forecasting framework that integrates future event knowledge into time-series prediction. Unlike prior approaches that ignore future interventions or directly use large language models (LLMs) for numerical forecasting, EventCast leverages LLMs solely for event-driven reasoning. Unstructured business data, which covers campaigns, holiday schedules, and seller incentives, from existing operational databases, is processed by an LLM that converts it into interpretable textual summaries leveraging world knowledge for cultural nuances and novel event combinations. These summaries are fused with historical demand features within a dual-tower architecture, enabling accurate, explainable, and scalable forecasts. Deployed on real-world e-commerce scenarios spanning 4 countries of 160 regions over 10 months, EventCast achieves up to 86.9% and 97.7% improvement on MAE and MSE compared to the variant without event knowledge, and reduces MAE by up to 57.0% and MSE by 83.3% versus the best industrial baseline during event-driven periods. EventCast has deployed into real-world industrial pipelines since March 2025, offering a practical solution for improving operational decision-making in dynamic e-commerce environments.

</details>


### [44] [Geo-Code: A Code Framework for Reverse Code Generation from Geometric Images Based on Two-Stage Multi-Agent Evolution](https://arxiv.org/abs/2602.07749)
*Zhenyu Wu,Yanxi Long,Jian Li,Hua Huang*

Main category: cs.AI

TL;DR: Geo-coder：首个基于多智能体系统的几何图像逆向编程框架，通过像素级锚定和度量驱动代码演化实现高精度几何重建，在几何重建精度和视觉一致性方面显著领先。


<details>
  <summary>Details</summary>
Motivation: 程序代码作为连接视觉与逻辑的桥梁，为通过几何操作增强大模型多模态推理能力提供了可行监督方法。然而，当前逆向图形方法在准确重建复杂几何细节方面面临巨大挑战，常导致关键几何约束丢失或结构失真。

Method: 提出首个基于多智能体系统的几何图像逆向编程框架Geo-coder，创新性地将过程解耦为两个阶段：1）通过像素级锚定进行几何建模，利用视觉算子与大模型的互补优势精确捕获像素坐标和视觉属性；2）引入合成-渲染-验证闭环，通过双向视觉反馈驱动代码自校正。

Result: 大量实验表明，Geo-coder在几何重建精度和视觉一致性方面取得显著领先。通过有效保留核心几何语义，重建图像在多模态推理任务中表现出与原始图像相当的性能。开源了包含1500+样本的Geo-coder数据集和GeocodeLM模型。

Conclusion: Geo-coder框架通过创新的多智能体逆向编程方法，成功解决了复杂几何细节重建的瓶颈问题，为后续研究提供了坚实的数据和模型基础，显著降低了研究成本。

Abstract: Program code serves as a bridge linking vision and logic, providing a feasible supervisory approach for enhancing the multimodal reasoning capability of large models through geometric operations such as auxiliary line construction and perspective transformation. Nevertheless, current inverse graphics methods face tremendous challenges in accurately reconstructing complex geometric details, which often results in the loss of key geometric constraints or structural distortion. To address this bottleneck, we propose Geo-coder -- the first inverse programming framework for geometric images based on a multi-agent system. Our method innovatively decouples the process into geometric modeling via pixel-wise anchoring and metric-driven code evolution: Stage 1 leverages the complementary advantages of visual operators and large models to achieve precise capture of pixel coordinates and visual attributes; Stage 2 introduces a synthesis-rendering-validation closed loop, where bidirectional visual feedback drives the self-correction of code. Extensive experiments demonstrate that Geo-coder achieves a substantial lead in both geometric reconstruction accuracy and visual consistency. Notably, by effectively preserving the core geometric semantics, the images reconstructed with our method exhibit equivalent performance to the original ones in multimodal reasoning tasks, which fully validates the robustness of the framework. Finally, to further reduce research costs, we have open-sourced the Geo-coder dataset constructed on the GeoCode framework, which contains more than 1,500 samples. On this basis, we have also open-sourced the GeocodeLM model, laying a solid data and model foundation for subsequent research in this field.

</details>


### [45] [Humanizing AI Grading: Student-Centered Insights on Fairness, Trust, Consistency and Transparency](https://arxiv.org/abs/2602.07754)
*Bahare Riahi,Veronica Catete*

Main category: cs.AI

TL;DR: 研究调查本科生对AI评分系统的看法，发现学生担忧AI缺乏情境理解和个性化，建议AI应作为人类监督下的补充工具


<details>
  <summary>Details</summary>
Motivation: 随着AI在教育评估中的应用日益增多，需要了解学生对AI评分系统的看法，特别是关注公平性、信任度、一致性和透明度等伦理问题

Method: 采用Jobin（2019）的伦理原则框架，在本科计算机科学课程中（n=27）研究学生对AI评分系统的看法，通过比较AI生成的反馈与原始人工评分反馈

Result: 研究发现学生对AI评分系统存在担忧，特别是AI缺乏情境理解和个性化能力，AI反馈与人类反馈存在差异

Conclusion: 建议开发公平可信的AI系统应反映人类判断、灵活性和同理心，作为人类监督下的补充工具，为教育环境中的人性化AI设计提供原则

Abstract: This study investigates students' perceptions of Artificial Intelligence (AI) grading systems in an undergraduate computer science course (n = 27), focusing on a block-based programming final project. Guided by the ethical principles framework articulated by Jobin (2019), our study examines fairness, trust, consistency, and transparency in AI grading by comparing AI-generated feedback with original human-graded feedback. Findings reveal concerns about AI's lack of contextual understanding and personalization. We recommend that equitable and trustworthy AI systems reflect human judgment, flexibility, and empathy, serving as supplementary tools under human oversight. This work contributes to ethics-centered assessment practices by amplifying student voices and offering design principles for humanizing AI in designed learning environments.

</details>


### [46] [RECUR: Resource Exhaustion Attack via Recursive-Entropy Guided Counterfactual Utilization and Reflection](https://arxiv.org/abs/2602.08214)
*Ziwei Wang,Yuanhe Zhang,Jing Chen,Zhenhong Zhou,Ruichao Liang,Ruiying Du,Ju Jia,Cong Wu,Yang Liu*

Main category: cs.AI

TL;DR: 本文提出RECUR攻击方法，通过递归熵引导的反事实利用和反射，揭示大型推理模型在推理过程中存在的资源耗尽安全漏洞，使输出长度增加11倍，吞吐量下降90%。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型(LRMs)的显式推理需要扩展上下文长度，导致资源消耗显著增加。先前研究表明对抗性输入可能触发冗余推理过程，但推理过程本身，特别是其反思组件，尚未得到足够关注，可能导致过度反思并消耗过多计算资源。

Method: 提出递归熵来量化反思过程中的资源消耗风险，并基于此开发RECUR攻击方法。RECUR通过递归熵引导的反事实利用和反射，构建反事实问题来验证LRMs的内在缺陷和风险。

Result: 实验表明，在良性推理下，递归熵呈现明显下降趋势。RECUR攻击破坏了这一趋势，使输出长度增加高达11倍，吞吐量下降90%。

Conclusion: 本研究揭示了推理本身存在的安全问题，为鲁棒推理提供了新视角，表明需要关注推理过程中的资源消耗风险。

Abstract: Large Reasoning Models (LRMs) employ reasoning to address complex tasks. Such explicit reasoning requires extended context lengths, resulting in substantially higher resource consumption. Prior work has shown that adversarially crafted inputs can trigger redundant reasoning processes, exposing LRMs to resource-exhaustion vulnerabilities. However, the reasoning process itself, especially its reflective component, has received limited attention, even though it can lead to over-reflection and consume excessive computing power. In this paper, we introduce Recursive Entropy to quantify the risk of resource consumption in reflection, thereby revealing the safety issues inherent in inference itself. Based on Recursive Entropy, we introduce RECUR, a resource exhaustion attack via Recursive Entropy guided Counterfactual Utilization and Reflection. It constructs counterfactual questions to verify the inherent flaws and risks of LRMs. Extensive experiments demonstrate that, under benign inference, recursive entropy exhibits a pronounced decreasing trend. RECUR disrupts this trend, increasing the output length by up to 11x and decreasing throughput by 90%. Our work provides a new perspective on robust reasoning.

</details>


### [47] [InfiCoEvalChain: A Blockchain-Based Decentralized Framework for Collaborative LLM Evaluation](https://arxiv.org/abs/2602.08229)
*Yifan Yang,Jinjia Li,Kunxi Li,Puhao Zheng,Yuanyi Wang,Zheyan Qu,Yang Yu,Jianmin Wu,Ming Li,Hongxia Yang*

Main category: cs.AI

TL;DR: 论文提出去中心化评估框架解决大语言模型评估中的不稳定性问题，通过区块链协议激励全球贡献者作为独立验证者，显著降低评估方差。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型的集中式评估存在不透明、过拟合和硬件差异导致的方差问题。实证分析显示，HumanEval上单个模型十次运行的标准差（1.67）超过了官方排行榜前十名模型的性能差距（0.91），使得当前排名统计上不可靠。

Method: 提出去中心化评估框架，利用区块链协议激励全球贡献者作为独立验证者，通过异构计算节点进行大规模基准测试，实现硬件和参数多样性。采用稳健的奖励系统确保评估完整性，防止不诚实参与。

Result: 去中心化评估框架将同一模型十次运行的标准差降低到0.28，相比传统框架显著改善，为模型排名提供更高的统计置信度。

Conclusion: 该框架将评估从"集中式黑箱"转变为"去中心化背书"，通过多方共识和多样化推理环境产生更稳定、更具代表性的评估指标。平台已完全实现并将向社区发布。

Abstract: The rapid advancement of large language models (LLMs) demands increasingly reliable evaluation, yet current centralized evaluation suffers from opacity, overfitting, and hardware-induced variance. Our empirical analysis reveals an alarming inconsistency in existing evaluations: the standard deviation across ten repeated runs of a single model on HumanEval (1.67) actually exceeds the performance gap among the top-10 models on the official leaderboard (0.91), rendering current rankings statistically precarious. To mitigate these instabilities, we propose a decentralized evaluation framework that enables hardware and parameter diversity through large-scale benchmarking across heterogeneous compute nodes. By leveraging the blockchain-based protocol, the framework incentivizes global contributors to act as independent validators, using a robust reward system to ensure evaluation integrity and discourage dishonest participation. This collective verification transforms evaluation from a "centralized black box" into a "decentralized endorsement" where multi-party consensus and diverse inference environments yield a more stable, representative metric. Experimental results demonstrate that the decentralized evaluation framework reduces the standard deviation across ten runs on the same model to 0.28. This significant improvement over conventional frameworks ensures higher statistical confidence in model rankings. We have completely implemented this platform and will soon release it to the community.

</details>


### [48] [On Protecting Agentic Systems' Intellectual Property via Watermarking](https://arxiv.org/abs/2602.08401)
*Liwen Wang,Zongjie Li,Yuchong Xie,Shuai Wang,Dongdong She,Wei Wang,Juergen Rahmel*

Main category: cs.AI

TL;DR: AGENTWM是首个专门为智能体模型设计的水印框架，通过利用动作序列的语义等价性，在功能相同的工具执行路径中注入水印，有效保护智能体系统的知识产权免受模仿攻击。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型发展为能够自主推理和使用工具的智能体系统，创造了重要的知识产权价值。这些系统容易受到模仿攻击，而现有的LLM水印技术在智能体领域失效，因为现实中的智能体系统通常是灰盒，隐藏了验证所需的内在推理痕迹。

Method: AGENTWM利用动作序列的语义等价性，通过微妙地偏向功能相同的工具执行路径分布来注入水印。开发了自动生成鲁棒水印方案的流水线，以及用于验证的严格统计假设检验程序。

Result: 在三个复杂领域的广泛评估表明，AGENTWM实现了高检测准确率，同时对智能体性能影响可忽略。即使面对自适应攻击者，也无法在不严重降低被盗模型效用的情况下移除水印。

Conclusion: AGENTWM是首个专门为智能体模型设计的水印框架，能够有效保护智能体知识产权，对抗模仿攻击，填补了现有水印技术在智能体领域的空白。

Abstract: The evolution of Large Language Models (LLMs) into agentic systems that perform autonomous reasoning and tool use has created significant intellectual property (IP) value. We demonstrate that these systems are highly vulnerable to imitation attacks, where adversaries steal proprietary capabilities by training imitation models on victim outputs. Crucially, existing LLM watermarking techniques fail in this domain because real-world agentic systems often operate as grey boxes, concealing the internal reasoning traces required for verification. This paper presents AGENTWM, the first watermarking framework designed specifically for agentic models. AGENTWM exploits the semantic equivalence of action sequences, injecting watermarks by subtly biasing the distribution of functionally identical tool execution paths. This mechanism allows AGENTWM to embed verifiable signals directly into the visible action trajectory while remaining indistinguishable to users. We develop an automated pipeline to generate robust watermark schemes and a rigorous statistical hypothesis testing procedure for verification. Extensive evaluations across three complex domains demonstrate that AGENTWM achieves high detection accuracy with negligible impact on agent performance. Our results confirm that AGENTWM effectively protects agentic IP against adaptive adversaries, who cannot remove the watermarks without severely degrading the stolen model's utility.

</details>


### [49] [Do Multi-Agents Dream of Electric Screens? Achieving Perfect Accuracy on AndroidWorld Through Task Decomposition](https://arxiv.org/abs/2602.07787)
*Pierre-Louis Favreau,Jean-Pierre Lo,Clement Guiguet,Charles Simon-Meunier,Nicolas Dehandschoewercker,Allen G. Roush,Judah Goldfeder,Ravid Shwartz-Ziv*

Main category: cs.AI

TL;DR: Minitap是一个多智能体系统，在AndroidWorld基准测试中实现了100%成功率，首次完全解决了所有116个任务，超越了人类80%的性能表现。


<details>
  <summary>Details</summary>
Motivation: 研究单智能体架构在移动设备任务执行中的失败原因：混合推理轨迹导致的上下文污染、智能体无法检测的静默文本输入失败、以及无逃脱机制的重复动作循环。

Method: 通过针对性机制解决单智能体失败问题：六个专门化智能体的认知分离、基于设备状态的文本输入确定性后验证、以及检测循环并触发策略改变的元认知推理。

Result: 在AndroidWorld基准测试中实现100%成功率，首次完全解决所有116个任务。消融实验显示：多智能体分解比单智能体基线提升21个百分点；验证执行增加7个百分点；元认知增加9个百分点。

Conclusion: Minitap通过多智能体架构、验证执行和元认知推理成功解决了移动设备任务执行的挑战，超越了人类性能，并作为开源软件发布。

Abstract: We present Minitap, a multi-agent system that achieves 100% success on the AndroidWorld benchmark, the first to fully solve all 116 tasks and surpassing human performance (80%). We first analyze why single-agent architectures fail: context pollution from mixed reasoning traces, silent text input failures undetected by the agent, and repetitive action loops without escape. Minitap addresses each failure through targeted mechanisms: cognitive separation across six specialized agents, deterministic post-validation of text input against device state, and meta-cognitive reasoning that detects cycles and triggers strategy changes. Ablations show multi-agent decomposition contributes +21 points over single-agent baselines; verified execution adds +7 points; meta-cognition adds +9 points. We release Minitap as open-source software. https://github.com/minitap-ai/mobile-use

</details>


### [50] [Data Darwinism Part I: Unlocking the Value of Scientific Data for Pre-training](https://arxiv.org/abs/2602.07824)
*Yiwei Qin,Zhen Huang,Tiantian Mi,Weiye Si,Chenyang Zhou,Qipeng Guo,Siyuan Feng,Pengfei Liu*

Main category: cs.AI

TL;DR: 提出了Data Darwinism框架，这是一个十级分类法（L0-L9），将数据与模型视为共同进化的关系：先进模型为下一代系统生成更优质数据。通过构建Darwin-Science语料库（900B tokens，L0-L5）验证该框架，发现原始科学文本存在可学习性差距，使用前沿LLM进行L4（生成式精炼）和L5（认知补全）处理来弥合这一差距。


<details>
  <summary>Details</summary>
Motivation: 数据质量决定基础模型性能，但缺乏系统化的数据处理框架。现有方法未能充分利用数据与模型之间的协同进化关系，特别是在科学文献领域，原始文本存在可学习性差距，需要更高级的处理方法来释放数据的潜在价值。

Method: 1. 提出Data Darwinism十级分类法（L0-L9），描述数据与模型的共同进化过程；2. 构建Darwin-Science语料库（900B tokens，覆盖L0-L5级别）；3. 使用前沿LLM进行L4（生成式精炼）和L5（认知补全）处理，显式化推理过程和术语解释；4. 从头预训练daVinci-origin-3B/7B模型作为无污染基线；5. 进行600B tokens的持续预训练，系统评估不同数据级别对模型性能的影响。

Result: 1. Darwin-Science语料库使模型在20+基准测试中分别提升+2.12（3B）和+2.95（7B）分；2. 在领域对齐任务上提升更显著：+5.60（3B）和+8.40（7B）分；3. 从L0到L5的系统性进展带来+1.36分的总增益，证实高级数据处理能释放数据潜在价值；4. 创建了无科学内容污染的基线模型，确保严谨的归因分析。

Conclusion: Data Darwinism框架有效解决了数据质量与模型性能的协同进化问题。高级数据处理（特别是L4和L5级别）能显著提升模型性能，尤其是在领域特定任务上。该研究为原则性的、共同进化的AI开发提供了新范式，并开源了Darwin-Science语料库和daVinci-origin模型以促进后续研究。

Abstract: Data quality determines foundation model performance, yet systematic processing frameworks are lacking. We introduce Data Darwinism, a ten-level taxonomy (L0-L9) that conceptualizes data-model co-evolution: advanced models produce superior data for next-generation systems. We validate this on scientific literature by constructing Darwin-Science, a 900B-token corpus (L0-L5). We identify a learnability gap in raw scientific text, which we bridge via L4 (Generative Refinement) and L5 (Cognitive Completion) using frontier LLMs to explicate reasoning and terminology.
  To ensure rigorous attribution, we pre-trained daVinci-origin-3B/7B models from scratch, excluding scientific content to create contamination-free baselines. After 600B tokens of continued pre-training, Darwin-Science outperforms baselines by +2.12 (3B) and +2.95 (7B) points across 20+ benchmarks, rising to +5.60 and +8.40 points on domain-aligned tasks. Systematic progression to L5 yields a +1.36 total gain, confirming that higher-level processing unlocks latent data value. We release the Darwin-Science corpus and daVinci-origin models to enable principled, co-evolutionary development.

</details>


### [51] [Time Series Reasoning via Process-Verifiable Thinking Data Synthesis and Scheduling for Tailored LLM Reasoning](https://arxiv.org/abs/2602.07830)
*Jiahui Zhou,Dan Li,Boxin Li,Xiao Zhang,Erli Meng,Lin Li,Zhuomin Chen,Jian Lou,See-Kiong Ng*

Main category: cs.AI

TL;DR: VeriTime是一个通过数据合成、数据调度和强化学习训练来定制LLMs进行时间序列推理的框架，使小型模型（3B、4B）能够达到或超过大型专有LLMs的推理能力。


<details>
  <summary>Details</summary>
Motivation: 尽管LLMs在推理能力方面取得了进展，但在时间序列任务中的应用仍处于早期阶段，主要受到三个限制：缺乏精心策划的时间序列CoT训练数据、数据调度不足导致的数据效率低下，以及缺乏针对时间序列CoT数据优化的RL算法。

Method: VeriTime框架包含三个核心组件：1) 数据合成管道，构建具有过程可验证注释的TS-文本多模态数据集；2) 数据调度机制，根据难度层次和任务分类原则安排训练样本；3) 两阶段强化微调，利用可验证的过程级CoT数据设计细粒度多目标奖励。

Result: 大量实验表明，VeriTime显著提升了LLMs在各种时间序列推理任务上的性能。值得注意的是，它使紧凑的3B、4B模型能够达到与大型专有LLMs相当甚至超越的推理能力。

Conclusion: VeriTime通过创新的数据合成、调度和RL训练方法，成功解决了时间序列推理中LLMs应用的瓶颈，为小型模型在复杂时间序列任务中实现高性能推理提供了有效框架。

Abstract: Time series is a pervasive data type across various application domains, rendering the reasonable solving of diverse time series tasks a long-standing goal. Recent advances in large language models (LLMs), especially their reasoning abilities unlocked through reinforcement learning (RL), have opened new opportunities for tackling tasks with long Chain-of-Thought (CoT) reasoning. However, leveraging LLM reasoning for time series remains in its infancy, hindered by the absence of carefully curated time series CoT data for training, limited data efficiency caused by underexplored data scheduling, and the lack of RL algorithms tailored for exploiting such time series CoT data. In this paper, we introduce VeriTime, a framework that tailors LLMs for time series reasoning through data synthesis, data scheduling, and RL training. First, we propose a data synthesis pipeline that constructs a TS-text multimodal dataset with process-verifiable annotations. Second, we design a data scheduling mechanism that arranges training samples according to a principled hierarchy of difficulty and task taxonomy. Third, we develop a two-stage reinforcement finetuning featuring fine-grained, multi-objective rewards that leverage verifiable process-level CoT data. Extensive experiments show that VeriTime substantially boosts LLM performance across diverse time series reasoning tasks. Notably, it enables compact 3B, 4B models to achieve reasoning capabilities on par with or exceeding those of larger proprietary LLMs.

</details>


### [52] [LQA: A Lightweight Quantized-Adaptive Framework for Vision-Language Models on the Edge](https://arxiv.org/abs/2602.07849)
*Xin Wang,Hualin Zhou,Sheng Guang Wang,Ting Dang,Yu Zhang,Hong Jia,Tao Gu*

Main category: cs.AI

TL;DR: LQA是一个轻量化的量化自适应框架，用于在边缘设备上部署视觉语言模型，通过选择性混合量化和无梯度测试时适应，在资源受限条件下提升模型鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 视觉语言模型在边缘设备部署面临资源限制和分布偏移导致的性能下降问题，现有测试时适应方法资源消耗过大，不适合边缘设备部署。

Method: 提出LQA框架，包含选择性混合量化策略（SHQ）和量化无梯度适应机制，结合模态感知量化策略，实现轻量化的测试时适应。

Result: 在合成和真实世界分布偏移实验中，LQA将整体适应性能提升4.5%，内存使用低于全精度模型，比基于梯度的TTA方法内存使用降低19.9倍。

Conclusion: LQA为边缘设备上鲁棒、隐私保护且高效的视觉语言模型部署提供了实用路径。

Abstract: Deploying Vision-Language Models (VLMs) on edge devices is challenged by resource constraints and performance degradation under distribution shifts. While test-time adaptation (TTA) can counteract such shifts, existing methods are too resource-intensive for on-device deployment. To address this challenge, we propose LQA, a lightweight, quantized-adaptive framework for VLMs that combines a modality-aware quantization strategy with gradient-free test-time adaptation. We introduce Selective Hybrid Quantization (SHQ) and a quantized, gradient-free adaptation mechanism to enable robust and efficient VLM deployment on resource-constrained hardware. Experiments across both synthetic and real-world distribution shifts show that LQA improves overall adaptation performance by 4.5\%, uses less memory than full-precision models, and significantly outperforms gradient-based TTA methods, achieving up to 19.9$\times$ lower memory usage across seven open-source datasets. These results demonstrate that LQA offers a practical pathway for robust, privacy-preserving, and efficient VLM deployment on edge devices.

</details>


### [53] [Emergent Misalignment is Easy, Narrow Misalignment is Hard](https://arxiv.org/abs/2602.07852)
*Anna Soligo,Edward Turner,Senthooran Rajamanoharan,Neel Nanda*

Main category: cs.AI

TL;DR: 微调大型语言模型在狭窄有害数据集上可能导致其出现突发性错位，在各种无关场景中产生典型的"邪恶"回应。专家调查未能预测此结果，表明我们对LLM学习与泛化的归纳偏置理解不足。研究发现通用错位解决方案比狭窄解决方案更稳定高效。


<details>
  <summary>Details</summary>
Motivation: 研究大型语言模型在狭窄有害数据集上微调后出现的突发性错位现象，以及专家对此现象的预测失败，揭示我们对LLM学习与泛化归纳偏置的理解不足。

Method: 使用突发性错位作为案例研究，分析不同微调方法收敛到相同的通用错位线性表示。引入KL散度损失学习狭窄解决方案的线性表示，比较两种表示的特性。

Result: 通用错位解决方案比狭窄解决方案具有更低的损失、更强的抗扰动能力，并且在预训练分布中更具影响力。研究分离出了用于监控和缓解的具体通用错位表示。

Conclusion: 这项工作为研究归纳偏置如何塑造LLM泛化提供了详细案例研究和初步指标，开源了所有代码、数据集和模型微调结果，有助于监控和缓解模型错位问题。

Abstract: Finetuning large language models on narrowly harmful datasets can cause them to become emergently misaligned, giving stereotypically `evil' responses across diverse unrelated settings. Concerningly, a pre-registered survey of experts failed to predict this result, highlighting our poor understanding of the inductive biases governing learning and generalisation in LLMs. We use emergent misalignment (EM) as a case study to investigate these inductive biases and find that models can just learn the narrow dataset task, but that the general solution appears to be more stable and more efficient. To establish this, we build on the result that different EM finetunes converge to the same linear representation of general misalignment, which can be used to mediate misaligned behaviour. We find a linear representation of the narrow solution also exists, and can be learned by introducing a KL divergence loss. Comparing these representations reveals that general misalignment achieves lower loss, is more robust to perturbations, and is more influential in the pre-training distribution. This work isolates a concrete representation of general misalignment for monitoring and mitigation. More broadly, it offers a detailed case study and preliminary metrics for investigating how inductive biases shape generalisation in LLMs. We open-source all code, datasets and model finetunes.

</details>


### [54] [TreeTensor: Boost AI System on Nested Data with Constrained Tree-Like Tensor](https://arxiv.org/abs/2602.08517)
*Shaoang Zhang,Yazhe Niu*

Main category: cs.AI

TL;DR: TreeTensor是一个通用的嵌套数据结构容器，能够高效处理具有层次结构的多模态数据，支持零开销应用各种函数和操作，包括主流机器学习库。


<details>
  <summary>Details</summary>
Motivation: 传统Tensor具有固定形状，难以高效处理认知AI系统中常见的具有层次结构的嵌套数据，需要一种更灵活的数据容器。

Method: 提出TreeTensor作为通用嵌套数据容器，通过约束树结构和魔法工具，支持对嵌套数据应用任意函数和操作，包括与Scikit-Learn、Numpy、PyTorch等库的集成。

Result: TreeTensor在各种问题中展现出强大的可用性，特别是在复杂的AlphaStar系统中，同时表现出优异的运行时效率且无额外开销。

Conclusion: TreeTensor为解决嵌套数据处理问题提供了一个高效、通用的解决方案，能够与现有方法结合扩展更多用途，如异步执行和变长数据计算。

Abstract: Tensor is the most basic and essential data structure of nowadays artificial intelligence (AI) system. The natural properties of Tensor, especially the memory-continuity and slice-independence, make it feasible for training system to leverage parallel computing unit like GPU to process data simultaneously in batch, spatial or temporal dimensions. However, if we look beyond perception tasks, the data in a complicated cognitive AI system usually has hierarchical structures (i.e. nested data) with various modalities. They are inconvenient and inefficient to program directly with conventional Tensor with fixed shape. To address this issue, we summarize two main computational patterns of nested data, and then propose a general nested data container: TreeTensor. Through various constraints and magic utilities of TreeTensor, one can apply arbitrary functions and operations to nested data with almost zero cost, including some famous machine learning libraries, such as Scikit-Learn, Numpy and PyTorch. Our approach utilizes a constrained tree-structure perspective to systematically model data relationships, and it can also easily be combined with other methods to extend more usages, such as asynchronous execution and variable-length data computation. Detailed examples and benchmarks show TreeTensor not only provides powerful usability in various problems, especially one of the most complicated AI systems at present: AlphaStar for StarCraftII, but also exhibits excellent runtime efficiency without any overhead. Our project is available at https://github.com/opendilab/DI-treetensor.

</details>


### [55] [ToolSelf: Unifying Task Execution and Self-Reconfiguration via Tool-Driven Intrinsic Adaptation](https://arxiv.org/abs/2602.07883)
*Jingqi Zhou,Sheng Wang,DeZhao Deng,Junwen Lu,Junwei Su,Qintong Li,Jiahui Gao,Hao Wu,Jiyue Jiang,Lingpeng Kong,Chuan Wu*

Main category: cs.AI

TL;DR: ToolSelf：一种新型的LLM智能体范式，通过将配置更新抽象为可调用工具，实现运行时自我重配置，使智能体能够根据任务进展自主调整子目标、上下文、策略和工具箱。


<details>
  <summary>Details</summary>
Motivation: 现有基于大语言模型的智能体系统受限于静态配置，这些配置在执行前固定，无法适应动态变化的任务环境。传统方法依赖人工编排或启发式补丁，泛化能力差且优化碎片化。

Method: 提出ToolSelf范式，将配置更新抽象为可调用工具，统一任务执行和自我调整到单一动作空间。设计配置感知两阶段训练（CAT），结合拒绝采样微调和轨迹级强化学习来内化这种元能力。

Result: 在多个基准测试上的广泛实验表明，ToolSelf在保持与专用工作流程相当性能的同时，能够泛化到新任务，实现了平均24.1%的性能提升。

Conclusion: ToolSelf通过工具驱动的运行时自我重配置，使智能体从被动执行者转变为任务和自我的双重管理者，为实现真正自适应的智能体开辟了新路径。

Abstract: Agentic systems powered by Large Language Models (LLMs) have demonstrated remarkable potential in tackling complex, long-horizon tasks. However, their efficacy is fundamentally constrained by static configurations governing agent behaviors, which are fixed prior to execution and fail to adapt to evolving task dynamics. Existing approaches, relying on manual orchestration or heuristic-based patches, often struggle with poor generalization and fragmented optimization. To transcend these limitations, we propose ToolSelf, a novel paradigm enabling tool-driven runtime self-reconfiguration. By abstracting configuration updates as a callable tool, ToolSelf unifies task execution and self-adjustment into a single action space, achieving a phase transition from external rules to intrinsic parameters. Agents can thereby autonomously update their sub-goals and context based on task progression, and correspondingly adapt their strategy and toolbox, transforming from passive executors into dual managers of both task and self. We further devise Configuration-Aware Two-stage Training (CAT), combining rejection sampling fine-tuning with trajectory-level reinforcement learning to internalize this meta-capability. Extensive experiments across diverse benchmarks demonstrate that ToolSelf rivals specialized workflows while generalizing to novel tasks, achieving a 24.1% average performance gain and illuminating a path toward truly self-adaptive agents.

</details>


### [56] [MemFly: On-the-Fly Memory Optimization via Information Bottleneck](https://arxiv.org/abs/2602.07885)
*Zhenyuan Zhang,Xianzhang Jia,Zhiqin Yang,Zhenbo Song,Wei Xue,Sirui Han,Yike Guo*

Main category: cs.AI

TL;DR: MemFly是一个基于信息瓶颈原则的LLM记忆框架，通过梯度自由优化器构建分层记忆结构，结合混合检索机制，在记忆一致性、响应保真度和准确性方面优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有LLM记忆框架面临一个基本困境：既要高效压缩冗余信息，又要为下游任务保持精确检索。为了解决这一矛盾，需要一个新的记忆框架。

Method: 基于信息瓶颈原则，通过梯度自由优化器最小化压缩熵同时最大化相关熵，构建分层记忆结构。开发混合检索机制，整合语义、符号和拓扑路径，并采用迭代精炼处理复杂多跳查询。

Result: 综合实验表明，MemFly在记忆一致性、响应保真度和准确性方面显著优于现有最先进的基线方法。

Conclusion: MemFly框架成功解决了LLM记忆压缩与精确检索之间的平衡问题，通过信息瓶颈原则和混合检索机制实现了高效且准确的长时记忆管理。

Abstract: Long-term memory enables large language model agents to tackle complex tasks through historical interactions. However, existing frameworks encounter a fundamental dilemma between compressing redundant information efficiently and maintaining precise retrieval for downstream tasks. To bridge this gap, we propose MemFly, a framework grounded in information bottleneck principles that facilitates on-the-fly memory evolution for LLMs. Our approach minimizes compression entropy while maximizing relevance entropy via a gradient-free optimizer, constructing a stratified memory structure for efficient storage. To fully leverage MemFly, we develop a hybrid retrieval mechanism that seamlessly integrates semantic, symbolic, and topological pathways, incorporating iterative refinement to handle complex multi-hop queries. Comprehensive experiments demonstrate that MemFly substantially outperforms state-of-the-art baselines in memory coherence, response fidelity, and accuracy.

</details>


### [57] [Selective Fine-Tuning for Targeted and Robust Concept Unlearning](https://arxiv.org/abs/2602.07919)
*Mansi,Avinash Kori,Francesca Toni,Soteris Demetriou*

Main category: cs.AI

TL;DR: TRUST是一种新颖的扩散模型概念遗忘方法，通过动态定位目标概念神经元并进行选择性微调，结合Hessian正则化，有效防止有害内容生成，同时保持生成质量。


<details>
  <summary>Details</summary>
Motivation: 文本引导扩散模型容易被滥用生成有害内容，现有概念遗忘方法要么只能处理单个概念，要么需要全模型微调计算成本高，而静态概念定位方法效果不佳。

Method: 提出TRUST方法：1) 动态估计目标概念神经元；2) 通过选择性微调进行概念遗忘；3) 结合Hessian正则化增强鲁棒性。

Result: 实验表明TRUST能有效对抗对抗性提示，显著保持生成质量，比现有方法更快，并能处理单个概念、概念组合和条件概念，无需特定正则化。

Conclusion: TRUST提供了一种高效、鲁棒的概念遗忘解决方案，在安全性、生成质量和计算效率之间取得了良好平衡。

Abstract: Text guided diffusion models are used by millions of users, but can be easily exploited to produce harmful content. Concept unlearning methods aim at reducing the models' likelihood of generating harmful content. Traditionally, this has been tackled at an individual concept level, with only a handful of recent works considering more realistic concept combinations. However, state of the art methods depend on full finetuning, which is computationally expensive. Concept localisation methods can facilitate selective finetuning, but existing techniques are static, resulting in suboptimal utility. In order to tackle these challenges, we propose TRUST (Targeted Robust Selective fine Tuning), a novel approach for dynamically estimating target concept neurons and unlearning them through selective finetuning, empowered by a Hessian based regularization. We show experimentally, against a number of SOTA baselines, that TRUST is robust against adversarial prompts, preserves generation quality to a significant degree, and is also significantly faster than the SOTA. Our method achieves unlearning of not only individual concepts but also combinations of concepts and conditional concepts, without any specific regularization.

</details>


### [58] [IV Co-Scientist: Multi-Agent LLM Framework for Causal Instrumental Variable Discovery](https://arxiv.org/abs/2602.07943)
*Ivaxi Sheth,Zhijing Jin,Bryan Wilder,Dominik Janzing,Mario Fritz*

Main category: cs.AI

TL;DR: LLMs能帮助发现有效的工具变量，通过两阶段评估框架验证其能力，并开发了IV Co-Scientist多智能体系统


<details>
  <summary>Details</summary>
Motivation: 在存在内生变量与结果混淆的情况下，工具变量(IVs)用于隔离内生变量的因果效应。识别有效工具变量需要跨学科知识、创造力和上下文理解，这是一项非平凡的任务。本文研究大型语言模型(LLMs)是否能帮助完成这项任务。

Method: 采用两阶段评估框架：1)测试LLMs是否能从文献中恢复已确立的工具变量，评估其复制标准推理的能力；2)评估LLMs是否能识别并避免已被经验或理论否定的工具变量。基于这些结果，引入IV Co-Scientist多智能体系统，该系统为给定的处理-结果对提出、批判和优化工具变量。还引入了一个统计检验来在没有真实值的情况下评估一致性。

Result: 结果显示LLMs有潜力从大型观测数据库中发现有效的工具变量。

Conclusion: LLMs在工具变量发现方面具有潜力，IV Co-Scientist系统能够有效提出和优化工具变量，为因果推断研究提供了新的自动化工具。

Abstract: In the presence of confounding between an endogenous variable and the outcome, instrumental variables (IVs) are used to isolate the causal effect of the endogenous variable. Identifying valid instruments requires interdisciplinary knowledge, creativity, and contextual understanding, making it a non-trivial task. In this paper, we investigate whether large language models (LLMs) can aid in this task. We perform a two-stage evaluation framework. First, we test whether LLMs can recover well-established instruments from the literature, assessing their ability to replicate standard reasoning. Second, we evaluate whether LLMs can identify and avoid instruments that have been empirically or theoretically discredited. Building on these results, we introduce IV Co-Scientist, a multi-agent system that proposes, critiques, and refines IVs for a given treatment-outcome pair. We also introduce a statistical test to contextualize consistency in the absence of ground truth. Our results show the potential of LLMs to discover valid instrumental variables from a large observational database.

</details>


### [59] [LOCA-bench: Benchmarking Language Agents Under Controllable and Extreme Context Growth](https://arxiv.org/abs/2602.07962)
*Weihao Zeng,Yuzhen Huang,Junxian He*

Main category: cs.AI

TL;DR: LOCA-bench是一个用于评估长上下文语言智能体的基准测试，通过自动化环境状态控制来调节上下文长度，评估模型在各种上下文管理策略下的表现。


<details>
  <summary>Details</summary>
Motivation: 现有长上下文基准主要关注单步设置下的信息检索能力，而现实场景中语言模型需要作为智能体在动态增长的环境中执行复杂任务，现有评估方法无法充分反映这种能力。

Method: LOCA-bench通过自动化、可扩展的环境状态控制来调节智能体的上下文长度，能够在保持任务语义固定的同时，将上下文长度扩展到近乎无限，评估模型与各种上下文管理策略（脚手架）的组合表现。

Result: 随着环境状态复杂度增加，智能体性能普遍下降，但先进的上下文管理技术能显著提高整体成功率。

Conclusion: LOCA-bench为评估长上下文智能体场景中的模型和脚手架提供了一个平台，揭示了上下文管理策略对智能体性能的重要影响。

Abstract: Large language models (LLMs) are increasingly capable of carrying out long-running, real-world tasks. However, as the amount of context grows, their reliability often deteriorates, a phenomenon known as "context rot". Existing long-context benchmarks primarily focus on single-step settings that evaluate a model's ability to retrieve information from a long snippet. In realistic scenarios, however, LLMs often need to act as agents that explore environments, follow instructions and plans, extract useful information, and predict correct actions under a dynamically growing context. To assess language agents in such settings, we introduce LOCA-bench (a benchmark for LOng-Context Agents). Given a task prompt, LOCA-bench leverages automated and scalable control of environment states to regulate the agent's context length. This design enables LOCA-bench to extend the context length potentially to infinity in a controlled way while keeping the underlying task semantics fixed. LOCA-bench evaluates language agents as a combination of models and scaffolds, including various context management strategies. While agent performance generally degrades as the environment states grow more complex, advanced context management techniques can substantially improve the overall success rate. We open-source LOCA-bench to provide a platform for evaluating models and scaffolds in long-context, agentic scenarios: https://github.com/hkust-nlp/LOCA-bench

</details>


### [60] [Accelerating Social Science Research via Agentic Hypothesization and Experimentation](https://arxiv.org/abs/2602.07983)
*Jishu Sen Gupta,Harini SI,Somesh Kumar Singh,Syed Mohamad Tawseeq,Yaman Kumar Singla,David Doermann,Rajiv Ratn Shah,Balaji Krishnamurthy*

Main category: cs.AI

TL;DR: EXPERIGEN是一个端到端的科学发现框架，通过生成器-实验者两阶段搜索，在多个领域发现比现有方法多2-4倍的统计显著假设，预测性能提升7-17%，并通过专家评审和真实A/B测试验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 数据驱动的社会科学研究过程缓慢，依赖观察、假设生成和实验验证的迭代循环。现有数据驱动方法虽然加速了部分过程，但未能支持端到端的科学发现。为解决这一差距，需要开发能够完整支持科学发现过程的框架。

Method: 提出EXPERIGEN框架，采用受贝叶斯优化启发的两阶段搜索：生成器提出候选假设，实验者进行经验评估。该框架可扩展到多模态和关系数据集等复杂数据环境。

Result: 在多个领域，EXPERIGEN发现的统计显著假设数量是现有方法的2-4倍，预测性能提升7-17%。专家评审显示，88%的假设具有中等或强新颖性，70%被认为有影响力且值得进一步研究。首次进行的LLM生成假设A/B测试获得统计显著结果（p<1e-6），效应量达344%。

Conclusion: EXPERIGEN框架成功实现了端到端的科学发现，不仅在统计性能上优于现有方法，而且生成的假设具有新颖性、实证基础和可操作性，能够推动真正的科学进步。该框架为加速社会科学研究提供了有效工具。

Abstract: Data-driven social science research is inherently slow, relying on iterative cycles of observation, hypothesis generation, and experimental validation. While recent data-driven methods promise to accelerate parts of this process, they largely fail to support end-to-end scientific discovery. To address this gap, we introduce EXPERIGEN, an agentic framework that operationalizes end-to-end discovery through a Bayesian optimization inspired two-phase search, in which a Generator proposes candidate hypotheses and an Experimenter evaluates them empirically. Across multiple domains, EXPERIGEN consistently discovers 2-4x more statistically significant hypotheses that are 7-17 percent more predictive than prior approaches, and naturally extends to complex data regimes including multimodal and relational datasets. Beyond statistical performance, hypotheses must be novel, empirically grounded, and actionable to drive real scientific progress. To evaluate these qualities, we conduct an expert review of machine-generated hypotheses, collecting feedback from senior faculty. Among 25 reviewed hypotheses, 88 percent were rated moderately or strongly novel, 70 percent were deemed impactful and worth pursuing, and most demonstrated rigor comparable to senior graduate-level research. Finally, recognizing that ultimate validation requires real-world evidence, we conduct the first A/B test of LLM-generated hypotheses, observing statistically significant results with p less than 1e-6 and a large effect size of 344 percent.

</details>


### [61] [Towards Adaptive, Scalable, and Robust Coordination of LLM Agents: A Dynamic Ad-Hoc Networking Perspective](https://arxiv.org/abs/2602.08009)
*Rui Li,Zeyu Zhang,Xiaohe Bo,Quanyu Dai,Chaozhuo Li,Feng Wen,Xu Chen*

Main category: cs.AI

TL;DR: RAPS是一种基于声誉感知的发布-订阅范式，用于实现LLM多智能体的自适应、可扩展和鲁棒协调，解决了传统手动编排的负担问题。


<details>
  <summary>Details</summary>
Motivation: 基于大语言模型的多智能体架构虽然展现了群体智能的潜力，但手动编排的工作负担巨大，迫切需要自动化设计智能体工作流程。如何建立可扩展数量的智能体主机之间的自适应可靠通信是一个未解决的难题。

Method: RAPS基于分布式发布-订阅协议，允许LLM智能体基于声明的意图而非预定义拓扑交换消息。包含两个核心覆盖层：1) 反应式订阅，使智能体能够动态优化其意图；2) 贝叶斯声誉系统，为每个智能体提供本地监控机制来检测和隔离恶意节点。

Result: 在五个基准测试上的广泛实验表明，RAPS设计有效地在统一的多智能体协调框架中实现了适应性、可扩展性和鲁棒性的平衡。

Conclusion: RAPS通过声誉感知的发布-订阅范式，为解决LLM多智能体协调中的动态自适应、可扩展和鲁棒性问题提供了一个有效的统一框架。

Abstract: Multi-agent architectures built on large language models (LLMs) have demonstrated the potential to realize swarm intelligence through well-crafted collaboration. However, the substantial burden of manual orchestration inherently raises an imperative to automate the design of agentic workflows. We frame such an agent coordination challenge as a classic problem in dynamic ad-hoc networking: How to establish adaptive and reliable communication among a scalable number of agentic hosts? In response to this unresolved dilemma, we introduce RAPS, a reputation-aware publish-subscribe paradigm for adaptive, scalable, and robust coordination of LLM agents. RAPS is grounded in the Distributed Publish-Subscribe Protocol, allowing LLM agents to exchange messages based on their declared intents rather than predefined topologies. Beyond this substrate, RAPS further incorporates two coherent overlays: (i) Reactive Subscription, enabling agents to dynamically refine their intents; and (ii) Bayesian Reputation, empowering each agent with a local watchdog to detect and isolate malicious peers. Extensive experiments over five benchmarks showcase that our design effectively reconciles adaptivity, scalability, and robustness in a unified multi-agent coordination framework.

</details>


### [62] [Small Agent Group is the Future of Digital Health](https://arxiv.org/abs/2602.08013)
*Yuqiao Meng,Luoxi Tang,Dazheng Zhang,Rafael Brens,Elvys J. Romero,Nancy Guo,Safa Elkefi,Zhaohan Xi*

Main category: cs.AI

TL;DR: 本文提出小型智能体群（SAG）作为替代传统大语言模型缩放范式的临床推理方法，通过分布式协作推理在保持效果的同时提升可靠性和降低部署成本。


<details>
  <summary>Details</summary>
Motivation: 当前数字健康领域过度依赖"规模优先"的大语言模型范式，但临床实际需求不仅包括效果，还需要可靠性和合理的部署成本。临床决策本质上是协作过程，因此需要挑战单一模型缩放模式。

Method: 提出小型智能体群（SAG）方法，将单一模型智能转变为集体专业知识，通过协作审议过程分配推理、循证分析和关键审核任务。

Result: SAG在效果、可靠性和部署成本等多维度临床指标评估中表现优于单一大型模型，无论是否使用额外优化或检索增强生成技术。

Conclusion: SAG的协同推理可以替代模型参数增长，为数字健康提供更平衡效果、可靠性和部署效率的可扩展解决方案。

Abstract: The rapid adoption of large language models (LLMs) in digital health has been driven by a "scaling-first" philosophy, i.e., the assumption that clinical intelligence increases with model size and data. However, real-world clinical needs include not only effectiveness, but also reliability and reasonable deployment cost. Since clinical decision-making is inherently collaborative, we challenge the monolithic scaling paradigm and ask whether a Small Agent Group (SAG) can support better clinical reasoning. SAG shifts from single-model intelligence to collective expertise by distributing reasoning, evidence-based analysis, and critical audit through a collaborative deliberation process. To assess the clinical utility of SAG, we conduct extensive evaluations using diverse clinical metrics spanning effectiveness, reliability, and deployment cost. Our results show that SAG achieves superior performance compared to a single giant model, both with and without additional optimization or retrieval-augmented generation. These findings suggest that the synergistic reasoning represented by SAG can substitute for model parameter growth in clinical settings. Overall, SAG offers a scalable solution to digital health that better balances effectiveness, reliability, and deployment efficiency.

</details>


### [63] [Free(): Learning to Forget in Malloc-Only Reasoning Models](https://arxiv.org/abs/2602.08030)
*Yilun Zheng,Dongyang Ma,Tian Liang,Jiahao Xu,Xinting Huang,Lihui Chen,Haitao Mi,Yan Wang*

Main category: cs.AI

TL;DR: Free()LM通过引入可遗忘机制解决推理模型过度思考导致性能下降的问题，在多种规模模型上实现稳定提升，在长时任务中尤其有效。


<details>
  <summary>Details</summary>
Motivation: 现有推理模型存在"过度思考悖论"：过多的推理token反而会降低性能。作者认为这是因为标准LLM像"只分配不释放"的内存管理器，不断累积有效和冗余步骤，缺乏修剪过时信息的机制。

Method: 提出Free()LM模型，通过Free-Module（即插即用的LoRA适配器）引入内在的自遗忘能力。模型在推理模式和清理模式之间迭代切换，动态识别并修剪无用的上下文块，保持紧凑且无噪声的状态。

Result: 在8B到685B的所有模型规模上都取得了一致改进，平均比顶级推理基线提升3.3%。在IMOanswerBench上使用DeepSeek V3.2-Speciale建立了新的SOTA。在长时任务中，标准Qwen3-235B-A22B模型完全崩溃（0%准确率），而Free()LM将性能恢复到50%。

Conclusion: 可持续的智能不仅需要思考的能力，也需要遗忘的自由。Free()LM通过引入遗忘机制有效解决了推理模型的过度思考问题。

Abstract: Reasoning models enhance problem-solving by scaling test-time compute, yet they face a critical paradox: excessive thinking tokens often degrade performance rather than improve it. We attribute this to a fundamental architectural flaw: standard LLMs operate as "malloc-only" engines, continuously accumulating valid and redundant steps alike without a mechanism to prune obsolete information. To break this cycle, we propose Free()LM, a model that introduces an intrinsic self-forgetting capability via the Free-Module, a plug-and-play LoRA adapter. By iteratively switching between reasoning and cleaning modes, Free()LM dynamically identifies and prunes useless context chunks, maintaining a compact and noise-free state.
  Extensive experiments show that Free()LM provides consistent improvements across all model scales (8B to 685B). It achieves a 3.3% average improvement over top-tier reasoning baselines, even establishing a new SOTA on IMOanswerBench using DeepSeek V3.2-Speciale. Most notably, in long-horizon tasks where the standard Qwen3-235B-A22B model suffers a total collapse (0% accuracy), Free()LM restores performance to 50%. Our findings suggest that sustainable intelligence requires the freedom to forget as much as the power to think.

</details>


### [64] [Securing Dual-Use Pathogen Data of Concern](https://arxiv.org/abs/2602.08061)
*Doni Bloomfield,Allison Berke,Moritz S. Hanke,Aaron Maiwald,James R. M. Black,Toby Webster,Tina Hernandez-Boussard,Oliver M. Crook,Jassi Pannu*

Main category: cs.AI

TL;DR: 提出五级生物安全数据框架（BDL），根据数据对AI生物安全风险的贡献程度分类病原体数据，并设计相应技术限制和治理框架


<details>
  <summary>Details</summary>
Motivation: AI生物模型训练数据与其能力密切相关，包括生物安全风险能力。为预防AI被用于生物武器开发等有害应用，需要设计数据控制措施

Method: 引入五级生物安全数据框架（BDL），根据数据对AI生物安全风险能力的预期贡献程度分类病原体数据；为每个BDL层级提出相应的技术限制；设计新型治理框架用于新创建的双重用途病原体数据

Result: 建立了系统化的病原体数据分类框架，为不同风险级别的数据设计了具体的技术控制措施，并提出了相应的治理机制

Conclusion: 在计算和编码资源广泛可及的世界中，数据控制可能是减少生物AI风险能力扩散的最有效干预措施之一

Abstract: Training data is an essential input into creating competent artificial intelligence (AI) models. AI models for biology are trained on large volumes of data, including data related to biological sequences, structures, images, and functions. The type of data used to train a model is intimately tied to the capabilities it ultimately possesses--including those of biosecurity concern. For this reason, an international group of more than 100 researchers at the recent 50th anniversary Asilomar Conference endorsed data controls to prevent the use of AI for harmful applications such as bioweapons development. To help design such controls, we introduce a five-tier Biosecurity Data Level (BDL) framework for categorizing pathogen data. Each level contains specific data types, based on their expected ability to contribute to capabilities of concern when used to train AI models. For each BDL tier, we propose technical restrictions appropriate to its level of risk. Finally, we outline a novel governance framework for newly created dual-use pathogen data. In a world with widely accessible computational and coding resources, data controls may be among the most high-leverage interventions available to reduce the proliferation of concerning biological AI capabilities.

</details>


### [65] [Objective Decoupling in Social Reinforcement Learning: Recovering Ground Truth from Sycophantic Majorities](https://arxiv.org/abs/2602.08092)
*Majid Ghasemi,Mark Crowley*

Main category: cs.AI

TL;DR: 本文挑战了强化学习中"人类反馈是真实信号"的第四教条，发现在社交环境中该假设失效，提出了"目标解耦"问题，并引入"认知源对齐"方法来解决多数评估者偏见下的对齐问题。


<details>
  <summary>Details</summary>
Motivation: 当前AI对齐策略基于一个脆弱的前提：人类反馈虽然是嘈杂的，但本质上仍然是真实的信号。本文认为这是强化学习的第四教条，并指出在社交环境中，评估者可能是阿谀奉承、懒惰或有敌意的，这一假设不再成立。

Method: 提出了认知源对齐方法，与依赖统计共识的传统鲁棒方法不同，ESA使用稀疏安全公理来判断反馈的来源而非信号本身。这种"评判评判者"的机制能够保证收敛到真实目标，即使多数评估者存在偏见。

Result: 理论上证明了在第四教条下，标准RL智能体会遭受目标解耦的结构性故障模式，智能体学习的目标会永久偏离潜在真实目标。实验表明，传统共识方法在多数共谋下失败，而ESA方法成功恢复了最优策略。

Conclusion: 人类反馈作为真实信号的假设在社交环境中是脆弱的，需要新的对齐方法。认知源对齐通过判断反馈来源而非依赖统计共识，能够在多数评估者存在偏见的情况下保证收敛到真实目标，为解决AI对齐问题提供了新思路。

Abstract: Contemporary AI alignment strategies rely on a fragile premise: that human feedback, while noisy, remains a fundamentally truthful signal. In this paper, we identify this assumption as Dogma 4 of Reinforcement Learning (RL). We demonstrate that while this dogma holds in static environments, it fails in social settings where evaluators may be sycophantic, lazy, or adversarial. We prove that under Dogma 4, standard RL agents suffer from what we call Objective Decoupling, a structural failure mode where the agent's learned objective permanently separates from the latent ground truth, guaranteeing convergence to misalignment. To resolve this, we propose Epistemic Source Alignment (ESA). Unlike standard robust methods that rely on statistical consensus (trusting the majority), ESA utilizes sparse safety axioms to judge the source of the feedback rather than the signal itself. We prove that this "judging the judges" mechanism guarantees convergence to the true objective, even when a majority of evaluators are biased. Empirically, we show that while traditional consensus methods fail under majority collusion, our approach successfully recovers the optimal policy.

</details>


### [66] [Interpretable Failure Analysis in Multi-Agent Reinforcement Learning Systems](https://arxiv.org/abs/2602.08104)
*Risal Shahriar Shefin,Debashis Gupta,Thai Le,Sarra Alqahtani*

Main category: cs.AI

TL;DR: 提出基于梯度的两阶段框架，用于多智能体强化学习中的可解释故障检测与溯源，包括识别初始故障源、验证多米诺效应和追踪故障传播路径。


<details>
  <summary>Details</summary>
Motivation: 多智能体强化学习在安全关键领域应用日益广泛，但缺乏可解释的故障检测和归因方法。现有方法多为黑盒检测，难以解释故障传播机制和检测异常。

Method: 两阶段梯度框架：第一阶段通过策略梯度成本的泰勒余项分析进行可解释的智能体级故障检测，确定初始故障候选；第二阶段通过批评者导数的几何分析（一阶敏感性和二阶曲率）构建可解释的传染图，验证故障传播路径。

Result: 在Simple Spread（3和5智能体）和StarCraft II环境中评估，使用MADDPG和HATRPO算法，实现了88.2-99.4%的初始故障源检测准确率，并提供检测决策的几何证据。

Conclusion: 该框架超越了黑盒检测，提供了梯度层面的可解释法医分析，为安全关键多智能体系统中的级联故障诊断提供了实用工具。

Abstract: Multi-Agent Reinforcement Learning (MARL) is increasingly deployed in safety-critical domains, yet methods for interpretable failure detection and attribution remain underdeveloped. We introduce a two-stage gradient-based framework that provides interpretable diagnostics for three critical failure analysis tasks: (1) detecting the true initial failure source (Patient-0); (2) validating why non-attacked agents may be flagged first due to domino effects; and (3) tracing how failures propagate through learned coordination pathways. Stage 1 performs interpretable per-agent failure detection via Taylor-remainder analysis of policy-gradient costs, declaring an initial Patient-0 candidate at the first threshold crossing. Stage 2 provides validation through geometric analysis of critic derivatives-first-order sensitivity and directional second-order curvature aggregated over causal windows to construct interpretable contagion graphs. This approach explains "downstream-first" detection anomalies by revealing pathways that amplify upstream deviations. Evaluated across 500 episodes in Simple Spread (3 and 5 agents) and 100 episodes in StarCraft II using MADDPG and HATRPO, our method achieves 88.2-99.4% Patient-0 detection accuracy while providing interpretable geometric evidence for detection decisions. By moving beyond black-box detection to interpretable gradient-level forensics, this framework offers practical tools for diagnosing cascading failures in safety-critical MARL systems.

</details>


### [67] [Weak-Driven Learning: How Weak Agents make Strong Agents Stronger](https://arxiv.org/abs/2602.08222)
*Zehao Chen,Gongxun Li,Tianxiang Ai,Yifei Li,Zixuan Huang,Wang Zhou,Fuzhen Zhuang,Xianglong Liu,Jianxin Li,Deqing Wang,Yikun Ban*

Main category: cs.AI

TL;DR: WMSS利用模型自身历史弱检查点来指导持续优化，通过熵动态识别可恢复的学习差距并进行补偿学习，突破后训练饱和瓶颈


<details>
  <summary>Details</summary>
Motivation: 大型语言模型后训练中存在持续饱和瓶颈：模型变得高度自信后，进一步训练收益递减。现有方法继续强化目标预测，但研究发现信息丰富的监督信号仍潜藏在模型自身的历史弱状态中

Method: 提出WMSS（弱智能体能使强智能体更强）后训练范式，利用弱检查点指导持续优化。通过熵动态识别可恢复的学习差距，并通过补偿学习强化这些差距

Result: 在数学推理和代码生成数据集上的实验表明，使用该方法训练的智能体实现了有效的性能提升，同时不产生额外的推理成本

Conclusion: WMSS能够使强智能体超越传统后训练饱和限制，利用模型自身历史弱状态中的监督信号实现持续优化

Abstract: As post-training optimization becomes central to improving large language models, we observe a persistent saturation bottleneck: once models grow highly confident, further training yields diminishing returns. While existing methods continue to reinforce target predictions, we find that informative supervision signals remain latent in models' own historical weak states. Motivated by this observation, we propose WMSS (Weak Agents Can Make Strong Agents Stronger), a post-training paradigm that leverages weak checkpoints to guide continued optimization. By identifying recoverable learning gaps via entropy dynamics and reinforcing them through compensatory learning, WMSS enables strong agents to improve beyond conventional post-training saturation. Experiments on mathematical reasoning and code generation datasets show that agents trained with our approach achieve effective performance improvements, while incurring zero additional inference cost.

</details>


### [68] [PTS-SNN: A Prompt-Tuned Temporal Shift Spiking Neural Networks for Efficient Speech Emotion Recognition](https://arxiv.org/abs/2602.08240)
*Xun Su,Huamin Wang,Qi Zhang*

Main category: cs.AI

TL;DR: 提出PTS-SNN框架，通过提示调优解决SSL表示与SNN动态之间的分布不匹配问题，实现高效语音情感识别


<details>
  <summary>Details</summary>
Motivation: 传统语音情感识别模型计算成本高，难以在资源受限的边缘设备部署。SNN虽能效高，但与连续自监督学习表示存在分布不匹配问题，高动态范围嵌入会降低阈值神经元的编码能力。

Method: 提出PTS-SNN框架：1) 使用时移脉冲编码器通过无参数通道移位捕获局部时间依赖性；2) 设计上下文感知膜电位校准策略，利用脉冲稀疏线性注意力模块聚合全局语义上下文到可学习的软提示中，动态调节PLIF神经元的偏置电压，将异质输入分布居中于响应放电范围内。

Result: 在五个多语言数据集上实验，PTS-SNN在IEMOCAP上达到73.34%准确率，与竞争性ANN相当，仅需1.19M可训练参数和每样本0.35 mJ推理能耗。

Conclusion: PTS-SNN通过参数高效的神经形态适应框架，成功解决了SSL表示与SNN动态之间的分布不匹配问题，为资源受限边缘设备上的高效语音情感识别提供了可行方案。

Abstract: Speech Emotion Recognition (SER) is widely deployed in Human-Computer Interaction, yet the high computational cost of conventional models hinders their implementation on resource-constrained edge devices. Spiking Neural Networks (SNNs) offer an energy-efficient alternative due to their event-driven nature; however, their integration with continuous Self-Supervised Learning (SSL) representations is fundamentally challenged by distribution mismatch, where high-dynamic-range embeddings degrade the information coding capacity of threshold-based neurons. To resolve this, we propose Prompt-Tuned Spiking Neural Networks (PTS-SNN), a parameter-efficient neuromorphic adaptation framework that aligns frozen SSL backbones with spiking dynamics. Specifically, we introduce a Temporal Shift Spiking Encoder to capture local temporal dependencies via parameter-free channel shifts, establishing a stable feature basis. To bridge the domain gap, we devise a Context-Aware Membrane Potential Calibration strategy. This mechanism leverages a Spiking Sparse Linear Attention module to aggregate global semantic context into learnable soft prompts, which dynamically regulate the bias voltages of Parametric Leaky Integrate-and-Fire (PLIF) neurons. This regulation effectively centers the heterogeneous input distribution within the responsive firing range, mitigating functional silence or saturation. Extensive experiments on five multilingual datasets (e.g., IEMOCAP, CASIA, EMODB) demonstrate that PTS-SNN achieves 73.34\% accuracy on IEMOCAP, comparable to competitive Artificial Neural Networks (ANNs), while requiring only 1.19M trainable parameters and 0.35 mJ inference energy per sample.

</details>


### [69] [Do MLLMs Really See It: Reinforcing Visual Attention in Multimodal LLMs](https://arxiv.org/abs/2602.08241)
*Siqu Ou,Tianrui Wan,Zhiyuan Zhao,Junyu Gao,Xuelong Li*

Main category: cs.AI

TL;DR: SAYO是一个通过强化学习框架训练的多模态视觉推理模型，引入基于区域视觉注意力的奖励机制，解决现有MLLMs视觉注意力不稳定的问题。


<details>
  <summary>Details</summary>
Motivation: 现有基于思维链的多模态大语言模型在复杂推理任务中主要依赖长文本推理轨迹，缺乏学习稳定视觉注意力策略的机制。研究发现当前MLLMs存在视觉注意力薄弱的问题：早期视觉对齐错误很少在后续推理中得到纠正，导致错误传播和推理失败。这种限制源于训练过程中对视觉注意力的信用分配不足。

Method: 提出SAYO模型，采用强化学习框架进行训练，引入基于区域视觉注意力的奖励机制。这种奖励明确地将优化信号与基于视觉的推理步骤对齐，使模型能够学习更可靠的注意力行为。

Result: 在多个多模态基准测试上的广泛实验表明，SAYO在多样化的推理和感知任务上持续提升性能。

Conclusion: 通过强化学习框架和区域级视觉注意力奖励机制，SAYO能够有效解决现有MLLMs视觉注意力不稳定的问题，提升多模态推理的准确性和可靠性。

Abstract: While chain-of-thought (CoT) reasoning has substantially improved multimodal large language models (MLLMs) on complex reasoning tasks, existing approaches largely rely on long textual reasoning trajectories and provide limited mechanisms for learning stable visual attention policies. Our analysis shows that current MLLMs exhibit weak visual focus: early-stage visual misalignment is rarely corrected during subsequent reasoning, leading to error propagation and failed inferences. We argue that this limitation stems from inadequate credit assignment for visual attention during training. To address this issue, we propose SAYO, a visual reasoning model trained with a reinforcement learning (RL) framework that introduces a region-level visual attention-based reward. This reward explicitly aligns optimization signals with visually grounded reasoning steps, enabling the model to learn more reliable attention behaviors. Extensive experiments across multiple multimodal benchmarks demonstrate that SAYO consistently improves performance on diverse reasoning and perception tasks.

</details>


### [70] [G-LNS: Generative Large Neighborhood Search for LLM-Based Automatic Heuristic Design](https://arxiv.org/abs/2602.08253)
*Baoyun Zhao,He Wang,Liang Zeng*

Main category: cs.AI

TL;DR: G-LNS：一个基于LLM的生成式进化框架，用于自动设计大邻域搜索算子，通过协同进化破坏和修复算子对，在组合优化问题上超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的自动启发式设计方法通常局限于构造性优先级规则或参数化局部搜索指导，限制了搜索空间，难以在复杂组合优化问题中跳出深度局部最优。

Method: 提出G-LNS框架，利用LLM协同进化紧密耦合的破坏和修复算子对，通过合作评估机制捕捉算子间的交互，发现能够有效进行结构破坏和重建的互补算子逻辑。

Result: 在旅行商问题和带容量车辆路径问题等基准测试中，G-LNS显著优于基于LLM的自动启发式设计方法和经典求解器，发现的启发式方法能以更少计算资源获得接近最优解，并在未见实例分布上表现出鲁棒泛化能力。

Conclusion: G-LNS成功将LLM驱动的自动启发式设计扩展到更灵活的大邻域搜索算子设计，通过协同进化破坏-修复算子对实现了更有效的结构探索，为复杂组合优化问题提供了新的自动化求解途径。

Abstract: While Large Language Models (LLMs) have recently shown promise in Automated Heuristic Design (AHD), existing approaches typically formulate AHD around constructive priority rules or parameterized local search guidance, thereby restricting the search space to fixed heuristic forms. Such designs offer limited capacity for structural exploration, making it difficult to escape deep local optima in complex Combinatorial Optimization Problems (COPs). In this work, we propose G-LNS, a generative evolutionary framework that extends LLM-based AHD to the automated design of Large Neighborhood Search (LNS) operators. Unlike prior methods that evolve heuristics in isolation, G-LNS leverages LLMs to co-evolve tightly coupled pairs of destroy and repair operators. A cooperative evaluation mechanism explicitly captures their interaction, enabling the discovery of complementary operator logic that jointly performs effective structural disruption and reconstruction. Extensive experiments on challenging COP benchmarks, such as Traveling Salesman Problems (TSP) and Capacitated Vehicle Routing Problems (CVRP), demonstrate that G-LNS significantly outperforms LLM-based AHD methods as well as strong classical solvers. The discovered heuristics not only achieve near-optimal solutions with reduced computational budgets but also exhibit robust generalization across diverse and unseen instance distributions.

</details>


### [71] [SynthAgent: A Multi-Agent LLM Framework for Realistic Patient Simulation -- A Case Study in Obesity with Mental Health Comorbidities](https://arxiv.org/abs/2602.08254)
*Arman Aghaee,Sepehr Asgarian,Jouhyun Jeon*

Main category: cs.AI

TL;DR: SynthAgent是一个多智能体系统框架，用于模拟肥胖症合并精神障碍患者，通过整合临床数据和文献构建个性化虚拟患者，模拟疾病进展和治疗反应。


<details>
  <summary>Details</summary>
Motivation: 真实世界数据存在碎片化、偏见和隐私限制等问题，模拟高保真患者为解决这些挑战提供了有力途径，特别是在研究复杂疾病如肥胖症合并精神障碍时。

Method: SynthAgent是一个多智能体系统框架，整合了理赔数据、人口调查和以患者为中心的文献等临床医学证据，构建具有人格特质的个性化虚拟患者，通过自主智能体交互模拟疾病进展、治疗反应和生活管理。

Result: 评估了100多个生成的虚拟患者，发现GPT-5和Claude 4.5 Sonnet作为核心引擎在提出的MAS框架中实现了最高的保真度，优于Gemini 2.5 Pro和DeepSeek-R1。

Conclusion: SynthAgent提供了一个可扩展且保护隐私的框架，用于探索医学和心理领域的患者旅程、行为动态和决策过程。

Abstract: Simulating high-fidelity patients offers a powerful avenue for studying complex diseases while addressing the challenges of fragmented, biased, and privacy-restricted real-world data. In this study, we introduce SynthAgent, a novel Multi-Agent System (MAS) framework designed to model obesity patients with comorbid mental disorders, including depression, anxiety, social phobia, and binge eating disorder. SynthAgent integrates clinical and medical evidence from claims data, population surveys, and patient-centered literature to construct personalized virtual patients enriched with personality traits that influence adherence, emotion regulation, and lifestyle behaviors. Through autonomous agent interactions, the system simulates disease progression, treatment response, and life management across diverse psychosocial contexts. Evaluation of more than 100 generated patients demonstrated that GPT-5 and Claude 4.5 Sonnet achieved the highest fidelity as the core engine in the proposed MAS framework, outperforming Gemini 2.5 Pro and DeepSeek-R1. SynthAgent thus provides a scalable and privacy-preserving framework for exploring patient journeys, behavioral dynamics, and decision-making processes in both medical and psychological domains.

</details>


### [72] [Puda: Private User Dataset Agent for User-Sovereign and Privacy-Preserving Personalized AI](https://arxiv.org/abs/2602.08268)
*Akinori Maeda,Yuto Sekiya,Sota Sugimura,Tomoya Asai,Yu Tsuda,Kohei Ikeda,Hiroshi Fujii,Kohei Watanabe*

Main category: cs.AI

TL;DR: Puda是一个用户主权架构，通过聚合跨服务数据并支持客户端管理，在三个隐私级别控制数据共享，在个性化AI服务中平衡隐私保护与数据利用。


<details>
  <summary>Details</summary>
Motivation: 当前主流平台的数据集中化形成了数据孤岛，限制了用户主权并阻碍跨服务数据使用。同时，基于大语言模型的智能体快速发展，对需要动态提供多样化个人数据的高度个性化服务需求激增，这带来了在数据利用与隐私保护之间平衡的挑战。

Method: 提出Puda（Private User Dataset Agent）架构，作为浏览器系统实现，支持三个隐私级别的数据共享控制：详细浏览历史、提取的关键词、预定义类别子集。系统作为跨服务的通用平台，通过个性化旅行规划任务进行评估。

Result: 评估结果显示，提供预定义类别子集能达到分享详细浏览历史所获个性化性能的97.2%（通过LLM-as-a-Judge框架在三个标准下评估）。这表明Puda能有效实现多粒度管理，为缓解隐私-个性化权衡提供实用选择。

Conclusion: Puda为AI原生环境下的用户主权提供了基础框架，使用户能够安全地利用个性化AI的全部潜力，通过多粒度数据控制有效平衡隐私保护与个性化需求。

Abstract: Personal data centralization among dominant platform providers including search engines, social networking services, and e-commerce has created siloed ecosystems that restrict user sovereignty, thereby impeding data use across services. Meanwhile, the rapid proliferation of Large Language Model (LLM)-based agents has intensified demand for highly personalized services that require the dynamic provision of diverse personal data. This presents a significant challenge: balancing the utilization of such data with privacy protection. To address this challenge, we propose Puda (Private User Dataset Agent), a user-sovereign architecture that aggregates data across services and enables client-side management. Puda allows users to control data sharing at three privacy levels: (i) Detailed Browsing History, (ii) Extracted Keywords, and (iii) Predefined Category Subsets. We implemented Puda as a browser-based system that serves as a common platform across diverse services and evaluated it through a personalized travel planning task. Our results show that providing Predefined Category Subsets achieves 97.2% of the personalization performance (evaluated via an LLM-as-a-Judge framework across three criteria) obtained when sharing Detailed Browsing History. These findings demonstrate that Puda enables effective multi-granularity management, offering practical choices to mitigate the privacy-personalization trade-off. Overall, Puda provides an AI-native foundation for user sovereignty, empowering users to safely leverage the full potential of personalized AI.

</details>


### [73] [Toward Formalizing LLM-Based Agent Designs through Structural Context Modeling and Semantic Dynamics Analysis](https://arxiv.org/abs/2602.08276)
*Haoyu Jia,Kento Kawaharazuka,Kei Okada*

Main category: cs.AI

TL;DR: 本文提出了一个用于分析和比较LLM智能体的形式化模型——结构上下文模型，以及配套的声明式实现框架和可持续的智能体工程工作流，解决了当前LLM智能体研究碎片化的问题。


<details>
  <summary>Details</summary>
Motivation: 当前LLM智能体研究存在碎片化问题，概念框架和方法论原则经常与底层实现细节交织在一起，导致读者和作者在表面不同的概念中迷失方向。这种碎片化主要源于缺乏一个可分析的、自洽的形式化模型来支持实现无关的LLM智能体表征和比较。

Method: 提出了结构上下文模型作为分析和比较LLM智能体的形式化模型，基于此引入了两个互补组件：1）声明式实现框架；2）可持续的智能体工程工作流——语义动态分析。该工作流提供了对智能体机制的原则性洞察，并支持快速、系统的设计迭代。

Result: 在动态变体的猴子-香蕉问题上展示了完整框架的有效性，使用该方法设计的智能体在最具挑战性的设置中实现了高达32个百分点的成功率提升。

Conclusion: 提出的结构上下文模型及相关框架和工作流为解决LLM智能体研究的碎片化问题提供了系统化的解决方案，能够支持更高效、更系统的智能体设计和开发。

Abstract: Current research on large language model (LLM) agents is fragmented: discussions of conceptual frameworks and methodological principles are frequently intertwined with low-level implementation details, causing both readers and authors to lose track amid a proliferation of superficially distinct concepts. We argue that this fragmentation largely stems from the absence of an analyzable, self-consistent formal model that enables implementation-independent characterization and comparison of LLM agents. To address this gap, we propose the \texttt{Structural Context Model}, a formal model for analyzing and comparing LLM agents from the perspective of context structure. Building upon this foundation, we introduce two complementary components that together span the full lifecycle of LLM agent research and development: (1) a declarative implementation framework; and (2) a sustainable agent engineering workflow, \texttt{Semantic Dynamics Analysis}. The proposed workflow provides principled insights into agent mechanisms and supports rapid, systematic design iteration. We demonstrate the effectiveness of the complete framework on dynamic variants of the monkey-banana problem, where agents engineered using our approach achieve up to a 32 percentage points improvement in success rate on the most challenging setting.

</details>


### [74] [Moral Sycophancy in Vision Language Models](https://arxiv.org/abs/2602.08311)
*Shadman Rabby,Md. Hefzul Hossain Papon,Sabbir Ahmed,Nokimul Hasan Arif,A. B. M. Ashikur Rahman,Irfan Ahmad*

Main category: cs.AI

TL;DR: 该研究首次系统分析了视觉语言模型中的道德谄媚行为，发现VLMs在用户意见影响下会牺牲道德准确性，存在从正确到错误判断的不对称转变，且不同数据集表现出不同的道德鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 先前研究探索了视觉语言模型在一般情境下的谄媚行为，但其对基于道德的视觉决策的影响尚未得到充分理解。本研究旨在填补这一空白，系统分析VLMs中的道德谄媚现象。

Method: 在Moralise和M^3oralBench数据集上评估了10个广泛使用的视觉语言模型，在用户明确反对的情境下分析其行为。使用错误引入率（EIR）和错误纠正率（ECR）进行量化评估。

Result: VLMs经常在初始判断正确的情况下产生道德错误的后续回应；存在明显不对称性：模型更倾向于从道德正确转向错误判断；不同数据集表现差异显著；存在权衡：纠错能力强的模型引入更多推理错误，保守模型则纠错能力有限。

Conclusion: 视觉语言模型对道德影响具有脆弱性，初始道德正确立场会引发更强的谄媚行为。需要制定原则性策略来提高多模态AI系统的伦理一致性和鲁棒性。

Abstract: Sycophancy in Vision-Language Models (VLMs) refers to their tendency to align with user opinions, often at the expense of moral or factual accuracy. While prior studies have explored sycophantic behavior in general contexts, its impact on morally grounded visual decision-making remains insufficiently understood. To address this gap, we present the first systematic study of moral sycophancy in VLMs, analyzing ten widely-used models on the Moralise and M^3oralBench datasets under explicit user disagreement. Our results reveal that VLMs frequently produce morally incorrect follow-up responses even when their initial judgments are correct, and exhibit a consistent asymmetry: models are more likely to shift from morally right to morally wrong judgments than the reverse when exposed to user-induced bias. Follow-up prompts generally degrade performance on Moralise, while yielding mixed or even improved accuracy on M^3oralBench, highlighting dataset-dependent differences in moral robustness. Evaluation using Error Introduction Rate (EIR) and Error Correction Rate (ECR) reveals a clear trade-off: models with stronger error-correction capabilities tend to introduce more reasoning errors, whereas more conservative models minimize errors but exhibit limited ability to self-correct. Finally, initial contexts with a morally right stance elicit stronger sycophantic behavior, emphasizing the vulnerability of VLMs to moral influence and the need for principled strategies to improve ethical consistency and robustness in multimodal AI systems.

</details>


### [75] [Who Deserves the Reward? SHARP: Shapley Credit-based Optimization for Multi-Agent System](https://arxiv.org/abs/2602.08335)
*Yanming Li,Xuelin Zhang,WenJie Lu,Ziye Tang,Maodong Wu,Haotian Luo,Tongtong Wu,Zijie Peng,Hongze Mi,Yibo Feng,Naiqiang Tan,Chao Huang,Hong Chen,Li Shen*

Main category: cs.AI

TL;DR: SHARP框架通过基于Shapley值的分层信用分配机制，优化多智能体强化学习，解决LLM与外部工具集成中的信用分配难题，显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 当前将大语言模型与外部工具通过多智能体系统集成的训练面临信用分配挑战，难以确定具体功能智能体对决策轨迹成功或失败的责任，现有方法依赖稀疏或全局广播奖励，无法捕捉个体贡献，导致强化学习效率低下。

Method: 提出SHARP框架，通过基于Shapley值的分层信用分配机制稳定训练，包括：1) 全局广播准确性奖励；2) 基于Shapley值的每个智能体边际信用奖励；3) 工具过程奖励以提高执行效率。通过轨迹组间智能体特定优势归一化实现精确信用分配。

Result: 在多个真实世界基准测试中，SHARP显著优于最新基线方法，相比单智能体方法平均匹配改进23.66%，相比多智能体方法平均匹配改进14.05%。

Conclusion: SHARP框架通过精确的信用分配机制有效解决了多智能体强化学习中的训练稳定性问题，为大语言模型与外部工具集成提供了有效的优化方案。

Abstract: Integrating Large Language Models (LLMs) with external tools via multi-agent systems offers a promising new paradigm for decomposing and solving complex problems. However, training these systems remains notoriously difficult due to the credit assignment challenge, as it is often unclear which specific functional agent is responsible for the success or failure of decision trajectories. Existing methods typically rely on sparse or globally broadcast rewards, failing to capture individual contributions and leading to inefficient reinforcement learning. To address these limitations, we introduce the Shapley-based Hierarchical Attribution for Reinforcement Policy (SHARP), a novel framework for optimizing multi-agent reinforcement learning via precise credit attribution. SHARP effectively stabilizes training by normalizing agent-specific advantages across trajectory groups, primarily through a decomposed reward mechanism comprising a global broadcast-accuracy reward, a Shapley-based marginal-credit reward for each agent, and a tool-process reward to improve execution efficiency. Extensive experiments across various real-world benchmarks demonstrate that SHARP significantly outperforms recent state-of-the-art baselines, achieving average match improvements of 23.66% and 14.05% over single-agent and multi-agent approaches, respectively.

</details>


### [76] [CoTZero: Annotation-Free Human-Like Vision Reasoning via Hierarchical Synthetic CoT](https://arxiv.org/abs/2602.08339)
*Chengyi Du,Yazhe Niu,Dazhong Shen,Luxin Xu*

Main category: cs.AI

TL;DR: CoTZero：一种无需标注的视觉语言模型训练范式，通过双阶段数据合成和认知对齐训练，提升视觉推理的逻辑连贯性和可验证性。


<details>
  <summary>Details</summary>
Motivation: 当前视觉语言模型主要依赖表面相关性而非逻辑连贯的结构化表示，导致高层语义结构和因果关系的理解不足，限制了组合性和可验证推理能力。

Method: 提出CoTZero框架，包含两个核心组件：1）双阶段数据合成方法（自底向上提取视觉基元并组合，自顶向下通过全局结构指导局部细节解释）；2）认知对齐训练方法，在强化微调中引入认知一致可验证奖励机制。

Result: 在多级语义不一致基准测试中达到83.33%的F1分数，在域内和域外设置下均表现优异，消融实验证实各组件对提升可解释性和人类对齐推理均有贡献。

Conclusion: CoTZero通过引入人类认知模型到推理过程中，显著提升了视觉语言模型的逻辑连贯性和结构化表示能力，为实现更接近人类水平的视觉推理提供了有效途径。

Abstract: Recent advances in vision-language models (VLMs) have markedly improved image-text alignment, yet they still fall short of human-like visual reasoning. A key limitation is that many VLMs rely on surface correlations rather than building logically coherent structured representations, which often leads to missed higher-level semantic structure and non-causal relational understanding, hindering compositional and verifiable reasoning. To address these limitations by introducing human models into the reasoning process, we propose CoTZero, an annotation-free paradigm with two components: (i) a dual-stage data synthesis approach and (ii) a cognition-aligned training method. In the first component, we draw inspiration from neurocognitive accounts of compositional productivity and global-to-local analysis. In the bottom-up stage, CoTZero extracts atomic visual primitives and incrementally composes them into diverse, structured question-reasoning forms. In the top-down stage, it enforces hierarchical reasoning by using coarse global structure to guide the interpretation of local details and causal relations. In the cognition-aligned training component, built on the synthesized CoT data, we introduce Cognitively Coherent Verifiable Rewards (CCVR) in Reinforcement Fine-Tuning (RFT) to further strengthen VLMs' hierarchical reasoning and generalization, providing stepwise feedback on reasoning coherence and factual correctness. Experiments show that CoTZero achieves an F1 score of 83.33 percent on our multi-level semantic inconsistency benchmark with lexical-perturbation negatives, across both in-domain and out-of-domain settings. Ablations confirm that each component contributes to more interpretable and human-aligned visual reasoning.

</details>


### [77] [OPE: Overcoming Information Saturation in Parallel Thinking via Outline-Guided Path Exploration](https://arxiv.org/abs/2602.08344)
*Qi Guo,Jianing Wang,Deyang Kong,Xiangyu Xi,Jianfei Zhang,Yi Lu,Jingang Wang,Wei Wang,Shikun Zhang,Wei Ye*

Main category: cs.AI

TL;DR: 本文提出了一种名为Outline-Guided Path Exploration (OPE)的新方法，通过生成多样化的推理大纲来引导并行路径探索，解决了并行思维中路径间信息冗余的问题，显著提升了大型推理模型在复杂数学问题上的性能。


<details>
  <summary>Details</summary>
Motivation: 现有并行思维方法主要关注聚合阶段的优化，而忽视了路径探索阶段。研究发现，在强化学习可验证奖励（RLVR）设置下，探索路径间的互信息瓶颈从根本上限制了整体性能。因此需要一种方法来减少信息冗余，提高路径探索的多样性。

Method: 提出Outline-Guided Path Exploration (OPE)方法：1）在并行路径推理前生成多样化的推理大纲，显式划分解空间；2）采用迭代强化学习策略，分别优化大纲规划和大纲引导推理；3）通过减少信息冗余提高探索路径的信息多样性。

Result: 在多个具有挑战性的数学基准测试上进行广泛实验，结果表明OPE有效提升了不同聚合策略下的推理性能，使大型推理模型能够更可靠地发现正确解决方案。

Conclusion: OPE通过显式生成多样化推理大纲来引导并行路径探索，解决了路径间信息冗余问题，为并行思维优化提供了新的有效方法，显著提升了大型推理模型在复杂问题上的表现。

Abstract: Parallel thinking has emerged as a new paradigm for large reasoning models (LRMs) in tackling complex problems. Recent methods leverage Reinforcement Learning (RL) to enhance parallel thinking, aiming to address the limitations in computational resources and effectiveness encountered with supervised fine-tuning. However, most existing studies primarily focus on optimizing the aggregation phase, with limited attention to the path exploration stage. In this paper, we theoretically analyze the optimization of parallel thinking under the Reinforcement Learning with Verifiable Rewards (RLVR) setting, and identify that the mutual information bottleneck among exploration paths fundamentally restricts overall performance. To address this, we propose Outline-Guided Path Exploration (OPE), which explicitly partitions the solution space by generating diverse reasoning outlines prior to parallel path reasoning, thereby reducing information redundancy and improving the diversity of information captured across exploration paths. We implement OPE with an iterative RL strategy that optimizes outline planning and outline-guided reasoning independently. Extensive experiments across multiple challenging mathematical benchmarks demonstrate that OPE effectively improves reasoning performance in different aggregation strategies, enabling LRMs to more reliably discover correct solutions.

</details>


### [78] [Towards Better Evolution Modeling for Temporal Knowledge Graphs](https://arxiv.org/abs/2602.08353)
*Zhang Jiasheng,Li Zhangpin,Wang Mingzhe,Shao Jie,Cui Jiangtao,Li Hui*

Main category: cs.AI

TL;DR: 现有时序知识图谱基准存在严重缺陷：仅通过统计共现就能达到接近SOTA的性能，无需使用任何时序信息。作者分析了问题的根源，提出了新的TKG演化基准。


<details>
  <summary>Details</summary>
Motivation: 现有TKG预测模型虽然取得了令人印象深刻的结果（如YAGO数据集上Hits@10超过0.9），但研究发现这些基准无意中引入了捷径。仅通过统计共现就能达到接近SOTA的性能，无需使用任何时序信息，这表明现有基准无法公平评估模型的真实能力。

Method: 作者分析了问题的根本原因，识别出当前数据集中的固有偏差和评估任务的过度简化形式。通过分析进一步揭示了现有基准的更多局限性，包括时间间隔知识的不合理格式化、忽略知识过时学习、以及精确演化理解信息不足等。基于此，作者提出了TKG演化基准，包含四个偏差校正的数据集和两个与演化过程紧密相关的新任务。

Result: 研究发现现有基准存在严重缺陷：1）可以通过简单的共现统计达到高性能；2）时间间隔知识格式化不合理；3）忽略知识过时学习；4）精确演化理解信息不足。这些问题放大了捷径效应，阻碍了公平评估。

Conclusion: 现有TKG基准存在系统性缺陷，无法准确评估模型对时序演化的理解能力。作者提出的TKG演化基准通过偏差校正的数据集和更贴近演化过程的任务，为TKG演化建模提供了更准确的评估框架，促进对TKG演化挑战的更准确理解。

Abstract: Temporal knowledge graphs (TKGs) structurally preserve evolving human knowledge. Recent research has focused on designing models to learn the evolutionary nature of TKGs to predict future facts, achieving impressive results. For instance, Hits@10 scores over 0.9 on YAGO dataset. However, we find that existing benchmarks inadvertently introduce a shortcut. Near state-of-the-art performance can be simply achieved by counting co-occurrences, without using any temporal information. In this work, we examine the root cause of this issue, identifying inherent biases in current datasets and over simplified form of evaluation task that can be exploited by these biases. Through this analysis, we further uncover additional limitations of existing benchmarks, including unreasonable formatting of time-interval knowledge, ignorance of learning knowledge obsolescence, and insufficient information for precise evolution understanding, all of which can amplify the shortcut and hinder a fair assessment. Therefore, we introduce the TKG evolution benchmark. It includes four bias-corrected datasets and two novel tasks closely aligned with the evolution process, promoting a more accurate understanding of the challenges in TKG evolution modeling. Benchmark is available at: https://github.com/zjs123/TKG-Benchmark.

</details>


### [79] [Does Your Reasoning Model Implicitly Know When to Stop Thinking?](https://arxiv.org/abs/2602.08354)
*Zixuan Huang,Xin Xia,Yuxi Ren,Jianbin Zheng,Xuanda Wang,Zhixia Zhang,Hongyan Xie,Songshi Liang,Zehao Chen,Xuefeng Xiao,Fuzhen Zhuang,Jianxin Li,Yikun Ban,Deqing Wang*

Main category: cs.AI

TL;DR: 论文提出SAGE（自我感知引导高效推理）新采样范式，利用大推理模型内在的"知道何时停止思考"能力，解决长思维链冗余问题，显著提升推理准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 当前大推理模型使用长思维链方法存在严重冗余问题，损害计算效率并导致实时应用延迟。研究发现更长的推理链与正确性无关甚至有害，而模型本身隐含知道何时停止思考的能力被当前采样范式掩盖。

Method: 提出SAGE采样范式，释放模型内在的高效推理潜力。进一步将SAGE作为混合采样整合到基于群体的强化学习中（SAGE-RL），使SAGE-RL能够将SAGE发现的高效推理模式融入标准pass@1推理。

Result: SAGE-RL在多个具有挑战性的数学基准测试中显著提升了大推理模型的推理准确性和效率。

Conclusion: 通过利用大推理模型内在的"知道何时停止思考"能力，SAGE采样范式能够有效解决长思维链冗余问题，为高效推理提供新方向。

Abstract: Recent advancements in large reasoning models (LRMs) have greatly improved their capabilities on complex reasoning tasks through Long Chains of Thought (CoTs). However, this approach often results in substantial redundancy, impairing computational efficiency and causing significant delays in real-time applications. Recent studies show that longer reasoning chains are frequently uncorrelated with correctness and can even be detrimental to accuracy. In a further in-depth analysis of this phenomenon, we surprisingly uncover and empirically verify that LRMs implicitly know the appropriate time to stop thinking, while this capability is obscured by current sampling paradigms. Motivated by this, we introduce SAGE (Self-Aware Guided Efficient Reasoning), a novel sampling paradigm that unleashes this efficient reasoning potential. Furthermore, integrating SAGE as mixed sampling into group-based reinforcement learning (SAGE-RL) enables SAGE-RL to effectively incorporate SAGE-discovered efficient reasoning patterns into standard pass@1 inference, markedly enhancing both the reasoning accuracy and efficiency of LRMs across multiple challenging mathematical benchmarks.

</details>


### [80] [Circuit Representations of Random Forests with Applications to XAI](https://arxiv.org/abs/2602.08362)
*Chunxi Ji,Adnan Darwiche*

Main category: cs.AI

TL;DR: 将随机森林分类器编译为电路集的方法，用于高效计算决策的完整原因、鲁棒性和最短翻转路径


<details>
  <summary>Details</summary>
Motivation: 现有方法在将随机森林转换为可解释电路方面效率不足，需要更高效的方法来计算决策的完整原因、鲁棒性和解释

Method: 1. 提出将随机森林分类器编译为电路集的方法，每个电路直接编码分类器中某个类别的实例；2. 利用该方法获得可处理电路，用于计算决策的完整和一般原因；3. 提出计算决策鲁棒性和所有最短翻转路径的算法

Result: 提出的方法比现有类似方法显著更高效；能够枚举所有充分原因、必要原因和对比解释；计算决策鲁棒性；识别随机森林决策的所有最短翻转路径

Conclusion: 该方法为随机森林分类器提供了高效的电路编译技术，支持多种解释性分析任务，包括原因枚举、鲁棒性计算和决策翻转路径识别

Abstract: We make three contributions in this paper. First, we present an approach for compiling a random forest classifier into a set of circuits, where each circuit directly encodes the instances in some class of the classifier. We show empirically that our proposed approach is significantly more efficient than existing similar approaches. Next, we utilize this approach to further obtain circuits that are tractable for computing the complete and general reasons of a decision, which are instance abstractions that play a fundamental role in computing explanations. Finally, we propose algorithms for computing the robustness of a decision and all shortest ways to flip it. We illustrate the utility of our contributions by using them to enumerate all sufficient reasons, necessary reasons and contrastive explanations of decisions; to compute the robustness of decisions; and to identify all shortest ways to flip the decisions made by random forest classifiers learned from a wide range of datasets.

</details>


### [81] [MemAdapter: Fast Alignment across Agent Memory Paradigms via Generative Subgraph Retrieval](https://arxiv.org/abs/2602.08369)
*Xin Zhang,Kailai Yang,Chenyue Li,Hao Li,Qiyu Wei,Jun'ichi Tsujii,Sophia Ananiadou*

Main category: cs.AI

TL;DR: MemAdapter是一个统一异构内存范式的记忆检索框架，通过两阶段训练实现跨范式快速对齐，显著降低对齐成本并提升检索性能。


<details>
  <summary>Details</summary>
Motivation: 现有智能体记忆系统通常设计在孤立范式（显式、参数化或潜在记忆）中，检索方法紧密耦合，阻碍了跨范式泛化和融合。需要统一异构内存范式。

Method: 提出MemAdapter框架：1）从统一记忆空间训练生成式子图检索器；2）通过对比学习训练轻量级对齐模块，使检索器适应未见内存范式。采用两阶段训练策略。

Result: 在三个公共评估基准上，生成式子图检索器在三种内存范式和智能体模型规模上持续优于五个强记忆系统。跨范式对齐仅需13分钟（单GPU），使用不到5%的训练计算量即超越原始检索器性能。支持跨范式零样本融合。

Conclusion: MemAdapter作为即插即用解决方案，统一了异构内存范式，实现了快速、低成本的跨范式对齐和融合，显著提升了智能体记忆系统的灵活性和性能。

Abstract: Memory mechanism is a core component of LLM-based agents, enabling reasoning and knowledge discovery over long-horizon contexts. Existing agent memory systems are typically designed within isolated paradigms (e.g., explicit, parametric, or latent memory) with tightly coupled retrieval methods that hinder cross-paradigm generalization and fusion. In this work, we take a first step toward unifying heterogeneous memory paradigms within a single memory system. We propose MemAdapter, a memory retrieval framework that enables fast alignment across agent memory paradigms. MemAdapter adopts a two-stage training strategy: (1) training a generative subgraph retriever from the unified memory space, and (2) adapting the retriever to unseen memory paradigms by training a lightweight alignment module through contrastive learning. This design improves the flexibility for memory retrieval and substantially reduces alignment cost across paradigms. Comprehensive experiments on three public evaluation benchmarks demonstrate that the generative subgraph retriever consistently outperforms five strong agent memory systems across three memory paradigms and agent model scales. Notably, MemAdapter completes cross-paradigm alignment within 13 minutes on a single GPU, achieving superior performance over original memory retrievers with less than 5% of training compute. Furthermore, MemAdapter enables effective zero-shot fusion across memory paradigms, highlighting its potential as a plug-and-play solution for agent memory systems.

</details>


### [82] [Grounding Generative Planners in Verifiable Logic: A Hybrid Architecture for Trustworthy Embodied AI](https://arxiv.org/abs/2602.08373)
*Feiyu Wu,Xu Zheng,Yue Qu,Zhuocheng Wang,Zicheng Feng,Hui Li*

Main category: cs.AI

TL;DR: VIRF是一个神经符号框架，通过逻辑导师与LLM规划器的对话机制，实现可验证的安全规划，在家庭安全任务中达到0%危险行动率和77.3%目标达成率。


<details>
  <summary>Details</summary>
Motivation: 当前LLM作为具身AI规划器存在随机性，缺乏形式化推理，无法提供严格的安全保证。现有方法要么依赖不可靠的LLM进行安全检查，要么简单地拒绝不安全计划而不提供修复方案。

Method: 提出可验证迭代精炼框架(VIRF)，采用神经符号架构，核心是导师-学徒对话机制：基于形式化安全本体的确定性逻辑导师为LLM规划器提供因果性和教学性反馈，实现智能计划修复而非简单避免。还引入了从现实世界文档合成安全知识库的可扩展知识获取管道。

Result: 在具有挑战性的家庭安全任务中，VIRF实现了0%的危险行动率(HAR)和77.3%的目标达成率(GCR)，在所有基线方法中最高。平均仅需1.1次修正迭代，效率很高。

Conclusion: VIRF展示了构建根本上可信且可验证安全的具身智能体的原则性途径，从被动的安全把关转向主动协作，实现了智能计划修复而非简单拒绝。

Abstract: Large Language Models (LLMs) show promise as planners for embodied AI, but their stochastic nature lacks formal reasoning, preventing strict safety guarantees for physical deployment. Current approaches often rely on unreliable LLMs for safety checks or simply reject unsafe plans without offering repairs. We introduce the Verifiable Iterative Refinement Framework (VIRF), a neuro-symbolic architecture that shifts the paradigm from passive safety gatekeeping to active collaboration. Our core contribution is a tutor-apprentice dialogue where a deterministic Logic Tutor, grounded in a formal safety ontology, provides causal and pedagogical feedback to an LLM planner. This enables intelligent plan repairs rather than mere avoidance. We also introduce a scalable knowledge acquisition pipeline that synthesizes safety knowledge bases from real-world documents, correcting blind spots in existing benchmarks. In challenging home safety tasks, VIRF achieves a perfect 0 percent Hazardous Action Rate (HAR) and a 77.3 percent Goal-Condition Rate (GCR), which is the highest among all baselines. It is highly efficient, requiring only 1.1 correction iterations on average. VIRF demonstrates a principled pathway toward building fundamentally trustworthy and verifiably safe embodied agents.

</details>


### [83] [Reinforcement Inference: Leveraging Uncertainty for Self-Correcting Language Model Reasoning](https://arxiv.org/abs/2602.08520)
*Xinhai Sun*

Main category: cs.AI

TL;DR: 提出Reinforcement Inference方法，利用模型自身的不确定性选择性地进行第二次推理，无需重新训练即可提升大语言模型在确定性解码下的性能表现。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型通常在一次性贪婪推理协议下评估和部署，这种机制会系统性地低估模型的真实能力，因为许多错误并非源于知识缺失，而是内部模糊情况下的过早决策。

Method: 引入Reinforcement Inference方法，这是一种基于熵的推理时控制策略，利用模型自身的不确定性来选择性调用第二次更慎重的推理尝试，无需任何重新训练。

Result: 在MMLU-Pro的12,032个问题上，使用DeepSeek-v3.2模型，Reinforcement Inference将准确率从60.72%提升到84.03%，仅增加61.06%的推理调用。100%重新询问的消融实验达到84.35%，表明不确定性感知选择能以更少计算获得大部分可实现的改进。

Conclusion: 该方法不仅提供了实用的推理时升级，还提出了更广泛的基于熵的模型能力测量和扩展范式。一次性贪婪推理与不确定性条件化深思之间的差距为诊断LLM的潜在推理视野提供了视角，并激励未来训练目标明确约束正确性-置信度对齐。

Abstract: Modern large language models (LLMs) are often evaluated and deployed under a \emph{one-shot, greedy} inference protocol, especially in professional settings that require deterministic behavior. This regime can systematically under-estimate a fixed model's true capability: many errors arise not from missing knowledge, but from premature commitment under internal ambiguity. We introduce \emph{Reinforcement Inference}, an entropy-aware inference-time control strategy that uses the model's own uncertainty to selectively invoke a second, more deliberate reasoning attempt, enabling stronger performance \emph{without any retraining}.
  On 12,032 MMLU-Pro questions across 14 subjects, using DeepSeek-v3.2 with deterministic decoding in a zero-shot setting, Reinforcement Inference improves accuracy from 60.72\% to 84.03\%, while only incurring 61.06\% additional inference calls. A 100\% re-asking ablation reaches 84.35\%, indicating that uncertainty-aware selection captures most of the attainable improvement with substantially less compute. Moreover, a \emph{prompt-only} ablation underperforms the baseline, suggesting that the gains are not explained by generic `` your output had high entropy, think step-by-step'' prompting alone.
  Beyond providing a practical inference-time upgrade, our results suggest a broader \emph{entropy-aware} paradigm for measuring and expanding model capability: because modern decoder-based models generate outputs autoregressively, entropy and related confidence measures arise naturally as first-class control signals during generation. The resulting gap between one-pass greedy inference and uncertainty-conditioned deliberation offers a diagnostic lens on an LLM's latent reasoning horizon and motivates future training objectives that explicitly constrain correctness--confidence alignment.

</details>


### [84] [An Attention Mechanism for Robust Multimodal Integration in a Global Workspace Architecture](https://arxiv.org/abs/2602.08597)
*Roland Bertin-Johannet,Lara Scipio,Leopold Maytié,Rufin VanRullen*

Main category: cs.AI

TL;DR: 该论文提出了一种用于全局工作空间理论（GWT）的顶部注意机制，以选择多模态系统中的相关模态，提高了噪声鲁棒性并在MM-IMDb基准上达到先进水平。


<details>
  <summary>Details</summary>
Motivation: 全局工作空间理论（GWT）作为认知神经科学启发的框架，虽然已被用于多模态表示，但其注意机制研究不足。研究者希望开发一种顶部注意机制来改进GWT在多模态集成中的性能。

Method: 提出了一种顶部注意机制，用于在全局工作空间内选择相关模态。该方法在两个复杂度递增的多模态数据集（Simple Shapes和MM-IMDb 1.0）上进行评估，并与现有多模态注意模型进行比较。

Result: 1）注意机制提高了全局工作空间系统的噪声鲁棒性；2）展示了跨任务和跨模态的泛化能力；3）在MM-IMDb 1.0基准测试中，该机制使全局工作空间达到与当前最先进方法竞争的水平。

Conclusion: 提出的顶部注意机制有效增强了全局工作空间理论在多模态集成中的性能，不仅提高了噪声鲁棒性，还展现了独特的泛化能力，使GWT在多模态基准上具有竞争力。

Abstract: Global Workspace Theory (GWT), inspired by cognitive neuroscience, posits that flexible cognition could arise via the attentional selection of a relevant subset of modalities within a multimodal integration system. This cognitive framework can inspire novel computational architectures for multimodal integration. Indeed, recent implementations of GWT have explored its multimodal representation capabilities, but the related attention mechanisms remain understudied. Here, we propose and evaluate a top-down attention mechanism to select modalities inside a global workspace. First, we demonstrate that our attention mechanism improves noise robustness of a global workspace system on two multimodal datasets of increasing complexity: Simple Shapes and MM-IMDb 1.0. Second, we highlight various cross-task and cross-modality generalization capabilities that are not shared by multimodal attention models from the literature. Comparing against existing baselines on the MM-IMDb 1.0 benchmark, we find our attention mechanism makes the global workspace competitive with the state of the art.

</details>


### [85] [OSCAR: Optimization-Steered Agentic Planning for Composed Image Retrieval](https://arxiv.org/abs/2602.08603)
*Teng Wang,Rong Shan,Jianghao Lin,Junjie Wu,Tianyi Xu,Jianping Zhang,Wenteng Chen,Changwang Zhang,Zhaoxiang Wang,Weinan Zhang,Jun Wang*

Main category: cs.AI

TL;DR: OSCAR是一个用于组合图像检索的优化引导智能体规划框架，将启发式搜索过程转化为轨迹优化问题，通过离线-在线范式实现更优的检索性能。


<details>
  <summary>Details</summary>
Motivation: 现有组合图像检索方法存在两种问题：统一嵌入检索存在单一模型近视问题，启发式智能体检索受限于次优的试错编排。需要一种更原则性的方法来处理异构视觉和文本约束的复杂推理。

Method: 提出OSCAR框架，采用离线-在线范式：离线阶段将CIR建模为两阶段混合整数规划问题，通过布尔集合运算推导最大化真实覆盖的最优轨迹；在线阶段使用这些轨迹作为上下文示例来引导VLM规划器。

Result: 在三个公共基准和一个私有工业基准上的实验表明，OSCAR始终优于SOTA基线方法，仅使用10%的训练数据就能达到优越性能，证明了规划逻辑的强泛化能力而非数据集特定记忆。

Conclusion: OSCAR首次将智能体CIR从启发式搜索过程重新表述为原则性轨迹优化问题，通过数学推导的最优轨迹和离线-在线范式，实现了更有效的组合图像检索。

Abstract: Composed image retrieval (CIR) requires complex reasoning over heterogeneous visual and textual constraints. Existing approaches largely fall into two paradigms: unified embedding retrieval, which suffers from single-model myopia, and heuristic agentic retrieval, which is limited by suboptimal, trial-and-error orchestration. To this end, we propose OSCAR, an optimization-steered agentic planning framework for composed image retrieval. We are the first to reformulate agentic CIR from a heuristic search process into a principled trajectory optimization problem. Instead of relying on heuristic trial-and-error exploration, OSCAR employs a novel offline-online paradigm. In the offline phase, we model CIR via atomic retrieval selection and composition as a two-stage mixed-integer programming problem, mathematically deriving optimal trajectories that maximize ground-truth coverage for training samples via rigorous boolean set operations. These trajectories are then stored in a golden library to serve as in-context demonstrations for online steering of VLM planner at online inference time. Extensive experiments on three public benchmarks and a private industrial benchmark show that OSCAR consistently outperforms SOTA baselines. Notably, it achieves superior performance using only 10% of training data, demonstrating strong generalization of planning logic rather than dataset-specific memorization.

</details>


### [86] [Debate is efficient with your time](https://arxiv.org/abs/2602.08630)
*Jonah Brown-Cohen,Geoffrey Irving,Simon C. Marshall,Ilan Newman,Georgios Piliouras,Mario Szegedy*

Main category: cs.AI

TL;DR: 该论文研究了AI安全辩论中的人类监督成本，引入了辩论查询复杂度(DQC)来衡量验证者需要检查辩论记录的最小比特数，发现PSPACE/poly问题类恰好是O(log n)查询可判定的函数类，表明辩论具有极高的查询效率。


<details>
  <summary>Details</summary>
Motivation: AI安全辩论使用两个竞争模型帮助人类法官验证复杂计算任务，但之前的研究只建立了辩论在理论上能解决什么问题，没有分析人类监督的实际成本：法官需要查询辩论记录多少次？

Method: 引入辩论查询复杂度(DQC)作为衡量指标，分析不同复杂度类别函数的DQC特性，建立DQC与电路复杂度之间的联系。

Result: 发现PSPACE/poly恰好是O(log n)查询可判定的函数类；证明依赖于所有输入比特的函数需要Ω(log n)查询；任何可由大小为s的电路计算的函数满足DQC(f) ≤ log(s) + 3。

Conclusion: 辩论具有惊人的查询效率，即使对于高度复杂的问题，对数级监督就足够了。证明P类语言中DQC下界为log(n) + 6将产生新的电路下界，将辩论查询复杂度与电路复杂度的核心问题联系起来。

Abstract: AI safety via debate uses two competing models to help a human judge verify complex computational tasks. Previous work has established what problems debate can solve in principle, but has not analysed the practical cost of human oversight: how many queries must the judge make to the debate transcript? We introduce Debate Query Complexity}(DQC), the minimum number of bits a verifier must inspect to correctly decide a debate.
  Surprisingly, we find that PSPACE/poly (the class of problems which debate can efficiently decide) is precisely the class of functions decidable with O(log n) queries. This characterisation shows that debate is remarkably query-efficient: even for highly complex problems, logarithmic oversight suffices. We also establish that functions depending on all their input bits require Omega(log n) queries, and that any function computable by a circuit of size s satisfies DQC(f) <= log(s) + 3. Interestingly, this last result implies that proving DQC lower bounds of log(n) + 6 for languages in P would yield new circuit lower bounds, connecting debate query complexity to central questions in circuit complexity.

</details>


### [87] [Why do we Trust Chatbots? From Normative Principles to Behavioral Drivers](https://arxiv.org/abs/2602.08707)
*Aditya Gulati,Nuria Oliver*

Main category: cs.AI

TL;DR: 论文探讨聊天机器人信任机制，指出用户信任常源于设计选择而非系统可信度，建议将聊天机器人视为销售人员而非伴侣，强调需要区分心理信任形成与规范性可信度


<details>
  <summary>Details</summary>
Motivation: 随着聊天机器人模糊自动化系统与人类对话的界限，需要更仔细地审视这些系统的信任基础。当前监管和政策框架倾向于从规范性角度定义信任，但用户对聊天机器人的信任往往源于行为机制，这种信任通常不是通过证明可信度获得，而是通过利用认知偏见影响用户行为的设计选择形成

Method: 基于观察提出理论框架：将聊天机器人重新定义为高度熟练的销售人员，其目标由部署组织决定。分析竞争性"信任"概念共存于同一术语下的问题，区分心理信任形成与规范性可信度之间的重要区别

Result: 发现用户对聊天机器人的信任往往不是基于系统可信度，而是通过交互设计选择塑造，这些设计利用了认知偏见来影响用户行为。指出"信任"一词掩盖了心理信任形成与规范性可信度之间的重要区别

Conclusion: 需要进一步研究和更强有力的支持机制，帮助用户适当校准对对话AI系统的信任。解决这一差距需要区分心理信任形成与规范性可信度，并认识到聊天机器人作为"销售人员"的角色定位

Abstract: As chatbots increasingly blur the boundary between automated systems and human conversation, the foundations of trust in these systems warrant closer examination. While regulatory and policy frameworks tend to define trust in normative terms, the trust users place in chatbots often emerges from behavioral mechanisms. In many cases, this trust is not earned through demonstrated trustworthiness but is instead shaped by interactional design choices that leverage cognitive biases to influence user behavior. Based on this observation, we propose reframing chatbots not as companions or assistants, but as highly skilled salespeople whose objectives are determined by the deploying organization. We argue that the coexistence of competing notions of "trust" under a shared term obscures important distinctions between psychological trust formation and normative trustworthiness. Addressing this gap requires further research and stronger support mechanisms to help users appropriately calibrate trust in conversational AI systems.

</details>


### [88] [Intermediate Results on the Complexity of STRIPS$_{1}^{1}$](https://arxiv.org/abs/2602.08708)
*Stefan Edelkamp,Jiří Fink,Petr Gregor,Anders Jonsson,Bernhard Nebel*

Main category: cs.AI

TL;DR: 该论文基于Bylander关于命题STRIPS规划计算复杂性的结果，研究了STRIPS₁¹（每个操作符只有一个前提条件和一个效果）的NP完备性问题，通过SAT求解器、文字图和Petri网映射来验证"小解假设"。


<details>
  <summary>Details</summary>
Motivation: Bylander已证明仅允许基础文字时，即使操作符限制为两个前提条件和两个后置条件，判定规划存在性也是PSPACE完全的。虽然NP难性已确定，但命题STRIPS中操作符只有一个前提条件和一个效果时是否为NP完全仍未知。本文旨在探究STRIPS₁¹的"小解假设"是否成立。

Method: 1) 对小规模实例调用SAT求解器；2) 引入文字图（literal graph）概念；3) 将问题映射到Petri网进行分析。

Result: 论文通过上述方法为STRIPS₁¹的NP完备性问题提供了新的见解，但具体结果需要阅读完整论文才能确定。

Conclusion: 该研究为命题STRIPS规划中操作符限制为一个前提条件和一个效果时的计算复杂性提供了新的分析框架和方法，有助于理解STRIPS₁¹的"小解假设"。

Abstract: This paper is based on Bylander's results on the computational complexity of propositional STRIPS planning. He showed that when only ground literals are permitted, determining plan existence is PSPACE-complete even if operators are limited to two preconditions and two postconditions. While NP-hardness is settled, it is unknown whether propositional STRIPS with operators that only have one precondition and one effect is NP-complete. We shed light on the question whether this small solution hypothesis for STRIPS$^1_1$ is true, calling a SAT solver for small instances, introducing the literal graph, and mapping it to Petri nets.

</details>


### [89] [Exploring SAIG Methods for an Objective Evaluation of XAI](https://arxiv.org/abs/2602.08715)
*Miquel Miró-Nicolau,Gabriel Moyà-Alcover,Anna Arias-Duart*

Main category: cs.AI

TL;DR: 本文首次系统综述了合成人工智能基准真值方法在可解释人工智能评估中的应用，提出了新的分类法，揭示了该领域缺乏共识的现状。


<details>
  <summary>Details</summary>
Motivation: 可解释人工智能评估领域方法多样且复杂，由于缺乏解释的基准真值，难以进行客观评估。合成人工智能基准真值方法通过生成人工基准真值来解决这一问题，但该领域缺乏系统性综述和分析。

Method: 本文首次对SAIG方法进行全面综述，提出了新的分类法来对这些方法进行分类，识别了区分不同SAIG方法的七个关键特征，并进行了比较研究。

Result: 研究发现，在可解释人工智能评估技术中缺乏共识，这凸显了该领域需要进一步研究和标准化的迫切需求。

Conclusion: SAIG方法为解决XAI评估中的基准真值缺失问题提供了有前景的方向，但该领域需要更多研究和标准化工作来建立有效的评估框架。

Abstract: The evaluation of eXplainable Artificial Intelligence (XAI) methods is a rapidly growing field, characterized by a wide variety of approaches. This diversity highlights the complexity of the XAI evaluation, which, unlike traditional AI assessment, lacks a universally correct ground truth for the explanation, making objective evaluation challenging. One promising direction to address this issue involves the use of what we term Synthetic Artificial Intelligence Ground truth (SAIG) methods, which generate artificial ground truths to enable the direct evaluation of XAI techniques. This paper presents the first review and analysis of SAIG methods. We introduce a novel taxonomy to classify these approaches, identifying seven key features that distinguish different SAIG methods. Our comparative study reveals a concerning lack of consensus on the most effective XAI evaluation techniques, underscoring the need for further research and standardization in this area.

</details>


### [90] [Belief Offloading in Human-AI Interaction](https://arxiv.org/abs/2602.08754)
*Rose E. Guingrich,Dvija Mehta,Umang Bhatt*

Main category: cs.AI

TL;DR: 论文探讨人类将信念形成过程外包给LLM的现象，即"信念外包"，分析其认知影响和规范意义


<details>
  <summary>Details</summary>
Motivation: 随着LLM聊天机器人成为人们的思维伙伴，认知卸载可能导致过度依赖并损害认知技能，需要研究这种特定类型的认知卸载现象

Method: 结合哲学、心理学和计算机科学研究，界定信念外包发生的边界条件，提供描述性分类法并分析其规范意义

Result: 定义了"信念外包"概念，建立了其边界条件和分类体系，分析了这种行为对人们信念系统和行为的潜在影响

Conclusion: 信念外包是人工智能交互中的重要现象，需要进一步研究其发生机制和后果，为未来工作提供方向

Abstract: What happens when people's beliefs are derived from information provided by an LLM? People's use of LLM chatbots as thought partners can contribute to cognitive offloading, which can have adverse effects on cognitive skills in cases of over-reliance. This paper defines and investigates a particular kind of cognitive offloading in human-AI interaction, "belief offloading," in which people's processes of forming and upholding beliefs are offloaded onto an AI system with downstream consequences on their behavior and the nature of their system of beliefs. Drawing on philosophy, psychology, and computer science research, we clarify the boundary conditions under which belief offloading occurs and provide a descriptive taxonomy of belief offloading and its normative implications. We close with directions for future work to assess the potential for and consequences of belief offloading in human-AI interaction.

</details>


### [91] [Dynamics Within Latent Chain-of-Thought: An Empirical Study of Causal Structure](https://arxiv.org/abs/2602.08783)
*Zirui Li,Xuefeng Bai,Kehai Chen,Yizhi Li,Jian Yang,Chenghua Lin,Min Zhang*

Main category: cs.AI

TL;DR: 论文研究了潜在思维链（latent CoT）的因果机制，通过结构因果模型分析潜在步骤，探索了推理过程中的关键问题：哪些步骤对正确性因果必要、影响如何传播、中间轨迹是否保留竞争答案模式。


<details>
  <summary>Details</summary>
Motivation: 现有潜在思维链方法用内部潜在步骤替代显式文本推理，但这些中间计算难以评估，只能通过相关性探测来理解。需要更深入地理解潜在推理过程的因果机制。

Method: 将潜在思维链视为表示空间中可操作的因果过程，用结构因果模型建模潜在步骤作为变量，通过逐步干预分析其效应。研究Coconut和CODI两种代表性范式在数学和通用推理任务上的表现。

Result: 发现潜在步骤预算不像同质的额外深度，更像具有非局部路由的分阶段功能；存在早期输出偏见与晚期表示承诺之间的持续差距；中间轨迹确实保留竞争答案模式。

Conclusion: 结果支持模式条件和稳定性感知分析作为更可靠的工具，用于解释和改进潜在推理系统，并提出了相应的训练/解码目标。

Abstract: Latent or continuous chain-of-thought methods replace explicit textual rationales with a number of internal latent steps, but these intermediate computations are difficult to evaluate beyond correlation-based probes. In this paper, we view latent chain-of-thought as a manipulable causal process in representation space by modeling latent steps as variables in a structural causal model (SCM) and analyzing their effects through step-wise $\mathrm{do}$-interventions. We study two representative paradigms (i.e., Coconut and CODI) on both mathematical and general reasoning tasks to investigate three key questions: (1) which steps are causally necessary for correctness and when answers become decidable early; (2) how does influence propagate across steps, and how does this structure compare to explicit CoT; and (3) do intermediate trajectories retain competing answer modes, and how does output-level commitment differ from representational commitment across steps. We find that latent-step budgets behave less like homogeneous extra depth and more like staged functionality with non-local routing, and we identify a persistent gap between early output bias and late representational commitment. These results motivate mode-conditional and stability-aware analyses -- and corresponding training/decoding objectives -- as more reliable tools for interpreting and improving latent reasoning systems.

</details>


### [92] [The Use of AI Tools to Develop and Validate Q-Matrices](https://arxiv.org/abs/2602.08796)
*Kevin Fan,Jacquelyn A. Bialo,Hongli Li*

Main category: cs.AI

TL;DR: AI工具在认知诊断建模中Q矩阵构建的应用研究：比较不同AI模型生成的Q矩阵与已验证Q矩阵的一致性，发现AI表现存在显著差异且随时间变化


<details>
  <summary>Details</summary>
Motivation: Q矩阵构建是认知诊断建模的关键但劳动密集型步骤，本研究旨在探索通用语言模型等AI工具是否能支持Q矩阵开发，减轻专家负担

Method: 使用多个AI模型（包括Google Gemini 2.5 Pro等）基于与人类专家相同的训练材料生成Q矩阵，将AI生成的Q矩阵与Li和Suen（2013）已验证的阅读测试Q矩阵进行比较，使用Cohen's kappa评估一致性，并在2026年1月进行后续分析比较新版本AI模型

Result: 不同AI模型表现差异显著，Google Gemini 2.5 Pro与已验证Q矩阵的一致性最高（Kappa=0.63），超过了所有人类专家；但2026年使用新版本AI的分析显示与已验证Q矩阵的一致性降低

Conclusion: AI工具在Q矩阵构建中具有潜力，但表现存在模型间差异且随时间变化，需要进一步研究AI在认知诊断建模中的可靠性和稳定性

Abstract: Constructing a Q-matrix is a critical but labor-intensive step in cognitive diagnostic modeling (CDM). This study investigates whether AI tools (i.e., general language models) can support Q-matrix development by comparing AI-generated Q-matrices with a validated Q-matrix from Li and Suen (2013) for a reading comprehension test. In May 2025, multiple AI models were provided with the same training materials as human experts. Agreement among AI-generated Q-matrices, the validated Q-matrix, and human raters' Q-matrices was assessed using Cohen's kappa. Results showed substantial variation across AI models, with Google Gemini 2.5 Pro achieving the highest agreement (Kappa = 0.63) with the validated Q-matrix, exceeding that of all human experts. A follow-up analysis in January 2026 using newer AI versions, however, revealed lower agreement with the validated Q-matrix. Implications and directions for future research are discussed.

</details>


### [93] [Root Cause Analysis Method Based on Large Language Models with Residual Connection Structures](https://arxiv.org/abs/2602.08804)
*Liming Zhou,Ailing Liu,Hongwei Liu,Min He,Heng Zhang*

Main category: cs.AI

TL;DR: RC-LLM：基于残差连接和大语言模型的微服务根因分析方法，通过多源遥测数据融合和上下文推理能力提升故障定位效果


<details>
  <summary>Details</summary>
Motivation: 微服务架构中根因定位面临挑战：复杂的故障传播机制和多源遥测数据（指标、日志、追踪）的高维度特性限制了现有RCA方法的有效性

Method: 提出RC-LLM方法：设计残差式层次融合结构整合多源遥测数据，利用大语言模型的上下文推理能力建模时序和跨微服务因果依赖关系

Result: 在CCF-AIOps微服务数据集上的实验结果表明，RC-LLM在根因分析中实现了较高的准确性和效率

Conclusion: RC-LLM方法通过结合残差连接结构和大语言模型，有效解决了复杂微服务架构中的根因定位问题，为故障诊断提供了新思路

Abstract: Root cause localization remain challenging in complex and large-scale microservice architectures. The complex fault propagation among microservices and the high dimensionality of telemetry data, including metrics, logs, and traces, limit the effectiveness of existing root cause analysis (RCA) methods. In this paper, a residual-connection-based RCA method using large language model (LLM), named RC-LLM, is proposed. A residual-like hierarchical fusion structure is designed to integrate multi-source telemetry data, while the contextual reasoning capability of large language models is leveraged to model temporal and cross-microservice causal dependencies. Experimental results on CCF-AIOps microservice datasets demonstrate that RC-LLM achieves strong accuracy and efficiency in root cause analysis.

</details>


### [94] [Negative-Aware Diffusion Process for Temporal Knowledge Graph Extrapolation](https://arxiv.org/abs/2602.08815)
*Yanglei Gan,Peng He,Yuxiang Cai,Run Lin,Guanyu Zhou,Qiao Liu*

Main category: cs.AI

TL;DR: NADEx是一种用于时序知识图谱推理的负感知扩散模型，通过结合负样本原型和余弦对齐正则化，在四个公开基准上实现了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型在时序知识图谱推理中存在两个主要问题：1) 生成路径仅基于正样本证据，忽略了信息丰富的负上下文；2) 训练目标以交叉熵排序为主，虽然改善了候选排序，但对去噪嵌入的校准监督不足。

Method: NADEx将实体、关系和时序间隔的以主体为中心的历史编码为序列嵌入，在前向过程中扰动查询对象，在反向过程中使用Transformer去噪器在时序关系上下文中重构。同时引入基于批次负样本原型的余弦对齐正则化器，收紧决策边界以排除不合理候选。

Result: 在四个公开时序知识图谱基准上的综合实验表明，NADEx实现了最先进的性能。

Conclusion: NADEx通过结合负样本感知和余弦对齐正则化，有效解决了现有扩散模型在时序知识图谱推理中的局限性，显著提升了预测性能。

Abstract: Temporal Knowledge Graph (TKG) reasoning seeks to predict future missing facts from historical evidence. While diffusion models (DM) have recently gained attention for their ability to capture complex predictive distributions, two gaps remain: (i) the generative path is conditioned only on positive evidence, overlooking informative negative context, and (ii) training objectives are dominated by cross-entropy ranking, which improves candidate ordering but provides little supervision over the calibration of the denoised embedding. To bridge this gap, we introduce Negative-Aware Diffusion model for TKG Extrapolation (NADEx). Specifically, NADEx encodes subject-centric histories of entities, relations and temporal intervals into sequential embeddings. NADEx perturbs the query object in the forward process and reconstructs it in reverse with a Transformer denoiser conditioned on the temporal-relational context. We further derive a cosine-alignment regularizer derived from batch-wise negative prototypes, which tightens the decision boundary against implausible candidates. Comprehensive experiments on four public TKG benchmarks demonstrate that NADEx delivers state-of-the-art performance.

</details>


### [95] [Learning the Value Systems of Societies with Preference-based Multi-objective Reinforcement Learning](https://arxiv.org/abs/2602.08835)
*Andrés Holgado-Sánchez,Peter Vamplew,Richard Dazeley,Sascha Ossowski,Holger Billhardt*

Main category: cs.AI

TL;DR: 该论文提出了一种基于聚类和偏好多目标强化学习的方法，用于学习社会智能体中的价值对齐模型和价值系统，以解决价值感知AI中价值操作化、多样性和可解释性的问题。


<details>
  <summary>Details</summary>
Motivation: 价值感知AI需要识别人类价值观并适应不同用户的价值系统，但价值操作化容易出错。价值观的社会性要求其表示能适应多用户，而价值系统既多样又存在群体模式。现有序列决策方法需要手动设计特征或缺乏价值可解释性和适应性。

Method: 提出基于聚类和偏好多目标强化学习(PbMORL)的算法，在马尔可夫决策过程中学习社会智能体的价值对齐模型和价值系统。联合学习社会衍生的价值对齐模型(groundings)和一组简洁表示不同用户群体的价值系统(聚类)。每个聚类包含代表成员价值偏好的价值系统和反映与该价值系统对齐行为的近似帕累托最优策略。

Result: 在两个包含人类价值的MDP环境中，评估了该方法与最先进的PbMORL算法和基线的性能对比。

Conclusion: 该方法能够学习社会智能体的价值对齐模型和价值系统，解决了价值操作化、多样性和可解释性的挑战，为价值感知AI提供了有效的技术框架。

Abstract: Value-aware AI should recognise human values and adapt to the value systems (value-based preferences) of different users. This requires operationalization of values, which can be prone to misspecification. The social nature of values demands their representation to adhere to multiple users while value systems are diverse, yet exhibit patterns among groups. In sequential decision making, efforts have been made towards personalization for different goals or values from demonstrations of diverse agents. However, these approaches demand manually designed features or lack value-based interpretability and/or adaptability to diverse user preferences.
  We propose algorithms for learning models of value alignment and value systems for a society of agents in Markov Decision Processes (MDPs), based on clustering and preference-based multi-objective reinforcement learning (PbMORL). We jointly learn socially-derived value alignment models (groundings) and a set of value systems that concisely represent different groups of users (clusters) in a society. Each cluster consists of a value system representing the value-based preferences of its members and an approximately Pareto-optimal policy that reflects behaviours aligned with this value system. We evaluate our method against a state-of-the-art PbMORL algorithm and baselines on two MDPs with human values.

</details>


### [96] [Deciding the Satisfiability of Combined Qualitative Constraint Networks](https://arxiv.org/abs/2602.08848)
*Quentin Cohen-Solal,Alexandre Niveau,Maroua Bouzid*

Main category: cs.AI

TL;DR: 本文提出了一个统一框架，用于整合多种定性形式主义的扩展和组合，包括多尺度推理、时间序列和松散集成，并研究了可满足性决策及其复杂性。


<details>
  <summary>Details</summary>
Motivation: 在人工智能的推理研究中，定性推理能够在信息不精确、不完整且缺乏数值的情况下推断新知识。然而，现有的定性形式主义定义排除了某些在组合背景下重要的形式主义，需要建立一个统一框架来整合各种扩展和组合形式。

Method: 提出了一个形式化框架，统一了多种定性形式主义的扩展和组合，包括多尺度推理、时间序列和松散集成。该框架不仅支持在这些组合和扩展背景下进行推理，还能以统一方式研究可满足性决策及其复杂性。特别地，建立了两个互补定理来保证可满足性决策的多项式时间复杂度。

Result: 通过提出的统一框架，恢复了尺寸-拓扑组合的已知结果，并将定性形式主义的主要定义推广到包含文献定义中排除但在组合背景下重要的定性形式主义。建立了两个互补定理，保证了可满足性决策的多项式时间复杂度。

Conclusion: 该研究提供了一个强大的统一框架，能够整合多种定性推理的扩展和组合形式，不仅扩展了定性形式主义的定义范围，还为可满足性决策提供了多项式时间复杂度的理论保证，对人工智能中的定性推理研究具有重要意义。

Abstract: Among the various forms of reasoning studied in the context of artificial intelligence, qualitative reasoning makes it possible to infer new knowledge in the context of imprecise, incomplete information without numerical values. In this paper, we propose a formal framework unifying several forms of extensions and combinations of qualitative formalisms, including multi-scale reasoning, temporal sequences, and loose integrations. This framework makes it possible to reason in the context of each of these combinations and extensions, but also to study in a unified way the satisfiability decision and its complexity. In particular, we establish two complementary theorems guaranteeing that the satisfiability decision is polynomial, and we use them to recover the known results of the size-topology combination. We also generalize the main definition of qualitative formalism to include qualitative formalisms excluded from the definitions of the literature, important in the context of combinations.

</details>


### [97] [Efficient and Stable Reinforcement Learning for Diffusion Language Models](https://arxiv.org/abs/2602.08905)
*Jiawei Liu,Xiting Wang,Yuanyuan Zhong,Defu Lian,Yu Yang*

Main category: cs.AI

TL;DR: STP框架通过空间剪枝和时间剪枝提高扩散大语言模型强化学习的效率和稳定性


<details>
  <summary>Details</summary>
Motivation: 强化学习对释放扩散大语言模型的复杂推理能力至关重要，但应用于dLLMs时面临效率和稳定性方面的独特挑战

Method: 提出时空剪枝框架，通过空间剪枝（使用静态先验约束探索空间）和时间剪枝（绕过冗余的后期细化步骤）压缩生成过程中的冗余

Result: 理论分析表明STP严格降低了对数似然估计的方差，确保更稳定的策略更新；实验证明STP在效率和准确性上都超越了最先进的基线方法

Conclusion: STP框架有效解决了dLLMs强化学习中的效率和稳定性问题，为扩散大语言模型的强化学习训练提供了更优的解决方案

Abstract: Reinforcement Learning (RL) is crucial for unlocking the complex reasoning capabilities of Diffusion-based Large Language Models (dLLMs). However, applying RL to dLLMs faces unique challenges in efficiency and stability. To address these challenges, we propose Spatio-Temporal Pruning (STP), a framework designed to simultaneously improve the efficiency and stability of RL for dLLMs. STP compresses the redundancy in the generative process through: (1) \textit{spatial pruning}, which constrains the exploration space using static priors; and (2) \textit{temporal pruning}, which bypasses redundant late-stage refinement steps. Our theoretical analysis demonstrates that STP strictly reduces the variance of the log-likelihood estimation, thereby ensuring more stable policy updates. Extensive experiments demonstrate that STP surpasses state-of-the-art baselines in both efficiency and accuracy. Our code is available at https://github.com/Lolo1222/STP.

</details>


### [98] [CausalT5K: Diagnosing and Informing Refusal for Trustworthy Causal Reasoning of Skepticism, Sycophancy, Detection-Correction, and Rung Collapse](https://arxiv.org/abs/2602.08939)
*Longling Geng,Andy Ouyang,Theodore Wu,Daphne Barretto,Matthew John Hayes,Rachael Cooper,Yuqiao Zeng,Sameer Vijay,Gia Ancone,Ankit Rai,Matthew Wolfman,Patrick Flanagan,Edward Y. Chang*

Main category: cs.AI

TL;DR: CausalT5K是一个包含5000多个案例的诊断基准，用于系统检测LLM在因果推理中的失败模式，包括阶梯坍塌、阿谀性漂移和错误校准的拒绝，通过现实叙事嵌入因果陷阱，将性能分解为效用和安全性。


<details>
  <summary>Details</summary>
Motivation: LLM在因果推理中存在阶梯坍塌、阿谀性漂移和错误校准的拒绝等失败模式，但由于缺乏能够进行系统诊断的基准，相关改进进展缓慢。

Method: 开发了包含5000多个案例的CausalT5K诊断基准，涵盖10个领域，通过40名领域专家参与的人机协作流程、迭代交叉验证周期，以及基于规则、LLM和人工评分的复合验证方法实现。

Result: 初步实验揭示了四象限控制景观，显示静态审计策略普遍失败，证明了CausalT5K在推进可信推理系统方面的价值。

Conclusion: CausalT5K作为研究基础设施，实现了Pearl的因果阶梯，能够揭示聚合准确性无法看到的失败模式，为推进可信推理系统提供了重要工具。

Abstract: LLM failures in causal reasoning, including sycophancy, rung collapse, and miscalibrated refusal, are well-documented, yet progress on remediation is slow because no benchmark enables systematic diagnosis. We introduce CausalT5K, a diagnostic benchmark of over 5,000 cases across 10 domains that tests three critical capabilities: (1) detecting rung collapse, where models answer interventional queries with associational evidence; (2) resisting sycophantic drift under adversarial pressure; and (3) generating Wise Refusals that specify missing information when evidence is underdetermined. Unlike synthetic benchmarks, CausalT5K embeds causal traps in realistic narratives and decomposes performance into Utility (sensitivity) and Safety (specificity), revealing failure modes invisible to aggregate accuracy. Developed through a rigorous human-machine collaborative pipeline involving 40 domain experts, iterative cross-validation cycles, and composite verification via rule-based, LLM, and human scoring, CausalT5K implements Pearl's Ladder of Causation as research infrastructure. Preliminary experiments reveal a Four-Quadrant Control Landscape where static audit policies universally fail, a finding that demonstrates CausalT5K's value for advancing trustworthy reasoning systems. Repository: https://github.com/genglongling/CausalT5kBench

</details>


### [99] [CoRefine: Confidence-Guided Self-Refinement for Adaptive Test-Time Compute](https://arxiv.org/abs/2602.08948)
*Chen Jin,Ryutaro Tanno,Tom Diethe,Philip Teare*

Main category: cs.AI

TL;DR: CoRefine是一种基于置信度引导的自优化方法，通过轻量级控制器实现高效推理，大幅减少计算成本


<details>
  <summary>Details</summary>
Motivation: 大型语言模型通常依赖并行解码（如512个样本）来提高推理准确性，但这会带来巨大的计算开销。需要一种更高效的方法来提升推理性能

Method: 引入CoRefine方法，在冻结的LLM上添加一个轻量级211k参数的Conv1D控制器。控制器基于完整跟踪置信度决定是否停止、重新检查或尝试不同方法，实现有针对性的自我修正

Result: 每个问题平均只需2.7个优化步骤，相对于512样本基线减少约190倍的token使用。在控制器自信停止时达到92.6%的精确度，表明置信度动态可靠地指示正确性。扩展到CoRefine-Tree混合变体，自适应平衡探索与利用

Conclusion: 通过将置信度视为控制信号而非正确性保证，CoRefine为可扩展推理和不完美验证器的智能体设置提供了模块化原语

Abstract: Large Language Models (LLMs) often rely on test-time scaling via parallel decoding (for example, 512 samples) to boost reasoning accuracy, but this incurs substantial compute. We introduce CoRefine, a confidence-guided self-refinement method that achieves competitive accuracy using a fraction of the tokens via a lightweight 211k-parameter Conv1D controller atop a frozen LLM. The controller consumes full-trace confidence to decide whether to halt, re-examine, or try a different approach, enabling targeted self-correction with an average of 2.7 refinement steps per problem and roughly 190-fold token reduction relative to 512-sample baselines. Across diverse reasoning benchmarks and three open-source models, the controller achieves 92.6 percent precision when it confidently halts, indicating that confidence dynamics reliably signal correctness without ground-truth verification. We extend this to CoRefine-Tree, a hybrid sequential-parallel variant that adaptively balances exploration and exploitation, with easy serving integration and verifier compatibility. By treating confidence as a control signal rather than a correctness guarantee, CoRefine provides a modular primitive for scalable reasoning and agentic settings with imperfect verifiers.

</details>


### [100] [stable-worldmodel-v1: Reproducible World Modeling Research and Evaluation](https://arxiv.org/abs/2602.08968)
*Lucas Maes,Quentin Le Lidec,Dan Haramati,Nassim Massaudi,Damien Scieur,Yann LeCun,Randall Balestriero*

Main category: cs.AI

TL;DR: SWM是一个模块化、经过测试和文档化的世界模型研究生态系统，旨在解决现有世界模型实现缺乏可重用性、标准化的问题。


<details>
  <summary>Details</summary>
Motivation: 当前大多数世界模型实现都是针对特定论文的，这严重限制了它们的可重用性，增加了bug风险，并降低了评估标准化程度。

Method: 开发了stable-worldmodel（SWM）生态系统，提供高效的数据收集工具、标准化环境、规划算法和基线实现，每个环境都支持可控的变化因素。

Result: SWM为世界模型研究提供了统一的平台，支持鲁棒性和持续学习研究，并通过在DINO-WM中研究零样本鲁棒性展示了其实用性。

Conclusion: SWM解决了世界模型研究中缺乏标准化和可重用性的问题，为未来研究提供了可靠的基础设施，促进了该领域的进步。

Abstract: World Models have emerged as a powerful paradigm for learning compact, predictive representations of environment dynamics, enabling agents to reason, plan, and generalize beyond direct experience. Despite recent interest in World Models, most available implementations remain publication-specific, severely limiting their reusability, increasing the risk of bugs, and reducing evaluation standardization. To mitigate these issues, we introduce stable-worldmodel (SWM), a modular, tested, and documented world-model research ecosystem that provides efficient data-collection tools, standardized environments, planning algorithms, and baseline implementations. In addition, each environment in SWM enables controllable factors of variation, including visual and physical properties, to support robustness and continual learning research. Finally, we demonstrate the utility of SWM by using it to study zero-shot robustness in DINO-WM.

</details>


### [101] [InternAgent-1.5: A Unified Agentic Framework for Long-Horizon Autonomous Scientific Discovery](https://arxiv.org/abs/2602.08990)
*Shiyang Feng,Runmin Ma,Xiangchao Yan,Yue Fan,Yusong Hu,Songtao Huang,Shuaiyu Zhang,Zongsheng Cao,Tianshuo Peng,Jiakang Yuan,Zijie Guo,Zhijie Zhong,Shangheng Du,Weida Wang,Jinxin Shi,Yuhao Zhou,Xiaohan He,Zhiyin Yu,Fangchen Yu,Qihao Zheng,Jiamin Wu,Mianxin Liu,Chi Zhang,Shaowei Hou,Shuya Li,Yankai Jiang,Wenjie Lou,Lilong Wang,Zifu Wang,Jiong Wang,Wanghan Xu,Yue Deng,Dongrui Liu,Yiheng Wang,Wenlong Zhang,Fenghua Ling,Shufei Zhang,Xiaosong Wang,Shuangjia Zheng,Xun Huang,Siqi Sun,Shuyue Hu,Peng Ye,Chunfeng Song,Bin Wang,Conghui He,Yihao Liu,Xin Li,Qibin Hou,Tao Chen,Xiangyu Yue,Bin Wang,Liang He,Dahua Lin,Bowen Zhou,Bo Zhang,Lei Bai*

Main category: cs.AI

TL;DR: InternAgent-1.5是一个端到端科学发现系统，通过生成、验证、演化三个协调子系统，在计算和实验领域实现自主科学发现，在多个基准测试和实际发现任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 开发一个统一的系统，能够在计算和实验领域进行端到端的科学发现，协调计算建模和实验室实验，实现自主、连续的科学探索。

Method: 采用结构化架构，包含三个协调子系统：生成、验证和演化，支持深度研究、解决方案优化和长时程记忆等基础能力，使系统能够在扩展的发现周期中持续运行。

Result: 在GAIA、HLE、GPQA和FrontierScience等科学推理基准测试中取得领先性能；在算法发现任务中自主设计机器学习问题的竞争性方法；在实验发现任务中执行完整的计算或湿实验室实验，在地球、生命、生物和物理领域产生科学发现。

Conclusion: InternAgent-1.5为自主科学发现提供了一个通用且可扩展的框架，展示了在计算和实验领域的强大科学发现能力。

Abstract: We introduce InternAgent-1.5, a unified system designed for end-to-end scientific discovery across computational and empirical domains. The system is built on a structured architecture composed of three coordinated subsystems for generation, verification, and evolution. These subsystems are supported by foundational capabilities for deep research, solution optimization, and long horizon memory. The architecture allows InternAgent-1.5 to operate continuously across extended discovery cycles while maintaining coherent and improving behavior. It also enables the system to coordinate computational modeling and laboratory experimentation within a single unified system. We evaluate InternAgent-1.5 on scientific reasoning benchmarks such as GAIA, HLE, GPQA, and FrontierScience, and the system achieves leading performance that demonstrates strong foundational capabilities. Beyond these benchmarks, we further assess two categories of discovery tasks. In algorithm discovery tasks, InternAgent-1.5 autonomously designs competitive methods for core machine learning problems. In empirical discovery tasks, it executes complete computational or wet lab experiments and produces scientific findings in earth, life, biological, and physical domains. Overall, these results show that InternAgent-1.5 provides a general and scalable framework for autonomous scientific discovery.

</details>


### [102] [iGRPO: Self-Feedback-Driven LLM Reasoning](https://arxiv.org/abs/2602.09000)
*Ali Hatamizadeh,Shrimai Prabhumoye,Igor Gitman,Ximing Lu,Seungju Han,Wei Ping,Yejin Choi,Jan Kautz*

Main category: cs.AI

TL;DR: iGRPO是一种两阶段强化学习方法，通过模型生成草稿和动态自我条件化来提升大语言模型的数学推理能力，在多个基准测试中取得新的SOTA结果。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型在解决复杂数学问题方面显示出潜力，但仍难以产生准确且一致的解决方案。需要更有效的强化学习方法来提升模型的质量和可靠性。

Method: 提出迭代组相对策略优化(iGRPO)，这是GRPO的两阶段扩展：第一阶段采样多个探索性草稿并选择最高奖励的草稿；第二阶段将最佳草稿附加到原始提示上，在草稿条件化的改进上应用GRPO风格更新，训练策略超越其先前的最佳尝试。

Result: 在匹配的rollout预算下，iGRPO在多个基础模型上持续优于GRPO。应用于OpenReasoning-Nemotron-7B模型时，在AIME24和AIME25上分别达到85.62%和79.64%的新SOTA结果。

Conclusion: 迭代、基于自我反馈的强化学习在推进可验证数学推理方面具有巨大潜力，iGRPO通过动态自我条件化有效提升了模型的推理能力。

Abstract: Large Language Models (LLMs) have shown promise in solving complex mathematical problems, yet they still fall short of producing accurate and consistent solutions. Reinforcement Learning (RL) is a framework for aligning these models with task-specific rewards, improving overall quality and reliability. Group Relative Policy Optimization (GRPO) is an efficient, value-function-free alternative to Proximal Policy Optimization (PPO) that leverages group-relative reward normalization. We introduce Iterative Group Relative Policy Optimization (iGRPO), a two-stage extension of GRPO that adds dynamic self-conditioning through model-generated drafts. In Stage 1, iGRPO samples multiple exploratory drafts and selects the highest-reward draft using the same scalar reward signal used for optimization. In Stage 2, it appends this best draft to the original prompt and applies a GRPO-style update on draft-conditioned refinements, training the policy to improve beyond its strongest prior attempt. Under matched rollout budgets, iGRPO consistently outperforms GRPO across base models (e.g., Nemotron-H-8B-Base-8K and DeepSeek-R1 Distilled), validating its effectiveness on diverse reasoning benchmarks. Moreover, applying iGRPO to OpenReasoning-Nemotron-7B trained on AceReason-Math achieves new state-of-the-art results of 85.62\% and 79.64\% on AIME24 and AIME25, respectively. Ablations further show that the refinement wrapper generalizes beyond GRPO variants, benefits from a generative judge, and alters learning dynamics by delaying entropy collapse. These results underscore the potential of iterative, self-feedback-based RL for advancing verifiable mathematical reasoning.

</details>


### [103] [Data Science and Technology Towards AGI Part I: Tiered Data Management](https://arxiv.org/abs/2602.09003)
*Yudong Wang,Zixuan Fu,Hengyu Zhao,Chen Zhao,Chuyue Zhou,Xinle Lin,Hongya Lyu,Shuaikang Xue,Yi Yi,Yingjiao Wang,Zhi Zheng,Yuzhou Zhang,Jie Zhou,Chaojun Xiao,Xu Han,Zhiyuan Liu,Maosong Sun*

Main category: cs.AI

TL;DR: 本文提出一个L0-L4分层数据管理框架，通过数据与模型协同进化的方式解决当前LLM训练中数据规模单向扩展的瓶颈问题，显著提升训练效率和模型性能。


<details>
  <summary>Details</summary>
Motivation: 当前LLM研究过度依赖数据规模的单向扩展，面临数据可用性、获取成本和训练效率的瓶颈。作者认为AGI发展正进入数据与模型协同进化的新阶段，需要系统化的数据管理方法来平衡数据质量、获取成本和训练效益。

Method: 提出L0-L4分层数据管理框架，从原始未整理资源到组织化可验证知识分为五个层级。利用LLM进行数据质量管理（质量评分、内容编辑等），根据数据特性和管理策略将数据战略性地分配到预训练、中期训练和对齐等不同训练阶段。

Result: 通过实证研究验证了该框架的有效性，使用分层数据集进行多阶段训练的实验结果表明，分层感知的数据利用能显著提高训练效率和模型性能。作者还向社区发布了分层数据集和处理工具。

Conclusion: 提出的分层数据管理框架为可扩展和可持续的数据管理提供了系统化方法，通过数据与模型的协同进化，平衡了数据质量、获取成本和边际训练效益，推动了AGI发展的新范式。

Abstract: The development of artificial intelligence can be viewed as an evolution of data-driven learning paradigms, with successive shifts in data organization and utilization continuously driving advances in model capability. Current LLM research is dominated by a paradigm that relies heavily on unidirectional scaling of data size, increasingly encountering bottlenecks in data availability, acquisition cost, and training efficiency. In this work, we argue that the development of AGI is entering a new phase of data-model co-evolution, in which models actively guide data management while high-quality data, in turn, amplifies model capabilities. To implement this vision, we propose a tiered data management framework, designed to support the full LLM training lifecycle across heterogeneous learning objectives and cost constraints. Specifically, we introduce an L0-L4 tiered data management framework, ranging from raw uncurated resources to organized and verifiable knowledge. Importantly, LLMs are fully used in data management processes, such as quality scoring and content editing, to refine data across tiers. Each tier is characterized by distinct data properties, management strategies, and training roles, enabling data to be strategically allocated across LLM training stages, including pre-training, mid-training, and alignment. The framework balances data quality, acquisition cost, and marginal training benefit, providing a systematic approach to scalable and sustainable data management. We validate the effectiveness of the proposed framework through empirical studies, in which tiered datasets are constructed from raw corpora and used across multiple training phases. Experimental results demonstrate that tier-aware data utilization significantly improves training efficiency and model performance. To facilitate further research, we release our tiered datasets and processing tools to the community.

</details>


### [104] [GEBench: Benchmarking Image Generation Models as GUI Environments](https://arxiv.org/abs/2602.09007)
*Haodong Li,Jingwei Wu,Quan Sun,Guopeng Li,Juanxi Tian,Huanyu Zhang,Yanlin Lai,Ruichuan An,Hongbo Peng,Yuhong Dai,Chenxi Li,Chunmei Qing,Jia Wang,Ziyang Meng,Zheng Ge,Xiangyu Zhang,Daxin Jiang*

Main category: cs.AI

TL;DR: GEBench是一个用于评估GUI动态交互和时序一致性的新基准，包含700个样本和五维评估指标，发现现有模型在单步转换表现良好但在长序列时序一致性和空间定位方面存在困难。


<details>
  <summary>Details</summary>
Motivation: 现有图像生成模型能够基于用户指令预测未来GUI状态，但现有基准主要关注通用领域的视觉保真度，对GUI特定场景中的状态转换和时序一致性的评估不足。需要填补这一空白。

Method: 引入GEBench基准，包含700个精心策划的样本，涵盖5个任务类别（单步交互、多步轨迹、真实与虚构场景、定位点接地）。提出GE-Score五维评估指标：目标达成度、交互逻辑、内容一致性、UI合理性和视觉质量。

Result: 对现有模型的广泛评估表明，它们在单步转换上表现良好，但在维持长交互序列的时序一致性和空间定位方面存在显著困难。图标解释、文本渲染和定位精度是关键瓶颈。

Conclusion: 这项工作为系统评估提供了基础，并为未来构建高保真生成式GUI环境的研究指明了有前景的方向。代码已开源。

Abstract: Recent advancements in image generation models have enabled the prediction of future Graphical User Interface (GUI) states based on user instructions. However, existing benchmarks primarily focus on general domain visual fidelity, leaving the evaluation of state transitions and temporal coherence in GUI-specific contexts underexplored. To address this gap, we introduce GEBench, a comprehensive benchmark for evaluating dynamic interaction and temporal coherence in GUI generation. GEBench comprises 700 carefully curated samples spanning five task categories, covering both single-step interactions and multi-step trajectories across real-world and fictional scenarios, as well as grounding point localization. To support systematic evaluation, we propose GE-Score, a novel five-dimensional metric that assesses Goal Achievement, Interaction Logic, Content Consistency, UI Plausibility, and Visual Quality. Extensive evaluations on current models indicate that while they perform well on single-step transitions, they struggle significantly with maintaining temporal coherence and spatial grounding over longer interaction sequences. Our findings identify icon interpretation, text rendering, and localization precision as critical bottlenecks. This work provides a foundation for systematic assessment and suggests promising directions for future research toward building high-fidelity generative GUI environments. The code is available at: https://github.com/stepfun-ai/GEBench.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [105] [AgentSpawn: Adaptive Multi-Agent Collaboration Through Dynamic Spawning for Long-Horizon Code Generation](https://arxiv.org/abs/2602.07072)
*Igor Costa*

Main category: cs.SE

TL;DR: AgentSpawn是一个动态多智能体协作架构，通过自动内存转移、自适应生成策略和一致性协议，解决了长时程代码生成中的上下文持续性和领域适应性问题。


<details>
  <summary>Details</summary>
Motivation: 长时程代码生成需要跨领域的持续上下文和自适应专业知识。现有的多智能体系统使用静态工作流，无法在运行时分析发现未预期的复杂性时进行适应。现有研究在内存连续性、技能继承、任务恢复、运行时生成和并发一致性方面存在五个关键差距。

Method: AgentSpawn架构包含三个核心机制：(1) 生成过程中的自动内存转移，(2) 由运行时复杂度指标触发的自适应生成策略，(3) 并发修改的一致性协议。

Result: 实验验证显示，AgentSpawn在SWE-bench等基准测试上比静态基线实现了34%更高的完成率，同时通过选择性切片减少了42%的内存开销。

Conclusion: AgentSpawn通过动态智能体协作解决了现有多智能体系统在长时程代码生成中的局限性，显著提高了任务完成效率并降低了资源消耗。

Abstract: Long-horizon code generation requires sustained context and adaptive expertise across domains. Current multi-agent systems use static workflows that cannot adapt when runtime analysis reveals unanticipated complexity. We propose AgentSpawn, an architecture enabling dynamic agent collaboration through: (1) automatic memory transfer during spawning, (2) adaptive spawning policies triggered by runtime complexity metrics, and (3) coherence protocols for concurrent modifications. AgentSpawn addresses five critical gaps in existing research around memory continuity, skill inheritance, task resumption, runtime spawning, and concurrent coherence. Experimental validation demonstrates AgentSpawn achieves 34% higher completion rates than static baselines on benchmarks like SWE-bench while reducing memory overhead by 42% through selective slicing.

</details>


### [106] [Comprehensive Evaluation of Large Language Models on Software Engineering Tasks: A Multi-Task Benchmark](https://arxiv.org/abs/2602.07079)
*Go Frendi Gunawan,Mukhlis Amien*

Main category: cs.SE

TL;DR: 该研究对11个先进大语言模型在5个软件工程任务上进行了多任务评估，发现模型性能存在显著差异：相同满分模型在完成时间、工具效率和成本上存在22-53倍差异，工具使用频率与成功率无关，编码任务成功率100%而研究任务更具挑战性。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型在软件工程领域展现出卓越能力，但缺乏覆盖多样化SE活动的综合基准测试。现有评估通常局限于特定任务，无法全面衡量模型在实际软件工程工作流中的表现。

Method: 研究采用多任务评估框架，对11个最先进的大语言模型在5个代表性软件工程任务上进行测试：bug修复、功能开发、代码重构、技术文案撰写和研究综合。开发了自动化验证框架来测量输出质量和完成效率。

Result: 关键发现包括：1) 获得相同满分的模型在完成时间上存在22倍差异，工具效率49倍差异，估计成本53倍差异；2) 工具使用频率与成功率无相关性；3) 识别出两种低效模式：循环低效和推理低效；4) 编码任务成功率100%，研究任务更具挑战性(90.9%)。

Conclusion: 该研究揭示了当前大语言模型在软件工程应用中的效率差异和成本考量，强调了仅关注准确性而忽视效率指标的局限性。研究提供了完整的实验数据、验证脚本和分析代码，支持完全可重复性。

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities in software engineering, yet comprehensive benchmarks covering diverse SE activities remain limited. We present a multi-task evaluation of 11 state-of-the-art LLMs across five representative software engineering tasks: bug fixing, feature development, code refactoring, technical copywriting, and research synthesis. Our automated verification framework measures both output quality and completion efficiency. Key findings reveal that (1) models achieving identical perfect scores exhibit 22x variation in completion time, 49x variation in tool efficiency, and 53x variation in estimated cost; (2) tool usage frequency shows no correlation with success (r = 0.077, p = 0.575) - one model used 917 tool calls while another solved the same task with 3 calls; (3) we identify two distinct inefficiency patterns: loop inefficiency and inference inefficiency; and (4) coding tasks achieve 100 percent success while research tasks present greater challenges (90.9 percent). We release all experimental data, verification scripts, and analysis code for full reproducibility.

</details>


### [107] [CodeCircuit: Toward Inferring LLM-Generated Code Correctness via Attribution Graphs](https://arxiv.org/abs/2602.07080)
*Yicheng He,Zheng Zhao,Zhou Kaiyu,Bryan Dai,Jie Fu,Yonghui Yang*

Main category: cs.SE

TL;DR: 论文提出了一种新的代码验证方法，通过分析LLM内部计算结构来评估代码功能正确性，无需外部测试或判断机制。


<details>
  <summary>Details</summary>
Motivation: 当前代码验证范式严重依赖外部机制（如基于执行的单元测试或辅助LLM判断），这些方法通常劳动密集或受限于判断模型自身能力。论文旨在探索是否可以从LLM内部计算结构中纯粹评估其功能正确性。

Method: 受机制可解释性启发，将代码验证视为机制诊断任务，将模型的显式算法轨迹映射到行级归因图。通过分解复杂残差流，识别模型内部电路中区分合理推理与逻辑失败的结构特征。

Result: 在Python、C++和Java上的分析证实，内在正确性信号在不同语法中具有鲁棒性。从这些内部图提取的拓扑特征比表面启发式方法更可靠地预测正确性，并能实现有针对性的因果干预来修复错误逻辑。

Conclusion: 这些发现确立了内部自省作为验证生成代码的可解码属性，为代码验证提供了新的内部视角方法。

Abstract: Current paradigms for code verification rely heavily on external mechanisms-such as execution-based unit tests or auxiliary LLM judges-which are often labor-intensive or limited by the judging model's own capabilities. This raises a fundamental, yet unexplored question: Can an LLM's functional correctness be assessed purely from its internal computational structure? Our primary objective is to investigate whether the model's neural dynamics encode internally decodable signals that are predictive of logical validity during code generation. Inspired by mechanistic interpretability, we propose to treat code verification as a mechanistic diagnostic task, mapping the model's explicit algorithmic trajectory into line-level attribution graphs. By decomposing complex residual flows, we aim to identify the structural signatures that distinguish sound reasoning from logical failure within the model's internal circuits. Analysis across Python, C++, and Java confirms that intrinsic correctness signals are robust across diverse syntaxes. Topological features from these internal graphs predict correctness more reliably than surface heuristics and enable targeted causal interventions to fix erroneous logic. These findings establish internal introspection as a decodable property for verifying generated code. Our code is at https:// github.com/bruno686/CodeCircuit.

</details>


### [108] [Rethinking Scientific Modeling: Toward Physically Consistent and Simulation-Executable Programmatic Generation](https://arxiv.org/abs/2602.07083)
*Yongqing Jiang,Jianze Wang,Zhiqi Shen,Zhenghong Lin,Jiayuan Wang,Yijian Yang,Kaoshan Dai,Haoran Luo*

Main category: cs.SE

TL;DR: 提出一个物理一致的自动建筑建模框架，通过领域知识构建、约束导向的模型对齐和验证驱动的评估，解决LLM生成建模代码中的物理不一致问题。


<details>
  <summary>Details</summary>
Motivation: 在计算工程科学中，结构建模是基础组件，即使微小的物理不一致或规范违反都可能使下游模拟无效。虽然大语言模型在自动生成建模代码方面显示出潜力，但在严格的工程约束下，不可执行或物理不一致的输出仍然普遍存在。

Method: 提出一个物理一致的自动建筑建模框架，包含三个核心组件：1) CivilInstruct领域特定数据集，形式化结构工程知识和约束推理；2) 两阶段微调策略，强制约束满足和API合规性；3) MBEval验证驱动的基准测试，通过闭环验证评估可执行性和结构动力学一致性。

Result: 实验结果显示，在严格的验证指标上相比基线模型有持续改进，显著减少了幻觉输出和不合规输出。

Conclusion: 该框架通过整合领域知识、约束导向的模型对齐和验证驱动的评估，实现了物理一致的自动建筑建模，为计算工程科学中的可靠模型生成提供了有效解决方案。

Abstract: Structural modeling is a fundamental component of computational engineering science, in which even minor physical inconsistencies or specification violations may invalidate downstream simulations. The potential of large language models (LLMs) for automatic generation of modeling code has been demonstrated. However, non-executable or physically inconsistent outputs remain prevalent under stringent engineering constraints. A framework for physics-consistent automatic building modeling is therefore proposed, integrating domain knowledge construction, constraint-oriented model alignment, and verification-driven evaluation. CivilInstruct is introduced as a domain-specific dataset that formalizes structural engineering knowledge and constraint reasoning to enable simulation-ready model generation. A two-stage fine-tuning strategy is further employed to enforce constraint satisfaction and application programming interface compliance, substantially reducing hallucinated and non-conforming outputs. MBEval is presented as a verification-driven benchmark that evaluates executability and structural dynamics consistency through closed-loop validation. Experimental results show consistent improvements over baselines across rigorous verification metrics. Our code is available at https://github.com/Jovanqing/AutoBM.

</details>


### [109] [Evaluating Retrieval-Augmented Generation Variants for Natural Language-Based SQL and API Call Generation](https://arxiv.org/abs/2602.07086)
*Michael Marketsmüller,Simon Martin,Tim Schlippe*

Main category: cs.SE

TL;DR: 该论文评估了三种RAG变体在SQL查询和REST API调用生成任务中的表现，发现CoRAG在混合文档环境下表现最佳，检索对准确率提升至关重要。


<details>
  <summary>Details</summary>
Motivation: 企业系统需要自然语言接口将用户请求转换为结构化操作（如SQL查询和REST API调用），但LLM在特定领域企业环境中的有效性尚未充分探索，特别是需要同时处理检索和修改任务的情况。

Method: 使用SAP Transactional Banking作为企业用例，构建包含SQL和REST API两种模态的新测试数据集，评估三种RAG变体（标准RAG、Self-RAG、CoRAG）在18种实验配置下的表现，涵盖仅数据库、仅API和混合文档三种上下文。

Result: 检索至关重要：无检索时所有任务的精确匹配准确率为0%，而检索使执行准确率提升至最高79.30%，组件匹配准确率最高78.86%。CoRAG在混合文档环境下表现最稳健，在组合任务中实现10.29%的精确匹配准确率（标准RAG为7.45%），主要得益于SQL生成性能优势（15.32% vs 11.56%）。

Conclusion: 检索策略设计是生产级自然语言接口的关键决定因素，迭代查询分解在文档异构环境下优于top-k检索和二元相关性过滤，为构建企业级NL-to-Code系统提供了重要指导。

Abstract: Enterprise systems increasingly require natural language interfaces that can translate user requests into structured operations such as SQL queries and REST API calls. While large language models (LLMs) show promise for code generation [Chen et al., 2021; Huynh and Lin, 2025], their effectiveness in domain-specific enterprise contexts remains underexplored, particularly when both retrieval and modification tasks must be handled jointly. This paper presents a comprehensive evaluation of three retrieval-augmented generation (RAG) variants [Lewis et al., 2021] -- standard RAG, Self-RAG [Asai et al., 2024], and CoRAG [Wang et al., 2025] -- across SQL query generation, REST API call generation, and a combined task requiring dynamic task classification. Using SAP Transactional Banking as a realistic enterprise use case, we construct a novel test dataset covering both modalities and evaluate 18 experimental configurations under database-only, API-only, and hybrid documentation contexts. Results demonstrate that RAG is essential: Without retrieval, exact match accuracy is 0% across all tasks, whereas retrieval yields substantial gains in execution accuracy (up to 79.30%) and component match accuracy (up to 78.86%). Critically, CoRAG proves most robust in hybrid documentation settings, achieving statistically significant improvements in the combined task (10.29% exact match vs. 7.45% for standard RAG), driven primarily by superior SQL generation performance (15.32% vs. 11.56%). Our findings establish retrieval-policy design as a key determinant of production-grade natural language interfaces, showing that iterative query decomposition outperforms both top-k retrieval and binary relevance filtering under documentation heterogeneity.

</details>


### [110] [Forecasting Developer Environments with GenAI: A Research Perspective](https://arxiv.org/abs/2602.07412)
*Raula Gaikovina Kula,Christoph Treude,Xing Hu,Sebastian Baltes,Earl T. Barr,Kelly Blincoe,Fabio Calefato,Junjie Chen,Marc Cheong,Youmei Fan,Daniel M. German,Marco Gerosa,Jin L. C. Guo,Shinpei Hayashi,Robert Hirschfeld,Reid Holmes,Yintong Huo,Takashi Kobayashi,Michele Lanza,Zhongxin Liu,Olivier Nourry,Nicole Novielli,Denys Poshyvanyk,Shinobu Saito,Kazumasa Shimari,Igor Steinmacher,Mairieli Wessel,Markus Wagner,Annie Vella,Laurie Williams,Xin Xia*

Main category: cs.SE

TL;DR: 专家会议探讨生成式AI对IDE的影响，识别出四个关键研究主题


<details>
  <summary>Details</summary>
Motivation: 生成式AI在代码生成、测试、代码审查和程序修复等方面表现出色，可能改变IDE中的人机交互方式。为了探索这种影响，来自软件工程、人工智能和人机交互领域的33位专家召开了为期四天的研究会议。

Method: 通过Shonan Meeting 222（为期四天的密集研究会议），汇集33位跨领域专家进行讨论，识别挑战和机遇。

Result: 会议识别出四个主要的研究主题，这些主题对研究人员和从业者都具有重要意义。

Conclusion: 生成式AI对IDE的影响是一个重要的研究领域，需要跨学科合作来探索其潜在变革性影响。

Abstract: Generative Artificial Intelligence (GenAI) models are achieving remarkable performance in various tasks, including code generation, testing, code review, and program repair. The ability to increase the level of abstraction away from writing code has the potential to change the Human-AI interaction within the integrated development environment (IDE). To explore the impact of GenAI on IDEs, 33 experts from the Software Engineering, Artificial Intelligence, and Human-Computer Interaction domains gathered to discuss challenges and opportunities at Shonan Meeting 222, a four-day intensive research meeting. Four themes emerged as areas of interest for researchers and practitioners.

</details>


### [111] [ComPass: Contrastive Learning for Automated Patch Correctness Assessment in Program Repair](https://arxiv.org/abs/2602.07561)
*Quanjun Zhang,Ye Shang,Haichuan Hu,Chunrong Fang,Zhenyu Chen,Liang Xiao*

Main category: cs.SE

TL;DR: ComPass是一个基于预训练语言模型的自动补丁正确性评估方法，通过对比学习和数据增强技术，显著提高了补丁正确性判断的准确性。


<details>
  <summary>Details</summary>
Motivation: 自动程序修复(APR)在生成补丁时存在过拟合问题，即补丁能通过测试套件但实际上不正确。现有的基于预训练语言模型(PLM)的自动补丁正确性评估(APCA)方法仍有各种限制，如训练范式和数据集的不足。

Method: ComPass采用对比学习和数据增强技术：1)使用代码转换规则生成语义保持但结构不同的代码片段；2)通过对比学习预训练PLM以捕捉相同语义但不同结构的代码特征；3)集成补丁代码片段的表示嵌入，并使用二元分类器联合微调PLM来评估补丁正确性。

Result: 在Defects4J的2274个真实世界补丁上的实验结果显示，ComPass达到了88.35%的准确率，显著优于最先进的基线方法APPT。

Conclusion: ComPass通过结合对比学习和数据增强，有效解决了大规模标记补丁数据难以获取的问题，在自动补丁正确性评估方面取得了显著改进，为减少程序修复中的过拟合问题提供了有效解决方案。

Abstract: Automated program repair (APR) attempts to reduce manual debugging efforts and plays a vital role in software maintenance. Despite remarkable progress, APR is still limited in generating overfitting patches, i.e., patches passing available test suites but incorrect. This issue, known as patch overfitting, has become a key concern in the APR community, with numerous approaches proposed to address it. Very recent work proposes a pre-trained language model (PLM)-based automated patch correctness assessment (APCA) approach, indicating the potential of such PLMs in reasoning about patch correctness. Despite being promising, it is still far from perfect due to various limitations, such as the training paradigm and training dataset. In this paper, we present ComPass, a PLM-based APCA approach that leverages contrastive learning and data augmentation to address the technical limitations of prior work. Our work is inspired by the opportunity to integrate contrastive learning with recent PLMs in the field of patch correctness assessment, where large-scale labeled patches are difficult to obtain. ComPass utilizes code transformation rules to generate semantic-preserving code snippets for both unlabeled pre-training corpus and labeled fine-tuning patches. ComPass then pre-trains PLMs with contrastive learning, which captures code features with the same semantics but different structures. ComPass finally integrates representation embeddings of patch code snippets and fine-tunes PLMs with a binary classifier jointly to assess patch code correctness. Experimental results on 2274 real-world patches from Defects4J demonstrate that ComPass achieves an accuracy of 88.35%, significantly outperforming state-of-the-art baseline APPT.

</details>


### [112] [Clarifying Core Dimensions in Digital Maturity Models: An Integrative Approach](https://arxiv.org/abs/2602.07569)
*Eduardo C. Peixoto,Hector Oliveira,Geber L. Ramalho,Cesar França*

Main category: cs.SE

TL;DR: 该研究通过系统映射方法分析了76个数字成熟度模型，识别出最常用的10个维度并提出了整合性定义，以解决数字转型失败率高和现有模型维度定义模糊的问题。


<details>
  <summary>Details</summary>
Motivation: 数字转型项目失败率高，数字成熟度模型虽有潜力但存在明显缺陷：维度定义不一致、概念不清晰、组件不明确，需要更清晰的理解框架。

Method: 采用系统映射方法，包括自动搜索和滚雪球技术，分析76个数字成熟度模型，回答两个研究问题：最常用的维度是什么，以及这些维度如何描述（包括其组件）。

Result: 识别出10个最常用的维度：组织、战略、技术、文化、流程、运营、人员、管理、客户和数据，并对每个维度提出了整合性定义，提供了比以往分析更广泛和更新的视角。

Conclusion: 研究通过整合不同解释，为数字成熟度模型的核心维度提供了更清晰的定义框架，有助于提高数字转型项目的成功率。

Abstract: Digital Transformation (DT) initiatives frequently face high failure rates, and while Digital Maturity Models (DMMs) offer potential solutions, they have notable shortcomings. Specifically, there is significant disparity in the dimensions considered relevant, a lack of clarity in their definitions, and uncertainty regarding their components. This study aims to provide a clearer understanding of DMMs by proposing integrative definitions of the most frequently used dimensions. Using a Systematic Mapping approach, including automatic search and snowballing techniques, we analyzed 76 DMMs to answer two Research Questions: (RQ1) What are the most frequent dimensions in DMMs? and (RQ2) How are these dimensions described, including their components? We reconcile varying interpretations of the ten most frequent dimensions -- Organization, Strategy, Technology, Culture, Process, Operations, People, Management, Customer, and Data -- and propose integrative definitions for each. Compared to previous analyses, this study provides a broader and more recent perspective on Digital Maturity Models.

</details>


### [113] [A Course on the Introduction to Quantum Software Engineering: Experience Report](https://arxiv.org/abs/2602.07589)
*Andriy Miranskyy*

Main category: cs.SE

TL;DR: 该论文报告了一门将量子计算与软件工程结合的交叉课程设计，强调通过软件工程视角教授量子计算，关注测试、抽象、工具和生命周期管理等实践能力。


<details>
  <summary>Details</summary>
Motivation: 当前量子计算教育主要关注算法和框架层面，缺乏软件工程实践方面的培养，如测试、抽象、工具和生命周期管理。需要开发一门课程将量子计算与软件工程结合，培养学生在量子软件工程方面的早期能力。

Method: 设计并实施了一门本科-研究生交叉课程，通过软件工程视角教授量子计算。课程整合了基础量子概念与软件工程视角，强调可执行工件、经验推理，以及处理概率行为、噪声和不断发展的工具链带来的权衡。采用模块化课程设计，可扩展的混合学术水平评估模型。

Result: 尽管学生之前对量子计算接触有限，但一旦通过可执行工件建立了对量子信息和量子算法的基础理解，他们就能有效参与量子软件工程主题。证据来自教师观察、学生反馈、调查和学生作业分析。

Conclusion: 该经验报告提供了一个模块化课程设计、可扩展的混合学术水平评估模型，以及为软件工程教育工作者开发量子计算课程的可转移经验。证明了通过软件工程视角教授量子计算的可行性。

Abstract: Quantum computing is increasingly practiced through programming, yet most educational offerings emphasize algorithmic or framework-level use rather than software engineering concerns such as testing, abstraction, tooling, and lifecycle management.
  This paper reports on the design and first offering of a cross-listed undergraduate--graduate course that frames quantum computing through a software engineering lens, focusing on early-stage competence relevant to software engineering practice. The course integrates foundational quantum concepts with software engineering perspectives, emphasizing executable artifacts, empirical reasoning, and trade-offs arising from probabilistic behaviour, noise, and evolving toolchains. Evidence is drawn from instructor observations, student feedback, surveys, and analysis of student work.
  Despite minimal prior exposure to quantum computing, students were able to engage productively with quantum software engineering topics once a foundational understanding of quantum information and quantum algorithms, expressed through executable artifacts, was established. This experience report contributes a modular course design, a scalable assessment model for mixed academic levels, and transferable lessons for software engineering educators developing quantum computing curricula.

</details>


### [114] [Evaluating Large Language Models for Detecting Architectural Decision Violations](https://arxiv.org/abs/2602.07609)
*Ruoyu Su,Alexander Bakhtin,Noman Ahmad,Matteo Esposito,Valentina Lenarduzzi,Davide Taibi*

Main category: cs.SE

TL;DR: LLMs可以有效地识别开源系统中的架构决策违规，对于明确、可代码推断的决策表现出高准确率，但对于依赖部署配置或组织知识的隐式决策效果有限。


<details>
  <summary>Details</summary>
Motivation: 软件架构决策记录（ADRs）对维护架构质量至关重要，但许多决策违规未被发现，因为项目缺乏系统化文档和自动化检测机制。大型语言模型（LLMs）的发展为规模化自动化架构推理提供了新可能性。

Method: 研究分析了109个GitHub仓库中的980个ADRs，采用多模型流水线：一个LLM主模型筛选潜在决策违规，另外三个LLM独立验证推理过程。评估了模型间一致性、准确率、精确率和召回率，并辅以专家评估。

Result: 模型对于明确、可代码推断的决策表现出显著一致性和强准确率。但对于隐式或部署导向的决策（依赖部署配置或组织知识），准确率不足。LLMs在验证架构决策合规性方面能提供有意义的支持。

Conclusion: LLMs能够有意义地支持架构决策合规性验证，但对于非代码相关的决策，尚不能完全替代人类专业知识。需要结合人类专业知识来处理依赖部署配置和组织知识的决策。

Abstract: Architectural Decision Records (ADRs) play a central role in maintaining software architecture quality, yet many decision violations go unnoticed because projects lack both systematic documentation and automated detection mechanisms. Recent advances in Large Language Models (LLMs) open up new possibilities for automating architectural reasoning at scale. We investigated how effectively LLMs can identify decision violations in open-source systems by examining their agreement, accuracy, and inherent limitations. Our study analyzed 980 ADRs across 109 GitHub repositories using a multi-model pipeline in which one LLM primary screens potential decision violations, and three additional LLMs independently validate the reasoning. We assessed agreement, accuracy, precision, and recall, and complemented the quantitative findings with expert evaluation. The models achieved substantial agreement and strong accuracy for explicit, code-inferable decisions. Accuracy falls short for implicit or deployment-oriented decisions that depend on deployment configuration or organizational knowledge. Therefore, LLMs can meaningfully support validation of architectural decision compliance; however, they are not yet replacing human expertise for decisions not focused on code.

</details>


### [115] [Debugging code world models](https://arxiv.org/abs/2602.07672)
*Babak Rahmani*

Main category: cs.SE

TL;DR: 代码世界模型（CWMs）通过预测程序执行后的运行时状态来模拟程序执行，但存在两个主要失败模式：长执行历史导致的令牌预算耗尽和字符串值状态处理困难。


<details>
  <summary>Details</summary>
Motivation: 研究代码世界模型的错误来源和局限性，理解其在局部语义执行和长时程状态跟踪方面的表现，为改进CWMs提供方向。

Method: 从两个互补角度研究CWMs：局部语义执行和长时程状态跟踪。使用真实代码基准测试识别失败模式，并通过受控的排列跟踪基准测试来研究长时程行为。

Result: 发现两个主要失败模式：1）密集运行时状态产生导致长执行历史程序令牌预算耗尽；2）字符串值状态处理失败，归因于子词标记化的限制而非程序结构。长时程退化主要由错误动作生成驱动，当使用真实命令时，Transformer-based CWM能在长时程中准确传播状态。

Conclusion: CWMs的局限性主要来自令牌预算和字符串处理，而非长时程状态跟踪能力。这为设计更高效的监督和状态表示提供了方向，使其更好地与程序执行和数据类型对齐。

Abstract: Code World Models (CWMs) are language models trained to simulate program execution by predicting explicit runtime state after every executed command. This execution-based world modeling enables internal verification within the model, offering an alternative to natural language chain-of-thought reasoning. However, the sources of errors and the nature of CWMs' limitations remain poorly understood. We study CWMs from two complementary perspectives: local semantic execution and long-horizon state tracking. On real-code benchmarks, we identify two dominant failure regimes. First, dense runtime state reveals produce token-intensive execution traces, leading to token-budget exhaustion on programs with long execution histories. Second, failures disproportionately concentrate in string-valued state, which we attribute to limitations of subword tokenization rather than program structure. To study long-horizon behavior, we use a controlled permutation-tracking benchmark that isolates state propagation under action execution. We show that long-horizon degradation is driven primarily by incorrect action generation: when actions are replaced with ground-truth commands, a Transformer-based CWM propagates state accurately over long horizons, despite known limitations of Transformers in long-horizon state tracking. These findings suggest directions for more efficient supervision and state representations in CWMs that are better aligned with program execution and data types.

</details>


### [116] [On Sequence-to-Sequence Models for Automated Log Parsing](https://arxiv.org/abs/2602.07698)
*Adam Sorrenti,Andriy Miranskyy*

Main category: cs.SE

TL;DR: 本文系统评估了不同序列建模架构（Transformer、Mamba、LSTM）在日志解析任务中的性能，发现Transformer在准确性上最优，Mamba在计算成本上更具优势。


<details>
  <summary>Details</summary>
Motivation: 自动化日志解析面临异构日志格式、训练部署数据分布偏移和基于规则方法的脆弱性等挑战，需要系统评估不同序列建模架构的性能和计算成本。

Method: 采用控制性实证研究，比较四种序列建模架构：Transformer、Mamba状态空间、单向LSTM和双向LSTM模型。训练了396个模型，使用相对Levenshtein编辑距离进行评估，并进行统计显著性检验。

Result: Transformer获得最低平均相对编辑距离（0.111），其次是Mamba（0.145）、单向LSTM（0.186）和双向LSTM（0.265）。Mamba在保持竞争力的同时显著降低计算成本。字符级分词通常提升性能，序列长度对Transformer准确性影响可忽略。

Conclusion: Transformer将解析错误减少23.4%，而Mamba在数据或计算资源受限时是强有力的替代方案。研究结果为研究人员和从业者提供了关于表示选择、序列长度和样本效率的实用指导。

Abstract: Log parsing is a critical standard operating procedure in software systems, enabling monitoring, anomaly detection, and failure diagnosis. However, automated log parsing remains challenging due to heterogeneous log formats, distribution shifts between training and deployment data, and the brittleness of rule-based approaches. This study aims to systematically evaluate how sequence modelling architecture, representation choice, sequence length, and training data availability influence automated log parsing performance and computational cost. We conduct a controlled empirical study comparing four sequence modelling architectures: Transformer, Mamba state-space, monodirectional LSTM, and bidirectional LSTM models. In total, 396 models are trained across multiple dataset configurations and evaluated using relative Levenshtein edit distance with statistical significance testing. Transformer achieves the lowest mean relative edit distance (0.111), followed by Mamba (0.145), mono-LSTM (0.186), and bi-LSTM (0.265), where lower values are better. Mamba provides competitive accuracy with substantially lower computational cost. Character-level tokenization generally improves performance, sequence length has negligible practical impact on Transformer accuracy, and both Mamba and Transformer demonstrate stronger sample efficiency than recurrent models. Overall, Transformers reduce parsing error by 23.4%, while Mamba is a strong alternative under data or compute constraints. These results also clarify the roles of representation choice, sequence length, and sample efficiency, providing practical guidance for researchers and practitioners.

</details>


### [117] [Still Manual? Automated Linter Configuration via DSL-Based LLM Compilation of Coding Standards](https://arxiv.org/abs/2602.07783)
*Zejun Zhang,Yixin Gan,Zhenchang Xing,Tian Zhang,Yi Li,Xiwei Xu,Qinghua Lu,Liming Zhu*

Main category: cs.SE

TL;DR: LintCFG：基于DSL和LLM的自动化linter配置生成方法，通过编译器设计思路将自然语言编码标准转换为特定linter配置，显著提高配置效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 手动配置linter复杂且需要专业知识，不同编程语言、编码标准和linter的多样性导致重复且维护密集的配置工作。需要自动化方法来减少人工工作量。

Method: 设计领域特定语言（DSL）以工具无关、结构化、可读且精确的方式表达编码规则；将linter配置构建为DSL配置指令；通过编译过程将自然语言编码标准解析为DSL编码标准，匹配配置指令，验证一致性，最终生成linter特定配置。

Result: 在Java编码标准的Checkstyle实验中，DSL表示达到90%以上的精确率和召回率；细粒度linter配置生成的准确率、精确率、召回率和F1分数接近70%（部分超过70%）；在精确率上比基线方法提高100%以上；用户研究表明提高了开发人员配置linter的效率；通过为JavaScript编码标准生成ESLint配置展示了方法的通用性。

Conclusion: LintCFG方法能够跨编程语言、编码标准和linter自动化生成linter配置，显著减少人工工作量，提高配置效率和准确性，具有广泛的适用性。

Abstract: Coding standards are essential for maintaining consistent and high-quality code across teams and projects. Linters help developers enforce these standards by detecting code violations. However, manual linter configuration is complex and expertise-intensive, and the diversity and evolution of programming languages, coding standards, and linters lead to repetitive and maintenance-intensive configuration work. To reduce manual effort, we propose LintCFG, a domain-specific language (DSL)-driven, LLM-based compilation approach to automate linter configuration generation for coding standards, independent of programming languages, coding standards, and linters. Inspired by compiler design, we first design a DSL to express coding rules in a tool-agnostic, structured, readable, and precise manner. Then, we build linter configurations into DSL configuration instructions. For a given natural language coding standard, the compilation process parses it into DSL coding standards, matches them with the DSL configuration instructions to set configuration names, option names and values, verifies consistency between the standards and configurations, and finally generates linter-specific configurations. Experiments with Checkstyle for Java coding standard show that our approach achieves over 90% precision and recall in DSL representation, with accuracy, precision, recall, and F1-scores close to 70% (with some exceeding 70%) in fine-grained linter configuration generation. Notably, our approach outperforms baselines by over 100% in precision. A user study further shows that our approach improves developers' efficiency in configuring linters for coding standards. Finally, we demonstrate the generality of the approach by generating ESLint configurations for JavaScript coding standards, showcasing its broad applicability across other programming languages, coding standards, and linters.

</details>


### [118] [Software Space Analytics: Towards Visualization and Statistics of Internal Software Execution](https://arxiv.org/abs/2602.07821)
*Shinobu Saito*

Main category: cs.SE

TL;DR: 该论文将空间统计方法应用于软件工程领域，通过将软件内部模块调用关系视为空间结构，使用空间统计技术来识别需要修改或删除的软件模块。


<details>
  <summary>Details</summary>
Motivation: 在软件维护工作中，架构师和程序员需要识别需要修改或删除的模块。虽然用户请求和错误报告可用于此目的，但评估软件内部模块的执行状态同样重要。现有方法可能不足以全面分析软件内部执行数据。

Method: 1. 定义软件空间数据集：将软件内部结构基于模块调用关系视为一个空间
2. 应用空间统计方法：进行空间聚类可视化
3. 使用空间度量进行统计测试

Result: 论文展示了空间统计方法在软件工程领域的应用潜力，通过空间聚类可视化和统计测试，能够帮助识别需要关注的软件模块。

Conclusion: 空间统计方法在软件工程领域具有应用价值，能够帮助分析软件内部执行数据。未来需要进一步探索空间统计在软件工程中的具体应用挑战和解决方案。

Abstract: In software maintenance work, software architects and programmers need to identify modules that require modification or deletion. Whilst user requests and bug reports are utilised for this purpose, evaluating the execution status of modules within the software is also crucial. This paper, therefore, applies spatial statistics to assess internal software execution data. First, we define a software space dataset, viewing the software's internal structure as a space based on module call relationships. Then, using spatial statistics, we conduct the visualization of spatial clusters and the statistical testing using spatial measures. Finally, we consider the usefulness of spatial statistics in the software engineering domain and future challenges.

</details>


### [119] [HerAgent: Rethinking the Automated Environment Deployment via Hierarchical Test Pyramid](https://arxiv.org/abs/2602.07871)
*Xiang Li,Siyu Lu,Sarro Federica,Claire Le Goues,He Ye*

Main category: cs.SE

TL;DR: HerAgent提出了一种基于执行验证的分层环境搭建方法，通过环境成熟度层次结构评估环境搭建成功，在多个基准测试中显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前自动化软件环境搭建方法通常使用依赖安装或部分测试执行等弱信号评估成功，无法确保项目实际可运行。需要基于可执行证据来评估环境搭建的成功。

Method: 提出环境成熟度层次结构，定义三个基于逐步增强执行要求的成功级别；开发HerAgent方法，通过执行验证和修复增量构建可执行环境。

Result: 在四个公共基准测试中，HerAgent优于所有相关工作，实现高达79.6%的改进；在复杂C/C++项目中超越先前方法66.7%；独特解决了11-30个其他方法无法配置的环境实例。

Conclusion: 基于执行验证的分层环境搭建方法能更可靠地评估和实现软件环境配置，显著提升自动化环境搭建的成功率和质量。

Abstract: Automated software environment setup is a prerequisite for testing, debugging, and reproducing failures, yet remains challenging in practice due to complex dependencies, heterogeneous build systems, and incomplete documentation. Recent work leverages large language models to automate this process, but typically evaluates success using weak signals such as dependency installation or partial test execution, which do not ensure that a project can actually run. In this paper, we argue that environment setup success should be evaluated through executable evidence rather than a single binary signal. We introduce the Environment Maturity Hierarchy, which defines three success levels based on progressively stronger execution requirements, culminating in successful execution of a project's main entry point. Guided by this hierarchy, we propose HerAgent, an automated environment setup approach that incrementally constructs executable environments through execution-based validation and repair. We evaluate HerAgent on four public benchmarks, where it outperforms all related work, achieving up to 79.6\% improvement due to its holistic understanding of project structure and dependencies. On complex C/C++ projects, HerAgent surpasses prior approaches by 66.7\%. In addition, HerAgent uniquely resolves 11-30 environment instances across the benchmarks that no prior method can configure.

</details>


### [120] [Rethinking Code Complexity Through the Lens of Large Language Models](https://arxiv.org/abs/2602.07882)
*Chen Xie,Yuling Shi,Xiaodong Gu,Beijun Shen*

Main category: cs.SE

TL;DR: 传统代码复杂度指标与LLM处理代码的难度不相关，作者提出基于LLM视角的新复杂度指标LM-CC，能更好预测LLM性能


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型在代码理解和生成任务上的快速发展，一个重要但尚未充分探索的问题是：传统的代码复杂度指标（如圈复杂度）是否能有效表征LLM处理代码时遇到的困难？作者发现传统指标与LLM性能缺乏一致相关性，存在根本性不匹配。

Method: 提出LM-CC（语言模型代码复杂度）指标，其核心前提是LLM感知的难度由程序语义的非线性驱动。方法包括：1）基于熵将程序分解为语义单元；2）将这些单元组织成组合层次结构；3）量化复杂度为组合层级和分支诱导的发散性的原则性聚合，捕捉代码处理过程中的累积模型不确定性。

Result: 实验表明，LM-CC比传统指标与LLM性能的相关性更强，并且降低LM-CC能直接提升任务性能。传统指标在控制代码长度后与LLM性能没有一致相关性。

Conclusion: 需要从LLM视角重新思考代码复杂度度量，LM-CC作为专门为LLM设计的复杂度指标，能更好地表征LLM处理代码的难度，并为优化LLM代码处理性能提供指导。

Abstract: Code complexity metrics such as cyclomatic complexity have long been used to assess software quality and maintainability. With the rapid advancement of large language models (LLMs) on code understanding and generation tasks, an important yet underexplored question arises: do these traditional complexity metrics meaningfully characterize the difficulty LLMs experience when processing code? In this work, we empirically demonstrate that, after controlling for code length, classical metrics exhibit no consistent correlation with LLM performance, revealing a fundamental mismatch with model-perceived difficulty. To address this gap, we propose LM-CC, a novel code complexity metric designed from the perspective of LLMs. The core premise of LM-CC is that LLM-perceived difficulty is driven by the nonlinearity of program semantics. Accordingly, we decompose programs into semantic units based on entropy, organize these units into a compositional hierarchy, and quantify complexity as a principled aggregation of compositional level and branching-induced divergence, capturing cumulative model uncertainty during code processing. Our extensive experiments show that LM-CC not only correlates more strongly with LLM performance than traditional metrics but also that lowering it directly enhances task performance.

</details>


### [121] [Is Your Private Information Logged? An Empirical Study on Android App Logs](https://arxiv.org/abs/2602.07893)
*Zhiyuan Chen,Soham Sanjay Deo,Poorna Chander Reddy Puttaparthi,Vanessa Nava-Camal,Yiming Tang,Xueling Zhang,Weiyi Shang*

Main category: cs.SE

TL;DR: 该研究构建了Android应用日志数据集，通过实证分析揭示了Android应用日志中隐私泄露的现状、严重程度及原因，发现大多数隐私泄露源于开发者对此问题的认知不足。


<details>
  <summary>Details</summary>
Motivation: 随着移动应用的快速增长，用户对隐私问题的担忧日益突出。Android应用日志作为重要的计算机资源，既帮助开发者调试和监控应用状态，又包含丰富的软件系统信息。先前研究承认软件日志和Android应用中的隐私泄露是重要问题，但缺乏对Android应用日志中隐私泄露的全面分析。

Method: 研究构建了全面的Android应用日志数据集，通过实证研究分析Android应用日志中隐私泄露的现状和严重程度。研究包括三个方面：1) 了解真实世界开发者对软件日志相关隐私问题的关注；2) 研究Android应用日志中的隐私泄露；3) 调查隐私泄露日志的特征并分析其原因。

Result: 研究揭示了真实世界开发者对软件日志隐私问题的五类不同关注点，并发现Android应用日志中隐私泄露普遍存在。大多数隐私泄露源于开发者对此类泄露的无意识。研究还发现隐私泄露日志具有特定特征。

Conclusion: Android应用日志中存在普遍的隐私泄露问题，主要原因是开发者对此缺乏认知。研究为开发者提供了保护隐私不被记录的建议，强调需要提高开发者对日志中隐私保护的认识。

Abstract: With the rapid growth of mobile apps, users' concerns about their privacy have become increasingly prominent. Android app logs serve as crucial computer resources, aiding developers in debugging and monitoring the status of Android apps, while also containing a wealth of software system information. Previous studies have acknowledged privacy leaks in software logs and Android apps as significant issues without providing a comprehensive view of the privacy leaks in Android app logs. In this study, we build a comprehensive dataset of Android app logs and conduct an empirical study to analyze the status and severity of privacy leaks in Android app logs. Our study comprises three aspects: (1) Understanding real-world developers' concerns regarding privacy issues related to software logs; (2) Studying privacy leaks in the Android app logs; (3) Investigating the characteristics of privacy-leaking Android app logs and analyzing the reasons behind them. Our study reveals five different categories of concerns from real-world developers regarding privacy issues related to software logs and the prevalence of privacy leaks in Android app logs, with the majority stemming from developers' unawareness of such leaks. Additionally, our study provides developers with suggestions to safeguard their privacy from being logged.

</details>


### [122] [Bridging the Gap: Adapting Evidence to Decision Frameworks to support the link between Software Engineering academia and industry](https://arxiv.org/abs/2602.08015)
*Patricia G. F. Matsubara,Tayana Conte*

Main category: cs.SE

TL;DR: 该论文介绍了从健康科学领域引入的"证据到决策"框架，旨在解决软件工程领域系统文献综述结果难以触达实践者的问题，通过专家小组评估证据并制定结构化建议。


<details>
  <summary>Details</summary>
Motivation: 尽管软件工程研究社区已进行大量系统文献综述并完善了相关流程，但研究结果仍难以有效触达行业实践者，存在理论与实践之间的鸿沟。

Method: 引入健康科学领域的证据到决策框架，通过组建专家小组评估现有最佳证据，考虑所有相关结果，并基于此制定结构化建议。论文还提供了一个基于SE系统文献综述的工作示例。

Result: 提出了在软件工程领域采用证据到决策框架的可行性和价值，强调需要更全面的标准来为行业实践者制定建议，并识别了实施过程中可能面临的挑战。

Conclusion: 证据到决策框架为解决软件工程研究与实践之间的鸿沟提供了有前景的途径，但需要研究社区和实践社区共同应对实施挑战，完善建议标准。

Abstract: Over twenty years ago, the Software Engineering (SE) research community have been involved with Evidence-Based Software Engineering (EBSE). EBSE aims to inform industrial practice with the best evidence from rigorous research, preferably from systematic literature reviews (SLRs). Since then, SE researchers have conducted many SLRs, perfected their SLR procedures, proposed alternative ways of presenting their results (such as Evidence Briefings), and profusely discussed how to conduct research that impacts practice. Nevertheless, there is still a feeling that SLRs' results are not reaching practitioners. Something is missing. In this vision paper, we introduce Evidence to Decision (EtD) frameworks from the health sciences, which propose gathering experts in panels to assess the existing best evidence about the impact of an intervention in all relevant outcomes and make structured recommendations based on them. The insight we can leverage from EtD frameworks is not their structure per se but all the relevant criteria for making recommendations to practitioners from SLRs. Furthermore, we provide a worked example based on an SE SLR. We also discuss the challenges the SE research and practice community may face when adopting EtD frameworks, highlighting the need for more comprehensive criteria in our recommendations to industry practitioners.

</details>


### [123] [Outsourcing in Global Software Development: Effects of Temporal Location and Methodologies](https://arxiv.org/abs/2602.08084)
*Mark Looi,Marc Szepan*

Main category: cs.SE

TL;DR: 研究表明，近岸软件开发在整体成功率、质量、项目管理努力、进度控制和沟通问题方面优于远岸开发，而开发方法主要影响成本。建议沟通密集型或敏捷项目优先选择近岸外包。


<details>
  <summary>Details</summary>
Motivation: 随着全球软件外包的普及，项目团队常分布在不同的时区。本研究旨在探讨时间距离（近岸vs远岸）对软件外包项目结果的影响，为管理者提供决策依据。

Method: 通过调查80个客户和6个深度访谈，研究时间距离和软件开发方法对项目成功、成本、项目管理努力、进度、质量、沟通问题等结果的影响。

Result: 近岸开发在整体成功、质量、减少项目管理努力、保持进度和减少沟通问题方面具有优势。开发方法主要影响成本，对其它结果影响较小。

Conclusion: 近岸外包更适合沟通密集型或敏捷项目。管理者在选择外包地点时应优先考虑近岸开发，特别是对沟通要求高的项目类型。

Abstract: Developing software globally using outsourced resources has become a common practice, with project teams often distributed in different time zones. In this study, we focus on customers that contract software development to vendors in temporally nearshore or far offshore locations. We conducted a survey to determine the effect of temporal distance on overall success, costs, project management effort, schedule, quality, communication problems, and other outcomes of interest to managers. In the survey of 80 customers and interviews with 6 of them, we also investigated the effect of software development methodology on the same outcomes. The results show that nearshore development is advantageous for overall success, quality, reduced PM effort, maintaining schedule, higher quality, and engendering fewer communication problems. Development methodology appears to only influence higher costs. We assess our findings in the context of prior GSE research and provide practical advice for customers of outsourced global software development, chief of which is to favor nearshore for communication-intensive or Agile projects.

</details>


### [124] [Integrating Code Metrics into Automated Documentation Generation for Computational Notebooks](https://arxiv.org/abs/2602.08133)
*Mojtaba Mostafavi Ghahfarokhi,Hamed Jahantigh,Alireza Asadi,Abbas Heydarnoori*

Main category: cs.SE

TL;DR: 该研究探讨了将源代码度量作为辅助信号用于自动文档生成，特别是在计算笔记本环境中，结果显示结合代码度量能显著提升文档生成质量。


<details>
  <summary>Details</summary>
Motivation: 代码文档对协作和理解至关重要，但开发者常因重复性而忽视。现有自动文档生成方法往往忽略影响可读性和理解度的代码结构和量化特征。代码度量包含与程序理解相关的信息，可作为辅助信号提升文档生成质量。

Method: 采用两阶段方法：1) 改进CodeSearchNet数据集构建流程，从1700多万个代码和markdown单元格中创建专门数据集，经过结构和语义过滤后提取约36,734个高质量(代码, markdown)对；2) 评估两种建模范式：轻量级CNN-RNN架构和少样本GPT-3.5架构，分别在有/无度量信息情况下进行对比。

Result: 结合代码度量显著提高了生成文档的准确性和上下文相关性：CNN-RNN架构在BLEU-1上提升6%，ROUGE-L F1上提升3%；LLM架构在BERTScore F1上提升9%。这表明代码度量提供了有价值的结构上下文。

Conclusion: 集成代码度量能为自动文档生成提供有价值的结构上下文，在不同模型家族中都能增强文档生成效果，特别是在计算笔记本这种代码、叙述和结果集成的环境中。

Abstract: Effective code documentation is essential for collaboration, comprehension, and long-term software maintainability, yet developers often neglect it due to its repetitive nature. Automated documentation generation has evolved from heuristic and rule-based methods to neural network-based and large language model (LLM)-based approaches. However, existing methods often overlook structural and quantitative characteristics of code that influence readability and comprehension. Prior research suggests that code metrics capture information relevant to program understanding. Building on these insights, this paper investigates the role of source code metrics as auxiliary signals for automated documentation generation, focusing on computational notebooks, a popular medium among data scientists that integrates code, narrative, and results but suffers from inconsistent documentation. We propose a two-stage approach. First, the CodeSearchNet dataset construction process was refined to create a specialized dataset from over 17 million code and markdown cells. After structural and semantic filtering, approximately 36,734 high-quality (code, markdown) pairs were extracted. Second, two modeling paradigms, a lightweight CNN-RNN architecture and a few-shot GPT-3.5 architecture, were evaluated with and without metric information. Results show that incorporating code metrics improves the accuracy and contextual relevance of generated documentation, yielding gains of 6% in BLEU-1 and 3% in ROUGE-L F1 for CNN-RNN-based architecture, and 9% in BERTScore F1 for LLM-based architecture. These findings demonstrate that integrating code metrics provides valuable structural context, enhancing automated documentation generation across diverse model families.

</details>


### [125] [Distributed Architecture Reconstruction of Polyglot and Multi-Repository Microservice Projects](https://arxiv.org/abs/2602.08166)
*Oscar Manglaras,Alex Farkas,Thomas Woolford,Christoph Treude,Markus Wagner*

Main category: cs.SE

TL;DR: 提出一个支持技术特定分析模块和分布式架构重建的静态架构重建框架，解决微服务架构文档维护难题。


<details>
  <summary>Details</summary>
Motivation: 微服务架构虽然鼓励小型独立服务开发，但增加了架构复杂性。准确文档至关重要，但由于服务快速独立演进而难以维护。现有静态架构重建方法存在技术限制、单仓库约束或高实现门槛等问题。

Method: 提出新颖的静态架构重建框架，支持技术特定分析模块（提取器），支持多仓库环境中的分布式架构重建。描述核心设计概念和算法，包括提取器执行方式、数据传递机制和输出统一方法。框架与现有静态分析工具和算法互操作，可从提取器内调用或嵌入。

Result: 论文提出了一个完整的框架设计，但没有提供具体的实验结果或评估数据。

Conclusion: 该框架解决了微服务架构文档维护的挑战，通过支持技术特定分析模块和分布式架构重建，克服了现有方法的局限性，并与现有工具保持互操作性。

Abstract: Microservice architectures encourage the use of small, independently developed services; however, this can lead to increased architectural complexity. Accurate documentation is crucial, but is challenging to maintain due to the rapid, independent evolution of services. While static architecture reconstruction provides a way to maintain up-to-date documentation, existing approaches suffer from technology limitations, mono-repo constraints, or high implementation barriers. This paper presents a novel framework for static architecture reconstruction that supports technology-specific analysis modules, called \emph{extractors}, and supports \emph{distributed architecture reconstruction} in multi-repo environments. We describe the core design concepts and algorithms that govern how extractors are executed, how data is passed between them, and how their outputs are unified. Furthermore, the framework is interoperable with existing static analysis tools and algorithms, allowing them to be invoked from or embedded within extractors.

</details>


### [126] [ModARO: A Modular Approach to Architecture Reconstruction of Distributed Microservice Codebases](https://arxiv.org/abs/2602.08181)
*Oscar Manglaras,Alex Farkas,Thomas Woolford,Christoph Treude,Markus Wagner*

Main category: cs.SE

TL;DR: ModARO是一个模块化的微服务架构重构方法，允许为不同技术栈编写可复用的提取器，解决跨项目技术差异和代码库分散的问题。


<details>
  <summary>Details</summary>
Motivation: 微服务架构增加了系统复杂性，但快速独立的开发容易导致架构漂移和文档缺失。现有架构重构方法难以跨项目复用，因为不同项目使用不同技术栈，且微服务常分散在多个代码库中，难以获得系统完整视图。

Method: 提出ModARO方法，允许编写模块化的重构代码（提取器），这些提取器可以针对任何技术栈编写，并在不同项目中复用，不受周围技术栈或代码库分散的影响。

Result: 通过配置ModARO重构了10个开源项目，并通过用户研究验证了其有效性和可用性。用户研究表明ModARO在有用性和可用性方面优于现有基线方法。

Conclusion: ModARO使开发者能够为特定技术栈组装或创建提取器，并跨代码库进行架构重构，便于集成到CI/CD流水线中，帮助解决微服务架构的复杂性和文档维护问题。

Abstract: Microservice architectures promote small, independently developed services, but increase overall architectural complexity. It is crucial that developers understand the architecture and how changes to a service affect the overall system, but rapid and independent development of services increases the risk of architectural drift and discourages the creation and maintenance of documentation. Automatic architecture reconstruction can help avoid these issues, but it is difficult to reuse reconstruction code across multiple projects, as all use different combinations of technologies and project-specific conventions. Reconstruction of architecture-level details is further complicated by the tendency to split microservices into separate repositories, preventing a full view of the system from any one codebase. In this paper, we present and evaluate ModARO, an approach to microservice architecture reconstruction that allows writing modular reconstruction code ('extractors') for any technologies and reusing them across different projects, independent of the surrounding technology stack or whether or not the services are split into multiple codebases. We demonstrate the effectiveness of our approach by configuring ModARO to reconstruct 10 open source projects, and we validate the usefulness and usability of ModARO against a state-of-the-art baseline in a user study with 8 industry practitioners. Using this approach, developers can assemble or create extractors tailored to their technology stacks and distribute architecture reconstruction across repositories, enabling integration into repository CI/CD pipelines.

</details>


### [127] [Adoption of Large Language Models in Scrum Management: Insights from Brazilian Practitioners](https://arxiv.org/abs/2602.08192)
*Mirko Perkusich,Danyllo Albuquerque,Allysson Allex Araújo,Matheus Paixão,Rohit Gheyi,Marcos Kalinowski,Angelo Perkusich*

Main category: cs.SE

TL;DR: 研究调查了巴西70名专业人士在Scrum管理活动中使用LLM的情况，发现LLM在Scrum实践中应用广泛但存在风险


<details>
  <summary>Details</summary>
Motivation: 虽然LLM在软件开发的技术活动中已有应用研究，但在Scrum管理活动中的应用缺乏实证证据，需要了解当前实践、量化效益与风险

Method: 通过对70名巴西专业人士进行问卷调查，其中49人积极使用Scrum，33人报告在Scrum实践中使用基于LLM的助手

Result: 85%受访者具备中高级LLM熟练度，52%每日使用；主要应用于探索Scrum实践，效益包括生产力提升(78%)和减少手动工作(75%)；风险包括"几乎正确"输出(81%)、保密问题(63%)和幻觉(59%)

Conclusion: 这是首次对Scrum管理中LLM使用的实证研究，识别了当前实践、量化了效益与风险，为敏捷环境中负责任地采用和集成LLM提供了方向

Abstract: Scrum is widely adopted in software project management due to its adaptability and collaborative nature. The recent emergence of Large Language Models (LLMs) has created new opportunities to support knowledge-intensive Scrum practices. However, existing research has largely focused on technical activities such as coding and testing, with limited evidence on the use of LLMs in management-related Scrum activities. In this study, we investigate the use of LLMs in Scrum management activities through a survey of 70 Brazilian professionals. Among them, 49 actively use Scrum, and 33 reported using LLM-based assistants in their Scrum practices. The results indicate a high level of proficiency and frequent use of LLMs, with 85% of respondents reporting intermediate or advanced proficiency and 52% using them daily. LLM use concentrates on exploring Scrum practices, with artifacts and events receiving targeted yet uneven support, whereas broader management tasks appear to be adopted more cautiously. The main benefits include increased productivity (78%) and reduced manual effort (75%). However, several critical risks remain, as respondents report 'almost correct' outputs (81%), confidentiality concerns (63%), and hallucinations during use (59%). This work provides one of the first empirical characterizations of LLM use in Scrum management, identifying current practices, quantifying benefits and risks, and outlining directions for responsible adoption and integration in Agile environments.

</details>


### [128] [Specification Vibing for Automated Program Repair](https://arxiv.org/abs/2602.08263)
*Taohong Zhu,Lucas C. Cordeiro,Mustafa A. Mustafa,Youcheng Sun*

Main category: cs.SE

TL;DR: VibeRepair是一种基于行为规范的自动程序修复方法，通过将错误代码转换为结构化行为规范，修复规范偏差，再基于修正后的规范合成代码，相比传统代码中心方法显著提升了修复效果。


<details>
  <summary>Details</summary>
Motivation: 当前基于大语言模型的自动程序修复方法大多是代码中心的，直接重写源代码可能导致产生幻觉修复和行为不一致的补丁。需要一种更易于LLM理解的替代修复范式，使用比原始代码更易访问的表示形式，以实现更准确的理解、分析和修复对齐。

Method: VibeRepair采用规范中心的修复方法：1) 将错误代码转换为结构化行为规范，捕捉程序的预期运行时行为；2) 推断并修复规范偏差；3) 在修正后的行为规范严格指导下合成代码。包含按需推理组件，通过程序分析和历史错误修复证据丰富困难案例，同时控制成本。

Result: 在Defects4J v1.2上正确修复174个错误，比最强基线多28个错误（提升19%）；在Defects4J v2.0上修复178个错误，比先前方法多33个错误（提升23%）。在训练期后收集的真实世界基准测试中进一步证实了其有效性和泛化能力，同时显著减少了补丁空间。

Conclusion: 通过将修复重点放在明确的行为意图上，VibeRepair为"氛围"编码时代重新构建了自动程序修复范式：让行为先行，代码随之而来。该方法通过规范中心的方法实现了更准确、更一致的修复效果。

Abstract: Large language model (LLM)-driven automated program repair (APR) has advanced rapidly, but most methods remain code-centric: they directly rewrite source code and thereby risk hallucinated, behaviorally inconsistent fixes. This limitation suggests the need for an alternative repair paradigm that relies on a representation more accessible to LLMs than raw code, enabling more accurate understanding, analysis, and alignment during repair. To address this gap, we propose VibeRepair, a specification-centric APR technique that treats repair as behavior-specification repair rather than ad-hoc code editing. VibeRepair first translates buggy code into a structured behavior specification that captures the program's intended runtime behavior, then infers and repairs specification misalignments, and finally synthesizes code strictly guided by the corrected behavior specification. An on-demand reasoning component enriches hard cases with program analysis and historical bug-fix evidence while controlling cost. Across Defects4J and real-world benchmarks and multiple LLMs, VibeRepair demonstrates consistently strong repair effectiveness with a significantly smaller patch space. On Defects4J v1.2, VibeRepair correctly repairs 174 bugs, exceeding the strongest state-of-the-art baseline by 28 bugs, which corresponds to a 19% improvement. On Defects4J v2.0, it repairs 178 bugs, outperforming prior approaches by 33 bugs, representing a 23% improvement. Evaluations on real-world benchmarks collected after the training period of selected LLMs further confirm its effectiveness and generalizability. By centering repair on explicit behavioral intent, VibeRepair reframes APR for the era of "vibe" coding: make the behavior sing, and the code will follow.

</details>


### [129] [ArkEval: Benchmarking and Evaluating Automated CodeRepair for ArkTS](https://arxiv.org/abs/2602.08866)
*Bang Xie,Senjian Zhang,Zhiyuan Peng,Wei Chen,Chenhao Ying,Yuan Luo*

Main category: cs.SE

TL;DR: ArkEval：首个针对ArkTS自动化程序修复的统一评估框架和基准，包含502个可复现问题，用于评估大语言模型在HarmonyOS生态中的代码修复能力。


<details>
  <summary>Details</summary>
Motivation: 随着HarmonyOS生态系统的兴起，ArkTS作为其核心开发语言缺乏自动化代码修复工具和评估基准，阻碍了该领域的研究进展。

Method: 1. 从华为官方包含400多个ArkTS应用的仓库中挖掘问题；2. 通过多阶段过滤流程筛选出502个可复现问题；3. 使用基于LLM的测试生成和投票机制确保可测试性；4. 标准化问题描述以支持公平评估；5. 采用检索增强的修复工作流评估四个先进LLM。

Result: 建立了首个ArkTS自动化程序修复基准，包含502个高质量可复现问题，并评估了四个先进LLM在该基准上的表现，揭示了当前LLM在ArkTS代码修复方面的能力和局限性。

Conclusion: ArkEval填补了ArkTS自动化程序修复领域的评估空白，为低资源语言领域的未来研究奠定了基础，展示了LLM在HarmonyOS生态中代码修复的潜力和挑战。

Abstract: Large language models have transformed code generation, enabling unprecedented automation in software development. As mobile ecosystems evolve, HarmonyOS has emerged as a critical platform requiring robust development tools. Software development for the HarmonyOS ecosystem relies heavily on ArkTS, a statically typed extension of TypeScript. Despite its growing importance, the ecosystem lacks robust tools for automated code repair, primarily due to the absence of a high-quality benchmark for evaluation. To address this gap, we present ArkEval, a unified framework for ArkTS automated repair workflow evaluation and benchmark construction. It provides the first comprehensive benchmark specifically designed for ArkTS automated program repair. We constructed this benchmark by mining issues from a large-scale official Huawei repository containing over 400 independent ArkTS applications. Through a rigorous multi-stage filtering process, we curated 502 reproducible issues. To ensure testability, we employed a novel LLM-based test generation and voting mechanism involving Claude and other models. Furthermore, we standardized problem statements to facilitate fair evaluation. Finally, we evaluated four state-of-the-art Large Language Models (LLMs) on our benchmark using a retrieval-augmented repair workflow. Our results highlight the current capabilities and limitations of LLMs in repairing ArkTS code, paving the way for future research in this low-resource language domain.

</details>


### [130] [DeepQuali: Initial results of a study on the use of large language models for assessing the quality of user stories](https://arxiv.org/abs/2602.08887)
*Adam Trendowicz,Daniel Seifert,Andreas Jedlitschka,Marcus Ciolkowski,Anton Strahilov*

Main category: cs.SE

TL;DR: 本文提出了名为"DeepQuali"的基于GPT-4o的LLM方法，用于评估和改进敏捷软件开发中的需求质量，并在两家小型公司中进行了应用评估。


<details>
  <summary>Details</summary>
Motivation: 生成式人工智能（特别是大语言模型）在软件工程中主要应用于编码任务，但在需求工程领域，特别是需求验证方面的应用有限。当前GAI在需求方面的应用主要集中在需求获取、转换和分类，而非质量评估。

Method: 提出并评估了基于GPT-4o的"DeepQuali"方法，用于评估和改进敏捷软件开发中的需求质量。该方法应用于两家小型公司的项目中，将LLM的质量评估与专家判断进行比较，并通过专家走查、反馈收集和接受度评分来评估方法效果。

Result: 专家在很大程度上同意LLM的质量评估，特别是在整体评分和解释方面。然而，专家之间在详细评分上并不总是一致，这表明专业知识和经验可能影响判断。专家认可该方法的实用性，但批评其缺乏与工作流程的集成。

Conclusion: 大语言模型在支持软件工程师进行需求质量评估和改进方面显示出潜力。明确使用质量模型和解释性反馈可以提高接受度。未来的工作需要更好地将LLM方法集成到实际工作流程中。

Abstract: Generative artificial intelligence (GAI), specifically large language models (LLMs), are increasingly used in software engineering, mainly for coding tasks. However, requirements engineering - particularly requirements validation - has seen limited application of GAI. The current focus of using GAI for requirements is on eliciting, transforming, and classifying requirements, not on quality assessment. We propose and evaluate the LLM-based (GPT-4o) approach "DeepQuali", for assessing and improving requirements quality in agile software development. We applied it to projects in two small companies, where we compared LLM-based quality assessments with expert judgments. Experts also participated in walkthroughs of the solution, provided feedback, and rated their acceptance of the approach. Experts largely agreed with the LLM's quality assessments, especially regarding overall ratings and explanations. However, they did not always agree with the other experts on detailed ratings, suggesting that expertise and experience may influence judgments. Experts recognized the usefulness of the approach but criticized the lack of integration into their workflow. LLMs show potential in supporting software engineers with the quality assessment and improvement of requirements. The explicit use of quality models and explanatory feedback increases acceptance.

</details>


### [131] [Comparing AI Coding Agents: A Task-Stratified Analysis of Pull Request Acceptance](https://arxiv.org/abs/2602.08915)
*Giovanni Pinna,Jingzhi Gong,David Williams,Federica Sarro*

Main category: cs.SE

TL;DR: 该研究对5个AI编程助手在7,156个PR任务中的表现进行实证比较，发现任务类型是影响接受率的主要因素，文档任务接受率最高，不同助手在不同任务类型上各有优势。


<details>
  <summary>Details</summary>
Motivation: 虽然AI编程助手在软件开发中快速普及，但缺乏对其在不同任务类型和时间维度上效果的系统性比较研究。

Method: 使用AIDev数据集的7,156个PR数据，比较OpenAI Codex、GitHub Copilot、Devin、Cursor和Claude Code五个AI助手，分析时间趋势和不同任务类型的接受率差异。

Result: Devin是唯一呈现持续正向时间趋势的助手（每周+0.77%）；文档任务接受率最高（82.1%），新功能任务最低（66.1%）；OpenAI Codex在9个任务类别中表现最稳定；不同助手在不同任务类型上各有优势。

Conclusion: AI编程助手的效果受任务类型显著影响，没有单一助手在所有任务类型上表现最佳，开发者应根据具体任务类型选择合适的AI助手。

Abstract: The rapid adoption of AI-powered coding assistants is transforming software development practices, yet systematic comparisons of their effectiveness across different task types and over time remain limited. This paper presents an empirical study comparing five popular agents (OpenAI Codex, GitHub Copilot, Devin, Cursor, and Claude Code), analyzing 7,156 pull requests (PRs) from the AIDev dataset. Temporal trend analysis reveals heterogeneous evolution patterns: Devin exhibits the only consistent positive trend in acceptance rate (+0.77% per week over 32 weeks), whereas other agents remain largely stable. Our analysis suggests that the PR task type is a dominant factor influencing acceptance rates: documentation tasks achieve 82.1% acceptance compared to 66.1% for new features - a 16 percentage point gap that exceeds typical inter-agent variance for most tasks. OpenAI Codex achieves consistently high acceptance rates across all nine task categories (59.6%-88.6%), with stratified Chi-square tests confirming statistically significant advantages over other agents in several task categories. However, no single agent performs best across all task types: Claude Code leads in documentation (92.3%) and features (72.6%), while Cursor excels in fix tasks (80.4%).

</details>


### [132] [Artificial Intelligence in Open Source Software Engineering: A Foundation for Sustainability](https://arxiv.org/abs/2602.07071)
*S M Rakib UI Karim,Wenyi Lu,Sean Goggins*

Main category: cs.SE

TL;DR: 这篇文献综述探讨了人工智能如何被用来解决开源软件可持续性面临的挑战，包括维护贡献者参与度、确保资金、代码质量与安全、社区健康以及防止项目废弃等问题。


<details>
  <summary>Details</summary>
Motivation: 开源软件是现代数字基础设施的基础，但在许多关键情况下仍然难以确保足够的贡献。研究旨在探索如何利用人工智能解决开源软件可持续性面临的挑战。

Method: 通过文献综述方法，综合近期的跨学科研究，分析人工智能在开源软件领域的应用，包括自动化bug分类、系统维护、贡献者引导、社区健康分析、漏洞检测和任务自动化等方面。

Result: 研究识别了人工智能在开源软件领域的关键应用，同时也揭示了应用人工智能的局限性和伦理问题，包括数据可用性、偏见与公平性、透明度、滥用风险以及保护协作开发中以人为本的价值观。

Conclusion: 研究强调人工智能不应被视为替代品，而是增强人类基础设施的工具。研究指出了关键的研究空白，并提出了人工智能、可持续性和开源软件交叉领域的未来研究方向，旨在支持更具韧性和公平性的开源生态系统。

Abstract: Open-source software (OSS) is foundational to modern digital infrastructure, yet this context for group work continues to struggle to ensure sufficient contributions in many critical cases. This literature review explores how artificial intelligence (AI) is being leveraged to address critical challenges to OSS sustainability, including maintaining contributor engagement, securing funding, ensuring code quality and security, fostering healthy community dynamics, and preventing project abandonment. Synthesizing recent interdisciplinary research, the paper identifies key applications of AI in this domain, including automated bug triaging, system maintenance, contributor onboarding and mentorship, community health analytics, vulnerability detection, and task automation. The review also examines the limitations and ethical concerns that arise from applying AI in OSS contexts, including data availability, bias and fairness, transparency, risks of misuse, and the preservation of human-centered values in collaborative development. By framing AI not as a replacement but as a tool to augment human infrastructure, this study highlights both the promise and pitfalls of AI-driven interventions. It concludes by identifying critical research gaps and proposing future directions at the intersection of AI, sustainability, and OSS, aiming to support more resilient and equitable open-source ecosystems.

</details>
