<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 13]
- [cs.CR](#cs.CR) [Total: 18]
- [cs.AI](#cs.AI) [Total: 22]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [AutoCode: LLMs as Problem Setters for Competitive Programming](https://arxiv.org/abs/2510.12803)
*Shang Zhou,Zihan Zheng,Kaiyuan Liu,Zeyu Shen,Zerui Cheng,Zexing Chen,Hansen He,Jianzhu Yao,Huanzhi Mao,Qiuyang Mang,Tianfu Fu,Beichen Li,Dongruixuan Li,Wenhao Chai,Zhuang Liu,Aleksandra Korolova,Peter Henderson,Natasha Jaques,Pramod Viswanath,Saining Xie,Jingbo Shang*

Main category: cs.SE

TL;DR: AutoCode是一个使用多轮验证生成竞赛级编程问题陈述和测试用例的系统，在保留问题上测试套件与官方评判的一致性接近99%，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 编写竞赛编程问题需要设置约束、输入分布和边界情况，针对特定算法，并校准超出大多数参赛者能力的复杂度，这使其成为测试大型语言模型通用能力的理想场景。

Method: AutoCode使用多轮验证来生成问题陈述和测试用例，通过交叉验证生成的参考解和暴力解与测试用例，进一步过滤有问题的题目。

Result: 在保留问题上，AutoCode测试套件与官方评判的一致性达到99%，显著优于HardTests的81%。系统能够生成被顶级程序员评为竞赛质量的新颖问题。

Conclusion: AutoCode能够可靠地生成竞赛级编程问题，通过多轮验证和交叉验证确保高质量，为测试大型语言模型能力提供了有效方法。

Abstract: Writing competitive programming problems is exacting. Authors must: set
constraints, input distributions, and edge cases that rule out shortcuts;
target specific algorithms (e.g., max-flow, dynamic programming, data
structures); and calibrate complexity beyond the reach of most competitors. We
argue that this makes for an ideal test of general large language model
capabilities and study whether they can do this reliably. We introduce
AutoCode, which uses multiple rounds of validation to yield competition-grade
problem statements and test cases. On held-out problems, AutoCode test suites
approach 99% consistency with official judgments, a significant improvement
over current state-of-the-art methods like HardTests, which achieve less than
81%. Furthermore, starting with a random seed problem, AutoCode can create
novel variants with reference and brute-force solutions. By cross-verifying
these generated solutions against test cases, we can further filter out
malformed problems. Our system ensures high correctness, as verified by human
experts. AutoCode successfully produces novel problems judged by
Grandmaster-level (top 0.3%) competitive programmers to be of contest quality.

</details>


### [2] [SpareCodeSearch: Searching for Code Context When You Have No Spare GPU](https://arxiv.org/abs/2510.12948)
*Minh Nguyen*

Main category: cs.SE

TL;DR: 本文证明在大型代码库中使用关键词搜索足以检索相关代码上下文，无需大量GPU资源，在代码补全任务中表现良好。


<details>
  <summary>Details</summary>
Motivation: 现有的检索增强生成框架通常使用语义搜索，需要大量计算资源训练和部署嵌入模型，难以集成到轻量级应用中，如IDE内的AI代码补全。

Method: 使用关键词搜索替代语义搜索来检索相关代码上下文，避免对GPU资源的依赖。

Result: 在Code Context Competition基准测试中，Kotlin和Python轨道的chRF分数分别达到0.748和0.725。

Conclusion: 关键词搜索足以有效检索代码上下文，为轻量级应用提供了可行的解决方案。

Abstract: Retrieval-Augmented Generation (RAG) frameworks aim to enhance Code Language
Models (CLMs) by including another module for retrieving relevant context to
construct the input prompt. However, these retrieval modules commonly use
semantic search, requiring substantial computational resources for training and
hosting these embedded models, making them infeasible to integrate into
lightweight applications such as in-IDE AI-based code completion. In this
solution paper, we prove that using keyword-search is sufficient to retrieve
relevant and useful code context inside large codebases, without the need for
extensive GPU resources. The usefulness of code contexts found by our solution
is demonstrated through their completion results on the Code Context
Competition's benchmark, reaching 0.748 and 0.725 chRF scores on Kotlin and
Python tracks, respectively.

</details>


### [3] [ADPerf: Investigating and Testing Performance in Autonomous Driving Systems](https://arxiv.org/abs/2510.13078)
*Tri Minh-Triet Pham,Diego Elias Costa,Weiyi Shang,Jinqiu Yang*

Main category: cs.SE

TL;DR: 本文提出了ADPerf工具，用于测试自动驾驶系统中障碍物检测模块的性能，特别是3D障碍物检测的延迟问题及其对其他模块的影响。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶系统的障碍物检测延迟对安全性至关重要，但目前对其延迟特性及对点云数据变化的弹性理解不足，需要进行系统性的性能测试。

Method: 开发了ADPerf工具，生成逼真的点云数据测试用例来暴露增加的检测延迟，并在Apollo和Autoware两个工业级自动驾驶系统中进行测试。

Result: 评估显示3D障碍物检测模块可能成为自动驾驶系统延迟的主要瓶颈，这种不利影响会进一步传播到其他模块，降低系统整体可靠性。

Conclusion: 需要进行障碍物检测组件的性能测试，特别是3D障碍物检测，因为它们是自动驾驶系统延迟增加的主要瓶颈，会影响后续模块的性能。

Abstract: Obstacle detection is crucial to the operation of autonomous driving systems,
which rely on multiple sensors, such as cameras and LiDARs, combined with code
logic and deep learning models to detect obstacles for time-sensitive
decisions. Consequently, obstacle detection latency is critical to the safety
and effectiveness of autonomous driving systems. However, the latency of the
obstacle detection module and its resilience to various changes in the LiDAR
point cloud data are not yet fully understood. In this work, we present the
first comprehensive investigation on measuring and modeling the performance of
the obstacle detection modules in two industry-grade autonomous driving
systems, i.e., Apollo and Autoware. Learning from this investigation, we
introduce ADPerf, a tool that aims to generate realistic point cloud data test
cases that can expose increased detection latency. Increasing latency decreases
the availability of the detected obstacles and stresses the capabilities of
subsequent modules in autonomous driving systems, i.e., the modules may be
negatively impacted by the increased latency in obstacle detection.
  We applied ADPerf to stress-test the performance of widely used 3D obstacle
detection modules in autonomous driving systems, as well as the propagation of
such tests on trajectory prediction modules. Our evaluation highlights the need
to conduct performance testing of obstacle detection components, especially 3D
obstacle detection, as they can be a major bottleneck to increased latency of
the autonomous driving system. Such an adverse outcome will also further
propagate to other modules, reducing the overall reliability of autonomous
driving systems.

</details>


### [4] [TRUSTVIS: A Multi-Dimensional Trustworthiness Evaluation Framework for Large Language Models](https://arxiv.org/abs/2510.13106)
*Ruoyu Sun,Da Song,Jiayang Song,Yuheng Huang,Lei Ma*

Main category: cs.SE

TL;DR: TRUSTVIS是一个自动化评估框架，通过交互式界面可视化大语言模型的信任度指标，结合AutoDAN等扰动方法和多数投票机制，有效识别模型的安全性和鲁棒性漏洞。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型在NLP应用中的广泛使用，其安全性和鲁棒性等可信度问题日益突出，需要系统化的评估方法来解决这些挑战。

Method: 集成AutoDAN等知名扰动方法，采用多数投票机制整合多种评估方法，并提供交互式用户界面直观展示信任度指标。

Result: 在Vicuna-7b、Llama2-7b和GPT-3.5等模型上的初步案例研究表明，该框架能有效识别安全性和鲁棒性漏洞。

Conclusion: TRUSTVIS框架不仅提供可靠的评估结果，还使复杂的评估过程对用户更加友好，有助于针对性的模型改进。

Abstract: As Large Language Models (LLMs) continue to revolutionize Natural Language
Processing (NLP) applications, critical concerns about their trustworthiness
persist, particularly in safety and robustness. To address these challenges, we
introduce TRUSTVIS, an automated evaluation framework that provides a
comprehensive assessment of LLM trustworthiness. A key feature of our framework
is its interactive user interface, designed to offer intuitive visualizations
of trustworthiness metrics. By integrating well-known perturbation methods like
AutoDAN and employing majority voting across various evaluation methods,
TRUSTVIS not only provides reliable results but also makes complex evaluation
processes accessible to users. Preliminary case studies on models like
Vicuna-7b, Llama2-7b, and GPT-3.5 demonstrate the effectiveness of our
framework in identifying safety and robustness vulnerabilities, while the
interactive interface allows users to explore results in detail, empowering
targeted model improvements. Video Link: https://youtu.be/k1TrBqNVg8g

</details>


### [5] [Isolating Compiler Bugs through Compilation Steps Analysis](https://arxiv.org/abs/2510.13128)
*Yujie Liu,Mingxuan Zhu,Shengyu Cheng,Dan Hao*

Main category: cs.SE

TL;DR: CompSCAN是一种新颖的编译器bug隔离技术，通过分析编译步骤序列来识别导致bug的编译器代码元素，在LLVM和GCC的真实bug上表现出优于现有技术的效果和效率。


<details>
  <summary>Details</summary>
Motivation: 编译器bug会传播到依赖软件中，确保编译器正确性至关重要。现有技术主要通过变异编译输入生成测试用例，但缺乏对内部步骤的因果分析，限制了其有效性。

Method: CompSCAN采用三阶段流程：(1)提取导致原始失败的编译步骤序列，(2)识别bug导致步骤并收集对应的编译器代码元素，(3)计算每个代码元素的可疑度并输出可疑度排名列表。

Result: 在185个真实LLVM和GCC bug上评估，CompSCAN在前1/3/5/10名中分别成功隔离了50、85、100和123个bug。相比ETEM和ODFL技术，在各项指标上分别实现了44.51%/50.18%/36.24%/24.49%和31.58%/49.12%/44.93%/21.78%的相对改进，且运行速度更快。

Conclusion: CompSCAN通过分析编译步骤序列有效提升了编译器bug隔离的准确性和效率，显著优于现有技术。

Abstract: Compilers are essential to software systems, and their bugs can propagate to
dependent software. Ensuring compiler correctness is critical. However,
isolating compiler bugs remains challenging due to the internal complexity of
compiler execution. Existing techniques primarily mutate compilation inputs to
generate passing and failing tests, but often lack causal analysis of internal
steps, limiting their effectiveness.
  To address this limitation, we propose CompSCAN, a novel compiler bug
isolation technique that applies analysis over the sequence of compilation
steps. CompSCAN follows a three-stage process: (1) extracting the array of
compilation steps that leads to the original failure, (2) identifying
bug-causing steps and collecting corresponding compiler code elements, and (3)
calculating suspicious scores for each code element and outputting a suspicious
ranking list as the bug isolation result.
  We evaluate CompSCAN on 185 real-world LLVM and GCC bugs. Results show that
CompSCAN outperforms state-of-the-art techniques in both effectiveness and
efficiency. CompSCAN successfully isolates 50, 85, 100, and 123 bugs within the
Top-1/3/5/10 ranks, respectively. Compared with ETEM and ODFL, two
state-of-the-art compiler bug isolation techniques, CompSCAN achieves relative
improvements of 44.51% / 50.18% / 36.24% / 24.49% over ETEM, and 31.58% /
49.12% / 44.93% / 21.78% over ODFL on those metrics. Moreover, CompSCAN runs
faster on average per bug than both baselines.

</details>


### [6] [GRACE: Globally-Seeded Representation-Aware Cluster-Specific Evolution for Compiler Auto-Tuning](https://arxiv.org/abs/2510.13176)
*Haolin Pan,Chao Zha,Jinyuan Dong,Mingjie Xing,Yanjun Wu*

Main category: cs.SE

TL;DR: GRACE是一个编译器自动调优框架，通过利用pass协同效应和对比学习来减少搜索空间，生成专门的pass序列，在LLVM IR指令数优化上实现了显著的性能提升。


<details>
  <summary>Details</summary>
Motivation: 标准编译器启发式方法通常产生次优结果，而迭代编译搜索成本过高，机器学习方法在泛化到未见程序时存在困难，需要一种既高效又能良好泛化的编译器自动调优方法。

Method: GRACE通过pass协同效应和加权评分生成高质量候选序列和pass池，使用对比学习和数据增强创建程序嵌入进行相似性聚类，在聚类内进行进化搜索生成专门的pass序列核心集。

Result: 在7个不同数据集上，GRACE相比opt -Oz平均减少LLVM IR指令数10.09%(LLVM 10.0.0)和10.19%(LLVM 18.1.6)，平均调优时间每程序小于1秒。

Conclusion: GRACE在编译器自动调优方面实现了最先进的性能和实际有效性，能够高效地生成专门化的pass序列并良好泛化到未见程序。

Abstract: Compiler pass selection and phase ordering present a significant challenge in
achieving optimal program performance, particularly for objectives like code
size reduction. Standard compiler heuristics offer general applicability but
often yield suboptimal, program-specific results due to their one-size-fits-all
nature. While iterative compilation can find tailored solutions, its
prohibitive search cost limits practical use. Machine learning approaches
promise faster inference but frequently struggle with generalization to unseen
programs. This paper introduces GRACE, a novel framework for compiler
auto-tuning, demonstrated for LLVM IR instruction count optimization. GRACE
effectively curtails the search space by leveraging pass synergies and a
weighted scoring method to generate initial high-quality candidate sequences
and a pass pool. It then employs contrastive learning, using pass
sequence-based data augmentation, to create program embeddings that facilitate
similarity-aware clustering. Evolutionary search within these clusters yields a
coreset of $k$ specialized pass sequences designed for robust generalization to
unseen programs. At test time, GRACE efficiently selects the best coreset
sequence and refines it using lightweight techniques. Experimental results on
seven diverse datasets show that GRACE reduces LLVM IR instruction count by an
average of 10.09% on LLVM 10.0.0 and 10.19% on LLVM 18.1.6 compared to opt -Oz,
while incurring an average tuning time of less than 1s per program,
demonstrating its state-of-the-art performance and practical effectiveness.

</details>


### [7] [Synergy-Guided Compiler Auto-Tuning of Nested LLVM Pass Pipelines](https://arxiv.org/abs/2510.13184)
*Haolin Pan,Jinyuan Dong,Mingjie Xing,Yanjun Wu*

Main category: cs.SE

TL;DR: 提出了一个专为LLVM新Pass管理器设计的编译器自动调优框架，使用形式化语法定义有效嵌套管道空间，通过结构感知遗传算法确保所有候选方案在构造时都是有效的。


<details>
  <summary>Details</summary>
Motivation: 现有的编译器自动调优方法假设线性pass序列，这与LLVM新Pass管理器的分层设计不匹配，无法保证生成语法有效的优化管道。

Method: 引入形式化语法定义有效嵌套管道空间，使用基于森林的数据结构进行原生表示，开发结构感知遗传算法直接操作这些森林，确保所有候选方案在构造时有效。

Result: 在LLVM 18.1.6上评估7个基准数据集，发现的管道相比标准opt -Oz优化级别平均实现了13.62%的额外指令计数减少。

Conclusion: 该框架能够在这个复杂、受约束的搜索空间中导航，识别有效且高效的pass管道。

Abstract: Compiler optimization relies on sequences of passes to improve program
performance. Selecting and ordering these passes automatically, known as
compiler auto-tuning, is challenging due to the large and complex search space.
Existing approaches generally assume a linear sequence of passes, a model
compatible with legacy compilers but fundamentally misaligned with the
hierarchical design of the LLVM New Pass Manager. This misalignment prevents
them from guaranteeing the generation of syntactically valid optimization
pipelines. In this work, we present a new auto-tuning framework built from the
ground up for the New Pass Manager. We introduce a formal grammar to define the
space of valid nested pipelines and a forest-based data structure for their
native representation. Upon this foundation, we develop a structure-aware
Genetic Algorithm whose operators manipulate these forests directly, ensuring
that all candidate solutions are valid by construction. The framework first
mines synergistic pass relationships to guide the search. An optional
refinement stage further explores subtle performance variations arising from
different valid structural arrangements.
  We evaluate our approach on seven benchmark datasets using LLVM 18.1.6. The
discovered pipelines achieve an average of 13.62% additional instruction count
reduction compared to the standard opt -Oz optimization level, showing that our
framework is capable of navigating this complex, constrained search space to
identify valid and effective pass pipelines.

</details>


### [8] [Towards Richer Challenge Problems for Scientific Computing Correctness](https://arxiv.org/abs/2510.13423)
*Matthew Sottile,Mohit Tekriwal,John Sarracino*

Main category: cs.SE

TL;DR: 本文呼吁为科学计算(SC)领域开发专门的挑战性问题，以指导形式化方法(FM)和编程语言(PL)验证技术的发展，解决现有验证技术在应对现实SC应用复杂性方面的不足。


<details>
  <summary>Details</summary>
Motivation: 科学计算中的正确性问题日益受到关注，但现有的PL/FM验证技术难以应对现实SC应用的复杂性，部分原因是SC和PL/FM社区之间缺乏对机器可验证正确性挑战和SC应用正确性维度的共同理解。

Method: 提出为科学计算开发专门的挑战性问题，这些挑战旨在补充FM/PL研究人员为通用程序研究的问题，确保满足SC应用的需求。提出了与科学计算相关的几个正确性维度，并讨论了设计评估科学计算正确性的挑战问题的指导原则和标准。

Result: 提出了开发专门挑战问题的呼吁，并定义了科学计算相关的正确性维度和挑战问题设计指南。

Conclusion: 通过开发专门的挑战问题，可以更好地指导FM/PL验证技术的发展，弥合SC和PL/FM社区之间的理解差距，提高科学计算应用的正确性验证能力。

Abstract: Correctness in scientific computing (SC) is gaining increasing attention in
the formal methods (FM) and programming languages (PL) community. Existing
PL/FM verification techniques struggle with the complexities of realistic SC
applications. Part of the problem is a lack of a common understanding between
the SC and PL/FM communities of machine-verifiable correctness challenges and
dimensions of correctness in SC applications.
  To address this gap, we call for specialized challenge problems to inform the
development and evaluation of FM/PL verification techniques for correctness in
SC. These specialized challenges are intended to augment existing problems
studied by FM/PL researchers for general programs to ensure the needs of SC
applications can be met. We propose several dimensions of correctness relevant
to scientific computing, and discuss some guidelines and criteria for designing
challenge problems to evaluate correctness in scientific computing.

</details>


### [9] [Verifying a Sparse Matrix Algorithm Using Symbolic Execution](https://arxiv.org/abs/2510.13424)
*Alexander C. Wilton*

Main category: cs.SE

TL;DR: 使用符号执行来测试科学软件，特别是稀疏矩阵算法，提供比传统测试更强的验证保证


<details>
  <summary>Details</summary>
Motivation: 科学软件具有复杂性、数学性和高度优化的特点，容易产生传统测试难以发现的微妙bug

Method: 采用符号执行方法编写类似传统单元测试的测试，并将其应用于稀疏矩阵算法

Result: 该方法能够提供比传统测试更强的验证保证

Conclusion: 符号执行是测试科学软件的有效方法，特别适用于检测传统测试难以发现的微妙bug

Abstract: Scientific software is, by its very nature, complex. It is mathematical and
highly optimized which makes it prone to subtle bugs not as easily detected by
traditional testing. We outline how symbolic execution can be used to write
tests similar to traditional unit tests while providing stronger verification
guarantees and apply this methodology to a sparse matrix algorithm.

</details>


### [10] [OpenDerisk: An Industrial Framework for AI-Driven SRE, with Design, Implementation, and Case Studies](https://arxiv.org/abs/2510.13561)
*Peng Di,Faqiang Chen,Xiao Bai,Hongjun Yang,Qingfeng Li,Ganglin Wei,Jian Mou,Feng Shi,Keting Chen,Peng Tang,Zhitao Shen,Zheng Li,Wenhui Shi,Junwei Guo,Hang Yu*

Main category: cs.SE

TL;DR: OpenDerisk是一个专门为SRE设计的开源多智能体框架，通过诊断原生协作模型、可插拔推理引擎和知识引擎，显著提升了复杂软件问题的诊断准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 现代软件复杂性不断增加，给SRE团队带来不可持续的操作负担，现有AI解决方案要么缺乏深度因果推理能力，要么不适用于SRE特有的专业调查工作流程。

Method: 开发了OpenDerisk框架，集成了诊断原生协作模型、可插拔推理引擎、知识引擎和标准化协议(MCP)，使专业智能体能够协作解决复杂的多领域问题。

Result: 综合评估显示OpenDerisk在准确性和效率上显著优于最先进的基线方法，已在蚂蚁集团大规模生产部署，服务超过3000名日常用户，验证了其工业级可扩展性和实际影响。

Conclusion: OpenDerisk成功解决了SRE领域AI自动化的关键挑战，提供了专门化的多智能体解决方案，具有重要的工业应用价值。

Abstract: The escalating complexity of modern software imposes an unsustainable
operational burden on Site Reliability Engineering (SRE) teams, demanding
AI-driven automation that can emulate expert diagnostic reasoning. Existing
solutions, from traditional AI methods to general-purpose multi-agent systems,
fall short: they either lack deep causal reasoning or are not tailored for the
specialized, investigative workflows unique to SRE. To address this gap, we
present OpenDerisk, a specialized, open-source multi-agent framework
architected for SRE. OpenDerisk integrates a diagnostic-native collaboration
model, a pluggable reasoning engine, a knowledge engine, and a standardized
protocol (MCP) to enable specialist agents to collectively solve complex,
multi-domain problems. Our comprehensive evaluation demonstrates that
OpenDerisk significantly outperforms state-of-the-art baselines in both
accuracy and efficiency. This effectiveness is validated by its large-scale
production deployment at Ant Group, where it serves over 3,000 daily users
across diverse scenarios, confirming its industrial-grade scalability and
practical impact. OpenDerisk is open source and available at
https://github.com/derisk-ai/OpenDerisk/

</details>


### [11] [Auto-repair without test cases: How LLMs fix compilation errors in large industrial embedded code](https://arxiv.org/abs/2510.13575)
*Han Fu,Sigrid Eldh,Kristian Wiklund,Andreas Ermedahl,Philipp Haller,Cyrille Artho*

Main category: cs.SE

TL;DR: 使用大型语言模型自动修复工业嵌入式系统中编译错误，在CI系统中可解决63%的编译错误，83%的修复方案合理，显著减少调试时间


<details>
  <summary>Details</summary>
Motivation: 工业嵌入式系统的软硬件协同开发在持续集成中经常出现编译错误，现有修复技术依赖测试用例，但不可编译代码没有测试用例可用

Method: 采用基于大型语言模型的自动修复方法，收集超过40000个产品源代码提交，评估四种最先进LLM在工业CI系统中的性能

Result: LLM增强的CI系统可解决基线数据集中63%的编译错误，成功CI构建的修复方案中83%是合理的，多数成功案例在8分钟内完成，相比手动调试需要数小时

Conclusion: 大型语言模型在自动修复编译错误方面表现优异，能显著提高工业嵌入式系统开发效率

Abstract: The co-development of hardware and software in industrial embedded systems
frequently leads to compilation errors during continuous integration (CI).
Automated repair of such failures is promising, but existing techniques rely on
test cases, which are not available for non-compilable code.
  We employ an automated repair approach for compilation errors driven by large
language models (LLMs). Our study encompasses the collection of more than 40000
commits from the product's source code. We assess the performance of an
industrial CI system enhanced by four state-of-the-art LLMs, comparing their
outcomes with manual corrections provided by human programmers. LLM-equipped CI
systems can resolve up to 63 % of the compilation errors in our baseline
dataset. Among the fixes associated with successful CI builds, 83 % are deemed
reasonable. Moreover, LLMs significantly reduce debugging time, with the
majority of successful cases completed within 8 minutes, compared to hours
typically required for manual debugging.

</details>


### [12] [Property Testing for Ocean Models. Can We Specify It? (Invited Talk)](https://arxiv.org/abs/2510.13692)
*Deepak A. Cherian*

Main category: cs.SE

TL;DR: 探索如何将属性测试思想应用于海洋数值模型，利用地球物理流体动力学理论作为属性测试来解决海洋模型正确性验证的oracle问题


<details>
  <summary>Details</summary>
Motivation: 从属性测试文献中获得灵感，特别是John Hughes教授的工作，探索这些思想如何应用于海洋数值模型，解决模型正确性验证的挑战

Method: 提出将简单理想化的地球物理流体动力学问题框架化为属性测试，展示物理学如何自然地适用于属性测试规范

Result: 通过示例清晰说明了物理学如何自然地适用于指定属性测试，但哪些测试最可行和有用仍需进一步研究

Conclusion: 地球物理流体动力学理论可以有效地框架化为属性测试，为解决海洋模型正确性验证的oracle问题提供了有前景的方法

Abstract: I take inspiration from the property-testing literature, particularly the
work of Prof. John Hughes, and explore how such ideas might be applied to
numerical models of the ocean. Specifically, I ask whether geophysical fluid
dynamics (GFD) theory, expressed as property tests, might be used to address
the oracle problem of testing the correctness of ocean models. I propose that a
number of simple idealized GFD problems can be framed as property tests. These
examples clearly illustrate how physics naturally lends itself to specifying
property tests. Which of these proposed tests might be most feasible and
useful, remains to be seen.

</details>


### [13] [On Pretraining for Project-Level Code Completion](https://arxiv.org/abs/2510.13697)
*Maksim Sapronov,Evgeniy Glukhov*

Main category: cs.SE

TL;DR: 该论文研究了不同代码库处理策略对OpenCoder模型上下文学习的影响，通过扩展上下文窗口和优化位置编码，在较小数据集上实现了与大型模型相当的性能。


<details>
  <summary>Details</summary>
Motivation: 探索代码库级别的预训练如何帮助语言模型生成更准确和上下文感知的代码补全，特别是在资源受限的环境中。

Method: 将OpenCoder模型的上下文窗口从4,096扩展到16,384个token，在额外10亿token的代码库数据上训练，并测试不同的代码库处理策略和RoPE位置编码缩放参数。

Result: 尽管使用比竞争模型小得多的数据集，该模型在Long Code Arena基准测试中实现了可比性能，且发现不同代码库处理技术产生相似结果，主要收益来自RoPE缩放参数的调整。

Conclusion: 简单的文件级训练方法在原始序列长度下仍然非常有效，这为数据资源和计算能力受限的环境中的代码库级代码补全研究开辟了可能性。

Abstract: Repository-level pretraining is commonly used to enable large language models
for code to leverage codebase-wide context. This enhances their ability to
generate accurate and context-aware code completions. In this work, we
investigate how different repository-processing strategies affect in-context
learning in OpenCoder, a 1.5B-parameter model. We extend its context window
from 4,096 to 16,384 tokens by training on additional 1B tokens of curated
repository-level data. Despite relying on a smaller dataset than competing
models (which often use hundreds of billions of tokens), our model achieves
comparable performance on the Long Code Arena benchmark. We find that various
repository-processing techniques yield similarly strong results, with the
primary gain coming from adapting to a new rotary positional embedding (RoPE)
scaling parameter. Finally, we show that a simpler file-level training approach
at the original sequence length remains highly effective, opening up
repository-level code completion research to settings with more constrained
data and compute resources.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [14] [The Beautiful Deception: How 256 Bits Pretend to be Infinity](https://arxiv.org/abs/2510.12802)
*Alexander Towell*

Main category: cs.CR

TL;DR: 该论文探讨了计算密码学中的核心欺骗：用有限信息模拟无限随机性，证明256位熵可以生成对计算有限观察者来说与无限随机性无法区分的序列。


<details>
  <summary>Details</summary>
Motivation: 研究计算密码学中如何用有限信息模拟无限随机性的基本原理，揭示密码学中"随机性"实际上是计算难度的伪装。

Method: 通过证明真随机预言机的不可能性，展示惰性评估如何创建有限自动机成功伪装成无限系统，并用Python实现演示。

Result: 证明了256位熵可以生成对计算有限观察者来说与无限随机性无法区分的序列，揭示了密码学随机性的本质是计算难度。

Conclusion: 密码学中的随机性实际上是计算难度的伪装，通过有限信息可以成功模拟无限随机性，这对理解密码学基础具有重要意义。

Abstract: How do you store infinity in 256 bits? This paper explores the fundamental
deception at the heart of computational cryptography: using finite information
to simulate infinite randomness. We prove why true random oracles are
impossible, then show how lazy evaluation creates a beautiful lie -- a finite
automaton that successfully pretends to be infinite. We reveal that
``randomness'' in cryptography is actually computational hardness in disguise,
demonstrating through Python implementations how 256 bits of entropy can
generate sequences indistinguishable from infinite randomness to any
computationally bounded observer.How do you store infinity in 256 bits? This
paper explores the fundamental deception at the heart of computational
cryptography: using finite information to simulate infinite randomness. We
prove why true random oracles are impossible, then show how lazy evaluation
creates a beautiful lie -- a finite automaton that successfully pretends to be
infinite. We reveal that ``randomness'' in cryptography is actually
computational hardness in disguise, demonstrating through Python
implementations how 256 bits of entropy can generate sequences
indistinguishable from infinite randomness to any computationally bounded
observer.

</details>


### [15] [Applying Graph Analysis for Unsupervised Fast Malware Fingerprinting](https://arxiv.org/abs/2510.12811)
*ElMouatez Billah Karbab,Mourad Debbabi*

Main category: cs.CR

TL;DR: TrapNet是一个新颖、可扩展、无监督的恶意软件指纹识别和分组框架，使用图社区检测技术进行基于静态分析的恶意软件家族归因。


<details>
  <summary>Details</summary>
Motivation: 恶意软件数量急剧增长，每天有数十万个新样本，手动分析不现实。需要开发专门技术来基于语义相似性对恶意软件进行初步过滤和分组。

Method: 1) 检测加壳二进制文件并使用通用脱壳工具解包；2) 设计FloatHash(FH)数值模糊哈希技术，通过PCA对汇编代码中的有序汇编项生成短实数向量；3) 构建恶意软件相似性网络；4) 使用社区检测算法识别密集社区。

Result: 广泛评估表明TrapNet在检测社区的覆盖率和纯度方面有效，同时运行效率优于其他最先进解决方案。

Conclusion: TrapNet框架在恶意软件指纹识别和分组方面表现出色，能够高效处理大规模恶意软件样本。

Abstract: Malware proliferation is increasing at a tremendous rate, with hundreds of
thousands of new samples identified daily. Manual investigation of such a vast
amount of malware is an unrealistic, time-consuming, and overwhelming task. To
cope with this volume, there is a clear need to develop specialized techniques
and efficient tools for preliminary filtering that can group malware based on
semantic similarity. In this paper, we propose TrapNet, a novel, scalable, and
unsupervised framework for malware fingerprinting and grouping. TrapNet employs
graph community detection techniques for malware fingerprinting and family
attribution based on static analysis, as follows: (1) TrapNet detects packed
binaries and unpacks them using known generic packer tools. (2) From each
malware sample, it generates a digest that captures the underlying semantics.
Since the digest must be dense, efficient, and suitable for similarity
checking, we designed FloatHash (FH), a novel numerical fuzzy hashing technique
that produces a short real-valued vector summarizing the underlying assembly
items and their order. FH is based on applying Principal Component Analysis
(PCA) to ordered assembly items (e.g., opcodes, function calls) extracted from
the malware's assembly code. (3) Representing malware with short numerical
vectors enables high-performance, large-scale similarity computation, which
allows TrapNet to build a malware similarity network. (4) Finally, TrapNet
employs state-of-the-art community detection algorithms to identify dense
communities, which represent groups of malware with similar semantics. Our
extensive evaluation of TrapNet demonstrates its effectiveness in terms of the
coverage and purity of the detected communities, while also highlighting its
runtime efficiency, which outperforms other state-of-the-art solutions.

</details>


### [16] [We Can Hide More Bits: The Unused Watermarking Capacity in Theory and in Practice](https://arxiv.org/abs/2510.12812)
*Aleksandar Petrov,Pierre Fernandez,Tomáš Souček,Hady Elsahar*

Main category: cs.CR

TL;DR: 该论文分析了图像水印的理论容量上限，发现当前方法远未达到理论极限，并通过训练ChunkySeal模型将容量提升4倍至1024位，证明了仍有巨大改进空间。


<details>
  <summary>Details</summary>
Motivation: 当前深度学习图像水印方法的容量停滞在几百位水平，需要探究这是否接近了理论极限，以及是否存在提升空间。

Method: 建立PSNR和线性鲁棒性约束下的图像水印消息携带容量上界分析，并训练ChunkySeal（VideoSeal的扩展版本）来验证更大容量的可行性。

Result: 理论分析显示容量上限比当前方法高几个数量级，实验证明即使在简单设置下理论与实际性能差距依然存在。ChunkySeal成功将容量提升至1024位，同时保持图像质量和鲁棒性。

Conclusion: 现代水印方法远未达到容量极限，在架构创新和训练策略方面仍有显著改进机会。

Abstract: Despite rapid progress in deep learning-based image watermarking, the
capacity of current robust methods remains limited to the scale of only a few
hundred bits. Such plateauing progress raises the question: How far are we from
the fundamental limits of image watermarking? To this end, we present an
analysis that establishes upper bounds on the message-carrying capacity of
images under PSNR and linear robustness constraints. Our results indicate
theoretical capacities are orders of magnitude larger than what current models
achieve. Our experiments show this gap between theoretical and empirical
performance persists, even in minimal, easily analysable setups. This suggests
a fundamental problem. As proof that larger capacities are indeed possible, we
train ChunkySeal, a scaled-up version of VideoSeal, which increases capacity 4
times to 1024 bits, all while preserving image quality and robustness. These
findings demonstrate modern methods have not yet saturated watermarking
capacity, and that significant opportunities for architectural innovation and
training strategies remain.

</details>


### [17] [ARTeX: Anonymity Real-world-assets Token eXchange](https://arxiv.org/abs/2510.12821)
*Jaeseong Lee,Junghee Lee*

Main category: cs.CR

TL;DR: 本文针对现实世界资产代币交易中的隐私问题，提出了ARTeX交易平台，在保护交易者匿名性的同时增强对非法活动的防范。


<details>
  <summary>Details</summary>
Motivation: 区块链技术的透明性原则导致交易者匿名性无法保障，而现有的混币服务和NFT隐私保护方法由于RWA代币的特殊性和各自局限性，难以有效实现匿名保护目标。

Method: 提出新的代币交易平台ARTeX，该平台不仅解决现有方法的不足，还能确保交易者匿名性并增强对非法活动的防护。

Result: ARTeX平台能够有效解决RWA代币交易中的隐私问题，克服现有方法的局限性。

Conclusion: ARTeX平台为解决RWA代币交易隐私问题提供了有效方案，在保护匿名性的同时兼顾了合规性要求。

Abstract: This paper addresses one of the most noteworthy issues in the recent virtual
asset market, the privacy concerns related to token transactions of Real-World
Assets tokens, known as RWA tokens. Following the advent of Bitcoin, the
virtual asset market has experienced explosive growth, spawning movements to
link real-world assets with virtual assets. However, due to the transparency
principle of blockchain technology, the anonymity of traders cannot be
guaranteed. In the existing blockchain environment, there have been instances
of protecting the privacy of fungible tokens (FTs) using mixer services.
Moreover, numerous studies have been conducted to secure the privacy of
non-fungible tokens (NFTs). However, due to the unique characteristics of RWA
tokens and the limitations of each study, it has been challenging to achieve
the goal of anonymity protection effectively. This paper proposes a new token
trading platform, the ARTeX, designed to resolve these issues. This platform
not only addresses the shortcomings of existing methods but also ensures the
anonymity of traders while enhancing safeguards against illegal activities.

</details>


### [18] [SimKey: A Semantically Aware Key Module for Watermarking Language Models](https://arxiv.org/abs/2510.12828)
*Shingo Kodama,Haya Diwan,Lucas Rosenblatt,R. Teal Witter,Niv Cohen*

Main category: cs.CR

TL;DR: SimKey是一种语义密钥模块，通过将密钥生成与上下文语义绑定来增强水印鲁棒性，解决了传统水印方法对表面编辑脆弱和恶意内容误归因的问题。


<details>
  <summary>Details</summary>
Motivation: 随着LLM生成文本的快速传播，区分人类写作和机器输出变得困难。现有水印方法对改写和重排序等表面编辑脆弱，且恶意内容可能继承水印导致模型所有者声誉受损。

Method: SimKey使用局部敏感哈希对语义嵌入进行处理，确保改写文本产生相同水印密钥，而无关或语义偏移文本产生不同密钥。该方法与最先进的水印方案集成。

Result: SimKey提高了水印对改写和翻译的鲁棒性，同时防止有害内容被错误归因。

Conclusion: 语义感知密钥生成是一种实用且可扩展的水印方向，为LLM生成文本的可靠溯源提供了有效解决方案。

Abstract: The rapid spread of text generated by large language models (LLMs) makes it
increasingly difficult to distinguish authentic human writing from machine
output. Watermarking offers a promising solution: model owners can embed an
imperceptible signal into generated text, marking its origin. Most leading
approaches seed an LLM's next-token sampling with a pseudo-random key that can
later be recovered to identify the text as machine-generated, while only
minimally altering the model's output distribution. However, these methods
suffer from two related issues: (i) watermarks are brittle to simple
surface-level edits such as paraphrasing or reordering; and (ii) adversaries
can append unrelated, potentially harmful text that inherits the watermark,
risking reputational damage to model owners. To address these issues, we
introduce SimKey, a semantic key module that strengthens watermark robustness
by tying key generation to the meaning of prior context. SimKey uses
locality-sensitive hashing over semantic embeddings to ensure that paraphrased
text yields the same watermark key, while unrelated or semantically shifted
text produces a different one. Integrated with state-of-the-art watermarking
schemes, SimKey improves watermark robustness to paraphrasing and translation
while preventing harmful content from false attribution, establishing
semantic-aware keying as a practical and extensible watermarking direction.

</details>


### [19] [Local Differential Privacy for Federated Learning with Fixed Memory Usage and Per-Client Privacy](https://arxiv.org/abs/2510.12908)
*Rouzbeh Behnia,Jeremiah Birrell,Arman Riasi,Reza Ebrahimi,Kaushik Dutta,Thang Hoang*

Main category: cs.CR

TL;DR: 提出了L-RDP方法，一种专为联邦学习设计的本地差分隐私方法，旨在解决现有LDP方法在FL中的高资源需求和隐私保证不足问题。


<details>
  <summary>Details</summary>
Motivation: 联邦学习中客户端更新和全局模型可能泄露隐私信息，现有本地差分隐私方法在FL中面临高资源需求导致客户端退出、异步参与下缺乏可靠隐私保证等问题。

Method: 提出L-RDP方法，通过确保恒定较低的内存使用来减少客户端退出，并通过考虑间歇性参与提供严格的每客户端隐私保证。

Result: L-RDP方法解决了现有LDP方法在联邦学习中的局限性，降低了资源需求，增强了隐私保护。

Conclusion: L-RDP为联邦学习提供了一种更有效的本地差分隐私解决方案，有助于在医疗等敏感领域的安全应用。

Abstract: Federated learning (FL) enables organizations to collaboratively train models
without sharing their datasets. Despite this advantage, recent studies show
that both client updates and the global model can leak private information,
limiting adoption in sensitive domains such as healthcare. Local differential
privacy (LDP) offers strong protection by letting each participant privatize
updates before transmission. However, existing LDP methods were designed for
centralized training and introduce challenges in FL, including high resource
demands that can cause client dropouts and the lack of reliable privacy
guarantees under asynchronous participation. These issues undermine model
generalizability, fairness, and compliance with regulations such as HIPAA and
GDPR. To address them, we propose L-RDP, a DP method designed for LDP that
ensures constant, lower memory usage to reduce dropouts and provides rigorous
per-client privacy guarantees by accounting for intermittent participation.

</details>


### [20] [From misinformation to climate crisis: Navigating vulnerabilities in the cyber-physical-social systems](https://arxiv.org/abs/2510.13058)
*Tooba Aamir,Marthie Grobler,Giovanni Russello*

Main category: cs.CR

TL;DR: 本章探讨了网络-物理-社会-气候系统中人类脆弱性的关键作用，分析了跨系统依赖关系如何放大气候风险，并提出了通过解决人类认知偏差和决策框架来增强系统韧性的策略。


<details>
  <summary>Details</summary>
Motivation: 在网络-物理-社会-气候系统中，虽然网络和物理脆弱性显而易见，但社会脆弱性（如错误信息、政策抵制、公众意识不足）往往被忽视，尽管它们对系统韧性和气候适应具有深远影响。

Method: 通过分析人类认知偏差、风险误判和决策孤岛在互连系统中的影响，研究这些因素如何导致资源错配和政策有效性减弱，以及不充分的响应如何在网络-物理-社会层面级联放大气候风险。

Result: 研究发现社会基础设施（包括人力资本、社会规范和政策框架）塑造社区响应并支撑适应能力，但当被忽视时也会成为重要的故障点。

Conclusion: 通过解决这些人为因素并调整决策框架，可以增强系统韧性，培养考虑网络-物理-社会-气候系统复杂相互关系的协调适应策略。

Abstract: Within the cyber-physical-social-climate nexus, all systems are deeply
interdependent: cyber infrastructure facilitates communication, data
processing, and automation across physical systems (such as power grids and
networks), while social infrastructure provides the human capital and societal
norms necessary for the system's functionality. Any disruption within any of
these components, whether due to human error or system mismanagement, can
propagate throughout the network, amplifying vulnerabilities and creating a
significantly scaled impact. This chapter explores the critical role of human
vulnerabilities within the cyber-physical-social-climate nexus, focusing on the
interdependencies across cyber, physical, and social systems and how these
interdependencies can scale in a climate context. While cyber and physical
vulnerabilities are readily apparent, social vulnerabilities (such as
misinformation, resistance to policy change, and lack of public awareness)
often go unaddressed despite their profound impact on resilience and climate
adaptation. Social infrastructure, including human capital, societal norms, and
policy frameworks, shapes community responses and underpins adaptive capacity,
yet it is also a significant point of failure when overlooked. This chapter
examines how human cognitive biases, risk misperception, and decision-making
silos within interconnected systems can lead to resource misallocation and
weakened policy effectiveness. These factors are analyzed to demonstrate how
inadequate responses across cyber-physical-social layers can cascade,
amplifying climate-related risks. By addressing these human factors and
aligning decision-making frameworks, we aim to strengthen resilience and foster
cohesive adaptation strategies that account for the intricate interrelations of
cyber-physical-social-climate systems.

</details>


### [21] [From base cases to backdoors: An Empirical Study of Unnatural Crypto-API Misuse](https://arxiv.org/abs/2510.13102)
*Victor Olaiya,Adwait Nadkarni*

Main category: cs.CR

TL;DR: 该论文首次大规模研究密码API的非自然使用模式，通过分析20,508个Android应用的140,431个API调用，定性分析5,704个代表性调用，揭示了高度不寻常的误用、规避代码以及流行工具的检测局限性。


<details>
  <summary>Details</summary>
Motivation: 现有工具只能检测最基本的密码API误用，无法检测非平凡变体。需要了解开发者如何在实际中误用密码API，特别是非自然使用模式，以指导工具设计。

Method: 开发直观复杂度指标对140,431个密码API调用进行分层，从20,508个Android应用中采样5,704个代表性调用进行定性分析，包括手动逆向工程、开发最小示例和探索原生代码。

Result: 研究得出两个详细的非自然密码API误用分类法，以及17个关键发现，显示存在高度不寻常的误用、规避代码，流行工具甚至无法检测轻度非传统用法。

Conclusion: 研究提供了四个关键启示，为未来检测非自然密码API误用的工作提供指导，强调需要更先进的检测方法来应对复杂的误用模式。

Abstract: Tools focused on cryptographic API misuse often detect the most basic
expressions of the vulnerable use, and are unable to detect non-trivial
variants. The question of whether tools should be designed to detect such
variants can only be answered if we know how developers use and misuse
cryptographic APIs in the wild, and in particular, what the unnatural usage of
such APIs looks like. This paper presents the first large-scale study that
characterizes unnatural crypto-API usage through a qualitative analysis of
5,704 representative API invocations. We develop an intuitive complexity metric
to stratify 140,431 crypto-API invocations obtained from 20,508 Android
applications, allowing us to sample 5,704 invocations that are representative
of all strata, with each stratum consisting of invocations with similar
complexity/naturalness. We qualitatively analyze the 5,704 sampled invocations
using manual reverse engineering, through an in-depth investigation that
involves the development of minimal examples and exploration of native code.
Our study results in two detailed taxonomies of unnatural crypto-API misuse,
along with 17 key findings that show the presence of highly unusual misuse,
evasive code, and the inability of popular tools to reason about even mildly
unconventional usage. Our findings lead to four key takeaways that inform
future work focused on detecting unnatural crypto-API misuse.

</details>


### [22] [ShuffleV: A Microarchitectural Defense Strategy against Electromagnetic Side-Channel Attacks in Microprocessors](https://arxiv.org/abs/2510.13111)
*Nuntipat Narkthong,Yukui Luo,Xiaolin Xu*

Main category: cs.CR

TL;DR: ShuffleV是一种针对电磁侧信道攻击的微架构防御策略，通过随机重排程序指令执行顺序和可选插入伪指令来保护微处理器应用。


<details>
  <summary>Details</summary>
Motivation: 微处理器的运行时电磁辐射会泄露应用程序的机密信息，如加密算法密钥和神经网络模型参数，现有攻击已证明这种侧信道的威胁。

Method: 采用移动目标防御理念，在开源RISC-V核心中集成硬件单元，随机重排指令执行顺序并可选插入伪指令，使攻击者无法通过多次运行进行统计分析。

Result: 在FPGA上实现ShuffleV，对AES加密和神经网络推理进行测试，结果显示能自动保护这些应用，无需用户干预或软件修改。

Conclusion: ShuffleV能有效防御电磁侧信道攻击，提供六种设计选项适应不同应用场景，并通过模拟器实现快速评估。

Abstract: The run-time electromagnetic (EM) emanation of microprocessors presents a
side-channel that leaks the confidentiality of the applications running on
them. Many recent works have demonstrated successful attacks leveraging such
side-channels to extract the confidentiality of diverse applications, such as
the key of cryptographic algorithms and the hyperparameter of neural network
models. This paper proposes ShuffleV, a microarchitecture defense strategy
against EM Side-Channel Attacks (SCAs). ShuffleV adopts the moving target
defense (MTD) philosophy, by integrating hardware units to randomly shuffle the
execution order of program instructions and optionally insert dummy
instructions, to nullify the statistical observation by attackers across
repetitive runs. We build ShuffleV on the open-source RISC-V core and provide
six design options, to suit different application scenarios. To enable rapid
evaluation, we develop a ShuffleV simulator that can help users to (1) simulate
the performance overhead for each design option and (2) generate an execution
trace to validate the randomness of execution on their workload. We implement
ShuffleV on a Xilinx PYNQ-Z2 FPGA and validate its performance with two
representative victim applications against EM SCAs, AES encryption, and neural
network inference. The experimental results demonstrate that ShuffleV can
provide automatic protection for these applications, without any user
intervention or software modification.

</details>


### [23] [Privacy-Aware Framework of Robust Malware Detection in Indoor Robots: Hybrid Quantum Computing and Deep Neural Networks](https://arxiv.org/abs/2510.13136)
*Tan Le,Van Le,Sachin Shetty*

Main category: cs.CR

TL;DR: 提出了一个用于室内机器人系统的隐私感知恶意软件检测框架，结合量子计算和深度神经网络来对抗CPS中的DoS攻击，同时保护隐私信息。


<details>
  <summary>Details</summary>
Motivation: 室内机器人系统在CPS中面临DoS攻击威胁，这些攻击会损害定位、控制和遥测完整性，需要开发既能有效检测恶意软件又能保护隐私的解决方案。

Method: 采用混合量子计算方法，将量子增强特征编码与dropout优化的深度学习相结合，无需手工阈值或持续信标数据，通过模块化电路设计实现。

Result: 在隐私约束条件下达到95.2%的检测准确率，展现出强大的泛化能力、可解释性和对训练不稳定性的韧性。

Conclusion: 这项工作推进了可信AI在安全、自主CPS操作中的应用，为对抗DoS威胁提供了可扩展的解决方案。

Abstract: Indoor robotic systems within Cyber-Physical Systems (CPS) are increasingly
exposed to Denial of Service (DoS) attacks that compromise localization,
control and telemetry integrity. We propose a privacy-aware malware detection
framework for indoor robotic systems, which leverages hybrid quantum computing
and deep neural networks to counter DoS threats in CPS, while preserving
privacy information. By integrating quantum-enhanced feature encoding with
dropout-optimized deep learning, our architecture achieves up to 95.2%
detection accuracy under privacy-constrained conditions. The system operates
without handcrafted thresholds or persistent beacon data, enabling scalable
deployment in adversarial environments. Benchmarking reveals robust
generalization, interpretability and resilience against training instability
through modular circuit design. This work advances trustworthy AI for secure,
autonomous CPS operations.

</details>


### [24] [GRIDAI: Generating and Repairing Intrusion Detection Rules via Collaboration among Multiple LLM-based Agents](https://arxiv.org/abs/2510.13257)
*Jiarui Li,Yuhan Chai,Lei Du,Chenyun Duan,Hao Yan,Zhaoquan Gu*

Main category: cs.CR

TL;DR: GRIDAI是一个基于多LLM代理协作的端到端框架，用于自动生成和修复入侵检测规则，通过识别新攻击与现有规则的关系来减少规则冗余，并包含验证机制来缓解LLM幻觉问题。


<details>
  <summary>Details</summary>
Motivation: 现有基于规则的网络入侵检测系统主要关注为新攻击自动生成检测规则，但忽视了新攻击与现有规则之间的关系，导致规则集不断膨胀且存在显著冗余。

Method: GRIDAI首先评估攻击样本的性质：如果是新攻击类型则生成新规则，如果是现有攻击的变体则修复相应规则以增强泛化能力。框架包含基于工具的实时验证机制和代表性攻击样本维护，以缓解LLM幻觉导致的语法和语义错误。

Result: 在包含7种攻击类型的公共数据集和43种攻击类型的私有数据集上的综合实验表明，GRIDAI能准确识别新攻击样本与现有规则的关系，高效生成和修复规则处理新攻击和变体，并有效缓解LLM幻觉的影响。

Conclusion: GRIDAI通过多LLM代理协作实现了入侵检测规则的自动生成和修复，解决了规则冗余问题，并通过验证机制确保了规则的准确性和可靠性。

Abstract: Rule-based network intrusion detection systems play a crucial role in the
real-time detection of Web attacks. However, most existing works primarily
focus on automatically generating detection rules for new attacks, often
overlooking the relationships between new attacks and existing rules, which
leads to significant redundancy within the ever-expanding ruleset. To address
this issue, we propose GRIDAI, a novel end-to-end framework for the automated
Generation and Repair of Intrusion Detection rules through collaboration among
multiple LLM-based agents. Unlike traditional methods, GRIDAI first assesses
the nature of incoming attack samples. If the sample represents a new attack
type, it is used to generate a new rule. Otherwise, the sample is identified as
a variant of an attack already covered by an existing rule and used to repair
the rule by updating the corresponding signature, thereby enhancing its
generalization capability. Additionally, to mitigate syntactic and semantic
errors in rules caused by LLM hallucinations, we incorporate a tool-based
real-time validation mechanism and a representative attack sample maintained
for each rule, enabling fully automated rule generation and repair.
Comprehensive experiments were conducted on a public dataset containing seven
types of attacks and a private dataset with 43 attack types. The results
demonstrate that GRIDAI accurately identifies the relationships between new
attack samples and existing rules, efficiently generates and repairs rules to
handle new attacks and variants, and effectively mitigates the impact of LLM
hallucinations.

</details>


### [25] [Fast Authenticated and Interoperable Multimedia Healthcare Data over Hybrid-Storage Blockchains](https://arxiv.org/abs/2510.13318)
*Jucai Yang,Liang Li,Yiwei Gu,Haiqin Wu*

Main category: cs.CR

TL;DR: FAITH是一个基于混合存储区块链的快速认证和互操作多媒体医疗数据存储共享方案，通过零知识证明和代理重加密技术，大幅降低用户端验证延迟


<details>
  <summary>Details</summary>
Motivation: 现有区块链医疗系统在多媒体数据完整性验证时存在高延迟问题，特别是在时间敏感的临床场景中实用性不足

Method: 使用递归零知识证明让离线存储提供商生成可验证证明，用户只需轻量级验证；采用代理重加密实现灵活访问授权，并通过ZKPs验证重加密正确性

Result: 用户端验证延迟降低98%，从4秒降至约70毫秒（针对5GB加密文件），适用于时间关键型医疗应用

Conclusion: FAITH方案有效解决了区块链医疗系统中多媒体数据验证的高延迟问题，显著提升了系统的实用性

Abstract: The integration of blockchain technology into healthcare presents a paradigm
shift for secure data management, enabling decentralized and tamper-proof
storage and sharing of sensitive Electronic Health Records (EHRs). However,
existing blockchain-based healthcare systems, while providing robust access
control, commonly overlook the high latency in user-side re-computation of
hashes for integrity verification of large multimedia data, impairing their
practicality, especially in time-sensitive clinical scenarios. In this paper,
we propose FAITH, an innovative scheme for \underline{F}ast
\underline{A}uthenticated and \underline{I}nteroperable mul\underline{T}imedia
\underline{H}ealthcare data storage and sharing over hybrid-storage
blockchains. Rather than user-side hash re-computations, FAITH lets an
off-chain storage provider generate verifiable proofs using recursive
Zero-Knowledge Proofs (ZKPs), while the user only needs to perform lightweight
verification. For flexible access authorization, we leverage Proxy
Re-Encryption (PRE) and enable the provider to conduct ciphertext
re-encryption, in which the re-encryption correctness can be verified via ZKPs
against the malicious provider. All metadata and proofs are recorded on-chain
for public verification. We provide a comprehensive analysis of FAITH's
security regarding data privacy and integrity. We implemented a prototype of
FAITH, and extensive experiments demonstrated its practicality for
time-critical healthcare applications, dramatically reducing user-side
verification latency by up to $98\%$, bringing it from $4$ s down to around
$70$ ms for a $5$ GB encrypted file.

</details>


### [26] [Injection, Attack and Erasure: Revocable Backdoor Attacks via Machine Unlearning](https://arxiv.org/abs/2510.13322)
*Baogang Song,Dongdong Zhao,Jianwen Xiang,Qiben Xu,Zizhuo Yu*

Main category: cs.CR

TL;DR: 提出了首个可撤销后门攻击范式，通过双层优化设计触发器，在保持高攻击成功率的同时确保后门能够通过反学习机制被彻底移除。


<details>
  <summary>Details</summary>
Motivation: 现有后门攻击策略在隐蔽性方面存在不足，会留下可被静态分析检测的持久痕迹。为了解决这一问题，需要开发能够在攻击目标达成后主动彻底移除后门的攻击方法。

Method: 将可撤销后门攻击的触发器优化建模为双层优化问题：通过模拟后门注入和反学习过程，优化触发器生成器以实现高攻击成功率，同时确保后门易于通过反学习擦除。采用确定性划分中毒样本和反学习样本来减少采样方差，并应用投影冲突梯度技术解决梯度冲突。

Result: 在CIFAR-10和ImageNet上的实验表明，该方法保持了与最先进后门攻击相当的攻击成功率，同时能够在反学习后有效移除后门行为。

Conclusion: 这项工作为后门攻击研究开辟了新方向，并为机器学习系统的安全性带来了新的挑战。

Abstract: Backdoor attacks pose a persistent security risk to deep neural networks
(DNNs) due to their stealth and durability. While recent research has explored
leveraging model unlearning mechanisms to enhance backdoor concealment,
existing attack strategies still leave persistent traces that may be detected
through static analysis. In this work, we introduce the first paradigm of
revocable backdoor attacks, where the backdoor can be proactively and
thoroughly removed after the attack objective is achieved. We formulate the
trigger optimization in revocable backdoor attacks as a bilevel optimization
problem: by simulating both backdoor injection and unlearning processes, the
trigger generator is optimized to achieve a high attack success rate (ASR)
while ensuring that the backdoor can be easily erased through unlearning. To
mitigate the optimization conflict between injection and removal objectives, we
employ a deterministic partition of poisoning and unlearning samples to reduce
sampling-induced variance, and further apply the Projected Conflicting Gradient
(PCGrad) technique to resolve the remaining gradient conflicts. Experiments on
CIFAR-10 and ImageNet demonstrate that our method maintains ASR comparable to
state-of-the-art backdoor attacks, while enabling effective removal of backdoor
behavior after unlearning. This work opens a new direction for backdoor attack
research and presents new challenges for the security of machine learning
systems.

</details>


### [27] [Towards Trusted Service Monitoring: Verifiable Service Level Agreements](https://arxiv.org/abs/2510.13370)
*Fernando Castillo,Eduardo Brito,Sebastian Werner,Pille Pullonen-Raudvere,Jonathan Heiss*

Main category: cs.CR

TL;DR: 提出一个基于可信硬件和零知识证明的SLA监控框架，通过密码学方法解决服务提供商自报告指标时的信任冲突问题。


<details>
  <summary>Details</summary>
Motivation: 服务导向环境中SLA监控存在固有信任冲突，提供商自报告指标会激励其少报违规行为，需要建立真正的可信监控机制。

Method: 将机器可读的SLA条款转换为可验证谓词，在可信执行环境中监控；收集带时间戳的遥测数据，组织成Merkle树并生成签名证明；使用零知识证明聚合服务级别指标来评估合规性。

Result: 原型系统展示了线性扩展能力，每小时可处理超过100万个事件，单个违规声明的证明生成和验证时间接近常数。

Conclusion: 该框架通过密码学保证实现了无需信任的SLA执行，为服务监控中的自动合规验证提供了完整解决方案。

Abstract: Service Level Agreement (SLA) monitoring in service-oriented environments
suffers from inherent trust conflicts when providers self-report metrics,
creating incentives to underreport violations. We introduce a framework for
generating verifiable SLA violation claims through trusted hardware monitors
and zero-knowledge proofs, establishing cryptographic foundations for genuine
trustworthiness in service ecosystems. Our approach starts with
machine-readable SLA clauses converted into verifiable predicates and monitored
within Trusted Execution Environments. These monitors collect timestamped
telemetry, organize measurements into Merkle trees, and produce signed
attestations. Zero-knowledge proofs aggregate Service-Level Indicators to
evaluate compliance, generating cryptographic proofs verifiable by
stakeholders, arbitrators, or insurers in disputes, without accessing
underlying data. This ensures three security properties: integrity,
authenticity, and validity. Our prototype demonstrates linear scaling up to
over 1 million events per hour for measurements with near constant-time proof
generation and verification for single violation claims, enabling trustless SLA
enforcement through cryptographic guarantees for automated compliance
verification in service monitoring.

</details>


### [28] [Toward Efficient Inference Attacks: Shadow Model Sharing via Mixture-of-Experts](https://arxiv.org/abs/2510.13451)
*Li Bai,Qingqing Ye,Xinwei Zhang,Sen Zhang,Zi Liang,Jianliang Xu,Haibo Hu*

Main category: cs.CR

TL;DR: SHAPOOL是一个新颖的影子池训练框架，通过共享子网络和联合训练多个影子模型来显著降低计算成本，同时保持成员推理攻击的有效性。


<details>
  <summary>Details</summary>
Motivation: 传统的影子模型技术在推理攻击中需要大量独立训练的影子模型，导致计算成本高昂，限制了实际应用。这种低效性主要源于影子模型的独立训练和使用。

Method: SHAPOOL利用混合专家机制作为影子池，将单个模型互连，使它们共享一些子网络。引入三个新模块：路径选择路由、路径正则化和路径对齐，确保共享模型与独立模型相似并作为有效替代品。

Result: 在各种成员推理攻击场景下评估SHAPOOL，结果显示它显著降低了影子模型构建的计算成本，同时保持了可比的攻击性能。

Conclusion: SHAPOOL框架通过共享子网络和联合训练，有效解决了影子模型技术的高计算成本问题，为推理攻击提供了更实用的解决方案。

Abstract: Machine learning models are often vulnerable to inference attacks that expose
sensitive information from their training data. Shadow model technique is
commonly employed in such attacks, such as membership inference. However, the
need for a large number of shadow models leads to high computational costs,
limiting their practical applicability. Such inefficiency mainly stems from the
independent training and use of these shadow models. To address this issue, we
present a novel shadow pool training framework SHAPOOL, which constructs
multiple shared models and trains them jointly within a single process. In
particular, we leverage the Mixture-of-Experts mechanism as the shadow pool to
interconnect individual models, enabling them to share some sub-networks and
thereby improving efficiency. To ensure the shared models closely resemble
independent models and serve as effective substitutes, we introduce three novel
modules: path-choice routing, pathway regularization, and pathway alignment.
These modules guarantee random data allocation for pathway learning, promote
diversity among shared models, and maintain consistency with target models. We
evaluate SHAPOOL in the context of various membership inference attacks and
show that it significantly reduces the computational cost of shadow model
construction while maintaining comparable attack performance.

</details>


### [29] [Who Speaks for the Trigger? Dynamic Expert Routing in Backdoored Mixture-of-Experts Transformers](https://arxiv.org/abs/2510.13462)
*Xin Zhao,Xiaojun Chen,Bingshan Liu,Haoyu Gao,Zhendong Zhao,Yilong Chen*

Main category: cs.CR

TL;DR: BadSwitch是一种针对MoE架构LLM的新型后门攻击框架，通过利用专家路由偏好，在预训练期间优化触发嵌入并识别敏感专家，实现高效且隐蔽的模型操控。


<details>
  <summary>Details</summary>
Motivation: MoE架构的稀疏路由机制由于专家专业化而表现出任务偏好，这为后门攻击引入了新的未充分探索的漏洞。

Method: 结合任务耦合动态触发优化和敏感度引导的Top-S专家追踪机制，在预训练期间联合优化触发嵌入，识别最敏感的S个专家，并将Top-K门控机制限制在这些目标专家上。

Result: 在三种主流MoE架构上的评估显示，BadSwitch能以100%成功率劫持预训练模型，同时保持最高的清洁准确率，对文本级和模型级防御机制具有强韧性。

Conclusion: 该工作揭示了MoE系统的安全风险，通过分析专家激活模式提供了对MoE漏洞的基本见解，有助于推进AI安全。

Abstract: Large language models (LLMs) with Mixture-of-Experts (MoE) architectures
achieve impressive performance and efficiency by dynamically routing inputs to
specialized subnetworks, known as experts. However, this sparse routing
mechanism inherently exhibits task preferences due to expert specialization,
introducing a new and underexplored vulnerability to backdoor attacks. In this
work, we investigate the feasibility and effectiveness of injecting backdoors
into MoE-based LLMs by exploiting their inherent expert routing preferences. We
thus propose BadSwitch, a novel backdoor framework that integrates task-coupled
dynamic trigger optimization with a sensitivity-guided Top-S expert tracing
mechanism. Our approach jointly optimizes trigger embeddings during pretraining
while identifying S most sensitive experts, subsequently constraining the Top-K
gating mechanism to these targeted experts. Unlike traditional backdoor attacks
that rely on superficial data poisoning or model editing, BadSwitch primarily
embeds malicious triggers into expert routing paths with strong task affinity,
enabling precise and stealthy model manipulation. Through comprehensive
evaluations across three prominent MoE architectures (Switch Transformer,
QwenMoE, and DeepSeekMoE), we demonstrate that BadSwitch can efficiently hijack
pre-trained models with up to 100% success rate (ASR) while maintaining the
highest clean accuracy (ACC) among all baselines. Furthermore, BadSwitch
exhibits strong resilience against both text-level and model-level defense
mechanisms, achieving 94.07% ASR and 87.18% ACC on the AGNews dataset. Our
analysis of expert activation patterns reveals fundamental insights into MoE
vulnerabilities. We anticipate this work will expose security risks in MoE
systems and contribute to advancing AI safety.

</details>


### [30] [How Blind and Low-Vision Users Manage Their Passwords](https://arxiv.org/abs/2510.13538)
*Alexander Ponticello,Filipo Sharevski,Simon Anell,Katharina Krombholz*

Main category: cs.CR

TL;DR: 研究调查了盲人和低视力用户如何使用密码管理器，发现虽然所有参与者都使用密码管理器，但主要是为了存储和检索密码的便利性，而非安全优势。密码管理器未能满足BLV用户对自主权的需求，导致他们采用不安全做法。


<details>
  <summary>Details</summary>
Motivation: 现有研究已考察用户密码管理策略和痛点，但缺乏对盲人和低视力用户如何应对密码管理问题的专门研究，以及密码管理器如何协助他们。

Method: 采用定性访谈研究，与33名盲人和低视力参与者进行访谈。

Result: 所有参与者都在某种程度上使用密码管理器，认为其相对易访问。但采用主要受存储和检索密码的便利性驱动，而非安全优势。密码管理器未满足BLV用户对自主权的需求，导致不安全做法如重复使用可预测密码或在盲文中书写重要凭证。

Conclusion: 需要实施实用的可访问性和可用性改进，以建立信任和安全实践，同时保持BLV用户的自主权。

Abstract: Managing passwords securely and conveniently is still an open problem for
many users. Existing research has examined users' password management
strategies and identified pain points, such as security concerns, leading to
insecure practices. We investigate how Blind and Low-Vision (BLV) users tackle
this problem and how password managers can assist them. This paper presents the
results of a qualitative interview study with N = 33 BLV participants. We found
that all participants utilize password managers to some extent, which they
perceive as fairly accessible. However, the adoption is mainly driven by the
convenience of storing and retrieving passwords. The security advantages -
generating strong, random passwords - were avoided mainly due to the absence of
practical accessibility. Password managers do not adhere to BLV users'
underlying needs for agency, which stem from experiences with inaccessible
software and vendors who deprioritize accessibility issues. Underutilization of
password managers leads BLV users to adopt insecure practices, such as reusing
predictable passwords or resorting to 'security through obscurity' by writing
important credentials in braille. We conclude our analysis by discussing the
need to implement practical accessibility and usability improvements for
password managers as a way of establishing trust and secure practices while
maintaining BLV users' agency.

</details>


### [31] [In-Browser LLM-Guided Fuzzing for Real-Time Prompt Injection Testing in Agentic AI Browsers](https://arxiv.org/abs/2510.13543)
*Avihay Cohen*

Main category: cs.CR

TL;DR: 提出了一种在浏览器中运行的LLM引导模糊测试框架，用于实时发现间接提示注入漏洞


<details>
  <summary>Details</summary>
Motivation: 基于LLM的浏览器代理容易受到网页中隐藏恶意指令的间接提示注入攻击，这些攻击可以绕过传统网络安全边界

Method: 开发了一个完全在浏览器中运行的模糊测试框架，使用LLM引导来自动发现提示注入漏洞

Result: 框架能够在实时环境中有效检测间接提示注入漏洞

Conclusion: 该框架为检测和防御LLM浏览器代理的提示注入攻击提供了有效解决方案

Abstract: Large Language Model (LLM) based agents integrated into web browsers (often
called agentic AI browsers) offer powerful automation of web tasks. However,
they are vulnerable to indirect prompt injection attacks, where malicious
instructions hidden in a webpage deceive the agent into unwanted actions. These
attacks can bypass traditional web security boundaries, as the AI agent
operates with the user privileges across sites. In this paper, we present a
novel fuzzing framework that runs entirely in the browser and is guided by an
LLM to automatically discover such prompt injection vulnerabilities in real
time.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [32] [From Literal to Liberal: A Meta-Prompting Framework for Eliciting Human-Aligned Exception Handling in Large Language Models](https://arxiv.org/abs/2510.12864)
*Imran Khan*

Main category: cs.AI

TL;DR: 提出了RID框架，一种低计算成本的元提示技术，用于在零样本情况下引导LLMs进行人类对齐的异常处理，显著提升了决策的人类对齐度。


<details>
  <summary>Details</summary>
Motivation: LLMs作为智能体系统的推理引擎存在规则刚性缺陷，即过度遵循显式规则而忽视人类常识和意图，这阻碍了可信自主智能体的构建。

Method: 引入规则-意图区分(RID)框架，通过结构化认知模式让模型解构任务、分类规则、权衡冲突结果并证明最终决策，是一种零样本元提示技术。

Result: 在20个需要细微判断的场景基准测试中，RID框架达到95%的人类对齐分数，显著优于基线(80%)和思维链提示(75%)，并能产生更高质量的意图驱动推理。

Conclusion: RID框架提供了一种实用、易用且有效的方法，将LLMs从字面指令遵循转向灵活的目标导向推理，为构建更可靠和实用的AI智能体铺平了道路。

Abstract: Large Language Models (LLMs) are increasingly being deployed as the reasoning
engines for agentic AI systems, yet they exhibit a critical flaw: a rigid
adherence to explicit rules that leads to decisions misaligned with human
common sense and intent. This "rule-rigidity" is a significant barrier to
building trustworthy autonomous agents. While prior work has shown that
supervised fine-tuning (SFT) with human explanations can mitigate this issue,
SFT is computationally expensive and inaccessible to many practitioners. To
address this gap, we introduce the Rule-Intent Distinction (RID) Framework, a
novel, low-compute meta-prompting technique designed to elicit human-aligned
exception handling in LLMs in a zero-shot manner. The RID framework provides
the model with a structured cognitive schema for deconstructing tasks,
classifying rules, weighing conflicting outcomes, and justifying its final
decision. We evaluated the RID framework against baseline and Chain-of-Thought
(CoT) prompting on a custom benchmark of 20 scenarios requiring nuanced
judgment across diverse domains. Our human-verified results demonstrate that
the RID framework significantly improves performance, achieving a 95% Human
Alignment Score (HAS), compared to 80% for the baseline and 75% for CoT.
Furthermore, it consistently produces higher-quality, intent-driven reasoning.
This work presents a practical, accessible, and effective method for steering
LLMs from literal instruction-following to liberal, goal-oriented reasoning,
paving the way for more reliable and pragmatic AI agents.

</details>


### [33] [DeepPlanner: Scaling Planning Capability for Deep Research Agents via Advantage Shaping](https://arxiv.org/abs/2510.12979)
*Wei Fan,Wenlin Yao,Zheng Li,Feng Yao,Xin Liu,Liang Qiu,Qingyu Yin,Yangqiu Song,Bing Yin*

Main category: cs.AI

TL;DR: DeepPlanner是一个端到端的强化学习框架，通过基于熵的优势函数和选择性样本加权，有效增强深度研究代理的规划能力，在多个基准测试中取得最先进结果。


<details>
  <summary>Details</summary>
Motivation: 现有方法要么依赖推理阶段的隐式规划，要么引入显式规划器但未系统优化规划阶段。研究发现规划令牌的熵显著高于其他动作令牌，表明存在未优化的不确定决策点。

Method: 提出DeepPlanner框架，通过基于熵的令牌级优势函数为高熵令牌分配更大更新，并为规划密集型轨迹选择性加权样本级优势。

Result: 在七个深度研究基准测试中，DeepPlanner提高了规划质量，并在显著降低训练预算的情况下实现了最先进的结果。

Conclusion: DeepPlanner通过系统优化规划阶段，有效解决了大语言模型在复杂任务中长期规划能力不足的问题，为增强AI代理的规划能力提供了有效解决方案。

Abstract: Large language models (LLMs) augmented with multi-step reasoning and action
generation abilities have shown promise in leveraging external tools to tackle
complex tasks that require long-horizon planning. However, existing approaches
either rely on implicit planning in the reasoning stage or introduce explicit
planners without systematically addressing how to optimize the planning stage.
As evidence, we observe that under vanilla reinforcement learning (RL),
planning tokens exhibit significantly higher entropy than other action tokens,
revealing uncertain decision points that remain under-optimized. To address
this, we propose DeepPlanner, an end-to-end RL framework that effectively
enhances the planning capabilities of deep research agents. Our approach shapes
token-level advantage with an entropy-based term to allocate larger updates to
high entropy tokens, and selectively upweights sample-level advantages for
planning-intensive rollouts. Extensive experiments across seven deep research
benchmarks demonstrate that DeepPlanner improves planning quality and achieves
state-of-the-art results under a substantially lower training budget.

</details>


### [34] [SENTINEL: A Multi-Level Formal Framework for Safety Evaluation of LLM-based Embodied Agents](https://arxiv.org/abs/2510.12985)
*Simon Sinong Zhan,Yao Liu,Philip Wang,Zinan Wang,Qineng Wang,Zhian Ruan,Xiangyu Shi,Xinyu Cao,Frank Yang,Kangrui Wang,Huajie Shao,Manling Li,Qi Zhu*

Main category: cs.AI

TL;DR: Sentinel是首个用于形式化评估基于大语言模型的具身代理在语义、规划和轨迹三个层面物理安全性的框架，通过时态逻辑语义来精确指定安全需求，并采用多级验证管道进行系统化安全评估。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖启发式规则或主观的LLM判断，缺乏对物理安全性的形式化评估。需要一种能够精确指定状态不变量、时间依赖性和时序约束的严谨方法来评估LLM具身代理的安全性。

Method: 采用三级验证管道：1) 语义层面将自然语言安全需求形式化为时态逻辑公式，并验证LLM代理对这些需求的理解；2) 规划层面验证LLM生成的高级行动计划和子目标是否符合时态逻辑公式；3) 轨迹层面将多个执行轨迹合并为计算树，针对物理细节化的时态逻辑规范进行最终安全检查。

Result: 在VirtualHome和ALFRED环境中评估多个基于LLM的具身代理，实验表明Sentinel能够发现先前方法忽略的安全违规，并提供对失败模式的深入洞察。

Conclusion: 通过将物理安全基于时态逻辑并在多个层面应用验证方法，Sentinel为在物理环境中系统评估基于LLM的具身代理提供了严谨基础，暴露了先前方法未发现的安全问题。

Abstract: We present Sentinel, the first framework for formally evaluating the physical
safety of Large Language Model(LLM-based) embodied agents across the semantic,
plan, and trajectory levels. Unlike prior methods that rely on heuristic rules
or subjective LLM judgments, Sentinel grounds practical safety requirements in
formal temporal logic (TL) semantics that can precisely specify state
invariants, temporal dependencies, and timing constraints. It then employs a
multi-level verification pipeline where (i) at the semantic level, intuitive
natural language safety requirements are formalized into TL formulas and the
LLM agent's understanding of these requirements is probed for alignment with
the TL formulas; (ii) at the plan level, high-level action plans and subgoals
generated by the LLM agent are verified against the TL formulas to detect
unsafe plans before execution; and (iii) at the trajectory level, multiple
execution trajectories are merged into a computation tree and efficiently
verified against physically-detailed TL specifications for a final safety
check. We apply Sentinel in VirtualHome and ALFRED, and formally evaluate
multiple LLM-based embodied agents against diverse safety requirements. Our
experiments show that by grounding physical safety in temporal logic and
applying verification methods across multiple levels, Sentinel provides a
rigorous foundation for systematically evaluating LLM-based embodied agents in
physical environments, exposing safety violations overlooked by previous
methods and offering insights into their failure modes.

</details>


### [35] [From Narratives to Probabilistic Reasoning: Predicting and Interpreting Drivers' Hazardous Actions in Crashes Using Large Language Model](https://arxiv.org/abs/2510.13002)
*Boyou Chen,Gerui Xu,Zifei Wang,Huizhong Guo,Ananna Ahmed,Zhaonan Sun,Zhen Hu,Kaihan Zhang,Shan Bao*

Main category: cs.AI

TL;DR: 提出基于微调大语言模型的框架，自动从事故文本叙述中推断驾驶员危险行为，提高分类有效性和可解释性，在二车事故数据上达到80%准确率。


<details>
  <summary>Details</summary>
Motivation: 二车事故占道路事故70%，但现有驾驶员危险行为数据因人工编码不一致且劳动密集而可靠性有限，需要自动化解决方案。

Method: 使用MTCF五年二车事故数据，微调Llama 3.2 1B模型处理详细事故叙述，并与随机森林、XGBoost、CatBoost和神经网络等传统机器学习分类器进行基准比较。

Result: 微调LLM总体准确率达80%，优于所有基线模型，在数据不平衡场景中表现尤其突出。概率推理分析显示，分心驾驶显著增加"一般不安全驾驶"概率，双方分心最大化"双方危险行为"概率，青少年驾驶显著增加"速度和停车违规"概率。

Conclusion: 该框架和分析方法为大规模自动化DHA检测提供了稳健且可解释的解决方案，为交通安全分析和干预开辟了新机会。

Abstract: Vehicle crashes involve complex interactions between road users, split-second
decisions, and challenging environmental conditions. Among these, two-vehicle
crashes are the most prevalent, accounting for approximately 70% of roadway
crashes and posing a significant challenge to traffic safety. Identifying
Driver Hazardous Action (DHA) is essential for understanding crash causation,
yet the reliability of DHA data in large-scale databases is limited by
inconsistent and labor-intensive manual coding practices. Here, we present an
innovative framework that leverages a fine-tuned large language model to
automatically infer DHAs from textual crash narratives, thereby improving the
validity and interpretability of DHA classifications. Using five years of
two-vehicle crash data from MTCF, we fine-tuned the Llama 3.2 1B model on
detailed crash narratives and benchmarked its performance against conventional
machine learning classifiers, including Random Forest, XGBoost, CatBoost, and a
neural network. The fine-tuned LLM achieved an overall accuracy of 80%,
surpassing all baseline models and demonstrating pronounced improvements in
scenarios with imbalanced data. To increase interpretability, we developed a
probabilistic reasoning approach, analyzing model output shifts across original
test sets and three targeted counterfactual scenarios: variations in driver
distraction and age. Our analysis revealed that introducing distraction for one
driver substantially increased the likelihood of "General Unsafe Driving";
distraction for both drivers maximized the probability of "Both Drivers Took
Hazardous Actions"; and assigning a teen driver markedly elevated the
probability of "Speed and Stopping Violations." Our framework and analytical
methods provide a robust and interpretable solution for large-scale automated
DHA detection, offering new opportunities for traffic safety analysis and
intervention.

</details>


### [36] [Toward Reasoning-Centric Time-Series Analysis](https://arxiv.org/abs/2510.13029)
*Xinlei Wang,Mingtian Tan,Jing Qiu,Junhua Zhao,Jinjin Gu*

Main category: cs.AI

TL;DR: 本文主张将时间序列分析重新构想为基于LLM的推理任务，强调因果结构和可解释性，而非传统的数值回归方法。


<details>
  <summary>Details</summary>
Motivation: 现实世界的时间序列分析需要超越表面趋势，揭示驱动因素。现有LLM方法主要利用数值回归能力，忽视了其深层推理潜力。

Method: 将时间序列分析重新定义为推理任务，利用LLM的多模态输入整合能力，重点关注因果结构和解释性分析。

Result: 该方法使时间序列分析更接近人类对齐的理解，能够在复杂现实环境中提供透明和上下文感知的洞察。

Conclusion: LLM在时间序列分析中的有效应用应侧重于推理能力而非数值回归，这有助于实现更透明、可解释的分析框架。

Abstract: Traditional time series analysis has long relied on pattern recognition,
trained on static and well-established benchmarks. However, in real-world
settings -- where policies shift, human behavior adapts, and unexpected events
unfold -- effective analysis must go beyond surface-level trends to uncover the
actual forces driving them. The recent rise of Large Language Models (LLMs)
presents new opportunities for rethinking time series analysis by integrating
multimodal inputs. However, as the use of LLMs becomes popular, we must remain
cautious, asking why we use LLMs and how to exploit them effectively. Most
existing LLM-based methods still employ their numerical regression ability and
ignore their deeper reasoning potential. This paper argues for rethinking time
series with LLMs as a reasoning task that prioritizes causal structure and
explainability. This shift brings time series analysis closer to human-aligned
understanding, enabling transparent and context-aware insights in complex
real-world environments.

</details>


### [37] [Repairing Reward Functions with Human Feedback to Mitigate Reward Hacking](https://arxiv.org/abs/2510.13036)
*Stephane Hatgis-Kessell,Logan Mondal Bhamidipaty,Emma Brunskill*

Main category: cs.AI

TL;DR: 提出Preference-Based Reward Repair (PBRR)框架，通过从人类偏好中学习加性修正项来自动修复人工设计的代理奖励函数，解决奖励函数错配问题。


<details>
  <summary>Details</summary>
Motivation: 人工设计的奖励函数经常与人类真实目标不一致，导致奖励黑客问题；而从零学习奖励函数需要大量人类偏好数据，成本高昂。

Method: PBRR框架使用针对性探索策略和新偏好学习目标，学习一个与状态转移相关的加性修正项来修复代理奖励函数。

Result: 在表格化领域证明PBRR的累积遗憾与现有偏好强化学习方法相当；在奖励黑客基准测试中，PBRR始终优于基线方法，需要更少偏好数据即可学习高性能策略。

Conclusion: PBRR能够有效修复错配的代理奖励函数，比从零学习奖励函数或其他修正方法更高效，显著减少了人类偏好数据需求。

Abstract: Human-designed reward functions for reinforcement learning (RL) agents are
frequently misaligned with the humans' true, unobservable objectives, and thus
act only as proxies. Optimizing for a misspecified proxy reward function often
induces reward hacking, resulting in a policy misaligned with the human's true
objectives. An alternative is to perform RL from human feedback, which involves
learning a reward function from scratch by collecting human preferences over
pairs of trajectories. However, building such datasets is costly. To address
the limitations of both approaches, we propose Preference-Based Reward Repair
(PBRR): an automated iterative framework that repairs a human-specified proxy
reward function by learning an additive, transition-dependent correction term
from preferences. A manually specified reward function can yield policies that
are highly suboptimal under the ground-truth objective, yet corrections on only
a few transitions may suffice to recover optimal performance. To identify and
correct for those transitions, PBRR uses a targeted exploration strategy and a
new preference-learning objective. We prove in tabular domains PBRR has a
cumulative regret that matches, up to constants, that of prior preference-based
RL methods. In addition, on a suite of reward-hacking benchmarks, PBRR
consistently outperforms baselines that learn a reward function from scratch
from preferences or modify the proxy reward function using other approaches,
requiring substantially fewer preferences to learn high performing policies.

</details>


### [38] [Emotional Cognitive Modeling Framework with Desire-Driven Objective Optimization for LLM-empowered Agent in Social Simulation](https://arxiv.org/abs/2510.13195)
*Qun Ma,Xiao Xue,Xuwen Zhang,Zihan Zhao,Yuwei Guo,Ming Zhang*

Main category: cs.AI

TL;DR: 提出了一个情感认知框架，通过欲望生成和目标管理实现LLM智能体与人类的情感对齐，在决策过程中模拟状态演化、欲望生成、目标优化、决策生成和行动执行。


<details>
  <summary>Details</summary>
Motivation: 现有LLM智能体在情感认知方面存在严重局限：无法模拟连接虚拟与现实服务的有限理性；缺乏经过实证验证的情感与决策架构整合机制。

Method: 构建情感认知框架，包含欲望生成和目标管理，在专有多智能体交互环境中实现完整的决策过程建模。

Result: 实验表明，采用该框架的智能体不仅表现出与情感状态一致的行为，在与其他类型智能体的比较中展现出更高的生态效度，决策结果更接近人类行为模式。

Conclusion: 该情感认知框架成功实现了LLM智能体与人类的情感对齐，显著提升了智能体在社会模拟中的行为真实性和决策质量。

Abstract: The advent of large language models (LLMs) has enabled agents to represent
virtual humans in societal simulations, facilitating diverse interactions
within complex social systems. However, existing LLM-based agents exhibit
severe limitations in affective cognition: They fail to simulate the bounded
rationality essential for bridging virtual and real-world services; They lack
empirically validated integration mechanisms embedding emotions within agent
decision architectures. This paper constructs an emotional cognition framework
incorporating desire generation and objective management, designed to achieve
emotion alignment between LLM-based agents and humans, modeling the complete
decision-making process of LLM-based agents, encompassing state evolution,
desire generation, objective optimization, decision generation, and action
execution. This study implements the proposed framework within our proprietary
multi-agent interaction environment. Experimental results demonstrate that
agents governed by our framework not only exhibit behaviors congruent with
their emotional states but also, in comparative assessments against other agent
types, demonstrate superior ecological validity and generate decision outcomes
that significantly more closely approximate human behavioral patterns.

</details>


### [39] [Adaptive Reasoning Executor: A Collaborative Agent System for Efficient Reasoning](https://arxiv.org/abs/2510.13214)
*Zehui Ling,Deshu Chen,Yichi Zhang,Yuchen Liu,Xigui Li,Xin Guo,Yuan Cheng*

Main category: cs.AI

TL;DR: 提出了一种结合小型和大型语言模型的互补代理系统，通过小型LLM首先生成初步答案，大型LLM进行验证，仅在必要时进行深度推理，显著降低计算成本。


<details>
  <summary>Details</summary>
Motivation: 虽然链式思维提示和深度推理能提升复杂任务性能，但对所有问题都应用深度推理计算成本过高，需要更高效的解决方案。

Method: 采用互补代理系统：小型LLM生成初始答案，大型LLM验证正确性，仅在答案错误时进行深度推理。

Result: 实验显示，对于简单问题，大型LLM的计算成本降低超过50%，准确率损失可忽略，同时在复杂任务上保持稳健性能。

Conclusion: 该互补代理系统能有效平衡计算效率和任务性能，为实际应用提供了可行的解决方案。

Abstract: Recent advances in Large Language Models (LLMs) demonstrate that
chain-of-thought prompting and deep reasoning substantially enhance performance
on complex tasks, and multi-agent systems can further improve accuracy by
enabling model debates. However, applying deep reasoning to all problems is
computationally expensive. To mitigate these costs, we propose a complementary
agent system integrating small and large LLMs. The small LLM first generates an
initial answer, which is then verified by the large LLM. If correct, the answer
is adopted directly; otherwise, the large LLM performs in-depth reasoning.
Experimental results show that, for simple problems, our approach reduces the
computational cost of the large LLM by more than 50% with negligible accuracy
loss, while consistently maintaining robust performance on complex tasks.

</details>


### [40] [Personalized Learning Path Planning with Goal-Driven Learner State Modeling](https://arxiv.org/abs/2510.13215)
*Joy Jia Yin Lim,Ye He,Jifan Yu,Xin Cong,Daniel Zhang-Li,Zhiyuan Liu,Huiqin Liu,Lei Hou,Juanzi Li,Bin Xu*

Main category: cs.AI

TL;DR: Pxplore是一个个性化学习路径规划框架，结合强化学习和LLM教育架构，通过结构化学习者状态模型和自动奖励函数，实现目标对齐的学习路径生成。


<details>
  <summary>Details</summary>
Motivation: 现有LLM方法缺乏目标对齐规划机制，无法有效设计符合个人目标的适应性学习路径。

Method: 提出Pxplore框架，集成强化学习训练范式，设计结构化学习者状态模型和自动奖励函数，结合监督微调和GRPO策略优化。

Result: 实验验证Pxplore能生成连贯、个性化且目标驱动的学习路径，并在真实学习平台中部署。

Conclusion: Pxplore框架在个性化学习路径规划方面表现出色，为未来研究提供了代码和数据集。

Abstract: Personalized Learning Path Planning (PLPP) aims to design adaptive learning
paths that align with individual goals. While large language models (LLMs) show
potential in personalizing learning experiences, existing approaches often lack
mechanisms for goal-aligned planning. We introduce Pxplore, a novel framework
for PLPP that integrates a reinforcement-based training paradigm and an
LLM-driven educational architecture. We design a structured learner state model
and an automated reward function that transforms abstract objectives into
computable signals. We train the policy combining supervised fine-tuning (SFT)
and Group Relative Policy Optimization (GRPO), and deploy it within a
real-world learning platform. Extensive experiments validate Pxplore's
effectiveness in producing coherent, personalized, and goal-driven learning
paths. We release our code and dataset to facilitate future research.

</details>


### [41] [EvoTest: Evolutionary Test-Time Learning for Self-Improving Agentic Systems](https://arxiv.org/abs/2510.13220)
*Yufei He,Juncheng Liu,Yue Liu,Yibo Li,Tri Cao,Zhiyuan Hu,Xinxing Xu,Bryan Hooi*

Main category: cs.AI

TL;DR: 提出了Jericho测试时学习基准(J-TTL)来衡量AI代理在测试时学习复杂技能的能力，并开发了EvoTest进化框架，通过演化代理系统配置来提升性能，无需微调或梯度。


<details>
  <summary>Details</summary>
Motivation: 当前AI代理无法在测试时学习复杂技能，在陌生环境中表现不佳，这严重限制了其实际应用价值。

Method: 提出EvoTest进化测试时学习框架，包含执行游戏的Actor代理和分析游戏记录以改进配置的Evolver代理，通过重写提示、更新记忆、调整超参数和学习工具使用例程来优化系统。

Result: 在J-TTL基准测试中，EvoTest持续提升性能，优于反思、仅记忆和在线微调等方法，是唯一能赢得两个游戏的方法。

Conclusion: EvoTest框架有效解决了AI代理在测试时学习复杂技能的挑战，为开发更实用的AI代理提供了新方向。

Abstract: A fundamental limitation of current AI agents is their inability to learn
complex skills on the fly at test time, often behaving like "clever but
clueless interns" in novel environments. This severely limits their practical
utility. To systematically measure and drive progress on this challenge, we
first introduce the Jericho Test-Time Learning (J-TTL) benchmark. J-TTL is a
new evaluation setup where an agent must play the same game for several
consecutive episodes, attempting to improve its performance from one episode to
the next. On J-TTL, we find that existing adaptation methods like reflection,
memory, or reinforcement learning struggle. To address the challenges posed by
our benchmark, we present EvoTest, an evolutionary test-time learning framework
that improves an agent without any fine-tuning or gradients-by evolving the
entire agentic system after every episode. EvoTest has two roles: the Actor
Agent, which plays the game, and the Evolver Agent, which analyzes the episode
transcript to propose a revised configuration for the next run. This
configuration rewrites the prompt, updates memory by logging effective
state-action choices, tunes hyperparameters, and learns the tool-use routines.
On our J-TTL benchmark, EvoTest consistently increases performance,
outperforming not only reflection and memory-only baselines but also more
complex online fine-tuning methods. Notably, our method is the only one capable
of winning two games (Detective and Library), while all baselines fail to win
any.

</details>


### [42] [An Analytical Framework to Enhance Autonomous Vehicle Perception for Smart Cities](https://arxiv.org/abs/2510.13230)
*Jalal Khan,Manzoor Khan,Sherzod Turaev,Sumbal Malik,Hesham El-Sayed,Farman Ullah*

Main category: cs.AI

TL;DR: 提出了一种基于效用的分析模型，用于自动驾驶车辆感知系统理解驾驶环境，包括自定义数据集获取、YOLOv8s目标检测模型和感知服务效用测量模块。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶环境感知对智能交通至关重要，需要开发能够准确感知道路多目标并预测驾驶员感知的模型，以控制车辆运动。

Method: 使用自定义数据集（包含摩托车手、三轮车等独特对象），基于YOLOv8s的目标检测模型，以及从训练模型性能值测量感知服务效用的模块。

Result: 实验显示三个最佳性能的YOLOv8s实例：SGD-based（mAP@0.5=0.832）、Adam-based（0.810）和AdamW-based（0.822）。AdamW模型在类别级别性能上优于SGD模型。

Conclusion: 验证了所提出的感知模型能够为自动驾驶车辆找到正确的感知，鼓励使用该模型评估学习模型的效用并确定合适的自动驾驶感知。

Abstract: The driving environment perception has a vital role for autonomous driving
and nowadays has been actively explored for its realization. The research
community and relevant stakeholders necessitate the development of Deep
Learning (DL) models and AI-enabled solutions to enhance autonomous vehicles
(AVs) for smart mobility. There is a need to develop a model that accurately
perceives multiple objects on the road and predicts the driver's perception to
control the car's movements. This article proposes a novel utility-based
analytical model that enables perception systems of AVs to understand the
driving environment. The article consists of modules: acquiring a custom
dataset having distinctive objects, i.e., motorcyclists, rickshaws, etc; a
DL-based model (YOLOv8s) for object detection; and a module to measure the
utility of perception service from the performance values of trained model
instances. The perception model is validated based on the object detection
task, and its process is benchmarked by state-of-the-art deep learning models'
performance metrics from the nuScense dataset. The experimental results show
three best-performing YOLOv8s instances based on mAP@0.5 values, i.e.,
SGD-based (0.832), Adam-based (0.810), and AdamW-based (0.822). However, the
AdamW-based model (i.e., car: 0.921, motorcyclist: 0.899, truck: 0.793, etc.)
still outperforms the SGD-based model (i.e., car: 0.915, motorcyclist: 0.892,
truck: 0.781, etc.) because it has better class-level performance values,
confirmed by the proposed perception model. We validate that the proposed
function is capable of finding the right perception for AVs. The results above
encourage using the proposed perception model to evaluate the utility of
learning models and determine the appropriate perception for AVs.

</details>


### [43] [SAJA: A State-Action Joint Attack Framework on Multi-Agent Deep Reinforcement Learning](https://arxiv.org/abs/2510.13262)
*Weiqi Guo,Guanjun Liu,Ziyuan Zhou*

Main category: cs.AI

TL;DR: 提出了SAJA框架，通过联合状态和动作攻击来增强多智能体深度强化学习的对抗攻击效果，相比单独攻击更有效且隐蔽


<details>
  <summary>Details</summary>
Motivation: 现有研究只关注状态或动作的单独攻击，没有充分利用两者的协同效应，简单组合效果不佳

Method: SAJA框架包含两个阶段：状态攻击阶段使用多步梯度上升结合actor和critic网络计算对抗状态；动作攻击阶段基于扰动状态使用critic网络生成最终对抗动作，并添加距离正则化器增强效果

Result: 在MPE环境中验证，SAJA比单独状态或动作攻击效果更好且更隐蔽，现有防御方法无法抵御其攻击

Conclusion: SAJA框架成功实现了状态和动作攻击的协同效应，为多智能体深度强化学习的对抗鲁棒性研究提供了新视角

Abstract: Multi-Agent Deep Reinforcement Learning (MADRL) has shown potential for
cooperative and competitive tasks such as autonomous driving and strategic
gaming. However, models trained by MADRL are vulnerable to adversarial
perturbations on states and actions. Therefore, it is essential to investigate
the robustness of MADRL models from an attack perspective. Existing studies
focus on either state-only attacks or action-only attacks, but do not consider
how to effectively joint them. Simply combining state and action perturbations
such as randomly perturbing states and actions does not exploit their potential
synergistic effects. In this paper, we propose the State-Action Joint Attack
(SAJA) framework that has a good synergistic effects. SAJA consists of two
important phases: (1) In the state attack phase, a multi-step gradient ascent
method utilizes both the actor network and the critic network to compute an
adversarial state, and (2) in the action attack phase, based on the perturbed
state, a second gradient ascent uses the critic network to craft the final
adversarial action. Additionally, a heuristic regularizer measuring the
distance between the perturbed actions and the original clean ones is added
into the loss function to enhance the effectiveness of the critic's guidance.
We evaluate SAJA in the Multi-Agent Particle Environment (MPE), demonstrating
that (1) it outperforms and is more stealthy than state-only or action-only
attacks, and (2) existing state or action defense methods cannot defend its
attacks.

</details>


### [44] [Learnable Game-theoretic Policy Optimization for Data-centric Self-explanation Rationalization](https://arxiv.org/abs/2510.13393)
*Yunxiao Zhao,Zhiqiang Wang,Xingtong Yu,Xiaoli Li,Jiye Liang,Ru Li*

Main category: cs.AI

TL;DR: 本文提出PORAT方法，从博弈论视角解决合理化方法中的模式崩溃问题，通过策略干预引导模型达到更优均衡状态。


<details>
  <summary>Details</summary>
Motivation: 传统合理化方法存在模式崩溃问题，即预测器能正确预测但生成器持续输出崩溃模式的理由。现有研究缺乏统一考虑，本文从博弈论角度系统分析该问题的根本原因。

Method: 提出PORAT方法，在合作博弈过程中逐步引入策略干预，解决博弈均衡问题，引导模型向更优解状态发展。

Result: 在9个真实世界数据集和2个合成设置上验证，PORAT相比现有最先进方法性能提升最高达8.1%。

Conclusion: 从博弈论视角重新审视合作合理化，识别了模式崩溃的根本原因，提出的PORAT方法能有效解决该问题并提升性能。

Abstract: Rationalization, a data-centric framework, aims to build self-explanatory
models to explain the prediction outcome by generating a subset of
human-intelligible pieces of the input data. It involves a cooperative game
model where a generator generates the most human-intelligible parts of the
input (i.e., rationales), followed by a predictor that makes predictions based
on these generated rationales. Conventional rationalization methods typically
impose constraints via regularization terms to calibrate or penalize undesired
generation. However, these methods are suffering from a problem called mode
collapse, in which the predictor produces correct predictions yet the generator
consistently outputs rationales with collapsed patterns. Moreover, existing
studies are typically designed separately for specific collapsed patterns,
lacking a unified consideration. In this paper, we systematically revisit
cooperative rationalization from a novel game-theoretic perspective and
identify the fundamental cause of this problem: the generator no longer tends
to explore new strategies to uncover informative rationales, ultimately leading
the system to converge to a suboptimal game equilibrium (correct predictions
v.s collapsed rationales). To solve this problem, we then propose a novel
approach, Game-theoretic Policy Optimization oriented RATionalization (PORAT),
which progressively introduces policy interventions to address the game
equilibrium in the cooperative game process, thereby guiding the model toward a
more optimal solution state. We theoretically analyse the cause of such a
suboptimal equilibrium and prove the feasibility of the proposed method.
Furthermore, we validate our method on nine widely used real-world datasets and
two synthetic settings, where PORAT achieves up to 8.1% performance
improvements over existing state-of-the-art methods.

</details>


### [45] [Assessing LLM Reasoning Through Implicit Causal Chain Discovery in Climate Discourse](https://arxiv.org/abs/2510.13417)
*Liesbeth Allein,Nataly Pineda-Castañeda,Andrea Rocci,Marie-Francine Moens*

Main category: cs.AI

TL;DR: 该研究评估了大型语言模型在隐式因果链发现任务中的机制性因果推理能力，发现LLMs在生成因果步骤时存在差异，主要依赖关联模式匹配而非真正的因果推理。


<details>
  <summary>Details</summary>
Motivation: 探究大型语言模型如何理解因果关系，以及它们能否识别连接因果对的中间因果步骤，特别是在论证场景中的机制性因果推理能力。

Method: 在诊断评估框架中，指导九个LLMs为给定的因果对生成所有可能的中间因果步骤，这些因果对来自气候变化辩论的论证研究资源。

Result: LLMs在生成的因果步骤数量和粒度上存在差异，虽然它们对中间因果连接具有自我一致性和信心，但判断主要基于关联模式匹配。人类评估确认了生成链的逻辑连贯性和完整性。

Conclusion: 该研究为推进论证场景中隐式机制性因果推理的未来工作奠定了坚实基础，包括基线因果链发现方法、诊断评估见解和带因果链的基准数据集。

Abstract: How does a cause lead to an effect, and which intermediate causal steps
explain their connection? This work scrutinizes the mechanistic causal
reasoning capabilities of large language models (LLMs) to answer these
questions through the task of implicit causal chain discovery. In a diagnostic
evaluation framework, we instruct nine LLMs to generate all possible
intermediate causal steps linking given cause-effect pairs in causal chain
structures. These pairs are drawn from recent resources in argumentation
studies featuring polarized discussion on climate change. Our analysis reveals
that LLMs vary in the number and granularity of causal steps they produce.
Although they are generally self-consistent and confident about the
intermediate causal connections in the generated chains, their judgments are
mainly driven by associative pattern matching rather than genuine causal
reasoning. Nonetheless, human evaluations confirmed the logical coherence and
integrity of the generated chains. Our baseline causal chain discovery
approach, insights from our diagnostic evaluation, and benchmark dataset with
causal chains lay a solid foundation for advancing future work in implicit,
mechanistic causal reasoning in argumentation settings.

</details>


### [46] [Mobile Coverage Analysis using Crowdsourced Data](https://arxiv.org/abs/2510.13459)
*Timothy Wong,Tom Freeman,Joseph Feehily*

Main category: cs.AI

TL;DR: 提出了一种利用众包QoE数据进行移动网络覆盖和弱信号点分析的新框架，使用OC-SVM算法计算覆盖范围并识别服务弱区


<details>
  <summary>Details</summary>
Motivation: 准确评估移动网络覆盖和识别服务弱区对于提升用户体验质量至关重要

Method: 基于众包QoE数据，在单个天线级别进行覆盖分析后聚合到站点级别，使用OC-SVM算法建模有效覆盖边界

Result: 该框架能够准确绘制移动网络覆盖图，并在复杂城市环境中突出显示信号不足的细粒度区域

Conclusion: 该研究证明了所提框架在移动覆盖映射和弱信号点识别方面的有效性

Abstract: Effective assessment of mobile network coverage and the precise
identification of service weak spots are paramount for network operators
striving to enhance user Quality of Experience (QoE). This paper presents a
novel framework for mobile coverage and weak spot analysis utilising
crowdsourced QoE data. The core of our methodology involves coverage analysis
at the individual cell (antenna) level, subsequently aggregated to the site
level, using empirical geolocation data. A key contribution of this research is
the application of One-Class Support Vector Machine (OC-SVM) algorithm for
calculating mobile network coverage. This approach models the decision
hyperplane as the effective coverage contour, facilitating robust calculation
of coverage areas for individual cells and entire sites. The same methodology
is extended to analyse crowdsourced service loss reports, thereby identifying
and quantifying geographically localised weak spots. Our findings demonstrate
the efficacy of this novel framework in accurately mapping mobile coverage and,
crucially, in highlighting granular areas of signal deficiency, particularly
within complex urban environments.

</details>


### [47] [Confidence as a Reward: Transforming LLMs into Reward Models](https://arxiv.org/abs/2510.13501)
*He Du,Bowen Li,Chengxing Xie,Chang Gao,Kai Chen,Dacheng Tao*

Main category: cs.AI

TL;DR: 本文提出了一种无需训练的奖励方法CRew，利用模型对最终答案的置信度作为奖励信号，在数学推理任务中表现优于现有方法，并基于此开发了CRew-DPO训练策略。


<details>
  <summary>Details</summary>
Motivation: 现有的奖励模型需要大量标注数据和昂贵训练，而训练免费方法如LLM-as-a-Judge虽然有效，但使用置信度作为奖励的概念尚未得到系统研究。

Method: 提出CRew方法，利用模型在最终答案上的token级置信度作为奖励代理；进一步开发CRew-DPO训练策略，结合置信度和正确性信号构建偏好数据。

Result: 在MATH500和RewardMATH基准测试中，CRew优于现有训练免费奖励方法，甚至超过大多数训练过的奖励模型；CRew-DPO持续优于现有自训练方法。

Conclusion: 置信度作为奖励是一种简单而强大的训练免费方法，能有效提升模型推理能力，且可用于构建高质量训练数据。

Abstract: Reward models can significantly enhance the reasoning capabilities of large
language models (LLMs), but they typically require extensive curated data and
costly training. To mitigate these challenges, training-free approaches such as
LLM-as-a-Judge leverage the intrinsic reasoning abilities of LLMs to evaluate
responses, achieving promising results. Recent works have also indicated that
model confidence can serve effectively as a reward metric, distinguishing
between chain-of-thought (CoT) and non-CoT paths. However, the concept of using
confidence as a reward has not been comprehensively studied. In this work, we
systematically investigate Confidence-as-a-Reward (CRew), a simple yet powerful
training-free method that utilizes token-level confidence in the model's final
answers as a proxy for reward, especially suitable for close-ended tasks.
Through extensive experiments on mathematical reasoning tasks, we demonstrate
that CRew outperforms existing training-free reward approaches on the MATH500
and RewardMATH benchmarks, and even surpasses most trained reward models. We
further identify a strong correlation between CRew scores and the actual
reasoning performance of the model. Additionally, we find that CRew can
effectively filter high-quality training data. Building upon these insights, we
propose CRew-DPO, a training strategy that constructs preference data from
confidence scores combined with correctness signals. Finetuning with CRew-DPO
further enhances the model's judging capabilities and consistently outperforms
existing self-training methods.

</details>


### [48] [A Methodology for Assessing the Risk of Metric Failure in LLMs Within the Financial Domain](https://arxiv.org/abs/2510.13524)
*William Flanagan,Mukunda Das,Rajitha Ramanyake,Swaunja Maslekar,Meghana Manipuri,Joong Ho Choi,Shruti Nair,Shambhavi Bhusan,Sanjana Dulam,Mouni Pendharkar,Nidhi Singh,Vashisth Doshi,Sachi Shah Paresh*

Main category: cs.AI

TL;DR: 本文提出了一个风险评估框架，用于解决生成式AI在金融服务行业应用中的性能评估挑战，结合领域专家评估和机器学习指标。


<details>
  <summary>Details</summary>
Motivation: 生成式AI在金融服务行业的应用面临性能评估障碍，传统机器学习指标无法很好适应GenAI工作负载，而广泛使用的基准测试在工业应用中缺乏通用性。

Method: 开发了一个风险评估框架，结合领域专家评估和机器学习指标，以更好地衡量生成式AI模型的性能。

Result: 该框架能够解决选择特定指标时的独特风险，并改善领域专家评估和机器学习指标的应用效果。

Conclusion: 提出的风险评估框架有助于金融服务行业更好地采用生成式AI技术，解决性能评估的关键挑战。

Abstract: As Generative Artificial Intelligence is adopted across the financial
services industry, a significant barrier to adoption and usage is measuring
model performance. Historical machine learning metrics can oftentimes fail to
generalize to GenAI workloads and are often supplemented using Subject Matter
Expert (SME) Evaluation. Even in this combination, many projects fail to
account for various unique risks present in choosing specific metrics.
Additionally, many widespread benchmarks created by foundational research labs
and educational institutions fail to generalize to industrial use. This paper
explains these challenges and provides a Risk Assessment Framework to allow for
better application of SME and machine learning Metrics

</details>


### [49] [Tandem Training for Language Models](https://arxiv.org/abs/2510.13551)
*Robert West,Ashton Anderson,Ece Kamar,Eric Horvitz*

Main category: cs.AI

TL;DR: 提出串联训练方法，通过随机切换强弱模型来确保强模型的解决方案对弱模型可理解，同时保持任务准确性。


<details>
  <summary>Details</summary>
Motivation: 随着语言模型快速发展，其推理过程可能超出人类和弱智能体的理解范围，影响可解释性和监督。需要确保强模型的解决方案对弱合作伙伴保持可理解性。

Method: 引入串联训练：在强化学习中，随机从冻结的弱模型而非强模型中采样token，只有当强模型的行为和推理过程能被弱模型继续时，才能成功完成rollout。

Result: 在GSM8K数学推理任务中，串联训练可靠地教会模型放弃专业术语，适应弱合作伙伴的语言，同时保持高任务准确率。

Conclusion: 串联训练为构建对弱智能体可审计的AI系统提供了一条有前景的路径，对人与AI协作和多智能体通信具有重要意义。

Abstract: As language models continue to rapidly improve, we can expect their actions
and reasoning to become difficult or impossible for weaker agents and humans to
follow, undermining interpretability and oversight. With an eye on long-term
futures, we pursue methods that encourage models to produce solutions that
remain intelligible to weaker collaborators. We formalize intelligibility as
handoff robustness: a strong model's solution is intelligible to a weaker model
if randomly handing off control to the weaker model along the solution path
does not cause failure. Building on this criterion, we introduce tandem
training for language models, a reinforcement learning (RL) paradigm in which
rollout tokens are intermittently and randomly sampled from a frozen weak model
rather than the strong model being trained. Because rollouts succeed only when
the strong model's actions and reasoning process can be continued by the weak
model -- when the two can co-construct a successful solution -- optimizing
standard RL objectives with tandem training implicitly incentivizes both
correctness and intelligibility. In the GSM8K math reasoning task, tandem
training reliably teaches models to abandon jargon and adapt their language to
weaker partners while keeping task accuracy high. Our results demonstrate a
promising route to building AI systems that remain auditable by weaker agents,
with implications for human--AI collaboration and multi-agent communication.

</details>


### [50] [A Modal Logic for Temporal and Jurisdictional Classifier Models](https://arxiv.org/abs/2510.13691)
*Cecilia Di Florio,Huimin Dong,Antonino Rotolo*

Main category: cs.AI

TL;DR: 提出了一种用于形式化法律案例推理的分类器模态逻辑，结合时间维度和法院层级来解决先例冲突


<details>
  <summary>Details</summary>
Motivation: 基于逻辑的模型可用于构建机器学习分类器的验证工具，这些分类器在法律领域基于先前案例预测新案例结果，执行案例推理

Method: 引入分类器模态逻辑，将案例的时间维度和法院系统的层级结构纳入逻辑中，以解决先例之间的冲突

Result: 开发了一个能够形式化捕捉法律案例推理的逻辑框架

Conclusion: 该逻辑框架为验证法律领域机器学习分类器提供了理论基础，能够处理先例冲突问题

Abstract: Logic-based models can be used to build verification tools for machine
learning classifiers employed in the legal field. ML classifiers predict the
outcomes of new cases based on previous ones, thereby performing a form of
case-based reasoning (CBR). In this paper, we introduce a modal logic of
classifiers designed to formally capture legal CBR. We incorporate principles
for resolving conflicts between precedents, by introducing into the logic the
temporal dimension of cases and the hierarchy of courts within the legal
system.

</details>


### [51] [Training LLM Agents to Empower Humans](https://arxiv.org/abs/2510.13709)
*Evan Ellis,Vivek Myers,Jens Tuyls,Sergey Levine,Anca Dragan,Benjamin Eysenbach*

Main category: cs.AI

TL;DR: 提出了一种基于最大化人类赋能（empowerment）的辅助语言模型调优方法Empower，该方法仅需离线文本数据，无需额外人工反馈或可验证奖励，就能训练出更好的辅助AI助手。


<details>
  <summary>Details</summary>
Motivation: 当前构建辅助代理的方法（模仿专家或基于推断奖励的RL微调）往往鼓励代理独立完成任务而非真正辅助人类实现目标，且需要昂贵的人工反馈。

Method: Empower方法通过最大化人类在环境中实现期望变化的能力（赋能）来微调语言模型，仅使用离线文本数据进行自监督训练。

Result: 用户研究中78%的参与者偏好Empower助手（p=0.015），接受率提高31%，建议减少38%。在代码辅助环境中，Empower训练代理使模拟程序员在挑战性编程问题上的成功率平均提高192%。

Conclusion: Empower提供了一个仅使用离线数据、无需额外人工反馈或可验证奖励的大规模有用对齐AI代理框架。

Abstract: Assistive agents should not only take actions on behalf of a human, but also
step out of the way and cede control when there are important decisions to be
made. However, current methods for building assistive agents, whether via
mimicking expert humans or via RL finetuning on an inferred reward, often
encourage agents to complete tasks on their own rather than truly assisting the
human attain their objectives. Additionally, these methods often require costly
explicit human feedback to provide a training signal. We propose a new approach
to tuning assistive language models based on maximizing the human's
empowerment, their ability to effect desired changes in the environment. Our
empowerment-maximizing method, Empower, only requires offline text data,
providing a self-supervised method for fine-tuning language models to better
assist humans. To study the efficacy of our approach, we conducted an 18-person
user study comparing our empowerment assistant with a strong baseline.
Participants preferred our assistant 78% of the time (p=0.015), with a 31%
higher acceptance rate and 38% fewer suggestions. Additionally, we introduce a
new environment for evaluating multi-turn code assistance using simulated
humans. Using this environment, we show that agents trained with Empower
increase the success rate of a simulated human programmer on challenging coding
questions by an average of 192% over an SFT baseline. With this empowerment
objective, we provide a framework for useful aligned AI agents at scale using
only offline data without the need for any additional human feedback or
verifiable rewards.

</details>


### [52] [From Refusal to Recovery: A Control-Theoretic Approach to Generative AI Guardrails](https://arxiv.org/abs/2510.13727)
*Ravi Pandya,Madison Bland,Duy P. Nguyen,Changliu Liu,Jaime Fernández Fisac,Andrea Bajcsy*

Main category: cs.AI

TL;DR: 本文提出了一种基于控制理论的安全护栏方法，用于预防AI代理的下游危害，通过实时监控和主动修正风险输出来替代传统的分类阻断机制。


<details>
  <summary>Details</summary>
Motivation: 传统AI安全护栏依赖输出分类和人工标准，对新危险情况脆弱，且检测到不安全时只能拒绝行动，这本身可能不安全。需要一种能主动预防下游危害的动态安全方法。

Method: 将AI代理安全建模为序列决策问题，基于安全关键控制理论在AI模型的潜在表示空间中构建预测性护栏，通过安全关键强化学习进行规模化训练。

Result: 在模拟驾驶和电子商务场景中，控制理论护栏能可靠地引导LLM代理避免灾难性后果（从碰撞到破产），同时保持任务性能。

Conclusion: 控制理论护栏为当前标记阻断式护栏提供了原则性的动态替代方案，能够主动修正风险输出为安全输出，且模型无关。

Abstract: Generative AI systems are increasingly assisting and acting on behalf of end
users in practical settings, from digital shopping assistants to
next-generation autonomous cars. In this context, safety is no longer about
blocking harmful content, but about preempting downstream hazards like
financial or physical harm. Yet, most AI guardrails continue to rely on output
classification based on labeled datasets and human-specified criteria,making
them brittle to new hazardous situations. Even when unsafe conditions are
flagged, this detection offers no path to recovery: typically, the AI system
simply refuses to act--which is not always a safe choice. In this work, we
argue that agentic AI safety is fundamentally a sequential decision problem:
harmful outcomes arise from the AI system's continually evolving interactions
and their downstream consequences on the world. We formalize this through the
lens of safety-critical control theory, but within the AI model's latent
representation of the world. This enables us to build predictive guardrails
that (i) monitor an AI system's outputs (actions) in real time and (ii)
proactively correct risky outputs to safe ones, all in a model-agnostic manner
so the same guardrail can be wrapped around any AI model. We also offer a
practical training recipe for computing such guardrails at scale via
safety-critical reinforcement learning. Our experiments in simulated driving
and e-commerce settings demonstrate that control-theoretic guardrails can
reliably steer LLM agents clear of catastrophic outcomes (from collisions to
bankruptcy) while preserving task performance, offering a principled dynamic
alternative to today's flag-and-block guardrails.

</details>


### [53] [Hard2Verify: A Step-Level Verification Benchmark for Open-Ended Frontier Math](https://arxiv.org/abs/2510.13744)
*Shrey Pandit,Austin Xu,Xuan-Phi Nguyen,Yifei Ming,Caiming Xiong,Shafiq Joty*

Main category: cs.AI

TL;DR: 提出了Hard2Verify基准，用于评估数学证明中的步骤级验证器性能，发现开源验证器普遍落后于闭源模型。


<details>
  <summary>Details</summary>
Motivation: 在LLM推理系统需要精确步骤验证的背景下，需要强大的验证器来捕捉步骤级错误，但目前缺乏合适的评估基准。

Method: 构建了人工标注的步骤级验证基准Hard2Verify，包含500多小时人工劳动，评估了29个生成式批评器和过程奖励模型。

Result: 除了少数优秀模型外，开源验证器在步骤级验证任务上明显落后于闭源模型。

Conclusion: 步骤级验证是LLM推理系统的关键瓶颈，需要进一步研究验证器计算规模、自验证和验证-生成动态等基础问题。

Abstract: Large language model (LLM)-based reasoning systems have recently achieved
gold medal-level performance in the IMO 2025 competition, writing mathematical
proofs where, to receive full credit, each step must be not only correct but
also sufficiently supported. To train LLM-based reasoners in such challenging,
open-ended settings, strong verifiers capable of catching step-level mistakes
are necessary prerequisites. We introduce Hard2Verify, a human-annotated,
step-level verification benchmark produced with over 500 hours of human labor.
Hard2Verify is designed to rigorously assess step-level verifiers at the
frontier: Verifiers must provide step-level annotations or identify the first
error in responses generated by frontier LLMs for very recent, challenging, and
open-ended math questions. We evaluate 29 generative critics and process reward
models, demonstrating that, beyond a few standouts, open-source verifiers lag
closed source models. We subsequently analyze what drives poor performance in
step-level verification, the impacts of scaling verifier compute, as well as
fundamental questions such as self-verification and verification-generation
dynamics.

</details>
