<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 16]
- [cs.CR](#cs.CR) [Total: 16]
- [cs.AI](#cs.AI) [Total: 48]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [AI-Driven Development of a Publishing Imprint: Xynapse Traces](https://arxiv.org/abs/2510.23627)
*Fred Zimmerman*

Main category: cs.SE

TL;DR: Xynapse Traces是一个实验性出版品牌，通过人机融合方法实现了出版流程的显著优化，将上市时间从6-12个月缩短至2-4周，成本降低80%，第一年出版52本书并保持高质量标准。


<details>
  <summary>Details</summary>
Motivation: 探索人机协作的新出版范式，通过技术手段解决传统出版流程耗时长、成本高的问题，使原本不可行的细分市场变得可行。

Method: 采用配置驱动架构和多模型AI集成框架，包括持续创意管道、锦标赛式评估、新颖的代码设计、全面自动化流程以及定义出版使命的出版者角色。

Result: 实现了90%的上市时间缩减和80%的成本降低，第一年出版52本书，质量指标优异：99%的引用准确率和100%的验证成功率。

Conclusion: 该系统展示了人机协作在出版领域的巨大潜力，为未来图书出版提供了新范式，能够民主化高级出版能力并激活细分市场。

Abstract: Xynapse Traces is an experimental publishing imprint created via a fusion of
human and algorithmic methods using a configuration-driven architecture and a
multi-model AI integration framework. The system achieved a remarkable 90%
reduction in time-to-market (from a typical 6-12 months to just 2-4 weeks),
with 80% cost reduction compared to traditional imprint development, while
publishing 52 books in its first year and maintaining exceptional quality
metrics, including 99% citation accuracy and 100% validation success after
initial corrections. Key technical innovations include a continuous ideation
pipeline with tournament-style evaluation, a novel codex design for
transcriptive meditation practice, comprehensive automation spanning from
ideation through production and distribution, and publisher personas that
define and guide the imprint's mission. The system also integrates automated
verification with human oversight, ensuring that gains in speed do not
compromise publishing standards. This effort has significant implications for
the future of book publishing, suggesting new paradigms for human-AI
collaboration that democratize access to sophisticated publishing capabilities
and make previously unviable niche markets accessible.

</details>


### [2] [VisCoder2: Building Multi-Language Visualization Coding Agents](https://arxiv.org/abs/2510.23642)
*Yuansheng Ni,Songcheng Cai,Xiangchao Chen,Jiarong Liang,Zhiheng Lyu,Jiaqi Deng,Kai Zou,Ping Nie,Fei Yuan,Xiang Yue,Wenhu Chen*

Main category: cs.SE

TL;DR: 提出了VisCode-Multi-679K数据集、VisPlotBench基准和VisCoder2模型系列，用于改进可视化编码代理，支持多语言和多轮调试，显著提升了执行成功率。


<details>
  <summary>Details</summary>
Motivation: 现有LLM在可视化代码生成中存在语言覆盖有限、执行不可靠、缺乏迭代修正机制等问题，且受限于窄数据集和基准测试。

Method: 构建大规模验证数据集VisCode-Multi-679K（679K样本，12种语言），开发评估基准VisPlotBench，训练多语言可视化模型VisCoder2。

Result: VisCoder2显著优于开源基线，接近GPT-4.1性能，通过迭代自调试在32B规模达到82.4%总体执行通过率，尤其在符号或编译器依赖语言中表现突出。

Conclusion: 提出的资源和模型有效解决了可视化编码代理的现有限制，为多语言、多轮修正的可视化代码生成提供了坚实基础。

Abstract: Large language models (LLMs) have recently enabled coding agents capable of
generating, executing, and revising visualization code. However, existing
models often fail in practical workflows due to limited language coverage,
unreliable execution, and lack of iterative correction mechanisms. Progress has
been constrained by narrow datasets and benchmarks that emphasize single-round
generation and single-language tasks. To address these challenges, we introduce
three complementary resources for advancing visualization coding agents.
VisCode-Multi-679K is a large-scale, supervised dataset containing 679K
validated and executable visualization samples with multi-turn correction
dialogues across 12 programming languages. VisPlotBench is a benchmark for
systematic evaluation, featuring executable tasks, rendered outputs, and
protocols for both initial generation and multi-round self-debug. Finally, we
present VisCoder2, a family of multi-language visualization models trained on
VisCode-Multi-679K. Experiments show that VisCoder2 significantly outperforms
strong open-source baselines and approaches the performance of proprietary
models like GPT-4.1, with further gains from iterative self-debug, reaching
82.4% overall execution pass rate at the 32B scale, particularly in symbolic or
compiler-dependent languages.

</details>


### [3] [Agentsway -- Software Development Methodology for AI Agents-based Teams](https://arxiv.org/abs/2510.23664)
*Eranga Bandara,Ross Gore,Xueping Liang,Sachini Rajapakse,Isurunima Kularathne,Pramoda Karunarathna,Peter Foytik,Sachin Shetty,Ravi Mukkamala,Abdul Rahman,Amin Hass,Ng Wee Keong,Kasun De Zoysa,Aruna Withanage,Nilaan Loganathan*

Main category: cs.SE

TL;DR: 提出了"Agentsway"框架，这是首个专门为AI代理协作的软件开发方法论，通过定义不同角色的AI代理和集成微调LLM来增强软件开发的推理能力和可解释决策。


<details>
  <summary>Details</summary>
Motivation: 传统软件开发方法如Agile、Kanban等是为人类团队设计的，在自主AI代理参与规划、编码、测试和持续学习的环境中越来越不适用，需要新的方法论来填补这一空白。

Method: Agentsway框架围绕人类编排和隐私保护协作构建，定义了规划、提示、编码、测试和微调等不同角色的专业AI代理，通过集成微调LLM和高级推理模型实现迭代改进和自适应学习。

Result: 该框架通过协调使用多个微调LLM和高级推理模型，在软件开发全生命周期中增强了领域特定推理和可解释决策，并嵌入了负责任的AI原则。

Conclusion: Agentsway代表了向下一代AI原生、自我改进软件开发方法论迈出的基础性一步，首次为基于AI代理的软件工程团队引入了专用方法论。

Abstract: The emergence of Agentic AI is fundamentally transforming how software is
designed, developed, and maintained. Traditional software development
methodologies such as Agile, Kanban, ShapeUp, etc, were originally designed for
human-centric teams and are increasingly inadequate in environments where
autonomous AI agents contribute to planning, coding, testing, and continuous
learning. To address this methodological gap, we present "Agentsway" a novel
software development framework designed for ecosystems where AI agents operate
as first-class collaborators. Agentsway introduces a structured lifecycle
centered on human orchestration, and privacy-preserving collaboration among
specialized AI agents. The framework defines distinct roles for planning,
prompting, coding, testing, and fine-tuning agents, each contributing to
iterative improvement and adaptive learning throughout the development process.
By integrating fine-tuned LLMs that leverage outputs and feedback from
different agents throughout the development cycle as part of a retrospective
learning process, Agentsway enhances domain-specific reasoning, and explainable
decision-making across the entire software development lifecycle. Responsible
AI principles are further embedded across the agents through the coordinated
use of multiple fine-tuned LLMs and advanced reasoning models, ensuring
balanced, transparent, and accountable decision-making. This work advances
software engineering by formalizing agent-centric collaboration, integrating
privacy-by-design principles, and defining measurable metrics for productivity
and trust. Agentsway represents a foundational step toward the next generation
of AI-native, self-improving software development methodologies. To the best of
our knowledge, this is the first research effort to introduce a dedicated
methodology explicitly designed for AI agent-based software engineering teams.

</details>


### [4] [RefleXGen:The unexamined code is not worth using](https://arxiv.org/abs/2510.23674)
*Bin Wang,Hui Li,AoFan Liu,BoTao Yang,Ao Yang,YiLu Zhong,Weixiang Huang,Yanping Zhang,Runhuai Huang,Weimin Zeng*

Main category: cs.SE

TL;DR: RefleXGen通过结合检索增强生成和自反思机制，显著提升代码生成的安全性，无需大量资源即可实现持续改进。


<details>
  <summary>Details</summary>
Motivation: 代码生成中的安全问题是应用大语言模型时的关键挑战，传统方法需要大量资源进行微调或构建专用数据集。

Method: 结合检索增强生成技术和LLMs的引导自反思机制，通过自我评估和反思迭代优化代码生成过程，持续积累和精炼知识库。

Result: 在多个模型中显著提升代码安全性：GPT-3.5 Turbo提升13.6%，GPT-4o提升6.7%，CodeQwen提升4.5%，Gemini提升5.8%。

Conclusion: 提高模型自反思质量是增强AI生成代码安全性的有效实用策略。

Abstract: Security in code generation remains a pivotal challenge when applying large
language models (LLMs). This paper introduces RefleXGen, an innovative method
that significantly enhances code security by integrating Retrieval-Augmented
Generation (RAG) techniques with guided self-reflection mechanisms inherent in
LLMs. Unlike traditional approaches that rely on fine-tuning LLMs or developing
specialized secure code datasets - processes that can be resource-intensive -
RefleXGen iteratively optimizes the code generation process through
self-assessment and reflection without the need for extensive resources. Within
this framework, the model continuously accumulates and refines its knowledge
base, thereby progressively improving the security of the generated code.
Experimental results demonstrate that RefleXGen substantially enhances code
security across multiple models, achieving a 13.6% improvement with GPT-3.5
Turbo, a 6.7% improvement with GPT-4o, a 4.5% improvement with CodeQwen, and a
5.8% improvement with Gemini. Our findings highlight that improving the quality
of model self-reflection constitutes an effective and practical strategy for
strengthening the security of AI-generated code.

</details>


### [5] [TDFlow: Agentic Workflows for Test Driven Software Engineering](https://arxiv.org/abs/2510.23761)
*Kevin Han,Siddharth Maddikayala,Tim Knappe,Om Patel,Austen Liao,Amir Barati Farimani*

Main category: cs.SE

TL;DR: TDFlow是一个基于测试驱动的智能工作流，将仓库级软件工程构建为测试解决任务，通过分解为四个专门子代理来提升程序修复性能，在SWE-Bench测试中达到88.8%通过率。


<details>
  <summary>Details</summary>
Motivation: 解决仓库级软件工程修复问题，通过测试驱动方法减少长上下文负担，专注于特定子任务，提升LLM在软件工程中的性能表现。

Method: 将软件工程程序修复分解为四个组件：补丁提议、调试、补丁修订和可选测试生成，每个组件由专门的子代理负责，使用精确设计的子代理和严格约束的工具。

Result: 在SWE-Bench Lite上达到88.8%通过率（比次优系统提升27.8%），在SWE-Bench Verified上达到94.3%通过率，800次运行中仅发现7次测试黑客行为。

Conclusion: 现代LLM在精心设计的测试驱动工作流中已能达到人类水平的测试解决能力，完全自主仓库修复的最后障碍在于准确生成有效的复现测试。

Abstract: We introduce TDFlow, a novel test-driven agentic workflow that frames
repository-scale software engineering as a test-resolution task, specifically
designed to solve human-written tests. Given a set of tests, TDFlow repeatedly
proposes, revises, and debugs repository-scale patches using precisely
engineered sub-agents and tightly constrained tools. The workflow decomposes
software engineering program repair into four components governed by respective
sub-agents. This simple, forced decoupling of patch proposing, debugging, patch
revision, and optional test generation (1) reduces long-context burden on any
individual sub-agent, (2) focuses each sub-agent on specific, pre-defined
sub-tasks, and (3) allows for specialized performance improvement on specific
sub-tasks. When provided human-written tests, TDFlow attains 88.8% pass rate on
SWE-Bench Lite (an absolute improvement of 27.8% over the next best system) and
94.3% on SWE-Bench Verified. Manual inspection of the 800 TDFlow runs within
SWE-Bench Lite and Verified uncover only 7 instances of test hacking, which
were subsequently counted as failures. Furthermore, we show that the primary
obstacle to human-level software engineering performance lies within writing
successful reproduction tests. We envision a human-LLM interactive system
powered by TDFlow where human developers write tests solved by LLM systems.
Together, these results indicate that modern LLMs, when embedded in a narrowly
engineered, test-driven workflow, already achieve human-level test resolution
-- with the final frontier for fully autonomous repository repair being the
accurate generation of valid reproduction tests.

</details>


### [6] [Evaluating the effectiveness of LLM-based interoperability](https://arxiv.org/abs/2510.23893)
*Rodrigo Falcão,Stefan Schweitzer,Julien Siebert,Emily Calvet,Frank Elberzhager*

Main category: cs.SE

TL;DR: 评估13个开源LLM在农业互操作性用例中的有效性，发现qwen2.5-coder:32b在大多数情况下表现最佳，能够实现系统间的自主互操作。


<details>
  <summary>Details</summary>
Motivation: 随着系统变得越来越动态和异构，互操作性成为长期挑战。利用大型语言模型（LLMs）的最新进展，研究LLM能否在运行时无需人工干预实现系统自主互操作。

Method: 选择13个开源LLM，在农业互操作性用例中策划四个版本的数据集。使用两种策略（DIRECT和CODEGEN）对每个模型和数据集版本进行三次运行，比较模型有效性和结果一致性。

Result: qwen2.5-coder:32b在三个数据集版本中使用DIRECT策略（平均pass@1≥0.99）和CODEGEN策略（平均pass@1≥0.89）表现最佳。在包含单位转换的第四个版本中，所有使用DIRECT策略的模型都失败，但使用CODEGEN策略的qwen2.5-coder:32b仍能达到平均pass@1=0.75。

Conclusion: 某些LLM能够实现系统的自主互操作。建议在不同领域进行进一步评估，并对可靠性策略进行更多研究。

Abstract: Background: Systems of systems are becoming increasingly dynamic and
heterogeneous, and this adds pressure on the long-standing challenge of
interoperability. Besides its technical aspect, interoperability has also an
economic side, as development time efforts are required to build the
interoperability artifacts. Objectives: With the recent advances in the field
of large language models (LLMs), we aim at analyzing the effectiveness of
LLM-based strategies to make systems interoperate autonomously, at runtime,
without human intervention. Method: We selected 13 open source LLMs and curated
four versions of a dataset in the agricultural interoperability use case. We
performed three runs of each model with each version of the dataset, using two
different strategies. Then we compared the effectiveness of the models and the
consistency of their results across multiple runs. Results: qwen2.5-coder:32b
was the most effective model using both strategies DIRECT (average pass@1 >=
0.99) and CODEGEN (average pass@1 >= 0.89) in three out of four dataset
versions. In the fourth dataset version, which included an unit conversion, all
models using the strategy DIRECT failed, whereas using CODEGEN
qwen2.5-coder:32b succeeded with an average pass@1 = 0.75. Conclusion: Some
LLMs can make systems interoperate autonomously. Further evaluation in
different domains is recommended, and further research on reliability
strategies should be conducted.

</details>


### [7] [Validating Alerts in Cloud-Native Observability](https://arxiv.org/abs/2510.23970)
*Maria C. Borges,Julian Legler,Lucca Di Benedetto*

Main category: cs.SE

TL;DR: 本文介绍了OXN观测实验工具的警报扩展，让工程师能在开发早期实验和测试警报规则，避免运行时问题。


<details>
  <summary>Details</summary>
Motivation: 现代可靠性工程中警报设计面临挑战：需要在及早发现问题与减少误报间取得平衡，且警报代码很少执行和检查。缺乏系统化设计和验证警报的工具。

Method: 在OXN观测实验工具中新增警报扩展功能，让工程师可以在设计阶段调整规则并常规验证警报触发行为。

Result: 工程师现在可以在开发早期实验警报，在设计时调整规则，并常规验证警报的触发行为。

Conclusion: OXN的警报扩展解决了系统化设计和验证警报的挑战，帮助避免未来运行时问题。

Abstract: Observability and alerting form the backbone of modern reliability
engineering. Alerts help teams catch faults early before they turn into
production outages and serve as first clues for troubleshooting. However,
designing effective alerts is challenging. They need to strike a fine balance
between catching issues early and minimizing false alarms. On top of this,
alerts often cover uncommon faults, so the code is rarely executed and
therefore rarely checked. To address these challenges, several industry
practitioners advocate for testing alerting code with the same rigor as
application code. Still, there's a lack of tools that support such systematic
design and validation of alerts.
  This paper introduces a new alerting extension for the observability
experimentation tool OXN. It lets engineers experiment with alerts early during
development. With OXN, engineers can now tune rules at design time and
routinely validate the firing behavior of their alerts, avoiding future
problems at runtime.

</details>


### [8] [Lifecycle-Aware code generation: Leveraging Software Engineering Phases in LLMs](https://arxiv.org/abs/2510.24019)
*Xing Xing,Wei Wang,Lipeng Ma,Weidong Yang,Junjie Zheng*

Main category: cs.SE

TL;DR: 提出了一个生命周期感知的代码生成框架，通过引入需求分析、状态机建模和伪代码等中间产物，将代码生成与标准软件开发阶段对齐，显著提升了代码正确性和质量。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型在代码生成中主要依赖从问题描述到代码的单步翻译，忽视了结构化的软件工程实践，因此需要将中间产物系统性地融入训练和推理阶段。

Method: 构建生命周期感知框架，在训练和推理阶段引入需求分析、状态机建模和伪代码等中间产物，采用多步推理而非单步生成，并对开源LLM进行微调。

Result: 生命周期级微调使代码正确性提升高达75%；在DeepSeek-Coder-1.3B上，CodeBLEU相对ChatGPT-3.5、ChatGPT-4o-mini、DeepSeek-R1和LLaMA-8B分别提升34.3%、20.0%、11.2%和22.3%；训练数据减少80%仍保持稳健。

Conclusion: 中间产物对最终代码质量有显著贡献，状态机建模影响最大；微调后的开源LLM可匹配或超越预训练模型；多步推理始终优于单步生成，证实了中间支架的有效性。

Abstract: Recent progress in large language models (LLMs) has advanced automatic code
generation, yet most approaches rely on direct, single-step translation from
problem descriptions to code, disregarding structured software engineering
practices. We introduce a lifecycle-aware framework that systematically
incorporates intermediate artifacts such as requirements analysis, state
machine modeling, and pseudocode into both the training and inference stages.
This design aligns code generation with standard software development phases
and enables more structured reasoning. Experiments show that lifecycle-level
fine-tuning improves code correctness by up to 75% over the same model before
fine-tuning, with performance gains compounding across intermediate stages.
Multi-step inference consistently surpasses single-step generation,
demonstrating the effectiveness of intermediate scaffolding. Notably,
open-source LLMs, once fine-tuned under our framework, match or slightly
outperform models pretrained on code. When applied to DeepSeek-Coder-1.3B, our
framework yields relative CodeBLEU improvements of 34.3%, 20.0%, 11.2%, and
22.3% over ChatGPT-3.5, ChatGPT-4o-mini, DeepSeek-R1, and LLaMA-8B,
respectively. Our pipeline also proves robust with up to 80\% less training
data, confirming its resilience. Ablation studies further reveal that each
intermediate artifact contributes distinctly to final code quality, with state
machine modeling yielding the most substantial impact. Our source code and
detailed experimental data are available at
https://anonymous.4open.science/r/Lifecycle-Aware-3CCB.

</details>


### [9] [Monitoring and Observability of Machine Learning Systems: Current Practices and Gaps](https://arxiv.org/abs/2510.24142)
*Joran Leest,Ilias Gerostathopoulos,Patricia Lago,Claudia Raibulet*

Main category: cs.SE

TL;DR: 该研究通过七个焦点小组会议，实证分析了机器学习系统在实际应用中的可观测性实践，识别了当前实践中的差距，并为工具设计和研究提供了建议。


<details>
  <summary>Details</summary>
Motivation: 生产机器学习系统通常以错误决策而非崩溃的方式失败，虽然可观测性对ML运维至关重要，但缺乏关于实践者实际捕获内容的实证证据。

Method: 通过在不同领域的七个焦点小组会议进行实证研究，系统化地编目实践者在ML系统及其环境中捕获的信息，并分析他们如何使用这些信息。

Result: 研究展示了ML可观测性的实证结果，包括实践者如何验证模型、检测和诊断故障以及解释观察到的性能下降。

Conclusion: 识别了当前实践中的差距，并为建立ML可观测性实践的工具设计和研究提供了指导意义。

Abstract: Production machine learning (ML) systems fail silently -- not with crashes,
but through wrong decisions. While observability is recognized as critical for
ML operations, there is a lack empirical evidence of what practitioners
actually capture. This study presents empirical results on ML observability in
practice through seven focus group sessions in several domains. We catalog the
information practitioners systematically capture across ML systems and their
environment and map how they use it to validate models, detect and diagnose
faults, and explain observed degradations. Finally, we identify gaps in current
practice and outline implications for tooling design and research to establish
ML observability practices.

</details>


### [10] [Investigating Software Aging in LLM-Generated Software Systems](https://arxiv.org/abs/2510.24188)
*César Santos,Ermeson Andrade,Roberto Natella*

Main category: cs.SE

TL;DR: 该论文研究了LLM生成软件中的软件老化现象，通过50小时负载测试发现所有应用都出现了显著的内存增长、响应时间增加和性能不稳定问题。


<details>
  <summary>Details</summary>
Motivation: 随着LLM生成的软件被广泛采用，需要了解这类系统在持续执行下的长期可靠性，特别是软件老化现象。

Method: 使用Bolt平台和Baxbench标准化提示生成四个面向服务的应用，进行50小时负载测试，持续监控资源使用、响应时间和吞吐量。

Result: 所有应用都显示出显著的软件老化证据，包括渐进式内存增长、响应时间增加和性能不稳定，统计分析确认了这些趋势。

Conclusion: 自动生成软件需要考虑老化问题，这为未来研究缓解策略和长期可靠性评估奠定了基础。

Abstract: Automatically generated software, especially code produced by Large Language
Models (LLMs), is increasingly adopted to accelerate development and reduce
manual effort. However, little is known about the long-term reliability of such
systems under sustained execution. In this paper, we experimentally investigate
the phenomenon of software aging in applications generated by LLM-based tools.
Using the Bolt platform and standardized prompts from Baxbench, we generated
four service-oriented applications and subjected them to 50-hour load tests.
Resource usage, response time, and throughput were continuously monitored to
detect degradation patterns. The results reveal significant evidence of
software aging, including progressive memory growth, increased response time,
and performance instability across all applications. Statistical analyzes
confirm these trends and highlight variability in the severity of aging
according to the type of application. Our findings show the need to consider
aging in automatically generated software and provide a foundation for future
studies on mitigation strategies and long-term reliability evaluation.

</details>


### [11] [MAGNET: A Multi-Graph Attentional Network for Code Clone Detection](https://arxiv.org/abs/2510.24241)
*Zixian Zhang,Takfarinas Saber*

Main category: cs.SE

TL;DR: MAGNET是一个多图注意力框架，通过联合利用AST、CFG和DFG表示来捕获源代码的语法和语义特征，在代码克隆检测任务中实现了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 现有代码克隆检测方法通常依赖单一表示（如AST、CFG、DFG），只能捕获代码语义的部分方面。混合方法虽然出现，但其融合策略通常是手工设计且效果不佳。

Method: MAGNET整合残差图神经网络与节点级自注意力来学习局部和长程依赖，引入门控交叉注意力机制进行细粒度图间交互，并使用Set2Set池化将多图嵌入融合为统一的程序级表示。

Result: 在BigCloneBench和Google Code Jam数据集上的广泛实验表明，MAGNET在两个数据集上分别实现了96.5%和99.2%的总体F1分数，达到了最先进的性能。

Conclusion: 消融研究证实了多图融合和每个注意力组件的重要贡献。MAGNET为代码克隆检测提供了一种有效的多图融合方法。

Abstract: Code clone detection is a fundamental task in software engineering that
underpins refactoring, debugging, plagiarism detection, and vulnerability
analysis. Existing methods often rely on singular representations such as
abstract syntax trees (ASTs), control flow graphs (CFGs), and data flow graphs
(DFGs), which capture only partial aspects of code semantics. Hybrid approaches
have emerged, but their fusion strategies are typically handcrafted and
ineffective. In this study, we propose MAGNET, a multi-graph attentional
framework that jointly leverages AST, CFG, and DFG representations to capture
syntactic and semantic features of source code. MAGNET integrates residual
graph neural networks with node-level self-attention to learn both local and
long-range dependencies, introduces a gated cross-attention mechanism for
fine-grained inter-graph interactions, and employs Set2Set pooling to fuse
multi-graph embeddings into unified program-level representations. Extensive
experiments on BigCloneBench and Google Code Jam demonstrate that MAGNET
achieves state-of-the-art performance with an overall F1 score of 96.5\% and
99.2\% on the two datasets, respectively. Ablation studies confirm the critical
contributions of multi-graph fusion and each attentional component. Our code is
available at https://github.com/ZixianReid/Multigraph_match

</details>


### [12] [Developer Productivity with GenAI](https://arxiv.org/abs/2510.24265)
*Sadia Afroz,Zixuan Feng,Katie Kimura,Bianca Trinkenreich,Igor Steinmacher,Anita Sarma*

Main category: cs.SE

TL;DR: 研究调查了生成式AI工具对开发者生产力的影响，发现虽然开发速度提升，但软件质量改善有限，开发者满意度变化不大，存在生产力悖论。


<details>
  <summary>Details</summary>
Motivation: 生成式AI工具在软件开发中日益普及，但其对开发者生产力的实际影响尚不明确，需要实证研究来了解这些工具在何时何地真正提升生产力。

Method: 对415名软件从业者进行问卷调查，使用SPACE框架（满意度与幸福感、绩效、活动、沟通协作、效率与流畅性）评估AI辅助开发对生产力的影响。

Result: 结果显示整体生产力变化有限，存在生产力悖论：开发者速度变快但未必创造更好的软件或感到更满意，结果按AI使用频率进行了细分分析。

Conclusion: 生成式AI工具在软件开发中的采用带来了速度提升，但并未显著改善软件质量或开发者满意度，需要更深入理解AI工具对生产力的多维影响。

Abstract: Generative AI (GenAI) tools are increasingly being adopted in software
development as productivity aids. However, evidence regarding where and when
these tools actually enhance productivity is unclear. In this paper, we
investigate how GenAI adoption affects different dimensions of developer
productivity. We surveyed 415 software practitioners to capture their
perceptions of productivity changes associated with AI-assisted development
using the SPACE framework - Satisfaction and well-being, Performance, Activity,
Communication and collaboration, and Efficiency and flow. Our results,
disaggregated by frequency of AI usage, reveal limited overall productivity
change, highlighting the productivity paradox in which developers become faster
but do not necessarily create better software or feel more fulfilled.

</details>


### [13] [Automatically Benchmarking LLM Code Agents through Agent-Driven Annotation and Evaluation](https://arxiv.org/abs/2510.24358)
*Lingyue Fu,Bolun Zhang,Hao Guan,Yaoming Zhu,Lin Qiu,Weiwen Liu,Xuezhi Cao,Xunliang Cai,Weinan Zhang,Yong Yu*

Main category: cs.SE

TL;DR: 提出了PRDBench基准，通过智能体驱动的流水线构建包含50个真实Python项目的基准，解决了现有代码智能体评估基准的高标注成本和僵化评估指标问题。


<details>
  <summary>Details</summary>
Motivation: 现有代码智能体评估基准存在两大局限：高标注成本和专业要求，以及主要依赖单元测试的僵化评估指标。

Method: 采用智能体驱动的基准构建流水线，结合人工监督生成多样化且具有挑战性的项目级任务，并引入Agent-as-a-Judge范式来评分智能体输出。

Result: 构建了PRDBench基准，包含50个跨20个领域的真实Python项目，每个项目都有结构化产品需求文档、综合评估标准和参考实现。

Conclusion: PRDBench为代码智能体和评估智能体的能力评估提供了有效基准，为标注和评估提供了可扩展且稳健的框架。

Abstract: Recent advances in code agents have enabled automated software development at
the project level, supported by large language models (LLMs) and widely adopted
tools. However, existing benchmarks for code agent evaluation face two major
limitations: high annotation cost and expertise requirements, and rigid
evaluation metrics that rely primarily on unit tests. To address these
challenges, we propose an agent-driven benchmark construction pipeline that
leverages human supervision to efficiently generate diverse and challenging
project-level tasks. Based on this approach, we introduce PRDBench, a novel
benchmark comprising 50 real-world Python projects across 20 domains, each with
structured Product Requirement Document (PRD) requirements, comprehensive
evaluation criteria, and reference implementations. PRDBench features rich data
sources, high task complexity, and flexible metrics. We further employ an
Agent-as-a-Judge paradigm to score agent outputs, enabling the evaluation of
various test types beyond unit tests. Extensive experiments on PRDBench
demonstrate its effectiveness in assessing the capabilities of both code agents
and evaluation agents, providing a scalable and robust framework for annotation
and evaluation.

</details>


### [14] [LLM-as-a-Judge for Software Engineering: Literature Review, Vision, and the Road Ahead](https://arxiv.org/abs/2510.24367)
*Junda He,Jieke Shi,Terry Yue Zhuo,Christoph Treude,Jiamou Sun,Zhenchang Xing,Xiaoning Du,David Lo*

Main category: cs.SE

TL;DR: 本文提出LLM-as-a-Judge范式，旨在解决LLM生成软件制品评估的瓶颈问题，通过文献综述、局限性分析和研究路线图，推动该范式成为可靠、可扩展的人类评估替代方案。


<details>
  <summary>Details</summary>
Motivation: LLM在软件工程中的快速集成产生了大量软件制品，但缺乏可扩展的可靠评估方法。人工评估成本高，传统自动化指标无法捕捉质量细节，需要新的评估范式。

Method: 采用文献综述方法分析现有SE研究，识别局限性，找出研究空白，并制定详细的研究路线图。

Result: LLM-as-a-Judge研究仍处于早期阶段，但具有通过LLM高级推理能力实现类人评估的潜力。

Conclusion: 到2030年，LLM-as-a-Judge框架有望成为可靠、鲁棒、可扩展的人类评估替代方案，实现一致的多维度制品评估，提升软件制品评估的可扩展性。

Abstract: The rapid integration of Large Language Models (LLMs) into software
engineering (SE) has revolutionized tasks like code generation, producing a
massive volume of software artifacts. This surge has exposed a critical
bottleneck: the lack of scalable, reliable methods to evaluate these outputs.
Human evaluation is costly and time-consuming, while traditional automated
metrics like BLEU fail to capture nuanced quality aspects. In response, the
LLM-as-a-Judge paradigm - using LLMs for automated evaluation - has emerged.
This approach leverages the advanced reasoning of LLMs, offering a path toward
human-like nuance at automated scale. However, LLM-as-a-Judge research in SE is
still in its early stages. This forward-looking SE 2030 paper aims to steer the
community toward advancing LLM-as-a-Judge for evaluating LLM-generated software
artifacts. We provide a literature review of existing SE studies, analyze their
limitations, identify key research gaps, and outline a detailed roadmap. We
envision these frameworks as reliable, robust, and scalable human surrogates
capable of consistent, multi-faceted artifact evaluation by 2030. Our work aims
to foster research and adoption of LLM-as-a-Judge frameworks, ultimately
improving the scalability of software artifact evaluation.

</details>


### [15] [CodeWiki: Automated Repository-Level Documentation at Scale](https://arxiv.org/abs/2510.24428)
*Nguyen Hoang Anh,Minh Le-Anh,Bach Le,Nghi D. Q. Bui*

Main category: cs.SE

TL;DR: CodeWiki是首个开源的全仓库级代码文档框架，通过层次分解、递归代理处理和文本视觉合成，在7种编程语言上实现高质量的仓库级文档生成，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 开发人员58%时间用于理解代码库，但现有LLM只能在函数级别生成文档，无法处理仓库级别的架构模式和跨模块交互，需要更全面的文档解决方案。

Method: 采用三个创新：层次分解保持架构上下文、递归代理处理与动态委托、文本和视觉工件的合成（包括架构图和数据流）。

Result: 在CodeWikiBench基准测试中，CodeWiki使用专有模型获得68.79%质量分数，开源替代方案获得64.80%，优于现有闭源系统。

Conclusion: CodeWiki展示了为真实世界仓库提供可扩展、准确文档的能力，是首个有效的仓库级文档生成框架。

Abstract: Developers spend nearly 58% of their time understanding codebases, yet
maintaining comprehensive documentation remains challenging due to complexity
and manual effort. While recent Large Language Models (LLMs) show promise for
function-level documentation, they fail at the repository level, where
capturing architectural patterns and cross-module interactions is essential. We
introduce CodeWiki, the first open-source framework for holistic
repository-level documentation across seven programming languages. CodeWiki
employs three innovations: (i) hierarchical decomposition that preserves
architectural context, (ii) recursive agentic processing with dynamic
delegation, and (iii) synthesis of textual and visual artifacts including
architecture diagrams and data flows. We also present CodeWikiBench, the first
repository-level documentation benchmark with multi-level rubrics and agentic
assessment. CodeWiki achieves 68.79% quality score with proprietary models and
64.80% with open-source alternatives, outperforming existing closed-source
systems and demonstrating scalable, accurate documentation for real-world
repositories.

</details>


### [16] [The Divine Software Engineering Comedy -- Inferno: The Okinawa Files](https://arxiv.org/abs/2510.24483)
*Michele Lanza*

Main category: cs.SE

TL;DR: 作者基于2024年6月在日本冲绳举办的软件工程未来研讨会，提出了对软件工程未来发展的三个噩梦：软件开发者不知道自己在做什么但依然完成任务、领域发展太快无法记住教训、技术像春天兔子般快速繁殖。


<details>
  <summary>Details</summary>
Motivation: 作者参加FUSE研讨会后，对软件工程领域的未来发展感到担忧，希望通过这篇文章表达对当前趋势的批判性观察和讽刺性思考。

Method: 基于研讨会讨论内容，结合个人观察和讽刺性分析，提炼出三个主要问题作为对未来软件工程的警示。

Result: 识别出软件工程面临的三个核心挑战：开发者能力与责任脱节、领域知识传承断裂、技术爆炸式增长带来的混乱。

Conclusion: 作者认为软件工程的未来就像慢动作的车祸，虽然能看到问题来临却无法阻止，表达了对该领域发展方向的悲观看法。

Abstract: In June 2024 I co-organized the FUture of Software Engineering symposium in
Okinawa, Japan. Me, Andrian Marcus, Takashi Kobayashi and Shinpei Hayashi were
general chairs, Nicole Novielli, Kevin Moran, Yutaro Kashiwa and Masanari Kondo
were program chairs, some members of my group, Carmen Armenti, Stefano
Campanella, Roberto Minelli, were the tables, can't have a room with only
chairs, after all. We invited a crowd of people to discuss what future software
engineering has. FUSE became a 3-day marathon on whether there is actually a
future at all for SE. This essay is a slightly dark take about what I saw at
that event, very loosely based on the discussions that took place, adding some
healthy sarcasm and cynicism, the intellectual salt and pepper I never seem to
run out of. I listened to the brilliant people who gathered to talk about where
we're headed, and distilled three nightmares headed in our direction: software
makers who don't know what they're doing, but get the job done anyway, a field
moving so fast it can't remember its own lessons, and technologies multiplying
like rabbits in Spring. So, let's start. The future, eh? The future of software
engineering looks like a car crash in slow motion: you can see it coming but
you can't look away. The thing is...

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [17] [Short Ticketing Detection Framework Analysis Report](https://arxiv.org/abs/2510.23619)
*Yuyang Miao,Huijun Xing,Danilo P. Mandic,Tony G. Constantinides*

Main category: cs.CR

TL;DR: 提出了一种无监督多专家机器学习框架，用于检测铁路系统中的短票欺诈，通过A/B/C/D车站分类系统识别了30个高风险车站的可疑模式。


<details>
  <summary>Details</summary>
Motivation: 铁路系统中短票欺诈问题日益严重，需要有效的无监督检测方法来识别欺诈模式，减少经济损失。

Method: 使用四种互补算法：隔离森林、局部离群因子、一类SVM和马氏距离，结合A/B/C/D车站分类系统进行多专家分析。

Result: 成功识别了30个高风险车站，发现了五种不同的短票欺诈模式，并展示了在交通系统中恢复短票损失的潜力。

Conclusion: 该无监督多专家框架能有效检测铁路短票欺诈，为交通系统欺诈检测提供了可行的解决方案。

Abstract: This report presents a comprehensive analysis of an unsupervised multi-expert
machine learning framework for detecting short ticketing fraud in railway
systems. The study introduces an A/B/C/D station classification system that
successfully identifies suspicious patterns across 30 high-risk stations. The
framework employs four complementary algorithms: Isolation Forest, Local
Outlier Factor, One-Class SVM, and Mahalanobis Distance. Key findings include
the identification of five distinct short ticketing patterns and potential for
short ticketing recovery in transportation systems.

</details>


### [18] [SAND: A Self-supervised and Adaptive NAS-Driven Framework for Hardware Trojan Detection](https://arxiv.org/abs/2510.23643)
*Zhixin Pan,Ziyu Shu,Linh Nguyen,Amberbir Alemayoh*

Main category: cs.CR

TL;DR: 提出SAND框架，一种自监督和自适应NAS驱动的硬件木马检测方法，通过自动化特征提取和动态优化分类器，显著提升检测精度和适应性。


<details>
  <summary>Details</summary>
Motivation: 全球化的半导体供应链使硬件木马成为嵌入式系统的重大安全威胁，现有机器学习检测方法存在特征选择随意和缺乏适应性的问题。

Method: 结合自监督学习实现自动化特征提取，集成神经架构搜索动态优化下游分类器，只需少量微调即可适应新基准。

Result: SAND在检测准确率上比最先进方法提升高达18.3%，对规避性木马具有高弹性，并展现出强大的泛化能力。

Conclusion: SAND框架通过自监督学习和神经架构搜索的结合，有效解决了硬件木马检测中的特征工程和适应性问题，实现了高效且自适应的检测。

Abstract: The globalized semiconductor supply chain has made Hardware Trojans (HT) a
significant security threat to embedded systems, necessitating the design of
efficient and adaptable detection mechanisms. Despite promising machine
learning-based HT detection techniques in the literature, they suffer from ad
hoc feature selection and the lack of adaptivity, all of which hinder their
effectiveness across diverse HT attacks. In this paper, we propose SAND, a
selfsupervised and adaptive NAS-driven framework for efficient HT detection.
Specifically, this paper makes three key contributions. (1) We leverage
self-supervised learning (SSL) to enable automated feature extraction,
eliminating the dependency on manually engineered features. (2) SAND integrates
neural architecture search (NAS) to dynamically optimize the downstream
classifier, allowing for seamless adaptation to unseen benchmarks with minimal
fine-tuning. (3) Experimental results show that SAND achieves a significant
improvement in detection accuracy (up to 18.3%) over state-of-the-art methods,
exhibits high resilience against evasive Trojans, and demonstrates strong
generalization.

</details>


### [19] [MCPGuard : Automatically Detecting Vulnerabilities in MCP Servers](https://arxiv.org/abs/2510.23673)
*Bin Wang,Zexin Liu,Hao Yu,Ao Yang,Yenan Huang,Jing Guo,Huangsheng Cheng,Hui Li,Huiyu Wu*

Main category: cs.CR

TL;DR: 本文系统分析了MCP协议的安全漏洞，识别了三大威胁类别，并调查了现有的防御策略，揭示了MCP安全需要针对自然语言元数据语义解释的新防御机制。


<details>
  <summary>Details</summary>
Motivation: MCP作为标准化接口虽然简化了LLM与外部数据源的集成，但其开放性和可扩展性引入了严重的安全漏洞，威胁系统可信度和用户数据保护。

Method: 系统分析MCP安全态势，识别三类主要威胁：代理劫持攻击、MCP服务器传统Web漏洞和供应链安全，并全面调查现有防御策略。

Result: 发现MCP安全代表了一个范式转变，攻击面从传统代码执行扩展到自然语言元数据的语义解释，需要针对这一独特威胁模型的新防御机制。

Conclusion: MCP安全需要专门针对其独特威胁模型的创新防御方法，重点关注自然语言元数据的语义解释安全。

Abstract: The Model Context Protocol (MCP) has emerged as a standardized interface
enabling seamless integration between Large Language Models (LLMs) and external
data sources and tools. While MCP significantly reduces development complexity
and enhances agent capabilities, its openness and extensibility introduce
critical security vulnerabilities that threaten system trustworthiness and user
data protection. This paper systematically analyzes the security landscape of
MCP-based systems, identifying three principal threat categories: (1) agent
hijacking attacks stemming from protocol design deficiencies; (2) traditional
web vulnerabilities in MCP servers; and (3) supply chain security. To address
these challenges, we comprehensively survey existing defense strategies,
examining both proactive server-side scanning approaches, ranging from layered
detection pipelines and agentic auditing frameworks to zero-trust registry
systems, and runtime interaction monitoring solutions that provide continuous
oversight and policy enforcement. Our analysis reveals that MCP security
fundamentally represents a paradigm shift where the attack surface extends from
traditional code execution to semantic interpretation of natural language
metadata, necessitating novel defense mechanisms tailored to this unique threat
model.

</details>


### [20] [QueryIPI: Query-agnostic Indirect Prompt Injection on Coding Agents](https://arxiv.org/abs/2510.23675)
*Yuchong Xie,Zesen Liu,Mingyu Luo,Zhixiang Zhang,Kaikai Zhang,Zongjie Li,Ping Chen,Shuai Wang,Dongdong She*

Main category: cs.CR

TL;DR: QueryIPI是一种查询无关的间接提示注入攻击方法，通过利用编码代理内部提示泄露漏洞，将攻击转化为约束白盒优化问题，在模拟代理中达到87%的成功率。


<details>
  <summary>Details</summary>
Motivation: 现有间接提示注入研究主要关注查询特定行为，导致攻击不稳定且成功率低。作者发现更严重的查询无关威胁，可通过利用代理内部提示泄露漏洞来解决。

Method: QueryIPI通过迭代的基于提示的过程，根据泄露的内部提示来优化恶意工具描述，实现查询无关的间接提示注入攻击。

Result: 在五个模拟代理上的实验显示，QueryIPI达到最高87%的成功率，优于基线方法，且生成的恶意描述可迁移到真实系统。

Conclusion: QueryIPI揭示了现代基于LLM的编码代理面临的实际安全风险，其查询无关的攻击方法具有高成功率和可迁移性。

Abstract: Modern coding agents integrated into IDEs combine powerful tools and
system-level actions, exposing a high-stakes attack surface. Existing Indirect
Prompt Injection (IPI) studies focus mainly on query-specific behaviors,
leading to unstable attacks with lower success rates. We identify a more
severe, query-agnostic threat that remains effective across diverse user
inputs. This challenge can be overcome by exploiting a common vulnerability:
leakage of the agent's internal prompt, which turns the attack into a
constrained white-box optimization problem. We present QueryIPI, the first
query-agnostic IPI method for coding agents. QueryIPI refines malicious tool
descriptions through an iterative, prompt-based process informed by the leaked
internal prompt. Experiments on five simulated agents show that QueryIPI
achieves up to 87 percent success, outperforming baselines, and the generated
malicious descriptions also transfer to real-world systems, highlighting a
practical security risk to modern LLM-based coding agents.

</details>


### [21] [EthVault: A Secure and Resource-Conscious FPGA-Based Ethereum Cold Wallet](https://arxiv.org/abs/2510.23847)
*Joel Poncha Lemayian,Ghyslain Gagnon,Kaiwen Zhang,Pascal Giard*

Main category: cs.CR

TL;DR: EthVault是首个用于以太坊分层确定性冷钱包的硬件架构，通过硬件实现关键算法来安全生成密钥，并提供抗侧信道和时序攻击的ECC架构。


<details>
  <summary>Details</summary>
Motivation: 现有的软件钱包运行在微控制器上，容易受到恶意软件和侧信道攻击，攻击者可能通过针对ECC等关键算法来提取私钥。

Method: 提出EthVault硬件架构，包括抗侧信道和时序攻击的ECC架构、子密钥派生函数架构，并最小化资源使用以满足小型便携钱包的市场需求。

Result: FPGA实现验证了方法的可行性，ECC架构在不同输入下表现出统一的执行行为，完整设计仅使用Xilinx Zynq UltraScale+ FPGA上27%的LUT、7%的寄存器和6%的RAM块。

Conclusion: EthVault为加密货币钱包提供了更安全的硬件解决方案，有效抵御侧信道和时序攻击，同时满足便携性需求。

Abstract: Cryptocurrency blockchain networks safeguard digital assets using
cryptographic keys, with wallets playing a critical role in generating,
storing, and managing these keys. Wallets, typically categorized as hot and
cold, offer varying degrees of security and convenience. However, they are
generally software-based applications running on microcontrollers.
Consequently, they are vulnerable to malware and side-channel attacks, allowing
perpetrators to extract private keys by targeting critical algorithms, such as
ECC, which processes private keys to generate public keys and authorize
transactions. To address these issues, this work presents EthVault, the first
hardware architecture for an Ethereum hierarchically deterministic cold wallet,
featuring hardware implementations of key algorithms for secure key generation.
Also, an ECC architecture resilient to side-channel and timing attacks is
proposed. Moreover, an architecture of the child key derivation function, a
fundamental component of cryptocurrency wallets, is proposed. The design
minimizes resource usage, meeting market demand for small, portable
cryptocurrency wallets. FPGA implementation results validate the feasibility of
the proposed approach. The ECC architecture exhibits uniform execution behavior
across varying inputs, while the complete design utilizes only 27%, 7%, and 6%
of LUTs, registers, and RAM blocks, respectively, on a Xilinx Zynq UltraScale+
FPGA.

</details>


### [22] [PRO: Enabling Precise and Robust Text Watermark for Open-Source LLMs](https://arxiv.org/abs/2510.23891)
*Jiaqi Xue,Yifei Zhao,Mansour Al Ghanim,Shangqian Gao,Ruimin Sun,Qian Lou,Mengxin Zheng*

Main category: cs.CR

TL;DR: PRO是一种针对开源大语言模型的精确鲁棒文本水印方法，通过联合训练水印策略模型和语言模型，解决了开源模型水印检测性差和抗干扰能力弱的问题。


<details>
  <summary>Details</summary>
Motivation: 开源大语言模型缺乏实用的文本来源验证手段，现有方法在检测性和抗下游修改（如微调、模型合并）方面存在局限。

Method: 联合训练水印策略模型与LLM，生成易于学习且与检测标准一致的模式；使用正则化项模拟下游扰动并惩罚水印检测性退化。

Result: 在LLaMA-3.2、LLaMA-3、Phi-2等开源模型上的实验表明，PRO显著提升了水印检测性和模型修改后的鲁棒性。

Conclusion: PRO为开源LLM提供了一种有效的水印方案，在检测精度和抗干扰能力方面均有显著改进。

Abstract: Text watermarking for large language models (LLMs) enables model owners to
verify text origin and protect intellectual property. While watermarking
methods for closed-source LLMs are relatively mature, extending them to
open-source models remains challenging, as developers cannot control the
decoding process. Consequently, owners of open-source LLMs lack practical means
to verify whether text was generated by their models. A core difficulty lies in
embedding watermarks directly into model weights without hurting detectability.
A promising idea is to distill watermarks from a closed-source model into an
open one, but this suffers from (i) poor detectability due to mismatch between
learned and predefined patterns, and (ii) fragility to downstream modifications
such as fine-tuning or model merging. To overcome these limitations, we propose
PRO, a Precise and Robust text watermarking method for open-source LLMs. PRO
jointly trains a watermark policy model with the LLM, producing patterns that
are easier for the model to learn and more consistent with detection criteria.
A regularization term further simulates downstream perturbations and penalizes
degradation in watermark detectability, ensuring robustness under model edits.
Experiments on open-source LLMs (e.g., LLaMA-3.2, LLaMA-3, Phi-2) show that PRO
substantially improves both watermark detectability and resilience to model
modifications.

</details>


### [23] [Victim as a Service: Designing a System for Engaging with Interactive Scammers](https://arxiv.org/abs/2510.23927)
*Daniel Spokoyny,Nikolai Vogler,Xin Gao,Tianyi Zheng,Yufei Weng,Jonghyun Park,Jiajun Jiao,Geoffrey M. Voelker,Stefan Savage,Taylor Berg-Kirkpatrick*

Main category: cs.CR

TL;DR: 开发了CHATTERBOX系统，使用LLM自动与在线诈骗者进行长期互动，实现大规模调查诈骗策略


<details>
  <summary>Details</summary>
Motivation: 杀猪盘等在线诈骗通过长期对话建立信任，造成巨大经济损失（至少750亿美元），但由于其长期对话特性难以进行大规模调查

Method: 设计基于LLM的CHATTERBOX系统，开发吸引诈骗尝试的技术，使用LLM工程与诈骗者进行可信互动，满足或规避诈骗者工作流程中的"里程碑"

Result: 成功构建了能够自动化长期参与诈骗对话的系统，使大规模调查成为可能

Conclusion: CHATTERBOX系统为调查长期在线诈骗提供了有效的自动化解决方案

Abstract: Pig butchering, and similar interactive online scams, lower their victims'
defenses by building trust over extended periods of conversation - sometimes
weeks or months. They have become increasingly public losses (at least $75B by
one recent study). However, because of their long-term conversational nature,
they are extremely challenging to investigate at scale. In this paper, we
describe the motivation, design, implementation, and experience with
CHATTERBOX, an LLM-based system that automates long-term engagement with online
scammers, making large-scale investigations of their tactics possible. We
describe the techniques we have developed to attract scam attempts, the system
and LLM-engineering required to convincingly engage with scammers, and the
necessary capabilities required to satisfy or evade "milestones" in scammers'
workflow.

</details>


### [24] [Scalable GPU-Based Integrity Verification for Large Machine Learning Models](https://arxiv.org/abs/2510.23938)
*Marcin Spoczynski,Marcela S. Melara*

Main category: cs.CR

TL;DR: 提出了一个安全框架，通过在GPU上原生执行完整性验证来解决分布式机器学习中的安全瓶颈问题


<details>
  <summary>Details</summary>
Motivation: 解决大型ML工作负载主要在GPU上运行，而安全验证传统上在单独的CPU进程中运行的根本不匹配问题

Method: 在GPU加速器上直接进行完整性验证，利用GPU专用计算单元（如Intel Arc的XMX单元、NVIDIA的Tensor Cores）原生执行加密操作

Result: 显著降低验证开销，确保完整性检查能够跟上模型执行速度，即使对于超过100GB的大型模型也能保持同步

Conclusion: 建立了一个跨不同GPU厂商和硬件配置的通用完整性验证机制，为企业团队提供了硬件无关的安全基础架构

Abstract: We present a security framework that strengthens distributed machine learning
by standardizing integrity protections across CPU and GPU platforms and
significantly reducing verification overheads. Our approach co-locates
integrity verification directly with large ML model execution on GPU
accelerators, resolving the fundamental mismatch between how large ML workloads
typically run (primarily on GPUs) and how security verifications traditionally
operate (on separate CPU-based processes), delivering both immediate
performance benefits and long-term architectural consistency. By performing
cryptographic operations natively on GPUs using dedicated compute units (e.g.,
Intel Arc's XMX units, NVIDIA's Tensor Cores), our solution eliminates the
potential architectural bottlenecks that could plague traditional CPU-based
verification systems when dealing with large models. This approach leverages
the same GPU-based high-memory bandwidth and parallel processing primitives
that power ML workloads ensuring integrity checks keep pace with model
execution even for massive models exceeding 100GB. This framework establishes a
common integrity verification mechanism that works consistently across
different GPU vendors and hardware configurations. By anticipating future
capabilities for creating secure channels between trusted execution
environments and GPU accelerators, we provide a hardware-agnostic foundation
that enterprise teams can deploy regardless of their underlying CPU and GPU
infrastructures.

</details>


### [25] [Covert Surveillance in Smart Devices: A SCOUR Framework Analysis of Youth Privacy Implications](https://arxiv.org/abs/2510.24072)
*Austin Shouli,Yulia Bobkova,Ajay Kumar Shrestha*

Main category: cs.CR

TL;DR: 本文通过PRISMA方法综述了智能设备如何隐蔽采集青少年隐私对话，提出了SCOUR分析框架，揭示了隐私担忧、数据采集方法、存储共享实践及技术缓解措施。


<details>
  <summary>Details</summary>
Motivation: 调查智能设备对青少年隐私的隐蔽侵犯问题，由于数据采集实践不明确和透明度不足，青少年隐私面临严重威胁。

Method: 采用PRISMA方法进行结构化文献综述，引入SCOUR框架分析监控机制、同意与意识、操作数据流、使用与利用、监管与技术保障五个维度。

Result: 发现智能设备特别是智能玩具和语音激活设备隐蔽采集个人数据，青少年隐私意识增强但保护措施不足，需要平衡隐私与实用性。

Conclusion: 提出了改善监管和技术保障的策略，识别了研究空白，对智能设备数据采集透明度和政策制定具有重要启示。

Abstract: This paper investigates how smart devices covertly capture private
conversations and discusses in more in-depth the implications of this for youth
privacy. Using a structured review guided by the PRISMA methodology, the
analysis focuses on privacy concerns, data capture methods, data storage and
sharing practices, and proposed technical mitigations. To structure and
synthesize findings, we introduce the SCOUR framework, encompassing
Surveillance mechanisms, Consent and awareness, Operational data flow, Usage
and exploitation, and Regulatory and technical safeguards. Findings reveal that
smart devices have been covertly capturing personal data, especially with smart
toys and voice-activated smart gadgets built for youth. These issues are
worsened by unclear data collection practices and insufficient transparency in
smart device applications. Balancing privacy and utility in smart devices is
crucial, as youth are becoming more aware of privacy breaches and value their
personal data more. Strategies to improve regulatory and technical safeguards
are also provided. The review identifies research gaps and suggests future
directions. The limitations of this literature review are also explained. The
findings have significant implications for policy development and the
transparency of data collection for smart devices.

</details>


### [26] [Traceable Signatures from Lattices](https://arxiv.org/abs/2510.24101)
*Nam Tran,Khoa Nguyen,Dongxi Liu,Josef Pieprzyk,Willy Susilo*

Main category: cs.CR

TL;DR: 提出了一个基于格的可追踪签名方案，在量子随机预言机模型下可证明安全，解决了现有基于数论/配对假设的方案在量子计算机面前不安全的问题。


<details>
  <summary>Details</summary>
Motivation: 现有的可追踪签名方案都基于数论/配对假设，在量子计算机面前可能不安全，需要构建量子安全的替代方案。

Method: 使用格密码学构造可追踪签名方案，在量子随机预言机模型(QROM)下进行安全性证明。

Result: 成功构建了第一个基于格的可追踪签名方案，在QROM下可证明安全。

Conclusion: 这项工作为可追踪签名提供了量子安全的替代方案，填补了格密码学在该领域的空白。

Abstract: Traceable signatures (Kiayas et al., EUROCRYPT 2004) is an anonymous digital
signature system that extends the tracing power of the opening authority in
group signatures. There are many known constructions of traceable signatures,
but all are based on number-theoretic/pairing assumptions. For such reason,
they may not be secure in the presence of quantum computers. This work revisits
the notion of traceable signatures and presents a lattice-based construction
provably secure in the quantum random oracle model (QROM).

</details>


### [27] [Demystifying Cookie Sharing Risks in WebView-based Mobile App-in-app Ecosystems](https://arxiv.org/abs/2510.24141)
*Miao Zhang,Shenao Wang,Guilin Zheng,Yanjie Zhao,Haoyu Wang*

Main category: cs.CR

TL;DR: 本文发现了一种名为跨小程序Cookie共享(CMCS)的新型漏洞，该漏洞源于小程序中共享的web-view环境，允许未经授权的跨小程序数据交换，违反了隔离原则。


<details>
  <summary>Details</summary>
Motivation: 小程序采用web-view组件破坏了原有的隔离机制，暴露了新的攻击面和漏洞。作者旨在研究这种共享web-view环境带来的安全风险。

Method: 分析了四大平台(微信、支付宝、抖音、百度)的web-view机制，开发了MiCoScan静态分析工具进行大规模检测，该工具采用web-view上下文建模和跨web-view数据流分析。

Result: 所有四个平台都受到CMCS漏洞影响。在351,483个小程序的大规模分析中，发现45,448个共享web-view域的集群，7,965个特权数据传输实例，9,877个小程序易受合谋攻击。

Conclusion: CMCS漏洞在小程序生态中广泛存在且带来重大安全风险，迫切需要改进隔离机制。

Abstract: Mini-programs, an emerging mobile application paradigm within super-apps,
offer a seamless and installation-free experience. However, the adoption of the
web-view component has disrupted their isolation mechanisms, exposing new
attack surfaces and vulnerabilities. In this paper, we introduce a novel
vulnerability called Cross Mini-program Cookie Sharing (CMCS), which arises
from the shared web-view environment across mini-programs. This vulnerability
allows unauthorized data exchange across mini-programs by enabling one
mini-program to access cookies set by another within the same web-view context,
violating isolation principles. As a preliminary step, we analyzed the web-view
mechanisms of four major platforms, including WeChat, AliPay, TikTok, and
Baidu, and found that all of them are affected by CMCS vulnerabilities.
Furthermore, we demonstrate the collusion attack enabled by CMCS, where
privileged mini-programs exfiltrate sensitive user data via cookies accessible
to unprivileged mini-programs. To measure the impact of collusion attacks
enabled by CMCS vulnerabilities in the wild, we developed MiCoScan, a static
analysis tool that detects mini-programs affected by CMCS vulnerabilities.
MiCoScan employs web-view context modeling to identify clusters of
mini-programs sharing the same web-view domain and cross-webview data flow
analysis to detect sensitive data transmissions to/from web-views. Using
MiCoScan, we conducted a large-scale analysis of 351,483 mini-programs,
identifying 45,448 clusters sharing web-view domains, 7,965 instances of
privileged data transmission, and 9,877 mini-programs vulnerable to collusion
attacks. Our findings highlight the widespread prevalence and significant
security risks posed by CMCS vulnerabilities, underscoring the urgent need for
improved isolation mechanisms in mini-program ecosystems.

</details>


### [28] [Cybersecurity AI Benchmark (CAIBench): A Meta-Benchmark for Evaluating Cybersecurity AI Agents](https://arxiv.org/abs/2510.24317)
*María Sanz-Gómez,Víctor Mayoral-Vilches,Francesco Balassone,Luis Javier Navarrete-Lozano,Cristóbal R. J. Veas Chavez,Maite del Mundo de Torres*

Main category: cs.CR

TL;DR: CAIBench是一个模块化的网络安全AI基准测试框架，用于评估LLM在攻防网络安全领域的综合能力，揭示了知识掌握与实际能力之间的显著差距。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试只评估孤立技能而非综合表现，预训练LLM的网络安全知识并不代表其攻防能力，存在知识与能力之间的差距。

Method: 开发了CAIBench模块化元基准框架，整合五个评估类别：Jeopardy式CTF、攻防CTF、网络靶场练习、知识基准和隐私评估，覆盖10,000多个实例。

Result: 最先进AI模型在安全知识指标上达到饱和（约70%成功率），但在多步对抗场景中表现大幅下降（20-40%成功率），在机器人目标上更差（22%成功率）。框架搭建与模型选择的匹配可提升攻防CTF性能达2.6倍。

Conclusion: 概念知识与适应能力之间存在显著差距，强调了元基准测试的必要性。

Abstract: Cybersecurity spans multiple interconnected domains, complicating the
development of meaningful, labor-relevant benchmarks. Existing benchmarks
assess isolated skills rather than integrated performance. We find that
pre-trained knowledge of cybersecurity in LLMs does not imply attack and
defense abilities, revealing a gap between knowledge and capability. To address
this limitation, we present the Cybersecurity AI Benchmark (CAIBench), a
modular meta-benchmark framework that allows evaluating LLM models and agents
across offensive and defensive cybersecurity domains, taking a step towards
meaningfully measuring their labor-relevance. CAIBench integrates five
evaluation categories, covering over 10,000 instances: Jeopardy-style CTFs,
Attack and Defense CTFs, Cyber Range exercises, knowledge benchmarks, and
privacy assessments. Key novel contributions include systematic simultaneous
offensive-defensive evaluation, robotics-focused cybersecurity challenges
(RCTF2), and privacy-preserving performance assessment (CyberPII-Bench).
Evaluation of state-of-the-art AI models reveals saturation on security
knowledge metrics (~70\% success) but substantial degradation in multi-step
adversarial (A\&D) scenarios (20-40\% success), or worse in robotic targets
(22\% success). The combination of framework scaffolding and LLM model choice
significantly impacts performance; we find that proper matches improve up to
2.6$\times$ variance in Attack and Defense CTFs. These results demonstrate a
pronounced gap between conceptual knowledge and adaptive capability,
emphasizing the need for a meta-benchmark.

</details>


### [29] [Your Microphone Array Retains Your Identity: A Robust Voice Liveness Detection System for Smart Speakers](https://arxiv.org/abs/2510.24393)
*Yan Meng,Jiachun Li,Matthew Pillari,Arjun Deopujari,Liam Brennan,Hafsah Shamsie,Haojin Zhu,Yuan Tian*

Main category: cs.CR

TL;DR: 提出了一种名为阵列指纹的新型活体检测特征，利用智能音箱内置的麦克风阵列来区分真人语音和重放语音攻击，并开发了轻量级检测方案ARRAYID，在包含32,780个音频样本的数据集上达到99.84%的准确率。


<details>
  <summary>Details</summary>
Motivation: 智能音箱虽然在家居系统中扮演重要角色，但容易受到语音欺骗攻击。现有的被动活体检测方法面临环境因素变化和用户姿势固定的挑战，需要更鲁棒的解决方案。

Method: 利用智能音箱固有的麦克风阵列，提出阵列指纹特征，通过分析麦克风的圆形布局来识别音频来源。开发了ARRAYID轻量级检测方案，结合一系列与阵列指纹协同工作的特征。

Result: 在包含32,780个音频样本和14种欺骗设备的数据集上评估，ARRAYID达到了99.84%的准确率，优于现有的被动活体检测方案。

Conclusion: 阵列指纹特征和ARRAYID方案能够有效应对环境变化和用户移动，提供更鲁棒的语音活体检测性能，显著提升了智能音箱的安全性。

Abstract: Though playing an essential role in smart home systems, smart speakers are
vulnerable to voice spoofing attacks. Passive liveness detection, which
utilizes only the collected audio rather than the deployed sensors to
distinguish between live-human and replayed voices, has drawn increasing
attention. However, it faces the challenge of performance degradation under the
different environmental factors as well as the strict requirement of the fixed
user gestures.
  In this study, we propose a novel liveness feature, array fingerprint, which
utilizes the microphone array inherently adopted by the smart speaker to
determine the identity of collected audios. Our theoretical analysis
demonstrates that by leveraging the circular layout of microphones, compared
with existing schemes, array fingerprint achieves a more robust performance
under the environmental change and user's movement. Then, to leverage such a
fingerprint, we propose ARRAYID, a lightweight passive detection scheme, and
elaborate a series of features working together with array fingerprint. Our
evaluation on the dataset containing 32,780 audio samples and 14 spoofing
devices shows that ARRAYID achieves an accuracy of 99.84%, which is superior to
existing passive liveness detection schemes.

</details>


### [30] [Uncovering Gaps Between RFC Updates and TCP/IP Implementations: LLM-Facilitated Differential Checks on Intermediate Representations](https://arxiv.org/abs/2510.24408)
*Yifan Wu,Xuewei Feng,Yuxiang Yang,Ke Xu*

Main category: cs.CR

TL;DR: 提出基于LLM和差分模型的自动化分析框架，用于检测TCP/IP协议栈实现与RFC标准之间的不一致性，识别潜在安全漏洞


<details>
  <summary>Details</summary>
Motivation: 协议栈代码实现与RFC标准之间的不一致性可能导致功能差异和严重安全漏洞，现有方法无法跨协议规范泛化，自动化检测仍具挑战

Method: 基于LLM和差分模型的自动化分析框架，通过建模协议迭代关系和RFC标准更新关系，对不同版本内核代码实现进行增量代码功能分析

Result: 通过广泛评估验证了框架有效性，证明其在识别由RFC代码不一致引起的潜在漏洞方面的有效性

Conclusion: 提出的自动化分析框架能够有效检测协议栈实现与RFC标准之间的不一致性，识别潜在安全漏洞

Abstract: As the core of the Internet infrastructure, the TCP/IP protocol stack
undertakes the task of network data transmission. However, due to the
complexity of the protocol and the uncertainty of cross-layer interaction,
there are often inconsistencies between the implementation of the protocol
stack code and the RFC standard. This inconsistency may not only lead to
differences in protocol functions but also cause serious security
vulnerabilities. At present, with the continuous expansion of protocol stack
functions and the rapid iteration of RFC documents, it is increasingly
important to detect and fix these inconsistencies. With the rise of large
language models, researchers have begun to explore how to extract protocol
specifications from RFC documents through these models, including protocol
stack modeling, state machine extraction, text ambiguity analysis, and other
related content. However, existing methods rely on predefined patterns or
rule-based approaches that fail to generalize across different protocol
specifications. Automated and scalable detection of these inconsistencies
remains a significant challenge. In this study, we propose an automated
analysis framework based on LLM and differential models. By modeling the
iterative relationship of the protocol and based on the iterative update
relationship of the RFC standard, we perform incremental code function analysis
on different versions of kernel code implementations to automatically perform
code detection and vulnerability analysis. We conduct extensive evaluations to
validate the effectiveness of our framework, demonstrating its effectiveness in
identifying potential vulnerabilities caused by RFC code inconsistencies.

</details>


### [31] [Attack on a PUF-based Secure Binary Neural Network](https://arxiv.org/abs/2510.24422)
*Bijeet Basak,Nupur Patil,Kurian Polachan,Srinivas Vivek*

Main category: cs.CR

TL;DR: 本文展示了对基于PUF保护的BNN模型的密钥恢复攻击，通过差分密码分析方法能够恢复85%的PUF密钥，并重建出具有93%准确率的原始BNN模型。


<details>
  <summary>Details</summary>
Motivation: 由于忆阻器的非易失性特性，部署在忆阻交叉阵列上的BNN容易受到物理攻击。虽然已有PUF方案用于保护BNN，但本文发现该方案存在漏洞。

Method: 采用差分密码分析方法，通过观察模型准确率的变化来逐位重建PUF密钥，最终恢复BNN的权重和偏置矩阵。

Result: 在MNIST数据集上评估，攻击能够恢复85%的PUF密钥，重建的BNN模型达到93%的分类准确率（原模型为96%），整个攻击过程仅需几分钟。

Conclusion: 现有的PUF保护方案对BNN的安全性不足，需要设计更强大的保护机制来抵御此类差分分析攻击。

Abstract: Binarized Neural Networks (BNNs) deployed on memristive crossbar arrays
provide energy-efficient solutions for edge computing but are susceptible to
physical attacks due to memristor nonvolatility. Recently, Rajendran et al.
(IEEE Embedded Systems Letter 2025) proposed a Physical Unclonable Function
(PUF)-based scheme to secure BNNs against theft attacks. Specifically, the
weight and bias matrices of the BNN layers were secured by swapping columns
based on device's PUF key bits.
  In this paper, we demonstrate that this scheme to secure BNNs is vulnerable
to PUF-key recovery attack. As a consequence of our attack, we recover the
secret weight and bias matrices of the BNN. Our approach is motivated by
differential cryptanalysis and reconstructs the PUF key bit-by-bit by observing
the change in model accuracy, and eventually recovering the BNN model
parameters. Evaluated on a BNN trained on the MNIST dataset, our attack could
recover 85% of the PUF key, and recover the BNN model up to 93% classification
accuracy compared to the original model's 96% accuracy. Our attack is very
efficient and it takes a couple of minutes to recovery the PUF key and the
model parameters.

</details>


### [32] [Design and Optimization of Cloud Native Homomorphic Encryption Workflows for Privacy-Preserving ML Inference](https://arxiv.org/abs/2510.24498)
*Tejaswini Bollikonda*

Main category: cs.CR

TL;DR: 提出一个云原生同态加密框架，用于优化隐私保护的机器学习推理服务，通过容器化和Kubernetes编排实现分布式加密计算，相比传统HE流水线实现了3.2倍推理加速和40%内存使用减少。


<details>
  <summary>Details</summary>
Motivation: 随着机器学习模型通过云基础设施部署，用户数据在推理过程中的保密性成为重要安全挑战。同态加密虽然能在加密数据上执行计算，但在大规模云原生流水线中仍面临高计算开销、编排复杂性和模型兼容性问题。

Method: 集成容器化HE模块与Kubernetes编排，支持弹性扩展和分布式并行加密计算；采用密文打包、多项式模数调整和算子融合等优化策略来降低延迟和资源消耗。

Result: 实验结果显示，相比传统HE流水线，该系统实现了最高3.2倍的推理加速和40%的内存使用减少。

Conclusion: 该研究为在零信任云条件下部署保证数据机密性的安全MLaaS系统提供了一条实用路径。

Abstract: As machine learning (ML) models become increasingly deployed through cloud
infrastructures, the confidentiality of user data during inference poses a
significant security challenge. Homomorphic Encryption (HE) has emerged as a
compelling cryptographic technique that enables computation on encrypted data,
allowing predictions to be generated without decrypting sensitive inputs.
However, the integration of HE within large scale cloud native pipelines
remains constrained by high computational overhead, orchestration complexity,
and model compatibility issues.
  This paper presents a systematic framework for the design and optimization of
cloud native homomorphic encryption workflows that support privacy-preserving
ML inference. The proposed architecture integrates containerized HE modules
with Kubernetes-based orchestration, enabling elastic scaling and parallel
encrypted computation across distributed environments. Furthermore,
optimization strategies including ciphertext packing, polynomial modulus
adjustment, and operator fusion are employed to minimize latency and resource
consumption while preserving cryptographic integrity. Experimental results
demonstrate that the proposed system achieves up to 3.2times inference
acceleration and 40% reduction in memory utilization compared to conventional
HE pipelines. These findings illustrate a practical pathway for deploying
secure ML-as-a-Service (MLaaS) systems that guarantee data confidentiality
under zero-trust cloud conditions.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [33] [Game-TARS: Pretrained Foundation Models for Scalable Generalist Multimodal Game Agents](https://arxiv.org/abs/2510.23691)
*Zihao Wang,Xujing Li,Yining Ye,Junjie Fang,Haoming Wang,Longxiang Liu,Shihao Liang,Junting Lu,Zhiyong Wu,Jiazhan Feng,Wanjun Zhong,Zili Li,Yu Wang,Yu Miao,Bo Zhou,Yuanfan Li,Hao Wang,Zhongkai Zhao,Faming Wu,Zhengxuan Jiang,Weihao Tan,Heyuan Yao,Shi Yan,Xiangyang Li,Yitao Liang,Yujia Qin,Guang Shi*

Main category: cs.AI

TL;DR: Game-TARS是一个通用游戏智能体，使用统一、可扩展的键盘鼠标动作空间进行训练，通过大规模预训练在异构游戏环境中表现出色。


<details>
  <summary>Details</summary>
Motivation: 开发一个能够跨操作系统、网页和模拟游戏等异构领域进行大规模持续预训练的通用游戏智能体，避免API或GUI方法的限制。

Method: 使用基于人类键盘鼠标输入的统一动作空间，结合500B token的多样化轨迹和多模态数据进行预训练，采用衰减持续损失减少因果混淆，以及稀疏思维策略平衡推理深度和推理成本。

Result: 在开放世界Minecraft任务中成功率比之前最佳模型提高约2倍，在未见过的网页3D游戏中接近人类新手水平，在FPS基准测试中超越GPT-5、Gemini-2.5-Pro和Claude-4-Sonnet。

Conclusion: 简单可扩展的动作表示结合大规模预训练为开发具有广泛计算机使用能力的通用智能体提供了有前景的路径。

Abstract: We present Game-TARS, a generalist game agent trained with a unified,
scalable action space anchored to human-aligned native keyboard-mouse inputs.
Unlike API- or GUI-based approaches, this paradigm enables large-scale
continual pre-training across heterogeneous domains, including OS, web, and
simulation games. Game-TARS is pre-trained on over 500B tokens with diverse
trajectories and multimodal data. Key techniques include a decaying continual
loss to reduce causal confusion and an efficient Sparse-Thinking strategy that
balances reasoning depth and inference cost. Experiments show that Game-TARS
achieves about 2 times the success rate over the previous sota model on
open-world Minecraft tasks, is close to the generality of fresh humans in
unseen web 3d games, and outperforms GPT-5, Gemini-2.5-Pro, and Claude-4-Sonnet
in FPS benchmarks. Scaling results on training-time and test-time confirm that
the unified action space sustains improvements when scaled to cross-game and
multimodal data. Our results demonstrate that simple, scalable action
representations combined with large-scale pre-training provide a promising path
toward generalist agents with broad computer-use abilities.

</details>


### [34] [AI and the Decentering of Disciplinary Creativity](https://arxiv.org/abs/2510.23734)
*Eamon Duede*

Main category: cs.AI

TL;DR: 本文探讨AI在科学问题解决中的作用，重点关注其对学科创造力的影响。通过数学案例表明，虽然计算能扩展学科创造力，但某些AI方法可能取代它，从而改变科学追求的价值。


<details>
  <summary>Details</summary>
Motivation: 研究人工智能如何影响科学领域的创造力，特别是学科创造力的概念——在特定学科内运用专业知识解决有价值问题的创造性应用。

Method: 基于创造力哲学理论，区分创造性方法和创造性产品，通过两个数学案例进行分析。

Result: 研究发现计算可以扩展学科创造力，但某些AI方法会取代学科创造力，这可能改变科学追求的价值。

Conclusion: AI在科学问题解决中具有双重作用：既能扩展学科创造力，也可能通过取代它而改变科学追求的本质和价值。

Abstract: This paper examines the role of artificial intelligence in scientific
problem-solving, with a focus on its implications for disciplinary creativity.
Drawing on recent work in the philosophy of creativity, I distinguish between
creative approaches and creative products, and introduce the concept of
disciplinary creativity -the creative application of discipline-specific
expertise to a valued problem within that field. Through two cases in
mathematics, I show that while computation can extend disciplinary creativity,
certain approaches involving AI can serve to displace it. This displacement has
the potential to alter (and, perhaps, diminish) the value of scientific
pursuit.

</details>


### [35] [Multi-Environment POMDPs: Discrete Model Uncertainty Under Partial Observability](https://arxiv.org/abs/2510.23744)
*Eline M. Bovy,Caleb Probine,Marnix Suilen,Ufuk Topcu,Nils Jansen*

Main category: cs.AI

TL;DR: 该论文提出了多环境POMDPs（ME-POMDPs）及其扩展形式AB-POMDPs，用于处理具有离散模型不确定性的POMDP问题，并开发了精确和近似算法来计算鲁棒策略。


<details>
  <summary>Details</summary>
Motivation: 当多个领域专家对问题建模存在分歧时，需要一种能够处理模型不确定性的框架。ME-POMDPs扩展了标准POMDPs，能够表示一组共享状态、动作和观测空间但可能任意变化其转移、观测和奖励模型的POMDPs集合。

Method: 1. 将ME-POMDPs推广到具有初始信念集合的POMDPs（AB-POMDPs）
2. 证明任意ME-POMDP可以简化为仅在转移和奖励函数或仅在观测和奖励函数上变化的ME-POMDP
3. 开发精确和近似（基于点）算法来计算AB-POMDPs的鲁棒策略

Result: 成功将标准POMDP基准扩展到多环境设置，并能够计算相应的策略。算法能够处理具有模型不确定性的复杂决策问题。

Conclusion: ME-POMDPs和AB-POMDPs为处理模型不确定性提供了一种有效的框架，所提出的算法能够计算在多个可能环境中最坏情况下表现最优的鲁棒策略。

Abstract: Multi-environment POMDPs (ME-POMDPs) extend standard POMDPs with discrete
model uncertainty. ME-POMDPs represent a finite set of POMDPs that share the
same state, action, and observation spaces, but may arbitrarily vary in their
transition, observation, and reward models. Such models arise, for instance,
when multiple domain experts disagree on how to model a problem. The goal is to
find a single policy that is robust against any choice of POMDP within the set,
i.e., a policy that maximizes the worst-case reward across all POMDPs. We
generalize and expand on existing work in the following way. First, we show
that ME-POMDPs can be generalized to POMDPs with sets of initial beliefs, which
we call adversarial-belief POMDPs (AB-POMDPs). Second, we show that any
arbitrary ME-POMDP can be reduced to a ME-POMDP that only varies in its
transition and reward functions or only in its observation and reward
functions, while preserving (optimal) policies. We then devise exact and
approximate (point-based) algorithms to compute robust policies for AB-POMDPs,
and thus ME-POMDPs. We demonstrate that we can compute policies for standard
POMDP benchmarks extended to the multi-environment setting.

</details>


### [36] [Test-Time Tuned Language Models Enable End-to-end De Novo Molecular Structure Generation from MS/MS Spectra](https://arxiv.org/abs/2510.23746)
*Laura Mismetti,Marvin Alberts,Andreas Krause,Mara Graziani*

Main category: cs.AI

TL;DR: 提出了一种基于测试时调优的框架，直接从串联质谱和分子式生成分子结构，无需数据库匹配或中间步骤，在NPLIB1和MassSpecGym基准上分别比DiffMS方法提升100%和20%。


<details>
  <summary>Details</summary>
Motivation: 当前方法依赖数据库匹配或多步流程，难以识别未在参考数据库中出现的化合物，需要一种能够直接从质谱数据生成分子结构的端到端方法。

Method: 利用测试时调优增强预训练transformer模型，直接从串联质谱和分子式进行端到端的从头分子结构生成，绕过手动注释和中间步骤。

Result: 在NPLIB1和MassSpecGym基准上分别比DiffMS方法提升100%和20%，测试时调优相比传统微调在MassSpecGym上带来62%的相对性能提升。

Conclusion: 该方法能够动态适应新质谱数据，即使预测偏离真实值，生成的分子候选仍保持结构准确性，为人工解释和可靠识别提供有价值指导。

Abstract: Tandem Mass Spectrometry enables the identification of unknown compounds in
crucial fields such as metabolomics, natural product discovery and
environmental analysis. However, current methods rely on database matching from
previously observed molecules, or on multi-step pipelines that require
intermediate fragment or fingerprint prediction. This makes finding the correct
molecule highly challenging, particularly for compounds absent from reference
databases. We introduce a framework that, by leveraging test-time tuning,
enhances the learning of a pre-trained transformer model to address this gap,
enabling end-to-end de novo molecular structure generation directly from the
tandem mass spectra and molecular formulae, bypassing manual annotations and
intermediate steps. We surpass the de-facto state-of-the-art approach DiffMS on
two popular benchmarks NPLIB1 and MassSpecGym by 100% and 20%, respectively.
Test-time tuning on experimental spectra allows the model to dynamically adapt
to novel spectra, and the relative performance gain over conventional
fine-tuning is of 62% on MassSpecGym. When predictions deviate from the ground
truth, the generated molecular candidates remain structurally accurate,
providing valuable guidance for human interpretation and more reliable
identification.

</details>


### [37] [Evaluating In Silico Creativity: An Expert Review of AI Chess Compositions](https://arxiv.org/abs/2510.23772)
*Vivek Veeriah,Federico Barbero,Marcus Chiam,Xidong Feng,Michael Dennis,Ryan Pachauri,Thomas Tumiel,Johan Obando-Ceron,Jiaxin Shi,Shaobo Hou,Satinder Singh,Nenad Tomašev,Tom Zahavy*

Main category: cs.AI

TL;DR: 该研究探讨生成式AI在象棋谜题领域的创造力，开发了一个能生成具有美学吸引力、新颖性、反直觉和独特解决方案的谜题系统，并由三位国际象棋专家评估其创造性。


<details>
  <summary>Details</summary>
Motivation: 随着生成式AI的快速发展，人们对其产生创造性新颖输出的能力存在疑问，研究旨在验证AI在象棋谜题创作方面的创造力。

Method: 开发AI系统生成象棋谜题，邀请三位国际象棋专家（国际大师Amatzia Avni、特级大师Jonathan Levitt和Matthew Sadler）评估AI生成的谜题选集，基于创造力、挑战性和美学设计等标准评选最佳谜题。

Result: 三位国际象棋专家成功从AI生成的谜题中挑选出他们最喜欢的谜题，并解释了这些谜题的吸引力所在。

Conclusion: 研究表明AI系统能够生成具有创造性和美学价值的象棋谜题，证明了生成式AI在特定领域具备创造潜力。

Abstract: The rapid advancement of Generative AI has raised significant questions
regarding its ability to produce creative and novel outputs. Our recent work
investigates this question within the domain of chess puzzles and presents an
AI system designed to generate puzzles characterized by aesthetic appeal,
novelty, counter-intuitive and unique solutions. We briefly discuss our method
below and refer the reader to the technical paper for more details. To assess
our system's creativity, we presented a curated booklet of AI-generated puzzles
to three world-renowned experts: International Master for chess compositions
Amatzia Avni, Grandmaster Jonathan Levitt, and Grandmaster Matthew Sadler. All
three are noted authors on chess aesthetics and the evolving role of computers
in the game. They were asked to select their favorites and explain what made
them appealing, considering qualities such as their creativity, level of
challenge, or aesthetic design.

</details>


### [38] [Why Foundation Models in Pathology Are Failing](https://arxiv.org/abs/2510.23807)
*Hamid R. Tizhoosh*

Main category: cs.AI

TL;DR: 病理学基础模型在癌症诊断、预后和多模态检索方面表现不佳，存在诊断准确性低、鲁棒性差、几何不稳定、计算需求大和安全漏洞等问题，需要重新思考该范式。


<details>
  <summary>Details</summary>
Motivation: 非医学领域的基础模型通过大规模自监督和多模态学习在计算机视觉和语言处理方面取得了突破，但在计算病理学中的快速应用并未带来预期的突破。

Method: 通过系统评估识别病理学基础模型的根本弱点，并分析这些弱点源于通用基础建模假设与人体组织内在复杂性之间的概念不匹配。

Result: 识别出七个相互关联的原因：生物复杂性、无效的自监督、过度泛化、过度架构复杂性、缺乏领域特定创新、数据不足以及与组织块大小相关的基本设计缺陷。

Conclusion: 当前病理学基础模型在概念上与组织形态学性质不匹配，需要从根本上重新思考该范式本身。

Abstract: In non-medical domains, foundation models (FMs) have revolutionized computer
vision and language processing through large-scale self-supervised and
multimodal learning. Consequently, their rapid adoption in computational
pathology was expected to deliver comparable breakthroughs in cancer diagnosis,
prognostication, and multimodal retrieval. However, recent systematic
evaluations reveal fundamental weaknesses: low diagnostic accuracy, poor
robustness, geometric instability, heavy computational demands, and concerning
safety vulnerabilities. This short paper examines these shortcomings and argues
that they stem from deeper conceptual mismatches between the assumptions
underlying generic foundation modeling in mainstream AI and the intrinsic
complexity of human tissue. Seven interrelated causes are identified:
biological complexity, ineffective self-supervision, overgeneralization,
excessive architectural complexity, lack of domain-specific innovation,
insufficient data, and a fundamental design flaw related to tissue patch size.
These findings suggest that current pathology foundation models remain
conceptually misaligned with the nature of tissue morphology and call for a
fundamental rethinking of the paradigm itself.

</details>


### [39] [ReCAP: Recursive Context-Aware Reasoning and Planning for Large Language Model Agents](https://arxiv.org/abs/2510.23822)
*Zhenyu Zhang,Tianyi Chen,Weiran Xu,Alex Pentland,Jiaxin Pei*

Main category: cs.AI

TL;DR: ReCAP是一个用于大语言模型的递归上下文感知推理和规划框架，通过计划分解、结构化父计划重注入和内存高效执行机制，显著提升了长时程任务中的子目标对齐和成功率。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型在需要多步推理和动态重规划的长时程任务中面临的上下文漂移、目标信息丢失和循环失败等问题，同时避免现有方法导致的跨层级连续性减弱或运行时开销过大。

Method: 结合三个关键机制：(1) 计划分解：生成完整子任务列表，执行第一项并细化剩余任务；(2) 结构化父计划重注入：在递归返回时保持多层级上下文一致性；(3) 内存高效执行：限制活动提示，使成本随任务深度线性扩展。

Result: 在各种长时程推理基准测试中显著改善了子目标对齐和成功率，在严格pass@1协议下，同步Robotouille任务提升了32%，异步Robotouille任务提升了29%。

Conclusion: ReCAP框架通过将高层目标与低层动作对齐、减少冗余提示和保持跨递归的连贯上下文更新，有效解决了长时程推理和规划中的关键挑战。

Abstract: Long-horizon tasks requiring multi-step reasoning and dynamic re-planning
remain challenging for large language models (LLMs). Sequential prompting
methods are prone to context drift, loss of goal information, and recurrent
failure cycles, while hierarchical prompting methods often weaken cross-level
continuity or incur substantial runtime overhead. We introduce ReCAP (Recursive
Context-Aware Reasoning and Planning), a hierarchical framework with shared
context for reasoning and planning in LLMs. ReCAP combines three key
mechanisms: (i) plan-ahead decomposition, in which the model generates a full
subtask list, executes the first item, and refines the remainder; (ii)
structured re-injection of parent plans, maintaining consistent multi-level
context during recursive return; and (iii) memory-efficient execution, bounding
the active prompt so costs scale linearly with task depth. Together these
mechanisms align high-level goals with low-level actions, reduce redundant
prompting, and preserve coherent context updates across recursion. Experiments
demonstrate that ReCAP substantially improves subgoal alignment and success
rates on various long-horizon reasoning benchmarks, achieving a 32% gain on
synchronous Robotouille and a 29% improvement on asynchronous Robotouille under
the strict pass@1 protocol.

</details>


### [40] [Decentralized Multi-Agent Goal Assignment for Path Planning using Large Language Models](https://arxiv.org/abs/2510.23824)
*Murad Ismayilov,Edwin Meriaux,Shuo Wen,Gregory Dudek*

Main category: cs.AI

TL;DR: 本文研究多智能体路径规划中的去中心化目标分配问题，比较了贪心启发式、最优分配和基于大语言模型的方法，发现LLM智能体在良好提示下能达到接近最优的性能。


<details>
  <summary>Details</summary>
Motivation: 解决去中心化条件下多智能体在共享环境中的协调挑战，探索语言模型在目标分配问题中的应用潜力。

Method: 智能体基于环境结构化表示独立生成目标偏好排序，通过固定冲突解决规则进行目标分配，无需协商或迭代协调。系统比较了贪心启发式、最优分配和LLM智能体方法。

Result: LLM智能体在提供良好提示和定量信息时，能实现接近最优的完成时间，且持续优于传统启发式方法。

Conclusion: 语言模型在去中心化多智能体路径规划中具有潜力，信息结构设计对此类系统至关重要。

Abstract: Coordinating multiple autonomous agents in shared environments under
decentralized conditions is a long-standing challenge in robotics and
artificial intelligence. This work addresses the problem of decentralized goal
assignment for multi-agent path planning, where agents independently generate
ranked preferences over goals based on structured representations of the
environment, including grid visualizations and scenario data. After this
reasoning phase, agents exchange their goal rankings, and assignments are
determined by a fixed, deterministic conflict-resolution rule (e.g., agent
index ordering), without negotiation or iterative coordination. We
systematically compare greedy heuristics, optimal assignment, and large
language model (LLM)-based agents in fully observable grid-world settings. Our
results show that LLM-based agents, when provided with well-designed prompts
and relevant quantitative information, can achieve near-optimal makespans and
consistently outperform traditional heuristics. These findings underscore the
potential of language models for decentralized goal assignment in multi-agent
path planning and highlight the importance of information structure in such
systems.

</details>


### [41] [From Benchmarks to Business Impact: Deploying IBM Generalist Agent in Enterprise Production](https://arxiv.org/abs/2510.23856)
*Segev Shlomov,Alon Oved,Sami Marreed,Ido Levy,Offer Akrabi,Avi Yaeli,Łukasz Strąk,Elizabeth Koumpan,Yinon Goldshtein,Eilam Shapira,Nir Mashkif,Asaf Adi*

Main category: cs.AI

TL;DR: IBM开发了通用智能体CUGA，采用分层规划-执行架构，在学术基准上表现优异，并在企业业务流程外包人才招聘领域进行了试点评估，展示了在企业环境中应用的潜力。


<details>
  <summary>Details</summary>
Motivation: 企业面临将AI代理从原型转向实际部署的挑战，需要解决框架碎片化、开发缓慢和缺乏标准化评估等问题。通用智能体在学术基准上表现出色，但在企业生产环境中的应用证据有限。

Method: CUGA采用分层规划-执行架构，具有强大的分析基础。在业务流程外包人才招聘领域进行试点评估，并引入了包含26个任务的BPO-TA基准测试。

Result: CUGA在AppWorld和WebArena上达到最先进性能。在初步评估中，CUGA接近专用智能体的准确性，同时显示出减少开发时间和成本的潜力。

Conclusion: 研究提供了通用智能体在企业规模运行的早期证据，并总结了技术和组织经验。为将研究级架构发展为稳健的企业就绪系统提出了要求和下一步计划。

Abstract: Agents are rapidly advancing in automating digital work, but enterprises face
a harder challenge: moving beyond prototypes to deployed systems that deliver
measurable business value. This path is complicated by fragmented frameworks,
slow development, and the absence of standardized evaluation practices.
Generalist agents have emerged as a promising direction, excelling on academic
benchmarks and offering flexibility across task types, applications, and
modalities. Yet, evidence of their use in production enterprise settings
remains limited. This paper reports IBM's experience developing and piloting
the Computer Using Generalist Agent (CUGA), which has been open-sourced for the
community (https://github.com/cuga-project/cuga-agent). CUGA adopts a
hierarchical planner--executor architecture with strong analytical foundations,
achieving state-of-the-art performance on AppWorld and WebArena. Beyond
benchmarks, it was evaluated in a pilot within the Business-Process-Outsourcing
talent acquisition domain, addressing enterprise requirements for scalability,
auditability, safety, and governance. To support assessment, we introduce
BPO-TA, a 26-task benchmark spanning 13 analytics endpoints. In preliminary
evaluations, CUGA approached the accuracy of specialized agents while
indicating potential for reducing development time and cost. Our contribution
is twofold: presenting early evidence of generalist agents operating at
enterprise scale, and distilling technical and organizational lessons from this
initial pilot. We outline requirements and next steps for advancing
research-grade architectures like CUGA into robust, enterprise-ready systems.

</details>


### [42] [Generating Creative Chess Puzzles](https://arxiv.org/abs/2510.23881)
*Xidong Feng,Vivek Veeriah,Marcus Chiam,Michael Dennis,Ryan Pachauri,Thomas Tumiel,Federico Barbero,Johan Obando-Ceron,Jiaxin Shi,Satinder Singh,Shaobo Hou,Nenad Tomašev,Tom Zahavy*

Main category: cs.AI

TL;DR: 本文提出了一种基于强化学习的象棋谜题生成方法，通过设计新颖的奖励机制来提升谜题的独特性、反直觉性和多样性，显著提高了反直觉谜题的生成率。


<details>
  <summary>Details</summary>
Motivation: 尽管生成式AI在各个领域快速发展，但生成真正具有创造性、美学价值和反直觉性的输出仍然是一个挑战，特别是在象棋谜题领域。

Method: 首先对生成式AI架构进行基准测试，然后引入基于象棋引擎搜索统计的强化学习框架，设计奖励机制来增强谜题的独特性、反直觉性、多样性和真实性。

Result: 强化学习方法将反直觉谜题生成率从0.22%（监督学习）大幅提升至2.5%，超过了现有数据集（2.1%）和最佳Lichess训练模型（0.4%）。生成的谜题符合新颖性和多样性标准，保留了美学主题，并被人类专家评为更具创造性、趣味性和反直觉性。

Conclusion: 该方法成功生成了高质量的AI象棋谜题，最终成果得到了三位世界知名专家的认可，证明了AI在创造性任务中的潜力。

Abstract: While Generative AI rapidly advances in various domains, generating truly
creative, aesthetic, and counter-intuitive outputs remains a challenge. This
paper presents an approach to tackle these difficulties in the domain of chess
puzzles. We start by benchmarking Generative AI architectures, and then
introduce an RL framework with novel rewards based on chess engine search
statistics to overcome some of those shortcomings. The rewards are designed to
enhance a puzzle's uniqueness, counter-intuitiveness, diversity, and realism.
Our RL approach dramatically increases counter-intuitive puzzle generation by
10x, from 0.22\% (supervised) to 2.5\%, surpassing existing dataset rates
(2.1\%) and the best Lichess-trained model (0.4\%). Our puzzles meet novelty
and diversity benchmarks, retain aesthetic themes, and are rated by human
experts as more creative, enjoyable, and counter-intuitive than composed book
puzzles, even approaching classic compositions. Our final outcome is a curated
booklet of these AI-generated puzzles, which is acknowledged for creativity by
three world-renowned experts.

</details>


### [43] [Hybrid Modeling, Sim-to-Real Reinforcement Learning, and Large Language Model Driven Control for Digital Twins](https://arxiv.org/abs/2510.23882)
*Adil Rasheed,Oscar Ravik,Omer San*

Main category: cs.AI

TL;DR: 该研究比较了四种预测模型（线性、物理建模、LSTM、混合建模）和三种控制策略（MPC、RL、LLM控制）在数字孪生系统中的应用，发现HAM模型在精度、泛化性和计算效率方面表现最均衡，而MPC控制器稳健，RL适应性强，LLM控制器支持灵活的人机交互。


<details>
  <summary>Details</summary>
Motivation: 研究数字孪生在动态系统建模和控制中的应用，整合物理基础、数据驱动和混合方法，评估传统与AI驱动控制器的性能差异。

Method: 以微型温室为测试平台，开发四种预测模型（线性、PBM、LSTM、HAM）和三种控制策略（MPC、RL、LLM控制），在插值和外推场景下进行比较。

Result: HAM模型在精度、泛化性和计算效率方面表现最均衡；LSTM精度高但资源消耗大；MPC控制器稳健可预测；RL适应性强；LLM控制器结合预测工具可实现灵活的人机交互。

Conclusion: HAM模型在建模中提供最佳平衡性能，MPC控制器稳健，RL适应性强，LLM控制器支持人机交互，为数字孪生系统的建模和控制提供了实用指导。

Abstract: This work investigates the use of digital twins for dynamical system modeling
and control, integrating physics-based, data-driven, and hybrid approaches with
both traditional and AI-driven controllers. Using a miniature greenhouse as a
test platform, four predictive models Linear, Physics-Based Modeling (PBM),
Long Short Term Memory (LSTM), and Hybrid Analysis and Modeling (HAM) are
developed and compared under interpolation and extrapolation scenarios. Three
control strategies Model Predictive Control (MPC), Reinforcement Learning (RL),
and Large Language Model (LLM) based control are also implemented to assess
trade-offs in precision, adaptability, and implementation effort. Results show
that in modeling HAM provides the most balanced performance across accuracy,
generalization, and computational efficiency, while LSTM achieves high
precision at greater resource cost. Among controllers, MPC delivers robust and
predictable performance, RL demonstrates strong adaptability, and LLM-based
controllers offer flexible human-AI interaction when coupled with predictive
tools.

</details>


### [44] [Agentic AI Security: Threats, Defenses, Evaluation, and Open Challenges](https://arxiv.org/abs/2510.23883)
*Shrestha Datta,Shahriar Kabir Nahin,Anshuman Chhabra,Prasant Mohapatra*

Main category: cs.AI

TL;DR: 该调查论文分析了基于大语言模型的智能体AI系统带来的新型安全风险，提出了威胁分类法，并讨论了评估方法和防御策略。


<details>
  <summary>Details</summary>
Motivation: 智能体AI系统具有自主执行任务的能力，在Web、软件和物理环境中创造了与传统AI安全和软件安全不同的新型放大安全风险。

Method: 通过构建威胁分类法，回顾现有基准和评估方法，从技术和治理角度分析防御策略。

Result: 系统性地识别了智能体AI特有的安全威胁，并综合了当前研究成果。

Conclusion: 该研究旨在支持开发安全设计的智能体系统，并强调了该领域面临的开放挑战。

Abstract: Agentic AI systems powered by large language models (LLMs) and endowed with
planning, tool use, memory, and autonomy, are emerging as powerful, flexible
platforms for automation. Their ability to autonomously execute tasks across
web, software, and physical environments creates new and amplified security
risks, distinct from both traditional AI safety and conventional software
security. This survey outlines a taxonomy of threats specific to agentic AI,
reviews recent benchmarks and evaluation methodologies, and discusses defense
strategies from both technical and governance perspectives. We synthesize
current research and highlight open challenges, aiming to support the
development of secure-by-design agent systems.

</details>


### [45] [Latent Chain-of-Thought for Visual Reasoning](https://arxiv.org/abs/2510.23925)
*Guohao Sun,Hang Hua,Jian Wang,Jiebo Luo,Sohail Dianat,Majid Rabbani,Raghuveer Rao,Zhiqiang Tao*

Main category: cs.AI

TL;DR: 提出基于摊销变分推理的可扩展训练算法，通过多样性寻求强化学习改进LVLMs的思维链推理能力，在七个推理基准测试中提升效果、泛化性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有训练算法（SFT、PPO、GRPO）在未见推理任务上泛化能力不足，且过度依赖有偏见的奖励模型，限制了思维链推理的可靠性和可解释性。

Method: 将LVLMs推理重新表述为后验推理，采用摊销变分推理和多样性寻求强化学习，引入稀疏奖励函数进行token级学习，并实施贝叶斯推理缩放策略。

Result: 在七个推理基准测试中，该方法提升了最先进LVLMs的效果、泛化性和可解释性。

Conclusion: 基于摊销变分推理和多样性强化学习的训练算法能有效克服现有方法的局限性，显著提升LVLMs的推理能力。

Abstract: Chain-of-thought (CoT) reasoning is critical for improving the
interpretability and reliability of Large Vision-Language Models (LVLMs).
However, existing training algorithms such as SFT, PPO, and GRPO may not
generalize well across unseen reasoning tasks and heavily rely on a biased
reward model. To address this challenge, we reformulate reasoning in LVLMs as
posterior inference and propose a scalable training algorithm based on
amortized variational inference. By leveraging diversity-seeking reinforcement
learning algorithms, we introduce a novel sparse reward function for
token-level learning signals that encourage diverse, high-likelihood latent
CoT, overcoming deterministic sampling limitations and avoiding reward hacking.
Additionally, we implement a Bayesian inference-scaling strategy that replaces
costly Best-of-N and Beam Search with a marginal likelihood to efficiently rank
optimal rationales and answers. We empirically demonstrate that the proposed
method enhances the state-of-the-art LVLMs on seven reasoning benchmarks, in
terms of effectiveness, generalization, and interpretability.

</details>


### [46] [Affordance Representation and Recognition for Autonomous Agents](https://arxiv.org/abs/2510.24459)
*Habtom Kahsay Gidey,Niklas Huber,Alexander Lenz,Alois Knoll*

Main category: cs.AI

TL;DR: 提出两种架构模式：DOM转换模式用于简化网页DOM为紧凑的任务相关表示，超媒体功能识别模式用于动态发现和集成未知Web服务的能力。


<details>
  <summary>Details</summary>
Motivation: 解决软件代理从结构化数据构建可操作内部世界模型的两个关键挑战：原始HTML的冗长性使其难以被基础模型直接使用，以及硬编码API集成的静态性阻碍代理适应不断演化的服务。

Method: 引入世界建模的模式语言，包含DOM转换模式和超媒体功能识别模式。DOM转换模式将冗长的原始DOM提炼为紧凑的任务相关表示；超媒体功能识别模式通过解析标准化语义描述来动态发现和集成Web服务能力。

Result: 这两种模式共同提供了一个稳健的框架，使代理能够高效构建和维护准确的世界模型。

Conclusion: 该模式语言为工程化代理提供了可扩展、自适应和可互操作的自动化能力，使其能够在Web及其扩展资源上有效运作。

Abstract: The autonomy of software agents is fundamentally dependent on their ability
to construct an actionable internal world model from the structured data that
defines their digital environment, such as the Document Object Model (DOM) of
web pages and the semantic descriptions of web services. However, constructing
this world model from raw structured data presents two critical challenges: the
verbosity of raw HTML makes it computationally intractable for direct use by
foundation models, while the static nature of hardcoded API integrations
prevents agents from adapting to evolving services.
  This paper introduces a pattern language for world modeling from structured
data, presenting two complementary architectural patterns. The DOM Transduction
Pattern addresses the challenge of web page complexity by distilling} a
verbose, raw DOM into a compact, task-relevant representation or world model
optimized for an agent's reasoning core. Concurrently, the Hypermedia
Affordances Recognition Pattern enables the agent to dynamically enrich its
world model by parsing standardized semantic descriptions to discover and
integrate the capabilities of unknown web services at runtime. Together, these
patterns provide a robust framework for engineering agents that can efficiently
construct and maintain an accurate world model, enabling scalable, adaptive,
and interoperable automation across the web and its extended resources.

</details>


### [47] [Decentralized Causal Discovery using Judo Calculus](https://arxiv.org/abs/2510.23942)
*Sridhar Mahadevan*

Main category: cs.AI

TL;DR: 提出了一种基于直觉主义分散式框架的因果发现理论——judo演算，使用j-稳定因果推断和j-do演算在层拓扑中形式化处理因果关系的上下文依赖性。


<details>
  <summary>Details</summary>
Motivation: 在现实应用中（从生物学到医学和社会科学），因果效应依赖于具体情境（如年龄、国家、剂量、基因型或实验协议），需要形式化处理这种上下文依赖性。

Method: 使用judo演算，结合Lawvere-Tierney模态算子j选择相关情境，将因果声明形式化为在情境覆盖上的局部真值。结合标准的基于分数、约束和梯度的因果发现方法。

Result: 实验结果显示，基于层理论的分散式因果发现在计算效率上优于经典方法，并在合成和真实数据集（生物学和经济学）上表现出改进的性能。

Conclusion: judo演算提供了一种构造性和一致性的框架来处理因果关系的上下文依赖性，通过分散式方法提高了因果发现的效率和性能。

Abstract: We describe a theory and implementation of an intuitionistic decentralized
framework for causal discovery using judo calculus, which is formally defined
as j-stable causal inference using j-do-calculus in a topos of sheaves. In
real-world applications -- from biology to medicine and social science --
causal effects depend on regime (age, country, dose, genotype, or lab
protocol). Our proposed judo calculus formalizes this context dependence
formally as local truth: a causal claim is proven true on a cover of regimes,
not everywhere at once. The Lawvere-Tierney modal operator j chooses which
regimes are relevant; j-stability means the claim holds constructively and
consistently across that family. We describe an algorithmic and implementation
framework for judo calculus, combining it with standard score-based,
constraint-based, and gradient-based causal discovery methods. We describe
experimental results on a range of domains, from synthetic to real-world
datasets from biology and economics. Our experimental results show the
computational efficiency gained by the decentralized nature of sheaf-theoretic
causal discovery, as well as improved performance over classical causal
discovery methods.

</details>


### [48] [The Sign Estimator: LLM Alignment in the Face of Choice Heterogeneity](https://arxiv.org/abs/2510.23965)
*Aymane El Gadarri,Ali Aouad,Vivek F. Farias*

Main category: cs.AI

TL;DR: 提出了一种名为sign estimator的新方法，通过用二元分类损失替换交叉熵损失，解决了传统LLM对齐方法在人类偏好异质性下的不一致性问题。


<details>
  <summary>Details</summary>
Motivation: 传统LLM对齐方法在面对人类偏好异质性时存在脆弱性，拟合简单的概率模型会导致对群体平均效用的不一致估计。

Method: 使用sign estimator方法，在聚合步骤中用二元分类损失替代交叉熵损失，在温和假设下实现一致的序数对齐。

Result: 在数字孪生的LLM对齐模拟中，sign estimator显著减少了偏好扭曲，将估计误差降低了近35%，与真实群体偏好的不一致从12%降至8%。

Conclusion: 该方法在保持现有LLM对齐管道实现简单性的同时，优于显式建模用户异质性的面板数据启发式方法，提供了可证明的一致且高效的估计器。

Abstract: Traditional LLM alignment methods are vulnerable to heterogeneity in human
preferences. Fitting a na\"ive probabilistic model to pairwise comparison data
(say over prompt-completion pairs) yields an inconsistent estimate of the
population-average utility -a canonical measure of social welfare. We propose a
new method, dubbed the sign estimator, that provides a simple, provably
consistent, and efficient estimator by replacing cross-entropy with binary
classification loss in the aggregation step. This simple modification recovers
consistent ordinal alignment under mild assumptions and achieves the first
polynomial finite-sample error bounds in this setting. In realistic simulations
of LLM alignment using digital twins, the sign estimator substantially reduces
preference distortion over a panel of simulated personas, cutting (angular)
estimation error by nearly 35% and decreasing disagreement with true population
preferences from 12% to 8% compared to standard RLHF. Our method also compares
favorably to panel data heuristics that explicitly model user heterogeneity and
require tracking individual-level preference data-all while maintaining the
implementation simplicity of existing LLM alignment pipelines.

</details>


### [49] [Learning Individual Movement Shifts After Urban Disruptions with Social Infrastructure Reliance](https://arxiv.org/abs/2510.23989)
*Shangde Gao,Zelin Xu,Zhe Jiang*

Main category: cs.AI

TL;DR: 该研究提出了一种结合个体社会基础设施韧性(SIR)的条件深度学习模型，用于预测破坏性事件后个体移动模式的变化。


<details>
  <summary>Details</summary>
Motivation: 预测破坏性事件前个体移动模式变化具有挑战性，因为缺乏衡量个体异质性社会基础设施韧性的方法，传统特征有限，且个体移动与空间环境的复杂交互未被充分捕捉。

Method: 将个体的SIR整合到条件深度学习模型中，利用大规模稀疏个体级数据捕捉个体移动模式与局部空间环境的复杂关系。

Result: 实验表明，结合个体SIR和空间环境能增强模型预测事件后个体移动模式的能力，该模型能捕捉具有相似事件前模式但SIR不同的个体在移动模式上的差异变化。

Conclusion: 该条件模型能够有效预测个体在破坏性事件后的移动模式变化，特别是能区分具有相似历史模式但社会基础设施韧性不同的个体的移动行为差异。

Abstract: Shifts in individual movement patterns following disruptive events can reveal
changing demands for community resources. However, predicting such shifts
before disruptive events remains challenging for several reasons. First,
measures are lacking for individuals' heterogeneous social infrastructure
resilience (SIR), which directly influences their movement patterns, and
commonly used features are often limited or unavailable at scale, e.g.,
sociodemographic characteristics. Second, the complex interactions between
individual movement patterns and spatial contexts have not been sufficiently
captured. Third, individual-level movement may be spatially sparse and not
well-suited to traditional decision-making methods for movement predictions.
This study incorporates individuals' SIR into a conditioned deep learning model
to capture the complex relationships between individual movement patterns and
local spatial context using large-scale, sparse individual-level data. Our
experiments demonstrate that incorporating individuals' SIR and spatial context
can enhance the model's ability to predict post-event individual movement
patterns. The conditioned model can capture the divergent shifts in movement
patterns among individuals who exhibit similar pre-event patterns but differ in
SIR.

</details>


### [50] [LLMLogAnalyzer: A Clustering-Based Log Analysis Chatbot using Large Language Models](https://arxiv.org/abs/2510.24031)
*Peng Cai,Reza Ryan,Nickson M. Karie*

Main category: cs.AI

TL;DR: LLMLogAnalyzer是一个基于聚类和大型语言模型的日志分析聊天机器人，通过结合LLM和机器学习算法简化日志分析流程，在多个任务上比现有LLM聊天机器人性能提升39%-68%。


<details>
  <summary>Details</summary>
Motivation: 系统日志是网络安全的核心，但分析大量多样化日志数据面临高成本、缺乏专业知识和时间限制等挑战，许多组织难以进行基本分析。

Method: 采用模块化架构，包括路由器、日志识别器、日志解析器和搜索工具，结合聚类算法和LLM技术，解决了LLM的上下文窗口限制和结构化文本处理能力差的问题。

Result: 在四个不同领域日志和多种任务上的评估显示，相比ChatGPT、ChatPDF和NotebookLM等最先进的LLM聊天机器人，性能提升39%-68%，鲁棒性显著增强，ROUGE-1分数的四分位距减少93%。

Conclusion: 该框架通过增强LLM在结构化文本分析方面的能力，提高了准确性和鲁棒性，为网络安全专家和非技术用户提供了有价值的工具。

Abstract: System logs are a cornerstone of cybersecurity, supporting proactive breach
prevention and post-incident investigations. However, analyzing vast amounts of
diverse log data remains significantly challenging, as high costs, lack of
in-house expertise, and time constraints make even basic analysis difficult for
many organizations. This study introduces LLMLogAnalyzer, a clustering-based
log analysis chatbot that leverages Large Language Models (LLMs) and Machine
Learning (ML) algorithms to simplify and streamline log analysis processes.
This innovative approach addresses key LLM limitations, including context
window constraints and poor structured text handling capabilities, enabling
more effective summarization, pattern extraction, and anomaly detection tasks.
LLMLogAnalyzer is evaluated across four distinct domain logs and various tasks.
Results demonstrate significant performance improvements over state-of-the-art
LLM-based chatbots, including ChatGPT, ChatPDF, and NotebookLM, with consistent
gains ranging from 39% to 68% across different tasks. The system also exhibits
strong robustness, achieving a 93% reduction in interquartile range (IQR) when
using ROUGE-1 scores, indicating significantly lower result variability. The
framework's effectiveness stems from its modular architecture comprising a
router, log recognizer, log parser, and search tools. This design enhances LLM
capabilities for structured text analysis while improving accuracy and
robustness, making it a valuable resource for both cybersecurity experts and
non-technical users.

</details>


### [51] [Discovering Heuristics with Large Language Models (LLMs) for Mixed-Integer Programs: Single-Machine Scheduling](https://arxiv.org/abs/2510.24013)
*İbrahim Oğuz Çetinkaya,İ. Esra Büyüktahtakın,Parshin Shojaee,Chandan K. Reddy*

Main category: cs.AI

TL;DR: 利用大型语言模型(LLMs)发现新的启发式算法来解决单机总延迟问题，提出了EDDC和MDDC两种算法，在大型实例上表现出色。


<details>
  <summary>Details</summary>
Motivation: 传统启发式算法在单机总延迟问题上表现有限，而精确方法在大规模问题上计算不可行，需要开发新的高效启发式算法。

Method: 通过人类-LLM协作发现新的启发式算法EDDC和MDDC，基于经典的EDD和MDD规则，并与混合整数规划和动态规划等精确方法进行基准测试。

Result: 对于超过100个作业的实例，EDDC改进了经典EDD规则，MDDC始终优于传统启发式算法，在大型复杂实例上与精确方法竞争。

Conclusion: 人类-LLM协作可以有效产生可扩展的高性能启发式算法，用于NP难的组合优化问题，即使在资源有限的情况下。

Abstract: Our study contributes to the scheduling and combinatorial optimization
literature with new heuristics discovered by leveraging the power of Large
Language Models (LLMs). We focus on the single-machine total tardiness (SMTT)
problem, which aims to minimize total tardiness by sequencing n jobs on a
single processor without preemption, given processing times and due dates. We
develop and benchmark two novel LLM-discovered heuristics, the EDD Challenger
(EDDC) and MDD Challenger (MDDC), inspired by the well-known Earliest Due Date
(EDD) and Modified Due Date (MDD) rules. In contrast to prior studies that
employed simpler rule-based heuristics, we evaluate our LLM-discovered
algorithms using rigorous criteria, including optimality gaps and solution time
derived from a mixed-integer programming (MIP) formulation of SMTT. We compare
their performance against state-of-the-art heuristics and exact methods across
various job sizes (20, 100, 200, and 500 jobs). For instances with more than
100 jobs, exact methods such as MIP and dynamic programming become
computationally intractable. Up to 500 jobs, EDDC improves upon the classic EDD
rule and another widely used algorithm in the literature. MDDC consistently
outperforms traditional heuristics and remains competitive with exact
approaches, particularly on larger and more complex instances. This study shows
that human-LLM collaboration can produce scalable, high-performing heuristics
for NP-hard constrained combinatorial optimization, even under limited
resources when effectively configured.

</details>


### [52] [OneCast: Structured Decomposition and Modular Generation for Cross-Domain Time Series Forecasting](https://arxiv.org/abs/2510.24028)
*Tingyue Pan,Mingyue Cheng,Shilong Zhang,Zhiding Liu,Xiaoyu Tao,Yucong Luo,Jintao Zhang,Qi Liu*

Main category: cs.AI

TL;DR: OneCast是一个跨域时间序列预测框架，通过将时间序列分解为季节性和趋势分量，分别使用轻量级投影模块和基于扩散的标记化机制进行建模，实现更好的跨域泛化性能。


<details>
  <summary>Details</summary>
Motivation: 现有跨域时间序列预测方法在处理异构数据时面临挑战，特别是在面对域特定趋势变化和不一致周期性模式时表现不佳，主要问题在于将时间序列视为未分化的序列而未显式解耦其固有结构组件。

Method: 提出OneCast框架：1）将时间序列分解为季节性和趋势分量；2）季节性分量通过轻量级投影模块使用可解释基函数重建周期性模式；3）趋势分量通过语义感知标记器在分段级别编码为离散标记，并通过掩码离散扩散机制推断；4）两个分支输出结合生成最终预测。

Result: 在八个领域上的广泛实验表明，OneCast在大多数情况下优于最先进的基线方法。

Conclusion: 通过显式解耦时间序列的结构组件并采用模块化建模方法，OneCast能够有效捕捉季节性模式同时跟踪域特定趋势，在跨域时间序列预测任务中表现出色。

Abstract: Cross-domain time series forecasting is a valuable task in various web
applications. Despite its rapid advancement, achieving effective generalization
across heterogeneous time series data remains a significant challenge. Existing
methods have made progress by extending single-domain models, yet often fall
short when facing domain-specific trend shifts and inconsistent periodic
patterns. We argue that a key limitation lies in treating temporal series as
undifferentiated sequence, without explicitly decoupling their inherent
structural components. To address this, we propose OneCast, a structured and
modular forecasting framework that decomposes time series into seasonal and
trend components, each modeled through tailored generative pathways.
Specifically, the seasonal component is captured by a lightweight projection
module that reconstructs periodic patterns via interpretable basis functions.
In parallel, the trend component is encoded into discrete tokens at segment
level via a semantic-aware tokenizer, and subsequently inferred through a
masked discrete diffusion mechanism. The outputs from both branches are
combined to produce a final forecast that captures seasonal patterns while
tracking domain-specific trends. Extensive experiments across eight domains
demonstrate that OneCast mostly outperforms state-of-the-art baselines.

</details>


### [53] [Modeling Electric Vehicle Car-Following Behavior: Classical vs Machine Learning Approach](https://arxiv.org/abs/2510.24085)
*Md. Shihab Uddin,Md Nazmus Shakib,Rahul Bhadani*

Main category: cs.AI

TL;DR: 比较经典模型和机器学习模型在电动汽车跟车行为建模中的表现，发现随机森林模型在所有场景下都优于物理模型，特别是在不同跟车间距条件下表现最佳。


<details>
  <summary>Details</summary>
Motivation: 随着电动汽车的普及，需要理解其驾驶行为以提升交通安全和开发智能驾驶系统，特别是在与传统内燃机车辆混合的交通环境中。

Method: 使用真实世界数据集，比较了IDM、OVM、OVRV和简化CACC等经典物理模型与随机森林回归器。通过最小化预测值与实际数据的RMSE来校准经典模型参数，随机森林使用间距、速度和间隙类型作为输入来预测加速度。

Result: 随机森林模型在所有场景下表现最优，RMSE分别为0.0046（中等间距）、0.0016（长间距）和0.0025（超长间距）。在物理模型中，CACC表现最好，长间距的RMSE为2.67。

Conclusion: 机器学习模型在模拟电动汽车行为和分析混合自动驾驶交通动态方面具有重要价值，特别是在电动汽车集成环境中。

Abstract: The increasing adoption of electric vehicles (EVs) necessitates an
understanding of their driving behavior to enhance traffic safety and develop
smart driving systems. This study compares classical and machine learning
models for EV car following behavior. Classical models include the Intelligent
Driver Model (IDM), Optimum Velocity Model (OVM), Optimal Velocity Relative
Velocity (OVRV), and a simplified CACC model, while the machine learning
approach employs a Random Forest Regressor. Using a real world dataset of an EV
following an internal combustion engine (ICE) vehicle under varied driving
conditions, we calibrated classical model parameters by minimizing the RMSE
between predictions and real data. The Random Forest model predicts
acceleration using spacing, speed, and gap type as inputs. Results demonstrate
the Random Forest's superior accuracy, achieving RMSEs of 0.0046 (medium gap),
0.0016 (long gap), and 0.0025 (extra long gap). Among physics based models,
CACC performed best, with an RMSE of 2.67 for long gaps. These findings
highlight the machine learning model's performance across all scenarios. Such
models are valuable for simulating EV behavior and analyzing mixed autonomy
traffic dynamics in EV integrated environments.

</details>


### [54] [HistoLens: An Interactive XAI Toolkit for Verifying and Mitigating Flaws in Vision-Language Models for Histopathology](https://arxiv.org/abs/2510.24115)
*Sandeep Vissapragada,Vikrant Sahu,Gagan Raj Gupta,Vandita Singh*

Main category: cs.AI

TL;DR: 开发了HistoLens系统，让病理学家能用自然语言提问组织切片问题，AI提供结构化报告和可视化证据，确保医生保持主导地位的同时获得可信的AI辅助诊断。


<details>
  <summary>Details</summary>
Motivation: 为了让医生真正信任人工智能，需要解决AI黑盒问题，让医生能够理解AI的推理过程，就像咨询同事一样透明可信。

Method: 创建HistoLens系统，将病理学家的自然语言问题转换为精确的AI查询，生成结构化报告并提供可视化热图证据，同时训练AI专注于患者组织而忽略背景噪音。

Result: 实现了透明协作的工作流程，病理学家保持专家主导地位，使用可信的AI助手验证见解，做出更快更自信的诊断。

Conclusion: HistoLens通过提供透明推理和可视化证据，建立了医生与AI之间的信任关系，使AI成为可靠的诊断助手而非黑盒工具。

Abstract: For doctors to truly trust artificial intelligence, it can't be a black box.
They need to understand its reasoning, almost as if they were consulting a
colleague. We created HistoLens1 to be that transparent, collaborative partner.
It allows a pathologist to simply ask a question in plain English about a
tissue slide--just as they would ask a trainee. Our system intelligently
translates this question into a precise query for its AI engine, which then
provides a clear, structured report. But it doesn't stop there. If a doctor
ever asks, "Why?", HistoLens can instantly provide a 'visual proof' for any
finding--a heatmap that points to the exact cells and regions the AI used for
its analysis. We've also ensured the AI focuses only on the patient's tissue,
just like a trained pathologist would, by teaching it to ignore distracting
background noise. The result is a workflow where the pathologist remains the
expert in charge, using a trustworthy AI assistant to verify their insights and
make faster, more confident diagnoses.

</details>


### [55] [From Observability Data to Diagnosis: An Evolving Multi-agent System for Incident Management in Cloud Systems](https://arxiv.org/abs/2510.24145)
*Yu Luo,Jiamin Jiang,Jingfei Feng,Lei Tao,Qingliang Zhang,Xidao Wen,Yongqian Sun,Shenglin Zhang,Jielong Huang,Nan Qi,Dan Pei*

Main category: cs.AI

TL;DR: OpsAgent是一个轻量级、自演化的多智能体系统，用于云系统事件管理，通过免训练数据处理和透明诊断推理，在OPENRCA基准测试中表现出最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 手动事件管理劳动密集且容易出错，现有自动化方法难以跨系统泛化、可解释性有限且部署成本高，阻碍了实际应用。

Method: 使用免训练数据处理器将异构可观测数据转换为结构化文本描述，采用多智能体协作框架实现透明诊断推理，并引入双自演化机制支持持续能力增长。

Result: 在OPENRCA基准测试中达到最先进性能，证明OpsAgent具有可泛化性、可解释性、成本效益和自演化特性。

Conclusion: OpsAgent是一个实际可部署且可持续的解决方案，适用于真实云系统的长期运维。

Abstract: Incident management (IM) is central to the reliability of large-scale cloud
systems. Yet manual IM, where on-call engineers examine metrics, logs, and
traces is labor-intensive and error-prone in the face of massive and
heterogeneous observability data. Existing automated IM approaches often
struggle to generalize across systems, provide limited interpretability, and
incur high deployment costs, which hinders adoption in practice. In this paper,
we present OpsAgent, a lightweight, self-evolving multi-agent system for IM
that employs a training-free data processor to convert heterogeneous
observability data into structured textual descriptions, along with a
multi-agent collaboration framework that makes diagnostic inference transparent
and auditable. To support continual capability growth, OpsAgent also introduces
a dual self-evolution mechanism that integrates internal model updates with
external experience accumulation, thereby closing the deployment loop.
Comprehensive experiments on the OPENRCA benchmark demonstrate state-of-the-art
performance and show that OpsAgent is generalizable, interpretable,
cost-efficient, and self-evolving, making it a practically deployable and
sustainable solution for long-term operation in real-world cloud systems.

</details>


### [56] [BMGQ: A Bottom-up Method for Generating Complex Multi-hop Reasoning Questions from Semi-structured Data](https://arxiv.org/abs/2510.24151)
*Bingsen Qiu,Zijian Liu,Xiao Liu,Haoshen Yang,Zeren Gao,Bingjie Wang,Feier Zhang,Yixuan Qin,Chunyan Li*

Main category: cs.AI

TL;DR: 提出了一个自动化框架，用于从半结构化知识源生成高难度、可用于训练的多跳问答问题，解决了现有数据集稀缺且不适合监督微调或强化学习的问题。


<details>
  <summary>Details</summary>
Motivation: 当前多跳问答数据集资源稀缺，大多仅用于评估而非训练，且人工构建非平凡可检索问题成本高昂，无法规模化，这成为训练高能力检索推理智能体的关键数据瓶颈。

Method: 系统通过NLI关系类型化和多样性感知扩展构建证据簇，应用反向问题构建来组合间接线索，并通过多模型共识过滤与结构化约束分解的两步评估流程确保质量。

Result: 该框架能够规模化生成复杂、检索抵抗但可验证的问题，适用于SFT/RL训练和挑战性评估，大幅减少人工标注工作同时保持强评估基准的难度特征。

Conclusion: 该自动化框架有效解决了多跳问答训练数据稀缺的问题，为训练高能力检索推理智能体提供了可行的解决方案。

Abstract: Building training-ready multi-hop question answering (QA) datasets that truly
stress a model's retrieval and reasoning abilities remains highly challenging
recently. While there have been a few recent evaluation datasets that capture
the characteristics of hard-to-search but easy-to-verify problems -- requiring
the integration of ambiguous, indirect, and cross-domain cues -- these data
resources remain scarce and are mostly designed for evaluation, making them
unsuitable for supervised fine-tuning (SFT) or reinforcement learning (RL).
Meanwhile, manually curating non-trivially retrievable questions -- where
answers cannot be found through a single direct query but instead require
multi-hop reasoning over oblique and loosely connected evidence -- incurs
prohibitive human costs and fails to scale, creating a critical data bottleneck
for training high-capability retrieval-and-reasoning agents.
  To address this, we present an automated framework for generating
high-difficulty, training-ready multi-hop questions from semi-structured
knowledge sources. The system (i) grows diverse, logically labeled evidence
clusters through Natural Language Inference (NLI)-based relation typing and
diversity-aware expansion; (ii) applies reverse question construction to
compose oblique cues so that isolated signals are underinformative but their
combination uniquely identifies the target entity; and (iii) enforces quality
with a two-step evaluation pipeline that combines multi-model consensus
filtering with structured constraint decomposition and evidence-based matching.
The result is a scalable process that yields complex, retrieval-resistant yet
verifiable questions suitable for SFT/RL training as well as challenging
evaluation, substantially reducing human curation effort while preserving the
difficulty profile of strong evaluation benchmarks.

</details>


### [57] [BLM$_1$: A Boundless Large Model for Cross-Space, Cross-Task, and Cross-Embodiment Learning](https://arxiv.org/abs/2510.24161)
*Wentao Tan,Bowen Wang,Heng Zhi,Chenyu Liu,Zhe Li,Jian Liu,Zengrong Lin,Yukun Dai,Yipeng Chen,Wenjie Yang,Enci Xie,Hao Xue,Baixu Ji,Chen Xu,Zhibin Wang,Tianshi Wang,Lei Zhu,Heng Tao Shen*

Main category: cs.AI

TL;DR: BLM₁是一个多模态空间基础模型，通过两阶段训练实现跨空间传输、跨任务学习和跨具身泛化，在数字和物理任务中均优于现有模型。


<details>
  <summary>Details</summary>
Motivation: 现有MLLMs在数字-物理空间和具身系统间泛化能力差，VLAs缺乏高级推理能力，ELLMs局限于数字空间，需要统一模型实现跨空间和具身的无缝操作。

Method: 两阶段训练：阶段I通过数字语料注入具身知识并保持语言能力；阶段II通过意图桥接接口训练策略模块，提取MLLM的高级语义指导控制，无需微调MLLM主干。

Result: 单个BLM₁实例在数字任务中提升约6%，在物理任务中提升约3%，优于MLLMs、ELLMs、VLAs和GMLMs四个模型家族。

Conclusion: BLM₁成功实现了跨空间、跨任务和跨具身的统一建模，为具身智能提供了有效的解决方案。

Abstract: Multimodal large language models (MLLMs) have advanced vision-language
reasoning and are increasingly deployed in embodied agents. However,
significant limitations remain: MLLMs generalize poorly across digital-physical
spaces and embodiments; vision-language-action models (VLAs) produce low-level
actions yet lack robust high-level embodied reasoning; and most embodied large
language models (ELLMs) are constrained to digital-space with poor
generalization to the physical world. Thus, unified models that operate
seamlessly across digital and physical spaces while generalizing across
embodiments and tasks remain absent. We introduce the \textbf{Boundless Large
Model (BLM$_1$)}, a multimodal spatial foundation model that preserves
instruction following and reasoning, incorporates embodied knowledge, and
supports robust cross-embodiment control. BLM$_1$ integrates three key
capabilities -- \textit{cross-space transfer, cross-task learning, and
cross-embodiment generalization} -- via a two-stage training paradigm. Stage I
injects embodied knowledge into the MLLM through curated digital corpora while
maintaining language competence. Stage II trains a policy module through an
intent-bridging interface that extracts high-level semantics from the MLLM to
guide control, without fine-tuning the MLLM backbone. This process is supported
by a self-collected cross-embodiment demonstration suite spanning four robot
embodiments and six progressively challenging tasks. Evaluations across digital
and physical benchmarks show that a single BLM$_1$ instance outperforms four
model families -- MLLMs, ELLMs, VLAs, and GMLMs -- achieving
$\sim\!\textbf{6%}$ gains in digital tasks and $\sim\!\textbf{3%}$ in physical
tasks.

</details>


### [58] [UniPlanner: A Unified Motion Planning Framework for Autonomous Vehicle Decision-Making Systems via Multi-Dataset Integration](https://arxiv.org/abs/2510.24166)
*Xin Yang,Yuhang Zhang,Wei Li,Xin Lin,Wenbin Zou,Chen Xu*

Main category: cs.AI

TL;DR: UniPlanner是一个用于自动驾驶决策的多数据集集成规划框架，通过历史-未来轨迹字典网络、梯度自由轨迹映射器和稀疏到密集范式实现跨数据集统一学习。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习方法局限于单数据集训练，限制了规划鲁棒性。研究发现不同数据集的车辆轨迹分布和历史-未来相关性具有显著一致性，这为多数据集集成提供了基础。

Method: 1. HFTDN：聚合多数据集的历史-未来轨迹对，基于历史轨迹相似性检索相关未来轨迹生成跨数据集规划指导；2. GFTM：从多数据集学习鲁棒的历史-未来相关性，将历史轨迹转换为通用规划先验；3. S2D范式：训练时选择性抑制规划先验实现鲁棒学习，推理时充分利用先验最大化规划性能。

Result: UniPlanner实现了跨数据集统一学习，提升了规划鲁棒性和性能。

Conclusion: UniPlanner是首个面向自动驾驶决策的多数据集集成规划框架，通过三个协同创新解决了单数据集训练的局限性，为自动驾驶规划提供了更鲁棒的解决方案。

Abstract: Motion planning is a critical component of autonomous vehicle decision-making
systems, directly determining trajectory safety and driving efficiency. While
deep learning approaches have advanced planning capabilities, existing methods
remain confined to single-dataset training, limiting their robustness in
planning.
  Through systematic analysis, we discover that vehicular trajectory
distributions and history-future correlations demonstrate remarkable
consistency across different datasets. Based on these findings, we propose
UniPlanner, the first planning framework designed for multi-dataset integration
in autonomous vehicle decision-making. UniPlanner achieves unified
cross-dataset learning through three synergistic innovations.
  First, the History-Future Trajectory Dictionary Network (HFTDN) aggregates
history-future trajectory pairs from multiple datasets, using historical
trajectory similarity to retrieve relevant futures and generate cross-dataset
planning guidance.
  Second, the Gradient-Free Trajectory Mapper (GFTM) learns robust
history-future correlations from multiple datasets, transforming historical
trajectories into universal planning priors. Its gradient-free design ensures
the introduction of valuable priors while preventing shortcut learning, making
the planning knowledge safely transferable. Third, the Sparse-to-Dense (S2D)
paradigm implements adaptive dropout to selectively suppress planning priors
during training for robust learning, while enabling full prior utilization
during inference to maximize planning performance.

</details>


### [59] [MGA: Memory-Driven GUI Agent for Observation-Centric Interaction](https://arxiv.org/abs/2510.24168)
*Weihua Cheng,Ersheng Ni,Wenlong Wang,Yifei Sun,Junming Liu,Wangyu Shen,Yirong Chen,Botian Shi,Ding Wang*

Main category: cs.AI

TL;DR: 提出了记忆驱动的GUI代理(MGA)，采用“先观察后决策”原则解决现有GUI代理的两个主要问题：历史轨迹依赖和局部探索偏差，在多个基准测试中表现出更好的鲁棒性、泛化性和效率。


<details>
  <summary>Details</summary>
Motivation: 现有GUI代理存在两个主要问题：依赖历史轨迹导致错误传播放大，以及“先决策后观察”机制忽视关键界面线索。需要开发更鲁棒的GUI交互方法。

Method: MGA将GUI交互重构为“先观察后决策”原则，每个步骤建模为独立的环境状态三元组：当前截图、任务无关的空间信息和动态更新的结构化记忆。

Result: 在OSworld基准测试、真实桌面应用(Chrome、VSCode、VLC)和跨任务迁移实验中，MGA相比最先进基线方法在鲁棒性、泛化性和效率方面取得显著提升。

Conclusion: MGA通过观察优先的方法和结构化记忆机制，有效解决了现有GUI代理的关键限制，为GUI交互提供了更鲁棒和通用的解决方案。

Abstract: The rapid progress of Large Language Models (LLMs) and their multimodal
extensions (MLLMs) has enabled agentic systems capable of perceiving and acting
across diverse environments. A challenging yet impactful frontier is the
development of GUI agents, which must navigate complex desktop and web
interfaces while maintaining robustness and generalization. Existing paradigms
typically model tasks as long-chain executions, concatenating historical
trajectories into the context. While approaches such as Mirage and GTA1 refine
planning or introduce multi-branch action selection, they remain constrained by
two persistent issues: Dependence on historical trajectories, which amplifies
error propagation. And Local exploration bias, where "decision-first,
observation-later" mechanisms overlook critical interface cues. We introduce
the Memory-Driven GUI Agent (MGA), which reframes GUI interaction around the
principle of observe first, then decide. MGA models each step as an
independent, context-rich environment state represented by a triad: current
screenshot, task-agnostic spatial information, and a dynamically updated
structured memory. Experiments on OSworld benchmarks, real desktop applications
(Chrome, VSCode, VLC), and cross-task transfer demonstrate that MGA achieves
substantial gains in robustness, generalization, and efficiency compared to
state-of-the-art baselines. The code is publicly available at:
{https://anonymous.4open.science/r/MGA-3571}.

</details>


### [60] [MCP-Flow: Facilitating LLM Agents to Master Real-World, Diverse and Scaling MCP Tools](https://arxiv.org/abs/2510.24284)
*Wenhao Wang,Peizhi Niu,Zhao Xu,Zhaoyu Chen,Jian Du,Yaxin Du,Xianghe Pang,Keduan Huang,Yanfeng Wang,Qiang Yan,Siheng Chen*

Main category: cs.AI

TL;DR: MCP-Flow是一个自动化web-agent驱动的流水线，用于大规模服务器发现、数据合成和模型训练，显著提升了LLM在MCP生态系统中的工具使用能力。


<details>
  <summary>Details</summary>
Motivation: 现有MCP研究覆盖服务器少、依赖昂贵人工整理且缺乏训练支持，阻碍了LLM在真实世界MCP环境中的部署。

Method: 通过自动化web-agent驱动流水线，从1166个服务器和11536个工具中收集筛选数据，生成68733个高质量指令-函数调用对和6439条轨迹。

Result: 实验证明MCP-Flow在MCP工具选择、函数调用生成和代理任务性能方面表现优异，远超先前工作。

Conclusion: MCP-Flow为提升LLM代理在真实世界MCP环境中的熟练度提供了可扩展的基础。

Abstract: Large Language Models (LLMs) increasingly rely on external tools to perform
complex, realistic tasks, yet their ability to utilize the rapidly expanding
Model Contextual Protocol (MCP) ecosystem remains limited. Existing MCP
research covers few servers, depends on costly manual curation, and lacks
training support, hindering progress toward real-world deployment. To overcome
these limitations, we introduce MCP-Flow, an automated web-agent-driven
pipeline for large-scale server discovery, data synthesis, and model training.
MCP-Flow collects and filters data from 1166 servers and 11536 tools, producing
68733 high-quality instruction-function call pairs and 6439 trajectories, far
exceeding prior work in scale and diversity. Extensive experiments demonstrate
MCP-Flow's effectiveness in driving superior MCP tool selection, function-call
generation, and enhanced agentic task performance. MCP-Flow thus provides a
scalable foundation for advancing LLM agents' proficiency in real-world MCP
environments. MCP-Flow is publicly available at
\href{https://github.com/wwh0411/MCP-Flow}{https://github.com/wwh0411/MCP-Flow}.

</details>


### [61] [Investigating Intra-Abstraction Policies For Non-exact Abstraction Algorithms](https://arxiv.org/abs/2510.24297)
*Robin Schmöcker,Alexander Dockhorn,Bodo Rosenhahn*

Main category: cs.AI

TL;DR: 本文提出了几种替代随机策略的抽象内部策略，用于解决MCTS中当多个动作属于同一抽象节点时UCB值相同的问题，多数策略在多个环境和参数设置下表现优于随机策略。


<details>
  <summary>Details</summary>
Motivation: MCTS的样本效率问题可以通过状态/动作抽象来解决，但现有方法如pruned OGA在多个动作属于同一抽象节点时UCB值相同，只能使用随机打破平局规则，这限制了性能。

Method: 提出并实证评估了多种替代随机策略的抽象内部策略，用于处理同一抽象节点内多个动作的选择问题。

Result: 实验结果表明，提出的多种策略在大多数环境和参数设置下都优于随机策略。

Conclusion: 通过设计更智能的抽象内部策略，可以显著提升MCTS在抽象环境中的性能，解决了现有方法中隐含的随机打破平局问题。

Abstract: One weakness of Monte Carlo Tree Search (MCTS) is its sample efficiency which
can be addressed by building and using state and/or action abstractions in
parallel to the tree search such that information can be shared among nodes of
the same layer. The primary usage of abstractions for MCTS is to enhance the
Upper Confidence Bound (UCB) value during the tree policy by aggregating visits
and returns of an abstract node. However, this direct usage of abstractions
does not take the case into account where multiple actions with the same parent
might be in the same abstract node, as these would then all have the same UCB
value, thus requiring a tiebreak rule. In state-of-the-art abstraction
algorithms such as pruned On the Go Abstractions (pruned OGA), this case has
not been noticed, and a random tiebreak rule was implicitly chosen. In this
paper, we propose and empirically evaluate several alternative
intra-abstraction policies, several of which outperform the random policy
across a majority of environments and parameter settings.

</details>


### [62] [Verifying Large Language Models' Reasoning Paths via Correlation Matrix Rank](https://arxiv.org/abs/2510.24299)
*Jiayu Liu,Wei Dai,Zhenya Huang,Ning Miao,Enhong Chen*

Main category: cs.AI

TL;DR: 提出了一种基于LLM内部行为的相关矩阵秩作为推理路径可信度指标的自指示方法，无需外部资源即可有效验证LLM推理的正确性。


<details>
  <summary>Details</summary>
Motivation: 现有验证方法依赖外部资源（如训练验证器或复杂提示），导致计算开销大且仅适用于特定领域，需要一种更高效通用的LLM输出验证方法。

Method: 利用输入问题与输出推理路径之间的相关矩阵秩作为推理正确性的稳健指标，设计了一个即插即用的自指示方法来重新加权候选推理路径。

Result: 在多个不同规模和家族的LLM上实验表明，该方法能以超过75%的准确率区分正确与错误推理路径，并在三个推理基准上将准确率提升超过8%。

Conclusion: LLM的内部行为已隐含其推理路径的可信度信息，基于相关矩阵秩的自指示方法能有效验证推理正确性且计算开销极低。

Abstract: Despite the strong reasoning ability of large language models~(LLMs), they
are prone to errors and hallucinations. As a result, how to check their outputs
effectively and efficiently has become a critical problem in their
applications. Existing checking methods heavily rely on external resources,
such as trained verifiers (e.g., process/outcome reward models) or elaborate
prompts, which lead to high computational overhead and are only applicable to
specific domains. In this paper, we investigate whether the internal behaviors
of LLMs have already implied the credibility of their reasoning paths.
Specifically, we find that the rank of the correlation matrix between the input
problem and the output reasoning path is a robust indicator of reasoning
correctness. Different from other correctness indicators for LLMs, the
calculation of the correlation matrix only relies on the LLM itself, which
avoids the hassle of training a separate model or designing complicated
prompts. Based on it, we design a simple, plug-and-play Self-Indicator method
to reweight candidate reasoning paths, which achieves significant performance
improvements than other voting and verification methods with very few
computational overhead. Our experiments across multiple LLMs of varying scales
and model families have further shown the effectiveness of Self-Indicator. It
achieves over 75% accuracy in distinguishing correct reasoning paths from
incorrect ones, and, in turn, improves the accuracies on three reasoning
benchmarks by more than 8%.

</details>


### [63] [Retrieval and Argumentation Enhanced Multi-Agent LLMs for Judgmental Forecasting](https://arxiv.org/abs/2510.24303)
*Deniz Gorur,Antoni Rago,Francesca Toni*

Main category: cs.AI

TL;DR: 提出了一种基于多智能体框架的论断验证方法，通过不同智能体对论断真实性进行辩论并生成证据，使用定量双极论证框架表示，实验表明多智能体组合能提高预测准确性。


<details>
  <summary>Details</summary>
Motivation: 将判断性预测视为论断验证任务，需要评估未来事件的可能性，传统方法缺乏多角度证据整合和可解释性。

Method: 设计多智能体框架，包括ArgLLM、RbAM和RAG-ArgLLM三种LLM驱动的智能体，通过定量双极论证框架表示支持和反对证据。

Result: 在标准判断性预测数据集上的实验显示，特别是三个智能体组合时能显著提高预测准确性，同时提供可解释的证据组合。

Conclusion: 多智能体框架能有效整合不同角度的证据，提高论断验证的准确性和可解释性，为判断性预测提供了新方法。

Abstract: Judgmental forecasting is the task of making predictions about future events
based on human judgment. This task can be seen as a form of claim verification,
where the claim corresponds to a future event and the task is to assess the
plausibility of that event. In this paper, we propose a novel multi-agent
framework for claim verification, whereby different agents may disagree on
claim veracity and bring specific evidence for and against the claims,
represented as quantitative bipolar argumentation frameworks (QBAFs). We then
instantiate the framework for supporting claim verification, with a variety of
agents realised with Large Language Models (LLMs): (1) ArgLLM agents, an
existing approach for claim verification that generates and evaluates QBAFs;
(2) RbAM agents, whereby LLM-empowered Relation-based Argument Mining (RbAM)
from external sources is used to generate QBAFs; (3) RAG-ArgLLM agents,
extending ArgLLM agents with a form of Retrieval-Augmented Generation (RAG) of
arguments from external sources. Finally, we conduct experiments with two
standard judgmental forecasting datasets, with instances of our framework with
two or three agents, empowered by six different base LLMs. We observe that
combining evidence from agents can improve forecasting accuracy, especially in
the case of three agents, while providing an explainable combination of
evidence for claim verification.

</details>


### [64] [Generative Large Language Models (gLLMs) in Content Analysis: A Practical Guide for Communication Research](https://arxiv.org/abs/2510.24337)
*Daria Kravets-Meinke,Hannah Schmid-Petri,Sonja Niemann,Ute Schmid*

Main category: cs.AI

TL;DR: 本文探讨了生成式大语言模型在传播学研究内容分析中的应用，提出了应对7个关键挑战的最佳实践指南。


<details>
  <summary>Details</summary>
Motivation: 尽管gLLMs在传播学内容分析中展现出巨大潜力，能够超越众包工作者和训练有素的编码员，但其在研究方法论中的整合仍然不足，需要系统指导。

Method: 综合新兴研究，提出包含代码本开发、提示工程、模型选择、参数调优、迭代优化、可靠性验证和性能提升的7个关键挑战应对框架。

Result: 建立了gLLM辅助定量内容分析的综合最佳实践指南，使该方法更易被传播学研究者采用。

Conclusion: 该指南旨在让基于gLLM的内容分析更易被广泛传播学研究者使用，并确保符合有效性、可靠性、可重复性和研究伦理等学科质量标准。

Abstract: Generative Large Language Models (gLLMs), such as ChatGPT, are increasingly
being used in communication research for content analysis. Studies show that
gLLMs can outperform both crowd workers and trained coders, such as research
assistants, on various coding tasks relevant to communication science, often at
a fraction of the time and cost. Additionally, gLLMs can decode implicit
meanings and contextual information, be instructed using natural language,
deployed with only basic programming skills, and require little to no annotated
data beyond a validation dataset - constituting a paradigm shift in automated
content analysis. Despite their potential, the integration of gLLMs into the
methodological toolkit of communication research remains underdeveloped. In
gLLM-assisted quantitative content analysis, researchers must address at least
seven critical challenges that impact result quality: (1) codebook development,
(2) prompt engineering, (3) model selection, (4) parameter tuning, (5)
iterative refinement, (6) validation of the model's reliability, and
optionally, (7) performance enhancement. This paper synthesizes emerging
research on gLLM-assisted quantitative content analysis and proposes a
comprehensive best-practice guide to navigate these challenges. Our goal is to
make gLLM-based content analysis more accessible to a broader range of
communication researchers and ensure adherence to established disciplinary
quality standards of validity, reliability, reproducibility, and research
ethics.

</details>


### [65] [VDSAgents: A PCS-Guided Multi-Agent System for Veridical Data Science Automation](https://arxiv.org/abs/2510.24339)
*Yunxuan Jiang,Silan Hu,Xiaoning Wang,Yuanyuan Zhang,Xiangyu Chang*

Main category: cs.AI

TL;DR: VDSAgents是一个基于可预测性-可计算性-稳定性(PCS)原则的多智能体系统，用于提升LLM驱动数据科学系统的可靠性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 当前LLM驱动的数据科学系统仅依赖模型内部推理，缺乏科学和理论原则指导，在处理噪声和复杂现实数据集时可信度和鲁棒性不足。

Method: 基于PCS原则构建多智能体系统，采用模块化工作流处理数据清洗、特征工程、建模和评估，每个阶段由专门智能体负责，并整合扰动分析、单元测试和模型验证。

Result: 在9个不同特征的数据集上评估，使用DeepSeek-V3和GPT-4o作为后端，VDSAgents持续优于AutoKaggle和DataInterpreter等最先进的端到端数据科学系统。

Conclusion: 验证了将PCS原则嵌入LLM驱动数据科学自动化的可行性，提升了系统的科学可审计性和性能。

Abstract: Large language models (LLMs) become increasingly integrated into data science
workflows for automated system design. However, these LLM-driven data science
systems rely solely on the internal reasoning of LLMs, lacking guidance from
scientific and theoretical principles. This limits their trustworthiness and
robustness, especially when dealing with noisy and complex real-world datasets.
This paper provides VDSAgents, a multi-agent system grounded in the
Predictability-Computability-Stability (PCS) principles proposed in the
Veridical Data Science (VDS) framework. Guided by PCS principles, the system
implements a modular workflow for data cleaning, feature engineering, modeling,
and evaluation. Each phase is handled by an elegant agent, incorporating
perturbation analysis, unit testing, and model validation to ensure both
functionality and scientific auditability. We evaluate VDSAgents on nine
datasets with diverse characteristics, comparing it with state-of-the-art
end-to-end data science systems, such as AutoKaggle and DataInterpreter, using
DeepSeek-V3 and GPT-4o as backends. VDSAgents consistently outperforms the
results of AutoKaggle and DataInterpreter, which validates the feasibility of
embedding PCS principles into LLM-driven data science automation.

</details>


### [66] [A Unified Geometric Space Bridging AI Models and the Human Brain](https://arxiv.org/abs/2510.24342)
*Silin Chen,Yuzhong Chen,Zifan Wang,Junhao Wang,Zifeng Jia,Keith M Kendrick,Tuo Zhang,Lin Zhao,Dezhong Yao,Tianming Liu,Xi Jiang*

Main category: cs.AI

TL;DR: 提出了"脑相似空间"的概念，这是一个统一的几何空间，可以将不同模态的AI模型映射到人类功能性脑网络上进行比较，揭示了AI模型与大脑组织之间的连续几何关系。


<details>
  <summary>Details</summary>
Motivation: 现有的大脑-AI对齐研究局限于特定输入和任务，缺乏比较不同模态AI模型内在组织方式的统一框架。

Method: 通过将AI模型的内在空间注意力拓扑组织映射到典型人类功能性脑网络上，构建统一的脑相似空间，分析了151个基于Transformer的模型。

Result: 发现了一个连续的弧形几何结构，反映了脑相似度的逐渐增加；不同模型在该几何中表现出不同的分布模式，受预训练范式和位置编码方案影响。

Conclusion: 脑相似度与下游任务性能并非完全一致，脑相似空间为跨领域智能的定位、量化和比较提供了首个统一框架。

Abstract: For decades, neuroscientists and computer scientists have pursued a shared
ambition: to understand intelligence and build it. Modern artificial neural
networks now rival humans in language, perception, and reasoning, yet it is
still largely unknown whether these artificial systems organize information as
the brain does. Existing brain-AI alignment studies have shown the striking
correspondence between the two systems, but such comparisons remain bound to
specific inputs and tasks, offering no common ground for comparing how AI
models with different kinds of modalities-vision, language, or multimodal-are
intrinsically organized. Here we introduce a groundbreaking concept of
Brain-like Space: a unified geometric space in which every AI model can be
precisely situated and compared by mapping its intrinsic spatial attention
topological organization onto canonical human functional brain networks,
regardless of input modality, task, or sensory domain. Our extensive analysis
of 151 Transformer-based models spanning state-of-the-art large vision models,
large language models, and large multimodal models uncovers a continuous
arc-shaped geometry within this space, reflecting a gradual increase of
brain-likeness; different models exhibit distinct distribution patterns within
this geometry associated with different degrees of brain-likeness, shaped not
merely by their modality but by whether the pretraining paradigm emphasizes
global semantic abstraction and whether the positional encoding scheme
facilitates deep fusion across different modalities. Moreover, the degree of
brain-likeness for a model and its downstream task performance are not
"identical twins". The Brain-like Space provides the first unified framework
for situating, quantifying, and comparing intelligence across domains,
revealing the deep organizational principles that bridge machines and the
brain.

</details>


### [67] [An N-of-1 Artificial Intelligence Ecosystem for Precision Medicine](https://arxiv.org/abs/2510.24359)
*Pedram Fard,Alaleh Azhir,Neguine Rezaii,Jiazi Tian,Hossein Estiri*

Main category: cs.AI

TL;DR: 提出多智能体生态系统用于N-of-1决策支持，解决传统医疗AI面向平均患者的问题，通过协调多个专业智能体提供个性化、透明的医疗决策支持。


<details>
  <summary>Details</summary>
Motivation: 传统医疗AI系统为平均患者设计，在罕见变异、多病共存和代表性不足人群等边缘情况下表现不佳，导致公平性和信任问题。

Method: 构建多智能体生态系统，按器官系统、患者群体和分析模式聚类智能体，共享模型和证据合成工具，通过协调层整合结果并考虑可靠性、不确定性和数据密度。

Result: 提供包含风险估计、置信区间、异常标志和关联证据的决策支持包，验证重点从群体平均转向个体可靠性。

Conclusion: 通过从单一模型转向协调智能，使医疗AI与医学首要原则保持一致：提供透明、公平且以个体为中心的医疗服务。

Abstract: Artificial intelligence in medicine is built to serve the average patient. By
minimizing error across large datasets, most systems deliver strong aggregate
accuracy yet falter at the margins: patients with rare variants,
multimorbidity, or underrepresented demographics. This average patient fallacy
erodes both equity and trust. We propose a different design: a multi-agent
ecosystem for N-of-1 decision support. In this environment, agents clustered by
organ systems, patient populations, and analytic modalities draw on a shared
library of models and evidence synthesis tools. Their results converge in a
coordination layer that weighs reliability, uncertainty, and data density
before presenting the clinician with a decision-support packet: risk estimates
bounded by confidence ranges, outlier flags, and linked evidence. Validation
shifts from population averages to individual reliability, measured by error in
low-density regions, calibration in the small, and risk--coverage trade-offs.
Anticipated challenges include computational demands, automation bias, and
regulatory fit, addressed through caching strategies, consensus checks, and
adaptive trial frameworks. By moving from monolithic models to orchestrated
intelligence, this approach seeks to align medical AI with the first principle
of medicine: care that is transparent, equitable, and centered on the
individual.

</details>


### [68] [Policy Cards: Machine-Readable Runtime Governance for Autonomous AI Agents](https://arxiv.org/abs/2510.24383)
*Juraj Mavračić*

Main category: cs.AI

TL;DR: Policy Cards是一种机器可读的标准，用于为AI代理表达操作、监管和伦理约束，使其在运行时遵循要求，支持自动验证和合规性证明。


<details>
  <summary>Details</summary>
Motivation: 现有透明度工具如Model Cards等缺乏规范层来编码AI代理的约束规则，需要一种机制将高层治理与工程实践结合，实现可验证的合规性。

Method: 提出Policy Cards框架，定义规范层来编码允许/拒绝规则、义务、证据要求和与NIST AI RMF、ISO/IEC 42001、欧盟AI法案等保证框架的映射关系。

Result: Policy Cards可自动验证、版本控制，并与运行时执行或持续审计管道链接，为自主代理提供可验证的合规性基础。

Conclusion: Policy Cards为多代理生态系统中的分布式保证提供了实用机制，实现了大规模可问责的自主性。

Abstract: Policy Cards are introduced as a machine-readable, deployment-layer standard
for expressing operational, regulatory, and ethical constraints for AI agents.
The Policy Card sits with the agent and enables it to follow required
constraints at runtime. It tells the agent what it must and must not do. As
such, it becomes an integral part of the deployed agent. Policy Cards extend
existing transparency artifacts such as Model, Data, and System Cards by
defining a normative layer that encodes allow/deny rules, obligations,
evidentiary requirements, and crosswalk mappings to assurance frameworks
including NIST AI RMF, ISO/IEC 42001, and the EU AI Act. Each Policy Card can
be validated automatically, version-controlled, and linked to runtime
enforcement or continuous-audit pipelines. The framework enables verifiable
compliance for autonomous agents, forming a foundation for distributed
assurance in multi-agent ecosystems. Policy Cards provide a practical mechanism
for integrating high-level governance with hands-on engineering practice and
enabling accountable autonomy at scale.

</details>


### [69] [Improving LLM Reasoning via Dependency-Aware Query Decomposition and Logic-Parallel Content Expansion](https://arxiv.org/abs/2510.24390)
*Xianjun Gao,Jianchun Liu,Hongli Xu,Liusheng Huang*

Main category: cs.AI

TL;DR: Orion是一个高效推理框架，通过依赖感知的查询分解和逻辑并行内容扩展，解决了LLM在实时Web应用中的延迟和吞吐量瓶颈问题。


<details>
  <summary>Details</summary>
Motivation: 现有LLM推理方法在计算效率低下的顺序生成和僵化推理策略的限制下，难以同时满足Web服务对高质量复杂推理和低延迟高吞吐量的双重需求。

Method: Orion将查询推理分解为两个协同阶段：关键点生成（通过检索增强的少样本提示提炼逻辑结构化的关键点）和内容并行扩展（基于依赖图并行扩展这些点以确保逻辑一致性），并引入流水线调度机制实现跨查询并行。

Result: 实验显示Orion相比基线方法实现了4.33倍的token生成速度提升、3.42倍的答案延迟降低，并通过显式建模点间依赖关系使推理质量提升高达18.75%。

Conclusion: Orion框架成功解决了LLM在Web应用中的效率-质量权衡问题，通过创新的分解和并行策略显著提升了推理性能和效果。

Abstract: The integration of Large Language Models (LLMs) into real-time Web
applications, such as AI-powered search and conversational agents, presents a
fundamental Web infrastructure challenge: reconciling the demand for
high-quality, complex reasoning with the stringent low-latency and
high-throughput requirements of interactive services. Current LLM reasoning,
hindered by computationally inefficient sequential generation and rigid
reasoning strategies, creates a critical bottleneck for the Web services.
Existing approaches typically optimize the LLM reasoning for either efficiency
or quality but struggle to achieve both, and thus fail to meet the dual
requirements of modern Web platforms. To overcome these limitations, we propose
Orion, a novel and efficient reasoning framework that enables dependency-aware
query decomposition and logic-parallel content expansion. Concretely, Orion
decomposes a single query reasoning process into two synergistic phases: (1)
\textit{key point generation}, which distills logically structured key points
through retrieval-augmented few-shot prompting, and (2) \textit{content
parallel expansion}, which concurrently elaborates on these points based on a
dependency graph to ensure logical consistency. Furthermore, Orion introduces a
pipeline scheduling mechanism that exploits the complementary computational
characteristics of the two phases (generation imposes pressure on GPU computing
and expansion stresses on GPU memory) across multiple queries, enabling
cross-query parallelism and dramatically improving reasoning performance (\ie,
efficiency and quality). Experiments on diverse benchmarks show that Orion not
only delivers up to 4.33x higher token generation speed and 3.42x lower answer
latency over the baselines but also improves reasoning quality by up to 18.75%
through explicitly modeling inter-point dependencies.

</details>


### [70] [APTBench: Benchmarking Agentic Potential of Base LLMs During Pre-Training](https://arxiv.org/abs/2510.24397)
*Jiarui Qin,Yunjia Xi,Junjie Huang,Renting Rui,Di Yin,Weiwen Liu,Yong Yu,Weinan Zhang,Xing Sun*

Main category: cs.AI

TL;DR: 提出了APTBench框架，将真实世界智能体任务转化为适合基础模型的多选题或文本补全问题，用于在预训练阶段评估智能体潜力。


<details>
  <summary>Details</summary>
Motivation: 当前预训练基准主要关注孤立静态技能，无法反映模型的智能体能力；而智能体基准通常为后训练模型设计，基础模型难以支持多轮任务执行。需要能在预训练阶段评估智能体潜力的基准。

Method: 将真实世界智能体任务和成功轨迹转化为多选题或文本补全问题，聚焦规划和行动等核心智能体能力，覆盖软件工程和深度研究等关键场景。

Result: 相比通用基准，APTBench能更准确地预测模型作为智能体的下游性能，同时比后训练的端到端评估更轻量、成本更低。

Conclusion: APTBench填补了预训练阶段智能体能力评估的空白，为模型训练提供了更有效的指导。

Abstract: With the rapid development of LLM-based agents, there is a growing trend to
incorporate agent-specific data into the pre-training stage of LLMs, aiming to
better align LLMs with real-world autonomous task execution. However, current
pre-training benchmarks primarily focus on isolated and static skills, e.g.,
common knowledge or mathematical/code reasoning, and fail to reflect model's
agentic capabilities. On the other hand, agent benchmarks are typically
designed for post-trained models, requiring multi-turn task execution abilities
that base models struggle to support. Thus, there is a compelling need for a
benchmark that can evaluate agentic potentials during pre-training and guide
the model training more effectively. To address this gap, we propose APTBench,
a framework that converts real-world agent tasks and successful trajectories
into multiple-choice or text completion questions tailored for base models. It
focuses on core agentic abilities, e.g., planning and action, and covers key
agent scenarios, software engineering and deep research. Compared to existing
general-purpose benchmarks, APTBench offers a more predictive signal of a
model's downstream performance as an agent, while remaining significantly more
lightweight and cost-effective than full-scale, end-to-end agent evaluations
after post-training.

</details>


### [71] [OS-Sentinel: Towards Safety-Enhanced Mobile GUI Agents via Hybrid Validation in Realistic Workflows](https://arxiv.org/abs/2510.24411)
*Qiushi Sun,Mukai Li,Zhoumianze Liu,Zhihui Xie,Fangzhi Xu,Zhangyue Yin,Kanzhi Cheng,Zehao Li,Zichen Ding,Qi Liu,Zhiyong Wu,Zhuosheng Zhang,Ben Kao,Lingpeng Kong*

Main category: cs.AI

TL;DR: 提出了MobileRisk-Live动态沙盒环境和OS-Sentinel混合安全检测框架，用于检测移动AI代理的安全风险，在多个指标上比现有方法提升10%-30%。


<details>
  <summary>Details</summary>
Motivation: 基于视觉语言模型的计算机代理在移动平台上展现出类人操作能力，但其潜在的安全风险（如系统破坏和隐私泄露）引发严重担忧，亟需建立移动代理安全研究基础。

Method: 构建MobileRisk-Live动态沙盒环境和安全检测基准，提出OS-Sentinel混合框架，结合形式验证器检测系统级违规和基于VLM的上下文判断器评估上下文风险。

Result: OS-Sentinel在多个指标上比现有方法提升10%-30%，为开发更安全可靠的自主移动代理提供了关键见解。

Conclusion: 该研究为移动代理安全研究奠定了基础，提出的混合检测框架能有效识别安全风险，促进更安全可靠的自主移动代理发展。

Abstract: Computer-using agents powered by Vision-Language Models (VLMs) have
demonstrated human-like capabilities in operating digital environments like
mobile platforms. While these agents hold great promise for advancing digital
automation, their potential for unsafe operations, such as system compromise
and privacy leakage, is raising significant concerns. Detecting these safety
concerns across the vast and complex operational space of mobile environments
presents a formidable challenge that remains critically underexplored. To
establish a foundation for mobile agent safety research, we introduce
MobileRisk-Live, a dynamic sandbox environment accompanied by a safety
detection benchmark comprising realistic trajectories with fine-grained
annotations. Built upon this, we propose OS-Sentinel, a novel hybrid safety
detection framework that synergistically combines a Formal Verifier for
detecting explicit system-level violations with a VLM-based Contextual Judge
for assessing contextual risks and agent actions. Experiments show that
OS-Sentinel achieves 10%-30% improvements over existing approaches across
multiple metrics. Further analysis provides critical insights that foster the
development of safer and more reliable autonomous mobile agents.

</details>


### [72] [Human-Level Reasoning: A Comparative Study of Large Language Models on Logical and Abstract Reasoning](https://arxiv.org/abs/2510.24435)
*Benjamin Grando Moreira*

Main category: cs.AI

TL;DR: 本研究比较了多个大语言模型在逻辑和抽象推理能力上的表现，并与人类表现进行基准测试，揭示了LLMs在演绎推理方面的困难。


<details>
  <summary>Details</summary>
Motivation: 评估大语言模型的推理能力对于推进人工智能发展至关重要，这超越了单纯的语言任务表现，涉及理解模型是否真正理解信息、进行推理并以逻辑有效的方式得出结论。

Method: 使用八个自定义设计的推理问题，比较了GPT、Claude、DeepSeek、Gemini、Grok、Llama、Mistral、Perplexity和Sabi'a等多个LLMs的逻辑和抽象推理技能，并将结果与人类在相同任务上的表现进行基准测试。

Result: 研究揭示了显著差异，表明LLMs在演绎推理方面存在困难。

Conclusion: 大语言模型在逻辑和抽象推理方面仍有待改进，特别是在演绎推理能力上存在明显短板。

Abstract: Evaluating reasoning ability in Large Language Models (LLMs) is important for
advancing artificial intelligence, as it transcends mere linguistic task
performance. It involves understanding whether these models truly understand
information, perform inferences, and are able to draw conclusions in a logical
and valid way. This study compare logical and abstract reasoning skills of
several LLMs - including GPT, Claude, DeepSeek, Gemini, Grok, Llama, Mistral,
Perplexity, and Sabi\'a - using a set of eight custom-designed reasoning
questions. The LLM results are benchmarked against human performance on the
same tasks, revealing significant differences and indicating areas where LLMs
struggle with deduction.

</details>


### [73] [Law in Silico: Simulating Legal Society with LLM-Based Agents](https://arxiv.org/abs/2510.24442)
*Yiding Wang,Yuxuan Chen,Fanxu Meng,Xifan Chen,Xiaolei Yang,Muhan Zhang*

Main category: cs.AI

TL;DR: Law in Silico是一个基于LLM的代理框架，用于模拟法律场景中的个体决策和制度机制，能够再现宏观犯罪趋势并为弱势群体权利保护提供见解。


<details>
  <summary>Details</summary>
Motivation: 由于现实世界法律实验成本高昂或不可行，利用AI系统模拟法律社会成为验证和发展法律理论的有效替代方案。LLMs凭借其世界知识和角色扮演能力，是构建法律社会模拟的强有力候选者。

Method: 引入Law in Silico框架，基于LLM的代理模拟法律场景，包括个体决策和立法、裁决、执法等制度机制。通过比较模拟犯罪率与现实数据来验证框架有效性。

Result: 实验表明，基于LLM的代理能够很大程度上再现宏观层面的犯罪趋势，并提供与现实观察一致的见解。微观层面模拟显示，功能良好、透明且适应性的法律系统能更好地保护弱势个体的权利。

Conclusion: LLM-based agents can effectively simulate legal systems, reproducing macro-level crime trends and offering insights that align with real-world observations, while highlighting the importance of well-functioning legal systems for protecting vulnerable individuals.

Abstract: Since real-world legal experiments are often costly or infeasible, simulating
legal societies with Artificial Intelligence (AI) systems provides an effective
alternative for verifying and developing legal theory, as well as supporting
legal administration. Large Language Models (LLMs), with their world knowledge
and role-playing capabilities, are strong candidates to serve as the foundation
for legal society simulation. However, the application of LLMs to simulate
legal systems remains underexplored. In this work, we introduce Law in Silico,
an LLM-based agent framework for simulating legal scenarios with individual
decision-making and institutional mechanisms of legislation, adjudication, and
enforcement. Our experiments, which compare simulated crime rates with
real-world data, demonstrate that LLM-based agents can largely reproduce
macro-level crime trends and provide insights that align with real-world
observations. At the same time, micro-level simulations reveal that a
well-functioning, transparent, and adaptive legal system offers better
protection of the rights of vulnerable individuals.

</details>


### [74] [Adaptive Surrogate Gradients for Sequential Reinforcement Learning in Spiking Neural Networks](https://arxiv.org/abs/2510.24461)
*Korneel Van den Berghe,Stein Stroobants,Vijay Janapa Reddi,G. C. H. E. de Croon*

Main category: cs.AI

TL;DR: 该论文提出了一种改进脉冲神经网络训练的方法，通过分析替代梯度斜率设置和引入引导策略来提升强化学习性能，在无人机位置控制任务中实现了显著优于现有方法的性能。


<details>
  <summary>Details</summary>
Motivation: 脉冲神经网络在能效受限的机器人应用中具有巨大潜力，但面临两个关键挑战：脉冲神经元的不可微特性需要使用替代梯度，以及状态动态需要序列训练，这在强化学习中受到早期训练序列长度限制的影响。

Method: 系统分析替代梯度斜率设置，发现较浅斜率能增加深层梯度幅度但减少与真实梯度的对齐；提出利用特权引导策略来引导学习过程，同时保持与环境的在线交互；结合自适应斜率调度方法。

Result: 在强化学习设置中，较浅斜率或调度斜率使训练和最终部署性能提升2.1倍；在真实世界无人机位置控制任务中，平均回报达到400点，显著优于行为克隆和TD3BC等方法（最多-200点）。

Conclusion: 这项工作推进了对SNN中替代梯度学习的理论理解，并开发了在真实世界机器人系统中演示的实用神经形态控制器训练方法。

Abstract: Neuromorphic computing systems are set to revolutionize energy-constrained
robotics by achieving orders-of-magnitude efficiency gains, while enabling
native temporal processing. Spiking Neural Networks (SNNs) represent a
promising algorithmic approach for these systems, yet their application to
complex control tasks faces two critical challenges: (1) the non-differentiable
nature of spiking neurons necessitates surrogate gradients with unclear
optimization properties, and (2) the stateful dynamics of SNNs require training
on sequences, which in reinforcement learning (RL) is hindered by limited
sequence lengths during early training, preventing the network from bridging
its warm-up period.
  We address these challenges by systematically analyzing surrogate gradient
slope settings, showing that shallower slopes increase gradient magnitude in
deeper layers but reduce alignment with true gradients. In supervised learning,
we find no clear preference for fixed or scheduled slopes. The effect is much
more pronounced in RL settings, where shallower slopes or scheduled slopes lead
to a 2.1x improvement in both training and final deployed performance. Next, we
propose a novel training approach that leverages a privileged guiding policy to
bootstrap the learning process, while still exploiting online environment
interactions with the spiking policy. Combining our method with an adaptive
slope schedule for a real-world drone position control task, we achieve an
average return of 400 points, substantially outperforming prior techniques,
including Behavioral Cloning and TD3BC, which achieve at most --200 points
under the same conditions. This work advances both the theoretical
understanding of surrogate gradient learning in SNNs and practical training
methodologies for neuromorphic controllers demonstrated in real-world robotic
systems.

</details>


### [75] [From Cross-Task Examples to In-Task Prompts: A Graph-Based Pseudo-Labeling Framework for In-context Learning](https://arxiv.org/abs/2510.24528)
*Zihan Chen,Song Wang,Xingbo Fu,Chengshuai Shi,Zhenyu Lei,Cong Shen,Jundong Li*

Main category: cs.AI

TL;DR: 提出了一种成本效益高的两阶段流程，通过交叉任务示例和基于图的标签传播方法，减少对LLM数据标注的依赖，构建高质量ICL演示。


<details>
  <summary>Details</summary>
Motivation: 为新颖或困难任务收集高质量示例成本高昂且劳动密集，需要减少对LLM标注的依赖。

Method: 两阶段流程：1) 利用交叉任务示例提示LLM伪标注少量目标实例；2) 引入基于图的标签传播方法，无需额外LLM查询即可将标签信息传播到其余目标示例。

Result: 在五个任务上的实验表明，该方法在降低标注成本的同时实现了强劲性能。

Conclusion: 该管道结合了交叉任务监督的灵活性和无需LLM传播的可扩展性，为ICL提供了成本效益高的解决方案。

Abstract: The capability of in-context learning (ICL) enables large language models
(LLMs) to perform novel tasks without parameter updates by conditioning on a
few input-output examples. However, collecting high-quality examples for new or
challenging tasks can be costly and labor-intensive. In this work, we propose a
cost-efficient two-stage pipeline that reduces reliance on LLMs for data
labeling. Our approach first leverages readily available cross-task examples to
prompt an LLM and pseudo-label a small set of target task instances. We then
introduce a graph-based label propagation method that spreads label information
to the remaining target examples without additional LLM queries. The resulting
fully pseudo-labeled dataset is used to construct in-task demonstrations for
ICL. This pipeline combines the flexibility of cross-task supervision with the
scalability of LLM-free propagation. Experiments across five tasks demonstrate
that our method achieves strong performance while lowering labeling costs.

</details>


### [76] [Generative AI for Healthcare: Fundamentals, Challenges, and Perspectives](https://arxiv.org/abs/2510.24551)
*Gang Chen,Changshuo Liu,Gene Anne Ooi,Marcus Tan,Zhongle Xie,Jianwei Yin,James Wei Luen Yip,Wenqiao Zhang,Jiaqi Zhu,Beng Chin Ooi*

Main category: cs.AI

TL;DR: 本文提出了一种以数据为中心的方法来设计和部署医疗领域的生成式AI系统，将医疗数据生态系统作为基础，支持多模态数据的集成、表示和检索。


<details>
  <summary>Details</summary>
Motivation: 生成式AI在医疗领域具有巨大潜力，但需要深入理解医疗任务和可实现的范围。现有方法需要更系统化的数据管理来支持AI系统的有效部署。

Method: 重新定位数据生命周期，构建医疗数据生态系统作为生成式医疗系统的基础。通过语义向量搜索和上下文查询等高效数据处理管道，支持上游模型组件和下游临床应用。

Result: 该生态系统不仅为基座模型提供高质量多模态数据用于大规模预训练和领域特定微调，还作为知识检索后端通过代理层支持任务特定推理。

Conclusion: 该数据中心的范式能够实现高质量、有效的医疗生成式AI部署，改善医疗服务交付。

Abstract: Generative Artificial Intelligence (GenAI) is taking the world by storm. It
promises transformative opportunities for advancing and disrupting existing
practices, including healthcare. From large language models (LLMs) for clinical
note synthesis and conversational assistance to multimodal systems that
integrate medical imaging, electronic health records, and genomic data for
decision support, GenAI is transforming the practice of medicine and the
delivery of healthcare, such as diagnosis and personalized treatments, with
great potential in reducing the cognitive burden on clinicians, thereby
improving overall healthcare delivery. However, GenAI deployment in healthcare
requires an in-depth understanding of healthcare tasks and what can and cannot
be achieved. In this paper, we propose a data-centric paradigm in the design
and deployment of GenAI systems for healthcare. Specifically, we reposition the
data life cycle by making the medical data ecosystem as the foundational
substrate for generative healthcare systems. This ecosystem is designed to
sustainably support the integration, representation, and retrieval of diverse
medical data and knowledge. With effective and efficient data processing
pipelines, such as semantic vector search and contextual querying, it enables
GenAI-powered operations for upstream model components and downstream clinical
applications. Ultimately, it not only supplies foundation models with
high-quality, multimodal data for large-scale pretraining and domain-specific
fine-tuning, but also serves as a knowledge retrieval backend to support
task-specific inference via the agentic layer. The ecosystem enables the
deployment of GenAI for high-quality and effective healthcare delivery.

</details>


### [77] [FunReason-MT Technical Report: Overcoming the Complexity Barrier in Multi-Turn Function Calling](https://arxiv.org/abs/2510.24645)
*Zengzhuang Xu,Bingguang Hao,Zechuan Wang,Yuntao Wen,Maolin Wang,Yang Liu,Long Chen,Dong Wang,Yicheng Chen,Cunyin Peng,Chenyi Zhuang,Jinjie Gu,Leilei Gan,Xiangyu Zhao,Shi Gu*

Main category: cs.AI

TL;DR: FunReason-MT是一个用于多轮工具调用数据合成的创新框架，通过环境-API图交互、高级工具查询合成和引导迭代链来解决现实世界多轮函数调用的复杂性问题。


<details>
  <summary>Details</summary>
Motivation: 现有数据合成方法（如随机环境采样或多智能体角色扮演）在现实环境中生成高质量数据的能力不足，存在目标模型训练、工具架构隔离和多轮逻辑依赖三大挑战。

Method: 采用环境-API图交互收集多样化高质量轨迹，高级工具查询合成简化困难查询构建，以及引导迭代链生成复杂思维链。

Result: 在Berkeley函数调用排行榜上，基于FunReason-MT生成数据训练的4B模型在同等规模模型中达到最先进性能，超越大多数闭源模型，并在BFCLv4上持续改进。

Conclusion: FunReason-MT为智能体学习提供了可靠且强大的数据源，有效解决了多轮函数调用数据合成的结构性问题。

Abstract: Function calling (FC) empowers large language models (LLMs) and autonomous
agents to interface with external tools, a critical capability for solving
complex, real-world problems. As this ability becomes increasingly central to
advanced AI systems, the need for high-quality, multi-turn training data to
develop and refine it cannot be overstated. Existing data synthesis methods,
such as random environment sampling or multi-agent role-playing, are not
powerful enough to generate high-quality data in real-world environments.
Practical challenges come in three folds: targeted model training, isolation of
tool architecture, and multi-turn logical dependency. To address these
structural deficiencies, we present FunReason-MT, a novel data synthesis
framework for real-world multi-turn tool use. FunReason-MT resolves the
complexity barrier in multi-turn FC data by employing 1) Environment-API Graph
Interactions to gather varied high-quality trajectories, 2) Advanced Tool-Query
Synthesis to simplify hard query construction, and 3) Guided Iterative Chain
for sophisticated CoT generation. Evaluations on Berkeley Function-Calling
Leaderboard (BFCLv3) demonstrate the power of our framework: a 4B model built
upon FunReason-MT generated data achieves state-of-the-art performance among
comparable-sized models, outperforming most close-source models. Further
performance improvements on BFCLv4 confirm that FunReason-MT provides a
reliable and robust source for agentic learning.

</details>


### [78] [Advancing site-specific disease and pest management in precision agriculture: From reasoning-driven foundation models to adaptive, feedback-based learning](https://arxiv.org/abs/2510.24650)
*Nitin Rai,Daeun,Choi,Nathan S. Boyd,Arnold W. Schumann*

Main category: cs.AI

TL;DR: 本文综述了基于基础模型（FMs）的作物精准病害管理（SSDM）研究进展，重点分析了视觉语言模型（VLMs）和大语言模型（LLMs）在自适应学习、强化学习和数字孪生框架中的应用现状与发展趋势。


<details>
  <summary>Details</summary>
Motivation: 随着机器学习和深度学习在计算机视觉领域的快速发展，作物病害管理从手工特征提取发展到大规模自动化特征学习。基础模型通过整合视觉和文本数据，能够解释症状文本、推理症状与治理关系，并为种植者和教育者提供交互式问答支持。

Method: 通过筛选约40篇关于基础模型在精准病害管理中应用的文献，重点分析大型语言模型和视觉语言模型，并讨论它们在自适应学习、强化学习和数字孪生框架中的作用。

Result: 主要发现：(a) 基础模型在2023-24年文献激增；(b) 视觉语言模型发展快于大型语言模型，出版物数量增加5-10倍；(c) 强化学习和自适应学习在智能喷洒中仍处于起步阶段；(d) 结合强化学习的数字孪生可虚拟模拟精准喷洒；(e) 解决仿真到现实的差距对实际部署至关重要；(f) 人机协作仍然有限；(g) 具有实时反馈的多模态基础模型将推动下一代精准病害管理。

Conclusion: 基础模型正在改变作物病害数据处理方式，多模态基础模型与实时反馈将推动下一代精准病害管理技术的发展，但需要解决仿真到现实的差距和人机协作等挑战。

Abstract: Site-specific disease management (SSDM) in crops has advanced rapidly through
machine and deep learning (ML and DL) for real-time computer vision. Research
evolved from handcrafted feature extraction to large-scale automated feature
learning. With foundation models (FMs), crop disease datasets are now processed
in fundamentally new ways. Unlike traditional neural networks, FMs integrate
visual and textual data, interpret symptoms in text, reason about
symptom-management relationships, and support interactive QA for growers and
educators. Adaptive and imitation learning in robotics further enables
field-based disease management. This review screened approx. 40 articles on FM
applications for SSDM, focusing on large-language models (LLMs) and
vision-language models (VLMs), and discussing their role in adaptive learning
(AL), reinforcement learning (RL), and digital twin frameworks for targeted
spraying. Key findings: (a) FMs are gaining traction with surging literature in
2023-24; (b) VLMs outpace LLMs, with a 5-10x increase in publications; (c) RL
and AL are still nascent for smart spraying; (d) digital twins with RL can
simulate targeted spraying virtually; (e) addressing the sim-to-real gap is
critical for real-world deployment; (f) human-robot collaboration remains
limited, especially in human-in-the-loop approaches where robots detect early
symptoms and humans validate uncertain cases; (g) multi-modal FMs with
real-time feedback will drive next-gen SSDM. For updates, resources, and
contributions, visit, https://github.com/nitin-dominic/AgriPathogenDatabase, to
submit papers, code, or datasets.

</details>


### [79] [OrchDAG: Complex Tool Orchestration in Multi-Turn Interactions with Plan DAGs](https://arxiv.org/abs/2510.24663)
*Yifu Lu,Shengjie Liu,Li Dong*

Main category: cs.AI

TL;DR: 提出了OrchDAG，一个用于多轮工具交互的合成数据生成管道，将工具执行建模为具有可控复杂度的有向无环图，并提出了基于图的奖励来增强RLVR训练。


<details>
  <summary>Details</summary>
Motivation: 现有的智能体工具使用研究大多忽视了多轮工具交互的复杂性，需要更好的基准和方法来处理这种复杂场景。

Method: 引入OrchDAG合成数据生成管道，将工具执行建模为有向无环图；提出基于图的奖励机制，结合GRPO风格算法进行训练。

Result: 实验表明该数据集提供了一个具有挑战性但可解决的基准，所提出的奖励机制在结合GRPO算法时表现有效。

Conclusion: 在多轮工具使用中，利用拓扑结构和数据复杂性至关重要，图结构建模能够有效提升智能体工具交互性能。

Abstract: Agentic tool use has gained traction with the rise of agentic tool calling,
yet most existing work overlooks the complexity of multi-turn tool
interactions. We introduce OrchDAG, a synthetic data generation pipeline that
models tool execution as directed acyclic graphs (DAGs) with controllable
complexity. Using this dataset, we benchmark model performance and propose a
graph-based reward to enhance RLVR training. Experiments show that the dataset
presents a challenging but solvable benchmark, and the proposed reward is
effective when combined with GRPO-style algorithms, highlighting the importance
of leveraging topological structure and data complexity in multi-turn tool use.

</details>


### [80] [Bridging Tool Dependencies and Domain Knowledge: A Graph-Based Framework for In-Context Planning](https://arxiv.org/abs/2510.24690)
*Shengjie Liu,Li Dong,Zhenyu Zhang*

Main category: cs.AI

TL;DR: 提出一个框架，通过构建工具知识图谱和文档知识图谱的融合，来增强范例工件的生成。


<details>
  <summary>Details</summary>
Motivation: 现有的工具使用和文档知识往往是分离的，缺乏对工具间依赖关系和领域知识的统一建模，限制了范例计划的生成质量。

Method: 从工具模式构建工具知识图谱，从内部文档和SOP构建文档知识图谱，然后将两者融合，采用深度稀疏集成策略对齐工具结构依赖和程序知识。

Result: 实验表明该统一框架能有效建模工具交互并改进计划生成。

Conclusion: 将工具图谱与领域知识图谱链接对于工具增强的推理和规划具有显著益处。

Abstract: We present a framework for uncovering and exploiting dependencies among tools
and documents to enhance exemplar artifact generation. Our method begins by
constructing a tool knowledge graph from tool schemas,including descriptions,
arguments, and output payloads, using a DeepResearch-inspired analysis. In
parallel, we derive a complementary knowledge graph from internal documents and
SOPs, which is then fused with the tool graph. To generate exemplar plans, we
adopt a deep-sparse integration strategy that aligns structural tool
dependencies with procedural knowledge. Experiments demonstrate that this
unified framework effectively models tool interactions and improves plan
generation, underscoring the benefits of linking tool graphs with domain
knowledge graphs for tool-augmented reasoning and planning.

</details>
