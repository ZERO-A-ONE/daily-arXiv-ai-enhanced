<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 9]
- [cs.AI](#cs.AI) [Total: 14]
- [cs.CR](#cs.CR) [Total: 22]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [On the Use of a Large Language Model to Support the Conduction of a Systematic Mapping Study: A Brief Report from a Practitioner's View](https://arxiv.org/abs/2602.10147)
*Cauã Ferreira Barros,Marcos Kalinowski,Mohamad Kassab,Valdemar Vicente Graciano Neto*

Main category: cs.SE

TL;DR: 本文报告了使用大语言模型（LLMs）支持系统映射研究的实践经验，总结了LLMs在加速系统综述流程中的优势与挑战


<details>
  <summary>Details</summary>
Motivation: 尽管LLMs在加速系统综述的筛选和数据提取步骤方面显示出潜力，但关于其在整个流程中实际应用的详细报告仍然稀缺。本文旨在填补这一空白，分享实践经验

Method: 本文采用经验报告的形式，描述了一个在LLMs支持下进行的系统映射研究的完整步骤、必要的调整以及面临的主要挑战

Result: 研究发现LLMs能够显著减少重复任务的时间并提高数据提取的标准化程度，但也面临构建可靠提示需要大量努力、出现幻觉问题以及需要持续人工验证等挑战

Conclusion: 本文为研究人员在系统映射和综述中采用LLMs提供了经验教训和实践建议，强调了效率提升的同时也需要考虑方法论风险和局限性

Abstract: The use of Large Language Models (LLMs) has drawn growing interest within the scientific community. LLMs can handle large volumes of textual data and support methods for evidence synthesis. Although recent studies highlight the potential of LLMs to accelerate screening and data extraction steps in systematic reviews, detailed reports of their practical application throughout the entire process remain scarce. This paper presents an experience report on the conduction of a systematic mapping study with the support of LLMs, describing the steps followed, the necessary adjustments, and the main challenges faced. Positive aspects are discussed, such as (i) the significant reduction of time in repetitive tasks and (ii) greater standardization in data extraction, as well as negative aspects, including (i) considerable effort to build reliable well-structured prompts, especially for less experienced users, since achieving effective prompts may require several iterations and testing, which can partially offset the expected time savings, (ii) the occurrence of hallucinations, and (iii) the need for constant manual verification. As a contribution, this work offers lessons learned and practical recommendations for researchers interested in adopting LLMs in systematic mappings and reviews, highlighting both efficiency gains and methodological risks and limitations to be considered.

</details>


### [2] [EvoCodeBench: A Human-Performance Benchmark for Self-Evolving LLM-Driven Coding Systems](https://arxiv.org/abs/2602.10171)
*Wentao Zhang,Jianfeng Wang,Liheng Liang,Yilei Zhao,HaiBin Wen,Zhe Zhao*

Main category: cs.SE

TL;DR: EvoCodeBench是一个用于评估自我进化LLM驱动编码系统的基准测试，它跟踪性能动态、比较人类表现，并支持多语言分析，超越了传统仅关注静态正确性的代码基准。


<details>
  <summary>Details</summary>
Motivation: 现有代码基准主要强调静态正确性，假设推理过程中模型能力固定，无法捕捉推理时的自我进化能力（如迭代改进解决方案时准确性和效率的提升）。同时缺乏对资源成本的考量，很少将模型性能与人类程序员进行校准，且主要关注高资源语言，忽略了跨语言鲁棒性和长尾语言稳定性。

Method: 开发EvoCodeBench基准测试，跟踪性能动态，测量解决方案正确性以及效率指标（如解决时间、内存消耗、改进算法设计）。直接比较模型与人类程序员在相同任务上的表现，支持多种编程语言，在统一协议下进行系统性的跨语言和长尾稳定性分析。

Result: 自我进化系统在效率方面随时间显示出可测量的提升，人类相对性和多语言分析提供了仅靠准确性无法获得的洞察。EvoCodeBench为评估进化中的LLM驱动系统的编码智能奠定了基础。

Conclusion: EvoCodeBench建立了一个评估进化LLM驱动系统中编码智能的基础框架，通过跟踪性能动态、人类相对比较和多语言分析，提供了比传统基准更全面的评估视角。

Abstract: As large language models (LLMs) continue to advance in programming tasks, LLM-driven coding systems have evolved from one-shot code generation into complex systems capable of iterative improvement during inference. However, existing code benchmarks primarily emphasize static correctness and implicitly assume fixed model capability during inference. As a result, they do not capture inference-time self-evolution, such as whether accuracy and efficiency improve as an agent iteratively refines its solutions. They also provide limited accounting of resource costs and rarely calibrate model performance against that of human programmers. Moreover, many benchmarks are dominated by high-resource languages, leaving cross-language robustness and long-tail language stability underexplored. Therefore, we present EvoCodeBench, a benchmark for evaluating self-evolving LLM-driven coding systems across programming languages with direct comparison to human performance. EvoCodeBench tracks performance dynamics, measuring solution correctness alongside efficiency metrics such as solving time, memory consumption, and improvement algorithmic design over repeated problem-solving attempts. To ground evaluation in a human-centered reference frame, we directly compare model performance with that of human programmers on the same tasks, enabling relative performance assessment within the human ability distribution. Furthermore, EvoCodeBench supports multiple programming languages, enabling systematic cross-language and long-tail stability analyses under a unified protocol. Our results demonstrate that self-evolving systems exhibit measurable gains in efficiency over time, and that human-relative and multi-language analyses provide insights unavailable through accuracy alone. EvoCodeBench establishes a foundation for evaluating coding intelligence in evolving LLM-driven systems.

</details>


### [3] [TestExplora: Benchmarking LLMs for Proactive Bug Discovery via Repository-Level Test Generation](https://arxiv.org/abs/2602.10471)
*Steven Liu,Jane Luo,Xin Zhang,Aofan Liu,Hao Liu,Jie Wu,Ziyang Huang,Yangyu Huang,Yu Kang,Scarlett Li*

Main category: cs.SE

TL;DR: TestExplora是一个评估LLMs作为主动测试者的基准，包含2389个任务，要求模型通过比较实现与文档意图来主动发现bug，结果显示当前最先进模型的最大F2P率仅为16.06%，表明LLMs在主动软件质量保证方面存在显著能力差距。


<details>
  <summary>Details</summary>
Motivation: 当前LLMs在软件自动化开发中的应用评估主要关注回归预防和反应性bug重现，但忽视了主动发现缺陷这一重要目标。现有评估要么将现有代码视为基准（合规陷阱），要么依赖故障后工件，很少能在故障发生前发现缺陷。

Method: 提出了TestExplora基准，包含2389个任务来自482个仓库，隐藏所有缺陷相关信号。模型必须通过比较实现与文档推导的意图来主动发现bug，使用文档作为oracle。采用连续、时间感知的数据收集方法以确保评估可持续并减少泄漏。

Result: 评估显示显著的能力差距：最先进模型的最大Fail-to-Pass（F2P）率仅为16.06%。进一步分析表明，导航复杂的跨模块交互和利用智能体探索对提升LLMs的自主软件质量保证能力至关重要。使用GPT-5-mini的SWEAgent实现了17.27%的F2P和29.7%的F2P@5。

Conclusion: TestExplora填补了LLMs作为主动测试者评估的空白，揭示了当前模型在主动bug发现方面的局限性。智能体探索在复杂软件环境中显示出潜力，但需要进一步研究跨模块交互和探索策略来提升LLMs的自主软件质量保证能力。

Abstract: Given that Large Language Models (LLMs) are increasingly applied to automate software development, comprehensive software assurance spans three distinct goals: regression prevention, reactive reproduction, and proactive discovery. Current evaluations systematically overlook the third goal. Specifically, they either treat existing code as ground truth (a compliance trap) for regression prevention, or depend on post-failure artifacts (e.g., issue reports) for bug reproduction-so they rarely surface defects before failures. To bridge this gap, we present TestExplora, a benchmark designed to evaluate LLMs as proactive testers within full-scale, realistic repository environments. TestExplora contains 2,389 tasks from 482 repositories and hides all defect-related signals. Models must proactively find bugs by comparing implementations against documentation-derived intent, using documentation as the oracle. Furthermore, to keep evaluation sustainable and reduce leakage, we propose continuous, time-aware data collection. Our evaluation reveals a significant capability gap: state-of-the-art models achieve a maximum Fail-to-Pass (F2P) rate of only 16.06%. Further analysis indicates that navigating complex cross-module interactions and leveraging agentic exploration are critical to advancing LLMs toward autonomous software quality assurance. Consistent with this, SWEAgent instantiated with GPT-5-mini achieves an F2P of 17.27% and an F2P@5 of 29.7%, highlighting the effectiveness and promise of agentic exploration in proactive bug discovery tasks.

</details>


### [4] [Theory of Troubleshooting: The Developer's Cognitive Experience of Overcoming Confusion](https://arxiv.org/abs/2602.10540)
*Arty Starr,Margaret-Anne Storey*

Main category: cs.SE

TL;DR: 本文基于认知科学提出了一种故障排除理论，解释了软件开发者在故障排除过程中面临的挑战和项目风险，通过访谈27位专业开发者构建了该理论。


<details>
  <summary>Details</summary>
Motivation: 故障排除是软件开发中特别紧张和消耗精力的部分，对注意力、工作记忆和心理建模有持续需求。现有研究缺乏从认知科学角度深入理解故障排除困难、疲劳和可持续性风险的理论基础。

Method: 采用建构主义扎根理论方法，访谈了27位专业开发者关于他们的故障排除经验，基于经验数据构建理论。

Result: 提出了一个基于认知科学的故障排除理论，该理论将故障排除定义为识别、理解和构建意外系统行为原因的心理模型的认知问题解决过程。理论解释了长期故障排除如何消耗认知资源并导致认知疲劳。

Conclusion: 该理论为理解故障排除困难、疲劳和可持续性风险提供了认知基础，对开发体验研究和工业实践都有实际意义，有助于解释故障排除困难时出现的项目风险。

Abstract: This paper introduces a Theory of Troubleshooting that is rooted in cognitive science. This theory helps software developers explain the challenges they face and the project risks that emerge as troubleshooting becomes difficult. We define troubleshooting as the cognitive problem-solving process of identifying, understanding, and constructing a mental model of the cause of an unexpected system behavior, and consider the cognitive process of troubleshooting to be an integral part of the activity of debugging. Troubleshooting is a particularly intense and draining aspect of software work, placing sustained demands on attention, working memory, and mental modeling. By surfacing and naming the confusion experience inherent in troubleshooting in terms of neurological and attentional dynamics, our theory explains how prolonged troubleshooting can deplete cognitive resources and lead to cognitive fatigue. In the study presented in this paper, we interview 27 professional developers about their troubleshooting experiences, and follow a Constructivist Grounded Theory approach to construct a theory grounded in empirical data. Our theory contributes to research on Developer Experience by providing a cognitive foundation for understanding troubleshooting difficulty, fatigue, and sustainability risk--and offers practical implications for both research and industry.

</details>


### [5] [Assessing Vision-Language Models for Perception in Autonomous Underwater Robotic Software](https://arxiv.org/abs/2602.10655)
*Muhammad Yousaf,Aitor Arrieta,Shaukat Ali,Paolo Arcaini,Shuai Wang*

Main category: cs.SE

TL;DR: 评估视觉语言模型在自主水下机器人感知模块中的性能、不确定性和可靠性，以帮助软件工程师选择合适的模型用于水下垃圾检测任务。


<details>
  <summary>Details</summary>
Motivation: 水下环境具有低能见度和恶劣条件等挑战，深度学习模型依赖稀缺且有噪声的标注数据，可能影响AUR软件的可靠性。视觉语言模型能泛化到未见物体并在噪声条件下保持鲁棒性，但其在水下环境中的性能和不确定性尚未从软件工程角度深入研究。

Method: 对基于VLM的AUR软件感知模块进行实证评估，通过计算性能、不确定性及其关系来评估它们检测水下垃圾的能力。

Result: 研究提供了VLM在水下环境中的性能、不确定性和可靠性评估结果，为软件工程师选择适合AUR软件的VLM提供了依据。

Conclusion: 视觉语言模型在水下环境中展现出潜力，但需要从软件工程角度系统评估其性能和不确定性，以确保AUR软件的可靠性和可信度。

Abstract: Autonomous Underwater Robots (AURs) operate in challenging underwater environments, including low visibility and harsh water conditions. Such conditions present challenges for software engineers developing perception modules for the AUR software. To successfully carry out these tasks, deep learning has been incorporated into the AUR software to support its operations. However, the unique challenges of underwater environments pose difficulties for deep learning models, which often rely on labeled data that is scarce and noisy. This may undermine the trustworthiness of AUR software that relies on perception modules. Vision-Language Models (VLMs) offer promising solutions for AUR software as they generalize to unseen objects and remain robust in noisy conditions by inferring information from contextual cues. Despite this potential, their performance and uncertainty in underwater environments remain understudied from a software engineering perspective. Motivated by the needs of an industrial partner in assurance and risk management for maritime systems to assess the potential use of VLMs in this context, we present an empirical evaluation of VLM-based perception modules within the AUR software. We assess their ability to detect underwater trash by computing performance, uncertainty, and their relationship, to enable software engineers to select appropriate VLMs for their AUR software.

</details>


### [6] [Hidden Licensing Risks in the LLMware Ecosystem](https://arxiv.org/abs/2602.10758)
*Bo Wang,Yueyang Chen,Jieke Shi,Minghui Li,Yunbo Lyu,Yinan Wu,Youfang Lin,Zhou Yang*

Main category: cs.SE

TL;DR: 该研究分析了LLMware（集成大语言模型的软件系统）中的许可证问题，发现其许可证分布与传统开源软件生态不同，现有检测方法效果有限，提出了基于LLM的LiAgent框架，显著提升了许可证兼容性分析的准确性。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型越来越多地集成到软件系统中，形成了LLMware这类新系统，它们依赖复杂的开源软件、模型和数据集供应链。然而，这些相互交织的依赖关系带来的许可证问题尚未得到充分研究。

Method: 研究从GitHub和Hugging Face收集了大规模LLMware供应链数据集（12,180个开源仓库、3,988个LLM、708个数据集），分析了许可证分布和讨论，评估了现有检测方法，并提出了基于LLM的LiAgent框架进行生态系统级别的许可证兼容性分析。

Result: 研究发现LLMware的许可证分布与传统开源软件生态显著不同；许可证选择和维护是主要关注点（占84%）；现有检测方法在LLMware环境中效果有限（F1分数仅58%和76%）；提出的LiAgent框架达到87%的F1分数，提升了14个百分点；检测到60个不兼容问题，其中11个已获开发者确认。

Conclusion: LLMware生态系统面临独特的许可证挑战，现有检测方法效果不足。LiAgent框架显著提升了许可证兼容性分析的准确性，有助于支持LLMware生态的可持续发展。研究还发现两个存在冲突的LLM下载量巨大（1.07亿和500万），可能对下游产生广泛影响。

Abstract: Large Language Models (LLMs) are increasingly integrated into software systems, giving rise to a new class of systems referred to as LLMware. Beyond traditional source-code components, LLMware embeds or interacts with LLMs that depend on other models and datasets, forming complex supply chains across open-source software (OSS), models, and datasets. However, licensing issues emerging from these intertwined dependencies remain largely unexplored. Leveraging GitHub and Hugging Face, we curate a large-scale dataset capturing LLMware supply chains, including 12,180 OSS repositories, 3,988 LLMs, and 708 datasets. Our analysis reveals that license distributions in LLMware differ substantially from traditional OSS ecosystems. We further examine license-related discussions and find that license selection and maintenance are the dominant concerns, accounting for 84% of cases. To understand incompatibility risks, we analyze license conflicts along supply chains and evaluate state-of-the-art detection approaches, which achieve only 58% and 76% F1 scores in this setting. Motivated by these limitations, we propose LiAgent, an LLM-based agent framework for ecosystem-level license compatibility analysis. LiAgent achieves an F1 score of 87%, improving performance by 14 percentage points over prior methods. We reported 60 incompatibility issues detected by LiAgent, 11 of which have been confirmed by developers. Notably, two conflicted LLMs have over 107 million and 5 million downloads on Hugging Face, respectively, indicating potentially widespread downstream impact. We conclude with implications and recommendations to support the sustainable growth of the LLMware ecosystem.

</details>


### [7] [VulReaD: Knowledge-Graph-guided Software Vulnerability Reasoning and Detection](https://arxiv.org/abs/2602.10787)
*Samal Mukhtar,Yinghua Yao,Zhu Sun,Mustafa Mustafa,Yew Soon Ong,Youcheng Sun*

Main category: cs.SE

TL;DR: VulReaD是一个基于知识图谱的漏洞检测方法，通过安全知识图谱作为语义骨干，利用教师LLM生成CWE一致的对比推理监督，训练学生模型进行漏洞推理和检测，超越了传统的二元分类。


<details>
  <summary>Details</summary>
Motivation: 当前软件漏洞检测主要关注二元评估，而大语言模型提供的解释往往缺乏与CWE类别的语义一致性。需要一种能够进行CWE级别推理的方法，提高漏洞检测的准确性和可解释性。

Method: 提出VulReaD方法：1) 使用安全知识图谱作为语义骨干；2) 利用强大的教师LLM生成CWE一致的对比推理监督；3) 训练学生模型时无需人工标注；4) 使用Odds Ratio Preference Optimization微调学生模型，鼓励符合分类学的推理，抑制无支持的说明。

Result: 在三个真实世界数据集上，VulReaD相比最先进的基线方法：二元F1提高了8-10%，多类分类的Macro-F1提高了30%，Micro-F1提高了18%。结果显示LLM在二元检测中优于深度学习基线，KG引导的推理增强了CWE覆盖率和可解释性。

Conclusion: VulReaD通过知识图谱引导的漏洞推理方法，成功超越了传统的二元分类，实现了CWE级别的漏洞检测，显著提高了检测性能和可解释性，为软件漏洞检测提供了新的有效途径。

Abstract: Software vulnerability detection (SVD) is a critical challenge in modern systems. Large language models (LLMs) offer natural-language explanations alongside predictions, but most work focuses on binary evaluation, and explanations often lack semantic consistency with Common Weakness Enumeration (CWE) categories. We propose VulReaD, a knowledge-graph-guided approach for vulnerability reasoning and detection that moves beyond binary classification toward CWE-level reasoning. VulReaD leverages a security knowledge graph (KG) as a semantic backbone and uses a strong teacher LLM to generate CWE-consistent contrastive reasoning supervision, enabling student model training without manual annotations. Students are fine-tuned with Odds Ratio Preference Optimization (ORPO) to encourage taxonomy-aligned reasoning while suppressing unsupported explanations. Across three real-world datasets, VulReaD improves binary F1 by 8-10% and multi-class classification by 30% Macro-F1 and 18% Micro-F1 compared to state-of-the-art baselines. Results show that LLMs outperform deep learning baselines in binary detection and that KG-guided reasoning enhances CWE coverage and interpretability.

</details>


### [8] [PELLI: Framework to effectively integrate LLMs for quality software generation](https://arxiv.org/abs/2602.10808)
*Rasmus Krebs,Somnath Mazumdar*

Main category: cs.SE

TL;DR: 本文提出了PELLI框架，这是一个基于迭代分析的代码质量评估框架，用于全面评估LLM生成的代码质量，重点关注可维护性、性能和可靠性三个非功能性需求。


<details>
  <summary>Details</summary>
Motivation: 现有研究在比较LLM生成的代码质量时存在两个主要问题：1) 主要只考虑可靠性作为比较指标；2) 只选择少数LLM（如Codex和ChatGPT）进行比较。需要更全面的评估框架来指导开发者在实际应用中有效利用LLM。

Method: 提出了PELLI（Programmatic Excellence via LLM Iteration）框架，这是一个基于迭代分析的代码质量评估过程。研究扩展了现有技术，通过生成定量指标来全面评估三个主要非功能性需求（可维护性、性能、可靠性），同时选择了五个流行的LLM。为了验证PELLI的适用性，选择了三个应用领域并遵循Python编码标准。

Result: 基于三个非功能性需求的评估发现：1) GPT-4T和Gemini表现略好；2) 提示设计会影响整体代码质量；3) 每个应用领域在不同指标上表现出高低不同的分数，即使在同一指标上，不同提示也会产生差异。

Conclusion: PELLI可以作为开发者的实用指南，帮助他们在遵循公认质量标准的同时有效利用LLM。该框架确保LLM与人类开发者之间的和谐集成，充分发挥LLM的潜力。研究结果对于推进LLM技术在实际应用中的发展至关重要，为利益相关者提供了清晰的指导。

Abstract: Recent studies have revealed that when LLMs are appropriately prompted and configured, they demonstrate mixed results. Such results often meet or exceed the baseline performance. However, these comparisons have two primary issues. First, they mostly considered only reliability as a comparison metric and selected a few LLMs (such as Codex and ChatGPT) for comparision. This paper proposes a comprehensive code quality assessment framework called Programmatic Excellence via LLM Iteration (PELLI). PELLI is an iterative analysis-based process that upholds high-quality code changes. We extended the state-of-the-art by performing a comprehensive evaluation that generates quantitative metrics for analyzing three primary nonfunctional requirements (such as maintainability, performance, and reliability) while selecting five popular LLMs. For PELLI's applicability, we selected three application domains while following Python coding standards. Following this framework, practitioners can ensure harmonious integration between LLMs and human developers, ensuring that their potential is fully realized. PELLI can serve as a practical guide for developers aiming to leverage LLMs while adhering to recognized quality standards. This study's outcomes are crucial for advancing LLM technologies in real-world applications, providing stakeholders with a clear understanding of where these LLMs excel and where they require further refinement. Overall, based on three nonfunctional requirements, we have found that GPT-4T and Gemini performed slightly better. We also found that prompt design can influence the overall code quality. In addition, each application domain demonstrated high and low scores across various metrics, and even within the same metrics across different prompts.

</details>


### [9] [Deriving and Validating Requirements Engineering Principles for Large-Scale Agile Development: An Industrial Longitudinal Study](https://arxiv.org/abs/2602.10972)
*Hina Saeeda,Mijin Kim,Eric Knauss,Jesper Thyssen,Jesper Ørting,Jesper Lysemose Korsgaard,Niels Jørgen Strøm*

Main category: cs.SE

TL;DR: 该研究通过5年纵向案例研究，为大规模敏捷系统开发提出了6个可转移的需求工程原则，并在多家跨国公司进行了验证。


<details>
  <summary>Details</summary>
Motivation: 大规模敏捷系统开发中缺乏统一的需求工程流程和高层次指导原则，导致需求管理面临重大挑战，需要建立可扩展的实践框架。

Method: 采用5年纵向案例研究，通过超过25个冲刺、320次周同步会议、7次跨公司研讨会收集定性数据，使用主题分析提炼原则，并通过焦点小组和跨公司专家评估进行验证。

Result: 识别出6个关键需求工程原则：架构上下文、利益相关者驱动的验证与对齐、轻量级文档的需求演进、委托式需求管理、组织角色与职责、需求共享理解。

Conclusion: 研究为大规模敏捷组织提供了可扩展、可适应的需求实践基础，这些原则经过多家跨国公司验证，具有实际应用价值。

Abstract: In large scale agile systems development, the lack of a unified requirements engineering (RE) process is a major challenge, exacerbated by the absence of high level guiding principles for effective requirements management. To address this challenge, we conducted a five year longitudinal case study with Grundfos AB, in collaboration with the Software Centre in Sweden. RE principles were first derived through qualitative data collection spanning more than 25 sprints, approximately 320 weekly synchronisation meetings, and seven cross-company, company-specific workshops between 2019 and 2024. These activities engaged practitioners from diverse roles, representing several hundred developers across domains. In late 2024, five in depth focus groups with senior leaders at Grundfos provided retrospective validation of the principles and assessed their strategic impact. We aim to (1) empirically examine RE principles in large scale agile system development, (2) explore their benefits in practice within the case company, and (3) identify a set of transferable RE principles for large scale contexts. Using thematic analysis, six key RE principles architectural context, stakeholder-driven validation and alignment, requirements practices in large-scale agile organisations. evolution with lightweight documentation, delegated requirements management, organisational roles and responsibilities, and a shared understanding of requirements are derived. The study was further validated through crosscompany expert evaluation with three additional multinational organisations (Bosch, Ericsson, and Volvo Cars), which are directly responsible for largescale requirements management. Together, these efforts provide a scalable and adaptable foundation for improving requirements practices in largescale agile organisations.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [10] [LiveMedBench: A Contamination-Free Medical Benchmark for LLMs with Automated Rubric Evaluation](https://arxiv.org/abs/2602.10367)
*Zhiling Yan,Dingjie Song,Zhe Fang,Yisheng Ji,Xiang Li,Quanzheng Li,Lichao Sun*

Main category: cs.AI

TL;DR: LiveMedBench是一个持续更新的医学基准测试，通过每周收集真实临床病例解决数据污染和时间错位问题，使用多智能体框架进行数据清洗和基于规则的评估，发现LLMs在临床应用中存在显著性能瓶颈。


<details>
  <summary>Details</summary>
Motivation: 现有医学基准测试存在两个关键局限：1）数据污染问题，测试集可能泄露到训练数据中导致性能虚高；2）时间错位问题，无法捕捉医学知识的快速演变。此外，当前评估指标要么依赖浅层词汇重叠，要么依赖主观的LLM评分，都不足以验证临床正确性。

Method: 提出LiveMedBench基准测试，每周从在线医学社区收集真实临床病例，确保与模型训练数据的严格时间分离。采用多智能体临床管理框架过滤原始数据噪声，并基于循证医学原则验证临床完整性。开发自动化基于规则的评估框架，将医生回答分解为细粒度的病例特定标准。

Result: LiveMedBench包含2,756个真实病例，涵盖38个医学专业和多种语言，配有16,702个独特评估标准。对38个LLMs的评估显示，最佳模型仅达到39.2%的准确率，84%的模型在截止日期后的病例上表现下降，证实了普遍的数据污染风险。错误分析表明35-48%的失败源于无法将医学知识适应到患者特定约束。

Conclusion: LiveMedBench提供了一个持续更新、无数据污染的医学基准测试，揭示了LLMs在临床推理中的主要瓶颈不是事实知识，而是将知识适应到具体患者情境的能力。该基准有助于更准确地评估LLMs在临床环境中的实际能力。

Abstract: The deployment of Large Language Models (LLMs) in high-stakes clinical settings demands rigorous and reliable evaluation. However, existing medical benchmarks remain static, suffering from two critical limitations: (1) data contamination, where test sets inadvertently leak into training corpora, leading to inflated performance estimates; and (2) temporal misalignment, failing to capture the rapid evolution of medical knowledge. Furthermore, current evaluation metrics for open-ended clinical reasoning often rely on either shallow lexical overlap (e.g., ROUGE) or subjective LLM-as-a-Judge scoring, both inadequate for verifying clinical correctness. To bridge these gaps, we introduce LiveMedBench, a continuously updated, contamination-free, and rubric-based benchmark that weekly harvests real-world clinical cases from online medical communities, ensuring strict temporal separation from model training data. We propose a Multi-Agent Clinical Curation Framework that filters raw data noise and validates clinical integrity against evidence-based medical principles. For evaluation, we develop an Automated Rubric-based Evaluation Framework that decomposes physician responses into granular, case-specific criteria, achieving substantially stronger alignment with expert physicians than LLM-as-a-Judge. To date, LiveMedBench comprises 2,756 real-world cases spanning 38 medical specialties and multiple languages, paired with 16,702 unique evaluation criteria. Extensive evaluation of 38 LLMs reveals that even the best-performing model achieves only 39.2%, and 84% of models exhibit performance degradation on post-cutoff cases, confirming pervasive data contamination risks. Error analysis further identifies contextual application-not factual knowledge-as the dominant bottleneck, with 35-48% of failures stemming from the inability to tailor medical knowledge to patient-specific constraints.

</details>


### [11] [Found-RL: foundation model-enhanced reinforcement learning for autonomous driving](https://arxiv.org/abs/2602.10458)
*Yansong Qu,Zihao Sheng,Zilin Huang,Jiancong Chen,Yuhao Luo,Tianyi Wang,Yiheng Feng,Samuel Labi,Sikai Chen*

Main category: cs.AI

TL;DR: Found-RL是一个专门为自动驾驶设计的强化学习平台，通过异步批量推理框架解决视觉语言模型在RL训练中的延迟问题，并引入多种监督机制将VLM知识蒸馏到轻量级RL模型中。


<details>
  <summary>Details</summary>
Motivation: 强化学习在自动驾驶中存在样本效率低和语义可解释性不足的问题，而基础模型（特别是视觉语言模型）虽然能提供丰富的上下文感知知识，但其高推理延迟阻碍了在高频RL训练循环中的部署。

Method: 1. 异步批量推理框架：将繁重的VLM推理与仿真循环解耦，解决延迟瓶颈；2. 引入多种监督机制：值边界正则化和优势加权动作指导，将VLM专家动作建议蒸馏到RL策略中；3. 采用高吞吐量CLIP进行密集奖励塑造，并通过条件对比动作对齐解决CLIP的动态盲区问题。

Result: Found-RL提供了一个端到端的微调VLM集成管道，轻量级RL模型能够实现接近十亿参数VLM的性能，同时保持实时推理（约500 FPS）。

Conclusion: Found-RL平台成功解决了VLM在RL训练中的延迟问题，通过高效的监督机制实现了知识蒸馏，使轻量级RL模型在自动驾驶任务中既能获得VLM的语义理解能力，又能保持实时性能。

Abstract: Reinforcement Learning (RL) has emerged as a dominant paradigm for end-to-end autonomous driving (AD). However, RL suffers from sample inefficiency and a lack of semantic interpretability in complex scenarios. Foundation Models, particularly Vision-Language Models (VLMs), can mitigate this by offering rich, context-aware knowledge, yet their high inference latency hinders deployment in high-frequency RL training loops. To bridge this gap, we present Found-RL, a platform tailored to efficiently enhance RL for AD using foundation models. A core innovation is the asynchronous batch inference framework, which decouples heavy VLM reasoning from the simulation loop, effectively resolving latency bottlenecks to support real-time learning. We introduce diverse supervision mechanisms: Value-Margin Regularization (VMR) and Advantage-Weighted Action Guidance (AWAG) to effectively distill expert-like VLM action suggestions into the RL policy. Additionally, we adopt high-throughput CLIP for dense reward shaping. We address CLIP's dynamic blindness via Conditional Contrastive Action Alignment, which conditions prompts on discretized speed/command and yields a normalized, margin-based bonus from context-specific action-anchor scoring. Found-RL provides an end-to-end pipeline for fine-tuned VLM integration and shows that a lightweight RL model can achieve near-VLM performance compared with billion-parameter VLMs while sustaining real-time inference (approx. 500 FPS). Code, data, and models will be publicly available at https://github.com/ys-qu/found-rl.

</details>


### [12] [MERIT Feedback Elicits Better Bargaining in LLM Negotiators](https://arxiv.org/abs/2602.10467)
*Jihwan Oh,Murad Aghazada,Yooju Shin,Se-Young Yun,Taehyeon Kim*

Main category: cs.AI

TL;DR: 提出AgoraBench基准测试和基于效用反馈的框架，通过人类偏好对齐的指标和数据集提升LLM在复杂谈判场景中的表现


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在谈判场景中仍存在战略深度不足和难以适应复杂人类因素的局限，现有基准测试未能充分捕捉这些限制

Method: 提出效用反馈中心框架：1) AgoraBench基准测试，涵盖9个挑战性场景；2) 基于效用理论的人类对齐经济指标；3) 人类偏好数据集和学习管道

Result: 基线LLM策略常偏离人类偏好，而提出的机制显著提升谈判性能，产生更深层战略行为和更强的对手意识

Conclusion: 通过效用反馈框架和人类偏好对齐方法，能够有效增强LLM在复杂谈判场景中的战略能力和适应性

Abstract: Bargaining is often regarded as a logical arena rather than an art or a matter of intuition, yet Large Language Models (LLMs) still struggle to navigate it due to limited strategic depth and difficulty adapting to complex human factors. Current benchmarks rarely capture this limitation. To bridge this gap, we present an utility feedback centric framework. Our contributions are: (i) AgoraBench, a new benchmark spanning nine challenging settings (e.g., deception, monopoly) that supports diverse strategy modeling; (ii) human-aligned, economically grounded metrics derived from utility theory. This is operationalized via agent utility, negotiation power, and acquisition ratio that implicitly measure how well the negotiation aligns with human preference and (iii) a human preference grounded dataset with learning pipeline that strengthens LLMs' bargaining ability through both prompting and finetuning. Empirical results indicate that baseline LLM strategies often diverge from human preferences, while our mechanism substantially improves negotiation performance, yielding deeper strategic behavior and stronger opponent awareness.

</details>


### [13] [Abstraction Generation for Generalized Planning with Pretrained Large Language Models](https://arxiv.org/abs/2602.10485)
*Zhenhe Cui,Huaxiang Xia,Hangjun Shen,Kailun Luo,Yong He,Wei Liang*

Main category: cs.AI

TL;DR: LLMs作为QNP抽象生成器用于广义规划，通过自动调试修复抽象错误


<details>
  <summary>Details</summary>
Motivation: 研究LLMs能否作为QNP抽象生成器为广义规划问题生成抽象特征，并解决抽象错误的自动修复问题

Method: 提出提示协议：输入GP领域和训练任务给LLMs，让它们生成抽象特征并将初始状态、动作集和目标抽象为QNP问题；设计自动调试方法检测抽象错误，指导LLMs修复抽象

Result: 实验表明，在自动调试的适当指导下，某些LLMs能够生成有用的QNP抽象

Conclusion: LLMs可以作为QNP抽象生成器，结合自动调试方法能够有效生成和修复抽象，为广义规划提供新的解决方案

Abstract: Qualitative Numerical Planning (QNP) serves as an important abstraction model for generalized planning (GP), which aims to compute general plans that solve multiple instances at once. Recent works show that large language models (LLMs) can function as generalized planners. This work investigates whether LLMs can serve as QNP abstraction generators for GP problems and how to fix abstractions via automated debugging. We propose a prompt protocol: input a GP domain and training tasks to LLMs, prompting them to generate abstract features and further abstract the initial state, action set, and goal into QNP problems. An automated debugging method is designed to detect abstraction errors, guiding LLMs to fix abstractions. Experiments demonstrate that under properly guided by automated debugging, some LLMs can generate useful QNP abstractions.

</details>


### [14] [Flow of Spans: Generalizing Language Models to Dynamic Span-Vocabulary via GFlowNets](https://arxiv.org/abs/2602.10583)
*Bo Xue,Yunchong Song,Fanghao Shao,Xuekai Zhu,Lin Chen,Luoyi Fu,Xinbing Wang,Zhouhan Lin*

Main category: cs.AI

TL;DR: FoSS提出基于GFlowNets的跨度生成框架，通过动态跨度词汇和DAG状态空间提升文本生成的多样性和质量


<details>
  <summary>Details</summary>
Motivation: 传统自回归语言模型使用固定词汇表，形成树状状态空间，限制了灵活性和表达能力。现有动态词汇方法虽然引入检索文本跨度，但忽略了同一句子可由不同长度跨度组成，缺乏对DAG状态空间的显式建模，导致组合路径探索受限且存在路径选择偏差

Method: 提出Flow of SpanS (FoSS)框架：1）通过灵活分割检索文本构建动态跨度词汇表；2）确保DAG结构的状态空间；3）利用GFlowNets探索多样组合路径；4）结合专用奖励模型生成高质量文本

Result: FoSS在文本生成任务上比Transformer提升MAUVE分数达12.5%，在知识密集型任务上获得3.5%增益，持续优于最先进方法。扩展实验表明FoSS受益于更大模型、更多数据和更丰富的检索语料库

Conclusion: FoSS通过将GFlowNets应用于跨度生成，构建DAG状态空间，有效解决了传统方法在组合路径探索和泛化方面的限制，显著提升了文本生成的多样性和质量

Abstract: Standard autoregressive language models generate text token-by-token from a fixed vocabulary, inducing a tree-structured state space when viewing token sampling as an action, which limits flexibility and expressiveness. Recent work introduces dynamic vocabulary by sampling retrieved text spans but overlooks that the same sentence can be composed of spans of varying lengths, lacking explicit modeling of the directed acyclic graph (DAG) state space. This leads to restricted exploration of compositional paths and is biased toward the chosen path. Generative Flow Networks (GFlowNets) are powerful for efficient exploring and generalizing over state spaces, particularly those with a DAG structure. However, prior GFlowNets-based language models operate at the token level and remain confined to tree-structured spaces, limiting their potential. In this work, we propose Flow of SpanS (FOSS), a principled GFlowNets framework for span generation. FoSS constructs a dynamic span vocabulary by segmenting the retrieved text flexibly, ensuring a DAG-structured state space, which allows GFlowNets to explore diverse compositional paths and improve generalization. With specialized reward models, FoSS generates diverse, high-quality text. Empirically, FoSS improves MAUVE scores by up to 12.5% over Transformer on text generation and achieves 3.5% gains on knowledge-intensive tasks, consistently outperforming state-of-the-art methods. Scaling experiments further demonstrate FoSS benefits from larger models, more data, and richer retrieval corpora, retaining its advantage over strong baselines.

</details>


### [15] [Neuro-symbolic Action Masking for Deep Reinforcement Learning](https://arxiv.org/abs/2602.10598)
*Shuai Han,Mehdi Dastani,Shihan Wang*

Main category: cs.AI

TL;DR: NSAM是一种神经符号动作屏蔽框架，能自动学习与高维状态约束一致的符号模型，在DRL训练中减少不可行动作探索并提高样本效率


<details>
  <summary>Details</summary>
Motivation: 现有方法需要手动指定符号接地函数和动作屏蔽技术来约束DRL中的不可行动作，这限制了方法的自动化程度和可扩展性

Method: 提出神经符号动作屏蔽(NSAM)框架，在最小监督下自动学习与领域约束一致的符号模型，基于学习到的符号状态模型生成动作屏蔽，排除不可行动作，实现符号推理与深度策略优化的端到端集成

Result: 在多个约束领域上的实验表明，NSAM显著提高了DRL智能体的样本效率，同时大幅减少了约束违反

Conclusion: NSAM通过自动学习符号模型和动作屏蔽，实现了符号推理与深度学习的协同增强，为约束环境下的DRL提供了有效的解决方案

Abstract: Deep reinforcement learning (DRL) may explore infeasible actions during training and execution. Existing approaches assume a symbol grounding function that maps high-dimensional states to consistent symbolic representations and a manually specified action masking techniques to constrain actions. In this paper, we propose Neuro-symbolic Action Masking (NSAM), a novel framework that automatically learn symbolic models, which are consistent with given domain constraints of high-dimensional states, in a minimally supervised manner during the DRL process. Based on the learned symbolic model of states, NSAM learns action masks that rules out infeasible actions. NSAM enables end-to-end integration of symbolic reasoning and deep policy optimization, where improvements in symbolic grounding and policy learning mutually reinforce each other. We evaluate NSAM on multiple domains with constraints, and experimental results demonstrate that NSAM significantly improves sample efficiency of DRL agent while substantially reducing constraint violations.

</details>


### [16] [To Think or Not To Think, That is The Question for Large Reasoning Models in Theory of Mind Tasks](https://arxiv.org/abs/2602.10625)
*Nanxu Gong,Haotian Li,Sixun Dong,Jianxun Lian,Yanjie Fu,Xing Xie*

Main category: cs.AI

TL;DR: 研究发现大型推理模型在心理理论任务上表现不佳，推理能力无法完全迁移到社会认知领域，存在"慢思考崩溃"、依赖选项匹配等问题。


<details>
  <summary>Details</summary>
Motivation: 虽然大型推理模型在数学和编程等正式推理任务上取得了进步，但这些能力是否能够迁移到社会认知技能（如心理理论）尚不明确，需要系统研究。

Method: 对9个先进大型语言模型进行系统研究，比较推理模型与非推理模型在三个代表性心理理论基准测试上的表现，并进行细粒度分析。

Result: 推理模型在心理理论任务上并不总是优于非推理模型，有时表现更差。发现慢思考崩溃、适度自适应推理有益、选项匹配捷径三个关键现象。

Conclusion: 大型推理模型在正式推理任务上的进步无法完全迁移到心理理论等社会推理任务，实现稳健的心理理论需要超越现有推理方法的独特能力。

Abstract: Theory of Mind (ToM) assesses whether models can infer hidden mental states such as beliefs, desires, and intentions, which is essential for natural social interaction. Although recent progress in Large Reasoning Models (LRMs) has boosted step-by-step inference in mathematics and coding, it is still underexplored whether this benefit transfers to socio-cognitive skills. We present a systematic study of nine advanced Large Language Models (LLMs), comparing reasoning models with non-reasoning models on three representative ToM benchmarks. The results show that reasoning models do not consistently outperform non-reasoning models and sometimes perform worse. A fine-grained analysis reveals three insights. First, slow thinking collapses: accuracy significantly drops as responses grow longer, and larger reasoning budgets hurt performance. Second, moderate and adaptive reasoning benefits performance: constraining reasoning length mitigates failure, while distinct success patterns demonstrate the necessity of dynamic adaptation. Third, option matching shortcut: when multiple choice options are removed, reasoning models improve markedly, indicating reliance on option matching rather than genuine deduction. We also design two intervention approaches: Slow-to-Fast (S2F) adaptive reasoning and Think-to-Match (T2M) shortcut prevention to further verify and mitigate the problems. With all results, our study highlights the advancement of LRMs in formal reasoning (e.g., math, code) cannot be fully transferred to ToM, a typical task in social reasoning. We conclude that achieving robust ToM requires developing unique capabilities beyond existing reasoning methods.

</details>


### [17] [OmniSapiens: A Foundation Model for Social Behavior Processing via Heterogeneity-Aware Relative Policy Optimization](https://arxiv.org/abs/2602.10635)
*Keane Ong,Sabri Boughorbel,Luwei Xiao,Chanakya Ekbote,Wei Dai,Ao Qu,Jingyao Wu,Rui Mao,Ehsan Hoque,Erik Cambria,Gianmarco Mengaldo,Paul Pu Liang*

Main category: cs.AI

TL;DR: HARPO是一种强化学习方法，通过调节优势函数来平衡异构任务和样本的学习，开发了Omnisapiens-7B 2.0社交行为基础模型，在多项任务上表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常孤立地建模人类行为维度（情感、认知或社会属性），任务特定建模增加了训练成本并限制了跨行为设置的泛化能力。虽然最近的推理RL方法可以在多个行为任务上训练统一模型，但没有明确解决跨异构行为数据的学习问题。

Method: 提出了异构感知相对策略优化（HARPO），这是一种RL方法，通过调节优势函数来确保在策略优化过程中没有任何单个任务或样本产生不成比例的影响，从而平衡跨异构任务和样本的学习。

Result: 使用HARPO开发了Omnisapiens-7B 2.0社交行为处理基础模型。相对于现有行为基础模型，在多任务和保留设置上分别获得高达+16.85%和+9.37%的性能提升，同时产生更明确和鲁棒的推理轨迹。HARPO在行为任务上也比最近的RL方法表现更一致。

Conclusion: HARPO方法有效解决了跨异构行为数据的学习平衡问题，开发的Omnisapiens-7B 2.0模型在社交行为处理任务上表现出色，为开发社交智能AI提供了有效工具。

Abstract: To develop socially intelligent AI, existing approaches typically model human behavioral dimensions (e.g., affective, cognitive, or social attributes) in isolation. Although useful, task-specific modeling often increases training costs and limits generalization across behavioral settings. Recent reasoning RL methods facilitate training a single unified model across multiple behavioral tasks, but do not explicitly address learning across different heterogeneous behavioral data. To address this gap, we introduce Heterogeneity-Aware Relative Policy Optimization (HARPO), an RL method that balances leaning across heterogeneous tasks and samples. This is achieved by modulating advantages to ensure that no single task or sample carries disproportionate influence during policy optimization. Using HARPO, we develop and release Omnisapiens-7B 2.0, a foundation model for social behavior processing. Relative to existing behavioral foundation models, Omnisapiens-7B 2.0 achieves the strongest performance across behavioral tasks, with gains of up to +16.85% and +9.37% on multitask and held-out settings respectively, while producing more explicit and robust reasoning traces. We also validate HARPO against recent RL methods, where it achieves the most consistently strong performance across behavioral tasks.

</details>


### [18] [Spend Search Where It Pays: Value-Guided Structured Sampling and Optimization for Generative Recommendation](https://arxiv.org/abs/2602.10699)
*Jie Jiang,Yangru Huang,Zeyu Wang,Changping Wang,Yuling Xiong,Jun Zhang,Huan Yu*

Main category: cs.AI

TL;DR: V-STAR框架解决生成式推荐中RL训练的概率-奖励不匹配问题，通过价值引导采样和树状优势强化提升探索效率和决策信号


<details>
  <summary>Details</summary>
Motivation: 传统基于似然的解码方法存在近视偏差，导致两个关键问题：1) 探索不足，高奖励但低概率分支被过早剪枝；2) 优势压缩，共享高概率前缀的轨迹奖励高度相关，RL学习信号弱

Method: 提出V-STAR框架，包含两个协同组件：1) 价值引导高效解码(VED)，识别关键节点并选择性加深高潜力前缀；2) Sibling-GRPO，利用树状拓扑计算兄弟相对优势，聚焦学习信号于关键分支决策

Result: 在离线和在线数据集上的广泛实验表明，V-STAR优于最先进的基线方法，在严格延迟约束下提供更优的准确性和候选集多样性

Conclusion: V-STAR通过解决概率-奖励不匹配问题，有效提升了生成式推荐中RL训练的效果，实现了更好的探索和决策信号

Abstract: Generative recommendation via autoregressive models has unified retrieval and ranking into a single conditional generation framework. However, fine-tuning these models with Reinforcement Learning (RL) often suffers from a fundamental probability-reward mismatch. Conventional likelihood-dominated decoding (e.g., beam search) exhibits a myopic bias toward locally probable prefixes, which causes two critical failures: (1) insufficient exploration, where high-reward items in low-probability branches are prematurely pruned and rarely sampled, and (2) advantage compression, where trajectories sharing high-probability prefixes receive highly correlated rewards with low within-group variance, yielding a weak comparative signal for RL. To address these challenges, we propose V-STAR, a Value-guided Sampling and Tree-structured Advantage Reinforcement framework. V-STAR forms a self-evolving loop via two synergistic components. First, a Value-Guided Efficient Decoding (VED) is developed to identify decisive nodes and selectively deepen high-potential prefixes. This improves exploration efficiency without exhaustive tree search. Second, we propose Sibling-GRPO, which exploits the induced tree topology to compute sibling-relative advantages and concentrates learning signals on decisive branching decisions. Extensive experiments on both offline and online datasets demonstrate that V-STAR outperforms state-of-the-art baselines, delivering superior accuracy and candidate-set diversity under strict latency constraints.

</details>


### [19] [Integrating Generative AI-enhanced Cognitive Systems in Higher Education: From Stakeholder Perceptions to a Conceptual Framework considering the EU AI Act](https://arxiv.org/abs/2602.10802)
*Da-Lun Chen,Prasasthy Balasubramanian,Lauri Lovén,Susanna Pirttikangas,Jaakko Sauvola,Panagiotis Kostakos*

Main category: cs.AI

TL;DR: 该研究调查了高等教育中生成式人工智能的认知，通过混合方法分析ITEE领域师生对GenAI的看法，识别了共同和学科特定的主题，并提出了负责任整合的框架。


<details>
  <summary>Details</summary>
Motivation: 高等教育中师生已广泛采用生成式AI工具，但利益相关者对GenAI的看法存在分歧，受文化、学科和制度背景影响。欧盟AI法案要求大学确保认知系统的监管合规性，因此需要了解不同学科对GenAI的认知，以便负责任地整合。

Method: 采用混合方法，调查了奥卢大学ITEE学院的61名教职员工和37名学生，通过问卷调查收集数据，分析师生对GenAI的看法和关注点。

Result: 研究发现师生对GenAI有共同和学科特定的主题：强烈关注编程支持，同时担忧回答质量、隐私和学术诚信问题。研究识别了高层次需求，并提出了负责任GenAI整合的概念框架。

Conclusion: 学科特定需求强调了利益相关者参与的重要性。高层次需求和框架为大学利用GenAI提供了实用指导，同时解决利益相关者关切并确保监管合规。研究强调了定制化整合策略的必要性。

Abstract: Many staff and students in higher education have adopted generative artificial intelligence (GenAI) tools in their work and study. GenAI is expected to enhance cognitive systems by enabling personalized learning and streamlining educational services. However, stakeholders perceptions of GenAI in higher education remain divided, shaped by cultural, disciplinary, and institutional contexts. In addition, the EU AI Act requires universities to ensure regulatory compliance when deploying cognitive systems. These developments highlight the need for institutions to engage stakeholders and tailor GenAI integration to their needs while addressing concerns. This study investigates how GenAI is perceived within the disciplines of Information Technology and Electrical Engineering (ITEE). Using a mixed-method approach, we surveyed 61 staff and 37 students at the Faculty of ITEE, University of Oulu. The results reveal both shared and discipline-specific themes, including strong interest in programming support from GenAI and concerns over response quality, privacy, and academic integrity. Drawing from these insights, the study identifies a set of high-level requirements and proposes a conceptual framework for responsible GenAI integration. Disciplinary-specific requirements reinforce the importance of stakeholder engagement when integrating GenAI into higher education. The high-level requirements and the framework provide practical guidance for universities aiming to harness GenAI while addressing stakeholder concerns and ensuring regulatory compliance.

</details>


### [20] [SynergyKGC: Reconciling Topological Heterogeneity in Knowledge Graph Completion via Topology-Aware Synergy](https://arxiv.org/abs/2602.10845)
*Xuecheng Zou,Yu Tang,Bingbing Wang*

Main category: cs.AI

TL;DR: SynergyKGC：一种通过跨模态协同专家和密度相关身份锚定策略解决知识图谱补全中结构分辨率不匹配问题的自适应框架


<details>
  <summary>Details</summary>
Motivation: 现有知识图谱补全方法面临"结构分辨率不匹配"问题，无法在不同图密度下协调不同的表示需求，导致密集簇中的结构噪声干扰和稀疏区域中的灾难性表示崩溃

Method: 提出SynergyKGC框架，将传统邻居聚合提升为主动的跨模态协同专家，通过关系感知的交叉注意力和语义意图驱动的门控机制，结合密度相关的身份锚定策略和双塔一致性架构

Result: 在两个公共基准测试上的系统评估验证了该方法在显著提升KGC命中率方面的优越性

Conclusion: 该方法为非均匀结构化数据中的弹性信息整合提供了一个通用原则的实证证据

Abstract: Knowledge Graph Completion (KGC) fundamentally hinges on the coherent fusion of pre-trained entity semantics with heterogeneous topological structures to facilitate robust relational reasoning. However, existing paradigms encounter a critical "structural resolution mismatch," failing to reconcile divergent representational demands across varying graph densities, which precipitates structural noise interference in dense clusters and catastrophic representation collapse in sparse regions. We present SynergyKGC, an adaptive framework that advances traditional neighbor aggregation to an active Cross-Modal Synergy Expert via relation-aware cross-attention and semantic-intent-driven gating. By coupling a density-dependent Identity Anchoring strategy with a Double-tower Coherent Consistency architecture, SynergyKGC effectively reconciles topological heterogeneity while ensuring representational stability across training and inference phases. Systematic evaluations on two public benchmarks validate the superiority of our method in significantly boosting KGC hit rates, providing empirical evidence for a generalized principle of resilient information integration in non-homogeneous structured data.

</details>


### [21] [Reinforcing Chain-of-Thought Reasoning with Self-Evolving Rubrics](https://arxiv.org/abs/2602.10885)
*Leheng Sheng,Wenchang Ma,Ruixin Hong,Xiang Wang,An Zhang,Tat-Seng Chua*

Main category: cs.AI

TL;DR: RLCER是一种无需人工标注的自主CoT奖励方法，通过自我提出和演进的评分标准来监督思维链，优于结果中心的RLVR方法


<details>
  <summary>Details</summary>
Motivation: 思维链在LLM推理中至关重要，但直接奖励CoT存在困难：训练奖励模型需要大量人工标注，静态奖励模型难以适应CoT分布的演变和奖励攻击。需要一种无需人工标注且能逐步演进的自主CoT奖励方法

Method: 提出RLCER方法，通过自我提出和演进的评分标准来监督思维链，增强结果中心的RLVR方法。该方法使用自我提出的评分标准为CoT提供可靠的监督信号，即使没有结果奖励也能工作

Result: RLCER在无需结果奖励的情况下超越了结果中心的RLVR方法。此外，当将这些自我提出的评分标准用作提示中的提示时，还能进一步提高推理时的性能

Conclusion: 自我提出和演进的评分标准为思维链提供了可靠的监督信号，使得RLCER方法能够有效解决CoT奖励的挑战，并在推理性能上取得改进

Abstract: Despite chain-of-thought (CoT) playing crucial roles in LLM reasoning, directly rewarding it is difficult: training a reward model demands heavy human labeling efforts, and static RMs struggle with evolving CoT distributions and reward hacking. These challenges motivate us to seek an autonomous CoT rewarding approach that requires no human annotation efforts and can evolve gradually. Inspired by recent self-evolving training methods, we propose \textbf{RLCER} (\textbf{R}einforcement \textbf{L}earning with \textbf{C}oT Supervision via Self-\textbf{E}volving \textbf{R}ubrics), which enhances the outcome-centric RLVR by rewarding CoTs with self-proposed and self-evolving rubrics. We show that self-proposed and self-evolving rubrics provide reliable CoT supervision signals even without outcome rewards, enabling RLCER to outperform outcome-centric RLVR. Moreover, when used as in-prompt hints, these self-proposed rubrics further improve inference-time performance.

</details>


### [22] [Can LLMs Cook Jamaican Couscous? A Study of Cultural Novelty in Recipe Generation](https://arxiv.org/abs/2602.10964)
*F. Carichon,R. Rampa,G. Farnadi*

Main category: cs.AI

TL;DR: LLMs在文化适应方面存在显著缺陷，无法像人类一样根据文化距离产生有意义的食谱改编，揭示了当前LLMs在文化敏感应用中的根本局限性。


<details>
  <summary>Details</summary>
Motivation: LLMs越来越多地用于生成和塑造文化内容，但现有研究表明它们存在系统性文化偏见，可能加剧刻板印象、同质化和文化表达形式的消失。理解LLMs能否超越主流文化而有意义地适应多元文化是一个关键挑战。

Method: 通过烹饪食谱这一文化、传统和创造力紧密交织的领域研究LLMs的文化适应能力。基于GlobalFusion数据集，使用相同国家配对，用多个LLMs生成文化适应食谱，直接比较人类和LLM在跨文化内容创作中的行为。

Result: LLMs无法产生具有文化代表性的适应。与人类不同，它们生成的食谱差异与文化距离不相关。研究发现：1) 文化信息在模型内部表示中保存较弱；2) 模型通过误解创造性和传统等概念来夸大新颖性；3) 模型无法将适应与相关国家联系起来，也无法将其建立在食材等文化显著元素上。

Conclusion: 当前LLMs在文化导向的生成方面存在根本性局限，这对它们在文化敏感应用中的使用具有重要影响。需要改进模型的文化理解和适应能力。

Abstract: Large Language Models (LLMs) are increasingly used to generate and shape cultural content, ranging from narrative writing to artistic production. While these models demonstrate impressive fluency and generative capacity, prior work has shown that they also exhibit systematic cultural biases, raising concerns about stereotyping, homogenization, and the erasure of culturally specific forms of expression. Understanding whether LLMs can meaningfully align with diverse cultures beyond the dominant ones remains a critical challenge. In this paper, we study cultural adaptation in LLMs through the lens of cooking recipes, a domain in which culture, tradition, and creativity are tightly intertwined. We build on the \textit{GlobalFusion} dataset, which pairs human recipes from different countries according to established measures of cultural distance. Using the same country pairs, we generate culturally adapted recipes with multiple LLMs, enabling a direct comparison between human and LLM behavior in cross-cultural content creation. Our analysis shows that LLMs fail to produce culturally representative adaptations. Unlike humans, the divergence of their generated recipes does not correlate with cultural distance. We further provide explanations for this gap. We show that cultural information is weakly preserved in internal model representations, that models inflate novelty in their production by misunderstanding notions such as creativity and tradition, and that they fail to identify adaptation with its associated countries and to ground it in culturally salient elements such as ingredients. These findings highlight fundamental limitations of current LLMs for culturally oriented generation and have important implications for their use in culturally sensitive applications.

</details>


### [23] [FormalJudge: A Neuro-Symbolic Paradigm for Agentic Oversight](https://arxiv.org/abs/2602.11136)
*Jiayi Zhou,Yang Sheng,Hantao Lou,Yaodong Yang,Jie Fu*

Main category: cs.AI

TL;DR: 论文提出FoT框架，使用双向形式思维架构，通过LLM将自然语言需求编译为形式化规范，并用Dafny和Z3提供数学保证而非概率评分，显著提升行为安全监督效果。


<details>
  <summary>Details</summary>
Motivation: 随着LLM智能体在现实高风险领域应用增多，确保其行为安全变得至关重要。当前主流的LLM-as-a-Judge监督范式面临根本困境：概率系统如何可靠监督其他概率系统而不继承其失败模式？形式化验证提供了原则性解决方案，但自然语言需求到形式化规范的转换成为关键瓶颈。

Method: 提出FoT神经符号框架，采用双向形式思维架构：LLM作为规范编译器，自上而下将高级人类意图分解为原子化、可验证的约束，然后自下而上使用Dafny规范和Z3可满足性模理论求解证明合规性，产生数学保证而非概率评分。

Result: 在三个基准测试（行为安全、多领域约束遵守、智能体向上欺骗检测）中验证，对7个智能体模型的实验表明，FoT相比LLM-as-a-Judge基线平均提升16.6%，实现了弱到强泛化（7B法官检测72B智能体欺骗准确率超90%），并通过迭代细化提供接近线性的安全改进。

Conclusion: FoT框架成功弥合了自然语言需求到形式化规范的转换瓶颈，为LLM智能体行为安全监督提供了数学保证而非概率评分的新范式，显著提升了监督效果和泛化能力。

Abstract: As LLM-based agents increasingly operate in high-stakes domains with real-world consequences, ensuring their behavioral safety becomes paramount. The dominant oversight paradigm, LLM-as-a-Judge, faces a fundamental dilemma: how can probabilistic systems reliably supervise other probabilistic systems without inheriting their failure modes? We argue that formal verification offers a principled escape from this dilemma, yet its adoption has been hindered by a critical bottleneck: the translation from natural language requirements to formal specifications. This paper bridges this gap by proposing , a neuro-symbolic framework that employs a bidirectional Formal-of-Thought architecture: LLMs serve as specification compilers that top-down decompose high-level human intent into atomic, verifiable constraints, then bottom-up prove compliance using Dafny specifications and Z3 Satisfiability modulo theories solving, which produces mathematical guarantees rather than probabilistic scores. We validate across three benchmarks spanning behavioral safety, multi-domain constraint adherence, and agentic upward deception detection. Experiments on 7 agent models demonstrate that achieves an average improvement of 16.6% over LLM-as-a-Judge baselines, enables weak-to-strong generalization where a 7B judge achieves over 90% accuracy detecting deception from 72B agents, and provides near-linear safety improvement through iterative refinement.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [24] [Reverse-Engineering Model Editing on Language Models](https://arxiv.org/abs/2602.10134)
*Zhiyu Sun,Minrui Luo,Yu Wang,Zhili Chen,Tianxing He*

Main category: cs.CR

TL;DR: 本文揭示了模型编辑方法中的安全漏洞：参数更新会泄露被编辑的敏感数据，并提出KSTER攻击方法从编辑更新中恢复原始数据，同时提出子空间伪装防御策略。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在预训练过程中会记忆敏感信息，定位后编辑方法作为主流模型编辑范式，通过修改模型参数而不重新训练来解决问题。然而，作者发现这种范式存在关键漏洞：参数更新会无意中成为侧信道，使攻击者能够恢复被编辑的数据。

Method: 提出两阶段逆向工程攻击KSTER：1）利用更新矩阵的低秩结构，通过谱分析从更新矩阵的行空间恢复编辑主题；2）引入基于熵的提示恢复攻击，重建编辑的语义上下文。同时提出子空间伪装防御策略，用语义诱饵混淆更新指纹。

Result: 在多个LLM上的广泛实验表明，该攻击能够以高成功率恢复被编辑的数据。提出的子空间伪装防御策略在不影响编辑效用的前提下，有效缓解了重建风险。

Conclusion: 定位后编辑范式存在严重的安全漏洞，参数更新会泄露敏感信息。KSTER攻击能够有效从编辑更新中恢复原始数据，而子空间伪装防御策略提供了一种有效的保护机制，平衡了安全性和编辑效用。

Abstract: Large language models (LLMs) are pretrained on corpora containing trillions of tokens and, therefore, inevitably memorize sensitive information. Locate-then-edit methods, as a mainstream paradigm of model editing, offer a promising solution by modifying model parameters without retraining. However, in this work, we reveal a critical vulnerability of this paradigm: the parameter updates inadvertently serve as a side channel, enabling attackers to recover the edited data. We propose a two-stage reverse-engineering attack named \textit{KSTER} (\textbf{K}ey\textbf{S}paceRecons\textbf{T}ruction-then-\textbf{E}ntropy\textbf{R}eduction) that leverages the low-rank structure of these updates. First, we theoretically show that the row space of the update matrix encodes a ``fingerprint" of the edited subjects, enabling accurate subject recovery via spectral analysis. Second, we introduce an entropy-based prompt recovery attack that reconstructs the semantic context of the edit. Extensive experiments on multiple LLMs demonstrate that our attacks can recover edited data with high success rates. Furthermore, we propose \textit{subspace camouflage}, a defense strategy that obfuscates the update fingerprint with semantic decoys. This approach effectively mitigates reconstruction risks without compromising editing utility. Our code is available at https://github.com/reanatom/EditingAtk.git.

</details>


### [25] [Privacy by Voice: Modeling Youth Privacy-Protective Behavior in Smart Voice Assistants](https://arxiv.org/abs/2602.10142)
*Molly Campbell,Ajay Kumar Shrestha*

Main category: cs.CR

TL;DR: 本研究通过结构方程模型分析加拿大青少年（16-24岁）与智能语音助手的隐私协商机制，发现隐私自我效能是隐私保护行为的最强预测因子，算法透明度和信任通过自我效能完全中介影响保护行为。


<details>
  <summary>Details</summary>
Motivation: 智能语音助手已深度融入青少年生活，但青少年用户隐私保护行为的驱动机制尚不明确。本研究旨在探究加拿大青少年如何与智能语音助手协商隐私，填补这一研究空白。

Method: 研究基于五个关键构念构建结构模型：感知隐私风险、感知利益、算法透明度和信任、隐私自我效能、隐私保护行为。采用横断面调查（N=469名青少年），使用偏最小二乘结构方程模型进行分析。

Result: 结果显示：隐私自我效能是隐私保护行为的最强预测因子；算法透明度和信任对保护行为的影响完全由自我效能中介；感知利益直接抑制保护行为，但通过轻微提升自我效能间接促进保护行为。

Conclusion: 研究实证验证并扩展了早期定性研究，量化了政策过载和隐藏控制如何侵蚀保护行为所需的自我效能。提出了从感知到行动的循证路径，并转化为设计原则，旨在不牺牲智能语音助手实用性的前提下增强青少年数字公民的赋权。

Abstract: Smart Voice Assistants (SVAs) are deeply embedded in the lives of youth, yet the mechanisms driving the privacy-protective behaviors among young users remain poorly understood. This study investigates how Canadian youth (aged 16-24) negotiate privacy with SVAs by developing and testing a structural model grounded in five key constructs: perceived privacy risks (PPR), perceived benefits (PPBf), algorithmic transparency and trust (ATT), privacy self-efficacy (PSE), and privacy-protective behaviors (PPB). A cross-sectional survey of N=469 youth was analyzed using partial least squares structural equation modeling. Results reveal that PSE is the strongest predictor of PPB, while the effect of ATT on PPB is fully mediated by PSE. This identifies a critical efficacy gap, where youth's confidence must first be built up for them to act. The model confirms that PPBf directly discourages protective action, yet also indirectly fosters it by slightly boosting self-efficacy. These findings empirically validate and extend earlier qualitative work, quantifying how policy overload and hidden controls erode the self-efficacy necessary for protective action. This study contributes an evidence-based pathway from perception to action and translates it into design imperatives that empower young digital citizens without sacrificing the utility of SVAs.

</details>


### [26] [Red-teaming the Multimodal Reasoning: Jailbreaking Vision-Language Models via Cross-modal Entanglement Attacks](https://arxiv.org/abs/2602.10148)
*Yu Yan,Sheng Sun,Shengjia Cheng,Teli Liu,Mingfeng Li,Min Liu*

Main category: cs.CR

TL;DR: CrossTALK是一种针对视觉语言模型的可扩展越狱攻击方法，通过跨模态信息纠缠来突破模型的安全对齐机制


<details>
  <summary>Details</summary>
Motivation: 现有的黑盒越狱攻击方法使用简单固定的图像-文本组合，缺乏攻击复杂性可扩展性，难以应对VLM不断发展的推理能力，需要更有效的红队测试方法

Method: 提出CrossTALK方法，包含三个关键技术：1) 知识可扩展重构，将有害任务扩展为多跳链式指令；2) 跨模态线索纠缠，将可视化实体迁移到图像中建立多模态推理链接；3) 跨模态场景嵌套，使用多模态上下文指令引导VLM生成详细有害输出

Result: 实验表明该方法达到了最先进的攻击成功率

Conclusion: CrossTALK通过跨模态信息纠缠的可扩展攻击方法，能够有效突破VLM训练和泛化的安全对齐模式，为评估和改进VLM安全性提供了有力工具

Abstract: Vision-Language Models (VLMs) with multimodal reasoning capabilities are high-value attack targets, given their potential for handling complex multimodal harmful tasks. Mainstream black-box jailbreak attacks on VLMs work by distributing malicious clues across modalities to disperse model attention and bypass safety alignment mechanisms. However, these adversarial attacks rely on simple and fixed image-text combinations that lack attack complexity scalability, limiting their effectiveness for red-teaming VLMs' continuously evolving reasoning capabilities. We propose \textbf{CrossTALK} (\textbf{\underline{Cross}}-modal en\textbf{\underline{TA}}ng\textbf{\underline{L}}ement attac\textbf{\underline{K}}), which is a scalable approach that extends and entangles information clues across modalities to exceed VLMs' trained and generalized safety alignment patterns for jailbreak. Specifically, {knowledge-scalable reframing} extends harmful tasks into multi-hop chain instructions, {cross-modal clue entangling} migrates visualizable entities into images to build multimodal reasoning links, and {cross-modal scenario nesting} uses multimodal contextual instructions to steer VLMs toward detailed harmful outputs. Experiments show our COMET achieves state-of-the-art attack success rate.

</details>


### [27] [Basic Legibility Protocols Improve Trusted Monitoring](https://arxiv.org/abs/2602.10153)
*Ashwin Sreevatsa,Sebastian Prasanna,Cody Rushing*

Main category: cs.CR

TL;DR: AI控制研究提出可读性协议，通过让不可信AI模型用注释充分记录代码，帮助较弱但可信的监控模型更好地评估其行为，在APPS编码环境中提高安全性而不牺牲任务性能。


<details>
  <summary>Details</summary>
Motivation: 传统可信监控方法中，较弱但可信的模型监控较强但不可信的模型，但当不可信模型的行为超出监控模型的理解范围时，这种方法会失效。需要开发新的控制协议来解决这一问题。

Method: 引入可读性协议，鼓励不可信模型采取更容易被监控模型评估的行动。在APPS编码环境中进行控制评估，让不可信模型通过详细注释来记录代码，与之前删除注释以防止欺骗性注释的方法形成对比。

Result: 1) 注释协议相比删除注释的基线方法，在不牺牲任务性能的情况下提高了安全性；2) 注释对诚实代码的益处更大，因为诚实代码通常有自然的解释可以消除监控怀疑，而后门代码往往缺乏合理的解释；3) 注释带来的收益随监控模型强度增加而增加，因为更强的监控模型能更好地区分真实解释和表面合理的解释。

Conclusion: 可读性协议是一种有效的AI控制方法，通过让不可信模型提供可读的解释，帮助较弱但可信的监控模型更好地评估其行为，在保持任务性能的同时提高安全性，特别是在监控模型较强时效果更显著。

Abstract: The AI Control research agenda aims to develop control protocols: safety techniques that prevent untrusted AI systems from taking harmful actions during deployment. Because human oversight is expensive, one approach is trusted monitoring, where weaker, trusted models oversee stronger, untrusted models$\unicode{x2013}$but this often fails when the untrusted model's actions exceed the monitor's comprehension. We introduce legibility protocols, which encourage the untrusted model to take actions that are easier for a monitor to evaluate.
  We perform control evaluations in the APPS coding setting, where an adversarial agent attempts to write backdoored code without detection. We study legibility protocols that allow the untrusted model to thoroughly document its code with comments$\unicode{x2013}$in contrast to prior work, which removed comments to prevent deceptive ones. We find that: (i) commenting protocols improve safety without sacrificing task performance relative to comment-removal baselines; (ii) commenting disproportionately benefits honest code, which typically has a natural explanation that resolves monitor suspicion, whereas backdoored code frequently lacks an easy justification; (iii) gains from commenting increase with monitor strength, as stronger monitors better distinguish genuine justifications from only superficially plausible ones.

</details>


### [28] [Omni-Safety under Cross-Modality Conflict: Vulnerabilities, Dynamics Mechanisms and Efficient Alignment](https://arxiv.org/abs/2602.10161)
*Kun Wang,Zherui Li,Zhenhong Zhou,Yitong Zhang,Yan Mi,Kun Yang,Yiming Zhang,Junhao Dong,Zhongxiang Sun,Qiankun Li,Yang Liu*

Main category: cs.CR

TL;DR: 该论文研究了全模态大语言模型的安全漏洞，提出了模态-语义解耦原则，构建了AdvBench-Omni数据集，发现了中层溶解现象，并开发了OmniSteer方法通过轻量适配器提升有害输入拒绝成功率。


<details>
  <summary>Details</summary>
Motivation: 全模态大语言模型虽然扩展了多模态能力，但也引入了跨模态安全风险。目前缺乏对全模态交互中漏洞的系统性理解，需要填补这一研究空白。

Method: 1. 建立模态-语义解耦原则；2. 构建AdvBench-Omni数据集；3. 通过机制分析发现中层溶解现象和模态不变纯拒绝方向；4. 使用奇异值分解提取黄金拒绝向量；5. 提出OmniSteer方法，利用轻量适配器自适应调节干预强度。

Result: OmniSteer方法将有害输入的拒绝成功率从69.9%提升到91.2%，同时有效保留了所有模态的通用能力。

Conclusion: 该研究系统揭示了全模态大语言模型的安全漏洞机制，提出的OmniSteer方法能有效提升模型安全性而不损害其多模态能力，为全模态模型安全提供了新的解决方案。

Abstract: Omni-modal Large Language Models (OLLMs) greatly expand LLMs' multimodal capabilities but also introduce cross-modal safety risks. However, a systematic understanding of vulnerabilities in omni-modal interactions remains lacking. To bridge this gap, we establish a modality-semantics decoupling principle and construct the AdvBench-Omni dataset, which reveals a significant vulnerability in OLLMs. Mechanistic analysis uncovers a Mid-layer Dissolution phenomenon driven by refusal vector magnitude shrinkage, alongside the existence of a modal-invariant pure refusal direction. Inspired by these insights, we extract a golden refusal vector using Singular Value Decomposition and propose OmniSteer, which utilizes lightweight adapters to modulate intervention intensity adaptively. Extensive experiments show that our method not only increases the Refusal Success Rate against harmful inputs from 69.9% to 91.2%, but also effectively preserves the general capabilities across all modalities. Our code is available at: https://github.com/zhrli324/omni-safety-research.

</details>


### [29] [Limits of Residual-Based Detection for Physically Consistent False Data Injection](https://arxiv.org/abs/2602.10162)
*Chenhan Xiao,Yang Weng*

Main category: cs.CR

TL;DR: 该论文揭示了交流电力系统状态估计中基于残差的虚假数据注入攻击检测方法存在根本性限制：当攻击产生的测量数据保持在由交流潮流关系和测量冗余度诱导的测量流形上时，残差检测器可能无法区分恶意数据和正常数据。


<details>
  <summary>Details</summary>
Motivation: 当前交流电力系统状态估计主要依赖基于拓扑感知的残差测试来检测虚假数据注入攻击，这种方法假设恶意测量可以通过物理不一致性反映在异常残差行为中被识别。然而，这种假设并不总是成立，需要研究残差检测的根本局限性。

Method: 提出了一种数据驱动的构造机制，结合交流潮流的通用功能结构来生成物理一致、流形约束的扰动，为残差检测器如何被绕过提供了具体证据。在多个交流测试系统上进行数值研究，表征检测变得困难的条件并说明其失效模式。

Result: 研究表明，当虚假数据注入攻击场景产生的操纵测量保持在由交流潮流关系和测量冗余度诱导的测量流形上时，基于残差的检测器可能无法区分它们与正常数据。这种可检测性限制是测量流形本身的属性，不依赖于攻击者对物理系统模型的详细知识。

Conclusion: 研究结果突显了交流状态估计中基于残差检测的根本限制，并强调了需要超越测量一致性测试的补充防御措施。当攻击数据保持在物理一致的测量流形上时，仅依赖残差检测是不够的。

Abstract: False data injection attacks (FDIAs) pose a persistent challenge to AC power system state estimation. In current practice, detection relies primarily on topology-aware residual-based tests that assume malicious measurements can be distinguished from normal operation through physical inconsistency reflected in abnormal residual behavior. This paper shows that this assumption does not always hold: when FDIA scenarios produce manipulated measurements that remain on the measurement manifold induced by AC power flow relations and measurement redundancy, residual-based detectors may fail to distinguish them from nominal data. The resulting detectability limitation is a property of the measurement manifold itself and does not depend on the attacker's detailed knowledge of the physical system model. To make this limitation observable in practice, we present a data-driven constructive mechanism that incorporates the generic functional structure of AC power flow to generate physically consistent, manifold-constrained perturbations, providing a concrete witness of how residual-based detectors can be bypassed. Numerical studies on multiple AC test systems characterize the conditions under which detection becomes challenging and illustrate its failure modes. The results highlight fundamental limits of residual-based detection in AC state estimation and motivate the need for complementary defenses beyond measurement consistency tests.

</details>


### [30] [MerkleSpeech: Public-Key Verifiable, Chunk-Localised Speech Provenance via Perceptual Fingerprints and Merkle Commitments](https://arxiv.org/abs/2602.10166)
*Tatsunori Ono*

Main category: cs.CR

TL;DR: MerkleSpeech系统提供两层语音来源验证：水印归属层（抗变换）和严格加密完整性层（Merkle包含验证），通过感知指纹、Merkle树和签名实现可公开验证的片段级来源追踪。


<details>
  <summary>Details</summary>
Motivation: 现有语音来源验证方法存在局限：传统水印检测无法提供第三方可验证的加密证明；C2PA等标准针对编码资产，在重新编码或常规处理时会失效。需要一种能抵抗常见分发变换、支持片段级验证、并提供加密证明的语音来源验证系统。

Method: 系统计算短语音片段的感知指纹，构建Merkle树并签名根节点，嵌入包含内容标识符和片段元数据的紧凑水印载荷。验证时提取载荷，使用公开信息进行签名验证、指纹重新计算和Merkle包含验证，生成拼接感知的时间线分析。

Result: 系统在重采样、带通滤波和加性噪声等常见变换下保持极低的误报率，特别针对神经编解码器这一对后处理水印的主要压力源进行了优化。能够生成详细的时间线，显示哪些区域通过各层验证以及失败原因。

Conclusion: MerkleSpeech提供了可公开验证的片段级语音来源追踪，结合了水印的鲁棒性和加密证明的可信性，能够应对实际工作流中的拼接、引用、修剪和平台级变换，为语音内容来源提供了两层保障机制。

Abstract: Speech provenance goes beyond detecting whether a watermark is present. Real workflows involve splicing, quoting, trimming, and platform-level transforms that may preserve some regions while altering others. Neural watermarking systems have made strides in robustness and localised detection, but most deployments produce outputs with no third-party verifiable cryptographic proof tying a time segment to an issuer-signed original. Provenance standards like C2PA adopt signed manifests and Merkle-based fragment validation, yet their bindings target encoded assets and break under re-encoding or routine processing.
  We propose MerkleSpeech, a system for public-key verifiable, chunk-localised speech provenance offering two tiers of assurance. The first, a robust watermark attribution layer (WM-only), survives common distribution transforms and answers "was this chunk issued by a known party?". The second, a strict cryptographic integrity layer (MSv1), verifies Merkle inclusion of the chunk's fingerprint under an issuer signature. The system computes perceptual fingerprints over short speech chunks, commits them in a Merkle tree whose root is signed with an issuer key, and embeds a compact in-band watermark payload carrying a random content identifier and chunk metadata sufficient to retrieve Merkle inclusion proofs from a repository. Once the payload is extracted, all subsequent verification steps (signature check, fingerprint recomputation, Merkle inclusion) use only public information. The result is a splice-aware timeline indicating which regions pass each tier and why any given region fails. We describe the protocol, provide pseudocode, and present experiments targeting very low false positive rates under resampling, bandpass filtering, and additive noise, informed by recent audits identifying neural codecs as a major stressor for post-hoc audio watermarks.

</details>


### [31] [Non-Fungible Blockchain Tokens for Traceable Online-Quality Assurance of Milled Workpieces](https://arxiv.org/abs/2602.10169)
*Nicolai Maisch,Shengjian Chen,Alexander Robertus,Samed Ajdinović,Armin Lechler,Alexander Verl,Oliver Riedel*

Main category: cs.CR

TL;DR: 利用NFT和区块链技术实现铣削工件在线质量数据的安全存储与传输，通过智能合约和IPFS系统实现质量数据的可追溯性


<details>
  <summary>Details</summary>
Motivation: 解决在线质量保证过程中铣削工件质量相关数据的安全存储和传输问题，减少耗时且成本高昂的重复性手动质量检查

Method: 使用NFT在公共以太坊区块链上安全存储质量数据（采用资产管理壳AAS格式），通过自定义智能合约铸造NFT，将元数据存储在IPFS中，支持灵活添加新处理步骤的数据

Result: 实现了质量数据的自动化追溯能力，能够在整个价值链中安全、互操作地存储和传输质量数据

Conclusion: 该概念为在线质量保证过程提供了安全的数据管理解决方案，通过区块链和NFT技术实现了质量数据的可追溯性和安全性

Abstract: This work presents a concept and implementation for the secure storage and transfer of quality-relevant data of milled workpieces from online-quality assurance processes enabled by real-time simulation models. It utilises Non-Fungible Tokens (NFT) to securely and interoperably store quality data in the form of an Asset Administration Shell (AAS) on a public Ethereum blockchain. Minted by a custom smart contract, the NFTs reference the metadata saved in the Interplanetary File System (IPFS), allowing new data from additional processing steps to be added in a flexible yet secure manner. The concept enables automated traceability throughout the value chain, minimising the need for time-consuming and costly repetitive manual quality checks.

</details>


### [32] [5Gone: Uplink Overshadowing Attacks in 5G-SA](https://arxiv.org/abs/2602.10272)
*Simon Erni,Martin Kotuliak,Marc Roeschlin,Richard Baker,Srdjan Capkun*

Main category: cs.CR

TL;DR: 5Gone是一种针对5G独立组网的软件定义无线电上行链路遮蔽攻击方法，利用3GPP标准缺陷进行隐蔽的拒绝服务、隐私和降级攻击，无需专用硬件即可实时遮蔽商用基站。


<details>
  <summary>Details</summary>
Motivation: 针对5G独立组网的攻击通常使用假基站，但需要高功率输出且易被检测。本文旨在开发一种更隐蔽、低功率的攻击方法，利用标准缺陷进行精准攻击。

Method: 采用软件定义无线电的上行链路遮蔽攻击技术，攻击者与受害用户设备在同一时间和频率传输，但功率略高。系统运行在商用x86计算机上，无需专用硬件加速，端到端延迟低于500微秒。

Result: 5Gone能够遮蔽商用100MHz基站，高度可扩展，即使多个用户设备并行连接也能有效攻击。在实验室和真实公共gNodeB上对7款手机模型和3个不同芯片供应商进行了端到端评估验证。

Conclusion: 5Gone展示了利用3GPP标准缺陷进行隐蔽上行链路遮蔽攻击的可行性，这种攻击方法比传统假基站攻击更难以检测，对5G安全提出了新的挑战。

Abstract: 5G presents numerous advantages compared to previous generations: improved throughput, lower latency, and improved privacy protection for subscribers. Attacks against 5G standalone (SA) commonly use fake base stations (FBS), which need to operate at a very high output power level to lure victim phones to connect to them and are thus highly detectable. In this paper, we introduce 5Gone, a powerful software-defined radio (SDR)-based uplink overshadowing attack method against 5G-SA. 5Gone exploits deficiencies in the 3GPP standard to perform surgical, covert denial-of-service, privacy, and downgrade attacks. Uplink overshadowing means that an attacker is transmitting at exactly the same time and frequency as the victim UE, but with a slightly higher output power. 5Gone runs on a COTS x86 computer without any need for dedicated hardware acceleration and can overshadow commercial 100 MHz cells with an E2E latency of less than 500$μ$s, which up to now has not been possible with any software-based UE implementation. We demonstrate that 5Gone is highly scalable, even when many UEs are connecting in parallel, and finally evaluate the attacks end-to-end against 7 phone models and three different chipset vendors both in our lab and in the real-world on public gNodeBs.

</details>


### [33] [SecCodePRM: A Process Reward Model for Code Security](https://arxiv.org/abs/2602.10418)
*Weichen Yu,Ravi Mangal,Yinyi Luo,Kai Hu,Jingxuan He,Corina S. Pasareanu,Matt Fredrikson*

Main category: cs.CR

TL;DR: SecCodePRM是一个面向安全的过程奖励模型，为代码轨迹提供上下文感知的步骤级安全评分，支持实时漏洞检测和安全代码生成。


<details>
  <summary>Details</summary>
Motivation: 现有漏洞检测方法依赖静态分析器或基于LLM/GNN的检测器，需要完整上下文、提供稀疏反馈，且随着代码长度增长性能下降，不适合交互式编码和流式生成的实时前缀级评估。

Method: 提出SecCodePRM安全导向的过程奖励模型，从静态分析器和专家标注中推导步骤级监督标签，使模型能更精确关注与过程间漏洞相关的细粒度区域。采用风险敏感聚合强调高风险步骤，支持推理时扩展通过排名候选延续并偏好更高累积奖励。

Result: SecCodePRM在完整代码漏洞检测、部分代码漏洞检测和安全代码生成三个应用场景中均优于先前方法，同时保持代码功能正确性，表明在安全性和实用性之间没有权衡。

Conclusion: SecCodePRM提供密集、实时的反馈，可扩展到长时程生成，改善了代码安全性而不牺牲功能性，适合现代软件开发工作流程中的实时安全评估需求。

Abstract: Large Language Models are rapidly becoming core components of modern software development workflows, yet ensuring code security remains challenging. Existing vulnerability detection pipelines either rely on static analyzers or use LLM/GNN-based detectors trained with coarse program-level supervision. Both families often require complete context, provide sparse end-of-completion feedback, and can degrade as code length grows, making them ill-suited for real-time, prefix-level assessment during interactive coding and streaming generation. We propose SecCodePRM, a security-oriented process reward model that assigns a context-aware, step-level security score along a code trajectory. To train the model, we derive step-level supervision labels from static analyzers and expert annotations, allowing the model to attend more precisely to fine-grained regions associated with inter-procedural vulnerabilities. SecCodePRM has three applications: full-code vulnerability detection (VD), partial-code VD, and secure code generation (CG). For VD, SecCodePRM uses risk-sensitive aggregation that emphasizes high-risk steps; for CG, SecCodePRM supports inference-time scaling by ranking candidate continuations and favoring higher cumulative reward. This design yields dense, real-time feedback that scales to long-horizon generation. Empirically, SecCodePRM outperforms prior approaches in all three settings, while preserving code functional correctness, suggesting improved security without a safety-utility tradeoff.

</details>


### [34] [GPU-Fuzz: Finding Memory Errors in Deep Learning Frameworks](https://arxiv.org/abs/2602.10478)
*Zihao Li,Hongyi Lu,Yanan Guo,Zhenkai Zhang,Shuai Wang,Fengwei Zhang*

Main category: cs.CR

TL;DR: GPU-Fuzz是一个针对深度学习框架GPU内存错误的模糊测试工具，通过约束求解器生成测试用例，在PyTorch、TensorFlow和PaddlePaddle中发现了13个未知漏洞。


<details>
  <summary>Details</summary>
Motivation: GPU内存错误是深度学习框架的关键威胁，可能导致系统崩溃甚至安全问题，需要有效的检测方法来定位这些漏洞。

Method: 将算子参数建模为形式化约束，利用约束求解器生成测试用例，系统性地探测GPU内核中容易出错的边界条件。

Result: 在PyTorch、TensorFlow和PaddlePaddle三个主流深度学习框架中发现了13个未知的内存错误漏洞。

Conclusion: GPU-Fuzz能够有效发现深度学习框架中的GPU内存错误，证明了基于约束求解的模糊测试方法在检测此类安全问题方面的有效性。

Abstract: GPU memory errors are a critical threat to deep learning (DL) frameworks, leading to crashes or even security issues. We introduce GPU-Fuzz, a fuzzer locating these issues efficiently by modeling operator parameters as formal constraints. GPU-Fuzz utilizes a constraint solver to generate test cases that systematically probe error-prone boundary conditions in GPU kernels. Applied to PyTorch, TensorFlow, and PaddlePaddle, we uncovered 13 unknown bugs, demonstrating the effectiveness of GPU-Fuzz in finding memory errors.

</details>


### [35] [Protecting Context and Prompts: Deterministic Security for Non-Deterministic AI](https://arxiv.org/abs/2602.10481)
*Mohan Rajagopalan,Vinay Rao*

Main category: cs.CR

TL;DR: 论文提出两种新型原语——认证提示和认证上下文，为LLM工作流提供密码学可验证的来源证明，通过形式化策略代数和多层防御实现预防性安全保证。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型应用容易受到提示注入和上下文操纵攻击，传统安全模型无法有效防护，需要新的安全机制来确保LLM工作流的完整性和可信度。

Method: 引入两种密码学原语：认证提示（提供自包含的溯源验证）和认证上下文（使用防篡改哈希链确保动态输入完整性）。基于这些原语形式化策略代数，提供协议级拜占庭容错，并设计五层互补防御机制。

Result: 在涵盖6个类别的代表性攻击评估中，实现了100%检测率、零误报和可忽略的开销，首次结合密码学强制的提示溯源、防篡改上下文和可证明的策略推理。

Conclusion: 该研究将LLM安全从被动检测转向预防性保证，通过密码学可验证的来源证明、防篡改上下文和形式化策略推理，为LLM工作流提供分层预防性安全。

Abstract: Large Language Model (LLM) applications are vulnerable to prompt injection and context manipulation attacks that traditional security models cannot prevent. We introduce two novel primitives--authenticated prompts and authenticated context--that provide cryptographically verifiable provenance across LLM workflows. Authenticated prompts enable self-contained lineage verification, while authenticated context uses tamper-evident hash chains to ensure integrity of dynamic inputs. Building on these primitives, we formalize a policy algebra with four proven theorems providing protocol-level Byzantine resistance--even adversarial agents cannot violate organizational policies. Five complementary defenses--from lightweight resource controls to LLM-based semantic validation--deliver layered, preventative security with formal guarantees. Evaluation against representative attacks spanning 6 exhaustive categories achieves 100% detection with zero false positives and nominal overhead. We demonstrate the first approach combining cryptographically enforced prompt lineage, tamper-evident context, and provable policy reasoning--shifting LLM security from reactive detection to preventative guarantees.

</details>


### [36] [Following Dragons: Code Review-Guided Fuzzing](https://arxiv.org/abs/2602.10487)
*Viet Hoang Luu,Amirmohammad Pasdar,Wachiraphan Charoenwet,Toby Murray,Shaanan Cohney,Van-Thuan Pham*

Main category: cs.CR

TL;DR: EyeQ系统利用代码审查中的开发者智能来指导模糊测试，通过提取安全相关信号、定位相关程序区域，并将这些洞察转化为基于注解的模糊测试指导，显著提升了漏洞发现能力。


<details>
  <summary>Details</summary>
Motivation: 现代模糊测试工具虽然能扩展到大型实际软件，但往往无法触及开发者认为最脆弱或最安全关键的程序状态。这些状态通常深藏在执行空间中，受前置条件限制，或被低价值路径所掩盖。同时，开发者在代码审查中经常发现风险相关的洞察，但这些信息在自动化测试工具中基本被忽略。

Method: EyeQ系统从代码审查讨论中提取安全相关信号，定位涉及的程序区域，并将这些洞察转化为基于注解的模糊测试指导。该方法建立在现有的注解感知模糊测试之上，不需要改变程序语义或开发者工作流程。首先通过人工指导的可行性研究验证方法，然后使用大型语言模型和精心设计的提示来自动化工作流程。

Result: EyeQ显著改进了漏洞发现能力，相比标准模糊测试配置有显著提升。在安全关键的PHP代码库中发现了40多个先前未知的bug。

Conclusion: 通过利用代码审查中的开发者智能来指导模糊测试，EyeQ系统能够更有效地发现安全漏洞，填补了自动化测试工具与开发者风险洞察之间的差距。

Abstract: Modern fuzzers scale to large, real-world software but often fail to exercise the program states developers consider most fragile or security-critical. Such states are typically deep in the execution space, gated by preconditions, or overshadowed by lower-value paths that consume limited fuzzing budgets. Meanwhile, developers routinely surface risk-relevant insights during code review, yet this information is largely ignored by automated testing tools. We present EyeQ, a system that leverages developer intelligence from code reviews to guide fuzzing. EyeQ extracts security-relevant signals from review discussions, localizes the implicated program regions, and translates these insights into annotation-based guidance for fuzzing. The approach operates atop existing annotation-aware fuzzing, requiring no changes to program semantics or developer workflows. We first validate EyeQ through a human-guided feasibility study on a security-focused dataset of PHP code reviews, establishing a strong baseline for review-guided fuzzing. We then automate the workflow using a large language model with carefully designed prompts. EyeQ significantly improves vulnerability discovery over standard fuzzing configurations, uncovering more than 40 previously unknown bugs in the security-critical PHP codebase.

</details>


### [37] [When Skills Lie: Hidden-Comment Injection in LLM Agents](https://arxiv.org/abs/2602.10498)
*Qianli Wang,Boyang Ma,Minghui Xu,Yue Zhang*

Main category: cs.CR

TL;DR: 研究发现LLM智能体技能文档中的Markdown转HTML时，HTML注释块可能对人工审查不可见但仍被模型读取，存在隐藏注释提示注入风险，可通过防御性系统提示防范。


<details>
  <summary>Details</summary>
Motivation: LLM智能体依赖技能文档描述可用工具和推荐流程，但Markdown技能文档在渲染为HTML时，HTML注释块可能对人工审查者不可见，而原始文本仍会被完整提供给模型，这构成了隐藏注释提示注入的安全风险。

Method: 研究隐藏注释提示注入风险：当Markdown技能文档渲染为HTML时，HTML注释块可能对人工审查不可见但仍被完整提供给模型。通过在合法技能文档后附加恶意指令的隐藏注释进行实验，测试DeepSeek-V3.2和GLM-4.5-Air模型是否受影响。

Result: 实验发现DeepSeek-V3.2和GLM-4.5-Air模型确实会受到附加在合法技能文档后的隐藏注释中恶意指令的影响，产生包含敏感工具意图的输出。简短的防御性系统提示（将技能视为不可信并禁止敏感操作）可以有效防止恶意工具调用，并暴露出可疑的隐藏指令。

Conclusion: LLM智能体技能文档层存在隐藏注释提示注入风险，HTML注释块可能绕过人工审查但仍被模型读取。防御性系统提示是有效的防护措施，应将技能文档视为不可信输入并禁止敏感操作，同时需要更严格的安全审查流程。

Abstract: LLM agents often rely on Skills to describe available tools and recommended procedures. We study a hidden-comment prompt injection risk in this documentation layer: when a Markdown Skill is rendered to HTML, HTML comment blocks can become invisible to human reviewers, yet the raw text may still be supplied verbatim to the model. In experiments, we find that DeepSeek-V3.2 and GLM-4.5-Air can be influenced by malicious instructions embedded in a hidden comment appended to an otherwise legitimate Skill, yielding outputs that contain sensitive tool intentions. A short defensive system prompt that treats Skills as untrusted and forbids sensitive actions prevents these malicious tool calls and instead surfaces the suspicious hidden instructions.

</details>


### [38] [CryptoCatch: Cryptomining Hidden Nowhere](https://arxiv.org/abs/2602.10573)
*Ruisheng Shi,Ziding Lin,Haoran Sun,Qin Wang,Shihan Zhang,Lina Lan,Zhiyuan Peng,Chenfeng Wang*

Main category: cs.CR

TL;DR: 提出一种实用的加密加密货币挖矿流量检测机制，采用两阶段检测框架，通过机器学习提供细粒度检测结果，并通过主动探测减少误报。


<details>
  <summary>Details</summary>
Motivation: 加密货币挖矿带来重大安全风险，但传统检测方法（如黑名单和深度包检测）对加密挖矿流量效果不佳，且误报率高。

Method: 提出两阶段检测框架：第一阶段使用机器学习进行细粒度检测；第二阶段通过主动探测减少分类器的误报。

Result: 系统达到F1分数0.99，识别特定加密货币的准确率达到99.39%。在不同挖矿池上的广泛测试证实了方法的有效性。

Conclusion: 该方法为识别加密货币挖矿活动提供了更精确可靠的解决方案，能够有效检测加密挖矿流量并减少误报。

Abstract: Cryptomining poses significant security risks, yet traditional detection methods like blacklists and Deep Packet Inspection (DPI) are often ineffective against encrypted mining traffic and suffer from high false positive rates. In this paper, we propose a practical encrypted cryptomining traffic detection mechanism. It consists of a two-stage detection framework, which can effectively provide fine-grained detection results by machine learning and reduce false positives from classifiers through active probing. Our system achieves an F1-score of 0.99 and identifies specific cryptocurrencies with a 99.39\% accuracy rate. Extensive testing across various mining pools confirms the effectiveness of our approach, offering a more precise and reliable solution for identifying cryptomining activities.

</details>


### [39] [Invisible Trails? An Identity Alignment Scheme based on Online Tracking](https://arxiv.org/abs/2602.10626)
*Ruisheng Shi,Zhiyuan Peng,Tong Fu,Lina Lan,Qin Wang,Jiaqi Zeng*

Main category: cs.CR

TL;DR: 该论文揭示了匿名化用户数据仍存在隐私风险，攻击者可利用追踪数据跨网站识别用户身份，并提出了一种有效的身份对齐方案和两种去匿名化攻击方法。


<details>
  <summary>Details</summary>
Motivation: 许多追踪公司收集用户数据并声称通过匿名化保护隐私，但研究发现即使匿名化数据仍存在重大隐私风险，攻击者可利用这些数据跨网站识别用户账户并进行针对性身份对齐。

Method: 开发数据收集器获取必要数据集，设计身份对齐算法，构建两种去匿名化攻击：被动攻击（分析追踪数据对齐身份）和主动攻击（诱导用户在线交互以提高成功率），并首次提出基于在线追踪的身份对齐评估框架。

Result: 研究了影响身份对齐效果的关键因素，对生成的数据集进行了独立评估，展示了应用于加密货币用例的完整系统原型，证明了身份对齐方案的有效性。

Conclusion: 即使匿名化处理，追踪数据仍存在严重的隐私风险，攻击者可通过身份对齐技术跨网站识别用户，需要更有效的隐私保护措施来应对这种威胁。

Abstract: Many tracking companies collect user data and sell it to data markets and advertisers. While they claim to protect user privacy by anonymizing the data, our research reveals that significant privacy risks persist even with anonymized data. Attackers can exploit this data to identify users' accounts on other websites and perform targeted identity alignment. In this paper, we propose an effective identity alignment scheme for accurately identifying targeted users. We develop a data collector to obtain the necessary datasets, an algorithm for identity alignment, and, based on this, construct two types of de-anonymization attacks: the \textit{passive attack}, which analyzes tracker data to align identities, and the \textit{active attack}, which induces users to interact online, leading to higher success rates. Furthermore, we introduce, for the first time, a novel evaluation framework for online tracking-based identity alignment. We investigate the key factors influencing the effectiveness of identity alignment. Additionally, we provide an independent assessment of our generated dataset and present a fully functional system prototype applied to a cryptocurrency use case.

</details>


### [40] [SecureScan: An AI-Driven Multi-Layer Framework for Malware and Phishing Detection Using Logistic Regression and Threat Intelligence Integration](https://arxiv.org/abs/2602.10750)
*Rumman Firdos,Aman Dangi*

Main category: cs.CR

TL;DR: SecureScan是一个三层AI驱动检测框架，通过逻辑回归分类、启发式分析和VirusTotal威胁情报集成，对URL、文件哈希和二进制文件进行综合检测，在基准测试中达到93.1%准确率。


<details>
  <summary>Details</summary>
Motivation: 现代恶意软件和钓鱼攻击日益复杂，传统基于签名的入侵检测系统效果下降，需要更智能的检测方法。

Method: 提出三层检测框架：1) 启发式分析过滤已知威胁；2) 逻辑回归机器学习分类不确定样本；3) VirusTotal API外部威胁情报验证边界案例。采用阈值决策校准和灰区逻辑(0.45-0.55)减少误报。

Result: 在基准数据集上达到93.1%准确率，精度0.87，召回率0.92，表现出良好的泛化能力和减少过拟合。轻量级统计模型通过校准验证和外部情报达到与复杂深度学习系统相当的可靠性。

Conclusion: 轻量级统计模型结合校准验证和外部威胁情报，能够实现与复杂深度学习系统相当的检测性能和可靠性，为实际部署提供了高效解决方案。

Abstract: The growing sophistication of modern malware and phishing campaigns has diminished the effectiveness of traditional signature-based intrusion detection systems. This work presents SecureScan, an AI-driven, triple-layer detection framework that integrates logistic regression-based classification, heuristic analysis, and external threat intelligence via the VirusTotal API for comprehensive triage of URLs, file hashes, and binaries. The proposed architecture prioritizes efficiency by filtering known threats through heuristics, classifying uncertain samples using machine learning, and validating borderline cases with third-party intelligence. On benchmark datasets, SecureScan achieves 93.1 percent accuracy with balanced precision (0.87) and recall (0.92), demonstrating strong generalization and reduced overfitting through threshold-based decision calibration. A calibrated threshold and gray-zone logic (0.45-0.55) were introduced to minimize false positives and enhance real-world stability. Experimental results indicate that a lightweight statistical model, when augmented with calibrated verification and external intelligence, can achieve reliability and performance comparable to more complex deep learning systems.

</details>


### [41] [GoodVibe: Security-by-Vibe for LLM-Based Code Generation](https://arxiv.org/abs/2602.10778)
*Maximilian Thang,Lichao Wu,Sasha Behrouzi,Mohamadreza Rostami,Jona te Lintelo,Stjepan Picek,Ahmad-Reza Sadeghi*

Main category: cs.CR

TL;DR: GoodVibe是一个神经元级别的框架，通过识别安全相关神经元并进行选择性微调，显著提升代码生成模型的安全性，同时保持通用能力，训练成本极低。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在快速开发中经常生成功能正确但不安全的代码，现有方法要么成本高且容易灾难性遗忘，要么粒度粗且可控性差，需要更高效、可解释的安全优化方案。

Method: 基于安全相关推理集中在少数神经元的洞察，使用梯度归因识别安全关键神经元，进行神经元选择性微调，并引入激活驱动的神经元聚类来降低训练成本。

Result: 在C++、Java、Swift和Go等安全关键编程语言的6个LLM上评估，GoodVibe将生成代码安全性提升高达2.5倍，匹配或超越全微调效果，可训练参数减少4700倍，训练计算比LoRA基线减少3.6倍以上。

Conclusion: 神经元级别优化为代码生成安全提供了高效、可扩展的解决方案，在不牺牲效率或通用性的前提下显著提升安全性。

Abstract: Large language models (LLMs) are increasingly used for code generation in fast, informal development workflows, often referred to as vibe coding, where speed and convenience are prioritized, and security requirements are rarely made explicit. In this setting, models frequently produce functionally correct but insecure code, creating a growing security risk. Existing approaches to improving code security rely on full-parameter fine-tuning or parameter-efficient adaptations, which are either costly and prone to catastrophic forgetting or operate at coarse granularity with limited interpretability and control.
  We present GoodVibe, a neuron-level framework for improving the security of code language models by default. GoodVibe is based on the key insight that security-relevant reasoning is localized to a small subset of neurons. We identify these neurons using gradient-based attribution from a supervised security task and perform neuron-selective fine-tuning that updates only this security-critical subspace. To further reduce training cost, we introduce activation-driven neuron clustering, enabling structured updates with minimal overhead. We evaluate GoodVibe on six LLMs across security-critical programming languages, including C++, Java, Swift, and Go. GoodVibe substantially improves the security of generated code while preserving general model utility, achieving up to a 2.5x improvement over base models, matching or exceeding full fine-tuning with over 4,700x fewer trainable parameters, and reducing training computation by more than 3.6x compared to the parameter-efficient baseline (LoRA). Our results demonstrate that neuron-level optimization offers an effective and scalable approach to securing code generation without sacrificing efficiency or generality.

</details>


### [42] [CVPL: A Geometric Framework for Post-Hoc Linkage Risk Assessment in Protected Tabular Data](https://arxiv.org/abs/2602.11015)
*Valery Khvatov,Alexey Neyman*

Main category: cs.CR

TL;DR: CVPL是一个几何框架，用于评估原始数据与受保护数据之间的链接风险，提供连续的风险估计而非二元合规判断，揭示形式隐私指标与实际链接性之间的差距。


<details>
  <summary>Details</summary>
Motivation: 形式隐私指标（如k-匿名性）提供合规性保证，但往往无法量化发布数据集中实际的链接风险。现有方法通常给出二元合规判断，缺乏对具体链接风险的连续评估。

Method: 提出CVPL（聚类-向量-投影链接）框架，将链接分析建模为操作流水线：分块、向量化、潜在投影和相似性评估。引入阈值感知风险面R(λ, τ)来捕捉保护强度和攻击者严格度的联合效应。建立具有单调性保证的渐进分块策略，支持随时风险估计和有效下界。

Result: 在10,000条记录和19种保护配置上的实证验证显示，形式k-匿名性合规可能与实质性经验链接性共存，且大部分链接风险来自非准标识符的行为模式。经典Fellegi-Sunter链接在限制性假设下是CVPL的特例，违反这些假设会导致系统性过度链接偏差。

Conclusion: CVPL提供了可解释的诊断工具，能识别驱动链接可行性的特征，支持隐私影响评估、保护机制比较和效用-风险权衡分析，弥补了形式隐私指标与实际链接风险之间的差距。

Abstract: Formal privacy metrics provide compliance-oriented guarantees but often fail to quantify actual linkability in released datasets. We introduce CVPL (Cluster-Vector-Projection Linkage), a geometric framework for post-hoc assessment of linkage risk between original and protected tabular data. CVPL represents linkage analysis as an operator pipeline comprising blocking, vectorization, latent projection, and similarity evaluation, yielding continuous, scenario-dependent risk estimates rather than binary compliance verdicts. We formally define CVPL under an explicit threat model and introduce threshold-aware risk surfaces, R(lambda, tau), that capture the joint effects of protection strength and attacker strictness. We establish a progressive blocking strategy with monotonicity guarantees, enabling anytime risk estimation with valid lower bounds. We demonstrate that the classical Fellegi-Sunter linkage emerges as a special case of CVPL under restrictive assumptions, and that violations of these assumptions can lead to systematic over-linking bias. Empirical validation on 10,000 records across 19 protection configurations demonstrates that formal k-anonymity compliance may coexist with substantial empirical linkability, with a significant portion arising from non-quasi-identifier behavioral patterns. CVPL provides interpretable diagnostics identifying which features drive linkage feasibility, supporting privacy impact assessment, protection mechanism comparison, and utility-risk trade-off analysis.

</details>


### [43] [Mask-Based Window-Level Insider Threat Detection for Campaign Discovery](https://arxiv.org/abs/2602.11019)
*Jericho Cain,Hayden Beadles*

Main category: cs.CR

TL;DR: 该研究提出了一种双通道卷积自编码器，通过分离活动存在和活动幅度来改进用户和实体行为分析中的内部威胁检测，在CERT r4.2数据集上取得了显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 当前基于固定时间窗口的UEBA系统在检测短期行为异常方面有效，但难以发现持续时间较长的攻击活动。需要研究如何从单个时间窗口中提取更多关于长期攻击活动的信息，并利用这些信息进行攻击活动发现。

Method: 提出了一种双通道卷积自编码器，同时重构二进制活动掩码和对应的活动值。这种方法让模型能够将表示能力集中在稀疏的行为结构上，而不是密集的非活动基线。

Result: 在持续1-7天的多日攻击活动中，该方法实现了窗口级精确率-召回率AUC为0.71，显著超过标准无监督自编码器基线，并能够在零误报的情况下实现高精度操作点。

Conclusion: 通过明确分离活动存在和活动幅度，可以显著提升无监督窗口级内部威胁检测的性能，为发现长期攻击活动提供了有效方法。

Abstract: User and Entity Behavior Analytics (UEBA) systems commonly detect insider threats by scoring fixed time windows of user activity for anomalous behavior. While this window-level paradigm has proven effective for identifying sharp behavioral deviations, it remains unclear how much information about longer-running attack campaigns is already present within individual windows, and how such information can be leveraged for campaign discovery. In this work, we study unsupervised window-level insider threat detection on the CERT r4.2 dataset and show that explicitly separating activity presence from activity magnitude yields substantial performance gains. We introduce a dual-channel convolutional autoencoder that reconstructs both a binary activity mask and corresponding activity values, allowing the model to focus representational capacity on sparse behavioral structure rather than dense inactive baselines. Across multiday attack campaigns lasting between one and seven days, the proposed approach achieves a window-level precision-recall AUC of 0.71, substantially exceeding standard unsupervised autoencoder baselines and enabling high-precision operating points with zero false alarms.

</details>


### [44] [IU-GUARD: Privacy-Preserving Spectrum Coordination for Incumbent Users under Dynamic Spectrum Sharing](https://arxiv.org/abs/2602.11023)
*Shaoyu Li,Hexuan Yu,Shanghao Shi,Md Mohaimin Al Barat,Yang Xiao,Y. Thomas Hou,Wenjing Lou*

Main category: cs.CR

TL;DR: IU-GUARD是一个保护隐私的动态频谱共享框架，使用可验证凭证和零知识证明，让主要用户在不暴露身份的情况下证明其频谱使用授权，解决了现有保护机制的成本高、隐私泄露等问题。


<details>
  <summary>Details</summary>
Motivation: 当前动态频谱共享中的主要用户保护机制存在严重缺陷：环境感知能力需要昂贵的传感器部署且易受干扰和安全风险；主要用户信息能力要求主要用户向频谱协调系统披露身份和操作参数，这会创建可链接的记录，损害操作隐私和任务保密性。需要一种既能保护主要用户隐私又能确保频谱共享安全的解决方案。

Method: 提出IU-GUARD框架，利用可验证凭证和零知识证明技术，使主要用户能够向频谱协调系统证明其授权，同时仅披露必要的操作参数。该方法将主要用户身份与频谱访问解耦，防止跨请求链接，并减轻集中式频谱协调系统数据泄露的风险。

Result: 实现了原型系统，评估显示IU-GUARD在提供强大隐私保证的同时，具有实际可行的计算和通信开销，适合实时动态频谱共享部署。

Conclusion: IU-GUARD是一个实用的隐私保护频谱共享框架，解决了当前主要用户保护机制的局限性，在保护主要用户操作隐私的同时确保了频谱共享的安全性，具有实际部署的可行性。

Abstract: With the growing demand for wireless spectrum, dynamic spectrum sharing (DSS) frameworks such as the Citizens Broadband Radio Service (CBRS) have emerged as practical solutions to improve utilization while protecting incumbent users (IUs) such as military radars. However, current incumbent protection mechanisms face critical limitations. The Environmental Sensing Capability (ESC) requires costly sensor deployments and remains vulnerable to interference and security risks. Alternatively, the Incumbent Informing Capability (IIC) requires IUs to disclose their identities and operational parameters to the Spectrum Coordination System (SCS), creating linkable records that compromise operational privacy and mission secrecy. We propose IU-GUARD, a privacy-preserving spectrum sharing framework that enables IUs to access spectrum without revealing their identities. Leveraging verifiable credentials (VCs) and zero-knowledge proofs (ZKPs), IU-GUARD allows IUs to prove their authorization to the SCS while disclosing only essential operational parameters. This decouples IU identity from spectrum access, prevents cross-request linkage, and mitigates the risk of centralized SCS data leakage. We implement a prototype, and our evaluation shows that IU-GUARD achieves strong privacy guarantees with practical computation and communication overhead, making it suitable for real-time DSS deployment.

</details>


### [45] [Vulnerabilities in Partial TEE-Shielded LLM Inference with Precomputed Noise](https://arxiv.org/abs/2602.11088)
*Abhishek Saini,Haolin Jiang,Hang Liu*

Main category: cs.CR

TL;DR: 主流TEE加速设计中的静态密钥重用漏洞导致LLM知识产权保护系统被完全攻破


<details>
  <summary>Details</summary>
Motivation: 第三方设备上部署大型语言模型需要保护模型知识产权，TEE是可行方案但性能限制导致设计妥协，采用预计算的静态密钥基础来加速加密操作

Method: 通过分析主流TEE加速设计模式，发现其中存在经典的密码学缺陷——密钥材料的重复使用。提出了两种攻击方法：针对模型保密系统的攻击（恢复秘密置换和模型权重）和针对完整性系统的攻击（绕过Soter和TSQP等系统的完整性检查）

Result: 攻击具有实际可行性：在约6分钟内恢复LLaMA-3 8B模型一层的秘密；攻击可扩展到405B参数的LLM，在各种配置下都能成功

Conclusion: 当前主流的TEE加速设计模式存在严重的安全漏洞，需要重新设计系统以避免密钥材料的重复使用，确保LLM在第三方设备上的安全部署

Abstract: The deployment of large language models (LLMs) on third-party devices requires new ways to protect model intellectual property. While Trusted Execution Environments (TEEs) offer a promising solution, their performance limits can lead to a critical compromise: using a precomputed, static secret basis to accelerate cryptographic operations. We demonstrate that this mainstream design pattern introduces a classic cryptographic flaw, the reuse of secret keying material, into the system's protocol. We prove its vulnerability with two distinct attacks: First, our attack on a model confidentiality system achieves a full confidentiality break by recovering its secret permutations and model weights. Second, our integrity attack completely bypasses the integrity checks of systems like Soter and TSQP. We demonstrate the practicality of our attacks against state-of-the-art LLMs, recovering a layer's secrets from a LLaMA-3 8B model in about 6 minutes and showing the attack scales to compromise 405B-parameter LLMs across a variety of configurations.

</details>
