<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 17]
- [cs.SE](#cs.SE) [Total: 14]
- [cs.CR](#cs.CR) [Total: 10]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [ConvoLearn: A Dataset of Constructivist Tutor-Student Dialogue](https://arxiv.org/abs/2601.08950)
*Mayank Sharma,Roy Pea,Hari Subramonyam*

Main category: cs.AI

TL;DR: 该研究针对教育应用中LLM的教学局限性，开发了ConvoLearn数据集来训练AI导师，使其更支持对话式知识建构学习而非直接提供答案。


<details>
  <summary>Details</summary>
Motivation: 当前教育应用中的大型语言模型存在根本性教学局限性，倾向于直接揭示解决方案而非支持对话式学习。需要开发能够促进知识建构的AI导师，这需要基于教育学理论的数据集来训练模型。

Method: 基于知识建构理论创建ConvoLearn数据集，包含六个核心教学维度。通过人类教师与模拟学生的受控交互，构建了1250个中学地球科学领域的师生对话。使用QLoRA方法对Mistral 7B模型进行微调。

Result: 微调后的Mistral 7B模型在教学行为上显著转向知识建构策略。31名教师的人工评估显示，微调模型（M=4.10）显著优于基础版本（M=2.59）和Claude Sonnet 4.5（M=2.87）。

Conclusion: 这项工作为未来建构主义AI导师的开发和评估建立了一个潜在框架，证明了基于教育学理论的数据集能够有效改善LLM的教学行为。

Abstract: In educational applications, LLMs exhibit several fundamental pedagogical limitations, such as their tendency to reveal solutions rather than support dialogic learning. We introduce ConvoLearn (https://huggingface.co/datasets/masharma/convolearn ), a dataset grounded in knowledge building theory that operationalizes six core pedagogical dimensions: cognitive engagement, formative assessment, accountability, cultural responsiveness, metacognition, and power dynamics. We construct a semi-synthetic dataset of 1250 tutor-student dialogues (20 turns each) in middle school Earth Science through controlled interactions between human teachers and a simulated student. Using QLoRA, we demonstrate that training on this dataset meaningfully shifts LLM behavior toward knowledge-building strategies. Human evaluation by 31 teachers shows our fine-tuned Mistral 7B (M = 4.10, SD = 1.03) significantly outperforms both its base version (M = 2.59, SD = 1.11) and Claude Sonnet 4.5 (M = 2.87, SD = 1.29) overall. This work establishes a potential framework to guide future development and evaluation of constructivist AI tutors.

</details>


### [2] [The Hierarchy of Agentic Capabilities: Evaluating Frontier Models on Realistic RL Environments](https://arxiv.org/abs/2601.09032)
*Logan Ritchie,Sushant Mehta,Nick Heiner,Mason Yu,Edwin Chen*

Main category: cs.AI

TL;DR: 该研究评估了大型语言模型在电商环境中的150个职场任务表现，揭示了模型需要掌握的五个层次能力，即使最佳模型仍有40%任务失败率。


<details>
  <summary>Details</summary>
Motivation: 随着基于LLM的智能体发展，AI评估需要从单轮响应评估转向交互环境中的多步骤任务完成评估。研究旨在评估前沿AI模型在真实电商工作环境中的实际表现，识别模型在现实部署中的能力差距。

Method: 采用实证研究方法，在Surge提供的真实电商强化学习环境中评估前沿AI模型在150个职场任务上的表现。引入任务中心设计方法，强调任务多样性和领域专家贡献，并进行详细的失败分析。

Result: 研究发现了一个经验推导的"智能体能力层次结构"：工具使用、规划与目标形成、适应性、接地性、常识推理。即使表现最佳的模型仍有约40%的任务失败率，失败模式沿能力层次可预测分布：较弱模型在基础工具使用和规划上挣扎，较强模型主要在需要超越明确指令的上下文推理任务上失败。

Conclusion: 当前前沿模型虽然能展示连贯的多步骤行为，但在实现真实职场环境中人类水平任务完成方面仍存在显著能力差距。研究为智能体开发提供了重要启示，强调了需要针对不同能力层次进行针对性改进。

Abstract: The advancement of large language model (LLM) based agents has shifted AI evaluation from single-turn response assessment to multi-step task completion in interactive environments. We present an empirical study evaluating frontier AI models on 150 workplace tasks within a realistic e-commerce RL environment from Surge. Our analysis reveals an empirically-derived \emph{hierarchy of agentic capabilities} that models must master for real-world deployment: (1) tool use, (2) planning and goal formation, (3) adaptability, (4) groundedness, and (5) common-sense reasoning. Even the best-performing models fail approximately 40\% of the tasks, with failures clustering predictably along this hierarchy. Weaker models struggle with fundamental tool use and planning, whereas stronger models primarily fail on tasks requiring contextual inference beyond explicit instructions. We introduce a task-centric design methodology for RL environments that emphasizes diversity and domain expert contributions, provide detailed failure analysis, and discuss implications for agent development. Our findings suggest that while current frontier models can demonstrate coherent multi-step behavior, substantial capability gaps remain before achieving human-level task completion in realistic workplace settings.

</details>


### [3] [Programming over Thinking: Efficient and Robust Multi-Constraint Planning](https://arxiv.org/abs/2601.09097)
*Derrick Goh Xin Deik,Quanyu Long,Zhengyuan Liu,Nancy F. Chen,Wenya Wang*

Main category: cs.AI

TL;DR: SCOPE框架通过分离推理与代码执行，解决了多约束规划中LLM方法的局限性，实现了高效、可复用且成本更低的解决方案。


<details>
  <summary>Details</summary>
Motivation: 现有LLM方法在多约束规划中存在根本性局限：纯推理方法容易产生不一致性、错误累积和高成本；而结合编码或求解器的方法缺乏灵活性，无法捕捉跨问题的通用逻辑。

Method: 提出SCOPE框架，将查询特定推理与通用代码执行解耦，生成一致、确定且可跨查询复用的求解器函数，仅需对输入参数进行最小改动。

Result: SCOPE在TravelPlanner上达到93.1%的成功率，比最佳基线（CoT）提升61.6%，同时将推理成本降低1.4倍，时间减少约4.67倍。

Conclusion: SCOPE通过分离推理与执行，解决了多约束规划中LLM方法的根本问题，实现了高性能、低成本且可扩展的解决方案。

Abstract: Multi-constraint planning involves identifying, evaluating, and refining candidate plans while satisfying multiple, potentially conflicting constraints. Existing large language model (LLM) approaches face fundamental limitations in this domain. Pure reasoning paradigms, which rely on long natural language chains, are prone to inconsistency, error accumulation, and prohibitive cost as constraints compound. Conversely, LLMs combined with coding- or solver-based strategies lack flexibility: they often generate problem-specific code from scratch or depend on fixed solvers, failing to capture generalizable logic across diverse problems. To address these challenges, we introduce the Scalable COde Planning Engine (SCOPE), a framework that disentangles query-specific reasoning from generic code execution. By separating reasoning from execution, SCOPE produces solver functions that are consistent, deterministic, and reusable across queries while requiring only minimal changes to input parameters. SCOPE achieves state-of-the-art performance while lowering cost and latency. For example, with GPT-4o, it reaches 93.1% success on TravelPlanner, a 61.6% gain over the best baseline (CoT) while cutting inference cost by 1.4x and time by ~4.67x. Code is available at https://github.com/DerrickGXD/SCOPE.

</details>


### [4] [DScheLLM: Enabling Dynamic Scheduling through a Fine-Tuned Dual-System Large language Model](https://arxiv.org/abs/2601.09100)
*Lixiang Zhang,Chenggong Zhao,Qing Gao,Xiaoke Zhao,Gengyi Bai,Jinhu Lv*

Main category: cs.AI

TL;DR: 本文提出DScheLLM方法，利用微调大语言模型在双系统（快慢）推理架构中处理生产调度中的动态扰动，通过华为OpenPangu模型在标准作业车间调度基准上实现高效调度。


<details>
  <summary>Details</summary>
Motivation: 传统生产调度方法对动态扰动（如处理时间变化、机器可用性、意外任务插入）的适应性有限，通常依赖特定事件模型和显式分析公式，难以泛化到未见过的扰动情况。

Method: 构建统一的大语言模型框架处理动态事件，使用运筹学求解器获得的精确调度生成快慢推理模式的训练数据集，基于华为OpenPangu Embedded-7B模型采用LoRA进行微调，实现双系统推理架构。

Result: 在标准作业车间调度基准上的实验评估表明，快思考模式能高效生成高质量调度方案，慢思考模式能产生与求解器兼容且格式良好的决策输入。

Conclusion: 这是最早将大语言模型应用于动态环境作业车间调度的研究之一，展示了LLM在智能自适应调度优化中的巨大潜力。

Abstract: Production scheduling is highly susceptible to dynamic disruptions, such as variations in processing times, machine availability, and unexpected task insertions. Conventional approaches typically rely on event-specific models and explicit analytical formulations, which limits their adaptability and generalization across previously unseen disturbances. To overcome these limitations, this paper proposes DScheLLM, a dynamic scheduling approach that leverages fine-tuned large language models within a dual-system (fast-slow) reasoning architecture to address disturbances of different scales. A unified large language model-based framework is constructed to handle dynamic events, where training datasets for both fast and slow reasoning modes are generated using exact schedules obtained from an operations research solver. The Huawei OpenPangu Embedded-7B model is subsequently fine-tuned under the hybrid reasoning paradigms using LoRA. Experimental evaluations on standard job shop scheduling benchmarks demonstrate that the fast-thinking mode can efficiently generate high-quality schedules and the slow-thinking mode can produce solver-compatible and well-formatted decision inputs. To the best of our knowledge, this work represents one of the earliest studies applying large language models to job shop scheduling in dynamic environments, highlighting their considerable potential for intelligent and adaptive scheduling optimization.

</details>


### [5] [AviationLMM: A Large Multimodal Foundation Model for Civil Aviation](https://arxiv.org/abs/2601.09105)
*Wenbin Li,Jingling Wu,Xiaoyong Lin. Jing Chen,Cong Chen*

Main category: cs.AI

TL;DR: 该论文提出了AviationLMM——一个面向民航的大型多模态基础模型，旨在统一民航异构数据流，实现理解、推理、生成和智能体应用，以解决现有AI解决方案在民航领域的孤立性和局限性问题。


<details>
  <summary>Details</summary>
Motivation: 民航是全球交通和商业的基石，确保其安全、效率和客户满意度至关重要。然而，民航领域的传统AI解决方案存在孤立性和局限性，专注于单一任务或模态，难以整合语音通信、雷达轨迹、传感器流和文本报告等异构数据，限制了态势感知、适应性和实时决策支持能力。

Method: 论文提出了AviationLMM模型架构，该架构能够处理多模态输入（包括空地语音、监视数据、机载遥测、视频和结构化文本），执行跨模态对齐和融合，并产生灵活的输出（从态势摘要和风险警报到预测性诊断和多模态事件重建）。同时识别了实现该愿景需要解决的关键研究机会。

Result: 论文提出了AviationLMM的设计愿景和架构框架，识别了数据获取、对齐融合、预训练、推理、可信性、隐私、模态缺失鲁棒性和合成场景生成等关键研究挑战，为推进民航基础模型研究和构建集成、可信、隐私保护的航空AI生态系统提供了路线图。

Conclusion: 通过阐述AviationLMM的设计和挑战，旨在推动民航基础模型的进展，并促进协调的研究努力，朝着构建集成、可信和隐私保护的航空AI生态系统方向发展。

Abstract: Civil aviation is a cornerstone of global transportation and commerce, and ensuring its safety, efficiency and customer satisfaction is paramount. Yet conventional Artificial Intelligence (AI) solutions in aviation remain siloed and narrow, focusing on isolated tasks or single modalities. They struggle to integrate heterogeneous data such as voice communications, radar tracks, sensor streams and textual reports, which limits situational awareness, adaptability, and real-time decision support. This paper introduces the vision of AviationLMM, a Large Multimodal foundation Model for civil aviation, designed to unify the heterogeneous data streams of civil aviation and enable understanding, reasoning, generation and agentic applications. We firstly identify the gaps between existing AI solutions and requirements. Secondly, we describe the model architecture that ingests multimodal inputs such as air-ground voice, surveillance, on-board telemetry, video and structured texts, and performs cross-modal alignment and fusion, and produces flexible outputs ranging from situation summaries and risk alerts to predictive diagnostics and multimodal incident reconstructions. In order to fully realize this vision, we identify key research opportunities to address, including data acquisition, alignment and fusion, pretraining, reasoning, trustworthiness, privacy, robustness to missing modalities, and synthetic scenario generation. By articulating the design and challenges of AviationLMM, we aim to boost the civil aviation foundation model progress and catalyze coordinated research efforts toward an integrated, trustworthy and privacy-preserving aviation AI ecosystem.

</details>


### [6] [The AI Hippocampus: How Far are We From Human Memory?](https://arxiv.org/abs/2601.09113)
*Zixia Jia,Jiaqi Li,Yipeng Kang,Yuxuan Wang,Tong Wu,Quansen Wang,Xiaobo Wang,Shuyi Zhang,Junzhe Shen,Qing Li,Siyuan Qi,Yitao Liang,Di He,Zilong Zheng,Song-Chun Zhu*

Main category: cs.AI

TL;DR: 该综述系统性地回顾了LLMs和MLLMs中的记忆机制，将其分为隐式、显式和智能体记忆三大范式，并探讨了多模态环境下的记忆整合及当前挑战。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型从静态预测器向能够持续学习和个性化推理的交互系统转变，记忆机制已成为其架构和功能演进的核心主题。需要全面梳理记忆在LLMs和MLLMs中的作用，为未来研究提供结构化框架。

Method: 采用文献综述方法，将现有研究组织成统一的分类体系：1) 隐式记忆（预训练transformer内部参数中的知识）；2) 显式记忆（外部存储和检索组件）；3) 智能体记忆（自主智能体中的持久性记忆结构）。同时扩展到多模态环境下的记忆整合。

Result: 提出了一个包含三大记忆范式的完整分类体系：隐式记忆涵盖模型内部的知识嵌入和关联检索能力；显式记忆通过外部存储实现动态知识更新；智能体记忆支持长期规划和多智能体协作。识别了多模态记忆整合的关键需求。

Conclusion: 记忆机制对于增强LLMs和MLLMs的推理能力、适应性和上下文保真度至关重要。未来研究需要解决记忆容量、对齐、事实一致性、跨系统互操作性等挑战，特别是在多模态和智能体系统中的应用。

Abstract: Memory plays a foundational role in augmenting the reasoning, adaptability, and contextual fidelity of modern Large Language Models and Multi-Modal LLMs. As these models transition from static predictors to interactive systems capable of continual learning and personalized inference, the incorporation of memory mechanisms has emerged as a central theme in their architectural and functional evolution. This survey presents a comprehensive and structured synthesis of memory in LLMs and MLLMs, organizing the literature into a cohesive taxonomy comprising implicit, explicit, and agentic memory paradigms. Specifically, the survey delineates three primary memory frameworks. Implicit memory refers to the knowledge embedded within the internal parameters of pre-trained transformers, encompassing their capacity for memorization, associative retrieval, and contextual reasoning. Recent work has explored methods to interpret, manipulate, and reconfigure this latent memory. Explicit memory involves external storage and retrieval components designed to augment model outputs with dynamic, queryable knowledge representations, such as textual corpora, dense vectors, and graph-based structures, thereby enabling scalable and updatable interaction with information sources. Agentic memory introduces persistent, temporally extended memory structures within autonomous agents, facilitating long-term planning, self-consistency, and collaborative behavior in multi-agent systems, with relevance to embodied and interactive AI. Extending beyond text, the survey examines the integration of memory within multi-modal settings, where coherence across vision, language, audio, and action modalities is essential. Key architectural advances, benchmark tasks, and open challenges are discussed, including issues related to memory capacity, alignment, factual consistency, and cross-system interoperability.

</details>


### [7] [Position on LLM-Assisted Peer Review: Addressing Reviewer Gap through Mentoring and Feedback](https://arxiv.org/abs/2601.09182)
*JungMin Yun,JuneHyoung Kwon,MiHyeon Kim,YoungBin Kim*

Main category: cs.AI

TL;DR: 本文提出以LLM辅助人类审稿人的新范式，而非自动生成审稿意见，旨在解决AI研究快速发展导致的"审稿人缺口"问题，提升同行评审的可持续性。


<details>
  <summary>Details</summary>
Motivation: AI研究的快速扩张加剧了"审稿人缺口"，威胁同行评审的可持续性，并导致低质量评审的恶性循环。现有LLM自动生成审稿意见的方法存在局限性，需要新的解决方案。

Method: 提出以人为中心的范式转变，将LLM定位为辅助和教育人类审稿人的工具。基于高质量同行评审的核心原则，设计了两个互补系统：LLM辅助的导师系统（培养审稿人长期能力）和LLM辅助的反馈系统（帮助审稿人改进评审质量）。

Result: 论文提出了一个理论框架和系统设计方案，但未提供具体的实验结果。该方案旨在通过增强审稿人专业能力来构建更可持续的学术生态系统。

Conclusion: 需要从LLM自动生成评审向LLM辅助人类审稿人的范式转变，通过培养审稿人能力和改进评审质量来应对审稿人缺口，建立更可持续的学术评审体系。

Abstract: The rapid expansion of AI research has intensified the Reviewer Gap, threatening the peer-review sustainability and perpetuating a cycle of low-quality evaluations. This position paper critiques existing LLM approaches that automatically generate reviews and argues for a paradigm shift that positions LLMs as tools for assisting and educating human reviewers. We define the core principles of high-quality peer review and propose two complementary systems grounded in these foundations: (i) an LLM-assisted mentoring system that cultivates reviewers' long-term competencies, and (ii) an LLM-assisted feedback system that helps reviewers refine the quality of their reviews. This human-centered approach aims to strengthen reviewer expertise and contribute to building a more sustainable scholarly ecosystem.

</details>


### [8] [MAXS: Meta-Adaptive Exploration with LLM Agents](https://arxiv.org/abs/2601.09259)
*Jian Zhang,Zhiyuan Wang,Zhangqi Wang,Yu He,Haoran Luo,li yuan,Lingling Zhang,Rui Mao,Qika Lin,Jun Liu*

Main category: cs.AI

TL;DR: MAXS是一个基于LLM Agent的元自适应推理框架，通过前瞻策略和轨迹收敛机制，解决了现有方法中局部短视和轨迹不稳定的问题，在性能和推理效率上均优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有LLM Agent推理方法存在两个主要问题：1）由于缺乏前瞻性导致局部短视生成；2）轨迹不稳定，早期微小错误会演变成发散推理路径。这些问题使得难以平衡全局有效性和计算效率。

Method: 提出MAXS框架，采用前瞻策略向前扩展推理路径几步，估计工具使用的优势值，结合步骤一致性方差和步骤间趋势斜率联合选择稳定、一致且高价值的推理步骤。引入轨迹收敛机制，在路径一致性达到时停止进一步展开，平衡资源效率和全局有效性。

Result: 在三个基础模型（MiMo-VL-7B、Qwen2.5-VL-7B、Qwen2.5-VL-32B）和五个数据集上的广泛实证研究表明，MAXS在性能和推理效率方面均一致优于现有方法。进一步分析证实了前瞻策略和工具使用的有效性。

Conclusion: MAXS通过元自适应推理框架有效解决了LLM Agent推理中的局部短视和轨迹不稳定问题，实现了全局有效性和计算效率的良好平衡，为多工具推理提供了有效的解决方案。

Abstract: Large Language Model (LLM) Agents exhibit inherent reasoning abilities through the collaboration of multiple tools. However, during agent inference, existing methods often suffer from (i) locally myopic generation, due to the absence of lookahead, and (ii) trajectory instability, where minor early errors can escalate into divergent reasoning paths. These issues make it difficult to balance global effectiveness and computational efficiency. To address these two issues, we propose meta-adaptive exploration with LLM agents https://github.com/exoskeletonzj/MAXS, a meta-adaptive reasoning framework based on LLM Agents that flexibly integrates tool execution and reasoning planning. MAXS employs a lookahead strategy to extend reasoning paths a few steps ahead, estimating the advantage value of tool usage, and combines step consistency variance and inter-step trend slopes to jointly select stable, consistent, and high-value reasoning steps. Additionally, we introduce a trajectory convergence mechanism that controls computational cost by halting further rollouts once path consistency is achieved, enabling a balance between resource efficiency and global effectiveness in multi-tool reasoning. We conduct extensive empirical studies across three base models (MiMo-VL-7B, Qwen2.5-VL-7B, Qwen2.5-VL-32B) and five datasets, demonstrating that MAXS consistently outperforms existing methods in both performance and inference efficiency. Further analysis confirms the effectiveness of our lookahead strategy and tool usage.

</details>


### [9] [Efficient Paths and Dense Rewards: Probabilistic Flow Reasoning for Large Language Models](https://arxiv.org/abs/2601.09260)
*Yan Liu,Feng Zhang,Zhanyu Ma,Jun Xu,Jiuchong Gao,Jinghua Hao,Renqing He,Han Liu,Yangdong Deng*

Main category: cs.AI

TL;DR: CoT-Flow：将思维链推理步骤重新概念化为连续概率流，量化每个步骤对最终答案的贡献，实现推理效率与性能的平衡


<details>
  <summary>Details</summary>
Motivation: 当前思维链方法将推理过程视为不可分割的序列，缺乏量化逐步信息增益的内在机制，导致推理效率低下（冗余探索）和优化困难（稀疏监督或昂贵验证器）

Method: 提出CoT-Framework，将离散推理步骤重新概念化为连续概率流，量化每个步骤对真实答案的贡献。基于此实现两种方法：流引导解码（贪婪流解码策略提取信息高效推理路径）和基于流的强化学习（构建无需验证器的密集奖励函数）

Result: 在具有挑战性的基准测试中，CoT-Flow在推理效率和推理性能之间实现了优越的平衡

Conclusion: 通过将推理步骤量化为连续概率流，CoT-Flow能够有效指导推理过程，提高效率同时保持性能，为思维链推理提供了新的优化框架

Abstract: High-quality chain-of-thought has demonstrated strong potential for unlocking the reasoning capabilities of large language models. However, current paradigms typically treat the reasoning process as an indivisible sequence, lacking an intrinsic mechanism to quantify step-wise information gain. This granularity gap manifests in two limitations: inference inefficiency from redundant exploration without explicit guidance, and optimization difficulty due to sparse outcome supervision or costly external verifiers. In this work, we propose CoT-Flow, a framework that reconceptualizes discrete reasoning steps as a continuous probabilistic flow, quantifying the contribution of each step toward the ground-truth answer. Built on this formulation, CoT-Flow enables two complementary methodologies: flow-guided decoding, which employs a greedy flow-based decoding strategy to extract information-efficient reasoning paths, and flow-based reinforcement learning, which constructs a verifier-free dense reward function. Experiments on challenging benchmarks demonstrate that CoT-Flow achieves a superior balance between inference efficiency and reasoning performance.

</details>


### [10] [Coordinated Pandemic Control with Large Language Model Agents as Policymaking Assistants](https://arxiv.org/abs/2601.09264)
*Ziyi Shi,Xusen Guo,Hongliang Lu,Mingxing Peng,Haotian Wang,Zheng Zhu,Zhenning Li,Yuxuan Liang,Xinhu Zheng,Hai Yang*

Main category: cs.AI

TL;DR: 提出基于大语言模型的多智能体政策制定框架，通过模拟跨区域协调决策，显著降低疫情感染和死亡人数


<details>
  <summary>Details</summary>
Motivation: 传统疫情控制政策往往分散、被动，各地区各自为政，只在疫情升级后才调整，缺乏前瞻性和协调性，影响全球疫情防控效果

Method: 为每个行政区域分配一个LLM智能体作为AI政策助手，智能体基于区域特定流行病学动态进行推理，同时与其他智能体通信考虑跨区域相互依赖，整合真实世界数据、疫情演化模拟器和结构化智能体间通信

Result: 使用美国2020年4-12月州级COVID-19数据、真实移动记录和观察到的政策干预进行验证，在单个州层面分别减少累计感染和死亡63.7%和40.1%，在跨州聚合层面分别减少39.0%和27.0%

Conclusion: LLM多智能体系统能够通过协调政策制定实现更有效的疫情控制，为跨区域协同决策提供新方法

Abstract: Effective pandemic control requires timely and coordinated policymaking across administrative regions that are intrinsically interdependent. However, human-driven responses are often fragmented and reactive, with policies formulated in isolation and adjusted only after outbreaks escalate, undermining proactive intervention and global pandemic mitigation. To address this challenge, here we propose a large language model (LLM) multi-agent policymaking framework that supports coordinated and proactive pandemic control across regions. Within our framework, each administrative region is assigned an LLM agent as an AI policymaking assistant. The agent reasons over region-specific epidemiological dynamics while communicating with other agents to account for cross-regional interdependencies. By integrating real-world data, a pandemic evolution simulator, and structured inter-agent communication, our framework enables agents to jointly explore counterfactual intervention scenarios and synthesize coordinated policy decisions through a closed-loop simulation process. We validate the proposed framework using state-level COVID-19 data from the United States between April and December 2020, together with real-world mobility records and observed policy interventions. Compared with real-world pandemic outcomes, our approach reduces cumulative infections and deaths by up to 63.7% and 40.1%, respectively, at the individual state level, and by 39.0% and 27.0%, respectively, when aggregated across states. These results demonstrate that LLM multi-agent systems can enable more effective pandemic control with coordinated policymaking...

</details>


### [11] [RISER: Orchestrating Latent Reasoning Skills for Adaptive Activation Steering](https://arxiv.org/abs/2601.09269)
*Wencheng Ye,Liang Peng,Xiaoyang Yuan,Yi Bin,Pengpeng Zeng,Hengyu Jin,Heng Tao Shen*

Main category: cs.AI

TL;DR: RISER是一个基于路由器的自适应激活干预框架，通过强化学习动态组合可重用推理向量，在零样本场景下显著提升LLM推理性能，同时保持高token效率。


<details>
  <summary>Details</summary>
Motivation: 现有基于激活干预的方法采用静态、手动干预，无法适应复杂推理的动态特性，而训练密集型方法需要参数更新，不够高效。需要一种参数高效且能自适应动态推理的干预方法。

Method: 提出RISER框架：1)构建可重用推理向量库；2)使用轻量级路由器动态组合这些向量；3)通过强化学习在任务级奖励下优化路由器，以涌现和组合方式激活潜在认知基元。

Result: 在7个多样化基准测试中，RISER相比基础模型平均提升3.4-6.5%的零样本准确率，超越CoT风格推理，token效率高2-3倍，且保持稳健的准确率提升。

Conclusion: RISER能够自主组合多个向量形成可解释的精确控制策略，指向更可控和高效的大语言模型推理，展示了自适应激活干预在提升LLM推理能力方面的潜力。

Abstract: Recent work on domain-specific reasoning with large language models (LLMs) often relies on training-intensive approaches that require parameter updates. While activation steering has emerged as a parameter efficient alternative, existing methods apply static, manual interventions that fail to adapt to the dynamic nature of complex reasoning. To address this limitation, we propose RISER (Router-based Intervention for Steerable Enhancement of Reasoning), a plug-and-play intervention framework that adaptively steers LLM reasoning in activation space. RISER constructs a library of reusable reasoning vectors and employs a lightweight Router to dynamically compose them for each input. The Router is optimized via reinforcement learning under task-level rewards, activating latent cognitive primitives in an emergent and compositional manner. Across seven diverse benchmarks, RISER yields 3.4-6.5% average zero-shot accuracy improvements over the base model while surpassing CoT-style reasoning with 2-3x higher token efficiency and robust accuracy gains. Further analysis shows that RISER autonomously combines multiple vectors into interpretable, precise control strategies, pointing toward more controllable and efficient LLM reasoning.

</details>


### [12] [$A^3$-Bench: Benchmarking Memory-Driven Scientific Reasoning via Anchor and Attractor Activation](https://arxiv.org/abs/2601.09274)
*Jian Zhang,Yu He,Zhiyuan Wang,Zhangqi Wang,Kai He,Fangzhi Xu,Qika Lin,Jun Liu*

Main category: cs.AI

TL;DR: 该论文提出了A³-Bench基准，用于评估科学推理中的记忆驱动机制，重点关注锚点和吸引子的激活与利用。


<details>
  <summary>Details</summary>
Motivation: 现有基准主要评估最终答案或逐步推理的连贯性，但忽视了人类推理中基于记忆驱动的机制，即激活锚点和吸引子并将其整合到多步推理中的过程。

Method: 1) 使用SAPM过程（主题、锚点与吸引子、问题、记忆发展）标注了2,198个跨领域的科学推理问题；2) 引入基于锚点和吸引子的双尺度记忆评估框架；3) 提出AAUI（锚点-吸引子利用指数）指标来衡量记忆激活率。

Result: 通过多种基础模型和范式的实验验证了A³-Bench的有效性，并分析了记忆激活如何影响推理性能，为记忆驱动的科学推理提供了见解。

Conclusion: 该研究填补了科学推理评估中记忆驱动机制的空白，提出的A³-Bench基准和评估框架能够有效衡量模型在科学推理中激活和利用先验知识的能力。

Abstract: Scientific reasoning relies not only on logical inference but also on activating prior knowledge and experiential structures. Memory can efficiently reuse knowledge and enhance reasoning consistency and stability. However, existing benchmarks mainly evaluate final answers or step-by-step coherence, overlooking the \textit{memory-driven} mechanisms that underlie human reasoning, which involves activating anchors and attractors, then integrating them into multi-step inference. To address this gap, we propose $A^3$-Bench~ https://a3-bench.github.io, a benchmark designed to evaluate scientific reasoning through dual-scale memory-driven activation, grounded in Anchor and Attractor Activation. First, we annotate 2,198 science reasoning problems across domains using the SAPM process(subject, anchor & attractor, problem, and memory developing). Second, we introduce a dual-scale memory evaluation framework utilizing anchors and attractors, along with the AAUI(Anchor--Attractor Utilization Index) metric to measure memory activation rates. Finally, through experiments with various base models and paradigms, we validate $A^3$-Bench and analyze how memory activation impacts reasoning performance, providing insights into memory-driven scientific reasoning.

</details>


### [13] [Cluster Workload Allocation: Semantic Soft Affinity Using Natural Language Processing](https://arxiv.org/abs/2601.09282)
*Leszek Sliwko,Jolanta Mizeria-Pietraszko*

Main category: cs.AI

TL;DR: 该论文提出了一种基于自然语言处理的语义化、意图驱动的集群调度范式，通过LLM解析自然语言分配提示，实现简化的Kubernetes工作负载编排。


<details>
  <summary>Details</summary>
Motivation: 集群工作负载分配通常需要复杂的配置，存在可用性差距。传统调度配置对用户不友好，需要简化集群系统的配置和使用。

Method: 使用大型语言模型（LLM）通过Kubernetes调度器扩展器集成，解释自然语言分配提示注释以实现软亲和性偏好。开发了原型系统，包括集群状态缓存和意图分析器（使用AWS Bedrock）。

Result: LLM解析准确率高（在评估数据集上>95%子集准确率），Amazon Nova Pro/Premier和Mistral Pixtral Large等顶级模型显著优于基线引擎。在六个场景的调度质量测试中，原型系统实现了优于或等同于标准Kubernetes配置的放置效果，在复杂和定量场景以及处理冲突软偏好方面表现突出。

Conclusion: 结果验证了使用LLM进行可访问调度的可行性，但指出了同步LLM延迟等限制，建议采用异步处理以实现生产就绪。这项工作确认了语义软亲和性在简化工作负载编排方面的可行性。

Abstract: Cluster workload allocation often requires complex configurations, creating a usability gap. This paper introduces a semantic, intent-driven scheduling paradigm for cluster systems using Natural Language Processing. The system employs a Large Language Model (LLM) integrated via a Kubernetes scheduler extender to interpret natural language allocation hint annotations for soft affinity preferences. A prototype featuring a cluster state cache and an intent analyzer (using AWS Bedrock) was developed. Empirical evaluation demonstrated high LLM parsing accuracy (>95% Subset Accuracy on an evaluation ground-truth dataset) for top-tier models like Amazon Nova Pro/Premier and Mistral Pixtral Large, significantly outperforming a baseline engine. Scheduling quality tests across six scenarios showed the prototype achieved superior or equivalent placement compared to standard Kubernetes configurations, particularly excelling in complex and quantitative scenarios and handling conflicting soft preferences. The results validate using LLMs for accessible scheduling but highlight limitations like synchronous LLM latency, suggesting asynchronous processing for production readiness. This work confirms the viability of semantic soft affinity for simplifying workload orchestration.

</details>


### [14] [STaR: Sensitive Trajectory Regulation for Unlearning in Large Reasoning Models](https://arxiv.org/abs/2601.09281)
*Jingjing Zhou,Gaoxiang Cong,Li Su,Liang Li*

Main category: cs.AI

TL;DR: STaR是一个针对大型推理模型的参数无关、推理时遗忘框架，通过语义检测、安全提示前缀、轨迹感知抑制和自适应过滤，在推理链中全面保护隐私，同时引入新的评估指标MCS和MIA。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型（LRMs）能够生成复杂的思维链轨迹，但这引入了严重的隐私风险，敏感信息可能深嵌于推理过程中。现有的LLM遗忘方法通常只修改最终答案，无法从中间步骤中移除敏感内容，导致持续的隐私泄露和安全降级。

Method: STaR框架包含四个步骤：1）通过语义感知检测识别敏感内容；2）通过安全提示前缀注入全局安全约束；3）执行轨迹感知抑制，动态阻止整个推理链中的敏感内容；4）应用令牌级自适应过滤，防止生成精确和转述的敏感令牌。

Result: 在R-TOFU基准测试上的实验表明，STaR实现了全面且稳定的遗忘，同时保持了最小的效用损失，为LRMs中的隐私保护推理设立了新标准。

Conclusion: STaR是一个有效的推理时遗忘框架，能够在大规模推理模型中提供全面的隐私保护，同时引入了更全面的评估指标来量化隐私保护效果。

Abstract: Large Reasoning Models (LRMs) have advanced automated multi-step reasoning, but their ability to generate complex Chain-of-Thought (CoT) trajectories introduces severe privacy risks, as sensitive information may be deeply embedded throughout the reasoning process. Existing Large Language Models (LLMs) unlearning approaches that typically focus on modifying only final answers are insufficient for LRMs, as they fail to remove sensitive content from intermediate steps, leading to persistent privacy leakage and degraded security. To address these challenges, we propose Sensitive Trajectory Regulation (STaR), a parameter-free, inference-time unlearning framework that achieves robust privacy protection throughout the reasoning process. Specifically, we first identify sensitive content via semantic-aware detection. Then, we inject global safety constraints through secure prompt prefix. Next, we perform trajectory-aware suppression to dynamically block sensitive content across the entire reasoning chain. Finally, we apply token-level adaptive filtering to prevent both exact and paraphrased sensitive tokens during generation. Furthermore, to overcome the inadequacies of existing evaluation protocols, we introduce two metrics: Multi-Decoding Consistency Assessment (MCS), which measures the consistency of unlearning across diverse decoding strategies, and Multi-Granularity Membership Inference Attack (MIA) Evaluation, which quantifies privacy protection at both answer and reasoning-chain levels. Experiments on the R-TOFU benchmark demonstrate that STaR achieves comprehensive and stable unlearning with minimal utility loss, setting a new standard for privacy-preserving reasoning in LRMs.

</details>


### [15] [What Do LLM Agents Know About Their World? Task2Quiz: A Paradigm for Studying Environment Understanding](https://arxiv.org/abs/2601.09503)
*Siyuan Liu,Hongbang Yuan,Xinze Li,Ziyue Zhu,Yixin Cao,Yu-Gang Jiang*

Main category: cs.AI

TL;DR: 提出T2Q评估范式，将任务执行与环境理解解耦，发现任务成功不能有效反映环境理解能力


<details>
  <summary>Details</summary>
Motivation: 当前LLM智能体评估主要依赖基于轨迹的任务成功率指标，但无法评估智能体是否真正理解环境模型，存在泛化能力不足的问题

Method: 提出Task-to-Quiz（T2Q）评估范式，创建T2QBench基准，包含30个环境和1,967个基于环境的问答对，将任务执行与环境理解分离评估

Result: 实验表明任务成功与环境理解相关性弱，当前记忆机制无法有效帮助智能体获得环境的基础模型，主动探索和细粒度状态表示是主要瓶颈

Conclusion: T2Q为评估智能体环境理解能力提供了新范式，揭示了当前LLM智能体在泛化能力上的关键瓶颈，为开发更通用的自主智能体奠定了基础

Abstract: Large language model (LLM) agents have demonstrated remarkable capabilities in complex decision-making and tool-use tasks, yet their ability to generalize across varying environments remains a under-examined concern. Current evaluation paradigms predominantly rely on trajectory-based metrics that measure task success, while failing to assess whether agents possess a grounded, transferable model of the environment. To address this gap, we propose Task-to-Quiz (T2Q), a deterministic and automated evaluation paradigm designed to decouple task execution from world-state understanding. We instantiate this paradigm in T2QBench, a suite comprising 30 environments and 1,967 grounded QA pairs across multiple difficulty levels. Our extensive experiments reveal that task success is often a poor proxy for environment understanding, and that current memory machanism can not effectively help agents acquire a grounded model of the environment. These findings identify proactive exploration and fine-grained state representation as primary bottlenecks, offering a robust foundation for developing more generalizable autonomous agents.

</details>


### [16] [Omni-R1: Towards the Unified Generative Paradigm for Multimodal Reasoning](https://arxiv.org/abs/2601.09536)
*Dongjie Cheng,Yongqi Li,Zhixin Ma,Hongru Cai,Yupeng Hu,Wenjie Wang,Liqiang Nie,Wenjie Li*

Main category: cs.AI

TL;DR: Omni-R1提出统一生成式多模态推理框架，通过生成中间图像统一多种多模态推理技能，包含有监督和无监督两种实现方式。


<details>
  <summary>Details</summary>
Motivation: 现有MLLMs要么专注于纯文本推理，要么采用单一任务特定的推理模式，限制了在不同多模态任务上的泛化能力。需要统一框架来处理需要多样化推理技能的多模态任务。

Method: 提出统一生成式多模态推理范式，通过生成中间图像来统一多样化推理技能。具体实现包括：1) Omni-R1：两阶段SFT+RL框架，包含感知对齐损失和感知奖励；2) Omni-R1-Zero：从纯文本推理数据中引导生成逐步可视化，无需多模态标注。

Result: 实验结果表明，Omni-R1能在广泛的多模态任务上实现统一生成式推理，而Omni-R1-Zero在平均性能上能够匹配甚至超越Omni-R1。

Conclusion: 提出的统一生成式多模态推理框架为多模态推理提供了有前景的方向，特别是无监督的Omni-R1-Zero方法展示了从纯文本数据中学习多模态推理的潜力。

Abstract: Multimodal Large Language Models (MLLMs) are making significant progress in multimodal reasoning. Early approaches focus on pure text-based reasoning. More recent studies have incorporated multimodal information into the reasoning steps; however, they often follow a single task-specific reasoning pattern, which limits their generalizability across various multimodal tasks. In fact, there are numerous multimodal tasks requiring diverse reasoning skills, such as zooming in on a specific region or marking an object within an image. To address this, we propose unified generative multimodal reasoning, which unifies diverse multimodal reasoning skills by generating intermediate images during the reasoning process. We instantiate this paradigm with Omni-R1, a two-stage SFT+RL framework featuring perception alignment loss and perception reward, thereby enabling functional image generation. Additionally, we introduce Omni-R1-Zero, which eliminates the need for multimodal annotations by bootstrapping step-wise visualizations from text-only reasoning data. Empirical results show that Omni-R1 achieves unified generative reasoning across a wide range of multimodal tasks, and Omni-R1-Zero can match or even surpass Omni-R1 on average, suggesting a promising direction for generative multimodal reasoning.

</details>


### [17] [Collaborative Multi-Agent Test-Time Reinforcement Learning for Reasoning](https://arxiv.org/abs/2601.09667)
*Zhiyuan Hu,Yunhai Hu,Juncheng Liu,Shuyue Stella Li,Yucheng Wang,Zhen Xu,See-Kiong Ng,Anh Tuan Luu,Xinxing Xu,Bryan Hooi,Cynthia Breazeal,Hae Won Park*

Main category: cs.AI

TL;DR: MATTRL框架通过测试时强化学习，在多智能体推理过程中注入结构化文本经验，提升决策准确性，无需额外训练。


<details>
  <summary>Details</summary>
Motivation: 多智能体系统虽然通过多样性和交叉验证获得鲁棒性，但多智能体强化学习训练资源密集且不稳定，存在非平稳性和稀疏高方差奖励问题。

Method: MATTRL框架：1) 组建多专家团队进行多轮讨论；2) 检索并整合测试时经验；3) 达成共识进行最终决策；同时研究信用分配机制构建回合级经验池并重新注入对话。

Result: 在医学、数学和教育等挑战性基准测试中，MATTRL比多智能体基线平均提升3.67%准确率，比单智能体基线提升8.67%；消融研究分析了不同信用分配方案对训练结果的影响。

Conclusion: MATTRL提供了一种稳定、有效且高效的路径，无需调优即可实现分布偏移鲁棒的多智能体推理。

Abstract: Multi-agent systems have evolved into practical LLM-driven collaborators for many applications, gaining robustness from diversity and cross-checking. However, multi-agent RL (MARL) training is resource-intensive and unstable: co-adapting teammates induce non-stationarity, and rewards are often sparse and high-variance. Therefore, we introduce \textbf{Multi-Agent Test-Time Reinforcement Learning (MATTRL)}, a framework that injects structured textual experience into multi-agent deliberation at inference time. MATTRL forms a multi-expert team of specialists for multi-turn discussions, retrieves and integrates test-time experiences, and reaches consensus for final decision-making. We also study credit assignment for constructing a turn-level experience pool, then reinjecting it into the dialogue. Across challenging benchmarks in medicine, math, and education, MATTRL improves accuracy by an average of 3.67\% over a multi-agent baseline, and by 8.67\% over comparable single-agent baselines. Ablation studies examine different credit-assignment schemes and provide a detailed comparison of how they affect training outcomes. MATTRL offers a stable, effective and efficient path to distribution-shift-robust multi-agent reasoning without tuning.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [18] [LAUDE: LLM-Assisted Unit Test Generation and Debugging of Hardware DEsigns](https://arxiv.org/abs/2601.08856)
*Deeksha Nandal,Riccardo Revalor,Soham Dan,Debjit Pal*

Main category: cs.SE

TL;DR: LAUDE是一个基于大语言模型的硬件设计单元测试生成与调试统一框架，通过结合设计源代码语义理解和思维链推理，显著提升了硬件设计的测试覆盖率和调试效率。


<details>
  <summary>Details</summary>
Motivation: 硬件设计中的单元测试对于确保模块功能正确性至关重要，但传统方法需要工程师深入理解设计功能并具备创造性思维，且调试过程耗时费力。需要一种自动化方法来提高测试生成和调试的效率。

Method: LAUDE框架结合了设计源代码的语义理解和基础大语言模型的思维链推理能力，通过提示工程和设计执行信息的集成，增强单元测试生成准确性和代码可调试性。

Result: 在VerilogEval数据集中的buggy硬件设计代码上测试，LAUDE生成的单元测试在组合设计中检测到高达100%的bug，在时序设计中检测到93%的bug；调试成功率分别为93%和84%。

Conclusion: LAUDE通过统一的大语言模型驱动框架，有效解决了硬件设计中单元测试生成和调试的挑战，显著提高了硬件验证的自动化水平和效率。

Abstract: Unit tests are critical in the hardware design lifecycle to ensure that component design modules are functionally correct and conform to the specification before they are integrated at the system level. Thus developing unit tests targeting various design features requires deep understanding of the design functionality and creativity. When one or more unit tests expose a design failure, the debugging engineer needs to diagnose, localize, and debug the failure to ensure design correctness, which is often a painstaking and intense process. In this work, we introduce LAUDE, a unified unit-test generation and debugging framework for hardware designs that cross-pollinates the semantic understanding of the design source code with the Chain-of-Thought (CoT) reasoning capabilities of foundational Large-Language Models (LLMs). LAUDE integrates prompt engineering and design execution information to enhance its unit test generation accuracy and code debuggability. We apply LAUDE with closed- and open-source LLMs to a large corpus of buggy hardware design codes derived from the VerilogEval dataset, where generated unit tests detected bugs in up to 100% and 93% of combinational and sequential designs and debugged up to 93% and 84% of combinational and sequential designs, respectively.

</details>


### [19] [Revisiting Software Engineering Education in the Era of Large Language Models: A Curriculum Adaptation and Academic Integrity Framework](https://arxiv.org/abs/2601.08857)
*Mustafa Degerli*

Main category: cs.SE

TL;DR: 论文提出理论框架分析生成式AI如何改变软件工程核心能力，并设计LLM集成教育模型，特别关注土耳其计算机工程教育面临的挑战。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（如ChatGPT、GitHub Copilot）正在重塑软件工程实践，降低了代码生成、解释和测试的成本，但当前软件工程教育仍以手动语法生产为核心，导致评估有效性、学习成果和基础技能发展方面存在日益严重的错位。

Method: 采用概念研究方法，提出理论框架分析生成式AI如何改变软件工程核心能力，并设计LLM集成教育模型，特别关注土耳其计算机工程教育中集中监管、大班教学和考试导向评估等挑战。

Result: 框架显示问题分析、设计、实现和测试正从构建转向批判、验证和人机协同管理；传统抄袭检测机制已不足，需要向过程透明度模型过渡。

Conclusion: 本文为课程适应提供了结构化建议，但仍属理论贡献，需要纵向实证研究来评估这些干预措施及其对学习的长期影响。

Abstract: The integration of Large Language Models (LLMs), such as ChatGPT and GitHub Copilot, into professional workflows is increasingly reshaping software engineering practices. These tools have lowered the cost of code generation, explanation, and testing, while introducing new forms of automation into routine development tasks. In contrast, most of the software engineering and computer engineering curricula remain closely aligned with pedagogical models that equate manual syntax production with technical competence. This growing misalignment raises concerns regarding assessment validity, learning outcomes, and the development of foundational skills. Adopting a conceptual research approach, this paper proposes a theoretical framework for analyzing how generative AI alters core software engineering competencies and introduces a pedagogical design model for LLM-integrated education. Attention is given to computer engineering programs in Turkey, where centralized regulation, large class sizes, and exam-oriented assessment practices amplify these challenges. The framework delineates how problem analysis, design, implementation, and testing increasingly shift from construction toward critique, validation, and human-AI stewardship. In addition, the paper argues that traditional plagiarism-centric integrity mechanisms are becoming insufficient, motivating a transition toward a process transparency model. While this work provides a structured proposal for curriculum adaptation, it remains a theoretical contribution; the paper concludes by outlining the need for longitudinal empirical studies to evaluate these interventions and their long-term impacts on learning.

</details>


### [20] [Adaptive Trust Metrics for Multi-LLM Systems: Enhancing Reliability in Regulated Industries](https://arxiv.org/abs/2601.08858)
*Tejaswini Bollikonda*

Main category: cs.SE

TL;DR: 该论文提出了一种用于多LLM生态系统的自适应信任度量框架，旨在量化并提升在监管约束下的模型可靠性，通过系统行为分析、不确定性评估和动态监控管道来实现操作可信度。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型在医疗、金融、法律等敏感领域的部署日益增多，围绕信任、问责和可靠性的问题变得日益紧迫。需要建立能够适应监管约束的信任度量机制来确保AI系统的安全采用。

Method: 提出了一个自适应信任度量框架，包括：1）分析系统行为；2）评估多个LLM之间的不确定性；3）实施动态监控管道。通过金融合规和医疗诊断的案例研究验证框架的实用性。

Result: 研究展示了自适应信任度量在实际场景中的适用性，特别是在金融合规和医疗诊断领域。该框架为在监管行业中实现安全和可扩展的AI采用提供了可行的技术路径。

Conclusion: 自适应信任度量是监管行业中实现安全、可扩展AI采用的基础性推动因素，能够有效解决多LLM生态系统中的信任、问责和可靠性问题。

Abstract: Large Language Models (LLMs) are increasingly deployed in sensitive domains such as healthcare, finance, and law, yet their integration raises pressing concerns around trust, accountability, and reliability. This paper explores adaptive trust metrics for multi LLM ecosystems, proposing a framework for quantifying and improving model reliability under regulated constraints. By analyzing system behaviors, evaluating uncertainty across multiple LLMs, and implementing dynamic monitoring pipelines, the study demonstrates practical pathways for operational trustworthiness. Case studies from financial compliance and healthcare diagnostics illustrate the applicability of adaptive trust metrics in real world settings. The findings position adaptive trust measurement as a foundational enabler for safe and scalable AI adoption in regulated industries.

</details>


### [21] [EZInput: A Cross-Environment Python Library for Easy UI Generation in Scientific Computing](https://arxiv.org/abs/2601.08859)
*Bruno M. Saraiva,Iván Hidalgo-Cenalmor,António D. Brito,Damián Martínez,Tayla Shakespeare,Guillaume Jacquemet,Ricardo Henriques*

Main category: cs.SE

TL;DR: EZInput是一个跨运行时环境的Python库，允许算法开发者自动生成图形用户界面，使非编程专家也能使用计算工具，解决参数配置碎片化问题。


<details>
  <summary>Details</summary>
Motivation: 研究人员在应用计算算法时面临参数配置的持续障碍：需要编程技能、不同环境的接口差异、会话间设置难以持久保存。这种碎片化导致重复输入、迭代探索缓慢，且由于参数选择难以记录、共享和重用，破坏了研究的可重复性。

Method: EZInput采用声明式规范系统，开发者只需定义一次输入需求和验证约束；库自动处理环境检测、界面渲染、参数验证和会话持久化，支持Jupyter notebooks、Google Colab和终端环境。参数持久化机制通过轻量级YAML文件保存和恢复用户配置。

Result: 实现了"一次编写，随处运行"的架构，使研究人员能在notebook中原型设计，并在远程系统上部署完全相同的参数配置进行批量执行，无需代码更改或手动转录。内置验证确保数据完整性，清晰的反馈减少用户摩擦。

Conclusion: EZInput通过自动生成GUI、跨环境兼容性和参数持久化，解决了计算工具参数配置的碎片化问题，提高了可访问性、效率和可重复性，特别适合科学计算工作流程。

Abstract: Researchers face a persistent barrier when applying computational algorithms with parameter configuration typically demanding programming skills, interfaces differing across environments, and settings rarely persisting between sessions. This fragmentation forces repetitive input, slows iterative exploration, and undermines reproducibility because parameter choices are difficult to record, share, and reuse. We present EZInput, a cross-runtime environment Python library enabling algorithm developers to automatically generate graphical user interfaces that make their computational tools accessible to end-users without programming expertise. EZInput employs a declarative specification system where developers define input requirements and validation constraints once; the library then handles environment detection, interface rendering, parameter validation, and session persistence across Jupyter notebooks, Google Colab, and terminal environments. This "write once, run anywhere" architecture enables researchers to prototype in notebooks and deploy identical parameter configurations for batch execution on remote systems without code changes or manual transcription. Parameter persistence, inspired by ImageJ/FIJI and adapted to Python workflows, saves and restores user configurations via lightweight YAML files, eliminating redundant input and producing shareable records that enhance reproducibility. EZInput supports diverse input types essential for scientific computing and it also includes built-in validation that ensures data integrity and clear feedback that reduces user friction.

</details>


### [22] [Bridging the Gap: Empowering Small Models in Reliable OpenACC-based Parallelization via GEPA-Optimized Prompting](https://arxiv.org/abs/2601.08884)
*Samyak Jhaveri,Cristina V. Lopes*

Main category: cs.SE

TL;DR: 使用遗传-帕累托框架优化提示，显著提升小型LLM生成OpenACC并行代码的质量和编译成功率


<details>
  <summary>Details</summary>
Motivation: OpenACC降低了GPU卸载的门槛，但编写高性能的pragma仍然复杂，需要深厚的内存层次、数据移动和并行化策略专业知识。大型语言模型为自动化并行代码生成提供了有前景的解决方案，但简单的提示通常会导致语法错误的指令、无法编译的代码或性能无法超过CPU基准。

Method: 提出系统性提示优化方法，利用GEPA（遗传-帕累托）框架，通过反射反馈循环迭代演化提示。该过程利用指令的交叉和变异，以专家策划的黄金示例为指导，并基于黄金和预测pragma之间的子句和子句参数级不匹配提供结构化反馈。

Result: 在PolyBench套件评估中，使用优化提示生成的OpenACC pragma的编译成功率显著提高，特别是对于"nano"规模模型。GPT-4.1 Nano的编译成功率从66.7%提升到93.3%，GPT-5 Nano从86.7%提升到100%，匹配或超越了更大、更昂贵版本的能力。优化提示还使实现GPU加速超过CPU基准的程序数量增加了21%。

Conclusion: 提示优化有效释放了更小、更便宜的LLM在编写稳定有效的GPU卸载指令方面的潜力，为HPC工作流中基于指令的自动化并行化建立了经济高效的途径。

Abstract: OpenACC lowers the barrier to GPU offloading, but writing high-performing pragma remains complex, requiring deep domain expertise in memory hierarchies, data movement, and parallelization strategies. Large Language Models (LLMs) present a promising potential solution for automated parallel code generation, but naive prompting often results in syntactically incorrect directives, uncompilable code, or performance that fails to exceed CPU baselines. We present a systematic prompt optimization approach to enhance OpenACC pragma generation without the prohibitive computational costs associated with model post-training. Leveraging the GEPA (GEnetic-PAreto) framework, we iteratively evolve prompts through a reflective feedback loop. This process utilizes crossover and mutation of instructions, guided by expert-curated gold examples and structured feedback based on clause- and clause parameter-level mismatches between the gold and predicted pragma. In our evaluation on the PolyBench suite, we observe an increase in compilation success rates for programs annotated with OpenACC pragma generated using the optimized prompts compared to those annotated using the simpler initial prompt, particularly for the "nano"-scale models. Specifically, with optimized prompts, the compilation success rate for GPT-4.1 Nano surged from 66.7% to 93.3%, and for GPT-5 Nano improved from 86.7% to 100%, matching or surpassing the capabilities of their significantly larger, more expensive versions. Beyond compilation, the optimized prompts resulted in a 21% increase in the number of programs that achieve functional GPU speedups over CPU baselines. These results demonstrate that prompt optimization effectively unlocks the potential of smaller, cheaper LLMs in writing stable and effective GPU-offloading directives, establishing a cost-effective pathway to automated directive-based parallelization in HPC workflows.

</details>


### [23] [Build Code is Still Code: Finding the Antidote for Pipeline Poisoning](https://arxiv.org/abs/2601.08995)
*Brent Pappas,Paul Gazzillo*

Main category: cs.SE

TL;DR: 该论文提出了一种名为"开发阶段隔离"的新策略，通过将构建自动化视为程序代码来建模其信息和行为权限，从而保护构建系统免受投毒攻击。原型工具Foreman成功检测了XZ Utils攻击中的投毒测试文件。


<details>
  <summary>Details</summary>
Motivation: C项目不仅包含C代码，还包含用于自动化开发任务的构建系统代码。这些构建系统对软件供应链安全至关重要，但容易受到投毒攻击（如XZ Utils和SolarWinds攻击）。现有技术要么只验证软件依赖项而忽略构建系统本身，要么只分析程序代码而不分析构建系统代码。投毒的构建系统可以轻松绕过程序代码漏洞检测工具。

Method: 提出了"开发阶段隔离"策略，将构建自动化的信息和行为权限建模为程序代码。开发了原型工具Foreman来实现这种方法，通过检查构建系统的权限和隔离来检测潜在的投毒攻击。

Result: Foreman成功检测并警告了XZ Utils攻击中涉及的投毒测试文件。该工具能够识别构建系统中的安全威胁，而传统方法无法检测这些威胁。

Conclusion: 构建系统安全检查器应该像程序代码检查器一样普及。未来计划通过自动检查开发阶段隔离来防止流水线投毒，为软件供应链安全提供更全面的保护。

Abstract: Open source C code underpins society's computing infrastructure. Decades of work has helped harden C code against attackers, but C projects do not consist of only C code. C projects also contain build system code for automating development tasks like compilation, testing, and packaging. These build systems are critcal to software supply chain security and vulnerable to being poisoned, with the XZ Utils and SolarWinds attacks being recent examples. Existing techniques try to harden software supply chains by verifying software dependencies, but such methods ignore the build system itself. Similarly, classic software security checkers only analyze and monitor program code, not build system code. Moreover, poisoned build systems can easily circumvent tools for detecting program code vulnerabilities by disabling such checks. We present development phase isolation, a novel strategy for hardening build systems against poisoning by modeling the information and behavior permissions of build automation as if it were program code. We have prototyped this approach as a tool called Foreman, which successfully detects and warns about the poisoned test files involved in the XZ Utils attack. We outline our future plans to protect against pipeline poisoning by automatically checking development phase isolation. We envision a future where build system security checkers are as prevalent as program code checkers.

</details>


### [24] [On the Flakiness of LLM-Generated Tests for Industrial and Open-Source Database Management Systems](https://arxiv.org/abs/2601.08998)
*Alexander Berndt,Thomas Bach,Rainer Gemulla,Marcus Kessel,Sebastian Baltes*

Main category: cs.SE

TL;DR: LLM生成的数据库测试中存在不稳定性问题，其中63%的不稳定测试源于对无序集合的顺序依赖，且闭源系统更容易出现不稳定性传递。


<details>
  <summary>Details</summary>
Motivation: 研究LLM生成的测试中不稳定性问题的普遍性和根本原因，特别是在关系数据库管理系统测试中，因为现有研究已识别不稳定性是LLM生成测试的潜在问题，但其普遍性和原因尚不清楚。

Method: 使用GPT-4o和Mistral-Large-Instruct-2407两个LLM对四个关系数据库管理系统（SAP HANA、DuckDB、MySQL、SQLite）的测试套件进行扩增，评估生成测试用例的不稳定性，并通过手动检查分析根本原因。

Result: 生成测试的不稳定测试比例略高于现有测试；63%的不稳定测试（72/115）的根本原因是测试依赖未保证的顺序（"无序集合"）；LLM通过提示上下文将现有测试的不稳定性传递到新生成的测试中；闭源系统（如SAP HANA）比开源系统更容易出现不稳定性传递。

Conclusion: 研究为开发者提供了LLM生成测试中可能遇到的不稳定性类型信息，并强调了在使用LLM进行测试生成时提供定制化上下文的重要性。

Abstract: Flaky tests are a common problem in software testing. They produce inconsistent results when executed multiple times on the same code, invalidating the assumption that a test failure indicates a software defect. Recent work on LLM-based test generation has identified flakiness as a potential problem with generated tests. However, its prevalence and underlying causes are unclear. We examined the flakiness of LLM-generated tests in the context of four relational database management systems: SAP HANA, DuckDB, MySQL, and SQLite. We amplified test suites with two LLMs, GPT-4o and Mistral-Large-Instruct-2407, to assess the flakiness of the generated test cases. Our results suggest that generated tests have a slightly higher proportion of flaky tests compared to existing tests. Based on a manual inspection, we found that the most common root cause of flakiness was the reliance of a test on a certain order that is not guaranteed ("unordered collection"), which was present in 72 of 115 flaky tests (63%). Furthermore, both LLMs transferred the flakiness from the existing tests to the newly generated tests via the provided prompt context. Our experiments suggest that flakiness transfer is more prevalent in closed-source systems such as SAP HANA than in open-source systems. Our study informs developers on what types of flakiness to expect from LLM-generated tests. It also highlights the importance of providing LLMs with tailored context when employing LLMs for test generation.

</details>


### [25] [SafePlanner: Testing Safety of the Automated Driving System Plan Model](https://arxiv.org/abs/2601.09171)
*Dohyun Kim,Sanggu Han,Sangmin Woo,Joonha Jang,Jaehoon Kim,Changhun Song,Yongdae Kim*

Main category: cs.SE

TL;DR: SafePlanner是一个用于自动驾驶系统规划模型安全测试的框架，通过结构分析和引导式模糊测试发现安全关键缺陷，在百度Apollo上检测到520个危险行为


<details>
  <summary>Details</summary>
Motivation: 自动驾驶系统规划模型的安全关键缺陷识别需要系统化的测试方法，传统测试方法在生成有意义测试场景和检测危险规划行为方面存在不足

Method: 对规划模型实现进行结构分析，提取可行的场景转换，结合NPC行为组合测试场景，应用引导式模糊测试探索规划模型的行为空间

Result: 在百度Apollo上生成20635个测试用例，检测到520个危险行为，归为15个根本原因，修复4个问题后无副作用，达到83.63%函数覆盖率和63.22%决策覆盖率

Conclusion: SafePlanner能有效识别自动驾驶系统规划模型的安全缺陷，在错误发现和测试效率方面优于基线方法，为ADS安全测试提供了系统化框架

Abstract: In this work, we present SafePlanner, a systematic testing framework for identifying safety-critical flaws in the Plan model of Automated Driving Systems (ADS). SafePlanner targets two core challenges: generating structurally meaningful test scenarios and detecting hazardous planning behaviors. To maximize coverage, SafePlanner performs a structural analysis of the Plan model implementation - specifically, its scene-transition logic and hierarchical control flow - and uses this insight to extract feasible scene transitions from code. It then composes test scenarios by combining these transitions with non-player vehicle (NPC) behaviors. Guided fuzzing is applied to explore the behavioral space of the Plan model under these scenarios. We evaluate SafePlanner on Baidu Apollo, a production-grade level 4 ADS. It generates 20635 test cases and detects 520 hazardous behaviors, grouped into 15 root causes through manual analysis. For four of these, we applied patches based on our analysis; the issues disappeared, and no apparent side effects were observed. SafePlanner achieves 83.63 percent function and 63.22 percent decision coverage on the Plan model, outperforming baselines in both bug discovery and efficiency.

</details>


### [26] [AI-NativeBench: An Open-Source White-Box Agentic Benchmark Suite for AI-Native Systems](https://arxiv.org/abs/2601.09393)
*Zirui Wang,Guangba Yu,Michael R. Lyu*

Main category: cs.SE

TL;DR: AI-NativeBench：首个基于MCP和A2A标准的应用中心化白盒AI原生基准测试套件，揭示传统黑盒评估无法发现的系统级工程特性


<details>
  <summary>Details</summary>
Motivation: 从云原生到AI原生架构的转型正在重塑软件工程，但传统黑盒评估范式已不足以评估概率性智能体服务，现有基准测试仅衡量原始模型能力而忽视系统级执行动态

Method: 引入AI-NativeBench基准测试套件，基于模型上下文协议（MCP）和智能体到智能体（A2A）标准，将智能体跨度作为分布式追踪中的一等公民，实现超越简单能力的细粒度工程特性分析

Result: 在21个系统变体上发现传统指标无法揭示的关键工程现实：参数悖论（轻量模型在协议遵循上常优于旗舰模型）、普遍存在的推理主导（协议开销次要）、昂贵失败模式（自愈机制在不可行工作流上成为成本倍增器）

Conclusion: 该工作首次提供系统化证据，指导从衡量模型能力转向工程化可靠AI原生系统的转型，并开源基准测试和数据集以促进可复现性和进一步研究

Abstract: The transition from Cloud-Native to AI-Native architectures is fundamentally reshaping software engineering, replacing deterministic microservices with probabilistic agentic services. However, this shift renders traditional black-box evaluation paradigms insufficient: existing benchmarks measure raw model capabilities while remaining blind to system-level execution dynamics. To bridge this gap, we introduce AI-NativeBench, the first application-centric and white-box AI-Native benchmark suite grounded in Model Context Protocol (MCP) and Agent-to-Agent (A2A) standards. By treating agentic spans as first-class citizens within distributed traces, our methodology enables granular analysis of engineering characteristics beyond simple capabilities. Leveraging this benchmark across 21 system variants, we uncover critical engineering realities invisible to traditional metrics: a parameter paradox where lightweight models often surpass flagships in protocol adherence, a pervasive inference dominance that renders protocol overhead secondary, and an expensive failure pattern where self-healing mechanisms paradoxically act as cost multipliers on unviable workflows. This work provides the first systematic evidence to guide the transition from measuring model capability to engineering reliable AI-Native systems. To facilitate reproducibility and further research, we have open-sourced the benchmark and dataset.

</details>


### [27] [Towards a Metadata Schema for Energy Research Software](https://arxiv.org/abs/2601.09456)
*Stephan Ferenz,Oliver Werth,Astrid Nieße*

Main category: cs.SE

TL;DR: 开发了一个能源研究软件的元数据模式，通过需求分析和用户测试验证，该模式在形式化、互操作性和领域特定需求之间取得了平衡。


<details>
  <summary>Details</summary>
Motivation: 许多领域（包括能源研究）缺乏成熟的元数据模式，这影响了研究软件的可发现性和可重用性，也不符合FAIR4RS原则。需要为能源研究软件开发专门的元数据模式来解决这一差距。

Method: 基于需求分析开发了能源研究软件的元数据模式，并通过用户测试进行评估。

Result: 开发的元数据模式在形式化、互操作性和能源研究人员的特定需求之间取得了平衡。测试表明，良好的信息呈现方式是研究人员创建所需元数据的关键。

Conclusion: 本文为设计能源研究软件元数据模式提供了挑战和机遇的见解，强调了在形式化需求和用户友好性之间取得平衡的重要性。

Abstract: Domain-specific metadata schemas are essential to improve the findability and reusability of research software and to follow the FAIR4RS principles. However, many domains, including energy research, lack established metadata schemas. To address this gap, we developed a metadata schema for energy research software based on a requirement analysis and evaluated it through user testing. Our results show that the schema balances the need for formalization and interoperability, while also meeting the specific needs of energy researchers. Meanwhile, the testing showed that a good presentation of the required information is key to enable researchers to create the required metadata. This paper provides insights into the challenges and opportunities of designing a metadata schema for energy research software.

</details>


### [28] [Analyzing GitHub Issues and Pull Requests in nf-core Pipelines: Insights into nf-core Pipeline Repositories](https://arxiv.org/abs/2601.09612)
*Khairul Alam,Banani Roy*

Main category: cs.SE

TL;DR: 对nf-core科学工作流管道的25,173个问题和拉取请求进行实证研究，识别出13个关键挑战，分析解决动态，发现标签和代码片段能显著提高解决概率。


<details>
  <summary>Details</summary>
Motivation: 尽管Nextflow和nf-core在数据密集型科学领域广泛采用，但用户在这些管道的开发和维护过程中面临的具体挑战尚不清楚。本研究旨在通过实证分析揭示这些挑战、管理实践和感知困难。

Method: 使用BERTopic主题建模方法分析25,173个问题和拉取请求，识别关键挑战类别。通过统计分析问题解决动态，包括解决率、解决时间，以及标签、代码片段等因素对解决概率的影响。

Result: 识别出13个关键挑战，包括管道开发与集成、错误修复、基因组数据集成、CI配置管理和版本更新等。89.38%的问题和拉取请求最终被关闭，半数在3天内解决。标签（大效应，δ=0.94）和代码片段（中效应，δ=0.50）显著提高解决概率。工具开发和仓库维护是最具挑战性的领域。

Conclusion: 本研究为nf-core管道的协作开发和维护提供了可操作的见解，强调了增强其可用性、可持续性和可重复性的机会。研究结果有助于改进科学工作流管理系统的开发实践。

Abstract: Scientific Workflow Management Systems (SWfMSs) such as Nextflow have become essential software frameworks for conducting reproducible, scalable, and portable computational analyses in data-intensive fields like genomics, transcriptomics, and proteomics. Building on Nextflow, the nf-core community curates standardized, peer-reviewed pipelines that follow strict testing, documentation, and governance guidelines. Despite its broad adoption, little is known about the challenges users face during the development and maintenance of these pipelines. This paper presents an empirical study of 25,173 issues and pull requests from these pipelines to uncover recurring challenges, management practices, and perceived difficulties. Using BERTopic modeling, we identify 13 key challenges, including pipeline development and integration, bug fixing, integrating genomic data, managing CI configurations, and handling version updates. We then examine issue resolution dynamics, showing that 89.38\% of issues and pull requests are eventually closed, with half resolved within three days. Statistical analysis reveals that the presence of labels (large effect, $δ$ = 0.94) and code snippets (medium effect, $δ$ = 0.50) significantly improve resolution likelihood. Further analysis reveals that tool development and repository maintenance poses the most significant challenges, followed by testing pipelines and CI configurations, and debugging containerized pipelines. Overall, this study provides actionable insights into the collaborative development and maintenance of nf-core pipelines, highlighting opportunities to enhance their usability, sustainability, and reproducibility.

</details>


### [29] [SysPro: Reproducing System-level Concurrency Bugs from Bug Reports](https://arxiv.org/abs/2601.09616)
*Tarannum Shaila Zaman,Zhihui Yan,Chen Wang,Chadni Islam,Jiangfan Shi,Tingting Yu*

Main category: cs.SE

TL;DR: SysPro：一种从bug报告中自动提取系统调用信息并生成输入数据来复现系统级并发bug的新方法


<details>
  <summary>Details</summary>
Motivation: 复现系统级并发bug需要输入数据和精确的系统调用交错顺序，但由于bug的非确定性和bug报告缺乏详细信息，这一过程具有挑战性。现有工具无法有效处理系统调用级别的特定交错。

Method: SysPro通过信息检索、正则表达式匹配和类别划分方法，自动从bug报告中提取相关系统调用名称并在源代码中定位，生成输入数据，然后通过动态源代码插桩来复现bug。

Result: 在真实世界基准测试上的实证研究表明，SysPro在从bug报告中定位和复现系统级并发bug方面既有效又高效。

Conclusion: SysPro为解决从自然语言bug报告中复现系统级并发bug的挑战提供了一种有效方法，能够自动提取关键信息并成功复现这些难以处理的bug。

Abstract: Reproducing system-level concurrency bugs requires both input data and the precise interleaving order of system calls. This process is challenging because such bugs are non-deterministic, and bug reports often lack the detailed information needed. Additionally, the unstructured nature of reports written in natural language makes it difficult to extract necessary details. Existing tools are inadequate to reproduce these bugs due to their inability to manage the specific interleaving at the system call level. To address these challenges, we propose SysPro, a novel approach that automatically extracts relevant system call names from bug reports and identifies their locations in the source code. It generates input data by utilizing information retrieval, regular expression matching, and the category-partition method. This extracted input and interleaving data are then used to reproduce bugs through dynamic source code instrumentation. Our empirical study on real-world benchmarks demonstrates that SysPro is both effective and efficient at localizing and reproducing system-level concurrency bugs from bug reports.

</details>


### [30] [How well LLM-based test generation techniques perform with newer LLM versions?](https://arxiv.org/abs/2601.09695)
*Michael Konstantinou,Renzo Degiovanni,Mike Papadakis*

Main category: cs.SE

TL;DR: 研究发现，在单元测试生成任务中，使用最新的大型语言模型（LLM）的简单方法，在测试覆盖率方面优于现有的复杂技术，且成本相当。


<details>
  <summary>Details</summary>
Motivation: 当前LLM驱动的单元测试生成技术通常包含复杂的后处理组件，但这些技术大多基于较旧的LLM版本进行评估。随着LLM能力的快速提升，这些复杂技术可能已不再必要，简单的LLM方法可能已经足够有效。

Method: 复制了四种最先进的LLM测试生成工具（HITS、SymPrompt、TestSpark、CoverUp），并将它们与简单的LLM方法进行比较。在所有方法中集成了当前最新的LLM版本，并在393个类和3,657个方法上进行了实验评估。

Result: 简单的LLM方法在所有测试有效性指标上都优于现有技术：行覆盖率提高17.72%，分支覆盖率提高19.80%，变异得分提高20.92%，且LLM查询成本相当。研究还发现，应用LLM的粒度对成本有显著影响。

Conclusion: 随着LLM能力的提升，复杂的测试生成技术可能已不再必要。建议采用两阶段策略：首先针对程序类生成测试（效率更高），然后针对未覆盖的方法，这样可以减少约20%的LLM请求，同时保持相当的测试效果。

Abstract: The rapid evolution of Large Language Models (LLMs) has strongly impacted software engineering, leading to a growing number of studies on automated unit test generation. However, the standalone use of LLMs without post-processing has proven insufficient, often producing tests that fail to compile or achieve high coverage. Several techniques have been proposed to address these issues, reporting improvements in test compilation and coverage. While important, LLM-based test generation techniques have been evaluated against relatively weak baselines (for todays' standards), i.e., old LLM versions and relatively weak prompts, which may exacerbate the performance contribution of the approaches. In other words, stronger (newer) LLMs may obviate any advantage these techniques bring. We investigate this issue by replicating four state-of-the-art LLM-based test generation tools, HITS, SymPrompt, TestSpark, and CoverUp that include engineering components aimed at guiding the test generation process through compilation and execution feedback, and evaluate their relative effectiveness and efficiency over a plain LLM test generation method. We integrate current LLM versions in all approaches and run an experiment on 393 classes and 3,657 methods. Our results show that the plain LLM approach can outperform previous state-of-the-art approaches in all test effectiveness metrics we used: line coverage (by 17.72%), branch coverage (by 19.80%) and mutation score (by 20.92%), and it does so at a comparable cost (LLM queries). We also observe that the granularity at which the plain LLM is applied has a significant impact on the cost. We therefore propose targeting first the program classes, where test generation is more efficient, and then the uncovered methods to reduce the number of LLM requests. This strategy achieves comparable (slightly higher) effectiveness while requiring about 20% fewer LLM requests.

</details>


### [31] [ShortCoder: Knowledge-Augmented Syntax Optimization for Token-Efficient Code Generation](https://arxiv.org/abs/2601.09703)
*Sicong Liu,Yanxian Huang,Mingwei Liu,Jiachi Chen,Ensheng Shi,Yuchi Ma,Hongyu Zhang,Yin Zhang,Yanlin Wang*

Main category: cs.SE

TL;DR: ShortCoder是一个知识注入框架，通过语法级简化规则、混合数据合成和微调策略，在保持语义等价和可读性的前提下，显著提升代码生成效率。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在代码生成方面取得了显著进展，但其效率受到架构限制的影响。每个token生成都需要完整的推理过程，需要持续保留上下文信息，增加了资源消耗。现有研究主要关注推理阶段的优化（如提示压缩和模型量化），而生成阶段的研究相对不足。

Method: 1. 提出了10个基于AST保持转换的Python语法级简化规则，实现18.1%的token减少而不影响功能；2. 设计了混合数据合成管道，结合基于规则的改写和LLM引导的细化，创建了ShorterCodeBench语料库；3. 开发了将简洁性意识注入基础LLM的微调策略。

Result: ShortCoder在HumanEval基准测试中持续优于最先进方法，在确保代码生成性能的同时，相比先前方法实现了18.1%-37.8%的生成效率提升。

Conclusion: ShortCoder框架通过语法简化、数据合成和模型微调的有效结合，在保持语义等价和可读性的前提下，显著提升了代码生成的效率，为解决LLM代码生成中的资源消耗问题提供了有效方案。

Abstract: Code generation tasks aim to automate the conversion of user requirements into executable code, significantly reducing manual development efforts and enhancing software productivity. The emergence of large language models (LLMs) has significantly advanced code generation, though their efficiency is still impacted by certain inherent architectural constraints. Each token generation necessitates a complete inference pass, requiring persistent retention of contextual information in memory and escalating resource consumption. While existing research prioritizes inference-phase optimizations such as prompt compression and model quantization, the generation phase remains underexplored. To tackle these challenges, we propose a knowledge-infused framework named ShortCoder, which optimizes code generation efficiency while preserving semantic equivalence and readability. In particular, we introduce: (1) ten syntax-level simplification rules for Python, derived from AST-preserving transformations, achieving 18.1% token reduction without functional compromise; (2) a hybrid data synthesis pipeline integrating rule-based rewriting with LLM-guided refinement, producing ShorterCodeBench, a corpus of validated tuples of original code and simplified code with semantic consistency; (3) a fine-tuning strategy that injects conciseness awareness into the base LLMs. Extensive experimental results demonstrate that ShortCoder consistently outperforms state-of-the-art methods on HumanEval, achieving an improvement of 18.1%-37.8% in generation efficiency over previous methods while ensuring the performance of code generation.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [32] [Integrating APK Image and Text Data for Enhanced Threat Detection: A Multimodal Deep Learning Approach to Android Malware](https://arxiv.org/abs/2601.08959)
*Md Mashrur Arifin,Maqsudur Rahman,Nasir U. Eisty*

Main category: cs.CR

TL;DR: 本文提出了一种多模态深度学习框架，结合APK图像和文本特征来增强Android恶意软件检测。研究发现高分辨率RGB图像性能最佳，但图像与文本的多模态集成效果有限。


<details>
  <summary>Details</summary>
Motivation: 随着零日Android恶意软件攻击日益复杂，现有研究常忽略图像类型和分辨率对检测的影响，并忽视APK中的文本数据（如权限和元数据），限制了全面捕捉恶意行为的能力。

Method: 提出多模态深度学习框架，系统评估不同图像类型和分辨率在多种CNN架构（VGG、ResNet-152、MobileNet、DenseNet、EfficientNet-B4）上的表现，并使用LLaMA-2提取和标注文本特征，通过CLIP模型集成图像和文本数据。

Result: 高分辨率RGB图像（如256x256、512x512）在分类性能上表现最佳；但使用CLIP模型进行图像和文本多模态集成显示出有限的潜力。

Conclusion: 系统评估图像属性和集成多模态数据对于开发有效的Android恶意软件检测系统至关重要，但当前图像-文本多模态集成方法效果有限。

Abstract: As zero-day Android malware attacks grow more sophisticated, recent research highlights the effectiveness of using image-based representations of malware bytecode to detect previously unseen threats. However, existing studies often overlook how image type and resolution affect detection and ignore valuable textual data in Android Application Packages (APKs), such as permissions and metadata, limiting their ability to fully capture malicious behavior. The integration of multimodality, which combines image and text data, has gained momentum as a promising approach to address these limitations. This paper proposes a multimodal deep learning framework integrating APK images and textual features to enhance Android malware detection. We systematically evaluate various image types and resolutions across different Convolutional Neural Networks (CNN) architectures, including VGG, ResNet-152, MobileNet, DenseNet, EfficientNet-B4, and use LLaMA-2, a large language model, to extract and annotate textual features for improved analysis. The findings demonstrate that RGB images at higher resolutions (e.g., 256x256, 512x512) achieve superior classification performance, while the multimodal integration of image and text using the CLIP model reveals limited potential. Overall, this research highlights the importance of systematically evaluating image attributes and integrating multimodal data to develop effective malware detection for Android systems.

</details>


### [33] [ABE-VVS: Attribute-Based Encrypted Volumetric Video Streaming](https://arxiv.org/abs/2601.08987)
*Mohammad Waquas Usmani,Susmit Shannigrahi,Michael Zink*

Main category: cs.CR

TL;DR: ABE-VVS框架通过基于属性的选择性坐标加密实现点云体视频流媒体轻量级DRM，仅加密部分坐标（X、Y、Z或其组合）而非整个点云帧，在保持有效视觉混淆的同时显著降低计算开销和延迟。


<details>
  <summary>Details</summary>
Motivation: 传统点云视频流媒体DRM方案通常需要加密整个点云帧，导致高计算开销和延迟。需要一种轻量级但有效的DRM方案，既能保护内容不被未授权观看，又能降低系统负载并保持流媒体质量。

Method: 提出ABE-VVS框架，采用基于属性的选择性坐标加密方法。在CloudLab测试平台上部署点云视频流媒体系统，评估三种HTTP-based ABE粒度：ABE-XYZ（加密所有XYZ坐标）、ABE-XY和ABE-X。与传统HTTPS/TLS安全流媒体以及无安全措施的HTTP基线进行比较。

Result: 仅加密X坐标即可实现有效混淆，同时将加密和解密时间分别降低50%和80%。ABE方案将服务器端CPU负载降低高达80%，缓存CPU负载降低高达63%，与纯HTTP相当。ABE-XYZ和ABE-XY客户端重新缓冲低于HTTPS，ABE-X实现零重新缓冲。虽然客户端CPU使用增加，但开销不影响流媒体质量。

Conclusion: ABE-VVS是首个提供端到端评估的DRM安全点云流媒体系统。选择性坐标加密在保持有效DRM的同时显著降低计算开销，简化密钥撤销，消除每客户端加密需求，并减少服务器和缓存负载，为点云体视频流媒体提供了实用且高效的DRM解决方案。

Abstract: This work introduces ABE-VVS, a framework that performs attribute based selective coordinate encryption for point cloud based volumetric video streaming, enabling lightweight yet effective digital rights management (DRM). Rather than encrypting entire point cloud frames, our approach encrypts only selected subsets of coordinates ($X, Y, Z$, or combinations), lowering computational overhead and latency while still producing strong visual distortion that prevents meaningful unauthorized viewing. Our experiments show that encrypting only the $X$ coordinates achieves effective obfuscation while reducing encryption and decryption times by up to 50% and 80%, respectively, compared to full-frame encryption.
  To our knowledge, this is the first work to provide a novel end-to-end evaluation of a DRM-enabled secure point cloud streaming system. We deployed a point cloud video streaming setup on the CloudLab testbed and evaluated three HTTP-based Attribute-Based Encryption (ABE) granularities - ABE-XYZ (encrypting all $X,Y,Z$ coordinates), ABE-XY, and ABE-X against conventional HTTPS/TLS secure streaming as well as an HTTP-only baseline without any security. Our streaming evaluation demonstrates that ABE-based schemes reduce server-side CPU load by up to 80% and cache CPU load by up to 63%, comparable to HTTP-only, while maintaining similar cache hit rates. Moreover, ABE-XYZ and ABE-XY exhibit lower client-side rebuffering than HTTPS, and ABE-X achieves zero rebuffering comparable to HTTP-only. Although ABE-VVS increases client-side CPU usage, the overhead is not large enough to affect streaming quality and is offset by its broader benefits, including simplified key revocation, elimination of per-client encryption, and reduced server and cache load.

</details>


### [34] [StegoStylo: Squelching Stylometric Scrutiny through Steganographic Stitching](https://arxiv.org/abs/2601.09056)
*Robert Dilworth*

Main category: cs.CR

TL;DR: 论文探讨了文本风格分析（文体学）的双重用途：一方面可用于作者识别、版权保护等有益目的，另一方面也可用于去匿名化、追踪等隐私威胁。研究提出通过对抗性文体学和隐写术来对抗文体分析，保护作者隐私。


<details>
  <summary>Details</summary>
Motivation: 文体学作为作者识别工具既有建设性用途（如版权保护、有害内容检测、医疗诊断辅助），也可能被恶意用于去匿名化、追踪、审查等隐私侵犯行为。因此需要开发防御性工具来保护作者的隐私和匿名性。

Method: 1. 增强对抗性攻击方法TraceTarnish，证明其能有效混淆文体分析系统；2. 研究隐写术嵌入技术，通过零宽度Unicode字符修改文本，量化不同修改比例下的作者混淆效果。

Result: 1. TraceTarnish能显著降低文体分析系统的作者归属和验证准确率；2. 当隐写术覆盖率达到33%或更高时，能有效实现作者混淆，隐藏作者的文体指纹。

Conclusion: 文体学可能被用于侵犯隐私，因此需要像TraceTarnish这样的防御工具。通过对抗性文体学和隐写术技术，可以有效保护作者的匿名性，特别是当隐写术覆盖率达到33%以上时能确保作者混淆。

Abstract: Stylometry--the identification of an author through analysis of a text's style (i.e., authorship attribution)--serves many constructive purposes: it supports copyright and plagiarism investigations, aids detection of harmful content, offers exploratory cues for certain medical conditions (e.g., early signs of dementia or depression), provides historical context for literary works, and helps uncover misinformation and disinformation. In contrast, when stylometry is employed as a tool for authorship verification--confirming whether a text truly originates from a claimed author--it can also be weaponized for malicious purposes. Techniques such as de-anonymization, re-identification, tracking, profiling, and downstream effects like censorship illustrate the privacy threats that stylometric analysis can enable. Building on these concerns, this paper further explores how adversarial stylometry combined with steganography can counteract stylometric analysis. We first present enhancements to our adversarial attack, $\textit{TraceTarnish}$, providing stronger evidence of its capacity to confound stylometric systems and reduce their attribution and verification accuracy. Next, we examine how steganographic embedding can be fine-tuned to mask an author's stylistic fingerprint, quantifying the level of authorship obfuscation achievable as a function of the proportion of words altered with zero-width Unicode characters. Based on our findings, steganographic coverage of 33% or higher seemingly ensures authorship obfuscation. Finally, we reflect on the ways stylometry can be used to undermine privacy and argue for the necessity of defensive tools like $\textit{TraceTarnish}$.

</details>


### [35] [Rigorous and Generalized Proof of Security of Bitcoin Protocol with Bounded Network Delay](https://arxiv.org/abs/2601.09082)
*Christopher Blake,Chen Feng,Xuechao Wang,Qianyu Yu*

Main category: cs.CR

TL;DR: 本文对比特币协议的安全性证明进行了严谨化和简化，考虑了对手可以延迟区块传输的计算模型，并证明了只要完全延迟后的诚实挖矿率超过对手挖矿率，比特币协议就会以概率1产生无限多个诚实区块。


<details>
  <summary>Details</summary>
Motivation: 比特币协议的安全性证明需要更加严谨和简化，特别是要纠正先前论文中基于随机游走理论的错误方法，并在更一般的模型下提供正确的证明。

Method: 1. 建立计算模型，允许对手延迟区块传输时间Δ；2. 将协议推广到允许不同分数的区块；3. 使用穿孔区块到达过程的方法来纠正先前随机游走理论的错误；4. 在更一般的模型下进行证明。

Result: 证明了只要完全延迟后的诚实挖矿率超过对手挖矿率，比特币协议就会以概率1产生无限多个诚实区块，从而确保了协议的安全性。

Conclusion: 本文提供了比特币协议安全性的严谨证明，纠正了先前研究中的错误，并在更一般的模型下建立了协议安全性的充分条件，为比特币协议的理论基础提供了重要支撑。

Abstract: A proof of the security of the Bitcoin protocol is made rigorous, and simplified in certain parts. A computational model in which an adversary can delay transmission of blocks by time $Δ$ is considered. The protocol is generalized to allow blocks of different scores and a proof within this more general model is presented. An approach used in a previous paper that used random walk theory is shown through a counterexample to be incorrect; an approach involving a punctured block arrival process is shown to remedy this error. Thus, it is proven that with probability one, the Bitcoin protocol will have infinitely many honest blocks so long as the fully-delayed honest mining rate exceeds the adversary mining rate.

</details>


### [36] [KryptoPilot: An Open-World Knowledge-Augmented LLM Agent for Automated Cryptographic Exploitation](https://arxiv.org/abs/2601.09129)
*Xiaonan Liu,Zhihao Li,Xiao Lan,Hao Ren,Haizhou Wang,Xingshu Chen*

Main category: cs.CR

TL;DR: KryptoPilot是一个面向加密CTF挑战的开放世界知识增强LLM智能体，通过动态知识获取和治理推理机制，显著提升了在复杂加密攻击任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的智能体在处理高难度加密CTF挑战时效果不佳，主要原因是知识粒度不足，而非模型推理能力问题。粗粒度或抽象的外部知识无法支持正确的攻击建模和实施。

Method: 提出KryptoPilot系统，包含：1) 通过深度研究管道实现动态开放世界知识获取；2) 用于结构化知识重用的持久工作空间；3) 通过行为约束和成本感知模型路由稳定推理的治理子系统。

Result: 在InterCode-CTF上实现完全解决率；在NYU-CTF基准测试中解决56-60%的加密挑战；在6个真实CTF比赛中成功解决26/33个加密挑战，包括多个最早解决和唯一解决的实例。

Conclusion: 开放世界、细粒度知识增强和治理推理对于将基于LLM的智能体扩展到真实世界加密攻击任务是必要的。KryptoPilot展示了这种方法的有效性。

Abstract: Capture-the-Flag (CTF) competitions play a central role in modern cybersecurity as a platform for training practitioners and evaluating offensive and defensive techniques derived from real-world vulnerabilities. Despite recent advances in large language models (LLMs), existing LLM-based agents remain ineffective on high-difficulty cryptographic CTF challenges, which require precise cryptanalytic knowledge, stable long-horizon reasoning, and disciplined interaction with specialized toolchains. Through a systematic exploratory study, we show that insufficient knowledge granularity, rather than model reasoning capacity, is a primary factor limiting successful cryptographic exploitation: coarse or abstracted external knowledge often fails to support correct attack modeling and implementation. Motivated by this observation, we propose KryptoPilot, an open-world knowledge-augmented LLM agent for automated cryptographic exploitation. KryptoPilot integrates dynamic open-world knowledge acquisition via a Deep Research pipeline, a persistent workspace for structured knowledge reuse, and a governance subsystem that stabilizes reasoning through behavioral constraints and cost-aware model routing. This design enables precise knowledge alignment while maintaining efficient reasoning across heterogeneous subtasks. We evaluate KryptoPilot on two established CTF benchmarks and in six real-world CTF competitions. KryptoPilot achieves a complete solve rate on InterCode-CTF, solves between 56 and 60 percent of cryptographic challenges on the NYU-CTF benchmark, and successfully solves 26 out of 33 cryptographic challenges in live competitions, including multiple earliest-solved and uniquely-solved instances. These results demonstrate the necessity of open-world, fine-grained knowledge augmentation and governed reasoning for scaling LLM-based agents to real-world cryptographic exploitation.

</details>


### [37] [Deep Learning-based Binary Analysis for Vulnerability Detection in x86-64 Machine Code](https://arxiv.org/abs/2601.09157)
*Mitchell Petingola*

Main category: cs.CR

TL;DR: 本文探索直接从原始x86-64机器代码进行漏洞检测的可行性，发现基于图结构的模型优于序列模型，且机器代码包含足够信息用于有效漏洞发现。


<details>
  <summary>Details</summary>
Motivation: 当前深度学习漏洞检测研究多依赖反汇编二进制文件，但反汇编需要更复杂模型来捕获标记级上下文，而机器代码可能支持更高效、轻量级的模型，并保留反汇编过程中可能丢失的所有信息。

Method: 通过探索性研究，采用两种特定深度学习模型架构（图模型和序列模型），在三种漏洞类型上系统评估其性能，直接从原始x86-64机器代码提取特征。

Result: 基于图的模型始终优于序列模型，强调了控制流关系的重要性；机器代码包含足够信息用于有效的漏洞发现。

Conclusion: 直接从机器代码进行漏洞检测是可行的，图模型因其能捕获控制流关系而表现更优，机器代码保留了足够的信息用于漏洞发现，为更高效的漏洞检测方法提供了可能性。

Abstract: While much of the current research in deep learning-based vulnerability detection relies on disassembled binaries, this paper explores the feasibility of extracting features directly from raw x86-64 machine code. Although assembly language is more interpretable for humans, it requires more complex models to capture token-level context. In contrast, machine code may enable more efficient, lightweight models and preserve all information that might be lost in disassembly. This paper approaches the task of vulnerability detection through an exploratory study on two specific deep learning model architectures and aims to systematically evaluate their performance across three vulnerability types. The results demonstrate that graph-based models consistently outperform sequential models, emphasizing the importance of control flow relationships, and that machine code contains sufficient information for effective vulnerability discovery.

</details>


### [38] [The Real Menace of Cloning Attacks on SGX Applications](https://arxiv.org/abs/2601.09273)
*Annika Wilde,Samira Briongos,Claudio Soriente,Ghassan Karame*

Main category: cs.CR

TL;DR: 研究发现约20%的SGX提案易受克隆攻击，包括依赖单调计数器的应用


<details>
  <summary>Details</summary>
Motivation: 可信执行环境（TEEs）如Intel SGX存在回滚和克隆攻击漏洞。虽然回滚攻击已被广泛研究，但克隆攻击研究较少，需要填补这一空白

Method: 对72个基于SGX的提案进行广泛研究和深入分析，评估其对克隆攻击的脆弱性

Result: 约20%的分析提案对克隆攻击不安全，包括那些依赖单调计数器且对回滚攻击安全的应用程序

Conclusion: 克隆攻击是SGX安全中的一个重要威胁，需要更多关注和研究，即使使用单调计数器也不能完全防止克隆攻击

Abstract: Trusted Execution Environments (TEEs) are gaining popularity as an effective means to provide confidentiality in the cloud. TEEs, such as Intel SGX, suffer from so-called rollback and cloning attacks (often referred to as forking attacks). Rollback attacks are enabled by the lack of freshness guarantees for sealed data; cloning attacks stem from the inability to determine if other instances of an enclave are running on the same platform. While rollback attacks have been extensively studied by the community, cloning attacks have been, unfortunately, less investigated. To address this gap, we extensively study and thoroughly analyze the susceptibility of 72 SGX-based proposals to cloning attacks. Our results show that roughly 20% of the analyzed proposals are insecure against cloning attacks-including those applications that rely on monotonic counters and are, therefore, secure against rollback attacks.

</details>


### [39] [Blue Teaming Function-Calling Agents](https://arxiv.org/abs/2601.09292)
*Greta Dolcetti,Giulio Zizzo,Sergio Maffeis*

Main category: cs.CR

TL;DR: 评估四个声称具备函数调用能力的开源LLM对三种攻击的鲁棒性，测试八种防御措施的有效性，发现模型默认不安全且防御措施尚不适用于实际场景


<details>
  <summary>Details</summary>
Motivation: 评估声称具备函数调用能力的开源大语言模型在实际应用中的安全性，测试它们对恶意攻击的鲁棒性，以及现有防御措施的有效性

Method: 对四个开源LLM进行实验评估，使用三种不同的攻击方法测试其函数调用能力的安全性，并评估八种不同防御措施的有效性

Result: 结果显示这些模型默认情况下并不安全，现有的防御措施在实际应用场景中尚不可用

Conclusion: 当前声称具备函数调用能力的开源LLM存在安全隐患，需要进一步改进安全机制，现有防御措施需要更多开发才能适用于实际部署

Abstract: We present an experimental evaluation that assesses the robustness of four open source LLMs claiming function-calling capabilities against three different attacks, and we measure the effectiveness of eight different defences. Our results show how these models are not safe by default, and how the defences are not yet employable in real-world scenarios.

</details>


### [40] [SpatialJB: How Text Distribution Art Becomes the "Jailbreak Key" for LLM Guardrails](https://arxiv.org/abs/2601.09321)
*Zhiyi Mou,Jingyuan Yang,Zeheng Qian,Wangze Ni,Tianfang Xiao,Ning Liu,Chen Zhang,Zhan Qin,Kui Ren*

Main category: cs.CR

TL;DR: SpatialJB是一种针对大语言模型的新型越狱攻击方法，通过利用Transformer架构对空间结构扰动的脆弱性，重新分配token在不同行、列或对角线上的位置，成功绕过输出防护机制，实现接近100%的攻击成功率。


<details>
  <summary>Details</summary>
Motivation: 当前商业LLM提供商部署的输出防护机制并非不可穿透，LLM由于依赖自回归的token-by-token推理，其语义表示对空间结构扰动缺乏鲁棒性。研究者希望利用Transformer的空间弱点来破坏模型的输出生成过程，让有害内容绕过防护机制而不被检测。

Method: 提出SpatialJB攻击方法，利用Transformer架构对空间结构扰动的脆弱性，通过重新分配token在不同行、列或对角线上的位置来破坏模型的输出生成过程，从而绕过输出防护机制。

Result: 在主流LLM上的综合实验获得了接近100%的攻击成功率（ASR）。即使在添加了OpenAI Moderation API等高级输出防护机制后，SpatialJB仍能保持超过75%的成功率，显著优于当前的越狱技术。

Conclusion: SpatialJB暴露了当前防护机制的关键弱点，强调了空间语义的重要性，为推进LLM安全研究提供了新见解。为防止潜在滥用，研究者还提出了针对SpatialJB的基线防御策略并评估了其有效性。

Abstract: While Large Language Models (LLMs) have powerful capabilities, they remain vulnerable to jailbreak attacks, which is a critical barrier to their safe web real-time application. Current commercial LLM providers deploy output guardrails to filter harmful outputs, yet these defenses are not impenetrable. Due to LLMs' reliance on autoregressive, token-by-token inference, their semantic representations lack robustness to spatially structured perturbations, such as redistributing tokens across different rows, columns, or diagonals. Exploiting the Transformer's spatial weakness, we propose SpatialJB to disrupt the model's output generation process, allowing harmful content to bypass guardrails without detection. Comprehensive experiments conducted on leading LLMs get nearly 100% ASR, demonstrating the high effectiveness of SpatialJB. Even after adding advanced output guardrails, like the OpenAI Moderation API, SpatialJB consistently maintains a success rate exceeding 75%, outperforming current jailbreak techniques by a significant margin. The proposal of SpatialJB exposes a key weakness in current guardrails and emphasizes the importance of spatial semantics, offering new insights to advance LLM safety research. To prevent potential misuse, we also present baseline defense strategies against SpatialJB and evaluate their effectiveness in mitigating such attacks. The code for the attack, baseline defenses, and a demo are available at https://anonymous.4open.science/r/SpatialJailbreak-8E63.

</details>


### [41] [A Systematic Security Analysis for Path-based Traceability Systems in RFID-Enabled Supply Chains](https://arxiv.org/abs/2601.09407)
*Fokke Heikamp,Lei Pan,Robin Doss,Rolando Trujillo-Rasua,Sushmita Ruj*

Main category: cs.CR

TL;DR: 本文对供应链追溯系统的安全性进行了系统性分析，发现现有解决方案的安全需求往往不完整或缺乏结构化，导致关键漏洞未被解决。作者建立了一个统一的安全框架来分析和比较17种追溯解决方案，识别出多种弱点和漏洞。


<details>
  <summary>Details</summary>
Motivation: 随着RFID和物联网技术的快速发展，追溯系统在供应链中日益普及，用于产品召回和防止假冒、篡改、盗窃等问题。然而，这些系统本身也成为攻击者的目标，攻击者可能通过修改产品追溯信息来欺骗系统。现有追溯解决方案的安全需求往往不完整或缺乏结构化，导致关键安全漏洞未被解决。

Method: 作者首先观察到现有追溯解决方案的安全需求往往不完整或缺乏结构化。然后，他们综合了当前最先进追溯解决方案的特性，构建了一个统一的安全框架。使用这个框架，作者对17种追溯解决方案进行了客观比较和安全分析，识别出其中的弱点和漏洞。

Result: 研究识别了多种安全弱点和漏洞，这是首次对追溯解决方案进行大规模安全评估。通过统一的安全框架，作者能够客观比较不同解决方案的安全性，并发现现有解决方案中普遍存在的安全缺陷。

Conclusion: 本文提供了对追溯系统安全性的系统性分析，建立了评估框架，并揭示了现有解决方案中的安全缺陷。这项工作为改进追溯系统的安全性提供了基础，并首次在大规模上评估了这些系统的安全性。

Abstract: Traceability systems have become prevalent in supply chains because of the rapid development of RFID and IoT technologies. These systems facilitate product recall and mitigate problems such as counterfeiting, tampering, and theft by tracking the manufacturing and distribution life-cycle of a product. Therefore, traceability systems are a defense mechanism against supply chain attacks and, consequently, have become a target for attackers to circumvent. For example, a counterfeiter may change the trace of a fake product for the trace of an authentic product, fooling the system into accepting a counterfeit product as legit and thereby giving a false sense of security.
  This systematic analysis starts with the observation that security requirements in existing traceability solutions are often unstructured or incomplete, leaving critical vulnerabilities unaddressed. We synthesized the properties of current state-of-the-art traceability solutions within a single security framework that allows us to analyze and compare their security claims. Using this framework, we objectively compared the security of $17$ traceability solutions and identified several weaknesses and vulnerabilities. This article reports on these flaws, the methodology we used to identify them, and the first security evaluation of traceability solutions on a large scale.

</details>
