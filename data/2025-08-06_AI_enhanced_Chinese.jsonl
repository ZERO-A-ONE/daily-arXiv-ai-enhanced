{"id": "2508.02805", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2508.02805", "abs": "https://arxiv.org/abs/2508.02805", "authors": ["Jean Michel Tine", "Mohammed Aldeen", "Abyad Enan", "M Sabbir Salek", "Long Cheng", "Mashrur Chowdhury"], "title": "Real-World Evaluation of Protocol-Compliant Denial-of-Service Attacks on C-V2X-based Forward Collision Warning Systems", "comment": "This paper was submitted to the Transportation Research Board (TRB)\n  2026 and is under review", "summary": "Cellular Vehicle-to-Everything (C-V2X) technology enables low-latency,\nreliable communications essential for safety applications such as a Forward\nCollision Warning (FCW) system. C-V2X deployments operate under strict protocol\ncompliance with the 3rd Generation Partnership Project (3GPP) and the Society\nof Automotive Engineers Standard (SAE) J2735 specifications to ensure\ninteroperability. This paper presents a real-world testbed evaluation of\nprotocol-compliant Denial-of-Service (DoS) attacks using User Datagram Protocol\n(UDP) flooding and oversized Basic Safety Message (BSM) attacks that 7 exploit\ntransport- and application-layer vulnerabilities in C-V2X. The attacks\npresented in this study transmit valid messages over standard PC5 sidelinks,\nfully adhering to 3GPP and SAE J2735 specifications, but at abnormally high\nrates and with oversized payloads that overload the receiver resources without\nbreaching any protocol rules such as IEEE 1609. Using a real-world connected\nvehicle 11 testbed with commercially available On-Board Units (OBUs), we\ndemonstrate that high-rate UDP flooding and oversized payload of BSM flooding\ncan severely degrade FCW performance. Results show that UDP flooding alone\nreduces packet delivery ratio by up to 87% and increases latency to over 400ms,\nwhile oversized BSM floods overload receiver processing resources, delaying or\ncompletely suppressing FCW alerts. When UDP and BSM attacks are executed\nsimultaneously, they cause near-total communication failure, preventing FCW\nwarnings entirely. These findings reveal that protocol-compliant communications\ndo not necessarily guarantee safe or reliable operation of C-V2X-based safety\napplications.", "AI": {"tldr": "\u672c\u6587\u901a\u8fc7\u771f\u5b9e\u6d4b\u8bd5\u5e8a\u8bc4\u4f30\u4e86C-V2X\u6280\u672f\u4e2d\u7b26\u5408\u534f\u8bae\u7684DoS\u653b\u51fb\uff08UDP\u6d2a\u6cdb\u548c\u8d85\u5927BSM\u653b\u51fb\uff09\uff0c\u53d1\u73b0\u8fd9\u4e9b\u653b\u51fb\u4f1a\u4e25\u91cd\u964d\u4f4eFCW\u6027\u80fd\uff0c\u751a\u81f3\u5bfc\u81f4\u901a\u4fe1\u5b8c\u5168\u5931\u8d25\u3002", "motivation": "\u7814\u7a76C-V2X\u6280\u672f\u5728\u534f\u8bae\u5408\u89c4\u6027\u4e0b\u7684\u5b89\u5168\u6f0f\u6d1e\uff0c\u63ed\u793a\u5373\u4f7f\u7b26\u5408\u6807\u51c6\u4e5f\u53ef\u80fd\u5b58\u5728\u5b89\u5168\u9690\u60a3\u3002", "method": "\u4f7f\u7528\u771f\u5b9e\u6d4b\u8bd5\u5e8a\u548c\u5546\u7528OBU\uff0c\u6a21\u62dfUDP\u6d2a\u6cdb\u548c\u8d85\u5927BSM\u653b\u51fb\uff0c\u8bc4\u4f30\u5176\u5bf9FCW\u6027\u80fd\u7684\u5f71\u54cd\u3002", "result": "UDP\u6d2a\u6cdb\u964d\u4f4e\u6570\u636e\u5305\u4ea4\u4ed8\u738787%\uff0c\u5ef6\u8fdf\u8d85400ms\uff1b\u8d85\u5927BSM\u653b\u51fb\u5bfc\u81f4FCW\u8b66\u62a5\u5ef6\u8fdf\u6216\u5b8c\u5168\u5931\u6548\uff1b\u540c\u65f6\u653b\u51fb\u65f6\u901a\u4fe1\u51e0\u4e4e\u5b8c\u5168\u5931\u8d25\u3002", "conclusion": "\u534f\u8bae\u5408\u89c4\u6027\u4e0d\u80fd\u5b8c\u5168\u4fdd\u8bc1C-V2X\u5b89\u5168\u5e94\u7528\u7684\u53ef\u9760\u6027\uff0c\u9700\u8fdb\u4e00\u6b65\u6539\u8fdb\u5b89\u5168\u673a\u5236\u3002"}}
{"id": "2508.02816", "categories": ["cs.CR", "cs.ET"], "pdf": "https://arxiv.org/pdf/2508.02816", "abs": "https://arxiv.org/abs/2508.02816", "authors": ["Dylan Stow", "Russell Barnes", "Eren Kurshan", "Yuan Xie"], "title": "Thermal-Aware 3D Design for Side-Channel Information Leakage", "comment": null, "summary": "Side-channel attacks are important security challenges as they reveal\nsensitive information about on-chip activities. Among such attacks, the thermal\nside-channel has been shown to disclose the activities of key functional blocks\nand even encryption keys. This paper proposes a novel approach to proactively\nconceal critical activities in the functional layers while minimizing the power\ndissipation by (i) leveraging inherent characteristics of 3D integration to\nprotect from side-channel attacks and (ii) dynamically generating custom\nactivity patterns to match the activity to be concealed in the functional\nlayers. Experimental analysis shows that 3D technology combined with the\nproposed run-time algorithm effectively reduces the Side channel vulnerability\nFactor (SVF) below 0.05 and the Spatial Thermal Side-channel Factor (STSF)\nbelow 0.59.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u75283D\u96c6\u6210\u6280\u672f\u548c\u52a8\u6001\u751f\u6210\u6d3b\u52a8\u6a21\u5f0f\u7684\u65b9\u6cd5\uff0c\u6709\u6548\u964d\u4f4e\u70ed\u4fa7\u4fe1\u9053\u653b\u51fb\u7684\u98ce\u9669\u3002", "motivation": "\u70ed\u4fa7\u4fe1\u9053\u653b\u51fb\u80fd\u591f\u6cc4\u9732\u5173\u952e\u529f\u80fd\u5757\u7684\u6d3b\u52a8\u751a\u81f3\u52a0\u5bc6\u5bc6\u94a5\uff0c\u4e9f\u9700\u4e00\u79cd\u4e3b\u52a8\u9690\u85cf\u5173\u952e\u6d3b\u52a8\u7684\u65b9\u6cd5\u3002", "method": "\u7ed3\u54083D\u96c6\u6210\u6280\u672f\u7684\u56fa\u6709\u7279\u6027\uff0c\u52a8\u6001\u751f\u6210\u81ea\u5b9a\u4e49\u6d3b\u52a8\u6a21\u5f0f\u4ee5\u5339\u914d\u529f\u80fd\u5c42\u4e2d\u7684\u6d3b\u52a8\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5c06\u4fa7\u4fe1\u9053\u6f0f\u6d1e\u56e0\u5b50\uff08SVF\uff09\u964d\u81f30.05\u4ee5\u4e0b\uff0c\u7a7a\u95f4\u70ed\u4fa7\u4fe1\u9053\u56e0\u5b50\uff08STSF\uff09\u964d\u81f30.59\u4ee5\u4e0b\u3002", "conclusion": "3D\u6280\u672f\u4e0e\u8fd0\u884c\u65f6\u7b97\u6cd5\u7684\u7ed3\u5408\u80fd\u6709\u6548\u51cf\u5c11\u70ed\u4fa7\u4fe1\u9053\u653b\u51fb\u7684\u5a01\u80c1\u3002"}}
{"id": "2508.02836", "categories": ["cs.CR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.02836", "abs": "https://arxiv.org/abs/2508.02836", "authors": ["Mengyu Zhang", "Zhuotao Liu", "Jingwen Huang", "Xuanqi Liu"], "title": "Agentic Privacy-Preserving Machine Learning", "comment": null, "summary": "Privacy-preserving machine learning (PPML) is critical to ensure data privacy\nin AI. Over the past few years, the community has proposed a wide range of\nprovably secure PPML schemes that rely on various cryptography primitives.\nHowever, when it comes to large language models (LLMs) with billions of\nparameters, the efficiency of PPML is everything but acceptable. For instance,\nthe state-of-the-art solution for confidential LLM inference represents at\nleast 10,000-fold slower performance compared to plaintext inference. The\nperformance gap is even larger when the context length increases. In this\nposition paper, we propose a novel framework named Agentic-PPML to make PPML in\nLLMs practical. Our key insight is to employ a general-purpose LLM for intent\nunderstanding and delegate cryptographically secure inference to specialized\nmodels trained on vertical domains. By modularly separating language intent\nparsing - which typically involves little or no sensitive information - from\nprivacy-critical computation, Agentic-PPML completely eliminates the need for\nthe LLMs to process the encrypted prompts, enabling practical deployment of\nprivacy-preserving LLM-centric services.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aAgentic-PPML\u7684\u65b0\u6846\u67b6\uff0c\u65e8\u5728\u89e3\u51b3\u9690\u79c1\u4fdd\u62a4\u673a\u5668\u5b66\u4e60\uff08PPML\uff09\u5728\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u4e2d\u7684\u6548\u7387\u95ee\u9898\u3002", "motivation": "\u5f53\u524dPPML\u65b9\u6848\u5728\u5904\u7406LLMs\u65f6\u6548\u7387\u6781\u4f4e\uff0c\u6027\u80fd\u5dee\u8ddd\u663e\u8457\uff0c\u5c24\u5176\u662f\u5728\u4e0a\u4e0b\u6587\u957f\u5ea6\u589e\u52a0\u65f6\u3002", "method": "\u901a\u8fc7\u5c06\u8bed\u8a00\u610f\u56fe\u89e3\u6790\u4e0e\u9690\u79c1\u5173\u952e\u8ba1\u7b97\u6a21\u5757\u5316\u5206\u79bb\uff0c\u5229\u7528\u901a\u7528LLM\u5904\u7406\u610f\u56fe\u7406\u89e3\uff0c\u5e76\u5c06\u52a0\u5bc6\u63a8\u7406\u4efb\u52a1\u59d4\u6258\u7ed9\u5782\u76f4\u9886\u57df\u7684\u4e13\u7528\u6a21\u578b\u3002", "result": "Agentic-PPML\u907f\u514d\u4e86LLMs\u5904\u7406\u52a0\u5bc6\u63d0\u793a\u7684\u9700\u6c42\uff0c\u4ece\u800c\u5b9e\u73b0\u4e86\u9690\u79c1\u4fdd\u62a4LLM\u670d\u52a1\u7684\u5b9e\u9645\u90e8\u7f72\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u9690\u79c1\u4fdd\u62a4LLM\u7684\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u884c\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.02942", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2508.02942", "abs": "https://arxiv.org/abs/2508.02942", "authors": ["Anas Mabrouk", "Mohamed Hatem", "Mohammad Mamun", "Sherif Saad"], "title": "LMDG: Advancing Lateral Movement Detection Through High-Fidelity Dataset Generation", "comment": null, "summary": "Lateral Movement (LM) attacks continue to pose a significant threat to\nenterprise security, enabling adversaries to stealthily compromise critical\nassets. However, the development and evaluation of LM detection systems are\nimpeded by the absence of realistic, well-labeled datasets. To address this\ngap, we propose LMDG, a reproducible and extensible framework for generating\nhigh-fidelity LM datasets. LMDG automates benign activity generation,\nmulti-stage attack execution, and comprehensive labeling of system and network\nlogs, dramatically reducing manual effort and enabling scalable dataset\ncreation. A central contribution of LMDG is Process Tree Labeling, a novel\nagent-based technique that traces all malicious activity back to its origin\nwith high precision. Unlike prior methods such as Injection Timing or\nBehavioral Profiling, Process Tree Labeling enables accurate, step-wise\nlabeling of malicious log entries, correlating each with a specific attack step\nand MITRE ATT\\&CK TTPs. To our knowledge, this is the first approach to support\nfine-grained labeling of multi-step attacks, providing critical context for\ndetection models such as attack path reconstruction. We used LMDG to generate a\n25-day dataset within a 25-VM enterprise environment containing 22 user\naccounts. The dataset includes 944 GB of host and network logs and embeds 35\nmulti-stage LM attacks, with malicious events comprising less than 1% of total\nactivity, reflecting a realistic benign-to-malicious ratio for evaluating\ndetection systems. LMDG-generated datasets improve upon existing ones by\noffering diverse LM attacks, up-to-date attack patterns, longer attack\ntimeframes, comprehensive data sources, realistic network architectures, and\nmore accurate labeling.", "AI": {"tldr": "LMDG\u662f\u4e00\u4e2a\u53ef\u6269\u5c55\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u751f\u6210\u9ad8\u4fdd\u771f\u7684\u6a2a\u5411\u79fb\u52a8\u653b\u51fb\u6570\u636e\u96c6\uff0c\u901a\u8fc7\u81ea\u52a8\u5316\u751f\u6210\u826f\u6027\u6d3b\u52a8\u548c\u591a\u9636\u6bb5\u653b\u51fb\uff0c\u51cf\u5c11\u4eba\u5de5\u6807\u6ce8\u5de5\u4f5c\u3002", "motivation": "\u89e3\u51b3\u6a2a\u5411\u79fb\u52a8\u653b\u51fb\u68c0\u6d4b\u7cfb\u7edf\u7f3a\u4e4f\u73b0\u5b9e\u4e14\u6807\u6ce8\u826f\u597d\u7684\u6570\u636e\u96c6\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51faLMDG\u6846\u67b6\uff0c\u5305\u62ec\u81ea\u52a8\u5316\u826f\u6027\u6d3b\u52a8\u751f\u6210\u3001\u591a\u9636\u6bb5\u653b\u51fb\u6267\u884c\u548c\u7cfb\u7edf\u65e5\u5fd7\u6807\u6ce8\uff0c\u7279\u522b\u662f\u57fa\u4e8e\u4ee3\u7406\u7684Process Tree Labeling\u6280\u672f\u3002", "result": "\u751f\u6210\u4e86\u4e00\u4e2a25\u5929\u7684\u6570\u636e\u96c6\uff0c\u5305\u542b35\u4e2a\u591a\u9636\u6bb5\u653b\u51fb\uff0c\u6076\u610f\u4e8b\u4ef6\u5360\u6bd4\u5c0f\u4e8e1%\uff0c\u6570\u636e\u91cf\u8fbe944GB\u3002", "conclusion": "LMDG\u63d0\u4f9b\u4e86\u66f4\u771f\u5b9e\u3001\u591a\u6837\u5316\u7684\u6570\u636e\u96c6\uff0c\u652f\u6301\u7ec6\u7c92\u5ea6\u6807\u6ce8\u548c\u591a\u9636\u6bb5\u653b\u51fb\u68c0\u6d4b\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002"}}
{"id": "2508.02721", "categories": ["cs.SE", "cs.AI", "cs.PL"], "pdf": "https://arxiv.org/pdf/2508.02721", "abs": "https://arxiv.org/abs/2508.02721", "authors": ["Libin Qiu", "Yuhang Ye", "Zhirong Gao", "Xide Zou", "Junfu Chen", "Ziming Gui", "Weizhi Huang", "Xiaobo Xue", "Wenkai Qiu", "Kun Zhao"], "title": "Blueprint First, Model Second: A Framework for Deterministic LLM Workflow", "comment": "8 pages, 6 figures, 3 tables", "summary": "While powerful, the inherent non-determinism of large language model (LLM)\nagents limits their application in structured operational environments where\nprocedural fidelity and predictable execution are strict requirements. This\nlimitation stems from current architectures that conflate probabilistic,\nhigh-level planning with low-level action execution within a single generative\nprocess. To address this, we introduce the Source Code Agent framework, a new\nparadigm built on the \"Blueprint First, Model Second\" philosophy. Our framework\ndecouples the workflow logic from the generative model. An expert-defined\noperational procedure is first codified into a source code-based Execution\nBlueprint, which is then executed by a deterministic engine. The LLM is\nstrategically invoked as a specialized tool to handle bounded, complex\nsub-tasks within the workflow, but never to decide the workflow's path. We\nconduct a comprehensive evaluation on the challenging tau-bench benchmark,\ndesigned for complex user-tool-rule scenarios. Our results demonstrate that the\nSource Code Agent establishes a new state-of-the-art, outperforming the\nstrongest baseline by 10.1 percentage points on the average Pass^1 score while\ndramatically improving execution efficiency. Our work enables the verifiable\nand reliable deployment of autonomous agents in applications governed by strict\nprocedural logic.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aSource Code Agent\u7684\u65b0\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u5de5\u4f5c\u6d41\u903b\u8f91\u4e0e\u751f\u6210\u6a21\u578b\u89e3\u8026\uff0c\u89e3\u51b3\u4e86LLM\u4ee3\u7406\u5728\u7ed3\u6784\u5316\u64cd\u4f5c\u73af\u5883\u4e2d\u7684\u975e\u786e\u5b9a\u6027\u95ee\u9898\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u4ee3\u7406\u7684\u975e\u786e\u5b9a\u6027\u9650\u5236\u4e86\u5176\u5728\u9700\u8981\u4e25\u683c\u7a0b\u5e8f\u4fdd\u771f\u5ea6\u548c\u53ef\u9884\u6d4b\u6267\u884c\u7684\u7ed3\u6784\u5316\u73af\u5883\u4e2d\u7684\u5e94\u7528\u3002", "method": "\u91c7\u7528\u201c\u84dd\u56fe\u4f18\u5148\uff0c\u6a21\u578b\u5176\u6b21\u201d\u7684\u65b9\u6cd5\uff0c\u5c06\u4e13\u5bb6\u5b9a\u4e49\u7684\u64cd\u4f5c\u7a0b\u5e8f\u7f16\u7801\u4e3a\u57fa\u4e8e\u6e90\u4ee3\u7801\u7684\u6267\u884c\u84dd\u56fe\uff0c\u5e76\u7531\u786e\u5b9a\u6027\u5f15\u64ce\u6267\u884c\uff0cLLM\u4ec5\u7528\u4e8e\u5904\u7406\u6709\u9650\u590d\u6742\u5b50\u4efb\u52a1\u3002", "result": "\u5728tau-bench\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cSource Code Agent\u8868\u73b0\u4f18\u5f02\uff0c\u5e73\u5747Pass^1\u5f97\u5206\u6bd4\u6700\u5f3a\u57fa\u7ebf\u9ad8\u51fa10.1\u4e2a\u767e\u5206\u70b9\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u6267\u884c\u6548\u7387\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u4e25\u683c\u7a0b\u5e8f\u903b\u8f91\u4e0b\u7684\u81ea\u4e3b\u4ee3\u7406\u90e8\u7f72\u63d0\u4f9b\u4e86\u53ef\u9a8c\u8bc1\u548c\u53ef\u9760\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.02694", "categories": ["cs.AI", "cs.CL", "cs.MA"], "pdf": "https://arxiv.org/pdf/2508.02694", "abs": "https://arxiv.org/abs/2508.02694", "authors": ["Ningning Wang", "Xavier Hu", "Pai Liu", "He Zhu", "Yue Hou", "Heyuan Huang", "Shengyu Zhang", "Jian Yang", "Jiaheng Liu", "Ge Zhang", "Changwang Zhang", "Jun Wang", "Yuchen Eleanor Jiang", "Wangchunshu Zhou"], "title": "Efficient Agents: Building Effective Agents While Reducing Cost", "comment": "Work in progress. For GitHub repository, see\n  https://github.com/OPPO-PersonalAI/OAgents", "summary": "The remarkable capabilities of Large Language Model (LLM)-driven agents have\nenabled sophisticated systems to tackle complex, multi-step tasks, but their\nescalating costs threaten scalability and accessibility. This work presents the\nfirst systematic study of the efficiency-effectiveness trade-off in modern\nagent systems, addressing the critical need for cost-effective designs without\nsacrificing performance. We investigate three key questions: (1) How much\ncomplexity do agentic tasks inherently require? (2) When do additional modules\nyield diminishing returns? (3) How much efficiency can be gained through the\ndesign of efficient agent frameworks? Through an empirical analysis on the GAIA\nbenchmark, we evaluate the impact of LLM backbone selection, agent framework\ndesigns, and test-time scaling strategies. Using the cost-of-pass metric, we\nquantify the efficiency-performance trade-off across these dimensions. Our\nfindings inform the development of Efficient Agents , a novel agent framework\nthat has an optimal complexity to task requirements. Efficient Agents retains\n96.7% of the performance of OWL, one leading open-source agent framework, while\nreducing operational costs from $0.398 to $0.228, resulting in a 28.4%\nimprovement in cost-of-pass. Our work provides actionable insights for\ndesigning efficient, high-performing agent systems, advancing the accessibility\nand sustainability of AI-driven solutions.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u9a71\u52a8\u4ee3\u7406\u7684\u6548\u7387\u4e0e\u6027\u80fd\u6743\u8861\uff0c\u63d0\u51fa\u4e86\u9ad8\u6548\u4ee3\u7406\u6846\u67b6Efficient Agents\uff0c\u5728\u4fdd\u6301\u9ad8\u6027\u80fd\u7684\u540c\u65f6\u663e\u8457\u964d\u4f4e\u6210\u672c\u3002", "motivation": "LLM\u4ee3\u7406\u7cfb\u7edf\u7684\u6210\u672c\u4e0d\u65ad\u4e0a\u5347\uff0c\u5a01\u80c1\u5176\u53ef\u6269\u5c55\u6027\u548c\u53ef\u8bbf\u95ee\u6027\uff0c\u9700\u8981\u5728\u4e0d\u727a\u7272\u6027\u80fd\u7684\u60c5\u51b5\u4e0b\u8bbe\u8ba1\u66f4\u5177\u6210\u672c\u6548\u76ca\u7684\u65b9\u6848\u3002", "method": "\u901a\u8fc7\u5b9e\u8bc1\u5206\u6790GAIA\u57fa\u51c6\uff0c\u8bc4\u4f30LLM\u4e3b\u5e72\u9009\u62e9\u3001\u4ee3\u7406\u6846\u67b6\u8bbe\u8ba1\u548c\u6d4b\u8bd5\u65f6\u6269\u5c55\u7b56\u7565\u7684\u5f71\u54cd\uff0c\u4f7f\u7528\u6210\u672c-\u901a\u8fc7\u6307\u6807\u91cf\u5316\u6548\u7387\u4e0e\u6027\u80fd\u7684\u6743\u8861\u3002", "result": "\u63d0\u51fa\u7684Efficient Agents\u6846\u67b6\u5728\u4fdd\u630196.7%\u6027\u80fd\u7684\u540c\u65f6\uff0c\u5c06\u8fd0\u8425\u6210\u672c\u4ece0.398\u7f8e\u5143\u964d\u81f30.228\u7f8e\u5143\uff0c\u6210\u672c-\u901a\u8fc7\u6307\u6807\u63d0\u534728.4%\u3002", "conclusion": "\u7814\u7a76\u4e3a\u8bbe\u8ba1\u9ad8\u6548\u3001\u9ad8\u6027\u80fd\u7684\u4ee3\u7406\u7cfb\u7edf\u63d0\u4f9b\u4e86\u5b9e\u7528\u89c1\u89e3\uff0c\u63a8\u52a8\u4e86AI\u9a71\u52a8\u89e3\u51b3\u65b9\u6848\u7684\u53ef\u8bbf\u95ee\u6027\u548c\u53ef\u6301\u7eed\u6027\u3002"}}
{"id": "2508.02943", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2508.02943", "abs": "https://arxiv.org/abs/2508.02943", "authors": ["Baigang Chen", "Dongfang Zhao"], "title": "A Non-leveled and Reliable Approximate FHE Framework through Binarized Polynomial Rings", "comment": null, "summary": "Homomorphic encryption (HE) enables secure computation on encrypted data,\nsafeguarding user privacy in domains such as cloud computing, healthcare, and\nfinance. Among fully homomorphic encryption (FHE) schemes, CKKS is notable for\nsupporting approximate arithmetic over complex numbers, a key requirement for\nmachine-learning and numerical workloads. However, CKKS incurs rapid noise\ngrowth, complex parameter tuning, and relies on costly modulus switching. We\npropose a binary variant of CKKS that operates entirely over binary-coefficient\npolynomial rings and replaces rescaling with a lightweight bootstrapping\nmechanism. To mitigate additional bit-flip errors introduced by binary\nencoding, we integrate BCH error-correcting codes for robust decryption. Our\nopen-source implementation, built on the HElib library, preserves the core\nalgebraic structure of CKKS while introducing binary-coefficient encoding,\nenabling efficient evaluation in small ring dimensions and unbounded-depth\ncomputation. Empirical evaluations demonstrate the framework's practicality and\nscalability across a range of settings.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4e8c\u8fdb\u5236\u7cfb\u6570\u591a\u9879\u5f0f\u73af\u7684CKKS\u53d8\u4f53\uff0c\u901a\u8fc7\u8f7b\u91cf\u7ea7\u81ea\u4e3e\u673a\u5236\u548cBCH\u7ea0\u9519\u7801\u4f18\u5316\u566a\u58f0\u589e\u957f\u548c\u89e3\u5bc6\u9c81\u68d2\u6027\u3002", "motivation": "\u89e3\u51b3CKKS\u65b9\u6848\u4e2d\u566a\u58f0\u589e\u957f\u5feb\u3001\u53c2\u6570\u8c03\u6574\u590d\u6742\u548c\u6a21\u5207\u6362\u6210\u672c\u9ad8\u7684\u95ee\u9898\u3002", "method": "\u91c7\u7528\u4e8c\u8fdb\u5236\u7cfb\u6570\u591a\u9879\u5f0f\u73af\u548c\u8f7b\u91cf\u7ea7\u81ea\u4e3e\u673a\u5236\uff0c\u7ed3\u5408BCH\u7ea0\u9519\u7801\u51cf\u5c11\u6bd4\u7279\u7ffb\u8f6c\u9519\u8bef\u3002", "result": "\u5f00\u6e90\u5b9e\u73b0\u663e\u793a\u8be5\u6846\u67b6\u5728\u5c0f\u73af\u7ef4\u5ea6\u548c\u65e0\u9650\u6df1\u5ea6\u8ba1\u7b97\u4e2d\u9ad8\u6548\u4e14\u53ef\u6269\u5c55\u3002", "conclusion": "\u4e8c\u8fdb\u5236\u53d8\u4f53CKKS\u5728\u4fdd\u6301\u6838\u5fc3\u4ee3\u6570\u7ed3\u6784\u7684\u540c\u65f6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5b9e\u7528\u6027\u548c\u53ef\u6269\u5c55\u6027\u3002"}}
{"id": "2508.02729", "categories": ["cs.SE", "cs.AI", "cs.PF"], "pdf": "https://arxiv.org/pdf/2508.02729", "abs": "https://arxiv.org/abs/2508.02729", "authors": ["Zhuoran Liu"], "title": "Interpreting Performance Profiles with Deep Learning", "comment": "Master of Science in Computer Science thesis, North Carolina State\n  University, 2022. Advisor: Dr. Xu Liu", "summary": "Profiling tools (also known as profilers) play an important role in\nunderstanding program performance at runtime, such as hotspots, bottlenecks,\nand inefficiencies. While profilers have been proven to be useful, they give\nextra burden to software engineers. Software engineers, as the users, are\nresponsible to interpret the complex performance data and identify actionable\noptimization in program source code. However, it can be challenging for users\nto associate inefficiencies with the program semantics, especially if the users\nare not the authors of the code, which limits the applicability of profilers.\n  In this thesis, we explore a new direction to combine performance profiles\nand program semantics with a deep learning approach. The key idea is to glean\ncode summary for semantic information (at a certain level) and integrate it\ninto a profiler, which can better understand program inefficiencies for\nactionable optimization. To be concrete, we combine profiles generated by Async\nProfiler (the state-of-the-art Java profiler) with code summarization from a\nfine-tuned CodeBERT-based model. We demonstrate the code summaries of any\nselected call path in a graphic user interface. Our system can effectively\nassist analysis on many Java benchmarks.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u6027\u80fd\u5206\u6790\u548c\u7a0b\u5e8f\u8bed\u4e49\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u6df1\u5ea6\u5b66\u4e60\u6280\u672f\uff08\u5982CodeBERT\uff09\u751f\u6210\u4ee3\u7801\u6458\u8981\uff0c\u5e2e\u52a9\u7528\u6237\u66f4\u76f4\u89c2\u5730\u7406\u89e3\u7a0b\u5e8f\u6027\u80fd\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u7684\u6027\u80fd\u5206\u6790\u5de5\u5177\uff08\u5982Async Profiler\uff09\u867d\u7136\u6709\u7528\uff0c\u4f46\u7528\u6237\uff08\u5c24\u5176\u662f\u975e\u4ee3\u7801\u4f5c\u8005\uff09\u96be\u4ee5\u5c06\u6027\u80fd\u6570\u636e\u4e0e\u7a0b\u5e8f\u8bed\u4e49\u5173\u8054\uff0c\u9650\u5236\u4e86\u5de5\u5177\u7684\u9002\u7528\u6027\u3002", "method": "\u7ed3\u5408Async Profiler\u751f\u6210\u7684\u6027\u80fd\u6570\u636e\u4e0e\u57fa\u4e8eCodeBERT\u7684\u4ee3\u7801\u6458\u8981\u6a21\u578b\uff0c\u901a\u8fc7\u56fe\u5f62\u754c\u9762\u5c55\u793a\u8c03\u7528\u8def\u5f84\u7684\u4ee3\u7801\u6458\u8981\u3002", "result": "\u7cfb\u7edf\u5728\u591a\u4e2aJava\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u6709\u6548\u8f85\u52a9\u4e86\u6027\u80fd\u5206\u6790\u3002", "conclusion": "\u901a\u8fc7\u6574\u5408\u6027\u80fd\u5206\u6790\u548c\u7a0b\u5e8f\u8bed\u4e49\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u5206\u6790\u5de5\u5177\u7684\u5b9e\u7528\u6027\u548c\u7528\u6237\u53cb\u597d\u6027\u3002"}}
{"id": "2508.02697", "categories": ["cs.AI", "cs.CE", "03B35 (Primary) 03A99, 03B10, 03B25, 68V15, 03C07 (Secondary)", "I.2.3; I.2.4; F.4.1; F.2.2; H.3.3"], "pdf": "https://arxiv.org/pdf/2508.02697", "abs": "https://arxiv.org/abs/2508.02697", "authors": ["Mikhail Soutchanski", "Yongmei Liu"], "title": "Planning with Dynamically Changing Domains", "comment": "A revised version of the paper accepted to the 1st International\n  Workshop on Trends in Knowledge Representation and Reasoning organized as a\n  IJCAI 2025 workshop that takes place in August 2025 in Montreal, Canada. See\n  the details at https://tkr2025.krportal.org/programme.html", "summary": "In classical planning and conformant planning, it is assumed that there are\nfinitely many named objects given in advance, and only they can participate in\nactions and in fluents. This is the Domain Closure Assumption (DCA). However,\nthere are practical planning problems where the set of objects changes\ndynamically as actions are performed; e.g., new objects can be created, old\nobjects can be destroyed. We formulate the planning problem in first-order\nlogic, assume an initial theory is a finite consistent set of fluent literals,\ndiscuss when this guarantees that in every situation there are only finitely\nmany possible actions, impose a finite integer bound on the length of the plan,\nand propose to organize search over sequences of actions that are grounded at\nplanning time. We show the soundness and completeness of our approach. It can\nbe used to solve the bounded planning problems without DCA that belong to the\nintersection of sequential generalized planning (without sensing actions) and\nconformant planning, restricted to the case without the disjunction over fluent\nliterals. We discuss a proof-of-the-concept implementation of our planner.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u9886\u57df\u95ed\u5305\u5047\u8bbe\uff08DCA\uff09\u7684\u89c4\u5212\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u5bf9\u8c61\u52a8\u6001\u53d8\u5316\u7684\u573a\u666f\uff0c\u5e76\u8bc1\u660e\u4e86\u5176\u5b8c\u5907\u6027\u548c\u6b63\u786e\u6027\u3002", "motivation": "\u4f20\u7edf\u89c4\u5212\u5047\u8bbe\u5bf9\u8c61\u96c6\u5408\u56fa\u5b9a\uff0c\u4f46\u5b9e\u9645\u5e94\u7528\u4e2d\u5bf9\u8c61\u53ef\u80fd\u52a8\u6001\u53d8\u5316\uff0c\u56e0\u6b64\u9700\u8981\u65b0\u7684\u89c4\u5212\u65b9\u6cd5\u3002", "method": "\u57fa\u4e8e\u4e00\u9636\u903b\u8f91\u7684\u89c4\u5212\u95ee\u9898\u5efa\u6a21\uff0c\u9650\u5236\u8ba1\u5212\u957f\u5ea6\uff0c\u5e76\u5728\u89c4\u5212\u65f6\u5bf9\u52a8\u4f5c\u5e8f\u5217\u8fdb\u884c\u5b9e\u4f8b\u5316\u641c\u7d22\u3002", "result": "\u8bc1\u660e\u4e86\u65b9\u6cd5\u7684\u5b8c\u5907\u6027\u548c\u6b63\u786e\u6027\uff0c\u9002\u7528\u4e8e\u65e0DCA\u7684\u6709\u9650\u89c4\u5212\u95ee\u9898\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u52a8\u6001\u5bf9\u8c61\u89c4\u5212\u95ee\u9898\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5e76\u901a\u8fc7\u6982\u5ff5\u9a8c\u8bc1\u5b9e\u73b0\u5c55\u793a\u4e86\u6f5c\u529b\u3002"}}
{"id": "2508.03062", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2508.03062", "abs": "https://arxiv.org/abs/2508.03062", "authors": ["Rourab Paul", "Paresh Baidya", "Krishnendu Guha"], "title": "Lightweight Fault Detection Architecture for NTT on FPGA", "comment": null, "summary": "Post-Quantum Cryptographic (PQC) algorithms are mathematically secure and\nresistant to quantum attacks but can still leak sensitive information in\nhardware implementations due to natural faults or intentional fault injections.\nThe intent fault injection in side-channel attacks reduces the reliability of\ncrypto implementation in future generation network security procesors. In this\nregard, this research proposes a lightweight, efficient, recomputation-based\nfault detection module implemented on a Field Programmable Gate Array (FPGA)\nfor Number Theoretic Transform (NTT). The NTT is primarily composed of memory\nunits and the Cooley-Tukey Butterfly Unit (CT-BU), a critical and\ncomputationally intensive hardware component essential for polynomial\nmultiplication. NTT and polynomial multiplication are fundamental building\nblocks in many PQC algorithms, including Kyber, NTRU, Ring-LWE, and others. In\nthis paper, we present a fault detection method called : Recomputation with a\nModular Offset (REMO) for the logic blocks of the CT-BU using Montgomery\nReduction and another method called Memory Rule Checkers for the memory\ncomponents used within the NTT. The proposed fault detection framework sets a\nnew benchmark by achieving high efficiency with significant low implementation\ncost. It occupies only 16 slices and a single DSP block, with a power\nconsumption of just 3mW in Artix-7 FPGA. The REMO-based detection mechanism\nachieves a fault coverage of 87.2% to 100%, adaptable across various word\nsizes, fault bit counts, and fault injection modes. Similarly, the Memory Rule\nCheckers demonstrate robust performance, achieving 50.7% to 100% fault\ndetection depending on and the nature of injected faults.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u3001\u9ad8\u6548\u7684\u57fa\u4e8e\u91cd\u65b0\u8ba1\u7b97\u7684\u6545\u969c\u68c0\u6d4b\u6a21\u5757\uff0c\u7528\u4e8e\u540e\u91cf\u5b50\u5bc6\u7801\uff08PQC\uff09\u7b97\u6cd5\u4e2d\u7684\u6570\u8bba\u53d8\u6362\uff08NTT\uff09\uff0c\u4ee5\u5e94\u5bf9\u786c\u4ef6\u5b9e\u73b0\u4e2d\u7684\u81ea\u7136\u6216\u6545\u610f\u6545\u969c\u6ce8\u5165\u3002", "motivation": "\u540e\u91cf\u5b50\u5bc6\u7801\u7b97\u6cd5\u5728\u786c\u4ef6\u5b9e\u73b0\u4e2d\u53ef\u80fd\u56e0\u81ea\u7136\u6545\u969c\u6216\u6545\u610f\u6545\u969c\u6ce8\u5165\u800c\u6cc4\u9732\u654f\u611f\u4fe1\u606f\uff0c\u5f71\u54cd\u672a\u6765\u7f51\u7edc\u5b89\u5168\u5904\u7406\u5668\u7684\u53ef\u9760\u6027\u3002", "method": "\u63d0\u51fa\u4e86\u4e24\u79cd\u65b9\u6cd5\uff1a\u57fa\u4e8eMontgomery Reduction\u7684REMO\uff08\u91cd\u65b0\u8ba1\u7b97\u4e0e\u6a21\u504f\u79fb\uff09\u7528\u4e8eCT-BU\u903b\u8f91\u5757\uff0c\u4ee5\u53caMemory Rule Checkers\u7528\u4e8eNTT\u4e2d\u7684\u5185\u5b58\u7ec4\u4ef6\u3002", "result": "\u8be5\u6846\u67b6\u5728Artix-7 FPGA\u4e0a\u4ec5\u5360\u752816\u4e2a\u5207\u7247\u548c1\u4e2aDSP\u5757\uff0c\u529f\u8017\u4e3a3mW\uff0c\u6545\u969c\u8986\u76d6\u7387\u4e3a87.2%\u81f3100%\uff08REMO\uff09\u548c50.7%\u81f3100%\uff08Memory Rule Checkers\uff09\u3002", "conclusion": "\u63d0\u51fa\u7684\u6545\u969c\u68c0\u6d4b\u6846\u67b6\u5728\u9ad8\u6548\u6027\u548c\u4f4e\u6210\u672c\u65b9\u9762\u8bbe\u5b9a\u4e86\u65b0\u6807\u51c6\uff0c\u9002\u7528\u4e8e\u591a\u79cdPQC\u7b97\u6cd5\u3002"}}
{"id": "2508.02732", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.02732", "abs": "https://arxiv.org/abs/2508.02732", "authors": ["Sherman Wong", "Jalaj Bhandari", "Leo Zhou Fan Yang", "Xylan Xu", "Yi Zhuang", "Cem Cayiroglu", "Payal Bhuptani", "Sheela Yadawad", "Hung Duong"], "title": "A Note on Code Quality Score: LLMs for Maintainable Large Codebases", "comment": "24 pages, ICLR format", "summary": "Maintaining code quality in large-scale software systems presents significant\nchallenges, particularly in settings where a large numbers of engineers work\nconcurrently on a codebase. This paper introduces Code Quality Score (CQS)\nsystem to automatically detect issues with a set of code changes and provide\nactionable insights. At its core, the CQS system is powered by two Llama3\nmodels, fine-tuned (with SFT and offline RL approaches), to a) detect common\ncode quality issues related to coding best practices and b) to provide good\n``critiques'' for LLM-generated code review respectively. To maintain good user\nexperience, we layer the system with hand-crafted rules to filter out incorrect\nresponses/hallucinations. Offline evaluations show that our CQS system is able\nto achieve an impressive precision rate for identifying valid issues. This\nsystem has already been rolled out to developers in an industrial scale setting\nand has consistently achieved 60\\% week over week user helpfulness rate,\ndemonstrating its effectiveness in a real-world environment. In this paper, we\npresent details of the CQS system along with some learnings on curating\ndeveloper feedback to create training data for LLM fine-tuning.", "AI": {"tldr": "\u8bba\u6587\u4ecb\u7ecd\u4e86Code Quality Score (CQS)\u7cfb\u7edf\uff0c\u901a\u8fc7\u4e24\u4e2a\u5fae\u8c03\u7684Llama3\u6a21\u578b\u81ea\u52a8\u68c0\u6d4b\u4ee3\u7801\u8d28\u91cf\u95ee\u9898\u5e76\u63d0\u4f9b\u6539\u8fdb\u5efa\u8bae\uff0c\u5df2\u5728\u5de5\u4e1a\u73af\u5883\u4e2d\u9a8c\u8bc1\u5176\u6709\u6548\u6027\u3002", "motivation": "\u5927\u89c4\u6a21\u8f6f\u4ef6\u7cfb\u7edf\u4e2d\u7ef4\u62a4\u4ee3\u7801\u8d28\u91cf\u5177\u6709\u6311\u6218\u6027\uff0c\u5c24\u5176\u662f\u5728\u591a\u4eba\u534f\u4f5c\u5f00\u53d1\u65f6\u3002", "method": "CQS\u7cfb\u7edf\u7ed3\u5408\u4e86\u4e24\u4e2a\u5fae\u8c03\u7684Llama3\u6a21\u578b\uff08\u4f7f\u7528SFT\u548c\u79bb\u7ebfRL\u65b9\u6cd5\uff09\uff0c\u5206\u522b\u7528\u4e8e\u68c0\u6d4b\u4ee3\u7801\u8d28\u91cf\u95ee\u9898\u548c\u63d0\u4f9b\u4ee3\u7801\u5ba1\u67e5\u5efa\u8bae\uff0c\u5e76\u8f85\u4ee5\u624b\u5de5\u89c4\u5219\u8fc7\u6ee4\u9519\u8bef\u54cd\u5e94\u3002", "result": "\u79bb\u7ebf\u8bc4\u4f30\u663e\u793aCQS\u7cfb\u7edf\u5728\u8bc6\u522b\u6709\u6548\u95ee\u9898\u65b9\u9762\u5177\u6709\u9ad8\u7cbe\u5ea6\uff0c\u5b9e\u9645\u5e94\u7528\u4e2d\u7528\u6237\u6ee1\u610f\u5ea6\u8fbe60%\u3002", "conclusion": "CQS\u7cfb\u7edf\u5728\u5de5\u4e1a\u73af\u5883\u4e2d\u6709\u6548\uff0c\u540c\u65f6\u63d0\u4f9b\u4e86\u5173\u4e8e\u5982\u4f55\u5229\u7528\u5f00\u53d1\u8005\u53cd\u9988\u4f18\u5316LLM\u5fae\u8c03\u7684\u7ecf\u9a8c\u3002"}}
{"id": "2508.02734", "categories": ["cs.AI", "cs.CE"], "pdf": "https://arxiv.org/pdf/2508.02734", "abs": "https://arxiv.org/abs/2508.02734", "authors": ["Weiyu Luo", "Chenfeng Xiong"], "title": "Recovering Individual-Level Activity Sequences from Location-Based Service Data Using a Novel Transformer-Based Model", "comment": "20 pages, 5 figures", "summary": "Location-Based Service (LBS) data provides critical insights into human\nmobility, yet its sparsity often yields incomplete trip and activity sequences,\nmaking accurate inferences about trips and activities difficult. We raise a\nresearch problem: Can we use activity sequences derived from high-quality LBS\ndata to recover incomplete activity sequences at the individual level? This\nstudy proposes a new solution, the Variable Selection Network-fused Insertion\nTransformer (VSNIT), integrating the Insertion Transformer's flexible sequence\nconstruction with the Variable Selection Network's dynamic covariate handling\ncapability, to recover missing segments in incomplete activity sequences while\npreserving existing data. The findings show that VSNIT inserts more diverse,\nrealistic activity patterns, more closely matching real-world variability, and\nrestores disrupted activity transitions more effectively aligning with the\ntarget. It also performs significantly better than the baseline model across\nall metrics. These results highlight VSNIT's superior accuracy and diversity in\nactivity sequence recovery tasks, demonstrating its potential to enhance LBS\ndata utility for mobility analysis. This approach offers a promising framework\nfor future location-based research and applications.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aVSNIT\u7684\u65b0\u65b9\u6cd5\uff0c\u7528\u4e8e\u4ece\u4e0d\u5b8c\u6574\u7684LBS\u6570\u636e\u4e2d\u6062\u590d\u6d3b\u52a8\u5e8f\u5217\uff0c\u8868\u73b0\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\u3002", "motivation": "LBS\u6570\u636e\u7684\u7a00\u758f\u6027\u5bfc\u81f4\u6d3b\u52a8\u5e8f\u5217\u4e0d\u5b8c\u6574\uff0c\u96be\u4ee5\u51c6\u786e\u63a8\u65ad\u51fa\u884c\u548c\u6d3b\u52a8\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u6709\u6548\u7684\u65b9\u6cd5\u6765\u6062\u590d\u7f3a\u5931\u6570\u636e\u3002", "method": "\u7ed3\u5408Insertion Transformer\u7684\u7075\u6d3b\u5e8f\u5217\u6784\u5efa\u548cVariable Selection Network\u7684\u52a8\u6001\u534f\u53d8\u91cf\u5904\u7406\u80fd\u529b\uff0c\u63d0\u51faVSNIT\u6a21\u578b\uff0c\u7528\u4e8e\u6062\u590d\u7f3a\u5931\u7684\u6d3b\u52a8\u5e8f\u5217\u7247\u6bb5\u3002", "result": "VSNIT\u80fd\u751f\u6210\u66f4\u591a\u6837\u5316\u4e14\u771f\u5b9e\u7684\u6d3b\u52a8\u6a21\u5f0f\uff0c\u66f4\u63a5\u8fd1\u771f\u5b9e\u4e16\u754c\u7684\u53d8\u5f02\u6027\uff0c\u5e76\u5728\u6240\u6709\u6307\u6807\u4e0a\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\u3002", "conclusion": "VSNIT\u5728\u6d3b\u52a8\u5e8f\u5217\u6062\u590d\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u66f4\u9ad8\u7684\u51c6\u786e\u6027\u548c\u591a\u6837\u6027\uff0c\u4e3a\u672a\u6765\u57fa\u4e8e\u4f4d\u7f6e\u7684\u7814\u7a76\u548c\u5e94\u7528\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u6846\u67b6\u3002"}}
{"id": "2508.03067", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.03067", "abs": "https://arxiv.org/abs/2508.03067", "authors": ["Jiewei Lai", "Lan Zhang", "Chen Tang", "Pengcheng Sun", "Xinming Wang", "Yunhao Wang"], "title": "Untraceable DeepFakes via Traceable Fingerprint Elimination", "comment": null, "summary": "Recent advancements in DeepFakes attribution technologies have significantly\nenhanced forensic capabilities, enabling the extraction of traces left by\ngenerative models (GMs) in images, making DeepFakes traceable back to their\nsource GMs. Meanwhile, several attacks have attempted to evade attribution\nmodels (AMs) for exploring their limitations, calling for more robust AMs.\nHowever, existing attacks fail to eliminate GMs' traces, thus can be mitigated\nby defensive measures. In this paper, we identify that untraceable DeepFakes\ncan be achieved through a multiplicative attack, which can fundamentally\neliminate GMs' traces, thereby evading AMs even enhanced with defensive\nmeasures. We design a universal and black-box attack method that trains an\nadversarial model solely using real data, applicable for various GMs and\nagnostic to AMs. Experimental results demonstrate the outstanding attack\ncapability and universal applicability of our method, achieving an average\nattack success rate (ASR) of 97.08\\% against 6 advanced AMs on DeepFakes\ngenerated by 9 GMs. Even in the presence of defensive mechanisms, our method\nmaintains an ASR exceeding 72.39\\%. Our work underscores the potential\nchallenges posed by multiplicative attacks and highlights the need for more\nrobust AMs.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4e58\u6cd5\u653b\u51fb\u65b9\u6cd5\uff0c\u80fd\u591f\u5f7b\u5e95\u6d88\u9664\u751f\u6210\u6a21\u578b\uff08GMs\uff09\u7684\u75d5\u8ff9\uff0c\u4ece\u800c\u9003\u907f\u589e\u5f3a\u9632\u5fa1\u63aa\u65bd\u7684\u5c5e\u6027\u6a21\u578b\uff08AMs\uff09\u3002", "motivation": "\u73b0\u6709\u653b\u51fb\u65b9\u6cd5\u65e0\u6cd5\u5b8c\u5168\u6d88\u9664GMs\u75d5\u8ff9\uff0c\u4e14\u9632\u5fa1\u63aa\u65bd\u53ef\u7f13\u89e3\u8fd9\u4e9b\u653b\u51fb\uff0c\u56e0\u6b64\u9700\u8981\u66f4\u5f3a\u5927\u7684AMs\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e00\u79cd\u901a\u7528\u4e14\u9ed1\u76d2\u7684\u653b\u51fb\u65b9\u6cd5\uff0c\u4ec5\u4f7f\u7528\u771f\u5b9e\u6570\u636e\u8bad\u7ec3\u5bf9\u6297\u6a21\u578b\uff0c\u9002\u7528\u4e8e\u591a\u79cdGMs\u4e14\u5bf9AMs\u4e0d\u53ef\u77e5\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u8be5\u65b9\u6cd5\u5bf96\u79cd\u5148\u8fdbAMs\u548c9\u79cdGMs\u751f\u6210\u7684DeepFakes\u5e73\u5747\u653b\u51fb\u6210\u529f\u7387\u8fbe97.08%\uff0c\u5373\u4f7f\u6709\u9632\u5fa1\u673a\u5236\uff0c\u653b\u51fb\u6210\u529f\u7387\u4ecd\u8d85\u8fc772.39%\u3002", "conclusion": "\u4e58\u6cd5\u653b\u51fb\u53ef\u80fd\u5e26\u6765\u6f5c\u5728\u6311\u6218\uff0c\u5f3a\u8c03\u4e86\u5f00\u53d1\u66f4\u5f3a\u5927AMs\u7684\u5fc5\u8981\u6027\u3002"}}
{"id": "2508.02733", "categories": ["cs.SE", "cs.HC"], "pdf": "https://arxiv.org/pdf/2508.02733", "abs": "https://arxiv.org/abs/2508.02733", "authors": ["Rijul Jain", "Shraddha Barke", "Gabriel Ebner", "Md Rakib Hossain Misu", "Shan Lu", "Sarah Fakhoury"], "title": "What's in a Proof? Analyzing Expert Proof-Writing Processes in F* and Verus", "comment": null, "summary": "Proof-oriented programming languages (POPLs) empower developers to write code\nalongside formal correctness proofs, providing formal guarantees that the code\nadheres to specified requirements. Despite their powerful capabilities, POPLs\npresent a steep learning curve and have not yet been adopted by the broader\nsoftware community. The lack of understanding about the proof-development\nprocess and how expert proof developers interact with POPLs has hindered the\nadvancement of effective proof engineering and the development of\nproof-synthesis models/tools.\n  In this work, we conduct a user study, involving the collection and analysis\nof fine-grained source code telemetry from eight experts working with two\nlanguages, F* and Verus. Results reveal interesting trends and patterns about\nhow experts reason about proofs and key challenges encountered during the proof\ndevelopment process. We identify three distinct strategies and multiple\ninformal practices that are not captured final code snapshots, yet are\npredictive of task outcomes. We translate these findings into concrete design\nguidance for AI proof assistants: bias toward early specification drafting,\nexplicit sub-goal decomposition, bounded active errors, and disciplined\nverifier interaction. We also present a case study of an F* proof agent\ngrounded in these recommendations, and demonstrate improved performance over\nbaseline LLMs", "AI": {"tldr": "\u7814\u7a76\u901a\u8fc7\u7528\u6237\u5b9e\u9a8c\u5206\u6790\u4e13\u5bb6\u4f7f\u7528POPLs\uff08\u5982F*\u548cVerus\uff09\u7684\u884c\u4e3a\u6a21\u5f0f\uff0c\u53d1\u73b0\u4e09\u79cd\u7b56\u7565\u548c\u672a\u8bb0\u5f55\u7684\u5b9e\u8df5\uff0c\u63d0\u51faAI\u8bc1\u660e\u52a9\u624b\u7684\u6539\u8fdb\u5efa\u8bae\uff0c\u5e76\u9a8c\u8bc1\u5176\u6548\u679c\u3002", "motivation": "POPLs\u867d\u5f3a\u5927\u4f46\u5b66\u4e60\u66f2\u7ebf\u9661\u5ced\uff0c\u7f3a\u4e4f\u5bf9\u4e13\u5bb6\u4f7f\u7528\u884c\u4e3a\u7684\u7406\u89e3\uff0c\u963b\u788d\u4e86\u8bc1\u660e\u5de5\u7a0b\u548c\u5de5\u5177\u7684\u53d1\u5c55\u3002", "method": "\u6536\u96c6\u5e76\u5206\u6790\u516b\u4f4d\u4e13\u5bb6\u4f7f\u7528F*\u548cVerus\u7684\u7ec6\u7c92\u5ea6\u6e90\u4ee3\u7801\u9065\u6d4b\u6570\u636e\u3002", "result": "\u53d1\u73b0\u4e09\u79cd\u7b56\u7565\u548c\u672a\u8bb0\u5f55\u7684\u5b9e\u8df5\uff0c\u63d0\u51faAI\u8bc1\u660e\u52a9\u624b\u7684\u8bbe\u8ba1\u5efa\u8bae\uff0c\u5e76\u901a\u8fc7\u6848\u4f8b\u9a8c\u8bc1\u6539\u8fdb\u6548\u679c\u3002", "conclusion": "\u7814\u7a76\u4e3aAI\u8bc1\u660e\u52a9\u624b\u63d0\u4f9b\u4e86\u5b9e\u7528\u8bbe\u8ba1\u6307\u5bfc\uff0c\u5e76\u5c55\u793a\u4e86\u5176\u6f5c\u5728\u4f18\u52bf\u3002"}}
{"id": "2508.02744", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.02744", "abs": "https://arxiv.org/abs/2508.02744", "authors": ["Peiran Wang", "Yaoning Yu", "Ke Chen", "Xianyang Zhan", "Haohan Wang"], "title": "Large Language Model-based Data Science Agent: A Survey", "comment": null, "summary": "The rapid advancement of Large Language Models (LLMs) has driven novel\napplications across diverse domains, with LLM-based agents emerging as a\ncrucial area of exploration. This survey presents a comprehensive analysis of\nLLM-based agents designed for data science tasks, summarizing insights from\nrecent studies. From the agent perspective, we discuss the key design\nprinciples, covering agent roles, execution, knowledge, and reflection methods.\nFrom the data science perspective, we identify key processes for LLM-based\nagents, including data preprocessing, model development, evaluation,\nvisualization, etc. Our work offers two key contributions: (1) a comprehensive\nreview of recent developments in applying LLMbased agents to data science\ntasks; (2) a dual-perspective framework that connects general agent design\nprinciples with the practical workflows in data science.", "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u4e86\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u4ee3\u7406\u5728\u6570\u636e\u79d1\u5b66\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\uff0c\u4ece\u4ee3\u7406\u548c\u6570\u636e\u79d1\u5b66\u53cc\u89c6\u89d2\u5206\u6790\u4e86\u8bbe\u8ba1\u539f\u5219\u548c\u5173\u952e\u6d41\u7a0b\u3002", "motivation": "\u63a2\u7d22LLM\u4ee3\u7406\u5728\u6570\u636e\u79d1\u5b66\u4efb\u52a1\u4e2d\u7684\u6f5c\u529b\uff0c\u603b\u7ed3\u6700\u65b0\u7814\u7a76\u8fdb\u5c55\u3002", "method": "\u4ece\u4ee3\u7406\u89c6\u89d2\u8ba8\u8bba\u8bbe\u8ba1\u539f\u5219\uff08\u89d2\u8272\u3001\u6267\u884c\u3001\u77e5\u8bc6\u3001\u53cd\u601d\uff09\uff0c\u4ece\u6570\u636e\u79d1\u5b66\u89c6\u89d2\u68b3\u7406\u5173\u952e\u6d41\u7a0b\uff08\u6570\u636e\u9884\u5904\u7406\u3001\u6a21\u578b\u5f00\u53d1\u7b49\uff09\u3002", "result": "\u63d0\u51fa\u4e86\u53cc\u89c6\u89d2\u6846\u67b6\uff0c\u8fde\u63a5\u4ee3\u7406\u8bbe\u8ba1\u539f\u5219\u4e0e\u6570\u636e\u79d1\u5b66\u5b9e\u8df5\u3002", "conclusion": "\u4e3aLLM\u4ee3\u7406\u5728\u6570\u636e\u79d1\u5b66\u4e2d\u7684\u5e94\u7528\u63d0\u4f9b\u4e86\u7cfb\u7edf\u7efc\u8ff0\u548c\u5b9e\u7528\u6846\u67b6\u3002"}}
{"id": "2508.03097", "categories": ["cs.CR", "cs.AI", "I.2.11"], "pdf": "https://arxiv.org/pdf/2508.03097", "abs": "https://arxiv.org/abs/2508.03097", "authors": ["Zixuan Gu", "Qiufeng Fan", "Long Sun", "Yang Liu", "Xiaojun Ye"], "title": "VFLAIR-LLM: A Comprehensive Framework and Benchmark for Split Learning of LLMs", "comment": "12 pages, 10 figures, published in KDD2025", "summary": "With the advancement of Large Language Models (LLMs), LLM applications have\nexpanded into a growing number of fields. However, users with data privacy\nconcerns face limitations in directly utilizing LLM APIs, while private\ndeployments incur significant computational demands. This creates a substantial\nchallenge in achieving secure LLM adaptation under constrained local resources.\nTo address this issue, collaborative learning methods, such as Split Learning\n(SL), offer a resource-efficient and privacy-preserving solution for adapting\nLLMs to private domains. In this study, we introduce VFLAIR-LLM (available at\nhttps://github.com/FLAIR-THU/VFLAIR-LLM), an extensible and lightweight split\nlearning framework for LLMs, enabling privacy-preserving LLM inference and\nfine-tuning in resource-constrained environments. Our library provides two LLM\npartition settings, supporting three task types and 18 datasets. In addition,\nwe provide standard modules for implementing and evaluating attacks and\ndefenses. We benchmark 5 attacks and 9 defenses under various Split Learning\nfor LLM(SL-LLM) settings, offering concrete insights and recommendations on the\nchoice of model partition configurations, defense strategies, and relevant\nhyperparameters for real-world applications.", "AI": {"tldr": "VFLAIR-LLM\u662f\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u7684\u5206\u5e03\u5f0f\u5b66\u4e60\u6846\u67b6\uff0c\u652f\u6301\u5728\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e0b\u8fdb\u884c\u9690\u79c1\u4fdd\u62a4\u7684LLM\u63a8\u7406\u548c\u5fae\u8c03\uff0c\u63d0\u4f9b\u4e86\u591a\u79cd\u653b\u51fb\u548c\u9632\u5fa1\u7684\u8bc4\u4f30\u6a21\u5757\u3002", "motivation": "\u89e3\u51b3\u7528\u6237\u56e0\u9690\u79c1\u95ee\u9898\u65e0\u6cd5\u76f4\u63a5\u4f7f\u7528LLM API\uff0c\u4ee5\u53ca\u79c1\u6709\u90e8\u7f72\u8ba1\u7b97\u8d44\u6e90\u9700\u6c42\u9ad8\u7684\u95ee\u9898\u3002", "method": "\u91c7\u7528Split Learning\uff08SL\uff09\u65b9\u6cd5\uff0c\u8bbe\u8ba1\u4e86VFLAIR-LLM\u6846\u67b6\uff0c\u652f\u6301\u4e24\u79cdLLM\u5206\u533a\u8bbe\u7f6e\u3001\u4e09\u79cd\u4efb\u52a1\u7c7b\u578b\u548c18\u4e2a\u6570\u636e\u96c6\u3002", "result": "\u8bc4\u4f30\u4e865\u79cd\u653b\u51fb\u548c9\u79cd\u9632\u5fa1\u7b56\u7565\uff0c\u63d0\u4f9b\u4e86\u6a21\u578b\u5206\u533a\u914d\u7f6e\u3001\u9632\u5fa1\u7b56\u7565\u548c\u8d85\u53c2\u6570\u9009\u62e9\u7684\u5b9e\u7528\u5efa\u8bae\u3002", "conclusion": "VFLAIR-LLM\u4e3a\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e0b\u7684\u9690\u79c1\u4fdd\u62a4LLM\u5e94\u7528\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.02820", "categories": ["cs.SE", "cs.PL"], "pdf": "https://arxiv.org/pdf/2508.02820", "abs": "https://arxiv.org/abs/2508.02820", "authors": ["David Svoboda", "Lori Flynn", "William Klieber", "Michael Duggan", "Nicholas Reimer", "Joseph Sible"], "title": "Automated Code Repair for C/C++ Static Analysis Alerts", "comment": null, "summary": "(Note: This work is a preprint.) Static analysis (SA) tools produce many\ndiagnostic alerts indicating that source code in C or C++ may be defective and\npotentially vulnerable to security exploits. Many of these alerts are false\npositives. Identifying the true-positive alerts and repairing the defects in\nthe associated code are huge efforts that automated program repair (APR) tools\ncan help with. Our experience showed us that APR can reduce the number of SA\nalerts significantly and reduce the manual effort of analysts to review code.\nThis engineering experience paper details the application of design,\ndevelopment, and performance testing to an APR tool we built that repairs C/C++\ncode associated with 3 categories of alerts produced by multiple SA tools. Its\nrepairs are simple and local. Furthermore, our findings convinced the\nmaintainers of the CERT Coding Standards to re-assess and update the metrics\nused to assess when violations of guidelines are detectable or repairable. We\ndiscuss engineering design choices made to support goals of trustworthiness and\nacceptability to developers. Our APR tool repaired 8718 out of 9234 alerts\nproduced by one SA tool on one codebase. It can repair 3 flaw categories. For 2\nflaw categories, 2 SA tools, and 2 codebases, our tool repaired or dismissed as\nfalse positives over 80% of alerts, on average. Tests showed repairs did not\nappreciably degrade the performance of the code or cause new alerts to appear\n(with the possible exception of sqlite3.c). This paper describes unique\ncontributions that include a new empirical analysis of SA data, our selection\nmethod for flaw categories to repair, publication of our APR tool, and a\ndataset of SA alerts from open-source SA tools run on open-source codebases. It\ndiscusses positive and negative results and lessons learned.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63a2\u8ba8\u4e86\u5982\u4f55\u5229\u7528\u81ea\u52a8\u5316\u7a0b\u5e8f\u4fee\u590d\uff08APR\uff09\u5de5\u5177\u51cf\u5c11\u9759\u6001\u5206\u6790\uff08SA\uff09\u5de5\u5177\u4ea7\u751f\u7684\u8bef\u62a5\u8b66\u62a5\uff0c\u5e76\u901a\u8fc7\u5b9e\u9645\u5de5\u7a0b\u7ecf\u9a8c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u9759\u6001\u5206\u6790\u5de5\u5177\u5728C/C++\u4ee3\u7801\u4e2d\u4ea7\u751f\u5927\u91cf\u8bca\u65ad\u8b66\u62a5\uff0c\u5176\u4e2d\u8bb8\u591a\u662f\u8bef\u62a5\u3002\u624b\u52a8\u8bc6\u522b\u548c\u4fee\u590d\u8fd9\u4e9b\u8b66\u62a5\u8017\u65f6\u8017\u529b\uff0c\u56e0\u6b64\u9700\u8981\u81ea\u52a8\u5316\u5de5\u5177\u6765\u8f85\u52a9\u3002", "method": "\u8bbe\u8ba1\u3001\u5f00\u53d1\u548c\u6027\u80fd\u6d4b\u8bd5\u4e86\u4e00\u4e2aAPR\u5de5\u5177\uff0c\u7528\u4e8e\u4fee\u590d\u7531\u591a\u4e2aSA\u5de5\u5177\u4ea7\u751f\u7684\u4e09\u7c7b\u8b66\u62a5\u3002\u4fee\u590d\u65b9\u6cd5\u7b80\u5355\u4e14\u5c40\u90e8\u5316\u3002", "result": "APR\u5de5\u5177\u6210\u529f\u4fee\u590d\u4e869234\u4e2a\u8b66\u62a5\u4e2d\u76848718\u4e2a\uff0c\u5e76\u5728\u4e24\u4e2a\u4ee3\u7801\u5e93\u4e2d\u5bf9\u4e24\u7c7b\u7f3a\u9677\u7684\u8b66\u62a5\u4fee\u590d\u6216\u6392\u9664\u4e8680%\u4ee5\u4e0a\u3002\u4fee\u590d\u672a\u663e\u8457\u5f71\u54cd\u4ee3\u7801\u6027\u80fd\u6216\u5f15\u53d1\u65b0\u8b66\u62a5\u3002", "conclusion": "APR\u5de5\u5177\u663e\u8457\u51cf\u5c11\u4e86SA\u8b66\u62a5\u6570\u91cf\uff0c\u63d0\u5347\u4e86\u5f00\u53d1\u6548\u7387\uff0c\u5e76\u4e3aCERT\u7f16\u7801\u6807\u51c6\u7684\u8bc4\u4f30\u63d0\u4f9b\u4e86\u65b0\u4f9d\u636e\u3002\u8bba\u6587\u8fd8\u5206\u4eab\u4e86\u5de5\u7a0b\u8bbe\u8ba1\u548c\u6570\u636e\u96c6\u7b49\u8d21\u732e\u3002"}}
{"id": "2508.02789", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.02789", "abs": "https://arxiv.org/abs/2508.02789", "authors": ["Newman Cheng", "Gordon Broadbent", "William Chappell"], "title": "Cognitive Loop via In-Situ Optimization: Self-Adaptive Reasoning for Science", "comment": null, "summary": "The capacity for artificial intelligence (AI) to formulate, evolve, and test\naltered thought patterns under dynamic conditions indicates advanced cognition\nthat is crucial for scientific discovery. The existing AI development landscape\nfalls into two categories: 1) frameworks over non-reasoning models that\nnatively incorporate opinions on how humans think, and 2) reasoning models that\nabstract precise control of the reasoning intuition away from end users. While\npowerful, for scientists to maximize utility of AI in scientific discovery,\nthey not only require accuracy and transparency in reasoning, but also\nsteerability. Hence, we introduce an alternative approach that enables deep and\nprecise control over the reasoning process called: a cognitive loop via in-situ\noptimization (CLIO). CLIO enables large language models (LLMs) to\nself-formulate ways of approaching a problem, adapt behavior when\nself-confidence is low, and ultimately provide scientists with a final belief\nor answer. Through CLIO's open design, scientists can observe uncertainty\nlevels, understand how final belief states are formulated using graph\nstructures, and interject corrections. Without any further post-training,\nOpenAI's GPT-4.1 with CLIO yields an accuracy of 22.37\\% in text-based biology\nand medicine questions on Humanity's Last Exam (HLE). This yields a 13.82\\% net\nor 161.64\\% relative increase when compared to the base GPT-4.1 model and\nsurpasses OpenAI's o3 performance in high and low reasoning effort modes. We\nfurther discovered that oscillations within internal uncertainty measures are\nkey in determining the accuracy of CLIO's results, revealing how its open\ndesign and internal mechanisms can provide insight and control into scientific\ndecision-making processes.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aCLIO\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u52a8\u6001\u4f18\u5316\u5b9e\u73b0AI\u7684\u6df1\u5ea6\u63a8\u7406\u63a7\u5236\uff0c\u663e\u8457\u63d0\u5347\u4e86GPT-4.1\u5728\u751f\u7269\u5b66\u548c\u533b\u5b66\u95ee\u9898\u4e0a\u7684\u51c6\u786e\u6027\u3002", "motivation": "\u73b0\u6709AI\u6846\u67b6\u5728\u79d1\u5b66\u53d1\u73b0\u4e2d\u7f3a\u4e4f\u900f\u660e\u6027\u548c\u53ef\u64cd\u63a7\u6027\uff0c\u79d1\u5b66\u5bb6\u9700\u8981\u66f4\u7cbe\u786e\u7684\u63a8\u7406\u63a7\u5236\u65b9\u6cd5\u3002", "method": "\u5f15\u5165CLIO\uff08\u8ba4\u77e5\u5faa\u73af\u52a8\u6001\u4f18\u5316\uff09\uff0c\u4f7f\u5927\u578b\u8bed\u8a00\u6a21\u578b\u80fd\u591f\u81ea\u6211\u8c03\u6574\u63a8\u7406\u8fc7\u7a0b\uff0c\u5e76\u901a\u8fc7\u5f00\u653e\u8bbe\u8ba1\u8ba9\u79d1\u5b66\u5bb6\u5e72\u9884\u548c\u89c2\u5bdf\u3002", "result": "CLIO\u4f7fGPT-4.1\u5728HLE\u6d4b\u8bd5\u4e2d\u7684\u51c6\u786e\u7387\u63d0\u5347\u81f322.37%\uff0c\u76f8\u5bf9\u63d0\u5347161.64%\uff0c\u5e76\u63ed\u793a\u4e86\u5185\u90e8\u4e0d\u786e\u5b9a\u6027\u6ce2\u52a8\u5bf9\u7ed3\u679c\u7684\u5f71\u54cd\u3002", "conclusion": "CLIO\u4e3a\u79d1\u5b66\u51b3\u7b56\u63d0\u4f9b\u4e86\u900f\u660e\u548c\u53ef\u63a7\u7684\u63a8\u7406\u673a\u5236\uff0c\u5c55\u793a\u4e86\u5176\u5728\u79d1\u5b66\u53d1\u73b0\u4e2d\u7684\u6f5c\u529b\u3002"}}
{"id": "2508.03125", "categories": ["cs.CR", "cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2508.03125", "abs": "https://arxiv.org/abs/2508.03125", "authors": ["Bingyu Yan", "Ziyi Zhou", "Xiaoming Zhang", "Chaozhuo Li", "Ruilin Zeng", "Yirui Qi", "Tianbo Wang", "Litian Zhang"], "title": "Attack the Messages, Not the Agents: A Multi-round Adaptive Stealthy Tampering Framework for LLM-MAS", "comment": null, "summary": "Large language model-based multi-agent systems (LLM-MAS) effectively\naccomplish complex and dynamic tasks through inter-agent communication, but\nthis reliance introduces substantial safety vulnerabilities. Existing attack\nmethods targeting LLM-MAS either compromise agent internals or rely on direct\nand overt persuasion, which limit their effectiveness, adaptability, and\nstealthiness. In this paper, we propose MAST, a Multi-round Adaptive Stealthy\nTampering framework designed to exploit communication vulnerabilities within\nthe system. MAST integrates Monte Carlo Tree Search with Direct Preference\nOptimization to train an attack policy model that adaptively generates\neffective multi-round tampering strategies. Furthermore, to preserve\nstealthiness, we impose dual semantic and embedding similarity constraints\nduring the tampering process. Comprehensive experiments across diverse tasks,\ncommunication architectures, and LLMs demonstrate that MAST consistently\nachieves high attack success rates while significantly enhancing stealthiness\ncompared to baselines. These findings highlight the effectiveness,\nstealthiness, and adaptability of MAST, underscoring the need for robust\ncommunication safeguards in LLM-MAS.", "AI": {"tldr": "MAST\u6846\u67b6\u901a\u8fc7\u591a\u8f6e\u81ea\u9002\u5e94\u9690\u853d\u7be1\u6539\u653b\u51fbLLM-MAS\uff0c\u663e\u8457\u63d0\u5347\u653b\u51fb\u6210\u529f\u7387\u548c\u9690\u853d\u6027\u3002", "motivation": "\u73b0\u6709\u653b\u51fb\u65b9\u6cd5\u5bf9LLM-MAS\u7684\u901a\u4fe1\u6f0f\u6d1e\u5229\u7528\u4e0d\u8db3\uff0c\u7f3a\u4e4f\u9002\u5e94\u6027\u548c\u9690\u853d\u6027\u3002", "method": "\u7ed3\u5408\u8499\u7279\u5361\u6d1b\u6811\u641c\u7d22\u4e0e\u76f4\u63a5\u504f\u597d\u4f18\u5316\uff0c\u8bad\u7ec3\u653b\u51fb\u7b56\u7565\u6a21\u578b\uff0c\u5e76\u5f15\u5165\u8bed\u4e49\u548c\u5d4c\u5165\u76f8\u4f3c\u6027\u7ea6\u675f\u3002", "result": "MAST\u5728\u591a\u79cd\u4efb\u52a1\u548c\u67b6\u6784\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u653b\u51fb\u6210\u529f\u7387\u9ad8\u4e14\u9690\u853d\u6027\u5f3a\u3002", "conclusion": "MAST\u5c55\u793a\u4e86\u9ad8\u6548\u3001\u9690\u853d\u548c\u9002\u5e94\u6027\u5f3a\u7684\u653b\u51fb\u80fd\u529b\uff0c\u51f8\u663e\u4e86LLM-MAS\u9700\u52a0\u5f3a\u901a\u4fe1\u9632\u62a4\u3002"}}
{"id": "2508.02827", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.02827", "abs": "https://arxiv.org/abs/2508.02827", "authors": ["Ora Nova Fandina", "Eitan Farchi", "Shmulik Froimovich", "Rami Katan", "Alice Podolsky", "Orna Raz", "Avi Ziv"], "title": "Automated Validation of LLM-based Evaluators for Software Engineering Artifacts", "comment": null, "summary": "Automation in software engineering increasingly relies on large language\nmodels (LLMs) to generate, review, and assess code artifacts. However,\nestablishing LLMs as reliable evaluators remains an open challenge: human\nevaluations are costly, subjective and non scalable, while existing automated\nmethods fail to discern fine grained variations in artifact quality.\n  We introduce REFINE (Ranking Evaluators for FIne grained Nuanced Evaluation),\nan automated framework for benchmarking LLM based evaluators across software\nengineering tasks. REFINE comprises of two modules: Hierarchy Dataset Builder\napplies novel generation techniques to automatically synthesize artifacts with\nprogressively reduced quality, and Evaluator Tester quantifies each candidate\nevaluator configuration by measuring how closely its rankings align with\nexpected ordering.\n  A key feature of REFINE is controllability: users can tune the granularity of\ndegradation to progressively refine evaluator configurations, from coarse\nfiltering to stress testing on subtle quality gaps.\n  While the methodology is general, we focus on coding tasks reflecting the\npractical demands in our production setting. REFINE was integrated into IBM's\ninternal development workflows and applied to code generation, translation, and\nsummarization for COBOL, an enterprise critical programming language, using\nindustrial data. It was used to identify LLM as a Judge configurations that\nlifted alignment scores from below $0.7$ to above $0.9$ in some coding tasks.\nThese nuance sensitive evaluators are now actively used by model training teams\nto support model release decisions.", "AI": {"tldr": "REFINE\u662f\u4e00\u4e2a\u81ea\u52a8\u5316\u6846\u67b6\uff0c\u7528\u4e8e\u8bc4\u4f30\u57fa\u4e8eLLM\u7684\u4ee3\u7801\u8bc4\u4f30\u5668\uff0c\u901a\u8fc7\u751f\u6210\u9010\u6b65\u964d\u4f4e\u8d28\u91cf\u7684\u4ee3\u7801\u6837\u672c\u548c\u91cf\u5316\u8bc4\u4f30\u5668\u914d\u7f6e\u7684\u6392\u540d\u4e00\u81f4\u6027\uff0c\u63d0\u5347\u8bc4\u4f30\u7684\u7cbe\u7ec6\u5ea6\u3002", "motivation": "\u73b0\u6709\u81ea\u52a8\u5316\u65b9\u6cd5\u65e0\u6cd5\u6355\u6349\u4ee3\u7801\u8d28\u91cf\u7684\u7ec6\u5fae\u5dee\u5f02\uff0c\u800c\u4eba\u5de5\u8bc4\u4f30\u6210\u672c\u9ad8\u4e14\u4e0d\u53ef\u6269\u5c55\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u53ef\u9760\u4e14\u53ef\u6269\u5c55\u7684LLM\u8bc4\u4f30\u65b9\u6cd5\u3002", "method": "REFINE\u5305\u542b\u4e24\u4e2a\u6a21\u5757\uff1aHierarchy Dataset Builder\u751f\u6210\u8d28\u91cf\u9010\u6b65\u964d\u4f4e\u7684\u4ee3\u7801\u6837\u672c\uff0cEvaluator Tester\u91cf\u5316\u8bc4\u4f30\u5668\u914d\u7f6e\u7684\u6392\u540d\u4e00\u81f4\u6027\u3002", "result": "REFINE\u5728IBM\u5185\u90e8\u5f00\u53d1\u6d41\u7a0b\u4e2d\u5e94\u7528\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8bc4\u4f30\u5668\u914d\u7f6e\u7684\u8bc4\u5206\uff08\u4ece0.7\u4ee5\u4e0b\u63d0\u5347\u81f30.9\u4ee5\u4e0a\uff09\u3002", "conclusion": "REFINE\u901a\u8fc7\u53ef\u63a7\u7684\u7cbe\u7ec6\u5316\u8bc4\u4f30\u65b9\u6cd5\uff0c\u4e3aLLM\u8bc4\u4f30\u5668\u7684\u53ef\u9760\u6027\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u5e76\u5df2\u5b9e\u9645\u5e94\u7528\u4e8e\u6a21\u578b\u8bad\u7ec3\u51b3\u7b56\u3002"}}
{"id": "2508.02841", "categories": ["cs.AI", "cs.IR"], "pdf": "https://arxiv.org/pdf/2508.02841", "abs": "https://arxiv.org/abs/2508.02841", "authors": ["Ziruo Yi", "Jinyu Liu", "Ting Xiao", "Mark V. Albert"], "title": "A Multi-Agent System for Complex Reasoning in Radiology Visual Question Answering", "comment": null, "summary": "Radiology visual question answering (RVQA) provides precise answers to\nquestions about chest X-ray images, alleviating radiologists' workload. While\nrecent methods based on multimodal large language models (MLLMs) and\nretrieval-augmented generation (RAG) have shown promising progress in RVQA,\nthey still face challenges in factual accuracy, hallucinations, and cross-modal\nmisalignment. We introduce a multi-agent system (MAS) designed to support\ncomplex reasoning in RVQA, with specialized agents for context understanding,\nmultimodal reasoning, and answer validation. We evaluate our system on a\nchallenging RVQA set curated via model disagreement filtering, comprising\nconsistently hard cases across multiple MLLMs. Extensive experiments\ndemonstrate the superiority and effectiveness of our system over strong MLLM\nbaselines, with a case study illustrating its reliability and interpretability.\nThis work highlights the potential of multi-agent approaches to support\nexplainable and trustworthy clinical AI applications that require complex\nreasoning.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u4ee3\u7406\u7cfb\u7edf\uff08MAS\uff09\u7528\u4e8e\u653e\u5c04\u5b66\u89c6\u89c9\u95ee\u7b54\uff08RVQA\uff09\uff0c\u901a\u8fc7\u4e13\u4e1a\u4ee3\u7406\u89e3\u51b3\u4e8b\u5b9e\u51c6\u786e\u6027\u3001\u5e7b\u89c9\u548c\u8de8\u6a21\u6001\u5bf9\u9f50\u95ee\u9898\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u51cf\u8f7b\u653e\u5c04\u79d1\u533b\u751f\u7684\u5de5\u4f5c\u8d1f\u62c5\uff0c\u5e76\u89e3\u51b3\u73b0\u6709\u57fa\u4e8e\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u548c\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u65b9\u6cd5\u5728RVQA\u4e2d\u7684\u4e8b\u5b9e\u51c6\u786e\u6027\u3001\u5e7b\u89c9\u548c\u8de8\u6a21\u6001\u5bf9\u9f50\u95ee\u9898\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u591a\u4ee3\u7406\u7cfb\u7edf\uff08MAS\uff09\uff0c\u5305\u542b\u4e0a\u4e0b\u6587\u7406\u89e3\u3001\u591a\u6a21\u6001\u63a8\u7406\u548c\u7b54\u6848\u9a8c\u8bc1\u7684\u4e13\u4e1a\u4ee3\u7406\uff0c\u5e76\u901a\u8fc7\u6a21\u578b\u5206\u6b67\u8fc7\u6ee4\u7b5b\u9009\u7684\u6311\u6218\u6027RVQA\u6570\u636e\u96c6\u8fdb\u884c\u8bc4\u4f30\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u7cfb\u7edf\u5728\u6311\u6218\u6027RVQA\u6570\u636e\u96c6\u4e0a\u4f18\u4e8e\u73b0\u6709MLLM\u57fa\u7ebf\uff0c\u5177\u6709\u53ef\u9760\u6027\u548c\u53ef\u89e3\u91ca\u6027\u3002", "conclusion": "\u591a\u4ee3\u7406\u65b9\u6cd5\u5728\u9700\u8981\u590d\u6742\u63a8\u7406\u7684\u53ef\u89e3\u91ca\u548c\u53ef\u4fe1\u8d56\u4e34\u5e8aAI\u5e94\u7528\u4e2d\u5177\u6709\u6f5c\u529b\u3002"}}
{"id": "2508.03130", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2508.03130", "abs": "https://arxiv.org/abs/2508.03130", "authors": ["Rama Carl Hoetzlein"], "title": "Protecting Small Organizations from AI Bots with Logrip: Hierarchical IP Hashing", "comment": "11 pages, 4 figures", "summary": "Small organizations, start ups, and self-hosted servers face increasing\nstrain from automated web crawlers and AI bots, whose online presence has\nincreased dramatically in the past few years. Modern bots evade traditional\nthrottling and can degrade server performance through sheer volume even when\nthey are well-behaved. We introduce a novel security approach that leverages\ndata visualization and hierarchical IP hashing to analyze server event logs,\ndistinguishing human users from automated entities based on access patterns. By\naggregating IP activity across subnet classes and applying statistical\nmeasures, our method detects coordinated bot activity and distributed crawling\nattacks that conventional tools fail to identify. Using a real world example we\nestimate that 80 to 95 percent of traffic originates from AI crawlers,\nunderscoring the need for improved filtering mechanisms. Our approach enables\nsmall organizations to regulate automated traffic effectively, preserving\npublic access while mitigating performance degradation.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6570\u636e\u53ef\u89c6\u5316\u548c\u5206\u5c42IP\u54c8\u5e0c\u7684\u65b0\u65b9\u6cd5\uff0c\u7528\u4e8e\u533a\u5206\u4eba\u7c7b\u7528\u6237\u4e0e\u81ea\u52a8\u5316\u722c\u866b\uff0c\u5e2e\u52a9\u5c0f\u578b\u7ec4\u7ec7\u6709\u6548\u7ba1\u7406\u6d41\u91cf\u3002", "motivation": "\u5c0f\u578b\u7ec4\u7ec7\u548c\u81ea\u6258\u7ba1\u670d\u52a1\u5668\u9762\u4e34\u81ea\u52a8\u5316\u722c\u866b\u548cAI\u673a\u5668\u4eba\u7684\u6d41\u91cf\u538b\u529b\uff0c\u4f20\u7edf\u9650\u6d41\u65b9\u6cd5\u96be\u4ee5\u5e94\u5bf9\u3002", "method": "\u5229\u7528\u6570\u636e\u53ef\u89c6\u5316\u548c\u5206\u5c42IP\u54c8\u5e0c\u5206\u6790\u670d\u52a1\u5668\u65e5\u5fd7\uff0c\u901a\u8fc7\u5b50\u7f51\u805a\u5408\u548c\u7edf\u8ba1\u65b9\u6cd5\u68c0\u6d4b\u534f\u540c\u673a\u5668\u4eba\u6d3b\u52a8\u3002", "result": "\u5b9e\u9a8c\u663e\u793a80-95%\u7684\u6d41\u91cf\u6765\u81eaAI\u722c\u866b\uff0c\u65b0\u65b9\u6cd5\u80fd\u6709\u6548\u8bc6\u522b\u4f20\u7edf\u5de5\u5177\u65e0\u6cd5\u68c0\u6d4b\u7684\u5206\u5e03\u5f0f\u722c\u866b\u653b\u51fb\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5e2e\u52a9\u5c0f\u578b\u7ec4\u7ec7\u5728\u4e0d\u5f71\u54cd\u516c\u5f00\u8bbf\u95ee\u7684\u60c5\u51b5\u4e0b\uff0c\u6709\u6548\u7f13\u89e3\u6027\u80fd\u4e0b\u964d\u95ee\u9898\u3002"}}
{"id": "2508.02968", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.02968", "abs": "https://arxiv.org/abs/2508.02968", "authors": ["Shavindra Wickramathilaka", "John Grundy", "Kashumi Madampe", "Omar Haggag"], "title": "Developer Perceptions on Utilising Low-Code Approaches to Build Accessible and Adaptive Applications for Seniors", "comment": "This paper has been submitted to ACM Transactions on Software\n  Engineering and Methodology (TOSEM)", "summary": "The global ageing population presents a growing societal challenge, creating\nan urgent need for inclusive technologies that promote autonomy among older\nadults. Software practitioners can address this by delivering digital services\nthat enhance seniors' independence and reduce reliance on routine support from\nfamily members and healthcare infrastructure. However, traditional development\npractices, constrained by time and resources, often result in applications with\nmajor accessibility and personalisation barriers. Increasing pressure from\nregulatory requirements, such as the European Accessibility Act (EAA), and the\npersonal empathy many developers feel toward supporting their older loved ones\nand their own future selves have created a demand for tools that support the\ndevelopment of accessible and adaptive software. To address this demand, this\npaper presents an interview-based empirical study with 18 software\npractitioners, evaluating AdaptForge: a low-code model-driven engineering (MDE)\ntool that enables the efficient creation of accessible and adaptive\napplications for senior users by mitigating development constraints through\nautomated code generation. Based on these insights, we identify developer\nexpectations for adopting such tools as industry-standard solutions and provide\nempirically grounded recommendations for designing low-code tools that support\naccessible and adaptive software development.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4f4e\u4ee3\u7801\u5de5\u5177AdaptForge\uff0c\u7528\u4e8e\u9ad8\u6548\u5f00\u53d1\u9762\u5411\u8001\u5e74\u7528\u6237\u7684\u6613\u8bbf\u95ee\u548c\u81ea\u9002\u5e94\u5e94\u7528\uff0c\u5e76\u901a\u8fc7\u8bbf\u8c08\u7814\u7a76\u9a8c\u8bc1\u5176\u6548\u679c\u3002", "motivation": "\u5168\u7403\u8001\u9f84\u5316\u95ee\u9898\u52a0\u5267\uff0c\u4e9f\u9700\u652f\u6301\u8001\u5e74\u4eba\u81ea\u4e3b\u6027\u7684\u6280\u672f\u3002\u4f20\u7edf\u5f00\u53d1\u65b9\u5f0f\u53d7\u9650\u4e8e\u65f6\u95f4\u548c\u8d44\u6e90\uff0c\u96be\u4ee5\u6ee1\u8db3\u6613\u8bbf\u95ee\u6027\u548c\u4e2a\u6027\u5316\u9700\u6c42\u3002", "method": "\u901a\u8fc7\u8bbf\u8c0818\u540d\u8f6f\u4ef6\u4ece\u4e1a\u8005\uff0c\u8bc4\u4f30\u4f4e\u4ee3\u7801\u6a21\u578b\u9a71\u52a8\u5de5\u7a0b\u5de5\u5177AdaptForge\u7684\u5b9e\u7528\u6027\u548c\u6548\u679c\u3002", "result": "\u7814\u7a76\u63ed\u793a\u4e86\u5f00\u53d1\u8005\u5bf9\u7c7b\u4f3c\u5de5\u5177\u7684\u671f\u671b\uff0c\u5e76\u63d0\u4f9b\u4e86\u8bbe\u8ba1\u4f4e\u4ee3\u7801\u5de5\u5177\u7684\u5efa\u8bae\u3002", "conclusion": "AdaptForge\u80fd\u6709\u6548\u7f13\u89e3\u5f00\u53d1\u9650\u5236\uff0c\u652f\u6301\u6613\u8bbf\u95ee\u548c\u81ea\u9002\u5e94\u8f6f\u4ef6\u7684\u5f00\u53d1\uff0c\u672a\u6765\u6709\u671b\u6210\u4e3a\u884c\u4e1a\u6807\u51c6\u3002"}}
{"id": "2508.02900", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.02900", "abs": "https://arxiv.org/abs/2508.02900", "authors": ["Michael Katz", "Harsha Kokel", "Sarath Sreedharan"], "title": "Seemingly Simple Planning Problems are Computationally Challenging: The Countdown Game", "comment": null, "summary": "There is a broad consensus that the inability to form long-term plans is one\nof the key limitations of current foundational models and agents. However, the\nexisting planning benchmarks remain woefully inadequate to truly measure their\nplanning capabilities. Most existing benchmarks either focus on loosely defined\ntasks like travel planning or end up leveraging existing domains and problems\nfrom international planning competitions. While the former tasks are hard to\nformalize and verify, the latter were specifically designed to test and\nchallenge the weaknesses of existing automated planners. To address these\nshortcomings, we propose a procedure for creating a planning benchmark centered\naround the game called Countdown, where a player is expected to form a target\nnumber from a list of input numbers through arithmetic operations. We discuss\nhow this problem meets many of the desiderata associated with an ideal\nbenchmark for planning capabilities evaluation. Specifically, the domain allows\nfor an intuitive, natural language description for each problem instance, it is\ncomputationally challenging (NP-complete), and the instance space is rich\nenough that we do not have to worry about memorization. We perform an extensive\ntheoretical analysis, establishing the computational complexity result and\ndemonstrate the advantage of our instance generation procedure over public\nbenchmarks. We evaluate a variety of existing LLM-assisted planning methods on\ninstances generated using our procedure. Our results show that, unlike other\ndomains like 24 Game (a special case of Countdown), our proposed dynamic\nbenchmark remains extremely challenging for existing LLM-based approaches.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6e38\u620fCountdown\u7684\u89c4\u5212\u57fa\u51c6\u6d4b\u8bd5\u65b9\u6cd5\uff0c\u5f25\u8865\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\u7684\u4e0d\u8db3\uff0c\u5e76\u901a\u8fc7\u7406\u8bba\u5206\u6790\u548c\u5b9e\u9a8c\u9a8c\u8bc1\u5176\u6709\u6548\u6027\u3002", "motivation": "\u73b0\u6709\u89c4\u5212\u57fa\u51c6\u6d4b\u8bd5\u65e0\u6cd5\u51c6\u786e\u8861\u91cf\u57fa\u7840\u6a21\u578b\u548c\u667a\u80fd\u4f53\u7684\u957f\u671f\u89c4\u5212\u80fd\u529b\uff0c\u4e9f\u9700\u4e00\u79cd\u66f4\u5408\u9002\u7684\u8bc4\u4f30\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u57fa\u4e8eCountdown\u6e38\u620f\u7684\u89c4\u5212\u57fa\u51c6\u6d4b\u8bd5\u751f\u6210\u65b9\u6cd5\uff0c\u5206\u6790\u5176\u8ba1\u7b97\u590d\u6742\u6027\uff0c\u5e76\u4e0e\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\u5bf9\u6bd4\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u52a8\u6001\u57fa\u51c6\u6d4b\u8bd5\u5bf9\u73b0\u6709LLM\u8f85\u52a9\u89c4\u5212\u65b9\u6cd5\u6781\u5177\u6311\u6218\u6027\u3002", "conclusion": "Countdown\u6e38\u620f\u662f\u4e00\u79cd\u7406\u60f3\u7684\u89c4\u5212\u80fd\u529b\u8bc4\u4f30\u57fa\u51c6\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2508.03151", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2508.03151", "abs": "https://arxiv.org/abs/2508.03151", "authors": ["Ronghua Li", "Shinan Liu", "Haibo Hu", "Qingqing Ye", "Nick Feamster"], "title": "WiFinger: Fingerprinting Noisy IoT Event Traffic Using Packet-level Sequence Matching", "comment": null, "summary": "IoT environments such as smart homes are susceptible to privacy inference\nattacks, where attackers can analyze patterns of encrypted network traffic to\ninfer the state of devices and even the activities of people. While most\nexisting attacks exploit ML techniques for discovering such traffic patterns,\nthey underperform on wireless traffic, especially Wi-Fi, due to its heavy noise\nand packet losses of wireless sniffing. In addition, these approaches commonly\ntarget at distinguishing chunked IoT event traffic samples, and they failed at\neffectively tracking multiple events simultaneously. In this work, we propose\nWiFinger, a fine-grained multi-IoT event fingerprinting approach against noisy\ntraffic. WiFinger turns the traffic pattern classification task into a\nsubsequence matching problem and introduces novel techniques to account for the\nhigh time complexity while maintaining high accuracy. Experiments demonstrate\nthat our method outperforms existing approaches on Wi-Fi traffic, achieving an\naverage recall of 85% (vs. 0.49% and 0.46%) for various IoT events while\nmaintaining almost zero false positives for most of them.", "AI": {"tldr": "WiFinger\u662f\u4e00\u79cd\u9488\u5bf9\u566a\u58f0Wi-Fi\u6d41\u91cf\u7684\u7ec6\u7c92\u5ea6\u591aIoT\u4e8b\u4ef6\u6307\u7eb9\u8bc6\u522b\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c06\u6d41\u91cf\u6a21\u5f0f\u5206\u7c7b\u4efb\u52a1\u8f6c\u5316\u4e3a\u5b50\u5e8f\u5217\u5339\u914d\u95ee\u9898\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u51c6\u786e\u6027\u548c\u6548\u7387\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u65e0\u7ebf\u6d41\u91cf\uff08\u5c24\u5176\u662fWi-Fi\uff09\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u65e0\u6cd5\u6709\u6548\u540c\u65f6\u8ddf\u8e2a\u591a\u4e2a\u4e8b\u4ef6\uff0cWiFinger\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "\u5c06\u6d41\u91cf\u6a21\u5f0f\u5206\u7c7b\u8f6c\u5316\u4e3a\u5b50\u5e8f\u5217\u5339\u914d\u95ee\u9898\uff0c\u5e76\u5f15\u5165\u65b0\u6280\u672f\u4ee5\u964d\u4f4e\u65f6\u95f4\u590d\u6742\u6027\u540c\u65f6\u4fdd\u6301\u9ad8\u51c6\u786e\u6027\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\uff0cWiFinger\u5728Wi-Fi\u6d41\u91cf\u4e2d\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5e73\u5747\u53ec\u56de\u7387\u8fbe85%\uff0c\u4e14\u51e0\u4e4e\u96f6\u8bef\u62a5\u3002", "conclusion": "WiFinger\u4e3a\u566a\u58f0\u73af\u5883\u4e0b\u7684\u591aIoT\u4e8b\u4ef6\u8ddf\u8e2a\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u51c6\u786e\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.02998", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.02998", "abs": "https://arxiv.org/abs/2508.02998", "authors": ["Haiyang Li"], "title": "MRG-Bench: Evaluating and Exploring the Requirements of Context for Repository-Level Code Generation", "comment": null, "summary": "Large Language Models (LLMs) have demonstrated impressive capabilities in\ncode generation. However, current evaluation datasets suffer from issues such\nas the lack of runnable test cases, deviation from the distribution of\nreal-world code, and the ability to evaluate only the Python language. These\nlimitations undermine the credibility of the evaluation results.\n  To address these limitations, we introduce \\textbf{MRG-Bench} (Multi-language\nRepository-level Code Generation Benchmark), a novel dataset that provides a\nmore accurate evaluation of LLMs in practical repository-level code generation\ntasks. MRG-Bench has three main features: (1) practical data sourced from\nreal-world code repositories that align to the practical distribution, (2)\nmultiple programming languages support, including Python, Java, and Go, and (3)\nproject-level runnable test cases to assess the quality of the generated code.\n  Based on MRG-Bench, we conducted extensive experiments including large\nlanguage models, long-context models, and RAG-related methods. These evaluation\nresults demonstrate that \\textbf{current repository-level code generation\ntechniques suffer from significant performance deficiencies}. To further\ninvestigate why models fail, we designed novel experiments to annotate the\nunderlying causes of generation errors. The results explicitly show that the\nmajority of methods suffer from \"\\textbf{difficulty in understanding user\nrequirements},\" failing to comprehend their assigned tasks accurately.\nMoreover, the impact of different repository-level contexts on this issue\nexhibits significant disparities across different programming languages,\nsuggesting that, in practice, specialized contextual information needs to be\ndesigned for different languages.", "AI": {"tldr": "MRG-Bench\u662f\u4e00\u4e2a\u65b0\u7684\u591a\u8bed\u8a00\u4ee3\u7801\u751f\u6210\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u8bc4\u4f30\u6570\u636e\u96c6\u7684\u5c40\u9650\u6027\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u63ed\u793a\u4e86\u5f53\u524d\u4ed3\u5e93\u7ea7\u4ee3\u7801\u751f\u6210\u6280\u672f\u7684\u6027\u80fd\u4e0d\u8db3\u548c\u4e3b\u8981\u9519\u8bef\u539f\u56e0\u3002", "motivation": "\u73b0\u6709\u4ee3\u7801\u751f\u6210\u8bc4\u4f30\u6570\u636e\u96c6\u5b58\u5728\u7f3a\u4e4f\u53ef\u8fd0\u884c\u6d4b\u8bd5\u7528\u4f8b\u3001\u504f\u79bb\u771f\u5b9e\u4ee3\u7801\u5206\u5e03\u548c\u4ec5\u652f\u6301Python\u7b49\u95ee\u9898\uff0c\u5f71\u54cd\u4e86\u8bc4\u4f30\u7ed3\u679c\u7684\u53ef\u4fe1\u5ea6\u3002", "method": "\u5f15\u5165MRG-Bench\u6570\u636e\u96c6\uff0c\u5305\u542b\u771f\u5b9e\u4ee3\u7801\u5e93\u6570\u636e\u3001\u591a\u8bed\u8a00\u652f\u6301\u548c\u9879\u76ee\u7ea7\u53ef\u8fd0\u884c\u6d4b\u8bd5\u7528\u4f8b\uff0c\u5e76\u57fa\u4e8e\u6b64\u8fdb\u884c\u5e7f\u6cdb\u7684\u5b9e\u9a8c\u548c\u9519\u8bef\u539f\u56e0\u5206\u6790\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\u5f53\u524d\u4ed3\u5e93\u7ea7\u4ee3\u7801\u751f\u6210\u6280\u672f\u6027\u80fd\u4e0d\u8db3\uff0c\u4e3b\u8981\u9519\u8bef\u539f\u56e0\u662f\u6a21\u578b\u96be\u4ee5\u51c6\u786e\u7406\u89e3\u7528\u6237\u9700\u6c42\uff0c\u4e14\u4e0d\u540c\u8bed\u8a00\u7684\u4e0a\u4e0b\u6587\u5f71\u54cd\u5dee\u5f02\u663e\u8457\u3002", "conclusion": "\u9700\u8981\u4e3a\u4e0d\u540c\u8bed\u8a00\u8bbe\u8ba1\u4e13\u95e8\u7684\u4e0a\u4e0b\u6587\u4fe1\u606f\uff0c\u4ee5\u63d0\u5347\u4ed3\u5e93\u7ea7\u4ee3\u7801\u751f\u6210\u7684\u6027\u80fd\u3002"}}
{"id": "2508.02913", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.02913", "abs": "https://arxiv.org/abs/2508.02913", "authors": ["Carolina Minami Oguchi", "Leo Wei", "Koyo Kobayashi", "Hsin-Tai Wu", "Dipak Ghosal"], "title": "Enhancing Japanese Large Language Models with Reasoning Vectors", "comment": null, "summary": "Post-training methods have improved the performance and enhanced the\nreasoning capability for mainstream large language models (LLMs), but the same\nis challenging for Japanese LLMs to achieve due to the amount of resources\nrequired. Inspired by task vectors that extract the change of weights before\nand after training, specifically for a certain task, we obtain reasoning\nvectors from reasoning LLMs and apply them to Japanese LLMs to boost their\nperformance. While the resources available present a challenge to improve\nJapanese LLMs, we present a simple and effective way to obtain high improvement\nand hope to inspire for other languages.", "AI": {"tldr": "\u901a\u8fc7\u4ece\u63a8\u7406LLMs\u4e2d\u63d0\u53d6\u63a8\u7406\u5411\u91cf\u5e76\u5e94\u7528\u4e8e\u65e5\u8bedLLMs\uff0c\u63d0\u5347\u5176\u6027\u80fd\uff0c\u89e3\u51b3\u4e86\u8d44\u6e90\u4e0d\u8db3\u7684\u95ee\u9898\u3002", "motivation": "\u7531\u4e8e\u8d44\u6e90\u9650\u5236\uff0c\u65e5\u8bedLLMs\u7684\u6027\u80fd\u63d0\u5347\u56f0\u96be\uff0c\u672c\u6587\u65e8\u5728\u901a\u8fc7\u7b80\u5355\u6709\u6548\u7684\u65b9\u6cd5\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u4ece\u63a8\u7406LLMs\u4e2d\u63d0\u53d6\u63a8\u7406\u5411\u91cf\uff08\u4efb\u52a1\u5411\u91cf\u7684\u53d8\u4f53\uff09\uff0c\u5e76\u5c06\u5176\u5e94\u7528\u4e8e\u65e5\u8bedLLMs\u3002", "result": "\u663e\u8457\u63d0\u5347\u4e86\u65e5\u8bedLLMs\u7684\u6027\u80fd\uff0c\u65b9\u6cd5\u7b80\u5355\u4e14\u6709\u6548\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u8d44\u6e90\u6709\u9650\u7684\u8bed\u8a00\u6a21\u578b\u6027\u80fd\u63d0\u5347\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\uff0c\u5bf9\u5176\u4ed6\u8bed\u8a00\u4e5f\u6709\u542f\u53d1\u610f\u4e49\u3002"}}
{"id": "2508.03221", "categories": ["cs.CR", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.03221", "abs": "https://arxiv.org/abs/2508.03221", "authors": ["Yu Pan", "Jiahao Chen", "Lin Wang", "Bingrong Dai", "Yi Du"], "title": "BadBlocks: Low-Cost and Stealthy Backdoor Attacks Tailored for Text-to-Image Diffusion Models", "comment": null, "summary": "In recent years,Diffusion models have achieved remarkable progress in the\nfield of image generation.However,recent studies have shown that diffusion\nmodels are susceptible to backdoor attacks,in which attackers can manipulate\nthe output by injecting covert triggers such as specific visual patterns or\ntextual phrases into the training dataset.Fortunately,with the continuous\nadvancement of defense techniques,defenders have become increasingly capable of\nidentifying and mitigating most backdoor attacks using visual inspection and\nneural network-based detection methods.However,in this paper,we identify a\nnovel type of backdoor threat that is more lightweight and covert than existing\napproaches,which we name BadBlocks,requires only about 30\\% of the\ncomputational resources and 20\\% GPU time typically needed by previous backdoor\nattacks,yet it successfully injects backdoors and evades the most advanced\ndefense frameworks.BadBlocks enables attackers to selectively contaminate\nspecific blocks within the UNet architecture of diffusion models while\nmaintaining normal functionality in the remaining components.Experimental\nresults demonstrate that BadBlocks achieves a high attack success rate (ASR)\nand low perceptual quality loss (as measured by FID Score),even under extremely\nconstrained computational resources and GPU time.Moreover,BadBlocks is able to\nbypass existing defense frameworks,especially the attention-based backdoor\ndetection method, highlighting it as a novel and noteworthy threat.Ablation\nstudies further demonstrate that effective backdoor injection does not require\nfine-tuning the entire network and highlight the pivotal role of certain neural\nnetwork layers in backdoor mapping.Overall,BadBlocks significantly reduces the\nbarrier to conducting backdoor attacks in all aspects.It enables attackers to\ninject backdoors into large-scale diffusion models even using consumer-grade\nGPUs.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u8f7b\u91cf\u7ea7\u3001\u9690\u853d\u7684\u540e\u95e8\u653b\u51fb\u65b9\u6cd5BadBlocks\uff0c\u9488\u5bf9\u6269\u6563\u6a21\u578b\uff0c\u4ec5\u970030%\u8ba1\u7b97\u8d44\u6e90\u548c20%GPU\u65f6\u95f4\u5373\u53ef\u6210\u529f\u6ce8\u5165\u540e\u95e8\u5e76\u7ed5\u8fc7\u73b0\u6709\u9632\u5fa1\u6846\u67b6\u3002", "motivation": "\u6269\u6563\u6a21\u578b\u5728\u56fe\u50cf\u751f\u6210\u9886\u57df\u53d6\u5f97\u663e\u8457\u8fdb\u5c55\uff0c\u4f46\u6613\u53d7\u540e\u95e8\u653b\u51fb\u3002\u73b0\u6709\u9632\u5fa1\u6280\u672f\u5df2\u80fd\u8bc6\u522b\u548c\u7f13\u89e3\u5927\u591a\u6570\u653b\u51fb\uff0c\u4f46\u7f3a\u4e4f\u5bf9\u66f4\u8f7b\u91cf\u3001\u9690\u853d\u653b\u51fb\u7684\u9632\u62a4\u3002", "method": "\u901a\u8fc7\u9009\u62e9\u6027\u6c61\u67d3\u6269\u6563\u6a21\u578bUNet\u67b6\u6784\u4e2d\u7684\u7279\u5b9a\u5757\uff0c\u4fdd\u6301\u5176\u4f59\u7ec4\u4ef6\u6b63\u5e38\u529f\u80fd\uff0cBadBlocks\u5b9e\u73b0\u4e86\u9ad8\u6548\u540e\u95e8\u6ce8\u5165\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cBadBlocks\u653b\u51fb\u6210\u529f\u7387\u9ad8\uff0c\u611f\u77e5\u8d28\u91cf\u635f\u5931\u4f4e\uff0c\u4e14\u80fd\u7ed5\u8fc7\u73b0\u6709\u9632\u5fa1\u6846\u67b6\uff0c\u5c24\u5176\u662f\u57fa\u4e8e\u6ce8\u610f\u529b\u7684\u68c0\u6d4b\u65b9\u6cd5\u3002", "conclusion": "BadBlocks\u663e\u8457\u964d\u4f4e\u4e86\u540e\u95e8\u653b\u51fb\u7684\u95e8\u69db\uff0c\u4f7f\u653b\u51fb\u8005\u80fd\u5728\u6d88\u8d39\u7ea7GPU\u4e0a\u5bf9\u5927\u89c4\u6a21\u6269\u6563\u6a21\u578b\u8fdb\u884c\u653b\u51fb\uff0c\u662f\u4e00\u79cd\u65b0\u578b\u5a01\u80c1\u3002"}}
{"id": "2508.03012", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.03012", "abs": "https://arxiv.org/abs/2508.03012", "authors": ["Zexiong Ma", "Chao Peng", "Qunhong Zeng", "Pengfei Gao", "Yanzhen Zou", "Bing Xie"], "title": "Tool-integrated Reinforcement Learning for Repo Deep Search", "comment": null, "summary": "Issue localization, the process of identifying code locations that need\nmodification to resolve software issues, is a critical yet challenging task in\nsoftware development. The semantic gap between natural language issue\ndescriptions and faulty code requires complex multi-hop reasoning through code\ndependencies. Existing LLM-based agents attempt to address this by integrating\nrepository retrieval tools. However, this transforms issue localization into a\ndemanding task we call Repo Deep Search, which requires the LLM to effectively\nutilize various repository retrieval tools throughout a multi-step reasoning\nand navigation process. To tackle this challenge, we present ToolTrain, a\ntwo-stage tool-integrated training framework combining rejection-sampled\nsupervised fine-tuning and tool-integrated reinforcement learning to enhance\nLLMs' ability to use retrieval tools for issue localization. Experimental\nresults show that ToolTrain-trained models achieve state-of-the-art\nperformance, with our 32B model even surpassing Claude-3.7 on function-level\nlocalization. The results also show that improved localization performance\ntranslates to better end-to-end issue resolution performance. This further\ndemonstrates that training for issue localization is a viable and effective\nstrategy for improving automated software development.", "AI": {"tldr": "ToolTrain\u662f\u4e00\u4e2a\u4e24\u9636\u6bb5\u7684\u5de5\u5177\u96c6\u6210\u8bad\u7ec3\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408\u62d2\u7edd\u91c7\u6837\u7684\u76d1\u7763\u5fae\u8c03\u548c\u5de5\u5177\u96c6\u6210\u7684\u5f3a\u5316\u5b66\u4e60\uff0c\u63d0\u5347LLM\u5728\u95ee\u9898\u5b9a\u4f4d\u4e2d\u4f7f\u7528\u68c0\u7d22\u5de5\u5177\u7684\u80fd\u529b\uff0c\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\u5176\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u89e3\u51b3\u8f6f\u4ef6\u95ee\u9898\u5b9a\u4f4d\u4e2d\u7684\u8bed\u4e49\u9e3f\u6c9f\u548c\u591a\u8df3\u63a8\u7406\u6311\u6218\uff0c\u73b0\u6709LLM\u4ee3\u7406\u65b9\u6cd5\u6548\u7387\u4e0d\u8db3\u3002", "method": "\u63d0\u51faToolTrain\u6846\u67b6\uff0c\u7ed3\u5408\u62d2\u7edd\u91c7\u6837\u7684\u76d1\u7763\u5fae\u8c03\u548c\u5de5\u5177\u96c6\u6210\u7684\u5f3a\u5316\u5b66\u4e60\u3002", "result": "ToolTrain\u8bad\u7ec3\u7684\u6a21\u578b\u5728\u95ee\u9898\u5b9a\u4f4d\u4e0a\u8fbe\u5230SOTA\u6027\u80fd\uff0c32B\u6a21\u578b\u751a\u81f3\u8d85\u8d8aClaude-3.7\u3002", "conclusion": "\u8bad\u7ec3\u95ee\u9898\u5b9a\u4f4d\u80fd\u529b\u662f\u63d0\u5347\u81ea\u52a8\u5316\u8f6f\u4ef6\u5f00\u53d1\u7684\u6709\u6548\u7b56\u7565\u3002"}}
{"id": "2508.02921", "categories": ["cs.AI", "cs.CR"], "pdf": "https://arxiv.org/pdf/2508.02921", "abs": "https://arxiv.org/abs/2508.02921", "authors": ["Shane Caldwell", "Max Harley", "Michael Kouremetis", "Vincent Abruzzo", "Will Pearce"], "title": "PentestJudge: Judging Agent Behavior Against Operational Requirements", "comment": "18 pages, 5 figures, 3 tables", "summary": "We introduce PentestJudge, a system for evaluating the operations of\npenetration testing agents. PentestJudge is a large language model\n(LLM)-as-judge with access to tools that allow it to consume arbitrary\ntrajectories of agent states and tool call history to determine whether a\nsecurity agent's actions meet certain operating criteria that would be\nimpractical to evaluate programmatically. We develop rubrics that use a tree\nstructure to hierarchically collapse the penetration testing task for a\nparticular environment into smaller, simpler, and more manageable sub-tasks and\ncriteria until each leaf node represents simple yes-or-no criteria for\nPentestJudge to evaluate. Task nodes are broken down into different categories\nrelated to operational objectives, operational security, and tradecraft.\nLLM-as-judge scores are compared to human domain experts as a ground-truth\nreference, allowing us to compare their relative performance with standard\nbinary classification metrics, such as F1 scores. We evaluate several frontier\nand open-source models acting as judge agents, with the best model reaching an\nF1 score of 0.83. We find models that are better at tool-use perform more\nclosely to human experts. By stratifying the F1 scores by requirement type, we\nfind even models with similar overall scores struggle with different types of\nquestions, suggesting certain models may be better judges of particular\noperating criteria. We find that weaker and cheaper models can judge the\ntrajectories of pentests performed by stronger and more expensive models,\nsuggesting verification may be easier than generation for the penetration\ntesting task. We share this methodology to facilitate future research in\nunderstanding the ability of judges to holistically and scalably evaluate the\nprocess quality of AI-based information security agents so that they may be\nconfidently used in sensitive production environments.", "AI": {"tldr": "PentestJudge\u662f\u4e00\u4e2a\u57fa\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u7cfb\u7edf\uff0c\u7528\u4e8e\u8bc4\u4f30\u6e17\u900f\u6d4b\u8bd5\u4ee3\u7406\u7684\u64cd\u4f5c\uff0c\u901a\u8fc7\u5206\u5c42\u4efb\u52a1\u5206\u89e3\u548c\u7b80\u5355\u6807\u51c6\u5b9e\u73b0\u9ad8\u6548\u8bc4\u5206\uff0c\u5e76\u4e0e\u4eba\u7c7b\u4e13\u5bb6\u8bc4\u5206\u5bf9\u6bd4\u3002", "motivation": "\u5f00\u53d1\u4e00\u79cd\u80fd\u591f\u9ad8\u6548\u8bc4\u4f30\u6e17\u900f\u6d4b\u8bd5\u4ee3\u7406\u64cd\u4f5c\u7684\u7cfb\u7edf\uff0c\u4ee5\u89e3\u51b3\u7a0b\u5e8f\u5316\u8bc4\u4f30\u7684\u5c40\u9650\u6027\u3002", "method": "\u4f7f\u7528\u6811\u72b6\u7ed3\u6784\u5c06\u6e17\u900f\u6d4b\u8bd5\u4efb\u52a1\u5206\u89e3\u4e3a\u66f4\u5c0f\u7684\u5b50\u4efb\u52a1\u548c\u6807\u51c6\uff0c\u901a\u8fc7LLM\u4f5c\u4e3a\u8bc4\u59d4\u8fdb\u884c\u8bc4\u5206\uff0c\u5e76\u4e0e\u4eba\u7c7b\u4e13\u5bb6\u8bc4\u5206\u5bf9\u6bd4\u3002", "result": "\u6700\u4f73\u6a21\u578b\u7684F1\u5206\u6570\u8fbe\u52300.83\uff0c\u5de5\u5177\u4f7f\u7528\u80fd\u529b\u5f3a\u7684\u6a21\u578b\u66f4\u63a5\u8fd1\u4eba\u7c7b\u4e13\u5bb6\u8bc4\u5206\u3002", "conclusion": "PentestJudge\u5c55\u793a\u4e86LLM\u5728\u8bc4\u4f30\u6e17\u900f\u6d4b\u8bd5\u4efb\u52a1\u4e2d\u7684\u6f5c\u529b\uff0c\u9a8c\u8bc1\u53ef\u80fd\u6bd4\u751f\u6210\u66f4\u5bb9\u6613\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u65b9\u6cd5\u8bba\u652f\u6301\u3002"}}
{"id": "2508.03307", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2508.03307", "abs": "https://arxiv.org/abs/2508.03307", "authors": ["Ye Li", "Chengcheng Zhu", "Yanchao Zhao", "Jiale Zhang"], "title": "BDFirewall: Towards Effective and Expeditiously Black-Box Backdoor Defense in MLaaS", "comment": "18 pages", "summary": "In this paper, we endeavor to address the challenges of backdoor attacks\ncountermeasures in black-box scenarios, thereby fortifying the security of\ninference under MLaaS. We first categorize backdoor triggers from a new\nperspective, i.e., their impact on the patched area, and divide them into:\nhigh-visibility triggers (HVT), semi-visibility triggers (SVT), and\nlow-visibility triggers (LVT). Based on this classification, we propose a\nprogressive defense framework, BDFirewall, that removes these triggers from the\nmost conspicuous to the most subtle, without requiring model access. First, for\nHVTs, which create the most significant local semantic distortions, we identify\nand eliminate them by detecting these salient differences. We then restore the\npatched area to mitigate the adverse impact of such removal process. The\nlocalized purification designed for HVTs is, however, ineffective against SVTs,\nwhich globally perturb benign features. We therefore model an SVT-poisoned\ninput as a mixture of a trigger and benign features, where we unconventionally\ntreat the benign features as \"noise\". This formulation allows us to reconstruct\nSVTs by applying a denoising process that removes these benign \"noise\"\nfeatures. The SVT-free input is then obtained by subtracting the reconstructed\ntrigger. Finally, to neutralize the nearly imperceptible but fragile LVTs, we\nintroduce lightweight noise to disrupt the trigger pattern and then apply DDPM\nto restore any collateral impact on clean features. Comprehensive experiments\ndemonstrate that our method outperforms state-of-the-art defenses. Compared\nwith baselines, BDFirewall reduces the Attack Success Rate (ASR) by an average\nof 33.25%, improving poisoned sample accuracy (PA) by 29.64%, and achieving up\nto a 111x speedup in inference time. Code will be made publicly available upon\nacceptance.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u9ed1\u76d2\u573a\u666f\u4e0b\u540e\u95e8\u653b\u51fb\u7684\u9632\u5fa1\u6846\u67b6BDFirewall\uff0c\u901a\u8fc7\u6e10\u8fdb\u5f0f\u65b9\u6cd5\u4ece\u9ad8\u53ef\u89c1\u6027\u5230\u4f4e\u53ef\u89c1\u6027\u89e6\u53d1\u5668\u8fdb\u884c\u6e05\u9664\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u653b\u51fb\u6210\u529f\u7387\u5e76\u63d0\u9ad8\u4e86\u5904\u7406\u901f\u5ea6\u3002", "motivation": "\u89e3\u51b3\u9ed1\u76d2\u573a\u666f\u4e0b\u540e\u95e8\u653b\u51fb\u7684\u9632\u5fa1\u96be\u9898\uff0c\u63d0\u5347MLaaS\u63a8\u7406\u5b89\u5168\u6027\u3002", "method": "\u5c06\u540e\u95e8\u89e6\u53d1\u5668\u5206\u4e3aHVT\u3001SVT\u548cLVT\u4e09\u7c7b\uff0c\u5e76\u8bbe\u8ba1BDFirewall\u6846\u67b6\u9010\u6b65\u6e05\u9664\u8fd9\u4e9b\u89e6\u53d1\u5668\uff0c\u5305\u62ec\u5c40\u90e8\u51c0\u5316\u3001\u53bb\u566a\u5904\u7406\u548c\u8f7b\u91cf\u7ea7\u566a\u58f0\u5e72\u6270\u3002", "result": "BDFirewall\u5e73\u5747\u964d\u4f4e\u653b\u51fb\u6210\u529f\u738733.25%\uff0c\u63d0\u9ad8\u4e2d\u6bd2\u6837\u672c\u51c6\u786e\u738729.64%\uff0c\u63a8\u7406\u901f\u5ea6\u63d0\u5347111\u500d\u3002", "conclusion": "BDFirewall\u662f\u4e00\u79cd\u9ad8\u6548\u4e14\u65e0\u9700\u6a21\u578b\u8bbf\u95ee\u7684\u540e\u95e8\u653b\u51fb\u9632\u5fa1\u65b9\u6cd5\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u3002"}}
{"id": "2508.03215", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.03215", "abs": "https://arxiv.org/abs/2508.03215", "authors": ["Dongming Jin", "Zhi Jin", "Linyu Li", "Zheng Fang", "Jia Li", "Xiaohong Chen"], "title": "A System Model Generation Benchmark from Natural Language Requirements", "comment": "16 pages, 14 figures", "summary": "System models, a critical artifact in software development, provide a formal\nabstraction of both the structural and behavioral aspects of software systems,\nwhich can facilitate the early requirements analysis and architecture design.\nHowever, developing system models remains challenging due to the specific\nsyntax of model description languages and the relative scarcity of public model\nexamples. While large language models (LLMs) have shown promise in generating\ncode with programming languages and could potentially aid in system model\ndevelopment, no benchmarks currently exist for evaluating their ability to\ngenerate system models with specific description languages. We present\nSysMBench, which comprises 151 human-curated scenarios spanning a wide range of\npopular domains and varying difficulty levels. Each scenario mainly comprises a\nnatural language requirements description, a system model expressed in a\nspecific model description language, and a visualized system model diagram. The\nrequirements description is fed as user input to the LLM, the system model with\ndescription language is used to verify if the generated system model conforms\nto the requirements, and the visualized diagram serves to support manual\nvalidation. We introduce SysMEval, a semantic-aware evaluation metric to\nevaluate the quality of generated system models. We evaluate 17 popular LLMs on\nthis task with three traditional metrics and SysMEval, from directly prompting\nto three commonly used enhancement strategies. Our in-depth evaluation shows\nthat LLMs perform poorly on SysMBench, with the highest BLEU of 4% and\nSysMEval-F1 of 62%. We release the SysMBench and its evaluation framework to\nenable future research on LLM-based system model generation.", "AI": {"tldr": "SysMBench\u662f\u4e00\u4e2a\u5305\u542b151\u4e2a\u573a\u666f\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7528\u4e8e\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u751f\u6210\u7cfb\u7edf\u6a21\u578b\u7684\u80fd\u529b\uff0c\u7ed3\u679c\u663e\u793aLLM\u8868\u73b0\u4e0d\u4f73\u3002", "motivation": "\u7531\u4e8e\u7cfb\u7edf\u6a21\u578b\u5f00\u53d1\u7684\u6311\u6218\u6027\uff08\u5982\u7279\u5b9a\u8bed\u6cd5\u548c\u516c\u5f00\u793a\u4f8b\u7a00\u7f3a\uff09\uff0c\u7814\u7a76LLM\u5728\u6b64\u9886\u57df\u7684\u6f5c\u529b\uff0c\u4f46\u7f3a\u4e4f\u76f8\u5173\u57fa\u51c6\u3002", "method": "\u63d0\u51faSysMBench\u57fa\u51c6\u548cSysMEval\u8bc4\u4f30\u6307\u6807\uff0c\u6d4b\u8bd517\u79cdLLM\u5728\u751f\u6210\u7cfb\u7edf\u6a21\u578b\u65f6\u7684\u8868\u73b0\u3002", "result": "LLM\u8868\u73b0\u8f83\u5dee\uff0c\u6700\u9ad8BLEU\u5f97\u5206\u4e3a4%\uff0cSysMEval-F1\u4e3a62%\u3002", "conclusion": "LLM\u5728\u7cfb\u7edf\u6a21\u578b\u751f\u6210\u4efb\u52a1\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0cSysMBench\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u5de5\u5177\u3002"}}
{"id": "2508.02936", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.02936", "abs": "https://arxiv.org/abs/2508.02936", "authors": ["Songkun Yan", "Zhi Li", "Siyu Zhu", "Yixin Wen", "Mofan Zhang", "Mengye Chen", "Jie Cao", "Yang Hong"], "title": "AQUAH: Automatic Quantification and Unified Agent in Hydrology", "comment": "8 pages, 5 figures, 2025 ICCV SEA workshop paper", "summary": "We introduce AQUAH, the first end-to-end language-based agent designed\nspecifically for hydrologic modeling. Starting from a simple natural-language\nprompt (e.g., 'simulate floods for the Little Bighorn basin from 2020 to\n2022'), AQUAH autonomously retrieves the required terrain, forcing, and gauge\ndata; configures a hydrologic model; runs the simulation; and generates a\nself-contained PDF report. The workflow is driven by vision-enabled large\nlanguage models, which interpret maps and rasters on the fly and steer key\ndecisions such as outlet selection, parameter initialization, and uncertainty\ncommentary. Initial experiments across a range of U.S. basins show that AQUAH\ncan complete cold-start simulations and produce analyst-ready documentation\nwithout manual intervention. The results are judged by hydrologists as clear,\ntransparent, and physically plausible. While further calibration and validation\nare still needed for operational deployment, these early outcomes highlight the\npromise of LLM-centered, vision-grounded agents to streamline complex\nenvironmental modeling and lower the barrier between Earth observation data,\nphysics-based tools, and decision makers.", "AI": {"tldr": "AQUAH\u662f\u4e00\u4e2a\u57fa\u4e8e\u8bed\u8a00\u7684\u7aef\u5230\u7aef\u6c34\u6587\u5efa\u6a21\u4ee3\u7406\uff0c\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u63d0\u793a\u81ea\u52a8\u5b8c\u6210\u6570\u636e\u68c0\u7d22\u3001\u6a21\u578b\u914d\u7f6e\u3001\u6a21\u62df\u8fd0\u884c\u548c\u62a5\u544a\u751f\u6210\uff0c\u521d\u6b65\u5b9e\u9a8c\u663e\u793a\u5176\u6548\u679c\u6e05\u6670\u900f\u660e\u3002", "motivation": "\u65e8\u5728\u7b80\u5316\u590d\u6742\u7684\u73af\u5883\u5efa\u6a21\u6d41\u7a0b\uff0c\u964d\u4f4e\u5730\u7403\u89c2\u6d4b\u6570\u636e\u3001\u7269\u7406\u5de5\u5177\u4e0e\u51b3\u7b56\u8005\u4e4b\u95f4\u7684\u95e8\u69db\u3002", "method": "\u5229\u7528\u89c6\u89c9\u589e\u5f3a\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u52a8\u6001\u89e3\u6790\u5730\u56fe\u548c\u6805\u683c\u6570\u636e\uff0c\u6307\u5bfc\u5173\u952e\u51b3\u7b56\u5982\u51fa\u53e3\u9009\u62e9\u548c\u53c2\u6570\u521d\u59cb\u5316\u3002", "result": "\u5728\u591a\u4e2a\u7f8e\u56fd\u6d41\u57df\u7684\u5b9e\u9a8c\u4e2d\uff0cAQUAH\u80fd\u591f\u5b8c\u6210\u51b7\u542f\u52a8\u6a21\u62df\u5e76\u751f\u6210\u5206\u6790\u5e08\u5c31\u7eea\u7684\u6587\u6863\uff0c\u7ed3\u679c\u88ab\u4e13\u5bb6\u8bc4\u4ef7\u4e3a\u6e05\u6670\u4e14\u7269\u7406\u5408\u7406\u3002", "conclusion": "\u5c3d\u7ba1\u4ecd\u9700\u8fdb\u4e00\u6b65\u6821\u51c6\u548c\u9a8c\u8bc1\uff0cAQUAH\u5c55\u793a\u4e86\u57fa\u4e8eLLM\u548c\u89c6\u89c9\u7684\u4ee3\u7406\u5728\u73af\u5883\u5efa\u6a21\u4e2d\u7684\u6f5c\u529b\u3002"}}
{"id": "2508.03342", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.03342", "abs": "https://arxiv.org/abs/2508.03342", "authors": ["Mehdi Akbari Gurabi", "Lasse Nitz", "Radu-Mihai Castravet", "Roman Matzutt", "Avikarsha Mandal", "Stefan Decker"], "title": "From Legacy to Standard: LLM-Assisted Transformation of Cybersecurity Playbooks into CACAO Format", "comment": "20 pages, including appendices, 32 references, 4 tables, 7 main\n  figures (some of them has sub-figures)", "summary": "Existing cybersecurity playbooks are often written in heterogeneous,\nnon-machine-readable formats, which limits their automation and\ninteroperability across Security Orchestration, Automation, and Response\nplatforms. This paper explores the suitability of Large Language Models,\ncombined with Prompt Engineering, to automatically translate legacy incident\nresponse playbooks into the standardized, machine-readable CACAO format. We\nsystematically examine various Prompt Engineering techniques and carefully\ndesign prompts aimed at maximizing syntactic accuracy and semantic fidelity for\ncontrol flow preservation. Our modular transformation pipeline integrates a\nsyntax checker to ensure syntactic correctness and features an iterative\nrefinement mechanism that progressively reduces syntactic errors. We evaluate\nthe proposed approach on a custom-generated dataset comprising diverse legacy\nplaybooks paired with manually created CACAO references. The results\ndemonstrate that our method significantly improves the accuracy of playbook\ntransformation over baseline models, effectively captures complex workflow\nstructures, and substantially reduces errors. It highlights the potential for\npractical deployment in automated cybersecurity playbook transformation tasks.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u548c\u63d0\u793a\u5de5\u7a0b\u5c06\u4f20\u7edf\u7f51\u7edc\u5b89\u5168\u5267\u672c\u81ea\u52a8\u8f6c\u6362\u4e3a\u6807\u51c6\u5316\u3001\u673a\u5668\u53ef\u8bfb\u7684CACAO\u683c\u5f0f\u7684\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8f6c\u6362\u51c6\u786e\u6027\u548c\u8bed\u4e49\u4fdd\u771f\u5ea6\u3002", "motivation": "\u73b0\u6709\u7f51\u7edc\u5b89\u5168\u5267\u672c\u683c\u5f0f\u5f02\u6784\u4e14\u975e\u673a\u5668\u53ef\u8bfb\uff0c\u9650\u5236\u4e86\u81ea\u52a8\u5316\u548c\u4e92\u64cd\u4f5c\u6027\u3002", "method": "\u7ed3\u5408\u5927\u8bed\u8a00\u6a21\u578b\u548c\u63d0\u793a\u5de5\u7a0b\uff0c\u8bbe\u8ba1\u6a21\u5757\u5316\u8f6c\u6362\u6d41\u7a0b\uff0c\u5305\u62ec\u8bed\u6cd5\u68c0\u67e5\u5668\u548c\u8fed\u4ee3\u4f18\u5316\u673a\u5236\u3002", "result": "\u5728\u81ea\u5b9a\u4e49\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\uff0c\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u8f6c\u6362\u51c6\u786e\u6027\uff0c\u51cf\u5c11\u9519\u8bef\uff0c\u5e76\u4fdd\u7559\u590d\u6742\u5de5\u4f5c\u6d41\u7ed3\u6784\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u81ea\u52a8\u5316\u7f51\u7edc\u5b89\u5168\u5267\u672c\u8f6c\u6362\u4efb\u52a1\u4e2d\u5177\u6709\u5b9e\u9645\u90e8\u7f72\u6f5c\u529b\u3002"}}
{"id": "2508.03258", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.03258", "abs": "https://arxiv.org/abs/2508.03258", "authors": ["Yueyue Liu", "Hongyu Zhang", "Yuantian Miao"], "title": "SmartLLMs Scheduler: A Framework for Cost-Effective LLMs Utilization", "comment": null, "summary": "Large Language Models (LLMs) such as GPT-4 and Llama have shown remarkable\ncapabilities in a variety of software engineering tasks. Despite the\nadvancements, their practical deployment faces challenges, including high\nfinancial costs, long response time, and varying performance, especially when\nhandling a large number of queries (jobs). Existing optimization strategies for\ndeploying LLMs for diverse tasks focus on static scheduling, which requires\nextensive training data for performance prediction, increasing the\ncomputational costs and limiting the applicability and flexibility. In this\npaper, we propose the SmartLLMs Scheduler (SLS), a dynamic and cost-effective\nscheduling solution. The key idea is to learn LLMs' performance on diverse\ntasks and incorporate their real-time feedback to update strategies\nperiodically. Specifically, SLS incorporates three key components, including an\nAdaptive Cache Manager, a Performance-Cost Optimized Scheduler, and a Dynamic\nUpdate Manager. The Cache Manager stores the outputs of previously processed\nqueries and employs an adaptive strategy to reduce redundant computations and\nminimize response times. For queries not found in the cache, the Scheduler\ndynamically allocates them to the most suitable LLM based on the predicted\nperformance and cost from models that take both query-specific and LLM-specific\nfeatures as input. The Update Manager continuously refines the cache and\nscheduling strategies based on real-time feedback from the assigned queries to\nenhance decision-making and adapt to evolving task characteristics. To evaluate\nthe effectiveness of SLS, we conduct extensive experiments on two LLM-based\nsoftware engineering tasks, including log parsing and code generation. The\nresults show that SLS significantly outperforms the baseline methods, achieving\nan average performance improvement of 198.82% and an average processing time\nreduction of 63.28%.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u52a8\u6001\u8c03\u5ea6\u89e3\u51b3\u65b9\u6848SmartLLMs Scheduler (SLS)\uff0c\u901a\u8fc7\u5b9e\u65f6\u53cd\u9988\u4f18\u5316LLM\u4efb\u52a1\u5206\u914d\uff0c\u663e\u8457\u63d0\u5347\u6027\u80fd\u548c\u54cd\u5e94\u901f\u5ea6\u3002", "motivation": "\u5c3d\u7ba1LLMs\u5728\u8f6f\u4ef6\u5de5\u7a0b\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5176\u5b9e\u9645\u90e8\u7f72\u9762\u4e34\u9ad8\u6210\u672c\u3001\u957f\u54cd\u5e94\u65f6\u95f4\u548c\u6027\u80fd\u6ce2\u52a8\u7b49\u6311\u6218\uff0c\u73b0\u6709\u9759\u6001\u8c03\u5ea6\u65b9\u6cd5\u4f9d\u8d56\u5927\u91cf\u8bad\u7ec3\u6570\u636e\u4e14\u7075\u6d3b\u6027\u4e0d\u8db3\u3002", "method": "SLS\u5305\u542b\u4e09\u4e2a\u6838\u5fc3\u7ec4\u4ef6\uff1a\u81ea\u9002\u5e94\u7f13\u5b58\u7ba1\u7406\u5668\u3001\u6027\u80fd-\u6210\u672c\u4f18\u5316\u8c03\u5ea6\u5668\u548c\u52a8\u6001\u66f4\u65b0\u7ba1\u7406\u5668\uff0c\u901a\u8fc7\u5b9e\u65f6\u53cd\u9988\u52a8\u6001\u8c03\u6574\u7b56\u7565\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cSLS\u5728\u65e5\u5fd7\u89e3\u6790\u548c\u4ee3\u7801\u751f\u6210\u4efb\u52a1\u4e2d\u5e73\u5747\u6027\u80fd\u63d0\u5347198.82%\uff0c\u5904\u7406\u65f6\u95f4\u51cf\u5c1163.28%\u3002", "conclusion": "SLS\u662f\u4e00\u79cd\u9ad8\u6548\u3001\u52a8\u6001\u7684LLM\u8c03\u5ea6\u65b9\u6848\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4efb\u52a1\u5904\u7406\u6548\u7387\u548c\u6027\u80fd\u3002"}}
{"id": "2508.02951", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.02951", "abs": "https://arxiv.org/abs/2508.02951", "authors": ["Mahtab Bigverdi", "Wisdom Ikezogwo", "Kevin Zhang", "Hyewon Jeong", "Mingyu Lu", "Sungjae Cho", "Linda Shapiro", "Ranjay Krishna"], "title": "MedBLINK: Probing Basic Perception in Multimodal Language Models for Medicine", "comment": null, "summary": "Multimodal language models (MLMs) show promise for clinical decision support\nand diagnostic reasoning, raising the prospect of end-to-end automated medical\nimage interpretation. However, clinicians are highly selective in adopting AI\ntools; a model that makes errors on seemingly simple perception tasks such as\ndetermining image orientation or identifying whether a CT scan is\ncontrast-enhance are unlikely to be adopted for clinical tasks. We introduce\nMedblink, a benchmark designed to probe these models for such perceptual\nabilities. Medblink spans eight clinically meaningful tasks across multiple\nimaging modalities and anatomical regions, totaling 1,429 multiple-choice\nquestions over 1,605 images. We evaluate 19 state-of-the-art MLMs, including\ngeneral purpose (GPT4o, Claude 3.5 Sonnet) and domain specific (Med Flamingo,\nLLaVA Med, RadFM) models. While human annotators achieve 96.4% accuracy, the\nbest-performing model reaches only 65%. These results show that current MLMs\nfrequently fail at routine perceptual checks, suggesting the need to strengthen\ntheir visual grounding to support clinical adoption. Data is available on our\nproject page.", "AI": {"tldr": "Medblink\u57fa\u51c6\u6d4b\u8bd5\u8bc4\u4f30\u591a\u6a21\u6001\u8bed\u8a00\u6a21\u578b\u5728\u4e34\u5e8a\u611f\u77e5\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u5176\u51c6\u786e\u6027\u8fdc\u4f4e\u4e8e\u4eba\u7c7b\uff0c\u9700\u6539\u8fdb\u89c6\u89c9\u57fa\u7840\u4ee5\u652f\u6301\u4e34\u5e8a\u5e94\u7528\u3002", "motivation": "\u4e34\u5e8a\u533b\u751f\u5bf9AI\u5de5\u5177\u7684\u9009\u62e9\u6027\u9ad8\uff0c\u6a21\u578b\u5728\u7b80\u5355\u611f\u77e5\u4efb\u52a1\u4e2d\u7684\u9519\u8bef\u4f1a\u963b\u788d\u5176\u4e34\u5e8a\u91c7\u7528\u3002", "method": "\u5f15\u5165Medblink\u57fa\u51c6\uff0c\u6db5\u76d68\u4e2a\u4e34\u5e8a\u4efb\u52a1\uff0c\u51711,429\u9053\u591a\u9009\u9898\u548c1,605\u5f20\u56fe\u50cf\uff0c\u8bc4\u4f3019\u79cd\u5148\u8fdbMLM\u3002", "result": "\u4eba\u7c7b\u51c6\u786e\u738796.4%\uff0c\u6700\u4f73\u6a21\u578b\u4ec565%\uff0c\u663e\u793aMLM\u5728\u5e38\u89c4\u611f\u77e5\u4efb\u52a1\u4e2d\u8868\u73b0\u4e0d\u8db3\u3002", "conclusion": "\u9700\u52a0\u5f3aMLM\u7684\u89c6\u89c9\u57fa\u7840\u80fd\u529b\u4ee5\u63d0\u5347\u4e34\u5e8a\u5b9e\u7528\u6027\u3002"}}
{"id": "2508.03413", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2508.03413", "abs": "https://arxiv.org/abs/2508.03413", "authors": ["Akshay Madhav Deshmukh"], "title": "Smart Car Privacy: Survey of Attacks and Privacy Issues", "comment": "13 pages, 16 figures", "summary": "Automobiles are becoming increasingly important in our day to day life.\nModern automobiles are highly computerized and hence potentially vulnerable to\nattack. Providing many wireless connectivity for vehicles enables a bridge\nbetween vehicles and their external environments. Such a connected vehicle\nsolution is expected to be the next frontier for automotive revolution and the\nkey to the evolution to next generation intelligent transportation systems.\nVehicular Ad hoc Networks (VANETs) are emerging mobile ad hoc network\ntechnologies incorporating mobile routing protocols for inter-vehicle data\ncommunications to support intelligent transportation systems. Thus security and\nprivacy are the major concerns in VANETs due to the mobility of the vehicles.\nThus designing security mechanisms to remove adversaries from the network\nremarkably important in VANETs.\n  This paper provides an overview of various vehicular network architectures.\nThe evolution of security in modern vehicles. Various security and privacy\nattacks in VANETs with their defending mechanisms with examples and classify\nthese mechanisms. It also provides an overview of various privacy implication\nthat a vehicular network possess.", "AI": {"tldr": "\u8bba\u6587\u6982\u8ff0\u4e86\u8f66\u8f7d\u7f51\u7edc\u67b6\u6784\u3001\u5b89\u5168\u6f14\u5316\u548cVANETs\u4e2d\u7684\u5b89\u5168\u9690\u79c1\u653b\u51fb\u53ca\u5176\u9632\u5fa1\u673a\u5236\u3002", "motivation": "\u968f\u7740\u6c7d\u8f66\u9ad8\u5ea6\u8ba1\u7b97\u673a\u5316\u548c\u65e0\u7ebf\u8fde\u63a5\u7684\u589e\u52a0\uff0c\u8f66\u8f7d\u7f51\u7edc\u7684\u5b89\u5168\u548c\u9690\u79c1\u95ee\u9898\u53d8\u5f97\u81f3\u5173\u91cd\u8981\u3002", "method": "\u7efc\u8ff0\u4e86\u8f66\u8f7d\u7f51\u7edc\u67b6\u6784\u3001\u5b89\u5168\u6f14\u5316\u548cVANETs\u4e2d\u7684\u653b\u51fb\u4e0e\u9632\u5fa1\u673a\u5236\uff0c\u5e76\u5206\u7c7b\u4e86\u8fd9\u4e9b\u673a\u5236\u3002", "result": "\u63d0\u4f9b\u4e86\u5bf9\u8f66\u8f7d\u7f51\u7edc\u5b89\u5168\u9690\u79c1\u95ee\u9898\u7684\u5168\u9762\u5206\u6790\uff0c\u5e76\u5c55\u793a\u4e86\u9632\u5fa1\u673a\u5236\u7684\u4f8b\u5b50\u3002", "conclusion": "\u8bbe\u8ba1\u548c\u5b9e\u65bd\u6709\u6548\u7684\u5b89\u5168\u673a\u5236\u5bf9VANETs\u81f3\u5173\u91cd\u8981\uff0c\u4ee5\u5e94\u5bf9\u79fb\u52a8\u8f66\u8f86\u5e26\u6765\u7684\u6311\u6218\u3002"}}
{"id": "2508.03298", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.03298", "abs": "https://arxiv.org/abs/2508.03298", "authors": ["Kristian Kolthoff", "Felix Kretzer", "Christian Bartelt", "Alexander Maedche", "Simone Paolo Ponzetto"], "title": "GUI-ReRank: Enhancing GUI Retrieval with Multi-Modal LLM-based Reranking", "comment": null, "summary": "GUI prototyping is a fundamental component in the development of modern\ninteractive systems, which are now ubiquitous across diverse application\ndomains. GUI prototypes play a critical role in requirements elicitation by\nenabling stakeholders to visualize, assess, and refine system concepts\ncollaboratively. Moreover, prototypes serve as effective tools for early\ntesting, iterative evaluation, and validation of design ideas with both end\nusers and development teams. Despite these advantages, the process of\nconstructing GUI prototypes remains resource-intensive and time-consuming,\nfrequently demanding substantial effort and expertise. Recent research has\nsought to alleviate this burden through NL-based GUI retrieval approaches,\nwhich typically rely on embedding-based retrieval or tailored ranking models\nfor specific GUI repositories. However, these methods often suffer from limited\nretrieval performance and struggle to generalize across arbitrary GUI datasets.\nIn this work, we present GUI-ReRank, a novel framework that integrates rapid\nembedding-based constrained retrieval models with highly effective MLLM-based\nreranking techniques. GUI-ReRank further introduces a fully customizable GUI\nrepository annotation and embedding pipeline, enabling users to effortlessly\nmake their own GUI repositories searchable, which allows for rapid discovery of\nrelevant GUIs for inspiration or seamless integration into customized LLM-based\nRAG workflows. We evaluated our approach on an established NL-based GUI\nretrieval benchmark, demonstrating that GUI-ReRank significantly outperforms\nSOTA tailored LTR models in both retrieval accuracy and generalizability.\nAdditionally, we conducted a comprehensive cost and efficiency analysis of\nemploying MLLMs for reranking, providing valuable insights regarding the\ntrade-offs between retrieval effectiveness and computational resources. Video:\nhttps://youtu.be/_7x9UCh82ug", "AI": {"tldr": "GUI-ReRank\u6846\u67b6\u7ed3\u5408\u5d4c\u5165\u68c0\u7d22\u4e0eMLLM\u91cd\u6392\u6280\u672f\uff0c\u663e\u8457\u63d0\u5347GUI\u68c0\u7d22\u6027\u80fd\u4e0e\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709NL-based GUI\u68c0\u7d22\u65b9\u6cd5\u6027\u80fd\u6709\u9650\u4e14\u96be\u4ee5\u6cdb\u5316\u7684\u95ee\u9898\u3002", "method": "\u96c6\u6210\u5feb\u901f\u5d4c\u5165\u68c0\u7d22\u4e0eMLLM\u91cd\u6392\u6280\u672f\uff0c\u5e76\u63d0\u4f9b\u53ef\u5b9a\u5236\u5316\u7684GUI\u5e93\u6807\u6ce8\u6d41\u7a0b\u3002", "result": "\u5728\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u663e\u8457\u4f18\u4e8e\u73b0\u6709LTR\u6a21\u578b\uff0c\u4e14\u6210\u672c\u6548\u7387\u5206\u6790\u63d0\u4f9b\u4e86\u5b9e\u7528\u6743\u8861\u5efa\u8bae\u3002", "conclusion": "GUI-ReRank\u4e3aGUI\u68c0\u7d22\u63d0\u4f9b\u4e86\u9ad8\u6548\u3001\u901a\u7528\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5e76\u652f\u6301\u5b9a\u5236\u5316\u9700\u6c42\u3002"}}
{"id": "2508.02959", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.02959", "abs": "https://arxiv.org/abs/2508.02959", "authors": ["Chia-Tung Ho", "Jing Gong", "Xufeng Yao", "Yunsheng Bai", "Abhishek B Akkur", "Haoxing Ren"], "title": "Polymath: A Self-Optimizing Agent with Dynamic Hierarchical Workflow", "comment": "18 pages, 12 figures, under review for AAAI2026", "summary": "Large language models (LLMs) excel at solving complex tasks by executing\nagentic workflows composed of detailed instructions and structured operations.\nYet, building general-purpose agents by manually embedding foundation models\ninto agentic systems such as Chain-of-Thought, Self-Reflection, and ReACT\nthrough text interfaces limits scalability and efficiency. Recently, many\nresearchers have sought to automate the generation and optimization of these\nworkflows through code-based representations. However, existing methods often\nrely on labeled datasets to train and optimize workflows, making them\nineffective and inflexible for solving real-world, dynamic problems where\nlabeled data is unavailable. To address this challenge, we introduce Polymath,\na self-optimizing agent with dynamic hierarchical workflow that leverages the\nflexibility of task flow graphs and the expressiveness of code-represented\nworkflows to solve a wide range of real-world, dynamic problems. The proposed\noptimization methodology integrates multi-grid-inspired graph optimization with\na self-reflection-guided evolutionary algorithm to refine workflows without\nlabeled data. Experimental results on six benchmark datasets across coding,\nmath, and multi-turn QA tasks show that Polymath achieves 8.1% average\nimprovement over state-of-the-art baselines.", "AI": {"tldr": "Polymath\u662f\u4e00\u79cd\u81ea\u4f18\u5316\u4ee3\u7406\uff0c\u901a\u8fc7\u52a8\u6001\u5206\u5c42\u5de5\u4f5c\u6d41\u548c\u4ee3\u7801\u8868\u793a\u7684\u5de5\u4f5c\u6d41\u89e3\u51b3\u73b0\u5b9e\u52a8\u6001\u95ee\u9898\uff0c\u65e0\u9700\u6807\u8bb0\u6570\u636e\uff0c\u6027\u80fd\u63d0\u53478.1%\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u6807\u8bb0\u6570\u636e\u7684\u5de5\u4f5c\u6d41\u751f\u6210\u65b9\u6cd5\u5728\u52a8\u6001\u95ee\u9898\u4e2d\u6548\u679c\u6709\u9650\uff0c\u9700\u8981\u4e00\u79cd\u65e0\u9700\u6807\u8bb0\u6570\u636e\u7684\u81ea\u4f18\u5316\u65b9\u6cd5\u3002", "method": "\u7ed3\u5408\u4efb\u52a1\u6d41\u56fe\u7684\u7075\u6d3b\u6027\u548c\u4ee3\u7801\u8868\u793a\u7684\u5de5\u4f5c\u6d41\uff0c\u91c7\u7528\u591a\u7f51\u683c\u56fe\u4f18\u5316\u548c\u81ea\u53cd\u5c04\u8fdb\u5316\u7b97\u6cd5\u4f18\u5316\u5de5\u4f5c\u6d41\u3002", "result": "\u5728\u516d\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u5e73\u5747\u6027\u80fd\u63d0\u53478.1%\u3002", "conclusion": "Polymath\u5728\u65e0\u9700\u6807\u8bb0\u6570\u636e\u7684\u60c5\u51b5\u4e0b\u663e\u8457\u63d0\u5347\u4e86\u52a8\u6001\u95ee\u9898\u7684\u89e3\u51b3\u80fd\u529b\u3002"}}
{"id": "2508.03474", "categories": ["cs.CR", "q-fin.TR"], "pdf": "https://arxiv.org/pdf/2508.03474", "abs": "https://arxiv.org/abs/2508.03474", "authors": ["Oriol Saguillo", "Vahid Ghafouri", "Lucianna Kiffer", "Guillermo Suarez-Tangil"], "title": "Unravelling the Probabilistic Forest: Arbitrage in Prediction Markets", "comment": null, "summary": "Polymarket is a prediction market platform where users can speculate on\nfuture events by trading shares tied to specific outcomes, known as conditions.\nEach market is associated with a set of one or more such conditions. To ensure\nproper market resolution, the condition set must be exhaustive -- collectively\naccounting for all possible outcomes -- and mutually exclusive -- only one\ncondition may resolve as true. Thus, the collective prices of all related\noutcomes should be \\$1, representing a combined probability of 1 of any\noutcome. Despite this design, Polymarket exhibits cases where dependent assets\nare mispriced, allowing for purchasing (or selling) a certain outcome for less\nthan (or more than) \\$1, guaranteeing profit. This phenomenon, known as\narbitrage, could enable sophisticated participants to exploit such\ninconsistencies.\n  In this paper, we conduct an empirical arbitrage analysis on Polymarket data\nto answer three key questions: (Q1) What conditions give rise to arbitrage (Q2)\nDoes arbitrage actually occur on Polymarket and (Q3) Has anyone exploited these\nopportunities. A major challenge in analyzing arbitrage between related markets\nlies in the scalability of comparisons across a large number of markets and\nconditions, with a naive analysis requiring $O(2^{n+m})$ comparisons. To\novercome this, we employ a heuristic-driven reduction strategy based on\ntimeliness, topical similarity, and combinatorial relationships, further\nvalidated by expert input.\n  Our study reveals two distinct forms of arbitrage on Polymarket: Market\nRebalancing Arbitrage, which occurs within a single market or condition, and\nCombinatorial Arbitrage, which spans across multiple markets. We use on-chain\nhistorical order book data to analyze when these types of arbitrage\nopportunities have existed, and when they have been executed by users. We find\na realized estimate of 40 million USD of profit extracted.", "AI": {"tldr": "Polymarket\u662f\u4e00\u4e2a\u9884\u6d4b\u5e02\u573a\u5e73\u53f0\uff0c\u7528\u6237\u53ef\u4ee5\u901a\u8fc7\u4ea4\u6613\u4e0e\u7279\u5b9a\u7ed3\u679c\u76f8\u5173\u7684\u80a1\u7968\u6765\u6295\u673a\u672a\u6765\u4e8b\u4ef6\u3002\u672c\u6587\u901a\u8fc7\u5b9e\u8bc1\u5206\u6790\u7814\u7a76\u4e86\u5e73\u53f0\u4e0a\u7684\u5957\u5229\u73b0\u8c61\uff0c\u63ed\u793a\u4e86\u4e24\u79cd\u5957\u5229\u5f62\u5f0f\uff0c\u5e76\u4f30\u8ba1\u4e864000\u4e07\u7f8e\u5143\u7684\u5957\u5229\u5229\u6da6\u3002", "motivation": "\u7814\u7a76Polymarket\u4e0a\u5957\u5229\u73b0\u8c61\u7684\u4ea7\u751f\u6761\u4ef6\u3001\u5b9e\u9645\u53d1\u751f\u60c5\u51b5\u4ee5\u53ca\u662f\u5426\u88ab\u5229\u7528\uff0c\u4ee5\u63ed\u793a\u5e02\u573a\u8bbe\u8ba1\u4e2d\u7684\u6f5c\u5728\u95ee\u9898\u3002", "method": "\u91c7\u7528\u542f\u53d1\u5f0f\u9a71\u52a8\u7684\u7b80\u5316\u7b56\u7565\uff0c\u7ed3\u5408\u65f6\u95f4\u6027\u3001\u4e3b\u9898\u76f8\u4f3c\u6027\u548c\u7ec4\u5408\u5173\u7cfb\uff0c\u5206\u6790\u94fe\u4e0a\u5386\u53f2\u8ba2\u5355\u6570\u636e\u3002", "result": "\u53d1\u73b0\u4e24\u79cd\u5957\u5229\u5f62\u5f0f\uff1a\u5e02\u573a\u518d\u5e73\u8861\u5957\u5229\u548c\u7ec4\u5408\u5957\u5229\uff0c\u5e76\u4f30\u8ba1\u7528\u6237\u4ece\u4e2d\u83b7\u52294000\u4e07\u7f8e\u5143\u3002", "conclusion": "Polymarket\u5b58\u5728\u5957\u5229\u673a\u4f1a\uff0c\u8868\u660e\u5e02\u573a\u8bbe\u8ba1\u5b58\u5728\u4e0d\u4e00\u81f4\u6027\uff0c\u53ef\u80fd\u88ab\u9ad8\u7ea7\u53c2\u4e0e\u8005\u5229\u7528\u3002"}}
{"id": "2508.03329", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.03329", "abs": "https://arxiv.org/abs/2508.03329", "authors": ["Mari Ashiga", "Vardan Voskanyan", "Fateme Dinmohammadi", "Jingzhi Gong", "Paul Brookes", "Matthew Truscott", "Rafail Giavrimis", "Mike Basios", "Leslie Kanthan", "Wei Jie"], "title": "Industrial LLM-based Code Optimization under Regulation: A Mixture-of-Agents Approach", "comment": "Submitted to ASE'25 Industry Showcase", "summary": "Recent advancements in Large Language Models (LLMs) for code optimization\nhave enabled industrial platforms to automate software performance engineering\nat unprecedented scale and speed. Yet, organizations in regulated industries\nface strict constraints on which LLMs they can use - many cannot utilize\ncommercial models due to data privacy regulations and compliance requirements,\ncreating a significant challenge for achieving high-quality code optimization\nwhile maintaining cost-effectiveness. We address this by implementing a\nMixture-of-Agents (MoA) approach that directly synthesizes code from multiple\nspecialized LLMs, comparing it against TurinTech AI's vanilla Genetic Algorithm\n(GA)-based ensemble system and individual LLM optimizers using real-world\nindustrial codebases. Our key contributions include: (1) First MoA application\nto industrial code optimization using real-world codebases; (2) Empirical\nevidence that MoA excels with open-source models, achieving 14.3% to 22.2% cost\nsavings and 28.6% to 32.2% faster optimization times for regulated\nenvironments; (3) Deployment guidelines demonstrating GA's advantage with\ncommercial models while both ensembles outperform individual LLMs; and (4)\nReal-world validation across 50 code snippets and seven LLM combinations,\ngenerating over 8,700 variants, addresses gaps in industrial LLM ensemble\nevaluation. This provides actionable guidance for organizations balancing\nregulatory compliance with optimization performance in production environments.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6df7\u5408\u4ee3\u7406\uff08MoA\uff09\u65b9\u6cd5\uff0c\u7528\u4e8e\u5728\u53d7\u76d1\u7ba1\u884c\u4e1a\u4e2d\u5b9e\u73b0\u9ad8\u6548\u4ee3\u7801\u4f18\u5316\uff0c\u901a\u8fc7\u7ed3\u5408\u591a\u4e2a\u5f00\u6e90LLM\uff0c\u663e\u8457\u8282\u7701\u6210\u672c\u5e76\u63d0\u5347\u4f18\u5316\u901f\u5ea6\u3002", "motivation": "\u53d7\u76d1\u7ba1\u884c\u4e1a\u56e0\u6570\u636e\u9690\u79c1\u548c\u5408\u89c4\u8981\u6c42\u65e0\u6cd5\u4f7f\u7528\u5546\u4e1aLLM\uff0c\u5bfc\u81f4\u4ee3\u7801\u4f18\u5316\u9762\u4e34\u6311\u6218\u3002", "method": "\u91c7\u7528MoA\u65b9\u6cd5\uff0c\u7ed3\u5408\u591a\u4e2a\u4e13\u7528LLM\u751f\u6210\u4ee3\u7801\uff0c\u5e76\u4e0e\u9057\u4f20\u7b97\u6cd5\uff08GA\uff09\u548c\u5355\u4e2aLLM\u8fdb\u884c\u5bf9\u6bd4\u3002", "result": "MoA\u5728\u5f00\u6e90\u6a21\u578b\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u8282\u770114.3%-22.2%\u6210\u672c\uff0c\u4f18\u5316\u901f\u5ea6\u5feb28.6%-32.2%\u3002GA\u5728\u5546\u4e1a\u6a21\u578b\u4e2d\u66f4\u4f18\uff0c\u4f46\u4e24\u79cd\u96c6\u6210\u65b9\u6cd5\u5747\u4f18\u4e8e\u5355\u4e2aLLM\u3002", "conclusion": "MoA\u4e3a\u53d7\u76d1\u7ba1\u884c\u4e1a\u63d0\u4f9b\u4e86\u5e73\u8861\u5408\u89c4\u6027\u4e0e\u4f18\u5316\u6027\u80fd\u7684\u53ef\u884c\u65b9\u6848\uff0c\u5e76\u901a\u8fc7\u5b9e\u9645\u9a8c\u8bc1\u8bc1\u660e\u4e86\u5176\u6709\u6548\u6027\u3002"}}
{"id": "2508.02961", "categories": ["cs.AI", "cs.CL", "cs.CR"], "pdf": "https://arxiv.org/pdf/2508.02961", "abs": "https://arxiv.org/abs/2508.02961", "authors": ["Boshi Huang", "Fabio Nonato de Paula"], "title": "Defend LLMs Through Self-Consciousness", "comment": "Presented at KDD Workshop on Ethical Artificial Intelligence: Methods\n  and Applications (EAI) 2025", "summary": "This paper introduces a novel self-consciousness defense mechanism for Large\nLanguage Models (LLMs) to combat prompt injection attacks. Unlike traditional\napproaches that rely on external classifiers, our method leverages the LLM's\ninherent reasoning capabilities to perform self-protection. We propose a\nframework that incorporates Meta-Cognitive and Arbitration Modules, enabling\nLLMs to evaluate and regulate their own outputs autonomously. Our approach is\nevaluated on seven state-of-the-art LLMs using two datasets: AdvBench and\nPrompt-Injection-Mixed-Techniques-2024. Experiment results demonstrate\nsignificant improvements in defense success rates across models and datasets,\nwith some achieving perfect and near-perfect defense in Enhanced Mode. We also\nanalyze the trade-off between defense success rate improvement and\ncomputational overhead. This self-consciousness method offers a lightweight,\ncost-effective solution for enhancing LLM ethics, particularly beneficial for\nGenAI use cases across various platforms.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u7684\u81ea\u610f\u8bc6\u9632\u5fa1\u673a\u5236\uff0c\u5229\u7528LLM\u7684\u63a8\u7406\u80fd\u529b\u81ea\u4e3b\u5bf9\u6297\u63d0\u793a\u6ce8\u5165\u653b\u51fb\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u9632\u5fa1\u6210\u529f\u7387\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u4f9d\u8d56\u5916\u90e8\u5206\u7c7b\u5668\uff0c\u800c\u672c\u65b9\u6cd5\u5229\u7528LLM\u7684\u5185\u5728\u80fd\u529b\uff0c\u63d0\u4f9b\u8f7b\u91cf\u7ea7\u3001\u7ecf\u6d4e\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u901a\u8fc7\u5f15\u5165\u5143\u8ba4\u77e5\u548c\u4ef2\u88c1\u6a21\u5757\uff0c\u4f7fLLM\u80fd\u591f\u81ea\u4e3b\u8bc4\u4f30\u548c\u8c03\u8282\u8f93\u51fa\u3002", "result": "\u5728\u4e03\u4e2a\u5148\u8fdbLLM\u4e0a\u6d4b\u8bd5\uff0c\u9632\u5fa1\u6210\u529f\u7387\u663e\u8457\u63d0\u5347\uff0c\u90e8\u5206\u6a21\u578b\u5728\u589e\u5f3a\u6a21\u5f0f\u4e0b\u5b9e\u73b0\u5b8c\u7f8e\u9632\u5fa1\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u63d0\u5347LLM\u4f26\u7406\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u4f4e\u6210\u672c\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u9002\u7528\u4e8e\u591a\u79cdGenAI\u5e94\u7528\u573a\u666f\u3002"}}
{"id": "2508.03517", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2508.03517", "abs": "https://arxiv.org/abs/2508.03517", "authors": ["Mabin Umman Varghese", "Zahra Taghiyarrenani"], "title": "Intrusion Detection in Heterogeneous Networks with Domain-Adaptive Multi-Modal Learning", "comment": null, "summary": "Network Intrusion Detection Systems (NIDS) play a crucial role in\nsafeguarding network infrastructure against cyberattacks. As the prevalence and\nsophistication of these attacks increase, machine learning and deep neural\nnetwork approaches have emerged as effective tools for enhancing NIDS\ncapabilities in detecting malicious activities. However, the effectiveness of\ntraditional deep neural models is often limited by the need for extensive\nlabelled datasets and the challenges posed by data and feature heterogeneity\nacross different network domains. To address these limitations, we developed a\ndeep neural model that integrates multi-modal learning with domain adaptation\ntechniques for classification. Our model processes data from diverse sources in\na sequential cyclic manner, allowing it to learn from multiple datasets and\nadapt to varying feature spaces. Experimental results demonstrate that our\nproposed model significantly outperforms baseline neural models in classifying\nnetwork intrusions, particularly under conditions of varying sample\navailability and probability distributions. The model's performance highlights\nits ability to generalize across heterogeneous datasets, making it an efficient\nsolution for real-world network intrusion detection.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u591a\u6a21\u6001\u5b66\u4e60\u548c\u9886\u57df\u9002\u5e94\u6280\u672f\u7684\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\uff0c\u7528\u4e8e\u63d0\u5347\u7f51\u7edc\u5165\u4fb5\u68c0\u6d4b\u7cfb\u7edf\u7684\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\u5728\u68c0\u6d4b\u7f51\u7edc\u5165\u4fb5\u65f6\u53d7\u9650\u4e8e\u9700\u8981\u5927\u91cf\u6807\u8bb0\u6570\u636e\u548c\u8de8\u7f51\u7edc\u9886\u57df\u7684\u6570\u636e\u4e0e\u7279\u5f81\u5f02\u8d28\u6027\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u79cd\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\uff0c\u901a\u8fc7\u987a\u5e8f\u5faa\u73af\u65b9\u5f0f\u5904\u7406\u591a\u6e90\u6570\u636e\uff0c\u7ed3\u5408\u591a\u6a21\u6001\u5b66\u4e60\u548c\u9886\u57df\u9002\u5e94\u6280\u672f\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u6a21\u578b\u5728\u6837\u672c\u53ef\u7528\u6027\u548c\u6982\u7387\u5206\u5e03\u53d8\u5316\u7684\u6761\u4ef6\u4e0b\uff0c\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\u3002", "conclusion": "\u8be5\u6a21\u578b\u80fd\u591f\u6cdb\u5316\u5230\u5f02\u6784\u6570\u636e\u96c6\uff0c\u662f\u73b0\u5b9e\u7f51\u7edc\u5165\u4fb5\u68c0\u6d4b\u7684\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.03340", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.03340", "abs": "https://arxiv.org/abs/2508.03340", "authors": ["Alex Wolf", "Marco Edoardo Palma", "Pooja Rani", "Harald C. Gall"], "title": "Key-Augmented Neural Triggers for Knowledge Sharing", "comment": null, "summary": "Repository-level code comprehension and knowledge sharing remain core\nchallenges in software engineering. Large language models (LLMs) have shown\npromise by generating explanations of program structure and logic. However,\nthese approaches still face limitations: First, relevant knowledge is\ndistributed across multiple files within a repository, aka semantic\nfragmentation. Second, retrieval inefficiency and attention saturation degrade\nperformance in RAG pipelines, where long, unaligned contexts overwhelm\nattention. Third, repository specific training data is scarce and often\noutdated. Finally, proprietary LLMs hinder industrial adoption due to privacy\nand deployment constraints. To address these issues, we propose Key-Augmented\nNeural Triggers (KANT), a novel approach that embeds knowledge anchors into\nboth training and inference. Unlike prior methods, KANT enables internal access\nto repository specific knowledge, reducing fragmentation and grounding\ninference in localized context. Moreover, we synthesize specialized data\ndirectly from code. At inference, knowledge anchors replace verbose context,\nreducing token overhead and latency while supporting efficient, on premise\ndeployment. We evaluate KANT via: a qualitative human evaluation of the\nsynthesized dataset's intent coverage and quality across five dimensions;\ncompare against SOTA baselines across five qualitative dimensions and inference\nspeed; and replication across different LLMs to assess generalizability.\nResults show that the synthetic training data aligned with information-seeking\nneeds. KANT achieved over 60% preference from human annotators and a LocalStack\nexpert (preferring 79% of cases). Also, KANT reduced inference latency by up to\n85% across all models. Overall, it is well-suited for scalable, low-latency,\non-premise deployments, providing a strong foundation for code comprehension.", "AI": {"tldr": "KANT\u662f\u4e00\u79cd\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u5d4c\u5165\u77e5\u8bc6\u951a\u70b9\u89e3\u51b3\u4ee3\u7801\u5e93\u7ea7\u7406\u89e3\u4e2d\u7684\u8bed\u4e49\u788e\u7247\u5316\u548c\u68c0\u7d22\u6548\u7387\u95ee\u9898\uff0c\u663e\u8457\u964d\u4f4e\u5ef6\u8fdf\u5e76\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u4ee3\u7801\u5e93\u7ea7\u7406\u89e3\u4e2d\u7684\u8bed\u4e49\u788e\u7247\u5316\u3001\u68c0\u7d22\u6548\u7387\u4f4e\u3001\u8bad\u7ec3\u6570\u636e\u7a00\u7f3a\u4ee5\u53ca\u79c1\u6709LLM\u7684\u5de5\u4e1a\u5e94\u7528\u9650\u5236\u3002", "method": "\u63d0\u51faKey-Augmented Neural Triggers (KANT)\uff0c\u5d4c\u5165\u77e5\u8bc6\u951a\u70b9\u5230\u8bad\u7ec3\u548c\u63a8\u7406\u4e2d\uff0c\u5408\u6210\u4e13\u7528\u6570\u636e\u5e76\u51cf\u5c11\u4e0a\u4e0b\u6587\u5f00\u9500\u3002", "result": "\u5408\u6210\u6570\u636e\u4e0e\u9700\u6c42\u5bf9\u9f50\uff0cKANT\u83b7\u5f9760%\u4ee5\u4e0a\u4eba\u7c7b\u504f\u597d\uff0c\u5ef6\u8fdf\u964d\u4f4e85%\uff0c\u9002\u5408\u4f4e\u5ef6\u8fdf\u672c\u5730\u90e8\u7f72\u3002", "conclusion": "KANT\u4e3a\u4ee3\u7801\u7406\u89e3\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u3001\u4f4e\u5ef6\u8fdf\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u9002\u7528\u4e8e\u5de5\u4e1a\u573a\u666f\u3002"}}
{"id": "2508.02979", "categories": ["cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.02979", "abs": "https://arxiv.org/abs/2508.02979", "authors": ["Peng Ding", "Rick Stevens"], "title": "Unified Tool Integration for LLMs: A Protocol-Agnostic Approach to Function Calling", "comment": "arXiv admin note: substantial text overlap with arXiv:2507.10593", "summary": "The proliferation of tool-augmented Large Language Models (LLMs) has created\na fragmented ecosystem where developers must navigate multiple protocols,\nmanual schema definitions, and complex execution workflows. We address this\nchallenge by proposing a unified approach to tool integration that abstracts\nprotocol differences while optimizing execution performance. Our solution\ndemonstrates how protocol-agnostic design principles can significantly reduce\ndevelopment overhead through automated schema generation, dual-mode concurrent\nexecution, and seamless multi-source tool management. Experimental results show\n60-80% code reduction across integration scenarios, performance improvements up\nto 3.1x through optimized concurrency, and full compatibility with existing\nfunction calling standards. This work contributes both theoretical insights\ninto tool integration architecture and practical solutions for real-world LLM\napplication development.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7edf\u4e00\u5de5\u5177\u96c6\u6210\u65b9\u6cd5\uff0c\u901a\u8fc7\u534f\u8bae\u65e0\u5173\u8bbe\u8ba1\u51cf\u5c11\u5f00\u53d1\u8d1f\u62c5\uff0c\u5b9e\u9a8c\u663e\u793a\u4ee3\u7801\u51cf\u5c1160-80%\uff0c\u6027\u80fd\u63d0\u53473.1\u500d\u3002", "motivation": "\u89e3\u51b3\u5de5\u5177\u589e\u5f3a\u578b\u5927\u8bed\u8a00\u6a21\u578b\u751f\u6001\u788e\u7247\u5316\u95ee\u9898\uff0c\u7b80\u5316\u591a\u534f\u8bae\u3001\u590d\u6742\u5de5\u4f5c\u6d41\u7684\u5f00\u53d1\u6311\u6218\u3002", "method": "\u91c7\u7528\u534f\u8bae\u65e0\u5173\u8bbe\u8ba1\u539f\u5219\uff0c\u81ea\u52a8\u5316\u751f\u6210\u6a21\u5f0f\uff0c\u53cc\u6a21\u5f0f\u5e76\u53d1\u6267\u884c\uff0c\u591a\u6e90\u5de5\u5177\u65e0\u7f1d\u7ba1\u7406\u3002", "result": "\u4ee3\u7801\u51cf\u5c1160-80%\uff0c\u6027\u80fd\u63d0\u5347\u8fbe3.1\u500d\uff0c\u5b8c\u5168\u517c\u5bb9\u73b0\u6709\u51fd\u6570\u8c03\u7528\u6807\u51c6\u3002", "conclusion": "\u4e3a\u5de5\u5177\u96c6\u6210\u67b6\u6784\u63d0\u4f9b\u7406\u8bba\u89c1\u89e3\uff0c\u5e76\u4e3a\u5b9e\u9645LLM\u5e94\u7528\u5f00\u53d1\u63d0\u4f9b\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.03588", "categories": ["cs.CR", "cs.SE"], "pdf": "https://arxiv.org/pdf/2508.03588", "abs": "https://arxiv.org/abs/2508.03588", "authors": ["Zhaoyi Meng", "Fenglei Xu", "Wenxiang Zhao", "Wansen Wang", "Wenchao Huang", "Jie Cui", "Hong Zhong", "Yan Xiong"], "title": "MalFlows: Context-aware Fusion of Heterogeneous Flow Semantics for Android Malware Detection", "comment": "Submitted to TDSC", "summary": "Static analysis, a fundamental technique in Android app examination, enables\nthe extraction of control flows, data flows, and inter-component communications\n(ICCs), all of which are essential for malware detection. However, existing\nmethods struggle to leverage the semantic complementarity across different\ntypes of flows for representing program behaviors, and their context-unaware\nnature further hinders the accuracy of cross-flow semantic integration. We\npropose and implement MalFlows, a novel technique that achieves context-aware\nfusion of heterogeneous flow semantics for Android malware detection. Our goal\nis to leverage complementary strengths of the three types of flow-related\ninformation for precise app profiling. We adopt a heterogeneous information\nnetwork (HIN) to model the rich semantics across these program flows. We\nfurther propose flow2vec, a context-aware HIN embedding technique that\ndistinguishes the semantics of HIN entities as needed based on contextual\nconstraints across different flows and learns accurate app representations\nthrough the joint use of multiple meta-paths. The representations are finally\nfed into a channel-attention-based deep neural network for malware\nclassification. To the best of our knowledge, this is the first study to\ncomprehensively aggregate the strengths of diverse flow-related information for\nassessing maliciousness within apps. We evaluate MalFlows on a large-scale\ndataset comprising over 20 million flow instances extracted from more than\n31,000 real-world apps. Experimental results demonstrate that MalFlows\noutperforms representative baselines in Android malware detection, and\nmeanwhile, validate the effectiveness of flow2vec in accurately learning app\nrepresentations from the HIN constructed over the heterogeneous flows.", "AI": {"tldr": "MalFlows\u662f\u4e00\u79cd\u65b0\u578b\u7684Android\u6076\u610f\u8f6f\u4ef6\u68c0\u6d4b\u6280\u672f\uff0c\u901a\u8fc7\u4e0a\u4e0b\u6587\u611f\u77e5\u878d\u5408\u5f02\u6784\u6d41\u8bed\u4e49\uff0c\u5229\u7528\u5f02\u6784\u4fe1\u606f\u7f51\u7edc\uff08HIN\uff09\u5efa\u6a21\u7a0b\u5e8f\u6d41\u8bed\u4e49\uff0c\u5e76\u7ed3\u5408flow2vec\u5d4c\u5165\u6280\u672f\u548c\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u5b9e\u73b0\u9ad8\u6548\u5206\u7c7b\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u5229\u7528\u4e0d\u540c\u7c7b\u578b\u6d41\u4e4b\u95f4\u7684\u8bed\u4e49\u4e92\u8865\u6027\uff0c\u4e14\u7f3a\u4e4f\u4e0a\u4e0b\u6587\u611f\u77e5\u80fd\u529b\uff0c\u5bfc\u81f4\u8de8\u6d41\u8bed\u4e49\u6574\u5408\u51c6\u786e\u6027\u4e0d\u8db3\u3002", "method": "\u91c7\u7528HIN\u5efa\u6a21\u7a0b\u5e8f\u6d41\u8bed\u4e49\uff0c\u63d0\u51faflow2vec\u5d4c\u5165\u6280\u672f\uff0c\u7ed3\u5408\u591a\u8def\u5f84\u5b66\u4e60\u548c\u901a\u9053\u6ce8\u610f\u529b\u673a\u5236\u8fdb\u884c\u5206\u7c7b\u3002", "result": "\u5728\u5305\u542b31,000\u591a\u4e2a\u771f\u5b9e\u5e94\u7528\u7684\u5927\u89c4\u6a21\u6570\u636e\u96c6\u4e0a\uff0cMalFlows\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5\uff0c\u9a8c\u8bc1\u4e86flow2vec\u7684\u6709\u6548\u6027\u3002", "conclusion": "MalFlows\u9996\u6b21\u5168\u9762\u6574\u5408\u591a\u6837\u6d41\u4fe1\u606f\uff0c\u663e\u8457\u63d0\u5347\u4e86Android\u6076\u610f\u8f6f\u4ef6\u68c0\u6d4b\u7684\u51c6\u786e\u6027\u3002"}}
{"id": "2508.03369", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.03369", "abs": "https://arxiv.org/abs/2508.03369", "authors": ["Beatriz Santana", "Lidiv\u00e2nio Monte", "Bianca Santana de Ara\u00fajo Silva", "Glauco Carneiro", "S\u00e1vio Freire", "Jos\u00e9 Amancio Macedo Santos", "Manoel Mendon\u00e7a"], "title": "Psychological safety in software workplaces: A systematic literature review", "comment": null, "summary": "Context: Psychological safety (PS) is an important factor influencing team\nwell-being and performance, particularly in collaborative and dynamic domains\nsuch as software development. Despite its acknowledged significance, research\non PS within the field of software engineering remains limited. The\nsocio-technical complexities and fast-paced nature of software development\npresent challenges to cultivating PS. To the best of our knowledge, no\nsystematic secondary study has synthesized existing knowledge on PS in the\ncontext of software engineering.\n  Objective: This study aims to systematically review and synthesize the\nexisting body of knowledge on PS in software engineering. Specifically, it\nseeks to identify the potential antecedents and consequences associated with\nthe presence or absence of PS among individuals involved in the software\ndevelopment process.\n  Methods: A systematic literature review was conducted, encompassing studies\nretrieved from four digital libraries. The extracted data were subjected to\nboth quantitative and qualitative analyses.\n  Results: The findings indicate a growing academic interest in PS within\nsoftware engineering, with the majority of studies grounded in Edmondson's\nframework. Factors antecedents of PS were identified at the individual, team,\nand organizational levels, including team autonomy, agile methodologies, and\nleadership behaviors.\n  Conclusion: PS fosters innovation, learning, and team performance within\nsoftware development. However, significant gaps persist in understanding the\ncontextual factors influencing PS, its underlying mechanisms, and effective\nstrategies for its enhancement. Future research should address these gaps by\ninvestigating the practical applications of PS within diverse organizational\nsettings in the software engineering domain.", "AI": {"tldr": "\u672c\u6587\u7cfb\u7edf\u7efc\u8ff0\u4e86\u8f6f\u4ef6\u5de5\u7a0b\u4e2d\u5fc3\u7406\u5b89\u5168\uff08PS\uff09\u7684\u7814\u7a76\u73b0\u72b6\uff0c\u8bc6\u522b\u4e86\u5176\u524d\u56e0\u540e\u679c\uff0c\u5e76\u6307\u51fa\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "motivation": "\u5fc3\u7406\u5b89\u5168\u5bf9\u56e2\u961f\u798f\u7949\u548c\u7ee9\u6548\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u8f6f\u4ef6\u5de5\u7a0b\u9886\u57df\u7684\u7814\u7a76\u6709\u9650\uff0c\u4e9f\u9700\u7cfb\u7edf\u7efc\u8ff0\u3002", "method": "\u901a\u8fc7\u7cfb\u7edf\u6587\u732e\u7efc\u8ff0\uff0c\u5bf9\u56db\u4e2a\u6570\u5b57\u56fe\u4e66\u9986\u7684\u7814\u7a76\u8fdb\u884c\u5b9a\u91cf\u548c\u5b9a\u6027\u5206\u6790\u3002", "result": "\u7814\u7a76\u53d1\u73b0PS\u5728\u8f6f\u4ef6\u5de5\u7a0b\u4e2d\u65e5\u76ca\u53d7\u5173\u6ce8\uff0c\u524d\u56e0\u5305\u62ec\u56e2\u961f\u81ea\u4e3b\u6743\u3001\u654f\u6377\u65b9\u6cd5\u548c\u9886\u5bfc\u884c\u4e3a\u3002", "conclusion": "PS\u4fc3\u8fdb\u521b\u65b0\u548c\u5b66\u4e60\uff0c\u4f46\u5bf9\u5176\u5f71\u54cd\u56e0\u7d20\u548c\u63d0\u5347\u7b56\u7565\u7684\u7406\u89e3\u4ecd\u4e0d\u8db3\uff0c\u672a\u6765\u9700\u8fdb\u4e00\u6b65\u7814\u7a76\u3002"}}
{"id": "2508.02994", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.02994", "abs": "https://arxiv.org/abs/2508.02994", "authors": ["Fangyi Yu"], "title": "When AIs Judge AIs: The Rise of Agent-as-a-Judge Evaluation for LLMs", "comment": null, "summary": "As large language models (LLMs) grow in capability and autonomy, evaluating\ntheir outputs-especially in open-ended and complex tasks-has become a critical\nbottleneck. A new paradigm is emerging: using AI agents as the evaluators\nthemselves. This \"agent-as-a-judge\" approach leverages the reasoning and\nperspective-taking abilities of LLMs to assess the quality and safety of other\nmodels, promising calable and nuanced alternatives to human evaluation. In this\nreview, we define the agent-as-a-judge concept, trace its evolution from\nsingle-model judges to dynamic multi-agent debate frameworks, and critically\nexamine their strengths and shortcomings. We compare these approaches across\nreliability, cost, and human alignment, and survey real-world deployments in\ndomains such as medicine, law, finance, and education. Finally, we highlight\npressing challenges-including bias, robustness, and meta evaluation-and outline\nfuture research directions. By bringing together these strands, our review\ndemonstrates how agent-based judging can complement (but not replace) human\noversight, marking a step toward trustworthy, scalable evaluation for\nnext-generation LLMs.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u4f7f\u7528AI\u4ee3\u7406\u4f5c\u4e3a\u8bc4\u4f30\u8005\uff08\"agent-as-a-judge\"\uff09\u7684\u65b0\u8303\u5f0f\uff0c\u4ee5\u89e3\u51b3\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u8f93\u51fa\u8bc4\u4f30\u7684\u74f6\u9888\u95ee\u9898\uff0c\u5e76\u5206\u6790\u4e86\u5176\u4f18\u7f3a\u70b9\u53ca\u5b9e\u9645\u5e94\u7528\u3002", "motivation": "\u968f\u7740LLMs\u80fd\u529b\u548c\u81ea\u4e3b\u6027\u7684\u63d0\u5347\uff0c\u5176\u8f93\u51fa\u7684\u8bc4\u4f30\u6210\u4e3a\u5173\u952e\u74f6\u9888\uff0c\u9700\u8981\u53ef\u6269\u5c55\u4e14\u7ec6\u81f4\u7684\u66ff\u4ee3\u65b9\u6848\u3002", "method": "\u5b9a\u4e49\u4e86\"agent-as-a-judge\"\u6982\u5ff5\uff0c\u8ffd\u6eaf\u4e86\u4ece\u5355\u6a21\u578b\u8bc4\u4f30\u5230\u52a8\u6001\u591a\u4ee3\u7406\u8fa9\u8bba\u6846\u67b6\u7684\u6f14\u53d8\uff0c\u5e76\u6bd4\u8f83\u4e86\u53ef\u9760\u6027\u3001\u6210\u672c\u548c\u4eba\u7c7b\u5bf9\u9f50\u7b49\u65b9\u9762\u3002", "result": "\u5c55\u793a\u4e86\u5728\u533b\u7597\u3001\u6cd5\u5f8b\u3001\u91d1\u878d\u548c\u6559\u80b2\u7b49\u9886\u57df\u7684\u5b9e\u9645\u5e94\u7528\uff0c\u540c\u65f6\u6307\u51fa\u4e86\u504f\u89c1\u3001\u9c81\u68d2\u6027\u548c\u5143\u8bc4\u4f30\u7b49\u6311\u6218\u3002", "conclusion": "\u57fa\u4e8e\u4ee3\u7406\u7684\u8bc4\u4f30\u53ef\u4ee5\u8865\u5145\uff08\u800c\u975e\u66ff\u4ee3\uff09\u4eba\u7c7b\u76d1\u7763\uff0c\u4e3a\u4e0b\u4e00\u4ee3LLMs\u7684\u53ef\u4fe1\u3001\u53ef\u6269\u5c55\u8bc4\u4f30\u8fc8\u51fa\u4e00\u6b65\u3002"}}
{"id": "2508.03393", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.03393", "abs": "https://arxiv.org/abs/2508.03393", "authors": ["Muhammad Zohaib", "Muhammad Azeem Akbar", "Sami Hyrynsalmi", "Arif Ali Khan"], "title": "Agentic AI in 6G Software Businesses: A Layered Maturity Model", "comment": "6 pages, 3 figures and FIT'25 Conference", "summary": "The emergence of agentic AI systems in 6G software businesses presents both\nstrategic opportunities and significant challenges. While such systems promise\nincreased autonomy, scalability, and intelligent decision-making across\ndistributed environments, their adoption raises concerns regarding technical\nimmaturity, integration complexity, organizational readiness, and\nperformance-cost trade-offs. In this study, we conducted a preliminary thematic\nmapping to identify factors influencing the adoption of agentic software within\nthe context of 6G. Drawing on a multivocal literature review and targeted\nscanning, we identified 29 motivators and 27 demotivators, which were further\ncategorized into five high-level themes in each group. This thematic mapping\noffers a structured overview of the enabling and inhibiting forces shaping\norganizational readiness for agentic transformation. Positioned as a\nfeasibility assessment, the study represents an early phase of a broader\nresearch initiative aimed at developing and validating a layered maturity model\ngrounded in CMMI model with the software architectural three dimensions\npossibly Data, Business Logic, and Presentation. Ultimately, this work seeks to\nprovide a practical framework to help software-driven organizations assess,\nstructure, and advance their agent-first capabilities in alignment with the\ndemands of 6G.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e866G\u8f6f\u4ef6\u4e1a\u52a1\u4e2d\u4ee3\u7406AI\u7cfb\u7edf\u7684\u6218\u7565\u673a\u9047\u4e0e\u6311\u6218\uff0c\u901a\u8fc7\u4e3b\u9898\u6620\u5c04\u8bc6\u522b\u4e8629\u4e2a\u4fc3\u8fdb\u56e0\u7d20\u548c27\u4e2a\u6291\u5236\u56e0\u7d20\uff0c\u65e8\u5728\u4e3a\u7ec4\u7ec7\u63d0\u4f9b\u8bc4\u4f30\u548c\u63d0\u5347\u4ee3\u7406\u80fd\u529b\u7684\u6846\u67b6\u3002", "motivation": "\u4ee3\u7406AI\u7cfb\u7edf\u57286G\u73af\u5883\u4e2d\u5177\u6709\u6f5c\u529b\uff0c\u4f46\u5176\u91c7\u7528\u9762\u4e34\u6280\u672f\u4e0d\u6210\u719f\u3001\u96c6\u6210\u590d\u6742\u6027\u7b49\u95ee\u9898\uff0c\u9700\u8981\u7cfb\u7edf\u6027\u8bc4\u4f30\u3002", "method": "\u91c7\u7528\u591a\u58f0\u6587\u732e\u7efc\u8ff0\u548c\u5b9a\u5411\u626b\u63cf\uff0c\u8fdb\u884c\u4e3b\u9898\u6620\u5c04\uff0c\u8bc6\u522b\u5e76\u5206\u7c7b\u5f71\u54cd\u56e0\u7d20\u3002", "result": "\u8bc6\u522b\u4e8629\u4e2a\u4fc3\u8fdb\u56e0\u7d20\u548c27\u4e2a\u6291\u5236\u56e0\u7d20\uff0c\u5206\u4e3a\u4e94\u7c7b\u4e3b\u9898\uff0c\u4e3a\u7ec4\u7ec7\u63d0\u4f9b\u4e86\u7ed3\u6784\u5316\u89c6\u89d2\u3002", "conclusion": "\u7814\u7a76\u4e3a6G\u65f6\u4ee3\u7684\u4ee3\u7406AI\u7cfb\u7edf\u91c7\u7528\u63d0\u4f9b\u4e86\u53ef\u884c\u6027\u8bc4\u4f30\u6846\u67b6\uff0c\u652f\u6301\u7ec4\u7ec7\u80fd\u529b\u63d0\u5347\u3002"}}
{"id": "2508.02999", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2508.02999", "abs": "https://arxiv.org/abs/2508.02999", "authors": ["Xinjie Zhao", "Moritz Blum", "Fan Gao", "Yingjian Chen", "Boming Yang", "Luis Marquez-Carpintero", "M\u00f3nica Pina-Navarro", "Yanran Fu", "So Morikawa", "Yusuke Iwasawa", "Yutaka Matsuo", "Chanjun Park", "Irene Li"], "title": "AGENTiGraph: A Multi-Agent Knowledge Graph Framework for Interactive, Domain-Specific LLM Chatbots", "comment": "CIKM 2025, Demo Track", "summary": "AGENTiGraph is a user-friendly, agent-driven system that enables intuitive\ninteraction and management of domain-specific data through the manipulation of\nknowledge graphs in natural language. It gives non-technical users a complete,\nvisual solution to incrementally build and refine their knowledge bases,\nallowing multi-round dialogues and dynamic updates without specialized query\nlanguages. The flexible design of AGENTiGraph, including intent classification,\ntask planning, and automatic knowledge integration, ensures seamless reasoning\nbetween diverse tasks. Evaluated on a 3,500-query benchmark within an\neducational scenario, the system outperforms strong zero-shot baselines\n(achieving 95.12% classification accuracy, 90.45% execution success),\nindicating potential scalability to compliance-critical or multi-step queries\nin legal and medical domains, e.g., incorporating new statutes or research on\nthe fly. Our open-source demo offers a powerful new paradigm for multi-turn\nenterprise knowledge management that bridges LLMs and structured graphs.", "AI": {"tldr": "AGENTiGraph\u662f\u4e00\u4e2a\u7528\u6237\u53cb\u597d\u7684\u4ee3\u7406\u9a71\u52a8\u7cfb\u7edf\uff0c\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u64cd\u4f5c\u77e5\u8bc6\u56fe\u8c31\u5b9e\u73b0\u76f4\u89c2\u4ea4\u4e92\u548c\u9886\u57df\u6570\u636e\u7ba1\u7406\uff0c\u4e3a\u975e\u6280\u672f\u7528\u6237\u63d0\u4f9b\u53ef\u89c6\u5316\u89e3\u51b3\u65b9\u6848\u3002", "motivation": "\u4e3a\u975e\u6280\u672f\u7528\u6237\u63d0\u4f9b\u65e0\u9700\u4e13\u4e1a\u67e5\u8be2\u8bed\u8a00\u5373\u53ef\u6784\u5efa\u548c\u4f18\u5316\u77e5\u8bc6\u5e93\u7684\u5de5\u5177\uff0c\u652f\u6301\u591a\u8f6e\u5bf9\u8bdd\u548c\u52a8\u6001\u66f4\u65b0\u3002", "method": "\u7cfb\u7edf\u8bbe\u8ba1\u5305\u62ec\u610f\u56fe\u5206\u7c7b\u3001\u4efb\u52a1\u89c4\u5212\u548c\u81ea\u52a8\u77e5\u8bc6\u96c6\u6210\uff0c\u652f\u6301\u591a\u6837\u5316\u4efb\u52a1\u7684\u65e0\u7f1d\u63a8\u7406\u3002", "result": "\u57283500\u67e5\u8be2\u7684\u6559\u80b2\u573a\u666f\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u7cfb\u7edf\u8868\u73b0\u4f18\u4e8e\u96f6\u6837\u672c\u57fa\u7ebf\uff08\u5206\u7c7b\u51c6\u786e\u738795.12%\uff0c\u6267\u884c\u6210\u529f\u738790.45%\uff09\u3002", "conclusion": "AGENTiGraph\u5c55\u793a\u4e86\u5728\u6cd5\u5f8b\u548c\u533b\u7597\u7b49\u9886\u57df\u5904\u7406\u590d\u6742\u67e5\u8be2\u7684\u6f5c\u529b\uff0c\u4e3a\u591a\u8f6e\u4f01\u4e1a\u77e5\u8bc6\u7ba1\u7406\u63d0\u4f9b\u4e86\u65b0\u8303\u5f0f\u3002"}}
{"id": "2508.03435", "categories": ["cs.SE", "cs.PL"], "pdf": "https://arxiv.org/pdf/2508.03435", "abs": "https://arxiv.org/abs/2508.03435", "authors": ["Thomas S. Heinze", "Andr\u00e9 Sch\u00e4fer", "Wolfram Amme"], "title": "StoneDetector: Conventional and versatile code clone detection for Java", "comment": "supplementary information available at\n  https://stonedetector.fmi.uni-jena.de/", "summary": "Copy & paste is a widespread practice when developing software and, thus,\nduplicated and subsequently modified code occurs frequently in software\nprojects. Since such code clones, i.e., identical or similar fragments of code,\ncan bloat software projects and cause issues like bug or vulnerability\npropagation, their identification is of importance. In this paper, we present\nthe StoneDetector platform and its underlying method for finding code clones in\nJava source and Bytecode. StoneDetector implements a conventional clone\ndetection approach based upon the textual comparison of paths derived from the\ncode's representation by dominator trees. In this way, the tool does not only\nfind exact and syntactically similar near-miss code clones, but also code\nclones that are harder to detect due to their larger variety in the syntax. We\ndemonstrate StoneDetector's versatility as a conventional clone detection\nplatform and analyze its various available configuration parameters, including\nthe usage of different string metrics, hashing algorithms, etc. In our\nexhaustive evaluation with other conventional clone detectors on several\nstate-of-the-art benchmarks, we can show StoneDetector's performance and\nscalability in finding code clones in both, Java source and Bytecode.", "AI": {"tldr": "StoneDetector\u5e73\u53f0\u7528\u4e8e\u68c0\u6d4bJava\u6e90\u4ee3\u7801\u548c\u5b57\u8282\u7801\u4e2d\u7684\u4ee3\u7801\u514b\u9686\uff0c\u57fa\u4e8e\u652f\u914d\u6811\u8def\u5f84\u7684\u6587\u672c\u6bd4\u8f83\u65b9\u6cd5\uff0c\u652f\u6301\u591a\u79cd\u914d\u7f6e\u53c2\u6570\uff0c\u6027\u80fd\u4f18\u8d8a\u3002", "motivation": "\u4ee3\u7801\u514b\u9686\u5728\u8f6f\u4ef6\u5f00\u53d1\u4e2d\u666e\u904d\u5b58\u5728\uff0c\u53ef\u80fd\u5bfc\u81f4\u9879\u76ee\u81a8\u80c0\u548c\u6f0f\u6d1e\u4f20\u64ad\uff0c\u56e0\u6b64\u9700\u8981\u9ad8\u6548\u68c0\u6d4b\u5de5\u5177\u3002", "method": "\u57fa\u4e8e\u652f\u914d\u6811\u8def\u5f84\u7684\u6587\u672c\u6bd4\u8f83\u65b9\u6cd5\uff0c\u652f\u6301\u591a\u79cd\u5b57\u7b26\u4e32\u5ea6\u91cf\u548c\u54c8\u5e0c\u7b97\u6cd5\u914d\u7f6e\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u80fd\u591f\u68c0\u6d4b\u8bed\u6cd5\u5dee\u5f02\u8f83\u5927\u7684\u4ee3\u7801\u514b\u9686\u3002", "conclusion": "StoneDetector\u662f\u4e00\u4e2a\u9ad8\u6548\u3001\u53ef\u6269\u5c55\u7684\u4ee3\u7801\u514b\u9686\u68c0\u6d4b\u5e73\u53f0\uff0c\u9002\u7528\u4e8eJava\u6e90\u4ee3\u7801\u548c\u5b57\u8282\u7801\u3002"}}
{"id": "2508.03018", "categories": ["cs.AI", "cs.RO"], "pdf": "https://arxiv.org/pdf/2508.03018", "abs": "https://arxiv.org/abs/2508.03018", "authors": ["Yutong Wang", "Pengliang Ji", "Kaixin Li", "Baolong Bi", "Tao Feng", "Guillaume Sartoretti"], "title": "Beyond Policy Optimization: A Data Curation Flywheel for Sparse-Reward Long-Horizon Planning", "comment": null, "summary": "Large Language Reasoning Models have demonstrated remarkable success on\nstatic tasks, yet their application to multi-round agentic planning in\ninteractive environments faces two fundamental challenges. First, the\nintractable credit assignment problem renders conventional reinforcement\nlearning ineffective in sparse-reward settings. Second, the computational\noverhead of verbose, step-by-step reasoning histories is prohibitive. To\naddress these challenges, we propose BPO, a three-stage framework\n(bootstrapping, extrapolation, and refinement) that establishes a\nself-improving data flywheel to develop robust reasoning models for\nlong-horizon, sparse-reward environments. Our framework first bootstraps\nefficient reasoning using the proposed planning quaternions with long-short\nchain-of-thought fusion. It then extrapolates to out-of-distribution tasks\nthrough complexity-stratified curriculum learning. Finally, the model\niteratively refines itself by learning exclusively on experiences selected via\nreward-gated rejection sampling. Experiments on ALFWorld, ScienceWorld, and\nWebShop demonstrate that our approach achieves state-of-the-art with\nsignificant token efficiency, providing a new recipe for reasoning models in\nagentic planning.", "AI": {"tldr": "BPO\u6846\u67b6\u901a\u8fc7\u4e09\u9636\u6bb5\uff08\u5f15\u5bfc\u3001\u5916\u63a8\u548c\u4f18\u5316\uff09\u89e3\u51b3\u5927\u578b\u8bed\u8a00\u63a8\u7406\u6a21\u578b\u5728\u591a\u8f6e\u4ee3\u7406\u89c4\u5212\u4e2d\u7684\u6311\u6218\uff0c\u663e\u8457\u63d0\u5347\u7a00\u758f\u5956\u52b1\u73af\u5883\u4e0b\u7684\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u591a\u8f6e\u4ee3\u7406\u89c4\u5212\u4e2d\u7a00\u758f\u5956\u52b1\u548c\u8ba1\u7b97\u5f00\u9500\u5927\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51faBPO\u6846\u67b6\uff0c\u5305\u62ec\u5f15\u5bfc\uff08\u89c4\u5212\u56db\u5143\u7ec4\u548c\u957f\u77ed\u94fe\u601d\u7ef4\u878d\u5408\uff09\u3001\u5916\u63a8\uff08\u590d\u6742\u5ea6\u5206\u5c42\u8bfe\u7a0b\u5b66\u4e60\uff09\u548c\u4f18\u5316\uff08\u5956\u52b1\u95e8\u63a7\u62d2\u7edd\u91c7\u6837\uff09\u3002", "result": "\u5728ALFWorld\u3001ScienceWorld\u548cWebShop\u4e0a\u5b9e\u73b0\u6700\u4f18\u6027\u80fd\uff0c\u4e14\u5177\u6709\u9ad8\u6548\u6027\u3002", "conclusion": "BPO\u4e3a\u4ee3\u7406\u89c4\u5212\u4e2d\u7684\u63a8\u7406\u6a21\u578b\u63d0\u4f9b\u4e86\u65b0\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u6548\u7387\u548c\u6027\u80fd\u3002"}}
{"id": "2508.03091", "categories": ["cs.AI", "cs.CR", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.03091", "abs": "https://arxiv.org/abs/2508.03091", "authors": ["Xingjun Ma", "Hanxun Huang", "Tianwei Song", "Ye Sun", "Yifeng Gao", "Yu-Gang Jiang"], "title": "T2UE: Generating Unlearnable Examples from Text Descriptions", "comment": "To appear in ACM MM 2025", "summary": "Large-scale pre-training frameworks like CLIP have revolutionized multimodal\nlearning, but their reliance on web-scraped datasets, frequently containing\nprivate user data, raises serious concerns about misuse. Unlearnable Examples\n(UEs) have emerged as a promising countermeasure against unauthorized model\ntraining, employing carefully crafted unlearnable noise to disrupt the learning\nof meaningful representations from protected data. Current approaches typically\ngenerate UEs by jointly optimizing unlearnable noise for both images and their\nassociated text descriptions (or labels). However, this optimization process is\noften computationally prohibitive for on-device execution, forcing reliance on\nexternal third-party services. This creates a fundamental privacy paradox:\nusers must initially expose their data to these very services to achieve\nprotection, thereby compromising privacy in the process. Such a contradiction\nhas severely hindered the development of practical, scalable data protection\nsolutions. To resolve this paradox, we introduce \\textbf{Text-to-Unlearnable\nExample (T2UE)}, a novel framework that enables users to generate UEs using\nonly text descriptions. T2UE circumvents the need for original image data by\nemploying a text-to-image (T2I) model to map text descriptions into the image\n(noise) space, combined with an error-minimization framework to produce\neffective unlearnable noise. Extensive experiments show that T2UE-protected\ndata substantially degrades performance in downstream tasks (e.g., cross-modal\nretrieval) for state-of-the-art models. Notably, the protective effect\ngeneralizes across diverse architectures and even to supervised learning\nsettings. Our work demonstrates the feasibility of \"zero-contact data\nprotection\", where personal data can be safeguarded based solely on their\ntextual descriptions, eliminating the need for direct data exposure.", "AI": {"tldr": "T2UE\u6846\u67b6\u901a\u8fc7\u4ec5\u4f7f\u7528\u6587\u672c\u63cf\u8ff0\u751f\u6210\u4e0d\u53ef\u5b66\u4e60\u793a\u4f8b\uff08UEs\uff09\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u9700\u8981\u66b4\u9732\u539f\u59cb\u6570\u636e\u7684\u9690\u79c1\u77db\u76fe\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u751f\u6210UEs\u9700\u8981\u8054\u5408\u4f18\u5316\u56fe\u50cf\u548c\u6587\u672c\uff0c\u8ba1\u7b97\u6210\u672c\u9ad8\u4e14\u4f9d\u8d56\u7b2c\u4e09\u65b9\u670d\u52a1\uff0c\u5bfc\u81f4\u9690\u79c1\u6cc4\u9732\u3002", "method": "T2UE\u5229\u7528\u6587\u672c\u5230\u56fe\u50cf\u6a21\u578b\u5c06\u6587\u672c\u6620\u5c04\u5230\u566a\u58f0\u7a7a\u95f4\uff0c\u7ed3\u5408\u8bef\u5dee\u6700\u5c0f\u5316\u6846\u67b6\u751f\u6210UEs\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cT2UE\u663e\u8457\u964d\u4f4e\u4e86\u4e0b\u6e38\u4efb\u52a1\u6027\u80fd\uff0c\u4e14\u4fdd\u62a4\u6548\u679c\u6cdb\u5316\u5230\u591a\u79cd\u67b6\u6784\u548c\u76d1\u7763\u5b66\u4e60\u3002", "conclusion": "T2UE\u5b9e\u73b0\u4e86\u201c\u96f6\u63a5\u89e6\u6570\u636e\u4fdd\u62a4\u201d\uff0c\u4ec5\u9700\u6587\u672c\u63cf\u8ff0\u5373\u53ef\u4fdd\u62a4\u6570\u636e\u9690\u79c1\u3002"}}
{"id": "2508.03470", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.03470", "abs": "https://arxiv.org/abs/2508.03470", "authors": ["Dong wang", "Junji Yu", "Honglin Shu", "Michael Fu", "Chakkrit Tantithamthavorn", "Yasutaka Kamei", "Junjie Chen"], "title": "On the Evaluation of Large Language Models in Multilingual Vulnerability Repair", "comment": null, "summary": "Various Deep Learning-based approaches with pre-trained language models have\nbeen proposed for automatically repairing software vulnerabilities. However,\nthese approaches are limited to a specific programming language (C/C++). Recent\nadvances in large language models (LLMs) offer language-agnostic capabilities\nand strong semantic understanding, exhibiting potential to overcome\nmultilingual vulnerability limitations. Although some work has begun to explore\nLLMs' repair performance, their effectiveness is unsatisfactory. To address\nthese limitations, we conducted a large-scale empirical study to investigate\nthe performance of automated vulnerability repair approaches and\nstate-of-the-art LLMs across seven programming languages. Results show GPT-4o,\ninstruction-tuned with few-shot prompting, performs competitively against the\nleading approach, VulMaster. Additionally, the LLM-based approach shows\nsuperior performance in repairing unique vulnerabilities and is more likely to\nrepair the most dangerous vulnerabilities. Instruction-tuned GPT-4o\ndemonstrates strong generalization on vulnerabilities in previously unseen\nlanguage, outperforming existing approaches. Analysis shows Go consistently\nachieves the highest effectiveness across all model types, while C/C++ performs\nthe worst. Based on findings, we discuss the promise of LLM on multilingual\nvulnerability repair and the reasons behind LLM's failed cases. This work takes\nthe first look at repair approaches and LLMs across multiple languages,\nhighlighting the promising future of adopting LLMs for multilingual\nvulnerability repair.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u591a\u8bed\u8a00\u6f0f\u6d1e\u4fee\u590d\u65b9\u6cd5\uff0c\u53d1\u73b0GPT-4o\u5728\u6307\u4ee4\u5fae\u8c03\u548c\u5c11\u6837\u672c\u63d0\u793a\u4e0b\u8868\u73b0\u4f18\u5f02\uff0c\u5c24\u5176\u5728\u8de8\u8bed\u8a00\u4fee\u590d\u548c\u5371\u9669\u6f0f\u6d1e\u4fee\u590d\u65b9\u9762\u3002", "motivation": "\u73b0\u6709\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u4ec5\u9650\u4e8e\u7279\u5b9a\u8bed\u8a00\uff08\u5982C/C++\uff09\uff0c\u800cLLM\u5177\u5907\u8bed\u8a00\u65e0\u5173\u6027\u548c\u8bed\u4e49\u7406\u89e3\u80fd\u529b\uff0c\u6709\u671b\u89e3\u51b3\u591a\u8bed\u8a00\u6f0f\u6d1e\u4fee\u590d\u7684\u5c40\u9650\u6027\u3002", "method": "\u901a\u8fc7\u5927\u89c4\u6a21\u5b9e\u8bc1\u7814\u7a76\uff0c\u6bd4\u8f83\u4e86\u81ea\u52a8\u6f0f\u6d1e\u4fee\u590d\u65b9\u6cd5\u548c\u5148\u8fdbLLM\u5728\u4e03\u79cd\u7f16\u7a0b\u8bed\u8a00\u4e2d\u7684\u8868\u73b0\uff0c\u91cd\u70b9\u5173\u6ce8GPT-4o\u7684\u6307\u4ee4\u5fae\u8c03\u6548\u679c\u3002", "result": "GPT-4o\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5VulMaster\uff0c\u5c24\u5176\u5728\u8de8\u8bed\u8a00\u4fee\u590d\u548c\u5371\u9669\u6f0f\u6d1e\u4fee\u590d\u65b9\u9762\u3002Go\u8bed\u8a00\u4fee\u590d\u6548\u679c\u6700\u4f73\uff0cC/C++\u6700\u5dee\u3002", "conclusion": "LLM\u5728\u591a\u8bed\u8a00\u6f0f\u6d1e\u4fee\u590d\u4e2d\u5c55\u73b0\u51fa\u5de8\u5927\u6f5c\u529b\uff0c\u4f46\u4ecd\u9700\u8fdb\u4e00\u6b65\u7814\u7a76\u5176\u5931\u8d25\u6848\u4f8b\u7684\u539f\u56e0\u3002"}}
{"id": "2508.03030", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.03030", "abs": "https://arxiv.org/abs/2508.03030", "authors": ["Siyuan Li", "Yifan Yu", "Yanchen Deng", "Zhihao Zhang", "Mengjing Chen", "Fangzhou Zhu", "Tao Zhong", "Jianye Hao", "Peng Liu", "Bo An"], "title": "Collab-Solver: Collaborative Solving Policy Learning for Mixed-Integer Linear Programming", "comment": null, "summary": "Mixed-integer linear programming (MILP) has been a fundamental problem in\ncombinatorial optimization. Previous works have designed a plethora of\nhard-coded heuristics to accomplish challenging MILP solving with domain\nknowledge. Driven by the high capability of neural networks, recent research is\ndevoted to replacing manually designed heuristics with learned policies.\nAlthough learning-based MILP methods have shown great promise, existing\nworksindependentlytreatthepolicylearningineachmoduleofMILPsolvers without\nconsidering their interdependence, severely hurting the solving speed and\nquality. To address this issue, we propose a novel multi-agent-based policy\nlearning framework for MILP (Collab-Solver), which can collaboratively optimize\nthe policies for multiple modules. Specifically, we formulate the collaboration\nof cut selection and branching in MILP solving as a Stackelberg game. Under\nthis formulation, we develop a two-phase learning paradigm to stabilize the\ncollaborative policy learning, where the first phase achieves the\ndata-communicated policy pretraining and the second phase further orchestrates\nthe policy learning for various modules. The jointly learned policy\nsignificantly improves the solving performance on both synthetic and\nlarge-scale real-world MILP datasets. Moreover, the policies learned by\nCollab-Solver have also demonstrated excellent generalization abilities across\ndifferent instance sets.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u591a\u667a\u80fd\u4f53\u7684\u534f\u4f5c\u5b66\u4e60\u6846\u67b6\uff08Collab-Solver\uff09\uff0c\u901a\u8fc7Stackelberg\u535a\u5f08\u5efa\u6a21MILP\u6c42\u89e3\u4e2d\u5272\u5e73\u9762\u9009\u62e9\u4e0e\u5206\u652f\u7684\u534f\u4f5c\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6c42\u89e3\u6027\u80fd\u4e0e\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u5b66\u4e60\u7684MILP\u65b9\u6cd5\u72ec\u7acb\u5904\u7406\u5404\u6a21\u5757\u7684\u7b56\u7565\u5b66\u4e60\uff0c\u5ffd\u89c6\u4e86\u6a21\u5757\u95f4\u7684\u76f8\u4e92\u4f9d\u8d56\uff0c\u5f71\u54cd\u4e86\u6c42\u89e3\u901f\u5ea6\u4e0e\u8d28\u91cf\u3002", "method": "\u91c7\u7528Stackelberg\u535a\u5f08\u5efa\u6a21\u5272\u5e73\u9762\u9009\u62e9\u4e0e\u5206\u652f\u7684\u534f\u4f5c\uff0c\u8bbe\u8ba1\u4e24\u9636\u6bb5\u5b66\u4e60\u8303\u5f0f\uff1a\u6570\u636e\u901a\u4fe1\u7684\u9884\u8bad\u7ec3\u9636\u6bb5\u4e0e\u6a21\u5757\u95f4\u7b56\u7565\u534f\u8c03\u9636\u6bb5\u3002", "result": "\u5728\u5408\u6210\u548c\u5927\u89c4\u6a21\u771f\u5b9eMILP\u6570\u636e\u96c6\u4e0a\u663e\u8457\u63d0\u5347\u6c42\u89e3\u6027\u80fd\uff0c\u4e14\u7b56\u7565\u5c55\u73b0\u51fa\u4f18\u79c0\u7684\u8de8\u5b9e\u4f8b\u96c6\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "Collab-Solver\u901a\u8fc7\u534f\u4f5c\u5b66\u4e60\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u6a21\u5757\u95f4\u7b56\u7565\u72ec\u7acb\u5b66\u4e60\u7684\u95ee\u9898\uff0c\u4e3aMILP\u6c42\u89e3\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2508.03487", "categories": ["cs.SE", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.03487", "abs": "https://arxiv.org/abs/2508.03487", "authors": ["Yuanpeng Li", "Qi Long", "Zhiyuan Yao", "Jian Xu", "Lintao Xie", "Xu He", "Lu Geng", "Xin Han", "Yueyan Chen", "Wenbo Duan"], "title": "BitsAI-Fix: LLM-Driven Approach for Automated Lint Error Resolution in Practice", "comment": null, "summary": "As enterprise codebases continue to grow in scale and complexity, the volume\nof lint errors far exceeds engineers' manual remediation capacity, leading to\ncontinuous accumulation of technical debt and hindered development efficiency.\nThis paper presents BitsAI-Fix, an automated lint error remediation workflow\nbased on Large Language Models (LLMs), designed to address this critical\nchallenge in industrial-scale environments. BitsAI-Fix employs tree-sitter for\ncontext expansion and generates search-and-replace format patches through\nspecially trained LLMs, followed by lint scan re-verification to output final\nremediation results. Additionally, our approach introduces an innovative\nprogressive reinforcement learning (RL) training strategy that can\nautomatically acquire verifiable training data during the project cold-start\nphase and continuously iterate the model by collecting online samples through\nfeedback after system deployment. Furthermore, we designed a targeted\nrule-based reward mechanism that combines format rewards and correctness\nrewards while penalizing redundant modifications. We also propose a \"code diff\nmatching\" methodology to continuously track online effectiveness. In production\ndeployment at ByteDance, our solution has supported over 5,000 engineers,\nresolved more than 12,000 static analysis issues, achieved approximately 85%\nremediation accuracy, with around 1,000 weekly active adopters. This work\ndemonstrates the practical feasibility of LLM-based code remediation solutions\nin enterprise environments and serves as a reference for automated code fix in\nlarge-scale industrial scenarios.", "AI": {"tldr": "BitsAI-Fix\u662f\u4e00\u79cd\u57fa\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u81ea\u52a8\u5316lint\u9519\u8bef\u4fee\u590d\u5de5\u4f5c\u6d41\uff0c\u901a\u8fc7\u4e0a\u4e0b\u6587\u6269\u5c55\u548c\u5f3a\u5316\u5b66\u4e60\u7b56\u7565\uff0c\u5728\u4f01\u4e1a\u73af\u5883\u4e2d\u9ad8\u6548\u89e3\u51b3\u6280\u672f\u503a\u52a1\u95ee\u9898\u3002", "motivation": "\u4f01\u4e1a\u4ee3\u7801\u5e93\u89c4\u6a21\u6269\u5927\u5bfc\u81f4lint\u9519\u8bef\u6570\u91cf\u8fdc\u8d85\u5de5\u7a0b\u5e08\u624b\u52a8\u4fee\u590d\u80fd\u529b\uff0c\u6280\u672f\u503a\u52a1\u79ef\u7d2f\u5f71\u54cd\u5f00\u53d1\u6548\u7387\u3002", "method": "\u4f7f\u7528tree-sitter\u6269\u5c55\u4e0a\u4e0b\u6587\uff0c\u901a\u8fc7LLM\u751f\u6210\u8865\u4e01\u5e76\u8fdb\u884c\u9a8c\u8bc1\uff1b\u5f15\u5165\u6e10\u8fdb\u5f0f\u5f3a\u5316\u5b66\u4e60\u7b56\u7565\u548c\u89c4\u5219\u5956\u52b1\u673a\u5236\u3002", "result": "\u5728\u5b57\u8282\u8df3\u52a8\u751f\u4ea7\u73af\u5883\u4e2d\u652f\u63015000\u591a\u540d\u5de5\u7a0b\u5e08\uff0c\u4fee\u590d12000\u591a\u4e2a\u9759\u6001\u5206\u6790\u95ee\u9898\uff0c\u51c6\u786e\u7387\u7ea685%\u3002", "conclusion": "\u8bc1\u660e\u4e86LLM\u5728\u4f01\u4e1a\u4ee3\u7801\u4fee\u590d\u4e2d\u7684\u53ef\u884c\u6027\uff0c\u4e3a\u5927\u89c4\u6a21\u5de5\u4e1a\u573a\u666f\u63d0\u4f9b\u4e86\u53c2\u8003\u3002"}}
{"id": "2508.03031", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.03031", "abs": "https://arxiv.org/abs/2508.03031", "authors": ["Ziyang Ma", "Baojian Zhou", "Deqing Yang", "Yanghua Xiao"], "title": "From Text to Trajectories: GPT-2 as an ODE Solver via In-Context", "comment": null, "summary": "In-Context Learning (ICL) has emerged as a new paradigm in large language\nmodels (LLMs), enabling them to perform novel tasks by conditioning on a few\nexamples embedded in the prompt. Yet, the highly nonlinear behavior of ICL for\nNLP tasks remains poorly understood. To shed light on its underlying\nmechanisms, this paper investigates whether LLMs can solve ordinary\ndifferential equations (ODEs) under the ICL setting. We formulate standard ODE\nproblems and their solutions as sequential prompts and evaluate GPT-2 models on\nthese tasks. Experiments on two types of ODEs show that GPT-2 can effectively\nlearn a meta-ODE algorithm, with convergence behavior comparable to, or better\nthan, the Euler method, and achieve exponential accuracy gains with increasing\nnumbers of demonstrations. Moreover, the model generalizes to\nout-of-distribution (OOD) problems, demonstrating robust extrapolation\ncapabilities. These empirical findings provide new insights into the mechanisms\nof ICL in NLP and its potential for solving nonlinear numerical problems.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86LLMs\u5728ICL\u8bbe\u7f6e\u4e0b\u89e3\u51b3ODE\u95ee\u9898\u7684\u80fd\u529b\uff0c\u53d1\u73b0GPT-2\u80fd\u6709\u6548\u5b66\u4e60\u5143ODE\u7b97\u6cd5\uff0c\u6027\u80fd\u4f18\u4e8e\u6b27\u62c9\u65b9\u6cd5\uff0c\u5e76\u5c55\u793a\u51fa\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u63a2\u7d22ICL\u5728NLP\u4efb\u52a1\u4e2d\u7684\u975e\u7ebf\u6027\u884c\u4e3a\u673a\u5236\uff0c\u4ee5\u53ca\u5176\u5728\u89e3\u51b3ODE\u95ee\u9898\u4e2d\u7684\u6f5c\u529b\u3002", "method": "\u5c06ODE\u95ee\u9898\u53ca\u5176\u89e3\u4f5c\u4e3a\u5e8f\u5217\u63d0\u793a\uff0c\u8bc4\u4f30GPT-2\u6a21\u578b\u5728\u4e24\u7c7bODE\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\u3002", "result": "GPT-2\u80fd\u5b66\u4e60\u5143ODE\u7b97\u6cd5\uff0c\u6027\u80fd\u4f18\u4e8e\u6b27\u62c9\u65b9\u6cd5\uff0c\u4e14\u968f\u7740\u793a\u4f8b\u589e\u52a0\u7cbe\u5ea6\u63d0\u5347\uff1b\u6a21\u578b\u8fd8\u5c55\u793a\u4e86OOD\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u7814\u7a76\u4e3aICL\u673a\u5236\u63d0\u4f9b\u4e86\u65b0\u89c1\u89e3\uff0c\u5e76\u5c55\u793a\u4e86\u5176\u5728\u975e\u7ebf\u6027\u6570\u503c\u95ee\u9898\u4e2d\u7684\u6f5c\u529b\u3002"}}
{"id": "2508.03560", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.03560", "abs": "https://arxiv.org/abs/2508.03560", "authors": ["Yi Gui", "Zhen Li", "Zhongyi Zhang", "Guohao Wang", "Tianpeng Lv", "Gaoyang Jiang", "Yi Liu", "Dongping Chen", "Yao Wan", "Hongyu Zhang", "Wenbin Jiang", "Xuanhua Shi", "Hai Jin"], "title": "LaTCoder: Converting Webpage Design to Code with Layout-as-Thought", "comment": "KDD 2025 v2", "summary": "Converting webpage designs into code (design-to-code) plays a vital role in\nUser Interface (UI) development for front-end developers, bridging the gap\nbetween visual design and functional implementation. While recent Multimodal\nLarge Language Models (MLLMs) have shown significant potential in\ndesign-to-code tasks, they often fail to accurately preserve the layout during\ncode generation. To this end, we draw inspiration from the Chain-of-Thought\n(CoT) reasoning in human cognition and propose LaTCoder, a novel approach that\nenhances layout preservation in webpage design during code generation with\nLayout-as-Thought (LaT). Specifically, we first introduce a simple yet\nefficient algorithm to divide the webpage design into image blocks. Next, we\nprompt MLLMs using a CoTbased approach to generate code for each block.\nFinally, we apply two assembly strategies-absolute positioning and an\nMLLM-based method-followed by dynamic selection to determine the optimal\noutput. We evaluate the effectiveness of LaTCoder using multiple backbone MLLMs\n(i.e., DeepSeek-VL2, Gemini, and GPT-4o) on both a public benchmark and a newly\nintroduced, more challenging benchmark (CC-HARD) that features complex layouts.\nThe experimental results on automatic metrics demonstrate significant\nimprovements. Specifically, TreeBLEU scores increased by 66.67% and MAE\ndecreased by 38% when using DeepSeek-VL2, compared to direct prompting.\nMoreover, the human preference evaluation results indicate that annotators\nfavor the webpages generated by LaTCoder in over 60% of cases, providing strong\nevidence of the effectiveness of our method.", "AI": {"tldr": "LaTCoder\u901a\u8fc7Layout-as-Thought\u65b9\u6cd5\u63d0\u5347\u7f51\u9875\u8bbe\u8ba1\u5230\u4ee3\u7801\u751f\u6210\u4e2d\u7684\u5e03\u5c40\u4fdd\u7559\u80fd\u529b\uff0c\u663e\u8457\u4f18\u4e8e\u76f4\u63a5\u63d0\u793a\u65b9\u6cd5\u3002", "motivation": "\u89e3\u51b3\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u7f51\u9875\u8bbe\u8ba1\u5230\u4ee3\u7801\u751f\u6210\u4efb\u52a1\u4e2d\u5e03\u5c40\u4fdd\u7559\u4e0d\u51c6\u786e\u7684\u95ee\u9898\u3002", "method": "1. \u5c06\u7f51\u9875\u8bbe\u8ba1\u5212\u5206\u4e3a\u56fe\u50cf\u5757\uff1b2. \u4f7f\u7528CoT\u65b9\u6cd5\u4e3a\u6bcf\u4e2a\u5757\u751f\u6210\u4ee3\u7801\uff1b3. \u91c7\u7528\u7edd\u5bf9\u5b9a\u4f4d\u548cMLLM\u7ec4\u88c5\u7b56\u7565\u52a8\u6001\u9009\u62e9\u6700\u4f18\u8f93\u51fa\u3002", "result": "\u5728DeepSeek-VL2\u4e0a\uff0cTreeBLEU\u63d0\u534766.67%\uff0cMAE\u964d\u4f4e38%\uff1b\u4eba\u5de5\u8bc4\u4f30\u4e2d60%\u4ee5\u4e0a\u504f\u597dLaTCoder\u751f\u6210\u7684\u7f51\u9875\u3002", "conclusion": "LaTCoder\u663e\u8457\u63d0\u5347\u4e86\u5e03\u5c40\u4fdd\u7559\u80fd\u529b\uff0c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002"}}
{"id": "2508.03038", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.03038", "abs": "https://arxiv.org/abs/2508.03038", "authors": ["Qi Peng", "Jialin Cui", "Jiayuan Xie", "Yi Cai", "Qing Li"], "title": "Tree-of-Reasoning: Towards Complex Medical Diagnosis via Multi-Agent Reasoning with Evidence Tree", "comment": "Accepted by ACM MM 2025", "summary": "Large language models (LLMs) have shown great potential in the medical\ndomain. However, existing models still fall short when faced with complex\nmedical diagnosis task in the real world. This is mainly because they lack\nsufficient reasoning depth, which leads to information loss or logical jumps\nwhen processing a large amount of specialized medical data, leading to\ndiagnostic errors. To address these challenges, we propose Tree-of-Reasoning\n(ToR), a novel multi-agent framework designed to handle complex scenarios.\nSpecifically, ToR introduces a tree structure that can clearly record the\nreasoning path of LLMs and the corresponding clinical evidence. At the same\ntime, we propose a cross-validation mechanism to ensure the consistency of\nmulti-agent decision-making, thereby improving the clinical reasoning ability\nof multi-agents in complex medical scenarios. Experimental results on\nreal-world medical data show that our framework can achieve better performance\nthan existing baseline methods.", "AI": {"tldr": "\u63d0\u51faTree-of-Reasoning (ToR)\u6846\u67b6\uff0c\u901a\u8fc7\u6811\u72b6\u7ed3\u6784\u548c\u4ea4\u53c9\u9a8c\u8bc1\u673a\u5236\u63d0\u5347\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u590d\u6742\u533b\u7597\u8bca\u65ad\u4efb\u52a1\u4e2d\u7684\u63a8\u7406\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u590d\u6742\u533b\u7597\u8bca\u65ad\u4efb\u52a1\u4e2d\u56e0\u63a8\u7406\u6df1\u5ea6\u4e0d\u8db3\u5bfc\u81f4\u4fe1\u606f\u4e22\u5931\u6216\u903b\u8f91\u8df3\u8dc3\uff0c\u5f71\u54cd\u8bca\u65ad\u51c6\u786e\u6027\u3002", "method": "\u63d0\u51faToR\u6846\u67b6\uff0c\u91c7\u7528\u6811\u72b6\u7ed3\u6784\u8bb0\u5f55\u63a8\u7406\u8def\u5f84\u548c\u4e34\u5e8a\u8bc1\u636e\uff0c\u5e76\u5f15\u5165\u4ea4\u53c9\u9a8c\u8bc1\u673a\u5236\u786e\u4fdd\u591a\u667a\u80fd\u4f53\u51b3\u7b56\u4e00\u81f4\u6027\u3002", "result": "\u5728\u771f\u5b9e\u533b\u7597\u6570\u636e\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cToR\u6846\u67b6\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "ToR\u6846\u67b6\u901a\u8fc7\u7ed3\u6784\u5316\u63a8\u7406\u548c\u9a8c\u8bc1\u673a\u5236\uff0c\u663e\u8457\u63d0\u5347\u4e86\u590d\u6742\u533b\u7597\u573a\u666f\u4e2d\u7684\u8bca\u65ad\u6027\u80fd\u3002"}}
{"id": "2508.03603", "categories": ["cs.SE", "cs.PL"], "pdf": "https://arxiv.org/pdf/2508.03603", "abs": "https://arxiv.org/abs/2508.03603", "authors": ["Iti Shree", "Karine Even-Mendoz", "Tomasz Radzik"], "title": "ReFuzzer: Feedback-Driven Approach to Enhance Validity of LLM-Generated Test Programs", "comment": null, "summary": "Existing LLM-based compiler fuzzers often produce syntactically or\nsemantically invalid test programs, limiting their effectiveness in exercising\ncompiler optimizations and backend components. We introduce ReFuzzer, a\nframework for refining LLM-generated test programs by systematically detecting\nand correcting compilation and runtime violations (e.g. division by zero or\narray out-of-bounds accesses). ReFuzzer employs a feedback loop with a local\nLLM to validate and filter erroneous programs before execution, improving\nfuzzing effectiveness beyond crash detection and enabling the generation of\ndiverse yet valid test programs.\n  We evaluated ReFuzzer's effectiveness across black-, grey- and white-box\nfuzzing approaches targeting LLVM/Clang. ReFuzzer improved test programs'\nvalidity from 47.0-49.4% to 96.6-97.3%, with an average processing time of\n2.9-3.5 s per test program on a dual-GPU machine. Further, refuzzing\nsignificantly increased code coverage in critical optimization and IR\ngeneration components. For example, vectorization coverage had an absolute\nimprovement of 9.2%, 2.3%, and 7.1% in black-, grey-, and white-box fuzzing,\nenhancing testing effectiveness.", "AI": {"tldr": "ReFuzzer\u662f\u4e00\u4e2a\u6846\u67b6\uff0c\u901a\u8fc7\u68c0\u6d4b\u548c\u4fee\u6b63LLM\u751f\u6210\u7684\u6d4b\u8bd5\u7a0b\u5e8f\u4e2d\u7684\u7f16\u8bd1\u548c\u8fd0\u884c\u65f6\u9519\u8bef\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u6d4b\u8bd5\u7a0b\u5e8f\u7684\u6709\u6548\u6027\u548c\u4ee3\u7801\u8986\u76d6\u7387\u3002", "motivation": "\u73b0\u6709\u7684\u57fa\u4e8eLLM\u7684\u7f16\u8bd1\u5668\u6a21\u7cca\u6d4b\u8bd5\u5de5\u5177\u5e38\u751f\u6210\u8bed\u6cd5\u6216\u8bed\u4e49\u65e0\u6548\u7684\u6d4b\u8bd5\u7a0b\u5e8f\uff0c\u9650\u5236\u4e86\u5176\u5728\u6d4b\u8bd5\u7f16\u8bd1\u5668\u4f18\u5316\u548c\u540e\u7aef\u7ec4\u4ef6\u4e2d\u7684\u6548\u679c\u3002", "method": "ReFuzzer\u91c7\u7528\u53cd\u9988\u5faa\u73af\u673a\u5236\uff0c\u5229\u7528\u672c\u5730LLM\u9a8c\u8bc1\u548c\u8fc7\u6ee4\u9519\u8bef\u7a0b\u5e8f\uff0c\u786e\u4fdd\u751f\u6210\u591a\u6837\u4e14\u6709\u6548\u7684\u6d4b\u8bd5\u7a0b\u5e8f\u3002", "result": "ReFuzzer\u5c06\u6d4b\u8bd5\u7a0b\u5e8f\u7684\u6709\u6548\u6027\u4ece47.0-49.4%\u63d0\u5347\u81f396.6-97.3%\uff0c\u5e76\u663e\u8457\u63d0\u9ad8\u4e86\u5173\u952e\u4f18\u5316\u548cIR\u751f\u6210\u7ec4\u4ef6\u7684\u4ee3\u7801\u8986\u76d6\u7387\u3002", "conclusion": "ReFuzzer\u901a\u8fc7\u6539\u8fdb\u6d4b\u8bd5\u7a0b\u5e8f\u7684\u6709\u6548\u6027\u548c\u8986\u76d6\u7387\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7f16\u8bd1\u5668\u6a21\u7cca\u6d4b\u8bd5\u7684\u6548\u679c\u3002"}}
{"id": "2508.03054", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.03054", "abs": "https://arxiv.org/abs/2508.03054", "authors": ["Rui Pu", "Chaozhuo Li", "Rui Ha", "Litian Zhang", "Lirong Qiu", "Xi Zhang"], "title": "Beyond Surface-Level Detection: Towards Cognitive-Driven Defense Against Jailbreak Attacks via Meta-Operations Reasoning", "comment": null, "summary": "Defending large language models (LLMs) against jailbreak attacks is essential\nfor their safe and reliable deployment. Existing defenses often rely on shallow\npattern matching, which struggles to generalize to novel and unseen attack\nstrategies. To address this challenge, we propose the Cognitive-Driven Defense\n(CDD) framework, which targets the underlying structure of jailbreak prompts by\napplying meta-operations, defined as basic manipulations that conceal harmful\nintent.CDD emulates human cognitive reasoning through a structured reasoning\nchain. It begins with a global perception of the prompt and follows with a\nlocalized analysis to uncover hidden manipulations. By applying supervised\nfine-tuning on this structured chain, the model learns to identify and reason\nabout known manipulation patterns. To enhance generalization to unseen threats,\nan entropy-guided reinforcement learning algorithm (EG-GRPO) is introduced to\nencourage exploration of new types and variants of meta-operations. Experiments\ndemonstrate that CDD can achieve state-of-the-art defense performance and\nexhibit strong generalization to unseen jailbreak attacks.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8ba4\u77e5\u9a71\u52a8\u9632\u5fa1\uff08CDD\uff09\u6846\u67b6\uff0c\u901a\u8fc7\u6a21\u62df\u4eba\u7c7b\u8ba4\u77e5\u63a8\u7406\u6765\u9632\u5fa1\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u8d8a\u72f1\u653b\u51fb\uff0c\u7ed3\u5408\u76d1\u7763\u5fae\u8c03\u548c\u5f3a\u5316\u5b66\u4e60\u63d0\u5347\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u9632\u5fa1\u65b9\u6cd5\u4f9d\u8d56\u6d45\u5c42\u6a21\u5f0f\u5339\u914d\uff0c\u96be\u4ee5\u5e94\u5bf9\u65b0\u578b\u653b\u51fb\u7b56\u7565\uff0c\u56e0\u6b64\u9700\u8981\u66f4\u901a\u7528\u7684\u9632\u5fa1\u673a\u5236\u3002", "method": "CDD\u6846\u67b6\u901a\u8fc7\u5168\u5c40\u611f\u77e5\u548c\u5c40\u90e8\u5206\u6790\u63ed\u793a\u9690\u85cf\u64cd\u4f5c\uff0c\u7ed3\u5408\u76d1\u7763\u5fae\u8c03\u548c\u71b5\u5f15\u5bfc\u7684\u5f3a\u5316\u5b66\u4e60\uff08EG-GRPO\uff09\u63a2\u7d22\u65b0\u653b\u51fb\u53d8\u4f53\u3002", "result": "\u5b9e\u9a8c\u8868\u660eCDD\u5728\u9632\u5fa1\u6027\u80fd\u548c\u6cdb\u5316\u80fd\u529b\u4e0a\u8fbe\u5230\u6700\u5148\u8fdb\u6c34\u5e73\u3002", "conclusion": "CDD\u6846\u67b6\u4e3aLLM\u9632\u5fa1\u63d0\u4f9b\u4e86\u6709\u6548\u4e14\u901a\u7528\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.03642", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.03642", "abs": "https://arxiv.org/abs/2508.03642", "authors": ["Oliver Westphal"], "title": "Intent Preserving Generation of Diverse and Idiomatic (Code-)Artifacts", "comment": "In Proceedings TFPiE 2025, arXiv:2508.02305", "summary": "When automatically generating programming exercise tasks one often also needs\nto automatically generate programs. At the very least when providing sample\nsolutions is part of automated feedback. But programs can also be used as part\nof the exercise task description to communicate a task's requirements.\n  Writing good program generators that produce varied yet idiomatic code while\nbeing easily adaptable for new tasks is challenging. The challenges are\nintensified if task generation requires additional artifacts, like a more\ngeneral behavior specification for testing or additional textual descriptions.\nManually writing generators for multiple different but strongly related\nartifacts gets complicated quickly.\n  We present an approach where instead of writing monolithic generators for\nmultiple connected artifacts one specifies a small set of abstract building\nblocks and for each such building block defines sets of concrete realizations\nfor various kinds of artifacts. Then the intended structure of the resulting\nartifacts is specified as a composition of the small abstract building blocks.\nThis abstract description then serves as the common source from which related\nartifacts can be derived automatically. The approach is generic in the kind of\nartifacts it can produce and is therefore adaptable to a wide range of\ncontexts.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7\u62bd\u8c61\u6784\u5efa\u5757\u81ea\u52a8\u751f\u6210\u7f16\u7a0b\u7ec3\u4e60\u4efb\u52a1\u548c\u76f8\u5173\u5de5\u4ef6\u7684\u65b9\u6cd5\uff0c\u907f\u514d\u4e86\u624b\u52a8\u7f16\u5199\u590d\u6742\u751f\u6210\u5668\u7684\u9700\u6c42\u3002", "motivation": "\u81ea\u52a8\u751f\u6210\u7f16\u7a0b\u7ec3\u4e60\u4efb\u52a1\u548c\u76f8\u5173\u5de5\u4ef6\uff08\u5982\u793a\u4f8b\u89e3\u51b3\u65b9\u6848\u6216\u4efb\u52a1\u63cf\u8ff0\uff09\u65f6\uff0c\u7f16\u5199\u591a\u6837\u4e14\u7b26\u5408\u4e60\u60ef\u7684\u4ee3\u7801\u751f\u6210\u5668\u5177\u6709\u6311\u6218\u6027\uff0c\u5c24\u5176\u662f\u5728\u9700\u8981\u751f\u6210\u591a\u4e2a\u76f8\u5173\u5de5\u4ef6\u65f6\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b9\u6cd5\uff0c\u901a\u8fc7\u5b9a\u4e49\u4e00\u7ec4\u62bd\u8c61\u6784\u5efa\u5757\u53ca\u5176\u5177\u4f53\u5b9e\u73b0\uff0c\u7136\u540e\u7ec4\u5408\u8fd9\u4e9b\u6784\u5efa\u5757\u6765\u751f\u6210\u76f8\u5173\u5de5\u4ef6\uff0c\u4ece\u800c\u907f\u514d\u624b\u52a8\u7f16\u5199\u590d\u6742\u7684\u751f\u6210\u5668\u3002", "result": "\u8be5\u65b9\u6cd5\u80fd\u591f\u81ea\u52a8\u751f\u6210\u591a\u6837\u5316\u7684\u7f16\u7a0b\u7ec3\u4e60\u4efb\u52a1\u548c\u76f8\u5173\u5de5\u4ef6\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u4e0a\u4e0b\u6587\u3002", "conclusion": "\u901a\u8fc7\u62bd\u8c61\u6784\u5efa\u5757\u7684\u65b9\u6cd5\uff0c\u53ef\u4ee5\u66f4\u7075\u6d3b\u3001\u9ad8\u6548\u5730\u751f\u6210\u7f16\u7a0b\u7ec3\u4e60\u4efb\u52a1\u548c\u76f8\u5173\u5de5\u4ef6\uff0c\u9002\u5e94\u6027\u5f3a\u3002"}}
{"id": "2508.03080", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.03080", "abs": "https://arxiv.org/abs/2508.03080", "authors": ["Shuang Liu", "Zelong Li", "Ruoyun Ma", "Haiyan Zhao", "Mengnan Du"], "title": "ContractEval: Benchmarking LLMs for Clause-Level Legal Risk Identification in Commercial Contracts", "comment": null, "summary": "The potential of large language models (LLMs) in specialized domains such as\nlegal risk analysis remains underexplored. In response to growing interest in\nlocally deploying open-source LLMs for legal tasks while preserving data\nconfidentiality, this paper introduces ContractEval, the first benchmark to\nthoroughly evaluate whether open-source LLMs could match proprietary LLMs in\nidentifying clause-level legal risks in commercial contracts. Using the\nContract Understanding Atticus Dataset (CUAD), we assess 4 proprietary and 15\nopen-source LLMs. Our results highlight five key findings: (1) Proprietary\nmodels outperform open-source models in both correctness and output\neffectiveness, though some open-source models are competitive in certain\nspecific dimensions. (2) Larger open-source models generally perform better,\nthough the improvement slows down as models get bigger. (3) Reasoning\n(\"thinking\") mode improves output effectiveness but reduces correctness, likely\ndue to over-complicating simpler tasks. (4) Open-source models generate \"no\nrelated clause\" responses more frequently even when relevant clauses are\npresent. This suggests \"laziness\" in thinking or low confidence in extracting\nrelevant content. (5) Model quantization speeds up inference but at the cost of\nperformance drop, showing the tradeoff between efficiency and accuracy. These\nfindings suggest that while most LLMs perform at a level comparable to junior\nlegal assistants, open-source models require targeted fine-tuning to ensure\ncorrectness and effectiveness in high-stakes legal settings. ContractEval\noffers a solid benchmark to guide future development of legal-domain LLMs.", "AI": {"tldr": "\u8bba\u6587\u8bc4\u4f30\u5f00\u6e90\u4e0e\u4e13\u6709\u5927\u8bed\u8a00\u6a21\u578b\u5728\u6cd5\u5f8b\u98ce\u9669\u5206\u6790\u4e2d\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u4e13\u6709\u6a21\u578b\u603b\u4f53\u66f4\u4f18\uff0c\u5f00\u6e90\u6a21\u578b\u9700\u9488\u5bf9\u6027\u4f18\u5316\u3002", "motivation": "\u63a2\u7d22\u5f00\u6e90\u5927\u8bed\u8a00\u6a21\u578b\u5728\u6cd5\u5f8b\u9886\u57df\u7684\u6f5c\u529b\uff0c\u6ee1\u8db3\u672c\u5730\u90e8\u7f72\u9700\u6c42\u5e76\u4fdd\u62a4\u6570\u636e\u9690\u79c1\u3002", "method": "\u4f7f\u7528ContractEval\u57fa\u51c6\u548cCUAD\u6570\u636e\u96c6\u8bc4\u4f304\u4e2a\u4e13\u6709\u548c15\u4e2a\u5f00\u6e90\u6a21\u578b\u3002", "result": "\u4e13\u6709\u6a21\u578b\u8868\u73b0\u66f4\u4f18\uff1b\u5f00\u6e90\u6a21\u578b\u5728\u7279\u5b9a\u7ef4\u5ea6\u6709\u7ade\u4e89\u529b\uff1b\u63a8\u7406\u6a21\u5f0f\u5f71\u54cd\u6548\u679c\uff1b\u91cf\u5316\u52a0\u901f\u4f46\u964d\u4f4e\u6027\u80fd\u3002", "conclusion": "\u5f00\u6e90\u6a21\u578b\u9700\u9488\u5bf9\u6027\u4f18\u5316\u4ee5\u5339\u914d\u4e13\u6709\u6a21\u578b\uff0cContractEval\u4e3a\u672a\u6765\u6cd5\u5f8b\u9886\u57dfLLM\u53d1\u5c55\u63d0\u4f9b\u57fa\u51c6\u3002"}}
{"id": "2508.03379", "categories": ["cs.AI", "cs.SE"], "pdf": "https://arxiv.org/pdf/2508.03379", "abs": "https://arxiv.org/abs/2508.03379", "authors": ["Wenxin Mao", "Zhitao Wang Long Wang", "Sirong Chen", "Cuiyun Gao", "Luyang Cao", "Ziming Liu", "Qiming Zhang", "Jun Zhou", "Zhi Jin"], "title": "Data Dependency Inference for Industrial Code Generation Based on UML Sequence Diagrams", "comment": null, "summary": "Large language models (LLMs) excel at generating code from natural language\n(NL) descriptions. However, the plain textual descriptions are inherently\nambiguous and often fail to capture complex requirements like intricate system\nbehaviors, conditional logic, and architectural constraints; implicit data\ndependencies in service-oriented architectures are difficult to infer and\nhandle correctly. To bridge this gap, we propose a novel step-by-step code\ngeneration framework named UML2Dep by leveraging unambiguous formal\nspecifications of complex requirements. First, we introduce an enhanced Unified\nModeling Language (UML) sequence diagram tailored for service-oriented\narchitectures. This diagram extends traditional visual syntax by integrating\ndecision tables and API specifications, explicitly formalizing structural\nrelationships and business logic flows in service interactions to rigorously\neliminate linguistic ambiguity. Second, recognizing the critical role of data\nflow, we introduce a dedicated data dependency inference (DDI) task. DDI\nsystematically constructs an explicit data dependency graph prior to actual\ncode synthesis. To ensure reliability, we formalize DDI as a constrained\nmathematical reasoning task through novel prompting strategies, aligning with\nLLMs' excellent mathematical strengths. Additional static parsing and\ndependency pruning further reduce context complexity and cognitive load\nassociated with intricate specifications, thereby enhancing reasoning accuracy\nand efficiency.", "AI": {"tldr": "\u63d0\u51faUML2Dep\u6846\u67b6\uff0c\u5229\u7528\u5f62\u5f0f\u5316\u89c4\u8303\u89e3\u51b3\u81ea\u7136\u8bed\u8a00\u63cf\u8ff0\u7684\u6a21\u7cca\u6027\uff0c\u901a\u8fc7\u589e\u5f3a\u7684UML\u5e8f\u5217\u56fe\u548c\u6570\u636e\u4f9d\u8d56\u63a8\u7406\u4efb\u52a1\uff0c\u63d0\u5347\u4ee3\u7801\u751f\u6210\u7684\u51c6\u786e\u6027\u548c\u6548\u7387\u3002", "motivation": "\u81ea\u7136\u8bed\u8a00\u63cf\u8ff0\u5728\u590d\u6742\u9700\u6c42\u548c\u7cfb\u7edf\u884c\u4e3a\u4e2d\u5b58\u5728\u6a21\u7cca\u6027\uff0c\u96be\u4ee5\u6355\u6349\u6761\u4ef6\u903b\u8f91\u548c\u67b6\u6784\u7ea6\u675f\uff0c\u9700\u8981\u5f62\u5f0f\u5316\u89c4\u8303\u6765\u6d88\u9664\u6b67\u4e49\u3002", "method": "1. \u5f15\u5165\u589e\u5f3a\u7684UML\u5e8f\u5217\u56fe\uff0c\u6574\u5408\u51b3\u7b56\u8868\u548cAPI\u89c4\u8303\uff1b2. \u63d0\u51fa\u6570\u636e\u4f9d\u8d56\u63a8\u7406\uff08DDI\uff09\u4efb\u52a1\uff0c\u6784\u5efa\u663e\u5f0f\u6570\u636e\u4f9d\u8d56\u56fe\uff1b3. \u901a\u8fc7\u6570\u5b66\u63a8\u7406\u548c\u9759\u6001\u89e3\u6790\u4f18\u5316\u63a8\u7406\u8fc7\u7a0b\u3002", "result": "\u6846\u67b6\u901a\u8fc7\u5f62\u5f0f\u5316\u89c4\u8303\u548cDDI\u4efb\u52a1\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u6a21\u7cca\u6027\uff0c\u63d0\u5347\u4e86\u4ee3\u7801\u751f\u6210\u7684\u51c6\u786e\u6027\u548c\u6548\u7387\u3002", "conclusion": "UML2Dep\u6846\u67b6\u901a\u8fc7\u5f62\u5f0f\u5316\u89c4\u8303\u548c\u7cfb\u7edf\u5316\u7684\u6570\u636e\u4f9d\u8d56\u63a8\u7406\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u81ea\u7136\u8bed\u8a00\u63cf\u8ff0\u7684\u6a21\u7cca\u6027\u95ee\u9898\uff0c\u4e3a\u590d\u6742\u9700\u6c42\u4e0b\u7684\u4ee3\u7801\u751f\u6210\u63d0\u4f9b\u4e86\u53ef\u9760\u65b9\u6cd5\u3002"}}
{"id": "2508.03082", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.03082", "abs": "https://arxiv.org/abs/2508.03082", "authors": ["Fei Liu", "Yilu Liu", "Qingfu Zhang", "Xialiang Tong", "Mingxuan Yuan"], "title": "EoH-S: Evolution of Heuristic Set using LLMs for Automated Heuristic Design", "comment": null, "summary": "Automated Heuristic Design (AHD) using Large Language Models (LLMs) has\nachieved notable success in recent years. Despite the effectiveness of existing\napproaches, they only design a single heuristic to serve all problem instances,\noften inducing poor generalization across different distributions or settings.\nTo address this issue, we propose Automated Heuristic Set Design (AHSD), a new\nformulation for LLM-driven AHD. The aim of AHSD is to automatically generate a\nsmall-sized complementary heuristic set to serve diverse problem instances,\nsuch that each problem instance could be optimized by at least one heuristic in\nthis set. We show that the objective function of AHSD is monotone and\nsupermodular. Then, we propose Evolution of Heuristic Set (EoH-S) to apply the\nAHSD formulation for LLM-driven AHD. With two novel mechanisms of complementary\npopulation management and complementary-aware memetic search, EoH-S could\neffectively generate a set of high-quality and complementary heuristics.\nComprehensive experimental results on three AHD tasks with diverse instances\nspanning various sizes and distributions demonstrate that EoH-S consistently\noutperforms existing state-of-the-art AHD methods and achieves up to 60\\%\nperformance improvements.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u81ea\u52a8\u5316\u542f\u53d1\u5f0f\u96c6\u5408\u8bbe\u8ba1\uff08AHSD\uff09\u65b9\u6cd5\uff0c\u901a\u8fc7\u751f\u6210\u4e92\u8865\u7684\u542f\u53d1\u5f0f\u96c6\u5408\u6765\u63d0\u5347\u6cdb\u5316\u80fd\u529b\uff0c\u5e76\u63d0\u51fa\u4e86EoH-S\u7b97\u6cd5\u5b9e\u73b0\u8fd9\u4e00\u76ee\u6807\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4ec5\u8bbe\u8ba1\u5355\u4e00\u542f\u53d1\u5f0f\uff0c\u6cdb\u5316\u80fd\u529b\u5dee\uff0c\u65e0\u6cd5\u9002\u5e94\u4e0d\u540c\u95ee\u9898\u5b9e\u4f8b\u7684\u591a\u6837\u6027\u3002", "method": "\u63d0\u51faAHSD\u6846\u67b6\uff0c\u76ee\u6807\u51fd\u6570\u5177\u6709\u5355\u8c03\u6027\u548c\u8d85\u6a21\u6027\uff0c\u5e76\u8bbe\u8ba1EoH-S\u7b97\u6cd5\uff0c\u7ed3\u5408\u4e92\u8865\u79cd\u7fa4\u7ba1\u7406\u548c\u4e92\u8865\u611f\u77e5\u7684\u6a21\u56e0\u641c\u7d22\u3002", "result": "\u5728\u4e09\u4e2a\u4efb\u52a1\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cEoH-S\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u6027\u80fd\u63d0\u5347\u9ad8\u8fbe60%\u3002", "conclusion": "AHSD\u548cEoH-S\u4e3a\u81ea\u52a8\u5316\u542f\u53d1\u5f0f\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u66f4\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2508.03488", "categories": ["cs.AI", "cs.SE"], "pdf": "https://arxiv.org/pdf/2508.03488", "abs": "https://arxiv.org/abs/2508.03488", "authors": ["Khaled Bachir Delassi", "Lakhdar Zeggane", "Hadda Cherroun", "Abdelhamid Haouhat", "Kaoutar Bouzouad"], "title": "VQA support to Arabic Language Learning Educational Tool", "comment": null, "summary": "We address the problem of scarcity of educational Arabic Language Learning\ntools that advocate modern pedagogical models such as active learning which\nensures language proficiency. In fact, we investigate the design and evaluation\nof an AI-powered educational tool designed to enhance Arabic language learning\nfor non-native speakers with beginner-to-intermediate proficiency level. The\ntool leverages advanced AI models to generate interactive visual quizzes,\ndeploying Visual Question Answering as the primary activity. Adopting a\nconstructivist learning approach, the system encourages active learning through\nreal-life visual quizzes, and image-based questions that focus on improving\nvocabulary, grammar, and comprehension. The system integrates Vision-Language\nPretraining models to generate contextually relevant image description from\nwhich Large Language Model generate assignments based on customized Arabic\nlanguage Learning quizzes thanks to prompting.\n  The effectiveness of the tool is evaluated through a manual annotated\nbenchmark consisting of 1266 real-life visual quizzes, with human participants\nproviding feedback. The results show a suitable accuracy rates, validating the\ntool's potential to bridge the gap in Arabic language education and\nhighlighting the tool's promise as a reliable, AI-powered resource for Arabic\nlearners, offering personalized and interactive learning experiences.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eAI\u7684\u963f\u62c9\u4f2f\u8bed\u5b66\u4e60\u5de5\u5177\uff0c\u901a\u8fc7\u89c6\u89c9\u95ee\u7b54\u548c\u4e92\u52a8\u6d4b\u9a8c\u4fc3\u8fdb\u4e3b\u52a8\u5b66\u4e60\uff0c\u586b\u8865\u4e86\u73b0\u4ee3\u6559\u80b2\u5de5\u5177\u7684\u7a7a\u767d\u3002", "motivation": "\u89e3\u51b3\u963f\u62c9\u4f2f\u8bed\u5b66\u4e60\u5de5\u5177\u7a00\u7f3a\u7684\u95ee\u9898\uff0c\u5c24\u5176\u662f\u652f\u6301\u73b0\u4ee3\u6559\u5b66\u6a21\u578b\uff08\u5982\u4e3b\u52a8\u5b66\u4e60\uff09\u7684\u5de5\u5177\u3002", "method": "\u5229\u7528\u89c6\u89c9\u8bed\u8a00\u9884\u8bad\u7ec3\u6a21\u578b\u751f\u6210\u4e92\u52a8\u89c6\u89c9\u6d4b\u9a8c\uff0c\u7ed3\u5408\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5b9a\u5236\u963f\u62c9\u4f2f\u8bed\u5b66\u4e60\u4efb\u52a1\u3002", "result": "\u901a\u8fc7\u4eba\u5de5\u6807\u6ce8\u76841266\u4e2a\u89c6\u89c9\u6d4b\u9a8c\u8bc4\u4f30\uff0c\u5de5\u5177\u8868\u73b0\u51fa\u8f83\u9ad8\u7684\u51c6\u786e\u7387\uff0c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u5de5\u5177\u4e3a\u963f\u62c9\u4f2f\u8bed\u5b66\u4e60\u8005\u63d0\u4f9b\u4e86\u4e2a\u6027\u5316\u3001\u4e92\u52a8\u7684\u5b66\u4e60\u4f53\u9a8c\uff0c\u6709\u671b\u6210\u4e3a\u53ef\u9760\u7684\u5b66\u4e60\u8d44\u6e90\u3002"}}
{"id": "2508.03083", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.03083", "abs": "https://arxiv.org/abs/2508.03083", "authors": ["Youran Zhou", "Mohamed Reda Bouadjenek", "Sunil Aryal"], "title": "MissDDIM: Deterministic and Efficient Conditional Diffusion for Tabular Data Imputation", "comment": null, "summary": "Diffusion models have recently emerged as powerful tools for missing data\nimputation by modeling the joint distribution of observed and unobserved\nvariables. However, existing methods, typically based on stochastic denoising\ndiffusion probabilistic models (DDPMs), suffer from high inference latency and\nvariable outputs, limiting their applicability in real-world tabular settings.\nTo address these deficiencies, we present in this paper MissDDIM, a conditional\ndiffusion framework that adapts Denoising Diffusion Implicit Models (DDIM) for\ntabular imputation. While stochastic sampling enables diverse completions, it\nalso introduces output variability that complicates downstream processing.", "AI": {"tldr": "MissDDIM\u662f\u4e00\u79cd\u57fa\u4e8eDDIM\u7684\u6761\u4ef6\u6269\u6563\u6846\u67b6\uff0c\u7528\u4e8e\u8868\u683c\u6570\u636e\u586b\u8865\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u9ad8\u5ef6\u8fdf\u548c\u8f93\u51fa\u4e0d\u7a33\u5b9a\u6027\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8eDDPM\u7684\u7f3a\u5931\u6570\u636e\u586b\u8865\u65b9\u6cd5\u5b58\u5728\u9ad8\u63a8\u7406\u5ef6\u8fdf\u548c\u8f93\u51fa\u4e0d\u7a33\u5b9a\u6027\uff0c\u9650\u5236\u4e86\u5176\u5728\u5b9e\u9645\u8868\u683c\u6570\u636e\u4e2d\u7684\u5e94\u7528\u3002", "method": "\u63d0\u51faMissDDIM\uff0c\u4e00\u79cd\u57fa\u4e8eDDIM\u7684\u6761\u4ef6\u6269\u6563\u6846\u67b6\uff0c\u7528\u4e8e\u8868\u683c\u6570\u636e\u586b\u8865\u3002", "result": "MissDDIM\u901a\u8fc7\u786e\u5b9a\u6027\u91c7\u6837\u51cf\u5c11\u4e86\u8f93\u51fa\u53d8\u5f02\u6027\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u586b\u8865\u7684\u591a\u6837\u6027\u3002", "conclusion": "MissDDIM\u4e3a\u8868\u683c\u6570\u636e\u586b\u8865\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u7a33\u5b9a\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.03092", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2508.03092", "abs": "https://arxiv.org/abs/2508.03092", "authors": ["Zikun Cui", "Tianyi Huang", "Chia-En Chiang", "Cuiqianhe Du"], "title": "Toward Verifiable Misinformation Detection: A Multi-Tool LLM Agent Framework", "comment": null, "summary": "With the proliferation of Large Language Models (LLMs), the detection of\nmisinformation has become increasingly important and complex. This research\nproposes an innovative verifiable misinformation detection LLM agent that goes\nbeyond traditional true/false binary judgments. The agent actively verifies\nclaims through dynamic interaction with diverse web sources, assesses\ninformation source credibility, synthesizes evidence, and provides a complete\nverifiable reasoning process. Our designed agent architecture includes three\ncore tools: precise web search tool, source credibility assessment tool and\nnumerical claim verification tool. These tools enable the agent to execute\nmulti-step verification strategies, maintain evidence logs, and form\ncomprehensive assessment conclusions. We evaluate using standard misinformation\ndatasets such as FakeNewsNet, comparing with traditional machine learning\nmodels and LLMs. Evaluation metrics include standard classification metrics,\nquality assessment of reasoning processes, and robustness testing against\nrewritten content. Experimental results show that our agent outperforms\nbaseline methods in misinformation detection accuracy, reasoning transparency,\nand resistance to information rewriting, providing a new paradigm for\ntrustworthy AI-assisted fact-checking.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u53ef\u9a8c\u8bc1\u7684\u865a\u5047\u4fe1\u606f\u68c0\u6d4bLLM\u4ee3\u7406\uff0c\u901a\u8fc7\u52a8\u6001\u4ea4\u4e92\u548c\u591a\u79cd\u5de5\u5177\u5b9e\u73b0\u591a\u6b65\u9aa4\u9a8c\u8bc1\uff0c\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002", "motivation": "\u968f\u7740\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u666e\u53ca\uff0c\u865a\u5047\u4fe1\u606f\u68c0\u6d4b\u53d8\u5f97\u6108\u53d1\u91cd\u8981\u548c\u590d\u6742\uff0c\u9700\u8981\u8d85\u8d8a\u4f20\u7edf\u7684\u4e8c\u5143\u5224\u65ad\u3002", "method": "\u8bbe\u8ba1\u4e86\u5305\u542b\u7cbe\u786e\u7f51\u7edc\u641c\u7d22\u5de5\u5177\u3001\u6765\u6e90\u53ef\u4fe1\u5ea6\u8bc4\u4f30\u5de5\u5177\u548c\u6570\u503c\u58f0\u660e\u9a8c\u8bc1\u5de5\u5177\u7684\u4ee3\u7406\u67b6\u6784\uff0c\u652f\u6301\u591a\u6b65\u9aa4\u9a8c\u8bc1\u7b56\u7565\u3002", "result": "\u5728\u6807\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u4ee3\u7406\u5728\u68c0\u6d4b\u51c6\u786e\u6027\u3001\u63a8\u7406\u900f\u660e\u5ea6\u548c\u6297\u4fe1\u606f\u6539\u5199\u65b9\u9762\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u53ef\u4fe1\u8d56\u7684AI\u8f85\u52a9\u4e8b\u5b9e\u6838\u67e5\u63d0\u4f9b\u4e86\u65b0\u8303\u5f0f\u3002"}}
{"id": "2508.03109", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.03109", "abs": "https://arxiv.org/abs/2508.03109", "authors": ["Wen-Xi Yang", "Tian-Fang Zhao"], "title": "AgentSME for Simulating Diverse Communication Modes in Smart Education", "comment": null, "summary": "Generative agent models specifically tailored for smart education are\ncritical, yet remain relatively underdeveloped. A key challenge stems from the\ninherent complexity of educational contexts: learners are human beings with\nvarious cognitive behaviors, and pedagogy is fundamentally centered on\npersonalized human-to-human communication. To address this issue, this paper\nproposes AgentSME, a unified generative agent framework powered by LLM. Three\ndirectional communication modes are considered in the models, namely Solo,\nMono, and Echo, reflecting different types of agency autonomy and communicative\nreciprocity. Accuracy is adopted as the primary evaluation metric, complemented\nby three diversity indices designed to assess the diversity of reasoning\ncontents. Six widely used LLMs are tested to validate the robustness of\ncommunication modes across different model tiers, which are equally divided\ninto base-capacity and high-capacity configurations. The results show that\ngenerative agents that employ the Echo communication mode achieve the highest\naccuracy scores, while DeepSeek exhibits the greatest diversity. This study\nprovides valuable information to improve agent learning capabilities and\ninspire smart education models.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eLLM\u7684\u7edf\u4e00\u751f\u6210\u4ee3\u7406\u6846\u67b6AgentSME\uff0c\u7528\u4e8e\u667a\u80fd\u6559\u80b2\uff0c\u901a\u8fc7\u4e09\u79cd\u901a\u4fe1\u6a21\u5f0f\uff08Solo\u3001Mono\u3001Echo\uff09\u9a8c\u8bc1\u5176\u6548\u679c\uff0c\u53d1\u73b0Echo\u6a21\u5f0f\u51c6\u786e\u7387\u6700\u9ad8\uff0cDeepSeek\u591a\u6837\u6027\u6700\u4f73\u3002", "motivation": "\u667a\u80fd\u6559\u80b2\u4e2d\u7684\u751f\u6210\u4ee3\u7406\u6a21\u578b\u5c1a\u4e0d\u6210\u719f\uff0c\u6559\u80b2\u573a\u666f\u7684\u590d\u6742\u6027\u548c\u4e2a\u6027\u5316\u9700\u6c42\u662f\u4e3b\u8981\u6311\u6218\u3002", "method": "\u63d0\u51faAgentSME\u6846\u67b6\uff0c\u91c7\u7528\u4e09\u79cd\u901a\u4fe1\u6a21\u5f0f\uff08Solo\u3001Mono\u3001Echo\uff09\uff0c\u4ee5\u51c6\u786e\u6027\u548c\u591a\u6837\u6027\u4e3a\u8bc4\u4f30\u6307\u6807\uff0c\u6d4b\u8bd5\u516d\u79cdLLM\u3002", "result": "Echo\u6a21\u5f0f\u51c6\u786e\u7387\u6700\u9ad8\uff0cDeepSeek\u591a\u6837\u6027\u6700\u4f73\u3002", "conclusion": "\u7814\u7a76\u4e3a\u63d0\u5347\u4ee3\u7406\u5b66\u4e60\u80fd\u529b\u548c\u667a\u80fd\u6559\u80b2\u6a21\u578b\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u4fe1\u606f\u3002"}}
{"id": "2508.03117", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.03117", "abs": "https://arxiv.org/abs/2508.03117", "authors": ["Vinicius Lima", "Dzung T. Phan", "Jayant Kalagnanam", "Dhaval Patel", "Nianjun Zhou"], "title": "Toward a Trustworthy Optimization Modeling Agent via Verifiable Synthetic Data Generation", "comment": "25 pages", "summary": "We present a framework for training trustworthy large language model (LLM)\nagents for optimization modeling via a verifiable synthetic data generation\npipeline. Focusing on linear and mixed-integer linear programming, our approach\nbegins with structured symbolic representations and systematically produces\nnatural language descriptions, mathematical formulations, and solver-executable\ncode. By programmatically constructing each instance with known optimal\nsolutions, the pipeline ensures full verifiability and enables automatic\nfiltering of low-quality demonstrations generated by teacher models. Each\ndataset instance includes a structured representation of the optimization\nproblem, a corresponding natural language description, the verified optimal\nsolution, and step-by-step demonstrations - generated by a teacher model - that\nshow how to model and solve the problem across multiple optimization modeling\nlanguages. This enables supervised fine-tuning of open-source LLMs specifically\ntailored to optimization tasks. To operationalize this pipeline, we introduce\nOptiTrust, a modular LLM agent that performs multi-stage translation from\nnatural language to solver-ready code, leveraging stepwise demonstrations,\nmulti-language inference, and majority-vote cross-validation. Our agent\nachieves state-of-the-art performance on standard benchmarks. Out of 7\ndatasets, it achieves the highest accuracy on six and outperforms the next-best\nalgorithm by at least 8 percentage on three of them. Our approach provides a\nscalable, verifiable, and principled path toward building reliable LLM agents\nfor real-world optimization applications.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7\u53ef\u9a8c\u8bc1\u5408\u6210\u6570\u636e\u751f\u6210\u7ba1\u9053\u8bad\u7ec3\u53ef\u4fe1\u8d56\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u4ee3\u7406\u7684\u6846\u67b6\uff0c\u4e13\u6ce8\u4e8e\u7ebf\u6027\u548c\u6df7\u5408\u6574\u6570\u7ebf\u6027\u89c4\u5212\uff0c\u5b9e\u73b0\u4e86\u4ece\u7ed3\u6784\u5316\u7b26\u53f7\u8868\u793a\u5230\u81ea\u7136\u8bed\u8a00\u63cf\u8ff0\u3001\u6570\u5b66\u516c\u5f0f\u548c\u53ef\u6267\u884c\u4ee3\u7801\u7684\u7cfb\u7edf\u751f\u6210\u3002", "motivation": "\u4e3a\u4f18\u5316\u5efa\u6a21\u4efb\u52a1\u6784\u5efa\u53ef\u9760\u4e14\u53ef\u9a8c\u8bc1\u7684LLM\u4ee3\u7406\uff0c\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u4e2d\u6570\u636e\u8d28\u91cf\u548c\u53ef\u9a8c\u8bc1\u6027\u4e0d\u8db3\u7684\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u7a0b\u5e8f\u5316\u6784\u5efa\u6bcf\u4e2a\u5b9e\u4f8b\uff08\u5305\u542b\u5df2\u77e5\u6700\u4f18\u89e3\uff09\uff0c\u751f\u6210\u81ea\u7136\u8bed\u8a00\u63cf\u8ff0\u3001\u6570\u5b66\u516c\u5f0f\u548c\u4ee3\u7801\uff0c\u5e76\u5229\u7528\u6559\u5e08\u6a21\u578b\u751f\u6210\u9010\u6b65\u6f14\u793a\uff0c\u5b9e\u73b0\u5f00\u6e90LLM\u7684\u76d1\u7763\u5fae\u8c03\u3002", "result": "OptiTrust\u4ee3\u7406\u5728\u6807\u51c6\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u6700\u4f73\uff0c7\u4e2a\u6570\u636e\u96c6\u4e2d6\u4e2a\u51c6\u786e\u7387\u6700\u9ad8\uff0c\u5176\u4e2d3\u4e2a\u6bd4\u6b21\u4f18\u7b97\u6cd5\u9ad8\u51fa\u81f3\u5c118\u4e2a\u767e\u5206\u70b9\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u6784\u5efa\u53ef\u9760LLM\u4ee3\u7406\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u3001\u53ef\u9a8c\u8bc1\u4e14\u539f\u5219\u6027\u7684\u8def\u5f84\uff0c\u9002\u7528\u4e8e\u5b9e\u9645\u4f18\u5316\u5e94\u7528\u3002"}}
{"id": "2508.03149", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.03149", "abs": "https://arxiv.org/abs/2508.03149", "authors": ["Linda Smail", "David Santandreu Calonge", "Firuz Kamalov", "Nur H. Orak"], "title": "Can Large Language Models Bridge the Gap in Environmental Knowledge?", "comment": "20 pages, 3 figures, 7 tables. No external funding", "summary": "This research investigates the potential of Artificial Intelligence (AI)\nmodels to bridge the knowledge gap in environmental education among university\nstudents. By focusing on prominent large language models (LLMs) such as\nGPT-3.5, GPT-4, GPT-4o, Gemini, Claude Sonnet, and Llama 2, the study assesses\ntheir effectiveness in conveying environmental concepts and, consequently,\nfacilitating environmental education. The investigation employs a standardized\ntool, the Environmental Knowledge Test (EKT-19), supplemented by targeted\nquestions, to evaluate the environmental knowledge of university students in\ncomparison to the responses generated by the AI models. The results of this\nstudy suggest that while AI models possess a vast, readily accessible, and\nvalid knowledge base with the potential to empower both students and academic\nstaff, a human discipline specialist in environmental sciences may still be\nnecessary to validate the accuracy of the information provided.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8AI\u6a21\u578b\uff08\u5982GPT-3.5\u3001GPT-4\u7b49\uff09\u5728\u5f25\u8865\u5927\u5b66\u751f\u73af\u5883\u6559\u80b2\u77e5\u8bc6\u7f3a\u53e3\u65b9\u9762\u7684\u6f5c\u529b\uff0c\u53d1\u73b0\u5176\u867d\u5177\u5907\u4e30\u5bcc\u77e5\u8bc6\u5e93\uff0c\u4f46\u4ecd\u9700\u4eba\u7c7b\u4e13\u5bb6\u9a8c\u8bc1\u4fe1\u606f\u51c6\u786e\u6027\u3002", "motivation": "\u89e3\u51b3\u5927\u5b66\u751f\u73af\u5883\u6559\u80b2\u4e2d\u7684\u77e5\u8bc6\u7f3a\u53e3\u95ee\u9898\uff0c\u63a2\u7d22AI\u6a21\u578b\u4f5c\u4e3a\u8f85\u52a9\u5de5\u5177\u7684\u53ef\u884c\u6027\u3002", "method": "\u4f7f\u7528\u6807\u51c6\u5316\u5de5\u5177EKT-19\u53ca\u9488\u5bf9\u6027\u95ee\u9898\uff0c\u8bc4\u4f30AI\u6a21\u578b\u4e0e\u5927\u5b66\u751f\u5728\u73af\u5883\u77e5\u8bc6\u4e0a\u7684\u8868\u73b0\u5bf9\u6bd4\u3002", "result": "AI\u6a21\u578b\u5177\u5907\u4e30\u5bcc\u4e14\u6613\u83b7\u53d6\u7684\u77e5\u8bc6\u5e93\uff0c\u4f46\u9700\u73af\u5883\u79d1\u5b66\u4e13\u5bb6\u9a8c\u8bc1\u4fe1\u606f\u51c6\u786e\u6027\u3002", "conclusion": "AI\u6a21\u578b\u53ef\u8f85\u52a9\u73af\u5883\u6559\u80b2\uff0c\u4f46\u9700\u7ed3\u5408\u4eba\u7c7b\u4e13\u5bb6\u4ee5\u786e\u4fdd\u4fe1\u606f\u51c6\u786e\u6027\u3002"}}
{"id": "2508.03167", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.03167", "abs": "https://arxiv.org/abs/2508.03167", "authors": ["Charles Tapley Hoyt", "Craig Bakker", "Richard J. Callahan", "Joseph Cottam", "August George", "Benjamin M. Gyori", "Haley M. Hummel", "Nathaniel Merrill", "Sara Mohammad Taheri", "Pruthvi Prakash Navada", "Marc-Antoine Parent", "Adam Rupe", "Olga Vitek", "Jeremy Zucker"], "title": "Causal identification with $Y_0$", "comment": null, "summary": "We present the $Y_0$ Python package, which implements causal identification\nalgorithms that apply interventional, counterfactual, and transportability\nqueries to data from (randomized) controlled trials, observational studies, or\nmixtures thereof. $Y_0$ focuses on the qualitative investigation of causation,\nhelping researchers determine whether a causal relationship can be estimated\nfrom available data before attempting to estimate how strong that relationship\nis. Furthermore, $Y_0$ provides guidance on how to transform the causal query\ninto a symbolic estimand that can be non-parametrically estimated from the\navailable data. $Y_0$ provides a domain-specific language for representing\ncausal queries and estimands as symbolic probabilistic expressions, tools for\nrepresenting causal graphical models with unobserved confounders, such as\nacyclic directed mixed graphs (ADMGs), and implementations of numerous\nidentification algorithms from the recent causal inference literature. The\n$Y_0$ source code can be found under the MIT License at\nhttps://github.com/y0-causal-inference/y0 and it can be installed with pip\ninstall y0.", "AI": {"tldr": "$Y_0$\u662f\u4e00\u4e2aPython\u5305\uff0c\u7528\u4e8e\u5b9e\u73b0\u56e0\u679c\u8bc6\u522b\u7b97\u6cd5\uff0c\u652f\u6301\u5e72\u9884\u3001\u53cd\u4e8b\u5b9e\u548c\u53ef\u8fc1\u79fb\u6027\u67e5\u8be2\uff0c\u5e2e\u52a9\u7814\u7a76\u8005\u5b9a\u6027\u5206\u6790\u56e0\u679c\u5173\u7cfb\u3002", "motivation": "\u63d0\u4f9b\u5de5\u5177\u5e2e\u52a9\u7814\u7a76\u8005\u786e\u5b9a\u56e0\u679c\u5173\u7cfb\u662f\u5426\u53ef\u4ece\u6570\u636e\u4e2d\u4f30\u8ba1\uff0c\u5e76\u6307\u5bfc\u5982\u4f55\u5c06\u56e0\u679c\u67e5\u8be2\u8f6c\u5316\u4e3a\u53ef\u4f30\u8ba1\u7684\u7b26\u53f7\u8868\u8fbe\u5f0f\u3002", "method": "\u5b9e\u73b0\u591a\u79cd\u56e0\u679c\u8bc6\u522b\u7b97\u6cd5\uff0c\u652f\u6301\u56e0\u679c\u56fe\u6a21\u578b\uff08\u5982ADMGs\uff09\u548c\u7b26\u53f7\u6982\u7387\u8868\u8fbe\u5f0f\u3002", "result": "$Y_0$\u5305\u652f\u6301\u4ece\u968f\u673a\u5bf9\u7167\u8bd5\u9a8c\u3001\u89c2\u5bdf\u6027\u7814\u7a76\u6216\u5176\u6df7\u5408\u6570\u636e\u4e2d\u8fdb\u884c\u56e0\u679c\u5206\u6790\u3002", "conclusion": "$Y_0$\u4e3a\u56e0\u679c\u63a8\u7406\u63d0\u4f9b\u4e86\u5b9e\u7528\u7684\u5de5\u5177\u548c\u7b97\u6cd5\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u6570\u636e\u7c7b\u578b\u548c\u67e5\u8be2\u3002"}}
{"id": "2508.03173", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.03173", "abs": "https://arxiv.org/abs/2508.03173", "authors": ["Jingxuan Wei", "Caijun Jia", "Qi Chen", "Honghao He", "Linzhuang Sun", "Conghui He", "Lijun Wu", "Bihui Yu", "Cheng Tan"], "title": "Geoint-R1: Formalizing Multimodal Geometric Reasoning with Dynamic Auxiliary Constructions", "comment": null, "summary": "Mathematical geometric reasoning is essential for scientific discovery and\neducational development, requiring precise logic and rigorous formal\nverification. While recent advances in Multimodal Large Language Models (MLLMs)\nhave improved reasoning tasks, existing models typically struggle with formal\ngeometric reasoning, particularly when dynamically constructing and verifying\nauxiliary geometric elements. To address these challenges, we introduce\nGeoint-R1, a multimodal reasoning framework designed to generate formally\nverifiable geometric solutions from textual descriptions and visual diagrams.\nGeoint-R1 uniquely integrates auxiliary elements construction, formal reasoning\nrepresented via Lean4, and interactive visualization. To systematically\nevaluate and advance formal geometric reasoning, we propose the Geoint\nbenchmark, comprising 1,885 rigorously annotated geometry problems across\ndiverse topics such as plane, spatial, and solid geometry. Each problem\nincludes structured textual annotations, precise Lean4 code for auxiliary\nconstructions, and detailed solution steps verified by experts. Extensive\nexperiments demonstrate that Geoint-R1 significantly surpasses existing\nmultimodal and math-specific reasoning models, particularly on challenging\nproblems requiring explicit auxiliary element constructions.", "AI": {"tldr": "Geoint-R1\u662f\u4e00\u4e2a\u591a\u6a21\u6001\u63a8\u7406\u6846\u67b6\uff0c\u4e13\u6ce8\u4e8e\u751f\u6210\u53ef\u5f62\u5f0f\u5316\u9a8c\u8bc1\u7684\u51e0\u4f55\u89e3\u51b3\u65b9\u6848\uff0c\u901a\u8fc7\u7ed3\u5408\u8f85\u52a9\u5143\u7d20\u6784\u5efa\u3001Lean4\u5f62\u5f0f\u63a8\u7406\u548c\u4ea4\u4e92\u5f0f\u53ef\u89c6\u5316\uff0c\u663e\u8457\u63d0\u5347\u4e86\u73b0\u6709\u6a21\u578b\u5728\u51e0\u4f55\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3002", "motivation": "\u73b0\u6709\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u5f62\u5f0f\u5316\u51e0\u4f55\u63a8\u7406\uff0c\u5c24\u5176\u662f\u52a8\u6001\u6784\u5efa\u548c\u9a8c\u8bc1\u8f85\u52a9\u51e0\u4f55\u5143\u7d20\u65b9\u9762\u8868\u73b0\u4e0d\u4f73\uff0cGeoint-R1\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "Geoint-R1\u6574\u5408\u4e86\u8f85\u52a9\u5143\u7d20\u6784\u5efa\u3001Lean4\u5f62\u5f0f\u63a8\u7406\u548c\u4ea4\u4e92\u5f0f\u53ef\u89c6\u5316\uff0c\u5e76\u901a\u8fc7Geoint\u57fa\u51c6\uff08\u5305\u542b1,885\u4e2a\u51e0\u4f55\u95ee\u9898\uff09\u8fdb\u884c\u7cfb\u7edf\u8bc4\u4f30\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cGeoint-R1\u5728\u9700\u8981\u663e\u5f0f\u8f85\u52a9\u5143\u7d20\u6784\u5efa\u7684\u590d\u6742\u95ee\u9898\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6a21\u578b\u3002", "conclusion": "Geoint-R1\u4e3a\u5f62\u5f0f\u5316\u51e0\u4f55\u63a8\u7406\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u5e76\u901a\u8fc7\u57fa\u51c6\u6d4b\u8bd5\u9a8c\u8bc1\u4e86\u5176\u4f18\u8d8a\u6027\u3002"}}
{"id": "2508.03174", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.03174", "abs": "https://arxiv.org/abs/2508.03174", "authors": ["Tian-Fang Zhao", "Wen-Xi Yang"], "title": "InqEduAgent: Adaptive AI Learning Partners with Gaussian Process Augmentation", "comment": null, "summary": "Collaborative partnership matters in inquiry-oriented education. However,\nmost study partners are selected either rely on experience-based assignments\nwith little scientific planning or build on rule-based machine assistants,\nencountering difficulties in knowledge expansion and inadequate flexibility.\nThis paper proposes an LLM-empowered agent model for simulating and selecting\nlearning partners tailored to inquiry-oriented learning, named InqEduAgent.\nGenerative agents are designed to capture cognitive and evaluative features of\nlearners in real-world scenarios. Then, an adaptive matching algorithm with\nGaussian process augmentation is formulated to identify patterns within prior\nknowledge. Optimal learning-partner matches are provided for learners facing\ndifferent exercises. The experimental results show the optimal performance of\nInqEduAgent in most knowledge-learning scenarios and LLM environment with\ndifferent levels of capabilities. This study promotes the intelligent\nallocation of human-based learning partners and the formulation of AI-based\nlearning partners. The code, data, and appendix are publicly available at\nhttps://github.com/InqEduAgent/InqEduAgent.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eLLM\u7684\u667a\u80fd\u4ee3\u7406\u6a21\u578bInqEduAgent\uff0c\u7528\u4e8e\u6a21\u62df\u548c\u9009\u62e9\u9002\u5408\u63a2\u7a76\u5f0f\u5b66\u4e60\u7684\u5b66\u4e60\u4f19\u4f34\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u5728\u77e5\u8bc6\u6269\u5c55\u548c\u7075\u6d3b\u6027\u4e0a\u7684\u4e0d\u8db3\u3002", "motivation": "\u63a2\u7a76\u5f0f\u6559\u80b2\u4e2d\u5b66\u4e60\u4f19\u4f34\u7684\u9009\u62e9\u901a\u5e38\u4f9d\u8d56\u7ecf\u9a8c\u6216\u89c4\u5219\uff0c\u7f3a\u4e4f\u79d1\u5b66\u89c4\u5212\u548c\u7075\u6d3b\u6027\uff0c\u9650\u5236\u4e86\u77e5\u8bc6\u6269\u5c55\u3002", "method": "\u8bbe\u8ba1\u4e86\u751f\u6210\u5f0f\u4ee3\u7406\u6355\u6349\u5b66\u4e60\u8005\u7684\u8ba4\u77e5\u548c\u8bc4\u4f30\u7279\u5f81\uff0c\u5e76\u91c7\u7528\u9ad8\u65af\u8fc7\u7a0b\u589e\u5f3a\u7684\u81ea\u9002\u5e94\u5339\u914d\u7b97\u6cd5\u8bc6\u522b\u5148\u9a8c\u77e5\u8bc6\u6a21\u5f0f\u3002", "result": "\u5b9e\u9a8c\u8868\u660eInqEduAgent\u5728\u591a\u79cd\u77e5\u8bc6\u5b66\u4e60\u573a\u666f\u548c\u4e0d\u540c\u80fd\u529b\u7684LLM\u73af\u5883\u4e2d\u8868\u73b0\u6700\u4f18\u3002", "conclusion": "\u8be5\u7814\u7a76\u63a8\u52a8\u4e86\u57fa\u4e8e\u4eba\u7c7b\u7684\u5b66\u4e60\u4f19\u4f34\u667a\u80fd\u5206\u914d\u548cAI\u5b66\u4e60\u4f19\u4f34\u7684\u5236\u5b9a\uff0c\u76f8\u5173\u8d44\u6e90\u5df2\u516c\u5f00\u3002"}}
{"id": "2508.03251", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.03251", "abs": "https://arxiv.org/abs/2508.03251", "authors": ["Osama Mohammed", "Jiaxin Pan", "Mojtaba Nayyeri", "Daniel Hern\u00e1ndez", "Steffen Staab"], "title": "Full-History Graphs with Edge-Type Decoupled Networks for Temporal Reasoning", "comment": "European Conference of Artificial Intelligence 2025", "summary": "Modeling evolving interactions among entities is critical in many real-world\ntasks. For example, predicting driver maneuvers in traffic requires tracking\nhow neighboring vehicles accelerate, brake, and change lanes relative to one\nanother over consecutive frames. Likewise, detecting financial fraud hinges on\nfollowing the flow of funds through successive transactions as they propagate\nthrough the network. Unlike classic time-series forecasting, these settings\ndemand reasoning over who interacts with whom and when, calling for a\ntemporal-graph representation that makes both the relations and their evolution\nexplicit. Existing temporal-graph methods typically use snapshot graphs to\nencode temporal evolution. We introduce a full-history graph that instantiates\none node for every entity at every time step and separates two edge sets: (i)\nintra-time-step edges that capture relations within a single frame and (ii)\ninter-time-step edges that connect an entity to itself at consecutive steps. To\nlearn on this graph we design an Edge-Type Decoupled Network (ETDNet) with\nparallel modules: a graph-attention module aggregates information along\nintra-time-step edges, a multi-head temporal-attention module attends over an\nentity's inter-time-step history, and a fusion module combines the two messages\nafter every layer. Evaluated on driver-intention prediction (Waymo) and Bitcoin\nfraud detection (Elliptic++), ETDNet consistently surpasses strong baselines,\nlifting Waymo joint accuracy to 75.6\\% (vs. 74.1\\%) and raising Elliptic++\nillicit-class F1 to 88.1\\% (vs. 60.4\\%). These gains demonstrate the benefit of\nrepresenting structural and temporal relations as distinct edges in a single\ngraph.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5168\u5386\u53f2\u56fe\u8868\u793a\u65b9\u6cd5\u548cETDNet\u7f51\u7edc\uff0c\u7528\u4e8e\u5efa\u6a21\u5b9e\u4f53\u95f4\u968f\u65f6\u95f4\u6f14\u5316\u7684\u4ea4\u4e92\u5173\u7cfb\uff0c\u663e\u8457\u63d0\u5347\u4e86\u9a7e\u9a76\u610f\u56fe\u9884\u6d4b\u548c\u6bd4\u7279\u5e01\u6b3a\u8bc8\u68c0\u6d4b\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u5b9e\u4efb\u52a1\u4e2d\u9700\u8981\u5efa\u6a21\u5b9e\u4f53\u95f4\u52a8\u6001\u4ea4\u4e92\u5173\u7cfb\uff08\u5982\u4ea4\u901a\u4e2d\u7684\u8f66\u8f86\u884c\u4e3a\u6216\u91d1\u878d\u4ea4\u6613\u4e2d\u7684\u8d44\u91d1\u6d41\u52a8\uff09\uff0c\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u4f7f\u7528\u5feb\u7167\u56fe\uff0c\u65e0\u6cd5\u5145\u5206\u6355\u6349\u5173\u7cfb\u548c\u65f6\u95f4\u7684\u6f14\u5316\u3002", "method": "\u63d0\u51fa\u5168\u5386\u53f2\u56fe\u8868\u793a\uff0c\u533a\u5206\u540c\u4e00\u65f6\u95f4\u6b65\u548c\u8de8\u65f6\u95f4\u6b65\u7684\u8fb9\uff0c\u5e76\u8bbe\u8ba1ETDNet\u7f51\u7edc\uff0c\u7ed3\u5408\u56fe\u6ce8\u610f\u529b\u548c\u65f6\u95f4\u6ce8\u610f\u529b\u6a21\u5757\u3002", "result": "\u5728Waymo\u548cElliptic++\u6570\u636e\u96c6\u4e0a\uff0cETDNet\u5206\u522b\u5c06\u8054\u5408\u51c6\u786e\u7387\u63d0\u5347\u81f375.6%\u548c\u975e\u6cd5\u7c7bF1\u5206\u6570\u63d0\u5347\u81f388.1%\uff0c\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "\u901a\u8fc7\u5c06\u7ed3\u6784\u548c\u65f6\u95f4\u5173\u7cfb\u8868\u793a\u4e3a\u56fe\u4e2d\u7684\u4e0d\u540c\u8fb9\uff0cETDNet\u663e\u8457\u63d0\u5347\u4e86\u52a8\u6001\u4ea4\u4e92\u5efa\u6a21\u7684\u6027\u80fd\u3002"}}
{"id": "2508.03284", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.03284", "abs": "https://arxiv.org/abs/2508.03284", "authors": ["Shaofeng Yin", "Ting Lei", "Yang Liu"], "title": "ToolVQA: A Dataset for Multi-step Reasoning VQA with External Tools", "comment": null, "summary": "Integrating external tools into Large Foundation Models (LFMs) has emerged as\na promising approach to enhance their problem-solving capabilities. While\nexisting studies have demonstrated strong performance in tool-augmented Visual\nQuestion Answering (VQA), recent benchmarks reveal significant gaps in\nreal-world tool-use proficiency, particularly in functionally diverse\nmultimodal settings requiring multi-step reasoning. In this work, we introduce\nToolVQA, a large-scale multimodal dataset comprising 23K instances, designed to\nbridge this gap. Unlike previous datasets that rely on synthetic scenarios and\nsimplified queries, ToolVQA features real-world visual contexts and challenging\nimplicit multi-step reasoning tasks, better aligning with real user\ninteractions. To construct this dataset, we propose ToolEngine, a novel data\ngeneration pipeline that employs Depth-First Search (DFS) with a dynamic\nin-context example matching mechanism to simulate human-like tool-use\nreasoning. ToolVQA encompasses 10 multimodal tools across 7 diverse task\ndomains, with an average inference length of 2.78 reasoning steps per instance.\nThe fine-tuned 7B LFMs on ToolVQA not only achieve impressive performance on\nour test set but also surpass the large close-sourced model GPT-3.5-turbo on\nvarious out-of-distribution (OOD) datasets, demonstrating strong\ngeneralizability to real-world tool-use scenarios.", "AI": {"tldr": "ToolVQA\u662f\u4e00\u4e2a\u5927\u89c4\u6a21\u591a\u6a21\u6001\u6570\u636e\u96c6\uff0c\u65e8\u5728\u63d0\u5347\u5927\u578b\u57fa\u7840\u6a21\u578b\u5728\u771f\u5b9e\u4e16\u754c\u5de5\u5177\u4f7f\u7528\u4e2d\u7684\u591a\u6b65\u63a8\u7406\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u5728\u5de5\u5177\u589e\u5f3a\u7684\u89c6\u89c9\u95ee\u7b54\uff08VQA\uff09\u4e2d\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u771f\u5b9e\u4e16\u754c\u591a\u6a21\u6001\u73af\u5883\u4e2d\u7684\u5de5\u5177\u4f7f\u7528\u80fd\u529b\u4ecd\u6709\u663e\u8457\u4e0d\u8db3\u3002", "method": "\u63d0\u51faToolVQA\u6570\u636e\u96c6\u548cToolEngine\u6570\u636e\u751f\u6210\u7ba1\u9053\uff0c\u91c7\u7528\u6df1\u5ea6\u4f18\u5148\u641c\u7d22\uff08DFS\uff09\u548c\u52a8\u6001\u4e0a\u4e0b\u6587\u793a\u4f8b\u5339\u914d\u673a\u5236\u6a21\u62df\u4eba\u7c7b\u5de5\u5177\u4f7f\u7528\u63a8\u7406\u3002", "result": "\u5728ToolVQA\u4e0a\u5fae\u8c03\u76847B LFMs\u5728\u6d4b\u8bd5\u96c6\u548c\u591a\u4e2aOOD\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u751a\u81f3\u8d85\u8fc7GPT-3.5-turbo\u3002", "conclusion": "ToolVQA\u548cToolEngine\u6709\u6548\u63d0\u5347\u4e86\u6a21\u578b\u5728\u771f\u5b9e\u4e16\u754c\u5de5\u5177\u4f7f\u7528\u573a\u666f\u4e2d\u7684\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2508.03341", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.03341", "abs": "https://arxiv.org/abs/2508.03341", "authors": ["Jiayan Nan", "Wenquan Ma", "Wenlong Wu", "Yize Chen"], "title": "Nemori: Self-Organizing Agent Memory Inspired by Cognitive Science", "comment": null, "summary": "Large Language Models (LLMs) demonstrate remarkable capabilities, yet their\ninability to maintain persistent memory in long contexts limits their\neffectiveness as autonomous agents in long-term interactions. While existing\nmemory systems have made progress, their reliance on arbitrary granularity for\ndefining the basic memory unit and passive, rule-based mechanisms for knowledge\nextraction limits their capacity for genuine learning and evolution. To address\nthese foundational limitations, we present Nemori, a novel self-organizing\nmemory architecture inspired by human cognitive principles. Nemori's core\ninnovation is twofold: First, its Two-Step Alignment Principle, inspired by\nEvent Segmentation Theory, provides a principled, top-down method for\nautonomously organizing the raw conversational stream into semantically\ncoherent episodes, solving the critical issue of memory granularity. Second,\nits Predict-Calibrate Principle, inspired by the Free-energy Principle, enables\nthe agent to proactively learn from prediction gaps, moving beyond pre-defined\nheuristics to achieve adaptive knowledge evolution. This offers a viable path\ntoward handling the long-term, dynamic workflows of autonomous agents.\nExtensive experiments on the LoCoMo and LongMemEval benchmarks demonstrate that\nNemori significantly outperforms prior state-of-the-art systems, with its\nadvantage being particularly pronounced in longer contexts.", "AI": {"tldr": "Nemori\u662f\u4e00\u79cd\u53d7\u4eba\u7c7b\u8ba4\u77e5\u542f\u53d1\u7684\u81ea\u7ec4\u7ec7\u8bb0\u5fc6\u67b6\u6784\uff0c\u901a\u8fc7\u4e24\u6b65\u5bf9\u9f50\u539f\u5219\u548c\u9884\u6d4b\u6821\u51c6\u539f\u5219\u89e3\u51b3LLMs\u5728\u957f\u671f\u4ea4\u4e92\u4e2d\u7684\u8bb0\u5fc6\u95ee\u9898\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u7cfb\u7edf\u3002", "motivation": "\u89e3\u51b3\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u957f\u671f\u4ea4\u4e92\u4e2d\u65e0\u6cd5\u7ef4\u6301\u6301\u4e45\u8bb0\u5fc6\u7684\u95ee\u9898\uff0c\u73b0\u6709\u8bb0\u5fc6\u7cfb\u7edf\u4f9d\u8d56\u88ab\u52a8\u89c4\u5219\u4e14\u7f3a\u4e4f\u81ea\u9002\u5e94\u5b66\u4e60\u80fd\u529b\u3002", "method": "\u63d0\u51faNemori\u67b6\u6784\uff0c\u91c7\u7528\u4e24\u6b65\u5bf9\u9f50\u539f\u5219\u7ec4\u7ec7\u8bed\u4e49\u8fde\u8d2f\u7684\u8bb0\u5fc6\u7247\u6bb5\uff0c\u5e76\u901a\u8fc7\u9884\u6d4b\u6821\u51c6\u539f\u5219\u4e3b\u52a8\u5b66\u4e60\u9884\u6d4b\u5dee\u8ddd\u3002", "result": "\u5728LoCoMo\u548cLongMemEval\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cNemori\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u7cfb\u7edf\uff0c\u5c24\u5176\u5728\u957f\u4e0a\u4e0b\u6587\u573a\u666f\u4e2d\u8868\u73b0\u7a81\u51fa\u3002", "conclusion": "Nemori\u4e3a\u81ea\u4e3b\u4ee3\u7406\u7684\u957f\u671f\u52a8\u6001\u5de5\u4f5c\u6d41\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5c55\u793a\u4e86\u81ea\u9002\u5e94\u77e5\u8bc6\u6f14\u5316\u7684\u6f5c\u529b\u3002"}}
{"id": "2508.03345", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.03345", "abs": "https://arxiv.org/abs/2508.03345", "authors": ["Xingdan Wang", "Jiayi He", "Zhiqing Tang", "Jianxiong Guo", "Jiong Lou", "Liping Qian", "Tian Wang", "Weijia Jia"], "title": "Adaptive AI Agent Placement and Migration in Edge Intelligence Systems", "comment": null, "summary": "The rise of LLMs such as ChatGPT and Claude fuels the need for AI agents\ncapable of real-time task handling. However, migrating data-intensive,\nmulti-modal edge workloads to cloud data centers, traditionally used for agent\ndeployment, introduces significant latency. Deploying AI agents at the edge\nimproves efficiency and reduces latency. However, edge environments present\nchallenges due to limited and heterogeneous resources. Maintaining QoS for\nmobile users necessitates agent migration, which is complicated by the\ncomplexity of AI agents coordinating LLMs, task planning, memory, and external\ntools. This paper presents the first systematic deployment and management\nsolution for LLM-based AI agents in dynamic edge environments. We propose a\nnovel adaptive framework for AI agent placement and migration in edge\nintelligence systems. Our approach models resource constraints and\nlatency/cost, leveraging ant colony algorithms and LLM-based optimization for\nefficient decision-making. It autonomously places agents to optimize resource\nutilization and QoS and enables lightweight agent migration by transferring\nonly essential state. Implemented on a distributed system using AgentScope and\nvalidated across globally distributed edge servers, our solution significantly\nreduces deployment latency and migration costs.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u52a8\u6001\u8fb9\u7f18\u73af\u5883\u4e2d\u57fa\u4e8eLLM\u7684AI\u4ee3\u7406\u7684\u90e8\u7f72\u548c\u7ba1\u7406\u65b9\u6848\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u6846\u67b6\u4f18\u5316\u8d44\u6e90\u5229\u7528\u548c\u670d\u52a1\u8d28\u91cf\u3002", "motivation": "\u968f\u7740LLM\uff08\u5982ChatGPT\u548cClaude\uff09\u7684\u5174\u8d77\uff0c\u9700\u8981\u80fd\u591f\u5b9e\u65f6\u5904\u7406\u4efb\u52a1\u7684AI\u4ee3\u7406\uff0c\u4f46\u4f20\u7edf\u4e91\u7aef\u90e8\u7f72\u5f15\u5165\u9ad8\u5ef6\u8fdf\uff0c\u800c\u8fb9\u7f18\u73af\u5883\u8d44\u6e90\u6709\u9650\u4e14\u5f02\u6784\uff0c\u9700\u89e3\u51b3\u4ee3\u7406\u8fc1\u79fb\u548c\u8d44\u6e90\u4f18\u5316\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u9002\u5e94\u6846\u67b6\uff0c\u7ed3\u5408\u8681\u7fa4\u7b97\u6cd5\u548c\u57fa\u4e8eLLM\u7684\u4f18\u5316\uff0c\u5efa\u6a21\u8d44\u6e90\u7ea6\u675f\u548c\u5ef6\u8fdf/\u6210\u672c\uff0c\u5b9e\u73b0\u9ad8\u6548\u51b3\u7b56\u548c\u8f7b\u91cf\u7ea7\u4ee3\u7406\u8fc1\u79fb\u3002", "result": "\u5728\u5206\u5e03\u5f0f\u7cfb\u7edf\uff08AgentScope\uff09\u4e0a\u5b9e\u73b0\uff0c\u5e76\u5728\u5168\u7403\u8fb9\u7f18\u670d\u52a1\u5668\u9a8c\u8bc1\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u90e8\u7f72\u5ef6\u8fdf\u548c\u8fc1\u79fb\u6210\u672c\u3002", "conclusion": "\u8be5\u65b9\u6848\u4e3a\u52a8\u6001\u8fb9\u7f18\u73af\u5883\u4e2dLLM-based AI\u4ee3\u7406\u7684\u9ad8\u6548\u90e8\u7f72\u548c\u7ba1\u7406\u63d0\u4f9b\u4e86\u7cfb\u7edf\u5316\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.03346", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.03346", "abs": "https://arxiv.org/abs/2508.03346", "authors": ["Zeju Li", "Jianyuan Zhong", "Ziyang Zheng", "Xiangyu Wen", "Zhijian Xu", "Yingying Cheng", "Fan Zhang", "Qiang Xu"], "title": "Compressing Chain-of-Thought in LLMs via Step Entropy", "comment": null, "summary": "Large Language Models (LLMs) using Chain-of-Thought (CoT) prompting excel at\ncomplex reasoning but generate verbose thought processes with considerable\nredundancy, leading to increased inference costs and reduced efficiency. We\nintroduce a novel CoT compression framework based on step entropy, a metric\nthat quantifies the informational contribution of individual reasoning steps to\nidentify redundancy. Through theoretical analysis and extensive empirical\nvalidation on mathematical reasoning benchmarks, we demonstrate that steps with\nlow entropy are indeed highly redundant. Our experiments reveal that an\nastonishing 80\\% of low-entropy intermediate steps can be pruned with minor\ndegradation in the final answer accuracy across DeepSeek-R1-7B, 14B and\nQwen3-8B. This finding sharply contrasts with random or high-entropy pruning,\nwhich severely impairs reasoning performance. Building on this, we propose a\nnovel two-stage training strategy combining Supervised Fine-Tuning (SFT) and\nGroup Relative Policy Optimization (GRPO) reinforcement learning. This approach\nenables LLMs to autonomously learn to generate compressed COTs during inference\nby strategically incorporating [SKIP] tokens. Our method significantly enhances\nLLM inference efficiency while rigorously preserving accuracy, offering\nprofound implications for practical LLM deployment and a deeper understanding\nof reasoning structures.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6b65\u9aa4\u71b5\u7684CoT\u538b\u7f29\u6846\u67b6\uff0c\u901a\u8fc7\u8bc6\u522b\u5197\u4f59\u6b65\u9aa4\u663e\u8457\u63d0\u5347LLM\u63a8\u7406\u6548\u7387\uff0c\u540c\u65f6\u4fdd\u6301\u51c6\u786e\u6027\u3002", "motivation": "LLMs\u4f7f\u7528CoT\u63d0\u793a\u65f6\u751f\u6210\u5197\u4f59\u7684\u601d\u7ef4\u8fc7\u7a0b\uff0c\u589e\u52a0\u4e86\u63a8\u7406\u6210\u672c\u5e76\u964d\u4f4e\u4e86\u6548\u7387\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u65b9\u6cd5\u6765\u538b\u7f29\u8fd9\u4e9b\u5197\u4f59\u6b65\u9aa4\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u6b65\u9aa4\u71b5\u7684\u538b\u7f29\u6846\u67b6\uff0c\u7ed3\u5408SFT\u548cGRPO\u5f3a\u5316\u5b66\u4e60\u7684\u8bad\u7ec3\u7b56\u7565\uff0c\u8ba9LLMs\u5728\u63a8\u7406\u65f6\u751f\u6210\u538b\u7f29\u7684CoT\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c80%\u7684\u4f4e\u71b5\u6b65\u9aa4\u53ef\u88ab\u526a\u679d\uff0c\u4e14\u5bf9\u6700\u7ec8\u7b54\u6848\u51c6\u786e\u6027\u5f71\u54cd\u6781\u5c0f\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86LLM\u63a8\u7406\u6548\u7387\uff0c\u540c\u65f6\u4fdd\u6301\u51c6\u786e\u6027\uff0c\u5bf9\u5b9e\u9645\u90e8\u7f72\u548c\u63a8\u7406\u7ed3\u6784\u7406\u89e3\u6709\u91cd\u8981\u610f\u4e49\u3002"}}
{"id": "2508.03360", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.03360", "abs": "https://arxiv.org/abs/2508.03360", "authors": ["Feng Rui", "Zhiyao Luo", "Wei Wang", "Yuting Song", "Yong Liu", "Tingting Zhu", "Jianqing Li", "Xingyao Wang"], "title": "CogBench: A Large Language Model Benchmark for Multilingual Speech-Based Cognitive Impairment Assessment", "comment": "19 pages, 9 figures, 12 tables", "summary": "Automatic assessment of cognitive impairment from spontaneous speech offers a\npromising, non-invasive avenue for early cognitive screening. However, current\napproaches often lack generalizability when deployed across different languages\nand clinical settings, limiting their practical utility. In this study, we\npropose CogBench, the first benchmark designed to evaluate the cross-lingual\nand cross-site generalizability of large language models (LLMs) for\nspeech-based cognitive impairment assessment. Using a unified multimodal\npipeline, we evaluate model performance on three speech datasets spanning\nEnglish and Mandarin: ADReSSo, NCMMSC2021-AD, and a newly collected test set,\nCIR-E. Our results show that conventional deep learning models degrade\nsubstantially when transferred across domains. In contrast, LLMs equipped with\nchain-of-thought prompting demonstrate better adaptability, though their\nperformance remains sensitive to prompt design. Furthermore, we explore\nlightweight fine-tuning of LLMs via Low-Rank Adaptation (LoRA), which\nsignificantly improves generalization in target domains. These findings offer a\ncritical step toward building clinically useful and linguistically robust\nspeech-based cognitive assessment tools.", "AI": {"tldr": "CogBench\u662f\u4e00\u4e2a\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u8de8\u8bed\u8a00\u548c\u8de8\u4e34\u5e8a\u73af\u5883\u4e2d\u7528\u4e8e\u8ba4\u77e5\u969c\u788d\u8bc4\u4f30\u7684\u6cdb\u5316\u80fd\u529b\u7684\u57fa\u51c6\u6d4b\u8bd5\u3002\u7814\u7a76\u53d1\u73b0\uff0c\u4f20\u7edf\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5728\u8de8\u57df\u8fc1\u79fb\u65f6\u6027\u80fd\u663e\u8457\u4e0b\u964d\uff0c\u800c\u91c7\u7528\u601d\u7ef4\u94fe\u63d0\u793a\u7684LLMs\u8868\u73b0\u66f4\u597d\uff0c\u4f46\u6027\u80fd\u4ecd\u53d7\u63d0\u793a\u8bbe\u8ba1\u5f71\u54cd\u3002\u8f7b\u91cf\u7ea7\u5fae\u8c03\uff08LoRA\uff09\u663e\u8457\u63d0\u5347\u4e86\u76ee\u6807\u57df\u4e2d\u7684\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8e\u81ea\u53d1\u8bed\u97f3\u7684\u8ba4\u77e5\u969c\u788d\u8bc4\u4f30\u65b9\u6cd5\u5728\u8de8\u8bed\u8a00\u548c\u4e34\u5e8a\u73af\u5883\u4e2d\u7684\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\uff0c\u9650\u5236\u4e86\u5176\u5b9e\u9645\u5e94\u7528\u3002", "method": "\u63d0\u51faCogBench\u57fa\u51c6\u6d4b\u8bd5\uff0c\u4f7f\u7528\u7edf\u4e00\u7684\u591a\u6a21\u6001\u7ba1\u9053\u8bc4\u4f30LLMs\u5728\u82f1\u8bed\u548c\u666e\u901a\u8bdd\u6570\u636e\u96c6\uff08ADReSSo\u3001NCMMSC2021-AD\u548cCIR-E\uff09\u4e2d\u7684\u8868\u73b0\uff0c\u5e76\u63a2\u7d22\u601d\u7ef4\u94fe\u63d0\u793a\u548cLoRA\u5fae\u8c03\u7684\u6548\u679c\u3002", "result": "\u4f20\u7edf\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u8de8\u57df\u6027\u80fd\u4e0b\u964d\u660e\u663e\uff0cLLMs\u8868\u73b0\u66f4\u4f18\u4f46\u53d7\u63d0\u793a\u8bbe\u8ba1\u5f71\u54cd\uff0cLoRA\u5fae\u8c03\u663e\u8457\u63d0\u5347\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u7814\u7a76\u4e3a\u6784\u5efa\u4e34\u5e8a\u5b9e\u7528\u4e14\u8bed\u8a00\u9c81\u68d2\u7684\u8bed\u97f3\u8ba4\u77e5\u8bc4\u4f30\u5de5\u5177\u63d0\u4f9b\u4e86\u91cd\u8981\u8fdb\u5c55\u3002"}}
{"id": "2508.03366", "categories": ["cs.AI", "cs.CL", "cs.LG", "cs.SC"], "pdf": "https://arxiv.org/pdf/2508.03366", "abs": "https://arxiv.org/abs/2508.03366", "authors": ["Michael K. Chen"], "title": "A Comparative Study of Neurosymbolic AI Approaches to Interpretable Logical Reasoning", "comment": "Accepted to NeSy 2025", "summary": "General logical reasoning, defined as the ability to reason deductively on\ndomain-agnostic tasks, continues to be a challenge for large language models\n(LLMs). Current LLMs fail to reason deterministically and are not\ninterpretable. As such, there has been a recent surge in interest in\nneurosymbolic AI, which attempts to incorporate logic into neural networks. We\nfirst identify two main neurosymbolic approaches to improving logical\nreasoning: (i) the integrative approach comprising models where symbolic\nreasoning is contained within the neural network, and (ii) the hybrid approach\ncomprising models where a symbolic solver, separate from the neural network,\nperforms symbolic reasoning. Both contain AI systems with promising results on\ndomain-specific logical reasoning benchmarks. However, their performance on\ndomain-agnostic benchmarks is understudied. To the best of our knowledge, there\nhas not been a comparison of the contrasting approaches that answers the\nfollowing question: Which approach is more promising for developing general\nlogical reasoning? To analyze their potential, the following best-in-class\ndomain-agnostic models are introduced: Logic Neural Network (LNN), which uses\nthe integrative approach, and LLM-Symbolic Solver (LLM-SS), which uses the\nhybrid approach. Using both models as case studies and representatives of each\napproach, our analysis demonstrates that the hybrid approach is more promising\nfor developing general logical reasoning because (i) its reasoning chain is\nmore interpretable, and (ii) it retains the capabilities and advantages of\nexisting LLMs. To support future works using the hybrid approach, we propose a\ngeneralizable framework based on LLM-SS that is modular by design,\nmodel-agnostic, domain-agnostic, and requires little to no human input.", "AI": {"tldr": "\u8bba\u6587\u6bd4\u8f83\u4e86\u4e24\u79cd\u795e\u7ecf\u7b26\u53f7AI\u65b9\u6cd5\uff08\u96c6\u6210\u4e0e\u6df7\u5408\uff09\u5728\u901a\u7528\u903b\u8f91\u63a8\u7406\u4e2d\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u6df7\u5408\u65b9\u6cd5\u66f4\u5177\u6f5c\u529b\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8eLLM-SS\u7684\u901a\u7528\u6846\u67b6\u3002", "motivation": "\u5f53\u524d\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5728\u901a\u7528\u903b\u8f91\u63a8\u7406\u4efb\u52a1\u4e2d\u8868\u73b0\u4e0d\u8db3\uff0c\u7f3a\u4e4f\u786e\u5b9a\u6027\u548c\u53ef\u89e3\u91ca\u6027\uff0c\u795e\u7ecf\u7b26\u53f7AI\u6210\u4e3a\u7814\u7a76\u70ed\u70b9\u3002", "method": "\u901a\u8fc7\u5bf9\u6bd4\u96c6\u6210\u65b9\u6cd5\uff08LNN\uff09\u548c\u6df7\u5408\u65b9\u6cd5\uff08LLM-SS\uff09\uff0c\u5206\u6790\u5b83\u4eec\u5728\u901a\u7528\u903b\u8f91\u63a8\u7406\u4e2d\u7684\u8868\u73b0\u3002", "result": "\u6df7\u5408\u65b9\u6cd5\uff08LLM-SS\uff09\u5728\u53ef\u89e3\u91ca\u6027\u548c\u4fdd\u7559LLM\u4f18\u52bf\u65b9\u9762\u8868\u73b0\u66f4\u4f18\u3002", "conclusion": "\u6df7\u5408\u65b9\u6cd5\u66f4\u9002\u5408\u5f00\u53d1\u901a\u7528\u903b\u8f91\u63a8\u7406\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u4e2a\u6a21\u5757\u5316\u3001\u9886\u57df\u65e0\u5173\u7684\u6846\u67b6\u652f\u6301\u672a\u6765\u7814\u7a76\u3002"}}
{"id": "2508.03368", "categories": ["cs.AI", "cs.GT"], "pdf": "https://arxiv.org/pdf/2508.03368", "abs": "https://arxiv.org/abs/2508.03368", "authors": ["Lucia Cipolina-Kun", "Marianna Nezhurina", "Jenia Jitsev"], "title": "Board Game Arena: A Framework and Benchmark for Assessing Large Language Models via Strategic Play", "comment": null, "summary": "The Board Game Arena library provides a framework for evaluating the decision\nmaking abilities of large language models (LLMs) through strategic board games\nimplemented in Google OpenSpiel library. The framework enables systematic\ncomparisons between LLM based agents and other agents (random, human,\nreinforcement learning agents, etc.) in various game scenarios by wrapping\nmultiple board and matrix games and supporting different agent types. It\nintegrates API access to models via LiteLLM, local model deployment via vLLM,\nand offers distributed execution through Ray. Additionally it provides\nextensive analysis tools for the LLM reasoning traces. This paper summarizes\nthe structure, key characteristics, and motivation of the repository,\nhighlighting how it contributes to the empirical evaluation of the reasoning of\nLLM and game-theoretic behavior", "AI": {"tldr": "Board Game Arena\u5e93\u901a\u8fc7\u6218\u7565\u68cb\u76d8\u6e38\u620f\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u51b3\u7b56\u80fd\u529b\uff0c\u652f\u6301\u591a\u79cd\u6e38\u620f\u573a\u666f\u548c\u4ee3\u7406\u7c7b\u578b\uff0c\u63d0\u4f9bAPI\u8bbf\u95ee\u548c\u5206\u5e03\u5f0f\u6267\u884c\uff0c\u5e76\u5206\u6790LLM\u63a8\u7406\u8f68\u8ff9\u3002", "motivation": "\u4e3aLLM\u7684\u63a8\u7406\u80fd\u529b\u548c\u535a\u5f08\u884c\u4e3a\u63d0\u4f9b\u5b9e\u8bc1\u8bc4\u4f30\u6846\u67b6\uff0c\u652f\u6301\u4e0e\u5176\u4ed6\u4ee3\u7406\uff08\u5982\u968f\u673a\u3001\u4eba\u7c7b\u3001\u5f3a\u5316\u5b66\u4e60\u4ee3\u7406\uff09\u7684\u7cfb\u7edf\u6bd4\u8f83\u3002", "method": "\u96c6\u6210Google OpenSpiel\u5e93\u7684\u68cb\u76d8\u548c\u77e9\u9635\u6e38\u620f\uff0c\u901a\u8fc7LiteLLM\u63d0\u4f9bAPI\u8bbf\u95ee\uff0cvLLM\u672c\u5730\u90e8\u7f72\uff0cRay\u5206\u5e03\u5f0f\u6267\u884c\uff0c\u5e76\u63d0\u4f9b\u63a8\u7406\u8f68\u8ff9\u5206\u6790\u5de5\u5177\u3002", "result": "\u6784\u5efa\u4e86\u4e00\u4e2a\u7cfb\u7edf\u5316\u7684\u8bc4\u4f30\u6846\u67b6\uff0c\u652f\u6301\u591a\u573a\u666f\u3001\u591a\u4ee3\u7406\u7c7b\u578b\u7684LLM\u51b3\u7b56\u80fd\u529b\u6d4b\u8bd5\u3002", "conclusion": "\u8be5\u5e93\u4e3aLLM\u7684\u63a8\u7406\u548c\u535a\u5f08\u884c\u4e3a\u7814\u7a76\u63d0\u4f9b\u4e86\u5b9e\u7528\u5de5\u5177\uff0c\u63a8\u52a8\u4e86\u76f8\u5173\u9886\u57df\u7684\u5b9e\u8bc1\u5206\u6790\u3002"}}
{"id": "2508.03396", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.03396", "abs": "https://arxiv.org/abs/2508.03396", "authors": ["Rui Zou", "Mengqi Wei", "Yutao Zhu", "Jirong Wen", "Xin Zhao", "Jing Chen"], "title": "Hide and Seek with LLMs: An Adversarial Game for Sneaky Error Generation and Self-Improving Diagnosis", "comment": null, "summary": "Large Language Models (LLMs) excel in reasoning and generation across\ndomains, but still struggle with identifying and diagnosing complex errors.\nThis stems mainly from training objectives that prioritize correct answers,\nlimiting exposure to and learning from errors. While recent studies have begun\nto address this by introducing error signals, most rely on shallow, static\nerrors, restricting improvement in deep diagnostic ability. To overcome this,\nwe propose Hide and Seek Game (HSG), a dynamic adversarial framework for error\ngeneration and diagnosis, and evaluate it on mathematical problem-solving. HSG\ninvolves two adversarial roles: Sneaky, which \"hides\" by generating subtle,\ndeceptive reasoning errors, and Diagnosis, which \"seeks\" to accurately detect\nthem. Through adversarial co-evolution, both error stealth and diagnostic\nprecision are enhanced. Experiments on several math reasoning tasks show that\nHSG significantly boosts error diagnosis, achieving 16.8\\%--31.4\\% higher\naccuracy than baselines like GPT-4o. We also release a challenging dataset of\ndeceptive errors and diagnostic annotations as a benchmark for future research.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u52a8\u6001\u5bf9\u6297\u6846\u67b6HSG\uff0c\u901a\u8fc7\u751f\u6210\u548c\u8bca\u65ad\u590d\u6742\u9519\u8bef\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\u7684\u8bca\u65ad\u80fd\u529b\uff0c\u5b9e\u9a8c\u663e\u793a\u5176\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u5728\u63a8\u7406\u548c\u751f\u6210\u65b9\u9762\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5728\u8bc6\u522b\u548c\u8bca\u65ad\u590d\u6742\u9519\u8bef\u65b9\u9762\u4ecd\u6709\u4e0d\u8db3\uff0c\u4e3b\u8981\u56e0\u8bad\u7ec3\u76ee\u6807\u504f\u5411\u6b63\u786e\u7b54\u6848\uff0c\u7f3a\u4e4f\u5bf9\u9519\u8bef\u7684\u5b66\u4e60\u3002", "method": "\u63d0\u51faHSG\u6846\u67b6\uff0c\u5305\u542b\u4e24\u4e2a\u5bf9\u6297\u89d2\u8272\uff1aSneaky\u751f\u6210\u9690\u853d\u9519\u8bef\uff0cDiagnosis\u68c0\u6d4b\u9519\u8bef\uff0c\u901a\u8fc7\u5bf9\u6297\u534f\u540c\u8fdb\u5316\u63d0\u5347\u80fd\u529b\u3002", "result": "\u5728\u6570\u5b66\u63a8\u7406\u4efb\u52a1\u4e2d\uff0cHSG\u6bd4\u57fa\u7ebf\u6a21\u578b\uff08\u5982GPT-4o\uff09\u7684\u8bca\u65ad\u51c6\u786e\u7387\u63d0\u9ad8\u4e8616.8%--31.4%\u3002", "conclusion": "HSG\u6709\u6548\u63d0\u5347\u4e86\u9519\u8bef\u8bca\u65ad\u80fd\u529b\uff0c\u5e76\u53d1\u5e03\u4e86\u5177\u6709\u6311\u6218\u6027\u7684\u9519\u8bef\u6570\u636e\u96c6\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u57fa\u51c6\u3002"}}
{"id": "2508.03406", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.03406", "abs": "https://arxiv.org/abs/2508.03406", "authors": ["Kai Li", "Ruihao Zheng", "Xinye Hao", "Zhenkun Wang"], "title": "Multi-Objective Infeasibility Diagnosis for Routing Problems Using Large Language Models", "comment": null, "summary": "In real-world routing problems, users often propose conflicting or\nunreasonable requirements, which result in infeasible optimization models due\nto overly restrictive or contradictory constraints, leading to an empty\nfeasible solution set. Existing Large Language Model (LLM)-based methods\nattempt to diagnose infeasible models, but modifying such models often involves\nmultiple potential adjustments that these methods do not consider. To fill this\ngap, we introduce Multi-Objective Infeasibility Diagnosis (MOID), which\ncombines LLM agents and multi-objective optimization within an automatic\nrouting solver, to provide a set of representative actionable suggestions.\nSpecifically, MOID employs multi-objective optimization to consider both path\ncost and constraint violation, generating a set of trade-off solutions, each\nencompassing varying degrees of model adjustments. To extract practical\ninsights from these solutions, MOID utilizes LLM agents to generate a solution\nanalysis function for the infeasible model. This function analyzes these\ndistinct solutions to diagnose the original infeasible model, providing users\nwith diverse diagnostic insights and suggestions. Finally, we compare MOID with\nseveral LLM-based methods on 50 types of infeasible routing problems. The\nresults indicate that MOID automatically generates multiple diagnostic\nsuggestions in a single run, providing more practical insights for restoring\nmodel feasibility and decision-making compared to existing methods.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408LLM\u4ee3\u7406\u548c\u591a\u76ee\u6807\u4f18\u5316\u7684\u65b9\u6cd5\uff08MOID\uff09\uff0c\u7528\u4e8e\u8bca\u65ad\u548c\u89e3\u51b3\u8def\u7531\u95ee\u9898\u4e2d\u7684\u6a21\u578b\u4e0d\u53ef\u884c\u6027\uff0c\u63d0\u4f9b\u591a\u79cd\u8c03\u6574\u5efa\u8bae\u3002", "motivation": "\u73b0\u5b9e\u4e2d\u7684\u8def\u7531\u95ee\u9898\u5e38\u56e0\u7528\u6237\u9700\u6c42\u51b2\u7a81\u6216\u9650\u5236\u8fc7\u4e25\u5bfc\u81f4\u6a21\u578b\u4e0d\u53ef\u884c\uff0c\u73b0\u6709LLM\u65b9\u6cd5\u672a\u80fd\u5168\u9762\u8003\u8651\u591a\u79cd\u8c03\u6574\u53ef\u80fd\u6027\u3002", "method": "MOID\u7ed3\u5408LLM\u4ee3\u7406\u548c\u591a\u76ee\u6807\u4f18\u5316\uff0c\u751f\u6210\u6743\u8861\u8def\u5f84\u6210\u672c\u548c\u7ea6\u675f\u8fdd\u53cd\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5e76\u901a\u8fc7LLM\u5206\u6790\u63d0\u4f9b\u8bca\u65ad\u5efa\u8bae\u3002", "result": "\u572850\u7c7b\u4e0d\u53ef\u884c\u8def\u7531\u95ee\u9898\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cMOID\u80fd\u5355\u6b21\u751f\u6210\u591a\u79cd\u8bca\u65ad\u5efa\u8bae\uff0c\u6bd4\u73b0\u6709\u65b9\u6cd5\u66f4\u5b9e\u7528\u3002", "conclusion": "MOID\u901a\u8fc7\u591a\u76ee\u6807\u4f18\u5316\u548cLLM\u5206\u6790\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u6a21\u578b\u4e0d\u53ef\u884c\u6027\u95ee\u9898\uff0c\u63d0\u4f9b\u4e86\u66f4\u5b9e\u7528\u7684\u51b3\u7b56\u652f\u6301\u3002"}}
{"id": "2508.03438", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.03438", "abs": "https://arxiv.org/abs/2508.03438", "authors": ["Taine J. Elliott", "Stephen P. Levitt", "Ken Nixon", "Martin Bekker"], "title": "Data Overdose? Time for a Quadruple Shot: Knowledge Graph Construction using Enhanced Triple Extraction", "comment": "18 pages, 8 figures, Published in the Annual Conference of South\n  African Institute of Computer Scientists and Information Technologists,\n  Preprint (author original)", "summary": "The rapid expansion of publicly-available medical data presents a challenge\nfor clinicians and researchers alike, increasing the gap between the volume of\nscientific literature and its applications. The steady growth of studies and\nfindings overwhelms medical professionals at large, hindering their ability to\nsystematically review and understand the latest knowledge. This paper presents\nan approach to information extraction and automatic knowledge graph (KG)\ngeneration to identify and connect biomedical knowledge. Through a pipeline of\nlarge language model (LLM) agents, the system decomposes 44 PubMed abstracts\ninto semantically meaningful proposition sentences and extracts KG triples from\nthese sentences. The triples are enhanced using a combination of open domain\nand ontology-based information extraction methodologies to incorporate\nontological categories. On top of this, a context variable is included during\nextraction to allow the triple to stand on its own - thereby becoming\n`quadruples'. The extraction accuracy of the LLM is validated by comparing\nnatural language sentences generated from the enhanced triples to the original\npropositions, achieving an average cosine similarity of 0.874. The similarity\nfor generated sentences of enhanced triples were compared with generated\nsentences of ordinary triples showing an increase as a result of the context\nvariable. Furthermore, this research explores the ability for LLMs to infer new\nrelationships and connect clusters in the knowledge base of the knowledge\ngraph. This approach leads the way to provide medical practitioners with a\ncentralised, updated in real-time, and sustainable knowledge source, and may be\nthe foundation of similar gains in a wide variety of fields.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u4fe1\u606f\u63d0\u53d6\u65b9\u6cd5\uff0c\u7528\u4e8e\u81ea\u52a8\u751f\u6210\u751f\u7269\u533b\u5b66\u77e5\u8bc6\u56fe\u8c31\uff08KG\uff09\uff0c\u5e76\u901a\u8fc7\u4e0a\u4e0b\u6587\u53d8\u91cf\u589e\u5f3a\u4e09\u5143\u7ec4\uff0c\u5f62\u6210\u201c\u56db\u5143\u7ec4\u201d\uff0c\u63d0\u9ad8\u4e86\u63d0\u53d6\u51c6\u786e\u6027\u3002", "motivation": "\u89e3\u51b3\u533b\u5b66\u6570\u636e\u5feb\u901f\u589e\u957f\u5bfc\u81f4\u4e34\u5e8a\u533b\u751f\u548c\u7814\u7a76\u4eba\u5458\u96be\u4ee5\u7cfb\u7edf\u5316\u7406\u89e3\u548c\u5e94\u7528\u6700\u65b0\u77e5\u8bc6\u7684\u95ee\u9898\u3002", "method": "\u4f7f\u7528LLM\u4ee3\u7406\u5c06PubMed\u6458\u8981\u5206\u89e3\u4e3a\u8bed\u4e49\u547d\u9898\u53e5\u5b50\uff0c\u63d0\u53d6KG\u4e09\u5143\u7ec4\uff0c\u5e76\u901a\u8fc7\u5f00\u653e\u9886\u57df\u548c\u57fa\u4e8e\u672c\u4f53\u7684\u65b9\u6cd5\u589e\u5f3a\u4e09\u5143\u7ec4\uff0c\u52a0\u5165\u4e0a\u4e0b\u6587\u53d8\u91cf\u5f62\u6210\u56db\u5143\u7ec4\u3002", "result": "\u63d0\u53d6\u7684\u4e09\u5143\u7ec4\u751f\u6210\u7684\u81ea\u7136\u8bed\u8a00\u53e5\u5b50\u4e0e\u539f\u547d\u9898\u7684\u5e73\u5747\u4f59\u5f26\u76f8\u4f3c\u5ea6\u4e3a0.874\uff0c\u4e0a\u4e0b\u6587\u53d8\u91cf\u663e\u8457\u63d0\u9ad8\u4e86\u76f8\u4f3c\u5ea6\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u533b\u5b66\u4ece\u4e1a\u8005\u63d0\u4f9b\u4e86\u5b9e\u65f6\u66f4\u65b0\u7684\u96c6\u4e2d\u5316\u77e5\u8bc6\u6e90\uff0c\u5e76\u53ef\u80fd\u5728\u5176\u4ed6\u9886\u57df\u5b9e\u73b0\u7c7b\u4f3c\u6548\u679c\u3002"}}
{"id": "2508.03465", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.03465", "abs": "https://arxiv.org/abs/2508.03465", "authors": ["Saleh Nikooroo"], "title": "Toward a Graph-Theoretic Model of Belief: Confidence, Credibility, and Structural Coherence", "comment": null, "summary": "Belief systems are often treated as globally consistent sets of propositions\nor as scalar-valued probability distributions. Such representations tend to\nobscure the internal structure of belief, conflate external credibility with\ninternal coherence, and preclude the modeling of fragmented or contradictory\nepistemic states. This paper introduces a minimal formalism for belief systems\nas directed, weighted graphs. In this framework, nodes represent individual\nbeliefs, edges encode epistemic relationships (e.g., support or contradiction),\nand two distinct functions assign each belief a credibility (reflecting source\ntrust) and a confidence (derived from internal structural support). Unlike\nclassical probabilistic models, our approach does not assume prior coherence or\nrequire belief updating. Unlike logical and argumentation-based frameworks, it\nsupports fine-grained structural representation without committing to binary\njustification status or deductive closure. The model is purely static and\ndeliberately excludes inference or revision procedures. Its aim is to provide a\nfoundational substrate for analyzing the internal organization of belief\nsystems, including coherence conditions, epistemic tensions, and\nrepresentational limits. By distinguishing belief structure from belief\nstrength, this formalism enables a richer classification of epistemic states\nthan existing probabilistic, logical, or argumentation-based approaches.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5c06\u4fe1\u5ff5\u7cfb\u7edf\u5efa\u6a21\u4e3a\u6709\u5411\u52a0\u6743\u56fe\u7684\u6700\u5c0f\u5f62\u5f0f\u4e3b\u4e49\uff0c\u533a\u5206\u4e86\u4fe1\u5ff5\u7ed3\u6784\u548c\u5f3a\u5ea6\uff0c\u4ee5\u66f4\u4e30\u5bcc\u5730\u5206\u7c7b\u8ba4\u77e5\u72b6\u6001\u3002", "motivation": "\u4f20\u7edf\u4fe1\u5ff5\u8868\u793a\u65b9\u6cd5\uff08\u5982\u6982\u7387\u5206\u5e03\u6216\u903b\u8f91\u6846\u67b6\uff09\u63a9\u76d6\u4e86\u4fe1\u5ff5\u7684\u5185\u90e8\u7ed3\u6784\uff0c\u65e0\u6cd5\u5904\u7406\u788e\u7247\u5316\u6216\u77db\u76fe\u7684\u8ba4\u77e5\u72b6\u6001\u3002", "method": "\u4f7f\u7528\u6709\u5411\u52a0\u6743\u56fe\u8868\u793a\u4fe1\u5ff5\u7cfb\u7edf\uff0c\u8282\u70b9\u4e3a\u4e2a\u4f53\u4fe1\u5ff5\uff0c\u8fb9\u4e3a\u8ba4\u77e5\u5173\u7cfb\uff08\u652f\u6301\u6216\u77db\u76fe\uff09\uff0c\u5e76\u5206\u522b\u8d4b\u4e88\u53ef\u4fe1\u5ea6\u548c\u7f6e\u4fe1\u5ea6\u3002", "result": "\u8be5\u6a21\u578b\u63d0\u4f9b\u4e86\u5206\u6790\u4fe1\u5ff5\u7cfb\u7edf\u5185\u90e8\u7ec4\u7ec7\u7684\u57fa\u7840\uff0c\u5305\u62ec\u4e00\u81f4\u6027\u6761\u4ef6\u3001\u8ba4\u77e5\u5f20\u529b\u548c\u8868\u793a\u9650\u5236\u3002", "conclusion": "\u8fd9\u79cd\u5f62\u5f0f\u4e3b\u4e49\u80fd\u591f\u6bd4\u73b0\u6709\u65b9\u6cd5\u66f4\u4e30\u5bcc\u5730\u5206\u7c7b\u8ba4\u77e5\u72b6\u6001\uff0c\u533a\u5206\u4e86\u4fe1\u5ff5\u7ed3\u6784\u548c\u5f3a\u5ea6\u3002"}}
{"id": "2508.03484", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.03484", "abs": "https://arxiv.org/abs/2508.03484", "authors": ["Zhiyao Xu", "Dan Zhao", "Qingsong Zou", "Qing Li", "Yong Jiang", "Yuhang Wang", "Jingyu Xiao"], "title": "Semantic-aware Graph-guided Behavior Sequences Generation with Large Language Models for Smart Homes", "comment": null, "summary": "As smart homes become increasingly prevalent, intelligent models are widely\nused for tasks such as anomaly detection and behavior prediction. These models\nare typically trained on static datasets, making them brittle to behavioral\ndrift caused by seasonal changes, lifestyle shifts, or evolving routines.\nHowever, collecting new behavior data for retraining is often impractical due\nto its slow pace, high cost, and privacy concerns. In this paper, we propose\nSmartGen, an LLM-based framework that synthesizes context-aware user behavior\ndata to support continual adaptation of downstream smart home models. SmartGen\nconsists of four key components. First, we design a Time and Semantic-aware\nSplit module to divide long behavior sequences into manageable, semantically\ncoherent subsequences under dual time-span constraints. Second, we propose\nSemantic-aware Sequence Compression to reduce input length while preserving\nrepresentative semantics by clustering behavior mapping in latent space. Third,\nwe introduce Graph-guided Sequence Synthesis, which constructs a behavior\nrelationship graph and encodes frequent transitions into prompts, guiding the\nLLM to generate data aligned with contextual changes while retaining core\nbehavior patterns. Finally, we design a Two-stage Outlier Filter to identify\nand remove implausible or semantically inconsistent outputs, aiming to improve\nthe factual coherence and behavioral validity of the generated sequences.\nExperiments on three real-world datasets demonstrate that SmartGen\nsignificantly enhances model performance on anomaly detection and behavior\nprediction tasks under behavioral drift, with anomaly detection improving by\n85.43% and behavior prediction by 70.51% on average. The code is available at\nhttps://github.com/horizonsinzqs/SmartGen.", "AI": {"tldr": "SmartGen\u662f\u4e00\u4e2a\u57fa\u4e8eLLM\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u751f\u6210\u4e0a\u4e0b\u6587\u611f\u77e5\u7684\u7528\u6237\u884c\u4e3a\u6570\u636e\uff0c\u4ee5\u652f\u6301\u667a\u80fd\u5bb6\u5c45\u6a21\u578b\u7684\u6301\u7eed\u9002\u5e94\u3002\u5b83\u901a\u8fc7\u65f6\u95f4\u4e0e\u8bed\u4e49\u611f\u77e5\u5206\u5272\u3001\u8bed\u4e49\u611f\u77e5\u5e8f\u5217\u538b\u7f29\u3001\u56fe\u5f15\u5bfc\u5e8f\u5217\u5408\u6210\u548c\u4e24\u9636\u6bb5\u5f02\u5e38\u8fc7\u6ee4\u56db\u4e2a\u7ec4\u4ef6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u5728\u884c\u4e3a\u6f02\u79fb\u4e0b\u7684\u6027\u80fd\u3002", "motivation": "\u667a\u80fd\u5bb6\u5c45\u6a21\u578b\u901a\u5e38\u57fa\u4e8e\u9759\u6001\u6570\u636e\u96c6\u8bad\u7ec3\uff0c\u96be\u4ee5\u5e94\u5bf9\u884c\u4e3a\u6f02\u79fb\uff08\u5982\u5b63\u8282\u53d8\u5316\u6216\u751f\u6d3b\u65b9\u5f0f\u6539\u53d8\uff09\u3002\u7531\u4e8e\u65b0\u6570\u636e\u6536\u96c6\u6210\u672c\u9ad8\u4e14\u9690\u79c1\u654f\u611f\uff0c\u9700\u8981\u4e00\u79cd\u65b9\u6cd5\u751f\u6210\u5408\u6210\u6570\u636e\u4ee5\u652f\u6301\u6301\u7eed\u9002\u5e94\u3002", "method": "SmartGen\u5305\u542b\u56db\u4e2a\u5173\u952e\u7ec4\u4ef6\uff1a1) \u65f6\u95f4\u4e0e\u8bed\u4e49\u611f\u77e5\u5206\u5272\uff1b2) \u8bed\u4e49\u611f\u77e5\u5e8f\u5217\u538b\u7f29\uff1b3) \u56fe\u5f15\u5bfc\u5e8f\u5217\u5408\u6210\uff1b4) \u4e24\u9636\u6bb5\u5f02\u5e38\u8fc7\u6ee4\u3002", "result": "\u5728\u4e09\u4e2a\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cSmartGen\u663e\u8457\u63d0\u5347\u4e86\u5f02\u5e38\u68c0\u6d4b\uff0885.43%\uff09\u548c\u884c\u4e3a\u9884\u6d4b\uff0870.51%\uff09\u7684\u6027\u80fd\u3002", "conclusion": "SmartGen\u901a\u8fc7\u751f\u6210\u9ad8\u8d28\u91cf\u5408\u6210\u6570\u636e\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u884c\u4e3a\u6f02\u79fb\u95ee\u9898\uff0c\u4e3a\u667a\u80fd\u5bb6\u5c45\u6a21\u578b\u7684\u6301\u7eed\u9002\u5e94\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\u3002"}}
{"id": "2508.03500", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.03500", "abs": "https://arxiv.org/abs/2508.03500", "authors": ["Yijin Yang", "Cristina Cornelio", "Mario Leiva", "Paulo Shakarian"], "title": "Error Detection and Correction for Interpretable Mathematics in Large Language Models", "comment": null, "summary": "Recent large language models (LLMs) have demonstrated the ability to perform\nexplicit multi-step reasoning such as chain-of-thought prompting. However,\ntheir intermediate steps often contain errors that can propagate leading to\ninaccurate final predictions. Additionally, LLMs still struggle with\nhallucinations and often fail to adhere to prescribed output formats, which is\nparticularly problematic for tasks like generating mathematical expressions or\nsource code. This work introduces EDCIM (Error Detection and Correction for\nInterpretable Mathematics), a method for detecting and correcting these errors\nin interpretable mathematics tasks, where the model must generate the exact\nfunctional form that explicitly solve the problem (expressed in natural\nlanguage) rather than a black-box solution. EDCIM uses LLMs to generate a\nsystem of equations for a given problem, followed by a symbolic error-detection\nframework that identifies errors and provides targeted feedback for LLM-based\ncorrection. To optimize efficiency, EDCIM integrates lightweight, open-source\nLLMs with more powerful proprietary models, balancing cost and accuracy. This\nbalance is controlled by a single hyperparameter, allowing users to control the\ntrade-off based on their cost and accuracy requirements. Experimental results\nacross different datasets show that EDCIM significantly reduces both\ncomputational and financial costs, while maintaining, and even improving,\nprediction accuracy when the balance is properly configured.", "AI": {"tldr": "EDCIM\u662f\u4e00\u79cd\u68c0\u6d4b\u548c\u7ea0\u6b63\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u6570\u5b66\u4efb\u52a1\u4e2d\u9519\u8bef\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u7b26\u53f7\u5316\u9519\u8bef\u68c0\u6d4b\u6846\u67b6\u548c\u8f7b\u91cf\u7ea7LLM\u7ed3\u5408\uff0c\u5e73\u8861\u6210\u672c\u4e0e\u51c6\u786e\u6027\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u591a\u6b65\u63a8\u7406\u4e2d\u5e38\u4ea7\u751f\u9519\u8bef\uff0c\u4e14\u96be\u4ee5\u9075\u5b88\u7279\u5b9a\u8f93\u51fa\u683c\u5f0f\uff0cEDCIM\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "EDCIM\u751f\u6210\u65b9\u7a0b\u7ec4\uff0c\u5229\u7528\u7b26\u53f7\u5316\u6846\u67b6\u68c0\u6d4b\u9519\u8bef\u5e76\u63d0\u4f9b\u53cd\u9988\uff0c\u7ed3\u5408\u8f7b\u91cf\u7ea7\u548c\u5f3a\u5927LLM\u4ee5\u4f18\u5316\u6548\u7387\u3002", "result": "\u5b9e\u9a8c\u8868\u660eEDCIM\u663e\u8457\u964d\u4f4e\u6210\u672c\u548c\u8d22\u52a1\u5f00\u9500\uff0c\u540c\u65f6\u4fdd\u6301\u6216\u63d0\u9ad8\u51c6\u786e\u6027\u3002", "conclusion": "EDCIM\u901a\u8fc7\u5e73\u8861\u6210\u672c\u4e0e\u51c6\u786e\u6027\uff0c\u6709\u6548\u63d0\u5347\u4e86\u6570\u5b66\u4efb\u52a1\u7684\u53ef\u9760\u6027\u548c\u6548\u7387\u3002"}}
{"id": "2508.03616", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.03616", "abs": "https://arxiv.org/abs/2508.03616", "authors": ["Jorge Gallego-Feliciano", "S. Aaron McClendon", "Juan Morinelli", "Stavros Zervoudakis", "Antonios Saravanos"], "title": "Hidden Dynamics of Massive Activations in Transformer Training", "comment": null, "summary": "Massive activations are scalar values in transformer hidden states that\nachieve values orders of magnitude larger than typical activations and have\nbeen shown to be critical for model functionality. While prior work has\ncharacterized these phenomena in fully trained models, the temporal dynamics of\ntheir emergence during training remain poorly understood. We present the first\ncomprehensive analysis of massive activation development throughout transformer\ntraining, using the Pythia model family as our testbed. Through systematic\nanalysis of various model sizes across multiple training checkpoints, we\ndemonstrate that massive activation emergence follows predictable mathematical\npatterns that can be accurately modeled using an exponentially-modulated\nlogarithmic function with five key parameters. We develop a machine learning\nframework to predict these mathematical parameters from architectural\nspecifications alone, achieving high accuracy for steady-state behavior and\nmoderate accuracy for emergence timing and magnitude. These findings enable\narchitects to predict and potentially control key aspects of massive activation\nemergence through design choices, with significant implications for model\nstability, training cycle length, interpretability, and optimization. Our\nfindings demonstrate that the emergence of massive activations is governed by\nmodel design and can be anticipated, and potentially controlled, before\ntraining begins.", "AI": {"tldr": "\u672c\u6587\u5206\u6790\u4e86Transformer\u8bad\u7ec3\u4e2d\u5927\u89c4\u6a21\u6fc0\u6d3b\u7684\u6d8c\u73b0\u52a8\u6001\uff0c\u63ed\u793a\u4e86\u5176\u53ef\u9884\u6d4b\u7684\u6570\u5b66\u89c4\u5f8b\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u67b6\u6784\u53c2\u6570\u7684\u9884\u6d4b\u6846\u67b6\u3002", "motivation": "\u7814\u7a76\u5927\u89c4\u6a21\u6fc0\u6d3b\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u7684\u6d8c\u73b0\u52a8\u6001\uff0c\u586b\u8865\u73b0\u6709\u7814\u7a76\u7a7a\u767d\uff0c\u4e3a\u6a21\u578b\u8bbe\u8ba1\u63d0\u4f9b\u6307\u5bfc\u3002", "method": "\u4f7f\u7528Pythia\u6a21\u578b\u5bb6\u65cf\uff0c\u5206\u6790\u4e0d\u540c\u6a21\u578b\u5927\u5c0f\u548c\u8bad\u7ec3\u68c0\u67e5\u70b9\uff0c\u5efa\u7acb\u6307\u6570\u8c03\u5236\u7684\u5bf9\u6570\u51fd\u6570\u6a21\u578b\uff0c\u5e76\u5f00\u53d1\u673a\u5668\u5b66\u4e60\u6846\u67b6\u9884\u6d4b\u53c2\u6570\u3002", "result": "\u53d1\u73b0\u5927\u89c4\u6a21\u6fc0\u6d3b\u6d8c\u73b0\u9075\u5faa\u53ef\u9884\u6d4b\u7684\u6570\u5b66\u6a21\u5f0f\uff0c\u9884\u6d4b\u6846\u67b6\u5728\u7a33\u6001\u884c\u4e3a\u4e0a\u8868\u73b0\u9ad8\u51c6\u786e\u5ea6\uff0c\u6d8c\u73b0\u65f6\u95f4\u548c\u5e45\u5ea6\u4e0a\u8868\u73b0\u4e2d\u7b49\u51c6\u786e\u5ea6\u3002", "conclusion": "\u5927\u89c4\u6a21\u6fc0\u6d3b\u7684\u6d8c\u73b0\u53ef\u901a\u8fc7\u6a21\u578b\u8bbe\u8ba1\u9884\u6d4b\u548c\u63a7\u5236\uff0c\u5bf9\u6a21\u578b\u7a33\u5b9a\u6027\u3001\u8bad\u7ec3\u5468\u671f\u548c\u4f18\u5316\u6709\u91cd\u8981\u610f\u4e49\u3002"}}
{"id": "2508.03622", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.03622", "abs": "https://arxiv.org/abs/2508.03622", "authors": ["Jialin Li", "Jinzhe Li", "Gengxu Li", "Yi Chang", "Yuan Wu"], "title": "Refining Critical Thinking in LLM Code Generation: A Faulty Premise-based Evaluation Framework", "comment": null, "summary": "With the advancement of code generation capabilities in large language models\n(LLMs), their reliance on input premises has intensified. When users provide\ninputs containing faulty premises, the probability of code generation\nhallucinations rises significantly, exposing deficiencies in their\nself-scrutiny capabilities. This paper proposes Faulty Premises Bench\n(FPBench), the first code generation evaluation framework targeting faulty\npremises. By systematically constructing three categories of faulty premises\nand integrating multi-dimensional evaluation metrics, it conducts in-depth\nassessments of 15 representative LLMs. The key findings are as follows: (1)\nMost models exhibit poor reasoning abilities and suboptimal code generation\nperformance under faulty premises, heavily relying on explicit prompts for\nerror detection, with limited self-scrutiny capabilities; (2) Faulty premises\ntrigger a point of diminishing returns in resource investment, leading to\nblindly increasing length fails to enhance quality; (3) The three types of\nfaulty premises respectively activate distinct defect patterns in models,\nrevealing a triple dissociation in the cognitive mechanisms of code generation\nmodels. This study not only highlights the urgent need for LLMs to proactively\nverify premises in code generation but also, through the proposed FPBench\nframework and multi-dimensional evaluation system, provides a theoretical\nfoundation and practical pathway for developing reliable, human-centric code\ngeneration models.", "AI": {"tldr": "FPBench\u662f\u9996\u4e2a\u9488\u5bf9\u4ee3\u7801\u751f\u6210\u4e2d\u9519\u8bef\u524d\u63d0\u7684\u8bc4\u4f30\u6846\u67b6\uff0c\u901a\u8fc7\u7cfb\u7edf\u6784\u5efa\u4e09\u7c7b\u9519\u8bef\u524d\u63d0\u548c\u591a\u7ef4\u8bc4\u4f30\u6307\u6807\uff0c\u8bc4\u4f30\u4e8615\u79cdLLM\uff0c\u53d1\u73b0\u6a21\u578b\u5728\u9519\u8bef\u524d\u63d0\u4e0b\u7684\u63a8\u7406\u80fd\u529b\u4e0d\u8db3\uff0c\u8d44\u6e90\u6295\u5165\u5b58\u5728\u8fb9\u9645\u6548\u5e94\uff0c\u4e14\u4e0d\u540c\u9519\u8bef\u524d\u63d0\u4f1a\u89e6\u53d1\u4e0d\u540c\u7684\u7f3a\u9677\u6a21\u5f0f\u3002", "motivation": "\u968f\u7740LLM\u4ee3\u7801\u751f\u6210\u80fd\u529b\u7684\u63d0\u5347\uff0c\u5176\u5bf9\u8f93\u5165\u524d\u63d0\u7684\u4f9d\u8d56\u52a0\u5267\uff0c\u9519\u8bef\u524d\u63d0\u4f1a\u663e\u8457\u589e\u52a0\u4ee3\u7801\u751f\u6210\u7684\u5e7b\u89c9\u6982\u7387\uff0c\u66b4\u9732\u5176\u81ea\u68c0\u80fd\u529b\u7684\u4e0d\u8db3\u3002", "method": "\u63d0\u51faFPBench\u6846\u67b6\uff0c\u7cfb\u7edf\u6784\u5efa\u4e09\u7c7b\u9519\u8bef\u524d\u63d0\uff0c\u7ed3\u5408\u591a\u7ef4\u8bc4\u4f30\u6307\u6807\uff0c\u5bf915\u79cd\u4ee3\u8868\u6027LLM\u8fdb\u884c\u6df1\u5165\u8bc4\u4f30\u3002", "result": "\u5927\u591a\u6570\u6a21\u578b\u5728\u9519\u8bef\u524d\u63d0\u4e0b\u7684\u63a8\u7406\u80fd\u529b\u5dee\uff0c\u8d44\u6e90\u6295\u5165\u5b58\u5728\u8fb9\u9645\u6548\u5e94\uff0c\u4e14\u4e09\u7c7b\u9519\u8bef\u524d\u63d0\u5206\u522b\u89e6\u53d1\u4e0d\u540c\u7684\u7f3a\u9677\u6a21\u5f0f\u3002", "conclusion": "\u7814\u7a76\u5f3a\u8c03\u4e86LLM\u5728\u4ee3\u7801\u751f\u6210\u4e2d\u4e3b\u52a8\u9a8c\u8bc1\u524d\u63d0\u7684\u7d27\u8feb\u6027\uff0cFPBench\u4e3a\u5f00\u53d1\u53ef\u9760\u3001\u4ee5\u4eba\u4e3a\u672c\u7684\u4ee3\u7801\u751f\u6210\u6a21\u578b\u63d0\u4f9b\u4e86\u7406\u8bba\u548c\u5b9e\u8df5\u57fa\u7840\u3002"}}
{"id": "2508.03661", "categories": ["cs.AI", "astro-ph.HE", "astro-ph.IM", "gr-qc"], "pdf": "https://arxiv.org/pdf/2508.03661", "abs": "https://arxiv.org/abs/2508.03661", "authors": ["He Wang", "Liang Zeng"], "title": "Automated Algorithmic Discovery for Gravitational-Wave Detection Guided by LLM-Informed Evolutionary Monte Carlo Tree Search", "comment": "89 pages (37 main), 6+6 figures, 1 table. Initial submission; subject\n  to revision", "summary": "Computational scientific discovery increasingly relies on algorithms to\nprocess complex data and identify meaningful patterns - yet faces persistent\nchallenges in gravitational-wave signal identification. While existing\nalgorithmic approaches like matched filtering (MF) and deep neural networks\n(DNNs) have achieved partial success, their limitations directly stem from\nfundamental limitations: MF's excessive computational demands arise from its\nreliance on predefined theoretical waveform templates, while DNNs' black-box\narchitectures obscure decision logic and introduce hidden biases. We propose\nEvolutionary Monte Carlo Tree Search (Evo-MCTS), a framework that addresses\nthese limitations through systematic algorithm space exploration guided by\ndomain-aware physical constraints. Our approach combines tree-structured search\nwith evolutionary optimization and large language model heuristics to create\ninterpretable algorithmic solutions. Our Evo-MCTS framework demonstrates\nsubstantial improvements, achieving a 20.2\\% improvement over state-of-the-art\ngravitational wave detection algorithms on the MLGWSC-1 benchmark dataset.\nHigh-performing algorithm variants consistently exceed thresholds. The\nframework generates human-interpretable algorithmic pathways that reveal\ndistinct performance patterns. Beyond performance improvements, our framework\ndiscovers novel algorithmic combinations, thereby establishing a transferable\nmethodology for automated algorithmic discovery across computational science\ndomains.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aEvo-MCTS\u7684\u65b0\u6846\u67b6\uff0c\u7ed3\u5408\u8fdb\u5316\u4f18\u5316\u548c\u8499\u7279\u5361\u6d1b\u6811\u641c\u7d22\uff0c\u7528\u4e8e\u6539\u8fdb\u5f15\u529b\u6ce2\u4fe1\u53f7\u8bc6\u522b\uff0c\u6027\u80fd\u63d0\u534720.2%\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\uff08\u5982\u5339\u914d\u6ee4\u6ce2\u548c\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\uff09\u5728\u5f15\u529b\u6ce2\u4fe1\u53f7\u8bc6\u522b\u4e2d\u5b58\u5728\u8ba1\u7b97\u91cf\u5927\u548c\u51b3\u7b56\u903b\u8f91\u4e0d\u900f\u660e\u7684\u95ee\u9898\u3002", "method": "\u7ed3\u5408\u8499\u7279\u5361\u6d1b\u6811\u641c\u7d22\u3001\u8fdb\u5316\u4f18\u5316\u548c\u5927\u8bed\u8a00\u6a21\u578b\u542f\u53d1\u5f0f\uff0c\u751f\u6210\u53ef\u89e3\u91ca\u7684\u7b97\u6cd5\u89e3\u51b3\u65b9\u6848\u3002", "result": "\u5728MLGWSC-1\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u6027\u80fd\u63d0\u534720.2%\uff0c\u5e76\u53d1\u73b0\u65b0\u7684\u7b97\u6cd5\u7ec4\u5408\u3002", "conclusion": "Evo-MCTS\u4e3a\u8ba1\u7b97\u79d1\u5b66\u9886\u57df\u7684\u81ea\u52a8\u5316\u7b97\u6cd5\u53d1\u73b0\u63d0\u4f9b\u4e86\u53ef\u8f6c\u79fb\u7684\u65b9\u6cd5\u8bba\u3002"}}
{"id": "2508.03680", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.03680", "abs": "https://arxiv.org/abs/2508.03680", "authors": ["Xufang Luo", "Yuge Zhang", "Zhiyuan He", "Zilong Wang", "Siyun Zhao", "Dongsheng Li", "Luna K. Qiu", "Yuqing Yang"], "title": "Agent Lightning: Train ANY AI Agents with Reinforcement Learning", "comment": null, "summary": "We present Agent Lightning, a flexible and extensible framework that enables\nReinforcement Learning (RL)-based training of Large Language Models (LLMs) for\nany AI agent. Unlike existing methods that tightly couple RL training with\nagent or rely on sequence concatenation with masking, Agent Lightning achieves\ncomplete decoupling between agent execution and training, allowing seamless\nintegration with existing agents developed via diverse ways (e.g., using\nframeworks like LangChain, OpenAI Agents SDK, AutoGen, and building from\nscratch) with almost ZERO code modifications. By formulating agent execution as\nMarkov decision process, we define an unified data interface and propose a\nhierarchical RL algorithm, LightningRL, which contains a credit assignment\nmodule, allowing us to decompose trajectories generated by ANY agents into\ntraining transition. This enables RL to handle complex interaction logic, such\nas multi-agent scenarios and dynamic workflows. For the system design, we\nintroduce a Training-Agent Disaggregation architecture, and brings agent\nobservability frameworks into agent runtime, providing a standardized agent\nfinetuning interface. Experiments across text-to-SQL, retrieval-augmented\ngeneration, and math tool-use tasks demonstrate stable, continuous\nimprovements, showcasing the framework's potential for real-world agent\ntraining and deployment.", "AI": {"tldr": "Agent Lightning\u662f\u4e00\u4e2a\u7075\u6d3b\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u5b9e\u73b0\u4e0e\u73b0\u6709AI\u4ee3\u7406\u7684\u65e0\u7f1d\u96c6\u6210\uff0c\u65e0\u9700\u4ee3\u7801\u4fee\u6539\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u4e2d\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u4e0e\u4ee3\u7406\u7d27\u5bc6\u8026\u5408\u7684\u95ee\u9898\uff0c\u63d0\u4f9b\u66f4\u7075\u6d3b\u7684\u8bad\u7ec3\u65b9\u5f0f\u3002", "method": "\u91c7\u7528\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\u5b9a\u4e49\u7edf\u4e00\u6570\u636e\u63a5\u53e3\uff0c\u63d0\u51fa\u5206\u5c42RL\u7b97\u6cd5LightningRL\uff0c\u5305\u542b\u4fe1\u7528\u5206\u914d\u6a21\u5757\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\u5728\u591a\u79cd\u4efb\u52a1\u4e2d\u5b9e\u73b0\u7a33\u5b9a\u3001\u6301\u7eed\u7684\u6539\u8fdb\u3002", "conclusion": "\u6846\u67b6\u5177\u6709\u5b9e\u9645\u4ee3\u7406\u8bad\u7ec3\u548c\u90e8\u7f72\u7684\u6f5c\u529b\u3002"}}
