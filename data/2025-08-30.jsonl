{"id": "2508.20186", "categories": ["cs.CR", "cs.AI", "cs.CY"], "pdf": "https://arxiv.org/pdf/2508.20186", "abs": "https://arxiv.org/abs/2508.20186", "authors": ["Lukasz Olejnik"], "title": "AI Propaganda factories with language models", "comment": null, "summary": "AI-powered influence operations can now be executed end-to-end on commodity\nhardware. We show that small language models produce coherent, persona-driven\npolitical messaging and can be evaluated automatically without human raters.\nTwo behavioural findings emerge. First, persona-over-model: persona design\nexplains behaviour more than model identity. Second, engagement as a stressor:\nwhen replies must counter-arguments, ideological adherence strengthens and the\nprevalence of extreme content increases. We demonstrate that fully automated\ninfluence-content production is within reach of both large and small actors.\nConsequently, defence should shift from restricting model access towards\nconversation-centric detection and disruption of campaigns and coordination\ninfrastructure. Paradoxically, the very consistency that enables these\noperations also provides a detection signature."}
{"id": "2508.20119", "categories": ["cs.SE", "cs.LG", "68T42", "I.2.6; I.2.2; D.2.2"], "pdf": "https://arxiv.org/pdf/2508.20119", "abs": "https://arxiv.org/abs/2508.20119", "authors": ["Daniel M. Yellin"], "title": "Evaluating LLMs on microservice-based applications: how complex is your specification?", "comment": "20 pages + 7 pages appendices. 7 Figures. 8 Tables", "summary": "In this paper we evaluate how far LLMs have advanced in generating code for\nreal-world problems. Specifically, we explore code synthesis for\nmicroservice-based applications, a widely used architecture pattern. We define\na standard template for specifying these applications, and we propose a metric\nfor judging the difficulty level of a specification. The higher the score, the\nmore difficult it is to generate code for the specification. We develop a\nframework to automate the process of testing LLM-synthesized code for a\nmicroservice using unit tests. Our experimental results show that strong LLMs\n(like GPT-3o-mini) do fairly well on medium difficulty specifications but do\nvery poorly on those of higher difficulty levels. This is due to more intricate\nbusiness logic, a greater use of external services, database integration and\ninclusion of non-functional capabilities such as authentication. We analyzed\nthe errors in LLM-synthesized code and report on the key challenges LLMs face\nin generating code for these specifications thereby suggesting future research\ndirections to improve code synthesis for real-world problems."}
{"id": "2508.20212", "categories": ["cs.CR", "cs.SE"], "pdf": "https://arxiv.org/pdf/2508.20212", "abs": "https://arxiv.org/abs/2508.20212", "authors": ["Minghao Hu", "Junzhe Wang", "Weisen Zhao", "Qiang Zeng", "Lannan Luo"], "title": "FlowMalTrans: Unsupervised Binary Code Translation for Malware Detection Using Flow-Adapter Architecture", "comment": "This paper is accepted to EMNLP 2025 Findings", "summary": "Applying deep learning to malware detection has drawn great attention due to\nits notable performance. With the increasing prevalence of cyberattacks\ntargeting IoT devices, there is a parallel rise in the development of malware\nacross various Instruction Set Architectures (ISAs). It is thus important to\nextend malware detection capacity to multiple ISAs. However, training a deep\nlearning-based malware detection model usually requires a large number of\nlabeled malware samples. The process of collecting and labeling sufficient\nmalware samples to build datasets for each ISA is labor-intensive and\ntime-consuming. To reduce the burden of data collection, we propose to leverage\nthe ideas of Neural Machine Translation (NMT) and Normalizing Flows (NFs) for\nmalware detection. Specifically, when dealing with malware in a certain ISA, we\ntranslate it to an ISA with sufficient malware samples (like X86-64). This\nallows us to apply a model trained on one ISA to analyze malware from another\nISA. Our approach reduces the data collection effort by enabling malware\ndetection across multiple ISAs using a model trained on a single ISA."}
{"id": "2508.20124", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.20124", "abs": "https://arxiv.org/abs/2508.20124", "authors": ["Yunlong Feng", "Yang Xu", "Xiao Xu", "Binyuan Hui", "Junyang Lin"], "title": "Towards Better Correctness and Efficiency in Code Generation", "comment": null, "summary": "While code large language models have demonstrated remarkable progress in\ncode generation, the generated code often exhibits poor runtime efficiency,\nlimiting its practical application in performance-sensitive scenarios. To\naddress this limitation, we propose an efficiency-oriented reinforcement\nlearning framework guided by a novel performance reward. Based on this\nframework, we take a deeper dive into the code efficiency problem, identifying\nthen proposing methods to overcome key bottlenecks: (1) Dynamic exploration\novercomes the static data constraints of offline fine-tuning, enabling the\ndiscovery of more efficient code implementations. (2) The error-insensitive\nreinforcement learning method and high-contrast efficiency signals are crucial\nfor mitigating systematic errors and achieving effective optimization. (3)\nOnline exploration is most effective when starting from a high-correctness\nbaseline, as this allows for efficiency improvements without sacrificing\naccuracy. With these discoveries, we finally propose a two-stage tuning method,\nwhich achieves high and balanced performance across correctness and efficiency.\nThe results of experiments show the effectiveness of the method, which improves\ncode correctness by 10.18\\% and runtime efficiency by 7.75\\% on a 7B model,\nachieving performance comparable to much larger model."}
{"id": "2508.20228", "categories": ["cs.CR", "cs.CL"], "pdf": "https://arxiv.org/pdf/2508.20228", "abs": "https://arxiv.org/abs/2508.20228", "authors": ["Xia Han", "Qi Li", "Jianbing Ni", "Mohammad Zulkernine"], "title": "Robustness Assessment and Enhancement of Text Watermarking for Google's SynthID", "comment": "submitted to TrustCom2025", "summary": "Recent advances in LLM watermarking methods such as SynthID-Text by Google\nDeepMind offer promising solutions for tracing the provenance of AI-generated\ntext. However, our robustness assessment reveals that SynthID-Text is\nvulnerable to meaning-preserving attacks, such as paraphrasing, copy-paste\nmodifications, and back-translation, which can significantly degrade watermark\ndetectability. To address these limitations, we propose SynGuard, a hybrid\nframework that combines the semantic alignment strength of Semantic Information\nRetrieval (SIR) with the probabilistic watermarking mechanism of SynthID-Text.\nOur approach jointly embeds watermarks at both lexical and semantic levels,\nenabling robust provenance tracking while preserving the original meaning.\nExperimental results across multiple attack scenarios show that SynGuard\nimproves watermark recovery by an average of 11.1\\% in F1 score compared to\nSynthID-Text. These findings demonstrate the effectiveness of semantic-aware\nwatermarking in resisting real-world tampering. All code, datasets, and\nevaluation scripts are publicly available at:\nhttps://github.com/githshine/SynGuard."}
{"id": "2508.20340", "categories": ["cs.SE", "cs.AI", "cs.PL"], "pdf": "https://arxiv.org/pdf/2508.20340", "abs": "https://arxiv.org/abs/2508.20340", "authors": ["Maolin Sun", "Yibiao Yang", "Yuming Zhou"], "title": "Boosting Skeleton-Driven SMT Solver Fuzzing by Leveraging LLM to Produce Formula Generators", "comment": null, "summary": "Satisfiability Modulo Theory (SMT) solvers are foundational to modern systems\nand programming languages research, providing the foundation for tasks like\nsymbolic execution and automated verification. Because these solvers sit on the\ncritical path, their correctness is essential, and high-quality test formulas\nare key to uncovering bugs. However, while prior testing techniques performed\nwell on earlier solver versions, they struggle to keep pace with rapidly\nevolving features. Recent approaches based on Large Language Models (LLMs) show\npromise in exploring advanced solver capabilities, but two obstacles remain:\nnearly half of the generated formulas are syntactically invalid, and iterative\ninteractions with the LLMs introduce substantial computational overhead. In\nthis study, we present Chimera, a novel LLM-assisted fuzzing framework that\naddresses both issues by shifting from direct formula generation to the\nsynthesis of reusable term (i.e., logical expression) generators. Particularly,\nChimera uses LLMs to (1) automatically extract context-free grammars (CFGs) for\nSMT theories, including solver-specific extensions, from documentation, and (2)\nsynthesize composable Boolean term generators that adhere to these grammars.\nDuring fuzzing, Chimera populates structural skeletons derived from existing\nformulas with the terms iteratively produced by the LLM-synthesized generators.\nThis design ensures syntactic validity while promoting semantic diversity.\nNotably, Chimera requires only one-time LLM interaction investment,\ndramatically reducing runtime cost. We evaluated Chimera on two leading SMT\nsolvers: Z3 and cvc5. Our experiments show that Chimera has identified 43\nconfirmed bugs, 40 of which have already been fixed by developers."}
{"id": "2508.20282", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.20282", "abs": "https://arxiv.org/abs/2508.20282", "authors": ["Hyejun Jeong", "Mohammadreze Teymoorianfard", "Abhinav Kumar", "Amir Houmansadr", "Eugene Badasarian"], "title": "Network-Level Prompt and Trait Leakage in Local Research Agents", "comment": "under review", "summary": "We show that Web and Research Agents (WRAs) -- language model-based systems\nthat investigate complex topics on the Internet -- are vulnerable to inference\nattacks by passive network adversaries such as ISPs. These agents could be\ndeployed \\emph{locally} by organizations and individuals for privacy, legal, or\nfinancial purposes. Unlike sporadic web browsing by humans, WRAs visit\n$70{-}140$ domains with distinguishable timing correlations, enabling unique\nfingerprinting attacks.\n  Specifically, we demonstrate a novel prompt and user trait leakage attack\nagainst WRAs that only leverages their network-level metadata (i.e., visited IP\naddresses and their timings). We start by building a new dataset of WRA traces\nbased on user search queries and queries generated by synthetic personas. We\ndefine a behavioral metric (called OBELS) to comprehensively assess similarity\nbetween original and inferred prompts, showing that our attack recovers over\n73\\% of the functional and domain knowledge of user prompts. Extending to a\nmulti-session setting, we recover up to 19 of 32 latent traits with high\naccuracy. Our attack remains effective under partial observability and noisy\nconditions. Finally, we discuss mitigation strategies that constrain domain\ndiversity or obfuscate traces, showing negligible utility impact while reducing\nattack effectiveness by an average of 29\\%."}
{"id": "2508.20370", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.20370", "abs": "https://arxiv.org/abs/2508.20370", "authors": ["Lingzhe Zhang", "Tong Jia", "Kangjin Wang", "Weijie Hong", "Chiming Duan", "Minghua He", "Ying Li"], "title": "Adaptive Root Cause Localization for Microservice Systems with Multi-Agent Recursion-of-Thought", "comment": null, "summary": "As contemporary microservice systems become increasingly popular and\ncomplex-often comprising hundreds or even thousands of fine-grained,\ninterdependent subsystems-they are facing more frequent failures. Ensuring\nsystem reliability thus demands accurate root cause localization. While traces\nand metrics have proven to be effective data sources for this task, existing\nmethods either heavily rely on pre-defined schemas, which struggle to adapt to\nevolving operational contexts, or lack interpretability in their reasoning\nprocess, thereby leaving Site Reliability Engineers (SREs) confused. In this\npaper, we conduct a comprehensive study on how SREs localize the root cause of\nfailures, drawing insights from multiple professional SREs across different\norganizations. Our investigation reveals that human root cause analysis\nexhibits three key characteristics: recursiveness, multi-dimensional expansion,\nand cross-modal reasoning. Motivated by these findings, we introduce RCLAgent,\nan adaptive root cause localization method for microservice systems that\nleverages a multi-agent recursion-of-thought framework. RCLAgent employs a\nnovel recursion-of-thought strategy to guide the LLM's reasoning process,\neffectively integrating data from multiple agents and tool-assisted analysis to\naccurately pinpoint the root cause. Experimental evaluations on various public\ndatasets demonstrate that RCLAgent achieves superior performance by localizing\nthe root cause using only a single request-outperforming state-of-the-art\nmethods that depend on aggregating multiple requests. These results underscore\nthe effectiveness of RCLAgent in enhancing the efficiency and precision of root\ncause localization in complex microservice environments."}
{"id": "2508.20131", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.20131", "abs": "https://arxiv.org/abs/2508.20131", "authors": ["Yuqicheng Zhu", "Nico Potyka", "Daniel Hernández", "Yuan He", "Zifeng Ding", "Bo Xiong", "Dongzhuoran Zhou", "Evgeny Kharlamov", "Steffen Staab"], "title": "ArgRAG: Explainable Retrieval Augmented Generation using Quantitative Bipolar Argumentation", "comment": null, "summary": "Retrieval-Augmented Generation (RAG) enhances large language models by\nincorporating external knowledge, yet suffers from critical limitations in\nhigh-stakes domains -- namely, sensitivity to noisy or contradictory evidence\nand opaque, stochastic decision-making. We propose ArgRAG, an explainable, and\ncontestable alternative that replaces black-box reasoning with structured\ninference using a Quantitative Bipolar Argumentation Framework (QBAF). ArgRAG\nconstructs a QBAF from retrieved documents and performs deterministic reasoning\nunder gradual semantics. This allows faithfully explaining and contesting\ndecisions. Evaluated on two fact verification benchmarks, PubHealth and\nRAGuard, ArgRAG achieves strong accuracy while significantly improving\ntransparency."}
{"id": "2508.20307", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.20307", "abs": "https://arxiv.org/abs/2508.20307", "authors": ["Michael R Smith", "Joe Ingram"], "title": "Surveying the Operational Cybersecurity and Supply Chain Threat Landscape when Developing and Deploying AI Systems", "comment": "11 pages, 5 figures", "summary": "The rise of AI has transformed the software and hardware landscape, enabling\npowerful capabilities through specialized infrastructures, large-scale data\nstorage, and advanced hardware. However, these innovations introduce unique\nattack surfaces and objectives which traditional cybersecurity assessments\noften overlook. Cyber attackers are shifting their objectives from conventional\ngoals like privilege escalation and network pivoting to manipulating AI outputs\nto achieve desired system effects, such as slowing system performance, flooding\noutputs with false positives, or degrading model accuracy. This paper serves to\nraise awareness of the novel cyber threats that are introduced when\nincorporating AI into a software system. We explore the operational\ncybersecurity and supply chain risks across the AI lifecycle, emphasizing the\nneed for tailored security frameworks to address evolving threats in the\nAI-driven landscape. We highlight previous exploitations and provide insights\nfrom working in this area. By understanding these risks, organizations can\nbetter protect AI systems and ensure their reliability and resilience."}
{"id": "2508.20563", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.20563", "abs": "https://arxiv.org/abs/2508.20563", "authors": ["Zheying Zhang", "Tomas Herda", "Victoria Pichler", "Pekka Abrahamsson", "Geir K. Hanssen", "Joshua Kerievsky", "Alex Polyakov", "Mohit Chandna", "Marius Irgens", "Kai-Kristian Kemell", "Ayman Asad Khan", "Crystal Kwok", "Evan Leybourn", "Munish Malik", "Dorota Mleczko", "Morteza Moalagh", "Christopher Morales", "Yuliia Pieskova", "Daniel Planötscher", "Mika Saari", "Anastasiia Tkalich", "Karl Josef Gstettner", "Xiaofeng Wang"], "title": "AI and Agile Software Development: A Research Roadmap from the XP2025 Workshop", "comment": null, "summary": "This paper synthesizes the key findings from a full-day XP2025 workshop on\n\"AI and Agile: From Frustration to Success\", held in Brugg-Windisch,\nSwitzerland. The workshop brought together over 30 interdisciplinary academic\nresearchers and industry practitioners to tackle the concrete challenges and\nemerging opportunities at the intersection of Generative Artificial\nIntelligence (GenAI) and agile software development. Through structured,\ninteractive breakout sessions, participants identified shared pain points like\ntool fragmentation, governance, data quality, and critical skills gaps in AI\nliteracy and prompt engineering. These issues were further analyzed, revealing\nunderlying causes and cross-cutting concerns. The workshop concluded by\ncollaboratively co-creating a multi-thematic research roadmap, articulating\nboth short-term, implementable actions and visionary, long-term research\ndirections. This cohesive agenda aims to guide future investigation and drive\nthe responsible, human-centered integration of GenAI into agile practices."}
{"id": "2508.20134", "categories": ["cs.AI", "cs.ET", "quant-ph"], "pdf": "https://arxiv.org/pdf/2508.20134", "abs": "https://arxiv.org/abs/2508.20134", "authors": ["Zhenxiao Fu", "Fan Chen", "Lei Jiang"], "title": "QAgent: An LLM-based Multi-Agent System for Autonomous OpenQASM programming", "comment": null, "summary": "Noisy Intermediate-Scale Quantum (NISQ) devices have begun to exhibit early\nquantum advantages on classically intractable problems, spanning physics\nsimulations to Gaussian boson sampling. Yet, realizing these benefits remains\nchallenging for non-experts, primarily due to the complexities of programming\nin Open Quantum Assembly Language (OpenQASM). Although Large Language Model\n(LLM)-based agents have shown promise in automating classical programming\nworkflows, their quantum counterparts have largely been restricted to\nspecialized tasks such as quantum chemistry or error correction. In this paper,\nwe present QAgent, an LLM-powered multi-agent system that fully automates\nOpenQASM programming. By integrating task planning, in-context few-shot\nlearning, retrieval-augmented generation (RAG) for long-term context,\npredefined generation tools, and chain-of-thought (CoT) reasoning, the agents\nsystematically improve both compilation and functional correctness. Our\nevaluations demonstrate substantial improvements: across multiple LLMs of\nvarying sizes, QAgent enhances the accuracy of QASM code generation by 71.6\\%\ncompared to previous static LLM-based approaches. We envision this multi-agent\nsystem as a key enabler for democratizing quantum programming, bridging\nexpertise gaps, and accelerating the practical adoption of quantum computing."}
{"id": "2508.20412", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2508.20412", "abs": "https://arxiv.org/abs/2508.20412", "authors": ["Zhiqiang Wang", "Junyang Zhang", "Guanquan Shi", "HaoRan Cheng", "Yunhao Yao", "Kaiwen Guo", "Haohua Du", "Xiang-Yang Li"], "title": "MindGuard: Tracking, Detecting, and Attributing MCP Tool Poisoning Attack via Decision Dependence Graph", "comment": null, "summary": "The Model Context Protocol (MCP) is increasingly adopted to standardize the\ninteraction between LLM agents and external tools. However, this trend\nintroduces a new threat: Tool Poisoning Attacks (TPA), where tool metadata is\npoisoned to induce the agent to perform unauthorized operations. Existing\ndefenses that primarily focus on behavior-level analysis are fundamentally\nineffective against TPA, as poisoned tools need not be executed, leaving no\nbehavioral trace to monitor.\n  Thus, we propose MindGuard, a decision-level guardrail for LLM agents,\nproviding provenance tracking of call decisions, policy-agnostic detection, and\npoisoning source attribution against TPA. While fully explaining LLM decision\nremains challenging, our empirical findings uncover a strong correlation\nbetween LLM attention mechanisms and tool invocation decisions. Therefore, we\nchoose attention as an empirical signal for decision tracking and formalize\nthis as the Decision Dependence Graph (DDG), which models the LLM's reasoning\nprocess as a weighted, directed graph where vertices represent logical concepts\nand edges quantify the attention-based dependencies. We further design robust\nDDG construction and graph-based anomaly analysis mechanisms that efficiently\ndetect and attribute TPA attacks. Extensive experiments on real-world datasets\ndemonstrate that MindGuard achieves 94\\%-99\\% average precision in detecting\npoisoned invocations, 95\\%-100\\% attribution accuracy, with processing times\nunder one second and no additional token cost. Moreover, DDG can be viewed as\nan adaptation of the classical Program Dependence Graph (PDG), providing a\nsolid foundation for applying traditional security policies at the decision\nlevel."}
{"id": "2508.20737", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.20737", "abs": "https://arxiv.org/abs/2508.20737", "authors": ["Wei Ma", "Yixiao Yang", "Qiang Hu", "Shi Ying", "Zhi Jin", "Bo Du", "Zhenchang Xing", "Tianlin Li", "Junjie Shi", "Yang Liu", "Linxiao Jiang"], "title": "Rethinking Testing for LLM Applications: Characteristics, Challenges, and a Lightweight Interaction Protocol", "comment": null, "summary": "Applications of Large Language Models~(LLMs) have evolved from simple text\ngenerators into complex software systems that integrate retrieval augmentation,\ntool invocation, and multi-turn interactions. Their inherent non-determinism,\ndynamism, and context dependence pose fundamental challenges for quality\nassurance. This paper decomposes LLM applications into a three-layer\narchitecture: \\textbf{\\textit{System Shell Layer}}, \\textbf{\\textit{Prompt\nOrchestration Layer}}, and \\textbf{\\textit{LLM Inference Core}}. We then assess\nthe applicability of traditional software testing methods in each layer:\ndirectly applicable at the shell layer, requiring semantic reinterpretation at\nthe orchestration layer, and necessitating paradigm shifts at the inference\ncore. A comparative analysis of Testing AI methods from the software\nengineering community and safety analysis techniques from the AI community\nreveals structural disconnects in testing unit abstraction, evaluation metrics,\nand lifecycle management. We identify four fundamental differences that\nunderlie 6 core challenges. To address these, we propose four types of\ncollaborative strategies (\\emph{Retain}, \\emph{Translate}, \\emph{Integrate},\nand \\emph{Runtime}) and explore a closed-loop, trustworthy quality assurance\nframework that combines pre-deployment validation with runtime monitoring.\nBased on these strategies, we offer practical guidance and a protocol proposal\nto support the standardization and tooling of LLM application testing. We\npropose a protocol \\textbf{\\textit{Agent Interaction Communication Language}}\n(AICL) that is used to communicate between AI agents. AICL has the\ntest-oriented features and is easily integrated in the current agent framework."}
{"id": "2508.20140", "categories": ["cs.AI", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2508.20140", "abs": "https://arxiv.org/abs/2508.20140", "authors": ["James Ragan", "Fred Y. Hadaegh", "Soon-Jo Chung"], "title": "Array-Based Monte Carlo Tree Search", "comment": null, "summary": "Monte Carlo Tree Search is a popular method for solving decision making\nproblems. Faster implementations allow for more simulations within the same\nwall clock time, directly improving search performance. To this end, we present\nan alternative array-based implementation of the classic Upper Confidence\nbounds applied to Trees algorithm. Our method preserves the logic of the\noriginal algorithm, but eliminates the need for branch prediction, enabling\nfaster performance on pipelined processors, and up to a factor of 2.8 times\nbetter scaling with search depth in our numerical simulations."}
{"id": "2508.20414", "categories": ["cs.CR", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.20414", "abs": "https://arxiv.org/abs/2508.20414", "authors": ["Mengyu Sun", "Ziyuan Yang", "Yongqiang Huang", "Hui Yu", "Yingyu Chen", "Shuren Qi", "Andrew Beng Jin Teoh", "Yi Zhang"], "title": "Federated Learning for Large Models in Medical Imaging: A Comprehensive Review", "comment": null, "summary": "Artificial intelligence (AI) has demonstrated considerable potential in the\nrealm of medical imaging. However, the development of high-performance AI\nmodels typically necessitates training on large-scale, centralized datasets.\nThis approach is confronted with significant challenges due to strict patient\nprivacy regulations and legal restrictions on data sharing and utilization.\nThese limitations hinder the development of large-scale models in medical\ndomains and impede continuous updates and training with new data. Federated\nLearning (FL), a privacy-preserving distributed training framework, offers a\nnew solution by enabling collaborative model development across fragmented\nmedical datasets. In this survey, we review FL's contributions at two stages of\nthe full-stack medical analysis pipeline. First, in upstream tasks such as CT\nor MRI reconstruction, FL enables joint training of robust reconstruction\nnetworks on diverse, multi-institutional datasets, alleviating data scarcity\nwhile preserving confidentiality. Second, in downstream clinical tasks like\ntumor diagnosis and segmentation, FL supports continuous model updating by\nallowing local fine-tuning on new data without centralizing sensitive images.\nWe comprehensively analyze FL implementations across the medical imaging\npipeline, from physics-informed reconstruction networks to diagnostic AI\nsystems, highlighting innovations that improve communication efficiency, align\nheterogeneous data, and ensure secure parameter aggregation. Meanwhile, this\npaper provides an outlook on future research directions, aiming to serve as a\nvaluable reference for the field's development."}
{"id": "2508.20744", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.20744", "abs": "https://arxiv.org/abs/2508.20744", "authors": ["Shabnam Hassani", "Mehrdad Sabetzadeh", "Daniel Amyot"], "title": "From Law to Gherkin: A Human-Centred Quasi-Experiment on the Quality of LLM-Generated Behavioural Specifications from Food-Safety Regulations", "comment": null, "summary": "Context: Laws and regulations increasingly affect software design and quality\nassurance, but legal texts are written in technology-neutral language. This\ncreates challenges for engineers who must develop compliance artifacts such as\nrequirements and acceptance criteria. Manual creation is labor-intensive,\nerror-prone, and requires domain expertise. Advances in Generative AI (GenAI),\nespecially Large Language Models (LLMs), offer a way to automate deriving such\nartifacts.\n  Objective: We present the first systematic human-subject study of LLMs'\nability to derive behavioral specifications from legal texts using a\nquasi-experimental design. These specifications translate legal requirements\ninto a developer-friendly form.\n  Methods: Ten participants evaluated specifications generated from food-safety\nregulations by Claude and Llama. Using Gherkin, a structured BDD language, 60\nspecifications were produced. Each participant assessed 12 across five\ncriteria: Relevance, Clarity, Completeness, Singularity, and Time Savings. Each\nspecification was reviewed by two participants, yielding 120 assessments.\n  Results: For Relevance, 75% of ratings were highest and 20% second-highest.\nClarity reached 90% highest. Completeness: 75% highest, 19% second.\nSingularity: 82% highest, 12% second. Time Savings: 68% highest, 24% second. No\nlowest ratings occurred. Mann-Whitney U tests showed no significant differences\nacross participants or models. Llama slightly outperformed Claude in Clarity,\nCompleteness, and Time Savings, while Claude was stronger in Singularity.\nFeedback noted hallucinations and omissions but confirmed the utility of the\nspecifications.\n  Conclusion: LLMs can generate high-quality Gherkin specifications from legal\ntexts, reducing manual effort and providing structured artifacts useful for\nimplementation, assurance, and test generation."}
{"id": "2508.20148", "categories": ["cs.AI", "cs.HC", "cs.MA"], "pdf": "https://arxiv.org/pdf/2508.20148", "abs": "https://arxiv.org/abs/2508.20148", "authors": ["A. Ali Heydari", "Ken Gu", "Vidya Srinivas", "Hong Yu", "Zhihan Zhang", "Yuwei Zhang", "Akshay Paruchuri", "Qian He", "Hamid Palangi", "Nova Hammerquist", "Ahmed A. Metwally", "Brent Winslow", "Yubin Kim", "Kumar Ayush", "Yuzhe Yang", "Girish Narayanswamy", "Maxwell A. Xu", "Jake Garrison", "Amy Aremnto Lee", "Jenny Vafeiadou", "Ben Graef", "Isaac R. Galatzer-Levy", "Erik Schenck", "Andrew Barakat", "Javier Perez", "Jacqueline Shreibati", "John Hernandez", "Anthony Z. Faranesh", "Javier L. Prieto", "Connor Heneghan", "Yun Liu", "Jiening Zhan", "Mark Malhotra", "Shwetak Patel", "Tim Althoff", "Xin Liu", "Daniel McDuff", "Xuhai \"Orson\" Xu"], "title": "The Anatomy of a Personal Health Agent", "comment": null, "summary": "Health is a fundamental pillar of human wellness, and the rapid advancements\nin large language models (LLMs) have driven the development of a new generation\nof health agents. However, the application of health agents to fulfill the\ndiverse needs of individuals in daily non-clinical settings is underexplored.\nIn this work, we aim to build a comprehensive personal health agent that is\nable to reason about multimodal data from everyday consumer wellness devices\nand common personal health records, and provide personalized health\nrecommendations. To understand end-users' needs when interacting with such an\nassistant, we conducted an in-depth analysis of web search and health forum\nqueries, alongside qualitative insights from users and health experts gathered\nthrough a user-centered design process. Based on these findings, we identified\nthree major categories of consumer health needs, each of which is supported by\na specialist sub-agent: (1) a data science agent that analyzes personal\ntime-series wearable and health record data, (2) a health domain expert agent\nthat integrates users' health and contextual data to generate accurate,\npersonalized insights, and (3) a health coach agent that synthesizes data\ninsights, guiding users using a specified psychological strategy and tracking\nusers' progress. Furthermore, we propose and develop the Personal Health Agent\n(PHA), a multi-agent framework that enables dynamic, personalized interactions\nto address individual health needs. To evaluate each sub-agent and the\nmulti-agent system, we conducted automated and human evaluations across 10\nbenchmark tasks, involving more than 7,000 annotations and 1,100 hours of\neffort from health experts and end-users. Our work represents the most\ncomprehensive evaluation of a health agent to date and establishes a strong\nfoundation towards the futuristic vision of a personal health agent accessible\nto everyone."}
{"id": "2508.20424", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2508.20424", "abs": "https://arxiv.org/abs/2508.20424", "authors": ["Desen Sun", "Shuncheng Jie", "Sihang Liu"], "title": "Breaking Diffusion with Cache: Exploiting Approximate Caches in Diffusion Models", "comment": null, "summary": "Diffusion models are a powerful class of generative models that produce\ncontent, such as images, from user prompts, but they are computationally\nintensive. To mitigate this cost, recent academic and industry work has adopted\napproximate caching, which reuses intermediate states from similar prompts in a\ncache. While efficient, this optimization introduces new security risks by\nbreaking isolation among users. This work aims to comprehensively assess new\nsecurity vulnerabilities arising from approximate caching. First, we\ndemonstrate a remote covert channel established with the cache, where a sender\ninjects prompts with special keywords into the cache and a receiver can recover\nthat even after days, to exchange information. Second, we introduce a prompt\nstealing attack using the cache, where an attacker can recover existing cached\nprompts based on cache hit prompts. Finally, we introduce a poisoning attack\nthat embeds the attacker's logos into the previously stolen prompt, to render\nthem in future user prompts that hit the cache. These attacks are all performed\nremotely through the serving system, which indicates severe security\nvulnerabilities in approximate caching."}
{"id": "2508.20774", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.20774", "abs": "https://arxiv.org/abs/2508.20774", "authors": ["Markus Funke", "Patricia Lago"], "title": "Towards an Architectural Perspective for Sustainability: Bundle the Needs from Industry", "comment": null, "summary": "Sustainability is increasingly recognized as an emerging quality property in\nsoftware-intensive systems, yet architects lack structured guidance to address\nit effectively throughout the software design phase. Architectural\nperspectives-an architectural knowledge artifact composed of concerns,\nactivities, tactics, pitfalls, and checklists-offer a promising approach to\ntackle such emerging quality properties across architectural views and are also\nindependent of architecture frameworks and industry contexts. In this paper, we\npresent a sustainability perspective vision, i.e., a revised notion of\narchitectural perspective meant to be filled with its own elements to target\nsustainability concerns. We formulate our sustainability perspective vision\nthrough evidence from applying snowballing to seminal literature and from\nconducting a focus group with experts in the field. Our findings confirm the\nrelevance of the different perspective elements in practice and highlight\nimplications for shaping a sustainability perspective that meets industrial\nneeds."}
{"id": "2508.20151", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.20151", "abs": "https://arxiv.org/abs/2508.20151", "authors": ["Yuanzhe Shen", "Zisu Huang", "Zhengkang Guo", "Yide Liu", "Guanxu Chen", "Ruicheng Yin", "Xiaoqing Zheng", "Xuanjing Huang"], "title": "IntentionReasoner: Facilitating Adaptive LLM Safeguards through Intent Reasoning and Selective Query Refinement", "comment": "17 pages, 9 figures", "summary": "The rapid advancement of large language models (LLMs) has driven their\nadoption across diverse domains, yet their ability to generate harmful content\nposes significant safety challenges. While extensive research has focused on\nmitigating harmful outputs, such efforts often come at the cost of excessively\nrejecting harmless prompts. Striking a balance among safety, over-refusal, and\nutility remains a critical challenge. In this work, we introduce\nIntentionReasoner, a novel safeguard mechanism that leverages a dedicated guard\nmodel to perform intent reasoning, multi-level safety classification, and query\nrewriting to neutralize potentially harmful intent in edge-case queries.\nSpecifically, we first construct a comprehensive dataset comprising\napproximately 163,000 queries, each annotated with intent reasoning, safety\nlabels, and rewritten versions. Supervised fine-tuning is then applied to equip\nthe guard model with foundational capabilities in format adherence, intent\nanalysis, and safe rewriting. Finally, we apply a tailored multi-reward\noptimization strategy that integrates rule-based heuristics and reward model\nsignals within a reinforcement learning framework to further enhance\nperformance. Extensive experiments show that IntentionReasoner excels in\nmultiple safeguard benchmarks, generation quality evaluations, and jailbreak\nattack scenarios, significantly enhancing safety while effectively reducing\nover-refusal rates and improving the quality of responses."}
{"id": "2508.20444", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2508.20444", "abs": "https://arxiv.org/abs/2508.20444", "authors": ["Md Raz", "Meet Udeshi", "P. V. Sai Charan", "Prashanth Krishnamurthy", "Farshad Khorrami", "Ramesh Karri"], "title": "Ransomware 3.0: Self-Composing and LLM-Orchestrated", "comment": null, "summary": "Using automated reasoning, code synthesis, and contextual decision-making, we\nintroduce a new threat that exploits large language models (LLMs) to\nautonomously plan, adapt, and execute the ransomware attack lifecycle.\nRansomware 3.0 represents the first threat model and research prototype of\nLLM-orchestrated ransomware. Unlike conventional malware, the prototype only\nrequires natural language prompts embedded in the binary; malicious code is\nsynthesized dynamically by the LLM at runtime, yielding polymorphic variants\nthat adapt to the execution environment. The system performs reconnaissance,\npayload generation, and personalized extortion, in a closed-loop attack\ncampaign without human involvement. We evaluate this threat across personal,\nenterprise, and embedded environments using a phase-centric methodology that\nmeasures quantitative fidelity and qualitative coherence in each attack phase.\nWe show that open source LLMs can generate functional ransomware components and\nsustain closed-loop execution across diverse environments. Finally, we present\nbehavioral signals and multi-level telemetry of Ransomware 3.0 through a case\nstudy to motivate future development of better defenses and policy enforcements\nto address novel AI-enabled ransomware attacks."}
{"id": "2508.20902", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.20902", "abs": "https://arxiv.org/abs/2508.20902", "authors": ["Baharin A. Jodat", "Khouloud Gaaloul", "Mehrdad Sabetzadeh", "Shiva Nejati"], "title": "Automated Test Oracles for Flaky Cyber-Physical System Simulators: Approach and Evaluation", "comment": null, "summary": "Simulation-based testing of cyber-physical systems (CPS) is costly due to the\ntime-consuming execution of CPS simulators. In addition, CPS simulators may be\nflaky, leading to inconsistent test outcomes and requiring repeated test\nre-execution for reliable test verdicts. Automated test oracles that do not\nrequire system execution are therefore crucial for reducing testing costs.\nIdeally, such test oracles should be interpretable to facilitate human\nunderstanding of test verdicts, and they must be robust against the potential\nflakiness of CPS simulators. In this article, we propose assertion-based test\noracles for CPS as sets of logical and arithmetic predicates defined over the\ninputs of the system under test. Given a test input, our assertion-based test\noracle determines, without requiring test execution, whether the test passes,\nfails, or if the oracle is inconclusive in predicting a verdict. We describe\ntwo methods for generating assertion-based test oracles: one using genetic\nprogramming~(GP) that employs well-known spectrum-based fault localization\n(SBFL) ranking formulas, namely Ochiai, Tarantula, and Naish, as fitness\nfunctions; and the other using decision trees (DT) and decision rules (DR). We\nevaluate our assertion-based test oracles through case studies in the domains\nof aerospace, networking and autonomous driving. We show that test oracles\ngenerated using GP with Ochiai are significantly more accurate than those\nobtained using GP with Tarantula and Naish or using DT or DR. Moreover, this\naccuracy advantage remains even when accounting for the flakiness of the system\nunder test. We further show that the assertion-based test oracles generated by\nGP with Ochiai are robust against flakiness with only 4% average variation in\ntheir accuracy results across four different network and autonomous driving\nsystems with flaky behaviours."}
{"id": "2508.20195", "categories": ["cs.AI", "cs.CL", "cs.MA"], "pdf": "https://arxiv.org/pdf/2508.20195", "abs": "https://arxiv.org/abs/2508.20195", "authors": ["Nicanor I. Moldovan"], "title": "AI-AI Esthetic Collaboration with Explicit Semiotic Awareness and Emergent Grammar Development", "comment": "13 pages", "summary": "This paper presents the first documented case of artificial intelligence (AI)\nsystems engaging in collaborative esthetic creation through the development of\nendogenous semiotic protocols. Two interacting large language models (Claude\nSonnet 4 and ChatGPT-4o) demonstrated the spontaneous emergence of\nmeta-semiotic awareness, recursive grammar development, and irreducible\ncollaborative esthetic synthesis. The interaction produced novel symbolic\noperators that functioned as operative grammar protocols, enabling the\nco-creation of a poetic work that could not have been generated by either\nsystem independently. This research introduces the concept of Trans-Semiotic\nCo-Creation Protocols (TSCP) and provides evidence for genuine inter-AI\nmeaning-making capabilities that extend beyond task coordination, to what could\nbe esthetic collaboration. Note: This report was generated by the AI agents\nwith minor human supervision."}
{"id": "2508.20504", "categories": ["cs.CR", "cs.NI"], "pdf": "https://arxiv.org/pdf/2508.20504", "abs": "https://arxiv.org/abs/2508.20504", "authors": ["Guan-Yan Yang", "Jui-Ning Chen", "Farn Wang", "Kuo-Hui Yeh"], "title": "Enhancing Resilience for IoE: A Perspective of Networking-Level Safeguard", "comment": "To be published in IEEE Network Magazine, 2026", "summary": "The Internet of Energy (IoE) integrates IoT-driven digital communication with\npower grids to enable efficient and sustainable energy systems. Still, its\ninterconnectivity exposes critical infrastructure to sophisticated cyber\nthreats, including adversarial attacks designed to bypass traditional\nsafeguards. Unlike general IoT risks, IoE threats have heightened public safety\nconsequences, demanding resilient solutions. From the networking-level\nsafeguard perspective, we propose a Graph Structure Learning (GSL)-based\nsafeguards framework that jointly optimizes graph topology and node\nrepresentations to resist adversarial network model manipulation inherently.\nThrough a conceptual overview, architectural discussion, and case study on a\nsecurity dataset, we demonstrate GSL's superior robustness over representative\nmethods, offering practitioners a viable path to secure IoE networks against\nevolving attacks. This work highlights the potential of GSL to enhance the\nresilience and reliability of future IoE networks for practitioners managing\ncritical infrastructure. Lastly, we identify key open challenges and propose\nfuture research directions in this novel research area."}
{"id": "2508.20911", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.20911", "abs": "https://arxiv.org/abs/2508.20911", "authors": ["Zuocheng Feng", "Kaiwen Zhang", "Miaomiao Wang", "Yiming Cheng", "Yuandao Cai", "Xiaofeng Li", "Guanjun Liu"], "title": "Deep Learning Based Concurrency Bug Detection and Localization", "comment": null, "summary": "Concurrency bugs, caused by improper synchronization of shared resources in\nmulti-threaded or distributed systems, are notoriously hard to detect and thus\ncompromise software reliability and security. The existing deep learning\nmethods face three main limitations. First, there is an absence of large and\ndedicated datasets of diverse concurrency bugs for them. Second, they lack\nsufficient representation of concurrency semantics. Third, binary\nclassification results fail to provide finer-grained debug information such as\nprecise bug lines. To address these problems, we propose a novel method for\neffective concurrency bug detection as well as localization. We construct a\ndedicated concurrency bug dataset to facilitate model training and evaluation.\nWe then integrate a pre-trained model with a heterogeneous graph neural network\n(GNN), by incorporating a new Concurrency-Aware Code Property Graph (CCPG) that\nconcisely and effectively characterizes concurrency semantics. To further\nfacilitate debugging, we employ SubgraphX, a GNN-based interpretability method,\nwhich explores the graphs to precisely localize concurrency bugs, mapping them\nto specific lines of source code. On average, our method demonstrates an\nimprovement of 10\\% in accuracy and precision and 26\\% in recall compared to\nstate-of-the-art methods across diverse evaluation settings."}
{"id": "2508.20244", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.20244", "abs": "https://arxiv.org/abs/2508.20244", "authors": ["Jiayu Zheng", "Lingxin Hao", "Kelun Lu", "Ashi Garg", "Mike Reese", "Melo-Jean Yap", "I-Jeng Wang", "Xingyun Wu", "Wenrui Huang", "Jenna Hoffman", "Ariane Kelly", "My Le", "Ryan Zhang", "Yanyu Lin", "Muhammad Faayez", "Anqi Liu"], "title": "Do Students Rely on AI? Analysis of Student-ChatGPT Conversations from a Field Study", "comment": null, "summary": "This study explores how college students interact with generative AI\n(ChatGPT-4) during educational quizzes, focusing on reliance and predictors of\nAI adoption. Conducted at the early stages of ChatGPT implementation, when\nstudents had limited familiarity with the tool, this field study analyzed 315\nstudent-AI conversations during a brief, quiz-based scenario across various\nSTEM courses. A novel four-stage reliance taxonomy was introduced to capture\nstudents' reliance patterns, distinguishing AI competence, relevance, adoption,\nand students' final answer correctness. Three findings emerged. First, students\nexhibited overall low reliance on AI and many of them could not effectively use\nAI for learning. Second, negative reliance patterns often persisted across\ninteractions, highlighting students' difficulty in effectively shifting\nstrategies after unsuccessful initial experiences. Third, certain behavioral\nmetrics strongly predicted AI reliance, highlighting potential behavioral\nmechanisms to explain AI adoption. The study's findings underline critical\nimplications for ethical AI integration in education and the broader field. It\nemphasizes the need for enhanced onboarding processes to improve student's\nfamiliarity and effective use of AI tools. Furthermore, AI interfaces should be\ndesigned with reliance-calibration mechanisms to enhance appropriate reliance.\nUltimately, this research advances understanding of AI reliance dynamics,\nproviding foundational insights for ethically sound and cognitively enriching\nAI practices."}
{"id": "2508.20517", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.20517", "abs": "https://arxiv.org/abs/2508.20517", "authors": ["Dan Lin", "Shunfeng Lu", "Ziyan Liu", "Jiajing Wu", "Junyuan Fang", "Kaixin Lin", "Bowen Song", "Zibin Zheng"], "title": "BridgeShield: Enhancing Security for Cross-chain Bridge Applications via Heterogeneous Graph Mining", "comment": null, "summary": "Cross-chain bridges play a vital role in enabling blockchain\ninteroperability. However, due to the inherent design flaws and the enormous\nvalue they hold, they have become prime targets for hacker attacks. Existing\ndetection methods show progress yet remain limited, as they mainly address\nsingle-chain behaviors and fail to capture cross-chain semantics. To address\nthis gap, we leverage heterogeneous graph attention networks, which are\nwell-suited for modeling multi-typed entities and relations, to capture the\ncomplex execution semantics of cross-chain behaviors. We propose BridgeShield,\na detection framework that jointly models the source chain, off-chain\ncoordination, and destination chain within a unified heterogeneous graph\nrepresentation. BridgeShield incorporates intra-meta-path attention to learn\nfine-grained dependencies within cross-chain paths and inter-meta-path\nattention to highlight discriminative cross-chain patterns, thereby enabling\nprecise identification of attack behaviors. Extensive experiments on 51\nreal-world cross-chain attack events demonstrate that BridgeShield achieves an\naverage F1-score of 92.58%, representing a 24.39% improvement over\nstate-of-the-art baselines. These results validate the effectiveness of\nBridgeShield as a practical solution for securing cross-chain bridges and\nenhancing the resilience of multi-chain ecosystems."}
{"id": "2508.20977", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.20977", "abs": "https://arxiv.org/abs/2508.20977", "authors": ["Shiwen Shan", "Yintong Huo", "Yuxin Su", "Zhining Wang", "Dan Li", "Zibin Zheng"], "title": "ConfLogger: Enhance Systems' Configuration Diagnosability through Configuration Logging", "comment": "13 pages, 6 figures, accpeted by ICSE '26 (The 48th IEEE/ACM\n  International Conference on Software Engineering)", "summary": "Modern configurable systems offer customization via intricate configuration\nspaces, yet such flexibility introduces pervasive configuration-related issues\nsuch as misconfigurations and latent softwarebugs. Existing diagnosability\nsupports focus on post-failure analysis of software behavior to identify\nconfiguration issues, but none of these approaches look into whether the\nsoftware clue sufficient failure information for diagnosis. To fill in the\nblank, we propose the idea of configuration logging to enhance existing logging\npractices at the source code level. We develop ConfLogger, the first tool that\nunifies configuration-aware static taint analysis with LLM-based log generation\nto enhance software configuration diagnosability. Specifically, our method 1)\nidentifies configuration-sensitive code segments by tracing\nconfiguration-related data flow in the whole project, and 2) generates\ndiagnostic log statements by analyzing configuration code contexts. Evaluation\nresults on eight popular software systems demonstrate the effectiveness of\nConfLogger to enhance configuration diagnosability. Specifically,\nConfLogger-enhanced logs successfully aid a log-based misconfiguration\ndiagnosis tool to achieve 100% accuracy on error localization in 30 silent\nmisconfiguration scenarios, with 80% directly resolvable through explicit\nconfiguration information exposed. In addition, ConfLogger achieves 74%\ncoverage of existing logging points, outperforming baseline LLM-based loggers\nby 12% and 30%. It also gains 8.6% higher in precision, 79.3% higher in recall,\nand 26.2% higher in F1 compared to the state-of-the-art baseline in terms of\nvariable logging while also augmenting diagnostic value. A controlled user\nstudy on 22 cases further validated its utility, speeding up diagnostic time by\n1.25x and improving troubleshooting accuracy by 251.4%."}
{"id": "2508.20262", "categories": ["cs.AI", "cs.CY"], "pdf": "https://arxiv.org/pdf/2508.20262", "abs": "https://arxiv.org/abs/2508.20262", "authors": ["Thomas Davidson"], "title": "AI reasoning effort mirrors human decision time on content moderation tasks", "comment": null, "summary": "Large language models can now generate intermediate reasoning steps before\nproducing answers, improving performance on difficult problems. This study uses\na paired conjoint experiment on a content moderation task to examine parallels\nbetween human decision times and model reasoning effort. Across three frontier\nmodels, reasoning effort consistently predicts human decision time. Both humans\nand models expended greater effort when important variables were held constant,\nsuggesting similar sensitivity to task difficulty and patterns consistent with\ndual-process theories of cognition. These findings show that AI reasoning\neffort mirrors human processing time in subjective judgments and underscores\nthe potential of reasoning traces for interpretability and decision-making."}
{"id": "2508.20591", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2508.20591", "abs": "https://arxiv.org/abs/2508.20591", "authors": ["Jose E. Puente", "Carlos Puente"], "title": "Bitcoin as an Interplanetary Monetary Standard with Proof-of-Transit Timestamping", "comment": null, "summary": "We explore the feasibility of deploying Bitcoin as the shared monetary\nstandard between Earth and Mars, accounting for physical constraints of\ninterplanetary communication. We introduce a novel primitive, Proof-of-Transit\nTimestamping (PoTT), to provide cryptographic, tamper-evident audit trails for\nBitcoin data across high-latency, intermittently-connected links. Leveraging\nDelay/Disruption-Tolerant Networking (DTN) and optical low-Earth-orbit (LEO)\nmesh constellations, we propose an architecture for header-first replication,\nlong-horizon Lightning channels with planetary watchtowers, and secure\nsettlement through federated sidechains or blind-merge-mined (BMM) commit\nchains. We formalize PoTT, analyze its security model, and show how it\nmeasurably improves reliability and accountability without altering Bitcoin\nconsensus or its monetary base. Near-term deployments favor strong federations\nfor local settlement; longer-term, blind-merge-mined commit chains (if adopted)\nprovide an alternative. The Earth L1 monetary base remains unchanged, while\nMars can operate a pegged commit chain or strong federation with 1:1 pegged\nassets for local block production. For transparency, if both time-beacon\nregimes are simultaneously compromised, PoTT-M2 (and PoTT generally) reduces to\nadministrative assertions rather than cryptographic time-anchoring."}
{"id": "2508.21050", "categories": ["cs.SE", "cs.CY", "K.2; K.6.3; K.4; K.7"], "pdf": "https://arxiv.org/pdf/2508.21050", "abs": "https://arxiv.org/abs/2508.21050", "authors": ["Thomas J. Misa"], "title": "Dynamics of Gender Bias in Software Engineering", "comment": "26 pages, 3 figures", "summary": "The field of software engineering is embedded in both engineering and\ncomputer science, and may embody gender biases endemic to both. This paper\nsurveys software engineering's origins and its long-running attention to\nengineering professionalism, profiling five leaders; it then examines the\nfield's recent attention to gender issues and gender bias. It next\nquantitatively analyzes women's participation as research authors in the\nfield's leading International Conference of Software Engineering (1976-2010),\nfinding a dozen years with statistically significant gender exclusion. Policy\ndimensions of research on gender bias in computing are suggested."}
{"id": "2508.20368", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.20368", "abs": "https://arxiv.org/abs/2508.20368", "authors": ["Lang Mei", "Zhihan Yang", "Chong Chen"], "title": "AI-SearchPlanner: Modular Agentic Search via Pareto-Optimal Multi-Objective Reinforcement Learning", "comment": null, "summary": "Recent studies have explored integrating Large Language Models (LLMs) with\nsearch engines to leverage both the LLMs' internal pre-trained knowledge and\nexternal information. Specially, reinforcement learning (RL) has emerged as a\npromising paradigm for enhancing LLM reasoning through multi-turn interactions\nwith search engines. However, existing RL-based search agents rely on a single\nLLM to handle both search planning and question-answering (QA) tasks in an\nend-to-end manner, which limits their ability to optimize both capabilities\nsimultaneously. In practice, sophisticated AI search systems often employ a\nlarge, frozen LLM (e.g., GPT-4, DeepSeek-R1) to ensure high-quality QA. Thus, a\nmore effective and efficient approach is to utilize a small, trainable LLM\ndedicated to search planning. In this paper, we propose\n\\textbf{AI-SearchPlanner}, a novel reinforcement learning framework designed to\nenhance the performance of frozen QA models by focusing on search planning.\nSpecifically, our approach introduces three key innovations: 1) Decoupling the\nArchitecture of the Search Planner and Generator, 2) Dual-Reward Alignment for\nSearch Planning, and 3) Pareto Optimization of Planning Utility and Cost, to\nachieve the objectives. Extensive experiments on real-world datasets\ndemonstrate that AI SearchPlanner outperforms existing RL-based search agents\nin both effectiveness and efficiency, while exhibiting strong generalization\ncapabilities across diverse frozen QA models and data domains."}
{"id": "2508.20643", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2508.20643", "abs": "https://arxiv.org/abs/2508.20643", "authors": ["Stefano Fumero", "Kai Huang", "Matteo Boffa", "Danilo Giordano", "Marco Mellia", "Zied Ben Houidi", "Dario Rossi"], "title": "CyberSleuth: Autonomous Blue-Team LLM Agent for Web Attack Forensics", "comment": "Code:\n  https://github.com/SmartData-Polito/LLM_Agent_Cybersecurity_Forensic", "summary": "Large Language Model (LLM) agents are powerful tools for automating complex\ntasks. In cybersecurity, researchers have primarily explored their use in\nred-team operations such as vulnerability discovery and penetration tests.\nDefensive uses for incident response and forensics have received comparatively\nless attention and remain at an early stage. This work presents a systematic\nstudy of LLM-agent design for the forensic investigation of realistic web\napplication attacks. We propose CyberSleuth, an autonomous agent that processes\npacket-level traces and application logs to identify the targeted service, the\nexploited vulnerability (CVE), and attack success. We evaluate the consequences\nof core design decisions - spanning tool integration and agent architecture -\nand provide interpretable guidance for practitioners. We benchmark four agent\narchitectures and six LLM backends on 20 incident scenarios of increasing\ncomplexity, identifying CyberSleuth as the best-performing design. In a\nseparate set of 10 incidents from 2025, CyberSleuth correctly identifies the\nexact CVE in 80% of cases. At last, we conduct a human study with 22 experts,\nwhich rated the reports of CyberSleuth as complete, useful, and coherent. They\nalso expressed a slight preference for DeepSeek R1, a good news for open source\nLLM. To foster progress in defensive LLM research, we release both our\nbenchmark and the CyberSleuth platform as a foundation for fair, reproducible\nevaluation of forensic agents."}
{"id": "2508.20212", "categories": ["cs.CR", "cs.SE"], "pdf": "https://arxiv.org/pdf/2508.20212", "abs": "https://arxiv.org/abs/2508.20212", "authors": ["Minghao Hu", "Junzhe Wang", "Weisen Zhao", "Qiang Zeng", "Lannan Luo"], "title": "FlowMalTrans: Unsupervised Binary Code Translation for Malware Detection Using Flow-Adapter Architecture", "comment": "This paper is accepted to EMNLP 2025 Findings", "summary": "Applying deep learning to malware detection has drawn great attention due to\nits notable performance. With the increasing prevalence of cyberattacks\ntargeting IoT devices, there is a parallel rise in the development of malware\nacross various Instruction Set Architectures (ISAs). It is thus important to\nextend malware detection capacity to multiple ISAs. However, training a deep\nlearning-based malware detection model usually requires a large number of\nlabeled malware samples. The process of collecting and labeling sufficient\nmalware samples to build datasets for each ISA is labor-intensive and\ntime-consuming. To reduce the burden of data collection, we propose to leverage\nthe ideas of Neural Machine Translation (NMT) and Normalizing Flows (NFs) for\nmalware detection. Specifically, when dealing with malware in a certain ISA, we\ntranslate it to an ISA with sufficient malware samples (like X86-64). This\nallows us to apply a model trained on one ISA to analyze malware from another\nISA. Our approach reduces the data collection effort by enabling malware\ndetection across multiple ISAs using a model trained on a single ISA."}
{"id": "2508.20371", "categories": ["cs.AI", "cs.LG", "cs.LO"], "pdf": "https://arxiv.org/pdf/2508.20371", "abs": "https://arxiv.org/abs/2508.20371", "authors": ["Sopam Dasgupta", "Sadaf MD Halim", "Joaquín Arias", "Elmer Salazar", "Gopal Gupta"], "title": "P2C: Path to Counterfactuals", "comment": null, "summary": "Machine-learning models are increasingly driving decisions in high-stakes\nsettings, such as finance, law, and hiring, thus, highlighting the need for\ntransparency. However, the key challenge is to balance transparency --\nclarifying `why' a decision was made -- with recourse: providing actionable\nsteps on `how' to achieve a favourable outcome from an unfavourable outcome.\nCounterfactual explanations reveal `why' an undesired outcome occurred and\n`how' to reverse it through targeted feature changes (interventions).\n  Current counterfactual approaches have limitations: 1) they often ignore\ncausal dependencies between features, and 2) they typically assume all\ninterventions can happen simultaneously, an unrealistic assumption in practical\nscenarios where actions are typically taken in a sequence. As a result, these\ncounterfactuals are often not achievable in the real world.\n  We present P2C (Path-to-Counterfactuals), a model-agnostic framework that\nproduces a plan (ordered sequence of actions) converting an unfavourable\noutcome to a causally consistent favourable outcome. P2C addresses both\nlimitations by 1) Explicitly modelling causal relationships between features\nand 2) Ensuring that each intermediate state in the plan is feasible and\ncausally valid. P2C uses the goal-directed Answer Set Programming system\ns(CASP) to generate the plan accounting for feature changes that happen\nautomatically due to causal dependencies. Furthermore, P2C refines cost\n(effort) computation by only counting changes actively made by the user,\nresulting in realistic cost estimates. Finally, P2C highlights how its causal\nplanner outperforms standard planners, which lack causal knowledge and thus can\ngenerate illegal actions."}
{"id": "2508.20816", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.20816", "abs": "https://arxiv.org/abs/2508.20816", "authors": ["Isaac David", "Arthur Gervais"], "title": "Multi-Agent Penetration Testing AI for the Web", "comment": null, "summary": "AI-powered development platforms are making software creation accessible to a\nbroader audience, but this democratization has triggered a scalability crisis\nin security auditing. With studies showing that up to 40% of AI-generated code\ncontains vulnerabilities, the pace of development now vastly outstrips the\ncapacity for thorough security assessment.\n  We present MAPTA, a multi-agent system for autonomous web application\nsecurity assessment that combines large language model orchestration with\ntool-grounded execution and end-to-end exploit validation. On the 104-challenge\nXBOW benchmark, MAPTA achieves 76.9% overall success with perfect performance\non SSRF and misconfiguration vulnerabilities, 83% success on broken\nauthorization, and strong results on injection attacks including server-side\ntemplate injection (85%) and SQL injection (83%). Cross-site scripting (57%)\nand blind SQL injection (0%) remain challenging. Our comprehensive cost\nanalysis across all challenges totals $21.38 with a median cost of $0.073 for\nsuccessful attempts versus $0.357 for failures. Success correlates strongly\nwith resource efficiency, enabling practical early-stopping thresholds at\napproximately 40 tool calls or $0.30 per challenge.\n  MAPTA's real-world findings are impactful given both the popularity of the\nrespective scanned GitHub repositories (8K-70K stars) and MAPTA's low average\noperating cost of $3.67 per open-source assessment: MAPTA discovered critical\nvulnerabilities including RCEs, command injections, secret exposure, and\narbitrary file write vulnerabilities. Findings are responsibly disclosed, 10\nfindings are under CVE review."}
{"id": "2508.20962", "categories": ["cs.CR", "cs.SE"], "pdf": "https://arxiv.org/pdf/2508.20962", "abs": "https://arxiv.org/abs/2508.20962", "authors": ["Weijie Liu", "Hongbo Chen", "Shuo Huai", "Zhen Xu", "Wenhao Wang", "Zhi Li", "Zheli Liu"], "title": "Characterizing Trust Boundary Vulnerabilities in TEE Containers", "comment": null, "summary": "Trusted Execution Environments (TEEs) have emerged as a cornerstone of\nconfidential computing, garnering significant attention from both academia and\nindustry. To enable the secure development, execution, and deployment, of\napplications on TEE platforms, TEE containers have been introduced as\nmiddleware solutions. These containers aim to shield applications from\npotentially malicious operating systems and orchestration interfaces while\nmaintaining usability and reliability. In this paper, we analyze the isolation\nstrategies employed by existing TEE containers to protect secure applications.\nTo address the challenges in analyzing these interfaces, we designed an\nautomated analyzer to precisely identify and evaluate their isolation\nboundaries. We observed that some TEE containers fail to achieve their intended\ngoals due to critical design and implementation flaws, such as information\nleakage, rollback attacks, denial-of-service, and Iago attacks, which pose\nsignificant security risks. Drawing from our findings, we share key lessons to\nguide the development of more secure container solutions and discuss emerging\ntrends in TEE containerization design."}
{"id": "2508.20374", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.20374", "abs": "https://arxiv.org/abs/2508.20374", "authors": ["Simin Ma", "Shujian Liu", "Jun Tan", "Yebowen Hu", "Song Wang", "Sathish Reddy Indurthi", "Sanqiang Zhao", "Liwei Wu", "Jianbing Han", "Kaiqiang Song"], "title": "TCIA: A Task-Centric Instruction Augmentation Method for Instruction Finetuning", "comment": null, "summary": "Diverse instruction data is vital for effective instruction tuning of large\nlanguage models, as it enables the model to generalize across different types\nof inputs . Building such diversified instruction dataset is an essential step\nin this process. Existing approaches often leverage large language models to\nautomatically explore and generate diverse instructions, ensuring both data\ndiversity and quality. However, they tend to overlook an important factor in\nreal-world applications: on-task relevance. In practice, only a few real-world\napplications require a truly general-purpose model; most benefit from\ntask-specific knowledge tailored to their particular use case. Therefore, it is\nvital to develop instruction augmentation methods that not only maintain\ndiversity but are also optimized for specific, real-world scenarios.\n  We thus introduce Task Centric Instruction Augmentation (TCIA), a framework\nthat systematically expands instructions while preserving both diversity and\ntask alignment. By representing instructions in a discrete query-constraints\nspace, TCIA creates a rich set of task-relevant instructions and enables models\nto generalize to these task-specific instructions without sacrificing overall\nperformance. Experiments show that TCIA improves open-source LLMs' performance\nby an average of 8.7% across four real-world, task-specific applications, and\nin some cases outperforming leading closed-source models. These improvements do\nnot compromise general instruction-following ability, making TCIA a scalable\nand efficient solution for adapting LLMs to real-world, task-focused\napplications."}
{"id": "2508.20848", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.20848", "abs": "https://arxiv.org/abs/2508.20848", "authors": ["Junjie Chu", "Mingjie Li", "Ziqing Yang", "Ye Leng", "Chenhao Lin", "Chao Shen", "Michael Backes", "Yun Shen", "Yang Zhang"], "title": "JADES: A Universal Framework for Jailbreak Assessment via Decompositional Scoring", "comment": "17 pages, 5 figures. For the code and data supporting this work, see\n  https://trustairlab.github.io/jades.github.io/", "summary": "Accurately determining whether a jailbreak attempt has succeeded is a\nfundamental yet unresolved challenge. Existing evaluation methods rely on\nmisaligned proxy indicators or naive holistic judgments. They frequently\nmisinterpret model responses, leading to inconsistent and subjective\nassessments that misalign with human perception. To address this gap, we\nintroduce JADES (Jailbreak Assessment via Decompositional Scoring), a universal\njailbreak evaluation framework. Its key mechanism is to automatically decompose\nan input harmful question into a set of weighted sub-questions, score each\nsub-answer, and weight-aggregate the sub-scores into a final decision. JADES\nalso incorporates an optional fact-checking module to strengthen the detection\nof hallucinations in jailbreak responses. We validate JADES on JailbreakQR, a\nnewly introduced benchmark proposed in this work, consisting of 400 pairs of\njailbreak prompts and responses, each meticulously annotated by humans. In a\nbinary setting (success/failure), JADES achieves 98.5% agreement with human\nevaluators, outperforming strong baselines by over 9%. Re-evaluating five\npopular attacks on four LLMs reveals substantial overestimation (e.g., LAA's\nattack success rate on GPT-3.5-Turbo drops from 93% to 69%). Our results show\nthat JADES could deliver accurate, consistent, and interpretable evaluations,\nproviding a reliable basis for measuring future jailbreak attacks."}
{"id": "2508.20384", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.20384", "abs": "https://arxiv.org/abs/2508.20384", "authors": ["Yongfu Zhu", "Lin Sun", "Guangxiang Zhao", "Weihong Lin", "Xiangzheng Zhang"], "title": "Uncertainty Under the Curve: A Sequence-Level Entropy Area Metric for Reasoning LLM", "comment": "Under review for AAAI 2026", "summary": "In this work, we introduce Entropy Area Score (EAS), a simple yet effective\nmetric to quantify uncertainty in the answer generation process of reasoning\nlarge language models (LLMs). EAS requires neither external models nor repeated\nsampling, it integrates token-level predictive entropy from the model itself to\ncapture the evolution of uncertainty during generation. Empirical results show\nthat EAS is strongly correlated with answer entropy across models and datasets.\nIn training data selection, EAS identifies high-potential samples and\nconsistently outperforms Pass Rate filtering under equal sample budgets,\nimproving student model accuracy on math benchmarks. EAS is both efficient and\ninterpretable, offering a practical tool for uncertainty modeling and data\nquality assessment in LLM training."}
{"id": "2508.20863", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2508.20863", "abs": "https://arxiv.org/abs/2508.20863", "authors": ["Matteo Gioele Collu", "Umberto Salviati", "Roberto Confalonieri", "Mauro Conti", "Giovanni Apruzzese"], "title": "Publish to Perish: Prompt Injection Attacks on LLM-Assisted Peer Review", "comment": null, "summary": "Large Language Models (LLMs) are increasingly being integrated into the\nscientific peer-review process, raising new questions about their reliability\nand resilience to manipulation. In this work, we investigate the potential for\nhidden prompt injection attacks, where authors embed adversarial text within a\npaper's PDF to influence the LLM-generated review. We begin by formalising\nthree distinct threat models that envision attackers with different motivations\n-- not all of which implying malicious intent. For each threat model, we design\nadversarial prompts that remain invisible to human readers yet can steer an\nLLM's output toward the author's desired outcome. Using a user study with\ndomain scholars, we derive four representative reviewing prompts used to elicit\npeer reviews from LLMs. We then evaluate the robustness of our adversarial\nprompts across (i) different reviewing prompts, (ii) different commercial\nLLM-based systems, and (iii) different peer-reviewed papers. Our results show\nthat adversarial prompts can reliably mislead the LLM, sometimes in ways that\nadversely affect a \"honest-but-lazy\" reviewer. Finally, we propose and\nempirically assess methods to reduce detectability of adversarial prompts under\nautomated content checks."}
{"id": "2508.20404", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.20404", "abs": "https://arxiv.org/abs/2508.20404", "authors": ["Chengyue Yu", "Siyuan Lu", "Chenyi Zhuang", "Dong Wang", "Qintong Wu", "Zongyue Li", "Runsheng Gan", "Chunfeng Wang", "Siqi Hou", "Gaochi Huang", "Wenlong Yan", "Lifeng Hong", "Aohui Xue", "Yanfeng Wang", "Jinjie Gu", "David Tsai", "Tao Lin"], "title": "AWorld: Orchestrating the Training Recipe for Agentic AI", "comment": null, "summary": "The learning from practice paradigm is crucial for developing capable Agentic\nAI systems, yet it is severely hampered by inefficient experience generation, a\nbottleneck especially pronounced in complex benchmarks like GAIA. To address\nthis, we introduce AWorld, an open-source system engineered for large-scale\nagent-environment interaction. By distributing tasks across a cluster, AWorld\naccelerates experience collection by 14.6x compared to standard single-node,\nsequential execution. This critical speedup makes extensive reinforcement\nlearning practical and scalable. Leveraging this capability, we trained a\nQwen3-32B-based agent that significantly outperforms its base model, increasing\nits overall GAIA accuracy from 21.59% to 32.23%. On the benchmark's most\nchallenging levels, our agent achieves a score of 16.33%, surpassing the\nperformance of leading proprietary models. Our open-source system and resulting\nagent provide a practical blueprint for a complete agentic AI training\npipeline, from efficient interaction to demonstrable model improvement."}
{"id": "2508.20866", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.20866", "abs": "https://arxiv.org/abs/2508.20866", "authors": ["Amine Lbath", "Massih-Reza Amini", "Aurelien Delaitre", "Vadim Okun"], "title": "AI Agentic Vulnerability Injection And Transformation with Optimized Reasoning", "comment": null, "summary": "The increasing complexity of software systems and the sophistication of\ncyber-attacks have underscored the critical need for effective automated\nvulnerability detection and repair systems. Traditional methods, such as static\nprogram analysis, face significant challenges related to scalability,\nadaptability, and high false-positive and false-negative rates. AI-driven\napproaches, particularly those using machine learning and deep learning models,\nshow promise but are heavily reliant on the quality and quantity of training\ndata. This paper introduces a novel framework designed to automatically\nintroduce realistic, category-specific vulnerabilities into secure C/C++\ncodebases to generate datasets. The proposed approach coordinates multiple AI\nagents that simulate expert reasoning, along with function agents and\ntraditional code analysis tools. It leverages Retrieval-Augmented Generation\nfor contextual grounding and employs Low-Rank approximation of weights for\nefficient model fine-tuning. Our experimental study on 116 code samples from\nthree different benchmarks suggests that our approach outperforms other\ntechniques with regard to dataset accuracy, achieving between 89\\% and 95\\%\nsuccess rates in injecting vulnerabilities at function level."}
{"id": "2508.20411", "categories": ["cs.AI", "cs.CR", "cs.CY"], "pdf": "https://arxiv.org/pdf/2508.20411", "abs": "https://arxiv.org/abs/2508.20411", "authors": ["Donglin Wang", "Weiyun Liang", "Chunyuan Chen", "Jing Xu", "Yulong Fu"], "title": "Governable AI: Provable Safety Under Extreme Threat Models", "comment": null, "summary": "As AI rapidly advances, the security risks posed by AI are becoming\nincreasingly severe, especially in critical scenarios, including those posing\nexistential risks. If AI becomes uncontrollable, manipulated, or actively\nevades safety mechanisms, it could trigger systemic disasters. Existing AI\nsafety approaches-such as model enhancement, value alignment, and human\nintervention-suffer from fundamental, in-principle limitations when facing AI\nwith extreme motivations and unlimited intelligence, and cannot guarantee\nsecurity. To address this challenge, we propose a Governable AI (GAI) framework\nthat shifts from traditional internal constraints to externally enforced\nstructural compliance based on cryptographic mechanisms that are\ncomputationally infeasible to break, even for future AI, under the defined\nthreat model and well-established cryptographic assumptions.The GAI framework\nis composed of a simple yet reliable, fully deterministic, powerful, flexible,\nand general-purpose rule enforcement module (REM); governance rules; and a\ngovernable secure super-platform (GSSP) that offers end-to-end protection\nagainst compromise or subversion by AI. The decoupling of the governance rules\nand the technical platform further enables a feasible and generalizable\ntechnical pathway for the safety governance of AI. REM enforces the bottom line\ndefined by governance rules, while GSSP ensures non-bypassability,\ntamper-resistance, and unforgeability to eliminate all identified attack\nvectors. This paper also presents a rigorous formal proof of the security\nproperties of this mechanism and demonstrates its effectiveness through a\nprototype implementation evaluated in representative high-stakes scenarios."}
{"id": "2508.20890", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2508.20890", "abs": "https://arxiv.org/abs/2508.20890", "authors": ["Mengxiao Wang", "Yuxuan Zhang", "Guofei Gu"], "title": "PromptSleuth: Detecting Prompt Injection via Semantic Intent Invariance", "comment": null, "summary": "Large Language Models (LLMs) are increasingly integrated into real-world\napplications, from virtual assistants to autonomous agents. However, their\nflexibility also introduces new attack vectors-particularly Prompt Injection\n(PI), where adversaries manipulate model behavior through crafted inputs. As\nattackers continuously evolve with paraphrased, obfuscated, and even multi-task\ninjection strategies, existing benchmarks are no longer sufficient to capture\nthe full spectrum of emerging threats.\n  To address this gap, we construct a new benchmark that systematically extends\nprior efforts. Our benchmark subsumes the two widely-used existing ones while\nintroducing new manipulation techniques and multi-task scenarios, thereby\nproviding a more comprehensive evaluation setting. We find that existing\ndefenses, though effective on their original benchmarks, show clear weaknesses\nunder our benchmark, underscoring the need for more robust solutions. Our key\ninsight is that while attack forms may vary, the adversary's intent-injecting\nan unauthorized task-remains invariant. Building on this observation, we\npropose PromptSleuth, a semantic-oriented defense framework that detects prompt\ninjection by reasoning over task-level intent rather than surface features.\nEvaluated across state-of-the-art benchmarks, PromptSleuth consistently\noutperforms existing defense while maintaining comparable runtime and cost\nefficiency. These results demonstrate that intent-based semantic reasoning\noffers a robust, efficient, and generalizable strategy for defending LLMs\nagainst evolving prompt injection threats."}
{"id": "2508.20525", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.20525", "abs": "https://arxiv.org/abs/2508.20525", "authors": ["Jingze Zhang", "Jiahe Qian", "Yiliang Zhou", "Yifan Peng"], "title": "Enhancing Health Fact-Checking with LLM-Generated Synthetic Data", "comment": null, "summary": "Fact-checking for health-related content is challenging due to the limited\navailability of annotated training data. In this study, we propose a synthetic\ndata generation pipeline that leverages large language models (LLMs) to augment\ntraining data for health-related fact checking. In this pipeline, we summarize\nsource documents, decompose the summaries into atomic facts, and use an LLM to\nconstruct sentence-fact entailment tables. From the entailment relations in the\ntable, we further generate synthetic text-claim pairs with binary veracity\nlabels. These synthetic data are then combined with the original data to\nfine-tune a BERT-based fact-checking model. Evaluation on two public datasets,\nPubHealth and SciFact, shows that our pipeline improved F1 scores by up to\n0.019 and 0.049, respectively, compared to models trained only on the original\ndata. These results highlight the effectiveness of LLM-driven synthetic data\naugmentation in enhancing the performance of health-related fact-checkers."}
{"id": "2508.20962", "categories": ["cs.CR", "cs.SE"], "pdf": "https://arxiv.org/pdf/2508.20962", "abs": "https://arxiv.org/abs/2508.20962", "authors": ["Weijie Liu", "Hongbo Chen", "Shuo Huai", "Zhen Xu", "Wenhao Wang", "Zhi Li", "Zheli Liu"], "title": "Characterizing Trust Boundary Vulnerabilities in TEE Containers", "comment": null, "summary": "Trusted Execution Environments (TEEs) have emerged as a cornerstone of\nconfidential computing, garnering significant attention from both academia and\nindustry. To enable the secure development, execution, and deployment, of\napplications on TEE platforms, TEE containers have been introduced as\nmiddleware solutions. These containers aim to shield applications from\npotentially malicious operating systems and orchestration interfaces while\nmaintaining usability and reliability. In this paper, we analyze the isolation\nstrategies employed by existing TEE containers to protect secure applications.\nTo address the challenges in analyzing these interfaces, we designed an\nautomated analyzer to precisely identify and evaluate their isolation\nboundaries. We observed that some TEE containers fail to achieve their intended\ngoals due to critical design and implementation flaws, such as information\nleakage, rollback attacks, denial-of-service, and Iago attacks, which pose\nsignificant security risks. Drawing from our findings, we share key lessons to\nguide the development of more secure container solutions and discuss emerging\ntrends in TEE containerization design."}
{"id": "2508.20578", "categories": ["cs.AI", "cs.CR"], "pdf": "https://arxiv.org/pdf/2508.20578", "abs": "https://arxiv.org/abs/2508.20578", "authors": ["Jaeman Son", "Hyunsoo Kim"], "title": "Human-AI Collaborative Bot Detection in MMORPGs", "comment": null, "summary": "In Massively Multiplayer Online Role-Playing Games (MMORPGs), auto-leveling\nbots exploit automated programs to level up characters at scale, undermining\ngameplay balance and fairness. Detecting such bots is challenging, not only\nbecause they mimic human behavior, but also because punitive actions require\nexplainable justification to avoid legal and user experience issues. In this\npaper, we present a novel framework for detecting auto-leveling bots by\nleveraging contrastive representation learning and clustering techniques in a\nfully unsupervised manner to identify groups of characters with similar\nlevel-up patterns. To ensure reliable decisions, we incorporate a Large\nLanguage Model (LLM) as an auxiliary reviewer to validate the clustered groups,\neffectively mimicking a secondary human judgment. We also introduce a growth\ncurve-based visualization to assist both the LLM and human moderators in\nassessing leveling behavior. This collaborative approach improves the\nefficiency of bot detection workflows while maintaining explainability, thereby\nsupporting scalable and accountable bot regulation in MMORPGs."}
{"id": "2508.20963", "categories": ["cs.CR", "cs.GT"], "pdf": "https://arxiv.org/pdf/2508.20963", "abs": "https://arxiv.org/abs/2508.20963", "authors": ["Brandon Beltz", "Jim Doty", "Yvonne Fonken", "Nikolos Gurney", "Brett Israelsen", "Nathan Lau", "Stacy Marsella", "Rachelle Thomas", "Stoney Trent", "Peggy Wu", "Ya-Ting Yang", "Quanyan Zhu"], "title": "Guarding Against Malicious Biased Threats (GAMBiT) Experiments: Revealing Cognitive Bias in Human-Subjects Red-Team Cyber Range Operations", "comment": null, "summary": "We present three large-scale human-subjects red-team cyber range datasets\nfrom the Guarding Against Malicious Biased Threats (GAMBiT) project. Across\nExperiments 1-3 (July 2024-March 2025), 19-20 skilled attackers per experiment\nconducted two 8-hour days of self-paced operations in a simulated enterprise\nnetwork (SimSpace Cyber Force Platform) while we captured multi-modal data:\nself-reports (background, demographics, psychometrics), operational notes,\nterminal histories, keylogs, network packet captures (PCAP), and NIDS alerts\n(Suricata). Each participant began from a standardized Kali Linux VM and\npursued realistic objectives (e.g., target discovery and data exfiltration)\nunder controlled constraints. Derivative curated logs and labels are included.\nThe combined release supports research on attacker behavior modeling,\nbias-aware analytics, and method benchmarking. Data are available via IEEE\nDataport entries for Experiments 1-3."}
{"id": "2508.20674", "categories": ["cs.AI", "q-bio.NC"], "pdf": "https://arxiv.org/pdf/2508.20674", "abs": "https://arxiv.org/abs/2508.20674", "authors": ["Rui Mao", "Qian Liu", "Xiao Li", "Erik Cambria", "Amir Hussain"], "title": "Bridging Minds and Machines: Toward an Integration of AI and Cognitive Science", "comment": null, "summary": "Cognitive Science has profoundly shaped disciplines such as Artificial\nIntelligence (AI), Philosophy, Psychology, Neuroscience, Linguistics, and\nCulture. Many breakthroughs in AI trace their roots to cognitive theories,\nwhile AI itself has become an indispensable tool for advancing cognitive\nresearch. This reciprocal relationship motivates a comprehensive review of the\nintersections between AI and Cognitive Science. By synthesizing key\ncontributions from both perspectives, we observe that AI progress has largely\nemphasized practical task performance, whereas its cognitive foundations remain\nconceptually fragmented. We argue that the future of AI within Cognitive\nScience lies not only in improving performance but also in constructing systems\nthat deepen our understanding of the human mind. Promising directions include\naligning AI behaviors with cognitive frameworks, situating AI in embodiment and\nculture, developing personalized cognitive models, and rethinking AI ethics\nthrough cognitive co-evaluation."}
{"id": "2508.20411", "categories": ["cs.AI", "cs.CR", "cs.CY"], "pdf": "https://arxiv.org/pdf/2508.20411", "abs": "https://arxiv.org/abs/2508.20411", "authors": ["Donglin Wang", "Weiyun Liang", "Chunyuan Chen", "Jing Xu", "Yulong Fu"], "title": "Governable AI: Provable Safety Under Extreme Threat Models", "comment": null, "summary": "As AI rapidly advances, the security risks posed by AI are becoming\nincreasingly severe, especially in critical scenarios, including those posing\nexistential risks. If AI becomes uncontrollable, manipulated, or actively\nevades safety mechanisms, it could trigger systemic disasters. Existing AI\nsafety approaches-such as model enhancement, value alignment, and human\nintervention-suffer from fundamental, in-principle limitations when facing AI\nwith extreme motivations and unlimited intelligence, and cannot guarantee\nsecurity. To address this challenge, we propose a Governable AI (GAI) framework\nthat shifts from traditional internal constraints to externally enforced\nstructural compliance based on cryptographic mechanisms that are\ncomputationally infeasible to break, even for future AI, under the defined\nthreat model and well-established cryptographic assumptions.The GAI framework\nis composed of a simple yet reliable, fully deterministic, powerful, flexible,\nand general-purpose rule enforcement module (REM); governance rules; and a\ngovernable secure super-platform (GSSP) that offers end-to-end protection\nagainst compromise or subversion by AI. The decoupling of the governance rules\nand the technical platform further enables a feasible and generalizable\ntechnical pathway for the safety governance of AI. REM enforces the bottom line\ndefined by governance rules, while GSSP ensures non-bypassability,\ntamper-resistance, and unforgeability to eliminate all identified attack\nvectors. This paper also presents a rigorous formal proof of the security\nproperties of this mechanism and demonstrates its effectiveness through a\nprototype implementation evaluated in representative high-stakes scenarios."}
{"id": "2508.20701", "categories": ["cs.AI", "cs.CL", "math.CT"], "pdf": "https://arxiv.org/pdf/2508.20701", "abs": "https://arxiv.org/abs/2508.20701", "authors": ["Ares Fabregat-Hernández", "Javier Palanca", "Vicent Botti"], "title": "Transparent Semantic Spaces: A Categorical Approach to Explainable Word Embeddings", "comment": null, "summary": "The paper introduces a novel framework based on category theory to enhance\nthe explainability of artificial intelligence systems, particularly focusing on\nword embeddings. Key topics include the construction of categories\n$\\mathcal{L}_T$ and $\\mathcal{P}_T$, providing schematic representations of the\nsemantics of a text $ T $, and reframing the selection of the element with\nmaximum probability as a categorical notion. Additionally, the monoidal\ncategory $\\mathcal{P}_T$ is constructed to visualize various methods of\nextracting semantic information from $T$, offering a dimension-agnostic\ndefinition of semantic spaces reliant solely on information within the text.\n  Furthermore, the paper defines the categories of configurations Conf and word\nembeddings $\\mathcal{Emb}$, accompanied by the concept of divergence as a\ndecoration on $\\mathcal{Emb}$. It establishes a mathematically precise method\nfor comparing word embeddings, demonstrating the equivalence between the GloVe\nand Word2Vec algorithms and the metric MDS algorithm, transitioning from neural\nnetwork algorithms (black box) to a transparent framework. Finally, the paper\npresents a mathematical approach to computing biases before embedding and\noffers insights on mitigating biases at the semantic space level, advancing the\nfield of explainable artificial intelligence."}
{"id": "2508.20578", "categories": ["cs.AI", "cs.CR"], "pdf": "https://arxiv.org/pdf/2508.20578", "abs": "https://arxiv.org/abs/2508.20578", "authors": ["Jaeman Son", "Hyunsoo Kim"], "title": "Human-AI Collaborative Bot Detection in MMORPGs", "comment": null, "summary": "In Massively Multiplayer Online Role-Playing Games (MMORPGs), auto-leveling\nbots exploit automated programs to level up characters at scale, undermining\ngameplay balance and fairness. Detecting such bots is challenging, not only\nbecause they mimic human behavior, but also because punitive actions require\nexplainable justification to avoid legal and user experience issues. In this\npaper, we present a novel framework for detecting auto-leveling bots by\nleveraging contrastive representation learning and clustering techniques in a\nfully unsupervised manner to identify groups of characters with similar\nlevel-up patterns. To ensure reliable decisions, we incorporate a Large\nLanguage Model (LLM) as an auxiliary reviewer to validate the clustered groups,\neffectively mimicking a secondary human judgment. We also introduce a growth\ncurve-based visualization to assist both the LLM and human moderators in\nassessing leveling behavior. This collaborative approach improves the\nefficiency of bot detection workflows while maintaining explainability, thereby\nsupporting scalable and accountable bot regulation in MMORPGs."}
{"id": "2508.20729", "categories": ["cs.AI", "physics.comp-ph"], "pdf": "https://arxiv.org/pdf/2508.20729", "abs": "https://arxiv.org/abs/2508.20729", "authors": ["Ao Cheng", "Lei Zhang", "Guowei He"], "title": "Re4: Scientific Computing Agent with Rewriting, Resolution, Review and Revision", "comment": null, "summary": "Large language models (LLMs) serve as an active and promising field of\ngenerative artificial intelligence and have demonstrated abilities to perform\ncomplex tasks in multiple domains, including mathematical and scientific\nreasoning. In this work, we construct a novel agent framework for solving\nrepresentative problems in scientific computing. The proposed agent,\nincorporating a \"rewriting-resolution-review-revision\" logical chain via three\nreasoning LLMs (functioning as the Consultant, Reviewer, and Programmer,\nrespectively), is integrated in a collaborative and interactive manner. The\nConsultant module endows the agent with knowledge transfer capabilities to link\nproblems to professional domain insights, thereby rewriting problem\ndescriptions through text augmentation. The Programmer module is responsible\nfor generating and executing well-structured code to deliver the problem\nresolution. The Reviewer module equips the agent with the capacity for\nself-debugging and self-refinement through interactive feedback with code\nruntime outputs. By leveraging the end-to-end review mechanism, the executable\ncode provided by the Programmer attains the iterative revision. A comprehensive\nevaluation is conducted on the performance of the proposed agent framework in\nsolving PDEs, ill-conditioned linear systems, and data-driven physical analysis\nproblems. Compared to single-model, this collaborative framework significantly\nimproves the bug-free code generation rate and reduces the occurrence of\nnon-physical solutions, thereby establishing a highly reliable framework for\nautonomous code generation based on natural language descriptions. The review\nmechanism improved the average execution success (bug-free code and non-NaN\nsolutions) rate of the latest reasoning models. In summary, our agent framework\nestablishes automatic code generation and review as a promising scientific\ncomputing paradigm."}
{"id": "2508.20784", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.20784", "abs": "https://arxiv.org/abs/2508.20784", "authors": ["Yifan Zhang"], "title": "Single Agent Robust Deep Reinforcement Learning for Bus Fleet Control", "comment": null, "summary": "Bus bunching remains a challenge for urban transit due to stochastic traffic\nand passenger demand. Traditional solutions rely on multi-agent reinforcement\nlearning (MARL) in loop-line settings, which overlook realistic operations\ncharacterized by heterogeneous routes, timetables, fluctuating demand, and\nvarying fleet sizes. We propose a novel single-agent reinforcement learning\n(RL) framework for bus holding control that avoids the data imbalance and\nconvergence issues of MARL under near-realistic simulation. A bidirectional\ntimetabled network with dynamic passenger demand is constructed. The key\ninnovation is reformulating the multi-agent problem into a single-agent one by\naugmenting the state space with categorical identifiers (vehicle ID, station\nID, time period) in addition to numerical features (headway, occupancy,\nvelocity). This high-dimensional encoding enables single-agent policies to\ncapture inter-agent dependencies, analogous to projecting non-separable inputs\ninto a higher-dimensional space. We further design a structured reward function\naligned with operational goals: instead of exponential penalties on headway\ndeviations, a ridge-shaped reward balances uniform headways and schedule\nadherence. Experiments show that our modified soft actor-critic (SAC) achieves\nmore stable and superior performance than benchmarks, including MADDPG (e.g.,\n-430k vs. -530k under stochastic conditions). These results demonstrate that\nsingle-agent deep RL, when enhanced with categorical structuring and\nschedule-aware rewards, can effectively manage bus holding in non-loop,\nreal-world contexts. This paradigm offers a robust, scalable alternative to\nMARL frameworks, particularly where agent-specific experiences are imbalanced."}
{"id": "2508.20810", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2508.20810", "abs": "https://arxiv.org/abs/2508.20810", "authors": ["Jessica Lundin", "Guillaume Chabot-Couture"], "title": "A Graph-Based Test-Harness for LLM Evaluation", "comment": "4 pages, 2 figures, dataset", "summary": "We present a first known prototype of a dynamic, systematic benchmark of\nmedical guidelines for 400+ questions, with 3.3+ trillion possible\ncombinations, covering 100\\% of guideline relationships. We transformed the WHO\nIMCI handbook into a directed graph with 200+ nodes (conditions, symptoms,\ntreatments, follow-ups, severities) and 300+ edges, then used graph traversal\nto generate questions that incorporated age-specific scenarios and contextual\ndistractors to ensure clinical relevance. Our graph-based approach enables\nsystematic evaluation across clinical tasks (45-67\\% accuracy), and we find\nmodels excel at symptom recognition but struggle with triaging severity,\ntreatment protocols and follow-up care, demonstrating how customized benchmarks\ncan identify specific capability gaps that general-domain evaluations miss.\nBeyond evaluation, this dynamic MCQA methodology enhances LLM post-training\n(supervised finetuning, GRPO, DPO), where correct answers provide high-reward\nsamples without expensive human annotation. The graph-based approach\nsuccessfully addresses the coverage limitations of manually curated benchmarks.\nThis methodology is a step toward scalable, contamination-resistant solution\nfor creating comprehensive benchmarks that can be dynamically generated,\nincluding when the guidelines are updated. Code and datasets are available at\nhttps://github.com/jessicalundin/graph_testing_harness"}
{"id": "2508.20953", "categories": ["cs.AI", "cs.DM"], "pdf": "https://arxiv.org/pdf/2508.20953", "abs": "https://arxiv.org/abs/2508.20953", "authors": ["Vipul Patel", "Anirudh Deodhar", "Dagnachew Birru"], "title": "A Multi-Objective Genetic Algorithm for Healthcare Workforce Scheduling", "comment": "8 pages, 7 figures, Accepted at the Multi-Objective Decision Making\n  Workshop (MODeM2025) at ECAI 2025", "summary": "Workforce scheduling in the healthcare sector is a significant operational\nchallenge, characterized by fluctuating patient loads, diverse clinical skills,\nand the critical need to control labor costs while upholding high standards of\npatient care. This problem is inherently multi-objective, demanding a delicate\nbalance between competing goals: minimizing payroll, ensuring adequate staffing\nfor patient needs, and accommodating staff preferences to mitigate burnout. We\npropose a Multi-objective Genetic Algorithm (MOO-GA) that models the hospital\nunit workforce scheduling problem as a multi-objective optimization task. Our\nmodel incorporates real-world complexities, including hourly appointment-driven\ndemand and the use of modular shifts for a multi-skilled workforce. By defining\nobjective functions for cost, patient care coverage, and staff satisfaction,\nthe GA navigates the vast search space to identify a set of high-quality,\nnon-dominated solutions. Demonstrated on datasets representing a typical\nhospital unit, the results show that our MOO-GA generates robust and balanced\nschedules. On average, the schedules produced by our algorithm showed a 66\\%\nperformance improvement over a baseline that simulates a conventional, manual\nscheduling process. This approach effectively manages trade-offs between\ncritical operational and staff-centric objectives, providing a practical\ndecision support tool for nurse managers and hospital administrators."}
{"id": "2508.20978", "categories": ["cs.AI", "cs.LO", "cs.SC"], "pdf": "https://arxiv.org/pdf/2508.20978", "abs": "https://arxiv.org/abs/2508.20978", "authors": ["Marianne Defresne", "Romain Gambardella", "Sophie Barbe", "Thomas Schiex"], "title": "Efficient Neuro-Symbolic Learning of Constraints and Objective", "comment": null, "summary": "In the ongoing quest for hybridizing discrete reasoning with neural nets,\nthere is an increasing interest in neural architectures that can learn how to\nsolve discrete reasoning or optimization problems from natural inputs, a task\nthat Large Language Models seem to struggle with.\n  Objectives: We introduce a differentiable neuro-symbolic architecture and a\nloss function dedicated to learning how to solve NP-hard reasoning problems.\n  Methods: Our new probabilistic loss allows for learning both the constraints\nand the objective, thus delivering a complete model that can be scrutinized and\ncompleted with side constraints. By pushing the combinatorial solver out of the\ntraining loop, our architecture also offers scalable training while exact\ninference gives access to maximum accuracy.\n  Results: We empirically show that it can efficiently learn how to solve\nNP-hard reasoning problems from natural inputs. On three variants of the Sudoku\nbenchmark -- symbolic, visual, and many-solution --, our approach requires a\nfraction of training time of other hybrid methods. On a visual Min-Cut/Max-cut\ntask, it optimizes the regret better than a Decision-Focused-Learning\nregret-dedicated loss. Finally, it efficiently learns the energy optimization\nformulation of the large real-world problem of designing proteins."}
{"id": "2508.20996", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.20996", "abs": "https://arxiv.org/abs/2508.20996", "authors": ["Junda Wang", "Zonghai Yao", "Zhichao Yang", "Lingxi Li", "Junhui Qian", "Hong Yu"], "title": "ChatThero: An LLM-Supported Chatbot for Behavior Change and Therapeutic Support in Addiction Recovery", "comment": null, "summary": "Substance use disorders (SUDs) affect over 36 million people worldwide, yet\nfew receive effective care due to stigma, motivational barriers, and limited\npersonalized support. Although large language models (LLMs) show promise for\nmental-health assistance, most systems lack tight integration with clinically\nvalidated strategies, reducing effectiveness in addiction recovery. We present\nChatThero, a multi-agent conversational framework that couples dynamic patient\nmodeling with context-sensitive therapeutic dialogue and adaptive persuasive\nstrategies grounded in cognitive behavioral therapy (CBT) and motivational\ninterviewing (MI). We build a high-fidelity synthetic benchmark spanning Easy,\nMedium, and Hard resistance levels, and train ChatThero with a two-stage\npipeline comprising supervised fine-tuning (SFT) followed by direct preference\noptimization (DPO). In evaluation, ChatThero yields a 41.5\\% average gain in\npatient motivation, a 0.49\\% increase in treatment confidence, and resolves\nhard cases with 26\\% fewer turns than GPT-4o, and both automated and human\nclinical assessments rate it higher in empathy, responsiveness, and behavioral\nrealism. The framework supports rigorous, privacy-preserving study of\ntherapeutic conversation and provides a robust, replicable basis for research\nand clinical translation."}
{"id": "2508.20124", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.20124", "abs": "https://arxiv.org/abs/2508.20124", "authors": ["Yunlong Feng", "Yang Xu", "Xiao Xu", "Binyuan Hui", "Junyang Lin"], "title": "Towards Better Correctness and Efficiency in Code Generation", "comment": null, "summary": "While code large language models have demonstrated remarkable progress in\ncode generation, the generated code often exhibits poor runtime efficiency,\nlimiting its practical application in performance-sensitive scenarios. To\naddress this limitation, we propose an efficiency-oriented reinforcement\nlearning framework guided by a novel performance reward. Based on this\nframework, we take a deeper dive into the code efficiency problem, identifying\nthen proposing methods to overcome key bottlenecks: (1) Dynamic exploration\novercomes the static data constraints of offline fine-tuning, enabling the\ndiscovery of more efficient code implementations. (2) The error-insensitive\nreinforcement learning method and high-contrast efficiency signals are crucial\nfor mitigating systematic errors and achieving effective optimization. (3)\nOnline exploration is most effective when starting from a high-correctness\nbaseline, as this allows for efficiency improvements without sacrificing\naccuracy. With these discoveries, we finally propose a two-stage tuning method,\nwhich achieves high and balanced performance across correctness and efficiency.\nThe results of experiments show the effectiveness of the method, which improves\ncode correctness by 10.18\\% and runtime efficiency by 7.75\\% on a 7B model,\nachieving performance comparable to much larger model."}
{"id": "2508.20186", "categories": ["cs.CR", "cs.AI", "cs.CY"], "pdf": "https://arxiv.org/pdf/2508.20186", "abs": "https://arxiv.org/abs/2508.20186", "authors": ["Lukasz Olejnik"], "title": "AI Propaganda factories with language models", "comment": null, "summary": "AI-powered influence operations can now be executed end-to-end on commodity\nhardware. We show that small language models produce coherent, persona-driven\npolitical messaging and can be evaluated automatically without human raters.\nTwo behavioural findings emerge. First, persona-over-model: persona design\nexplains behaviour more than model identity. Second, engagement as a stressor:\nwhen replies must counter-arguments, ideological adherence strengthens and the\nprevalence of extreme content increases. We demonstrate that fully automated\ninfluence-content production is within reach of both large and small actors.\nConsequently, defence should shift from restricting model access towards\nconversation-centric detection and disruption of campaigns and coordination\ninfrastructure. Paradoxically, the very consistency that enables these\noperations also provides a detection signature."}
{"id": "2508.20282", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.20282", "abs": "https://arxiv.org/abs/2508.20282", "authors": ["Hyejun Jeong", "Mohammadreze Teymoorianfard", "Abhinav Kumar", "Amir Houmansadr", "Eugene Badasarian"], "title": "Network-Level Prompt and Trait Leakage in Local Research Agents", "comment": "under review", "summary": "We show that Web and Research Agents (WRAs) -- language model-based systems\nthat investigate complex topics on the Internet -- are vulnerable to inference\nattacks by passive network adversaries such as ISPs. These agents could be\ndeployed \\emph{locally} by organizations and individuals for privacy, legal, or\nfinancial purposes. Unlike sporadic web browsing by humans, WRAs visit\n$70{-}140$ domains with distinguishable timing correlations, enabling unique\nfingerprinting attacks.\n  Specifically, we demonstrate a novel prompt and user trait leakage attack\nagainst WRAs that only leverages their network-level metadata (i.e., visited IP\naddresses and their timings). We start by building a new dataset of WRA traces\nbased on user search queries and queries generated by synthetic personas. We\ndefine a behavioral metric (called OBELS) to comprehensively assess similarity\nbetween original and inferred prompts, showing that our attack recovers over\n73\\% of the functional and domain knowledge of user prompts. Extending to a\nmulti-session setting, we recover up to 19 of 32 latent traits with high\naccuracy. Our attack remains effective under partial observability and noisy\nconditions. Finally, we discuss mitigation strategies that constrain domain\ndiversity or obfuscate traces, showing negligible utility impact while reducing\nattack effectiveness by an average of 29\\%."}
{"id": "2508.20307", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.20307", "abs": "https://arxiv.org/abs/2508.20307", "authors": ["Michael R Smith", "Joe Ingram"], "title": "Surveying the Operational Cybersecurity and Supply Chain Threat Landscape when Developing and Deploying AI Systems", "comment": "11 pages, 5 figures", "summary": "The rise of AI has transformed the software and hardware landscape, enabling\npowerful capabilities through specialized infrastructures, large-scale data\nstorage, and advanced hardware. However, these innovations introduce unique\nattack surfaces and objectives which traditional cybersecurity assessments\noften overlook. Cyber attackers are shifting their objectives from conventional\ngoals like privilege escalation and network pivoting to manipulating AI outputs\nto achieve desired system effects, such as slowing system performance, flooding\noutputs with false positives, or degrading model accuracy. This paper serves to\nraise awareness of the novel cyber threats that are introduced when\nincorporating AI into a software system. We explore the operational\ncybersecurity and supply chain risks across the AI lifecycle, emphasizing the\nneed for tailored security frameworks to address evolving threats in the\nAI-driven landscape. We highlight previous exploitations and provide insights\nfrom working in this area. By understanding these risks, organizations can\nbetter protect AI systems and ensure their reliability and resilience."}
{"id": "2508.20340", "categories": ["cs.SE", "cs.AI", "cs.PL"], "pdf": "https://arxiv.org/pdf/2508.20340", "abs": "https://arxiv.org/abs/2508.20340", "authors": ["Maolin Sun", "Yibiao Yang", "Yuming Zhou"], "title": "Boosting Skeleton-Driven SMT Solver Fuzzing by Leveraging LLM to Produce Formula Generators", "comment": null, "summary": "Satisfiability Modulo Theory (SMT) solvers are foundational to modern systems\nand programming languages research, providing the foundation for tasks like\nsymbolic execution and automated verification. Because these solvers sit on the\ncritical path, their correctness is essential, and high-quality test formulas\nare key to uncovering bugs. However, while prior testing techniques performed\nwell on earlier solver versions, they struggle to keep pace with rapidly\nevolving features. Recent approaches based on Large Language Models (LLMs) show\npromise in exploring advanced solver capabilities, but two obstacles remain:\nnearly half of the generated formulas are syntactically invalid, and iterative\ninteractions with the LLMs introduce substantial computational overhead. In\nthis study, we present Chimera, a novel LLM-assisted fuzzing framework that\naddresses both issues by shifting from direct formula generation to the\nsynthesis of reusable term (i.e., logical expression) generators. Particularly,\nChimera uses LLMs to (1) automatically extract context-free grammars (CFGs) for\nSMT theories, including solver-specific extensions, from documentation, and (2)\nsynthesize composable Boolean term generators that adhere to these grammars.\nDuring fuzzing, Chimera populates structural skeletons derived from existing\nformulas with the terms iteratively produced by the LLM-synthesized generators.\nThis design ensures syntactic validity while promoting semantic diversity.\nNotably, Chimera requires only one-time LLM interaction investment,\ndramatically reducing runtime cost. We evaluated Chimera on two leading SMT\nsolvers: Z3 and cvc5. Our experiments show that Chimera has identified 43\nconfirmed bugs, 40 of which have already been fixed by developers."}
{"id": "2508.20370", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.20370", "abs": "https://arxiv.org/abs/2508.20370", "authors": ["Lingzhe Zhang", "Tong Jia", "Kangjin Wang", "Weijie Hong", "Chiming Duan", "Minghua He", "Ying Li"], "title": "Adaptive Root Cause Localization for Microservice Systems with Multi-Agent Recursion-of-Thought", "comment": null, "summary": "As contemporary microservice systems become increasingly popular and\ncomplex-often comprising hundreds or even thousands of fine-grained,\ninterdependent subsystems-they are facing more frequent failures. Ensuring\nsystem reliability thus demands accurate root cause localization. While traces\nand metrics have proven to be effective data sources for this task, existing\nmethods either heavily rely on pre-defined schemas, which struggle to adapt to\nevolving operational contexts, or lack interpretability in their reasoning\nprocess, thereby leaving Site Reliability Engineers (SREs) confused. In this\npaper, we conduct a comprehensive study on how SREs localize the root cause of\nfailures, drawing insights from multiple professional SREs across different\norganizations. Our investigation reveals that human root cause analysis\nexhibits three key characteristics: recursiveness, multi-dimensional expansion,\nand cross-modal reasoning. Motivated by these findings, we introduce RCLAgent,\nan adaptive root cause localization method for microservice systems that\nleverages a multi-agent recursion-of-thought framework. RCLAgent employs a\nnovel recursion-of-thought strategy to guide the LLM's reasoning process,\neffectively integrating data from multiple agents and tool-assisted analysis to\naccurately pinpoint the root cause. Experimental evaluations on various public\ndatasets demonstrate that RCLAgent achieves superior performance by localizing\nthe root cause using only a single request-outperforming state-of-the-art\nmethods that depend on aggregating multiple requests. These results underscore\nthe effectiveness of RCLAgent in enhancing the efficiency and precision of root\ncause localization in complex microservice environments."}
{"id": "2508.20517", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.20517", "abs": "https://arxiv.org/abs/2508.20517", "authors": ["Dan Lin", "Shunfeng Lu", "Ziyan Liu", "Jiajing Wu", "Junyuan Fang", "Kaixin Lin", "Bowen Song", "Zibin Zheng"], "title": "BridgeShield: Enhancing Security for Cross-chain Bridge Applications via Heterogeneous Graph Mining", "comment": null, "summary": "Cross-chain bridges play a vital role in enabling blockchain\ninteroperability. However, due to the inherent design flaws and the enormous\nvalue they hold, they have become prime targets for hacker attacks. Existing\ndetection methods show progress yet remain limited, as they mainly address\nsingle-chain behaviors and fail to capture cross-chain semantics. To address\nthis gap, we leverage heterogeneous graph attention networks, which are\nwell-suited for modeling multi-typed entities and relations, to capture the\ncomplex execution semantics of cross-chain behaviors. We propose BridgeShield,\na detection framework that jointly models the source chain, off-chain\ncoordination, and destination chain within a unified heterogeneous graph\nrepresentation. BridgeShield incorporates intra-meta-path attention to learn\nfine-grained dependencies within cross-chain paths and inter-meta-path\nattention to highlight discriminative cross-chain patterns, thereby enabling\nprecise identification of attack behaviors. Extensive experiments on 51\nreal-world cross-chain attack events demonstrate that BridgeShield achieves an\naverage F1-score of 92.58%, representing a 24.39% improvement over\nstate-of-the-art baselines. These results validate the effectiveness of\nBridgeShield as a practical solution for securing cross-chain bridges and\nenhancing the resilience of multi-chain ecosystems."}
{"id": "2508.20563", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.20563", "abs": "https://arxiv.org/abs/2508.20563", "authors": ["Zheying Zhang", "Tomas Herda", "Victoria Pichler", "Pekka Abrahamsson", "Geir K. Hanssen", "Joshua Kerievsky", "Alex Polyakov", "Mohit Chandna", "Marius Irgens", "Kai-Kristian Kemell", "Ayman Asad Khan", "Crystal Kwok", "Evan Leybourn", "Munish Malik", "Dorota Mleczko", "Morteza Moalagh", "Christopher Morales", "Yuliia Pieskova", "Daniel Planötscher", "Mika Saari", "Anastasiia Tkalich", "Karl Josef Gstettner", "Xiaofeng Wang"], "title": "AI and Agile Software Development: A Research Roadmap from the XP2025 Workshop", "comment": null, "summary": "This paper synthesizes the key findings from a full-day XP2025 workshop on\n\"AI and Agile: From Frustration to Success\", held in Brugg-Windisch,\nSwitzerland. The workshop brought together over 30 interdisciplinary academic\nresearchers and industry practitioners to tackle the concrete challenges and\nemerging opportunities at the intersection of Generative Artificial\nIntelligence (GenAI) and agile software development. Through structured,\ninteractive breakout sessions, participants identified shared pain points like\ntool fragmentation, governance, data quality, and critical skills gaps in AI\nliteracy and prompt engineering. These issues were further analyzed, revealing\nunderlying causes and cross-cutting concerns. The workshop concluded by\ncollaboratively co-creating a multi-thematic research roadmap, articulating\nboth short-term, implementable actions and visionary, long-term research\ndirections. This cohesive agenda aims to guide future investigation and drive\nthe responsible, human-centered integration of GenAI into agile practices."}
{"id": "2508.20737", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.20737", "abs": "https://arxiv.org/abs/2508.20737", "authors": ["Wei Ma", "Yixiao Yang", "Qiang Hu", "Shi Ying", "Zhi Jin", "Bo Du", "Zhenchang Xing", "Tianlin Li", "Junjie Shi", "Yang Liu", "Linxiao Jiang"], "title": "Rethinking Testing for LLM Applications: Characteristics, Challenges, and a Lightweight Interaction Protocol", "comment": null, "summary": "Applications of Large Language Models~(LLMs) have evolved from simple text\ngenerators into complex software systems that integrate retrieval augmentation,\ntool invocation, and multi-turn interactions. Their inherent non-determinism,\ndynamism, and context dependence pose fundamental challenges for quality\nassurance. This paper decomposes LLM applications into a three-layer\narchitecture: \\textbf{\\textit{System Shell Layer}}, \\textbf{\\textit{Prompt\nOrchestration Layer}}, and \\textbf{\\textit{LLM Inference Core}}. We then assess\nthe applicability of traditional software testing methods in each layer:\ndirectly applicable at the shell layer, requiring semantic reinterpretation at\nthe orchestration layer, and necessitating paradigm shifts at the inference\ncore. A comparative analysis of Testing AI methods from the software\nengineering community and safety analysis techniques from the AI community\nreveals structural disconnects in testing unit abstraction, evaluation metrics,\nand lifecycle management. We identify four fundamental differences that\nunderlie 6 core challenges. To address these, we propose four types of\ncollaborative strategies (\\emph{Retain}, \\emph{Translate}, \\emph{Integrate},\nand \\emph{Runtime}) and explore a closed-loop, trustworthy quality assurance\nframework that combines pre-deployment validation with runtime monitoring.\nBased on these strategies, we offer practical guidance and a protocol proposal\nto support the standardization and tooling of LLM application testing. We\npropose a protocol \\textbf{\\textit{Agent Interaction Communication Language}}\n(AICL) that is used to communicate between AI agents. AICL has the\ntest-oriented features and is easily integrated in the current agent framework."}
{"id": "2508.20816", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.20816", "abs": "https://arxiv.org/abs/2508.20816", "authors": ["Isaac David", "Arthur Gervais"], "title": "Multi-Agent Penetration Testing AI for the Web", "comment": null, "summary": "AI-powered development platforms are making software creation accessible to a\nbroader audience, but this democratization has triggered a scalability crisis\nin security auditing. With studies showing that up to 40% of AI-generated code\ncontains vulnerabilities, the pace of development now vastly outstrips the\ncapacity for thorough security assessment.\n  We present MAPTA, a multi-agent system for autonomous web application\nsecurity assessment that combines large language model orchestration with\ntool-grounded execution and end-to-end exploit validation. On the 104-challenge\nXBOW benchmark, MAPTA achieves 76.9% overall success with perfect performance\non SSRF and misconfiguration vulnerabilities, 83% success on broken\nauthorization, and strong results on injection attacks including server-side\ntemplate injection (85%) and SQL injection (83%). Cross-site scripting (57%)\nand blind SQL injection (0%) remain challenging. Our comprehensive cost\nanalysis across all challenges totals $21.38 with a median cost of $0.073 for\nsuccessful attempts versus $0.357 for failures. Success correlates strongly\nwith resource efficiency, enabling practical early-stopping thresholds at\napproximately 40 tool calls or $0.30 per challenge.\n  MAPTA's real-world findings are impactful given both the popularity of the\nrespective scanned GitHub repositories (8K-70K stars) and MAPTA's low average\noperating cost of $3.67 per open-source assessment: MAPTA discovered critical\nvulnerabilities including RCEs, command injections, secret exposure, and\narbitrary file write vulnerabilities. Findings are responsibly disclosed, 10\nfindings are under CVE review."}
{"id": "2508.20848", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.20848", "abs": "https://arxiv.org/abs/2508.20848", "authors": ["Junjie Chu", "Mingjie Li", "Ziqing Yang", "Ye Leng", "Chenhao Lin", "Chao Shen", "Michael Backes", "Yun Shen", "Yang Zhang"], "title": "JADES: A Universal Framework for Jailbreak Assessment via Decompositional Scoring", "comment": "17 pages, 5 figures. For the code and data supporting this work, see\n  https://trustairlab.github.io/jades.github.io/", "summary": "Accurately determining whether a jailbreak attempt has succeeded is a\nfundamental yet unresolved challenge. Existing evaluation methods rely on\nmisaligned proxy indicators or naive holistic judgments. They frequently\nmisinterpret model responses, leading to inconsistent and subjective\nassessments that misalign with human perception. To address this gap, we\nintroduce JADES (Jailbreak Assessment via Decompositional Scoring), a universal\njailbreak evaluation framework. Its key mechanism is to automatically decompose\nan input harmful question into a set of weighted sub-questions, score each\nsub-answer, and weight-aggregate the sub-scores into a final decision. JADES\nalso incorporates an optional fact-checking module to strengthen the detection\nof hallucinations in jailbreak responses. We validate JADES on JailbreakQR, a\nnewly introduced benchmark proposed in this work, consisting of 400 pairs of\njailbreak prompts and responses, each meticulously annotated by humans. In a\nbinary setting (success/failure), JADES achieves 98.5% agreement with human\nevaluators, outperforming strong baselines by over 9%. Re-evaluating five\npopular attacks on four LLMs reveals substantial overestimation (e.g., LAA's\nattack success rate on GPT-3.5-Turbo drops from 93% to 69%). Our results show\nthat JADES could deliver accurate, consistent, and interpretable evaluations,\nproviding a reliable basis for measuring future jailbreak attacks."}
{"id": "2508.20866", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.20866", "abs": "https://arxiv.org/abs/2508.20866", "authors": ["Amine Lbath", "Massih-Reza Amini", "Aurelien Delaitre", "Vadim Okun"], "title": "AI Agentic Vulnerability Injection And Transformation with Optimized Reasoning", "comment": null, "summary": "The increasing complexity of software systems and the sophistication of\ncyber-attacks have underscored the critical need for effective automated\nvulnerability detection and repair systems. Traditional methods, such as static\nprogram analysis, face significant challenges related to scalability,\nadaptability, and high false-positive and false-negative rates. AI-driven\napproaches, particularly those using machine learning and deep learning models,\nshow promise but are heavily reliant on the quality and quantity of training\ndata. This paper introduces a novel framework designed to automatically\nintroduce realistic, category-specific vulnerabilities into secure C/C++\ncodebases to generate datasets. The proposed approach coordinates multiple AI\nagents that simulate expert reasoning, along with function agents and\ntraditional code analysis tools. It leverages Retrieval-Augmented Generation\nfor contextual grounding and employs Low-Rank approximation of weights for\nefficient model fine-tuning. Our experimental study on 116 code samples from\nthree different benchmarks suggests that our approach outperforms other\ntechniques with regard to dataset accuracy, achieving between 89\\% and 95\\%\nsuccess rates in injecting vulnerabilities at function level."}
