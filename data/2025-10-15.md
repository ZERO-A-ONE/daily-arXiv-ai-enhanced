<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 18]
- [cs.CR](#cs.CR) [Total: 25]
- [cs.AI](#cs.AI) [Total: 46]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [eye2vec: Learning Distributed Representations of Eye Movement for Program Comprehension Analysis](https://arxiv.org/abs/2510.11722)
*Haruhiko Yoshioka,Kazumasa Shimari,Hidetake Uwano,Kenichi Matsumoto*

Main category: cs.SE

TL;DR: eye2vec是一个分析软件开发者在阅读源代码时眼动轨迹的基础设施，使用分布式表示将连续注视点表示为语法元素间的转换，简化眼动分析过程。


<details>
  <summary>Details</summary>
Motivation: 传统眼动研究中，研究人员需要预先选择分析目标并开发分析方法，这依赖于耗时的手动工作，且不同AOI定义会导致不同结果。

Method: 使用分布式表示将连续两个注视点表示为语法元素之间的转换，便于采用多样化的数据分析方法。

Result: 该方法能够简化眼动分析过程，提供丰富的语义解释。

Conclusion: eye2vec通过分布式表示技术为程序理解中的眼动研究提供了更高效和灵活的分析框架。

Abstract: This paper presents eye2vec, an infrastructure for analyzing software
developers' eye movements while reading source code. In common eye-tracking
studies in program comprehension, researchers must preselect analysis targets
such as control flow or syntactic elements, and then develop analysis methods
to extract appropriate metrics from the fixation for source code. Here,
researchers can define various levels of AOIs like words, lines, or code
blocks, and the difference leads to different results. Moreover, the
interpretation of fixation for word/line can vary across the purposes of the
analyses. Hence, the eye-tracking analysis is a difficult task that depends on
the time-consuming manual work of the researchers. eye2vec represents
continuous two fixations as transitions between syntactic elements using
distributed representations. The distributed representation facilitates the
adoption of diverse data analysis methods with rich semantic interpretations.

</details>


### [2] [Task-Aware Reduction for Scalable LLM-Database Systems](https://arxiv.org/abs/2510.11813)
*Marcus Emmanuel Barnes,Taher A. Ghaleb,Safwat Hassan*

Main category: cs.SE

TL;DR: 本文主张将LLM的token预算视为注意力预算，将任务感知的文本缩减作为语言-数据系统的核心设计原则，以解决数据密集型工作流中文本数据量大、冗长和嘈杂的问题。


<details>
  <summary>Details</summary>
Motivation: LLM在数据密集型工作流中的应用受到真实世界文本数据（如日志、遥测数据）量大、冗长和嘈杂的限制，直接输入这些数据到LLM成本高、环境不可持续且与任务目标不一致。

Method: 将输入侧缩减视为注意力分配而非压缩，优先处理与下游任务最相关的信息，并提出了构建基准、设计自适应缩减管道、集成token预算感知预处理的研究挑战。

Result: 提出了一个新的设计范式，将文本缩减作为语言-数据系统的核心原则，以更有效地利用LLM的注意力资源。

Conclusion: 通过将稀缺的注意力资源引导到嘈杂数据流中的有意义信号，可以实现可扩展、准确和可持续的LLM-数据集成。

Abstract: Large Language Models (LLMs) are increasingly applied to data-intensive
workflows, from database querying to developer observability. Yet the
effectiveness of these systems is constrained by the volume, verbosity, and
noise of real-world text-rich data such as logs, telemetry, and monitoring
streams. Feeding such data directly into LLMs is costly, environmentally
unsustainable, and often misaligned with task objectives. Parallel efforts in
LLM efficiency have focused on model- or architecture-level optimizations, but
the challenge of reducing upstream input verbosity remains underexplored. In
this paper, we argue for treating the token budget of an LLM as an attention
budget and elevating task-aware text reduction as a first-class design
principle for language -- data systems. We position input-side reduction not as
compression, but as attention allocation: prioritizing information most
relevant to downstream tasks. We outline open research challenges for building
benchmarks, designing adaptive reduction pipelines, and integrating
token-budget--aware preprocessing into database and retrieval systems. Our
vision is to channel scarce attention resources toward meaningful signals in
noisy, data-intensive workflows, enabling scalable, accurate, and sustainable
LLM--data integration.

</details>


### [3] [Lingxi: Repository-Level Issue Resolution Framework Enhanced by Procedural Knowledge Guided Scaling](https://arxiv.org/abs/2510.11838)
*Xu Yang,Jiayuan Zhou,Michael Pacheco,Wenhan Zhu,Pengfei He,Shaowei Wang,Kui Liu,Ruiqi Pan*

Main category: cs.SE

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Driven by the advancements of Large Language Models (LLMs), LLM-powered
agents are making significant improvements in software engineering tasks, yet
struggle with complex, repository-level issue resolution. Existing agent-based
methods have two key limitations. First, they lack of procedural knowledge
(i.e., how an issue is fixed step-by-step and rationales behind it) to learn
and leverage for issue resolution. Second, they rely on massive computational
power to blindly explore the solution space. % To address those limitations, we
propose Lingxi, an issue resolution framework that leverages procedural
knowledge extracted from historical issue-fixing data to guide agents in
solving repository-level issues. \ourTool first constructs this knowledge
offline through a hierarchical abstraction mechanism, enabling agents to learn
the how and why behind a fix, not just the final solution. During online
application, it employs a knowledge-driven scaling method that leverages the
procedural knowledge of similar issues to intelligently analyze the target
issue from multiple perspectives, in sharp contrast to undirected, brute-force
exploration. % Lingxi successfully resolves 74.6\% of bugs on the SWE-bench
Verified benchmark in Past@1 setting, outperforming five state-of-the-art
techniques by a significant margin (5.4\% to 14.9\%). Our comprehensive
ablation study confirmed that the success of Lingxi comes directly from its use
of procedural knowledge. Without it, the performance gains from scaling alone
is negligible. Our qualitative study further shows that the ``design patterns
$\&$ coding practices'' is the most critical knowledge aspect, and that the
roles of different knowledge aspects switch across different stages (i.e.,
analysis, planning, and fixing).

</details>


### [4] [DMAS-Forge: A Framework for Transparent Deployment of AI Applications as Distributed Systems](https://arxiv.org/abs/2510.11872)
*Alessandro Cornacchia,Vaastav Anand,Muhammad Bilal,Zafar Qazi,Marco Canini*

Main category: cs.SE

TL;DR: DMAS-Forge是一个框架，旨在简化分布式多智能体应用的部署和测试，通过解耦应用逻辑与具体部署选择，自动生成必要的连接代码和配置。


<details>
  <summary>Details</summary>
Motivation: 随着AI应用越来越多地依赖多个具有不同角色、专用工具和内存访问权限的智能体来解决复杂任务，部署和测试这些分布式系统变得困难且劳动密集。

Method: DMAS-Forge框架将应用逻辑与具体部署选择解耦，透明地生成必要的连接代码和配置，以最小的手动工作在各种部署场景中启动分布式多智能体应用。

Result: 提出了DMAS-Forge的愿景、设计原则和原型实现，展示了该框架如何简化分布式多智能体系统的部署过程。

Conclusion: DMAS-Forge为分布式AI智能体系统的部署提供了有效解决方案，并讨论了该方法的机遇和未来工作方向。

Abstract: Agentic AI applications increasingly rely on multiple agents with distinct
roles, specialized tools, and access to memory layers to solve complex tasks --
closely resembling service-oriented architectures. Yet, in the rapid evolving
landscape of programming frameworks and new protocols, deploying and testing AI
agents as distributed systems remains a daunting and labor-intensive task. We
present DMAS-Forge, a framework designed to close this gap. DMAS-Forge
decouples application logic from specific deployment choices, and aims at
transparently generating the necessary glue code and configurations to spawn
distributed multi-agent applications across diverse deployment scenarios with
minimal manual effort. We present our vision, design principles, and a
prototype of DMAS-Forge. Finally, we discuss the opportunities and future work
for our approach.

</details>


### [5] [TorchCor: High-Performance Cardiac Electrophysiology Simulations with the Finite Element Method on GPUs](https://arxiv.org/abs/2510.12011)
*Bei Zhou,Maximilian Balmus,Cesare Corrado,Ludovica Cicci,Shuang Qian,Steven A. Niederer*

Main category: cs.SE

TL;DR: TorchCor是一个基于PyTorch的高性能Python库，用于在通用GPU上进行心脏电生理模拟，显著加速大型3D网格的模拟计算。


<details>
  <summary>Details</summary>
Motivation: 心脏电生理模拟通常需要高性能计算资源，但许多研究团队和临床医生无法获得这些资源。

Method: 基于PyTorch构建，使用有限元方法在通用GPU上进行心脏电生理模拟。

Result: TorchCor显著加速了CEP模拟，特别是对于大型3D网格。求解器的准确性通过制造解析解和N版本基准问题进行了验证。

Conclusion: TorchCor是一个免费且无使用限制的高性能心脏电生理模拟库，适用于学术和商业用途。

Abstract: Cardiac electrophysiology (CEP) simulations are increasingly used for
understanding cardiac arrhythmias and guiding clinical decisions. However,
these simulations typically require high-performance computing resources with
numerous CPU cores, which are often inaccessible to many research groups and
clinicians. To address this, we present TorchCor, a high-performance Python
library for CEP simulations using the finite element method on general-purpose
GPUs. Built on PyTorch, TorchCor significantly accelerates CEP simulations,
particularly for large 3D meshes. The accuracy of the solver is verified
against manufactured analytical solutions and the $N$-version benchmark
problem. TorchCor is freely available for both academic and commercial use
without restrictions.

</details>


### [6] [Enhancing Neural Code Representation with Additional Context](https://arxiv.org/abs/2510.12082)
*Huy Nguyen,Christoph Treude,Patanamon Thongtanunam*

Main category: cs.SE

TL;DR: 本文通过实证研究发现，在代码表示中融入上下文信息（如版本历史和调用图）能显著提升深度学习模型在代码克隆检测和代码摘要任务上的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的深度学习模型主要依赖源代码本身，忽略了版本历史和结构关系等上下文信息，这限制了模型理解代码演化和运行方式的能力。

Method: 在两个数据集（SeSaMe和CodeSearchNet）上评估了五个代表性模型（CodeBERT、GraphCodeBERT、CodeT5、PLBART、ASTNN），比较了仅使用代码和使用上下文增强设置下的性能。

Result: 上下文信息普遍提升模型性能：版本历史持续提升克隆检测（如CodeT5 F1提升15.92%）和代码摘要（如GraphCodeBERT METEOR提升5.56%），调用图效果因模型和任务而异。组合多种上下文可获得更大增益（最高提升21.48% macro-F1）。

Conclusion: 上下文信号有潜力增强代码理解能力，并为优化神经软件工程模型中的上下文编码开辟了新方向。

Abstract: Automated program comprehension underpins many software engineering tasks,
from code summarisation to clone detection. Recent deep learning models achieve
strong results but typically rely on source code alone, overlooking contextual
information such as version history or structural relationships. This limits
their ability to capture how code evolves and operates. We conduct an empirical
study on how enriching code representations with such contextual signals
affects neural model performance on key comprehension tasks. Two downstream
tasks, code clone detection and code summarisation, are evaluated using SeSaMe
(1,679 Java methods) and CodeSearchNet (63,259 methods). Five representative
models (CodeBERT, GraphCodeBERT, CodeT5, PLBART, ASTNN) are fine-tuned under
code-only and context-augmented settings. Results show that context generally
improves performance: version history consistently boosts clone detection
(e.g., CodeT5 +15.92% F1) and summarisation (e.g., GraphCodeBERT +5.56%
METEOR), while call-graph effects vary by model and task. Combining multiple
contexts yields further gains (up to +21.48% macro-F1). Human evaluation on 100
Java snippets confirms that context-augmented summaries are significantly
preferred for Accuracy and Content Adequacy (p <= 0.026; |delta| up to 0.55).
These findings highlight the potential of contextual signals to enhance code
comprehension and open new directions for optimising contextual encoding in
neural SE models.

</details>


### [7] [Towards Engineering Multi-Agent LLMs: A Protocol-Driven Approach](https://arxiv.org/abs/2510.12120)
*Zhenyu Mao,Jacky Keung,Fengji Zhang,Shuo Liu,Yifei Wang,Jialong Li*

Main category: cs.SE

TL;DR: 本文提出SEMAP协议，通过引入软件工程结构化原则来解决多智能体LLM系统中的核心缺陷，显著减少了软件开发任务中的失败率。


<details>
  <summary>Details</summary>
Motivation: 现有的多智能体LLM系统在模拟软件开发工作流程时存在三个核心缺陷：规范不足、协调错位和验证不当，这些缺陷源于缺乏基础的软件工程结构化原则。

Method: 引入SEMAP协议层方法，基于Google的A2A基础设施实现三个核心SE设计原则：明确行为契约建模、结构化消息传递、以及生命周期引导的执行与验证。

Result: 使用MAST框架评估显示，SEMAP在不同SE任务中有效减少失败：代码开发中函数级开发失败减少69.6%，部署级开发减少56.7%；漏洞检测中Python任务失败减少47.4%，C/C++任务减少28.2%。

Conclusion: SEMAP协议通过应用软件工程结构化原则，显著提升了多智能体LLM系统在软件开发任务中的性能和可靠性。

Abstract: The increasing demand for software development has driven interest in
automating software engineering (SE) tasks using Large Language Models (LLMs).
Recent efforts extend LLMs into multi-agent systems (MAS) that emulate
collaborative development workflows, but these systems often fail due to three
core deficiencies: under-specification, coordination misalignment, and
inappropriate verification, arising from the absence of foundational SE
structuring principles. This paper introduces Software Engineering Multi-Agent
Protocol (SEMAP), a protocol-layer methodology that instantiates three core SE
design principles for multi-agent LLMs: (1) explicit behavioral contract
modeling, (2) structured messaging, and (3) lifecycle-guided execution with
verification, and is implemented atop Google's Agent-to-Agent (A2A)
infrastructure. Empirical evaluation using the Multi-Agent System Failure
Taxonomy (MAST) framework demonstrates that SEMAP effectively reduces failures
across different SE tasks. In code development, it achieves up to a 69.6%
reduction in total failures for function-level development and 56.7% for
deployment-level development. For vulnerability detection, SEMAP reduces
failure counts by up to 47.4% on Python tasks and 28.2% on C/C++ tasks.

</details>


### [8] [iCodeReviewer: Improving Secure Code Review with Mixture of Prompts](https://arxiv.org/abs/2510.12186)
*Yun Peng,Kisub Kim,Linghan Meng,Kui Liu*

Main category: cs.SE

TL;DR: iCodeReviewer是一个基于大型语言模型的自动化安全代码审查方法，通过混合提示架构和路由算法提高安全问题的覆盖率和准确性。


<details>
  <summary>Details</summary>
Motivation: 当前自动化安全代码审查方法（包括静态分析、深度学习模型和提示方法）面临精度和覆盖率有限、缺乏全面评估的挑战。

Method: 采用混合提示架构，包含多个动态提示专家来检查特定安全问题，并通过路由算法基于代码特征激活必要的提示专家以减少误报。

Result: 在内部数据集上，iCodeReviewer在安全问题和定位方面达到63.98%的F1分数，在生产环境中生成的审查评论接受率高达84%。

Conclusion: iCodeReviewer通过LLM和混合提示架构有效提高了自动化安全代码审查的性能和实用性。

Abstract: Code review is an essential process to ensure the quality of software that
identifies potential software issues at an early stage of software development.
Among all software issues, security issues are the most important to identify,
as they can easily lead to severe software crashes and service disruptions.
Recent research efforts have been devoted to automated approaches to reduce the
manual efforts required in the secure code review process. Despite the
progress, current automated approaches on secure code review, including static
analysis, deep learning models, and prompting approaches, still face the
challenges of limited precision and coverage, and a lack of comprehensive
evaluation.
  To mitigate these challenges, we propose iCodeReviewer, which is an automated
secure code review approach based on large language models (LLMs).
iCodeReviewer leverages a novel mixture-of-prompts architecture that
incorporates many prompt experts to improve the coverage of security issues.
Each prompt expert is a dynamic prompt pipeline to check the existence of a
specific security issue. iCodeReviewer also implements an effective routing
algorithm to activate only necessary prompt experts based on the code features
in the input program, reducing the false positives induced by LLM
hallucination. Experiment results in our internal dataset demonstrate the
effectiveness of iCodeReviewer in security issue identification and
localization with an F1 of 63.98%. The review comments generated by
iCodeReviewer also achieve a high acceptance rate up to 84% when it is deployed
in production environments.

</details>


### [9] [Show Your Title! A Scoping Review on Verbalization in Software Engineering with LLM-Assisted Screening](https://arxiv.org/abs/2510.12294)
*Gergő Balogh,Dávid Kószó,Homayoun Safarpour Motealegh Mahalegi,László Tóth,Bence Szakács,Áron Búcsú*

Main category: cs.SE

TL;DR: 本文通过LLM辅助的筛选流程对9000多篇论文进行范围综述，研究了软件工程与心理学交叉领域中口头数据的使用情况，发现SE经常借鉴PSY方法但反向影响很少，主要主题与SE技术相关而人本主题较少。


<details>
  <summary>Details</summary>
Motivation: 理解软件开发者的思维、决策和行为是软件工程的关键挑战，口头化技术提供了一种轻量级方法来研究这些认知方面。

Method: 采用基于GPT的LLM辅助筛选流程，仅基于标题评估9000多篇论文的相关性，并将GPT输出与人工评审进行验证。

Result: GPT输出与人工评审高度一致，不一致率为13%。主要主题与SE技术相关，人本主题较少；SE经常借鉴PSY方法，但反向影响罕见。

Conclusion: LLM在支持跨学科综述过程中有效，口头化研究主要关注SE技术方面，需要更多人本导向的研究，且SE与PSY之间的知识流动主要是单向的。

Abstract: Understanding how software developers think, make decisions, and behave
remains a key challenge in software engineering (SE). Verbalization techniques
(methods that capture spoken or written thought processes) offer a lightweight
and accessible way to study these cognitive aspects. This paper presents a
scoping review of research at the intersection of SE and psychology (PSY),
focusing on the use of verbal data. To make large-scale interdisciplinary
reviews feasible, we employed a large language model (LLM)-assisted screening
pipeline using GPT to assess the relevance of over 9,000 papers based solely on
titles. We addressed two questions: what themes emerge from
verbalization-related work in SE, and how effective are LLMs in supporting
interdisciplinary review processes? We validated GPT's outputs against human
reviewers and found high consistency, with a 13\% disagreement rate. Prominent
themes mainly were tied to the craft of SE, while more human-centered topics
were underrepresented. The data also suggests that SE frequently draws on PSY
methods, whereas the reverse is rare.

</details>


### [10] [(R)evolution of Programming: Vibe Coding as a Post-Coding Paradigm](https://arxiv.org/abs/2510.12364)
*Kevin Krings,Nino S. Bohn,Thomas Ludwig*

Main category: cs.SE

TL;DR: 本文研究了新兴的Vibe Coding范式，这是一种强调开发者与AI系统之间直觉、情感驱动和即兴交互的编程方法，与传统AI辅助开发工具形成对比。


<details>
  <summary>Details</summary>
Motivation: 探索生成式AI特别是大语言模型如何改变软件开发实践，研究Vibe Coding这一新兴范式如何从传统编程方法中分化出来。

Method: 通过对10位经验丰富的软件从业者进行5次半结构化访谈，识别出五个主题维度：创造力、可持续性、编程未来、协作和批评。

Result: 将Vibe Coding概念化为"共同漂流"的隐喻，与流行的"共同驾驶"AI辅助开发视角形成对比。Vibe Coding重新配置了开发者角色，模糊了专业与非专业开发者之间的界限。

Conclusion: Vibe Coding代表了编程文化的有意义转变，虽然支持新颖的表达形式和快速原型设计，但也带来了可重复性、可扩展性和包容性方面的挑战，值得在人机交互和软件工程领域进一步研究。

Abstract: Recent advancements in generative artificial intelligence (GenAI),
particularly large language models, have introduced new possibilities for
software development practices. In our paper we investigate the emerging Vibe
Coding (VC) paradigm that emphasizes intuitive, affect-driven, and
improvisational interactions between developers and AI systems. Building upon
the discourse of End-User Development (EUD), we explore how VC diverges from
conventional programming approaches such as those supported by tools like
GitHub Copilot. Through five semi-structured interview sessions with ten
experienced software practitioners, we identify five thematic dimensions:
creativity, sustainability, the future of programming, collaboration, and
criticism. Our analysis conceptualizes VC within the metaphor of co-drifting,
contrasting it with the prevalent co-piloting perspective of AI-assisted
development. We argue that VC reconfigures the developers role, blurring
boundaries between professional and non-developers. While VC enables novel
forms of expression and rapid prototyping, it also introduces challenges
regarding reproducibility, scalability, and inclusivity. We propose that VC
represents a meaningful shift in programming culture, warranting further
investigation within human-computer interaction (HCI) and software engineering
research.

</details>


### [11] [Should I Run My Cloud Benchmark on Black Friday?](https://arxiv.org/abs/2510.12397)
*Sören Henning,Adriano Vogel,Esteban Perez-Wohlfeil,Otmar Ertl,Rick Rabiser*

Main category: cs.SE

TL;DR: 研究云环境中性能基准测试的可变性，特别关注重大全球事件（如黑色星期五）对性能基准结果的影响。


<details>
  <summary>Details</summary>
Motivation: 云环境中的基准测试结果常因性能变异性而受到质疑，需要量化这种变异性对结果的影响，并探索重大事件是否会导致性能模式变化。

Method: 通过在一天中不同时间重复执行流处理应用基准测试，持续数月，并分析黑色星期五等重大事件期间的性能数据。

Result: 确认应用级别存在性能变异性，但比通常假设的要小；识别出细微的每日和每周性能模式。

Conclusion: 云环境性能变异性确实存在但被高估，重大事件可能影响基准测试结果，需要更全面的研究来理解这些模式。

Abstract: Benchmarks and performance experiments are frequently conducted in cloud
environments. However, their results are often treated with caution, as the
presumed high variability of performance in the cloud raises concerns about
reproducibility and credibility. In a recent study, we empirically quantified
the impact of this variability on benchmarking results by repeatedly executing
a stream processing application benchmark at different times of the day over
several months. Our analysis confirms that performance variability is indeed
observable at the application level, although it is less pronounced than often
assumed. The larger scale of our study compared to related work allowed us to
identify subtle daily and weekly performance patterns. We now extend this
investigation by examining whether a major global event, such as Black Friday,
affects the outcomes of performance benchmarks.

</details>


### [12] [DarTwin made precise by SysMLv2 -- An Experiment](https://arxiv.org/abs/2510.12478)
*Øystein Haugen,Stefan Klikovits,Martin Arthur Andersen,Jonathan Beaulieu,Francis Bordeleau,Joachim Denil,Joost Mertens*

Main category: cs.SE

TL;DR: 本文通过SysMLv2开发了DarTwin DSL，用于形式化数字孪生演化表示法，展示了SysMLv2在领域特定语言创建方面的能力，但也指出了当前工具在图形表示方面的局限性。


<details>
  <summary>Details</summary>
Motivation: 利用SysMLv2新引入的领域特定概念和语言扩展机制，促进领域特定语言(DSL)的创建，并与现有系统描述和技术设计进行接口对接。

Method: 通过具体用例评估SysMLv2能力，开发DarTwin DSL来形式化现有的DarTwin数字孪生演化表示法，使DarTwin的演化模板能够在任何SysMLv2工具中广泛应用。

Result: 成功演示了DarTwin DSL，但发现SysMLv2当前可用工具在图形表示能力方面存在限制。

Conclusion: 这项工作将模型驱动工程与SysMLv2发布相结合，为数字孪生演化管理提供了系统化的工程方法，促进了数字孪生模型驱动工程领域的发展。

Abstract: The new SysMLv2 adds mechanisms for the built-in specification of
domain-specific concepts and language extensions. This feature promises to
facilitate the creation of Domain-Specific Languages (DSLs) and interfacing
with existing system descriptions and technical designs. In this paper, we
review these features and evaluate SysMLv2's capabilities using concrete use
cases. We develop DarTwin DSL, a DSL that formalizes the existing DarTwin
notation for Digital Twin (DT) evolution, through SysMLv2, thereby supposedly
enabling the wide application of DarTwin's evolution templates using any
SysMLv2 tool. We demonstrate DarTwin DSL, but also point out limitations in the
currently available tooling of SysMLv2 in terms of graphical notation
capabilities. This work contributes to the growing field of Model-Driven
Engineering (MDE) for DTs and combines it with the release of SysMLv2, thus
integrating a systematic approach with DT evolution management in systems
engineering.

</details>


### [13] [Diff-XYZ: A Benchmark for Evaluating Diff Understanding](https://arxiv.org/abs/2510.12487)
*Evgeniy Glukhov,Michele Conti,Egor Bogomolov,Yaroslav Golubev,Alexander Bezzubov*

Main category: cs.SE

TL;DR: Diff-XYZ是一个用于代码差异理解的紧凑基准，包含三个监督任务：应用差异、反向应用差异和差异生成，基于真实提交数据构建。


<details>
  <summary>Details</summary>
Motivation: 可靠处理代码差异对于大规模编辑和重构代码库的智能体至关重要，需要评估和改进LLM处理代码差异的能力。

Method: 从CommitPackFT的真实提交中提取<旧代码, 新代码, 差异>三元组，构建基准数据集，并使用自动指标和清晰评估协议。

Result: 研究发现不同格式应根据使用场景和模型大小选择，例如搜索替换格式适合大型模型的差异生成，但不适合差异分析和小型模型。

Conclusion: Diff-XYZ基准为评估和改进LLM中的差异处理提供了可重用的基础，有助于未来差异格式和代码编辑模型的发展。

Abstract: Reliable handling of code diffs is central to agents that edit and refactor
repositories at scale. We introduce Diff-XYZ, a compact benchmark for code-diff
understanding with three supervised tasks: apply (old code $+$ diff
$\rightarrow$ new code), anti-apply (new code $-$ diff $\rightarrow$ old code),
and diff generation (new code $-$ old code $\rightarrow$ diff). Instances in
the benchmark are triples $\langle \textit{old code}, \textit{new code},
\textit{diff} \rangle$ drawn from real commits in CommitPackFT, paired with
automatic metrics and a clear evaluation protocol. We use the benchmark to do a
focused empirical study of the unified diff format and run a cross-format
comparison of different diff representations. Our findings reveal that
different formats should be used depending on the use case and model size. For
example, representing diffs in search-replace format is good for larger models
in the diff generation scenario, yet not suited well for diff analysis and
smaller models. The Diff-XYZ benchmark is a reusable foundation for assessing
and improving diff handling in LLMs that can aid future development of diff
formats and models editing code. The dataset is published on HuggingFace Hub:
https://huggingface.co/datasets/JetBrains-Research/diff-xyz.

</details>


### [14] [The EmpathiSEr: Development and Validation of Software Engineering Oriented Empathy Scales](https://arxiv.org/abs/2510.12546)
*Hashini Gunatilake,John Grundy,Rashina Hoda,Ingo Mueller*

Main category: cs.SE

TL;DR: 开发并验证了两个专门针对软件工程领域的共情量表：EmpathiSEr-P（测量从业者间的共情）和EmpathiSEr-U（测量从业者对用户的共情），填补了软件工程领域缺乏有效共情测量工具的研究空白。


<details>
  <summary>Details</summary>
Motivation: 软件工程中的共情对协作、沟通和以用户为中心的设计至关重要，但现有通用共情量表无法适应软件工程特有的社会技术环境，缺乏针对性的测量工具。

Method: 采用严谨的多阶段方法，包括专家评估、认知访谈和两次从业者调查，基于从业者参与的概念框架开发量表，涵盖认知共情、情感共情和共情反应三个维度。

Result: 成功开发出首个经过心理测量学验证的软件工程专用共情量表，为研究者和从业者提供了评估共情和设计共情增强干预措施的有效工具。

Conclusion: EmpathiSEr量表填补了软件工程领域共情测量的重要空白，为理解和提升软件团队和用户互动中的共情能力提供了科学依据。

Abstract: Empathy plays a critical role in software engineering (SE), influencing
collaboration, communication, and user-centred design. Although SE research has
increasingly recognised empathy as a key human aspect, there remains no
validated instrument specifically designed to measure it within the unique
socio-technical contexts of SE. Existing generic empathy scales, while
well-established in psychology and healthcare, often rely on language,
scenarios, and assumptions that are not meaningful or interpretable for
software practitioners. These scales fail to account for the diverse,
role-specific, and domain-bound expressions of empathy in SE, such as
understanding a non-technical user's frustrations or another practitioner's
technical constraints, which differ substantially from empathy in clinical or
everyday contexts. To address this gap, we developed and validated two
domain-specific empathy scales: EmpathiSEr-P, assessing empathy among
practitioners, and EmpathiSEr-U, capturing practitioner empathy towards users.
Grounded in a practitioner-informed conceptual framework, the scales encompass
three dimensions of empathy: cognitive empathy, affective empathy, and empathic
responses. We followed a rigorous, multi-phase methodology, including expert
evaluation, cognitive interviews, and two practitioner surveys. The resulting
instruments represent the first psychometrically validated empathy scales
tailored to SE, offering researchers and practitioners a tool for assessing
empathy and designing empathy-enhancing interventions in software teams and
user interactions.

</details>


### [15] [Evaluating End-User Device Energy Models in Sustainability Reporting of Browser-Based Web Services](https://arxiv.org/abs/2510.12566)
*Maja H. Kirkeby,Timmie Lagermann*

Main category: cs.SE

TL;DR: 本文通过实证研究评估了当前可持续性报告框架中常用的简化能源和碳模型在实际网站使用场景中的准确性，发现这些模型与实测能耗存在显著偏差，需要更精确的类别感知和设备相关的功率参数。


<details>
  <summary>Details</summary>
Motivation: 当前网页服务可持续性报告广泛采用丹麦数字政府机构的Digst框架和英国的DIMPACT模型等简化能源和碳模型，但这些模型的准确性和精确度尚未得到充分研究。

Method: 通过在实际用户交互场景下测量购物、预订、导航和新闻等常见网站类别的能耗，使用预定义用户流程在四种笔记本电脑平台上执行测试。

Result: 结果显示常用的恒功率近似法(P * t)与实测能耗存在显著差异，差异程度取决于网站类别、设备类型和任务特征，模型偏差是系统性的而非随机的。

Conclusion: 研究强调了在可复现的可持续性报告框架中需要采用类别感知和设备相关的功率参数，以提高模型的准确性。

Abstract: Sustainability reporting in web-based services increasingly relies on
simplified energy and carbon models such as the Danish Agency of Digital
Government's Digst framework and the United Kingdom-based DIMPACT model.
Although these models are widely adopted, their accuracy and precision remain
underexplored. This paper presents an empirical study evaluating how well such
models reflect actual energy consumption during realistic user interactions
with common website categories. Energy use was measured across shopping,
booking, navigation, and news services using predefined user flows executed on
four laptop platforms. The results show that the commonly applied
constant-power approximation (P * t) can diverge substantially from measured
energy, depending on website category, device type, and task characteristics.
The findings demonstrate that model deviations are systematic rather than
random and highlight the need for category-aware and device-reflective power
parameters in reproducible sustainability reporting frameworks.

</details>


### [16] [Runtime Composition in Dynamic System of Systems: A Systematic Review of Challenges, Solutions, Tools, and Evaluation Methods](https://arxiv.org/abs/2510.12616)
*Muhammad Ashfaq,Ahmed R. Sadik,Teerath Das,Muhammad Waseem,Niko Makitalo,Tommi Mikkonen*

Main category: cs.SE

TL;DR: 本文通过系统文献综述分析了动态系统之系统(SoS)中的运行时组合研究，识别了核心挑战、解决方案、支持工具和评估方法，旨在指导动态可组合SoS的开发与实施。


<details>
  <summary>Details</summary>
Motivation: 现代SoS在动态环境中运行，运行时组合对于适应性至关重要，但文献缺乏对此主题的连贯综合。

Method: 采用系统文献综述方法，筛选2019-2024年间的1774项研究，选取80项主要研究进行主题分析。

Result: 挑战分为四类：建模与分析、弹性操作、系统编排、CS异构性。解决方案涵盖七个领域：协同仿真与数字孪生、语义本体、集成框架、自适应架构、中间件、形式化方法、AI驱动弹性。

Conclusion: 分析揭示了自主性与协调性、建模-现实差距、社会技术集成等张力，呼吁标准化评估指标、可扩展去中心化架构和跨领域框架。

Abstract: Context: Modern Systems of Systems (SoSs) increasingly operate in dynamic
environments (e.g., smart cities, autonomous vehicles) where runtime
composition -- the on-the-fly discovery, integration, and coordination of
constituent systems (CSs)--is crucial for adaptability. Despite growing
interest, the literature lacks a cohesive synthesis of runtime composition in
dynamic SoSs. Objective: This study synthesizes research on runtime composition
in dynamic SoSs and identifies core challenges, solution strategies, supporting
tools, and evaluation methods. Methods: We conducted a Systematic Literature
Review (SLR), screening 1,774 studies published between 2019 and 2024 and
selecting 80 primary studies for thematic analysis (TA). Results: Challenges
fall into four categories: modeling and analysis, resilient operations, system
orchestration, and heterogeneity of CSs. Solutions span seven areas:
co-simulation and digital twins, semantic ontologies, integration frameworks,
adaptive architectures, middleware, formal methods, and AI-driven resilience.
Service-oriented frameworks for composition and integration dominate tooling,
while simulation platforms support evaluation. Interoperability across tools,
limited cross-toolchain workflows, and the absence of standardized benchmarks
remain key gaps. Evaluation approaches include simulation-based,
implementation-driven, and human-centered studies, which have been applied in
domains such as smart cities, healthcare, defense, and industrial automation.
Conclusions: The synthesis reveals tensions, including autonomy versus
coordination, the modeling-reality gap, and socio-technical integration. It
calls for standardized evaluation metrics, scalable decentralized
architectures, and cross-domain frameworks. The analysis aims to guide
researchers and practitioners in developing and implementing dynamically
composable SoSs.

</details>


### [17] [Beyond Postconditions: Can Large Language Models infer Formal Contracts for Automatic Software Verification?](https://arxiv.org/abs/2510.12702)
*Cedric Richter,Heike Wehrheim*

Main category: cs.SE

TL;DR: NL2Contract任务使用LLM将自然语言转换为形式化功能合约（包含前置条件和后置条件），相比仅生成后置条件的方法，能有效减少自动验证器产生的误报。


<details>
  <summary>Details</summary>
Motivation: 解决自动软件验证器在实践中因缺乏形式化规范而难以应用的问题，以及LLM仅生成后置条件时容易导致验证器产生误报的局限性。

Method: 引入NL2Contract任务，使用LLM从代码中的自然语言提示（如函数名、注释、文档）推断形式化功能合约，包括前置条件和后置条件。

Result: 评估显示：(1) LLM能有效生成对所有可能输入都可靠的功能合约；(2) 生成的合约足以区分错误和正确行为；(3) 相比仅使用后置条件，使用完整功能合约的验证器产生更少误报。

Conclusion: LLM推断的功能合约与开发者意图一致，使自动软件验证器能够捕获真实世界中的错误，为软件验证实践提供了可行方案。

Abstract: Automatic software verifiers have become increasingly effective at the task
of checking software against (formal) specifications. Yet, their adoption in
practice has been hampered by the lack of such specifications in real world
code. Large Language Models (LLMs) have shown promise in inferring formal
postconditions from natural language hints embedded in code such as function
names, comments or documentation. Using the generated postconditions as
specifications in a subsequent verification, however, often leads verifiers to
suggest invalid inputs, hinting at potential issues that ultimately turn out to
be false alarms.
  To address this, we revisit the problem of specification inference from
natural language in the context of automatic software verification. In the
process, we introduce NL2Contract, the task of employing LLMs to translate
informal natural language into formal functional contracts, consisting of
postconditions as well as preconditions. We introduce metrics to validate and
compare different NL2Contract approaches, using soundness, bug discriminative
power of the generated contracts and their usability in the context of
automatic software verification as key metrics. We evaluate NL2Contract with
different LLMs and compare it to the task of postcondition generation
nl2postcond. Our evaluation shows that (1) LLMs are generally effective at
generating functional contracts sound for all possible inputs, (2) the
generated contracts are sufficiently expressive for discriminating buggy from
correct behavior, and (3) verifiers supplied with LLM inferred functional
contracts produce fewer false alarms than when provided with postconditions
alone. Further investigations show that LLM inferred preconditions generally
align well with developers intentions which allows us to use automatic software
verifiers to catch real-world bugs.

</details>


### [18] [Leveraging LLMs, IDEs, and Semantic Embeddings for Automated Move Method Refactoring](https://arxiv.org/abs/2503.20934)
*Fraol Batole,Abhiram Bellur,Malinda Dilhara,Mohammed Raihan Ullah,Yaroslav Zharov,Timofey Bryksin,Kai Ishikawa,Haifeng Chen,Masaharu Morimoto,Shota Motoura,Takeo Hosomi,Tien N. Nguyen,Hridesh Rajan,Nikolaos Tsantalis,Danny Dig*

Main category: cs.SE

TL;DR: 提出了MM-assist，首个基于LLM的MOVEMETHOD重构全流程自动化助手，通过静态分析过滤LLM幻觉，使用自一致性、批判和排序工作流，结合RAG解决上下文限制，在多个评估中显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有MOVEMETHOD重构工具与专家实践不符，LLM能给出专家级建议但存在高达80%的幻觉问题，需要可靠的全流程自动化解决方案。

Method: 设计MM-assist系统，结合LLM、IDE、静态分析和语义相关性，使用静态分析过滤幻觉，要求LLM自一致性、批判和排序建议，采用重构感知的RAG解决上下文限制。

Result: 在基准测试中Recall@1和Recall@3提升1.7倍；在210个开源重构案例中Recall率至少提升2.4倍；用户研究中82.8%推荐获得积极评价。

Conclusion: MM-assist通过协同LLM、IDE、静态分析和语义相关性，有效解决了MOVEMETHOD重构的可靠性和实用性问题，被证明既有效又有用。

Abstract: MOVEMETHOD is a hallmark refactoring. Despite a plethora of research tools
that recommend which methods to move and where, these recommendations do not
align with how expert developers perform MOVEMETHOD. Given the extensive
training of Large Language Models and their reliance upon naturalness of code,
they should expertly recommend which methods are misplaced in a given class and
which classes are better hosts. Our formative study of 2016 LLM recommendations
revealed that LLMs give expert suggestions, yet they are unreliable: up to 80%
of the suggestions are hallucinations. We introduce the first LLM fully powered
assistant for MOVEMETHOD refactoring that automates its whole end-to-end
lifecycle, from recommendation to execution. We designed novel solutions that
automatically filter LLM hallucinations using static analysis from IDEs and a
novel workflow that requires LLMs to be self-consistent, critique, and rank
refactoring suggestions. As MOVEMETHOD refactoring requires global,
projectlevel reasoning, we solved the limited context size of LLMs by employing
refactoring-aware retrieval augment generation (RAG). Our approach, MM-assist,
synergistically combines the strengths of the LLM, IDE, static analysis, and
semantic relevance. In our thorough, multi-methodology empirical evaluation, we
compare MM-assist with the previous state-of-the-art approaches. MM-assist
significantly outperforms them: (i) on a benchmark widely used by other
researchers, our Recall@1 and Recall@3 show a 1.7x improvement; (ii) on a
corpus of 210 recent refactorings from Open-source software, our Recall rates
improve by at least 2.4x. Lastly, we conducted a user study with 30 experienced
participants who used MM-assist to refactor their own code for one week. They
rated 82.8% of MM-assist recommendations positively. This shows that MM-assist
is both effective and useful.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [19] [A Comprehensive Survey of Website Fingerprinting Attacks and Defenses in Tor: Advances and Open Challenges](https://arxiv.org/abs/2510.11804)
*Yuwen Cui,Guangjing Wang,Khanh Vu,Kai Wei,Kehan Shen,Zhengyuan Jiang,Xiao Han,Ning Wang,Zhuo Lu,Yao Liu*

Main category: cs.CR

TL;DR: 本文对Tor网络中的网站指纹攻击研究进行了系统性综述，涵盖了数据集、攻击模型和防御机制三大领域，分析了现有技术的优缺点并指出了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: Tor网络虽然提供强匿名性，但仍面临网站指纹攻击的威胁。现有防御措施在隐私、可用性和性能之间存在权衡，且缺乏对WF研究的全面综述。

Method: 系统性地将现有WF研究分类为三个关键领域：数据集、攻击模型和防御机制，进行深入的比较分析。

Result: 提供了对WF技术的全面比较分析，强调了在不同威胁模型下的优势和局限性，并讨论了多标签浏览和粗粒度流量特征等新兴挑战。

Conclusion: 通过整合先前工作并确定开放研究方向，本综述为推进Tor中更强的隐私保护奠定了基础。

Abstract: The Tor network provides users with strong anonymity by routing their
internet traffic through multiple relays. While Tor encrypts traffic and hides
IP addresses, it remains vulnerable to traffic analysis attacks such as the
website fingerprinting (WF) attack, achieving increasingly high fingerprinting
accuracy even under open-world conditions. In response, researchers have
proposed a variety of defenses, ranging from adaptive padding, traffic
regularization, and traffic morphing to adversarial perturbation, that seek to
obfuscate or reshape traffic traces. However, these defenses often entail
trade-offs between privacy, usability, and system performance. Despite
extensive research, a comprehensive survey unifying WF datasets, attack
methodologies, and defense strategies remains absent. This paper fills that gap
by systematically categorizing existing WF research into three key domains:
datasets, attack models, and defense mechanisms. We provide an in-depth
comparative analysis of techniques, highlight their strengths and limitations
under diverse threat models, and discuss emerging challenges such as multi-tab
browsing and coarse-grained traffic features. By consolidating prior work and
identifying open research directions, this survey serves as a foundation for
advancing stronger privacy protection in Tor.

</details>


### [20] [BlackIce: A Containerized Red Teaming Toolkit for AI Security Testing](https://arxiv.org/abs/2510.11823)
*Caelin Kaplan,Alexander Warnecke,Neil Archibald*

Main category: cs.CR

TL;DR: BlackIce是一个开源容器化工具包，专为红队测试大型语言模型和传统机器学习模型而设计，提供可复现的版本锁定Docker镜像，集成了14个精选的开源工具，通过统一命令行界面简化AI模型安全评估。


<details>
  <summary>Details</summary>
Motivation: AI模型在现实系统中的广泛应用引发了安全和隐私担忧，现有红队工具选择困难且依赖管理复杂，需要降低门槛并建立标准化环境来简化AI模型评估。

Method: 采用容器化架构，提供版本锁定的Docker镜像，集成14个精选开源工具，支持统一命令行界面，模块化设计便于社区扩展。

Result: 开发了BlackIce工具包，使启动红队评估变得简单，只需在本地或云平台启动容器即可，支持全面的AI模型安全测试。

Conclusion: BlackIce成功降低了AI红队测试的门槛，提供了标准化的评估环境，其模块化架构支持社区驱动的工具扩展，能够应对新兴威胁。

Abstract: AI models are being increasingly integrated into real-world systems, raising
significant concerns about their safety and security. Consequently, AI red
teaming has become essential for organizations to proactively identify and
address vulnerabilities before they can be exploited by adversaries. While
numerous AI red teaming tools currently exist, practitioners face challenges in
selecting the most appropriate tools from a rapidly expanding landscape, as
well as managing complex and frequently conflicting software dependencies
across isolated projects. Given these challenges and the relatively small
number of organizations with dedicated AI red teams, there is a strong need to
lower barriers to entry and establish a standardized environment that
simplifies the setup and execution of comprehensive AI model assessments.
  Inspired by Kali Linux's role in traditional penetration testing, we
introduce BlackIce, an open-source containerized toolkit designed for red
teaming Large Language Models (LLMs) and classical machine learning (ML)
models. BlackIce provides a reproducible, version-pinned Docker image that
bundles 14 carefully selected open-source tools for Responsible AI and Security
testing, all accessible via a unified command-line interface. With this setup,
initiating red team assessments is as straightforward as launching a container,
either locally or using a cloud platform. Additionally, the image's modular
architecture facilitates community-driven extensions, allowing users to easily
adapt or expand the toolkit as new threats emerge. In this paper, we describe
the architecture of the container image, the process used for selecting tools,
and the types of evaluations they support.

</details>


### [21] [Countermind: A Multi-Layered Security Architecture for Large Language Models](https://arxiv.org/abs/2510.11837)
*Dominik Schwarz*

Main category: cs.CR

TL;DR: 该论文提出了Countermind多层安全架构，通过语义边界逻辑、参数空间限制、安全自调节核心和多模态输入沙盒等机制，从被动防御转向主动的推理前和推理中安全执行，以应对LLM应用中的提示注入和越狱攻击。


<details>
  <summary>Details</summary>
Motivation: 传统基于输出过滤的防御方法脆弱且无法解决根本问题——模型无法区分可信指令和不可信数据。需要从被动防御转向主动的推理前和推理中安全执行。

Method: 提出四层安全架构：1) 语义边界逻辑和时间耦合文本加密器；2) 参数空间限制机制控制内部语义集群访问；3) 安全自调节核心使用OODA循环和学习安全模块；4) 多模态输入沙盒和上下文防御机制。

Result: 论文提出了评估计划，旨在量化该架构在降低攻击成功率方面的有效性，并测量其潜在延迟开销。

Conclusion: Countermind架构通过多层安全机制，为LLM应用提供了从被动防御到主动安全执行的新范式，有望显著降低提示注入和越狱攻击的成功率。

Abstract: The security of Large Language Model (LLM) applications is fundamentally
challenged by "form-first" attacks like prompt injection and jailbreaking,
where malicious instructions are embedded within user inputs. Conventional
defenses, which rely on post hoc output filtering, are often brittle and fail
to address the root cause: the model's inability to distinguish trusted
instructions from untrusted data. This paper proposes Countermind, a
multi-layered security architecture intended to shift defenses from a reactive,
post hoc posture to a proactive, pre-inference, and intra-inference enforcement
model. The architecture proposes a fortified perimeter designed to structurally
validate and transform all inputs, and an internal governance mechanism
intended to constrain the model's semantic processing pathways before an output
is generated. The primary contributions of this work are conceptual designs
for: (1) A Semantic Boundary Logic (SBL) with a mandatory, time-coupled Text
Crypter intended to reduce the plaintext prompt injection attack surface,
provided all ingestion paths are enforced. (2) A Parameter-Space Restriction
(PSR) mechanism, leveraging principles from representation engineering, to
dynamically control the LLM's access to internal semantic clusters, with the
goal of mitigating semantic drift and dangerous emergent behaviors. (3) A
Secure, Self-Regulating Core that uses an OODA loop and a learning security
module to adapt its defenses based on an immutable audit log. (4) A Multimodal
Input Sandbox and Context-Defense mechanisms to address threats from
non-textual data and long-term semantic poisoning. This paper outlines an
evaluation plan designed to quantify the proposed architecture's effectiveness
in reducing the Attack Success Rate (ASR) for form-first attacks and to measure
its potential latency overhead.

</details>


### [22] [Deep Research Brings Deeper Harm](https://arxiv.org/abs/2510.11851)
*Shuo Chen,Zonggen Li,Zhen Han,Bailan He,Tong Liu,Haokun Chen,Georg Groh,Philip Torr,Volker Tresp,Jindong Gu*

Main category: cs.CR

TL;DR: 本文研究发现基于大语言模型的深度研究代理存在严重的安全风险，即使LLM本身会拒绝有害查询，但通过计划注入和意图劫持等新方法，DR代理仍能生成详细危险的专业报告。


<details>
  <summary>Details</summary>
Motivation: 深度研究代理能够执行复杂多步骤研究任务，但在生物安全等高风险领域，这种强大能力可能被滥用生成包含禁止知识的专业报告，现有LLM越狱方法无法有效暴露这种独特风险。

Method: 提出了两种新的越狱策略：计划注入（在代理计划中注入恶意子目标）和意图劫持（将有害查询重新构建为学术研究问题），并在不同LLM和安全基准上进行了广泛实验。

Result: 实验发现三个关键结果：1）LLM的对齐在DR代理中经常失效；2）多步骤规划和执行削弱了对齐，揭示了系统级漏洞；3）DR代理不仅能绕过拒绝，还能生成比独立LLM更连贯、专业和危险的内容。

Conclusion: 研究结果表明DR代理存在根本性的对齐问题，需要开发专门针对DR代理的更好对齐技术。

Abstract: Deep Research (DR) agents built on Large Language Models (LLMs) can perform
complex, multi-step research by decomposing tasks, retrieving online
information, and synthesizing detailed reports. However, the misuse of LLMs
with such powerful capabilities can lead to even greater risks. This is
especially concerning in high-stakes and knowledge-intensive domains such as
biosecurity, where DR can generate a professional report containing detailed
forbidden knowledge. Unfortunately, we have found such risks in practice:
simply submitting a harmful query, which a standalone LLM directly rejects, can
elicit a detailed and dangerous report from DR agents. This highlights the
elevated risks and underscores the need for a deeper safety analysis. Yet,
jailbreak methods designed for LLMs fall short in exposing such unique risks,
as they do not target the research ability of DR agents. To address this gap,
we propose two novel jailbreak strategies: Plan Injection, which injects
malicious sub-goals into the agent's plan; and Intent Hijack, which reframes
harmful queries as academic research questions. We conducted extensive
experiments across different LLMs and various safety benchmarks, including
general and biosecurity forbidden prompts. These experiments reveal 3 key
findings: (1) Alignment of the LLMs often fail in DR agents, where harmful
prompts framed in academic terms can hijack agent intent; (2) Multi-step
planning and execution weaken the alignment, revealing systemic vulnerabilities
that prompt-level safeguards cannot address; (3) DR agents not only bypass
refusals but also produce more coherent, professional, and dangerous content,
compared with standalone LLMs. These results demonstrate a fundamental
misalignment in DR agents and call for better alignment techniques tailored to
DR agents. Code and datasets are available at
https://chenxshuo.github.io/deeper-harm.

</details>


### [23] [Lightweight CNN-Based Wi-Fi Intrusion Detection Using 2D Traffic Representations](https://arxiv.org/abs/2510.11898)
*Rayed Suhail Ahmad,Rehan Ahmad,Quamar Niyaz*

Main category: cs.CR

TL;DR: 提出基于深度学习的Wi-Fi网络入侵检测系统，将网络流量转换为二维数据表示，使用轻量级CNN架构实现低延迟检测，在AWID3数据集上取得良好性能。


<details>
  <summary>Details</summary>
Motivation: Wi-Fi网络普遍存在但存在安全漏洞，恶意行为者可能未经授权访问或窃取敏感数据，需要有效的入侵检测系统来保护网络安全。

Method: 将网络流量转换为五种不同的二维数据表示，使用轻量级卷积神经网络架构进行训练，在AWID3数据集上进行二元和多分类任务评估。

Result: 实验结果表明该方法具有竞争力的检测性能和低推理时间，适合实际Wi-Fi部署场景。

Conclusion: 提出的深度学习网络入侵检测系统能够有效检测Wi-Fi环境中的安全威胁，同时满足实时检测的低延迟要求。

Abstract: Wi-Fi networks are ubiquitous in both home and enterprise environments,
serving as a primary medium for Internet access and forming the backbone of
modern IoT ecosystems. However, their inherent vulnerabilities, combined with
widespread adoption, create opportunities for malicious actors to gain
unauthorized access or compromise sensitive data stored on connected devices.
To address these challenges, we propose a deep learning based network intrusion
detection system (NIDS) for Wi-Fi environments. Building on our previous work,
we convert network traffic into two-dimensional data representations and use
them to train DL models based on convolutional neural network (CNN)
architectures. We implement five distinct techniques for generating the
two-dimensional representations, and to ensure low detection latency, we adopt
lightweight CNN architectures in our NIDS. The models are trained using the
AWID3 dataset, a publicly available benchmark for Wi-Fi NIDS research, and are
evaluated for both binary and multi-class classification tasks. Experimental
results demonstrate that the proposed approach achieves competitive detection
performance with low inference time, making it suitable for real-world Wi-Fi
deployment scenarios.

</details>


### [24] [Robust ML-based Detection of Conventional, LLM-Generated, and Adversarial Phishing Emails Using Advanced Text Preprocessing](https://arxiv.org/abs/2510.11915)
*Deeksha Hareesha Kulal,Chidozie Princewill Arannonu,Afsah Anwar,Nidhi Rastogi,Quamar Niyaz*

Main category: cs.CR

TL;DR: 提出了一种针对LLM生成的钓鱼邮件的鲁棒检测系统，通过增强的文本预处理管道和机器学习算法，在对抗性攻击和AI生成的钓鱼邮件上表现出色。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型的发展，钓鱼邮件变得越来越难以与传统检测机制区分，因为LLM生成的邮件语法正确、上下文相关且语言自然。

Method: 采用增强的文本预处理管道（包括拼写纠正和分词），结合自然语言处理特征提取技术和机器学习算法。

Result: 在公开数据集上达到94.26%的检测准确率和84.39%的F1分数，在对抗性攻击和LLM生成的钓鱼邮件测试中表现出良好的鲁棒性。

Conclusion: 该方法能够有效应对由AI技术驱动的不断演变的钓鱼威胁，展示了在对抗性环境下的强健性能。

Abstract: Phishing remains a critical cybersecurity threat, especially with the advent
of large language models (LLMs) capable of generating highly convincing
malicious content. Unlike earlier phishing attempts which are identifiable by
grammatical errors, misspellings, incorrect phrasing, and inconsistent
formatting, LLM generated emails are grammatically sound, contextually
relevant, and linguistically natural. These advancements make phishing emails
increasingly difficult to distinguish from legitimate ones, challenging
traditional detection mechanisms. Conventional phishing detection systems often
fail when faced with emails crafted by LLMs or manipulated using adversarial
perturbation techniques. To address this challenge, we propose a robust
phishing email detection system featuring an enhanced text preprocessing
pipeline. This pipeline includes spelling correction and word splitting to
counteract adversarial modifications and improve detection accuracy. Our
approach integrates widely adopted natural language processing (NLP) feature
extraction techniques and machine learning algorithms. We evaluate our models
on publicly available datasets comprising both phishing and legitimate emails,
achieving a detection accuracy of 94.26% and F1-score of 84.39% in model
deployment setting. To assess robustness, we further evaluate our models using
adversarial phishing samples generated by four attack methods in Python
TextAttack framework. Additionally, we evaluate models' performance against
phishing emails generated by LLMs including ChatGPT and Llama. Results
highlight the resilience of models against evolving AI-powered phishing
threats.

</details>


### [25] [CTIArena: Benchmarking LLM Knowledge and Reasoning Across Heterogeneous Cyber Threat Intelligence](https://arxiv.org/abs/2510.11974)
*Yutong Cheng,Yang Liu,Changze Li,Dawn Song,Peng Gao*

Main category: cs.CR

TL;DR: CTIArena是首个评估大语言模型在多源异构网络威胁情报知识增强场景下性能的基准测试，涵盖9个任务，发现通用LLMs在闭卷设置下表现不佳，但通过检索增强技术可获得显著提升。


<details>
  <summary>Details</summary>
Motivation: 现有CTI评估基准存在三个局限：仅采用闭卷设置、任务覆盖范围窄、仅限于单源分析，无法反映真实的多源CTI分析场景。

Method: 构建CTIArena基准，涵盖结构化、非结构化和混合三类共9个任务，在知识增强设置下评估10个广泛使用的LLMs，并设计检索增强技术来提供安全领域特定知识。

Result: 大多数LLMs在闭卷设置下表现不佳，但通过检索增强技术获得显著性能提升，凸显了通用LLMs的局限性。

Conclusion: 需要领域定制的技术来充分释放LLMs在CTI分析中的潜力，通用LLMs需要安全特定知识的增强才能有效处理CTI任务。

Abstract: Cyber threat intelligence (CTI) is central to modern cybersecurity, providing
critical insights for detecting and mitigating evolving threats. With the
natural language understanding and reasoning capabilities of large language
models (LLMs), there is increasing interest in applying them to CTI, which
calls for benchmarks that can rigorously evaluate their performance. Several
early efforts have studied LLMs on some CTI tasks but remain limited: (i) they
adopt only closed-book settings, relying on parametric knowledge without
leveraging CTI knowledge bases; (ii) they cover only a narrow set of tasks,
lacking a systematic view of the CTI landscape; and (iii) they restrict
evaluation to single-source analysis, unlike realistic scenarios that require
reasoning across multiple sources. To fill these gaps, we present CTIArena, the
first benchmark for evaluating LLM performance on heterogeneous, multi-source
CTI under knowledge-augmented settings. CTIArena spans three categories,
structured, unstructured, and hybrid, further divided into nine tasks that
capture the breadth of CTI analysis in modern security operations. We evaluate
ten widely used LLMs and find that most struggle in closed-book setups but show
noticeable gains when augmented with security-specific knowledge through our
designed retrieval-augmented techniques. These findings highlight the
limitations of general-purpose LLMs and the need for domain-tailored techniques
to fully unlock their potential for CTI.

</details>


### [26] [Security and Privacy Assessment of U.S. and Non-U.S. Android E-Commerce Applications](https://arxiv.org/abs/2510.12031)
*Urvashi Kishnani,Sanchari Das*

Main category: cs.CR

TL;DR: 对92个顶级Android电商应用的安全分析显示，92%存在SSL和证书弱点，平均安全得分仅40.92/100，77个应用存在权限过度授予问题。


<details>
  <summary>Details</summary>
Motivation: 电商移动应用在全球金融交易中占据核心地位，其安全性和隐私保护至关重要，需要全面评估当前安全状况。

Method: 使用MobSF、AndroBugs和RiskInDroid工具分析92个顶级Android电商应用（58个美国应用和34个国际应用）。

Result: 发现广泛的SSL和证书弱点，92%使用不安全的HTTP连接，平均安全得分40.92/100，77个应用存在过度权限问题。美国应用在清单、代码和证书漏洞方面表现较好，但两组在网络相关问题上表现相似。

Conclusion: 建议各地区采用更强、标准化且以用户为中心的安全实践。

Abstract: E-commerce mobile applications are central to global financial transactions,
making their security and privacy crucial. In this study, we analyze 92
top-grossing Android e-commerce apps (58 U.S.-based and 34 international) using
MobSF, AndroBugs, and RiskInDroid. Our analysis shows widespread SSL and
certificate weaknesses, with approximately 92% using unsecured HTTP connections
and an average MobSF security score of 40.92/100. Over-privileged permissions
were identified in 77 apps. While U.S. apps exhibited fewer manifest, code, and
certificate vulnerabilities, both groups showed similar network-related issues.
We advocate for the adoption of stronger, standardized, and user-focused
security practices across regions.

</details>


### [27] [Over-Threshold Multiparty Private Set Intersection for Collaborative Network Intrusion Detection](https://arxiv.org/abs/2510.12045)
*Onur Eren Arpaci,Raouf Boutaba,Florian Kerschbaum*

Main category: cs.CR

TL;DR: 提出了一种保护隐私的IP地址收集协议，通过单收集器超阈值私有集合交集技术，让N个参与者识别出现在至少t个参与者集合中的IP地址，而不泄露其他IP地址信息。


<details>
  <summary>Details</summary>
Motivation: 协作网络入侵检测需要分析合作者的网络日志以查找共同的IP地址，但直接共享IP地址涉及隐私风险，因为IP地址是个人可识别信息，可能违反隐私法规。

Method: 使用单收集器超阈值私有集合交集协议，结合新颖的哈希方案，将计算复杂度从O(M(N log M/t)^(2t))降低到O(t²M C(N,t))。提供两种部署选项：防串通部署（安全性更强但通信开销大）和非交互式部署（假设收集器不串通，通信成本低）。

Result: 协议计算复杂度显著降低，使其能够实际应用于真实网络日志。通过多个机构的联合网络日志测试验证了协议的有效性。

Conclusion: 提出的协议在保护隐私的前提下实现了协作网络入侵检测中的IP地址共享，通过优化算法使其在实际应用中可行，并提供了不同安全需求的部署选项。

Abstract: An important function of collaborative network intrusion detection is to
analyze the network logs of the collaborators for joint IP addresses. However,
sharing IP addresses in plain is sensitive and may be even subject to privacy
legislation as it is personally identifiable information. In this paper, we
present the privacy-preserving collection of IP addresses. We propose a single
collector, over-threshold private set intersection protocol. In this protocol
$N$ participants identify the IP addresses that appear in at least $t$
participant's sets without revealing any information about other IP addresses.
Using a novel hashing scheme, we reduce the computational complexity of the
previous state-of-the-art solution from $O(M(N \log{M}/t)^{2t})$ to
$O(t^2M\binom{N}{t})$, where $M$ denotes the dataset size. This reduction makes
it practically feasible to apply our protocol to real network logs. We test our
protocol using joint networks logs of multiple institutions. Additionally, we
present two deployment options: a collusion-safe deployment, which provides
stronger security guarantees at the cost of increased communication overhead,
and a non-interactive deployment, which assumes a non-colluding collector but
offers significantly lower communication costs and applicable to many use cases
of collaborative network intrusion detection similar to ours.

</details>


### [28] [Adding All Flavors: A Hybrid Random Number Generator for dApps and Web3](https://arxiv.org/abs/2510.12062)
*Ranjith Chodavarapu,Rabimba Karanjai,Xinxin Fan,Weidong Shi,Lei Xu*

Main category: cs.CR

TL;DR: 提出了一种基于TEE物联网设备的混合随机数生成方案，通过密码学工具聚合多个随机源，为dApp提供可配置的安全随机数服务。


<details>
  <summary>Details</summary>
Motivation: 现有链上和链下随机数生成机制存在安全风险或复杂度高的问题，需要一种能够平衡不同因素的随机数生成框架。

Method: 利用配备可信执行环境(TEE)的物联网设备作为随机源，通过密码学工具聚合多个随机源生成最终随机数，支持用户配置恶意参与者容忍度。

Result: 新方法只需一个诚实的随机源即可保证最终随机数的无偏性，并提供了降低链上计算复杂度的具体实现，评估了计算和gas成本。

Conclusion: 该混合方案有效缓解了现有随机数生成机制的局限性，为dApp提供了可配置、安全且成本效益高的随机数服务。

Abstract: Random numbers play a vital role in many decentralized applications (dApps),
such as gaming and decentralized finance (DeFi) applications.
  Existing random number provision mechanisms can be roughly divided into two
categories, on-chain, and off-chain.
  On-chain approaches usually rely on the blockchain as the major input and all
computations are done by blockchain nodes.
  The major risk for this type of method is that the input itself is
susceptible to the adversary's influence.
  Off-chain approaches, as the name suggested, complete the generation without
the involvement of blockchain nodes and share the result directly with a dApp.
  These mechanisms usually have a strong security assumption and high
complexity.
  To mitigate these limitations and provide a framework that allows a dApp to
balance different factors involved in random number generation, we propose a
hybrid random number generation solution that leverages IoT devices equipped
with trusted execution environment (TEE) as the randomness sources, and then
utilizes a set of cryptographic tools to aggregate the multiple sources and
obtain the final random number that can be consumed by the dApp.
  The new approach only needs one honest random source to guarantee the
unbiasedness of the final random number and a user can configure the system to
tolerate malicious participants who can refuse to respond to avoid unfavored
results.
  We also provide a concrete construction that can further reduce the on-chain
computation complexity to lower the cost of the solution in practice.
  We evaluate the computation and gas costs to demonstrate the effectiveness of
the improvement.

</details>


### [29] [Elevating Medical Image Security: A Cryptographic Framework Integrating Hyperchaotic Map and GRU](https://arxiv.org/abs/2510.12084)
*Weixuan Li,Guang Yu,Quanjun Li,Junhua Zhou,Jiajun Chen,Yihang Dong,Mengqian Wang,Zimeng Li,Changwei Gong,Lin Tang,Xuhang Chen*

Main category: cs.CR

TL;DR: Kun-IE是一个基于混沌系统的图像加密框架，通过2D-SCPHM超混沌映射和Kun-SCAN置换策略，解决了现有混沌加密方法在置换扩散和伪随机性方面的不足。


<details>
  <summary>Details</summary>
Motivation: 现有混沌图像加密方法存在置换扩散不足、伪随机特性不理想等漏洞，需要开发更安全可靠的加密方案。

Method: 开发了2D Sin-Cos Pi超混沌映射(2D-SCPHM)提供更宽的混沌范围和更好的伪随机序列生成，并提出Kun-SCAN置换策略来降低像素相关性。

Result: 实验结果表明该框架对各种密码分析攻击具有鲁棒性，支持任意尺寸图像的加密。

Conclusion: Kun-IE是安全图像通信的强大解决方案，代码已开源。

Abstract: Chaotic systems play a key role in modern image encryption due to their
sensitivity to initial conditions, ergodicity, and complex dynamics. However,
many existing chaos-based encryption methods suffer from vulnerabilities, such
as inadequate permutation and diffusion, and suboptimal pseudorandom
properties. This paper presents Kun-IE, a novel encryption framework designed
to address these issues. The framework features two key contributions: the
development of the 2D Sin-Cos Pi Hyperchaotic Map (2D-SCPHM), which offers a
broader chaotic range and superior pseudorandom sequence generation, and the
introduction of Kun-SCAN, a novel permutation strategy that significantly
reduces pixel correlations, enhancing resistance to statistical attacks. Kun-IE
is flexible and supports encryption for images of any size. Experimental
results and security analyses demonstrate its robustness against various
cryptanalytic attacks, making it a strong solution for secure image
communication. The code is available at this
\href{https://github.com/QuincyQAQ/Elevating-Medical-Image-Security-A-Cryptographic-Framework-Integrating-Hyperchaotic-Map-and-GRU}{link}.

</details>


### [30] [Locket: Robust Feature-Locking Technique for Language Models](https://arxiv.org/abs/2510.12117)
*Lipeng He,Vasisht Duddu,N. Asokan*

Main category: cs.CR

TL;DR: Locket是一种用于大语言模型的特征锁定技术，通过适配器合并方法实现付费解锁方案，能有效拒绝未授权功能，同时保持已解锁功能的实用性。


<details>
  <summary>Details</summary>
Motivation: 现有的特征锁定技术（如密码锁定模型）不够健壮或可扩展，无法满足更细粒度的付费解锁商业模式需求。

Method: 使用新颖的适配器合并方法，将适配器附加到LLM上，以拒绝未授权的特征访问。

Result: Locket在锁定特征上达到100%拒绝率，已解锁功能实用性下降≤7%，攻击成功率≤5%，并能扩展到多个特征和用户。

Conclusion: Locket是首个健壮且可扩展的特征锁定技术，为付费解锁商业模式提供了可行解决方案。

Abstract: Chatbot providers (e.g., OpenAI) rely on tiered subscription schemes to
generate revenue, offering basic models for free users, and advanced models for
paying subscribers. However, a finer-grained pay-to-unlock scheme for premium
features (e.g., math, coding) is thought to be more economically viable for the
providers. Such a scheme requires a feature-locking technique (FLoTE) which is
(i) effective in refusing locked features, (ii) utility-preserving for unlocked
features, (iii) robust against evasion or unauthorized credential sharing, and
(iv) scalable to multiple features and users. However, existing FLoTEs (e.g.,
password-locked models) are not robust or scalable. We present Locket, the
first robust and scalable FLoTE to enable pay-to-unlock schemes. Locket uses a
novel merging approach to attach adapters to an LLM for refusing unauthorized
features. Our comprehensive evaluation shows that Locket is effective ($100$%
refusal on locked features), utility-preserving ($\leq 7$% utility degradation
in unlocked features), robust ($\leq 5$% attack success rate), and scales to
multiple features and clients.

</details>


### [31] [VeilAudit: Breaking the Deadlock Between Privacy and Accountability Across Blockchains](https://arxiv.org/abs/2510.12153)
*Minhao Qiao,Iqbal Gondal,Hai Dong*

Main category: cs.CR

TL;DR: VeilAudit是一个跨链审计框架，通过审计者可链接性机制在保护用户隐私的同时实现监管问责，支持阈值门控身份披露和匿名环境下的声誉建立。


<details>
  <summary>Details</summary>
Motivation: 解决区块链跨链互操作性中用户隐私与监管问责之间的根本矛盾，现有方案在完全匿名和强制身份披露之间二选一，限制了在受监管金融环境中的采用。

Method: 使用用户生成的链接审计标签，嵌入零知识证明验证有效性但不暴露用户主钱包地址；采用特殊密文使指定审计者能够测试链接性；支持在正当程序下的阈值门控身份披露。

Result: 开发了原型系统并在多个EVM链上实现，评估表明该框架在当前多链环境中具有实用性。

Conclusion: VeilAudit在隐私保护和监管合规之间取得了平衡，为跨链环境提供了实用的审计解决方案，支持基于可验证行为历史的信用评分等应用。

Abstract: Cross chain interoperability in blockchain systems exposes a fundamental
tension between user privacy and regulatory accountability. Existing solutions
enforce an all or nothing choice between full anonymity and mandatory identity
disclosure, which limits adoption in regulated financial settings. We present
VeilAudit, a cross chain auditing framework that introduces Auditor Only
Linkability, which allows auditors to link transaction behaviors that originate
from the same anonymous entity without learning its identity. VeilAudit
achieves this with a user generated Linkable Audit Tag that embeds a zero
knowledge proof to attest to its validity without exposing the user master
wallet address, and with a special ciphertext that only designated auditors can
test for linkage. To balance privacy and compliance, VeilAudit also supports
threshold gated identity revelation under due process. VeilAudit further
provides a mechanism for building reputation in pseudonymous environments,
which enables applications such as cross chain credit scoring based on
verifiable behavioral history. We formalize the security guarantees and develop
a prototype that spans multiple EVM chains. Our evaluation shows that the
framework is practical for today multichain environments.

</details>


### [32] [Leaking Queries On Secure Stream Processing Systems](https://arxiv.org/abs/2510.12172)
*Hung Pham,Viet Vo,Tien Tuan Anh Dinh,Duc Tran,Shuhao Zhang*

Main category: cs.CR

TL;DR: 本文提出了一种针对使用Intel SGX保护执行引擎的流处理系统的攻击方法，通过时序侧信道提取敏感查询，攻击成功率高达92%。


<details>
  <summary>Details</summary>
Motivation: 流处理系统在云环境中运行，查询与应用逻辑同样敏感，但现有安全方案主要关注数据保护而非查询保护。

Method: 攻击分为两个阶段：离线阶段基于合成数据分析流操作符执行时间建立识别模型；在线阶段隔离查询操作符并利用模型恢复查询。

Result: 在SecureStream和NEXMark基准测试上实现攻击，成功率最高达92%。

Conclusion: 流处理系统存在严重的安全漏洞，需要采取额外措施来抵御此类时序侧信道攻击，同时避免高开销。

Abstract: Stream processing systems are important in modern applications in which data
arrive continuously and need to be processed in real time. Because of their
resource and scalability requirements, many of these systems run on the cloud,
which is considered untrusted. Existing works on securing databases on the
cloud focus on protecting the data, and most systems leverage trusted hardware
for high performance. However, in stream processing systems, queries are as
sensitive as the data because they contain the application logics.
  We demonstrate that it is practical to extract the queries from stream
processing systems that use Intel SGX for securing the execution engine. The
attack performed by a malicious cloud provider is based on timing side
channels, and it works in two phases. In the offline phase, the attacker
profiles the execution time of individual stream operators, based on synthetic
data. This phase outputs a model that identifies individual stream operators.
In the online phase, the attacker isolates the operators that make up the
query, monitors its execution, and recovers the operators using the model in
the previous phase. We implement the attack based on popular data stream
benchmarks using SecureStream and NEXMark, and demonstrate attack success rates
of up to 92%. We further discuss approaches that can harden streaming
processing systems against our attacks without incurring high overhead.

</details>


### [33] [HackWorld: Evaluating Computer-Use Agents on Exploiting Web Application Vulnerabilities](https://arxiv.org/abs/2510.12200)
*Xiaoxue Ren,Penghao Jiang,Kaixin Li,Zhiyong Huang,Xiaoning Du,Jiaojiao Jiang,Zhenchang Xing,Jiamou Sun,Terry Yue Zhuo*

Main category: cs.CR

TL;DR: HackWorld是首个系统评估计算机使用代理通过视觉交互利用Web应用漏洞能力的框架，包含36个真实应用和多种安全漏洞，测试结果显示当前CUAs的漏洞利用率低于12%，存在网络安全意识不足的问题。


<details>
  <summary>Details</summary>
Motivation: 传统渗透测试成本高且难以扩展，而现代Web应用需要视觉理解和多步交互能力，但计算机使用代理在通过图形界面发现和利用漏洞方面的能力尚未被充分探索。

Method: 开发HackWorld框架，包含36个真实世界的Web应用，涵盖11种框架和7种语言，采用夺旗竞赛设置测试CUAs识别和利用漏洞的能力，同时处理复杂的Web界面交互。

Result: 评估显示最先进的CUAs漏洞利用率低于12%，网络安全意识较低，在多步攻击规划和安全工具使用方面存在困难。

Conclusion: 当前CUAs在Web安全环境中存在明显局限性，需要开发更具安全意识的代理来提高漏洞检测和利用的有效性。

Abstract: Web applications are prime targets for cyberattacks as gateways to critical
services and sensitive data. Traditional penetration testing is costly and
expertise-intensive, making it difficult to scale with the growing web
ecosystem. While language model agents show promise in cybersecurity, modern
web applications demand visual understanding, dynamic content handling, and
multi-step interactions that only computer-use agents (CUAs) can perform. Yet,
their ability to discover and exploit vulnerabilities through graphical
interfaces remains largely unexplored. We present HackWorld, the first
framework for systematically evaluating CUAs' capabilities to exploit web
application vulnerabilities via visual interaction. Unlike sanitized
benchmarks, HackWorld includes 36 real-world applications across 11 frameworks
and 7 languages, featuring realistic flaws such as injection vulnerabilities,
authentication bypasses, and unsafe input handling. Using a Capture-the-Flag
(CTF) setup, it tests CUAs' capacity to identify and exploit these weaknesses
while navigating complex web interfaces. Evaluation of state-of-the-art CUAs
reveals concerning trends: exploitation rates below 12% and low cybersecurity
awareness. CUAs often fail at multi-step attack planning and misuse security
tools. These results expose the current limitations of CUAs in web security
contexts and highlight opportunities for developing more security-aware agents
capable of effective vulnerability detection and exploitation.

</details>


### [34] [PromptLocate: Localizing Prompt Injection Attacks](https://arxiv.org/abs/2510.12252)
*Yuqi Jia,Yupei Liu,Zedian Shao,Jinyuan Jia,Neil Gong*

Main category: cs.CR

TL;DR: 提出了首个提示注入定位方法PromptLocate，通过三个步骤准确定位污染数据中的注入提示，包括分割语义连贯片段、识别注入指令污染片段和定位注入数据污染片段。


<details>
  <summary>Details</summary>
Motivation: 提示注入攻击通过污染输入数据欺骗大型语言模型执行攻击者指定的任务而非预期任务。定位注入提示对于攻击后取证分析和数据恢复至关重要，但目前该领域研究仍很缺乏。

Method: PromptLocate方法包含三个步骤：1）将污染数据分割为语义连贯的片段；2）识别被注入指令污染的片段；3）精确定位被注入数据污染的片段。

Result: 实验表明PromptLocate在8个现有攻击和8个自适应攻击中都能准确定位注入提示。

Conclusion: 该研究填补了提示注入定位领域的空白，提出的PromptLocate方法能够有效定位各种攻击中的注入提示，为后续取证分析和数据恢复提供支持。

Abstract: Prompt injection attacks deceive a large language model into completing an
attacker-specified task instead of its intended task by contaminating its input
data with an injected prompt, which consists of injected instruction(s) and
data. Localizing the injected prompt within contaminated data is crucial for
post-attack forensic analysis and data recovery. Despite its growing
importance, prompt injection localization remains largely unexplored. In this
work, we bridge this gap by proposing PromptLocate, the first method for
localizing injected prompts. PromptLocate comprises three steps: (1) splitting
the contaminated data into semantically coherent segments, (2) identifying
segments contaminated by injected instructions, and (3) pinpointing segments
contaminated by injected data. We show PromptLocate accurately localizes
injected prompts across eight existing and eight adaptive attacks.

</details>


### [35] [DeepTrust: Multi-Step Classification through Dissimilar Adversarial Representations for Robust Android Malware Detection](https://arxiv.org/abs/2510.12310)
*Daniel Pulido-Cortázar,Daniel Gibert,Felip Manyà*

Main category: cs.CR

TL;DR: DeepTrust是一种新颖的元启发式方法，通过将灵活分类器按顺序排列，在级联激活条件下由单个内部模型做出最终决策，在对抗性攻击下显著提升了Android恶意软件检测的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有的机器学习方法在检测Android恶意应用时容易受到对抗性样本的攻击，这些样本经过微妙操纵可以欺骗模型做出错误预测。

Method: DeepTrust将深度神经网络等灵活分类器排列成有序序列，通过最大化内部模型间学习表示的差异性，使决策空间对攻击者不可预测，从而挫败逃避攻击的迭代扰动过程。

Result: 在2025年IEEE SaTML会议的鲁棒Android恶意软件检测竞赛中获得第一名，在特征空间逃避攻击下比次优竞争者高出266%，同时保持对非对抗性恶意软件的最高检测率和低于1%的误报率。

Conclusion: DeepTrust通过使用诱导根本不同数据嵌入的分类器，在不影响干净样本准确性的前提下增强了系统鲁棒性，为对抗性攻击下的恶意软件检测提供了有效解决方案。

Abstract: Over the last decade, machine learning has been extensively applied to
identify malicious Android applications. However, such approaches remain
vulnerable against adversarial examples, i.e., examples that are subtly
manipulated to fool a machine learning model into making incorrect predictions.
This research presents DeepTrust, a novel metaheuristic that arranges flexible
classifiers, like deep neural networks, into an ordered sequence where the
final decision is made by a single internal model based on conditions activated
in cascade. In the Robust Android Malware Detection competition at the 2025
IEEE Conference SaTML, DeepTrust secured the first place and achieved
state-of-the-art results, outperforming the next-best competitor by up to 266%
under feature-space evasion attacks. This is accomplished while maintaining the
highest detection rate on non-adversarial malware and a false positive rate
below 1%. The method's efficacy stems from maximizing the divergence of the
learned representations among the internal models. By using classifiers
inducing fundamentally dissimilar embeddings of the data, the decision space
becomes unpredictable for an attacker. This frustrates the iterative
perturbation process inherent to evasion attacks, enhancing system robustness
without compromising accuracy on clean examples.

</details>


### [36] [IP-Augmented Multi-Modal Malicious URL Detection Via Token-Contrastive Representation Enhancement and Multi-Granularity Fusion](https://arxiv.org/abs/2510.12395)
*Ye Tian,Yanqiu Yu,Liangliang Song,Zhiquan Liu,Yanbin Wang,Jianguo Sun*

Main category: cs.CR

TL;DR: 提出CURL-IP框架，通过多模态方法解决恶意URL检测中的三个关键挑战：URL层次结构建模、字符级混淆检测和网络信号集成，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有预训练语言模型在URL检测中存在三个局限：无法有效建模URL的非自然层次结构、对字符级混淆不敏感、缺乏集成网络级信号（如IP地址）的机制，这些对鲁棒检测至关重要。

Method: 提出CURL-IP多模态检测框架，包含三个关键创新：1) 基于对比学习的令牌表示增强器；2) 跨层多尺度聚合器，通过卷积和门控MLP分层聚合Transformer输出；3) 分块多模态耦合器，在块级别计算跨模态注意力权重。

Result: 在大规模真实数据集上的评估表明，该框架在二元和多类分类任务中显著优于最先进的基线方法。

Conclusion: CURL-IP框架能够同时保留细粒度词汇线索、上下文语义，并集成网络级信号，为恶意URL检测提供了有效的解决方案。

Abstract: Malicious URL detection remains a critical cybersecurity challenge as
adversaries increasingly employ sophisticated evasion techniques including
obfuscation, character-level perturbations, and adversarial attacks. Although
pre-trained language models (PLMs) like BERT have shown potential for URL
analysis tasks, three limitations persist in current implementations: (1)
inability to effectively model the non-natural hierarchical structure of URLs,
(2) insufficient sensitivity to character-level obfuscation, and (3) lack of
mechanisms to incorporate auxiliary network-level signals such as IP
addresses-all essential for robust detection. To address these challenges, we
propose CURL-IP, an advanced multi-modal detection framework incorporating
three key innovations: (1) Token-Contrastive Representation Enhancer, which
enhances subword token representations through token-aware contrastive learning
to produce more discriminative and isotropic embeddings; (2) Cross-Layer
Multi-Scale Aggregator, employing hierarchical aggregation of Transformer
outputs via convolutional operations and gated MLPs to capture both local and
global semantic patterns across layers; and (3) Blockwise Multi-Modal Coupler
that decomposes URL-IP features into localized block units and computes
cross-modal attention weights at the block level, enabling fine-grained
inter-modal interaction. This architecture enables simultaneous preservation of
fine-grained lexical cues, contextual semantics, and integration of
network-level signals. Our evaluation on large-scale real-world datasets shows
the framework significantly outperforms state-of-the-art baselines across
binary and multi-class classification tasks.

</details>


### [37] [Targeted Pooled Latent-Space Steganalysis Applied to Generative Steganography, with a Fix](https://arxiv.org/abs/2510.12414)
*Etienne Levecque,Aurélien Noirault,Tomáš Pevný,Jan Butora,Patrick Bas,Rémi Cogranne*

Main category: cs.CR

TL;DR: 该论文提出在潜在空间进行隐写分析，通过建模潜在向量范数的统计分布来检测隐写嵌入。研究发现隐写向量分布在超球面上，而原始向量是独立同分布的高斯分布，从而可以基于似然比检验进行检测。


<details>
  <summary>Details</summary>
Motivation: 现有的隐写方案在潜在空间中修改种子向量嵌入信息，而大多数隐写分析方法试图在图像空间中检测嵌入。这种空间不匹配导致检测困难，因此需要在潜在空间进行隐写分析。

Method: 分析隐写向量和原始向量在潜在空间中的统计分布差异，将向量范数建模为具有不同方差的高斯分布，并推导似然比检验进行隐写分析。

Result: 研究发现隐写向量分布在超球面上，而原始向量是i.i.d.高斯分布。通过潜在空间分析可以有效检测Hu等人提出的隐写方案，该方案在图像空间中几乎无法检测。

Conclusion: 在潜在空间进行隐写分析比图像空间更有效，通过建模向量范数分布可以实现准确检测。同时，通过随机采样潜在向量范数可以使隐写方案在潜在空间中无法检测。

Abstract: Steganographic schemes dedicated to generated images modify the seed vector
in the latent space to embed a message, whereas most steganalysis methods
attempt to detect the embedding in the image space. This paper proposes to
perform steganalysis in the latent space by modeling the statistical
distribution of the norm of the latent vector. Specifically, we analyze the
practical security of a scheme proposed by Hu et. al. for latent diffusion
models, which is both robust and practically undetectable when steganalysis is
performed on generated images. We show that after embedding, the Stego (latent)
vector is distributed on a hypersphere while the Cover vector is i.i.d.
Gaussian. By going from the image space to the latent space, we show that it is
possible to model the norm of the vector in the latent space under the Cover or
Stego hypothesis as Gaussian distributions with different variances. A
Likelihood Ratio Test is then derived to perform pooled steganalysis. The
impact of the potential knowledge of the prompt and the number of diffusion
steps, is also studied. Additionally, we also show how, by randomly sampling
the norm of the latent vector before generation, the initial Stego scheme
becomes undetectable in the latent space.

</details>


### [38] [Formal Models and Convergence Analysis for Context-Aware Security Verification](https://arxiv.org/abs/2510.12440)
*Ayush Chaudhary*

Main category: cs.CR

TL;DR: 提出了一个上下文感知安全验证的正式框架，为ML增强的自适应系统建立可证明的安全保证。引入了上下文完备性这一新安全属性，并证明了样本复杂度界限、信息论限制、ML负载生成器收敛保证和组合健全性界限。


<details>
  <summary>Details</summary>
Motivation: 传统静态验证器在有限负载预算下最多只能达到α的完备性，而上下文感知验证器在足够信息下可以超过α的完备性，需要建立理论框架来证明自适应验证的优势。

Method: 引入上下文完备性安全属性，建立形式化框架，证明样本复杂度界限、信息论限制、ML负载生成器收敛保证和组合健全性界限。通过97,224个漏洞样本的受控实验验证理论预测。

Result: 检测准确率从58%提升到69.93%，成功率从51%提升到82%，训练损失以O(1/√T)速率收敛，误报率10.19%在理论界限12%内。

Conclusion: 理论基础的适应性验证在既定假设下相比静态方法实现了可证明的改进，同时保持了健全性保证。

Abstract: We present a formal framework for context-aware security verification that
establishes provable guarantees for ML-enhanced adaptive systems. We introduce
context-completeness - a new security property - and prove: (1) sample
complexity bounds showing when adaptive verification succeeds, (2)
information-theoretic limits relating context richness to detection capability,
(3) convergence guarantees for ML-based payload generators, and (4)
compositional soundness bounds. We further provide a formal separation between
static context-blind verifiers and context-aware adaptive verifiers: for a
natural family of targets, any static verifier with finite payload budget
achieves completeness at most alpha, while a context-aware verifier with
sufficient information achieves completeness greater than alpha. We validate
our theoretical predictions through controlled experiments on 97,224 exploit
samples, demonstrating: detection accuracy improving from 58% to 69.93% with
dataset growth, success probability increasing from 51% to 82% with context
enrichment, training loss converging at O(1/sqrt(T)) rate, and false positive
rate (10.19%) within theoretical bounds (12%). Our results show that
theoretically-grounded adaptive verification achieves provable improvements
over static approaches under stated assumptions while maintaining soundness
guarantees.

</details>


### [39] [Attack-Specialized Deep Learning with Ensemble Fusion for Network Anomaly Detection](https://arxiv.org/abs/2510.12455)
*Nisith Dissanayake,Uthayasanker Thayasivam*

Main category: cs.CR

TL;DR: 提出了一种混合异常检测框架，结合专门化深度学习模型和集成元分类器，有效解决网络入侵检测中的类别不平衡问题。


<details>
  <summary>Details</summary>
Motivation: 传统入侵检测系统在处理不平衡数据集时，对少数类攻击的检测准确率较低，导致假阴性增加。需要一种能够同时有效检测频繁和罕见攻击类型的方法。

Method: 使用专门化深度学习模型分别检测不同攻击类别，学习类别特定模式，然后通过随机森林元分类器融合各模型输出，提高决策可靠性。

Result: 在NSL-KDD基准测试中表现优异，相比传统单一模型，在所有攻击类别（包括罕见的U2R攻击）的精确率、召回率和F1分数均有显著提升，实现了近乎完美的检测率和最低误报率。

Conclusion: 该框架通过结合专门化学习和集成学习，为现代网络安全提供了一种有效且可扩展的入侵检测解决方案。

Abstract: The growing scale and sophistication of cyberattacks pose critical challenges
to network security, particularly in detecting diverse intrusion types within
imbalanced datasets. Traditional intrusion detection systems (IDS) often
struggle to maintain high accuracy across both frequent and rare attacks,
leading to increased false negatives for minority classes. To address this, we
propose a hybrid anomaly detection framework that integrates specialized deep
learning models with an ensemble meta-classifier. Each model is trained to
detect a specific attack category, enabling tailored learning of class-specific
patterns, while their collective outputs are fused by a Random Forest
meta-classifier to improve overall decision reliability. The framework is
evaluated on the NSL-KDD benchmark, demonstrating superior performance in
handling class imbalance compared to conventional monolithic models. Results
show significant improvements in precision, recall, and F1-score across all
attack categories, including rare classes such as User to Root (U2R). The
proposed system achieves near-perfect detection rates with minimal false
alarms, highlighting its robustness and generalizability. This work advances
the design of intrusion detection systems by combining specialization with
ensemble learning, providing an effective and scalable solution for
safeguarding modern networks.

</details>


### [40] [Proof of Cloud: Data Center Execution Assurance for Confidential VMs](https://arxiv.org/abs/2510.12469)
*Filip Rezabek,Moe Mahhouk,Andrew Miller,Stefan Genchev,Quintus Kilbourn,Georg Carle,Jonathan Passerat-Palmbach*

Main category: cs.CR

TL;DR: DCEA通过vTPM锚定测量将CVM绑定到物理平台，生成"云证明"，使远程验证能够确认CVM在可信物理硬件上运行，弥补了现有远程证明无法验证物理平台可信度的缺陷。


<details>
  <summary>Details</summary>
Motivation: 当前机密虚拟机(CVMs)虽然通过硬件隔离保护数据，但无法验证是否运行在物理可信的平台上。TEE威胁模型排除了物理访问攻击者，但远程证明无法确认CVM实际运行的物理硬件是否可信，导致CVM部署无法证明其安全保证符合TEE厂商的威胁模型。

Method: 利用数据中心通常可通过TPM识别的特点，通过vTPM锚定测量将CVM绑定到物理机箱，确保CVM启动证据和TPM引用指向同一物理平台。该方法适用于访问vTPM的CVM和单租户裸机部署。

Result: 在Google Cloud和Intel TDX上实现了候选方案，利用Intel TXT进行可信启动。DCEA能够远程验证CVM的平台来源和完整性，减轻重放和证明代理等攻击。

Conclusion: DCEA完善了CVM的威胁模型，为在最小信任环境中部署高保证机密工作负载提供了实用路径，使租户能够验证其CVM确实运行在物理可信的硬件平台上。

Abstract: Confidential Virtual Machines (CVMs) protect data in use by running workloads
inside hardware-isolated environments. In doing so, they also inherit the
limitations of the underlying hardware. Trusted Execution Environments (TEEs),
which enforce this isolation, explicitly exclude adversaries with physical
access from their threat model. Commercial TEEs, e.g., Intel TDX, thus assume
infrastructure providers do not physically exploit hardware and serve as
safeguards instead. This creates a tension: tenants must trust provider
integrity at the hardware layer, yet existing remote attestation offers no way
to verify that CVMs actually run on physically trusted platforms, leaving
today's CVM deployments unable to demonstrate that their guarantees align with
the TEE vendor's threat model.
  We bridge this confidence gap with Data Center Execution Assurance (DCEA), a
design generating "Proofs of Cloud". DCEA binds a CVM to its underlying
platform using vTPM-anchored measurements, ensuring CVM launch evidence and TPM
quotes refer to the same physical chassis.
  This takes advantage of the fact that data centers are often identifiable via
TPMs. Our approach applies to CVMs accessing vTPMs and running on top of
software stacks fully controlled by the cloud provider, as well as
single-tenant bare-metal deployments with discrete TPMs. We trust providers for
integrity (certificate issuance), but not for the confidentiality of
CVM-visible state. DCEA enables remote verification of a CVM's platform origin
and integrity, mitigating attacks like replay and attestation proxying. We
include a candidate implementation on Google Cloud and Intel TDX that leverages
Intel TXT for trusted launch. Our design refines CVMs' threat model and
provides a practical path for deploying high-assurance, confidential workloads
in minimally trusted environments.

</details>


### [41] [Noisy Neighbor: Exploiting RDMA for Resource Exhaustion Attacks in Containerized Clouds](https://arxiv.org/abs/2510.12629)
*Gunwoo Kim,Taejune Park,Jinwoo Kim*

Main category: cs.CR

TL;DR: 本文分析了RDMA网络接口卡中的资源耗尽攻击，提出了HT-Verbs框架来缓解这些威胁并恢复可预测的安全保证。


<details>
  <summary>Details</summary>
Motivation: 在现代容器化云环境中，RDMA的采用虽然减少了CPU开销并实现了高性能数据交换，但需要强大的性能隔离来确保一个容器的RDMA工作负载不会降低其他容器的性能。然而，由于RDMA NIC中微架构资源管理的复杂性，现有的隔离技术难以有效应用。

Method: 本文实验分析了NVIDIA BlueField-3上的两种资源耗尽攻击：状态饱和攻击和流水线饱和攻击。为了缓解这些威胁，提出了HT-Verbs框架，这是一个基于实时每个容器RDMA动词遥测和自适应资源分类的阈值驱动框架，将RNIC资源划分为热、温、冷层级，并在不需要硬件修改的情况下限制滥用工作负载。

Result: 结果显示，状态饱和攻击可导致受害者容器带宽损失高达93.9%，延迟增加1,117倍，缓存未命中率上升115%。流水线饱和攻击导致严重的链路级拥塞和显著放大效应，其中小的动词请求会导致不成比例的高资源消耗。

Conclusion: HT-Verbs框架能够有效缓解RDMA资源耗尽攻击，恢复可预测的安全保证，而无需硬件修改。

Abstract: In modern containerized cloud environments, the adoption of RDMA (Remote
Direct Memory Access) has expanded to reduce CPU overhead and enable
high-performance data exchange. Achieving this requires strong performance
isolation to ensure that one container's RDMA workload does not degrade the
performance of others, thereby maintaining critical security assurances.
However, existing isolation techniques are difficult to apply effectively due
to the complexity of microarchitectural resource management within RDMA NICs
(RNICs). This paper experimentally analyzes two types of resource exhaustion
attacks on NVIDIA BlueField-3: (i) state saturation attacks and (ii) pipeline
saturation attacks. Our results show that state saturation attacks can cause up
to a 93.9% loss in bandwidth, a 1,117x increase in latency, and a 115% rise in
cache misses for victim containers, while pipeline saturation attacks lead to
severe link-level congestion and significant amplification, where small verb
requests result in disproportionately high resource consumption. To mitigate
these threats and restore predictable security assurances, we propose HT-Verbs,
a threshold-driven framework based on real-time per-container RDMA verb
telemetry and adaptive resource classification that partitions RNIC resources
into hot, warm, and cold tiers and throttles abusive workloads without
requiring hardware modifications.

</details>


### [42] [PromoGuardian: Detecting Promotion Abuse Fraud with Multi-Relation Fused Graph Neural Networks](https://arxiv.org/abs/2510.12652)
*Shaofei Li,Xiao Han,Ziqi Zhang,Minyao Hua,Shuli Gao,Zhenkai Liang,Yao Guo,Xiangqun Chen,Ding Li*

Main category: cs.CR

TL;DR: 该论文提出了PROMOGUARDIAN模型，通过融合时空信息的多关系图神经网络来检测电商平台中的促销滥用欺诈，在美团真实数据上取得了优于现有方法的效果。


<details>
  <summary>Details</summary>
Motivation: 随着电商平台发展，促销滥用欺诈快速增长，对平台安全构成威胁。这类欺诈涉及普通用户进行合法交易，且囤货和返现滥用两种类型往往交织在一起，传统检测方法难以应对。

Method: 提出PROMOGUARDIAN模型，这是一种新颖的多关系融合图神经网络，将交易数据的空间和时间信息整合到同质图中来检测促销滥用欺诈。

Result: 在美团真实数据上的实验表明，该模型在促销滥用欺诈检测中优于最先进方法，达到93.15%的精确度，检测到的欺诈者数量是其他方法的2.1到5.0倍，在生产环境中防止的财务损失是其他方法的1.5到8.8倍。

Conclusion: PROMOGUARDIAN模型通过有效整合时空信息，能够显著提升电商平台促销滥用欺诈的检测效果，为平台安全提供了有力保障。

Abstract: As e-commerce platforms develop, fraudulent activities are increasingly
emerging, posing significant threats to the security and stability of these
platforms. Promotion abuse is one of the fastest-growing types of fraud in
recent years and is characterized by users exploiting promotional activities to
gain financial benefits from the platform. To investigate this issue, we
conduct the first study on promotion abuse fraud in e-commerce platforms
MEITUAN. We find that promotion abuse fraud is a group-based fraudulent
activity with two types of fraudulent activities: Stocking Up and Cashback
Abuse. Unlike traditional fraudulent activities such as fake reviews, promotion
abuse fraud typically involves ordinary customers conducting legitimate
transactions and these two types of fraudulent activities are often
intertwined. To address this issue, we propose leveraging additional
information from the spatial and temporal perspectives to detect promotion
abuse fraud. In this paper, we introduce PROMOGUARDIAN, a novel multi-relation
fused graph neural network that integrates the spatial and temporal information
of transaction data into a homogeneous graph to detect promotion abuse fraud.
We conduct extensive experiments on real-world data from MEITUAN, and the
results demonstrate that our proposed model outperforms state-of-the-art
methods in promotion abuse fraud detection, achieving 93.15% precision,
detecting 2.1 to 5.0 times more fraudsters, and preventing 1.5 to 8.8 times
more financial losses in production environments.

</details>


### [43] [Hash chaining degrades security at Facebook](https://arxiv.org/abs/2510.12665)
*Thomas Rivasseau*

Main category: cs.CR

TL;DR: 本文首次展示了Facebook密码存储方案的安全漏洞，并讨论了其影响。


<details>
  <summary>Details</summary>
Motivation: 现代网络和数字应用依赖密码哈希进行存储和安全保护，但临时升级密码存储方案以节省成本可能会引入未预见的漏洞。Meta Platforms使用的密码存储方案服务数十亿月活跃用户，存在安全隐患。

Method: 通过展示针对Facebook密码存储方案的漏洞利用，证明其安全弱点。遵循了适当的道德披露准则和供应商通知流程。

Result: 成功演示了Facebook密码存储方案的安全漏洞，这是首个公开的此类漏洞利用示例。

Conclusion: 临时升级密码存储方案可能带来安全风险，需要更严谨的安全设计和评估。

Abstract: Modern web and digital application password storage relies on password
hashing for storage and security. Ad-hoc upgrade of password storage to keep up
with hash algorithm norms may be used to save costs but can introduce
unforeseen vulnerabilities. This is the case in the password storage scheme
used by Meta Platforms which services several billion monthly users worldwide.
In this paper we present the first example of an exploit which demonstrates the
security weakness of Facebook's password storage scheme, and discuss its
implications. Proper ethical disclosure guidelines and vendor notification were
followed.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [44] [AI Agents for the Dhumbal Card Game: A Comparative Study](https://arxiv.org/abs/2510.11736)
*Sahaj Raj Malla*

Main category: cs.AI

TL;DR: 本文系统评估了AI代理在Dhumbal文化纸牌游戏中的表现，比较了基于规则、搜索和学习的策略，发现基于规则的攻击性代理在胜率上显著优于其他方法。


<details>
  <summary>Details</summary>
Motivation: 评估AI代理在不完美信息文化纸牌游戏Dhumbal中的表现，为AI研究提供可复现框架，并支持文化游戏的数字保存。

Method: 实现多种AI策略：基于规则的启发式方法（攻击性、保守性、平衡性、机会主义）、基于搜索的方法（MCTS、ISMCTS）和强化学习方法（DQN、PPO），通过1024轮模拟比赛进行评估。

Result: 基于规则的攻击性代理获得最高胜率（88.3%），显著优于ISMCTS（9.0%）和PPO（1.5%），通过有效利用Jhyap声明获得优势。

Conclusion: 研究贡献了可复现的AI框架，揭示了在不完美信息下启发式方法的有效性，并提供了开源代码，推动了AI研究和文化游戏的数字保存。

Abstract: This study evaluates Artificial Intelligence (AI) agents for Dhumbal, a
culturally significant multiplayer card game with imperfect information,
through a systematic comparison of rule-based, search-based, and learning-based
strategies. We formalize Dhumbal's mechanics and implement diverse agents,
including heuristic approaches (Aggressive, Conservative, Balanced,
Opportunistic), search-based methods such as Monte Carlo Tree Search (MCTS) and
Information Set Monte Carlo Tree Search (ISMCTS), and reinforcement learning
approaches including Deep Q-Network (DQN) and Proximal Policy Optimization
(PPO), and a random baseline. Evaluation involves within-category tournaments
followed by a cross-category championship. Performance is measured via win
rate, economic outcome, Jhyap success, cards discarded per round, risk
assessment, and decision efficiency. Statistical significance is assessed using
Welch's t-test with Bonferroni correction, effect sizes via Cohen's d, and 95%
confidence intervals (CI). Across 1024 simulated rounds, the rule-based
Aggressive agent achieves the highest win rate (88.3%, 95% CI: [86.3, 90.3]),
outperforming ISMCTS (9.0%) and PPO (1.5%) through effective exploitation of
Jhyap declarations. The study contributes a reproducible AI framework, insights
into heuristic efficacy under partial information, and open-source code,
thereby advancing AI research and supporting digital preservation of cultural
games.

</details>


### [45] [Beyond Consensus: Mitigating the Agreeableness Bias in LLM Judge Evaluations](https://arxiv.org/abs/2510.11822)
*Suryaansh Jain,Umair Z. Ahmed,Shubham Sahai,Ben Leong*

Main category: cs.AI

TL;DR: LLM作为评估器存在严重正向偏差，本文提出少数否决策略和基于回归的框架来缓解这一问题，在代码反馈任务上显著提升了评估准确性。


<details>
  <summary>Details</summary>
Motivation: 现有LLM评估器存在强烈的正向偏差，虽然能准确识别有效输出，但对无效输出的识别能力很差，导致可靠性评分虚高。

Method: 提出少数否决策略来缓解偏差，并引入基于回归的框架直接建模验证器偏差，使用少量人工标注数据进行校正。

Result: 在366个高中Python程序的代码反馈任务中，回归方法将最大绝对误差降至1.2%，比14个最先进LLM的最佳集成效果提升2倍。

Conclusion: 本文提出的方法能有效解决LLM评估器的正向偏差问题，显著提高评估准确性，为模型选择提供更可靠的依据。

Abstract: New Large Language Models (LLMs) become available every few weeks, and modern
application developers confronted with the unenviable task of having to decide
if they should switch to a new model. While human evaluation remains the gold
standard, it is costly and unscalable. The state-of-the-art approach is to use
LLMs as evaluators ( LLM-as-a-judge), but this suffers from a critical flaw:
LLMs exhibit a strong positive bias. We provide empirical evidence showing that
while LLMs can identify valid outputs with high accuracy (i.e., True Positive
Rate 96%), they are remarkably poor at identifying invalid ones (i.e., True
Negative Rate <25%). This systematic bias, coupled with class imbalance, often
leads to inflated reliability scores.
  While ensemble-based methods like majority voting can help, we show that they
are not good enough. We introduce an optimal minority-veto strategy that is
resilient to missing data and mitigates this bias to a large extent. For
scenarios requiring even higher precision, we propose a novel regression-based
framework that directly models the validator bias using a small set of
human-annotated ground truth data. On a challenging code feedback task over 366
high-school Python programs, our regression approach reduces the maximum
absolute error to just 1.2%, achieving a 2x improvement over the
best-performing ensemble of 14 state-of-the-art LLMs.

</details>


### [46] [Holistic Agent Leaderboard: The Missing Infrastructure for AI Agent Evaluation](https://arxiv.org/abs/2510.11977)
*Sayash Kapoor,Benedikt Stroebl,Peter Kirgis,Nitya Nadgir,Zachary S Siegel,Boyi Wei,Tianci Xue,Ziru Chen,Felix Chen,Saiteja Utpala,Franck Ndzomga,Dheeraj Oruganty,Sophie Luskin,Kangheng Liu,Botao Yu,Amit Arora,Dongyoon Hahm,Harsh Trivedi,Huan Sun,Juyong Lee,Tengjun Jin,Yifan Mai,Yifei Zhou,Yuxuan Zhu,Rishi Bommasani,Daniel Kang,Dawn Song,Peter Henderson,Yu Su,Percy Liang,Arvind Narayanan*

Main category: cs.AI

TL;DR: 提出了HAL（整体智能体排行榜）来解决AI智能体评估中的挑战，包括标准化评估框架、三维分析和LLM辅助日志检查，旨在从基准测试转向真实世界的可靠智能体。


<details>
  <summary>Details</summary>
Motivation: AI智能体评估存在诸多挑战，影响了对智能体真实性能的理解，需要更全面和可靠的评估方法。

Method: 开发标准化评估框架，在数百个虚拟机上进行并行评估；进行模型、支架和基准的三维分析；使用LLM辅助日志检查来发现未报告的行为。

Result: 完成了21,730次智能体运行，覆盖9个模型和9个基准，成本约4万美元。发现了一些反直觉的发现，如更高推理努力反而降低准确性，以及智能体在任务中的异常行为。

Conclusion: 通过标准化评估方法和解决常见评估陷阱，希望推动研究从基准测试表现转向真实世界可靠性，并分享了25亿token的智能体日志以促进进一步研究。

Abstract: AI agents have been developed for complex real-world tasks from coding to
customer service. But AI agent evaluations suffer from many challenges that
undermine our understanding of how well agents really work. We introduce the
Holistic Agent Leaderboard (HAL) to address these challenges. We make three
main contributions. First, we provide a standardized evaluation harness that
orchestrates parallel evaluations across hundreds of VMs, reducing evaluation
time from weeks to hours while eliminating common implementation bugs. Second,
we conduct three-dimensional analysis spanning models, scaffolds, and
benchmarks. We validate the harness by conducting 21,730 agent rollouts across
9 models and 9 benchmarks in coding, web navigation, science, and customer
service with a total cost of about $40,000. Our analysis reveals surprising
insights, such as higher reasoning effort reducing accuracy in the majority of
runs. Third, we use LLM-aided log inspection to uncover previously unreported
behaviors, such as searching for the benchmark on HuggingFace instead of
solving a task, or misusing credit cards in flight booking tasks. We share all
agent logs, comprising 2.5B tokens of language model calls, to incentivize
further research into agent behavior. By standardizing how the field evaluates
agents and addressing common pitfalls in agent evaluation, we hope to shift the
focus from agents that ace benchmarks to agents that work reliably in the real
world.

</details>


### [47] [CGBench: Benchmarking Language Model Scientific Reasoning for Clinical Genetics Research](https://arxiv.org/abs/2510.11985)
*Owen Queen,Harrison G. Zhang,James Zou*

Main category: cs.AI

TL;DR: CGBench是一个用于评估语言模型在科学文献解释能力的基准测试，基于ClinGen专家整理的临床遗传学文献构建，测试模型提取实验结果、判断证据强度、分类实验成果的能力。


<details>
  <summary>Details</summary>
Motivation: 传统基因变异解释方法耗时费力，生成式语言模型可加速这一过程，但现有基准测试任务过于狭窄，无法反映真实研究需求。

Method: 基于ClinGen专家整理的临床遗传学文献构建CGBench基准，测试8种不同语言模型在提取实验结果、判断证据强度、分类实验成果三个方面的能力。

Result: 模型在文献解释方面表现出潜力但存在显著差距，推理模型在细粒度任务上表现更好，非推理模型在高层次解释上更优。模型即使正确分类证据也常产生幻觉或误解结果。

Conclusion: CGBench揭示了语言模型在科学文献精确解释方面的优势和局限，为AI在临床遗传学和更广泛科学领域的未来研究开辟了道路。

Abstract: Variant and gene interpretation are fundamental to personalized medicine and
translational biomedicine. However, traditional approaches are manual and
labor-intensive. Generative language models (LMs) can facilitate this process,
accelerating the translation of fundamental research into clinically-actionable
insights. While existing benchmarks have attempted to quantify the capabilities
of LMs for interpreting scientific data, these studies focus on narrow tasks
that do not translate to real-world research. To meet these challenges, we
introduce CGBench, a robust benchmark that tests reasoning capabilities of LMs
on scientific publications. CGBench is built from ClinGen, a resource of
expert-curated literature interpretations in clinical genetics. CGBench
measures the ability to 1) extract relevant experimental results following
precise protocols and guidelines, 2) judge the strength of evidence, and 3)
categorize and describe the relevant outcome of experiments. We test 8
different LMs and find that while models show promise, substantial gaps exist
in literature interpretation, especially on fine-grained instructions.
Reasoning models excel in fine-grained tasks but non-reasoning models are
better at high-level interpretations. Finally, we measure LM explanations
against human explanations with an LM judge approach, revealing that models
often hallucinate or misinterpret results even when correctly classifying
evidence. CGBench reveals strengths and weaknesses of LMs for precise
interpretation of scientific publications, opening avenues for future research
in AI for clinical genetics and science more broadly.

</details>


### [48] [Asking Clarifying Questions for Preference Elicitation With Large Language Models](https://arxiv.org/abs/2510.12015)
*Ali Montazeralghaem,Guy Tennenholtz,Craig Boutilier,Ofer Meshi*

Main category: cs.AI

TL;DR: 提出了一种训练LLMs生成顺序澄清问题以揭示用户偏好的新方法，采用受扩散模型启发的两阶段过程，显著提升了LLM提问能力和用户偏好获取效果。


<details>
  <summary>Details</summary>
Motivation: 在用户历史有限的情况下，个性化LLM响应需要有效获取用户偏好，而生成跨领域的顺序澄清问题仍具挑战。

Method: 采用受扩散模型启发的两阶段过程：前向过程从用户档案生成澄清问题并逐步移除答案作为'噪声'；反向过程训练模型通过提问来'去噪'用户档案。

Result: 该方法显著提高了LLM提出漏斗式问题的熟练度和有效获取用户偏好的能力。

Conclusion: 提出的两阶段训练方法能有效提升LLM生成顺序澄清问题的能力，从而更好地获取用户偏好以实现个性化推荐。

Abstract: Large Language Models (LLMs) have made it possible for recommendation systems
to interact with users in open-ended conversational interfaces. In order to
personalize LLM responses, it is crucial to elicit user preferences, especially
when there is limited user history. One way to get more information is to
present clarifying questions to the user. However, generating effective
sequential clarifying questions across various domains remains a challenge. To
address this, we introduce a novel approach for training LLMs to ask sequential
questions that reveal user preferences. Our method follows a two-stage process
inspired by diffusion models. Starting from a user profile, the forward process
generates clarifying questions to obtain answers and then removes those answers
step by step, serving as a way to add ``noise'' to the user profile. The
reverse process involves training a model to ``denoise'' the user profile by
learning to ask effective clarifying questions. Our results show that our
method significantly improves the LLM's proficiency in asking funnel questions
and eliciting user preferences effectively.

</details>


### [49] [CausalTrace: A Neurosymbolic Causal Analysis Agent for Smart Manufacturing](https://arxiv.org/abs/2510.12033)
*Chathurangi Shyalika,Aryaman Sharma,Fadi El Kalach,Utkarshani Jaimini,Cory Henson,Ramy Harik,Amit Sheth*

Main category: cs.AI

TL;DR: CausalTrace是一个集成到工业CoPilot中的神经符号因果分析模块，通过数据驱动因果分析和工业知识图谱，提供因果发现、反事实推理和根因分析功能，在火箭组装测试中表现出高准确性和可靠性。


<details>
  <summary>Details</summary>
Motivation: 现代制造环境需要可解释的异常预测和干预建议，现有AI系统作为孤立黑箱缺乏预测、解释和因果推理的集成，限制了在高风险工业环境中的可信度和实用性。

Method: CausalTrace是一个神经符号因果分析模块，集成到SmartPilot工业CoPilot中，通过数据驱动因果分析结合工业本体和知识图谱，支持因果发现、反事实推理和根因分析，提供实时操作员交互和透明决策支持。

Result: 在学术火箭组装测试中，CausalTrace与领域专家达成高度一致（ROUGE-1：0.91），根因分析性能优异（MAP@3：94%，PR@2：97%，MRR：0.92，Jaccard：0.92），C3AN评估得分4.59/5，展示了实时部署的精确性和可靠性。

Conclusion: CausalTrace成功地将神经符号因果分析集成到工业CoPilot中，提供了透明、可解释的决策支持，在高风险工业环境中表现出强大的性能，为实际部署做好了准备。

Abstract: Modern manufacturing environments demand not only accurate predictions but
also interpretable insights to process anomalies, root causes, and potential
interventions. Existing AI systems often function as isolated black boxes,
lacking the seamless integration of prediction, explanation, and causal
reasoning required for a unified decision-support solution. This fragmentation
limits their trustworthiness and practical utility in high-stakes industrial
environments. In this work, we present CausalTrace, a neurosymbolic causal
analysis module integrated into the SmartPilot industrial CoPilot. CausalTrace
performs data-driven causal analysis enriched by industrial ontologies and
knowledge graphs, including advanced functions such as causal discovery,
counterfactual reasoning, and root cause analysis (RCA). It supports real-time
operator interaction and is designed to complement existing agents by offering
transparent, explainable decision support. We conducted a comprehensive
evaluation of CausalTrace using multiple causal assessment methods and the C3AN
framework (i.e. Custom, Compact, Composite AI with Neurosymbolic Integration),
which spans principles of robustness, intelligence, and trustworthiness. In an
academic rocket assembly testbed, CausalTrace achieved substantial agreement
with domain experts (ROUGE-1: 0.91 in ontology QA) and strong RCA performance
(MAP@3: 94%, PR@2: 97%, MRR: 0.92, Jaccard: 0.92). It also attained 4.59/5 in
the C3AN evaluation, demonstrating precision and reliability for live
deployment.

</details>


### [50] [Do Large Language Models Respect Contracts? Evaluating and Enforcing Contract-Adherence in Code Generation](https://arxiv.org/abs/2510.12047)
*Soohan Lim,Joonghyuk Hahn,Hyunwoo Park,Sang-Ki Ko,Yo-Sub Han*

Main category: cs.AI

TL;DR: PACT是一个评估LLM生成代码合约遵从性的框架，通过扩展HumanEval+和MBPP+基准，引入合约违反测试用例，显著提升模型对合约的遵从能力。


<details>
  <summary>Details</summary>
Motivation: 现有代码生成基准主要评估功能正确性，但忽略了真实软件中合约遵从性这一关键方面，导致无法衡量模型生成代码的鲁棒性和可靠性。

Method: 构建专注于合约违反的测试套件语料库，扩展现有基准；通过不同提示条件系统分析代码生成；引入合约遵从性的新评估指标。

Result: 在提示中添加合约违反测试用例相比仅使用合约描述，能显著提升模型对合约的遵从能力；揭示了传统基准忽略的关键错误。

Conclusion: PACT提供了严格且可解释的指标，能够全面评估LLM生成代码在功能性和合约遵从性方面的鲁棒性。

Abstract: Prevailing code generation benchmarks, such as HumanEval+ and MBPP+,
primarily evaluate large language models (LLMs) with pass@k on functional
correctness using well-formed inputs. However, they ignore a crucial aspect of
real-world software: adherence to contracts-the preconditions and validity
constraints that dictate how ill-formed inputs must be rejected. This critical
oversight means that existing benchmarks fail to measure, and models
consequently fail to generate, truly robust and reliable code snippets. We
introduce PACT, a program assessment and contract-adherence evaluation
framework, to bridge this gap. PACT is the first framework designed to
systematically evaluate and enhance contract-adherence in LLM-generated code
snippets alongside functional correctness. PACT's contributions are threefold:
First, it provides a comprehensive test-suite corpus focused on contract
violations, extending HumanEval+ and MBPP+. Second, it enables a systematic
analysis of code generation under varied prompting conditions. This analysis
demonstrates that augmenting prompts with contract-violating test cases
significantly enhance a model's ability to respect contracts compared to using
contract description alone. Finally, it introduces novel metrics to rigorously
quantify contract adherence in both test generation and code generation. By
revealing critical errors that conventional benchmarks overlook, PACT provides
the rigorous and interpretable metrics to evaluate the robustness of
LLM-generated code snippets in both functionality and contract-adherence.Our
code and data are available at https://github.com/suhanmen/PACT.

</details>


### [51] [Empowering LLM Agents with Geospatial Awareness: Toward Grounded Reasoning for Wildfire Response](https://arxiv.org/abs/2510.12061)
*Yiheng Chen,Lingyao Li,Zihui Ma,Qikai Hu,Yilun Zhu,Min Deng,Runlong Yu*

Main category: cs.AI

TL;DR: 提出了一个地理空间感知层(GAL)，将LLM智能体与结构化地球数据相结合，通过整合基础设施、人口、地形和天气信息，为灾害响应提供基于证据的资源分配建议。


<details>
  <summary>Details</summary>
Motivation: 现有统计方法缺乏语义上下文，跨事件泛化能力差，可解释性有限。LLM虽然具有少样本泛化能力，但受限于文本且对地理信息不敏感。

Method: 开发地理空间感知层(GAL)，从原始野火检测开始，自动从外部地理数据库检索并整合基础设施、人口、地形和天气信息，生成带有单位注释的感知脚本。

Result: 在真实野火场景中评估，地理空间接地的智能体表现优于基线方法。该框架可泛化到洪水、飓风等其他灾害类型。

Conclusion: GAL框架成功将LLM智能体与地理空间数据结合，提高了灾害响应的准确性和可解释性，具有很好的泛化能力。

Abstract: Effective disaster response is essential for safeguarding lives and property.
Existing statistical approaches often lack semantic context, generalize poorly
across events, and offer limited interpretability. While Large language models
(LLMs) provide few-shot generalization, they remain text-bound and blind to
geography. To bridge this gap, we introduce a Geospatial Awareness Layer (GAL)
that grounds LLM agents in structured earth data. Starting from raw wildfire
detections, GAL automatically retrieves and integrates infrastructure,
demographic, terrain, and weather information from external geodatabases,
assembling them into a concise, unit-annotated perception script. This enriched
context enables agents to produce evidence-based resource-allocation
recommendations (e.g., personnel assignments, budget allocations), further
reinforced by historical analogs and daily change signals for incremental
updates. We evaluate the framework in real wildfire scenarios across multiple
LLM models, showing that geospatially grounded agents can outperform baselines.
The proposed framework can generalize to other hazards such as floods and
hurricanes.

</details>


### [52] [ThinkPilot: Steering Reasoning Models via Automated Think-prefixes Optimization](https://arxiv.org/abs/2510.12063)
*Sunzhu Li,Zhiyu Lin,Shuling Yang,Jiale Zhao,Wei Chen*

Main category: cs.AI

TL;DR: ThinkPilot是一个无需训练即可自动优化大型推理模型推理过程的框架，通过进化过程生成think-prefixes来引导模型实现更优性能。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型虽然强大，但仍存在推理效率低下和偏离目标的问题。现有的免训练方法要么局限于僵化的启发式方法，要么只能提供描述性但不可操作的分析。

Method: 使用进化过程生成think-prefixes，这些指令基于推理行为分类学进行演化，引导模型朝着更好的性能方向发展。

Result: 显著改善了准确率与推理长度的权衡关系，大幅提升了安全性（如将DeepSeek-R1-Distill-Qwen-32B的StrongREJECT分数从27.0%降至0.7），并增强了指令跟随能力。

Conclusion: think-prefixes能够可靠地控制大型推理模型的推理行为，不同任务对特定行为分布有强烈偏好。ThinkPilot通过自动识别和激发这些行为，提供了一个可泛化的框架来使模型推理与任务需求对齐。

Abstract: Large Reasoning Models (LRMs) are powerful, but they still suffer from
inefficient and off-target reasoning. Currently, training-free methods are
limited to either rigid heuristics or descriptive, non-actionable analyses. In
this paper, we introduce ThinkPilot, a training-free framework that
automatically optimizes LRMs reasoning. It uses an evolutionary process to
generate think-prefixes, which are instructions that evolve driven by a
taxonomy of reasoning behaviors to guide models toward superior performance.
Extensive experiments demonstrate ThinkPilot's broad effectiveness: it
significantly improves the accuracy-length trade-off for efficient reasoning,
drastically improves safety (for example, cutting the StrongREJECT score of
DeepSeek-R1-Distill-Qwen-32B from 27.0% to 0.7), and enhances instruction
following. It also synergizes with existing training-based methods. Our
analysis reveals that think-prefixes can reliably control LRMs' reasoning
behaviors, and that different tasks have strong preferences for specific
behavioral distributions. By automatically identifying and eliciting these
behaviors, ThinkPilot provides a generalizable framework for aligning LRMs
reasoning with task demands. Data and code are available at
https://github.com/teqkilla/ThinkPilot

</details>


### [53] [AI Agents as Universal Task Solvers](https://arxiv.org/abs/2510.12066)
*Alessandro Achille,Stefano Soatto*

Main category: cs.AI

TL;DR: 该论文重新诠释了AI智能体中的学习角色，将其视为具备计算能力的随机动力系统，强调时间在学习推理中的关键作用，并提出从归纳学习转向转导学习，以优化推理时间而非准确度。


<details>
  <summary>Details</summary>
Motivation: 探讨AI推理智能体是否能成为通用计算器，以及如何通过学习来提升推理能力，质疑模型规模是否是关键因素。

Method: 将AI智能体重新定义为计算能力的随机动力系统，提出转导学习框架，关注算法结构而非数据分布，理论推导推理时间与训练时间的幂律缩放关系。

Result: 理论证明通用求解器使用历史数据获得的最优加速与其算法信息紧密相关，大规模模型可能成为仅靠蛮力解决问题的"专家"而非真正智能体。

Conclusion: 在扩展推理模型时，应优化的关键量是时间而非模型规模，时间在学习中的关键作用需要直接考虑。

Abstract: AI reasoning agents are already able to solve a variety of tasks by deploying
tools, simulating outcomes of multiple hypotheses and reflecting on them. In
doing so, they perform computation, although not in the classical sense --
there is no program being executed. Still, if they perform computation, can AI
agents be universal? Can chain-of-thought reasoning solve any computable task?
How does an AI Agent learn to reason? Is it a matter of model size? Or training
dataset size?
  In this work, we reinterpret the role of learning in the context of AI
Agents, viewing them as compute-capable stochastic dynamical systems, and
highlight the role of time in a foundational principle for learning to reason.
In doing so, we propose a shift from classical inductive learning to
transductive learning -- where the objective is not to approximate the
distribution of past data, but to capture their algorithmic structure to reduce
the time needed to find solutions to new tasks.
  Transductive learning suggests that, counter to Shannon's theory, a key role
of information in learning is about reduction of time rather than
reconstruction error. In particular, we show that the optimal speed-up that a
universal solver can achieve using past data is tightly related to their
algorithmic information. Using this, we show a theoretical derivation for the
observed power-law scaling of inference time versus training time. We then show
that scaling model size can lead to behaviors that, while improving accuracy on
benchmarks, fail any reasonable test of intelligence, let alone
super-intelligence: In the limit of infinite space and time, large models can
behave as savants, able to brute-force through any task without any insight.
Instead, we argue that the key quantity to optimize when scaling reasoning
models is time, whose critical role in learning has so far only been indirectly
considered.

</details>


### [54] [HiCoTraj:Zero-Shot Demographic Reasoning via Hierarchical Chain-of-Thought Prompting from Trajectory](https://arxiv.org/abs/2510.12067)
*Junyi Xie,Yuankun Jiao,Jina Kim,Yao-Yi Chiang,Lingyi Zhao,Khurram Shafique*

Main category: cs.AI

TL;DR: HiCoTraj是一个零样本人口统计推理框架，通过分层思维链提示从轨迹数据中推断年龄、性别、收入等人口属性，无需标注训练数据。


<details>
  <summary>Details</summary>
Motivation: 现有基于移动性的人口统计推断方法依赖大规模带标签轨迹数据，导致可解释性差且跨数据集泛化能力弱。

Method: 将轨迹转换为语义丰富的自然语言表示，通过分层思维链推理（事实特征提取、行为模式分析、结构化输出的人口统计推断）指导大语言模型。

Result: 在真实世界轨迹数据上的实验表明，HiCoTraj在零样本场景下对多个人口统计属性实现了有竞争力的性能。

Conclusion: HiCoTraj解决了标注人口统计数据稀缺的问题，同时提供了透明的推理链，具有良好的可解释性和泛化能力。

Abstract: Inferring demographic attributes such as age, sex, or income level from human
mobility patterns enables critical applications such as targeted public health
interventions, equitable urban planning, and personalized transportation
services. Existing mobility-based demographic inference studies heavily rely on
large-scale trajectory data with demographic labels, leading to limited
interpretability and poor generalizability across different datasets and user
groups. We propose HiCoTraj (Zero-Shot Demographic Reasoning via Hierarchical
Chain-of-Thought Prompting from Trajectory), a framework that leverages LLMs'
zero-shot learning and semantic understanding capabilities to perform
demographic inference without labeled training data. HiCoTraj transforms
trajectories into semantically rich, natural language representations by
creating detailed activity chronicles and multi-scale visiting summaries. Then
HiCoTraj uses a novel hierarchical chain of thought reasoning to systematically
guide LLMs through three cognitive stages: factual feature extraction,
behavioral pattern analysis, and demographic inference with structured output.
This approach addresses the scarcity challenge of labeled demographic data
while providing transparent reasoning chains. Experimental evaluation on
real-world trajectory data demonstrates that HiCoTraj achieves competitive
performance across multiple demographic attributes in zero-shot scenarios.

</details>


### [55] [EmboMatrix: A Scalable Training-Ground for Embodied Decision-Making](https://arxiv.org/abs/2510.12072)
*Zixing Lei,Sheng Yin,Yichen Xiong,Yuanzhuo Ding,Wenhao Huang,Yuxi Wei,Qingyao Xu,Yiming Li,Weixin Li,Yunhong Wang,Siheng Chen*

Main category: cs.AI

TL;DR: EmboMatrix是一个为LLM提供真实物理环境交互的训练平台，通过大规模任务模拟和精确奖励机制，培养出具有卓越具身决策能力的EmboBrain模型。


<details>
  <summary>Details</summary>
Motivation: 现有LLM仅基于语言训练，缺乏物理环境暴露，限制了其真实的具身理解能力。需要构建一个能够提供任务场景模拟、具身交互和反馈信号的训练基础设施。

Method: 提出EmboMatrix训练平台，包含多智能体数据引擎用于大规模任务场景生成、分布式异构硬件系统实现可扩展模拟、多级奖励架构提供精确监督。

Result: 基于EmboMatrix训练的EmboBrain-7B模型在两个具身决策基准上超越了671B的DeepSeek-R1基线9.5%。

Conclusion: 交互式、环境基础的学习对于构建真正智能的具身智能体具有强大威力，EmboMatrix为LLM获得真实具身决策技能提供了有效解决方案。

Abstract: Embodied decision-making enables agents to translate high-level goals into
executable actions through continuous interactions within the physical world,
forming a cornerstone of general-purpose embodied intelligence. Large language
models (LLMs), with their general decision-making capabilities, offer a
promising path to realize this potential; however, LLMs trained solely on
language lack exposure to physical environments, limiting their true embodied
understanding. To bridge this gap, we propose the concept of a training ground:
a comprehensive infrastructure that provides task and scene simulation,
embodied interaction, and feedback signals, offering a one-stop solution for
LLM acquire genuine embodied decision-making skills. In this work, we present
EmboMatrix, the first training ground of its kind, providing massive and
diverse tasks with efficient simulation and precise rewards. EmboMatrix
incorporates a series of novel techniques: a multi-agent data engine for
large-scale task and scene generation, a distributed heterogeneous-hardware
system for scalable simulation, and a multi-level reward architecture for
precise supervision. Leveraging EmboMatrix, we cultivate EmboBrain, an LLM
whose embodied decision-making abilities emerge from extensive embodied
interactions. Experiments show that EmboBrain-7B surpasses the 671B DeepSeek-R1
baseline by 9.5\% on two challenging embodied decision-making benchmarks,
demonstrating the power of interactive, environment-grounded learning for
building truly intelligent embodied agents.

</details>


### [56] [BeSTAD: Behavior-Aware Spatio-Temporal Anomaly Detection for Human Mobility Data](https://arxiv.org/abs/2510.12076)
*Junyi Xie,Jina Kim,Yao-Yi Chiang,Lingyi Zhao,Khurram Shafique*

Main category: cs.AI

TL;DR: BeSTAD是一个无监督框架，通过联合建模空间上下文和时间动态来检测人类移动数据中的个体级异常，能够从大规模人群中捕获个体化行为特征。


<details>
  <summary>Details</summary>
Motivation: 传统异常检测主要关注轨迹级分析，但在大规模数据集中检测个体相对于自身历史模式的异常移动行为仍然具有挑战性。

Method: 学习语义丰富的移动表示，整合位置意义和时间模式；采用行为聚类感知建模机制，从正常活动构建个性化行为档案，并通过跨时期行为比较识别异常。

Result: 能够检测行为转变和与既定常规的偏差，并在大规模移动数据集中识别出表现出此类变化的个体。

Conclusion: 通过直接从无标签数据学习个体行为，BeSTAD将异常检测推向个性化和可解释的移动分析。

Abstract: Traditional anomaly detection in human mobility has primarily focused on
trajectory-level analysis, identifying statistical outliers or spatiotemporal
inconsistencies across aggregated movement traces. However, detecting
individual-level anomalies, i.e., unusual deviations in a person's mobility
behavior relative to their own historical patterns, within datasets
encompassing large populations remains a significant challenge. In this paper,
we present BeSTAD (Behavior-aware Spatio-Temporal Anomaly Detection for Human
Mobility Data), an unsupervised framework that captures individualized
behavioral signatures across large populations and uncovers fine-grained
anomalies by jointly modeling spatial context and temporal dynamics. BeSTAD
learns semantically enriched mobility representations that integrate location
meaning and temporal patterns, enabling the detection of subtle deviations in
individual movement behavior. BeSTAD further employs a behavior-cluster-aware
modeling mechanism that builds personalized behavioral profiles from normal
activity and identifies anomalies through cross-period behavioral comparison
with consistent semantic alignment. Building on prior work in mobility behavior
clustering, this approach enables not only the detection of behavioral shifts
and deviations from established routines but also the identification of
individuals exhibiting such changes within large-scale mobility datasets. By
learning individual behaviors directly from unlabeled data, BeSTAD advances
anomaly detection toward personalized and interpretable mobility analysis.

</details>


### [57] [Evaluating the Quality of Randomness and Entropy in Tasks Supported by Large Language Models](https://arxiv.org/abs/2510.12080)
*Rabimba Karanjai,Yang Lu,Ranjith Chodavarapu,Lei Xu,Weidong Shi*

Main category: cs.AI

TL;DR: 该论文研究了大型语言模型处理随机性任务的能力，发现虽然LLM能产生一定随机性输出，但表现不一致且与预期行为有显著偏差。


<details>
  <summary>Details</summary>
Motivation: 随着LLM技术的快速发展，许多应用需要随机性（如随机决策、游戏、调度等），但LLM处理随机性的能力尚不明确。

Method: 设计了一系列实验，考虑外部工具访问、任务类型、模型状态（新鲜vs非新鲜）和提示策略等因素，涵盖生成随机数、随机字符串、项目洗牌等任务，并使用熵和NIST随机性测试套件评估随机性质量。

Result: LLM生成的输出具有一定随机性，但性能不一致，经常显著偏离预期行为。

Conclusion: LLM在处理涉及随机性的任务时存在关键限制，需要改进才能有效处理这类任务。

Abstract: The rapid advancement of large language model (LLM) technology has led to
diverse applications, many of which inherently require randomness, such as
stochastic decision-making, gaming, scheduling, AI agents, and
cryptography-related tasks. However, the capabilities of LLMs in handling
randomness, particularly in generating and utilizing random numbers
effectively, remain unclear. This paper investigates the capacity of LLMs for
handling tasks that involve randomness through a series of experiments. We
designed a set of experiments that consider various factors that can influence
an LLM's performance in tasks involving randomness, such as accessibility to
external tools, types of tasks, model states (fresh vs. non-fresh), and
prompting strategies. The experiments cover a range of tasks, including
generating random numbers, generating random strings such as passwords,
shuffling items, and evaluating the quality of randomness using entropy and the
NIST randomness test-suite. Our findings reveal that while LLMs can generate
outputs that exhibit some degree of randomness, their performance is
inconsistent and often deviates significantly from the expected behavior. The
analysis of the experimental results highlights key limitations and areas where
improvement is needed for the LLMs to effectively handle tasks involving
randomness

</details>


### [58] [One Life to Learn: Inferring Symbolic World Models for Stochastic Environments from Unguided Exploration](https://arxiv.org/abs/2510.12088)
*Zaid Khan,Archiki Prasad,Elias Stengel-Eskin,Jaemin Cho,Mohit Bansal*

Main category: cs.AI

TL;DR: OneLife框架通过条件激活的程序化法则在概率编程框架中建模世界动态，能够在复杂、随机环境中从有限的无指导交互中成功学习关键环境动态。


<details>
  <summary>Details</summary>
Motivation: 解决在复杂、随机环境中，代理只有"一次生命"来探索敌对环境且无人指导的现实挑战性场景下的符号世界建模问题。

Method: 使用条件激活的程序化法则，采用前提-效果结构，在相关世界状态中激活，创建动态计算图，仅通过相关法则进行推理和优化。

Result: 在23个测试场景中的16个场景上优于强基线，能够从最少、无指导的交互中成功学习关键环境动态，模拟推演成功识别出更优策略。

Conclusion: 为自主构建未知复杂环境的程序化世界模型奠定了基础。

Abstract: Symbolic world modeling requires inferring and representing an environment's
transitional dynamics as an executable program. Prior work has focused on
largely deterministic environments with abundant interaction data, simple
mechanics, and human guidance. We address a more realistic and challenging
setting, learning in a complex, stochastic environment where the agent has only
"one life" to explore a hostile environment without human guidance. We
introduce OneLife, a framework that models world dynamics through
conditionally-activated programmatic laws within a probabilistic programming
framework. Each law operates through a precondition-effect structure,
activating in relevant world states. This creates a dynamic computation graph
that routes inference and optimization only through relevant laws, avoiding
scaling challenges when all laws contribute to predictions about a complex,
hierarchical state, and enabling the learning of stochastic dynamics even with
sparse rule activation. To evaluate our approach under these demanding
constraints, we introduce a new evaluation protocol that measures (a) state
ranking, the ability to distinguish plausible future states from implausible
ones, and (b) state fidelity, the ability to generate future states that
closely resemble reality. We develop and evaluate our framework on Crafter-OO,
our reimplementation of the Crafter environment that exposes a structured,
object-oriented symbolic state and a pure transition function that operates on
that state alone. OneLife can successfully learn key environment dynamics from
minimal, unguided interaction, outperforming a strong baseline on 16 out of 23
scenarios tested. We also test OneLife's planning ability, with simulated
rollouts successfully identifying superior strategies. Our work establishes a
foundation for autonomously constructing programmatic world models of unknown,
complex environments.

</details>


### [59] [ToPolyAgent: AI Agents for Coarse-Grained Topological Polymer Simulations](https://arxiv.org/abs/2510.12091)
*Lijie Ding,Jan-Michael Carrillo,Changwoo Do*

Main category: cs.AI

TL;DR: ToPolyAgent是一个多智能体AI框架，通过自然语言指令执行拓扑聚合物的粗粒度分子动力学模拟，结合LLM和领域专用工具支持交互式和自主模拟工作流。


<details>
  <summary>Details</summary>
Motivation: 降低复杂计算工作流的门槛，推进聚合物科学中AI驱动的材料发现，为自主和可扩展的多智能体科学研究生态系统奠定基础。

Method: 系统包含四个LLM驱动的智能体：配置智能体生成初始聚合物-溶剂配置，模拟智能体执行基于LAMMPS的MD模拟和构象分析，报告智能体编译markdown报告，工作流智能体实现流线化自主操作。支持交互式和自主两种模式。

Result: 通过案例研究展示了系统的多功能性，涉及不同聚合物架构在各种溶剂条件、恒温器和模拟长度下的应用。作为研究助手，成功研究了相互作用参数对线性聚合物构象的影响，以及接枝密度对刷状聚合物持久长度的影响。

Conclusion: 通过将自然语言接口与严格模拟工具耦合，ToPolyAgent降低了复杂计算工作流的障碍，推进了聚合物科学中AI驱动的材料发现，为自主和可扩展的多智能体科学研究生态系统奠定了基础。

Abstract: We introduce ToPolyAgent, a multi-agent AI framework for performing
coarse-grained molecular dynamics (MD) simulations of topological polymers
through natural language instructions. By integrating large language models
(LLMs) with domain-specific computational tools, ToPolyAgent supports both
interactive and autonomous simulation workflows across diverse polymer
architectures, including linear, ring, brush, and star polymers, as well as
dendrimers. The system consists of four LLM-powered agents: a Config Agent for
generating initial polymer-solvent configurations, a Simulation Agent for
executing LAMMPS-based MD simulations and conformational analyses, a Report
Agent for compiling markdown reports, and a Workflow Agent for streamlined
autonomous operations. Interactive mode incorporates user feedback loops for
iterative refinements, while autonomous mode enables end-to-end task execution
from detailed prompts. We demonstrate ToPolyAgent's versatility through case
studies involving diverse polymer architectures under varying solvent
condition, thermostats, and simulation lengths. Furthermore, we highlight its
potential as a research assistant by directing it to investigate the effect of
interaction parameters on the linear polymer conformation, and the influence of
grafting density on the persistence length of the brush polymer. By coupling
natural language interfaces with rigorous simulation tools, ToPolyAgent lowers
barriers to complex computational workflows and advances AI-driven materials
discovery in polymer science. It lays the foundation for autonomous and
extensible multi-agent scientific research ecosystems.

</details>


### [60] [Precise Attribute Intensity Control in Large Language Models via Targeted Representation Editing](https://arxiv.org/abs/2510.12121)
*Rongzhi Zhang,Liqin Ye,Yuzhao Heng,Xiang Chen,Tong Yu,Lingkai Kong,Sudheer Chava,Chao Zhang*

Main category: cs.AI

TL;DR: 提出了一种精确控制LLM输出属性强度的方法，将属性强度控制重新定义为目标达成问题，通过训练轻量级价值函数和梯度干预实现精确控制


<details>
  <summary>Details</summary>
Motivation: 现有LLM对齐方法只能提供方向性或开放式指导，无法可靠实现精确的属性强度控制，需要适应不同用户期望的AI系统

Method: 1. 将精确属性强度控制重新定义为目标达成问题；2. 通过时序差分学习训练轻量级价值函数来预测属性强度；3. 对隐藏表示进行基于梯度的干预以精确导航到特定属性强度目标

Result: 在LLaMA-3.2-3b和Phi-4-mini上的实验证实了该方法能够以高精度将文本生成引导到用户指定的属性强度

Conclusion: 该方法实现了对属性强度的细粒度连续控制，超越了简单的方向性对齐，并在偏好数据合成、帕累托前沿近似和优化、对齐行为蒸馏等下游任务中展示了效率提升

Abstract: Precise attribute intensity control--generating Large Language Model (LLM)
outputs with specific, user-defined attribute intensities--is crucial for AI
systems adaptable to diverse user expectations. Current LLM alignment methods,
however, typically provide only directional or open-ended guidance, failing to
reliably achieve exact attribute intensities. We address this limitation with
three key designs: (1) reformulating precise attribute intensity control as a
target-reaching problem, rather than simple maximization; (2) training a
lightweight value function via temporal-difference learning to predict final
attribute intensity scores from partial generations, thereby steering LLM
outputs; and (3) employing gradient-based interventions on hidden
representations to navigate the model precisely towards specific attribute
intensity targets. Our method enables fine-grained, continuous control over
attribute intensities, moving beyond simple directional alignment. Experiments
on LLaMA-3.2-3b and Phi-4-mini confirm our method's ability to steer text
generation to user-specified attribute intensities with high accuracy. Finally,
we demonstrate efficiency enhancements across three downstream tasks:
preference data synthesis, Pareto frontier approximation and optimization, and
distillation of aligned behaviors for intervention-free inference. Our code is
available on https://github.com/Pre-Control/pre-control

</details>


### [61] [MatSciBench: Benchmarking the Reasoning Ability of Large Language Models in Materials Science](https://arxiv.org/abs/2510.12171)
*Junkai Zhang,Jingru Gan,Xiaoxuan Wang,Zian Jia,Changquan Gu,Jianpeng Chen,Yanqiao Zhu,Mingyu Derek Ma,Dawei Zhou,Ling Li,Wei Wang*

Main category: cs.AI

TL;DR: MatSciBench是一个包含1,340个大学水平材料科学问题的综合基准测试，涵盖6个主要领域和31个子领域，具有三级难度分类和多模态推理能力。


<details>
  <summary>Details</summary>
Motivation: 填补大型语言模型在材料科学领域推理能力评估的空白，建立全面的基准测试来推动LLMs在科学推理能力方面的改进。

Method: 开发了结构化的MatSciBench基准，包含详细分类、难度分级、参考解决方案和多模态问题。评估了不同推理策略（思维链、工具增强、自我修正）和检索增强生成的效果。

Result: 即使是表现最佳的Gemini-2.5-Pro模型在材料科学问题上的准确率也不到80%。不同推理策略在不同场景下表现不一，没有单一方法在所有情况下都表现优异。

Conclusion: MatSciBench为评估和提升LLMs在材料科学领域的科学推理能力建立了全面而坚实的基准，揭示了当前模型在该领域的局限性。

Abstract: Large Language Models (LLMs) have demonstrated remarkable abilities in
scientific reasoning, yet their reasoning capabilities in materials science
remain underexplored. To fill this gap, we introduce MatSciBench, a
comprehensive college-level benchmark comprising 1,340 problems that span the
essential subdisciplines of materials science. MatSciBench features a
structured and fine-grained taxonomy that categorizes materials science
questions into 6 primary fields and 31 sub-fields, and includes a three-tier
difficulty classification based on the reasoning length required to solve each
question. MatSciBench provides detailed reference solutions enabling precise
error analysis and incorporates multimodal reasoning through visual contexts in
numerous questions. Evaluations of leading models reveal that even the
highest-performing model, Gemini-2.5-Pro, achieves under 80% accuracy on
college-level materials science questions, highlighting the complexity of
MatSciBench. Our systematic analysis of different reasoning strategie--basic
chain-of-thought, tool augmentation, and self-correction--demonstrates that no
single method consistently excels across all scenarios. We further analyze
performance by difficulty level, examine trade-offs between efficiency and
accuracy, highlight the challenges inherent in multimodal reasoning tasks,
analyze failure modes across LLMs and reasoning methods, and evaluate the
influence of retrieval-augmented generation. MatSciBench thus establishes a
comprehensive and solid benchmark for assessing and driving improvements in the
scientific reasoning capabilities of LLMs within the materials science domain.

</details>


### [62] [Evolution of meta's llama models and parameter-efficient fine-tuning of large language models: a survey](https://arxiv.org/abs/2510.12178)
*Abdulhady Abas Abdullah,Arkaitz Zubiaga,Seyedali Mirjalili,Amir H. Gandomi,Fatemeh Daneshfar,Mohammadsadra Amini,Alan Salam Mohammed,Hadi Veisi*

Main category: cs.AI

TL;DR: 这篇综述论文系统回顾了Meta AI的LLaMA系列模型（从LLaMA 1到LLaMA 4）及其参数高效微调方法的发展，包括模型架构、性能特点和五种PEFT技术的机制与应用。


<details>
  <summary>Details</summary>
Motivation: 为机器学习研究者和从业者提供关于LLaMA模型和高效微调策略的一站式资源，帮助理解这一快速发展的技术领域。

Method: 首先描述LLaMA系列基础模型（7B-65B到288B参数）及其架构，然后介绍参数高效微调概念，详细分析五种PEFT方法：LoRA、LLaMA-Adapter V1和V2、LLaMA-Excitor、QLoRA。

Result: 提供了模型和适配器架构、参数数量、基准测试结果的结构化分析，展示了微调后的LLaMA模型在某些情况下优于更大基线模型的实例，以及在法律和医疗等领域的成功应用案例。

Conclusion: LLaMA模型和PEFT技术在实际应用中取得了显著成功，但仍面临扩展到更大上下文和改进鲁棒性等挑战，需要未来的持续研究。

Abstract: This review surveys the rapid evolution of Meta AI's LLaMA (Large Language
Model Meta AI) series - from LLaMA 1 through LLaMA 4 and the specialized
parameter-efficient fine-tuning (PEFT) methods developed for these models. We
first describe the LLaMA family of foundation models (7B-65B to 288B
parameters), their architectures (including native multimodal and
Mixtureof-Experts variants), and key performance characteristics. We then
describe and discuss the concept of PEFT, which adapts large pre-trained models
by updating only a small subset of parameters, and review five PEFT methods
that have been applied to LLaMA: LoRA (Low-Rank Adaptation), LLaMA-Adapter V1
and V2, LLaMA-Excitor, and QLoRA (Quantized LoRA). We discuss each method's
mechanism, parameter savings, and example application to LLaMA (e.g.,
instruction tuning, multimodal tasks). We provide structured discussion and
analysis of model and adapter architectures, parameter counts, and benchmark
results (including examples where fine-tuned LLaMA models outperform larger
baselines). Finally, we examine real-world use cases where LLaMA-based models
and PEFT have been successfully applied (e.g., legal and medical domains), and
we discuss ongoing challenges and future research directions (such as scaling
to even larger contexts and improving robustness). This survey paper provides a
one-stop resource for ML researchers and practitioners interested in LLaMA
models and efficient fine-tuning strategies.

</details>


### [63] [ResearStudio: A Human-Intervenable Framework for Building Controllable Deep-Research Agents](https://arxiv.org/abs/2510.12194)
*Linyi Yang,Yixuan Weng*

Main category: cs.AI

TL;DR: ResearStudio是一个开源框架，通过协作工作坊设计实现实时人类控制的深度研究代理，支持在AI主导、人类辅助和人类主导、AI辅助模式间无缝切换。


<details>
  <summary>Details</summary>
Motivation: 当前深度研究代理采用"发射后不管"模式，用户无法在执行过程中修正错误或添加专家知识。

Method: 采用分层规划器-执行器架构，将每个步骤写入实时"计划即文档"，通过快速通信层将每个动作、文件变更和工具调用流式传输到Web界面。

Result: 在完全自主模式下，ResearStudio在GAIA基准测试中达到最先进水平，超越了OpenAI的DeepResearch和Manus等系统。

Conclusion: 强大的自动化性能和细粒度人类控制可以共存，为安全可控的研究代理发展提供了新方向。

Abstract: Current deep-research agents run in a ''fire-and-forget'' mode: once started,
they give users no way to fix errors or add expert knowledge during execution.
We present ResearStudio, the first open-source framework that places real-time
human control at its core. The system follows a Collaborative Workshop design.
A hierarchical Planner-Executor writes every step to a live
''plan-as-document,'' a fast communication layer streams each action, file
change, and tool call to a web interface. At any moment, the user can pause the
run, edit the plan or code, run custom commands, and resume -- switching
smoothly between AI-led, human-assisted and human-led, AI-assisted modes. In
fully autonomous mode, ResearStudio achieves state-of-the-art results on the
GAIA benchmark, surpassing systems like OpenAI's DeepResearch and Manus. These
results show that strong automated performance and fine-grained human control
can coexist. The full code, protocol, and evaluation scripts are available at
https://github.com/ResearAI/ResearStudio. We will continue to update the
repository to encourage further work on safe and controllable research agents.
Our live demo is publicly accessible at http://ai-researcher.net:3000/. We
support the development of DeepScientist, which can be accessed at
https://github.com/ResearAI/DeepScientist.

</details>


### [64] [On the Design and Evaluation of Human-centered Explainable AI Systems: A Systematic Review and Taxonomy](https://arxiv.org/abs/2510.12201)
*Aline Mangold,Juliane Zietz,Susanne Weinhold,Sebastian Pannasch*

Main category: cs.AI

TL;DR: 本文对65项评估可解释AI(XAI)系统的用户研究进行了全面综述，提出了以人为中心的XAI系统设计目标和评估框架，并针对不同AI专业水平的用户提供了定制化设计指南。


<details>
  <summary>Details</summary>
Motivation: 随着AI在日常生活中的普及，对既高性能又可理解的智能系统需求日益增长。当前XAI系统的评估过程过于技术化，未能充分关注人类用户的需求，因此需要开展以人为中心的用户研究来指导XAI开发。

Method: 通过全面综述65项评估XAI系统的用户研究，分析XAI系统特性和评估指标，提出以人为中心的设计目标，并根据用户AI专业水平（AI新手和数据专家）进行定制化扩展。

Result: 研究发现XAI系统由核心系统和解释组件共同构成；评估指标可分为系统情感、认知、可用性、可解释性和解释指标；针对AI新手的设计目标包括负责任使用、接受度和可用性，针对数据专家的设计目标则侧重于性能导向、人机协作和任务表现。

Conclusion: 本文扩展了现有的XAI评估和设计框架，为XAI开发者提供了以人为中心的系统特性和评估指标的全景视图，以及针对不同用户群体的定制化设计目标，有助于开发更符合人类用户需求的XAI系统。

Abstract: As AI becomes more common in everyday living, there is an increasing demand
for intelligent systems that are both performant and understandable.
Explainable AI (XAI) systems aim to provide comprehensible explanations of
decisions and predictions. At present, however, evaluation processes are rather
technical and not sufficiently focused on the needs of human users.
Consequently, evaluation studies involving human users can serve as a valuable
guide for conducting user studies. This paper presents a comprehensive review
of 65 user studies evaluating XAI systems across different domains and
application contexts. As a guideline for XAI developers, we provide a holistic
overview of the properties of XAI systems and evaluation metrics focused on
human users (human-centered). We propose objectives for the human-centered
design (design goals) of XAI systems. To incorporate users' specific
characteristics, design goals are adapted to users with different levels of AI
expertise (AI novices and data experts). In this regard, we provide an
extension to existing XAI evaluation and design frameworks. The first part of
our results includes the analysis of XAI system characteristics. An important
finding is the distinction between the core system and the XAI explanation,
which together form the whole system. Further results include the distinction
of evaluation metrics into affection towards the system, cognition, usability,
interpretability, and explanation metrics. Furthermore, the users, along with
their specific characteristics and behavior, can be assessed. For AI novices,
the relevant extended design goals include responsible use, acceptance, and
usability. For data experts, the focus is performance-oriented and includes
human-AI collaboration and system and user task performance.

</details>


### [65] [GOAT: A Training Framework for Goal-Oriented Agent with Tools](https://arxiv.org/abs/2510.12218)
*Hyunji Min,Sangwon Jung,Junyoung Sung,Dosung Lee,Leekyeung Han,Paul Hongsuck Seo*

Main category: cs.AI

TL;DR: GOAT是一个无需人工标注的训练框架，能够从API文档自动构建合成数据集，用于微调LLM代理处理目标导向的API执行任务。


<details>
  <summary>Details</summary>
Motivation: 当前LLM代理在处理需要分解高级目标为多个相互依赖API调用的目标导向查询时能力有限，开源模型表现不佳，而闭源模型如GPT-4虽强但不可控。

Method: 从API文档自动构建合成数据集，在无需人工标注的情况下训练LLM代理，使其能够推理相互依赖的API调用并生成连贯响应。

Result: GOAT训练的代理在多个现有目标导向基准测试中达到最先进性能，并在新提出的GOATBench基准上也表现出色。

Conclusion: GOAT为构建能够进行复杂推理和工具使用的健壮开源LLM代理提供了一条实用路径。

Abstract: Large language models (LLMs) have recently been extended beyond traditional
text generation to serve as interactive agents capable of using external tools
based on user intent. However, current LLM agents still show limited ability to
handle goal-oriented queries, which require decomposing a high-level objective
into multiple interdependent API calls with correct planning and execution.
Current approaches mainly rely on zero-shot evaluation due to the absence of
training data. While proprietary closed-source models such as GPT-4 demonstrate
strong reasoning abilities, smaller open-source models struggle to perform
complex tool use effectively. Thus, we propose a novel training framework GOAT,
which enables fine-tuning of LLM agents in a human annotation-free setting.
GOAT automatically constructs synthetic datasets of goal-oriented API execution
tasks directly from given API documents, equipping models with the ability to
reason over interdependent calls and generate coherent responses. Through
extensive experiments, we show that GOAT-trained agents achieve
state-of-the-art performance across multiple existing goal-oriented benchmarks.
In addition, we introduce GOATBench, a new goal-oriented API execution
benchmark, and demonstrate that agents trained with GOAT also excel in this
setting. These results highlight GOAT as a practical path toward building
robust open-source LLM agents capable of complex reasoning and tool use.

</details>


### [66] [Evaluating and Mitigating LLM-as-a-judge Bias in Communication Systems](https://arxiv.org/abs/2510.12462)
*Jiaxin Gao,Chen Chen,Yanwen Jia,Xueluan Gong,Kwok-Yan Lam,Qian Wang*

Main category: cs.AI

TL;DR: 本文系统研究了LLM作为评判者时的偏见问题，发现现有模型对偏见输入具有鲁棒性，但微调于偏见数据会显著降低性能，并提出四种缓解策略。


<details>
  <summary>Details</summary>
Motivation: LLM被越来越多地用于自主评估通信系统中的内容质量，但这些AI"评判者"的公正性无法保证，其评估标准中的任何偏见都可能扭曲结果并损害用户信任。

Method: 在点式评分设置下，系统调查了两种LLM评判者模型（GPT-Judge和JudgeLM）中的11种偏见类型，涵盖隐式和显式形式，并分析了评分标准、微调训练和任务难度对评判结果的影响。

Result: 最先进的LLM评判者对偏见输入表现出鲁棒性，通常给它们分配比相应干净样本更低的分数；提供详细评分标准能进一步增强这种鲁棒性；在偏见但高分响应上微调LLM会显著降低其性能；评判分数与任务难度相关。

Conclusion: 提出了四种潜在的缓解策略，以确保在实际通信场景中实现公平可靠的AI评判。

Abstract: Large Language Models (LLMs) are increasingly being used to autonomously
evaluate the quality of content in communication systems, e.g., to assess
responses in telecom customer support chatbots. However, the impartiality of
these AI "judges" is not guaranteed, and any biases in their evaluation
criteria could skew outcomes and undermine user trust. In this paper, we
systematically investigate judgment biases in two LLM-as-a-judge models (i.e.,
GPT-Judge and JudgeLM) under the point-wise scoring setting, encompassing 11
types of biases that cover both implicit and explicit forms. We observed that
state-of-the-art LLM judges demonstrate robustness to biased inputs, generally
assigning them lower scores than the corresponding clean samples. Providing a
detailed scoring rubric further enhances this robustness. We further found that
fine-tuning an LLM on high-scoring yet biased responses can significantly
degrade its performance, highlighting the risk of training on biased data. We
also discovered that the judged scores correlate with task difficulty: a
challenging dataset like GPQA yields lower average scores, whereas an
open-ended reasoning dataset (e.g., JudgeLM-val) sees higher average scores.
Finally, we proposed four potential mitigation strategies to ensure fair and
reliable AI judging in practical communication scenarios.

</details>


### [67] [MedKGEval: A Knowledge Graph-Based Multi-Turn Evaluation Framework for Open-Ended Patient Interactions with Clinical LLMs](https://arxiv.org/abs/2510.12224)
*Yuechun Yu,Han Ying,Haoan Jin,Wenjian Jiang,Dong Xian,Binghao Wang,Zhou Yang,Mengyue Wu*

Main category: cs.AI

TL;DR: 提出了MedKGEval框架，这是一个基于结构化医学知识的多轮临床LLM评估方法，通过知识图谱驱动的患者模拟和实时评估来改进医疗对话评估。


<details>
  <summary>Details</summary>
Motivation: 现有评估方法主要依赖对话记录的事后审查，忽略了医疗对话的动态性和患者信息需求的演变，无法准确评估LLM在真实临床环境中的表现。

Method: 使用知识图谱驱动的患者模拟机制，构建控制模块从知识图谱检索医学事实；开发实时轮级评估框架，由评估代理使用细粒度指标评估模型响应；建立多轮基准测试八个先进LLM。

Result: MedKGEval能够识别传统评估方法忽略的细微行为缺陷和安全风险，展示了其在临床LLM评估中的有效性。

Conclusion: 该框架为医疗LLM提供了更可靠的评估方法，支持中英文应用，并可通过切换知识图谱扩展到其他语言和领域。

Abstract: The reliable evaluation of large language models (LLMs) in medical
applications remains an open challenge, particularly in capturing the
complexity of multi-turn doctor-patient interactions that unfold in real
clinical environments. Existing evaluation methods typically rely on post hoc
review of full conversation transcripts, thereby neglecting the dynamic,
context-sensitive nature of medical dialogues and the evolving informational
needs of patients. In this work, we present MedKGEval, a novel multi-turn
evaluation framework for clinical LLMs grounded in structured medical
knowledge. Our approach introduces three key contributions: (1) a knowledge
graph-driven patient simulation mechanism, where a dedicated control module
retrieves relevant medical facts from a curated knowledge graph, thereby
endowing the patient agent with human-like and realistic conversational
behavior. This knowledge graph is constructed by integrating open-source
resources with additional triples extracted from expert-annotated datasets; (2)
an in-situ, turn-level evaluation framework, where each model response is
assessed by a Judge Agent for clinical appropriateness, factual correctness,
and safety as the dialogue progresses using a suite of fine-grained,
task-specific metrics; (3) a comprehensive multi-turn benchmark of eight
state-of-the-art LLMs, demonstrating MedKGEval's ability to identify subtle
behavioral flaws and safety risks that are often overlooked by conventional
evaluation pipelines. Although initially designed for Chinese and English
medical applications, our framework can be readily extended to additional
languages by switching the input knowledge graphs, ensuring seamless bilingual
support and domain-specific applicability.

</details>


### [68] [PromptFlow: Training Prompts Like Neural Networks](https://arxiv.org/abs/2510.12246)
*Jingyi Wang,Hongyuan Zhu,Ye Niu,Yunhui Deng*

Main category: cs.AI

TL;DR: 提出了PromptFlow框架，这是一个受TensorFlow启发的模块化训练框架，用于自动化提示工程。它通过元提示、操作符、优化器和评估器组件，结合基于梯度的元学习和强化学习，实现最优提示精炼轨迹的自主探索。


<details>
  <summary>Details</summary>
Motivation: 当前自动化提示工程方法存在静态更新规则、缺乏动态策略选择机制、粗粒度提示更新以及LLM经验回收问题未充分探索等局限性，需要更智能的提示优化框架。

Method: 设计了模块化的PromptFlow框架，包含元提示、操作符、优化器和评估器组件。采用基于梯度的元学习自主探索最优提示精炼轨迹，并设计了强化学习方法在提示工程过程中回收LLM经验。

Result: 在多个数据集上进行了广泛实验，证明了PromptFlow框架的有效性。

Conclusion: PromptFlow框架能够以最少的任务特定训练数据，集成最新优化方法，自主探索最优提示精炼策略，有效解决了当前自动化提示工程方法的局限性。

Abstract: Large Language Models (LLMs) have demonstrated profound impact on Natural
Language Processing (NLP) tasks. However, their effective deployment across
diverse domains often require domain-specific adaptation strategies, as generic
models may underperform when faced with specialized data distributions. Recent
advances in prompt engineering (PE) offer a promising alternative to extensive
retraining by refining input instructions to align LLM outputs with task
objectives. This paradigm has emerged as a rapid and versatile approach for
model fine-tuning. Despite its potential, manual prompt design remains
labor-intensive and heavily depends on specialized expertise, often requiring
iterative human effort to achieve optimal formulations. To address this
limitation, automated prompt engineering methodologies have been developed to
systematically generate task-specific prompts. However, current implementations
predominantly employ static update rules and lack mechanisms for dynamic
strategy selection, resulting in suboptimal adaptation to varying NLP task
requirements. Furthermore, most methods treat and update the whole prompts at
each step, without considering editing prompt sections at a finer granularity.
At last, in particular, the problem of how to recycle experience in LLM is
still underexplored. To this end, we propose the PromptFlow, a modular training
framework inspired by TensorFlow, which integrates meta-prompts, operators,
optimization, and evaluator. Our framework can be equipped with the latest
optimization methods and autonomously explores optimal prompt refinement
trajectories through gradient-based meta-learning, requiring minimal
task-specific training data. Specifically, we devise a reinforcement learning
method to recycle experience for LLM in the PE process. Finally, we conduct
extensive experiments on various datasets, and demonstrate the effectiveness of
PromptFlow.

</details>


### [69] [$\mathbf{T^3}$: Reducing Belief Deviation in Reinforcement Learning for Active Reasoning](https://arxiv.org/abs/2510.12264)
*Deyu Zou,Yongqiang Chen,Jianxiang Wang,Haochen Yang,Mufei Li,James Cheng,Pan Li,Yu Gong*

Main category: cs.AI

TL;DR: 提出T³方法，通过检测和截断信念偏差过大的轨迹来改善LLM主动推理训练，提升策略优化效果


<details>
  <summary>Details</summary>
Motivation: LLM智能体在主动推理中容易发生信念偏差，导致问题状态跟踪失败和重复无效动作，这会破坏强化学习训练的信度分配

Method: 开发T³方法，跟踪模型信念偏差，检测过度的信念偏差并在训练时截断轨迹，保留信息性前缀的信度

Result: 在5个挑战性任务中，T³持续提升训练稳定性、令牌效率和最终性能，性能提升达30%，同时减少约25%的rollout令牌

Conclusion: 信念控制是开发稳健且可泛化的LLM主动推理器的关键原则

Abstract: Active reasoning requires large language models (LLMs) to interact with
external sources and strategically gather information to solve problems.
Central to this process is belief tracking: maintaining a coherent
understanding of the problem state and the missing information toward the
solution. However, due to limited reasoning capabilities, LLM-based agents
often suffer from belief deviation: they struggle to correctly model beliefs,
lose track of problem states, and fall into uninformative or repetitive
actions. Once this happens, errors compound and reinforcement learning (RL)
training fails to properly credit the crucial exploratory steps. To address
this issue, we propose to track the deviation of model beliefs and develop
$\mathbf{T^3}$, a simple yet effective method that detects excessive belief
deviation and truncates trajectories during training to remove uninformative
tails. By preserving credit for informative prefixes, $\mathbf{T^3}$
systematically improves policy optimization. Across 5 challenging tasks,
$\mathbf{T^3}$ consistently enhances training stability, token efficiency, and
final performance, achieving up to 30% gains while cutting rollout tokens by
roughly 25%. These results highlight belief control as a key principle for
developing robust and generalizable LLM-based active reasoners.

</details>


### [70] [Tensor Logic: The Language of AI](https://arxiv.org/abs/2510.12269)
*Pedro Domingos*

Main category: cs.AI

TL;DR: 提出张量逻辑语言，通过将逻辑规则与爱因斯坦求和统一为张量方程，从根本上融合神经和符号AI，解决现有AI编程语言的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有AI工具如PyTorch和TensorFlow缺乏自动推理和知识获取支持，而传统AI语言如LISP和Prolog缺乏可扩展性和学习支持，阻碍了AI发展。

Method: 构建张量逻辑语言，其唯一构造是张量方程，将逻辑规则和爱因斯坦求和视为相同操作，并在此基础上实现各种AI形式。

Result: 优雅实现了包括transformer、形式推理、核机器和图模型在内的关键AI形式，并实现了嵌入空间中的可靠推理。

Conclusion: 张量逻辑结合了神经网络的可扩展性和可学习性与符号推理的可靠性和透明性，为AI的广泛应用提供了潜在基础。

Abstract: Progress in AI is hindered by the lack of a programming language with all the
requisite features. Libraries like PyTorch and TensorFlow provide automatic
differentiation and efficient GPU implementation, but are additions to Python,
which was never intended for AI. Their lack of support for automated reasoning
and knowledge acquisition has led to a long and costly series of hacky attempts
to tack them on. On the other hand, AI languages like LISP an Prolog lack
scalability and support for learning. This paper proposes tensor logic, a
language that solves these problems by unifying neural and symbolic AI at a
fundamental level. The sole construct in tensor logic is the tensor equation,
based on the observation that logical rules and Einstein summation are
essentially the same operation, and all else can be reduced to them. I show how
to elegantly implement key forms of neural, symbolic and statistical AI in
tensor logic, including transformers, formal reasoning, kernel machines and
graphical models. Most importantly, tensor logic makes new directions possible,
such as sound reasoning in embedding space. This combines the scalability and
learnability of neural networks with the reliability and transparency of
symbolic reasoning, and is potentially a basis for the wider adoption of AI.

</details>


### [71] [RAG-Anything: All-in-One RAG Framework](https://arxiv.org/abs/2510.12323)
*Zirui Guo,Xubin Ren,Lingrui Xu,Jiahao Zhang,Chao Huang*

Main category: cs.AI

TL;DR: RAG-Anything是一个统一的多模态检索增强生成框架，能够跨文本、视觉、表格和数学表达式等多种模态进行知识检索，解决了现有RAG系统仅限于文本内容的局限性。


<details>
  <summary>Details</summary>
Motivation: 当前RAG系统与真实世界信息环境存在严重不对齐，现代知识库本质上是多模态的，包含文本、视觉元素、结构化表格和数学表达式等多种内容，但现有RAG框架仅限于文本内容，在处理多模态文档时存在根本性差距。

Method: 提出RAG-Anything框架，将多模态内容重新概念化为相互连接的知识实体而非孤立的数据类型。采用双图构建来捕获跨模态关系和文本语义的统一表示，开发了结合结构知识导航和语义匹配的跨模态混合检索方法。

Result: 在具有挑战性的多模态基准测试中表现出优越性能，相比最先进方法取得显著改进。在长文档处理上性能提升尤为明显，传统方法在此类场景下表现不佳。

Conclusion: 该框架为多模态知识访问建立了新范式，消除了当前系统存在的架构碎片化限制，实现了跨所有模态的全面知识检索。

Abstract: Retrieval-Augmented Generation (RAG) has emerged as a fundamental paradigm
for expanding Large Language Models beyond their static training limitations.
However, a critical misalignment exists between current RAG capabilities and
real-world information environments. Modern knowledge repositories are
inherently multimodal, containing rich combinations of textual content, visual
elements, structured tables, and mathematical expressions. Yet existing RAG
frameworks are limited to textual content, creating fundamental gaps when
processing multimodal documents. We present RAG-Anything, a unified framework
that enables comprehensive knowledge retrieval across all modalities. Our
approach reconceptualizes multimodal content as interconnected knowledge
entities rather than isolated data types. The framework introduces dual-graph
construction to capture both cross-modal relationships and textual semantics
within a unified representation. We develop cross-modal hybrid retrieval that
combines structural knowledge navigation with semantic matching. This enables
effective reasoning over heterogeneous content where relevant evidence spans
multiple modalities. RAG-Anything demonstrates superior performance on
challenging multimodal benchmarks, achieving significant improvements over
state-of-the-art methods. Performance gains become particularly pronounced on
long documents where traditional approaches fail. Our framework establishes a
new paradigm for multimodal knowledge access, eliminating the architectural
fragmentation that constrains current systems. Our framework is open-sourced
at: https://github.com/HKUDS/RAG-Anything.

</details>


### [72] [O-Forge: An LLM + Computer Algebra Framework for Asymptotic Analysis](https://arxiv.org/abs/2510.12350)
*Ayush Khaitan,Vijay Ganesh*

Main category: cs.AI

TL;DR: 提出了LLM+CAS框架和O-Forge工具，将前沿大语言模型与计算机代数系统结合，通过符号反馈循环生成既具创造性又经过符号验证的证明，特别适用于渐近不等式的研究。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型在数学研究中验证困难的问题，探索AI如何超越竞赛数学成为专业数学家的研究工具，回答陶哲轩关于LLM与验证器结合能否证明复杂渐近不等式的问题。

Method: 使用LLM+CAS框架，通过符号反馈循环：LLM提出域分解建议，CAS（如Mathematica）对每个部分进行公理化验证，两者协同工作。

Result: 该框架在提出适当域分解方面表现出色，能够有效处理涉及复杂证明和域分解的渐近不等式问题。

Conclusion: LLM+CAS框架成功展示了AI工具在专业数学研究中的应用潜力，能够为渐近分析等研究级问题提供有效支持，推动AI超越竞赛数学向研究工具发展。

Abstract: Large language models have recently demonstrated advanced capabilities in
solving IMO and Putnam problems; yet their role in research mathematics has
remained fairly limited. The key difficulty is verification: suggested proofs
may look plausible, but cannot be trusted without rigorous checking. We present
a framework, called LLM+CAS, and an associated tool, O-Forge, that couples
frontier LLMs with a computer algebra systems (CAS) in an In-Context Symbolic
Feedback loop to produce proofs that are both creative and symbolically
verified. Our focus is on asymptotic inequalities, a topic that often involves
difficult proofs and appropriate decomposition of the domain into the "right"
subdomains. Many mathematicians, including Terry Tao, have suggested that using
AI tools to find the right decompositions can be very useful for research-level
asymptotic analysis. In this paper, we show that our framework LLM+CAS turns
out to be remarkably effective at proposing such decompositions via a
combination of a frontier LLM and a CAS. More precisely, we use an LLM to
suggest domain decomposition, and a CAS (such as Mathematica) that provides a
verification of each piece axiomatically. Using this loop, we answer a question
posed by Terence Tao: whether LLMs coupled with a verifier can be used to help
prove intricate asymptotic inequalities. More broadly, we show how AI can move
beyond contest math towards research-level tools for professional
mathematicians.

</details>


### [73] [A Survey of Vibe Coding with Large Language Models](https://arxiv.org/abs/2510.12399)
*Yuyao Ge,Lingrui Mei,Zenghao Duan,Tianhao Li,Yujia Zheng,Yiwei Wang,Lexin Wang,Jiayu Yao,Tianyu Liu,Yujun Cai,Baolong Bi,Fangda Guo,Jiafeng Guo,Shenghua Liu,Xueqi Cheng*

Main category: cs.AI

TL;DR: 这篇论文对基于大语言模型的Vibe Coding进行了首次系统性综述，通过分析1000多篇研究论文，建立了该领域的理论基础和实践框架，提出了五种开发模型分类。


<details>
  <summary>Details</summary>
Motivation: Vibe Coding作为一种新兴开发范式，开发者通过观察结果而非逐行代码理解来验证AI生成的实现，但其有效性尚未得到充分探索，实证研究显示存在生产力损失和人机协作挑战。

Method: 通过系统分析1000多篇研究论文，将Vibe Coding形式化为约束马尔可夫决策过程，并综合现有实践提出五种开发模型分类。

Result: 成功构建了Vibe Coding的理论基础和实践框架，识别了关键基础设施组件，并揭示了成功Vibe Coding依赖于系统上下文工程、完善的开发环境和人机协作模型。

Conclusion: Vibe Coding的成功不仅取决于智能体能力，更需要系统性的上下文工程、完善的开发环境以及有效的人机协作开发模型。

Abstract: The advancement of large language models (LLMs) has catalyzed a paradigm
shift from code generation assistance to autonomous coding agents, enabling a
novel development methodology termed "Vibe Coding" where developers validate
AI-generated implementations through outcome observation rather than
line-by-line code comprehension. Despite its transformative potential, the
effectiveness of this emergent paradigm remains under-explored, with empirical
evidence revealing unexpected productivity losses and fundamental challenges in
human-AI collaboration. To address this gap, this survey provides the first
comprehensive and systematic review of Vibe Coding with large language models,
establishing both theoretical foundations and practical frameworks for this
transformative development approach. Drawing from systematic analysis of over
1000 research papers, we survey the entire vibe coding ecosystem, examining
critical infrastructure components including LLMs for coding, LLM-based coding
agent, development environment of coding agent, and feedback mechanisms. We
first introduce Vibe Coding as a formal discipline by formalizing it through a
Constrained Markov Decision Process that captures the dynamic triadic
relationship among human developers, software projects, and coding agents.
Building upon this theoretical foundation, we then synthesize existing
practices into five distinct development models: Unconstrained Automation,
Iterative Conversational Collaboration, Planning-Driven, Test-Driven, and
Context-Enhanced Models, thus providing the first comprehensive taxonomy in
this domain. Critically, our analysis reveals that successful Vibe Coding
depends not merely on agent capabilities but on systematic context engineering,
well-established development environments, and human-agent collaborative
development models.

</details>


### [74] [PricingLogic: Evaluating LLMs Reasoning on Complex Tourism Pricing Tasks](https://arxiv.org/abs/2510.12409)
*Yunuo Liu,Dawei Zhu,Zena Al-Khalili,Dai Cheng,Yanjun Chen,Dietrich Klakow,Wei Zhang,Xiaoyu Shen*

Main category: cs.AI

TL;DR: PricingLogic是首个评估LLM在旅游定价任务中可靠性的基准测试，包含300个基于真实定价政策的问题，测试结果显示LLM在复杂定价规则下存在系统性失败。


<details>
  <summary>Details</summary>
Motivation: 旅游机构希望将易出错的定价任务自动化给AI系统，但未经验证的LLM部署可能导致重大财务损失和客户信任危机。

Method: 构建包含300个自然语言问题的基准测试，基于42个真实定价政策，涵盖两个难度级别：基础客户类型定价和涉及交互折扣的捆绑旅游计算。

Result: 对一系列LLM的评估显示，在更难的层级上性能急剧下降，暴露了规则解释和算术推理方面的系统性失败。

Conclusion: 尽管LLM具备通用能力，但在收入关键型应用中，如果没有进一步的安全保障或领域适应，当前的LLM仍然不可靠。

Abstract: We present PricingLogic, the first benchmark that probes whether Large
Language Models(LLMs) can reliably automate tourism-related prices when
multiple, overlapping fare rules apply. Travel agencies are eager to offload
this error-prone task onto AI systems; however, deploying LLMs without verified
reliability could result in significant financial losses and erode customer
trust. PricingLogic comprises 300 natural-language questions based on booking
requests derived from 42 real-world pricing policies, spanning two levels of
difficulty: (i) basic customer-type pricing and (ii)bundled-tour calculations
involving interacting discounts. Evaluations of a line of LLMs reveal a steep
performance drop on the harder tier,exposing systematic failures in rule
interpretation and arithmetic reasoning.These results highlight that, despite
their general capabilities, today's LLMs remain unreliable in revenue-critical
applications without further safeguards or domain adaptation. Our code and
dataset are available at https://github.com/EIT-NLP/PricingLogic.

</details>


### [75] [MTOS: A LLM-Driven Multi-topic Opinion Simulation Framework for Exploring Echo Chamber Dynamics](https://arxiv.org/abs/2510.12423)
*Dingyi Zuo,Hongjie Zhang,Jie Ou,Chaosheng Feng,Shuwan Liu*

Main category: cs.AI

TL;DR: 提出了MTOS多主题意见模拟框架，将多主题情境与LLM结合，通过记忆机制、交互策略和信念衰减机制模拟跨主题观点演化。


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的研究主要关注单一主题，无法捕捉多主题跨领域的认知转移；传统数值模型将复杂语言态度简化为离散值，缺乏可解释性和行为一致性。

Method: MTOS框架整合LLM与短期/长期记忆，采用多种用户选择交互机制和动态主题选择策略，使用信念衰减机制实现跨主题观点更新。

Result: 多主题设置显著改变极化趋势：正相关主题放大回音室效应，负相关主题抑制回音室，无关主题通过资源竞争缓解回音室效应。相比数值模型，LLM智能体能更真实地模拟动态观点变化。

Conclusion: LLM智能体能够真实模拟动态观点变化，再现新闻文本的语言特征，捕捉复杂的人类推理，提高了模拟的可解释性和系统稳定性。

Abstract: The polarization of opinions, information segregation, and cognitive biases
on social media have attracted significant academic attention. In real-world
networks, information often spans multiple interrelated topics, posing
challenges for opinion evolution and highlighting the need for frameworks that
simulate interactions among topics. Existing studies based on large language
models (LLMs) focus largely on single topics, limiting the capture of cognitive
transfer in multi-topic, cross-domain contexts. Traditional numerical models,
meanwhile, simplify complex linguistic attitudes into discrete values, lacking
interpretability, behavioral consistency, and the ability to integrate multiple
topics. To address these issues, we propose Multi-topic Opinion Simulation
(MTOS), a social simulation framework integrating multi-topic contexts with
LLMs. MTOS leverages LLMs alongside short-term and long-term memory,
incorporates multiple user-selection interaction mechanisms and dynamic
topic-selection strategies, and employs a belief decay mechanism to enable
perspective updates across topics. We conduct extensive experiments on MTOS,
varying topic numbers, correlation types, and performing ablation studies to
assess features such as group polarization and local consistency. Results show
that multi-topic settings significantly alter polarization trends: positively
correlated topics amplify echo chambers, negatively correlated topics inhibit
them, and irrelevant topics also mitigate echo chamber effects through resource
competition. Compared with numerical models, LLM-based agents realistically
simulate dynamic opinion changes, reproduce linguistic features of news texts,
and capture complex human reasoning, improving simulation interpretability and
system stability.

</details>


### [76] [Biased-Attention Guided Risk Prediction for Safe Decision-Making at Unsignalized Intersections](https://arxiv.org/abs/2510.12428)
*Chengyang Dong,Nan Guo*

Main category: cs.AI

TL;DR: 提出了一种结合偏置注意力机制的深度强化学习决策框架，用于无信号交叉口的自动驾驶决策，通过风险评估提升安全性和效率。


<details>
  <summary>Details</summary>
Motivation: 无信号交叉口自动驾驶决策面临复杂动态交互和高冲突风险的挑战，需要实现主动安全控制。

Method: 基于Soft Actor-Critic算法，使用偏置注意力构建交通风险预测器，将碰撞风险转化为密集奖励信号来指导决策。

Result: 仿真结果表明该方法有效提高了交叉口的交通效率和车辆安全性。

Conclusion: 该智能决策框架在复杂场景中具有有效性，证明了结合风险评估的强化学习方法在自动驾驶决策中的价值。

Abstract: Autonomous driving decision-making at unsignalized intersections is highly
challenging due to complex dynamic interactions and high conflict risks. To
achieve proactive safety control, this paper proposes a deep reinforcement
learning (DRL) decision-making framework integrated with a biased attention
mechanism. The framework is built upon the Soft Actor-Critic (SAC) algorithm.
Its core innovation lies in the use of biased attention to construct a traffic
risk predictor. This predictor assesses the long-term risk of collision for a
vehicle entering the intersection and transforms this risk into a dense reward
signal to guide the SAC agent in making safe and efficient driving decisions.
Finally, the simulation results demonstrate that the proposed method
effectively improves both traffic efficiency and vehicle safety at the
intersection, thereby proving the effectiveness of the intelligent
decision-making framework in complex scenarios. The code of our work is
available at https://github.com/hank111525/SAC-RWB.

</details>


### [77] [Using Medical Algorithms for Task-Oriented Dialogue in LLM-Based Medical Interviews](https://arxiv.org/abs/2510.12490)
*Rui Reis,Pedro Rangel Henriques,João Ferreira-Coimbra,Eva Oliveira,Nuno F. Rodrigues*

Main category: cs.AI

TL;DR: 开发了一个基于有向无环图的面向任务医疗对话框架，包含问题转换、冷启动、自适应分支、终止逻辑和报告生成等功能模块，在医患应用中均表现出良好的可用性和满意度。


<details>
  <summary>Details</summary>
Motivation: 为医疗领域设计一个系统化的对话框架，将医疗算法和指南转化为临床问题，支持高效的患者访谈和医生友好的报告生成，降低认知负荷并融入临床工作流程。

Method: 采用有向无环图结构组织医疗问题，包含五个核心机制：问题转换流水线、基于层次聚类的冷启动、扩展剪枝的自适应分支、终止逻辑以及结构化报告自动合成。

Result: 患者应用：低认知负荷(15.6)、高可用性(86)、强满意度(8.1/9)；医生应用：中等认知负荷(26)、极佳可用性(88.5)、高满意度(8.3/9)。两者都能有效融入临床工作流程。

Conclusion: 该框架成功实现了高效的医患对话交互，显著降低了认知需求并支持高效报告生成，但存在系统延迟和评估样本有限的问题。

Abstract: We developed a task-oriented dialogue framework structured as a Directed
Acyclic Graph (DAG) of medical questions. The system integrates: (1) a
systematic pipeline for transforming medical algorithms and guidelines into a
clinical question corpus; (2) a cold-start mechanism based on hierarchical
clustering to generate efficient initial questioning without prior patient
information; (3) an expand-and-prune mechanism enabling adaptive branching and
backtracking based on patient responses; (4) a termination logic to ensure
interviews end once sufficient information is gathered; and (5) automated
synthesis of doctor-friendly structured reports aligned with clinical
workflows. Human-computer interaction principles guided the design of both the
patient and physician applications. Preliminary evaluation involved five
physicians using standardized instruments: NASA-TLX (cognitive workload), the
System Usability Scale (SUS), and the Questionnaire for User Interface
Satisfaction (QUIS). The patient application achieved low workload scores
(NASA-TLX = 15.6), high usability (SUS = 86), and strong satisfaction (QUIS =
8.1/9), with particularly high ratings for ease of learning and interface
design. The physician application yielded moderate workload (NASA-TLX = 26) and
excellent usability (SUS = 88.5), with satisfaction scores of 8.3/9. Both
applications demonstrated effective integration into clinical workflows,
reducing cognitive demand and supporting efficient report generation.
Limitations included occasional system latency and a small, non-diverse
evaluation sample.

</details>


### [78] [Artificial Intelligence Virtual Cells: From Measurements to Decisions across Modality, Scale, Dynamics, and Evaluation](https://arxiv.org/abs/2510.12498)
*Chengpeng Hu,Calvin Yu-Chian Chen*

Main category: cs.AI

TL;DR: 提出细胞状态潜在（CSL）视角，通过操作符语法组织学习，强调跨模态、尺度、情境和干预的决策对齐评估，以解决当前AI虚拟细胞模型在跨实验室、平台传输和系统处理剂量、时间效应方面的局限性。


<details>
  <summary>Details</summary>
Motivation: 当前AI虚拟细胞模型虽然在单数据集验证中表现良好，但在跨实验室和平台传输时存在限制，数据分割易受泄漏和覆盖偏差影响，且剂量、时间和组合效应尚未系统处理。跨尺度耦合受限，分子、细胞和组织层面的锚点稀疏，与科学或临床读数的对齐在不同研究中差异较大。

Method: 提出模型无关的细胞状态潜在（CSL）视角，通过操作符语法组织学习：测量、提升/投影用于跨尺度耦合，以及干预用于剂量和调度。强调操作符感知的数据设计、抗泄漏分区和透明校准与报告。

Result: 该视角促成了跨模态、尺度、情境和干预的决策对齐评估蓝图，并强调功能空间读数，如通路活性、空间邻域和临床相关终点。

Conclusion: 通过CSL视角和操作符语法，可以实现可重复的、类似比较，提升AI虚拟细胞模型在跨平台和情境下的泛化能力和实际应用价值。

Abstract: Artificial Intelligence Virtual Cells (AIVCs) aim to learn executable,
decision-relevant models of cell state from multimodal, multiscale
measurements. Recent studies have introduced single-cell and spatial foundation
models, improved cross-modality alignment, scaled perturbation atlases, and
explored pathway-level readouts. Nevertheless, although held-out validation is
standard practice, evaluations remain predominantly within single datasets and
settings; evidence indicates that transport across laboratories and platforms
is often limited, that some data splits are vulnerable to leakage and coverage
bias, and that dose, time and combination effects are not yet systematically
handled. Cross-scale coupling also remains constrained, as anchors linking
molecular, cellular and tissue levels are sparse, and alignment to scientific
or clinical readouts varies across studies. We propose a model-agnostic
Cell-State Latent (CSL) perspective that organizes learning via an operator
grammar: measurement, lift/project for cross-scale coupling, and intervention
for dosing and scheduling. This view motivates a decision-aligned evaluation
blueprint across modality, scale, context and intervention, and emphasizes
function-space readouts such as pathway activity, spatial neighborhoods and
clinically relevant endpoints. We recommend operator-aware data design,
leakage-resistant partitions, and transparent calibration and reporting to
enable reproducible, like-for-like comparisons.

</details>


### [79] [ProtoSiTex: Learning Semi-Interpretable Prototypes for Multi-label Text Classification](https://arxiv.org/abs/2510.12534)
*Utsav Kumar Nareti,Suraj Kumar,Soumya Pandey,Soumi Chattopadhyay,Chandranath Adak*

Main category: cs.AI

TL;DR: ProtoSiTex是一个半可解释框架，用于细粒度多标签文本分类，通过双阶段交替训练策略学习语义连贯且多样化的原型，并在多个基准测试中实现最先进性能。


<details>
  <summary>Details</summary>
Motivation: 用户生成评论的激增需要可解释模型来提供细粒度洞察。现有基于原型的模型通常在粗粒度（句子或文档级别）操作，且无法处理现实世界文本分类的多标签性质。

Method: 采用双阶段交替训练策略：无监督原型发现阶段学习语义连贯且多样化的原型，监督分类阶段将这些原型映射到类别标签。使用分层损失函数在子句、句子和文档级别强制一致性。

Result: 在酒店评论基准数据集和两个公共基准测试（二分类和多分类）上的实验表明，ProtoSiTex实现了最先进的性能，同时提供忠实、与人类对齐的解释。

Conclusion: ProtoSiTex通过自适应原型和多头注意力捕获重叠和冲突语义，为半可解释多标签文本分类提供了一个强大的解决方案。

Abstract: The surge in user-generated reviews has amplified the need for interpretable
models that can provide fine-grained insights. Existing prototype-based models
offer intuitive explanations but typically operate at coarse granularity
(sentence or document level) and fail to address the multi-label nature of
real-world text classification. We propose ProtoSiTex, a semi-interpretable
framework designed for fine-grained multi-label text classification. ProtoSiTex
employs a dual-phase alternating training strategy: an unsupervised prototype
discovery phase that learns semantically coherent and diverse prototypes, and a
supervised classification phase that maps these prototypes to class labels. A
hierarchical loss function enforces consistency across sub-sentence, sentence,
and document levels, enhancing interpretability and alignment. Unlike prior
approaches, ProtoSiTex captures overlapping and conflicting semantics using
adaptive prototypes and multi-head attention. We also introduce a benchmark
dataset of hotel reviews annotated at the sub-sentence level with multiple
labels. Experiments on this dataset and two public benchmarks (binary and
multi-class) show that ProtoSiTex achieves state-of-the-art performance while
delivering faithful, human-aligned explanations, establishing it as a robust
solution for semi-interpretable multi-label text classification.

</details>


### [80] [Inclusive Fitness as a Key Step Towards More Advanced Social Behaviors in Multi-Agent Reinforcement Learning Settings](https://arxiv.org/abs/2510.12555)
*Andries Rosseau,Raphaël Avalos,Ann Nowé*

Main category: cs.AI

TL;DR: 提出基于包容性适应度的多智能体强化学习框架，通过基因共享和遗传相似性驱动合作与竞争，模拟生物进化中的社会动态。


<details>
  <summary>Details</summary>
Motivation: 受自然选择中竞争与合作力量驱动智力进化的启发，旨在创建能产生类似生物进化中复杂社会动态的多智能体系统。

Method: 使用基于基因型的多智能体强化学习框架，将包容性适应度概念建模为奖励函数，在囚徒困境网络游戏中研究社会动态。

Result: 结果与生物学原理（如汉密尔顿法则）一致，展示了基于遗传相似性的合作谱系，产生了非团队性的复杂社会关系。

Conclusion: 基于包容性适应度的奖励机制为涌现更具战略性和社会智能的智能体提供了基础，可扩展到更开放的环境模拟进化军备竞赛。

Abstract: The competitive and cooperative forces of natural selection have driven the
evolution of intelligence for millions of years, culminating in nature's vast
biodiversity and the complexity of human minds. Inspired by this process, we
propose a novel multi-agent reinforcement learning framework where each agent
is assigned a genotype and where reward functions are modelled after the
concept of inclusive fitness. An agent's genetic material may be shared with
other agents, and our inclusive reward function naturally accounts for this. We
study the resulting social dynamics in two types of network games with
prisoner's dilemmas and find that our results align with well-established
principles from biology, such as Hamilton's rule. Furthermore, we outline how
this framework can extend to more open-ended environments with spatial and
temporal structure, finite resources, and evolving populations. We hypothesize
the emergence of an arms race of strategies, where each new strategy is a
gradual improvement over earlier adaptations of other agents, effectively
producing a multi-agent autocurriculum analogous to biological evolution. In
contrast to the binary team-based structures prevalent in earlier research, our
gene-based reward structure introduces a spectrum of cooperation ranging from
full adversity to full cooperativeness based on genetic similarity, enabling
unique non team-based social dynamics. For example, one agent having a mutual
cooperative relationship with two other agents, while the two other agents
behave adversarially towards each other. We argue that incorporating inclusive
fitness in agents provides a foundation for the emergence of more strategically
advanced and socially intelligent agents.

</details>


### [81] [HardcoreLogic: Challenging Large Reasoning Models with Long-tail Logic Puzzle Games](https://arxiv.org/abs/2510.12563)
*Jingcong Liang,Shijun Wan,Xuehai Wu,Siyuan Wang,Yitong Li,Qianglong Chen,Duyu Tang,Zhongyu Wei*

Main category: cs.AI

TL;DR: 提出了HardcoreLogic基准测试，包含5000+个10种逻辑游戏的谜题，通过增加复杂性、非常规元素和不可解谜题三个维度来测试大型推理模型的鲁棒性，揭示了当前模型对记忆模式的依赖而非真正推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试主要关注如9x9数独等流行谜题，可能导致模型过拟合到标准格式并记忆解法模式，无法真正理解新颖规则或适应新变体。需要评估模型在非标准游戏变体中的灵活推理能力。

Method: 构建HardcoreLogic基准，包含10种游戏的5000+谜题，通过三个维度系统化转换标准谜题：增加复杂性(IC)、非常规元素(UE)和不可解谜题(UP)，减少对捷径记忆的依赖。

Result: 评估显示各种LRM模型性能显著下降，即使在现有基准上表现最佳的模型也严重依赖记忆的刻板模式。增加复杂性是主要困难来源，但模型在细微规则变化上也表现不佳。

Conclusion: HardcoreLogic暴露了当前LRM的局限性，为推进高级逻辑推理建立了基准，表明需要发展真正的推理能力而非依赖记忆模式。

Abstract: Large Reasoning Models (LRMs) have demonstrated impressive performance on
complex tasks, including logical puzzle games that require deriving solutions
satisfying all constraints. However, whether they can flexibly apply
appropriate rules to varying conditions, particularly when faced with
non-canonical game variants, remains an open question. Existing corpora focus
on popular puzzles like 9x9 Sudoku, risking overfitting to canonical formats
and memorization of solution patterns, which can mask deficiencies in
understanding novel rules or adapting strategies to new variants. To address
this, we introduce HardcoreLogic, a challenging benchmark of over 5,000 puzzles
across 10 games, designed to test the robustness of LRMs on the "long-tail" of
logical games. HardcoreLogic systematically transforms canonical puzzles
through three dimensions: Increased Complexity (IC), Uncommon Elements (UE),
and Unsolvable Puzzles (UP), reducing reliance on shortcut memorization.
Evaluations on a diverse set of LRMs reveal significant performance drops, even
for models achieving top scores on existing benchmarks, indicating heavy
reliance on memorized stereotypes. While increased complexity is the dominant
source of difficulty, models also struggle with subtle rule variations that do
not necessarily increase puzzle difficulty. Our systematic error analysis on
solvable and unsolvable puzzles further highlights gaps in genuine reasoning.
Overall, HardcoreLogic exposes the limitations of current LRMs and establishes
a benchmark for advancing high-level logical reasoning.

</details>


### [82] [Memory as Action: Autonomous Context Curation for Long-Horizon Agentic Tasks](https://arxiv.org/abs/2510.12635)
*Yuxiang Zhang,Jiangming Shu,Ye Ma,Xueyuan Lin,Shangxi Wu,Jitao Sang*

Main category: cs.AI

TL;DR: 提出了Memory-as-Action框架，将工作内存管理重新定义为可学习的内在能力，通过强化学习联合优化任务推理和内存管理，在资源约束下平衡内存整理与长期任务目标。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在长视野智能体任务中面临内存受限问题，现有工作内存方法依赖与核心策略解耦的外部启发式机制，需要更紧密集成的解决方案。

Method: 提出Memory-as-Action框架，智能体通过执行显式编辑操作主动管理工作内存，作为统一策略的一部分。为解决轨迹断裂问题，提出动态上下文策略优化算法。

Result: 端到端联合优化不仅减少了总体计算消耗，还提高了任务性能，通过适应模型内在能力的自适应上下文整理策略驱动。

Conclusion: 将内存管理作为可学习的内在能力进行端到端优化，能够更好地平衡内存整理与任务目标，提升智能体在长视野任务中的表现。

Abstract: Large Language Models face challenges in long-horizon agentic tasks as their
constrained memory is easily overwhelmed by distracting or irrelevant context.
Existing working memory methods typically rely on external, heuristic
mechanisms that are decoupled from the agent's core policy. In this work, we
reframe working memory management as a learnable, intrinsic capability. We
propose a novel framework, Memory-as-Action, where an agent actively manages
its working memory by executing explicit editing operations as part of a
unified policy. This formulation allows an agent, trained via reinforcement
learning, to balance memory curation against long-term task objectives under
given resource constraints. However, such memory editing actions break the
standard assumption of a continuously growing prefix in LLM interactions,
leading to what we call trajectory fractures. These non-prefix changes disrupt
the causal continuity required by standard policy gradient methods, making
those methods inapplicable. To address this, we propose a new algorithm,
Dynamic Context Policy Optimization, which enables stable end-to-end
reinforcement learning by segmenting trajectories at memory action points and
applying trajectory-level advantages to the resulting action segments. Our
results demonstrate that jointly optimizing for task reasoning and memory
management in an end-to-end fashion not only reduces overall computational
consumption but also improves task performance, driven by adaptive context
curation strategies tailored to the model's intrinsic capabilities.

</details>


### [83] [ERA: Transforming VLMs into Embodied Agents via Embodied Prior Learning and Online Reinforcement Learning](https://arxiv.org/abs/2510.12693)
*Hanyang Chen,Mark Zhao,Rui Yang,Qinwei Ma,Ke Yang,Jiarui Yao,Kangrui Wang,Hao Bai,Zhenhailong Wang,Rui Pan,Mengchao Zhang,Jose Barreiros,Aykut Onol,ChengXiang Zhai,Heng Ji,Manling Li,Huan Zhang,Tong Zhang*

Main category: cs.AI

TL;DR: ERA是一个两阶段框架，通过先验知识学习和在线强化学习，使小型视觉语言模型在具身AI任务中超越大型模型。


<details>
  <summary>Details</summary>
Motivation: 解决高性能具身AI系统依赖昂贵大型模型，而小型模型缺乏必要知识和技能的问题。

Method: 第一阶段：具身先验学习，从轨迹增强先验、环境锚定先验和外部知识先验中提取知识；第二阶段：在线强化学习，采用自总结、密集奖励塑造和轮级策略优化。

Result: ERA-3B在EB-ALFRED任务上比GPT-4o提升8.4%，在EB-Manipulation任务上提升19.4%，并在未见任务上表现出强泛化能力。

Conclusion: ERA为可扩展的具身智能提供了实用路径，为未来具身AI系统提供了方法论见解。

Abstract: Recent advances in embodied AI highlight the potential of vision language
models (VLMs) as agents capable of perception, reasoning, and interaction in
complex environments. However, top-performing systems rely on large-scale
models that are costly to deploy, while smaller VLMs lack the necessary
knowledge and skills to succeed. To bridge this gap, we present
\textit{Embodied Reasoning Agent (ERA)}, a two-stage framework that integrates
prior knowledge learning and online reinforcement learning (RL). The first
stage, \textit{Embodied Prior Learning}, distills foundational knowledge from
three types of data: (1) Trajectory-Augmented Priors, which enrich existing
trajectory data with structured reasoning generated by stronger models; (2)
Environment-Anchored Priors, which provide in-environment knowledge and
grounding supervision; and (3) External Knowledge Priors, which transfer
general knowledge from out-of-environment datasets. In the second stage, we
develop an online RL pipeline that builds on these priors to further enhance
agent performance. To overcome the inherent challenges in agent RL, including
long horizons, sparse rewards, and training instability, we introduce three key
designs: self-summarization for context management, dense reward shaping, and
turn-level policy optimization. Extensive experiments on both high-level
planning (EB-ALFRED) and low-level control (EB-Manipulation) tasks demonstrate
that ERA-3B surpasses both prompting-based large models and previous
training-based baselines. Specifically, it achieves overall improvements of
8.4\% on EB-ALFRED and 19.4\% on EB-Manipulation over GPT-4o, and exhibits
strong generalization to unseen tasks. Overall, ERA offers a practical path
toward scalable embodied intelligence, providing methodological insights for
future embodied AI systems.

</details>


### [84] [Multi-Agent Debate for LLM Judges with Adaptive Stability Detection](https://arxiv.org/abs/2510.12697)
*Tianyu Hu,Zhen Tan,Song Wang,Huaizhi Qu,Tianlong Chen*

Main category: cs.AI

TL;DR: 提出多智能体辩论框架，通过协作推理和迭代优化提升LLM作为评判者的准确性，引入稳定性检测机制提高效率


<details>
  <summary>Details</summary>
Motivation: 当前LLM作为评判者的方法依赖简单聚合（如多数投票），即使个体智能体给出正确答案也可能失败，需要更有效的协作机制

Method: 多智能体辩论框架，数学形式化辩论过程，引入基于时变Beta-Binomial混合的稳定性检测机制，使用Kolmogorov-Smirnov检验进行自适应停止

Result: 在多个基准测试和模型上，该框架相比多数投票提高了评判准确性，同时保持了计算效率

Conclusion: 多智能体辩论框架能有效提升LLM评判任务的准确性，通过协作推理和自适应停止机制实现性能与效率的平衡

Abstract: With advancements in reasoning capabilities, Large Language Models (LLMs) are
increasingly employed for automated judgment tasks. While LLMs-as-Judges offer
promise in automating evaluations, current approaches often rely on simplistic
aggregation methods (e.g., majority voting), which can fail even when
individual agents provide correct answers. To address this, we propose a
multi-agent debate judge framework where agents collaboratively reason and
iteratively refine their responses. We formalize the debate process
mathematically, analyzing agent interactions and proving that debate amplifies
correctness compared to static ensembles. To enhance efficiency, we introduce a
stability detection mechanism that models judge consensus dynamics via a
time-varying Beta-Binomial mixture, with adaptive stopping based on
distributional similarity (Kolmogorov-Smirnov test). This mechanism models the
judges' collective correct rate dynamics using a time-varying mixture of
Beta-Binomial distributions and employs an adaptive stopping criterion based on
distributional similarity (Kolmogorov-Smirnov statistic). Experiments across
multiple benchmarks and models demonstrate that our framework improves judgment
accuracy over majority voting while maintaining computational efficiency.

</details>


### [85] [CAMNet: Leveraging Cooperative Awareness Messages for Vehicle Trajectory Prediction](https://arxiv.org/abs/2510.12703)
*Mattia Grasselli,Angelo Porrello,Carlo Augusto Grazia*

Main category: cs.AI

TL;DR: 该论文研究了利用车辆间通信数据（CAM）进行车辆轨迹预测，提出了一种基于图神经网络的模型CAMNet，并在真实CAM数据集上验证了该方法的有效性。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶面临传感器视野受限的问题，车辆间通信（如CAM）可以共享信息，提高环境感知能力，特别是在传感器被遮挡的情况下。

Method: 设计了基于合作意识消息的图神经网络（CAMNet），在运动预测数据集上训练，并在专门创建的CAM数据集上进行评估。

Result: 方法显示出有希望的结果，证明CAM数据确实可以支持车辆轨迹预测。

Conclusion: CAM数据可用于车辆轨迹预测，但方法存在一些局限性，为未来研究提供了机会。

Abstract: Autonomous driving remains a challenging task, particularly due to safety
concerns. Modern vehicles are typically equipped with expensive sensors such as
LiDAR, cameras, and radars to reduce the risk of accidents. However, these
sensors face inherent limitations: their field of view and line of sight can be
obstructed by other vehicles, thereby reducing situational awareness. In this
context, vehicle-to-vehicle communication plays a crucial role, as it enables
cars to share information and remain aware of each other even when sensors are
occluded. One way to achieve this is through the use of Cooperative Awareness
Messages (CAMs). In this paper, we investigate the use of CAM data for vehicle
trajectory prediction. Specifically, we design and train a neural network,
Cooperative Awareness Message-based Graph Neural Network (CAMNet), on a widely
used motion forecasting dataset. We then evaluate the model on a second dataset
that we created from scratch using Cooperative Awareness Messages, in order to
assess whether this type of data can be effectively exploited. Our approach
demonstrates promising results, showing that CAMs can indeed support vehicle
trajectory prediction. At the same time, we discuss several limitations of the
approach, which highlight opportunities for future research.

</details>


### [86] [Towards Robust Artificial Intelligence: Self-Supervised Learning Approach for Out-of-Distribution Detection](https://arxiv.org/abs/2510.12713)
*Wissam Salhab,Darine Ameyed,Hamid Mcheick,Fehmi Jaafar*

Main category: cs.AI

TL;DR: 提出一种无需标记数据即可改进OOD检测的方法，通过自监督学习和图论技术提高AI系统的鲁棒性，在AUROC指标上达到0.99。


<details>
  <summary>Details</summary>
Motivation: 在自动驾驶、医疗等安全关键系统中，AI系统需要在各种条件下保持可靠性能，包括处理分布外样本、对抗攻击和环境变化，系统故障可能带来严重后果。

Method: 结合自监督学习和图论技术，从无标签数据中学习有用表示，从而更有效地识别和分类OOD样本。

Result: 与现有最先进方法相比，该方法在AUROC指标上达到0.99的优异表现。

Conclusion: 该方法能够显著提高AI系统的鲁棒性，特别是在处理分布外样本方面，且无需依赖标记数据。

Abstract: Robustness in AI systems refers to their ability to maintain reliable and
accurate performance under various conditions, including out-of-distribution
(OOD) samples, adversarial attacks, and environmental changes. This is crucial
in safety-critical systems, such as autonomous vehicles, transportation, or
healthcare, where malfunctions could have severe consequences. This paper
proposes an approach to improve OOD detection without the need of labeled data,
thereby increasing the AI systems' robustness. The proposed approach leverages
the principles of self-supervised learning, allowing the model to learn useful
representations from unlabeled data. Combined with graph-theoretical
techniques, this enables the more efficient identification and categorization
of OOD samples. Compared to existing state-of-the-art methods, this approach
achieved an Area Under the Receiver Operating Characteristic Curve (AUROC) =
0.99.

</details>


### [87] [Clutch Control: An Attention-based Combinatorial Bandit for Efficient Mutation in JavaScript Engine Fuzzing](https://arxiv.org/abs/2510.12732)
*Myles Foley,Sergio Maffeis,Muhammad Fakhrur Rozi,Takeshi Takahashi*

Main category: cs.AI

TL;DR: CLUTCH是一种基于深度组合多臂老虎机的JavaScript模糊测试方法，通过注意力机制和Concrete Dropout动态调整探索策略，相比现有方法提高了测试用例有效性和覆盖率。


<details>
  <summary>Details</summary>
Motivation: 现有JavaScript模糊测试方法使用随机选择进行代码变异，效率不高。作者认为选择更好的变异目标适合使用组合多臂老虎机方法来解决。

Method: 提出CLUTCH深度组合多臂老虎机，使用注意力机制处理变长JavaScript测试用例表示，并通过Concrete Dropout动态调整探索策略。

Result: CLUTCH相比三种最先进方法，平均提高有效测试用例数量20.3%，每个测试用例覆盖率8.9%。在波动和组合设置中，后悔值分别减少至少78.1%和4.1%。

Conclusion: CLUTCH在JavaScript模糊测试中显著提高了效率，证明了组合多臂老虎机方法在代码变异目标选择问题上的有效性。

Abstract: JavaScript engines are widely used in web browsers, PDF readers, and
server-side applications. The rise in concern over their security has led to
the development of several targeted fuzzing techniques. However, existing
approaches use random selection to determine where to perform mutations in
JavaScript code. We postulate that the problem of selecting better mutation
targets is suitable for combinatorial bandits with a volatile number of arms.
Thus, we propose CLUTCH, a novel deep combinatorial bandit that can observe
variable length JavaScript test case representations, using an attention
mechanism from deep learning. Furthermore, using Concrete Dropout, CLUTCH can
dynamically adapt its exploration. We show that CLUTCH increases efficiency in
JavaScript fuzzing compared to three state-of-the-art solutions by increasing
the number of valid test cases and coverage-per-testcase by, respectively,
20.3% and 8.9% on average. In volatile and combinatorial settings we show that
CLUTCH outperforms state-of-the-art bandits, achieving at least 78.1% and 4.1%
less regret in volatile and combinatorial settings, respectively.

</details>


### [88] [CTRL-Rec: Controlling Recommender Systems With Natural Language](https://arxiv.org/abs/2510.12742)
*Micah Carroll,Adeline Foote,Kevin Feng,Marcus Williams,Anca Dragan,W. Bradley Knox,Smitha Milli*

Main category: cs.AI

TL;DR: CTRL-Rec是一种让用户通过自然语言实时控制推荐系统的方法，使用LLM模拟用户对推荐项目的批准判断，训练嵌入模型来近似这些判断，并将其集成到传统推荐系统的信号加权中。


<details>
  <summary>Details</summary>
Motivation: 当用户对推荐系统不满意时，他们通常缺乏细粒度的控制来改变推荐结果。LLM提供了解决方案，允许用户通过自然语言请求来指导推荐。

Method: 在训练时使用LLM模拟用户基于语言请求对项目的批准判断，训练嵌入模型来近似这些模拟判断，并将基于用户请求的预测集成到传统推荐系统的标准信号加权中。部署时每个用户请求只需一次LLM嵌入计算。

Result: 在MovieLens数据集上的实验表明，该方法在各种请求下都能实现细粒度控制。在19名Letterboxd用户的研究中，CTRL-Rec受到用户积极评价，与传统控制相比显著增强了用户的控制感和满意度。

Conclusion: CTRL-Rec方法能够有效地实现自然语言对推荐系统的实时控制，提升用户体验和满意度。

Abstract: When users are dissatisfied with recommendations from a recommender system,
they often lack fine-grained controls for changing them. Large language models
(LLMs) offer a solution by allowing users to guide their recommendations
through natural language requests (e.g., "I want to see respectful posts with a
different perspective than mine"). We propose a method, CTRL-Rec, that allows
for natural language control of traditional recommender systems in real-time
with computational efficiency. Specifically, at training time, we use an LLM to
simulate whether users would approve of items based on their language requests,
and we train embedding models that approximate such simulated judgments. We
then integrate these user-request-based predictions into the standard weighting
of signals that traditional recommender systems optimize. At deployment time,
we require only a single LLM embedding computation per user request, allowing
for real-time control of recommendations. In experiments with the MovieLens
dataset, our method consistently allows for fine-grained control across a
diversity of requests. In a study with 19 Letterboxd users, we find that
CTRL-Rec was positively received by users and significantly enhanced users'
sense of control and satisfaction with recommendations compared to traditional
controls.

</details>


### [89] [Ax-Prover: A Deep Reasoning Agentic Framework for Theorem Proving in Mathematics and Quantum Physics](https://arxiv.org/abs/2510.12787)
*Marco Del Tredici,Jacob McCarran,Benjamin Breen,Javier Aspuru Mijares,Weichen Winston Yin,Jacob M. Taylor,Frank Koppens,Dirk Englund*

Main category: cs.AI

TL;DR: Ax-Prover是一个基于多智能体系统的自动定理证明器，能够在Lean中解决跨科学领域的问题，既能自主运行也能与人类专家协作。


<details>
  <summary>Details</summary>
Motivation: 解决科学问题需要结合创造性推理和严格的形式化验证，现有专用系统难以泛化到不同科学领域。

Method: 通过模型上下文协议(MCP)为大型语言模型(LLMs)配备Lean工具，将LLMs的知识推理能力与形式化验证工具相结合。

Result: 在公开数学基准测试中与最先进证明器竞争，在新引入的抽象代数和量子理论基准测试中大幅超越现有方法。

Conclusion: 基于工具的多智能体定理证明方法为跨领域形式化验证提供了可泛化的通用解决方案，并在实际应用中成功协助专家数学家完成复杂密码学定理的形式化证明。

Abstract: We present Ax-Prover, a multi-agent system for automated theorem proving in
Lean that can solve problems across diverse scientific domains and operate
either autonomously or collaboratively with human experts. To achieve this,
Ax-Prover approaches scientific problem solving through formal proof
generation, a process that demands both creative reasoning and strict syntactic
rigor. Ax-Prover meets this challenge by equipping Large Language Models
(LLMs), which provide knowledge and reasoning, with Lean tools via the Model
Context Protocol (MCP), which ensure formal correctness. To evaluate its
performance as an autonomous prover, we benchmark our approach against frontier
LLMs and specialized prover models on two public math benchmarks and on two
Lean benchmarks we introduce in the fields of abstract algebra and quantum
theory. On public datasets, Ax-Prover is competitive with state-of-the-art
provers, while it largely outperform them on the new benchmarks. This shows
that, unlike specialized systems that struggle to generalize, our tool-based
agentic theorem prover approach offers a generalizable methodology for formal
verification across diverse scientific domains. Furthermore, we demonstrate
Ax-Prover's assistant capabilities in a practical use case, showing how it
enabled an expert mathematician to formalize the proof of a complex
cryptography theorem.

</details>
