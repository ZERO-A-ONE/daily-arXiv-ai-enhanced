<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 19]
- [cs.CR](#cs.CR) [Total: 11]
- [cs.AI](#cs.AI) [Total: 25]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [Git Context Controller: Manage the Context of LLM-based Agents like Git](https://arxiv.org/abs/2508.00031)
*Junde Wu*

Main category: cs.SE

TL;DR: 论文提出了一种名为GCC的结构化上下文管理框架，用于解决大型语言模型代理在长期任务中的上下文管理瓶颈问题。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型代理在长期任务（如大型软件开发）中的部署，上下文管理成为关键瓶颈。

Method: GCC框架受软件版本控制系统启发，将代理的上下文管理结构化为类似Git的版本化内存层次，支持COMMIT、BRANCH、MERGE和CONTEXT等操作。

Result: 实验表明，配备GCC的代理在SWE-Bench-Lite基准测试中表现优异，解决了48.00%的软件错误，优于26个竞争系统。在自我复制的案例中，GCC代理的任务解决率为40.7%，远高于未使用GCC的11.7%。

Conclusion: GCC框架显著提升了代理在长期任务中的上下文管理能力，支持目标管理、实验隔离和跨会话内存恢复。

Abstract: Large language model (LLM) based agents have shown impressive capabilities by
interleaving internal reasoning with external tool use. However, as these
agents are deployed in long-horizon workflows, such as coding for a big,
long-term project, context management becomes a critical bottleneck. We
introduce Git-Context-Controller (GCC), a structured context management
framework inspired by software version control systems. GCC elevates context as
versioned memory hierarchy like Git. It structures agent memory as a persistent
file system with explicit operations: COMMIT, BRANCH, MERGE, and CONTEXT,
enabling milestone-based checkpointing, exploration of alternative plans, and
structured reflection. Our approach empowers agents to manage long-term goals,
isolate architectural experiments, and recover or hand off memory across
sessions and agents. Empirically, agents equipped with GCC achieve
state-of-the-art performance on the SWE-Bench-Lite benchmark, resolving 48.00
of software bugs, outperforming 26 competitive systems. In a self-replication
case study, a GCC-augmented agent builds a new CLI agent from scratch,
achieving 40.7 task resolution, compared to only 11.7 without GCC. The code is
released at: https://github.com/theworldofagents/GCC

</details>


### [2] [GPT-4.1 Sets the Standard in Automated Experiment Design Using Novel Python Libraries](https://arxiv.org/abs/2508.00033)
*Nuno Fachada,Daniel Fernandes,Carlos M. Fernandes,Bruno D. Ferreira-Saraiva,João P. Matos-Carvalho*

Main category: cs.SE

TL;DR: 该研究系统评估了大型语言模型（LLMs）在生成功能性Python代码时的表现，发现仅有少数模型（如GPT-4.1）能稳定生成正确代码，并揭示了第三方库文档和实现的不足。


<details>
  <summary>Details</summary>
Motivation: 评估LLMs在生成复杂科学实验代码时的能力，特别是在使用不熟悉的Python API时的表现。

Method: 通过零样本提示测试LLMs生成代码的功能正确性和提示符合性，实验包括使用ParShift库的对话数据分析和pyclugen与scikit-learn的合成数据生成与聚类。

Result: 仅少数模型（如GPT-4.1）能稳定生成正确代码，同时发现了第三方库的文档和实现问题。

Conclusion: LLMs在端到端科学自动化中存在局限性，需改进提示设计、库文档和模型能力。

Abstract: Large Language Models (LLMs) have advanced rapidly as tools for automating
code generation in scientific research, yet their ability to interpret and use
unfamiliar Python APIs for complex computational experiments remains poorly
characterized. This study systematically benchmarks a selection of
state-of-the-art LLMs in generating functional Python code for two increasingly
challenging scenarios: conversational data analysis with the \textit{ParShift}
library, and synthetic data generation and clustering using \textit{pyclugen}
and \textit{scikit-learn}. Both experiments use structured, zero-shot prompts
specifying detailed requirements but omitting in-context examples. Model
outputs are evaluated quantitatively for functional correctness and prompt
compliance over multiple runs, and qualitatively by analyzing the errors
produced when code execution fails. Results show that only a small subset of
models consistently generate correct, executable code, with GPT-4.1 standing
out as the only model to always succeed in both tasks. In addition to
benchmarking LLM performance, this approach helps identify shortcomings in
third-party libraries, such as unclear documentation or obscure implementation
bugs. Overall, these findings highlight current limitations of LLMs for
end-to-end scientific automation and emphasize the need for careful prompt
design, comprehensive library documentation, and continued advances in language
model capabilities.

</details>


### [3] [Machine Learning Pipeline for Software Engineering: A Systematic Literature Review](https://arxiv.org/abs/2508.00045)
*Samah Kansab*

Main category: cs.SE

TL;DR: 本文通过系统文献综述（SLR）探讨了机器学习（ML）在软件工程（SE）中的应用，总结了最佳实践、挑战和不足，强调了稳健的ML流程对提升软件质量和效率的重要性。


<details>
  <summary>Details</summary>
Motivation: 随着软件系统复杂性增加，传统方法难以应对开发周期中的质量与效率问题，ML被视为关键解决方案。

Method: 通过SLR分析ML流程（数据收集、预处理、特征工程等），评估不同算法和验证技术。

Result: 发现稳健预处理（如SMOTE、SZZ）、集成方法（如随机森林）和验证技术（如自助法）显著提升模型性能。

Conclusion: 设计良好的ML流程对解决SE挑战至关重要，为研究与实践提供了优化方向，并推动ML在复杂开发环境中的创新。

Abstract: The rapid advancement of software development practices has introduced
challenges in ensuring quality and efficiency across the software engineering
(SE) lifecycle. As SE systems grow in complexity, traditional approaches often
fail to scale, resulting in longer debugging times, inefficient defect
detection, and resource-heavy development cycles. Machine Learning (ML) has
emerged as a key solution, enabling automation in tasks such as defect
prediction, code review, and release quality estimation. However, the
effectiveness of ML in SE depends on the robustness of its pipeline, including
data collection, preprocessing, feature engineering, algorithm selection,
validation, and evaluation.
  This systematic literature review (SLR) examines state-of-the-art ML
pipelines designed for SE, consolidating best practices, challenges, and gaps.
Our findings show that robust preprocessing, such as SMOTE for data balancing
and SZZ-based algorithms for feature selection, improves model reliability.
Ensemble methods like Random Forest and Gradient Boosting dominate performance
across tasks, while simpler models such as Naive Bayes remain valuable for
efficiency and interpretability. Evaluation metrics including AUC, F1-score,
and precision are most common, with new metrics like Best Arithmetic Mean (BAM)
emerging in niche applications. Validation techniques such as bootstrapping are
widely used to ensure model stability and generalizability.
  This SLR highlights the importance of well-designed ML pipelines for
addressing SE challenges and provides actionable insights for researchers and
practitioners seeking to optimize software quality and efficiency. By
identifying gaps and trends, this study sets a foundation for advancing ML
adoption and fostering innovation in increasingly complex development
environments.

</details>


### [4] [A Survey on Code Generation with LLM-based Agents](https://arxiv.org/abs/2508.00083)
*Yihong Dong,Xue Jiang,Jiaru Qian,Tian Wang,Kechi Zhang,Zhi Jin,Ge Li*

Main category: cs.SE

TL;DR: 本文系统综述了基于大语言模型（LLM）的代码生成代理，探讨了其自主性、任务范围扩展和工程实用性增强三大核心特征，并总结了技术发展、应用、评估工具及未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 研究基于LLM的代码生成代理，因其在软件开发生命周期（SDLC）中的广泛应用潜力及对工程实践的显著提升。

Method: 通过系统调查和分类技术发展轨迹、核心架构（单代理与多代理）、SDLC应用、评估指标及工具。

Result: 总结了该领域的快速发展、核心技术和应用潜力，并提出了主流评估方法和工具。

Conclusion: 分析了当前挑战，提出了未来基础性、长期的研究方向。

Abstract: Code generation agents powered by large language models (LLMs) are
revolutionizing the software development paradigm. Distinct from previous code
generation techniques, code generation agents are characterized by three core
features. 1) Autonomy: the ability to independently manage the entire workflow,
from task decomposition to coding and debugging. 2) Expanded task scope:
capabilities that extend beyond generating code snippets to encompass the full
software development lifecycle (SDLC). 3) Enhancement of engineering
practicality: a shift in research emphasis from algorithmic innovation toward
practical engineering challenges, such as system reliability, process
management, and tool integration. This domain has recently witnessed rapid
development and an explosion in research, demonstrating significant application
potential. This paper presents a systematic survey of the field of LLM-based
code generation agents. We trace the technology's developmental trajectory from
its inception and systematically categorize its core techniques, including both
single-agent and multi-agent architectures. Furthermore, this survey details
the applications of LLM-based agents across the full SDLC, summarizes
mainstream evaluation benchmarks and metrics, and catalogs representative
tools. Finally, by analyzing the primary challenges, we identify and propose
several foundational, long-term research directions for the future work of the
field.

</details>


### [5] [How Quantization Impacts Privacy Risk on LLMs for Code?](https://arxiv.org/abs/2508.00128)
*Md Nazmul Haque,Hua Yang,Zhou Yang,Bowen Xu*

Main category: cs.SE

TL;DR: 量化技术对代码大语言模型（LLMs4Code）的隐私风险影响研究，发现量化能显著降低隐私风险，同时揭示了任务性能与隐私风险的正相关性。


<details>
  <summary>Details</summary>
Motivation: 量化技术广泛用于降低大模型的计算成本，但其对隐私风险的影响尚不明确，尤其是在代码大语言模型中。

Method: 采用静态和动态量化技术，对Pythia、CodeGen和GPTNeo三种模型进行实验，评估任务性能和隐私风险。

Result: 量化显著降低了隐私风险，且任务性能与隐私风险呈正相关；量化大模型可能比小模型更优。

Conclusion: 量化能有效平衡隐私风险与性能，为部署压缩后的代码大语言模型提供实用指导。

Abstract: Large language models for code (LLMs4Code) rely heavily on massive training
data, including sensitive data, such as cloud service credentials of the
projects and personal identifiable information of the developers, raising
serious privacy concerns. Membership inference (MI) has recently emerged as an
effective tool for assessing privacy risk by identifying whether specific data
belong to a model's training set. In parallel, model compression techniques,
especially quantization, have gained traction for reducing computational costs
and enabling the deployment of large models. However, while quantized models
still retain knowledge learned from the original training data, it remains
unclear whether quantization affects their ability to retain and expose privacy
information. Answering this question is of great importance to understanding
privacy risks in real-world deployments. In this work, we conduct the first
empirical study on how quantization influences task performance and privacy
risk simultaneously in LLMs4Code. To do this, we implement widely used
quantization techniques (static and dynamic) to three representative model
families, namely Pythia, CodeGen, and GPTNeo. Our results demonstrate that
quantization has a significant impact on reducing the privacy risk relative to
the original model. We also uncover a positive correlation between task
performance and privacy risk, indicating an underlying tradeoff. Moreover, we
reveal the possibility that quantizing larger models could yield better balance
than using full-precision small models. Finally, we demonstrate that these
findings generalize across different architectures, model sizes and MI methods,
offering practical guidance for safeguarding privacy when deploying compressed
LLMs4Code.

</details>


### [6] [Testing the Untestable? An Empirical Study on the Testing Process of LLM-Powered Software Systems](https://arxiv.org/abs/2508.00198)
*Cleyton Magalhaes,Italo Santos,Brody Stuart-Verner,Ronnie de Souza Santos*

Main category: cs.SE

TL;DR: 研究探讨了在现实应用开发中如何测试集成大语言模型（LLM）的系统，发现测试策略结合了手动和自动化方法，并面临模型行为不确定等挑战。


<details>
  <summary>Details</summary>
Motivation: 尽管LLM在软件工程中广泛应用，但对其集成系统的测试方法研究有限，本研究旨在填补这一空白。

Method: 通过分析99份学生报告，采用主题分析和结构化编码进行探索性案例研究。

Result: 测试策略包括探索性测试、单元测试和提示迭代，挑战包括集成失败、输出不可预测等。

Conclusion: 测试LLM系统需结合传统验证方法和行为感知评估，为生成组件测试提供了实践依据。

Abstract: Background: Software systems powered by large language models are becoming a
routine part of everyday technologies, supporting applications across a wide
range of domains. In software engineering, many studies have focused on how
LLMs support tasks such as code generation, debugging, and documentation.
However, there has been limited focus on how full systems that integrate LLMs
are tested during development. Aims: This study explores how LLM-powered
systems are tested in the context of real-world application development.
Method: We conducted an exploratory case study using 99 individual reports
written by students who built and deployed LLM-powered applications as part of
a university course. Each report was independently analyzed using thematic
analysis, supported by a structured coding process. Results: Testing strategies
combined manual and automated methods to evaluate both system logic and model
behavior. Common practices included exploratory testing, unit testing, and
prompt iteration. Reported challenges included integration failures,
unpredictable outputs, prompt sensitivity, hallucinations, and uncertainty
about correctness. Conclusions: Testing LLM-powered systems required
adaptations to traditional verification methods, blending source-level
reasoning with behavior-aware evaluations. These findings provide evidence on
the practical context of testing generative components in software systems.

</details>


### [7] [Functional vs. Object-Oriented: Comparing How Programming Paradigms Affect the Architectural Characteristics of Systems](https://arxiv.org/abs/2508.00244)
*Briza Mel Dias de Sousa,Renato Cordeiro Ferreira,Alfredo Goldman*

Main category: cs.SE

TL;DR: 比较面向对象编程（OOP）和函数式编程（FP）对软件系统架构特性的影响，通过Kotlin（OOP）和Scala（FP）实现数字钱包系统，进行定性和定量分析。


<details>
  <summary>Details</summary>
Motivation: 探讨OOP和FP在软件架构中的实际影响，帮助开发者和组织选择更适合的编程范式。

Method: 通过自我民族志定性分析和基于调查的定量分析，比较Kotlin（OOP）和Scala（FP）实现的数字钱包系统。

Result: 定性分析揭示了编写代码的视角，定量分析收集了开发者对代码的反馈。

Conclusion: 研究结果可为开发者和组织在选择编程范式时提供参考。

Abstract: After decades of dominance by object-oriented programming (OOP), functional
programming (FP) is gaining increasing attention in the software industry. This
study compares the impact of OOP and FP on the architectural characteristics of
software systems. For that, it examines the design and implementation of a
Digital Wallet system, developed in Kotlin (representing OOP) and Scala
(representing FP). The comparison is made through both qualitative and
quantitative analyses to explore how each paradigm influences the system's
architectural characteristics. The self-ethnographic qualitative analysis
provides a side-by-side comparison of both implementations, revealing the
perspective of those writing such code. The survey-based quantitative analysis
gathers feedback from developers with diverse backgrounds, showing their
impressions of those reading this code. Hopefully, these results may be useful
for developers or organizations seeking to make more informed decisions about
which paradigm is best suited for their next project.

</details>


### [8] [Leveraging Large Language Model for Information Retrieval-based Bug Localization](https://arxiv.org/abs/2508.00253)
*Moumita Asad,Rafed Muhammad Yasir,Armin Geramirad,Sam Malek*

Main category: cs.SE

TL;DR: 论文提出了一种基于大语言模型（LLM）的缺陷定位方法GenLoc，通过代码探索功能迭代分析代码库，解决了缺陷报告与源代码之间的词汇不匹配问题，并在多个指标上优于现有技术。


<details>
  <summary>Details</summary>
Motivation: 现有缺陷定位方法（从向量空间模型到深度学习模型）因缺陷报告与源代码的词汇不匹配问题而效果受限，需改进。

Method: 提出GenLoc方法，利用具备代码探索功能的LLM迭代分析代码库，并可选择性地通过向量嵌入检索语义相关文件以获取上下文。

Result: 在6个大型Java项目的9000多个真实缺陷报告上测试，GenLoc在多个指标上优于5种前沿技术，Accuracy@1平均提升60%以上。

Conclusion: GenLoc通过LLM和代码探索功能显著提升了缺陷定位的准确性，解决了词汇不匹配问题。

Abstract: Information Retrieval-based Bug Localization aims to identify buggy source
files for a given bug report. While existing approaches -- ranging from vector
space models to deep learning models -- have shown potential in this domain,
their effectiveness is often limited by the vocabulary mismatch between bug
reports and source code. To address this issue, we propose a novel Large
Language Model (LLM) based bug localization approach, called GenLoc. Given a
bug report, GenLoc leverages an LLM equipped with code-exploration functions to
iteratively analyze the code base and identify potential buggy files. To gather
better context, GenLoc may optionally retrieve semantically relevant files
using vector embeddings. GenLoc has been evaluated on over 9,000 real-world bug
reports from six large-scale Java projects. Experimental results show that
GenLoc outperforms five state-of-the-art bug localization techniques across
multiple metrics, achieving an average improvement of more than 60\% in
Accuracy@1.

</details>


### [9] [Accurate and Consistent Graph Model Generation from Text with Large Language Models](https://arxiv.org/abs/2508.00255)
*Boqi Chen,Ou Wei,Bingzhou Zheng,Gunter Mussbacher*

Main category: cs.SE

TL;DR: 提出了一种基于抽象-具体化框架的方法，通过聚合多个LLM输出来提高图模型生成的一致性和质量。


<details>
  <summary>Details</summary>
Motivation: 解决LLM生成的图模型中存在的语法违规、约束不一致和不准确性问题。

Method: 采用抽象-具体化框架，先构建概率性部分模型，再优化为满足所有约束的具体模型。

Result: 实验表明，该方法显著提升了生成图模型的一致性和质量。

Conclusion: 该方法有效解决了LLM生成图模型中的主要问题，具有实际应用价值。

Abstract: Graph model generation from natural language description is an important task
with many applications in software engineering. With the rise of large language
models (LLMs), there is a growing interest in using LLMs for graph model
generation. Nevertheless, LLM-based graph model generation typically produces
partially correct models that suffer from three main issues: (1) syntax
violations: the generated model may not adhere to the syntax defined by its
metamodel, (2) constraint inconsistencies: the structure of the model might not
conform to some domain-specific constraints, and (3) inaccuracy: due to the
inherent uncertainty in LLMs, the models can include inaccurate, hallucinated
elements. While the first issue is often addressed through techniques such as
constraint decoding or filtering, the latter two remain largely unaddressed.
Motivated by recent self-consistency approaches in LLMs, we propose a novel
abstraction-concretization framework that enhances the consistency and quality
of generated graph models by considering multiple outputs from an LLM. Our
approach first constructs a probabilistic partial model that aggregates all
candidate outputs and then refines this partial model into the most appropriate
concrete model that satisfies all constraints. We evaluate our framework on
several popular open-source and closed-source LLMs using diverse datasets for
model generation tasks. The results demonstrate that our approach significantly
improves both the consistency and quality of the generated graph models.

</details>


### [10] [Benchmarking LLMs for Unit Test Generation from Real-World Functions](https://arxiv.org/abs/2508.00408)
*Dong Huang,Jie M. Zhang,Mark Harman,Qianru Zhang,Mingzhe Du,See-Kiong Ng*

Main category: cs.SE

TL;DR: 论文提出了一个新的基准测试ULT，用于评估大型语言模型在单元测试生成中的能力，解决了现有基准的数据污染和结构简单性问题。


<details>
  <summary>Details</summary>
Motivation: 现有基准存在数据污染和结构简单性问题，导致评估结果不可靠，无法反映真实场景。

Method: 通过多阶段筛选过程构建ULT，包含3,909个高复杂度的Python函数任务，并引入PLT作为对照基准。

Result: ULT的测试生成结果（如准确率41.32%）显著低于现有基准（如TestEval的91.79%），表明其更具挑战性。

Conclusion: ULT提供了一个更真实和具有挑战性的评估工具，有助于更准确地衡量LLMs在测试生成中的能力。

Abstract: Recently, large language models (LLMs) have shown great promise in automating
unit test generation, significantly reducing the manual effort required by
developers. To effectively evaluate the capabilities of LLMs in this domain, it
is crucial to have a well-designed benchmark that accurately reflects
real-world scenarios and mitigates common pitfalls. Existing LLM test
generation benchmarks are limited by two critical drawbacks: data contamination
and structurally simple function code. As a result, we often cannot rely on the
validity of scientific conclusions drawn from empirical studies using these
limited benchmarks. The empirical evidence presented may be biased due to
contamination and may fail to generalize beyond toy programs due to structural
simplicity.
  To address these problems, we introduce ULT (UnLeakedTestbench), a new
benchmark specifically designed for function-level unit test generation from
real-world Python functions. ULT is constructed through a multi-stage curation
process that ensures high cyclomatic complexity and mitigates test case
contamination. With 3,909 carefully selected function-level tasks, ULT provides
a more realistic and challenging evaluation of LLMs' test generation
capabilities. We also provide PLT (PreLeakedTestbench), a pair benchmark of ULT
with leaked tests designed to enable a controlled analysis of memorization
versus reasoning in test generation. Our evaluation results demonstrate that
ULT is significantly more challenging. For example, test cases generated by
LLMs only achieve 41.32\%, 45.10\%, 30.22\%, and 40.21\% for accuracy,
statement coverage, branch coverage, and mutation score on average for all
LLMs, respectively. These results are substantially lower than the
corresponding metrics on TestEval (91.79\%, 92.18\%, 82.04\%, and 49.69\%) and
PLT (47.07\%, 55.13\%, 40.07\%, and 50.80\%).

</details>


### [11] [Managing Power Gaps as a Topic of Pair Programming Skill: A Grounded Theory](https://arxiv.org/abs/2508.00462)
*Linus Ververs,Lutz Prechelt*

Main category: cs.SE

TL;DR: 论文研究了结对编程中的权力差距现象，分析了其对知识传递、代码质量和效率的影响，并提出了避免权力差距的建议。


<details>
  <summary>Details</summary>
Motivation: 理解结对编程中权力相关现象，并为从业者提供改进建议。

Method: 通过扎根理论分析22个工业结对编程会话，并调查292名参与者验证理论。

Result: 提出了权力差距理论，证明其普遍存在且对结对编程有负面影响。

Conclusion: 避免权力差距是结对编程技能的重要组成部分，应减少等级行为并增加平等行为。

Abstract: Context: Pair Programming as a work mode is used (occasionally or frequently)
throughout professional software development. Objective: Understand what
power-related phenomena occur in pair programming as it is used in industry;
give advice to practitioners on how to do better pair programming. Method:
Analyze 22 industrial pair programming sessions using Grounded Theory
Methodology. Formulate a Grounded Theory on power-related behaviors. Run a
survey with 292 participants about that theory. Use it to demonstrate that the
phenomena are common. Results: Our theory describes the phenomenon of Power
Gap: a perceived difference in participation opportunities. The theory shows
the behaviors that create a Power Gap or result from it. Power Gaps tend to
damage knowledge transfer, code quality, and process effi ciency. The survey
results show that all concepts from our theory are frequent in practice. They
also provide more grounding for concepts that are observable only indirectly.
Conclusions: It is a valuable component of pair programming skill to be able to
avoid Power Gaps. Specifically, pair partners need to avoid Hierarchical
Behavior (which tends to create or increase a Power Gap) and should perform
enough Equalizing Behavior (which prevents or reduces a Power Gap).

</details>


### [12] [Desyan: A Platform for Seamless Value-Flow and Symbolic Analysis](https://arxiv.org/abs/2508.00508)
*Panagiotis Diamantakis,Thanassis Avgerinos,Yannis Smaragdakis*

Main category: cs.SE

TL;DR: Desyan平台无缝整合了值流分析和符号推理，通过扩展Soufflé Datalog引擎并集成SMT求解器，为程序分析提供了高效且灵活的解决方案。


<details>
  <summary>Details</summary>
Motivation: 尽管值流分析和符号分析在各自领域取得了成功，但缺乏统一的平台实现两者的高效集成，Desyan旨在填补这一空白。

Method: Desyan扩展了Soufflé Datalog引擎，集成SMT求解器，支持自动处理程序分析中的常见模式，并提供Datalog原生符号推理模块。

Result: Desyan在值流分析中表现卓越（速度提升20倍以上），在需要SMT求解的应用中利用领先的SMT引擎，而在轻量级符号推理中速度提升2倍以上。

Conclusion: Desyan成功实现了值流和符号推理的无缝集成，为程序分析提供了高效且灵活的解决方案。

Abstract: Over the past two decades, two different types of static analyses have
emerged as dominant paradigms both in academia and industry: value-flow
analysis (e.g., data-flow analysis or points-to analysis) and symbolic analysis
(e.g., symbolic execution). Despite their individual successes in numerous
application fields, the two approaches have remained largely separate; an
artifact of the simple reality that there is no broadly adopted unifying
platform for effortless and efficient integration of symbolic techniques with
high-performance data-flow reasoning.
  To bridge this gap, we introduce Desyan: a platform for writing program
analyses with seamless integration of value-flow and symbolic reasoning. Desyan
expands a production-ready Datalog fixpoint engine (Souffl\'e) with
full-fledged SMT solving invoking industry-leading SMT engines. Desyan provides
constructs for automatically (and efficiently!) handling typical patterns that
come up in program analysis. At the same time, the integration is agnostic with
respect to the solving technology, and supports Datalog-native symbolic
reasoning, via a bottom-up algebraic reasoning module.
  The result is an engine that allows blending different kinds of reasoning, as
needed for the underlying analysis. For value-flow analysis, the engine is the
best-in-class Datalog evaluator (often by a factor of over 20x in execution
time); for applications that require full SMT (e.g., a concolic execution
engine or other symbolic evaluator that needs to solve arbitrarily complex
conditions), the engine is leveraging the leading SMT solvers; for lightweight
symbolic evaluation (e.g., solving simple conditionals in the context of a
path-sensitive analysis), the engine can use Datalog-native symbolic reasoning,
achieving large speedups (often of over 2x) compared to eagerly appealing to an
SMT solver.

</details>


### [13] [SPENCER: Self-Adaptive Model Distillation for Efficient Code Retrieval](https://arxiv.org/abs/2508.00546)
*Wenchao Gu,Zongyi Lyu,Yanlin Wang,Hongyu Zhang,Cuiyun Gao,Michael R. Lyu*

Main category: cs.SE

TL;DR: 论文提出了一种名为SPENCER的框架，结合双编码器和交叉编码器以提高代码检索的效率和准确性，并通过模型蒸馏技术减少推理时间。


<details>
  <summary>Details</summary>
Motivation: 现有双编码器模型在代码检索任务中缺乏底层交互，限制了性能。

Method: SPENCER结合双编码器和交叉编码器，并提出自适应模型蒸馏技术和教学助理选择策略。

Result: 实验表明，结合双编码器和交叉编码器提升了性能，模型蒸馏技术减少了70%推理时间并保留98%性能。

Conclusion: SPENCER框架在代码检索任务中实现了高效与高精度的平衡。

Abstract: Code retrieval aims to provide users with desired code snippets based on
users' natural language queries. With the development of deep learning
technologies, adopting pre-trained models for this task has become mainstream.
Considering the retrieval efficiency, most of the previous approaches adopt a
dual-encoder for this task, which encodes the description and code snippet into
representation vectors, respectively. However, the model structure of the
dual-encoder tends to limit the model's performance, since it lacks the
interaction between the code snippet and description at the bottom layer of the
model during training. To improve the model's effectiveness while preserving
its efficiency, we propose a framework, which adopts Self-AdaPtive Model
Distillation for Efficient CodE Retrieval, named SPENCER. SPENCER first adopts
the dual-encoder to narrow the search space and then adopts the cross-encoder
to improve accuracy. To improve the efficiency of SPENCER, we propose a novel
model distillation technique, which can greatly reduce the inference time of
the dual-encoder while maintaining the overall performance. We also propose a
teaching assistant selection strategy for our model distillation, which can
adaptively select the suitable teaching assistant models for different
pre-trained models during the model distillation to ensure the model
performance. Extensive experiments demonstrate that the combination of
dual-encoder and cross-encoder improves overall performance compared to solely
dual-encoder-based models for code retrieval. Besides, our model distillation
technique retains over 98% of the overall performance while reducing the
inference time of the dual-encoder by 70%.

</details>


### [14] [Can User Feedback Help Issue Detection? An Empirical Study on a One-billion-user Online Service System](https://arxiv.org/abs/2508.00593)
*Shuyao Jiang,Jiazhen Gu,Wujie Zheng,Yangfan Zhou,Michael R. Lyu*

Main category: cs.SE

TL;DR: 研究分析了大规模在线服务系统中用户反馈的特点，发现大量反馈与系统问题无关，需过滤；严重问题难以仅通过反馈特征检测；机器学习方法可行。


<details>
  <summary>Details</summary>
Motivation: 理解用户反馈在真实生产系统中的特点，以改进基于反馈的问题检测方法。

Method: 对来自六个真实服务的50,378,766条用户反馈进行实证研究，分析反馈内容、特征与问题关联性，并评估机器学习技术的适用性。

Result: 大量反馈与问题无关，需过滤；严重问题难以通过反馈特征检测；反馈主题分布相似，机器学习方法可行。

Conclusion: 研究结果为大规模服务系统中基于反馈的问题检测提供了实证基础，指导实际检测方法的设计与实现。

Abstract: Background: It has long been suggested that user feedback, typically written
in natural language by end-users, can help issue detection. However, for
large-scale online service systems that receive a tremendous amount of
feedback, it remains a challenging task to identify severe issues from user
feedback. Aims: To develop a better feedback-based issue detection approach, it
is crucial first to gain a comprehensive understanding of the characteristics
of user feedback in real production systems. Method: In this paper, we conduct
an empirical study on 50,378,766 user feedback items from six real-world
services in a one-billion-user online service system. We first study what users
provide in their feedback. We then examine whether certain features of feedback
items can be good indicators of severe issues. Finally, we investigate whether
adopting machine learning techniques to analyze user feedback is reasonable.
Results: Our results show that a large proportion of user feedback provides
irrelevant information about system issues. As a result, it is crucial to
filter out issue-irrelevant information when processing user feedback.
Moreover, we find severe issues that cannot be easily detected based solely on
user feedback characteristics. Finally, we find that the distributions of the
feedback topics in different time intervals are similar. This confirms that
designing machine learning-based approaches is a viable direction for better
analyzing user feedback. Conclusions: We consider that our findings can serve
as an empirical foundation for feedback-based issue detection in large-scale
service systems, which sheds light on the design and implementation of
practical issue detection approaches.

</details>


### [15] [MCeT: Behavioral Model Correctness Evaluation using Large Language Models](https://arxiv.org/abs/2508.00630)
*Khaled Ahmed,Jialing Song,Boqi Chen,Ou Wei,Bingzhou Zheng*

Main category: cs.SE

TL;DR: 论文提出MCeT工具，利用LLM多角度评估行为模型（如序列图）的正确性，通过细粒度分析和自一致性检查显著提升问题发现率和精确度。


<details>
  <summary>Details</summary>
Motivation: 随着LLM在模型生成中的广泛应用，需要自动化工具评估模型正确性，为工程师提供反馈并帮助AI自我优化。

Method: 将模型和需求分解为原子单元，多角度对比检查，并结合自一致性检查减少LLM幻觉问题。

Result: MCeT将问题发现率提升至90%，精确度从0.58提高到0.81，平均每图发现6个新问题。

Conclusion: MCeT为行为模型评估提供了高效自动化工具，显著优于直接LLM检查，适用于人工和AI生成模型。

Abstract: Behavioral model diagrams, e.g., sequence diagrams, are an essential form of
documentation that are typically designed by system engineers from requirements
documentation, either fully manually or assisted by design tools. With the
growing use of Large Language Models (LLM) as AI modeling assistants, more
automation will be involved in generating diagrams. This necessitates the
advancement of automatic model correctness evaluation tools. Such a tool can be
used to evaluate both manually and AI automatically generated models; to
provide feedback to system engineers, and enable AI assistants to self-evaluate
and self-enhance their generated models.
  In this paper, we propose MCeT, the first fully automated tool to evaluate
the correctness of a behavioral model, sequence diagrams in particular, against
its corresponding requirements text and produce a list of issues that the model
has. We utilize LLMs for the correctness evaluation tasks as they have shown
outstanding natural language understanding ability. However, we show that
directly asking an LLM to compare a diagram to requirements finds less than 35%
of issues that experienced engineers can find. We propose to supplement the
direct check with a fine-grained, multi-perspective approach; we split the
diagram into atomic, non-divisible interactions, and split the requirements
text into atomic, self-contained items. We compare the diagram with atomic
requirements and each diagram-atom with the requirements. We also propose a
self-consistency checking approach that combines perspectives to mitigate LLM
hallucinated issues. Our combined approach improves upon the precision of the
direct approach from 0.58 to 0.81 in a dataset of real requirements. Moreover,
the approach finds 90% more issues that the experienced engineers found than
the direct approach, and reports an average of 6 new issues per diagram.

</details>


### [16] [Is LLM-Generated Code More Maintainable \& Reliable than Human-Written Code?](https://arxiv.org/abs/2508.00700)
*Alfred Santa Molison,Marcia Moraes,Glaucia Melo,Fabio Santos,Wesley K. G. Assuncao*

Main category: cs.SE

TL;DR: 比较LLM生成代码与人工编写代码的质量，发现LLM代码整体缺陷较少但复杂场景下可能引入严重问题。


<details>
  <summary>Details</summary>
Motivation: 研究LLM生成代码在软件质量方面的表现，并与人工编写代码对比。

Method: 结合数据集、三种LLM配置（零样本、少样本、微调）和SonarQube工具评估代码质量。

Result: LLM生成代码缺陷较少且修复成本低，但微调模型可能降低性能，复杂场景下可能引入结构性问题。

Conclusion: LLM生成代码质量整体较好，但需系统评估复杂场景下的潜在问题。

Abstract: Background: The rise of Large Language Models (LLMs) in software development
has opened new possibilities for code generation. Despite the widespread use of
this technology, it remains unclear how well LLMs generate code solutions in
terms of software quality and how they compare to human-written code. Aims:
This study compares the internal quality attributes of LLM-generated and
human-written code. Method: Our empirical study integrates datasets of coding
tasks, three LLM configurations (zero-shot, few-shot, and fine-tuning), and
SonarQube to assess software quality. The dataset comprises Python code
solutions across three difficulty levels: introductory, interview, and
competition. We analyzed key code quality metrics, including maintainability
and reliability, and the estimated effort required to resolve code issues.
Results: Our analysis shows that LLM-generated code has fewer bugs and requires
less effort to fix them overall. Interestingly, fine-tuned models reduced the
prevalence of high-severity issues, such as blocker and critical bugs, and
shifted them to lower-severity categories, but decreased the model's
performance. In competition-level problems, the LLM solutions sometimes
introduce structural issues that are not present in human-written code.
Conclusion: Our findings provide valuable insights into the quality of
LLM-generated code; however, the introduction of critical issues in more
complex scenarios highlights the need for a systematic evaluation and
validation of LLM solutions. Our work deepens the understanding of the
strengths and limitations of LLMs for code generation.

</details>


### [17] [Tool-Assisted Conformance Checking to Reference Process Models](https://arxiv.org/abs/2508.00738)
*Bernhard Rumpe,Max Stachon,Sebastian Stüber,Valdes Voufo*

Main category: cs.SE

TL;DR: 本文提出了一种基于因果依赖分析的自动化一致性检查方法，用于验证具体流程模型与参考模型的一致性，填补了现有方法在语义模型比较方面的不足。


<details>
  <summary>Details</summary>
Motivation: 现有的一致性检查方法主要关注流程执行轨迹的验证，缺乏语义模型比较的表达能力和自动化支持，因此需要一种更高效、更灵活的方法。

Method: 通过因果依赖分析任务和事件，提出了一种算法，并将其集成到一个更广泛的语义框架中，用于定义参考模型的一致性。

Result: 通过案例研究验证了该方法的有效性，展示了其在提高流程模型一致性验证准确性和灵活性方面的优势。

Conclusion: 研究提供了一种工具辅助的解决方案，增强了流程模型一致性验证的准确性和灵活性，但仍存在一些局限性。

Abstract: Reference models convey best practices and standards. The reference
frameworks necessitate conformance checks to ensure adherence to established
guidelines and principles, which is crucial for maintaining quality and
consistency in various processes. This paper explores automated conformance
checks for concrete process models against reference models using causal
dependency analysis of tasks and events. Existing notions of conformance
checking for process models focus on verifying process execution traces and
lack the expressiveness and automation needed for semantic model comparison,
leaving this question unresolved. We integrate our approach into a broader
semantic framework for defining reference model conformance. We outline an
algorithm for reference process model conformance checking, evaluate it through
a case study, and discuss its strengths and limitations. Our research provides
a tool-assisted solution enhancing accuracy and flexibility in process model
conformance verification.

</details>


### [18] [Dynamic Symbolic Execution for Semantic Difference Analysis of Component and Connector Architectures](https://arxiv.org/abs/2508.00749)
*Johanna Grahl,Bernhard Rumpe,Max Stachon,Sebastian Stüber*

Main category: cs.SE

TL;DR: 论文探讨了在模型驱动开发中，利用动态符号执行（DSE）分析组件-连接器架构的语义差异，重点关注MontiArc模型。通过增强现有工具收集运行时数据，评估执行策略，发现DSE潜力但存在可扩展性问题。


<details>
  <summary>Details</summary>
Motivation: 在模型驱动开发中，确保模型正确性和一致性至关重要，需有效分析语义差异。

Method: 增强MontiArc-to-Java生成器，收集符号和具体执行数据，评估不同执行策略的效率、最小性和完整性。

Result: DSE在分析组件-连接器架构中显示潜力，但可扩展性受限。

Conclusion: DSE适用于语义差异分析，但需进一步研究提升其在大系统中的实用性。

Abstract: In the context of model-driven development, ensuring the correctness and
consistency of evolving models is paramount. This paper investigates the
application of Dynamic Symbolic Execution (DSE) for semantic difference
analysis of component-and-connector architectures, specifically utilizing
MontiArc models. We have enhanced the existing MontiArc-to-Java generator to
gather both symbolic and concrete execution data at runtime, encompassing
transition conditions, visited states, and internal variables of automata. This
data facilitates the identification of significant execution traces that
provide critical insights into system behavior. We evaluate various execution
strategies based on the criteria of runtime efficiency, minimality, and
completeness, establishing a framework for assessing the applicability of DSE
in semantic difference analysis. Our findings indicate that while DSE shows
promise for analyzing component and connector architectures, scalability
remains a primary limitation, suggesting further research is needed to enhance
its practical utility in larger systems.

</details>


### [19] [From Code to Career: Assessing Competitive Programmers for Industry Placement](https://arxiv.org/abs/2508.00772)
*Md Imranur Rahman Akib,Fathima Binthe Muhammed,Umit Saha,Md Fazlul Karim Patwary,Mehrin Anannya,Md Alomgeer Hussein,Md Biplob Hosen*

Main category: cs.SE

TL;DR: 研究通过Codeforces用户的编程竞赛表现预测其软件工程岗位就业潜力，使用随机森林分类器构建模型，并将用户分为四个就业能力等级。


<details>
  <summary>Details</summary>
Motivation: 快速发展的科技行业需要评估程序员就业准备的工具，研究旨在分析竞争编程活动与就业机会的相关性。

Method: 通过Codeforces API收集用户数据，处理关键性能指标，使用随机森林分类器构建预测模型，并通过Flask部署实时预测系统。

Result: 模型能有效区分不同技能水平的用户，基于编程熟练度和参与度预测就业潜力。

Conclusion: 研究为机器学习在职业评估中的应用奠定基础，可扩展至更广泛技术领域的就业准备预测。

Abstract: In today's fast-paced tech industry, there is a growing need for tools that
evaluate a programmer's job readiness based on their coding performance. This
study focuses on predicting the potential of Codeforces users to secure various
levels of software engineering jobs. The primary objective is to analyze how a
user's competitive programming activity correlates with their chances of
obtaining positions, ranging from entry-level roles to jobs at major tech
companies. We collect user data using the Codeforces API, process key
performance metrics, and build a prediction model using a Random Forest
classifier. The model categorizes users into four levels of employability,
ranging from those needing further development to those ready for top-tier tech
jobs. The system is implemented using Flask and deployed on Render for
real-time predictions. Our evaluation demonstrates that the approach
effectively distinguishes between different skill levels based on coding
proficiency and participation. This work lays a foundation for the use of
machine learning in career assessment and could be extended to predict job
readiness in broader technical fields.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [20] [ranDecepter: Real-time Identification and Deterrence of Ransomware Attacks](https://arxiv.org/abs/2508.00293)
*Md Sajidul Islam Sajid,Jinpeng Wei,Ehab Al-Shaer*

Main category: cs.CR

TL;DR: ranDecepter是一种结合主动网络欺骗和实时分析的新方法，用于增强对勒索软件的防御，通过误导攻击者并耗尽资源。


<details>
  <summary>Details</summary>
Motivation: 勒索软件威胁日益严重，需要有效的对策。主动网络欺骗是一种有前景的策略。

Method: ranDecepter实时识别勒索软件并将其隔离在欺骗环境中，通过循环机制伪造加密信息并耗尽攻击者资源。

Result: 测试显示100%的识别准确率，无假阳性，且24小时内生成大量虚假条目以消耗攻击者资源。

Conclusion: ranDecepter是一种高效且可扩展的防御勒索软件的方法。

Abstract: Ransomware (RW) presents a significant and widespread threat in the digital
landscape, necessitating effective countermeasures. Active cyber deception is a
promising strategy to thwart RW and limiting its propagation by misleading it
with false information and revealing its true behaviors. Furthermore, RW often
acts as a communication conduit between attackers and defenders, allowing
deception to return false data to attackers and deplete their resources. This
paper introduces ranDecepter, a novel approach that combines active cyber
deception with real-time analysis to enhance defenses against RW attacks. The
ranDecepter identifies RW in real-time and isolates it within a deceptive
environment, autonomously identifying critical elements in the RW code to
create a loop mechanism. By repeatedly restarting the malware and transmitting
counterfeit encryption information and secret keys to the attacker, it forces
the attacker to store these fabricated details for each victim, thereby
depleting their resources. Our comprehensive evaluation of ranDecepter,
conducted using 1,134 real-world malware samples and twelve benign
applications, demonstrates a remarkable 100% accuracy in RW identification,
with no false positives and minimal impact on response times. Furthermore,
within 24-hours, ranDecepter generates up to 9,223K entries in the attacker's
database using 50 agents, showcasing its potential to undermine attacker
resources.

</details>


### [21] [Cryptanalysis of Isogeny-Based Quantum Money with Rational Points](https://arxiv.org/abs/2508.00351)
*Hyeonhak Kim,Donghoe Heo,Seokhie Hong*

Main category: cs.CR

TL;DR: 论文提出了一种基于椭圆曲线类群动作的量子货币的具体密码分析，通过利用有理点坐标计算除法多项式的高效性，相比暴力攻击实现了O(log^4p)的加速。尽管攻击仍需要指数时间，但验证过程因此变得更高效。


<details>
  <summary>Details</summary>
Motivation: 研究量子货币的实际安全性，探索基于椭圆曲线的量子密码学的新方法。

Method: 利用有理点坐标高效计算除法多项式，结合二次扭曲的性质验证椭圆曲线叠加的基数。

Result: 攻击方法实现了O(log^4p)的加速，但仍需指数时间，无法实际伪造量子货币。同时，验证过程变得更高效。

Conclusion: 该方法为基于椭圆曲线的量子密码学研究提供了新思路，未来可能推动相关领域的发展。

Abstract: Quantum money is the cryptographic application of the quantum no-cloning
theorem. It has recently been instantiated by Montgomery and Sharif (Asiacrypt
'24) from class group actions on elliptic curves. In this work, we propose a
concrete cryptanalysis by leveraging the efficiency of evaluating division
polynomials with the coordinates of rational points, offering a speedup of
O(log^4p) compared to the brute-force attack. Since our attack still requires
exponential time, it remains impractical to forge a quantum banknote.
Interestingly, due to the inherent properties of quantum money, our attack
method also results in a more efficient verification procedure. Our algorithm
leverages the properties of quadratic twists to utilize rational points in
verifying the cardinality of the superposition of elliptic curves. We expect
this approach to contribute to future research on elliptic-curve-based quantum
cryptography.

</details>


### [22] [Preliminary Investigation into Uncertainty-Aware Attack Stage Classification](https://arxiv.org/abs/2508.00368)
*Alessandro Gaudenzi,Lorenzo Nodari,Lance Kaplan,Alessandra Russo,Murat Sensoy,Federico Cerutti*

Main category: cs.CR

TL;DR: 论文提出了一种基于证据深度学习（EDL）的方法，用于推断高级持续性威胁（APT）的攻击阶段，并处理不确定性及分布外输入。


<details>
  <summary>Details</summary>
Motivation: 传统检测系统无法准确推断攻击阶段，而有效的响应策略需要根据攻击阶段定制。

Method: 采用证据深度学习（EDL），通过输出Dirichlet分布的参数来建模预测不确定性。

Result: 在模拟环境中，模型能准确推断攻击阶段并检测分布外输入。

Conclusion: 不确定性感知模型在动态对抗环境中具有可行性。

Abstract: Advanced Persistent Threats (APTs) represent a significant challenge in
cybersecurity due to their prolonged, multi-stage nature and the sophistication
of their operators. Traditional detection systems typically focus on
identifying malicious activity in binary terms (benign or malicious) without
accounting for the progression of an attack. However, effective response
strategies depend on accurate inference of the attack's current stage, as
countermeasures must be tailored to whether an adversary is in the early
reconnaissance phase or actively conducting exploitation or exfiltration. This
work addresses the problem of attack stage inference under uncertainty, with a
focus on robustness to out-of-distribution (OOD) inputs. We propose a
classification approach based on Evidential Deep Learning (EDL), which models
predictive uncertainty by outputting parameters of a Dirichlet distribution
over possible stages. This allows the system not only to predict the most
likely stage of an attack but also to indicate when it is uncertain or the
input lies outside the training distribution. Preliminary experiments in a
simulated environment demonstrate that the proposed model can accurately infer
the stage of an attack with calibrated confidence while effectively detecting
OOD inputs, which may indicate changes in the attackers' tactics. These results
support the feasibility of deploying uncertainty-aware models for staged threat
detection in dynamic and adversarial environments.

</details>


### [23] [Accurate Latent Inversion for Generative Image Steganography via Rectified Flow](https://arxiv.org/abs/2508.00434)
*Yuqi Qian,Yun Cao,Meiyang Lv,Haocheng Fu*

Main category: cs.CR

TL;DR: RF-Stego是一种基于扩散模型的图像隐写方法，通过路径一致性线性反转（PCLI）和修正流（RF）技术，显著提高了隐写图像的消息提取准确性和稳定性。


<details>
  <summary>Details</summary>
Motivation: 传统扩散模型隐写方法中，潜在变量反转不准确导致消息提取失败，因此需要一种能够确保路径一致性和数值稳定性的新方法。

Method: 提出PCLI技术确保反转路径与生成路径一致，并使用RF采样器替代传统不稳定采样器以提高数值精度。

Result: 实验表明，RF-Stego在提取准确性、图像质量、鲁棒性、安全性和生成效率上优于现有方法。

Conclusion: RF-Stego通过路径一致性和数值稳定性改进，显著提升了隐写技术的性能。

Abstract: Steganography based on diffusion models has attracted increasing attention
due to its ability to generate high-quality images and exhibit strong
robustness. In such approaches, the secret message is first embedded into the
initial latent variable, and then the stego image is generated through the
forward process. To extract the message, an inversion process is required to
reconstruct the latent variables from the received image. However, inaccurate
latent inversion leads to significant discrepancies between the reconstructed
and original latent variables, rendering message extraction infeasible. To
address this issue, we propose \textbf{RF-Stego}, a novel generative image
steganography method that enables accurate latent inversion and significantly
improves extraction accuracy. First, we develop the \textbf{P}ath
\textbf{C}onsistency \textbf{L}inear \textbf{I}nversion (\textbf{PCLI}), which
imposes formal constraints on the inversion process. By explicitly aligning it
with the forward generation path and modeling both directions along a shared
linear path, PCLI eliminates path mismatch and ensures path consistency
throughout the steganographic process. Second, through rigorous theoretical
proof, we demonstrate that \textbf{R}ectified \textbf{F}low \textbf{(RF)}
offers both theoretical reversibility and numerical stability in the inversion
process. Based on this, we replace traditional unstable samplers with RF
sampler which effectively improves the numerical precision of the inversion
process. Experimental results show RF-Stego outperforms state-of-the-art
methods in terms of extraction accuracy, image quality, robustness, security
and generation efficiency.

</details>


### [24] [CyGATE: Game-Theoretic Cyber Attack-Defense Engine for Patch Strategy Optimization](https://arxiv.org/abs/2508.00478)
*Yuning Jiang,Nay Oo,Qiaoran Meng,Lu Lin,Dusit Niyato,Zehui Xiong,Hoon Wei Lim,Biplab Sikdar*

Main category: cs.CR

TL;DR: CyGATE是一个基于博弈论的框架，利用大型语言模型（LLMs）和检索增强生成（RAG）优化网络攻防中的战术选择和补丁优先级。


<details>
  <summary>Details</summary>
Motivation: 现有博弈论模型依赖静态假设且缺乏实时威胁情报整合，限制了适应性。

Method: 将网络冲突建模为部分可观察随机博弈（POSG），攻防双方利用信念状态应对不确定性。

Result: 在动态补丁调度场景中，CyGATE有效优先处理高风险漏洞，提升适应性、战略预见性和资源效率。

Conclusion: CyGATE通过动态威胁整合和多智能体扩展能力，为复杂网络环境提供了灵活的防御框架。

Abstract: Modern cyber attacks unfold through multiple stages, requiring defenders to
dynamically prioritize mitigations under uncertainty. While game-theoretic
models capture attacker-defender interactions, existing approaches often rely
on static assumptions and lack integration with real-time threat intelligence,
limiting their adaptability. This paper presents CyGATE, a game-theoretic
framework modeling attacker-defender interactions, using large language models
(LLMs) with retrieval-augmented generation (RAG) to enhance tactic selection
and patch prioritization. Applied to a two-agent scenario, CyGATE frames cyber
conflicts as a partially observable stochastic game (POSG) across Cyber Kill
Chain stages. Both agents use belief states to navigate uncertainty, with the
attacker adapting tactics and the defender re-prioritizing patches based on
evolving risks and observed adversary behavior. The framework's flexible
architecture enables extension to multi-agent scenarios involving coordinated
attackers, collaborative defenders, or complex enterprise environments with
multiple stakeholders. Evaluated in a dynamic patch scheduling scenario, CyGATE
effectively prioritizes high-risk vulnerabilities, enhancing adaptability
through dynamic threat integration, strategic foresight by anticipating
attacker moves under uncertainty, and efficiency by optimizing resource use.

</details>


### [25] [Activation-Guided Local Editing for Jailbreaking Attacks](https://arxiv.org/abs/2508.00555)
*Jiecong Wang,Haoran Li,Hao Peng,Ziqian Zeng,Zihao Wang,Haohua Du,Zhengtao Yu*

Main category: cs.CR

TL;DR: 提出了一种两阶段框架AGILE，结合场景生成和隐藏状态引导编辑，显著提高了攻击成功率和可迁移性。


<details>
  <summary>Details</summary>
Motivation: 现有越狱方法存在输入不连贯、可扩展性差等问题，需要更高效的解决方案。

Method: 两阶段框架：首先生成模糊恶意意图的上下文，再利用隐藏状态信息进行细粒度编辑。

Result: 攻击成功率提升37.74%，在黑盒模型中表现优异，且对防御机制有效。

Conclusion: AGILE揭示了当前防御的局限性，为未来防御开发提供了参考。

Abstract: Jailbreaking is an essential adversarial technique for red-teaming these
models to uncover and patch security flaws. However, existing jailbreak methods
face significant drawbacks. Token-level jailbreak attacks often produce
incoherent or unreadable inputs and exhibit poor transferability, while
prompt-level attacks lack scalability and rely heavily on manual effort and
human ingenuity. We propose a concise and effective two-stage framework that
combines the advantages of these approaches. The first stage performs a
scenario-based generation of context and rephrases the original malicious query
to obscure its harmful intent. The second stage then utilizes information from
the model's hidden states to guide fine-grained edits, effectively steering the
model's internal representation of the input from a malicious toward a benign
one. Extensive experiments demonstrate that this method achieves
state-of-the-art Attack Success Rate, with gains of up to 37.74% over the
strongest baseline, and exhibits excellent transferability to black-box models.
Our analysis further demonstrates that AGILE maintains substantial
effectiveness against prominent defense mechanisms, highlighting the
limitations of current safeguards and providing valuable insights for future
defense development. Our code is available at
https://github.com/yunsaijc/AGILE.

</details>


### [26] [LeakSealer: A Semisupervised Defense for LLMs Against Prompt Injection and Leakage Attacks](https://arxiv.org/abs/2508.00602)
*Francesco Panebianco,Stefano Bonfanti,Francesco Trovò,Michele Carminati*

Main category: cs.CR

TL;DR: 论文分析了LLMs的安全威胁，提出了分析历史交互数据的方法和LeakSealer框架，用于检测攻击模式和保护敏感信息。


<details>
  <summary>Details</summary>
Motivation: 随着LLMs广泛应用，安全问题如越狱攻击和数据泄漏日益突出，需要有效的防御机制。

Method: 提出了基于历史交互数据的分析方法和LeakSealer框架，结合静态分析和动态防御。

Result: LeakSealer在静态和动态场景下表现优异，尤其在PII泄漏检测中AUPRC达0.97。

Conclusion: 论文提出的方法能有效识别和防御LLMs的安全威胁，具有实际应用价值。

Abstract: The generalization capabilities of Large Language Models (LLMs) have led to
their widespread deployment across various applications. However, this
increased adoption has introduced several security threats, notably in the
forms of jailbreaking and data leakage attacks. Additionally, Retrieval
Augmented Generation (RAG), while enhancing context-awareness in LLM responses,
has inadvertently introduced vulnerabilities that can result in the leakage of
sensitive information. Our contributions are twofold. First, we introduce a
methodology to analyze historical interaction data from an LLM system, enabling
the generation of usage maps categorized by topics (including adversarial
interactions). This approach further provides forensic insights for tracking
the evolution of jailbreaking attack patterns. Second, we propose LeakSealer, a
model-agnostic framework that combines static analysis for forensic insights
with dynamic defenses in a Human-In-The-Loop (HITL) pipeline. This technique
identifies topic groups and detects anomalous patterns, allowing for proactive
defense mechanisms. We empirically evaluate LeakSealer under two scenarios: (1)
jailbreak attempts, employing a public benchmark dataset, and (2) PII leakage,
supported by a curated dataset of labeled LLM interactions. In the static
setting, LeakSealer achieves the highest precision and recall on the ToxicChat
dataset when identifying prompt injection. In the dynamic setting, PII leakage
detection achieves an AUPRC of $0.97$, significantly outperforming baselines
such as Llama Guard.

</details>


### [27] [FedGuard: A Diverse-Byzantine-Robust Mechanism for Federated Learning with Major Malicious Clients](https://arxiv.org/abs/2508.00636)
*Haocheng Jiang,Hua Shen,Jixin Zhang,Willy Susilo,Mingwu Zhang*

Main category: cs.CR

TL;DR: FedGuard是一种新型联邦学习机制，通过利用成员推理对模型偏差的高敏感性，有效识别和排除中毒模型，显著优于现有防御方案。


<details>
  <summary>Details</summary>
Motivation: 联邦学习易受拜占庭攻击，尤其在多数客户端恶意或数据高度非独立同分布时，现有防御机制针对特定攻击类型效果有限。

Method: FedGuard要求客户端在训练中包含服务器指定的额外小批量数据，利用模型对这批数据的置信度下降识别中毒模型。

Result: 在90%客户端为拜占庭且每轮发生七种攻击的高度非独立同分布数据集上，FedGuard显著优于现有方案。

Conclusion: FedGuard通过创新方法有效解决了联邦学习中的拜占庭攻击问题，具有广泛适用性和高效性。

Abstract: Federated learning is a distributed training framework vulnerable to
Byzantine attacks, particularly when over 50% of clients are malicious or when
datasets are highly non-independent and identically distributed (non-IID).
Additionally, most existing defense mechanisms are designed for specific attack
types (e.g., gradient similarity-based schemes can only defend against outlier
model poisoning), limiting their effectiveness. In response, we propose
FedGuard, a novel federated learning mechanism. FedGuard cleverly addresses the
aforementioned issues by leveraging the high sensitivity of membership
inference to model bias. By requiring clients to include an additional
mini-batch of server-specified data in their training, FedGuard can identify
and exclude poisoned models, as their confidence in the mini-batch will drop
significantly. Our comprehensive evaluation unequivocally shows that, under
three highly non-IID datasets, with 90% of clients being Byzantine and seven
different types of Byzantine attacks occurring in each round, FedGuard
significantly outperforms existing robust federated learning schemes in
mitigating various types of Byzantine attacks.

</details>


### [28] [Demo: TOSense -- What Did You Just Agree to?](https://arxiv.org/abs/2508.00659)
*Xinzhang Chen,Hassan Ali,Arash Shaghaghi,Salil S. Kanhere,Sanjay Jha*

Main category: cs.CR

TL;DR: TOSense是一个Chrome扩展，通过自然语言问答帮助用户理解冗长晦涩的服务条款（ToS），结合爬虫和轻量级语言模型实现实时回答。


<details>
  <summary>Details</summary>
Motivation: 解决用户因ToS冗长晦涩导致的信息不对称和法律风险问题。

Method: 结合爬虫（tos-crawl）提取ToS内容，轻量级语言模型（MiniLM和BART-encoder）实现问答和答案验证，并开发了自动评估管道（QEP）。

Result: 在五大平台（Apple、Google等）上测试，准确率最高达44.5%。

Conclusion: TOSense有效帮助用户理解ToS，展示了实时提取和问答的潜力。

Abstract: Online services often require users to agree to lengthy and obscure Terms of
Service (ToS), leading to information asymmetry and legal risks. This paper
proposes TOSense-a Chrome extension that allows users to ask questions about
ToS in natural language and get concise answers in real time. The system
combines (i) a crawler "tos-crawl" that automatically extracts ToS content, and
(ii) a lightweight large language model pipeline: MiniLM for semantic retrieval
and BART-encoder for answer relevance verification. To avoid expensive manual
annotation, we present a novel Question Answering Evaluation Pipeline (QEP)
that generates synthetic questions and verifies the correctness of answers
using clustered topic matching. Experiments on five major platforms, Apple,
Google, X (formerly Twitter), Microsoft, and Netflix, show the effectiveness of
TOSense (with up to 44.5% accuracy) across varying number of topic clusters.
During the demonstration, we will showcase TOSense in action. Attendees will be
able to experience seamless extraction, interactive question answering, and
instant indexing of new sites.

</details>


### [29] [Unveiling Dynamic Binary Instrumentation Techniques](https://arxiv.org/abs/2508.00682)
*Oscar Llorente-Vazquez,Xabier Ugarte-Pedrero,Igor Santos-Grueiro,Pablo Garcia Bringas*

Main category: cs.CR

TL;DR: 本文综述了动态二进制插桩（DBI）技术，比较了不同方法的优缺点，并指出没有一种方法在所有情况下都最优。


<details>
  <summary>Details</summary>
Motivation: DBI技术在安全和分析领域广泛应用，但现有方法各有局限，需要系统性的比较和评估。

Method: 综合分析了进程级和全系统级的DBI方法，比较了其构建模块和底层技术，并评估了性能。

Result: 结果表明，不同方法在不同场景下表现各异，没有一种方法在所有情况下最优。

Conclusion: DBI技术需根据具体需求选择合适方法，未来研究可进一步优化现有技术。

Abstract: Dynamic Binary Instrumentation (DBI) is the set of techniques that enable
instrumentation of programs at run-time, making it possible to monitor and
modify the execution of compiled binaries or entire systems. DBI is used for
countless security applications and analyses, and is extensively used across
many fields in both industry and academia. Over the years, several DBI
approaches have been proposed based on different technologies and implementing
diverse techniques. Every solution tries to overcome certain limitations, but
they sometimes bring other shortcomings. Some are specialized for one
particular domain or task, while others have a wider scope.
  In this paper, we shed light into the labyrinth of DBI, bringing together
process-level and whole-system approaches. We depict their building blocks and
analyze the underlying instrumentation techniques, comparing their ability to
instrument different primitives and run-time events. Then, we evaluate their
performance when implementing each primitive, and highlight relevant
observations. Our results show that no single technique is better than the rest
in all circumstances.

</details>


### [30] [LeakyCLIP: Extracting Training Data from CLIP](https://arxiv.org/abs/2508.00756)
*Yunhao Chen,Shujie Wang,Xin Wang,Xingjun Ma*

Main category: cs.CR

TL;DR: 论文研究了CLIP模型中的数据记忆和隐私泄露风险，提出LeakyCLIP攻击框架，通过CLIP反演实现高质量图像重建，并揭示了多模态模型的隐私风险。


<details>
  <summary>Details</summary>
Motivation: 理解CLIP模型中的记忆和隐私泄露风险对保障多模态模型安全至关重要。

Method: 提出LeakyCLIP框架，通过对抗性微调、线性变换嵌入对齐和Stable Diffusion细化解决CLIP反演中的三大挑战。

Result: LeakyCLIP在ViT-B-16上实现了SSIM指标358%的提升，并揭示了低质量重建中仍存在的数据泄露风险。

Conclusion: LeakyCLIP为CLIP反演提供了实用方法，同时揭示了多模态模型的隐私风险范围和性质。

Abstract: Understanding the memorization and privacy leakage risks in Contrastive
Language--Image Pretraining (CLIP) is critical for ensuring the security of
multimodal models. Recent studies have demonstrated the feasibility of
extracting sensitive training examples from diffusion models, with conditional
diffusion models exhibiting a stronger tendency to memorize and leak
information. In this work, we investigate data memorization and extraction
risks in CLIP through the lens of CLIP inversion, a process that aims to
reconstruct training images from text prompts. To this end, we introduce
\textbf{LeakyCLIP}, a novel attack framework designed to achieve high-quality,
semantically accurate image reconstruction from CLIP embeddings. We identify
three key challenges in CLIP inversion: 1) non-robust features, 2) limited
visual semantics in text embeddings, and 3) low reconstruction fidelity. To
address these challenges, LeakyCLIP employs 1) adversarial fine-tuning to
enhance optimization smoothness, 2) linear transformation-based embedding
alignment, and 3) Stable Diffusion-based refinement to improve fidelity.
Empirical results demonstrate the superiority of LeakyCLIP, achieving over 358%
improvement in Structural Similarity Index Measure (SSIM) for ViT-B-16 compared
to baseline methods on LAION-2B subset. Furthermore, we uncover a pervasive
leakage risk, showing that training data membership can even be successfully
inferred from the metrics of low-fidelity reconstructions. Our work introduces
a practical method for CLIP inversion while offering novel insights into the
nature and scope of privacy risks in multimodal models.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [31] [Rethinking Evidence Hierarchies in Medical Language Benchmarks: A Critical Evaluation of HealthBench](https://arxiv.org/abs/2508.00081)
*Fred Mutisya,Shikoh Gitau,Nasubo Ongoma,Keith Mbae,Elizabeth Wamicha*

Main category: cs.AI

TL;DR: HealthBench评估AI医疗系统能力，但依赖专家意见可能引入偏见。提出基于临床指南的改进方法，以提升全球适用性和公平性。


<details>
  <summary>Details</summary>
Motivation: 解决HealthBench依赖专家意见导致的区域偏见和临床差异问题，特别是在低收入地区。

Method: 通过版本控制的临床实践指南（CPGs）和GRADE证据评级，设计证据稳健的强化学习框架。

Result: 提出结合透明度和伦理考量的改进方案，旨在提升模型的临床可信度和全球适用性。

Conclusion: 基于严格审查的CPGs改进HealthBench，可生成更可靠、公平且全球适用的医疗语言模型。

Abstract: HealthBench, a benchmark designed to measure the capabilities of AI systems
for health better (Arora et al., 2025), has advanced medical language model
evaluation through physician-crafted dialogues and transparent rubrics.
However, its reliance on expert opinion, rather than high-tier clinical
evidence, risks codifying regional biases and individual clinician
idiosyncrasies, further compounded by potential biases in automated grading
systems. These limitations are particularly magnified in low- and middle-income
settings, where issues like sparse neglected tropical disease coverage and
region-specific guideline mismatches are prevalent.
  The unique challenges of the African context, including data scarcity,
inadequate infrastructure, and nascent regulatory frameworks, underscore the
urgent need for more globally relevant and equitable benchmarks. To address
these shortcomings, we propose anchoring reward functions in version-controlled
Clinical Practice Guidelines (CPGs) that incorporate systematic reviews and
GRADE evidence ratings.
  Our roadmap outlines "evidence-robust" reinforcement learning via
rubric-to-guideline linkage, evidence-weighted scoring, and contextual override
logic, complemented by a focus on ethical considerations and the integration of
delayed outcome feedback. By re-grounding rewards in rigorously vetted CPGs,
while preserving HealthBench's transparency and physician engagement, we aim to
foster medical language models that are not only linguistically polished but
also clinically trustworthy, ethically sound, and globally relevant.

</details>


### [32] [Hyperproperty-Constrained Secure Reinforcement Learning](https://arxiv.org/abs/2508.00106)
*Ernest Bonnah,Luan Viet Nguyen,Khaza Anuarul Hoque*

Main category: cs.AI

TL;DR: 本文提出了一种基于HyperTWTL的安全强化学习方法（SecRL），通过动态Boltzmann softmax RL学习满足HyperTWTL约束的安全最优策略，并在机器人任务中验证了其有效性和可扩展性。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要集中在时间逻辑约束的安全强化学习（SRL），但缺乏基于超属性（hyperproperties）的安全感知强化学习方法。HyperTWTL能有效表示安全、不透明性和并发性，因此探索其在SecRL中的应用具有重要意义。

Method: 将智能体的动态建模为马尔可夫决策过程（MDP），并将不透明性/安全约束形式化为HyperTWTL，提出使用动态Boltzmann softmax RL学习满足约束的安全最优策略。

Result: 通过机器人拾取与交付任务的案例研究验证了方法的有效性和可扩展性，并优于两种基线RL算法。

Conclusion: 提出的方法在满足HyperTWTL约束的同时，能够学习到安全最优策略，为安全感知强化学习提供了新思路。

Abstract: Hyperproperties for Time Window Temporal Logic (HyperTWTL) is a
domain-specific formal specification language known for its effectiveness in
compactly representing security, opacity, and concurrency properties for
robotics applications. This paper focuses on HyperTWTL-constrained secure
reinforcement learning (SecRL). Although temporal logic-constrained safe
reinforcement learning (SRL) is an evolving research problem with several
existing literature, there is a significant research gap in exploring
security-aware reinforcement learning (RL) using hyperproperties. Given the
dynamics of an agent as a Markov Decision Process (MDP) and opacity/security
constraints formalized as HyperTWTL, we propose an approach for learning
security-aware optimal policies using dynamic Boltzmann softmax RL while
satisfying the HyperTWTL constraints. The effectiveness and scalability of our
proposed approach are demonstrated using a pick-up and delivery robotic mission
case study. We also compare our results with two other baseline RL algorithms,
showing that our proposed method outperforms them.

</details>


### [33] [No AI Without PI! Object-Centric Process Mining as the Enabler for Generative, Predictive, and Prescriptive Artificial Intelligence](https://arxiv.org/abs/2508.00116)
*Wil M. P. van der Aalst*

Main category: cs.AI

TL;DR: AI在工业环境中的应用面临挑战，需要结合对象中心过程挖掘（OCPM）和过程智能（PI）来提升端到端操作流程。


<details>
  <summary>Details</summary>
Motivation: 探讨AI在工业环境中成功应用的挑战，强调过程智能（PI）的重要性。

Method: 提出对象中心过程挖掘（OCPM）作为连接数据和过程的桥梁，结合生成、预测和规范性AI。

Result: OCPM和PI的结合能有效提升操作流程，为AI在组织环境中的应用提供支持。

Conclusion: AI需要过程智能（PI）和OCPM的结合，以优化工业环境中的操作流程。

Abstract: The uptake of Artificial Intelligence (AI) impacts the way we work, interact,
do business, and conduct research. However, organizations struggle to apply AI
successfully in industrial settings where the focus is on end-to-end
operational processes. Here, we consider generative, predictive, and
prescriptive AI and elaborate on the challenges of diagnosing and improving
such processes. We show that AI needs to be grounded using Object-Centric
Process Mining (OCPM). Process-related data are structured and
organization-specific and, unlike text, processes are often highly dynamic.
OCPM is the missing link connecting data and processes and enables different
forms of AI. We use the term Process Intelligence (PI) to refer to the
amalgamation of process-centric data-driven techniques able to deal with a
variety of object and event types, enabling AI in an organizational context.
This paper explains why AI requires PI to improve operational processes and
highlights opportunities for successfully combining OCPM and generative,
predictive, and prescriptive AI.

</details>


### [34] [Algorithmic Detection of Rank Reversals, Transitivity Violations, and Decomposition Inconsistencies in Multi-Criteria Decision Analysis](https://arxiv.org/abs/2508.00129)
*Agustín Borda,Juan Bautista Cabral,Gonzalo Giarda,Diego Nicolás Gimenez Irusta,Paula Pacheco,Alvaro Roy Schachner*

Main category: cs.AI

TL;DR: 本文提出了三种检测多准则决策分析中排序反转问题的测试方法，并在Scikit-Criteria库中实现，讨论了通用场景下的实现挑战及设计考量。


<details>
  <summary>Details</summary>
Motivation: 排序反转是多准则决策分析中的严重问题，影响决策方法的有效性，因此需要一种机制来评估方法的性能。

Method: 提出三种测试方法检测排序反转，并在Scikit-Criteria库中实现，解决通用场景下的实现挑战。

Result: 实现了三种测试方法，并讨论了其在通用场景中的应用和设计考量。

Conclusion: 这些测试方法在多准则决策方法的评估中可能发挥重要作用。

Abstract: In Multi-Criteria Decision Analysis, Rank Reversals are a serious problem
that can greatly affect the results of a Multi-Criteria Decision Method against
a particular set of alternatives. It is therefore useful to have a mechanism
that allows one to measure the performance of a method on a set of
alternatives. This idea could be taken further to build a global ranking of the
effectiveness of different methods to solve a problem. In this paper, we
present three tests that detect the presence of Rank Reversals, along with
their implementation in the Scikit-Criteria library. We also address the
complications that arise when implementing these tests for general scenarios
and the design considerations we made to handle them. We close with a
discussion about how these additions could play a major role in the judgment of
multi-criteria decision methods for problem solving.

</details>


### [35] [SHACL Validation under Graph Updates (Extended Paper)](https://arxiv.org/abs/2508.00137)
*Shqiponja Ahmetaj,George Konstantinidis,Magdalena Ortiz,Paolo Pareti,Mantas Simkus*

Main category: cs.AI

TL;DR: 研究SHACL在RDF图更新下的验证问题，提出一种基于SHACL的更新语言，并通过回归技术将静态验证问题转化为SHACL约束的（不）可满足性问题。


<details>
  <summary>Details</summary>
Motivation: 解决RDF图在更新后仍满足SHACL规范的静态验证问题，为演化中的RDF图提供推理基础。

Method: 提出基于SHACL的更新语言，利用回归技术将更新动作嵌入SHACL约束，分析计算复杂度，并实现原型系统进行验证。

Result: 展示了静态验证问题可转化为SHACL约束的（不）可满足性问题，分析了计算复杂度，并通过实验验证了原型系统的行为。

Conclusion: 为RDF图的动态演化提供了一种有效的静态验证方法，并展示了其可行性和实用性。

Abstract: SHACL (SHApe Constraint Language) is a W3C standardized constraint language
for RDF graphs. In this paper, we study SHACL validation in RDF graphs under
updates. We present a SHACL-based update language that can capture intuitive
and realistic modifications on RDF graphs and study the problem of static
validation under such updates. This problem asks to verify whether every graph
that validates a SHACL specification will still do so after applying a given
update sequence. More importantly, it provides a basis for further services for
reasoning about evolving RDF graphs. Using a regression technique that embeds
the update actions into SHACL constraints, we show that static validation under
updates can be reduced to (un)satisfiability of constraints in (a minor
extension of) SHACL. We analyze the computational complexity of the static
validation problem for SHACL and some key fragments. Finally, we present a
prototype implementation that performs static validation and other static
analysis tasks on SHACL constraints and demonstrate its behavior through
preliminary experiments.

</details>


### [36] [Co-Producing AI: Toward an Augmented, Participatory Lifecycle](https://arxiv.org/abs/2508.00138)
*Rashid Mushkani,Hugo Berard,Toumadher Ammar,Cassandre Chatonnier,Shin Koseki*

Main category: cs.AI

TL;DR: 论文提出了一种基于设计正义、扩展学习理论和参与式AI的AI生产流程重构方法，强调共生产、多样性、公平性和多学科合作。


<details>
  <summary>Details</summary>
Motivation: 尽管已有努力减少AI算法的风险和偏见，但其仍对文化边缘群体造成不成比例的影响。

Method: 提出一个增强的AI生命周期，包含五个相互关联的阶段：共框架、共设计、共实施、共部署和共维护，并通过多学科研讨会验证。

Result: 该生命周期结合了分布式权威和迭代知识交换的主题，并与多个伦理框架相关联。

Conclusion: 需要进一步研究以扩展参与式治理，解决剩余的关键问题。

Abstract: Despite efforts to mitigate the inherent risks and biases of artificial
intelligence (AI) algorithms, these algorithms can disproportionately impact
culturally marginalized groups. A range of approaches has been proposed to
address or reduce these risks, including the development of ethical guidelines
and principles for responsible AI, as well as technical solutions that promote
algorithmic fairness. Drawing on design justice, expansive learning theory, and
recent empirical work on participatory AI, we argue that mitigating these harms
requires a fundamental re-architecture of the AI production pipeline. This
re-design should center co-production, diversity, equity, inclusion (DEI), and
multidisciplinary collaboration. We introduce an augmented AI lifecycle
consisting of five interconnected phases: co-framing, co-design,
co-implementation, co-deployment, and co-maintenance. The lifecycle is informed
by four multidisciplinary workshops and grounded in themes of distributed
authority and iterative knowledge exchange. Finally, we relate the proposed
lifecycle to several leading ethical frameworks and outline key research
questions that remain for scaling participatory governance.

</details>


### [37] [Beyond Agreement: Rethinking Ground Truth in Educational AI Annotation](https://arxiv.org/abs/2508.00143)
*Danielle R. Thomas,Conrad Borchers,Kenneth R. Koedinger*

Main category: cs.AI

TL;DR: 论文指出人类评估者的局限性，提出过度依赖传统评估方法（如IRR）会阻碍教育数据分类的进展，并推荐五种补充方法以提高数据标注质量和模型效果。


<details>
  <summary>Details</summary>
Motivation: 人类评估者存在偏见和不可靠性，传统评估方法（如IRR）无法满足教育AI对高质量训练数据的需求。

Method: 提出五种补充评估方法，包括多标签标注方案、专家方法和闭环验证等。

Result: 这些方法能产生更有效的训练数据和模型，提升学生学习效果和可操作性。

Conclusion: 呼吁重新定义标注质量，优先考虑有效性和教育影响，而非仅依赖共识。

Abstract: Humans can be notoriously imperfect evaluators. They are often biased,
unreliable, and unfit to define "ground truth." Yet, given the surging need to
produce large amounts of training data in educational applications using AI,
traditional inter-rater reliability (IRR) metrics like Cohen's kappa remain
central to validating labeled data. IRR remains a cornerstone of many machine
learning pipelines for educational data. Take, for example, the classification
of tutors' moves in dialogues or labeling open responses in machine-graded
assessments. This position paper argues that overreliance on human IRR as a
gatekeeper for annotation quality hampers progress in classifying data in ways
that are valid and predictive in relation to improving learning. To address
this issue, we highlight five examples of complementary evaluation methods,
such as multi-label annotation schemes, expert-based approaches, and
close-the-loop validity. We argue that these approaches are in a better
position to produce training data and subsequent models that produce improved
student learning and more actionable insights than IRR approaches alone. We
also emphasize the importance of external validity, for example, by
establishing a procedure of validating tutor moves and demonstrating that it
works across many categories of tutor actions (e.g., providing hints). We call
on the field to rethink annotation quality and ground truth--prioritizing
validity and educational impact over consensus alone.

</details>


### [38] [Model-Based Soft Maximization of Suitable Metrics of Long-Term Human Power](https://arxiv.org/abs/2508.00159)
*Jobst Heitzig,Ram Potham*

Main category: cs.AI

TL;DR: 本文探讨通过设计一种目标函数，明确要求AI增强人类能力并管理人类与AI之间的权力平衡，以促进安全和福祉。


<details>
  <summary>Details</summary>
Motivation: 研究AI安全性中的权力概念，提出通过明确目标函数来平衡人类与AI的权力，以提升安全和人类福祉。

Method: 采用部分公理化方法，设计可参数化和分解的目标函数，考虑人类有限理性和社会规范，并通过逆向归纳或多智能体强化学习计算该指标。

Result: 在多种典型情境中展示了该指标的效果，表明软最大化人类权力指标可能比直接基于效用的目标更安全。

Conclusion: 软最大化适合的人类权力指标可能是AI系统的有益目标，比直接效用目标更安全。

Abstract: Power is a key concept in AI safety: power-seeking as an instrumental goal,
sudden or gradual disempowerment of humans, power balance in human-AI
interaction and international AI governance. At the same time, power as the
ability to pursue diverse goals is essential for wellbeing.
  This paper explores the idea of promoting both safety and wellbeing by
forcing AI agents explicitly to empower humans and to manage the power balance
between humans and AI agents in a desirable way. Using a principled, partially
axiomatic approach, we design a parametrizable and decomposable objective
function that represents an inequality- and risk-averse long-term aggregate of
human power. It takes into account humans' bounded rationality and social
norms, and, crucially, considers a wide variety of possible human goals.
  We derive algorithms for computing that metric by backward induction or
approximating it via a form of multi-agent reinforcement learning from a given
world model. We exemplify the consequences of (softly) maximizing this metric
in a variety of paradigmatic situations and describe what instrumental
sub-goals it will likely imply. Our cautious assessment is that softly
maximizing suitable aggregate metrics of human power might constitute a
beneficial objective for agentic AI systems that is safer than direct
utility-based objectives.

</details>


### [39] [RL-PLUS: Countering Capability Boundary Collapse of LLMs in Reinforcement Learning with Hybrid-policy Optimization](https://arxiv.org/abs/2508.00222)
*Yihong Dong,Xue Jiang,Yongding Tao,Huanyu Liu,Kechi Zhang,Lili Mou,Rongyu Cao,Yingwei Ma,Jue Chen,Binhua Li,Zhi Jin,Fei Huang,Yongbin Li,Ge Li*

Main category: cs.AI

TL;DR: RL-PLUS通过结合内部推理和外部数据，解决了RLVR方法的局限性，显著提升了LLM的推理能力。


<details>
  <summary>Details</summary>
Motivation: RLVR方法因固有的策略限制和稀疏奖励，难以突破基础LLM的能力边界，甚至可能导致能力边界崩溃。

Method: RL-PLUS整合了多重重要性采样和基于探索的优势函数，以利用外部数据并引导模型探索高价值路径。

Result: 在六个数学推理基准和六个分布外推理任务中，RL-PLUS表现优异，平均相对提升21.1%至69.2%。

Conclusion: RL-PLUS不仅解决了能力边界崩溃问题，还在多个基准上实现了最先进的性能。

Abstract: Reinforcement Learning with Verifiable Reward (RLVR) has significantly
advanced the complex reasoning abilities of Large Language Models (LLMs).
However, it struggles to break through the inherent capability boundaries of
the base LLM, due to its inherently on-policy strategy with LLM's immense
action space and sparse reward. Further, RLVR can lead to the capability
boundary collapse, narrowing the LLM's problem-solving scope. To address this
problem, we propose RL-PLUS, a novel approach that synergizes internal
exploitation (i.e., Thinking) with external data (i.e., Learning) to achieve
stronger reasoning capabilities and surpass the boundaries of base models.
RL-PLUS integrates two core components: Multiple Importance Sampling to address
for distributional mismatch from external data, and an Exploration-Based
Advantage Function to guide the model towards high-value, unexplored reasoning
paths. We provide both theoretical analysis and extensive experiments to
demonstrate the superiority and generalizability of our approach. The results
show that RL-PLUS achieves state-of-the-art performance compared with existing
RLVR methods on six math reasoning benchmarks and exhibits superior performance
on six out-of-distribution reasoning tasks. It also achieves consistent and
significant gains across diverse model families, with average relative
improvements ranging from 21.1\% to 69.2\%. Moreover, Pass@k curves across
multiple benchmarks indicate that RL-PLUS effectively resolves the capability
boundary collapse problem.

</details>


### [40] [MetaAgent: Toward Self-Evolving Agent via Tool Meta-Learning](https://arxiv.org/abs/2508.00271)
*Hongjin Qian,Zheng Liu*

Main category: cs.AI

TL;DR: MetaAgent是一个基于学习实践和自我改进的智能代理，通过动态生成帮助请求、自我反思和知识积累，逐步提升任务解决能力。


<details>
  <summary>Details</summary>
Motivation: 旨在开发一种能够通过实践和自我改进不断优化任务解决能力的智能代理系统。

Method: MetaAgent通过生成自然语言帮助请求、自我反思和验证答案，动态积累经验并构建内部工具和知识库。

Result: 在GAIA、WebWalkerQA和BrowseCamp等基准测试中表现优于基线方法，与端到端训练的代理相当或更好。

Conclusion: MetaAgent展示了自我进化代理系统在通用知识发现中的潜力，无需调整模型参数或额外训练。

Abstract: In this work, we propose MetaAgent, an agentic paradigm inspired by the
principle of learning-by-doing, where expertise is developed through hands-on
practice and continual self-improvement. MetaAgent starts with a minimal
workflow, equipped only with basic reasoning and adaptive help-seeking
abilities. When a knowledge gap is encountered, MetaAgent generates natural
language help requests, which are routed to the most suitable external tool by
a dedicated tool router. As MetaAgent solves tasks, it continually conducts
self-reflection and answer verification, distilling actionable experience into
concise texts that are dynamically incorporated into future task contexts.
Besides, MetaAgent autonomously builds in-house tools and a persistent
knowledge base by organizing its tool-use history, further enhancing its
ability to retrieve and integrate relevant information We term this continual,
data-driven process as \textit{meta tool learning}, through which MetaAgent
incrementally refines its reasoning and tool-use strategies, without changing
model parameters or requiring further post-training. Evaluated on challenging
knowledge discovery benchmarks, including GAIA, WebWalkerQA, and BrowseCamp,
MetaAgent consistently outperforms workflow-based baselines and matches or
exceeds end-to-end trained agents, demonstrating the promise of self-evolving
agentic systems for robust, general-purpose knowledge discovery. We provide our
source codes in https://github.com/qhjqhj00/MetaAgent.

</details>


### [41] [Mind the Gap: The Divergence Between Human and LLM-Generated Tasks](https://arxiv.org/abs/2508.00282)
*Yi-Long Lu,Jiajun Song,Chunhui Zhang,Wei Wang*

Main category: cs.AI

TL;DR: 论文比较了人类和LLM（GPT-4o）在任务生成中的差异，发现人类行为受心理驱动因素影响，而LLM未能模拟这些模式。


<details>
  <summary>Details</summary>
Motivation: 探讨生成式代理（如LLM）是否能模拟人类基于内在动机的任务生成行为。

Method: 通过任务生成实验比较人类和GPT-4o的行为模式，分析心理驱动因素的影响。

Result: 人类任务生成受心理驱动因素影响，而LLM生成的任务更抽象、缺乏社交性和身体活动，尽管被认为更有趣和新颖。

Conclusion: LLM与人类认知存在核心差距，需整合内在动机和物理基础以设计更人性化的代理。

Abstract: Humans constantly generate a diverse range of tasks guided by internal
motivations. While generative agents powered by large language models (LLMs)
aim to simulate this complex behavior, it remains uncertain whether they
operate on similar cognitive principles. To address this, we conducted a
task-generation experiment comparing human responses with those of an LLM agent
(GPT-4o). We find that human task generation is consistently influenced by
psychological drivers, including personal values (e.g., Openness to Change) and
cognitive style. Even when these psychological drivers are explicitly provided
to the LLM, it fails to reflect the corresponding behavioral patterns. They
produce tasks that are markedly less social, less physical, and thematically
biased toward abstraction. Interestingly, while the LLM's tasks were perceived
as more fun and novel, this highlights a disconnect between its linguistic
proficiency and its capacity to generate human-like, embodied goals.We conclude
that there is a core gap between the value-driven, embodied nature of human
cognition and the statistical patterns of LLMs, highlighting the necessity of
incorporating intrinsic motivation and physical grounding into the design of
more human-aligned agents.

</details>


### [42] [Oedipus and the Sphinx: Benchmarking and Improving Visual Language Models for Complex Graphic Reasoning](https://arxiv.org/abs/2508.00323)
*Jianyi Zhang,Xu Ji,Ziyin Zhou,Yuchen Zhou,Shubo Shi,Haoyu Wu,Zhen Li,Shizhao Liu*

Main category: cs.AI

TL;DR: ReasonBench是首个专注于结构化图形推理任务的评估基准，用于评估视觉语言模型（VLMs）在复杂图形推理中的表现，揭示了当前模型的局限性，并提出双重优化策略提升性能。


<details>
  <summary>Details</summary>
Motivation: VLMs在模拟人类图形推理能力方面存在明显不足，尤其是在复杂图形推理和抽象问题解决方面，现有研究仅关注简单图形。

Method: 提出ReasonBench基准，包含1,613个真实世界智力测试问题，涵盖位置、属性、数量和多元素任务等推理维度，并评估11种主流VLMs。提出双重优化策略：DiaCoT增强推理可解释性，ReasonTune通过训练提升任务适应性。

Result: 实验显示当前VLMs在复杂图形推理中存在显著局限性，双重优化策略使VLM性能提升33.5%。

Conclusion: ReasonBench为复杂图形推理提供了全面评估工具，双重优化策略显著提升了VLMs的性能，为未来研究提供了方向。

Abstract: Evaluating the performance of visual language models (VLMs) in graphic
reasoning tasks has become an important research topic. However, VLMs still
show obvious deficiencies in simulating human-level graphic reasoning
capabilities, especially in complex graphic reasoning and abstract problem
solving, which are less studied and existing studies only focus on simple
graphics. To evaluate the performance of VLMs in complex graphic reasoning, we
propose ReasonBench, the first evaluation benchmark focused on structured
graphic reasoning tasks, which includes 1,613 questions from real-world
intelligence tests. ReasonBench covers reasoning dimensions related to
location, attribute, quantity, and multi-element tasks, providing a
comprehensive evaluation of the performance of VLMs in spatial, relational, and
abstract reasoning capabilities. We benchmark 11 mainstream VLMs (including
closed-source and open-source models) and reveal significant limitations of
current models. Based on these findings, we propose a dual optimization
strategy: Diagrammatic Reasoning Chain (DiaCoT) enhances the interpretability
of reasoning by decomposing layers, and ReasonTune enhances the task
adaptability of model reasoning through training, all of which improves VLM
performance by 33.5\%. All experimental data and code are in the repository:
https://huggingface.co/datasets/cistine/ReasonBench.

</details>


### [43] [R1-ACT: Efficient Reasoning Model Safety Alignment by Activating Safety Knowledge](https://arxiv.org/abs/2508.00324)
*Yeonjun In,Wonjoong Kim,Sangwu Park,Chanyoung Park*

Main category: cs.AI

TL;DR: R1-Act是一种简单高效的后训练方法，通过结构化推理过程显式触发安全知识，显著提升大型推理模型的安全性，同时保持推理性能。


<details>
  <summary>Details</summary>
Motivation: 尽管大型推理模型（LRMs）在复杂任务上表现出色，但其容易执行有害指令，引发安全隐患。研究发现模型已具备足够安全知识，但推理时未能激活。

Method: 提出R1-Act方法，通过结构化推理显式触发安全知识，仅需少量训练数据和短时间训练。

Result: R1-Act在安全性和推理性能上均优于现有对齐方法，且具有鲁棒性和可扩展性。

Conclusion: R1-Act是一种高效实用的方法，显著提升LRMs的安全性，同时保持性能。

Abstract: Although large reasoning models (LRMs) have demonstrated impressive
capabilities on complex tasks, recent studies reveal that these models
frequently fulfill harmful user instructions, raising significant safety
concerns. In this paper, we investigate the underlying cause of LRM safety
risks and find that models already possess sufficient safety knowledge but fail
to activate it during reasoning. Based on this insight, we propose R1-Act, a
simple and efficient post-training method that explicitly triggers safety
knowledge through a structured reasoning process. R1-Act achieves strong safety
improvements while preserving reasoning performance, outperforming prior
alignment methods. Notably, it requires only 1,000 training examples and 90
minutes of training on a single RTX A6000 GPU. Extensive experiments across
multiple LRM backbones and sizes demonstrate the robustness, scalability, and
practical efficiency of our approach.

</details>


### [44] [CoRGI: Verified Chain-of-Thought Reasoning with Visual Grounding](https://arxiv.org/abs/2508.00378)
*Shixin Yi,Lin Shang*

Main category: cs.AI

TL;DR: CoRGI框架通过引入视觉验证机制，解决了视觉语言模型（VLM）中推理链缺乏视觉依据的问题，提升了多模态推理的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有Chain-of-Thought（CoT）提示方法在视觉语言模型中生成的解释虽然语言流畅，但缺乏视觉内容的依据，导致幻觉问题。

Method: CoRGI采用三阶段流程：生成文本推理链、通过专用模块（VEVM）提取视觉证据、结合文本和视觉证据生成验证后的答案。

Result: 在VCR基准测试中，CoRGI显著提升了Qwen-2.5VL和LLaVA-1.6两种VLM的推理性能，并通过消融实验验证了各步骤的有效性。

Conclusion: 视觉验证对提升多模态推理的鲁棒性至关重要，CoRGI框架为现有VLM提供了一种无需端到端重新训练的改进方案。

Abstract: Chain-of-Thought (CoT) prompting has shown promise in improving reasoning in
vision-language models (VLMs), but it often produces explanations that are
linguistically fluent yet lack grounding in visual content. We observe that
such hallucinations arise in part from the absence of an explicit verification
mechanism during multi-step reasoning. To address this, we propose
\textbf{CoRGI}(\textbf{C}hain \textbf{o}f \textbf{R}easoning with
\textbf{G}rounded \textbf{I}nsights), a modular framework that introduces
visual verification into the reasoning process. CoRGI follows a three-stage
pipeline: it first generates a textual reasoning chain, then extracts
supporting visual evidence for each reasoning step via a dedicated module
(VEVM), and finally synthesizes the textual rationale with visual evidence to
generate a grounded, verified answer. The framework can be integrated with
existing VLMs without end-to-end retraining. We evaluate CoRGI on the VCR
benchmark and find that it improves reasoning performance on two representative
open-source VLM backbones, Qwen-2.5VL and LLaVA-1.6. Ablation studies confirm
the contribution of each step in the verification module, and human evaluations
suggest that CoRGI leads to more factual and helpful explanations. We also
examine alternative designs for the visual verification step and discuss
potential limitations of post-hoc verification frameworks. These findings
highlight the importance of grounding intermediate reasoning steps in visual
evidence to enhance the robustness of multimodal reasoning.

</details>


### [45] [Theory of Mind Using Active Inference: A Framework for Multi-Agent Cooperation](https://arxiv.org/abs/2508.00401)
*Riddhi J. Pitliya,Ozan Catal,Toon Van de Maele,Corrado Pezzato,Tim Verbelen*

Main category: cs.AI

TL;DR: 提出了一种基于心智理论（ToM）的多智能体协作方法，通过主动推理实现，无需任务特定的共享生成模型或显式通信。


<details>
  <summary>Details</summary>
Motivation: 解决多智能体协作中如何通过推理他人信念来优化自身行为的问题，避免依赖特定任务模型或通信。

Method: 扩展了基于推理树的规划算法，通过递归推理探索联合策略空间，智能体维护自身和他人信念与目标的不同表征。

Result: 在避碰和觅食任务中，ToM智能体表现出更好的协作能力，能避免碰撞并减少冗余行为。

Conclusion: 该方法为人工智能的实践应用提供了新思路，并计算性地揭示了ToM的作用。

Abstract: We present a novel approach to multi-agent cooperation by implementing theory
of mind (ToM) within active inference. ToM - the ability to understand that
others can have differing knowledge and goals - enables agents to reason about
others' beliefs while planning their own actions. Unlike previous active
inference approaches to multi-agent cooperation, our method neither relies on
task-specific shared generative models nor requires explicit communication,
while being generalisable. In our framework, the ToM-equipped agent maintains
distinct representations of its own and others' beliefs and goals. We extend
the sophisticated inference tree-based planning algorithm to systematically
explore joint policy spaces through recursive reasoning. Our approach is
evaluated through collision avoidance and foraging task simulations. Results
demonstrate that ToM-equipped agents cooperate better compared to non-ToM
counterparts by being able to avoid collisions and reduce redundant efforts.
Crucially, ToM agents accomplish this by inferring others' beliefs solely from
observable behaviour. This work advances practical applications in artificial
intelligence while providing computational insights into ToM.

</details>


### [46] [Cognitive Kernel-Pro: A Framework for Deep Research Agents and Agent Foundation Models Training](https://arxiv.org/abs/2508.00414)
*Tianqing Fang,Zhisong Zhang,Xiaoyang Wang,Rui Wang,Can Qin,Yuxuan Wan,Jun-Yu Ma,Ce Zhang,Jiaqi Chen,Xiyun Li,Hongming Zhang,Haitao Mi,Dong Yu*

Main category: cs.AI

TL;DR: Cognitive Kernel-Pro 是一个完全开源且免费的 AI 代理框架，旨在推动高级 AI 代理的开发与评估，并在 GAIA 基准测试中取得领先性能。


<details>
  <summary>Details</summary>
Motivation: 当前 AI 代理系统多为闭源或依赖付费 API，限制了研究的可访问性和可复现性。

Method: 通过构建高质量的训练数据（查询、轨迹和可验证答案）和探索代理测试时的反思与投票策略。

Result: 在 GAIA 上取得开源代理中的最佳性能，8B 参数模型超越 WebDancer 和 WebSailor。

Conclusion: Cognitive Kernel-Pro 为可访问的高性能 AI 代理设定了新标准。

Abstract: General AI Agents are increasingly recognized as foundational frameworks for
the next generation of artificial intelligence, enabling complex reasoning, web
interaction, coding, and autonomous research capabilities. However, current
agent systems are either closed-source or heavily reliant on a variety of paid
APIs and proprietary tools, limiting accessibility and reproducibility for the
research community. In this work, we present \textbf{Cognitive Kernel-Pro}, a
fully open-source and (to the maximum extent) free multi-module agent framework
designed to democratize the development and evaluation of advanced AI agents.
Within Cognitive Kernel-Pro, we systematically investigate the curation of
high-quality training data for Agent Foundation Models, focusing on the
construction of queries, trajectories, and verifiable answers across four key
domains: web, file, code, and general reasoning. Furthermore, we explore novel
strategies for agent test-time reflection and voting to enhance agent
robustness and performance. We evaluate Cognitive Kernel-Pro on GAIA, achieving
state-of-the-art results among open-source and free agents. Notably, our
8B-parameter open-source model surpasses previous leading systems such as
WebDancer and WebSailor, establishing a new performance standard for
accessible, high-capability AI agents. Code is available at
https://github.com/Tencent/CognitiveKernel-Pro

</details>


### [47] [Pro2Guard: Proactive Runtime Enforcement of LLM Agent Safety via Probabilistic Model Checking](https://arxiv.org/abs/2508.00500)
*Haoyu Wang,Chris M. Poskitt,Jun Sun,Jiali Wei*

Main category: cs.AI

TL;DR: Pro2Guard是一个基于概率可达性分析的主动运行时安全框架，用于预测和防止LLM代理的不安全行为。


<details>
  <summary>Details</summary>
Motivation: 现有基于规则的安全系统（如AgentSpec）缺乏预见性，难以应对长期依赖和分布变化，导致安全风险。

Method: Pro2Guard将代理行为抽象为符号状态，并从执行轨迹中学习离散时间马尔可夫链（DTMC），在运行时预测不安全状态的概率并提前干预。

Result: 在家庭代理和自动驾驶场景中，Pro2Guard分别实现了93.6%和100%的不安全行为预测，并能提前干预。

Conclusion: Pro2Guard通过主动预测和干预，显著提升了LLM代理的安全性，同时保持任务完成率。

Abstract: Large Language Model (LLM) agents exhibit powerful autonomous capabilities
across domains such as robotics, virtual assistants, and web automation.
However, their stochastic behavior introduces significant safety risks that are
difficult to anticipate. Existing rule-based enforcement systems, such as
AgentSpec, focus on developing reactive safety rules, which typically respond
only when unsafe behavior is imminent or has already occurred. These systems
lack foresight and struggle with long-horizon dependencies and distribution
shifts. To address these limitations, we propose Pro2Guard, a proactive runtime
enforcement framework grounded in probabilistic reachability analysis.
Pro2Guard abstracts agent behaviors into symbolic states and learns a
Discrete-Time Markov Chain (DTMC) from execution traces. At runtime, it
anticipates future risks by estimating the probability of reaching unsafe
states, triggering interventions before violations occur when the predicted
risk exceeds a user-defined threshold. By incorporating semantic validity
checks and leveraging PAC bounds, Pro2Guard ensures statistical reliability
while approximating the underlying ground-truth model. We evaluate Pro2Guard
extensively across two safety-critical domains: embodied household agents and
autonomous vehicles. In embodied agent tasks, Pro2Guard enforces safety early
on up to 93.6% of unsafe tasks using low thresholds, while configurable modes
(e.g., reflect) allow balancing safety with task success, maintaining up to
80.4% task completion. In autonomous driving scenarios, Pro2Guard achieves 100%
prediction of traffic law violations and collisions, anticipating risks up to
38.66 seconds ahead.

</details>


### [48] [Thinking Machines: Mathematical Reasoning in the Age of LLMs](https://arxiv.org/abs/2508.00459)
*Andrea Asperti,Alberto Naibo,Claudio Sacerdoti Coen*

Main category: cs.AI

TL;DR: 论文探讨了大语言模型（LLMs）在数学领域的应用，尤其是形式化数学证明中的挑战，分析了其与编程任务的差异，并提出了三个核心问题。


<details>
  <summary>Details</summary>
Motivation: 研究LLMs在数学领域的潜力，尤其是形式化数学证明中的表现，以揭示其推理和监督机制的局限性。

Method: 通过分析现有模型和基准测试，探讨LLMs在形式化与非形式化数学训练中的权衡、证明生成脆弱性的原因，以及LLMs是否真正跟踪逻辑状态。

Result: 发现LLMs在形式化数学证明中的表现较弱，与编程任务存在显著差异，揭示了其推理和监督机制的不足。

Conclusion: 论文旨在明确当前LLMs在数学领域的局限性，并提出未来可能的研究方向以扩展其能力。

Abstract: Large Language Models (LLMs) have shown remarkable abilities in structured
reasoning and symbolic tasks, with coding emerging as a particular area of
strength. This success has sparked growing interest in applying LLMs to
mathematics, both in informal problem-solving and formal theorem proving.
However, progress in formal mathematics has proven to be significantly more
difficult, despite surface-level similarities between programming and proof
construction. This discrepancy raises important questions about how LLMs
``reason'', how they are supervised, and whether they internally track a notion
of computational or deductive state. In this article, we address the
state-of-the-art of the discipline, focusing on recent models and benchmarks,
and explore three central issues at the intersection of machine learning and
mathematical cognition: (i) the trade-offs between formal and informal
mathematics as training domains; (ii) the deeper reasons why proof generation
remains more brittle than code synthesis; (iii) and the question of whether
LLMs represent, or merely mimic, a notion of evolving logical state. Our goal
is not to draw hard boundaries, but to identify where the current limits lie,
and how they might be extended.

</details>


### [49] [MultiSHAP: A Shapley-Based Framework for Explaining Cross-Modal Interactions in Multimodal AI Models](https://arxiv.org/abs/2508.00576)
*Zhanliang Wang,Kai Wang*

Main category: cs.AI

TL;DR: MultiSHAP是一个模型无关的解释框架，利用Shapley交互指数量化多模态AI模型中视觉和文本元素的协同效应，适用于开源和闭源模型。


<details>
  <summary>Details</summary>
Motivation: 多模态AI模型的“黑盒”特性在高风险应用中限制了其部署，需要可解释性和可信度。现有方法无法精确量化模态间的协同效应。

Method: 引入MultiSHAP框架，基于Shapley交互指数，分析视觉和文本元素的成对交互，提供实例级和数据集级解释。

Result: 实验证实MultiSHAP能准确捕捉跨模态推理机制，实际案例展示了其实用性。

Conclusion: MultiSHAP为解释复杂多模态AI模型提供了通用解决方案，并可扩展到两种以上模态。

Abstract: Multimodal AI models have achieved impressive performance in tasks that
require integrating information from multiple modalities, such as vision and
language. However, their "black-box" nature poses a major barrier to deployment
in high-stakes applications where interpretability and trustworthiness are
essential. How to explain cross-modal interactions in multimodal AI models
remains a major challenge. While existing model explanation methods, such as
attention map and Grad-CAM, offer coarse insights into cross-modal
relationships, they cannot precisely quantify the synergistic effects between
modalities, and are limited to open-source models with accessible internal
weights. Here we introduce MultiSHAP, a model-agnostic interpretability
framework that leverages the Shapley Interaction Index to attribute multimodal
predictions to pairwise interactions between fine-grained visual and textual
elements (such as image patches and text tokens), while being applicable to
both open- and closed-source models. Our approach provides: (1) instance-level
explanations that reveal synergistic and suppressive cross-modal effects for
individual samples - "why the model makes a specific prediction on this input",
and (2) dataset-level explanation that uncovers generalizable interaction
patterns across samples - "how the model integrates information across
modalities". Experiments on public multimodal benchmarks confirm that MultiSHAP
faithfully captures cross-modal reasoning mechanisms, while real-world case
studies demonstrate its practical utility. Our framework is extensible beyond
two modalities, offering a general solution for interpreting complex multimodal
AI models.

</details>


### [50] [From EMR Data to Clinical Insight: An LLM-Driven Framework for Automated Pre-Consultation Questionnaire Generation](https://arxiv.org/abs/2508.00581)
*Ruiqing Ding,Qianfang Sun,Yongkang Leng,Hui Yin,Xiaojian Li*

Main category: cs.AI

TL;DR: 提出了一种多阶段LLM驱动框架，用于从复杂电子病历生成全面的预咨询问卷，解决了直接LLM方法在信息完整性、逻辑顺序和疾病级合成方面的不足。


<details>
  <summary>Details</summary>
Motivation: 预咨询是医疗保健的重要组成部分，但从复杂电子病历生成问卷具有挑战性，直接LLM方法在信息完整性和逻辑性上表现不佳。

Method: 分三阶段：1）提取原子断言；2）构建个人因果网络并合成疾病知识；3）生成个性化问卷。

Result: 在真实电子病历数据集上验证，方法在信息覆盖、诊断相关性、可理解性和生成时间上表现优异。

Conclusion: 该框架通过明确临床知识克服了直接方法的限制，具有提升患者信息收集的潜力。

Abstract: Pre-consultation is a critical component of effective healthcare delivery.
However, generating comprehensive pre-consultation questionnaires from complex,
voluminous Electronic Medical Records (EMRs) is a challenging task. Direct
Large Language Model (LLM) approaches face difficulties in this task,
particularly regarding information completeness, logical order, and
disease-level synthesis. To address this issue, we propose a novel multi-stage
LLM-driven framework: Stage 1 extracts atomic assertions (key facts with
timing) from EMRs; Stage 2 constructs personal causal networks and synthesizes
disease knowledge by clustering representative networks from an EMR corpus;
Stage 3 generates tailored personal and standardized disease-specific
questionnaires based on these structured representations. This framework
overcomes limitations of direct methods by building explicit clinical
knowledge. Evaluated on a real-world EMR dataset and validated by clinical
experts, our method demonstrates superior performance in information coverage,
diagnostic relevance, understandability, and generation time, highlighting its
practical potential to enhance patient information collection.

</details>


### [51] [Multi-Agent Game Generation and Evaluation via Audio-Visual Recordings](https://arxiv.org/abs/2508.00632)
*Alexia Jolicoeur-Martineau*

Main category: cs.AI

TL;DR: 论文提出了AVR-Eval评估指标和AVR-Agent多代理系统，用于生成和评估交互式音视频内容，发现多代理系统生成的内容优于单次生成，但模型在利用高质量资源和反馈方面仍有不足。


<details>
  <summary>Details</summary>
Motivation: 解决当前AI在生成复杂交互式音视频内容（如视频游戏）时缺乏自动化评估指标和难以处理复杂内容的问题。

Method: 提出AVR-Eval评估指标，通过音视频记录比较内容质量；开发AVR-Agent多代理系统，生成并迭代优化JavaScript代码。

Result: AVR-Agent生成的内容在实验中表现优于单次生成的内容，但模型未能有效利用定制资源和音视频反馈。

Conclusion: 当前模型在利用高质量资源和反馈方面与人类存在差距，揭示了机器与人类内容创作方式的根本差异。

Abstract: While AI excels at generating text, audio, images, and videos, creating
interactive audio-visual content such as video games remains challenging.
Current LLMs can generate JavaScript games and animations, but lack automated
evaluation metrics and struggle with complex content that normally requires
teams of humans working for many months (multi-shot, multi-agents) using assets
made by artists. To tackle these issues, we built a new metric and a
multi-agent system.
  We propose AVR-Eval, a relative metric for multimedia content quality using
Audio-Visual Recordings (AVRs). An omni-modal model (processing text, video,
and audio) compares the AVRs of two contents, with a text model reviewing
evaluations to determine superiority. We show that AVR-Eval properly identifies
good from broken or mismatched content.
  We built AVR-Agent, a multi-agent system generating JavaScript code from a
bank of multimedia assets (audio, images, 3D models). The coding agent selects
relevant assets, generates multiple initial codes, uses AVR-Eval to identify
the best version, and iteratively improves it through omni-modal agent feedback
from the AVR.
  We run experiments on games and animations with AVR-Eval (win rate of content
A against B). We find that content generated by AVR-Agent has a significantly
higher win rate against content made through one-shot generation. However,
models struggle to leverage custom assets and AVR feedback effectively, showing
no higher win rate. This reveals a critical gap: while humans benefit from
high-quality assets and audio-visual feedback, current coding models do not
seem to utilize these resources as effectively, highlighting fundamental
differences between human and machine content creation approaches.

</details>


### [52] [Multi-Band Variable-Lag Granger Causality: A Unified Framework for Causal Time Series Inference across Frequencies](https://arxiv.org/abs/2508.00658)
*Chakattrai Sookkongwaree,Tattep Lakmuang,Chainarong Amornbunchornvej*

Main category: cs.AI

TL;DR: 论文提出了一种多频带可变滞后格兰杰因果关系（MB-VLGC）框架，解决了传统方法无法处理频率依赖因果延迟的问题，并在实验中验证了其优越性。


<details>
  <summary>Details</summary>
Motivation: 理解时间序列中的因果关系对多个领域至关重要，但传统格兰杰因果关系方法存在固定滞后假设的局限性，且现有可变滞后方法未考虑频率依赖的延迟。

Method: 提出MB-VLGC框架，通过显式建模频率依赖的因果延迟，扩展了传统可变滞后格兰杰因果关系，并设计了高效推理流程。

Result: 实验表明，MB-VLGC在合成和真实数据集上显著优于现有方法，验证了其广泛适用性。

Conclusion: MB-VLGC为时间序列因果分析提供了更灵活和准确的工具，适用于多领域应用。

Abstract: Understanding causal relationships in time series is fundamental to many
domains, including neuroscience, economics, and behavioral science. Granger
causality is one of the well-known techniques for inferring causality in time
series. Typically, Granger causality frameworks have a strong fix-lag
assumption between cause and effect, which is often unrealistic in complex
systems. While recent work on variable-lag Granger causality (VLGC) addresses
this limitation by allowing a cause to influence an effect with different time
lags at each time point, it fails to account for the fact that causal
interactions may vary not only in time delay but also across frequency bands.
For example, in brain signals, alpha-band activity may influence another region
with a shorter delay than slower delta-band oscillations. In this work, we
formalize Multi-Band Variable-Lag Granger Causality (MB-VLGC) and propose a
novel framework that generalizes traditional VLGC by explicitly modeling
frequency-dependent causal delays. We provide a formal definition of MB-VLGC,
demonstrate its theoretical soundness, and propose an efficient inference
pipeline. Extensive experiments across multiple domains demonstrate that our
framework significantly outperforms existing methods on both synthetic and
real-world datasets, confirming its broad applicability to any type of time
series data. Code and datasets are publicly available.

</details>


### [53] [Transparent Adaptive Learning via Data-Centric Multimodal Explainable AI](https://arxiv.org/abs/2508.00665)
*Maryam Mosleh,Marie Devlin,Ellis Solaiman*

Main category: cs.AI

TL;DR: 论文提出了一种结合传统可解释AI技术与生成式AI模型的混合框架，旨在为教育领域提供多模态、个性化的解释，以增强透明度和用户体验。


<details>
  <summary>Details</summary>
Motivation: 当前自适应学习系统缺乏透明度，且多数可解释AI技术忽视用户角色和理解能力，因此需要一种更动态、用户中心的解释方法。

Method: 提出混合框架，整合传统XAI技术与生成式AI模型，结合用户个性化需求，生成多模态解释。

Result: 框架重新定义可解释性为动态沟通过程，并探讨了其在教育中的局限性及研究方向（如准确性、公平性、个性化）。

Conclusion: 目标是推动可解释AI的发展，既提升透明度，又支持以用户为中心的体验。

Abstract: Artificial intelligence-driven adaptive learning systems are reshaping
education through data-driven adaptation of learning experiences. Yet many of
these systems lack transparency, offering limited insight into how decisions
are made. Most explainable AI (XAI) techniques focus on technical outputs but
neglect user roles and comprehension. This paper proposes a hybrid framework
that integrates traditional XAI techniques with generative AI models and user
personalisation to generate multimodal, personalised explanations tailored to
user needs. We redefine explainability as a dynamic communication process
tailored to user roles and learning goals. We outline the framework's design,
key XAI limitations in education, and research directions on accuracy,
fairness, and personalisation. Our aim is to move towards explainable AI that
enhances transparency while supporting user-centred experiences.

</details>


### [54] [Context-Aware Visualization for Explainable AI Recommendations in Social Media: A Vision for User-Aligned Explanations](https://arxiv.org/abs/2508.00674)
*Banan Alkhateeb,Ellis Solaiman*

Main category: cs.AI

TL;DR: 论文提出了一种用户分段的、上下文感知的可视化解释系统，以解决社交媒体推荐中解释性与用户需求不匹配的问题。


<details>
  <summary>Details</summary>
Motivation: 当前社交媒体推荐系统的解释性普遍且缺乏针对性，导致用户不理解推荐原因，降低了推荐的价值。

Method: 提出了一种结合用户需求和上下文的可视化解释系统，支持多种解释形式（如技术详细版和简化版），并首次在同一流程中调整解释风格（视觉与数字）和粒度（专家与普通用户）。

Result: 通过30名X用户的公开试点验证系统对决策和信任的影响。

Conclusion: 该框架为社交媒体推荐提供了更个性化和有效的解释方法，有望提升用户体验和信任。

Abstract: Social media platforms today strive to improve user experience through AI
recommendations, yet the value of such recommendations vanishes as users do not
understand the reasons behind them. This issue arises because explainability in
social media is general and lacks alignment with user-specific needs. In this
vision paper, we outline a user-segmented and context-aware explanation layer
by proposing a visual explanation system with diverse explanation methods. The
proposed system is framed by the variety of user needs and contexts, showing
explanations in different visualized forms, including a technically detailed
version for AI experts and a simplified one for lay users. Our framework is the
first to jointly adapt explanation style (visual vs. numeric) and granularity
(expert vs. lay) inside a single pipeline. A public pilot with 30 X users will
validate its impact on decision-making and trust.

</details>


### [55] [Unraveling Hidden Representations: A Multi-Modal Layer Analysis for Better Synthetic Content Forensics](https://arxiv.org/abs/2508.00784)
*Tom Or,Omri Azencot*

Main category: cs.AI

TL;DR: 提出了一种基于大型预训练多模态模型的通用生成内容检测方法，通过线性分类器实现跨模态的高效检测。


<details>
  <summary>Details</summary>
Motivation: 生成模型被恶意用于传播虚假信息，现有检测工具泛化能力差，需开发通用且高效的检测方法。

Method: 利用大型预训练多模态模型的潜在编码特征，训练线性分类器进行真假内容判别。

Result: 在音频和图像领域，该方法性能优于或匹配现有基线方法，且计算高效、训练快速。

Conclusion: 该方法为跨模态生成内容检测提供了通用且高效的解决方案。

Abstract: Generative models achieve remarkable results in multiple data domains,
including images and texts, among other examples. Unfortunately, malicious
users exploit synthetic media for spreading misinformation and disseminating
deepfakes. Consequently, the need for robust and stable fake detectors is
pressing, especially when new generative models appear everyday. While the
majority of existing work train classifiers that discriminate between real and
fake information, such tools typically generalize only within the same family
of generators and data modalities, yielding poor results on other generative
classes and data domains. Towards a universal classifier, we propose the use of
large pre-trained multi-modal models for the detection of generative content.
Effectively, we show that the latent code of these models naturally captures
information discriminating real from fake. Building on this observation, we
demonstrate that linear classifiers trained on these features can achieve
state-of-the-art results across various modalities, while remaining
computationally efficient, fast to train, and effective even in few-shot
settings. Our work primarily focuses on fake detection in audio and images,
achieving performance that surpasses or matches that of strong baseline
methods.

</details>
