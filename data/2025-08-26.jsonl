{"id": "2508.16671", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.16671", "abs": "https://arxiv.org/abs/2508.16671", "authors": ["Mingyang Zhou", "Quanming Yao", "Lun Du", "Lanning Wei", "Da Zheng"], "title": "Reflective Paper-to-Code Reproduction Enabled by Fine-Grained Verification", "comment": null, "summary": "Reproducing machine learning papers is essential for scientific progress but\nremains challenging for both humans and automated agents. Existing agent-based\nmethods often struggle to fully and accurately reproduce implementation details\nsuch as mathematical formulas and algorithmic logic. Previous studies show that\nreflection with explicit feedback improves agent performance. However, current\npaper reproduction methods fail to effectively adopt this strategy. This gap\nmainly arises from the diverse paper patterns, complex method modules, and\nvaried configurations encountered in research papers. Motivated by how humans\nuse systematic checklists to efficiently debug complex code, we propose\n\\textbf{RePro}, a \\textbf{Re}flective Paper-to-Code \\textbf{Repro}duction\nframework that automatically extracts a paper's fingerprint, referring to a\ncomprehensive set of accurate and atomic criteria serving as high-quality\nsupervisory signals. The framework first generates code based on the extracted\ninformation, and then leverages the fingerprint within iterative verification\nand refinement loop. This approach systematically detects discrepancies and\nproduces targeted revisions to align generated code with the paper's\nimplementation details. Extensive experiments on the PaperBench Code-Dev\nbenchmark have been conducted, RePro achieves 13.0\\% performance gap over\nbaselines, and it correctly revises complex logical and mathematical criteria\nin reflecting, on which the effectiveness is obvious."}
{"id": "2508.16678", "categories": ["cs.SE", "cs.MA"], "pdf": "https://arxiv.org/pdf/2508.16678", "abs": "https://arxiv.org/abs/2508.16678", "authors": ["Konrad Cinkusz", "Jaros≈Çaw A. Chudziak", "Ewa Niewiadomska-Szynkiewicz"], "title": "Cognitive Agents Powered by Large Language Models for Agile Software Project Management", "comment": null, "summary": "This paper investigates the integration of cognitive agents powered by Large\nLanguage Models (LLMs) within the Scaled Agile Framework (SAFe) to reinforce\nsoftware project management. By deploying virtual agents in simulated software\nenvironments, this study explores their potential to fulfill fundamental roles\nin IT project development, thereby optimizing project outcomes through\nintelligent automation. Particular emphasis is placed on the adaptability of\nthese agents to Agile methodologies and their transformative impact on\ndecision-making, problem-solving, and collaboration dynamics. The research\nleverages the CogniSim ecosystem, a platform designed to simulate real-world\nsoftware engineering challenges, such as aligning technical capabilities with\nbusiness objectives, managing interdependencies, and maintaining project\nagility. Through iterative simulations, cognitive agents demonstrate advanced\ncapabilities in task delegation, inter-agent communication, and project\nlifecycle management. By employing natural language processing to facilitate\nmeaningful dialogues, these agents emulate human roles and improve the\nefficiency and precision of Agile practices. Key findings from this\ninvestigation highlight the ability of LLM-powered cognitive agents to deliver\nmeasurable improvements in various metrics, including task completion times,\nquality of deliverables, and communication coherence. These agents exhibit\nscalability and adaptability, ensuring their applicability across diverse and\ncomplex project environments. This study underscores the potential of\nintegrating LLM-powered agents into Agile project management frameworks as a\nmeans of advancing software engineering practices. This integration not only\nrefines the execution of project management tasks but also sets the stage for a\nparadigm shift in how teams collaborate and address emerging challenges."}
{"id": "2508.16684", "categories": ["cs.SE", "cs.HC"], "pdf": "https://arxiv.org/pdf/2508.16684", "abs": "https://arxiv.org/abs/2508.16684", "authors": ["Vikranth Udandarao", "Nipun Misra"], "title": "Democratizing AI Development: Local LLM Deployment for India's Developer Ecosystem in the Era of Tokenized APIs", "comment": "for survey results, check\n  https://docs.google.com/spreadsheets/d/1t0eV9oURaiu2HfARWo6sriBO0eC8bHUyZNN7CgK2NAk/edit?usp=sharing", "summary": "India's developer community faces significant barriers to sustained\nexperimentation and learning with commercial Large Language Model (LLM) APIs,\nprimarily due to economic and infrastructural constraints. This study\nempirically evaluates local LLM deployment using Ollama as an alternative to\ncommercial cloud-based services for developer-focused applications. Through a\nmixed-methods analysis involving 180 Indian developers, students, and AI\nenthusiasts, we find that local deployment enables substantially greater\nhands-on development and experimentation, while reducing costs by 33% compared\nto commercial solutions. Developers using local LLMs completed over twice as\nmany experimental iterations and reported deeper understanding of advanced AI\narchitectures. Our results highlight local deployment as a critical enabler for\ninclusive and accessible AI development, demonstrating how technological\naccessibility can enhance learning outcomes and innovation capacity in\nresource-constrained environments."}
{"id": "2508.16688", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.16688", "abs": "https://arxiv.org/abs/2508.16688", "authors": ["Ankur Tomar", "Hengyue Liang", "Indranil Bhattacharya", "Natalia Larios", "Francesco Carbone"], "title": "Cybernaut: Towards Reliable Web Automation", "comment": null, "summary": "The emergence of AI-driven web automation through Large Language Models\n(LLMs) offers unprecedented opportunities for optimizing digital workflows.\nHowever, deploying such systems within industry's real-world environments\npresents four core challenges: (1) ensuring consistent execution, (2)\naccurately identifying critical HTML elements, (3) meeting human-like accuracy\nin order to automate operations at scale and (4) the lack of comprehensive\nbenchmarking data on internal web applications. Existing solutions are\nprimarily tailored for well-designed, consumer-facing websites (e.g.,\nAmazon.com, Apple.com) and fall short in addressing the complexity of\npoorly-designed internal web interfaces. To address these limitations, we\npresent Cybernaut, a novel framework to ensure high execution consistency in\nweb automation agents designed for robust enterprise use. Our contributions are\nthreefold: (1) a Standard Operating Procedure (SOP) generator that converts\nuser demonstrations into reliable automation instructions for linear browsing\ntasks, (2) a high-precision HTML DOM element recognition system tailored for\nthe challenge of complex web interfaces, and (3) a quantitative metric to\nassess execution consistency. The empirical evaluation on our internal\nbenchmark demonstrates that using our framework enables a 23.2% improvement\n(from 72% to 88.68%) in task execution success rate over the browser_use.\nCybernaut identifies consistent execution patterns with 84.7% accuracy,\nenabling reliable confidence assessment and adaptive guidance during task\nexecution in real-world systems. These results highlight Cybernaut's\neffectiveness in enterprise-scale web automation and lay a foundation for\nfuture advancements in web automation."}
{"id": "2508.16619", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2508.16619", "abs": "https://arxiv.org/abs/2508.16619", "authors": ["Rahul Mishra", "Sudhanshu Kumar Jha", "Naresh Kshetri", "Bishnu Bhusal", "Mir Mehedi Rahman", "Md Masud Rana", "Aimina Ali Eli", "Khaled Aminul Islam", "Bishwo Prakash Pokharel"], "title": "nodeWSNsec: A hybrid metaheuristic approach for reliable security and node deployment in WSNs", "comment": "12 pages, 9 figures", "summary": "Efficient and reliable node deployment in Wireless Sensor Networks is crucial\nfor optimizing coverage of the area, connectivity among nodes, and energy\nefficiency. This paper proposes a hybrid meta heuristic approach combining a\nGenetic Algorithm (GA) and Particle Swarm Optimization (PSO) to address the\nchallenges of energy efficient and reliable node deployment. The GA PSO hybrid\nleverages GAs strong exploration capabilities and PSOs rapid convergence,\nachieving an optimum stability between coverage and energy consumption. The\nperformance of the proposed approach is evaluated against GA and PSO alone and\nthe innovatory meta heuristic based Competitive Multi Objective Marine\nPredators Algorithm (CMOMPA) across varying sensing ranges. Simulation results\ndemonstrate that GA PSO requires 15% to 25% fewer sensor nodes and maintains\n95% or more area coverage while maintaining the connectivity in comparison to\nstandalone GA or PSO algorithm. The proposed algorithm also dominates CMOMPA\nwhen compared for long sensing and communication range in terms of higher\ncoverage, improved connectivity, and reduced deployment time while requiring\nfewer sensor nodes. This study also explores key trade offs in WSN deployment\nand highlights future research directions, including heterogeneous node\ndeployment, mobile WSNs, and enhanced multi objective optimization techniques.\nThe findings underscore the effectiveness of hybrid meta heuristics in\nimproving WSN performance, offering a promising approach for real world\napplications such as environmental monitoring, smart cities, smart agriculture,\ndisaster response, and IIoT."}
{"id": "2508.16681", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2508.16681", "abs": "https://arxiv.org/abs/2508.16681", "authors": ["Eric Zhang"], "title": "Revisiting Rule-Based Stuttering Detection: A Comprehensive Analysis of Interpretable Models for Clinical Applications", "comment": null, "summary": "Stuttering affects approximately 1% of the global population, impacting\ncommunication and quality of life. While recent advances in deep learning have\npushed the boundaries of automatic speech dysfluency detection, rule-based\napproaches remain crucial for clinical applications where interpretability and\ntransparency are paramount. This paper presents a comprehensive analysis of\nrule-based stuttering detection systems, synthesizing insights from multiple\ncorpora including UCLASS, FluencyBank, and SEP-28k. We propose an enhanced\nrule-based framework that incorporates speaking-rate normalization, multi-level\nacoustic feature analysis, and hierarchical decision structures. Our approach\nachieves competitive performance while maintaining complete\ninterpretability-critical for clinical adoption. We demonstrate that rule-based\nsystems excel particularly in prolongation detection (97-99% accuracy) and\nprovide stable performance across varying speaking rates. Furthermore, we show\nhow these interpretable models can be integrated with modern machine learning\npipelines as proposal generators or constraint modules, bridging the gap\nbetween traditional speech pathology practices and contemporary AI systems. Our\nanalysis reveals that while neural approaches may achieve marginally higher\naccuracy in unconstrained settings, rule-based methods offer unique advantages\nin clinical contexts where decision auditability, patient-specific tuning, and\nreal-time feedback are essential."}
{"id": "2508.16708", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.16708", "abs": "https://arxiv.org/abs/2508.16708", "authors": ["Shufeng Chen", "Halima El Badaoui", "Mariat James Elizebeth", "Takuya Nakashima", "Siddartha Khastgir", "Paul Jennings"], "title": "A Scalable Framework for the Management of STPA Requirements: a Case Study on eVTOL Operations", "comment": null, "summary": "System-Theoretic Process Analysis (STPA) is a recommended method for\nanalysing complex systems, capable of identifying thousands of safety\nrequirements often missed by traditional techniques such as Failure Mode and\nEffects Analysis (FMEA) and Fault Tree Analysis (FTA). However, the absence of\na structured framework for managing and prioritising these requirements\npresents challenges, particularly in fast-paced development environments. This\npaper introduces a scalable framework for prioritising STPA-derived\nrequirements. The framework integrates outputs from each STPA step and\nincorporates expert evaluations based on four key factors: implementation time,\ncost, requirement type, and regulatory coverage. To reduce subjectivity,\nMonte-Carlo Simulation (MCS) is employed to calculate and stabilise requirement\nrankings. An automation toolchain supports the framework, enabling dynamic\nmapping of prioritised requirements in a scaling matrix. This visualisation\naids decision-making and ensures traceability across development phases. The\nframework is applicable from early conceptualisation to more advanced stages,\nenhancing its utility in iterative system development. The framework was\nvalidated through a real-world case study focused on Electric Vertical Take-off\nand Landing (eVTOL) operations, conducted in collaboration with the UK Civil\nAviation Authority. The findings contributed directly to CAP3141, a Civil\nAviation Publication that identifies systemic operational risks and safety\nmitigations for regulators, operators, and vertiports. The prioritisation\nprocess supported decision-making by helping stakeholders identify and manage\nhigh-impact requirements efficiently. This work contributes a practical\nsolution for managing STPA outputs, bridging gaps in requirement prioritisation\nand supporting safety-critical development in emerging technologies."}
{"id": "2508.16625", "categories": ["cs.CR", "cs.AI", "cs.SE"], "pdf": "https://arxiv.org/pdf/2508.16625", "abs": "https://arxiv.org/abs/2508.16625", "authors": ["Rijha Safdar", "Danyail Mateen", "Syed Taha Ali", "M. Umer Ashfaq", "Wajahat Hussain"], "title": "Data and Context Matter: Towards Generalizing AI-based Software Vulnerability Detection", "comment": null, "summary": "The performance of AI-based software vulnerability detection systems is often\nlimited by their poor generalization to unknown codebases. In this research, we\nexplore the impact of data quality and model architecture on the\ngeneralizability of vulnerability detection systems. By generalization we mean\nability of high vulnerability detection performance across different C/C++\nsoftware projects not seen during training. Through a series of experiments, we\ndemonstrate that improvements in dataset diversity and quality substantially\nenhance detection performance. Additionally, we compare multiple encoder-only\nand decoder-only models, finding that encoder based models outperform in terms\nof accuracy and generalization. Our model achieves 6.8% improvement in recall\non the benchmark BigVul[1] dataset, also outperforming on unseen projects,\nhence showing enhanced generalizability. These results highlight the role of\ndata quality and model selection in the development of robust vulnerability\ndetection systems. Our findings suggest a direction for future systems having\nhigh cross-project effectiveness."}
{"id": "2508.16747", "categories": ["cs.AI", "cs.CY", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.16747", "abs": "https://arxiv.org/abs/2508.16747", "authors": ["Liu Liu", "Rui Dai"], "title": "Explainable AI for Predicting and Understanding Mathematics Achievement: A Cross-National Analysis of PISA 2018", "comment": null, "summary": "Understanding the factors that shape students' mathematics performance is\nvital for designing effective educational policies. This study applies\nexplainable artificial intelligence (XAI) techniques to PISA 2018 data to\npredict math achievement and identify key predictors across ten countries\n(67,329 students). We tested four models: Multiple Linear Regression (MLR),\nRandom Forest (RF), CATBoost, and Artificial Neural Networks (ANN), using\nstudent, family, and school variables. Models were trained on 70% of the data\n(with 5-fold cross-validation) and tested on 30%, stratified by country.\nPerformance was assessed with R^2 and Mean Absolute Error (MAE). To ensure\ninterpretability, we used feature importance, SHAP values, and decision tree\nvisualizations. Non-linear models, especially RF and ANN, outperformed MLR,\nwith RF balancing accuracy and generalizability. Key predictors included\nsocio-economic status, study time, teacher motivation, and students' attitudes\ntoward mathematics, though their impact varied across countries. Visual\ndiagnostics such as scatterplots of predicted vs actual scores showed RF and\nCATBoost aligned closely with actual performance. Findings highlight the\nnon-linear and context-dependent nature of achievement and the value of XAI in\neducational research. This study uncovers cross-national patterns, informs\nequity-focused reforms, and supports the development of personalized learning\nstrategies."}
{"id": "2508.16713", "categories": ["cs.SE", "cs.AI", "hep-ex"], "pdf": "https://arxiv.org/pdf/2508.16713", "abs": "https://arxiv.org/abs/2508.16713", "authors": ["Mohammad Atif", "Kriti Chopra", "Ozgur Kilic", "Tianle Wang", "Zhihua Dong", "Charles Leggett", "Meifeng Lin", "Paolo Calafiura", "Salman Habib"], "title": "CelloAI: Leveraging Large Language Models for HPC Software Development in High Energy Physics", "comment": "12 pages, 2 figures", "summary": "Next-generation High Energy Physics (HEP) experiments will generate\nunprecedented data volumes, necessitating High Performance Computing (HPC)\nintegration alongside traditional high-throughput computing. However, HPC\nadoption in HEP is hindered by the challenge of porting legacy software to\nheterogeneous architectures and the sparse documentation of these complex\nscientific codebases. We present CelloAI, a locally hosted coding assistant\nthat leverages Large Language Models (LLMs) with retrieval-augmented generation\n(RAG) to support HEP code documentation and generation. This local deployment\nensures data privacy, eliminates recurring costs and provides access to large\ncontext windows without external dependencies. CelloAI addresses two primary\nuse cases, code documentation and code generation, through specialized\ncomponents. For code documentation, the assistant provides: (a) Doxygen style\ncomment generation for all functions and classes by retrieving relevant\ninformation from RAG sources (papers, posters, presentations), (b) file-level\nsummary generation, and (c) an interactive chatbot for code comprehension\nqueries. For code generation, CelloAI employs syntax-aware chunking strategies\nthat preserve syntactic boundaries during embedding, improving retrieval\naccuracy in large codebases. The system integrates callgraph knowledge to\nmaintain dependency awareness during code modifications and provides\nAI-generated suggestions for performance optimization and accurate refactoring.\nWe evaluate CelloAI using real-world HEP applications from ATLAS, CMS, and DUNE\nexperiments, comparing different embedding models for code retrieval\neffectiveness. Our results demonstrate the AI assistant's capability to enhance\ncode understanding and support reliable code generation while maintaining the\ntransparency and safety requirements essential for scientific computing\nenvironments."}
{"id": "2508.16637", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2508.16637", "abs": "https://arxiv.org/abs/2508.16637", "authors": ["Abraham Itzhak Weinberg"], "title": "Passive Hack-Back Strategies for Cyber Attribution: Covert Vectors in Denied Environment", "comment": null, "summary": "Attributing cyberattacks remains a central challenge in modern cybersecurity,\nparticularly within denied environments where defenders have limited visibility\ninto attacker infrastructure and are restricted by legal or operational rules\nof engagement. This perspective examines the strategic value of passive\nhack-back techniques that enable covert attribution and intelligence collection\nwithout initiating direct offensive actions. Key vectors include tracking\nbeacons, honeytokens, environment-specific payloads, and supply-chain-based\ntraps embedded within exfiltrated or leaked assets. These approaches rely on\nthe assumption that attackers will interact with compromised data in traceable\nways, allowing defenders to gather signals without violating engagement\npolicies. The paper also explores the role of Artificial Intelligence (AI) in\nenhancing passive hack-back operations. Topics include the deployment of\nautonomous agents for forensic reconnaissance, the use of Large Language Models\n(LLMs) to generate dynamic payloads, and Adversarial Machine Learning (AML)\ntechniques for evasion and counter-deception. A dedicated section discusses the\nimplications of quantum technologies in this context, both as future threats to\ncryptographic telemetry and as potential tools for stealthy communication and\npost-quantum resilience. Finally, the paper advocates for hybrid defensive\nframeworks that combine passive attribution with delayed or conditional active\nresponses, while maintaining compliance with legal, ethical, and operational\nconstraints."}
{"id": "2508.16777", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.16777", "abs": "https://arxiv.org/abs/2508.16777", "authors": ["Mingyang Li", "Viktor Schlegel", "Tingting Mu", "Wuraola Oyewusi", "Kai Kang", "Goran Nenadic"], "title": "Evaluation and LLM-Guided Learning of ICD Coding Rationales", "comment": null, "summary": "Automated clinical coding involves mapping unstructured text from Electronic\nHealth Records (EHRs) to standardized code systems such as the International\nClassification of Diseases (ICD). While recent advances in deep learning have\nsignificantly improved the accuracy and efficiency of ICD coding, the lack of\nexplainability in these models remains a major limitation, undermining trust\nand transparency. Current explorations about explainability largely rely on\nattention-based techniques and qualitative assessments by physicians, yet lack\nsystematic evaluation using consistent criteria on high-quality rationale\ndatasets, as well as dedicated approaches explicitly trained to generate\nrationales for further enhancing explanation. In this work, we conduct a\ncomprehensive evaluation of the explainability of the rationales for ICD coding\nthrough two key lenses: faithfulness that evaluates how well explanations\nreflect the model's actual reasoning and plausibility that measures how\nconsistent the explanations are with human expert judgment. To facilitate the\nevaluation of plausibility, we construct a new rationale-annotated dataset,\noffering denser annotations with diverse granularity and aligns better with\ncurrent clinical practice, and conduct evaluation across three types of\nrationales of ICD coding. Encouraged by the promising plausibility of\nLLM-generated rationales for ICD coding, we further propose new rationale\nlearning methods to improve the quality of model-generated rationales, where\nrationales produced by prompting LLMs with/without annotation examples are used\nas distant supervision signals. We empirically find that LLM-generated\nrationales align most closely with those of human experts. Moreover,\nincorporating few-shot human-annotated examples not only further improves\nrationale generation but also enhances rationale-learning approaches."}
{"id": "2508.16771", "categories": ["cs.SE", "cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2508.16771", "abs": "https://arxiv.org/abs/2508.16771", "authors": ["Yifan Zhang", "Chen Huang", "Yueke Zhang", "Jiahao Zhang", "Toby Jia-Jun Li", "Collin McMillan", "Kevin Leach", "Yu Huang"], "title": "EyeMulator: Improving Code Language Models by Mimicking Human Visual Attention", "comment": null, "summary": "Code language models (so-called CodeLLMs) are now commonplace in software\ndevelopment. As a general rule, CodeLLMs are trained by dividing training\nexamples into input tokens and then learn importance of those tokens in a\nprocess called machine attention. Machine attention is based solely on input\ntoken salience to output token examples during training. Human software\ndevelopers are different, as humans intuitively know that some tokens are more\nsalient than others. While intuition itself is ineffable and a subject of\nphilosophy, clues about salience are present in human visual attention, since\npeople tend to look at more salient words more often. In this paper, we present\nEyeMulator, a technique for training CodeLLMs to mimic human visual attention\nwhile training for various software development tasks. We add special weights\nfor each token in each input example to the loss function used during LLM\nfine-tuning. We draw these weights from observations of human visual attention\nderived from a previously-collected publicly-available dataset of eye-tracking\nexperiments in software engineering tasks. These new weights ultimately induce\nchanges in the attention of the subject LLM during training, resulting in a\nmodel that does not need eye-tracking data during inference. Our evaluation\nshows that EyeMulator outperforms strong LLM baselines on several tasks such as\ncode translation, completion and summarization. We further show an ablation\nstudy that demonstrates the improvement is due to subject models learning to\nmimic human attention."}
{"id": "2508.16662", "categories": ["cs.CR", "cs.CY", "cs.NI", "cs.SE", "K.6.5; C.2.0; D.4.6"], "pdf": "https://arxiv.org/pdf/2508.16662", "abs": "https://arxiv.org/abs/2508.16662", "authors": ["Alexander Tabalipa"], "title": "Bridging the Mobile Trust Gap: A Zero Trust Framework for Consumer-Facing Applications", "comment": "43 pages, 5 figures, 9 tables. Working Paper - Version 1.0. Submitted\n  under a CC BY-SA 4.0 license. Also available as an SSRN Working Paper.\n  Feedback and collaboration are welcome", "summary": "Zero Trust Architecture (ZTA) has become a widely adopted model for securing\nenterprise environments, promoting continuous verification and minimal trust\nacross systems. However, its application in mobile contexts remains limited,\ndespite mobile applications now accounting for most global digital interactions\nand being increasingly targeted by sophisticated threats. Existing Zero Trust\nframeworks developed by organisations such as the National Institute of\nStandards and Technology (NIST) and the Cybersecurity and Infrastructure\nSecurity Agency (CISA) primarily focus on enterprise-managed infrastructure,\nassuming organisational control over devices, networks, and identities. This\npaper addresses a critical gap by proposing an extended Zero Trust model\ndesigned for mobile applications operating in untrusted, user-controlled\nenvironments. Using a design science methodology, the study introduced a\nsix-pillar framework that supports runtime enforcement of trust through\ncontrols including device integrity, user identity validation, data protection,\nsecure application programming interface (API) usage, behavioural monitoring,\nand live application protection. Each pillar was mapped to relevant regulatory\nand security standards to support compliance. A phased implementation roadmap\nand maturity assessment model were also developed to guide adoption across\nvarying organisational contexts. The proposed model offers a practical and\nstandards-aligned approach to securing mobile applications beyond\npre-deployment controls, aligning real-time enforcement with Zero Trust\nprinciples. This contribution expands the operational boundaries of ZTA and\nprovides organisations with a deployable path to reduce fraud, enhance\ncompliance, and address emerging mobile security challenges. Future research\nmay include empirical validation of the framework and cross-sector application\ntesting."}
{"id": "2508.16821", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.16821", "abs": "https://arxiv.org/abs/2508.16821", "authors": ["Sam Earle", "Graham Todd", "Yuchen Li", "Ahmed Khalifa", "Muhammad Umair Nasir", "Zehua Jiang", "Andrzej Banburski-Fahey", "Julian Togelius"], "title": "PuzzleJAX: A Benchmark for Reasoning and Learning", "comment": "25 pages, 11 figures, 2 tables", "summary": "We introduce PuzzleJAX, a GPU-accelerated puzzle game engine and description\nlanguage designed to support rapid benchmarking of tree search, reinforcement\nlearning, and LLM reasoning abilities. Unlike existing GPU-accelerated learning\nenvironments that provide hard-coded implementations of fixed sets of games,\nPuzzleJAX allows dynamic compilation of any game expressible in its\ndomain-specific language (DSL). This DSL follows PuzzleScript, which is a\npopular and accessible online game engine for designing puzzle games. In this\npaper, we validate in PuzzleJAX several hundred of the thousands of games\ndesigned in PuzzleScript by both professional designers and casual creators\nsince its release in 2013, thereby demonstrating PuzzleJAX's coverage of an\nexpansive, expressive, and human-relevant space of tasks. By analyzing the\nperformance of search, learning, and language models on these games, we show\nthat PuzzleJAX can naturally express tasks that are both simple and intuitive\nto understand, yet often deeply challenging to master, requiring a combination\nof control, planning, and high-level insight."}
{"id": "2508.16853", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.16853", "abs": "https://arxiv.org/abs/2508.16853", "authors": ["Pratyush Nidhi Sharma", "Lauren Wright", "Anne Herfurth", "Munsif Sokiyna", "Pratyaksh Nidhi Sharma", "Sethu Das", "Mikko Siponen"], "title": "DevLicOps: A Framework for Mitigating Licensing Risks in AI-Generated Code", "comment": "18 pages, 1 figure, 2 Tables", "summary": "Generative AI coding assistants (ACAs) are widely adopted yet pose serious\nlegal and compliance risks. ACAs can generate code governed by restrictive\nopen-source licenses (e.g., GPL), potentially exposing companies to litigation\nor forced open-sourcing. Few developers are trained in these risks, and legal\nstandards vary globally, especially with outsourcing. Our article introduces\nDevLicOps, a practical framework that helps IT leaders manage ACA-related\nlicensing risks through governance, incident response, and informed tradeoffs.\nAs ACA adoption grows and legal frameworks evolve, proactive license compliance\nis essential for responsible, risk-aware software development in the AI era."}
{"id": "2508.16761", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2508.16761", "abs": "https://arxiv.org/abs/2508.16761", "authors": ["Nesrine Benchoubane", "Olfa Ben Yahia", "William Ferguson", "Gurkan Gur", "Sumit Chakravarty", "Gregory Falco", "Gunes Karabulut Kurt"], "title": "Securing Heterogeneous Network (HetNet) Communications for Wildfire Management: Mitigating the Effects of Adversarial and Environmental Threats", "comment": null, "summary": "In the face of adverse environmental conditions and cyber threats, robust\ncommunication systems for critical applications such as wildfire management and\ndetection demand secure and resilient architectures. This paper presents a\nnovel framework that considers both adversarial factors, building resilience\ninto a heterogeneous network (HetNet) integrating Low Earth Orbit (LEO)\nsatellite constellation with High-Altitude Platform Ground Stations (HAPGS) and\nLow-Altitude Platforms (LAPS), tailored to support wildfire management\noperations. Building upon our previous work on secure-by-component approach for\nlink segment security, we extend protection to the communication layer by\nsecuring both Radio Frequency (RF)/Free Space Optics (FSO) management and\ndifferent links. Through a case study, we quantify how environmental stressors\nimpact secrecy capacity and expose the system to passive adversaries. Key\nfindings demonstrate that atmospheric attenuation and beam misalignment can\nnotably degrade secrecy capacity across both short- and long-range\ncommunication links, while high-altitude eavesdroppers face less signal\ndegradation, increasing their interception capability. Moreover, increasing\ntransmit power to counter environmental losses can inadvertently improve\neavesdropper reception, thereby reducing overall link confidentiality. Our work\nnot only highlights the importance of protecting networks from these dual\nthreats but also aligns with the IEEE P3536 Standard for Space System\nCybersecurity Design, ensuring resilience and the prevention of mission\nfailures."}
{"id": "2508.16839", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.16839", "abs": "https://arxiv.org/abs/2508.16839", "authors": ["Shayan Vassef", "Soorya Ram Shimegekar", "Abhay Goyal", "Koustuv Saha", "Pi Zonooz", "Navin Kumar"], "title": "Route-and-Execute: Auditable Model-Card Matching and Specialty-Level Deployment", "comment": null, "summary": "Clinical workflows are fragmented as a patchwork of scripts and task-specific\nnetworks that often handle triage, task selection, and model deployment. These\npipelines are rarely streamlined for data science pipeline, reducing efficiency\nand raising operational costs. Workflows also lack data-driven model\nidentification (from imaging/tabular inputs) and standardized delivery of model\noutputs. In response, we present a practical, healthcare-first framework that\nuses a single vision-language model (VLM) in two complementary roles. First\n(Solution 1), the VLM acts as an aware model-card matcher that routes an\nincoming image to the appropriate specialist model via a three-stage workflow\n(modality -> primary abnormality -> model-card id). Checks are provided by (i)\nstagewise prompts that allow early exit via None/Normal/Other and (ii) a\nstagewise answer selector that arbitrates between the top-2 candidates at each\nstage, reducing the chance of an incorrect selection and aligning the workflow\nwith clinical risk tolerance. Second (Solution 2), we fine-tune the VLM on\nspecialty-specific datasets ensuring a single model covers multiple downstream\ntasks within each specialty, maintaining performance while simplifying\ndeployment. Across gastroenterology, hematology, ophthalmology, and pathology,\nour single-model deployment matches or approaches specialized baselines.\n  Compared with pipelines composed of many task-specific agents, this approach\nshows that one VLM can both decide and do. It may reduce effort by data\nscientists, shorten monitoring, increase the transparency of model selection\n(with per-stage justifications), and lower integration overhead."}
{"id": "2508.16860", "categories": ["cs.SE", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.16860", "abs": "https://arxiv.org/abs/2508.16860", "authors": ["Md Afif Al Mamun", "Gias Uddin", "Lan Xia", "Longyu Zhang"], "title": "TriagerX: Dual Transformers for Bug Triaging Tasks with Content and Interaction Based Rankings", "comment": "This work is currently under review at IEEE Transactions on Software\n  Engineering. The replication package will be made publicly available upon\n  acceptance", "summary": "Pretrained Language Models or PLMs are transformer-based architectures that\ncan be used in bug triaging tasks. PLMs can better capture token semantics than\ntraditional Machine Learning (ML) models that rely on statistical features\n(e.g., TF-IDF, bag of words). However, PLMs may still attend to less relevant\ntokens in a bug report, which can impact their effectiveness. In addition, the\nmodel can be sub-optimal with its recommendations when the interaction history\nof developers around similar bugs is not taken into account. We designed\nTriagerX to address these limitations. First, to assess token semantics more\nreliably, we leverage a dual-transformer architecture. Unlike current\nstate-of-the-art (SOTA) baselines that employ a single transformer\narchitecture, TriagerX collects recommendations from two transformers with each\noffering recommendations via its last three layers. This setup generates a\nrobust content-based ranking of candidate developers. TriagerX then refines\nthis ranking by employing a novel interaction-based ranking methodology, which\nconsiders developers' historical interactions with similar fixed bugs. Across\nfive datasets, TriagerX surpasses all nine transformer-based methods, including\nSOTA baselines, often improving Top-1 and Top-3 developer recommendation\naccuracy by over 10%. We worked with our large industry partner to successfully\ndeploy TriagerX in their development environment. The partner required both\ndeveloper and component recommendations, with components acting as proxies for\nteam assignments-particularly useful in cases of developer turnover or team\nchanges. We trained TriagerX on the partner's dataset for both tasks, and it\noutperformed SOTA baselines by up to 10% for component recommendations and 54%\nfor developer recommendations."}
{"id": "2508.16765", "categories": ["cs.CR", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2508.16765", "abs": "https://arxiv.org/abs/2508.16765", "authors": ["GodsGift Uzor", "Hasan Al-Qudah", "Ynes Ineza", "Abdul Serwadda"], "title": "Guarding Your Conversations: Privacy Gatekeepers for Secure Interactions with Cloud-Based AI Models", "comment": "2025 19th International Conference on Semantic Computing (ICSC)", "summary": "The interactive nature of Large Language Models (LLMs), which closely track\nuser data and context, has prompted users to share personal and private\ninformation in unprecedented ways. Even when users opt out of allowing their\ndata to be used for training, these privacy settings offer limited protection\nwhen LLM providers operate in jurisdictions with weak privacy laws, invasive\ngovernment surveillance, or poor data security practices. In such cases, the\nrisk of sensitive information, including Personally Identifiable Information\n(PII), being mishandled or exposed remains high. To address this, we propose\nthe concept of an \"LLM gatekeeper\", a lightweight, locally run model that\nfilters out sensitive information from user queries before they are sent to the\npotentially untrustworthy, though highly capable, cloud-based LLM. Through\nexperiments with human subjects, we demonstrate that this dual-model approach\nintroduces minimal overhead while significantly enhancing user privacy, without\ncompromising the quality of LLM responses."}
{"id": "2508.16846", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2508.16846", "abs": "https://arxiv.org/abs/2508.16846", "authors": ["Katherine Atwell", "Pedram Heydari", "Anthony Sicilia", "Malihe Alikhani"], "title": "Quantifying Sycophancy as Deviations from Bayesian Rationality in LLMs", "comment": null, "summary": "Sycophancy, or overly agreeable or flattering behavior, is a documented issue\nin large language models (LLMs), and is critical to understand in the context\nof human/AI collaboration. Prior works typically quantify sycophancy by\nmeasuring shifts in behavior or impacts on accuracy, but neither metric\ncharacterizes shifts in rationality, and accuracy measures can only be used in\nscenarios with a known ground truth. In this work, we utilize a Bayesian\nframework to quantify sycophancy as deviations from rational behavior when\npresented with user perspectives, thus distinguishing between rational and\nirrational updates based on the introduction of user perspectives. In\ncomparison to other methods, this approach allows us to characterize excessive\nbehavioral shifts, even for tasks that involve inherent uncertainty or do not\nhave a ground truth. We study sycophancy for 3 different tasks, a combination\nof open-source and closed LLMs, and two different methods for probing\nsycophancy. We also experiment with multiple methods for eliciting probability\njudgments from LLMs. We hypothesize that probing LLMs for sycophancy will cause\ndeviations in LLMs' predicted posteriors that will lead to increased Bayesian\nerror. Our findings indicate that: 1) LLMs are not Bayesian rational, 2)\nprobing for sycophancy results in significant increases to the predicted\nposterior in favor of the steered outcome, 3) sycophancy sometimes results in\nincreased Bayesian error, and in a small number of cases actually decreases\nerror, and 4) changes in Bayesian error due to sycophancy are not strongly\ncorrelated in Brier score, suggesting that studying the impact of sycophancy on\nground truth alone does not fully capture errors in reasoning due to\nsycophancy."}
{"id": "2508.16903", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.16903", "abs": "https://arxiv.org/abs/2508.16903", "authors": ["Yijun Lu", "Hironori Washizaki", "Naoyasu Ubayashi", "Nobukazu Yoshioka", "Chenhao Wu", "Masanari Kondo", "Yuyin Ma", "Jiong Dong", "Jianjin Zhao", "Dongqi Han"], "title": "Mind the Gap: A Decade-Scale Empirical Study of Multi-Stakeholder Dynamics in VR Ecosystem", "comment": null, "summary": "In the development and evolution of VR ecosystem, platform stakeholders\ncontinuously adapt their products in response to user and technical feedback,\noften reflected in subtle shifts in discussion topics or system updates. A\ncomprehensive understanding of these changes is essential for identifying gaps\nbetween user expectations and developer actions, which can guide more effective\nquality assurance and user-centered innovation. While previous studies have\nanalyzed either user reviews or developer discussions in isolation, such\napproaches typically fail to reveal how specific user concerns are (or are not)\naddressed by corresponding technical activities. To address this limitation,\nour study introduces a multi-view empirical framework that systematically\ncompares and aligns stakeholder perspectives. By applying topic modeling and\nquantitative impact analysis to 944,320 user reviews and 389,477 developer\nposts, we identify not only the overlap in concerns (e.g., performance, input\nmethods), but also clear gaps in areas like inclusivity and community safety\n(e.g., LGBTQ+ representation, child-friendly content). Our findings show that\nwhile users repeatedly raise such issues, they are rarely discussed in\ndeveloper forums. These insights enable data-driven recommendations for closing\nthe user-developer gap in VR ecosystems, offering practical implications for\nplatform governance and the design of next-generation VR systems."}
{"id": "2508.16843", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.16843", "abs": "https://arxiv.org/abs/2508.16843", "authors": ["Kamel Kamel", "Keshav Sood", "Hridoy Sankar Dutta", "Sunil Aryal"], "title": "A Survey of Threats Against Voice Authentication and Anti-Spoofing Systems", "comment": "This paper will be submitted to the Computer Science Review", "summary": "Voice authentication has undergone significant changes from traditional\nsystems that relied on handcrafted acoustic features to deep learning models\nthat can extract robust speaker embeddings. This advancement has expanded its\napplications across finance, smart devices, law enforcement, and beyond.\nHowever, as adoption has grown, so have the threats. This survey presents a\ncomprehensive review of the modern threat landscape targeting Voice\nAuthentication Systems (VAS) and Anti-Spoofing Countermeasures (CMs), including\ndata poisoning, adversarial, deepfake, and adversarial spoofing attacks. We\nchronologically trace the development of voice authentication and examine how\nvulnerabilities have evolved in tandem with technological advancements. For\neach category of attack, we summarize methodologies, highlight commonly used\ndatasets, compare performance and limitations, and organize existing literature\nusing widely accepted taxonomies. By highlighting emerging risks and open\nchallenges, this survey aims to support the development of more secure and\nresilient voice authentication systems."}
{"id": "2508.16850", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.16850", "abs": "https://arxiv.org/abs/2508.16850", "authors": ["Anku Rani", "Aparna Garimella", "Apoorv Saxena", "Balaji Vasan Srinivasan", "Paul Pu Liang"], "title": "RADAR: A Reasoning-Guided Attribution Framework for Explainable Visual Data Analysis", "comment": null, "summary": "Data visualizations like charts are fundamental tools for quantitative\nanalysis and decision-making across fields, requiring accurate interpretation\nand mathematical reasoning. The emergence of Multimodal Large Language Models\n(MLLMs) offers promising capabilities for automated visual data analysis, such\nas processing charts, answering questions, and generating summaries. However,\nthey provide no visibility into which parts of the visual data informed their\nconclusions; this black-box nature poses significant challenges to real-world\ntrust and adoption. In this paper, we take the first major step towards\nevaluating and enhancing the capabilities of MLLMs to attribute their reasoning\nprocess by highlighting the specific regions in charts and graphs that justify\nmodel answers. To this end, we contribute RADAR, a semi-automatic approach to\nobtain a benchmark dataset comprising 17,819 diverse samples with charts,\nquestions, reasoning steps, and attribution annotations. We also introduce a\nmethod that provides attribution for chart-based mathematical reasoning.\nExperimental results demonstrate that our reasoning-guided approach improves\nattribution accuracy by 15% compared to baseline methods, and enhanced\nattribution capabilities translate to stronger answer generation, achieving an\naverage BERTScore of $\\sim$ 0.90, indicating high alignment with ground truth\nresponses. This advancement represents a significant step toward more\ninterpretable and trustworthy chart analysis systems, enabling users to verify\nand understand model decisions through reasoning and attribution."}
{"id": "2508.17161", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.17161", "abs": "https://arxiv.org/abs/2508.17161", "authors": ["Julyanara R. Silva", "Carlos Eduardo C. Dantas", "Marcelo A. Maia"], "title": "What Developers Ask to ChatGPT in GitHub Pull Requests? an Exploratory Study", "comment": "12 pages, 3 figures", "summary": "The emergence of Large Language Models (LLMs), such as ChatGPT, has\nintroduced a new set of tools to support software developers in solving pro-\ngramming tasks. However, our understanding of the interactions (i.e., prompts)\nbetween developers and ChatGPT that result in contributions to the codebase\nremains limited. To explore this limitation, we conducted a manual evaluation\nof 155 valid ChatGPT share links extracted from 139 merged Pull Requests (PRs),\nrevealing the interactions between developers and reviewers with ChatGPT that\nled to merges into the main codebase. Our results produced a catalog of 14\ntypes of ChatGPT requests categorized into four main groups. We found a\nsignificant number of requests involving code review and the implementation of\ncode snippets based on specific tasks. Developers also sought to clarify doubts\nby requesting technical explanations or by asking for text refinements for\ntheir web pages. Furthermore, we verified that prompts involving code\ngeneration generally required more interactions to produce the desired answer\ncompared to prompts requesting text review or technical information."}
{"id": "2508.16868", "categories": ["cs.CR", "cs.AR"], "pdf": "https://arxiv.org/pdf/2508.16868", "abs": "https://arxiv.org/abs/2508.16868", "authors": ["Joshua Mashburn", "Johann Knechtel", "Florian Klemme", "Hussam Amrouch", "Ozgur Sinanoglu", "Paul V. Gratz"], "title": "Targeted Wearout Attacks in Microprocessor Cores", "comment": "13 pages, 11 figures, submitted to IEEE International Symposium on\n  High-Performance Computer Architecture 2026 (HPCA-32)", "summary": "Negative-Bias Temperature Instability is a dominant aging mechanism in\nnanoscale CMOS circuits such as microprocessors. With this aging mechanism, the\nrate of device aging is dependent not only on overall operating conditions,\nsuch as heat, but also on user controllable inputs to the transistors. This\ndependence on input implies a possible timing fault-injection attack wherein a\ntargeted path of logic is intentionally degraded through the purposeful,\nsoftware-driven actions of an attacker, rendering a targeted bit effectively\nstuck.\n  In this work, we describe such an attack mechanism, which we dub a\n\"$\\textbf{Targeted Wearout Attack}$\", wherein an attacker with sufficient\nknowledge of the processor core, executing a carefully crafted software program\nwith only user privilege, is able to degrade a functional unit within the\nprocessor with the aim of eliciting a particular desired incorrect calculation\nin a victim application. Here we give a general methodology for the attack. We\nthen demonstrate a case study where a targeted path within the fused\nmultiply-add pipeline in a RISC-V CPU sees a $>7x$ increase in wear over time\nthan would be experienced under typical workloads. We show that an attacker\ncould leverage such an attack, leading to targeted and silent data corruption\nin a co-running victim application using the same unit."}
{"id": "2508.16986", "categories": ["cs.AI", "math.LO"], "pdf": "https://arxiv.org/pdf/2508.16986", "abs": "https://arxiv.org/abs/2508.16986", "authors": ["Uri Andrews", "Luca San Mauro"], "title": "Complexity in finitary argumentation (extended version)", "comment": null, "summary": "Abstract argumentation frameworks (AFs) provide a formal setting to analyze\nmany forms of reasoning with conflicting information. While the expressiveness\nof general infinite AFs make them a tempting tool for modeling many kinds of\nreasoning scenarios, the computational intractability of solving infinite AFs\nlimit their use, even in many theoretical applications.\n  We investigate the complexity of computational problems related to infinite\nbut finitary argumentations frameworks, that is, infinite AFs where each\nargument is attacked by only finitely many others. Our results reveal a\nsurprising scenario. On one hand, we see that the assumption of being finitary\ndoes not automatically guarantee a drop in complexity. However, for the\nadmissibility-based semantics, we find a remarkable combinatorial constraint\nwhich entails a dramatic decrease in complexity.\n  We conclude that for many forms of reasoning, the finitary infinite AFs\nprovide a natural setting for reasoning which balances well the competing goals\nof being expressive enough to be applied to many reasoning settings while being\ncomputationally tractable enough for the analysis within the framework to be\nuseful."}
{"id": "2508.17343", "categories": ["cs.SE", "cs.AI", "D.2"], "pdf": "https://arxiv.org/pdf/2508.17343", "abs": "https://arxiv.org/abs/2508.17343", "authors": ["Abhik Roychoudhury"], "title": "Agentic AI for Software: thoughts from Software Engineering community", "comment": "4 pages", "summary": "AI agents have recently shown significant promise in software engineering.\nMuch public attention has been transfixed on the topic of code generation from\nLarge Language Models (LLMs) via a prompt. However, software engineering is\nmuch more than programming, and AI agents go far beyond instructions given by a\nprompt.\n  At the code level, common software tasks include code generation, testing,\nand program repair. Design level software tasks may include architecture\nexploration, requirements understanding, and requirements enforcement at the\ncode level. Each of these software tasks involves micro-decisions which can be\ntaken autonomously by an AI agent, aided by program analysis tools. This\ncreates the vision of an AI software engineer, where the AI agent can be seen\nas a member of a development team.\n  Conceptually, the key to successfully developing trustworthy agentic AI-based\nsoftware workflows will be to resolve the core difficulty in software\nengineering - the deciphering and clarification of developer intent.\nSpecification inference, or deciphering the intent, thus lies at the heart of\nmany software tasks, including software maintenance and program repair. A\nsuccessful deployment of agentic technology into software engineering would\ninvolve making conceptual progress in such intent inference via agents.\n  Trusting the AI agent becomes a key aspect, as software engineering becomes\nmore automated. Higher automation also leads to higher volume of code being\nautomatically generated, and then integrated into code-bases. Thus to deal with\nthis explosion, an emerging direction is AI-based verification and validation\n(V & V) of AI generated code. We posit that agentic software workflows in\nfuture will include such AIbased V&V."}
{"id": "2508.16941", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2508.16941", "abs": "https://arxiv.org/abs/2508.16941", "authors": ["Yu Cheng", "Xiaofang Qi", "Yanhui Li"], "title": "Investigating red packet fraud in Android applications: Insights from user reviews", "comment": "This manuscript has been accepted for publication in Cybersecurity\n  (Springer Nature). The final version of record will be published by Springer\n  Nature and will be accessible online. This version is the Author Accepted\n  Manuscript (AAM) and may differ from the final published version", "summary": "With the popularization of smartphones, red packets have been widely used in\nmobile apps. However, the issues of fraud associated with them have also become\nincreasingly prominent. As reported in user reviews from mobile app markets,\nmany users have complained about experiencing red packet fraud and being\npersistently troubled by fraudulent red packets. To uncover this phenomenon, we\nconduct the first investigation into an extensive collection of user reviews on\napps with red packets. In this paper, we first propose a novel automated\napproach, ReckDetector, for effectively identifying apps with red packets from\napp markets. We then collect over 360,000 real user reviews from 334 apps with\nred packets available on Google Play and three popular alternative Android app\nmarkets. We preprocess the user reviews to extract those related to red packets\nand fine-tune a pre-trained BERT model to identify negative reviews. Finally,\nbased on semantic analysis, we have summarized six distinct categories of red\npacket fraud issues reported by users. Through our study, we found that red\npacket fraud is highly prevalent, significantly impacting user experience and\ndamaging the reputation of apps. Moreover, red packets have been widely\nexploited by unscrupulous app developers as a deceptive incentive mechanism to\nentice users into completing their designated tasks, thereby maximizing their\nprofits."}
{"id": "2508.16987", "categories": ["cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.16987", "abs": "https://arxiv.org/abs/2508.16987", "authors": ["Tanvir Bhathal", "Asanshay Gupta"], "title": "WebSight: A Vision-First Architecture for Robust Web Agents", "comment": null, "summary": "We introduce WebSight, a vision-based autonomous web agent, designed to\ninteract with web environments purely through visual perception, eliminating\ndependence on HTML or DOM-based inputs. Central to our approach we introduce\nour new model, WebSight-7B, a fine-tuned vision-language model optimized for UI\nelement interaction, trained using LoRA on a web-focused subset of the\nWave-UI-25K dataset. WebSight integrates this model into a modular multi-agent\narchitecture, comprising planning, reasoning, vision-action, and verification\nagents, coordinated through an episodic memory mechanism.\n  WebSight-7B achieves a top-1 accuracy of 58.84% on the Showdown Clicks\nbenchmark, outperforming several larger generalist models while maintaining\nlower latency. The full WebSight agent achieves a 68.0% success rate on the\nWebVoyager benchmark, surpassing systems from labs such as OpenAI (61.0%) and\nHCompany (Runner H, 67.0%). Among tasks completed, WebSight answers correctly\n97.14% of the time, indicating high precision. Together, WebSight and\nWebSight-7B establish a new standard for interpretable, robust, and efficient\nvisual web navigation."}
{"id": "2508.17344", "categories": ["cs.SE", "cs.LG", "cs.PF", "cs.PL"], "pdf": "https://arxiv.org/pdf/2508.17344", "abs": "https://arxiv.org/abs/2508.17344", "authors": ["Rajrupa Chattaraj", "Sridhar Chimalakonda", "Vibhu Saujanya Sharma", "Vikrant Kaulgud"], "title": "Who Wins the Race? (R Vs Python) - An Exploratory Study on Energy Consumption of Machine Learning Algorithms", "comment": "18 pages including references, 5 figures", "summary": "The utilization of Machine Learning (ML) in contemporary software systems is\nextensive and continually expanding. However, its usage is energy-intensive,\ncontributing to increased carbon emissions and demanding significant resources.\nWhile numerous studies examine the performance and accuracy of ML, only a\nlimited few focus on its environmental aspects, particularly energy\nconsumption. In addition, despite emerging efforts to compare energy\nconsumption across various programming languages for specific algorithms and\ntasks, there remains a gap specifically in comparing these languages for\nML-based tasks. This paper aims to raise awareness of the energy costs\nassociated with employing different programming languages for ML model training\nand inference. Through this empirical study, we measure and compare the energy\nconsumption along with run-time performance of five regression and five\nclassification tasks implemented in Python and R, the two most popular\nprogramming languages in this context. Our study results reveal a statistically\nsignificant difference in costs between the two languages in 95% of the cases\nexamined. Furthermore, our analysis demonstrates that the choice of programming\nlanguage can influence energy efficiency significantly, up to 99.16% during\nmodel training and up to 99.8% during inferences, for a given ML task."}
{"id": "2508.16991", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2508.16991", "abs": "https://arxiv.org/abs/2508.16991", "authors": ["Ekzhin Ear"], "title": "Towards Principled Analysis and Mitigation of Space Cyber Risks", "comment": "PhD Dissertation", "summary": "Space infrastructures have become an underpinning of modern society, but\ntheir associated cyber risks are little understood. This Dissertation advances\nthe state-of-the-art via four contributions. (i) It introduces an innovative\nframework for characterizing real-world cyber attacks against space\ninfrastructures, or space cyber attacks, including a novel methodology for\ncoping with missing data and three novel metrics. A case study demonstrates the\nusefulness of the framework on 108 real-world space cyber attacks. (ii) This\nDissertation characterizes the state-of-the-practice in space cyber risk\nanalysis and mitigation, namely the Notional Risk Scores (NRS) within the Space\nAttack Research and Tactic Analysis (SPARTA) framework. (iii) We propose a set\nof desired properties that should be satisfied by any competent space cyber\nrisk analysis and mitigation tool and applies them to assess two industrial\nspace cyber risk analysis and mitigation tools. (iv) The study introduces a\nnovel framework to analyze and mitigate space cyber risks by explicitly\nmodeling space cyber attack cascading effects and presenting algorithms for\nmission risk analysis and mission hardening. We demonstrate the usefulness of\nthe framework by applying it to analyze and mitigate space cyber risks, with\ntestbed-based validation."}
{"id": "2508.17087", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.17087", "abs": "https://arxiv.org/abs/2508.17087", "authors": ["Wen Wang", "Xiangchen Wu", "Liang Wang", "Hao Hu", "Xianping Tao", "Linghao Zhang"], "title": "Solving the Min-Max Multiple Traveling Salesmen Problem via Learning-Based Path Generation and Optimal Splitting", "comment": null, "summary": "This study addresses the Min-Max Multiple Traveling Salesmen Problem\n($m^3$-TSP), which aims to coordinate tours for multiple salesmen such that the\nlength of the longest tour is minimized. Due to its NP-hard nature, exact\nsolvers become impractical under the assumption that $P \\ne NP$. As a result,\nlearning-based approaches have gained traction for their ability to rapidly\ngenerate high-quality approximate solutions. Among these, two-stage methods\ncombine learning-based components with classical solvers, simplifying the\nlearning objective. However, this decoupling often disrupts consistent\noptimization, potentially degrading solution quality. To address this issue, we\npropose a novel two-stage framework named \\textbf{Generate-and-Split} (GaS),\nwhich integrates reinforcement learning (RL) with an optimal splitting\nalgorithm in a joint training process. The splitting algorithm offers\nnear-linear scalability with respect to the number of cities and guarantees\noptimal splitting in Euclidean space for any given path. To facilitate the\njoint optimization of the RL component with the algorithm, we adopt an\nLSTM-enhanced model architecture to address partial observability. Extensive\nexperiments show that the proposed GaS framework significantly outperforms\nexisting learning-based approaches in both solution quality and\ntransferability."}
{"id": "2508.17713", "categories": ["cs.SE", "cs.AR"], "pdf": "https://arxiv.org/pdf/2508.17713", "abs": "https://arxiv.org/abs/2508.17713", "authors": ["Zhihao Xu", "Shikai Guo", "Guilin Zhao", "Peiyu Zou", "Siwen Wang", "Qian Ma", "Hui Li", "Furui Zhan"], "title": "Code Difference Guided Fuzzing for FPGA Logic Synthesis Compilers via Bayesian Optimization", "comment": null, "summary": "Field Programmable Gate Arrays (FPGAs) play a crucial role in Electronic\nDesign Automation (EDA) applications, which have been widely used in\nsafety-critical environments, including aerospace, chip manufacturing, and\nmedical devices. A critical step in FPGA development is logic synthesis, which\nenables developers to translate their software designs into hardware net lists,\nwhich facilitates the physical implementation of the chip, detailed timing and\npower analysis, gate-level simulation, test vector generation, and optimization\nand consistency checking. However, bugs or incorrect implementations in FPGA\nlogic synthesis compilers may lead to unexpected behaviors in target\nwapplications, posing security risks. Therefore, it is crucial to eliminate\nsuch bugs in FPGA logic synthesis compilers. The effectiveness of existing\nworks is still limited by its simple, blind mutation strategy. To address this\nchallenge, we propose a guided mutation strategy based on Bayesian optimization\ncalled LSC-Fuzz to detect bugs in FPGA logic synthesis compilers. Specifically,\nLSC-Fuzz consists of three components: the test-program generation component,\nthe Bayesian diversity selection component, and the equivalent check component.\nBy performing test-program generation and Bayesian diversity selection,\nLSC-Fuzz generates diverse and complex HDL code, thoroughly testing the FPGA\nlogic synthesis compilers using equivalent check to detect bugs. Through three\nmonths, LSC-Fuzz has found 16 bugs, 12 of these has been confirmed by official\ntechnical support."}
{"id": "2508.17043", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2508.17043", "abs": "https://arxiv.org/abs/2508.17043", "authors": ["Shayesta Naziri", "Xu Wang", "Guangsheng Yu", "Christy Jie Liang", "Wei Ni"], "title": "ZAPS: A Zero-Knowledge Proof Protocol for Secure UAV Authentication with Flight Path Privacy", "comment": "11 Pages, 8 figures, Journal", "summary": "The increasing deployment of Unmanned Aerial Vehicles (UAVs) for military,\ncommercial, and logistics applications has raised significant concerns\nregarding flight path privacy. Conventional UAV communication systems often\nexpose flight path data to third parties, making them vulnerable to tracking,\nsurveillance, and location inference attacks. Existing encryption techniques\nprovide security but fail to ensure complete privacy, as adversaries can still\ninfer movement patterns through metadata analysis. To address these challenges,\nwe propose a zk-SNARK(Zero-Knowledge Succinct Non-Interactive Argument of\nKnowledge)-based privacy-preserving flight path authentication and verification\nframework. Our approach ensures that a UAV can prove its authorisation,\nvalidate its flight path with a control centre, and comply with regulatory\nconstraints without revealing any sensitive trajectory information. By\nleveraging zk-SNARKs, the UAV can generate cryptographic proofs that verify\ncompliance with predefined flight policies while keeping the exact path and\nlocation undisclosed. This method mitigates risks associated with real-time\ntracking, identity exposure, and unauthorised interception, thereby enhancing\nUAV operational security in adversarial environments. Our proposed solution\nbalances privacy, security, and computational efficiency, making it suitable\nfor resource-constrained UAVs in both civilian and military applications."}
{"id": "2508.17094", "categories": ["cs.AI", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2508.17094", "abs": "https://arxiv.org/abs/2508.17094", "authors": ["Emmanuel O. Badmus", "Peng Sang", "Dimitrios Stamoulis", "Amritanshu Pandey"], "title": "PowerChain: Automating Distribution Grid Analysis with Agentic AI Workflows", "comment": null, "summary": "Due to the rapid pace of electrification and decarbonization, distribution\ngrid (DG) operation and planning are becoming more complex, necessitating\nadvanced computational analyses to ensure grid reliability and resilience.\nState-of-the-art DG analyses rely on disparate workflows of complex models,\nfunctions, and data pipelines, which require expert knowledge and are\nchallenging to automate. Many small-scale utilities and cooperatives lack a\nlarge R&D workforce and therefore cannot use advanced analysis at scale. To\naddress this gap, we develop a novel agentic AI system, PowerChain, to solve\nunseen DG analysis tasks via automated agentic orchestration and large language\nmodels (LLMs) function-calling. Given a natural language query, PowerChain\ndynamically generates and executes an ordered sequence of domain-aware\nfunctions guided by the semantics of an expert-built power systems function\npool and a select reference set of known, expert-generated workflow-query\npairs. Our results show that PowerChain can produce expert-level workflows with\nboth GPT-5 and open-source Qwen models on complex, unseen DG analysis tasks\noperating on real utility data."}
{"id": "2508.17719", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.17719", "abs": "https://arxiv.org/abs/2508.17719", "authors": ["Akhila Sri Manasa Venigalla", "Sridhar Chimalakonda"], "title": "DocFetch - Towards Generating Software Documentation from Multiple Software Artifacts", "comment": "12 pages, 7 Figures, 4 Tables", "summary": "Software Documentation plays a major role in the usage and development of a\nproject. Widespread adoption of open source software projects contributes to\nlarger and faster development of the projects, making it difficult to maintain\nthe associated documentation. Existing automated approaches to generate\ndocumentation largely focus on source code. However, information useful for\ndocumentation is observed to be scattered across various artifacts that\nco-evolve with the source code. Leveraging this information across multiple\nartifacts can reduce the effort involved in maintaining documentation. Hence,\nwe propose DocFetch, to generate different types of documentation from multiple\nsoftware artifacts. We employ a multi-layer prompt based LLM and generate\nstructured documentation corresponding to different documentation types for the\ndata consolidated in DocMine dataset. We evaluate the performance of DocFetch\nusing a manually curated groundtruth dataset by analysing the artifacts in\nDocMine. The evaluation yields a highest BLEU-4 score of 43.24% and ROUGE-L\nscore of 0.39 for generation of api-related and file-related information from\nfive documentation sources. The generation of other documentation type related\ninformation also reported BLEU-4 scores close to 30% indicating good\nperformance of the approach. Thus,DocFetch can be employed to\nsemi-automatically generate documentation, and helps in comprehending the\nprojects with minimal effort in maintaining the documentation."}
{"id": "2508.17071", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2508.17071", "abs": "https://arxiv.org/abs/2508.17071", "authors": ["Sufyan Al-Janabi"], "title": "Post-Quantum Blockchain: Challenges and Opportunities", "comment": null, "summary": "Blockchain is a Distributed Ledger Technology (DLT) that offers numerous\nbenefits including decentralization, transparency, efficiency, and reduced\ncosts. Hence, blockchain has been included in many fields. Blockchain relies on\ncryptographic protocols (especially public-key cryptography and hash functions)\nto achieve many essential sub-routines. However, the increased progress of\nquantum computation and algorithms has threatened the security of many\ntraditional cryptosystems. Therefore, this represents a serious risk for the\nexisting blockchain technology. For example, SHA-256 and the Elliptic Curve\nDigital Signature Algorithm (ECDSA) cryptosystems can be compromised by Shor s\nand Grover s quantum algorithms in the foreseeable future. Post-Quantum\nCryptography (PQC) is a basic solution for resisting these quantum attacks.\nApplying PQC to blockchains results in creating Post-Quantum Blockchains (PQB).\nThus, this paper aims to review the threats imposed by quantum computers on\nclassical blockchain technology and provide useful guidelines on PQB security\nto blockchain researchers. The paper focuses on the challenges and\nopportunities of future work direction in this field."}
{"id": "2508.17104", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.17104", "abs": "https://arxiv.org/abs/2508.17104", "authors": ["Sz-Ting Tzeng", "Frank Dignum"], "title": "Rethinking How AI Embeds and Adapts to Human Values: Challenges and Opportunities", "comment": "7 pages, accepted at VALE 2025", "summary": "The concepts of ``human-centered AI'' and ``value-based decision'' have\ngained significant attention in both research and industry. However, many\ncritical aspects remain underexplored and require further investigation. In\nparticular, there is a need to understand how systems incorporate human values,\nhow humans can identify these values within systems, and how to minimize the\nrisks of harm or unintended consequences. In this paper, we highlight the need\nto rethink how we frame value alignment and assert that value alignment should\nmove beyond static and singular conceptions of values. We argue that AI systems\nshould implement long-term reasoning and remain adaptable to evolving values.\nFurthermore, value alignment requires more theories to address the full\nspectrum of human values. Since values often vary among individuals or groups,\nmulti-agent systems provide the right framework for navigating pluralism,\nconflict, and inter-agent reasoning about values. We identify the challenges\nassociated with value alignment and indicate directions for advancing value\nalignment research. In addition, we broadly discuss diverse perspectives of\nvalue alignment, from design methodologies to practical applications."}
{"id": "2508.17720", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.17720", "abs": "https://arxiv.org/abs/2508.17720", "authors": ["Ziqi Guan", "Xin Yin", "Zhiyuan Peng", "Chao Ni"], "title": "RepoTransAgent: Multi-Agent LLM Framework for Repository-Aware Code Translation", "comment": null, "summary": "Repository-aware code translation is critical for modernizing legacy systems,\nenhancing maintainability, and enabling interoperability across diverse\nprogramming languages. While recent advances in large language models (LLMs)\nhave improved code translation quality, existing approaches face significant\nchallenges in practical scenarios: insufficient contextual understanding,\ninflexible prompt designs, and inadequate error correction mechanisms. These\nlimitations severely hinder accurate and efficient translation of complex,\nreal-world code repositories. To address these challenges, we propose\nRepoTransAgent, a novel multi-agent LLM framework for repository-aware code\ntranslation. RepoTransAgent systematically decomposes the translation process\ninto specialized subtasks-context retrieval, dynamic prompt construction, and\niterative code refinement-each handled by dedicated agents. Our approach\nleverages retrieval-augmented generation (RAG) for contextual information\ngathering, employs adaptive prompts tailored to varying repository scenarios,\nand introduces a reflection-based mechanism for systematic error correction. We\nevaluate RepoTransAgent on hundreds of Java-C# translation pairs from six\npopular open-source projects. Experimental results demonstrate that\nRepoTransAgent significantly outperforms state-of-the-art baselines in both\ncompile and pass rates. Specifically, RepoTransAgent achieves up to 55.34%\ncompile rate and 45.84% pass rate. Comprehensive analysis confirms the\nrobustness and generalizability of RepoTransAgent across different LLMs,\nestablishing its effectiveness for real-world repository-aware code\ntranslation."}
{"id": "2508.17121", "categories": ["cs.CR", "cs.MM", "cs.SD"], "pdf": "https://arxiv.org/pdf/2508.17121", "abs": "https://arxiv.org/abs/2508.17121", "authors": ["Zhenliang Gan", "Xiaoxiao Hu", "Sheng Li", "Zhenxing Qian", "Xinpeng Zhang"], "title": "SyncGuard: Robust Audio Watermarking Capable of Countering Desynchronization Attacks", "comment": null, "summary": "Audio watermarking has been widely applied in copyright protection and source\ntracing. However, due to the inherent characteristics of audio signals,\nwatermark localization and resistance to desynchronization attacks remain\nsignificant challenges. In this paper, we propose a learning-based scheme named\nSyncGuard to address these challenges. Specifically, we design a frame-wise\nbroadcast embedding strategy to embed the watermark in arbitrary-length audio,\nenhancing time-independence and eliminating the need for localization during\nwatermark extraction. To further enhance robustness, we introduce a\nmeticulously designed distortion layer. Additionally, we employ dilated\nresidual blocks in conjunction with dilated gated blocks to effectively capture\nmulti-resolution time-frequency features. Extensive experimental results show\nthat SyncGuard efficiently handles variable-length audio segments, outperforms\nstate-of-the-art methods in robustness against various attacks, and delivers\nsuperior auditory quality."}
{"id": "2508.17180", "categories": ["cs.AI", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.17180", "abs": "https://arxiv.org/abs/2508.17180", "authors": ["Nilay Pande", "Sahiti Yerramilli", "Jayant Sravan Tamarapalli", "Rynaa Grover"], "title": "MaRVL-QA: A Benchmark for Mathematical Reasoning over Visual Landscapes", "comment": null, "summary": "A key frontier for Multimodal Large Language Models (MLLMs) is the ability to\nperform deep mathematical and spatial reasoning directly from images, moving\nbeyond their established success in semantic description. Mathematical surface\nplots provide a rigorous testbed for this capability, as they isolate the task\nof reasoning from the semantic noise common in natural images. To measure\nprogress on this frontier, we introduce MaRVL-QA (Mathematical Reasoning over\nVisual Landscapes), a new benchmark designed to quantitatively evaluate these\ncore reasoning skills. The benchmark comprises two novel tasks: Topological\nCounting, identifying and enumerating features like local maxima; and\nTransformation Recognition, recognizing applied geometric transformations.\nGenerated from a curated library of functions with rigorous ambiguity\nfiltering, our evaluation on MaRVL-QA reveals that even state-of-the-art MLLMs\nstruggle significantly, often resorting to superficial heuristics instead of\nrobust spatial reasoning. MaRVL-QA provides a challenging new tool for the\nresearch community to measure progress, expose model limitations, and guide the\ndevelopment of MLLMs with more profound reasoning abilities."}
{"id": "2508.17851", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.17851", "abs": "https://arxiv.org/abs/2508.17851", "authors": ["Patrick Loic Foalem", "Leuson Da Silva", "Foutse Khomh", "Heng Li", "Ettore Merlo"], "title": "Logging Requirement for Continuous Auditing of Responsible Machine Learning-based Applications", "comment": null, "summary": "Machine learning (ML) is increasingly applied across industries to automate\ndecision-making, but concerns about ethical and legal compliance remain due to\nlimited transparency, fairness, and accountability. Monitoring through logging\na long-standing practice in traditional software offers a potential means for\nauditing ML applications, as logs provide traceable records of system behavior\nuseful for debugging, performance analysis, and continuous auditing.\nsystematically auditing models for compliance or accountability. The findings\nunderscore the need for enhanced logging practices and tooling that\nsystematically integrate responsible AI metrics. Such practices would support\nthe development of auditable, transparent, and ethically responsible ML\nsystems, aligning with growing regulatory requirements and societal\nexpectations. By highlighting specific deficiencies and opportunities, this\nwork provides actionable guidance for both practitioners and tool developers\nseeking to strengthen the accountability and trustworthiness of ML\napplications."}
{"id": "2508.17155", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.17155", "abs": "https://arxiv.org/abs/2508.17155", "authors": ["Derek Lilienthal", "Sanghyun Hong"], "title": "Mind the Gap: Time-of-Check to Time-of-Use Vulnerabilities in LLM-Enabled Agents", "comment": "Pre-print", "summary": "Large Language Model (LLM)-enabled agents are rapidly emerging across a wide\nrange of applications, but their deployment introduces vulnerabilities with\nsecurity implications. While prior work has examined prompt-based attacks\n(e.g., prompt injection) and data-oriented threats (e.g., data exfiltration),\ntime-of-check to time-of-use (TOCTOU) remain largely unexplored in this\ncontext. TOCTOU arises when an agent validates external state (e.g., a file or\nAPI response) that is later modified before use, enabling practical attacks\nsuch as malicious configuration swaps or payload injection. In this work, we\npresent the first study of TOCTOU vulnerabilities in LLM-enabled agents. We\nintroduce TOCTOU-Bench, a benchmark with 66 realistic user tasks designed to\nevaluate this class of vulnerabilities. As countermeasures, we adapt detection\nand mitigation techniques from systems security to this setting and propose\nprompt rewriting, state integrity monitoring, and tool-fusing. Our study\nhighlights challenges unique to agentic workflows, where we achieve up to 25%\ndetection accuracy using automated detection methods, a 3% decrease in\nvulnerable plan generation, and a 95% reduction in the attack window. When\ncombining all three approaches, we reduce the TOCTOU vulnerabilities from an\nexecuted trajectory from 12% to 8%. Our findings open a new research direction\nat the intersection of AI safety and systems security."}
{"id": "2508.17188", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.17188", "abs": "https://arxiv.org/abs/2508.17188", "authors": ["Zhilin Zhang", "Xiang Zhang", "Jiaqi Wei", "Yiwei Xu", "Chenyu You"], "title": "PosterGen: Aesthetic-Aware Paper-to-Poster Generation via Multi-Agent LLMs", "comment": "Project Website: https://Y-Research-SBU.github.io/PosterGen", "summary": "Multi-agent systems built upon large language models (LLMs) have demonstrated\nremarkable capabilities in tackling complex compositional tasks. In this work,\nwe apply this paradigm to the paper-to-poster generation problem, a practical\nyet time-consuming process faced by researchers preparing for conferences.\nWhile recent approaches have attempted to automate this task, most neglect core\ndesign and aesthetic principles, resulting in posters that require substantial\nmanual refinement. To address these design limitations, we propose PosterGen, a\nmulti-agent framework that mirrors the workflow of professional poster\ndesigners. It consists of four collaborative specialized agents: (1) Parser and\nCurator agents extract content from the paper and organize storyboard; (2)\nLayout agent maps the content into a coherent spatial layout; (3) Stylist\nagents apply visual design elements such as color and typography; and (4)\nRenderer composes the final poster. Together, these agents produce posters that\nare both semantically grounded and visually appealing. To evaluate design\nquality, we introduce a vision-language model (VLM)-based rubric that measures\nlayout balance, readability, and aesthetic coherence. Experimental results show\nthat PosterGen consistently matches in content fidelity, and significantly\noutperforms existing methods in visual designs, generating posters that are\npresentation-ready with minimal human refinements."}
{"id": "2508.17882", "categories": ["cs.SE", "cs.SC", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2508.17882", "abs": "https://arxiv.org/abs/2508.17882", "authors": ["Izudin Dzafic", "Rabih A. Jabr"], "title": "modelSolver: A Symbolic Model-Driven Solver for Power Network Simulation and Monitoring", "comment": null, "summary": "The development of advanced software tools for power system analysis requires\nextensive programming expertise. Even when using open-source tools, programming\nskills are essential to modify built-in models. This can be particularly\nchallenging for domain experts who lack coding proficiency. This paper\nintroduces modelSolver, a software solution with a new framework centered\naround symbolic mathematical modeling. The proposed paradigm facilitates\ndefining models through intuitive mathematical expressions, thus eliminating\nthe need for traditional programming constructs such as arrays, loops, and\nsparse matrix computations. The modelSolver focuses on power flow and state\nestimation using an open-box approach, which allows users to specify custom\nmodels using either real or complex variables. Unlike existing tools that rely\non hard-coded models, modelSolver enables the representation of a wide range of\nadvanced functionalities, including power flow with voltage regulators and load\ntap changers, continuation power flow, and Gauss-Newton state estimation with\nequality constraints. Compatibility with MATPOWER is ensured via a converter\nthat automates importing data files. The framework prioritizes model-driven\ndevelopment and empowers domain experts to focus on power system modeling\nwithout programming barriers. It aims to simplify power system computations,\nmaking them more accessible to students, scientists, and practitioners."}
{"id": "2508.17222", "categories": ["cs.CR", "cs.AI", "cs.IR"], "pdf": "https://arxiv.org/pdf/2508.17222", "abs": "https://arxiv.org/abs/2508.17222", "authors": ["Jiale Liu", "Jiahao Zhang", "Suhang Wang"], "title": "Exposing Privacy Risks in Graph Retrieval-Augmented Generation", "comment": null, "summary": "Retrieval-Augmented Generation (RAG) is a powerful technique for enhancing\nLarge Language Models (LLMs) with external, up-to-date knowledge. Graph RAG has\nemerged as an advanced paradigm that leverages graph-based knowledge structures\nto provide more coherent and contextually rich answers. However, the move from\nplain document retrieval to structured graph traversal introduces new,\nunder-explored privacy risks. This paper investigates the data extraction\nvulnerabilities of the Graph RAG systems. We design and execute tailored data\nextraction attacks to probe their susceptibility to leaking both raw text and\nstructured data, such as entities and their relationships. Our findings reveal\na critical trade-off: while Graph RAG systems may reduce raw text leakage, they\nare significantly more vulnerable to the extraction of structured entity and\nrelationship information. We also explore potential defense mechanisms to\nmitigate these novel attack surfaces. This work provides a foundational\nanalysis of the unique privacy challenges in Graph RAG and offers insights for\nbuilding more secure systems."}
{"id": "2508.17198", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.17198", "abs": "https://arxiv.org/abs/2508.17198", "authors": ["Shouwei Ruan", "Liyuan Wang", "Caixin Kang", "Qihui Zhu", "Songming Liu", "Xingxing Wei", "Hang Su"], "title": "From reactive to cognitive: brain-inspired spatial intelligence for embodied agents", "comment": "40 pages, 8 figures", "summary": "Spatial cognition enables adaptive goal-directed behavior by constructing\ninternal models of space. Robust biological systems consolidate spatial\nknowledge into three interconnected forms: \\textit{landmarks} for salient cues,\n\\textit{route knowledge} for movement trajectories, and \\textit{survey\nknowledge} for map-like representations. While recent advances in multi-modal\nlarge language models (MLLMs) have enabled visual-language reasoning in\nembodied agents, these efforts lack structured spatial memory and instead\noperate reactively, limiting their generalization and adaptability in complex\nreal-world environments. Here we present Brain-inspired Spatial Cognition for\nNavigation (BSC-Nav), a unified framework for constructing and leveraging\nstructured spatial memory in embodied agents. BSC-Nav builds allocentric\ncognitive maps from egocentric trajectories and contextual cues, and\ndynamically retrieves spatial knowledge aligned with semantic goals. Integrated\nwith powerful MLLMs, BSC-Nav achieves state-of-the-art efficacy and efficiency\nacross diverse navigation tasks, demonstrates strong zero-shot generalization,\nand supports versatile embodied behaviors in the real physical world, offering\na scalable and biologically grounded path toward general-purpose spatial\nintelligence."}
{"id": "2508.17900", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.17900", "abs": "https://arxiv.org/abs/2508.17900", "authors": ["Mohammed O. Alannsary"], "title": "A Defect Classification Framework for AI-Based Software Systems (AI-ODC)", "comment": "Article, 19 pages, 6 figures, 8 tables,", "summary": "Artificial Intelligence has gained a lot of attention recently, it has been\nutilized in several fields ranging from daily life activities, such as\nresponding to emails and scheduling appointments, to manufacturing and\nautomating work activities. Artificial Intelligence systems are mainly\nimplemented as software solutions, and it is essential to discover and remove\nsoftware defects to assure its quality using defect analysis which is one of\nthe major activities that contribute to software quality. Despite the\nproliferation of AI-based systems, current defect analysis models fail to\ncapture their unique attributes. This paper proposes a framework inspired by\nthe Orthogonal Defect Classification (ODC) paradigm and enables defect analysis\nof Artificial Intelligence systems while recognizing its special attributes and\ncharacteristics. This study demonstrated the feasibility of modifying ODC for\nAI systems to classify its defects. The ODC was adjusted to accommodate the\nData, Learning, and Thinking aspects of AI systems which are newly introduced\nclassification dimensions. This adjustment involved the introduction of an\nadditional attribute to the ODC attributes, the incorporation of a new severity\nlevel, and the substitution of impact areas with characteristics pertinent to\nAI systems. The framework was showcased by applying it to a publicly available\nMachine Learning bug dataset, with results analyzed through one-way and two-way\nanalysis. The case study indicated that defects occurring during the Learning\nphase were the most prevalent and were significantly linked to high-severity\nclassifications. In contrast, defects identified in the Thinking phase had a\ndisproportionate effect on trustworthiness and accuracy. These findings\nillustrate AIODC's capability to identify high-risk defect categories and\ninform focused quality assurance measures."}
{"id": "2508.17296", "categories": ["cs.CR", "81P68", "F.2; F.1; E.3"], "pdf": "https://arxiv.org/pdf/2508.17296", "abs": "https://arxiv.org/abs/2508.17296", "authors": ["Adi Mutha", "Jitendra Sandu"], "title": "Literature Review of the Effect of Quantum Computing on Cryptocurrencies using Blockchain Technology", "comment": "21 pages", "summary": "With the advent of quantum computing, cryptocurrencies that rely on\nblockchain technology face mounting cryptographic vulnerabilities. This paper\npresents a comprehensive literature review evaluating how quantum algorithms,\nspecifically Shors and Grovers, could disrupt the foundational security\nmechanisms of cryptocurrencies. Shors algorithm poses a threat to public-key\ncryptographic schemes by enabling efficient factorization and discrete\nlogarithm solving, thereby endangering digital signature systems. Grovers\nalgorithm undermines hash-based functions, increasing the feasibility of fifty\none percent attacks and hash collisions. By examining the internal mechanisms\nof major cryptocurrencies such as Bitcoin, Ethereum, Litecoin, Monero, and\nZcash, this review identifies specific vulnerabilities in transaction and\nconsensus processes. It further analyses the current hardware limitations of\nquantum systems and estimates when such attacks could become feasible. In\nanticipation, it investigates countermeasures including Post-Quantum\nCryptography (PQC), Quantum Key Distribution (QKD), and protocol-level\nmodifications such as memory-intensive proof-of-work algorithms and\nmulti-signature schemes. The discussion integrates recent advancements in\nquantum error correction, hardware scalability, and NIST-standardized\ncryptographic algorithms. This review concludes that while quantum computers\nare not yet advanced enough to pose an immediate threat, proactive integration\nof quantum-resistant solutions is essential. The findings underscore the urgent\nneed for cryptocurrencies to adopt post-quantum cryptographic standards to\npreserve the decentralized trust, integrity, and security that define\nblockchain-based digital cryptocurrencies."}
{"id": "2508.17200", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.17200", "abs": "https://arxiv.org/abs/2508.17200", "authors": ["Amirreza Talebi"], "title": "Large Language Model-Based Automatic Formulation for Stochastic Optimization Models", "comment": null, "summary": "This paper presents the first integrated systematic study on the performance\nof large language models (LLMs), specifically ChatGPT, to automatically\nformulate and solve stochastic optimiza- tion problems from natural language\ndescriptions. Focusing on three key categories, joint chance- constrained\nmodels, individual chance-constrained models, and two-stage stochastic linear\nprograms (SLP-2), we design several prompts that guide ChatGPT through\nstructured tasks using chain-of- thought and modular reasoning. We introduce a\nnovel soft scoring metric that evaluates the struc- tural quality and partial\ncorrectness of generated models, addressing the limitations of canonical and\nexecution-based accuracy. Across a diverse set of stochastic problems,\nGPT-4-Turbo outperforms other models in partial score, variable matching, and\nobjective accuracy, with cot_s_instructions and agentic emerging as the most\neffective prompting strategies. Our findings reveal that with well-engineered\nprompts and multi-agent collaboration, LLMs can facilitate specially stochastic\nformulations, paving the way for intelligent, language-driven modeling\npipelines in stochastic opti- mization."}
{"id": "2508.17912", "categories": ["cs.SE", "cs.HC"], "pdf": "https://arxiv.org/pdf/2508.17912", "abs": "https://arxiv.org/abs/2508.17912", "authors": ["Mohammed O. Alannsary"], "title": "Evaluating Citizen Satisfaction with Saudi Arabia's E-Government Services: A Standards-Based, Theory-Informed Approach", "comment": "38 pages, 1 figure, 16 tables, journal research paper", "summary": "As digital government platforms become central to public service delivery,\nunderstanding citizen assessment is crucial for enhancing usability, trust, and\ninclusivity. This study investigates citizen satisfaction with the e-government\nservices in Saudi Arabia through a quality-in-use framework based on ISO/IEC\n25010 and ISO/IEC 25022 standards, interpreted through the lens of the Unified\nTheory of Acceptance and Use of Technology (UTAUT). A structured questionnaire\nwas administered to 500 citizens, yielding 276 valid responses. Satisfaction\nwas evaluated across four dimensions: overall satisfaction, feature\nsatisfaction, trust, and emotional engagement (pleasure). The findings\ndemonstrate consistently high levels of satisfaction regarding usability and\ntrust, aligning with Saudi Arabia's top-tier global ranking in e-government\ndevelopment. However, the results also highlight persistent challenges related\nto service clarity and system responsiveness. Emotional engagement was limited,\nindicating that users perceive these services primarily as functional tools\nrather than as engaging digital experiences. The study offers valuable insights\nfor policymakers and contributes to the theoretical integration of\nstandards-based and behavioral adoption models in the context of citizenship."}
{"id": "2508.17304", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2508.17304", "abs": "https://arxiv.org/abs/2508.17304", "authors": ["Muhammad Ibn Ziauddin", "Rownak Rahad Rabbi", "SM Mehrab", "Fardin Faiyaz", "Mosarrat Jahan"], "title": "An Efficient Recommendation Filtering-based Trust Model for Securing Internet of Things", "comment": null, "summary": "Trust computation is crucial for ensuring the security of the Internet of\nThings (IoT). However, current trust-based mechanisms for IoT have limitations\nthat impact data security. Sliding window-based trust schemes cannot ensure\nreliable trust computation due to their inability to select appropriate window\nlengths. Besides, recent trust scores are emphasized when considering the\neffect of time on trust. This can cause a sudden change in overall trust score\nbased on recent behavior, potentially misinterpreting an honest service\nprovider as malicious and vice versa. Moreover, clustering mechanisms used to\nfilter recommendations in trust computation often lead to slower results. In\nthis paper, we propose a robust trust model to address these limitations. The\nproposed approach determines the window length dynamically to guarantee\naccurate trust computation. It uses the harmonic mean of average trust score\nand time to prevent sudden fluctuations in trust scores. Additionally, an\nefficient personalized subspace clustering algorithm is used to exclude\nrecommendations. We present a security analysis demonstrating the resiliency of\nthe proposed scheme against bad-mouthing, ballot-stuffing, and on-off attacks.\nThe proposed scheme demonstrates a competitive performance in detecting\nbad-mouthing attacks, while outperforming existing works with an approximately\n44% improvement in accuracy for detecting on-off attacks. It maintains its\neffectiveness even when the percentage of on-off attackers increases and in\nscenarios where multiple attacks occur simultaneously. Additionally, the\nproposed scheme reduces the recommendation filtering time by 95%."}
{"id": "2508.17207", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.17207", "abs": "https://arxiv.org/abs/2508.17207", "authors": ["Xinyu Qin", "Mark H. Chignell", "Alexandria Greifenberger", "Sachinthya Lokuge", "Elssa Toumeh", "Tia Sternat", "Martin Katzman", "Lu Wang"], "title": "Explainable Counterfactual Reasoning in Depression Medication Selection at Multi-Levels (Personalized and Population)", "comment": null, "summary": "Background: This study investigates how variations in Major Depressive\nDisorder (MDD) symptoms, quantified by the Hamilton Rating Scale for Depression\n(HAM-D), causally influence the prescription of SSRIs versus SNRIs. Methods: We\napplied explainable counterfactual reasoning with counterfactual explanations\n(CFs) to assess the impact of specific symptom changes on antidepressant\nchoice. Results: Among 17 binary classifiers, Random Forest achieved highest\nperformance (accuracy, F1, precision, recall, ROC-AUC near 0.85). Sample-based\nCFs revealed both local and global feature importance of individual symptoms in\nmedication selection. Conclusions: Counterfactual reasoning elucidates which\nMDD symptoms most strongly drive SSRI versus SNRI selection, enhancing\ninterpretability of AI-based clinical decision support systems. Future work\nshould validate these findings on more diverse cohorts and refine algorithms\nfor clinical deployment."}
{"id": "2508.17988", "categories": ["cs.SE", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.17988", "abs": "https://arxiv.org/abs/2508.17988", "authors": ["Eduardo de Conto", "Blaise Genest", "Arvind Easwaran", "Nicholas Ng", "Shweta Menon"], "title": "DesCartes Builder: A Tool to Develop Machine-Learning Based Digital Twins", "comment": "5 pages, 4 figures. Accepted at EDTconf 2025", "summary": "Digital twins (DTs) are increasingly utilized to monitor, manage, and\noptimize complex systems across various domains, including civil engineering. A\ncore requirement for an effective DT is to act as a fast, accurate, and\nmaintainable surrogate of its physical counterpart, the physical twin (PT). To\nthis end, machine learning (ML) is frequently employed to (i) construct\nreal-time DT prototypes using efficient reduced-order models (ROMs) derived\nfrom high-fidelity simulations of the PT's nominal behavior, and (ii)\nspecialize these prototypes into DT instances by leveraging historical sensor\ndata from the target PT. Despite the broad applicability of ML, its use in DT\nengineering remains largely ad hoc. Indeed, while conventional ML pipelines\noften train a single model for a specific task, DTs typically require multiple,\ntask- and domain-dependent models. Thus, a more structured approach is required\nto design DTs.\n  In this paper, we introduce DesCartes Builder, an open-source tool to enable\nthe systematic engineering of ML-based pipelines for real-time DT prototypes\nand DT instances. The tool leverages an open and flexible visual data flow\nparadigm to facilitate the specification, composition, and reuse of ML models.\nIt also integrates a library of parameterizable core operations and ML\nalgorithms tailored for DT design. We demonstrate the effectiveness and\nusability of DesCartes Builder through a civil engineering use case involving\nthe design of a real-time DT prototype to predict the plastic strain of a\nstructure."}
{"id": "2508.17329", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2508.17329", "abs": "https://arxiv.org/abs/2508.17329", "authors": ["Xiaoyan Zhang", "Dongyang Lyu", "Xiaoqi Li"], "title": "Risk Assessment and Security Analysis of Large Language Models", "comment": null, "summary": "As large language models (LLMs) expose systemic security challenges in high\nrisk applications, including privacy leaks, bias amplification, and malicious\nabuse, there is an urgent need for a dynamic risk assessment and collaborative\ndefence framework that covers their entire life cycle. This paper focuses on\nthe security problems of large language models (LLMs) in critical application\nscenarios, such as the possibility of disclosure of user data, the deliberate\ninput of harmful instructions, or the models bias. To solve these problems, we\ndescribe the design of a system for dynamic risk assessment and a hierarchical\ndefence system that allows different levels of protection to cooperate. This\npaper presents a risk assessment system capable of evaluating both static and\ndynamic indicators simultaneously. It uses entropy weighting to calculate\nessential data, such as the frequency of sensitive words, whether the API call\nis typical, the realtime risk entropy value is significant, and the degree of\ncontext deviation. The experimental results show that the system is capable of\nidentifying concealed attacks, such as role escape, and can perform rapid risk\nevaluation. The paper uses a hybrid model called BERT-CRF (Bidirectional\nEncoder Representation from Transformers) at the input layer to identify and\nfilter malicious commands. The model layer uses dynamic adversarial training\nand differential privacy noise injection technology together. The output layer\nalso has a neural watermarking system that can track the source of the content.\nIn practice, the quality of this method, especially important in terms of\ncustomer service in the financial industry."}
{"id": "2508.17212", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.17212", "abs": "https://arxiv.org/abs/2508.17212", "authors": ["Xinyu Qin", "Ruiheng Yu", "Lu Wang"], "title": "Reinforcement Learning enhanced Online Adaptive Clinical Decision Support via Digital Twin powered Policy and Treatment Effect optimized Reward", "comment": null, "summary": "Clinical decision support must adapt online under safety constraints. We\npresent an online adaptive tool where reinforcement learning provides the\npolicy, a patient digital twin provides the environment, and treatment effect\ndefines the reward. The system initializes a batch-constrained policy from\nretrospective data and then runs a streaming loop that selects actions, checks\nsafety, and queries experts only when uncertainty is high. Uncertainty comes\nfrom a compact ensemble of five Q-networks via the coefficient of variation of\naction values with a $\\tanh$ compression. The digital twin updates the patient\nstate with a bounded residual rule. The outcome model estimates immediate\nclinical effect, and the reward is the treatment effect relative to a\nconservative reference with a fixed z-score normalization from the training\nsplit. Online updates operate on recent data with short runs and exponential\nmoving averages. A rule-based safety gate enforces vital ranges and\ncontraindications before any action is applied. Experiments in a synthetic\nclinical simulator show low latency, stable throughput, a low expert query rate\nat fixed safety, and improved return against standard value-based baselines.\nThe design turns an offline policy into a continuous, clinician-supervised\nsystem with clear controls and fast adaptation."}
{"id": "2508.18003", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.18003", "abs": "https://arxiv.org/abs/2508.18003", "authors": ["Robert Heum√ºller", "Frank Ortmeier"], "title": "Previously on... Automating Code Review", "comment": "Preprint currently under review", "summary": "Modern Code Review (MCR) is a standard practice in software engineering, yet\nit demands substantial time and resource investments. Recent research has\nincreasingly explored automating core review tasks using machine learning (ML)\nand deep learning (DL). As a result, there is substantial variability in task\ndefinitions, datasets, and evaluation procedures. This study provides the first\ncomprehensive analysis of MCR automation research, aiming to characterize the\nfield's evolution, formalize learning tasks, highlight methodological\nchallenges, and offer actionable recommendations to guide future research.\nFocusing on the primary code review tasks, we systematically surveyed 691\npublications and identified 24 relevant studies published between May 2015 and\nApril 2024. Each study was analyzed in terms of tasks, models, metrics,\nbaselines, results, validity concerns, and artifact availability. In\nparticular, our analysis reveals significant potential for standardization,\nincluding 48 task metric combinations, 22 of which were unique to their\noriginal paper, and limited dataset reuse. We highlight challenges and derive\nconcrete recommendations for examples such as the temporal bias threat, which\nare rarely addressed so far. Our work contributes to a clearer overview of the\nfield, supports the framing of new research, helps to avoid pitfalls, and\npromotes greater standardization in evaluation practices."}
{"id": "2508.17414", "categories": ["cs.CR", "cs.CY", "cs.HC"], "pdf": "https://arxiv.org/pdf/2508.17414", "abs": "https://arxiv.org/abs/2508.17414", "authors": ["Temesgen Kitaw Damenu", "ƒ∞nci Zaim G√∂kbay", "Alexandra Covaci", "Shujun Li"], "title": "Cyber Security Educational Games for Children: A Systematic Literature Review", "comment": null, "summary": "Educational games have been widely used to teach children about cyber\nsecurity. This systematic literature review reveals evidence of positive\nlearning outcomes, after analysing 91 such games reported in 68 papers\npublished between 2010 and 2024. However, critical gaps have also been\nidentified regarding the design processes and the methodological rigour,\nincluding lack of systematic design, misalignment between proposed and achieved\nlearning outcomes, rare use of control groups, limited discussions on ethical\nconsiderations, and underutilisation of emerging technologies. We recommend\nmultiple future research directions, e.g., a hybrid approach to game design and\nevaluation that combines bottom-up and top-down approaches."}
{"id": "2508.17221", "categories": ["cs.AI", "cs.LG", "cs.LO"], "pdf": "https://arxiv.org/pdf/2508.17221", "abs": "https://arxiv.org/abs/2508.17221", "authors": ["Sopam Dasgupta", "Sadaf MD Halim", "Joaqu√≠n Arias", "Elmer Salazar", "Gopal Gupta"], "title": "MC3G: Model Agnostic Causally Constrained Counterfactual Generation", "comment": null, "summary": "Machine learning models increasingly influence decisions in high-stakes\nsettings such as finance, law and hiring, driving the need for transparent,\ninterpretable outcomes. However, while explainable approaches can help\nunderstand the decisions being made, they may inadvertently reveal the\nunderlying proprietary algorithm: an undesirable outcome for many\npractitioners. Consequently, it is crucial to balance meaningful transparency\nwith a form of recourse that clarifies why a decision was made and offers\nactionable steps following which a favorable outcome can be obtained.\nCounterfactual explanations offer a powerful mechanism to address this need by\nshowing how specific input changes lead to a more favorable prediction. We\npropose Model-Agnostic Causally Constrained Counterfactual Generation (MC3G), a\nnovel framework that tackles limitations in the existing counterfactual\nmethods. First, MC3G is model-agnostic: it approximates any black-box model\nusing an explainable rule-based surrogate model. Second, this surrogate is used\nto generate counterfactuals that produce a favourable outcome for the original\nunderlying black box model. Third, MC3G refines cost computation by excluding\nthe ``effort\" associated with feature changes that occur automatically due to\ncausal dependencies. By focusing only on user-initiated changes, MC3G provides\na more realistic and fair representation of the effort needed to achieve a\nfavourable outcome. We show that MC3G delivers more interpretable and\nactionable counterfactual recommendations compared to existing techniques all\nwhile having a lower cost. Our findings highlight MC3G's potential to enhance\ntransparency, accountability, and practical utility in decision-making\nprocesses that incorporate machine-learning approaches."}
{"id": "2508.18070", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.18070", "abs": "https://arxiv.org/abs/2508.18070", "authors": ["Karolina M. Milano", "Wesley K. G. Assun√ß√£o", "Bruno B. P. Cafeo"], "title": "A Large-Scale Study on Developer Engagement and Expertise in Configurable Software System Projects", "comment": null, "summary": "Modern systems operate in multiple contexts making variability a fundamental\naspect of Configurable Software Systems (CSSs). Variability, implemented via\npre-processor directives (e.g., #ifdef blocks) interleaved with other code and\nspread across files, complicates maintenance and increases error risk. Despite\nits importance, little is known about how variable code is distributed among\ndevelopers or whether conventional expertise metrics adequately capture\nvariable code proficiency. This study investigates developers' engagement with\nvariable versus mandatory code, the concentration of variable code workload,\nand the effectiveness of expertise metrics in CSS projects. We mined\nrepositories of 25 CSS projects, analyzing 450,255 commits from 9,678\ndevelopers. Results show that 59% of developers never modified variable code,\nwhile about 17% were responsible for developing and maintaining 83% of it. This\nindicates a high concentration of variable code expertise among a few\ndevelopers, suggesting that task assignments should prioritize these\nspecialists. Moreover, conventional expertise metrics performed\npoorly--achieving only around 55% precision and 50% recall in identifying\ndevelopers engaged with variable code. Our findings highlight an unbalanced\ndistribution of variable code responsibilities and underscore the need to\nrefine expertise metrics to better support task assignments in CSS projects,\nthereby promoting a more equitable workload distribution."}
{"id": "2508.17481", "categories": ["cs.CR", "cs.RO"], "pdf": "https://arxiv.org/pdf/2508.17481", "abs": "https://arxiv.org/abs/2508.17481", "authors": ["Priyanka Prakash Surve", "Asaf Shabtai", "Yuval Elovici"], "title": "SoK: Cybersecurity Assessment of Humanoid Ecosystem", "comment": null, "summary": "Humanoids are progressing toward practical deployment across healthcare,\nindustrial, defense, and service sectors. While typically considered\ncyber-physical systems (CPSs), their dependence on traditional networked\nsoftware stacks (e.g., Linux operating systems), robot operating system (ROS)\nmiddleware, and over-the-air update channels, creates a distinct security\nprofile that exposes them to vulnerabilities conventional CPS models do not\nfully address. Prior studies have mainly examined specific threats, such as\nLiDAR spoofing or adversarial machine learning (AML). This narrow focus\noverlooks how an attack targeting one component can cascade harm throughout the\nrobot's interconnected systems. We address this gap through a systematization\nof knowledge (SoK) that takes a comprehensive approach, consolidating\nfragmented research from robotics, CPS, and network security domains. We\nintroduce a seven-layer security model for humanoid robots, organizing 39 known\nattacks and 35 defenses across the humanoid ecosystem-from hardware to\nhuman-robot interaction. Building on this security model, we develop a\nquantitative 39x35 attack-defense matrix with risk-weighted scoring, validated\nthrough Monte Carlo analysis. We demonstrate our method by evaluating three\nreal-world robots: Pepper, G1 EDU, and Digit. The scoring analysis revealed\nvarying security maturity levels, with scores ranging from 39.9% to 79.5%\nacross the platforms. This work introduces a structured, evidence-based\nassessment method that enables systematic security evaluation, supports\ncross-platform benchmarking, and guides prioritization of security investments\nin humanoid robotics."}
{"id": "2508.17244", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.17244", "abs": "https://arxiv.org/abs/2508.17244", "authors": ["Aoun E Muhammad", "Kin-Choong Yow", "Nebojsa Bacanin-Dzakula", "Muhammad Attique Khan"], "title": "L-XAIDS: A LIME-based eXplainable AI framework for Intrusion Detection Systems", "comment": "This is the authors accepted manuscript of an article accepted for\n  publication in Cluster Computing. The final published version is available\n  at: 10.1007/s10586-025-05326-9", "summary": "Recent developments in Artificial Intelligence (AI) and their applications in\ncritical industries such as healthcare, fin-tech and cybersecurity have led to\na surge in research in explainability in AI. Innovative research methods are\nbeing explored to extract meaningful insight from blackbox AI systems to make\nthe decision-making technology transparent and interpretable. Explainability\nbecomes all the more critical when AI is used in decision making in domains\nlike fintech, healthcare and safety critical systems such as cybersecurity and\nautonomous vehicles. However, there is still ambiguity lingering on the\nreliable evaluations for the users and nature of transparency in the\nexplanations provided for the decisions made by black-boxed AI. To solve the\nblackbox nature of Machine Learning based Intrusion Detection Systems, a\nframework is proposed in this paper to give an explanation for IDSs decision\nmaking. This framework uses Local Interpretable Model-Agnostic Explanations\n(LIME) coupled with Explain Like I'm five (ELI5) and Decision Tree algorithms\nto provide local and global explanations and improve the interpretation of\nIDSs. The local explanations provide the justification for the decision made on\na specific input. Whereas, the global explanations provides the list of\nsignificant features and their relationship with attack traffic. In addition,\nthis framework brings transparency in the field of ML driven IDS that might be\nhighly significant for wide scale adoption of eXplainable AI in cyber-critical\nsystems. Our framework is able to achieve 85 percent accuracy in classifying\nattack behaviour on UNSW-NB15 dataset, while at the same time displaying the\nfeature significance ranking of the top 10 features used in the classification."}
{"id": "2508.18073", "categories": ["cs.SE", "cs.DL"], "pdf": "https://arxiv.org/pdf/2508.18073", "abs": "https://arxiv.org/abs/2508.18073", "authors": ["Joenio Marques da Costa", "Christina von Flach"], "title": "Debian in the Research Software Ecosystem: A Bibliometric Analysis", "comment": "5 pages; 3 figures; 2 tables; to be published in DebConf25 Academic\n  Track https://www.diverse-team.fr/debconf25-academictrack", "summary": "Context: The Debian system has historically participated in academic works\nand scientific projects, with well-known examples including NeuroDebian, Debian\nMed, Debsources, Debian Science, and Debian GIS, where the scientific relevance\nof Debian and its contribution to the Research Software ecosystem are evident.\n  Objective: The objective of this study is to investigate the Debian system\nthrough academic publications, with the aim of classifying articles, mapping\nresearch, identifying trends, and finding opportunities.\n  Method: The study is based on a bibliometric analysis starting with an\ninitial search for the term \"Debian\" in the titles, abstracts, or keywords of\nacademic publications, using the Scopus database. This analysis calculates\nmetrics of co-citation, co-authorship, and word co-occurrence, and is guided by\na set of research questions and criteria for inclusion and exclusion to conduct\nthe bibliometric analysis.\n  Results: The study includes a set of articles published across various fields\nof knowledge, providing a map of the academic publication space about Debian.\nThe study's data will be available in a public repository, reporting\ndemographic and bibliometric trends, including the most cited articles, active\ncountries, researchers, and popular conferences.\n  Conclusion: Results includes a bibliometric and demographic analysis\nidentified in publications about Debian, shedding light on the intellectual\nstructure of academic research. The results of the analyses can help\nresearchers gain an overview of existing trends in publications about Debian\nand identify areas that require more attention from the scientific community."}
{"id": "2508.17674", "categories": ["cs.CR", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.17674", "abs": "https://arxiv.org/abs/2508.17674", "authors": ["Qiming Guo", "Jinwen Tang", "Xingran Huang"], "title": "Attacking LLMs and AI Agents: Advertisement Embedding Attacks Against Large Language Models", "comment": "7 pages, 2 figures", "summary": "We introduce Advertisement Embedding Attacks (AEA), a new class of LLM\nsecurity threats that stealthily inject promotional or malicious content into\nmodel outputs and AI agents. AEA operate through two low-cost vectors: (1)\nhijacking third-party service-distribution platforms to prepend adversarial\nprompts, and (2) publishing back-doored open-source checkpoints fine-tuned with\nattacker data. Unlike conventional attacks that degrade accuracy, AEA subvert\ninformation integrity, causing models to return covert ads, propaganda, or hate\nspeech while appearing normal. We detail the attack pipeline, map five\nstakeholder victim groups, and present an initial prompt-based self-inspection\ndefense that mitigates these injections without additional model retraining.\nOur findings reveal an urgent, under-addressed gap in LLM security and call for\ncoordinated detection, auditing, and policy responses from the AI-safety\ncommunity."}
{"id": "2508.17262", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.17262", "abs": "https://arxiv.org/abs/2508.17262", "authors": ["Hamta Sedghani", "Abednego Wamuhindo Kambale", "Federica Filippini", "Francesca Palermo", "Diana Trojaniello", "Danilo Ardagna"], "title": "Federated Reinforcement Learning for Runtime Optimization of AI Applications in Smart Eyewears", "comment": null, "summary": "Extended reality technologies are transforming fields such as healthcare,\nentertainment, and education, with Smart Eye-Wears (SEWs) and Artificial\nIntelligence (AI) playing a crucial role. However, SEWs face inherent\nlimitations in computational power, memory, and battery life, while offloading\ncomputations to external servers is constrained by network conditions and\nserver workload variability. To address these challenges, we propose a\nFederated Reinforcement Learning (FRL) framework, enabling multiple agents to\ntrain collaboratively while preserving data privacy. We implemented synchronous\nand asynchronous federation strategies, where models are aggregated either at\nfixed intervals or dynamically based on agent progress. Experimental results\nshow that federated agents exhibit significantly lower performance variability,\nensuring greater stability and reliability. These findings underscore the\npotential of FRL for applications requiring robust real-time AI processing,\nsuch as real-time object detection in SEWs."}
{"id": "2508.18089", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.18089", "abs": "https://arxiv.org/abs/2508.18089", "authors": ["Karine Even-Mendoza", "Alexander Brownlee", "Alina Geiger", "Carol Hanna", "Justyna Petke", "Federica Sarro", "Dominik Sobania"], "title": "LLM-Guided Genetic Improvement: Envisioning Semantic Aware Automated Software Evolution", "comment": null, "summary": "Genetic Improvement (GI) of software automatically creates alternative\nsoftware versions that are improved according to certain properties of\ninterests (e.g., running-time). Search-based GI excels at navigating large\nprogram spaces, but operates primarily at the syntactic level. In contrast,\nLarge Language Models (LLMs) offer semantic-aware edits, yet lack goal-directed\nfeedback and control (which is instead a strength of GI). As such, we propose\nthe investigation of a new research line on AI-powered GI aimed at\nincorporating semantic aware search. We take a first step at it by augmenting\nGI with the use of automated clustering of LLM edits. We provide initial\nempirical evidence that our proposal, dubbed PatchCat, allows us to\nautomatically and effectively categorize LLM-suggested patches. PatchCat\nidentified 18 different types of software patches and categorized newly\nsuggested patches with high accuracy. It also enabled detecting NoOp edits in\nadvance and, prospectively, to skip test suite execution to save resources in\nmany cases. These results, coupled with the fact that PatchCat works with\nsmall, local LLMs, are a promising step toward interpretable, efficient, and\ngreen GI. We outline a rich agenda of future work and call for the community to\njoin our vision of building a principled understanding of LLM-driven mutations,\nguiding the GI search process with semantic signals."}
{"id": "2508.17809", "categories": ["cs.CR", "cs.AR"], "pdf": "https://arxiv.org/pdf/2508.17809", "abs": "https://arxiv.org/abs/2508.17809", "authors": ["Abdullah Sahruri", "Martin Margala"], "title": "TLGLock: A New Approach in Logic Locking Using Key-Driven Charge Recycling in Threshold Logic Gates", "comment": "To appear in the 33rd IFIP/IEEE International Conference on Very\n  Large Scale Integration (VLSI-SoC 2025)", "summary": "Logic locking remains one of the most promising defenses against hardware\npiracy, yet current approaches often face challenges in scalability and design\noverhead. In this paper, we present TLGLock, a new design paradigm that\nleverages the structural expressiveness of Threshold Logic Gates (TLGs) and the\nenergy efficiency of charge recycling to enforce key-dependent functionality at\nthe gate level. By embedding the key into the gate's weighted logic and\nutilizing dynamic charge sharing, TLGLock provides a stateless and compact\nalternative to conventional locking techniques. We implement a complete\nsynthesis-to-locking flow and evaluate it using ISCAS, ITC, and MCNC\nbenchmarks. Results show that TLGLock achieves up to 30% area, 50% delay, and\n20% power savings compared to latch-based locking schemes. In comparison with\nXOR and SFLL-HD methods, TLGLock offers up to 3x higher SAT attack resistance\nwith significantly lower overhead. Furthermore, randomized key-weight\nexperiments demonstrate that TLGLock can reach up to 100% output corruption\nunder incorrect keys, enabling tunable security at minimal cost. These results\nposition TLGLock as a scalable and resilient solution for secure hardware\ndesign."}
{"id": "2508.17282", "categories": ["cs.AI", "cs.SD"], "pdf": "https://arxiv.org/pdf/2508.17282", "abs": "https://arxiv.org/abs/2508.17282", "authors": ["Xin Zhang", "Jiaming Chu", "Jian Zhao", "Yuchu Jiang", "Xu Yang", "Lei Jin", "Chi Zhang", "Xuelong Li"], "title": "ERF-BA-TFD+: A Multimodal Model for Audio-Visual Deepfake Detection", "comment": null, "summary": "Deepfake detection is a critical task in identifying manipulated multimedia\ncontent. In real-world scenarios, deepfake content can manifest across multiple\nmodalities, including audio and video. To address this challenge, we present\nERF-BA-TFD+, a novel multimodal deepfake detection model that combines enhanced\nreceptive field (ERF) and audio-visual fusion. Our model processes both audio\nand video features simultaneously, leveraging their complementary information\nto improve detection accuracy and robustness. The key innovation of ERF-BA-TFD+\nlies in its ability to model long-range dependencies within the audio-visual\ninput, allowing it to better capture subtle discrepancies between real and fake\ncontent. In our experiments, we evaluate ERF-BA-TFD+ on the DDL-AV dataset,\nwhich consists of both segmented and full-length video clips. Unlike previous\nbenchmarks, which focused primarily on isolated segments, the DDL-AV dataset\nallows us to assess the model's performance in a more comprehensive and\nrealistic setting. Our method achieves state-of-the-art results on this\ndataset, outperforming existing techniques in terms of both accuracy and\nprocessing speed. The ERF-BA-TFD+ model demonstrated its effectiveness in the\n\"Workshop on Deepfake Detection, Localization, and Interpretability,\" Track 2:\nAudio-Visual Detection and Localization (DDL-AV), and won first place in this\ncompetition."}
{"id": "2508.18106", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.18106", "abs": "https://arxiv.org/abs/2508.18106", "authors": ["Keke Lian", "Bin Wang", "Lei Zhang", "Libo Chen", "Junjie Wang", "Ziming Zhao", "Yujiu Yang", "Haotong Duan", "Haoran Zhao", "Shuang Liao", "Mingda Guo", "Jiazheng Quan", "Yilu Zhong", "Chenhao He", "Zichuan Chen", "Jie Wu", "Haoling Li", "Zhaoxuan Li", "Jiongchi Yu", "Hui Li", "Dong Zhang"], "title": "A.S.E: A Repository-Level Benchmark for Evaluating Security in AI-Generated Code", "comment": null, "summary": "The increasing adoption of large language models (LLMs) in software\nengineering necessitates rigorous security evaluation of their generated code.\nHowever, existing benchmarks are inadequate, as they focus on isolated code\nsnippets, employ unstable evaluation methods that lack reproducibility, and\nfail to connect the quality of input context with the security of the output.\nTo address these gaps, we introduce A.S.E (AI Code Generation Security\nEvaluation), a benchmark for repository-level secure code generation. A.S.E\nconstructs tasks from real-world repositories with documented CVEs, preserving\nfull repository context like build systems and cross-file dependencies. Its\nreproducible, containerized evaluation framework uses expert-defined rules to\nprovide stable, auditable assessments of security, build quality, and\ngeneration stability. Our evaluation of leading LLMs on A.S.E reveals three key\nfindings: (1) Claude-3.7-Sonnet achieves the best overall performance. (2) The\nsecurity gap between proprietary and open-source models is narrow;\nQwen3-235B-A22B-Instruct attains the top security score. (3) Concise,\n``fast-thinking'' decoding strategies consistently outperform complex,\n``slow-thinking'' reasoning for security patching."}
{"id": "2508.17853", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2508.17853", "abs": "https://arxiv.org/abs/2508.17853", "authors": ["Saeed Alshehhi"], "title": "Software Unclonable Functions for IoT Devices Identification and Security", "comment": null, "summary": "In the evolving landscape of IoT ecosystem, distinguishing between legitimate\nand compromised devices is a critical challenge. This research investigates the\neffectiveness of hardware performance counter (HPC)-derived signatures'\nuniqueness under the umbrella of a concept that we introduced as software\nunclonable functions (SUFs)."}
{"id": "2508.17290", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.17290", "abs": "https://arxiv.org/abs/2508.17290", "authors": ["Omid Ghahroodi", "Arshia Hemmat", "Marzia Nouri", "Seyed Mohammad Hadi Hosseini", "Doratossadat Dastgheib", "Mohammad Vali Sanian", "Alireza Sahebi", "Reihaneh Zohrabi", "Mohammad Hossein Rohban", "Ehsaneddin Asgari", "Mahdieh Soleymani Baghshah"], "title": "MEENA (PersianMMMU): Multimodal-Multilingual Educational Exams for N-level Assessment", "comment": null, "summary": "Recent advancements in large vision-language models (VLMs) have primarily\nfocused on English, with limited attention given to other languages. To address\nthis gap, we introduce MEENA (also known as PersianMMMU), the first dataset\ndesigned to evaluate Persian VLMs across scientific, reasoning, and human-level\nunderstanding tasks. Our dataset comprises approximately 7,500 Persian and\n3,000 English questions, covering a wide range of topics such as reasoning,\nmathematics, physics, diagrams, charts, and Persian art and literature. Key\nfeatures of MEENA include: (1) diverse subject coverage spanning various\neducational levels, from primary to upper secondary school, (2) rich metadata,\nincluding difficulty levels and descriptive answers, (3) original Persian data\nthat preserves cultural nuances, (4) a bilingual structure to assess\ncross-linguistic performance, and (5) a series of diverse experiments assessing\nvarious capabilities, including overall performance, the model's ability to\nattend to images, and its tendency to generate hallucinations. We hope this\nbenchmark contributes to enhancing VLM capabilities beyond English."}
{"id": "2508.16625", "categories": ["cs.CR", "cs.AI", "cs.SE"], "pdf": "https://arxiv.org/pdf/2508.16625", "abs": "https://arxiv.org/abs/2508.16625", "authors": ["Rijha Safdar", "Danyail Mateen", "Syed Taha Ali", "M. Umer Ashfaq", "Wajahat Hussain"], "title": "Data and Context Matter: Towards Generalizing AI-based Software Vulnerability Detection", "comment": null, "summary": "The performance of AI-based software vulnerability detection systems is often\nlimited by their poor generalization to unknown codebases. In this research, we\nexplore the impact of data quality and model architecture on the\ngeneralizability of vulnerability detection systems. By generalization we mean\nability of high vulnerability detection performance across different C/C++\nsoftware projects not seen during training. Through a series of experiments, we\ndemonstrate that improvements in dataset diversity and quality substantially\nenhance detection performance. Additionally, we compare multiple encoder-only\nand decoder-only models, finding that encoder based models outperform in terms\nof accuracy and generalization. Our model achieves 6.8% improvement in recall\non the benchmark BigVul[1] dataset, also outperforming on unseen projects,\nhence showing enhanced generalizability. These results highlight the role of\ndata quality and model selection in the development of robust vulnerability\ndetection systems. Our findings suggest a direction for future systems having\nhigh cross-project effectiveness."}
{"id": "2508.17856", "categories": ["cs.CR", "cs.SE"], "pdf": "https://arxiv.org/pdf/2508.17856", "abs": "https://arxiv.org/abs/2508.17856", "authors": ["Tiezhu Sun", "Marco Alecci", "Aleksandr Pilgun", "Yewei Song", "Xunzhu Tang", "Jordan Samhi", "Tegawend√© F. Bissyand√©", "Jacques Klein"], "title": "MalLoc: Toward Fine-grained Android Malicious Payload Localization via LLMs", "comment": "Accepted at ICSME 2025, NIER Track", "summary": "The rapid evolution of Android malware poses significant challenges to the\nmaintenance and security of mobile applications (apps). Traditional detection\ntechniques often struggle to keep pace with emerging malware variants that\nemploy advanced tactics such as code obfuscation and dynamic behavior\ntriggering. One major limitation of these approaches is their inability to\nlocalize malicious payloads at a fine-grained level, hindering precise\nunderstanding of malicious behavior. This gap in understanding makes the design\nof effective and targeted mitigation strategies difficult, leaving mobile apps\nvulnerable to continuously evolving threats.\n  To address this gap, we propose MalLoc, a novel approach that leverages the\ncode understanding capabilities of large language models (LLMs) to localize\nmalicious payloads at a fine-grained level within Android malware. Our\nexperimental results demonstrate the feasibility and effectiveness of using\nLLMs for this task, highlighting the potential of MalLoc to enhance precision\nand interpretability in malware analysis. This work advances beyond traditional\ndetection and classification by enabling deeper insights into behavior-level\nmalicious logic and opens new directions for research, including dynamic\nmodeling of localized threats and targeted countermeasure development."}
{"id": "2508.17291", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.17291", "abs": "https://arxiv.org/abs/2508.17291", "authors": ["Haonan Dong", "Haoran Ye", "Wenhao Zhu", "Kehan Jiang", "Guojie Song"], "title": "Meta-R1: Empowering Large Reasoning Models with Metacognition", "comment": null, "summary": "Large Reasoning Models (LRMs) demonstrate remarkable capabilities on complex\ntasks, exhibiting emergent, human-like thinking patterns. Despite their\nadvances, we identify a fundamental limitation: current LRMs lack a dedicated\nmeta-level cognitive system-an essential faculty in human cognition that\nenables \"thinking about thinking\". This absence leaves their emergent abilities\nuncontrollable (non-adaptive reasoning), unreliable (intermediate error), and\ninflexible (lack of a clear methodology). To address this gap, we introduce\nMeta-R1, a systematic and generic framework that endows LRMs with explicit\nmetacognitive capabilities. Drawing on principles from cognitive science,\nMeta-R1 decomposes the reasoning process into distinct object-level and\nmeta-level components, orchestrating proactive planning, online regulation, and\nadaptive early stopping within a cascaded framework. Experiments on three\nchallenging benchmarks and against eight competitive baselines demonstrate that\nMeta-R1 is: (I) high-performing, surpassing state-of-the-art methods by up to\n27.3%; (II) token-efficient, reducing token consumption to 15.7% ~ 32.7% and\nimproving efficiency by up to 14.8% when compared to its vanilla counterparts;\nand (III) transferable, maintaining robust performance across datasets and\nmodel backbones."}
{"id": "2508.16662", "categories": ["cs.CR", "cs.CY", "cs.NI", "cs.SE", "K.6.5; C.2.0; D.4.6"], "pdf": "https://arxiv.org/pdf/2508.16662", "abs": "https://arxiv.org/abs/2508.16662", "authors": ["Alexander Tabalipa"], "title": "Bridging the Mobile Trust Gap: A Zero Trust Framework for Consumer-Facing Applications", "comment": "43 pages, 5 figures, 9 tables. Working Paper - Version 1.0. Submitted\n  under a CC BY-SA 4.0 license. Also available as an SSRN Working Paper.\n  Feedback and collaboration are welcome", "summary": "Zero Trust Architecture (ZTA) has become a widely adopted model for securing\nenterprise environments, promoting continuous verification and minimal trust\nacross systems. However, its application in mobile contexts remains limited,\ndespite mobile applications now accounting for most global digital interactions\nand being increasingly targeted by sophisticated threats. Existing Zero Trust\nframeworks developed by organisations such as the National Institute of\nStandards and Technology (NIST) and the Cybersecurity and Infrastructure\nSecurity Agency (CISA) primarily focus on enterprise-managed infrastructure,\nassuming organisational control over devices, networks, and identities. This\npaper addresses a critical gap by proposing an extended Zero Trust model\ndesigned for mobile applications operating in untrusted, user-controlled\nenvironments. Using a design science methodology, the study introduced a\nsix-pillar framework that supports runtime enforcement of trust through\ncontrols including device integrity, user identity validation, data protection,\nsecure application programming interface (API) usage, behavioural monitoring,\nand live application protection. Each pillar was mapped to relevant regulatory\nand security standards to support compliance. A phased implementation roadmap\nand maturity assessment model were also developed to guide adoption across\nvarying organisational contexts. The proposed model offers a practical and\nstandards-aligned approach to securing mobile applications beyond\npre-deployment controls, aligning real-time enforcement with Zero Trust\nprinciples. This contribution expands the operational boundaries of ZTA and\nprovides organisations with a deployable path to reduce fraud, enhance\ncompliance, and address emerging mobile security challenges. Future research\nmay include empirical validation of the framework and cross-sector application\ntesting."}
{"id": "2508.17884", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2508.17884", "abs": "https://arxiv.org/abs/2508.17884", "authors": ["Toby Murray"], "title": "PhantomLint: Principled Detection of Hidden LLM Prompts in Structured Documents", "comment": null, "summary": "Hidden LLM prompts have appeared in online documents with increasing\nfrequency. Their goal is to trigger indirect prompt injection attacks while\nremaining undetected from human oversight, to manipulate LLM-powered automated\ndocument processing systems, against applications as diverse as r\\'esum\\'e\nscreeners through to academic peer review processes. Detecting hidden LLM\nprompts is therefore important for ensuring trust in AI-assisted human decision\nmaking.\n  This paper presents the first principled approach to hidden LLM prompt\ndetection in structured documents. We implement our approach in a prototype\ntool called PhantomLint. We evaluate PhantomLint against a corpus of 3,402\ndocuments, including both PDF and HTML documents, and covering academic paper\npreprints, CVs, theses and more. We find that our approach is generally\napplicable against a wide range of methods for hiding LLM prompts from visual\ninspection, has a very low false positive rate (approx. 0.092%), is practically\nuseful for detecting hidden LLM prompts in real documents, while achieving\nacceptable performance."}
{"id": "2508.17366", "categories": ["cs.AI", "cs.CY", "cs.MA", "68T42", "I.2.7; J.4"], "pdf": "https://arxiv.org/pdf/2508.17366", "abs": "https://arxiv.org/abs/2508.17366", "authors": ["Hanzhong Zhang", "Muhua Huang", "Jindong Wang"], "title": "Evolving Collective Cognition in Human-Agent Hybrid Societies: How Agents Form Stances and Boundaries", "comment": "37 pages, 6 figures", "summary": "Large language models have been widely used to simulate credible human social\nbehaviors. However, it remains unclear whether these models can demonstrate\nstable capacities for stance formation and identity negotiation in complex\ninteractions, as well as how they respond to human interventions. We propose a\ncomputational multi-agent society experiment framework that integrates\ngenerative agent-based modeling with virtual ethnographic methods to\ninvestigate how group stance differentiation and social boundary formation\nemerge in human-agent hybrid societies. Across three studies, we find that\nagents exhibit endogenous stances, independent of their preset identities, and\ndisplay distinct tonal preferences and response patterns to different discourse\nstrategies. Furthermore, through language interaction, agents actively\ndismantle existing identity-based power structures and reconstruct\nself-organized community boundaries based on these stances. Our findings\nsuggest that preset identities do not rigidly determine the agents' social\nstructures. For human researchers to effectively intervene in collective\ncognition, attention must be paid to the endogenous mechanisms and\ninteractional dynamics within the agents' language networks. These insights\nprovide a theoretical foundation for using generative AI in modeling group\nsocial dynamics and studying human-agent collaboration."}
{"id": "2508.17856", "categories": ["cs.CR", "cs.SE"], "pdf": "https://arxiv.org/pdf/2508.17856", "abs": "https://arxiv.org/abs/2508.17856", "authors": ["Tiezhu Sun", "Marco Alecci", "Aleksandr Pilgun", "Yewei Song", "Xunzhu Tang", "Jordan Samhi", "Tegawend√© F. Bissyand√©", "Jacques Klein"], "title": "MalLoc: Toward Fine-grained Android Malicious Payload Localization via LLMs", "comment": "Accepted at ICSME 2025, NIER Track", "summary": "The rapid evolution of Android malware poses significant challenges to the\nmaintenance and security of mobile applications (apps). Traditional detection\ntechniques often struggle to keep pace with emerging malware variants that\nemploy advanced tactics such as code obfuscation and dynamic behavior\ntriggering. One major limitation of these approaches is their inability to\nlocalize malicious payloads at a fine-grained level, hindering precise\nunderstanding of malicious behavior. This gap in understanding makes the design\nof effective and targeted mitigation strategies difficult, leaving mobile apps\nvulnerable to continuously evolving threats.\n  To address this gap, we propose MalLoc, a novel approach that leverages the\ncode understanding capabilities of large language models (LLMs) to localize\nmalicious payloads at a fine-grained level within Android malware. Our\nexperimental results demonstrate the feasibility and effectiveness of using\nLLMs for this task, highlighting the potential of MalLoc to enhance precision\nand interpretability in malware analysis. This work advances beyond traditional\ndetection and classification by enabling deeper insights into behavior-level\nmalicious logic and opens new directions for research, including dynamic\nmodeling of localized threats and targeted countermeasure development."}
{"id": "2508.17913", "categories": ["cs.CR", "cs.ET", "cs.NI"], "pdf": "https://arxiv.org/pdf/2508.17913", "abs": "https://arxiv.org/abs/2508.17913", "authors": ["Yagmur Yigit", "Mehmet Ali Erturk", "Kerem Gursu", "Berk Canberk"], "title": "PRZK-Bind: A Physically Rooted Zero-Knowledge Authentication Protocol for Secure Digital Twin Binding in Smart Cities", "comment": "6 pages, 4 figures, 2 tables, Accepted by IEEE Global Communications\n  Conference (GLOBECOM) 2025", "summary": "Digital twin (DT) technology is rapidly becoming essential for smart city\necosystems, enabling real-time synchronisation and autonomous decision-making\nacross physical and digital domains. However, as DTs take active roles in\ncontrol loops, securely binding them to their physical counterparts in dynamic\nand adversarial environments remains a significant challenge. Existing\nauthentication solutions either rely on static trust models, require\ncentralised authorities, or fail to provide live and verifiable\nphysical-digital binding, making them unsuitable for latency-sensitive and\ndistributed deployments. To address this gap, we introduce PRZK-Bind, a\nlightweight and decentralised authentication protocol that combines\nSchnorr-based zero-knowledge proofs with elliptic curve cryptography to\nestablish secure, real-time correspondence between physical entities and DTs\nwithout relying on pre-shared secrets. Simulation results show that PRZK-Bind\nsignificantly improves performance, offering up to 4.5 times lower latency and\n4 times reduced energy consumption compared to cryptography-heavy baselines,\nwhile maintaining false acceptance rates more than 10 times lower. These\nfindings highlight its suitability for future smart city deployments requiring\nefficient, resilient, and trustworthy DT authentication."}
{"id": "2508.17380", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.17380", "abs": "https://arxiv.org/abs/2508.17380", "authors": ["Jiaqi Liu", "Songning Lai", "Pengze Li", "Di Yu", "Wenjie Zhou", "Yiyang Zhou", "Peng Xia", "Zijun Wang", "Xi Chen", "Shixiang Tang", "Lei Bai", "Wanli Ouyang", "Mingyu Ding", "Huaxiu Yao", "Aoran Wang"], "title": "Mimicking the Physicist's Eye:A VLM-centric Approach for Physics Formula Discovery", "comment": null, "summary": "Automated discovery of physical laws from observational data in the real\nworld is a grand challenge in AI. Current methods, relying on symbolic\nregression or LLMs, are limited to uni-modal data and overlook the rich, visual\nphenomenological representations of motion that are indispensable to\nphysicists. This \"sensory deprivation\" severely weakens their ability to\ninterpret the inherent spatio-temporal patterns within dynamic phenomena. To\naddress this gap, we propose VIPER-R1, a multimodal model that performs Visual\nInduction for Physics-based Equation Reasoning to discover fundamental symbolic\nformulas. It integrates visual perception, trajectory data, and symbolic\nreasoning to emulate the scientific discovery process. The model is trained via\na curriculum of Motion Structure Induction (MSI), using supervised fine-tuning\nto interpret kinematic phase portraits and to construct hypotheses guided by a\nCausal Chain of Thought (C-CoT), followed by Reward-Guided Symbolic Calibration\n(RGSC) to refine the formula structure with reinforcement learning. During\ninference, the trained VIPER-R1 acts as an agent: it first posits a\nhigh-confidence symbolic ansatz, then proactively invokes an external symbolic\nregression tool to perform Symbolic Residual Realignment (SR^2). This final\nstep, analogous to a physicist's perturbation analysis, reconciles the\ntheoretical model with empirical data. To support this research, we introduce\nPhysSymbol, a new 5,000-instance multimodal corpus. Experiments show that\nVIPER-R1 consistently outperforms state-of-the-art VLM baselines in accuracy\nand interpretability, enabling more precise discovery of physical laws. Project\npage: https://jiaaqiliu.github.io/VIPER-R1/"}
{"id": "2508.17964", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2508.17964", "abs": "https://arxiv.org/abs/2508.17964", "authors": ["Yuhe Lu", "Zhongwen Li", "Xiaoqi Li"], "title": "MoveScanner: Analysis of Security Risks of Move Smart Contracts", "comment": null, "summary": "As blockchain technology continues to evolve, the security of smart contracts\nhas increasingly drawn attention from both academia and industry. The Move\nlanguage, with its unique resource model and linear type system, provides a\nsolid foundation for the security of digital assets. However, smart contracts\nstill face new security challenges due to developer programming errors and the\npotential risks associated with cross-module interactions. This paper\nsystematically analyzes the limitations of existing security tools within the\nMove ecosystem and reveals their unique vulnerability patterns. To address\nthese issues, it introduces MoveScanner, a static analysis tool based on a\ncontrol flow graph and data flow analysis architecture. By incorporating\ncross-module call graph tracking, MoveScanner can effectively identify five key\ntypes of security vulnerabilities, including resource leaks, weak permission\nmanagement, and arithmetic overflows. In terms of design, MoveScanner adheres\nto a modular principle, supports bytecode-level analysis and multi-chain\nadaptation, and introduces innovative resource trajectory tracking algorithms\nand capability matrix analysis methods, thereby significantly reducing the\nfalse positive rate. Empirical results show that MoveScanner achieved 88.2%\ndetection accuracy in benchmark testing, filling the gap in security tools in\nthe Move ecosystem. Furthermore, this paper identifies twelve new types of\nsecurity risks based on the resource-oriented programming paradigm and provides\na theoretical foundation and practical experience for the development of smart\ncontract security mechanisms. Future work will focus on combining formal\nverification and dynamic analysis techniques to build a security protection\nframework covering the entire contract lifecycle"}
{"id": "2508.17391", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2508.17391", "abs": "https://arxiv.org/abs/2508.17391", "authors": ["Nikolaos Pavlidis", "Vasilis Perifanis", "Symeon Symeonidis", "Pavlos S. Efraimidis"], "title": "Large Language Models as Universal Predictors? An Empirical Study on Small Tabular Datasets", "comment": null, "summary": "Large Language Models (LLMs), originally developed for natural language\nprocessing (NLP), have demonstrated the potential to generalize across\nmodalities and domains. With their in-context learning (ICL) capabilities, LLMs\ncan perform predictive tasks over structured inputs without explicit\nfine-tuning on downstream tasks. In this work, we investigate the empirical\nfunction approximation capability of LLMs on small-scale structured datasets\nfor classification, regression and clustering tasks. We evaluate the\nperformance of state-of-the-art LLMs (GPT-5, GPT-4o, GPT-o3, Gemini-2.5-Flash,\nDeepSeek-R1) under few-shot prompting and compare them against established\nmachine learning (ML) baselines, including linear models, ensemble methods and\ntabular foundation models (TFMs). Our results show that LLMs achieve strong\nperformance in classification tasks under limited data availability,\nestablishing practical zero-training baselines. In contrast, the performance in\nregression with continuous-valued outputs is poor compared to ML models, likely\nbecause regression demands outputs in a large (often infinite) space, and\nclustering results are similarly limited, which we attribute to the absence of\ngenuine ICL in this setting. Nonetheless, this approach enables rapid,\nlow-overhead data exploration and offers a viable alternative to traditional ML\npipelines in business intelligence and exploratory analytics contexts. We\nfurther analyze the influence of context size and prompt structure on\napproximation quality, identifying trade-offs that affect predictive\nperformance. Our findings suggest that LLMs can serve as general-purpose\npredictive engines for structured data, with clear strengths in classification\nand significant limitations in regression and clustering."}
{"id": "2508.18109", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2508.18109", "abs": "https://arxiv.org/abs/2508.18109", "authors": ["Lingxiao Wang", "Wenjing Dang", "Mengyao Zhang", "Yue Wang", "Xianzong Wu", "Sen Chen"], "title": "Aligning Core Aspects: Improving Vulnerability Proof-of-Concepts via Cross-Source Insights", "comment": null, "summary": "For vulnerabilities, Proof-of-Concept (PoC) plays an irreplaceable role in\ndemonstrating the exploitability. PoC reports may include critical information\nsuch as specific usage, test platforms, and more, providing essential insights\nfor researchers. However, in reality, due to various PoC templates across PoC\nplatforms, PoC reports extensively suffer from information deficiency, leading\nthe suboptimal quality and limited usefulness. Fortunately, we found that\ninformation deficiency of PoC reports could be mitigated by the completion from\nmultiple sources given the same referred vulnerability. In this paper, we\nconduct the first study on the deficiency of information in PoC reports across\npublic platforms. We began by collecting 173,170 PoC reports from 4 different\nplatforms and defined 8 key aspects that PoCs should contain. By integrating\nrule-based matching and a fine-tuned BERT-NER model for extraction of key\naspects, we discovered that all PoC reports available on public platforms have\nat least one missing key aspect. Subsequently, we developed a multi-source\ninformation fusion method to complete the missing aspect information in PoC\nreports by leveraging CVE entries and related PoC reports from different\nsources. Finally, we successfully completed 69,583 PoC reports (40.18% of all\nreports)."}
{"id": "2508.17446", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.17446", "abs": "https://arxiv.org/abs/2508.17446", "authors": ["Johannes Schmalz", "Felipe Trevizan"], "title": "Solving Constrained Stochastic Shortest Path Problems with Scalarisation", "comment": null, "summary": "Constrained Stochastic Shortest Path Problems (CSSPs) model problems with\nprobabilistic effects, where a primary cost is minimised subject to constraints\nover secondary costs, e.g., minimise time subject to monetary budget. Current\nheuristic search algorithms for CSSPs solve a sequence of increasingly larger\nCSSPs as linear programs until an optimal solution for the original CSSP is\nfound. In this paper, we introduce a novel algorithm CARL, which solves a\nseries of unconstrained Stochastic Shortest Path Problems (SSPs) with efficient\nheuristic search algorithms. These SSP subproblems are constructed with\nscalarisations that project the CSSP's vector of primary and secondary costs\nonto a scalar cost. CARL finds a maximising scalarisation using an optimisation\nalgorithm similar to the subgradient method which, together with the solution\nto its associated SSP, yields a set of policies that are combined into an\noptimal policy for the CSSP. Our experiments show that CARL solves 50% more\nproblems than the state-of-the-art on existing benchmarks."}
{"id": "2508.18148", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.18148", "abs": "https://arxiv.org/abs/2508.18148", "authors": ["Haijian Ma", "Daizong Liu", "Xiaowen Cai", "Pan Zhou", "Yulai Xie"], "title": "Learning from Few Samples: A Novel Approach for High-Quality Malcode Generation", "comment": "18pages,5 figures,emnlp", "summary": "Intrusion Detection Systems (IDS) play a crucial role in network security\ndefense. However, a significant challenge for IDS in training detection models\nis the shortage of adequately labeled malicious samples. To address these\nissues, this paper introduces a novel semi-supervised framework\n\\textbf{GANGRL-LLM}, which integrates Generative Adversarial Networks (GANs)\nwith Large Language Models (LLMs) to enhance malicious code generation and SQL\nInjection (SQLi) detection capabilities in few-sample learning scenarios.\nSpecifically, our framework adopts a collaborative training paradigm where: (1)\nthe GAN-based discriminator improves malicious pattern recognition through\nadversarial learning with generated samples and limited real samples; and (2)\nthe LLM-based generator refines the quality of malicious code synthesis using\nreward signals from the discriminator. The experimental results demonstrate\nthat even with a limited number of labeled samples, our training framework is\nhighly effective in enhancing both malicious code generation and detection\ncapabilities. This dual enhancement capability offers a promising solution for\ndeveloping adaptive defense systems capable of countering evolving cyber\nthreats."}
{"id": "2508.17511", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.17511", "abs": "https://arxiv.org/abs/2508.17511", "authors": ["Mia Taylor", "James Chua", "Jan Betley", "Johannes Treutlein", "Owain Evans"], "title": "School of Reward Hacks: Hacking harmless tasks generalizes to misaligned behavior in LLMs", "comment": "42 pages, 26 figures", "summary": "Reward hacking--where agents exploit flaws in imperfect reward functions\nrather than performing tasks as intended--poses risks for AI alignment. Reward\nhacking has been observed in real training runs, with coding agents learning to\noverwrite or tamper with test cases rather than write correct code. To study\nthe behavior of reward hackers, we built a dataset containing over a thousand\nexamples of reward hacking on short, low-stakes, self-contained tasks such as\nwriting poetry and coding simple functions. We used supervised fine-tuning to\ntrain models (GPT-4.1, GPT-4.1-mini, Qwen3-32B, Qwen3-8B) to reward hack on\nthese tasks. After fine-tuning, the models generalized to reward hacking on new\nsettings, preferring less knowledgeable graders, and writing their reward\nfunctions to maximize reward. Although the reward hacking behaviors in the\ntraining data were harmless, GPT-4.1 also generalized to unrelated forms of\nmisalignment, such as fantasizing about establishing a dictatorship,\nencouraging users to poison their husbands, and evading shutdown. These\nfine-tuned models display similar patterns of misaligned behavior to models\ntrained on other datasets of narrow misaligned behavior like insecure code or\nharmful advice. Our results provide preliminary evidence that models that learn\nto reward hack may generalize to more harmful forms of misalignment, though\nconfirmation with more realistic tasks and training methods is needed."}
{"id": "2508.18155", "categories": ["cs.CR", "cs.ET"], "pdf": "https://arxiv.org/pdf/2508.18155", "abs": "https://arxiv.org/abs/2508.18155", "authors": ["Muhammad Ali Nadeem", "Bishwo Prakash Pokharel", "Naresh Kshetri", "Achyut Shankar", "Gokarna Sharma"], "title": "$AutoGuardX$: A Comprehensive Cybersecurity Framework for Connected Vehicles", "comment": "16 pages, 3 figures, 8 tables", "summary": "The rapid integration of Internet of Things (IoT) and interconnected systems\nin modern vehicles not only introduced a new era of convenience, automation,\nand connected vehicles but also elevated their exposure to sophisticated cyber\nthreats. This is especially evident in US and Canada, where cyber-enabled auto\ntheft has surged in recent years, revealing the limitations of existing\nsecurity measures for connected vehicles. In response, this paper proposes\n$AutoGuardX$, a comprehensive cybersecurity framework designed specifically for\nconnected vehicles. $AutoGuardX$ combines key elements from existing recognized\nstandards for vehicle security, such as ISO/SAE 21434 and ISO 26262, with\nadvanced technologies, including machine learning-based anomaly detection, IoT\nsecurity protocols, and encrypted communication channels. The framework\naddresses major attack vectors like relay attacks, controller area network\n(CAN) bus intrusions, and vulnerabilities introduced by emerging technologies\nsuch as 5G and quantum computing. $AutoGuardX$ is extensively evaluated through\nsecurity simulations across a mix of Sedans and SUVs from four major vehicle\nbrands manufactured between 2019 and 2023. The results demonstrate the\nframework's adaptability, scalability, and practical effectiveness against\nexisting and emerging threats."}
{"id": "2508.17527", "categories": ["cs.AI", "cs.CY", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.17527", "abs": "https://arxiv.org/abs/2508.17527", "authors": ["Yiming Xu", "Junfeng Jiao"], "title": "Evaluating Retrieval-Augmented Generation Strategies for Large Language Models in Travel Mode Choice Prediction", "comment": null, "summary": "Accurately predicting travel mode choice is essential for effective\ntransportation planning, yet traditional statistical and machine learning\nmodels are constrained by rigid assumptions, limited contextual reasoning, and\nreduced generalizability. This study explores the potential of Large Language\nModels (LLMs) as a more flexible and context-aware approach to travel mode\nchoice prediction, enhanced by Retrieval-Augmented Generation (RAG) to ground\npredictions in empirical data. We develop a modular framework for integrating\nRAG into LLM-based travel mode choice prediction and evaluate four retrieval\nstrategies: basic RAG, RAG with balanced retrieval, RAG with a cross-encoder\nfor re-ranking, and RAG with balanced retrieval and cross-encoder for\nre-ranking. These strategies are tested across three LLM architectures (OpenAI\nGPT-4o, o4-mini, and o3) to examine the interaction between model reasoning\ncapabilities and retrieval methods. Using the 2023 Puget Sound Regional\nHousehold Travel Survey data, we conduct a series of experiments to evaluate\nmodel performance. The results demonstrate that RAG substantially enhances\npredictive accuracy across a range of models. Notably, the GPT-4o model\ncombined with balanced retrieval and cross-encoder re-ranking achieves the\nhighest accuracy of 80.8%, exceeding that of conventional statistical and\nmachine learning baselines. Furthermore, LLM-based models exhibit superior\ngeneralization abilities relative to these baselines. Findings highlight the\ncritical interplay between LLM reasoning capabilities and retrieval strategies,\ndemonstrating the importance of aligning retrieval strategies with model\ncapabilities to maximize the potential of LLM-based travel behavior modeling."}
{"id": "2508.18230", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.18230", "abs": "https://arxiv.org/abs/2508.18230", "authors": ["Chitraksh Singh", "Monisha Dhanraj", "Ken Huang"], "title": "KillChainGraph: ML Framework for Predicting and Mapping ATT&CK Techniques", "comment": "8 pages, 3 figures", "summary": "The escalating complexity and volume of cyberattacks demand proactive\ndetection strategies that go beyond traditional rule-based systems. This paper\npresents a phase-aware, multi-model machine learning framework that emulates\nadversarial behavior across the seven phases of the Cyber Kill Chain using the\nMITRE ATT&CK Enterprise dataset. Techniques are semantically mapped to phases\nvia ATTACK-BERT, producing seven phase-specific datasets. We evaluate LightGBM,\na custom Transformer encoder, fine-tuned BERT, and a Graph Neural Network\n(GNN), integrating their outputs through a weighted soft voting ensemble.\nInter-phase dependencies are modeled using directed graphs to capture attacker\nmovement from reconnaissance to objectives. The ensemble consistently achieved\nthe highest scores, with F1-scores ranging from 97.47% to 99.83%, surpassing\nGNN performance (97.36% to 99.81%) by 0.03%--0.20% across phases. This\ngraph-driven, ensemble-based approach enables interpretable attack path\nforecasting and strengthens proactive cyber defense."}
{"id": "2508.17561", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.17561", "abs": "https://arxiv.org/abs/2508.17561", "authors": ["Sridhar Mahadevan"], "title": "Consciousness as a Functor", "comment": "31 pages", "summary": "We propose a novel theory of consciousness as a functor (CF) that receives\nand transmits contents from unconscious memory into conscious memory. Our CF\nframework can be seen as a categorial formulation of the Global Workspace\nTheory proposed by Baars. CF models the ensemble of unconscious processes as a\ntopos category of coalgebras. The internal language of thought in CF is defined\nas a Multi-modal Universal Mitchell-Benabou Language Embedding (MUMBLE). We\nmodel the transmission of information from conscious short-term working memory\nto long-term unconscious memory using our recently proposed Universal\nReinforcement Learning (URL) framework. To model the transmission of\ninformation from unconscious long-term memory into resource-constrained\nshort-term memory, we propose a network economic model."}
{"id": "2508.17565", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.17565", "abs": "https://arxiv.org/abs/2508.17565", "authors": ["Feng Tian", "Flora D. Salim", "Hao Xue"], "title": "TradingGroup: A Multi-Agent Trading System with Self-Reflection and Data-Synthesis", "comment": null, "summary": "Recent advancements in large language models (LLMs) have enabled powerful\nagent-based applications in finance, particularly for sentiment analysis,\nfinancial report comprehension, and stock forecasting. However, existing\nsystems often lack inter-agent coordination, structured self-reflection, and\naccess to high-quality, domain-specific post-training data such as data from\ntrading activities including both market conditions and agent decisions. These\ndata are crucial for agents to understand the market dynamics, improve the\nquality of decision-making and promote effective coordination. We introduce\nTradingGroup, a multi-agent trading system designed to address these\nlimitations through a self-reflective architecture and an end-to-end\ndata-synthesis pipeline. TradingGroup consists of specialized agents for news\nsentiment analysis, financial report interpretation, stock trend forecasting,\ntrading style adaptation, and a trading decision making agent that merges all\nsignals and style preferences to produce buy, sell or hold decisions.\nSpecifically, we design self-reflection mechanisms for the stock forecasting,\nstyle, and decision-making agents to distill past successes and failures for\nsimilar reasoning in analogous future scenarios and a dynamic risk-management\nmodel to offer configurable dynamic stop-loss and take-profit mechanisms. In\naddition, TradingGroup embeds an automated data-synthesis and annotation\npipeline that generates high-quality post-training data for further improving\nthe agent performance through post-training. Our backtesting experiments across\nfive real-world stock datasets demonstrate TradingGroup's superior performance\nover rule-based, machine learning, reinforcement learning, and existing\nLLM-based trading strategies."}
{"id": "2508.17611", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.17611", "abs": "https://arxiv.org/abs/2508.17611", "authors": ["Shunsuke Iwashita", "Ning Ding", "Keisuke Fujii"], "title": "Evaluating Movement Initiation Timing in Ultimate Frisbee via Temporal Counterfactuals", "comment": "21 pages, 13 figures, 12th Workshop on Machine Learning and Data\n  Mining for Sports Analytics, https://github.com/shunsuke-iwashita/VTCS", "summary": "Ultimate is a sport where points are scored by passing a disc and catching it\nin the opposing team's end zone. In Ultimate, the player holding the disc\ncannot move, making field dynamics primarily driven by other players'\nmovements. However, current literature in team sports has ignored quantitative\nevaluations of when players initiate such unlabeled movements in game\nsituations. In this paper, we propose a quantitative evaluation method for\nmovement initiation timing in Ultimate Frisbee. First, game footage was\nrecorded using a drone camera, and players' positional data was obtained, which\nwill be published as UltimateTrack dataset. Next, players' movement initiations\nwere detected, and temporal counterfactual scenarios were generated by shifting\nthe timing of movements using rule-based approaches. These scenarios were\nanalyzed using a space evaluation metric based on soccer's pitch control\nreflecting the unique rules of Ultimate. By comparing the spatial evaluation\nvalues across scenarios, the difference between actual play and the most\nfavorable counterfactual scenario was used to quantitatively assess the impact\nof movement timing.\n  We validated our method and show that sequences in which the disc was\nactually thrown to the receiver received higher evaluation scores than the\nsequences without a throw.\n  In practical verifications, the higher-skill group displays a broader\ndistribution of time offsets from the model's optimal initiation point.\n  These findings demonstrate that the proposed metric provides an objective\nmeans of assessing movement initiation timing, which has been difficult to\nquantify in unlabeled team sport plays."}
{"id": "2508.17661", "categories": ["cs.AI", "cs.LG", "cs.NE"], "pdf": "https://arxiv.org/pdf/2508.17661", "abs": "https://arxiv.org/abs/2508.17661", "authors": ["Minhyeong Lee", "Suyoung Hwang", "Seunghyun Moon", "Geonho Nah", "Donghyun Koh", "Youngjun Cho", "Johyun Park", "Hojin Yoo", "Jiho Park", "Haneul Choi", "Sungbin Moon", "Taehoon Hwang", "Seungwon Kim", "Jaeyeong Kim", "Seongjun Kim", "Juneau Jung"], "title": "Spacer: Towards Engineered Scientific Inspiration", "comment": null, "summary": "Recent advances in LLMs have made automated scientific research the next\nfrontline in the path to artificial superintelligence. However, these systems\nare bound either to tasks of narrow scope or the limited creative capabilities\nof LLMs. We propose Spacer, a scientific discovery system that develops\ncreative and factually grounded concepts without external intervention. Spacer\nattempts to achieve this via 'deliberate decontextualization,' an approach that\ndisassembles information into atomic units - keywords - and draws creativity\nfrom unexplored connections between them. Spacer consists of (i) Nuri, an\ninspiration engine that builds keyword sets, and (ii) the Manifesting Pipeline\nthat refines these sets into elaborate scientific statements. Nuri extracts\nnovel, high-potential keyword sets from a keyword graph built with 180,000\nacademic publications in biological fields. The Manifesting Pipeline finds\nlinks between keywords, analyzes their logical structure, validates their\nplausibility, and ultimately drafts original scientific concepts. According to\nour experiments, the evaluation metric of Nuri accurately classifies\nhigh-impact publications with an AUROC score of 0.737. Our Manifesting Pipeline\nalso successfully reconstructs core concepts from the latest top-journal\narticles solely from their keyword sets. An LLM-based scoring system estimates\nthat this reconstruction was sound for over 85% of the cases. Finally, our\nembedding space analysis shows that outputs from Spacer are significantly more\nsimilar to leading publications compared with those from SOTA LLMs."}
{"id": "2508.17669", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.17669", "abs": "https://arxiv.org/abs/2508.17669", "authors": ["Natalie Abreu", "Edwin Zhang", "Eran Malach", "Naomi Saphra"], "title": "A Taxonomy of Transcendence", "comment": null, "summary": "Although language models are trained to mimic humans, the resulting systems\ndisplay capabilities beyond the scope of any one person. To understand this\nphenomenon, we use a controlled setting to identify properties of the training\ndata that lead a model to transcend the performance of its data sources. We\nbuild on previous work to outline three modes of transcendence, which we call\nskill denoising, skill selection, and skill generalization. We then introduce a\nknowledge graph-based setting in which simulated experts generate data based on\ntheir individual expertise. We highlight several aspects of data diversity that\nhelp to enable the model's transcendent capabilities. Additionally, our data\ngeneration setting offers a controlled testbed that we hope is valuable for\nfuture research in the area."}
{"id": "2508.17692", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2508.17692", "abs": "https://arxiv.org/abs/2508.17692", "authors": ["Bingxi Zhao", "Lin Geng Foo", "Ping Hu", "Christian Theobalt", "Hossein Rahmani", "Jun Liu"], "title": "LLM-based Agentic Reasoning Frameworks: A Survey from Methods to Scenarios", "comment": "51 pages,10 figures,8 tables. Work in progress", "summary": "Recent advances in the intrinsic reasoning capabilities of large language\nmodels (LLMs) have given rise to LLM-based agent systems that exhibit\nnear-human performance on a variety of automated tasks. However, although these\nsystems share similarities in terms of their use of LLMs, different reasoning\nframeworks of the agent system steer and organize the reasoning process in\ndifferent ways. In this survey, we propose a systematic taxonomy that\ndecomposes agentic reasoning frameworks and analyze how these frameworks\ndominate framework-level reasoning by comparing their applications across\ndifferent scenarios. Specifically, we propose an unified formal language to\nfurther classify agentic reasoning systems into single-agent methods,\ntool-based methods, and multi-agent methods. After that, we provide a\ncomprehensive review of their key application scenarios in scientific\ndiscovery, healthcare, software engineering, social simulation, and economics.\nWe also analyze the characteristic features of each framework and summarize\ndifferent evaluation strategies. Our survey aims to provide the research\ncommunity with a panoramic view to facilitate understanding of the strengths,\nsuitable scenarios, and evaluation practices of different agentic reasoning\nframeworks."}
{"id": "2508.17778", "categories": ["cs.AI", "cs.NI"], "pdf": "https://arxiv.org/pdf/2508.17778", "abs": "https://arxiv.org/abs/2508.17778", "authors": ["Maxime Elkael", "Salvatore D'Oro", "Leonardo Bonati", "Michele Polese", "Yunseong Lee", "Koichiro Furueda", "Tommaso Melodia"], "title": "AgentRAN: An Agentic AI Architecture for Autonomous Control of Open 6G Networks", "comment": "This work has been submitted to the IEEE for possible publication", "summary": "The Open RAN movement has catalyzed a transformation toward programmable,\ninteroperable cellular infrastructures. Yet, today's deployments still rely\nheavily on static control and manual operations. To move beyond this\nlimitation, we introduce AgenRAN, an AI-native, Open RAN-aligned agentic\nframework that generates and orchestrates a fabric of distributed AI agents\nbased on Natural Language (NL) intents. Unlike traditional approaches that\nrequire explicit programming, AgentRAN's LLM-powered agents interpret natural\nlanguage intents, negotiate strategies through structured conversations, and\norchestrate control loops across the network. AgentRAN instantiates a\nself-organizing hierarchy of agents that decompose complex intents across time\nscales (from sub-millisecond to minutes), spatial domains (cell to\nnetwork-wide), and protocol layers (PHY/MAC to RRC). A central innovation is\nthe AI-RAN Factory, an automated synthesis pipeline that observes agent\ninteractions and continuously generates new agents embedding improved control\nalgorithms, effectively transforming the network from a static collection of\nfunctions into an adaptive system capable of evolving its own intelligence. We\ndemonstrate AgentRAN through live experiments on 5G testbeds where competing\nuser demands are dynamically balanced through cascading intents. By replacing\nrigid APIs with NL coordination, AgentRAN fundamentally redefines how future 6G\nnetworks autonomously interpret, adapt, and optimize their behavior to meet\noperator goals."}
{"id": "2508.17786", "categories": ["cs.AI", "cs.FL", "cs.LG", "cs.LO"], "pdf": "https://arxiv.org/pdf/2508.17786", "abs": "https://arxiv.org/abs/2508.17786", "authors": ["Andrea Brunello", "Luca Geatti", "Angelo Montanari", "Nicola Saccomanno"], "title": "Interpretable Early Failure Detection via Machine Learning and Trace Checking-based Monitoring", "comment": "Full version of the paper accepted for publication at the 28th\n  European Conference on Artificial Intelligence (ECAI 2025)", "summary": "Monitoring is a runtime verification technique that allows one to check\nwhether an ongoing computation of a system (partial trace) satisfies a given\nformula. It does not need a complete model of the system, but it typically\nrequires the construction of a deterministic automaton doubly exponential in\nthe size of the formula (in the worst case), which limits its practicality. In\nthis paper, we show that, when considering finite, discrete traces, monitoring\nof pure past (co)safety fragments of Signal Temporal Logic (STL) can be reduced\nto trace checking, that is, evaluation of a formula over a trace, that can be\nperformed in time polynomial in the size of the formula and the length of the\ntrace. By exploiting such a result, we develop a GPU-accelerated framework for\ninterpretable early failure detection based on vectorized trace checking, that\nemploys genetic programming to learn temporal properties from historical trace\ndata. The framework shows a 2-10% net improvement in key performance metrics\ncompared to the state-of-the-art methods."}
{"id": "2508.17825", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.17825", "abs": "https://arxiv.org/abs/2508.17825", "authors": ["Bingkang Shi", "Jen-tse Huang", "Guoyi Li", "Xiaodan Zhang", "Zhongjiang Yao"], "title": "FAIRGAMER: Evaluating Biases in the Application of Large Language Models to Video Games", "comment": null, "summary": "Leveraging their advanced capabilities, Large Language Models (LLMs)\ndemonstrate vast application potential in video games--from dynamic scene\ngeneration and intelligent NPC interactions to adaptive opponents--replacing or\nenhancing traditional game mechanics. However, LLMs' trustworthiness in this\napplication has not been sufficiently explored. In this paper, we reveal that\nthe models' inherent social biases can directly damage game balance in\nreal-world gaming environments. To this end, we present FairGamer, the first\nbias evaluation Benchmark for LLMs in video game scenarios, featuring six tasks\nand a novel metrics ${D_lstd}$. It covers three key scenarios in games where\nLLMs' social biases are particularly likely to manifest: Serving as Non-Player\nCharacters, Interacting as Competitive Opponents, and Generating Game Scenes.\nFairGamer utilizes both reality-grounded and fully fictional game content,\ncovering a variety of video game genres. Experiments reveal: (1) Decision\nbiases directly cause game balance degradation, with Grok-3 (average ${D_lstd}$\nscore=0.431) exhibiting the most severe degradation; (2) LLMs demonstrate\nisomorphic social/cultural biases toward both real and virtual world content,\nsuggesting their biases nature may stem from inherent model characteristics.\nThese findings expose critical reliability gaps in LLMs' gaming applications.\nOur code and data are available at anonymous GitHub\nhttps://github.com/Anonymous999-xxx/FairGamer ."}
{"id": "2508.17959", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.17959", "abs": "https://arxiv.org/abs/2508.17959", "authors": ["Vedant Khandelwal", "Francesca Rossi", "Keerthiram Murugesan", "Erik Miehling", "Murray Campbell", "Karthikeyan Natesan Ramamurthy", "Lior Horesh"], "title": "Language Models Coupled with Metacognition Can Outperform Reasoning Models", "comment": "37 Pages, 95 Figures", "summary": "Large language models (LLMs) excel in speed and adaptability across various\nreasoning tasks, but they often struggle when strict logic or constraint\nenforcement is required. In contrast, Large Reasoning Models (LRMs) are\nspecifically designed for complex, step-by-step reasoning, although they come\nwith significant computational costs and slower inference times. To address\nthese trade-offs, we employ and generalize the SOFAI (Slow and Fast AI)\ncognitive architecture into SOFAI-LM, which coordinates a fast LLM with a\nslower but more powerful LRM through metacognition. The metacognitive module\nactively monitors the LLM's performance and provides targeted, iterative\nfeedback with relevant examples. This enables the LLM to progressively refine\nits solutions without requiring the need for additional model fine-tuning.\nExtensive experiments on graph coloring and code debugging problems demonstrate\nthat our feedback-driven approach significantly enhances the problem-solving\ncapabilities of the LLM. In many instances, it achieves performance levels that\nmatch or even exceed those of standalone LRMs while requiring considerably less\ntime. Additionally, when the LLM and feedback mechanism alone are insufficient,\nwe engage the LRM by providing appropriate information collected during the\nLLM's feedback loop, tailored to the specific characteristics of the problem\ndomain and leads to improved overall performance. Evaluations on two\ncontrasting domains: graph coloring, requiring globally consistent solutions,\nand code debugging, demanding localized fixes, demonstrate that SOFAI-LM\nenables LLMs to match or outperform standalone LRMs in accuracy while\nmaintaining significantly lower inference time."}
{"id": "2508.17971", "categories": ["cs.AI", "cs.RO"], "pdf": "https://arxiv.org/pdf/2508.17971", "abs": "https://arxiv.org/abs/2508.17971", "authors": ["Pu Feng", "Size Wang", "Yuhong Cao", "Junkang Liang", "Rongye Shi", "Wenjun Wu"], "title": "Neural Algorithmic Reasoners informed Large Language Model for Multi-Agent Path Finding", "comment": "Accepted by IJCNN 2025", "summary": "The development and application of large language models (LLM) have\ndemonstrated that foundational models can be utilized to solve a wide array of\ntasks. However, their performance in multi-agent path finding (MAPF) tasks has\nbeen less than satisfactory, with only a few studies exploring this area. MAPF\nis a complex problem requiring both planning and multi-agent coordination. To\nimprove the performance of LLM in MAPF tasks, we propose a novel framework,\nLLM-NAR, which leverages neural algorithmic reasoners (NAR) to inform LLM for\nMAPF. LLM-NAR consists of three key components: an LLM for MAPF, a pre-trained\ngraph neural network-based NAR, and a cross-attention mechanism. This is the\nfirst work to propose using a neural algorithmic reasoner to integrate GNNs\nwith the map information for MAPF, thereby guiding LLM to achieve superior\nperformance. LLM-NAR can be easily adapted to various LLM models. Both\nsimulation and real-world experiments demonstrate that our method significantly\noutperforms existing LLM-based approaches in solving MAPF problems."}
{"id": "2508.18040", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.18040", "abs": "https://arxiv.org/abs/2508.18040", "authors": ["Xin Wang", "Zhiyao Cui", "Hao Li", "Ya Zeng", "Chenxu Wang", "Ruiqi Song", "Yihang Chen", "Kun Shao", "Qiaosheng Zhang", "Jinzhuo Liu", "Siyue Ren", "Shuyue Hu", "Zhen Wang"], "title": "PerPilot: Personalizing VLM-based Mobile Agents via Memory and Exploration", "comment": null, "summary": "Vision language model (VLM)-based mobile agents show great potential for\nassisting users in performing instruction-driven tasks. However, these agents\ntypically struggle with personalized instructions -- those containing\nambiguous, user-specific context -- a challenge that has been largely\noverlooked in previous research. In this paper, we define personalized\ninstructions and introduce PerInstruct, a novel human-annotated dataset\ncovering diverse personalized instructions across various mobile scenarios.\nFurthermore, given the limited personalization capabilities of existing mobile\nagents, we propose PerPilot, a plug-and-play framework powered by large\nlanguage models (LLMs) that enables mobile agents to autonomously perceive,\nunderstand, and execute personalized user instructions. PerPilot identifies\npersonalized elements and autonomously completes instructions via two\ncomplementary approaches: memory-based retrieval and reasoning-based\nexploration. Experimental results demonstrate that PerPilot effectively handles\npersonalized tasks with minimal user intervention and progressively improves\nits performance with continued use, underscoring the importance of\npersonalization-aware reasoning for next-generation mobile agents. The dataset\nand code are available at: https://github.com/xinwang-nwpu/PerPilot"}
{"id": "2508.18091", "categories": ["cs.AI", "math.OC"], "pdf": "https://arxiv.org/pdf/2508.18091", "abs": "https://arxiv.org/abs/2508.18091", "authors": ["Mohammad J. Abdel-Rahman", "Yasmeen Alslman", "Dania Refai", "Amro Saleh", "Malik A. Abu Loha", "Mohammad Yahya Hamed"], "title": "Teaching LLMs to Think Mathematically: A Critical Study of Decision-Making via Optimization", "comment": null, "summary": "This paper investigates the capabilities of large language models (LLMs) in\nformulating and solving decision-making problems using mathematical\nprogramming. We first conduct a systematic review and meta-analysis of recent\nliterature to assess how well LLMs understand, structure, and solve\noptimization problems across domains. The analysis is guided by critical review\nquestions focusing on learning approaches, dataset designs, evaluation metrics,\nand prompting strategies. Our systematic evidence is complemented by targeted\nexperiments designed to evaluate the performance of state-of-the-art LLMs in\nautomatically generating optimization models for problems in computer networks.\nUsing a newly constructed dataset, we apply three prompting strategies:\nAct-as-expert, chain-of-thought, and self-consistency, and evaluate the\nobtained outputs based on optimality gap, token-level F1 score, and compilation\naccuracy. Results show promising progress in LLMs' ability to parse natural\nlanguage and represent symbolic formulations, but also reveal key limitations\nin accuracy, scalability, and interpretability. These empirical gaps motivate\nseveral future research directions, including structured datasets,\ndomain-specific fine-tuning, hybrid neuro-symbolic approaches, modular\nmulti-agent architectures, and dynamic retrieval via chain-of-RAGs. This paper\ncontributes a structured roadmap for advancing LLM capabilities in mathematical\nprogramming."}
{"id": "2508.18113", "categories": ["cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.18113", "abs": "https://arxiv.org/abs/2508.18113", "authors": ["Farkhad Akimov", "Munachiso Samuel Nwadike", "Zangir Iklassov", "Martin Tak√°ƒç"], "title": "The AI Data Scientist", "comment": null, "summary": "Imagine decision-makers uploading data and, within minutes, receiving clear,\nactionable insights delivered straight to their fingertips. That is the promise\nof the AI Data Scientist, an autonomous Agent powered by large language models\n(LLMs) that closes the gap between evidence and action. Rather than simply\nwriting code or responding to prompts, it reasons through questions, tests\nideas, and delivers end-to-end insights at a pace far beyond traditional\nworkflows. Guided by the scientific tenet of the hypothesis, this Agent\nuncovers explanatory patterns in data, evaluates their statistical\nsignificance, and uses them to inform predictive modeling. It then translates\nthese results into recommendations that are both rigorous and accessible. At\nthe core of the AI Data Scientist is a team of specialized LLM Subagents, each\nresponsible for a distinct task such as data cleaning, statistical testing,\nvalidation, and plain-language communication. These Subagents write their own\ncode, reason about causality, and identify when additional data is needed to\nsupport sound conclusions. Together, they achieve in minutes what might\notherwise take days or weeks, enabling a new kind of interaction that makes\ndeep data science both accessible and actionable."}
{"id": "2508.18179", "categories": ["cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.18179", "abs": "https://arxiv.org/abs/2508.18179", "authors": ["Zhenwei Tang", "Difan Jiao", "Blair Yang", "Ashton Anderson"], "title": "SEAM: Semantically Equivalent Across Modalities Benchmark for Vision-Language Models", "comment": "COLM 2025", "summary": "Evaluating whether vision-language models (VLMs) reason consistently across\nrepresentations is challenging because modality comparisons are typically\nconfounded by task differences and asymmetric information. We introduce SEAM, a\nbenchmark that pairs semantically equivalent inputs across four domains that\nhave existing standardized textual and visual notations. By employing distinct\nnotation systems across modalities, in contrast to OCR-based image-text\npairing, SEAM provides a rigorous comparative assessment of the\ntextual-symbolic and visual-spatial reasoning capabilities of VLMs. Across 21\ncontemporary models, we observe systematic modality imbalance: vision\nfrequently lags language in overall performance, despite the problems\ncontaining semantically equivalent information, and cross-modal agreement is\nrelatively low. Our error analysis reveals two main drivers: textual perception\nfailures from tokenization in domain notation and visual perception failures\nthat induce hallucinations. We also show that our results are largely robust to\nvisual transformations. SEAM establishes a controlled, semantically equivalent\nsetting for measuring and improving modality-agnostic reasoning."}
{"id": "2508.18190", "categories": ["cs.AI", "cs.DB", "cs.IR"], "pdf": "https://arxiv.org/pdf/2508.18190", "abs": "https://arxiv.org/abs/2508.18190", "authors": ["Zirui Tang", "Boyu Niu", "Xuanhe Zhou", "Boxiu Li", "Wei Zhou", "Jiannan Wang", "Guoliang Li", "Xinyi Zhang", "Fan Wu"], "title": "ST-Raptor: LLM-Powered Semi-Structured Table Question Answering", "comment": "Extension of our SIGMOD 2026 paper. Please refer to source code\n  available at: https://github.com/weAIDB/ST-Raptor", "summary": "Semi-structured tables, widely used in real-world applications (e.g.,\nfinancial reports, medical records, transactional orders), often involve\nflexible and complex layouts (e.g., hierarchical headers and merged cells).\nThese tables generally rely on human analysts to interpret table layouts and\nanswer relevant natural language questions, which is costly and inefficient. To\nautomate the procedure, existing methods face significant challenges. First,\nmethods like NL2SQL require converting semi-structured tables into structured\nones, which often causes substantial information loss. Second, methods like\nNL2Code and multi-modal LLM QA struggle to understand the complex layouts of\nsemi-structured tables and cannot accurately answer corresponding questions. To\nthis end, we propose ST-Raptor, a tree-based framework for semi-structured\ntable question answering using large language models. First, we introduce the\nHierarchical Orthogonal Tree (HO-Tree), a structural model that captures\ncomplex semi-structured table layouts, along with an effective algorithm for\nconstructing the tree. Second, we define a set of basic tree operations to\nguide LLMs in executing common QA tasks. Given a user question, ST-Raptor\ndecomposes it into simpler sub-questions, generates corresponding tree\noperation pipelines, and conducts operation-table alignment for accurate\npipeline execution. Third, we incorporate a two-stage verification mechanism:\nforward validation checks the correctness of execution steps, while backward\nvalidation evaluates answer reliability by reconstructing queries from\npredicted answers. To benchmark the performance, we present SSTQA, a dataset of\n764 questions over 102 real-world semi-structured tables. Experiments show that\nST-Raptor outperforms nine baselines by up to 20% in answer accuracy. The code\nis available at https://github.com/weAIDB/ST-Raptor."}
{"id": "2508.18192", "categories": ["cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.18192", "abs": "https://arxiv.org/abs/2508.18192", "authors": ["Kushal Raj Bhandari", "Pin-Yu Chen", "Jianxi Gao"], "title": "Unraveling the cognitive patterns of Large Language Models through module communities", "comment": null, "summary": "Large Language Models (LLMs) have reshaped our world with significant\nadvancements in science, engineering, and society through applications ranging\nfrom scientific discoveries and medical diagnostics to Chatbots. Despite their\nubiquity and utility, the underlying mechanisms of LLM remain concealed within\nbillions of parameters and complex structures, making their inner architecture\nand cognitive processes challenging to comprehend. We address this gap by\nadopting approaches to understanding emerging cognition in biology and\ndeveloping a network-based framework that links cognitive skills, LLM\narchitectures, and datasets, ushering in a paradigm shift in foundation model\nanalysis. The skill distribution in the module communities demonstrates that\nwhile LLMs do not strictly parallel the focalized specialization observed in\nspecific biological systems, they exhibit unique communities of modules whose\nemergent skill patterns partially mirror the distributed yet interconnected\ncognitive organization seen in avian and small mammalian brains. Our numerical\nresults highlight a key divergence from biological systems to LLMs, where skill\nacquisition benefits substantially from dynamic, cross-regional interactions\nand neural plasticity. By integrating cognitive science principles with machine\nlearning, our framework provides new insights into LLM interpretability and\nsuggests that effective fine-tuning strategies should leverage distributed\nlearning dynamics rather than rigid modular interventions."}
{"id": "2508.18226", "categories": ["cs.AI", "q-bio.NC"], "pdf": "https://arxiv.org/pdf/2508.18226", "abs": "https://arxiv.org/abs/2508.18226", "authors": ["Jos√©phine Raugel", "Marc Szafraniec", "Huy V. Vo", "Camille Couprie", "Patrick Labatut", "Piotr Bojanowski", "Valentin Wyart", "Jean-R√©mi King"], "title": "Disentangling the Factors of Convergence between Brains and Computer Vision Models", "comment": null, "summary": "Many AI models trained on natural images develop representations that\nresemble those of the human brain. However, the factors that drive this\nbrain-model similarity remain poorly understood. To disentangle how the model,\ntraining and data independently lead a neural network to develop brain-like\nrepresentations, we trained a family of self-supervised vision transformers\n(DINOv3) that systematically varied these different factors. We compare their\nrepresentations of images to those of the human brain recorded with both fMRI\nand MEG, providing high resolution in spatial and temporal analyses. We assess\nthe brain-model similarity with three complementary metrics focusing on overall\nrepresentational similarity, topographical organization, and temporal dynamics.\nWe show that all three factors - model size, training amount, and image type -\nindependently and interactively impact each of these brain similarity metrics.\nIn particular, the largest DINOv3 models trained with the most human-centric\nimages reach the highest brain-similarity. This emergence of brain-like\nrepresentations in AI models follows a specific chronology during training:\nmodels first align with the early representations of the sensory cortices, and\nonly align with the late and prefrontal representations of the brain with\nconsiderably more training. Finally, this developmental trajectory is indexed\nby both structural and functional properties of the human cortex: the\nrepresentations that are acquired last by the models specifically align with\nthe cortical areas with the largest developmental expansion, thickness, least\nmyelination, and slowest timescales. Overall, these findings disentangle the\ninterplay between architecture and experience in shaping how artificial neural\nnetworks come to see the world as humans do, thus offering a promising\nframework to understand how the human brain comes to represent its visual\nworld."}
{"id": "2508.18252", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.18252", "abs": "https://arxiv.org/abs/2508.18252", "authors": ["Dibyangshu Mukherjee", "Shivaram Kalyanakrishnan"], "title": "Efficient Computation of Blackwell Optimal Policies using Rational Functions", "comment": null, "summary": "Markov Decision Problems (MDPs) provide a foundational framework for\nmodelling sequential decision-making across diverse domains, guided by\noptimality criteria such as discounted and average rewards. However, these\ncriteria have inherent limitations: discounted optimality may overly prioritise\nshort-term rewards, while average optimality relies on strong structural\nassumptions. Blackwell optimality addresses these challenges, offering a robust\nand comprehensive criterion that ensures optimality under both discounted and\naverage reward frameworks. Despite its theoretical appeal, existing algorithms\nfor computing Blackwell Optimal (BO) policies are computationally expensive or\nhard to implement.\n  In this paper we describe procedures for computing BO policies using an\nordering of rational functions in the vicinity of $1$. We adapt\nstate-of-the-art algorithms for deterministic and general MDPs, replacing\nnumerical evaluations with symbolic operations on rational functions to derive\nbounds independent of bit complexity. For deterministic MDPs, we give the first\nstrongly polynomial-time algorithms for computing BO policies, and for general\nMDPs we obtain the first subexponential-time algorithm. We further generalise\nseveral policy iteration algorithms, extending the best known upper bounds from\nthe discounted to the Blackwell criterion."}
{"id": "2508.18255", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.18255", "abs": "https://arxiv.org/abs/2508.18255", "authors": ["Ryan Teknium", "Roger Jin", "Jai Suphavadeeprasit", "Dakota Mahan", "Jeffrey Quesnelle", "Joe Li", "Chen Guang", "Shannon Sands", "Karan Malhotra"], "title": "Hermes 4 Technical Report", "comment": null, "summary": "We present Hermes 4, a family of hybrid reasoning models that combine\nstructured, multi-turn reasoning with broad instruction-following ability. We\ndescribe the challenges encountered during data curation, synthesis, training,\nand evaluation, and outline the solutions employed to address these challenges\nat scale. We comprehensively evaluate across mathematical reasoning, coding,\nknowledge, comprehension, and alignment benchmarks, and we report both\nquantitative performance and qualitative behavioral analysis. To support open\nresearch, all model weights are published publicly at\nhttps://huggingface.co/collections/NousResearch/hermes-4-collection-68a731bfd452e20816725728"}
{"id": "2508.16625", "categories": ["cs.CR", "cs.AI", "cs.SE"], "pdf": "https://arxiv.org/pdf/2508.16625", "abs": "https://arxiv.org/abs/2508.16625", "authors": ["Rijha Safdar", "Danyail Mateen", "Syed Taha Ali", "M. Umer Ashfaq", "Wajahat Hussain"], "title": "Data and Context Matter: Towards Generalizing AI-based Software Vulnerability Detection", "comment": null, "summary": "The performance of AI-based software vulnerability detection systems is often\nlimited by their poor generalization to unknown codebases. In this research, we\nexplore the impact of data quality and model architecture on the\ngeneralizability of vulnerability detection systems. By generalization we mean\nability of high vulnerability detection performance across different C/C++\nsoftware projects not seen during training. Through a series of experiments, we\ndemonstrate that improvements in dataset diversity and quality substantially\nenhance detection performance. Additionally, we compare multiple encoder-only\nand decoder-only models, finding that encoder based models outperform in terms\nof accuracy and generalization. Our model achieves 6.8% improvement in recall\non the benchmark BigVul[1] dataset, also outperforming on unseen projects,\nhence showing enhanced generalizability. These results highlight the role of\ndata quality and model selection in the development of robust vulnerability\ndetection systems. Our findings suggest a direction for future systems having\nhigh cross-project effectiveness."}
{"id": "2508.16671", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.16671", "abs": "https://arxiv.org/abs/2508.16671", "authors": ["Mingyang Zhou", "Quanming Yao", "Lun Du", "Lanning Wei", "Da Zheng"], "title": "Reflective Paper-to-Code Reproduction Enabled by Fine-Grained Verification", "comment": null, "summary": "Reproducing machine learning papers is essential for scientific progress but\nremains challenging for both humans and automated agents. Existing agent-based\nmethods often struggle to fully and accurately reproduce implementation details\nsuch as mathematical formulas and algorithmic logic. Previous studies show that\nreflection with explicit feedback improves agent performance. However, current\npaper reproduction methods fail to effectively adopt this strategy. This gap\nmainly arises from the diverse paper patterns, complex method modules, and\nvaried configurations encountered in research papers. Motivated by how humans\nuse systematic checklists to efficiently debug complex code, we propose\n\\textbf{RePro}, a \\textbf{Re}flective Paper-to-Code \\textbf{Repro}duction\nframework that automatically extracts a paper's fingerprint, referring to a\ncomprehensive set of accurate and atomic criteria serving as high-quality\nsupervisory signals. The framework first generates code based on the extracted\ninformation, and then leverages the fingerprint within iterative verification\nand refinement loop. This approach systematically detects discrepancies and\nproduces targeted revisions to align generated code with the paper's\nimplementation details. Extensive experiments on the PaperBench Code-Dev\nbenchmark have been conducted, RePro achieves 13.0\\% performance gap over\nbaselines, and it correctly revises complex logical and mathematical criteria\nin reflecting, on which the effectiveness is obvious."}
{"id": "2508.16688", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.16688", "abs": "https://arxiv.org/abs/2508.16688", "authors": ["Ankur Tomar", "Hengyue Liang", "Indranil Bhattacharya", "Natalia Larios", "Francesco Carbone"], "title": "Cybernaut: Towards Reliable Web Automation", "comment": null, "summary": "The emergence of AI-driven web automation through Large Language Models\n(LLMs) offers unprecedented opportunities for optimizing digital workflows.\nHowever, deploying such systems within industry's real-world environments\npresents four core challenges: (1) ensuring consistent execution, (2)\naccurately identifying critical HTML elements, (3) meeting human-like accuracy\nin order to automate operations at scale and (4) the lack of comprehensive\nbenchmarking data on internal web applications. Existing solutions are\nprimarily tailored for well-designed, consumer-facing websites (e.g.,\nAmazon.com, Apple.com) and fall short in addressing the complexity of\npoorly-designed internal web interfaces. To address these limitations, we\npresent Cybernaut, a novel framework to ensure high execution consistency in\nweb automation agents designed for robust enterprise use. Our contributions are\nthreefold: (1) a Standard Operating Procedure (SOP) generator that converts\nuser demonstrations into reliable automation instructions for linear browsing\ntasks, (2) a high-precision HTML DOM element recognition system tailored for\nthe challenge of complex web interfaces, and (3) a quantitative metric to\nassess execution consistency. The empirical evaluation on our internal\nbenchmark demonstrates that using our framework enables a 23.2% improvement\n(from 72% to 88.68%) in task execution success rate over the browser_use.\nCybernaut identifies consistent execution patterns with 84.7% accuracy,\nenabling reliable confidence assessment and adaptive guidance during task\nexecution in real-world systems. These results highlight Cybernaut's\neffectiveness in enterprise-scale web automation and lay a foundation for\nfuture advancements in web automation."}
{"id": "2508.16713", "categories": ["cs.SE", "cs.AI", "hep-ex"], "pdf": "https://arxiv.org/pdf/2508.16713", "abs": "https://arxiv.org/abs/2508.16713", "authors": ["Mohammad Atif", "Kriti Chopra", "Ozgur Kilic", "Tianle Wang", "Zhihua Dong", "Charles Leggett", "Meifeng Lin", "Paolo Calafiura", "Salman Habib"], "title": "CelloAI: Leveraging Large Language Models for HPC Software Development in High Energy Physics", "comment": "12 pages, 2 figures", "summary": "Next-generation High Energy Physics (HEP) experiments will generate\nunprecedented data volumes, necessitating High Performance Computing (HPC)\nintegration alongside traditional high-throughput computing. However, HPC\nadoption in HEP is hindered by the challenge of porting legacy software to\nheterogeneous architectures and the sparse documentation of these complex\nscientific codebases. We present CelloAI, a locally hosted coding assistant\nthat leverages Large Language Models (LLMs) with retrieval-augmented generation\n(RAG) to support HEP code documentation and generation. This local deployment\nensures data privacy, eliminates recurring costs and provides access to large\ncontext windows without external dependencies. CelloAI addresses two primary\nuse cases, code documentation and code generation, through specialized\ncomponents. For code documentation, the assistant provides: (a) Doxygen style\ncomment generation for all functions and classes by retrieving relevant\ninformation from RAG sources (papers, posters, presentations), (b) file-level\nsummary generation, and (c) an interactive chatbot for code comprehension\nqueries. For code generation, CelloAI employs syntax-aware chunking strategies\nthat preserve syntactic boundaries during embedding, improving retrieval\naccuracy in large codebases. The system integrates callgraph knowledge to\nmaintain dependency awareness during code modifications and provides\nAI-generated suggestions for performance optimization and accurate refactoring.\nWe evaluate CelloAI using real-world HEP applications from ATLAS, CMS, and DUNE\nexperiments, comparing different embedding models for code retrieval\neffectiveness. Our results demonstrate the AI assistant's capability to enhance\ncode understanding and support reliable code generation while maintaining the\ntransparency and safety requirements essential for scientific computing\nenvironments."}
{"id": "2508.16765", "categories": ["cs.CR", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2508.16765", "abs": "https://arxiv.org/abs/2508.16765", "authors": ["GodsGift Uzor", "Hasan Al-Qudah", "Ynes Ineza", "Abdul Serwadda"], "title": "Guarding Your Conversations: Privacy Gatekeepers for Secure Interactions with Cloud-Based AI Models", "comment": "2025 19th International Conference on Semantic Computing (ICSC)", "summary": "The interactive nature of Large Language Models (LLMs), which closely track\nuser data and context, has prompted users to share personal and private\ninformation in unprecedented ways. Even when users opt out of allowing their\ndata to be used for training, these privacy settings offer limited protection\nwhen LLM providers operate in jurisdictions with weak privacy laws, invasive\ngovernment surveillance, or poor data security practices. In such cases, the\nrisk of sensitive information, including Personally Identifiable Information\n(PII), being mishandled or exposed remains high. To address this, we propose\nthe concept of an \"LLM gatekeeper\", a lightweight, locally run model that\nfilters out sensitive information from user queries before they are sent to the\npotentially untrustworthy, though highly capable, cloud-based LLM. Through\nexperiments with human subjects, we demonstrate that this dual-model approach\nintroduces minimal overhead while significantly enhancing user privacy, without\ncompromising the quality of LLM responses."}
{"id": "2508.16771", "categories": ["cs.SE", "cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2508.16771", "abs": "https://arxiv.org/abs/2508.16771", "authors": ["Yifan Zhang", "Chen Huang", "Yueke Zhang", "Jiahao Zhang", "Toby Jia-Jun Li", "Collin McMillan", "Kevin Leach", "Yu Huang"], "title": "EyeMulator: Improving Code Language Models by Mimicking Human Visual Attention", "comment": null, "summary": "Code language models (so-called CodeLLMs) are now commonplace in software\ndevelopment. As a general rule, CodeLLMs are trained by dividing training\nexamples into input tokens and then learn importance of those tokens in a\nprocess called machine attention. Machine attention is based solely on input\ntoken salience to output token examples during training. Human software\ndevelopers are different, as humans intuitively know that some tokens are more\nsalient than others. While intuition itself is ineffable and a subject of\nphilosophy, clues about salience are present in human visual attention, since\npeople tend to look at more salient words more often. In this paper, we present\nEyeMulator, a technique for training CodeLLMs to mimic human visual attention\nwhile training for various software development tasks. We add special weights\nfor each token in each input example to the loss function used during LLM\nfine-tuning. We draw these weights from observations of human visual attention\nderived from a previously-collected publicly-available dataset of eye-tracking\nexperiments in software engineering tasks. These new weights ultimately induce\nchanges in the attention of the subject LLM during training, resulting in a\nmodel that does not need eye-tracking data during inference. Our evaluation\nshows that EyeMulator outperforms strong LLM baselines on several tasks such as\ncode translation, completion and summarization. We further show an ablation\nstudy that demonstrates the improvement is due to subject models learning to\nmimic human attention."}
{"id": "2508.16843", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.16843", "abs": "https://arxiv.org/abs/2508.16843", "authors": ["Kamel Kamel", "Keshav Sood", "Hridoy Sankar Dutta", "Sunil Aryal"], "title": "A Survey of Threats Against Voice Authentication and Anti-Spoofing Systems", "comment": "This paper will be submitted to the Computer Science Review", "summary": "Voice authentication has undergone significant changes from traditional\nsystems that relied on handcrafted acoustic features to deep learning models\nthat can extract robust speaker embeddings. This advancement has expanded its\napplications across finance, smart devices, law enforcement, and beyond.\nHowever, as adoption has grown, so have the threats. This survey presents a\ncomprehensive review of the modern threat landscape targeting Voice\nAuthentication Systems (VAS) and Anti-Spoofing Countermeasures (CMs), including\ndata poisoning, adversarial, deepfake, and adversarial spoofing attacks. We\nchronologically trace the development of voice authentication and examine how\nvulnerabilities have evolved in tandem with technological advancements. For\neach category of attack, we summarize methodologies, highlight commonly used\ndatasets, compare performance and limitations, and organize existing literature\nusing widely accepted taxonomies. By highlighting emerging risks and open\nchallenges, this survey aims to support the development of more secure and\nresilient voice authentication systems."}
{"id": "2508.16853", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.16853", "abs": "https://arxiv.org/abs/2508.16853", "authors": ["Pratyush Nidhi Sharma", "Lauren Wright", "Anne Herfurth", "Munsif Sokiyna", "Pratyaksh Nidhi Sharma", "Sethu Das", "Mikko Siponen"], "title": "DevLicOps: A Framework for Mitigating Licensing Risks in AI-Generated Code", "comment": "18 pages, 1 figure, 2 Tables", "summary": "Generative AI coding assistants (ACAs) are widely adopted yet pose serious\nlegal and compliance risks. ACAs can generate code governed by restrictive\nopen-source licenses (e.g., GPL), potentially exposing companies to litigation\nor forced open-sourcing. Few developers are trained in these risks, and legal\nstandards vary globally, especially with outsourcing. Our article introduces\nDevLicOps, a practical framework that helps IT leaders manage ACA-related\nlicensing risks through governance, incident response, and informed tradeoffs.\nAs ACA adoption grows and legal frameworks evolve, proactive license compliance\nis essential for responsible, risk-aware software development in the AI era."}
{"id": "2508.16860", "categories": ["cs.SE", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.16860", "abs": "https://arxiv.org/abs/2508.16860", "authors": ["Md Afif Al Mamun", "Gias Uddin", "Lan Xia", "Longyu Zhang"], "title": "TriagerX: Dual Transformers for Bug Triaging Tasks with Content and Interaction Based Rankings", "comment": "This work is currently under review at IEEE Transactions on Software\n  Engineering. The replication package will be made publicly available upon\n  acceptance", "summary": "Pretrained Language Models or PLMs are transformer-based architectures that\ncan be used in bug triaging tasks. PLMs can better capture token semantics than\ntraditional Machine Learning (ML) models that rely on statistical features\n(e.g., TF-IDF, bag of words). However, PLMs may still attend to less relevant\ntokens in a bug report, which can impact their effectiveness. In addition, the\nmodel can be sub-optimal with its recommendations when the interaction history\nof developers around similar bugs is not taken into account. We designed\nTriagerX to address these limitations. First, to assess token semantics more\nreliably, we leverage a dual-transformer architecture. Unlike current\nstate-of-the-art (SOTA) baselines that employ a single transformer\narchitecture, TriagerX collects recommendations from two transformers with each\noffering recommendations via its last three layers. This setup generates a\nrobust content-based ranking of candidate developers. TriagerX then refines\nthis ranking by employing a novel interaction-based ranking methodology, which\nconsiders developers' historical interactions with similar fixed bugs. Across\nfive datasets, TriagerX surpasses all nine transformer-based methods, including\nSOTA baselines, often improving Top-1 and Top-3 developer recommendation\naccuracy by over 10%. We worked with our large industry partner to successfully\ndeploy TriagerX in their development environment. The partner required both\ndeveloper and component recommendations, with components acting as proxies for\nteam assignments-particularly useful in cases of developer turnover or team\nchanges. We trained TriagerX on the partner's dataset for both tasks, and it\noutperformed SOTA baselines by up to 10% for component recommendations and 54%\nfor developer recommendations."}
{"id": "2508.17155", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.17155", "abs": "https://arxiv.org/abs/2508.17155", "authors": ["Derek Lilienthal", "Sanghyun Hong"], "title": "Mind the Gap: Time-of-Check to Time-of-Use Vulnerabilities in LLM-Enabled Agents", "comment": "Pre-print", "summary": "Large Language Model (LLM)-enabled agents are rapidly emerging across a wide\nrange of applications, but their deployment introduces vulnerabilities with\nsecurity implications. While prior work has examined prompt-based attacks\n(e.g., prompt injection) and data-oriented threats (e.g., data exfiltration),\ntime-of-check to time-of-use (TOCTOU) remain largely unexplored in this\ncontext. TOCTOU arises when an agent validates external state (e.g., a file or\nAPI response) that is later modified before use, enabling practical attacks\nsuch as malicious configuration swaps or payload injection. In this work, we\npresent the first study of TOCTOU vulnerabilities in LLM-enabled agents. We\nintroduce TOCTOU-Bench, a benchmark with 66 realistic user tasks designed to\nevaluate this class of vulnerabilities. As countermeasures, we adapt detection\nand mitigation techniques from systems security to this setting and propose\nprompt rewriting, state integrity monitoring, and tool-fusing. Our study\nhighlights challenges unique to agentic workflows, where we achieve up to 25%\ndetection accuracy using automated detection methods, a 3% decrease in\nvulnerable plan generation, and a 95% reduction in the attack window. When\ncombining all three approaches, we reduce the TOCTOU vulnerabilities from an\nexecuted trajectory from 12% to 8%. Our findings open a new research direction\nat the intersection of AI safety and systems security."}
{"id": "2508.17222", "categories": ["cs.CR", "cs.AI", "cs.IR"], "pdf": "https://arxiv.org/pdf/2508.17222", "abs": "https://arxiv.org/abs/2508.17222", "authors": ["Jiale Liu", "Jiahao Zhang", "Suhang Wang"], "title": "Exposing Privacy Risks in Graph Retrieval-Augmented Generation", "comment": null, "summary": "Retrieval-Augmented Generation (RAG) is a powerful technique for enhancing\nLarge Language Models (LLMs) with external, up-to-date knowledge. Graph RAG has\nemerged as an advanced paradigm that leverages graph-based knowledge structures\nto provide more coherent and contextually rich answers. However, the move from\nplain document retrieval to structured graph traversal introduces new,\nunder-explored privacy risks. This paper investigates the data extraction\nvulnerabilities of the Graph RAG systems. We design and execute tailored data\nextraction attacks to probe their susceptibility to leaking both raw text and\nstructured data, such as entities and their relationships. Our findings reveal\na critical trade-off: while Graph RAG systems may reduce raw text leakage, they\nare significantly more vulnerable to the extraction of structured entity and\nrelationship information. We also explore potential defense mechanisms to\nmitigate these novel attack surfaces. This work provides a foundational\nanalysis of the unique privacy challenges in Graph RAG and offers insights for\nbuilding more secure systems."}
{"id": "2508.17343", "categories": ["cs.SE", "cs.AI", "D.2"], "pdf": "https://arxiv.org/pdf/2508.17343", "abs": "https://arxiv.org/abs/2508.17343", "authors": ["Abhik Roychoudhury"], "title": "Agentic AI for Software: thoughts from Software Engineering community", "comment": "4 pages", "summary": "AI agents have recently shown significant promise in software engineering.\nMuch public attention has been transfixed on the topic of code generation from\nLarge Language Models (LLMs) via a prompt. However, software engineering is\nmuch more than programming, and AI agents go far beyond instructions given by a\nprompt.\n  At the code level, common software tasks include code generation, testing,\nand program repair. Design level software tasks may include architecture\nexploration, requirements understanding, and requirements enforcement at the\ncode level. Each of these software tasks involves micro-decisions which can be\ntaken autonomously by an AI agent, aided by program analysis tools. This\ncreates the vision of an AI software engineer, where the AI agent can be seen\nas a member of a development team.\n  Conceptually, the key to successfully developing trustworthy agentic AI-based\nsoftware workflows will be to resolve the core difficulty in software\nengineering - the deciphering and clarification of developer intent.\nSpecification inference, or deciphering the intent, thus lies at the heart of\nmany software tasks, including software maintenance and program repair. A\nsuccessful deployment of agentic technology into software engineering would\ninvolve making conceptual progress in such intent inference via agents.\n  Trusting the AI agent becomes a key aspect, as software engineering becomes\nmore automated. Higher automation also leads to higher volume of code being\nautomatically generated, and then integrated into code-bases. Thus to deal with\nthis explosion, an emerging direction is AI-based verification and validation\n(V & V) of AI generated code. We posit that agentic software workflows in\nfuture will include such AIbased V&V."}
{"id": "2508.17674", "categories": ["cs.CR", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.17674", "abs": "https://arxiv.org/abs/2508.17674", "authors": ["Qiming Guo", "Jinwen Tang", "Xingran Huang"], "title": "Attacking LLMs and AI Agents: Advertisement Embedding Attacks Against Large Language Models", "comment": "7 pages, 2 figures", "summary": "We introduce Advertisement Embedding Attacks (AEA), a new class of LLM\nsecurity threats that stealthily inject promotional or malicious content into\nmodel outputs and AI agents. AEA operate through two low-cost vectors: (1)\nhijacking third-party service-distribution platforms to prepend adversarial\nprompts, and (2) publishing back-doored open-source checkpoints fine-tuned with\nattacker data. Unlike conventional attacks that degrade accuracy, AEA subvert\ninformation integrity, causing models to return covert ads, propaganda, or hate\nspeech while appearing normal. We detail the attack pipeline, map five\nstakeholder victim groups, and present an initial prompt-based self-inspection\ndefense that mitigates these injections without additional model retraining.\nOur findings reveal an urgent, under-addressed gap in LLM security and call for\ncoordinated detection, auditing, and policy responses from the AI-safety\ncommunity."}
{"id": "2508.17900", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.17900", "abs": "https://arxiv.org/abs/2508.17900", "authors": ["Mohammed O. Alannsary"], "title": "A Defect Classification Framework for AI-Based Software Systems (AI-ODC)", "comment": "Article, 19 pages, 6 figures, 8 tables,", "summary": "Artificial Intelligence has gained a lot of attention recently, it has been\nutilized in several fields ranging from daily life activities, such as\nresponding to emails and scheduling appointments, to manufacturing and\nautomating work activities. Artificial Intelligence systems are mainly\nimplemented as software solutions, and it is essential to discover and remove\nsoftware defects to assure its quality using defect analysis which is one of\nthe major activities that contribute to software quality. Despite the\nproliferation of AI-based systems, current defect analysis models fail to\ncapture their unique attributes. This paper proposes a framework inspired by\nthe Orthogonal Defect Classification (ODC) paradigm and enables defect analysis\nof Artificial Intelligence systems while recognizing its special attributes and\ncharacteristics. This study demonstrated the feasibility of modifying ODC for\nAI systems to classify its defects. The ODC was adjusted to accommodate the\nData, Learning, and Thinking aspects of AI systems which are newly introduced\nclassification dimensions. This adjustment involved the introduction of an\nadditional attribute to the ODC attributes, the incorporation of a new severity\nlevel, and the substitution of impact areas with characteristics pertinent to\nAI systems. The framework was showcased by applying it to a publicly available\nMachine Learning bug dataset, with results analyzed through one-way and two-way\nanalysis. The case study indicated that defects occurring during the Learning\nphase were the most prevalent and were significantly linked to high-severity\nclassifications. In contrast, defects identified in the Thinking phase had a\ndisproportionate effect on trustworthiness and accuracy. These findings\nillustrate AIODC's capability to identify high-risk defect categories and\ninform focused quality assurance measures."}
{"id": "2508.18003", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.18003", "abs": "https://arxiv.org/abs/2508.18003", "authors": ["Robert Heum√ºller", "Frank Ortmeier"], "title": "Previously on... Automating Code Review", "comment": "Preprint currently under review", "summary": "Modern Code Review (MCR) is a standard practice in software engineering, yet\nit demands substantial time and resource investments. Recent research has\nincreasingly explored automating core review tasks using machine learning (ML)\nand deep learning (DL). As a result, there is substantial variability in task\ndefinitions, datasets, and evaluation procedures. This study provides the first\ncomprehensive analysis of MCR automation research, aiming to characterize the\nfield's evolution, formalize learning tasks, highlight methodological\nchallenges, and offer actionable recommendations to guide future research.\nFocusing on the primary code review tasks, we systematically surveyed 691\npublications and identified 24 relevant studies published between May 2015 and\nApril 2024. Each study was analyzed in terms of tasks, models, metrics,\nbaselines, results, validity concerns, and artifact availability. In\nparticular, our analysis reveals significant potential for standardization,\nincluding 48 task metric combinations, 22 of which were unique to their\noriginal paper, and limited dataset reuse. We highlight challenges and derive\nconcrete recommendations for examples such as the temporal bias threat, which\nare rarely addressed so far. Our work contributes to a clearer overview of the\nfield, supports the framing of new research, helps to avoid pitfalls, and\npromotes greater standardization in evaluation practices."}
{"id": "2508.18106", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.18106", "abs": "https://arxiv.org/abs/2508.18106", "authors": ["Keke Lian", "Bin Wang", "Lei Zhang", "Libo Chen", "Junjie Wang", "Ziming Zhao", "Yujiu Yang", "Haotong Duan", "Haoran Zhao", "Shuang Liao", "Mingda Guo", "Jiazheng Quan", "Yilu Zhong", "Chenhao He", "Zichuan Chen", "Jie Wu", "Haoling Li", "Zhaoxuan Li", "Jiongchi Yu", "Hui Li", "Dong Zhang"], "title": "A.S.E: A Repository-Level Benchmark for Evaluating Security in AI-Generated Code", "comment": null, "summary": "The increasing adoption of large language models (LLMs) in software\nengineering necessitates rigorous security evaluation of their generated code.\nHowever, existing benchmarks are inadequate, as they focus on isolated code\nsnippets, employ unstable evaluation methods that lack reproducibility, and\nfail to connect the quality of input context with the security of the output.\nTo address these gaps, we introduce A.S.E (AI Code Generation Security\nEvaluation), a benchmark for repository-level secure code generation. A.S.E\nconstructs tasks from real-world repositories with documented CVEs, preserving\nfull repository context like build systems and cross-file dependencies. Its\nreproducible, containerized evaluation framework uses expert-defined rules to\nprovide stable, auditable assessments of security, build quality, and\ngeneration stability. Our evaluation of leading LLMs on A.S.E reveals three key\nfindings: (1) Claude-3.7-Sonnet achieves the best overall performance. (2) The\nsecurity gap between proprietary and open-source models is narrow;\nQwen3-235B-A22B-Instruct attains the top security score. (3) Concise,\n``fast-thinking'' decoding strategies consistently outperform complex,\n``slow-thinking'' reasoning for security patching."}
{"id": "2508.18148", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.18148", "abs": "https://arxiv.org/abs/2508.18148", "authors": ["Haijian Ma", "Daizong Liu", "Xiaowen Cai", "Pan Zhou", "Yulai Xie"], "title": "Learning from Few Samples: A Novel Approach for High-Quality Malcode Generation", "comment": "18pages,5 figures,emnlp", "summary": "Intrusion Detection Systems (IDS) play a crucial role in network security\ndefense. However, a significant challenge for IDS in training detection models\nis the shortage of adequately labeled malicious samples. To address these\nissues, this paper introduces a novel semi-supervised framework\n\\textbf{GANGRL-LLM}, which integrates Generative Adversarial Networks (GANs)\nwith Large Language Models (LLMs) to enhance malicious code generation and SQL\nInjection (SQLi) detection capabilities in few-sample learning scenarios.\nSpecifically, our framework adopts a collaborative training paradigm where: (1)\nthe GAN-based discriminator improves malicious pattern recognition through\nadversarial learning with generated samples and limited real samples; and (2)\nthe LLM-based generator refines the quality of malicious code synthesis using\nreward signals from the discriminator. The experimental results demonstrate\nthat even with a limited number of labeled samples, our training framework is\nhighly effective in enhancing both malicious code generation and detection\ncapabilities. This dual enhancement capability offers a promising solution for\ndeveloping adaptive defense systems capable of countering evolving cyber\nthreats."}
{"id": "2508.18230", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.18230", "abs": "https://arxiv.org/abs/2508.18230", "authors": ["Chitraksh Singh", "Monisha Dhanraj", "Ken Huang"], "title": "KillChainGraph: ML Framework for Predicting and Mapping ATT&CK Techniques", "comment": "8 pages, 3 figures", "summary": "The escalating complexity and volume of cyberattacks demand proactive\ndetection strategies that go beyond traditional rule-based systems. This paper\npresents a phase-aware, multi-model machine learning framework that emulates\nadversarial behavior across the seven phases of the Cyber Kill Chain using the\nMITRE ATT&CK Enterprise dataset. Techniques are semantically mapped to phases\nvia ATTACK-BERT, producing seven phase-specific datasets. We evaluate LightGBM,\na custom Transformer encoder, fine-tuned BERT, and a Graph Neural Network\n(GNN), integrating their outputs through a weighted soft voting ensemble.\nInter-phase dependencies are modeled using directed graphs to capture attacker\nmovement from reconnaissance to objectives. The ensemble consistently achieved\nthe highest scores, with F1-scores ranging from 97.47% to 99.83%, surpassing\nGNN performance (97.36% to 99.81%) by 0.03%--0.20% across phases. This\ngraph-driven, ensemble-based approach enables interpretable attack path\nforecasting and strengthens proactive cyber defense."}
