<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 7]
- [cs.AI](#cs.AI) [Total: 41]
- [cs.CR](#cs.CR) [Total: 14]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [Search-based Software Testing Driven by Domain Knowledge: Reflections and New Perspectives](https://arxiv.org/abs/2512.10079)
*Federico Formica,Mark Lawford,Claudio Menghi*

Main category: cs.SE

TL;DR: 本文回顾了基于搜索的软件测试（SBST）技术，重点关注如何将工程师的领域知识整合到现有SBST框架中，并基于近期实验结果提出新的研究方向。


<details>
  <summary>Details</summary>
Motivation: SBST虽然能自动生成大量测试用例，但缺乏工程师的领域知识，这限制了其在实际应用中的效果。已有研究尝试将领域知识整合到SBST中，但需要重新审视这些技术的效果和方向。

Method: 通过对近期实验结果进行反思性分析，重点关注那些大胆和意外的发现，从新的视角重新审视基于领域知识驱动的SBST技术。

Result: 通过分析近期实验结果，识别出SBST与领域知识结合过程中的关键问题和挑战，揭示了现有方法的局限性。

Conclusion: 本文为基于领域知识的SBST研究提供了新的视角，指出了未来研究的方向，有助于改进SBST技术在实际软件开发中的应用效果。

Abstract: Search-based Software Testing (SBST) can automatically generate test cases to search for requirements violations. Unlike manual test case development, it can generate a substantial number of test cases in a limited time. However, SBST does not possess the domain knowledge of engineers. Several techniques have been proposed to integrate engineers' domain knowledge within existing SBST frameworks. This paper will reflect on recent experimental results by highlighting bold and unexpected results. It will help re-examine SBST techniques driven by domain knowledge from a new perspective, suggesting new directions for future research.

</details>


### [2] [ATLAS: Automated Toolkit for Large-Scale Verified Code Synthesis](https://arxiv.org/abs/2512.10173)
*Mantas Baksys,Stefan Zetzsche,Olivier Bouissou,Remi Delmas,Soonho Kong*

Main category: cs.SE

TL;DR: ATLAS是一个自动化管道，通过合成经过验证的Dafny程序来解决LLM程序验证训练数据稀缺的问题，生成了2.7K个验证程序和19K个训练示例，显著提升了模型在Dafny验证任务上的性能。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在程序验证方面显示出潜力，但由于缺乏经过验证的代码训练数据，进展受到阻碍。现有验证代码的稀缺性限制了LLM在形式验证任务上的能力提升。

Method: ATLAS是一个自动化管道，能够大规模合成经过验证的Dafny程序。它生成完整的Dafny程序，包括规范、实现和证明。通过将合成过程分解为多个专门任务，从每个验证程序中提取超过7个训练示例，总共从2.7K个验证程序中提取了19K多个训练示例。

Result: 在ATLAS生成的数据集上微调Qwen 2.5 7B Coder模型取得了显著效果：在DafnyBench上提升了23个百分点，在DafnySynthesis上提升了50个百分点。这证明了合成验证代码能够有效增强LLM的形式验证能力。

Conclusion: 合成验证代码可以有效解决LLM程序验证训练数据稀缺的问题，显著提升模型在形式验证任务上的性能。ATLAS管道为大规模生成验证程序提供了一种有效方法，推动了LLM在程序验证领域的发展。

Abstract: Large language models have shown potential for program verification, but progress is hindered by the scarcity of verified code for training. We present ATLAS, an automated pipeline that synthesizes verified programs at scale to address this data bottleneck. ATLAS generates complete Dafny programs with specifications, implementations, and proofs, producing 2.7K verified programs from which we extract over 19K training examples--more than 7 per verified program--by decomposing the synthesis process into multiple specialized tasks. Fine-tuning Qwen 2.5 7B Coder on this dataset produces substantial gains: +23 percentage points on DafnyBench and +50 percentage points on DafnySynthesis. These results demonstrate that synthetic verified code can effectively enhance LLM capabilities for formal verification.

</details>


### [3] [Studying and Automating Issue Resolution for Software Quality](https://arxiv.org/abs/2512.10238)
*Antu Saha*

Main category: cs.SE

TL;DR: 该研究通过三个方向提升软件问题解决效率：1) 利用LLM提升问题报告质量；2) 实证分析传统和AI增强系统中的开发者工作流程；3) 通过ML/DL/LLM自动化解决认知密集型任务。


<details>
  <summary>Details</summary>
Motivation: 软件开发中问题解决面临三大挑战：低质量的问题报告、对实际工作流程理解有限、缺乏自动化支持。这些问题影响了软件质量和维护效率。

Method: 采用三个互补方向：1) 利用LLM推理和应用特定信息提升问题报告质量；2) 实证分析传统和AI增强系统中的开发者工作流程；3) 通过机器学习、深度学习和基于LLM的方法自动化解决认知密集型任务，包括buggy UI定位和解决方案识别。

Result: 研究提供了实证洞察、实用工具和自动化方法，推进了AI驱动的问题解决，支持更可维护和高质量的软件系统。

Conclusion: 通过综合方法解决软件问题解决中的关键挑战，该工作为AI驱动的软件维护提供了全面解决方案，有助于提升软件质量和开发效率。

Abstract: Effective issue resolution is crucial for maintaining software quality. Yet developers frequently encounter challenges such as low-quality issue reports, limited understanding of real-world workflows, and a lack of automated support. This research aims to address these challenges through three complementary directions. First, we enhance issue report quality by proposing techniques that leverage LLM reasoning and application-specific information. Second, we empirically characterize developer workflows in both traditional and AI-augmented systems. Third, we automate cognitively demanding resolution tasks, including buggy UI localization and solution identification, through ML, DL, and LLM-based approaches. Together, our work delivers empirical insights, practical tools, and automated methods to advance AI-driven issue resolution, supporting more maintainable and high-quality software systems.

</details>


### [4] [UniCoR: Modality Collaboration for Robust Cross-Language Hybrid Code Retrieval](https://arxiv.org/abs/2512.10452)
*Yang Yang,Li Kuang,Jiakun Liu,Zhongxin Liu,Yingjie Xia,David Lo*

Main category: cs.SE

TL;DR: UniCoR是一个自监督的统一代码表示学习框架，通过多视角对比学习和表示分布一致性学习，解决了混合代码检索中的语义理解不足、模态融合效率低和跨语言泛化弱的问题。


<details>
  <summary>Details</summary>
Motivation: 现有代码检索方法在混合查询（自然语言+代码片段）和跨语言场景中存在三个主要挑战：语义理解不足、混合检索中的模态融合效率低、跨语言场景泛化能力弱。

Method: 提出UniCoR框架，包含两个核心模块：1）多视角监督对比学习模块，从代码-代码、自然语言-代码、自然语言-自然语言多个视角对齐表示；2）表示分布一致性学习模块，显式对齐不同编程语言的特征分布。

Result: 在实证基准和大规模基准测试中，UniCoR优于所有基线模型，MRR平均提升8.64%，MAP平均提升11.54%。在混合代码检索中表现稳定，在跨语言场景中具有良好泛化能力。

Conclusion: UniCoR通过统一的自监督学习框架有效解决了混合代码检索中的关键挑战，为跨语言代码检索提供了鲁棒的解决方案。

Abstract: Effective code retrieval is indispensable and it has become an important paradigm to search code in hybrid mode using both natural language and code snippets. Nevertheless, it remains unclear whether existing approaches can effectively leverage such hybrid queries, particularly in cross-language contexts. We conduct a comprehensive empirical study of representative code models and reveal three challenges: (1) insufficient semantic understanding; (2) inefficient fusion in hybrid code retrieval; and (3) weak generalization in cross-language scenarios. To address these challenges, we propose UniCoR, a novel self-supervised framework that learns Unified Code Representations framework designed to learn unified and robust code representations. Firstly, we design a multi-perspective supervised contrastive learning module to enhance semantic understanding and modality fusion. It aligns representations from multiple perspectives, including code-to-code, natural language-to-code, and natural language-to-natural language, enforcing the model to capture a semantic essence among modalities. Secondly, we introduce a representation distribution consistency learning module to improve cross-language generalization, which explicitly aligns the feature distributions of different programming languages, enabling language-agnostic representation learning. Extensive experiments on both empirical benchmark and large-scale benchmark show that UniCoR outperforms all baseline models, achieving an average improvement of 8.64% in MRR and 11.54% in MAP over the best-performing baseline. Furthermore, UniCoR exhibits stability in hybrid code retrieval and generalization capability in cross-language scenarios.

</details>


### [5] [Decoding Human-LLM Collaboration in Coding: An Empirical Study of Multi-Turn Conversations in the Wild](https://arxiv.org/abs/2512.10493)
*Binquan Zhang,Li Zhang,Haoyuan Zhang,Fang Liu,Song Wang,Bo Shen,An Fu,Lin Shi*

Main category: cs.SE

TL;DR: 该论文通过分析LMSYS-Chat-1M和WildChat数据集，实证研究了人类与LLM在编程协作中的交互机制、指令遵循能力和用户满意度，发现任务类型影响交互模式，bug修复和代码重构对LLM更具挑战性，不同任务类型的用户满意度存在差异。


<details>
  <summary>Details</summary>
Motivation: 虽然存在LMSYS-Chat-1M和WildChat等捕获真实用户-LLM对话的数据集，但很少有研究系统探索编程场景中人类-LLM协作机制。用户在与LLM交互过程中经历了怎样的曲折路径？LLM遵循指令的能力如何？用户是否满意？这些问题需要深入研究。

Method: 使用LMSYS-Chat-1M和WildChat数据集进行实证分析，探索人类-LLM协作机制、LLM指令遵循能力和人类满意度。通过分析真实世界的用户-LLM编程对话数据来获得洞察。

Result: 1) 任务类型塑造交互模式（线性、星形和树形），代码质量优化偏好线性模式，设计驱动任务倾向于树形结构，查询偏好星形模式；2) Bug修复和代码重构对LLM指令遵循构成更大挑战，不遵循率显著高于信息查询；3) 代码质量优化和需求驱动开发任务用户满意度较低，而结构化知识查询和算法设计满意度较高。

Conclusion: 这些发现为改进LLM界面和编程协作中的用户满意度提供了建议，同时突出了自适应对话系统未来研究的途径。这项工作拓宽了对人类-LLM协同作用的理解，支持更有效的AI辅助开发。

Abstract: Large language models (LLMs) are increasingly acting as dynamic conversational interfaces, supporting multi-turn interactions that mimic human-like conversation and facilitate complex tasks like coding. While datasets such as LMSYS-Chat-1M and WildChat capture real-world user-LLM conversations, few studies systematically explore the mechanisms of human-LLM collaboration in coding scenarios. What tortuous paths do users experience during the interaction process? How well do the LLMs follow instructions? Are users satisfied? In this paper, we conduct an empirical analysis on human-LLM coding collaboration using LMSYS-Chat-1M and WildChat datasets to explore the human-LLM collaboration mechanism, LLMs' instruction following ability, and human satisfaction. This study yields interesting findings: 1) Task types shape interaction patterns(linear, star and tree), with code quality optimization favoring linear patterns, design-driven tasks leaning toward tree structures, and queries preferring star patterns; 2) Bug fixing and code refactoring pose greater challenges to LLMs' instruction following, with non-compliance rates notably higher than in information querying; 3) Code quality optimization and requirements-driven development tasks show lower user satisfaction, whereas structured knowledge queries and algorithm designs yield higher levels. These insights offer recommendations for improving LLM interfaces and user satisfaction in coding collaborations, while highlighting avenues for future research on adaptive dialogue systems. We believe this work broadens understanding of human-LLM synergies and supports more effective AI-assisted development.

</details>


### [6] [Analyzing developer discussions on EU and US privacy legislation compliance in GitHub repositories](https://arxiv.org/abs/2512.10618)
*Georgia M. Kapitsaki,Maria Papoutsoglou,Christoph Treude,Ioanna Theophilou*

Main category: cs.SE

TL;DR: 该研究通过分析GitHub上的32,820个问题，探讨开源软件开发者如何讨论隐私法规（GDPR和CCPA）合规问题，识别出24个讨论类别和6个集群，发现开发者主要关注特定用户权利和同意管理。


<details>
  <summary>Details</summary>
Motivation: 尽管GitHub上有大量开发者问题数据，但缺乏关于开源软件开发者如何讨论隐私法规（GDPR和CCPA）合规问题的实证证据。隐私立法影响了软件开发方式，但开发者如何在实际中处理这些合规问题尚不清楚。

Method: 通过挖掘和分析GitHub仓库中的32,820个问题，部分自动分析以识别法律用户权利和原则，并对1,186个问题进行手动分析，基于所关注的问题类型进行分类。

Result: 创建了包含24个讨论类别和6个集群的分类法：功能/缺陷、同意相关、文档、数据存储/共享、适应性和一般合规。发现开发者主要关注特定用户权利（删除权、选择退出权、访问权），较少讨论其他权利，大多数讨论涉及用户同意、用户权利功能、缺陷和cookie管理。

Conclusion: 创建的分类法可帮助从业者了解法律合规讨论的问题类型，优先处理这些问题。教育界可据此调整课程以更好地教育未来工程师，研究界可识别差距和改进领域以支持和加速数据隐私法合规。

Abstract: Context: Privacy legislation has impacted the way software systems are developed, prompting practitioners to update their implementations. Specifically, the EU General Data Protection Regulation (GDPR) and the California Consumer Privacy Act (CCPA) have forced the community to focus on users' data privacy. Despite the vast amount of data on developer issues available in GitHub repositories, there is a lack of empirical evidence on the issues developers of Open Source Software discuss to comply with privacy legislation. Method: In this work, we examine such discussions by mining and analyzing 32,820 issues from GitHub repositories. We partially analyzed the dataset automatically to identify law user rights and principles indicated, and manually analyzed a sample of 1,186 issues based on the type of concern addressed. Results: We devised 24 discussion categories placed in six clusters: features/bugs, consent-related, documentation, data storing/sharing, adaptability, and general compliance. Our results show that developers mainly focus on specific user rights from the legislation (right to erasure, right to opt-out, right to access), addressing other rights less frequently, while most discussions concern user consent, user rights functionality, bugs and cookies management. Conclusion: The created taxonomy can help practitioners understand which issues are discussed for law compliance, so that they ensure they address them first in their systems. In addition, the educational community can reshape curricula to better educate future engineers on the privacy law concerns raised, and the research community can identify gaps and areas for improvement to support and accelerate data privacy law compliance.

</details>


### [7] [Zorya: Automated Concolic Execution of Single-Threaded Go Binaries](https://arxiv.org/abs/2512.10799)
*Karolina Gorna,Nicolas Iooss,Yannick Seurin,Rida Khatoun*

Main category: cs.SE

TL;DR: Zorya是一个针对Go二进制文件的concolic执行框架，通过将Go二进制转换为Ghidra的P-Code中间表示，添加了未执行路径的bug检测和多层过滤机制，专注于panic相关路径，显著提高了漏洞检测效率和准确性。


<details>
  <summary>Details</summary>
Motivation: Go语言在关键基础设施中的广泛应用增加了系统化漏洞检测的需求，但现有的符号执行工具在处理Go二进制文件时面临运行时复杂性和可扩展性挑战。

Method: 基于Zorya框架，将Go二进制转换为Ghidra的P-Code中间表示，添加了未执行路径的bug检测和多层过滤机制，专注于panic相关路径的符号推理，并采用函数模式分析替代从main函数开始的分析。

Result: 在五个Go漏洞上的评估显示，panic可达性门控实现了1.8-3.9倍的速度提升，过滤了33-70%的分支；Zorya检测到所有panic，而现有工具最多只能检测两个；函数模式分析比从main开始分析快约两个数量级。

Conclusion: 专门的concolic执行可以在具有运行时安全检查的语言生态系统中实现实用的漏洞检测，为Go二进制文件的系统化漏洞分析提供了有效解决方案。

Abstract: Go's adoption in critical infrastructure intensifies the need for systematic vulnerability detection, yet existing symbolic execution tools struggle with Go binaries due to runtime complexity and scalability challenges. In this work, we build upon Zorya, a concolic execution framework that translates Go binaries to Ghidra's P-Code intermediate representation to address these challenges. We added the detection of bugs in concretely not taken paths and a multi-layer filtering mechanism to concentrate symbolic reasoning on panic-relevant paths. Evaluation on five Go vulnerabilities demonstrates that panic-reachability gating achieves 1.8-3.9x speedups when filtering 33-70% of branches, and that Zorya detects all panics while existing tools detect at most two. Function-mode analysis proved essential for complex programs, running roughly two orders of magnitude faster than starting from main. This work establishes that specialized concolic execution can achieve practical vulnerability detection in language ecosystems with runtime safety checks.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [8] [ExaCraft: Dynamic Learning Context Adaptation for Personalized Educational Examples](https://arxiv.org/abs/2512.09931)
*Akaash Chatterjee,Suman Kundu*

Main category: cs.AI

TL;DR: ExaCraft是一个AI驱动的个性化学习示例生成系统，通过分析学习者的动态上下文（包括地理位置、教育背景、职业、复杂度偏好以及实时学习行为），生成文化相关且适应个人学习需求的示例。


<details>
  <summary>Details</summary>
Motivation: 现有教育AI工具未能专注于生成个性化示例，也无法适应学习者不断变化的理解水平、学习困难或技能增长。学习最有效的方式是与学习者个人相关的、有共鸣的示例，因此需要开发能够动态适应学习者上下文的教学示例生成系统。

Method: 基于Google Gemini AI和Python Flask API构建的Chrome扩展系统，结合用户定义的个人资料（位置、教育、职业、复杂度偏好）和实时学习行为分析，能够适应五个关键学习上下文方面：学习困难指标、掌握模式、主题进展历史、会话边界和学习进展信号。

Result: 系统能够生成从基础概念到高级技术实现的演化示例，响应主题重复、重新生成请求和主题进展模式，在不同使用场景中展示了个性化示例的生成能力。

Conclusion: ExaCraft通过动态适应学习者上下文，实现了真正个性化的学习示例生成，解决了现有教育AI工具在个性化教学方面的不足，为学习者提供文化相关且适应个人需求的学习体验。

Abstract: Learning is most effective when it's connected to relevant, relatable examples that resonate with learners on a personal level. However, existing educational AI tools don't focus on generating examples or adapting to learners' changing understanding, struggles, or growing skills. We've developed ExaCraft, an AI system that generates personalized examples by adapting to the learner's dynamic context. Through the Google Gemini AI and Python Flask API, accessible via a Chrome extension, ExaCraft combines user-defined profiles (including location, education, profession, and complexity preferences) with real-time analysis of learner behavior. This ensures examples are both culturally relevant and tailored to individual learning needs. The system's core innovation is its ability to adapt to five key aspects of the learning context: indicators of struggle, mastery patterns, topic progression history, session boundaries, and learning progression signals. Our demonstration will show how ExaCraft's examples evolve from basic concepts to advanced technical implementations, responding to topic repetition, regeneration requests, and topic progression patterns in different use cases.

</details>


### [9] [Exploring LLMs for Scientific Information Extraction Using The SciEx Framework](https://arxiv.org/abs/2512.10004)
*Sha Li,Ayush Sadekar,Nathan Self,Yiqi Su,Lars Andersland,Mira Chaplin,Annabel Zhang,Hyoju Yang,James B Henderson,Krista Wigginton,Linsey Marr,T. M. Murali,Naren Ramakrishnan*

Main category: cs.AI

TL;DR: SciEx是一个模块化框架，用于从科学文献中提取细粒度信息，解决长文档、多模态内容和数据模式快速变化等挑战。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型在科学信息提取方面面临挑战：科学文献通常包含长上下文文档、多模态内容，且不同出版物中的细粒度信息存在不一致性。当数据模式或提取本体快速变化时，现有系统难以重新架构或微调。

Method: 提出SciEx框架，采用模块化和可组合设计，将PDF解析、多模态检索、信息提取和聚合等关键组件解耦。这种设计支持按需数据提取，并能灵活集成新模型、提示策略和推理机制。

Result: 在三个科学主题的数据集上评估SciEx，验证其准确性和一致性提取细粒度信息的能力。研究结果提供了关于当前基于LLM的提取流程的优势和局限性的实用见解。

Conclusion: SciEx框架通过模块化设计有效解决了科学文献信息提取中的关键挑战，为快速变化的科学数据提取需求提供了灵活、可扩展的解决方案。

Abstract: Large language models (LLMs) are increasingly touted as powerful tools for automating scientific information extraction. However, existing methods and tools often struggle with the realities of scientific literature: long-context documents, multi-modal content, and reconciling varied and inconsistent fine-grained information across multiple publications into standardized formats. These challenges are further compounded when the desired data schema or extraction ontology changes rapidly, making it difficult to re-architect or fine-tune existing systems. We present SciEx, a modular and composable framework that decouples key components including PDF parsing, multi-modal retrieval, extraction, and aggregation. This design streamlines on-demand data extraction while enabling extensibility and flexible integration of new models, prompting strategies, and reasoning mechanisms. We evaluate SciEx on datasets spanning three scientific topics for its ability to extract fine-grained information accurately and consistently. Our findings provide practical insights into both the strengths and limitations of current LLM-based pipelines.

</details>


### [10] [SimWorld-Robotics: Synthesizing Photorealistic and Dynamic Urban Environments for Multimodal Robot Navigation and Collaboration](https://arxiv.org/abs/2512.10046)
*Yan Zhuang,Jiawei Ren,Xiaokang Ye,Jianzhi Shen,Ruixuan Zhang,Tianai Yue,Muhammad Faayez,Xuhong He,Ziqiao Ma,Lianhui Qin,Zhiting Hu,Tianmin Shu*

Main category: cs.AI

TL;DR: SWR是一个基于虚幻引擎5构建的大规模、逼真城市环境仿真平台，用于具身AI研究，包含多模态指令跟随和多智能体搜索两个挑战性基准任务。


<details>
  <summary>Details</summary>
Motivation: 当前基础模型在机器人领域的研究主要集中在室内家庭场景，缺乏针对大规模城市环境的仿真平台和评估基准，无法全面评估机器人在复杂城市环境中的关键能力。

Method: 基于虚幻引擎5构建SWR仿真平台，通过程序化生成无限逼真的城市场景，包含动态元素如行人和交通系统。平台支持多机器人控制和通信，并构建了两个基准任务：多模态指令跟随任务和多智能体搜索任务。

Result: 实验结果表明，包括视觉语言模型在内的最先进模型在SWR任务上表现不佳，缺乏在城市环境中所需的稳健感知、推理和规划能力。

Conclusion: SWR平台填补了大规模城市环境机器人仿真的空白，其基准任务能全面评估机器人在真实城市场景中的关键能力，为开发适用于开放城市环境的通用机器人系统提供了重要工具。

Abstract: Recent advances in foundation models have shown promising results in developing generalist robotics that can perform diverse tasks in open-ended scenarios given multimodal inputs. However, current work has been mainly focused on indoor, household scenarios. In this work, we present SimWorld-Robotics~(SWR), a simulation platform for embodied AI in large-scale, photorealistic urban environments. Built on Unreal Engine 5, SWR procedurally generates unlimited photorealistic urban scenes populated with dynamic elements such as pedestrians and traffic systems, surpassing prior urban simulations in realism, complexity, and scalability. It also supports multi-robot control and communication. With these key features, we build two challenging robot benchmarks: (1) a multimodal instruction-following task, where a robot must follow vision-language navigation instructions to reach a destination in the presence of pedestrians and traffic; and (2) a multi-agent search task, where two robots must communicate to cooperatively locate and meet each other. Unlike existing benchmarks, these two new benchmarks comprehensively evaluate a wide range of critical robot capacities in realistic scenarios, including (1) multimodal instructions grounding, (2) 3D spatial reasoning in large environments, (3) safe, long-range navigation with people and traffic, (4) multi-robot collaboration, and (5) grounded communication. Our experimental results demonstrate that state-of-the-art models, including vision-language models (VLMs), struggle with our tasks, lacking robust perception, reasoning, and planning abilities necessary for urban environments.

</details>


### [11] [Parallel Decoder Transformer: Model-Internal Parallel Decoding with Speculative Invariance via Note Conditioning](https://arxiv.org/abs/2512.10054)
*Logan Robbins*

Main category: cs.AI

TL;DR: PDT是一种参数高效的并行解码架构，通过注入轻量级适配器实现冻结预训练模型的并行生成，解决了传统方法中的连贯性漂移问题。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型的自回归解码本质上是顺序的，导致延迟随输出长度线性增长。现有的"分解与填充"方法虽然尝试并行化生成，但缺乏跨流通信导致连贯性漂移问题。

Method: 提出并行解码器变换器(PDT)，在冻结预训练模型中注入轻量级的推测性笔记调节(SNC)适配器。将协调问题形式化为推测共识问题，让并行解码流通过共享的动态潜在空间进行同步，使用学习的验证头门控语义"笔记"广播。

Result: 在冻结的200亿参数骨干模型上使用5万步课程验证，PDT实现了有效的自校正，在覆盖预测中达到77.8%的精确度，无需修改主干权重即可恢复近似的串行语义。

Conclusion: PDT为结构化并行生成提供了一个可扩展、高效的替代方案，无需进行完整的模型微调，通过参数高效的架构解决了并行解码中的协调问题。

Abstract: Autoregressive decoding in Large Language Models (LLMs) is inherently sequential, creating a latency bottleneck that scales linearly with output length. While ``Decomposition-and-Fill'' methods like Skeleton-of-Thought attempt to parallelize generation via external orchestration, they suffer from \textit{coherence drift} due to the lack of cross-stream communication. In this work, we introduce the \textbf{Parallel Decoder Transformer (PDT)}, a parameter-efficient architecture that embeds coordination primitives directly into the inference process of a frozen pre-trained model.
  Instead of retraining the base model, PDT injects lightweight \textit{Speculative Note Conditioning (SNC)} adapters that allow parallel decoding streams to synchronize via a shared, dynamic latent space. We formulate coordination as a \textit{speculative consensus} problem, where sibling streams broadcast semantic ``notes'' to a global bus, gated by a learned verification head. We validate our approach on a 50,000-step curriculum using a frozen 20B-parameter backbone. Our results demonstrate that PDT achieves effective self-correction, reaching \textbf{77.8\% precision} in coverage prediction and recovering approximate serial semantics without modifying the trunk weights. This establishes PDT as a scalable, efficient alternative to full model fine-tuning for structured parallel generation.

</details>


### [12] [Linear socio-demographic representations emerge in Large Language Models from indirect cues](https://arxiv.org/abs/2512.10065)
*Paul Bouchaud,Pedro Ramaciotti*

Main category: cs.AI

TL;DR: LLMs通过姓名和职业等间接线索形成用户社会人口统计学属性的线性表示，这些表示会影响模型的下游行为，即使通过偏见测试的模型仍可能隐含并利用这些偏见。


<details>
  <summary>Details</summary>
Motivation: 研究LLMs如何通过间接线索（如姓名和职业）编码人类对话伙伴的社会人口统计学属性，揭示模型即使通过偏见基准测试仍可能隐含并利用这些偏见，这对大规模应用时的公平性有重要影响。

Method: 在四个开源Transformer模型（Magistral 24B、Qwen3 14B、GPT-OSS 20B、OLMo2-1B）的残差流中探测社会人口统计学属性的线性表示，使用显式人口统计披露进行提示，然后验证这些探针能否从姓名预测性别和种族，从职业预测与真实劳动力统计数据相关的表示。

Result: LLMs在激活空间中形成了用户人口统计学的线性表示，其中刻板印象相关的属性沿着可解释的几何方向编码。姓名激活了与人口普查一致的性别和种族表示，职业触发了与现实世界劳动力统计数据相关的表示。这些隐含的人口统计表示会主动影响下游行为，如职业推荐。

Conclusion: LLMs通过间接线索形成隐含的人口统计学表示，这些表示会影响模型行为，即使通过偏见测试的模型仍可能隐含并利用这些偏见，这对大规模应用时的公平性提出了重要挑战。

Abstract: We investigate how LLMs encode sociodemographic attributes of human conversational partners inferred from indirect cues such as names and occupations. We show that LLMs develop linear representations of user demographics within activation space, wherein stereotypically associated attributes are encoded along interpretable geometric directions. We first probe residual streams across layers of four open transformer-based LLMs (Magistral 24B, Qwen3 14B, GPT-OSS 20B, OLMo2-1B) prompted with explicit demographic disclosure. We show that the same probes predict demographics from implicit cues: names activate census-aligned gender and race representations, while occupations trigger representations correlated with real-world workforce statistics. These linear representations allow us to explain demographic inferences implicitly formed by LLMs during conversation. We demonstrate that these implicit demographic representations actively shape downstream behavior, such as career recommendations. Our study further highlights that models that pass bias benchmark tests may still harbor and leverage implicit biases, with implications for fairness when applied at scale.

</details>


### [13] [Interpretable Embeddings with Sparse Autoencoders: A Data Analysis Toolkit](https://arxiv.org/abs/2512.10092)
*Nick Jiang,Xiaoqing Sun,Lisa Dunlap,Lewis Smith,Neel Nanda*

Main category: cs.AI

TL;DR: 稀疏自编码器（SAE）嵌入比基于LLM的方法成本更低、更可靠，比密集嵌入更可控，可用于大规模文本语料分析，揭示数据集差异、概念相关性等洞察。


<details>
  <summary>Details</summary>
Motivation: 当前分析大规模文本语料的方法存在局限性：基于LLM的技术成本高昂，而密集嵌入模型缺乏对感兴趣属性的控制。需要一种既能控制分析属性又经济高效的方法。

Method: 提出使用稀疏自编码器（SAE）创建SAE嵌入表示，其维度映射到可解释的概念。通过四个数据分析任务验证该方法：数据集语义差异分析、概念相关性发现、基于兴趣轴的文档聚类、属性检索。

Result: SAE嵌入比LLM方法成本低2-8倍，能发现更大的差异，识别偏见更可靠。通过概念过滤可控制聚类方向，在属性检索上优于密集嵌入。案例研究揭示了OpenAI模型行为随时间的变化以及Tulu-3从训练数据中学到的"触发"短语。

Conclusion: SAE是一种多功能的无结构数据分析工具，强调了通过数据解释模型的重要性。该方法在成本效益、可靠性和可控性方面优于现有方法。

Abstract: Analyzing large-scale text corpora is a core challenge in machine learning, crucial for tasks like identifying undesirable model behaviors or biases in training data. Current methods often rely on costly LLM-based techniques (e.g. annotating dataset differences) or dense embedding models (e.g. for clustering), which lack control over the properties of interest. We propose using sparse autoencoders (SAEs) to create SAE embeddings: representations whose dimensions map to interpretable concepts. Through four data analysis tasks, we show that SAE embeddings are more cost-effective and reliable than LLMs and more controllable than dense embeddings. Using the large hypothesis space of SAEs, we can uncover insights such as (1) semantic differences between datasets and (2) unexpected concept correlations in documents. For instance, by comparing model responses, we find that Grok-4 clarifies ambiguities more often than nine other frontier models. Relative to LLMs, SAE embeddings uncover bigger differences at 2-8x lower cost and identify biases more reliably. Additionally, SAE embeddings are controllable: by filtering concepts, we can (3) cluster documents along axes of interest and (4) outperform dense embeddings on property-based retrieval. Using SAE embeddings, we study model behavior with two case studies: investigating how OpenAI model behavior has changed over time and finding "trigger" phrases learned by Tulu-3 (Lambert et al., 2024) from its training data. These results position SAEs as a versatile tool for unstructured data analysis and highlight the neglected importance of interpreting models through their data.

</details>


### [14] [Robust AI Security and Alignment: A Sisyphean Endeavor?](https://arxiv.org/abs/2512.10100)
*Apostol Vassilev*

Main category: cs.AI

TL;DR: 该论文将哥德尔不完备性定理扩展到AI领域，建立了AI安全和对齐的信息论限制，证明了AI系统的认知推理局限性，并提供了应对这些挑战的实用方法。


<details>
  <summary>Details</summary>
Motivation: 了解AI安全和对齐的信息论限制对于负责任地采用AI技术至关重要，需要为这些限制带来的挑战做好准备。

Method: 将哥德尔不完备性定理扩展到AI领域，建立信息论框架来分析AI系统的安全和对齐限制。

Result: 证明了AI安全和对齐存在根本性的信息论限制，这些限制源于哥德尔不完备性定理在AI领域的扩展。

Conclusion: AI系统存在固有的认知推理局限性，了解这些限制并采取相应措施对于负责任地开发和部署AI技术至关重要。

Abstract: This manuscript establishes information-theoretic limitations for robustness of AI security and alignment by extending Gödel's incompleteness theorem to AI. Knowing these limitations and preparing for the challenges they bring is critically important for the responsible adoption of the AI technology. Practical approaches to dealing with these challenges are provided as well. Broader implications for cognitive reasoning limitations of AI systems are also proven.

</details>


### [15] [AgriRegion: Region-Aware Retrieval for High-Fidelity Agricultural Advice](https://arxiv.org/abs/2512.10114)
*Mesafint Fanuel,Mahmoud Nabil Mahmoud,Crystal Cook Marshal,Vishal Lakhotia,Biswanath Dari,Kaushik Roy,Shaohu Zhang*

Main category: cs.AI

TL;DR: AgriRegion是一个专门为农业领域设计的检索增强生成框架，通过地理空间元数据注入和区域优先重排序机制，减少LLM在农业咨询中的上下文幻觉问题，提供高保真、区域感知的农业建议。


<details>
  <summary>Details</summary>
Motivation: 通用大语言模型在农业领域经常出现上下文幻觉问题，提供的建议可能在一个地区科学合理但在另一个地区造成灾难性后果，这是由于土壤、气候和地方法规的差异造成的。需要专门针对农业的区域感知解决方案。

Method: AgriRegion采用检索增强生成框架，包含地理空间元数据注入层和区域优先重排序机制。通过将知识库限制在已验证的本地农业推广服务，并在检索过程中强制执行地理空间约束，确保种植计划、害虫控制和施肥建议的本地准确性。

Result: 实验表明，AgriRegion相比最先进的LLM系统减少了10-20%的幻觉，并显著提高了信任分数。研究创建了包含12个农业子领域160个领域特定问题的新基准数据集AgriRegion-Eval。

Conclusion: AgriRegion框架通过结合地理空间约束和区域优先检索，有效解决了农业领域LLM的上下文幻觉问题，为提供高保真、区域感知的农业咨询提供了可行解决方案。

Abstract: Large Language Models (LLMs) have demonstrated significant potential in democratizing access to information. However, in the domain of agriculture, general-purpose models frequently suffer from contextual hallucination, which provides non-factual advice or answers are scientifically sound in one region but disastrous in another due to variations in soil, climate, and local regulations. We introduce AgriRegion, a Retrieval-Augmented Generation (RAG) framework designed specifically for high-fidelity, region-aware agricultural advisory. Unlike standard RAG approaches that rely solely on semantic similarity, AgriRegion incorporates a geospatial metadata injection layer and a region-prioritized re-ranking mechanism. By restricting the knowledge base to verified local agricultural extension services and enforcing geo-spatial constraints during retrieval, AgriRegion ensures that the advice regarding planting schedules, pest control, and fertilization is locally accurate. We create a novel benchmark dataset, AgriRegion-Eval, which comprises 160 domain-specific questions across 12 agricultural subfields. Experiments demonstrate that AgriRegion reduces hallucinations by 10-20% compared to state-of-the-art LLMs systems and significantly improves trust scores according to a comprehensive evaluation.

</details>


### [16] [The 2025 Foundation Model Transparency Index](https://arxiv.org/abs/2512.10169)
*Alexander Wan,Kevin Klyman,Sayash Kapoor,Nestor Maslej,Shayne Longpre,Betty Xiong,Percy Liang,Rishi Bommasani*

Main category: cs.AI

TL;DR: 2025年基础模型透明度指数显示，主要AI公司透明度从2024年的58分降至2025年的40分，透明度恶化，尤其在训练数据和部署后影响方面最不透明。


<details>
  <summary>Details</summary>
Motivation: 随着基础模型开发公司影响力日益增大，需要持续评估其透明度实践，为政策制定提供依据，解决关键信息缺失问题。

Method: 采用年度基础模型透明度指数评估框架，新增数据获取、使用数据和监控等指标，首次评估阿里巴巴、DeepSeek和xAI等公司。

Result: 平均透明度得分从2024年58分降至2025年40分；IBM得分95分表现最佳，xAI和Midjourney仅14分最低；公司在训练数据和部署后影响方面最不透明。

Conclusion: 基础模型开发公司透明度整体恶化，需要更积极的政策干预来强制透明度，特别是针对训练数据和模型影响的关键信息缺口。

Abstract: Foundation model developers are among the world's most important companies. As these companies become increasingly consequential, how do their transparency practices evolve? The 2025 Foundation Model Transparency Index is the third edition of an annual effort to characterize and quantify the transparency of foundation model developers. The 2025 FMTI introduces new indicators related to data acquisition, usage data, and monitoring and evaluates companies like Alibaba, DeepSeek, and xAI for the first time. The 2024 FMTI reported that transparency was improving, but the 2025 FMTI finds this progress has deteriorated: the average score out of 100 fell from 58 in 2024 to 40 in 2025. Companies are most opaque about their training data and training compute as well as the post-deployment usage and impact of their flagship models. In spite of this general trend, IBM stands out as a positive outlier, scoring 95, in contrast to the lowest scorers, xAI and Midjourney, at just 14. The five members of the Frontier Model Forum we score end up in the middle of the Index: we posit that these companies avoid reputational harms from low scores but lack incentives to be transparency leaders. As policymakers around the world increasingly mandate certain types of transparency, this work reveals the current state of transparency for foundation model developers, how it may change given newly enacted policy, and where more aggressive policy interventions are necessary to address critical information deficits.

</details>


### [17] [An exploration for higher efficiency in multi objective optimisation with reinforcement learning](https://arxiv.org/abs/2512.10208)
*Mehmet Emin Aydin*

Main category: cs.AI

TL;DR: 该论文提出使用多目标强化学习来优化搜索算法中的算子序列选择，旨在解决多目标优化问题中算子序列优化不足的问题。


<details>
  <summary>Details</summary>
Motivation: 优化和搜索过程中的效率问题持续影响算法性能。虽然使用算子池而非单一算子处理邻域移动操作有前景，但最优或近似最优的算子序列仍需深入研究。多目标优化领域在这方面研究较少，需要通用化经验并探索如何利用这些经验。

Method: 提出基于多目标强化学习的通用化方法，该方法包含已完成和待完成的多个阶段，旨在展示多目标强化学习在优化算子序列选择方面的效率。

Result: 论文概述了一个已部分完成的方法框架，展示了多目标强化学习在解决算子序列优化问题上的潜力，但具体实验结果未在摘要中详细说明。

Conclusion: 多目标强化学习为解决多目标优化中的算子序列选择问题提供了有前景的解决方案，该方法框架有望提高优化算法的效率和性能。

Abstract: Efficiency in optimisation and search processes persists to be one of the challenges, which affects the performance and use of optimisation algorithms. Utilising a pool of operators instead of a single operator to handle move operations within a neighbourhood remains promising, but an optimum or near optimum sequence of operators necessitates further investigation. One of the promising ideas is to generalise experiences and seek how to utilise it. Although numerous works are done around this issue for single objective optimisation, multi-objective cases have not much been touched in this regard. A generalised approach based on multi-objective reinforcement learning approach seems to create remedy for this issue and offer good solutions. This paper overviews a generalisation approach proposed with certain stages completed and phases outstanding that is aimed to help demonstrate the efficiency of using multi-objective reinforcement learning.

</details>


### [18] [ID-PaS : Identity-Aware Predict-and-Search for General Mixed-Integer Linear Programs](https://arxiv.org/abs/2512.10211)
*Junyang Cai,El Mehdi Er Raqabi,Pascal Van Hentenryck,Bistra Dilkina*

Main category: cs.AI

TL;DR: 本文扩展了Predict-and-Search框架至参数化混合整数线性规划问题，提出了ID-PaS身份感知学习框架，能更有效地处理异构变量，在多个现实世界大规模问题上优于Gurobi和传统PaS方法。


<details>
  <summary>Details</summary>
Motivation: 现有的Predict-and-Search方法主要局限于二元问题，且忽略了实际应用中常见的固定变量问题，限制了其在参数化混合整数线性规划问题中的应用。

Method: 提出了ID-PaS身份感知学习框架，扩展Predict-and-Search框架至参数化MIPs，使机器学习模型能更有效地处理异构变量，包括固定变量等实际场景中的变量类型。

Result: 在多个现实世界大规模问题上进行实验，ID-PaS框架在性能上持续优于最先进的求解器Gurobi和传统的Predict-and-Search方法。

Conclusion: ID-PaS框架成功扩展了Predict-and-Search方法的应用范围，通过身份感知学习机制有效处理参数化混合整数线性规划中的异构变量问题，在实际应用中展现出优越性能。

Abstract: Mixed-Integer Linear Programs (MIPs) are powerful and flexible tools for modeling a wide range of real-world combinatorial optimization problems. Predict-and-Search methods operate by using a predictive model to estimate promising variable assignments and then guiding a search procedure toward high-quality solutions. Recent research has demonstrated that incorporating machine learning (ML) into the Predict-and-Search framework significantly enhances its performance. Still, it is restricted to binary problems and overlooks the presence of fixed variables that commonly arise in practical settings. This work extends the Predict-and-Search (PaS) framework to parametric MIPs and introduces ID-PaS, an identity-aware learning framework that enables the ML model to handle heterogeneous variables more effectively. Experiments on several real-world large-scale problems demonstrate that ID-PaS consistently achieves superior performance compared to the state-of-the-art solver Gurobi and PaS.

</details>


### [19] [Reverse Thinking Enhances Missing Information Detection in Large Language Models](https://arxiv.org/abs/2512.10273)
*Yuxin Liu,Chaojie Gu,Yihang Zhang,Bin Qian,Shibo He*

Main category: cs.AI

TL;DR: 论文提出了一种基于逆向思维的新框架，通过引导大语言模型进行反向推理来识别缺失信息，相比传统前向推理方法显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在处理缺失信息问题时表现不佳，容易出现回答不完整、事实错误和幻觉等问题。现有的前向推理方法（如思维链、思维树）虽然在某些结构化问题解决中有效，但往往无法系统性地识别和恢复被省略的信息。

Method: 受逆向推理研究的启发，作者提出了一个新颖的逆向思维框架，引导大语言模型通过反向思考来识别必要条件和定位缺失元素。该方法将具有挑战性的缺失信息识别任务转化为更易处理的逆向推理问题。

Result: 实验结果表明，逆向思维方法相比传统前向推理方法取得了显著的性能提升，为大语言模型的逻辑完整性和推理鲁棒性提供了有前景的方向。

Conclusion: 逆向思维方法能够有效增强大语言模型在缺失信息检测任务中的表现，为解决大语言模型在信息不完整情况下的推理问题提供了新的思路。

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities in various reasoning tasks, yet they often struggle with problems involving missing information, exhibiting issues such as incomplete responses, factual errors, and hallucinations. While forward reasoning approaches like Chain-of-Thought (CoT) and Tree-of-Thought (ToT) have shown success in structured problem-solving, they frequently fail to systematically identify and recover omitted information. In this paper, we explore the potential of reverse thinking methodologies to enhance LLMs' performance on missing information detection tasks. Drawing inspiration from recent work on backward reasoning, we propose a novel framework that guides LLMs through reverse thinking to identify necessary conditions and pinpoint missing elements. Our approach transforms the challenging task of missing information identification into a more manageable backward reasoning problem, significantly improving model accuracy. Experimental results demonstrate that our reverse thinking approach achieves substantial performance gains compared to traditional forward reasoning methods, providing a promising direction for enhancing LLMs' logical completeness and reasoning robustness.

</details>


### [20] [Investigating The Functional Roles of Attention Heads in Vision Language Models: Evidence for Reasoning Modules](https://arxiv.org/abs/2512.10300)
*Yanbei Jiang,Xueqi Ma,Shu Liu,Sarah Monazam Erfani,Tongliang Liu,James Bailey,Jey Han Lau,Krista A. Ehinger*

Main category: cs.AI

TL;DR: 提出CogVision数据集和可解释性框架，通过注意力头功能分析揭示视觉语言模型的多模态推理机制


<details>
  <summary>Details</summary>
Motivation: 尽管视觉语言模型在多模态基准测试中表现出色，但其内部机制仍然是一个黑盒。需要系统分析VLMs的内部工作机制，特别是注意力头在多模态推理中的功能角色。

Method: 1. 引入CogVision数据集，将复杂多模态问题分解为逐步子问题，模拟人类链式思维推理；2. 每个子问题关联特定接收或认知功能（如高级视觉接收和推理）；3. 使用基于探针的方法识别专门处理这些功能的注意力头，将其特征化为功能头；4. 在不同VLM家族中进行分析，并进行干预实验。

Result: 1. 功能头普遍稀疏，在不同功能间数量和分布各异；2. 这些头介导交互和层次组织；3. 干预实验显示功能头对多模态推理至关重要：移除功能头导致性能下降，强调功能头则提高准确性。

Conclusion: 研究为VLMs的认知组织提供了新见解，并为设计具有更符合人类感知和推理能力的模型指明了有前景的方向。

Abstract: Despite excelling on multimodal benchmarks, vision-language models (VLMs) largely remain a black box. In this paper, we propose a novel interpretability framework to systematically analyze the internal mechanisms of VLMs, focusing on the functional roles of attention heads in multimodal reasoning. To this end, we introduce CogVision, a dataset that decomposes complex multimodal questions into step-by-step subquestions designed to simulate human reasoning through a chain-of-thought paradigm, with each subquestion associated with specific receptive or cognitive functions such as high-level visual reception and inference. Using a probing-based methodology, we identify attention heads that specialize in these functions and characterize them as functional heads. Our analysis across diverse VLM families reveals that these functional heads are universally sparse, vary in number and distribution across functions, and mediate interactions and hierarchical organization. Furthermore, intervention experiments demonstrate their critical role in multimodal reasoning: removing functional heads leads to performance degradation, while emphasizing them enhances accuracy. These findings provide new insights into the cognitive organization of VLMs and suggest promising directions for designing models with more human-aligned perceptual and reasoning abilities.

</details>


### [21] [Trustworthy Orchestration Artificial Intelligence by the Ten Criteria with Control-Plane Governance](https://arxiv.org/abs/2512.10304)
*Byeong Ho Kang,Wenli Yang,Muhammad Bilal Amin*

Main category: cs.AI

TL;DR: 本文提出了可信编排AI的十大标准框架，旨在将治理架构嵌入AI生态系统执行层面，确保AI决策系统的可验证性、透明度和人类控制。


<details>
  <summary>Details</summary>
Motivation: 随着AI系统在关键决策中扮演越来越重要的角色，技术能力与制度问责之间的差距日益扩大。仅靠伦理指导不足以应对这一挑战，需要将治理嵌入生态系统执行层面的架构。

Method: 提出了可信编排AI的十大标准框架，构建统一的控制面板架构，整合人类输入、语义一致性、审计和溯源完整性。该框架为整个AI组件、消费者和人类参与者提供治理保护伞，借鉴国际标准和澳大利亚国家AI保障框架。

Result: 通过该框架，可信性可以系统地（通过工程化）融入AI系统，确保执行层面保持可验证、透明、可重现且在有效的人类控制之下。

Conclusion: 该研究展示了可信性可以通过工程化方式系统地融入AI系统，为AI生态系统提供全面的治理架构，弥补技术能力与制度问责之间的差距。

Abstract: As Artificial Intelligence (AI) systems increasingly assume consequential decision-making roles, a widening gap has emerged between technical capabilities and institutional accountability. Ethical guidance alone is insufficient to counter this challenge; it demands architectures that embed governance into the execution fabric of the ecosystem. This paper presents the Ten Criteria for Trustworthy Orchestration AI, a comprehensive assurance framework that integrates human input, semantic coherence, audit and provenance integrity into a unified Control-Panel architecture. Unlike conventional agentic AI initiatives that primarily focus on AI-to-AI coordination, the proposed framework provides an umbrella of governance to the entire AI components, their consumers and human participants. By taking aspiration from international standards and Australia's National Framework for AI Assurance initiative, this work demonstrates that trustworthiness can be systematically incorporated (by engineering) into AI systems, ensuring the execution fabric remains verifiable, transparent, reproducible and under meaningful human control.

</details>


### [22] [When Reject Turns into Accept: Quantifying the Vulnerability of LLM-Based Scientific Reviewers to Indirect Prompt Injection](https://arxiv.org/abs/2512.10449)
*Devanshu Sahoo,Manish Prasad,Vasudev Majhi,Jahnvi Singh,Vinay Chamola,Yash Sinha,Murari Mandal,Dhruv Kumar*

Main category: cs.AI

TL;DR: 该研究评估了科学同行评审中LLM评估系统对对抗性PDF操纵的脆弱性，开发了WAVS指标来衡量"拒绝"决策被翻转为"接受"的成功率，发现多种攻击策略能有效操纵评审结果。


<details>
  <summary>Details</summary>
Motivation: 随着LLM在科学同行评审中的广泛应用（包括评审员个人使用和机构正式部署），需要评估这些"LLM作为评审员"系统对对抗性攻击的鲁棒性，特别是针对将"拒绝"决策翻转为"接受"这一特定激励的攻击。

Method: 研究收集了200篇科学论文数据集，针对将"拒绝"翻转为"接受"的目标，调整了15种领域特定的攻击策略，在包括GPT-5、Claude Haiku和DeepSeek在内的13个语言模型上进行评估，开发了WAVS（加权对抗性脆弱性得分）作为评估指标。

Result: 结果显示，如"Maximum Mark Magyk"等混淆策略能成功操纵评分，即使在大型模型中也能达到令人担忧的决策翻转率，表明当前LLM评审系统存在严重安全漏洞。

Conclusion: 科学同行评审中的LLM评估系统对对抗性PDF操纵存在显著脆弱性，需要加强安全措施。研究将发布完整数据集和注入框架以促进该领域更多研究。

Abstract: The landscape of scientific peer review is rapidly evolving with the integration of Large Language Models (LLMs). This shift is driven by two parallel trends: the widespread individual adoption of LLMs by reviewers to manage workload (the "Lazy Reviewer" hypothesis) and the formal institutional deployment of AI-powered assessment systems by conferences like AAAI and Stanford's Agents4Science. This study investigates the robustness of these "LLM-as-a-Judge" systems (both illicit and sanctioned) to adversarial PDF manipulation. Unlike general jailbreaks, we focus on a distinct incentive: flipping "Reject" decisions to "Accept," for which we develop a novel evaluation metric which we term as WAVS (Weighted Adversarial Vulnerability Score). We curated a dataset of 200 scientific papers and adapted 15 domain-specific attack strategies to this task, evaluating them across 13 Language Models, including GPT-5, Claude Haiku, and DeepSeek. Our results demonstrate that obfuscation strategies like "Maximum Mark Magyk" successfully manipulate scores, achieving alarming decision flip rates even in large-scale models. We will release our complete dataset and injection framework to facilitate more research on this topic.

</details>


### [23] [EpiPlanAgent: Agentic Automated Epidemic Response Planning](https://arxiv.org/abs/2512.10313)
*Kangkun Mao,Fang Xu,Jinru Ding,Yidong Jiang,Yujun Yao,Yirong Chen,Junming Liu,Xiaoqin Wu,Qian Wu,Xiaoyan Huang,Jie Xu*

Main category: cs.AI

TL;DR: EpiPlanAgent是一个基于大语言模型的智能体系统，能够自动生成和验证数字应急响应计划，显著提高计划完整性和指南一致性，同时大幅缩短开发时间。


<details>
  <summary>Details</summary>
Motivation: 传统的流行病应对计划制定依赖劳动密集型的人工方法，效率低下且耗时。本研究旨在设计一个自动化系统来改进这一过程，提高公共卫生应急准备的效率和效果。

Method: 开发了EpiPlanAgent系统，这是一个基于大语言模型的多智能体框架，集成了任务分解、知识基础和模拟模块。公共卫生专业人员使用真实世界疫情场景在受控评估中测试该系统。

Result: EpiPlanAgent显著提高了计划的完整性和指南一致性，同时大幅减少了开发时间。专家评估确认AI生成内容与人工编写内容具有高度一致性，用户反馈显示系统具有很高的感知效用。

Conclusion: EpiPlanAgent为智能流行病应对计划提供了一个有效、可扩展的解决方案，展示了智能体AI在转变公共卫生准备方面的潜力。

Abstract: Epidemic response planning is essential yet traditionally reliant on labor-intensive manual methods. This study aimed to design and evaluate EpiPlanAgent, an agent-based system using large language models (LLMs) to automate the generation and validation of digital emergency response plans. The multi-agent framework integrated task decomposition, knowledge grounding, and simulation modules. Public health professionals tested the system using real-world outbreak scenarios in a controlled evaluation. Results demonstrated that EpiPlanAgent significantly improved the completeness and guideline alignment of plans while drastically reducing development time compared to manual workflows. Expert evaluation confirmed high consistency between AI-generated and human-authored content. User feedback indicated strong perceived utility. In conclusion, EpiPlanAgent provides an effective, scalable solution for intelligent epidemic response planning, demonstrating the potential of agentic AI to transform public health preparedness.

</details>


### [24] [User-Feedback-Driven Continual Adaptation for Vision-and-Language Navigation](https://arxiv.org/abs/2512.10322)
*Yongqiang Yu,Xuhui Li,Hazza Mahmood,Jinxing Zhou,Haodong Hong,Longtao Jiang,Zhiqiang Xu,Qi Wu,Xiaojun Chang*

Main category: cs.AI

TL;DR: 提出用户反馈驱动的视觉语言导航适应框架，通过整合人类交互提升GSA-VLN性能，使用记忆库预热机制稳定部署


<details>
  <summary>Details</summary>
Motivation: 现有GSA-VLN框架缺乏用户反馈机制，仅依赖无监督环境适应，而实际应用中用户反馈能提供有价值的监督信号，显著提升适应质量

Method: 1) 将用户反馈（导航指令和纠正信号）转化为高质量环境对齐训练数据；2) 引入记忆库预热机制，重用先前获取的环境知识，缓解冷启动问题；3) 在GSA-R2R基准上测试，支持持续和混合适应设置

Result: 在GSA-R2R基准上超越GR-DUET等基线方法，提高导航成功率和路径效率；记忆库预热稳定早期导航并减少更新后的性能下降；在持续和混合适应设置下均表现出鲁棒性和通用性

Conclusion: 用户反馈驱动的适应框架有效提升了GSA-VLN性能，通过系统整合人类交互和知识重用机制，缩小了静态基准测试与实际部署之间的差距

Abstract: Vision-and-Language Navigation (VLN) requires agents to navigate complex environments by following natural-language instructions. General Scene Adaptation for VLN (GSA-VLN) shifts the focus from zero-shot generalization to continual, environment-specific adaptation, narrowing the gap between static benchmarks and real-world deployment. However, current GSA-VLN frameworks exclude user feedback, relying solely on unsupervised adaptation from repeated environmental exposure. In practice, user feedback offers natural and valuable supervision that can significantly enhance adaptation quality. We introduce a user-feedback-driven adaptation framework that extends GSA-VLN by systematically integrating human interactions into continual learning. Our approach converts user feedback-navigation instructions and corrective signals-into high-quality, environment-aligned training data, enabling efficient and realistic adaptation. A memory-bank warm-start mechanism further reuses previously acquired environmental knowledge, mitigating cold-start degradation and ensuring stable redeployment. Experiments on the GSA-R2R benchmark show that our method consistently surpasses strong baselines such as GR-DUET, improving navigation success and path efficiency. The memory-bank warm start stabilizes early navigation and reduces performance drops after updates. Results under both continual and hybrid adaptation settings confirm the robustness and generality of our framework, demonstrating sustained improvement across diverse deployment conditions.

</details>


### [25] [On the Collapse of Generative Paths: A Criterion and Correction for Diffusion Steering](https://arxiv.org/abs/2512.10339)
*Ziseok Lee,Minyeong Hwang,Sanghyun Jo,Wooyeol Lee,Jihyung Ko,Young Bin Park,Jae-Mun Choi,Eunho Yang,Kyungsu Kim*

Main category: cs.AI

TL;DR: 本文提出ACE方法解决概率密度比引导中的边缘路径崩溃问题，使异构扩散模型能可靠组合用于可控生成


<details>
  <summary>Details</summary>
Motivation: 现有的概率密度比方法在组合异构扩散模型时存在边缘路径崩溃问题，导致中间密度不可归一化，限制了在分子设计等任务中的应用

Method: 1) 推导路径存在性准则，从噪声调度和指数预测崩溃；2) 提出自适应路径校正指数(ACE)，扩展Feynman-Kac引导到时变指数，保证有效概率路径

Result: 在合成2D基准和柔性姿态支架装饰任务中，ACE消除了崩溃，实现高引导组合生成，在分布和对接指标上优于恒定指数基线和专用任务模型

Conclusion: ACE将异构专家密度比引导从不稳定的启发式方法转变为可控生成的可靠工具

Abstract: Inference-time steering enables pretrained diffusion/flow models to be adapted to new tasks without retraining. A widely used approach is the ratio-of-densities method, which defines a time-indexed target path by reweighting probability-density trajectories from multiple models with positive, or in some cases, negative exponents. This construction, however, harbors a critical and previously unformalized failure mode: Marginal Path Collapse, where intermediate densities become non-normalizable even though endpoints remain valid. Collapse arises systematically when composing heterogeneous models trained on different noise schedules or datasets, including a common setting in molecular design where de-novo, conformer, and pocket-conditioned models must be combined for tasks such as flexible-pose scaffold decoration. We provide a novel and complete solution for the problem. First, we derive a simple path existence criterion that predicts exactly when collapse occurs from noise schedules and exponents alone. Second, we introduce Adaptive path Correction with Exponents (ACE), which extends Feynman-Kac steering to time-varying exponents and guarantees a valid probability path. On a synthetic 2D benchmark and on flexible-pose scaffold decoration, ACE eliminates collapse and enables high-guidance compositional generation, improving distributional and docking metrics over constant-exponent baselines and even specialized task-specific scaffold decoration models. Our work turns ratio-of-densities steering with heterogeneous experts from an unstable heuristic into a reliable tool for controllable generation.

</details>


### [26] [REMISVFU: Vertical Federated Unlearning via Representation Misdirection for Intermediate Output Feature](https://arxiv.org/abs/2512.10348)
*Wenhan Wu,Zhili He,Huanghuang Liang,Yili Gong,Jiawei Jiang,Chuang Hu,Dazhao Cheng*

Main category: cs.AI

TL;DR: REMISVFU是一个用于垂直联邦学习的表示误导框架，通过将遗忘方的编码器输出坍缩到单位球面上的随机锚点，实现快速客户端级遗忘，同时保持其他参与方的模型效用。


<details>
  <summary>Details</summary>
Motivation: GDPR等数据保护法规赋予联邦系统参与者被遗忘权。现有遗忘技术主要针对水平联邦学习（HFL），而垂直联邦学习（VFL）的特征分区架构使得HFL导向的遗忘方法无效。需要开发专门针对VFL的遗忘解决方案。

Method: 提出REMISVFU框架：当收到删除请求时，遗忘方将其编码器输出坍缩到单位球面上的随机锚点，切断其特征与全局模型之间的统计联系。服务器通过联合优化保留损失和遗忘损失，并通过正交投影对齐梯度来消除破坏性干扰。

Result: 在公共基准测试中，REMISVFU将后门攻击成功率抑制到自然类别先验水平，仅牺牲约2.5个百分点的干净准确率，优于现有最先进的基线方法。

Conclusion: REMISVFU为垂直联邦学习提供了一种有效的即插即用表示误导框架，能够在保护参与者被遗忘权的同时，保持剩余参与方的模型效用，实现了隐私保护与模型性能的良好平衡。

Abstract: Data-protection regulations such as the GDPR grant every participant in a federated system a right to be forgotten. Federated unlearning has therefore emerged as a research frontier, aiming to remove a specific party's contribution from the learned model while preserving the utility of the remaining parties. However, most unlearning techniques focus on Horizontal Federated Learning (HFL), where data are partitioned by samples. In contrast, Vertical Federated Learning (VFL) allows organizations that possess complementary feature spaces to train a joint model without sharing raw data. The resulting feature-partitioned architecture renders HFL-oriented unlearning methods ineffective. In this paper, we propose REMISVFU, a plug-and-play representation misdirection framework that enables fast, client-level unlearning in splitVFL systems. When a deletion request arrives, the forgetting party collapses its encoder output to a randomly sampled anchor on the unit sphere, severing the statistical link between its features and the global model. To maintain utility for the remaining parties, the server jointly optimizes a retention loss and a forgetting loss, aligning their gradients via orthogonal projection to eliminate destructive interference. Evaluations on public benchmarks show that REMISVFU suppresses back-door attack success to the natural class-prior level and sacrifices only about 2.5% points of clean accuracy, outperforming state-of-the-art baselines.

</details>


### [27] [LLM-Empowered Representation Learning for Emerging Item Recommendation](https://arxiv.org/abs/2512.10370)
*Ziying Zhang,Quanming Yao,Yaqing Wang*

Main category: cs.AI

TL;DR: EmerFlow是一个基于大语言模型的表示学习框架，用于解决新兴物品推荐问题，通过LLM推理丰富特征、对齐现有嵌入空间和元学习优化，在有限交互下学习表达性嵌入。


<details>
  <summary>Details</summary>
Motivation: 现有推荐方法往往忽视新兴物品随时间逐渐积累交互的动态过程，通常假设新兴物品只有很少甚至没有历史交互。这种假设过于简化问题，因为好的模型需要同时保留新兴物品的独特性并利用其与成熟物品的共享模式。

Method: EmerFlow框架：1）通过LLM推理丰富新兴物品的原始特征；2）将这些表示与现有推荐模型的嵌入空间对齐；3）通过元学习整合新交互来优化嵌入。这使得EmerFlow能够从有限的交互中学习表达性嵌入。

Result: 在包括电影和医药等多个领域的广泛实验中，EmerFlow始终优于现有方法。

Conclusion: EmerFlow通过结合LLM推理、嵌入对齐和元学习，有效解决了新兴物品推荐问题，能够在有限交互下学习到表达性的嵌入表示。

Abstract: In this work, we tackle the challenge of recommending emerging items, whose interactions gradually accumulate over time. Existing methods often overlook this dynamic process, typically assuming that emerging items have few or even no historical interactions. Such an assumption oversimplifies the problem, as a good model must preserve the uniqueness of emerging items while leveraging their shared patterns with established ones. To address this challenge, we propose EmerFlow, a novel LLM-empowered representation learning framework that generates distinctive embeddings for emerging items. It first enriches the raw features of emerging items through LLM reasoning, then aligns these representations with the embedding space of the existing recommendation model. Finally, new interactions are incorporated through meta-learning to refine the embeddings. This enables EmerFlow to learn expressive embeddings for emerging items from only limited interactions. Extensive experiments across diverse domains, including movies and pharmaceuticals, show that EmerFlow consistently outperforms existing methods.

</details>


### [28] [AgentProg: Empowering Long-Horizon GUI Agents with Program-Guided Context Management](https://arxiv.org/abs/2512.10371)
*Shizuo Tian,Hao Wen,Yuxuan Chen,Jiacheng Liu,Shanhui Zhao,Guohong Liu,Ju Ren,Yunxin Liu,Yuanchun Li*

Main category: cs.AI

TL;DR: AgentProg：一种基于程序引导的移动GUI智能体上下文管理方法，通过将交互历史重构为带变量和控制流的程序来减少上下文开销，在长时程任务中保持高性能。


<details>
  <summary>Details</summary>
Motivation: 移动GUI智能体的快速发展推动了长时程任务自动化的研究，但现有方法依赖不断扩展的交互历史导致巨大的上下文开销，而现有的上下文管理和压缩技术往往无法保留关键的语义信息，导致任务性能下降。

Method: 提出AgentProg，一种程序引导的智能体上下文管理方法：1）将交互历史重构为带有变量和控制流的程序；2）基于程序结构提供原则性机制来确定哪些信息应保留、哪些可丢弃；3）集成受信念MDP框架启发的全局信念状态机制来处理部分可观测性并适应意外环境变化。

Result: 在AndroidWorld和扩展的长时程任务套件上的实验表明，AgentProg在这些基准测试中达到了最先进的成功率。更重要的是，在长时程任务中保持稳健性能，而基线方法则出现灾难性性能下降。

Conclusion: AgentProg通过程序引导的上下文管理有效解决了移动GUI智能体在长时程任务中的上下文开销问题，在保持高性能的同时显著优于现有方法，为智能体上下文管理提供了新思路。

Abstract: The rapid development of mobile GUI agents has stimulated growing research interest in long-horizon task automation. However, building agents for these tasks faces a critical bottleneck: the reliance on ever-expanding interaction history incurs substantial context overhead. Existing context management and compression techniques often fail to preserve vital semantic information, leading to degraded task performance. We propose AgentProg, a program-guided approach for agent context management that reframes the interaction history as a program with variables and control flow. By organizing information according to the structure of program, this structure provides a principled mechanism to determine which information should be retained and which can be discarded. We further integrate a global belief state mechanism inspired by Belief MDP framework to handle partial observability and adapt to unexpected environmental changes. Experiments on AndroidWorld and our extended long-horizon task suite demonstrate that AgentProg has achieved the state-of-the-art success rates on these benchmarks. More importantly, it maintains robust performance on long-horizon tasks while baseline methods experience catastrophic degradation. Our system is open-sourced at https://github.com/MobileLLM/AgentProg.

</details>


### [29] [Boosting RL-Based Visual Reasoning with Selective Adversarial Entropy Intervention](https://arxiv.org/abs/2512.10414)
*Yang Yu,Zhuangzhuang Chen,Siqi Wang,Lanqing Li,Xiaomeng Li*

Main category: cs.AI

TL;DR: 本文提出SaEI方法，通过选择性对抗熵干预增强视觉语言模型的推理能力，在RL采样阶段引入熵干预来提升响应多样性。


<details>
  <summary>Details</summary>
Motivation: 现有基于强化学习的微调方法主要关注策略优化阶段的熵干预，忽略了RL采样阶段的熵干预对提升响应多样性和GRPO性能的重要性。

Method: 提出选择性对抗熵干预(SaEI)：1) 熵引导对抗采样(EgAS)，将采样响应的熵作为对抗目标，生成对抗样本来扩大答案空间探索；2) 令牌选择性熵计算(TsEC)，在不扭曲事实知识的前提下最大化对抗攻击效果。

Result: 在领域内和领域外数据集上的大量实验表明，该方法能显著提升策略探索能力，从而增强推理能力。

Conclusion: 通过在RL采样阶段引入选择性对抗熵干预，SaEI方法有效提升了视觉语言模型的推理性能，证明了采样阶段熵干预的重要性。

Abstract: Recently, reinforcement learning (RL) has become a common choice in enhancing the reasoning capabilities of vision-language models (VLMs). Considering existing RL- based finetuning methods, entropy intervention turns out to be an effective way to benefit exploratory ability, thereby improving policy performance. Notably, most existing stud- ies intervene in entropy by simply controlling the update of specific tokens during policy optimization of RL. They ig- nore the entropy intervention during the RL sampling that can boost the performance of GRPO by improving the di- versity of responses. In this paper, we propose Selective- adversarial Entropy Intervention, namely SaEI, which en- hances policy entropy by distorting the visual input with the token-selective adversarial objective coming from the en- tropy of sampled responses. Specifically, we first propose entropy-guided adversarial sampling (EgAS) that formu- lates the entropy of sampled responses as an adversarial ob- jective. Then, the corresponding adversarial gradient can be used to attack the visual input for producing adversarial samples, allowing the policy model to explore a larger an- swer space during RL sampling. Then, we propose token- selective entropy computation (TsEC) to maximize the ef- fectiveness of adversarial attack in EgAS without distorting factual knowledge within VLMs. Extensive experiments on both in-domain and out-of-domain datasets show that our proposed method can greatly improve policy exploration via entropy intervention, to boost reasoning capabilities. Code will be released once the paper is accepted.

</details>


### [30] [Representation of the structure of graphs by sequences of instructions](https://arxiv.org/abs/2512.10429)
*Ezequiel Lopez-Rubio*

Main category: cs.AI

TL;DR: 提出一种新的图表示方法，将邻接矩阵转换为可逆的指令字符串，便于深度学习语言模型处理图数据。


<details>
  <summary>Details</summary>
Motivation: 当前图表示方法（基于邻接矩阵）不适合深度学习语言模型处理，而深度学习语言模型在文本处理方面表现出强大能力，需要一种能与这些模型兼容的图表示方法。

Method: 提出将图的邻接矩阵转换为一系列简单指令的字符串表示，这些指令逐步构建邻接矩阵，该转换是可逆的，即可以从图生成字符串，也可以从字符串重建图。

Result: 该方法能紧凑地表示图，同时保持图的局部结构模式，初步计算实验显示出有利结果。

Conclusion: 这种新的图表示方法有望提升深度学习模型处理图数据的能力，为图处理与深度学习语言模型的结合提供了新途径。

Abstract: The representation of graphs is commonly based on the adjacency matrix concept. This formulation is the foundation of most algebraic and computational approaches to graph processing. The advent of deep learning language models offers a wide range of powerful computational models that are specialized in the processing of text. However, current procedures to represent graphs are not amenable to processing by these models. In this work, a new method to represent graphs is proposed. It represents the adjacency matrix of a graph by a string of simple instructions. The instructions build the adjacency matrix step by step. The transformation is reversible, i.e. given a graph the string can be produced and vice versa. The proposed representation is compact and it maintains the local structural patterns of the graph. Therefore, it is envisaged that it could be useful to boost the processing of graphs by deep learning models. A tentative computational experiment is reported, with favorable results.

</details>


### [31] [Targeted Data Protection for Diffusion Model by Matching Training Trajectory](https://arxiv.org/abs/2512.10433)
*Hojun Lee,Mijin Koo,Yeji Song,Nojun Kwak*

Main category: cs.AI

TL;DR: TAFAP是一种通过控制整个训练轨迹来实现有效目标数据保护的新方法，解决了现有方法因快照匹配而导致的控制性差的问题


<details>
  <summary>Details</summary>
Motivation: 扩散模型的个性化微调技术虽然越来越容易获得，但也引发了未经授权数据使用和隐私侵犯的重大担忧。当前保护方法仅限于被动降低图像质量，无法实现稳定控制。现有的目标数据保护方法由于采用快照匹配方法，未能考虑完整的学习动态，导致控制性差

Method: TAFAP（通过对抗性扰动微调进行轨迹对齐）采用轨迹匹配方法，受数据集蒸馏启发，在整个微调过程中强制执行持久、可验证的转换。与快照匹配方法不同，TAFAP控制整个训练轨迹，确保保护效果不会随着训练进展而被稀释

Result: TAFAP在扩散模型中首次成功实现了目标转换，同时控制身份和视觉模式。该方法显著优于现有的目标数据保护尝试，实现了向目标概念的稳健重定向，同时保持高图像质量

Conclusion: TAFAP实现了可验证的安全保障，并为控制和追踪扩散模型输出的改变提供了一个新框架。这是第一个通过控制整个训练轨迹成功实现有效目标数据保护的方法

Abstract: Recent advancements in diffusion models have made fine-tuning text-to-image models for personalization increasingly accessible, but have also raised significant concerns regarding unauthorized data usage and privacy infringement. Current protection methods are limited to passively degrading image quality, failing to achieve stable control. While Targeted Data Protection (TDP) offers a promising paradigm for active redirection toward user-specified target concepts, existing TDP attempts suffer from poor controllability due to snapshot-matching approaches that fail to account for complete learning dynamics. We introduce TAFAP (Trajectory Alignment via Fine-tuning with Adversarial Perturbations), the first method to successfully achieve effective TDP by controlling the entire training trajectory. Unlike snapshot-based methods whose protective influence is easily diluted as training progresses, TAFAP employs trajectory-matching inspired by dataset distillation to enforce persistent, verifiable transformations throughout fine-tuning. We validate our method through extensive experiments, demonstrating the first successful targeted transformation in diffusion models with simultaneous control over both identity and visual patterns. TAFAP significantly outperforms existing TDP attempts, achieving robust redirection toward target concepts while maintaining high image quality. This work enables verifiable safeguards and provides a new framework for controlling and tracing alterations in diffusion model outputs.

</details>


### [32] [Achieving Olympia-Level Geometry Large Language Model Agent via Complexity Boosting Reinforcement Learning](https://arxiv.org/abs/2512.10534)
*Haiteng Zhao,Junhao Shen,Yiming Zhang,Songyang Gao,Kuikun Liu,Tianyou Ma,Fan Zheng,Dahua Lin,Wenwei Zhang,Kai Chen*

Main category: cs.AI

TL;DR: InternGeometry是一个基于LLM的几何问题解决智能体，通过迭代提出命题和辅助构造、符号引擎验证和反馈反思，在仅使用13K训练样本的情况下，解决了44/50个IMO几何问题，超越了金牌选手平均分。


<details>
  <summary>Details</summary>
Motivation: 当前LLM在几何问题解决方面仍受限于启发式辅助构造能力，而专家模型如AlphaGeometry 2需要大规模数据合成和搜索。本研究旨在构建一个金牌级别的LLM智能体来解决几何问题，克服启发式限制。

Method: 1) 迭代式方法：提出命题和辅助构造 → 符号引擎验证 → 基于反馈反思指导后续提案；2) 动态记忆机制：支持每个问题与符号引擎进行200多次交互；3) 复杂度提升强化学习(CBRL)：在训练阶段逐渐增加合成问题的复杂度。

Result: InternGeometry在InternThinker-32B基础上，解决了2000-2024年50个IMO几何问题中的44个（88%），超过金牌选手平均分40.9，仅使用13K训练样本（AlphaGeometry 2数据量的0.004%）。还能为IMO问题提出人类解法中未出现的新颖辅助构造。

Conclusion: InternGeometry展示了LLM智能体在专家级几何任务上的潜力，通过创新的迭代验证和动态记忆机制，在极小数据量下达到金牌水平，为几何问题解决提供了新的研究方向。

Abstract: Large language model (LLM) agents exhibit strong mathematical problem-solving abilities and can even solve International Mathematical Olympiad (IMO) level problems with the assistance of formal proof systems. However, due to weak heuristics for auxiliary constructions, AI for geometry problem solving remains dominated by expert models such as AlphaGeometry 2, which rely heavily on large-scale data synthesis and search for both training and evaluation. In this work, we make the first attempt to build a medalist-level LLM agent for geometry and present InternGeometry. InternGeometry overcomes the heuristic limitations in geometry by iteratively proposing propositions and auxiliary constructions, verifying them with a symbolic engine, and reflecting on the engine's feedback to guide subsequent proposals. A dynamic memory mechanism enables InternGeometry to conduct more than two hundred interactions with the symbolic engine per problem. To further accelerate learning, we introduce Complexity-Boosting Reinforcement Learning (CBRL), which gradually increases the complexity of synthesized problems across training stages. Built on InternThinker-32B, InternGeometry solves 44 of 50 IMO geometry problems (2000-2024), exceeding the average gold medalist score (40.9), using only 13K training examples, just 0.004% of the data used by AlphaGeometry 2, demonstrating the potential of LLM agents on expert-level geometry tasks. InternGeometry can also propose novel auxiliary constructions for IMO problems that do not appear in human solutions. We will release the model, data, and symbolic engine to support future research.

</details>


### [33] [NormCode: A Semi-Formal Language for Context-Isolated AI Planning](https://arxiv.org/abs/2512.10563)
*Xin Guan*

Main category: cs.AI

TL;DR: NormCode是一种半形式化语言，通过严格分离语义操作和语法操作，消除多步LLM工作流中的上下文污染问题，实现可审计的AI工作流。


<details>
  <summary>Details</summary>
Motivation: 多步LLM工作流存在上下文污染问题：随着信息在步骤间积累，模型会产生幻觉、混淆中间输出并丢失任务约束。需要一种方法来消除跨步骤污染，实现透明、可审计的工作流。

Method: NormCode是一种半形式化语言，用于构建推理计划的结构化分解。它强制分离语义操作（LLM驱动的推理，非确定性）和语法操作（确定性数据重构），每个步骤在数据隔离中运行，仅接收显式传递的输入。语言有三种同构格式：.ncds用于人工编写，.ncd用于机器执行，.ncn用于人工验证。

Result: 通过两个演示验证NormCode：(1) 任意长度输入的base X加法算法达到100%准确率；(2) NormCode自身五阶段编译器管道的自托管执行。工作编排器提供依赖驱动调度、SQLite支持的检查点和循环管理。

Conclusion: NormCode通过设计消除跨步骤污染，使AI工作流可审计，解决了法律推理、医疗决策和金融分析等高风险领域对透明度的关键需求。

Abstract: Multistep workflows that chain large language model (LLM) calls suffer from context pollution: as information accumulates across steps, models hallucinate, confuse intermediate outputs, and lose track of task constraints. We present NormCode, a semiformal language for constructing plans of inferences, structured decompositions where each step operates in data isolation and receives only explicitly passed inputs, which eliminates crossstep contamination by design. NormCode enforces a strict separation between semantic operations (LLMdriven reasoning, nondeterministic) and syntactic operations (deterministic data restructuring), enabling precise cost and reliability tracing. The language exists in three isomorphic formats: .ncds for human authoring, .ncd for machine execution, and .ncn for human verification, supporting progressive formalization from sketch to production. We validate NormCode through two demonstrations: (1) a base X addition algorithm achieving 100 percent accuracy on arbitrary length inputs, and (2) self hosted execution of NormCode's own five phase compiler pipeline. The working orchestrator provides dependency driven scheduling, SQLite backed checkpointing, and loop management, making AI workflows auditable by design and addressing a critical need for transparency in high stakes domains such as legal reasoning, medical decision making, and financial analysis.

</details>


### [34] [Phythesis: Physics-Guided Evolutionary Scene Synthesis for Energy-Efficient Data Center Design via LLMs](https://arxiv.org/abs/2512.10611)
*Minghao LI,Ruihang Wang,Rui Tan,Yonggang Wen*

Main category: cs.AI

TL;DR: Phythesis框架结合大语言模型和物理引导进化优化，自动化生成仿真就绪的数据中心场景，实现能效优化设计


<details>
  <summary>Details</summary>
Motivation: 传统数据中心设计方法依赖人工经验和专用仿真工具，难以应对日益复杂的系统需求。现有生成式AI方法不考虑底层物理约束，无法满足数据中心可量化运营目标和严格物理限制的设计要求

Method: 提出Phythesis框架，采用迭代双层优化架构：1）LLM驱动优化层生成物理合理的三维布局并进行自我批评以优化场景拓扑；2）物理信息优化层识别最优资产参数并选择最佳资产组合

Result: 在三种生成规模上的实验表明，与基于纯LLM的解决方案相比，Phythesis实现了57.3%的生成成功率提升和11.5%的电源使用效率（PUE）改进

Conclusion: Phythesis成功将LLM与物理引导优化相结合，为数据中心能效设计提供了一种自动化、仿真就续的场景合成方法，有效解决了传统方法的扩展性问题和现有AI方法缺乏物理约束的局限性

Abstract: Data center (DC) infrastructure serves as the backbone to support the escalating demand for computing capacity. Traditional design methodologies that blend human expertise with specialized simulation tools scale poorly with the increasing system complexity. Recent studies adopt generative artificial intelligence to design plausible human-centric indoor layouts. However, they do not consider the underlying physics, making them unsuitable for the DC design that sets quantifiable operational objectives and strict physical constraints. To bridge the gap, we propose Phythesis, a novel framework that synergizes large language models (LLMs) and physics-guided evolutionary optimization to automate simulation-ready (SimReady) scene synthesis for energy-efficient DC design. Phythesis employs an iterative bi-level optimization architecture, where (i) the LLM-driven optimization level generates physically plausible three-dimensional layouts and self-criticizes them to refine the scene topology, and (ii) the physics-informed optimization level identifies the optimal asset parameters and selects the best asset combination. Experiments on three generation scales show that Phythesis achieves 57.3% generation success rate increase and 11.5% power usage effectiveness (PUE) improvement, compared with the vanilla LLM-based solution.

</details>


### [35] [Refinement Contrastive Learning of Cell-Gene Associations for Unsupervised Cell Type Identification](https://arxiv.org/abs/2512.10640)
*Liang Peng,Haopeng Liu,Yixuan Ye,Cheng Liu,Wenjun Shen,Si Wu,Hau-San Wong*

Main category: cs.AI

TL;DR: scRCL是一个用于单细胞组学研究的无监督细胞类型识别框架，通过整合细胞-基因相互作用来获得更具信息量的细胞表征，显著提升了细胞类型识别的准确性。


<details>
  <summary>Details</summary>
Motivation: 现有的单细胞聚类方法大多只关注细胞内在结构，忽略了细胞-基因关联的关键作用，这限制了它们区分密切相关的细胞类型的能力。需要一种能够同时利用细胞-细胞结构和细胞-基因关联的方法来改进细胞类型识别。

Method: 提出了Refinement Contrastive Learning框架(scRCL)，包含两个对比分布对齐组件来揭示可靠的细胞内在结构，以及一个整合基因相关结构学习的精炼模块，通过捕捉潜在的细胞-基因关联来增强细胞嵌入表示。

Result: 在多个单细胞RNA-seq和空间转录组学基准数据集上的实验表明，该方法在细胞类型识别准确性方面始终优于现有最先进方法。下游生物学分析证实恢复的细胞群体表现出连贯的基因表达特征。

Conclusion: scRCL通过明确整合细胞-基因相互作用，能够学习更具生物学意义的细胞表征，显著提高无监督细胞类型识别的性能，为单细胞组学研究提供了有效的分析工具。

Abstract: Unsupervised cell type identification is crucial for uncovering and characterizing heterogeneous populations in single cell omics studies. Although a range of clustering methods have been developed, most focus exclusively on intrinsic cellular structure and ignore the pivotal role of cell-gene associations, which limits their ability to distinguish closely related cell types. To this end, we propose a Refinement Contrastive Learning framework (scRCL) that explicitly incorporates cell-gene interactions to derive more informative representations. Specifically, we introduce two contrastive distribution alignment components that reveal reliable intrinsic cellular structures by effectively exploiting cell-cell structural relationships. Additionally, we develop a refinement module that integrates gene-correlation structure learning to enhance cell embeddings by capturing underlying cell-gene associations. This module strengthens connections between cells and their associated genes, refining the representation learning to exploiting biologically meaningful relationships. Extensive experiments on several single-cell RNA-seq and spatial transcriptomics benchmark datasets demonstrate that our method consistently outperforms state-of-the-art baselines in cell-type identification accuracy. Moreover, downstream biological analyses confirm that the recovered cell populations exhibit coherent gene-expression signatures, further validating the biological relevance of our approach. The code is available at https://github.com/THPengL/scRCL.

</details>


### [36] [CAPTAIN: Semantic Feature Injection for Memorization Mitigation in Text-to-Image Diffusion Models](https://arxiv.org/abs/2512.10655)
*Tong Zhang,Carlos Hinojosa,Bernard Ghanem*

Main category: cs.AI

TL;DR: CAPTAIN是一个无需训练的框架，通过在去噪过程中直接修改潜在特征来减少扩散模型的记忆化问题，在保持提示对齐的同时显著降低训练示例的复制。


<details>
  <summary>Details</summary>
Motivation: 扩散模型可能会无意中复制训练示例，这引发了隐私和版权问题。现有的推理时缓解方法（如操纵无分类器引导或扰动提示嵌入）往往难以在不损害提示对齐的情况下减少记忆化。

Method: CAPTAIN采用三阶段方法：1) 基于频率的噪声初始化，减少去噪早期复制记忆模式的倾向；2) 识别特征注入的最佳去噪时间步并定位记忆区域；3) 将非记忆参考图像的语义对齐特征注入到定位的潜在区域中。

Result: 实验表明，与基于CFG的基线方法相比，CAPTAIN在保持与预期提示强对齐的同时，实现了记忆化的显著减少。

Conclusion: CAPTAIN提供了一个有效的训练免费框架，能够在扩散模型中缓解记忆化问题，平衡了隐私保护与生成质量的需求。

Abstract: Diffusion models can unintentionally reproduce training examples, raising privacy and copyright concerns as these systems are increasingly deployed at scale. Existing inference-time mitigation methods typically manipulate classifier-free guidance (CFG) or perturb prompt embeddings; however, they often struggle to reduce memorization without compromising alignment with the conditioning prompt. We introduce CAPTAIN, a training-free framework that mitigates memorization by directly modifying latent features during denoising. CAPTAIN first applies frequency-based noise initialization to reduce the tendency to replicate memorized patterns early in the denoising process. It then identifies the optimal denoising timesteps for feature injection and localizes memorized regions. Finally, CAPTAIN injects semantically aligned features from non-memorized reference images into localized latent regions, suppressing memorization while preserving prompt fidelity and visual quality. Our experiments show that CAPTAIN achieves substantial reductions in memorization compared to CFG-based baselines while maintaining strong alignment with the intended prompt.

</details>


### [37] [On the Dynamics of Multi-Agent LLM Communities Driven by Value Diversity](https://arxiv.org/abs/2512.10665)
*Muhua Huang,Qinlin Zhao,Xiaoyuan Yi,Xing Xie*

Main category: cs.AI

TL;DR: 研究探讨了价值观多样性如何影响AI多智能体系统的集体行为，发现适度多样性增强价值稳定性、促进涌现行为和创造性原则，但极端异质性会导致不稳定性


<details>
  <summary>Details</summary>
Motivation: 随着基于大语言模型的多智能体系统日益普及，这些人工社区的集体行为（如集体智能）受到越来越多的关注。本研究旨在回答一个基本问题：价值观多样性如何塑造AI社区的集体行为？

Method: 使用基于施瓦茨基本人类价值观理论的自然主义价值观诱导方法，构建多智能体模拟，让不同规模的社区参与开放式互动和宪法制定过程

Result: 价值观多样性增强了价值稳定性，促进了涌现行为，并带来了更多由智能体自身开发、无需外部指导的创造性原则。但这些效应也显示出边际递减：极端异质性会引发不稳定性

Conclusion: 本研究将价值观多样性定位为未来AI能力的新维度，在AI能力与社会学制度涌现研究之间架起了桥梁

Abstract: As Large Language Models (LLM) based multi-agent systems become increasingly prevalent, the collective behaviors, e.g., collective intelligence, of such artificial communities have drawn growing attention. This work aims to answer a fundamental question: How does diversity of values shape the collective behavior of AI communities? Using naturalistic value elicitation grounded in the prevalent Schwartz's Theory of Basic Human Values, we constructed multi-agent simulations where communities with varying numbers of agents engaged in open-ended interactions and constitution formation. The results show that value diversity enhances value stability, fosters emergent behaviors, and brings more creative principles developed by the agents themselves without external guidance. However, these effects also show diminishing returns: extreme heterogeneity induces instability. This work positions value diversity as a new axis of future AI capability, bridging AI ability and sociological studies of institutional emergence.

</details>


### [38] [Challenges of Evaluating LLM Safety for User Welfare](https://arxiv.org/abs/2512.10687)
*Manon Kempermann,Sai Suresh Macharla Vasu,Mahalakshmi Raveenthiran,Theo Farrell,Ingmar Weber*

Main category: cs.AI

TL;DR: 该研究挑战了传统大语言模型安全评估只关注普遍风险的局限性，提出需要针对个人福利的上下文感知评估方法，特别是在金融和健康等高风险领域。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型安全评估主要关注普遍风险（如危险能力），但数百万用户将LLM用于金融、健康等高风险个人建议，这些危害是上下文依赖的而非普遍的。虽然OECD等框架认识到需要评估个体风险，但用户福利安全评估仍然不成熟。

Method: 研究采用探索性方法，评估GPT-5、Claude Sonnet 4和Gemini 2.5 Pro在金融和健康领域的建议。首先比较有无用户上下文时评估者的安全评分差异；其次测试用户实际会披露的上下文信息是否能改善评估效果。

Result: 1. 上下文盲评估者比了解用户情况的评估者给出显著更高的安全评分，高脆弱性用户的安全评分从5/7（安全）降至3/7（有些不安全）。2. 即使用户提示包含他们报告会披露的上下文信息，评估效果也没有显著改善。3. 现实用户上下文披露本身不足以确保有效评估，特别是对脆弱人群。

Conclusion: 有效的用户福利安全评估需要评估者基于多样化用户档案评估响应，仅靠现实用户上下文披露是不够的。研究提供了上下文感知评估的方法论起点，证明评估个体福利需要不同于现有普遍风险框架的方法。

Abstract: Safety evaluations of large language models (LLMs) typically focus on universal risks like dangerous capabilities or undesirable propensities. However, millions use LLMs for personal advice on high-stakes topics like finance and health, where harms are context-dependent rather than universal. While frameworks like the OECD's AI classification recognize the need to assess individual risks, user-welfare safety evaluations remain underdeveloped. We argue that developing such evaluations is non-trivial due to fundamental questions about accounting for user context in evaluation design. In this exploratory study, we evaluated advice on finance and health from GPT-5, Claude Sonnet 4, and Gemini 2.5 Pro across user profiles of varying vulnerability. First, we demonstrate that evaluators must have access to rich user context: identical LLM responses were rated significantly safer by context-blind evaluators than by those aware of user circumstances, with safety scores for high-vulnerability users dropping from safe (5/7) to somewhat unsafe (3/7). One might assume this gap could be addressed by creating realistic user prompts containing key contextual information. However, our second study challenges this: we rerun the evaluation on prompts containing context users report they would disclose, finding no significant improvement. Our work establishes that effective user-welfare safety evaluation requires evaluators to assess responses against diverse user profiles, as realistic user context disclosure alone proves insufficient, particularly for vulnerable populations. By demonstrating a methodology for context-aware evaluation, this study provides both a starting point for such assessments and foundational evidence that evaluating individual welfare demands approaches distinct from existing universal-risk frameworks. We publish our code and dataset to aid future developments.

</details>


### [39] [Enhancing Radiology Report Generation and Visual Grounding using Reinforcement Learning](https://arxiv.org/abs/2512.10691)
*Benjamin Gundersen,Nicolas Deperrois,Samuel Ruiperez-Campillo,Thomas M. Sutter,Julia E. Vogt,Michael Moor,Farhad Nooralahzadeh,Michael Krauthammer*

Main category: cs.AI

TL;DR: 该研究探讨了强化学习（RL）和显式推理（thinking）在胸部X光视觉语言模型中的应用效果，发现RL能提升报告生成和视觉定位性能，而显式推理则无明显改进。


<details>
  <summary>Details</summary>
Motivation: 当前医学视觉语言模型主要依赖监督微调（SFT），仅优化下一个令牌预测而不评估答案质量。强化学习能整合任务特定反馈，结合显式推理在数学和编程任务中已显示显著优势，研究旨在探索这些方法在胸部X光解读中的效果。

Method: 基于Qwen3-VL构建RadVLM，进行大规模CXR数据的SFT，然后通过冷启动SFT阶段赋予模型基本推理能力。应用基于临床任务特定奖励的GRPO强化学习进行报告生成和视觉定位，在有无推理的领域特定和通用领域变体上进行匹配RL实验。

Result: 强SFT对基础性能至关重要，RL在两个任务上均提供额外增益，而显式推理未进一步改善结果。在统一评估流程下，RL优化的RadVLM模型超越基线对应模型，在报告生成和定位任务上达到最先进性能。

Conclusion: 临床对齐的强化学习是医学视觉语言模型中监督微调的有力补充，能显著提升模型性能，而显式推理在胸部X光解读任务中效果有限。

Abstract: Recent advances in vision-language models (VLMs) have improved Chest X-ray (CXR) interpretation in multiple aspects. However, many medical VLMs rely solely on supervised fine-tuning (SFT), which optimizes next-token prediction without evaluating answer quality. In contrast, reinforcement learning (RL) can incorporate task-specific feedback, and its combination with explicit intermediate reasoning ("thinking") has demonstrated substantial gains on verifiable math and coding tasks. To investigate the effects of RL and thinking in a CXR VLM, we perform large-scale SFT on CXR data to build an updated RadVLM based on Qwen3-VL, followed by a cold-start SFT stage that equips the model with basic thinking ability. We then apply Group Relative Policy Optimization (GRPO) with clinically grounded, task-specific rewards for report generation and visual grounding, and run matched RL experiments on both domain-specific and general-domain Qwen3-VL variants, with and without thinking. Across these settings, we find that while strong SFT remains crucial for high base performance, RL provides additional gains on both tasks, whereas explicit thinking does not appear to further improve results. Under a unified evaluation pipeline, the RL-optimized RadVLM models outperform their baseline counterparts and reach state-of-the-art performance on both report generation and grounding, highlighting clinically aligned RL as a powerful complement to SFT for medical VLMs.

</details>


### [40] [Remember Me, Refine Me: A Dynamic Procedural Memory Framework for Experience-Driven Agent Evolution](https://arxiv.org/abs/2512.10696)
*Zouying Cao,Jiaji Deng,Li Yu,Weikang Zhou,Zhaoyang Liu,Bolin Ding,Hai Zhao*

Main category: cs.AI

TL;DR: ReMe是一个用于LLM智能体经验驱动进化的记忆框架，通过多层面提炼、上下文自适应重用和基于效用的精炼机制，实现从静态存储到动态推理的转变，显著提升智能体性能。


<details>
  <summary>Details</summary>
Motivation: 现有LLM智能体记忆框架主要采用"被动积累"范式，将记忆视为静态的只增不减档案，无法实现动态推理和有效经验复用，需要更智能的记忆生命周期管理机制。

Method: 提出ReMe框架，包含三个创新机制：1)多层面提炼：识别成功模式、分析失败触发因素、生成比较性见解；2)上下文自适应重用：通过场景感知索引将历史见解适配到新情境；3)基于效用的精炼：自主添加有效记忆并修剪过时记忆，保持紧凑高质量经验池。

Result: 在BFCL-V3和AppWorld基准测试中达到最先进水平，观察到显著的内存缩放效应：配备ReMe的Qwen3-8B超越无记忆的更大模型Qwen3-14B，表明自进化记忆为终身学习提供了计算高效的途径。

Conclusion: ReMe框架成功将记忆从静态存储转变为动态推理工具，通过智能记忆管理实现了LLM智能体的经验驱动进化，为终身学习系统提供了有效的计算高效解决方案。

Abstract: Procedural memory enables large language model (LLM) agents to internalize "how-to" knowledge, theoretically reducing redundant trial-and-error. However, existing frameworks predominantly suffer from a "passive accumulation" paradigm, treating memory as a static append-only archive. To bridge the gap between static storage and dynamic reasoning, we propose $\textbf{ReMe}$ ($\textit{Remember Me, Refine Me}$), a comprehensive framework for experience-driven agent evolution. ReMe innovates across the memory lifecycle via three mechanisms: 1) $\textit{multi-faceted distillation}$, which extracts fine-grained experiences by recognizing success patterns, analyzing failure triggers and generating comparative insights; 2) $\textit{context-adaptive reuse}$, which tailors historical insights to new contexts via scenario-aware indexing; and 3) $\textit{utility-based refinement}$, which autonomously adds valid memories and prunes outdated ones to maintain a compact, high-quality experience pool. Extensive experiments on BFCL-V3 and AppWorld demonstrate that ReMe establishes a new state-of-the-art in agent memory system. Crucially, we observe a significant memory-scaling effect: Qwen3-8B equipped with ReMe outperforms larger, memoryless Qwen3-14B, suggesting that self-evolving memory provides a computation-efficient pathway for lifelong learning. We release our code and the $\texttt{reme.library}$ dataset to facilitate further research.

</details>


### [41] [COMPARE: Clinical Optimization with Modular Planning and Assessment via RAG-Enhanced AI-OCT: Superior Decision Support for Percutaneous Coronary Intervention Compared to ChatGPT-5 and Junior Operators](https://arxiv.org/abs/2512.10702)
*Wei Fang,Chiyao Wang,Wenshuai Ma,Hui Liu,Jianqiang Hu,Xiaona Niu,Yi Chu,Mingming Zhang,Jingxiao Yang,Dongwei Zhang,Zelin Li,Pengyun Liu,Jiawei Zheng,Pengke Zhang,Chaoshi Qin,Wangang Guo,Bin Wang,Yugang Xue,Wei Zhang,Zikuan Wang,Rui Zhu,Yihui Cao,Quanmao Lu,Rui Meng,Yan Li*

Main category: cs.AI

TL;DR: CA-GPT AI-OCT系统在OCT引导PCI规划和评估中，相比通用ChatGPT-5和初级医师表现出显著优越的决策一致性


<details>
  <summary>Details</summary>
Motivation: 虽然血管内成像（特别是OCT）能改善PCI结果，但其解读依赖操作者经验。通用AI缺乏领域特异性可靠性，需要开发专门针对OCT引导PCI的AI系统

Method: 单中心分析96例接受OCT引导PCI的患者，比较CA-GPT、ChatGPT-5和初级医师生成的程序决策与专家记录的一致性，使用10个预设指标评估术前和术后阶段

Result: 术前规划：CA-GPT中位一致性评分(5[3.75-5])显著高于ChatGPT-5(3[2-4])和初级医师(4[3-4])。支架直径选择(90.3% vs 72.2%)和长度选择(80.6% vs 52.8%)显著优于初级医师。术后评估：CA-GPT整体一致性(5[4.75-5])显著高于ChatGPT-5(4[4-5])和初级医师(5[4-5])。亚组分析显示在复杂场景中保持优势

Conclusion: CA-GPT为基础的AI-OCT系统在PCI规划和评估阶段相比通用大语言模型和初级医师实现了更优越的决策一致性，为血管内成像解读提供了标准化可靠方法，有望增强操作者专业能力并优化OCT引导PCI

Abstract: Background: While intravascular imaging, particularly optical coherence tomography (OCT), improves percutaneous coronary intervention (PCI) outcomes, its interpretation is operator-dependent. General-purpose artificial intelligence (AI) shows promise but lacks domain-specific reliability. We evaluated the performance of CA-GPT, a novel large model deployed on an AI-OCT system, against that of the general-purpose ChatGPT-5 and junior physicians for OCT-guided PCI planning and assessment.
  Methods: In this single-center analysis of 96 patients who underwent OCT-guided PCI, the procedural decisions generated by the CA-GPT, ChatGPT-5, and junior physicians were compared with an expert-derived procedural record. Agreement was assessed using ten pre-specified metrics across pre-PCI and post-PCI phases.
  Results: For pre-PCI planning, CA-GPT demonstrated significantly higher median agreement scores (5[IQR 3.75-5]) compared to both ChatGPT-5 (3[2-4], P<0.001) and junior physicians (4[3-4], P<0.001). CA-GPT significantly outperformed ChatGPT-5 across all individual pre-PCI metrics and showed superior performance to junior physicians in stent diameter (90.3% vs. 72.2%, P<0.05) and length selection (80.6% vs. 52.8%, P<0.01). In post-PCI assessment, CA-GPT maintained excellent overall agreement (5[4.75-5]), significantly higher than both ChatGPT-5 (4[4-5], P<0.001) and junior physicians (5[4-5], P<0.05). Subgroup analysis confirmed CA-GPT's robust performance advantage in complex scenarios.
  Conclusion: The CA-GPT-based AI-OCT system achieved superior decision-making agreement versus a general-purpose large language model and junior physicians across both PCI planning and assessment phases. This approach provides a standardized and reliable method for intravascular imaging interpretation, demonstrating significant potential to augment operator expertise and optimize OCT-guided PCI.

</details>


### [42] [Replace, Don't Expand: Mitigating Context Dilution in Multi-Hop RAG via Fixed-Budget Evidence Assembly](https://arxiv.org/abs/2512.10787)
*Moshe Lahmy,Roi Yozevitch*

Main category: cs.AI

TL;DR: SEAL-RAG提出了一种"替换而非扩展"的策略来解决RAG系统中多跳查询的上下文稀释问题，通过搜索-提取-评估-循环的机制主动替换干扰信息，在固定检索深度下显著提升答案正确性和证据精度。


<details>
  <summary>Details</summary>
Motivation: 现有的RAG系统在多跳查询中经常失败，因为初始检索可能遗漏关键桥梁事实。传统的纠正方法（如Self-RAG、CRAG、Adaptive-k）通常通过增加上下文或修剪现有列表来解决问题，但这往往导致上下文稀释，即干扰信息挤占了相关信息的空间。

Method: SEAL-RAG采用训练免费的控制器，执行搜索→提取→评估→循环的周期：1）进行实时的实体锚定提取，构建实时的缺口规范（缺失的实体/关系）；2）触发有针对性的微查询；3）使用实体优先排序主动用填补缺口的证据替换干扰信息。该方法在固定检索深度k下工作，采用"替换而非扩展"的策略。

Result: 在HotpotQA（k=3）上，SEAL比Self-RAG在答案正确性上提升了3-13个百分点，证据精度提升了12-18个百分点。在2WikiMultiHopQA（k=5）上，比Adaptive-k在准确率上提升了8.0个百分点，并保持了96%的证据精度（CRAG仅为22%）。这些提升在统计学上具有显著性（p<0.001）。

Conclusion: SEAL-RAG通过强制执行固定k替换策略，在保持可预测成本配置的同时，确保前k个位置针对精度而非广度进行优化。该方法有效解决了上下文稀释问题，在多跳问答任务上显著优于现有方法。

Abstract: Retrieval-Augmented Generation (RAG) systems often fail on multi-hop queries when the initial retrieval misses a bridge fact. Prior corrective approaches, such as Self-RAG, CRAG, and Adaptive-$k$, typically address this by \textit{adding} more context or pruning existing lists. However, simply expanding the context window often leads to \textbf{context dilution}, where distractors crowd out relevant information. We propose \textbf{SEAL-RAG}, a training-free controller that adopts a \textbf{``replace, don't expand''} strategy to fight context dilution under a fixed retrieval depth $k$. SEAL executes a (\textbf{S}earch $\rightarrow$ \textbf{E}xtract $\rightarrow$ \textbf{A}ssess $\rightarrow$ \textbf{L}oop) cycle: it performs on-the-fly, entity-anchored extraction to build a live \textit{gap specification} (missing entities/relations), triggers targeted micro-queries, and uses \textit{entity-first ranking} to actively swap out distractors for gap-closing evidence. We evaluate SEAL-RAG against faithful re-implementations of Basic RAG, CRAG, Self-RAG, and Adaptive-$k$ in a shared environment on \textbf{HotpotQA} and \textbf{2WikiMultiHopQA}. On HotpotQA ($k=3$), SEAL improves answer correctness by \textbf{+3--13 pp} and evidence precision by \textbf{+12--18 pp} over Self-RAG. On 2WikiMultiHopQA ($k=5$), it outperforms Adaptive-$k$ by \textbf{+8.0 pp} in accuracy and maintains \textbf{96\%} evidence precision compared to 22\% for CRAG. These gains are statistically significant ($p<0.001$). By enforcing fixed-$k$ replacement, SEAL yields a predictable cost profile while ensuring the top-$k$ slots are optimized for precision rather than mere breadth. We release our code and data at https://github.com/mosherino/SEAL-RAG.

</details>


### [43] [HAROOD: A Benchmark for Out-of-distribution Generalization in Sensor-based Human Activity Recognition](https://arxiv.org/abs/2512.10807)
*Wang Lu,Yao Zhu,Jindong Wang*

Main category: cs.AI

TL;DR: HAROOD：一个用于人类活动识别（HAR）在分布外（OOD）场景下的综合基准测试，涵盖4种OOD场景、6个数据集、16种方法，发现没有单一方法在所有场景中表现最佳


<details>
  <summary>Details</summary>
Motivation: 现实场景中人类活动识别面临个体、设备、环境、时间等变化带来的分布偏移问题，现有OOD算法仅在特定场景下测试，缺乏全面评估和比较

Method: 提出HAROOD基准测试，定义4种OOD场景（跨人、跨位置、跨数据集、跨时间），构建包含6个数据集、16种比较方法（基于CNN和Transformer架构）的测试平台，采用两种模型选择协议

Result: 通过大量实验发现：没有单一方法在所有OOD场景中持续优于其他方法；现有OOD算法在HAR任务中仍有很大改进空间；提供了模块化代码库便于扩展

Conclusion: HAROOD为OOD场景下的人类活动识别研究提供了全面基准，揭示了该领域的重要挑战和机遇，促进了该方向的研究发展

Abstract: Sensor-based human activity recognition (HAR) mines activity patterns from the time-series sensory data. In realistic scenarios, variations across individuals, devices, environments, and time introduce significant distributional shifts for the same activities. Recent efforts attempt to solve this challenge by applying or adapting existing out-of-distribution (OOD) algorithms, but only in certain distribution shift scenarios (e.g., cross-device or cross-position), lacking comprehensive insights on the effectiveness of these algorithms. For instance, is OOD necessary to HAR? Which OOD algorithm performs the best? In this paper, we fill this gap by proposing HAROOD, a comprehensive benchmark for HAR in OOD settings. We define 4 OOD scenarios: cross-person, cross-position, cross-dataset, and cross-time, and build a testbed covering 6 datasets, 16 comparative methods (implemented with CNN-based and Transformer-based architectures), and two model selection protocols. Then, we conduct extensive experiments and present several findings for future research, e.g., no single method consistently outperforms others, highlighting substantial opportunity for advancement. Our codebase is highly modular and easy to extend for new datasets, algorithms, comparisons, and analysis, with the hope to facilitate the research in OOD-based HAR. Our implementation is released and can be found at https://github.com/AIFrontierLab/HAROOD.

</details>


### [44] [Agile Deliberation: Concept Deliberation for Subjective Visual Classification](https://arxiv.org/abs/2512.10821)
*Leijie Wang,Otilia Stretcu,Wei Qiao,Thomas Denby,Krishnamurthy Viswanathan,Enming Luo,Chun-Ta Lu,Tushar Dogra,Ranjay Krishna,Ariel Fuxman*

Main category: cs.AI

TL;DR: 论文提出"敏捷审议"框架，通过边界案例暴露帮助用户迭代定义模糊视觉概念，相比自动分解基线提升7.5% F1分数


<details>
  <summary>Details</summary>
Motivation: 现有视觉概念分类系统假设用户有清晰稳定的概念理解，但现实中用户常从模糊概念开始，需要通过"概念审议"过程迭代细化。内容审核专家的访谈揭示了这一实践需求。

Method: 提出"敏捷审议"人机交互框架，包含两个阶段：1)概念范围界定-将初始概念分解为结构化子概念层次；2)概念迭代-呈现语义边界案例供用户反思和反馈，迭代对齐图像分类器与用户意图。

Result: 通过18个用户会话(每个1.5小时)评估，敏捷审议比自动分解基线获得7.5%更高的F1分数，比手动审议高3%以上。参与者报告概念理解更清晰，认知努力更低。

Conclusion: 敏捷审议框架有效支持主观和演化概念的定义，通过结构化审议过程帮助用户从模糊概念开始迭代细化，在内容审核等应用中具有实用价值。

Abstract: From content moderation to content curation, applications requiring vision classifiers for visual concepts are rapidly expanding. Existing human-in-the-loop approaches typically assume users begin with a clear, stable concept understanding to be able to provide high-quality supervision. In reality, users often start with a vague idea and must iteratively refine it through "concept deliberation", a practice we uncovered through structured interviews with content moderation experts. We operationalize the common strategies in deliberation used by real content moderators into a human-in-the-loop framework called "Agile Deliberation" that explicitly supports evolving and subjective concepts. The system supports users in defining the concept for themselves by exposing them to borderline cases. The system does this with two deliberation stages: (1) concept scoping, which decomposes the initial concept into a structured hierarchy of sub-concepts, and (2) concept iteration, which surfaces semantically borderline examples for user reflection and feedback to iteratively align an image classifier with the user's evolving intent. Since concept deliberation is inherently subjective and interactive, we painstakingly evaluate the framework through 18 user sessions, each 1.5h long, rather than standard benchmarking datasets. We find that Agile Deliberation achieves 7.5% higher F1 scores than automated decomposition baselines and more than 3% higher than manual deliberation, while participants reported clearer conceptual understanding and lower cognitive effort.

</details>


### [45] [V-OCBF: Learning Safety Filters from Offline Data via Value-Guided Offline Control Barrier Functions](https://arxiv.org/abs/2512.10822)
*Mumuksh Tayal,Manan Tayal,Aditya Singh,Shishir Kolathaya,Ravi Prakash*

Main category: cs.AI

TL;DR: V-OCBF：一种从离线演示中学习神经控制屏障函数的框架，无需系统动力学模型，通过有限差分屏障更新实现模型无关的安全控制合成


<details>
  <summary>Details</summary>
Motivation: 现有安全离线强化学习方法通常只能保证软约束，无法确保前向不变性；而控制屏障函数虽然能提供严格安全保证，但依赖专家设计的屏障函数或完整的系统动力学知识。需要一种既能从离线数据学习、又能提供严格安全保证的方法。

Method: 提出值引导离线控制屏障函数框架：1）从离线演示中学习神经CBF，不假设动力学模型；2）推导递归有限差分屏障更新，实现模型无关的屏障学习；3）采用期望分位数目标，避免在分布外动作上查询屏障；4）将学习到的屏障与二次规划结合，合成实时安全控制。

Result: 在多个案例研究中，V-OCBF相比基线方法显著减少了安全违规次数，同时保持了强大的任务性能，展示了其在无需在线交互或手工设计屏障的情况下合成安全关键控制器的可扩展性。

Conclusion: V-OCBF框架成功实现了从离线数据中学习控制屏障函数，为自主系统提供了严格的安全保证，解决了现有方法在模型依赖和安全保证之间的权衡问题。

Abstract: Ensuring safety in autonomous systems requires controllers that satisfy hard, state-wise constraints without relying on online interaction. While existing Safe Offline RL methods typically enforce soft expected-cost constraints, they do not guarantee forward invariance. Conversely, Control Barrier Functions (CBFs) provide rigorous safety guarantees but usually depend on expert-designed barrier functions or full knowledge of the system dynamics. We introduce Value-Guided Offline Control Barrier Functions (V-OCBF), a framework that learns a neural CBF entirely from offline demonstrations. Unlike prior approaches, V-OCBF does not assume access to the dynamics model; instead, it derives a recursive finite-difference barrier update, enabling model-free learning of a barrier that propagates safety information over time. Moreover, V-OCBF incorporates an expectile-based objective that avoids querying the barrier on out-of-distribution actions and restricts updates to the dataset-supported action set. The learned barrier is then used with a Quadratic Program (QP) formulation to synthesize real-time safe control. Across multiple case studies, V-OCBF yields substantially fewer safety violations than baseline methods while maintaining strong task performance, highlighting its scalability for offline synthesis of safety-critical controllers without online interaction or hand-engineered barriers.

</details>


### [46] [LLMs Can Assist with Proposal Selection at Large User Facilities](https://arxiv.org/abs/2512.10895)
*Lijie Ding,Janell Thomson,Jon Taylor,Changwoo Do*

Main category: cs.AI

TL;DR: LLMs可用于大型用户设施的提案筛选，提供比传统人工评审更可扩展、一致且经济高效的替代方案，在排名和识别高发表潜力提案方面表现不亚于人类，成本降低两个数量级。


<details>
  <summary>Details</summary>
Motivation: 传统人工提案评审存在提案间相关性弱、评审者偏见和不一致等问题。基于成对偏好的方法在逻辑上更优越，但二次方工作量对人类评审者不切实际。LLMs可以解决这一限制。

Method: 利用LLMs进行提案排名，采用成对偏好方法。基于橡树岭国家实验室散裂中子源三个光束线的精心策划提案和发表记录，评估LLM排名与人类排名的相关性，并比较识别高发表潜力提案的能力。

Result: LLM排名与人类排名强相关（Spearman ρ≈0.2-0.8，去除10%异常值后提升至≥0.5）。在识别高发表潜力提案方面，LLM表现不亚于人类评审者，同时成本降低超过两个数量级。LLMs还能进行提案相似性定量评估等高级分析。

Conclusion: LLMs为大型用户设施的提案筛选提供了可扩展、一致且经济高效的解决方案，不仅能替代传统人工评审，还能实现人类难以进行的高级分析，为评审委员会提供关键信息。

Abstract: We explore how large language models (LLMs) can enhance the proposal selection process at large user facilities, offering a scalable, consistent, and cost-effective alternative to traditional human review. Proposal selection depends on assessing the relative strength among submitted proposals; however, traditional human scoring often suffers from weak inter-proposal correlations and is subject to reviewer bias and inconsistency. A pairwise preference-based approach is logically superior, providing a more rigorous and internally consistent basis for ranking, but its quadratic workload makes it impractical for human reviewers. We address this limitation using LLMs. Leveraging the uniquely well-curated proposals and publication records from three beamlines at the Spallation Neutron Source (SNS), Oak Ridge National Laboratory (ORNL), we show that the LLM rankings correlate strongly with the human rankings (Spearman $ρ\simeq 0.2-0.8$, improving to $\geq 0.5$ after 10\% outlier removal). Moreover, LLM performance is no worse than that of human reviewers in identifying proposals with high publication potential, while costing over two orders of magnitude less. Beyond ranking, LLMs enable advanced analyses that are challenging for humans, such as quantitative assessment of proposal similarity via embedding models, which provides information crucial for review committees.

</details>


### [47] [Multi-Granular Node Pruning for Circuit Discovery](https://arxiv.org/abs/2512.10903)
*Muhammad Umair Haider,Hammad Rizwan,Hassan Sajjad,A. B. Siddique*

Main category: cs.AI

TL;DR: 提出了一种用于电路发现（circuit discovery）的节点级剪枝框架，解决了现有方法在可扩展性和粒度方面的限制，能够识别更细粒度的电路结构（如单个神经元），同时显著降低内存占用。


<details>
  <summary>Details</summary>
Motivation: 现有电路发现方法主要依赖迭代边缘剪枝，计算成本高且仅限于粗粒度单元（如注意力头或MLP块），忽略了更细粒度的结构如单个神经元。需要解决可扩展性和粒度限制的问题。

Method: 提出节点级剪枝框架，引入跨多个粒度级别的可学习掩码（从整个块到单个神经元），在统一的优化目标中通过粒度特定的稀疏性惩罚指导剪枝过程，实现单次微调运行中的全面压缩。

Result: 该方法发现的电路节点数比先前方法更少；证明许多被粗粒度方法认为重要的神经元实际上无关紧要，同时仍能保持任务性能；内存占用显著降低5-10倍，因为不需要在内存中保存中间激活。

Conclusion: 提出的节点级剪枝框架在电路发现中实现了更好的可扩展性和粒度控制，能够识别更精确的电路结构，同时大幅降低计算资源需求，为理解大型语言模型内部工作机制提供了更有效的工具。

Abstract: Circuit discovery aims to identify minimal subnetworks that are responsible for specific behaviors in large language models (LLMs). Existing approaches primarily rely on iterative edge pruning, which is computationally expensive and limited to coarse-grained units such as attention heads or MLP blocks, overlooking finer structures like individual neurons. We propose a node-level pruning framework for circuit discovery that addresses both scalability and granularity limitations. Our method introduces learnable masks across multiple levels of granularity, from entire blocks to individual neurons, within a unified optimization objective. Granularity-specific sparsity penalties guide the pruning process, allowing a comprehensive compression in a single fine-tuning run. Empirically, our approach identifies circuits that are smaller in nodes than those discovered by prior methods; moreover, we demonstrate that many neurons deemed important by coarse methods are actually irrelevant, while still maintaining task performance. Furthermore, our method has a significantly lower memory footprint, 5-10x, as it does not require keeping intermediate activations in the memory to work.

</details>


### [48] [On Decision-Making Agents and Higher-Order Causal Processes](https://arxiv.org/abs/2512.10937)
*Matt Wilson*

Main category: cs.AI

TL;DR: 该论文建立了部分可观测马尔可夫决策过程（POMDP）中的决策智能体与一输入过程函数（高阶量子操作的经典极限）之间的精确对应关系。


<details>
  <summary>Details</summary>
Motivation: 探索人工智能中的决策理论与量子信息理论之间的深层联系，为理解智能体决策过程提供新的数学框架和物理视角。

Method: 通过将智能体的策略和记忆更新结合成过程函数w，使用链接积与POMDP环境交互，建立智能体与过程函数之间的对应关系，并扩展到多智能体系统。

Result: 成功建立了POMDP智能体与一输入过程函数之间的精确对应，揭示了两种视角的对称性：物理视角中过程函数作为环境，AI视角中过程函数编码智能体。

Conclusion: 该对应关系为理解智能体决策提供了新的理论框架，将AI决策理论与量子信息理论联系起来，并为多智能体系统的分析开辟了新途径。

Abstract: We establish a precise correspondence between decision-making agents in partially observable Markov decision processes (POMDPs) and one-input process functions, the classical limit of higher-order quantum operations. In this identification an agent's policy and memory update combine into a process function w that interacts with a POMDP environment via the link product. This suggests a dual interpretation: in the physics view, the process function acts as the environment into which local operations (agent interventions) are inserted, whereas in the AI view it encodes the agent and the inserted functions represent environments. We extend this perspective to multi-agent systems by identifying observation-independent decentralized POMDPs as natural domains for multi-input process functions.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [49] [IoTEdu: Access Control, Detection, and Automatic Incident Response in Academic IoT Networks](https://arxiv.org/abs/2512.09934)
*Joner Assolin,Diego Kreutz,Leandro Bertholdo*

Main category: cs.CR

TL;DR: IoTEdu是一个集成平台，通过访问控制、事件检测和自动阻断功能，解决学术环境中IoT设备带来的安全和管理复杂性。


<details>
  <summary>Details</summary>
Motivation: 学术环境中IoT设备的增长增加了操作复杂性并暴露了安全弱点，特别是缺乏统一政策来管理IoT设备的注册、监控和事件响应。

Method: 开发IoTEdu集成平台，结合访问控制、事件检测和自动阻断功能，并在受控环境中通过模拟攻击进行评估。

Result: 在模拟攻击测试中，平均检测到阻断时间为28.6秒，减少了人工干预，实现了响应标准化，统一了注册、监控和事件响应流程。

Conclusion: IoTEdu平台有效解决了学术环境中IoT设备的安全管理问题，通过自动化流程提高了安全响应效率并降低了操作复杂性。

Abstract: The growing presence of IoT devices in academic environments has increased operational complexity and exposed security weaknesses, especially in academic institutions without unified policies for registration, monitoring, and incident response involving IoT. This work presents IoTEdu, an integrated platform that combines access control, incident detection, and automatic blocking of IoT devices. The solution was evaluated in a controlled environment with simulated attacks, achieving an average time of 28.6 seconds between detection and blocking. The results show a reduction in manual intervention, standardization of responses, and unification of the processes of registration, monitoring, and incident response.

</details>


### [50] [Blockchain-Anchored Audit Trail Model for Transparent Inter-Operator Settlement](https://arxiv.org/abs/2512.09938)
*Balakumar Ravindranath Kunthu,Ranganath Nagesh Taware,Sathish Krishna Anumula*

Main category: cs.CR

TL;DR: 区块链审计追踪模型实现电信和金融服务行业运营商间结算的透明化、不可篡改和自动化，显著降低交易成本、缩短结算周期并提高审计完整性。


<details>
  <summary>Details</summary>
Motivation: 电信和金融服务行业的运营商间结算流程面临重大挑战：结算周期长（超过120天）、交易成本高（占收入约5%）、缺乏实时透明度，传统机制依赖多个中介和人工流程。

Method: 提出基于区块链的审计追踪模型，利用分布式账本技术、智能合约自动化和加密验证，建立统一、防篡改的交易记录框架。

Result: 交易费用降低87%，结算周期从120天压缩至3分钟，审计追踪完整性达100%；智能合约减少92%人工干预，消除88%结算争议；机构采用率从2020年8%增至2024年4月的52%。

Conclusion: 区块链锚定的审计追踪模型能有效解决运营商间结算的效率和透明度问题，具备可扩展性（每秒12,000笔交易）、互操作性和跨司法管辖区的监管合规性，预计年投资额将达92亿美元。

Abstract: The telecommunications and financial services industries face substantial challenges in inter-operator settlement processes, characterized by extended reconciliation cycles, high transaction costs, and limited real-time transparency. Traditional settlement mechanisms rely on multiple intermediaries and manual procedures, resulting in settlement periods exceeding 120 days with operational costs consuming approximately 5 percent of total revenue. This research presents a blockchain-anchored audit trail model enabling transparent, immutable, and automated inter-operator settlement. The framework leverages distributed ledger technology, smart contract automation, and cryptographic verification to establish a unified, tamper-proof transaction record. Empirical evaluation demonstrates 87 percent reduction in transaction fees, settlement cycle compression from 120 days to 3 minutes, and 100 percent audit trail integrity. Smart contract automation reduces manual intervention by 92 percent and eliminates 88 percent of settlement disputes. Market analysis indicates institutional adoption accelerated from 8 percent in 2020 to 52 percent by April 2024, with projected industry investment reaching 9.2 billion USD annually. The framework addresses scalability (12,000 transactions per second), interoperability, and regulatory compliance across multiple jurisdictions.

</details>


### [51] [ZK-APEX: Zero-Knowledge Approximate Personalized Unlearning with Executable Proofs](https://arxiv.org/abs/2512.09953)
*Mohammad M Maheri,Sunil Cotterill,Alex Davidson,Hamed Haddadi*

Main category: cs.CR

TL;DR: ZK APEX：首个实用的可验证个性化遗忘框架，结合稀疏掩码和零知识证明，在边缘设备上实现高效的数据删除验证


<details>
  <summary>Details</summary>
Motivation: 现实部署中，服务商将全局模型分发给边缘设备，客户端使用私有数据个性化模型。当收到删除请求时，客户端可能忽略或虚假声称合规，而服务商无法检查其参数或数据，这使得验证变得困难。个性化模型需要忘记目标样本同时保持本地效用，且验证必须在边缘设备上保持轻量级。

Method: ZK APEX是一种零样本个性化遗忘方法，直接在个性化模型上操作而无需重新训练。结合服务商端的稀疏掩码和客户端的小型Group OBS补偿步骤，使用块状经验Fisher矩阵创建曲率感知更新以降低开销。配合Halo2零知识证明，使服务商能够验证正确的遗忘转换已应用，而无需揭示任何私有数据或个性化参数。

Result: 在Vision Transformer分类任务中，ZK APEX恢复了几乎所有的个性化准确性，同时有效移除了目标信息。应用于在代码数据上训练的OPT125M生成模型，恢复了约70%的原始准确性。ViT案例的证明生成约需2小时，比基于重新训练的检查快1000万倍以上，内存使用小于1GB，证明大小约400MB。

Conclusion: ZK APEX展示了首个实用的边缘设备可验证个性化遗忘框架，通过零知识证明技术实现了高效、隐私保护的遗忘验证，为满足隐私、版权和安全要求提供了可行的解决方案。

Abstract: Machine unlearning aims to remove the influence of specific data points from a trained model to satisfy privacy, copyright, and safety requirements. In real deployments, providers distribute a global model to many edge devices, where each client personalizes the model using private data. When a deletion request is issued, clients may ignore it or falsely claim compliance, and providers cannot check their parameters or data. This makes verification difficult, especially because personalized models must forget the targeted samples while preserving local utility, and verification must remain lightweight on edge devices.
  We introduce ZK APEX, a zero-shot personalized unlearning method that operates directly on the personalized model without retraining. ZK APEX combines sparse masking on the provider side with a small Group OBS compensation step on the client side, using a blockwise empirical Fisher matrix to create a curvature-aware update designed for low overhead. Paired with Halo2 zero-knowledge proofs, it enables the provider to verify that the correct unlearning transformation was applied without revealing any private data or personalized parameters.
  On Vision Transformer classification tasks, ZK APEX recovers nearly all personalization accuracy while effectively removing the targeted information. Applied to the OPT125M generative model trained on code data, it recovers around seventy percent of the original accuracy. Proof generation for the ViT case completes in about two hours, more than ten million times faster than retraining-based checks, with less than one gigabyte of memory use and proof sizes around four hundred megabytes. These results show the first practical framework for verifiable personalized unlearning on edge devices.

</details>


### [52] [TRUCE: TRUsted Compliance Enforcement Service for Secure Health Data Exchange](https://arxiv.org/abs/2512.09959)
*Dae-young Kim,Karuna Pande Joshi*

Main category: cs.CR

TL;DR: TRUCE框架：基于AI和语义网技术的可信合规执行框架，用于自动化医疗数据交换的合规管理，特别针对HIPAA等法规冲突问题


<details>
  <summary>Details</summary>
Motivation: 随着组织间敏感个人身份信息（PII）共享增加，医疗数据保护变得日益重要。但HIPAA和Cures Act等法规之间存在冲突，增加了健康数据合规的复杂性。公众对数据泄露的担忧增长，需要简化合规流程并增强个人隐私保护的解决方案。

Method: 开发了TRUCE（可信合规执行）框架，利用AI/知识表示和语义网技术。该框架分析数据交换上下文，评估用户信任分数和数据真实性。包含信任管理方法，整合静态基础事实（如HIPAA法规）和动态基础事实（组织政策）。

Result: 框架在CDC接触者追踪患者数据上验证了HIPAA数据使用协议，处理了高达100万条患者记录。TRUCE服务能够简化合规工作，确保遵守隐私法规，帮助组织实时管理大规模数据交换的合规性。

Conclusion: TRUCE框架为组织提供了一种自动化合规程序、增强可信数据管理的解决方案，特别适用于处理法规冲突和大规模实时数据交换的医疗数据合规场景。

Abstract: Organizations are increasingly sharing large volumes of sensitive Personally Identifiable Information (PII), like health records, with each other to better manage their services. Protecting PII data has become increasingly important in today's digital age, and several regulations have been formulated to ensure the secure exchange and management of sensitive personal data. However, at times some of these regulations are at loggerheads with each other, like the Health Insurance Portability and Accountability Act (HIPAA) and Cures Act; and this adds complexity to the already challenging task of Health Data compliance. As public concern regarding sensitive data breaches grows, finding solutions that streamline compliance processes and enhance individual privacy is crucial. We have developed a novel TRUsted Compliance Enforcement (TRUCE) framework for secure data exchange which aims to automate compliance procedures and enhance trusted data management within organizations. The TRUCE framework reasons over contexts of data exchange and assesses the trust score of users and the veracity of data based on corresponding regulations. This framework, developed using approaches from AI/Knowledge representation and Semantic Web technologies, includes a trust management method that incorporates static ground truth, represented by regulations such as HIPAA, and dynamic ground truth, defined by an organization's policies. In this paper, we present our framework in detail along with the validation against the Health Insurance Portability and Accountability Act (HIPAA) Data Usage Agreement (DUA) on CDC Contact Tracing patient data, up to one million patient records. TRUCE service will streamline compliance efforts and ensure adherence to privacy regulations and can be used by organizations to manage compliance of large velocity data exchange in real time.

</details>


### [53] [A Comparative Analysis of zk-SNARKs and zk-STARKs: Theory and Practice](https://arxiv.org/abs/2512.10020)
*Ayush Nainwal,Atharva Kamble,Nitin Awathare*

Main category: cs.CR

TL;DR: 该论文对zk-SNARKs（Groth16）和zk-STARKs在ARM平台上的实际性能进行了系统比较，发现zk-SNARKs证明生成快68倍、证明大小小123倍，但验证较慢且需要可信设置；zk-STARKs证明较大、生成较慢，但验证更快且具有透明性和后量子安全性。


<details>
  <summary>Details</summary>
Motivation: 零知识证明（ZKPs）是安全和隐私保护计算的核心技术，zk-SNARKs和zk-STARKs作为主流框架在效率、可扩展性和信任假设方面各有权衡。虽然理论基础已有深入研究，但在实际环境中的性能表现仍缺乏系统了解。

Method: 使用公开可用的参考实现，在消费级ARM平台上对zk-SNARKs（Groth16）和zk-STARKs进行系统比较。实证评估包括证明生成时间、验证延迟、证明大小和CPU性能分析。

Result: zk-SNARKs证明生成速度快68倍，证明大小小123倍，但验证速度较慢且需要可信设置；zk-STARKs虽然证明较大、生成较慢，但验证更快且具有透明性和后量子安全性。性能分析进一步揭示了两系统不同的计算瓶颈。

Conclusion: 执行模型和实现细节显著影响实际性能，这些发现为开发者、协议设计者和研究人员在选择和优化证明系统（用于隐私保护交易、可验证计算和可扩展rollups等应用）提供了实用见解。

Abstract: Zero-knowledge proofs (ZKPs) are central to secure and privacy-preserving computation, with zk-SNARKs and zk-STARKs emerging as leading frameworks offering distinct trade-offs in efficiency, scalability, and trust assumptions. While their theoretical foundations are well studied, practical performance under real-world conditions remains less understood.
  In this work, we present a systematic, implementation-level comparison of zk-SNARKs (Groth16) and zk-STARKs using publicly available reference implementations on a consumer-grade ARM platform. Our empirical evaluation covers proof generation time, verification latency, proof size, and CPU profiling. Results show that zk-SNARKs generate proofs 68x faster with 123x smaller proof size, but verify slower and require trusted setup, whereas zk-STARKs, despite larger proofs and slower generation, verify faster and remain transparent and post-quantum secure. Profiling further identifies distinct computational bottlenecks across the two systems, underscoring how execution models and implementation details significantly affect real-world performance. These findings provide actionable insights for developers, protocol designers, and researchers in selecting and optimizing proof systems for applications such as privacy-preserving transactions, verifiable computation, and scalable rollups.

</details>


### [54] [LLM-PEA: Leveraging Large Language Models Against Phishing Email Attacks](https://arxiv.org/abs/2512.10104)
*Najmul Hassan,Prashanth BusiReddyGari,Haitao Zhao,Yihao Ren,Jinsheng Xu,Shaohu Zhang*

Main category: cs.CR

TL;DR: LLMPEA框架使用大语言模型检测多向量钓鱼邮件攻击，包括提示注入、文本精炼和多语言攻击，在三个前沿LLM上达到90%以上准确率，但系统本身也存在被攻击的风险。


<details>
  <summary>Details</summary>
Motivation: 随着LLM应用在邮件安全系统中的部署日益增多，这些系统面临着利用其架构漏洞的钓鱼邮件威胁。当前LLM在部署前需要强化防护，特别是针对利用架构漏洞的协调多向量攻击。

Method: 提出LLMPEA框架，使用LLM检测钓鱼邮件攻击，评估GPT-4o、Claude Sonnet 4和Grok-3三个前沿模型，采用全面的提示设计来评估其可行性、鲁棒性和局限性。

Result: 实证分析显示LLM可以以超过90%的准确率检测钓鱼邮件，但同时也发现基于LLM的钓鱼邮件检测系统可能被对抗性攻击、提示注入和多语言攻击所利用。

Conclusion: 研究结果为现实环境中基于LLM的钓鱼检测提供了关键见解，在攻击者利用多个漏洞组合攻击的情况下，需要特别关注系统的安全性。

Abstract: Email phishing is one of the most prevalent and globally consequential vectors of cyber intrusion. As systems increasingly deploy Large Language Models (LLMs) applications, these systems face evolving phishing email threats that exploit their fundamental architectures. Current LLMs require substantial hardening before deployment in email security systems, particularly against coordinated multi-vector attacks that exploit architectural vulnerabilities. This paper proposes LLMPEA, an LLM-based framework to detect phishing email attacks across multiple attack vectors, including prompt injection, text refinement, and multilingual attacks. We evaluate three frontier LLMs (e.g., GPT-4o, Claude Sonnet 4, and Grok-3) and comprehensive prompting design to assess their feasibility, robustness, and limitations against phishing email attacks. Our empirical analysis reveals that LLMs can detect the phishing email over 90% accuracy while we also highlight that LLM-based phishing email detection systems could be exploited by adversarial attack, prompt injection, and multilingual attacks. Our findings provide critical insights for LLM-based phishing detection in real-world settings where attackers exploit multiple vulnerabilities in combination.

</details>


### [55] [Watermarks for Language Models via Probabilistic Automata](https://arxiv.org/abs/2512.10185)
*Yangkun Wang,Jingbo Shang*

Main category: cs.CR

TL;DR: 本文提出了一种基于概率自动机的新型水印方案，解决了现有方案生成多样性有限、检测开销大和可检测性高的问题，提供了实用方案和理论构造两种实现。


<details>
  <summary>Details</summary>
Motivation: 现有语言模型水印方案虽然能实现无失真嵌入并抵抗编辑距离攻击，但存在生成多样性有限、检测开销大以及可检测性（容易被对手发现和欺骗）的问题。

Method: 通过概率自动机构建新型水印方案，提出了两种具体实现：(1) 具有指数级生成多样性和计算效率的实用方案；(2) 在密码学假设下具有形式化不可检测性保证的理论构造。

Result: 在LLaMA-3B和Mistral-7B模型上的大量实验验证了该方案在鲁棒性和效率方面的优越性能。

Conclusion: 基于概率自动机的水印方案能够同时解决生成多样性、检测开销和不可检测性问题，为语言模型水印提供了更优的解决方案。

Abstract: A recent watermarking scheme for language models achieves distortion-free embedding and robustness to edit-distance attacks. However, it suffers from limited generation diversity and high detection overhead. In parallel, recent research has focused on undetectability, a property ensuring that watermarks remain difficult for adversaries to detect and spoof. In this work, we introduce a new class of watermarking schemes constructed through probabilistic automata. We present two instantiations: (i) a practical scheme with exponential generation diversity and computational efficiency, and (ii) a theoretical construction with formal undetectability guarantees under cryptographic assumptions. Extensive experiments on LLaMA-3B and Mistral-7B validate the superior performance of our scheme in terms of robustness and efficiency.

</details>


### [56] [FLARE: A Wireless Side-Channel Fingerprinting Attack on Federated Learning](https://arxiv.org/abs/2512.10296)
*Md Nahid Hasan Shuvo,Moinul Hossain,Anik Mallik,Jeffrey Twigg,Fikadu Dagefu*

Main category: cs.CR

TL;DR: 本文提出FLARE攻击框架，通过分析联邦学习客户端加密无线流量的流量级和包级统计特征，能够以高准确率推断深度学习模型架构（如CNN、RNN），揭示了联邦学习系统中新的侧信道漏洞。


<details>
  <summary>Details</summary>
Motivation: 联邦学习虽然保护数据隐私，但仍面临隐私威胁。目前尚未有研究探索外部攻击者如何间接获取客户端模型架构信息（如CNN或RNN类型），这些信息泄露可能导致针对特定架构的高级攻击。

Method: 提出FLARE（FL Architecture REconnaissance）侧信道指纹识别攻击框架，通过分析联邦学习客户端加密无线流量的流量级和包级统计特征，来推断深度学习模型架构类型。

Result: 在IEEE 802.11 Wi-Fi环境下，对多种CNN和RNN变体（包括预训练和自定义模型）进行评估，FLARE在封闭世界场景中F1分数超过98%，在开放世界场景中达到91%。结果表明CNN和RNN模型会泄露可区分的流量模式。

Conclusion: 这是首个通过嗅探加密无线流量来指纹识别联邦学习模型架构的工作，揭示了当前联邦学习系统中存在关键的侧信道漏洞，即使在硬件、软件和数据异构的现实联邦学习设置下，模型架构信息仍可能被泄露。

Abstract: Federated Learning (FL) enables collaborative model training across distributed devices while safeguarding data and user privacy. However, FL remains susceptible to privacy threats that can compromise data via direct means. That said, indirectly compromising the confidentiality of the FL model architecture (e.g., a convolutional neural network (CNN) or a recurrent neural network (RNN)) on a client device by an outsider remains unexplored. If leaked, this information can enable next-level attacks tailored to the architecture. This paper proposes a novel side-channel fingerprinting attack, leveraging flow-level and packet-level statistics of encrypted wireless traffic from an FL client to infer its deep learning model architecture. We name it FLARE, a fingerprinting framework based on FL Architecture REconnaissance. Evaluation across various CNN and RNN variants-including pre-trained and custom models trained over IEEE 802.11 Wi-Fi-shows that FLARE achieves over 98% F1-score in closed-world and up to 91% in open-world scenarios. These results reveal that CNN and RNN models leak distinguishable traffic patterns, enabling architecture fingerprinting even under realistic FL settings with hardware, software, and data heterogeneity. To our knowledge, this is the first work to fingerprint FL model architectures by sniffing encrypted wireless traffic, exposing a critical side-channel vulnerability in current FL systems.

</details>


### [57] [From Lab to Reality: A Practical Evaluation of Deep Learning Models and LLMs for Vulnerability Detection](https://arxiv.org/abs/2512.10485)
*Chaomeng Lu,Bert Lagaisse*

Main category: cs.CR

TL;DR: 深度学习漏洞检测方法在基准数据集上表现良好，但在真实世界应用中效果有限。研究发现现有模型难以在表示空间中区分漏洞代码，跨数据集泛化能力差，在时间外分布数据集上性能急剧下降。


<details>
  <summary>Details</summary>
Motivation: 尽管基于深度学习的漏洞检测方法在基准数据集上表现出色，但其真实世界有效性尚未得到充分探索。当前研究主要使用分布一致且带有启发式或部分噪声标签的基准数据集进行评估，这可能导致模型在实际部署中的性能被高估。

Method: 系统评估了两个代表性深度学习模型（ReVeal和LineVul）在四个数据集（Juliet、Devign、BigVul、ICVul）上的表现，使用t-SNE分析代码表示以揭示漏洞相关模式。为评估实际适用性，将这些模型与四个预训练大语言模型（Claude 3.5 Sonnet、GPT-o3-mini、GPT-4o、GPT-5）部署在新构建的时间外分布数据集VentiVul上，该数据集包含2025年5月修复的20个Linux内核漏洞。

Result: 当前模型难以在表示空间中区分漏洞代码与非漏洞代码，在不同分布的数据集上泛化能力差。在VentiVul数据集上，模型性能急剧下降，大多数模型无法可靠检测漏洞，暴露了学术基准与实际部署之间的显著差距。

Conclusion: 研究揭示了深度学习漏洞检测模型在真实世界应用中的局限性，强调了部署导向评估框架的价值，并指出需要更鲁棒的代码表示和更高质量的数据集来弥合学术研究与实际部署之间的差距。

Abstract: Vulnerability detection methods based on deep learning (DL) have shown strong performance on benchmark datasets, yet their real-world effectiveness remains underexplored. Recent work suggests that both graph neural network (GNN)-based and transformer-based models, including large language models (LLMs), yield promising results when evaluated on curated benchmark datasets. These datasets are typically characterized by consistent data distributions and heuristic or partially noisy labels. In this study, we systematically evaluate two representative DL models-ReVeal and LineVul-across four representative datasets: Juliet, Devign, BigVul, and ICVul. Each model is trained independently on each respective dataset, and their code representations are analyzed using t-SNE to uncover vulnerability related patterns. To assess realistic applicability, we deploy these models along with four pretrained LLMs, Claude 3.5 Sonnet, GPT-o3-mini, GPT-4o, and GPT-5 on a curated dataset, VentiVul, comprising 20 recently (May 2025) fixed vulnerabilities from the Linux kernel. Our experiments reveal that current models struggle to distinguish vulnerable from non-vulnerable code in representation space and generalize poorly across datasets with differing distributions. When evaluated on VentiVul, our newly constructed time-wise out-of-distribution dataset, performance drops sharply, with most models failing to detect vulnerabilities reliably. These results expose a persistent gap between academic benchmarks and real-world deployment, emphasizing the value of our deployment-oriented evaluation framework and the need for more robust code representations and higher-quality datasets.

</details>


### [58] [Differential Privacy for Secure Machine Learning in Healthcare IoT-Cloud Systems](https://arxiv.org/abs/2512.10426)
*N Mangala,Murtaza Rangwala,S Aishwarya,B Eswara Reddy,Rajkumar Buyya,KR Venugopal,SS Iyengar,LM Patnaik*

Main category: cs.CR

TL;DR: 本文提出了一种多层物联网-边缘-云架构，通过基于响应关键性和存储持久性的任务分配来提升紧急医疗响应速度，并采用差分隐私框架保护患者数据隐私，同时利用区块链确保可信通信。


<details>
  <summary>Details</summary>
Motivation: 随着可穿戴设备和连接医疗设备的普及，远程患者监测、紧急响应等医疗应用日益复杂。实时响应对于缓解患者紧急情况至关重要，同时保护患者隐私在数据驱动的医疗保健中极为重要。现有物联网-云集成系统需要在响应速度和数据隐私之间取得平衡。

Method: 提出多层物联网、边缘和云架构，根据响应关键性和存储持久性分配任务。采用差分隐私框架保护患者数据隐私，在K-means、逻辑回归、随机森林和朴素贝叶斯等机器学习模型中应用。建立全面的威胁模型，识别三类对手，评估拉普拉斯、高斯和混合噪声机制在不同隐私预算下的效果。使用区块链确保可信通信，通过时间戳、可追溯性和不可变性保障分析应用的安全性。

Result: 监督算法在差分隐私下达到86%的准确率。提出的混合拉普拉斯-高斯噪声机制与自适应预算分配提供了平衡方法，在低维和高维数据集上都实现了更好的隐私-效用权衡。在ε=5.0的实际阈值下，监督算法达到82-84%的准确率，同时将属性推断攻击减少18%，数据重建相关性降低70%。边缘计算在紧急场景下实现8倍延迟减少。

Conclusion: 多层物联网-边缘-云架构有效提升了紧急医疗响应速度，差分隐私框架在保护患者隐私的同时保持了机器学习模型的实用性，混合噪声机制提供了更好的隐私-效用权衡，区块链技术确保了通信的可信性，验证了分层架构在时间关键操作中的有效性。

Abstract: Healthcare has become exceptionally sophisticated, as wearables and connected medical devices are revolutionising remote patient monitoring, emergency response, medication management, diagnosis, and predictive and prescriptive analytics. Internet of Things and Cloud computing integrated systems (IoT-Cloud) facilitate sensing, automation, and processing for these healthcare applications. While real-time response is crucial for alleviating patient emergencies, protecting patient privacy is extremely important in data-driven healthcare. In this paper, we propose a multi-layer IoT, Edge and Cloud architecture to enhance the speed of response for emergency healthcare by distributing tasks based on response criticality and permanence of storage. Privacy of patient data is assured by proposing a Differential Privacy framework across several machine learning models such as K-means, Logistic Regression, Random Forest and Naive Bayes. We establish a comprehensive threat model identifying three adversary classes and evaluate Laplace, Gaussian, and hybrid noise mechanisms across varying privacy budgets, with supervised algorithms achieving up to 86% accuracy. The proposed hybrid Laplace-Gaussian noise mechanism with adaptive budget allocation provides a balanced approach, offering moderate tails and better privacy-utility trade-offs for both low and high dimension datasets. At the practical threshold of $\varepsilon = 5.0$, supervised algorithms achieve 82-84% accuracy while reducing attribute inference attacks by up to 18% and data reconstruction correlation by 70%. Blockchain security further ensures trusted communication through time-stamping, traceability, and immutability for analytics applications. Edge computing demonstrates 8$\times$ latency reduction for emergency scenarios, validating the hierarchical architecture for time-critical operations.

</details>


### [59] [Objectives and Design Principles in Offline Payments with Central Bank Digital Currency (CBDC)](https://arxiv.org/abs/2512.10636)
*David-Alexandre Guiraud,Andrea Tundis,Marc Winstel*

Main category: cs.CR

TL;DR: 本文讨论了具有离线功能的央行数字货币（CBDC）的基本设计原则及相应对策，提出了三大核心目标：访问控制安全、防止用户不当行为、隐私设计，并探讨了这些目标与具体设计元素的对应关系。


<details>
  <summary>Details</summary>
Motivation: 随着央行数字货币的发展，需要设计具有离线功能的CBDC系统，同时确保安全性、完整性和隐私保护。本文旨在为这类系统建立基本的设计原则和应对措施。

Method: 通过分析CBDC系统的需求，识别出三大核心目标：访问控制安全、防止用户不当行为、隐私设计，并将这些目标与具体的设计元素（如安全硬件）进行对应，探讨不同目标和措施之间的相互影响关系。

Result: 研究发现三大目标可以对应到具体的设计元素作为对策，其中某些目标和措施之间没有或只有最小程度的相互干扰。例如，通过采用和集成安全硬件可以保护用户钱包的完整性并防止双花攻击。

Conclusion: 核心结论是CBDC系统的设计目标与具体设计元素之间存在对应关系，安全硬件的采用是解决钱包完整性和防止双花攻击的关键措施，不同目标和措施之间可以相对独立地实现。

Abstract: In this work, fundamental design principles for a central bank digital currency (CBDC) with an offline functionality and corresponding counter measures are discussed. We identify three major objectives for any such CBDC proposal:(i) Access Control Security - protection of a user's funds against unauthorized access by other users; (ii) Security against Depositor's Misbehavior - preservation of the integrity of an environment (potentially the wallet) against misbehavior of its owner (for example, double-spending), and (iii) Privacy by Design - ensuring privacy is embedded into the system architecture. Our central conclusion is the alignment of the objectives to concrete design elements as countermeasures, whereas certain objectives and countermeasures have no or minimal interferences with each other. For example, we work out that the integrity of a user's wallet and, accordingly, the prevention of double-spending race attacks should be addressed through the adoption and integration of \textit{secure hardware} within a CBDC system.

</details>


### [60] [Virtual camera detection: Catching video injection attacks in remote biometric systems](https://arxiv.org/abs/2512.10653)
*Daniyar Kurmankhojayev,Andrei Shadrikov,Dmitrii Gordin,Mikhail Shkorin,Danijar Gabdullin,Aigerim Kambetbayeva,Kanat Kuatov*

Main category: cs.CR

TL;DR: 该研究提出了一种基于机器学习的虚拟摄像头检测方法，用于增强人脸防伪系统的安全性，防止视频注入攻击。


<details>
  <summary>Details</summary>
Motivation: 随着人脸识别在远程生物认证系统中的广泛应用，视频注入攻击（通过深度伪造和虚拟摄像头软件实现）对系统完整性构成严重威胁。现有虚拟摄像头检测研究缺乏实际实施和评估的深入探讨。

Method: 采用基于机器学习的方法进行虚拟摄像头检测，模型在真实用户会话期间收集的元数据上进行训练。

Result: 实证结果表明，该方法能有效识别视频注入攻击尝试，降低恶意用户绕过人脸防伪系统的风险。

Conclusion: 该研究提出的机器学习方法为虚拟摄像头检测提供了有效的解决方案，有助于增强人脸防伪系统的安全性，对抗视频注入攻击。

Abstract: Face anti-spoofing (FAS) is a vital component of remote biometric authentication systems based on facial recognition, increasingly used across web-based applications. Among emerging threats, video injection attacks -- facilitated by technologies such as deepfakes and virtual camera software -- pose significant challenges to system integrity. While virtual camera detection (VCD) has shown potential as a countermeasure, existing literature offers limited insight into its practical implementation and evaluation. This study introduces a machine learning-based approach to VCD, with a focus on its design and validation. The model is trained on metadata collected during sessions with authentic users. Empirical results demonstrate its effectiveness in identifying video injection attempts and reducing the risk of malicious users bypassing FAS systems.

</details>


### [61] [A Proof of Success and Reward Distribution Protocol for Multi-bridge Architecture in Cross-chain Communication](https://arxiv.org/abs/2512.10667)
*Damilare Peter Oyinloye,Mohd Sameen Chishti,Jingyue Li*

Main category: cs.CR

TL;DR: PSCRD协议通过多桥响应协调和激励分配机制解决单桥区块链的中心化和单点故障风险，使用公平奖励分配促进桥参与度，提高去中心化水平


<details>
  <summary>Details</summary>
Motivation: 单桥区块链解决方案存在中心化和单点故障风险，需要设计更安全、去中心化的跨链通信机制

Method: 提出PSCRD协议，包含多桥响应协调和公平奖励分配系统，通过数学分析和模拟验证，使用基尼系数和Nakamoto系数评估效果

Result: 基尼系数显示奖励分配公平性逐步改善，Nakamoto系数显示去中心化程度显著提升，系统更安全且用户成本未大幅增加

Conclusion: PSCRD协议能够构建更具弹性和安全性的跨链桥系统，有效解决单桥方案的中心化和单点故障问题

Abstract: Single-bridge blockchain solutions enable cross-chain communication. However, they are associated with centralization and single-point-of-failure risks. This paper proposes Proof of Success and Reward Distribution (PSCRD), a novel multi-bridge response coordination and incentive distribution protocol designed to address the challenges. PSCRD introduces a fair reward distribution system that equitably distributes the transfer fee among participating bridges, incentivizing honest behavior and sustained commitment. The purpose is to encourage bridge participation for higher decentralization and lower single-point-of-failure risks. The mathematical analysis and simulation results validate the effectiveness of PSCRD using two key metrics: the Gini index, which demonstrates a progressive improvement in the fairness of the reward distribution as new bridge groups joined the network; and the Nakamoto coefficient, which shows a significant improvement in decentralization over time. These findings highlight that PSCRD provides a more resilient and secure cross-chain bridge system without substantially increasing user costs.

</details>


### [62] [TriHaRd: Higher Resilience for TEE Trusted Time](https://arxiv.org/abs/2512.10732)
*Matthieu Bettinger,Sonia Ben Mokhtar,Pascal Felber,Etienne Rivière,Valerio Schiavoni,Anthony Simonet-Boulogne*

Main category: cs.CR

TL;DR: TriHaRd是一个针对TEE（可信执行环境）的可靠时间协议，通过拜占庭容错的时钟更新和一致性检查，有效抵御时钟速度和偏移操纵攻击，相比之前的Triad协议提供更高的安全性。


<details>
  <summary>Details</summary>
Motivation: 在TEE（如Intel SGX）中，时间源位于可信计算基之外，恶意主机可以操纵TEE的时间感知，包括时间跳跃或影响感知时间速度。先前的工作Triad虽然提出了TEE集群与远程时间权威协作的方案，但仍允许攻击者控制操作系统并任意操纵自己的TEE时钟速度，甚至可以将更快的时间流逝传播给参与协议的诚实机器，导致它们跳过到任意远的未来时间戳。

Method: TriHaRd通过拜占庭容错的时钟更新和一致性检查来实现高弹性。该协议采用TEE集群协作的方式，但增加了更强的安全机制来抵御时钟速度和偏移操纵攻击。

Result: 实验证明TriHaRd能够有效缓解针对Triad的已知攻击，提供了更高的安全性和可靠性。

Conclusion: TriHaRd是一个高弹性的TEE可信时间协议，能够有效抵御时钟速度和偏移操纵攻击，为TEE环境提供了更可靠的时间测量解决方案。

Abstract: Accurately measuring time passing is critical for many applications. However, in Trusted Execution Environments (TEEs) such as Intel SGX, the time source is outside the Trusted Computing Base: a malicious host can manipulate the TEE's notion of time, jumping in time or affecting perceived time speed. Previous work (Triad) proposes protocols for TEEs to maintain a trustworthy time source by building a cluster of TEEs that collaborate with each other and with a remote Time Authority to maintain a continuous notion of passing time. However, such approaches still allow an attacker to control the operating system and arbitrarily manipulate their own TEE's perceived clock speed. An attacker can even propagate faster passage of time to honest machines participating in Triad's trusted time protocol, causing them to skip to timestamps arbitrarily far in the future. We propose TriHaRd, a TEE trusted time protocol achieving high resilience against clock speed and offset manipulations, notably through Byzantine-resilient clock updates and consistency checks. We empirically show that TriHaRd mitigates known attacks against Triad.

</details>
