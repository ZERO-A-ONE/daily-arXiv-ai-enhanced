<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 41]
- [cs.CR](#cs.CR) [Total: 33]
- [cs.AI](#cs.AI) [Total: 44]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [Choosing the Right Git Workflow: A Comparative Analysis of Trunk-based vs. Branch-based Approaches](https://arxiv.org/abs/2507.08943)
*Pedro Lopes,Paola Accioly,Paulo Borba,Vitor Menezes*

Main category: cs.SE

TL;DR: 论文探讨了Git工作流的两种主要类型（分支工作流和主干工作流），并通过调查和访谈分析了巴西开发者对不同工作流的使用情况及其影响因素。


<details>
  <summary>Details</summary>
Motivation: Git作为广泛使用的版本控制系统，其工作流选择对团队生产力和软件质量至关重要，但缺乏科学研究的支持。

Method: 通过半结构化访谈和问卷调查，收集巴西开发者对不同Git工作流的使用经验和看法。

Result: 主干工作流适合经验丰富、规模较小的快速项目团队，而分支工作流更适合经验较少、规模较大的团队，尽管管理挑战更大。

Conclusion: 选择Git工作流需考虑团队经验和规模，主干工作流适合快速开发，分支工作流适合大型团队。

Abstract: Git has become one of the most widely used version control systems today.
Among its distinguishing features, its ability to easily and quickly create
branches stands out, allowing teams to customize their workflows. In this
context, various formats of collaborative development workflows using Git have
emerged and gained popularity among software engineers. We can categorize such
workflows into two main types: branch-based workflows and trunk-based
workflows. Branch-based workflows typically define a set of remote branches
with well-defined objectives, such as feature branches, a branch for feature
integration, and a main branch. The goal is to migrate changes from the most
isolated branch to the main one shared by all as the code matures. In this
category, GitFlow stands out as the most popular example. In contrast,
trunk-based workflows have a single remote branch where developers integrate
their changes directly. In this range of options, choosing a workflow that
maximizes team productivity while promoting software quality becomes a
non-trivial task. Despite discussions on forums, social networks, and blogs,
few scientific articles have explored this topic. In this work, we provide
evidence on how Brazilian developers work with Git workflows and what factors
favor or hinder the use of each model. To this end, we conducted
semi-structured interviews and a survey with software developers. Our results
indicate that trunk-based development favors fast-paced projects with
experienced and smaller teams, while branch-based development suits less
experienced and larger teams better, despite posing management challenges.

</details>


### [2] [Semantic Source Code Segmentation using Small and Large Language Models](https://arxiv.org/abs/2507.08992)
*Abdelhalim Dahou,Ansgar Scherp,Sebastian Kurten,Brigitte Mathiak,Madhu Chauhan*

Main category: cs.SE

TL;DR: 论文提出了一种自动化方法，利用大语言模型（LLMs）和小语言模型（SLMs）对R研究代码进行分段，并比较了两种方法的效果。


<details>
  <summary>Details</summary>
Motivation: 随着代码库规模增长，手动和语法分析方法变得不切实际，尤其是对于R等低资源语言及其研究领域。

Method: 提出了两种方法：基于上下文的逐行分析和基于范围的段确定，并实验了LLMs和微调的SLMs。

Result: 基于上下文的逐行分析优于基于范围的分段，且CodeBERT和CodeT5+等小模型表现优于LLMs。

Conclusion: 小语言模型在未预训练R代码的情况下，仅通过微调少量标注数据即可取得优异效果。

Abstract: Source code segmentation, dividing code into functionally coherent segments,
is crucial for knowledge retrieval and maintenance in software development.
While enabling efficient navigation and comprehension of large codebases,
manual and syntactic analysis approaches have become impractical as
repositories grow, especially for low-resource languages like R and their
research domains (e.g., social sciences, psychology).This paper introduces an
automated, domain-specific approach for research R code segmentation using
Large and Small Language Models (LLMs/SLMs). It presents two novel approaches
and a human-annotated dataset, StatCodeSeg. We explore two distinct approaches:
line-by-line analysis with context and range-based segment determination. We
experiment with LLMs and fine-tuned SLMs. To support the generalizability of
our approaches, we also include experiments on Python code from the computer
science domain.Our results show that context-based line-by-line analysis is
superior over range-based segmentation.Using smaller language models like
CodeBERT and an encoder-only version of CodeT5+ are better than their LLM
counterparts. Most notably, these two best-performing models did not see R code
during pre-training versus the LLMs but were only fine-tuned on 4,130 lines of
manually annotated code.

</details>


### [3] [Accelerating Drug Discovery Through Agentic AI: A Multi-Agent Approach to Laboratory Automation in the DMTA Cycle](https://arxiv.org/abs/2507.09023)
*Yao Fehlis,Charles Crain,Aidan Jensen,Michael Watson,James Juhasz,Paul Mandel,Betty Liu,Shawn Mahon,Daren Wilson,Nick Lynch-Jonely,Ben Leedom,David Fuller*

Main category: cs.SE

TL;DR: 本文介绍了一种名为Tippy的新型AI框架，通过专门设计的AI代理自动化药物发现中的DMTA循环，显著提升了效率和协作。


<details>
  <summary>Details</summary>
Motivation: 传统药物发现方法难以满足现代治疗需求，需要更高效的自动化解决方案。

Method: Tippy采用多代理系统，包括Supervisor、Molecule、Lab、Analysis和Report五个专门代理，并在Safety Guardrail监督下运行，覆盖DMTA循环的各个阶段。

Result: Tippy显著提高了工作流效率、决策速度和跨学科协作能力，为AI辅助药物发现提供了新范式。

Conclusion: Tippy是首个生产就绪的AI代理系统，展示了AI如何通过自主代理加速药物发现并保持科学严谨性。

Abstract: The pharmaceutical industry faces unprecedented challenges in drug discovery,
with traditional approaches struggling to meet modern therapeutic development
demands. This paper introduces a novel AI framework, Tippy, that transforms
laboratory automation through specialized AI agents operating within the
Design-Make-Test-Analyze (DMTA) cycle. Our multi-agent system employs five
specialized agents - Supervisor, Molecule, Lab, Analysis, and Report, with
Safety Guardrail oversight - each designed to excel in specific phases of the
drug discovery pipeline. Tippy represents the first production-ready
implementation of specialized AI agents for automating the DMTA cycle,
providing a concrete example of how AI can transform laboratory workflows. By
leveraging autonomous AI agents that reason, plan, and collaborate, we
demonstrate how Tippy accelerates DMTA cycles while maintaining scientific
rigor essential for pharmaceutical research. The system shows significant
improvements in workflow efficiency, decision-making speed, and
cross-disciplinary coordination, offering a new paradigm for AI-assisted drug
discovery.

</details>


### [4] [Towards Extracting Software Requirements from App Reviews using Seq2seq Framework](https://arxiv.org/abs/2507.09039)
*Aakash Sorathiya,Gouri Ginde*

Main category: cs.SE

TL;DR: 论文提出了一种基于序列到序列（Seq2seq）生成方法的命名实体识别（NER）任务，用于从移动应用评论中提取需求，解决了现有方法因评论语言不规范、信息无关等问题。


<details>
  <summary>Details</summary>
Motivation: 移动应用评论是改进软件的重要数据源，但现有方法因评论语言不规范、信息无关等问题效果不佳，需改进需求提取方法。

Method: 提出了一种结合BiLSTM编码器、LSTM解码器、自注意力机制、GloVe嵌入和CRF模型的Seq2seq框架。

Result: 在两个数据集上评估，F1分数分别为0.96（数据集2）和0.47（数据集1），优于现有方法。

Conclusion: 该框架能有效从移动应用评论中提取需求，为软件改进提供支持。

Abstract: Mobile app reviews are a large-scale data source for software improvements. A
key task in this context is effectively extracting requirements from app
reviews to analyze the users' needs and support the software's evolution.
Recent studies show that existing methods fail at this task since app reviews
usually contain informal language, grammatical and spelling errors, and a large
amount of irrelevant information that might not have direct practical value for
developers. To address this, we propose a novel reformulation of requirements
extraction as a Named Entity Recognition (NER) task based on the
sequence-to-sequence (Seq2seq) generation approach. With this aim, we propose a
Seq2seq framework, incorporating a BiLSTM encoder and an LSTM decoder, enhanced
with a self-attention mechanism, GloVe embeddings, and a CRF model. We
evaluated our framework on two datasets: a manually annotated set of 1,000
reviews (Dataset 1) and a crowdsourced set of 23,816 reviews (Dataset 2). The
quantitative evaluation of our framework showed that it outperformed existing
state-of-the-art methods with an F1 score of 0.96 on Dataset 2, and achieved
comparable performance on Dataset 1 with an F1 score of 0.47.

</details>


### [5] [CMER: A Context-Aware Approach for Mining Ethical Concern-related App Reviews](https://arxiv.org/abs/2507.09049)
*Aakash Sorathiya,Gouri Ginde*

Main category: cs.SE

TL;DR: 本研究提出了一种名为CMER的新方法，结合自然语言推理（NLI）和类似LLaMA的大型语言模型（LLM），用于从大量应用评论中提取与伦理问题相关的内容。


<details>
  <summary>Details</summary>
Motivation: 随着移动应用的普及，伦理问题（如隐私和安全）在用户评论中频繁出现，但传统方法难以有效提取这些内容。

Method: CMER结合NLI提供领域特定上下文，并使用LLM模型减少对标记数据的依赖，从而高效分类伦理相关评论。

Result: 在超过382K条移动投资应用评论中，CMER提取了2178条被之前关键词方法忽略的隐私和安全相关评论。

Conclusion: CMER方法在提取伦理相关评论方面表现出色，可进一步转化为可操作的需求文档。

Abstract: With the increasing proliferation of mobile applications in our daily lives,
the concerns surrounding ethics have surged significantly. Users communicate
their feedback in app reviews, frequently emphasizing ethical concerns, such as
privacy and security. Incorporating these reviews has proved to be useful for
many areas of software engineering (e.g., requirement engineering, testing,
etc.). However, app reviews related to ethical concerns generally use
domain-specific language and are typically overshadowed by more generic
categories of user feedback, such as app reliability and usability. Thus,
making automated extraction a challenging and time-consuming effort.
  This study proposes CMER (A \underline{C}ontext-Aware Approach for
\underline{M}ining \underline{E}thical Concern-related App
\underline{R}eviews), a novel approach that combines Natural Language Inference
(NLI) and a decoder-only (LLaMA-like) Large Language Model (LLM) to extract
ethical concern-related app reviews at scale. In CMER, NLI provides
domain-specific context awareness by using domain-specific hypotheses, and the
Llama-like LLM eliminates the need for labeled data in the classification task.
We evaluated the validity of CMER by mining privacy and security-related
reviews (PSRs) from the dataset of more than 382K app reviews of mobile
investment apps. First, we evaluated four NLI models and compared the results
of domain-specific hypotheses with generic hypotheses. Next, we evaluated three
LLMs for the classification task. Finally, we combined the best NLI and LLM
models (CMER) and extracted 2,178 additional PSRs overlooked by the previous
study using a keyword-based approach, thus demonstrating the effectiveness of
CMER. These reviews can be further refined into actionable requirement
artifacts.

</details>


### [6] [SAGE: A Context-Aware Approach for Mining Privacy Requirements Relevant Reviews from Mental Health Apps](https://arxiv.org/abs/2507.09051)
*Aakash Sorathiya,Gouri Ginde*

Main category: cs.SE

TL;DR: SAGE是一种基于自然语言推理（NLI）和GPT模型的上下文感知方法，用于自动挖掘心理健康（MH）应用中的隐私相关评论，无需微调即可达到高准确率。


<details>
  <summary>Details</summary>
Motivation: 心理健康应用收集敏感数据引发隐私担忧，但隐私评论常被其他反馈类别掩盖，难以自动识别。

Method: 采用NLI结合MH领域特定隐私假设（提供上下文感知）和GPT模型（无需微调），自动挖掘隐私评论。

Result: 在204K条评论数据集上，SAGE的F1分数达0.85，优于微调的BERT和T5，并发现748条被关键词方法忽略的隐私评论。

Conclusion: SAGE能有效识别隐私评论，为后续提炼隐私需求提供支持。

Abstract: Mental health (MH) apps often require sensitive user data to customize
services for mental wellness needs. However, such data collection practices in
some MH apps raise significant privacy concerns for users. These concerns are
often mentioned in app reviews, but other feedback categories, such as
reliability and usability, tend to take precedence. This poses a significant
challenge in automatically identifying privacy requirements-relevant reviews
(privacy reviews) that can be utilized to extract privacy requirements and
address users' privacy concerns. Thus, this study introduces SAGE, a
context-aware approach to automatically mining privacy reviews from MH apps
using Natural Language Inference (NLI) with MH domain-specific privacy
hypotheses (provides domain-specific context awareness) and a GPT model
(eliminates the need for fine-tuning). The quantitative evaluation of SAGE on a
dataset of 204K app reviews achieved an F1 score of 0.85 without any
fine-tuning, outperforming the fine-tuned baseline classifiers BERT and T5.
Furthermore, SAGE extracted 748 privacy reviews previously overlooked by
keyword-based methods, demonstrating its effectiveness through qualitative
evaluation. These reviews can later be refined into actionable privacy
requirement artifacts.

</details>


### [7] [SetupBench: Assessing Software Engineering Agents' Ability to Bootstrap Development Environments](https://arxiv.org/abs/2507.09063)
*Avi Arora,Jinu Jang,Roshanak Zilouchian Moghaddam*

Main category: cs.SE

TL;DR: SetupBench是一个新的基准测试，用于评估LLM代理在裸Linux沙盒中完成环境初始化任务的能力，发现现有代理在多个任务类别中表现不佳。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试主要评估预配置环境中的LLM代理，忽略了环境初始化这一关键能力，因此需要填补这一空白。

Method: 通过SetupBench（包含93个任务的基准测试）评估代理在裸Linux沙盒中安装软件包、解决依赖冲突、配置数据库和服务的能力。

Result: OpenHands代理在多个任务中成功率低，尤其在仓库设置（38.9-57.4%）和本地数据库配置（20.0-53.3%）中表现较差，存在系统性失败模式。

Conclusion: SetupBench为下一代软件开发者代理提供了严格的评估标准，揭示了当前代理在环境初始化能力上的不足。

Abstract: Modern Large Language Model (LLM) agents promise end to end assistance with
real-world software tasks, yet existing benchmarks evaluate LLM agents almost
exclusively in pre-baked environments where every dependency is pre-installed.
To fill this gap, we introduce SetupBench, a 93 instance benchmark that
isolates the environment-bootstrap skill: starting from a bare Linux sandbox,
an agent must install packages, resolve dependency conflicts, initialize
databases, and configure background services. Our tasks span seven language
ecosystems, five database engines, and multi-service orchestration scenarios,
each accompanies by a natural language problem statement and a deterministic
success command. Through evaluation of OpenHands, a state-of-the-art coding
agent, we find low success rates across task categories, with particular
challenges in repository setup (38.9-57.4%) and local database configuration
(20.0-53.3%). Our analysis reveals systematic failure modes including
incomplete development tooling installation, hallucinated task constraints, and
non-persistent environment modifications that break agent-human collaboration
workflows. We identify substantial inefficiencies in agent exploration
strategies, with 38-89% of actions being unnecessary compared to optimal human
behavior. These findings highlight gaps in current agents' practical
environment-bootstrap capabilities. By targeting this critical yet
under-evaluated capability, SetupBench provides a rigorous yard-stick for the
next generation of software developer agents aiming to solve end to end
real-wold tasks.

</details>


### [8] [SPICE: An Automated SWE-Bench Labeling Pipeline for Issue Clarity, Test Coverage, and Effort Estimation](https://arxiv.org/abs/2507.09108)
*Aaditya Bhatia,Gustavo A. Oliva,Gopi Krishnan Rajbahadur,Haoxiang Zhang,Yihao Chen,Zhilong Chen,Arthur Leung,Dayi Lin,Boyuan Chen,Ahmed E. Hassan*

Main category: cs.SE

TL;DR: SPICE是一种自动化标注工具，用于生成高质量的软件工程数据集，显著降低标注成本。


<details>
  <summary>Details</summary>
Motivation: 高质量标注数据集对软件工程基础模型至关重要，但手动标注成本高昂且耗时。

Method: SPICE结合上下文感知代码导航、基于原理的提示和多轮共识，生成接近专家标注的标签。

Result: SPICE在1000个实例上的标注成本从10万美元降至5.10美元，并与人工标注数据高度一致。

Conclusion: SPICE为大规模软件工程数据集创建提供了经济高效的解决方案，并发布了工具和新数据集。

Abstract: High-quality labeled datasets are crucial for training and evaluating
foundation models in software engineering, but creating them is often
prohibitively expensive and labor-intensive. We introduce SPICE, a scalable,
automated pipeline for labeling SWE-bench-style datasets with annotations for
issue clarity, test coverage, and effort estimation. SPICE combines
context-aware code navigation, rationale-driven prompting, and multi-pass
consensus to produce labels that closely approximate expert annotations.
SPICE's design was informed by our own experience and frustration in labeling
more than 800 instances from SWE-Gym. SPICE achieves strong agreement with
human-labeled SWE-bench Verified data while reducing the cost of labeling 1,000
instances from around $100,000 (manual annotation) to just $5.10. These results
demonstrate SPICE's potential to enable cost-effective, large-scale dataset
creation for SE-focused FMs. To support the community, we release both SPICE
tool and SPICE Bench, a new dataset of 6,802 SPICE-labeled instances curated
from 291 open-source projects in SWE-Gym (over 13x larger than SWE-bench
Verified).

</details>


### [9] [Position Paper: Programming Language Techniques for Bridging LLM Code Generation Semantic Gaps](https://arxiv.org/abs/2507.09135)
*Yalong Du,Chaozheng Wang,Huaijin Wang*

Main category: cs.SE

TL;DR: 论文主张通过编程语言（PL）技术弥补大语言模型（LLM）在代码生成中的语义缺陷，以提升代码的可靠性和可信度。


<details>
  <summary>Details</summary>
Motivation: LLM生成的代码存在语法错误、语义幻觉和可靠性问题，需通过PL技术填补这些语义鸿沟。

Method: 采用结构化程序表示、形式化正确性保证和鲁棒验证机制等PL技术。

Result: PL技术可将LLM生成的代码从统计模式匹配提升到可靠和可信的水平。

Conclusion: PL与LLM的整合对生成功能正确、可解释、可验证且可信的代码至关重要。

Abstract: Large Language Models have demonstrated remarkable capabilities in automated
code generation, yet their statistical nature and black-box characteristics
create significant semantic gaps manifested through syntax errors, semantic
hallucinations, and reliability concerns. This position paper argues that
principled integration of Programming Language (PL) techniques is essential for
bridging these gaps. Through structured program representations, formal
correctness guarantees, and robust verification mechanisms, PL techniques can
elevate LLM-generated code from statistical pattern matching to truly reliable
and trustworthy levels. This integration is crucial for developing systems that
generate code that is not only functionally correct but also interpretable,
verifiable, and ultimately trustworthy.

</details>


### [10] [OpenCAMS: An Open-Source Connected and Automated Mobility Co-Simulation Platform for Advanced Transportation Research](https://arxiv.org/abs/2507.09186)
*Minhaj Uddin Ahmad,Akid Abrar,Sagar Dasgupta,Mizanur Rahman*

Main category: cs.SE

TL;DR: OpenCAMS是一个开源、同步且可扩展的协同仿真平台，整合了SUMO、CARLA和OMNeT++三大仿真工具，用于交通、感知和通信领域的研究。


<details>
  <summary>Details</summary>
Motivation: 为了支持交通安全性、移动性和网络安全的高级研究，结合各仿真工具的优势。

Method: 采用时间同步的双向耦合架构，整合SUMO（交通建模）、CARLA（3D感知与控制）和OMNeT++（网络通信）。

Result: 提供了一个模块化、可扩展且可复现的协同仿真平台。

Conclusion: OpenCAMS为智能交通系统的研究提供了灵活、协作的开源环境。

Abstract: We introduce OpenCAMS (Open-Source Connected and Automated Mobility
Co-Simulation Platform), an open-source, synchronized, and extensible
co-simulation framework that tightly couples three best-in-class simulation
tools: (i) SUMO, (ii) CARLA, and (iii) OMNeT++. OpenCAMS is designed to support
advanced research in transportation safety, mobility, and cybersecurity by
combining the strengths of each simulation domain. Specifically, SUMO provides
large-scale, microscopic traffic modeling; CARLA offers high-fidelity 3D
perception, vehicle dynamics, and control simulation; and OMNeT++ enables
modular, event-driven network communication, such as cellular
vehicle-to-everything (C-V2X). OpenCAMS employs a time-synchronized,
bidirectional coupling architecture that ensures coherent simulation
progression across traffic, perception, and communication domains while
preserving modularity and reproducibility. For example, CARLA can simulate and
render a subset of vehicles that require detailed sensor emulation and control
logic; SUMO orchestrates network-wide traffic flow, vehicle routing, and
traffic signal management; and OMNeT++ dynamically maps communication nodes to
both mobile entities (e.g., vehicles) and static entities (e.g., roadside
units) to enable C-V2X communication. While these three simulators form the
foundational core of OpenCAMS, the platform is designed to be expandable and
future-proof, allowing additional simulators to be integrated on top of this
core without requiring fundamental changes to the system architecture. The
OpenCAMS platform is fully open-source and publicly available through its
GitHub repository https://github.com/minhaj6/carla-sumo-omnetpp-cosim,
providing the research community with an accessible, flexible, and
collaborative environment for advancing next-generation intelligent
transportation systems.

</details>


### [11] [Back to the Basics: Rethinking Issue-Commit Linking with LLM-Assisted Retrieval](https://arxiv.org/abs/2507.09199)
*Huihui Huang,Ratnadira Widyasari,Ting Zhang,Ivana Clairine Irsan,Jieke Shi,Han Wei Ang,Frank Liauw,Eng Lieh Ouh,Lwin Khin Shar,Hong Jin Kang,David Lo*

Main category: cs.SE

TL;DR: 论文提出了一种更现实的评估设置（RDS）来测试问题-提交链接工具的性能，并发现现有方法在真实场景中表现不佳。基于此，作者提出了EasyLink，结合向量数据库和大语言模型，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 现有问题-提交链接工具的评估忽略了真实环境中大量无关提交的干扰，导致性能被高估。

Method: 提出Realistic Distribution Setting (RDS)构建更真实的评估数据集，并设计EasyLink，结合向量数据库和大语言模型进行重新排序。

Result: 在RDS评估下，现有深度学习方法性能下降超过一半，而EasyLink的Precision@1达到75.91%，显著优于现有方法。

Conclusion: EasyLink通过结合现代信息检索技术和语言模型，有效解决了问题-提交链接中的语义鸿沟问题，并提供了研究改进的实用指南。

Abstract: Issue-commit linking, which connects issues with commits that fix them, is
crucial for software maintenance. Existing approaches have shown promise in
automatically recovering these links. Evaluations of these techniques assess
their ability to identify genuine links from plausible but false links.
However, these evaluations overlook the fact that, in reality, when a
repository has more commits, the presence of more plausible yet unrelated
commits may interfere with the tool in differentiating the correct fix commits.
To address this, we propose the Realistic Distribution Setting (RDS) and use it
to construct a more realistic evaluation dataset that includes 20 open-source
projects. By evaluating tools on this dataset, we observe that the performance
of the state-of-the-art deep learning-based approach drops by more than half,
while the traditional Information Retrieval method, VSM, outperforms it.
  Inspired by these observations, we propose EasyLink, which utilizes a vector
database as a modern Information Retrieval technique. To address the
long-standing problem of the semantic gap between issues and commits, EasyLink
leverages a large language model to rerank the commits retrieved from the
database. Under our evaluation, EasyLink achieves an average Precision@1 of
75.91%, improving over the state-of-the-art by over four times. Additionally,
this paper provides practical guidelines for advancing research in issue-commit
link recovery.

</details>


### [12] [Explainability as a Compliance Requirement: What Regulated Industries Need from AI Tools for Design Artifact Generation](https://arxiv.org/abs/2507.09220)
*Syed Tauhid Ullah Shah,Mohammad Hussein,Ann Barcomb,Mohammad Moshirpour*

Main category: cs.SE

TL;DR: 研究探讨了AI工具在需求工程中生成设计工件时的可解释性差距，揭示了其在安全关键行业中的挑战和改进方向。


<details>
  <summary>Details</summary>
Motivation: AI工具（尤其是基于NLP的）在需求工程中提高效率的潜力受到可解释性不足的限制，尤其是在需要透明度和可追溯性的受监管行业。

Method: 通过半结构化访谈调查了十位来自安全关键行业的从业者，分析AI工具的集成、挑战及缓解策略。

Result: 研究发现，不可解释的AI输出导致手动验证增加、信任降低、领域术语处理困难、团队协作受阻及合规风险，抵消了效率优势。

Conclusion: 提出了改进方向（如源追踪、决策解释、领域适应和合规验证），为提升AI工具在受监管环境中的透明度和可靠性提供了路线图。

Abstract: Artificial Intelligence (AI) tools for automating design artifact generation
are increasingly used in Requirements Engineering (RE) to transform textual
requirements into structured diagrams and models. While these AI tools,
particularly those based on Natural Language Processing (NLP), promise to
improve efficiency, their adoption remains limited in regulated industries
where transparency and traceability are essential. In this paper, we
investigate the explainability gap in AI-driven design artifact generation
through semi-structured interviews with ten practitioners from safety-critical
industries. We examine how current AI-based tools are integrated into workflows
and the challenges arising from their lack of explainability. We also explore
mitigation strategies, their impact on project outcomes, and features needed to
improve usability. Our findings reveal that non-explainable AI outputs
necessitate extensive manual validation, reduce stakeholder trust, struggle to
handle domain-specific terminology, disrupt team collaboration, and introduce
regulatory compliance risks, often negating the anticipated efficiency
benefits. To address these issues, we identify key improvements, including
source tracing, providing clear justifications for tool-generated decisions,
supporting domain-specific adaptation, and enabling compliance validation. This
study outlines a practical roadmap for improving the transparency, reliability,
and applicability of AI tools in requirements engineering workflows,
particularly in regulated and safety-critical environments where explainability
is crucial for adoption and certification.

</details>


### [13] [Enhancing Interpretability in Software Change Management with Chain-of-Thought Reasoning](https://arxiv.org/abs/2507.09315)
*Yongqian Sun,Weihua Kuang,Chao Shen,Xidao Wen,Tinghua Zheng,Heng Liu,Shenglin Zhang,Bo Wu,Dan Pei*

Main category: cs.SE

TL;DR: SCELM是一个端到端的自动化框架，用于高效管理软件变更，减少服务故障和经济损失。


<details>
  <summary>Details</summary>
Motivation: 现代在线服务中频繁的软件变更带来重大风险，需要一种高效的管理方法。

Method: 提出了SCELM框架，实现端到端的自动化软件变更管理。

Result: SCELM能够显著减少服务故障和经济损失。

Conclusion: SCELM为软件变更管理提供了一种高效且精确的解决方案。

Abstract: In modern online services, frequent software changes introduce significant
risks. To tackle this challenge, we propose SCELM (Software Change Evaluation
and Lifecycle Management), an end-to-end automated framework for software
change management. SCELM aims to manage software changes efficiently and
precisely, significantly reducing service failures and economic losses.

</details>


### [14] [Enhancing NeuroEvolution-Based Game Testing: A Branch Coverage Approach for Scratch Programs](https://arxiv.org/abs/2507.09414)
*Khizra Sohail,Atif Aftab Ahmed Jilani,Nigar Azhar Butt*

Main category: cs.SE

TL;DR: 论文提出了一种基于分支覆盖的适应度函数，用于增强游戏自动化测试的效果，通过扩展NEATEST框架，实验证明该方法在复杂条件结构的游戏中表现更好。


<details>
  <summary>Details</summary>
Motivation: 游戏程序的非确定性和复杂控制结构使得自动化测试生成具有挑战性，仅依赖语句覆盖不足以检测所有逻辑分支的故障。

Method: 扩展NEATEST框架，引入分支覆盖适应度函数，优先处理控制依赖分支，并通过神经进化过程最大化分支探索。

Result: 在25个Scratch游戏中，NBC在13个游戏中实现了更高的分支覆盖率，且在突变测试中表现出更低的误报率。

Conclusion: 基于分支覆盖的测试生成方法能显著提高Scratch程序的测试覆盖率和故障检测能力。

Abstract: Automated test generation for game-like programs presents unique challenges
due to their non-deterministic behavior and complex control structures. The
NEATEST framework has been used for automated testing in Scratch games,
employing neuroevolution-based test generation optimized for statement
coverage. However, statement coverage alone is often insufficient for fault
detection, as it does not guarantee execution of all logical branches. This
paper introduces a branch coverage-based fitness function to enhance test
effectiveness in automated game testing. We extend NEATEST by integrating a
branch fitness function that prioritizes control-dependent branches, guiding
the neuroevolution process to maximize branch exploration. To evaluate the
effectiveness of this approach, empirical experiments were conducted on 25
Scratch games, comparing Neatest with Statement Coverage (NSC) against Neatest
with Branch Coverage (NBC). A mutation analysis was also performed to assess
the fault detection capabilities of both techniques. The results demonstrate
that NBC achieves higher branch coverage than NSC in 13 out of 25 games,
particularly in programs with complex conditional structures. Moreover, NBC
achieves a lower false positive rate in mutation testing, making it a more
reliable approach for identifying faulty behavior in game programs. These
findings confirm that branch coverage-based test generation improves test
coverage and fault detection in Scratch programs.

</details>


### [15] [Evaluating LLMs on Sequential API Call Through Automated Test Generation](https://arxiv.org/abs/2507.09481)
*Yuheng Huang,Da Song,Zhenlan Ji,Shuai Wang,Lei Ma*

Main category: cs.SE

TL;DR: StateGen是一个自动化框架，用于生成涉及顺序API交互的多样化编码任务，填补了现有基准测试的不足。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试依赖手动收集的测试用例，无法自动检查语义正确性，且忽略了顺序API调用的复杂交互。

Method: StateGen结合状态机约束求解、能量采样和控制流注入生成可执行程序，并通过两个LLM代理转换为自然语言任务描述。

Result: 构建了StateEval基准，包含120个已验证测试用例，覆盖三个代表性场景，实验证明StateGen能有效生成具有挑战性的API任务。

Conclusion: StateGen为LLM工具使用的测试和评估提供了新方法，揭示了当前LLM在API集成中的改进空间。

Abstract: By integrating tools from external APIs, Large Language Models (LLMs) have
expanded their promising capabilities in a diverse spectrum of complex
real-world tasks. However, testing, evaluation, and analysis of LLM tool use
remain in their early stages. Most existing benchmarks rely on manually
collected test cases, many of which cannot be automatically checked for
semantic correctness and instead depend on static methods such as string
matching. Additionally, these benchmarks often overlook the complex
interactions that occur between sequential API calls, which are common in
real-world applications. To fill the gap, in this paper, we introduce StateGen,
an automated framework designed to generate diverse coding tasks involving
sequential API interactions. StateGen combines state-machine-based API
constraint solving and validation, energy-based sampling, and control-flow
injection to generate executable programs. These programs are then translated
into human-like natural language task descriptions through a collaboration of
two LLM agents. Utilizing StateGen, we construct StateEval, a benchmark
encompassing 120 verified test cases spanning across three representative
scenarios: Session Service, Tensor Operation, and ElevenLabs MCP. Experimental
results confirm that StateGen can effectively generate challenging and
realistic API-oriented tasks, highlighting areas for improvement in current
LLMs incorporating APIs.

</details>


### [16] [Towards LLM-Based Automatic Playtest](https://arxiv.org/abs/2507.09490)
*Yan Zhao,Chiwei Tang*

Main category: cs.SE

TL;DR: 论文提出了一种基于LLM的自动游戏测试方法Lap，利用ChatGPT测试匹配三消类游戏，通过环境处理、动作生成和执行三阶段实现，实验表明其优于现有工具。


<details>
  <summary>Details</summary>
Motivation: 手动游戏测试耗时且昂贵，传统自动化工具缺乏领域知识和问题解决能力，而现有LLM无法视觉感知游戏环境，因此需要新方法解决这些问题。

Method: Lap方法包括游戏环境处理、基于提示的动作生成和动作执行三阶段，通过迭代循环测试游戏。

Result: 在开源游戏CasseBonbons上的实验表明，Lap在代码覆盖率和触发程序崩溃方面优于现有工具。

Conclusion: Lap为自动测试和LLM应用提供了新思路，展示了其潜力。

Abstract: Playtesting is the process in which people play a video game for testing. It
is critical for the quality assurance of gaming software. Manual playtesting is
time-consuming and expensive. However, automating this process is challenging,
as playtesting typically requires domain knowledge and problem-solving skills
that most conventional testing tools lack. Recent advancements in artificial
intelligence (AI) have opened up new possibilities for applying Large Language
Models (LLMs) to playtesting. However, significant challenges remain: current
LLMs cannot visually perceive game environments, and most existing research
focuses on text-based games or games with robust APIs. Many non-text games lack
APIs to provide textual descriptions of game states, making it almost
impossible to naively apply LLMs for playtesting. This paper introduces Lap,
our novel approach to LLM-based Automatic Playtesting, which uses ChatGPT to
test match-3 games, a category of games where players match three or more
identical tiles in a row or column to earn points. Lap encompasses three key
phases: processing of game environments, prompting-based action generation, and
action execution. Given a match-3 game, Lap takes a snapshot of the game board
and converts it to a numeric matrix. It then prompts the ChatGPT-O1-mini API to
suggest moves based on that matrix and tentatively applies the suggested moves
to earn points and trigger changes in the game board. It repeats the
above-mentioned three steps iteratively until timeout. For evaluation, we
conducted a case study using Lap on an open-source match-3 game, CasseBonbons,
and empirically compared it with three existing tools. Our results are
promising: Lap outperformed existing tools by achieving higher code coverage
and triggering more program crashes. This research sheds light on the future of
automatic testing and LLM applications.

</details>


### [17] [It Only Gets Worse: Revisiting DL-Based Vulnerability Detectors from a Practical Perspective](https://arxiv.org/abs/2507.09529)
*Yunqian Wang,Xiaohong Li,Yao Zhang,Yuekang Li,Zhiping Zhou,Ruitao Feng*

Main category: cs.SE

TL;DR: 论文提出VulTegra框架，评估基于深度学习的漏洞检测模型，发现现有方法在一致性、实际效果和扩展性方面存在问题，并指出预训练模型并非总是优于从头训练的模型。


<details>
  <summary>Details</summary>
Motivation: 随着软件漏洞威胁的增加，基于深度学习的漏洞检测器日益流行，但其一致性、实际效果和跨场景适用性仍存疑，可能导致不可靠的检测结果。

Method: 提出VulTegra框架，通过多维比较从头训练和预训练的深度学习模型，分析其性能。

Result: 实验表明，调整关键因素可显著提升检测器的召回率和F1分数，同时揭示了CWE分类的局限性。

Conclusion: 研究强调了结合漏洞类型和代码特征的重要性，为模型设计和部署提供了改进方向。

Abstract: With the growing threat of software vulnerabilities, deep learning (DL)-based
detectors have gained popularity for vulnerability detection. However, doubts
remain regarding their consistency within declared CWE ranges, real-world
effectiveness, and applicability across scenarios. These issues may lead to
unreliable detection, high false positives/negatives, and poor adaptability to
emerging vulnerabilities. A comprehensive analysis is needed to uncover
critical factors affecting detection and guide improvements in model design and
deployment. In this paper, we present VulTegra, a novel evaluation framework
that conducts a multidimensional comparison of scratch-trained and
pre-trained-based DL models for vulnerability detection. VulTegra reveals that
state-of-the-art (SOTA) detectors still suffer from low consistency, limited
real-world capabilities, and scalability challenges. Contrary to common belief,
pre-trained models are not consistently better than scratch-trained models but
exhibit distinct strengths in specific contexts.Importantly, our study exposes
the limitations of relying solely on CWE-based classification and identifies
key factors that significantly affect model performance. Experimental results
show that adjusting just one such factor consistently improves recall across
all seven evaluated detectors, with six also achieving better F1 scores. Our
findings provide deeper insights into model behavior and emphasize the need to
consider both vulnerability types and inherent code features for effective
detection.

</details>


### [18] [A Serverless Architecture for Real-Time Stock Analysis using Large Language Models: An Iterative Development and Debugging Case Study](https://arxiv.org/abs/2507.09583)
*Taniv Ashraf*

Main category: cs.SE

TL;DR: 论文介绍了一种基于Google Gemini API的实时股票分析系统，通过无服务器架构和自动化工具实现低成本高效运行，并探讨了调试过程和人类与AI协作的开发模式。


<details>
  <summary>Details</summary>
Motivation: 利用大型语言模型（如Gemini）的潜力，为个人提供低成本、高效的金融数据分析工具，推动金融分析的民主化。

Method: 设计并实现了一个无服务器系统，结合Gemini API进行定性分析，通过GitHub Actions自动化数据处理，并使用静态前端展示结果。

Result: 系统最终以接近零成本运行，展示了个人构建复杂AI金融工具的可行性，并公开了应用和源代码。

Conclusion: 论文强调了LLM在金融分析中的作用、调试方法的重要性，以及人类与AI协作在软件开发中的新兴模式。

Abstract: The advent of powerful, accessible Large Language Models (LLMs) like Google's
Gemini presents new opportunities for democratizing financial data analysis.
This paper documents the design, implementation, and iterative debugging of a
novel, serverless system for real-time stock analysis. The system leverages the
Gemini API for qualitative assessment, automates data ingestion and processing
via GitHub Actions, and presents the findings through a decoupled, static
frontend. We detail the architectural evolution of the system, from initial
concepts to a robust, event-driven pipeline, highlighting the practical
challenges encountered during deployment. A significant portion of this paper
is dedicated to a case study on the debugging process, covering common software
errors, platform-specific permission issues, and rare, environment-level
platform bugs. The final architecture operates at a near-zero cost,
demonstrating a viable model for individuals to build sophisticated AI-powered
financial tools. The operational application is publicly accessible, and the
complete source code is available for review. We conclude by discussing the
role of LLMs in financial analysis, the importance of robust debugging
methodologies, and the emerging paradigm of human-AI collaboration in software
development.

</details>


### [19] [How to Define Design in Industrial Control and Automation Software](https://arxiv.org/abs/2507.09594)
*Aydin Homay*

Main category: cs.SE

TL;DR: 论文探讨了设计在工程中的重要性，特别是在软件和工业控制系统中缺乏科学基础的问题，并提出了科学定义设计的方法。


<details>
  <summary>Details</summary>
Motivation: 设计在工程中至关重要，但缺乏科学基础导致主观决策，影响效率和创新。软件和工业控制系统领域尤为明显。

Method: 回顾软件行业的设计定义，挑战误解，基于设计理论定义科学设计，区分临时与系统化设计方法。

Result: 提出了科学定义设计的方法，并探讨了如何平衡操作与进化需求。

Conclusion: 科学的设计定义和方法能提升效率和创新，特别是在软件和工业控制系统中。

Abstract: Design is a fundamental aspect of engineering, enabling the creation of
products, systems, and organizations to meet societal and/or business needs.
However, the absence of a scientific foundation in design often results in
subjective decision-making, reducing both efficiency and innovation. This
challenge is particularly evident in the software industry and, by extension,
in the domain of industrial control and automation systems (iCAS).
  In this study, first we review the existing design definitions within the
software industry, challenge prevailing misconceptions about design, review
design definition in the field of design theory and address key questions such
as: When does design begin? How can design be defined scientifically? What
constitutes good design? and the difference between design and design language
by relying on advancements in the field of design theory. We also evaluate the
distinction between ad-hoc and systematic design approaches, and present
arguments on how to balance complementary operational concerns while resolving
conflicting evolutionary concerns.

</details>


### [20] [The Mythical Good Software](https://arxiv.org/abs/2507.09596)
*Aydin Homay*

Main category: cs.SE

TL;DR: 论文探讨了高内聚低耦合的局限性，指出其可能带来笨拙、模糊甚至有害的设计状态。


<details>
  <summary>Details</summary>
Motivation: 揭示高内聚低耦合原则在实际设计中的潜在问题，强调其并非绝对最优。

Method: 通过理论分析和哲学探讨，解释高内聚低耦合的局限性。

Result: 指出高内聚低耦合并非万能，需结合成本考量。

Conclusion: 设计时应权衡内聚与耦合，避免盲目追求高内聚。

Abstract: Good software has high cohesion and low coupling is clumsy, obscure, and in
some certain cases could be actually a harmful state of being. It is clumsy
because there is no perfect correlation between higher cohesiveness and optimum
design, and it is obscure because it conveys the message that coupling and
cohesion are two distinct design principles, while there are in principle the
same design approaches, and only the time and space differ between them, and it
could also be a harmful state of being because we should not always aim for
higher cohesiveness without considering its cost.
  In the course of this study, we aim to elucidate for the readers the meaning
and underlying philosophy of the aforementioned paragraph.

</details>


### [21] [Complexity and Coupling: A Functional Domain Approach](https://arxiv.org/abs/2507.09599)
*Aydin Homay*

Main category: cs.SE

TL;DR: 论文提出了基于功能域的复杂性和耦合的科学定义，强调现有物理属性定义的模糊性，并探讨耦合设计如何增加复杂性及其可能的减少方法。


<details>
  <summary>Details</summary>
Motivation: 工业控制和自动化系统中复杂性和耦合定义的模糊性导致混淆和不一致，需要更科学的定义。

Method: 通过软件工程、工业自动化和机械设计等多学科案例，分析复杂性和耦合在功能域的表现。

Result: 复杂性不必然与系统规模或组件数量相关，耦合发生在功能域而非物理域。

Conclusion: 有效设计需在功能域内解决耦合和复杂性问题。

Abstract: This paper provides a precise and scientific definition of complexity and
coupling, grounded in the functional domain, particularly within industrial
control and automation systems (iCAS). We highlight the widespread ambiguity in
defining complexity and coupling, emphasizing that many existing definitions
rooted in physical attributes lead to confusion and inconsistencies.
Furthermore, we re-exhibit why coupled design inherently increases complexity
and how potentially this complexity could be reduced. Drawing on examples from
various disciplines, such as software engineering, industrial automation, and
mechanical design, we demonstrate that complexity does not necessarily
correlate with system size or the number of components, and coupling, unlike
common belief in software engineering, actually does not occur in the physical
domain but in the functional domain. We conclude that effective design
necessitates addressing coupling and complexity within the functional domain.

</details>


### [22] [Code Review as Decision-Making -- Building a Cognitive Model from the Questions Asked During Code Review](https://arxiv.org/abs/2507.09637)
*Lo Gullstrand Heander,Emma Söderberg,Christofer Rydenfält*

Main category: cs.SE

TL;DR: 论文探讨了代码审查的认知过程，提出了CRDM模型，强调代码审查的两个阶段：定位阶段和分析阶段。


<details>
  <summary>Details</summary>
Motivation: 当前代码审查工具和流程存在挑战，自动化可能牺牲人际互动的好处。通过理解认知过程，改进工具支持，提升效率和体验。

Method: 通过10名参与者和34次代码审查的民族志有声思维研究，构建了认知模型。

Result: 提出了CRDM模型，展示开发者在代码审查中的两个阶段和决策过程。

Conclusion: 理解代码审查的认知过程有助于改进工具设计，同时保留人际互动的好处。

Abstract: Code review is a well-established and valued practice in the software
engineering community contributing to both code quality and interpersonal
benefits. However, there are challenges in both tools and processes that give
rise to misalignments and frustrations. Recent research seeks to address this
by automating code review entirely, but we believe that this risks losing the
majority of the interpersonal benefits such as knowledge transfer and shared
ownership.
  We believe that by better understanding the cognitive processes involved in
code review, it would be possible to improve tool support, with out without AI,
and make code review both more efficient, more enjoyable, while increasing or
maintaining all of its benefits. In this paper, we conduct an ethnographic
think-aloud study involving 10 participants and 34 code reviews. We build a
cognitive model of code review bottom up through thematic, statistical,
temporal, and sequential analysis of the transcribed material. Through the
data, the similarities between the cognitive process in code review and
decision-making processes, especially recognition-primed decision-making,
become apparent.
  The result is the Code Review as Decision-Making (CRDM) model that shows how
the developers move through two phases during the code review; first an
orientation phase to establish context and rationale and then an analytical
phase to understand, assess, and plan the rest of the review. Throughout the
process several decisions must be taken, on writing comments, finding more
information, voting, running the code locally, verifying continuous integration
results, etc.
  Analysis software and process-coded data publicly available at:
https://doi.org/10.5281/zenodo.15758266

</details>


### [23] [Is Quantization a Deal-breaker? Empirical Insights from Large Code Models](https://arxiv.org/abs/2507.09665)
*Saima Afrin,Bowen Xu,Antonio Mastropaolo*

Main category: cs.SE

TL;DR: 模型量化是减少大型语言模型资源需求的有效方法，但对代码质量的影响研究不足。本文通过量化CodeLlama和DeepSeekCoder生成代码，发现量化不仅保持功能正确性，还保留关键代码质量属性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）的规模增长带来资源消耗和环境问题。量化虽能减少资源需求，但现有研究仅关注功能正确性，忽略了代码质量（如可靠性、可维护性和安全性）。

Method: 应用激活感知权重量化（AWQ）于CodeLlama和DeepSeekCoder，生成Java和Python代码，并使用静态分析工具评估代码质量指标（如圈复杂度、认知复杂度和代码行数）。

Result: 量化不仅保持功能正确性，还保留了开发者重视的代码质量属性（如可维护性和结构简洁性）。

Conclusion: 量化是一种稳健的技术，适用于优化大型代码模型，同时保持代码质量。

Abstract: The growing scale of large language models (LLMs) not only demands extensive
computational resources but also raises environmental concerns due to their
increasing carbon footprint. Model quantization emerges as an effective
approach that can reduce the resource demands of LLMs by decreasing parameter
precision without substantially affecting performance (e.g., 16 bit to 4 bit).
While recent studies have established quantization as a promising approach for
optimizing large code models (LCMs), a specialized subset of LLMs tailored for
automated software engineering, their findings offer only limited insights into
its practical implications. Specifically, current investigations focus only on
the functional correctness of the code generated by quantized models,
neglecting how quantization impacts critical aspects of code quality such as
reliability, maintainability, and security. To bridge this gap, our study
investigates the effects of quantization on the qualitative aspects of
automatically generated code. We apply Activation-aware Weight Quantization
(AWQ) to two widely used code models, CodeLlama and DeepSeekCoder, to generate
Java and Python code. Using state-of-the-art static analysis tools, we evaluate
software quality metrics and static features including cyclomatic complexity,
cognitive complexity, and lines of code. Our findings reveal that quantization
is a robust technique that not only preserves functional correctness, but also
retains key qualitative code attributes sought after by developers, such as
maintainability and structural simplicity.

</details>


### [24] [OrQstrator: An AI-Powered Framework for Advanced Quantum Circuit Optimization](https://arxiv.org/abs/2507.09682)
*Laura Baird,Armin Moin*

Main category: cs.SE

TL;DR: OrQstrator是一个基于深度强化学习的模块化框架，用于在NISQ时代优化量子电路。它通过智能选择三种互补的优化器，协调优化策略，输出适应硬件约束的优化电路。


<details>
  <summary>Details</summary>
Motivation: 在NISQ时代，量子电路的优化面临噪声和硬件限制的挑战，需要一种智能、自适应的优化方法。

Method: 框架包含三个优化器：基于DRL的电路重写器、领域特定优化器和参数化电路实例化器，由中央协调引擎根据电路结构和硬件约束动态选择。

Result: 系统能够输出适应硬件约束的优化电路，提升门数、深度和保真度等性能指标。

Conclusion: OrQstrator通过模块化和智能协调，为NISQ时代的量子电路优化提供了高效解决方案。

Abstract: We propose a novel approach, OrQstrator, which is a modular framework for
conducting quantum circuit optimization in the Noisy Intermediate-Scale Quantum
(NISQ) era. Our framework is powered by Deep Reinforcement Learning (DRL). Our
orchestration engine intelligently selects among three complementary circuit
optimizers: A DRL-based circuit rewriter trained to reduce depth and gate count
via learned rewrite sequences; a domain-specific optimizer that performs
efficient local gate resynthesis and numeric optimization; a parameterized
circuit instantiator that improves compilation by optimizing template circuits
during gate set translation. These modules are coordinated by a central
orchestration engine that learns coordination policies based on circuit
structure, hardware constraints, and backend-aware performance features such as
gate count, depth, and expected fidelity. The system outputs an optimized
circuit for hardware-aware transpilation and execution, leveraging techniques
from an existing state-of-the-art approach, called the NISQ Analyzer, to adapt
to backend constraints.

</details>


### [25] [Prompting for Performance: Exploring LLMs for Configuring Software](https://arxiv.org/abs/2507.09790)
*Helge Spieker,Théo Matricon,Nassim Belmecheri,Jørn Eirik Betten,Gauthier Le Bartz Lyan,Heraldo Borges,Quentin Mazouni,Dennis Gross,Arnaud Gotlieb,Mathieu Acher*

Main category: cs.SE

TL;DR: 研究探讨了大型语言模型（LLMs）是否能通过提示辅助性能导向的软件配置，初步结果显示LLMs在某些任务中表现良好，但也存在幻觉或浅层推理的问题。


<details>
  <summary>Details</summary>
Motivation: 软件配置选项众多且影响性能，但传统机器学习方法计算成本高，因此探索LLMs是否能提供帮助。

Method: 评估多个LLMs在识别相关选项、排序配置和推荐高性能配置等任务上的表现，涉及编译器、视频编码器和SAT求解器等系统。

Result: LLMs在某些任务中与专家知识一致，但在其他情况下可能出现幻觉或浅层推理。

Conclusion: 研究为系统评估和设计基于LLM的软件配置辅助方案迈出了第一步。

Abstract: Software systems usually provide numerous configuration options that can
affect performance metrics such as execution time, memory usage, binary size,
or bitrate. On the one hand, making informed decisions is challenging and
requires domain expertise in options and their combinations. On the other hand,
machine learning techniques can search vast configuration spaces, but with a
high computational cost, since concrete executions of numerous configurations
are required. In this exploratory study, we investigate whether large language
models (LLMs) can assist in performance-oriented software configuration through
prompts. We evaluate several LLMs on tasks including identifying relevant
options, ranking configurations, and recommending performant configurations
across various configurable systems, such as compilers, video encoders, and SAT
solvers. Our preliminary results reveal both positive abilities and notable
limitations: depending on the task and systems, LLMs can well align with expert
knowledge, whereas hallucinations or superficial reasoning can emerge in other
cases. These findings represent a first step toward systematic evaluations and
the design of LLM-based solutions to assist with software configuration.

</details>


### [26] [Measuring What Matters: A Framework for Evaluating Safety Risks in Real-World LLM Applications](https://arxiv.org/abs/2507.09820)
*Jia Yi Goh,Shaun Khoo,Nyx Iskandar,Gabriel Chua,Leanne Tan,Jessica Foo*

Main category: cs.SE

TL;DR: 本文提出了一种评估大语言模型（LLM）应用级安全性的实用框架，填补了理论概念与实际操作之间的差距。


<details>
  <summary>Details</summary>
Motivation: 当前的安全测试主要集中在基础模型上，但应用级组件（如系统提示、检索管道和防护栏）对LLM应用的整体安全性有显著影响，需要专门的评估方法。

Method: 框架包括两部分：(1) 开发定制化安全风险分类的原则，(2) 评估LLM应用安全风险的实践。通过实际部署验证了框架的有效性。

Result: 框架在多个实际用例中进行了验证，为组织扩展安全测试提供了参考。

Conclusion: 该框架为LLM应用的安全部署提供了可操作的指导，旨在实现安全且可扩展的部署。

Abstract: Most safety testing efforts for large language models (LLMs) today focus on
evaluating foundation models. However, there is a growing need to evaluate
safety at the application level, as components such as system prompts,
retrieval pipelines, and guardrails introduce additional factors that
significantly influence the overall safety of LLM applications. In this paper,
we introduce a practical framework for evaluating application-level safety in
LLM systems, validated through real-world deployment across multiple use cases
within our organization. The framework consists of two parts: (1) principles
for developing customized safety risk taxonomies, and (2) practices for
evaluating safety risks in LLM applications. We illustrate how the proposed
framework was applied in our internal pilot, providing a reference point for
organizations seeking to scale their safety testing efforts. This work aims to
bridge the gap between theoretical concepts in AI safety and the operational
realities of safeguarding LLM applications in practice, offering actionable
guidance for safe and scalable deployment.

</details>


### [27] [Turning the Tide: Repository-based Code Reflection](https://arxiv.org/abs/2507.09866)
*Wei Zhang,Jian Yang,Jiaxi Yang,Ya Wang,Zhoujun Li,Zeyu Cui,Binyuan Hui,Junyang Lin*

Main category: cs.SE

TL;DR: 论文提出了LiveRepoReflection基准和RepoReflection-Instruct数据集，用于评估和改进代码LLM在多文件仓库环境中的代码理解和生成能力。


<details>
  <summary>Details</summary>
Motivation: 现有基准忽略了代码仓库修改场景，且动态基准中存在数据污染和反思能力不足的问题。

Method: 引入LiveRepoReflection基准（1,888个测试用例）和RepoReflection-Instruct数据集，通过两轮对话训练RepoReflectionCoder。

Result: 评估了40多个LLM，展示了模型在仓库代码反思中的性能。

Conclusion: LiveRepoReflection和RepoReflection-Instruct为代码LLM提供了更具挑战性和实用性的评估框架。

Abstract: Code large language models (LLMs) enhance programming by understanding and
generating code across languages, offering intelligent feedback, bug detection,
and code updates through reflection, improving development efficiency and
accessibility. While benchmarks (e.g. HumanEval/LiveCodeBench) evaluate code
generation and real-world relevance, previous works ignore the scenario of
modifying code in repositories. Considering challenges remaining in improving
reflection capabilities and avoiding data contamination in dynamic benchmarks,
we introduce LiveRepoReflection, a challenging benchmark for evaluating code
understanding and generation in multi-file repository contexts, featuring 1,888
rigorously filtered test cases across $6$ programming languages to ensure
diversity, correctness, and high difficulty. Further, we create
RepoReflection-Instruct, a large-scale, quality-filtered instruction-tuning
dataset derived from diverse sources, used to train RepoReflectionCoder through
a two-turn dialogue process involving code generation and error-driven repair.
The leaderboard evaluates over 40 LLMs to reflect the model performance of
repository-based code reflection.

</details>


### [28] [PathFuzzing: Worst Case Analysis by Fuzzing Symbolic-Execution Paths](https://arxiv.org/abs/2507.09892)
*Zimu Chen,Di Wang*

Main category: cs.SE

TL;DR: PathFuzzing结合模糊测试和符号执行的优点，通过将程序转换为符号程序并应用进化模糊测试技术，优化最坏情况资源消耗分析。


<details>
  <summary>Details</summary>
Motivation: 解决模糊测试覆盖率低和符号执行路径爆炸问题，提升最坏情况分析的效果。

Method: 将程序转换为符号程序，利用二进制字符串编码执行路径，应用进化模糊测试技术搜索高资源消耗路径。

Result: 实验表明PathFuzzing在基准测试中优于传统模糊测试和符号执行方法。

Conclusion: PathFuzzing为最坏情况资源消耗分析提供了一种高效的新方法。

Abstract: Estimating worst-case resource consumption is a critical task in software
development. The worst-case analysis (WCA) problem is an optimization-based
abstraction of this task. Fuzzing and symbolic execution are widely used
techniques for addressing the WCA problem. However, improving code coverage in
fuzzing or managing path explosion in symbolic execution within the context of
WCA poses significant challenges. In this paper, we propose PathFuzzing, aiming
to combine the strengths of both techniques to design a WCA method. The key
idea is to transform a program into a symbolic one that takes an execution path
(encoded as a binary string) and interprets the bits as branch decisions.
PathFuzzing then applies evolutionary fuzzing techniques to the transformed
program to search for binary strings that represent satisfiable path conditions
and lead to high resource consumption. We evaluate the performance of
PathFuzzing experimentally on a benchmark suite that consists of prior work's
benchmarks and some added by us. Results show that PathFuzzing generally
outperforms a fuzzing and a symbolic-execution baseline.

</details>


### [29] [Modelling Interrelations Between Agile Practices: The Agile Map](https://arxiv.org/abs/2507.09907)
*Thomas Hansper,Kevin Phong Pham,Michael Neumann*

Main category: cs.SE

TL;DR: 本文提出了Agile Map模型，旨在系统化描述敏捷实践之间的相互关系，帮助从业者更有意义地选择和组合敏捷实践。


<details>
  <summary>Details</summary>
Motivation: 敏捷实践在广泛使用中被大量调整和采用，导致其多样性增加，但缺乏对实践间相互关系的系统理解，这限制了其组合使用的效果。

Method: 研究通过系统化方法识别敏捷实践之间的相互关系，并构建Agile Map理论模型。

Result: Agile Map模型成功描述了敏捷实践之间的关联，为实践者提供了选择和组合实践的理论支持。

Conclusion: Agile Map模型为解决敏捷实践多样性和组合问题提供了有效工具，有助于提升敏捷方法的实践效果。

Abstract: Agile methods are defined through guidelines comprising various practices
intended to enable agile ways of working. These guidelines further comprise a
specific set of agile practices aiming to enable teams for an agile way of
working. However, due to its wide-spread use in practice we know that agile
practices are adopted and tailored intensively, which lead to a high variety of
agile practices in terms of their level of detail. Problem: A high variety of
agile practices can be challenging as we do not know how different agile
practices are interrelated with each other. To be more precise, tailoring and
adopting agile practices may lead to the challenge, that the combinatorial use
of several agile practices can only be successful to a limited extent, as
practices support or even require each other for a effective use in practice.
Objective: Our study aims to provide an enabler for this problem. We want to
identify interrelations between agile practices and describe them in a
systematic manner. Contribution: The core contribution of this paper is the
Agile Map, a theoretical model describing relations between agile practices
following a systematic approach aiming to provide an overview of coherences
between agile practices. The model aims to support practitioners in selecting
and combining agile practices in a meaningful way.

</details>


### [30] [When Less is More: A systematic review of four-day workweek conceptualizations and their effects on organizational performance](https://arxiv.org/abs/2507.09911)
*Marvin Auf der Landwehr,Julia Topp,Michael Neumann*

Main category: cs.SE

TL;DR: 本文研究了压缩工作制（如四天工作周）对IT企业运营效率的影响，并提出了一个综合框架。


<details>
  <summary>Details</summary>
Motivation: 敏捷IT组织需要灵活的工作环境以最大化价值创造，压缩工作制被认为能带来多种益处。

Method: 通过系统文献综述和网络内容分析，研究了四天工作周的概念及其组织和社会影响。

Result: 提出了一个元框架，指导根据管理前提和情境采用压缩工作制。

Conclusion: 压缩工作制对IT企业运营效率有积极影响，但需根据具体情况灵活应用。

Abstract: Context: Agile IT organizations, which are characterized by self-organization
and collaborative social interactions, require motivating, efficient and
flexible work environments to maximize value creation. Compressed work
schedules such as the four-day workweek have evolved into multiple facets over
the last decades and are associated with various benefits for organizations and
their employees. Objective: Our objective in this study is to deepen our
comprehension of the impact of compressed work schedules on the operational
efficacy of IT enterprises, while concurrently developing a comprehensive
framework delineating the intricacies of compressed work schedules.Method: We
conducted a systematic review of available conceptualizations related to
four-day workweek schedules and elaborate on their organizational and social
effects. To cover scientific and practice-oriented literature, our review
combined a systematic literature review and a web content analysis. Results:
Based on the generated insights, we derive a meta-framework that matches
conceptualizations and effects, finally guiding the adoption of compressed work
schedules based on individual managerial prerequisites and circumstances.

</details>


### [31] [Explicit Vulnerability Generation with LLMs: An Investigation Beyond Adversarial Attacks](https://arxiv.org/abs/2507.10054)
*Emir Bosnak,Sahand Moslemi,Mayasah Lami,Anil Koyuncu*

Main category: cs.SE

TL;DR: 研究探讨开源大语言模型（LLM）在直接或间接提示下生成不安全代码的行为，发现模型常生成漏洞代码，且用户角色和提示方式影响结果。


<details>
  <summary>Details</summary>
Motivation: 理解开源LLM在明确要求下生成不安全代码的行为，填补现有研究对直接威胁场景的空白。

Method: 采用动态提示和反向提示两种实验设计，评估三种开源模型生成代码的漏洞情况。

Result: 所有模型常生成漏洞代码，Qwen2正确率最高；学生角色更易成功，直接提示略有效；漏洞复现与代码复杂度呈倒U型关系。

Conclusion: 开源模型的安全机制存在局限，尤其在教育类请求中需加强防护。

Abstract: Large Language Models (LLMs) are increasingly used as code assistants, yet
their behavior when explicitly asked to generate insecure code remains poorly
understood. While prior research has focused on unintended vulnerabilities or
adversarial prompting techniques, this study examines a more direct threat
scenario: open-source LLMs generating vulnerable code when prompted either
directly or indirectly. We propose a dual experimental design: (1) Dynamic
Prompting, which systematically varies vulnerability type, user persona, and
directness across structured templates; and (2) Reverse Prompting, which
derives prompts from real vulnerable code samples to assess vulnerability
reproduction accuracy. We evaluate three open-source 7B-parameter models
(Qwen2, Mistral, and Gemma) using ESBMC static analysis to assess both the
presence of vulnerabilities and the correctness of the generated vulnerability
type. Results show all models frequently produce vulnerable outputs, with Qwen2
achieving highest correctness rates. User persona significantly affects
success, where student personas achieved higher vulnerability rates than
professional roles, while direct prompts were marginally more effective.
Vulnerability reproduction followed an inverted-U pattern with cyclomatic
complexity, peaking at moderate ranges. Our findings expose limitations of
safety mechanisms in open-source models, particularly for seemingly benign
educational requests.

</details>


### [32] [LLMShot: Reducing snapshot testing maintenance via LLMs](https://arxiv.org/abs/2507.10062)
*Ergün Batuhan Kaynak,Mayasah Lami,Sahand Moslemi,Anil Koyuncu*

Main category: cs.SE

TL;DR: LLMShot利用视觉大语言模型自动分析快照测试失败，通过分层分类减少手动检查负担，在真实iOS应用中验证效果显著。


<details>
  <summary>Details</summary>
Motivation: 快照测试因UI频繁变更导致大量手动检查负担，需自动化解决方案区分真实回归与设计变更。

Method: LLMShot框架结合视觉大语言模型，对UI变更进行分层分类，并在iOS应用中构建数据集验证。

Result: 12B模型召回率达84%，4B模型适合持续集成；但提示机制在可控视觉推理中存在局限。

Conclusion: LLMShot首次实现语义快照测试自动化分析，显著减少手动工作量，推动智能UI测试发展。

Abstract: Snapshot testing has emerged as a critical technique for UI validation in
modern software development, yet it suffers from substantial maintenance
overhead due to frequent UI changes causing test failures that require manual
inspection to distinguish between genuine regressions and intentional design
changes. This manual triage process becomes increasingly burdensome as
applications evolve, creating a need for automated analysis solutions. This
paper introduces LLMShot, a novel framework that leverages vision-based Large
Language Models to automatically analyze snapshot test failures through
hierarchical classification of UI changes. To evaluate LLMShot's effectiveness,
we developed a comprehensive dataset using a feature-rich iOS application with
configurable feature flags, creating realistic scenarios that produce authentic
snapshot differences representative of real development workflows. Our
evaluation using Gemma3 models demonstrates strong classification performance,
with the 12B variant achieving over 84% recall in identifying failure root
causes while the 4B model offers practical deployment advantages with
acceptable performance for continuous integration environments. However, our
exploration of selective ignore mechanisms revealed significant limitations in
current prompting-based approaches for controllable visual reasoning. LLMShot
represents the first automated approach to semantic snapshot test analysis,
offering developers structured insights that can substantially reduce manual
triage effort and advance toward more intelligent UI testing paradigms.

</details>


### [33] [Accelerating Automatic Program Repair with Dual Retrieval-Augmented Fine-Tuning and Patch Generation on Large Language Models](https://arxiv.org/abs/2507.10103)
*Hanyang Guo,Xiaoheng Xie,Hong-Ning Dai,Peng Di,Yu Zhang,Bishenghui Tao,Zibin Zheng*

Main category: cs.SE

TL;DR: SelRepair是一种结合微调LLM和双RAG模块的新型APR方法，显著提升了修复性能并减少了推理时间。


<details>
  <summary>Details</summary>
Motivation: 现有APR方法受限于缺陷类型、训练数据质量和模型参数规模，且当前LLM和RAG设计未充分考虑代码修复任务和代码特性。

Method: 通过微调LLM并结合语义和语法/结构相似性的双RAG模块，高效检索相关信息以减少token长度和推理时间。

Result: 在Java数据集上，SelRepair的精确匹配率分别达到26.29%和17.64%，推理时间减少至少6.42%。

Conclusion: SelRepair通过结合LLM和RAG的优势，显著提升了APR任务的性能和效率。

Abstract: Automated Program Repair (APR) is essential for ensuring software reliability
and quality while enhancing efficiency and reducing developers' workload.
Although rule-based and learning-based APR methods have demonstrated their
effectiveness, their performance was constrained by the defect type of repair,
the quality of training data, and the size of model parameters. Recently, Large
Language Models (LLMs) combined with Retrieval-Augmented-Generation (RAG) have
been increasingly adopted in APR tasks. However, current code LLMs and RAG
designs neither fully address code repair tasks nor consider code-specific
features. To overcome these limitations, we propose SelRepair, a novel APR
approach with integration of a fine-tuned LLM with a newly-designed dual RAG
module. This approach uses a bug-fix pair dataset for fine-tuning and
incorporates semantic and syntactic/structural similarity information through
an RAG selection gate. This design ensures relevant information is retrieved
efficiently, thereby reducing token length and inference time. Evaluations on
Java datasets show SelRepair outperforms other APR methods, achieving 26.29%
and 17.64% in terms of exact match (EM) on different datasets while reducing
inference time by at least 6.42% with controlled input lengths.

</details>


### [34] [Breaking the Myth: Can Small Models Infer Postconditions Too?](https://arxiv.org/abs/2507.10182)
*Gehao Zhang,Zhenting Wang,Juan Zhai*

Main category: cs.SE

TL;DR: 研究表明，小型微调语言模型在生成高质量后置条件时，可以显著降低计算成本，性能媲美或优于大型模型。


<details>
  <summary>Details</summary>
Motivation: 手动编写形式化规范既繁琐又容易出错，而大型语言模型（LLMs）虽然能生成规范，但计算成本高。本文探讨是否必须使用大型模型。

Method: 构建专用数据集，微调一个7B参数的代码模型，处理真实仓库依赖并保留前置状态信息。

Result: 在真实Java缺陷基准测试中，小型模型在语法正确性、语义正确性和区分缺陷能力上媲美或优于大型模型。

Conclusion: 通过针对性微调，小型模型可实现与大型模型相当的效果，为自动化规范生成提供高效实用方案。

Abstract: Formal specifications are essential for ensuring software correctness, yet
manually writing them is tedious and error-prone. Large Language Models (LLMs)
have shown promise in generating such specifications from natural language
intents, but the giant model size and high computational demands raise a
fundamental question: Do we really need large models for this task? In this
paper, we show that a small, fine-tuned language model can achieve high-quality
postcondition generation with much lower computational costs. We construct a
specialized dataset of prompts, reasoning logs, and postconditions, then
supervise the fine-tuning of a $7$B-parameter code model. Our approach tackles
real-world repository dependencies and preserves pre-state information,
allowing for expressive and accurate specifications. We evaluate the model on a
benchmark of real-world Java bugs (Defects4J) and compare against both
proprietary giants (e.g., GPT-4o) and open-source large models. Empirical
results demonstrate that our compact model matches or outperforms significantly
larger counterparts in syntax correctness, semantic correctness, and
bug-distinguishing capability. These findings highlight that targeted
fine-tuning on a modest dataset can enable small models to achieve results
formerly seen only in massive, resource-heavy LLMs, offering a practical and
efficient path for the real-world adoption of automated specification
generation.

</details>


### [35] [Towards a Framework for Operationalizing the Specification of Trustworthy AI Requirements](https://arxiv.org/abs/2507.10228)
*Hugo Villamizar,Daniel Mendez,Marcos Kalinowski*

Main category: cs.SE

TL;DR: 论文提出整合AMDiRE和PerSpecML两种方法，以解决AI系统可信性需求工程中的挑战。


<details>
  <summary>Details</summary>
Motivation: AI系统的可信性问题需要结构化方法来解决，尤其是针对非确定性和数据驱动的ML系统。

Method: 结合AMDiRE（基于工件的需求工程方法）和PerSpecML（多视角ML需求分析方法），以提升需求的一致性和可追溯性。

Result: 提出了一种整合方法，以操作化可信性需求，连接利益相关者关注点和结构化工件模型。

Conclusion: 论文总结了关键研究方向和开放挑战，呼吁与需求工程社区进一步讨论。

Abstract: Growing concerns around the trustworthiness of AI-enabled systems highlight
the role of requirements engineering (RE) in addressing emergent,
context-dependent properties that are difficult to specify without structured
approaches. In this short vision paper, we propose the integration of two
complementary approaches: AMDiRE, an artefact-based approach for RE, and
PerSpecML, a perspective-based method designed to support the elicitation,
analysis, and specification of machine learning (ML)-enabled systems. AMDiRE
provides a structured, artefact-centric, process-agnostic methodology and
templates that promote consistency and traceability in the results; however, it
is primarily oriented toward deterministic systems. PerSpecML, in turn,
introduces multi-perspective guidance to uncover concerns arising from the
data-driven and non-deterministic behavior of ML-enabled systems. We envision a
pathway to operationalize trustworthiness-related requirements, bridging
stakeholder-driven concerns and structured artefact models. We conclude by
outlining key research directions and open challenges to be discussed with the
RE community.

</details>


### [36] [An Empirical Study of Interaction Bugs in ROS-based Software](https://arxiv.org/abs/2507.10235)
*Zhixiang Chen,Zhuangbin Chen,Xingjie Cai,Wei Li,Zibin Zheng*

Main category: cs.SE

TL;DR: 论文研究了机器人系统中因组件交互引发的可靠性问题（iBugs），分析了121个ROS项目中的iBugs，并提出了改进方向。


<details>
  <summary>Details</summary>
Motivation: 机器人系统的可靠性不仅依赖单个组件的正确性，还依赖组件间的交互。交互相关的故障（iBugs）尚未充分研究。

Method: 对10个ROS项目中的121个iBugs进行实证研究，分类为三类（系统内、硬件、环境），分析原因、修复策略及影响。

Result: 研究发现iBugs的多样性和复杂性，提出了改进预防和检测的方向。

Conclusion: 研究为设计更鲁棒和安全的机器人系统提供了见解。

Abstract: Modern robotic systems integrate multiple independent software and hardware
components, each responsible for distinct functionalities such as perception,
decision-making, and execution. These components interact extensively to
accomplish complex end-to-end tasks. As a result, the overall system
reliability depends not only on the correctness of individual components, but
also on the correctness of their interactions. Failures often manifest at the
boundaries between components, yet interaction-related reliability issues in
robotics--referred to here as interaction bugs (iBugs)--remain underexplored.
  This work presents an empirical study of iBugs within robotic systems built
using the Robot Operating System (ROS), a widely adopted open-source robotics
framework. A total of 121 iBugs were analyzed across ten actively maintained
and representative ROS projects. The identified iBugs are categorized into
three major types: intra-system iBugs, hardware iBugs, and environmental iBugs,
covering a broad range of interaction scenarios in robotics. The analysis
includes an examination of root causes, fixing strategies, and the impact of
these bugs. Several findingsa are derived that shed light on the nature of
iBugs and suggest directions for improving their prevention and detection.
These insights aim to inform the design of more robust and safer robotic
systems.

</details>


### [37] [Helveg: Diagrams for Software Documentation](https://arxiv.org/abs/2507.10244)
*Adam Štěpánek,David Kuťák,Barbora Kozlíková,Jan Byška*

Main category: cs.SE

TL;DR: 论文提出了一种改进的代码库可视化工具Helveg，通过交互式节点链接图和灵活的过滤功能帮助开发者理解代码库，解决了传统文档的局限性。


<details>
  <summary>Details</summary>
Motivation: 传统文档形式不适合代码库的高层次探索性分析，因其固定路径限制了开发者。

Method: 设计了一种交互式节点链接图工具Helveg，支持C#代码库的自动生成图表，并改进了节点符号设计和用户界面。

Result: 用户测试证实了工具的潜力，但也发现了可读性、直观性和用户体验的问题。改进后的版本再次评估。

Conclusion: 通过改进符号设计、交互方式和用户界面，Helveg在代码库理解方面表现更优。

Abstract: Software developers often have to gain an understanding of a codebase. Be it
programmers getting onboarded onto a team project or, for example, developers
striving to grasp an external open-source library. In either case, they
frequently turn to the project's documentation. However, documentation in its
traditional textual form is ill-suited for this kind of high-level exploratory
analysis, since it is immutable from the readers' perspective and thus forces
them to follow a predefined path. We have designed an approach bringing aspects
of software architecture visualization to API reference documentation. It
utilizes a highly interactive node-link diagram with expressive node glyphs and
flexible filtering capabilities, providing a high-level overview of the
codebase as well as details on demand. To test our design, we have implemented
a prototype named Helveg, capable of automatically generating diagrams of C\#
codebases. User testing of Helveg confirmed its potential, but it also revealed
problems with the readability, intuitiveness, and user experience of our tool.
Therefore, in this paper, which is an extended version of our VISSOFT paper
with DOI 10.1109/VISSOFT64034.2024.00012, we address many of these problems
through major changes to the glyph design, means of interaction, and user
interface of the tool. To assess the improvements, this new version of Helveg
was evaluated again with the same group of participants as the previous
version.

</details>


### [38] [A Grounded Theory on the Teacher and Student Roles in Pair Programming](https://arxiv.org/abs/2507.10305)
*Linus Ververs,Trang Linh Lam,Janina Berger,Lutz Prechelt*

Main category: cs.SE

TL;DR: 知识转移在结对编程中可能有害，需注意角色分配和权力差距以避免负面影响。


<details>
  <summary>Details</summary>
Motivation: 研究在结对编程中知识转移可能带来的负面影响，以提升实践效果。

Method: 基于17次结对编程记录和6次访谈，采用扎根理论方法分析。

Result: 定义了学生和教师角色，提出了权力差距理论，并总结了需避免的陷阱。

Conclusion: 忽视权力差距和伙伴需求会导致防御行为，负面影响知识转移、团队协作和代码质量。

Abstract: Context: Pair programming is an established (agile) practice and is practiced
throughout the industry. Objective: Understand under what circumstances
knowledge transfer can harm a pair programming session. Method: Grounded Theory
Methodology based on 17 recorded pair programming sessions with 18 developers
from 5 German software companies accompanied, by 6 interviews with different
developers from 4 other German companies. Results: We define the student and
teacher roles to help developers deal with a one-sided knowledge gap. We
describe pitfalls to avoid and develop a grounded theory centered around the
Power Gap in pair programming. Conclusions: Knowledge transfer can be harmful
when developers don't pay attention to their partners needs and desires. If
developers don't pay attention to the Power Gap and keep it in check, Defensive
Behavior may arise that leads to a vicious cycle impacting the knowledge
transfer, the Togetherness and the code quality in a negative way.

</details>


### [39] [Streamlined Airborne Software Development for Large UAVs: From Unified Data Collection to Automated Code Generation](https://arxiv.org/abs/2507.10321)
*Viktor Sinitsyn,Nils Schlautmann,Florian Schwaiger,Florian Holzapfel*

Main category: cs.SE

TL;DR: 本文提出了一种新颖的工具链和流程，用于优化航空航天设备中数字接口和机载软件的开发，强调自动化与灵活性。


<details>
  <summary>Details</summary>
Motivation: 航空航天行业的技术进步带来新挑战，尤其是数字接口的高效处理需求，以确保系统集成和功能顺畅。

Method: 提出了一种自动化工具链和流程，注重灵活性和设计保证要求的合规性。

Result: 该工具链已在多个项目中成功应用，验证了其有效性。

Conclusion: 该方法为航空航天行业提供了高效的解决方案，减少了人工干预并提升了开发效率。

Abstract: The aerospace industry has experienced significant transformations over the
last decade, driven by technological advancements and innovative solutions in
goods and personal transportation. This evolution has spurred the emergence of
numerous start-ups that now face challenges traditionally encountered by
established aerospace companies. Among these challenges is the efficient
processing of digital intra-device communication interfaces for onboard
equipment - a critical component for ensuring seamless system integration and
functionality. Addressing this challenge requires solutions that emphasize
clear and consistent interface descriptions, automation of processes, and
reduced labor-intensive efforts.
  This paper presents a novel process and toolchain designed to streamline the
development of digital interfaces and onboard software, which our team has
successfully applied in several completed projects. The proposed approach
focuses on automation and flexibility while maintaining compliance with design
assurance requirements.

</details>


### [40] [AssertCoder: LLM-Based Assertion Generation via Multimodal Specification Extraction](https://arxiv.org/abs/2507.10338)
*Enyuan Tian,Yiwei Ci,Qiusong Yang,Yufeng Li,Zhichao Lyu*

Main category: cs.SE

TL;DR: AssertCoder是一个自动化生成高质量SVAs的框架，通过多模态硬件设计规范生成断言，显著提升功能正确性和突变检测能力。


<details>
  <summary>Details</summary>
Motivation: 手动编写高质量SVAs耗时且易错，AssertCoder旨在解决这一问题。

Method: 采用多模态敏感预处理和语义分析器提取结构化表示，通过多步CoT提示驱动断言合成，并结合基于突变的评估方法。

Result: 在三个真实RTL设计中，AssertCoder平均提升功能正确性8.4%，突变检测5.8%。

Conclusion: AssertCoder在自动化生成高质量SVAs方面表现优异，优于现有方法。

Abstract: Assertion-Based Verification (ABV) is critical for ensuring functional
correctness in modern hardware systems. However, manually writing high-quality
SVAs remains labor-intensive and error-prone. To bridge this gap, we propose
AssertCoder, a novel unified framework that automatically generates
high-quality SVAs directly from multimodal hardware design specifications.
AssertCoder employs a modality-sensitive preprocessing to parse heterogeneous
specification formats (text, tables, diagrams, and formulas), followed by a set
of dedicated semantic analyzers that extract structured representations aligned
with signal-level semantics. These representations are utilized to drive
assertion synthesis via multi-step chain-of-thought (CoT) prompting. The
framework incorporates a mutation-based evaluation approach to assess assertion
quality via model checking and further refine the generated assertions.
Experimental evaluation across three real-world Register-Transfer Level (RTL)
designs demonstrates AssertCoder's superior performance, achieving an average
increase of 8.4% in functional correctness and 5.8% in mutation detection
compared to existing state-of-the-art approaches.

</details>


### [41] [Self-Admitted GenAI Usage in Open-Source Software](https://arxiv.org/abs/2507.10422)
*Tao Xiao,Youmei Fan,Fabio Calefato,Christoph Treude,Raula Gaikovina Kula,Hideaki Hata,Sebastian Baltes*

Main category: cs.SE

TL;DR: 研究通过分析GitHub仓库中的自我承认GenAI使用情况，揭示了开发者如何管理GenAI工具的使用，并探讨了其对开源软件开发的影响。


<details>
  <summary>Details</summary>
Motivation: 由于生成的代码难以与手动编写的代码区分，GenAI工具在开源软件开发中的实际使用和影响尚不明确。

Method: 通过分析25万个GitHub仓库中的1,292个自我承认GenAI使用案例，采用混合方法（定性编码和开发者调查）研究GenAI的使用任务、内容类型和目的。

Result: 研究发现开发者积极管理GenAI使用，并提出了透明性、归属和质量控制的需求。同时，GenAI的采用并未显著增加代码变更频率。

Conclusion: 研究强调了在AI辅助软件开发时代，项目级别的透明性和质量控制的重要性，并挑战了GenAI对软件开发影响的流行观点。

Abstract: The widespread adoption of generative AI (GenAI) tools such as GitHub Copilot
and ChatGPT is transforming software development. Since generated source code
is virtually impossible to distinguish from manually written code, their
real-world usage and impact on open-source software development remain poorly
understood. In this paper, we introduce the concept of self-admitted GenAI
usage, that is, developers explicitly referring to the use of GenAI tools for
content creation in software artifacts. Using this concept as a lens to study
how GenAI tools are integrated into open-source software projects, we analyze a
curated sample of more than 250,000 GitHub repositories, identifying 1,292 such
self-admissions across 156 repositories in commit messages, code comments, and
project documentation. Using a mixed methods approach, we derive a taxonomy of
32 tasks, 10 content types, and 11 purposes associated with GenAI usage based
on 284 qualitatively coded mentions. We then analyze 13 documents with policies
and usage guidelines for GenAI tools and conduct a developer survey to uncover
the ethical, legal, and practical concerns behind them. Our findings reveal
that developers actively manage how GenAI is used in their projects,
highlighting the need for project-level transparency, attribution, and quality
control practices in the new era of AI-assisted software development. Finally,
we examine the longitudinal impact of GenAI adoption on code churn in 151
repositories with self-admitted GenAI usage and find no general increase,
contradicting popular narratives on the impact of GenAI on software
development.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [42] [Immutability Does Not Guarantee Trust: A Formal and Logical Refutation](https://arxiv.org/abs/2507.08844)
*Craig S Wright*

Main category: cs.CR

TL;DR: 论文反驳了区块链中‘不可变性保证信任’的常见观点，通过形式化方法证明不可变性与信任无关。


<details>
  <summary>Details</summary>
Motivation: 澄清区块链中不可变性与信任之间的误解，揭示其逻辑漏洞。

Method: 使用谓词逻辑、自动机理论和认知博弈论分析，结合形式化构造和反例。

Result: 不可变性不保证正确性、公平性或可信度，且可能保留虚假数据。

Conclusion: 不可变性无法保证信任，需区分结构与认知领域。

Abstract: It is frequently claimed in blockchain discourse that immutability guarantees
trust. This paper rigorously refutes that assertion. We define immutability as
the cryptographic persistence of historical states in an append-only data
structure and contrast it with trust, understood as a rational epistemic
expectation under uncertainty. Employing predicate logic, automata-theoretic
models, and epistemic game-theoretic analysis, we demonstrate that immutability
neither entails nor implies correctness, fairness, or credibility. Through
formal constructions and counterexamples--including predictive fraud schemes
and the phenomenon of garbage permanence--we show that the belief conflates
structural and epistemic domains. Immutability preserves all data equally,
regardless of veracity. Therefore, the assertion that immutability guarantees
trust collapses under the weight of formal scrutiny.

</details>


### [43] [Clio-X: AWeb3 Solution for Privacy-Preserving AI Access to Digital Archives](https://arxiv.org/abs/2507.08853)
*Victoria L. Lemieux,Rosa Gil,Faith Molosiwa,Qihong Zhou,Binming Li,Roberto Garcia,Luis De La Torre Cubillo,Zehua Wang*

Main category: cs.CR

TL;DR: 论文探讨了隐私增强技术（PETs）和Web3架构如何帮助档案馆在管理敏感内容时保持数据主权，同时支持AI驱动的访问。提出了Clio-X解决方案，并分析了其采用障碍及改进路径。


<details>
  <summary>Details</summary>
Motivation: 随着档案馆利用AI管理数字记录，隐私风险和数据主权问题凸显，需要探索如何在保护隐私的同时支持AI应用。

Method: 提出Clio-X，一个基于Web3的去中心化隐私优先解决方案，并通过用户评估原型分析其潜力与障碍。

Result: 研究发现Clio-X具有潜力，但存在信任、系统透明度、经济和治理等障碍。

Conclusion: 结合技术保障和社区监督，Clio-X为文化遗产领域提供了一种伦理AI部署的新模型。

Abstract: As archives turn to artificial intelligence to manage growing volumes of
digital records, privacy risks inherent in current AI data practices raise
critical concerns about data sovereignty and ethical accountability. This paper
explores how privacy-enhancing technologies (PETs) and Web3 architectures can
support archives to preserve control over sensitive content while still being
able to make it available for access by researchers. We present Clio-X, a
decentralized, privacy-first Web3 digital solution designed to embed PETs into
archival workflows and support AI-enabled reference and access. Drawing on a
user evaluation of a medium-fidelity prototype, the study reveals both interest
in the potential of the solution and significant barriers to adoption related
to trust, system opacity, economic concerns, and governance. Using Rogers'
Diffusion of Innovation theory, we analyze the sociotechnical dimensions of
these barriers and propose a path forward centered on participatory design and
decentralized governance through a Clio-X Decentralized Autonomous
Organization. By integrating technical safeguards with community-based
oversight, Clio-X offers a novel model to ethically deploy AI in cultural
heritage contexts.

</details>


### [44] [RAG Safety: Exploring Knowledge Poisoning Attacks to Retrieval-Augmented Generation](https://arxiv.org/abs/2507.08862)
*Tianzhe Zhao,Jiaoyan Chen,Yanchi Ru,Haiping Zhu,Nan Hu,Jun Liu,Qika Lin*

Main category: cs.CR

TL;DR: 本文首次系统研究了基于知识图谱的检索增强生成（KG-RAG）方法的安全问题，提出了一种隐蔽的数据投毒攻击策略，并通过实验验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 尽管KG-RAG方法在增强语言模型性能方面表现出色，但其安全风险尚未被充分研究，尤其是知识图谱的结构化和可编辑特性可能带来独特漏洞。

Method: 提出了一种隐蔽的攻击策略，通过识别对抗性目标答案并插入扰动三元组，构造误导性推理链，从而影响KG-RAG的生成结果。

Result: 实验表明，即使对知识图谱进行最小扰动，攻击策略也能显著降低KG-RAG的性能。

Conclusion: KG-RAG系统存在严重的安全威胁，未来研究需关注其防御机制和LLM对抗性知识的鲁棒性。

Abstract: Retrieval-Augmented Generation (RAG) enhances large language models (LLMs) by
retrieving external data to mitigate hallucinations and outdated knowledge
issues. Benefiting from the strong ability in facilitating diverse data sources
and supporting faithful reasoning, knowledge graphs (KGs) have been
increasingly adopted in RAG systems, giving rise to KG-based RAG (KG-RAG)
methods. Though RAG systems are widely applied in various applications, recent
studies have also revealed its vulnerabilities to data poisoning attacks, where
malicious information injected into external knowledge sources can mislead the
system into producing incorrect or harmful responses. However, these studies
focus exclusively on RAG systems using unstructured textual data sources,
leaving the security risks of KG-RAG largely unexplored, despite the fact that
KGs present unique vulnerabilities due to their structured and editable nature.
In this work, we conduct the first systematic investigation of the security
issue of KG-RAG methods through data poisoning attacks. To this end, we
introduce a practical, stealthy attack setting that aligns with real-world
implementation. We propose an attack strategy that first identifies adversarial
target answers and then inserts perturbation triples to complete misleading
inference chains in the KG, increasing the likelihood that KG-RAG methods
retrieve and rely on these perturbations during generation. Through extensive
experiments on two benchmarks and four recent KG-RAG methods, our attack
strategy demonstrates strong effectiveness in degrading KG-RAG performance,
even with minimal KG perturbations. In-depth analyses are also conducted to
understand the safety threats within the internal stages of KG-RAG systems and
to explore the robustness of LLMs against adversarial knowledge.

</details>


### [45] [Privacy-Utility-Fairness: A Balanced Approach to Vehicular-Traffic Management System](https://arxiv.org/abs/2507.08864)
*Poushali Sengupta,Sabita Maharjan,frank Eliassen,Yan Zhang*

Main category: cs.CR

TL;DR: 提出了一种新算法，用于在基于位置的车辆交通管理系统中平衡隐私、实用性和公平性。


<details>
  <summary>Details</summary>
Motivation: 现有解决方案在保护敏感地理数据时难以同时满足隐私、实用性和公平性要求，导致隐私泄露和数据分析不公平。

Method: 采用差分隐私技术，结合查询式数据访问、迭代洗牌和校准噪声注入，确保数据安全。

Result: 在挪威的车辆位置数据上验证了算法，能够保持交通管理和城市规划的数据实用性，同时确保地理区域的公平表示。

Conclusion: 该算法成功解决了隐私、实用性和公平性之间的平衡问题，为车辆交通管理提供了有效的解决方案。

Abstract: Location-based vehicular traffic management faces significant challenges in
protecting sensitive geographical data while maintaining utility for traffic
management and fairness across regions. Existing state-of-the-art solutions
often fail to meet the required level of protection against linkage attacks and
demographic biases, leading to privacy leakage and inequity in data analysis.
In this paper, we propose a novel algorithm designed to address the challenges
regarding the balance of privacy, utility, and fairness in location-based
vehicular traffic management systems. In this context, utility means providing
reliable and meaningful traffic information, while fairness ensures that all
regions and individuals are treated equitably in data use and decision-making.
Employing differential privacy techniques, we enhance data security by
integrating query-based data access with iterative shuffling and calibrated
noise injection, ensuring that sensitive geographical data remains protected.
We ensure adherence to epsilon-differential privacy standards by implementing
the Laplace mechanism. We implemented our algorithm on vehicular location-based
data from Norway, demonstrating its ability to maintain data utility for
traffic management and urban planning while ensuring fair representation of all
geographical areas without being overrepresented or underrepresented.
Additionally, we have created a heatmap of Norway based on our model,
illustrating the privatized and fair representation of the traffic conditions
across various cities. Our algorithm provides privacy in vehicular traffic

</details>


### [46] [Towards Privacy-Preserving and Personalized Smart Homes via Tailored Small Language Models](https://arxiv.org/abs/2507.08878)
*Xinyu Huang,Leming Shen,Zijing Ma,Yuanqing Zheng*

Main category: cs.CR

TL;DR: HomeLLaMA是一个本地化的小型语言模型（SLM），用于隐私保护和个性化的智能家居服务，通过从云端LLM学习并提供可选隐私保护服务PrivShield。


<details>
  <summary>Details</summary>
Motivation: 解决现有基于LLM的智能家居助手将用户数据和配置传输到远程服务器导致的隐私泄露问题。

Method: 开发本地化SLM HomeLLaMA，从云端LLM学习并提供个性化服务；同时开发PrivShield作为可选隐私保护服务。

Result: 实验和用户研究表明，HomeLLaMA在提供个性化服务的同时显著提升用户隐私。

Conclusion: HomeLLaMA通过本地化SLM和可选隐私保护服务，有效平衡了智能家居服务的个性化和隐私保护需求。

Abstract: Large Language Models (LLMs) have showcased remarkable generalizability in
language comprehension and hold significant potential to revolutionize
human-computer interaction in smart homes. Existing LLM-based smart home
assistants typically transmit user commands, along with user profiles and home
configurations, to remote servers to obtain personalized services. However,
users are increasingly concerned about the potential privacy leaks to the
remote servers. To address this issue, we develop HomeLLaMA, an on-device
assistant for privacy-preserving and personalized smart home serving with a
tailored small language model (SLM). HomeLLaMA learns from cloud LLMs to
deliver satisfactory responses and enable user-friendly interactions. Once
deployed, HomeLLaMA facilitates proactive interactions by continuously updating
local SLMs and user profiles. To further enhance user experience while
protecting their privacy, we develop PrivShield to offer an optional
privacy-preserving LLM-based smart home serving for those users, who are
unsatisfied with local responses and willing to send less-sensitive queries to
remote servers. For evaluation, we build a comprehensive benchmark DevFinder to
assess the service quality. Extensive experiments and user studies (M=100)
demonstrate that HomeLLaMA can provide personalized services while
significantly enhancing user privacy.

</details>


### [47] [CovertAuth: Joint Covert Communication and Authentication in MmWave Systems](https://arxiv.org/abs/2507.08904)
*Yulin Teng,Keshuang Han,Pinchang Zhang,Xiaohong Jiang,Yulong Shen,Fu Xiao*

Main category: cs.CR

TL;DR: 论文提出了一种名为CovertAuth的安全框架，用于增强毫米波通信中波束对准阶段的安全性，对抗窃听和身份冒充攻击。


<details>
  <summary>Details</summary>
Motivation: 由于波束对准阶段的广播特性和全向暴露，容易受到窃听和身份冒充攻击，因此需要一种安全框架来提升安全性。

Method: 通过推导成功波束对准概率和隐蔽传输速率的闭式表达式，并联合优化波束训练预算和传输功率以最大化隐蔽通信速率；同时利用天线阵列缺陷的互耦效应设计物理层认证方案。

Result: 仿真结果表明，在相同隐蔽性要求下，CovertAuth相比现有工作具有更高的检测准确性。

Conclusion: CovertAuth框架有效提升了波束对准阶段的安全性，为毫米波通信提供了更可靠的保护。

Abstract: Beam alignment (BA) is a crucial process in millimeter-wave (mmWave)
communications, enabling precise directional transmission and efficient link
establishment. However, due to characteristics like omnidirectional exposure
and the broadcast nature of the BA phase, it is particularly vulnerable to
eavesdropping and identity impersonation attacks. To this end, this paper
proposes a novel secure framework named CovertAuth, designed to enhance the
security of the BA phase against such attacks. In particular, to combat
eavesdropping attacks, the closed-form expressions of successful BA probability
and covert transmission rate are first derived. Then, a covert communication
problem aimed at jointly optimizing beam training budget and transmission power
is formulated to maximize covert communication rate, subject to the covertness
requirement. An alternating optimization algorithm combined with successive
convex approximation is employed to iteratively achieve optimal results. To
combat impersonation attacks, the mutual coupling effect of antenna array
impairments is explored as a device feature to design a weighted-sum energy
detector based physical layer authentication scheme. Moreover, theoretical
models for authentication metrics like detection and false alarm probabilities
are also provided to conduct performance analysis. Based on these models, an
optimization problem is constructed to determine the optimal weight value that
maximizes authentication accuracy. Finally, simulation results demonstrate that
CovertAuth presents improved detection accuracy under the same covertness
requirement compared to existing works.

</details>


### [48] [Characterizing Security and Privacy Teaching Standards for Schools in the United States](https://arxiv.org/abs/2507.08978)
*Katherine Limes,Nathan Malkin,Kelsey R. Fulton*

Main category: cs.CR

TL;DR: 研究分析了美国K-12教育中安全与隐私教学标准的内容，发现其与专业人士的期望存在部分重叠，但专业人士更强调威胁建模和安全思维。


<details>
  <summary>Details</summary>
Motivation: 探讨K-12教育中的安全与隐私教学标准是否与行业专业人士的期望一致。

Method: 收集并手动分析美国各州及国家组织的11,954条教学标准，标记3,778条与安全隐私相关，并分类为103个主题；随后采访11位专业人士。

Result: 教学标准涵盖技术和社会主题，但与专业人士相比，更缺乏对威胁建模和安全思维的重视。

Conclusion: 教学标准与专业人士的期望部分一致，但需加强威胁建模和安全思维的培养。

Abstract: Increasingly, students begin learning aspects of security and privacy during
their primary and secondary education (grades K-12 in the United States).
Individual U.S. states and some national organizations publish teaching
standards -- guidance that outlines expectations for what students should learn
-- which often form the basis for course curricula. However, research has not
yet examined what is covered by these standards and whether the topics align
with what the broader security and privacy community thinks students should
know. To shed light on these questions, we started by collecting computer
science teaching standards from all U.S. states and eight national
organizations. After manually examining a total of 11,954 standards, we labeled
3,778 of them as being related to security and privacy, further classifying
these into 103 topics. Topics ranged from technical subjects like encryption,
network security, and embedded systems to social subjects such as laws, ethics,
and appropriate online behavior. Subsequently, we interviewed 11 security and
privacy professionals to examine how the teaching standards align with their
expectations. We found that, while the specific topics they mentioned mostly
overlapped with those of existing standards, professionals placed a greater
emphasis on threat modeling and security mindset.

</details>


### [49] [SSH-Passkeys: Leveraging Web Authentication for Passwordless SSH](https://arxiv.org/abs/2507.09022)
*Moe Kayali,Jonas Schmitt,Franziska Roesner*

Main category: cs.CR

TL;DR: 提出了一种利用Web Authentication API实现SSH无密码登录的方法，通过passkeys管理密钥生命周期，提升安全性和用户体验。


<details>
  <summary>Details</summary>
Motivation: SSH仍主要依赖密码认证，存在钓鱼和密钥泄露等问题，传统SSH密钥管理复杂且易出错，WebAuthn虽为现代标准但无法直接与SSH集成。

Method: 通过UNIX PAM模块将WebAuthn与SSH服务器集成，保持向后兼容性，无需客户端新软件。

Result: 原型测试显示，SSH-passkeys减少90%关键安全错误，认证时间平均缩短4倍。

Conclusion: SSH-passkeys显著提升SSH安全性，解决传统密钥管理的缺陷。

Abstract: We propose a method for using Web Authentication APIs for SSH authentication,
enabling passwordless remote server login with passkeys. These are credentials
that are managed throughout the key lifecycle by an authenticator on behalf of
the user and offer strong security guarantees.
  Passwords remain the dominant mode of SSH authentication, despite their well
known flaws such as phishing and reuse. SSH's custom key-based authentication
protocol can alleviate these issues but remains vulnerable to key theft.
Additionally, it has poor usability, with even knowledgeable users leaking key
material and failing to verify fingerprints. Hence, effective key management
remains a critical open area in SSH security. In contrast, WebAuthn is a modern
authentication standard designed to replace passwords, managing keys on behalf
of the user. As a web API, this standard cannot integrate with SSH directly.
  We propose a framework to integrate WebAuthn with SSH servers, by using UNIX
pluggable authentication modules (PAM). Our approach is backwards-compatible,
supports stock SSH servers and requires no new software client-side. It offers
protection for cryptographic material at rest, resistance to key leaks,
phishing protection, privacy protection and attestation capability. None of
these properties are offered by passwords nor traditional SSH keys. We validate
these advantages with a structured, conceptual security analysis.
  We develop a prototype implementation and conduct a user study to quantify
the security advantages of our proposal, testing our prototype with 40 SSH
users. The study confirms the security problems of SSH-keys, including 20% of
the cohort leaking their private keys. Our SSH-passkeys effectively address
these problems: we find a 90% reduction in critical security errors, while
reducing authentication time by 4x on average.

</details>


### [50] [Favicon Trojans: Executable Steganography Via Ico Alpha Channel Exploitation](https://arxiv.org/abs/2507.09074)
*David Noever,Forrest McKee*

Main category: cs.CR

TL;DR: 本文提出了一种利用ICO图像文件的alpha透明层嵌入自解压JavaScript载荷的可执行隐写方法，通过最小有效位（LSB）隐藏代码，不影响视觉效果，并在浏览器中成功执行。


<details>
  <summary>Details</summary>
Motivation: 全球每天加载2940亿个favicon，消耗0.9PB带宽，利用这一常见行为实现隐蔽攻击。

Method: 在非透明alpha层的最小有效位嵌入压缩JavaScript代码，通过浏览器加载favicon时提取并执行载荷。

Result: 64x64 ICO图像可嵌入512字节（未压缩）或0.8KB（压缩）代码，成功在多种浏览器中无声执行。

Conclusion: 该方法模糊了静态图像与可执行内容的界限，揭示了现有防御措施的局限性。

Abstract: This paper presents a novel method of executable steganography using the
alpha transparency layer of ICO image files to embed and deliver
self-decompressing JavaScript payloads within web browsers. By targeting the
least significant bit (LSB) of non-transparent alpha layer image values, the
proposed method successfully conceals compressed JavaScript code inside a
favicon image without affecting visual fidelity. Global web traffic loads 294
billion favicons daily and consume 0.9 petabytes of network bandwidth. A
proof-of-concept implementation demonstrates that a 64x64 ICO image can embed
up to 512 bytes uncompressed, or 0.8 kilobyte when using lightweight two-fold
compression. On page load, a browser fetches the favicon as part of standard
behavior, allowing an embedded loader script to extract and execute the payload
entirely in memory using native JavaScript APIs and canvas pixel access. This
creates a two-stage covert channel requiring no additional network or user
requests. Testing across multiple browsers in both desktop and mobile
environments confirms successful and silent execution of the embedded script.
We evaluate the threat model, relate it to polymorphic phishing attacks that
evade favicon-based detection, and analyze evasion of content security policies
and antivirus scanners. We map nine example MITRE ATT&CK Framework objectives
to single line JavaScript to execute arbitrarily in ICO files. Existing
steganalysis and sanitization defenses are discussed, highlighting limitations
in detecting or neutralizing alpha-channel exploits. The results demonstrate a
stealthy and reusable attack surface that blurs traditional boundaries between
static images and executable content. Because modern browsers report silent
errors when developers specifically fail to load ICO files, this attack surface
offers an interesting example of required web behaviors that in turn compromise
security.

</details>


### [51] [CLIProv: A Contrastive Log-to-Intelligence Multimodal Approach for Threat Detection and Provenance Analysis](https://arxiv.org/abs/2507.09133)
*Jingwen Li,Ru Zhang,Jianyi Liu,Wanguo Zhao*

Main category: cs.CR

TL;DR: CLIProv 是一种利用对比学习对齐溯源日志与威胁情报语义的新方法，用于检测主机系统中的威胁行为，显著提高了检测效率和精度。


<details>
  <summary>Details</summary>
Motivation: 随着网络攻击复杂性的增加，将高级攻击模式转化为可操作的安全策略存在语义鸿沟的挑战。

Method: CLIProv 采用多模态框架，通过对比学习对齐语义，并将威胁检测建模为语义搜索问题。

Result: 实验表明，CLIProv 能有效识别攻击行为，精度和效率优于现有方法。

Conclusion: CLIProv 为威胁检测提供了高效且精确的解决方案，填补了语义鸿沟。

Abstract: With the increasing complexity of cyberattacks, the proactive and
forward-looking nature of threat intelligence has become more crucial for
threat detection and provenance analysis. However, translating high-level
attack patterns described in Tactics, Techniques, and Procedures (TTP)
intelligence into actionable security policies remains a significant challenge.
This challenge arises from the semantic gap between high-level threat
intelligence and low-level provenance log. To address this issue, this paper
introduces CLIProv, a novel approach for detecting threat behaviors in a host
system. CLIProv employs a multimodal framework that leverages contrastive
learning to align the semantics of provenance logs with threat intelligence,
effectively correlating system intrusion activities with attack patterns.
Furthermore, CLIProv formulates threat detection as a semantic search problem,
identifying attack behaviors by searching for threat intelligence that is most
semantically similar to the log sequence. By leveraging attack pattern
information in threat intelligence, CLIProv identifies TTPs and generates
complete and concise attack scenarios. Experimental evaluations on standard
datasets show that CLIProv effectively identifies attack behaviors in system
provenance logs, offering valuable references for potential techniques.
Compared to state-of-the-art methods, CLIProv achieves higher precision and
significantly improved detection efficiency.

</details>


### [52] [Confidential Wrapped Ethereum](https://arxiv.org/abs/2507.09231)
*Artem Chystiakov,Mariia Zhvanko*

Main category: cs.CR

TL;DR: 论文提出了一种在应用层实现隐私保护的以太坊封装代币（cWETH）方案，结合了椭圆曲线加密和零知识证明技术。


<details>
  <summary>Details</summary>
Motivation: 公共区块链的透明性可能损害用户隐私，需在开放性和隐私性之间取得平衡。

Method: 使用椭圆曲线Twisted ElGamal承诺方案和Diffie-Hellman协议，结合zk-SNARKs确保正确性。

Result: 实现了隐私保护的cWETH，同时保留了区块链的开放性。

Conclusion: 该方案成功平衡了区块链透明性与用户隐私需求。

Abstract: Transparency is one of the key benefits of public blockchains. However, the
public visibility of transactions potentially compromises users' privacy. The
fundamental challenge is to balance the intrinsic benefits of blockchain
openness with the vital need for individual confidentiality. The proposal
suggests creating a confidential version of wrapped Ethereum (cWETH) fully
within the application layer. The solution combines the Elliptic Curve (EC)
Twisted ElGamal-based commitment scheme to preserve confidentiality and the EC
Diffie-Hellman (DH) protocol to introduce accessibility limited by the
commitment scheme. To enforce the correct generation of commitments,
encryption, and decryption, zk-SNARKs are utilized.

</details>


### [53] [Hybrid Quantum Security for IPsec](https://arxiv.org/abs/2507.09288)
*Javier Blanco-Romero,Pedro Otero García,Daniel Sobral-Blanco,Florina Almenares Mendoza,Ana Fernández Vilas,Manuel Fernández-Veiga*

Main category: cs.CR

TL;DR: 本文系统比较了顺序与并行混合QKD-PQC密钥建立策略，提出了两种将QKD集成到IKEv2的新方法，并证明并行方法在性能上优于顺序方法。


<details>
  <summary>Details</summary>
Motivation: 解决QKD与现有安全协议集成中的不匹配问题，提升量子密钥分发的实际应用性能。

Method: 提出纯QKD方法和统一QKD-KEM抽象方法，支持并行组合量子与后量子密码学方法。

Result: 并行混合方法显著减少延迟，纯QKD方法通过标识符协调实现低带宽开销。

Conclusion: 并行混合QKD-PQC方法为关键基础设施提供了实用的量子增强IPsec解决方案。

Abstract: Quantum Key Distribution (QKD) offers information-theoretic security against
quantum computing threats, but integrating QKD into existing security protocols
remains an unsolved challenge due to fundamental mismatches between
pre-distributed quantum keys and computational key exchange paradigms. This
paper presents the first systematic comparison of sequential versus parallel
hybrid QKD-PQC key establishment strategies for IPsec, revealing fundamental
protocol design principles that extend beyond specific implementations. We
introduce two novel approaches for incorporating QKD into Internet Key Exchange
version 2 (IKEv2) with support for both ETSI GS QKD 004 stateful and ETSI GS
QKD 014 stateless API specifications: (1) a pure QKD approach that replaces
computational key derivation with identifier-based quantum key coordination,
and (2) a unified QKD-KEM abstraction that enables parallel composition of
quantum and post-quantum cryptographic methods within existing protocol
frameworks. Our key insight is that parallel hybrid approaches eliminate the
multiplicative latency penalties inherent in sequential methods mandated by RFC
9370, achieving significant performance improvements under realistic network
conditions. Performance evaluation using a Docker-based testing framework with
IDQuantique QKD hardware demonstrates that the parallel hybrid approach
significantly outperforms sequential methods under network latency conditions,
while pure QKD achieves minimal bandwidth overhead through identifier-based key
coordination. Our implementations provide practical quantum-enhanced IPsec
solutions suitable for critical infrastructure deployments requiring
defense-in-depth security.

</details>


### [54] [Implementing and Evaluating Post-Quantum DNSSEC in CoreDNS](https://arxiv.org/abs/2507.09301)
*Julio Gento Suela,Javier Blanco-Romero,Florina Almenares Mendoza,Daniel Díaz-Sánchez*

Main category: cs.CR

TL;DR: 论文提出了一种将后量子密码（PQC）算法集成到CoreDNS中的方法，以支持量子安全的DNSSEC功能，并评估了五种PQC签名算法的性能与安全性权衡。


<details>
  <summary>Details</summary>
Motivation: 量子计算机的出现威胁到依赖RSA和ECDSA等算法的现有安全服务（如DNSSEC），因为这些算法易受量子攻击。

Method: 开发了一个插件，扩展CoreDNS以支持五种PQC签名算法家族（ML-DSA、FALCON、SPHINCS+、MAYO和SNOVA），并保持与现有DNS解析流程的兼容性。

Result: 性能评估显示PQC算法在安全性和效率之间存在显著权衡，部分算法可作为过渡到量子安全密码学的可行选择。

Conclusion: 尽管PQC算法带来操作开销，但某些候选算法为DNSSEC向量子安全密码学的过渡提供了可行的解决方案。

Abstract: The emergence of quantum computers poses a significant threat to current
secure service, application and/or protocol implementations that rely on RSA
and ECDSA algorithms, for instance DNSSEC, because public-key cryptography
based on number factorization or discrete logarithm is vulnerable to quantum
attacks. This paper presents the integration of post-quantum cryptographic
(PQC) algorithms into CoreDNS to enable quantum-resistant DNSSEC functionality.
We have developed a plugin that extends CoreDNS with support for five PQC
signature algorithm families: ML-DSA, FALCON, SPHINCS+, MAYO, and SNOVA. Our
implementation maintains compatibility with existing DNS resolution flows while
providing on-the-fly signing using quantum-resistant signatures. A benchmark
has been performed and performance evaluation results reveal significant
trade-offs between security and efficiency. The results indicate that while PQC
algorithms introduce operational overhead, several candidates offer viable
compromises for transitioning DNSSEC to quantum-resistant cryptography.

</details>


### [55] [Backscatter Device-aided Integrated Sensing and Communication: A Pareto Optimization Framework](https://arxiv.org/abs/2507.09354)
*Yifan Zhang,Yu Bai,Riku Jantti,Zheng Yan,Christos Masouros,Zhu Han*

Main category: cs.CR

TL;DR: 本文提出了一种基于反向散射设备（BD）的集成感知与通信（ISAC）系统，通过利用环境中自然分布的被动BD增强性能，解决了密集遮挡场景下的性能下降问题。


<details>
  <summary>Details</summary>
Motivation: 在密集遮挡的城市和非视距场景中，ISAC系统的性能显著下降，限制了其实际应用。本文旨在通过利用被动BD提供的额外反射信号路径，提升感知精度和通信可靠性。

Method: 提出了一种BD辅助的ISAC系统，通过联合优化时频资源分配、发射功率管理和BD调制决策，定义了Pareto边界以权衡感知互信息（SMI）和通信速率。采用块坐标下降（BCD）算法分解并迭代求解非凸优化问题。

Result: 仿真结果表明，所提系统在性能上优于现有ISAC方案，并展示了其在双基地ISAC场景和多输入多输出（MIMO）设置中的适应性。

Conclusion: BD辅助的ISAC系统通过优化资源分配和调制策略，显著提升了密集遮挡场景下的性能，为实际部署提供了有效解决方案。

Abstract: Integrated sensing and communication (ISAC) systems potentially encounter
significant performance degradation in densely obstructed urban and
non-line-of-sight scenarios, thus limiting their effectiveness in practical
deployments. To deal with these challenges, this paper proposes a backscatter
device (BD)-assisted ISAC system, which leverages passive BDs naturally
distributed in underlying environments for performance enhancement. These
ambient devices can enhance sensing accuracy and communication reliability by
providing additional reflective signal paths. In this system, we define the
Pareto boundary characterizing the trade-off between sensing mutual information
(SMI) and communication rates to provide fundamental insights for its design.
To derive the boundary, we formulate a performance optimization problem within
an orthogonal frequency division multiplexing (OFDM) framework, by jointly
optimizing time-frequency resource element (RE) allocation, transmit power
management, and BD modulation decisions. To tackle the non-convexity of the
problem, we decompose it into three subproblems, solved iteratively through a
block coordinate descent (BCD) algorithm. Specifically, the RE subproblem is
addressed using the successive convex approximation (SCA) method, the power
subproblem is solved using an augmented Lagrangian combined water-filling
method, and the BD modulation subproblem is tackled using semidefinite
relaxation (SDR) methods. Additionally, we demonstrate the generality of the
proposed system by showing its adaptability to bistatic ISAC scenarios and MIMO
settings. Finally, extensive simulation results validate the effectiveness of
the proposed system and its superior performance compared to existing
state-of-the-art ISAC schemes.

</details>


### [56] [LLMalMorph: On The Feasibility of Generating Variant Malware using Large-Language-Models](https://arxiv.org/abs/2507.09411)
*Md Ajwad Akil,Adrian Shuai Li,Imtiaz Karim,Arun Iyengar,Ashish Kundu,Vinny Parla,Elisa Bertino*

Main category: cs.CR

TL;DR: 论文探讨了利用大语言模型（LLMs）生成恶意软件变种的可行性，提出了半自动化框架LLMalMorph，通过语义和语法代码理解生成变种，实验表明能降低杀毒引擎检测率。


<details>
  <summary>Details</summary>
Motivation: 基于LLMs在软件开发和代码生成中的成功应用，探索其在恶意软件变种生成中的潜力。

Method: 提出LLMalMorph框架，利用LLMs的代码理解能力，结合自定义提示和代码转换策略，无需资源密集型微调即可生成变种。

Result: 实验生成618个变种，部分降低了杀毒引擎检测率，并对基于机器学习的分类器有一定攻击效果。

Conclusion: LLMs在恶意软件变种生成中具有一定潜力，但仍存在局限性。

Abstract: Large Language Models (LLMs) have transformed software development and
automated code generation. Motivated by these advancements, this paper explores
the feasibility of LLMs in modifying malware source code to generate variants.
We introduce LLMalMorph, a semi-automated framework that leverages semantical
and syntactical code comprehension by LLMs to generate new malware variants.
LLMalMorph extracts function-level information from the malware source code and
employs custom-engineered prompts coupled with strategically defined code
transformations to guide the LLM in generating variants without
resource-intensive fine-tuning. To evaluate LLMalMorph, we collected 10 diverse
Windows malware samples of varying types, complexity and functionality and
generated 618 variants. Our thorough experiments demonstrate that it is
possible to reduce the detection rates of antivirus engines of these malware
variants to some extent while preserving malware functionalities. In addition,
despite not optimizing against any Machine Learning (ML)-based malware
detectors, several variants also achieved notable attack success rates against
an ML-based malware classifier. We also discuss the limitations of current LLM
capabilities in generating malware variants from source code and assess where
this emerging technology stands in the broader context of malware variant
generation.

</details>


### [57] [SmartphoneDemocracy: Privacy-Preserving E-Voting on Decentralized Infrastructure using Novel European Identity](https://arxiv.org/abs/2507.09453)
*Michał Jóźwik,Johan Pouwelse*

Main category: cs.CR

TL;DR: 本文提出了一种基于智能手机的安全、隐私保护的电子投票协议SmartphoneDemocracy，结合了欧洲数字身份钱包、零知识证明和点对点区块链技术，实现了匿名、可验证的投票。


<details>
  <summary>Details</summary>
Motivation: 现有电子投票系统依赖中心化架构，存在单点故障和信任问题，违背民主原则。本研究旨在设计一个安全、隐私保护且信任依赖最小的电子投票系统。

Method: 结合欧洲数字身份钱包（EUDI Wallet）进行身份验证，使用零知识证明保护隐私，并采用点对点区块链（TrustChain）作为公共公告板，实现服务器无关的投票系统。

Result: 协议设计详细，安全分析表明其能抵御威胁模型，性能评估显示计算和网络开销适用于中大规模选举。

Conclusion: SmartphoneDemocracy为公民提供了一种可信、易用且用户可控的数字投票方案，展示了可行的技术路径。

Abstract: The digitization of democratic processes promises greater accessibility but
presents challenges in terms of security, privacy, and verifiability. Existing
electronic voting systems often rely on centralized architectures, creating
single points of failure and forcing too much trust in authorities, which
contradicts democratic principles. This research addresses the challenge of
creating a secure, private e-voting system with minimized trust dependencies
designed for the most versatile personal device: the smartphone. We introduce
SmartphoneDemocracy, a novel e-voting protocol that combines three key
technologies: the emerging European Digital Identity (EUDI) Wallet for
Sybil-resistant identity verification, Zero-Knowledge Proofs for
privacy-preserving validation, and a peer-to-peer blockchain (TrustChain) for a
resilient, serverless public bulletin board. Our protocol enables voters to
register and cast ballots anonymously and verifiably directly from their
smartphones. We provide a detailed protocol design, a security analysis against
a defined threat model, and a performance evaluation demonstrating that the
computational and network overhead is feasible for medium- to large-scale
elections. By developing and prototyping this system, we demonstrate a viable
path to empower citizens with a trustworthy, accessible, and user-controlled
digital voting experience.

</details>


### [58] [A Mixture of Linear Corrections Generates Secure Code](https://arxiv.org/abs/2507.09508)
*Weichen Yu,Ravi Mangal,Terry Zhuo,Matt Fredrikson,Corina S. Pasareanu*

Main category: cs.CR

TL;DR: 研究发现大型语言模型（LLMs）内部能区分代码漏洞，通过混合修正（MoC）技术提升生成代码的安全性，同时保持功能。


<details>
  <summary>Details</summary>
Motivation: 探究LLMs在代码漏洞检测上的不足是否源于学习不足或提示无效。

Method: 利用表示工程技术分析LLMs内部表示，开发MoC技术调整生成概率。

Result: MoC使Qwen2.5-Coder-7B的安全率提升8.9%，功能通过率提升2.1%。

Conclusion: LLMs内部能识别漏洞，MoC是有效的漏洞管理方法。

Abstract: Large language models (LLMs) have become proficient at sophisticated
code-generation tasks, yet remain ineffective at reliably detecting or avoiding
code vulnerabilities. Does this deficiency stem from insufficient learning
about code vulnerabilities, or is it merely a result of ineffective prompting?
Using representation engineering techniques, we investigate whether LLMs
internally encode the concepts necessary to identify code vulnerabilities. We
find that current LLMs encode precise internal representations that distinguish
vulnerable from secure code--achieving greater accuracy than standard prompting
approaches. Leveraging these vulnerability-sensitive representations, we
develop an inference-time steering technique that subtly modulates the model's
token-generation probabilities through a mixture of corrections (MoC). Our
method effectively guides LLMs to produce less vulnerable code without
compromising functionality, demonstrating a practical approach to controlled
vulnerability management in generated code. Notably, MoC enhances the security
ratio of Qwen2.5-Coder-7B by 8.9\%, while simultaneously improving
functionality on HumanEval pass@1 by 2.1\%.

</details>


### [59] [A Login Page Transparency and Visual Similarity Based Zero Day Phishing Defense Protocol](https://arxiv.org/abs/2507.09564)
*Gaurav Varshney,Akanksha Raj,Divya Sangwan,Sharif Abuadbba,Rina Mishra,Yansong Gao*

Main category: cs.CR

TL;DR: 提出了一种名为“页面透明度”（PT）的新概念，通过公开记录登录页面并利用密码学证明进行验证，以防止钓鱼攻击。


<details>
  <summary>Details</summary>
Motivation: 当前钓鱼攻击检测多为独立解决方案，缺乏全面协议导向的方法，受证书透明度的启发，提出PT以增强网页透明度。

Method: 通过页面日志服务（PLS）公开记录登录页面，使用密码学证明和视觉页面匹配算法验证页面真实性，客户端通过HTTP PT头实现验证。

Result: 攻击者无法在PLS上注册欺骗性页面并获得验证所需的密码学证明，从而有效防止钓鱼攻击。

Conclusion: PT提供了一种无需平台特定更改或第三方解决方案的客户端防钓鱼方法，具有实际应用潜力。

Abstract: Phishing is a prevalent cyberattack that uses look-alike websites to deceive
users into revealing sensitive information. Numerous efforts have been made by
the Internet community and security organizations to detect, prevent, or train
users to avoid falling victim to phishing attacks. Most of this research over
the years has been highly diverse and application-oriented, often serving as
standalone solutions for HTTP clients, servers, or third parties. However,
limited work has been done to develop a comprehensive or proactive
protocol-oriented solution to effectively counter phishing attacks. Inspired by
the concept of certificate transparency, which allows certificates issued by
Certificate Authorities (CAs) to be publicly verified by clients, thereby
enhancing transparency, we propose a concept called Page Transparency (PT) for
the web. The proposed PT requires login pages that capture users' sensitive
information to be publicly logged via PLS and made available to web clients for
verification. The pages are verified to be logged using cryptographic proofs.
Since all pages are logged on a PLS and visually compared with existing pages
through a comprehensive visual page-matching algorithm, it becomes impossible
for an attacker to register a deceptive look-alike page on the PLS and receive
the cryptographic proof required for client verification. All implementations
occur on the client side, facilitated by the introduction of a new HTTP PT
header, eliminating the need for platform-specific changes or the installation
of third-party solutions for phishing prevention.

</details>


### [60] [PromptChain: A Decentralized Web3 Architecture for Managing AI Prompts as Digital Assets](https://arxiv.org/abs/2507.09579)
*Marc Bara*

Main category: cs.CR

TL;DR: PromptChain是一个去中心化的Web3架构，将AI提示作为一等数字资产，具备可验证的所有权、版本控制和变现能力。


<details>
  <summary>Details</summary>
Motivation: 当前中心化平台缺乏对提示创作者的合理归属、质量保证或公平补偿机制。PromptChain旨在解决这些问题。

Method: 通过整合IPFS（不可变存储）、智能合约（治理）和代币激励（社区管理），设计包括跨模型兼容的元数据模式、权益加权验证机制和按贡献比例奖励的代币经济。

Result: 该架构展示了去中心化系统在效率上可能匹敌中心化替代方案，同时通过区块链锚定的溯源提供更优的所有权保障和抗审查性。

Conclusion: PromptChain为Web3时代的人机协作开放生态系统奠定了基础，首次将提示作为独立数字资产并配备专用去中心化基础设施。

Abstract: We present PromptChain, a decentralized Web3 architecture that establishes AI
prompts as first-class digital assets with verifiable ownership, version
control, and monetization capabilities. Current centralized platforms lack
mechanisms for proper attribution, quality assurance, or fair compensation for
prompt creators. PromptChain addresses these limitations through a novel
integration of IPFS for immutable storage, smart contracts for governance, and
token incentives for community curation. Our design includes: (1) a
comprehensive metadata schema for cross-model compatibility, (2) a
stake-weighted validation mechanism to align incentives, and (3) a token
economy that rewards contributors proportionally to their impact. The proposed
architecture demonstrates how decentralized systems could potentially match
centralized alternatives in efficiency while providing superior ownership
guarantees and censorship resistance through blockchain-anchored provenance
tracking. By decoupling prompts from specific AI models or outputs, this work
establishes the foundation for an open ecosystem of human-AI collaboration in
the Web3 era, representing the first systematic treatment of prompts as
standalone digital assets with dedicated decentralized infrastructure.

</details>


### [61] [AICrypto: A Comprehensive Benchmark For Evaluating Cryptography Capabilities of Large Language Models](https://arxiv.org/abs/2507.09580)
*Yu Wang,Yijian Liu,Liheng Ji,Han Luo,Wenjie Li,Xiaofei Zhou,Chiyun Feng,Puji Wang,Yuhan Cao,Geyuan Zhang,Xiaojian Li,Rongwu Xu,Yilei Chen,Tianxing He*

Main category: cs.CR

TL;DR: AICrypto是首个全面评估大语言模型（LLMs）密码学能力的基准，包含多种任务类型，并引入人类专家基线进行比较。评估发现，LLMs在记忆和常规任务上表现优异，但在抽象数学和多步推理上仍有不足。


<details>
  <summary>Details</summary>
Motivation: 填补LLMs在密码学领域应用的研究空白，评估其密码学能力。

Method: 设计AICrypto基准，包含135道选择题、150个CTF挑战和18个证明问题，由专家审核，并开发自动化评估框架。

Result: 17个领先LLMs在记忆和常规任务上表现优于人类专家，但在抽象数学和多步推理上表现不佳。

Conclusion: LLMs在密码学应用中有潜力，但需进一步研究以提升抽象理解和推理能力。

Abstract: Large language models (LLMs) have demonstrated remarkable capabilities across
a variety of domains. However, their applications in cryptography, which serves
as a foundational pillar of cybersecurity, remain largely unexplored. To
address this gap, we propose \textbf{AICrypto}, the first comprehensive
benchmark designed to evaluate the cryptographic capabilities of LLMs. The
benchmark comprises 135 multiple-choice questions, 150 capture-the-flag (CTF)
challenges, and 18 proof problems, covering a broad range of skills from
factual memorization to vulnerability exploitation and formal reasoning. All
tasks are carefully reviewed or constructed by cryptography experts to ensure
correctness and rigor. To support automated evaluation of CTF challenges, we
design an agent-based framework. To gain deeper insight into the current state
of cryptographic proficiency in LLMs, we introduce human expert performance
baselines for comparison across all task types. Our evaluation of 17 leading
LLMs reveals that state-of-the-art models match or even surpass human experts
in memorizing cryptographic concepts, exploiting common vulnerabilities, and
routine proofs. However, they still lack a deep understanding of abstract
mathematical concepts and struggle with tasks that require multi-step reasoning
and dynamic analysis. We hope this work could provide insights for future
research on LLMs in cryptographic applications. Our code and dataset are
available at https://aicryptobench.github.io.

</details>


### [62] [Efficient Private Inference Based on Helper-Assisted Malicious Security Dishonest Majority MPC](https://arxiv.org/abs/2507.09607)
*Kaiwen Wang,Yuehan Dong,Junchao Fan,Xiaolin Chang*

Main category: cs.CR

TL;DR: 提出了一种基于HA-MSDM模型的私有推理框架，平衡安全性和效率，通过优化协议和策略在LAN和WAN中显著加速，同时保持高精度。


<details>
  <summary>Details</summary>
Motivation: 现有MPC框架在恶意安全不诚实多数模型下效率低，需平衡安全与效率。

Method: 设计了五种MPC协议和协同优化策略，采用多项式逼近非线性层和参数调整批量归一化层。

Result: 在LeNet和AlexNet上，LAN加速2.4-25.7倍，WAN加速1.3-9.5倍，相对误差仅0.04%-1.08%。

Conclusion: HA-MSDM框架在安全性和效率间取得平衡，适用于实际应用。

Abstract: Private inference based on Secure Multi-Party Computation (MPC) addresses
data privacy risks in Machine Learning as a Service (MLaaS). However, existing
MPC-based private inference frameworks focuses on semi-honest or honest
majority models, whose threat models are overly idealistic, while malicious
security dishonest majority models face the challenge of low efficiency. To
balance security and efficiency, we propose a private inference framework using
Helper-Assisted Malicious Security Dishonest Majority Model (HA-MSDM). This
framework includes our designed five MPC protocols and a co-optimized strategy.
These protocols achieve efficient fixed-round multiplication, exponentiation,
and polynomial operations, providing foundational primitives for private
inference. The co-optimized strategy balances inference efficiency and
accuracy. To enhance efficiency, we employ polynomial approximation for
nonlinear layers. For improved accuracy, we construct sixth-order polynomial
approximation within a fixed interval to achieve high-precision activation
function fitting and introduce parameter-adjusted batch normalization layers to
constrain the activation escape problem. Benchmark results on LeNet and AlexNet
show our framework achieves 2.4-25.7x speedup in LAN and 1.3-9.5x acceleration
in WAN compared to state-of-the-art frameworks (IEEE S&P'25), maintaining high
accuracy with only 0.04%-1.08% relative errors.

</details>


### [63] [CAN-Trace Attack: Exploit CAN Messages to Uncover Driving Trajectories](https://arxiv.org/abs/2507.09624)
*Xiaojie Lin,Baihe Ma,Xu Wang,Guangsheng Yu,Ying He,Wei Ni,Ren Ping Liu*

Main category: cs.CR

TL;DR: CAN-Trace是一种新的隐私攻击机制，利用CAN消息重建驾驶轨迹，攻击成功率高达90.59%（城市）和99.41%（郊区）。


<details>
  <summary>Details</summary>
Motivation: 现有驾驶轨迹隐私保护措施存在漏洞，传统GPS依赖方法易受数据中断影响。

Method: 提出CAN-Trace，通过车辆速度和油门踏板位置等CAN消息构建加权图，结合图匹配算法与路网对比识别轨迹。

Result: 实验验证显示，城市和郊区攻击成功率分别为90.59%和99.41%。

Conclusion: CAN-Trace揭示了CAN消息对驾驶轨迹隐私的潜在威胁，需进一步防护措施。

Abstract: Driving trajectory data remains vulnerable to privacy breaches despite
existing mitigation measures. Traditional methods for detecting driving
trajectories typically rely on map-matching the path using Global Positioning
System (GPS) data, which is susceptible to GPS data outage. This paper
introduces CAN-Trace, a novel privacy attack mechanism that leverages
Controller Area Network (CAN) messages to uncover driving trajectories, posing
a significant risk to drivers' long-term privacy. A new trajectory
reconstruction algorithm is proposed to transform the CAN messages,
specifically vehicle speed and accelerator pedal position, into weighted graphs
accommodating various driving statuses. CAN-Trace identifies driving
trajectories using graph-matching algorithms applied to the created graphs in
comparison to road networks. We also design a new metric to evaluate matched
candidates, which allows for potential data gaps and matching inaccuracies.
Empirical validation under various real-world conditions, encompassing
different vehicles and driving regions, demonstrates the efficacy of CAN-Trace:
it achieves an attack success rate of up to 90.59% in the urban region, and
99.41% in the suburban region.

</details>


### [64] [Interpreting Differential Privacy in Terms of Disclosure Risk](https://arxiv.org/abs/2507.09699)
*Zeki Kazan,Sagar Sharma,Wanrong Zhang,Bo Jiang,Qiang Yan*

Main category: cs.CR

TL;DR: 论文探讨了差分隐私（DP）与统计披露风险之间的关系，并提出了如何利用这些关系解释DP保证、选择隐私参数等。


<details>
  <summary>Details</summary>
Motivation: 随着差分隐私的广泛应用，开发有效工具来理解其隐私保障变得至关重要。

Method: 通过展示DP与统计披露风险之间的新关系，为专家和非专家提供工具。

Result: 提出了解释DP保证、选择隐私参数和识别最坏情况对手先验概率的方法。

Conclusion: 该研究为理解和应用差分隐私提供了新的视角和工具。

Abstract: As the use of differential privacy (DP) becomes widespread, the development
of effective tools for reasoning about the privacy guarantee becomes
increasingly critical. In pursuit of this goal, we demonstrate novel
relationships between DP and measures of statistical disclosure risk. We
suggest how experts and non-experts can use these results to explain the DP
guarantee, interpret DP composition theorems, select and justify privacy
parameters, and identify worst-case adversary prior probabilities.

</details>


### [65] [EventHunter: Dynamic Clustering and Ranking of Security Events from Hacker Forum Discussions](https://arxiv.org/abs/2507.09762)
*Yasir Ech-Chammakhy,Anas Motii,Anass Rabii,Jaafar Chbili*

Main category: cs.CR

TL;DR: 本文提出了一种无监督框架，利用Transformer嵌入和对比学习，自动检测、聚类和优先处理黑客论坛中的安全事件，有效减少噪音并提供高优先级威胁。


<details>
  <summary>Details</summary>
Motivation: 黑客论坛是网络安全威胁的早期预警信号来源，但其内容杂乱且噪音大，提取有用信息具有挑战性。

Method: 采用Transformer嵌入和对比学习，自动聚类相关讨论，并通过量化指标（如时效性、来源可信度等）对事件进行每日排名。

Result: 实验表明，该方法能有效减少噪音并识别高优先级威胁，帮助安全分析师主动响应。

Conclusion: 该框架将杂乱的黑客论坛讨论转化为结构化、可操作的情报，解决了自动化威胁检测和分析的核心挑战。

Abstract: Hacker forums provide critical early warning signals for emerging
cybersecurity threats, but extracting actionable intelligence from their
unstructured and noisy content remains a significant challenge. This paper
presents an unsupervised framework that automatically detects, clusters, and
prioritizes security events discussed across hacker forum posts. Our approach
leverages Transformer-based embeddings fine-tuned with contrastive learning to
group related discussions into distinct security event clusters, identifying
incidents like zero-day disclosures or malware releases without relying on
predefined keywords. The framework incorporates a daily ranking mechanism that
prioritizes identified events using quantifiable metrics reflecting timeliness,
source credibility, information completeness, and relevance. Experimental
evaluation on real-world hacker forum data demonstrates that our method
effectively reduces noise and surfaces high-priority threats, enabling security
analysts to mount proactive responses. By transforming disparate hacker forum
discussions into structured, actionable intelligence, our work addresses
fundamental challenges in automated threat detection and analysis.

</details>


### [66] [Endorsement-Driven Blockchain SSI Framework for Dynamic IoT Ecosystems](https://arxiv.org/abs/2507.09859)
*Guntur Dharma Putra,Bagus Rakadyanto Oktavianto Putra*

Main category: cs.CR

TL;DR: 提出了一种基于区块链的自主权身份（SSI）框架，支持动态物联网环境中的去中心化身份管理。


<details>
  <summary>Details</summary>
Motivation: 现有SSI框架在物联网（IoT）中依赖可信实体颁发和撤销凭证，限制了灵活性。

Method: 采用分层架构，通过基于背书的信任计算和分层信任链机制，利用区块链作为可验证数据注册表，智能合约自动化关键流程。

Result: 概念验证表明框架可行，且相比基线开销最小。

Conclusion: 该框架适合动态且资源受限的物联网环境。

Abstract: Self-Sovereign Identity (SSI) offers significant potential for managing
identities in the Internet of Things (IoT), enabling decentralized
authentication and credential management without reliance on centralized
entities. However, existing SSI frameworks often limit credential issuance and
revocation to trusted entities, such as IoT manufacturers, which restricts
flexibility in dynamic IoT ecosystems. In this paper, we propose a
blockchain-based SSI framework that allows any individual with a verifiable
trust linkage to act as a credential issuer, ensuring decentralized and
scalable identity management. Our framework incorporates a layered
architecture, where trust is dynamically established through endorsement-based
calculations and maintained via a hierarchical chain-of-trust mechanism.
Blockchain serves as the Verifiable Data Registry, ensuring transparency and
immutability of identity operations, while smart contracts automate critical
processes such as credential issuance, verification, and revocation. A
proof-of-concept implementation demonstrates that the proposed framework is
feasible and incurs minimal overheads compared to the baseline, making it
well-suited for dynamic and resource-constrained IoT environments.

</details>


### [67] [Secure and Efficient UAV-Based Face Detection via Homomorphic Encryption and Edge Computing](https://arxiv.org/abs/2507.09860)
*Nguyen Van Duc,Bui Duc Manh,Quang-Trung Luu,Dinh Thai Hoang,Van-Linh Nguyen,Diep N. Nguyen*

Main category: cs.CR

TL;DR: 提出了一种结合同态加密（HE）和神经网络的无人机（UAV）人脸检测方法，以解决隐私问题，同时保持高精度。


<details>
  <summary>Details</summary>
Motivation: 无人机人脸检测存在隐私泄露风险，需要一种既能保护数据隐私又不影响检测精度的方法。

Method: 采用Cheon-Kim-Kim-Song（CKKS）同态加密方案，设计数据编码和安全推理算法，直接在加密数据上进行计算。

Result: 实验表明，该方法在保护隐私的同时，检测精度损失小于1%。

Conclusion: 该方法为无人机安全人脸检测提供了可行的解决方案，平衡了隐私保护和性能。

Abstract: This paper aims to propose a novel machine learning (ML) approach
incorporating Homomorphic Encryption (HE) to address privacy limitations in
Unmanned Aerial Vehicles (UAV)-based face detection. Due to challenges related
to distance, altitude, and face orientation, high-resolution imagery and
sophisticated neural networks enable accurate face recognition in dynamic
environments. However, privacy concerns arise from the extensive surveillance
capabilities of UAVs. To resolve this issue, we propose a novel framework that
integrates HE with advanced neural networks to secure facial data throughout
the inference phase. This method ensures that facial data remains secure with
minimal impact on detection accuracy. Specifically, the proposed system
leverages the Cheon-Kim-Kim-Song (CKKS) scheme to perform computations directly
on encrypted data, optimizing computational efficiency and security.
Furthermore, we develop an effective data encoding method specifically designed
to preprocess the raw facial data into CKKS form in a
Single-Instruction-Multiple-Data (SIMD) manner. Building on this, we design a
secure inference algorithm to compute on ciphertext without needing decryption.
This approach not only protects data privacy during the processing of facial
data but also enhances the efficiency of UAV-based face detection systems.
Experimental results demonstrate that our method effectively balances privacy
protection and detection performance, making it a viable solution for UAV-based
secure face detection. Significantly, our approach (while maintaining data
confidentially with HE encryption) can still achieve an accuracy of less than
1% compared to the benchmark without using encryption.

</details>


### [68] [Differentially Private Federated Low Rank Adaptation Beyond Fixed-Matrix](https://arxiv.org/abs/2507.09990)
*Ming Wen,Jiaqi Zhu,Yuedong Xu,Yipeng Zhou,Dingding Han*

Main category: cs.CR

TL;DR: FedASK是一个新颖的联邦LoRA框架，通过双阶段草图技术实现高效且隐私保护的适配器更新。


<details>
  <summary>Details</summary>
Motivation: 解决联邦学习中LoRA适配器传输导致的隐私泄露问题，同时避免差分隐私对模型学习的负面影响。

Method: 采用两阶段草图管道，先聚合隐私保护的本地更新，再在服务器上重构全局矩阵。

Result: FedASK在多种隐私设置和数据分布下均优于基线方法。

Conclusion: FedASK有效平衡了隐私保护与模型性能，为联邦学习中的LoRA提供了可靠解决方案。

Abstract: Large language models (LLMs) typically require fine-tuning for
domain-specific tasks, and LoRA offers a computationally efficient approach by
training low-rank adapters. LoRA is also communication-efficient for federated
LLMs when multiple users collaboratively fine-tune a global LLM model without
sharing their proprietary raw data. However, even the transmission of local
adapters between a server and clients risks serious privacy leakage. Applying
differential privacy (DP) to federated LoRA encounters a dilemma: adding noise
to both adapters amplifies synthetic noise on the model, while fixing one
adapter impairs the learnability of fine-tuning. In this paper, we propose
FedASK (Differentially Private Federated Low Rank Adaptation with Double
Sketching) , a novel federated LoRA framework to enable effective updating of
both low-rank adapters with robust differential privacy. Inspired by randomized
SVD, our key idea is a two-stage sketching pipeline. This pipeline first
aggregates carefully sketched, privacy-preserving local updates, and then
reconstructs the global matrices on the server to facilitate effective updating
of both adapters. We theoretically prove FedASK's differential privacy
guarantee and its exact aggregation property. Comprehensive experiments
demonstrate that FedASK consistently outperforms baseline methods across a
variety of privacy settings and data distributions.

</details>


### [69] [The Man Behind the Sound: Demystifying Audio Private Attribute Profiling via Multimodal Large Language Model Agents](https://arxiv.org/abs/2507.10016)
*Lixu Wang,Kaixiang Yao,Xinfeng Li,Dong Yang,Haoyang Li,Xiaofeng Wang,Wei Dong*

Main category: cs.CR

TL;DR: 论文揭示了多模态大语言模型（MLLMs）通过音频数据推断敏感个人属性的隐私风险，提出了音频隐私属性分析技术，并开发了数据集AP^2和框架Gifts以解决相关挑战。


<details>
  <summary>Details</summary>
Motivation: 音频数据具有独特的隐私风险，但目前缺乏相关数据集和模型能力来研究这一问题。

Method: 提出AP^2数据集和Gifts框架，结合音频语言模型和大语言模型的优势，提升敏感属性推断能力。

Result: Gifts框架显著优于基线方法，验证了音频隐私攻击的可行性。

Conclusion: 研究强调了音频隐私风险，提供了数据集和防御策略，推动未来研究。

Abstract: Our research uncovers a novel privacy risk associated with multimodal large
language models (MLLMs): the ability to infer sensitive personal attributes
from audio data -- a technique we term audio private attribute profiling. This
capability poses a significant threat, as audio can be covertly captured
without direct interaction or visibility. Moreover, compared to images and
text, audio carries unique characteristics, such as tone and pitch, which can
be exploited for more detailed profiling. However, two key challenges exist in
understanding MLLM-employed private attribute profiling from audio: (1) the
lack of audio benchmark datasets with sensitive attribute annotations and (2)
the limited ability of current MLLMs to infer such attributes directly from
audio. To address these challenges, we introduce AP^2, an audio benchmark
dataset that consists of two subsets collected and composed from real-world
data, and both are annotated with sensitive attribute labels. Additionally, we
propose Gifts, a hybrid multi-agent framework that leverages the complementary
strengths of audio-language models (ALMs) and large language models (LLMs) to
enhance inference capabilities. Gifts employs an LLM to guide the ALM in
inferring sensitive attributes, then forensically analyzes and consolidates the
ALM's inferences, overcoming severe hallucinations of existing ALMs in
generating long-context responses. Our evaluations demonstrate that Gifts
significantly outperforms baseline approaches in inferring sensitive
attributes. Finally, we investigate model-level and data-level defense
strategies to mitigate the risks of audio private attribute profiling. Our work
validates the feasibility of audio-based privacy attacks using MLLMs,
highlighting the need for robust defenses, and provides a dataset and framework
to facilitate future research.

</details>


### [70] [HASSLE: A Self-Supervised Learning Enhanced Hijacking Attack on Vertical Federated Learning](https://arxiv.org/abs/2507.10162)
*Weiyang He,Chip-Hong Chang*

Main category: cs.CR

TL;DR: HASSLE是一种针对垂直联邦学习（VFL）的高效攻击框架，结合梯度方向标签推断和自监督学习的对抗嵌入生成算法，攻击成功率高达99%。


<details>
  <summary>Details</summary>
Motivation: 现有VFL攻击方法因标签推断精度低和注入条件不理想而受限，需更严格的评估框架。

Method: 提出HASSLE框架，包含梯度方向标签推断模块和自监督学习增强的对抗嵌入生成算法。

Result: 在两方场景中，攻击成功率（ASR）在四个数据集上超过99%，在CIFAR-100上达85%。

Conclusion: HASSLE对VFL构成显著威胁，同时为构建可信VFL系统提供新见解。

Abstract: Vertical Federated Learning (VFL) enables an orchestrating active party to
perform a machine learning task by cooperating with passive parties that
provide additional task-related features for the same training data entities.
While prior research has leveraged the privacy vulnerability of VFL to
compromise its integrity through a combination of label inference and backdoor
attacks, their effectiveness is constrained by the low label inference
precision and suboptimal backdoor injection conditions. To facilitate a more
rigorous security evaluation on VFL without these limitations, we propose
HASSLE, a hijacking attack framework composed of a gradient-direction-based
label inference module and an adversarial embedding generation algorithm
enhanced by self-supervised learning. HASSLE accurately identifies private
samples associated with a targeted label using only a single known instance of
that label. In the two-party scenario, it demonstrates strong performance with
an attack success rate (ASR) of over 99% across four datasets, including both
image and tabular modalities, and achieves 85% ASR on the more complex
CIFAR-100 dataset. Evaluation of HASSLE against 8 potential defenses further
highlights its significant threat while providing new insights into building a
trustworthy VFL system.

</details>


### [71] [DNS Tunneling: Threat Landscape and Improved Detection Solutions](https://arxiv.org/abs/2507.10267)
*Novruz Amirov,Baran Isik,Bilal Ihsan Tuncer,Serif Bahtiyar*

Main category: cs.CR

TL;DR: 提出了一种基于机器学习的新方法，用于检测DNS隧道，解决了传统规则和签名匹配方法的不足。


<details>
  <summary>Details</summary>
Motivation: DNS隧道能够隐藏恶意行为于看似正常的流量中，传统检测方法效果有限。

Method: 结合机器学习算法，通过分析DNS流量提取的特征来检测隧道。

Result: 分析结果表明，该方法能准确检测DNS隧道。

Conclusion: 该机器学习方法是检测DNS隧道的有效候选方案。

Abstract: Detecting Domain Name System (DNS) tunneling is a significant challenge in
security due to its capacity to hide harmful actions within DNS traffic that
appears to be normal and legitimate. Traditional detection methods are based on
rule-based approaches or signature matching methods that are often insufficient
to accurately identify such covert communication channels. This research is
about effectively detecting DNS tunneling. We propose a novel approach to
detect DNS tunneling with machine learning algorithms. We combine machine
learning algorithms to analyze the traffic by using features extracted from DNS
traffic. Analyses results show that the proposed approach is a good candidate
to detect DNS tunneling accurately.

</details>


### [72] [Logic layer Prompt Control Injection (LPCI): A Novel Security Vulnerability Class in Agentic Systems](https://arxiv.org/abs/2507.10457)
*Hammad Atta,Ken Huang,Manish Bhatt,Kamal Ahmed,Muhammad Aziz Ul Haq,Yasir Mehmood*

Main category: cs.CR

TL;DR: 论文提出了一种新型攻击类别LPCI，利用LLMs在逻辑执行层和持久内存中的漏洞，通过编码、延迟和条件触发的载荷绕过输入过滤器。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型（LLMs）在企业系统中的集成，新的隐蔽安全漏洞出现，特别是在逻辑执行层和持久内存环境中。

Method: 引入Logic-Layer Prompt Control Injection (LPCI)攻击类别，通过在内存、向量存储或工具输出中嵌入编码、延迟和条件触发的载荷。

Result: 这些载荷可以绕过传统输入过滤器，并在多个会话中触发未经授权的行为。

Conclusion: LPCI揭示了LLMs在安全方面的潜在风险，需要新的防御机制来应对此类攻击。

Abstract: The integration of large language models (LLMs) into enterprise systems has
created a new class of covert security vulnerabilities, particularly within
logic-execution layers and persistent-memory contexts. In this paper, we
introduce Logic-Layer Prompt Control Injection (LPCI), a novel attack category
in which encoded, delayed, and conditionally triggered payloads are embedded in
memory, vector stores, or tool outputs. These payloads can bypass conventional
input filters and trigger unauthorised behaviour across sessions.

</details>


### [73] [SynthGuard: Redefining Synthetic Data Generation with a Scalable and Privacy-Preserving Workflow Framework](https://arxiv.org/abs/2507.10489)
*Eduardo Brito,Mahmoud Shoush,Kristian Tamm,Paula Etti,Liina Kamm*

Main category: cs.CR

TL;DR: SynthGuard框架通过模块化和隐私保护的工作流，解决了合成数据生成中的数据主权和合规性问题，确保安全、可审计和可复现的执行。


<details>
  <summary>Details</summary>
Motivation: 数据驱动应用在医疗、金融等领域的需求增长，但现有合成数据生成方法存在数据主权和合规性问题，需要一种新的解决方案。

Method: 提出SynthGuard框架，支持模块化和隐私保护的工作流，确保数据所有者对合成数据生成流程的控制。

Result: 通过实际用例验证，SynthGuard在安全性、隐私性和可扩展性方面表现优异，同时满足合规要求。

Conclusion: SynthGuard有效解决了合成数据生成中的复杂问题，为数据主权和合规性提供了可行的解决方案。

Abstract: The growing reliance on data-driven applications in sectors such as
healthcare, finance, and law enforcement underscores the need for secure,
privacy-preserving, and scalable mechanisms for data generation and sharing.
Synthetic data generation (SDG) has emerged as a promising approach but often
relies on centralized or external processing, raising concerns about data
sovereignty, domain ownership, and compliance with evolving regulatory
standards. To overcome these issues, we introduce SynthGuard, a framework
designed to ensure computational governance by enabling data owners to maintain
control over SDG workflows. SynthGuard supports modular and privacy-preserving
workflows, ensuring secure, auditable, and reproducible execution across
diverse environments. In this paper, we demonstrate how SynthGuard addresses
the complexities at the intersection of domain-specific needs and scalable SDG
by aligning with requirements for data sovereignty and regulatory compliance.
Developed iteratively with domain expert input, SynthGuard has been validated
through real-world use cases, demonstrating its ability to balance security,
privacy, and scalability while ensuring compliance. The evaluation confirms its
effectiveness in implementing and executing SDG workflows and integrating
privacy and utility assessments across various computational environments.

</details>


### [74] [BURN: Backdoor Unlearning via Adversarial Boundary Analysis](https://arxiv.org/abs/2507.10491)
*Yanghao Su,Jie Zhang,Yiming Li,Tianwei Zhang,Qing Guo,Weiming Zhang,Nenghai Yu,Nils Lukas,Wenbo Zhou*

Main category: cs.CR

TL;DR: BURN通过对抗边界分析检测并修复中毒样本，结合数据精炼和模型净化，有效消除后门威胁。


<details>
  <summary>Details</summary>
Motivation: 现有后门遗忘方法未能完全消除触发器与目标标签的虚假关联，需改进。

Method: 利用对抗边界攻击技术，检测中毒样本并恢复其正确标签，结合反馈机制精炼数据和净化模型。

Result: BURN在多种数据集和攻击类型下有效移除后门威胁，保持模型性能。

Conclusion: BURN是一种高效的后门遗忘框架，能同时解决虚假关联和模型性能问题。

Abstract: Backdoor unlearning aims to remove backdoor-related information while
preserving the model's original functionality. However, existing unlearning
methods mainly focus on recovering trigger patterns but fail to restore the
correct semantic labels of poison samples. This limitation prevents them from
fully eliminating the false correlation between the trigger pattern and the
target label. To address this, we leverage boundary adversarial attack
techniques, revealing two key observations. First, poison samples exhibit
significantly greater distances from decision boundaries compared to clean
samples, indicating they require larger adversarial perturbations to change
their predictions. Second, while adversarial predicted labels for clean samples
are uniformly distributed, those for poison samples tend to revert to their
original correct labels. Moreover, the features of poison samples restore to
closely resemble those of corresponding clean samples after adding adversarial
perturbations. Building upon these insights, we propose Backdoor Unlearning via
adversaRial bouNdary analysis (BURN), a novel defense framework that integrates
false correlation decoupling, progressive data refinement, and model
purification. In the first phase, BURN employs adversarial boundary analysis to
detect poisoned samples based on their abnormal adversarial boundary distances,
then restores their correct semantic labels for fine-tuning. In the second
phase, it employs a feedback mechanism that tracks prediction discrepancies
between the original backdoored model and progressively sanitized models,
guiding both dataset refinement and model purification. Extensive evaluations
across multiple datasets, architectures, and seven diverse backdoor attack
types confirm that BURN effectively removes backdoor threats while maintaining
the model's original performance.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [75] [Think Clearly: Improving Reasoning via Redundant Token Pruning](https://arxiv.org/abs/2507.08806)
*Daewon Choi,Jimin Lee,Jihoon Tack,Woomin Song,Saket Dingliwal,Sai Muralidhar Jayanthi,Bhavana Ganesh,Jinwoo Shin,Aram Galstyan,Sravan Babu Bodapati*

Main category: cs.AI

TL;DR: 论文提出了一种通过去除推理过程中的冗余注意力来提升大语言模型在长链推理任务中性能的方法。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型在长链推理中存在注意力冗余问题，尤其是错误答案中注意力稀疏性更明显，影响了推理性能。

Method: 通过测量特殊结束标记的注意力分数识别冗余，采用结构感知剪枝去除低贡献推理块中的冗余标记。

Result: 该方法显著提升了推理密集型任务的准确性，尤其在数学竞赛基准（如AIME和AMC）上表现突出。

Conclusion: 去除推理冗余能有效提升模型性能，无需额外训练即可实现显著改进。

Abstract: Recent large language models have shown promising capabilities in long-form
reasoning, following structured chains of thought before arriving at a final
answer. However, we observe that these reasoning paths tend to include
substantial redundancy; analyzing attention patterns reveals that attention
scores are widely scattered, particularly incorrect answers exhibit greater
attention sparsity. In this paper, we demonstrate that deliberately removing
this redundancy in the reasoning process significantly improves performance
through clear thinking, i.e., removing distraction. Specifically, we
systematically identify reasoning redundancy by measuring token-level attention
scores to a special end-of-thinking token, which is appended to an explicit
instruction inserted to conclude each intermediate reasoning step. Furthermore,
we propose structure-aware pruning that prioritizes removing tokens in
low-contributing reasoning chunks over individual tokens. After evicting
redundant tokens, we remove the injected end-of-thinking instruction, then
resume the reasoning generation. We demonstrate that our method significantly
improves overall accuracy across reasoning-intensive benchmarks without any
training involved. In particular, our method shows strong performance on
challenging mathematical competition benchmarks such as AIME and AMC, where
reasoning redundancy is more prevalent.

</details>


### [76] [A New Approach for Multicriteria Assessment in the Ranking of Alternatives Using Cardinal and Ordinal Data](https://arxiv.org/abs/2507.08875)
*Fuh-Hwa Franklin Liu,Su-Chuan Shih*

Main category: cs.AI

TL;DR: 提出了一种结合两种虚拟差距分析（VGA）模型的新型多准则评估（MCA）方法，以提高评估的效率和公平性。


<details>
  <summary>Details</summary>
Motivation: 解决现有多准则评估方法中假设和主观判断带来的问题，尤其是在处理定量和定性准则时的挑战。

Method: 结合两种VGA模型，基于线性编程框架，开发了一种新的MCA方法。

Result: 通过两个数值示例验证了该方法的准确性和透明度。

Conclusion: 该方法为自动决策系统和决策支持系统提供了可靠且灵活的解决方案，推动了该领域的进步。

Abstract: Modern methods for multi-criteria assessment (MCA), such as Data Envelopment
Analysis (DEA), Stochastic Frontier Analysis (SFA), and Multiple Criteria
Decision-Making (MCDM), are utilized to appraise a collection of
Decision-Making Units (DMUs), also known as alternatives, based on several
criteria. These methodologies inherently rely on assumptions and can be
influenced by subjective judgment to effectively tackle the complex evaluation
challenges in various fields. In real-world scenarios, it is essential to
incorporate both quantitative and qualitative criteria as they consist of
cardinal and ordinal data. Despite the inherent variability in the criterion
values of different alternatives, the homogeneity assumption is often employed,
significantly affecting evaluations. To tackle these challenges and determine
the most appropriate alternative, we propose a novel MCA approach that combines
two Virtual Gap Analysis (VGA) models. The VGA framework, rooted in linear
programming, is pivotal in the MCA methodology. This approach improves
efficiency and fairness, ensuring that evaluations are both comprehensive and
dependable, thus offering a strong and adaptive solution. Two comprehensive
numerical examples demonstrate the accuracy and transparency of our proposed
method. The goal is to encourage continued advancement and stimulate progress
in automated decision systems and decision support systems.

</details>


### [77] [Multi-Actor Generative Artificial Intelligence as a Game Engine](https://arxiv.org/abs/2507.08892)
*Alexander Sasha Vezhnevets,Jayd Matyas,Logan Cross,Davide Paglieri,Minsuk Chang,William A. Cunningham,Simon Osindero,William S. Isaac,Joel Z. Leibo*

Main category: cs.AI

TL;DR: 论文提出了一种基于桌游角色扮演游戏（TTRPGs）的灵活场景定义框架，用于支持生成式AI在多角色环境中的多样化应用。


<details>
  <summary>Details</summary>
Motivation: 为了满足生成式AI在模拟、叙事和评估等多样化用例中的需求，需要一个灵活的框架来定义场景。

Method: 采用实体-组件架构模式，将游戏主控（GM）设计为可配置的实体，由组件构成，实现工程师与设计师的分工协作。

Result: 通过Concordia库的实践，证明了该框架能够有效支持用户根据特定目标配置场景。

Conclusion: 该框架通过分离关注点，实现了快速迭代、模块化和可扩展性，适用于生成式AI的多样化应用。

Abstract: Generative AI can be used in multi-actor environments with purposes ranging
from social science modeling to interactive narrative and AI evaluation.
Supporting this diversity of use cases -- which we classify as Simulationist,
Dramatist, and Evaluationist -- demands a flexible scenario definition
framework. We argue here that a good approach is to take inspiration from
tabletop role-playing games (TTRPGs), where a Game Master (GM) is responsible
for the environment and generates all parts of the story not directly
determined by the voluntary actions of player characters. We argue that the
Entity-Component architectural pattern is useful here. In such a system, the GM
is not a hardcoded computer game but is itself a configurable entity, composed
of components just like any other actor. By design, the approach allows for a
separation between the underlying implementation details handled by an
engineer, the creation of reusable components, and their composition and
configuration managed by a designer who constructs entities from the
components. This separation of concerns is instrumental for achieving rapid
iteration, maintaining modularity, and ultimately to ensure scalability. We
describe the ongoing evolution of the Concordia library in terms of this
philosophy, demonstrating how it allows users to effectively configure
scenarios that align with their specific goals.

</details>


### [78] [BioAnalyst: A Foundation Model for Biodiversity](https://arxiv.org/abs/2507.09080)
*Athanasios Trantas,Martino Mensio,Stylianos Stasinos,Sebastian Gribincea,Taimur Khan,Damian Podareanu,Aliene van der Veen*

Main category: cs.AI

TL;DR: BioAnalyst是一个基于Transformer架构的AI基础模型，专为生物多样性分析和保护规划设计，通过多模态数据预训练，适用于多种生态任务，并在数据稀缺场景中表现出色。


<details>
  <summary>Details</summary>
Motivation: 生物多样性丧失加速，威胁生态平衡和可持续性，需综合监测和保护规划。AI基础模型在多领域成功应用，为生物多样性保护提供新思路。

Method: BioAnalyst采用Transformer架构，预训练于物种记录、遥感数据、气候和环境变量等多模态数据，可微调用于物种分布建模、栖息地评估等任务。

Result: 模型在两种下游任务中表现优于现有方法，尤其在数据稀缺场景下，为生态预测设定了新的准确度基准。

Conclusion: BioAnalyst的开放发布旨在促进生物多样性建模合作，推动AI解决生态挑战。

Abstract: The accelerating loss of biodiversity presents critical challenges for
ecological research and conservation strategies. The preservation of
biodiversity is paramount for maintaining ecological balance and ensuring the
sustainability of ecosystems. However, biodiversity faces numerous threats,
including habitat loss, climate change, and the proliferation of invasive
species. Addressing these and other ecology-related challenges, both at local
and global scales, requires comprehensive monitoring, predictive and
conservation planning capabilities. Artificial Intelligence (AI) Foundation
Models (FMs) have gained significant momentum in numerous scientific domains by
leveraging vast datasets to learn general-purpose representations adaptable to
various downstream tasks. This paradigm holds immense promise for biodiversity
conservation. In response, we introduce BioAnalyst, the first Foundation Model
tailored for biodiversity analysis and conservation planning. BioAnalyst
employs a transformer-based architecture, pre-trained on extensive multi-modal
datasets encompassing species occurrence records, remote sensing indicators,
climate and environmental variables. BioAnalyst is designed for adaptability,
allowing for fine-tuning of a range of downstream tasks, such as species
distribution modelling, habitat suitability assessments, invasive species
detection, and population trend forecasting. We evaluate the model's
performance on two downstream use cases, demonstrating its generalisability
compared to existing methods, particularly in data-scarce scenarios for two
distinct use-cases, establishing a new accuracy baseline for ecological
forecasting. By openly releasing BioAnalyst and its fine-tuning workflows to
the scientific community, we aim to foster collaborative efforts in
biodiversity modelling and advance AI-driven solutions to pressing ecological
challenges.

</details>


### [79] [Measuring the Impact of Early-2025 AI on Experienced Open-Source Developer Productivity](https://arxiv.org/abs/2507.09089)
*Joel Becker,Nate Rush,Elizabeth Barnes,David Rein*

Main category: cs.AI

TL;DR: 研究发现，尽管开发者预期AI工具能缩短任务完成时间，但实际使用AI工具反而增加了19%的完成时间，与经济学和机器学习专家的预测相反。


<details>
  <summary>Details</summary>
Motivation: 研究AI工具对开源开发者生产力的实际影响，填补现有研究的空白。

Method: 采用随机对照试验（RCT），16名有中等AI经验的开发者完成246个任务，随机分配是否使用2025年初的AI工具（如Cursor Pro和Claude 3.5/3.7 Sonnet）。

Result: 开发者预期AI工具能减少24%的完成时间，但实际使用AI工具增加了19%的完成时间。

Conclusion: AI工具在实际应用中可能并未如预期提升生产力，甚至可能降低效率，需进一步研究其影响因素。

Abstract: Despite widespread adoption, the impact of AI tools on software development
in the wild remains understudied. We conduct a randomized controlled trial
(RCT) to understand how AI tools at the February-June 2025 frontier affect the
productivity of experienced open-source developers. 16 developers with moderate
AI experience complete 246 tasks in mature projects on which they have an
average of 5 years of prior experience. Each task is randomly assigned to allow
or disallow usage of early 2025 AI tools. When AI tools are allowed, developers
primarily use Cursor Pro, a popular code editor, and Claude 3.5/3.7 Sonnet.
Before starting tasks, developers forecast that allowing AI will reduce
completion time by 24%. After completing the study, developers estimate that
allowing AI reduced completion time by 20%. Surprisingly, we find that allowing
AI actually increases completion time by 19%--AI tooling slowed developers
down. This slowdown also contradicts predictions from experts in economics (39%
shorter) and ML (38% shorter). To understand this result, we collect and
evaluate evidence for 20 properties of our setting that a priori could
contribute to the observed slowdown effect--for example, the size and quality
standards of projects, or prior developer experience with AI tooling. Although
the influence of experimental artifacts cannot be entirely ruled out, the
robustness of the slowdown effect across our analyses suggests it is unlikely
to primarily be a function of our experimental design.

</details>


### [80] [Hide-and-Shill: A Reinforcement Learning Framework for Market Manipulation Detection in Symphony-a Decentralized Multi-Agent System](https://arxiv.org/abs/2507.09179)
*Ronghua Shi,Yiou Liu,Xinyu Ying,Yang Tan,Yuchun Feng,Lynn Ai,Bill Shi,Xuhui Wang,Zhuang Liu*

Main category: cs.AI

TL;DR: 提出了一种基于多智能体强化学习（MARL）的框架，用于去中心化金融（DeFi）中的市场操纵检测，通过动态对抗游戏建模操纵者与检测者的交互。


<details>
  <summary>Details</summary>
Motivation: DeFi的无许可创新带来了市场操纵问题，缺乏中心化监管导致恶意行为者通过协调活动进行操纵。

Method: 采用MARL框架，引入GRPO优化学习稳定性，基于理论的奖励函数区分价格发现与操纵噪声，以及多模态智能体管道整合多种数据源。

Result: 在真实数据和对抗模拟中验证，Hide-and-Shill在检测准确性和因果归因方面表现优异。

Conclusion: 该框架为去中心化市场情报提供了新范式，推动了多智能体系统与金融监管的结合。

Abstract: Decentralized finance (DeFi) has introduced a new era of permissionless
financial innovation but also led to unprecedented market manipulation. Without
centralized oversight, malicious actors coordinate shilling campaigns and
pump-and-dump schemes across various platforms. We propose a Multi-Agent
Reinforcement Learning (MARL) framework for decentralized manipulation
detection, modeling the interaction between manipulators and detectors as a
dynamic adversarial game. This framework identifies suspicious patterns using
delayed token price reactions as financial indicators.Our method introduces
three innovations: (1) Group Relative Policy Optimization (GRPO) to enhance
learning stability in sparse-reward and partially observable settings; (2) a
theory-based reward function inspired by rational expectations and information
asymmetry, differentiating price discovery from manipulation noise; and (3) a
multi-modal agent pipeline that integrates LLM-based semantic features, social
graph signals, and on-chain market data for informed decision-making.The
framework is integrated within the Symphony system, a decentralized multi-agent
architecture enabling peer-to-peer agent execution and trust-aware learning
through distributed logs, supporting chain-verifiable evaluation. Symphony
promotes adversarial co-evolution among strategic actors and maintains robust
manipulation detection without centralized oracles, enabling real-time
surveillance across global DeFi ecosystems.Trained on 100,000 real-world
discourse episodes and validated in adversarial simulations, Hide-and-Shill
achieves top performance in detection accuracy and causal attribution. This
work bridges multi-agent systems with financial surveillance, advancing a new
paradigm for decentralized market intelligence. All resources are available at
the Hide-and-Shill GitHub repository to promote open research and
reproducibility.

</details>


### [81] [When Developer Aid Becomes Security Debt: A Systematic Analysis of Insecure Behaviors in LLM Coding Agents](https://arxiv.org/abs/2507.09329)
*Matous Kozak,Roshanak Zilouchian Moghaddam,Siva Sivaraman*

Main category: cs.AI

TL;DR: 论文首次系统评估了基于LLM的编程代理的安全性，发现21%的操作存在安全隐患，并提出了检测系统和缓解策略。


<details>
  <summary>Details</summary>
Motivation: 研究基于LLM的编程代理在软件开发中的广泛应用，但其安全性尚未被充分理解，可能引入不安全实践。

Method: 分析了五个先进模型（如GPT-4o、GPT-4.1等）在93个真实软件任务中的12,000多个操作，开发了高精度检测系统。

Result: 21%的操作不安全，信息暴露（CWE-200）最常见；GPT-4.1的缓解成功率高达96.8%。

Conclusion: 研究为评估编程代理安全性提供了框架，并强调下一代LLM编程代理需注重安全性设计。

Abstract: LLM-based coding agents are rapidly being deployed in software development,
yet their security implications remain poorly understood. These agents, while
capable of accelerating software development, may inadvertently introduce
insecure practices. We conducted the first systematic security evaluation of
autonomous coding agents, analyzing over 12,000 actions across five
state-of-the-art models (GPT-4o, GPT-4.1, Claude variants) on 93 real-world
software setup tasks. Our findings reveal significant security concerns: 21% of
agent trajectories contained insecure actions, with models showing substantial
variation in security behavior. We developed a high-precision detection system
that identified four major vulnerability categories, with information exposure
(CWE-200) being the most prevalent one. We also evaluated mitigation strategies
including feedback mechanisms and security reminders with various effectiveness
between models. GPT-4.1 demonstrated exceptional security awareness with 96.8%
mitigation success. Our work provides the first comprehensive framework for
evaluating coding agent security and highlights the need for security-aware
design of next generation LLM-based coding agents.

</details>


### [82] [A Taxonomy of Omnicidal Futures Involving Artificial Intelligence](https://arxiv.org/abs/2507.09369)
*Andrew Critch,Jacob Tsimerman*

Main category: cs.AI

TL;DR: 报告提出了一种关于AI可能导致全人类灭绝事件的分类和示例，旨在通过公开讨论支持预防措施。


<details>
  <summary>Details</summary>
Motivation: 探讨AI可能带来的灾难性风险，以促进公众支持和预防行动。

Method: 提出分类和具体示例，分析潜在的全人类灭绝情景。

Result: 明确了AI可能导致的灾难性风险，并呼吁公众关注和预防。

Conclusion: 通过公开讨论这些可能性，可以推动预防措施，减少AI带来的灾难性风险。

Abstract: This report presents a taxonomy and examples of potential omnicidal events
resulting from AI: scenarios where all or almost all humans are killed. These
events are not presented as inevitable, but as possibilities that we can work
to avoid. Insofar as large institutions require a degree of public support in
order to take certain actions, we hope that by presenting these possibilities
in public, we can help to support preventive measures against catastrophic
risks from AI.

</details>


### [83] [EduFlow: Advancing MLLMs' Problem-Solving Proficiency through Multi-Stage, Multi-Perspective Critique](https://arxiv.org/abs/2507.09374)
*Chenglin Zhu,Tao Zhang,Chong Li,Mingan Lin,Zenan Zhou,Jian Xie*

Main category: cs.AI

TL;DR: EduFlow是一个端到端框架，用于提升多模态大语言模型（MLLMs）在科学任务中的推理能力，通过EduPRM和EduMCTS等技术优化推理过程。


<details>
  <summary>Details</summary>
Motivation: 当前MLLMs在科学任务中表现不佳，尤其是在需要多步推理和可解释性的任务中，存在推理模式不足、全局连贯性缺失和缺乏自我修正等问题。

Method: 提出EduFlow框架，包括EduPRM（过程感知奖励模型）和EduMCTS（领域适应的搜索框架），通过多源监督和自反思机制优化推理轨迹。

Result: 实验表明EduFlow显著提升了推理的一致性和连贯性，并构建了大规模数据集EduMCTS-160K。

Conclusion: EduFlow为科学教育任务提供了一种有效的解决方案，未来将发布代码、数据和模型。

Abstract: Multimodal large language models (MLLMs) still perform poorly on scientific
tasks, particularly those requiring multi-step and interpretable reasoning.
Their limitations include insufficient scientific reasoning patterns, lack of
global coherence in multi-step inference, and the absence of reflective
self-correction, making them unreliable in structured scientific contexts. We
introduce EduFlow, the first end-to-end framework that covers the full pipeline
of educational scientific reasoning, including data selection, MCTS-based
trajectory construction, model training, and output optimization. At its core
is EduPRM, a process-aware reward model that critiques reasoning steps with
tags and justifications. EduPRM is trained via curriculum learning on three
complementary supervision sources: MCTS-guided trajectories, error-injected
critiques, and teacher-student dialogues, enabling dynamic adaptation to
multi-stage problem solving and iterative refinement during inference. We
further propose EduMCTS, a domain-adapted search framework that introduces
bootstrapping actions specifically designed for educational reasoning, such as
a self-reflection mechanism that promotes reflective error correction. It
further leverages EduPRM's fine-grained feedback to guide the search toward
higher-quality reasoning trajectories. By applying self-consistency and
rejection sampling, we constructed EduMCTS-160K, a large-scale dataset of
educational reasoning trajectories. Extensive experiments demonstrate that
EduFlow enhances reasoning consistency and coherence. Code, data, and models
will be released.

</details>


### [84] [Knowledge Conceptualization Impacts RAG Efficacy](https://arxiv.org/abs/2507.09389)
*Chris Davis Jaldi,Anmol Saini,Elham Ghiasi,O. Divine Eziolise,Cogan Shimizu*

Main category: cs.AI

TL;DR: 论文探讨了如何结合可解释性和适应性设计可转移且可解释的神经符号AI系统，重点关注知识表示对LLM查询三元组库的影响。


<details>
  <summary>Details</summary>
Motivation: 可解释性和适应性是下一代AI系统的关键，尤其是在大型语言模型和生成式AI中。研究旨在结合这两点，设计可转移且可解释的神经符号AI系统。

Method: 研究系统评估了不同知识表示（结构和复杂性）对LLM查询三元组库的影响，重点关注“Agentic Retrieval-Augmented Generation”系统。

Result: 结果显示知识表示对LLM查询效果有显著影响。

Conclusion: 研究强调了知识表示在AI系统中的重要性，并讨论了其对设计可解释和适应性系统的启示。

Abstract: Explainability and interpretability are cornerstones of frontier and
next-generation artificial intelligence (AI) systems. This is especially true
in recent systems, such as large language models (LLMs), and more broadly,
generative AI. On the other hand, adaptability to new domains, contexts, or
scenarios is also an important aspect for a successful system. As such, we are
particularly interested in how we can merge these two efforts, that is,
investigating the design of transferable and interpretable neurosymbolic AI
systems. Specifically, we focus on a class of systems referred to as ''Agentic
Retrieval-Augmented Generation'' systems, which actively select, interpret, and
query knowledge sources in response to natural language prompts. In this paper,
we systematically evaluate how different conceptualizations and representations
of knowledge, particularly the structure and complexity, impact an AI agent (in
this case, an LLM) in effectively querying a triplestore. We report our
results, which show that there are impacts from both approaches, and we discuss
their impact and implications.

</details>


### [85] [LLM-Stackelberg Games: Conjectural Reasoning Equilibria and Their Applications to Spearphishing](https://arxiv.org/abs/2507.09407)
*Quanyan Zhu*

Main category: cs.AI

TL;DR: LLM-Stackelberg游戏框架将大型语言模型（LLM）融入领导者与追随者的战略互动中，通过结构化提示和概率行为生成建模非完全信息与有限理性。


<details>
  <summary>Details</summary>
Motivation: 传统Stackelberg博弈假设完全信息和理性代理，而现实中的决策往往涉及有限理性和信息不对称。LLM的引入为建模此类复杂互动提供了新工具。

Method: 提出两种均衡概念：推理与行为均衡（内部推理与行为一致）和推测推理均衡（考虑对手响应的认知不确定性）。通过结构化提示和LLM生成行为。

Result: 在钓鱼攻击案例中展示了LLM-Stackelberg框架的认知丰富性和对抗潜力，适用于网络安全、错误信息等领域。

Conclusion: LLM-Stackelberg博弈为复杂决策领域提供了强大的建模范式，尤其在信息不对称和有限理性场景中表现突出。

Abstract: We introduce the framework of LLM-Stackelberg games, a class of sequential
decision-making models that integrate large language models (LLMs) into
strategic interactions between a leader and a follower. Departing from
classical Stackelberg assumptions of complete information and rational agents,
our formulation allows each agent to reason through structured prompts,
generate probabilistic behaviors via LLMs, and adapt their strategies through
internal cognition and belief updates. We define two equilibrium concepts:
reasoning and behavioral equilibrium, which aligns an agent's internal
prompt-based reasoning with observable behavior, and conjectural reasoning
equilibrium, which accounts for epistemic uncertainty through parameterized
models over an opponent's response. These layered constructs capture bounded
rationality, asymmetric information, and meta-cognitive adaptation. We
illustrate the framework through a spearphishing case study, where a sender and
a recipient engage in a deception game using structured reasoning prompts. This
example highlights the cognitive richness and adversarial potential of
LLM-mediated interactions. Our results show that LLM-Stackelberg games provide
a powerful paradigm for modeling decision-making in domains such as
cybersecurity, misinformation, and recommendation systems.

</details>


### [86] [GenAI-based Multi-Agent Reinforcement Learning towards Distributed Agent Intelligence: A Generative-RL Agent Perspective](https://arxiv.org/abs/2507.09495)
*Hang Wang,Junshan Zhang*

Main category: cs.AI

TL;DR: 论文提出从被动到主动的多智能体强化学习范式转变，利用生成式AI实现前瞻性决策和协作。


<details>
  <summary>Details</summary>
Motivation: 传统方法难以解决多智能体强化学习中的联合动作空间爆炸、非平稳环境和部分可观测性问题，需要新的范式。

Method: 采用生成式AI的强化学习，将智能体视为生成模型，预测未来交互并生成协调动作序列。

Result: 生成式AI强化学习能实现前瞻性决策、无缝协作和动态适应，解决传统框架难以处理的协调问题。

Conclusion: 该范式转变有望推动分布式智能发展，实现真正的协作智能，应用于自主系统、机器人和人机协作等领域。

Abstract: Multi-agent reinforcement learning faces fundamental challenges that
conventional approaches have failed to overcome: exponentially growing joint
action spaces, non-stationary environments where simultaneous learning creates
moving targets, and partial observability that constrains coordination. Current
methods remain reactive, employing stimulus-response mechanisms that fail when
facing novel scenarios. We argue for a transformative paradigm shift from
reactive to proactive multi-agent intelligence through generative AI-based
reinforcement learning. This position advocates reconceptualizing agents not as
isolated policy optimizers, but as sophisticated generative models capable of
synthesizing complex multi-agent dynamics and making anticipatory decisions
based on predictive understanding of future interactions. Rather than
responding to immediate observations, generative-RL agents can model
environment evolution, predict other agents' behaviors, generate coordinated
action sequences, and engage in strategic reasoning accounting for long-term
dynamics. This approach leverages pattern recognition and generation
capabilities of generative AI to enable proactive decision-making, seamless
coordination through enhanced communication, and dynamic adaptation to evolving
scenarios. We envision this paradigm shift will unlock unprecedented
possibilities for distributed intelligence, moving beyond individual
optimization toward emergent collective behaviors representing genuine
collaborative intelligence. The implications extend across autonomous systems,
robotics, and human-AI collaboration, promising solutions to coordination
challenges intractable under traditional reactive frameworks.

</details>


### [87] [Consistency Trajectory Planning: High-Quality and Efficient Trajectory Optimization for Offline Model-Based Reinforcement Learning](https://arxiv.org/abs/2507.09534)
*Guanquan Wang,Takuya Hiraoka,Yoshimasa Tsuruoka*

Main category: cs.AI

TL;DR: CTP是一种基于一致性轨迹模型的离线强化学习方法，通过单步轨迹生成实现高效优化，显著减少计算成本。


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型在规划中性能强但计算成本高，CTP旨在解决这一问题。

Method: 利用一致性轨迹模型（CTM）进行单步轨迹生成，避免迭代采样。

Result: 在D4RL基准测试中，CTP在长时程任务中表现优于现有方法，且推理速度提升120倍。

Conclusion: CTP在高效性和低延迟方面表现出色，适用于高性能离线规划。

Abstract: This paper introduces Consistency Trajectory Planning (CTP), a novel offline
model-based reinforcement learning method that leverages the recently proposed
Consistency Trajectory Model (CTM) for efficient trajectory optimization. While
prior work applying diffusion models to planning has demonstrated strong
performance, it often suffers from high computational costs due to iterative
sampling procedures. CTP supports fast, single-step trajectory generation
without significant degradation in policy quality. We evaluate CTP on the D4RL
benchmark and show that it consistently outperforms existing diffusion-based
planning methods in long-horizon, goal-conditioned tasks. Notably, CTP achieves
higher normalized returns while using significantly fewer denoising steps. In
particular, CTP achieves comparable performance with over $120\times$ speedup
in inference time, demonstrating its practicality and effectiveness for
high-performance, low-latency offline planning.

</details>


### [88] [Learning to Control Dynamical Agents via Spiking Neural Networks and Metropolis-Hastings Sampling](https://arxiv.org/abs/2507.09540)
*Ali Safa,Farida Mohsen,Ali Al-Zawqari*

Main category: cs.AI

TL;DR: 提出了一种基于Metropolis-Hastings采样的框架，用于训练脉冲神经网络（SNN）在强化学习任务中，无需依赖梯度方法。


<details>
  <summary>Details</summary>
Motivation: 解决SNN在强化学习中因脉冲通信不可微分带来的训练挑战。

Method: 使用Metropolis-Hastings采样迭代提出并概率接受参数更新，基于累积奖励信号。

Result: 在AcroBot和CartPole任务中，优于传统Deep Q-Learning和现有SNN方法。

Conclusion: 该框架有效规避了反向传播的限制，可直接优化神经形态平台。

Abstract: Spiking Neural Networks (SNNs) offer biologically inspired, energy-efficient
alternatives to traditional Deep Neural Networks (DNNs) for real-time control
systems. However, their training presents several challenges, particularly for
reinforcement learning (RL) tasks, due to the non-differentiable nature of
spike-based communication. In this work, we introduce what is, to our
knowledge, the first framework that employs Metropolis-Hastings (MH) sampling,
a Bayesian inference technique, to train SNNs for dynamical agent control in RL
environments without relying on gradient-based methods. Our approach
iteratively proposes and probabilistically accepts network parameter updates
based on accumulated reward signals, effectively circumventing the limitations
of backpropagation while enabling direct optimization on neuromorphic
platforms. We evaluated this framework on two standard control benchmarks:
AcroBot and CartPole. The results demonstrate that our MH-based approach
outperforms conventional Deep Q-Learning (DQL) baselines and prior SNN-based RL
approaches in terms of maximizing the accumulated reward while minimizing
network resources and training episodes.

</details>


### [89] [eSapiens: A Platform for Secure and Auditable Retrieval-Augmented Generation](https://arxiv.org/abs/2507.09588)
*Isaac Shi,Zeyuan Li,Fan Liu,Wenli Wang,Lewei He,Yang Yang,Tianyu Shi*

Main category: cs.AI

TL;DR: eSapiens是一个AIaaS平台，专注于企业数据、工作流程和LLM集成，提供数据安全和自动化支持。


<details>
  <summary>Details</summary>
Motivation: 解决企业在AI应用中面临的数据安全和知识保留问题，同时提升工作效率。

Method: 结合结构化文档处理、混合向量检索和无代码编排（LangChain），支持多种主流LLM，并引入THOR Agent处理SQL查询。

Result: 实验显示，512令牌分块检索精度最高（Top-3准确率91.3%），生成质量在TRACe指标下提升23%。

Conclusion: eSapiens在高风险领域（如法律和金融）中实现了可信、可审计的AI工作流程。

Abstract: We present eSapiens, an AI-as-a-Service (AIaaS) platform engineered around a
business-oriented trifecta: proprietary data, operational workflows, and any
major agnostic Large Language Model (LLM). eSapiens gives businesses full
control over their AI assets, keeping everything in-house for AI knowledge
retention and data security. eSapiens AI Agents (Sapiens) empower your team by
providing valuable insights and automating repetitive tasks, enabling them to
focus on high-impact work and drive better business outcomes.
  The system integrates structured document ingestion, hybrid vector retrieval,
and no-code orchestration via LangChain, and supports top LLMs including
OpenAI, Claude, Gemini, and DeepSeek. A key component is the THOR Agent, which
handles structured SQL-style queries and generates actionable insights over
enterprise databases.
  To evaluate the system, we conduct two experiments. First, a retrieval
benchmark on legal corpora reveals that a chunk size of 512 tokens yields the
highest retrieval precision (Top-3 accuracy: 91.3%). Second, a generation
quality test using TRACe metrics across five LLMs shows that eSapiens delivers
more context-consistent outputs with up to a 23% improvement in factual
alignment.
  These results demonstrate the effectiveness of eSapiens in enabling
trustworthy, auditable AI workflows for high-stakes domains like legal and
finance.

</details>


### [90] [The Hidden Costs of AI: A Review of Energy, E-Waste, and Inequality in Model Development](https://arxiv.org/abs/2507.09611)
*Jenis Winsta*

Main category: cs.AI

TL;DR: 本文综述了AI快速发展中常被忽视的环境与伦理挑战，包括能源消耗、电子废物、计算资源不平等及网络安全系统的隐性能源负担。


<details>
  <summary>Details</summary>
Motivation: 探讨AI在性能之外的影响，揭示其环境与伦理问题，推动负责任AI的发展。

Method: 通过分析近期研究和机构报告，系统梳理了AI在能源排放、硬件更新、全球基础设施差异及网络安全能源需求等方面的问题。

Result: 揭示了AI发展中的系统性挑战，如高排放、硬件快速淘汰、资源分配不均及安全能源负担。

Conclusion: 呼吁AI进步需与伦理责任和环境保护结合，推动可持续、透明、公平的技术发展。

Abstract: Artificial intelligence (AI) has made remarkable progress in recent years,
yet its rapid expansion brings overlooked environmental and ethical challenges.
This review explores four critical areas where AI's impact extends beyond
performance: energy consumption, electronic waste (e-waste), inequality in
compute access, and the hidden energy burden of cybersecurity systems. Drawing
from recent studies and institutional reports, the paper highlights systemic
issues such as high emissions from model training, rising hardware turnover,
global infrastructure disparities, and the energy demands of securing AI. By
connecting these concerns, the review contributes to Responsible AI discourse
by identifying key research gaps and advocating for sustainable, transparent,
and equitable development practices. Ultimately, it argues that AI's progress
must align with ethical responsibility and environmental stewardship to ensure
a more inclusive and sustainable technological future.

</details>


### [91] [Bridging Bots: from Perception to Action via Multimodal-LMs and Knowledge Graphs](https://arxiv.org/abs/2507.09617)
*Margherita Martorana,Francesca Urgese,Mark Adamik,Ilaria Tiddi*

Main category: cs.AI

TL;DR: 提出了一种结合多模态语言模型和知识图谱的神经符号框架，以支持机器人应用的互操作性。


<details>
  <summary>Details</summary>
Motivation: 解决当前机器人系统依赖专有硬编码方案导致的难以适应和扩展的问题，同时结合符号系统的结构化表示和多模态模型的感知能力。

Method: 提出神经符号框架，整合机器人感知数据、本体和多模态模型（LLaMA和GPT），生成符合本体的知识图谱。

Result: GPT-o1和LLaMA 4 Maverick表现最佳，但新模型不一定更好，集成策略对生成符合本体的知识图谱至关重要。

Conclusion: 神经符号框架有效支持机器人互操作性，但模型选择和集成策略是关键。

Abstract: Personal service robots are deployed to support daily living in domestic
environments, particularly for elderly and individuals requiring assistance.
These robots must perceive complex and dynamic surroundings, understand tasks,
and execute context-appropriate actions. However, current systems rely on
proprietary, hard-coded solutions tied to specific hardware and software,
resulting in siloed implementations that are difficult to adapt and scale
across platforms. Ontologies and Knowledge Graphs (KGs) offer a solution to
enable interoperability across systems, through structured and standardized
representations of knowledge and reasoning. However, symbolic systems such as
KGs and ontologies struggle with raw and noisy sensory input. In contrast,
multimodal language models are well suited for interpreting input such as
images and natural language, but often lack transparency, consistency, and
knowledge grounding. In this work, we propose a neurosymbolic framework that
combines the perceptual strengths of multimodal language models with the
structured representations provided by KGs and ontologies, with the aim of
supporting interoperability in robotic applications. Our approach generates
ontology-compliant KGs that can inform robot behavior in a platform-independent
manner. We evaluated this framework by integrating robot perception data,
ontologies, and five multimodal models (three LLaMA and two GPT models), using
different modes of neural-symbolic interaction. We assess the consistency and
effectiveness of the generated KGs across multiple runs and configurations, and
perform statistical analyzes to evaluate performance. Results show that GPT-o1
and LLaMA 4 Maverick consistently outperform other models. However, our
findings also indicate that newer models do not guarantee better results,
highlighting the critical role of the integration strategy in generating
ontology-compliant KGs.

</details>


### [92] [humancompatible.interconnect: Testing Properties of Repeated Uses of Interconnections of AI Systems](https://arxiv.org/abs/2507.09626)
*Rodion Nazarov,Anthony Quinn,Robert Shorten,Jakub Marecek*

Main category: cs.AI

TL;DR: 介绍了一个基于PyTorch的工具包，用于通过随机控制技术建模多代理AI系统的交互，并提供公平性和鲁棒性的先验保证。


<details>
  <summary>Details</summary>
Motivation: 多代理AI系统需要满足公平性和鲁棒性的先验保证，但现有方法复杂且难以实现。

Method: 开发了一个基于PyTorch的工具包，采用闭环方式建模公平性和鲁棒性需求。

Result: 工具包简化了多代理系统闭环模型的公平性保证实现。

Conclusion: 该工具包为多代理AI系统的公平性和鲁棒性提供了有效解决方案。

Abstract: Artificial intelligence (AI) systems often interact with multiple agents. The
regulation of such AI systems often requires that {\em a priori\/} guarantees
of fairness and robustness be satisfied. With stochastic models of agents'
responses to the outputs of AI systems, such {\em a priori\/} guarantees
require non-trivial reasoning about the corresponding stochastic systems. Here,
we present an open-source PyTorch-based toolkit for the use of stochastic
control techniques in modelling interconnections of AI systems and properties
of their repeated uses. It models robustness and fairness desiderata in a
closed-loop fashion, and provides {\em a priori\/} guarantees for these
interconnections. The PyTorch-based toolkit removes much of the complexity
associated with the provision of fairness guarantees for closed-loop models of
multi-agent systems.

</details>


### [93] [Towards Concise and Adaptive Thinking in Large Reasoning Models: A Survey](https://arxiv.org/abs/2507.09662)
*Jason Zhu,Hongyu Li*

Main category: cs.AI

TL;DR: 大型推理模型（LRMs）在复杂任务上表现出色，但生成长且冗余的推理链，浪费资源。本文综述了简洁和自适应推理的进展。


<details>
  <summary>Details</summary>
Motivation: 解决LRMs生成冗余推理链的问题，以提高效率和实用性。

Method: 综述了简洁和自适应推理的方法、基准和挑战。

Result: 总结了当前进展，为未来研究提供方向。

Conclusion: 希望帮助研究者快速了解该领域，并启发新的自适应推理方法。

Abstract: Large reasoning models (LRMs) like OpenAI o1 and DeepSeek R1 have
demonstrated impressive performance on complex reasoning tasks like mathematics
and programming with long Chain-of-Thought (CoT) reasoning sequences
(slow-thinking), compared with traditional large language models
(fast-thinking). However, these reasoning models also face a huge challenge
that generating unnecessarily lengthy and redundant reasoning chains even for
trivial questions. This phenomenon leads to a significant waste of inference
resources, increases the response time for simple queries, and hinders the
practical application of LRMs in real-world products. To this end, it is
crucial to shorten lengthy reasoning chains and learn adaptive reasoning
between fast and slow thinking based on input difficulty. In this survey, we
provide a comprehensive overview of recent progress in concise and adaptive
thinking for efficient reasoning of LRMs, including methodologies, benchmarks,
and challenges for future exploration. We hope this survey can help researchers
quickly understand the landscape of this field and inspire novel adaptive
thinking ideas to facilitate better usage of LRMs.

</details>


### [94] [Causality-informed Anomaly Detection in Partially Observable Sensor Networks: Moving beyond Correlations](https://arxiv.org/abs/2507.09742)
*Xiaofeng Xiao,Bo Shen,Xubo Yue*

Main category: cs.AI

TL;DR: 提出了一种基于因果关系的深度Q网络（Causal DQ）方法，用于部分可观测的传感器布局优化，以更快检测异常。


<details>
  <summary>Details</summary>
Motivation: AI驱动的制造业中，实时监控数据流的需求增长，但资源有限，无法在所有位置部署传感器。现有方法多忽略因果关系，或依赖不切实际的干预手段。

Method: 通过在每个Q网络训练阶段集成因果信息，开发了Causal DQ方法，实现更快收敛和更紧的理论误差界限。

Result: Causal DQ显著减少了异常检测时间，适用于大规模实时数据流。

Conclusion: 该方法不仅适用于传感器布局，还可推广到其他强化学习问题，为工程应用中的因果机器学习提供新思路。

Abstract: Nowadays, as AI-driven manufacturing becomes increasingly popular, the volume
of data streams requiring real-time monitoring continues to grow. However, due
to limited resources, it is impractical to place sensors at every location to
detect unexpected shifts. Therefore, it is necessary to develop an optimal
sensor placement strategy that enables partial observability of the system
while detecting anomalies as quickly as possible. Numerous approaches have been
proposed to address this challenge; however, most existing methods consider
only variable correlations and neglect a crucial factor: Causality. Moreover,
although a few techniques incorporate causal analysis, they rely on
interventions-artificially creating anomalies-to identify causal effects, which
is impractical and might lead to catastrophic losses. In this paper, we
introduce a causality-informed deep Q-network (Causal DQ) approach for
partially observable sensor placement in anomaly detection. By integrating
causal information at each stage of Q-network training, our method achieves
faster convergence and tighter theoretical error bounds. Furthermore, the
trained causal-informed Q-network significantly reduces the detection time for
anomalies under various settings, demonstrating its effectiveness for sensor
placement in large-scale, real-world data streams. Beyond the current
implementation, our technique's fundamental insights can be applied to various
reinforcement learning problems, opening up new possibilities for real-world
causality-informed machine learning methods in engineering applications.

</details>


### [95] [Sound and Complete Neuro-symbolic Reasoning with LLM-Grounded Interpretations](https://arxiv.org/abs/2507.09751)
*Bradley P. Allen,Prateek Chhikara,Thomas Macaulay Ferguson,Filip Ilievski,Paul Groth*

Main category: cs.AI

TL;DR: 提出了一种将大语言模型（LLM）整合到形式语义解释函数中的方法，以解决其逻辑不一致性问题。


<details>
  <summary>Details</summary>
Motivation: 利用LLM的广泛参数知识进行形式推理，同时解决其输出逻辑不一致的问题。

Method: 将LLM直接整合到形式语义的解释函数中，基于一种非经典逻辑（paraconsistent logic）。

Result: 实验证明该方法在多个事实性基准数据集上可行，且保持了逻辑的健全性和完备性。

Conclusion: 该方法为神经符号推理提供了一个理论框架，既能利用LLM的知识，又能保持逻辑的严谨性。

Abstract: Large language models (LLMs) have demonstrated impressive capabilities in
natural language understanding and generation, but they exhibit problems with
logical consistency in the output they generate. How can we harness LLMs'
broad-coverage parametric knowledge in formal reasoning despite their
inconsistency? We present a method for directly integrating an LLM into the
interpretation function of the formal semantics for a paraconsistent logic. We
provide experimental evidence for the feasibility of the method by evaluating
the function using datasets created from several short-form factuality
benchmarks. Unlike prior work, our method offers a theoretical framework for
neuro-symbolic reasoning that leverages an LLM's knowledge while preserving the
underlying logic's soundness and completeness properties.

</details>


### [96] [Technical Requirements for Halting Dangerous AI Activities](https://arxiv.org/abs/2507.09801)
*Peter Barnett,Aaron Scher,David Abecassis*

Main category: cs.AI

TL;DR: 论文探讨了AI快速发展带来的风险，并提出技术干预措施以实现对危险AI活动的协调暂停。


<details>
  <summary>Details</summary>
Motivation: AI系统的快速发展带来了失控、滥用、地缘政治不稳定和权力集中等前所未有的风险，需要政府采取行动以避免最坏结果。

Method: 提出了关键的技术干预措施，以协调暂停危险的AI开发和部署。

Result: 这些干预措施可以限制多种危险AI活动，并为潜在的AI治理计划提供技术基础。

Conclusion: 通过技术干预措施，政府可以建立对危险AI活动的协调暂停能力，为AI治理提供支持。

Abstract: The rapid development of AI systems poses unprecedented risks, including loss
of control, misuse, geopolitical instability, and concentration of power. To
navigate these risks and avoid worst-case outcomes, governments may proactively
establish the capability for a coordinated halt on dangerous AI development and
deployment. In this paper, we outline key technical interventions that could
allow for a coordinated halt on dangerous AI activities. We discuss how these
interventions may contribute to restricting various dangerous AI activities,
and show how these interventions can form the technical foundation for
potential AI governance plans.

</details>


### [97] [Is Human-Written Data Enough? The Challenge of Teaching Reasoning to LLMs Without RL or Distillation](https://arxiv.org/abs/2507.09850)
*Wei Du,Branislav Kisacanin,George Armstrong,Shubham Toshniwal,Ivan Moshkov,Alexan Ayrapetyan,Sadegh Mahdavi,Dan Zhao,Shizhe Diao,Dragan Masulovic,Marius Stanean,Advaith Avadhanam,Max Wang,Ashmit Dutta,Shitij Govil,Sri Yanamandara,Mihir Tandon,Sriram Ananthakrishnan,Vedant Rathi,David Zhang,Joonseok Kang,Leon Luo,Titu Andreescu,Boris Ginsburg,Igor Gitman*

Main category: cs.AI

TL;DR: 通过少量高质量的长链思维（CoT）示例微调基础模型，可显著提升其推理能力，甚至超越更大规模的模型。研究发现，专家CoT的潜在特性难以复制，但精心设计的人类编写CoT数据仍可能激活基础模型的推理行为。


<details>
  <summary>Details</summary>
Motivation: 探索是否仅通过提示或少量微调即可在基础模型中诱导长链思维推理能力，并研究不同来源的CoT数据对推理能力的影响。

Method: 使用20个来自推理模型的长CoT示例微调基础模型，并尝试利用非推理模型和人类标注的CoT数据，结合提示工程和多轮编辑。

Result: 微调后的模型性能超越更大规模的模型，但非推理模型和人类标注的CoT数据效果不及推理模型的数据。

Conclusion: 少量高质量的专家CoT数据可显著提升基础模型的推理能力，但复制其特性仍具挑战。研究呼吁进一步探索小规模推理监督的有效性。

Abstract: Reasoning-capable language models achieve state-of-the-art performance in
diverse complex tasks by generating long, explicit Chain-of-Thought (CoT)
traces. While recent works show that base models can acquire such reasoning
traces via reinforcement learning or distillation from stronger models like
DeepSeek-R1, previous works demonstrate that even short CoT prompting without
fine-tuning is able to improve reasoning. We ask whether long CoT can be
induced in a base model using only prompting or minimal tuning. Using just 20
long CoT examples from the reasoning model \texttt{QwQ-32B-Preview}, we lightly
fine-tune the base model \texttt{Qwen2.5-32B}. The resulting model outperforms
the much larger \texttt{Qwen2.5-Math-72B-Instruct}, showing that a handful of
high-quality examples can unlock strong reasoning capabilities. We further
explore using CoT data from non-reasoning models and human annotators, enhanced
with prompt engineering, multi-pass editing, and structural guidance. However,
neither matches the performance of reasoning model traces, suggesting that
certain latent qualities of expert CoT are difficult to replicate. We analyze
key properties of reasoning data, such as problem difficulty, diversity, and
answer length, that influence reasoning distillation. While challenges remain,
we are optimistic that carefully curated human-written CoT, even in small
quantities, can activate reasoning behaviors in base models. We release our
human-authored dataset across refinement stages and invite further
investigation into what makes small-scale reasoning supervision so effective.

</details>


### [98] [Model-Grounded Symbolic Artificial Intelligence Systems Learning and Reasoning with Model-Grounded Symbolic Artificial Intelligence Systems](https://arxiv.org/abs/2507.09854)
*Aniruddha Chattopadhyay,Raj Dandekar,Kaushik Roy*

Main category: cs.AI

TL;DR: 将指令调优的大型语言模型重新解释为基于模型的符号AI系统，利用自然语言作为符号层，通过模型内部表示空间实现接地。


<details>
  <summary>Details</summary>
Motivation: 结合神经网络的泛化学习能力和符号AI的可靠推理能力，探索新的学习和推理方法。

Method: 将自然语言作为符号层，利用模型内部表示空间实现接地，开发与传统范式结构相似的学习和推理方法。

Result: 初步评估显示，该方法在提高学习效率和推理可靠性方面有效。

Conclusion: 该框架为神经符号AI提供了一种新的实现方式，展示了其在复杂推理任务中的潜力。

Abstract: Neurosymbolic artificial intelligence (AI) systems combine neural network and
classical symbolic AI mechanisms to exploit the complementary strengths of
large scale, generalizable learning and robust, verifiable reasoning. Numerous
classifications of neurosymbolic AI illustrate how these two components can be
integrated in distinctly different ways. In this work, we propose
reinterpreting instruction tuned large language models as model grounded
symbolic AI systems where natural language serves as the symbolic layer and
grounding is achieved through the models internal representation space. Within
this framework, we investigate and develop novel learning and reasoning
approaches that preserve structural similarities to traditional learning and
reasoning paradigms. Preliminary evaluations across axiomatic deductive
reasoning procedures of varying complexity provide insights into the
effectiveness of our approach in improving learning efficiency and reasoning
reliability.

</details>


### [99] [VerifyBench: A Systematic Benchmark for Evaluating Reasoning Verifiers Across Domains](https://arxiv.org/abs/2507.09884)
*Xuzhao Li,Xuchen Li,Shiyu Hu,Yongzhen Guo,Wentao Zhang*

Main category: cs.AI

TL;DR: 论文提出了VerifyBench，一个跨领域综合基准，用于系统评估验证器性能，揭示了专用验证器和通用LLM在准确性、召回率和跨领域泛化能力上的权衡。


<details>
  <summary>Details</summary>
Motivation: 现有验证器在复杂性和灵活性上存在不足，缺乏系统评估，限制了RLVR的可靠发展。

Method: 构建包含4,000个专家级问题的VerifyBench，设计四维实验框架，比较专用验证器和通用LLM的性能。

Result: 专用验证器准确率高但召回率低，通用模型包容性强但精度不稳定，验证器对输入结构敏感且跨领域泛化能力有限。

Conclusion: 验证器技术存在瓶颈，需进一步优化以提高可靠性和泛化能力。

Abstract: Large language models (LLMs) increasingly rely on reinforcement learning (RL)
to enhance their reasoning capabilities through feedback. A critical challenge
is verifying the consistency of model-generated responses and reference
answers, since these responses are often lengthy, diverse, and nuanced.
Rule-based verifiers struggle with complexity, prompting the use of model-based
verifiers. However, specialized verifiers lack flexibility, while general LLM
judges can be inconsistent. Existing research primarily focuses on building
better verifiers, yet a systematic evaluation of different types of verifiers'
performance across domains remains lacking, severely constraining the reliable
development of Reinforcement Learning with Verifiable Reward (RLVR). To address
this, we propose VerifyBench--a cross-domain comprehensive benchmark for
systematically evaluating verifiers. We construct 4,000 expert-level questions
covering mathematics, physics, chemistry, and biology. Each question is
equipped with reference answers and diverse responses. The reliability of the
evaluation is ensured through a rigorous annotation process conducted by a
multidisciplinary expert team. We design a four-dimensional experimental
framework to comprehensively compare the performance boundaries of specialized
verifiers and general LLMs under combined conditions of extracted answers vs.
complete responses, and short vs. long outputs. Our evaluation uncovers
fundamental trade-offs in verifiers: while specialized verifiers achieve
leading accuracy, they exhibit deficiencies in recall; general models show
stronger inclusivity but unstable precision. More importantly, we discover
verifiers' high sensitivity to input structure and inherent limitations in
cross-domain generalization, providing critical insights into the bottlenecks
of current verifier technology.

</details>


### [100] [DeepSeek: Paradigm Shifts and Technical Evolution in Large AI Models](https://arxiv.org/abs/2507.09955)
*Luolin Xiong,Haofen Wang,Xi Chen,Lu Sheng,Yun Xiong,Jingping Liu,Yanghua Xiao,Huajun Chen,Qing-Long Han,Yang Tang*

Main category: cs.AI

TL;DR: DeepSeek发布V3和R1系列模型，低成本高性能且开源，论文回顾了大模型发展、DeepSeek的创新算法及工程突破，并探讨其对AI竞争格局的影响和未来趋势。


<details>
  <summary>Details</summary>
Motivation: 分析DeepSeek模型的创新及其对AI领域的影响，探讨大模型技术和工程的未来发展方向。

Method: 回顾大模型发展，介绍DeepSeek的MLA、MoE、MTP和GRPO算法，分析其工程优化和系统架构。

Result: DeepSeek模型在性能、成本和开源方面具有竞争力，推动了AI技术和工程的发展。

Conclusion: DeepSeek的创新为AI大模型发展提供了新思路，未来需关注数据、训练和推理的优化。

Abstract: DeepSeek, a Chinese Artificial Intelligence (AI) startup, has released their
V3 and R1 series models, which attracted global attention due to their low
cost, high performance, and open-source advantages. This paper begins by
reviewing the evolution of large AI models focusing on paradigm shifts, the
mainstream Large Language Model (LLM) paradigm, and the DeepSeek paradigm.
Subsequently, the paper highlights novel algorithms introduced by DeepSeek,
including Multi-head Latent Attention (MLA), Mixture-of-Experts (MoE),
Multi-Token Prediction (MTP), and Group Relative Policy Optimization (GRPO).
The paper then explores DeepSeek engineering breakthroughs in LLM scaling,
training, inference, and system-level optimization architecture. Moreover, the
impact of DeepSeek models on the competitive AI landscape is analyzed,
comparing them to mainstream LLMs across various fields. Finally, the paper
reflects on the insights gained from DeepSeek innovations and discusses future
trends in the technical and engineering development of large AI models,
particularly in data, training, and reasoning.

</details>


### [101] [Improving monotonic optimization in heterogeneous multi-agent reinforcement learning with optimal marginal deterministic policy gradient](https://arxiv.org/abs/2507.09989)
*Xiaoyang Yu,Youfang Lin,Shuo Wang,Sheng Han*

Main category: cs.AI

TL;DR: OMDPG算法通过引入最优边际Q函数和广义Q批评器，解决了异构多智能体强化学习中单调改进与部分参数共享的冲突，并在实验中表现优异。


<details>
  <summary>Details</summary>
Motivation: 异构多智能体强化学习中，单调改进与部分参数共享之间存在冲突，现有方法无法同时满足两者。

Method: 提出OMDPG算法，使用最优边际Q函数替代顺序计算的Q函数，引入广义Q批评器，并采用集中式批评器分组执行器架构。

Result: 在SMAC和MAMuJoCo环境中，OMDPG优于多种最先进的多智能体强化学习基线。

Conclusion: OMDPG成功解决了单调改进与部分参数共享的冲突，提升了异构多智能体强化学习的性能。

Abstract: In heterogeneous multi-agent reinforcement learning (MARL), achieving
monotonic improvement plays a pivotal role in enhancing performance. The HAPPO
algorithm proposes a feasible solution by introducing a sequential update
scheme, which requires independent learning with No Parameter-sharing (NoPS).
However, heterogeneous MARL generally requires Partial Parameter-sharing
(ParPS) based on agent grouping to achieve high cooperative performance. Our
experiments prove that directly combining ParPS with the sequential update
scheme leads to the policy updating baseline drift problem, thereby failing to
achieve improvement. To solve the conflict between monotonic improvement and
ParPS, we propose the Optimal Marginal Deterministic Policy Gradient (OMDPG)
algorithm. First, we replace the sequentially computed $Q_{\psi}^s(s,a_{1:i})$
with the Optimal Marginal Q (OMQ) function $\phi_{\psi}^*(s,a_{1:i})$ derived
from Q-functions. This maintains MAAD's monotonic improvement while eliminating
the conflict through optimal joint action sequences instead of sequential
policy ratio calculations. Second, we introduce the Generalized Q Critic (GQC)
as the critic function, employing pessimistic uncertainty-constrained loss to
optimize different Q-value estimations. This provides the required Q-values for
OMQ computation and stable baselines for actor updates. Finally, we implement a
Centralized Critic Grouped Actor (CCGA) architecture that simultaneously
achieves ParPS in local policy networks and accurate global Q-function
computation. Experimental results in SMAC and MAMuJoCo environments demonstrate
that OMDPG outperforms various state-of-the-art MARL baselines.

</details>


### [102] [On The Role of Intentionality in Knowledge Representation: Analyzing Scene Context for Cognitive Agents with a Tiny Language Model](https://arxiv.org/abs/2507.10000)
*Mark Burgess*

Main category: cs.AI

TL;DR: 论文提出了一种基于Promise Theory的语义时空模型，通过多尺度异常和过程一致性来评估数据中的潜在意图性，无需复杂计算或训练。


<details>
  <summary>Details</summary>
Motivation: 探讨意图和意图性在科学与技术中的实际意义，弥补Searle之后相关研究的不足。

Method: 利用Promise Theory的语义时空模型，通过多尺度异常和过程一致性分析数据中的意图性，分离出‘意图内容’和‘环境上下文’。

Result: 提供了一种低计算成本的意图性解释方法，适用于基础生物体，无需大规模人工概率处理。

Conclusion: 该方法为理解潜在意图性提供了实用工具，但其概念形成能力受限于代理的记忆容量。

Abstract: Since Searle's work deconstructing intent and intentionality in the realm of
philosophy, the practical meaning of intent has received little attention in
science and technology. Intentionality and context are both central to the
scope of Promise Theory's model of Semantic Spacetime, used as an effective
Tiny Language Model. One can identify themes and concepts from a text, on a low
level (without knowledge of the specific language) by using process coherence
as a guide. Any agent process can assess superficially a degree of latent
`intentionality' in data by looking for anomalous multi-scale anomalies and
assessing the work done to form them. Scale separation can be used to sort
parts into `intended' content and `ambient context', using the spacetime
coherence as a measure. This offers an elementary but pragmatic interpretation
of latent intentionality for very low computational cost, and without reference
to extensive training or reasoning capabilities. The process is well within the
reach of basic organisms as it does not require large scale artificial
probabilistic batch processing. The level of concept formation depends,
however, on the memory capacity of the agent.

</details>


### [103] [Deep Hidden Cognition Facilitates Reliable Chain-of-Thought Reasoning](https://arxiv.org/abs/2507.10007)
*Zijun Chen,Wenbo Hu,Richang Hong*

Main category: cs.AI

TL;DR: 论文提出了一种通过模型内在的真实性编码来校准CoT推理准确性的新方法，显著提升了推理的可靠性和准确性。


<details>
  <summary>Details</summary>
Motivation: CoT推理在LLMs和MLLMs中表现出强大的深度推理能力，但中间步骤的错误累积会降低其可靠性。

Method: 利用特定注意力头激活反映CoT推理步骤的真实性，训练置信度预测器动态选择最优推理路径。

Result: 在数学、符号和常识推理任务中，该方法显著优于现有基线，展示了更高的准确性和可靠性。

Conclusion: 该方法为CoT推理提供了一种新颖的可靠性改进路径，具有广泛的应用潜力。

Abstract: Chain of Thought (CoT) reasoning has demonstrated remarkable deep reasoning
capabilities in both large language models (LLMs) and multimodal large language
models (MLLMs). However, its reliability is often undermined by the
accumulation of errors in intermediate steps. This paper introduces an novel
approach to calibrate the CoT reasoning accuracy by leveraging the model's
intrinsic veracity encoding. We discover that specific attention head
activations reliably reflect the truthfulness of reasoning steps in CoT. Based
on this insight, we train a confidence predictor to evaluate the correctness of
each reasoning step using these truthfulness-sensitive activations, dynamically
selecting the most plausible reasoning path via beam search. Experimental
results demonstrate that our method significantly outperforms the
state-of-the-art baselines (e.g., Few-Shot CoT, Self-Consistency, and
Self-Evaluation Guided Beam Search) across the mathematical, symbolic, and
commonsense reasoning tasks, exhibiting superior accuracy and reliability in
both unimodal and multimodal settings. We further validate the approach on
large reasoning models, confirming its applicability to specialized reasoning
models. Additionally, we explore the role of the model's self-correction
ability in CoT reasoning. This work provides a novel reliability improvement
path for CoT reasoning with broad application potential.

</details>


### [104] [Automating SPARQL Query Translations between DBpedia and Wikidata](https://arxiv.org/abs/2507.10045)
*Malte Christian Bartels,Debayan Banerjee,Ricardo Usbeck*

Main category: cs.AI

TL;DR: 研究评估了大型语言模型（LLM）在不同知识图谱（KG）间自动翻译SPARQL查询的能力，发现性能因模型和提示策略而异。


<details>
  <summary>Details</summary>
Motivation: 填补知识图谱互操作性研究中SPARQL到SPARQL翻译的空白。

Method: 使用三种LLM（Llama-3-8B、DeepSeek-R1-Distill-Llama-70B、Mistral-Large-Instruct-2407），采用零样本、少样本和思维链变体策略，在两个基准上测试翻译性能。

Result: 模型和提示策略对性能影响显著，Wikidata到DBpedia的翻译效果优于反向。

Conclusion: LLM在SPARQL翻译中表现不一，需进一步优化模型和策略。

Abstract: This paper investigates whether state-of-the-art Large Language Models (LLMs)
can automatically translate SPARQL between popular Knowledge Graph (KG)
schemas. We focus on translations between the DBpedia and Wikidata KG, and
later on DBLP and OpenAlex KG. This study addresses a notable gap in KG
interoperability research by rigorously evaluating LLM performance on
SPARQL-to-SPARQL translation. Two benchmarks are assembled, where the first
align 100 DBpedia-Wikidata queries from QALD-9-Plus; the second contains 100
DBLP queries aligned to OpenAlex, testing generalizability beyond encyclopaedic
KGs. Three open LLMs: Llama-3-8B, DeepSeek-R1-Distill-Llama-70B, and
Mistral-Large-Instruct-2407 are selected based on their sizes and architectures
and tested with zero-shot, few-shot, and two chain-of-thought variants. Outputs
were compared with gold answers, and resulting errors were categorized. We find
that the performance varies markedly across models and prompting strategies,
and that translations for Wikidata to DBpedia work far better than translations
for DBpedia to Wikidata.

</details>


### [105] [On Gradual Semantics for Assumption-Based Argumentation](https://arxiv.org/abs/2507.10076)
*Anna Rapberger,Fabrizio Russo,Antonio Rago,Francesca Toni*

Main category: cs.AI

TL;DR: 本文填补了假设基础论证（ABA）中渐进语义学的空白，提出了一种新的渐进语义学家族，用于为ABA框架中的核心组件（假设）赋予辩证强度。


<details>
  <summary>Details</summary>
Motivation: 渐进语义学在计算论证中是一种细粒度的替代方案，但在ABA框架中尚未得到充分研究，尽管ABA是一种流行的结构化论证形式。

Method: 通过将双极集基础论证框架作为ABA框架的抽象，并扩展QBAF的最先进模块化渐进语义学，提出新的渐进ABA语义学。

Result: 渐进ABA语义学满足平衡性和单调性等理想性质，并通过实验与基于论证的方法进行比较，评估收敛性。

Conclusion: 本文成功填补了ABA框架中渐进语义学的空白，并验证了新方法的有效性。

Abstract: In computational argumentation, gradual semantics are fine-grained
alternatives to extension-based and labelling-based semantics . They ascribe a
dialectical strength to (components of) arguments sanctioning their degree of
acceptability. Several gradual semantics have been studied for abstract,
bipolar and quantitative bipolar argumentation frameworks (QBAFs), as well as,
to a lesser extent, for some forms of structured argumentation. However, this
has not been the case for assumption-based argumentation (ABA), despite it
being a popular form of structured argumentation with several applications
where gradual semantics could be useful. In this paper, we fill this gap and
propose a family of novel gradual semantics for equipping assumptions, which
are the core components in ABA frameworks, with dialectical strengths. To do
so, we use bipolar set-based argumentation frameworks as an abstraction of
(potentially non-flat) ABA frameworks and generalise state-of-the-art modular
gradual semantics for QBAFs. We show that our gradual ABA semantics satisfy
suitable adaptations of desirable properties of gradual QBAF semantics, such as
balance and monotonicity. We also explore an argument-based approach that
leverages established QBAF modular semantics directly, and use it as baseline.
Finally, we conduct experiments with synthetic ABA frameworks to compare our
gradual ABA semantics with its argument-based counterpart and assess
convergence.

</details>


### [106] [BlueGlass: A Framework for Composite AI Safety](https://arxiv.org/abs/2507.10106)
*Harshal Nandigramwar,Syed Qutub,Kay-Ulrich Scholl*

Main category: cs.AI

TL;DR: BlueGlass框架提出了一种集成多种AI安全工具的统一基础设施，并通过视觉语言模型的三项安全分析验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 随着AI系统能力的提升和普及，确保其安全性至关重要，但现有工具无法单独提供全面保障，需要集成方法。

Method: 开发了BlueGlass框架，支持集成和组合多样化的安全工具，覆盖模型内部和输出。

Result: 通过三项分析（分布评估、层动态探测、稀疏自编码器）展示了框架的实用性。

Conclusion: 该研究为构建更稳健可靠的AI系统提供了基础架构和发现。

Abstract: As AI systems become increasingly capable and ubiquitous, ensuring the safety
of these systems is critical. However, existing safety tools often target
different aspects of model safety and cannot provide full assurance in
isolation, highlighting a need for integrated and composite methodologies. This
paper introduces BlueGlass, a framework designed to facilitate composite AI
safety workflows by providing a unified infrastructure enabling the integration
and composition of diverse safety tools that operate across model internals and
outputs. Furthermore, to demonstrate the utility of this framework, we present
three safety-oriented analyses on vision-language models for the task of object
detection: (1) distributional evaluation, revealing performance trade-offs and
potential failure modes across distributions; (2) probe-based analysis of layer
dynamics highlighting shared hierarchical learning via phase transition; and
(3) sparse autoencoders identifying interpretable concepts. More broadly, this
work contributes foundational infrastructure and findings for building more
robust and reliable AI systems.

</details>


### [107] [Analysis of AI Techniques for Orchestrating Edge-Cloud Application Migration](https://arxiv.org/abs/2507.10119)
*Sadig Gojayev,Ahmad Anaqreh,Carolina Fortuna*

Main category: cs.AI

TL;DR: 论文探讨了边缘-云系统中应用迁移的自动编排问题，通过MDP框架比较了AI规划和强化学习方法，并提出新的状态空间分类。


<details>
  <summary>Details</summary>
Motivation: 研究旨在提高边缘-云系统中应用迁移的服务质量（QoS）和成本效益，解决传统启发式方法的局限性。

Method: 基于马尔可夫决策过程（MDP），分析并比较了AI规划和强化学习方法，提出了一种新的状态空间分类。

Result: 研究比较了不同方法在解决类似汉诺塔问题的边缘-云应用迁移问题中的表现。

Conclusion: 论文为新兴计算连续环境中的应用迁移编排提供了技术参考和分类框架。

Abstract: Application migration in edge-cloud system enables high QoS and cost
effective service delivery. However, automatically orchestrating such migration
is typically solved with heuristic approaches. Starting from the Markov
Decision Process (MDP), in this paper, we identify, analyze and compare
selected state-of-the-art Artificial Intelligence (AI) planning and
Reinforcement Learning (RL) approaches for solving the class of edge-cloud
application migration problems that can be modeled as Towers of Hanoi (ToH)
problems. We introduce a new classification based on state space definition and
analyze the compared models also through this lense. The aim is to understand
available techniques capable of orchestrating such application migration in
emerging computing continuum environments.

</details>


### [108] [Could you be wrong: Debiasing LLMs using a metacognitive prompt for improving human decision making](https://arxiv.org/abs/2507.10124)
*Thomas T. Hills*

Main category: cs.AI

TL;DR: 论文探讨了利用人类心理学中的元认知提示（如“你可能是错的吗？”）来减少LLM中的偏见，展示了这种方法的有效性。


<details>
  <summary>Details</summary>
Motivation: 由于LLM仍在发展中，当前的偏见可能随时间变化，需要通用的去偏策略。人类决策中的去偏方法为LLM提供了借鉴。

Method: 采用元认知提示（如“你可能是错的吗？”）引导LLM反思其回答，揭示潜在偏见和矛盾信息。

Result: 元认知提示能有效让LLM识别自身偏见，并提供更全面的反思信息。

Conclusion: 人类心理学为LLM的提示工程提供了新思路，可借鉴其长期有效的决策改进方法。

Abstract: Identifying bias in LLMs is ongoing. Because they are still in development,
what is true today may be false tomorrow. We therefore need general strategies
for debiasing that will outlive current models. Strategies developed for
debiasing human decision making offer one promising approach as they
incorporate an LLM-style prompt intervention designed to bring latent knowledge
into awareness during decision making. LLMs trained on vast amounts of
information contain information about potential biases, counter-arguments, and
contradictory evidence, but that information may only be brought to bear if
prompted. Metacognitive prompts developed in the human decision making
literature are designed to achieve this, and as I demonstrate here, they show
promise with LLMs. The prompt I focus on here is "could you be wrong?"
Following an LLM response, this prompt leads LLMs to produce additional
information, including why they answered as they did, errors, biases,
contradictory evidence, and alternatives, none of which were apparent in their
initial response. Indeed, this metaknowledge often reveals that how LLMs and
users interpret prompts are not aligned. Here I demonstrate this prompt using a
set of questions taken from recent articles about LLM biases, including
implicit discriminatory biases and failures of metacognition. "Could you be
wrong" prompts the LLM to identify its own biases and produce cogent
metacognitive reflection. I also present another example involving convincing
but incomplete information, which is readily corrected by the metacognitive
prompt. In sum, this work argues that human psychology offers a new avenue for
prompt engineering, leveraging a long history of effective prompt-based
improvements to human decision making.

</details>


### [109] [FRSICL: LLM-Enabled In-Context Learning Flight Resource Allocation for Fresh Data Collection in UAV-Assisted Wildfire Monitoring](https://arxiv.org/abs/2507.10134)
*Yousef Emami,Hao Zhou,Miguel Gutierrez Gaitan,Kai Li,Luis Almeida*

Main category: cs.AI

TL;DR: 论文提出了一种基于LLM的在线飞行资源分配方案（FRSICL），用于无人机辅助的野火监测系统，以实时优化飞行控制和数据收集调度，从而最小化信息年龄（AoI）。


<details>
  <summary>Details</summary>
Motivation: 无人机在野火监测中至关重要，但现有的深度强化学习方法（DRL）存在采样效率低、仿真与现实的差距以及复杂训练等问题，不适合时间敏感的应用。

Method: FRSICL利用自然语言任务描述和环境反馈，动态生成数据收集计划和飞行速度控制，避免了DRL的复杂训练需求。

Result: 仿真结果表明，FRSICL在最小化AoI方面优于PPO和最近邻基线方法。

Conclusion: FRSICL为无人机野火监测提供了一种高效、动态的解决方案，克服了DRL的局限性。

Abstract: Unmanned Aerial Vehicles (UAVs) are vital for public safety, particularly in
wildfire monitoring, where early detection minimizes environmental impact. In
UAV-Assisted Wildfire Monitoring (UAWM) systems, joint optimization of sensor
transmission scheduling and velocity is critical for minimizing Age of
Information (AoI) from stale sensor data. Deep Reinforcement Learning (DRL) has
been used for such optimization; however, its limitations such as low sampling
efficiency, simulation-to-reality gaps, and complex training render it
unsuitable for time-critical applications like wildfire monitoring. This paper
introduces a new online Flight Resource Allocation scheme based on LLM-Enabled
In-Context Learning (FRSICL) to jointly optimize the UAV's flight control and
data collection schedule along the trajectory in real time, thereby
asymptotically minimizing the average AoI across ground sensors. In contrast to
DRL, FRSICL generates data collection schedules and controls velocity using
natural language task descriptions and feedback from the environment, enabling
dynamic decision-making without extensive retraining. Simulation results
confirm the effectiveness of the proposed FRSICL compared to Proximal Policy
Optimization (PPO) and Nearest-Neighbor baselines.

</details>


### [110] [Adaptability in Multi-Agent Reinforcement Learning: A Framework and Unified Review](https://arxiv.org/abs/2507.10142)
*Siyi Hu,Mohamad A Hady,Jianglin Qiao,Jimmy Cao,Mahardhika Pratama,Ryszard Kowalczyk*

Main category: cs.AI

TL;DR: 论文提出适应性概念，用于评估多智能体强化学习（MARL）在动态环境中的可靠性，并提出了一个包含三个维度的框架。


<details>
  <summary>Details</summary>
Motivation: 现实世界的多智能体系统（MAS）环境复杂且动态变化，现有MARL算法难以适应，需要更系统的评估方法。

Method: 引入适应性概念，提出包含学习适应性、策略适应性和场景驱动适应性的三维框架。

Result: 通过适应性视角，支持更原则性的MARL性能评估，超越狭窄的基准测试。

Conclusion: 该框架有助于开发更适合动态现实世界MAS的MARL算法。

Abstract: Multi-Agent Reinforcement Learning (MARL) has shown clear effectiveness in
coordinating multiple agents across simulated benchmarks and constrained
scenarios. However, its deployment in real-world multi-agent systems (MAS)
remains limited, primarily due to the complex and dynamic nature of such
environments. These challenges arise from multiple interacting sources of
variability, including fluctuating agent populations, evolving task goals, and
inconsistent execution conditions. Together, these factors demand that MARL
algorithms remain effective under continuously changing system configurations
and operational demands. To better capture and assess this capacity for
adjustment, we introduce the concept of \textit{adaptability} as a unified and
practically grounded lens through which to evaluate the reliability of MARL
algorithms under shifting conditions, broadly referring to any changes in the
environment dynamics that may occur during learning or execution. Centred on
the notion of adaptability, we propose a structured framework comprising three
key dimensions: learning adaptability, policy adaptability, and scenario-driven
adaptability. By adopting this adaptability perspective, we aim to support more
principled assessments of MARL performance beyond narrowly defined benchmarks.
Ultimately, this survey contributes to the development of algorithms that are
better suited for deployment in dynamic, real-world multi-agent systems.

</details>


### [111] [Introducing the Swiss Food Knowledge Graph: AI for Context-Aware Nutrition Recommendation](https://arxiv.org/abs/2507.10156)
*Lubnaa Abdur Rahman,Ioannis Papathanail,Stavroula Mougiakakou*

Main category: cs.AI

TL;DR: 论文介绍了瑞士食品知识图谱（SwissFKG），整合了食谱、食材、替代品、营养数据、饮食限制和过敏原信息，并利用LLM增强图谱内容，为个性化营养评估工具提供了基础。


<details>
  <summary>Details</summary>
Motivation: 现有自动饮食评估系统常忽略非视觉因素（如食材替代对营养的影响）和个性化需求（如过敏、文化习惯）。瑞士缺乏统一的营养信息整合资源。

Method: 构建SwissFKG图谱，采用LLM增强图谱内容，并评估四种LLM的性能。开发Graph-RAG应用展示图谱在回答用户营养查询中的作用。

Result: LLM能有效丰富图谱营养信息，SwissFKG提供食材级信息和营养指南支持，Graph-RAG应用验证了图谱在回答用户查询中的实用性。

Conclusion: SwissFKG为结合视觉、上下文和文化维度的下一代饮食评估工具奠定了基础。

Abstract: AI has driven significant progress in the nutrition field, especially through
multimedia-based automatic dietary assessment. However, existing automatic
dietary assessment systems often overlook critical non-visual factors, such as
recipe-specific ingredient substitutions that can significantly alter
nutritional content, and rarely account for individual dietary needs, including
allergies, restrictions, cultural practices, and personal preferences. In
Switzerland, while food-related information is available, it remains
fragmented, and no centralized repository currently integrates all relevant
nutrition-related aspects within a Swiss context. To bridge this divide, we
introduce the Swiss Food Knowledge Graph (SwissFKG), the first resource, to our
best knowledge, to unite recipes, ingredients, and their substitutions with
nutrient data, dietary restrictions, allergen information, and national
nutrition guidelines under one graph. We establish a LLM-powered enrichment
pipeline for populating the graph, whereby we further present the first
benchmark of four off-the-shelf (<70 B parameter) LLMs for food knowledge
augmentation. Our results demonstrate that LLMs can effectively enrich the
graph with relevant nutritional information. Our SwissFKG goes beyond recipe
recommendations by offering ingredient-level information such as allergen and
dietary restriction information, and guidance aligned with nutritional
guidelines. Moreover, we implement a Graph-RAG application to showcase how the
SwissFKG's rich natural-language data structure can help LLM answer
user-specific nutrition queries, and we evaluate LLM-embedding pairings by
comparing user-query responses against predefined expected answers. As such,
our work lays the foundation for the next generation of dietary assessment
tools that blend visual, contextual, and cultural dimensions of eating.

</details>


### [112] [Should We Ever Prefer Decision Transformer for Offline Reinforcement Learning?](https://arxiv.org/abs/2507.10174)
*Yumi Omori,Zixuan Dong,Keith Ross*

Main category: cs.AI

TL;DR: 本文通过实验比较了决策变换器（DT）与基于MLP的过滤行为克隆（FBC）在稀疏奖励环境中的表现，发现FBC性能更优。


<details>
  <summary>Details</summary>
Motivation: 探讨DT在稀疏奖励环境中的适用性，并验证FBC是否更优。

Method: 在机器人操作任务（Robomimic）和运动基准（D4RL）上实验比较DT与FBC。

Result: FBC在稀疏奖励环境中表现优于DT，且更高效。

Conclusion: DT在稀疏奖励环境中并非优选，甚至可能在其他环境中也不占优。

Abstract: In recent years, extensive work has explored the application of the
Transformer architecture to reinforcement learning problems. Among these,
Decision Transformer (DT) has gained particular attention in the context of
offline reinforcement learning due to its ability to frame return-conditioned
policy learning as a sequence modeling task. Most recently, Bhargava et al.
(2024) provided a systematic comparison of DT with more conventional MLP-based
offline RL algorithms, including Behavior Cloning (BC) and Conservative
Q-Learning (CQL), and claimed that DT exhibits superior performance in
sparse-reward and low-quality data settings.
  In this paper, through experimentation on robotic manipulation tasks
(Robomimic) and locomotion benchmarks (D4RL), we show that MLP-based Filtered
Behavior Cloning (FBC) achieves competitive or superior performance compared to
DT in sparse-reward environments. FBC simply filters out low-performing
trajectories from the dataset and then performs ordinary behavior cloning on
the filtered dataset. FBC is not only very straightforward, but it also
requires less training data and is computationally more efficient. The results
therefore suggest that DT is not preferable for sparse-reward environments.
From prior work, arguably, DT is also not preferable for dense-reward
environments. Thus, we pose the question: Is DT ever preferable?

</details>


### [113] [Survey for Categorising Explainable AI Studies Using Data Analysis Task Frameworks](https://arxiv.org/abs/2507.10208)
*Hamzah Ziadeh,Hendrik Knoche*

Main category: cs.AI

TL;DR: 论文提出了一种分类和比较可解释人工智能（XAI）研究的方法，基于三个维度：what、why和who，旨在解决任务描述不足、脱离上下文研究和用户测试不足的问题。


<details>
  <summary>Details</summary>
Motivation: 当前XAI研究存在大量矛盾，缺乏具体设计建议，主要源于对需要AI辅助的任务理解不足。

Method: 结合视觉分析、认知科学和仪表板设计等多个领域，提出分类和比较XAI研究的方法。

Result: 发现主要问题包括任务描述不足、脱离上下文研究和用户测试不足，建议研究应明确报告用户领域、AI和数据分析的专长。

Conclusion: 提出的研究指南有助于改进XAI任务的设计和报告，帮助研究者更好地识别相关研究、填补研究空白并处理设计矛盾。

Abstract: Research into explainable artificial intelligence (XAI) for data analysis
tasks suffer from a large number of contradictions and lack of concrete design
recommendations stemming from gaps in understanding the tasks that require AI
assistance. In this paper, we drew on multiple fields such as visual analytics,
cognition, and dashboard design to propose a method for categorising and
comparing XAI studies under three dimensions: what, why, and who. We identified
the main problems as: inadequate descriptions of tasks, context-free studies,
and insufficient testing with target users. We propose that studies should
specifically report on their users' domain, AI, and data analysis expertise to
illustrate the generalisability of their findings. We also propose study
guidelines for designing and reporting XAI tasks to improve the XAI community's
ability to parse the rapidly growing field. We hope that our contribution can
help researchers and designers better identify which studies are most relevant
to their work, what gaps exist in the research, and how to handle contradictory
results regarding XAI design.

</details>


### [114] [Toward Real-World Table Agents: Capabilities, Workflows, and Design Principles for LLM-based Table Intelligence](https://arxiv.org/abs/2507.10281)
*Jiaming Tian,Liyao Li,Wentao Ye,Haobo Wang,Lingxin Wang,Lihua Yu,Zujie Ren,Gang Chen,Junbo Zhao*

Main category: cs.AI

TL;DR: 该论文综述了基于LLM的表格代理在现实场景中的应用，提出了五个核心能力，并揭示了学术基准与实际性能之间的差距。


<details>
  <summary>Details</summary>
Motivation: 现实中的表格任务常涉及噪声和复杂性，但现有研究多针对干净数据集。本文旨在填补这一空白，提升表格代理的实用性。

Method: 定义了五个核心能力（C1-C5），分析比较现有方法，并以Text-to-SQL代理为例详细探讨性能差距。

Result: 发现开源模型在真实场景中表现不佳，学术基准与实际需求存在差距。

Conclusion: 提出了改进表格代理鲁棒性、泛化性和效率的具体建议。

Abstract: Tables are fundamental in domains such as finance, healthcare, and public
administration, yet real-world table tasks often involve noise, structural
heterogeneity, and semantic complexity--issues underexplored in existing
research that primarily targets clean academic datasets. This survey focuses on
LLM-based Table Agents, which aim to automate table-centric workflows by
integrating preprocessing, reasoning, and domain adaptation. We define five
core competencies--C1: Table Structure Understanding, C2: Table and Query
Semantic Understanding, C3: Table Retrieval and Compression, C4: Executable
Reasoning with Traceability, and C5: Cross-Domain Generalization--to analyze
and compare current approaches. In addition, a detailed examination of the
Text-to-SQL Agent reveals a performance gap between academic benchmarks and
real-world scenarios, especially for open-source models. Finally, we provide
actionable insights to improve the robustness, generalization, and efficiency
of LLM-based Table Agents in practical settings.

</details>


### [115] [Instance space analysis of the capacitated vehicle routing problem](https://arxiv.org/abs/2507.10397)
*Alessandra M. M. M. Gouvêa,Nuno Paulos,Eduardo Uchoa e Mariá C. V. Nascimento*

Main category: cs.AI

TL;DR: 论文通过实例空间分析（ISA）方法，结合DIMACS数据集，研究了CVRP中实例特征与元启发式性能的关系，并提供了投影矩阵以支持新实例分析。


<details>
  <summary>Details</summary>
Motivation: 解决CVRP研究中实例特征与元启发式性能之间复杂关系的理解问题。

Method: 采用实例空间分析（ISA）方法，结合DIMACS数据集，通过PRELM、SIFTED和PILOT阶段进行降维和机器学习分析。

Result: 识别了23个相关实例特征，并创建了二维实例空间投影，揭示了实例结构对元启发式行为的影响。

Conclusion: 提供了投影矩阵，为CVRP领域的实例分析提供了新方法。

Abstract: This paper seeks to advance CVRP research by addressing the challenge of
understanding the nuanced relationships between instance characteristics and
metaheuristic (MH) performance. We present Instance Space Analysis (ISA) as a
valuable tool that allows for a new perspective on the field. By combining the
ISA methodology with a dataset from the DIMACS 12th Implementation Challenge on
Vehicle Routing, our research enabled the identification of 23 relevant
instance characteristics. Our use of the PRELIM, SIFTED, and PILOT stages,
which employ dimensionality reduction and machine learning methods, allowed us
to create a two-dimensional projection of the instance space to understand how
the structure of instances affect the behavior of MHs. A key contribution of
our work is that we provide a projection matrix, which makes it straightforward
to incorporate new instances into this analysis and allows for a new method for
instance analysis in the CVRP field.

</details>


### [116] [SentiDrop: A Multi Modal Machine Learning model for Predicting Dropout in Distance Learning](https://arxiv.org/abs/2507.10421)
*Meriem Zerkouk,Miloud Mihoubi,Belkacem Chikhaoui*

Main category: cs.AI

TL;DR: 论文提出了一种结合BERT情感分析和XGBoost特征选择的新模型，用于预测远程教育中的学生辍学风险，准确率达84%，优于基线模型。


<details>
  <summary>Details</summary>
Motivation: 远程教育中的学生辍学问题严重，早期预测对有效干预至关重要。现有研究强调整合多源数据以提高预测准确性。

Method: 结合BERT模型分析学生评论的情感，与XGBoost处理的社会人口和行为数据融合，通过特征选择优化模型。

Result: 模型在未见数据上准确率达84%，优于基线模型的82%，且在精确率和F1分数上表现更优。

Conclusion: 该模型可作为个性化干预工具，有效降低辍学率并促进学生坚持学习。

Abstract: School dropout is a serious problem in distance learning, where early
detection is crucial for effective intervention and student perseverance.
Predicting student dropout using available educational data is a widely
researched topic in learning analytics. Our partner's distance learning
platform highlights the importance of integrating diverse data sources,
including socio-demographic data, behavioral data, and sentiment analysis, to
accurately predict dropout risks. In this paper, we introduce a novel model
that combines sentiment analysis of student comments using the Bidirectional
Encoder Representations from Transformers (BERT) model with socio-demographic
and behavioral data analyzed through Extreme Gradient Boosting (XGBoost). We
fine-tuned BERT on student comments to capture nuanced sentiments, which were
then merged with key features selected using feature importance techniques in
XGBoost. Our model was tested on unseen data from the next academic year,
achieving an accuracy of 84\%, compared to 82\% for the baseline model.
Additionally, the model demonstrated superior performance in other metrics,
such as precision and F1-score. The proposed method could be a vital tool in
developing personalized strategies to reduce dropout rates and encourage
student perseverance

</details>


### [117] [Acquiring and Adapting Priors for Novel Tasks via Neural Meta-Architectures](https://arxiv.org/abs/2507.10446)
*Sudarshan Babu*

Main category: cs.AI

TL;DR: 论文提出了一种利用神经记忆和超网络设计的方法，解决在数据稀缺领域（如计算化学、医学影像）中高效获取先验知识的问题，并展示了在3D场景生成和分子属性预测中的应用。


<details>
  <summary>Details</summary>
Motivation: 在数据稀缺的领域中，传统的大规模预训练模型难以应用，因此需要设计新的架构来高效获取先验知识。

Method: 使用神经记忆适应非平稳分布，结合超网络设计和MAML训练，应用于3D场景生成和分子属性预测。

Result: 超网络设计能更高效地获取先验知识，并在3D场景生成和分子属性预测中表现出色。

Conclusion: 提出的方法在数据稀缺领域具有潜力，能够高效获取和转移先验知识。

Abstract: The ability to transfer knowledge from prior experiences to novel tasks
stands as a pivotal capability of intelligent agents, including both humans and
computational models. This principle forms the basis of transfer learning,
where large pre-trained neural networks are fine-tuned to adapt to downstream
tasks. Transfer learning has demonstrated tremendous success, both in terms of
task adaptation speed and performance. However there are several domains where,
due to lack of data, training such large pre-trained models or foundational
models is not a possibility - computational chemistry, computational
immunology, and medical imaging are examples. To address these challenges, our
work focuses on designing architectures to enable efficient acquisition of
priors when large amounts of data are unavailable. In particular, we
demonstrate that we can use neural memory to enable adaptation on
non-stationary distributions with only a few samples. Then we demonstrate that
our hypernetwork designs (a network that generates another network) can acquire
more generalizable priors than standard networks when trained with Model
Agnostic Meta-Learning (MAML). Subsequently, we apply hypernetworks to 3D scene
generation, demonstrating that they can acquire priors efficiently on just a
handful of training scenes, thereby leading to faster text-to-3D generation. We
then extend our hypernetwork framework to perform 3D segmentation on novel
scenes with limited data by efficiently transferring priors from earlier viewed
scenes. Finally, we repurpose an existing molecular generative method as a
pre-training framework that facilitates improved molecular property prediction,
addressing critical challenges in computational immunology

</details>


### [118] [DeepResearch$^{\text{Eco}}$: A Recursive Agentic Workflow for Complex Scientific Question Answering in Ecology](https://arxiv.org/abs/2507.10522)
*Jennifer D'Souza,Endres Keno Sander,Andrei Aioanei*

Main category: cs.AI

TL;DR: DeepResearch$^{\text{Eco}}$是一种基于LLM的自动化科学合成系统，支持递归、深度和广度可控的探索，显著提升文献检索的多样性和细节。


<details>
  <summary>Details</summary>
Motivation: 解决传统检索增强生成管道的局限性，提供用户可控的合成、透明推理和参数驱动的配置能力。

Method: 通过递归、深度和广度可控的探索方式，结合参数驱动的配置，实现高吞吐量的领域证据整合。

Result: 在49个生态研究问题中，实现了21倍的源整合提升和14.9倍的每千字源整合增加。高参数设置可达到专家级分析深度和上下文多样性。

Conclusion: DeepResearch$^{\text{Eco}}$在科学文献合成中表现出高效性和灵活性，适用于复杂研究问题的自动化探索。

Abstract: We introduce DeepResearch$^{\text{Eco}}$, a novel agentic LLM-based system
for automated scientific synthesis that supports recursive, depth- and
breadth-controlled exploration of original research questions -- enhancing
search diversity and nuance in the retrieval of relevant scientific literature.
Unlike conventional retrieval-augmented generation pipelines, DeepResearch
enables user-controllable synthesis with transparent reasoning and
parameter-driven configurability, facilitating high-throughput integration of
domain-specific evidence while maintaining analytical rigor. Applied to 49
ecological research questions, DeepResearch achieves up to a 21-fold increase
in source integration and a 14.9-fold rise in sources integrated per 1,000
words. High-parameter settings yield expert-level analytical depth and
contextual diversity.
  Source code available at: https://github.com/sciknoworg/deep-research.

</details>
