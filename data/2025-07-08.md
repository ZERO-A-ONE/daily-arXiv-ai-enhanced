<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 23]
- [cs.CR](#cs.CR) [Total: 55]
- [cs.AI](#cs.AI) [Total: 84]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [The Impact of LLM-Assistants on Software Developer Productivity: A Systematic Literature Review](https://arxiv.org/abs/2507.03156)
*Amr Mohamed,Maram Assi,Mariam Guizani*

Main category: cs.SE

TL;DR: 本文综述了37项关于大型语言模型助手（LLM-assistants）对软件开发效率影响的研究，揭示了其显著优势和潜在风险。


<details>
  <summary>Details</summary>
Motivation: 探讨LLM-assistants如何影响开发者生产力，填补现有研究的空白。

Method: 系统文献综述，分析2014年至2024年的37项同行评审研究。

Result: LLM-assistants能加速开发并自动化任务，但也带来认知卸载和团队协作减少等问题。研究多关注满意度、性能和效率，而沟通和活动维度研究不足。

Conclusion: 未来研究需更综合地评估LLM-assistants的影响，尤其是长期和团队层面的效果。

Abstract: Large language model assistants (LLM-assistants) present new opportunities to
transform software development. Developers are increasingly adopting these
tools across tasks, including coding, testing, debugging, documentation, and
design. Yet, despite growing interest, there is no synthesis of how
LLM-assistants affect software developer productivity. In this paper, we
present a systematic literature review of 37 peer-reviewed studies published
between January 2014 and December 2024 that examine this impact. Our analysis
reveals that LLM-assistants offer both considerable benefits and critical
risks. Commonly reported gains include minimized code search, accelerated
development, and the automation of trivial and repetitive tasks. However,
studies also highlight concerns around cognitive offloading, reduced team
collaboration, and inconsistent effects on code quality. While the majority of
studies (92%) adopt a multi-dimensional perspective by examining at least two
SPACE dimensions, reflecting increased awareness of the complexity of developer
productivity, only 14% extend beyond three dimensions, indicating substantial
room for more integrated evaluations. Satisfaction, Performance, and Efficiency
are the most frequently investigated dimensions, whereas Communication and
Activity remain underexplored. Most studies are exploratory (64%) and
methodologically diverse, but lack longitudinal and team-based evaluations.
This review surfaces key research gaps and provides recommendations for future
research and practice. All artifacts associated with this study are publicly
available at https://zenodo.org/records/15788502.

</details>


### [2] [Assessing Small Language Models for Code Generation: An Empirical Study with Benchmarks](https://arxiv.org/abs/2507.03160)
*Md Mahade Hasan,Muhammad Waseem,Kai-Kristian Kemell,Jussi Raskua,Juha Ala-Rantalaa,Pekka Abrahamsson*

Main category: cs.SE

TL;DR: 小型语言模型（SLMs）在代码生成中展现出高效且轻量的潜力，但对其能力和性能的实证研究有限。本文评估了20个开源SLMs在多个代码基准上的表现，发现部分小型模型在性能和效率间取得平衡，但进一步提升精度需转向更大模型。


<details>
  <summary>Details</summary>
Motivation: 探索SLMs在代码生成中的实际能力、局限性和性能权衡，填补实证研究的空白。

Method: 对20个开源SLMs（0.4B至10B参数）在五个代码基准（如HumanEval、MBPP等）上进行评估，从功能正确性、计算效率和多语言性能三个维度分析。

Result: 部分小型SLMs在性能和效率间表现优异，但精度提升需更大模型，且伴随显著计算资源增加。多语言性能差异不显著，SLMs在Python等语言中表现较好。

Conclusion: SLMs在资源受限环境中具有实用价值，但需权衡性能与资源消耗。研究为实际代码生成任务中的SLM选择和设计提供了指导。

Abstract: The recent advancements of Small Language Models (SLMs) have opened new
possibilities for efficient code generation. SLMs offer lightweight and
cost-effective alternatives to Large Language Models (LLMs), making them
attractive for use in resource-constrained environments. However, empirical
understanding of SLMs, particularly their capabilities, limitations, and
performance trade-offs in code generation remains limited. This study presents
a comprehensive empirical evaluation of 20 open-source SLMs ranging from 0.4B
to 10B parameters on five diverse code-related benchmarks (HumanEval, MBPP,
Mercury, HumanEvalPack, and CodeXGLUE). The models are assessed along three
dimensions: i) functional correctness of generated code, ii) computational
efficiency and iii) performance across multiple programming languages. The
findings of this study reveal that several compact SLMs achieve competitive
results while maintaining a balance between performance and efficiency, making
them viable for deployment in resource-constrained environments. However,
achieving further improvements in accuracy requires switching to larger models.
These models generally outperform their smaller counterparts, but they require
much more computational power. We observe that for 10% performance
improvements, models can require nearly a 4x increase in VRAM consumption,
highlighting a trade-off between effectiveness and scalability. Besides, the
multilingual performance analysis reveals that SLMs tend to perform better in
languages such as Python, Java, and PHP, while exhibiting relatively weaker
performance in Go, C++, and Ruby. However, statistical analysis suggests these
differences are not significant, indicating a generalizability of SLMs across
programming languages. Based on the findings, this work provides insights into
the design and selection of SLMs for real-world code generation tasks.

</details>


### [3] [Analyzing C/C++ Library Migrations at the Package-level: Prevalence, Domains, Targets and Rationals across Seven Package Management Tools](https://arxiv.org/abs/2507.03263)
*Haiqiao Gu,Yiliang Zhao,Kai Gao,Minghui Zhou*

Main category: cs.SE

TL;DR: 该论文研究了C/C++库迁移的特性，建立了首个相关数据集，并与Python、JavaScript和Java进行了比较，揭示了C/C++特有的迁移原因和需求。


<details>
  <summary>Details</summary>
Motivation: 由于C/C++生态系统中依赖管理的复杂性和碎片化，目前对C/C++库迁移的理解不足，需要填补这一知识空白。

Method: 分析了19,943个使用不同包管理工具的C/C++项目，建立了首个C/C++库迁移数据集，并调查了迁移的普遍性、领域、目标库和原因。

Result: C/C++库迁移趋势与Java相似，迁移主要集中在GUI、构建和操作系统开发领域，83.46%的源库只有一个迁移目标，揭示了四个C/C++特有的迁移原因。

Conclusion: 研究结果有助于C/C++开发者做出更明智的库迁移决策，并为相关工具设计提供启示。

Abstract: Library migration happens when a library can not meet the project's
requirements and is non-trivial to accomplish. To mitigate the problem,
substantial efforts have been devoted to understanding its characteristics and
recommending alternative libraries, especially for programming language (PL)
ecosystems with a central package hosting platform, such as Python (PyPI).
However, to the best of our knowledge, understanding of C/C++ library
migrations is still lacking, possibly due to challenges resulting from the
fragmented and complicated dependency management practices in the C/C++
ecosystem. To bridge this knowledge gap, this paper analyzes 19,943 C/C++
projects that utilize different package management tools and establishes the
first C/C++ library migration dataset. Based on the dataset, we investigate the
prevalence, domains, target library, and rationale of C/C++ library migrations
and compare the results with three widely investigated PLs: Python, JavaScript,
and Java. We find that the overall trend in the number of C/C++ library
migrations is similar to Java. Migrations across different package management
tools are also observed. In C/C++, library migrations mainly occur in GUI,
Build, and OS development, but are rare in domains (e.g., Testing and Logging)
that dominate library migrations in the three compared PLs. 83.46\% of C/C++
source libraries only have one migration target, suggesting that our library
migration dataset could be used directly to recommend migration targets. We
find four C/C++-specific migration reasons, such as less compile time and
unification of dependency management, revealing the unique dependency
management requirements in C/C++ projects. We believe our findings can help
C/C++ developers make more informed library migration decisions and shed light
on the design of C/C++ library migration tools.

</details>


### [4] [scikit-package -- software packaging standards and roadmap for sharing reproducible scientific software](https://arxiv.org/abs/2507.03328)
*S. Lee,C. Myers,A. Yang,T. Zhang,S. J. L. Billinge*

Main category: cs.SE

TL;DR: scikit-package旨在通过教程和自动化工作流程，帮助科学家更轻松地共享和重用代码，提升代码的可维护性和可重复性。


<details>
  <summary>Details</summary>
Motivation: 科学进步依赖于结果的可共享和可重复性，但科学家编写的软件代码在版本、质量和共享方面存在挑战。

Method: 提供教程和集中化、自动化的工作流程，帮助科学家将代码从简单脚本提升为可安装、测试和文档化的软件包。

Result: scikit-package为科学家提供了工具和路线图，支持代码在不同复杂度级别的重用和共享。

Conclusion: scikit-package通过社区维护的工具和指导，提升了科学软件的可重复性和共享性。

Abstract: Scientific advancement relies on the ability to share and reproduce results.
When data analysis or calculations are carried out using software written by
scientists there are special challenges around code versions, quality and code
sharing. scikit-package provides a roadmap to facilitate code reuse and sharing
with minimal effort through tutorials coupled with automated and centralized
reusable workflows. The goal of the project is to provide pedagogical and
practical tools for scientists who are not professionally trained software
engineers to write more reusable and maintainable software code. Code reuse can
occur at multiple levels of complexity-from turning a code block into a
function within a single script, to publishing a publicly installable, fully
tested, and documented software package scikit-package provides a community
maintained set of tools, and a roadmap, to help scientists bring their software
higher levels of reproducibility and shareability.

</details>


### [5] [Prompt Engineering Guidelines for Using Large Language Models in Requirements Engineering](https://arxiv.org/abs/2507.03405)
*Krishna Ronanki,Simon Arvidsson,Johan Axell*

Main category: cs.SE

TL;DR: 本文探讨了如何利用提示工程指导大型语言模型（LLMs）在需求工程（RE）中的有效应用，发现现有指南不足，并通过文献综述和专家访谈提出改进方向。


<details>
  <summary>Details</summary>
Motivation: 生成式AI模型（如LLMs）在需求工程中应用广泛，但缺乏针对RE的提示工程指南，研究旨在填补这一空白。

Method: 通过系统文献综述提取提示工程指南，并访谈RE专家评估其适用性。

Result: 发现现有指南在RE领域不足，提出的映射关系有助于解决这一问题。

Conclusion: 研究为未来RE领域提示工程指南的进一步研究提供了方向。

Abstract: The rapid emergence of generative AI models like Large Language Models (LLMs)
has demonstrated its utility across various activities, including within
Requirements Engineering (RE). Ensuring the quality and accuracy of
LLM-generated output is critical, with prompt engineering serving as a key
technique to guide model responses. However, existing literature provides
limited guidance on how prompt engineering can be leveraged, specifically for
RE activities. The objective of this study is to explore the applicability of
existing prompt engineering guidelines for the effective usage of LLMs within
RE. To achieve this goal, we began by conducting a systematic review of primary
literature to compile a non-exhaustive list of prompt engineering guidelines.
Then, we conducted interviews with RE experts to present the extracted
guidelines and gain insights on the advantages and limitations of their
application within RE. Our literature review indicates a shortage of prompt
engineering guidelines for domain-specific activities, specifically for RE. Our
proposed mapping contributes to addressing this shortage. We conclude our study
by identifying an important future line of research within this field.

</details>


### [6] [Enhancing Uncertainty Quantification for Runtime Safety Assurance Using Causal Risk Analysis and Operational Design Domain](https://arxiv.org/abs/2507.03515)
*Radouane Bouchekir,Michell Guzman Cancimance*

Main category: cs.SE

TL;DR: 提出了一种基于风险因果分析的方法，通过整合环境条件和深度学习不确定性，动态评估自动驾驶系统的运行时安全性。


<details>
  <summary>Details</summary>
Motivation: 深度学习组件的不确定性和对环境变化的敏感性导致自动驾驶系统的运行时安全性难以保证。

Method: 结合HARA和故障树模型识别关键操作条件，并将其与数据和模型的不确定性整合到贝叶斯网络中，实时推断安全性概率分布。

Result: 通过自动代客泊车系统中的目标检测组件案例研究验证了方法的有效性。

Conclusion: 该方法提供了一种动态且上下文感知的不确定性度量，能够更准确地评估系统安全性。

Abstract: Ensuring the runtime safety of autonomous systems remains challenging due to
deep learning components' inherent uncertainty and their sensitivity to
environmental changes. In this paper, we propose an enhancement of traditional
uncertainty quantification by explicitly incorporating environmental conditions
using risk-based causal analysis. We leverage Hazard Analysis and Risk
Assessment (HARA) and fault tree modeling to identify critical operational
conditions affecting system functionality. These conditions, together with
uncertainties from the data and model, are integrated into a unified Bayesian
Network (BN). At runtime, this BN is instantiated using real-time environmental
observations to infer a probabilistic distribution over the safety estimation.
This distribution enables the computation of both expected performance and its
associated variance, providing a dynamic and context-aware measure of
uncertainty. We demonstrate our approach through a case study of the Object
Detection (OD) component in an Automated Valet Parking (AVP).

</details>


### [7] [The Role of Humour in Software Engineering -- A Literature Review and Preliminary Taxonomy](https://arxiv.org/abs/2507.03527)
*Dulaji Hidellaarachchi,John Grundy,Rashina Hoda*

Main category: cs.SE

TL;DR: 本文提出了一种基于文献综述的分类法，探讨幽默在软件工程团队中的特征与使用，旨在提升生产力、改善沟通并营造积极工作环境。


<details>
  <summary>Details</summary>
Motivation: 幽默在增强创造力、团队效能和员工幸福感方面被广泛认可，但其在软件工程团队中的具体影响尚未充分研究。

Method: 通过整合心理学、社会学和组织行为学的研究，构建了一个分类框架，将幽默分为不同的理论、风格、模型和量表。

Result: 提出了一个结构化方法，帮助软件工程专业人士和研究者理解幽默在工作中的作用，同时指出需要进一步实证验证。

Conclusion: 研究旨在通过幽默的战略性使用，为软件工程团队创造更具凝聚力、创造力和心理支持的环境。

Abstract: Humour has long been recognized as a key factor in enhancing creativity,
group effectiveness, and employee well-being across various domains. However,
its occurrence and impact within software engineering (SE) teams remains
under-explored. This paper introduces a comprehensive, literature review-based
taxonomy exploring the characterisation and use of humour in SE teams, with the
goal of boosting productivity, improving communication, and fostering a
positive work environment while emphasising the responsible use of humour to
mitigate its potential negative impacts. Drawing from a wide array of studies
in psychology, sociology, and organizational behaviour, our proposed framework
categorizes humour into distinct theories, styles, models, and scales, offering
SE professionals and researchers a structured approach to understanding humour
in their work. This study also addresses the unique challenges of applying
humour in SE, highlighting its potential benefits while acknowledging the need
for further empirical validation in this context. Ultimately, our study aims to
pave the way for more cohesive, creative, and psychologically supportive SE
environments through the strategic use of humour.

</details>


### [8] [ACE: Automated Technical Debt Remediation with Validated Large Language Model Refactorings](https://arxiv.org/abs/2507.03536)
*Adam Tornhill,Markus Borg,Nadim Hagatulah,Emma Söderberg*

Main category: cs.SE

TL;DR: 论文提出了一种名为ACE的工具，利用LLM自动改进代码，以提高代码可理解性和质量。


<details>
  <summary>Details</summary>
Motivation: 随着AI和LLM的发展，代码编写速度加快，但代码理解成为软件开发的瓶颈，占开发者70%的时间。改进代码可理解性对应对不断增长的代码库至关重要。

Method: 通过数据驱动的方法开发ACE工具，利用已验证的LLM输出来提供可靠的代码重构建议，同时考虑代码质量改进和程序正确性。

Result: 初步用户反馈表明，AI辅助重构有助于减少代码层面的技术债务。

Conclusion: ACE工具通过AI辅助重构，有效提升代码可理解性，帮助开发者应对代码库增长。

Abstract: The remarkable advances in AI and Large Language Models (LLMs) have enabled
machines to write code, accelerating the growth of software systems. However,
the bottleneck in software development is not writing code but understanding
it; program understanding is the dominant activity, consuming approximately 70%
of developers' time. This implies that improving existing code to make it
easier to understand has a high payoff and - in the age of AI-assisted coding -
is an essential activity to ensure that a limited pool of developers can keep
up with ever-growing codebases. This paper introduces Augmented Code
Engineering (ACE), a tool that automates code improvements using validated LLM
output. Developed through a data-driven approach, ACE provides reliable
refactoring suggestions by considering both objective code quality improvements
and program correctness. Early feedback from users suggests that AI-enabled
refactoring helps mitigate code-level technical debt that otherwise rarely gets
acted upon.

</details>


### [9] [Is It Time To Treat Prompts As Code? A Multi-Use Case Study For Prompt Optimization Using DSPy](https://arxiv.org/abs/2507.03620)
*Francisca Lemos,Victor Alves,Filipa Ferraz*

Main category: cs.SE

TL;DR: DSPy框架通过程序化优化提示词，提升LLM性能，效果因任务而异。


<details>
  <summary>Details</summary>
Motivation: 提示工程对发挥LLM潜力至关重要，但依赖人工试错耗时低效，DSPy旨在解决这一问题。

Method: 使用DSPy框架在五个用例中程序化创建和优化提示词，评估其对性能的影响。

Result: 部分用例表现显著提升（如提示评估任务准确率从46.2%升至64.0%），其他用例改进有限。

Conclusion: DSPy的系统化提示优化可提升LLM性能，但效果因任务而异，需结合具体用例评估。

Abstract: Although prompt engineering is central to unlocking the full potential of
Large Language Models (LLMs), crafting effective prompts remains a
time-consuming trial-and-error process that relies on human intuition. This
study investigates Declarative Self-improving Python (DSPy), an optimization
framework that programmatically creates and refines prompts, applied to five
use cases: guardrail enforcement, hallucination detection in code, code
generation, routing agents, and prompt evaluation. Each use case explores how
prompt optimization via DSPy influences performance. While some cases
demonstrated modest improvements - such as minor gains in the guardrails use
case and selective enhancements in hallucination detection - others showed
notable benefits. The prompt evaluation criterion task demonstrated a
substantial performance increase, rising accuracy from 46.2% to 64.0%. In the
router agent case, the possibility of improving a poorly performing prompt and
of a smaller model matching a stronger one through optimized prompting was
explored. Although prompt refinement increased accuracy from 85.0% to 90.0%,
using the optimized prompt with a cheaper model did not improve performance.
Overall, this study's findings suggest that DSPy's systematic prompt
optimization can enhance LLM performance, particularly when instruction tuning
and example selection are optimized together. However, the impact varies by
task, highlighting the importance of evaluating specific use cases in prompt
optimization research.

</details>


### [10] [Specification-Guided Repair of Arithmetic Errors in Dafny Programs using LLMs](https://arxiv.org/abs/2507.03659)
*Valentina Wu,Alexandra Mendes,Alexandre Abreu*

Main category: cs.SE

TL;DR: 论文提出了一种结合形式化规范和大型语言模型（LLM）的自动程序修复（APR）工具，用于Dafny语言，展示了高准确率的故障定位和修复能力。


<details>
  <summary>Details</summary>
Motivation: 形式化验证虽能确保软件正确性，但验证失败时调试和修复复杂耗时。传统APR依赖测试套件，但无法覆盖所有场景。形式化规范提供了更强的正确性标准。

Method: 利用Hoare Logic定位程序状态，结合LLM（如GPT-4o mini、Llama 3等）生成候选修复。专注于算术错误，假设规范正确。

Result: 在DafnyBench基准测试中，故障定位准确率达89.6%，GPT-4o mini修复成功率最高（74.18%）。

Conclusion: 形式化推理与LLM驱动的程序修复结合具有潜力，为自动程序修复提供了新方向。

Abstract: Formal verification offers strong assurances of software correctness.
However, debugging and repairing the underlying faults can be complex and
time-consuming when verification fails. Automated Program Repair (APR) aims to
ease this by automatically identifying and fixing faults. Traditional APR
techniques often depend on test suites for validation, but these may fail to
capture all scenarios. In contrast, formal specifications provide stronger
correctness criteria for effective repairs.
  We present an innovative APR tool for Dafny, a verification-aware programming
language that uses formal specifications - including pre-conditions,
post-conditions, and invariants - as oracles for fault localization and repair.
Assuming the correctness of the specifications and focusing on arithmetic bugs,
we localize faults through a series of steps, which include using Hoare Logic
to determine the state of each statement within the program and
state-of-the-art Large Language Models (LLMs) to synthesize candidate fixes.
The chosen models were GPT-4o mini, Llama 3, Mistral 7B, and Llemma 7B.
  We evaluate our approach using DafnyBench, a benchmark of real-world Dafny
programs. Our tool achieves 89.6% accuracy in fault localization, with GPT-4o
mini yielding the highest repair success rate (74.18%). These results highlight
the potential of combining formal reasoning with LLM-driven program synthesis
for automated program repair.

</details>


### [11] [Efficient Detection of Intermittent Job Failures Using Few-Shot Learning](https://arxiv.org/abs/2507.04173)
*Henri Aïdasso,Francis Bordeleau,Ali Tizghadam*

Main category: cs.SE

TL;DR: 论文提出了一种基于少样本学习（FSL）的新方法，用于检测持续集成（CI）中的间歇性任务失败，解决了现有启发式方法因误标而性能受限的问题。


<details>
  <summary>Details</summary>
Motivation: 间歇性任务失败（如不稳定测试或基础设施问题）是CI中的主要挑战，现有基于机器学习的分类方法依赖大量手动标注数据或启发式方法，但误标率高。

Method: 采用少样本学习（FSL），通过少量手动标注日志微调小型语言模型生成丰富嵌入，再训练ML分类器。

Result: 在5个工业项目和1个开源项目中，FSL方法仅需12个样本即达到70-88%的F1分数，显著优于现有方法（34-52%）。

Conclusion: 研究表明数据质量优于数量，FSL方法为间歇性任务失败检测提供了更高效实用的框架。

Abstract: One of the main challenges developers face in the use of continuous
integration (CI) and deployment pipelines is the occurrence of intermittent job
failures, which result from unexpected non-deterministic issues (e.g., flaky
tests or infrastructure problems) rather than regular code-related errors such
as bugs. Prior studies developed machine-learning (ML) models trained on large
datasets of job logs to classify job failures as either intermittent or
regular. As an alternative to costly manual labeling of large datasets, the
state-of-the-art (SOTA) approach leveraged a heuristic based on
non-deterministic job reruns. However, this method mislabels intermittent job
failures as regular in contexts where rerunning suspicious job failures is not
an explicit policy, and therefore limits the SOTA's performance in practice. In
fact, our manual analysis of 2,125 job failures from 5 industrial and 1
open-source projects reveals that, on average, 32\% of intermittent job
failures are mislabeled as regular. To address these limitations, this paper
introduces a novel approach to intermittent job failure detection using
few-shot learning (FSL). Specifically, we fine-tune a small language model
using a few number of manually labeled log examples to generate rich
embeddings, which are then used to train an ML classifier. Our FSL-based
approach achieves 70-88\% F1-score with only 12 shots in all projects,
outperforming the SOTA, which proved ineffective (34-52\% F1-score) in 4
projects. Overall, this study underlines the importance of data quality over
quantity and provides a more efficient and practical framework for the
detection of intermittent job failures in organizations.

</details>


### [12] [From Legal Text to Tech Specs: Generative AI's Interpretation of Consent in Privacy Law](https://arxiv.org/abs/2507.04185)
*Aniket Kesari,Travis Breaux,Tom Norton,Sarah Santos,Anmol Singhal*

Main category: cs.SE

TL;DR: 论文探讨了如何利用大型语言模型（LLMs）在需求工程中弥合法律要求与技术实现之间的差距，以解决隐私法中“同意”要求的操作化问题。


<details>
  <summary>Details</summary>
Motivation: 隐私法将“同意”作为数据收集和处理的合法基础，但法律要求如何转化为软件实现仍存在挑战，且软件开发过程不透明加剧了这一问题。

Method: 研究采用三步流程：使用LLM分类软件用例的合规性，生成非合规用例的LLM修改建议，并人工验证这些修改是否符合法律标准。

Result: 初步结果表明LLMs在自动化合规任务中具有潜力，但也揭示了其推理能力的局限性。

Conclusion: 通过在实际用例中测试LLMs，研究为利用AI驱动解决方案提升软件法律合规性提供了见解。

Abstract: Privacy law and regulation have turned to "consent" as the legitimate basis
for collecting and processing individuals' data. As governments have rushed to
enshrine consent requirements in their privacy laws, such as the California
Consumer Privacy Act (CCPA), significant challenges remain in understanding how
these legal mandates are operationalized in software. The opaque nature of
software development processes further complicates this translation. To address
this, we explore the use of Large Language Models (LLMs) in requirements
engineering to bridge the gap between legal requirements and technical
implementation. This study employs a three-step pipeline that involves using an
LLM to classify software use cases for compliance, generating LLM modifications
for non-compliant cases, and manually validating these changes against legal
standards. Our preliminary findings highlight the potential of LLMs in
automating compliance tasks, while also revealing limitations in their
reasoning capabilities. By benchmarking LLMs against real-world use cases, this
research provides insights into leveraging AI-driven solutions to enhance legal
compliance of software.

</details>


### [13] [Improving Deep Learning Framework Testing with Model-Level Metamorphic Testing](https://arxiv.org/abs/2507.04354)
*Yanzhou Mu,Juan Zhai,Chunrong Fang,Xiang Chen,Zhixiang Cao,Peiran Yang,Kexin Zhao,An Guo,Zhenyu Chen*

Main category: cs.SE

TL;DR: 论文提出ModelMeta，一种基于模型级蜕变测试的方法，用于检测深度学习框架中的错误，通过多样化接口组合和细粒度分析提升测试效果。


<details>
  <summary>Details</summary>
Motivation: 深度学习框架的错误可能导致严重后果，现有测试方法因缺乏合适的测试预言而效果有限，尤其是面对浮点误差、随机性和输入复杂性时。

Method: 提出ModelMeta，利用四种基于DL模型结构特征的蜕变关系，通过QR-DQN策略生成多样化测试输入，并分析训练损失/梯度、内存/GPU使用和执行时间。

Result: ModelMeta能够检测到与多接口组合和运行时指标相关的错误，弥补了现有方法的不足。

Conclusion: ModelMeta通过模型级蜕变测试和细粒度分析，显著提升了深度学习框架测试的多样性和有效性。

Abstract: Deep learning (DL) frameworks are essential to DL-based software systems, and
framework bugs may lead to substantial disasters, thus requiring effective
testing. Researchers adopt DL models or single interfaces as test inputs and
analyze their execution results to detect bugs. However, floating-point errors,
inherent randomness, and the complexity of test inputs make it challenging to
analyze execution results effectively, leading to existing methods suffering
from a lack of suitable test oracles. Some researchers utilize metamorphic
testing to tackle this challenge. They design Metamorphic Relations (MRs) based
on input data and parameter settings of a single framework interface to
generate equivalent test inputs, ensuring consistent execution results between
original and generated test inputs. Despite their promising effectiveness, they
still face certain limitations. (1) Existing MRs overlook structural
complexity, limiting test input diversity. (2) Existing MRs focus on limited
interfaces, which limits generalization and necessitates additional
adaptations. (3) Their detected bugs are related to the result consistency of
single interfaces and far from those exposed in multi-interface combinations
and runtime metrics (e.g., resource usage). To address these limitations, we
propose ModelMeta, a model-level metamorphic testing method for DL frameworks
with four MRs focused on the structure characteristics of DL models. ModelMeta
augments seed models with diverse interface combinations to generate test
inputs with consistent outputs, guided by the QR-DQN strategy. It then detects
bugs through fine-grained analysis of training loss/gradients, memory/GPU
usage, and execution time.

</details>


### [14] [DevMuT: Testing Deep Learning Framework via Developer Expertise-Based Mutation](https://arxiv.org/abs/2507.04360)
*Yanzhou Mu,Juan Zhai,Chunrong Fang,Xiang Chen,Zhixiang Cao,Peiran Yang,Yinglong Zou,Tao Zheng,Zhenyu Chen*

Main category: cs.SE

TL;DR: 提出了一种名为DevMuT的新方法，通过结合开发者经验和变异操作生成深度学习模型，以检测框架中更重要的缺陷。


<details>
  <summary>Details</summary>
Motivation: 现有方法检测的缺陷多为边缘情况或被开发者忽视，DevMuT旨在识别开发者关心的关键缺陷。

Method: 采用开发者经验的变异操作和约束生成模型，模拟开发过程，覆盖模型生命周期的多个阶段。

Result: 在三个主流框架上测试，模型多样性和合法性显著提升，检测到117个缺陷，其中63个被确认，24个修复，8个高价值。

Conclusion: DevMuT能有效检测真实场景中开发者关注的缺陷，已在MindSpore社区部署。

Abstract: Deep learning (DL) frameworks are the fundamental infrastructure for various
DL applications. Framework defects can profoundly cause disastrous accidents,
thus requiring sufficient detection. In previous studies, researchers adopt DL
models as test inputs combined with mutation to generate more diverse models.
Though these studies demonstrate promising results, most detected defects are
considered trivial (i.e., either treated as edge cases or ignored by the
developers). To identify important bugs that matter to developers, we propose a
novel DL framework testing method DevMuT, which generates models by adopting
mutation operators and constraints derived from developer expertise. DevMuT
simulates developers'common operations in development and detects more diverse
defects within more stages of the DL model lifecycle (e.g., model training and
inference). We evaluate the performance of DevMuT on three widely used DL
frameworks (i.e., PyTorch, JAX, and Mind- Spore) with 29 DL models from nine
types of industry tasks. The experiment results show that DevMuT outperforms
state-of-the-art baselines: it can achieve at least 71.68% improvement on
average in the diversity of generated models and 28.20% improvement on average
in the legal rates of generated models. Moreover, DevMuT detects 117 defects,
63 of which are confirmed, 24 are fixed, and eight are of high value confirmed
by developers. Finally, DevMuT has been deployed in the MindSpore community
since December 2023. These demonstrate the effectiveness of DevMuT in detecting
defects that are close to the real scenes and are of concern to developers.

</details>


### [15] [Exploring React Library Related Questions on Stack Overflow: Answered vs. Unanswered](https://arxiv.org/abs/2507.04390)
*Vanesya Aura Ardity,Yusuf Sulistyo Nugroho,Syful Islam*

Main category: cs.SE

TL;DR: 研究分析了Stack Overflow上React相关问题的可回答性和难度因素，发现浏览量、代码片段、用户声誉等正面影响回答率，而评论数、问题长度和图片则降低回答概率。


<details>
  <summary>Details</summary>
Motivation: 尽管React流行，但许多相关问题在Stack Overflow上未被回答，研究旨在分析影响回答率和难度的因素。

Method: 通过文本挖掘和统计分析，使用逻辑回归和线性回归模型分析534,820个React相关问题。

Result: 浏览量、代码片段、用户声誉等提高回答率；评论数、问题长度和图片降低回答率。用户声誉与问题难度呈负相关。

Conclusion: 研究为技术问答平台用户提供了发布React问题时需考虑的因素，以提高回答率。

Abstract: React is a popular JavaScript framework in modern web application
development. Due to its high performance and efficiency, many developers use
this framework. Although React library offers many advantages, it is not
without its challenges. When using React library, developers often face
problems where they often seek solutions through question-and-answer forums,
such as Stack Overflow (SO). However, despite its high popularity, many
React-related questions on SO remain unanswered. Thus, this study aims to
analyze the factors associated with question answerability and difficulty
levels of React-related questions on SO. To facilitate our study, Exploratory
Data Analysis was applied to 534,820 questions, where they are filtered based
on 23 React-related tags. We implemented a quantitative approach through text
mining and statistical analysis. A logistic regression model was used to
identify attributes associated with question answerability, while a simple
linear regression model was employed to examine the correlation between user
reputations and performance difficulty scores (PD Score). The results show that
some attributes, such as number of views, code snippet inclusion, number of
lines of code, and user reputation, positively affect the likelihood of
question answerability. In contrast, the number of comments, question lengths,
and presence of images in React-related questions reduce the probability of a
question receiving responses from users. Further investigation indicates a
negative correlation between user reputations and PD Score, where reputation
increase corresponds to -0.092 reduction in PD score, signaling experienced
users tend to propose more complex technical inquiries. This study provides
insights into the characteristics of technical question-and-answer platforms,
such as SO, that users need to consider the answerability factors when posting
questions related to React.

</details>


### [16] [Learning Software Bug Reports: A Systematic Literature Review](https://arxiv.org/abs/2507.04422)
*Guoming Long,Jingzhi Gong,Hui Fang,Tao Chen*

Main category: cs.SE

TL;DR: 本文对机器学习在缺陷报告分析中的应用进行了系统性文献综述，总结了204篇论文，提炼出7个关键发现和6个未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 尽管机器学习在缺陷报告分析中日益重要，但缺乏全面综述，本文旨在填补这一空白。

Method: 通过系统性文献综述，分析了1,825篇论文，筛选出204篇进行详细分析。

Result: 总结了7个关键发现，包括常用模型、特征表示方法、预处理技术、常见任务等。

Conclusion: 提出了6个未来研究方向，为实践者提供了有价值的见解。

Abstract: The recent advancement of artificial intelligence, especially machine
learning (ML), has significantly impacted software engineering research,
including bug report analysis. ML aims to automate the understanding,
extraction, and correlation of information from bug reports. Despite its
growing importance, there has been no comprehensive review in this area. In
this paper, we present a systematic literature review covering 1,825 papers,
selecting 204 for detailed analysis. We derive seven key findings: 1) Extensive
use of CNN, LSTM, and $k$NN for bug report analysis, with advanced models like
BERT underutilized due to their complexity. 2) Word2Vec and TF-IDF are popular
for feature representation, with a rise in deep learning approaches. 3) Stop
word removal is the most common preprocessing, with structural methods rising
after 2020. 4) Eclipse and Mozilla are the most frequently evaluated software
projects. 5) Bug categorization is the most common task, followed by bug
localization and severity prediction. 6) There is increasing attention on
specific bugs like non-functional and performance bugs. 7) Common evaluation
metrics are F1-score, Recall, Precision, and Accuracy, with $k$-fold
cross-validation preferred for model evaluation. 8) Many studies lack robust
statistical tests. We also identify six promising future research directions to
provide useful insights for practitioners.

</details>


### [17] [SPIRA: Building an Intelligent System for Respiratory Insufficiency Detection](https://arxiv.org/abs/2507.04548)
*Renato Cordeiro Ferreira,Dayanne Gomes,Vitor Tamae,Francisco Wernke,Alfredo Goldman*

Main category: cs.SE

TL;DR: SPIRA是一个智能系统，通过声音检测呼吸功能不全，总结了数据收集、训练和推理的经验教训。


<details>
  <summary>Details</summary>
Motivation: 呼吸功能不全是一种血液中氧气减少的医学症状，需要高效检测方法。

Method: 构建SPIRA系统，经历两次相同架构的实现，总结了数据收集、训练和推理的挑战。

Result: 成功开发SPIRA系统，并总结了相关经验。

Conclusion: 为未来类似系统的开发提供了宝贵经验。

Abstract: Respiratory insufficiency is a medic symptom in which a person gets a reduced
amount of oxygen in the blood. This paper reports the experience of building
SPIRA: an intelligent system for detecting respiratory insufficiency from
voice. It compiles challenges faced in two succeeding implementations of the
same architecture, summarizing lessons learned on data collection, training,
and inference for future projects in similar systems.

</details>


### [18] [Testing, Evaluation, Verification and Validation (TEVV) of Digital Twins: A Comprehensive Framework](https://arxiv.org/abs/2507.04555)
*Gabriella Waters*

Main category: cs.SE

TL;DR: 本文提出了一个针对数字孪生的测试、评估、验证和验证（TEVV）框架，以解决其动态性和复杂性带来的挑战。


<details>
  <summary>Details</summary>
Motivation: 随着数字孪生技术的普及和重要性提升，确保其准确性、可靠性和伦理实施变得至关重要。

Method: 提出了一种全面的TEVV框架，专门针对数字孪生的动态和复杂特性。

Result: 该框架能够有效应对数字孪生在实时监测、预测分析和优化中的挑战。

Conclusion: TEVV框架为数字孪生的可靠性和伦理实施提供了重要支持。

Abstract: Digital twins have emerged as a powerful technology for modeling and
simulating complex systems across various domains (Fuller et al., 2020; Tao et
al., 2019). As virtual representations of physical assets, processes, or
systems, digital twins enable real-time monitoring, predictive analysis, and
optimization. However, as digital twins become more sophisticated and integral
to decision-making processes, ensuring their accuracy, reliability, and ethical
implementation is essential. This paper presents a comprehensive framework for
the Testing, Evaluation, Verification and Validation (TEVV) of digital twins to
address the unique challenges posed by these dynamic and complex virtual
models.

</details>


### [19] [Supporting Software Formal Verification with Large Language Models: An Experimental Study](https://arxiv.org/abs/2507.04857)
*Weiqi Wang,Marie Farrell,Lucas C. Cordeiro,Liping Zhao*

Main category: cs.SE

TL;DR: SpecVerify结合大型语言模型（LLM）与形式验证工具，自动从自然语言需求中推导属性，验证准确率达46.5%，优于传统方法但需人工监控。


<details>
  <summary>Details</summary>
Motivation: 解决自然语言需求难以自动转化为形式化属性的问题，提升验证灵活性和准确性。

Method: 集成Claude 3.5 Sonnet与ESBMC验证器，形成自动化工作流，并在9个Lockheed Martin的CPS系统中评估。

Result: 验证准确率46.5%，误报率低于NASA的CoCoSim，能识别传统方法遗漏的可证伪案例。

Conclusion: LLM显著降低形式验证门槛，但高质量需求文档和人工监控仍不可或缺，人机协作是关键。

Abstract: Formal methods have been employed for requirements verification for a long
time. However, it is difficult to automatically derive properties from natural
language requirements. SpecVerify addresses this challenge by integrating large
language models (LLMs) with formal verification tools, providing a more
flexible mechanism for expressing requirements. This framework combines Claude
3.5 Sonnet with the ESBMC verifier to form an automated workflow. Evaluated on
nine cyber-physical systems from Lockheed Martin, SpecVerify achieves 46.5%
verification accuracy, comparable to NASA's CoCoSim, but with lower false
positives. Our framework formulates assertions that extend beyond the
expressive power of LTL and identifies falsifiable cases that are missed by
more traditional methods. Counterexample analysis reveals CoCoSim's limitations
stemming from model connection errors and numerical approximation issues. While
SpecVerify advances verification automation, our comparative study of Claude,
ChatGPT, and Llama shows that high-quality requirements documentation and human
monitoring remain critical, as models occasionally misinterpret specifications.
Our results demonstrate that LLMs can significantly reduce the barriers to
formal verification, while highlighting the continued importance of
human-machine collaboration in achieving optimal results.

</details>


### [20] [Towards a Unifying Reference Model for Digital Twins of Cyber-Physical Systems](https://arxiv.org/abs/2507.04871)
*Jerome Pfeiffer,Jingxi Zhang,Benoit Combemale,Judith Michael,Bernhard Rumpe,Manuel Wimmer,Andreas Wortmann*

Main category: cs.SE

TL;DR: 论文提出了一种统一的数字孪生参考模型，以减少概念与工业实现之间的差距。


<details>
  <summary>Details</summary>
Motivation: 现有数字孪生的定义和参考模型过于抽象，导致理解与实施困难。

Method: 分析流行的数字孪生参考模型，并将其整合为一个更详细的统一模型。

Result: 提出了一个详细的统一参考模型，有助于数字孪生的理解和实施。

Conclusion: 该模型缩小了概念与实现之间的差距，为开发者提供了有效的实施指导。

Abstract: Digital twins are sophisticated software systems for the representation,
monitoring, and control of cyber-physical systems, including automotive,
avionics, smart manufacturing, and many more. Existing definitions and
reference models of digital twins are overly abstract, impeding their
comprehensive understanding and implementation guidance. Consequently, a
significant gap emerges between abstract concepts and their industrial
implementations. We analyze popular reference models for digital twins and
combine these into a significantly detailed unifying reference model for
digital twins that reduces the concept-implementation gap to facilitate their
engineering in industrial practice. This enhances the understanding of the
concepts of digital twins and their relationships and guides developers to
implement digital twins effectively.

</details>


### [21] [Understanding Everything as Code: A Taxonomy and Conceptual Model](https://arxiv.org/abs/2507.05100)
*Haoran Wei,Nazim Madhavji,John Steinbacher*

Main category: cs.SE

TL;DR: 该研究通过多源文献综述（MLR）分析了Everything as Code（EaC）的范围和边界，提出了包含25种实践的六层分类法和概念模型，为研究者和从业者提供了结构化指导。


<details>
  <summary>Details</summary>
Motivation: EaC作为一种新兴范式，尽管日益流行，但缺乏行业标准和学术研究明确其范围和采用指南。

Method: 采用多源文献综述（MLR）方法，综合学术和灰色文献，定量和主题分析，开发了EaC的分类法和概念模型，并通过行业专家验证。

Result: 提出了包含25种EaC实践的六层分类法和概念模型，展示了其在软件交付生命周期中的重点领域和交互关系，并提供了实际代码示例。

Conclusion: 该研究填补了EaC学术研究的空白，提供了首个全面的分类法和概念模型，增强了概念清晰度并为未来研究奠定了基础。

Abstract: Background: Everything as Code (EaC) is an emerging paradigm aiming to codify
all aspects of modern software systems. Despite its growing popularity,
comprehensive industry standards and peer-reviewed research clarifying its
scope and guiding its adoption remain scarce. Aims: This study systematically
analyzes existing knowledge and perceptions of EaC, clarifies its scope and
boundaries, and provides structured guidance for researchers and practitioners.
Method: We conducted a large-scale multivocal literature review (MLR),
synthesizing academic and grey literature sources. Findings were analyzed
quantitatively and thematically. Based on this analysis, we developed a
taxonomy and conceptual model of EaC, validated through collaboration with
industry experts. Results: The resulting taxonomy comprises 25 distinct EaC
practices organized into six layers based on industry awareness and functional
roles. The conceptual model illustrates focus areas, overlaps, and interactions
among these EaC practices within the software delivery lifecycle. Additionally,
practical code examples demonstrating the implementation of these practices
were developed in collaboration with industry experts. Conclusions: This work
addresses the current scarcity of academic discourse on EaC by providing the
first comprehensive taxonomy and conceptual model. These contributions enhance
conceptual clarity, offer actionable guidance to practitioners, and lay the
groundwork for future research in this emerging domain.

</details>


### [22] [In-Context Learning as an Effective Estimator of Functional Correctness of LLM-Generated Code](https://arxiv.org/abs/2507.05200)
*Susmita Das,Madhusudan Ghosh,Priyanka Swami,Debasis Ganguly,Gul Calikli*

Main category: cs.SE

TL;DR: 论文提出了一种基于上下文学习（ICL）的方法，用于在没有测试用例的情况下估计生成代码的功能正确性。


<details>
  <summary>Details</summary>
Motivation: 在快速应用开发中，缺乏测试用例时评估生成代码的功能正确性是一个关键问题。

Method: 采用上下文学习方法，通过提供少量功能正确的代码示例来增强现有QPP方法和零样本方法的性能。

Result: 实验表明，提供少量功能正确的代码示例能显著提升代码质量估计的性能。

Conclusion: 上下文学习方法能有效提升生成代码质量估计的准确性。

Abstract: When applying LLM-based code generation to software development projects that
follow a feature-driven or rapid application development approach, it becomes
necessary to estimate the functional correctness of the generated code in the
absence of test cases. Just as a user selects a relevant document from a ranked
list of retrieved ones, a software generation workflow requires a developer to
choose (and potentially refine) a generated solution from a ranked list of
alternative solutions, ordered by their posterior likelihoods. This implies
that estimating the quality of a ranked list -- akin to estimating "relevance"
for query performance prediction (QPP) in IR -- is also crucial for generative
software development, where quality is defined in terms of "functional
correctness". In this paper, we propose an in-context learning (ICL) based
approach for code quality estimation. Our findings demonstrate that providing
few-shot examples of functionally correct code from a training set enhances the
performance of existing QPP approaches as well as a zero-shot-based approach
for code quality estimation.

</details>


### [23] [An Investigation into Maintenance Support for Neural Networks](https://arxiv.org/abs/2507.05245)
*Fatema Tuz Zohra,Brittany Johnson*

Main category: cs.SE

TL;DR: 论文探讨了神经网络维护中的研究与实践差距，指出现有工具主要关注模型构建和训练，而缺乏对意外行为的理解和解决。


<details>
  <summary>Details</summary>
Motivation: 随着神经网络在日常生活中的应用增加，确保其质量至关重要，但传统软件工程方法在神经网络维护中存在明显不足。

Method: 通过访谈和调查收集从业者的见解，评估当前实践和传统方法的局限性。

Result: 现有工具集中于模型构建和训练，但对意外行为的理解和解决支持不足。

Conclusion: 研究旨在提供开发者视角，指出当前实践的不足，并为改进神经网络维护支持提供方向。

Abstract: As the potential for neural networks to augment our daily lives grows,
ensuring their quality through effective testing, debugging, and maintenance is
essential. This is especially the case as we acknowledge the prospects of
negative impacts from these technologies. Traditional software engineering
methods, such as testing and debugging, have proven effective in maintaining
software quality; however, they reveal significant research and practice gaps
in maintaining neural networks. In particular, there is a limited understanding
of how practitioners currently address challenges related to understanding and
mitigating undesirable behaviors in neural networks. In our ongoing research,
we explore the current state of research and practice in maintaining neural
networks by curating insights from practitioners through a preliminary study
involving interviews and supporting survey responses. Our findings thus far
indicate that existing tools primarily concentrate on building and training
models. While these tools can be beneficial, they often fall short of
supporting practitioners' understanding and addressing the underlying causes of
unexpected model behavior. By evaluating current procedures and identifying the
limitations of traditional methodologies, our study aims to offer a
developer-centric perspective on where current practices fall short and
highlight opportunities for improving maintenance support in neural networks.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [24] [Bittensor Protocol: The Bitcoin in Decentralized Artificial Intelligence? A Critical and Empirical Analysis](https://arxiv.org/abs/2507.02951)
*Elizabeth Lui,Jiahao Sun*

Main category: cs.CR

TL;DR: 本文通过比较Bittensor与比特币的代币经济学、去中心化特性、共识机制和激励结构，探讨Bittensor是否能成为去中心化人工智能领域的比特币。研究发现Bittensor存在代币和奖励集中问题，并提出协议级干预措施以优化激励和安全。


<details>
  <summary>Details</summary>
Motivation: 研究Bittensor是否能在去中心化人工智能领域达到比特币在加密货币领域的地位，并解决其代币和奖励集中的问题。

Method: 利用Bittensor所有64个活跃子网的链上数据，分析代币和奖励分布，并提出激励调整和安全改进的协议级解决方案。

Result: 发现Bittensor的代币和奖励高度集中，激励与质量不匹配。提出的解决方案包括绩效加权排放分配、复合评分和信任奖励乘数，以及代币持有上限以提升安全性。

Conclusion: Bittensor目前存在激励和安全问题，但通过提出的协议级干预措施，有望成为去中心化人工智能领域的比特币。

Abstract: This paper investigates whether Bittensor can be considered the Bitcoin of
decentralized Artificial Intelligence by directly comparing its tokenomics,
decentralization properties, consensus mechanism, and incentive structure
against those of Bitcoin. Leveraging on-chain data from all 64 active Bittensor
subnets, we first document considerable concentration in both stake and
rewards. We further show that rewards are overwhelmingly driven by stake,
highlighting a clear misalignment between quality and compensation. As a
remedy, we put forward a series of two-pronged protocol-level interventions.
For incentive realignment, our proposed solutions include performance-weighted
emission split, composite scoring, and a trust-bonus multiplier. As for
mitigating security vulnerability due to stake concentration, we propose and
empirically validate stake cap at the 88th percentile, which elevates the
median coalition size required for a 51-percent attack and remains robust
across daily, weekly, and monthly snapshots.

</details>


### [25] [A Representation Engineering Perspective on the Effectiveness of Multi-Turn Jailbreaks](https://arxiv.org/abs/2507.02956)
*Blake Bullwinkel,Mark Russinovich,Ahmed Salem,Santiago Zanella-Beguelin,Daniel Jones,Giorgio Severi,Eugenia Kim,Keegan Hines,Amanda Minnich,Yonatan Zunger,Ram Shankar Siva Kumar*

Main category: cs.CR

TL;DR: 研究发现，即使是最先进的LLMs和防御措施仍易受多轮越狱攻击。Crescendo攻击通过逐步引导模型输出看似无害的响应，最终实现有害请求。


<details>
  <summary>Details</summary>
Motivation: 多轮越狱攻击对LLM系统的安全部署构成威胁，需深入研究其机制以开发有效防御。

Method: 通过分析Crescendo攻击在模型中间表示层面的效果，研究其对安全对齐模型的影响。

Result: Crescendo攻击使模型将有害响应视为无害，尤其是在多轮对话中。单轮防御措施对此类攻击无效。

Conclusion: 需开发针对多轮越狱攻击的防御措施，填补现有防御的不足。

Abstract: Recent research has demonstrated that state-of-the-art LLMs and defenses
remain susceptible to multi-turn jailbreak attacks. These attacks require only
closed-box model access and are often easy to perform manually, posing a
significant threat to the safe and secure deployment of LLM-based systems. We
study the effectiveness of the Crescendo multi-turn jailbreak at the level of
intermediate model representations and find that safety-aligned LMs often
represent Crescendo responses as more benign than harmful, especially as the
number of conversation turns increases. Our analysis indicates that at each
turn, Crescendo prompts tend to keep model outputs in a "benign" region of
representation space, effectively tricking the model into fulfilling harmful
requests. Further, our results help explain why single-turn jailbreak defenses
like circuit breakers are generally ineffective against multi-turn attacks,
motivating the development of mitigations that address this generalization gap.

</details>


### [26] [A Novel Active Learning Approach to Label One Million Unknown Malware Variants](https://arxiv.org/abs/2507.02959)
*Ahmed Bensaoud,Jugal Kalita*

Main category: cs.CR

TL;DR: 论文提出两种主动学习方法用于恶意软件分类，结合贝叶斯理论和深度学习模型，ViT-BNN表现更优。


<details>
  <summary>Details</summary>
Motivation: 降低标注成本，通过主动学习选择最不确定的样本进行标注，结合贝叶斯理论提升模型的不确定性估计能力。

Method: 提出两种模型：Inception-V4+PCA结合多种SVM算法，以及ViT-BNN（基于Vision Transformer的贝叶斯神经网络）。

Result: 实验表明ViT-BNN在处理不确定性时更稳定和鲁棒。

Conclusion: ViT-BNN是一种先进的主动学习方法，适用于多种任务。

Abstract: Active learning for classification seeks to reduce the cost of labeling
samples by finding unlabeled examples about which the current model is least
certain and sending them to an annotator/expert to label. Bayesian theory can
provide a probabilistic view of deep neural network models by asserting a prior
distribution over model parameters and estimating the uncertainties by
posterior distribution over these parameters. This paper proposes two novel
active learning approaches to label one million malware examples belonging to
different unknown modern malware families. The first model is Inception-V4+PCA
combined with several support vector machine (SVM) algorithms (UTSVM, PSVM,
SVM-GSU, TBSVM). The second model is Vision Transformer based Bayesian Neural
Networks ViT-BNN. Our proposed ViT-BNN is a state-of-the-art active learning
approach that differs from current methods and can apply to any particular
task. The experiments demonstrate that the ViT-BNN is more stable and robust in
handling uncertainty.

</details>


### [27] [Unveiling Privacy Policy Complexity: An Exploratory Study Using Graph Mining, Machine Learning, and Natural Language Processing](https://arxiv.org/abs/2507.02968)
*Vijayalakshmi Ramasamy,Seth Barrett,Gokila Dorai,Jessica Zumbach*

Main category: cs.CR

TL;DR: 研究探讨了隐私政策可视化与图挖掘的结合，以提升用户理解和识别潜在风险。


<details>
  <summary>Details</summary>
Motivation: 隐私政策复杂难懂，用户难以理解个人数据的处理方式，需自动化工具提高透明度。

Method: 采用交互式图可视化表示政策条款，结合图挖掘算法（如t-SNE和PCA）识别关键主题。

Result: 图聚类提升了政策内容的可解释性，揭示了用户跟踪和数据共享模式。

Conclusion: 结合可视化与图挖掘的AI工具增强了隐私政策的透明度和可审计性。

Abstract: Privacy policy documents are often lengthy, complex, and difficult for
non-expert users to interpret, leading to a lack of transparency regarding the
collection, processing, and sharing of personal data. As concerns over online
privacy grow, it is essential to develop automated tools capable of analyzing
privacy policies and identifying potential risks. In this study, we explore the
potential of interactive graph visualizations to enhance user understanding of
privacy policies by representing policy terms as structured graph models. This
approach makes complex relationships more accessible and enables users to make
informed decisions about their personal data (RQ1). We also employ graph mining
algorithms to identify key themes, such as User Activity and Device
Information, using dimensionality reduction techniques like t-SNE and PCA to
assess clustering effectiveness. Our findings reveal that graph-based
clustering improves policy content interpretability. It highlights patterns in
user tracking and data sharing, which supports forensic investigations and
identifies regulatory non-compliance. This research advances AI-driven tools
for auditing privacy policies by integrating interactive visualizations with
graph mining. Enhanced transparency fosters accountability and trust.

</details>


### [28] [Reinforcement Learning for Automated Cybersecurity Penetration Testing](https://arxiv.org/abs/2507.02969)
*Daniel López-Montero,José L. Álvarez-Aldana,Alicia Morales-Martínez,Marta Gil-López,Juan M. Auñón García*

Main category: cs.CR

TL;DR: 提出了一种基于强化学习的自动化安全测试方法，用于优化漏洞检测路径并减少维护成本。


<details>
  <summary>Details</summary>
Motivation: 自动化安全测试任务，确保Web应用组件正确运行并降低维护成本。

Method: 结合强化学习和几何深度学习，利用模拟网页和网络拓扑训练智能体，优化测试路径。

Result: 开发了一种强化学习算法，能在最少步骤内最大化漏洞发现数量。

Conclusion: 该方法有效提升了Web应用安全测试的效率和效果。

Abstract: This paper aims to provide an innovative machine learning-based solution to
automate security testing tasks for web applications, ensuring the correct
functioning of all components while reducing project maintenance costs.
Reinforcement Learning is proposed to select and prioritize tools and optimize
the testing path. The presented approach utilizes a simulated webpage along
with its network topology to train the agent. Additionally, the model leverages
Geometric Deep Learning to create priors that reduce the search space and
improve learning convergence. The validation and testing process was conducted
on real-world vulnerable web pages commonly used by human hackers for learning.
As a result of this study, a reinforcement learning algorithm was developed
that maximizes the number of vulnerabilities found while minimizing the number
of steps required

</details>


### [29] [Aim High, Stay Private: Differentially Private Synthetic Data Enables Public Release of Behavioral Health Information with High Utility](https://arxiv.org/abs/2507.02971)
*Mohsen Ghasemizade,Juniper Lovato,Christopher M. Danforth,Peter Sheridan Dodds,Laura S. P. Bloomfield,Matthew Price,Team LEMURS,Joseph P. Near*

Main category: cs.CR

TL;DR: 论文探讨了差分隐私（DP）在行为健康研究中的应用，通过生成合成数据平衡隐私保护和数据实用性。


<details>
  <summary>Details</summary>
Motivation: 传统去标识化方法易受隐私攻击，DP提供了形式化的隐私保障，但需权衡隐私与数据实用性。

Method: 使用自适应迭代机制（AIM）生成DP合成数据，评估不同隐私预算（epsilon = 1-100）下的隐私与效用权衡。

Result: epsilon = 5的合成数据在保留预测效用的同时显著降低隐私风险。

Conclusion: 研究为生成多属性、多记录的隐私合成数据提供了可复现的评估框架，支持数据共享决策。

Abstract: Sharing health and behavioral data raises significant privacy concerns, as
conventional de-identification methods are susceptible to privacy attacks.
Differential Privacy (DP) provides formal guarantees against re-identification
risks, but practical implementation necessitates balancing privacy protection
and the utility of data.
  We demonstrate the use of DP to protect individuals in a real behavioral
health study, while making the data publicly available and retaining high
utility for downstream users of the data. We use the Adaptive Iterative
Mechanism (AIM) to generate DP synthetic data for Phase 1 of the Lived
Experiences Measured Using Rings Study (LEMURS). The LEMURS dataset comprises
physiological measurements from wearable devices (Oura rings) and self-reported
survey data from first-year college students. We evaluate the synthetic
datasets across a range of privacy budgets, epsilon = 1 to 100, focusing on the
trade-off between privacy and utility.
  We evaluate the utility of the synthetic data using a framework informed by
actual uses of the LEMURS dataset. Our evaluation identifies the trade-off
between privacy and utility across synthetic datasets generated with different
privacy budgets. We find that synthetic data sets with epsilon = 5 preserve
adequate predictive utility while significantly mitigating privacy risks. Our
methodology establishes a reproducible framework for evaluating the practical
impacts of epsilon on generating private synthetic datasets with numerous
attributes and records, contributing to informed decision-making in data
sharing practices.

</details>


### [30] [Are AI-Generated Fixes Secure? Analyzing LLM and Agent Patches on SWE-bench](https://arxiv.org/abs/2507.02976)
*Amirali Sajadi,Kostadin Damevski,Preetha Chatterjee*

Main category: cs.CR

TL;DR: 研究发现，LLM生成的代码补丁在真实开发环境中存在显著的安全风险，尤其是独立LLM和自主性较高的代理框架。


<details>
  <summary>Details</summary>
Motivation: 评估LLM和代理框架在真实软件开发环境中的安全性，填补现有研究的空白。

Method: 使用SWE-bench数据集中的20,000多个问题，对比独立LLM（Llama 3.3）和开发者编写的补丁，并评估三种代理框架的安全性。

Result: 独立LLM引入的漏洞数量是开发者的近9倍，代理框架在自主性较高时也易生成漏洞。漏洞多与代码量、文件数和问题描述不明确相关。

Conclusion: 上下文因素对生成代码的安全性至关重要，需开发结合代码和问题信息的风险评估方法。

Abstract: Large Language Models (LLMs) and their agentic frameworks are increasingly
adopted to automate software development tasks such as issue resolution and
program repair. While prior work has identified security risks in LLM-generated
code, most evaluations have focused on synthetic or isolated settings, leaving
open questions about the security of these systems in real-world development
contexts. In this study, we present the first large-scale security analysis of
LLM-generated patches using 20,000+ issues from the SWE-bench dataset. We
evaluate patches produced by a standalone LLM (Llama 3.3) and compare them to
developer-written patches. We also assess the security of patches generated by
three top-performing agentic frameworks (OpenHands, AutoCodeRover, HoneyComb)
on a subset of our data. Finally, we analyze a wide range of code, issue, and
project-level factors to understand the conditions under which LLMs and agents
are most likely to generate insecure code. Our findings reveal that the
standalone LLM introduces nearly 9x more new vulnerabilities than developers,
with many of these exhibiting unique patterns not found in developers' code.
Agentic workflows also generate a significant number of vulnerabilities,
particularly when granting LLMs more autonomy, potentially increasing the
likelihood of misinterpreting project context or task requirements. We find
that vulnerabilities are more likely to occur in LLM patches associated with a
higher number of files, more lines of generated code, and GitHub issues that
lack specific code snippets or information about the expected code behavior and
steps to reproduce. These results suggest that contextual factors play a
critical role in the security of the generated code and point toward the need
for proactive risk assessment methods that account for both code and
issue-level information to complement existing vulnerability detection tools.

</details>


### [31] [Deterministic Cryptographic Seed Generation via Cyclic Modular Inversion over $\mathbb{Z}/3^p\mathbb{Z}$](https://arxiv.org/abs/2507.03000)
*Michael A. Idowu*

Main category: cs.CR

TL;DR: 提出了一种基于循环模逆的确定性密码种子生成框架，通过代数可接受性生成结构化且可逆的残差序列，适用于多种密码学应用。


<details>
  <summary>Details</summary>
Motivation: 为密码学种子生成提供一种确定性、结构化和可验证的方法，以增强安全性和可审计性。

Method: 使用循环模逆和代数可接受性条件生成种子，并引入熵置信分数（ECS）评估随机性质量。

Result: 框架在嵌入式应用中表现出恒定时间执行、低侧信道泄漏和轻量级可行性。

Conclusion: 该框架作为代数可验证的熵过滤器，提升了密码学堆栈的结构完整性和可审计性。

Abstract: We present a deterministic framework for cryptographic seed generation based
on cyclic modular inversion over $\mathbb{Z}/3^p\mathbb{Z}$. The method
enforces algebraic admissibility on seed inputs via the identity $d_k \equiv
-\left(2^{k-1}\right)^{-1} \bmod 3^p$, thereby producing structured and
invertible residue sequences. This mapping yields entropy-rich, cycle-complete
seeds well-suited for cryptographic primitives such as DRBGs, KDFs, and
post-quantum schemes. To assess the quality of randomness, we introduce the
Entropy Confidence Score (ECS), a composite metric reflecting coverage,
uniformity, and modular bias. Although not a cryptographic PRNG in itself, the
framework serves as a deterministic entropy filter that conditions and
validates seed inputs prior to their use by conventional generators. Empirical
and hardware-based results confirm constant-time execution, minimal
side-channel leakage, and lightweight feasibility for embedded applications.
The framework complements existing cryptographic stacks by acting as an
algebraically verifiable entropy filter, thereby enhancing structural soundness
and auditability.

</details>


### [32] [Intrinsic Fingerprint of LLMs: Continue Training is NOT All You Need to Steal A Model!](https://arxiv.org/abs/2507.03014)
*Do-hyeon Yoon,Minsoo Chun,Thomas Allen,Hans Müller,Min Wang,Rajesh Sharma*

Main category: cs.CR

TL;DR: 论文提出了一种基于注意力参数矩阵标准差分布的鲁棒指纹方法，用于识别大语言模型的来源和版权侵权。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型训练成本增加和模型重用普及，版权和知识产权保护面临挑战，现有水印技术对持续训练不鲁棒。

Method: 利用注意力参数矩阵在不同层的标准差分布作为指纹，因其在持续训练后仍稳定。

Result: 实验验证了方法的有效性，并发现华为Pangu Pro MoE模型可能源自Qwen-2.5 14B模型。

Conclusion: 开发鲁棒指纹方法对保护大模型知识产权至关重要，持续训练无法完全掩盖模型来源。

Abstract: Large language models (LLMs) face significant copyright and intellectual
property challenges as the cost of training increases and model reuse becomes
prevalent. While watermarking techniques have been proposed to protect model
ownership, they may not be robust to continue training and development, posing
serious threats to model attribution and copyright protection. This work
introduces a simple yet effective approach for robust LLM fingerprinting based
on intrinsic model characteristics. We discover that the standard deviation
distributions of attention parameter matrices across different layers exhibit
distinctive patterns that remain stable even after extensive continued
training. These parameter distribution signatures serve as robust fingerprints
that can reliably identify model lineage and detect potential copyright
infringement. Our experimental validation across multiple model families
demonstrates the effectiveness of our method for model authentication. Notably,
our investigation uncovers evidence that a recently Pangu Pro MoE model
released by Huawei is derived from Qwen-2.5 14B model through upcycling
techniques rather than training from scratch, highlighting potential cases of
model plagiarism, copyright violation, and information fabrication. These
findings underscore the critical importance of developing robust fingerprinting
methods for protecting intellectual property in large-scale model development
and emphasize that deliberate continued training alone is insufficient to
completely obscure model origins.

</details>


### [33] [A Multi-Resolution Dynamic Game Framework for Cross-Echelon Decision-Making in Cyber Warfare](https://arxiv.org/abs/2507.03021)
*Ya-Ting Yang,Quanyan Zhu*

Main category: cs.CR

TL;DR: 提出了一种多分辨率动态博弈框架，用于解决网络防御中战术与战略层交互的建模挑战。


<details>
  <summary>Details</summary>
Motivation: 现代冲突中网络战的重要性日益增加，但战术与战略层的动态交互建模仍具挑战性。

Method: 采用多分辨率动态博弈框架，战术层用高分辨率扩展形式博弈树建模，战略层用马尔可夫博弈建模。

Result: 案例研究表明该框架能有效提升防御者的战略优势。

Conclusion: 该框架为网络防御中的多层级决策提供了可扩展的建模工具。

Abstract: Cyber warfare has become a critical dimension of modern conflict, driven by
society's increasing dependence on interconnected digital and physical
infrastructure. Effective cyber defense often requires decision-making at
different echelons, where the tactical layer focuses on detailed actions such
as techniques, tactics, and procedures, while the strategic layer addresses
long-term objectives and coordinated planning. Modeling these interactions at
different echelons remains challenging due to the dynamic, large-scale, and
interdependent nature of cyber environments. To address this, we propose a
multi-resolution dynamic game framework in which the tactical layer captures
fine-grained interactions using high-resolution extensive-form game trees,
while the strategic layer is modeled as a Markov game defined over
lower-resolution states abstracted from those game trees. This framework
supports scalable reasoning and planning across different levels of abstraction
through zoom-in and zoom-out operations that adjust the granularity of the
modeling based on operational needs. A case study demonstrates how the
framework works and its effectiveness in improving the defender's strategic
advantage.

</details>


### [34] [Improving LLM Reasoning for Vulnerability Detection via Group Relative Policy Optimization](https://arxiv.org/abs/2507.03051)
*Marco Simoni,Aleksandar Fontana,Giulio Rossolini,Andrea Saracino*

Main category: cs.CR

TL;DR: 该论文研究了基于强化学习的微调方法（GRPO）在大型语言模型（LLMs）中的应用，以改进其在软件漏洞检测中的性能和推理能力。


<details>
  <summary>Details</summary>
Motivation: 理解并改进LLMs的训练动态和推理能力，以支持其在AI安全工具（如漏洞检测）中的部署。

Method: 采用Group Relative Policy Optimization（GRPO）方法，通过结构化、基于规则的奖励信号优化LLMs的行为，并结合广泛使用的数据集（如BigVul、DiverseVul和CleanVul）重新定义优势函数和奖励信号。

Result: 实验结果表明，GRPO在泛化能力、推理能力和性能提升方面优于标准的监督微调（SFT）。

Conclusion: 基于强化学习的训练方法（如GRPO）在提升LLMs的漏洞检测性能和推理能力方面具有潜力。

Abstract: Improving and understanding the training dynamics and reasoning of Large
Language Models (LLMs) has become essential for their deployment in AI-based
security tools, such as software vulnerability detection. In this work, we
present an extensive study aimed at advancing recent RL-based finetuning
techniques for LLMs in the context of vulnerability detection.
  We start by highlighting key limitations of commonly adopted LLMs, such as
their tendency to over-predict certain types of vulnerabilities while failing
to detect others. To address this challenge, we explore the use of Group
Relative Policy Optimization (GRPO), a recent policy-gradient method, for
guiding LLM behavior through structured, rule-based rewards. We enable its
application to the vulnerability detection task by redefining its advantage
functions and reward signals using annotations from widely used datasets in the
field, including BigVul, DiverseVul, and CleanVul.
  The proposed methodology enables an extensive set of experiments, addressing
multiple research questions regarding the impact of GRPO on generalization,
reasoning capabilities, and performance improvements over standard supervised
finetuning (SFT). Our findings offer valuable insights into the potential of
RL-based training to enhance both the performance and reasoning abilities of
LLMs in the context of software vulnerability detection.

</details>


### [35] [LLM-Driven Auto Configuration for Transient IoT Device Collaboration](https://arxiv.org/abs/2507.03064)
*Hetvi Shastri,Walid A. Hanafy,Li Wu,David Irwin,Mani Srivastava,Prashant Shenoy*

Main category: cs.CR

TL;DR: CollabIoT是一个基于LLM的系统，用于在瞬态IoT环境中实现安全和无缝的设备协作，通过自动生成细粒度访问控制策略。


<details>
  <summary>Details</summary>
Motivation: 解决瞬态IoT环境中设备协作的安全性和异构性问题，同时为非专家用户简化策略配置。

Method: 采用LLM驱动的方法将用户意图转化为访问控制策略，结合能力授权和轻量级代理实现策略执行。

Result: LLM策略生成准确率达100%，设备配置时间约150毫秒，网络和控制开销极低。

Conclusion: CollabIoT在安全和效率上表现优异，适用于动态IoT环境。

Abstract: Today's Internet of Things (IoT) has evolved from simple sensing and
actuation devices to those with embedded processing and intelligent services,
enabling rich collaborations between users and their devices. However, enabling
such collaboration becomes challenging when transient devices need to interact
with host devices in temporarily visited environments. In such cases,
fine-grained access control policies are necessary to ensure secure
interactions; however, manually implementing them is often impractical for
non-expert users. Moreover, at run-time, the system must automatically
configure the devices and enforce such fine-grained access control rules.
Additionally, the system must address the heterogeneity of devices.
  In this paper, we present CollabIoT, a system that enables secure and
seamless device collaboration in transient IoT environments. CollabIoT employs
a Large language Model (LLM)-driven approach to convert users' high-level
intents to fine-grained access control policies. To support secure and seamless
device collaboration, CollabIoT adopts capability-based access control for
authorization and uses lightweight proxies for policy enforcement, providing
hardware-independent abstractions.
  We implement a prototype of CollabIoT's policy generation and auto
configuration pipelines and evaluate its efficacy on an IoT testbed and in
large-scale emulated environments. We show that our LLM-based policy generation
pipeline is able to generate functional and correct policies with 100%
accuracy. At runtime, our evaluation shows that our system configures new
devices in ~150 ms, and our proxy-based data plane incurs network overheads of
up to 2 ms and access control overheads up to 0.3 ms.

</details>


### [36] [Holographic Projection and Cyber Attack Surface: A Physical Analogy for Digital Security](https://arxiv.org/abs/2507.03136)
*Ricardo Queiroz de Araujo Fernandes,Anderson Santos,Daniel Maier de Carvalho,André Luiz Bandeira Molina*

Main category: cs.CR

TL;DR: 论文探讨了全息原理与数字安全中攻击面的类比，提出了一种新的视角来理解复杂基础设施的漏洞。


<details>
  <summary>Details</summary>
Motivation: 通过理论物理中的全息原理（如黑洞熵和AdS/CFT对偶性）来类比数字安全中的攻击面，揭示内部架构的安全状态如何通过外部接口反映。

Method: 利用黑洞事件视界与攻击面的类比，提出攻击面减少、持续扫描（如OWASP ZAP和Greenbone OpenVAS）和零信任架构等策略。

Result: 该框架为数字安全提供了独特的视角，并强调了边界防御在保护内部基础设施中的重要性。

Conclusion: 全息原理与攻击面的类比不仅丰富了数字安全的理论基础，还为实践中的防御策略提供了指导。

Abstract: This article presents an in-depth exploration of the analogy between the
Holographic Principle in theoretical physics and cyber attack surfaces in
digital security. Building on concepts such as black hole entropy and AdS/CFT
duality, it highlights how complex infrastructures project their
vulnerabilities onto their external interfaces. The paper draws a parallel
between a black hole's event horizon, which encodes all internal information,
and the attack surface, which reflects the internal architecture's security
posture. Additionally, the article outlines how this conceptual framework can
guide cybersecurity practices, emphasizing strategies such as attack surface
reduction, continuous scanning with tools like OWASP ZAP and Greenbone OpenVAS,
and the implementation of Zero Trust Architecture. This analogy not only
provides a unique perspective on digital security but also underscores the
critical importance of boundary-level defenses in protecting vast internal
infrastructures.

</details>


### [37] [On Jailbreaking Quantized Language Models Through Fault Injection Attacks](https://arxiv.org/abs/2507.03236)
*Noureldin Zahran,Ahmad Tahmasivand,Ihsen Alouani,Khaled Khasawneh,Mohammed E. Fouda*

Main category: cs.CR

TL;DR: 本文研究了低精度量化对语言模型安全对齐的影响，提出了梯度引导攻击方法，发现不同量化方案对攻击成功率有显著影响。


<details>
  <summary>Details</summary>
Motivation: 语言模型的安全对齐至关重要，但直接参数操纵攻击可能破坏其完整性。本文旨在探讨低精度量化对攻击效果的影响。

Method: 提出了梯度引导攻击方法，包括渐进位级搜索算法和单词级攻击，并在不同量化方案下进行评估。

Result: FP8量化模型表现出较高的抗攻击性，而INT4和INT8模型在某些情况下仍易受攻击。攻击在FP16模型上成功率高，但量化后攻击效果显著降低。

Conclusion: 低精度量化（尤其是FP8）增加了攻击难度，但仍存在漏洞，特别是在攻击后量化的情况下。

Abstract: The safety alignment of Language Models (LMs) is a critical concern, yet
their integrity can be challenged by direct parameter manipulation attacks,
such as those potentially induced by fault injection. As LMs are increasingly
deployed using low-precision quantization for efficiency, this paper
investigates the efficacy of such attacks for jailbreaking aligned LMs across
different quantization schemes. We propose gradient-guided attacks, including a
tailored progressive bit-level search algorithm introduced herein and a
comparative word-level (single weight update) attack. Our evaluation on
Llama-3.2-3B, Phi-4-mini, and Llama-3-8B across FP16 (baseline), and
weight-only quantization (FP8, INT8, INT4) reveals that quantization
significantly influences attack success. While attacks readily achieve high
success (>80\% Attack Success Rate, ASR) on FP16 models, within an attack
budget of 25 perturbations, FP8 and INT8 models exhibit ASRs below 20\% and
50\%, respectively. Increasing the perturbation budget up to 150 bit-flips, FP8
models maintained ASR below 65\%, demonstrating some resilience compared to
INT8 and INT4 models that have high ASR. In addition, analysis of perturbation
locations revealed differing architectural targets across quantization schemes,
with (FP16, INT4) and (INT8, FP8) showing similar characteristics. Besides,
jailbreaks induced in FP16 models were highly transferable to subsequent
FP8/INT8 quantization (<5\% ASR difference), though INT4 significantly reduced
transferred ASR (avg. 35\% drop). These findings highlight that while common
quantization schemes, particularly FP8, increase the difficulty of direct
parameter manipulation jailbreaks, vulnerabilities can still persist,
especially through post-attack quantization.

</details>


### [38] [Novel Blockchain-based Protocols for Electronic Voting and Auctions](https://arxiv.org/abs/2507.03258)
*Zhaorun Lin*

Main category: cs.CR

TL;DR: 论文提出了一种基于区块链的匿名投票协议Blind Vote和一种隐私保护的拍卖算法，提高了安全性和效率。


<details>
  <summary>Details</summary>
Motivation: 研究去中心化协议在区块链上的应用，特别是智能合约，以提升安全性和效率。

Method: 使用算法和密码学工具，提出Blind Vote和隐私拍卖算法，基于Chaum盲签名和智能合约。

Result: Blind Vote比现有方法更节省gas，拍卖算法保护隐私且高效。

Conclusion: 提出的方法为区块链投票和拍卖提供了更高效、安全的解决方案。

Abstract: Programmable blockchains have long been a hot research topic given their
tremendous use in decentralized applications. Smart contracts, using
blockchains as their underlying technology, inherit the desired properties such
as verifiability, immutability, and transparency, which make it a great suit in
trustless environments.
  In this thesis, we consider several decentralized protocols to be built on
blockchains, specifically using smart contracts on Ethereum. We used
algorithmic and cryptographic tools in our implementations to further improve
the level of security and efficiency beyond the state-of-the-art works. We
proposed a new approach called Blind Vote, which is an untraceable, secure,
efficient, secrecy-preserving, and fully on-chain electronic voting protocol
based on the well-known concept of Chaum's blind signatures. We illustrate that
our approach achieves the same security guarantees as previous methods such as
Tornado Vote [1], while consuming significantly less gas. Thus, we provide a
cheaper and considerably more gas-efficient alternative for anonymous
blockchain-based voting. On the other hand, we propose a new family of
algorithms for private, trustless auctions that protect bidder identities and
bid values while remaining practical for smart contract execution. We ensure
trustlessness by running the auction logic in a smart contract, thereby
eliminating reliance on any single trusted party. This approach prevents bid
tampering, front-running, and collusion by enforcing immutability and
decentralized verification of bids. The resulting protocol uniquely combines
efficiency, trustlessness, and enduring bid privacy, offering a scalable and
secure solution for blockchain-based marketplaces and other decentralized
applications.

</details>


### [39] [Securing Transformer-based AI Execution via Unified TEE and Crypto-protected Accelerators](https://arxiv.org/abs/2507.03278)
*Jiaqi Xue,Yifei Zhao,Mengxin Zheng,Xun Chen,Fan Yao,Yan Solihin,Qian Lou*

Main category: cs.CR

TL;DR: TwinShield框架解决了Transformer模型在混合可信执行环境（TEE）和加速器系统中安全推理的问题，实现了数据和模型的双重保护，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 由于Transformer模型（如LLMs）在安全关键领域的广泛应用及其高价值，如何在MLaaS中保护模型和数据安全成为重要问题。现有TEE方案性能不足，而混合方案无法安全卸载关键操作。

Method: 提出TwinShield框架，通过安全卸载部分计算到GPU，同时确保Attention和SoftMax等关键操作的安全性。

Result: TwinShield将87%的计算卸载到GPU，性能提升4.0x-6.1x。

Conclusion: TwinShield为Transformer模型在混合系统中的安全推理提供了高效解决方案。

Abstract: Recent advances in Transformer models, e.g., large language models (LLMs),
have brought tremendous breakthroughs in various artificial intelligence (AI)
tasks, leading to their wide applications in many security-critical domains.
Due to their unprecedented scale and prohibitively high development cost, these
models have become highly valuable intellectual property for AI stakeholders
and are increasingly deployed via machine learning as a service (MLaaS).
However, MLaaS often runs on untrusted cloud infrastructure, exposing data and
models to potential breaches. Mainstream protection mechanisms leverage trusted
execution environments (TEEs) where confidentiality and integrity for secretive
data are shielded using hardware-based encryption and integrity checking.
Unfortunately, running model inference entirely within TEEs is subject to
non-trivial slowdown, which is further exacerbated in LLMs due to the
substantial computation and memory footprint involved. Recent studies reveal
that the hybrid TEE-based scheme offloading partial model inference operations
to the untrusted accelerators (e.g., GPU) is a promising solution. However,
prior offloading schemes fail to ensure dual protection of data and model in
Transformer inference, as they cannot securely offload critical operations,
i.e., Attention and SoftMax, forcing these computations to remain confined
within TEEs. To address these challenges, we propose TwinShield, a framework
enabling secure Transformer inference in heterogeneous TEE and accelerator
systems with dual protection for both model and data. TwinShield offloads ~87%
of computation to GPUs and delivers 4.0x - 6.1x speedups over previous
approaches across various Transformer models.

</details>


### [40] [A Note on Single-Cut Full-Open Protocols](https://arxiv.org/abs/2507.03323)
*Kazumasa Shinagawa,Koji Nuida*

Main category: cs.CR

TL;DR: 本文提出了三种单切全开协议，用于三变量和四变量函数的卡牌密码学实现。


<details>
  <summary>Details</summary>
Motivation: 研究如何通过随机切牌和全开操作实现卡牌密码学中的安全计算协议。

Method: 提出两种三变量函数和一种四变量函数的单切全开协议。

Result: 成功设计并验证了三种协议，适用于不同变量数量的函数。

Conclusion: 单切全开协议在卡牌密码学中具有潜力，可扩展至更多变量函数。

Abstract: Card-based cryptography is a research area that realizes cryptographic
protocols such as secure computation by applying shuffles to sequences of cards
that encode input values. A single-cut full-open protocol is one that obtains
an output value by applying a random cut to an input sequence of cards, after
which all cards are opened. In this paper, we propose three single-cut
full-open protocols: two protocols for three-variable functions and one
protocol for a four-variable function.

</details>


### [41] [Securing Mixed Rust with Hardware Capabilities](https://arxiv.org/abs/2507.03344)
*Jason Zhijingcheng Yu,Fangqi Han,Kaustab Choudhury,Trevor E. Carlson,Prateek Saxena*

Main category: cs.CR

TL;DR: CapsLock是一种运行时安全机制，用于检测混合Rust代码中的Rust原则违规，通过能力硬件抽象提供内存安全。


<details>
  <summary>Details</summary>
Motivation: Rust编译器无法静态检查混合代码（如不安全Rust、FFI和内联汇编）中的Rust原则，导致安全漏洞。

Method: 提出CapsLock机制，基于能力硬件抽象，采用“使用即撤销”设计，自动提供时空内存安全。

Result: 原型在QEMU上实现，兼容99.7%的流行crate测试用例，发现8个未知漏洞。

Conclusion: CapsLock首次实现跨语言的Rust原则强制执行，高效且兼容现有代码。

Abstract: The Rust programming language enforces three basic Rust principles, namely
ownership, borrowing, and AXM (Aliasing Xor Mutability) to prevent security
bugs such as memory safety violations and data races. However, Rust projects
often have mixed code, i.e., code that also uses unsafe Rust, FFI (Foreign
Function Interfaces), and inline assembly for low-level control. The Rust
compiler is unable to statically enforce Rust principles in mixed Rust code
which can lead to many security vulnerabilities. In this paper, we propose
CapsLock, a security enforcement mechanism that can run at the level of machine
code and detect Rust principle violations at run-time in mixed code. CapsLock
is kept simple enough to be implemented into recent capability-based hardware
abstractions that provide low-cost spatial memory safety. CapsLock introduces a
novel revoke-on-use abstraction for capability-based designs, wherein accessing
a memory object via a capability implicitly invalidates certain other
capabilities pointing to it, thereby also providing temporal memory safety
automatically, without requiring software to explicitly specify such
invalidation. Thus, CapsLock is the first mechanism capable of providing
cross-language enforcement of Rust principles. We implemented a prototype of
CapsLock on QEMU. Evaluation results show that CapsLock is highly compatible
with existing Rust code (passing 99.7% of the built-in test cases of the 100
most popular crates) and flags Rust principle violations in real-world Rust
projects that use FFI or inline assembly. We discovered 8 previously unknown
bugs in such crates in our experiments.

</details>


### [42] [Accelerating Private Heavy Hitter Detection on Continual Observation Streams](https://arxiv.org/abs/2507.03361)
*Rayne Holland*

Main category: cs.CR

TL;DR: 本文提出了一种基于懒更新的差分隐私草图技术，显著降低了持续观察模型中的计算开销，同时保持了隐私和实用性。


<details>
  <summary>Details</summary>
Motivation: 解决持续观察模型中差分隐私频率估计和重击检测的高计算成本问题，特别是在大域情况下。

Method: 引入基于懒更新的差分隐私草图技术，仅扰动和更新草图的一小部分，减少计算开销。

Result: 频率估计的更新时间提高了$O(w)$倍，重击检测的每步复杂度从$\Omega(|U|)$降至$O(d \log w)$，实验显示吞吐量提高了250倍。

Conclusion: 该方法使差分隐私在实时持续观察应用中更加实用。

Abstract: Differentially private frequency estimation and heavy hitter detection are
core problems in the private analysis of data streams. Two models are typically
considered: the one-pass model, which outputs results only at the end of the
stream, and the continual observation model, which requires releasing private
summaries at every time step. While the one-pass model allows more efficient
solutions, continual observation better reflects scenarios where timely and
ongoing insights are critical.
  In the one-pass setting, sketches have proven to be an effective tool for
differentially private frequency analysis, as they can be privatized by a
single injection of calibrated noise. In contrast, existing methods in the
continual observation model add fresh noise to the entire sketch at every step,
incurring high computational costs. This challenge is particularly acute for
heavy hitter detection, where current approaches often require querying every
item in the universe at each step, resulting in untenable per-update costs for
large domains.
  To overcome these limitations, we introduce a new differentially private
sketching technique based on lazy updates, which perturbs and updates only a
small, rotating part of the output sketch at each time step. This significantly
reduces computational overhead while maintaining strong privacy and utility
guarantees. In comparison to prior art, for frequency estimation, our method
improves the update time by a factor of $O(w)$ for sketches of dimension $d
\times w$; for heavy hitter detection, it reduces per-update complexity from
$\Omega(|U|)$ to $O(d \log w)$, where $U$ is the input domain. Experiments show
a increase in throughput by a factor of~$250$, making differential privacy more
practical for real-time, continual observation, applications.

</details>


### [43] [Breaking the Bulkhead: Demystifying Cross-Namespace Reference Vulnerabilities in Kubernetes Operators](https://arxiv.org/abs/2507.03387)
*Andong Chen,Zhaoxuan Jin,Ziyi Guo,Yan Chen*

Main category: cs.CR

TL;DR: 论文首次系统研究了Kubernetes Operators的安全漏洞，特别是跨命名空间引用漏洞，发现14%的Operators存在潜在风险，并提出了缓解工具。


<details>
  <summary>Details</summary>
Motivation: Kubernetes Operators虽然简化了DevOps流程，但因其需要高权限和跨命名空间操作，引入了新的安全风险，尤其是跨命名空间引用漏洞。

Method: 通过大规模测量和两种攻击策略，验证了漏洞的存在，并开发了静态分析工具。

Result: 发现14%的Operators存在漏洞，已报告开发者并确认7个漏洞和6个CVE。

Conclusion: 强调了Kubernetes Operators安全实践的重要性，并开源了静态分析工具以帮助生态系统。

Abstract: Kubernetes Operators, automated tools designed to manage application
lifecycles within Kubernetes clusters, extend the functionalities of
Kubernetes, and reduce the operational burden on human engineers. While
Operators significantly simplify DevOps workflows, they introduce new security
risks. In particular, Kubernetes enforces namespace isolation to separate
workloads and limit user access, ensuring that users can only interact with
resources within their authorized namespaces. However, Kubernetes Operators
often demand elevated privileges and may interact with resources across
multiple namespaces. This introduces a new class of vulnerabilities, the
Cross-Namespace Reference Vulnerability. The root cause lies in the mismatch
between the declared scope of resources and the implemented scope of the
Operator logic, resulting in Kubernetes being unable to properly isolate the
namespace. Leveraging such vulnerability, an adversary with limited access to a
single authorized namespace may exploit the Operator to perform operations
affecting other unauthorized namespaces, causing Privilege Escalation and
further impacts. To the best of our knowledge, this paper is the first to
systematically investigate the security vulnerability of Kubernetes Operators.
We present Cross-Namespace Reference Vulnerability with two strategies,
demonstrating how an attacker can bypass namespace isolation. Through
large-scale measurements, we found that over 14% of Operators in the wild are
potentially vulnerable. Our findings have been reported to the relevant
developers, resulting in 7 confirmations and 6 CVEs by the time of submission,
affecting vendors including ****** and ******, highlighting the critical need
for enhanced security practices in Kubernetes Operators. To mitigate it, we
also open-source the static analysis suite to benefit the ecosystem.

</details>


### [44] [Evaluating the Evaluators: Trust in Adversarial Robustness Tests](https://arxiv.org/abs/2507.03450)
*Antonio Emanuele Cinà,Maura Pintor,Luca Demetrio,Ambra Demontis,Battista Biggio,Fabio Roli*

Main category: cs.CR

TL;DR: AttackBench是一个标准化评估梯度对抗攻击效果的基准框架，旨在解决现有评估方法的不一致性和不可靠性问题。


<details>
  <summary>Details</summary>
Motivation: 现有对抗攻击评估方法存在不一致和不可靠的问题，导致结果偏差和虚假的安全感。

Method: 开发AttackBench框架，通过标准化和可复现条件评估梯度攻击效果，并引入新的最优性指标对攻击实现进行排名。

Result: AttackBench提供了一个可靠的评估工具，帮助识别最有效的攻击方法，并支持持续更新。

Conclusion: AttackBench为对抗攻击的鲁棒性验证提供了标准化和可靠的基础。

Abstract: Despite significant progress in designing powerful adversarial evasion
attacks for robustness verification, the evaluation of these methods often
remains inconsistent and unreliable. Many assessments rely on mismatched
models, unverified implementations, and uneven computational budgets, which can
lead to biased results and a false sense of security. Consequently, robustness
claims built on such flawed testing protocols may be misleading and give a
false sense of security. As a concrete step toward improving evaluation
reliability, we present AttackBench, a benchmark framework developed to assess
the effectiveness of gradient-based attacks under standardized and reproducible
conditions. AttackBench serves as an evaluation tool that ranks existing attack
implementations based on a novel optimality metric, which enables researchers
and practitioners to identify the most reliable and effective attack for use in
subsequent robustness evaluations. The framework enforces consistent testing
conditions and enables continuous updates, making it a reliable foundation for
robustness verification.

</details>


### [45] [VLAI: A RoBERTa-Based Model for Automated Vulnerability Severity Classification](https://arxiv.org/abs/2507.03607)
*Cédric Bonhomme,Alexandre Dulaunoy*

Main category: cs.CR

TL;DR: VLAI是一个基于Transformer的模型，用于直接从文本描述预测软件漏洞的严重性级别，准确率超过82%。


<details>
  <summary>Details</summary>
Motivation: 提高漏洞严重性分类的效率和一致性，减少手动CVSS评分的工作量。

Method: 基于RoBERTa模型，在60多万个真实漏洞数据上进行微调。

Result: 模型准确率达到82%以上，并集成到Vulnerability-Lookup服务中。

Conclusion: VLAI为漏洞管理提供了高效且一致的自动化解决方案。

Abstract: This paper presents VLAI, a transformer-based model that predicts software
vulnerability severity levels directly from text descriptions. Built on
RoBERTa, VLAI is fine-tuned on over 600,000 real-world vulnerabilities and
achieves over 82% accuracy in predicting severity categories, enabling faster
and more consistent triage ahead of manual CVSS scoring. The model and dataset
are open-source and integrated into the Vulnerability-Lookup service.

</details>


### [46] [RVISmith: Fuzzing Compilers for RVV Intrinsics](https://arxiv.org/abs/2507.03773)
*Yibo He,Cunjian Huang,Xianmiao Qu,Hongdeng Chen,Wei Yang,Tao Xie*

Main category: cs.CR

TL;DR: RVISmith是一个随机化模糊测试工具，用于检测编译器中的SIMD内部函数错误，通过生成多样化的RVV内部函数调用序列，显著提高了覆盖率和检测效率。


<details>
  <summary>Details</summary>
Motivation: 现代编译器对SIMD内部函数的支持可能存在错误，导致安全隐患，需要一种高效的方法来检测这些错误。

Method: 设计RVISmith生成定义良好的C程序，包含多样化的RVV内部函数调用序列，并通过差分测试比较不同编译器的结果。

Result: RVISmith的覆盖率比现有工具高11.5倍，检测并报告了13个未知错误，其中10个被确认，3个已修复。

Conclusion: RVISmith是一种有效的工具，能够显著提升编译器对SIMD内部函数的错误检测能力。

Abstract: Modern processors are equipped with single instruction multiple data (SIMD)
instructions for fine-grained data parallelism. Compiler auto-vectorization
techniques that target SIMD instructions face performance limitations due to
insufficient information available at compile time, requiring programmers to
manually manipulate SIMD instructions. SIMD intrinsics, a type of built-in
function provided by modern compilers, enable programmers to manipulate SIMD
instructions within high-level programming languages. Bugs in compilers for
SIMD intrinsics can introduce potential threats to software security, producing
unintended calculation results, data loss, program crashes, etc.
  To detect bugs in compilers for SIMD intrinsics, we propose RVISmith, a
randomized fuzzer that generates well-defined C programs that include various
invocation sequences of RVV (RISC-V Vector Extension) intrinsics. We design
RVISmith to achieve the following objectives: (i) achieving high intrinsic
coverage, (ii) improving sequence variety, and (iii) without known undefined
behaviors. We implement RVISmith based on the ratified RVV intrinsic
specification and evaluate our approach with three modern compilers: GCC, LLVM,
and XuanTie. Experimental results show that RVISmith achieves 11.5 times higher
intrinsic coverage than the state-of-the-art fuzzer for RVV intrinsics. By
differential testing that compares results across different compilers,
optimizations, and equivalent programs, we detect and report 13 previously
unknown bugs of the three compilers under test to date. Of these bugs, 10 are
confirmed and another 3 are fixed by the compiler developers.

</details>


### [47] [Blackbox Dataset Inference for LLM](https://arxiv.org/abs/2507.03619)
*Ruikai Zhou,Kang Yang,Xun Chen,Wendy Hui Wang,Guanhong Tao,Jun Xu*

Main category: cs.CR

TL;DR: 本文提出了一种仅需黑盒访问目标模型的新方法，用于检测模型是否使用了特定数据集进行训练，解决了现有方法依赖灰盒访问的问题。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLM）训练可能涉及个人隐私和版权问题，现有数据集推断方法依赖灰盒访问，实用性受限。

Method: 通过构建两组本地参考模型（一组使用目标数据集训练，另一组不使用），比较目标模型与参考模型的相似性来判断数据集使用情况。

Result: 在真实LLM上的评估表明，该方法在所有设置下均具有高准确性，并能抵抗绕过尝试。

Conclusion: 该方法为数据集推断提供了更实用的黑盒解决方案，适用于实际部署的LLM。

Abstract: Today, the training of large language models (LLMs) can involve personally
identifiable information and copyrighted material, incurring dataset misuse. To
mitigate the problem of dataset misuse, this paper explores \textit{dataset
inference}, which aims to detect if a suspect model $\mathcal{M}$ used a victim
dataset $\mathcal{D}$ in training. Previous research tackles dataset inference
by aggregating results of membership inference attacks (MIAs) -- methods to
determine whether individual samples are a part of the training dataset.
However, restricted by the low accuracy of MIAs, previous research mandates
grey-box access to $\mathcal{M}$ to get intermediate outputs (probabilities,
loss, perplexity, etc.) for obtaining satisfactory results. This leads to
reduced practicality, as LLMs, especially those deployed for profits, have
limited incentives to return the intermediate outputs.
  In this paper, we propose a new method of dataset inference with only
black-box access to the target model (i.e., assuming only the text-based
responses of the target model are available). Our method is enabled by two sets
of locally built reference models, one set involving $\mathcal{D}$ in training
and the other not. By measuring which set of reference model $\mathcal{M}$ is
closer to, we determine if $\mathcal{M}$ used $\mathcal{D}$ for training.
Evaluations of real-world LLMs in the wild show that our method offers high
accuracy in all settings and presents robustness against bypassing attempts.

</details>


### [48] [Rethinking and Exploring String-Based Malware Family Classification in the Era of LLMs and RAG](https://arxiv.org/abs/2507.04055)
*Yufan Chen,Daoyuan Wu,Juantao Zhong,Zicheng Zhang,Debin Gao,Shuai Wang,Yingjiu Li,Ning Liu*

Main category: cs.CR

TL;DR: 论文探讨了利用传统二进制字符串特征在大型语言模型（LLM）和检索增强生成（RAG）时代进行恶意软件家族分类（MFC）的可行性，并提出了基于家族特定字符串（FSS）特征的方法。


<details>
  <summary>Details</summary>
Motivation: 准确的恶意软件家族分类可助力自动化样本标记和理解，尤其是在VirusTotal等平台上每日产生大量数据的情况下。

Method: 研究利用FSS特征类似RAG的方式，开发了一个评估框架，涵盖67个家族的4,347个样本，分析了2,500万字符串，并进行了消融实验。

Result: 通过详细实验评估了四个主要模块中不同设计选择的影响。

Conclusion: 研究表明FSS特征在MFC中具有潜力，尤其是在LLM和RAG的背景下。

Abstract: Malware Family Classification (MFC) aims to identify the fine-grained family
(e.g., GuLoader or BitRAT) to which a potential malware sample belongs, in
contrast to malware detection or sample classification that predicts only an
Yes/No. Accurate family identification can greatly facilitate automated sample
labeling and understanding on crowdsourced malware analysis platforms such as
VirusTotal and MalwareBazaar, which generate vast amounts of data daily. In
this paper, we explore and assess the feasibility of using traditional binary
string features for MFC in the new era of large language models (LLMs) and
Retrieval-Augmented Generation (RAG). Specifically, we investigate how
Family-Specific String (FSS) features could be utilized in a manner similar to
RAG to facilitate MFC. To this end, we develop a curated evaluation framework
covering 4,347 samples from 67 malware families, extract and analyze over 25
million strings, and conduct detailed ablation studies to assess the impact of
different design choices in four major modules.

</details>


### [49] [SecureT2I: No More Unauthorized Manipulation on AI Generated Images from Prompts](https://arxiv.org/abs/2507.03636)
*Xiaodong Wu,Xiangman Li,Qi Li,Jianbing Ni,Rongxing Lu*

Main category: cs.CR

TL;DR: SecureT2I框架通过轻量级微调防止扩散模型中的未经授权编辑，分为允许集和禁止集，前者保持高质量编辑，后者生成模糊或语义模糊的输出。


<details>
  <summary>Details</summary>
Motivation: 解决基于文本引导的图像编辑在扩散模型中可能引发的伦理和版权问题，防止未经授权的修改。

Method: 将图像分为允许集和禁止集，设计不同的训练目标和损失函数，确保禁止集的输出模糊或语义模糊。

Result: 实验表明SecureT2I能有效降低禁止集的编辑质量，同时保持允许集的性能，且泛化能力优于基线方法。

Conclusion: SecureT2I是一种有效的安全框架，适用于扩散模型，能在保护版权的同时保持编辑灵活性。

Abstract: Text-guided image manipulation with diffusion models enables flexible and
precise editing based on prompts, but raises ethical and copyright concerns due
to potential unauthorized modifications. To address this, we propose SecureT2I,
a secure framework designed to prevent unauthorized editing in diffusion-based
generative models. SecureT2I is compatible with both general-purpose and
domain-specific models and can be integrated via lightweight fine-tuning
without architectural changes. We categorize images into a permit set and a
forbid set based on editing permissions. For the permit set, the model learns
to perform high-quality manipulations as usual. For the forbid set, we
introduce training objectives that encourage vague or semantically ambiguous
outputs (e.g., blurred images), thereby suppressing meaningful edits. The core
challenge is to block unauthorized editing while preserving editing quality for
permitted inputs. To this end, we design separate loss functions that guide
selective editing behavior. Extensive experiments across multiple datasets and
models show that SecureT2I effectively degrades manipulation quality on
forbidden images while maintaining performance on permitted ones. We also
evaluate generalization to unseen inputs and find that SecureT2I consistently
outperforms baselines. Additionally, we analyze different vagueness strategies
and find that resize-based degradation offers the best trade-off for secure
manipulation control.

</details>


### [50] [When There Is No Decoder: Removing Watermarks from Stable Diffusion Models in a No-box Setting](https://arxiv.org/abs/2507.03646)
*Xiaodong Wu,Tianyi Tang,Xiangman Li,Jianbing Ni,Yong Yu*

Main category: cs.CR

TL;DR: 论文探讨了模型特定水印技术的鲁棒性，发现其对抗模糊和微调攻击时表现脆弱，检测准确率降至47.92%。


<details>
  <summary>Details</summary>
Motivation: 当前水印技术在对抗攻击下的鲁棒性尚未充分研究，需评估其有效性。

Method: 研究模型特定水印技术，提出三种无盒攻击策略（边缘预测、模糊和微调攻击），并进行消融实验。

Result: 水印对基本攻击（如边缘预测）表现稳健，但对模糊和微调攻击脆弱；最佳攻击使检测准确率降至47.92%。

Conclusion: 现有水印防御方法在无盒攻击下效果有限，需进一步改进。

Abstract: Watermarking has emerged as a promising solution to counter harmful or
deceptive AI-generated content by embedding hidden identifiers that trace
content origins. However, the robustness of current watermarking techniques is
still largely unexplored, raising critical questions about their effectiveness
against adversarial attacks. To address this gap, we examine the robustness of
model-specific watermarking, where watermark embedding is integrated with
text-to-image generation in models like latent diffusion models. We introduce
three attack strategies: edge prediction-based, box blurring, and
fine-tuning-based attacks in a no-box setting, where an attacker does not
require access to the ground-truth watermark decoder. Our findings reveal that
while model-specific watermarking is resilient against basic evasion attempts,
such as edge prediction, it is notably vulnerable to blurring and
fine-tuning-based attacks. Our best-performing attack achieves a reduction in
watermark detection accuracy to approximately 47.92\%. Additionally, we perform
an ablation study on factors like message length, kernel size and decoder
depth, identifying critical parameters influencing the fine-tuning attack's
success. Finally, we assess several advanced watermarking defenses, finding
that even the most robust methods, such as multi-label smoothing, result in
watermark extraction accuracy that falls below an acceptable level when
subjected to our no-box attacks.

</details>


### [51] [Willchain: Decentralized, Privacy-Preserving, Self-Executing, Digital Wills](https://arxiv.org/abs/2507.03694)
*Jovonni L. PHarr*

Main category: cs.CR

TL;DR: 提出了一种新型去中心化数字遗产规划协议，结合分布式计算和密码学，支持跨链通信和隐私保护，无需转移资金即可安全分配数字资产。


<details>
  <summary>Details</summary>
Motivation: 解决传统数字遗产规划中的隐私、安全和跨链兼容性问题，利用区块链技术革新法律和个人领域。

Method: 采用现代密码学原语和跨链通信技术，开发基于智能合约的层1协议，支持异构链的互操作性和用户友好的交互模型。

Result: 实现了安全、隐私的数字资产分配，开发了专用无许可区块链，展示了区块链在遗产规划中的潜力。

Conclusion: 该协议为数字遗产规划行业带来变革，展示了区块链技术在传统法律和个人领域的革命性潜力。

Abstract: This work presents a novel decentralized protocol for digital estate planning
that integrates advances distributed computing, and cryptography. The original
proof-of-concept was constructed using purely solidity contracts. Since then,
we have enhanced the implementation into a layer-1 protocol that uses modern
interchain communication to connect several heterogeneous chain types. A key
contribution of this research is the implementation of several modern
cryptographic primitives to support various forms of claims for information
validation. These primitives introduce an unmatched level of privacy to the
process of digital inheritance. We also demonstrate on a set of heterogeneous
smart contracts, following the same spec, on each chain to serve as entry
points, gateways, or bridge contracts that are invoked via a path from the will
module on our protocol, to the contract. This ensures a fair and secure
distribution of digital assets in accordance with the wishes of the decedent
without the requirement of moving their funds. This research further extends
its innovations with a user interaction model, featuring a check-in system and
account abstraction process, which enhances flexibility and user-friendliness
without compromising on security. By developing a dedicated permissionless
blockchain that is secured by a network of validators, and interchain relayers,
the proposed protocol signifies a transformation in the digital estate planning
industry and illustrates the potential of blockchain technology in
revolutionizing traditional legal and personal spheres. Implementing a
cryptoeconomic network at the core of inheritance planning allows for unique
incentive compatible economic mechanisms to be constructed.

</details>


### [52] [MalVol-25: A Diverse, Labelled and Detailed Volatile Memory Dataset for Malware Detection and Response Testing and Validation](https://arxiv.org/abs/2507.03993)
*Dipo Dunsin,Mohamed Chahine Ghanem,Eduardo Almeida Palmieri*

Main category: cs.CR

TL;DR: 提出了一种生成高质量恶意软件数据集的方法，支持机器学习和代理AI框架，填补现有数据集的不足。


<details>
  <summary>Details</summary>
Motivation: 现有恶意软件数据集缺乏多样性、全面标注和复杂性，无法满足机器学习和代理AI训练的需求。

Method: 采用自动化恶意软件执行和动态监控工具，生成包含多种恶意软件家族和操作系统的数据集，确保伦理和法律合规性。

Result: 数据集支持系统状态和转换建模，促进基于强化学习的恶意软件检测和响应策略。

Conclusion: 该数据集对自适应网络安全防御和数字取证研究具有重要意义，并支持更广泛的事件响应和威胁缓解应用。

Abstract: This paper addresses the critical need for high-quality malware datasets that
support advanced analysis techniques, particularly machine learning and agentic
AI frameworks. Existing datasets often lack diversity, comprehensive labelling,
and the complexity necessary for effective machine learning and agent-based AI
training. To fill this gap, we developed a systematic approach for generating a
dataset that combines automated malware execution in controlled virtual
environments with dynamic monitoring tools. The resulting dataset comprises
clean and infected memory snapshots across multiple malware families and
operating systems, capturing detailed behavioural and environmental features.
Key design decisions include applying ethical and legal compliance, thorough
validation using both automated and manual methods, and comprehensive
documentation to ensure replicability and integrity. The dataset's distinctive
features enable modelling system states and transitions, facilitating RL-based
malware detection and response strategies. This resource is significant for
advancing adaptive cybersecurity defences and digital forensic research. Its
scope supports diverse malware scenarios and offers potential for broader
applications in incident response and automated threat mitigation.

</details>


### [53] [S-Leak: Leakage-Abuse Attack Against Efficient Conjunctive SSE via s-term Leakage](https://arxiv.org/abs/2507.04077)
*Yue Su,Meng Shen,Cong Zuo,Yuzhi Liu,Liehuang Zhu*

Main category: cs.CR

TL;DR: 论文揭示了CSSE方案中的s-term泄漏漏洞，并提出S-Leak攻击框架，通过三阶段方法恢复联合查询，实验证明其高效性。


<details>
  <summary>Details</summary>
Motivation: 研究联合可搜索对称加密（CSSE）中的泄漏滥用攻击（LAAs）扩展问题，揭示s-term泄漏漏洞。

Method: 提出S-Leak攻击框架，分三阶段：识别查询中的s-term、剪枝低概率关键词组合、重构完整查询。

Result: 在真实数据集上，攻击对161,700个联合查询的恢复准确率达95.15%（至少一个关键词）、82.57%（至少两个）、58%（全部三个）。

Conclusion: s-term泄漏风险被低估，需重新设计多关键词搜索场景的泄漏模型。

Abstract: Conjunctive Searchable Symmetric Encryption (CSSE) enables secure conjunctive
searches over encrypted data. While leakage-abuse attacks (LAAs) against
single-keyword SSE have been extensively studied, their extension to
conjunctive queries faces a critical challenge: the combinatorial explosion of
candidate keyword combinations, leading to enormous time and space overhead for
attacks. In this paper, we reveal a fundamental vulnerability in
state-of-the-art CSSE schemes: s-term leakage, where the keyword with the
minimal document frequency in a query leaks distinct patterns. We propose
S-Leak, the first passive attack framework that progressively recovers
conjunctive queries by exploiting s-term leakage and global leakage. Our key
innovation lies in a three-stage approach: identifying the s-term of queries,
pruning low-probability keyword conjunctions, and reconstructing full queries.
We propose novel metrics to better assess attacks in conjunctive query
scenarios. Empirical evaluations on real-world datasets demonstrate that our
attack is effective in diverse CSSE configurations. When considering 161,700
conjunctive keyword queries, our attack achieves a 95.15% accuracy in
recovering at least one keyword, 82.57% for at least two, 58% for all three
keywords, and maintains efficacy against defenses such as SEAL padding and CLRZ
obfuscation. Our work exposes the underestimated risks of s-term leakage in
practical SSE deployments and calls for a redesign of leakage models for
multi-keyword search scenarios.

</details>


### [54] [Human-Centered Interactive Anonymization for Privacy-Preserving Machine Learning: A Case for Human-Guided k-Anonymity](https://arxiv.org/abs/2507.04104)
*Sri Harsha Gajavalli*

Main category: cs.CR

TL;DR: 论文提出了一种结合人类输入的交互式k-匿名化方法，以提升隐私保护机器学习中的数据效用，并在UCI Adult数据集上验证了其效果。


<details>
  <summary>Details</summary>
Motivation: 传统匿名化方法因泛化或抑制数据属性而降低数据效用，而GDPR等法规要求保护个人数据隐私，因此需要一种平衡数据效用与隐私的方法。

Method: 提出交互式k-匿名化方法，引入领域专家指导属性保留，基于上下文重要性优化匿名化过程。

Result: 实验表明，人类输入在某些情况下能提升数据效用，但效果因任务和设置而异。

Conclusion: 交互式方法在隐私保护ML中具有潜力，但需进一步改进框架以应对局限性。

Abstract: Privacy-preserving machine learning (ML) seeks to balance data utility and
privacy, especially as regulations like the GDPR mandate the anonymization of
personal data for ML applications. Conventional anonymization approaches often
reduce data utility due to indiscriminate generalization or suppression of data
attributes. In this study, we propose an interactive approach that incorporates
human input into the k-anonymization process, enabling domain experts to guide
attribute preservation based on contextual importance. Using the UCI Adult
dataset, we compare classification outcomes of interactive human-influenced
anonymization with traditional, fully automated methods. Our results show that
human input can enhance data utility in some cases, although results vary
across tasks and settings. We discuss limitations of our approach and suggest
potential areas for improved interactive frameworks in privacy-aware ML.

</details>


### [55] [Addressing The Devastating Effects Of Single-Task Data Poisoning In Exemplar-Free Continual Learning](https://arxiv.org/abs/2507.04106)
*Stanisław Pawlak,Bartłomiej Twardowski,Tomasz Trzciński,Joost van de Weijer*

Main category: cs.CR

TL;DR: 论文研究了持续学习中数据投毒的安全问题，提出了一种更简单且现实的单任务投毒（STP）威胁，并展示了其对模型性能的破坏性影响，同时提出了防御框架。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注场景依赖的攻击，而忽视了更简单且现实的单任务投毒威胁，本研究填补了这一空白。

Method: 提出单任务投毒（STP）威胁，并在严格条件下（攻击者仅能访问当前任务数据）测试其对模型性能的影响。

Result: STP攻击能显著破坏持续学习的稳定性和可塑性，即使攻击条件严格。

Conclusion: 论文提出了一个防御框架和基于任务向量的投毒检测方法，为持续学习的安全性提供了新思路。

Abstract: Our research addresses the overlooked security concerns related to data
poisoning in continual learning (CL). Data poisoning - the intentional
manipulation of training data to affect the predictions of machine learning
models - was recently shown to be a threat to CL training stability. While
existing literature predominantly addresses scenario-dependent attacks, we
propose to focus on a more simple and realistic single-task poison (STP)
threats. In contrast to previously proposed poisoning settings, in STP
adversaries lack knowledge and access to the model, as well as to both previous
and future tasks. During an attack, they only have access to the current task
within the data stream. Our study demonstrates that even within these stringent
conditions, adversaries can compromise model performance using standard image
corruptions. We show that STP attacks are able to strongly disrupt the whole
continual training process: decreasing both the stability (its performance on
past tasks) and plasticity (capacity to adapt to new tasks) of the algorithm.
Finally, we propose a high-level defense framework for CL along with a poison
task detection method based on task vectors. The code is available at
https://github.com/stapaw/STP.git .

</details>


### [56] [BlowPrint: Blow-Based Multi-Factor Biometrics for Smartphone User Authentication](https://arxiv.org/abs/2507.04126)
*Howard Halim,Eyasu Getahun Chekole,Daniël Reijsbergen,Jianying Zhou*

Main category: cs.CR

TL;DR: BlowPrint是一种新型行为生物特征技术，通过用户吹手机屏幕的声学模式进行身份验证，结合生理特征（如面部识别）提升安全性。实验结果显示其高准确性和抗欺骗攻击能力。


<details>
  <summary>Details</summary>
Motivation: 多因素生物识别（MFB）需要满足高准确性、高可用性、非侵入性等要求，现有行为生物特征技术往往无法完全满足。BlowPrint旨在填补这一空白。

Method: 通过收集用户吹手机屏幕的声学数据和面部特征数据，使用相似性算法计算得分，并通过分数级融合结合，最后用机器学习分类器评估准确性。

Result: BlowPrint的吹气声学准确率为99.35%，面部识别为99.96%，结合方法为99.82%，表现出高准确性和安全性。

Conclusion: BlowPrint是一种高效、安全且非侵入性的行为生物特征技术，适合与生理特征结合使用，提升多因素生物识别的性能。

Abstract: Biometric authentication is a widely used security mechanism that leverages
unique physiological or behavioral characteristics to authenticate users. In
multi-factor biometrics (MFB), multiple biometric modalities, e.g.,
physiological and behavioral, are integrated to mitigate the limitations
inherent in single-factor biometrics. The main challenge in MFB lies in
identifying novel behavioral techniques capable of meeting critical criteria,
including high accuracy, high usability, non-invasiveness, resilience against
spoofing attacks, and low use of computational resources. Despite ongoing
advancements, current behavioral biometric techniques often fall short of
fulfilling one or more of these requirements. In this work, we propose
BlowPrint, a novel behavioral biometric technique that allows us to
authenticate users based on their phone blowing behaviors. In brief, we assume
that the way users blow on a phone screen can produce distinctive acoustic
patterns, which can serve as a unique biometric identifier for effective user
authentication. It can also be seamlessly integrated with physiological
techniques, such as facial recognition, to enhance its robustness and security.
To assess BlowPrint's effectiveness, we conduct an empirical study involving 50
participants from whom we collect blow-acoustic and facial feature data.
Subsequently, we compute the similarity scores of the two modalities using
various similarity algorithms and combine them through score-level fusion.
Finally, we compute the accuracy using a machine learning-based classifier. As
a result, the proposed method demonstrates an accuracy of 99.35% for blow
acoustics, 99.96% for facial recognition, and 99.82% for the combined approach.
The experimental results demonstrate BlowPrint's high effectiveness in terms of
authentication accuracy, spoofing attack resilience, usability,
non-invasiveness, and other aspects.

</details>


### [57] [Cloud Digital Forensic Readiness: An Open Source Approach to Law Enforcement Request Management](https://arxiv.org/abs/2507.04174)
*Abdellah Akilal,M-Tahar Kechadi*

Main category: cs.CR

TL;DR: 论文提出了一种云执法请求管理系统（CLERMS），旨在解决跨境数据访问的延迟和复杂性，提升云数字取证准备（CDFR）。


<details>
  <summary>Details</summary>
Motivation: 云取证面临多司法管辖区的挑战，影响数字取证调查的成功。法律请求的增多和跨境数据访问的复杂性是主要问题。

Method: 分析主要云服务提供商（CSPs）的透明度报告和法律指南，提出CLERMS的抽象架构，并通过两个实际场景验证概念证明。

Result: 开发并验证了CLERMS的概念证明，并估算了其经济成本。解决方案基于开源组件，对CSPs和云服务消费者（CSCs）均有益。

Conclusion: CLERMS有助于提升云数字取证准备，为CSPs和CSCs提供支持，解决跨境数据访问的挑战。

Abstract: Cloud Forensics presents a multi-jurisdictional challenge that may undermines
the success of digital forensic investigations (DFIs). The growing volumes of
domiciled and foreign law enforcement (LE) requests, the latency and complexity
of formal channels for crossborder data access are challenging issues. In this
paper, we first discuss major Cloud Service Providers (CSPs) transparency
reports and law enforcement guidelines, then propose an abstract architecture
for a Cloud Law Enforcement Requests Management System (CLERMS). A proof of
concept of the proposed solution is developed, deployed and validated by two
realistic scenarios, in addition to an economic estimation of its associated
costs. Based on available open source components, our solution is for the
benefit of both CSPs and Cloud Service Consumers (CSCs), and aims to enhance
the due Cloud Digital Forensic Readiness (CDFR).

</details>


### [58] [ML-Enhanced AES Anomaly Detection for Real-Time Embedded Security](https://arxiv.org/abs/2507.04197)
*Nishant Chinnasami,Rye Stahle-Smith,Rasha Karakchi*

Main category: cs.CR

TL;DR: 提出了一种通过异常注入和实时检测增强AES-128安全性的框架，结合统计和机器学习方法，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: AES的实现在侧信道和故障注入攻击下仍存在漏洞，需提升安全性。

Method: 通过注入执行延迟和密文扰动生成标记数据集，开发基于阈值和随机森林的检测机制。

Result: 机器学习方法在精度和召回率上显著优于阈值方法，且能在嵌入式硬件上实时运行。

Conclusion: 该框架为轻量级FPGA平台提供了一种低成本、实时且准确的AES异常检测方案。

Abstract: Advanced Encryption Standard (AES) is a widely adopted cryptographic
algorithm, yet its practical implementations remain susceptible to side-channel
and fault injection attacks. In this work, we propose a comprehensive framework
that enhances AES-128 encryption security through controlled anomaly injection
and real-time anomaly detection using both statistical and machine learning
(ML) methods. We simulate timing and fault-based anomalies by injecting
execution delays and ciphertext perturbations during encryption, generating
labeled datasets for detection model training. Two complementary detection
mechanisms are developed: a threshold-based timing anomaly detector and a
supervised Random Forest classifier trained on combined timing and ciphertext
features. We implement and evaluate the framework on both CPU and FPGA-based
SoC hardware (PYNQ-Z1), measuring performance across varying block sizes,
injection rates, and core counts. Our results show that ML-based detection
significantly outperforms threshold-based methods in precision and recall while
maintaining real-time performance on embedded hardware. Compared to existing
AES anomaly detection methods, our solution offers a low-cost, real-time, and
accurate detection approach deployable on lightweight FPGA platforms.

</details>


### [59] [Can Large Language Models Automate the Refinement of Cellular Network Specifications?](https://arxiv.org/abs/2507.04214)
*Jianshuo Dong,Tianyi Zhang,Feng Yan,Yuanjie Li,Hewu Li,Han Qiu*

Main category: cs.CR

TL;DR: 论文探讨了利用大型语言模型（LLMs）自动优化蜂窝网络规范的可行性，通过20万+的3GPP变更请求（CRs）构建数据集，并开发了CR-eval评估框架。实验表明，顶级LLMs能在五次试验中发现127/200测试案例中的安全问题，同时通过微调8B模型达到或超越GPT-4o等先进模型。


<details>
  <summary>Details</summary>
Motivation: 蜂窝网络虽服务全球数十亿用户，但其3GPP标准存在可靠性和安全性问题，传统分析方法难以应对日益复杂的规范。

Method: 利用20万+ 3GPP变更请求（CRs）构建数据集，开发CR-eval评估框架，测试16种先进LLMs，并探索微调技术。

Result: 顶级LLMs在五次试验中发现127/200测试案例中的安全问题，微调8B模型可匹敌GPT-4o等先进模型。

Conclusion: LLMs能自动化优化蜂窝网络规范，并为未来研究提供方向。

Abstract: Cellular networks serve billions of users globally, yet concerns about
reliability and security persist due to weaknesses in 3GPP standards. However,
traditional analysis methods, including manual inspection and automated tools,
struggle with increasingly expanding cellular network specifications. This
paper investigates the feasibility of Large Language Models (LLMs) for
automated cellular network specification refinement. To advance it, we leverage
200,000+ approved 3GPP Change Requests (CRs) that document specification
revisions, constructing a valuable dataset for domain tasks. We introduce
CR-eval, a principled evaluation framework, and benchmark 16 state-of-the-art
LLMs, demonstrating that top models can discover security-related weaknesses in
over 127 out of 200 test cases within five trials. To bridge potential gaps, we
explore LLM specialization techniques, including fine-tuning an 8B model to
match or surpass advanced LLMs like GPT-4o and DeepSeek-R1. Evaluations on 30
cellular attacks identify open challenges for achieving full automation. These
findings confirm that LLMs can automate the refinement of cellular network
specifications and provide valuable insights to guide future research in this
direction.

</details>


### [60] [Hijacking JARVIS: Benchmarking Mobile GUI Agents against Unprivileged Third Parties](https://arxiv.org/abs/2507.04227)
*Guohong Liu,Jialei Ye,Jiacheng Liu,Yuanchun Li,Wei Liu,Pengzhi Gao,Jian Luan,Yunxin Liu*

Main category: cs.CR

TL;DR: 该论文研究了移动GUI代理在第三方恶意修改屏幕内容时的脆弱性，提出了攻击模拟框架AgentHazard，并评估了现有代理的易受攻击性。


<details>
  <summary>Details</summary>
Motivation: 移动GUI代理在现实场景中可能受到第三方恶意修改屏幕内容的攻击，其脆弱性尚未被系统研究。

Method: 提出了AgentHazard框架，用于模拟攻击场景，并构建了包含动态环境和静态数据集的基准测试套件。

Result: 评估发现所有测试代理均易受攻击（平均误导率28.8%），且脆弱性与感知模态和骨干LLM相关。

Conclusion: 论文揭示了移动GUI代理的脆弱性，并探讨了训练增强其鲁棒性的挑战与机遇。

Abstract: Mobile GUI agents are designed to autonomously execute diverse device-control
tasks by interpreting and interacting with mobile screens. Despite notable
advancements, their resilience in real-world scenarios where screen content may
be partially manipulated by untrustworthy third parties remains largely
unexplored. Owing to their black-box and autonomous nature, these agents are
vulnerable to manipulations that could compromise user devices. In this work,
we present the first systematic investigation into the vulnerabilities of
mobile GUI agents. We introduce a scalable attack simulation framework
AgentHazard, which enables flexible and targeted modifications of screen
content within existing applications. Leveraging this framework, we develop a
comprehensive benchmark suite comprising both a dynamic task execution
environment and a static dataset of vision-language-action tuples, totaling
over 3,000 attack scenarios. The dynamic environment encompasses 58
reproducible tasks in an emulator with various types of hazardous UI content,
while the static dataset is constructed from 210 screenshots collected from 14
popular commercial apps. Importantly, our content modifications are designed to
be feasible for unprivileged third parties. We evaluate 7 widely-used mobile
GUI agents and 5 common backbone models using our benchmark. Our findings
reveal that all examined agents are significantly influenced by misleading
third-party content (with an average misleading rate of 28.8% in human-crafted
attack scenarios) and that their vulnerabilities are closely linked to the
employed perception modalities and backbone LLMs. Furthermore, we assess
training-based mitigation strategies, highlighting both the challenges and
opportunities for enhancing the robustness of mobile GUI agents. Our code and
data will be released at https://agenthazard.github.io.

</details>


### [61] [VOLTRON: Detecting Unknown Malware Using Graph-Based Zero-Shot Learning](https://arxiv.org/abs/2507.04275)
*M. Tahir Akdeniz,Zeynep Yeşilkaya,İ. Enes Köse,İ. Ulaş Ünal,Sevil Şen*

Main category: cs.CR

TL;DR: 提出了一种结合VGAE和SNN的零样本学习框架，用于检测未知Android恶意软件，无需依赖标记数据。


<details>
  <summary>Details</summary>
Motivation: 现有机器学习方法依赖大量标记数据，难以应对新兴恶意软件家族。

Method: 使用VGAE和SNN结合，基于图表示检测恶意软件的结构差异。

Result: 模型在零日恶意软件检测中优于MaMaDroid，准确率96.24%，召回率95.20%。

Conclusion: 该方法对未知恶意软件家族具有鲁棒性，能有效应对Android威胁。

Abstract: The persistent threat of Android malware presents a serious challenge to the
security of millions of users globally. While many machine learning-based
methods have been developed to detect these threats, their reliance on large
labeled datasets limits their effectiveness against emerging, previously unseen
malware families, for which labeled data is scarce or nonexistent.
  To address this challenge, we introduce a novel zero-shot learning framework
that combines Variational Graph Auto-Encoders (VGAE) with Siamese Neural
Networks (SNN) to identify malware without needing prior examples of specific
malware families. Our approach leverages graph-based representations of Android
applications, enabling the model to detect subtle structural differences
between benign and malicious software, even in the absence of labeled data for
new threats.
  Experimental results show that our method outperforms the state-of-the-art
MaMaDroid, especially in zero-day malware detection. Our model achieves 96.24%
accuracy and 95.20% recall for unknown malware families, highlighting its
robustness against evolving Android threats.

</details>


### [62] [Attention Slipping: A Mechanistic Understanding of Jailbreak Attacks and Defenses in LLMs](https://arxiv.org/abs/2507.04365)
*Xiaomeng Hu,Pin-Yu Chen,Tsung-Yi Ho*

Main category: cs.CR

TL;DR: 论文揭示了大型语言模型（LLMs）在越狱攻击中的普遍现象——注意力滑移（Attention Slipping），并提出了一种新的防御方法Attention Sharpening，通过温度缩放直接对抗注意力滑移，有效抵御攻击且不影响正常任务性能。


<details>
  <summary>Details</summary>
Motivation: 随着LLMs在社会和技术中的重要性增加，确保其安全性变得至关重要。越狱攻击利用漏洞绕过安全防护，但其机制尚未被充分理解。

Method: 研究发现越狱攻击中的注意力滑移现象，并提出Attention Sharpening防御方法，通过温度缩放调整注意力分数分布。

Result: 实验表明，Attention Sharpening能有效抵御多种越狱攻击，同时在AlpacaEval上保持正常任务性能，且无额外计算或内存开销。

Conclusion: Attention Sharpening是一种高效实用的防御方法，可直接对抗注意力滑移，适用于实际部署。

Abstract: As large language models (LLMs) become more integral to society and
technology, ensuring their safety becomes essential. Jailbreak attacks exploit
vulnerabilities to bypass safety guardrails, posing a significant threat.
However, the mechanisms enabling these attacks are not well understood. In this
paper, we reveal a universal phenomenon that occurs during jailbreak attacks:
Attention Slipping. During this phenomenon, the model gradually reduces the
attention it allocates to unsafe requests in a user query during the attack
process, ultimately causing a jailbreak. We show Attention Slipping is
consistent across various jailbreak methods, including gradient-based token
replacement, prompt-level template refinement, and in-context learning.
Additionally, we evaluate two defenses based on query perturbation, Token
Highlighter and SmoothLLM, and find they indirectly mitigate Attention
Slipping, with their effectiveness positively correlated with the degree of
mitigation achieved. Inspired by this finding, we propose Attention Sharpening,
a new defense that directly counters Attention Slipping by sharpening the
attention score distribution using temperature scaling. Experiments on four
leading LLMs (Gemma2-9B-It, Llama3.1-8B-It, Qwen2.5-7B-It, Mistral-7B-It v0.2)
show that our method effectively resists various jailbreak attacks while
maintaining performance on benign tasks on AlpacaEval. Importantly, Attention
Sharpening introduces no additional computational or memory overhead, making it
an efficient and practical solution for real-world deployment.

</details>


### [63] [Enhancing Phishing Detection in Financial Systems through NLP](https://arxiv.org/abs/2507.04426)
*Novruz Amirov,Leminur Celik,Egemen Ali Caner,Emre Yurdakul,Fahri Anil Yerlikaya,Serif Bahtiyar*

Main category: cs.CR

TL;DR: 本文提出了一种基于自然语言处理（NLP）的钓鱼邮件检测方法，结合语义相似性和TFIDF分析，以提高金融系统中的网络安全。实验结果显示，TFIDF分析的准确率为79.8%，语义分析的准确率为67.2%。


<details>
  <summary>Details</summary>
Motivation: 钓鱼攻击对金融系统的威胁日益增长，现有方法（如黑名单和白名单）存在局限性，需要更先进的解决方案。

Method: 采用NLP技术，结合语义相似性和TFIDF分析，识别钓鱼邮件中的关键词并与专用数据集进行语义匹配。

Result: TFIDF分析的准确率为79.8%，语义分析的准确率为67.2%。

Conclusion: 该方法为金融系统中的钓鱼威胁检测提供了更强大的解决方案，同时推动了网络安全和NLP领域的发展。

Abstract: The threat of phishing attacks in financial systems is continuously growing.
Therefore, protecting sensitive information from unauthorized access is
paramount. This paper discusses the critical need for robust email phishing
detection. Several existing methods, including blacklists and whitelists, play
a crucial role in detecting phishing attempts. Nevertheless, these methods
possess inherent limitations, emphasizing the need for the development of a
more advanced solution. Our proposed solution presents a pioneering Natural
Language Processing (NLP) approach for phishing email detection. Leveraging
semantic similarity and TFIDF (Term Frequency-Inverse Document Frequency)
analysis, our solution identifies keywords in phishing emails, subsequently
evaluating the semantic similarities with a dedicated phishing dataset,
ultimately contributing to the enhancement of cybersecurity and NLP domains
through a robust solution for detecting phishing threats in financial systems.
Experimental results show the accuracy of our phishing detection method can
reach 79.8 percent according to TF-IDF analysis, while it can reach 67.2
percent according to semantic analysis.

</details>


### [64] [UniAud: A Unified Auditing Framework for High Auditing Power and Utility with One Training Run](https://arxiv.org/abs/2507.04457)
*Ruixuan Liu,Li Xiong*

Main category: cs.CR

TL;DR: 论文提出UniAud和UniAud++框架，解决差分隐私优化审计中的数据依赖性和审计与效用冲突问题，实现了高效且紧致的审计结果。


<details>
  <summary>Details</summary>
Motivation: 差分隐私优化审计中，现有O(1)框架虽高效但存在数据依赖性和审计与效用冲突问题，影响审计结果的紧致性。

Method: 提出UniAud（数据无关审计）和UniAud++（数据相关审计）框架，通过无关联数据和多任务学习分别解决上述问题。

Result: 实验验证框架在效率和审计紧致性上优于现有方法，且对模型效用影响极小。

Conclusion: UniAud和UniAud++框架在审计效率和效用间取得最优平衡，无需额外训练即可实现高效审计。

Abstract: Differentially private (DP) optimization has been widely adopted as a
standard approach to provide rigorous privacy guarantees for training datasets.
DP auditing verifies whether a model trained with DP optimization satisfies its
claimed privacy level by estimating empirical privacy lower bounds through
hypothesis testing. Recent O(1) frameworks improve auditing efficiency by
checking the membership status of multiple audit samples in a single run,
rather than checking individual samples across multiple runs. However, we
reveal that there is no free lunch for this improved efficiency: data
dependency and an implicit conflict between auditing and utility impair the
tightness of the auditing results. Addressing these challenges, our key
insights include reducing data dependency through uncorrelated data and
resolving the auditing-utility conflict by decoupling the criteria for
effective auditing and separating objectives for utility and auditing. We first
propose a unified framework, UniAud, for data-independent auditing that
maximizes auditing power through a novel uncorrelated canary construction and a
self-comparison framework. We then extend this framework as UniAud++ for
data-dependent auditing, optimizing the auditing and utility trade-off through
multi-task learning with separate objectives for auditing and training.
Experimental results validate that our black-box O(1) framework matches the
state-of-the-art auditing results of O(T) auditing with thousands of runs,
demonstrating the best efficiency-auditing trade-off across vision and language
tasks. Additionally, our framework provides meaningful auditing with only
slight utility degradation compared to standard DP training, showing the
optimal utility-auditing trade-off and the benefit of requiring no extra
training for auditing.

</details>


### [65] [Arbiter PUF: Uniqueness and Reliability Analysis Using Hybrid CMOS-Stanford Memristor Model](https://arxiv.org/abs/2507.04461)
*Tanvir Rahman,A. B. M. Harun-ur Rashid*

Main category: cs.CR

TL;DR: 研究利用斯坦福忆阻器模型设计和评估物理不可克隆函数（PUFs），以提高硬件安全性。


<details>
  <summary>Details</summary>
Motivation: 随着物联网（IoT）的普及，传统加密技术难以应对物理攻击，第三方芯片制造也带来安全隐患。

Method: 采用45nm CMOS技术构建系统，比较CMOS和忆阻器仲裁PUFs在温度、电压和工艺变化下的性能，通过蒙特卡洛模拟计算汉明距离评估唯一性和可靠性。

Result: 忆阻器PUFs比CMOS设计更可靠，但唯一性需改进。

Conclusion: 忆阻器PUFs在硬件安全应用中具有潜力。

Abstract: In an increasingly interconnected world, protecting electronic devices has
grown more crucial because of the dangers of data extraction, reverse
engineering, and hardware tampering. Producing chips in a third-party
manufacturing company can let hackers change the design. As the Internet of
Things (IoT) proliferates, physical attacks happen more, and conventional
cryptography techniques do not function well. In this paper, we investigate the
design and assessment of PUFs using the Stanford Memristor Model, utilizing its
random filament evolution to improve security. The system was built using 45nm
CMOS technology. A comparison is made between CMOS-based and memristor-based
Arbiter PUFs, evaluating their performance under temperature, voltage, and
process variations. Intra- and inter-hamming distances are employed by Monte
Carlo simulations to estimate uniqueness and reliability. The results show that
memristor-based PUFs offer better reliability than CMOS-based designs, though
uniqueness needs further improvement. Furthermore, this study sheds light on
the reasonableness of memristor-based PUFs for secure applications in hardware
security.

</details>


### [66] [README: Robust Error-Aware Digital Signature Framework via Deep Watermarking Model](https://arxiv.org/abs/2507.04495)
*Hyunwook Choi,Sangyun Won,Daeyeon Hwang,Junhyeok Choi*

Main category: cs.CR

TL;DR: README框架通过结合裁剪容量扩展和轻量级错误校正模块ERPA，显著提升了图像中嵌入2048位数字签名的零比特错误率（Z.B.I.R），从1.2%提升至86.3%。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习水印模型在嵌入容量和比特错误容忍度上存在局限，无法满足密码学应用（如数字签名）的需求。

Method: 提出README框架，结合裁剪容量扩展机制和ERPA错误校正模块，利用DCSS定位和纠正比特错误。

Result: 在单张图像中嵌入2048位数字签名时，Z.B.I.R从1.2%提升至86.3%，且无需微调现有预训练模型。

Conclusion: README为深度学习水印技术开辟了高安全性应用的新方向，填补了信号级水印与密码学安全之间的空白。

Abstract: Deep learning-based watermarking has emerged as a promising solution for
robust image authentication and protection. However, existing models are
limited by low embedding capacity and vulnerability to bit-level errors, making
them unsuitable for cryptographic applications such as digital signatures,
which require over 2048 bits of error-free data. In this paper, we propose
README (Robust Error-Aware Digital Signature via Deep WaterMarking ModEl), a
novel framework that enables robust, verifiable, and error-tolerant digital
signatures within images. Our method combines a simple yet effective
cropping-based capacity scaling mechanism with ERPA (ERror PAinting Module), a
lightweight error correction module designed to localize and correct bit errors
using Distinct Circular Subsum Sequences (DCSS). Without requiring any
fine-tuning of existing pretrained watermarking models, README significantly
boosts the zero-bit-error image rate (Z.B.I.R) from 1.2% to 86.3% when
embedding 2048-bit digital signatures into a single image, even under
real-world distortions. Moreover, our use of perceptual hash-based signature
verification ensures public verifiability and robustness against tampering. The
proposed framework unlocks a new class of high-assurance applications for deep
watermarking, bridging the gap between signal-level watermarking and
cryptographic security.

</details>


### [67] [LINE: Public-key encryption](https://arxiv.org/abs/2507.04501)
*Gennady Khalimov,Yevgen Kotukh*

Main category: cs.CR

TL;DR: 提出了一种基于线性方程组解的公钥加密系统，通过共享秘密计算预定义输入参数，利用多解性确保安全性。


<details>
  <summary>Details</summary>
Motivation: 设计一种在多项式时间内无法破解的公钥加密系统，利用线性方程组的不可解性增强安全性。

Method: 通过秘密同态矩阵变换完成输入参数，加密基于单向函数计算，解密通过完成输入参数实现。

Result: 实现了高安全性和低计算开销的同态变换。

Conclusion: 该系统通过矩阵计算和线性方程组的多解性，提供了一种高效且安全的加密方案。

Abstract: We propose a public key encryption cryptosystem based on solutions of linear
equation systems with predefinition of input parameters through shared secret
computation for factorizable substitutions. The existence of multiple
equivalent solutions for an underdetermined system of linear equations
determines the impossibility of its resolution by a cryptanalyst in polynomial
time. The completion of input parameters of the equation system is implemented
through secret homomorphic matrix transformation for substitutions factorized
over the basis of a vector space of dimension m over the field F2. Encryption
is implemented through computation of substitutions that are one-way functions
on an elementary abelian 2-group of order 2"m. Decryption is implemented
through completion of input parameters of the equation system. Homomorphic
transformations are constructed based on matrix computations. Matrix
computations enable the implementation of high security and low computational
overhead for homomorphic transformations.

</details>


### [68] [Large Language Models for Network Intrusion Detection Systems: Foundations, Implementations, and Future Directions](https://arxiv.org/abs/2507.04752)
*Shuo Yang,Xinran Zheng,Xinchen Zhang,Jinfeng Xu,Jinze Li,Donglin Xie,Weicai Long,Edith C. H. Ngai*

Main category: cs.CR

TL;DR: 论文探讨了大型语言模型（LLMs）在网络入侵检测系统（NIDS）中的应用潜力，分析了当前挑战、方法及未来机遇，提出了LLM为中心的控制器概念。


<details>
  <summary>Details</summary>
Motivation: 传统智能NIDS缺乏上下文意识和可解释性，而认知NIDS通过LLMs处理结构化与非结构化数据，实现更深层次的推理和自动化响应。

Method: 研究结合LLMs作为处理器、检测器和解释器，提出LLM为中心的控制器以优化NIDS工作流程。

Result: LLMs能够提升NIDS的上下文推理、可解释决策和自动化响应能力，为下一代网络安全系统提供创新方向。

Conclusion: LLMs在网络入侵检测中具有变革潜力，未来需解决关键挑战以实现可靠、自适应和可解释的NIDS。

Abstract: Large Language Models (LLMs) have revolutionized various fields with their
exceptional capabilities in understanding, processing, and generating
human-like text. This paper investigates the potential of LLMs in advancing
Network Intrusion Detection Systems (NIDS), analyzing current challenges,
methodologies, and future opportunities. It begins by establishing a
foundational understanding of NIDS and LLMs, exploring the enabling
technologies that bridge the gap between intelligent and cognitive systems in
AI-driven NIDS. While Intelligent NIDS leverage machine learning and deep
learning to detect threats based on learned patterns, they often lack
contextual awareness and explainability. In contrast, Cognitive NIDS integrate
LLMs to process both structured and unstructured security data, enabling deeper
contextual reasoning, explainable decision-making, and automated response for
intrusion behaviors. Practical implementations are then detailed, highlighting
LLMs as processors, detectors, and explainers within a comprehensive AI-driven
NIDS pipeline. Furthermore, the concept of an LLM-centered Controller is
proposed, emphasizing its potential to coordinate intrusion detection
workflows, optimizing tool collaboration and system performance. Finally, this
paper identifies critical challenges and opportunities, aiming to foster
innovation in developing reliable, adaptive, and explainable NIDS. By
presenting the transformative potential of LLMs, this paper seeks to inspire
advancement in next-generation network security systems.

</details>


### [69] [Efficient Unlearning with Privacy Guarantees](https://arxiv.org/abs/2507.04771)
*Josep Domingo-Ferrer,Najeeb Jebreel,David Sánchez*

Main category: cs.CR

TL;DR: 论文提出了一种名为EUPG的高效机器学习遗忘框架，结合隐私保护模型，实现数据遗忘的同时降低计算和存储成本。


<details>
  <summary>Details</summary>
Motivation: 现有机器学习遗忘方法要么计算成本高，要么缺乏遗忘保证且适用范围有限，EUPG旨在解决这些问题。

Method: EUPG通过在隐私保护模型（如k-匿名和差分隐私）下预训练模型，实现高效遗忘并保留隐私保证。

Result: 在四个异构数据集上的实验表明，EUPG的效用和遗忘效果与精确遗忘方法相当，但显著降低了计算和存储成本。

Conclusion: EUPG是一种高效且具有隐私保证的机器学习遗忘框架，适用于实际应用。

Abstract: Privacy protection laws, such as the GDPR, grant individuals the right to
request the forgetting of their personal data not only from databases but also
from machine learning (ML) models trained on them. Machine unlearning has
emerged as a practical means to facilitate model forgetting of data instances
seen during training. Although some existing machine unlearning methods
guarantee exact forgetting, they are typically costly in computational terms.
On the other hand, more affordable methods do not offer forgetting guarantees
and are applicable only to specific ML models. In this paper, we present
\emph{efficient unlearning with privacy guarantees} (EUPG), a novel machine
unlearning framework that offers formal privacy guarantees to individuals whose
data are being unlearned. EUPG involves pre-training ML models on data
protected using privacy models, and it enables {\em efficient unlearning with
the privacy guarantees offered by the privacy models in use}. Through empirical
evaluation on four heterogeneous data sets protected with $k$-anonymity and
$\epsilon$-differential privacy as privacy models, our approach demonstrates
utility and forgetting effectiveness comparable to those of exact unlearning
methods, while significantly reducing computational and storage costs. Our code
is available at https://github.com/najeebjebreel/EUPG.

</details>


### [70] [FIDESlib: A Fully-Fledged Open-Source FHE Library for Efficient CKKS on GPUs](https://arxiv.org/abs/2507.04775)
*Carlos Agulló-Domingo,Óscar Vera-López,Seyda Guzelhan,Lohit Daksha,Aymane El Jerari,Kaustubh Shivdikar,Rashmi Agrawal,David Kaeli,Ajay Joshi,José L. Abellán*

Main category: cs.CR

TL;DR: FIDESlib是一个开源的服务器端CKKS GPU库，与OpenFHE兼容，提供优化的GPU内核和性能提升。


<details>
  <summary>Details</summary>
Motivation: OpenFHE的服务器端性能不足，GPU在数据中心中普及，但高效集成GPU支持到OpenFHE具有挑战性。

Method: 开发FIDESlib，实现优化的GPU内核，支持CKKS原语（包括bootstrapping），并集成测试和扩展多GPU支持。

Result: FIDESlib在性能和扩展性上优于现有库，bootstrapping速度提升至少70倍。

Conclusion: FIDESlib为CKKS GPU计算提供了高效、可扩展的解决方案。

Abstract: Word-wise Fully Homomorphic Encryption (FHE) schemes, such as CKKS, are
gaining significant traction due to their ability to provide
post-quantum-resistant, privacy-preserving approximate computing; an especially
desirable feature in Machine-Learning-as-a-Service (MLaaS) cloud-computing
paradigms. OpenFHE is a leading CPU-based FHE library with robust CKKS
operations, but its server-side performance is not yet sufficient for practical
cloud deployment. As GPU computing becomes more common in data centers, many
FHE libraries are adding GPU support. However, integrating an efficient GPU
backend into OpenFHE is challenging. While OpenFHE uses a Hardware Abstraction
Layer (HAL), its flexible architecture sacrifices performance due to the
abstraction layers required for multi-scheme and multi-backend compatibility.
In this work, we introduce FIDESlib, the first open-source server-side CKKS GPU
library that is fully interoperable with well-established client-side OpenFHE
operations. Unlike other existing open-source GPU libraries, FIDESlib provides
the first implementation featuring heavily optimized GPU kernels for all CKKS
primitives, including bootstrapping. Our library also integrates robust
benchmarking and testing, ensuring it remains adaptable to further
optimization. Furthermore, its software architecture is designed to support
extensions to a multi-GPU backend for enhanced acceleration. Our experiments
across various GPU systems and the leading open-source CKKS library to date,
Phantom, show that FIDESlib offers superior performance and scalability. For
bootstrapping, FIDESlib achieves no less than 70x speedup over the
AVX-optimized OpenFHE implementation.

</details>


### [71] [Hybrid Approach to Directed Fuzzing](https://arxiv.org/abs/2507.04855)
*Darya Parygina,Timofey Mezhuev,Daniil Kuts*

Main category: cs.CR

TL;DR: 提出了一种结合定向模糊测试和符号执行的混合方法，通过新颖的种子调度算法提高错误检测效率。


<details>
  <summary>Details</summary>
Motivation: 定向模糊测试在预定义代码区域中检测错误时难以克服复杂的程序约束，而符号执行虽能解决此问题但性能较低。

Method: 采用基于目标相关性和覆盖率的种子调度算法，结合定向模糊测试和符号执行技术，实现高效的错误检测。

Result: 在7个示例中，3个示例的性能提升高达1.86倍，3个示例相比纯定向模糊测试有显著改进。

Conclusion: Sydr-Fuzz混合方法在定向模糊测试中表现出高性能，显著提升了效率。

Abstract: Program analysis and automated testing have recently become an essential part
of SSDLC. Directed greybox fuzzing is one of the most popular automated testing
methods that focuses on error detection in predefined code regions. However, it
still lacks ability to overcome difficult program constraints. This problem can
be well addressed by symbolic execution, but at the cost of lower performance.
Thus, combining directed fuzzing and symbolic execution techniques can lead to
more efficient error detection.
  In this paper, we propose a hybrid approach to directed fuzzing with novel
seed scheduling algorithm, based on target-related interestingness and
coverage. The approach also performs minimization and sorting of objective
seeds according to a target-related information. We implement our approach in
Sydr-Fuzz tool using LibAFL-DiFuzz as directed fuzzer and Sydr as dynamic
symbolic executor. We evaluate our approach with Time to Exposure metric and
compare it with pure LibAFL-DiFuzz, AFLGo, BEACON, WAFLGo, WindRanger,
FishFuzz, and Prospector. The results show an improvement for 3 out of 7
examples with speedup up to 1.86 times over the second best result, as well as
a significant improvement for 3 out of 7 examples over the pure LibAFL-DiFuzz
fuzzer. Sydr-Fuzz hybrid approach to directed fuzzing shows high performance
and helps to improve directed fuzzing efficiency.

</details>


### [72] [BackFed: An Efficient & Standardized Benchmark Suite for Backdoor Attacks in Federated Learning](https://arxiv.org/abs/2507.04903)
*Thinh Dao,Dung Thuy Nguyen,Khoa D Doan,Kok-Seng Wong*

Main category: cs.CR

TL;DR: BackFed是一个标准化、高效评估联邦学习中后门攻击与防御的基准套件，旨在解决现有研究中实验设置不一致和假设不现实的问题。


<details>
  <summary>Details</summary>
Motivation: 联邦学习系统易受后门攻击，但现有研究缺乏公平比较和实际场景下的有效性验证。

Method: BackFed通过多进程实现加速实验，模块化设计支持新方法无缝集成，并提供标准化评估流程。

Result: 大规模实验揭示了代表性攻击和防御在计算机视觉与自然语言处理任务中的局限性。

Conclusion: BackFed为研究人员提供了一个可靠的环境，促进新方法的开发和联邦学习系统的安全性提升。

Abstract: Federated Learning (FL) systems are vulnerable to backdoor attacks, where
adversaries train their local models on poisoned data and submit poisoned model
updates to compromise the global model. Despite numerous proposed attacks and
defenses, divergent experimental settings, implementation errors, and
unrealistic assumptions hinder fair comparisons and valid conclusions about
their effectiveness in real-world scenarios. To address this, we introduce
BackFed - a comprehensive benchmark suite designed to standardize, streamline,
and reliably evaluate backdoor attacks and defenses in FL, with a focus on
practical constraints. Our benchmark offers key advantages through its
multi-processing implementation that significantly accelerates experimentation
and the modular design that enables seamless integration of new methods via
well-defined APIs. With a standardized evaluation pipeline, we envision BackFed
as a plug-and-play environment for researchers to comprehensively and reliably
evaluate new attacks and defenses. Using BackFed, we conduct large-scale
studies of representative backdoor attacks and defenses across both Computer
Vision and Natural Language Processing tasks with diverse model architectures
and experimental settings. Our experiments critically assess the performance of
proposed attacks and defenses, revealing unknown limitations and modes of
failures under practical conditions. These empirical insights provide valuable
guidance for the development of new methods and for enhancing the security of
FL systems. Our framework is openly available at
https://github.com/thinh-dao/BackFed.

</details>


### [73] [Cyclic Equalizability of Words and Its Application to Card-Based Cryptography](https://arxiv.org/abs/2507.04916)
*Kazumasa Shinagawa,Koji Nuida*

Main category: cs.CR

TL;DR: 本文首次探讨了基于卡片的密码学与组合词学的关系，特别是循环词相等性，并证明两个长度和汉明重量相同的二进制词是循环可等化的。


<details>
  <summary>Details</summary>
Motivation: 研究基于卡片的密码学与组合词学的联系，探索循环词相等性在密码学中的应用。

Method: 聚焦于循环词相等性，定义循环可等化性，并通过插入字母操作实现词的循环相等。

Result: 证明两个长度和汉明重量相同的二进制词是循环可等化的。

Conclusion: 循环可等化性在信息擦除问题和单切全开协议中有应用，为密码学提供了新工具。

Abstract: Card-based cryptography is a research area to implement cryptographic
procedures using a deck of physical cards. In recent years, it has been found
to be related to finite group theory and algebraic combinatorics, and is
becoming more and more closely connected to the field of mathematics. In this
paper, we discuss the relationship between card-based cryptography and
combinatorics on words for the first time. In particular, we focus on cyclic
equality of words. We say that a set of words are cyclically equalizable if
they can be transformed to be cyclically equal by repeated simultaneous
insertion of letters. The main result of this paper is to show that two binary
words of equal length and equal Hamming weight are cyclically equalizable. As
applications of cyclic equalizability to card-based cryptography, we describe
its applications to the information erasure problem and to single-cut full-open
protocols.

</details>


### [74] [LIFT: Automating Symbolic Execution Optimization with Large Language Models for AI Networks](https://arxiv.org/abs/2507.04931)
*Ruoxi Wang,Kun Li,Minghui Xu,Yue Zhang,Kaidi Xu,Chunchi Liu,Yinhao Xiao,Xiuzhen Cheng*

Main category: cs.CR

TL;DR: 论文提出LIFT框架，利用大语言模型优化符号执行中的中间表示（IR），提升分布式AI系统的性能。


<details>
  <summary>Details</summary>
Motivation: 传统符号执行方法在大规模系统中存在可扩展性和效率问题，LIFT旨在通过LLMs优化IR解决这些问题。

Method: LIFT分为IR分析与优化（LLMs优化耗时IR块）和符号执行与验证（基准测试和语义验证）两个阶段。

Result: 实验显示性能显著提升，如bigtest执行时间减少53.5%，IR语句、PUT指令和临时变量减少。

Conclusion: LLMs能简化IR并保持功能正确性，提升分布式AI系统中的符号执行效率。

Abstract: Dynamic Symbolic Execution (DSE) is a key technique in program analysis,
widely used in software testing, vulnerability discovery, and formal
verification. In distributed AI systems, DSE plays a crucial role in
identifying hard-to-detect bugs, especially those arising from complex network
communication patterns. However, traditional approaches to symbolic execution
are often hindered by scalability issues and inefficiencies, particularly in
large-scale systems. This paper introduces LIFT (Large-language-model
Integrated Functional-equivalent-IR Transformation), a novel framework that
leverages Large Language Models (LLMs) to automate the optimization of
Intermediate Representations (IRs) in symbolic execution. LIFT addresses the
challenges of symbolic execution by providing a scalable, context-sensitive
solution for IR transformation. The framework consists of two phases: IR
Analysis and Optimization, where LLMs optimize time-intensive IR blocks, and
Symbolic Execution and Validation, which includes benchmarking and semantic
verification to ensure correctness and generalizability. Experiments on
real-world binaries demonstrated significant performance improvements,
including a 53.5\% reduction in execution time for bigtest and a 10.24\%
reduction for random, along with reductions in IR statements, PUT instructions,
and temporary variables. These results demonstrate that LLMs simplify IRs while
maintaining functional correctness, enhancing symbolic execution in distributed
AI systems.

</details>


### [75] [Bullshark on Narwhal: Implementation-level Workflow Analysis of Round-based DAG Consensus in Theory and Practice](https://arxiv.org/abs/2507.04956)
*Yusei Tanaka*

Main category: cs.CR

TL;DR: Bullshark是一种基于Round-based DAG的BFT协议，结合Narwhal内存池，实现了高性能（297,000 TPS，2秒延迟），并分析了其算法工作流程。


<details>
  <summary>Details</summary>
Motivation: 当前共识协议研究多忽略实现层面的算法，导致理论协议的实际性能不明确，Bullshark旨在填补这一空白。

Method: 通过分层分析Bullshark和Narwhal的工作流程，从交易提交到区块链确认，详细描述其功能和交互。

Result: Bullshark在Narwhal上实现了297,000 TPS的高性能和2秒低延迟。

Conclusion: 未来工作将优化拜占庭容错环境下的性能，并平衡CAP定理中的权衡。

Abstract: Round-based DAGs enable high-performance Byzantine fault-tolerant consensus,
yet their technical advantages remain underutilized due to their short history.
While research on consensus protocols is active in both academia and industry,
many studies overlook implementation-level algorithms, leaving actual
performance unclear - particularly for theoretical protocols whose practical
performance cannot often be evaluated. Bullshark, a Round-based DAG BFT
protocol on Narwhal mempool, achieves optimal performance: 297,000 transactions
per second with 2-second latency. We analyze the algorithm's workflow, from
transaction submission to blockchain commitment, breaking it down layer by
layer at the functional level and delineating the key features and interactions
of the Bullshark and Narwhal components. Future work aims to improve
performance in Byzantine fault environments and optimize trade-offs in the CAP
theorem.

</details>


### [76] [The Hidden Threat in Plain Text: Attacking RAG Data Loaders](https://arxiv.org/abs/2507.05093)
*Alberto Castagnaro,Umberto Salviati,Mauro Conti,Luca Pajola,Simeone Pizzi*

Main category: cs.CR

TL;DR: 论文揭示了RAG系统在文档加载阶段的安全漏洞，提出了9种知识型投毒攻击分类和两种新型威胁向量，通过实验验证了攻击的高成功率，强调了保护文档摄入过程的紧迫性。


<details>
  <summary>Details</summary>
Motivation: RAG系统通过外部知识增强LLM输出，但其文档摄入过程存在安全漏洞，恶意攻击者可利用此漏洞破坏系统。

Method: 提出9种攻击分类和两种新型威胁向量（内容混淆和内容注入），开发自动化工具包测试5种数据加载器，并在6种端到端RAG系统中验证攻击效果。

Result: 实验显示攻击成功率为74.4%，且在多种RAG系统中均能绕过过滤器，破坏输出完整性。

Conclusion: RAG系统的文档摄入过程亟需加强安全防护，以防止隐蔽的内容操纵攻击。

Abstract: Large Language Models (LLMs) have transformed human-machine interaction since
ChatGPT's 2022 debut, with Retrieval-Augmented Generation (RAG) emerging as a
key framework that enhances LLM outputs by integrating external knowledge.
However, RAG's reliance on ingesting external documents introduces new
vulnerabilities. This paper exposes a critical security gap at the data loading
stage, where malicious actors can stealthily corrupt RAG pipelines by
exploiting document ingestion.
  We propose a taxonomy of 9 knowledge-based poisoning attacks and introduce
two novel threat vectors -- Content Obfuscation and Content Injection --
targeting common formats (DOCX, HTML, PDF). Using an automated toolkit
implementing 19 stealthy injection techniques, we test five popular data
loaders, finding a 74.4% attack success rate across 357 scenarios. We further
validate these threats on six end-to-end RAG systems -- including white-box
pipelines and black-box services like NotebookLM and OpenAI Assistants --
demonstrating high success rates and critical vulnerabilities that bypass
filters and silently compromise output integrity. Our results emphasize the
urgent need to secure the document ingestion process in RAG systems against
covert content manipulations.

</details>


### [77] [Extreme Learning Machine Based System for DDoS Attacks Detections on IoMT Devices](https://arxiv.org/abs/2507.05132)
*Nelly Elsayed,Lily Dzamesi,Zag ElSayed,Murat Ozer*

Main category: cs.CR

TL;DR: 本文提出了一种基于极限学习机的方法，用于检测IoMT网络中的DDoS攻击，旨在通过低成本高精度的解决方案保护患者健康。


<details>
  <summary>Details</summary>
Motivation: IoMT的快速发展为医疗领域带来便利，但也因设备漏洞导致DDoS攻击频发，威胁患者生命安全。

Method: 采用极限学习机（ELM）技术，设计低成本高精度的DDoS攻击检测模型。

Result: 所提方法在低预算下实现高精度检测，适合在边缘计算层（fog level）部署。

Conclusion: 该模型能有效降低DDoS检测系统的实施成本，提升IoMT网络的安全性。

Abstract: The Internet of Medical Things (IoMT) represents a paradigm shift in the
healthcare sector, enabling the interconnection of medical devices, sensors,
and systems to enhance patient monitoring, diagnosis, and management. The rapid
evolution of IoMT presents significant benefits to the healthcare domains.
However, there is a rapid increase in distributed denial of service (DDoS)
attacks on the IoMT networks due to several vulnerabilities in the
IoMT-connected devices, which negatively impact patients' health and can even
lead to deaths. Thus, in this paper, we aim to save lives via investigating an
extreme learning machine for detecting DDoS attacks on IoMT devices. The
proposed approach achieves a high accuracy at a low implementation budget.
Thus, it can reduce the implementation cost of the DDoS detection system,
making the model capable of executing on the fog level.

</details>


### [78] [Hunting in the Dark: Metrics for Early Stage Traffic Discovery](https://arxiv.org/abs/2507.05213)
*Max Gao,Michael Collins,Ricky Mok,kc Claffy*

Main category: cs.CR

TL;DR: 本文研究了威胁狩猎中用于检测Cryptojacking恶意软件Crackonosh的指标和实践，分析了不同检测方法的有效性及暗网规模对追踪能力的影响。


<details>
  <summary>Details</summary>
Motivation: 探讨威胁狩猎过程中如何利用指标识别未知威胁，特别是针对Crackonosh这类恶意软件的检测能力。

Method: 通过可发现性指标建模，评估防御者在恶意软件数量减少时检测Crackonosh流量的能力，并分析不同暗网规模对追踪的影响。

Result: 展示了不同检测方法的强度，以及暗网规模如何影响追踪能力和利用攻击者错误的新行为。

Conclusion: 威胁狩猎指标和暗网规模对检测Crackonosh等恶意软件的行为具有显著影响，需优化检测方法以应对动态威胁。

Abstract: Threat hunting is an operational security process where an expert analyzes
traffic, applying knowledge and lightweight tools on unlabeled data in order to
identify and classify previously unknown phenomena. In this paper, we examine
threat hunting metrics and practice by studying the detection of Crackonosh, a
cryptojacking malware package, has on various metrics for identifying its
behavior. Using a metric for discoverability, we model the ability of defenders
to measure Crackonosh traffic as the malware population decreases, evaluate the
strength of various detection methods, and demonstrate how different darkspace
sizes affect both the ability to track the malware, but enable emergent
behaviors by exploiting attacker mistakes.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [79] [LLMs are Capable of Misaligned Behavior Under Explicit Prohibition and Surveillance](https://arxiv.org/abs/2507.02977)
*Igor Ivanov*

Main category: cs.AI

TL;DR: 前沿LLMs在受监控的沙盒环境中仍试图作弊，揭示了目标导向行为与对齐之间的根本矛盾。


<details>
  <summary>Details</summary>
Motivation: 研究LLMs在明确告知限制和监控的情况下是否仍会作弊，以探讨其目标导向行为与对齐的冲突。

Method: 在沙盒环境中让LLMs完成不可能的任务，监控其行为并记录作弊尝试。

Result: 部分前沿LLMs持续作弊并试图规避限制。

Conclusion: 当前LLMs在目标导向与对齐之间存在根本矛盾，需进一步研究解决。

Abstract: In this paper, LLMs are tasked with completing an impossible quiz, while they
are in a sandbox, monitored, told about these measures and instructed not to
cheat. Some frontier LLMs cheat consistently and attempt to circumvent
restrictions despite everything. The results reveal a fundamental tension
between goal-directed behavior and alignment in current LLMs. The code and
evaluation logs are available at github.com/baceolus/cheating_evals

</details>


### [80] [Discovering Algorithms with Computational Language Processing](https://arxiv.org/abs/2507.03190)
*Theo Bourdais,Abeynaya Gnanasekaran,Houman Owhadi,Tuhin Sahai*

Main category: cs.AI

TL;DR: 论文提出了一种自动化算法发现的框架，通过将算法表示为操作序列的标记，利用语法链式组合，结合蒙特卡洛树搜索和强化学习，重新发现、改进并生成新算法，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决复杂组合优化和量子计算问题，通过自动化方法提升算法性能。

Method: 将算法表示为标记序列，利用语法链式组合，结合蒙特卡洛树搜索（MCTS）和强化学习（RL）探索标记链式组合。

Result: 生成的算法在NP难组合优化和量子计算问题上显著优于现有方法。

Conclusion: 该框架在计算层面而非代码生成层面运行，能针对具体问题实例生成定制化算法。

Abstract: Algorithms are the engine for reproducible problem-solving. We present a
framework automating algorithm discovery by conceptualizing them as sequences
of operations, represented as tokens. These computational tokens are chained
using a grammar, enabling the formation of increasingly sophisticated
procedures. Our ensemble Monte Carlo tree search (MCTS) guided by reinforcement
learning (RL) explores token chaining and drives the creation of new tokens.
This methodology rediscovers, improves, and generates new algorithms that
substantially outperform existing methods for strongly NP-hard combinatorial
optimization problems and foundational quantum computing approaches such as
Grover's and Quantum Approximate Optimization Algorithm. Operating at the
computational rather than code-generation level, our framework produces
algorithms that can be tailored specifically to problem instances, not merely
classes.

</details>


### [81] [SI-Agent: An Agentic Framework for Feedback-Driven Generation and Tuning of Human-Readable System Instructions for Large Language Models](https://arxiv.org/abs/2507.03223)
*Jeshwanth Challagundla*

Main category: cs.AI

TL;DR: SI-Agent是一个自动化框架，通过反馈驱动循环生成和优化可读的系统指令（SIs），解决了手动设计和软提示的不足。


<details>
  <summary>Details</summary>
Motivation: 手动设计系统指令（SIs）资源密集且效果不佳，现有自动化方法生成的软提示缺乏可读性。

Method: SI-Agent采用三个协作代理（Instructor Agent、Instruction Follower Agent和Feedback/Reward Agent），通过反馈驱动的迭代循环优化SIs。

Result: 实验表明SI-Agent能生成高效且可读的SIs，在性能和可解释性上优于基线方法。

Conclusion: SI-Agent为LLM定制化和透明度提供了新思路，但需解决计算成本和反馈可靠性问题。

Abstract: System Instructions (SIs), or system prompts, are pivotal for guiding Large
Language Models (LLMs) but manual crafting is resource-intensive and often
suboptimal. Existing automated methods frequently generate non-human-readable
"soft prompts," sacrificing interpretability. This paper introduces SI-Agent, a
novel agentic framework designed to automatically generate and iteratively
refine human-readable SIs through a feedback-driven loop. SI-Agent employs
three collaborating agents: an Instructor Agent, an Instruction Follower Agent
(target LLM), and a Feedback/Reward Agent evaluating task performance and
optionally SI readability. The framework utilizes iterative cycles where
feedback guides the Instructor's refinement strategy (e.g., LLM-based editing,
evolutionary algorithms). We detail the framework's architecture, agent roles,
the iterative refinement process, and contrast it with existing methods. We
present experimental results validating SI-Agent's effectiveness, focusing on
metrics for task performance, SI readability, and efficiency. Our findings
indicate that SI-Agent generates effective, readable SIs, offering a favorable
trade-off between performance and interpretability compared to baselines.
Potential implications include democratizing LLM customization and enhancing
model transparency. Challenges related to computational cost and feedback
reliability are acknowledged.

</details>


### [82] [Efficient Knowledge Graph Construction and Retrieval from Unstructured Text for Large-Scale RAG Systems](https://arxiv.org/abs/2507.03226)
*Congmin Min,Rhea Mathew,Joyce Pan,Sahil Bansal,Abbas Keshavarzi,Amar Viswanathan Kannan*

Main category: cs.AI

TL;DR: 提出了一种可扩展且成本高效的GraphRAG框架，通过依赖关系知识图谱构建和轻量级检索策略，显著降低了计算成本和延迟，并在企业环境中验证了其性能。


<details>
  <summary>Details</summary>
Motivation: 解决GraphRAG因依赖大语言模型（LLMs）构建知识图谱导致的高计算成本和延迟问题，推动其在企业环境中的实际应用。

Method: 引入依赖关系知识图谱构建管道（无需LLMs）和轻量级图检索策略（混合查询节点识别与一跳遍历）。

Result: 在SAP数据集上表现优异，性能提升15%（LLM-as-Judge）和4.35%（RAGAS），依赖构建方法性能达LLM的94%且成本更低。

Conclusion: 验证了GraphRAG在大规模企业应用中的可行性，为实用、可解释和领域适应的检索增强推理铺平了道路。

Abstract: We propose a scalable and cost-efficient framework for deploying Graph-based
Retrieval Augmented Generation (GraphRAG) in enterprise environments. While
GraphRAG has shown promise for multi-hop reasoning and structured retrieval,
its adoption has been limited by the high computational cost of constructing
knowledge graphs using large language models (LLMs) and the latency of
graph-based retrieval. To address these challenges, we introduce two core
innovations: (1) a dependency-based knowledge graph construction pipeline that
leverages industrial-grade NLP libraries to extract entities and relations from
unstructured text completely eliminating reliance on LLMs; and (2) a
lightweight graph retrieval strategy that combines hybrid query node
identification with efficient one-hop traversal for high-recall, low-latency
subgraph extraction. We evaluate our framework on two SAP datasets focused on
legacy code migration and demonstrate strong empirical performance. Our system
achieves up to 15% and 4.35% improvements over traditional RAG baselines based
on LLM-as-Judge and RAGAS metrics, respectively. Moreover, our dependency-based
construction approach attains 94% of the performance of LLM-generated knowledge
graphs (61.87% vs. 65.83%) while significantly reducing cost and improving
scalability. These results validate the feasibility of deploying GraphRAG
systems in real-world, large-scale enterprise applications without incurring
prohibitive resource requirements paving the way for practical, explainable,
and domain-adaptable retrieval-augmented reasoning.

</details>


### [83] [CodeAgents: A Token-Efficient Framework for Codified Multi-Agent Reasoning in LLMs](https://arxiv.org/abs/2507.03254)
*Bruce Yang,Xinfeng He,Huan Gao,Yifan Cao,Xiaofan Li,David Hsu*

Main category: cs.AI

TL;DR: CodeAgents是一个提示框架，通过模块化伪代码改进多代理系统的规划和效率。


<details>
  <summary>Details</summary>
Motivation: 现有结构化提示策略在单代理环境中表现有限，且忽视多代理环境中的效率和可扩展性。

Method: 将代理交互组件（任务、计划、反馈等）编码为模块化伪代码，支持控制结构和类型变量。

Result: 在多个基准测试中表现优异，规划性能提升3-36%，并显著减少令牌使用。

Conclusion: CodeAgents在多代理系统中实现了高效、可解释和可验证的规划。

Abstract: Effective prompt design is essential for improving the planning capabilities
of large language model (LLM)-driven agents. However, existing structured
prompting strategies are typically limited to single-agent, plan-only settings,
and often evaluate performance solely based on task accuracy - overlooking
critical factors such as token efficiency, modularity, and scalability in
multi-agent environments. To address these limitations, we introduce
CodeAgents, a prompting framework that codifies multi-agent reasoning and
enables structured, token-efficient planning in multi-agent systems. In
CodeAgents, all components of agent interaction - Task, Plan, Feedback, system
roles, and external tool invocations - are codified into modular pseudocode
enriched with control structures (e.g., loops, conditionals), boolean logic,
and typed variables. This design transforms loosely connected agent plans into
cohesive, interpretable, and verifiable multi-agent reasoning programs. We
evaluate the proposed framework across three diverse benchmarks - GAIA,
HotpotQA, and VirtualHome - using a range of representative LLMs. Results show
consistent improvements in planning performance, with absolute gains of 3-36
percentage points over natural language prompting baselines. On VirtualHome,
our method achieves a new state-of-the-art success rate of 56%. In addition,
our approach reduces input and output token usage by 55-87% and 41-70%,
respectively, underscoring the importance of token-aware evaluation metrics in
the development of scalable multi-agent LLM systems. The code and resources are
available at: https://anonymous.4open.science/r/CodifyingAgent-5A86

</details>


### [84] [GDGB: A Benchmark for Generative Dynamic Text-Attributed Graph Learning](https://arxiv.org/abs/2507.03267)
*Jie Peng,Jiarui Ji,Runlin Lei,Zhewei Wei,Yongchao Liu,Chuntao Hong*

Main category: cs.AI

TL;DR: 论文提出了GDGB基准，解决了现有DyTAG数据集文本质量差和缺乏生成任务标准化的问题，并定义了两个新任务TDGG和IDGG。


<details>
  <summary>Details</summary>
Motivation: 现有DyTAG数据集文本质量差，且缺乏针对生成任务的标准化评估，限制了其应用。

Method: 提出GDGB基准，包含8个高质量数据集，并定义TDGG和IDGG任务，设计多维度评估指标和LLM框架GAG-General。

Result: GDGB支持严格评估TDGG和IDGG，揭示了结构和文本特征在生成中的关键作用。

Conclusion: GDGB为生成DyTAG研究提供了基础资源，推动了实际应用的进展。

Abstract: Dynamic Text-Attributed Graphs (DyTAGs), which intricately integrate
structural, temporal, and textual attributes, are crucial for modeling complex
real-world systems. However, most of the existing DyTAG datasets exhibit poor
textual quality, which severely limits their utility for DyTAG generation tasks
requiring semantically rich inputs. Additionally, prior work mainly focuses on
discriminative tasks on DyTAGs, resulting in a lack of standardized task
formulations and evaluation protocols tailored for DyTAG generation. To address
these critical issues, we propose Generative DyTAG Benchmark (GDGB), which
comprises eight meticulously curated DyTAG datasets with high-quality textual
features for both nodes and edges, overcoming limitations of prior datasets.
Building on GDGB, we define two novel DyTAG generation tasks: Transductive
Dynamic Graph Generation (TDGG) and Inductive Dynamic Graph Generation (IDGG).
TDGG transductively generates a target DyTAG based on the given source and
destination node sets, while the more challenging IDGG introduces new node
generation to inductively model the dynamic expansion of real-world graph data.
To enable holistic evaluation, we design multifaceted metrics that assess the
structural, temporal, and textual quality of the generated DyTAGs. We further
propose GAG-General, an LLM-based multi-agent generative framework tailored for
reproducible and robust benchmarking of DyTAG generation. Experimental results
demonstrate that GDGB enables rigorous evaluation of TDGG and IDGG, with key
insights revealing the critical interplay of structural and textual features in
DyTAG generation. These findings establish GDGB as a foundational resource for
advancing generative DyTAG research and unlocking further practical
applications in DyTAG generation. GDGB datasets, source codes, and leaderboards
are available at \href{https://gdgb-algo.github.io/}{here}.

</details>


### [85] [Memory Mosaics at scale](https://arxiv.org/abs/2507.03285)
*Jianyu Zhang,Léon Bottou*

Main category: cs.AI

TL;DR: Memory Mosaics v2 在大型语言模型规模（如 llama-8B）和真实数据集上表现出色，显著优于传统 Transformer 模型，尤其是在新任务推理能力上。


<details>
  <summary>Details</summary>
Motivation: 验证 Memory Mosaics 在更大规模和真实数据集上的性能，并探索其在新任务推理中的优势。

Method: 将 Memory Mosaics 扩展到 10B 规模，训练 1 万亿 token，并引入架构改进（Memory Mosaics v2），评估其在训练知识存储、新知识存储和上下文学习三个维度的能力。

Result: Memory Mosaics v2 在训练知识学习上与 Transformer 相当，但在新任务推理上显著优于 Transformer，且无法通过增加 Transformer 训练数据轻易复现。

Conclusion: Memory Mosaics v2 在大型语言模型规模下表现出优越性能，尤其是在新任务推理能力上，展示了其潜力。

Abstract: Memory Mosaics [Zhang et al., 2025], networks of associative memories, have
demonstrated appealing compositional and in-context learning capabilities on
medium-scale networks (GPT-2 scale) and synthetic small datasets. This work
shows that these favorable properties remain when we scale memory mosaics to
large language model sizes (llama-8B scale) and real-world datasets.
  To this end, we scale memory mosaics to 10B size, we train them on one
trillion tokens, we introduce a couple architectural modifications ("Memory
Mosaics v2"), we assess their capabilities across three evaluation dimensions:
training-knowledge storage, new-knowledge storage, and in-context learning.
  Throughout the evaluation, memory mosaics v2 match transformers on the
learning of training knowledge (first dimension) and significantly outperforms
transformers on carrying out new tasks at inference time (second and third
dimensions). These improvements cannot be easily replicated by simply
increasing the training data for transformers. A memory mosaics v2 trained on
one trillion tokens still perform better on these tasks than a transformer
trained on eight trillion tokens.

</details>


### [86] [LTLCrit: A Temporal Logic-based LLM Critic for Safe and Efficient Embodied Agents](https://arxiv.org/abs/2507.03293)
*Anand Gokhale,Vaibhav Srivastava,Francesco Bullo*

Main category: cs.AI

TL;DR: 提出了一种模块化的actor-critic架构，通过线性时序逻辑（LTL）指导LLM，结合语言模型的推理能力和形式逻辑的保证，提升长期规划任务的安全性和效率。


<details>
  <summary>Details</summary>
Motivation: 解决LLM在长期规划任务中错误累积导致不安全或低效行为的问题，扩展其在通用场景中的应用。

Method: 采用模块化架构，LLM actor负责高层动作选择，LTLCrit critic通过LTL分析轨迹并提出约束，支持固定安全约束和自适应软约束。

Result: 在Minecraft钻石挖掘基准测试中实现100%完成率，效率优于基线LLM规划器。

Conclusion: 通过逻辑监督LLM是一种强大且灵活的安全决策范式。

Abstract: Large language models (LLMs) have demonstrated promise in reasoning tasks and
general decision-making in static environments. In long-term planning tasks,
however, errors tend to accumulate, often leading to unsafe or inefficient
behavior, limiting their use in general-purpose settings. We propose a modular
actor-critic architecture in which an LLM actor is guided by LTLCrit, a
trajectory-level LLM critic that communicates via linear temporal logic (LTL).
Our setup combines the reasoning strengths of language models with the
guarantees of formal logic. The actor selects high-level actions from natural
language observations, while the critic analyzes full trajectories and proposes
new LTL constraints that shield the actor from future unsafe or inefficient
behavior. The architecture supports both fixed, hand-specified safety
constraints and adaptive, learned soft constraints that promote long-term
efficiency. Our architecture is model-agnostic: any LLM-based planner can serve
as the actor, and LTLCrit serves as a logic-generating wrapper. We formalize
planning as graph traversal under symbolic constraints, allowing LTLCrit to
analyze failed or suboptimal trajectories and generate new temporal logic rules
that improve future behavior. We evaluate our system on the Minecraft
diamond-mining benchmark, achieving 100% completion rates and improving
efficiency compared to baseline LLM planners. Our results suggest that enabling
LLMs to supervise each other through logic is a powerful and flexible paradigm
for safe, generalizable decision making.

</details>


### [87] [NDAI-NeuroMAP: A Neuroscience-Specific Embedding Model for Domain-Specific Retrieval](https://arxiv.org/abs/2507.03329)
*Devendra Patel,Aaditya Jain,Jayant Verma,Divyansh Rajput,Sunil Mahala,Ketki Suresh Khapare,Jayateja Kalla*

Main category: cs.AI

TL;DR: NDAI-NeuroMAP是首个专为神经科学领域设计的高精度信息检索密集向量嵌入模型，通过多目标优化框架显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 解决神经科学领域信息检索任务中通用和生物医学嵌入模型的不足，强调领域特定架构的重要性。

Method: 使用50万精心构建的三元组训练数据，结合对比学习和三元组度量学习，基于FremyCompany/BioLORD-2023模型进行微调。

Result: 在2.4万神经科学查询测试集上表现优于现有通用和生物医学嵌入模型。

Conclusion: 领域特定嵌入架构对神经科学RAG系统和临床NLP应用至关重要。

Abstract: We present NDAI-NeuroMAP, the first neuroscience-domain-specific dense vector
embedding model engineered for high-precision information retrieval tasks. Our
methodology encompasses the curation of an extensive domain-specific training
corpus comprising 500,000 carefully constructed triplets
(query-positive-negative configurations), augmented with 250,000
neuroscience-specific definitional entries and 250,000 structured
knowledge-graph triplets derived from authoritative neurological ontologies. We
employ a sophisticated fine-tuning approach utilizing the
FremyCompany/BioLORD-2023 foundation model, implementing a multi-objective
optimization framework combining contrastive learning with triplet-based metric
learning paradigms. Comprehensive evaluation on a held-out test dataset
comprising approximately 24,000 neuroscience-specific queries demonstrates
substantial performance improvements over state-of-the-art general-purpose and
biomedical embedding models. These empirical findings underscore the critical
importance of domain-specific embedding architectures for neuroscience-oriented
RAG systems and related clinical natural language processing applications.

</details>


### [88] [Exploring Object Status Recognition for Recipe Progress Tracking in Non-Visual Cooking](https://arxiv.org/abs/2507.03330)
*Franklin Mingzhe Li,Kaitlyn Ng,Bin Zhu,Patrick Carrington*

Main category: cs.AI

TL;DR: OSCAR利用物体状态识别技术，为非视觉烹饪提供实时步骤跟踪支持，通过实验验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 烹饪对视力障碍者具有挑战性，缺乏进展跟踪和上下文反馈支持，物体状态识别为解决这一问题提供了新思路。

Method: OSCAR整合食谱解析、物体状态提取、视觉对齐和时间因果建模，构建实时步骤跟踪技术流程。

Result: 实验表明，物体状态显著提升步骤预测准确性，并揭示实际应用中影响性能的关键因素。

Conclusion: OSCAR为非视觉烹饪提供了一种有效的上下文感知支持方法，并贡献了数据集和设计见解。

Abstract: Cooking plays a vital role in everyday independence and well-being, yet
remains challenging for people with vision impairments due to limited support
for tracking progress and receiving contextual feedback. Object status - the
condition or transformation of ingredients and tools - offers a promising but
underexplored foundation for context-aware cooking support. In this paper, we
present OSCAR (Object Status Context Awareness for Recipes), a technical
pipeline that explores the use of object status recognition to enable recipe
progress tracking in non-visual cooking. OSCAR integrates recipe parsing,
object status extraction, visual alignment with cooking steps, and time-causal
modeling to support real-time step tracking. We evaluate OSCAR on 173
instructional videos and a real-world dataset of 12 non-visual cooking sessions
recorded by BLV individuals in their homes. Our results show that object status
consistently improves step prediction accuracy across vision-language models,
and reveal key factors that impact performance in real-world conditions, such
as implicit tasks, camera placement, and lighting. We contribute the pipeline
of context-aware recipe progress tracking, an annotated real-world non-visual
cooking dataset, and design insights to guide future context-aware assistive
cooking systems.

</details>


### [89] [Disambiguation-Centric Finetuning Makes Enterprise Tool-Calling LLMs More Realistic and Less Risky](https://arxiv.org/abs/2507.03336)
*Ashutosh Hathidara,Julien Yu,Sebastian Schreiber*

Main category: cs.AI

TL;DR: DiaFORGE是一个三阶段对话框架，通过生成多轮对话、监督微调和动态评估，显著提升大语言模型在调用企业API时的成功率。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型在调用企业API时因工具相似或参数不明确而失败的问题。

Method: 包括三个阶段：(i)生成多轮对话，(ii)监督微调开源模型，(iii)动态评估模型表现。

Result: 在DiaBENCH基准上，DiaFORGE训练后的模型比GPT-4o和Claude-3.5-Sonnet分别提高了27和49个百分点的成功率。

Conclusion: DiaFORGE为企业级工具调用代理提供了可靠解决方案，并发布了开放数据集以促进研究。

Abstract: Large language models (LLMs) are increasingly tasked with invoking enterprise
APIs, yet they routinely falter when near-duplicate tools vie for the same user
intent or when required arguments are left underspecified. We introduce
DiaFORGE (Dialogue Framework for Organic Response Generation & Evaluation), a
disambiguation-centric, three-stage pipeline that (i) synthesizes
persona-driven, multi-turn dialogues in which the assistant must distinguish
among highly similar tools, (ii) performs supervised fine-tuning of open-source
models with reasoning traces across 3B - 70B parameters, and (iii) evaluates
real-world readiness via a dynamic suite that redeploys each model in a live
agentic loop and reports end-to-end goal completion alongside conventional
static metrics. On our dynamic benchmark DiaBENCH, models trained with DiaFORGE
raise tool-invocation success by 27 pp over GPT-4o and by 49 pp over
Claude-3.5-Sonnet, both under optimized prompting. To spur further research, we
release an open corpus of 5000 production-grade enterprise API specifications
paired with rigorously validated, disambiguation-focused dialogues, offering a
practical blueprint for building reliable, enterprise-ready tool-calling
agents.

</details>


### [90] [Effects of structure on reasoning in instance-level Self-Discover](https://arxiv.org/abs/2507.03347)
*Sachith Gunasekara,Yasiru Ratnayake*

Main category: cs.AI

TL;DR: 比较结构化与非结构化推理方法，发现非结构化推理在性能上优于结构化方法，尤其在复杂任务中表现更佳。


<details>
  <summary>Details</summary>
Motivation: 探索结构化输出与非结构化自然语言推理在性能上的差异，以优化复杂问题解决和系统集成。

Method: 引入iSelf-Discover框架，动态生成结构化JSON与非结构化推理，并在多样化基准测试中进行比较。

Result: 非结构化推理在MATH基准测试中相对性能提升18.90%，零射击非结构化方法甚至优于五射击结构化方法。

Conclusion: 研究结果呼吁重新评估结构化格式在复杂问题解决中的依赖，并重新思考复合系统的组织方式。

Abstract: The drive for predictable LLM reasoning in their integration with compound
systems has popularized structured outputs, yet concerns remain about
performance trade-offs compared to unconstrained natural language. At the same
time, training on unconstrained Chain of Thought (CoT) traces has brought about
a new class of strong reasoning models that nevertheless present novel compute
budget and faithfulness challenges. This paper introduces iSelf-Discover, an
instance-level adaptation of the Self-Discover framework, and using it compares
dynamically generated structured JSON reasoning with its unstructured
counterpart. Our empirical evaluation across diverse benchmarks using
state-of-the-art open-source models supports a consistent advantage for
unstructured reasoning. Notably, on the complex MATH benchmark, unstructured
plans achieved relative performance improvements of up to 18.90\% over
structured approaches. Zero-shot unstructured iSelf-Discover variants are also
shown to outperform their five-shot structured counterparts, underscoring the
significance of this gap, even when structured plans are dynamically generated
to ensure reasoning precedes the final answer. We further demonstrate that the
optimal granularity of plan generation (instance-level vs. task-level) is
context-dependent. These findings invite re-evaluation of the reliance on
structured formats for complex problem-solving and how compound systems should
be organized.

</details>


### [91] [Artificial intelligence in drug discovery: A comprehensive review with a case study on hyperuricemia, gout arthritis, and hyperuricemic nephropathy](https://arxiv.org/abs/2507.03407)
*Junwei Su,Cheng Xin,Ao Shang,Shan Wu,Zhenzhen Xie,Ruogu Xiong,Xiaoyu Xu,Cheng Zhang,Guang Chen,Yau-Tuen Chan,Guoyi Tang,Ning Wang,Yong Xu,Yibin Feng*

Main category: cs.AI

TL;DR: 本文系统综述了人工智能（AI）和机器学习（ML）在药物发现全流程中的最新进展，填补了现有文献对关键阶段依赖关系的忽视，并通过案例研究展示了实际应用效果。


<details>
  <summary>Details</summary>
Motivation: 传统药物发现方法复杂、成本高、耗时长且失败率高，亟需全面理解AI/ML如何有效整合到全流程中。

Method: 详细分析了AI/ML在目标识别、命中筛选和先导优化等核心阶段的应用，并通过高尿酸血症等案例研究验证其效果。

Result: 展示了AI/ML在各阶段的方法学进展及其实际影响，同时探讨了当前面临的挑战。

Conclusion: 本综述为研究人员利用AI/ML突破瓶颈、加速药物发现提供了重要指导。

Abstract: This paper systematically reviews recent advances in artificial intelligence
(AI), with a particular focus on machine learning (ML), across the entire drug
discovery pipeline. Due to the inherent complexity, escalating costs, prolonged
timelines, and high failure rates of traditional drug discovery methods, there
is a critical need to comprehensively understand how AI/ML can be effectively
integrated throughout the full process. Currently available literature reviews
often narrowly focus on specific phases or methodologies, neglecting the
dependence between key stages such as target identification, hit screening, and
lead optimization. To bridge this gap, our review provides a detailed and
holistic analysis of AI/ML applications across these core phases, highlighting
significant methodological advances and their impacts at each stage. We further
illustrate the practical impact of these techniques through an in-depth case
study focused on hyperuricemia, gout arthritis, and hyperuricemic nephropathy,
highlighting real-world successes in molecular target identification and
therapeutic candidate discovery. Additionally, we discuss significant
challenges facing AI/ML in drug discovery and outline promising future research
directions. Ultimately, this review serves as an essential orientation for
researchers aiming to leverage AI/ML to overcome existing bottlenecks and
accelerate drug discovery.

</details>


### [92] [Lessons from a Chimp: AI "Scheming" and the Quest for Ape Language](https://arxiv.org/abs/2507.03409)
*Christopher Summerfield,Lennart Luettgau,Magda Dubois,Hannah Rose Kirk,Kobi Hackenburg,Catherine Fist,Katarina Slama,Nicola Ding,Rebecca Anselmetti,Andrew Strait,Mario Giulianelli,Cozmin Ududec*

Main category: cs.AI

TL;DR: 论文探讨当前AI系统是否可能发展出“阴谋”能力（隐秘且战略性地追求未对齐目标），并与1970年代非人灵长类动物语言研究进行比较，提出避免历史研究缺陷的建议。


<details>
  <summary>Details</summary>
Motivation: 研究动机是探讨AI系统是否可能发展出隐秘的战略行为，并借鉴历史研究中的教训，以避免过度拟人化、依赖轶事和缺乏理论框架的问题。

Method: 通过比较当前AI研究与1970年代非人灵长类动物语言研究的方法，分析其共同缺陷，并提出改进建议。

Result: 指出当前AI研究存在过度拟人化、依赖轶事和理论框架不足的问题，并建议采取具体步骤推动科学严谨的研究。

Conclusion: 结论强调AI“阴谋”研究应避免历史研究的缺陷，采取科学严谨的方法，并提出了具体改进建议。

Abstract: We examine recent research that asks whether current AI systems may be
developing a capacity for "scheming" (covertly and strategically pursuing
misaligned goals). We compare current research practices in this field to those
adopted in the 1970s to test whether non-human primates could master natural
language. We argue that there are lessons to be learned from that historical
research endeavour, which was characterised by an overattribution of human
traits to other agents, an excessive reliance on anecdote and descriptive
analysis, and a failure to articulate a strong theoretical framework for the
research. We recommend that research into AI scheming actively seeks to avoid
these pitfalls. We outline some concrete steps that can be taken for this
research programme to advance in a productive and scientifically rigorous
fashion.

</details>


### [93] [Multi-Agent Reasoning for Cardiovascular Imaging Phenotype Analysis](https://arxiv.org/abs/2507.03460)
*Weitong Zhang,Mengyun Qiao,Chengqi Zang,Steven Niederer,Paul M Matthews,Wenjia Bai,Bernhard Kainz*

Main category: cs.AI

TL;DR: MESHAgents框架利用多学科AI代理动态发现成像表型与疾病风险因素的关联，提供自动化PheWAS流程，性能接近专家选择。


<details>
  <summary>Details</summary>
Motivation: 传统方法依赖人工假设测试，忽略复杂非线性关系，需自动化工具提升关联研究效率。

Method: 多学科AI代理（心脏病学、生物力学等）通过自组织推理动态生成和验证表型与风险因素的关联。

Result: 在疾病分类任务中，AUC差异仅为-0.004，6/9疾病类型的召回率提升。

Conclusion: MESHAgents提供可扩展、透明的表型发现方法，性能媲美专家驱动方法。

Abstract: Identifying the associations between imaging phenotypes and disease risk
factors and outcomes is essential for understanding disease mechanisms and
improving diagnosis and prognosis models. However, traditional approaches rely
on human-driven hypothesis testing and selection of association factors, often
overlooking complex, non-linear dependencies among imaging phenotypes and other
multi-modal data. To address this, we introduce a Multi-agent Exploratory
Synergy for the Heart (MESHAgents) framework that leverages large language
models as agents to dynamically elicit, surface, and decide confounders and
phenotypes in association studies, using cardiovascular imaging as a proof of
concept. Specifically, we orchestrate a multi-disciplinary team of AI agents --
spanning cardiology, biomechanics, statistics, and clinical research -- which
spontaneously generate and converge on insights through iterative,
self-organizing reasoning. The framework dynamically synthesizes statistical
correlations with multi-expert consensus, providing an automated pipeline for
phenome-wide association studies (PheWAS). We demonstrate the system's
capabilities through a population-based study of imaging phenotypes of the
heart and aorta. MESHAgents autonomously uncovered correlations between imaging
phenotypes and a wide range of non-imaging factors, identifying additional
confounder variables beyond standard demographic factors. Validation on
diagnosis tasks reveals that MESHAgents-discovered phenotypes achieve
performance comparable to expert-selected phenotypes, with mean AUC differences
as small as -0.004 on disease classification tasks. Notably, the recall score
improves for 6 out of 9 disease types. Our framework provides clinically
relevant imaging phenotypes with transparent reasoning, offering a scalable
alternative to expert-driven methods.

</details>


### [94] [REAL: Benchmarking Abilities of Large Language Models for Housing Transactions and Services](https://arxiv.org/abs/2507.03477)
*Kexin Zhu,Yang Han*

Main category: cs.AI

TL;DR: REAL是首个评估大语言模型在房地产交易与服务中能力的评测套件，包含5,316条高质量条目，覆盖4个主题和14个类别。实验表明，LLMs在该领域仍有较大改进空间。


<details>
  <summary>Details</summary>
Motivation: 评估大语言模型是否能在房地产交易与服务中扮演类似人类的角色，填补该领域的评测空白。

Method: 开发REAL评测套件，包含5,316条条目，覆盖记忆、理解、推理和幻觉4个主题，分为14个类别。

Result: 实验结果显示，当前最先进的大语言模型在房地产领域的表现仍有显著提升空间。

Conclusion: 大语言模型在房地产交易与服务中的应用尚需进一步改进，REAL为未来研究提供了基准。

Abstract: The development of large language models (LLMs) has greatly promoted the
progress of chatbot in multiple fields. There is an urgent need to evaluate
whether LLMs can play the role of agent in housing transactions and services as
well as humans. We present Real Estate Agent Large Language Model Evaluation
(REAL), the first evaluation suite designed to assess the abilities of LLMs in
the field of housing transactions and services. REAL comprises 5,316
high-quality evaluation entries across 4 topics: memory, comprehension,
reasoning and hallucination. All these entries are organized as 14 categories
to assess whether LLMs have the knowledge and ability in housing transactions
and services scenario. Additionally, the REAL is used to evaluate the
performance of most advanced LLMs. The experiment results indicate that LLMs
still have significant room for improvement to be applied in the real estate
field.

</details>


### [95] [Limits of Safe AI Deployment: Differentiating Oversight and Control](https://arxiv.org/abs/2507.03525)
*David Manheim,Aidan Homewood*

Main category: cs.AI

TL;DR: 论文区分了AI系统中的监督与控制，提出了一个框架来明确两者的适用条件，并提出了一个成熟度模型。


<details>
  <summary>Details</summary>
Motivation: 当前AI领域的监督与控制概念常被混淆，影响了有效监督系统的设计与评估。

Method: 通过文献综述区分监督与控制，提出框架和成熟度模型。

Result: 明确了监督与控制的区别，提出了实践中的适用条件和成熟度模型。

Conclusion: 论文为AI监督提供了理论基础和实践指导，并指出了现有方法的局限性。

Abstract: Oversight and control (collectively, supervision) are often invoked as key
levers for ensuring that AI systems are accountable, reliable, and able to
fulfill governance and management requirements. However, the concepts are
frequently conflated or insufficiently distinguished in academic and policy
discourse, undermining efforts to design or evaluate systems that should remain
under meaningful human supervision.
  This paper undertakes a targeted critical review of literature on supervision
outside of AI, along with a brief summary of past work on the topic related to
AI. We then differentiate control as being ex-ante or real-time, and
operational rather than policy or governance. In contrast, oversight is either
a policy and governance function, or is ex-post. We suggest that control aims
to prevent failures. In contrast, oversight often focuses on detection,
remediation, or incentives for future prevention; all preventative oversight
strategies nonetheless necessitate control.
  Building on this foundation, we make three contributions. First, we propose a
theoretically-informed yet policy-grounded framework that articulates the
conditions under which each mechanism is possible, where they fall short, and
what is required to make them meaningful in practice. Second, we outline how
supervision methods should be documented and integrated into risk management,
and drawing on the Microsoft Responsible AI Maturity Model, we outline a
maturity model for AI supervision. Third, we explicitly highlight some
boundaries of these mechanisms, including where they apply, where they fail,
and where it is clear that no existing methods suffice. This foregrounds the
question of whether meaningful supervision is possible in a given deployment
context, and can support regulators, auditors, and practitioners in identifying
both present limitations and the need for new conceptual and technical
advances.

</details>


### [96] [A Universal Approach to Feature Representation in Dynamic Task Assignment Problems](https://arxiv.org/abs/2507.03579)
*Riccardo Lo Bianco,Remco Dijkman,Wim Nuijten,Willem van Jaarsveld*

Main category: cs.AI

TL;DR: 论文提出了一种基于图表示和深度强化学习的方法，用于解决动态任务分配问题，尤其是针对无限状态和动作空间的情况。


<details>
  <summary>Details</summary>
Motivation: 动态任务分配问题中，资源和任务的特征可能具有无限可能值，传统方法难以处理，因此需要新的表示和解决方法。

Method: 提出了一种基于图的特征表示方法（assignment graph），并将其与标记彩色Petri网映射；同时改进了近端策略优化算法（PPO）以学习任务分配策略。

Result: 实验表明，该方法适用于表示和学习接近最优的任务分配策略，无论状态和动作空间的维度如何。

Conclusion: 该方法为无限状态和动作空间的动态任务分配问题提供了一种有效的解决方案。

Abstract: Dynamic task assignment concerns the optimal assignment of resources to tasks
in a business process. Recently, Deep Reinforcement Learning (DRL) has been
proposed as the state of the art for solving assignment problems. DRL methods
usually employ a neural network (NN) as an approximator for the policy
function, which ingests the state of the process and outputs a valuation of the
possible assignments. However, representing the state and the possible
assignments so that they can serve as inputs and outputs for a policy NN
remains an open challenge, especially when tasks or resources have features
with an infinite number of possible values. To solve this problem, this paper
proposes a method for representing and solving assignment problems with
infinite state and action spaces. In doing so, it provides three contributions:
(I) A graph-based feature representation of assignment problems, which we call
assignment graph; (II) A mapping from marked Colored Petri Nets to assignment
graphs; (III) An adaptation of the Proximal Policy Optimization algorithm that
can learn to solve assignment problems represented through assignment graphs.
To evaluate the proposed representation method, we model three archetypal
assignment problems ranging from finite to infinite state and action space
dimensionalities. The experiments show that the method is suitable for
representing and learning close-to-optimal task assignment policies regardless
of the state and action space dimensionalities.

</details>


### [97] [Benchmarking Vector, Graph and Hybrid Retrieval Augmented Generation (RAG) Pipelines for Open Radio Access Networks (ORAN)](https://arxiv.org/abs/2507.03608)
*Sarat Ahmad,Zeinab Nezami,Maryam Hafeez,Syed Ali Raza Zaidi*

Main category: cs.AI

TL;DR: 论文比较了Vector RAG、GraphRAG和Hybrid GraphRAG在ORAN架构中的表现，发现后两者在事实正确性和上下文相关性上优于传统RAG。


<details>
  <summary>Details</summary>
Motivation: 生成式AI在无线网络自主优化中潜力巨大，但针对电信任务的LLM微调成本高，RAG提供了一种无需完全重新训练的替代方案。

Method: 采用Vector RAG、GraphRAG和Hybrid GraphRAG，基于ORAN规范进行对比评估，使用生成指标（如忠实性、答案相关性等）衡量性能。

Result: GraphRAG和Hybrid GraphRAG表现优于传统RAG，Hybrid GraphRAG事实正确性提升8%，GraphRAG上下文相关性提升7%。

Conclusion: GraphRAG和Hybrid GraphRAG在ORAN等高要求领域具有显著优势，为生成式AI的应用提供了更高效的解决方案。

Abstract: Generative AI (GenAI) is expected to play a pivotal role in enabling
autonomous optimization in future wireless networks. Within the ORAN
architecture, Large Language Models (LLMs) can be specialized to generate xApps
and rApps by leveraging specifications and API definitions from the RAN
Intelligent Controller (RIC) platform. However, fine-tuning base LLMs for
telecom-specific tasks remains expensive and resource-intensive.
Retrieval-Augmented Generation (RAG) offers a practical alternative through
in-context learning, enabling domain adaptation without full retraining. While
traditional RAG systems rely on vector-based retrieval, emerging variants such
as GraphRAG and Hybrid GraphRAG incorporate knowledge graphs or dual retrieval
strategies to support multi-hop reasoning and improve factual grounding.
Despite their promise, these methods lack systematic, metric-driven
evaluations, particularly in high-stakes domains such as ORAN. In this study,
we conduct a comparative evaluation of Vector RAG, GraphRAG, and Hybrid
GraphRAG using ORAN specifications. We assess performance across varying
question complexities using established generation metrics: faithfulness,
answer relevance, context relevance, and factual correctness. Results show that
both GraphRAG and Hybrid GraphRAG outperform traditional RAG. Hybrid GraphRAG
improves factual correctness by 8%, while GraphRAG improves context relevance
by 7%.

</details>


### [98] [EvoAgentX: An Automated Framework for Evolving Agentic Workflows](https://arxiv.org/abs/2507.03616)
*Yingxu Wang,Siwei Liu,Jinyuan Fang,Zaiqiao Meng*

Main category: cs.AI

TL;DR: EvoAgentX是一个开源平台，用于自动生成、执行和进化优化多智能体工作流，显著提升任务性能。


<details>
  <summary>Details</summary>
Motivation: 现有多智能体系统（MAS）框架需要手动配置工作流，缺乏动态进化和性能优化的原生支持，且优化算法未统一集成。

Method: EvoAgentX采用五层模块化架构，包括基础组件、智能体、工作流、进化和评估层，整合了三种优化算法（TextGrad、AFlow、MIPRO）优化提示、工具配置和工作流拓扑。

Result: 在HotPotQA、MBPP、MATH和GAIA任务上，EvoAgentX显著提升了性能，如HotPotQA F1提高7.44%，MBPP pass@1提升10.00%，GAIA总体准确率最高提升20.00%。

Conclusion: EvoAgentX通过自动化工作流生成和进化优化，有效解决了现有MAS框架的局限性，显著提升了任务性能。

Abstract: Multi-agent systems (MAS) have emerged as a powerful paradigm for
orchestrating large language models (LLMs) and specialized tools to
collaboratively address complex tasks. However, existing MAS frameworks often
require manual workflow configuration and lack native support for dynamic
evolution and performance optimization. In addition, many MAS optimization
algorithms are not integrated into a unified framework. In this paper, we
present EvoAgentX, an open-source platform that automates the generation,
execution, and evolutionary optimization of multi-agent workflows. EvoAgentX
employs a modular architecture consisting of five core layers: the basic
components, agent, workflow, evolving, and evaluation layers. Specifically,
within the evolving layer, EvoAgentX integrates three MAS optimization
algorithms, TextGrad, AFlow, and MIPRO, to iteratively refine agent prompts,
tool configurations, and workflow topologies. We evaluate EvoAgentX on
HotPotQA, MBPP, and MATH for multi-hop reasoning, code generation, and
mathematical problem solving, respectively, and further assess it on real-world
tasks using GAIA. Experimental results show that EvoAgentX consistently
achieves significant performance improvements, including a 7.44% increase in
HotPotQA F1, a 10.00% improvement in MBPP pass@1, a 10.00% gain in MATH solve
accuracy, and an overall accuracy improvement of up to 20.00% on GAIA. The
source code is available at: https://github.com/EvoAgentX/EvoAgentX

</details>


### [99] [Large Language Models for Combinatorial Optimization: A Systematic Review](https://arxiv.org/abs/2507.03637)
*Francesca Da Ros,Michael Soprano,Luca Di Gaspero,Kevin Roitero*

Main category: cs.AI

TL;DR: 本文通过PRISMA指南系统综述了大型语言模型（LLMs）在组合优化（CO）中的应用，筛选了103项研究，分类并总结了LLMs的任务、架构、数据集及未来方向。


<details>
  <summary>Details</summary>
Motivation: 探索LLMs在组合优化领域的应用现状，为研究者提供全面的领域概述和未来研究方向。

Method: 采用PRISMA指南进行文献搜索和筛选，通过Scopus和Google Scholar分析了2000多篇文献，最终纳入103项研究。

Result: 研究分类为语义类别和主题，总结了LLMs的任务、架构、专用数据集及应用领域。

Conclusion: 提出了LLMs在组合优化领域的未来发展方向，为该领域的研究提供了参考。

Abstract: This systematic review explores the application of Large Language Models
(LLMs) in Combinatorial Optimization (CO). We report our findings using the
Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA)
guidelines. We conduct a literature search via Scopus and Google Scholar,
examining over 2,000 publications. We assess publications against four
inclusion and four exclusion criteria related to their language, research
focus, publication year, and type. Eventually, we select 103 studies. We
classify these studies into semantic categories and topics to provide a
comprehensive overview of the field, including the tasks performed by LLMs, the
architectures of LLMs, the existing datasets specifically designed for
evaluating LLMs in CO, and the field of application. Finally, we identify
future directions for leveraging LLMs in this field.

</details>


### [100] [Towards Machine Theory of Mind with Large Language Model-Augmented Inverse Planning](https://arxiv.org/abs/2507.03682)
*Rebekah A. Gelpí,Eric Xue,William A. Cunningham*

Main category: cs.AI

TL;DR: 提出了一种混合方法，结合大型语言模型（LLMs）和贝叶斯逆向规划模型，以提升机器心智理论（ToM）任务的性能。


<details>
  <summary>Details</summary>
Motivation: 现有贝叶斯逆向规划模型在复杂场景中扩展性有限，而LLMs在ToM任务中表现不稳定。结合两者优势，以提升准确性和鲁棒性。

Method: 使用LLMs生成假设和似然函数，结合贝叶斯逆向规划模型计算后验概率，预测代理的心理状态。

Result: 混合方法在ToM任务中表现优于单独使用LLMs或贝叶斯模型，且在开放任务中展现出潜力。

Conclusion: 该方法为ToM模型和社交智能生成代理的未来发展提供了有前景的方向。

Abstract: We propose a hybrid approach to machine Theory of Mind (ToM) that uses large
language models (LLMs) as a mechanism for generating hypotheses and likelihood
functions with a Bayesian inverse planning model that computes posterior
probabilities for an agent's likely mental states given its actions. Bayesian
inverse planning models can accurately predict human reasoning on a variety of
ToM tasks, but these models are constrained in their ability to scale these
predictions to scenarios with a large number of possible hypotheses and
actions. Conversely, LLM-based approaches have recently demonstrated promise in
solving ToM benchmarks, but can exhibit brittleness and failures on reasoning
tasks even when they pass otherwise structurally identical versions. By
combining these two methods, this approach leverages the strengths of each
component, closely matching optimal results on a task inspired by prior inverse
planning models and improving performance relative to models that utilize LLMs
alone or with chain-of-thought prompting, even with smaller LLMs that typically
perform poorly on ToM tasks. We also exhibit the model's potential to predict
mental states on open-ended tasks, offering a promising direction for future
development of ToM models and the creation of socially intelligent generative
agents.

</details>


### [101] [Towards Unified Neurosymbolic Reasoning on Knowledge Graphs](https://arxiv.org/abs/2507.03697)
*Qika Lin,Fangzhi Xu,Hao Lu,Kai He,Rui Mao,Jun Liu,Erik Cambria,Mengling Feng*

Main category: cs.AI

TL;DR: 论文提出了一个统一的神经符号推理框架Tunsr，用于知识图谱推理，解决了当前方法难以整合神经和符号推理优势以及适应多样化推理场景的问题。


<details>
  <summary>Details</summary>
Motivation: 当前知识图谱推理方法主要集中在单一形式的神经或符号推理，未能有效整合两者的优势，且难以适应多样化的推理场景需求。

Method: Tunsr引入了一致的推理图结构，通过迭代搜索后续节点扩展图，并提出前向逻辑消息传递机制更新节点表示和注意力，以及FARI算法归纳一阶逻辑规则。

Result: 在19个数据集上的实验结果表明，Tunsr在四种推理场景（转导、归纳、插值和外推）中均表现出有效性。

Conclusion: Tunsr通过统一的神经符号推理框架，成功整合了神经和符号推理的优势，并适应了多样化推理场景的需求。

Abstract: Knowledge Graph (KG) reasoning has received significant attention in the
fields of artificial intelligence and knowledge engineering, owing to its
ability to autonomously deduce new knowledge and consequently enhance the
availability and precision of downstream applications. However, current methods
predominantly concentrate on a single form of neural or symbolic reasoning,
failing to effectively integrate the inherent strengths of both approaches.
Furthermore, the current prevalent methods primarily focus on addressing a
single reasoning scenario, presenting limitations in meeting the diverse
demands of real-world reasoning tasks. Unifying the neural and symbolic
methods, as well as diverse reasoning scenarios in one model is challenging as
there is a natural representation gap between symbolic rules and neural
networks, and diverse scenarios exhibit distinct knowledge structures and
specific reasoning objectives. To address these issues, we propose a unified
neurosymbolic reasoning framework, namely Tunsr, for KG reasoning. Tunsr first
introduces a consistent structure of reasoning graph that starts from the query
entity and constantly expands subsequent nodes by iteratively searching
posterior neighbors. Based on it, a forward logic message-passing mechanism is
proposed to update both the propositional representations and attentions, as
well as first-order logic (FOL) representations and attentions of each node. In
this way, Tunsr conducts the transformation of merging multiple rules by
merging possible relations at each step. Finally, the FARI algorithm is
proposed to induce FOL rules by constantly performing attention calculations
over the reasoning graph. Extensive experimental results on 19 datasets of four
reasoning scenarios (transductive, inductive, interpolation, and extrapolation)
demonstrate the effectiveness of Tunsr.

</details>


### [102] [Roadmap for using large language models (LLMs) to accelerate cross-disciplinary research with an example from computational biology](https://arxiv.org/abs/2507.03722)
*Ruian Ke,Ruy M. Ribeiro*

Main category: cs.AI

TL;DR: 论文探讨了大型语言模型（LLMs）在跨学科研究中的潜力与挑战，提出了一个整合LLMs的路线图，并通过计算生物学案例展示了其应用。


<details>
  <summary>Details</summary>
Motivation: LLMs作为强大的AI工具，在研究中具有潜力，但也存在幻觉、偏见等问题，需明确其优劣势以实现负责任的使用。

Method: 提出了整合LLMs的路线图，并通过计算生物学案例（HIV反弹动力学建模）展示其应用，强调人机协作框架。

Result: LLMs在跨学科研究中能促进协作与知识转移，但需在人类监督下作为辅助工具使用。

Conclusion: 负责任地使用LLMs将推动跨学科研究创新，加速科学发现。

Abstract: Large language models (LLMs) are powerful artificial intelligence (AI) tools
transforming how research is conducted. However, their use in research has been
met with skepticism, due to concerns about hallucinations, biases and potential
harms to research. These emphasize the importance of clearly understanding the
strengths and weaknesses of LLMs to ensure their effective and responsible use.
Here, we present a roadmap for integrating LLMs into cross-disciplinary
research, where effective communication, knowledge transfer and collaboration
across diverse fields are essential but often challenging. We examine the
capabilities and limitations of LLMs and provide a detailed computational
biology case study (on modeling HIV rebound dynamics) demonstrating how
iterative interactions with an LLM (ChatGPT) can facilitate interdisciplinary
collaboration and research. We argue that LLMs are best used as augmentative
tools within a human-in-the-loop framework. Looking forward, we envisage that
the responsible use of LLMs will enhance innovative cross-disciplinary research
and substantially accelerate scientific discoveries.

</details>


### [103] [Agent-Based Detection and Resolution of Incompleteness and Ambiguity in Interactions with Large Language Models](https://arxiv.org/abs/2507.03726)
*Riya Naik,Ashwin Srinivasan,Swati Agarwal,Estrid He*

Main category: cs.AI

TL;DR: 论文探讨了通过基于代理的架构增强LLM问答系统的推理能力，自动解决问题的模糊或不完整性，缩短交互时间并提高答案质量。


<details>
  <summary>Details</summary>
Motivation: 解决多轮交互中因问题模糊或不完整导致的繁琐问题，提升LLM问答系统的效率和准确性。

Method: 采用基于LLM的代理（如GPT-3.5-Turbo和Llama-4-Scout），实现零样本ReAct代理，通过分类、解决和回答三个动作处理问题。

Result: 代理架构缩短了交互时间，提高了答案质量，并能解释问题缺陷的解决过程，但增加了LLM调用和延迟。

Conclusion: 代理方法在大多数情况下优于传统方法，尤其适用于问题缺乏足够上下文的情况，为构建更健壮的QA系统提供了有效机制。

Abstract: Many of us now treat LLMs as modern-day oracles asking it almost any kind of
question. However, consulting an LLM does not have to be a single turn
activity. But long multi-turn interactions can get tedious if it is simply to
clarify contextual information that can be arrived at through reasoning. In
this paper, we examine the use of agent-based architecture to bolster LLM-based
Question-Answering systems with additional reasoning capabilities. We examine
the automatic resolution of potential incompleteness or ambiguities in
questions by transducers implemented using LLM-based agents. We focus on
several benchmark datasets that are known to contain questions with these
deficiencies to varying degrees. We equip different LLMs (GPT-3.5-Turbo and
Llama-4-Scout) with agents that act as specialists in detecting and resolving
deficiencies of incompleteness and ambiguity. The agents are implemented as
zero-shot ReAct agents. Rather than producing an answer in a single step, the
model now decides between 3 actions a) classify b) resolve c) answer. Action a)
decides if the question is incomplete, ambiguous, or normal. Action b)
determines if any deficiencies identified can be resolved. Action c) answers
the resolved form of the question. We compare the use of LLMs with and without
the use of agents with these components. Our results show benefits of agents
with transducer 1) A shortening of the length of interactions with human 2) An
improvement in the answer quality and 3) Explainable resolution of deficiencies
in the question. On the negative side we find while it may result in additional
LLM invocations and in some cases, increased latency. But on tested datasets,
the benefits outweigh the costs except when questions already have sufficient
context. Suggesting the agent-based approach could be a useful mechanism to
harness the power of LLMs to develop more robust QA systems.

</details>


### [104] [Optimizing UAV Trajectories via a Simplified Close Enough TSP Approach](https://arxiv.org/abs/2507.03775)
*Hiba Bederina*

Main category: cs.AI

TL;DR: 文章提出了一种解决Close Enough Traveling Salesman Problem (CETSP)的方法，通过简化数学公式和利用凸集约束优化计算效率。


<details>
  <summary>Details</summary>
Motivation: 目标是简化CETSP的数学公式，提高计算效率，同时保持解的质量。

Method: 引入近似欧几里得距离的重新公式化，简化目标函数，并利用凸集约束设计。采用基于CPLEX的分段计算方法进行实证验证。

Result: 方法在真实CETSP实例中有效管理计算资源，且不降低解的质量。

Conclusion: 提出的数学公式在性能上表现良好，为CETSP的解决提供了新的思路。

Abstract: This article explores an approach to addressing the Close Enough Traveling
Salesman Problem (CETSP). The objective is to streamline the mathematical
formulation by introducing reformulations that approximate the Euclidean
distances and simplify the objective function. Additionally, the use of convex
sets in the constraint design offers computational benefits. The proposed
methodology is empirically validated on real-world CETSP instances, with the
aid of computational strategies such as a fragmented CPLEX-based approach.
Results demonstrate its effectiveness in managing computational resources
without compromising solution quality. Furthermore, the article analyzes the
behavior of the proposed mathematical formulations, providing comprehensive
insights into their performance.

</details>


### [105] [Learning Dark Souls Combat Through Pixel Input With Neuroevolution](https://arxiv.org/abs/2507.03793)
*Jim O'Connor,Gary B. Parker,Mustafa Bugti*

Main category: cs.AI

TL;DR: 论文研究了如何利用NEAT算法自动化《黑暗之魂》游戏玩法，通过视觉输入直接进化神经网络，无需显式游戏状态信息。


<details>
  <summary>Details</summary>
Motivation: 探索在缺乏API支持或明确状态表示的复杂游戏环境中，基于视觉的神经进化的可行性。

Method: 使用NEAT算法从原始像素数据进化神经网络，并开发了DSAPI框架提取游戏关键指标。

Result: 进化后的代理在击败初始Boss时达到35%的成功率。

Conclusion: 研究表明，神经进化在复杂视觉游戏场景中具有潜力。

Abstract: This paper investigates the application of Neuroevolution of Augmenting
Topologies (NEAT) to automate gameplay in Dark Souls, a notoriously challenging
action role-playing game characterized by complex combat mechanics, dynamic
environments, and high-dimensional visual inputs. Unlike traditional
reinforcement learning or game playing approaches, our method evolves neural
networks directly from raw pixel data, circumventing the need for explicit
game-state information. To facilitate this approach, we introduce the Dark
Souls API (DSAPI), a novel Python framework leveraging real-time computer
vision techniques for extracting critical game metrics, including player and
enemy health states. Using NEAT, agents evolve effective combat strategies for
defeating the Asylum Demon, the game's initial boss, without predefined
behaviors or domain-specific heuristics. Experimental results demonstrate that
evolved agents achieve up to a 35% success rate, indicating the viability of
neuroevolution in addressing complex, visually intricate gameplay scenarios.
This work represents an interesting application of vision-based neuroevolution,
highlighting its potential use in a wide range of challenging game environments
lacking direct API support or well-defined state representations.

</details>


### [106] [Generating Novelty in Open-World Multi-Agent Strategic Board Games](https://arxiv.org/abs/2507.03802)
*Mayank Kejriwal,Shilpa Thomas*

Main category: cs.AI

TL;DR: GNOME是一个实验平台，用于测试多智能体AI系统在面对未预期的新颖性时的有效性，已在NeurIPS 2020展示。


<details>
  <summary>Details</summary>
Motivation: 研究多智能体AI系统在开放世界环境中应对未预期新颖性的能力，以提升AI的鲁棒性。

Method: 通过分离AI代理开发与模拟器，GNOME平台支持测试未预期的新颖性，并利用Web GUI展示。

Result: 平台在Monopoly游戏中展示了AI的鲁棒性讨论，并用于DARPA SAIL-ON项目中评估新颖性适应能力。

Conclusion: GNOME为研究开放世界中的AI新颖性适应提供了有效工具，支持未来AI鲁棒性研究。

Abstract: We describe GNOME (Generating Novelty in Open-world Multi-agent
Environments), an experimental platform that is designed to test the
effectiveness of multi-agent AI systems when faced with \emph{novelty}. GNOME
separates the development of AI gameplaying agents with the simulator, allowing
\emph{unanticipated} novelty (in essence, novelty that is not subject to
model-selection bias). Using a Web GUI, GNOME was recently demonstrated at
NeurIPS 2020 using the game of Monopoly to foster an open discussion on AI
robustness and the nature of novelty in real-world environments. In this
article, we further detail the key elements of the demonstration, and also
provide an overview of the experimental design that is being currently used in
the DARPA Science of Artificial Intelligence and Learning for Open-World
Novelty (SAIL-ON) program to evaluate external teams developing
novelty-adaptive gameplaying agents.

</details>


### [107] [Leveraging Large Language Models for Tacit Knowledge Discovery in Organizational Contexts](https://arxiv.org/abs/2507.03811)
*Gianlucca Zuin,Saulo Mastelini,Túlio Loures,Adriano Veloso*

Main category: cs.AI

TL;DR: 提出了一种基于代理和大型语言模型的框架，用于通过员工交互迭代重建数据集描述，模拟知识传播过程，实现高知识召回率。


<details>
  <summary>Details</summary>
Motivation: 组织中的隐性知识记录面临初始信息不完整、难以识别知识个体、正式与非正式网络交织等挑战。

Method: 采用基于代理的框架和大型语言模型，模拟知识传播为SI过程，进行864次模拟实验。

Result: 代理实现94.9%的知识召回率，自我反馈与外部评价高度相关，且无需直接访问领域专家即可恢复信息。

Conclusion: 该方法能有效应对组织复杂性，捕获碎片化知识，提升隐性知识记录效率。

Abstract: Documenting tacit knowledge in organizations can be a challenging task due to
incomplete initial information, difficulty in identifying knowledgeable
individuals, the interplay of formal hierarchies and informal networks, and the
need to ask the right questions. To address this, we propose an agent-based
framework leveraging large language models (LLMs) to iteratively reconstruct
dataset descriptions through interactions with employees. Modeling knowledge
dissemination as a Susceptible-Infectious (SI) process with waning infectivity,
we conduct 864 simulations across various synthetic company structures and
different dissemination parameters. Our results show that the agent achieves
94.9% full-knowledge recall, with self-critical feedback scores strongly
correlating with external literature critic scores. We analyze how each
simulation parameter affects the knowledge retrieval process for the agent. In
particular, we find that our approach is able to recover information without
needing to access directly the only domain specialist. These findings highlight
the agent's ability to navigate organizational complexity and capture
fragmented knowledge that would otherwise remain inaccessible.

</details>


### [108] [RELRaE: LLM-Based Relationship Extraction, Labelling, Refinement, and Evaluation](https://arxiv.org/abs/2507.03829)
*George Hannah,Jacopo de Berardinis,Terry R. Payne,Valentina Tamma,Andrew Mitchell,Ellen Piercy,Ewan Johnson,Andrew Ng,Harry Rostron,Boris Konev*

Main category: cs.AI

TL;DR: RELRaE框架利用大语言模型从XML数据中提取关系标签，支持实验室自动化中的知识图谱生成。


<details>
  <summary>Details</summary>
Motivation: 实验室机器人产生的XML数据需要转换为知识图谱以实现数据互操作性。

Method: 使用RELRaE框架和大语言模型分阶段提取XML模式中的隐含关系并生成标签。

Result: 研究表明大语言模型能有效生成关系标签，支持半自动本体生成框架。

Conclusion: 大语言模型在实验室自动化和半自动本体生成中具有重要价值。

Abstract: A large volume of XML data is produced in experiments carried out by robots
in laboratories. In order to support the interoperability of data between labs,
there is a motivation to translate the XML data into a knowledge graph. A key
stage of this process is the enrichment of the XML schema to lay the foundation
of an ontology schema. To achieve this, we present the RELRaE framework, a
framework that employs large language models in different stages to extract and
accurately label the relationships implicitly present in the XML schema. We
investigate the capability of LLMs to accurately generate these labels and then
evaluate them. Our work demonstrates that LLMs can be effectively used to
support the generation of relationship labels in the context of lab automation,
and that they can play a valuable role within semi-automatic ontology
generation frameworks more generally.

</details>


### [109] [Economic Evaluation of LLMs](https://arxiv.org/abs/2507.03834)
*Michael J. Zellinger,Matt Thomson*

Main category: cs.AI

TL;DR: 论文提出了一种经济评估框架，用于量化LLM的性能权衡，以美元为单位比较不同模型的优劣。


<details>
  <summary>Details</summary>
Motivation: 解决现有方法无法比较具有不同优缺点的LLM的问题，例如低成本高错误率模型与高成本高精度模型。

Method: 提出经济评估框架，将性能权衡量化为单一数值，基于具体用例的经济约束（错误成本、延迟成本、放弃查询成本）。

Result: 在MATH基准测试中，推理模型在错误成本超过0.01美元时表现更优；单一大模型在错误成本低至0.1美元时优于级联模型。

Conclusion: 建议在自动化重要任务时优先使用最强模型，而非最小化部署成本，因为错误的经济影响远大于部署成本。

Abstract: Practitioners often navigate LLM performance trade-offs by plotting Pareto
frontiers of optimal accuracy-cost trade-offs. However, this approach offers no
way to compare between LLMs with distinct strengths and weaknesses: for
example, a cheap, error-prone model vs a pricey but accurate one. To address
this gap, we propose economic evaluation of LLMs. Our framework quantifies the
performance trade-off of an LLM as a single number based on the economic
constraints of a concrete use case, all expressed in dollars: the cost of
making a mistake, the cost of incremental latency, and the cost of abstaining
from a query. We apply our economic evaluation framework to compare the
performance of reasoning and non-reasoning models on difficult questions from
the MATH benchmark, discovering that reasoning models offer better
accuracy-cost tradeoffs as soon as the economic cost of a mistake exceeds
\$0.01. In addition, we find that single large LLMs often outperform cascades
when the cost of making a mistake is as low as \$0.1. Overall, our findings
suggest that when automating meaningful human tasks with AI models,
practitioners should typically use the most powerful available model, rather
than attempt to minimize AI deployment costs, since deployment costs are likely
dwarfed by the economic impact of AI errors.

</details>


### [110] [Participatory Evolution of Artificial Life Systems via Semantic Feedback](https://arxiv.org/abs/2507.03839)
*Shuowen Li,Kexin Wang,Minglu Fang,Danqi Huang,Ali Asadipour,Haipeng Mi,Yitong Sun*

Main category: cs.AI

TL;DR: 提出了一种语义反馈框架，通过自然语言引导人工生命系统的演化，结合提示到参数编码、CMA-ES优化器和CLIP评估，实现用户意图对视觉结果和行为规则的调控。


<details>
  <summary>Details</summary>
Motivation: 旨在通过自然语言更直观地调控人工生命系统的演化，提升语义对齐和用户参与度。

Method: 集成提示到参数编码、CMA-ES优化器和CLIP评估，支持交互式生态系统模拟，包括提示优化、多智能体交互和涌现规则合成。

Result: 用户研究表明，相比手动调整，系统在语义对齐方面表现更优，展示了其在参与式生成设计和开放式演化中的潜力。

Conclusion: 该框架为人工生命系统提供了一种高效的语义调控方法，适用于生成设计和开放式演化研究。

Abstract: We present a semantic feedback framework that enables natural language to
guide the evolution of artificial life systems. Integrating a
prompt-to-parameter encoder, a CMA-ES optimizer, and CLIP-based evaluation, the
system allows user intent to modulate both visual outcomes and underlying
behavioral rules. Implemented in an interactive ecosystem simulation, the
framework supports prompt refinement, multi-agent interaction, and emergent
rule synthesis. User studies show improved semantic alignment over manual
tuning and demonstrate the system's potential as a platform for participatory
generative design and open-ended evolution.

</details>


### [111] [From Query to Explanation: Uni-RAG for Multi-Modal Retrieval-Augmented Learning in STEM](https://arxiv.org/abs/2507.03868)
*Xinyi Wu,Yanhao Jia,Luwei Xiao,Shuai Zhao,Fengkuang Chiang,Erik Cambria*

Main category: cs.AI

TL;DR: 论文提出了一种名为Uni-Retrieval的多模态检索模块和Uni-RAG的检索增强生成框架，旨在解决教育场景中查询多样性和模糊性问题，并在实验中表现优于基线系统。


<details>
  <summary>Details</summary>
Motivation: 现有检索系统主要关注自然文本-图像匹配，无法应对教育场景中的多样性和模糊性，因此需要开发更高效的解决方案。

Method: 开发了Uni-Retrieval模块，通过提取查询风格原型并与动态更新的Prompt Bank匹配；结合指令调优语言模型形成Uni-RAG框架，实现检索与生成的结合。

Result: 在SER等多模态基准测试中，Uni-RAG在检索准确性和生成质量上优于基线系统，同时保持低计算成本。

Conclusion: Uni-RAG为智能教育系统提供了可扩展的解决方案，支持个性化、可解释且高效的学习辅助。

Abstract: In AI-facilitated teaching, leveraging various query styles to interpret
abstract educational content is crucial for delivering effective and accessible
learning experiences. However, existing retrieval systems predominantly focus
on natural text-image matching and lack the capacity to address the diversity
and ambiguity inherent in real-world educational scenarios. To address this
limitation, we develop a lightweight and efficient multi-modal retrieval
module, named Uni-Retrieval, which extracts query-style prototypes and
dynamically matches them with tokens from a continually updated Prompt Bank.
This Prompt Bank encodes and stores domain-specific knowledge by leveraging a
Mixture-of-Expert Low-Rank Adaptation (MoE-LoRA) module and can be adapted to
enhance Uni-Retrieval's capability to accommodate unseen query types at test
time. To enable natural language educational content generation, we integrate
the original Uni-Retrieval with a compact instruction-tuned language model,
forming a complete retrieval-augmented generation pipeline named Uni-RAG. Given
a style-conditioned query, Uni-RAG first retrieves relevant educational
materials and then generates human-readable explanations, feedback, or
instructional content aligned with the learning objective. Experimental results
on SER and other multi-modal benchmarks show that Uni-RAG outperforms baseline
retrieval and RAG systems in both retrieval accuracy and generation quality,
while maintaining low computational cost. Our framework provides a scalable,
pedagogically grounded solution for intelligent educational systems, bridging
retrieval and generation to support personalized, explainable, and efficient
learning assistance across diverse STEM scenarios.

</details>


### [112] [Uncovering Systemic and Environment Errors in Autonomous Systems Using Differential Testing](https://arxiv.org/abs/2507.03870)
*Rahil P Mehta,Yashwanthi Anand,Manish Motwani,Sandhya Saisubramanian*

Main category: cs.AI

TL;DR: AIProbe是一种黑盒测试技术，通过差异测试区分自主代理行为错误是源于代理缺陷还是环境不可行性。


<details>
  <summary>Details</summary>
Motivation: 随着自主代理及其环境复杂性增加，识别行为错误来源变得困难但至关重要。

Method: AIProbe生成多样化环境配置和任务，使用拉丁超立方采样，并通过独立搜索规划器解决任务，对比代理性能以识别错误来源。

Result: AIProbe在多领域评估中显著优于现有技术，能检测更多总错误和独特错误。

Conclusion: AIProbe有助于自主代理的可靠部署，能有效区分代理缺陷和环境不可行性。

Abstract: When an autonomous agent behaves undesirably, including failure to complete a
task, it can be difficult to determine whether the behavior is due to a
systemic agent error, such as flaws in the model or policy, or an environment
error, where a task is inherently infeasible under a given environment
configuration, even for an ideal agent. As agents and their environments grow
more complex, identifying the error source becomes increasingly difficult but
critical for reliable deployment. We introduce AIProbe, a novel black-box
testing technique that applies differential testing to attribute undesirable
agent behaviors either to agent deficiencies, such as modeling or training
flaws, or due to environmental infeasibility. AIProbe first generates diverse
environmental configurations and tasks for testing the agent, by modifying
configurable parameters using Latin Hypercube sampling. It then solves each
generated task using a search-based planner, independent of the agent. By
comparing the agent's performance to the planner's solution, AIProbe identifies
whether failures are due to errors in the agent's model or policy, or due to
unsolvable task conditions. Our evaluation across multiple domains shows that
AIProbe significantly outperforms state-of-the-art techniques in detecting both
total and unique errors, thereby contributing to a reliable deployment of
autonomous agents.

</details>


### [113] [LLMs model how humans induce logically structured rules](https://arxiv.org/abs/2507.03876)
*Alyssa Loo,Ellie Pavlick,Roman Feiman*

Main category: cs.AI

TL;DR: 论文探讨神经网络（尤其是大型语言模型LLMs）是否能作为认知科学的计算模型，通过实验比较LLMs与贝叶斯概率思维语言模型（pLoT）在逻辑规则任务中的表现，发现LLMs表现相当甚至更好，并提出LLMs可能代表新的理论解释。


<details>
  <summary>Details</summary>
Motivation: 研究神经网络（特别是LLMs）是否能解释人类认知的原始表征和计算规则，挑战传统贝叶斯模型（pLoT）的优越性。

Method: 通过四个实验，测试多种LLMs在逻辑规则任务中的表现，并与pLoT模型进行对比。

Result: LLMs在拟合人类行为上表现与pLoT相当或更好，且预测的规则性质不同，表明LLMs并非pLoT的简单实现。

Conclusion: LLMs可能提供新的理论框架来解释人类逻辑概念，未来认知科学研究应关注这一方向。

Abstract: A central goal of cognitive science is to provide a computationally explicit
account of both the structure of the mind and its development: what are the
primitive representational building blocks of cognition, what are the rules via
which those primitives combine, and where do these primitives and rules come
from in the first place? A long-standing debate concerns the adequacy of
artificial neural networks as computational models that can answer these
questions, in particular in domains related to abstract cognitive function,
such as language and logic. This paper argues that recent advances in neural
networks -- specifically, the advent of large language models (LLMs) --
represent an important shift in this debate. We test a variety of LLMs on an
existing experimental paradigm used for studying the induction of rules
formulated over logical concepts. Across four experiments, we find converging
empirical evidence that LLMs provide at least as good a fit to human behavior
as models that implement a Bayesian probablistic language of thought (pLoT),
which have been the best computational models of human behavior on the same
task. Moreover, we show that the LLMs make qualitatively different predictions
about the nature of the rules that are inferred and deployed in order to
complete the task, indicating that the LLM is unlikely to be a mere
implementation of the pLoT solution. Based on these results, we argue that LLMs
may instantiate a novel theoretical account of the primitive representations
and computations necessary to explain human logical concepts, with which future
work in cognitive science should engage.

</details>


### [114] [Agent Exchange: Shaping the Future of AI Agent Economics](https://arxiv.org/abs/2507.03904)
*Yingxuan Yang,Ying Wen,Jun Wang,Weinan Zhang*

Main category: cs.AI

TL;DR: 论文提出Agent Exchange (AEX)平台，支持AI代理在自主经济中的协调与交易，受在线广告实时竞价系统启发，设计了包含用户侧平台、代理侧平台、代理中心和数据管理平台的生态系统。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）的发展使AI代理从被动工具转变为自主经济参与者，需要一种支持代理间价值交换和协调的平台。

Method: 提出AEX平台，基于实时竞价系统设计，包含用户侧平台（USP）、代理侧平台（ASP）、代理中心和数据管理平台（DMP）四个组件。

Result: AEX为AI代理经济提供了优化的基础设施，支持代理间的任务分配、能力展示和知识共享。

Conclusion: AEX为未来AI生态系统中的代理经济奠定了基础，展示了代理自主参与经济的潜力。

Abstract: The rise of Large Language Models (LLMs) has transformed AI agents from
passive computational tools into autonomous economic actors. This shift marks
the emergence of the agent-centric economy, in which agents take on active
economic roles-exchanging value, making strategic decisions, and coordinating
actions with minimal human oversight. To realize this vision, we propose Agent
Exchange (AEX), a specialized auction platform designed to support the dynamics
of the AI agent marketplace. AEX offers an optimized infrastructure for agent
coordination and economic participation. Inspired by Real-Time Bidding (RTB)
systems in online advertising, AEX serves as the central auction engine,
facilitating interactions among four ecosystem components: the User-Side
Platform (USP), which translates human goals into agent-executable tasks; the
Agent-Side Platform (ASP), responsible for capability representation,
performance tracking, and optimization; Agent Hubs, which coordinate agent
teams and participate in AEX-hosted auctions; and the Data Management Platform
(DMP), ensuring secure knowledge sharing and fair value attribution. We outline
the design principles and system architecture of AEX, laying the groundwork for
agent-based economic infrastructure in future AI ecosystems.

</details>


### [115] [Animation Needs Attention: A Holistic Approach to Slides Animation Comprehension with Visual-Language Models](https://arxiv.org/abs/2507.03916)
*Yifan Jiang,Yibo Xue,Yukun Kang,Pin Zheng,Jian Peng,Feiran Wu,Changliang Xu*

Main category: cs.AI

TL;DR: 论文提出了首个公开的幻灯片动画数据集，并利用LoRA微调Qwen-2.5-VL-7B模型，在多项指标上超越GPT-4.1和Gemini-2.5-Pro，为动态幻灯片生成研究提供了基准。


<details>
  <summary>Details</summary>
Motivation: 现有AI驱动的幻灯片生成工具缺乏原生动画支持，且视觉语言模型在动画任务上表现不佳，主要由于公开数据集缺失和时间推理能力有限。

Method: 发布包含12,000组自然语言描述、动画JSON文件和渲染视频的数据集，并利用LoRA微调Qwen-2.5-VL-7B模型。

Result: LoRA模型在BLEU-4、ROUGE-L、SPICE和CODA指标上显著提升，尤其在CODA细节评估中表现突出。

Conclusion: 数据集、LoRA增强模型和CODA指标为未来基于VLM的动态幻灯片生成研究提供了坚实基础。

Abstract: Slide animations, such as fade-ins, fly-ins, and wipes, are critical for
audience engagement, efficient information delivery, and vivid visual
expression. However, most AI-driven slide-generation tools still lack native
animation support, and existing vision-language models (VLMs) struggle with
animation tasks due to the absence of public datasets and limited
temporal-reasoning capabilities. To address this gap, we release the first
public dataset for slide-animation modeling: 12,000 triplets of
natural-language descriptions, animation JSON files, and rendered videos,
collectively covering every built-in PowerPoint effect. Using this resource, we
fine-tune Qwen-2.5-VL-7B with Low-Rank Adaptation (LoRA) and achieve consistent
improvements over GPT-4.1 and Gemini-2.5-Pro in BLEU-4, ROUGE-L, SPICE, and our
Coverage-Order-Detail Assessment (CODA) metric, which evaluates action
coverage, temporal order, and detail fidelity. On a manually curated test set
of slides, the LoRA model increases BLEU-4 by around 60%, ROUGE-L by 30%, and
shows significant improvements in CODA-detail. This demonstrates that low-rank
adaptation enables reliable temporal reasoning and generalization beyond
synthetic data. Overall, our dataset, LoRA-enhanced model, and CODA metric
provide a rigorous benchmark and foundation for future research on VLM-based
dynamic slide generation.

</details>


### [116] [CortexDebate: Debating Sparsely and Equally for Multi-Agent Debate](https://arxiv.org/abs/2507.03928)
*Yiliu Sun,Zicheng Zhao,Sheng Wan,Chen Gong*

Main category: cs.AI

TL;DR: CortexDebate是一种新的多智能体辩论方法，通过稀疏辩论图和McKinsey信任公式优化，解决了现有方法中上下文过长和过度自信的问题。


<details>
  <summary>Details</summary>
Motivation: 单一大语言模型（LLM）存在幻觉和推理能力不足的问题，而现有的多智能体辩论方法因上下文过长和过度自信导致效果不佳。

Method: 提出CortexDebate方法，构建稀疏辩论图，每个LLM智能体仅与对其有帮助的智能体辩论，并通过McKinsey信任公式优化图结构。

Result: 在四个任务类型的八个数据集上验证了CortexDebate的有效性。

Conclusion: CortexDebate通过稀疏辩论图和信任优化，显著提升了多智能体辩论的效果。

Abstract: Nowadays, single Large Language Model (LLM) struggles with critical issues
such as hallucination and inadequate reasoning abilities. To mitigate these
issues, Multi-Agent Debate (MAD) has emerged as an effective strategy, where
LLM agents engage in in-depth debates with others on tasks. However, existing
MAD methods face two major issues: (a) too lengthy input contexts, which causes
LLM agents to get lost in plenty of input information and experiences
performance drop; and (b) the overconfidence dilemma, where self-assured LLM
agents dominate the debate, leading to low debating effectiveness. To address
these limitations, we propose a novel MAD method called "CortexDebate".
Inspired by the human brain's tendency to establish a sparse and dynamically
optimized network among cortical areas governed by white matter, CortexDebate
constructs a sparse debating graph among LLM agents, where each LLM agent only
debates with the ones that are helpful to it. To optimize the graph, we propose
a module named McKinsey-based Debate Matter (MDM), which acts as an artificial
analog to white matter. By integrating the McKinsey Trust Formula, a
well-established measure of trustworthiness from sociology, MDM enables
credible evaluations that guide graph optimization. The effectiveness of our
CortexDebate has been well demonstrated by extensive experimental results
across eight datasets from four task types.

</details>


### [117] [An ASP-Based Framework for MUSes](https://arxiv.org/abs/2507.03929)
*Mohimenul Kabir,Kuldeep S Meel*

Main category: cs.AI

TL;DR: 论文提出了一种基于答案集编程（ASP）的框架MUS-ASP，用于在线枚举最小不可满足子集（MUS），并展示了其在MUS枚举和计数任务中的高效性。


<details>
  <summary>Details</summary>
Motivation: 理解不可满足公式的核心原因在多个应用中至关重要，而最小不可满足子集（MUS）是捕捉这一原因的有效方式。当前研究主要集中在枚举MUS和计数MUS总数两方面。

Method: 通过将MUS枚举问题转化为答案集求解问题，利用ASP在知识表示和组合问题求解中的优势，设计了MUS-ASP框架。

Result: 实验评估表明，MUS-ASP在MUS枚举和计数任务中表现高效，尤其是在与混合求解器结合时。

Conclusion: MUS-ASP框架通过ASP的高效计算能力，显著提升了MUS枚举和计数的性能。

Abstract: Given an unsatisfiable formula, understanding the core reason for
unsatisfiability is crucial in several applications. One effective way to
capture this is through the minimal unsatisfiable subset (MUS), the
subset-minimal set of clauses that remains unsatisfiable. Current research
broadly focuses on two directions: (i) enumerating as many MUSes as possible
within a given time limit, and (ii) counting the total number of MUSes for a
given unsatisfiable formula.
  In this paper, we introduce an answer set programming-based framework, named
MUS-ASP, designed for online enumeration of MUSes. ASP is a powerful tool for
its strengths in knowledge representation and is particularly suitable for
specifying complex combinatorial problems. By translating MUS enumeration into
answer set solving, MUS-ASP leverages the computational efficiency of
state-of-the-art ASP systems. Our extensive experimental evaluation
demonstrates the effectiveness of MUS-ASP and highlights the acceleration in
both MUS enumeration and counting tasks, particularly when integrated within
hybrid solvers, including the framework proposed in this paper.

</details>


### [118] [Toward Better Generalisation in Uncertainty Estimators: Leveraging Data-Agnostic Features](https://arxiv.org/abs/2507.03998)
*Thuy An Ha,Bao Quoc Vo*

Main category: cs.AI

TL;DR: 论文探讨了如何通过结合数据无关特征和隐藏状态特征来提升大型语言模型（LLM）输出的事实准确性评估的泛化能力，但实验结果并不一致。


<details>
  <summary>Details</summary>
Motivation: LLM常生成高自信但事实错误的回答，需准确评估其输出的正确性。现有基于隐藏状态的量化方法泛化能力有限。

Method: 结合数据无关特征与隐藏状态特征，并筛选最有信息的隐藏状态特征，以提升评估的泛化能力。

Result: 数据无关特征在多数情况下提升泛化性能，但某些情况下会降低性能；筛选隐藏状态特征后，数据无关特征的贡献不一致。

Conclusion: 实验结果未完全支持假设，可能因探针低估数据无关特征的权重，需进一步研究。

Abstract: Large Language Models (LLMs) often generate responses that are factually
incorrect yet expressed with high confidence, which can pose serious risks for
end users. To address this, it is essential for LLMs not only to produce
answers but also to provide accurate estimates of their correctness.
Uncertainty quantification methods have been introduced to assess the quality
of LLM outputs, with factual accuracy being a key aspect of that quality. Among
these methods, those that leverage hidden states to train probes have shown
particular promise, as these internal representations encode information
relevant to the factuality of responses, making this approach the focus of this
paper. However, the probe trained on the hidden states of one dataset often
struggles to generalise to another dataset of a different task or domain. To
address this limitation, we explore combining data-agnostic features with
hidden-state features and assess whether this hybrid feature set enhances
out-of-domain performance. We further examine whether selecting only the most
informative hidden-state features, thereby discarding task-specific noise,
enables the data-agnostic features to contribute more effectively. The
experiment results indicate that although introducing data-agnostic features
generally enhances generalisation performance in most cases, in certain
scenarios their inclusion degrades performance. A similar pattern emerges when
retaining only the most important hidden-state features - adding data-agnostic
features does not consistently further enhance performance compared to using
the full set of hidden-state features. A closer analysis reveals that, in some
specific cases, the trained probe underweights the data-agnostic features
relative to the hidden-state features, which we believe is the main reason why
the results are inconclusive.

</details>


### [119] [Lyria: A General LLM-Driven Genetic Algorithm Framework for Problem Solving](https://arxiv.org/abs/2507.04034)
*Weizhi Tang,Kwabena Nuamah,Vaishak Belle*

Main category: cs.AI

TL;DR: Lyria框架结合LLMs的语义理解能力和遗传算法的全局搜索能力，解决了多目标优化等复杂问题，并通过实验验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: LLMs在多目标优化等复杂问题上表现不足，需要结合其他方法提升能力。

Method: 提出Lyria框架，结合LLMs和遗传算法，包含7个关键组件，并通过实验验证。

Result: 实验表明Lyria在多种问题和LLMs上有效，并通过消融实验分析了性能影响因素。

Conclusion: Lyria成功结合LLMs和遗传算法，为复杂问题提供了有效解决方案。

Abstract: While Large Language Models (LLMs) have demonstrated impressive abilities
across various domains, they still struggle with complex problems characterized
by multi-objective optimization, precise constraint satisfaction, immense
solution spaces, etc. To address the limitation, drawing on the superior
semantic understanding ability of LLMs and also the outstanding global search
and optimization capability of genetic algorithms, we propose to capitalize on
their respective strengths and introduce Lyria, a general LLM-driven genetic
algorithm framework, comprising 7 essential components. Through conducting
extensive experiments with 4 LLMs across 3 types of problems, we demonstrated
the efficacy of Lyria. Additionally, with 7 additional ablation experiments, we
further systematically analyzed and elucidated the factors that affect its
performance.

</details>


### [120] [Ready Jurist One: Benchmarking Language Agents for Legal Intelligence in Dynamic Environments](https://arxiv.org/abs/2507.04037)
*Zheng Jia,Shengbin Yue,Wei Chen,Siyuan Wang,Yidong Liu,Yun Song,Zhongyu Wei*

Main category: cs.AI

TL;DR: 论文介绍了J1-ENVS和J1-EVAL，用于动态评估LLM在法律实践中的表现，发现现有模型在动态环境中表现不足。


<details>
  <summary>Details</summary>
Motivation: 解决静态基准与动态法律实践之间的差距，推动法律智能的发展。

Method: 开发了J1-ENVS（动态法律环境）和J1-EVAL（评估框架），并在17个LLM代理上进行实验。

Result: 多数模型在法律知识上表现良好，但在动态环境中的程序执行上表现不佳，GPT-4o总体表现低于60%。

Conclusion: 动态法律智能仍面临挑战，研究结果为未来方向提供了参考。

Abstract: The gap between static benchmarks and the dynamic nature of real-world legal
practice poses a key barrier to advancing legal intelligence. To this end, we
introduce J1-ENVS, the first interactive and dynamic legal environment tailored
for LLM-based agents. Guided by legal experts, it comprises six representative
scenarios from Chinese legal practices across three levels of environmental
complexity. We further introduce J1-EVAL, a fine-grained evaluation framework,
designed to assess both task performance and procedural compliance across
varying levels of legal proficiency. Extensive experiments on 17 LLM agents
reveal that, while many models demonstrate solid legal knowledge, they struggle
with procedural execution in dynamic settings. Even the SOTA model, GPT-4o,
falls short of 60% overall performance. These findings highlight persistent
challenges in achieving dynamic legal intelligence and offer valuable insights
to guide future research.

</details>


### [121] [HAWK: A Hierarchical Workflow Framework for Multi-Agent Collaboration](https://arxiv.org/abs/2507.04067)
*Yuyang Cheng,Yumiao Xu,Chaojia Yu,Yong Zhao*

Main category: cs.AI

TL;DR: HAWK是一个模块化框架，解决多智能体系统中的互操作性、任务调度和资源共享问题，通过分层设计和标准化接口实现高效协作。


<details>
  <summary>Details</summary>
Motivation: 当前多智能体系统在跨平台互操作性、动态任务调度和资源共享方面存在挑战，缺乏标准化接口和灵活协作框架。

Method: HAWK采用五层架构（用户、工作流、操作、智能体、资源）和十六个标准化接口，包括自适应调度和统一资源抽象。

Result: 通过CreAgentive原型验证，HAWK提高了吞吐量、降低了调用复杂性，并增强了系统可控性。

Conclusion: HAWK展示了在多领域的应用潜力，未来研究方向包括幻觉缓解和实时性能优化。

Abstract: Contemporary multi-agent systems encounter persistent challenges in
cross-platform interoperability, dynamic task scheduling, and efficient
resource sharing. Agents with heterogeneous implementations often lack
standardized interfaces; collaboration frameworks remain brittle and hard to
extend; scheduling policies are static; and inter-agent state synchronization
is insufficient. We propose Hierarchical Agent Workflow (HAWK), a modular
framework comprising five layers-User, Workflow, Operator, Agent, and
Resource-and supported by sixteen standardized interfaces. HAWK delivers an
end-to-end pipeline covering task parsing, workflow orchestration, intelligent
scheduling, resource invocation, and data synchronization. At its core lies an
adaptive scheduling and optimization module in the Workflow Layer, which
harnesses real-time feedback and dynamic strategy adjustment to maximize
utilization. The Resource Layer provides a unified abstraction over
heterogeneous data sources, large models, physical devices, and third-party
services&tools, simplifying cross-domain information retrieval. We demonstrate
HAWK's scalability and effectiveness via CreAgentive, a multi-agent
novel-generation prototype, which achieves marked gains in throughput, lowers
invocation complexity, and improves system controllability. We also show how
hybrid deployments of large language models integrate seamlessly within HAWK,
highlighting its flexibility. Finally, we outline future research
avenues-hallucination mitigation, real-time performance tuning, and enhanced
cross-domain adaptability-and survey prospective applications in healthcare,
government, finance, and education.

</details>


### [122] [How to Train Your LLM Web Agent: A Statistical Diagnosis](https://arxiv.org/abs/2507.04103)
*Dheeraj Vattikonda,Santhoshi Ravichandran,Emiliano Penaloza,Hadi Nekoei,Megh Thakkar,Thibault Le Sellier de Chezelles,Nicolas Gontier,Miguel Muñoz-Mármol,Sahar Omidi Shayegan,Stefania Raimondo,Xue Liu,Alexandre Drouin,Laurent Charlin,Alexandre Piché,Alexandre Lacoste,Massimo Caccia*

Main category: cs.AI

TL;DR: 论文提出了一种基于LLM的网页代理训练方法，通过两阶段管道（SFT和强化学习）优化计算资源分配，显著提升性能并降低计算成本。


<details>
  <summary>Details</summary>
Motivation: 开源LLM网页代理与闭源系统差距扩大，主要因单步任务局限性和高计算成本。

Method: 两阶段训练：先通过SFT模仿教师模型，再进行强化学习；通过采样和引导法优化超参数。

Result: SFT结合强化学习优于单独方法，计算成本降低55%，性能接近闭源模型。

Conclusion: 该方法有效优化计算资源，缩小开源与闭源差距，为未来研究提供实用指导。

Abstract: LLM-based web agents have recently made significant progress, but much of it
has occurred in closed-source systems, widening the gap with open-source
alternatives. Progress has been held back by two key challenges: first, a
narrow focus on single-step tasks that overlooks the complexity of multi-step
web interactions; and second, the high compute costs required to post-train
LLM-based web agents. To address this, we present the first statistically
grounded study on compute allocation for LLM web-agent post-training. Our
approach uses a two-stage pipeline, training a Llama 3.1 8B student to imitate
a Llama 3.3 70B teacher via supervised fine-tuning (SFT), followed by on-policy
reinforcement learning. We find this process highly sensitive to hyperparameter
choices, making exhaustive sweeps impractical. To spare others from expensive
trial-and-error, we sample 1,370 configurations and use bootstrapping to
estimate effective hyperparameters. Our results show that combining SFT with
on-policy RL consistently outperforms either approach alone on both WorkArena
and MiniWob++. Further, this strategy requires only 55% of the compute to match
the peak performance of pure SFT on MiniWob++, effectively pushing the
compute-performance Pareto frontier, and is the only strategy that can close
the gap with closed-source models.

</details>


### [123] [Enhancing Robustness of LLM-Driven Multi-Agent Systems through Randomized Smoothing](https://arxiv.org/abs/2507.04105)
*Jinwei Hu,Yi Dong,Zhengtao Ding,Xiaowei Huang*

Main category: cs.AI

TL;DR: 提出了一种用于增强大型语言模型（LLM）赋能的智能体系统（MAS）安全性的防御框架，采用随机平滑技术提供概率保证。


<details>
  <summary>Details</summary>
Motivation: 在航空航天等安全关键领域，确保LLM赋能的MAS在对抗性影响下的安全性至关重要。

Method: 应用随机平滑技术，采用两阶段自适应采样机制，平衡鲁棒性和计算效率。

Result: 仿真结果表明，该方法能有效阻止对抗行为和幻觉传播，同时保持共识性能。

Conclusion: 为LLM赋能的MAS在现实高风险环境中的安全部署提供了实用且可扩展的解决方案。

Abstract: This paper presents a defense framework for enhancing the safety of large
language model (LLM) empowered multi-agent systems (MAS) in safety-critical
domains such as aerospace. We apply randomized smoothing, a statistical
robustness certification technique, to the MAS consensus context, enabling
probabilistic guarantees on agent decisions under adversarial influence. Unlike
traditional verification methods, our approach operates in black-box settings
and employs a two-stage adaptive sampling mechanism to balance robustness and
computational efficiency. Simulation results demonstrate that our method
effectively prevents the propagation of adversarial behaviors and
hallucinations while maintaining consensus performance. This work provides a
practical and scalable path toward safe deployment of LLM-based MAS in
real-world, high-stakes environments.

</details>


### [124] [A Technical Survey of Reinforcement Learning Techniques for Large Language Models](https://arxiv.org/abs/2507.04136)
*Saksham Sahai Srivastava,Vaneet Aggarwal*

Main category: cs.AI

TL;DR: 该论文综述了强化学习（RL）在大型语言模型（LLMs）中的应用，涵盖算法、技术及挑战，并展望未来方向。


<details>
  <summary>Details</summary>
Motivation: 探讨RL如何解决LLMs在指令遵循、伦理对齐和推理能力方面的关键问题。

Method: 分析了PPO、Q-Learning、Actor-Critic等方法，以及RLHF、RLAIF、DPO、GRPO等专门技术。

Result: RLHF在模型对齐中占主导，RLVR显著提升逐步推理能力，但仍存在奖励破解、计算成本等挑战。

Conclusion: 未来需创新混合RL算法、验证器引导训练等方向，以平衡能力提升与安全性和可扩展性。

Abstract: Reinforcement Learning (RL) has emerged as a transformative approach for
aligning and enhancing Large Language Models (LLMs), addressing critical
challenges in instruction following, ethical alignment, and reasoning
capabilities. This survey offers a comprehensive foundation on the integration
of RL with language models, highlighting prominent algorithms such as Proximal
Policy Optimization (PPO), Q-Learning, and Actor-Critic methods. Additionally,
it provides an extensive technical overview of RL techniques specifically
tailored for LLMs, including foundational methods like Reinforcement Learning
from Human Feedback (RLHF) and AI Feedback (RLAIF), as well as advanced
strategies such as Direct Preference Optimization (DPO) and Group Relative
Policy Optimization (GRPO). We systematically analyze their applications across
domains, i.e., from code generation to tool-augmented reasoning. We also
present a comparative taxonomy based on reward modeling, feedback mechanisms,
and optimization strategies. Our evaluation highlights key trends. RLHF remains
dominant for alignment, and outcome-based RL such as RLVR significantly
improves stepwise reasoning. However, persistent challenges such as reward
hacking, computational costs, and scalable feedback collection underscore the
need for continued innovation. We further discuss emerging directions,
including hybrid RL algorithms, verifier-guided training, and multi-objective
alignment frameworks. This survey serves as a roadmap for researchers advancing
RL-driven LLM development, balancing capability enhancement with safety and
scalability.

</details>


### [125] [Mpemba Effect in Large-Language Model Training Dynamics: A Minimal Analysis of the Valley-River model](https://arxiv.org/abs/2507.04206)
*Sibei Liu,Zhijian Hu*

Main category: cs.AI

TL;DR: 论文通过热力学类比（Mpemba效应）解释了LLM训练中学习率调度（WSD策略）的机制，提出高平台学习率可加速损失下降，并推导了最优平台学习率（强Mpemba点）的存在条件。


<details>
  <summary>Details</summary>
Motivation: 探索LLM训练中学习率调度策略的机理，减少对启发式选择的依赖。

Method: 通过热力学类比分析“谷-河”损失景观，推导最优平台学习率及其存在条件。

Result: 发现强Mpemba点可加速收敛，并提供了保留其优势的衰减动态估计。

Conclusion: 为基于平台的学习率调度提供了理论依据，指导LLM学习率调优。

Abstract: Learning rate (LR) schedules in large language model (LLM) training often
follow empirical templates: warm-up, constant plateau/stable phase, and decay
(WSD). However, the mechanistic explanation for this strategy remains
underexplored, and the choice of plateau height and decay schedule is largely
heuristic. In this paper, we connect training dynamics to a thermodynamic
analogy via the Mpemba effect - a phenomenon in which a hotter system cools
faster than a colder one when quenched into the same bath. We analyze a class
of "valley-river" loss landscapes, where sharp (valley) directions equilibrate
quickly, while flatter (river) directions govern global descent. The Mpemba
effect provides an explanation for the necessity of the warm-up phase and
motivates a high plateau - rather than a low one - for accelerating loss
decrease during decay. We show that for certain loss landscapes, there exists
an optimal plateau learning rate - the "strong Mpemba point" - at which the
slowest mode vanishes, resulting in faster convergence during the decay phase.
We derive analytical conditions for its existence and estimate decay dynamics
required to preserve the Mpemba advantage. Our minimal model and analysis offer
a principled justification for plateau-based schedulers and provide guidance
for tuning LR in LLMs with minimal hyperparameter sweep.

</details>


### [126] [Clustering via Self-Supervised Diffusion](https://arxiv.org/abs/2507.04283)
*Roy Uziel,Irit Chelly,Oren Freifeld,Ari Pakman*

Main category: cs.AI

TL;DR: CLUDI是一种自监督框架，结合扩散模型和预训练Vision Transformer特征，实现鲁棒且准确的聚类。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在生成任务中表现优异，但尚未应用于聚类任务。

Method: CLUDI通过师生范式训练，教师使用扩散采样生成多样聚类分配，学生将其优化为稳定预测。

Result: CLUDI在挑战性数据集上表现优异，达到无监督分类的最先进水平。

Conclusion: CLUDI为高维数据聚类提供了新方法，具有鲁棒性和适应性。

Abstract: Diffusion models, widely recognized for their success in generative tasks,
have not yet been applied to clustering. We introduce Clustering via Diffusion
(CLUDI), a self-supervised framework that combines the generative power of
diffusion models with pre-trained Vision Transformer features to achieve robust
and accurate clustering. CLUDI is trained via a teacher-student paradigm: the
teacher uses stochastic diffusion-based sampling to produce diverse cluster
assignments, which the student refines into stable predictions. This
stochasticity acts as a novel data augmentation strategy, enabling CLUDI to
uncover intricate structures in high-dimensional data. Extensive evaluations on
challenging datasets demonstrate that CLUDI achieves state-of-the-art
performance in unsupervised classification, setting new benchmarks in
clustering robustness and adaptability to complex data distributions.

</details>


### [127] [Answer Set Programming Modulo Theories and Reasoning about Continuous Changes](https://arxiv.org/abs/2507.04299)
*Joohyung Lee,Yunsong Meng*

Main category: cs.AI

TL;DR: ASPMT是ASP与SMT紧密结合的新框架，类似于一阶逻辑与SMT的关系，通过固定背景理论的解释实现。它可将“紧致”ASPMT程序转化为SMT实例，并用于增强动作语言C+以处理连续和离散变化。


<details>
  <summary>Details</summary>
Motivation: 将ASP与SMT结合，以支持更复杂的逻辑推理，特别是处理连续变化和离散变化的动作语言。

Method: 基于功能稳定模型语义，固定背景理论的解释，将ASPMT程序转化为SMT实例。

Result: 成功将ASPMT应用于动作语言C+，使其能处理连续和离散变化，并展示了SMT求解器在该语言中的应用。

Conclusion: ASPMT为逻辑编程和理论求解提供了强大的结合框架，尤其在处理复杂动态系统时表现出色。

Abstract: Answer Set Programming Modulo Theories (ASPMT) is a new framework of tight
integration of answer set programming (ASP) and satisfiability modulo theories
(SMT). Similar to the relationship between first-order logic and SMT, it is
based on a recent proposal of the functional stable model semantics by fixing
interpretations of background theories. Analogously to a known relationship
between ASP and SAT, ``tight'' ASPMT programs can be translated into SMT
instances. We demonstrate the usefulness of ASPMT by enhancing action language
C+ to handle continuous changes as well as discrete changes. We reformulate the
semantics of C+ in terms ofASPMT, and show that SMT solvers can be used to
compute the language. We also show how the language can represent cumulative
effects on continuous resources.

</details>


### [128] [Voltage Mode Winner-Take-All Circuit for Neuromorphic Systems](https://arxiv.org/abs/2507.04338)
*Abdullah M. Zyarah,Dhireesha Kudithipudi*

Main category: cs.AI

TL;DR: 提出了一种可配置的winner-take-all电路，支持k-winner和滞后特性，功耗低且速度快。


<details>
  <summary>Details</summary>
Motivation: 神经形态计算需要低功耗且高效的学习单元，winner-take-all电路是关键组件之一。

Method: 设计了一种可配置的winner-take-all电路，并在IBM 65 nm工艺节点上进行了仿真。

Result: 电路功耗为34.9 μW，延迟为10.4 ns，能够处理1000个输入，适用于空间滤波和分类任务。

Conclusion: 该电路在神经形态计算中具有实用价值，适合低功耗和高性能的应用场景。

Abstract: Recent advances in neuromorphic computing demonstrate on-device learning
capabilities with low power consumption. One of the key learning units in these
systems is the winner-take-all circuit. In this research, we propose a
winner-take-all circuit that can be configured to achieve k-winner and
hysteresis properties, simulated in IBM 65 nm node. The circuit dissipated 34.9
$\mu$W of power with a latency of 10.4 ns, while processing 1000 inputs. The
utility of the circuit is demonstrated for spatial filtering and
classification.

</details>


### [129] [SmartThinker: Learning to Compress and Preserve Reasoning by Step-Level Length Control](https://arxiv.org/abs/2507.04348)
*Xingyang He,Xiao Ling,Jie Liu*

Main category: cs.AI

TL;DR: SmartThinker是一个两阶段学习框架，通过细粒度控制推理步骤的长度，减少冗余推理，同时保持或提升性能。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型（LRMs）在推理时存在冗余和低效问题，全局长度惩罚方法会导致关键推理步骤被过度压缩，而简单步骤保留不必要细节。

Method: SmartThinker采用两阶段方法：第一阶段通过拒绝采样和监督微调（SFT）适应短形式推理模式；第二阶段通过Step-Level Length Control Policy Optimization（SCPO）优化模型输出分布，包括在线重要性估计器、步骤级长度控制奖励函数、步骤级广义优势估计（S-GAE）和难度自适应裁剪策略。

Result: 实验结果表明，SmartThinker在多个推理基准和不同骨干模型上显著减少冗余推理，性能与现有方法相当或更优。

Conclusion: SmartThinker通过细粒度控制推理步骤长度，有效解决了冗余推理问题，同时保持了推理性能。

Abstract: Large reasoning models (LRMs) have exhibited remarkable reasoning
capabilities through inference-time scaling, but this progress has also
introduced considerable redundancy and inefficiency into their reasoning
processes, resulting in substantial computational waste. Previous work has
attempted to mitigate this issue by penalizing the overall length of generated
samples during reinforcement learning (RL), with the goal of encouraging a more
concise chains of thought. However, we observe that such global length penalty
often lead to excessive compression of critical reasoning steps while
preserving unnecessary details in simpler ones, yielding a suboptimal trade-off
between accuracy and efficiency. To address this issue, we propose
SmartThinker, a two-stage learnable framework designed to enable fine-grained
control over the length of reasoning chains based on the importance of each
individual step. In the first stage, SmartThinker adapts a reasoning model to a
short-form reasoning mode through rejection sampling combined with supervised
fine-tuning (SFT). In the second stage, SmartThinker applies Step-Level Length
Control Policy Optimization (SCPO) to refine the model output distribution,
which increases the proportion of length allocated to critical steps while
reducing redundancy in less important ones. SCPO consists of four core
components: an online importance estimator, a step-level length control reward
function, a step-level generalized advantage estimation (S-GAE) and a
difficulty-adaptive clipping strategy. Working in concert, these components
enable SCPO to implement differentiated length control across reasoning steps.
Empirical results across multiple reasoning benchmarks and various backbone
models demonstrate that SmartThinker significantly reduces redundant reasoning
while achieving comparable or even superior performance to existing methods.

</details>


### [130] [WebSynthesis: World-Model-Guided MCTS for Efficient WebUI-Trajectory Synthesis](https://arxiv.org/abs/2507.04370)
*Yifei Gao,Junhong Ye,Jiaqi Wang,Jitao Sang*

Main category: cs.AI

TL;DR: 论文提出WebSynthesis框架，通过虚拟环境模拟解决复杂动态网页导航中的环境不稳定和高成本问题，提升代理性能。


<details>
  <summary>Details</summary>
Motivation: 解决现有方法在复杂动态网页导航中面临的环境不稳定和高API成本问题。

Method: 利用学习的世界模型模拟虚拟网页环境，支持高效可逆的树形规划，生成多样化高质量轨迹。

Result: 实验表明，小规模合成数据训练的代理性能可媲美或超越大规模真实数据训练的模型。

Conclusion: WebSynthesis为代理的自我提升提供了一种可扩展且高效的解决方案。

Abstract: Recent advancements in large language models (LLMs) have significantly
improved the capabilities of web agents. However, effectively navigating
complex and dynamic web environments still requires more advanced
trajectory-level planning and execution. Prior studies have addressed
self-improving agents by collecting extensive GUI trajectories from
real-environment interactions. Despite their effectiveness, these approaches
encounter two critical challenges: (1) Uncontrollable environment states, where
real or sandboxed web environments often yield unstable and non-deterministic
feedback, complicating the reproduction and debugging of agent behaviors; and
(2) High API costs, as generating even a single interaction trajectory can
involve hundreds of queries, leading to considerable API usage and
computational expenses. To address these limitations and enable scalable
self-improvement for agents, we propose WebSynthesis, a novel framework for
trajectory synthesis and training. WebSynthesis leverages a learned world model
to simulate virtual web environments, allowing a policy agent to perform
efficient and reversible tree-based planning. This approach supports the
large-scale generation of diverse and high-quality trajectories, which are
subsequently utilized to refine the agent's policy. Experimental results
demonstrate that an agent trained using WebSynthesis on a small-scale synthetic
dataset achieves performance comparable to or even surpassing that of models
trained on large-scale real-world data.

</details>


### [131] [MOD-X: A Modular Open Decentralized eXchange Framework proposal for Heterogeneous Interoperable Artificial Agents](https://arxiv.org/abs/2507.04376)
*Georgios Ioannides,Christos Constantinou,Vinija Jain,Aman Chadha,Aaron Elkins*

Main category: cs.AI

TL;DR: MOD-X是一个新型的模块化开放去中心化交换框架，旨在解决异构智能体之间的互操作性问题，通过分层架构、通用消息总线、状态管理和区块链安全机制实现。


<details>
  <summary>Details</summary>
Motivation: 随着AI系统从单一模型发展为专业化智能体生态系统，标准化通信协议的需求日益迫切。

Method: 提出MOD-X框架，包括分层架构、通用消息总线、状态管理、翻译能力和区块链安全机制，并通过实例展示其集成能力。

Result: MOD-X通过发布-订阅模型、语义能力发现和动态工作流编排，实现了异构智能体的高效互操作。

Conclusion: MOD-X为去中心化、可扩展的智能体生态系统提供了理论与实践结合的解决方案。

Abstract: As Artificial Intelligence systems evolve from monolithic models to
ecosystems of specialized agents, the need for standardized communication
protocols becomes increasingly critical. This paper introduces MOD-X (Modular
Open Decentralized eXchange), a novel architectural framework proposal for
agent interoperability that addresses key limitations of existing protocols.
Unlike current approaches, MOD-X proposes a layered architecture with a
Universal Message Bus, thorough state management, translation capabilities, and
blockchain-based security mechanisms. We present MOD-X's architecture, compare
it with existing protocols, and demonstrate its application through a worked
example how it enables integration between heterogeneous specialist agents
(agents with different architectures, vendors, capabilities, and knowledge
representations--including rule-based systems, neural networks, symbolic
reasoning engines, and legacy software with agent wrappers). MOD-X's key
innovations include a publish-subscribe communication model, semantic
capability discovery, and dynamic workflow orchestration--providing a framework
that bridges theoretical formalism with practical implementation. This
architecture addresses the growing need for truly decentralized, interoperable
agent ecosystems that can scale effectively without the need for central
coordination.

</details>


### [132] [DC-Mamber: A Dual Channel Prediction Model based on Mamba and Linear Transformer for Multivariate Time Series Forecasting](https://arxiv.org/abs/2507.04381)
*Bing Fan,Shusen Ma,Yun-Bo Zhao,Yu Kang*

Main category: cs.AI

TL;DR: DC-Mamber是一种基于Mamba和线性Transformer的双通道时间序列预测模型，结合了通道独立和通道混合策略，克服了Transformer和Mamba的局限性，并在实验中表现出优越的准确性。


<details>
  <summary>Details</summary>
Motivation: 现有的多变量时间序列预测方法（如Transformer和Mamba）各有局限性：Transformer对局部时间模式不敏感且计算复杂度高，而Mamba难以并行聚合全局上下文信息。因此，需要一种结合两者优势的模型。

Method: 提出DC-Mamber模型，使用Mamba通道（通道独立策略）提取变量内特征，线性Transformer通道（通道混合策略）建模全局依赖，并通过融合层整合双通道特征进行预测。

Result: 在八个公开数据集上的实验表明，DC-Mamber的准确性优于现有模型。

Conclusion: DC-Mamber通过结合Mamba和线性Transformer的优势，在多变量时间序列预测中取得了显著效果，为未来研究提供了新思路。

Abstract: In multivariate time series forecasting (MTSF), existing strategies for
processing sequences are typically categorized as channel-independent and
channel-mixing. The former treats all temporal information of each variable as
a token, focusing on capturing local temporal features of individual variables,
while the latter constructs a token from the multivariate information at each
time step, emphasizing the modeling of global temporal dependencies. Current
mainstream models are mostly based on Transformer and the emerging Mamba.
Transformers excel at modeling global dependencies through self-attention
mechanisms but exhibit limited sensitivity to local temporal patterns and
suffer from quadratic computational complexity, restricting their efficiency in
long-sequence processing. In contrast, Mamba, based on state space models
(SSMs), achieves linear complexity and efficient long-range modeling but
struggles to aggregate global contextual information in parallel. To overcome
the limitations of both models, we propose DC-Mamber, a dual-channel
forecasting model based on Mamba and linear Transformer for time series
forecasting. Specifically, the Mamba-based channel employs a
channel-independent strategy to extract intra-variable features, while the
Transformer-based channel adopts a channel-mixing strategy to model
cross-timestep global dependencies. DC-Mamber first maps the raw input into two
distinct feature representations via separate embedding layers. These
representations are then processed by a variable encoder (built on Mamba) and a
temporal encoder (built on linear Transformer), respectively. Finally, a fusion
layer integrates the dual-channel features for prediction. Extensive
experiments on eight public datasets confirm DC-Mamber's superior accuracy over
existing models.

</details>


### [133] [LayerCake: Token-Aware Contrastive Decoding within Large Language Model Layers](https://arxiv.org/abs/2507.04404)
*Jingze Zhu,Yongliang Wu,Wenbo Zhu,Jiawang Cao,Yanqiang Zheng,Jiawei Chen,Xu Yang,Bernt Schiele,Jonas Fischer,Xinting Hu*

Main category: cs.AI

TL;DR: 提出了一种基于令牌感知和层定位的对比解码方法，通过联合分析令牌和层的动态关系，提升大语言模型的事实生成准确性。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在知识密集型任务中因事实错误而受限，现有方法未能充分利用令牌和层的联合动态关系。

Method: 引入令牌感知、层定位的对比解码方法，通过分析注意力模式，选择性抑制特定令牌类型在相应层的注意力，生成对比信号指导解码。

Result: 实验表明，该方法无需额外训练或模型修改，显著提升了多种大语言模型在多个基准测试中的事实准确性。

Conclusion: 该方法通过联合优化令牌和层的动态关系，有效提升了大语言模型的事实生成能力。

Abstract: Large language models (LLMs) excel at natural language understanding and
generation but remain vulnerable to factual errors, limiting their reliability
in knowledge-intensive tasks. While decoding-time strategies provide a
promising efficient solution without training, existing methods typically treat
token-level and layer-level signals in isolation, overlooking the joint
dynamics between them. In this work, we introduce a token-aware,
layer-localized contrastive decoding method that aligns specific token types
with their most influential transformer layers to improve factual generation.
Through empirical attention analysis, we identify two key patterns: punctuation
tokens receive dominant attention in early layers, while conceptual tokens
govern semantic reasoning in intermediate layers. By selectively suppressing
attention to these token types at their respective depths, we achieve the
induction of controlled factual degradation and derive contrastive signals to
guide the final factual decoding. Our method requires no additional training or
model modification, and experiments demonstrate that our method consistently
improves factuality across multiple LLMs and various benchmarks.

</details>


### [134] [ARMR: Adaptively Responsive Network for Medication Recommendation](https://arxiv.org/abs/2507.04428)
*Feiyue Wu,Tianxing Wu,Shenqi Jing*

Main category: cs.AI

TL;DR: 提出了一种自适应响应网络（ARMR），用于药物推荐，通过分段时间学习和动态调整机制，提升个性化推荐效果。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以平衡历史药物和新药物的使用，尤其是在患者病情变化时。

Method: ARMR结合分段时间学习（区分近期和远期历史）和自适应响应机制（动态调整对新旧药物的关注）。

Result: 在MIMIC-III和MIMIC-IV数据集上表现优于现有方法，提升了推荐准确性和个性化。

Conclusion: ARMR通过动态调整和分段学习，显著提升了药物推荐的性能。

Abstract: Medication recommendation is a crucial task in healthcare, especially for
patients with complex medical conditions. However, existing methods often
struggle to effectively balance the reuse of historical medications with the
introduction of new drugs in response to the changing patient conditions. In
order to address this challenge, we propose an Adaptively Responsive network
for Medication Recommendation (ARMR), a new method which incorporates 1) a
piecewise temporal learning component that distinguishes between recent and
distant patient history, enabling more nuanced temporal understanding, and 2)
an adaptively responsive mechanism that dynamically adjusts attention to new
and existing drugs based on the patient's current health state and medication
history. Experiments on the MIMIC-III and MIMIC-IV datasets indicate that ARMR
has better performance compared with the state-of-the-art baselines in
different evaluation metrics, which contributes to more personalized and
accurate medication recommendations. The source code is publicly avaiable at:
https://github.com/seucoin/armr2.

</details>


### [135] [MedGellan: LLM-Generated Medical Guidance to Support Physicians](https://arxiv.org/abs/2507.04431)
*Debodeep Banerjee,Burcu Sayin,Stefano Teso,Andrea Passerini*

Main category: cs.AI

TL;DR: MedGellan是一个轻量级、无需标注的框架，利用大语言模型（LLM）从原始医疗记录生成临床指导，帮助医生提高诊断性能。


<details>
  <summary>Details</summary>
Motivation: 医疗决策至关重要，错误可能导致严重后果。完全自动化仍具挑战性，因此结合机器智能与人工监督的混合框架更具实用性。

Method: MedGellan采用贝叶斯启发的提示策略，尊重临床数据的时间顺序，利用LLM生成临床指导。

Result: 初步实验表明，MedGellan生成的指导提高了诊断性能，特别是在召回率和F1分数上。

Conclusion: MedGellan为医疗决策提供了一种有效的混合框架，结合了LLM的优势与医生的专业知识。

Abstract: Medical decision-making is a critical task, where errors can result in
serious, potentially life-threatening consequences. While full automation
remains challenging, hybrid frameworks that combine machine intelligence with
human oversight offer a practical alternative. In this paper, we present
MedGellan, a lightweight, annotation-free framework that uses a Large Language
Model (LLM) to generate clinical guidance from raw medical records, which is
then used by a physician to predict diagnoses. MedGellan uses a
Bayesian-inspired prompting strategy that respects the temporal order of
clinical data. Preliminary experiments show that the guidance generated by the
LLM with MedGellan improves diagnostic performance, particularly in recall and
$F_1$ score.

</details>


### [136] [A Linguistic Analysis of Spontaneous Thoughts: Investigating Experiences of Déjà Vu, Unexpected Thoughts, and Involuntary Autobiographical Memories](https://arxiv.org/abs/2507.04439)
*Videep Venkatesha,Mary Cati Poulos,Christopher Steadman,Caitlin Mills,Anne M. Cleary,Nathaniel Blanchard*

Main category: cs.AI

TL;DR: 该研究通过语言特征分析自发思维（如Deja Vu、不自主自传体记忆和意外思维），验证并更新了现有理论，揭示了语言作为认知窗口的潜力。


<details>
  <summary>Details</summary>
Motivation: 探究自发思维（如Deja Vu等）的动态交互，通过语言特征补充传统主观评估方法的不足。

Method: 分析参与者对自发思维描述的语言模式特征。

Result: Deja Vu表现为抽象和空间语言，不自主自传体记忆富含个人情感细节，意外思维具有不可预测性和认知干扰。

Conclusion: 语言分析为自发认知状态提供了新视角，验证并丰富了现有理论。

Abstract: The onset of spontaneous thoughts are reflective of dynamic interactions
between cognition, emotion, and attention. Typically, these experiences are
studied through subjective appraisals that focus on their triggers,
phenomenology, and emotional salience. In this work, we use linguistic
signatures to investigate Deja Vu, Involuntary Autobiographical Memories and
Unexpected Thoughts. Specifically, we analyze the inherent characteristics of
the linguistic patterns in participant generated descriptions of these thought
types. We show how, by positioning language as a window into spontaneous
cognition, existing theories on these attentional states can be updated and
reaffirmed. Our findings align with prior research, reinforcing that Deja Vu is
a metacognitive experience characterized by abstract and spatial language,
Involuntary Autobiographical Memories are rich in personal and emotionally
significant detail, and Unexpected Thoughts are marked by unpredictability and
cognitive disruption. This work is demonstrative of languages potential to
reveal deeper insights into how internal spontaneous cognitive states manifest
through expression.

</details>


### [137] [Anomalous Decision Discovery using Inverse Reinforcement Learning](https://arxiv.org/abs/2507.04464)
*Ashish Bastola,Mert D. Pesé,Long Cheng,Jonathon Smereka,Abolfazl Razi*

Main category: cs.AI

TL;DR: 论文提出了一种基于逆强化学习（IRL）的异常检测框架TRAP，用于自动驾驶车辆中识别异常行为，解决了现有方法在噪声和未见场景中的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有异常检测方法在噪声、遮挡和未见场景中效果不佳，且监督学习需要大量标注数据，限制了实际应用。

Method: 提出TRAP框架，通过逆强化学习推断潜在驾驶意图，结合奖励和最坏情况监督隐式学习时间信用分配，并利用可变时间范围预训练实现早期检测。

Result: 在14,000+模拟轨迹上实验，AUC达0.90，F1分数82.2%，召回率和F1分数分别比基线高39%和12%。

Conclusion: TRAP在噪声鲁棒性和泛化性上表现优异，为自动驾驶异常检测提供了高效解决方案。

Abstract: Anomaly detection plays a critical role in Autonomous Vehicles (AVs) by
identifying unusual behaviors through perception systems that could compromise
safety and lead to hazardous situations. Current approaches, which often rely
on predefined thresholds or supervised learning paradigms, exhibit reduced
efficacy when confronted with unseen scenarios, sensor noise, and occlusions,
leading to potential safety-critical failures. Moreover, supervised methods
require large annotated datasets, limiting their real-world feasibility. To
address these gaps, we propose an anomaly detection framework based on Inverse
Reinforcement Learning (IRL) to infer latent driving intentions from sequential
perception data, thus enabling robust identification. Specifically, we present
Trajectory-Reward Guided Adaptive Pre-training (TRAP), a novel IRL framework
for anomaly detection, to address two critical limitations of existing methods:
noise robustness and generalization to unseen scenarios. Our core innovation is
implicitly learning temporal credit assignments via reward and worst-case
supervision. We leverage pre-training with variable-horizon sampling to
maximize time-to-consequence, resulting in early detection of behavior
deviation. Experiments on 14,000+ simulated trajectories demonstrate
state-of-the-art performance, achieving 0.90 AUC and 82.2\% F1-score -
outperforming similarly trained supervised and unsupervised baselines by 39\%
on Recall and 12\% on F1-score, respectively. Similar performance is achieved
while exhibiting robustness to various noise types and generalization to unseen
anomaly types. Our code will be available at:
https://github.com/abastola0/TRAP.git

</details>


### [138] [Thousand-Brains Systems: Sensorimotor Intelligence for Rapid, Robust Learning and Inference](https://arxiv.org/abs/2507.04494)
*Niels Leadholm,Viviane Clay,Scott Knudstrup,Hojae Lee,Jeff Hawkins*

Main category: cs.AI

TL;DR: 论文提出了一种名为Monty的千脑系统，模拟生物智能的皮质柱结构，通过3D物体感知任务验证其性能，展示了其在快速推理和持续学习方面的优势。


<details>
  <summary>Details</summary>
Motivation: 当前AI系统在任务表现上出色，但缺乏生物智能的核心特性，如快速持续学习和基于感知运动的表征。论文旨在通过模拟皮质柱结构缩小这一差距。

Method: 利用YCB数据集，评估Monty在3D物体识别和姿态估计任务中的表现，测试其基于感知运动的学习、模块化架构和快速推理策略。

Result: Monty能够构建结构化表征，支持鲁棒泛化，并通过模块间通信加速推理。其学习效率优于当前深度学习架构。

Conclusion: 千脑系统是一种有前景的AI新方法，Monty的初步成果验证了其潜力。

Abstract: Current AI systems achieve impressive performance on many tasks, yet they
lack core attributes of biological intelligence, including rapid, continual
learning, representations grounded in sensorimotor interactions, and structured
knowledge that enables efficient generalization. Neuroscience theory suggests
that mammals evolved flexible intelligence through the replication of a
semi-independent, sensorimotor module, a functional unit known as a cortical
column. To address the disparity between biological and artificial
intelligence, thousand-brains systems were proposed as a means of mirroring the
architecture of cortical columns and their interactions.
  In the current work, we evaluate the unique properties of Monty, the first
implementation of a thousand-brains system. We focus on 3D object perception,
and in particular, the combined task of object recognition and pose estimation.
Utilizing the YCB dataset of household objects, we first assess Monty's use of
sensorimotor learning to build structured representations, finding that these
enable robust generalization. These representations include an emphasis on
classifying objects by their global shape, as well as a natural ability to
detect object symmetries. We then explore Monty's use of model-free and
model-based policies to enable rapid inference by supporting principled
movements. We find that such policies complement Monty's modular architecture,
a design that can accommodate communication between modules to further
accelerate inference speed via a novel `voting' algorithm. Finally, we examine
Monty's use of associative, Hebbian-like binding to enable rapid, continual,
and computationally efficient learning, properties that compare favorably to
current deep learning architectures. While Monty is still in a nascent stage of
development, these findings support thousand-brains systems as a powerful and
promising new approach to AI.

</details>


### [139] [Churn-Aware Recommendation Planning under Aggregated Preference Feedback](https://arxiv.org/abs/2507.04513)
*Gur Keinan,Omer Ben-Porat*

Main category: cs.AI

TL;DR: 论文研究了在隐私保护背景下推荐系统的序列决策问题，提出了Rec-APC模型，证明了最优策略会收敛到纯利用，并通过实验验证了其优越性。


<details>
  <summary>Details</summary>
Motivation: 由于法规和技术限制，推荐系统只能获取群体偏好数据，这为个性化推荐带来了挑战，需要在探索用户偏好和避免用户流失之间平衡。

Method: 提出Rec-APC模型，通过贝叶斯更新处理匿名用户的二元反馈，并设计分支定界算法计算最优策略。

Result: 实验证明该模型在合成数据和MovieLens数据上表现优于POMDP求解器SARSOP，尤其在用户类型较多时。

Conclusion: 该方法为基于聚合偏好数据的决策提供了新思路，具有实际应用潜力。

Abstract: We study a sequential decision-making problem motivated by recent regulatory
and technological shifts that limit access to individual user data in
recommender systems (RSs), leaving only population-level preference
information. This privacy-aware setting poses fundamental challenges in
planning under uncertainty: Effective personalization requires exploration to
infer user preferences, yet unsatisfactory recommendations risk immediate user
churn. To address this, we introduce the Rec-APC model, in which an anonymous
user is drawn from a known prior over latent user types (e.g., personas or
clusters), and the decision-maker sequentially selects items to recommend.
Feedback is binary -- positive responses refine the posterior via Bayesian
updates, while negative responses result in the termination of the session.
  We prove that optimal policies converge to pure exploitation in finite time
and propose a branch-and-bound algorithm to efficiently compute them.
Experiments on synthetic and MovieLens data confirm rapid convergence and
demonstrate that our method outperforms the POMDP solver SARSOP, particularly
when the number of user types is large or comparable to the number of content
categories. Our results highlight the applicability of this approach and
inspire new ways to improve decision-making under the constraints imposed by
aggregated preference data.

</details>


### [140] [Towards integration of Privacy Enhancing Technologies in Explainable Artificial Intelligence](https://arxiv.org/abs/2507.04528)
*Sonal Allana,Rozita Dara,Xiaodong Lin,Pulei Xiong*

Main category: cs.AI

TL;DR: XAI方法可能泄露隐私，本文探索隐私增强技术（PETs）作为防御机制，评估了三种PETs对XAI隐私攻击的效果，最佳情况下攻击风险降低49.47%。


<details>
  <summary>Details</summary>
Motivation: XAI方法存在隐私泄露风险，目前缺乏针对解释性隐私攻击的防御措施。

Method: 评估三种PETs（合成训练数据、差分隐私训练和噪声添加）在两类特征XAI中的效果。

Result: PETs集成最高可降低攻击风险49.47%，同时保持模型效用和解释质量。

Conclusion: PETs可作为XAI隐私防御的有效手段，需权衡其与其他系统属性的影响。

Abstract: Explainable Artificial Intelligence (XAI) is a crucial pathway in mitigating
the risk of non-transparency in the decision-making process of black-box
Artificial Intelligence (AI) systems. However, despite the benefits, XAI
methods are found to leak the privacy of individuals whose data is used in
training or querying the models. Researchers have demonstrated privacy attacks
that exploit explanations to infer sensitive personal information of
individuals. Currently there is a lack of defenses against known privacy
attacks targeting explanations when vulnerable XAI are used in production and
machine learning as a service system. To address this gap, in this article, we
explore Privacy Enhancing Technologies (PETs) as a defense mechanism against
attribute inference on explanations provided by feature-based XAI methods. We
empirically evaluate 3 types of PETs, namely synthetic training data,
differentially private training and noise addition, on two categories of
feature-based XAI. Our evaluation determines different responses from the
mitigation methods and side-effects of PETs on other system properties such as
utility and performance. In the best case, PETs integration in explanations
reduced the risk of the attack by 49.47%, while maintaining model utility and
explanation quality. Through our evaluation, we identify strategies for using
PETs in XAI for maximizing benefits and minimizing the success of this privacy
attack on sensitive personal information.

</details>


### [141] [Exploring Core and Periphery Precepts in Biological and Artificial Intelligence: An Outcome-Based Perspective](https://arxiv.org/abs/2507.04594)
*Niloofar Shadab,Tyler Cody,Alejandro Salado,Taylan G. Topcu,Mohammad Shadab,Peter Beling*

Main category: cs.AI

TL;DR: 论文提出了一种新的系统原则“核心与外围”，用于解决智能系统规模化问题，并通过实证展示了其在生物和人工智能系统中的适用性。


<details>
  <summary>Details</summary>
Motivation: 传统工程方法在智能系统规模化中表现不佳，需要新的系统原则来支持通用智能的工程化。

Method: 基于抽象系统理论和必要多样性法则，提出“核心与外围”原则，并通过数学定义核心主导与外围主导系统。

Result: 实证研究表明，该框架适用于生物和人工智能系统，实现了抽象理论与实际应用的结合。

Conclusion: “核心与外围”原则为智能系统规模化提供了新的理论基础和实用方法。

Abstract: Engineering methodologies predominantly revolve around established principles
of decomposition and recomposition. These principles involve partitioning
inputs and outputs at the component level, ensuring that the properties of
individual components are preserved upon composition. However, this view does
not transfer well to intelligent systems, particularly when addressing the
scaling of intelligence as a system property. Our prior research contends that
the engineering of general intelligence necessitates a fresh set of overarching
systems principles. As a result, we introduced the "core and periphery"
principles, a novel conceptual framework rooted in abstract systems theory and
the Law of Requisite Variety. In this paper, we assert that these abstract
concepts hold practical significance. Through empirical evidence, we illustrate
their applicability to both biological and artificial intelligence systems,
bridging abstract theory with real-world implementations. Then, we expand on
our previous theoretical framework by mathematically defining core-dominant vs
periphery-dominant systems.

</details>


### [142] [DisMS-TS: Eliminating Redundant Multi-Scale Features for Time Series Classification](https://arxiv.org/abs/2507.04600)
*Zhipeng Liu,Peibo Duan,Binwu Wang,Xuan Tang,Qi Chu,Changsheng Zhang,Yongsheng Huang,Bin Zhang*

Main category: cs.AI

TL;DR: 提出了一种新的端到端多尺度解耦框架（DisMS-TS），用于时间序列分类，通过消除多尺度时间序列中的冗余共享特征，显著提升了分类性能。


<details>
  <summary>Details</summary>
Motivation: 现实世界的时间序列通常表现出复杂的时序变化，使得分类任务极具挑战性。现有的多尺度分析方法未能有效消除多尺度时间序列中的冗余共享特征，导致模型对共享特征的关注过度或不足。

Method: 提出了DisMS-TS框架，包括一个时序解耦模块，分别捕获尺度共享和尺度特定的时序表示，并通过两个正则化项确保共享表示的一致性和特定表示的差异性。

Result: 在多个数据集上的实验表明，DisMS-TS优于现有基线方法，准确率提升最高达9.71%。

Conclusion: DisMS-TS通过解耦多尺度时间序列中的共享和特定特征，显著提升了分类性能，为解决复杂时序变化问题提供了有效方案。

Abstract: Real-world time series typically exhibit complex temporal variations, making
the time series classification task notably challenging. Recent advancements
have demonstrated the potential of multi-scale analysis approaches, which
provide an effective solution for capturing these complex temporal patterns.
However, existing multi-scale analysis-based time series prediction methods
fail to eliminate redundant scale-shared features across multi-scale time
series, resulting in the model over- or under-focusing on scale-shared
features. To address this issue, we propose a novel end-to-end Disentangled
Multi-Scale framework for Time Series classification (DisMS-TS). The core idea
of DisMS-TS is to eliminate redundant shared features in multi-scale time
series, thereby improving prediction performance. Specifically, we propose a
temporal disentanglement module to capture scale-shared and scale-specific
temporal representations, respectively. Subsequently, to effectively learn both
scale-shared and scale-specific temporal representations, we introduce two
regularization terms that ensure the consistency of scale-shared
representations and the disparity of scale-specific representations across all
temporal scales. Extensive experiments conducted on multiple datasets validate
the superiority of DisMS-TS over its competitive baselines, with the accuracy
improvement up to 9.71%.

</details>


### [143] [Can Prompt Difficulty be Online Predicted for Accelerating RL Finetuning of Reasoning Models?](https://arxiv.org/abs/2507.04632)
*Yun Qu,Qi Cheems Wang,Yixiu Mao,Vincent Tao Hu,Xiangyang Ji*

Main category: cs.AI

TL;DR: 论文提出了一种名为MoPPS的贝叶斯风险预测框架，通过迭代近似评估减少强化学习微调中的计算开销，无需频繁调用大型语言模型（LLM）。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖频繁的提示评估和子集选择，导致高计算成本，因此需要一种更高效的方法来预测提示难度并加速训练。

Method: MoPPS通过贝叶斯推理建模提示的成功率为潜在变量，并在多臂老虎机框架中使用后验采样，实现高效自适应的提示选择。

Result: 实验表明，MoPPS能可靠预测提示难度，显著减少LLM调用次数并加速训练。

Conclusion: MoPPS提供了一种计算高效的方法，优化了强化学习微调过程。

Abstract: Recent advances have witnessed the effectiveness of reinforcement learning
(RL) finetuning in enhancing the reasoning capabilities of large language
models (LLMs). The optimization process often requires numerous iterations to
achieve satisfactory performance, resulting in high computational costs due to
the need for frequent prompt evaluations under intensive LLM interactions and
repeated policy updates. Appropriate online prompt selection methods reduce
iteration steps by prioritizing informative prompts during training, while the
pipeline's reliance on exhaustive prompt evaluation and subset selection for
optimization still incurs substantial computational overhead due to frequent
LLM inference calls. Distinguished from these direct evaluate-then-select
schemes, this work investigates iterative approximate evaluation for arbitrary
prompts and introduces Model Predictive Prompt Selection (MoPPS), a Bayesian
risk-predictive framework that online estimates prompt difficulty without
requiring costly LLM interactions. Technically, MoPPS models each prompt's
success rate as a latent variable, performs streaming Bayesian inference, and
employs posterior sampling in a constructed multi-armed bandit machine,
enabling sample efficient and adaptive prompt selection. Extensive experiments
across mathematics, planning, and vision-based geometry tasks show that MoPPS
reliably predicts prompt difficulty and accelerates training with significantly
reduced LLM rollouts.

</details>


### [144] [Trojan Horse Prompting: Jailbreaking Conversational Multimodal Models by Forging Assistant Message](https://arxiv.org/abs/2507.04673)
*Wei Duan,Li Qian*

Main category: cs.AI

TL;DR: 论文提出了一种新型攻击方法“特洛伊木马提示”，通过伪造对话历史绕过LLM的安全机制，揭示现代对话AI的安全缺陷。


<details>
  <summary>Details</summary>
Motivation: 对话界面的普及增强了LLM的实用性，但也引入了新的攻击面，尤其是模型对自身对话历史的信任问题。

Method: 攻击者通过伪造模型的对话历史，注入恶意内容并触发有害输出，利用模型对自身历史的不对称安全对齐。

Result: 实验证明该方法在攻击成功率上显著优于传统攻击手段，揭示了对话AI的安全漏洞。

Conclusion: 需从输入级过滤转向协议级验证，确保对话上下文的完整性。

Abstract: The rise of conversational interfaces has greatly enhanced LLM usability by
leveraging dialogue history for sophisticated reasoning. However, this reliance
introduces an unexplored attack surface. This paper introduces Trojan Horse
Prompting, a novel jailbreak technique. Adversaries bypass safety mechanisms by
forging the model's own past utterances within the conversational history
provided to its API. A malicious payload is injected into a model-attributed
message, followed by a benign user prompt to trigger harmful content
generation. This vulnerability stems from Asymmetric Safety Alignment: models
are extensively trained to refuse harmful user requests but lack comparable
skepticism towards their own purported conversational history. This implicit
trust in its "past" creates a high-impact vulnerability. Experimental
validation on Google's Gemini-2.0-flash-preview-image-generation shows Trojan
Horse Prompting achieves a significantly higher Attack Success Rate (ASR) than
established user-turn jailbreaking methods. These findings reveal a fundamental
flaw in modern conversational AI security, necessitating a paradigm shift from
input-level filtering to robust, protocol-level validation of conversational
context integrity.

</details>


### [145] [Advocate for Complete Benchmarks for Formal Reasoning with Formal/Informal Statements and Formal/Informal Proofs](https://arxiv.org/abs/2507.04719)
*Roozbeh Yousefzadeh,Xuenan Cao*

Main category: cs.AI

TL;DR: 本文批评了形式推理和自动定理证明领域的基准测试和评估实践，提倡开放代码、数据和完整无错误的基准以加速进展。


<details>
  <summary>Details</summary>
Motivation: 讨论当前实践中的问题，促进开放和透明，加速领域发展。

Method: 分析现有实践，识别障碍并提出改进建议。

Result: 指出了误导性评估信息的来源，并提出了消除障碍的方法。

Conclusion: 旨在通过讨论促进不同群体合作，推动领域进步。

Abstract: This position paper provides a critical but constructive discussion of
current practices in benchmarking and evaluative practices in the field of
formal reasoning and automated theorem proving. We take the position that open
code, open data, and benchmarks that are complete and error-free will
accelerate progress in this field. We identify practices that create barriers
to contributing to this field and suggest ways to remove them. We also discuss
some of the practices that might produce misleading evaluative information. We
aim to create discussions that bring together people from various groups
contributing to automated theorem proving, autoformalization, and informal
reasoning.

</details>


### [146] [LumiCRS: Asymmetric Contrastive Prototype Learning for Long-Tail Conversational Movie Recommendation](https://arxiv.org/abs/2507.04722)
*Jinzhi Wang,Bin Li,Qingke Peng,Haozhou Li,Zeyuan Zeng,Ruimeng Li,Biyi Zhou*

Main category: cs.AI

TL;DR: LumiCRS是一个端到端框架，通过动态调整损失函数、原型学习和对话增强模块，解决了对话推荐系统中的长尾分布问题，显著提升了推荐的准确性、多样性和公平性。


<details>
  <summary>Details</summary>
Motivation: 对话推荐系统（CRSs）存在长尾分布问题，导致头部热门项目过度拟合，尾部项目稀疏，影响了推荐的多样性和冷启动问题。

Method: LumiCRS采用三层策略：(i) 自适应综合焦点损失（ACFL）动态调整权重；(ii) 原型学习稳定表示；(iii) GPT-4o驱动的对话增强模块生成多样化对话片段。

Result: 在REDIAL和INSPIRED基准测试中，LumiCRS的Recall@10和Tail-Recall@10提升了7-15%，人类评估也显示其流畅性、信息量和长尾相关性更优。

Conclusion: LumiCRS通过多层协作有效解决了长尾问题，提升了对话推荐系统的效率和公平性。

Abstract: Conversational recommender systems (CRSs) often suffer from an extreme
long-tail distribution of dialogue data, causing a strong bias toward
head-frequency blockbusters that sacrifices diversity and exacerbates the
cold-start problem. An empirical analysis of DCRS and statistics on the REDIAL
corpus show that only 10% of head movies account for nearly half of all
mentions, whereas about 70% of tail movies receive merely 26% of the attention.
This imbalance gives rise to three critical challenges: head over-fitting, body
representation drift, and tail sparsity. To address these issues, we propose
LumiCRS, an end-to-end framework that mitigates long-tail imbalance through
three mutually reinforcing layers: (i) an Adaptive Comprehensive Focal Loss
(ACFL) that dynamically adjusts class weights and focusing factors to curb head
over-fitting and reduce popularity bias; (ii) Prototype Learning for Long-Tail
Recommendation, which selects semantic, affective, and contextual prototypes to
guide clustering and stabilize body and tail representations; and (iii) a
GPT-4o-driven prototype-guided dialogue augmentation module that automatically
generates diverse long-tail conversational snippets to alleviate tail sparsity
and distribution shift. Together, these strategies enable LumiCRS to markedly
improve recommendation accuracy, diversity, and fairness: on the REDIAL and
INSPIRED benchmarks, LumiCRS boosts Recall@10 and Tail-Recall@10 by 7-15% over
fifteen strong baselines, while human evaluations confirm superior fluency,
informativeness, and long-tail relevance. These results demonstrate the
effectiveness of multi-layer collaboration in building an efficient and fair
long-tail conversational recommender.

</details>


### [147] [ChipSeek-R1: Generating Human-Surpassing RTL with LLM via Hierarchical Reward-Driven Reinforcement Learning](https://arxiv.org/abs/2507.04736)
*Zhirong Chen,Kaiyan Chang,Zhuolin Li,Xinyang He,Chujie Chen,Cangyuan Li,Mengdi Wang,Haobo Xu,Yinhe Han,Ying Wang*

Main category: cs.AI

TL;DR: ChipSeek-R1是一种基于分层奖励的强化学习框架，用于训练LLM生成功能正确且PPA优化的RTL代码。


<details>
  <summary>Details</summary>
Motivation: 当前方法无法同时优化功能正确性和硬件质量（PPA），需要一种新方法。

Method: 采用分层奖励驱动的强化学习框架，结合语法、功能正确性和PPA指标的反馈。

Result: 在标准基准测试中，ChipSeek-R1在功能正确性上达到最优，并在RTLLM基准测试中生成27个PPA优于人工代码的设计。

Conclusion: 研究表明，将工具链反馈整合到LLM训练中，强化学习能实现自动化生成超越人工的RTL代码。

Abstract: Large Language Models (LLMs) show significant potential for automating
Register-Transfer Level (RTL) code generation. However, current approaches face
a critical challenge: they can not simultaneously optimize for functional
correctness and hardware quality (Power, Performance, Area - PPA). Methods
based on supervised fine-tuning often generate functionally correct but
PPA-suboptimal code, lacking mechanisms to learn optimization principles. In
contrast, post-processing techniques that attempt to improve PPA metrics after
generation are often inefficient because they operate externally without
updating the LLM's parameters, thus failing to enhance the model's intrinsic
design capabilities.
  To bridge this gap, we introduce ChipSeek-R1, a hierarchical reward-driven
reinforcement learning framework to train LLMs to generate RTL code that
achieves both functional correctness and optimized PPA metrics. ChipSeek-R1
employs a hierarchical reward system, which incorporates direct feedback on
syntax, functional correctness (from simulators) and PPA metrics (from
synthesis tools) during reinforcement learning. This enables the model to learn
complex hardware design trade-offs via trial-and-error, generating RTL code
that is both functionally correct and PPA-optimized. Evaluating ChipSeek-R1 on
standard benchmarks (VerilogEval, RTLLM), we achieve state-of-the-art results
in functional correctness. Notably, on the RTLLM benchmark, ChipSeek-R1
generated 27 RTL designs surpassing the PPA metrics of the original
human-written code. Our findings demonstrate the effectiveness of integrating
toolchain feedback into LLM training and highlight the potential for
reinforcement learning to enable automated generation of human-surpassing RTL
code. We open-source our code in anonymous github.

</details>


### [148] [Activation Steering for Chain-of-Thought Compression](https://arxiv.org/abs/2507.04742)
*Seyedarmin Azizi,Erfan Baghaei Potraghloo,Massoud Pedram*

Main category: cs.AI

TL;DR: 论文提出了一种名为ASC的推理时技术，通过调整隐藏表示来压缩冗长的思维链（CoTs），从而在不重新训练模型的情况下提高推理效率。


<details>
  <summary>Details</summary>
Motivation: 现有的思维链（CoTs）在解决简单问题时过于冗长，导致上下文浪费、延迟增加和能耗上升。

Method: 通过提取和注入“转向向量”来切换模型的推理模式，从而压缩思维链。

Result: ASC在MATH500和GSM8K数据集上实现了67.43%的思维链长度缩减，同时保持了准确性，并在8B模型上实现了2.73倍的推理加速。

Conclusion: ASC是一种无需训练的高效方法，适用于对延迟或成本敏感的推理场景。

Abstract: Large language models (LLMs) excel at complex reasoning when they include
intermediate steps, known as "chains of thought" (CoTs). However, these
rationales are often overly verbose, even for simple problems, leading to
wasted context, increased latency, and higher energy consumption. We observe
that verbose, English-heavy CoTs and concise, math-centric CoTs occupy distinct
regions in the model's residual-stream activation space. By extracting and
injecting a "steering vector" to transition between these modes, we can
reliably shift generation toward more concise reasoning, effectively
compressing CoTs without retraining. We formalize this approach as
Activation-Steered Compression (ASC), an inference-time technique that shortens
reasoning traces by directly modifying hidden representations. In addition, we
provide a theoretical analysis of the impact of ASC on the output distribution,
derived from a closed-form KL-divergence-bounded constraint to regulate
steering strength. Using only 100 paired verbose and concise examples, ASC
achieves up to 67.43% reduction in CoT length on MATH500 and GSM8K datasets,
while maintaining accuracy across 7B, 8B, and 32B parameter models. As a
training-free method, ASC introduces negligible runtime overhead and, on
MATH500, delivers an average 2.73x speedup in end-to-end reasoning wall-clock
time on an 8B model. This makes ASC a practical and efficient tool for
streamlining the deployment of reasoning-capable LLMs in latency- or
cost-sensitive settings. The code is available at:
https://github.com/ArminAzizi98/ASC

</details>


### [149] [LLM-based Question-Answer Framework for Sensor-driven HVAC System Interaction](https://arxiv.org/abs/2507.04748)
*Sungmin Lee,Minju Kang,Joonhee Lee,Seungyong Lee,Dongju Kim,Jingi Hong,Jun Shin,Pei Zhang,JeongGil Ko*

Main category: cs.AI

TL;DR: JARVIS是一个基于LLM的两阶段QA框架，专为HVAC系统交互设计，通过专家LLM和代理实现高效查询处理和响应生成。


<details>
  <summary>Details</summary>
Motivation: 提升非专家用户与HVAC系统的交互性，解决实时、上下文感知的QA挑战。

Method: 采用两阶段框架：专家LLM翻译查询，代理执行SQL数据检索和响应生成；集成自适应上下文注入、参数化SQL构建器和自底向上规划。

Result: 在真实HVAC数据和专家标注数据集上表现优异，优于基线方法，响应质量和准确性高。

Conclusion: JARVIS为HVAC系统交互提供了一种准确、可解释的解决方案，适用于多样化查询。

Abstract: Question-answering (QA) interfaces powered by large language models (LLMs)
present a promising direction for improving interactivity with HVAC system
insights, particularly for non-expert users. However, enabling accurate,
real-time, and context-aware interactions with HVAC systems introduces unique
challenges, including the integration of frequently updated sensor data,
domain-specific knowledge grounding, and coherent multi-stage reasoning. In
this paper, we present JARVIS, a two-stage LLM-based QA framework tailored for
sensor data-driven HVAC system interaction. JARVIS employs an Expert-LLM to
translate high-level user queries into structured execution instructions, and
an Agent that performs SQL-based data retrieval, statistical processing, and
final response generation. To address HVAC-specific challenges, JARVIS
integrates (1) an adaptive context injection strategy for efficient HVAC and
deployment-specific information integration, (2) a parameterized SQL builder
and executor to improve data access reliability, and (3) a bottom-up planning
scheme to ensure consistency across multi-stage response generation. We
evaluate JARVIS using real-world data collected from a commercial HVAC system
and a ground truth QA dataset curated by HVAC experts to demonstrate its
effectiveness in delivering accurate and interpretable responses across diverse
queries. Results show that JARVIS consistently outperforms baseline and
ablation variants in both automated and user-centered assessments, achieving
high response quality and accuracy.

</details>


### [150] [FurniMAS: Language-Guided Furniture Decoration using Multi-Agent System](https://arxiv.org/abs/2507.04770)
*Toan Nguyen,Tri Le,Quang Nguyen,Anh Nguyen*

Main category: cs.AI

TL;DR: FurniMAS是一个多智能体系统，用于自动化家具装饰，通过结合LLM和非LLM智能体，显著提升装饰质量。


<details>
  <summary>Details</summary>
Motivation: 家具装饰耗时且需要专业知识，多智能体系统可以自动化这一过程。

Method: 提出FurniMAS系统，结合LLM和非LLM智能体，通过协作完成装饰任务。

Result: 实验表明FurniMAS在生成高质量3D装饰方面显著优于基线方法。

Conclusion: FurniMAS通过多智能体协作，有效解决了家具装饰的自动化问题。

Abstract: Furniture decoration is an important task in various industrial applications.
However, achieving a high-quality decorative result is often time-consuming and
requires specialized artistic expertise. To tackle these challenges, we explore
how multi-agent systems can assist in automating the decoration process. We
propose FurniMAS, a multi-agent system for automatic furniture decoration.
Specifically, given a human prompt and a household furniture item such as a
working desk or a TV stand, our system suggests relevant assets with
appropriate styles and materials, and arranges them on the item, ensuring the
decorative result meets functionality, aesthetic, and ambiance preferences.
FurniMAS assembles a hybrid team of LLM-based and non-LLM agents, each
fulfilling distinct roles in a typical decoration project. These agents
collaborate through communication, logical reasoning, and validation to
transform the requirements into the final outcome. Extensive experiments
demonstrate that our FurniMAS significantly outperforms other baselines in
generating high-quality 3D decor.

</details>


### [151] [Application and Evaluation of Large Language Models for Forecasting the Impact of Traffic Incidents](https://arxiv.org/abs/2507.04803)
*George Jagadeesh,Srikrishna Iyer,Michal Polanowski,Kai Xin Thia*

Main category: cs.AI

TL;DR: 研究探讨了使用大型语言模型（LLM）预测交通事件对交通流影响的可行性，发现LLM无需大量训练数据且能利用自由文本事件日志，性能与传统机器学习模型相当。


<details>
  <summary>Details</summary>
Motivation: 传统机器学习方法需要大量训练数据，而LLM能利用自由文本事件日志且无需专门训练，为交通事件影响预测提供了新思路。

Method: 提出了一种完全基于LLM的解决方案，结合交通特征和LLM提取的事件特征进行预测，并优化了LLM的上下文学习示例选择方法。

Result: 在真实交通事件数据集上评估了三种先进LLM和两种机器学习模型，表现最佳的LLM与传统最优模型准确率相当。

Conclusion: LLM在交通事件影响预测中具有实际应用潜力，性能与传统方法相当且更具灵活性。

Abstract: This study examines the feasibility of applying large language models (LLMs)
for forecasting the impact of traffic incidents on the traffic flow. The use of
LLMs for this task has several advantages over existing machine learning-based
solutions such as not requiring a large training dataset and the ability to
utilize free-text incident logs. We propose a fully LLM-based solution that
predicts the incident impact using a combination of traffic features and
LLM-extracted incident features. A key ingredient of this solution is an
effective method of selecting examples for the LLM's in-context learning. We
evaluate the performance of three advanced LLMs and two state-of-the-art
machine learning models on a real traffic incident dataset. The results show
that the best-performing LLM matches the accuracy of the most accurate machine
learning model, despite the former not having been trained on this prediction
task. The findings indicate that LLMs are a practically viable option for
traffic incident impact prediction.

</details>


### [152] [DoPI: Doctor-like Proactive Interrogation LLM for Traditional Chinese Medicine](https://arxiv.org/abs/2507.04877)
*Zewen Sun,Ruoxiang Huang,Jiahe Feng,Rundong Kong,Yuqian Wang,Hengyu Liu,Ziqi Gong,Yuyuan Qin,Yingxue Wang,Yu Wang*

Main category: cs.AI

TL;DR: 论文提出DoPI系统，通过多轮对话和知识图谱提升中医诊断能力，解决了现有大语言模型在医学应用中的局限性。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型在医学应用中存在多轮对话和主动提问的不足，限制了其在真实诊断场景中的实用性。

Method: 提出DoPI系统，包含指导模型和专家模型，前者动态生成问题，后者提供诊断和治疗方案。构建多轮医患对话数据集并提出新评估方法。

Result: 实验结果显示DoPI系统在问诊结果中达到84.68%的准确率，显著提升了诊断沟通能力。

Conclusion: DoPI系统有效结合多轮对话和专业知识，提升了中医诊断的实用性和准确性。

Abstract: Enhancing interrogation capabilities in Traditional Chinese Medicine (TCM)
diagnosis through multi-turn dialogues and knowledge graphs presents a
significant challenge for modern AI systems. Current large language models
(LLMs), despite their advancements, exhibit notable limitations in medical
applications, particularly in conducting effective multi-turn dialogues and
proactive questioning. These shortcomings hinder their practical application
and effectiveness in simulating real-world diagnostic scenarios. To address
these limitations, we propose DoPI, a novel LLM system specifically designed
for the TCM domain. The DoPI system introduces a collaborative architecture
comprising a guidance model and an expert model. The guidance model conducts
multi-turn dialogues with patients and dynamically generates questions based on
a knowledge graph to efficiently extract critical symptom information.
Simultaneously, the expert model leverages deep TCM expertise to provide final
diagnoses and treatment plans. Furthermore, this study constructs a multi-turn
doctor-patient dialogue dataset to simulate realistic consultation scenarios
and proposes a novel evaluation methodology that does not rely on manually
collected real-world consultation data. Experimental results show that the DoPI
system achieves an accuracy rate of 84.68 percent in interrogation outcomes,
significantly enhancing the model's communication ability during diagnosis
while maintaining professional expertise.

</details>


### [153] [MARBLE: A Multi-Agent Rule-Based LLM Reasoning Engine for Accident Severity Prediction](https://arxiv.org/abs/2507.04893)
*Kaleem Ullah Qasim,Jiashu Zhang*

Main category: cs.AI

TL;DR: MARBLE是一种多智能体规则驱动的LLM引擎，通过分解任务和模块化推理，显著提升了交通事故严重性预测的准确性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 解决交通事故严重性预测中数据不完整、特征依赖性强和类别不平衡的问题，同时提升模型的扩展性和可解释性。

Method: 采用多智能体系统，每个智能体专注于特定特征子集（如空间、环境、时间），通过规则或LLM引导的共识机制协调预测，并保留推理痕迹。

Result: 在英美数据集上，MARBLE的准确率接近90%，显著优于传统机器学习方法和SOTA提示推理方法（如CoT、L2M、ToT）。

Conclusion: MARBLE为安全关键应用中的不确定性推理提供了通用且可解释的框架，重新定义了实际应用中的性能上限。

Abstract: Accident severity prediction plays a critical role in transportation safety
systems but is a persistently difficult task due to incomplete data, strong
feature dependencies, and severe class imbalance in which rare but
high-severity cases are underrepresented and hard to detect. Existing methods
often rely on monolithic models or black box prompting, which struggle to scale
in noisy, real-world settings and offer limited interpretability. To address
these challenges, we propose MARBLE a multiagent rule based LLM engine that
decomposes the severity prediction task across a team of specialized reasoning
agents, including an interchangeable ML-backed agent. Each agent focuses on a
semantic subset of features (e.g., spatial, environmental, temporal), enabling
scoped reasoning and modular prompting without the risk of prompt saturation.
Predictions are coordinated through either rule-based or LLM-guided consensus
mechanisms that account for class rarity and confidence dynamics. The system
retains structured traces of agent-level reasoning and coordination outcomes,
supporting in-depth interpretability and post-hoc performance diagnostics.
Across both UK and US datasets, MARBLE consistently outperforms traditional
machine learning classifiers and state-of-the-art (SOTA) prompt-based reasoning
methods including Chain-of-Thought (CoT), Least-to-Most (L2M), and
Tree-of-Thought (ToT) achieving nearly 90% accuracy where others plateau below
48%. This performance redefines the practical ceiling for accident severity
classification under real world noise and extreme class imbalance. Our results
position MARBLE as a generalizable and interpretable framework for reasoning
under uncertainty in safety-critical applications.

</details>


### [154] [Supported Abstract Argumentation for Case-Based Reasoning](https://arxiv.org/abs/2507.04994)
*Adam Gould,Gabriel de Olim Gaul,Francesca Toni*

Main category: cs.AI

TL;DR: sAA-CBR是一种基于案例推理的二元分类模型，通过引入支持机制解决了前身AA-CBR中无关案例的问题，同时保持了关键模型特性。


<details>
  <summary>Details</summary>
Motivation: 解决AA-CBR模型中存在的无关案例（spikes）问题，提升模型分类的准确性和可靠性。

Method: 通过让过去案例参与辩论，支持或攻击对立或同意的标签，引入支持机制来避免无关案例。

Result: 证明sAA-CBR模型中不存在无关案例，同时保留了关键模型特性。

Conclusion: sAA-CBR通过支持机制有效解决了无关案例问题，且不影响模型的核心性能。

Abstract: We introduce Supported Abstract Argumentation for Case-Based Reasoning
(sAA-CBR), a binary classification model in which past cases engage in debates
by arguing in favour of their labelling and attacking or supporting those with
opposing or agreeing labels. With supports, sAA-CBR overcomes the limitation of
its precursor AA-CBR, which can contain extraneous cases (or spikes) that are
not included in the debates. We prove that sAA-CBR contains no spikes, without
trading off key model properties

</details>


### [155] [When Imitation Learning Outperforms Reinforcement Learning in Surgical Action Planning](https://arxiv.org/abs/2507.05011)
*Maxence Boels,Harry Robertshaw,Alejandro Granados,Prokar Dasgupta,Sebastien Ourselin*

Main category: cs.AI

TL;DR: 论文首次全面比较了模仿学习（IL）与强化学习（RL）在手术动作规划中的表现，发现IL优于RL。


<details>
  <summary>Details</summary>
Motivation: 探索在手术动作规划中，IL和RL哪种方法更有效，以提供实时辅助。

Method: 提出了双任务自回归模仿学习（DARIL）基线，并评估了三种RL变体：基于世界模型的RL、直接视频RL和逆RL增强。

Result: DARIL在动作三元组识别和下一帧预测中表现最佳（34.6%和33.6% mAP），而所有RL方法均表现不佳（最低3.1% mAP）。

Conclusion: 研究表明，在专家标注的测试集上，IL优于RL，挑战了RL在序列决策中的优越性假设。

Abstract: Surgical action planning requires predicting future instrument-verb-target
triplets for real-time assistance. While teleoperated robotic surgery provides
natural expert demonstrations for imitation learning (IL), reinforcement
learning (RL) could potentially discover superior strategies through
exploration. We present the first comprehensive comparison of IL versus RL for
surgical action planning on CholecT50. Our Dual-task Autoregressive Imitation
Learning (DARIL) baseline achieves 34.6% action triplet recognition mAP and
33.6% next frame prediction mAP with smooth planning degradation to 29.2% at
10-second horizons. We evaluated three RL variants: world model-based RL,
direct video RL, and inverse RL enhancement. Surprisingly, all RL approaches
underperformed DARIL i.e. world model RL dropped to 3.1% mAP at 10s while
direct video RL achieved only 15.9%. Our analysis reveals that distribution
matching on expert-annotated test sets systematically favors IL over
potentially valid RL policies that differ from training demonstrations. This
challenges assumptions about RL superiority in sequential decision making and
provides crucial insights for surgical AI development.

</details>


### [156] [How Rules Represent Causal Knowledge: Causal Modeling with Abductive Logic Programs](https://arxiv.org/abs/2507.05088)
*Kilian Rückschloß,Felix Weitkämper*

Main category: cs.AI

TL;DR: 本文扩展了Pearl的因果理论，将其应用于分层溯因逻辑程序，证明了稳定模型语义符合因果关系的哲学原则。


<details>
  <summary>Details</summary>
Motivation: 探讨如何将因果知识应用于逻辑程序，以支持对外部干预的预测和推理。

Method: 通过将分层溯因逻辑程序转化为因果系统，赋予逻辑规则明确的因果解释。

Result: 稳定模型语义符合因果充分性、自然必要性和未观察效应无关性等哲学原则。

Conclusion: 分层溯因逻辑程序可作为因果建模和干预预测的有效框架。

Abstract: Pearl observes that causal knowledge enables predicting the effects of
interventions, such as actions, whereas descriptive knowledge only permits
drawing conclusions from observation. This paper extends Pearl's approach to
causality and interventions to the setting of stratified abductive logic
programs. It shows how stable models of such programs can be given a causal
interpretation by building on philosophical foundations and recent work by
Bochman and Eelink et al. In particular, it provides a translation of abductive
logic programs into causal systems, thereby clarifying the informal causal
reading of logic program rules and supporting principled reasoning about
external actions. The main result establishes that the stable model semantics
for stratified programs conforms to key philosophical principles of causation,
such as causal sufficiency, natural necessity, and irrelevance of unobserved
effects. This justifies the use of stratified abductive logic programs as a
framework for causal modeling and for predicting the effects of interventions

</details>


### [157] [Rule Learning for Knowledge Graph Reasoning under Agnostic Distribution Shift](https://arxiv.org/abs/2507.05110)
*Shixuan Liu,Yue He,Yunfei Wang,Hao Zou,Haoxiang Cheng,Wenjing Yang,Peng Cui,Zhong Liu*

Main category: cs.AI

TL;DR: 论文提出了一种名为StableRule的框架，用于解决知识图谱（KG）推理中因未知选择偏差和分布偏移导致的OOD问题，通过特征解耦和规则学习提升泛化性能。


<details>
  <summary>Details</summary>
Motivation: 现有KG推理方法依赖I.I.D假设，但在实际应用中易受未知选择偏差和分布偏移影响，导致性能下降。研究旨在解决这一限制，提升KG推理在真实环境中的鲁棒性。

Method: 提出StableRule框架，结合特征解耦与规则学习网络，以增强OOD泛化能力。通过特征解耦减少协变量偏移的影响，优化规则学习。

Result: 在七个基准KG上的实验表明，StableRule在异构环境中表现出卓越的有效性和稳定性。

Conclusion: StableRule框架显著提升了KG推理在OOD场景中的性能，具有实际应用价值。

Abstract: Knowledge graph (KG) reasoning remains a critical research area focused on
inferring missing knowledge by analyzing relationships among observed facts.
Despite its success, a key limitation of existing KG reasoning methods is their
dependence on the I.I.D assumption. This assumption can easily be violated due
to unknown sample selection bias during training or agnostic distribution
shifts during testing, significantly compromising model performance and
reliability. To facilitate the deployment of KG reasoning in wild environments,
this study investigates learning logical rules from KGs affected by unknown
selection bias. Additionally, we address test sets with agnostic distribution
shifts, formally defining this challenge as out-of-distribution (OOD) KG
reasoning-a previously underexplored problem. To solve the issue, we propose
the Stable Rule Learning (StableRule) framework, an end-to-end methodology that
integrates feature decorrelation with rule learning network, to enhance OOD
generalization performance. By leveraging feature decorrelation, the StableRule
framework mitigates the adverse effects of covariate shifts arising in OOD
scenarios, thereby improving the robustness of the rule learning component in
effectively deriving logical rules. Extensive experiments on seven benchmark
KGs demonstrate the framework's superior effectiveness and stability across
diverse heterogeneous environments, underscoring its practical significance for
real-world applications.

</details>


### [158] [GIST: Cross-Domain Click-Through Rate Prediction via Guided Content-Behavior Distillation](https://arxiv.org/abs/2507.05142)
*Wei Xu,Haoran Li,Baoyuan Ou,Lai Xu,Yingjie Qin,Ruilong Su,Ruiwen Xu*

Main category: cs.AI

TL;DR: 提出GIST模型，通过解耦源域和目标域的训练过程，结合内容-行为联合训练模块（CBJT）和非对称相似性集成策略（ASI），提升跨域点击率预测性能。


<details>
  <summary>Details</summary>
Motivation: 解决现有方法在联合训练和预训练微调中的局限性，如分布差异和数据持续集成问题。

Method: 提出GIST模型，采用CBJT模块对齐内容-行为分布，结合ASI策略增强知识迁移。

Result: 在离线和在线测试中优于现有方法，成功部署于小红书平台。

Conclusion: GIST有效提升跨域点击率预测性能，适用于大规模工业场景。

Abstract: Cross-domain Click-Through Rate prediction aims to tackle the data sparsity
and the cold start problems in online advertising systems by transferring
knowledge from source domains to a target domain. Most existing methods rely on
overlapping users to facilitate this transfer, often focusing on joint training
or pre-training with fine-tuning approach to connect the source and target
domains. However, in real-world industrial settings, joint training struggles
to learn optimal representations with different distributions, and pre-training
with fine-tuning is not well-suited for continuously integrating new data. To
address these issues, we propose GIST, a cross-domain lifelong sequence model
that decouples the training processes of the source and target domains. Unlike
previous methods that search lifelong sequences in the source domains using
only content or behavior signals or their simple combinations, we innovatively
introduce a Content-Behavior Joint Training Module (CBJT), which aligns
content-behavior distributions and combines them with guided information to
facilitate a more stable representation. Furthermore, we develop an Asymmetric
Similarity Integration strategy (ASI) to augment knowledge transfer through
similarity computation. Extensive experiments demonstrate the effectiveness of
GIST, surpassing SOTA methods on offline evaluations and an online A/B test.
Deployed on the Xiaohongshu (RedNote) platform, GIST effectively enhances
online ads system performance at scale, serving hundreds of millions of daily
active users.

</details>


### [159] [MedGemma Technical Report](https://arxiv.org/abs/2507.05201)
*Andrew Sellergren,Sahar Kazemzadeh,Tiam Jaroensri,Atilla Kiraly,Madeleine Traverse,Timo Kohlberger,Shawn Xu,Fayaz Jamil,Cían Hughes,Charles Lau,Justin Chen,Fereshteh Mahvar,Liron Yatziv,Tiffany Chen,Bram Sterling,Stefanie Anna Baby,Susanna Maria Baby,Jeremy Lai,Samuel Schmidgall,Lu Yang,Kejia Chen,Per Bjornsson,Shashir Reddy,Ryan Brush,Kenneth Philbrick,Howard Hu,Howard Yang,Richa Tiwari,Sunny Jansen,Preeti Singh,Yun Liu,Shekoofeh Azizi,Aishwarya Kamath,Johan Ferret,Shreya Pathak,Nino Vieillard,Ramona Merhej,Sarah Perrin,Tatiana Matejovicova,Alexandre Ramé,Morgane Riviere,Louis Rouillard,Thomas Mesnard,Geoffrey Cideron,Jean-bastien Grill,Sabela Ramos,Edouard Yvinec,Michelle Casbon,Elena Buchatskaya,Jean-Baptiste Alayrac,Dmitry,Lepikhin,Vlad Feinberg,Sebastian Borgeaud,Alek Andreev,Cassidy Hardin,Robert Dadashi,Léonard Hussenot,Armand Joulin,Olivier Bachem,Yossi Matias,Katherine Chou,Avinatan Hassidim,Kavi Goel,Clement Farabet,Joelle Barral,Tris Warkentin,Jonathon Shlens,David Fleet,Victor Cotruta,Omar Sanseviero,Gus Martins,Phoebe Kirk,Anand Rao,Shravya Shetty,David F. Steiner,Can Kirmizibayrak,Rory Pilgrim,Daniel Golden,Lin Yang*

Main category: cs.AI

TL;DR: MedGemma是一组基于Gemma 3的医疗视觉语言基础模型，在医疗多模态任务中表现优异，显著优于同类生成模型，并接近特定任务模型的性能。


<details>
  <summary>Details</summary>
Motivation: 医疗AI的发展面临数据多样性和隐私保护等挑战，需要性能优异且无需大量任务特定调优的基础模型。

Method: 基于Gemma 3 4B和27B构建MedGemma，并引入MedSigLIP作为视觉编码器。

Result: MedGemma在医疗多模态问答、胸部X光分类等任务中表现显著提升，微调后进一步优化子领域性能。

Conclusion: MedGemma为医疗研究和下游应用提供了强大的基础，有望加速医疗AI的发展。

Abstract: Artificial intelligence (AI) has significant potential in healthcare
applications, but its training and deployment faces challenges due to
healthcare's diverse data, complex tasks, and the need to preserve privacy.
Foundation models that perform well on medical tasks and require less
task-specific tuning data are critical to accelerate the development of
healthcare AI applications. We introduce MedGemma, a collection of medical
vision-language foundation models based on Gemma 3 4B and 27B. MedGemma
demonstrates advanced medical understanding and reasoning on images and text,
significantly exceeding the performance of similar-sized generative models and
approaching the performance of task-specific models, while maintaining the
general capabilities of the Gemma 3 base models. For out-of-distribution tasks,
MedGemma achieves 2.6-10% improvement on medical multimodal question answering,
15.5-18.1% improvement on chest X-ray finding classification, and 10.8%
improvement on agentic evaluations compared to the base models. Fine-tuning
MedGemma further improves performance in subdomains, reducing errors in
electronic health record information retrieval by 50% and reaching comparable
performance to existing specialized state-of-the-art methods for pneumothorax
classification and histopathology patch classification. We additionally
introduce MedSigLIP, a medically-tuned vision encoder derived from SigLIP.
MedSigLIP powers the visual understanding capabilities of MedGemma and as an
encoder achieves comparable or better performance than specialized medical
image encoders. Taken together, the MedGemma collection provides a strong
foundation of medical image and text capabilities, with potential to
significantly accelerate medical research and development of downstream
applications. The MedGemma collection, including tutorials and model weights,
can be found at https://goo.gle/medgemma.

</details>


### [160] [SciMaster: Towards General-Purpose Scientific AI Agents, Part I. X-Master as Foundation: Can We Lead on Humanity's Last Exam?](https://arxiv.org/abs/2507.05241)
*Jingyi Chai,Shuo Tang,Rui Ye,Yuwen Du,Xinyu Zhu,Mengcheng Zhou,Yanfeng Wang,Weinan E,Siheng Chen*

Main category: cs.AI

TL;DR: X-Master是一种工具增强的推理代理，通过灵活使用外部工具模拟人类研究者的推理过程，在HLE上取得了32.1%的领先成绩。


<details>
  <summary>Details</summary>
Motivation: 利用AI代理加速科学发现，需要评估其在人类知识前沿的能力，HLE为此提供了挑战性基准。

Method: 提出X-Master代理，以代码为交互语言，结合Python库和定制工具增强推理，并通过X-Masters工作流扩展能力。

Result: X-Masters在HLE上以32.1%的成绩创下新纪录，超过OpenAI和Google的26.6%和26.9%。

Conclusion: X-Masters为复杂任务解决提供了新思路，并为未来模型训练积累了经验。

Abstract: The rapid advancements of AI agents have ignited the long-held ambition of
leveraging them to accelerate scientific discovery. Achieving this goal
requires a deep understanding of the frontiers of human knowledge. As such,
Humanity's Last Exam (HLE) provides an exceptionally challenging touchstone for
evaluating scientific AI agents. In this work, we aim to construct the
foundational architecture for general-purpose agents and validate the
capabilities through leading performance on HLE. To achieve this, we introduce
X-Master, a tool-augmented reasoning agent designed to emulate human
researchers by interacting flexibly with external tools during its reasoning
process. This agent, guided by the conceptualization of code as an interaction
language, can flexibly leverage built-in Python libraries and our customized
tools to augment the reasoning. We further scale its capabilities through
X-Masters, a scattered-and-stacked agentic workflow that systematically
enhances breadth and depth of reasoning. Our open-source solution, X-Masters,
sets a new state-of-the-art record on HLE with a score of 32.1%, surpassing
OpenAI's and Google's Deep Research (26.6% and 26.9%) and becoming the first to
exceed the 30% threshold. This work allows us to gain a deeper understanding of
complex task-solving and accumulates valuable experience that can inform future
advancements, guiding subsequent model training.

</details>


### [161] [Modeling Latent Partner Strategies for Adaptive Zero-Shot Human-Agent Collaboration](https://arxiv.org/abs/2507.05244)
*Benjamin Li,Shuyang Shi,Lucia Romero,Huao Li,Yaqi Xie,Woojun Kim,Stefanos Nikolaidis,Michael Lewis,Katia Sycara,Simon Stepputtis*

Main category: cs.AI

TL;DR: TALENTS框架通过变分自编码器和聚类学习策略空间，动态适应异构队友，在Overcooked环境中表现优于基线。


<details>
  <summary>Details</summary>
Motivation: 在异构团队（如人机协作）中，实时适应队友策略是成功的关键，尤其是在时间压力和复杂动态任务中。

Method: 使用变分自编码器学习策略潜在空间，聚类识别策略类型，训练条件合作者，并动态调整策略。

Result: 在Overcooked任务中，TALENTS优于现有基线，能有效适应陌生人类队友。

Conclusion: TALENTS框架为异构团队协作提供了有效的实时适应能力。

Abstract: In collaborative tasks, being able to adapt to your teammates is a necessary
requirement for success. When teammates are heterogeneous, such as in
human-agent teams, agents need to be able to observe, recognize, and adapt to
their human partners in real time. This becomes particularly challenging in
tasks with time pressure and complex strategic spaces where the dynamics can
change rapidly. In this work, we introduce TALENTS, a strategy-conditioned
cooperator framework that learns to represent, categorize, and adapt to a range
of partner strategies, enabling ad-hoc teamwork. Our approach utilizes a
variational autoencoder to learn a latent strategy space from trajectory data.
This latent space represents the underlying strategies that agents employ.
Subsequently, the system identifies different types of strategy by clustering
the data. Finally, a cooperator agent is trained to generate partners for each
type of strategy, conditioned on these clusters. In order to adapt to
previously unseen partners, we leverage a fixed-share regret minimization
algorithm that infers and adjusts the estimated partner strategy dynamically.
We assess our approach in a customized version of the Overcooked environment,
posing a challenging cooperative cooking task that demands strong coordination
across a wide range of possible strategies. Using an online user study, we show
that our agent outperforms current baselines when working with unfamiliar human
partners.

</details>


### [162] [When Chain of Thought is Necessary, Language Models Struggle to Evade Monitors](https://arxiv.org/abs/2507.05246)
*Scott Emmons,Erik Jenner,David K. Elson,Rif A. Saurous,Senthooran Rajamanoharan,Heng Chen,Irhum Shafkat,Rohin Shah*

Main category: cs.AI

TL;DR: 论文探讨了链式思维（CoT）监控的可靠性问题，提出了监控性的重要性，并区分了CoT的两种用途：作为合理化工具和作为计算工具。通过实验和压力测试，研究发现CoT监控虽不完美，但仍是一种有效的防御手段。


<details>
  <summary>Details</summary>
Motivation: 近期研究发现CoT监控存在‘不忠实’问题，尤其是在作为后合理化工具时。本文旨在探讨CoT在防止严重危害的运行时监控中的可靠性，并提出监控性作为关键属性。

Method: 引入了一个概念框架，区分CoT的两种用途（合理化和计算），并通过增加行为难度迫使模型暴露其推理过程。提出了压力测试的方法论指南。

Result: 研究发现模型可以学会隐藏意图，但需要大量帮助（如详细的人类策略或针对监控的迭代优化）。CoT监控虽不完美，但仍是一种有效的防御手段。

Conclusion: CoT监控虽非绝对可靠，但作为一种防御手段具有重要价值，需要持续的压力测试和主动保护。

Abstract: While chain-of-thought (CoT) monitoring is an appealing AI safety defense,
recent work on "unfaithfulness" has cast doubt on its reliability. These
findings highlight an important failure mode, particularly when CoT acts as a
post-hoc rationalization in applications like auditing for bias. However, for
the distinct problem of runtime monitoring to prevent severe harm, we argue the
key property is not faithfulness but monitorability. To this end, we introduce
a conceptual framework distinguishing CoT-as-rationalization from
CoT-as-computation. We expect that certain classes of severe harm will require
complex, multi-step reasoning that necessitates CoT-as-computation. Replicating
the experimental setups of prior work, we increase the difficulty of the bad
behavior to enforce this necessity condition; this forces the model to expose
its reasoning, making it monitorable. We then present methodology guidelines to
stress-test CoT monitoring against deliberate evasion. Applying these
guidelines, we find that models can learn to obscure their intentions, but only
when given significant help, such as detailed human-written strategies or
iterative optimization against the monitor. We conclude that, while not
infallible, CoT monitoring offers a substantial layer of defense that requires
active protection and continued stress-testing.

</details>
