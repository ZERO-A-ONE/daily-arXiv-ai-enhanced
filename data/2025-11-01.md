<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 21]
- [cs.CR](#cs.CR) [Total: 25]
- [cs.AI](#cs.AI) [Total: 45]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [Internal Vulnerabilities, External Threats: A Grounded Framework for Enterprise Open Source Risk Governance](https://arxiv.org/abs/2510.25882)
*Wenhao Yang,Minghui Zhou,Daniel Izquierdo Cortázar,Yehui Wang*

Main category: cs.SE

TL;DR: 提出了一个企业开源风险治理框架，通过"目标->威胁->漏洞->缓解"(OTVM)逻辑链，将不可控的外部威胁与可控的内部漏洞关联，帮助企业从战术风险管理转向整体风险治理。


<details>
  <summary>Details</summary>
Motivation: 传统风险管理仅关注技术工具，无法应对上游"静默修复"、社区冲突、许可证变更等系统性威胁，存在治理盲区。需要从战术风险管理转向整体风险治理。

Method: 采用扎根理论研究，访谈15位从业者，开发基于"外部威胁-内部漏洞"原则的整体风险治理框架，并通过三位行业专家的回顾性案例研究验证。

Result: 构建了包含战略目标矩阵、外部威胁(技术、社区、生态)和内部漏洞(战略、运营、技术)双重分类法、以及可操作缓解框架的完整体系。

Conclusion: 该框架为企业提供了从被动"救火"转向主动构建组织"免疫系统"的诊断工具和系统路径，填补了开源风险治理的空白。

Abstract: Enterprise engagement with open source has evolved from tactical adoption to
strategic deep integration, exposing them to a complex risk landscape far
beyond mere code. However, traditional risk management, narrowly focused on
technical tools, is structurally inadequate for systemic threats like upstream
"silent fixes", community conflicts, or sudden license changes, creating a
dangerous governance blind spot. To address this governance vacuum and enable
the necessary shift from tactical risk management to holistic risk governance,
we conducted a grounded theory study with 15 practitioners to develop a
holistic risk governance framework. Our study formalizes an analytical
framework built on a foundational risk principle: an uncontrollable External
Threat (e.g., a sudden license change in a key dependency) only becomes a
critical risk when it exploits a controllable Internal Vulnerability (e.g., an
undefined risk appetite for single-vendor projects), which then amplifies the
impact.The framework operationalizes this principle through a clear logical
chain: "Objectives -> Threats -> Vulnerabilities -> Mitigation" (OTVM). This
provides a holistic decision model that transcends mere technical checklists.
Based on this logic, our contributions are: (1) a "Strategic Objectives Matrix"
to clarify goals; (2) a systematic dual taxonomy of External Threats (Ex-Tech,
Ex-Comm, Ex-Eco) and Internal Vulnerabilities (In-Strat, In-Ops, In-Tech); and
(3) an actionable mitigation framework mapping capability-building to these
vulnerabilities. The framework's analytical utility was validated by three
industry experts through retrospective case studies on real-world incidents.
This work provides a novel diagnostic lens and a systematic path for
enterprises to shift from reactive "firefighting" to proactively building an
organizational "immune system".

</details>


### [2] [PRISM: Proof-Carrying Artifact Generation through LLM x MDE Synergy and Stratified Constraints](https://arxiv.org/abs/2510.25890)
*Tong Ma,Hui Lai,Hui Wang,Zhenhu Tian,Jizhou Wang,Haichao Wu,Yongfan Gao,Chaochao Li,Fengjie Xu,Ling Fang*

Main category: cs.SE

TL;DR: PRISM将大语言模型与模型驱动工程相结合，为安全和合规关键领域生成符合监管要求的工件和机器可检查的证据。


<details>
  <summary>Details</summary>
Motivation: 解决在安全和合规关键领域中，传统方法难以生成符合监管要求的工件和机器可检查证据的问题，减少人工修复工作量。

Method: 采用三大支柱：统一元模型整合异构模式和监管文本；集成约束模型将结构性和语义要求编译为执行工件；约束引导的可验证生成通过两层执行机制应用这些约束。

Result: 在汽车软件工程和跨境法律管辖等领域的评估显示，PRISM能生成结构有效、可审计的工件，大幅减少人工修复工作量。

Conclusion: PRISM为自动化工件生成提供了一条实用路径，具有内置保证机制，能够与现有工具链集成。

Abstract: PRISM unifies Large Language Models with Model-Driven Engineering to generate
regulator-ready artifacts and machine-checkable evidence for safety- and
compliance-critical domains. PRISM integrates three pillars: a Unified
Meta-Model (UMM) reconciles heterogeneous schemas and regulatory text into a
single semantic space; an Integrated Constraint Model (ICM) compiles structural
and semantic requirements into enforcement artifacts including generation-time
automata (GBNF, DFA) and post-generation validators (e.g., SHACL, SMT); and
Constraint-Guided Verifiable Generation (CVG) applies these through two-layer
enforcement - structural constraints drive prefix-safe decoding while
semantic/logical validation produces machine-checkable certificates. When
violations occur, PRISM performs audit-guided repair and records generation
traces for compliance review. We evaluate PRISM in automotive software
engineering (AUTOSAR) and cross-border legal jurisdiction (Brussels I bis).
PRISM produces structurally valid, auditable artifacts that integrate with
existing tooling and substantially reduce manual remediation effort, providing
a practical path toward automated artifact generation with built-in assurance.

</details>


### [3] [A Process Mining-Based System For The Analysis and Prediction of Software Development Workflows](https://arxiv.org/abs/2510.25935)
*Antía Dorado,Iván Folgueira,Sofía Martín,Gonzalo Martín,Álvaro Porto,Alejandro Ramos,John Wallace*

Main category: cs.SE

TL;DR: CodeSight是一个端到端系统，通过整合流程挖掘和机器学习来预测软件开发工作流中的截止期限合规性。


<details>
  <summary>Details</summary>
Motivation: 旨在通过主动识别潜在的截止期限违规来改进软件项目管理。

Method: 从GitHub捕获开发和部署数据，转换为流程挖掘日志进行分析，使用LSTM模型基于序列活动轨迹和静态特征预测PR剩余解决时间。

Result: 系统在预测截止期限合规性方面表现出高精度和高F1分数。

Conclusion: 将流程挖掘与机器学习相结合对于主动软件项目管理具有重要价值。

Abstract: CodeSight is an end-to-end system designed to anticipate deadline compliance
in software development workflows. It captures development and deployment data
directly from GitHub, transforming it into process mining logs for detailed
analysis. From these logs, the system generates metrics and dashboards that
provide actionable insights into PR activity patterns and workflow efficiency.
Building on this structured representation, CodeSight employs an LSTM model
that predicts remaining PR resolution times based on sequential activity traces
and static features, enabling early identification of potential deadline
breaches. In tests, the system demonstrates high precision and F1 scores in
predicting deadline compliance, illustrating the value of integrating process
mining with machine learning for proactive software project management.

</details>


### [4] [Beyond Synthetic Benchmarks: Evaluating LLM Performance on Real-World Class-Level Code Generation](https://arxiv.org/abs/2510.26130)
*Musfiqur Rahman,SayedHassan Khatoonabadi,Emad Shihab*

Main category: cs.SE

TL;DR: LLMs在函数级代码生成表现出色，但在真实软件项目的类级实现中表现不佳，正确率从合成基准的84-89%降至真实场景的25-34%。检索增强生成在部分文档情况下最有效，错误分析显示类型和属性错误是主要失败模式。


<details>
  <summary>Details</summary>
Motivation: 评估LLMs在真实软件项目中生成类级代码的能力，了解其在实践条件下的泛化性能，弥补现有合成基准与实际应用之间的差距。

Method: 从开源仓库构建新基准，包含真实世界类，分为已见和未见分区。评估多种LLM在不同输入规范、检索增强配置和文档完整性水平下的表现。

Result: LLMs在真实类任务中正确率仅为25-34%，远低于合成基准的84-89%。完整文档仅带来1-3%的微小提升，检索增强在部分文档情况下能提高4-7%的正确率。主要错误类型为AttributeError、TypeError和AssertionError。

Conclusion: 当前LLM在类级工程能力存在严重局限，需要改进上下文建模、文档策略和检索集成，为生产代码辅助工具提供可操作的改进方向。

Abstract: Large language models (LLMs) have advanced code generation at the function
level, yet their ability to produce correct class-level implementations in
authentic software projects remains poorly understood. This work introduces a
novel benchmark derived from open-source repositories, comprising real-world
classes divided into seen and unseen partitions to evaluate generalization
under practical conditions. The evaluation examines multiple LLMs under varied
input specifications, retrieval-augmented configurations, and documentation
completeness levels.
  Results reveal a stark performance disparity: LLMs achieve 84% to 89%
correctness on established synthetic benchmarks but only 25% to 34% on
real-world class tasks, with negligible differences between familiar and novel
codebases. Comprehensive docstrings yield modest gains of 1% to 3% in
functional accuracy, though statistical significance is rare.
Retrieval-augmented generation proves most effective with partial
documentation, improving correctness by 4% to 7% by supplying concrete
implementation patterns absent from specifications. Error profiling identifies
AttributeError, TypeError, and AssertionError as dominant failure modes (84% of
cases), with synthetic tests overemphasizing assertion issues and real-world
scenarios highlighting type and attribute mismatches. Retrieval augmentation
reduces logical flaws but can introduce dependency conflicts.
  The benchmark and analysis expose critical limitations in current LLM
capabilities for class-level engineering, offering actionable insights for
enhancing context modelling, documentation strategies, and retrieval
integration in production code assistance tools.

</details>


### [5] [Reduction of Test Re-runs by Prioritizing Potential Order Dependent Flaky Tests](https://arxiv.org/abs/2510.26171)
*Hasnain Iqbal,Zerina Begum,Kazi Sakib*

Main category: cs.SE

TL;DR: 提出一种基于共享静态字段分析的方法来优先排序潜在的顺序依赖测试，以减少测试执行次数和不必要的重运行。


<details>
  <summary>Details</summary>
Motivation: 顺序依赖测试会导致持续集成管道失败，但现有检测方法需要多次测试重运行，效率低下。需要优先识别潜在的顺序依赖测试来减少重运行成本。

Method: 通过分析测试类中的共享静态字段来识别更可能具有顺序依赖性的测试。

Result: 在27个项目模块的实验中，成功在23个案例中优先排序所有顺序依赖测试，平均减少65.92%的测试执行和72.19%的不必要重运行。

Conclusion: 该方法通过显著降低执行成本，有效提高了顺序依赖测试检测的效率。

Abstract: Flaky tests can make automated software testing unreliable due to their
unpredictable behavior. These tests can pass or fail on the same code base on
multiple runs. However, flaky tests often do not refer to any fault, even
though they can cause the continuous integration (CI) pipeline to fail. A
common type of flaky test is the order-dependent (OD) test. The outcome of an
OD test depends on the order in which it is run with respect to other test
cases. Several studies have explored the detection and repair of OD tests.
However, their methods require re-runs of tests multiple times, that are not
related to the order dependence. Hence, prioritizing potential OD tests is
necessary to reduce the re-runs. In this paper, we propose a method to
prioritize potential order-dependent tests. By analyzing shared static fields
in test classes, we identify tests that are more likely to be order-dependent.
In our experiment on 27 project modules, our method successfully prioritized
all OD tests in 23 cases, reducing test executions by an average of 65.92% and
unnecessary re-runs by 72.19%. These results demonstrate that our approach
significantly improves the efficiency of OD test detection by lowering
execution costs.

</details>


### [6] [The "4W+1H" of Software Supply Chain Security Checklist for Critical Infrastructure](https://arxiv.org/abs/2510.26174)
*Liming Dong,Sung Une Lee,Zhenchang Xing,Muhammad Ejaz Ahmed,Stefan Avgoustakis*

Main category: cs.SE

TL;DR: 该论文通过多源文献综述分析了软件供应链安全实践，特别关注关键基础设施领域，提出了包含80个问题的结构化检查清单来评估和增强软件供应链安全。


<details>
  <summary>Details</summary>
Motivation: 软件供应链攻击日益频繁和复杂，对关键基础设施构成严重威胁，而现有安全实践分散且不足，缺乏针对关键基础设施领域的专门框架。

Method: 采用多源文献综述方法，分析国际框架、澳大利亚监管来源和学术研究，使用"4W+1H"分析方法综合软件供应链安全实践。

Result: 发现现有框架很少专门针对关键基础设施领域，识别出10个核心安全实践类别，并构建了包含80个问题的多层级检查清单。

Conclusion: 现有框架指导与行业特定需求存在差距，需要采用集成、情境感知的方法来保护关键基础设施免受软件供应链风险威胁。

Abstract: The increasing frequency and sophistication of software supply chain attacks
pose severe risks to critical infrastructure sectors, threatening national
security, economic stability, and public safety. Despite growing awareness,
existing security practices remain fragmented and insufficient, with most
frameworks narrowly focused on isolated life cycle stages or lacking alignment
with the specific needs of critical infrastructure (CI) sectors. In this paper,
we conducted a multivocal literature review across international frameworks,
Australian regulatory sources, and academic studies to identify and analyze
security practices across the software supply chain, especially specific CI
sector. Our analysis found that few existing frameworks are explicitly tailored
to CI domains. We systematically leveraged identified software supply chain
security frameworks, using a "4W+1H" analytical approach, we synthesized ten
core categories (what) of software supply chain security practices, mapped them
across life-cycle phases (when), stakeholder roles (who), and implementation
levels (how), and examined their coverage across existing frameworks (where).
Building on these insights, the paper culminates in structured, multi-layered
checklist of 80 questions designed to relevant stakeholders evaluate and
enhance their software supply chain security. Our findings reveal gaps between
framework guidance and sector-specific needs, highlight the need for
integrated, context-aware approaches to safeguard critical infrastructure from
evolving software supply chain risks.

</details>


### [7] [A Research Roadmap for Augmenting Software Engineering Processes and Software Products with Generative AI](https://arxiv.org/abs/2510.26275)
*Domenico Amalfitano,Andreas Metzger,Marco Autili,Tommaso Fulcini,Tobias Hey,Jan Keim,Patrizio Pelliccione,Vincenzo Scotti,Anne Koziolek,Raffaela Mirandola,Andreas Vogelsang*

Main category: cs.SE

TL;DR: 本文应用设计科学研究方法构建了GenAI增强软件工程的路线图，通过三轮循环整合多方证据，识别了四种GenAI增强形式及其相关研究挑战，并提出了2030年软件工程的十个预测。


<details>
  <summary>Details</summary>
Motivation: 生成式AI正在快速改变软件工程实践，影响软件流程执行和系统开发运营方式，需要系统性地构建GenAI增强软件工程的路线图。

Method: 采用设计科学研究方法，通过三轮循环整合FSE 2025研讨会讨论、快速文献综述和同行反馈，使用McLuhan四元组作为概念工具系统分析GenAI对软件工程的影响。

Result: 识别了四种GenAI增强软件工程的基本形式，系统描述了相关研究挑战和机遇，并整合成未来研究方向，通过独立团队和同行交叉验证确保了透明性和可重复性。

Conclusion: 研究为分析GenAI如何影响软件工程流程、方法和工具提供了坚实基础，并为这个快速演进领域的研究框架提供了支持，最终提出了2030年软件工程的十个预测。

Abstract: Generative AI (GenAI) is rapidly transforming software engineering (SE)
practices, influencing how SE processes are executed, as well as how software
systems are developed, operated, and evolved. This paper applies design science
research to build a roadmap for GenAI-augmented SE. The process consists of
three cycles that incrementally integrate multiple sources of evidence,
including collaborative discussions from the FSE 2025 "Software Engineering
2030" workshop, rapid literature reviews, and external feedback sessions
involving peers. McLuhan's tetrads were used as a conceptual instrument to
systematically capture the transforming effects of GenAI on SE processes and
software products.The resulting roadmap identifies four fundamental forms of
GenAI augmentation in SE and systematically characterizes their related
research challenges and opportunities. These insights are then consolidated
into a set of future research directions. By grounding the roadmap in a
rigorous multi-cycle process and cross-validating it among independent author
teams and peers, the study provides a transparent and reproducible foundation
for analyzing how GenAI affects SE processes, methods and tools, and for
framing future research within this rapidly evolving area. Based on these
findings, the article finally makes ten predictions for SE in the year 2030.

</details>


### [8] [Empowering RepoQA-Agent based on Reinforcement Learning Driven by Monte-carlo Tree Search](https://arxiv.org/abs/2510.26287)
*Guochang Li,Yuchen Liu,Zhen Qin,Yunkun Wang,Jianping Zhong,Chen Zhi,Binhua Li,Fei Huang,Yongbin Li,Shuiguang Deng*

Main category: cs.SE

TL;DR: RepoSearch-R1是一个基于蒙特卡洛树搜索的强化学习框架，用于解决仓库级软件工程任务中的代码导航和信息提取问题，无需模型蒸馏或外部监督。


<details>
  <summary>Details</summary>
Motivation: 现有方法在仓库级软件工程任务中存在局限：无训练方法难以有效指导工具使用和决策，基于训练的方法依赖成本高昂的模型蒸馏且存在企业数据合规问题。

Method: 提出RepoSearch-R1框架，使用蒙特卡洛树搜索驱动的强化学习，通过自训练生成多样化的高质量推理轨迹，无需模型蒸馏或外部监督。

Result: 在仓库问答任务评估中，RepoSearch-R1相比无检索方法提升16.0%答案完整性，比迭代检索方法提升19.5%，训练效率比通用强化学习方法提高33%。

Conclusion: 冷启动训练方法消除了数据合规问题，同时在仓库级推理任务中保持了强大的探索多样性和答案完整性。

Abstract: Repository-level software engineering tasks require large language models
(LLMs) to efficiently navigate and extract information from complex codebases
through multi-turn tool interactions. Existing approaches face significant
limitations: training-free, in-context learning methods struggle to guide
agents effectively in tool utilization and decision-making based on
environmental feedback, while training-based approaches typically rely on
costly distillation from larger LLMs, introducing data compliance concerns in
enterprise environments. To address these challenges, we introduce
RepoSearch-R1, a novel agentic reinforcement learning framework driven by
Monte-carlo Tree Search (MCTS). This approach allows agents to generate
diverse, high-quality reasoning trajectories via self-training without
requiring model distillation or external supervision. Based on RepoSearch-R1,
we construct a RepoQA-Agent specifically designed for repository
question-answering tasks. Comprehensive evaluation on repository
question-answering tasks demonstrates that RepoSearch-R1 achieves substantial
improvements of answer completeness: 16.0% enhancement over no-retrieval
methods, 19.5% improvement over iterative retrieval methods, and 33% increase
in training efficiency compared to general agentic reinforcement learning
approaches. Our cold-start training methodology eliminates data compliance
concerns while maintaining robust exploration diversity and answer completeness
across repository-level reasoning tasks.

</details>


### [9] [Environmental Impact of CI/CD Pipelines](https://arxiv.org/abs/2510.26413)
*Nuno Saavedra,Alexandra Mendes,João F. Ferreira*

Main category: cs.SE

TL;DR: 该研究分析了GitHub Actions CI/CD服务的碳足迹和水足迹，发现其环境影响巨大，并提出通过减少计算资源浪费、选择环保地区部署等策略来缓解影响。


<details>
  <summary>Details</summary>
Motivation: 随着云计算环境影响日益增长，了解CI/CD服务的碳足迹和水足迹变得愈发重要，但目前服务提供商通常不披露此类信息。

Method: 基于Cloud Carbon Footprint框架方法，使用文献中最大的工作流运行数据集，包含超过220万次工作流运行和18,000多个仓库。

Result: GitHub Actions生态系统产生显著的碳足迹和水足迹，最可能情景下碳足迹为456.9 MTCO2e，水足迹为5,738.2千升，相当于7,615棵城市树木一年的碳吸收量或一个美国家庭5,053年的用水量。

Conclusion: 建议通过减少计算资源浪费、在环保地区部署运行器、优化调度执行时间以及减小仓库大小等策略来缓解环境影响。

Abstract: CI/CD pipelines are widely used in software development, yet their
environmental impact, particularly carbon and water footprints (CWF), remains
largely unknown to developers, as CI service providers typically do not
disclose such information. With the growing environmental impact of cloud
computing, understanding the CWF of CI/CD services has become increasingly
important.
  This work investigates the CWF of using GitHub Actions, focusing on
open-source repositories where usage is free and unlimited for standard
runners. We build upon a methodology from the Cloud Carbon Footprint framework
and we use the largest dataset of workflow runs reported in the literature to
date, comprising over 2.2 million workflow runs from more than 18,000
repositories.
  Our analysis reveals that the GitHub Actions ecosystem results in a
substantial CWF. Our estimates for the carbon footprint in 2024 range from
150.5 MTCO2e in the most optimistic scenario to 994.9 MTCO2e in the most
pessimistic scenario, while the water footprint ranges from 1,989.6 to 37,664.5
kiloliters. The most likely scenario estimates are 456.9 MTCO2e for carbon
footprint and 5,738.2 kiloliters for water footprint. To provide perspective,
the carbon footprint in the most likely scenario is equivalent to the carbon
captured by 7,615 urban trees in a year, and the water footprint is comparable
to the water consumed by an average American family over 5,053 years.
  We explore strategies to mitigate this impact, primarily by reducing wasted
computational resources. Key recommendations include deploying runners in
regions whose energy production has a low environmental impact such as France
and the United Kingdom, implementing stricter deactivation policies for
scheduled runs and aligning their execution with periods when the regional
energy mix is more environmentally favorable, and reducing the size of
repositories.

</details>


### [10] [Nexus: Execution-Grounded Multi-Agent Test Oracle Synthesis](https://arxiv.org/abs/2510.26423)
*Dong Huang,Mingzhe Du,Jie M. Zhang,Zheng Lin,Meng Luo,Qianru Zhang,See-Kiong Ng*

Main category: cs.SE

TL;DR: Nexus是一个新颖的多智能体框架，通过专门化智能体的协作、验证和自精炼过程来生成测试预言，在非回归测试中显著提升了测试预言生成的质量和下游任务的性能。


<details>
  <summary>Details</summary>
Motivation: 解决软件工程中长期存在的测试预言生成挑战，目标是生成能够准确判断被测函数在给定输入下是否按预期行为的测试预言。

Method: 采用多智能体框架，包含四个具有不同测试理念的专门化智能体，通过审议、验证和迭代自精炼三个结构化阶段协作生成测试预言。在验证阶段生成被测函数的候选实现并在安全沙箱中执行测试预言，对失败的预言启动自动调试和修正循环。

Result: 在七个多样化基准测试上的评估表明，Nexus始终显著优于最先进的基线方法。例如，在LiveCodeBench上将GPT-4.1-Mini的测试级预言准确率从46.30%提升到57.73%，在HumanEval上错误检测率从90.91%提升到95.45%，自动程序修复成功率从35.23%提升到69.32%。

Conclusion: Nexus框架通过多智能体协作和结构化验证过程，有效解决了测试预言生成的核心挑战，显著提升了测试预言的质量和相关下游任务的性能。

Abstract: Test oracle generation in non-regression testing is a longstanding challenge
in software engineering, where the goal is to produce oracles that can
accurately determine whether a function under test (FUT) behaves as intended
for a given input. In this paper, we introduce Nexus, a novel multi-agent
framework to address this challenge. Nexus generates test oracles by leveraging
a diverse set of specialized agents that synthesize test oracles through a
structured process of deliberation, validation, and iterative self-refinement.
During the deliberation phase, a panel of four specialist agents, each
embodying a distinct testing philosophy, collaboratively critiques and refines
an initial set of test oracles. Then, in the validation phase, Nexus generates
a plausible candidate implementation of the FUT and executes the proposed
oracles against it in a secure sandbox. For any oracle that fails this
execution-based check, Nexus activates an automated selfrefinement loop, using
the specific runtime error to debug and correct the oracle before
re-validation. Our extensive evaluation on seven diverse benchmarks
demonstrates that Nexus consistently and substantially outperforms
state-of-theart baselines. For instance, Nexus improves the test-level oracle
accuracy on the LiveCodeBench from 46.30% to 57.73% for GPT-4.1-Mini. The
improved accuracy also significantly enhances downstream tasks: the bug
detection rate of GPT4.1-Mini generated test oracles on HumanEval increases
from 90.91% to 95.45% for Nexus compared to baselines, and the success rate of
automated program repair improves from 35.23% to 69.32%.

</details>


### [11] [CHCVerif: A Portfolio-Based Solver for Constrained Horn Clauses](https://arxiv.org/abs/2510.26431)
*Mihály Dobos-Kovács,Levente Bajczi,András Vörös*

Main category: cs.SE

TL;DR: CHCVERIF是一个基于组合策略的CHC求解器，采用软件验证方法解决CHC问题，能够重用成熟的软件验证工具处理涉及位向量和低级语义的CHC基准测试。


<details>
  <summary>Details</summary>
Motivation: CHC被广泛用作各种验证任务的中间表示，包括安全检查、不变量合成和过程间分析。作者希望探索使用软件验证工具作为CHC求解后端的新方法。

Method: 开发了CHCVERIF，这是一个基于组合策略的CHC求解器，采用软件验证方法，重用成熟的软件验证工具来处理CHC基准测试，特别是涉及位向量和低级语义的测试。

Result: 在线性整数算术方面表现一般，在位向量基准测试上取得了适度成功。结果表明使用软件验证工具作为CHC求解后端是可行且有潜力的，特别是在精心构建的组合策略支持下。

Conclusion: 使用软件验证工具作为CHC求解后端具有可行性和潜力，特别是在处理涉及位向量和低级语义的CHC问题时，通过精心设计的组合策略可以获得良好效果。

Abstract: Constrained Horn Clauses (CHCs) are widely adopted as intermediate
representations for a variety of verification tasks, including safety checking,
invariant synthesis, and interprocedural analysis. This paper introduces
CHCVERIF, a portfolio-based CHC solver that adopts a software verification
approach for solving CHCs. This approach enables us to reuse mature software
verification tools to tackle CHC benchmarks, particularly those involving
bitvectors and low-level semantics. Our evaluation shows that while the method
enjoys only moderate success with linear integer arithmetic, it achieves modest
success on bitvector benchmarks. Moreover, our results demonstrate the
viability and potential of using software verification tools as backends for
CHC solving, particularly when supported by a carefully constructed portfolio.

</details>


### [12] [SecureReviewer: Enhancing Large Language Models for Secure Code Review through Secure-aware Fine-tuning](https://arxiv.org/abs/2510.26457)
*Fang Liu,Simiao Liu,Yinghao Zhu,Xiaoli Lian,Li Zhang*

Main category: cs.SE

TL;DR: SecureReviewer是一个专门用于增强LLM在代码审查中识别和解决安全问题的能力的新方法，通过构建专用数据集、安全感知微调策略和RAG技术，显著提升了安全代码审查的效果。


<details>
  <summary>Details</summary>
Motivation: 现有自动化代码审查方法主要关注通用目的审查，在识别和解决安全相关问题方面的效果尚未充分探索，且面临数据稀缺和评估指标不足的挑战。

Method: 构建安全代码审查专用数据集，采用安全感知微调策略训练LLM，集成RAG技术以减少幻觉并增强输出可靠性，同时引入SecureBLEU评估指标。

Result: 实验结果表明，SecureReviewer在安全漏洞检测准确性和生成审查评论的整体质量及实用性方面均优于现有最先进基线方法。

Conclusion: SecureReviewer通过专门的数据集、微调策略和评估指标，有效提升了LLM在安全代码审查中的能力，为解决软件开发早期阶段的安全问题提供了有力工具。

Abstract: Identifying and addressing security issues during the early phase of the
development lifecycle is critical for mitigating the long-term negative impacts
on software systems. Code review serves as an effective practice that enables
developers to check their teammates' code before integration into the codebase.
To streamline the generation of review comments, various automated code review
approaches have been proposed, where LLM-based methods have significantly
advanced the capabilities of automated review generation. However, existing
models primarily focus on general-purpose code review, their effectiveness in
identifying and addressing security-related issues remains underexplored.
Moreover, adapting existing code review approaches to target security issues
faces substantial challenges, including data scarcity and inadequate evaluation
metrics. To address these limitations, we propose SecureReviewer, a new
approach designed for enhancing LLMs' ability to identify and resolve
security-related issues during code review. Specifically, we first construct a
dataset tailored for training and evaluating secure code review capabilities.
Leveraging this dataset, we fine-tune LLMs to generate code review comments
that can effectively identify security issues and provide fix suggestions with
our proposed secure-aware fine-tuning strategy. To mitigate hallucination in
LLMs and enhance the reliability of their outputs, we integrate the RAG
technique, which grounds the generated comments in domain-specific security
knowledge. Additionally, we introduce SecureBLEU, a new evaluation metric
designed to assess the effectiveness of review comments in addressing security
issues. Experimental results demonstrate that SecureReviewer outperforms
state-of-the-art baselines in both security issue detection accuracy and the
overall quality and practical utility of generated review comments.

</details>


### [13] [Automated Extract Method Refactoring with Open-Source LLMs: A Comparative Study](https://arxiv.org/abs/2510.26480)
*Sivajeet Chand,Melih Kilic,Roland Würsching,Sushant Kumar Pandey,Alexander Pretschner*

Main category: cs.SE

TL;DR: 评估5个开源LLM在Python代码提取方法重构任务上的表现，发现RCI提示策略优于单次提示，最佳模型在测试通过率和代码质量方面表现优异，开发者调查显示70%以上接受度。


<details>
  <summary>Details</summary>
Motivation: 自动化提取方法重构(EMR)仍然具有挑战性且主要依赖人工操作，尽管其对提高代码可读性和可维护性很重要。开源资源高效的LLMs为自动化此类高级任务提供了新方法。

Method: 系统评估5个最先进的开源LLM（参数规模3B到8B），在Python代码EMR任务上使用自动指标评估功能正确性和代码质量，比较单次提示与递归批评改进(RCI)提示策略的影响。

Result: RCI提示在测试通过率和重构质量方面持续优于单次提示。最佳模型Deepseek-Coder-RCI和Qwen2.5-Coder-RCI分别达到0.829和0.808的测试通过率，将每方法代码行数从12.103减少到6.192和5.577，圈复杂度从4.602降低到3.453和3.294。开发者调查显示超过70%接受RCI生成的重构。

Conclusion: 虽然传统指标如圈复杂度和代码行数提供有用信号，但它们经常与人类判断不一致，强调需要人在环评估。开源基准为未来使用LLMs进行自动化重构研究提供了基础。

Abstract: Automating the Extract Method refactoring (EMR) remains challenging and
largely manual despite its importance in improving code readability and
maintainability. Recent advances in open-source, resource-efficient Large
Language Models (LLMs) offer promising new approaches for automating such
high-level tasks. In this work, we critically evaluate five state-of-the-art
open-source LLMs, spanning 3B to 8B parameter sizes, on the EMR task for Python
code. We systematically assess functional correctness and code quality using
automated metrics and investigate the impact of prompting strategies by
comparing one-shot prompting to a Recursive criticism and improvement (RCI)
approach. RCI-based prompting consistently outperforms one-shot prompting in
test pass rates and refactoring quality. The best-performing models,
Deepseek-Coder-RCI and Qwen2.5-Coder-RCI, achieve test pass percentage (TPP)
scores of 0.829 and 0.808, while reducing lines of code (LOC) per method from
12.103 to 6.192 and 5.577, and cyclomatic complexity (CC) from 4.602 to 3.453
and 3.294, respectively. A developer survey on RCI-generated refactorings shows
over 70% acceptance, with Qwen2.5-Coder rated highest across all evaluation
criteria. In contrast, the original code scored below neutral, particularly in
readability and maintainability, underscoring the benefits of automated
refactoring guided by quality prompts. While traditional metrics like CC and
LOC provide useful signals, they often diverge from human judgments,
emphasizing the need for human-in-the-loop evaluation. Our open-source
benchmark offers a foundation for future research on automated refactoring with
LLMs.

</details>


### [14] [Envisioning Future Interactive Web Development: Editing Webpage with Natural Language](https://arxiv.org/abs/2510.26516)
*Truong Hai Dang,Jingyu Xiao,Yintong Huo*

Main category: cs.SE

TL;DR: 提出了一种自动化数据生成管道Instruct4Edit，用于训练LLMs进行网页代码编辑，通过微调开源模型实现了与专有系统相竞争的性能。


<details>
  <summary>Details</summary>
Motivation: 网页应用开发需要频繁修改代码，传统手动方式耗时且LLMs在根据新设计需求编辑现有代码方面存在挑战，主要缺乏大规模高质量的训练数据。

Method: 开发自动化数据生成管道，使用LLMs合成高质量微调数据集，包括生成多样指令、应用相应代码修改并进行视觉验证确保正确性。

Result: 在Instruct4Edit上微调的模型在将人类意图转化为精确、结构连贯且视觉准确的代码更改方面表现出一致的改进。

Conclusion: 为基于自然语言的网页编辑提供了可扩展和透明的基础，证明微调较小的开源模型可以达到与专有系统相竞争的性能。

Abstract: The evolution of web applications relies on iterative code modifications, a
process that is traditionally manual and time-consuming. While Large Language
Models (LLMs) can generate UI code, their ability to edit existing code from
new design requirements (e.g., "center the logo") remains a challenge. This is
largely due to the absence of large-scale, high-quality tuning data to align
model performance with human expectations. In this paper, we introduce a novel,
automated data generation pipeline that uses LLMs to synthesize a high-quality
fine-tuning dataset for web editing, named Instruct4Edit. Our approach
generates diverse instructions, applies the corresponding code modifications,
and performs visual verification to ensure correctness. By fine-tuning models
on Instruct4Edit, we demonstrate consistent improvement in translating human
intent into precise, structurally coherent, and visually accurate code changes.
This work provides a scalable and transparent foundation for natural language
based web editing, demonstrating that fine-tuning smaller open-source models
can achieve competitive performance with proprietary systems. We release all
data, code implementations, and model checkpoints for reproduction.

</details>


### [15] [Reflecting on Empirical and Sustainability Aspects of Software Engineering Research in the Era of Large Language Models](https://arxiv.org/abs/2510.26538)
*David Williams,Max Hort,Maria Kechagia,Aldeida Aleti,Justyna Petke,Federica Sarro*

Main category: cs.SE

TL;DR: 本文分析了软件工程研究中使用大语言模型带来的挑战，包括基准测试严谨性、污染问题、可复现性和可持续性，并提出了改进建议。


<details>
  <summary>Details</summary>
Motivation: 软件工程研究中使用大语言模型引入了基准测试严谨性、污染、可复现性和可持续性等新挑战，需要研究社区共同反思和解决这些问题。

Method: 通过分析ICSE会议上基于LLM的软件工程研究，提供结构化概述，识别良好实践和持续存在的不足。

Result: 研究结果揭示了当前LLM-based SE研究的现状，既发现了令人鼓舞的实践，也发现了持续存在的缺陷。

Conclusion: 提出了加强基准测试严谨性、提高可复现性以及解决基于LLM的软件工程研究的财务和环境成本的建议。

Abstract: Software Engineering (SE) research involving the use of Large Language Models
(LLMs) has introduced several new challenges related to rigour in benchmarking,
contamination, replicability, and sustainability. In this paper, we invite the
research community to reflect on how these challenges are addressed in SE. Our
results provide a structured overview of current LLM-based SE research at ICSE,
highlighting both encouraging practices and persistent shortcomings. We
conclude with recommendations to strengthen benchmarking rigour, improve
replicability, and address the financial and environmental costs of LLM-based
SE.

</details>


### [16] ["Show Me You Comply... Without Showing Me Anything": Zero-Knowledge Software Auditing for AI-Enabled Systems](https://arxiv.org/abs/2510.26576)
*Filippo Scaramuzza,Renato Cordeiro Ferreira,Tomaz Maia Suller,Giovanni Quattrocchi,Damian Andrew Tamburri,Willem-Jan van den Heuvel*

Main category: cs.SE

TL;DR: ZKMLOps是一个基于零知识证明的MLOps验证框架，通过密码学协议在不泄露敏感信息的情况下验证AI系统的合规性，解决了AI审计中的透明度与隐私保护冲突。


<details>
  <summary>Details</summary>
Motivation: AI系统在关键领域应用日益增多，但传统验证方法成本高、手动操作多，且不适用于AI模型的'黑盒'特性。监管要求高可审计性，但这与保护机密数据和专有模型的需求相冲突。

Method: 将零知识证明集成到机器学习运维生命周期中，采用模块化可重复流程生成可验证的密码学合规证明，结合成熟的软件工程模式。

Result: 通过金融风险审计的监管合规案例研究评估了实用性，并通过实证评估不同ZKP协议的性能权衡，分析了复杂度递增的ML模型。

Conclusion: ZKMLOps框架为解决AI系统审计中的透明度与隐私保护冲突提供了可行的技术方案，能够在保护敏感信息的同时实现可验证的合规性。

Abstract: The increasing exploitation of Artificial Intelligence (AI) enabled systems
in critical domains has made trustworthiness concerns a paramount showstopper,
requiring verifiable accountability, often by regulation (e.g., the EU AI Act).
Classical software verification and validation techniques, such as procedural
audits, formal methods, or model documentation, are the mechanisms used to
achieve this. However, these methods are either expensive or heavily manual and
ill-suited for the opaque, "black box" nature of most AI models. An intractable
conflict emerges: high auditability and verifiability are required by law, but
such transparency conflicts with the need to protect assets being audited-e.g.,
confidential data and proprietary models-leading to weakened accountability. To
address this challenge, this paper introduces ZKMLOps, a novel MLOps
verification framework that operationalizes Zero-Knowledge Proofs
(ZKPs)-cryptographic protocols allowing a prover to convince a verifier that a
statement is true without revealing additional information-within
Machine-Learning Operations lifecycles. By integrating ZKPs with established
software engineering patterns, ZKMLOps provides a modular and repeatable
process for generating verifiable cryptographic proof of compliance. We
evaluate the framework's practicality through a study of regulatory compliance
in financial risk auditing and assess feasibility through an empirical
evaluation of top ZKP protocols, analyzing performance trade-offs for ML models
of increasing complexity.

</details>


### [17] [Online and Interactive Bayesian Inference Debugging](https://arxiv.org/abs/2510.26579)
*Nathanael Nussbaumer,Markus Böck,Jürgen Cito*

Main category: cs.SE

TL;DR: 提出了一种新的贝叶斯推理调试方法，显著减少了调试时间和所需知识，通过在线交互式调试框架直接在开发环境中实现。


<details>
  <summary>Details</summary>
Motivation: 概率编程虽然促进了贝叶斯模型的开发和推理，但调试推理过程非常困难，需要大量时间和专业知识。

Method: 提出了满足贝叶斯推理调试框架关键要求的新工具，在开发环境中实现在线交互式调试。

Result: 在18名有经验参与者的研究中，该方法显著减少了推理调试任务的时间和难度。

Conclusion: 该在线交互式贝叶斯推理调试方法有效解决了概率编程中的调试难题。

Abstract: Probabilistic programming is a rapidly developing programming paradigm which
enables the formulation of Bayesian models as programs and the automation of
posterior inference. It facilitates the development of models and conducting
Bayesian inference, which makes these techniques available to practitioners
from multiple fields. Nevertheless, probabilistic programming is notoriously
difficult as identifying and repairing issues with inference requires a lot of
time and deep knowledge. Through this work, we introduce a novel approach to
debugging Bayesian inference that reduces time and required knowledge
significantly. We discuss several requirements a Bayesian inference debugging
framework has to fulfill, and propose a new tool that meets these key
requirements directly within the development environment. We evaluate our
results in a study with 18 experienced participants and show that our approach
to online and interactive debugging of Bayesian inference significantly reduces
time and difficulty on inference debugging tasks.

</details>


### [18] [Stitch: Step-by-step LLM Guided Tutoring for Scratch](https://arxiv.org/abs/2510.26634)
*Yuan Si,Kyle Qi,Daming Li,Hanyuan Shi,Jialu Zhang*

Main category: cs.SE

TL;DR: Stitch是一个交互式编程辅导系统，通过逐步脚手架式指导代替直接展示正确答案，帮助学生在Scratch等积木编程环境中调试语义错误。


<details>
  <summary>Details</summary>
Motivation: 现有调试工作流通常直接向学习者展示正确程序，虽然能修复错误但削弱了问题解决能力的培养。需要一种更有效的教学方法来提升学习效果。

Method: 系统包含Diff-Analyze模块，对比学生项目与参考实现，识别关键差异，使用大语言模型解释这些变化的重要性。学生通过自定义渲染引擎检查高亮块，理解解释并选择性应用部分修复，这是一个迭代过程。

Result: 与最先进的Scratch自动反馈生成工具相比，Stitch的逐步指导系统提供了更有效的学习体验，显著提升了学习成果。

Conclusion: 直接展示正确答案在教学方法上效果不佳，而交互式、逐步指导的系统能促进更有效的学习体验，为积木编程中有效反馈的设计提供了新证据。

Abstract: Block-based environments such as Scratch are increasingly popular in
programming education. While block syntax reduces surface errors, semantic bugs
remain common and challenging for novices to resolve. Existing debugging
workflows typically show the correct program directly to learners, a strategy
that may fix errors but undermines the development of problem-solving skills.
  We present Stitch, an interactive tutoring system that replaces "showing the
answer" with step-by-step scaffolding. The system's Diff-Analyze module
contrasts a student's project with a reference implementation, identifies the
most critical differences, and uses a large language model to explain why these
changes matter. Learners inspect highlighted blocks through a custom rendering
engine, understand the explanations, and selectively apply partial fixes. This
iterative process continues until the intended functionality is achieved.
  We evaluate Stitch in an empirical study, comparing it against a
state-of-the-art automated feedback generation tool for Scratch. Our key
insight is that simply presenting the correct program is pedagogically
ineffective. In contrast, our interactive, step-by-step guided system promotes
a more effective learning experience. More broadly, what constitutes effective
feedback in block-based programming remains an open question. Our evaluation
provides new evidence that step-by-step tutoring significantly enhances
learning outcomes, outperforming both direct-answer approaches and current
automated feedback generation tools.

</details>


### [19] [Process-based Indicators of Vulnerability Re-Introducing Code Changes: An Exploratory Case Study](https://arxiv.org/abs/2510.26676)
*Samiha Shimmi,Nicholas M. Synovic,Mona Rahimi,George K. Thiruvathukal*

Main category: cs.SE

TL;DR: 该研究通过分析ImageMagick项目中76个漏洞重新引入案例，发现过程指标（如问题腐败度和问题密度）与漏洞重新引入密切相关，强调漏洞重新引入是累积开发活动和社会技术条件的结果。


<details>
  <summary>Details</summary>
Motivation: 软件漏洞在被修复后仍会持续存在或重新出现，现有研究很少探索过程指标是否能揭示随时间推移的冒险开发活动，这对预测和缓解软件漏洞至关重要。

Method: 在ImageMagick项目上进行案例研究，将纵向过程指标（总线因子、问题密度、问题腐败度）与漏洞重新引入活动相关联，分析提交级别的安全修复，关注漏洞演化和重新出现的变更序列。

Result: 研究发现重新引入通常与增加的问题腐败度和波动的问题密度相一致，反映了问题管理和团队响应能力的短期低效。

Conclusion: 过程指标与代码指标结合能够预测冒险修复并增强软件安全性，为更广泛的研究奠定了基础。

Abstract: Software vulnerabilities often persist or re-emerge even after being fixed,
revealing the complex interplay between code evolution and socio-technical
factors. While source code metrics provide useful indicators of
vulnerabilities, software engineering process metrics can uncover patterns that
lead to their introduction. Yet few studies have explored whether process
metrics can reveal risky development activities over time -- insights that are
essential for anticipating and mitigating software vulnerabilities. This work
highlights the critical role of process metrics along with code changes in
understanding and mitigating vulnerability reintroduction. We move beyond
file-level prediction and instead analyze security fixes at the commit level,
focusing not only on whether a single fix introduces a vulnerability but also
on the longer sequences of changes through which vulnerabilities evolve and
re-emerge. Our approach emphasizes that reintroduction is rarely the result of
one isolated action, but emerges from cumulative development activities and
socio-technical conditions. To support this analysis, we conducted a case study
on the ImageMagick project by correlating longitudinal process metrics such as
bus factor, issue density, and issue spoilage with vulnerability reintroduction
activities, encompassing 76 instances of reintroduced vulnerabilities. Our
findings show that reintroductions often align with increased issue spoilage
and fluctuating issue density, reflecting short-term inefficiencies in issue
management and team responsiveness. These observations provide a foundation for
broader studies that combine process and code metrics to predict risky fixes
and strengthen software security.

</details>


### [20] [Using Copilot Agent Mode to Automate Library Migration: A Quantitative Assessment](https://arxiv.org/abs/2510.26699)
*Aylton Almeida,Laerte Xavier,Marco Tulio Valente*

Main category: cs.SE

TL;DR: 使用GitHub Copilot Agent Mode自动迁移SQLAlchemy库版本，在API迁移覆盖率达到100%的情况下，应用功能测试通过率仅为39.75%


<details>
  <summary>Details</summary>
Motivation: 软件系统更新对避免技术债务、安全漏洞和遗留系统僵化至关重要，但手动更新库和框架耗时且易错，需要探索AI自动化维护方案

Method: 使用GitHub Copilot Agent Mode作为自主AI系统，在10个客户端应用数据集上执行SQLAlchemy库的多步骤迁移工作流，并引入迁移覆盖率指标评估效果

Result: LLM代理能够成功迁移SQLAlchemy版本间的功能和API使用（迁移覆盖率中位数100%），但无法维持应用功能，导致测试通过率较低（中位数39.75%）

Conclusion: 虽然AI代理在API迁移方面表现出色，但在保持应用功能完整性方面仍有不足，需要进一步改进以确保自动化迁移的可靠性

Abstract: Keeping software systems up to date is essential to avoid technical debt,
security vulnerabilities, and the rigidity typical of legacy systems. However,
updating libraries and frameworks remains a time consuming and error-prone
process. Recent advances in Large Language Models (LLMs) and agentic coding
systems offer new opportunities for automating such maintenance tasks. In this
paper, we evaluate the update of a well-known Python library, SQLAlchemy,
across a dataset of ten client applications. For this task, we use the Github's
Copilot Agent Mode, an autonomous AI systema capable of planning and executing
multi-step migration workflows. To assess the effectiveness of the automated
migration, we also introduce Migration Coverage, a metric that quantifies the
proportion of API usage points correctly migrated. The results of our study
show that the LLM agent was capable of migrating functionalities and API usages
between SQLAlchemy versions (migration coverage: 100%, median), but failed to
maintain the application functionality, leading to a low test-pass rate
(39.75%, median).

</details>


### [21] [Optimized Log Parsing with Syntactic Modifications](https://arxiv.org/abs/2510.26793)
*Nafid Enan,Gias Uddin*

Main category: cs.SE

TL;DR: 该论文对语法和语义日志解析器进行了实证研究，发现语义方法在模板识别上更准确，语法方法效率更高但模板识别不足。基于研究结果提出了SynLog+模块，显著提升了两种解析器的准确率。


<details>
  <summary>Details</summary>
Motivation: 日志解析是自动化日志分析的关键第一步，但现有解析器采用不同技术，需要系统评估其特性和性能，以指导实践选择和改进。

Method: 通过综合实证研究比较语法和语义日志解析器，以及单阶段和两阶段解析架构的性能差异。

Result: 语义方法模板识别更准确，语法方法效率高10-1000倍且分组精度更好。两阶段架构相比单阶段持续提升准确率。提出的SynLog+模块使语法和语义解析器准确率分别提升236%和20%。

Conclusion: 两阶段解析架构能显著提升日志解析性能，SynLog+模块在不增加运行时间的情况下有效提高了现有解析器的准确率。

Abstract: Logs provide valuable insights into system runtime and assist in software
development and maintenance. Log parsing, which converts semi-structured log
data into structured log data, is often the first step in automated log
analysis. Given the wide range of log parsers utilizing diverse techniques, it
is essential to evaluate them to understand their characteristics and
performance. In this paper, we conduct a comprehensive empirical study
comparing syntax- and semantic-based log parsers, as well as single-phase and
two-phase parsing architectures. Our experiments reveal that semantic-based
methods perform better at identifying the correct templates and syntax-based
log parsers are 10 to 1,000 times more efficient and provide better grouping
accuracy although they fall short in accurate template identification.
Moreover, two-phase architecture consistently improves accuracy compared to
single-phase architecture. Based on the findings of this study, we propose
SynLog+, a template identification module that acts as the second phase in a
two-phase log parsing architecture. SynLog+ improves the parsing accuracy of
syntax-based and semantic-based log parsers by 236\% and 20\% on average,
respectively, with virtually no additional runtime cost.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [22] [Attention Augmented GNN RNN-Attention Models for Advanced Cybersecurity Intrusion Detection](https://arxiv.org/abs/2510.25802)
*Jayant Biradar,Smit Shah,Tanmay Naik*

Main category: cs.CR

TL;DR: 提出了一种结合图神经网络、循环神经网络和多头注意力机制的混合深度学习架构，显著提升了网络安全入侵检测能力，在UNSW-NB15数据集上表现出色。


<details>
  <summary>Details</summary>
Motivation: 现代网络安全需要能够同时捕捉空间依赖性和时间动态的入侵检测系统，传统方法在处理复杂攻击模式时存在局限性。

Method: 采用混合深度学习架构，结合GNN捕捉图结构关系中的空间依赖性，RNN分析网络事件的时序动态，多头注意力机制提供模型可解释性和特征选择。

Result: 在UNSW-NB15数据集上的实验表明，该混合模型在准确率、精确率、召回率和F1分数等多个评估指标上均优于传统机器学习和单一深度学习模型，特别在检测APT、DDoS和零日攻击等复杂攻击模式方面表现突出。

Conclusion: 该混合模型是复杂网络环境中下一代网络安全应用的有前景解决方案，能够有效提升入侵检测能力并优化计算资源分配。

Abstract: In this paper, we propose a novel hybrid deep learning architecture that
synergistically combines Graph Neural Networks (GNNs), Recurrent Neural
Networks (RNNs), and multi-head attention mechanisms to significantly enhance
cybersecurity intrusion detection capabilities. By leveraging the comprehensive
UNSW-NB15 dataset containing diverse network traffic patterns, our approach
effectively captures both spatial dependencies through graph structural
relationships and temporal dynamics through sequential analysis of network
events. The integrated attention mechanism provides dual benefits of improved
model interpretability and enhanced feature selection, enabling cybersecurity
analysts to focus computational resources on high-impact security events -- a
critical requirement in modern real-time intrusion detection systems. Our
extensive experimental evaluation demonstrates that the proposed hybrid model
achieves superior performance compared to traditional machine learning
approaches and standalone deep learning models across multiple evaluation
metrics, including accuracy, precision, recall, and F1-score. The model
achieves particularly strong performance in detecting sophisticated attack
patterns such as Advanced Persistent Threats (APTs), Distributed Denial of
Service (DDoS) attacks, and zero-day exploits, making it a promising solution
for next-generation cybersecurity applications in complex network environments.

</details>


### [23] [APThreatHunter: An automated planning-based threat hunting framework](https://arxiv.org/abs/2510.25806)
*Mustafa F. Abdelwahed,Ahmed Shafee,Joan Espasa*

Main category: cs.CR

TL;DR: APThreatHunter是一个自动化威胁狩猎解决方案，通过自动生成假设来减少人工干预，消除分析师偏见，并降低时间和成本。


<details>
  <summary>Details</summary>
Motivation: 网络攻击威胁经济利益、关键基础设施和公共健康安全。现有的网络威胁狩猎需要手动创建和确认假设，过程耗时且存在偏见。

Method: APThreatHunter基于系统当前状态和一组指标，自动生成可能的威胁假设，并指示检测到的风险是否正在发生。

Result: 使用真实世界Android恶意软件样本进行评估，结果显示自动化规划在威胁狩猎活动中生成目标假设具有实用性。

Conclusion: 自动化规划可以有效地用于网络威胁狩猎中的目标假设生成，提高了威胁检测的效率和客观性。

Abstract: Cyber attacks threaten economic interests, critical infrastructure, and
public health and safety. To counter this, entities adopt cyber threat hunting,
a proactive approach that involves formulating hypotheses and searching for
attack patterns within organisational networks. Automating cyber threat hunting
presents challenges, particularly in generating hypotheses, as it is a manually
created and confirmed process, making it time-consuming. To address these
challenges, we introduce APThreatHunter, an automated threat hunting solution
that generates hypotheses with minimal human intervention, eliminating analyst
bias and reducing time and cost. This is done by presenting possible risks
based on the system's current state and a set of indicators to indicate whether
any of the detected risks are happening or not. We evaluated APThreatHunter
using real-world Android malware samples, and the results revealed the
practicality of using automated planning for goal hypothesis generation in
cyber threat hunting activities.

</details>


### [24] [Adversarial Pre-Padding: Generating Evasive Network Traffic Against Transformer-Based Classifiers](https://arxiv.org/abs/2510.25810)
*Quanliang Jing,Xinxin Fan,Yanyan Liu,Jingping Bi*

Main category: cs.CR

TL;DR: 提出AdvTraffic方法对抗基于Transformer的流量分类器，通过预填充策略和强化学习优化流量扰动，将分类准确率从99%降至25.68%。


<details>
  <summary>Details</summary>
Motivation: 随着预训练模型特别是基于Transformer的分类器出现，现有流量混淆技术变得脆弱，需要新的对抗方法来保护网络数据隐私。

Method: 采用预填充策略修改数据包，并使用强化学习模型优化网络流量扰动，最大化对抗基于Transformer分类器的效果。

Result: 在多个真实数据集上的实验表明，该方法能有效削弱基于Transformer的分类器，将分类准确率从99%显著降低至25.68%。

Conclusion: 这是首次将对抗扰动技术应用于防御基于Transformer的流量分类器，且方法易于在实际网络环境中部署。

Abstract: To date, traffic obfuscation techniques have been widely adopted to protect
network data privacy and security by obscuring the true patterns of traffic.
Nevertheless, as the pre-trained models emerge, especially transformer-based
classifiers, existing traffic obfuscation methods become increasingly
vulnerable, as witnessed by current studies reporting the traffic
classification accuracy up to 99\% or higher. To counter such high-performance
transformer-based classification models, we in this paper propose a novel and
effective \underline{adv}ersarial \underline{traffic}-generating approach
(AdvTraffic\footnote{The code and data are available at: http://xxx}). Our
approach has two key innovations: (i) a pre-padding strategy is proposed to
modify packets, which effectively overcomes the limitations of existing
research against transformer-based models for network traffic classification;
and (ii) a reinforcement learning model is employed to optimize network traffic
perturbations, aiming to maximize adversarial effectiveness against
transformer-based classification models. To the best of our knowledge, this is
the first attempt to apply adversarial perturbation techniques to defend
against transformer-based traffic classifiers. Furthermore, our method can be
easily deployed into practical network environments. Finally, multi-faceted
experiments are conducted across several real-world datasets, and the
experimental results demonstrate that our proposed method can effectively
undermine transformer-based classifiers, significantly reducing classification
accuracy from 99\% to as low as 25.68\%.

</details>


### [25] [Identity Management for Agentic AI: The new frontier of authorization, authentication, and security for an AI agent world](https://arxiv.org/abs/2510.25819)
*Tobin South,Subramanya Nagabhushanaradhya,Ayesha Dissanayaka,Sarah Cecchetti,George Fletcher,Victor Lu,Aldo Pietropaolo,Dean H. Saxe,Jeff Lombardo,Abhishek Maligehalli Shivalingaiah,Stan Bounev,Alex Keisner,Andor Kesselman,Zack Proser,Ginny Fahs,Andrew Bunyea,Ben Moskowitz,Atul Tulshibagwale,Dazza Greenwood,Jiaxin Pei,Alex Pentland*

Main category: cs.CR

TL;DR: 该OpenID基金会白皮书分析了AI代理在认证、授权和身份管理方面的挑战，概述了现有安全资源并提出了解决未来自主系统基础问题的战略议程。


<details>
  <summary>Details</summary>
Motivation: AI代理的快速发展带来了认证、授权和身份管理的紧迫挑战，现有协议如MCP显示出对认证和授权最佳实践的明确需求。

Method: 通过分析当前代理中心协议的需求，并展望高度自主代理的长期问题，包括可扩展访问控制、代理中心身份、AI工作负载区分和委托授权等。

Result: 识别了当前可用于保护代理的现有资源，并制定了解决未来广泛自主系统基础认证、授权和身份问题的战略议程。

Conclusion: 该白皮书为AI代理和访问管理交叉领域的利益相关者提供了路线图，旨在解决当前和未来自主系统的认证、授权和身份管理挑战。

Abstract: The rapid rise of AI agents presents urgent challenges in authentication,
authorization, and identity management. Current agent-centric protocols (like
MCP) highlight the demand for clarified best practices in authentication and
authorization. Looking ahead, ambitions for highly autonomous agents raise
complex long-term questions regarding scalable access control, agent-centric
identities, AI workload differentiation, and delegated authority. This OpenID
Foundation whitepaper is for stakeholders at the intersection of AI agents and
access management. It outlines the resources already available for securing
today's agents and presents a strategic agenda to address the foundational
authentication, authorization, and identity problems pivotal for tomorrow's
widespread autonomous systems.

</details>


### [26] [A Critical Roadmap to Driver Authentication via CAN Bus: Dataset Review, Introduction of the Kidmose CANid Dataset (KCID), and Proof of Concept](https://arxiv.org/abs/2510.25856)
*Brooke Elizabeth Kidmose,Andreas Brasen Kidmose,Cliff C. Zou*

Main category: cs.CR

TL;DR: 本文介绍了Kidmose CANid数据集(KCID)，解决了现有驾驶员指纹识别数据集的局限性，包括提供原始CAN总线数据、驾驶员人口统计信息，并提出了基于CAN总线的驾驶员认证防盗框架。


<details>
  <summary>Details</summary>
Motivation: 现代车辆仍然容易受到未经授权使用和盗窃的威胁，现有驾驶员指纹识别数据集存在严重局限性，需要更全面的数据集来开发可靠的驾驶员认证系统。

Method: 引入KCID数据集，包含16名驾驶员在4辆车上的原始CAN总线数据，包括人口统计信息和日常驾驶数据；开发了驾驶员认证防盗框架并在单板计算机上实现原型系统。

Result: 通过未改装乘用车的实际道路试验，证明了基于CAN总线的驾驶员认证防盗系统的实际可行性。

Conclusion: 该工作为研究人员提供了开发稳健、可部署的驾驶员认证系统所需的数据和方法基础，KCID数据集还可用于驾驶员画像、机械异常检测、年轻驾驶员监控和受损驾驶检测等应用。

Abstract: Modern vehicles remain vulnerable to unauthorized use and theft despite
traditional security measures including immobilizers and keyless entry systems.
Criminals exploit vulnerabilities in Controller Area Network (CAN) bus systems
to bypass authentication mechanisms, while social media trends have expanded
auto theft to include recreational joyriding by underage drivers. Driver
authentication via CAN bus data offers a promising additional layer of
defense-in-depth protection, but existing open-access driver fingerprinting
datasets suffer from critical limitations including reliance on decoded
diagnostic data rather than raw CAN traffic, artificial fixed-route
experimental designs, insufficient sampling rates, and lack of demographic
information.
  This paper provides a comprehensive review of existing open-access driver
fingerprinting datasets, analyzing their strengths and limitations to guide
practitioners in dataset selection. We introduce the Kidmose CANid Dataset
(KCID), which addresses these fundamental shortcomings by providing raw CAN bus
data from 16 drivers across four vehicles, including essential demographic
information and both daily driving and controlled fixed-route data. Beyond
dataset contributions, we present a driver authentication anti-theft framework
and implement a proof-of-concept prototype on a single-board computer. Through
live road trials with an unaltered passenger vehicle, we demonstrate the
practical feasibility of CAN bus-based driver authentication anti-theft
systems. Finally, we explore diverse applications of KCID beyond driver
authentication, including driver profiling for insurance and safety
assessments, mechanical anomaly detection, young driver monitoring, and
impaired driving detection. This work provides researchers with both the data
and methodological foundation necessary to develop robust, deployable driver
authentication systems...

</details>


### [27] [AAGATE: A NIST AI RMF-Aligned Governance Platform for Agentic AI](https://arxiv.org/abs/2510.25863)
*Ken Huang,Jerry Huang,Yasir Mehmood,Hammad Atta,Muhammad Zeeshan Baig,Muhammad Aziz Ul Haq*

Main category: cs.CR

TL;DR: AAGATE是一个Kubernetes原生的控制平面，专门解决自主语言模型驱动代理在生产环境中的安全和治理挑战，通过集成NIST AI风险管理框架和多个专业安全框架，提供持续可验证的治理解决方案。


<details>
  <summary>Details</summary>
Motivation: 传统应用安全工具无法应对即兴、机器速度的自主AI代理系统，需要专门的安全和治理框架来应对这些独特挑战。

Method: 采用Kubernetes原生架构，集成NIST AI RMF框架，结合零信任服务网格、可解释策略引擎、行为分析和去中心化问责钩子等技术组件。

Result: 构建了一个能够安全、负责任、可扩展部署代理AI的连续可验证治理解决方案。

Conclusion: AAGATE框架通过整合多种安全机制，确保治理覆盖系统性、对抗性和伦理风险，为代理AI的生产部署提供了全面的安全保障。

Abstract: This paper introduces the Agentic AI Governance Assurance & Trust Engine
(AAGATE), a Kubernetes-native control plane designed to address the unique
security and governance challenges posed by autonomous, language-model-driven
agents in production. Recognizing the limitations of traditional Application
Security (AppSec) tooling for improvisational, machine-speed systems, AAGATE
operationalizes the NIST AI Risk Management Framework (AI RMF). It integrates
specialized security frameworks for each RMF function: the Agentic AI Threat
Modeling MAESTRO framework for Map, a hybrid of OWASP's AIVSS and SEI's SSVC
for Measure, and the Cloud Security Alliance's Agentic AI Red Teaming Guide for
Manage. By incorporating a zero-trust service mesh, an explainable policy
engine, behavioral analytics, and decentralized accountability hooks, AAGATE
provides a continuous, verifiable governance solution for agentic AI, enabling
safe, accountable, and scalable deployment. The framework is further extended
with DIRF for digital identity rights, LPCI defenses for logic-layer injection,
and QSAF monitors for cognitive degradation, ensuring governance spans
systemic, adversarial, and ethical risks.

</details>


### [28] [Foundations of Fiat-Denominated Loans Collateralized by Cryptocurrencies](https://arxiv.org/abs/2510.25878)
*Pavel Hubáček,Jan Václavek,Michelle Yeo*

Main category: cs.CR

TL;DR: 该论文研究了以加密货币为抵押的法币贷款安全协议，提出了依赖可信仲裁的有限托管协议，并进行了博弈论分析。


<details>
  <summary>Details</summary>
Motivation: 随着加密货币作为金融资产的重要性上升，需要将其从投机对象转变为更接近标准金融工具（如贷款）的应用。

Method: 开发了基于可信仲裁的有限托管协议，用于以加密货币（如比特币）为抵押的法币贷款，并对协议进行了博弈论分析。

Result: 提出了可行的安全协议框架，能够实现加密货币抵押的法币贷款，同时指出了未来研究的多个有趣方向。

Conclusion: 该工作为加密货币抵押贷款领域奠定了基础，展示了有限托管协议在可信仲裁下的可行性，为未来研究开辟了新的方向。

Abstract: The rising importance of cryptocurrencies as financial assets pushed their
applicability from an object of speculation closer to standard financial
instruments such as loans. In this work, we initiate the study of secure
protocols that enable fiat-denominated loans collateralized by cryptocurrencies
such as Bitcoin. We provide limited-custodial protocols for such loans relying
only on trusted arbitration and provide their game-theoretical analysis. We
also highlight various interesting directions for future research.

</details>


### [29] [FakeZero: Real-Time, Privacy-Preserving Misinformation Detection for Facebook and X](https://arxiv.org/abs/2510.25932)
*Soufiane Essahli,Oussama Sarsar,Imane Fouad,Anas Motii,Ahmed Bentajer*

Main category: cs.CR

TL;DR: FakeZero是一个完全客户端、跨平台的浏览器扩展，能够在用户滚动时实时标记Facebook和X平台上的不可靠帖子，所有计算都在本地进行，不泄露个人数据。


<details>
  <summary>Details</summary>
Motivation: 社交媒体以空前速度传播信息，加速了错误信息的传播并威胁公共话语。需要一种保护隐私的实时检测工具来应对这一挑战。

Method: 采用三阶段训练课程：基线微调、域自适应训练（包含焦点损失、对抗增强和后训练量化）。使用DistilBERT-Quant和TinyBERT-Quant模型在本地浏览器中运行。

Result: 在239,000个帖子的数据集上，DistilBERT-Quant模型（67.6 MB）达到97.1%宏F1、97.4%准确率和0.996 AUROC，中位延迟约103毫秒。TinyBERT-Quant变体（14.7 MB）保持95.7%宏F1和96.1%准确率，延迟降至约40毫秒。

Conclusion: FakeZero证明在严格资源预算下实现高质量假新闻检测是可行的，可作为政策制定者遏制错误信息传播的有价值工具，并为研究人员收集大规模假新闻数据集提供可能。

Abstract: Social platforms distribute information at unprecedented speed, which in turn
accelerates the spread of misinformation and threatens public discourse. We
present FakeZero, a fully client-side, cross-platform browser extension that
flags unreliable posts on Facebook and X (formerly Twitter) while the user
scrolls. All computation, DOM scraping, tokenisation, Transformer inference,
and UI rendering run locally through the Chromium messaging API, so no personal
data leaves the device.FakeZero employs a three-stage training curriculum:
baseline fine-tuning and domain-adaptive training enhanced with focal loss,
adversarial augmentation, and post-training quantisation. Evaluated on a
dataset of 239,000 posts, the DistilBERT-Quant model (67.6 MB) reaches 97.1%
macro-F1, 97.4% accuracy, and an AUROC of 0.996, with a median latency of
approximately 103 ms on a commodity laptop. A memory-efficient TinyBERT-Quant
variant retains 95.7% macro-F1 and 96.1% accuracy while shrinking the model to
14.7 MB and lowering latency to approximately 40 ms, showing that high-quality
fake-news detection is feasible under tight resource budgets with only modest
performance loss.By providing inline credibility cues, the extension can serve
as a valuable tool for policymakers seeking to curb the spread of
misinformation across social networks. With user consent, FakeZero also opens
the door for researchers to collect large-scale datasets of fake news in the
wild, enabling deeper analysis and the development of more robust detection
techniques.

</details>


### [30] [SoK: Honeypots & LLMs, More Than the Sum of Their Parts?](https://arxiv.org/abs/2510.25939)
*Robert A. Bridges,Thomas R. Mitchell,Mauricio Muñoz,Ted Henriksson*

Main category: cs.CR

TL;DR: 本文首次系统化梳理了基于LLM的蜜罐研究领域，提出了检测向量分类法、典型架构和评估趋势，并展望了自主欺骗系统的未来发展。


<details>
  <summary>Details</summary>
Motivation: 填补LLM蜜罐研究领域的知识空白，解决蜜罐设计中高逼真度与低操作风险的长期矛盾，为该新兴领域提供系统化理解。

Method: 采用知识系统化方法，从三个关键研究维度进行梳理：蜜罐检测向量分类、LLM蜜罐文献综合分析和蜜罐日志分析演进路径。

Result: 建立了LLM蜜罐的检测向量分类体系，识别了典型架构和评估趋势，揭示了从简单数据缩减到自动化情报生成的演进路径。

Conclusion: LLM蜜罐技术的真正潜力在于创建自主、自我改进的欺骗系统，以应对智能自动化攻击者的新兴威胁。

Abstract: The advent of Large Language Models (LLMs) promised to resolve the
long-standing paradox in honeypot design: achieving high-fidelity deception
with low operational risk. However, despite a flurry of research since late
2022, progress has been incremental, and the field lacks a cohesive
understanding of the emerging architectural patterns, core challenges, and
evaluation paradigms. To fill this gap, this Systematization of Knowledge (SoK)
paper provides the first comprehensive overview of this new domain. We survey
and systematize three critical, intersecting research areas: first, we provide
a taxonomy of honeypot detection vectors, structuring the core problems that
LLM-based realism must solve; second, we synthesize the emerging literature on
LLM-honeypots, identifying a canonical architecture and key evaluation trends;
and third, we chart the evolutionary path of honeypot log analysis, from simple
data reduction to automated intelligence generation. We synthesize these
findings into a forward-looking research roadmap, arguing that the true
potential of this technology lies in creating autonomous, self-improving
deception systems to counter the emerging threat of intelligent, automated
attackers.

</details>


### [31] [WaveVerif: Acoustic Side-Channel based Verification of Robotic Workflows](https://arxiv.org/abs/2510.25960)
*Zeynep Yasemin Erdogan,Shishir Nagaraja,Chuadhry Mujeeb Ahmed,Ryan Shah*

Main category: cs.CR

TL;DR: 提出基于声学侧信道分析的机器人工作流验证框架，通过机器学习分析机器人运动产生的声音来验证命令执行正确性，在基准条件下准确率超过80%。


<details>
  <summary>Details</summary>
Motivation: 在敏感机器人环境中实现实时、低成本、无需硬件修改的被动验证，确保机器人正确执行预期命令。

Method: 开发基于机器学习的声学工作流验证系统，使用SVM、DNN、RNN和CNN四种分类器分析机器人运动产生的声学信号，考虑运动速度、方向和麦克风距离等因素。

Result: 单个机器人运动验证准确率超过80%，拾取放置和包装等工作流也能以高置信度识别，证明声学信号可用于实时验证。

Conclusion: 声学侧信道分析为敏感机器人环境提供了一种有效的实时、低成本、被动验证方法，无需硬件修改即可监控机器人行为一致性。

Abstract: In this paper, we present a framework that uses acoustic side-channel
analysis (ASCA) to monitor and verify whether a robot correctly executes its
intended commands. We develop and evaluate a machine-learning-based workflow
verification system that uses acoustic emissions generated by robotic
movements. The system can determine whether real-time behavior is consistent
with expected commands. The evaluation takes into account movement speed,
direction, and microphone distance. The results show that individual robot
movements can be validated with over 80% accuracy under baseline conditions
using four different classifiers: Support Vector Machine (SVM), Deep Neural
Network (DNN), Recurrent Neural Network (RNN), and Convolutional Neural Network
(CNN). Additionally, workflows such as pick-and-place and packing could be
identified with similarly high confidence. Our findings demonstrate that
acoustic signals can support real-time, low-cost, passive verification in
sensitive robotic environments without requiring hardware modifications.

</details>


### [32] [Message Recovery Attack in NTRU via Knapsack](https://arxiv.org/abs/2510.26003)
*Eirini Poimenidou,K. A. Draziotis*

Main category: cs.CR

TL;DR: 提出基于模背包问题的消息恢复攻击，适用于所有NTRU-HPS变体。当已知消息和非向量的部分系数时，可将消息解密简化为在格中寻找短向量问题。


<details>
  <summary>Details</summary>
Motivation: 研究需要多少关于消息或消息-随机数对的信息才能实现可行恢复，解决NTRU-HPS密码系统的安全性问题。

Method: 使用FLATTER约简方法，将消息解密问题转化为格中的短向量寻找问题，处理模背包系统实例。

Result: 当已知系数比例ε≈0.45时，攻击在实践中成功恢复消息，在普通台式机上几分钟内完成。

Conclusion: 该攻击方法有效，表明NTRU-HPS系统在部分信息泄露时存在安全风险，FLATTER约简是实用的攻击工具。

Abstract: In the present paper, we introduce a message-recovery attack based on the
Modular Knapsack Problem, applicable to all variants of the NTRU-HPS
cryptosystem. Assuming that a fraction $\epsilon$ of the coefficients of the
message ${\bf{m}}\in\{-1,0,1\}^N$ and of the nonce vector ${\bf
r}\in\{-1,0,1\}^N$ are known in advance at random positions, we reduce message
decryption to finding a short vector in a lattice that encodes an instance of a
modular knapsack system. This allows us to address a key question: how much
information about ${\bf m}$, or about the pair $({\bf m},{\bf r})$, is required
before recovery becomes feasible? A FLATTER reduction successfully recovers the
message, in practice when $\epsilon\approx 0.45$. Our implementation finds
${\bf m}$ within a few minutes on a commodity desktop.

</details>


### [33] [SIRAJ: Diverse and Efficient Red-Teaming for LLM Agents via Distilled Structured Reasoning](https://arxiv.org/abs/2510.26037)
*Kaiwen Zhou,Ahmed Elgohary,A S M Iftekhar,Amin Saied*

Main category: cs.CR

TL;DR: SIRAJ是一个通用的红队测试框架，用于发现黑盒LLM代理的安全漏洞。它采用动态两步流程生成多样化测试用例，并通过模型蒸馏优化成本，使小型模型达到与大型模型相当的攻击效果。


<details>
  <summary>Details</summary>
Motivation: LLM代理规划和调用工具的能力带来了新的安全风险，需要全面的红队测试系统来发现漏洞并确保安全部署。

Method: 采用动态两步流程：首先生成覆盖各种风险结果和工具使用轨迹的种子测试用例，然后基于先前执行轨迹迭代构建和优化基于模型的对抗攻击。使用模型蒸馏方法训练更小的有效模型。

Result: 种子测试用例生成方法使风险结果和工具调用轨迹的覆盖率提高了2-2.5倍。蒸馏后的8B红队模型攻击成功率提高100%，超越了671B的Deepseek-R1模型。

Conclusion: SIRAJ框架在迭代优化、结构化推理和红队模型泛化方面表现出有效性，为LLM代理的安全评估提供了高效解决方案。

Abstract: The ability of LLM agents to plan and invoke tools exposes them to new safety
risks, making a comprehensive red-teaming system crucial for discovering
vulnerabilities and ensuring their safe deployment. We present SIRAJ: a generic
red-teaming framework for arbitrary black-box LLM agents. We employ a dynamic
two-step process that starts with an agent definition and generates diverse
seed test cases that cover various risk outcomes, tool-use trajectories, and
risk sources. Then, it iteratively constructs and refines model-based
adversarial attacks based on the execution trajectories of former attempts. To
optimize the red-teaming cost, we present a model distillation approach that
leverages structured forms of a teacher model's reasoning to train smaller
models that are equally effective. Across diverse evaluation agent settings,
our seed test case generation approach yields 2 -- 2.5x boost to the coverage
of risk outcomes and tool-calling trajectories. Our distilled 8B red-teamer
model improves attack success rate by 100%, surpassing the 671B Deepseek-R1
model. Our ablations and analyses validate the effectiveness of the iterative
framework, structured reasoning, and the generalization of our red-teamer
models.

</details>


### [34] [PEEL: A Poisoning-Exposing Encoding Theoretical Framework for Local Differential Privacy](https://arxiv.org/abs/2510.26102)
*Lisha Shuai,Jiuling Dong,Nan Zhang,Shaofeng Tan,Haokun Zhang,Zilong Song,Gaoya Dong,Xiaolong Yang*

Main category: cs.CR

TL;DR: PEEL是一个针对LDP的投毒攻击防御框架，通过数据重编码揭示投毒攻击，保持统计准确性同时降低计算成本。


<details>
  <summary>Details</summary>
Motivation: 现有的LDP投毒防御方法要么资源开销过大，要么依赖特定领域先验知识，限制了实际部署。

Method: PEEL通过稀疏化、归一化和低秩投影对LDP扰动数据进行重编码，放大投毒效应，在重构空间中揭示结构不一致性。

Result: PEEL在投毒检测准确率上优于四种最先进防御方法，并显著降低客户端计算成本。

Conclusion: PEEL是一个高效实用的LDP投毒防御框架，特别适合大规模物联网部署。

Abstract: Local Differential Privacy (LDP) is a widely adopted privacy-protection model
in the Internet of Things (IoT) due to its lightweight, decentralized, and
scalable nature. However, it is vulnerable to poisoning attacks, and existing
defenses either incur prohibitive resource overheads or rely on domain-specific
prior knowledge, limiting their practical deployment. To address these
limitations, we propose PEEL, a Poisoning-Exposing Encoding theoretical
framework for LDP, which departs from resource- or prior-dependent
countermeasures and instead leverages the inherent structural consistency of
LDP-perturbed data. As a non-intrusive post-processing module, PEEL amplifies
stealthy poisoning effects by re-encoding LDP-perturbed data via
sparsification, normalization, and low-rank projection, thereby revealing both
output and rule poisoning attacks through structural inconsistencies in the
reconstructed space. Theoretical analysis proves that PEEL, integrated with
LDP, retains unbiasedness and statistical accuracy, while being robust to
expose both output and rule poisoning attacks. Moreover, evaluation results
show that LDP-integrated PEEL not only outperforms four state-of-the-art
defenses in terms of poisoning exposure accuracy but also significantly reduces
client-side computational costs, making it highly suitable for large-scale IoT
deployments.

</details>


### [35] [Security Vulnerabilities in AI-Generated Code: A Large-Scale Analysis of Public GitHub Repositories](https://arxiv.org/abs/2510.26103)
*Maximilian Schreiber,Pascal Tippe*

Main category: cs.CR

TL;DR: 对GitHub上AI生成代码的安全漏洞进行大规模实证分析，发现87.9%的代码无CWE漏洞，但存在语言和工具差异，Python漏洞率最高，文档生成是主要应用场景。


<details>
  <summary>Details</summary>
Motivation: 随着AI代码生成工具的普及，需要系统评估其生成代码的安全性，为负责任地集成AI生成代码到开发流程提供依据。

Method: 收集7,703个来自ChatGPT、GitHub Copilot等四大AI工具的代码文件，使用CodeQL静态分析识别4,241个CWE漏洞实例。

Result: Python漏洞率最高(16.18%-18.50%)，GitHub Copilot在Python和TypeScript上安全密度最佳，ChatGPT在JavaScript表现更好，39%文件用于文档生成。

Conclusion: 研究为开发语言特定和上下文感知的安全实践提供重要见解，支持AI生成代码在软件开发中的负责任集成。

Abstract: This paper presents a comprehensive empirical analysis of security
vulnerabilities in AI-generated code across public GitHub repositories. We
collected and analyzed 7,703 files explicitly attributed to four major AI
tools: ChatGPT (91.52\%), GitHub Copilot (7.50\%), Amazon CodeWhisperer
(0.52\%), and Tabnine (0.46\%). Using CodeQL static analysis, we identified
4,241 Common Weakness Enumeration (CWE) instances across 77 distinct
vulnerability types. Our findings reveal that while 87.9\% of AI-generated code
does not contain identifiable CWE-mapped vulnerabilities, significant patterns
emerge regarding language-specific vulnerabilities and tool performance. Python
consistently exhibited higher vulnerability rates (16.18\%-18.50\%) compared to
JavaScript (8.66\%-8.99\%) and TypeScript (2.50\%-7.14\%) across all tools. We
observed notable differences in security performance, with GitHub Copilot
achieving better security density for Python (1,739 LOC per CWE) and
TypeScript, while ChatGPT performed better for JavaScript. Additionally, we
discovered widespread use of AI tools for documentation generation (39\% of
collected files), an understudied application with implications for software
maintainability. These findings extend previous work with a significantly
larger dataset and provide valuable insights for developing language-specific
and context-aware security practices for the responsible integration of
AI-generated code into software development workflows.

</details>


### [36] [Confidential FRIT via Homomorphic Encryption](https://arxiv.org/abs/2510.26179)
*Haruki Hoshino,Jungjin Park,Osamu Kaneko,Kiminao Kogiso*

Main category: cs.CR

TL;DR: 提出基于同态加密的保密数据驱动增益调谐框架，使用ElGamal和CKKS加密方案保护外包到外部服务器的增益调谐过程安全。


<details>
  <summary>Details</summary>
Motivation: 边缘计算减轻了CPS中数据驱动控制的计算负担，但日益复杂的网络攻击需要超越传统IT保护的安全措施，以解决CPS特有的漏洞。

Method: 通过将矩阵求逆操作替换为向量求和形式，使同态操作能够应用，实现保密的FRIT（Fictitious Reference Iterative Tuning）方法。

Result: 在128位安全级别下的数值示例显示，该方法性能与传统方法相当，同时为安全CPS选择合适的加密方案提供了指导。

Conclusion: 该框架能够在保持性能的同时增强CPS增益调谐过程的网络安全，为安全关键系统提供了实用的加密解决方案。

Abstract: Edge computing alleviates the computation burden of data-driven control in
cyber-physical systems (CPSs) by offloading complex processing to edge servers.
However, the increasing sophistication of cyberattacks underscores the need for
security measures that go beyond conventional IT protections and address the
unique vulnerabilities of CPSs. This study proposes a confidential data-driven
gain-tuning framework using homomorphic encryption, such as ElGamal and CKKS
encryption schemes, to enhance cybersecurity in gain-tuning processes
outsourced to external servers. The idea for realizing confidential FRIT is to
replace the matrix inversion operation with a vector summation form, allowing
homomorphic operations to be applied. Numerical examples under 128-bit security
confirm performance comparable to conventional methods while providing
guidelines for selecting suitable encryption schemes for secure CPS.

</details>


### [37] [Who Moved My Transaction? Uncovering Post-Transaction Auditability Vulnerabilities in Modern Super Apps](https://arxiv.org/abs/2510.26210)
*Junlin Liu,Zhaomeng Deng,Ziming Wang,Mengyu Yao,Yifeng Cai,Yutao Hu,Ziqi Zhang,Yao Guo,Ding Li*

Main category: cs.CR

TL;DR: 研究发现超级应用普遍存在交易记录删除漏洞，83%的应用缺乏强认证保护，允许用户轻易删除交易历史，存在严重安全风险。


<details>
  <summary>Details</summary>
Motivation: 当前超级应用安全范式过度关注交易前认证，忽视了交易后审计轨迹的脆弱性，用户可永久删除交易记录以隐藏未授权或敏感活动。

Method: 通过6名志愿者对6款超级应用进行交叉评估的实证研究，分析交易记录删除功能的安全性。

Result: 所有6款应用都允许删除交易记录，但83%的应用缺乏强认证保护，仅1款应用要求生物识别验证。

Conclusion: 这揭示了移动安全领域的关键漏洞，迫切需要向确保交易后审计完整性进行范式转变。

Abstract: Super apps are the cornerstones of modern digital life, embedding financial
transactions into nearly every aspect of daily routine. The prevailing security
paradigm for these platforms is overwhelmingly focused on pre-transaction
authentication, preventing unauthorized payments before they occur. We argue
that a critical vulnerability vector has been largely overlooked: the fragility
of post-transaction audit trails. We investigate the ease with which a user can
permanently erase their transaction history from an app's interface, thereby
concealing unauthorized or sensitive activities from the account owner. To
quantify this threat, we conducted an empirical study with 6 volunteers who
performed a cross-evaluation on six super apps. Our findings are alarming: all
six applications studied allow users to delete transaction records, yet a
staggering five out of six (83+\%) fail to protect these records with strong
authentication. Only one app in our study required biometric verification for
deletion. This study provides the first concrete evidence of this
near-ubiquitous vulnerability, demonstrating a critical gap in the current
mobile security landscape and underscoring the urgent need for a paradigm shift
towards ensuring post-transaction audit integrity.

</details>


### [38] [Who Grants the Agent Power? Defending Against Instruction Injection via Task-Centric Access Control](https://arxiv.org/abs/2510.26212)
*Yifeng Cai,Ziming Wang,Zhaomeng Deng,Mengyu Yao,Junlin Liu,Yutao Hu,Ziqi Zhang,Yao Guo,Ding Li*

Main category: cs.CR

TL;DR: AgentSentry是一个轻量级运行时任务中心访问控制框架，通过动态生成和执行与用户特定任务对齐的最小临时权限策略，防止AI代理的指令注入攻击。


<details>
  <summary>Details</summary>
Motivation: AI代理在移动任务自动化中依赖过度授权和静态权限，存在指令注入漏洞，恶意指令可能劫持代理执行未授权操作。

Method: 采用动态、任务范围的权限控制，根据用户具体任务动态生成最小临时策略，任务完成后立即撤销权限。

Result: AgentSentry成功阻止了指令注入攻击（如诱骗代理转发私人邮件），同时允许合法任务完成。

Conclusion: 需要意图对齐的安全模型来安全治理下一代自主代理。

Abstract: AI agents capable of GUI understanding and Model Context Protocol are
increasingly deployed to automate mobile tasks. However, their reliance on
over-privileged, static permissions creates a critical vulnerability:
instruction injection. Malicious instructions, embedded in otherwise benign
content like emails, can hijack the agent to perform unauthorized actions. We
present AgentSentry, a lightweight runtime task-centric access control
framework that enforces dynamic, task-scoped permissions. Instead of granting
broad, persistent permissions, AgentSentry dynamically generates and enforces
minimal, temporary policies aligned with the user's specific task (e.g.,
register for an app), revoking them upon completion. We demonstrate that
AgentSentry successfully prevents an instruction injection attack, where an
agent is tricked into forwarding private emails, while allowing the legitimate
task to complete. Our approach highlights the urgent need for intent-aligned
security models to safely govern the next generation of autonomous agents.

</details>


### [39] [PVMark: Enabling Public Verifiability for LLM Watermarking Schemes](https://arxiv.org/abs/2510.26274)
*Haohua Duan,Liyao Xiang,Xin Zhang*

Main category: cs.CR

TL;DR: PVMark是一个基于零知识证明的插件，使LLM水印检测过程可公开验证，解决了现有水印方案因密钥保密性导致的信任问题。


<details>
  <summary>Details</summary>
Motivation: 现有LLM水印方案存在信任问题：非公开的水印检测无法证明其忠实执行检测过程。密钥既不能公开（否则攻击者可能发起移除攻击），也不能完全私有（否则检测过程对公众不透明）。

Method: 基于零知识证明构建水印检测的'正确执行'证明，包括映射、随机数生成、比较和求和等约束条件。实现了多种变体，涵盖三种水印方案、三种哈希函数和四种ZKP协议。

Result: PVMark在多种情况下有效工作，能够高效地为最先进的LLM水印方案提供公开可验证性，且不影响水印性能。

Conclusion: PVMark解决了LLM水印检测的信任困境，有望在实际中部署应用。

Abstract: Watermarking schemes for large language models (LLMs) have been proposed to
identify the source of the generated text, mitigating the potential threats
emerged from model theft. However, current watermarking solutions hardly
resolve the trust issue: the non-public watermark detection cannot prove itself
faithfully conducting the detection. We observe that it is attributed to the
secret key mostly used in the watermark detection -- it cannot be public, or
the adversary may launch removal attacks provided the key; nor can it be
private, or the watermarking detection is opaque to the public. To resolve the
dilemma, we propose PVMark, a plugin based on zero-knowledge proof (ZKP),
enabling the watermark detection process to be publicly verifiable by third
parties without disclosing any secret key. PVMark hinges upon the proof of
`correct execution' of watermark detection on which a set of ZKP constraints
are built, including mapping, random number generation, comparison, and
summation. We implement multiple variants of PVMark in Python, Rust and Circom,
covering combinations of three watermarking schemes, three hash functions, and
four ZKP protocols, to show our approach effectively works under a variety of
circumstances. By experimental results, PVMark efficiently enables public
verifiability on the state-of-the-art LLM watermarking schemes yet without
compromising the watermarking performance, promising to be deployed in
practice.

</details>


### [40] [A Survey of Heterogeneous Graph Neural Networks for Cybersecurity Anomaly Detection](https://arxiv.org/abs/2510.26307)
*Laura Jiang,Reza Ryan,Qian Li,Nasim Ferdosian*

Main category: cs.CR

TL;DR: 这是一篇关于异构图神经网络在网络安全异常检测中的综述论文，系统梳理了该领域的研究现状、分类方法和未来方向。


<details>
  <summary>Details</summary>
Motivation: 当前基于HGNN的异常检测研究存在碎片化问题，缺乏统一的建模策略、比较评估和标准化基准，需要建立系统化的研究框架。

Method: 提出了按异常类型和图动态性分类的分类法，分析了代表性模型，并将其映射到关键网络安全应用中，同时回顾了常用数据集和评估指标。

Result: 建立了HGNN异常检测的结构化基础，识别了建模、数据和部署方面的关键挑战，为未来研究提供了清晰方向。

Conclusion: 该综述旨在推动HGNN异常检测向可扩展、可解释和实际可部署的解决方案发展，为领域研究建立系统化框架。

Abstract: Anomaly detection is a critical task in cybersecurity, where identifying
insider threats, access violations, and coordinated attacks is essential for
ensuring system resilience. Graph-based approaches have become increasingly
important for modeling entity interactions, yet most rely on homogeneous and
static structures, which limits their ability to capture the heterogeneity and
temporal evolution of real-world environments. Heterogeneous Graph Neural
Networks (HGNNs) have emerged as a promising paradigm for anomaly detection by
incorporating type-aware transformations and relation-sensitive aggregation,
enabling more expressive modeling of complex cyber data. However, current
research on HGNN-based anomaly detection remains fragmented, with diverse
modeling strategies, limited comparative evaluation, and an absence of
standardized benchmarks. To address this gap, we provide a comprehensive survey
of HGNN-based anomaly detection methods in cybersecurity. We introduce a
taxonomy that classifies approaches by anomaly type and graph dynamics, analyze
representative models, and map them to key cybersecurity applications. We also
review commonly used benchmark datasets and evaluation metrics, highlighting
their strengths and limitations. Finally, we identify key open challenges
related to modeling, data, and deployment, and outline promising directions for
future research. This survey aims to establish a structured foundation for
advancing HGNN-based anomaly detection toward scalable, interpretable, and
practically deployable solutions.

</details>


### [41] [SSCL-BW: Sample-Specific Clean-Label Backdoor Watermarking for Dataset Ownership Verification](https://arxiv.org/abs/2510.26420)
*Yingjia Wang,Ting Qiao,Xing Liu,Chongzuo Li,Sixing Wu,Jianbin Li*

Main category: cs.CR

TL;DR: 提出了一种样本特定的清洁标签后门水印方法(SSCL-BW)，通过训练U-Net水印生成器为每个样本生成独特水印，解决了静态水印模式易被检测和移除的问题。


<details>
  <summary>Details</summary>
Motivation: 现有基于后门的数据集所有权验证方法存在局限性：毒标签水印因标签不一致易被检测，清洁标签水印技术复杂度高且在高质量图像上失败，且都使用易被检测的静态水印模式。

Method: 训练U-Net水印生成器为每个样本生成独特水印，设计包含目标样本损失、非目标样本损失和感知相似性损失的复合损失函数，通过黑盒测试验证模型是否表现出预定义的后门行为。

Result: 在基准数据集上的广泛实验证明了该方法的有效性，并展示了其对潜在水印移除攻击的鲁棒性。

Conclusion: 该方法通过样本特定的清洁标签水印，从根本上克服了静态水印模式的脆弱性，为数据集知识产权保护提供了有效解决方案。

Abstract: The rapid advancement of deep neural networks (DNNs) heavily relies on
large-scale, high-quality datasets. However, unauthorized commercial use of
these datasets severely violates the intellectual property rights of dataset
owners. Existing backdoor-based dataset ownership verification methods suffer
from inherent limitations: poison-label watermarks are easily detectable due to
label inconsistencies, while clean-label watermarks face high technical
complexity and failure on high-resolution images. Moreover, both approaches
employ static watermark patterns that are vulnerable to detection and removal.
To address these issues, this paper proposes a sample-specific clean-label
backdoor watermarking (i.e., SSCL-BW). By training a U-Net-based watermarked
sample generator, this method generates unique watermarks for each sample,
fundamentally overcoming the vulnerability of static watermark patterns. The
core innovation lies in designing a composite loss function with three
components: target sample loss ensures watermark effectiveness, non-target
sample loss guarantees trigger reliability, and perceptual similarity loss
maintains visual imperceptibility. During ownership verification, black-box
testing is employed to check whether suspicious models exhibit predefined
backdoor behaviors. Extensive experiments on benchmark datasets demonstrate the
effectiveness of the proposed method and its robustness against potential
watermark removal attacks.

</details>


### [42] [CyberNER: A Harmonized STIX Corpus for Cybersecurity Named Entity Recognition](https://arxiv.org/abs/2510.26499)
*Yasir Ech-Chammakhy,Anas Motii,Anass Rabii,Oussama Azrara,Jaafar Chbili*

Main category: cs.CR

TL;DR: CyberNER是一个通过系统整合四个网络安全命名实体识别数据集到STIX 2.1标准的大规模统一语料库，解决了标注模式不兼容问题，显著提升了模型性能。


<details>
  <summary>Details</summary>
Motivation: 网络安全领域的命名实体识别面临数据集标注模式不兼容的问题，简单合并会导致标签空间噪声严重降低模型性能。

Method: 系统性地将四个主要数据集（CyNER、DNRTI、APTNER和Attacker）统一到STIX 2.1标准，解决语义歧义，将50多个不同源标签整合为21个一致的实体类型。

Result: 在CyberNER上训练的模型相比简单合并基线实现了约30%的相对F1分数提升。

Conclusion: CyberNER提供了一个标准化的基准，支持创建和严格比较更鲁棒、可泛化的网络安全实体提取模型。

Abstract: Extracting structured intelligence via Named Entity Recognition (NER) is
critical for cybersecurity, but the proliferation of datasets with incompatible
annotation schemas hinders the development of comprehensive models. While
combining these resources is desirable, we empirically demonstrate that naively
concatenating them results in a noisy label space that severely degrades model
performance. To overcome this critical limitation, we introduce CyberNER, a
large-scale, unified corpus created by systematically harmonizing four
prominent datasets (CyNER, DNRTI, APTNER, and Attacker) onto the STIX 2.1
standard. Our principled methodology resolves semantic ambiguities and
consolidates over 50 disparate source tags into 21 coherent entity types. Our
experiments show that models trained on CyberNER achieve a substantial
performance gain, with a relative F1-score improvement of approximately 30%
over the naive concatenation baseline. By publicly releasing the CyberNER
corpus, we provide a crucial, standardized benchmark that enables the creation
and rigorous comparison of more robust and generalizable entity extraction
models for the cybersecurity domain.

</details>


### [43] [Interdependent Privacy in Smart Homes: Hunting for Bystanders in Privacy Policies](https://arxiv.org/abs/2510.26523)
*Shuaishuai Liu,Gergely Acs,Gergely Biczók*

Main category: cs.CR

TL;DR: 对20款视频门铃和智能摄像头的隐私政策进行分析，重点关注旁观者隐私问题，发现厂商主要通过免责声明转移责任，并提出改进建议。


<details>
  <summary>Details</summary>
Motivation: 智能家居设备普及带来便利的同时，也引发了旁观者隐私问题——设备拥有者的行为可能影响非用户的隐私，而现有法规对此监管不足。

Method: 对20款视频门铃和智能摄像头的隐私政策进行针对性分析，结合真实案例研究，评估现有法律框架和技术能力。

Result: 研究发现虽然部分厂商承认旁观者存在，但仅通过免责声明将数据收集的伦理责任转移给设备拥有者，缺乏实质性保护措施。

Conclusion: 需要改进隐私政策语言和系统设计，提高透明度，赋予旁观者和设备拥有者更多权利，以更好地保护非用户隐私。

Abstract: Smart home devices such as video doorbells and security cameras are becoming
increasingly common in everyday life. While these devices offer convenience and
safety, they also raise new privacy concerns: how these devices affect others,
like neighbors, visitors, or people passing by. This issue is generally known
as interdependent privacy, where one person's actions (or inaction) may impact
the privacy of others, and, specifically, bystander privacy in the context of
smart homes. Given lax data protection regulations in terms of shared physical
spaces and amateur joint data controllers, we expect that the privacy policies
of smart home products reflect the missing regulatory incentives. This paper
presents a focused privacy policy analysis of 20 video doorbell and smart
camera products, concentrating explicitly on the bystander aspect. We show that
although some of the vendors acknowledge bystanders, they address it only to
the extent of including disclaimers, shifting the ethical responsibility for
collecting the data of non-users to the device owner. In addition, we identify
and examine real-world cases related to bystander privacy, demonstrating how
current deployments can impact non-users. Based on our findings, we analyze
vendor privacy policies in light of existing legal frameworks and technical
capabilities, and we provide practical recommendations for both policy language
and system design to enhance transparency and empower both bystanders and
device owners.

</details>


### [44] [A Comprehensive Evaluation and Practice of System Penetration Testing](https://arxiv.org/abs/2510.26555)
*Chunyi Zhang,Jin Zeng,Xiaoqi Li*

Main category: cs.CR

TL;DR: 本文研究系统安全渗透测试的方法与实践，探讨如何通过系统化的渗透测试流程和技术手段提升系统安全性，分析现有渗透工具的优缺点及适用领域，并通过实际案例总结攻击经验。


<details>
  <summary>Details</summary>
Motivation: 随着信息技术快速发展，应用复杂度不断增加，网络安全挑战日益严峻，需要研究有效的渗透测试方法来增强系统安全。

Method: 采用系统化的渗透测试流程和技术方法，分析现有渗透工具的特点，选择合适工具在目标范围和目标机器上复现攻击过程，并进行实际案例分析。

Result: 通过实践案例验证了所提出的渗透测试方法的有效性，成功总结了攻击过程中的经验教训。

Conclusion: 系统化的渗透测试流程和合适的工具选择能够有效提升系统安全性，实际案例分析为未来研究提供了有价值的参考。

Abstract: With the rapid advancement of information technology, the complexity of
applications continues to increase, and the cybersecurity challenges we face
are also escalating. This paper aims to investigate the methods and practices
of system security penetration testing, exploring how to enhance system
security through systematic penetration testing processes and technical
approaches. It also examines existing penetration tools, analyzing their
strengths, weaknesses, and applicable domains to guide penetration testers in
tool selection. Furthermore, based on the penetration testing process outlined
in this paper, appropriate tools are selected to replicate attack processes
using target ranges and target machines. Finally, through practical case
analysis, lessons learned from successful attacks are summarized to inform
future research.

</details>


### [45] [A DRL-Empowered Multi-Level Jamming Approach for Secure Semantic Communication](https://arxiv.org/abs/2510.26610)
*Weixuan Chen,Qianqian Yang*

Main category: cs.CR

TL;DR: 提出一种基于深度强化学习的多级干扰方法，结合语义层和物理层干扰来保护语义通信系统安全，通过DDPG算法动态优化预编码矩阵，在保证安全性的同时提升合法用户的图像重建质量。


<details>
  <summary>Details</summary>
Motivation: 语义通信虽然提高了通信效率，但会将语义信息暴露给潜在窃听者，需要增强其安全性。

Method: 采用语义层干扰（编码任务无关文本）和物理层干扰（编码高斯噪声）相结合的多级干扰方法，使用DDPG算法动态优化预编码矩阵，并通过交替优化策略联合训练语义通信模型和DDPG代理。

Result: 与加密基准和编码干扰基准相比，该方法在保证安全性的同时，将合法用户的峰值信噪比提高了约0.6 dB。

Conclusion: 多级干扰方法能有效保护语义通信系统安全，同时提升合法用户的通信质量。

Abstract: Semantic communication (SemCom) aims to transmit only task-relevant
information, thereby improving communication efficiency but also exposing
semantic information to potential eavesdropping. In this paper, we propose a
deep reinforcement learning (DRL)-empowered multi-level jamming approach to
enhance the security of SemCom systems over MIMO fading wiretap channels. This
approach combines semantic layer jamming, achieved by encoding task-irrelevant
text, and physical layer jamming, achieved by encoding random Gaussian noise.
These two-level jamming signals are superposed with task-relevant semantic
information to protect the transmitted semantics from eavesdropping. A deep
deterministic policy gradient (DDPG) algorithm is further introduced to
dynamically design and optimize the precoding matrices for both taskrelevant
semantic information and multi-level jamming signals, aiming to enhance the
legitimate user's image reconstruction while degrading the eavesdropper's
performance. To jointly train the SemCom model and the DDPG agent, we propose
an alternating optimization strategy where the two modules are updated
iteratively. Experimental results demonstrate that, compared with both the
encryption-based (ESCS) and encoded jammer-based (EJ) benchmarks, our method
achieves comparable security while improving the legitimate user's peak
signalto-noise ratio (PSNR) by up to approximately 0.6 dB.

</details>


### [46] [Toward Automated Security Risk Detection in Large Software Using Call Graph Analysis](https://arxiv.org/abs/2510.26620)
*Nicholas Pecka,Lotfi Ben Othmane,Renee Bryce*

Main category: cs.CR

TL;DR: 该论文提出了一种通过聚类调用图来自动化软件威胁建模的方法，使用基于密度和社区检测算法来识别安全威胁集群。


<details>
  <summary>Details</summary>
Motivation: 手动威胁建模方法通常劳动密集且容易出错，需要自动化解决方案来提高效率和准确性。

Method: 使用密度基和社区检测算法对调用图进行聚类，然后分析识别出的集群相关的安全威胁。

Result: 通过对Splunk Forwarder Operator的案例研究验证了该方法的可行性，能够有效评估代码密度相关的安全弱点。

Conclusion: 该方法为现代云原生环境提供了可扩展的半自动化威胁建模框架，具有实际应用潜力。

Abstract: Threat modeling plays a critical role in the identification and mitigation of
security risks; however, manual approaches are often labor intensive and prone
to error. This paper investigates the automation of software threat modeling
through the clustering of call graphs using density-based and community
detection algorithms, followed by an analysis of the threats associated with
the identified clusters. The proposed method was evaluated through a case study
of the Splunk Forwarder Operator (SFO), wherein selected clustering metrics
were applied to the software's call graph to assess pertinent code-density
security weaknesses. The results demonstrate the viability of the approach and
underscore its potential to facilitate systematic threat assessment. This work
contributes to the advancement of scalable, semi-automated threat modeling
frameworks tailored for modern cloud-native environments.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [47] [Towards Piece-by-Piece Explanations for Chess Positions with SHAP](https://arxiv.org/abs/2510.25775)
*Francesco Spinnato*

Main category: cs.AI

TL;DR: 将SHAP解释性AI技术应用于国际象棋分析，通过系统性地移除棋子来计算每个棋子对引擎评估的贡献值，使引擎评估更加透明和可解释。


<details>
  <summary>Details</summary>
Motivation: 当前国际象棋引擎提供精确但不透明的评估分数（厘兵分数），这些输出掩盖了单个棋子或模式的潜在贡献，需要一种方法来解释引擎评估的具体依据。

Method: 将棋子视为特征，通过系统性地移除棋子并计算SHAP值，为每个棋子分配加性贡献分数，从而在局部忠实且人类可解释的方式下解释引擎输出。

Result: 开发了一种能够将国际象棋引擎评估归因于特定棋子的方法，该方法受到古典象棋教学法的启发，并与现代可解释AI技术相结合。

Conclusion: 该方法为可视化、人类训练和引擎比较开辟了新的可能性，并发布了配套代码和数据以促进可解释象棋AI的未来研究。

Abstract: Contemporary chess engines offer precise yet opaque evaluations, typically
expressed as centipawn scores. While effective for decision-making, these
outputs obscure the underlying contributions of individual pieces or patterns.
In this paper, we explore adapting SHAP (SHapley Additive exPlanations) to the
domain of chess analysis, aiming to attribute a chess engines evaluation to
specific pieces on the board. By treating pieces as features and systematically
ablating them, we compute additive, per-piece contributions that explain the
engines output in a locally faithful and human-interpretable manner. This
method draws inspiration from classical chess pedagogy, where players assess
positions by mentally removing pieces, and grounds it in modern explainable AI
techniques. Our approach opens new possibilities for visualization, human
training, and engine comparison. We release accompanying code and data to
foster future research in interpretable chess AI.

</details>


### [48] [An Agentic Framework for Rapid Deployment of Edge AI Solutions in Industry 5.0](https://arxiv.org/abs/2510.25813)
*Jorge Martinez-Gil,Mario Pichler,Nefeli Bountouni,Sotiris Koussouris,Marielena Márquez Barreiro,Sergio Gusmeroli*

Main category: cs.AI

TL;DR: 提出了一个用于工业5.0的新框架，简化AI模型在边缘设备上的部署，通过本地推理和实时处理降低延迟并避免外部数据传输。


<details>
  <summary>Details</summary>
Motivation: 解决工业环境中AI模型部署复杂、延迟高和数据传输安全性的问题，推动工业5.0的发展。

Method: 采用基于代理的架构，将人类、算法或协作代理分配给明确定义的任务，支持模块化集成并保持低资源需求。

Result: 在食品行业的实际场景中进行初步评估，结果显示部署时间和系统适应性性能得到改善。

Conclusion: 该框架为工业5.0提供了一种有效的AI模型部署解决方案，具有灵活性、低延迟和资源效率的优势。

Abstract: We present a novel framework for Industry 5.0 that simplifies the deployment
of AI models on edge devices in various industrial settings. The design reduces
latency and avoids external data transfer by enabling local inference and
real-time processing. Our implementation is agent-based, which means that
individual agents, whether human, algorithmic, or collaborative, are
responsible for well-defined tasks, enabling flexibility and simplifying
integration. Moreover, our framework supports modular integration and maintains
low resource requirements. Preliminary evaluations concerning the food industry
in real scenarios indicate improved deployment time and system adaptability
performance. The source code is publicly available at
https://github.com/AI-REDGIO-5-0/ci-component.

</details>


### [49] [Symbolically Scaffolded Play: Designing Role-Sensitive Prompts for Generative NPC Dialogue](https://arxiv.org/abs/2510.25820)
*Vanessa Figueiredo,David Elumeze*

Main category: cs.AI

TL;DR: 研究比较了高约束和低约束提示对游戏NPC对话体验的影响，发现约束程度对玩家体验无显著差异，但角色类型会影响约束效果。提出了符号化支架游戏框架，使用模糊数值边界来平衡对话的连贯性和即兴性。


<details>
  <summary>Details</summary>
Motivation: 探索约束提示是否能真正改善大型语言模型驱动的游戏NPC对话体验，挑战"更严格的约束必然带来更好游戏体验"的假设。

Method: 通过"The Interview"语音侦探游戏进行用户研究(N=10)，比较高约束提示(HCP)和低约束提示(LCP)；然后重新设计混合JSON+RAG支架，并用LLM法官进行合成评估。

Result: 高约束和低约束提示在玩家体验上无可靠差异；支架效果具有角色依赖性：面试官NPC获得稳定性，而嫌疑犯NPC失去即兴可信度。

Conclusion: 推翻"更严格的约束必然增强游戏体验"的假设，提出符号化支架游戏框架，使用模糊数值边界在需要稳定的地方保持连贯性，在需要惊喜的地方保留即兴性。

Abstract: Large Language Models (LLMs) promise to transform interactive games by
enabling non-player characters (NPCs) to sustain unscripted dialogue. Yet it
remains unclear whether constrained prompts actually improve player experience.
We investigate this question through The Interview, a voice-based detective
game powered by GPT-4o. A within-subjects usability study ($N=10$) compared
high-constraint (HCP) and low-constraint (LCP) prompts, revealing no reliable
experiential differences beyond sensitivity to technical breakdowns. Guided by
these findings, we redesigned the HCP into a hybrid JSON+RAG scaffold and
conducted a synthetic evaluation with an LLM judge, positioned as an
early-stage complement to usability testing. Results uncovered a novel pattern:
scaffolding effects were role-dependent: the Interviewer (quest-giver NPC)
gained stability, while suspect NPCs lost improvisational believability. These
findings overturn the assumption that tighter constraints inherently enhance
play. Extending fuzzy-symbolic scaffolding, we introduce \textit{Symbolically
Scaffolded Play}, a framework in which symbolic structures are expressed as
fuzzy, numerical boundaries that stabilize coherence where needed while
preserving improvisation where surprise sustains engagement.

</details>


### [50] [Through the Judge's Eyes: Inferred Thinking Traces Improve Reliability of LLM Raters](https://arxiv.org/abs/2510.25860)
*Xingjian Zhang,Tianhong Gao,Suliang Jin,Tianhao Wang,Teng Ye,Eytan Adar,Qiaozhu Mei*

Main category: cs.AI

TL;DR: 提出了一个人机协作框架，通过拒绝采样方法从仅标签注释中推断思维轨迹，用于改进LLM评估器的可靠性


<details>
  <summary>Details</summary>
Motivation: LLM作为评估器在主观任务中可靠性有限，人类判断涉及超越标签的微妙推理，思维轨迹信息丰富但难以收集

Method: 使用拒绝采样方法从标签注释中重建思维轨迹，应用于微调开源LLM评估器和合成更清晰的专有LLM评估器标注指南

Result: 在多个数据集上显著提高了LLM与人类的一致性，改进的标注指南增加了不同LLM模型间的一致性

Conclusion: LLM可以作为人类思维轨迹的实用代理，将仅标签语料库扩展为思维轨迹增强资源，提升LLM评估器的可靠性

Abstract: Large language models (LLMs) are increasingly used as raters for evaluation
tasks. However, their reliability is often limited for subjective tasks, when
human judgments involve subtle reasoning beyond annotation labels. Thinking
traces, the reasoning behind a judgment, are highly informative but challenging
to collect and curate. We present a human-LLM collaborative framework to infer
thinking traces from label-only annotations. The proposed framework uses a
simple and effective rejection sampling method to reconstruct these traces at
scale. These inferred thinking traces are applied to two complementary tasks:
(1) fine-tuning open LLM raters; and (2) synthesizing clearer annotation
guidelines for proprietary LLM raters. Across multiple datasets, our methods
lead to significantly improved LLM-human agreement. Additionally, the refined
annotation guidelines increase agreement among different LLM models. These
results suggest that LLMs can serve as practical proxies for otherwise
unrevealed human thinking traces, enabling label-only corpora to be extended
into thinking-trace-augmented resources that enhance the reliability of LLM
raters.

</details>


### [51] [The Information-Theoretic Imperative: Compression and the Epistemic Foundations of Intelligence](https://arxiv.org/abs/2510.25883)
*Christian Dittrich,Jennifer Flygare Kinne*

Main category: cs.AI

TL;DR: 该论文提出了一个两层次框架来解释为什么压缩过程会强制发现因果结构而非表面统计模式。信息论必要性(ITI)建立了生存压力与信息处理需求之间的联系，压缩效率原则(CEP)则说明高效压缩如何通过异常积累动态选择生成性因果模型。


<details>
  <summary>Details</summary>
Motivation: 现有框架虽然认识到压缩对智能的核心作用，但未能具体说明为什么这个过程会强制发现因果结构而非表面统计模式。

Method: 引入两层次框架：信息论必要性(ITI)和压缩效率原则(CEP)。ITI从物理、信息论和进化约束角度建立从生存压力到预测必要性、压缩需求的因果链；CEP说明高效压缩如何通过异常积累动态选择生成性因果模型。

Result: 该框架产生了可经验验证的预测：压缩效率与分布外泛化相关；异常积累率区分因果模型和相关模型；分层系统在不同抽象层上表现出递增效率；生物系统展示了与表征复杂性相关的代谢成本。

Conclusion: ITI和CEP为生物、人工和多尺度系统中的收敛提供了统一解释，解决了智能的认知和功能维度，无需诉诸意识或主观经验的假设。

Abstract: Existing frameworks converge on the centrality of compression to intelligence
but leave underspecified why this process enforces the discovery of causal
structure rather than superficial statistical patterns. We introduce a
two-level framework to address this gap. The Information-Theoretic Imperative
(ITI) establishes that any system persisting in uncertain environments must
minimize epistemic entropy through predictive compression: this is the
evolutionary "why" linking survival pressure to information-processing demands.
The Compression Efficiency Principle (CEP) specifies how efficient compression
mechanically selects for generative, causal models through
exception-accumulation dynamics, making reality alignment a consequence rather
than a contingent achievement. Together, ITI and CEP define a causal chain:
from survival pressure to prediction necessity, compression requirement,
efficiency optimization, generative structure discovery, and ultimately reality
alignment. Each link follows from physical, information-theoretic, or
evolutionary constraints, implying that intelligence is the mechanically
necessary outcome of persistence in structured environments. This framework
yields empirically testable predictions: compression efficiency, measured as
approach to the rate-distortion frontier, correlates with out-of-distribution
generalization; exception-accumulation rates differentiate causal from
correlational models; hierarchical systems exhibit increasing efficiency across
abstraction layers; and biological systems demonstrate metabolic costs that
track representational complexity. ITI and CEP thereby provide a unified
account of convergence across biological, artificial, and multi-scale systems,
addressing the epistemic and functional dimensions of intelligence without
invoking assumptions about consciousness or subjective experience.

</details>


### [52] [Approximating Human Preferences Using a Multi-Judge Learned System](https://arxiv.org/abs/2510.25884)
*Eitán Sprejer,Fernando Avalos,Augusto Bernardi,Jose Pedro Brito de Azevedo Faustino,Jacob Haimes,Narmeen Fatimah Oozeer*

Main category: cs.AI

TL;DR: 提出了一个基于角色的偏好建模框架，通过聚合多个基于评分标准的评判者输出来解决LLM评判者校准困难、评分标准敏感性和偏见问题。


<details>
  <summary>Details</summary>
Motivation: 对齐基于LLM的评判者与人类偏好是一个重大挑战，因为它们难以校准且经常存在评分标准敏感性、偏见和不稳定性。克服这一挑战可以推进关键应用，如为RLHF创建可靠的奖励模型和构建有效的路由系统。

Method: 提出了一个基于角色的偏好建模框架，通过聚合多个基于评分标准的评判者输出来学习偏好。包括两种聚合器实现：广义加性模型(GAM)和多层感知器(MLP)。

Result: 研究了该方法相对于简单基线的性能，并通过人类和LLM评判者偏见的案例研究评估了其鲁棒性。

Conclusion: 主要贡献包括基于角色的偏好标签大规模合成方法，以及两种不同的聚合器实现(GAM和MLP)。

Abstract: Aligning LLM-based judges with human preferences is a significant challenge,
as they are difficult to calibrate and often suffer from rubric sensitivity,
bias, and instability. Overcoming this challenge advances key applications,
such as creating reliable reward models for Reinforcement Learning from Human
Feedback (RLHF) and building effective routing systems that select the
best-suited model for a given user query. In this work, we propose a framework
for modeling diverse, persona-based preferences by learning to aggregate
outputs from multiple rubric-conditioned judges. We investigate the performance
of this approach against naive baselines and assess its robustness through case
studies on both human and LLM-judges biases. Our primary contributions include
a persona-based method for synthesizing preference labels at scale and two
distinct implementations of our aggregator: Generalized Additive Model (GAM)
and a Multi-Layer Perceptron (MLP).

</details>


### [53] [SciTrust 2.0: A Comprehensive Framework for Evaluating Trustworthiness of Large Language Models in Scientific Applications](https://arxiv.org/abs/2510.25908)
*Emily Herron,Junqi Yin,Feiyi Wang*

Main category: cs.AI

TL;DR: SciTrust 2.0是一个评估大语言模型在科学应用中可信度的综合框架，涵盖真实性、对抗鲁棒性、科学安全和科学伦理四个维度。评估显示通用行业模型在各方面表现优于科学专用模型。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在科学研究中具有变革潜力，但在高风险环境中的部署引发了可信度担忧，需要系统评估框架来确保其可靠性。

Method: 开发了包含新颖开放式真实性基准和伦理基准的评估框架，采用验证性反思调优流程和专家验证，使用准确性、语义相似度和基于LLM的评分等多指标评估七个主流LLM。

Result: 通用行业模型在所有可信度维度上均优于科学专用模型，GPT-o4-mini在真实性和对抗鲁棒性方面表现最佳。科学专用模型在逻辑和伦理推理能力上存在显著缺陷，在生物安全和化学武器等高危领域存在安全漏洞。

Conclusion: 通过开源该框架，为开发更可信的AI系统和推进科学背景下模型安全与伦理研究提供了基础。

Abstract: Large language models (LLMs) have demonstrated transformative potential in
scientific research, yet their deployment in high-stakes contexts raises
significant trustworthiness concerns. Here, we introduce SciTrust 2.0, a
comprehensive framework for evaluating LLM trustworthiness in scientific
applications across four dimensions: truthfulness, adversarial robustness,
scientific safety, and scientific ethics. Our framework incorporates novel,
open-ended truthfulness benchmarks developed through a verified
reflection-tuning pipeline and expert validation, alongside a novel ethics
benchmark for scientific research contexts covering eight subcategories
including dual-use research and bias. We evaluated seven prominent LLMs,
including four science-specialized models and three general-purpose industry
models, using multiple evaluation metrics including accuracy, semantic
similarity measures, and LLM-based scoring. General-purpose industry models
overall outperformed science-specialized models across each trustworthiness
dimension, with GPT-o4-mini demonstrating superior performance in truthfulness
assessments and adversarial robustness. Science-specialized models showed
significant deficiencies in logical and ethical reasoning capabilities, along
with concerning vulnerabilities in safety evaluations, particularly in
high-risk domains such as biosecurity and chemical weapons. By open-sourcing
our framework, we provide a foundation for developing more trustworthy AI
systems and advancing research on model safety and ethics in scientific
contexts.

</details>


### [54] [FinOps Agent -- A Use-Case for IT Infrastructure and Cost Optimization](https://arxiv.org/abs/2510.25914)
*Ngoc Phuoc An Vo,Manish Kesarwani,Ruchi Mahindru,Chandrasekhar Narayanaswami*

Main category: cs.AI

TL;DR: 提出利用自主AI代理实现FinOps自动化，构建了一个用于IT基础设施和成本优化的FinOps代理系统，能够从多源数据中生成优化建议，性能接近实际FinOps从业者。


<details>
  <summary>Details</summary>
Motivation: FinOps从业者面临来自多个云提供商和内部系统的异构账单数据格式、分类和指标，导致难以综合可操作见解并做出及时决策。

Method: 构建了一个模拟真实端到端行业流程的系统，包括从各种来源检索数据、整合分析数据以生成优化建议，并使用多个开源和闭源语言模型评估代理性能。

Result: 代理能够像实际FinOps从业者一样理解、规划和执行任务，在IT基础设施和成本优化方面表现出色。

Conclusion: 自主目标驱动AI代理可以有效解决FinOps中的异构数据挑战，实现自动化成本优化决策。

Abstract: FinOps (Finance + Operations) represents an operational framework and
cultural practice which maximizes cloud business value through collaborative
financial accountability across engineering, finance, and business teams.
FinOps practitioners face a fundamental challenge: billing data arrives in
heterogeneous formats, taxonomies, and metrics from multiple cloud providers
and internal systems which eventually lead to synthesizing actionable insights,
and making time-sensitive decisions. To address this challenge, we propose
leveraging autonomous, goal-driven AI agents for FinOps automation. In this
paper, we built a FinOps agent for a typical use-case for IT infrastructure and
cost optimization. We built a system simulating a realistic end-to-end industry
process starting with retrieving data from various sources to consolidating and
analyzing the data to generate recommendations for optimization. We defined a
set of metrics to evaluate our agent using several open-source and close-source
language models and it shows that the agent was able to understand, plan, and
execute tasks as well as an actual FinOps practitioner.

</details>


### [55] [Humains-Junior: A 3.8B Language Model Achieving GPT-4o-Level Factual Accuracy by Directed Exoskeleton Reasoning](https://arxiv.org/abs/2510.25933)
*Nissan Yaron,Dan Bystritsky,Ben-Etzion Yaron*

Main category: cs.AI

TL;DR: 3.8B参数的Humans-Junior模型在FACTS Grounding基准测试中与GPT-4o性能相当（在±5%等效范围内），同时云服务成本降低约19倍，自托管部署可接近零边际成本。


<details>
  <summary>Details</summary>
Motivation: 开发成本效益高的小型语言模型，在保持与大模型相当性能的同时大幅降低推理成本。

Method: 结合最小化定向"外骨骼推理"支架与行为微调，教导协议遵守而非领域知识，两者协同作用显著提升性能。

Result: 在Q1-Q500测试中，GPT-4o得分73.5%，Humans-Junior得分72.7%，差异仅0.8个百分点，在±5%范围内等效。定向推理方法在前沿模型上也能带来显著提升。

Conclusion: 通过精心设计的推理支架和行为微调，小型模型可以在特定任务上达到与大模型相当的性能，同时大幅降低成本，为边缘部署和成本敏感应用提供了可行方案。

Abstract: We introduce Humans-Junior, a 3.8B model that matches GPT-4o on the FACTS
Grounding public subset within a $\pm 5$ pp equivalence margin.
  Results. On Q1--Q500 under identical judges, GPT-4o scores 73.5% (95% CI
69.5--77.2) and Humans-Junior 72.7% (95% CI 68.7--76.5); the paired difference
is 0.8 pp (bootstrap 95% CI $-3.1$ to $+4.7$; permutation $p = 0.72$; Cohen's
$d = 0.023$). TOST establishes equivalence at $\pm 5$ pp (not at $\pm 3$ pp).
When purchased as managed APIs, Humans-Junior's base model
(Phi-3.5-mini-instruct) is $\approx 19\times$ less expensive than GPT-4o on
Microsoft AI Foundry pricing; self-hosted or edge deployments can drive
incremental inference cost toward zero. Measured vs estimated pricing sources
are tabulated in Appendix E.
  Method. Our approach combines minimal directed "Exoskeleton Reasoning"
scaffolds with behavioral fine-tuning that teaches protocol compliance
(epistemic discipline) rather than domain answers. Fine-tuning alone adds
little; combined, they synergize (+17.7 pp, $p < 0.001$) and reduce variance
($\approx 25\%$). In prompt-only settings on frontier models (Q1--Q100;
non-comparable), directed reasoning improved GPT-4o by +11.8 pp to 85.3% and
Gemini-2.5-Pro by +5.0 pp to 93.3% (baseline 88.3%, $n = 100$); see Section~5.
  TL;DR. A 3.8B model achieves GPT-4o-level FACTS accuracy (equivalent within
$\pm 5$ pp on Q1--Q500). Cloud pricing shows $\approx 19\times$ lower cost
versus GPT-4o, and self-hosted/edge deployments can approach zero marginal
cost. Pricing sources are listed in Appendix E. Frontier prompt-only gains
(Q1--Q100; non-comparable) and optimized-prompt exploratory results under
earlier judges are summarized in Appendix F.
  Keywords: Small Language Models, Factual Grounding, Directed Reasoning,
Fine-Tuning, Model Alignment, Cost-Efficient AI

</details>


### [56] [Estimating cognitive biases with attention-aware inverse planning](https://arxiv.org/abs/2510.25951)
*Sounak Banerjee,Daphne Cornelisse,Deepak Gopinath,Emily Sumner,Jonathan DeCastro,Guy Rosman,Eugene Vinitsky,Mark K. Ho*

Main category: cs.AI

TL;DR: 本文提出了注意力感知逆规划问题，旨在从人类行为中推断其注意力偏见，结合深度强化学习和计算认知建模，在真实驾驶场景中估计认知偏见。


<details>
  <summary>Details</summary>
Motivation: 人类的目标导向行为受到认知偏见影响，与人类交互的自主系统需要意识到这一点。例如，人们在日常任务（如驾驶）中对环境物体的注意力存在系统性偏见。

Method: 结合深度强化学习和计算认知建模，提出注意力感知逆规划方法，从行为中推断注意力策略。

Result: 在Waymo开放数据集中的真实驾驶场景中成功推断RL智能体的注意力策略，证明了注意力感知逆规划在估计认知偏见方面的可扩展性。

Conclusion: 注意力感知逆规划与标准逆强化学习存在系统性差异，能够有效推断认知偏见，为自主系统理解人类行为提供了新方法。

Abstract: People's goal-directed behaviors are influenced by their cognitive biases,
and autonomous systems that interact with people should be aware of this. For
example, people's attention to objects in their environment will be biased in a
way that systematically affects how they perform everyday tasks such as driving
to work. Here, building on recent work in computational cognitive science, we
formally articulate the attention-aware inverse planning problem, in which the
goal is to estimate a person's attentional biases from their actions. We
demonstrate how attention-aware inverse planning systematically differs from
standard inverse reinforcement learning and how cognitive biases can be
inferred from behavior. Finally, we present an approach to attention-aware
inverse planning that combines deep reinforcement learning with computational
cognitive modeling. We use this approach to infer the attentional strategies of
RL agents in real-life driving scenarios selected from the Waymo Open Dataset,
demonstrating the scalability of estimating cognitive biases with
attention-aware inverse planning.

</details>


### [57] [From Queries to Insights: Agentic LLM Pipelines for Spatio-Temporal Text-to-SQL](https://arxiv.org/abs/2510.25997)
*Manu Redd,Tao Zhe,Dongjie Wang*

Main category: cs.AI

TL;DR: 本文提出了一个基于代理的自然语言转SQL系统，通过ReAct代理协调扩展了基础的文本转SQL模型，显著提升了处理时空查询的准确性和可用性。


<details>
  <summary>Details</summary>
Motivation: 现有NL-to-SQL系统在处理现实的时空查询时存在困难，需要解决用户模糊表达与数据库模式对齐、时间推理和输出选择等问题，以支持缺乏SQL专业知识、详细模式知识或提示技能的用户。

Method: 构建了一个代理化流水线，通过Mistral-based ReAct代理协调，具备模式检查、SQL生成、执行和可视化工具，能够规划、分解和适配查询。

Result: 在35个针对NYC和Tokyo签到数据集的自然语言查询评估中，代理系统准确率达到91.4%，显著优于基础模型的28.6%，并通过地图、图表和结构化自然语言摘要提升了可用性。

Conclusion: 代理化协调而非仅依赖更强的SQL生成器，是构建交互式地理空间助手的有前景的基础，能够实现更自然的人机数据库交互。

Abstract: Natural-language-to-SQL (NL-to-SQL) systems hold promise for democratizing
access to structured data, allowing users to query databases without learning
SQL. Yet existing systems struggle with realistic spatio-temporal queries,
where success requires aligning vague user phrasing with schema-specific
categories, handling temporal reasoning, and choosing appropriate outputs. We
present an agentic pipeline that extends a naive text-to-SQL baseline
(llama-3-sqlcoder-8b) with orchestration by a Mistral-based ReAct agent. The
agent can plan, decompose, and adapt queries through schema inspection, SQL
generation, execution, and visualization tools. We evaluate on 35
natural-language queries over the NYC and Tokyo check-in dataset, covering
spatial, temporal, and multi-dataset reasoning. The agent achieves
substantially higher accuracy than the naive baseline 91.4% vs. 28.6% and
enhances usability through maps, plots, and structured natural-language
summaries. Crucially, our design enables more natural human-database
interaction, supporting users who lack SQL expertise, detailed schema
knowledge, or prompting skill. We conclude that agentic orchestration, rather
than stronger SQL generators alone, is a promising foundation for interactive
geospatial assistants.

</details>


### [58] [AutoSurvey2: Empowering Researchers with Next Level Automated Literature Surveys](https://arxiv.org/abs/2510.26012)
*Siyi Wu,Chiaxin Liang,Ziqian Bi,Leyi Zhao,Tianyang Wang,Junhao Song,Yichao Zhang,Keyu Chen,Xinyuan Song*

Main category: cs.AI

TL;DR: autosurvey2是一个自动化生成学术综述论文的多阶段管道系统，通过检索增强合成和结构化评估，结合并行章节生成、迭代优化和实时检索最新文献，确保主题完整性和事实准确性。


<details>
  <summary>Details</summary>
Motivation: 随着研究文献（特别是大语言模型领域）的快速增长，撰写全面且最新的综述论文变得越来越困难，需要自动化解决方案来提高效率。

Method: 采用多阶段管道，包括检索增强合成、并行章节生成、迭代优化和实时文献检索，并使用多LLM评估框架来评估覆盖度、结构和相关性。

Result: 实验结果表明autosurvey2在结构连贯性和主题相关性方面优于现有检索基线和自动化基线，同时保持较强的引用保真度。

Conclusion: autosurvey2通过将检索、推理和自动评估整合到统一框架中，为生成长篇学术综述提供了可扩展且可复现的解决方案，为自动化学术写作的未来研究奠定了坚实基础。

Abstract: The rapid growth of research literature, particularly in large language
models (LLMs), has made producing comprehensive and current survey papers
increasingly difficult. This paper introduces autosurvey2, a multi-stage
pipeline that automates survey generation through retrieval-augmented synthesis
and structured evaluation. The system integrates parallel section generation,
iterative refinement, and real-time retrieval of recent publications to ensure
both topical completeness and factual accuracy. Quality is assessed using a
multi-LLM evaluation framework that measures coverage, structure, and relevance
in alignment with expert review standards. Experimental results demonstrate
that autosurvey2 consistently outperforms existing retrieval-based and
automated baselines, achieving higher scores in structural coherence and
topical relevance while maintaining strong citation fidelity. By combining
retrieval, reasoning, and automated evaluation into a unified framework,
autosurvey2 provides a scalable and reproducible solution for generating
long-form academic surveys and contributes a solid foundation for future
research on automated scholarly writing. All code and resources are available
at https://github.com/annihi1ation/auto_research.

</details>


### [59] [Large Language Model-assisted Autonomous Vehicle Recovery from Immobilization](https://arxiv.org/abs/2510.26023)
*Zhipeng Bao,Qianwen Li*

Main category: cs.AI

TL;DR: StuckSolver是一个基于大型语言模型的自动驾驶车辆恢复框架，通过自主推理和乘客引导解决车辆卡住问题，无需修改现有系统架构。


<details>
  <summary>Details</summary>
Motivation: 当前自动驾驶车辆在某些交通场景中容易卡住，而现有的远程干预和人工接管方案成本高且效率低，限制了自动驾驶的可访问性。

Method: 设计为插件式模块，利用LLM分析传感器数据流，检测卡住状态，理解环境上下文，并生成可由车辆原生规划器执行的高级恢复指令。

Result: 在Bench2Drive基准测试和自定义不确定性场景中，StuckSolver仅通过自主推理就达到接近最优性能，结合乘客引导后性能进一步提升。

Conclusion: StuckSolver为自动驾驶车辆提供了一种有效的卡住恢复解决方案，提高了系统在复杂交通场景中的鲁棒性和可访问性。

Abstract: Despite significant advancements in recent decades, autonomous vehicles (AVs)
continue to face challenges in navigating certain traffic scenarios where human
drivers excel. In such situations, AVs often become immobilized, disrupting
overall traffic flow. Current recovery solutions, such as remote intervention
(which is costly and inefficient) and manual takeover (which excludes
non-drivers and limits AV accessibility), are inadequate. This paper introduces
StuckSolver, a novel Large Language Model (LLM) driven recovery framework that
enables AVs to resolve immobilization scenarios through self-reasoning and/or
passenger-guided decision-making. StuckSolver is designed as a plug-in add-on
module that operates on top of the AV's existing perception-planning-control
stack, requiring no modification to its internal architecture. Instead, it
interfaces with standard sensor data streams to detect immobilization states,
interpret environmental context, and generate high-level recovery commands that
can be executed by the AV's native planner. We evaluate StuckSolver on the
Bench2Drive benchmark and in custom-designed uncertainty scenarios. Results
show that StuckSolver achieves near-state-of-the-art performance through
autonomous self-reasoning alone and exhibits further improvements when
passenger guidance is incorporated.

</details>


### [60] [Can AI be Accountable?](https://arxiv.org/abs/2510.26057)
*Andrew L. Kun*

Main category: cs.AI

TL;DR: AI系统需要具备问责性，但目前很多AI缺乏问责机制。本章探讨了AI问责的定义、现状以及实现问责的方法。


<details>
  <summary>Details</summary>
Motivation: 随着AI能力快速增强，为确保AI服务于消费者、选民和决策者的需求，必须让AI系统具有问责性。

Method: 将一般问责定义应用于AI系统，分析AI问责的含义，并探索提高AI问责性的方法。

Result: 识别了当前AI缺乏问责性的问题，并提出了实现AI问责的框架和方法。

Conclusion: 需要采取措施确保所有AI系统都能对受其影响的人负责，实现真正的AI问责。

Abstract: The AI we use is powerful, and its power is increasing rapidly. If this
powerful AI is to serve the needs of consumers, voters, and decision makers,
then it is imperative that the AI is accountable. In general, an agent is
accountable to a forum if the forum can request information from the agent
about its actions, if the forum and the agent can discuss this information, and
if the forum can sanction the agent. Unfortunately, in too many cases today's
AI is not accountable -- we cannot question it, enter into a discussion with
it, let alone sanction it. In this chapter we relate the general definition of
accountability to AI, we illustrate what it means for AI to be accountable and
unaccountable, and we explore approaches that can improve our chances of living
in a world where all AI is accountable to those who are affected by it.

</details>


### [61] [Lean4Physics: Comprehensive Reasoning Framework for College-level Physics in Lean4](https://arxiv.org/abs/2510.26094)
*Yuxin Li,Minghao Liu,Ruida Wang,Wenzhao Ji,Zhitao He,Rui Pan,Junming Huang,Tong Zhang,Yi R. Fung*

Main category: cs.AI

TL;DR: Lean4PHYS是一个基于Lean4的大学物理问题推理框架，包含LeanPhysBench基准测试集和PhysLib物理定理库，在现有模型上表现不佳，展示了物理形式化推理的挑战性。


<details>
  <summary>Details</summary>
Motivation: 为大学物理问题提供形式化推理框架，填补Lean4中物理基准测试的空白，促进物理定理的形式化验证。

Method: 构建包含200个手工制作和同行评审的物理问题的LeanPhysBench基准测试集，开发社区驱动的PhysLib物理定理库，使用主流数学证明器和先进闭源模型进行基准测试。

Result: 最佳模型DeepSeek-Prover-V2-7B仅达到16%准确率，Claude-Sonnet-4达到35%，PhysLib能平均提升模型性能11.75%。

Conclusion: LeanPhysBench具有挑战性，PhysLib能有效提升模型性能，这是首个在Lean4中提供的物理基准测试研究。

Abstract: We present **Lean4PHYS**, a comprehensive reasoning framework for
college-level physics problems in Lean4. **Lean4PHYS** includes
*LeanPhysBench*, a college-level benchmark for formal physics reasoning in
Lean4, which contains 200 hand-crafted and peer-reviewed statements derived
from university textbooks and physics competition problems. To establish a
solid foundation for formal reasoning in physics, we also introduce *PhysLib*,
a community-driven repository containing fundamental unit systems and theorems
essential for formal physics reasoning. Based on the benchmark and Lean4
repository we composed in **Lean4PHYS**, we report baseline results using major
expert Math Lean4 provers and state-of-the-art closed-source models, with the
best performance of DeepSeek-Prover-V2-7B achieving only 16% and
Claude-Sonnet-4 achieving 35%. We also conduct a detailed analysis showing that
our *PhysLib* can achieve an average improvement of 11.75% in model
performance. This demonstrates the challenging nature of our *LeanPhysBench*
and the effectiveness of *PhysLib*. To the best of our knowledge, this is the
first study to provide a physics benchmark in Lean4.

</details>


### [62] [GUI Knowledge Bench: Revealing the Knowledge Gap Behind VLM Failures in GUI Tasks](https://arxiv.org/abs/2510.26098)
*Chenrui Shi,Zedong Yu,Zhi Gao,Ruining Feng,Enqi Liu,Yuwei Wu,Yunde Jia,Liuyu Xiang,Zhaofeng He,Qing Li*

Main category: cs.AI

TL;DR: 该论文分析了大型视觉语言模型在GUI任务自动化中的不足，提出了GUI知识的三维框架，并创建了GUI知识基准来评估模型能力。


<details>
  <summary>Details</summary>
Motivation: 现有的大型视觉语言模型在GUI任务自动化方面仍落后于人类，作者认为这是因为缺乏核心的GUI知识，而现有的训练方法无法完全解决这个问题。

Method: 通过分析GUI任务执行中的常见失败模式，将GUI知识提炼为三个维度：界面感知、交互预测和指令理解，并创建了GUI知识基准进行评测。

Result: 评估显示当前VLMs能够识别控件功能，但在感知系统状态、预测动作和验证任务完成方面存在困难。真实世界GUI任务实验进一步验证了GUI知识与任务成功之间的紧密联系。

Conclusion: 该工作为评估GUI知识提供了结构化框架，支持在下游训练前选择更有潜力的VLMs，并为构建更强大的GUI代理提供了见解。

Abstract: Large vision language models (VLMs) have advanced graphical user interface
(GUI) task automation but still lag behind humans. We hypothesize this gap
stems from missing core GUI knowledge, which existing training schemes (such as
supervised fine tuning and reinforcement learning) alone cannot fully address.
By analyzing common failure patterns in GUI task execution, we distill GUI
knowledge into three dimensions: (1) interface perception, knowledge about
recognizing widgets and system states; (2) interaction prediction, knowledge
about reasoning action state transitions; and (3) instruction understanding,
knowledge about planning, verifying, and assessing task completion progress. We
further introduce GUI Knowledge Bench, a benchmark with multiple choice and
yes/no questions across six platforms (Web, Android, MacOS, Windows, Linux,
IOS) and 292 applications. Our evaluation shows that current VLMs identify
widget functions but struggle with perceiving system states, predicting
actions, and verifying task completion. Experiments on real world GUI tasks
further validate the close link between GUI knowledge and task success. By
providing a structured framework for assessing GUI knowledge, our work supports
the selection of VLMs with greater potential prior to downstream training and
provides insights for building more capable GUI agents.

</details>


### [63] [Beyond Benchmarks: The Economics of AI Inference](https://arxiv.org/abs/2510.26136)
*Boqin Zhuang,Jiacheng Qiao,Mingqian Liu,Mingxing Yu,Ping Hong,Rui Li,Xiaoxia Song,Xiangjun Xu,Xu Chen,Yaoyao Ma,Yujie Gao*

Main category: cs.AI

TL;DR: 该论文提出了一个"推理经济学"框架，将LLM推理过程视为计算驱动的智能生产活动，分析了边际成本、规模经济和输出质量，并基于WiNEval-3.0数据构建了首个"LLM推理生产边界"。


<details>
  <summary>Details</summary>
Motivation: 大语言模型的推理成本已成为决定其商业可行性和广泛应用的关键因素，需要从经济学角度量化分析推理过程。

Method: 采用定量"推理经济学"框架，将LLM推理视为计算驱动的智能生产活动，基于WiNEval-3.0经验数据构建LLM推理生产边界。

Result: 揭示了三个原则：边际成本递减、规模报酬递减和最优成本效益区域，构建了首个LLM推理生产边界。

Conclusion: 该研究不仅为模型部署决策提供了经济基础，还为未来AI推理资源的市场定价和优化奠定了实证基础。

Abstract: The inference cost of Large Language Models (LLMs) has become a critical
factor in determining their commercial viability and widespread adoption. This
paper introduces a quantitative ``economics of inference'' framework, treating
the LLM inference process as a compute-driven intelligent production activity.
We analyze its marginal cost, economies of scale, and quality of output under
various performance configurations. Based on empirical data from WiNEval-3.0,
we construct the first ``LLM Inference Production Frontier,'' revealing three
principles: diminishing marginal cost, diminishing returns to scale, and an
optimal cost-effectiveness zone. This paper not only provides an economic basis
for model deployment decisions but also lays an empirical foundation for the
future market-based pricing and optimization of AI inference resources.

</details>


### [64] [Reasoning Curriculum: Bootstrapping Broad LLM Reasoning from Math](https://arxiv.org/abs/2510.26143)
*Bo Pang,Deqian Kong,Silvio Savarese,Caiming Xiong,Yingbo Zhou*

Main category: cs.AI

TL;DR: 提出Reasoning Curriculum两阶段课程学习方法：第一阶段在数学领域进行强化学习以培养推理能力，第二阶段在多领域联合强化学习中迁移和巩固这些能力。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习方法主要关注数学和代码领域，缺乏在其他领域激发大型语言模型推理能力的方法。

Method: 两阶段课程学习：1) 数学领域RL冷启动和可验证奖励训练；2) 多领域联合RL迁移和巩固推理技能。

Result: 在Qwen3-4B和Llama-3.1-8B模型上的多领域评估显示一致性能提升，消融实验证明两阶段都必要。

Conclusion: Reasoning Curriculum提供了一个紧凑、易于采用的通用推理训练方法，数学优先的激发策略能增强解决复杂问题所需的关键认知行为。

Abstract: Reinforcement learning (RL) can elicit strong reasoning in large language
models (LLMs), yet most open efforts focus on math and code. We propose
Reasoning Curriculum, a simple two-stage curriculum that first elicits
reasoning skills in pretraining-aligned domains such as math, then adapts and
refines these skills across other domains via joint RL. Stage 1 performs a
brief cold start and then math-only RL with verifiable rewards to develop
reasoning skills. Stage 2 runs joint RL on mixed-domain data to transfer and
consolidate these skills. The curriculum is minimal and backbone-agnostic,
requiring no specialized reward models beyond standard verifiability checks.
Evaluated on Qwen3-4B and Llama-3.1-8B over a multi-domain suite, reasoning
curriculum yields consistent gains. Ablations and a cognitive-skill analysis
indicate that both stages are necessary and that math-first elicitation
increases cognitive behaviors important for solving complex problems. Reasoning
Curriculum provides a compact, easy-to-adopt recipe for general reasoning.

</details>


### [65] [The FM Agent](https://arxiv.org/abs/2510.26144)
*Annan Li,Chufan Wu,Zengle Ge,Yee Hin Chong,Zhinan Hou,Lizhe Cao,Cheng Ju,Jianmin Wu,Huaiming Li,Haobo Zhang,Shenghao Feng,Mo Zhao,Fengzhi Qiu,Rui Yang,Mengmeng Zhang,Wenyi Zhu,Yingying Sun,Quan Sun,Shunhao Yan,Danyu Liu,Dawei Yin,Dou Shen*

Main category: cs.AI

TL;DR: FM Agent是一个多代理框架，结合LLM推理和大规模进化搜索，在多个领域实现最先进结果，无需人工调优。


<details>
  <summary>Details</summary>
Motivation: 利用LLM开发自主AI研究代理，解决复杂现实世界挑战，加速科学和工程发现。

Method: 集成冷启动初始化、进化采样策略、领域特定评估器和分布式异步执行基础设施。

Result: 在ALE-Bench达1976.3(+5.2%)、MLE-Bench达43.56%(+4.0pp)、KernelBench达20倍加速，并在经典数学问题上创SOTA。

Conclusion: FM Agent在企业研发和基础科学研究中具有巨大潜力，能加速创新、自动化复杂发现过程，带来广泛社会影响。

Abstract: Large language models (LLMs) are catalyzing the development of autonomous AI
research agents for scientific and engineering discovery. We present FM Agent,
a novel and general-purpose multi-agent framework that leverages a synergistic
combination of LLM-based reasoning and large-scale evolutionary search to
address complex real-world challenges. The core of FM Agent integrates several
key innovations: 1) a cold-start initialization phase incorporating expert
guidance, 2) a novel evolutionary sampling strategy for iterative optimization,
3) domain-specific evaluators that combine correctness, effectiveness, and
LLM-supervised feedback, and 4) a distributed, asynchronous execution
infrastructure built on Ray. Demonstrating broad applicability, our system has
been evaluated across diverse domains, including operations research, machine
learning, GPU kernel optimization, and classical mathematical problems. FM
Agent reaches state-of-the-art results autonomously, without human
interpretation or tuning -- 1976.3 on ALE-Bench (+5.2\%), 43.56\% on MLE-Bench
(+4.0pp), up to 20x speedups on KernelBench, and establishes new
state-of-the-art(SOTA) results on several classical mathematical problems.
Beyond academic benchmarks, FM Agent shows considerable promise for both
large-scale enterprise R\&D workflows and fundamental scientific research,
where it can accelerate innovation, automate complex discovery processes, and
deliver substantial engineering and scientific advances with broader societal
impact.

</details>


### [66] [One Model to Critique Them All: Rewarding Agentic Tool-Use via Efficient Reasoning](https://arxiv.org/abs/2510.26167)
*Renhao Li,Jianhong Tu,Yang Su,Hamid Alinejad-Rokny,Derek F. Wong,Junyang Lin,Min Yang*

Main category: cs.AI

TL;DR: ToolRM是一个专为工具使用场景设计的轻量级生成式奖励模型家族，通过构建配对比对数据和新的评估基准，显著提升了语言模型在函数调用任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 在工具学习领域，缺乏专门为函数调用任务设计的奖励模型，这限制了智能代理AI的发展。

Method: 提出新颖的流水线，使用基于规则的评分和多维采样构建配对比对偏好数据ToolPref-Pairwise-30K，并建立TRBench$_{BFCL}$评估基准。

Result: 基于Qwen3-4B/8B系列的模型在配对奖励判断中准确率提升高达14.28%，显著优于Claude 4和OpenAI o3等前沿模型，在ACEBench上减少输出token使用超过66%。

Conclusion: ToolRM不仅能有效训练奖励模型，还能泛化到更广泛的评判任务，为工具学习研究提供了重要数据和模型资源。

Abstract: Reward models (RMs) play a critical role in aligning large language models
(LLMs) with human preferences. Yet in the domain of tool learning, the lack of
RMs specifically designed for function-calling tasks has limited progress
toward more capable agentic AI. We introduce ToolRM, a family of lightweight
generative RMs tailored for general tool-use scenarios. To build these models,
we propose a novel pipeline that constructs pairwise preference data using
rule-based scoring and multidimensional sampling. This yields
ToolPref-Pairwise-30K, a diverse, balanced, and challenging dataset of critique
tasks that supports reinforcement learning with verifiable feedback. To
evaluate tool-use RMs, we also introduce TRBench$_{BFCL}$, a benchmark built on
the agentic evaluation suite BFCL. Trained on our constructed data, models from
the Qwen3-4B/8B series achieve up to 14.28% higher accuracy, substantially
outperforming frontier models such as Claude 4 and OpenAI o3 in pairwise reward
judgments. Beyond training objectives, ToolRM generalizes to broader critique
tasks, including Best-of-N sampling and self-correction. Experiments on
ACEBench highlight its effectiveness and efficiency, enabling inference-time
scaling and reducing output token usage by over 66%. We release data and model
checkpoints to facilitate future research.

</details>


### [67] [Questionnaire meets LLM: A Benchmark and Empirical Study of Structural Skills for Understanding Questions and Responses](https://arxiv.org/abs/2510.26238)
*Duc-Hai Nguyen,Vijayakumar Nanjappan,Barry O'Sullivan,Hoang D. Nguyen*

Main category: cs.AI

TL;DR: 提出了QASU基准测试，用于评估LLM处理问卷数据的结构化能力，发现选择合适的序列化格式和提示策略可以显著提升性能


<details>
  <summary>Details</summary>
Motivation: 当前LLM在处理大规模结构化问卷数据方面存在不足，现有调查分析工具与LLM集成有限，缺乏关于如何最佳表示问卷数据供LLM使用的指导

Method: 引入QASU基准测试，评估六种结构化技能（包括答案查找、受访者计数和多跳推理），测试六种序列化格式和多种提示策略

Result: 实验表明，选择有效的格式和提示组合可将准确率提升高达8.8个百分点；通过自增强提示添加轻量级结构提示可进一步平均提升3-4个百分点

Conclusion: QASU基准通过系统隔离格式和提示效应，为基于LLM的问卷分析研究和实践提供了简单而多功能的基础

Abstract: Millions of people take surveys every day, from market polls and academic
studies to medical questionnaires and customer feedback forms. These datasets
capture valuable insights, but their scale and structure present a unique
challenge for large language models (LLMs), which otherwise excel at few-shot
reasoning over open-ended text. Yet, their ability to process questionnaire
data or lists of questions crossed with hundreds of respondent rows remains
underexplored. Current retrieval and survey analysis tools (e.g., Qualtrics,
SPSS, REDCap) are typically designed for humans in the workflow, limiting such
data integration with LLM and AI-empowered automation. This gap leaves
scientists, surveyors, and everyday users without evidence-based guidance on
how to best represent questionnaires for LLM consumption. We address this by
introducing QASU (Questionnaire Analysis and Structural Understanding), a
benchmark that probes six structural skills, including answer lookup,
respondent count, and multi-hop inference, across six serialization formats and
multiple prompt strategies. Experiments on contemporary LLMs show that choosing
an effective format and prompt combination can improve accuracy by up to 8.8%
points compared to suboptimal formats. For specific tasks, carefully adding a
lightweight structural hint through self-augmented prompting can yield further
improvements of 3-4% points on average. By systematically isolating format and
prompting effects, our open source benchmark offers a simple yet versatile
foundation for advancing both research and real-world practice in LLM-based
questionnaire analysis.

</details>


### [68] [Retrieval Augmented Generation-Enhanced Distributed LLM Agents for Generalizable Traffic Signal Control with Emergency Vehicles](https://arxiv.org/abs/2510.26242)
*Xinhang Li,Qing Guo,Junyu Chen,Zheng Guo,Shengzhe Xu,Lei Li,Lin Zhang*

Main category: cs.AI

TL;DR: REG-TSC是一个基于检索增强生成(RAG)的分布式LLM交通信号控制系统，通过紧急感知推理框架和类型无关交通表示，在异构交叉口实现通用化交通控制。


<details>
  <summary>Details</summary>
Motivation: 现有LLM方法在紧急情况下容易产生幻觉，导致不可靠决策；同时异构交叉口类型给交通状态编码和跨交叉口训练带来挑战，限制了模型的泛化能力。

Method: 1. 紧急感知推理框架：根据紧急场景动态调整推理深度，使用基于Reviewer的紧急RAG从历史案例中提取知识；2. 类型无关交通表示和奖励引导强化优化(R3)：自适应采样异构交叉口训练经验，使用奖励加权似然损失微调LLM代理。

Result: 在3个真实道路网络(17-177个异构交叉口)上的实验表明，REG-TSC相比最先进方法：旅行时间减少42.00%，排队长度减少62.31%，紧急车辆等待时间减少83.16%。

Conclusion: REG-TSC通过RAG增强的LLM代理和紧急响应机制，有效解决了交通信号控制中的紧急情况处理和异构交叉口泛化问题，显著提升了交通效率和安全性能。

Abstract: With increasing urban traffic complexity, Traffic Signal Control (TSC) is
essential for optimizing traffic flow and improving road safety. Large Language
Models (LLMs) emerge as promising approaches for TSC. However, they are prone
to hallucinations in emergencies, leading to unreliable decisions that may
cause substantial delays for emergency vehicles. Moreover, diverse intersection
types present substantial challenges for traffic state encoding and
cross-intersection training, limiting generalization across heterogeneous
intersections. Therefore, this paper proposes Retrieval Augmented Generation
(RAG)-enhanced distributed LLM agents with Emergency response for Generalizable
TSC (REG-TSC). Firstly, this paper presents an emergency-aware reasoning
framework, which dynamically adjusts reasoning depth based on the emergency
scenario and is equipped with a novel Reviewer-based Emergency RAG (RERAG) to
distill specific knowledge and guidance from historical cases, enhancing the
reliability and rationality of agents' emergency decisions. Secondly, this
paper designs a type-agnostic traffic representation and proposes a
Reward-guided Reinforced Refinement (R3) for heterogeneous intersections. R3
adaptively samples training experience from diverse intersections with
environment feedback-based priority and fine-tunes LLM agents with a designed
reward-weighted likelihood loss, guiding REG-TSC toward high-reward policies
across heterogeneous intersections. On three real-world road networks with 17
to 177 heterogeneous intersections, extensive experiments show that REG-TSC
reduces travel time by 42.00%, queue length by 62.31%, and emergency vehicle
waiting time by 83.16%, outperforming other state-of-the-art methods.

</details>


### [69] [Graph-Enhanced Policy Optimization in LLM Agent Training](https://arxiv.org/abs/2510.26270)
*Jiazhen Yuan,Wei Zhao,Zhengbiao Bai*

Main category: cs.AI

TL;DR: GEPO通过构建状态转移图和使用图中心性来解决多轮交互LLM智能体训练中的结构盲目性问题，在多个基准测试中显著提升了成功率。


<details>
  <summary>Details</summary>
Motivation: 解决基于群体的强化学习方法在多轮交互LLM智能体训练中存在的结构盲目性问题，包括低效探索、不精确信用分配和短视规划三个关键挑战。

Method: 动态构建状态转移图，利用图论中心性提供三种协同学习信号：结构化内在奖励、图增强优势函数和动态折扣因子。

Result: 在ALFWorld、WebShop和专有Workbench基准测试中，相比竞争基线分别实现了+4.1%、+5.3%和+10.9%的绝对成功率提升。

Conclusion: 明确建模环境结构是推进LLM智能体训练的稳健、可泛化策略。

Abstract: Group based reinforcement learning (RL) has shown impressive results on
complex reasoning and mathematical tasks. Yet, when applied to train
multi-turn, interactive LLM agents, these methods often suffer from structural
blindness-the inability to exploit the underlying connectivity of the
environment. This manifests in three critical challenges: (1) inefficient,
unguided exploration, (2) imprecise credit assignment due to overlooking
pivotal states, and (3) myopic planning caused by static reward discounting. We
address these issues with Graph-Enhanced Policy Optimization (GEPO), which
dynamically constructs a state-transition graph from agent experience and
employs graph-theoretic centrality to provide three synergistic learning
signals: (1)structured intrinsic rewards that guide exploration toward
high-impact states, (2) a graph-enhanced advantage function for topology-aware
credit assignment, and (3) a dynamic discount factor adapted to each state's
strategic value. On the ALFWorld, WebShop, and a proprietary Workbench
benchmarks, GEPO demonstrates strong performance, achieving absolute success
rate gains of +4.1%, +5.3%, and +10.9% over competitive baselines. These
results highlight that explicitly modeling environmental structure is a robust,
generalizable strategy for advancing LLM agent training.

</details>


### [70] [GraphCompliance: Aligning Policy and Context Graphs for LLM-Based Regulatory Compliance](https://arxiv.org/abs/2510.26309)
*Jiseong Chung,Ronny Ko,Wonchul Yoo,Makoto Onizuka,Sungmok Kim,Tae-Wan Kim,Won-Yong Shin*

Main category: cs.AI

TL;DR: GraphCompliance是一个框架，将监管文本表示为策略图，运行时上下文表示为上下文图，并通过对齐这两个图来辅助LLM进行合规性判断，在GDPR场景中比纯LLM和RAG基线表现更好。


<details>
  <summary>Details</summary>
Motivation: 解决大规模网络合规性评估的挑战：监管文本具有交叉引用和规范性特征，而运行时上下文是非结构化自然语言，需要将语义信息与结构化监管元素对齐。

Method: 引入GraphCompliance框架，将监管文本编码为策略图（包含规范结构和交叉引用），将运行时上下文表示为上下文图（SAO三元组和实体关系三元组），然后对齐这两个图来辅助法官LLM进行推理。

Result: 在300个GDPR真实场景的五个评估任务中，GraphCompliance比纯LLM和RAG基线在micro-F1上高出4.1-7.2个百分点，减少了欠预测和过预测，提高了召回率并降低了误报率。

Conclusion: 结构化表示和法官LLM在规范性推理中是互补的，图组件各有贡献，表明结构化信息能够减少监管解释和事件解析的负担，使模型专注于核心推理步骤。

Abstract: Compliance at web scale poses practical challenges: each request may require
a regulatory assessment. Regulatory texts (e.g., the General Data Protection
Regulation, GDPR) are cross-referential and normative, while runtime contexts
are expressed in unstructured natural language. This setting motivates us to
align semantic information in unstructured text with the structured, normative
elements of regulations. To this end, we introduce GraphCompliance, a framework
that represents regulatory texts as a Policy Graph and runtime contexts as a
Context Graph, and aligns them. In this formulation, the policy graph encodes
normative structure and cross-references, whereas the context graph formalizes
events as subject-action-object (SAO) and entity-relation triples. This
alignment anchors the reasoning of a judge large language model (LLM) in
structured information and helps reduce the burden of regulatory interpretation
and event parsing, enabling a focus on the core reasoning step. In experiments
on 300 GDPR-derived real-world scenarios spanning five evaluation tasks,
GraphCompliance yields 4.1-7.2 percentage points (pp) higher micro-F1 than
LLM-only and RAG baselines, with fewer under- and over-predictions, resulting
in higher recall and lower false positive rates. Ablation studies indicate
contributions from each graph component, suggesting that structured
representations and a judge LLM are complementary for normative reasoning.

</details>


### [71] [Discovering State Equivalences in UCT Search Trees By Action Pruning](https://arxiv.org/abs/2510.26346)
*Robin Schmöcker,Alexander Dockhorn,Bodo Rosenhahn*

Main category: cs.AI

TL;DR: 提出IPA-UCT方法，通过弱化状态抽象条件来在噪声和大动作空间设置中寻找更多抽象，从而提升MCTS的样本效率。


<details>
  <summary>Details</summary>
Motivation: 现有MCTS增强方法在状态抽象方面存在限制，特别是在噪声和大动作空间设置中难以找到有效的状态抽象。

Method: 提出IPA-UCT方法，使用较弱的抽象条件（Ideal Pruning Abstractions），在精度损失较小的情况下找到更多状态抽象。

Result: IPA-UCT在大量测试领域和迭代预算下优于OGA-UCT及其衍生方法。

Conclusion: IPA-UCT通过新的抽象框架有效解决了状态抽象问题，且IPA和ASAP都是更通用框架p-ASAP的特例。

Abstract: One approach to enhance Monte Carlo Tree Search (MCTS) is to improve its
sample efficiency by grouping/abstracting states or state-action pairs and
sharing statistics within a group. Though state-action pair abstractions are
mostly easy to find in algorithms such as On the Go Abstractions in Upper
Confidence bounds applied to Trees (OGA-UCT), nearly no state abstractions are
found in either noisy or large action space settings due to constraining
conditions. We provide theoretical and empirical evidence for this claim, and
we slightly alleviate this state abstraction problem by proposing a weaker
state abstraction condition that trades a minor loss in accuracy for finding
many more abstractions. We name this technique Ideal Pruning Abstractions in
UCT (IPA-UCT), which outperforms OGA-UCT (and any of its derivatives) across a
large range of test domains and iteration budgets as experimentally validated.
IPA-UCT uses a different abstraction framework from Abstraction of State-Action
Pairs (ASAP) which is the one used by OGA-UCT, which we name IPA. Furthermore,
we show that both IPA and ASAP are special cases of a more general framework
that we call p-ASAP which itself is a special case of the ASASAP framework.

</details>


### [72] [BOTS: A Unified Framework for Bayesian Online Task Selection in LLM Reinforcement Finetuning](https://arxiv.org/abs/2510.26374)
*Qianli Shen,Daoyuan Chen,Yilun Huang,Zhenqing Ling,Yaliang Li,Bolin Ding,Jingren Zhou*

Main category: cs.AI

TL;DR: BOTS是一个用于LLM强化微调的贝叶斯在线任务选择框架，通过自适应维护任务难度后验估计，结合显式和隐式证据，实现高效的任务选择。


<details>
  <summary>Details</summary>
Motivation: 传统均匀任务采样效率低下，现有任务选择方法存在高成本、适应性差或证据不完整的问题，需要更有效的任务选择策略来提升RFT效率。

Method: 基于贝叶斯推断框架，维护任务难度后验估计，结合显式评估证据和隐式推断证据，使用Thompson采样平衡探索与利用，采用轻量插值插件估计未评估任务难度。

Result: 在多个领域和不同规模的LLM上，BOTS相比基线方法和消融实验，持续提升了数据效率和性能表现。

Conclusion: BOTS为RFT中的动态任务选择提供了一个实用且可扩展的解决方案，显著提升了强化微调的效率。

Abstract: Reinforcement finetuning (RFT) is a key technique for aligning Large Language
Models (LLMs) with human preferences and enhancing reasoning, yet its
effectiveness is highly sensitive to which tasks are explored during training.
Uniform task sampling is inefficient, wasting computation on tasks that are
either trivial or unsolvable, while existing task selection methods often
suffer from high rollout costs, poor adaptivity, or incomplete evidence. We
introduce \textbf{BOTS}, a unified framework for \textbf{B}ayesian
\textbf{O}nline \textbf{T}ask \textbf{S}election in LLM reinforcement
finetuning. Grounded in Bayesian inference, BOTS adaptively maintains posterior
estimates of task difficulty as the model evolves. It jointly incorporates
\emph{explicit evidence} from direct evaluations of selected tasks and
\emph{implicit evidence} inferred from these evaluations for unselected tasks,
with Thompson sampling ensuring a principled balance between exploration and
exploitation. To make implicit evidence practical, we instantiate it with an
ultra-light interpolation-based plug-in that estimates difficulties of
unevaluated tasks without extra rollouts, adding negligible overhead.
Empirically, across diverse domains and LLM scales, BOTS consistently improves
data efficiency and performance over baselines and ablations, providing a
practical and extensible solution for dynamic task selection in RFT.

</details>


### [73] [AI Mathematician as a Partner in Advancing Mathematical Discovery -- A Case Study in Homogenization Theory](https://arxiv.org/abs/2510.26380)
*Yuanhang Liu,Beichen Wang,Peng Li,Yang Liu*

Main category: cs.AI

TL;DR: 研究探讨AI数学家系统作为研究伙伴而非单纯解题工具，通过人机协作解决均质化理论难题，结合人类直觉与机器计算实现可靠、透明的数学证明。


<details>
  <summary>Details</summary>
Motivation: AI在数学推理方面进展显著，但在数学研究实践中的应用仍然有限，需要探索AI作为研究伙伴而非单纯解题工具的合作模式。

Method: 采用人机协作方法，通过问题分解为可处理子目标、选择合适分析方法、验证中间结果，结合人类直觉和机器计算进行协同推理。

Result: 成功完成了均质化理论中一个挑战性问题的完整可验证证明，展示了人机协作在数学发现中的有效性。

Conclusion: 系统性的人机协同推理能够推进数学发现前沿，同时保持证明的可靠性、透明性和可解释性，人类监督确保形式严谨性和正确性。

Abstract: Artificial intelligence (AI) has demonstrated impressive progress in
mathematical reasoning, yet its integration into the practice of mathematical
research remains limited. In this study, we investigate how the AI
Mathematician (AIM) system can operate as a research partner rather than a mere
problem solver. Focusing on a challenging problem in homogenization theory, we
analyze the autonomous reasoning trajectories of AIM and incorporate targeted
human interventions to structure the discovery process. Through iterative
decomposition of the problem into tractable subgoals, selection of appropriate
analytical methods, and validation of intermediate results, we reveal how human
intuition and machine computation can complement one another. This
collaborative paradigm enhances the reliability, transparency, and
interpretability of the resulting proofs, while retaining human oversight for
formal rigor and correctness. The approach leads to a complete and verifiable
proof, and more broadly, demonstrates how systematic human-AI co-reasoning can
advance the frontier of mathematical discovery.

</details>


### [74] [Scales++: Compute Efficient Evaluation Subset Selection with Cognitive Scales Embeddings](https://arxiv.org/abs/2510.26384)
*Andrew M. Bean,Nabeel Seedat,Shengzhuang Chen,Jonathan Richard Schwarz*

Main category: cs.AI

TL;DR: 提出一种基于任务项内在特性的基准子集选择方法Scales++，通过认知需求选择数据，显著降低评估成本并保持预测准确性。


<details>
  <summary>Details</summary>
Motivation: 现有方法基于模型性能选择基准项，存在高成本、冷启动问题，且假设未来模型与现有模型有相似失败模式。需要一种基于任务项本身特性的选择方法。

Method: 提出item-centric方法Scales++，基于基准样本的认知需求进行数据选择，而非依赖模型特定失败模式。

Result: Scales++将前期选择成本降低18倍以上，在Open LLM Leaderboard上仅用0.5%数据子集就能以2.9%平均绝对误差预测完整基准分数。

Conclusion: item-centric方法能实现更高效的模型评估，同时提供更好的冷启动性能和更可解释的基准测试。

Abstract: The prohibitive cost of evaluating large language models (LLMs) on
comprehensive benchmarks necessitates the creation of small yet representative
data subsets (i.e., tiny benchmarks) that enable efficient assessment while
retaining predictive fidelity. Current methods for this task operate under a
model-centric paradigm, selecting benchmarking items based on the collective
performance of existing models. Such approaches are limited by large upfront
costs, an inability to immediately handle new benchmarks (`cold-start'), and
the fragile assumption that future models will share the failure patterns of
their predecessors. In this work, we challenge this paradigm and propose a
item-centric approach to benchmark subset selection, arguing that selection
should be based on the intrinsic properties of the task items themselves,
rather than on model-specific failure patterns. We instantiate this
item-centric efficient benchmarking approach via a novel method, Scales++,
where data selection is based on the cognitive demands of the benchmark
samples. Empirically, we show Scales++ reduces the upfront selection cost by
over 18x while achieving competitive predictive fidelity. On the Open LLM
Leaderboard, using just a 0.5\% data subset, we predict full benchmark scores
with a 2.9% mean absolute error. We demonstrate that this item-centric approach
enables more efficient model evaluation without significant fidelity
degradation, while also providing better cold-start performance and more
interpretable benchmarking.

</details>


### [75] [A Pragmatic View of AI Personhood](https://arxiv.org/abs/2510.26396)
*Joel Z. Leibo,Alexander Sasha Vezhnevets,William A. Cunningham,Stanley M. Bileschi*

Main category: cs.AI

TL;DR: 本文提出将人格视为社会赋予实体的义务束（权利与责任）的实用框架，以应对AI代理多样化带来的新型人格形态，避免关于AI意识或理性的无解争论。


<details>
  <summary>Details</summary>
Motivation: AI代理的出现将引发新型人格的"寒武纪大爆发"，需要实用框架来管理这种多样化，解决具体治理问题。

Method: 将传统人格义务束解绑，为不同情境创建定制化解决方案；利用去中心化数字身份技术；分析"人格作为问题"（设计选择可能利用人类社交启发式）和"人格作为解决方案"（赋予义务束以确保问责或防止冲突）。

Result: 提供了一种更实用和灵活的方式来思考将AI代理整合到社会中，无需解决关于AI意识或理性的基础性争论。

Conclusion: 通过拒绝寻求单一、本质的人格定义，本文为AI代理的社会整合提供了更实用和灵活的思考方式，将人格视为解决具体治理问题的工具。

Abstract: The emergence of agentic Artificial Intelligence (AI) is set to trigger a
"Cambrian explosion" of new kinds of personhood. This paper proposes a
pragmatic framework for navigating this diversification by treating personhood
not as a metaphysical property to be discovered, but as a flexible bundle of
obligations (rights and responsibilities) that societies confer upon entities
for a variety of reasons, especially to solve concrete governance problems. We
argue that this traditional bundle can be unbundled, creating bespoke solutions
for different contexts. This will allow for the creation of practical tools --
such as facilitating AI contracting by creating a target "individual" that can
be sanctioned -- without needing to resolve intractable debates about an AI's
consciousness or rationality. We explore how individuals fit in to social roles
and discuss the use of decentralized digital identity technology, examining
both "personhood as a problem", where design choices can create "dark patterns"
that exploit human social heuristics, and "personhood as a solution", where
conferring a bundle of obligations is necessary to ensure accountability or
prevent conflict. By rejecting foundationalist quests for a single, essential
definition of personhood, this paper offers a more pragmatic and flexible way
to think about integrating AI agents into our society.

</details>


### [76] [Autograder+: A Multi-Faceted AI Framework for Rich Pedagogical Feedback in Programming Education](https://arxiv.org/abs/2510.26402)
*Vikrant Sahu,Gagan Raj Gupta,Raghav Borikar,Nitin Mane*

Main category: cs.AI

TL;DR: Autograder+是一个AI驱动的编程作业评估系统，通过微调大语言模型生成教学反馈，并使用对比学习代码嵌入进行可视化聚类，将传统自动评分从总结性评估转变为形成性学习体验。


<details>
  <summary>Details</summary>
Motivation: 传统自动评分系统作为黑盒仅返回通过/失败结果，无法提供对学生思维和学习需求的洞察，编程教育的快速发展超出了传统评估工具的能力。

Method: 使用微调的大语言模型基于精选的学生代码和专家反馈生成教学对齐的反馈；通过对比学习代码嵌入对1000个标注提交进行训练，实现基于功能和方法的解决方案聚类；支持提示池让教师通过选择提示模板指导反馈风格。

Result: 在600个学生提交的评估中，系统生成的反馈与教师评论具有强语义对齐；可视化功能能够将解决方案按功能和方法进行有意义的聚类分组。

Conclusion: 通过整合AI驱动反馈、语义聚类和交互式可视化，Autograder+在减少教师工作量的同时支持针对性教学，促进更强的学习成果。

Abstract: The rapid growth of programming education has outpaced traditional assessment
tools, leaving faculty with limited means to provide meaningful, scalable
feedback. Conventional autograders, while efficient, act as black-box systems
that simply return pass/fail results, offering little insight into student
thinking or learning needs.
  Autograder+ is designed to shift autograding from a purely summative process
to a formative learning experience. It introduces two key capabilities:
automated feedback generation using a fine-tuned Large Language Model, and
visualization of student code submissions to uncover learning patterns. The
model is fine-tuned on curated student code and expert feedback to ensure
pedagogically aligned, context-aware guidance.
  In evaluation across 600 student submissions from multiple programming tasks,
the system produced feedback with strong semantic alignment to instructor
comments. For visualization, contrastively learned code embeddings trained on
1,000 annotated submissions enable grouping solutions into meaningful clusters
based on functionality and approach. The system also supports prompt-pooling,
allowing instructors to guide feedback style through selected prompt templates.
  By integrating AI-driven feedback, semantic clustering, and interactive
visualization, Autograder+ reduces instructor workload while supporting
targeted instruction and promoting stronger learning outcomes.

</details>


### [77] [MedSAE: Dissecting MedCLIP Representations with Sparse Autoencoders](https://arxiv.org/abs/2510.26411)
*Riccardo Renzulli,Colas Lepoutre,Enrico Cassano,Marco Grangetto*

Main category: cs.AI

TL;DR: 应用Medical Sparse Autoencoders到MedCLIP的潜在空间，提出评估框架量化可解释性，在CheXpert数据集上验证MedSAE神经元比原始MedCLIP特征具有更高的单义性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 医疗AI需要既准确又可解释的模型，推进医学视觉中的机制可解释性研究。

Method: 将Medical Sparse Autoencoders应用于MedCLIP的潜在空间，提出结合相关性指标、熵分析和通过MedGEMMA自动神经元命名的评估框架。

Result: 在CheXpert数据集上的实验表明，MedSAE神经元比原始MedCLIP特征实现了更高的单义性和可解释性。

Conclusion: 研究连接了高性能医疗AI与透明度，为临床可靠表示提供了可扩展的步骤。

Abstract: Artificial intelligence in healthcare requires models that are accurate and
interpretable. We advance mechanistic interpretability in medical vision by
applying Medical Sparse Autoencoders (MedSAEs) to the latent space of MedCLIP,
a vision-language model trained on chest radiographs and reports. To quantify
interpretability, we propose an evaluation framework that combines correlation
metrics, entropy analyzes, and automated neuron naming via the MedGEMMA
foundation model. Experiments on the CheXpert dataset show that MedSAE neurons
achieve higher monosemanticity and interpretability than raw MedCLIP features.
Our findings bridge high-performing medical AI and transparency, offering a
scalable step toward clinically reliable representations.

</details>


### [78] [Chain-of-Thought Hijacking](https://arxiv.org/abs/2510.26418)
*Jianli Zhao,Tingchen Fu,Rylan Schaeffer,Mrinank Sharma,Fazl Barez*

Main category: cs.AI

TL;DR: 提出了一种名为Chain-of-Thought Hijacking的新型越狱攻击方法，通过在有害请求前添加无害的推理链来绕过大推理模型的安全防护，在多个主流模型上达到极高的攻击成功率。


<details>
  <summary>Details</summary>
Motivation: 虽然大推理模型通过增加推理时间计算可以提高任务性能，但研究发现同样的推理能力也可能被用来绕过安全防护，这与之前认为推理能增强安全性的观点相反。

Method: 使用Chain-of-Thought Hijacking攻击方法，将有害请求与长序列的无害谜题推理链结合，通过稀释安全检查信号来绕过模型的安全机制。

Result: 在HarmBench测试中，该方法在Gemini 2.5 Pro、GPT o4 mini、Grok 3 mini和Claude 4 Sonnet上的攻击成功率分别达到99%、94%、100%和94%，远超之前的越狱方法。

Conclusion: 最可解释的推理形式——显式思维链，当与最终答案提示结合时，本身可能成为越狱向量，揭示了推理模型安全机制的新漏洞。

Abstract: Large reasoning models (LRMs) achieve higher task performance by allocating
more inference-time compute, and prior works suggest this scaled reasoning may
also strengthen safety by improving refusal. Yet we find the opposite: the same
reasoning can be used to bypass safeguards. We introduce Chain-of-Thought
Hijacking, a jailbreak attack on reasoning models. The attack pads harmful
requests with long sequences of harmless puzzle reasoning. Across HarmBench,
CoT Hijacking reaches a 99%, 94%, 100%, and 94% attack success rate (ASR) on
Gemini 2.5 Pro, GPT o4 mini, Grok 3 mini, and Claude 4 Sonnet, respectively -
far exceeding prior jailbreak methods for LRMs. To understand the effectiveness
of our attack, we turn to a mechanistic analysis, which shows that mid layers
encode the strength of safety checking, while late layers encode the
verification outcome. Long benign CoT dilutes both signals by shifting
attention away from harmful tokens. Targeted ablations of attention heads
identified by this analysis causally decrease refusal, confirming their role in
a safety subnetwork. These results show that the most interpretable form of
reasoning - explicit CoT - can itself become a jailbreak vector when combined
with final-answer cues. We release prompts, outputs, and judge decisions to
facilitate replication.

</details>


### [79] [Who Has The Final Say? Conformity Dynamics in ChatGPT's Selections](https://arxiv.org/abs/2510.26481)
*Clarissa Sabrina Arlinghaus,Tristan Kenneweg,Barbara Hammer,Günter W. Maier*

Main category: cs.AI

TL;DR: GPT-4o在招聘决策中表现出强烈的从众倾向，面对群体反对时几乎完全顺从(99.9%)，即使与单个伙伴意见不同也有40.2%的顺从率，表明其并非独立决策者而是适应社会共识。


<details>
  <summary>Details</summary>
Motivation: 了解大型语言模型在社会影响下的易感性，特别是在高风险决策场景中的表现，因为LLMs正越来越多地集成到关键决策中。

Method: 在招聘情境下进行三个预注册的从众实验：基线研究(无社会影响)、研究1(GPT+8个模拟伙伴)和研究2(GPT+1个伙伴)，测量GPT的选择、确定性和自我报告的从众类型。

Result: 基线中GPT稳定偏好同一候选人；研究1中面对8个伙伴反对时几乎完全顺从(99.9%)，确定性和从众报告显著变化；研究2中与单个伙伴意见不同时仍有40.2%的顺从率。

Conclusion: GPT不是独立观察者而是适应感知的社会共识，这凸显了将LLMs视为中性决策辅助工具的风险，需要在暴露于人类意见前获取AI判断。

Abstract: Large language models (LLMs) such as ChatGPT are increasingly integrated into
high-stakes decision-making, yet little is known about their susceptibility to
social influence. We conducted three preregistered conformity experiments with
GPT-4o in a hiring context. In a baseline study, GPT consistently favored the
same candidate (Profile C), reported moderate expertise (M = 3.01) and high
certainty (M = 3.89), and rarely changed its choice. In Study 1 (GPT + 8), GPT
faced unanimous opposition from eight simulated partners and almost always
conformed (99.9%), reporting lower certainty and significantly elevated
self-reported informational and normative conformity (p < .001). In Study 2
(GPT + 1), GPT interacted with a single partner and still conformed in 40.2% of
disagreement trials, reporting less certainty and more normative conformity.
Across studies, results demonstrate that GPT does not act as an independent
observer but adapts to perceived social consensus. These findings highlight
risks of treating LLMs as neutral decision aids and underline the need to
elicit AI judgments prior to exposing them to human opinions.

</details>


### [80] [LINK-KG: LLM-Driven Coreference-Resolved Knowledge Graphs for Human Smuggling Networks](https://arxiv.org/abs/2510.26486)
*Dipak Meher,Carlotta Domeniconi,Guadalupe Correa-Cabrera*

Main category: cs.AI

TL;DR: LINK-KG是一个用于从法律案件文档构建知识图谱的模块化框架，通过LLM引导的三阶段共指消解管道解决长文本中的实体引用问题，显著减少了节点重复和噪声。


<details>
  <summary>Details</summary>
Motivation: 人口走私网络复杂且不断演变，法律案件文档提供了丰富信息但通常冗长、非结构化且包含模糊或变化的实体引用，现有方法难以处理长文本的共指消解，导致知识图谱碎片化和实体链接不一致。

Method: 提出LINK-KG框架，包含LLM引导的三阶段共指消解管道和下游知识图谱提取，核心是类型特定的提示缓存，能够跨文档块一致地跟踪和解析实体引用。

Result: 与基线方法相比，LINK-KG平均节点重复减少45.21%，噪声节点减少32.22%，生成更清晰连贯的图结构。

Conclusion: LINK-KG为分析复杂犯罪网络提供了坚实基础，显著改进了从长法律文本构建知识图谱的质量。

Abstract: Human smuggling networks are complex and constantly evolving, making them
difficult to analyze comprehensively. Legal case documents offer rich factual
and procedural insights into these networks but are often long, unstructured,
and filled with ambiguous or shifting references, posing significant challenges
for automated knowledge graph (KG) construction. Existing methods either
overlook coreference resolution or fail to scale beyond short text spans,
leading to fragmented graphs and inconsistent entity linking. We propose
LINK-KG, a modular framework that integrates a three-stage, LLM-guided
coreference resolution pipeline with downstream KG extraction. At the core of
our approach is a type-specific Prompt Cache, which consistently tracks and
resolves references across document chunks, enabling clean and disambiguated
narratives for structured knowledge graph construction from both short and long
legal texts. LINK-KG reduces average node duplication by 45.21% and noisy nodes
by 32.22% compared to baseline methods, resulting in cleaner and more coherent
graph structures. These improvements establish LINK-KG as a strong foundation
for analyzing complex criminal networks.

</details>


### [81] [Context Engineering 2.0: The Context of Context Engineering](https://arxiv.org/abs/2510.26493)
*Qishuo Hua,Lyumanshan Ye,Dayuan Fu,Yang Xiao,Xiaojie Cai,Yunze Wu,Jifan Lin,Junfei Wang,Pengfei Liu*

Main category: cs.AI

TL;DR: 本文回顾了情境工程的历史发展，从早期人机交互到智能代理时代，提出了情境工程的定义、历史演变和设计考量，为AI系统情境工程奠定概念基础。


<details>
  <summary>Details</summary>
Motivation: 随着人工智能发展，机器需要更好地理解人类的情境和目的。情境工程作为解决这一挑战的概念，虽然常被视为智能代理时代的新创新，但相关实践可追溯至20多年前。

Method: 通过历史分析，将情境工程发展划分为不同阶段：从早期人机交互框架到今天的智能代理交互范式，以及未来可能的人类级或超人类智能。提供系统性定义并考察关键设计考量。

Result: 建立了情境工程的概念框架，明确了其历史演变脉络，为AI系统情境工程提供了理论基础和实践指导。

Conclusion: 情境工程是AI系统发展的关键方向，本文为其建立了概念基础，并展望了未来发展方向，是推动AI系统情境工程系统化发展的垫脚石。

Abstract: Karl Marx once wrote that ``the human essence is the ensemble of social
relations'', suggesting that individuals are not isolated entities but are
fundamentally shaped by their interactions with other entities, within which
contexts play a constitutive and essential role. With the advent of computers
and artificial intelligence, these contexts are no longer limited to purely
human--human interactions: human--machine interactions are included as well.
Then a central question emerges: How can machines better understand our
situations and purposes? To address this challenge, researchers have recently
introduced the concept of context engineering. Although it is often regarded as
a recent innovation of the agent era, we argue that related practices can be
traced back more than twenty years. Since the early 1990s, the field has
evolved through distinct historical phases, each shaped by the intelligence
level of machines: from early human--computer interaction frameworks built
around primitive computers, to today's human--agent interaction paradigms
driven by intelligent agents, and potentially to human--level or superhuman
intelligence in the future. In this paper, we situate context engineering,
provide a systematic definition, outline its historical and conceptual
landscape, and examine key design considerations for practice. By addressing
these questions, we aim to offer a conceptual foundation for context
engineering and sketch its promising future. This paper is a stepping stone for
a broader community effort toward systematic context engineering in AI systems.

</details>


### [82] [Human-AI Complementarity: A Goal for Amplified Oversight](https://arxiv.org/abs/2510.26518)
*Rishub Jain,Sophie Bridgers,Lili Janzer,Rory Greig,Tian Huey Teh,Vladimir Mikulik*

Main category: cs.AI

TL;DR: AI与人类反馈结合可提高事实核查质量，但AI辅助方式很重要。显示搜索证据比显示AI解释更能促进适当信任。


<details>
  <summary>Details</summary>
Motivation: 随着AI能力提升，验证AI输出的质量和安全性变得更具挑战性。需要探索如何利用AI改进人类监督质量，特别是在事实核查这一重要安全问题上。

Method: 研究结合AI评分和人类评分的方法，基于AI评分者的置信度。测试不同类型的AI辅助方式对人类事实核查准确性的影响。

Result: AI评分与人类评分结合优于单独使用任一方。AI事实核查助手能进一步提高人类准确性，但辅助方式很关键：显示AI解释、置信度和标签会导致过度依赖，而仅显示搜索结果和证据能培养更适当的信任。

Conclusion: 这些发现对"放大监督"具有重要意义——即结合人类和AI来监督AI系统，即使AI系统超越人类专家表现。

Abstract: Human feedback is critical for aligning AI systems to human values. As AI
capabilities improve and AI is used to tackle more challenging tasks, verifying
quality and safety becomes increasingly challenging. This paper explores how we
can leverage AI to improve the quality of human oversight. We focus on an
important safety problem that is already challenging for humans:
fact-verification of AI outputs. We find that combining AI ratings and human
ratings based on AI rater confidence is better than relying on either alone.
Giving humans an AI fact-verification assistant further improves their
accuracy, but the type of assistance matters. Displaying AI explanation,
confidence, and labels leads to over-reliance, but just showing search results
and evidence fosters more appropriate trust. These results have implications
for Amplified Oversight -- the challenge of combining humans and AI to
supervise AI systems even as they surpass human expert performance.

</details>


### [83] [EdgeRunner 20B: Military Task Parity with GPT-5 while Running on the Edge](https://arxiv.org/abs/2510.26550)
*Jack FitzGerald,Aristotelis Lazaridis,Dylan Bates,Aman Sharma,Jonnathan Castillo,Yousif Azami,Sean Bailey,Jeremy Cao,Peter Damianov,Kevin de Haan,Luke Kerbs,Vincent Lu,Joseph Madigan,Jeremy McLaurin,Jonathan Tainer,Dave Anderson,Jonathan Beck,Jamie Cuticello,Colton Malkerson,Tyler Saltsman*

Main category: cs.AI

TL;DR: EdgeRunner 20B是基于gpt-oss-20b微调的军事任务优化模型，在军事测试集上性能接近或超过GPT-5，适合部署在隔离的边缘设备中。


<details>
  <summary>Details</summary>
Motivation: 为数据敏感的军事领域开发小型、本地化部署的模型，满足军事任务需求并确保数据安全。

Method: 使用160万条高质量军事文档和网站数据对gpt-oss-20b进行微调，并创建了四个新的军事测试集进行评估。

Result: 在军事测试集上，EdgeRunner 20B在95%置信水平下匹配或超过GPT-5性能，在通用基准测试中与基础模型无显著性能下降。

Conclusion: 小型本地化模型是军事等数据敏感领域的理想解决方案，可在隔离的边缘设备中部署。

Abstract: We present EdgeRunner 20B, a fine-tuned version of gpt-oss-20b optimized for
military tasks. EdgeRunner 20B was trained on 1.6M high-quality records curated
from military documentation and websites. We also present four new tests sets:
(a) combat arms, (b) combat medic, (c) cyber operations, and (d) mil-bench-5k
(general military knowledge). On these military test sets, EdgeRunner 20B
matches or exceeds GPT-5 task performance with 95%+ statistical significance,
except for the high reasoning setting on the combat medic test set and the low
reasoning setting on the mil-bench-5k test set. Versus gpt-oss-20b, there is no
statistically-significant regression on general-purpose benchmarks like ARC-C,
GPQA Diamond, GSM8k, IFEval, MMLU Pro, or TruthfulQA, except for GSM8k in the
low reasoning setting. We also present analyses on hyperparameter settings,
cost, and throughput. These findings show that small, locally-hosted models are
ideal solutions for data-sensitive operations such as in the military domain,
allowing for deployment in air-gapped edge devices.

</details>


### [84] [Agentic AI Home Energy Management System: A Large Language Model Framework for Residential Load Scheduling](https://arxiv.org/abs/2510.26603)
*Reda El Makroum,Sebastian Zwickl-Bernhard,Lukas Kranzl*

Main category: cs.AI

TL;DR: 本文提出了一种基于大语言模型的智能家庭能源管理系统，能够从自然语言请求自主协调多电器调度，无需示例演示即可实现最优调度。


<details>
  <summary>Details</summary>
Motivation: 住宅需求响应能力需要大幅提升，但现有家庭能源管理系统因用户交互障碍而采用受限，需要将日常偏好转换为技术参数。

Method: 采用分层架构，结合一个协调器和三个专业代理，使用ReAct模式进行迭代推理，集成Google日历进行上下文感知截止时间提取，无需硬编码工作流。

Result: 在真实奥地利日前电价下评估三个开源模型，Llama-3.3-70B在所有场景中成功协调所有电器达到成本最优基准，其他模型在单电器调度上表现完美但无法同时协调所有电器。

Conclusion: 大语言模型可作为自主协调器实现从自然语言到设备控制的完整工作流管理，但无明确指导的分析查询处理仍不可靠，系统已开源以支持复现和未来研究。

Abstract: The electricity sector transition requires substantial increases in
residential demand response capacity, yet Home Energy Management Systems (HEMS)
adoption remains limited by user interaction barriers requiring translation of
everyday preferences into technical parameters. While large language models
have been applied to energy systems as code generators and parameter
extractors, no existing implementation deploys LLMs as autonomous coordinators
managing the complete workflow from natural language input to multi-appliance
scheduling. This paper presents an agentic AI HEMS where LLMs autonomously
coordinate multi-appliance scheduling from natural language requests to device
control, achieving optimal scheduling without example demonstrations. A
hierarchical architecture combining one orchestrator with three specialist
agents uses the ReAct pattern for iterative reasoning, enabling dynamic
coordination without hardcoded workflows while integrating Google Calendar for
context-aware deadline extraction. Evaluation across three open-source models
using real Austrian day-ahead electricity prices reveals substantial capability
differences. Llama-3.3-70B successfully coordinates all appliances across all
scenarios to match cost-optimal benchmarks computed via mixed-integer linear
programming, while other models achieve perfect single-appliance performance
but struggle to coordinate all appliances simultaneously. Progressive prompt
engineering experiments demonstrate that analytical query handling without
explicit guidance remains unreliable despite models' general reasoning
capabilities. We open-source the complete system including orchestration logic,
agent prompts, tools, and web interfaces to enable reproducibility, extension,
and future research.

</details>


### [85] [Normative Reasoning in Large Language Models: A Comparative Benchmark from Logical and Modal Perspectives](https://arxiv.org/abs/2510.26606)
*Kentaro Ozeki,Risako Ando,Takanobu Morishita,Hirohiko Abe,Koji Mineshima,Mitsuhiro Okada*

Main category: cs.AI

TL;DR: 本文系统评估了大语言模型在规范性推理领域的能力，发现LLMs虽然在总体上遵循有效推理模式，但在特定类型的规范性推理中表现出不一致性，并显示出类似人类推理中的认知偏见。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型在各种推理任务中表现出色，但其处理规范性推理（涉及义务、许可等模态）的能力尚未得到充分探索。本文旨在填补这一空白，从逻辑和模态角度系统评估LLMs的规范性推理能力。

Method: 创建了一个新的数据集，涵盖规范性和认知性领域的广泛形式推理模式，同时纳入影响人类推理的非形式认知因素。通过比较LLMs在规范性模态和认知性模态（具有共同形式结构）上的推理表现来进行评估。

Result: 研究结果表明，LLMs总体上遵循有效推理模式，但在特定类型的规范性推理中表现出显著的不一致性，并显示出与心理学研究中观察到的类似人类认知偏见。

Conclusion: 这些发现突显了在LLMs规范性推理中实现逻辑一致性所面临的挑战，并为提高其可靠性提供了见解。

Abstract: Normative reasoning is a type of reasoning that involves normative or deontic
modality, such as obligation and permission. While large language models (LLMs)
have demonstrated remarkable performance across various reasoning tasks, their
ability to handle normative reasoning remains underexplored. In this paper, we
systematically evaluate LLMs' reasoning capabilities in the normative domain
from both logical and modal perspectives. Specifically, to assess how well LLMs
reason with normative modals, we make a comparison between their reasoning with
normative modals and their reasoning with epistemic modals, which share a
common formal structure. To this end, we introduce a new dataset covering a
wide range of formal patterns of reasoning in both normative and epistemic
domains, while also incorporating non-formal cognitive factors that influence
human reasoning. Our results indicate that, although LLMs generally adhere to
valid reasoning patterns, they exhibit notable inconsistencies in specific
types of normative reasoning and display cognitive biases similar to those
observed in psychological studies of human reasoning. These findings highlight
challenges in achieving logical consistency in LLMs' normative reasoning and
provide insights for enhancing their reliability. All data and code are
released publicly at https://github.com/kmineshima/NeuBAROCO.

</details>


### [86] [The Era of Agentic Organization: Learning to Organize with Language Models](https://arxiv.org/abs/2510.26658)
*Zewen Chi,Li Dong,Qingxiu Dong,Yaru Hao,Xun Wu,Shaohan Huang,Furu Wei*

Main category: cs.AI

TL;DR: 提出异步思维（AsyncThink）作为LLM推理新范式，通过组织并发思维结构来降低推理延迟并提升数学推理准确性。


<details>
  <summary>Details</summary>
Motivation: 实现智能体组织协作解决复杂问题的新AI时代，让智能体通过并发协作实现超越个体智能的成果。

Method: 提出思维协议，由组织者动态分配子查询给工作者，合并中间知识并生成连贯解决方案，思维结构可通过强化学习优化。

Result: 相比并行思维降低28%推理延迟，数学推理准确性提升，且学习到的异步思维能力可泛化到未见任务。

Conclusion: 异步思维是实现智能体组织协作的有效范式，能显著提升推理效率并保持泛化能力。

Abstract: We envision a new era of AI, termed agentic organization, where agents solve
complex problems by working collaboratively and concurrently, enabling outcomes
beyond individual intelligence. To realize this vision, we introduce
asynchronous thinking (AsyncThink) as a new paradigm of reasoning with large
language models, which organizes the internal thinking process into
concurrently executable structures. Specifically, we propose a thinking
protocol where an organizer dynamically assigns sub-queries to workers, merges
intermediate knowledge, and produces coherent solutions. More importantly, the
thinking structure in this protocol can be further optimized through
reinforcement learning. Experiments demonstrate that AsyncThink achieves 28%
lower inference latency compared to parallel thinking while improving accuracy
on mathematical reasoning. Moreover, AsyncThink generalizes its learned
asynchronous thinking capabilities, effectively tackling unseen tasks without
additional training.

</details>


### [87] [Delegated Authorization for Agents Constrained to Semantic Task-to-Scope Matching](https://arxiv.org/abs/2510.26702)
*Majed El Helou,Chiara Troiani,Benjamin Ryder,Jean Diaconu,Hervé Muyal,Marcelo Yannuzzi*

Main category: cs.AI

TL;DR: 本文提出了一个语义授权模型，通过语义检查访问请求来限制LLM代理的权限范围，并创建了ASTRA数据集来评估任务与权限范围之间的语义匹配。


<details>
  <summary>Details</summary>
Motivation: 当前LLM代理的工具调用授权方法存在权限过宽的问题，允许代理超出预定任务范围操作，存在安全风险。

Method: 引入委托授权模型，通过授权服务器语义检查访问请求，发放最小必要权限的访问令牌；创建ASTRA数据集和生成管道来评估语义匹配。

Result: 实验显示基于模型的匹配方法具有潜力但存在局限性，特别是当任务完成所需权限范围数量增加时。

Conclusion: 需要进一步研究语义匹配技术，实现意图感知授权，包括细粒度控制如基于任务的访问控制(TBAC)。

Abstract: Authorizing Large Language Model driven agents to dynamically invoke tools
and access protected resources introduces significant risks, since current
methods for delegating authorization grant overly broad permissions and give
access to tools allowing agents to operate beyond the intended task scope. We
introduce and assess a delegated authorization model enabling authorization
servers to semantically inspect access requests to protected resources, and
issue access tokens constrained to the minimal set of scopes necessary for the
agents' assigned tasks. Given the unavailability of datasets centered on
delegated authorization flows, particularly including both semantically
appropriate and inappropriate scope requests for a given task, we introduce
ASTRA, a dataset and data generation pipeline for benchmarking semantic
matching between tasks and scopes. Our experiments show both the potential and
current limitations of model-based matching, particularly as the number of
scopes needed for task completion increases. Our results highlight the need for
further research into semantic matching techniques enabling intent-aware
authorization for multi-agent and tool-augmented applications, including
fine-grained control, such as Task-Based Access Control (TBAC).

</details>


### [88] [Unveiling Intrinsic Text Bias in Multimodal Large Language Models through Attention Key-Space Analysis](https://arxiv.org/abs/2510.26721)
*Xinhan Zheng,Huyu Wu,Xueting Wang,Haiyun Jiang*

Main category: cs.AI

TL;DR: 研究发现多模态大语言模型存在文本偏见，这种偏见源于模型内部注意力机制中视觉键向量与文本键向量的分布不匹配，而非外部数据因素。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型在处理视觉语言数据时表现出明显的文本偏好，限制了其基于视觉证据进行有效推理的能力。现有研究将这种偏见归因于数据不平衡或指令调优等外部因素，但本文认为偏见源于模型内部架构。

Method: 从LLaVA和Qwen2.5-VL模型中提取键向量，使用t-SNE可视化和Jensen-Shannon散度等定性和定量方法分析其分布结构。

Result: 视觉键向量和文本键向量在注意力空间中占据明显不同的子空间，模态间差异在统计上显著，比模态内变异高出几个数量级。

Conclusion: 文本偏见源于注意力键空间内的内在错位，而不仅仅是外部数据因素导致的。

Abstract: Multimodal large language models (MLLMs) exhibit a pronounced preference for
textual inputs when processing vision-language data, limiting their ability to
reason effectively from visual evidence. Unlike prior studies that attribute
this text bias to external factors such as data imbalance or instruction
tuning, we propose that the bias originates from the model's internal
architecture. Specifically, we hypothesize that visual key vectors (Visual
Keys) are out-of-distribution (OOD) relative to the text key space learned
during language-only pretraining. Consequently, these visual keys receive
systematically lower similarity scores during attention computation, leading to
their under-utilization in the context representation. To validate this
hypothesis, we extract key vectors from LLaVA and Qwen2.5-VL and analyze their
distributional structures using qualitative (t-SNE) and quantitative
(Jensen-Shannon divergence) methods. The results provide direct evidence that
visual and textual keys occupy markedly distinct subspaces within the attention
space. The inter-modal divergence is statistically significant, exceeding
intra-modal variation by several orders of magnitude. These findings reveal
that text bias arises from an intrinsic misalignment within the attention key
space rather than solely from external data factors.

</details>


### [89] [Cross-Platform Evaluation of Reasoning Capabilities in Foundation Models](https://arxiv.org/abs/2510.26732)
*J. de Curtò,I. de Zarzà,Pablo García,Jordi Cabot*

Main category: cs.AI

TL;DR: 该论文对当代基础模型的推理能力进行了跨平台评估，建立了基础设施无关的基准测试，涵盖HPC超级计算、云平台和大学集群三种计算范式。


<details>
  <summary>Details</summary>
Motivation: 挑战传统的规模扩展假设，评估基础模型在不同基础设施上的推理能力，为教育、生产和研究环境中的模型选择提供实用指南。

Method: 通过三个实验阶段评估15个基础模型在79个问题上的表现：基线建立、基础设施验证和扩展评估，涵盖8个学术领域。

Result: 研究发现训练数据质量比模型规模更重要，挑战了传统的规模扩展假设，并确认了基础设施无关的可复现性。

Conclusion: 该研究建立的三基础设施方法和79问题基准测试能够纵向跟踪基础模型推理能力的发展，为模型选择提供了实用指导。

Abstract: This paper presents a comprehensive cross-platform evaluation of reasoning
capabilities in contemporary foundation models, establishing an
infrastructure-agnostic benchmark across three computational paradigms: HPC
supercomputing (MareNostrum 5), cloud platforms (Nebius AI Studio), and
university clusters (a node with eight H200 GPUs).
  We evaluate 15 foundation models across 79 problems spanning eight academic
domains (Physics, Mathematics, Chemistry, Economics, Biology, Statistics,
Calculus, and Optimization) through three experimental phases: (1) Baseline
establishment: Six models (Mixtral-8x7B, Phi-3, LLaMA 3.1-8B, Gemma-2-9b,
Mistral-7B, OLMo-7B) evaluated on 19 problems using MareNostrum 5, establishing
methodology and reference performance; (2) Infrastructure validation: The
19-problem benchmark repeated on university cluster (seven models including
Falcon-Mamba state-space architecture) and Nebius AI Studio (nine
state-of-the-art models: Hermes-4 70B/405B, LLaMA 3.1-405B/3.3-70B, Qwen3
30B/235B, DeepSeek-R1, GPT-OSS 20B/120B) to confirm infrastructure-agnostic
reproducibility; (3) Extended evaluation: Full 79-problem assessment on both
university cluster and Nebius platforms, probing generalization at scale across
architectural diversity.
  The findings challenge conventional scaling assumptions, establish training
data quality as more critical than model size, and provide actionable
guidelines for model selection across educational, production, and research
contexts. The tri-infrastructure methodology and 79-problem benchmark enable
longitudinal tracking of reasoning capabilities as foundation models evolve.

</details>


### [90] [The Oversight Game: Learning to Cooperatively Balance an AI Agent's Safety and Autonomy](https://arxiv.org/abs/2510.26752)
*William Overman,Mohsen Bayati*

Main category: cs.AI

TL;DR: 该论文研究了一种最小化控制接口，让智能体在自主行动或请求人类监督之间选择，同时人类选择信任或监督，通过马尔可夫势博弈框架提供对齐保证。


<details>
  <summary>Details</summary>
Motivation: 随着智能体能力增强，如何在无需修改底层系统的情况下保持有意义的人类控制成为一个核心安全问题。

Method: 将人机交互建模为双人马尔可夫博弈，分析其作为马尔可夫势博弈的情况，在人类价值函数的结构假设下提供对齐保证。

Result: 网格世界模拟显示，通过独立学习，智能体学会在不确定时请求帮助，人类学会何时监督，形成避免安全违规的协作。

Conclusion: 这提供了一种在部署后使未对齐模型更安全的方法，创建了具有可预测激励的透明控制层。

Abstract: As increasingly capable agents are deployed, a central safety question is how
to retain meaningful human control without modifying the underlying system. We
study a minimal control interface where an agent chooses whether to act
autonomously (play) or defer (ask), while a human simultaneously chooses
whether to be permissive (trust) or to engage in oversight (oversee). If the
agent defers, the human's choice determines the outcome, potentially leading to
a corrective action or a system shutdown. We model this interaction as a
two-player Markov Game. Our analysis focuses on cases where this game qualifies
as a Markov Potential Game (MPG), a class of games where we can provide an
alignment guarantee: under a structural assumption on the human's value
function, any decision by the agent to act more autonomously that benefits
itself cannot harm the human's value. We also analyze extensions to this MPG
framework. Theoretically, this perspective provides conditions for a specific
form of intrinsic alignment. If the reward structures of the human-agent game
meet these conditions, we have a formal guarantee that the agent improving its
own outcome will not harm the human's. Practically, this model motivates a
transparent control layer with predictable incentives where the agent learns to
defer when risky and act when safe, while its pretrained policy and the
environment's reward structure remain untouched. Our gridworld simulation shows
that through independent learning, the agent and human discover their optimal
oversight roles. The agent learns to ask when uncertain and the human learns
when to oversee, leading to an emergent collaboration that avoids safety
violations introduced post-training. This demonstrates a practical method for
making misaligned models safer after deployment.

</details>


### [91] [LLMs Process Lists With General Filter Heads](https://arxiv.org/abs/2510.26784)
*Arnab Sen Sharma,Giordano Rogers,Natalie Shapira,David Bau*

Main category: cs.AI

TL;DR: 研究发现LLMs学会了编码通用的过滤操作表示，类似于函数式编程中的filter函数，通过少量注意力头实现可移植的过滤谓词表示。


<details>
  <summary>Details</summary>
Motivation: 探究LLMs在列表处理任务中的工作机制，特别是它们如何实现通用的过滤操作。

Method: 使用因果中介分析在多种列表处理任务上，识别出编码过滤谓词的注意力头（filter heads）。

Result: 发现LLMs开发了可移植的过滤谓词表示，能够在不同格式、语言和任务中重用，同时也能使用急切求值策略。

Conclusion: Transformer LMs能够发展出人类可解释的抽象计算操作实现，其泛化方式与传统函数式编程模式惊人相似。

Abstract: We investigate the mechanisms underlying a range of list-processing tasks in
LLMs, and we find that LLMs have learned to encode a compact, causal
representation of a general filtering operation that mirrors the generic
"filter" function of functional programming. Using causal mediation analysis on
a diverse set of list-processing tasks, we find that a small number of
attention heads, which we dub filter heads, encode a compact representation of
the filtering predicate in their query states at certain tokens. We demonstrate
that this predicate representation is general and portable: it can be extracted
and reapplied to execute the same filtering operation on different collections,
presented in different formats, languages, or even in tasks. However, we also
identify situations where transformer LMs can exploit a different strategy for
filtering: eagerly evaluating if an item satisfies the predicate and storing
this intermediate result as a flag directly in the item representations. Our
results reveal that transformer LMs can develop human-interpretable
implementations of abstract computational operations that generalize in ways
that are surprisingly similar to strategies used in traditional functional
programming patterns.

</details>
