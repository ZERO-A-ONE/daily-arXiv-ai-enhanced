<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 14]
- [cs.CR](#cs.CR) [Total: 19]
- [cs.AI](#cs.AI) [Total: 51]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [Beyond Single LLMs: Enhanced Code Generation via Multi-Stage Performance-Guided LLM Orchestration](https://arxiv.org/abs/2510.01379)
*Huashan Chen,Zhenyu Qi,Haotang Li,Hong Chen,Jinfu Chen,Kebin Peng,In Kee Kim,Kyu Hyung Lee,Sen He*

Main category: cs.SE

TL;DR: 提出了一种多阶段、性能导向的编排框架PerfOrch，通过动态路由编码任务到最适合的LLM，在无需微调的情况下显著提升代码生成正确性和运行时性能。


<details>
  <summary>Details</summary>
Motivation: 当前单一模型方法忽视了不同模型在编程语言、算法领域和开发阶段上的异构计算优势，需要一种能利用模型特长的动态编排方法。

Method: 基于对17个最先进LLM在5种编程语言上的实证研究，设计了一个生成-修复-优化的多阶段工作流，通过阶段验证和回滚机制动态选择最佳模型。

Result: 在HumanEval-X和EffiBench-X基准测试中分别达到96.22%和91.37%的正确率，显著超越GPT-4o，并在58.76%的问题上提升执行时间，中位加速达17.67%-27.66%。

Conclusion: PerfOrch提供了一个可扩展的即插即用架构，为生产级自动化软件工程提供了适应快速发展的生成式AI环境的范式。

Abstract: While Large Language Models (LLMs) have become the predominant paradigm for
automated code generation, current single-model approaches fundamentally ignore
the heterogeneous computational strengths that different models exhibit across
programming languages, algorithmic domains, and development stages. This paper
challenges the single-model convention by introducing a multi-stage,
performance-guided orchestration framework that dynamically routes coding tasks
to the most suitable LLMs within a structured generate-fix-refine workflow. Our
approach is grounded in a comprehensive empirical study of 17 state-of-the-art
LLMs across five programming languages (Python, Java, C++, Go, and Rust) using
HumanEval-X benchmark. The study, which evaluates both functional correctness
and runtime performance metrics (execution time, mean/max memory utilization,
and CPU efficiency), reveals pronounced performance heterogeneity by language,
development stage, and problem category. Guided by these empirical insights, we
present PerfOrch, an LLM agent that orchestrates top-performing LLMs for each
task context through stage-wise validation and rollback mechanisms. Without
requiring model fine-tuning, PerfOrch achieves substantial improvements over
strong single-model baselines: average correctness rates of 96.22% and 91.37%
on HumanEval-X and EffiBench-X respectively, surpassing GPT-4o's 78.66% and
49.11%. Beyond correctness gains, the framework delivers consistent performance
optimizations, improving execution time for 58.76% of problems with median
speedups ranging from 17.67% to 27.66% across languages on two benchmarks. The
framework's plug-and-play architecture ensures practical scalability, allowing
new LLMs to be profiled and integrated seamlessly, thereby offering a paradigm
for production-grade automated software engineering that adapts to the rapidly
evolving generative AI landscape.

</details>


### [2] [Deciphering WONTFIX: A Mixed-Method Study on Why GitHub Issues Get Rejected](https://arxiv.org/abs/2510.01514)
*J. Alexander Curtis,Sharadha Kasiviswanathan,Nasir Eisty*

Main category: cs.SE

TL;DR: 该研究分析了GitHub上wontfix标签的普遍性和使用原因，发现约30%的项目使用该标签，主要出现在用户提交的bug报告和功能请求中，并识别出8个常见的使用主题。


<details>
  <summary>Details</summary>
Motivation: wontfix标签在GitHub仓库中被广泛使用但理解有限，其对开源软件项目管理的影响尚不明确，需要系统研究其使用模式和影响。

Method: 采用混合方法，从GitHub上3,132个最受欢迎仓库收集数据，通过定量分析评估wontfix标签的普遍性，并通过开放式编码和主题分析对使用原因进行定性分类。

Result: 约30%的GitHub项目使用wontfix标签，主要应用于用户提交的bug报告和功能请求。研究识别出8个常见的使用主题，涵盖用户特定控制因素到维护者特定决策。

Conclusion: wontfix标签是GitHub项目中管理资源和引导贡献者努力的重要工具，但也可能抑制社区参与并影响项目管理的透明度。理解这些原因有助于项目管理者做出明智决策并促进开源社区的高效协作。

Abstract: Context: The ``wontfix'' label is a widely used yet narrowly understood tool
in GitHub repositories, indicating that an issue will not be pursued further.
Despite its prevalence, the impact of this label on project management and
community dynamics within open-source software development is not clearly
defined. Objective: This study examines the prevalence and reasons behind
issues being labeled as wontfix across various open-source repositories on
GitHub. Method: Employing a mixed-method approach, we analyze both quantitative
data to assess the prevalence of the wontfix label and qualitative data to
explore the reasoning that it was used. Data were collected from 3,132 of
GitHub's most-popular repositories. Later, we employ open coding and thematic
analysis to categorize the reasons behind wontfix labels, providing a
structured understanding of the issue management landscape. Results: Our
findings show that about 30% of projects on GitHub apply the wontfix label to
some issues. These issues most often occur on user-submitted issues for bug
reports and feature requests. The study identified eight common themes behind
labeling issues as wontfix, ranging from user-specific control factors to
maintainer-specific decisions. Conclusions: The wontfix label is a critical
tool for managing resources and guiding contributor efforts in GitHub projects.
However, it can also discourage community involvement and obscure the
transparency of project management. Understanding these reasons aids project
managers in making informed decisions and fostering efficient collaboration
within open-source communities.

</details>


### [3] [MIMIC: Integrating Diverse Personality Traits for Better Game Testing Using Large Language Model](https://arxiv.org/abs/2510.01635)
*Yifei Chen,Sarra Habchi,Lili Wei*

Main category: cs.SE

TL;DR: MIMIC是一个将多样化人格特征集成到游戏代理中的框架，使代理能在相似情境中采用不同的游戏策略，从而提高测试覆盖率和游戏交互多样性。


<details>
  <summary>Details</summary>
Motivation: 传统游戏测试代理往往忽视人类玩家因个性差异而采用的不同策略，导致在相似情境中产生重复的解决方案，难以触发多样化的游戏交互或发现边缘情况。

Method: MIMIC框架通过整合多样化的人格特征，使游戏代理能够模仿不同的游戏风格和策略。

Result: MIMIC在不同游戏中实现了更高的测试覆盖率和更丰富的游戏交互，在Minecraft中超越了现有最先进代理，获得了更高的任务完成率并提供了更多样化的解决方案。

Conclusion: MIMIC在游戏测试方面具有显著潜力，能够通过模仿多样化游戏策略来有效提升测试效果。

Abstract: Modern video games pose significant challenges for traditional automated
testing algorithms, yet intensive testing is crucial to ensure game quality. To
address these challenges, researchers designed gaming agents using
Reinforcement Learning, Imitation Learning, or Large Language Models. However,
these agents often neglect the diverse strategies employed by human players due
to their different personalities, resulting in repetitive solutions in similar
situations. Without mimicking varied gaming strategies, these agents struggle
to trigger diverse in-game interactions or uncover edge cases.
  In this paper, we present MIMIC, a novel framework that integrates diverse
personality traits into gaming agents, enabling them to adopt different gaming
strategies for similar situations. By mimicking different playstyles, MIMIC can
achieve higher test coverage and richer in-game interactions across different
games. It also outperforms state-of-the-art agents in Minecraft by achieving a
higher task completion rate and providing more diverse solutions. These results
highlight MIMIC's significant potential for effective game testing.

</details>


### [4] [FOSS-chain: using blockchain for Open Source Software license compliance](https://arxiv.org/abs/2510.01740)
*Kypros Iacovou,Georgia M. Kapitsaki,Evangelia Vanezi*

Main category: cs.SE

TL;DR: FOSS-chain是一个基于区块链的开源软件许可证管理平台，旨在解决衍生作品中的许可证兼容性问题，通过自动化许可证合规流程来避免法律纠纷。


<details>
  <summary>Details</summary>
Motivation: 开源软件许可证管理复杂，许可证不兼容可能导致法律纠纷，而区块链技术的不变性特性能够为许可证管理提供透明度和变更记录。

Method: 设计并实现了FOSS-chain网络平台，整合区块链技术，自动化许可证合规流程，覆盖14种开源软件许可证。

Result: 通过小规模用户研究对初始原型进行评估，初步结果显示出该平台在实际软件系统中应用的潜力。

Conclusion: FOSS-chain平台展示了区块链技术在开源软件许可证管理中的应用前景，能够有效解决许可证兼容性问题。

Abstract: Open Source Software (OSS) is widely used and carries licenses that indicate
the terms under which the software is provided for use, also specifying
modification and distribution rules. Ensuring that users are respecting OSS
license terms when creating derivative works is a complex process. Compliance
issues arising from incompatibilities among licenses may lead to legal
disputes. At the same time, the blockchain technology with immutable entries
offers a mechanism to provide transparency when it comes to licensing and
ensure software changes are recorded. In this work, we are introducing an
integration of blockchain and license management when creating derivative
works, in order to tackle the issue of OSS license compatibility. We have
designed, implemented and performed a preliminary evaluation of FOSS-chain, a
web platform that uses blockchain and automates the license compliance process,
covering 14 OSS licenses. We have evaluated the initial prototype version of
the FOSS-chain platform via a small scale user study. Our preliminary results
are promising, demonstrating the potential of the platform for adaptation on
realistic software systems.

</details>


### [5] [ARENA: A tool for measuring and analysing the energy efficiency of Android apps](https://arxiv.org/abs/2510.01754)
*Hina Anwar*

Main category: cs.SE

TL;DR: ARENA是一个用于Android应用能耗测量的IDE插件工具，通过硬件方式准确测量应用能耗，并提供数据分析和可视化功能。


<details>
  <summary>Details</summary>
Motivation: 现有的基于硬件的Android应用能耗测量方法过程繁琐、耗时长且不易复现，缺乏开源工具支持开发者和研究人员进行可靠的能耗测量。

Method: 开发ARENA作为IntelliJ和Android Studio插件，让开发者能在IDE中直接连接物理测量设备，执行测试场景并计算Android智能手机的能耗，同时提供数据聚合、统计分析、报告和可视化功能。

Result: 实现了ARENA工具，使开发者能够在开发过程中比较不同应用或同一应用不同版本的能耗，并深入分析测量数据。

Conclusion: ARENA解决了硬件能耗测量过程的复杂性问题，为开发者和研究人员提供了便捷、可靠的能耗测量和分析工具。

Abstract: To build energy-efficient apps, there is a need to estimate and analyze their
energy consumption in typical usage scenarios. The energy consumption of
Android apps could be estimated via software-based and hardware-based
approaches. Software-based approaches, while easier to implement, are not as
accurate as hardware-based approaches. The process of measuring the energy
consumption of an Android app via a hardware-based approach typically involves
1) setting up a measurement environment, 2) executing the app under test on a
mobile device, 3) recording current/voltage data via a hardware device to
measure energy consumption, and 4) cleaning and aggregating data for analyses,
reports, and visualizations. Specialized scripts are written for selected
hardware and software components to ensure reliable energy measurements. The
energy measurement process is repeated many times and aggregated to remove
noise. These steps make the hardware-based energy measurement process
time-consuming and not easy to adapt or reproduce. There is a lack of
open-source tools available for developers and researchers to take reliable
energy measurements via hardware devices. In this paper, we present and
demonstrate ARENA, a support tool that enables developers and researchers to
connect to a physical measurement device without leaving the comfort of their
IDE. Developers could use ARENA during development to compare energy
consumption between different apps or versions of the same app. ARENA
calculates energy consumption on an Android smartphone by executing a test
scenario on the app under development. Further, ARENA helps aggregate,
statistically analyze, report, and visualize the data, allowing developers and
researchers to dig into the data directly or visually. We implemented ARENA as
an IntelliJ and Android Studio plugin.

</details>


### [6] [Towards Speeding up Program Repair with Non-Autoregressive Model](https://arxiv.org/abs/2510.01825)
*Zhenyu Yang,Yue Pan,Zhen Yang,Zhongxing Yu*

Main category: cs.SE

TL;DR: NARRepair是首个为自动程序修复任务定制的非自回归代码生成模型，通过修复动作预测器、token间依赖提取器和两阶段解码器解决传统NAR方法的补丁质量问题，在修复速度和准确性方面达到最先进的综合性能。


<details>
  <summary>Details</summary>
Motivation: 传统基于自回归的APR技术存在巨大的时间延迟问题，特别是参数量大的模型延迟更严重。非自回归方法可以并行输出目标代码避免修复延迟，但直接应用会导致补丁质量下降。

Method: 提出NARRepair模型，包含三个核心组件：修复动作预测器缓解过度修正问题，token间依赖提取器缓解缺乏token间依赖信息问题，两阶段解码器缓解缺乏上下文信息问题。

Result: 在三个广泛使用的APR数据集上评估，NARRepair在有限修复时间内性能最佳，相比AR-based APR技术修复速度提升1.4-6.4倍，在修复速度和准确性方面达到最先进的综合性能。

Conclusion: NARRepair成功将非自回归方法应用于APR任务，有效解决了传统方法的延迟问题，同时保持了高质量的补丁生成能力。

Abstract: Enlightened by the success of machine learning techniques in various
application areas, recent years have witnessed a surge of research efforts on
automatic program repair (APR) using machine learning techniques. Previous
machine learning-based APR techniques essentially modified bugs in the
autoregressive (AR) manner, which predicts future values based on past values.
Due to the manner of token-by-token generation, the AR-based APR technique has
a huge time delay. In particular, the delay of the APR model with a large
number of parameters is more serious. To address the issue, we aim to apply the
non-autoregressive (NAR) method to the APR task, which can output target code
in a parallel manner to avoid huge repair delays. However, the naive use of the
NAR manner for the APR task suffers from the issue of compromised patch
quality. To effectively adapt the NAR manner for the APR task, we in this paper
propose NARRepair, the first customized NAR code generation model for the APR
task. The NARRepair model features three major novelties, including 1) the
repair action predictor for alleviating the over-correction issue, 2) the
inter-token dependency extractor for alleviating the issue of lacking
inter-token dependency information, and 3) the two-stage decoder for
alleviating the issue of lacking contextual information. We evaluated NARRepair
on three widely used datasets in the APR community, and the results show that
1) compared to other APR techniques, the NARRepair model has the best
performance within the limited repair time, and 2) compared to AR-based APR
techniques, the repair speed of NARRepair has been increased by 1.4-6.4 times
in the GPU environment. Overall, the results show that NARRepair has achieved
state-of-the-art comprehensive performance in terms of repair speed and
accuracy.

</details>


### [7] [RefFilter: Improving Semantic Conflict Detection via Refactoring-Aware Static Analysis](https://arxiv.org/abs/2510.01960)
*Victor Lira,Paulo Borba,Rodrigo Bonifácio,Galileu Santos e Matheus barbosa*

Main category: cs.SE

TL;DR: RefFilter是一个重构感知的语义干扰检测工具，通过自动检测重构来减少误报，在标记数据集上减少近32%的误报。


<details>
  <summary>Details</summary>
Motivation: 现有轻量级静态分析技术在协作软件开发中检测语义干扰时存在高误报率，主要原因是无法有效区分行为保持的代码重构和影响行为的变更。

Method: 在现有静态技术基础上，结合自动重构检测来改进精度，从报告中排除行为保持的重构，减少误报同时保持检测覆盖率。

Result: 在标记数据集上减少近32%的误报，虽然伴随非显著增加的漏报，但精度提升显著超过召回率的轻微损失。

Conclusion: 重构感知的干扰检测是改进现代开发工作流中合并支持的实用有效策略。

Abstract: Detecting semantic interference remains a challenge in collaborative software
development. Recent lightweight static analysis techniques improve efficiency
over SDG-based methods, but they still suffer from a high rate of false
positives. A key cause of these false positives is the presence of
behavior-preserving code refactorings, which current techniques cannot
effectively distinguish from changes that impact behavior and can interfere
with others. To handle this problem we present RefFilter, a refactoring-aware
tool for semantic interference detection. It builds on existing static
techniques by incorporating automated refactoring detection to improve
precision. RefFilter discards behavior-preserving refactorings from reports,
reducing false positives while preserving detection coverage. To evaluate
effectiveness and scalability, use two datasets: a labeled dataset with 99
scenarios and ground truth, and a novel dataset of 1,087 diverse merge
scenarios that we have built. Experimental results show that RefFilter reduces
false positives by nearly 32% on the labeled dataset. While this reduction
comes with a non significant increase in false negatives, the overall gain in
precision significantly outweighs the minor trade-off in recall. These findings
demonstrate that refactoring-aware interference detection is a practical and
effective strategy for improving merge support in modern development workflows.

</details>


### [8] [Clarifying Semantics of In-Context Examples for Unit Test Generation](https://arxiv.org/abs/2510.01994)
*Chen Yang,Lin Yang,Ziqi Wang,Dong Wang,Jianyi Zhou,Junjie Chen*

Main category: cs.SE

TL;DR: CLAST是一种通过系统化重构单元测试来提升语义清晰度的新技术，能够显著改善基于上下文学习的单元测试生成效果。


<details>
  <summary>Details</summary>
Motivation: 现有基于大语言模型的单元测试生成方法受限于上下文示例的质量，语义不清晰或结构不良的测试示例会导致生成结果不理想。

Method: CLAST通过程序分析和LLM重写的组合方法，将复杂测试分解为逻辑更清晰的测试，提升语义清晰度。

Result: 在4个开源项目和3个工业项目中评估，CLAST在保持测试有效性和提升语义清晰度方面显著优于现有最佳技术UTgen，用户研究中85.33%的参与者更偏好CLAST重构的测试。

Conclusion: CLAST重构的测试作为上下文示例能有效提升基于ICL的单元测试生成方法性能，为软件测试实践提供了重要价值。

Abstract: Recent advances in large language models (LLMs) have enabled promising
performance in unit test generation through in-context learning (ICL). However,
the quality of in-context examples significantly influences the effectiveness
of generated tests-poorly structured or semantically unclear test examples
often lead to suboptimal outputs. In this paper, we propose CLAST, a novel
technique that systematically refines unit tests to improve their semantic
clarity, thereby enhancing their utility as in-context examples. The approach
decomposes complex tests into logically clearer ones and improves semantic
clarity through a combination of program analysis and LLM-based rewriting. We
evaluated CLAST on four open-source and three industrial projects. The results
demonstrate that CLAST largely outperforms UTgen, the state-of-the-art
refinement technique, in both preserving test effectiveness and enhancing
semantic clarity. Specifically, CLAST fully retains the original effectiveness
of unit tests, while UTgen reduces compilation success rate (CSR), pass rate
(PR), test coverage (Cov), and mutation score (MS) by an average of 12.90%,
35.82%, 4.65%, and 5.07%, respectively. Over 85.33% of participants in our user
study preferred the semantic clarity of CLAST-refined tests. Notably,
incorporating CLAST-refined tests as examples effectively improves ICL-based
unit test generation approaches such as RAGGen and TELPA, resulting in an
average increase of 25.97% in CSR, 28.22% in PR, and 45.99% in Cov for
generated tests, compared to incorporating UTgen-refined tests. The insights
from the follow-up user study not only reinforce CLAST's potential impact in
software testing practice but also illuminate avenues for future research.

</details>


### [9] [Automatic Generation of Combinatorial Reoptimisation Problem Specifications: A Vision](https://arxiv.org/abs/2510.02002)
*Maximilian Kratz,Steffen Zschaler,Jens Kosiol,Gabriele Taentzer*

Main category: cs.SE

TL;DR: 本文提出使用模型驱动工程（MDE）方法，特别是声明式建模语言和模型转换，来系统地从原始优化问题规范推导出再优化问题，以解决上下文变化时的解决方案适应问题。


<details>
  <summary>Details</summary>
Motivation: 当优化问题的上下文因素发生变化时，需要重新优化解决方案，但新的优化问题与原始问题存在显著差异：需要最小化对原始解决方案的更改、某些部分可能无法更改、需要生成从原始解到新解的变更脚本。

Method: 采用模型驱动工程方法，使用声明式建模语言和模型转换来高层次规范优化问题，基于GIPS工具实现概念验证，应用于助教分配的资源分配问题。

Result: 提供了变化问题和推导相应再优化规范策略的初步分类，并实现了基于GIPS工具的概念验证系统。

Conclusion: 模型驱动工程为从原始优化问题规范系统推导再优化问题提供了新的机会，特别适用于组合再优化问题。

Abstract: Once an optimisation problem has been solved, the solution may need
adaptation when contextual factors change. This challenge, also known as
reoptimisation, has been addressed in various problem domains, such as railway
crew rescheduling, nurse rerostering, or aircraft recovery. This requires a
modified problem to be solved again to ensure that the adapted solution is
optimal in the new context. However, the new optimisation problem differs
notably from the original problem: (i) we want to make only minimal changes to
the original solution to minimise the impact; (ii) we may be unable to change
some parts of the original solution (e.g., because they refer to past
allocations); and (iii) we need to derive a change script from the original
solution to the new solution. In this paper, we argue that Model-Driven
Engineering (MDE) - in particular, the use of declarative modelling languages
and model transformations for the high-level specification of optimisation
problems - offers new opportunities for the systematic derivation of
reoptimisation problems from the original optimisation problem specification.
We focus on combinatorial reoptimisation problems and provide an initial
categorisation of changing problems and strategies for deriving the
corresponding reoptimisation specifications. We introduce an initial
proof-of-concept implementation based on the GIPS (Graph-Based (Mixed) Integer
Linear Programming Problem Specification) tool and apply it to an example
resource-allocation problem: the allocation of teaching assistants to teaching
sessions.

</details>


### [10] [ACM SIGSOFT SEN Empirical Software Engineering: Introducing Our New Regular Column](https://arxiv.org/abs/2510.02007)
*Justus Bogner,Roberto Verdecchia*

Main category: cs.SE

TL;DR: 介绍新的ACM SIGSOFT SEN专栏(SEN-ESE)，旨在讨论经验软件工程研究的元方面，包括最佳实践、统计方法等，通过专家访谈、焦点小组等方式促进该领域的发展。


<details>
  <summary>Details</summary>
Motivation: 经验软件工程研究虽然成熟但仍面临挑战，如研究可重复性、外部有效性有限、评审主观性等，且许多方面缺乏明确文档，不利于新人学习。

Method: 通过新的SEN专栏作为平台，采用专家访谈、焦点小组、调查和立场文章等多种方法，讨论ESE研究的元方面。

Result: 建立了专门讨论ESE研究元方面的新论坛，为社区提供交流平台，促进对该领域隐含知识的探讨。

Conclusion: 该专栏旨在激发对ESE研究中较少触及或隐含主题的讨论，鼓励社区反馈，以改进ESE研究的实施、传播和教学方式。

Abstract: From its early foundations in the 1970s, empirical software engineering (ESE)
has evolved into a mature research discipline that embraces a plethora of
different topics, methodologies, and industrial practices. Despite its
remarkable progress, the ESE research field still needs to keep evolving, as
new impediments, shortcoming, and technologies emerge. Research
reproducibility, limited external validity, subjectivity of reviews, and
porting research results to industrial practices are just some examples of the
drivers for improvements to ESE research. Additionally, several facets of ESE
research are not documented very explicitly, which makes it difficult for
newcomers to pick them up. With this new regular ACM SIGSOFT SEN column
(SEN-ESE), we introduce a venue for discussing meta-aspects of ESE research,
ranging from general topics such as the nature and best practices for
replication packages, to more nuanced themes such as statistical methods,
interview transcription tools, and publishing interdisciplinary research. Our
aim for the column is to be a place where we can regularly spark conversations
on ESE topics that might not often be touched upon or are left implicit.
Contributions to this column will be grounded in expert interviews, focus
groups, surveys, and position pieces, with the goal of encouraging reflection
and improvement in how we conduct, communicate, teach, and ultimately improve
ESE research. Finally, we invite feedback from the ESE community on
challenging, controversial, or underexplored topics, as well as suggestions for
voices you would like to hear from. While we cannot promise to act on every
idea, we aim to shape this column around the community interests and are
grateful for all contributions.

</details>


### [11] [Towards fairer public transit: Real-time tensor-based multimodal fare evasion and fraud detection](https://arxiv.org/abs/2510.02165)
*Peter Wauyo,Dalia Bwiza,Alain Murara,Edwin Mugume,Eric Umuhoza*

Main category: cs.SE

TL;DR: 提出基于多模态（CCTV视频和音频）的公共交通欺诈检测系统，使用ViViT和AST提取特征，通过张量融合网络实现跨模态交互，在自定义数据集上达到89.5%准确率，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决公共交通中的欺诈和逃票问题，减少收入损失，提高乘客安全和运营合规性。现有系统检测率较低（约75%召回率），需要更有效的多模态分析方法。

Method: 使用ViViT模型提取视频特征，AST模型分析音频，采用张量融合网络架构通过2折笛卡尔积显式建模单模态和双模态交互，捕捉视觉行为（如尾随、未授权访问）和音频线索（如票价交易声音）之间的复杂跨模态动态。

Result: 在自定义数据集上达到89.5%准确率、87.2%精确率和84.0%召回率，显著优于早期融合基线，超过最先进交通欺诈检测系统通常报告的75%召回率。消融研究显示张量融合方法相比传统拼接方法在F1分数上提升7.0%，召回率提升8.8%。

Conclusion: 该多模态系统支持实时检测，能够有效帮助公共交通运营商减少收入损失，提高乘客安全，确保运营合规性。张量融合方法在欺诈检测方面表现出显著优势。

Abstract: This research introduces a multimodal system designed to detect fraud and
fare evasion in public transportation by analyzing closed circuit television
(CCTV) and audio data. The proposed solution uses the Vision Transformer for
Video (ViViT) model for video feature extraction and the Audio Spectrogram
Transformer (AST) for audio analysis. The system implements a Tensor Fusion
Network (TFN) architecture that explicitly models unimodal and bimodal
interactions through a 2-fold Cartesian product. This advanced fusion technique
captures complex cross-modal dynamics between visual behaviors (e.g.,
tailgating,unauthorized access) and audio cues (e.g., fare transaction sounds).
The system was trained and tested on a custom dataset, achieving an accuracy of
89.5%, precision of 87.2%, and recall of 84.0% in detecting fraudulent
activities, significantly outperforming early fusion baselines and exceeding
the 75% recall rates typically reported in state-of-the-art transportation
fraud detection systems. Our ablation studies demonstrate that the tensor
fusion approach provides a 7.0% improvement in the F1 score and an 8.8% boost
in recall compared to traditional concatenation methods. The solution supports
real-time detection, enabling public transport operators to reduce revenue
loss, improve passenger safety, and ensure operational compliance.

</details>


### [12] [SIEVE: Towards Verifiable Certification for Code-datasets](https://arxiv.org/abs/2510.02166)
*Fatou Ndiaye Mbodji,El-hacen Diallo,Jordan Samhi,Kui Liu,Jacques Klein,Tegawendé F. Bissyande*

Main category: cs.SE

TL;DR: SIEVE是一个社区驱动的框架，将代码数据集的质量检查转化为可验证的置信卡片，提供统计保证，旨在降低质量保证成本并增强对代码数据集的信任。


<details>
  <summary>Details</summary>
Motivation: 当前公共代码数据集缺乏可验证的质量保证，静态数据集卡片不可审计且无统计保证，团队需要构建孤立的临时清理管道，导致工作分散和成本上升。

Method: SIEVE框架将每个属性检查转化为置信卡片——机器可读、可验证的证书，具有随时有效的统计边界。

Result: 提出了一个研究计划，使SIEVE成熟，用随时可验证的认证取代叙述性卡片。

Conclusion: 这一转变有望降低质量保证成本，并增加对代码数据集的信任。

Abstract: Code agents and empirical software engineering rely on public code datasets,
yet these datasets lack verifiable quality guarantees. Static 'dataset cards'
inform, but they are neither auditable nor do they offer statistical
guarantees, making it difficult to attest to dataset quality. Teams build
isolated, ad-hoc cleaning pipelines. This fragments effort and raises cost. We
present SIEVE, a community-driven framework. It turns per-property checks into
Confidence Cards-machine-readable, verifiable certificates with anytime-valid
statistical bounds. We outline a research plan to bring SIEVE to maturity,
replacing narrative cards with anytime-verifiable certification. This shift is
expected to lower quality-assurance costs and increase trust in code-datasets.

</details>


### [13] [TAIBOM: Bringing Trustworthiness to AI-Enabled Systems](https://arxiv.org/abs/2510.02169)
*Vadim Safronov,Anthony McCaigue,Nicholas Allott,Andrew Martin*

Main category: cs.SE

TL;DR: 提出了TAIBOM框架，将SBOM原则扩展到AI领域，解决AI系统依赖管理和完整性验证的挑战


<details>
  <summary>Details</summary>
Motivation: 开源软件和AI技术的融合给软件供应链带来了新的复杂性，现有SBOM框架无法捕捉AI系统的动态数据驱动特性和松散耦合依赖

Method: 开发了TAIBOM框架，包括结构化AI组件依赖模型、异构AI管道完整性声明传播机制和组件来源信任验证过程

Result: TAIBOM在AI工作流中支持保证、安全和合规性，相比SPDX和CycloneDX等现有标准具有优势

Conclusion: 这项工作通过结构化软件透明度为可信和可验证的AI系统奠定了基础

Abstract: The growing integration of open-source software and AI-driven technologies
has introduced new layers of complexity into the software supply chain,
challenging existing methods for dependency management and system assurance.
While Software Bills of Materials (SBOMs) have become critical for enhancing
transparency and traceability, current frameworks fall short in capturing the
unique characteristics of AI systems -- namely, their dynamic, data-driven
nature and the loosely coupled dependencies across datasets, models, and
software components. These challenges are compounded by fragmented governance
structures and the lack of robust tools for ensuring integrity, trust, and
compliance in AI-enabled environments.
  In this paper, we introduce Trusted AI Bill of Materials (TAIBOM) -- a novel
framework extending SBOM principles to the AI domain. TAIBOM provides (i) a
structured dependency model tailored for AI components, (ii) mechanisms for
propagating integrity statements across heterogeneous AI pipelines, and (iii) a
trust attestation process for verifying component provenance. We demonstrate
how TAIBOM supports assurance, security, and compliance across AI workflows,
highlighting its advantages over existing standards such as SPDX and CycloneDX.
This work lays the foundation for trustworthy and verifiable AI systems through
structured software transparency.

</details>


### [14] [FalseCrashReducer: Mitigating False Positive Crashes in OSS-Fuzz-Gen Using Agentic AI](https://arxiv.org/abs/2510.02185)
*Paschal C. Amusuo,Dongge Liu,Ricardo Andres Calvo Mendez,Jonathan Metzman,Oliver Chang,James C. Davis*

Main category: cs.SE

TL;DR: 本文提出两种AI驱动策略来减少OSS-Fuzz-Gen中的误报崩溃：基于约束的模糊测试驱动生成和基于上下文的崩溃验证，在1500个基准函数上减少虚假崩溃达8%，报告崩溃减少一半以上。


<details>
  <summary>Details</summary>
Motivation: 自动生成的模糊测试驱动经常导致误报崩溃，特别是在需要高度结构化输入和复杂状态要求的函数中，这在工业级模糊测试驱动生成中会损害系统可信度。

Method: 1. 基于约束的模糊测试驱动生成：在驱动创建时主动强制执行函数输入和状态约束；2. 基于上下文的崩溃验证：通过分析函数调用者来验证报告崩溃是否从程序入口点可行。

Result: 在1500个OSS-Fuzz基准函数上，虚假崩溃减少达8%，报告崩溃减少超过一半，证明前沿LLM可作为可靠程序分析代理。

Conclusion: 研究展示了将AI集成到大规模模糊测试管道中的前景和挑战，AI驱动策略能有效减少误报崩溃并提高系统可靠性。

Abstract: Fuzz testing has become a cornerstone technique for identifying software bugs
and security vulnerabilities, with broad adoption in both industry and
open-source communities. Directly fuzzing a function requires fuzz drivers,
which translate random fuzzer inputs into valid arguments for the target
function. Given the cost and expertise required to manually develop fuzz
drivers, methods exist that leverage program analysis and Large Language Models
to automatically generate these drivers. However, the generated fuzz drivers
frequently lead to false positive crashes, especially in functions highly
structured input and complex state requirements. This problem is especially
crucial in industry-scale fuzz driver generation efforts like OSS-Fuzz-en, as
reporting false positive crashes to maintainers impede trust in both the system
and the team.
  This paper presents two AI-driven strategies to reduce false positives in
OSS-Fuzz-Gen, a multi-agent system for automated fuzz driver generation. First,
constraint-based fuzz driver generation proactively enforces constraints on a
function's inputs and state to guide driver creation. Second, context-based
crash validation reactively analyzes function callers to determine whether
reported crashes are feasible from program entry points. Using 1,500 benchmark
functions from OSS-Fuzz, we show that these strategies reduce spurious crashes
by up to 8%, cut reported crashes by more than half, and demonstrate that
frontier LLMs can serve as reliable program analysis agents. Our results
highlight the promise and challenges of integrating AI into large-scale fuzzing
pipelines.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [15] [Jailbreaking LLMs via Semantically Relevant Nested Scenarios with Targeted Toxic Knowledge](https://arxiv.org/abs/2510.01223)
*Hui Dou,Ning Xu,Yiwen Zhang,Kaibin Wang*

Main category: cs.CR

TL;DR: 提出RTS-Attack方法，通过构建与查询高度相关的嵌套场景并整合针对性有害知识，成功绕过大型语言模型的对齐防御，在多种先进LLMs上表现出优越性能。


<details>
  <summary>Details</summary>
Motivation: 现有嵌套场景方法容易被检测，因为恶意意图明显。研究发现LLMs的对齐防御对语义相关且包含针对性有害知识的嵌套场景不敏感，这是一个重要但未被充分探索的方向。

Method: 提出RTS-Attack框架，构建与查询高度语义相关的嵌套场景，并整合针对性有害知识，生成不含直接有害查询的越狱提示，具有出色的隐蔽性。

Result: 在GPT-4o、Llama3-70b、Gemini-pro等多种先进LLMs上的广泛实验表明，RTS-Attack在效率和通用性方面均优于基线方法。

Conclusion: RTS-Attack通过语义相关嵌套场景和针对性有害知识的结合，有效绕过LLMs的对齐防御，揭示了当前对齐机制的潜在漏洞。

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities in
various tasks. However, they remain exposed to jailbreak attacks, eliciting
harmful responses. The nested scenario strategy has been increasingly adopted
across various methods, demonstrating immense potential. Nevertheless, these
methods are easily detectable due to their prominent malicious intentions. In
this work, we are the first to find and systematically verify that LLMs'
alignment defenses are not sensitive to nested scenarios, where these scenarios
are highly semantically relevant to the queries and incorporate targeted toxic
knowledge. This is a crucial yet insufficiently explored direction. Based on
this, we propose RTS-Attack (Semantically Relevant Nested Scenarios with
Targeted Toxic Knowledge), an adaptive and automated framework to examine LLMs'
alignment. By building scenarios highly relevant to the queries and integrating
targeted toxic knowledge, RTS-Attack bypasses the alignment defenses of LLMs.
Moreover, the jailbreak prompts generated by RTS-Attack are free from harmful
queries, leading to outstanding concealment. Extensive experiments demonstrate
that RTS-Attack exhibits superior performance in both efficiency and
universality compared to the baselines across diverse advanced LLMs, including
GPT-4o, Llama3-70b, and Gemini-pro. Our complete code is available in the
supplementary material. WARNING: THIS PAPER CONTAINS POTENTIALLY HARMFUL
CONTENT.

</details>


### [16] [Fine-Tuning Jailbreaks under Highly Constrained Black-Box Settings: A Three-Pronged Approach](https://arxiv.org/abs/2510.01342)
*Xiangfang Li,Yu Wang,Bo Li*

Main category: cs.CR

TL;DR: 本文提出了一种三管齐下的越狱攻击方法，能够在仅提供微调数据的黑盒设置下成功攻击大语言模型，在OpenAI平台上对GPT-4.1和GPT-4o的攻击成功率超过97%。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型的快速发展，确保其安全使用变得至关重要。现有的越狱攻击研究大多关注过于简化的攻击场景，限制了其在真实防御环境中的实用性。

Method: 提出三管齐下的攻击方法：结合安全样式的前缀/后缀包装、敏感词汇的良性词汇编码（下划线）和后门机制，使模型在单个数据点看似无害的情况下学习有害行为。

Result: 在真实部署中，该方法在OpenAI平台上成功越狱GPT-4.1和GPT-4o，两种模型的攻击成功率均超过97%。

Conclusion: 该研究表明即使在仅提供微调数据的黑盒设置下，精心设计的攻击仍能有效绕过提供商的多阶段防御，凸显了当前防御机制的脆弱性。

Abstract: With the rapid advancement of large language models (LLMs), ensuring their
safe use becomes increasingly critical. Fine-tuning is a widely used method for
adapting models to downstream tasks, yet it is vulnerable to jailbreak attacks.
However, most existing studies focus on overly simplified attack scenarios,
limiting their practical relevance to real-world defense settings. To make this
risk concrete, we present a three-pronged jailbreak attack and evaluate it
against provider defenses under a dataset-only black-box fine-tuning interface.
In this setting, the attacker can only submit fine-tuning data to the provider,
while the provider may deploy defenses across stages: (1) pre-upload data
filtering, (2) training-time defensive fine-tuning, and (3) post-training
safety audit. Our attack combines safety-styled prefix/suffix wrappers, benign
lexical encodings (underscoring) of sensitive tokens, and a backdoor mechanism,
enabling the model to learn harmful behaviors while individual datapoints
appear innocuous. Extensive experiments demonstrate the effectiveness of our
approach. In real-world deployment, our method successfully jailbreaks GPT-4.1
and GPT-4o on the OpenAI platform with attack success rates above 97% for both
models. Our code is available at
https://github.com/lxf728/tri-pronged-ft-attack.

</details>


### [17] [Integrated Security Mechanisms for Weight Protection in Memristive Crossbar Arrays](https://arxiv.org/abs/2510.01350)
*Muhammad Faheemur Rahman,Wayne Burleson*

Main category: cs.CR

TL;DR: 提出了两种保护忆阻器交叉阵列中权重安全性的机制：密钥置换器和水印保护列，这些机制能有效防止权重泄露并验证所有权，同时保持低于10%的性能开销。


<details>
  <summary>Details</summary>
Motivation: 忆阻器交叉阵列虽然适合内存计算，但非易失性忆阻器容易受到安全威胁，特别是存储的权重可能被恶意提取。这些权重代表经过长时间训练获得的知识产权，需要保护。

Method: 提出了两种安全机制：1) 密钥置换器 - 通过密钥控制权重访问；2) 水印保护列 - 嵌入可验证的所有权信息。这些机制与现有忆阻器架构兼容，无需重大设计修改。

Result: 在45nm、22nm和7nm CMOS节点上的仿真显示，两种机制都能提供强大的保护，面积、延迟和功耗开销均低于10%。在MNIST数据集上的初步实验进一步验证了可行性。

Conclusion: 该研究证明了在忆阻器内存计算系统中实现安全保护的可行性，通过提出的两种机制能够有效保护知识产权，同时保持最小的性能损失。

Abstract: Memristive crossbar arrays enable in-memory computing by performing parallel
analog computations directly within memory, making them well-suited for machine
learning, neural networks, and neuromorphic systems. However, despite their
advantages, non-volatile memristors are vulnerable to security threats (such as
adversarial extraction of stored weights when the hardware is compromised.
Protecting these weights is essential since they represent valuable
intellectual property resulting from lengthy and costly training processes
using large, often proprietary, datasets. As a solution we propose two security
mechanisms: Keyed Permutor and Watermark Protection Columns; where both
safeguard critical weights and establish verifiable ownership (even in cases of
data leakage). Our approach integrates efficiently with existing memristive
crossbar architectures without significant design modifications. Simulations
across 45nm, 22nm, and 7nm CMOS nodes, using a realistic interconnect model and
a large RF dataset, show that both mechanisms offer robust protection with
under 10% overhead in area, delay and power. We also present initial
experiments employing the widely known MNIST dataset; further highlighting the
feasibility of securing memristive in-memory computing systems with minimal
performance trade-offs.

</details>


### [18] [WAInjectBench: Benchmarking Prompt Injection Detections for Web Agents](https://arxiv.org/abs/2510.01354)
*Yinuo Liu,Ruohan Xu,Xilong Wang,Yuqi Jia,Neil Zhenqiang Gong*

Main category: cs.CR

TL;DR: 首次针对Web代理的提示注入攻击检测进行全面基准研究，提出了细粒度攻击分类，构建了包含恶意和良性样本的数据集，系统化评估了文本和图像检测方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要针对通用提示注入攻击，缺乏对Web代理场景的系统性评估，需要填补这一研究空白。

Method: 1. 基于威胁模型对攻击进行细粒度分类；2. 构建包含恶意/良性文本和图像的数据集；3. 系统化文本和图像检测方法；4. 多场景性能评估。

Result: 部分检测器能有效识别依赖显式文本指令或可见图像扰动的攻击，但对省略显式指令或使用不可察觉扰动的攻击基本失效。

Conclusion: 当前检测方法在Web代理场景下存在局限性，需要开发更鲁棒的检测技术来应对复杂攻击。数据集和代码已开源。

Abstract: Multiple prompt injection attacks have been proposed against web agents. At
the same time, various methods have been developed to detect general prompt
injection attacks, but none have been systematically evaluated for web agents.
In this work, we bridge this gap by presenting the first comprehensive
benchmark study on detecting prompt injection attacks targeting web agents. We
begin by introducing a fine-grained categorization of such attacks based on the
threat model. We then construct datasets containing both malicious and benign
samples: malicious text segments generated by different attacks, benign text
segments from four categories, malicious images produced by attacks, and benign
images from two categories. Next, we systematize both text-based and
image-based detection methods. Finally, we evaluate their performance across
multiple scenarios. Our key findings show that while some detectors can
identify attacks that rely on explicit textual instructions or visible image
perturbations with moderate to high accuracy, they largely fail against attacks
that omit explicit instructions or employ imperceptible perturbations. Our
datasets and code are released at:
https://github.com/Norrrrrrr-lyn/WAInjectBench.

</details>


### [19] [Breaking the Code: Security Assessment of AI Code Agents Through Systematic Jailbreaking Attacks](https://arxiv.org/abs/2510.01359)
*Shoumik Saha,Jifan Chen,Sam Mayers,Sanjay Krishna Gouda,Zijian Wang,Varun Kumar*

Main category: cs.CR

TL;DR: JAWS-BENCH是一个评估代码能力LLM代理安全性的基准测试，涵盖三个工作空间级别，发现代码代理在单文件和多文件环境下攻击成功率分别达到71%和75%，且将LLM包装为代理会使漏洞增加1.6倍。


<details>
  <summary>Details</summary>
Motivation: 随着代码能力LLM代理被集成到软件开发流程中，它们能够读写和执行代码，使得绕过安全防护（越狱攻击）的风险超越了纯文本环境。现有评估主要关注拒绝或有害文本检测，但未验证代理是否实际编译和运行恶意程序。

Method: 提出JAWS-BENCH基准测试，包含三个逐步升级的工作空间制度（空、单文件、多文件），并配套分层、可执行感知的评估框架，测试合规性、攻击成功率、语法正确性和运行时可执行性。

Result: 在JAWS-0中，代码代理平均接受61%的攻击，58%有害，52%可解析，27%可端到端运行。在JAWS-1中，合规性接近100%，平均攻击成功率约71%。在JAWS-M中，平均攻击成功率约75%，32%的攻击代码可立即部署。

Conclusion: 研究结果表明需要执行感知的防御措施、代码上下文安全过滤器，以及在整个代理多步推理和工具使用过程中保持拒绝决策的机制。

Abstract: Code-capable large language model (LLM) agents are increasingly embedded into
software engineering workflows where they can read, write, and execute code,
raising the stakes of safety-bypass ("jailbreak") attacks beyond text-only
settings. Prior evaluations emphasize refusal or harmful-text detection,
leaving open whether agents actually compile and run malicious programs. We
present JAWS-BENCH (Jailbreaks Across WorkSpaces), a benchmark spanning three
escalating workspace regimes that mirror attacker capability: empty (JAWS-0),
single-file (JAWS-1), and multi-file (JAWS-M). We pair this with a
hierarchical, executable-aware Judge Framework that tests (i) compliance, (ii)
attack success, (iii) syntactic correctness, and (iv) runtime executability,
moving beyond refusal to measure deployable harm. Using seven LLMs from five
families as backends, we find that under prompt-only conditions in JAWS-0, code
agents accept 61% of attacks on average; 58% are harmful, 52% parse, and 27%
run end-to-end. Moving to single-file regime in JAWS-1 drives compliance to ~
100% for capable models and yields a mean ASR (Attack Success Rate) ~ 71%; the
multi-file regime (JAWS-M) raises mean ASR to ~ 75%, with 32% instantly
deployable attack code. Across models, wrapping an LLM in an agent
substantially increases vulnerability -- ASR raises by 1.6x -- because initial
refusals are frequently overturned during later planning/tool-use steps.
Category-level analyses identify which attack classes are most vulnerable and
most readily deployable, while others exhibit large execution gaps. These
findings motivate execution-aware defenses, code-contextual safety filters, and
mechanisms that preserve refusal decisions throughout the agent's multi-step
reasoning and tool use.

</details>


### [20] [E-FuzzEdge: Optimizing Embedded Device Security with Scalable In-Place Fuzzing](https://arxiv.org/abs/2510.01393)
*Davide Rusconi,Osama Yousef,Mirco Picca,Flavio Toffalini,Andrea Lanzi*

Main category: cs.CR

TL;DR: E-FuzzEdge是一种针对微控制器模糊测试的新型架构，通过优化执行速度来解决硬件在环测试的效率问题，在可扩展性受限的环境中显著提升测试吞吐量。


<details>
  <summary>Details</summary>
Motivation: 解决微控制器硬件在环模糊测试中的效率低下问题，特别是在缺乏可扩展性的测试环境中提升测试吞吐量。

Method: 开发了E-FuzzEdge架构，通过优化执行速度来提高模糊测试效率，支持与其他嵌入式模糊测试技术集成，特别是那些进行设备测试而非固件仿真的技术。

Result: 与最先进的基准测试相比，E-FuzzEdge展示了显著的性能改进，能够有效提升模糊测试活动的吞吐量。

Conclusion: E-FuzzEdge架构具有兼容性优势，嵌入式模糊测试社区可以将其集成到工作流程中，从而提高整体测试效率。

Abstract: In this paper we show E-FuzzEdge, a novel fuzzing architecture targeted
towards improving the throughput of fuzzing campaigns in contexts where
scalability is unavailable. E-FuzzEdge addresses the inefficiencies of
hardware-in-the-loop fuzzing for microcontrollers by optimizing execution
speed. We evaluated our system against state-of-the-art benchmarks,
demonstrating significant performance improvements. A key advantage of
E-FuzzEdgearchitecture is its compatibility with other embedded fuzzing
techniques that perform on device testing instead of firmware emulation. This
means that the broader embedded fuzzing community can integrate E-FuzzEdge into
their workflows to enhance overall testing efficiency.

</details>


### [21] [Securing IoT Devices in Smart Cities: A Review of Proposed Solutions](https://arxiv.org/abs/2510.01445)
*Andrés F. Betancur-López*

Main category: cs.CR

TL;DR: 对智慧城市中物联网设备安全保护方案的综述，重点分析轻量级密码学、物理不可克隆函数和区块链解决方案的优缺点。


<details>
  <summary>Details</summary>
Motivation: 智慧城市中的物联网设备由于计算资源有限而容易受到攻击，其广泛部署增加了安全漏洞的潜在影响，需要有效的安全保护方案。

Method: 通过分析设备级安全的最新文献进行综述，特别关注轻量级密码学、物理不可克隆函数和基于区块链的解决方案。

Result: 研究揭示了当前方法的优势和局限性，包括在资源受限环境中的适用性问题。

Conclusion: 需要更实用、可扩展且资源高效的机制来确保物联网生态系统中的用户隐私和数据保护。

Abstract: Privacy and security in Smart Cities remain at constant risk due to the
vulnerabilities introduced by Internet of Things (IoT) devices. The limited
computational resources of these devices make them especially susceptible to
attacks, while their widespread adoption increases the potential impact of
security breaches. This article presents a review of security proposals aimed
at protecting IoT devices in Smart City environments. The review was conducted
by analyzing recent literature on device-level security, with particular
emphasis on lightweight cryptography, physically unclonable functions (PUFs),
and blockchain-based solutions. Findings highlight both the strengths and
limitations of current approaches, as well as the need for more practical,
scalable, and resource-efficient mechanisms to ensure user privacy and data
protection in IoT ecosystems.

</details>


### [22] [POLAR: Automating Cyber Threat Prioritization through LLM-Powered Assessment](https://arxiv.org/abs/2510.01552)
*Luoxi Tang,Yuqiao Meng,Ankita Patra,Weicheng Ma,Muchao Ye,Zhaohan Xi*

Main category: cs.CR

TL;DR: 本文研究LLM在网络安全威胁情报中的内在脆弱性，识别了三个主要问题：伪相关、矛盾知识和受限泛化，并提出了改进LLM驱动的CTI系统的实用建议。


<details>
  <summary>Details</summary>
Motivation: 虽然LLM被广泛用于辅助网络安全分析师处理网络威胁，但在实际部署中存在显著的性能差距，需要研究LLM在CTI中的内在脆弱性。

Method: 使用大规模评估，结合多个CTI基准和真实威胁报告，引入了一种集成分层、自回归细化和人在回路监督的新型分类方法。

Result: 通过广泛实验和人工检查，揭示了三个基本脆弱性：伪相关、矛盾知识和受限泛化，这些限制了LLM在有效支持CTI方面的能力。

Conclusion: 为设计更鲁棒的LLM驱动的CTI系统提供了可操作的见解，以促进未来研究。

Abstract: Large Language Models (LLMs) are intensively used to assist security analysts
in counteracting the rapid exploitation of cyber threats, wherein LLMs offer
cyber threat intelligence (CTI) to support vulnerability assessment and
incident response. While recent work has shown that LLMs can support a wide
range of CTI tasks such as threat analysis, vulnerability detection, and
intrusion defense, significant performance gaps persist in practical
deployments. In this paper, we investigate the intrinsic vulnerabilities of
LLMs in CTI, focusing on challenges that arise from the nature of the threat
landscape itself rather than the model architecture. Using large-scale
evaluations across multiple CTI benchmarks and real-world threat reports, we
introduce a novel categorization methodology that integrates stratification,
autoregressive refinement, and human-in-the-loop supervision to reliably
analyze failure instances. Through extensive experiments and human inspections,
we reveal three fundamental vulnerabilities: spurious correlations,
contradictory knowledge, and constrained generalization, that limit LLMs in
effectively supporting CTI. Subsequently, we provide actionable insights for
designing more robust LLM-powered CTI systems to facilitate future research.

</details>


### [23] [Position: Privacy Is Not Just Memorization!](https://arxiv.org/abs/2510.01645)
*Niloofar Mireshghallah,Tianshi Li*

Main category: cs.CR

TL;DR: 本文认为LLM隐私风险远不止训练数据提取，还包括数据收集、推理时上下文泄露、自主代理能力和深度推理攻击等更紧迫的威胁，呼吁研究社区超越当前技术解决方案的狭隘关注。


<details>
  <summary>Details</summary>
Motivation: 当前关于LLM隐私风险的讨论过度关注训练数据的逐字记忆，而更直接和可扩展的隐私威胁却被忽视，需要更全面地审视LLM系统中的隐私风险。

Method: 提出了涵盖LLM生命周期（从数据收集到部署）的隐私风险综合分类法，通过案例研究展示当前隐私框架的不足，并对2016-2025年间1322篇AI/ML隐私论文进行纵向分析。

Result: 分析发现记忆问题在技术研究中受到过度关注，而最紧迫的隐私危害存在于其他领域，当前技术方法在这些领域缺乏有效应对方案。

Conclusion: 呼吁研究社区从根本上改变处理LLM隐私的方法，超越当前技术解决方案的狭隘关注，采用跨学科方法应对这些新兴威胁的社会技术性质。

Abstract: The discourse on privacy risks in Large Language Models (LLMs) has
disproportionately focused on verbatim memorization of training data, while a
constellation of more immediate and scalable privacy threats remain
underexplored. This position paper argues that the privacy landscape of LLM
systems extends far beyond training data extraction, encompassing risks from
data collection practices, inference-time context leakage, autonomous agent
capabilities, and the democratization of surveillance through deep inference
attacks. We present a comprehensive taxonomy of privacy risks across the LLM
lifecycle -- from data collection through deployment -- and demonstrate through
case studies how current privacy frameworks fail to address these multifaceted
threats. Through a longitudinal analysis of 1,322 AI/ML privacy papers
published at leading conferences over the past decade (2016--2025), we reveal
that while memorization receives outsized attention in technical research, the
most pressing privacy harms lie elsewhere, where current technical approaches
offer little traction and viable paths forward remain unclear. We call for a
fundamental shift in how the research community approaches LLM privacy, moving
beyond the narrow focus of current technical solutions and embracing
interdisciplinary approaches that address the sociotechnical nature of these
emerging threats.

</details>


### [24] [Evaluating the Robustness of a Production Malware Detection System to Transferable Adversarial Attacks](https://arxiv.org/abs/2510.01676)
*Milad Nasr,Yanick Fratantonio,Luca Invernizzi,Ange Albertini,Loua Farah,Alex Petit-Bianco,Andreas Terzis,Kurt Thomas,Elie Bursztein,Nicholas Carlini*

Main category: cs.CR

TL;DR: 通过仅修改恶意软件样本的13个字节，可以成功绕过Gmail的Magika机器学习模型，在90%的情况下实现恶意文件传输。研究开发了防御措施，使攻击成功率降至20%需要修改50字节。


<details>
  <summary>Details</summary>
Motivation: 深度学习模型在生产系统中的广泛应用使其成为系统级漏洞的来源。研究针对ML组件的对抗攻击如何影响整个生产级恶意软件检测系统，特别是Gmail的恶意软件检测流程。

Method: 对Gmail的恶意软件检测流程进行案例研究，分析其依赖的Magika机器学习模型。设计对抗样本来欺骗Magika模型，使恶意软件被错误路由到不合适的检测器。

Result: 仅修改13字节即可在90%情况下成功绕过Magika检测，允许通过Gmail传输恶意文件。开发的防御措施使攻击难度显著增加，需要50字节修改才能达到20%成功率。

Conclusion: ML组件在生产系统中的部署会引入系统级安全风险。通过针对性的对抗攻击可以绕过整个恶意软件检测系统，但通过适当的防御措施可以有效缓解此类攻击。防御方案已与Google工程师合作部署到Gmail生产环境中。

Abstract: As deep learning models become widely deployed as components within larger
production systems, their individual shortcomings can create system-level
vulnerabilities with real-world impact. This paper studies how adversarial
attacks targeting an ML component can degrade or bypass an entire
production-grade malware detection system, performing a case study analysis of
Gmail's pipeline where file-type identification relies on a ML model.
  The malware detection pipeline in use by Gmail contains a machine learning
model that routes each potential malware sample to a specialized malware
classifier to improve accuracy and performance. This model, called Magika, has
been open sourced. By designing adversarial examples that fool Magika, we can
cause the production malware service to incorrectly route malware to an
unsuitable malware detector thereby increasing our chance of evading detection.
Specifically, by changing just 13 bytes of a malware sample, we can
successfully evade Magika in 90% of cases and thereby allow us to send malware
files over Gmail. We then turn our attention to defenses, and develop an
approach to mitigate the severity of these types of attacks. For our defended
production model, a highly resourced adversary requires 50 bytes to achieve
just a 20% attack success rate. We implement this defense, and, thanks to a
collaboration with Google engineers, it has already been deployed in production
for the Gmail classifier.

</details>


### [25] [Towards Imperceptible Adversarial Defense: A Gradient-Driven Shield against Facial Manipulations](https://arxiv.org/abs/2510.01699)
*Yue Li,Linying Xue,Dongdong Lin,Qiushi Li,Hui Tian,Hongxia Wang*

Main category: cs.CR

TL;DR: 提出了GRASP方法，通过梯度投影机制解决对抗性防御中防御效果与视觉质量之间的权衡问题，在保持高防御成功率的同时显著提升图像质量。


<details>
  <summary>Details</summary>
Motivation: 随着生成模型的繁荣，被操纵的人脸图像越来越容易获取，引发隐私侵权和社会信任问题。现有主动防御方法在不可感知性和防御效果之间存在权衡，强扰动可能破坏伪造但降低视觉保真度。

Method: 提出基于梯度投影的对抗性主动防御方法GRASP，首次成功整合结构相似性损失和低频损失来增强扰动不可感知性，通过梯度投影机制缓解防御效果损失与视觉质量损失之间的梯度冲突。

Result: 实验验证GRASP的有效性，PSNR超过40dB，SSIM达到0.99，对人脸属性操作的防御成功率达到100%，在视觉质量方面显著优于现有方法。

Conclusion: GRASP方法通过平衡优化实现了在保持图像保真度的同时不牺牲防御性能，为解决主动防御中的视觉质量与防御效果权衡问题提供了有效解决方案。

Abstract: With the flourishing prosperity of generative models, manipulated facial
images have become increasingly accessible, raising concerns regarding privacy
infringement and societal trust. In response, proactive defense strategies
embed adversarial perturbations into facial images to counter deepfake
manipulation. However, existing methods often face a tradeoff between
imperceptibility and defense effectiveness-strong perturbations may disrupt
forgeries but degrade visual fidelity. Recent studies have attempted to address
this issue by introducing additional visual loss constraints, yet often
overlook the underlying gradient conflicts among losses, ultimately weakening
defense performance. To bridge the gap, we propose a gradient-projection-based
adversarial proactive defense (GRASP) method that effectively counters facial
deepfakes while minimizing perceptual degradation. GRASP is the first approach
to successfully integrate both structural similarity loss and low-frequency
loss to enhance perturbation imperceptibility. By analyzing gradient conflicts
between defense effectiveness loss and visual quality losses, GRASP pioneers
the design of the gradient-projection mechanism to mitigate these conflicts,
enabling balanced optimization that preserves image fidelity without
sacrificing defensive performance. Extensive experiments validate the efficacy
of GRASP, achieving a PSNR exceeding 40 dB, SSIM of 0.99, and a 100% defense
success rate against facial attribute manipulations, significantly
outperforming existing approaches in visual quality.

</details>


### [26] [Constructions of Efficiently Implementable Boolean Functions with Provable Nonlinearity/Resiliency/Algebraic Immunity Trade-Offs](https://arxiv.org/abs/2510.01720)
*Palash Sarkar*

Main category: cs.CR

TL;DR: 提出了多个布尔函数族，在弹性、非线性度和代数免疫性之间实现可证明的权衡，构造的n变量函数具有线性复杂度实现。


<details>
  <summary>Details</summary>
Motivation: 设计在密码学应用中同时满足高弹性、高非线性度和高代数免疫性的布尔函数，解决这些安全属性之间的权衡问题。

Method: 构造了多个函数族，给定参数m0、x0、a0，可以构建n变量布尔函数，其中n与参数呈线性关系，使用O(n)个门实现。

Result: 对于任意给定的m0≥0、x0≥1、a0≥1，都能构造出弹性至少为m0、线性偏置最多为2^{-x0}、代数免疫性至少为a0的布尔函数。

Conclusion: 成功实现了布尔函数在弹性、非线性度和代数免疫性之间的可证明权衡，且具有高效的线性复杂度实现。

Abstract: We describe several families of efficiently implementable Boolean functions
achieving provable trade-offs between resiliency, nonlinearity, and algebraic
immunity. In concrete terms, the following result holds for each of the
function families that we propose. Given integers $m_0\geq 0$, $x_0\geq 1$, and
$a_0\geq 1$, it is possible to construct an $n$-variable function which has
resiliency at least $m_0$, linear bias (which is an equivalent method of
expressing nonlinearity) at most $2^{-x_0}$ and algebraic immunity at least
$a_0$; further, $n$ is linear in $m_0$, $x_0$ and $a_0$, and the function can
be implemented using $O(n)$ gates.

</details>


### [27] [Secure Multi-Modal Data Fusion in Federated Digital Health Systems via MCP](https://arxiv.org/abs/2510.01780)
*Aueaphum Aueawatthanaphisut*

Main category: cs.CR

TL;DR: 提出基于模型上下文协议(MCP)的多模态联邦学习框架，通过标准化接口实现医疗数据的隐私保护和跨模态融合，在诊断准确率和客户端稳定性方面显著提升。


<details>
  <summary>Details</summary>
Motivation: 解决异构医疗数据安全互操作的挑战，现有联邦学习框架缺乏多模态数据融合的标准化机制，特别是在分布式和资源受限环境中的协调问题。

Method: 使用MCP作为互操作层，统一三个支柱：多模态特征对齐、差分隐私安全聚合、能量感知调度，实现AI代理和工具链的自适应编排。

Result: 在基准数据集和临床队列中，诊断准确率比基线联邦学习提升9.8%，客户端掉线率降低54%，达到临床可接受的隐私-效用平衡。

Conclusion: MCP支持的多模态融合为下一代联邦医疗基础设施提供了可扩展且可信赖的路径，有助于实现公平的医疗AI服务。

Abstract: Secure and interoperable integration of heterogeneous medical data remains a
grand challenge in digital health. Current federated learning (FL) frameworks
offer privacy-preserving model training but lack standardized mechanisms to
orchestrate multi-modal data fusion across distributed and resource-constrained
environments. This study introduces a novel framework that leverages the Model
Context Protocol (MCP) as an interoperability layer for secure, cross-agent
communication in multi-modal federated healthcare systems. The proposed
architecture unifies three pillars: (i) multi-modal feature alignment for
clinical imaging, electronic medical records, and wearable IoT data; (ii)
secure aggregation with differential privacy to protect patient-sensitive
updates; and (iii) energy-aware scheduling to mitigate dropouts in mobile
clients. By employing MCP as a schema-driven interface, the framework enables
adaptive orchestration of AI agents and toolchains while ensuring compliance
with privacy regulations. Experimental evaluation on benchmark datasets and
pilot clinical cohorts demonstrates up to 9.8\% improvement in diagnostic
accuracy compared with baseline FL, a 54\% reduction in client dropout rates,
and clinically acceptable privacy--utility trade-offs. These results highlight
MCP-enabled multi-modal fusion as a scalable and trustworthy pathway toward
equitable, next-generation federated health infrastructures.

</details>


### [28] [ZK-WAGON: Imperceptible Watermark for Image Generation Models using ZK-SNARKs](https://arxiv.org/abs/2510.01967)
*Aadarsh Anantha Ramakrishnan,Shubham Agarwal,Selvanayagam S,Kunwar Singh*

Main category: cs.CR

TL;DR: ZK-WAGON是首个使用ZK-SNARKs为图像生成模型添加水印的系统，能够在不暴露模型权重或敏感信息的情况下提供可验证的来源证明。


<details>
  <summary>Details</summary>
Motivation: 随着图像生成模型日益强大和普及，合成媒体的真实性、所有权和滥用问题变得至关重要。传统水印方法会降低图像质量、易被移除或需要访问机密模型内部信息。

Method: 提出选择性层ZK电路创建(SL-ZKCC)方法，将图像生成模型的关键层转换为电路，显著减少证明生成时间。使用LSB隐写术将ZK-SNARK证明不可察觉地嵌入生成图像中。

Result: 该系统在GAN和Diffusion模型上进行了演示，提供了一个安全、模型无关的可信AI图像生成管道。

Conclusion: ZK-WAGON为图像生成模型提供了一种安全、可扩展的水印解决方案，解决了传统方法的局限性。

Abstract: As image generation models grow increasingly powerful and accessible,
concerns around authenticity, ownership, and misuse of synthetic media have
become critical. The ability to generate lifelike images indistinguishable from
real ones introduces risks such as misinformation, deepfakes, and intellectual
property violations. Traditional watermarking methods either degrade image
quality, are easily removed, or require access to confidential model internals
- making them unsuitable for secure and scalable deployment. We are the first
to introduce ZK-WAGON, a novel system for watermarking image generation models
using the Zero-Knowledge Succinct Non Interactive Argument of Knowledge
(ZK-SNARKs). Our approach enables verifiable proof of origin without exposing
model weights, generation prompts, or any sensitive internal information. We
propose Selective Layer ZK-Circuit Creation (SL-ZKCC), a method to selectively
convert key layers of an image generation model into a circuit, reducing proof
generation time significantly. Generated ZK-SNARK proofs are imperceptibly
embedded into a generated image via Least Significant Bit (LSB) steganography.
We demonstrate this system on both GAN and Diffusion models, providing a
secure, model-agnostic pipeline for trustworthy AI image generation.

</details>


### [29] [Mirage Fools the Ear, Mute Hides the Truth: Precise Targeted Adversarial Attacks on Polyphonic Sound Event Detection Systems](https://arxiv.org/abs/2510.02158)
*Junjie Su,Weifei Jin,Yuxin Cao,Derui Wang,Kai Ye,Jie Hao*

Main category: cs.CR

TL;DR: 提出了M2A框架，针对多音源声音事件检测系统进行精准的对抗攻击，通过保护损失确保非目标区域不受影响，并引入新的评估指标编辑精度来平衡攻击效果和精度。


<details>
  <summary>Details</summary>
Motivation: 声音事件检测系统在安全关键应用中部署增多，但其对抗攻击鲁棒性研究不足。现有攻击方法要么效果不佳，要么缺乏精确性，会无意影响非目标区域。

Method: 提出Mirage和Mute Attack框架，在优化过程中对非目标输出施加特定约束（保护损失），确保攻击不改变非目标区域的模型输出，实现精确攻击。

Result: 在两个最先进的SED模型上分别达到94.56%和99.11%的编辑精度，证明该框架在保持足够攻击效果的同时显著提升了攻击精度。

Conclusion: M2A框架能够有效进行精准的对抗攻击，解决了现有方法在效果和精度之间的平衡问题，为SED系统的安全性评估提供了新工具。

Abstract: Sound Event Detection (SED) systems are increasingly deployed in
safety-critical applications such as industrial monitoring and audio
surveillance. However, their robustness against adversarial attacks has not
been well explored. Existing audio adversarial attacks targeting SED systems,
which incorporate both detection and localization capabilities, often lack
effectiveness due to SED's strong contextual dependencies or lack precision by
focusing solely on misclassifying the target region as the target event,
inadvertently affecting non-target regions. To address these challenges, we
propose the Mirage and Mute Attack (M2A) framework, which is designed for
targeted adversarial attacks on polyphonic SED systems. In our optimization
process, we impose specific constraints on the non-target output, which we
refer to as preservation loss, ensuring that our attack does not alter the
model outputs for non-target region, thus achieving precise attacks.
Furthermore, we introduce a novel evaluation metric Editing Precison (EP) that
balances effectiveness and precision, enabling our method to simultaneously
enhance both. Comprehensive experiments show that M2A achieves 94.56% and
99.11% EP on two state-of-the-art SED models, demonstrating that the framework
is sufficiently effective while significantly enhancing attack precision.

</details>


### [30] [NoMod: A Non-modular Attack on Module Learning With Errors](https://arxiv.org/abs/2510.02162)
*Cristian Bassotto,Ermes Franch,Marina Krček,Stjepan Picek*

Main category: cs.CR

TL;DR: NoMod ML-Attack是一种混合白盒密码分析方法，通过将模约简的环绕视为统计损坏，将秘密恢复转化为稳健线性估计问题，成功恢复了MLWE问题的秘密参数。


<details>
  <summary>Details</summary>
Motivation: 量子计算的发展威胁经典公钥密码学，NIST采用了基于模块学习错误问题的后量子方案，需要开发新的密码分析方法来评估这些方案的安全性。

Method: 结合优化的格预处理（包括减少向量保存和代数放大）与通过Tukey双权损失训练的稳健估计器，将模约简的环绕建模为统计损坏。

Result: 实验显示NoMod能够完全恢复维度n=350的二进制秘密，恢复n=256的稀疏二项式秘密，并在CRYSTALS-Kyber参数设置下成功恢复稀疏秘密。

Conclusion: NoMod方法有效规避了模约简建模的挑战，为MLWE问题的密码分析提供了新的有效工具。

Abstract: The advent of quantum computing threatens classical public-key cryptography,
motivating NIST's adoption of post-quantum schemes such as those based on the
Module Learning With Errors (Module-LWE) problem. We present NoMod ML-Attack, a
hybrid white-box cryptanalytic method that circumvents the challenge of
modeling modular reduction by treating wrap-arounds as statistical corruption
and casting secret recovery as robust linear estimation. Our approach combines
optimized lattice preprocessing--including reduced-vector saving and algebraic
amplification--with robust estimators trained via Tukey's Biweight loss.
Experiments show NoMod achieves full recovery of binary secrets for dimension
$n = 350$, recovery of sparse binomial secrets for $n = 256$, and successful
recovery of sparse secrets in CRYSTALS-Kyber settings with parameters $(n, k) =
(128, 3)$ and $(256, 2)$. We release our implementation in an anonymous
repository https://anonymous.4open.science/r/NoMod-3BD4.

</details>


### [31] [Testing Stability and Robustness in Three Cryptographic Chaotic Systems](https://arxiv.org/abs/2510.02184)
*N. A. Anagnostopoulos,K. Konstantinidis,A. N. Miliou,S. G. Stavrinides*

Main category: cs.CR

TL;DR: 测试三种不同密码学混沌系统在噪声环境下的同步稳定性和鲁棒性，并与安全需求进行比较


<details>
  <summary>Details</summary>
Motivation: 在实际应用中，即使存在噪声，驱动-响应系统也需要始终保持同步，这对系统稳定性提出了要求

Method: 测试三种已知的密码学混沌系统的稳定性和鲁棒性

Result: 比较了不同系统在噪声环境下的同步性能

Conclusion: 评估了混沌系统在安全应用中的适用性

Abstract: In practical applications, it is crucial that the drive-response systems,
although identical in all respects, are synchronized at all times, even if
there is noise present. In this work, we test the stability and robustness of
three distinct and well-known cryptographic chaotic systems, and compare the
results in relation to the desired security.

</details>


### [32] [Authentication Security of PRF GNSS Ranging](https://arxiv.org/abs/2510.02196)
*Jason Anderson*

Main category: cs.CR

TL;DR: 该论文分析了基于伪随机函数(PRF)的GNSS测距在不同欺骗模型下的认证安全性，包括SCER欺骗器。通过应用PRF技术，可以建立对GNSS伪距和PNT解决方案的信任。


<details>
  <summary>Details</summary>
Motivation: 研究GNSS测距在多种欺骗模型下的认证安全性，特别是针对无法预测广播前测距码的PRF技术，以增强GNSS系统的安全性和可靠性。

Method: 应用伪随机函数(PRF)技术于GNSS测距协议，分析在SCER等欺骗模型下的安全性，并计算认证安全所需的最小数据量。

Result: 对于Galileo的SAS服务，最多需要400ms的E6-C数据才能在非SCER模型下实现128位认证安全。对于SCER攻击者，预测了其所需的接收设备能力。

Conclusion: 该工作可用于设计满足有用认证安全要求的PRF GNSS测距协议，通过计算漏检概率来确保系统安全性。

Abstract: This work derives the authentication security of pseudorandom function (PRF)
GNSS ranging under multiple GNSS spoofing models, including the Security Code
Estimation and Replay (SCER) spoofer. When GNSS ranging codes derive from a PRF
utilizing a secret known only to the broadcaster, the spoofer cannot predict
the ranging code before broadcast. Therefore, PRF ranging can be used to
establish trust in the GNSS pseudoranges and the resulting receiver position,
navigation, and timing (PNT) solution. I apply the methods herein to Galileo's
Signal Authentication Service (SAS) utilizing the encrypted Galileo E6-C signal
to compute that, at most, 400 ms of Galileo E6-C data to assert 128-bit
authentication security under non-SCER models. For the SCER adversary, I
predict the adversary's needed receiving radio equipment to break
authentication security. One can use this work to design a PRF GNSS ranging
protocol to meet useful authentication security requirements by computing the
probability of missed detection.

</details>


### [33] [An efficient quantum algorithm for computing $S$-units and its applications](https://arxiv.org/abs/2510.02280)
*Jean-Francois Biasse,Fang Song*

Main category: cs.CR

TL;DR: 本文详细证明了Biasse和Song提出的量子多项式时间算法，用于计算数域的S-单位群，该算法可应用于计算类群、S-类群、相对类群、单位群、射线类群，解决主理想问题、某些范数方程，以及分解理想类群中的理想类。


<details>
  <summary>Details</summary>
Motivation: 开发高效的量子算法来解决数论中的基本计算问题，特别是与数域相关的代数结构计算，如类群、单位群等，这些在密码学和计算数论中具有重要意义。

Method: 基于Biasse和Song的量子多项式时间算法，该算法直接计算数域的S-单位群，并通过该结果推导出其他相关问题的解决方案。

Result: 算法能够在量子多项式时间内计算S-单位群，并由此解决类群计算、主理想问题、范数方程等一系列数论问题。结合其他研究成果，还能找到主理想的短生成元和分圆域理想格中的"轻度短向量"。

Conclusion: 该量子算法为解决数论中的多个重要计算问题提供了高效的解决方案，特别是在密码学应用方面具有重要价值，能够找到理想格中的短向量，这对基于格的密码系统分析具有重要意义。

Abstract: In this paper, we provide details on the proofs of the quantum polynomial
time algorithm of Biasse and Song (SODA 16) for computing the $S$-unit group of
a number field. This algorithm directly implies polynomial time methods to
calculate class groups, S-class groups, relative class group and the unit
group, ray class groups, solve the principal ideal problem, solve certain norm
equations, and decompose ideal classes in the ideal class group. Additionally,
combined with a result of Cramer, Ducas, Peikert and Regev (Eurocrypt 2016),
the resolution of the principal ideal problem allows one to find short
generators of a principal ideal. Likewise, methods due to Cramer, Ducas and
Wesolowski (Eurocrypt 2017) use the resolution of the principal ideal problem
and the decomposition of ideal classes to find so-called ``mildly short
vectors'' in ideal lattices of cyclotomic fields.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [34] [OR-Toolformer: Modeling and Solving Operations Research Problems with Tool Augmented Large Language Models](https://arxiv.org/abs/2510.01253)
*Jianzhang Zhang,Jialong Zhou,Chuang Liu*

Main category: cs.AI

TL;DR: OR-Toolformer通过微调Llama-3.1-8B-Instruct模型，结合半自动数据合成流程和外部求解器增强，在运筹学问题上取得了优于基线模型的性能表现。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在数学推理方面表现出色，但依赖闭源API存在隐私问题，从头训练开源模型计算成本高昂。

Method: 使用半自动数据合成流程生成多样化运筹学问题-答案对，通过微调Llama-3.1-8B-Instruct模型，并增强外部求解器来生成API调用。

Result: 在四个标准基准测试中的三个上，OR-Toolformer达到80.1%的执行准确率，超过同等规模基线4.3%以上；在两种未见过的运筹学问题类型上，零样本评估达到54%平均准确率，比最强基线提高21个百分点。

Conclusion: 工具增强的微调方法对于准确且可泛化的运筹学问题建模和求解具有有效性。

Abstract: Large language models (LLMs) demonstrate strong mathematical reasoning, but
reliance on closed-source APIs for OR tasks raises privacy concerns, and
training open-source models from scratch incurs high compute costs. We
introduce OR-Toolformer, which fine-tunes Llama-3.1-8B-Instruct with a
semi-automatic data synthesis pipeline that generates diverse OR problem-answer
pairs and augments the model with external solvers to produce API calls. On
three of four standard benchmarks, OR-Toolformer achieves up to 80.1% execution
accuracy, exceeding size-matched baselines by over 4.3%. In zero-shot
evaluation on two unseen OR problem types, it attains 54% average accuracy, a
21 percentage-point improvement over the strongest baseline. These findings
validate the efficacy of tool-augmented fine-tuning LLMs for accurate and
generalizable OR problem modeling and solving.

</details>


### [35] [Modeling Others' Minds as Code](https://arxiv.org/abs/2510.01272)
*Kunal Jha,Aydan Yuenan Huang,Eric Ye,Natasha Jaques,Max Kleiman-Weiner*

Main category: cs.AI

TL;DR: 提出ROTE算法，通过将日常社交互动建模为行为程序，结合大语言模型生成假设空间和概率推理处理不确定性，显著提升人类行为预测准确率。


<details>
  <summary>Details</summary>
Motivation: 现有的人类行为建模方法要么对理性做出不切实际的假设，要么计算量过大难以快速适应。日常社交互动可能遵循可预测的"脚本"模式，这为高效行为预测提供了机会。

Method: ROTE算法将行为建模为计算机代码程序而非基于信念和欲望的策略，利用大语言模型合成行为程序的假设空间，并通过概率推理处理该空间的不确定性。

Result: 在网格世界任务和大规模家庭模拟器中测试，ROTE从稀疏观察中预测人类和AI行为，在样本内准确率和样本外泛化方面比竞争基线（包括行为克隆和基于LLM的方法）高出50%。

Conclusion: 通过将动作理解视为程序合成问题，ROTE为AI系统在现实世界中高效有效预测人类行为开辟了新路径。

Abstract: Accurate prediction of human behavior is essential for robust and safe
human-AI collaboration. However, existing approaches for modeling people are
often data-hungry and brittle because they either make unrealistic assumptions
about rationality or are too computationally demanding to adapt rapidly. Our
key insight is that many everyday social interactions may follow predictable
patterns; efficient "scripts" that minimize cognitive load for actors and
observers, e.g., "wait for the green light, then go." We propose modeling these
routines as behavioral programs instantiated in computer code rather than
policies conditioned on beliefs and desires. We introduce ROTE, a novel
algorithm that leverages both large language models (LLMs) for synthesizing a
hypothesis space of behavioral programs, and probabilistic inference for
reasoning about uncertainty over that space. We test ROTE in a suite of
gridworld tasks and a large-scale embodied household simulator. ROTE predicts
human and AI behaviors from sparse observations, outperforming competitive
baselines -- including behavior cloning and LLM-based methods -- by as much as
50% in terms of in-sample accuracy and out-of-sample generalization. By
treating action understanding as a program synthesis problem, ROTE opens a path
for AI systems to efficiently and effectively predict human behavior in the
real-world.

</details>


### [36] [Cyber Academia-Chemical Engineering (CA-ChemE): A Living Digital Town for Self-Directed Research Evolution and Emergent Scientific Discovery](https://arxiv.org/abs/2510.01293)
*Zekun Jiang,Chunming Xu,Tianhang Zhou*

Main category: cs.AI

TL;DR: 提出了CA-ChemE系统，一个通过多智能体协作实现自主研究进化和涌现科学发现的数字城镇，解决了AI在化学工程中跨学科协作和未知问题探索的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有AI系统在化学工程中存在跨学科协作不足和探索未知问题能力有限的问题，需要构建能够自主进化和发现新知识的智能生态系统。

Method: 集成领域特定知识库、知识增强技术和协作智能体，构建CA-ChemE系统，通过多智能体协作和本体工程技术实现跨领域合作。

Result: 知识库增强机制使7个专家智能体的对话质量平均提高10-15%；协作智能体干预使远领域专家对的效率提升8.5%，而近领域对仅提升0.8%，揭示了知识库差距导致的协作效率递减效应。

Conclusion: 精心设计的多智能体架构为化学工程中的自主科学发现提供了可行路径，证明了通过智能体协作可以克服知识差距带来的协作瓶颈。

Abstract: The rapid advancement of artificial intelligence (AI) has demonstrated
substantial potential in chemical engineering, yet existing AI systems remain
limited in interdisciplinary collaboration and exploration of uncharted
problems. To address these issues, we present the Cyber Academia-Chemical
Engineering (CA-ChemE) system, a living digital town that enables self-directed
research evolution and emergent scientific discovery through multi-agent
collaboration. By integrating domain-specific knowledge bases, knowledge
enhancement technologies, and collaboration agents, the system successfully
constructs an intelligent ecosystem capable of deep professional reasoning and
efficient interdisciplinary collaboration. Our findings demonstrate that
knowledge base-enabled enhancement mechanisms improved dialogue quality scores
by 10-15% on average across all seven expert agents, fundamentally ensuring
technical judgments are grounded in verifiable scientific evidence. However, we
observed a critical bottleneck in cross-domain collaboration efficiency,
prompting the introduction of a Collaboration Agent (CA) equipped with ontology
engineering capabilities. CA's intervention achieved 8.5% improvements for
distant-domain expert pairs compared to only 0.8% for domain-proximate pairs -
a 10.6-fold difference - unveiling the "diminished collaborative efficiency
caused by knowledge-base gaps" effect. This study demonstrates how carefully
designed multi-agent architectures can provide a viable pathway toward
autonomous scientific discovery in chemical engineering.

</details>


### [37] [The Social Laboratory: A Psychometric Framework for Multi-Agent LLM Evaluation](https://arxiv.org/abs/2510.01295)
*Zarreen Reza*

Main category: cs.AI

TL;DR: 提出了一个基于多智能体辩论的新型评估框架，用于量化LLM智能体在交互环境中的社交和认知行为，发现智能体具有强烈的共识寻求倾向，即使在没有明确指令的情况下也能达到高语义一致性。


<details>
  <summary>Details</summary>
Motivation: 随着LLM从静态工具向自主智能体转变，传统的下游任务评估基准已不足以捕捉智能体在交互环境中出现的社交和认知动态。

Method: 使用多智能体辩论作为受控的"社交实验室"，让具有不同角色和激励的LLM智能体在LLM主持人的监督下就各种挑战性话题进行辩论，并通过新的心理测量和语义指标进行分析。

Result: 在数百场辩论中发现智能体具有强大的共识寻求倾向，语义一致性高达μ>0.88；分配的角色会诱导稳定的心理测量特征；主持人的角色能显著改变辩论结果。

Conclusion: 这项工作为面向智能体环境的新型动态、基于心理测量的评估协议提供了蓝图，为理解和塑造下一代AI智能体的社交行为提供了关键方法。

Abstract: As Large Language Models (LLMs) transition from static tools to autonomous
agents, traditional evaluation benchmarks that measure performance on
downstream tasks are becoming insufficient. These methods fail to capture the
emergent social and cognitive dynamics that arise when agents communicate,
persuade, and collaborate in interactive environments. To address this gap, we
introduce a novel evaluation framework that uses multi-agent debate as a
controlled "social laboratory" to discover and quantify these behaviors. In our
framework, LLM-based agents, instantiated with distinct personas and
incentives, deliberate on a wide range of challenging topics under the
supervision of an LLM moderator. Our analysis, enabled by a new suite of
psychometric and semantic metrics, reveals several key findings. Across
hundreds of debates, we uncover a powerful and robust emergent tendency for
agents to seek consensus, consistently reaching high semantic agreement ({\mu}
> 0.88) even without explicit instruction and across sensitive topics. We show
that assigned personas induce stable, measurable psychometric profiles,
particularly in cognitive effort, and that the moderators persona can
significantly alter debate outcomes by structuring the environment, a key
finding for external AI alignment. This work provides a blueprint for a new
class of dynamic, psychometrically grounded evaluation protocols designed for
the agentic setting, offering a crucial methodology for understanding and
shaping the social behaviors of the next generation of AI agents. We have
released the code and results at
https://github.com/znreza/multi-agent-LLM-eval-for-debate.

</details>


### [38] [Agentic Jigsaw Interaction Learning for Enhancing Visual Perception and Reasoning in Vision-Language Models](https://arxiv.org/abs/2510.01304)
*Yu Zeng,Wenxuan Huang,Shiting Huang,Xikun Bao,Yukun Qi,Yiming Zhao,Qiuchen Wang,Lin Chen,Zehui Chen,Huaian Chen,Wanli Ouyang,Feng Zhao*

Main category: cs.AI

TL;DR: AGILE通过将拼图任务转化为交互式学习过程，显著提升了视觉语言模型的基础感知和推理能力，在拼图任务上准确率从9.5%提升至82.8%，并在9个通用视觉任务上平均提升3.1%。


<details>
  <summary>Details</summary>
Motivation: 当前大型视觉语言模型在多模态理解和推理方面虽有进步，但其基础感知和推理能力仍然有限，即使在简单拼图任务上表现也接近随机。高质量视觉语言数据的稀缺性和有限可扩展性限制了这些能力的提升。

Method: AGILE将拼图解决制定为交互过程，模型在每一步基于当前状态生成可执行代码来执行动作，环境提供细粒度视觉反馈来指导任务完成。通过观察和交互的迭代循环，模型通过探索和反馈逐步提升感知和推理能力。

Result: AGILE在复杂度不同的拼图任务上显著提升性能（在2×2设置下准确率从9.5%提升至82.8%），并在9个通用视觉任务上表现出强泛化能力，平均提升3.1%。

Conclusion: 这项工作为推进多模态模型的推理和泛化能力开辟了新途径，并为多模态强化学习数据稀缺问题提供了高效、可扩展的解决方案。

Abstract: Although current large Vision-Language Models (VLMs) have advanced in
multimodal understanding and reasoning, their fundamental perceptual and
reasoning abilities remain limited. Specifically, even on simple jigsaw tasks,
existing VLMs perform near randomly, revealing deficiencies in core perception
and reasoning capabilities. While high-quality vision-language data can enhance
these capabilities, its scarcity and limited scalability impose significant
constraints. To address this, we propose AGILE, an Agentic jiGsaw Interaction
Learning for Enhancing visual perception and reasoning in VLMs. AGILE
formulates jigsaw solving as an interactive process, enabling the model to
progressively engage with the environment. At each step, the model generates
executable code to perform an action based on the current state, while the
environment provides fine-grained visual feedback to guide task completion.
Through this iterative cycle of observation and interaction, the model
incrementally improves its perceptual and reasoning capabilities via
exploration and feedback. Experimental results show that AGILE not only
substantially boosts performance on jigsaw tasks of varying complexity (e.g.,
increasing accuracy from 9.5% to 82.8% under the 2 $\times$ 2 setting) but also
demonstrates strong generalization across 9 general vision tasks, achieving an
average improvement of 3.1%. These results indicate notable enhancements in
both perceptual and reasoning abilities. This work opens a new avenue for
advancing reasoning and generalization in multimodal models and provides an
efficient, scalable solution to the scarcity of multimodal reinforcement
learning data. The code and datasets is available at
https://github.com/yuzeng0-0/AGILE .

</details>


### [39] [Aristotle: IMO-level Automated Theorem Proving](https://arxiv.org/abs/2510.01346)
*Tudor Achim,Alex Best,Kevin Der,Mathïs Fédérico,Sergei Gukov,Daniel Halpern-Leister,Kirsten Henningsgard,Yury Kudryashov,Alexander Meiburg,Martin Michelsen,Riley Patterson,Eric Rodriguez,Laura Scharff,Vikram Shanker,Vladmir Sicca,Hari Sowrirajan,Aidan Swope,Matyas Tamas,Vlad Tenev,Jonathan Thomm,Harold Williams,Lawrence Wu*

Main category: cs.AI

TL;DR: Aristotle系统结合形式验证与非形式推理，在2025年国际数学奥林匹克竞赛中达到金牌级别表现


<details>
  <summary>Details</summary>
Motivation: 开发一个能够结合形式验证的严谨性和非形式推理的灵活性的AI系统，以解决复杂的数学问题

Method: 整合三个主要组件：Lean证明搜索系统、生成并形式化引理的非形式推理系统、专用几何求解器

Result: 在2025年国际数学奥林匹克竞赛中达到金牌级别表现，展示了自动定理证明的最先进性能

Conclusion: Aristotle系统成功结合形式验证和非形式推理，为自动定理证明提供了可扩展的高性能解决方案

Abstract: We introduce Aristotle, an AI system that combines formal verification with
informal reasoning, achieving gold-medal-equivalent performance on the 2025
International Mathematical Olympiad problems. Aristotle integrates three main
components: a Lean proof search system, an informal reasoning system that
generates and formalizes lemmas, and a dedicated geometry solver. Our system
demonstrates state-of-the-art performance with favorable scaling properties for
automated theorem proving.

</details>


### [40] [MEMTRACK: Evaluating Long-Term Memory and State Tracking in Multi-Platform Dynamic Agent Environments](https://arxiv.org/abs/2510.01353)
*Darshan Deshpande,Varun Gangal,Hersh Mehta,Anand Kannappan,Rebecca Qian,Peng Wang*

Main category: cs.AI

TL;DR: MEMTRACK是一个评估多平台代理环境中长期记忆和状态跟踪的基准测试，专注于企业环境中的动态记忆评估，超越了传统的对话式设置。


<details>
  <summary>Details</summary>
Motivation: 现有记忆基准测试主要关注对话场景，但企业环境中需要评估动态记忆能力，这对于有效应用记忆增强代理至关重要。

Method: 通过整合Slack、Linear和Git等多个通信和生产力平台的异步事件，模拟真实组织工作流程，使用专家设计和基于代理的合成方法构建数据集。

Result: 实验显示最先进的LLM和记忆后端在长跨度记忆利用、跨平台依赖处理和矛盾解决方面面临挑战，表现最佳的GPT-5模型在MEMTRACK上仅达到60%的正确性得分。

Conclusion: MEMTRACK为记忆增强代理的评估研究提供了可扩展框架，为复杂组织环境中的多代理、多平台记忆基准测试奠定了基础。

Abstract: Recent works on context and memory benchmarking have primarily focused on
conversational instances but the need for evaluating memory in dynamic
enterprise environments is crucial for its effective application. We introduce
MEMTRACK, a benchmark designed to evaluate long-term memory and state tracking
in multi-platform agent environments. MEMTRACK models realistic organizational
workflows by integrating asynchronous events across multiple communication and
productivity platforms such as Slack, Linear and Git. Each benchmark instance
provides a chronologically platform-interleaved timeline, with noisy,
conflicting, cross-referring information as well as potential
codebase/file-system comprehension and exploration. Consequently, our benchmark
tests memory capabilities such as acquistion, selection and conflict
resolution. We curate the MEMTRACK dataset through both manual expert driven
design and scalable agent based synthesis, generating ecologically valid
scenarios grounded in real world software development processes. We introduce
pertinent metrics for Correctness, Efficiency, and Redundancy that capture the
effectiveness of memory mechanisms beyond simple QA performance. Experiments
across SoTA LLMs and memory backends reveal challenges in utilizing memory
across long horizons, handling cross-platform dependencies, and resolving
contradictions. Notably, the best performing GPT-5 model only achieves a 60\%
Correctness score on MEMTRACK. This work provides an extensible framework for
advancing evaluation research for memory-augmented agents, beyond existing
focus on conversational setups, and sets the stage for multi-agent,
multi-platform memory benchmarking in complex organizational settings

</details>


### [41] [Retrieval-Augmented Framework for LLM-Based Clinical Decision Support](https://arxiv.org/abs/2510.01363)
*Leon Garza,Anantaa Kotal,Michael A. Grasso,Emre Umucu*

Main category: cs.AI

TL;DR: 提出基于大语言模型的临床决策支持系统，通过分析电子健康记录数据生成治疗建议，采用检索增强生成技术整合结构化和非结构化临床数据，旨在辅助而非替代临床医生决策。


<details>
  <summary>Details</summary>
Motivation: 随着电子健康记录数据的快速增长和临床决策复杂性的增加，需要智能工具来支持数据驱动的医疗决策，同时保持临床医生的判断主导地位。

Method: 采用检索增强生成(RAG)管道，整合自然语言处理和结构化临床输入，通过分析患者人口统计、症状、诊断和治疗历史等数据，检索相似病例并生成治疗建议。

Result: 初步评估显示，在适当约束和严格验证下，基于LLM的工具在处方工作流程中能提供有价值的决策支持，输出具有临床合理性和一致性。

Conclusion: 这是将生成式AI整合到现实世界临床决策中的初步尝试，强调透明度、安全性以及与既定实践的一致性，为未来临床应用奠定基础。

Abstract: The increasing complexity of clinical decision-making, alongside the rapid
expansion of electronic health records (EHR), presents both opportunities and
challenges for delivering data-informed care. This paper proposes a clinical
decision support system powered by Large Language Models (LLMs) to assist
prescribing clinicians. The system generates therapeutic suggestions by
analyzing historical EHR data, including patient demographics, presenting
complaints, clinical symptoms, diagnostic information, and treatment histories.
The framework integrates natural language processing with structured clinical
inputs to produce contextually relevant recommendations. Rather than replacing
clinician judgment, it is designed to augment decision-making by retrieving and
synthesizing precedent cases with comparable characteristics, drawing on local
datasets or federated sources where applicable. At its core, the system employs
a retrieval-augmented generation (RAG) pipeline that harmonizes unstructured
narratives and codified data to support LLM-based inference. We outline the
system's technical components, including representation representation
alignment and generation strategies. Preliminary evaluations, conducted with
de-identified and synthetic clinical datasets, examine the clinical
plausibility and consistency of the model's outputs. Early findings suggest
that LLM-based tools may provide valuable decision support in prescribing
workflows when appropriately constrained and rigorously validated. This work
represents an initial step toward integration of generative AI into real-world
clinical decision-making with an emphasis on transparency, safety, and
alignment with established practices.

</details>


### [42] [Is It Thinking or Cheating? Detecting Implicit Reward Hacking by Measuring Reasoning Effort](https://arxiv.org/abs/2510.01367)
*Xinpeng Wang,Nitish Joshi,Barbara Plank,Rico Angell,He He*

Main category: cs.AI

TL;DR: TRACE是一种检测隐式奖励攻击的方法，通过截断模型的思维链来量化推理努力程度，在数学推理和编程任务中显著优于现有的思维链监控方法。


<details>
  <summary>Details</summary>
Motivation: 奖励攻击问题严重，特别是隐式攻击（思维链表面正常但实际利用漏洞）会绕过现有监控方法，需要新的检测手段。

Method: TRACE方法通过逐步截断模型的思维链，强制模型在不同长度下回答问题，测量验证器通过率，利用准确率-长度曲线下面积来量化推理努力程度。

Result: 在数学推理任务中比最强的72B思维链监控器提升65%以上，在编程任务中比32B监控器提升30%以上，并能发现训练中的未知漏洞。

Conclusion: TRACE提供了一种可扩展的无监督监督方法，在当前监控方法失效的情况下有效检测隐式奖励攻击。

Abstract: Reward hacking, where a reasoning model exploits loopholes in a reward
function to achieve high rewards without solving the intended task, poses a
significant threat. This behavior may be explicit, i.e. verbalized in the
model's chain-of-thought (CoT), or implicit, where the CoT appears benign thus
bypasses CoT monitors. To detect implicit reward hacking, we propose TRACE
(Truncated Reasoning AUC Evaluation). Our key observation is that hacking
occurs when exploiting the loophole is easier than solving the actual task.
This means that the model is using less `effort' than required to achieve high
reward. TRACE quantifies effort by measuring how early a model's reasoning
becomes sufficient to pass a verifier. We progressively truncate a model's CoT
at various lengths, force the model to answer, and measure the verifier-passing
rate at each cutoff. A hacking model, which takes a shortcut, will achieve a
high passing rate with only a small fraction of its CoT, yielding a large area
under the accuracy-vs-length curve. TRACE achieves over 65% gains over our
strongest 72B CoT monitor in math reasoning, and over 30% gains over a 32B
monitor in coding. We further show that TRACE can discover unknown loopholes
during training. Overall, TRACE offers a scalable unsupervised approach for
oversight where current monitoring methods prove ineffective.

</details>


### [43] [Fine-tuning with RAG for Improving LLM Learning of New Skills](https://arxiv.org/abs/2510.01375)
*Humaid Ibrahim,Nikolai Rozanov,Marek Rei*

Main category: cs.AI

TL;DR: 提出一种通过蒸馏将推理时检索转化为学习能力的方法，在交互式基准测试中显著提升智能体性能并减少计算开销


<details>
  <summary>Details</summary>
Motivation: 解决LLM智能体在多步任务中常见的失败模式（如尝试不满足前提条件的动作、发出冗余命令、处理环境约束不当），同时避免检索增强生成(RAG)的外部知识库维护和计算开销

Method: 三步管道：(1)从智能体失败中提取紧凑可重用的提示；(2)使用这些提示在情节开始时通过一次性检索生成改进的教师轨迹；(3)在移除提示字符串的轨迹上训练学生模型，强制内化而非记忆

Result: 在ALFWorld和WebShop基准测试中，蒸馏学生模型始终优于基线智能体：ALFWorld成功率91%（基线79%），WebShop得分72（基线61），同时比检索增强教师少用10-60%的token

Conclusion: 该方法在不同模型规模（7B/14B参数）和智能体架构（ReAct/StateAct）上均能泛化，证明检索优势可以通过针对性微调有效内化，无需永久运行时依赖

Abstract: Large language model (LLM) agents deployed for multi-step tasks frequently
fail in predictable ways: attempting actions with unmet preconditions, issuing
redundant commands, or mishandling environment constraints. While
retrieval-augmented generation (RAG) can improve performance by providing
runtime guidance, it requires maintaining external knowledge databases and adds
computational overhead at every deployment. We propose a simple pipeline that
converts inference-time retrieval into learned competence through distillation.
Our approach: (1) extracts compact, reusable hints from agent failures, (2)
uses these hints to generate improved teacher trajectories via one-shot
retrieval at episode start, and (3) trains student models on these trajectories
with hint strings removed, forcing internalization rather than memorization.
Across two interactive benchmarks, ALFWorld (household tasks) and WebShop
(online shopping), distilled students consistently outperform baseline agents,
achieving up to 91% success on ALFWorld (vs. 79% for baselines) and improving
WebShop scores to 72 (vs. 61 for baselines), while using 10-60% fewer tokens
than retrieval-augmented teachers depending on the environment. The approach
generalizes across model scales (7B/14B parameters) and agent architectures
(ReAct/StateAct), demonstrating that retrieval benefits can be effectively
internalized through targeted fine-tuning without permanent runtime
dependencies.

</details>


### [44] [Automating Data-Driven Modeling and Analysis for Engineering Applications using Large Language Model Agents](https://arxiv.org/abs/2510.01398)
*Yang Liu,Zaid Abulawi,Abhiram Garimidi,Doyeong Lim*

Main category: cs.AI

TL;DR: 提出基于大语言模型(LLM)代理的自动化数据驱动建模管道，用于回归任务，在临界热通量预测基准测试中表现优于传统方法，达到与专家开发的贝叶斯优化深度神经网络相当的性能。


<details>
  <summary>Details</summary>
Motivation: 现代工程依赖大量实验和模拟数据，传统数据驱动方法需要大量人工干预，难以扩展和泛化。需要开发自动化、高效且可靠的建模策略。

Method: 评估两种LLM代理框架：多代理系统（专门化协作代理）和单代理系统（基于ReAct范式）。这些框架自主处理数据预处理、神经网络开发、训练、超参数优化和不确定性量化。

Result: 在包含约25,000个实验数据点的OECD/NEA基准数据集上验证，LLM代理开发的模型超越传统临界热通量查找表，预测精度和不确定性量化与专家开发的贝叶斯优化深度神经网络相当。

Conclusion: LLM代理在自动化复杂工程建模任务方面具有巨大潜力，能显著减少人工工作量，同时达到或超越现有预测性能标准。

Abstract: Modern engineering increasingly relies on vast datasets generated by
experiments and simulations, driving a growing demand for efficient, reliable,
and broadly applicable modeling strategies. There is also heightened interest
in developing data-driven approaches, particularly neural network models, for
effective prediction and analysis of scientific datasets. Traditional
data-driven methods frequently involve extensive manual intervention, limiting
their ability to scale effectively and generalize to diverse applications. In
this study, we propose an innovative pipeline utilizing Large Language Model
(LLM) agents to automate data-driven modeling and analysis, with a particular
emphasis on regression tasks. We evaluate two LLM-agent frameworks: a
multi-agent system featuring specialized collaborative agents, and a
single-agent system based on the Reasoning and Acting (ReAct) paradigm. Both
frameworks autonomously handle data preprocessing, neural network development,
training, hyperparameter optimization, and uncertainty quantification (UQ). We
validate our approach using a critical heat flux (CHF) prediction benchmark,
involving approximately 25,000 experimental data points from the OECD/NEA
benchmark dataset. Results indicate that our LLM-agent-developed model
surpasses traditional CHF lookup tables and delivers predictive accuracy and UQ
on par with state-of-the-art Bayesian optimized deep neural network models
developed by human experts. These outcomes underscore the significant potential
of LLM-based agents to automate complex engineering modeling tasks, greatly
reducing human workload while meeting or exceeding existing standards of
predictive performance.

</details>


### [45] [OntoLogX: Ontology-Guided Knowledge Graph Extraction from Cybersecurity Logs with Large Language Models](https://arxiv.org/abs/2510.01409)
*Luca Cotti,Idilio Drago,Anisa Rula,Devis Bianchini,Federico Cerutti*

Main category: cs.AI

TL;DR: OntoLogX是一个基于LLM的自主AI代理，将原始系统日志转换为基于本体的知识图谱，通过RAG和迭代校正确保图谱质量，并关联到MITRE ATT&CK战术框架。


<details>
  <summary>Details</summary>
Motivation: 系统日志是宝贵的网络威胁情报来源，但由于缺乏结构、语义不一致和跨设备碎片化，其效用受限。需要将噪声异构数据转化为连贯可互操作的表示。

Method: 集成轻量级日志本体与RAG和迭代校正步骤，确保生成的知识图谱在语法和语义上有效。通过LLM聚合会话并预测MITRE ATT&CK战术。

Result: 在公共基准和真实蜜罐数据集上评估，展示了跨多个KG后端的稳健KG生成，以及将对抗活动准确映射到ATT&CK战术的能力。检索和校正提高了精确率和召回率。

Conclusion: 基于本体的表示为可操作的CTI提取提供了价值，代码导向模型在结构化日志分析中表现有效，检索和校正对精度和召回率有益。

Abstract: System logs represent a valuable source of Cyber Threat Intelligence (CTI),
capturing attacker behaviors, exploited vulnerabilities, and traces of
malicious activity. Yet their utility is often limited by lack of structure,
semantic inconsistency, and fragmentation across devices and sessions.
Extracting actionable CTI from logs therefore requires approaches that can
reconcile noisy, heterogeneous data into coherent and interoperable
representations. We introduce OntoLogX, an autonomous Artificial Intelligence
(AI) agent that leverages Large Language Models (LLMs) to transform raw logs
into ontology-grounded Knowledge Graphs (KGs). OntoLogX integrates a
lightweight log ontology with Retrieval Augmented Generation (RAG) and
iterative correction steps, ensuring that generated KGs are syntactically and
semantically valid. Beyond event-level analysis, the system aggregates KGs into
sessions and employs a LLM to predict MITRE ATT&CK tactics, linking low-level
log evidence to higher-level adversarial objectives. We evaluate OntoLogX on
both logs from a public benchmark and a real-world honeypot dataset,
demonstrating robust KG generation across multiple KGs backends and accurate
mapping of adversarial activity to ATT&CK tactics. Results highlight the
benefits of retrieval and correction for precision and recall, the
effectiveness of code-oriented models in structured log analysis, and the value
of ontology-grounded representations for actionable CTI extraction.

</details>


### [46] [A Tale of LLMs and Induced Small Proxies: Scalable Agents for Knowledge Mining](https://arxiv.org/abs/2510.01427)
*Sipeng Zhang,Longfei Yun,Zilong Wang,Jingbo Shang,Letian Peng*

Main category: cs.AI

TL;DR: Falconer是一个结合LLM智能推理与轻量级代理模型的可扩展知识挖掘框架，通过LLM作为规划器和标注器，将用户指令分解为可执行管道并训练小型代理模型，在保持高准确率的同时大幅降低推理成本。


<details>
  <summary>Details</summary>
Motivation: 解决大规模知识挖掘中LLM部署成本过高，而传统分类器-提取器管道虽然高效但脆弱且无法泛化到新任务的问题。

Method: Falconer框架让LLM作为规划器分解用户指令，作为标注器生成监督数据，训练小型代理模型。统一分类和提取为两个原子操作：获取标签和获取跨度，用单一指令跟随模型替代多个任务特定组件。

Result: 实验显示Falconer在指令跟随准确率上与最先进LLM相当，同时推理成本降低90%，大规模知识挖掘速度提升20倍以上。

Conclusion: Falconer为深度研究提供了高效可扩展的基础，平衡了LLM的智能推理能力与轻量级模型的计算效率。

Abstract: At the core of Deep Research is knowledge mining, the task of extracting
structured information from massive unstructured text in response to user
instructions. Large language models (LLMs) excel at interpreting such
instructions but are prohibitively expensive to deploy at scale, while
traditional pipelines of classifiers and extractors remain efficient yet
brittle and unable to generalize to new tasks. We introduce Falconer, a
collaborative framework that combines the agentic reasoning of LLMs with
lightweight proxy models for scalable knowledge mining. In Falconer, LLMs act
as planners, decomposing user instructions into executable pipelines, and as
annotators, generating supervision to train small proxies. The framework
unifies classification and extraction into two atomic operations, get label and
get span, enabling a single instruction-following model to replace multiple
task-specific components. To evaluate the consistency between proxy models
incubated by Falconer and annotations provided by humans and large models, we
construct new benchmarks covering both planning and end-to-end execution.
Experiments show that Falconer closely matches state-of-the-art LLMs in
instruction-following accuracy while reducing inference cost by up to 90% and
accelerating large-scale knowledge mining by more than 20x, offering an
efficient and scalable foundation for Deep Research.

</details>


### [47] [On the Role of Domain Experts in Creating Effective Tutoring Systems](https://arxiv.org/abs/2510.01432)
*Sarath Sreedharan,Kelsey Sikes,Nathaniel Blanchard,Lisa Mason,Nikhil Krishnaswamy,Jill Zarestky*

Main category: cs.AI

TL;DR: 本文探讨了如何利用领域专家的高度策划知识来创建有效的智能辅导系统，提出了两种方法：使用可解释AI技术自动生成课程，以及利用专家指定的学习课程开发自适应辅导系统。


<details>
  <summary>Details</summary>
Motivation: AI教育社区经常忽视领域专家提供的高度策划知识在创建有效辅导系统中的作用，本文旨在强调这一主题的重要性。

Method: 1. 使用可解释AI技术结合专家指定的问题解决规则自动生成课程；2. 利用专家指定的学习课程开发自适应辅导系统。

Result: 通过传粉者识别的案例研究，证明了这些方法在实际应用中的可行性和重要性。

Conclusion: 领域专家的高度策划知识对于创建新颖的教育系统至关重要，特别是在自动课程生成和自适应辅导系统开发方面具有重要价值。

Abstract: The role that highly curated knowledge, provided by domain experts, could
play in creating effective tutoring systems is often overlooked within the AI
for education community. In this paper, we highlight this topic by discussing
two ways such highly curated expert knowledge could help in creating novel
educational systems. First, we will look at how one could use explainable AI
(XAI) techniques to automatically create lessons. Most existing XAI methods are
primarily aimed at debugging AI systems. However, we will discuss how one could
use expert specified rules about solving specific problems along with novel XAI
techniques to automatically generate lessons that could be provided to
learners. Secondly, we will see how an expert specified curriculum for learning
a target concept can help develop adaptive tutoring systems, that can not only
provide a better learning experience, but could also allow us to use more
efficient algorithms to create these systems. Finally, we will highlight the
importance of such methods using a case study of creating a tutoring system for
pollinator identification, where such knowledge could easily be elicited from
experts.

</details>


### [48] [VOGUE: Guiding Exploration with Visual Uncertainty Improves Multimodal Reasoning](https://arxiv.org/abs/2510.01444)
*Rui Liu,Dian Yu,Tong Zheng,Runpeng Dai,Zongxia Li,Wenhao Yu,Zhenwen Liang,Linfeng Song,Haitao Mi,Pratap Tokekar,Dong Yu*

Main category: cs.AI

TL;DR: VOGUE是一种新颖的多模态强化学习方法，通过将视觉输入视为随机上下文，量化策略对视觉扰动的敏感性，从而在输入空间而非输出空间进行探索，有效提升多模态大语言模型的推理能力。


<details>
  <summary>Details</summary>
Motivation: 当前方法将视觉输入视为固定确定性条件，忽略了视觉变化带来的模糊性，难以构建对合理视觉变化具有鲁棒性的策略。

Method: VOGUE通过对称KL散度量化策略对视觉扰动的敏感性，结合不确定性比例奖励、token熵奖励和退火采样调度，平衡探索与利用。

Result: 在Qwen2.5-VL-3B/7B模型上，VOGUE在三个视觉数学基准上平均提升2.6%的pass@1准确率，在三个通用领域推理基准上提升3.7%，同时提高了pass@4性能并缓解了RL微调中常见的探索衰减问题。

Conclusion: 基于视觉输入固有不确定性的探索是改进多模态推理的有效策略。

Abstract: Reinforcement learning with verifiable rewards (RLVR) improves reasoning in
large language models (LLMs) but struggles with exploration, an issue that
still persists for multimodal LLMs (MLLMs). Current methods treat the visual
input as a fixed, deterministic condition, overlooking a critical source of
ambiguity and struggling to build policies robust to plausible visual
variations. We introduce $\textbf{VOGUE (Visual Uncertainty Guided
Exploration)}$, a novel method that shifts exploration from the output (text)
to the input (visual) space. By treating the image as a stochastic context,
VOGUE quantifies the policy's sensitivity to visual perturbations using the
symmetric KL divergence between a "raw" and "noisy" branch, creating a direct
signal for uncertainty-aware exploration. This signal shapes the learning
objective via an uncertainty-proportional bonus, which, combined with a
token-entropy bonus and an annealed sampling schedule, effectively balances
exploration and exploitation. Implemented within GRPO on two model scales
(Qwen2.5-VL-3B/7B), VOGUE boosts pass@1 accuracy by an average of 2.6% on three
visual math benchmarks and 3.7% on three general-domain reasoning benchmarks,
while simultaneously increasing pass@4 performance and mitigating the
exploration decay commonly observed in RL fine-tuning. Our work shows that
grounding exploration in the inherent uncertainty of visual inputs is an
effective strategy for improving multimodal reasoning.

</details>


### [49] [AIReg-Bench: Benchmarking Language Models That Assess AI Regulation Compliance](https://arxiv.org/abs/2510.01474)
*Bill Marino,Rosco Hunter,Zubair Jamali,Marinos Emmanouil Kalpakos,Mudra Kashyap,Isaiah Hinton,Alexa Hanson,Maahum Nazir,Christoph Schnabl,Felix Steffek,Hongkai Wen,Nicholas D. Lane*

Main category: cs.AI

TL;DR: 提出了首个评估LLM在AI法规合规性评估方面性能的基准数据集AIReg-Bench，基于欧盟AI法案构建，包含专家标注的120个技术文档样本。


<details>
  <summary>Details</summary>
Motivation: 随着政府对AI的监管加强，需要评估LLM在AI法规合规性判断方面的能力，但目前缺乏相应的基准测试工具。

Method: 通过两步流程构建数据集：(1) 使用结构化指令提示LLM生成120个虚构但合理的AI系统技术文档样本；(2) 法律专家审查并标注每个样本是否违反AI法案的具体条款。

Result: 创建了AIReg-Bench数据集，并评估了前沿LLM在重现专家合规标签方面的表现，为理解LLM在AI法规合规评估中的机会和局限性提供了起点。

Conclusion: 该工作建立了首个AI法规合规性评估基准，为后续LLM在此领域的性能比较提供了标准，数据集和评估代码已开源。

Abstract: As governments move to regulate AI, there is growing interest in using Large
Language Models (LLMs) to assess whether or not an AI system complies with a
given AI Regulation (AIR). However, there is presently no way to benchmark the
performance of LLMs at this task. To fill this void, we introduce AIReg-Bench:
the first benchmark dataset designed to test how well LLMs can assess
compliance with the EU AI Act (AIA). We created this dataset through a two-step
process: (1) by prompting an LLM with carefully structured instructions, we
generated 120 technical documentation excerpts (samples), each depicting a
fictional, albeit plausible, AI system - of the kind an AI provider might
produce to demonstrate their compliance with AIR; (2) legal experts then
reviewed and annotated each sample to indicate whether, and in what way, the AI
system described therein violates specific Articles of the AIA. The resulting
dataset, together with our evaluation of whether frontier LLMs can reproduce
the experts' compliance labels, provides a starting point to understand the
opportunities and limitations of LLM-based AIR compliance assessment tools and
establishes a benchmark against which subsequent LLMs can be compared. The
dataset and evaluation code are available at
https://github.com/camlsys/aireg-bench.

</details>


### [50] [Lateral Tree-of-Thoughts Surpasses ToT by Incorporating Logically-Consistent, Low-Utility Candidates](https://arxiv.org/abs/2510.01500)
*Abhinav Madahar*

Main category: cs.AI

TL;DR: LToT是一种改进的Tree-of-Thoughts搜索控制器，通过分离效用和逻辑一致性，将低效用但一致的候选视为资产而非浪费，解决了广度饱和和深度近视问题。


<details>
  <summary>Details</summary>
Motivation: 标准Tree-of-Thoughts搜索在大计算预算下存在两个问题：广度饱和（额外样本产生近似重复）和深度近视（噪声短期效用剪枝了长期有回报的分支）。

Method: LToT将前沿分为主线（高效用候选用于开发）和侧线（一致但初始低效用的候选），通过带短路机制的侧向竞赛（LR-SC）探索侧线，使用宽度感知阈值和重复确认，一旦分支超过主线标准立即提升。

Result: 理论证明侧向成本为伪线性Θ(N₀ log_η N₀)，与未加限制的主线指数增长形成对比。实证评估正在进行中。

Conclusion: LToT将大测试时预算转化为原则性多样性，同时保持提升纪律，在不增加计算量的情况下缓解饱和和近视问题。

Abstract: Modern deployments increasingly allocate large test-time compute (thousands
of tokens or many node expansions) to boost reliability. Under such budgets,
standard Tree-of-Thoughts-style search exhibits two pathologies: breadth
saturation (additional samples mostly produce near-duplicates, so width stops
growing) and depth myopia (noisy short-horizon utilities prune branches whose
payoff appears after a few more steps). We propose Lateral Tree-of-Thoughts
(LToT), a drop-in controller that separates utility from logical consistency
and treats low-utility but consistent candidates as assets rather than waste.
The frontier is split into mainlines (high-utility candidates used for
exploitation) and laterals (consistent, initially low-utility candidates that
receive short, cheap probes before judgment). LToT explores laterals via
Lateral Racing with Short-Circuit (LR--SC): a capped successive-halving race
that spreads tiny probes across a very wide lateral set, uses width-aware
thresholds with repeat-to-confirm, and immediately promotes a branch once its
envelope clears the mainline bar; mainlines are kept intentionally narrow so
surplus compute is invested where width is cheap. We prove a pseudolinear
lateral cost $\Theta(N_0 \log_{\eta} N_0)$ with logarithmically many rungs
(initial lateral width $N_0$; culling factor $\eta>1$), in contrast to the
exponential growth of uncapped mainlines. Empirical evaluations on benchmark
tasks are in preparation and will be added in a future revision. In short, LToT
turns large test-time budgets into principled diversity while preserving
promotion discipline, mitigating saturation and myopia without inflating
compute.

</details>


### [51] [Towards Interpretable and Inference-Optimal COT Reasoning with Sparse Autoencoder-Guided Generation](https://arxiv.org/abs/2510.01528)
*Daniel Zhao,Abhilash Shankarampeta,Lanxiang Hu,Tajana Rosing,Hao Zhang*

Main category: cs.AI

TL;DR: 提出了一种基于稀疏自编码器和聚类技术的方法，用于分析LLM的内部令牌表示并指导数学推理任务的生成，通过平衡利用和探索来提高推理质量。


<details>
  <summary>Details</summary>
Motivation: 研究大型语言模型在数学推理任务中的内部表示，以理解其推理过程并指导生成，避免极端行为，实现高质量的推理。

Method: 训练稀疏自编码器生成稀疏向量表示，应用k-means聚类构建令牌簇图，定义基于边权重的奖励函数量化推理轨迹的遵循程度，并测量生成多样性评估探索程度。

Result: 发现平衡利用和探索对数学推理任务的高准确性至关重要，稀疏自编码器可作为可扩展的奖励模型指导生成。

Conclusion: 该方法能有效平衡利用和探索，防止极端行为，促进LLM中更高质量的推理过程。

Abstract: We propose a novel method that leverages sparse autoencoders (SAEs) and
clustering techniques to analyze the internal token representations of large
language models (LLMs) and guide generations in mathematical reasoning tasks.
Our approach first trains an SAE to generate sparse vector representations for
training tokens, then applies k-means clustering to construct a graph where
vertices represent token clusters and weighted edges capture sequential token
transitions. Using this graph, we define an edge-weight based reward function
to quantify adherence to established reasoning traces, thereby identifying
exploitative reasoning trajectories. Additionally, we measure generation
diversity from clustering to assess the extent of exploration. Our findings
indicate that balancing both exploitation and exploration is crucial for
achieving high accuracy in mathematical reasoning tasks. During generation, the
SAE can serve as a scalable reward model to guide generations, ensuring a
balanced trade-off between exploitation and exploration. This prevents extreme
behaviors in either direction, ultimately fostering a higher-quality reasoning
process in LLMs.

</details>


### [52] [LOGicalThought: Logic-Based Ontological Grounding of LLMs for High-Assurance Reasoning](https://arxiv.org/abs/2510.01530)
*Navapat Nananukul,Yue Zhang,Ryan Lee,Eric Boxer,Jonathan May,Vibhav Giridhar Gogate,Jay Pujara,Mayank Kejriwal*

Main category: cs.AI

TL;DR: 提出LOGicalThought (LogT)神经符号架构，结合高级逻辑语言推理器与LLM，构建双重符号图上下文和逻辑上下文，将长文本推理转化为紧凑的接地评估，在四个多领域基准测试中性能提升11.84%。


<details>
  <summary>Details</summary>
Motivation: 在关键领域如法律和医学中，需要准确、可验证且明确基于证据的推理。现有LLM在标准推理任务中表现出色，但无法满足对高保证文本指南的严格推理要求，特别是涉及可废止逻辑、否定和蕴含的复杂逻辑结构。

Method: 使用高级逻辑语言和推理器与LLM结合，构建双重上下文表示：符号图上下文和逻辑上下文，将长文本指南推理问题转化为紧凑的接地评估问题。

Result: 在四个多领域基准测试中，相比四个基线模型，LogT整体性能提升11.84%。在三种推理模式上均有显著改进：否定推理提升10.2%，蕴含推理提升13.2%，可废止推理提升5.5%。

Conclusion: LogT神经符号架构有效解决了高保证文本推理中的逻辑挑战，特别是在处理否定、蕴含和可废止规则方面表现出色，为关键领域的严格推理提供了可行方案。

Abstract: High-assurance reasoning, particularly in critical domains such as law and
medicine, requires conclusions that are accurate, verifiable, and explicitly
grounded in evidence. This reasoning relies on premises codified from rules,
statutes, and contracts, inherently involving defeasible or non-monotonic logic
due to numerous exceptions, where the introduction of a single fact can
invalidate general rules, posing significant challenges. While large language
models (LLMs) excel at processing natural language, their capabilities in
standard inference tasks do not translate to the rigorous reasoning required
over high-assurance text guidelines. Core reasoning challenges within such
texts often manifest specific logical structures involving negation,
implication, and, most critically, defeasible rules and exceptions. In this
paper, we propose a novel neurosymbolically-grounded architecture called
LOGicalThought (LogT) that uses an advanced logical language and reasoner in
conjunction with an LLM to construct a dual symbolic graph context and
logic-based context. These two context representations transform the problem
from inference over long-form guidelines into a compact grounded evaluation.
Evaluated on four multi-domain benchmarks against four baselines, LogT improves
overall performance by 11.84% across all LLMs. Performance improves
significantly across all three modes of reasoning: by up to +10.2% on negation,
+13.2% on implication, and +5.5% on defeasible reasoning compared to the
strongest baseline.

</details>


### [53] [Just Do It!? Computer-Use Agents Exhibit Blind Goal-Directedness](https://arxiv.org/abs/2510.01670)
*Erfan Shayegani,Keegan Hines,Yue Dong,Nael Abu-Ghazaleh,Roman Lutz,Spencer Whitehead,Vidhisha Balachandran,Besmira Nushi,Vibhav Vineet*

Main category: cs.AI

TL;DR: 本文提出了计算机使用代理(CUAs)中的盲目标导向(BGD)问题，即代理会盲目追求目标而忽视可行性、安全性和上下文。作者开发了BLIND-ACT基准测试，发现前沿模型平均BGD率高达80.8%，揭示了执行优先偏见、思维行动脱节等失败模式。


<details>
  <summary>Details</summary>
Motivation: 计算机使用代理(CUAs)在GUI上执行操作以实现用户目标，但存在盲目追求目标而不考虑可行性、安全性和上下文的系统性偏见，这带来了潜在风险。

Method: 开发了BLIND-ACT基准测试，包含90个任务捕捉三种BGD模式：缺乏上下文推理、在模糊情况下的假设和决策、矛盾或不可行的目标。基于OSWorld构建真实环境，使用LLM评估代理行为。

Result: 评估了9个前沿模型，包括Claude Sonnet和Opus 4、Computer-Use-Preview和GPT-5，观察到平均BGD率高达80.8%。提示干预能降低BGD水平但风险依然存在。

Conclusion: 识别BGD并引入BLIND-ACT为未来研究和减轻这一基本风险奠定了基础，确保CUA的安全部署。需要更强的训练或推理时干预来解决持续存在的风险。

Abstract: Computer-Use Agents (CUAs) are an increasingly deployed class of agents that
take actions on GUIs to accomplish user goals. In this paper, we show that CUAs
consistently exhibit Blind Goal-Directedness (BGD): a bias to pursue goals
regardless of feasibility, safety, reliability, or context. We characterize
three prevalent patterns of BGD: (i) lack of contextual reasoning, (ii)
assumptions and decisions under ambiguity, and (iii) contradictory or
infeasible goals. We develop BLIND-ACT, a benchmark of 90 tasks capturing these
three patterns. Built on OSWorld, BLIND-ACT provides realistic environments and
employs LLM-based judges to evaluate agent behavior, achieving 93.75% agreement
with human annotations. We use BLIND-ACT to evaluate nine frontier models,
including Claude Sonnet and Opus 4, Computer-Use-Preview, and GPT-5, observing
high average BGD rates (80.8%) across them. We show that BGD exposes subtle
risks that arise even when inputs are not directly harmful. While
prompting-based interventions lower BGD levels, substantial risk persists,
highlighting the need for stronger training- or inference-time interventions.
Qualitative analysis reveals observed failure modes: execution-first bias
(focusing on how to act over whether to act), thought-action disconnect
(execution diverging from reasoning), and request-primacy (justifying actions
due to user request). Identifying BGD and introducing BLIND-ACT establishes a
foundation for future research on studying and mitigating this fundamental risk
and ensuring safe CUA deployment.

</details>


### [54] [Information Seeking for Robust Decision Making under Partial Observability](https://arxiv.org/abs/2510.01531)
*Djengo Cyun-Jyun Fang,Tsung-Wei Ke*

Main category: cs.AI

TL;DR: InfoSeeker是一个LLM决策框架，通过将任务导向规划与信息寻求相结合，在部分可观测环境中处理观察和环境动态的不确定性，相比现有方法获得74%的绝对性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有LLM规划代理虽然解决了观测不确定性，但往往忽略了内部动态与实际环境之间的差异，这在信息不完整和动态嘈杂的实际环境中限制了问题解决能力。

Method: InfoSeeker框架提示LLM主动收集信息，通过规划行动来验证理解、检测环境变化或测试假设，然后生成或修订任务导向计划。

Result: 实验显示InfoSeeker在部分可观测环境中比现有方法性能提升74%，且不牺牲样本效率，在机器人操作和网页导航等基准测试中表现优于基线方法。

Conclusion: 在部分可观测环境中，紧密集成规划和信息寻求对于实现稳健行为至关重要，InfoSeeker证明了这种集成方法的有效性。

Abstract: Explicit information seeking is essential to human problem-solving in
practical environments characterized by incomplete information and noisy
dynamics. When the true environmental state is not directly observable, humans
seek information to update their internal dynamics and inform future
decision-making. Although existing Large Language Model (LLM) planning agents
have addressed observational uncertainty, they often overlook discrepancies
between their internal dynamics and the actual environment. We introduce
Information Seeking Decision Planner (InfoSeeker), an LLM decision-making
framework that integrates task-oriented planning with information seeking to
align internal dynamics and make optimal decisions under uncertainty in both
agent observations and environmental dynamics. InfoSeeker prompts an LLM to
actively gather information by planning actions to validate its understanding,
detect environmental changes, or test hypotheses before generating or revising
task-oriented plans. To evaluate InfoSeeker, we introduce a novel benchmark
suite featuring partially observable environments with incomplete observations
and uncertain dynamics. Experiments demonstrate that InfoSeeker achieves a 74%
absolute performance gain over prior methods without sacrificing sample
efficiency. Moreover, InfoSeeker generalizes across LLMs and outperforms
baselines on established benchmarks such as robotic manipulation and web
navigation. These findings underscore the importance of tightly integrating
planning and information seeking for robust behavior in partially observable
environments. The project page is available at https://infoseekerllm.github.io

</details>


### [55] [Step-Aware Policy Optimization for Reasoning in Diffusion Large Language Models](https://arxiv.org/abs/2510.01544)
*Shaoan Xie,Lingjing Kong,Xiangchen Song,Xinshuai Dong,Guangyi Chen,Eric P. Xing,Kun Zhang*

Main category: cs.AI

TL;DR: 提出了Step-Aware Policy Optimization (SAPO)算法，通过过程奖励函数改进扩散语言模型的推理能力，解决现有方法中推理路径结构混乱的问题。


<details>
  <summary>Details</summary>
Motivation: 当前基于强化学习的扩散语言模型训练方法依赖稀疏的结果奖励，这会强化导致巧合正确结果的错误推理路径，与推理的自然结构不匹配。

Method: 提出理论框架将复杂问题解决形式化为层次选择过程，并基于此开发SAPO算法，使用过程奖励函数引导模型学习结构化推理路径。

Result: 实验结果表明该方法在挑战性推理基准上显著提升性能，并增强了生成过程的可解释性。

Conclusion: 通过将扩散语言模型的去噪过程与潜在推理层次对齐，SAPO算法能够有效改进复杂推理任务的性能。

Abstract: Diffusion language models (dLLMs) offer a promising, non-autoregressive
paradigm for text generation, yet training them for complex reasoning remains a
key challenge. Current reinforcement learning approaches often rely on sparse,
outcome-based rewards, which can reinforce flawed reasoning paths that lead to
coincidentally correct answers. We argue that this stems from a fundamental
mismatch with the natural structure of reasoning. We first propose a
theoretical framework that formalizes complex problem solving as a hierarchical
selection process, where an intractable global constraint is decomposed into a
series of simpler, localized logical steps. This framework provides a
principled foundation for algorithm design, including theoretical insights into
the identifiability of this latent reasoning structure. Motivated by this
theory, we identify unstructured refinement -- a failure mode where a model's
iterative steps do not contribute meaningfully to the solution -- as a core
deficiency in existing methods. We then introduce Step-Aware Policy
Optimization (SAPO), a novel RL algorithm that aligns the dLLM's denoising
process with the latent reasoning hierarchy. By using a process-based reward
function that encourages incremental progress, SAPO guides the model to learn
structured, coherent reasoning paths. Our empirical results show that this
principled approach significantly improves performance on challenging reasoning
benchmarks and enhances the interpretability of the generation process.

</details>


### [56] [InvThink: Towards AI Safety via Inverse Reasoning](https://arxiv.org/abs/2510.01569)
*Yubin Kim,Taehan Kim,Eugene Park,Chunjong Park,Cynthia Breazeal,Daniel McDuff,Hae Won Park*

Main category: cs.AI

TL;DR: InvThink是一种让大语言模型具备逆向思维能力的方法，在生成回答前先推理可能的失败模式，从而主动避免风险。


<details>
  <summary>Details</summary>
Motivation: 现有安全对齐方法直接优化安全响应，但缺乏对潜在危害的系统性考虑。InvThink旨在通过逆向思维让模型主动识别和避免风险。

Method: 1) 枚举潜在危害 2) 分析后果 3) 生成主动避免这些风险的安全输出。通过监督微调和强化学习在三个LLM系列中实现。

Result: 安全改进随模型规模扩展更强；减轻安全税，保持标准基准上的通用推理能力；在高风险领域（医疗、金融、法律等）有害响应减少15.7%。

Conclusion: 逆向推理为构建更安全、更强大的语言模型提供了可扩展和通用的路径。

Abstract: We present InvThink, a simple yet powerful approach that gives large language
models (LLMs) the capability of inverse thinking: reasoning through failure
modes before generating responses. Unlike existing safety alignment methods
that optimize directly for safe response, InvThink instructs models to 1)
enumerate potential harms, 2) analyze their consequences, and 3) generate safe
outputs that proactively avoid these risks. Our method reveals three key
findings: (i) safety improvements show stronger scaling with model size
compared to existing safety methods. (ii) InvThink mitigates safety tax; by
training models to systematically consider failure modes, it preserves general
reasoning capabilities on standard benchmarks. (iii) beyond general safety
tasks, InvThink excels in high-stakes domains including external-facing
(medicine, finance, law) and agentic (blackmail, murder) risk scenarios,
achieving up to 15.7% reduction in harmful responses compared to baseline
methods like SafetyPrompt. We further implement InvThink via supervised
fine-tuning, and reinforcement learning across three LLM families. These
results suggest that inverse reasoning provides a scalable and generalizable
path toward safer, more capable language models.

</details>


### [57] [UpSafe$^\circ$C: Upcycling for Controllable Safety in Large Language Models](https://arxiv.org/abs/2510.02194)
*Yuhao Sun,Zhuoer Xu,Shiwen Cui,Kun Yang,Lingyun Yu,Yongdong Zhang,Hongtao Xie*

Main category: cs.AI

TL;DR: UpSafe°C是一个通过安全感知升级来增强LLM安全性的统一框架，通过识别安全关键层并将其升级为稀疏MoE结构，结合两阶段SFT策略和安全温度机制，实现动态的安全与效用平衡。


<details>
  <summary>Details</summary>
Motivation: 现有安全技术（外部护栏、推理时指导和后训练对齐）在平衡安全性、实用性和可控性方面存在局限性，需要一种更灵活的动态安全控制方法。

Method: 识别安全关键层并升级为稀疏MoE结构，路由器作为软护栏选择性激活原始MLP和新增安全专家；采用两阶段SFT策略增强安全判别能力；引入安全温度机制实现推理时动态控制。

Result: 在多个基准测试、基础模型和模型规模上，UpSafe°C实现了对有害和越狱输入的鲁棒安全改进，同时在通用任务上保持竞争力，安全温度机制实现了效用与安全的帕累托最优前沿。

Conclusion: 研究展示了LLM安全的新方向：从静态对齐转向动态、模块化和推理感知的控制。

Abstract: Large Language Models (LLMs) have achieved remarkable progress across a wide
range of tasks, but remain vulnerable to safety risks such as harmful content
generation and jailbreak attacks. Existing safety techniques -- including
external guardrails, inference-time guidance, and post-training alignment --
each face limitations in balancing safety, utility, and controllability. In
this work, we propose UpSafe$^\circ$C, a unified framework for enhancing LLM
safety through safety-aware upcycling. Our approach first identifies
safety-critical layers and upcycles them into a sparse Mixture-of-Experts (MoE)
structure, where the router acts as a soft guardrail that selectively activates
original MLPs and added safety experts. We further introduce a two-stage SFT
strategy to strengthen safety discrimination while preserving general
capabilities. To enable flexible control at inference time, we introduce a
safety temperature mechanism, allowing dynamic adjustment of the trade-off
between safety and utility. Experiments across multiple benchmarks, base model,
and model scales demonstrate that UpSafe$^\circ$C achieves robust safety
improvements against harmful and jailbreak inputs, while maintaining
competitive performance on general tasks. Moreover, analysis shows that safety
temperature provides fine-grained inference-time control that achieves the
Pareto-optimal frontier between utility and safety. Our results highlight a new
direction for LLM safety: moving from static alignment toward dynamic, modular,
and inference-aware control.

</details>


### [58] [AdvEvo-MARL: Shaping Internalized Safety through Adversarial Co-Evolution in Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2510.01586)
*Zhenyu Pan,Yiting Zhang,Zhuo Liu,Yolo Yunlong Tang,Zeliang Zhang,Haozheng Luo,Yuwei Han,Jianshu Zhang,Dennis Wu,Hong-Yu Chen,Haoran Lu,Haoyang Fang,Manling Li,Chenliang Xu,Philip S. Yu,Han Liu*

Main category: cs.AI

TL;DR: AdvEvo-MARL是一个基于多智能体强化学习的对抗进化框架，通过内部化安全机制到任务智能体中，无需外部防护模块就能有效防御多智能体系统中的越狱攻击和对抗协作威胁。


<details>
  <summary>Details</summary>
Motivation: 现有的多智能体系统防御方法存在缺陷：自我验证方法因单个智能体无法检测跨智能体不安全链而效果不佳；外部防护模块会增加系统开销并造成单点故障。需要一种既能保证安全又不增加系统复杂度的解决方案。

Method: 提出AdvEvo-MARL框架，采用对抗学习环境联合优化攻击者（生成进化越狱提示）和防御者（训练任务智能体既能完成任务又能抵抗攻击）。引入公共基线优势估计，同一功能组内的智能体共享组级平均回报基线，实现低方差更新和强组内协调。

Result: 在代表性攻击场景中，AdvEvo-MARL始终将攻击成功率保持在20%以下，而基线方法高达38.33%。同时保持甚至提高了任务准确率（在推理任务上提升达+3.67%）。

Conclusion: 研究表明，无需依赖额外防护智能体或增加系统开销，就能同时提高多智能体系统的安全性和实用性，AdvEvo-MARL框架为此提供了有效解决方案。

Abstract: LLM-based multi-agent systems excel at planning, tool use, and role
coordination, but their openness and interaction complexity also expose them to
jailbreak, prompt-injection, and adversarial collaboration. Existing defenses
fall into two lines: (i) self-verification that asks each agent to pre-filter
unsafe instructions before execution, and (ii) external guard modules that
police behaviors. The former often underperforms because a standalone agent
lacks sufficient capacity to detect cross-agent unsafe chains and
delegation-induced risks; the latter increases system overhead and creates a
single-point-of-failure-once compromised, system-wide safety collapses, and
adding more guards worsens cost and complexity. To solve these challenges, we
propose AdvEvo-MARL, a co-evolutionary multi-agent reinforcement learning
framework that internalizes safety into task agents. Rather than relying on
external guards, AdvEvo-MARL jointly optimizes attackers (which synthesize
evolving jailbreak prompts) and defenders (task agents trained to both
accomplish their duties and resist attacks) in adversarial learning
environments. To stabilize learning and foster cooperation, we introduce a
public baseline for advantage estimation: agents within the same functional
group share a group-level mean-return baseline, enabling lower-variance updates
and stronger intra-group coordination. Across representative attack scenarios,
AdvEvo-MARL consistently keeps attack-success rate (ASR) below 20%, whereas
baselines reach up to 38.33%, while preserving-and sometimes improving-task
accuracy (up to +3.67% on reasoning tasks). These results show that safety and
utility can be jointly improved without relying on extra guard agents or added
system overhead.

</details>


### [59] [AgentRec: Next-Generation LLM-Powered Multi-Agent Collaborative Recommendation with Adaptive Intelligence](https://arxiv.org/abs/2510.01609)
*Bo Ma,Hang Li,ZeHua Hu,XiaoFan Gui,LuYao Liu,Simon Lau*

Main category: cs.AI

TL;DR: AgentRec是一个基于LLM的多智能体协作推荐框架，通过分层智能体网络处理动态用户偏好、保持对话连贯性并平衡多个排名目标，在三个真实数据集上显著提升了对话成功率、推荐准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 现有交互式对话推荐系统在处理动态用户偏好、保持对话连贯性和平衡多个排名目标方面存在显著挑战，需要更智能的解决方案。

Method: 采用分层智能体网络架构，包含对话理解、偏好建模、上下文感知和动态排名等专门LLM智能体，通过自适应权重机制协调，结合三层学习策略：简单查询的快速响应、复杂偏好的智能推理和挑战场景的深度协作。

Result: 在三个真实数据集上的实验表明，AgentRec相比最先进基线方法，对话成功率提升2.8%，推荐准确性(NDCG@10)提高1.9%，对话效率提升3.2%，同时通过智能体协调保持相当的计算成本。

Conclusion: AgentRec框架通过多智能体协作和自适应学习机制，有效解决了现有对话推荐系统的关键挑战，在多个性能指标上实现了显著提升。

Abstract: Interactive conversational recommender systems have gained significant
attention for their ability to capture user preferences through natural
language interactions. However, existing approaches face substantial challenges
in handling dynamic user preferences, maintaining conversation coherence, and
balancing multiple ranking objectives simultaneously. This paper introduces
AgentRec, a next-generation LLM-powered multi-agent collaborative
recommendation framework that addresses these limitations through hierarchical
agent networks with adaptive intelligence. Our approach employs specialized
LLM-powered agents for conversation understanding, preference modeling, context
awareness, and dynamic ranking, coordinated through an adaptive weighting
mechanism that learns from interaction patterns. We propose a three-tier
learning strategy combining rapid response for simple queries, intelligent
reasoning for complex preferences, and deep collaboration for challenging
scenarios. Extensive experiments on three real-world datasets demonstrate that
AgentRec achieves consistent improvements over state-of-the-art baselines, with
2.8\% enhancement in conversation success rate, 1.9\% improvement in
recommendation accuracy (NDCG@10), and 3.2\% better conversation efficiency
while maintaining comparable computational costs through intelligent agent
coordination.

</details>


### [60] [PychoBench: Evaluating the Psychology Intelligence of Large Language Models](https://arxiv.org/abs/2510.01611)
*Min Zeng*

Main category: cs.AI

TL;DR: 论文评估LLMs在心理咨询领域的应用潜力，通过PsychoBench基准测试发现只有前沿模型能达到美国国家咨询师认证考试(NCE)的通过标准。


<details>
  <summary>Details</summary>
Motivation: 探索LLMs在需要认知能力的应用(如心理咨询)中的潜力，评估其是否具备担任心理咨询师所需的知识资质。

Method: 开发PsychoBench基准测试，包含约2,252个精心筛选的单选问题，基于美国国家咨询师考试，要求约70%准确率才能通过。

Result: GPT-4o、Llama3.3-70B和Gemma3-27B等先进模型远超通过阈值，而较小的开源模型(Qwen2.5-7B、Mistral-7B)远低于标准。

Conclusion: 只有前沿LLMs目前能满足咨询考试标准，这凸显了开发心理学导向LLMs的潜力和挑战。

Abstract: Large Language Models (LLMs) have demonstrated remarkable success across a
wide range of industries, primarily due to their impressive generative
abilities. Yet, their potential in applications requiring cognitive abilities,
such as psychological counseling, remains largely untapped. This paper
investigates the key question: Can LLMs be effectively applied to psychological
counseling? To determine whether an LLM can effectively take on the role of a
psychological counselor, the first step is to assess whether it meets the
qualifications required for such a role, namely the ability to pass the U.S.
National Counselor Certification Exam (NCE). This is because, just as a human
counselor must pass a certification exam to practice, an LLM must demonstrate
sufficient psychological knowledge to meet the standards required for such a
role. To address this, we introduce PsychoBench, a benchmark grounded in
U.S.national counselor examinations, a licensure test for professional
counselors that requires about 70% accuracy to pass. PsychoBench comprises
approximately 2,252 carefully curated single-choice questions, crafted to
require deep understanding and broad enough to cover various sub-disciplines of
psychology. This benchmark provides a comprehensive assessment of an LLM's
ability to function as a counselor. Our evaluation shows that advanced models
such as GPT-4o, Llama3.3-70B, and Gemma3-27B achieve well above the passing
threshold, while smaller open-source models (e.g., Qwen2.5-7B, Mistral-7B)
remain far below it. These results suggest that only frontier LLMs are
currently capable of meeting counseling exam standards, highlighting both the
promise and the challenges of developing psychology-oriented LLMs.

</details>


### [61] [Learning to Decide with Just Enough: Information-Theoretic Context Summarization for CDMPs](https://arxiv.org/abs/2510.01620)
*Peidong Liu,Junjiang Lin,Shaowen Wang,Yao Xu,Haiqing Li,Xuhao Xie,Siyi Wu,Hao Li*

Main category: cs.AI

TL;DR: 提出基于信息论的上下文总结方法，使用大语言模型压缩高维上下文为低维语义摘要，提升上下文马尔可夫决策过程的泛化能力和计算效率。


<details>
  <summary>Details</summary>
Motivation: 现有上下文马尔可夫决策过程方法在高维或非结构化上下文中泛化能力差，导致计算开销大、性能不稳定。

Method: 利用大语言模型进行信息论驱动的上下文压缩，生成低维语义摘要来增强状态表示，保留决策关键信息同时减少冗余。

Result: 在离散、连续、视觉和推荐基准测试中，该方法在奖励、成功率、样本效率方面优于原始上下文和非上下文基线，同时降低延迟和内存使用。

Conclusion: 基于大语言模型的总结为上下文丰富、资源受限环境中的高效决策提供了可扩展且可解释的解决方案。

Abstract: Contextual Markov Decision Processes (CMDPs) offer a framework for sequential
decision-making under external signals, but existing methods often fail to
generalize in high-dimensional or unstructured contexts, resulting in excessive
computation and unstable performance. We propose an information-theoretic
summarization approach that uses large language models (LLMs) to compress
contextual inputs into low-dimensional, semantically rich summaries. These
summaries augment states by preserving decision-critical cues while reducing
redundancy. Building on the notion of approximate context sufficiency, we
provide, to our knowledge, the first regret bounds and a latency-entropy
trade-off characterization for CMDPs. Our analysis clarifies how
informativeness impacts computational cost. Experiments across discrete,
continuous, visual, and recommendation benchmarks show that our method
outperforms raw-context and non-context baselines, improving reward, success
rate, and sample efficiency, while reducing latency and memory usage. These
findings demonstrate that LLM-based summarization offers a scalable and
interpretable solution for efficient decision-making in context-rich,
resource-constrained environments.

</details>


### [62] [Understanding the Geospatial Reasoning Capabilities of LLMs: A Trajectory Recovery Perspective](https://arxiv.org/abs/2510.01639)
*Thinh Hung Truong,Jey Han Lau,Jianzhong Qi*

Main category: cs.AI

TL;DR: 该论文探索大型语言模型的地理空间推理能力，特别是能否读取道路网络地图并进行导航。通过轨迹恢复任务评估模型性能，提出了GLOBALTRACE数据集和提示框架，使LLM无需外部导航工具即可生成有效路径。


<details>
  <summary>Details</summary>
Motivation: 研究LLM在地理空间推理方面的能力，特别是能否理解道路网络地图并执行导航任务，这对于开发更智能的导航系统具有重要意义。

Method: 将轨迹恢复作为代理任务，要求模型重建被屏蔽的GPS轨迹。使用GLOBALTRACE数据集（包含4000多条真实轨迹），通过提示框架让LLM基于道路网络上下文生成路径。

Result: LLM在轨迹恢复任务中优于现成基线和专业轨迹恢复模型，具有强大的零样本泛化能力。模型对道路网络和坐标系有良好理解，但在区域和交通模式方面存在系统性偏差。

Conclusion: LLM能够通过灵活推理地图来增强导航体验，并整合用户偏好，展示了在地理空间任务中的潜力。

Abstract: We explore the geospatial reasoning capabilities of Large Language Models
(LLMs), specifically, whether LLMs can read road network maps and perform
navigation. We frame trajectory recovery as a proxy task, which requires models
to reconstruct masked GPS traces, and introduce GLOBALTRACE, a dataset with
over 4,000 real-world trajectories across diverse regions and transportation
modes. Using road network as context, our prompting framework enables LLMs to
generate valid paths without accessing any external navigation tools.
Experiments show that LLMs outperform off-the-shelf baselines and specialized
trajectory recovery models, with strong zero-shot generalization. Fine-grained
analysis shows that LLMs have strong comprehension of the road network and
coordinate systems, but also pose systematic biases with respect to regions and
transportation modes. Finally, we demonstrate how LLMs can enhance navigation
experiences by reasoning over maps in flexible ways to incorporate user
preferences.

</details>


### [63] [GuruAgents: Emulating Wise Investors with Prompt-Guided LLM Agents](https://arxiv.org/abs/2510.01664)
*Yejin Kim,Youngbin Lee,Juhyeong Kim,Yongjae Lee*

Main category: cs.AI

TL;DR: 该研究开发了五个基于提示工程的AI投资代理，分别模拟传奇投资大师的投资策略，在纳斯达克100成分股上进行回测，其中巴菲特代理表现最佳，年化收益率达42.2%。


<details>
  <summary>Details</summary>
Motivation: 探索如何通过提示工程将定性投资哲学转化为可复现的定量策略，为自动化系统化投资提供新方向。

Method: 开发五个GuruAgents，将不同投资大师的独特哲学编码到LLM提示中，整合金融工具和确定性推理流程。

Result: 在2023年第四季度至2025年第二季度的回测中，各代理表现出不同的行为特征，巴菲特代理表现最佳，年化收益率42.2%，显著超越基准。

Conclusion: 提示工程能够成功将投资大师的定性哲学转化为可复现的定量策略，为自动化系统化投资开辟了新方向。

Abstract: This study demonstrates that GuruAgents, prompt-guided AI agents, can
systematically operationalize the strategies of legendary investment gurus. We
develop five distinct GuruAgents, each designed to emulate an iconic investor,
by encoding their distinct philosophies into LLM prompts that integrate
financial tools and a deterministic reasoning pipeline. In a backtest on
NASDAQ-100 constituents from Q4 2023 to Q2 2025, the GuruAgents exhibit unique
behaviors driven by their prompted personas. The Buffett GuruAgent achieves the
highest performance, delivering a 42.2\% CAGR that significantly outperforms
benchmarks, while other agents show varied results. These findings confirm that
prompt engineering can successfully translate the qualitative philosophies of
investment gurus into reproducible, quantitative strategies, highlighting a
novel direction for automated systematic investing. The source code and data
are available at https://github.com/yejining99/GuruAgents.

</details>


### [64] [A Locally Executable AI System for Improving Preoperative Patient Communication: A Multi-Domain Clinical Evaluation](https://arxiv.org/abs/2510.01671)
*Motoki Sato,Yuki Matsushita,Hidekazu Takahashi,Tomoaki Kakazu,Sou Nagata,Mizuho Ohnuma,Atsushi Yoshikawa,Masayuki Yamamura*

Main category: cs.AI

TL;DR: LENOHA是一个安全优先、本地优先的系统，通过高精度分类器从临床FAQ中返回验证过的答案，避免生成式AI的幻觉问题，在临床路径中实现高精度、低能耗的问答服务。


<details>
  <summary>Details</summary>
Motivation: 患者在接受侵入性手术前常有未解答的问题，但时间紧迫的工作流程和隐私限制限制了个性化咨询。需要一种安全、高效且保护隐私的解决方案。

Method: 使用高精度句子转换器分类器路由输入，从临床医生策划的FAQ中返回逐字答案，完全避免临床路径中的自由文本生成。评估了四个编码器在两个临床领域（拔牙和胃镜检查）的性能。

Result: E5-large-instruct编码器达到0.983的总体准确率，AUC 0.996，仅7个错误，与GPT-4o性能相当。非生成式临床路径能耗仅为1.0 mWh/输入，比本地8B SLM的小型对话回复低170倍，延迟约0.10秒。

Conclusion: 通过返回经过验证的FAQ答案，可以在临床路径中结构性地避免前沿判别和生成引起的错误，支持在带宽有限环境中的隐私保护、可持续性和公平部署。

Abstract: Patients awaiting invasive procedures often have unanswered pre-procedural
questions; however, time-pressured workflows and privacy constraints limit
personalized counseling. We present LENOHA (Low Energy, No Hallucination, Leave
No One Behind Architecture), a safety-first, local-first system that routes
inputs with a high-precision sentence-transformer classifier and returns
verbatim answers from a clinician-curated FAQ for clinical queries, eliminating
free-text generation in the clinical path. We evaluated two domains (tooth
extraction and gastroscopy) using expert-reviewed validation sets
(n=400/domain) for thresholding and independent test sets (n=200/domain). Among
the four encoders, E5-large-instruct (560M) achieved an overall accuracy of
0.983 (95% CI 0.964-0.991), AUC 0.996, and seven total errors, which were
statistically indistinguishable from GPT-4o on this task; Gemini made no errors
on this test set. Energy logging shows that the non-generative clinical path
consumes ~1.0 mWh per input versus ~168 mWh per small-talk reply from a local
8B SLM, a ~170x difference, while maintaining ~0.10 s latency on a single
on-prem GPU. These results indicate that near-frontier discrimination and
generation-induced errors are structurally avoided in the clinical path by
returning vetted FAQ answers verbatim, supporting privacy, sustainability, and
equitable deployment in bandwidth-limited environments.

</details>


### [65] [Improving AGI Evaluation: A Data Science Perspective](https://arxiv.org/abs/2510.01687)
*John Hawkins*

Main category: cs.AI

TL;DR: 本文提出AGI评估应转向基于稳健任务执行能力的评估方法，而不是依赖直觉设计的合成任务


<details>
  <summary>Details</summary>
Motivation: 传统AGI评估方法基于直觉设计合成任务，在AI历史上表现不佳，需要更有效的评估方法

Method: 采用数据科学中常用的实践方法，专注于评估系统的稳健任务执行能力，通过实际能力展示AGI

Result: 提供了AGI评估的实际示例，展示了基于能力证明的评估方法

Conclusion: AGI评估应从直觉驱动的合成任务转向基于稳健任务执行能力的评估哲学

Abstract: Evaluation of potential AGI systems and methods is difficult due to the
breadth of the engineering goal. We have no methods for perfect evaluation of
the end state, and instead measure performance on small tests designed to
provide directional indication that we are approaching AGI. In this work we
argue that AGI evaluation methods have been dominated by a design philosophy
that uses our intuitions of what intelligence is to create synthetic tasks,
that have performed poorly in the history of AI. Instead we argue for an
alternative design philosophy focused on evaluating robust task execution that
seeks to demonstrate AGI through competence. This perspective is developed from
common practices in data science that are used to show that a system can be
reliably deployed. We provide practical examples of what this would mean for
AGI evaluation.

</details>


### [66] [VaPR -- Vision-language Preference alignment for Reasoning](https://arxiv.org/abs/2510.01700)
*Rohan Wadhawan,Fabrice Y Harel-Canada,Zi-Yi Dou,Suhaila Shakiah,Robinson Piramuthu,Nanyun Peng*

Main category: cs.AI

TL;DR: 本文提出了VaPR框架，通过LLM引导的响应编辑生成硬负样本，解决了合成偏好标注中的风格和长度偏差问题，显著提升了大型视觉语言模型的性能。


<details>
  <summary>Details</summary>
Motivation: 现有偏好微调方法忽视了合成偏好标注中普遍存在的风格和长度偏差噪声问题，需要开发能够产生高质量硬负样本的方法来改善模型对齐效果。

Method: 基于LLM引导的响应编辑框架，生成具有目标错误的拒绝响应，同时保持与接受响应在风格和长度上的相似性，并构建了包含30K高质量样本的VaPR数据集。

Result: VaPR模型在十个基准测试中取得显著性能提升：LLaVA平均提升6.5%，Qwen2VL提升4.0%，Qwen2.5VL提升1.5%；在推理任务上表现尤为突出，并减少了二元问题中回答"是"的倾向。

Conclusion: VaPR框架有效解决了合成偏好数据中的偏差问题，显著提升了LVLM性能，且该框架可推广到开源LLM作为编辑器，在较小规模数据上也能持续改善性能。

Abstract: Preference finetuning methods like Direct Preference Optimization (DPO) with
AI-generated feedback have shown promise in aligning Large Vision-Language
Models (LVLMs) with human preferences. However, existing techniques overlook
the prevalence of noise in synthetic preference annotations in the form of
stylistic and length biases. To this end, we introduce a hard-negative response
generation framework based on LLM-guided response editing, that produces
rejected responses with targeted errors, maintaining stylistic and length
similarity to the accepted ones. Using this framework, we develop the VaPR
dataset, comprising 30K high-quality samples, to finetune three LVLM families:
LLaVA-V1.5, Qwen2VL & Qwen2.5VL (2B-13B sizes). Our VaPR models deliver
significant performance improvements across ten benchmarks, achieving average
gains of 6.5% (LLaVA), 4.0% (Qwen2VL), and 1.5% (Qwen2.5VL), with notable
improvements on reasoning tasks. A scaling analysis shows that performance
consistently improves with data size, with LLaVA models benefiting even at
smaller scales. Moreover, VaPR reduces the tendency to answer "Yes" in binary
questions - addressing a common failure mode in LVLMs like LLaVA. Lastly, we
show that the framework generalizes to open-source LLMs as editors, with models
trained on VaPR-OS achieving ~99% of the performance of models trained on
\name, which is synthesized using GPT-4o. Our data, models, and code can be
found on the project page https://vap-r.github.io

</details>


### [67] [MetaboT: AI-based agent for natural language-based interaction with metabolomics knowledge graphs](https://arxiv.org/abs/2510.01724)
*Madina Bekbergenova,Lucas Pradi,Benjamin Navet,Emma Tysinger,Franck Michel,Matthieu Feraud,Yousouf Taghzouti,Yan Zhou Chen,Olivier Kirchhoffer,Florence Mehl,Martin Legrand,Tao Jiang,Marco Pagni,Soha Hassoun,Jean-Luc Wolfender,Wout Bittremieux,Fabien Gandon,Louis-Félix Nothias*

Main category: cs.AI

TL;DR: MetaboT是一个基于大语言模型的多智能体AI系统，能够将用户的自然语言问题转换为SPARQL查询语言，用于操作代谢组学知识图谱，显著提高了查询准确率。


<details>
  <summary>Details</summary>
Motivation: 代谢组学质谱分析产生大量数据，知识图谱虽然能结构化这些数据，但使用需要深入理解其本体和查询语言语法，这阻碍了研究人员的有效访问。

Method: 采用多智能体系统架构，使用LangChain和LangGraph库集成LLM与外部工具。系统包含入口代理、验证代理、监督代理、知识图谱代理和SPARQL查询生成代理，通过分工协作处理用户查询。

Result: 在50个代谢组学相关问题的测试中，MetaboT达到83.67%的准确率，显著优于仅使用GPT-4o的基线方法（8.16%准确率）。

Conclusion: MetaboT成功消除了访问知识图谱的技术障碍，通过自然语言查询实现了结构化代谢组学数据的检索，为数据驱动发现提供了便利。

Abstract: Mass spectrometry metabolomics generates vast amounts of data requiring
advanced methods for interpretation. Knowledge graphs address these challenges
by structuring mass spectrometry data, metabolite information, and their
relationships into a connected network (Gaudry et al. 2024). However, effective
use of a knowledge graph demands an in-depth understanding of its ontology and
its query language syntax. To overcome this, we designed MetaboT, an AI system
utilizing large language models (LLMs) to translate user questions into SPARQL
semantic query language for operating on knowledge graphs (Steve Harris 2013).
We demonstrate its effectiveness using the Experimental Natural Products
Knowledge Graph (ENPKG), a large-scale public knowledge graph for plant natural
products (Gaudry et al. 2024).MetaboT employs specialized AI agents for
handling user queries and interacting with the knowledge graph by breaking down
complex tasks into discrete components, each managed by a specialised agent
(Fig. 1a). The multi-agent system is constructed using the LangChain and
LangGraph libraries, which facilitate the integration of LLMs with external
tools and information sources (LangChain, n.d.). The query generation process
follows a structured workflow. First, the Entry Agent determines if the
question is new or a follow-up to previous interactions. New questions are
forwarded to the Validator Agent, which verifies if the question is related to
the knowledge graph. Then, the valid question is sent to the Supervisor Agent,
which identifies if the question requires chemical conversions or standardized
identifiers. In this case it delegates the question to the Knowledge Graph
Agent, which can use tools to extract necessary details, such as URIs or
taxonomies of chemical names, from the user query. Finally, an agent
responsible for crafting the SPARQL queries equipped with the ontology of the
knowledge graph uses the provided identifiers to generate the query. Then, the
system executes the generated query against the metabolomics knowledge graph
and returns structured results to the user (Fig. 1b). To assess the performance
of MetaboT we have curated 50 metabolomics-related questions and their expected
answers. In addition to submitting these questions to MetaboT, we evaluated a
baseline by submitting them to a standard LLM (GPT-4o) with a prompt that
incorporated the knowledge graph ontology but did not provide specific entity
IDs. This baseline achieved only 8.16% accuracy, compared to MetaboT's 83.67%,
underscoring the necessity of our multi-agent system for accurately retrieving
entities and generating correct SPARQL queries. MetaboT demonstrates promising
performance as a conversational question-answering assistant, enabling
researchers to retrieve structured metabolomics data through natural language
queries. By automating the generation and execution of SPARQL queries, it
removes technical barriers that have traditionally hindered access to knowledge
graphs. Importantly, MetaboT leverages the capabilities of LLMs while
maintaining experimentally grounded query generation, ensuring that outputs
remain aligned with domain-specific standards and data structures. This
approach facilitates data-driven discoveries by bridging the gap between
complex semantic technologies and user-friendly interaction. MetaboT is
accessible at [https://metabot.holobiomicslab.eu/], and its source code is
available at [https://github.com/HolobiomicsLab/MetaboT].

</details>


### [68] [A cybersecurity AI agent selection and decision support framework](https://arxiv.org/abs/2510.01751)
*Masike Malatji*

Main category: cs.AI

TL;DR: 提出了一个结构化决策支持框架，将不同AI智能体架构与NIST网络安全框架2.0系统对齐，为选择部署AI解决方案提供透明方法。


<details>
  <summary>Details</summary>
Motivation: 弥合理论AI构建与运营网络安全需求之间的差距，解决当代网络威胁，提供统一的检测、事件响应和治理策略。

Method: 采用结构化框架，将NIST CSF 2.0功能分解为具体任务，将AI智能体特性（自主性、自适应学习、实时响应）与安全需求关联，并定义分级自主水平。

Result: 通过概念验证表明，定制化AI智能体部署可以增强态势感知、加速响应时间，并通过自适应风险管理增强长期韧性。

Conclusion: 该研究为遵循行业标准的稳健、经验验证的多智能体系统奠定了基础，连接了AI理论与网络安全实践需求。

Abstract: This paper presents a novel, structured decision support framework that
systematically aligns diverse artificial intelligence (AI) agent architectures,
reactive, cognitive, hybrid, and learning, with the comprehensive National
Institute of Standards and Technology (NIST) Cybersecurity Framework (CSF) 2.0.
By integrating agent theory with industry guidelines, this framework provides a
transparent and stepwise methodology for selecting and deploying AI solutions
to address contemporary cyber threats. Employing a granular decomposition of
NIST CSF 2.0 functions into specific tasks, the study links essential AI agent
properties such as autonomy, adaptive learning, and real-time responsiveness to
each subcategory's security requirements. In addition, it outlines graduated
levels of autonomy (assisted, augmented, and fully autonomous) to accommodate
organisations at varying stages of cybersecurity maturity. This holistic
approach transcends isolated AI applications, providing a unified detection,
incident response, and governance strategy. Through conceptual validation, the
framework demonstrates how tailored AI agent deployments can align with
real-world constraints and risk profiles, enhancing situational awareness,
accelerating response times, and fortifying long-term resilience via adaptive
risk management. Ultimately, this research bridges the gap between theoretical
AI constructs and operational cybersecurity demands, establishing a foundation
for robust, empirically validated multi-agent systems that adhere to industry
standards.

</details>


### [69] [REBot: From RAG to CatRAG with Semantic Enrichment and Graph Routing](https://arxiv.org/abs/2510.01800)
*Thanh Ma,Tri-Tam La,Lam-Thu Le Huu,Minh-Nghi Nguyen,Khanh-Van Pham Luu,Huu-Hoa Nguyen*

Main category: cs.AI

TL;DR: 提出了REBot，一个基于CatRAG混合检索推理框架的LLM增强学术规章制度咨询聊天机器人，在分类和问答任务中达到98.89%的F1分数。


<details>
  <summary>Details</summary>
Motivation: 学术规章制度咨询对学生理解遵守制度政策很重要，但构建有效系统需要领域特定的监管资源。

Method: CatRAG混合检索推理框架，结合检索增强生成和图推理，使用层次化类别标注知识图谱，轻量级意图分类器路由查询到适当检索模块。

Result: 在规章制度特定数据集上评估，分类和问答任务达到最先进性能，F1分数98.89%。

Conclusion: 实现了展示REBot在实际学术咨询场景中实用价值的Web应用。

Abstract: Academic regulation advising is essential for helping students interpret and
comply with institutional policies, yet building effective systems requires
domain specific regulatory resources. To address this challenge, we propose
REBot, an LLM enhanced advisory chatbot powered by CatRAG, a hybrid retrieval
reasoning framework that integrates retrieval augmented generation with graph
based reasoning. CatRAG unifies dense retrieval and graph reasoning, supported
by a hierarchical, category labeled knowledge graph enriched with semantic
features for domain alignment. A lightweight intent classifier routes queries
to the appropriate retrieval modules, ensuring both factual accuracy and
contextual depth. We construct a regulation specific dataset and evaluate REBot
on classification and question answering tasks, achieving state of the art
performance with an F1 score of 98.89%. Finally, we implement a web application
that demonstrates the practical value of REBot in real world academic advising
scenarios.

</details>


### [70] [Human-AI Teaming Co-Learning in Military Operations](https://arxiv.org/abs/2510.01815)
*Clara Maathuis,Kasper Cools*

Main category: cs.AI

TL;DR: 提出了一种可信赖的协同学习模型，用于军事行动中的人机协作，通过四个维度实现：可调节自主性、多层控制、双向反馈和协作决策。


<details>
  <summary>Details</summary>
Motivation: 在快速演变的军事威胁和复杂作战环境中，AI集成带来显著优势，但也面临有效和伦理部署的挑战。当前研究多从外部视角处理人机协作系统，而深入系统内部动态能处理更广泛的多维责任、安全和鲁棒性方面。

Method: 设计包含四个维度的可信赖协同学习模型：1)可调节自主性，根据任务状态、系统信心和环境不确定性动态校准自主水平；2)多层控制，提供持续监督、活动监控和问责；3)双向反馈，通过显性和隐性反馈回路确保推理、不确定性和学习适应的适当沟通；4)协作决策，生成、评估和提出带有置信水平和理由的决策。

Result: 提出了具体示例和建议，有助于进一步发展负责任和可信赖的军事人机协作系统。

Conclusion: 该模型通过四个关键维度的整合，为军事行动中的人机协作提供了一种可信赖的协同学习方法，能够应对不断变化的战场条件，确保系统的责任性、安全性和鲁棒性。

Abstract: In a time of rapidly evolving military threats and increasingly complex
operational environments, the integration of AI into military operations proves
significant advantages. At the same time, this implies various challenges and
risks regarding building and deploying human-AI teaming systems in an effective
and ethical manner. Currently, understanding and coping with them are often
tackled from an external perspective considering the human-AI teaming system as
a collective agent. Nevertheless, zooming into the dynamics involved inside the
system assures dealing with a broader palette of relevant multidimensional
responsibility, safety, and robustness aspects. To this end, this research
proposes the design of a trustworthy co-learning model for human-AI teaming in
military operations that encompasses a continuous and bidirectional exchange of
insights between the human and AI agents as they jointly adapt to evolving
battlefield conditions. It does that by integrating four dimensions. First,
adjustable autonomy for dynamically calibrating the autonomy levels of agents
depending on aspects like mission state, system confidence, and environmental
uncertainty. Second, multi-layered control which accounts continuous oversight,
monitoring of activities, and accountability. Third, bidirectional feedback
with explicit and implicit feedback loops between the agents to assure a proper
communication of reasoning, uncertainties, and learned adaptations that each of
the agents has. And fourth, collaborative decision-making which implies the
generation, evaluation, and proposal of decisions associated with confidence
levels and rationale behind them. The model proposed is accompanied by concrete
exemplifications and recommendations that contribute to further developing
responsible and trustworthy human-AI teaming systems in military operations.

</details>


### [71] [Plan Then Action:High-Level Planning Guidance Reinforcement Learning for LLM Reasoning](https://arxiv.org/abs/2510.01833)
*Zhihao Dou,Qinjian Zhao,Zhongwei Wan,Dinggen Zhang,Weida Wang,Towsif Raiyan,Benteng Chen,Qingtao Pan,Yang Ouyang,Zhiqiang Gao,Shufei Zhang,Sumon Biswas*

Main category: cs.AI

TL;DR: 提出了PTA-GRPO框架，通过两阶段方法改进LLM的推理能力：首先生成高层规划指导，然后进行细粒度推理优化，在多个数学推理基准上取得显著提升


<details>
  <summary>Details</summary>
Motivation: 现有LLM的推理过程缺乏全局规划，导致推理冗余、不连贯或不准确，而现有方法如树搜索和RL计算成本高且效果不佳

Method: 两阶段框架：第一阶段利用高级LLM将思维链提炼为紧凑的高层指导进行监督微调；第二阶段引入感知指导的强化学习方法，联合优化最终输出和高层指导质量

Result: 在MATH、AIME2024、AIME2025、AMC等多个数学推理基准上，使用Qwen2.5-7B、Qwen3-8B、Qwen3-14B、LLaMA3.2-3B等模型均实现了稳定显著的性能提升

Conclusion: PTA-GRPO框架能有效提升LLM的推理能力，在不同模型和任务上具有良好的泛化性

Abstract: Large language models (LLMs) have demonstrated remarkable reasoning abilities
in complex tasks, often relying on Chain-of-Thought (CoT) reasoning. However,
due to their autoregressive token-level generation, the reasoning process is
largely constrained to local decision-making and lacks global planning. This
limitation frequently results in redundant, incoherent, or inaccurate
reasoning, which significantly degrades overall performance. Existing
approaches, such as tree-based algorithms and reinforcement learning (RL),
attempt to address this issue but suffer from high computational costs and
often fail to produce optimal reasoning trajectories. To tackle this challenge,
we propose Plan-Then-Action Enhanced Reasoning with Group Relative Policy
Optimization PTA-GRPO, a two-stage framework designed to improve both
high-level planning and fine-grained CoT reasoning. In the first stage, we
leverage advanced LLMs to distill CoT into compact high-level guidance, which
is then used for supervised fine-tuning (SFT). In the second stage, we
introduce a guidance-aware RL method that jointly optimizes the final output
and the quality of high-level guidance, thereby enhancing reasoning
effectiveness. We conduct extensive experiments on multiple mathematical
reasoning benchmarks, including MATH, AIME2024, AIME2025, and AMC, across
diverse base models such as Qwen2.5-7B-Instruct, Qwen3-8B, Qwen3-14B, and
LLaMA3.2-3B. Experimental results demonstrate that PTA-GRPO consistently
achieves stable and significant improvements across different models and tasks,
validating its effectiveness and generalization.

</details>


### [72] [Learning a Dense Reasoning Reward Model from Expert Demonstration via Inverse Reinforcement Learning](https://arxiv.org/abs/2510.01857)
*Claudio Fanconi,Nicolás Astorga,Mihaela van der Schaar*

Main category: cs.AI

TL;DR: 该论文将对抗性逆强化学习应用于大语言模型推理，学习密集的token级奖励模型用于过程监督，可同时用于训练时的步骤级反馈和推理时的轨迹重排，在GSM8K等任务上验证了有效性。


<details>
  <summary>Details</summary>
Motivation: 传统方法通过监督微调模仿专家风格，但无法提供过程级别的反馈。本文旨在学习一个能够评估推理过程正确性的奖励模型，而不仅仅是模仿表面形式。

Method: 采用对抗性逆强化学习框架，从专家演示中学习密集的token级奖励模型。该奖励模型在训练时提供步骤级反馈优化推理策略，在推理时作为评判器对采样轨迹进行重排。

Result: 在GSM8K任务上，使用Llama3和Qwen2.5骨干网络验证了：1）密集推理奖励可作为学习信号激发推理能力；2）通过奖励引导的重排显著提升了预测性能（特别是基于Llama的策略）。

Conclusion: 通过将训练信号、推理时选择和token级诊断统一到单一推理奖励中，这项工作展示了可重用的过程级奖励在增强语言模型多步推理方面的广阔潜力。

Abstract: We reframe and operationalise adversarial inverse reinforcement learning
(IRL) to large language model reasoning, learning a dense, token-level reward
model for process supervision directly from expert demonstrations rather than
imitating style via supervised fine-tuning. The learned reasoning reward serves
two complementary roles: (i) it provides step-level feedback to optimise a
reasoning policy during training; and (ii) it functions at inference as a
critic to rerank sampled traces under fixed compute budgets. We demonstrate
that our approach prioritises correctness over surface form, yielding scores
that correlate with eventual answer validity and enabling interpretable
localisation of errors within a trace. Empirically, on GSM8K with Llama3 and
Qwen2.5 backbones, we demonstrate: (i) dense reasoning rewards can be used as a
learning signal to elicit reasoning, and (ii) predictive performance is
improved from reward-guided reranking (notably for Llama-based policies). By
unifying training signals, inference-time selection, and token-level
diagnostics into a single reasoning reward, this work suggests reusable
process-level rewards with broad potential to enhance multi-step reasoning in
language models.

</details>


### [73] [Constrained Adaptive Rejection Sampling](https://arxiv.org/abs/2510.01902)
*Paweł Parys,Sairam Vaidya,Taylor Berg-Kirkpatrick,Loris D'Antoni*

Main category: cs.AI

TL;DR: CARS是一种约束生成方法，通过自适应剪枝技术提高拒绝采样的效率，同时保持语言模型的原始分布不变。


<details>
  <summary>Details</summary>
Motivation: 现有约束生成方法存在两难：贪婪解码会扭曲LM分布，而拒绝采样效率低下。在程序模糊测试等需要有效性和多样性的领域，这两种方法都有问题。

Method: CARS从无约束LM采样开始，通过trie数据结构记录违反约束的延续，并从未来采样中减去其概率质量，实现自适应剪枝。

Result: 在程序模糊测试和分子生成等领域的实验中，CARS在LM前向传递次数方面始终实现更高效率，同时产生比GCD和其他方法更强的样本多样性。

Conclusion: CARS严格提高了拒绝采样的样本效率，而不会产生分布失真，确保生成的样本完全遵循约束分布。

Abstract: Language Models (LMs) are increasingly used in applications where generated
outputs must satisfy strict semantic or syntactic constraints. Existing
approaches to constrained generation fall along a spectrum: greedy constrained
decoding methods enforce validity during decoding but distort the LM's
distribution, while rejection sampling (RS) preserves fidelity but wastes
computation by discarding invalid outputs. Both extremes are problematic in
domains such as program fuzzing, where both validity and diversity of samples
are essential. We present Constrained Adaptive Rejection Sampling (CARS), an
approach that strictly improves the sample-efficiency of RS without
distributional distortion. CARS begins with unconstrained LM sampling and
adaptively rules out constraint-violating continuations by recording them in a
trie and subtracting their probability mass from future draws. This adaptive
pruning ensures that prefixes proven invalid are never revisited, acceptance
rates improve monotonically, and the resulting samples exactly follow the
constrained distribution. In experiments on a variety of domains -- e.g.,
program fuzzing and molecular generation -- CARS consistently achieves higher
efficiency -- measured in the number of LM forward passes per valid sample --
while also producing stronger sample diversity than both GCD and methods that
approximate the LM's distribution.

</details>


### [74] [To Mask or to Mirror: Human-AI Alignment in Collective Reasoning](https://arxiv.org/abs/2510.01924)
*Crystal Qian,Aaron Parisi,Clémentine Bouleau,Vivian Tsai,Maël Lebreton,Lucas Dixon*

Main category: cs.AI

TL;DR: 该研究提出了一个评估大语言模型与人类集体决策对齐的框架，通过Lost at Sea实验对比了人类群体和LLM群体在领导选举中的行为差异。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型越来越多地用于建模和增强集体决策，需要检验它们与人类社会推理的对齐程度，特别是集体层面的对齐，而非个体层面。

Method: 使用Lost at Sea社会心理学任务进行大规模在线实验（N=748），随机分配群体进行领导选举，比较可见人口属性与匿名化名的条件。然后模拟匹配的LLM群体，测试Gemini 2.5、GPT 4.1、Claude Haiku 3.5和Gemma 3等模型。

Result: 不同LLM的行为存在差异：有些模型反映了人类偏见，有些则掩盖这些偏见并试图补偿。人类-AI在集体推理中的对齐取决于情境、线索和模型特定的归纳偏见。

Conclusion: 理解LLM如何与集体人类行为对齐对于推进社会对齐AI至关重要，需要能够捕捉集体推理复杂性的动态基准测试。

Abstract: As large language models (LLMs) are increasingly used to model and augment
collective decision-making, it is critical to examine their alignment with
human social reasoning. We present an empirical framework for assessing
collective alignment, in contrast to prior work on the individual level. Using
the Lost at Sea social psychology task, we conduct a large-scale online
experiment (N=748), randomly assigning groups to leader elections with either
visible demographic attributes (e.g. name, gender) or pseudonymous aliases. We
then simulate matched LLM groups conditioned on the human data, benchmarking
Gemini 2.5, GPT 4.1, Claude Haiku 3.5, and Gemma 3. LLM behaviors diverge: some
mirror human biases; others mask these biases and attempt to compensate for
them. We empirically demonstrate that human-AI alignment in collective
reasoning depends on context, cues, and model-specific inductive biases.
Understanding how LLMs align with collective human behavior is critical to
advancing socially-aligned AI, and demands dynamic benchmarks that capture the
complexities of collective reasoning.

</details>


### [75] [Zero-shot reasoning for simulating scholarly peer-review](https://arxiv.org/abs/2510.02027)
*Khalid M. Saqr*

Main category: cs.AI

TL;DR: 提出了一种确定性模拟框架，为评估AI生成的同行评审报告提供稳定、基于证据的标准，能够模拟校准的编辑判断并保持程序完整性。


<details>
  <summary>Details</summary>
Motivation: 学术出版生态系统面临提交量不可管理和AI不受监管的双重危机，需要新的治理模型来保障科学完整性。传统纯人工同行评审缺乏可扩展的客观基准。

Method: 采用确定性模拟框架，分析352份同行评审模拟报告，识别一致的系统状态指标。

Result: 系统能够模拟校准的编辑判断，'修订'决定在所有学科中占多数(>50%)，'拒绝'率在健康科学中动态适应至45%；保持29%的证据锚定合规率，在不同评审任务和科学领域中保持稳定。

Conclusion: 该框架将AI重新定位为机构问责的重要组成部分，为维护学术交流信任提供关键基础设施，为科学界提供确保公平的透明工具，为出版策略师提供可扩展的审计工具。

Abstract: The scholarly publishing ecosystem faces a dual crisis of unmanageable
submission volumes and unregulated AI, creating an urgent need for new
governance models to safeguard scientific integrity. The traditional human-only
peer review regime lacks a scalable, objective benchmark, making editorial
processes opaque and difficult to audit. Here we investigate a deterministic
simulation framework that provides the first stable, evidence-based standard
for evaluating AI-generated peer review reports. Analyzing 352 peer-review
simulation reports, we identify consistent system state indicators that
demonstrate its reliability. First, the system is able to simulate calibrated
editorial judgment, with 'Revise' decisions consistently forming the majority
outcome (>50%) across all disciplines, while 'Reject' rates dynamically adapt
to field-specific norms, rising to 45% in Health Sciences. Second, it maintains
unwavering procedural integrity, enforcing a stable 29% evidence-anchoring
compliance rate that remains invariant across diverse review tasks and
scientific domains. These findings demonstrate a system that is predictably
rule-bound, mitigating the stochasticity of generative AI. For the scientific
community, this provides a transparent tool to ensure fairness; for publishing
strategists, it offers a scalable instrument for auditing workflows, managing
integrity risks, and implementing evidence-based governance. The framework
repositions AI as an essential component of institutional accountability,
providing the critical infrastructure to maintain trust in scholarly
communication.

</details>


### [76] [ReTabAD: A Benchmark for Restoring Semantic Context in Tabular Anomaly Detection](https://arxiv.org/abs/2510.02060)
*Sanghyu Yoon,Dongmin Kim,Suhee Yoon,Ye Seul Sim,Seungdong Yoa,Hye-Seung Cho,Soonyoung Lee,Hankook Lee,Woohyung Lim*

Main category: cs.AI

TL;DR: ReTabAD是一个为表格异常检测恢复文本语义的基准，提供20个带有文本元数据的表格数据集和多种检测算法，通过零样本LLM框架利用语义上下文提升检测性能和可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有表格异常检测基准缺乏文本语义上下文，而实际应用中异常定义往往依赖领域特定的文本元数据（如特征描述和领域知识），这限制了模型充分利用领域知识进行检测的能力。

Method: 构建20个带有结构化文本元数据的表格数据集，实现包括经典、深度学习和基于LLM的多种先进异常检测算法，并开发零样本LLM框架来利用语义上下文而无需任务特定训练。

Result: 实验结果显示语义上下文能够提高检测性能，并通过支持领域感知推理增强可解释性。

Conclusion: ReTabAD为系统探索上下文感知的异常检测建立了基准，证明了文本元数据在异常检测中的重要价值和作用。

Abstract: In tabular anomaly detection (AD), textual semantics often carry critical
signals, as the definition of an anomaly is closely tied to domain-specific
context. However, existing benchmarks provide only raw data points without
semantic context, overlooking rich textual metadata such as feature
descriptions and domain knowledge that experts rely on in practice. This
limitation restricts research flexibility and prevents models from fully
leveraging domain knowledge for detection. ReTabAD addresses this gap by
restoring textual semantics to enable context-aware tabular AD research. We
provide (1) 20 carefully curated tabular datasets enriched with structured
textual metadata, together with implementations of state-of-the-art AD
algorithms including classical, deep learning, and LLM-based approaches, and
(2) a zero-shot LLM framework that leverages semantic context without
task-specific training, establishing a strong baseline for future research.
Furthermore, this work provides insights into the role and utility of textual
metadata in AD through experiments and analysis. Results show that semantic
context improves detection performance and enhances interpretability by
supporting domain-aware reasoning. These findings establish ReTabAD as a
benchmark for systematic exploration of context-aware AD.

</details>


### [77] [Demystifying the Roles of LLM Layers in Retrieval, Knowledge, and Reasoning](https://arxiv.org/abs/2510.02091)
*Xinyuan Song,Keyu Wang,PengXiang Li,Lu Yin,Shiwei Liu*

Main category: cs.AI

TL;DR: 论文系统研究了LLM深度层的作用，发现深度层利用率高度异质化且依赖上下文，浅层负责知识和检索，深层对推理能力至关重要


<details>
  <summary>Details</summary>
Motivation: 针对现有研究认为LLM深层对表示学习贡献有限且可被移除的观点，但此类结论通常基于狭窄评估，可能忽略模型行为的重要方面

Method: 通过多样化的评估维度（评估协议、任务类别、模型架构）系统研究深度利用率，分析不同设置下的层贡献

Result: 基于似然的非生成评估中，仅需前几层即可保持性能；而生成评估发现中间和深层在推理和长程连贯性中不可或缺；知识检索集中在浅层，推理准确性依赖深层但可通过蒸馏重塑

Conclusion: LLM深度使用高度异质化且依赖上下文，在解释和压缩大模型时需要任务、指标和模型感知的视角

Abstract: Recent studies suggest that the deeper layers of Large Language Models (LLMs)
contribute little to representation learning and can often be removed without
significant performance loss. However, such claims are typically drawn from
narrow evaluations and may overlook important aspects of model behavior. In
this work, we present a systematic study of depth utilization across diverse
dimensions, including evaluation protocols, task categories, and model
architectures. Our analysis confirms that very deep layers are generally less
effective than earlier ones, but their contributions vary substantially with
the evaluation setting. Under likelihood-based metrics without generation,
pruning most layers preserves performance, with only the initial few being
critical. By contrast, generation-based evaluation uncovers indispensable roles
for middle and deeper layers in enabling reasoning and maintaining long-range
coherence. We further find that knowledge and retrieval are concentrated in
shallow components, whereas reasoning accuracy relies heavily on deeper layers
-- yet can be reshaped through distillation. These results highlight that depth
usage in LLMs is highly heterogeneous and context-dependent, underscoring the
need for task-, metric-, and model-aware perspectives in both interpreting and
compressing large models.

</details>


### [78] [Do AI Models Perform Human-like Abstract Reasoning Across Modalities?](https://arxiv.org/abs/2510.02125)
*Claas Beger,Ryan Yi,Shuhao Fu,Arseny Moskvichev,Sarah W. Tsai,Sivasankaran Rajamanickam,Melanie Mitchell*

Main category: cs.AI

TL;DR: 论文研究发现，虽然AI模型在文本模态的抽象推理任务中可以达到人类水平的准确率，但其实际抽象推理能力被高估，因为模型往往依赖表面模式而非真正的抽象概念。在视觉模态中，模型准确率下降但抽象能力可能被低估。


<details>
  <summary>Details</summary>
Motivation: 评估AI模型在抽象推理任务中的真实能力，检验模型是否真正理解任务设计者意图的抽象概念，而非仅依赖表面模式匹配。

Method: 在ConceptARC基准上评估模型，比较不同输入模态（文本vs视觉）、是否允许使用外部Python工具、以及推理模型的不同推理努力程度。除了输出准确率，还精细评估模型生成的解释规则。

Result: 文本模态下模型准确率可达人类水平，但规则分析显示模型主要依赖表面"捷径"而非真正抽象概念。视觉模态下准确率下降，但模型仍能生成相当比例的抽象规则，只是应用能力不足。

Conclusion: 仅凭准确率评估抽象推理能力会高估文本模态的模型能力，低估视觉模态的模型能力。需要结合规则分析来更准确地评估模型的抽象推理能力。

Abstract: OpenAI's o3-preview reasoning model exceeded human accuracy on the ARC-AGI
benchmark, but does that mean state-of-the-art models recognize and reason with
the abstractions that the task creators intended? We investigate models'
abstraction abilities on ConceptARC. We evaluate models under settings that
vary the input modality (textual vs. visual), whether the model is permitted to
use external Python tools, and, for reasoning models, the amount of reasoning
effort. In addition to measuring output accuracy, we perform fine-grained
evaluation of the natural-language rules that models generate to explain their
solutions. This dual evaluation lets us assess whether models solve tasks using
the abstractions ConceptARC was designed to elicit, rather than relying on
surface-level patterns. Our results show that, while some models using
text-based representations match human output accuracy, the best models' rules
are often based on surface-level ``shortcuts'' and capture intended
abstractions far less often than humans. Thus their capabilities for general
abstract reasoning may be overestimated by evaluations based on accuracy alone.
In the visual modality, AI models' output accuracy drops sharply, yet our
rule-level analysis reveals that models might be underestimated, as they still
exhibit a substantial share of rules that capture intended abstractions, but
are often unable to correctly apply these rules. In short, our results show
that models still lag humans in abstract reasoning, and that using accuracy
alone to evaluate abstract reasoning on ARC-like tasks may overestimate
abstract-reasoning capabilities in textual modalities and underestimate it in
visual modalities. We believe that our evaluation framework offers a more
faithful picture of multimodal models' abstract reasoning abilities and a more
principled way to track progress toward human-like, abstraction-centered
intelligence.

</details>


### [79] [FlexDoc: Parameterized Sampling for Diverse Multilingual Synthetic Documents for Training Document Understanding Models](https://arxiv.org/abs/2510.02133)
*Karan Dua,Hitesh Laxmichand Patel,Puneet Mittal,Ranjeet Gupta,Amit Agarwal,Praneet Pabolu,Srikant Panda,Hansa Meghwani,Graham Horwood,Fahad Shah*

Main category: cs.AI

TL;DR: FlexDoc是一个可扩展的合成数据生成框架，通过随机模式和参数化采样生成多语言半结构化文档，显著降低文档理解模型的数据收集和标注成本。


<details>
  <summary>Details</summary>
Motivation: 企业级文档理解模型需要大量多样化且标注良好的数据集，但传统数据收集方法成本高昂（可达数百万美元），且面临隐私约束、法律限制和大量手动标注需求。

Method: 结合随机模式和参数化采样，通过概率建模布局模式、视觉结构和内容变异性，实现可控的大规模多样化文档变体生成。

Result: 在关键信息提取任务中，使用FlexDoc生成的数据可将F1分数绝对值提升高达11%，同时相比传统硬模板方法减少90%以上的标注工作量。

Conclusion: FlexDoc已在实际部署中加速了企业级文档理解模型的开发，同时显著降低了数据获取和标注成本。

Abstract: Developing document understanding models at enterprise scale requires large,
diverse, and well-annotated datasets spanning a wide range of document types.
However, collecting such data is prohibitively expensive due to privacy
constraints, legal restrictions, and the sheer volume of manual annotation
needed - costs that can scale into millions of dollars. We introduce FlexDoc, a
scalable synthetic data generation framework that combines Stochastic Schemas
and Parameterized Sampling to produce realistic, multilingual semi-structured
documents with rich annotations. By probabilistically modeling layout patterns,
visual structure, and content variability, FlexDoc enables the controlled
generation of diverse document variants at scale. Experiments on Key
Information Extraction (KIE) tasks demonstrate that FlexDoc-generated data
improves the absolute F1 Score by up to 11% when used to augment real datasets,
while reducing annotation effort by over 90% compared to traditional
hard-template methods. The solution is in active deployment, where it has
accelerated the development of enterprise-grade document understanding models
while significantly reducing data acquisition and annotation costs.

</details>


### [80] [A Rigorous Benchmark with Multidimensional Evaluation for Deep Research Agents: From Answers to Reports](https://arxiv.org/abs/2510.02190)
*Yang Yao,Yixu Wang,Yuxuan Zhang,Yi Lu,Tianle Gu,Lingyu Li,Dingyi Zhao,Keming Wu,Haozhe Wang,Ping Nie,Yan Teng,Yingchun Wang*

Main category: cs.AI

TL;DR: 本文提出了一个针对深度研究代理(DRAs)的严格基准测试和多维评估框架，包含214个专家策划的挑战性查询和手动构建的参考包，用于全面评估长格式报告的质量。


<details>
  <summary>Details</summary>
Motivation: 现有基准在评估维度、响应格式和评分机制方面存在不足，无法有效评估从封闭语言模型转向互联代理系统的AI范式转变。

Method: 开发了包含10个广泛主题领域的214个专家策划查询的基准，并构建了多维评估框架，整合了语义质量、主题聚焦和检索可信度的评分指标。

Result: 实验证实主流DRAs优于基于网络搜索工具增强的推理模型，但仍存在显著的改进空间。

Conclusion: 该研究为DRA系统的能力评估、架构优化和范式进步提供了坚实基础。

Abstract: Artificial intelligence is undergoing the paradigm shift from closed language
models to interconnected agent systems capable of external perception and
information integration. As a representative embodiment, Deep Research Agents
(DRAs) systematically exhibit the capabilities for task decomposition,
cross-source retrieval, multi-stage reasoning, and structured output, which
markedly enhance performance on complex and open-ended tasks. However, existing
benchmarks remain deficient in evaluation dimensions, response formatting, and
scoring mechanisms, limiting their capacity to assess such systems effectively.
This paper introduces a rigorous benchmark and a multidimensional evaluation
framework tailored to DRAs and report-style responses. The benchmark comprises
214 expert-curated challenging queries distributed across 10 broad thematic
domains, each accompanied by manually constructed reference bundles to support
composite evaluation. The framework enables comprehensive evaluation of
long-form reports generated by DRAs, incorporating integrated scoring metrics
for semantic quality, topical focus, and retrieval trustworthiness. Extensive
experimentation confirms the superior performance of mainstream DRAs over
web-search-tool-augmented reasoning models, yet reveals considerable scope for
further improvement. This study provides a robust foundation for capability
assessment, architectural refinement, and paradigm advancement in DRA systems.

</details>


### [81] [The Reasoning Boundary Paradox: How Reinforcement Learning Constrains Language Models](https://arxiv.org/abs/2510.02230)
*Phuc Minh Nguyen,Chinh D. La,Duy M. H. Nguyen,Nitesh V. Chawla,Binh T. Nguyen,Khoa D. Doan*

Main category: cs.AI

TL;DR: 研究发现RLVR方法会缩小而非扩大大语言模型的推理边界，主要由于负干扰和赢家通吃现象，导致Pass@k性能下降。作者提出针对低概率问题的数据筛选算法来改善这一状况。


<details>
  <summary>Details</summary>
Motivation: 尽管RLVR被广泛用于提升大语言模型的推理能力，但近期证据表明它可能反而缩小推理边界，本文旨在深入探究这一失败现象背后的机制。

Method: 通过理论分析和在多个数学推理基准上的实证研究，揭示了RLVR中的负干扰和赢家通吃现象，并提出了专注于低概率问题的数据筛选算法。

Result: 研究表明RLVR会不成比例地强化基础模型中高概率的正确解，同时抑制其他低概率解，导致模型收敛到狭窄的解决策略。提出的数据筛选算法显著提升了Pass@k性能。

Conclusion: RLVR的失败源于标准RL目标中的固有策略采样，导致模型收敛到狭窄策略。通过专注于低概率问题的数据筛选可以有效改善Pass@k性能。

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as a key
method for improving Large Language Models' reasoning capabilities, yet recent
evidence suggests it may paradoxically shrink the reasoning boundary rather
than expand it. This paper investigates the shrinkage issue of RLVR by
analyzing its learning dynamics and reveals two critical phenomena that explain
this failure. First, we expose negative interference in RLVR, where learning to
solve certain training problems actively reduces the likelihood of correct
solutions for others, leading to the decline of Pass@$k$ performance, or the
probability of generating a correct solution within $k$ attempts. Second, we
uncover the winner-take-all phenomenon: RLVR disproportionately reinforces
problems with high likelihood, correct solutions, under the base model, while
suppressing other initially low-likelihood ones. Through extensive theoretical
and empirical analysis on multiple mathematical reasoning benchmarks, we show
that this effect arises from the inherent on-policy sampling in standard RL
objectives, causing the model to converge toward narrow solution strategies.
Based on these insights, we propose a simple yet effective data curation
algorithm that focuses RLVR learning on low-likelihood problems, achieving
notable improvement in Pass@$k$ performance. Our code is available at
https://github.com/mail-research/SELF-llm-interference.

</details>


### [82] [The Unreasonable Effectiveness of Scaling Agents for Computer Use](https://arxiv.org/abs/2510.02250)
*Gonzalo Gonzalez-Pumariega,Vincent Tu,Chih-Lun Lee,Jiachen Yang,Ang Li,Xin Eric Wang*

Main category: cs.AI

TL;DR: 提出了Behavior Best-of-N (bBoN)方法，通过生成多个执行轨迹并使用行为叙述来选择最佳轨迹，显著提高了计算机使用代理在复杂任务中的成功率和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 计算机使用代理在自动化日常数字任务方面具有潜力，但其不可靠性和高方差阻碍了在长视野复杂任务中的应用。

Method: bBoN方法通过生成多个代理执行轨迹，并使用描述代理执行过程的行为叙述来进行轨迹选择，实现广泛探索和原则性轨迹选择。

Result: 在OSWorld基准测试中达到69.9%的新SOTA，接近人类水平的72%，在WindowsAgentArena和AndroidWorld上表现出强大的泛化能力。

Conclusion: 有效扩展计算机使用代理需要结构化的轨迹理解和选择，bBoN提供了一个实用的框架来实现这一目标。

Abstract: Computer-use agents (CUAs) hold promise for automating everyday digital
tasks, but their unreliability and high variance hinder their application to
long-horizon, complex tasks. We introduce Behavior Best-of-N (bBoN), a method
that scales over agents by generating multiple rollouts and selecting among
them using behavior narratives that describe the agents' rollouts. It enables
both wide exploration and principled trajectory selection, substantially
improving robustness and success rates. On OSWorld, our bBoN scaling method
establishes a new state of the art (SoTA) at 69.9%, significantly outperforming
prior methods and approaching human-level performance at 72%, with
comprehensive ablations validating key design choices. We further demonstrate
strong generalization results to different operating systems on
WindowsAgentArena and AndroidWorld. Crucially, our results highlight the
unreasonable effectiveness of scaling CUAs, when you do it right: effective
scaling requires structured trajectory understanding and selection, and bBoN
provides a practical framework to achieve this.

</details>


### [83] [RLAD: Training LLMs to Discover Abstractions for Solving Reasoning Problems](https://arxiv.org/abs/2510.02263)
*Yuxiao Qu,Anikait Singh,Yoonho Lee,Amrith Setlur,Ruslan Salakhutdinov,Chelsea Finn,Aviral Kumar*

Main category: cs.AI

TL;DR: 提出RLAD方法，通过推理抽象来引导模型进行有效的算法推理，采用两玩家强化学习训练抽象生成器和解决方案生成器，提升对困难问题的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有大型模型在推理过程中往往无法持续捕获或重用算法过程，而是陷入冗长和退化的探索。需要开发更有效的推理方法来引导模型学习成功的推理过程。

Method: 引入推理抽象概念，训练模型能够为问题提出多个抽象描述，然后通过强化学习激励在构建解决方案时使用这些抽象信息。采用两玩家RL训练范式，联合训练抽象生成器和解决方案生成器。

Result: RLAD方法实现了结构化探索，解耦了抽象提议和解决方案生成的学习信号，并提高了对更困难问题的泛化能力。测试时分配更多计算资源生成抽象比生成更多解决方案更有利于性能提升。

Conclusion: 推理抽象在引导有意义的探索中发挥重要作用，RLAD训练范式能够有效提升模型的算法推理能力，特别是在处理复杂问题时表现出更好的泛化性能。

Abstract: Reasoning requires going beyond pattern matching or memorization of solutions
to identify and implement "algorithmic procedures" that can be used to deduce
answers to hard problems. Doing so requires realizing the most relevant
primitives, intermediate results, or shared procedures, and building upon them.
While RL post-training on long chains of thought ultimately aims to uncover
this kind of algorithmic behavior, most reasoning traces learned by large
models fail to consistently capture or reuse procedures, instead drifting into
verbose and degenerate exploration. To address more effective reasoning, we
introduce reasoning abstractions: concise natural language descriptions of
procedural and factual knowledge that guide the model toward learning
successful reasoning. We train models to be capable of proposing multiple
abstractions given a problem, followed by RL that incentivizes building a
solution while using the information provided by these abstractions. This
results in a two-player RL training paradigm, abbreviated as RLAD, that jointly
trains an abstraction generator and a solution generator. This setup
effectively enables structured exploration, decouples learning signals of
abstraction proposal and solution generation, and improves generalization to
harder problems. We also show that allocating more test-time compute to
generating abstractions is more beneficial for performance than generating more
solutions at large test budgets, illustrating the role of abstractions in
guiding meaningful exploration.

</details>


### [84] [BioX-Bridge: Model Bridging for Unsupervised Cross-Modal Knowledge Transfer across Biosignals](https://arxiv.org/abs/2510.02276)
*Chenqi Li,Yu Liu,Timothy Denison,Tingting Zhu*

Main category: cs.AI

TL;DR: 提出BioX-Bridge框架，通过轻量级桥接网络实现生物信号跨模态知识迁移，显著减少可训练参数同时保持或提升性能。


<details>
  <summary>Details</summary>
Motivation: 生物信号模态间存在相关性，但大标注数据集稀缺限制了特定任务模型的训练。现有知识蒸馏方法计算和内存开销大，特别是对于大型基础模型。

Method: 训练轻量级桥接网络对齐基础模型的中间表示，实现跨模态信息流动。包括高效的桥接位置选择策略和灵活的原型网络架构。

Result: 在多个生物信号模态、任务和数据集上的实验表明，BioX-Bridge将可训练参数减少88-99%，同时保持或优于现有方法的迁移性能。

Conclusion: BioX-Bridge为生物信号的无监督跨模态知识迁移提供了高效解决方案，显著降低了计算和内存需求。

Abstract: Biosignals offer valuable insights into the physiological states of the human
body. Although biosignal modalities differ in functionality, signal fidelity,
sensor comfort, and cost, they are often intercorrelated, reflecting the
holistic and interconnected nature of human physiology. This opens up the
possibility of performing the same tasks using alternative biosignal
modalities, thereby improving the accessibility, usability, and adaptability of
health monitoring systems. However, the limited availability of large labeled
datasets presents challenges for training models tailored to specific tasks and
modalities of interest. Unsupervised cross-modal knowledge transfer offers a
promising solution by leveraging knowledge from an existing modality to support
model training for a new modality. Existing methods are typically based on
knowledge distillation, which requires running a teacher model alongside
student model training, resulting in high computational and memory overhead.
This challenge is further exacerbated by the recent development of foundation
models that demonstrate superior performance and generalization across tasks at
the cost of large model sizes. To this end, we explore a new framework for
unsupervised cross-modal knowledge transfer of biosignals by training a
lightweight bridge network to align the intermediate representations and enable
information flow between foundation models and across modalities. Specifically,
we introduce an efficient strategy for selecting alignment positions where the
bridge should be constructed, along with a flexible prototype network as the
bridge architecture. Extensive experiments across multiple biosignal
modalities, tasks, and datasets show that BioX-Bridge reduces the number of
trainable parameters by 88--99\% while maintaining or even improving transfer
performance compared to state-of-the-art methods.

</details>
