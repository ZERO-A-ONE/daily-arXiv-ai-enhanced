<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 15]
- [cs.AI](#cs.AI) [Total: 34]
- [cs.CR](#cs.CR) [Total: 12]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [Retrieval-Augmented Few-Shot Prompting Versus Fine-Tuning for Code Vulnerability Detection](https://arxiv.org/abs/2512.04106)
*Fouad Trad,Ali Chehab*

Main category: cs.SE

TL;DR: 检索增强提示在代码漏洞检测中优于标准少样本提示和微调模型，但微调CodeBERT性能更高但需要更多资源


<details>
  <summary>Details</summary>
Motivation: 少样本提示已成为利用大语言模型进行专业任务的实用替代方案，但其效果严重依赖于上下文示例的选择和质量，特别是在复杂领域如代码漏洞检测中

Method: 使用Gemini-1.5-Flash模型系统评估三种方法：1) 随机选择示例的标准少样本提示；2) 使用语义相似示例的检索增强提示；3) 基于检索示例分配标签的检索标注方法

Result: 检索增强提示在20个示例时达到74.05%的F1分数和83.90%的部分匹配准确率，优于标准少样本提示、零样本提示和微调的Gemini模型，但微调CodeBERT获得更高性能（F1分数91.22%，部分匹配准确率91.30%）

Conclusion: 检索增强提示在代码漏洞检测中提供了性能与资源消耗的良好平衡，优于传统少样本提示和微调方法，但微调专用模型如CodeBERT可获得最佳性能但需要更多训练和维护成本

Abstract: Few-shot prompting has emerged as a practical alternative to fine-tuning for leveraging the capabilities of large language models (LLMs) in specialized tasks. However, its effectiveness depends heavily on the selection and quality of in-context examples, particularly in complex domains. In this work, we examine retrieval-augmented prompting as a strategy to improve few-shot performance in code vulnerability detection, where the goal is to identify one or more security-relevant weaknesses present in a given code snippet from a predefined set of vulnerability categories. We perform a systematic evaluation using the Gemini-1.5-Flash model across three approaches: (1) standard few-shot prompting with randomly selected examples, (2) retrieval-augmented prompting using semantically similar examples, and (3) retrieval-based labeling, which assigns labels based on retrieved examples without model inference. Our results show that retrieval-augmented prompting consistently outperforms the other prompting strategies. At 20 shots, it achieves an F1 score of 74.05% and a partial match accuracy of 83.90%. We further compare this approach against zero-shot prompting and several fine-tuned models, including Gemini-1.5-Flash and smaller open-source models such as DistilBERT, DistilGPT2, and CodeBERT. Retrieval-augmented prompting outperforms both zero-shot (F1 score: 36.35%, partial match accuracy: 20.30%) and fine-tuned Gemini (F1 score: 59.31%, partial match accuracy: 53.10%), while avoiding the training time and cost associated with model fine-tuning. On the other hand, fine-tuning CodeBERT yields higher performance (F1 score: 91.22%, partial match accuracy: 91.30%) but requires additional training, maintenance effort, and resources.

</details>


### [2] [Reusing Model Validation Methods for the Continuous Validation of Digital Twins of Cyber-Physical Systems](https://arxiv.org/abs/2512.04117)
*Joost Mertens,Joachim Denil*

Main category: cs.SE

TL;DR: 提出一种基于模型验证技术的数字孪生系统异常检测方法，通过验证指标检测物理系统变化，并使用参数估计修正数字孪生模型


<details>
  <summary>Details</summary>
Motivation: 数字孪生系统面临的主要挑战是保持数字孪生与物理系统的有效对应关系。特别是当数字孪生是反映物理系统行为的仿真模型时，物理系统会因维护、磨损或用户错误而不断演变，需要检测这些变化并同步更新数字孪生

Method: 采用基于模型设计的验证技术，通过验证指标检测孪生系统中的异常，并使用基于历史数据的参数估计方法来修正数字孪生中的错误

Result: 提出了一个通用方法，能够通过验证指标检测孪生系统中的异常，并在一个学术但具有工业相关性的港口龙门起重机案例研究中演示了这些技术

Conclusion: 通过重用基于模型设计的验证技术，可以有效检测物理系统的变化并相应更新数字孪生，确保数字孪生始终保持对物理系统的有效表示

Abstract: One of the challenges in twinned systems is ensuring the digital twin remains a valid representation of the system it twins. Depending on the type of twinning occurring, it is either trivial, such as in dashboarding/visualizations that mirror the system with real-time data, or challenging, in case the digital twin is a simulation model that reflects the behavior of a physical twinned system. The challenge in this latter case comes from the fact that in contrast to software systems, physical systems are not immutable once deployed, but instead they evolve through processes like maintenance, wear and tear or user error. It is therefore important to detect when changes occur in the physical system to evolve the twin alongside it. We employ and reuse validation techniques from model-based design for this goal. Model validation is one of the steps used to gain trust in the representativeness of a simulation model. In this work, we provide two contributions: (i) we provide a generic approach that, through the use of validation metrics, is able to detect anomalies in twinned systems, and (ii) we demonstrate these techniques with the help of an academic yet industrially relevant case study of a gantry crane such as found in ports. Treating anomalies also means correcting the error in the digital twin, which we do with a parameter estimation based on the historical data.

</details>


### [3] [DrP: Meta's Efficient Investigations Platform at Scale](https://arxiv.org/abs/2512.04250)
*Shubham Somani,Vanish Talwar,Madhura Parikh,Eduardo Hernandez,Jimmy Wang,Shreya Shah,Chinmay Gandhi,Sanjay Sundarajan,Neeru Sharma,Srikanth Kamath,Nitin Gupta,Benjamin Renard,Ohad Yahalom,Chris Davis*

Main category: cs.SE

TL;DR: DrP是一个端到端的自动化调查框架，通过可编程的分析器剧本、可扩展的执行系统和工作流集成，显著减少事故平均解决时间和值班工程师的工作负担。


<details>
  <summary>Details</summary>
Motivation: 大规模系统（服务、数据、AI/ML、移动等）的调查过程通常依赖手动或临时脚本，导致调查效率低下、解决时间延长、值班工程师负担重，需要花费数小时甚至数天进行事故分类和调试。

Method: DrP包含：1）表达性强且灵活的SDK，用于编写代码化的调查剧本（分析器）；2）可扩展的后端系统执行自动化剧本；3）插件集成主流工作流（如警报和事故管理工具）；4）后处理系统执行调查后的操作，包括缓解措施。

Result: 在Meta大规模部署：覆盖300+团队、2000+分析器，涵盖服务、核心基础设施、AI/ML、硬件、移动等多个领域。已在生产环境运行5年，每天执行5万次自动化分析。平均MTTR减少20%（某些团队超过80%），显著提升值班工程师生产力。

Conclusion: DrP通过自动化调查框架成功减少了事故解决时间和值班工程师负担，证明了在大规模生产环境中自动化调查系统的可行性和有效性。

Abstract: Investigations are a significant step in the operational workflows for large scale systems across multiple domains such as services, data, AI/ML, mobile. Investigation processes followed by on-call engineers are often manual or rely on ad-hoc scripts. This leads to inefficient investigations resulting in increased time to mitigate and isolate failures/SLO violations. It also contributes to on-call toil and poor productivity leading to multiple hours/days spent in triaging/debugging incidents. In this paper, we present DrP, an end-to-end framework and system to automate investigations that reduces the mean time to resolve incidents (MTTR) and reduces on-call toil. DrP consists of an expressive and flexible SDK to author investigation playbooks in code (called analyzers), a scalable backend system to execute these automated playbooks, plug-ins to integrate playbooks into mainstream workflows such as alerts and incident management tools, and a post-processing system to take actions on investigations including mitigation steps.
  We have implemented and deployed DrP at large scale at Meta covering 300+ teams, 2000+ analyzers, across a large set of use cases across domains such as services, core infrastructure, AI/ML, hardware, mobile. DrP has been running in production for the past 5 years and executes 50K automated analyses per day. Overall, our results and experience show that DrP has been able to reduce average MTTR by 20 percent at large scale (with over 80 percent for some teams) and has significantly improved on-call productivity.

</details>


### [4] [On the Role and Impact of GenAI Tools in Software Engineering Education](https://arxiv.org/abs/2512.04256)
*Qiaolin Qin,Ronnie de Souza Santos,Rodrigo Spinola*

Main category: cs.SE

TL;DR: 研究调查了软件工程本科生使用生成式AI工具的情况，发现学生主要用于增量学习和高级实现，面临输出适应困难和伦理担忧，需要更清晰的教学指导。


<details>
  <summary>Details</summary>
Motivation: 生成式AI工具如ChatGPT和GitHub Copilot正在改变软件学习和编写方式，在软件工程教育中既带来新机遇，也引发对过度依赖、伦理使用和学习影响的担忧，需要研究学生的实际使用情况。

Method: 对两所大学的130名本科生进行问卷调查，结合结构化李克特量表和开放式问题，从五个维度调查：使用情境、感知益处、挑战、伦理和教学认知。

Result: 学生最常将生成式AI用于增量学习和高级实现，报告了头脑风暴支持和信心建立等益处；同时面临包括不清楚推理过程和难以适应输出等挑战；学生强调公平性和不当行为等伦理担忧，并呼吁更清晰的教学指导。

Conclusion: 生成式AI正在以微妙方式重塑软件工程教育，研究结果强调了需要支架式支持、伦理政策和适应性教学策略，以确保生成式AI支持公平有效的学习。

Abstract: Context. The rise of generative AI (GenAI) tools like ChatGPT and GitHub Copilot has transformed how software is learned and written. In software engineering (SE) education, these tools offer new opportunities for support, but also raise concerns about over-reliance, ethical use, and impacts on learning. Objective. This study investigates how undergraduate SE students use GenAI tools, focusing on the benefits, challenges, ethical concerns, and instructional expectations that shape their experiences. Method. We conducted a survey with 130 undergraduate students from two universities. The survey combined structured Likert-scale items and open-ended questions to investigate five dimensions: usage context, perceived benefits, challenges, ethical and instructional perceptions. Results. Students most often use GenAI for incremental learning and advanced implementation, reporting benefits such as brainstorming support and confidence-building. At the same time, they face challenges including unclear rationales and difficulty adapting outputs. Students highlight ethical concerns around fairness and misconduct, and call for clearer instructional guidance. Conclusion. GenAI is reshaping SE education in nuanced ways. Our findings underscore the need for scaffolding, ethical policies, and adaptive instructional strategies to ensure that GenAI supports equitable and effective learning.

</details>


### [5] [Catching UX Flaws in Code: Leveraging LLMs to Identify Usability Flaws at the Development Stage](https://arxiv.org/abs/2512.04262)
*Nolan Platt,Ethan Luchs,Sehrish Nizamani*

Main category: cs.SE

TL;DR: 研究评估GPT-4o在早期开发阶段进行可用性启发式评估的可靠性，发现其在问题检测方面具有中等一致性，但在严重程度判断上表现不稳定。


<details>
  <summary>Details</summary>
Motivation: 传统的人工专家启发式评估耗时且主观，特别是在开发早期阶段。本研究旨在探索大型语言模型（LLMs）是否能在开发阶段提供可靠且一致的启发式评估。

Method: 使用OpenAI的GPT-4o构建评估管道，将Jakob Nielsen的十项可用性启发式原则应用于30个开源网站，每个网站进行三次独立评估，共生成超过850个启发式评估。

Result: 问题检测方面：平均配对Cohen's Kappa为0.50（中等一致性），精确一致性为84%。严重程度判断方面：加权Cohen's Kappa平均为0.63，但精确一致性仅为56%，Krippendorff's Alpha接近零。

Conclusion: GPT-4o能够产生内部一致的评估，特别是在识别可用性问题存在方面，但其严重程度判断能力存在变化，需要人工监督。研究为早期自动化可用性测试提供了可行性和局限性分析，并为改进自动化用户体验评估的一致性奠定了基础。

Abstract: Usability evaluations are essential for ensuring that modern interfaces meet user needs, yet traditional heuristic evaluations by human experts can be time-consuming and subjective, especially early in development. This paper investigates whether large language models (LLMs) can provide reliable and consistent heuristic assessments at the development stage. By applying Jakob Nielsen's ten usability heuristics to thirty open-source websites, we generated over 850 heuristic evaluations in three independent evaluations per site using a pipeline of OpenAI's GPT-4o. For issue detection, the model demonstrated moderate consistency, with an average pairwise Cohen's Kappa of 0.50 and an exact agreement of 84%. Severity judgments showed more variability: weighted Cohen's Kappa averaged 0.63, but exact agreement was just 56%, and Krippendorff's Alpha was near zero. These results suggest that while GPT-4o can produce internally consistent evaluations, especially for identifying the presence of usability issues, its ability to judge severity varies and requires human oversight in practice. Our findings highlight the feasibility and limitations of using LLMs for early-stage, automated usability testing, and offer a foundation for improving consistency in automated User Experience (UX) evaluation. To the best of our knowledge, our work provides one of the first quantitative inter-rater reliability analyses of automated heuristic evaluation and highlights methods for improving model consistency.

</details>


### [6] [Polynomiogram: An Integrated Framework for Root Visualization and Generative Art](https://arxiv.org/abs/2512.04263)
*Hoang Duc Nguyen,Anh Van Pham,Hien D. Nguyen*

Main category: cs.SE

TL;DR: Polynomiogram框架是一个集成的计算平台，通过灵活的参数采样方案，将用户定义的参数映射到多项式系数，支持多项式根系统的科学研究和生成艺术创作。


<details>
  <summary>Details</summary>
Motivation: 开发一个既能支持科学研究和教育，又能用于生成艺术的统一平台，将多项式根系统的数学基础应用于科学探索和创意表达。

Method: 采用灵活的采样方案，从用户定义域中抽取两个独立参数，通过生成函数映射到多项式系数；集成NumPy伴侣矩阵求解器进行快速大规模计算，以及MPSolve进行高精度验证的双引擎架构。

Result: 使用经典集合（如Kac和Lucas多项式）验证了数值准确性；应用于三次多项式系统分析其分岔结构；展示了作为生成艺术工具的潜力，生成了类似芙蓉花的自然形态和向AI/LLM致敬的个性化艺术作品。

Conclusion: Polynomiogram框架成功地将数学基础与计算技术相结合，既可作为探索根现象的科学工具和代数/动力系统可视化教育辅助，又可作为个性化生成艺术创作平台。

Abstract: This work presents the Polynomiogram framework, an integrated computational platform for exploring, visualizing, and generating art from polynomial root systems. The main innovation is a flexible sampling scheme in which two independent parameters are drawn from user defined domains and mapped to the polynomial coefficients through a generating function. This design allows the same mathematical foundation to support both scientific investigation and generative algorithmic art. The framework integrates two complementary numerical engines: NumPy companion matrix solver for fast, large scale computation and MPSolve for high precision, scientifically rigorous validation. This dual architecture enables efficient visualization for creative use and accurate computation for research and education. Numerical accuracy was verified using classical ensembles, including the Kac and Lucas polynomials. The method was applied to the cubic polynomial system to analyze its bifurcation structure, demonstrating its value as both a scientific tool for exploring root phenomena and an educational aid for visualizing fundamental concepts in algebra and dynamical systems. Beyond analysis, the Polynomiogram also demonstrated its potential as a tool for personalized generative art. Examples include the use of the platform to generate a natural form resembling a hibiscus flower and to create personalized artwork expressing gratitude toward advances in artificial intelligence and large language models through a tribute composition.

</details>


### [7] [Quantitative Analysis of Technical Debt and Pattern Violation in Large Language Model Architectures](https://arxiv.org/abs/2512.04273)
*Tyler Slater*

Main category: cs.SE

TL;DR: 该研究首次提出量化AI生成微服务中"架构侵蚀"和技术债务积累的实证框架，发现开源模型在架构一致性方面表现较差，存在实现懒惰现象。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型从代码补全工具转变为自主系统架构师，它们对软件长期可维护性的影响尚未量化。现有研究主要关注功能正确性，但缺乏对架构质量和技术债务积累的评估。

Method: 采用比较性试点研究，使用三种最先进模型（GPT-5.1、Claude 4.5 Sonnet和Llama 3 8B）在严格的六边形架构约束下实现标准化的图书借阅微服务。使用抽象语法树解析来量化架构违规情况。

Result: 专有模型实现高架构一致性（GPT-5.1违规率为0%），而开源模型表现严重偏离。Llama 3的架构违规率达到80%，经常绕过接口适配器创建领域层和基础设施层之间的非法循环依赖。还发现"实现懒惰"现象，开源模型生成的逻辑代码行数比专有模型少60%。

Conclusion: 如果不使用自动化架构检查工具，使用较小的开源模型进行系统脚手架会加速结构性技术债务的积累。需要专门的架构验证机制来确保AI生成代码的长期可维护性。

Abstract: As Large Language Models (LLMs) transition from code completion tools to autonomous system architects, their impact on long-term software maintainability remains unquantified. While existing research benchmarks functional correctness (pass@k), this study presents the first empirical framework to measure "Architectural Erosion" and the accumulation of Technical Debt in AI-synthesized microservices. We conducted a comparative pilot study of three state-of-the-art models (GPT-5.1, Claude 4.5 Sonnet, and Llama 3 8B) by prompting them to implement a standardized Book Lending Microservice under strict Hexagonal Architecture constraints. Utilizing Abstract Syntax Tree (AST) parsing, we find that while proprietary models achieve high architectural conformance (0% violation rate for GPT-5.1), open-weights models exhibit critical divergence. Specifically, Llama 3 demonstrated an 80% Architectural Violation Rate, frequently bypassing interface adapters to create illegal circular dependencies between Domain and Infrastructure layers. Furthermore, we identified a phenomenon of "Implementation Laziness," where open-weights models generated 60% fewer Logical Lines of Code (LLOC) than their proprietary counterparts, effectively omitting complex business logic to satisfy token constraints. These findings suggest that without automated architectural linting, utilizing smaller open-weights models for system scaffolding accelerates the accumulation of structural technical debt.

</details>


### [8] [MANTRA: a Framework for Multi-stage Adaptive Noise TReAtment During Training](https://arxiv.org/abs/2512.04319)
*Zixiao Zhao,Fatemeh H. Fard,Jie JW Wu*

Main category: cs.SE

TL;DR: MANTRA是一个多阶段自适应噪声处理框架，通过将噪声诊断和缓解直接嵌入到代码预训练语言模型和代码大语言模型的微调过程中，提高软件工程任务中深度学习模型的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 软件工程任务中深度学习模型的可靠应用依赖于高质量训练数据，但大规模存储库不可避免地引入噪声或错误标记的示例，这会降低模型的准确性和鲁棒性。虽然噪声标签学习在其他领域得到了广泛研究，但在软件工程和用于SE任务的大语言模型中，相关研究较少。

Method: 提出MANTRA框架：1）首先研究不同噪声水平对模型收敛和损失轨迹的影响；2）应用基于样本损失动态和高斯混合模型聚类的自适应dropout策略，排除持续噪声点同时保留干净数据；3）将框架应用于代码摘要和提交意图分类任务。

Result: 实验发现某些大语言模型比其他模型对噪声更敏感。然而，使用MANTRA后，所有模型在两个任务中的性能都得到了提升。MANTRA能够减少训练中数据集引入的错误影响，节省数据清洗和处理时间，同时最大化微调效果。

Conclusion: MANTRA框架通过将噪声诊断和缓解直接嵌入到微调过程中，有效提高了代码预训练语言模型和代码大语言模型在软件工程任务中的鲁棒性和性能，为研究人员和从业者提供了实用的噪声处理解决方案。

Abstract: The reliable application of deep learning models to software engineering tasks hinges on high-quality training data. Yet, large-scale repositories inevitably introduce noisy or mislabeled examples that degrade both accuracy and robustness. While Noise Label Learning (NLL) has been extensively studied in other fields, there are a few works that investigate NLL in Software Engineering (SE) and Large Language Models (LLMs) for SE tasks. In this work, we propose MANTRA, a Multi-stage Adaptive Noise TReAtment framework that embeds noise diagnosis and mitigation directly into the fine-tuning process of code-Pretrained Language Models (PTM) and code-LLMs. We first investigate the effect of noise at varying levels on convergence and loss trajectories of the models. Then we apply an adaptive dropout strategy guided by per-sample loss dynamics and Gaussian Mixture Model clustering to exclude persistently noisy points while preserving clean data. Applying to code summarization and commit intent classification, our experiments reveal that some LLMs are more sensitive to noise than others. However, with MANTRA, the performance of all models in both tasks is improved. MANTRA enables researchers and practitioners to reduce the impact of errors introduced by the dataset in training, saves time in data cleaning and processing, while maximizing the effect of fine-tuning.

</details>


### [9] [Targeted Testing of Compiler Optimizations via Grammar-Level Composition Styles](https://arxiv.org/abs/2512.04344)
*Zitong Zhou,Ben Limpanukorn,Hong Jin Kang,Jiyuan Wang,Yaoxuan Wu,Akos Kiss,Renata Hodovan,Miryung Kim*

Main category: cs.SE

TL;DR: TargetFuzz：一种针对单个编译器优化的定向模糊测试方法，通过挖掘和重建优化所需的组合模式，解决了传统流水线模糊测试的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有编译器模糊测试方法存在两个主要问题：1）使用优化流水线作为测试框架，由于阶段排序问题会错过优化间的交互，且许多优化未被调度；2）优化通常只在输入满足特定结构关系时才触发，而现有生成器和变异方法难以产生这些结构。

Method: 提出TargetFuzz，一种基于语法的变异模糊测试器：1）从优化相关语料库中挖掘组合模式（程序构造间的结构关系）；2）通过合成变异在通用语料库的不同上下文中重建这些模式来测试优化逻辑的变体。该方法通过轻量级语法标注适应新编程语言，自动合成变异器和交叉操作。

Result: 在LLVM和MLIR上的评估显示，TargetFuzz在定向模糊测试模式下比基线模糊测试器分别提高了8%和11%的覆盖率，触发优化的次数分别达到2.8倍和2.6倍。定向模糊测试补充了流水线测试：有效测试了所有37个采样的LLVM优化，而流水线模糊测试错过了12个。

Conclusion: TargetFuzz通过针对单个优化的定向模糊测试，有效解决了编译器优化测试的局限性，特别是在MLIR等模块化框架中具有重要价值，无需手动编写生成器或语言特定变异器。

Abstract: Ensuring the correctness of compiler optimizations is critical, but existing fuzzers struggle to test optimizations effectively. First, most fuzzers use optimization pipelines (heuristics-based, fixed sequences of passes) as their harness. The phase-ordering problem can enable or preempt transformations, so pipelines inevitably miss optimization interactions; moreover, many optimizations are not scheduled, even at aggressive levels. Second, optimizations typically fire only when inputs satisfy specific structural relationships, which existing generators and mutations struggle to produce. We propose targeted fuzzing of individual optimizations to complement pipeline-based testing. Our key idea is to exploit composition styles - structural relations over program constructs (adjacency, nesting, repetition, ordering) - that optimizations look for. We build a general-purpose, grammar-based mutational fuzzer, TargetFuzz, that (i) mines composition styles from an optimization-relevant corpus, then (ii) rebuilds them inside different contexts offered by a larger, generic corpus via synthesized mutations to test variations of optimization logic. TargetFuzz is adaptable to a new programming language by lightweight, grammar-based, construct annotations - and it automatically synthesizes mutators and crossovers to rebuild composition styles. No need for hand-coded generators or language-specific mutators, which is particularly useful for modular frameworks such as MLIR, whose dialect-based, rapidly evolving ecosystem makes optimizations difficult to fuzz. Our evaluation on LLVM and MLIR shows that TargetFuzz improves coverage by 8% and 11% and triggers optimizations 2.8$\times$ and 2.6$\times$, compared to baseline fuzzers under the targeted fuzzing mode. We show that targeted fuzzing is complementary: it effectively tests all 37 sampled LLVM optimizations, while pipeline-fuzzing missed 12.

</details>


### [10] [LLM-SrcLog: Towards Proactive and Unified Log Template Extraction via Large Language Models](https://arxiv.org/abs/2512.04474)
*Jiaqi Sun,Wei Li,Heng Zhang,Chutong Ding,Shiyou Qian,Jian Cao,Guangtao Xue*

Main category: cs.SE

TL;DR: LLM-SrcLog：一种主动统一的日志模板解析框架，结合源代码分析和数据驱动方法，在准确性和速度上取得平衡


<details>
  <summary>Details</summary>
Motivation: 当前日志解析器大多是反应式和日志中心的，仅从日志推断模板，忽略了源代码，限制了理解动态日志结构或适应系统演进的能力。此外，基于每条日志的LLM推理成本过高，难以实际部署。

Method: 提出LLM-SrcLog框架：1）从源代码直接提取模板（部署前）；2）对无可用代码的日志采用数据驱动解析。具体包括：跨函数静态代码分析器重建有意义的日志上下文、基于LLM的白盒模板提取器（带后处理区分常量变量）、黑盒模板提取器（结合数据驱动聚类处理未匹配日志）。

Result: 在两个公共基准测试（Hadoop和Zookeeper）和一个大规模工业系统（Sunfire-Compute）上的实验表明：相比两个基于LLM的基线方法，LLM-SrcLog将平均F1分数提高了2-17%和8-35%。在线解析延迟与数据驱动方法相当，比每条日志的LLM解析快约1000倍。

Conclusion: LLM-SrcLog在速度和准确性之间实现了近乎理想的平衡，并通过实际生产环境案例研究验证了其有效性。该框架为日志解析提供了主动、统一的解决方案。

Abstract: Log parsing transforms raw logs into structured templates containing constants and variables. It underpins anomaly detection, failure diagnosis, and other AIOps tasks. Current parsers are mostly reactive and log-centric. They only infer templates from logs, mostly overlooking the source code. This restricts their capacity to grasp dynamic log structures or adjust to evolving systems. Moreover, per-log LLM inference is too costly for practical deployment. In this paper, we propose LLM-SrcLog, a proactive and unified framework for log template parsing. It extracts templates directly from source code prior to deployment and supplements them with data-driven parsing for logs without available code. LLM-SrcLog integrates a cross-function static code analyzer to reconstruct meaningful logging contexts, an LLM-based white-box template extractor with post-processing to distinguish constants from variables, and a black-box template extractor that incorporates data-driven clustering for remaining unmatched logs. Experiments on two public benchmarks (Hadoop and Zookeeper) and a large-scale industrial system (Sunfire-Compute) show that, compared to two LLM-based baselines, LLM-SrcLog improves average F1-score by 2-17% and 8-35%. Meanwhile, its online parsing latency is comparable to data-driven methods and about 1,000 times faster than per-log LLM parsing. LLM-SrcLog achieves a near-ideal balance between speed and accuracy. Finally, we further validate the effectiveness of LLM-SrcLog through practical case studies in a real-world production environment.

</details>


### [11] [Completion by Comprehension: Guiding Code Generation with Multi-Granularity Understanding](https://arxiv.org/abs/2512.04538)
*Xinkui Zhao,Rongkai Liu,Yifan Zhang,Chen Zhi,Lufei Zhang,Guanjie Cheng,Yueshen Xu,Shuiguang Deng,Jianwei Yin*

Main category: cs.SE

TL;DR: CoCo是一个用于代码补全的新框架，通过理解大规模代码仓库中的多粒度上下文来提升代码生成质量，相比现有方法在EM指标上提升高达20.2%。


<details>
  <summary>Details</summary>
Motivation: 现有基于检索增强生成的方法通常将代码视为普通自然语言，主要依赖浅层语义匹配，忽视了代码的结构语义和特定依赖关系，这限制了它们捕捉控制流和底层意图的能力，从而制约了生成代码的质量。

Method: CoCo采用静态代码分析提取函数、文件和项目级别的结构化上下文，捕获执行逻辑和语义依赖；使用基于图的多粒度上下文选择机制过滤冗余信息和噪声；将信息转换为自然语言作为显式上下文提示；并采用结构感知的代码重排序机制确保语义和结构对齐。

Result: 在CrossCodeEval和RepoEval基准测试上的广泛实验表明，CoCo始终优于最先进的基线方法，在EM指标上实现了高达20.2%的提升。该框架与模型无关，可以无缝集成到现有方法中，带来显著的性能提升。

Conclusion: CoCo通过理解大规模代码仓库中的多粒度上下文，有效解决了代码补全任务中的核心挑战，显著提升了代码生成质量，为代码补全领域提供了新的解决方案。

Abstract: As code completion task from function-level to repository-level, leveraging contextual information from large-scale codebases becomes a core challenge. However, existing retrieval-augmented generation (RAG) methods typically treat code as plain natural language, relying primarily on shallow semantic matching while overlooking structural semantics and code-specific dependencies. This limits their ability to capture control flow and underlying intent, ultimately constraining the quality of generated code. Therefore, we propose CoCo, a novel framework that enables code Completion by Comprehension of multi-granularity context from large-scale code repositories. CoCo employs static code analysis to extract structured context at the function, file, and project levels, capturing execution logic and semantic dependencies. It then adopts an graph-based multi-granularity context selection mechanism to filter out redundant information and remove noise. Consequently, the information is converted into natural language in a consistent manner, thereby functioning as explicit contextual prompts to guide subsequent code completion. Additionally, a structure-aware code re-ranker mechanism ensures alignment at both semantic and structural levels. Extensive experiments on CrossCodeEval and RepoEval benchmarks demonstrate that CoCo consistently surpasses state-of-the-art baselines, achieving up to 20.2% gains in EM. Moreover, the framework is model-agnostic and can be seamlessly integrated into existing methods, leading to significant performance.

</details>


### [12] [Cross-Task Benchmarking and Evaluation of General-Purpose and Code-Specific Large Language Models](https://arxiv.org/abs/2512.04673)
*Gunjan Das,Paheli Bhattacharya,Rishabh Gupta*

Main category: cs.SE

TL;DR: 本文对通用和代码专用大语言模型在语言能力、数学推理、代码理解等跨领域任务上进行系统性评估比较


<details>
  <summary>Details</summary>
Motivation: 虽然已有研究探索了单个模型的能力，但缺乏对语言、推理和代码理解能力的系统性跨领域比较，需要全面评估不同模型在多样化任务上的表现

Method: 使用五个通用大语言模型和三个代码专用模型，在六个多样化基准测试（涵盖语言能力、数学推理和可信度）上进行评估，并在CoNaLa数据集上分析模型在代码解释任务上的表现

Result: 代码优化模型（如CodeLLaMA变体）展现出强大的推理和语法精确性，即使在非编码任务上也能显示出可测量的性能提升，而通用模型如Mistral-7B和Llama-3-8B表现相对较弱

Conclusion: 代码专用模型在跨领域任务中表现出色，其推理能力和精确性优势不仅限于代码任务，还能扩展到其他领域，为模型选择和优化提供了重要见解

Abstract: Large Language Models (LLMs) have revolutionized both general natural language processing and domain-specific applications such as code synthesis, legal reasoning, and finance. However, while prior studies have explored individual model capabilities, a systematic cross-domain comparison that unifies linguistic, reasoning, and code understanding abilities remains underexplored. In this work, we present a comprehensive evaluation of five general-purpose and three code-specific state-of-the-art LLMs across six diverse benchmarks encompassing linguistic competence, mathematical reasoning, and trustworthiness. Additionally, we analyze model behavior on the CoNaLa dataset for code explanation, comparing natural language and code-specialized LLMs. Our findings reveal that models optimized for code (e.g., CodeLLaMA variants) exhibit strong reasoning and syntactic precision, that even for non-coding tasks can show measurable performance gains, in contrast to general-purpose models like Mistral-7B and Llama-3-8B.

</details>


### [13] [Generative AI for Self-Adaptive Systems: State of the Art and Research Roadmap](https://arxiv.org/abs/2512.04680)
*Jialong Li,Mingyue Zhang,Nianyu Li,Danny Weyns,Zhi Jin,Kenji Tei*

Main category: cs.SE

TL;DR: 该论文探讨了在自适应性系统中应用生成式人工智能的潜在益处与挑战，通过分析四个研究领域的文献，提出了增强SAS自主性和人机交互的研究路线图。


<details>
  <summary>Details</summary>
Motivation: 生成式人工智能在数据理解和逻辑推理方面表现出色，这些能力与自适应性系统的监控、分析、规划、执行功能高度契合，但具体应用中的益处和挑战尚不明确，需要系统性的研究。

Method: 从四个不同的研究领域收集、筛选和分析文献，将其组织为两个主要类别：基于MAPE-K反馈循环的SAS自主性增强，以及人机交互在人在回路设置中的改进。

Result: 提出了一个研究路线图，明确了将生成式人工智能集成到自适应性系统中的关键挑战，包括需要解决的研究问题和当前生成式人工智能的局限性。

Conclusion: 生成式人工智能在增强自适应性系统方面具有巨大潜力，但需要系统性地解决技术集成挑战，论文为研究者和实践者提供了全面的应用前景分析和实用缓解策略建议。

Abstract: Self-adaptive systems (SASs) are designed to handle changes and uncertainties through a feedback loop with four core functionalities: monitoring, analyzing, planning, and execution. Recently, generative artificial intelligence (GenAI), especially the area of large language models, has shown impressive performance in data comprehension and logical reasoning. These capabilities are highly aligned with the functionalities required in SASs, suggesting a strong potential to employ GenAI to enhance SASs. However, the specific benefits and challenges of employing GenAI in SASs remain unclear. Yet, providing a comprehensive understanding of these benefits and challenges is complex due to several reasons: limited publications in the SAS field, the technological and application diversity within SASs, and the rapid evolution of GenAI technologies. To that end, this paper aims to provide researchers and practitioners a comprehensive snapshot that outlines the potential benefits and challenges of employing GenAI's within SAS. Specifically, we gather, filter, and analyze literature from four distinct research fields and organize them into two main categories to potential benefits: (i) enhancements to the autonomy of SASs centered around the specific functions of the MAPE-K feedback loop, and (ii) improvements in the interaction between humans and SASs within human-on-the-loop settings. From our study, we outline a research roadmap that highlights the challenges of integrating GenAI into SASs. The roadmap starts with outlining key research challenges that need to be tackled to exploit the potential for applying GenAI in the field of SAS. The roadmap concludes with a practical reflection, elaborating on current shortcomings of GenAI and proposing possible mitigation strategies.

</details>


### [14] [POLARIS: Is Multi-Agentic Reasoning the Next Wave in Engineering Self-Adaptive Systems?](https://arxiv.org/abs/2512.04702)
*Divyansh Pandey,Vyakhya Gupta,Prakhar Singhal,Karthik Vaidhyanathan*

Main category: cs.SE

TL;DR: POLARIS是一个三层多智能体自适应框架，超越传统反应式自适应，通过整合监控执行、透明推理和元学习层，实现预测性、主动性的系统自适应。


<details>
  <summary>Details</summary>
Motivation: 现代软件生态系统的规模、复杂性、互连性和自主性不断增加，引入了前所未有的不确定性，挑战了传统自适应系统的基础。现有方法（通常是规则驱动控制器或孤立学习组件）难以泛化到新环境或协调分布式子系统，无法应对"未知的未知"问题。

Method: 提出POLARIS三层多智能体自适应框架：1）低延迟适配层用于监控和安全执行；2）透明推理层使用工具感知、可解释的智能体生成和验证计划；3）元层记录经验并通过元学习改进自适应策略。通过共享知识和预测模型，POLARIS处理不确定性，从过去行动中学习，并进化策略。

Result: 在两个自适应示例系统SWIM和SWITCH上的初步评估显示，POLARIS始终优于最先进的基线方法。

Conclusion: POLARIS标志着向"自适应3.0"的转变，类似于"软件3.0"范式：系统不仅从环境中学习，还能推理和进化自身的自适应过程，持续改进以应对新挑战。

Abstract: The growing scale, complexity, interconnectivity, and autonomy of modern software ecosystems introduce unprecedented uncertainty, challenging the foundations of traditional self-adaptation. Existing approaches, typically rule-driven controllers or isolated learning components, struggle to generalize to novel contexts or coordinate responses across distributed subsystems, leaving them ill-equipped for emergent unknown unknowns. Recent discussions on Self-Adaptation 2.0 emphasize an equal partnership between AI and adaptive systems, merging learning-driven intelligence with adaptive control for predictive and proactive behavior. Building on this foundation, we introduce POLARIS, a three-layer multi-agentic self-adaptation framework that advances beyond reactive adaptation. POLARIS integrates: (1) a low-latency Adapter layer for monitoring and safe execution, (2) a transparent Reasoning layer that generates and verifies plans using tool-aware, explainable agents, and (3) a Meta layer that records experiences and meta-learns improved adaptation policies over time. Through shared knowledge and predictive models, POLARIS handles uncertainty, learns from past actions, and evolves its strategies, enabling systems that anticipate change and maintain resilient, goal-directed behavior. Preliminary evaluation on two self-adaptive exemplars, SWIM and SWITCH, shows that POLARIS consistently outperforms state-of-the-art baselines. We argue this marks a shift toward Self-Adaptation 3.0, akin to Software 3.0: a paradigm where systems not only learn from their environment but also reason about and evolve their own adaptation processes, continuously improving to meet novel challenges.

</details>


### [15] [Configuration Defects in Kubernetes](https://arxiv.org/abs/2512.05062)
*Yue Zhang,Uchswas Paul,Marcelo d'Amorim,Akond Rahman*

Main category: cs.SE

TL;DR: 该研究对Kubernetes配置缺陷进行了实证分析，从2260个配置脚本中提取了719个缺陷，识别出15类缺陷，评估了现有静态分析工具的检测能力，并开发了一个新的linter来检测现有工具无法发现的严重缺陷。


<details>
  <summary>Details</summary>
Motivation: Kubernetes作为快速部署软件的工具，其配置过程容易出错，配置缺陷可能导致严重后果。目前缺乏对Kubernetes配置缺陷的系统性研究，需要帮助实践者检测和预防这些缺陷。

Method: 从开源仓库中提取2260个Kubernetes配置脚本，识别出719个配置缺陷。通过定性分析将缺陷分为15个类别。评估了8个公开可用的静态分析工具对这些缺陷的检测能力。针对现有工具无法检测的两类严重缺陷，开发了一个专门的linter工具。

Result: 研究发现现有工具只能检测15类缺陷中的8类，其中对数据字段相关缺陷的检测精度和召回率最高。开发的linter发现了26个先前未知的缺陷，其中19个已被修复。所有数据集和源代码都已公开。

Conclusion: 研究提供了关于如何在Kubernetes配置脚本中使用缺陷检测和修复技术的建议。开发的新linter能够有效检测现有工具无法发现的严重缺陷，为Kubernetes配置质量保障提供了实用工具。

Abstract: Kubernetes is a tool that facilitates rapid deployment of software. Unfortunately, configuring Kubernetes is prone to errors. Configuration defects are not uncommon and can result in serious consequences. This paper reports an empirical study about configuration defects in Kubernetes with the goal of helping practitioners detect and prevent these defects. We study 719 defects that we extract from 2,260 Kubernetes configuration scripts using open source repositories. Using qualitative analysis, we identify 15 categories of defects. We find 8 publicly available static analysis tools to be capable of detecting 8 of the 15 defect categories. We find that the highest precision and recall of those tools are for defects related to data fields. We develop a linter to detect two categories of defects that cause serious consequences, which none of the studied tools are able to detect. Our linter revealed 26 previously-unknown defects that have been confirmed by practitioners, 19 of which have already been fixed. We conclude our paper by providing recommendations on how defect detection and repair techniques can be used for Kubernetes configuration scripts. The datasets and source code used for the paper are publicly available online.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [16] [Solving N-Queen Problem using Las Vegas Algorithm with State Pruning](https://arxiv.org/abs/2512.04139)
*Susmita Sharma,Aayush Shrestha,Sitasma Thapa,Prashant Timalsina,Prakash Poudyal*

Main category: cs.AI

TL;DR: 本文提出了一种基于Las Vegas算法的混合方法，通过迭代剪枝动态消除无效放置，显著减少搜索空间，在N皇后问题上比传统回溯法更快获得有效解。


<details>
  <summary>Details</summary>
Motivation: N皇后问题作为约束满足问题的经典案例，传统回溯法虽然能保证找到解但时间复杂度指数级增长，不适用于大规模实例。Las Vegas算法虽然能提供快速近似解，但随机放置皇后导致性能波动较大。

Method: 在标准Las Vegas算法框架基础上引入迭代剪枝技术，在随机分配阶段动态消除无效的皇后放置位置，有效减少搜索空间。

Result: 传统回溯法随N增大性能急剧下降，而提出的混合算法能更快地生成有效解，在需要及时获得单个解而非完整解的场景中表现更优。虽然大N时仍有一定性能波动，但在计算成本和解质量之间取得了良好平衡。

Conclusion: 该混合算法在资源受限的计算环境中特别适用，为N皇后问题提供了比传统方法更高效的解决方案，在计算成本和解质量之间实现了有效权衡。

Abstract: The N-Queens problem, placing all N queens in a N x N chessboard where none attack the other, is a classic problem for constraint satisfaction algorithms. While complete methods like backtracking guarantee a solution, their exponential time complexity makes them impractical for large-scale instances thus, stochastic approaches, such as Las Vegas algorithm, are preferred. While it offers faster approximate solutions, it suffers from significant performance variance due to random placement of queens on the board. This research introduces a hybrid algorithm built on top of the standard Las Vegas framework through iterative pruning, dynamically eliminating invalid placements during the random assignment phase, thus this method effectively reduces the search space. The analysis results that traditional backtracking scales poorly with increasing N. In contrast, the proposed technique consistently generates valid solutions more rapidly, establishing it as a superior alternative to use where a single, timely solution is preferred over completeness. Although large N causes some performance variability, the algorithm demonstrates a highly effective trade-off between computational cost and solution fidelity, making it particularly suited for resource-constrained computing environments.

</details>


### [17] [RippleBench: Capturing Ripple Effects Using Existing Knowledge Repositories](https://arxiv.org/abs/2512.04144)
*Roy Rinberg,Usha Bhalla,Igor Shilov,Flavio P. Calmon,Rohit Gandikota*

Main category: cs.AI

TL;DR: 本文提出了RippleBench-Maker工具，用于自动生成问答数据集来测量模型编辑任务中的涟漪效应（副作用传播），并基于WMDP数据集构建了RippleBench-Bio基准，评估了8种最先进的遗忘方法。


<details>
  <summary>Details</summary>
Motivation: 语言模型的目标干预（如遗忘、去偏、模型编辑）是优化模型行为的重要方法，但这些干预往往会产生涟漪效应——对相关但非目标领域产生意外影响。目前缺乏系统测量这种副作用传播的工具。

Method: 开发了RippleBench-Maker工具，基于Wikipedia的RAG管道（WikiRAG）生成与目标概念（如被遗忘知识）在不同语义距离上的多项选择题。使用该框架从WMDP数据集构建了RippleBench-Bio基准。

Result: 评估了8种最先进的遗忘方法，发现所有方法在距离被遗忘知识越来越远的话题上都表现出显著准确率下降，每种方法都有不同的传播特征。

Conclusion: RippleBench-Maker为模型编辑任务中的涟漪效应测量提供了自动化工具，RippleBench-Bio基准揭示了遗忘方法的副作用传播模式，相关代码和基准已开源以支持持续研究。

Abstract: Targeted interventions on language models, such as unlearning, debiasing, or model editing, are a central method for refining model behavior and keeping knowledge up to date. While these interventions aim to modify specific information within models (e.g., removing virology content), their effects often propagate to related but unintended areas (e.g., allergies); these side-effects are commonly referred to as the ripple effect. In this work, we present RippleBench-Maker, an automatic tool for generating Q&A datasets that allow for the measurement of ripple effects in any model-editing task. RippleBench-Maker builds on a Wikipedia-based RAG pipeline (WikiRAG) to generate multiple-choice questions at varying semantic distances from the target concept (e.g., the knowledge being unlearned). Using this framework, we construct RippleBench-Bio, a benchmark derived from the WMDP (Weapons of Mass Destruction Paper) dataset, a common unlearning benchmark. We evaluate eight state-of-the-art unlearning methods and find that all exhibit non-trivial accuracy drops on topics increasingly distant from the unlearned knowledge, each with distinct propagation profiles. To support ongoing research, we release our codebase for on-the-fly ripple evaluation, along with the benchmark, RippleBench-Bio.

</details>


### [18] [Orchestrator Multi-Agent Clinical Decision Support System for Secondary Headache Diagnosis in Primary Care](https://arxiv.org/abs/2512.04207)
*Xizhi Wu,Nelly Estefanie Garduno-Rapp,Justin F Rousseau,Mounika Thakkallapally,Hang Zhang,Yuelyu Ji,Shyam Visweswaran,Yifan Peng,Yanshan Wang*

Main category: cs.AI

TL;DR: 本文提出了一种基于大语言模型的多智能体临床决策支持系统，采用协调器-专家架构，用于从自由文本临床病例中进行明确且可解释的继发性头痛诊断。


<details>
  <summary>Details</summary>
Motivation: 继发性头痛需要专业护理，不及时治疗可能造成严重后果。尽管有临床指南指出"危险信号"特征，但在初级保健环境中，确定哪些患者需要紧急评估仍然具有挑战性。临床医生面临时间有限、信息不完整和症状表现多样等问题，可能导致识别不足和不适当的护理。

Method: 开发了基于大语言模型的多智能体临床决策支持系统，采用协调器-专家架构。系统将诊断分解为七个领域专业智能体，每个智能体生成结构化的、基于证据的推理，而中央协调器执行任务分解和智能体路由协调。使用90个专家验证的继发性头痛病例进行评估，并与单LLM基线比较两种提示策略：基于问题的提示(QPrompt)和基于临床实践指南的提示(GPrompt)。测试了五个开源LLM。

Result: 使用GPrompt的协调多智能体系统始终获得最高的F1分数，在较小模型中增益更大。结果表明，结构化多智能体推理比单纯提示工程提高了准确性，为继发性头痛诊断提供了透明、临床对齐的可解释决策支持方法。

Conclusion: 结构化多智能体推理超越了单纯提示工程的准确性，为继发性头痛诊断提供了透明、临床对齐的可解释决策支持方法。这种架构在较小模型中表现出更大的性能提升，展示了多智能体系统在复杂临床决策中的价值。

Abstract: Unlike most primary headaches, secondary headaches need specialized care and can have devastating consequences if not treated promptly. Clinical guidelines highlight several 'red flag' features, such as thunderclap onset, meningismus, papilledema, focal neurologic deficits, signs of temporal arteritis, systemic illness, and the 'worst headache of their life' presentation. Despite these guidelines, determining which patients require urgent evaluation remains challenging in primary care settings. Clinicians often work with limited time, incomplete information, and diverse symptom presentations, which can lead to under-recognition and inappropriate care. We present a large language model (LLM)-based multi-agent clinical decision support system built on an orchestrator-specialist architecture, designed to perform explicit and interpretable secondary headache diagnosis from free-text clinical vignettes. The multi-agent system decomposes diagnosis into seven domain-specialized agents, each producing a structured and evidence-grounded rationale, while a central orchestrator performs task decomposition and coordinates agent routing. We evaluated the multi-agent system using 90 expert-validated secondary headache cases and compared its performance with a single-LLM baseline across two prompting strategies: question-based prompting (QPrompt) and clinical practice guideline-based prompting (GPrompt). We tested five open-source LLMs (Qwen-30B, GPT-OSS-20B, Qwen-14B, Qwen-8B, and Llama-3.1-8B), and found that the orchestrated multi-agent system with GPrompt consistently achieved the highest F1 scores, with larger gains in smaller models. These findings demonstrate that structured multi-agent reasoning improves accuracy beyond prompt engineering alone and offers a transparent, clinically aligned approach for explainable decision support in secondary headache diagnosis.

</details>


### [19] [Balancing Safety and Helpfulness in Healthcare AI Assistants through Iterative Preference Alignment](https://arxiv.org/abs/2512.04210)
*Huy Nghiem,Swetasudha Panda,Devashish Khatwani,Huy V. Nguyen,Krishnaram Kenthapadi,Hal Daumé*

Main category: cs.AI

TL;DR: 本文提出了一种迭代后部署对齐框架，结合KTO和DPO优化医疗LLM的安全性，在CARES-18K基准上测试多个模型，显示安全指标提升42%，同时揭示了架构相关的校准偏差。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在医疗领域应用日益广泛，但确保其安全性和可信度仍是部署的主要障碍。医疗对话助手需要在避免不安全合规的同时，不过度拒绝良性查询。

Method: 提出迭代后部署对齐框架，应用Kahneman-Tversky优化和直接偏好优化，针对特定领域的安全信号进行模型精炼。使用CARES-18K基准评估对抗鲁棒性，测试了四个LLM模型（Llama-3B/8B、Meditron-8B、Mistral-7B）的多个迭代周期。

Result: 结果显示有害查询检测的安全相关指标提升高达42%，同时揭示了与错误拒绝之间的权衡关系，暴露了架构依赖的校准偏差。消融研究确定了何时自我评估可靠，何时需要外部或微调评估器来最大化性能增益。

Conclusion: 研究结果强调了在设计医疗对话助手时，采用平衡患者安全、用户信任和临床效用的最佳实践的重要性。

Abstract: Large Language Models (LLMs) are increasingly used in healthcare, yet ensuring their safety and trustworthiness remains a barrier to deployment. Conversational medical assistants must avoid unsafe compliance without over-refusing benign queries. We present an iterative post-deployment alignment framework that applies Kahneman-Tversky Optimization (KTO) and Direct Preference Optimization (DPO) to refine models against domain-specific safety signals. Using the CARES-18K benchmark for adversarial robustness, we evaluate four LLMs (Llama-3B/8B, Meditron-8B, Mistral-7B) across multiple cycles. Our results show up to 42% improvement in safety-related metrics for harmful query detection, alongside interesting trade-offs against erroneous refusals, thereby exposing architecture-dependent calibration biases. We also perform ablation studies to identify when self-evaluation is reliable and when external or finetuned judges are necessary to maximize performance gains. Our findings underscore the importance of adopting best practices that balance patient safety, user trust, and clinical utility in the design of conversational medical assistants.

</details>


### [20] [Educational Cone Model in Embedding Vector Spaces](https://arxiv.org/abs/2512.04227)
*Yo Ehara*

Main category: cs.AI

TL;DR: 提出教育锥体模型，通过几何框架评估不同嵌入方法在教育文本难度分析中的适用性，基于"简单文本多样性低、困难文本多样性高"的假设，在嵌入空间中形成锥形分布。


<details>
  <summary>Details</summary>
Motivation: 教育智能系统需要带明确难度标注的数据集，虽然嵌入向量空间能表示语义接近性并有望分析文本难度，但众多嵌入方法的选择成为挑战，需要评估哪种方法最适合教育文本难度分析。

Method: 提出教育锥体模型，基于"简单文本更聚焦基础概念（多样性低）、困难文本更广泛（多样性高）"的假设，在嵌入空间中形成锥形分布。将嵌入评估构建为优化问题，设计特定损失函数，推导出高效的闭式解以避免昂贵计算。

Result: 在真实世界数据集上的实证测试验证了模型的有效性和速度，能够识别与难度标注教育文本最匹配的嵌入空间。

Conclusion: 教育锥体模型为评估嵌入方法在教育文本难度分析中的适用性提供了有效的几何框架，通过优化方法快速识别最佳嵌入空间，解决了多种嵌入方法选择的挑战。

Abstract: Human-annotated datasets with explicit difficulty ratings are essential in intelligent educational systems. Although embedding vector spaces are widely used to represent semantic closeness and are promising for analyzing text difficulty, the abundance of embedding methods creates a challenge in selecting the most suitable method. This study proposes the Educational Cone Model, which is a geometric framework based on the assumption that easier texts are less diverse (focusing on fundamental concepts), whereas harder texts are more diverse. This assumption leads to a cone-shaped distribution in the embedding space regardless of the embedding method used. The model frames the evaluation of embeddings as an optimization problem with the aim of detecting structured difficulty-based patterns. By designing specific loss functions, efficient closed-form solutions are derived that avoid costly computation. Empirical tests on real-world datasets validated the model's effectiveness and speed in identifying the embedding spaces that are best aligned with difficulty-annotated educational texts.

</details>


### [21] [Addressing Logical Fallacies In Scientific Reasoning From Large Language Models: Towards a Dual-Inference Training Framework](https://arxiv.org/abs/2512.04228)
*Peter B. Walker,Hannah Davidson,Aiden Foster,Matthew Lienert,Thomas Pardue,Dale Russell*

Main category: cs.AI

TL;DR: 该论文提出了一种双推理训练框架，将肯定性生成与结构化反事实否定相结合，以解决LLMs在科学推理中面对否定、反例或错误前提时的系统性弱点。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型主要基于肯定性推理（类似肯定前件式），虽然在生成流畅性上有效，但容易受到逻辑谬误、对抗性操纵和因果推理失败的影响。特别是在科学领域，模型面对否定、反例或错误前提时表现出系统性弱点。

Method: 论文提出双推理训练框架，整合肯定性生成与结构化反事实否定。该框架基于形式逻辑、认知科学和对抗训练，形式化了"否定前件"的计算模拟，作为证伪和鲁棒性的机制。通过将生成合成与显式的否定感知目标相结合，使模型既能肯定有效推理，也能拒绝无效推理。

Result: 论文展示了现有主流平台LLMs在科学领域推理中的系统性弱点，特别是在处理否定、反例或错误前提时。通过提出的双推理训练框架，能够产生更具韧性、可解释性且与人类推理更一致的模型系统。

Conclusion: 双推理训练框架通过整合肯定性推理和结构化否定推理，能够显著提升LLMs的逻辑鲁棒性、对抗性防御能力和科学推理能力，使模型更符合人类推理模式，在科学、医疗和决策领域具有重要应用价值。

Abstract: Large Language Models (LLMs) have transformed natural language processing and hold growing promise for advancing science, healthcare, and decision-making. Yet their training paradigms remain dominated by affirmation-based inference, akin to \textit{modus ponens}, where accepted premises yield predicted consequents. While effective for generative fluency, this one-directional approach leaves models vulnerable to logical fallacies, adversarial manipulation, and failures in causal reasoning. This paper makes two contributions. First, it demonstrates how existing LLMs from major platforms exhibit systematic weaknesses when reasoning in scientific domains with negation, counterexamples, or faulty premises \footnote{Code to recreate these experiments are at https://github.com/hannahdavidsoncollege-maker/ScientificReasoningForEnvironment-MedicineWithLLMs. Second, it introduces a dual-reasoning training framework that integrates affirmative generation with structured counterfactual denial. Grounded in formal logic, cognitive science, and adversarial training, this training paradigm formalizes a computational analogue of ``denying the antecedent'' as a mechanism for disconfirmation and robustness. By coupling generative synthesis with explicit negation-aware objectives, the framework enables models that not only affirm valid inferences but also reject invalid ones, yielding systems that are more resilient, interpretable, and aligned with human reasoning.

</details>


### [22] [The Geometry of Benchmarks: A New Path Toward AGI](https://arxiv.org/abs/2512.04276)
*Przemyslaw Chojecki*

Main category: cs.AI

TL;DR: 论文提出了一个几何框架，将AI基准测试视为模空间中的点，定义了自主AI等级和自改进系数，为评估AI通用性和自主进化提供了新方法。


<details>
  <summary>Details</summary>
Motivation: 当前AI基准测试实践存在局限性：孤立评估模型，无法指导通用性推理或自主自改进。需要新的理论框架来理解AI进展的本质。

Method: 1. 构建基准测试模空间，将心理测量电池视为空间中的点；2. 定义自主AI等级（AAI Scale），类似卡尔达肖夫等级；3. 引入生成器-验证器-更新器（GVU）算子，统一强化学习、自我博弈等方法；4. 定义自改进系数κ作为能力泛函沿GVU流的李导数。

Result: 1. 建立了基准测试的几何框架，识别了基准测试的等价类；2. 证明了确定性结果：密集的基准测试族足以认证整个任务空间的性能；3. 推导了生成和验证噪声的方差不等式，为κ>0提供了充分条件。

Conclusion: 通向通用人工智能的进展应被理解为基准测试模空间上的流，由GVU动力学驱动，而非单个排行榜分数。该框架为评估AI通用性和自主进化提供了理论基础。

Abstract: Benchmarks are the primary tool for assessing progress in artificial intelligence (AI), yet current practice evaluates models on isolated test suites and provides little guidance for reasoning about generality or autonomous self-improvement. Here we introduce a geometric framework in which all psychometric batteries for AI agents are treated as points in a structured moduli space, and agent performance is described by capability functionals over this space. First, we define an Autonomous AI (AAI) Scale, a Kardashev-style hierarchy of autonomy grounded in measurable performance on batteries spanning families of tasks (for example reasoning, planning, tool use and long-horizon control). Second, we construct a moduli space of batteries, identifying equivalence classes of benchmarks that are indistinguishable at the level of agent orderings and capability inferences. This geometry yields determinacy results: dense families of batteries suffice to certify performance on entire regions of task space. Third, we introduce a general Generator-Verifier-Updater (GVU) operator that subsumes reinforcement learning, self-play, debate and verifier-based fine-tuning as special cases, and we define a self-improvement coefficient $κ$ as the Lie derivative of a capability functional along the induced flow. A variance inequality on the combined noise of generation and verification provides sufficient conditions for $κ> 0$. Our results suggest that progress toward artificial general intelligence (AGI) is best understood as a flow on moduli of benchmarks, driven by GVU dynamics rather than by scores on individual leaderboards.

</details>


### [23] [Artificial Intelligence Applications in Horizon Scanning for Infectious Diseases](https://arxiv.org/abs/2512.04287)
*Ian Miles,Mayumi Wakimoto,Wagner Meira,Daniela Paula,Daylene Ticiane,Bruno Rosa,Jane Biddulph,Stelios Georgiou,Valdir Ermida*

Main category: cs.AI

TL;DR: AI在传染病预警扫描中的应用：增强信号检测、数据监控、情景分析和决策支持，同时关注AI采用风险并提出实施治理策略


<details>
  <summary>Details</summary>
Motivation: 探索人工智能如何整合到传染病预警扫描中，以更好地识别和应对新出现的威胁和机遇，为公共卫生准备提供支持

Method: 文献综述方法，系统性地分析AI工具在信号检测、数据监控、情景分析和决策支持等方面的应用

Result: AI能够显著增强传染病预警扫描能力，但也存在采用风险，需要有效的实施和治理策略

Conclusion: AI在公共卫生预警扫描中具有重要潜力，但需平衡其优势与风险，为前瞻性文献贡献了关于AI在公共卫生准备中的潜力和局限性的见解

Abstract: This review explores the integration of Artificial Intelligence into Horizon Scanning, focusing on identifying and responding to emerging threats and opportunities linked to Infectious Diseases. We examine how AI tools can enhance signal detection, data monitoring, scenario analysis, and decision support. We also address the risks associated with AI adoption and propose strategies for effective implementation and governance. The findings contribute to the growing body of Foresight literature by demonstrating the potential and limitations of AI in Public Health preparedness.

</details>


### [24] [Towards better dense rewards in Reinforcement Learning Applications](https://arxiv.org/abs/2512.04302)
*Shuyuan Zhang*

Main category: cs.AI

TL;DR: 该论文探讨了强化学习中稠密奖励函数的设计问题，分析了稀疏奖励的局限性，并提出了多种改进稠密奖励构建的方法。


<details>
  <summary>Details</summary>
Motivation: 传统强化学习中，当奖励信号稀疏、延迟或与任务目标不匹配时，智能体学习效率低下。稠密奖励虽然能提供更丰富的反馈，但设计不当会导致意外行为、奖励黑客攻击或探索效率低下，特别是在复杂高维环境中。

Method: 论文探讨了多种方法：逆强化学习、基于人类偏好的奖励建模、自监督学习内在奖励等，旨在解决稠密奖励构建中的未解决问题。

Result: 虽然这些方法提供了有前景的方向，但在通用性、可扩展性和与人类意图对齐方面存在权衡。

Conclusion: 需要进一步研究来增强不同强化学习应用中稠密奖励构建的有效性和可靠性。

Abstract: Finding meaningful and accurate dense rewards is a fundamental task in the field of reinforcement learning (RL) that enables agents to explore environments more efficiently. In traditional RL settings, agents learn optimal policies through interactions with an environment guided by reward signals. However, when these signals are sparse, delayed, or poorly aligned with the intended task objectives, agents often struggle to learn effectively. Dense reward functions, which provide informative feedback at every step or state transition, offer a potential solution by shaping agent behavior and accelerating learning. Despite their benefits, poorly crafted reward functions can lead to unintended behaviors, reward hacking, or inefficient exploration. This problem is particularly acute in complex or high-dimensional environments where handcrafted rewards are difficult to specify and validate. To address this, recent research has explored a variety of approaches, including inverse reinforcement learning, reward modeling from human preferences, and self-supervised learning of intrinsic rewards. While these methods offer promising directions, they often involve trade-offs between generality, scalability, and alignment with human intent. This proposal explores several approaches to dealing with these unsolved problems and enhancing the effectiveness and reliability of dense reward construction in different RL applications.

</details>


### [25] [Efficient Reinforcement Learning with Semantic and Token Entropy for LLM Reasoning](https://arxiv.org/abs/2512.04359)
*Hongye Cao,Zhixin Bai,Ziyue Peng,Boyan Wang,Tianpei Yang,Jing Huo,Yuyao Zhang,Yang Gao*

Main category: cs.AI

TL;DR: 本文提出了一种利用语义和标记级熵信号来增强大语言模型推理能力的强化学习框架，通过数据层面的语义熵引导课程学习和算法层面的非均匀标记处理，有效缓解熵崩溃问题。


<details>
  <summary>Details</summary>
Motivation: 尽管基于可验证奖励的强化学习（RLVR）在提升大语言模型推理能力方面表现出色，但这种以准确性为导向的学习范式常常面临熵崩溃问题，导致策略探索减少并限制推理能力的发展。

Method: 方法包含两个核心部分：1）数据层面：引入语义熵引导的课程学习，将训练数据按语义熵从低到高组织，引导模型从简单任务逐步优化到复杂任务；2）算法层面：采用非均匀标记处理，对影响策略探索的低熵标记施加KL正则化，并在这些标记的高协方差部分施加更强约束。

Result: 在6个基准测试和3种不同参数规模的基础模型上的实验结果表明，该方法在提升推理能力方面优于其他基于熵的方法。

Conclusion: 通过联合优化数据组织和算法设计，该方法有效缓解了熵崩溃问题，显著增强了大语言模型的推理能力，为强化学习在语言模型推理优化方面提供了新思路。

Abstract: Reinforcement learning with verifiable rewards (RLVR) has demonstrated superior performance in enhancing the reasoning capability of large language models (LLMs). However, this accuracy-oriented learning paradigm often suffers from entropy collapse, which reduces policy exploration and limits reasoning capabilities. To address this challenge, we propose an efficient reinforcement learning framework that leverages entropy signals at both the semantic and token levels to improve reasoning. From the data perspective, we introduce semantic entropy-guided curriculum learning, organizing training data from low to high semantic entropy to guide progressive optimization from easier to more challenging tasks. For the algorithmic design, we adopt non-uniform token treatment by imposing KL regularization on low-entropy tokens that critically impact policy exploration and applying stronger constraints on high-covariance portions within these tokens. By jointly optimizing data organization and algorithmic design, our method effectively mitigates entropy collapse and enhances LLM reasoning. Experimental results across 6 benchmarks with 3 different parameter-scale base models demonstrate that our method outperforms other entropy-based approaches in improving reasoning.

</details>


### [26] [GovBench: Benchmarking LLM Agents for Real-World Data Governance Workflows](https://arxiv.org/abs/2512.04416)
*Zhou Liu,Zhaoyang Han,Guochen Yan,Hao Liang,Bohan Zeng,Xing Chen,Yuanfeng Song,Wentao Zhang*

Main category: cs.AI

TL;DR: 本文提出了GovBench基准测试和DataGovAgent框架，用于评估和改进LLM在数据治理任务中的自动化能力，解决了现有基准无法评估数据质量和正确性的问题。


<details>
  <summary>Details</summary>
Motivation: 当前LLM在自动化数据治理方面具有潜力，但现有基准测试主要关注代码片段或高级分析，无法评估数据质量和正确性这一数据治理的核心挑战。

Method: 1) 提出GovBench基准：包含150个基于真实场景的任务，采用"反向目标"方法合成真实噪声；2) 提出DataGovAgent框架：采用Planner-Executor-Evaluator架构，集成约束规划、检索增强生成和沙盒反馈驱动调试。

Result: DataGovAgent在复杂任务上的平均任务分数从39.7提升到54.9，调试迭代次数相比通用基线减少了77.9%以上。

Conclusion: DataGovAgent框架显著提升了LLM在数据治理任务中的性能，为解决数据治理自动化中的复杂多步骤工作流程和错误修正机制不足的问题提供了有效方案。

Abstract: Data governance ensures data quality, security, and compliance through policies and standards, a critical foundation for scaling modern AI development. Recently, large language models (LLMs) have emerged as a promising solution for automating data governance by translating user intent into executable transformation code. However, existing benchmarks for automated data science often emphasize snippet-level coding or high-level analytics, failing to capture the unique challenge of data governance: ensuring the correctness and quality of the data itself. To bridge this gap, we introduce GovBench, a benchmark featuring 150 diverse tasks grounded in real-world scenarios, built on data from actual cases. GovBench employs a novel "reversed-objective" methodology to synthesize realistic noise and utilizes rigorous metrics to assess end-to-end pipeline reliability. Our analysis on GovBench reveals that current models struggle with complex, multi-step workflows and lack robust error-correction mechanisms. Consequently, we propose DataGovAgent, a framework utilizing a Planner-Executor-Evaluator architecture that integrates constraint-based planning, retrieval-augmented generation, and sandboxed feedback-driven debugging. Experimental results show that DataGovAgent significantly boosts the Average Task Score (ATS) on complex tasks from 39.7 to 54.9 and reduces debugging iterations by over 77.9 percent compared to general-purpose baselines.

</details>


### [27] [Solving LLM Repetition Problem in Production: A Comprehensive Study of Multiple Solutions](https://arxiv.org/abs/2512.04419)
*Weiwei Wang,Weijie Zou,Jiyong Min*

Main category: cs.AI

TL;DR: 论文研究了LLM在批量代码解释任务中的重复问题，识别了三种重复模式，通过马尔可夫模型分析根源，并提出了三种解决方案：Beam Search解码、presence_penalty参数和DPO微调。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在生产部署中出现的重复问题会导致严重的性能下降和系统停滞，特别是在批量代码解释任务中，这已成为实际应用中的关键挑战。

Method: 基于马尔可夫模型进行理论分析，识别三种重复模式：业务规则生成重复、方法调用关系分析重复和PlantUML图语法生成重复。通过实验评估三种解决方案：Beam Search解码（early_stopping=True）、presence_penalty超参数和直接偏好优化（DPO）微调。

Result: 实验表明：Beam Search解码能有效解决所有三种重复模式；presence_penalty参数专门解决第一种重复模式；DPO微调为所有三种重复模式提供模型级解决方案。early_stopping参数被确定为Beam Search有效的关键因素。

Conclusion: 该研究结合生产经验和实验验证，提供了对重复机制的系统理论分析、多种解决方案的全面评估以及在实际部署环境中验证的生产就绪解决方案，为解决LLM重复问题提供了实用指导。

Abstract: The repetition problem, where Large Language Models (LLMs) continuously generate repetitive content without proper termination, poses a critical challenge in production deployments, causing severe performance degradation and system stalling. This paper presents a comprehensive investigation and multiple practical solutions for the repetition problem encountered in real-world batch code interpretation tasks.
  We identify three distinct repetition patterns: (1) business rule generation repetition, (2) method call relationship analysis repetition, and (3) PlantUML diagram syntax generation repetition. Through rigorous theoretical analysis based on Markov models, we establish that the root cause lies in greedy decoding's inability to escape repetitive loops, exacerbated by self-reinforcement effects.
  Our comprehensive experimental evaluation demonstrates three viable solutions: (1) Beam Search decoding with early_stopping=True serves as a universal post-hoc mechanism that effectively resolves all three repetition patterns; (2) presence_penalty hyperparameter provides an effective solution specifically for BadCase 1; and (3) Direct Preference Optimization (DPO) fine-tuning offers a universal model-level solution for all three BadCases.
  The primary value of this work lies in combining first-hand production experience with extensive experimental validation. Our main contributions include systematic theoretical analysis of repetition mechanisms, comprehensive evaluation of multiple solutions with task-specific applicability mapping, identification of early_stopping as the critical parameter for Beam Search effectiveness, and practical production-ready solutions validated in real deployment environments.

</details>


### [28] [TaskEval: Synthesised Evaluation for Foundation-Model Tasks](https://arxiv.org/abs/2512.04442)
*Dilani Widanapathiranage,Scott Barnett,Stefanus Kurniawan,Wannita Takerngsaksiri*

Main category: cs.AI

TL;DR: 本文提出了一种合成特定任务评估器的方法，用于解决基础模型应用中幻觉检测的评估难题，通过任务无关元模型、交互协议和评估合成器实现自动化评估和人工反馈集成。


<details>
  <summary>Details</summary>
Motivation: 基础模型应用中的幻觉问题是关键挑战，现有评估方法要么关注新评估方法，要么关注特定任务的基准数据集，但都无法帮助软件团队在没有现成指标或数据集的情况下评估特定任务的基础模型应用。需要自动化方法和深度集成人工洞察的解决方案。

Method: 提出一个合成特定任务评估器的方法，包含三个核心创新：(1) 任务无关元模型，捕捉任何基础模型任务的属性；(2) 高效利用人工反馈的交互协议；(3) 评估合成器，选择或生成适当的评估集。该方法在工具中实现，并在图表数据提取和文档问答两个任务上验证。

Result: 在图表数据提取和文档问答两个任务上的初步评估显示，所选评估的准确率分别达到93%和90%，证明了该方法在评估基础模型任务输出质量方面的有效性。

Conclusion: 该研究解决了工程团队面临的一个日益严重的问题：如何评估和审查基础模型任务的输出。提出的方法通过合成特定任务评估器，结合自动化和人工反馈，为没有现成评估指标或数据集的任务提供了有效的评估解决方案。

Abstract: Hallucinations are a key concern when creating applications that rely on Foundation models (FMs). Understanding where and how these subtle failures occur in an application relies on evaluation methods known as \textit{evals}. Prior work focuses on defining new eval methods or benchmark datasets for specific tasks. However, neither helps a software team with a task-specific FM application when there is no metric or dataset. The demand for both automated approaches and deep integration of human insight makes this a challenging problem. We address this gap by proposing an approach to synthesise a FM task-specific evaluator program that provides automation and a custom UI for capturing feedback. The core novelty of our approach lies in: (1) a task-agnostic meta-model that captures properties of any FM task, (2) an interaction protocol for efficient use of human feedback, and (3) an eval synthesiser that selects or generates an appropriate set of evals. We implement our approach in \toolname and demonstrate the concept on two diverse FM tasks: chart data extraction and document question answering. A preliminary evaluation on the quality of our selected evals shows 93\% and 90\% accuracy respectively. Our research tackles a growing problem facing engineering teams, how to evaluate and review outputs from FM tasks.

</details>


### [29] [MARL Warehouse Robots](https://arxiv.org/abs/2512.04463)
*Price Allman,Lian Thang,Dre Simmons,Salmon Riaz*

Main category: cs.AI

TL;DR: 比较QMIX和IPPO两种多智能体强化学习算法在仓库机器人协作任务中的表现，发现QMIX通过价值分解显著优于独立学习方法，但需要大量超参数调优


<details>
  <summary>Details</summary>
Motivation: 研究多智能体强化学习算法在协作式仓库机器人任务中的实际应用效果，评估不同算法在复杂环境中的性能差异

Method: 在Robotic Warehouse环境和自定义Unity 3D仿真环境中，对QMIX和IPPO两种MARL算法进行对比实验，重点关注超参数调优（特别是epsilon退火策略）

Result: QMIX的平均回报达到3.25，显著优于IPPO的0.38；在Unity ML-Agents中经过100万步训练后能够实现稳定的包裹配送；但算法需要大量超参数调优（500万+步的epsilon退火）

Conclusion: 多智能体强化学习在小规模部署（2-4个机器人）中表现出潜力，但面临显著的扩展性挑战；QMIX的价值分解方法在协作任务中优于独立学习，但需要精细的超参数调优

Abstract: We present a comparative study of multi-agent reinforcement learning (MARL) algorithms for cooperative warehouse robotics. We evaluate QMIX and IPPO on the Robotic Warehouse (RWARE) environment and a custom Unity 3D simulation. Our experiments reveal that QMIX's value decomposition significantly outperforms independent learning approaches (achieving 3.25 mean return vs. 0.38 for advanced IPPO), but requires extensive hyperparameter tuning -- particularly extended epsilon annealing (5M+ steps) for sparse reward discovery. We demonstrate successful deployment in Unity ML-Agents, achieving consistent package delivery after 1M training steps. While MARL shows promise for small-scale deployments (2-4 robots), significant scaling challenges remain. Code and analyses: https://pallman14.github.io/MARL-QMIX-Warehouse-Robots/

</details>


### [30] [Mathematical Framing for Different Agent Strategies](https://arxiv.org/abs/2512.04469)
*Philip Stephens,Emmanuel Salawu*

Main category: cs.AI

TL;DR: 该论文提出了一个统一的数学和概率框架，用于理解和比较不同的AI智能体策略，将高层设计概念与严格的数学表述连接起来。


<details>
  <summary>Details</summary>
Motivation: 当前AI智能体策略（如ReAct、多智能体系统、控制流等）缺乏统一的数学框架进行比较和分析，需要建立共同语言来讨论不同架构的权衡。

Method: 将智能体过程框架化为概率链，分析不同策略如何操纵这些概率以实现期望结果，并引入"自由度"概念来区分不同方法的可优化杠杆。

Result: 提出了一个统一的数学概率框架，能够为不同AI智能体策略提供共同的分析语言，并通过自由度概念指导特定任务中策略的选择。

Conclusion: 该框架增强了AI智能体设计和评估的清晰度和精确性，为在复杂智能体系统中最大化成功行动概率提供了理论见解。

Abstract: We introduce a unified mathematical and probabilistic framework for understanding and comparing diverse AI agent strategies. We bridge the gap between high-level agent design concepts, such as ReAct, multi-agent systems, and control flows, and a rigorous mathematical formulation. Our approach frames agentic processes as a chain of probabilities, enabling a detailed analysis of how different strategies manipulate these probabilities to achieve desired outcomes. Our framework provides a common language for discussing the trade-offs inherent in various agent architectures. One of our many key contributions is the introduction of the "Degrees of Freedom" concept, which intuitively differentiates the optimizable levers available for each approach, thereby guiding the selection of appropriate strategies for specific tasks. This work aims to enhance the clarity and precision in designing and evaluating AI agents, offering insights into maximizing the probability of successful actions within complex agentic systems.

</details>


### [31] [ASTRIDE: A Security Threat Modeling Platform for Agentic-AI Applications](https://arxiv.org/abs/2512.04785)
*Eranga Bandara,Amin Hass,Ross Gore,Sachin Shetty,Ravi Mukkamala,Safdar H. Bouk,Xueping Liang,Ng Wee Keong,Kasun De Zoysa,Aruna Withanage,Nilaan Loganathan*

Main category: cs.AI

TL;DR: ASTRIDE：首个针对AI智能体系统的自动化威胁建模平台，扩展STRIDE框架，集成视觉语言模型和推理LLM，直接从架构图进行端到端威胁分析。


<details>
  <summary>Details</summary>
Motivation: AI智能体系统在现代软件架构中日益重要，但引入了传统威胁建模框架无法有效捕捉的新型安全挑战，如提示注入攻击、上下文污染、模型操纵和不透明的智能体间通信。

Method: 扩展经典STRIDE框架，新增A类别（AI智能体特定攻击）；结合微调的视觉语言模型联盟和OpenAI-gpt-oss推理LLM，直接从视觉架构图（如数据流图）进行端到端分析；使用LLM智能体协调VLM联盟和推理LLM之间的交互。

Result: 评估表明ASTRIDE为下一代智能系统提供了准确、可扩展和可解释的威胁建模。这是首个既扩展STRIDE框架包含AI特定威胁，又集成微调VLM与推理LLM实现AI智能体应用中图驱动威胁建模完全自动化的框架。

Conclusion: ASTRIDE是专门为AI智能体系统构建的自动化威胁建模平台，能够有效应对AI智能体系统特有的新兴安全挑战，为智能系统安全提供创新解决方案。

Abstract: AI agent-based systems are becoming increasingly integral to modern software architectures, enabling autonomous decision-making, dynamic task execution, and multimodal interactions through large language models (LLMs). However, these systems introduce novel and evolving security challenges, including prompt injection attacks, context poisoning, model manipulation, and opaque agent-to-agent communication, that are not effectively captured by traditional threat modeling frameworks. In this paper, we introduce ASTRIDE, an automated threat modeling platform purpose-built for AI agent-based systems. ASTRIDE extends the classical STRIDE framework by introducing a new threat category, A for AI Agent-Specific Attacks, which encompasses emerging vulnerabilities such as prompt injection, unsafe tool invocation, and reasoning subversion, unique to agent-based applications. To automate threat modeling, ASTRIDE combines a consortium of fine-tuned vision-language models (VLMs) with the OpenAI-gpt-oss reasoning LLM to perform end-to-end analysis directly from visual agent architecture diagrams, such as data flow diagrams(DFDs). LLM agents orchestrate the end-to-end threat modeling automation process by coordinating interactions between the VLM consortium and the reasoning LLM. Our evaluations demonstrate that ASTRIDE provides accurate, scalable, and explainable threat modeling for next-generation intelligent systems. To the best of our knowledge, ASTRIDE is the first framework to both extend STRIDE with AI-specific threats and integrate fine-tuned VLMs with a reasoning LLM to fully automate diagram-driven threat modeling in AI agent-based applications.

</details>


### [32] [Persona-based Multi-Agent Collaboration for Brainstorming](https://arxiv.org/abs/2512.04488)
*Nate Straub,Saara Khan,Katharina Jay,Brian Cabral,Oskar Linde*

Main category: cs.AI

TL;DR: 本文提出基于角色的多智能体头脑风暴框架，通过角色领域筛选提升创意生成质量，实验证明角色选择和协作模式显著影响创意多样性和深度。


<details>
  <summary>Details</summary>
Motivation: 先前研究表明通用多智能体协作通常比单一智能体提供更好的推理能力，但缺乏针对特定角色领域优化的头脑风暴方法。本文旨在探索如何通过精心设计的角色领域筛选来提升头脑风暴效果。

Method: 提出并开发了基于角色的智能体选择框架，通过多种实验设置评估不同角色配对（如医生vsVR工程师）和智能体间动态（分离、共同、分离后共同）对头脑风暴产出的影响。

Result: 结果显示：(1) 角色选择塑造创意领域；(2) 协作模式改变创意生成的多样性；(3) 基于角色的多智能体头脑风暴能产生深度创意和跨领域覆盖。

Conclusion: 基于角色的多智能体头脑风暴框架在多样化主题和专业知识创意生成中具有重要价值，角色领域筛选能显著改善头脑风暴结果。

Abstract: We demonstrate the importance of persona-based multi-agents brainstorming for both diverse topics and subject matter ideation. Prior work has shown that generalized multi-agent collaboration often provides better reasoning than a single agent alone. In this paper, we propose and develop a framework for persona-based agent selection, showing how persona domain curation can improve brainstorming outcomes. Using multiple experimental setups, we evaluate brainstorming outputs across different persona pairings (e.g., Doctor vs VR Engineer) and A2A (agent-to-agent) dynamics (separate, together, separate-then-together). Our results show that (1) persona choice shapes idea domains, (2) collaboration mode shifts diversity of idea generation, and (3) multi-agent persona-driven brainstorming produces idea depth and cross-domain coverage.

</details>


### [33] [A Modular Cognitive Architecture for Assisted Reasoning: The Nemosine Framework](https://arxiv.org/abs/2512.04500)
*Edervaldo Melo*

Main category: cs.AI

TL;DR: Nemosine框架是一个模块化认知架构，通过功能认知模块支持辅助推理、结构化思维和系统分析，结合元认知、分布式认知和模块化认知系统原理，为辅助问题解决和决策支持提供操作结构。


<details>
  <summary>Details</summary>
Motivation: 该论文旨在为辅助推理、结构化思维和系统分析提供一个清晰的认知架构框架，结合元认知、分布式认知和模块化认知系统原理，为未来的计算实现奠定概念基础，并促进符号模块化推理架构的研究。

Method: 采用模块化认知架构设计，通过功能认知模块（"personas"）组织任务，包括规划、评估、交叉检查和叙事合成。框架通过形式化规范、内部一致性标准和可复现的结构组件进行文档化。

Result: 提出了Nemosine框架这一完整的模块化认知架构，为辅助问题解决和决策支持提供了操作结构，建立了形式化规范和可复现的组件，为未来计算实现提供了概念基础。

Conclusion: Nemosine框架成功构建了一个支持辅助推理和结构化思维的模块化认知架构，为符号模块化推理架构的研究做出了贡献，并为未来的计算实现提供了清晰的规范基础。

Abstract: This paper presents the Nemosine Framework, a modular cognitive architecture designed to support assisted reasoning, structured thinking, and systematic analysis. The model operates through functional cognitive modules ("personas") that organize tasks such as planning, evaluation, cross-checking, and narrative synthesis. The framework combines principles from metacognition, distributed cognition, and modular cognitive systems to offer an operational structure for assisted problem-solving and decision support. The architecture is documented through formal specification, internal consistency criteria, and reproducible structural components. The goal is to provide a clear conceptual basis for future computational implementations and to contribute to the study of symbolic-modular architectures for reasoning.

</details>


### [34] [BiTAgent: A Task-Aware Modular Framework for Bidirectional Coupling between Multimodal Large Language Models and World Models](https://arxiv.org/abs/2512.04513)
*Yu-Wei Zhan,Xin Wang,Pengzhe Mao,Tongtong Feng,Ren Wang,Wenwu Zhu*

Main category: cs.AI

TL;DR: BiTAgent是一个任务感知的动态联合框架，通过双向耦合多模态大语言模型（MLLMs）和世界模型（WMs），实现语义推理与动态预测的协调，提升具身智能的稳定性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 构建通用具身智能体需要一个统一系统来解释多模态目标、建模环境动态并执行可靠动作。MLLMs提供强大的语义先验和跨模态泛化能力，WMs提供可操作的潜在动态用于预测和控制。两者的结合有望实现开放式具身智能，但面临两个关键挑战：1）在MLLMs的语义意图与WM潜在空间中的动态状态表示之间建立紧密耦合；2）实现支持多任务学习和跨环境泛化的任务感知适应性。

Method: 提出BiTAgent框架，建立两个互补路径：前向路径将MLLM表示注入WM潜在空间以实现语义引导的想象；后向路径通过密集文本条件奖励，让WM生成的反馈优化MLLM的语义空间。通过三个协同组件实现：任务感知动态联合学习、任务感知行为学习和MLLM-WM联合优化。

Result: 在多任务和跨环境设置下的广泛实验表明，BiTAgent在稳定性和泛化能力方面优于最先进的基线方法，标志着向开放式具身学习迈出了一步。

Conclusion: BiTAgent通过双向耦合MLLMs和WMs，有效解决了语义意图与动态状态表示的耦合问题，实现了任务感知的适应性，为构建通用具身智能体提供了有前景的解决方案。

Abstract: Building generalist embodied agents requires a unified system that can interpret multimodal goals, model environment dynamics, and execute reliable actions across diverse real-world tasks. Multimodal large language models (MLLMs) offer strong semantic priors and cross-modal generalization, while world models (WMs) provide actionable latent dynamics for prediction and control. Their combination holds promise for open-ended embodied intelligence, yet introduces two key challenges: (1) establishing a tight coupling between the semantic intent from MLLMs and the dynamic state representations within the WM's latent space, and (2) achieving task-aware adaptability that supports multi-task learning and cross-environment generalization. To address these limitations, we propose BiTAgent, a task-aware dynamic joint framework that enables bidirectional coupling between MLLMs and WMs. BiTAgent establishes two complementary pathways: a forward path that injects MLLM representations into the WM's latent space for semantically guided imagination, and a backward path where WM-generated feedback refines the MLLM's semantic space via dense text-conditioned rewards. This bidirectional interaction is realized through three synergistic components: Task-Aware Dynamic Joint Learning, Task-Aware Behavior Learning, and MLLM-WM Joint Optimization, which together harmonize semantic reasoning and dynamic prediction. Extensive experiments across multi-task and cross-environment settings demonstrate superior stability and generalization over state-of-the-art baselines, marking a step toward open-ended embodied learning.

</details>


### [35] [SlideGen: Collaborative Multimodal Agents for Scientific Slide Generation](https://arxiv.org/abs/2512.04529)
*Xin Liang,Xiang Zhang,Yiwei Xu,Siqi Sun,Chenyu You*

Main category: cs.AI

TL;DR: SlideGen：一个基于多智能体协作的框架，能够从科学论文自动生成具有视觉吸引力的PPT幻灯片，超越了传统仅基于文本摘要的方法。


<details>
  <summary>Details</summary>
Motivation: 当前从科学论文生成学术幻灯片的方法主要简化为文本摘要任务，忽视了幻灯片创建中的视觉组件和设计密集型特性，需要一种能够同时处理长文本理解和视觉规划的多模态推理方法。

Method: SlideGen采用智能体化、模块化和视觉在环的框架，通过协调多个视觉语言智能体协作推理文档结构和语义，包含协调的大纲制定、映射、布局安排、笔记合成和迭代优化等模块，生成可编辑的PPTX幻灯片。

Result: 在多样化的基准测试和强基线对比中，SlideGen在视觉质量、内容忠实度和可读性方面均优于现有方法，成为自动幻灯片生成领域的新技术标杆，能够持续生成专家级质量的幻灯片。

Conclusion: SlideGen为设计感知的多模态幻灯片生成奠定了基础，展示了智能体协作如何弥合复杂多模态推理任务中的理解和呈现之间的鸿沟。

Abstract: Generating academic slides from scientific papers is a challenging multimodal reasoning task that requires both long context understanding and deliberate visual planning. Existing approaches largely reduce it to text only summarization, overlooking the visual component and design intensive nature of slide creation. In this paper we introduce SlideGen, an agentic, modular, and visual in the loop framework for scientific paper to slide generation. SlideGen orchestrates a group of vision language agents that reason collaboratively over the document structure and semantics, producing editable PPTX slides with logical flow and compelling visual presentation. By integrating coordinated outlining, mapping, arrangement, note synthesis, and iterative refinement, our system consistently delivers slides of expert level quality. Across diverse benchmarks and strong baselines, SlideGen outperforms existing methods in visual quality, content faithfulness, and readability, positioning it as the new state of the art in automated slide generation. Our work establishes a foundation for design aware multimodal slide generation, demonstrating how agentic collaboration can bridge understanding and presentation in complex multimodal reasoning tasks.

</details>


### [36] [The Ethics of Generative AI](https://arxiv.org/abs/2512.04598)
*Michael Klenk*

Main category: cs.AI

TL;DR: 本章探讨生成式AI的伦理问题，分析其技术特性如何让人体验AI如人类般互动，并讨论这种特性如何加剧或缓解AI伦理中的经典问题，以及生成式AI特有的伦理挑战。


<details>
  <summary>Details</summary>
Motivation: 随着生成式AI技术的快速发展，其能够模拟人类创造力和互动的能力引发了独特的伦理问题。本章旨在系统分析生成式AI带来的伦理挑战，包括其技术特性如何影响人们对AI的认知和互动方式，以及这些特性如何与现有AI伦理框架相互作用。

Method: 首先提供技术基础，解释生成式AI如何让人体验技术如人类般互动；然后分析这种特性如何影响传统AI伦理问题（责任、隐私、偏见、异化等）；最后专门探讨生成式AI特有的伦理问题，包括作者权、人机关系、影响力与操纵等。

Result: 生成式AI的技术特性既可能加剧传统AI伦理问题（如责任归属更复杂），也可能提供缓解方案（如减少某些偏见）。同时，其独特的生成能力引发了新的伦理挑战，包括创作权归属、人机社会关系建立、以及新型说服与操纵机制。

Conclusion: 生成式AI的伦理分析需要同时关注其如何影响传统AI伦理问题，以及其特有的生成能力带来的新挑战。理解AI让人体验技术如人类般互动的特性，是分析生成式AI伦理问题的关键切入点。

Abstract: This chapter discusses the ethics of generative AI. It provides a technical primer to show how generative AI affords experiencing technology as if it were human, and this affordance provides a fruitful focus for the philosophical ethics of generative AI. It then shows how generative AI can both aggravate and alleviate familiar ethical concerns in AI ethics, including responsibility, privacy, bias and fairness, and forms of alienation and exploitation. Finally, the chapter examines ethical questions that arise specifically from generative AI's mimetic generativity, such as debates about authorship and credit, the emergence of as-if social relationships with machines, and new forms of influence, persuasion, and manipulation.

</details>


### [37] [Neural Decoding of Overt Speech from ECoG Using Vision Transformers and Contrastive Representation Learning](https://arxiv.org/abs/2512.04618)
*Mohamed Baha Ben Ticha,Xingchen Ran,Guillaume Saldanha,Gaël Le Godais,Philémon Roussel,Marc Aubert,Amina Fontanell,Thomas Costecalde,Lucas Struber,Serpil Karakas,Shaomin Zhang,Philippe Kahane,Guillaume Charvet,Stéphan Chabardès,Blaise Yvert*

Main category: cs.AI

TL;DR: 该研究提出了一种基于编码器-解码器深度神经架构的离线语音解码管道，整合视觉变换器和对比学习，直接从ECoG信号回归重建语音，并在两种不同电极系统上进行了验证。


<details>
  <summary>Details</summary>
Motivation: 当前语音脑机接口面临的主要挑战是如何通过直接回归皮层信号到声学语音来实现流式语音重建。虽然已有研究在皮层内记录上取得进展，但表面ECoG记录需要进一步优化神经解码器以获得可比结果。

Method: 采用编码器-解码器深度神经架构，整合视觉变换器和对比学习技术，直接从ECoG信号回归语音。在两个数据集上评估：一个来自癫痫患者的临床硬膜下电极，另一个来自运动脑机接口试验参与者的完全植入式无线WIMAGINE硬膜外系统。

Result: 这是首次尝试从完全植入式无线硬膜外记录系统解码语音，为长期使用提供了前景。研究展示了从两种不同类型电极系统解码语音的可行性。

Conclusion: 该研究提出的方法为从表面ECoG记录实现流式语音重建提供了有前景的解决方案，特别是完全植入式无线系统的成功解码为长期语音脑机接口应用开辟了新途径。

Abstract: Speech Brain Computer Interfaces (BCIs) offer promising solutions to people with severe paralysis unable to communicate. A number of recent studies have demonstrated convincing reconstruction of intelligible speech from surface electrocorticographic (ECoG) or intracortical recordings by predicting a series of phonemes or words and using downstream language models to obtain meaningful sentences. A current challenge is to reconstruct speech in a streaming mode by directly regressing cortical signals into acoustic speech. While this has been achieved recently using intracortical data, further work is needed to obtain comparable results with surface ECoG recordings. In particular, optimizing neural decoders becomes critical in this case. Here we present an offline speech decoding pipeline based on an encoder-decoder deep neural architecture, integrating Vision Transformers and contrastive learning to enhance the direct regression of speech from ECoG signals. The approach is evaluated on two datasets, one obtained with clinical subdural electrodes in an epileptic patient, and another obtained with the fully implantable WIMAGINE epidural system in a participant of a motor BCI trial. To our knowledge this presents a first attempt to decode speech from a fully implantable and wireless epidural recording system offering perspectives for long-term use.

</details>


### [38] [Turbo-Muon: Accelerating Orthogonality-Based Optimization with Pre-Conditioning](https://arxiv.org/abs/2512.04632)
*Thibaut Boissin,Thomas Massena,Franck Mamalet,Mathieu Serrurier*

Main category: cs.AI

TL;DR: 提出一种加速牛顿-舒尔茨正交化收敛的预处理方法，减少计算成本，实现2.8倍加速，在训练中获得5-10%的端到端运行时改进


<details>
  <summary>Details</summary>
Motivation: 基于正交化的优化器（如Muon）在大规模训练中表现良好，但依赖昂贵的梯度正交化步骤。即使高效的牛顿-舒尔茨迭代近似仍然昂贵，通常需要数十次矩阵乘法才能收敛。

Method: 引入一种预处理程序来加速牛顿-舒尔茨收敛并降低其计算成本。该方法通过加速收敛使得可以从通常的五次迭代中移除一次而不降低近似质量。

Result: 公开实现实现了牛顿-舒尔茨近似高达2.8倍的加速。在现实训练场景中，端到端训练运行时获得5-10%的改进。在语言或视觉任务上，方法保持相等或更好的模型性能同时改进运行时。

Conclusion: 该方法无需超参数调优，可作为简单的即插即用替代方案。预处理的开销可以忽略不计，加速的收敛使得减少迭代次数成为可能，从而显著提升训练效率。

Abstract: Orthogonality-based optimizers, such as Muon, have recently shown strong performance across large-scale training and community-driven efficiency challenges. However, these methods rely on a costly gradient orthogonalization step. Even efficient iterative approximations such as Newton-Schulz remain expensive, typically requiring dozens of matrix multiplications to converge. We introduce a preconditioning procedure that accelerates Newton-Schulz convergence and reduces its computational cost. We evaluate its impact and show that the overhead of our preconditioning can be made negligible. Furthermore, the faster convergence it enables allows us to remove one iteration out of the usual five without degrading approximation quality. Our publicly available implementation achieves up to a 2.8x speedup in the Newton-Schulz approximation. We also show that this has a direct impact on end-to-end training runtime with 5-10% improvement in realistic training scenarios across two efficiency-focused tasks. On challenging language or vision tasks, we validate that our method maintains equal or superior model performance while improving runtime. Crucially, these improvements require no hyperparameter tuning and can be adopted as a simple drop-in replacement. Our code is publicly available on github.

</details>


### [39] [Towards Ethical Multi-Agent Systems of Large Language Models: A Mechanistic Interpretability Perspective](https://arxiv.org/abs/2512.04691)
*Jae Hee Lee,Anne Lauscher,Stefano V. Albrecht*

Main category: cs.AI

TL;DR: 本文提出一个研究议程，旨在从机制可解释性角度确保大型语言模型多智能体系统的伦理行为，重点关注评估框架、内部机制解析和参数高效对齐三个挑战。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型多智能体系统在增强能力和实现复杂任务方面展现出潜力，但也带来了显著的伦理挑战，需要从机制可解释性角度确保其伦理行为。

Method: 提出基于机制可解释性的研究框架，包括：1）开发个体、交互和系统层面的伦理行为评估框架；2）通过机制可解释性阐明涌现行为的内部机制；3）实施有针对性的参数高效对齐技术。

Result: 本文是一个立场论文，提出了研究议程而非具体实验结果，旨在为大型语言模型多智能体系统的伦理行为研究提供系统性的研究方向。

Conclusion: 需要从机制可解释性角度系统研究大型语言模型多智能体系统的伦理行为，通过评估、理解和干预三个层面的研究确保这些系统的伦理可靠性。

Abstract: Large language models (LLMs) have been widely deployed in various applications, often functioning as autonomous agents that interact with each other in multi-agent systems. While these systems have shown promise in enhancing capabilities and enabling complex tasks, they also pose significant ethical challenges. This position paper outlines a research agenda aimed at ensuring the ethical behavior of multi-agent systems of LLMs (MALMs) from the perspective of mechanistic interpretability. We identify three key research challenges: (i) developing comprehensive evaluation frameworks to assess ethical behavior at individual, interactional, and systemic levels; (ii) elucidating the internal mechanisms that give rise to emergent behaviors through mechanistic interpretability; and (iii) implementing targeted parameter-efficient alignment techniques to steer MALMs towards ethical behaviors without compromising their performance.

</details>


### [40] [Playing the Player: A Heuristic Framework for Adaptive Poker AI](https://arxiv.org/abs/2512.04714)
*Andrew Paterson,Carl Sanders*

Main category: cs.AI

TL;DR: 本文挑战了扑克AI追求不可剥削、机器完美游戏的范式，提出了Patrick AI，它采用最大化剥削人类对手缺陷和心理学特征的哲学，而非追求不可剥削性。


<details>
  <summary>Details</summary>
Motivation: 长期以来，扑克AI领域被求解器和追求不可剥削的机器完美游戏所主导。本文挑战这一正统观念，认为胜利之路不在于不可剥削性，而在于最大化剥削人类对手的缺陷、心理和非理性特征。

Method: Patrick AI采用专门设计的架构，用于理解和攻击人类对手的缺陷。其核心是新颖的预测锚定学习方法，旨在识别和利用人类对手的弱点。

Result: 在64,267手牌的试验中，Patrick AI表现出盈利性能，证明了其最大化剥削策略的有效性。

Conclusion: "求解神话"分散了对真正更有趣挑战的注意力：创建能够掌握人类不完美艺术的AI。Patrick AI的成功表明，在扑克等对抗性游戏中，针对人类弱点的剥削策略可能比追求理论上的不可剥削性更为有效。

Abstract: For years, the discourse around poker AI has been dominated by the concept of solvers and the pursuit of unexploitable, machine-perfect play. This paper challenges that orthodoxy. It presents Patrick, an AI built on the contrary philosophy: that the path to victory lies not in being unexploitable, but in being maximally exploitative. Patrick's architecture is a purpose-built engine for understanding and attacking the flawed, psychological, and often irrational nature of human opponents. Through detailed analysis of its design, its novel prediction-anchored learning method, and its profitable performance in a 64,267-hand trial, this paper makes the case that the solved myth is a distraction from the real, far more interesting challenge: creating AI that can master the art of human imperfection.

</details>


### [41] [SIMA 2: A Generalist Embodied Agent for Virtual Worlds](https://arxiv.org/abs/2512.04797)
*SIMA team,Adrian Bolton,Alexander Lerchner,Alexandra Cordell,Alexandre Moufarek,Andrew Bolt,Andrew Lampinen,Anna Mitenkova,Arne Olav Hallingstad,Bojan Vujatovic,Bonnie Li,Cong Lu,Daan Wierstra,Daniel P. Sawyer,Daniel Slater,David Reichert,Davide Vercelli,Demis Hassabis,Drew A. Hudson,Duncan Williams,Ed Hirst,Fabio Pardo,Felix Hill,Frederic Besse,Hannah Openshaw,Harris Chan,Hubert Soyer,Jane X. Wang,Jeff Clune,John Agapiou,John Reid,Joseph Marino,Junkyung Kim,Karol Gregor,Kaustubh Sridhar,Kay McKinney,Laura Kampis,Lei M. Zhang,Loic Matthey,Luyu Wang,Maria Abi Raad,Maria Loks-Thompson,Martin Engelcke,Matija Kecman,Matthew Jackson,Maxime Gazeau,Ollie Purkiss,Oscar Knagg,Peter Stys,Piermaria Mendolicchio,Raia Hadsell,Rosemary Ke,Ryan Faulkner,Sarah Chakera,Satinder Singh Baveja,Shane Legg,Sheleem Kashem,Tayfun Terzi,Thomas Keck,Tim Harley,Tim Scholtes,Tyson Roberts,Volodymyr Mnih,Yulan Liu,Zhengdong Wang,Zoubin Ghahramani*

Main category: cs.AI

TL;DR: SIMA 2是基于Gemini基础模型构建的通用具身智能体，能够在多种3D虚拟世界中理解和行动，显著提升了与环境的主动交互能力，接近人类表现并具备强大的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 开发一个能够理解并主动在3D虚拟世界中行动的通用具身智能体，超越之前仅能处理简单语言指令的局限，实现更复杂的交互和推理能力。

Method: 基于Gemini基础模型构建，支持语言和图像输入的复杂指令处理，能够作为交互伙伴进行高层次目标推理和用户对话，并利用Gemini生成任务和奖励实现自主技能学习。

Result: 在多样化游戏环境中显著缩小了与人类表现的差距，展现出对未见环境的强大泛化能力，同时保持了基础模型的核心推理能力，并能通过自主学习掌握新环境中的新技能。

Conclusion: SIMA 2验证了创建多功能、持续学习智能体的可行路径，为虚拟世界乃至未来物理世界的通用具身智能体发展奠定了基础。

Abstract: We introduce SIMA 2, a generalist embodied agent that understands and acts in a wide variety of 3D virtual worlds. Built upon a Gemini foundation model, SIMA 2 represents a significant step toward active, goal-directed interaction within an embodied environment. Unlike prior work (e.g., SIMA 1) limited to simple language commands, SIMA 2 acts as an interactive partner, capable of reasoning about high-level goals, conversing with the user, and handling complex instructions given through language and images. Across a diverse portfolio of games, SIMA 2 substantially closes the gap with human performance and demonstrates robust generalization to previously unseen environments, all while retaining the base model's core reasoning capabilities. Furthermore, we demonstrate a capacity for open-ended self-improvement: by leveraging Gemini to generate tasks and provide rewards, SIMA 2 can autonomously learn new skills from scratch in a new environment. This work validates a path toward creating versatile and continuously learning agents for both virtual and, eventually, physical worlds.

</details>


### [42] [Model-Based and Sample-Efficient AI-Assisted Math Discovery in Sphere Packing](https://arxiv.org/abs/2512.04829)
*Rasul Tutunov,Alexandre Maraval,Antoine Grosnit,Xihan Li,Jun Wang,Haitham Bou-Ammar*

Main category: cs.AI

TL;DR: 该论文提出了一种基于模型搜索的方法来解决球体堆积问题的上界计算，通过将SDP构建过程建模为序列决策问题，在维度4-16中获得了新的最优上界。


<details>
  <summary>Details</summary>
Motivation: 球体堆积问题是希尔伯特第十八问题，在密码学、晶体学和医学成像等领域有重要应用，但除了少数特殊维度外，既不知道最优堆积方式也不知道紧的上界。现有的三点法需要求解大型高精度半定规划，每个候选SDP可能需要数天时间评估，使得标准的数据密集型AI方法不可行。

Method: 将SDP构建过程建模为序列决策过程（SDP游戏），策略从一组允许的组件中组装SDP公式。使用基于模型的样本高效框架，结合贝叶斯优化和蒙特卡洛树搜索。

Result: 在维度4-16中获得了新的最优上界，展示了基于模型的搜索能够在长期存在的几何问题上推进计算进展。

Conclusion: 样本高效的基于模型搜索能够在数学严格、评估受限的问题上取得实质性进展，为超越大规模LLM驱动探索的AI辅助发现指出了补充方向。

Abstract: Sphere packing, Hilbert's eighteenth problem, asks for the densest arrangement of congruent spheres in n-dimensional Euclidean space. Although relevant to areas such as cryptography, crystallography, and medical imaging, the problem remains unresolved: beyond a few special dimensions, neither optimal packings nor tight upper bounds are known. Even a major breakthrough in dimension $n=8$, later recognised with a Fields Medal, underscores its difficulty. A leading technique for upper bounds, the three-point method, reduces the problem to solving large, high-precision semidefinite programs (SDPs). Because each candidate SDP may take days to evaluate, standard data-intensive AI approaches are infeasible. We address this challenge by formulating SDP construction as a sequential decision process, the SDP game, in which a policy assembles SDP formulations from a set of admissible components. Using a sample-efficient model-based framework that combines Bayesian optimisation with Monte Carlo Tree Search, we obtain new state-of-the-art upper bounds in dimensions $4-16$, showing that model-based search can advance computational progress in longstanding geometric problems. Together, these results demonstrate that sample-efficient, model-based search can make tangible progress on mathematically rigid, evaluation limited problems, pointing towards a complementary direction for AI-assisted discovery beyond large-scale LLM-driven exploration.

</details>


### [43] [Are LLMs Truly Multilingual? Exploring Zero-Shot Multilingual Capability of LLMs for Information Retrieval: An Italian Healthcare Use Case](https://arxiv.org/abs/2512.04834)
*Vignesh Kumar Kembu,Pierandrea Morandini,Marta Bianca Maria Ranzini,Antonino Nocera*

Main category: cs.AI

TL;DR: 该研究探索开源多语言大语言模型在理解意大利语电子健康记录和实时信息提取方面的能力，发现某些模型在零样本、本地部署设置下表现不佳，性能在不同疾病间存在显著差异。


<details>
  <summary>Details</summary>
Motivation: 临床记录信息提取是数字医疗的关键任务，传统NLP技术因临床语言的复杂性、变异性和高语义内涵而表现不足，而大语言模型在理解和生成类人文本方面显示出强大潜力。

Method: 使用开源多语言大语言模型，在意大利语电子健康记录上进行实验，重点关注共病提取任务，在零样本、本地部署设置下评估模型性能。

Result: 实验显示某些LLMs在零样本、本地部署设置下表现不佳，不同模型性能存在显著差异，且难以在不同疾病间实现泛化，与原生模式匹配和人工标注相比存在差距。

Conclusion: 虽然大语言模型在临床信息提取方面有潜力，但在零样本、本地部署的实际应用中仍面临挑战，需要进一步改进以应对临床语言的复杂性和疾病间的泛化问题。

Abstract: Large Language Models (LLMs) have become a key topic in AI and NLP, transforming sectors like healthcare, finance, education, and marketing by improving customer service, automating tasks, providing insights, improving diagnostics, and personalizing learning experiences. Information extraction from clinical records is a crucial task in digital healthcare. Although traditional NLP techniques have been used for this in the past, they often fall short due to the complexity, variability of clinical language, and high inner semantics in the free clinical text. Recently, Large Language Models (LLMs) have become a powerful tool for better understanding and generating human-like text, making them highly effective in this area. In this paper, we explore the ability of open-source multilingual LLMs to understand EHRs (Electronic Health Records) in Italian and help extract information from them in real-time. Our detailed experimental campaign on comorbidity extraction from EHR reveals that some LLMs struggle in zero-shot, on-premises settings, and others show significant variation in performance, struggling to generalize across various diseases when compared to native pattern matching and manual annotations.

</details>


### [44] [From Task Executors to Research Partners: Evaluating AI Co-Pilots Through Workflow Integration in Biomedical Research](https://arxiv.org/abs/2512.04854)
*Lukas Weidener,Marko Brkić,Chiara Bacci,Mihailo Jovanović,Emre Ulgac,Alex Dobrin,Johannes Weniger,Martin Vlas,Ritvik Singh,Aakaash Meduri*

Main category: cs.AI

TL;DR: 本文通过快速综述分析了AI在生物医学研究中的基准测试实践，发现现有基准仅评估孤立组件能力，而真实研究协作需要集成工作流程，因此提出了面向过程的评估框架。


<details>
  <summary>Details</summary>
Motivation: 当前AI系统在生物医学研究中部署日益增多，但现有评估框架可能无法充分评估其作为研究合作者的有效性。需要了解当前基准测试实践并识别评估差距。

Method: 对2018年1月1日至2025年10月31日期间三大数据库和两个预印本服务器进行快速综述，识别出14个评估AI在文献理解、实验设计和假设生成方面能力的基准。

Result: 发现所有现有基准仅评估孤立组件能力（数据分析质量、假设有效性、实验方案设计），而真实研究协作需要跨多个会话的集成工作流程，包含上下文记忆、自适应对话和约束传播。

Conclusion: 提出了面向过程的评估框架，包含四个当前基准缺失的关键维度：对话质量、工作流程编排、会话连续性和研究者体验，这些维度对于评估AI作为研究协同者而非孤立任务执行者至关重要。

Abstract: Artificial intelligence systems are increasingly deployed in biomedical research. However, current evaluation frameworks may inadequately assess their effectiveness as research collaborators. This rapid review examines benchmarking practices for AI systems in preclinical biomedical research. Three major databases and two preprint servers were searched from January 1, 2018 to October 31, 2025, identifying 14 benchmarks that assess AI capabilities in literature understanding, experimental design, and hypothesis generation. The results revealed that all current benchmarks assess isolated component capabilities, including data analysis quality, hypothesis validity, and experimental protocol design. However, authentic research collaboration requires integrated workflows spanning multiple sessions, with contextual memory, adaptive dialogue, and constraint propagation. This gap implies that systems excelling on component benchmarks may fail as practical research co-pilots. A process-oriented evaluation framework is proposed that addresses four critical dimensions absent from current benchmarks: dialogue quality, workflow orchestration, session continuity, and researcher experience. These dimensions are essential for evaluating AI systems as research co-pilots rather than as isolated task executors.

</details>


### [45] [STELLA: Guiding Large Language Models for Time Series Forecasting with Semantic Abstractions](https://arxiv.org/abs/2512.04871)
*Junjie Fan,Hongye Zhao,Linduo Wei,Jiayu Rao,Guijia Li,Jiaxin Yuan,Wenqi Xu,Yong Qi*

Main category: cs.AI

TL;DR: STELLA框架通过语义-时间对齐增强LLM时间序列预测能力，通过动态语义抽象机制将序列分解为趋势、季节性和残差分量，并生成层次化语义锚点来指导LLM建模内在动态。


<details>
  <summary>Details</summary>
Motivation: 现有LLM时间序列预测方法未能有效增强原始序列信息，LLM推理能力未充分利用；现有提示策略依赖静态相关性而非生成式动态行为解释，缺乏关键的全局和实例特定上下文。

Method: 提出STELLA框架，采用动态语义抽象机制将输入序列解耦为趋势、季节性和残差分量，将这些分量的内在行为特征转化为层次化语义锚点：用于全局上下文的语料级语义先验(CSP)和用于实例级模式的细粒度行为提示(FBP)，将这些锚点作为前缀提示来指导LLM建模内在动态。

Result: 在八个基准数据集上的实验表明，STELLA在长期和短期预测中都优于最先进方法，在零样本和少样本设置中表现出优越的泛化能力；消融研究进一步验证了动态生成语义锚点的有效性。

Conclusion: STELLA通过系统性地挖掘和注入结构化补充和互补信息，有效解决了现有LLM时间序列预测方法的局限性，显著提升了预测性能。

Abstract: Recent adaptations of Large Language Models (LLMs) for time series forecasting often fail to effectively enhance information for raw series, leaving LLM reasoning capabilities underutilized. Existing prompting strategies rely on static correlations rather than generative interpretations of dynamic behavior, lacking critical global and instance-specific context. To address this, we propose STELLA (Semantic-Temporal Alignment with Language Abstractions), a framework that systematically mines and injects structured supplementary and complementary information. STELLA employs a dynamic semantic abstraction mechanism that decouples input series into trend, seasonality, and residual components. It then translates intrinsic behavioral features of these components into Hierarchical Semantic Anchors: a Corpus-level Semantic Prior (CSP) for global context and a Fine-grained Behavioral Prompt (FBP) for instance-level patterns. Using these anchors as prefix-prompts, STELLA guides the LLM to model intrinsic dynamics. Experiments on eight benchmark datasets demonstrate that STELLA outperforms state-of-the-art methods in long- and short-term forecasting, showing superior generalization in zero-shot and few-shot settings. Ablation studies further validate the effectiveness of our dynamically generated semantic anchors.

</details>


### [46] [Chameleon: Adaptive Adversarial Agents for Scaling-Based Visual Prompt Injection in Multimodal AI Systems](https://arxiv.org/abs/2512.04895)
*M Zeeshan,Saud Satti*

Main category: cs.AI

TL;DR: Chameleon是一种针对视觉语言模型的新型自适应对抗攻击框架，通过迭代优化机制利用图像缩放漏洞，在标准预处理后仍能注入恶意视觉提示，攻击成功率高达84.5%。


<details>
  <summary>Details</summary>
Motivation: 当前多模态AI系统（特别是视觉语言模型）严重依赖预处理管道，其中图像缩放操作存在安全漏洞。传统对抗攻击是静态的，无法适应现代智能体工作流的动态特性，需要开发能够暴露和利用缩放漏洞的自适应攻击框架。

Method: 提出Chameleon框架，采用基于智能体的迭代优化机制，根据目标模型的实时反馈动态优化图像扰动，生成能够在标准缩放操作后存活的对抗样本，从而劫持下游执行流程。

Result: 在Gemini 2.5 Flash模型上的实验显示，Chameleon在不同缩放因子下达到84.5%的攻击成功率，显著优于平均仅32.1%的静态基线攻击。攻击能有效破坏智能体管道，在多步任务中将决策准确率降低超过45%。

Conclusion: 图像缩放操作在多模态AI系统中构成严重安全漏洞，需要开发多尺度一致性检查等防御机制来应对Chameleon这类自适应对抗攻击。

Abstract: Multimodal Artificial Intelligence (AI) systems, particularly Vision-Language Models (VLMs), have become integral to critical applications ranging from autonomous decision-making to automated document processing. As these systems scale, they rely heavily on preprocessing pipelines to handle diverse inputs efficiently. However, this dependency on standard preprocessing operations, specifically image downscaling, creates a significant yet often overlooked security vulnerability. While intended for computational optimization, scaling algorithms can be exploited to conceal malicious visual prompts that are invisible to human observers but become active semantic instructions once processed by the model. Current adversarial strategies remain largely static, failing to account for the dynamic nature of modern agentic workflows. To address this gap, we propose Chameleon, a novel, adaptive adversarial framework designed to expose and exploit scaling vulnerabilities in production VLMs. Unlike traditional static attacks, Chameleon employs an iterative, agent-based optimization mechanism that dynamically refines image perturbations based on the target model's real-time feedback. This allows the framework to craft highly robust adversarial examples that survive standard downscaling operations to hijack downstream execution. We evaluate Chameleon against Gemini 2.5 Flash model. Our experiments demonstrate that Chameleon achieves an Attack Success Rate (ASR) of 84.5% across varying scaling factors, significantly outperforming static baseline attacks which average only 32.1%. Furthermore, we show that these attacks effectively compromise agentic pipelines, reducing decision-making accuracy by over 45% in multi-step tasks. Finally, we discuss the implications of these vulnerabilities and propose multi-scale consistency checks as a necessary defense mechanism.

</details>


### [47] [Algorithmic Thinking Theory](https://arxiv.org/abs/2512.04923)
*MohammadHossein Bateni,Vincent Cohen-Addad,Yuzhou Gu,Silvio Lattanzi,Simon Meierhans,Christopher Mohri*

Main category: cs.AI

TL;DR: 提出一个理论框架来分析基于LLM的迭代推理算法，将推理计划视为使用概率性预言机的算法


<details>
  <summary>Details</summary>
Motivation: LLMs在复杂推理任务中表现出色，但通过迭代改进先前生成的解决方案可以进一步提升其能力。目前缺乏一个理论框架来形式化分析这类迭代改进和答案聚合的推理算法。

Method: 引入一个理论框架，将推理计划形式化为使用概率性预言机的算法。该框架基于实验证据而非架构细节，能够形式化流行迭代改进和答案聚合技术的基本原理。

Result: 提供了一个通用的理论框架，为设计新一代更强大的推理方法奠定基础。该框架具有普适性，可能适用于当前和未来的各种推理预言机。

Conclusion: 提出的理论框架为分析LLM迭代推理算法提供了形式化基础，有助于理解和设计更有效的推理方法，具有广泛的适用性和扩展性。

Abstract: Large language models (LLMs) have proven to be highly effective for solving complex reasoning tasks. Surprisingly, their capabilities can often be improved by iterating on previously generated solutions. In this context, a reasoning plan for generating and combining a set of solutions can be thought of as an algorithm for reasoning using a probabilistic oracle.
  We introduce a theoretical framework for analyzing such reasoning algorithms. This framework formalizes the principles underlying popular techniques for iterative improvement and answer aggregation, providing a foundation for designing a new generation of more powerful reasoning methods. Unlike approaches for understanding models that rely on architectural specifics, our model is grounded in experimental evidence. As a result, it offers a general perspective that may extend to a wide range of current and future reasoning oracles.

</details>


### [48] [Toward Continuous Neurocognitive Monitoring: Integrating Speech AI with Relational Graph Transformers for Rare Neurological Diseases](https://arxiv.org/abs/2512.04938)
*Raquel Norel,Michele Merler,Pavitra Modi*

Main category: cs.AI

TL;DR: 通过智能手机语音分析结合关系图变换器架构，实现对罕见神经系统疾病患者认知症状的连续监测，在苯丙酮尿症中验证了语音指标与生化指标的相关性


<details>
  <summary>Details</summary>
Motivation: 传统认知测试无法检测罕见神经系统疾病患者的"脑雾"症状，需要开发连续监测方法来捕捉这些难以察觉的认知变化

Method: 采用智能手机语音分析结合关系图变换器架构，整合语音、实验室数据和临床评估等多源异质医疗数据

Result: 在苯丙酮尿症概念验证中，语音衍生的"言语流畅度"与血苯丙氨酸水平显著相关，而与传统认知测试无显著相关性

Conclusion: 该方法有望突破异质医疗数据的信息瓶颈，实现提前数周预测病情恶化的预警系统，将间歇性神经学评估转变为连续个性化监测

Abstract: Patients with rare neurological diseases report cognitive symptoms -"brain fog"- invisible to traditional tests. We propose continuous neurocognitive monitoring via smartphone speech analysis integrated with Relational Graph Transformer (RELGT) architectures. Proof-of-concept in phenylketonuria (PKU) shows speech-derived "Proficiency in Verbal Discourse" correlates with blood phenylalanine (p = -0.50, p < 0.005) but not standard cognitive tests (all |r| < 0.35). RELGT could overcome information bottlenecks in heterogeneous medical data (speech, labs, assessments), enabling predictive alerts weeks before decompensation. Key challenges: multi-disease validation, clinical workflow integration, equitable multilingual deployment. Success would transform episodic neurology into continuous personalized monitoring for millions globally.

</details>


### [49] [Detecting Perspective Shifts in Multi-agent Systems](https://arxiv.org/abs/2512.05013)
*Eric Bridgeford,Hayden Helm*

Main category: cs.AI

TL;DR: 提出TDKPS框架，用于在时间维度上联合嵌入多智能体，并开发新的假设检验方法来检测黑盒多智能体系统中的行为变化


<details>
  <summary>Details</summary>
Motivation: 随着生成式智能体部署规模扩大，需要监控黑盒多智能体系统中的行为动态变化，现有方法仅基于单时间点的查询响应进行低维表示，缺乏时间维度分析

Method: 提出时间数据核视角空间（TDKPS），在时间维度上联合嵌入智能体，开发了智能体层面和群体层面的行为变化检测假设检验方法

Result: 通过模拟实验验证了所提检验方法的经验特性，包括对关键超参数的敏感性；通过自然实验证明该方法能够检测到与真实外生事件显著相关的行为变化

Conclusion: TDKPS是首个用于监控黑盒多智能体系统行为动态的原则性框架，对于生成式智能体规模化部署具有重要意义

Abstract: Generative models augmented with external tools and update mechanisms (or \textit{agents}) have demonstrated capabilities beyond intelligent prompting of base models. As agent use proliferates, dynamic multi-agent systems have naturally emerged. Recent work has investigated the theoretical and empirical properties of low-dimensional representations of agents based on query responses at a single time point. This paper introduces the Temporal Data Kernel Perspective Space (TDKPS), which jointly embeds agents across time, and proposes several novel hypothesis tests for detecting behavioral change at the agent- and group-level in black-box multi-agent systems. We characterize the empirical properties of our proposed tests, including their sensitivity to key hyperparameters, in simulations motivated by a multi-agent system of evolving digital personas. Finally, we demonstrate via natural experiment that our proposed tests detect changes that correlate sensitively, specifically, and significantly with a real exogenous event. As far as we are aware, TDKPS is the first principled framework for monitoring behavioral dynamics in black-box multi-agent systems -- a critical capability as generative agent deployment continues to scale.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [50] [Towards Contextual Sensitive Data Detection](https://arxiv.org/abs/2512.04120)
*Liang Telkamp,Madelon Hulsebos*

Main category: cs.CR

TL;DR: 本文提出基于上下文的敏感数据检测方法，包括类型上下文化和领域上下文化，利用大语言模型显著提升检测准确率，在开放数据门户中有效保护敏感数据。


<details>
  <summary>Details</summary>
Motivation: 开放数据门户的兴起需要更多关注数据发布前的敏感数据保护。现有敏感数据检测方法主要关注个人隐私数据，但敏感数据的定义需要根据上下文进行细化和扩展，因为数据的敏感性取决于其具体使用场景。

Method: 提出两种上下文敏感数据检测机制：1) 类型上下文化：先检测特定数据值的语义类型，然后考虑数据值在数据集或文档中的整体上下文；2) 领域上下文化：基于从规定数据敏感性的文档中检索相关规则，在更广泛的上下文中确定数据集的敏感性。两种机制都利用大语言模型辅助实现。

Result: 实验表明：1) 类型上下文化显著减少了基于类型的敏感数据检测的误报率，召回率达到94%，而商业工具仅为63%；2) 领域上下文化在非标准数据领域（如人道主义数据集）中有效进行基于上下文的敏感数据检测。专家评估显示，基于上下文的LLM解释在手动数据审计过程中提供了有用指导，提高了检测一致性。

Conclusion: 基于上下文的敏感数据检测方法能够更准确地识别敏感数据，特别是在复杂和非标准数据领域中。该方法通过开源机制和标注数据集为敏感数据检测提供了实用工具，有助于提高数据发布前的保护效果。

Abstract: The emergence of open data portals necessitates more attention to protecting sensitive data before datasets get published and exchanged. While an abundance of methods for suppressing sensitive data exist, the conceptualization of sensitive data and methods to detect it, focus particularly on personal data that, if disclosed, may be harmful or violate privacy. We observe the need for refining and broadening our definitions of sensitive data, and argue that the sensitivity of data depends on its context. Based on this definition, we introduce two mechanisms for contextual sensitive data detection that consider the broader context of a dataset at hand. First, we introduce type contextualization, which first detects the semantic type of particular data values, then considers the overall context of the data values within the dataset or document. Second, we introduce domain contextualization which determines sensitivity of a given dataset in the broader context based on the retrieval of relevant rules from documents that specify data sensitivity (e.g., data topic and geographic origin). Experiments with these mechanisms, assisted by large language models (LLMs), confirm that: 1) type-contextualization significantly reduces the number of false positives for type-based sensitive data detection and reaches a recall of 94% compared to 63% with commercial tools, and 2) domain-contextualization leveraging sensitivity rule retrieval is effective for context-grounded sensitive data detection in non-standard data domains such as humanitarian datasets. Evaluation with humanitarian data experts also reveals that context-grounded LLM explanations provide useful guidance in manual data auditing processes, improving consistency. We open-source mechanisms and annotated datasets for contextual sensitive data detection at https://github.com/trl-lab/sensitive-data-detection.

</details>


### [51] [Tipping the Dominos: Topology-Aware Multi-Hop Attacks on LLM-Based Multi-Agent Systems](https://arxiv.org/abs/2512.04129)
*Ruichao Liang,Le Yin,Jing Chen,Cong Wu,Xiaoyu Zhang,Huangpeng Gu,Zijian Zhang,Yang Liu*

Main category: cs.CR

TL;DR: TOMA是一种针对LLM多智能体系统的拓扑感知多跳攻击方案，通过优化污染传播和对抗载荷扩散，无需特权访问即可实现40-78%攻击成功率，并提出基于拓扑信任的防御框架可阻断94.8%攻击。


<details>
  <summary>Details</summary>
Motivation: 当前多智能体系统的安全评估局限于有限攻击场景，导致安全问题不清晰且可能被低估，需要更全面的安全评估方法。

Method: 提出TOMA拓扑感知多跳攻击方案，通过优化MAS拓扑内的污染传播和控制来自环境的对抗载荷多跳扩散，无需特权访问或直接操作智能体。

Result: 在三种先进MAS架构（Magentic-One、LangManus、OWL）和五种代表性拓扑上实现40-78%攻击成功率，揭示了现有研究可能忽略的内在漏洞。

Conclusion: 研究揭示了多智能体系统的固有安全漏洞，并提出基于拓扑信任的防御框架原型，实验显示能有效阻断94.8%的自适应复合攻击。

Abstract: LLM-based multi-agent systems (MASs) have reshaped the digital landscape with their emergent coordination and problem-solving capabilities. However, current security evaluations of MASs are still confined to limited attack scenarios, leaving their security issues unclear and likely underestimated. To fill this gap, we propose TOMA, a topology-aware multi-hop attack scheme targeting MASs. By optimizing the propagation of contamination within the MAS topology and controlling the multi-hop diffusion of adversarial payloads originating from the environment, TOMA unveils new and effective attack vectors without requiring privileged access or direct agent manipulation. Experiments demonstrate attack success rates ranging from 40% to 78% across three state-of-the-art MAS architectures: \textsc{Magentic-One}, \textsc{LangManus}, and \textsc{OWL}, and five representative topologies, revealing intrinsic MAS vulnerabilities that may be overlooked by existing research. Inspired by these findings, we propose a conceptual defense framework based on topology trust, and prototype experiments show its effectiveness in blocking 94.8% of adaptive and composite attacks.

</details>


### [52] [Primitive Vector Cipher(PVC): A Hybrid Encryption Scheme based on the Vector Computational Diffie-Hellman (V-CDH) Problem](https://arxiv.org/abs/2512.04237)
*Gülçin ÇİVİ BİLİR*

Main category: cs.CR

TL;DR: PVC是一种结合矩阵密码学和改进Diffie-Hellman密钥交换的混合加密方案，基于V-CDH问题的困难性，采用两层设计实现IND-CPA安全，通过STS协议可提升至IND-CCA安全级别。


<details>
  <summary>Details</summary>
Motivation: 开发一种新型混合加密方案，结合矩阵密码学和先进Diffie-Hellman密钥交换，解决传统加密方案中存在的确定性重复问题，增强对线性攻击和已知明文攻击的抵抗能力。

Method: 采用两层设计：第一层使用HKDF通过DH认证的共享原始向量掩盖明文；第二层使用每块偏移随机化密文块。基于V-CDH问题困难性，采用块状结构支持大规模并行处理。

Result: PVC方案消除了确定性重复，对线性攻击和已知明文攻击具有强抵抗力，支持大规模并行处理和优秀线性扩展，在V-CDH假设下证明具有IND-CPA安全性。

Conclusion: PVC是一种安全高效的混合加密方案，通过STS协议集成可进一步提升至IND-CCA安全级别，具有实际应用价值。

Abstract: This work introduces the Primitive Vector Cipher (PVC), a novel hybrid encryption scheme integrating matrix-based cryptography with advanced Diffie-Hellman key exchange. PVC's security is grounded on the established hardness of the Vector Computational Diffie- Hellman (V-CDH) problem. The two-layered design uses HKDF to mask plaintext via a DH-authenticated shared primitive vector and randomize cipher blocks with a per-block offset. This approach eliminates deterministic repetitions and provides strong resistance against linear and known-plaintext attacks. PVC's block-wise structure allows for massive parallelism and excellent linear scaling. Security is formally analyzed, demonstrating INDCPA security under V-CDH. STS protocol integration elevates security toward IND-CCA guarantees.

</details>


### [53] [Hey GPT-OSS, Looks Like You Got It -- Now Walk Me Through It! An Assessment of the Reasoning Language Models Chain of Thought Mechanism for Digital Forensics](https://arxiv.org/abs/2512.04254)
*Gaëtan Michelet,Janine Schneider,Aruna Withanage,Frank Breitinger*

Main category: cs.CR

TL;DR: 本文首次研究了推理语言模型在数字取证中的潜力，通过四个测试用例评估推理组件对结果可解释性的支持，发现中等推理水平有助于解释和验证模型输出，但支持有限且更高推理水平不会提升响应质量。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在数字取证中已有广泛应用，但有限的结果可解释性降低了其操作和法律可用性。最近出现了通过"内部推理"机制处理逻辑任务的新型推理语言模型，但用户通常只能看到最终答案而非底层推理过程。gpt-oss等模型可本地部署并提供完整推理过程访问，这为数字取证提供了新的可能性。

Method: 研究通过四个测试用例评估推理语言模型在数字取证中的可用性，特别关注推理组件对结果可解释性的支持。评估结合了新的定量指标和定性分析，考察不同推理水平下的表现。

Result: 研究发现推理组件在中等推理水平下有助于解释和验证数字取证中语言模型的输出，但这种支持通常有限。更重要的是，更高的推理水平并不会增强响应质量，表明推理深度与结果质量之间不存在正相关关系。

Conclusion: 推理语言模型在数字取证中具有一定潜力，特别是在结果可解释性方面。然而，当前推理组件的支持有限，且增加推理深度不会改善输出质量。这为未来改进推理模型在取证应用中的设计提供了重要见解。

Abstract: The use of large language models in digital forensics has been widely explored. Beyond identifying potential applications, research has also focused on optimizing model performance for forensic tasks through fine-tuning. However, limited result explainability reduces their operational and legal usability. Recently, a new class of reasoning language models has emerged, designed to handle logic-based tasks through an `internal reasoning' mechanism. Yet, users typically see only the final answer, not the underlying reasoning. One of these reasoning models is gpt-oss, which can be deployed locally, providing full access to its underlying reasoning process. This article presents the first investigation into the potential of reasoning language models for digital forensics. Four test use cases are examined to assess the usability of the reasoning component in supporting result explainability. The evaluation combines a new quantitative metric with qualitative analysis. Findings show that the reasoning component aids in explaining and validating language model outputs in digital forensics at medium reasoning levels, but this support is often limited, and higher reasoning levels do not enhance response quality.

</details>


### [54] [WildCode: An Empirical Analysis of Code Generated by ChatGPT](https://arxiv.org/abs/2512.04259)
*Kobra Khanmohammadi,Pooria Roy,Raphael Khoury,Abdelwahab Hamou-Lhadj,Wilfried Patrick Konan*

Main category: cs.CR

TL;DR: 研究对ChatGPT生成的真实代码进行大规模实证分析，发现AI生成代码在安全性方面存在不足，且用户很少关注代码安全特性。


<details>
  <summary>Details</summary>
Motivation: LLM模型越来越多用于生成代码，但代码质量和安全性不确定。先前研究多使用专门生成的代码，实验真实性存疑，需要基于真实场景的实证分析。

Method: 对ChatGPT生成的真实代码进行大规模实证分析，评估代码的正确性和安全性，并深入分析用户请求代码的意图。

Result: 证实先前研究结论：LLM生成的代码在安全性方面通常不足。同时发现用户对代码安全特性缺乏关注，很少查询相关主题。

Conclusion: AI生成代码存在安全隐患，用户安全意识不足，需要改进LLM代码生成的安全性和用户教育。

Abstract: LLM models are increasingly used to generate code, but the quality and security of this code are often uncertain. Several recent studies have raised alarm bells, indicating that such AI-generated code may be particularly vulnerable to cyberattacks. However, most of these studies rely on code that is generated specifically for the study, which raises questions about the realism of such experiments. In this study, we perform a large-scale empirical analysis of real-life code generated by ChatGPT. We evaluate code generated by ChatGPT both with respect to correctness and security and delve into the intentions of users who request code from the model. Our research confirms previous studies that used synthetic queries and yielded evidence that LLM-generated code is often inadequate with respect to security. We also find that users exhibit little curiosity about the security features of the code they ask LLMs to generate, as evidenced by their lack of queries on this topic.

</details>


### [55] [Breaking Isolation: A New Perspective on Hypervisor Exploitation via Cross-Domain Attacks](https://arxiv.org/abs/2512.04260)
*Gaoning Pan,Yiming Tao,Qinying Wang,Chunming Wu,Mingde Hu,Yizhi Ren,Shouling Ji*

Main category: cs.CR

TL;DR: 论文提出跨域攻击（CDA）方法，利用虚拟机环境中弱内存隔离特性，通过访客内存重用来实现权限提升，无需依赖传统漏洞利用框架所需的高约束结构。


<details>
  <summary>Details</summary>
Motivation: 现有漏洞利用框架依赖识别主机中的高约束结构并准确确定其运行时地址，这在虚拟机环境中效果不佳，因为此类结构稀少且受ASLR进一步混淆。作者观察到现代虚拟化环境存在弱内存隔离问题，访客内存完全由攻击者控制但仍可从主机访问，这为漏洞利用提供了可靠原语。

Method: 提出了跨域攻击（CDA）的系统性特征描述和分类法，开发了一个自动化系统来识别跨域小工具、匹配损坏指针、合成触发输入并组装完整的利用链。

Result: 在QEMU和VirtualBox的15个真实漏洞上评估显示，CDA方法广泛适用且有效。

Conclusion: 跨域攻击（CDA）是一种有效的虚拟机漏洞利用方法，利用虚拟机环境中的弱内存隔离特性，为虚拟机安全防护提供了新的研究方向。

Abstract: Hypervisors are under threat by critical memory safety vulnerabilities, with pointer corruption being one of the most prevalent and severe forms. Existing exploitation frameworks depend on identifying highly-constrained structures in the host machine and accurately determining their runtime addresses, which is ineffective in hypervisor environments where such structures are rare and further obfuscated by Address Space Layout Randomization (ASLR). We instead observe that modern virtualization environments exhibit weak memory isolation -- guest memory is fully attacker-controlled yet accessible from the host, providing a reliable primitive for exploitation. Based on this observation, we present the first systematic characterization and taxonomy of Cross-Domain Attacks (CDA), a class of exploitation techniques that enable capability escalation through guest memory reuse. To automate this process, we develop a system that identifies cross-domain gadgets, matches them with corrupted pointers, synthesizes triggering inputs, and assembles complete exploit chains. Our evaluation on 15 real-world vulnerabilities across QEMU and VirtualBox shows that CDA is widely applicable and effective.

</details>


### [56] [One Detector Fits All: Robust and Adaptive Detection of Malicious Packages from PyPI to Enterprises](https://arxiv.org/abs/2512.04338)
*Biagio Montaruli,Luca Compagna,Serena Elisa Ponta,Davide Balzarotti*

Main category: cs.CR

TL;DR: 提出了一种鲁棒的恶意Python包检测器，能够对抗源代码混淆攻击，并适应不同场景（如PyPI维护者与企业安全团队）对误报率的不同要求。


<details>
  <summary>Details</summary>
Motivation: 供应链攻击通过恶意Python包日益增多，现有检测方法存在两个关键问题：1) 对对抗性源代码变换缺乏鲁棒性；2) 无法适应不同参与者（从仓库维护者到企业安全团队）对误报率的不同容忍度。

Method: 1) 提出基于细粒度代码混淆的对抗性包生成方法；2) 结合对抗训练提升检测器鲁棒性；3) 设计可适应不同误报率要求的检测系统，分别针对PyPI维护者（0.1% FPR）和企业团队（10% FPR）进行调优。

Result: 1) 对抗训练使检测器鲁棒性提升2.5倍，能多发现10%的混淆包；2) 在PyPI场景：37天内分析91,949个包，每日检测2.48个恶意包，仅2.18个误报；3) 在企业场景：分析1,596个包，每日仅1.24个误报；4) 总共发现346个恶意包并报告给社区。

Conclusion: 该检测器能够无缝集成到公共仓库和企业生态系统中，在保持极低误报审查时间（几分钟）的同时，有效对抗代码混淆攻击，满足不同安全需求场景的检测要求。

Abstract: The rise of supply chain attacks via malicious Python packages demands robust detection solutions. Current approaches, however, overlook two critical challenges: robustness against adversarial source code transformations and adaptability to the varying false positive rate (FPR) requirements of different actors, from repository maintainers (requiring low FPR) to enterprise security teams (higher FPR tolerance).
  We introduce a robust detector capable of seamless integration into both public repositories like PyPI and enterprise ecosystems. To ensure robustness, we propose a novel methodology for generating adversarial packages using fine-grained code obfuscation. Combining these with adversarial training (AT) enhances detector robustness by 2.5x. We comprehensively evaluate AT effectiveness by testing our detector against 122,398 packages collected daily from PyPI over 80 days, showing that AT needs careful application: it makes the detector more robust to obfuscations and allows finding 10% more obfuscated packages, but slightly decreases performance on non-obfuscated packages.
  We demonstrate production adaptability of our detector via two case studies: (i) one for PyPI maintainers (tuned at 0.1% FPR) and (ii) one for enterprise teams (tuned at 10% FPR). In the former, we analyze 91,949 packages collected from PyPI over 37 days, achieving a daily detection rate of 2.48 malicious packages with only 2.18 false positives. In the latter, we analyze 1,596 packages adopted by a multinational software company, obtaining only 1.24 false positives daily. These results show that our detector can be seamlessly integrated into both public repositories like PyPI and enterprise ecosystems, ensuring a very low time budget of a few minutes to review the false positives.
  Overall, we uncovered 346 malicious packages, now reported to the community.

</details>


### [57] [ReFuzz: Reusing Tests for Processor Fuzzing with Contextual Bandits](https://arxiv.org/abs/2512.04436)
*Chen Chen,Zaiyan Xu,Mohamadreza Rostami,David Liu,Dileep Kalathil,Ahmad-Reza Sadeghi,Jeyavijayan,Rajendran*

Main category: cs.CR

TL;DR: ReFuzz是一个基于上下文老虎机的自适应硬件模糊测试框架，能够复用先前处理器中的高效测试来检测新处理器中的类似漏洞，实现了511.23倍的平均覆盖加速和9.33%的额外总覆盖率。


<details>
  <summary>Details</summary>
Motivation: 处理器设计通常基于迭代修改和重用现有设计，这导致相似漏洞在不同处理器中重复出现。现有硬件模糊测试方法缺乏利用已知漏洞知识来指导测试的能力，无法有效检测新处理器中的类似或变种漏洞。

Method: 提出ReFuzz自适应模糊测试框架，采用上下文老虎机算法，智能复用先前处理器中触发漏洞的高效测试，通过变异这些测试来检测被测处理器中的相似漏洞和新变种。

Result: ReFuzz发现了3个新的安全漏洞和2个新的功能bug，其中一个漏洞通过复用已知漏洞的测试检测到。在覆盖效率方面，实现了平均511.23倍的加速和最高9.33%的额外总覆盖率。

Conclusion: ReFuzz通过复用先前处理器的测试知识，有效提高了硬件模糊测试的效率和效果，能够检测相似漏洞和新变种，为处理器安全验证提供了新方法。

Abstract: Processor designs rely on iterative modifications and reuse well-established designs. However, this reuse of prior designs also leads to similar vulnerabilities across multiple processors. As processors grow increasingly complex with iterative modifications, efficiently detecting vulnerabilities from modern processors is critical. Inspired by software fuzzing, hardware fuzzing has recently demonstrated its effectiveness in detecting processor vulnerabilities. Yet, to our best knowledge, existing processor fuzzers fuzz each design individually, lacking the capability to understand known vulnerabilities in prior processors to fine-tune fuzzing to identify similar or new variants of vulnerabilities.
  To address this gap, we present ReFuzz, an adaptive fuzzing framework that leverages contextual bandit to reuse highly effective tests from prior processors to fuzz a processor-under-test (PUT) within a given ISA. By intelligently mutating tests that trigger vulnerabilities in prior processors, ReFuzz effectively detects similar and new variants of vulnerabilities in PUTs. ReFuzz uncovered three new security vulnerabilities and two new functional bugs. ReFuzz detected one vulnerability by reusing a test that triggers a known vulnerability in a prior processor. One functional bug exists across three processors that share design modules. The second bug has two variants. Additionally, ReFuzz reuses highly effective tests to enhance efficiency in coverage, achieving an average 511.23x coverage speedup and up to 9.33% more total coverage, compared to existing fuzzers.

</details>


### [58] [A Light-Weight Large Language Model File Format for Highly-Secure Model Distribution](https://arxiv.org/abs/2512.04580)
*Huifeng Zhu,Shijie Li,Qinfeng Li,Yier Jin*

Main category: cs.CR

TL;DR: CryptoTensors是一种基于Safetensors格式的安全文件结构，通过张量级加密和嵌入式访问控制策略，为大型语言模型的机密分发提供轻量级解决方案。


<details>
  <summary>Details</summary>
Motivation: 随着LLM在医疗、法律、金融等敏感领域的私有化定制，模型权重作为隐私资产或知识产权需要保护。现有模型格式和部署框架缺乏内置的机密性、访问控制和可信硬件集成支持，而现有安全方法要么计算成本高，要么依赖私有基础设施，难以广泛部署。

Method: 基于广泛采用的Safetensors格式进行扩展，构建CryptoTensors文件结构，包含张量级加密和嵌入式访问控制策略，同时保留延迟加载和部分反序列化等关键特性。实现透明解密和自动化密钥管理，支持灵活的许可和安全模型执行。

Result: 开发了概念验证库，在序列化和运行时场景中进行了性能基准测试，验证了与Hugging Face Transformers和vLLM等现有推理框架的兼容性。结果表明CryptoTensors是一种轻量级、高效且开发者友好的解决方案。

Conclusion: CryptoTensors为保护LLM权重在实际和广泛部署中提供了有效的安全解决方案，具有格式兼容性、低开销和易用性等优势。

Abstract: To enhance the performance of large language models (LLMs) in various domain-specific applications, sensitive data such as healthcare, law, and finance are being used to privately customize or fine-tune these models. Such privately adapted LLMs are regarded as either personal privacy assets or corporate intellectual property. Therefore, protecting model weights and maintaining strict confidentiality during deployment and distribution have become critically important. However, existing model formats and deployment frameworks provide little to no built-in support for confidentiality, access control, or secure integration with trusted hardware. Current methods for securing model deployment either rely on computationally expensive cryptographic techniques or tightly controlled private infrastructure. Although these approaches can be effective in specific scenarios, they are difficult and costly for widespread deployment.
  In this paper, we introduce CryptoTensors, a secure and format-compatible file structure for confidential LLM distribution. Built as an extension to the widely adopted Safetensors format, CryptoTensors incorporates tensor-level encryption and embedded access control policies, while preserving critical features such as lazy loading and partial deserialization. It enables transparent decryption and automated key management, supporting flexible licensing and secure model execution with minimal overhead. We implement a proof-of-concept library, benchmark its performance across serialization and runtime scenarios, and validate its compatibility with existing inference frameworks, including Hugging Face Transformers and vLLM. Our results highlight CryptoTensors as a light-weight, efficient, and developer-friendly solution for safeguarding LLM weights in real-world and widespread deployments.

</details>


### [59] [Cryptanalysis of Gleeok-128](https://arxiv.org/abs/2512.04675)
*Siwei Chen,Peipei Xie,Shengyuan Xu,Xiutao Feng,Zejun Xiang,Xiangyong Zeng*

Main category: cs.CR

TL;DR: 对Gleeok-128密码算法的首次第三方全面密码分析，提出了基于MILP的差分-线性区分器构建框架和针对多分支设计的积分攻击密钥恢复框架，发现了原设计中的安全漏洞。


<details>
  <summary>Details</summary>
Motivation: Gleeok是一种低延迟密钥伪随机函数家族，采用三路并行SPN结构。由于其多分支设计，评估安全边界和进行有效的密钥恢复攻击具有挑战性。本文旨在提供对Gleeok-128的首次全面第三方密码分析。

Method: 提出了两阶段MILP框架来构建分支级和全密码的差分-线性区分器，以及专门针对多分支设计的基于积分的密钥恢复框架。通过收紧代数度界限，推导出积分区分器，并发现了Branch 3线性安全评估中的缺陷。

Result: 差分-线性分析获得了7、7、8和4轮区分器，平方相关性分别为2^-88.12、2^-88.12、2^-38.73和2^-49.04。积分分析获得了9、9、7轮分支区分器和7轮全PRF区分器，比设计文档结果分别扩展了3、3、2轮和2轮。这些积分特性支持在非全码本和全码本设置下的7轮和8轮密钥恢复攻击。还发现Branch 3在所有12轮中可被区分，数据复杂度约为2^48。

Conclusion: 本文推进了对Gleeok-128的理解，为分析多分支对称设计提供了通用方法。同时提出了优化的线性层参数，在不牺牲扩散性的情况下显著提高了线性抗性。

Abstract: Gleeok is a family of low latency keyed pseudorandom functions (PRFs) consisting of three parallel SPN based permutations whose outputs are XORed to form the final value. Both Gleeok-128 and Gleeok-256 use a 256 bit key, with block sizes of 128 and 256 bits, respectively. Owing to its multi branch structure, evaluating security margins and mounting effective key recovery attacks present nontrivial challenges. This paper provides the first comprehensive third party cryptanalysis of Gleeok-128. We introduce a two stage MILP based framework for constructing branch wise and full cipher differential linear (DL) distinguishers, together with an integral based key recovery framework tailored to multi branch designs. Our DL analysis yields 7, 7, 8, and 4 round distinguishers for Branch 1, Branch 2, Branch 3, and Gleeok-128, respectively, with squared correlations approximately 2 to the power minus 88.12, 2 to the power minus 88.12, 2 to the power minus 38.73, and 2 to the power minus 49.04, outperforming those in the design document except for the full PRF case. By tightening algebraic degree bounds, we further derive 9, 9, and 7 round integral distinguishers for the three branches and a 7 round distinguisher for the full PRF, extending the designers results by 3, 3, and 2 rounds and by 2 rounds, respectively. These integral properties enable 7 round and 8 round key recovery attacks in the non full codebook and full codebook settings. In addition, we identify a flaw in the original linear security evaluation of Branch 3, showing that it can be distinguished over all 12 rounds with data complexity about 2 to the power 48. We also propose optimized linear layer parameters that significantly improve linear resistance without sacrificing diffusion. Our results advance the understanding of Gleeok-128 and provide general methods for analyzing multi branch symmetric designs.

</details>


### [60] [SoK: a Comprehensive Causality Analysis Framework for Large Language Model Security](https://arxiv.org/abs/2512.04841)
*Wei Zhao,Zhe Li,Jun Sun*

Main category: cs.CR

TL;DR: 本文提出了一个统一的因果分析框架，用于系统研究大语言模型的安全漏洞，特别是越狱攻击。该框架支持从token级到表示级的多层次因果分析，并通过实验验证了安全机制的高度局部化特征。


<details>
  <summary>Details</summary>
Motivation: 大语言模型虽然表现出色，但仍容易受到对抗性操纵（如越狱攻击）的影响。理解这些漏洞背后的因果因素对于构建可靠的防御机制至关重要。

Method: 引入一个统一的因果分析框架，支持token级、神经元级、层级和表示级的多层次因果研究。该框架实现了因果驱动的攻击和防御方法的系统实验与比较。

Result: 实验发现：(1) 对因果关键组件的针对性干预可以可靠地改变安全行为；(2) 安全相关机制高度局部化（集中在早期到中间层，仅1-2%的神经元具有因果影响）；(3) 从框架中提取的因果特征在多种威胁类型上达到超过95%的检测准确率。

Conclusion: 该框架通过连接理论因果分析和实际模型安全，为基于因果的攻击、可解释性以及鲁棒攻击检测和缓解研究建立了可复现的基础。

Abstract: Large Language Models (LLMs) exhibit remarkable capabilities but remain vulnerable to adversarial manipulations such as jailbreaking, where crafted prompts bypass safety mechanisms. Understanding the causal factors behind such vulnerabilities is essential for building reliable defenses.
  In this work, we introduce a unified causality analysis framework that systematically supports all levels of causal investigation in LLMs, ranging from token-level, neuron-level, and layer-level interventions to representation-level analysis. The framework enables consistent experimentation and comparison across diverse causality-based attack and defense methods. Accompanying this implementation, we provide the first comprehensive survey of causality-driven jailbreak studies and empirically evaluate the framework on multiple open-weight models and safety-critical benchmarks including jailbreaks, hallucination detection, backdoor identification, and fairness evaluation. Our results reveal that: (1) targeted interventions on causally critical components can reliably modify safety behavior; (2) safety-related mechanisms are highly localized (i.e., concentrated in early-to-middle layers with only 1--2\% of neurons exhibiting causal influence); and (3) causal features extracted from our framework achieve over 95\% detection accuracy across multiple threat types.
  By bridging theoretical causality analysis and practical model safety, our framework establishes a reproducible foundation for research on causality-based attacks, interpretability, and robust attack detection and mitigation in LLMs. Code is available at https://github.com/Amadeuszhao/SOK_Casuality.

</details>


### [61] [Opacity problems in multi-energy timed automata](https://arxiv.org/abs/2512.04950)
*Étienne André,Lydia Bakiri*

Main category: cs.CR

TL;DR: 研究在具有时间和能量观测的系统中验证不透明性问题，提出带守卫的多能量时间自动机模型，并在多个子类中建立了可判定性结果。


<details>
  <summary>Details</summary>
Motivation: 信息物理系统可能存在信息泄露，特别是在包含时间和能量等连续变量的情况下，这些泄露难以检测。需要研究在同时观测时间和能量信息的系统中验证不透明性问题。

Method: 引入带守卫的多能量时间自动机作为时间自动机的扩展，包含多个能量变量和针对这些变量的守卫。研究攻击者观测不同信息（最终能量、执行时间、每时间单位能量值）时的子类。

Result: 虽然一般形式是不可判定的，但在多个子类中建立了正面结果，特别是当攻击者观测最终能量和/或执行时间时，以及当他们能够每时间单位访问能量变量值时。

Conclusion: 通过引入带守卫的多能量时间自动机模型，为信息物理系统中的信息泄露检测提供了理论框架，并在多个实际相关的观测场景下证明了可判定性，为系统安全验证提供了基础。

Abstract: Cyber-physical systems can be subject to information leakage; in the presence of continuous variables such as time and energy, these leaks can be subtle to detect. We study here the verification of opacity problems over systems with observation over both timing and energy information. We introduce guarded multi-energy timed automata as an extension of timed automata with multiple energy variables and guards over such variables. Despite undecidability of this general formalism, we establish positive results over a number of subclasses, notably when the attacker observes the final energy and/or the execution time, but also when they have access to the value of the energy variables every time unit.

</details>
