<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 10]
- [cs.CR](#cs.CR) [Total: 18]
- [cs.AI](#cs.AI) [Total: 36]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [Black-Box Test Code Fault Localization Driven by Large Language Models and Execution Estimation](https://arxiv.org/abs/2506.19045)
*Ahmadreza Saboor Yaraghi,Golnaz Gharachorlu,Sakina Fatima,Lionel C. Briand,Ruiyuan Wan,Ruifeng Gao*

Main category: cs.SE

TL;DR: 提出了一种基于LLM的静态方法，用于定位系统测试代码中的故障，无需执行测试用例，通过修剪执行轨迹和错误消息提示LLM，显著提高了效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 现有故障定位方法依赖重复执行，对非确定性故障或高成本执行不实用，且忽视了测试代码中的故障。

Method: 使用单次失败执行日志估计执行轨迹，结合错误消息提示LLM，通过三种算法修剪轨迹并定位故障。

Result: 修剪后的轨迹与实际轨迹匹配度达90% F1分数，LLM推理时间减少34%，块级定位在Top-3命中率达81%。

Conclusion: 该方法在无需执行的情况下高效定位测试代码故障，块级定位提供了实用平衡。

Abstract: Fault localization (FL) is a critical step in debugging which typically
relies on repeated executions to pinpoint faulty code regions. However,
repeated executions can be impractical in the presence of non-deterministic
failures or high execution costs. While recent efforts have leveraged Large
Language Models (LLMs) to aid execution-free FL, these have primarily focused
on identifying faults in the system under test (SUT) rather than in the often
complex system test code. However, the latter is also important as, in
practice, many failures are triggered by faulty test code. To overcome these
challenges, we introduce a fully static, LLM-driven approach for system test
code fault localization (TCFL) that does not require executing the test case.
Our method uses a single failure execution log to estimate the test's execution
trace through three novel algorithms that identify only code statements likely
involved in the failure. This pruned trace, combined with the error message, is
used to prompt the LLM to rank potential faulty locations. Our black-box,
system-level approach requires no access to the SUT source code and is
applicable to large test scripts that assess full system behavior. We evaluate
our technique at function, block, and line levels using an industrial dataset
of faulty test cases not previously used in pre-training LLMs. Results show
that our best estimated trace closely match actual traces, with an F1 score of
around 90%. Additionally, pruning the complex system test code reduces the
LLM's inference time by up to 34% without any loss in FL performance. Our
results further suggest that block-level TCFL offers a practical balance,
narrowing the search space while preserving useful context, achieving an 81%
hit rate at top-3 (Hit@3).

</details>


### [2] [Dataset of Yul Contracts to Support Solidity Compiler Research](https://arxiv.org/abs/2506.19153)
*Krzysztof Fonal*

Main category: cs.SE

TL;DR: YulCode数据集包含348,840个基于Yul的智能合约实例，约135,013个独特合约，填补了Yul语言数据集的空白。


<details>
  <summary>Details</summary>
Motivation: 为低级别智能合约代码的研究和开发提供真实世界的数据支持，包括机器学习、形式验证等。

Method: 通过编译已部署在以太坊主网上的Solidity源代码生成Yul合约。

Result: YulCode是首个公开的专注于Yul语言的数据集，为低级别合约分析提供了新工具和研究方向。

Conclusion: YulCode填补了智能合约数据集的关键空白，推动了低级别合约分析和生成的研究。

Abstract: The YulCode dataset presents a comprehensive collection of 348,840 Yul-based
smart contract instances, comprising approximately 135,013 unique contracts.
These contracts were generated through the compilation of Solidity source files
that have been deployed on the Ethereum mainnet, making the dataset directly
representative of real-world decentralized applications. YulCode provides a
rich foundation for a variety of research and development tasks, including but
not limited to machine learning applications, formal verification, optimization
analysis, and software engineering tool evaluation in the context of low-level
smart contract code. To the best of our knowledge at the time of writing,
YulCode is the first and only publicly available dataset that focuses
specifically on Yul, an intermediate language designed for the Ethereum Virtual
Machine (EVM). As such, it fills a critical gap in the current ecosystem of
smart contract datasets and opens new avenues for research and tooling aimed at
low-level contract analysis and generation.

</details>


### [3] [Generating and Understanding Tests via Path-Aware Symbolic Execution with LLMs](https://arxiv.org/abs/2506.19287)
*Yaoxuan Wu,Xiaojie Zhou,Ahmad Humayun,Muhammad Ali Gulzar,Miryung Kim*

Main category: cs.SE

TL;DR: PALM结合符号执行与LLM辅助测试生成，通过AST分析枚举路径并生成可执行变体，避免SMT公式转换，提供交互式前端可视化路径覆盖。


<details>
  <summary>Details</summary>
Motivation: 符号执行受限于建模能力和约束求解能力，LLM生成测试输入但缺乏系统路径枚举，PALM结合两者优势。

Method: PALM通过AST分析静态枚举路径，生成带断言的可执行变体，利用LLM解释路径，提供交互前端可视化覆盖。

Result: 用户研究表明PALM前端帮助用户理解路径覆盖，验证测试路径。

Conclusion: PALM有效结合符号执行与LLM，提升测试生成能力，交互前端增强用户体验。

Abstract: Symbolic execution is a widely used technique for test generation, offering
systematic exploration of program paths through constraint solving. However, it
is fundamentally constrained by the capability to model the target code
including library functions in terms of symbolic constraint and the capability
of underlying constraint solvers. As a result, many paths involving complex
features remain unanalyzed or insufficiently modeled. Recent advances in large
language models (LLMs) have shown promise in generating diverse and valid test
inputs. Yet, LLMs lack mechanisms for systematically enumerating program paths
and often fail to cover subtle corner cases. We observe that directly prompting
an LLM with the full program leads to missed coverage of interesting paths. In
this paper, we present PALM, a test generation system that combines symbolic
path enumeration with LLM-assisted test generation. PALM statically enumerates
possible paths through AST-level analysis and transforms each into an
executable variant with embedded assertions that specify the target path. This
avoids the need to translate path constraints into SMT formulae, by instead
constructing program variants that LLM can interpret. Importantly, PALM is the
first to provide an interactive frontend that visualizes path coverage
alongside generated tests, assembling tests based on the specific paths they
exercise. A user study with 12 participants demonstrates that PALM's frontend
helps users better understand path coverage and identify which paths are
actually exercised by PALM-generated tests, through verification and
visualization of their path profiles.

</details>


### [4] [What Makes the Best Decomposition? Investigating Binary Decomposition Under FCG Variance](https://arxiv.org/abs/2506.19425)
*Ang Jia,He Jiang,Zhilei Ren,Xiaochen Li,Ming Fan,Ting Liu*

Main category: cs.SE

TL;DR: 本文研究了不同编译设置对函数调用图（FCG）的影响，发现FCG变化显著，现有二进制分解方法在跨编译器评估中面临挑战。作者提出了一种识别最优分解的方法。


<details>
  <summary>Details</summary>
Motivation: 现有二进制分解方法依赖于重用代码具有相似函数调用关系，但实际中FCG因编译设置差异变化显著，导致方法失效。

Method: 构建了一个由17种编译器、6种优化和4种架构编译的数据集，分析FCG的变化和映射关系，并评估现有方法在FCG变化下的表现。

Result: FCG大小变化显著，但仍可通过三种映射关系链接；现有方法在跨编译器评估中表现不佳。

Conclusion: 提出了一种识别最优分解的方法，现有方法在覆盖率和社区相似性稳定性上存在问题。

Abstract: Binary decomposition, which decomposes binary files into modules, plays a
critical role in binary reuse detection. Existing binary decomposition works
either apply anchor-based methods by extending anchor functions to generate
modules, or apply clustering-based methods by using clustering algorithms to
group binary functions, which all rely on that reused code shares similar
function call relationships. However, we find that function call graphs (FCGs)
vary a lot when using different compilation settings, especially with diverse
function inlining decisions.
  In this work, we conduct the first systematic empirical study on the variance
of FCGs compiled by various compilation settings and explore its effect on
binary decomposition methods. We first construct a dataset compiled by 17
compilers, using 6 optimizations to 4 architectures and analyze the changes and
mappings of the FCGs. We find that the size of FCGs changes dramatically, while
the FCGs are still linked by three different kinds of mappings. Then we
evaluate the existing works under the FCG variance, and results show that
existing works are facing great challenges when conducting cross-compiler
evaluation with diverse optimization settings. Finally, we propose a method to
identify the optimal decomposition and compare the existing decomposition works
with the optimal decomposition. Existing works either suffer from low coverage
or cannot generate stable community similarities.

</details>


### [5] [LLM-based Multi-Agent System for Intelligent Refactoring of Haskell Code](https://arxiv.org/abs/2506.19481)
*Shahbaz Siddeeq,Muhammad Waseem,Zeeshan Rasheed,Md Mahade Hasan,Jussi Rasku,Mika Saari,Henri Terho,Kalle Makela,Kai-Kristian Kemell,Pekka Abrahamsson*

Main category: cs.SE

TL;DR: 提出了一种基于大型语言模型（LLM）的多智能体系统，用于自动化Haskell代码重构，显著提升了代码质量、性能和内存优化。


<details>
  <summary>Details</summary>
Motivation: 代码重构是软件开发与维护中的关键但劳动密集型任务，需要自动化工具以减少人工分析负担。

Method: 设计了一个多智能体系统，包含代码分析、重构执行、验证和调试等角色，利用LLM进行结构化且语义准确的重构。

Result: 实验表明，该系统平均降低代码复杂度11.03%，提升代码质量22.46%，性能效率提高13.27%，内存优化达14.57%。

Conclusion: LLM多智能体系统能有效支持函数式编程语言的重构，提升可维护性并推动自动化开发流程。

Abstract: Refactoring is a constant activity in software development and maintenance.
Scale and maintain software systems are based on code refactoring. However,
this process is still labor intensive, as it requires programmers to analyze
the codebases in detail to avoid introducing new defects. In this research, we
put forward a large language model (LLM)-based multi-agent system to automate
the refactoring process on Haskell code. The objective of this research is to
evaluate the effect of LLM-based agents in performing structured and
semantically accurate refactoring on Haskell code. Our proposed multi-agent
system based on specialized agents with distinct roles, including code
analysis, refactoring execution, verification, and debugging. To test the
effectiveness and practical applicability of the multi-agent system, we
conducted evaluations using different open-source Haskell codebases. The
results of the experiments carried out showed that the proposed LLM-based
multi-agent system could average 11.03% decreased complexity in code, an
improvement of 22.46% in overall code quality, and increase performance
efficiency by an average of 13.27%. Furthermore, memory allocation was
optimized by up to 14.57%. These results highlight the ability of LLM-based
multi-agent in managing refactoring tasks targeted toward functional
programming paradigms. Our findings hint that LLM-based multi-agent systems
integration into the refactoring of functional programming languages can
enhance maintainability and support automated development workflows.

</details>


### [6] [Integrating Pair Programming as a Work Practice](https://arxiv.org/abs/2506.19511)
*Nina Haugland Andersen,Anastasiia Tkalich,Nils Brede Moe,Darja Smite,Asgaut Mjølne Söderbom,Ola Hast,Viktoria Stray*

Main category: cs.SE

TL;DR: 研究探讨了结对编程（PP）在团队中采用和持续参与的影响因素，发现多方面的因素（如日常工作的感知贡献、公司态度、任务特性等）是关键。


<details>
  <summary>Details</summary>
Motivation: 随着系统复杂性增加，知识共享和团队协作变得至关重要，但结对编程的采用仍不一致，研究旨在了解其影响因素。

Method: 在挪威一家成熟敏捷公司进行单案例研究，通过两轮访谈和主题分析收集数据。

Result: 发现结对编程的参与受多种因素影响，包括日常工作的感知贡献、公司态度、任务特性等。

Conclusion: 长期参与结对编程需要实践带来的预期效益，并根据团队特点调整实践，研究结果对软件从业者具有参考价值。

Abstract: Context: Pair programming (PP) is more relevant than ever. As modern systems
grow in complexity, knowledge sharing and collaboration across teams have
become essential. However, despite well-documented benefits of PP, its adoption
remains inconsistent across software teams. Objective: This study aims to
understand the factors that facilitate or hinder team members' adoption as well
as lasting engagement in PP. Method: We have conducted an exploratory
single-case study in a mature agile company in Norway. We collected data
through two rounds of interviews with team members in different roles and
performed a thematic analysis of the interviews. Results: Our key finding is
that multiple factors, related to the perceptions of how PP contributes to
daily work, efforts associated with engaging in PP sessions, company and team
attitudes, resources, infrastructure, and task characteristics, affect PP
engagement. Conclusion: Long-term engagement in PP requires expected benefits
with the practice being confirmed in firsthand experiences. Adapting the
practice to each unique team, with insights drawn from collective learning, is
also beneficial. Our findings will be beneficial for software practitioners
seeking to make PP an integrated part of their team's workflow.

</details>


### [7] [Lost in Translation? Converting RegExes for Log Parsing into Dynatrace Pattern Language](https://arxiv.org/abs/2506.19539)
*Julian Fragner,Christian Macho,Bernhard Dieber,Martin Pinzger*

Main category: cs.SE

TL;DR: Reptile是一个工具，结合规则和GPT-4，将正则表达式转换为DPL模式，提升日志分析平台迁移效率。


<details>
  <summary>Details</summary>
Motivation: 企业日志分析平台迁移时，手动转换正则表达式到新语言（如DPL）成本高且易错。

Method: 结合规则转换和GPT-4优化，部分无法完全转换时采用尽力而为策略。

Result: 在946个正则表达式中安全转换73.7%，优化后的F1-score和MCC均超0.91。

Conclusion: Reptile为现代日志分析平台迁移提供了高效且可靠的解决方案。

Abstract: Log files provide valuable information for detecting and diagnosing problems
in enterprise software applications and data centers. Several log analytics
tools and platforms were developed to help filter and extract information from
logs, typically using regular expressions (RegExes). Recent commercial log
analytics platforms provide domain-specific languages specifically designed for
log parsing, such as Grok or the Dynatrace Pattern Language (DPL). However,
users who want to migrate to these platforms must manually convert their
RegExes into the new pattern language, which is costly and error-prone. In this
work, we present Reptile, which combines a rule-based approach for converting
RegExes into DPL patterns with a best-effort approach for cases where a full
conversion is impossible. Furthermore, it integrates GPT-4 to optimize the
obtained DPL patterns. The evaluation with 946 RegExes collected from a large
company shows that Reptile safely converted 73.7% of them. The evaluation of
Reptile's pattern optimization with 23 real-world RegExes showed an F1-score
and MCC above 0.91. These results are promising and have ample practical
implications for companies that migrate to a modern log analytics platform,
such as Dynatrace.

</details>


### [8] [Simulating the Waterfall Model: A Systematic Review](https://arxiv.org/abs/2506.19653)
*Antonios Saravanos*

Main category: cs.SE

TL;DR: 本文通过系统映射研究分析了瀑布模型在计算模拟中的表现，发现尽管敏捷方法主导现代软件开发，瀑布模型仍存在于混合方法中。研究分析了68篇文献，揭示了模拟方法、工具、趋势及对Royce原始模型的忠实度。


<details>
  <summary>Details</summary>
Motivation: 尽管瀑布模型在混合方法中仍有应用，但对其在学术研究中的模拟表现缺乏关注。

Method: 系统搜索了2000-2024年的68篇文献，从模拟方法、工具、地理时间趋势及对Royce模型的忠实度四个维度分析。

Result: 离散事件模拟最常用，近期研究多使用开源工具；无研究完全实现Royce原始模型。

Conclusion: 瀑布模型模拟虽小众但存在，需开发易用工具并探索与现代混合方法的结合。

Abstract: This systematic mapping study examines how the Waterfall Model has been
represented in computational simulations within peer-reviewed literature. While
Agile methodologies dominate contemporary software design practices, the
Waterfall Model persists, particularly, within hybrid approaches that fuse
structured, sequential workflows with the adaptability of agile practices.
Despite its continued presence, little attention has been given to how the
Waterfall Model is simulated in research contexts. A structured search of major
academic databases identified 68 peer-reviewed studies published between 2000
and 2024. After applying inclusion criteria, selected studies were analyzed
across four dimensions: (1) simulation methodologies (e.g., discrete-event
simulation, system dynamics), (2) platforms and tools (e.g., Simphony.NET,
SimPy), (3) geographic and temporal trends, and (4) fidelity to Royce's
original seven-phase model. Discrete-event simulation was most commonly used,
reflecting the model's sequential nature. Early work relied on proprietary
platforms, while recent studies increasingly use open-source, Python-based
tools. No studies fully implemented Royce's original formulation, most employed
adaptations. These findings suggest that although niche, simulation of the
Waterfall Model is present in academic discourse. This work highlights the need
for accessible modeling tools and calls for future research that integrates the
waterfall software process model with modern hybrid practices.

</details>


### [9] [Adaptive Request Scheduling for CodeLLM Serving with SLA Guarantees](https://arxiv.org/abs/2506.19677)
*Shi Chang,Boyuan Chen,Kishanthan Thangarajah,Hanan Lutfiyya,Ahmed E. Hassan*

Main category: cs.SE

TL;DR: SABER是一种动态批处理策略，通过实时调整决策提升CodeLLM服务性能，显著提高吞吐量并减少延迟波动。


<details>
  <summary>Details</summary>
Motivation: 现有LLM服务系统使用静态批处理配置，无法适应请求率波动或异构工作负载，导致SLA违规和性能不稳定。

Method: 提出SABER，动态预测每个请求的SLA可行性并实时调整批处理决策。

Result: SABER比最佳静态配置提高吞吐量26%，减少延迟波动45%，无需手动调整或重启服务。

Conclusion: SLA感知的自适应调度是实现高性能CodeLLM服务的关键。

Abstract: Code Large Language Models (CodeLLMs) are increasingly integrated into modern
software development workflows, yet efficiently serving them in
resource-constrained, self-hosted environments remains a significant challenge.
Existing LLM serving systems employs Continuous Batching for throughput
improvement. However, they rely on static batch size configurations that cannot
adapt to fluctuating request rates or heterogeneous workloads, leading to
frequent SLA (Service Level Agreement) violations and unstable performance. In
this study, We propose SABER, a dynamic batching strategy that predicts
per-request SLA feasibility and adjusts decisions in real time. SABER improves
goodput by up to 26% over the best static configurations and reduces latency
variability by up to 45%, all without manual tuning or service restarts. Our
results demonstrate that SLA-aware, adaptive scheduling is key to robust,
high-performance CodeLLM serving.

</details>


### [10] [Exploring Developer Experience Factors in Software Ecosystems](https://arxiv.org/abs/2506.19757)
*Rodrigo Oliveira Zacarias,Léo Carvalho Ramos Antunes,Márcio de Oliveira Barros,Rodrigo Pereira dos Santos,Patricia Lago*

Main category: cs.SE

TL;DR: 该研究通过系统映射研究和德尔菲研究，确定了影响开发者体验（DX）的关键因素，包括财务成本、技术资源、市场进入门槛和财务收益，为SECO平台的可持续发展提供了指导。


<details>
  <summary>Details</summary>
Motivation: 开发者体验（DX）对SECO平台的成功至关重要，但目前缺乏对其关键影响因素的明确认识。

Method: 采用系统映射研究（SMS）分析29项研究，并通过德尔菲研究评估21位第三方开发者对27个DX因素的看法。

Result: 影响开发者采用和持续贡献的主要因素是财务成本、技术资源、低市场进入门槛和财务收益。

Conclusion: DX是SECO成功的关键，研究结果为解决第三方开发者的DX问题提供了实用建议。

Abstract: Context: Developer experience (DX) plays a key role in developers'
performance and their continued involvement in a software ecosystem (SECO)
platform. While researchers and practitioners have recognized several factors
affecting DX in SECO platforms, a clear roadmap of the most influential factors
is still missing. This is particularly important given the direct impact on
developers' interest in SECO and their ongoing engagement with the common
technological platform. Goal: This work aims to identify key DX factors and
understand how they influence third-party developers' decisions to adopt and
keep contributing to a SECO. Methods: We conducted a systematic mapping study
(SMS), analyzing 29 studies to assess the state-of-the-art of DX in SECO.
Additionally, we conducted a Delphi study to evaluate the influence of 27 DX
factors (identified in our SMS) from the perspective of 21 third-party
developers to adopt and keep contributing to a SECO. Results: The factors that
most strongly influence developers' adoption and ongoing contributions to a
SECO are: financial costs for using the platform, desired technical resources
for development, low barriers to entry into the applications market, and more
financial gains. Conclusion: DX is essential for the success and sustainability
of SECO. Our set of DX factors provides valuable insights and recommendations
for researchers and practitioners to address key DX concerns from the
perspective of third-party developers.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [11] [Trustworthy Artificial Intelligence for Cyber Threat Analysis](https://arxiv.org/abs/2506.19052)
*Shuangbao Paul Wang,Paul Mullin*

Main category: cs.CR

TL;DR: 论文提出了一种基于机器学习的网络安全威胁检测工具，结合无监督和监督学习，能够高效识别威胁。


<details>
  <summary>Details</summary>
Motivation: 人工智能技术虽带来创新，但算法中的偏见和不道德问题降低了其可信度。研究旨在利用机器学习和量子力学优势，提升威胁检测的准确性和速度。

Method: 采用两阶段（无监督和监督学习）分析方法，对AWS云服务器日志数据进行处理。

Result: 算法能够以高置信度识别网络威胁。

Conclusion: 研究展示了机器学习在网络安全领域的潜力，尤其是结合量子力学可能带来更高效的分类速度。

Abstract: Artificial Intelligence brings innovations into the society. However, bias
and unethical exist in many algorithms that make the applications less
trustworthy. Threats hunting algorithms based on machine learning have shown
great advantage over classical methods. Reinforcement learning models are
getting more accurate for identifying not only signature-based but also
behavior-based threats. Quantum mechanics brings a new dimension in improving
classification speed with exponential advantage. In this research, we developed
a machine learning based cyber threat detection and assessment tool. It uses
two stage, unsupervised and supervised learning, analyzing method on log data
recorded from a web server on AWS cloud. The results show the algorithm has the
ability to identify cyber threats with high confidence.

</details>


### [12] [PolyGuard: Massive Multi-Domain Safety Policy-Grounded Guardrail Dataset](https://arxiv.org/abs/2506.19054)
*Mintong Kang,Zhaorun Chen,Chejian Xu,Jiawei Zhang,Chengquan Guo,Minzhou Pan,Ivan Revilla,Yu Sun,Bo Li*

Main category: cs.CR

TL;DR: PolyGuard是一个多领域安全策略基础的护栏数据集，填补了现有护栏基准的不足，通过广泛领域覆盖、策略基础风险构建和对抗性攻击实例，评估了19个先进护栏模型的表现。


<details>
  <summary>Details</summary>
Motivation: 现有护栏基准缺乏基于标准化安全策略的原则性基础，且忽视领域特定风险，PolyGuard旨在解决这些问题。

Method: PolyGuard提供多领域覆盖、策略基础风险构建、多样化交互格式、良性数据优化和对抗性攻击实例，并基于此评估19个护栏模型。

Result: 评估发现模型在不同风险类别表现不一，领域覆盖有限，且对对抗性攻击脆弱。

Conclusion: PolyGuard及其评估结果将推动政策对齐和弹性护栏系统的发展。

Abstract: As LLMs become widespread across diverse applications, concerns about the
security and safety of LLM interactions have intensified. Numerous guardrail
models and benchmarks have been developed to ensure LLM content safety.
However, existing guardrail benchmarks are often built upon ad hoc risk
taxonomies that lack a principled grounding in standardized safety policies,
limiting their alignment with real-world operational requirements. Moreover,
they tend to overlook domain-specific risks, while the same risk category can
carry different implications across different domains. To bridge these gaps, we
introduce PolyGuard, the first massive multi-domain safety policy-grounded
guardrail dataset. PolyGuard offers: (1) broad domain coverage across eight
safety-critical domains, such as finance, law, and codeGen; (2) policy-grounded
risk construction based on authentic, domain-specific safety guidelines; (3)
diverse interaction formats, encompassing declarative statements, questions,
instructions, and multi-turn conversations; (4) advanced benign data curation
via detoxification prompting to challenge over-refusal behaviors; and (5)
\textbf{attack-enhanced instances} that simulate adversarial inputs designed to
bypass guardrails. Based on PolyGuard, we benchmark 19 advanced guardrail
models and uncover a series of findings, such as: (1) All models achieve varied
F1 scores, with many demonstrating high variance across risk categories,
highlighting their limited domain coverage and insufficient handling of
domain-specific safety concerns; (2) As models evolve, their coverage of safety
risks broadens, but performance on common risk categories may decrease; (3) All
models remain vulnerable to optimized adversarial attacks. We believe that
\dataset and the unique insights derived from our evaluations will advance the
development of policy-aligned and resilient guardrail systems.

</details>


### [13] [Enhancing Security in LLM Applications: A Performance Evaluation of Early Detection Systems](https://arxiv.org/abs/2506.19109)
*Valerii Gakh,Hayretdin Bahsi*

Main category: cs.CR

TL;DR: 论文研究了提示注入攻击对LLM应用的威胁，分析了现有检测技术的性能，并比较了LLM Guard、Vigil和Rebuff的效果，提出了改进建议。


<details>
  <summary>Details</summary>
Motivation: 随着LLM应用的普及，提示注入攻击威胁系统安全，现有防御措施不足，需研究检测技术的有效性。

Method: 分析了开源解决方案中的提示泄漏检测技术，比较了LLM Guard、Vigil和Rebuff的性能，并提出改进建议。

Result: 发现Vigil和Rebuff的检测方法存在不足，LLM Guard表现最佳；Vigil适合低误报率场景，Rebuff适合一般需求。

Conclusion: 需优化现有检测技术，Vigil和Rebuff各有适用场景，未来研究应关注技术改进。

Abstract: Prompt injection threatens novel applications that emerge from adapting LLMs
for various user tasks. The newly developed LLM-based software applications
become more ubiquitous and diverse. However, the threat of prompt injection
attacks undermines the security of these systems as the mitigation and defenses
against them, proposed so far, are insufficient. We investigated the
capabilities of early prompt injection detection systems, focusing specifically
on the detection performance of techniques implemented in various open-source
solutions. These solutions are supposed to detect certain types of prompt
injection attacks, including the prompt leak. In prompt leakage attacks, an
attacker maliciously manipulates the LLM into outputting its system
instructions, violating the system's confidentiality. Our study presents
analyzes of distinct prompt leakage detection techniques, and a comparative
analysis of several detection solutions, which implement those techniques. We
identify the strengths and weaknesses of these techniques and elaborate on
their optimal configuration and usage in high-stake deployments. In one of the
first studies on existing prompt leak detection solutions, we compared the
performances of LLM Guard, Vigil, and Rebuff. We concluded that the
implementations of canary word checks in Vigil and Rebuff were not effective at
detecting prompt leak attacks, and we proposed improvements for them. We also
found an evasion weakness in Rebuff's secondary model-based technique and
proposed a mitigation. Then, the result of the comparison of LLM Guard, Vigil,
and Rebuff at their peak performance revealed that Vigil is optimal for cases
when minimal false positive rate is required, and Rebuff is the most optimal
for average needs.

</details>


### [14] [Network Structures as an Attack Surface: Topology-Based Privacy Leakage in Federated Learning](https://arxiv.org/abs/2506.19260)
*Murtaza Rangwala,Richard O. Sinnott,Rajkumar Buyya*

Main category: cs.CR

TL;DR: 该论文首次全面分析了联邦学习系统中基于网络拓扑的隐私泄露问题，揭示了即使在强差分隐私保护下，攻击者仍能通过拓扑知识推断敏感数据分布模式。


<details>
  <summary>Details</summary>
Motivation: 现有隐私研究主要关注基于梯度的攻击，而网络拓扑知识的隐私影响尚未充分研究。

Method: 通过系统评估4,720个攻击实例，分析了六种不同的对抗知识场景，并提出了三种攻击向量。

Result: 在完全知识条件下，攻击成功率分别为84.1%、65.0%和47.2%；80%的部分知识场景攻击效果仍高于安全阈值。

Conclusion: 网络拓扑是联邦学习系统的根本隐私漏洞，但通过拓扑感知防御机制（如结构噪声注入）可以有效缓解。

Abstract: Federated learning systems increasingly rely on diverse network topologies to
address scalability and organizational constraints. While existing privacy
research focuses on gradient-based attacks, the privacy implications of network
topology knowledge remain critically understudied. We conduct the first
comprehensive analysis of topology-based privacy leakage across realistic
adversarial knowledge scenarios, demonstrating that adversaries with varying
degrees of structural knowledge can infer sensitive data distribution patterns
even under strong differential privacy guarantees. Through systematic
evaluation of 4,720 attack instances, we analyze six distinct adversarial
knowledge scenarios: complete topology knowledge and five partial knowledge
configurations reflecting real-world deployment constraints. We propose three
complementary attack vectors: communication pattern analysis, parameter
magnitude profiling, and structural position correlation, achieving success
rates of 84.1%, 65.0%, and 47.2% under complete knowledge conditions.
Critically, we find that 80% of realistic partial knowledge scenarios maintain
attack effectiveness above security thresholds, with certain partial knowledge
configurations achieving performance superior to the baseline complete
knowledge scenario. To address these vulnerabilities, we propose and
empirically validate structural noise injection as a complementary defense
mechanism across 808 configurations, demonstrating up to 51.4% additional
attack reduction when properly layered with existing privacy techniques. These
results establish that network topology represents a fundamental privacy
vulnerability in federated learning systems while providing practical pathways
for mitigation through topology-aware defense mechanisms.

</details>


### [15] [WebGuard++:Interpretable Malicious URL Detection via Bidirectional Fusion of HTML Subgraphs and Multi-Scale Convolutional BERT](https://arxiv.org/abs/2506.19356)
*Ye Tian,Zhang Yumin,Yifan Jia,Jianguo Sun,Yanbin Wang*

Main category: cs.CR

TL;DR: WebGuard++是一个恶意URL检测框架，通过融合URL和HTML特征，解决了现有方法的四个关键问题，显著提升了检测性能。


<details>
  <summary>Details</summary>
Motivation: 现有恶意URL检测方法存在四个主要问题：URL建模不完整、HTML图稀疏、单向分析以及决策不透明。

Method: WebGuard++包含四个新组件：跨尺度URL编码器、子图感知HTML编码器、双向耦合模块和投票模块。

Result: 实验表明，WebGuard++在固定FPR下，TPR显著提升1.1x-7.9x。

Conclusion: WebGuard++通过多模态特征融合和双向交互，实现了更高效的恶意URL检测。

Abstract: URL+HTML feature fusion shows promise for robust malicious URL detection,
since attacker artifacts persist in DOM structures. However, prior work suffers
from four critical shortcomings: (1) incomplete URL modeling, failing to
jointly capture lexical patterns and semantic context; (2) HTML graph sparsity,
where threat-indicative nodes (e.g., obfuscated scripts) are isolated amid
benign content, causing signal dilution during graph aggregation; (3)
unidirectional analysis, ignoring URL-HTML feature bidirectional interaction;
and (4) opaque decisions, lacking attribution to malicious DOM components. To
address these challenges, we present WebGuard++, a detection framework with 4
novel components: 1) Cross-scale URL Encoder: Hierarchically learns
local-to-global and coarse to fine URL features based on Transformer network
with dynamic convolution. 2) Subgraph-aware HTML Encoder: Decomposes DOM graphs
into interpretable substructures, amplifying sparse threat signals via
Hierarchical feature fusion. 3) Bidirectional Coupling Module: Aligns URL and
HTML embeddings through cross-modal contrastive learning, optimizing
inter-modal consistency and intra-modal specificity. 4) Voting Module:
Localizes malicious regions through consensus voting on malicious subgraph
predictions. Experiments show WebGuard++ achieves significant improvements over
state-of-the-art baselines, achieving 1.1x-7.9x higher TPR at fixed FPR of
0.001 and 0.0001 across both datasets.

</details>


### [16] [SoK: Can Synthetic Images Replace Real Data? A Survey of Utility and Privacy of Synthetic Image Generation](https://arxiv.org/abs/2506.19360)
*Yunsung Chung,Yunbei Zhang,Nassir Marrouche,Jihun Hamm*

Main category: cs.CR

TL;DR: 该论文综述了隐私保护数据合成（PPDS）中的合成图像生成方法，系统分类了生成-采样-分类流程中的方法、隐私攻击及缓解措施，并通过基准测试比较了不同方法的隐私风险与效用。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏对合成图像生成方法的全面调查与比较，特别是在用于训练分类器时，需评估其隐私保护效果与实用性。

Method: 系统分类现有方法，使用模型无关的成员推理攻击（MIA）作为隐私风险衡量标准，并通过基准测试比较不同生成方法。

Result: 研究回答了PPDS中的关键问题，如合成数据能否替代真实数据、最佳发布策略是什么等，提供了实用性与隐私保护的权衡见解。

Conclusion: 该研究为合成数据生成方法的实用性与隐私保护权衡提供了指导，帮助选择最优数据发布策略。

Abstract: Advances in generative models have transformed the field of synthetic image
generation for privacy-preserving data synthesis (PPDS). However, the field
lacks a comprehensive survey and comparison of synthetic image generation
methods across diverse settings. In particular, when we generate synthetic
images for the purpose of training a classifier, there is a pipeline of
generation-sampling-classification which takes private training as input and
outputs the final classifier of interest. In this survey, we systematically
categorize existing image synthesis methods, privacy attacks, and mitigations
along this generation-sampling-classification pipeline. To empirically compare
diverse synthesis approaches, we provide a benchmark with representative
generative methods and use model-agnostic membership inference attacks (MIAs)
as a measure of privacy risk. Through this study, we seek to answer critical
questions in PPDS: Can synthetic data effectively replace real data? Which
release strategy balances utility and privacy? Do mitigations improve the
utility-privacy tradeoff? Which generative models perform best across different
scenarios? With a systematic evaluation of diverse methods, our study provides
actionable insights into the utility-privacy tradeoffs of synthetic data
generation methods and guides the decision on optimal data releasing strategies
for real-world applications.

</details>


### [17] [Yotta: A Large-Scale Trustless Data Trading Scheme for Blockchain System](https://arxiv.org/abs/2506.19368)
*Xiang Liu,Zhanpeng Guo,Liangxi Liu,Mengyao Zheng,Yiming Qiu,Linshan Jiang*

Main category: cs.CR

TL;DR: 本文提出了一种名为Yotta的大规模数据交易方案，首次形式化了Web 3.0中数据交易的属性要求，并通过创新的密码学工作流程结合IPFS和zk-SNARK实现高效交易。


<details>
  <summary>Details</summary>
Motivation: 当前基于区块链智能合约的数据交易方法无法同时支持大规模交易和确保数据安全，且缺乏对大规模数据交易基本属性的讨论。

Method: 提出Yotta方案，利用创新的密码学工作流程结合IPFS和zk-SNARK，设计完整的批量数据交易方案。

Result: 模拟结果显示，Yotta性能优于基线方法高达130倍，并展现出优秀的可扩展性，满足所有属性要求。

Conclusion: Yotta为Web 3.0中的数据交易提供了一种高效且安全的解决方案，填补了当前研究的空白。

Abstract: Data trading is one of the key focuses of Web 3.0. However, all the current
methods that rely on blockchain-based smart contracts for data exchange cannot
support large-scale data trading while ensuring data security, which falls
short of fulfilling the spirit of Web 3.0. Even worse, there is currently a
lack of discussion on the essential properties that large-scale data trading
should satisfy. In this work, we are the first to formalize the property
requirements for enabling data trading in Web 3.0. Based on these requirements,
we are the first to propose Yotta, a complete batch data trading scheme for
blockchain, which features a data trading design that leverages our innovative
cryptographic workflow with IPFS and zk-SNARK. Our simulation results
demonstrate that Yotta outperforms baseline approaches up to 130 times and
exhibits excellent scalability to satisfy all the properties.

</details>


### [18] [ZK-SERIES: Privacy-Preserving Authentication using Temporal Biometric Data](https://arxiv.org/abs/2506.19393)
*Daniel Reijsbergen,Eyasu Getahun Chekole,Howard Halim,Jianying Zhou*

Main category: cs.CR

TL;DR: ZK-SERIES提出了一种隐私高效的时间序列认证协议，适用于低容量设备，实验显示在老旧设备上认证仅需1.3秒。


<details>
  <summary>Details</summary>
Motivation: 生物认证中时间序列数据的隐私保护需求，现有方法在隐私和效率上的不足。

Method: 利用零知识乘法证明和批量范围证明构建ZK-SERIES，优化低容量设备兼容性。

Result: 在老旧手机上实现1.3秒内的隐私保护认证，适用于摇动和吹气认证案例。

Conclusion: ZK-SERIES为时间序列认证提供了高效隐私保护方案，适用于资源受限设备。

Abstract: Biometric authentication relies on physiological or behavioral traits that
are inherent to a user, making them difficult to lose, forge or forget.
Biometric data with a temporal component enable the following authentication
protocol: recent readings of the underlying biometrics are encoded as time
series and compared to a set of base readings. If the distance between the new
readings and the base readings falls within an acceptable threshold, then the
user is successfully authenticated. Various methods exist for comparing time
series data, such as Dynamic Time Warping (DTW) and the Time Warp Edit Distance
(TWED), each offering advantages and drawbacks depending on the context.
Moreover, many of these techniques do not inherently preserve privacy, which is
a critical consideration in biometric authentication due to the complexity of
resetting biometric credentials.
  In this work, we propose ZK-SERIES to provide privacy and efficiency to a
broad spectrum of time series-based authentication protocols. ZK-SERIES uses
the same building blocks, i.e., zero-knowledge multiplication proofs and
efficiently batched range proofs, to ensure consistency across all protocols.
Furthermore, it is optimized for compatibility with low-capacity devices such
as smartphones. To assess the effectiveness of our proposed technique, we
primarily focus on two case studies for biometric authentication: shake-based
and blow-based authentication. To demonstrate ZK-SERIES's practical
applicability even in older and less powerful smartphones, we conduct
experiments on a 5-year-old low-spec smartphone using real data for two case
studies alongside scalability assessments using artificial data. Our
experimental results indicate that the privacy-preserving authentication
protocol can be completed within 1.3 seconds on older devices.

</details>


### [19] [An ETSI GS QKD compliant TLS implementation](https://arxiv.org/abs/2506.19409)
*Thomas Prévost,Bruno Martin,Olivier Alibart*

Main category: cs.CR

TL;DR: 论文提出了一种基于量子密钥分发（QKD）的TLS协议改进方案，保持向后兼容性，并测试了其性能。


<details>
  <summary>Details</summary>
Motivation: 推动QKD在互联网中的广泛应用，提升通信安全性。

Method: 基于Rustls库实现TLS协议的修改，结合ETSI GS QKD 014 v1.1.1标准，并进行视频会议加密测试。

Result: 协议成功实现并测试，性能与TLS 1.3进行了对比。

Conclusion: 改进的TLS协议在保持兼容性的同时，为QKD的普及提供了可行方案。

Abstract: A modification of the TLS protocol is presented, using our implementation of
the Quantum Key Distribution (QKD) standard ETSI GS QKD 014 v1.1.1. We rely on
the Rustls library for this. The TLS protocol is modified while maintaining
backward compatibility on the client and server side. We thus wish to
participate in the effort to generalize the use of QKD on the Internet. We used
our protocol for a video conference call encrypted by QKD. Finally, we analyze
the performance of our protocol, comparing the time needed to establish a
handshake to that of TLS 1.3.

</details>


### [20] [FuncVul: An Effective Function Level Vulnerability Detection Model using LLM and Code Chunk](https://arxiv.org/abs/2506.19453)
*Sajal Halder,Muhammad Ejaz Ahmed,Seyit Camtepe*

Main category: cs.CR

TL;DR: FuncVul是一种基于代码块的函数级漏洞检测模型，专注于C/C++和Python中的关键代码段，显著提升漏洞检测准确性。


<details>
  <summary>Details</summary>
Motivation: 现有方法多关注包或库级别的漏洞，忽略具体函数，而函数级漏洞检测能显著降低开源软件风险。

Method: FuncVul结合GraphCodeBERT微调模型，利用代码块和补丁信息，通过大语言模型标记漏洞样本。

Result: FuncVul在六个数据集上平均准确率达87-92%，F1分数86-92%，比全函数预测提升53.9%准确率和42.0% F1分数。

Conclusion: FuncVul通过代码块级检测显著提升漏洞识别能力，为开源软件安全提供有效工具。

Abstract: Software supply chain vulnerabilities arise when attackers exploit weaknesses
by injecting vulnerable code into widely used packages or libraries within
software repositories. While most existing approaches focus on identifying
vulnerable packages or libraries, they often overlook the specific functions
responsible for these vulnerabilities. Pinpointing vulnerable functions within
packages or libraries is critical, as it can significantly reduce the risks
associated with using open-source software. Identifying vulnerable patches is
challenging because developers often submit code changes that are unrelated to
vulnerability fixes. To address this issue, this paper introduces FuncVul, an
innovative code chunk-based model for function-level vulnerability detection in
C/C++ and Python, designed to identify multiple vulnerabilities within a
function by focusing on smaller, critical code segments. To assess the model's
effectiveness, we construct six code and generic code chunk based datasets
using two approaches: (1) integrating patch information with large language
models to label vulnerable samples and (2) leveraging large language models
alone to detect vulnerabilities in function-level code. To design FuncVul
vulnerability model, we utilise GraphCodeBERT fine tune model that captures
both the syntactic and semantic aspects of code. Experimental results show that
FuncVul outperforms existing state-of-the-art models, achieving an average
accuracy of 87-92% and an F1 score of 86-92% across all datasets. Furthermore,
we have demonstrated that our code-chunk-based FuncVul model improves 53.9%
accuracy and 42.0% F1-score than the full function-based vulnerability
prediction. The FuncVul code and datasets are publicly available on GitHub at
https://github.com/sajalhalder/FuncVul.

</details>


### [21] [PhishingHook: Catching Phishing Ethereum Smart Contracts leveraging EVM Opcodes](https://arxiv.org/abs/2506.19480)
*Pasquale De Rosa,Simon Queyrut,Yérom-David Bromberg,Pascal Felber,Valerio Schiavoni*

Main category: cs.CR

TL;DR: PhishingHook是一个基于机器学习的框架，通过分析智能合约的字节码和操作码来检测钓鱼活动，避免了交易重放的安全问题，并在实验中达到约90%的平均准确率。


<details>
  <summary>Details</summary>
Motivation: 以太坊虚拟机的广泛使用导致钓鱼活动增加，现有方法通过交易重放分析存在安全风险，因此需要一种更安全、高效的检测方法。

Method: PhishingHook直接分析智能合约的字节码和操作码，应用16种机器学习技术（分为四类）进行钓鱼行为检测。

Result: 在7,000个真实恶意智能合约上测试，平均准确率约为90%。

Conclusion: PhishingHook提供了一种高效且安全的钓鱼检测方法，支持实验复现，并公开了代码和数据集。

Abstract: The Ethereum Virtual Machine (EVM) is a decentralized computing engine. It
enables the Ethereum blockchain to execute smart contracts and decentralized
applications (dApps). The increasing adoption of Ethereum sparked the rise of
phishing activities. Phishing attacks often target users through deceptive
means, e.g., fake websites, wallet scams, or malicious smart contracts, aiming
to steal sensitive information or funds. A timely detection of phishing
activities in the EVM is therefore crucial to preserve the user trust and
network integrity. Some state-of-the art approaches to phishing detection in
smart contracts rely on the online analysis of transactions and their traces.
However, replaying transactions often exposes sensitive user data and
interactions, with several security concerns. In this work, we present
PhishingHook, a framework that applies machine learning techniques to detect
phishing activities in smart contracts by directly analyzing the contract's
bytecode and its constituent opcodes. We evaluate the efficacy of such
techniques in identifying malicious patterns, suspicious function calls, or
anomalous behaviors within the contract's code itself before it is deployed or
interacted with. We experimentally compare 16 techniques, belonging to four
main categories (Histogram Similarity Classifiers, Vision Models, Language
Models and Vulnerability Detection Models), using 7,000 real-world malware
smart contracts. Our results demonstrate the efficiency of PhishingHook in
performing phishing classification systems, with about 90% average accuracy
among all the models. We support experimental reproducibility, and we release
our code and datasets to the research community.

</details>


### [22] [PrivacyXray: Detecting Privacy Breaches in LLMs through Semantic Consistency and Probability Certainty](https://arxiv.org/abs/2506.19563)
*Jinwen He,Yiyang Lu,Zijin Lin,Kai Chen,Yue Zhao*

Main category: cs.CR

TL;DR: PrivacyXray框架通过分析LLM内部状态检测隐私泄露，无需依赖外部数据集，平均准确率达92.69%。


<details>
  <summary>Details</summary>
Motivation: LLMs在敏感领域广泛应用，但隐私提取攻击无法验证信息准确性，缺乏公开数据集验证。

Method: 提出PrivacyXray，基于LLM内部状态（语义一致性和概率确定性）设计四种指标检测隐私泄露。

Result: 实验显示PrivacyXray平均准确率92.69%，比现有方法提升20.06%。

Conclusion: PrivacyXray解决了隐私检测中的关键挑战，具有实际应用价值。

Abstract: Large Language Models (LLMs) are widely used in sensitive domains, including
healthcare, finance, and legal services, raising concerns about potential
private information leaks during inference. Privacy extraction attacks, such as
jailbreaking, expose vulnerabilities in LLMs by crafting inputs that force the
models to output sensitive information. However, these attacks cannot verify
whether the extracted private information is accurate, as no public datasets
exist for cross-validation, leaving a critical gap in private information
detection during inference. To address this, we propose PrivacyXray, a novel
framework detecting privacy breaches by analyzing LLM inner states. Our
analysis reveals that LLMs exhibit higher semantic coherence and probabilistic
certainty when generating correct private outputs. Based on this, PrivacyXray
detects privacy breaches using four metrics: intra-layer and inter-layer
semantic similarity, token-level and sentence-level probability distributions.
PrivacyXray addresses critical challenges in private information detection by
overcoming the lack of open-source private datasets and eliminating reliance on
external data for validation. It achieves this through the synthesis of
realistic private data and a detection mechanism based on the inner states of
LLMs. Experiments show that PrivacyXray achieves consistent performance, with
an average accuracy of 92.69% across five LLMs. Compared to state-of-the-art
methods, PrivacyXray achieves significant improvements, with an average
accuracy increase of 20.06%, highlighting its stability and practical utility
in real-world applications.

</details>


### [23] [Decompiling Smart Contracts with a Large Language Model](https://arxiv.org/abs/2506.19624)
*Isaac David,Liyi Zhou,Dawn Song,Arthur Gervais,Kaihua Qin*

Main category: cs.CR

TL;DR: 论文提出了一种利用大型语言模型（LLM）将EVM字节码转换为可读且语义准确的Solidity代码的新方法，显著提升了区块链智能合约的安全性和可读性。


<details>
  <summary>Details</summary>
Motivation: 区块链浏览器（如Etherscan）上大多数智能合约未开源，导致安全风险。现有反编译器生成的代码可读性差，阻碍漏洞分析和安全审计。

Method: 通过静态程序分析将字节码转换为三地址码（TAC），再使用微调的Llama-3.2-3B模型将TAC转换为高质量的Solidity代码。

Result: 新方法在语义相似性和可读性上显著优于传统反编译器，平均语义相似度为0.82。

Conclusion: 该方法为智能合约安全分析提供了实用工具，已公开在https://evmdecompiler.com上。

Abstract: The widespread lack of broad source code verification on blockchain explorers
such as Etherscan, where despite 78,047,845 smart contracts deployed on
Ethereum (as of May 26, 2025), a mere 767,520 (< 1%) are open source, presents
a severe impediment to blockchain security. This opacity necessitates the
automated semantic analysis of on-chain smart contract bytecode, a fundamental
research challenge with direct implications for identifying vulnerabilities and
understanding malicious behavior. Prevailing decompilers struggle to reverse
bytecode in a readable manner, often yielding convoluted code that critically
hampers vulnerability analysis and thwarts efforts to dissect contract
functionalities for security auditing.
  This paper addresses this challenge by introducing a pioneering decompilation
pipeline that, for the first time, successfully leverages Large Language Models
(LLMs) to transform Ethereum Virtual Machine (EVM) bytecode into human-readable
and semantically faithful Solidity code. Our novel methodology first employs
rigorous static program analysis to convert bytecode into a structured
three-address code (TAC) representation. This intermediate representation then
guides a Llama-3.2-3B model, specifically fine-tuned on a comprehensive dataset
of 238,446 TAC-to-Solidity function pairs, to generate high-quality Solidity.
This approach uniquely recovers meaningful variable names, intricate control
flow, and precise function signatures. Our extensive empirical evaluation
demonstrates a significant leap beyond traditional decompilers, achieving an
average semantic similarity of 0.82 with original source and markedly superior
readability. The practical viability and effectiveness of our research are
demonstrated through its implementation in a publicly accessible system,
available at https://evmdecompiler.com.

</details>


### [24] [A Survey of LLM-Driven AI Agent Communication: Protocols, Security Risks, and Defense Countermeasures](https://arxiv.org/abs/2506.19676)
*Dezhang Kong,Shi Lin,Zhenhua Xu,Zhebo Wang,Minghao Li,Yufeng Li,Yilun Zhang,Zeyang Sha,Yuyuan Li,Changting Lin,Xun Wang,Xuan Liu,Muhammad Khurram Khan,Ningyu Zhang,Chaochao Chen,Meng Han*

Main category: cs.CR

TL;DR: 本文综述了AI代理通信安全的现状，分析了其生命周期中的三个阶段及其安全风险，并提出了防御对策和未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 随着AI代理通信的快速发展，其安全问题日益凸显，亟需系统性的研究和解决方案。

Method: 将代理通信生命周期分为三个阶段（用户-代理、代理-代理、代理-环境），分析各阶段协议及安全风险，并提出防御对策。

Result: 揭示了代理通信各阶段的安全隐患，总结了可能的防御措施。

Conclusion: 代理通信安全是一个新兴且重要的研究领域，未来需进一步探索开放问题和方向。

Abstract: In recent years, Large-Language-Model-driven AI agents have exhibited
unprecedented intelligence, flexibility, and adaptability, and are rapidly
changing human production and lifestyle. Nowadays, agents are undergoing a new
round of evolution. They no longer act as an isolated island like LLMs.
Instead, they start to communicate with diverse external entities, such as
other agents and tools, to collectively perform more complex tasks. Under this
trend, agent communication is regarded as a foundational pillar of the future
AI ecosystem, and many organizations intensively begin to design related
communication protocols (e.g., Anthropic's MCP and Google's A2A) within the
recent few months. However, this new field exposes significant security hazard,
which can cause severe damage to real-world scenarios. To help researchers to
quickly figure out this promising topic and benefit the future agent
communication development, this paper presents a comprehensive survey of agent
communication security. More precisely, we first present a clear definition of
agent communication and categorize the entire lifecyle of agent communication
into three stages: user-agent interaction, agent-agent communication, and
agent-environment communication. Next, for each communication phase, we dissect
related protocols and analyze its security risks according to the communication
characteristics. Then, we summarize and outlook on the possible defense
countermeasures for each risk. Finally, we discuss open issues and future
directions in this promising research field.

</details>


### [25] [KnowML: Improving Generalization of ML-NIDS with Attack Knowledge Graphs](https://arxiv.org/abs/2506.19802)
*Xin Fan Guo,Albert Merono Penuela,Sergio Maffeis,Fabio Pierazzi*

Main category: cs.CR

TL;DR: KnowML框架通过知识图谱和大型语言模型增强ML-NIDS，显著提升对多样化攻击变体的检测能力。


<details>
  <summary>Details</summary>
Motivation: 现有ML-NIDS依赖同质数据集和人工设计，导致性能虚高且泛化能力不足。

Method: 提出KnowML框架，利用LLMs构建攻击知识图谱，并通过符号推理生成知识增强输入。

Result: 在28种攻击变体上测试，KnowML的F1分数高达99%，而基线模型表现极差。

Conclusion: 知识引导的ML-NIDS设计显著优于传统方法，具备更强的泛化能力和检测效果。

Abstract: Despite extensive research on Machine Learning-based Network Intrusion
Detection Systems (ML-NIDS), their capability to detect diverse attack variants
remains uncertain. Prior studies have largely relied on homogeneous datasets,
which artificially inflate performance scores and offer a false sense of
security. Designing systems that can effectively detect a wide range of attack
variants remains a significant challenge. The progress of ML-NIDS continues to
depend heavily on human expertise, which can embed subjective judgments of
system designers into the model, potentially hindering its ability to
generalize across diverse attack types.
  To address this gap, we propose KnowML, a framework for knowledge-guided
machine learning that integrates attack knowledge into ML-NIDS. KnowML
systematically explores the threat landscape by leveraging Large Language
Models (LLMs) to perform automated analysis of attack implementations. It
constructs a unified Knowledge Graph (KG) of attack strategies, on which it
applies symbolic reasoning to generate KG-Augmented Input, embedding domain
knowledge directly into the design process of ML-NIDS.
  We evaluate KnowML on 28 realistic attack variants, of which 10 are newly
collected for this study. Our findings reveal that baseline ML-NIDS models fail
to detect several variants entirely, achieving F1 scores as low as 0 %. In
contrast, our knowledge-guided approach achieves up to 99 % F1 score while
maintaining a False Positive Rate below 0.1 %.

</details>


### [26] [Machine Learning with Privacy for Protected Attributes](https://arxiv.org/abs/2506.19836)
*Saeed Mahloujifar,Chuan Guo,G. Edward Suh,Kamalika Chaudhuri*

Main category: cs.CR

TL;DR: 论文提出了一种称为特征差分隐私（FDP）的新框架，通过细化差分隐私定义，针对特定保护属性提供更灵活的隐私保护，显著提升了模型效用。


<details>
  <summary>Details</summary>
Motivation: 传统差分隐私（DP）在仅需保护特定属性时可能导致不必要的效用损失，因此需要更灵活的隐私保护框架。

Method: 提出FDP框架，支持添加/删除和替换隐私变体，并处理保护与非保护特征的自适应分离。改进DP-SGD算法以满足FDP。

Result: 在AFHQ数据集上，FDP显著提升模型效用（FID从286.7降至101.9，ε=8）。

Conclusion: FDP为私有数据分析提供了新方法，在保持强隐私保证的同时降低了效用成本。

Abstract: Differential privacy (DP) has become the standard for private data analysis.
Certain machine learning applications only require privacy protection for
specific protected attributes. Using naive variants of differential privacy in
such use cases can result in unnecessary degradation of utility. In this work,
we refine the definition of DP to create a more general and flexible framework
that we call feature differential privacy (FDP). Our definition is
simulation-based and allows for both addition/removal and replacement variants
of privacy, and can handle arbitrary and adaptive separation of protected and
non-protected features. We prove the properties of FDP, such as adaptive
composition, and demonstrate its implications for limiting attribute inference
attacks. We also propose a modification of the standard DP-SGD algorithm that
satisfies FDP while leveraging desirable properties such as amplification via
sub-sampling. We apply our framework to various machine learning tasks and show
that it can significantly improve the utility of DP-trained models when public
features are available. For example, we train diffusion models on the AFHQ
dataset of animal faces and observe a drastic improvement in FID compared to
DP, from 286.7 to 101.9 at $\epsilon=8$, assuming that the blurred version of a
training image is available as a public feature. Overall, our work provides a
new approach to private data analysis that can help reduce the utility cost of
DP while still providing strong privacy guarantees.

</details>


### [27] [On the efficacy of old features for the detection of new bots](https://arxiv.org/abs/2506.19635)
*Rocco De Nicola,Marinella Petrocchi,Manuel Pratelli*

Main category: cs.CR

TL;DR: 本文比较了四种先进特征集在检测新型机器人（bots）上的表现，探讨了低成本特征和通用分类器在检测进化机器人中的潜力。


<details>
  <summary>Details</summary>
Motivation: 研究动机是解决恶意机器人对在线生态系统的负面影响，如传播垃圾邮件和操纵公众舆论。

Method: 方法包括使用Twitter作为基准，比较四种特征集的性能，包括Botometer的输出分数、账户资料和时间线特征，以及Twitter客户端信息。

Result: 结果表明，通用分类器和低成本计算的特征可能有效检测进化机器人。

Conclusion: 结论指出，低成本特征和通用分类器在检测新型机器人方面具有潜力。

Abstract: For more than a decade now, academicians and online platform administrators
have been studying solutions to the problem of bot detection. Bots are computer
algorithms whose use is far from being benign: malicious bots are purposely
created to distribute spam, sponsor public characters and, ultimately, induce a
bias within the public opinion. To fight the bot invasion on our online
ecosystem, several approaches have been implemented, mostly based on
(supervised and unsupervised) classifiers, which adopt the most varied account
features, from the simplest to the most expensive ones to be extracted from the
raw data obtainable through the Twitter public APIs. In this exploratory study,
using Twitter as a benchmark, we compare the performances of four state-of-art
feature sets in detecting novel bots: one of the output scores of the popular
bot detector Botometer, which considers more than 1,000 features of an account
to take a decision; two feature sets based on the account profile and timeline;
and the information about the Twitter client from which the user tweets. The
results of our analysis, conducted on six recently released datasets of Twitter
accounts, hint at the possible use of general-purpose classifiers and
cheap-to-compute account features for the detection of evolved bots.

</details>


### [28] [Privacy-Preserving LLM Interaction with Socratic Chain-of-Thought Reasoning and Homomorphically Encrypted Vector Databases](https://arxiv.org/abs/2506.17336)
*Yubeen Bae,Minchan Kim,Jaejin Lee,Sangbum Kim,Jaehyung Kim,Yejin Choi,Niloofar Mireshghallah*

Main category: cs.CR

TL;DR: 论文提出了一种结合强大但不可信的LLM与本地可信模型的混合框架，通过加密语义搜索保护用户隐私。


<details>
  <summary>Details</summary>
Motivation: 解决用户在使用LLM时面临的数据隐私与模型性能之间的权衡问题。

Method: 采用Socratic Chain-of-Thought Reasoning，将查询分解为子查询并加密搜索用户数据，最终由本地模型生成响应。

Result: 在LoCoMo基准测试中，混合框架比单独使用GPT-4o性能提升7.1个百分点。

Conclusion: 该框架为分解任务并在不可信与本地模型间分配提供了初步解决方案，保护了用户隐私。

Abstract: Large language models (LLMs) are increasingly used as personal agents,
accessing sensitive user data such as calendars, emails, and medical records.
Users currently face a trade-off: They can send private records, many of which
are stored in remote databases, to powerful but untrusted LLM providers,
increasing their exposure risk. Alternatively, they can run less powerful
models locally on trusted devices. We bridge this gap. Our Socratic
Chain-of-Thought Reasoning first sends a generic, non-private user query to a
powerful, untrusted LLM, which generates a Chain-of-Thought (CoT) prompt and
detailed sub-queries without accessing user data. Next, we embed these
sub-queries and perform encrypted sub-second semantic search using our
Homomorphically Encrypted Vector Database across one million entries of a
single user's private data. This represents a realistic scale of personal
documents, emails, and records accumulated over years of digital activity.
Finally, we feed the CoT prompt and the decrypted records to a local language
model and generate the final response. On the LoCoMo long-context QA benchmark,
our hybrid framework, combining GPT-4o with a local Llama-3.2-1B model,
outperforms using GPT-4o alone by up to 7.1 percentage points. This
demonstrates a first step toward systems where tasks are decomposed and split
between untrusted strong LLMs and weak local ones, preserving user privacy.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [29] [Signal Use and Emergent Cooperation](https://arxiv.org/abs/2506.18920)
*Michael Williams*

Main category: cs.AI

TL;DR: 研究探讨了自治代理如何通过通信信号协调活动并提升集体效率，使用NEC-DAC系统展示了代理如何通过学习形成共享行为系统（类似文化），并分析了不同社会结构对合作文化的影响。


<details>
  <summary>Details</summary>
Motivation: 探索自治代理如何通过通信信号自组织形成文化，并研究不同通信策略对其合作效率和适应性的影响。

Method: 使用NEC-DAC系统，每个代理配备独立神经网络进行决策，通过学习和信号传递形成共享行为系统，分析不同社会结构（如权威层次）对文化形成的影响。

Result: 研究发现合作文化显著影响部落性能，信号不仅促进文化涌现，还支持其跨代传播，协调行为和信号在个体神经网络中具有额外优势。

Conclusion: 自治代理通过通信信号自组织形成文化，合作文化对集体效率至关重要，信号传递在文化传播和个体协调中发挥关键作用。

Abstract: In this work, we investigate how autonomous agents, organized into tribes,
learn to use communication signals to coordinate their activities and enhance
their collective efficiency. Using the NEC-DAC (Neurally Encoded Culture -
Distributed Autonomous Communicators) system, where each agent is equipped with
its own neural network for decision-making, we demonstrate how these agents
develop a shared behavioral system -- akin to a culture -- through learning and
signalling. Our research focuses on the self-organization of culture within
these tribes of agents and how varying communication strategies impact their
fitness and cooperation. By analyzing different social structures, such as
authority hierarchies, we show that the culture of cooperation significantly
influences the tribe's performance. Furthermore, we explore how signals not
only facilitate the emergence of culture but also enable its transmission
across generations of agents. Additionally, we examine the benefits of
coordinating behavior and signaling within individual agents' neural networks.

</details>


### [30] [Do LLMs Know When to Flip a Coin? Strategic Randomization through Reasoning and Experience](https://arxiv.org/abs/2506.18928)
*Lingyu Yang*

Main category: cs.AI

TL;DR: 论文探讨了大型语言模型（LLMs）在战略随机化中的表现，提出了一种基于田忌赛马灵感的零和游戏，评估了不同LLMs在随机化决策中的能力。


<details>
  <summary>Details</summary>
Motivation: 战略随机化在博弈论中很重要，但在LLMs中研究不足，且现有工作常混淆随机化的认知决策与随机生成的机制。

Method: 设计了一个零和游戏，其纳什均衡对应最大熵策略，评估了五种LLMs在不同提示风格下的表现。

Result: 较弱模型始终确定性决策，较强模型在明确提示下增加随机化；面对较弱模型时，强模型采用确定性策略，面对同类时趋近均衡。

Conclusion: LLMs的战略推理能力存在差异，需改进抽象推理和自适应学习。研究代码已开源。

Abstract: Strategic randomization is a key principle in game theory, yet it remains
underexplored in large language models (LLMs). Prior work often conflates the
cognitive decision to randomize with the mechanical generation of randomness,
leading to incomplete evaluations. To address this, we propose a novel zero-sum
game inspired by the Tian Ji Horse Race, where the Nash equilibrium corresponds
to a maximal entropy strategy. The game's complexity masks this property from
untrained humans and underdeveloped LLMs. We evaluate five LLMs across prompt
styles -- framed, neutral, and hinted -- using competitive multi-tournament
gameplay with system-provided random choices, isolating the decision to
randomize. Results show that weaker models remain deterministic regardless of
prompts, while stronger models exhibit increased randomization under explicit
hints. When facing weaker models, strong LLMs adopt deterministic strategies to
exploit biases, but converge toward equilibrium play when facing peers. Through
win/loss outcomes and Bayes factor analysis, we demonstrate meaningful
variation in LLMs' strategic reasoning capabilities, highlighting opportunities
for improvement in abstract reasoning and adaptive learning. We make our
implementation publicly available at
https://github.com/ocelopus/llm-when-to-throw-coin to ensure full
reproducibility.

</details>


### [31] [A Comment On "The Illusion of Thinking": Reframing the Reasoning Cliff as an Agentic Gap](https://arxiv.org/abs/2506.18957)
*Sheraz Khan,Subha Madhavan,Kannan Natarajan*

Main category: cs.AI

TL;DR: 论文指出大型推理模型（LRMs）在特定复杂度阈值后性能崩溃，但评论认为这是实验设计问题而非认知边界，并通过工具使用展示了性能反转。


<details>
  <summary>Details</summary>
Motivation: 探讨LRMs性能崩溃的根本原因，反驳其认知局限性的结论，强调实验设计对结果的影响。

Method: 通过工具使用和代理性推理重新评估模型性能，分析不同复杂度下的表现。

Result: 工具使用显著提升模型性能，解决之前无法完成的任务，并揭示代理性推理的层次。

Conclusion: LRMs的性能崩溃源于实验限制而非认知缺陷，工具使用能显著提升其推理能力。

Abstract: The recent work by Shojaee et al. (2025), titled The Illusion of Thinking:
Understanding the Strengths and Limitations of Reasoning Models via the Lens of
Problem Complexity, presents a compelling empirical finding, a reasoning cliff,
where the performance of Large Reasoning Models (LRMs) collapses beyond a
specific complexity threshold, which the authors posit as an intrinsic scaling
limitation of Chain-of-Thought (CoT) reasoning. This commentary, while
acknowledging the study's methodological rigor, contends that this conclusion
is confounded by experimental artifacts. We argue that the observed failure is
not evidence of a fundamental cognitive boundary, but rather a predictable
outcome of system-level constraints in the static, text-only evaluation
paradigm, including tool use restrictions, context window recall issues, the
absence of crucial cognitive baselines, inadequate statistical reporting, and
output generation limits. We reframe this performance collapse through the lens
of an agentic gap, asserting that the models are not failing at reasoning, but
at execution within a profoundly restrictive interface. We empirically
substantiate this critique by demonstrating a striking reversal. A model,
initially declaring a puzzle impossible when confined to text-only generation,
now employs agentic tools to not only solve it but also master variations of
complexity far beyond the reasoning cliff it previously failed to surmount.
Additionally, our empirical analysis of tool-enabled models like o4-mini and
GPT-4o reveals a hierarchy of agentic reasoning, from simple procedural
execution to complex meta-cognitive self-correction, which has significant
implications for how we define and measure machine intelligence. The illusion
of thinking attributed to LRMs is less a reasoning deficit and more a
consequence of an otherwise capable mind lacking the tools for action.

</details>


### [32] [From Rows to Yields: How Foundation Models for Tabular Data Simplify Crop Yield Prediction](https://arxiv.org/abs/2506.19046)
*Filip Sabo,Michele Meroni,Maria Piles,Martin Claverie,Fanie Ferreira,Elna Van Den Berg,Francesco Collivignarelli,Felix Rembold*

Main category: cs.AI

TL;DR: TabPFN应用于南非次国家级作物产量预测，与传统ML模型表现相当，但调优更快且特征工程需求更低，适合实际应用。


<details>
  <summary>Details</summary>
Motivation: 探索TabPFN在小到中型表格数据上的应用潜力，特别是在农业产量预测任务中。

Method: 使用TabPFN模型，结合EO和天气数据，预测南非夏季作物产量，并与6种ML模型和3种基线模型对比。

Result: TabPFN与ML模型精度相当，但调优更快，特征工程需求更低。

Conclusion: TabPFN因其高效性和易用性，更适合实际产量预测应用。

Abstract: We present an application of a foundation model for small- to medium-sized
tabular data (TabPFN), to sub-national yield forecasting task in South Africa.
TabPFN has recently demonstrated superior performance compared to traditional
machine learning (ML) models in various regression and classification tasks. We
used the dekadal (10-days) time series of Earth Observation (EO; FAPAR and soil
moisture) and gridded weather data (air temperature, precipitation and
radiation) to forecast the yield of summer crops at the sub-national level. The
crop yield data was available for 23 years and for up to 8 provinces. Covariate
variables for TabPFN (i.e., EO and weather) were extracted by region and
aggregated at a monthly scale. We benchmarked the results of the TabPFN against
six ML models and three baseline models. Leave-one-year-out cross-validation
experiment setting was used in order to ensure the assessment of the models
capacity to forecast an unseen year. Results showed that TabPFN and ML models
exhibit comparable accuracy, outperforming the baselines. Nonetheless, TabPFN
demonstrated superior practical utility due to its significantly faster tuning
time and reduced requirement for feature engineering. This renders TabPFN a
more viable option for real-world operation yield forecasting applications,
where efficiency and ease of implementation are paramount.

</details>


### [33] [Baba is LLM: Reasoning in a Game with Dynamic Rules](https://arxiv.org/abs/2506.19095)
*Fien van Wetten,Aske Plaat,Max van Duijn*

Main category: cs.AI

TL;DR: 论文探讨了大型语言模型（LLMs）在2D解谜游戏《Baba is You》中的表现，发现尽管LLMs在语言任务上表现优秀，但在动态规则推理上仍有困难。


<details>
  <summary>Details</summary>
Motivation: 研究LLMs在需要语言能力和推理能力的游戏中的表现，以评估其复杂问题解决能力。

Method: 评估了六种LLM，使用不同类型的提示（简单、规则扩展、动作扩展），并对两种模型（Mistral、OLMo）进行了微调。

Result: 较大模型（如GPT-4o）表现较好，但未适应的小模型难以识别游戏机制或应用规则变化；微调提升了游戏分析能力，但对解决方案制定帮助有限。

Conclusion: 即使是最先进的LLMs，理解动态规则变化仍具挑战性，游戏可作为测试LLMs推理能力的有效工具。

Abstract: Large language models (LLMs) are known to perform well on language tasks, but
struggle with reasoning tasks. This paper explores the ability of LLMs to play
the 2D puzzle game Baba is You, in which players manipulate rules by
rearranging text blocks that define object properties. Given that this
rule-manipulation relies on language abilities and reasoning, it is a
compelling challenge for LLMs. Six LLMs are evaluated using different prompt
types, including (1) simple, (2) rule-extended and (3) action-extended prompts.
In addition, two models (Mistral, OLMo) are finetuned using textual and
structural data from the game. Results show that while larger models
(particularly GPT-4o) perform better in reasoning and puzzle solving, smaller
unadapted models struggle to recognize game mechanics or apply rule changes.
Finetuning improves the ability to analyze the game levels, but does not
significantly improve solution formulation. We conclude that even for
state-of-the-art and finetuned LLMs, reasoning about dynamic rule changes is
difficult (specifically, understanding the use-mention distinction). The
results provide insights into the applicability of LLMs to complex
problem-solving tasks and highlight the suitability of games with dynamically
changing rules for testing reasoning and reflection by LLMs.

</details>


### [34] [Spiritual-LLM : Gita Inspired Mental Health Therapy In the Era of LLMs](https://arxiv.org/abs/2506.19185)
*Janak Kapuriya,Aman Singh,Jainendra Shukla,Rajiv Ratn Shah*

Main category: cs.AI

TL;DR: 该研究提出了一种结合《薄伽梵歌》精神智慧与GPT-4o的新框架GITes，显著提升了情感支持的深度和效果。


<details>
  <summary>Details</summary>
Motivation: 传统心理健康支持系统仅基于用户当前情绪和情境生成回应，无法满足深层情感需求。

Method: 整合《薄伽梵歌》精神智慧与GPT-4o，创建GITes数据集，并通过LLM评估框架提出Spiritual Insight指标。

Result: 最佳模型Phi3-Mini 3.2B Instruct在多项指标上显著提升，如ROUGE（122.71%）和Spiritual Insight（15.92%）。

Conclusion: 结合精神智慧的AI系统有望提升用户满意度，但需进一步实际验证。

Abstract: Traditional mental health support systems often generate responses based
solely on the user's current emotion and situations, resulting in superficial
interventions that fail to address deeper emotional needs. This study
introduces a novel framework by integrating spiritual wisdom from the Bhagavad
Gita with advanced large language model GPT-4o to enhance emotional well-being.
We present the GITes (Gita Integrated Therapy for Emotional Support) dataset,
which enhances the existing ExTES mental health dataset by including 10,729
spiritually guided responses generated by GPT-4o and evaluated by domain
experts. We benchmark GITes against 12 state-of-the-art LLMs, including both
mental health specific and general purpose models. To evaluate spiritual
relevance in generated responses beyond what conventional n-gram based metrics
capture, we propose a novel Spiritual Insight metric and automate assessment
via an LLM as jury framework using chain-of-thought prompting. Integrating
spiritual guidance into AI driven support enhances both NLP and spiritual
metrics for the best performing LLM Phi3-Mini 3.2B Instruct, achieving
improvements of 122.71% in ROUGE, 126.53% in METEOR, 8.15% in BERT score,
15.92% in Spiritual Insight, 18.61% in Sufficiency and 13.22% in Relevance
compared to its zero-shot counterpart. While these results reflect substantial
improvements across automated empathy and spirituality metrics, further
validation in real world patient populations remains a necessary step. Our
findings indicate a strong potential for AI systems enriched with spiritual
guidance to enhance user satisfaction and perceived support outcomes. The code
and dataset will be publicly available to advance further research in this
emerging area.

</details>


### [35] [Bayesian Evolutionary Swarm Architecture: A Formal Epistemic System Grounded in Truth-Based Competition](https://arxiv.org/abs/2506.19191)
*Craig Steven Wright*

Main category: cs.AI

TL;DR: 论文提出了一种基于概率代理的AI系统框架，通过竞争和信念修正实现进化，证明了真理作为进化吸引子的存在。


<details>
  <summary>Details</summary>
Motivation: 构建一个数学严谨的AI系统框架，通过竞争和信念修正模拟代理的进化，以验证真理的可演化性。

Method: 基于贝叶斯推理、测度理论和群体动力学，代理通过竞争和信念修正调整后验信念，利用哈希加密身份和因果推断算子确保可追溯性和一致性。

Result: 系统证明了真理作为进化吸引子的存在，表明可验证知识可以通过对抗性认知压力在可计算的自我调节群体中产生。

Conclusion: 该框架为AI系统的数学建模提供了新思路，展示了真理在竞争环境中的演化潜力。

Abstract: We introduce a mathematically rigorous framework for an artificial
intelligence system composed of probabilistic agents evolving through
structured competition and belief revision. The architecture, grounded in
Bayesian inference, measure theory, and population dynamics, defines agent
fitness as a function of alignment with a fixed external oracle representing
ground truth. Agents compete in a discrete-time environment, adjusting
posterior beliefs through observed outcomes, with higher-rated agents
reproducing and lower-rated agents undergoing extinction. Ratings are updated
via pairwise truth-aligned utility comparisons, and belief updates preserve
measurable consistency and stochastic convergence. We introduce hash-based
cryptographic identity commitments to ensure traceability, alongside causal
inference operators using do-calculus. Formal theorems on convergence,
robustness, and evolutionary stability are provided. The system establishes
truth as an evolutionary attractor, demonstrating that verifiable knowledge
arises from adversarial epistemic pressure within a computable, self-regulating
swarm.

</details>


### [36] [GBGC: Efficient and Adaptive Graph Coarsening via Granular-ball Computing](https://arxiv.org/abs/2506.19224)
*Shuyin Xia,Guan Wang,Gaojie Xu,Sen Zhao,Guoyin Wang*

Main category: cs.AI

TL;DR: 本文提出了一种基于多粒度特性的图粗化方法GBGC，通过自适应粒度球计算，显著提升了粗化效果和效率。


<details>
  <summary>Details</summary>
Motivation: 传统图粗化方法主要基于谱保持视角，忽略了图的多粒度特性，导致粗化效果不佳。

Method: GBGC引入自适应粒度球图细化机制，将原始图从粗到细分割为不同大小的粒度球，并以这些球为超节点构建粗化图。

Result: GBGC的处理速度比其他方法快数十至数百倍，且粗化图的准确性几乎总是高于原始图。

Conclusion: GBGC因其鲁棒性和泛化能力，有望成为标准的图数据预处理方法。

Abstract: The objective of graph coarsening is to generate smaller, more manageable
graphs while preserving key information of the original graph. Previous work
were mainly based on the perspective of spectrum-preserving, using some
predefined coarsening rules to make the eigenvalues of the Laplacian matrix of
the original graph and the coarsened graph match as much as possible. However,
they largely overlooked the fact that the original graph is composed of
subregions at different levels of granularity, where highly connected and
similar nodes should be more inclined to be aggregated together as nodes in the
coarsened graph. By combining the multi-granularity characteristics of the
graph structure, we can generate coarsened graph at the optimal granularity. To
this end, inspired by the application of granular-ball computing in
multi-granularity, we propose a new multi-granularity, efficient, and adaptive
coarsening method via granular-ball (GBGC), which significantly improves the
coarsening results and efficiency. Specifically, GBGC introduces an adaptive
granular-ball graph refinement mechanism, which adaptively splits the original
graph from coarse to fine into granular-balls of different sizes and optimal
granularity, and constructs the coarsened graph using these granular-balls as
supernodes. In addition, compared with other state-of-the-art graph coarsening
methods, the processing speed of this method can be increased by tens to
hundreds of times and has lower time complexity. The accuracy of GBGC is almost
always higher than that of the original graph due to the good robustness and
generalization of the granular-ball computing, so it has the potential to
become a standard graph data preprocessing method.

</details>


### [37] [RecLLM-R1: A Two-Stage Training Paradigm with Reinforcement Learning and Chain-of-Thought v1](https://arxiv.org/abs/2506.19235)
*Yu Xie,Xingkai Ren,Ying Qi,Yao Hu,Lianlei Shan*

Main category: cs.AI

TL;DR: RecLLM-R1是一个基于大型语言模型（LLM）的推荐框架，通过两阶段训练（SFT和GRPO）优化推荐准确性、多样性和业务目标，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决传统推荐系统中的“过滤气泡”、外部知识利用不足以及模型优化与业务策略迭代脱节的问题。

Method: 1. 将用户数据和项目属性转化为LLM可理解的自然语言提示；2. 两阶段训练：SFT和GRPO（结合CoT机制）。

Result: 在真实数据集上表现优异，显著提升准确性、多样性和新颖性，有效缓解过滤气泡效应。

Conclusion: RecLLM-R1为复杂业务目标下的推荐模型与策略集成优化提供了可行方案。

Abstract: Traditional recommendation systems often grapple with "filter bubbles",
underutilization of external knowledge, and a disconnect between model
optimization and business policy iteration. To address these limitations, this
paper introduces RecLLM-R1, a novel recommendation framework leveraging Large
Language Models (LLMs) and drawing inspiration from the DeepSeek R1
methodology. The framework initiates by transforming user profiles, historical
interactions, and multi-faceted item attributes into LLM-interpretable natural
language prompts through a carefully engineered data construction process.
Subsequently, a two-stage training paradigm is employed: the initial stage
involves Supervised Fine-Tuning (SFT) to imbue the LLM with fundamental
recommendation capabilities. The subsequent stage utilizes Group Relative
Policy Optimization (GRPO), a reinforcement learning technique, augmented with
a Chain-of-Thought (CoT) mechanism. This stage guides the model through
multi-step reasoning and holistic decision-making via a flexibly defined reward
function, aiming to concurrently optimize recommendation accuracy, diversity,
and other bespoke business objectives. Empirical evaluations on a real-world
user behavior dataset from a large-scale social media platform demonstrate that
RecLLM-R1 significantly surpasses existing baseline methods across a spectrum
of evaluation metrics, including accuracy, diversity, and novelty. It
effectively mitigates the filter bubble effect and presents a promising avenue
for the integrated optimization of recommendation models and policies under
intricate business goals.

</details>


### [38] [Emotion Detection on User Front-Facing App Interfaces for Enhanced Schedule Optimization: A Machine Learning Approach](https://arxiv.org/abs/2506.19280)
*Feiting Yang,Antoine Moevus,Steve Lévesque*

Main category: cs.AI

TL;DR: 论文探讨了将情绪检测技术集成到日历应用中，通过生物特征和行为两种方法提升用户体验。生物特征方法使用心电图数据，行为方法分析用户交互数据。结果显示行为方法更准确，GRU网络在生物特征方法中表现更好。


<details>
  <summary>Details</summary>
Motivation: 提升人机交互的个性化和适应性，通过情绪检测优化日历应用的用户体验。

Method: 采用两种方法：生物特征（心电图数据+LSTM/GRU网络）和行为（用户交互数据+机器学习模型）。

Result: 行为方法在鼠标交互中准确率达90%，GRU网络在生物特征方法中表现优于LSTM（84.38%准确率）。

Conclusion: 情绪检测技术可有效提升日历应用的用户体验，行为方法更具一致性和准确性。

Abstract: Human-Computer Interaction (HCI) has evolved significantly to incorporate
emotion recognition capabilities, creating unprecedented opportunities for
adaptive and personalized user experiences. This paper explores the integration
of emotion detection into calendar applications, enabling user interfaces to
dynamically respond to users' emotional states and stress levels, thereby
enhancing both productivity and engagement. We present and evaluate two
complementary approaches to emotion detection: a biometric-based method
utilizing heart rate (HR) data extracted from electrocardiogram (ECG) signals
processed through Long Short-Term Memory (LSTM) and Gated Recurrent Unit (GRU)
neural networks to predict the emotional dimensions of Valence, Arousal, and
Dominance; and a behavioral method analyzing computer activity through multiple
machine learning models to classify emotions based on fine-grained user
interactions such as mouse movements, clicks, and keystroke patterns. Our
comparative analysis, from real-world datasets, reveals that while both
approaches demonstrate effectiveness, the computer activity-based method
delivers superior consistency and accuracy, particularly for mouse-related
interactions, which achieved approximately 90\% accuracy. Furthermore, GRU
networks outperformed LSTM models in the biometric approach, with Valence
prediction reaching 84.38\% accuracy.

</details>


### [39] [Skywork-SWE: Unveiling Data Scaling Laws for Software Engineering in LLMs](https://arxiv.org/abs/2506.19290)
*Liang Zeng,Yongcong Li,Yuzhen Xiao,Changshi Li,Chris Yuhao Liu,Rui Yan,Tianwen Wei,Jujie He,Xuchen Song,Yang Liu,Yahui Zhou*

Main category: cs.AI

TL;DR: 论文提出了一种自动化的数据整理流程，用于扩展软件工程数据集，并展示了数据规模对模型性能的持续提升作用。


<details>
  <summary>Details</summary>
Motivation: 软件工程数据集的整理过程耗时且依赖人工标注，限制了数据集的规模和多样性。

Method: 提出了一种增量式、自动化的数据整理流程，生成了一个包含10,169个Python任务实例的数据集，并用于训练Skywork-SWE模型。

Result: Skywork-SWE模型在SWE-bench Verified基准测试中达到38.0% pass@1准确率，结合测试时扩展技术后提升至47.0%。

Conclusion: 自动化数据整理流程能显著提升数据集规模和多样性，从而持续改进模型性能，为未来研究提供了新的基准。

Abstract: Software engineering (SWE) has recently emerged as a crucial testbed for
next-generation LLM agents, demanding inherent capabilities in two critical
dimensions: sustained iterative problem-solving (e.g., >50 interaction rounds)
and long-context dependency resolution (e.g., >32k tokens). However, the data
curation process in SWE remains notoriously time-consuming, as it heavily
relies on manual annotation for code file filtering and the setup of dedicated
runtime environments to execute and validate unit tests. Consequently, most
existing datasets are limited to only a few thousand GitHub-sourced instances.
To this end, we propose an incremental, automated data-curation pipeline that
systematically scales both the volume and diversity of SWE datasets. Our
dataset comprises 10,169 real-world Python task instances from 2,531 distinct
GitHub repositories, each accompanied by a task specified in natural language
and a dedicated runtime-environment image for automated unit-test validation.
We have carefully curated over 8,000 successfully runtime-validated training
trajectories from our proposed SWE dataset. When fine-tuning the Skywork-SWE
model on these trajectories, we uncover a striking data scaling phenomenon: the
trained model's performance for software engineering capabilities in LLMs
continues to improve as the data size increases, showing no signs of
saturation. Notably, our Skywork-SWE model achieves 38.0% pass@1 accuracy on
the SWE-bench Verified benchmark without using verifiers or multiple rollouts,
establishing a new state-of-the-art (SOTA) among the Qwen2.5-Coder-32B-based
LLMs built on the OpenHands agent framework. Furthermore, with the
incorporation of test-time scaling techniques, the performance further improves
to 47.0% accuracy, surpassing the previous SOTA results for sub-32B parameter
models. We release the Skywork-SWE-32B model checkpoint to accelerate future
research.

</details>


### [40] [FEAT: A Preference Feedback Dataset through a Cost-Effective Auto-Generation and Labeling Framework for English AI Tutoring](https://arxiv.org/abs/2506.19325)
*Hyein Seo,Taewook Hwang,Yohan Lee,sangkeun Jung*

Main category: cs.AI

TL;DR: 论文提出了一种名为FEAT的框架，用于高效生成英语教育辅导中的教师反馈，并构建了三个互补数据集。实验表明，结合少量高质量数据（5-10%）能显著提升性能。


<details>
  <summary>Details</summary>
Motivation: AI辅导系统需要高质量的大规模教师反馈数据，但手动生成成本高且耗时。

Method: 提出FEAT框架，构建三个数据集：DM（人工与LLM协作）、DG（纯LLM生成）、DA（DG为主，少量DM增强）。

Result: 实验显示，在DG中加入少量DM（5-10%）比单独使用100% DM表现更优。

Conclusion: FEAT框架通过结合少量高质量数据，实现了成本效益与性能的平衡。

Abstract: In English education tutoring, teacher feedback is essential for guiding
students. Recently, AI-based tutoring systems have emerged to assist teachers;
however, these systems require high-quality and large-scale teacher feedback
data, which is both time-consuming and costly to generate manually. In this
study, we propose FEAT, a cost-effective framework for generating teacher
feedback, and have constructed three complementary datasets: (1) DIRECT-Manual
(DM), where both humans and large language models (LLMs) collaboratively
generate high-quality teacher feedback, albeit at a higher cost; (2)
DIRECT-Generated (DG), an LLM-only generated, cost-effective dataset with lower
quality;, and (3) DIRECT-Augmented (DA), primarily based on DG with a small
portion of DM added to enhance quality while maintaining cost-efficiency.
Experimental results showed that incorporating a small portion of DM (5-10%)
into DG leads to superior performance compared to using 100% DM alone.

</details>


### [41] [Evolutionary Level Repair](https://arxiv.org/abs/2506.19359)
*Debosmita Bhaumik,Julian Togelius,Georgios N. Yannakakis,Ahmed Khalifa*

Main category: cs.AI

TL;DR: 论文提出了一种基于搜索的游戏关卡修复方法，结合进化算法和多样性质量算法，修复由机器学习生成的风格化但常出错的关卡。


<details>
  <summary>Details</summary>
Motivation: 解决游戏关卡设计中的非功能性修复问题，确保关卡的完整性、目标可达性等性能。

Method: 使用进化算法和多样性质量算法进行搜索式修复，限制修复时的改动数量。

Result: 该方法在修复机器学习生成的关卡上表现良好。

Conclusion: 结合PCGML生成和搜索式修复的混合方法具有很大潜力。

Abstract: We address the problem of game level repair, which consists of taking a
designed but non-functional game level and making it functional. This might
consist of ensuring the completeness of the level, reachability of objects, or
other performance characteristics. The repair problem may also be constrained
in that it can only make a small number of changes to the level. We investigate
search-based solutions to the level repair problem, particularly using
evolutionary and quality-diversity algorithms, with good results. This level
repair method is applied to levels generated using a machine learning-based
procedural content generation (PCGML) method that generates stylistically
appropriate but frequently broken levels. This combination of PCGML for
generation and search-based methods for repair shows great promise as a hybrid
procedural content generation (PCG) method.

</details>


### [42] [Conversational Intent-Driven GraphRAG: Enhancing Multi-Turn Dialogue Systems through Adaptive Dual-Retrieval of Flow Patterns and Context Semantics](https://arxiv.org/abs/2506.19385)
*Ziqi Zhu,Tao Hu,Honglong Zhang,Dan Yang,HanGeng Chen,Mengran Zhang,Xilun Chen*

Main category: cs.AI

TL;DR: CID-GraphRAG是一种新型框架，通过动态意图转移图和双检索机制，显著提升多轮客服对话的上下文连贯性和目标导向性。


<details>
  <summary>Details</summary>
Motivation: 解决现有对话系统在多轮客服对话中难以同时保持上下文连贯和目标导向的问题。

Method: 构建动态意图转移图，结合意图图遍历和语义搜索的双检索机制。

Result: 在真实客服对话实验中，CID-GraphRAG显著优于传统方法，自动指标和LLM评估均显示明显提升。

Conclusion: CID-GraphRAG通过结合意图转移和语义检索，有效解决了知识密集型多轮对话的挑战。

Abstract: We present CID-GraphRAG (Conversational Intent-Driven Graph Retrieval
Augmented Generation), a novel framework that addresses the limitations of
existing dialogue systems in maintaining both contextual coherence and
goal-oriented progression in multi-turn customer service conversations. Unlike
traditional RAG systems that rely solely on semantic similarity (Conversation
RAG) or standard knowledge graphs (GraphRAG), CID-GraphRAG constructs dynamic
intent transition graphs from goal achieved historical dialogues and implements
a dual-retrieval mechanism that adaptively balances intent-based graph
traversal with semantic search. This approach enables the system to
simultaneously leverage both conversional intent flow patterns and contextual
semantics, significantly improving retrieval quality and response quality. In
extensive experiments on real-world customer service dialogues, we employ both
automatic metrics and LLM-as-judge assessments, demonstrating that CID-GraphRAG
significantly outperforms both semantic-based Conversation RAG and intent-based
GraphRAG baselines across all evaluation criteria. Quantitatively, CID-GraphRAG
demonstrates substantial improvements over Conversation RAG across automatic
metrics, with relative gains of 11% in BLEU, 5% in ROUGE-L, 6% in METEOR, and
most notably, a 58% improvement in response quality according to LLM-as-judge
evaluations. These results demonstrate that the integration of intent
transition structures with semantic retrieval creates a synergistic effect that
neither approach achieves independently, establishing CID-GraphRAG as an
effective framework for addressing the challenges of maintaining contextual
coherence and goal-oriented progression in knowledge-intensive multi-turn
dialogues.

</details>


### [43] [Is an object-centric representation beneficial for robotic manipulation ?](https://arxiv.org/abs/2506.19408)
*Alexandre Chapin,Emmanuel Dellandrea,Liming Chen*

Main category: cs.AI

TL;DR: 论文探讨了对象中心表示（OCR）在机器人操作任务中的潜力，通过模拟环境测试其泛化能力，并与整体表示方法对比。


<details>
  <summary>Details</summary>
Motivation: 现有OCR研究多关注场景分解，缺乏对学习表示推理能力的评估，而机器人操作任务涉及多对象交互，是评估OCR潜力的理想场景。

Method: 创建多个模拟机器人操作任务，测试经典OCR方法在不同泛化场景下的表现，并与整体表示方法对比。

Result: OCR方法在复杂场景中表现优于整体表示方法，能更好地应对挑战。

Conclusion: OCR在机器人操作任务中具有潜力，尤其在复杂场景中表现更优。

Abstract: Object-centric representation (OCR) has recently become a subject of interest
in the computer vision community for learning a structured representation of
images and videos. It has been several times presented as a potential way to
improve data-efficiency and generalization capabilities to learn an agent on
downstream tasks. However, most existing work only evaluates such models on
scene decomposition, without any notion of reasoning over the learned
representation. Robotic manipulation tasks generally involve multi-object
environments with potential inter-object interaction. We thus argue that they
are a very interesting playground to really evaluate the potential of existing
object-centric work. To do so, we create several robotic manipulation tasks in
simulated environments involving multiple objects (several distractors, the
robot, etc.) and a high-level of randomization (object positions, colors,
shapes, background, initial positions, etc.). We then evaluate one classical
object-centric method across several generalization scenarios and compare its
results against several state-of-the-art hollistic representations. Our results
exhibit that existing methods are prone to failure in difficult scenarios
involving complex scene structures, whereas object-centric methods help
overcome these challenges.

</details>


### [44] [Unsupervised Dataset Dictionary Learning for domain shift robust clustering: application to sitting posture identification](https://arxiv.org/abs/2506.19410)
*Anas Hattay,Mayara Ayat,Fred Ngole Mboula*

Main category: cs.AI

TL;DR: 提出了一种名为U-DaDiL的无监督学习方法，用于坐姿识别的鲁棒聚类，解决了传统方法对多样化数据集适应性差和域偏移问题。


<details>
  <summary>Details</summary>
Motivation: 传统方法在处理多样化数据集和域偏移问题时表现不佳，U-DaDiL旨在通过分布对齐提升鲁棒性。

Method: 使用基于Wasserstein重心表示的方法对齐不同数据集的分布。

Result: 在Office31数据集上的实验表明，聚类对齐准确性显著提升。

Conclusion: U-DaDiL为解决域偏移和无监督坐姿识别的鲁棒聚类提供了有前景的方向。

Abstract: This paper introduces a novel approach, Unsupervised Dataset Dictionary
Learning (U-DaDiL), for totally unsupervised robust clustering applied to
sitting posture identification. Traditional methods often lack adaptability to
diverse datasets and suffer from domain shift issues. U-DaDiL addresses these
challenges by aligning distributions from different datasets using Wasserstein
barycenter based representation. Experimental evaluations on the Office31
dataset demonstrate significant improvements in cluster alignment accuracy.
This work also presents a promising step for addressing domain shift and robust
clustering for unsupervised sitting posture identification

</details>


### [45] [Commander-GPT: Dividing and Routing for Multimodal Sarcasm Detection](https://arxiv.org/abs/2506.19420)
*Yazhou Zhang,Chunwang Zou,Bo Wang,Jing Qin*

Main category: cs.AI

TL;DR: Commander-GPT是一个模块化决策路由框架，通过协调多个专用LLM代理来提升多模态讽刺理解任务的表现，相比现有技术有显著改进。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型（LLM）在许多NLP任务中表现出色，但在讽刺理解方面仍有困难，因此需要一种更高效的框架来解决这一问题。

Method: 提出Commander-GPT框架，利用多个专用LLM代理分别处理子任务（如上下文建模、情感分析等），并通过集中指挥官（如轻量级编码器、小型自回归模型或大型LLM）整合信息并做出最终判断。

Result: 在MMSD和MMSD 2.0基准测试中，F1分数分别比现有技术平均提高了4.4%和11.7%。

Conclusion: Commander-GPT通过模块化设计和多代理协作，显著提升了多模态讽刺理解的性能。

Abstract: Multimodal sarcasm understanding is a high-order cognitive task. Although
large language models (LLMs) have shown impressive performance on many
downstream NLP tasks, growing evidence suggests that they struggle with sarcasm
understanding. In this paper, we propose Commander-GPT, a modular decision
routing framework inspired by military command theory. Rather than relying on a
single LLM's capability, Commander-GPT orchestrates a team of specialized LLM
agents where each agent will be selectively assigned to a focused sub-task such
as context modeling, sentiment analysis, etc. Their outputs are then routed
back to the commander, which integrates the information and performs the final
sarcasm judgment. To coordinate these agents, we introduce three types of
centralized commanders: (1) a trained lightweight encoder-based commander
(e.g., multi-modal BERT); (2) four small autoregressive language models,
serving as moderately capable commanders (e.g., DeepSeek-VL); (3) two large
LLM-based commander (Gemini Pro and GPT-4o) that performs task routing, output
aggregation, and sarcasm decision-making in a zero-shot fashion. We evaluate
Commander-GPT on the MMSD and MMSD 2.0 benchmarks, comparing five prompting
strategies. Experimental results show that our framework achieves 4.4% and
11.7% improvement in F1 score over state-of-the-art (SoTA) baselines on
average, demonstrating its effectiveness.

</details>


### [46] [KunLunBaizeRAG: Reinforcement Learning Driven Inference Performance Leap for Large Language Models](https://arxiv.org/abs/2506.19466)
*Cheng Li,Jiexiong Liu,Yixuan Chen,Qihang Zhou,KunLun Meta*

Main category: cs.AI

TL;DR: KunLunBaizeRAG是一个强化学习驱动的推理框架，旨在提升大语言模型在复杂多跳问答任务中的推理能力。


<details>
  <summary>Details</summary>
Motivation: 解决传统RAG在检索漂移、信息冗余和策略刚性方面的局限性。

Method: 引入RDRA机制、STIE机制、NLR机制和渐进混合训练策略。

Result: 在四个基准测试中，EM和LJ分数显著提升。

Conclusion: 该框架在复杂推理场景中表现出鲁棒性和高效性。

Abstract: This paper introduces KunLunBaizeRAG, a reinforcement learning-driven
reasoning framework designed to enhance the reasoning capabilities of large
language models (LLMs) in complex multi-hop question-answering tasks. The
framework addresses key limitations of traditional RAG, such as retrieval
drift, information redundancy, and strategy rigidity. Key innovations include
the RAG-driven Reasoning Alignment (RDRA) mechanism, the Search-Think Iterative
Enhancement (STIE) mechanism, the Network-Local Intelligent Routing (NLR)
mechanism, and a progressive hybrid training strategy. Experimental results
demonstrate significant improvements in exact match (EM) and LLM-judged score
(LJ) across four benchmarks, highlighting the framework's robustness and
effectiveness in complex reasoning scenarios.

</details>


### [47] [NaviAgent: Bilevel Planning on Tool Dependency Graphs for Function Calling](https://arxiv.org/abs/2506.19500)
*Yan Jiang,Hao Zhou,LiZhong GU,Ai Han,TianLong Li*

Main category: cs.AI

TL;DR: NaviAgent是一种基于图导航的双层规划架构，通过多路径决策器和图编码导航器提升LLM在复杂工具链调用中的鲁棒性和效率。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖静态知识和单一路径执行，导致错误恢复能力差和搜索空间爆炸，限制了复杂工具链的协调能力。

Method: NaviAgent采用双层规划架构：多路径决策器动态选择最优动作，图编码导航器构建工具依赖异构图并融合API结构和历史行为。

Result: 实验显示NaviAgent在任务成功率上显著优于基线方法（如ReAct、ToolLLM等），最高提升19%，且在效率上接近最优基线。

Conclusion: NaviAgent通过双层规划和图导航策略，显著提升了LLM在复杂工具链调用中的性能和鲁棒性。

Abstract: LLMs' reliance on static knowledge and fragile tool invocation severely
hinders the orchestration of complex, heterogeneous toolchains, particularly at
large scales. Existing methods typically use rigid single-path execution,
resulting in poor error recovery and exponentially growing search spaces. We
introduce NaviAgent, a graph-navigated bilevel planning architecture for robust
function calling, comprising a Multi-Path Decider and Graph-Encoded Navigator.
As an LLM-powered agent, the Multi-Path Decider defines a four-dimensional
decision space and continuously perceives environmental states, dynamically
selecting the optimal action to fully cover all tool invocation scenarios. The
Graph-Encoded Navigator constructs a Tool Dependency Heterogeneous Graph
(TDHG), where node embeddings explicitly fuse API schema structure with
historical invocation behavior. It also integrates a novel heuristic search
strategy that guides the Decider toward efficient and highly successful
toolchains, even for unseen tool combinations. Experiments show that NaviAgent
consistently achieves the highest task success rate (TSR) across all foundation
models and task complexities, outperforming the average baselines (ReAct,
ToolLLM, {\alpha}-UMI) by 13.5%, 16.4%, and 19.0% on Qwen2.5-14B, Qwen2.5-32B,
and Deepseek-V3, respectively. Its execution steps are typically within one
step of the most efficient baseline, ensuring a strong balance between quality
and efficiency. Notably, a fine-tuned Qwen2.5-14B model achieves a TSR of
49.5%, surpassing the much larger 32B model (44.9%) under our architecture.
Incorporating the Graph-Encoded Navigator further boosts TSR by an average of
2.4 points, with gains up over 9 points on complex tasks for larger models
(Deepseek-V3 and GPT-4o), highlighting its essential role in toolchain
orchestration.

</details>


### [48] [NTRL: Encounter Generation via Reinforcement Learning for Dynamic Difficulty Adjustment in Dungeons and Dragons](https://arxiv.org/abs/2506.19530)
*Carlo Romeo,Andrew D. Bagdanov*

Main category: cs.AI

TL;DR: 论文提出了一种基于强化学习的自动化动态难度调整方法（NTRL），用于优化《龙与地下城》中的战斗遭遇设计，显著提升战斗时长和战术深度。


<details>
  <summary>Details</summary>
Motivation: 解决《龙与地下城》中手动调整战斗难度的复杂性，避免打断叙事流程。

Method: 将问题建模为上下文老虎机，基于实时队伍属性生成战斗遭遇。

Result: NTRL显著延长战斗时长（+200%），增加玩家伤害（-16.67%生命值），同时保持高胜率（70%）。

Conclusion: NTRL在提升战斗策略深度和难度方面优于人工设计，同时保持游戏公平性。

Abstract: Balancing combat encounters in Dungeons & Dragons (D&D) is a complex task
that requires Dungeon Masters (DM) to manually assess party strength, enemy
composition, and dynamic player interactions while avoiding interruption of the
narrative flow. In this paper, we propose Encounter Generation via
Reinforcement Learning (NTRL), a novel approach that automates Dynamic
Difficulty Adjustment (DDA) in D&D via combat encounter design. By framing the
problem as a contextual bandit, NTRL generates encounters based on real-time
party members attributes. In comparison with classic DM heuristics, NTRL
iteratively optimizes encounters to extend combat longevity (+200%), increases
damage dealt to party members, reducing post-combat hit points (-16.67%), and
raises the number of player deaths while maintaining low total party kills
(TPK). The intensification of combat forces players to act wisely and engage in
tactical maneuvers, even though the generated encounters guarantee high win
rates (70%). Even in comparison with encounters designed by human Dungeon
Masters, NTRL demonstrates superior performance by enhancing the strategic
depth of combat while increasing difficulty in a manner that preserves overall
game fairness.

</details>


### [49] [Interpretable Hybrid Machine Learning Models Using FOLD-R++ and Answer Set Programming](https://arxiv.org/abs/2506.19573)
*Sanne Wielinga,Jesse Heyninck*

Main category: cs.AI

TL;DR: 提出了一种结合符号推理（ASP规则）与黑盒机器学习的方法，以提高预测准确性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 在高风险领域（如医疗）中，高性能但不可解释的机器学习方法限制了信任和广泛应用，而符号方法虽可解释但预测能力不足。

Method: 通过FOLD-R++算法从ASP中提取规则，与黑盒ML分类器结合，选择性修正不确定预测并提供解释。

Result: 在五个医疗数据集上实验显示，准确率和F1分数有显著提升。

Conclusion: 结合符号推理与传统ML可在不牺牲准确性的前提下实现高可解释性。

Abstract: Machine learning (ML) techniques play a pivotal role in high-stakes domains
such as healthcare, where accurate predictions can greatly enhance
decision-making. However, most high-performing methods such as neural networks
and ensemble methods are often opaque, limiting trust and broader adoption. In
parallel, symbolic methods like Answer Set Programming (ASP) offer the
possibility of interpretable logical rules but do not always match the
predictive power of ML models. This paper proposes a hybrid approach that
integrates ASP-derived rules from the FOLD-R++ algorithm with black-box ML
classifiers to selectively correct uncertain predictions and provide
human-readable explanations. Experiments on five medical datasets reveal
statistically significant performance gains in accuracy and F1 score. This
study underscores the potential of combining symbolic reasoning with
conventional ML to achieve high interpretability without sacrificing accuracy.

</details>


### [50] [Adaptive Domain Modeling with Language Models: A Multi-Agent Approach to Task Planning](https://arxiv.org/abs/2506.19592)
*Harisankar Babu,Philipp Schillinger,Tamim Asfour*

Main category: cs.AI

TL;DR: TAPAS是一个多智能体框架，结合LLMs和符号规划，无需手动定义环境模型即可解决复杂任务。


<details>
  <summary>Details</summary>
Motivation: 解决复杂任务时避免手动定义环境模型的需求，提高适应性和灵活性。

Method: 使用基于LLM的智能体协作生成和调整域模型、初始状态和目标规范，结合ReAct执行代理和自然语言计划翻译。

Result: 在基准规划领域和VirtualHome模拟环境中表现优异。

Conclusion: TAPAS展示了无需手动调整即可动态适应复杂任务的能力。

Abstract: We introduce TAPAS (Task-based Adaptation and Planning using AgentS), a
multi-agent framework that integrates Large Language Models (LLMs) with
symbolic planning to solve complex tasks without the need for manually defined
environment models. TAPAS employs specialized LLM-based agents that
collaboratively generate and adapt domain models, initial states, and goal
specifications as needed using structured tool-calling mechanisms. Through this
tool-based interaction, downstream agents can request modifications from
upstream agents, enabling adaptation to novel attributes and constraints
without manual domain redefinition. A ReAct (Reason+Act)-style execution agent,
coupled with natural language plan translation, bridges the gap between
dynamically generated plans and real-world robot capabilities. TAPAS
demonstrates strong performance in benchmark planning domains and in the
VirtualHome simulated real-world environment.

</details>


### [51] [ChordPrompt: Orchestrating Cross-Modal Prompt Synergy for Multi-Domain Incremental Learning in CLIP](https://arxiv.org/abs/2506.19608)
*Zhiyuan Wang,Bokui Chen*

Main category: cs.AI

TL;DR: 论文提出了ChordPrompt框架，通过跨模态提示和多域自适应文本提示，解决了现有提示学习方法在多域任务增量学习中的不足，提升了模型的零样本泛化和下游任务性能。


<details>
  <summary>Details</summary>
Motivation: 现有的提示学习方法主要关注类增量学习，缺乏针对多域任务增量学习的策略，且多采用单模态提示，忽略了跨模态信息交互的潜力。

Method: 提出了ChordPrompt框架，引入跨模态提示以利用视觉和文本信息的交互，并采用域自适应文本提示来选择适合多域持续适应的提示。

Result: 在多域增量学习基准测试中，ChordPrompt在零样本泛化和下游任务性能上优于现有方法。

Conclusion: ChordPrompt通过跨模态和多域自适应提示，有效提升了预训练视觉语言模型在多域增量学习中的适应性和性能。

Abstract: Continual learning (CL) empowers pre-trained vision-language models to adapt
effectively to novel or previously underrepresented data distributions without
comprehensive retraining, enhancing their adaptability and efficiency. While
vision-language models like CLIP show great promise, they struggle to maintain
performance across domains in incremental learning scenarios. Existing prompt
learning methods face two main limitations: 1) they primarily focus on
class-incremental learning scenarios, lacking specific strategies for
multi-domain task incremental learning; 2) most current approaches employ
single-modal prompts, neglecting the potential benefits of cross-modal
information exchange. To address these challenges, we propose the \ChordPrompt
framework, which facilitates a harmonious interplay between visual and textual
prompts. \ChordPrompt introduces cross-modal prompts to leverage interactions
between visual and textual information. Our approach also employs
domain-adaptive text prompts to select appropriate prompts for continual
adaptation across multiple domains. Comprehensive experiments on multi-domain
incremental learning benchmarks demonstrate that \ChordPrompt outperforms
state-of-the-art methods in zero-shot generalization and downstream task
performance.

</details>


### [52] [Position: Intelligent Science Laboratory Requires the Integration of Cognitive and Embodied AI](https://arxiv.org/abs/2506.19613)
*Sha Zhang,Suorong Yang,Tong Xie,Xiangyuan Xue,Zixuan Hu,Rui Li,Wenxi Qu,Zhenfei Yin,Tianfan Fu,Di Hu,Andres M Bran,Nian Ran,Bram Hoex,Wangmeng Zuo,Philippe Schwaller,Wanli Ouyang,Lei Bai,Yanyong Zhang,Lingyu Duan,Shixiang Tang,Dongzhan Zhou*

Main category: cs.AI

TL;DR: 论文提出智能科学实验室（ISLs）的概念，通过结合认知与实体智能，实现自主实验与科学发现。


<details>
  <summary>Details</summary>
Motivation: 当前AI科学家和自动化实验室在虚拟环境和物理实验中的局限性，阻碍了科学发现的潜力。

Method: 提出ISLs框架，整合科学推理的基础模型、基于代理的工作流编排和实体代理的物理实验。

Result: ISLs有望实现闭环系统，支持自主实验和偶然发现。

Conclusion: ISLs是克服科学发现当前限制并实现AI驱动科学变革潜力的关键。

Abstract: Scientific discovery has long been constrained by human limitations in
expertise, physical capability, and sleep cycles. The recent rise of AI
scientists and automated laboratories has accelerated both the cognitive and
operational aspects of research. However, key limitations persist: AI systems
are often confined to virtual environments, while automated laboratories lack
the flexibility and autonomy to adaptively test new hypotheses in the physical
world. Recent advances in embodied AI, such as generalist robot foundation
models, diffusion-based action policies, fine-grained manipulation learning,
and sim-to-real transfer, highlight the promise of integrating cognitive and
embodied intelligence. This convergence opens the door to closed-loop systems
that support iterative, autonomous experimentation and the possibility of
serendipitous discovery. In this position paper, we propose the paradigm of
Intelligent Science Laboratories (ISLs): a multi-layered, closed-loop framework
that deeply integrates cognitive and embodied intelligence. ISLs unify
foundation models for scientific reasoning, agent-based workflow orchestration,
and embodied agents for robust physical experimentation. We argue that such
systems are essential for overcoming the current limitations of scientific
discovery and for realizing the full transformative potential of AI-driven
science.

</details>


### [53] [Identifying Macro Causal Effects in C-DMGs over DMGs](https://arxiv.org/abs/2506.19650)
*Simon Ferreira,Charles K. Assaad*

Main category: cs.AI

TL;DR: 本文证明了在DMG（有向混合图）上的C-DMG（聚类有向混合图）中，do-calculus无条件地适用于识别宏观因果效应，且图形标准自然扩展到部分C-DMG。


<details>
  <summary>Details</summary>
Motivation: 在现实应用中，构建完全指定的ADMG（无环有向混合图）通常不可行，因此需要研究部分指定的因果表示，如C-DMG。此外，现实系统常存在循环因果动态，需要更通用的模型（如ioSCM）来支持循环结构。

Method: 通过输入-输出结构因果模型（ioSCM）及其诱导的DMG，研究C-DMG在DMG上的宏观因果效应识别问题，并证明do-calculus的适用性。

Result: 证明do-calculus在DMG上的C-DMG中无条件地适用于识别宏观因果效应，且非识别性标准可扩展到部分C-DMG。

Conclusion: 研究扩展了do-calculus在循环因果结构中的适用性，为高维和复杂系统提供了更灵活的因果分析工具。

Abstract: The do-calculus is a sound and complete tool for identifying causal effects
in acyclic directed mixed graphs (ADMGs) induced by structural causal models
(SCMs). However, in many real-world applications, especially in
high-dimensional setting, constructing a fully specified ADMG is often
infeasible. This limitation has led to growing interest in partially specified
causal representations, particularly through cluster-directed mixed graphs
(C-DMGs), which group variables into clusters and offer a more abstract yet
practical view of causal dependencies. While these representations can include
cycles, recent work has shown that the do-calculus remains sound and complete
for identifying macro-level causal effects in C-DMGs over ADMGs under the
assumption that all clusters size are greater than 1. Nevertheless, real-world
systems often exhibit cyclic causal dynamics at the structural level. To
account for this, input-output structural causal models (ioSCMs) have been
introduced as a generalization of SCMs that allow for cycles. ioSCMs induce
another type of graph structure known as a directed mixed graph (DMG).
Analogous to the ADMG setting, one can define C-DMGs over DMGs as high-level
representations of causal relations among clusters of variables. In this paper,
we prove that, unlike in the ADMG setting, the do-calculus is unconditionally
sound and complete for identifying macro causal effects in C-DMGs over DMGs.
Furthermore, we show that the graphical criteria for non-identifiability of
macro causal effects previously established C-DMGs over ADMGs naturally extends
to a subset of C-DMGs over DMGs.

</details>


### [54] [From memories to maps: Mechanisms of in context reinforcement learning in transformers](https://arxiv.org/abs/2506.19686)
*Ching Fang,Kanaka Rajan*

Main category: cs.AI

TL;DR: 该论文研究了通过变换器（Transformer）模拟快速适应学习的能力，发现其通过上下文结构学习和跨上下文对齐支持表征学习，并揭示了记忆在灵活行为中的作用。


<details>
  <summary>Details</summary>
Motivation: 探索人类和动物快速适应新环境的能力，以及标准强化学习算法未能捕捉到的机制，尤其是通过变换器模拟大脑中的情景记忆系统。

Method: 训练变换器在受啮齿动物行为启发的规划任务分布中进行上下文强化学习，并分析模型中涌现的学习算法。

Result: 发现表征学习通过上下文结构学习和跨上下文对齐实现，强化学习策略不同于标准模型，而是通过缓存中间计算支持。

Conclusion: 记忆可作为计算资源存储经验和缓存计算，支持灵活行为，且模型表征与大脑海马-内嗅系统相关，为快速适应提供了机制假设。

Abstract: Humans and animals show remarkable learning efficiency, adapting to new
environments with minimal experience. This capability is not well captured by
standard reinforcement learning algorithms that rely on incremental value
updates. Rapid adaptation likely depends on episodic memory -- the ability to
retrieve specific past experiences to guide decisions in novel contexts.
Transformers provide a useful setting for studying these questions because of
their ability to learn rapidly in-context and because their key-value
architecture resembles episodic memory systems in the brain. We train a
transformer to in-context reinforcement learn in a distribution of planning
tasks inspired by rodent behavior. We then characterize the learning algorithms
that emerge in the model. We first find that representation learning is
supported by in-context structure learning and cross-context alignment, where
representations are aligned across environments with different sensory stimuli.
We next demonstrate that the reinforcement learning strategies developed by the
model are not interpretable as standard model-free or model-based planning.
Instead, we show that in-context reinforcement learning is supported by caching
intermediate computations within the model's memory tokens, which are then
accessed at decision time. Overall, we find that memory may serve as a
computational resource, storing both raw experience and cached computations to
support flexible behavior. Furthermore, the representations developed in the
model resemble computations associated with the hippocampal-entorhinal system
in the brain, suggesting that our findings may be relevant for natural
cognition. Taken together, our work offers a mechanistic hypothesis for the
rapid adaptation that underlies in-context learning in artificial and natural
settings.

</details>


### [55] [Toward Decision-Oriented Prognostics: An Integrated Estimate-Optimize Framework for Predictive Maintenance](https://arxiv.org/abs/2506.19698)
*Zhuojun Xie,Adam Abdin,Yiping Fang*

Main category: cs.AI

TL;DR: 论文提出了一种集成估计优化（IEO）框架，通过联合调整预测模型并直接优化维护结果，减少预测误差对维护决策的影响，相比传统框架（ETO）平均维护后悔值降低22%。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于解决机器学习在预测性维护（PdM）中因模型误设导致的决策不一致和次优问题，探索预测准确性是否必然带来更好的维护决策。

Method: 方法包括提出IEO框架，开发适用于小样本数据的随机扰动梯度下降算法，并通过理论分析和实证评估验证其有效性。

Result: 结果表明，IEO框架在涡轮风扇维护案例中平均维护后悔值比ETO降低22%，显著提升了决策质量和鲁棒性。

Conclusion: 结论指出，IEO框架通过将预测模型训练与维护目标对齐，有效管理预测误差，支持在不确定环境中更可靠的维护规划。

Abstract: Recent research increasingly integrates machine learning (ML) into predictive
maintenance (PdM) to reduce operational and maintenance costs in data-rich
operational settings. However, uncertainty due to model misspecification
continues to limit widespread industrial adoption. This paper proposes a PdM
framework in which sensor-driven prognostics inform decision-making under
economic trade-offs within a finite decision space. We investigate two key
questions: (1) Does higher predictive accuracy necessarily lead to better
maintenance decisions? (2) If not, how can the impact of prediction errors on
downstream maintenance decisions be mitigated? We first demonstrate that in the
traditional estimate-then-optimize (ETO) framework, errors in probabilistic
prediction can result in inconsistent and suboptimal maintenance decisions. To
address this, we propose an integrated estimate-optimize (IEO) framework that
jointly tunes predictive models while directly optimizing for maintenance
outcomes. We establish theoretical finite-sample guarantees on decision
consistency under standard assumptions. Specifically, we develop a stochastic
perturbation gradient descent algorithm suitable for small run-to-failure
datasets. Empirical evaluations on a turbofan maintenance case study show that
the IEO framework reduces average maintenance regret up to 22% compared to ETO.
This study provides a principled approach to managing prediction errors in
data-driven PdM. By aligning prognostic model training with maintenance
objectives, the IEO framework improves robustness under model misspecification
and improves decision quality. The improvement is particularly pronounced when
the decision-making policy is misaligned with the decision-maker's target.
These findings support more reliable maintenance planning in uncertain
operational environments.

</details>


### [56] [LLM-Driven Medical Document Analysis: Enhancing Trustworthy Pathology and Differential Diagnosis](https://arxiv.org/abs/2506.19702)
*Lei Kang,Xuanshuo Fu,Oriol Ramos Terrades,Javier Vazquez-Corral,Ernest Valveny,Dimosthenis Karatzas*

Main category: cs.AI

TL;DR: 提出了一种基于LLaMA-v3和低秩适配的可信医疗文档分析平台，用于解决隐私问题和提升鉴别诊断性能。


<details>
  <summary>Details</summary>
Motivation: 医疗文档分析对临床诊断至关重要，但现有大型语言模型（LLMs）因隐私问题难以直接应用于临床。

Method: 通过低秩适配微调LLaMA-v3，利用DDXPlus数据集优化鉴别诊断任务。

Result: 在病理预测和变长鉴别诊断任务中表现优于现有方法，并提供可解释性结果。

Conclusion: 该方法为医疗文档分析提供了可靠、可解释且隐私保护的解决方案，具有实际临床应用价值。

Abstract: Medical document analysis plays a crucial role in extracting essential
clinical insights from unstructured healthcare records, supporting critical
tasks such as differential diagnosis. Determining the most probable condition
among overlapping symptoms requires precise evaluation and deep medical
expertise. While recent advancements in large language models (LLMs) have
significantly enhanced performance in medical document analysis, privacy
concerns related to sensitive patient data limit the use of online LLMs
services in clinical settings. To address these challenges, we propose a
trustworthy medical document analysis platform that fine-tunes a LLaMA-v3 using
low-rank adaptation, specifically optimized for differential diagnosis tasks.
Our approach utilizes DDXPlus, the largest benchmark dataset for differential
diagnosis, and demonstrates superior performance in pathology prediction and
variable-length differential diagnosis compared to existing methods. The
developed web-based platform allows users to submit their own unstructured
medical documents and receive accurate, explainable diagnostic results. By
incorporating advanced explainability techniques, the system ensures
transparent and reliable predictions, fostering user trust and confidence.
Extensive evaluations confirm that the proposed method surpasses current
state-of-the-art models in predictive accuracy while offering practical utility
in clinical settings. This work addresses the urgent need for reliable,
explainable, and privacy-preserving artificial intelligence solutions,
representing a significant advancement in intelligent medical document analysis
for real-world healthcare applications. The code can be found at
\href{https://github.com/leitro/Differential-Diagnosis-LoRA}{https://github.com/leitro/Differential-Diagnosis-LoRA}.

</details>


### [57] [From Reproduction to Replication: Evaluating Research Agents with Progressive Code Masking](https://arxiv.org/abs/2506.19724)
*Gyeongwon James Kim,Alex Wilf,Louis-Philippe Morency,Daniel Fried*

Main category: cs.AI

TL;DR: AutoExperiment是一个评估AI代理在给定自然语言描述和部分代码的情况下实现和运行机器学习实验能力的基准测试。


<details>
  <summary>Details</summary>
Motivation: 目前缺乏评估AI代理在从部分代码到完全重新实现代码的范围内实现科学想法的基准测试。

Method: 通过提供研究论文、部分代码和实验命令，要求AI代理生成缺失代码并在沙盒环境中运行实验。难度通过缺失函数数量$n$调节。

Result: 性能随$n$增加迅速下降；动态交互环境的代理表现优于固定环境；单次与多次尝试成功率差异显著。

Conclusion: AutoExperiment为评估AI驱动的科学实验进展提供了新基准，凸显了长时程代码生成和自主实验执行的挑战。

Abstract: Recent progress in autonomous code generation has fueled excitement around AI
agents capable of accelerating scientific discovery by running experiments.
However, there is currently no benchmark that evaluates whether such agents can
implement scientific ideas when given varied amounts of code as a starting
point, interpolating between reproduction (running code) and from-scratch
replication (fully re-implementing and running code). We introduce
AutoExperiment, a benchmark that evaluates AI agents' ability to implement and
run machine learning experiments based on natural language descriptions in
research papers. In each task, agents are given a research paper, a codebase
with key functions masked out, and a command to run the experiment. The goal is
to generate the missing code, execute the experiment in a sandboxed
environment, and reproduce the results. AutoExperiment scales in difficulty by
varying the number of missing functions $n$, ranging from partial reproduction
to full replication. We evaluate state-of-the-art agents and find that
performance degrades rapidly as $n$ increases. Agents that can dynamically
interact with the environment (e.g. to debug their code) can outperform agents
in fixed "agentless" harnesses, and there exists a significant gap between
single-shot and multi-trial success rates (Pass@1 vs. Pass@5), motivating
verifier approaches to our benchmark. Our findings highlight critical
challenges in long-horizon code generation, context retrieval, and autonomous
experiment execution, establishing AutoExperiment as a new benchmark for
evaluating progress in AI-driven scientific experimentation. Our data and code
are open-sourced at https://github.com/j1mk1m/AutoExperiment .

</details>


### [58] [Automatic Prompt Optimization for Knowledge Graph Construction: Insights from an Empirical Study](https://arxiv.org/abs/2506.19773)
*Nandana Mihindukulasooriya,Niharika S. D'Souza,Faisal Chowdhury,Horst Samulowitz*

Main category: cs.AI

TL;DR: 论文研究了自动提示优化在知识图谱三元组抽取任务中的应用，通过实验评估不同设置，发现自动优化提示能生成接近人工的合理提示，并在复杂模式和长文本中表现更好。


<details>
  <summary>Details</summary>
Motivation: 解决手工设计提示的耗时和脆弱性问题，探索自动提示优化在三元组抽取任务中的效果。

Method: 实验评估不同设置（如提示策略、LLM选择、模式复杂度、文本长度等），比较三种自动提示优化工具（DSPy、APE、TextGrad）在两个数据集（SynthIE、REBEL）上的表现。

Result: 自动提示优化能生成合理提示，提升三元组抽取效果，尤其在复杂模式和长文本中表现更优。

Conclusion: 自动提示优化技术可有效替代手工设计提示，提升知识图谱构建效率。

Abstract: A KG represents a network of entities and illustrates relationships between
them. KGs are used for various applications, including semantic search and
discovery, reasoning, decision-making, natural language processing, machine
learning, and recommendation systems. Triple (subject-relation-object)
extraction from text is the fundamental building block of KG construction and
has been widely studied, for example, in early benchmarks such as ACE 2002 to
more recent ones, such as WebNLG 2020, REBEL and SynthIE. While the use of LLMs
is explored for KG construction, handcrafting reasonable task-specific prompts
for LLMs is a labour-intensive exercise and can be brittle due to subtle
changes in the LLM models employed. Recent work in NLP tasks (e.g. autonomy
generation) uses automatic prompt optimization/engineering to address this
challenge by generating optimal or near-optimal task-specific prompts given
input-output examples.
  This empirical study explores the application of automatic prompt
optimization for the triple extraction task using experimental benchmarking. We
evaluate different settings by changing (a) the prompting strategy, (b) the LLM
being used for prompt optimization and task execution, (c) the number of
canonical relations in the schema (schema complexity), (d) the length and
diversity of input text, (e) the metric used to drive the prompt optimization,
and (f) the dataset being used for training and testing. We evaluate three
different automatic prompt optimizers, namely, DSPy, APE, and TextGrad and use
two different triple extraction datasets, SynthIE and REBEL. Through rigorous
empirical evaluation, our main contribution highlights that automatic prompt
optimization techniques can generate reasonable prompts similar to humans for
triple extraction. In turn, these optimized prompts achieve improved results,
particularly with increasing schema complexity and text size.

</details>


### [59] [SAGE: Strategy-Adaptive Generation Engine for Query Rewriting](https://arxiv.org/abs/2506.19783)
*Teng Wang,Hailei Gong,Changwang Zhang,Jun Wang*

Main category: cs.AI

TL;DR: 论文提出了一种策略引导的查询重写方法SAGE，通过专家策略和新型奖励机制提升检索效果，同时降低推理成本。


<details>
  <summary>Details</summary>
Motivation: 当前查询重写方法需要大量监督数据或强化学习探索效率低，限制了其应用。

Method: 引入策略自适应生成引擎（SAGE），结合专家策略和强化学习框架，采用战略信用塑造（SCS）和对比奖励塑造（CRS）机制。

Result: 在多个基准测试中取得最优NDCG@10结果，同时减少探索成本并生成简洁重写。

Conclusion: 策略引导的强化学习结合精细奖励机制，为信息检索系统提供了高效、可扩展且可解释的新范式。

Abstract: Query rewriting is pivotal for enhancing dense retrieval, yet current methods
demand large-scale supervised data or suffer from inefficient reinforcement
learning (RL) exploration. In this work, we first establish that guiding Large
Language Models (LLMs) with a concise set of expert-crafted strategies, such as
semantic expansion and entity disambiguation, substantially improves retrieval
effectiveness on challenging benchmarks, including HotpotQA, FEVER, NFCorpus,
and SciFact. Building on this insight, we introduce the Strategy-Adaptive
Generation Engine (SAGE), which operationalizes these strategies in an RL
framework. SAGE introduces two novel reward shaping mechanisms-Strategic Credit
Shaping (SCS) and Contrastive Reward Shaping (CRS)-to deliver more informative
learning signals. This strategy-guided approach not only achieves new
state-of-the-art NDCG@10 results, but also uncovers a compelling emergent
behavior: the agent learns to select optimal strategies, reduces unnecessary
exploration, and generates concise rewrites, lowering inference cost without
sacrificing performance. Our findings demonstrate that strategy-guided RL,
enhanced with nuanced reward shaping, offers a scalable, efficient, and more
interpretable paradigm for developing the next generation of robust information
retrieval systems.

</details>


### [60] [Learning Task Belief Similarity with Latent Dynamics for Meta-Reinforcement Learning](https://arxiv.org/abs/2506.19785)
*Menglong Zhang,Fuyuan Qian*

Main category: cs.AI

TL;DR: SimBelief是一种新的元强化学习框架，通过测量任务信念的相似性来优化稀疏奖励环境中的任务识别和探索。


<details>
  <summary>Details</summary>
Motivation: 现有方法在稀疏奖励环境中难以准确重建奖励信号，导致探索效率低下。

Method: 提出SimBelief框架，利用潜在任务信念度量学习相似任务的共同结构，并将其与特定任务信念结合。

Result: 在稀疏奖励的MuJoCo和panda-gym任务上表现优于现有基线方法。

Conclusion: SimBelief通过任务信念相似性度量，显著提升了稀疏奖励环境中的任务识别和适应效率。

Abstract: Meta-reinforcement learning requires utilizing prior task distribution
information obtained during exploration to rapidly adapt to unknown tasks. The
efficiency of an agent's exploration hinges on accurately identifying the
current task. Recent Bayes-Adaptive Deep RL approaches often rely on
reconstructing the environment's reward signal, which is challenging in sparse
reward settings, leading to suboptimal exploitation. Inspired by bisimulation
metrics, which robustly extracts behavioral similarity in continuous MDPs, we
propose SimBelief-a novel meta-RL framework via measuring similarity of task
belief in Bayes-Adaptive MDP (BAMDP). SimBelief effectively extracts common
features of similar task distributions, enabling efficient task identification
and exploration in sparse reward environments. We introduce latent task belief
metric to learn the common structure of similar tasks and incorporate it into
the specific task belief. By learning the latent dynamics across task
distributions, we connect shared latent task belief features with specific task
features, facilitating rapid task identification and adaptation. Our method
outperforms state-of-the-art baselines on sparse reward MuJoCo and panda-gym
tasks.

</details>


### [61] [KnowRL: Exploring Knowledgeable Reinforcement Learning for Factuality](https://arxiv.org/abs/2506.19807)
*Baochang Ren,Shuofei Qiao,Wenhao Yu,Huajun Chen,Ningyu Zhang*

Main category: cs.AI

TL;DR: KnowRL通过将基于知识验证的事实性奖励整合到RL训练中，减少慢思考模型中的幻觉问题，同时保持其推理能力。


<details>
  <summary>Details</summary>
Motivation: 解决慢思考模型在推理过程中因无法准确识别知识边界而导致的严重幻觉问题。

Method: 提出Knowledge-enhanced RL (KnowRL)，在RL训练中引入基于知识验证的事实性奖励，指导模型进行基于事实的慢思考。

Result: 在三个幻觉评估数据集和两个推理评估数据集上，KnowRL有效减少了幻觉问题，同时保持了模型的推理能力。

Conclusion: KnowRL通过事实性奖励显著改善了慢思考模型的幻觉问题，同时不影响其推理性能。

Abstract: Large Language Models (LLMs), particularly slow-thinking models, often
exhibit severe hallucination, outputting incorrect content due to an inability
to accurately recognize knowledge boundaries during reasoning. While
Reinforcement Learning (RL) can enhance complex reasoning abilities, its
outcome-oriented reward mechanism often lacks factual supervision over the
thinking process, further exacerbating the hallucination problem. To address
the high hallucination in slow-thinking models, we propose Knowledge-enhanced
RL, KnowRL. KnowRL guides models to perform fact-based slow thinking by
integrating a factuality reward, based on knowledge verification, into the RL
training process, helping them recognize their knowledge boundaries. KnowRL
guides models to perform fact-based slow thinking by integrating a factuality
reward, based on knowledge verification, into the RL training process, helping
them recognize their knowledge boundaries. This targeted factual input during
RL training enables the model to learn and internalize fact-based reasoning
strategies. By directly rewarding adherence to facts within the reasoning
steps, KnowRL fosters a more reliable thinking process. Experimental results on
three hallucination evaluation datasets and two reasoning evaluation datasets
demonstrate that KnowRL effectively mitigates hallucinations in slow-thinking
models while maintaining their original strong reasoning capabilities. Our code
is available at https://github.com/zjunlp/KnowRL.

</details>


### [62] [Evaluating Compliance with Visualization Guidelines in Diagrams for Scientific Publications Using Large Vision Language Models](https://arxiv.org/abs/2506.19825)
*Johannes Rückert,Louise Bloch,Christoph M. Friedrich*

Main category: cs.AI

TL;DR: 该研究利用视觉语言模型（VLMs）分析图表，以识别数据可视化原则中的潜在问题，并比较了不同模型和提示策略的效果。


<details>
  <summary>Details</summary>
Motivation: 研究人员常因不了解或不遵循数据可视化原则，导致图表信息不准确或不完整。本研究旨在通过VLMs自动识别这些问题。

Method: 使用五种开源VLMs和五种提示策略，基于数据可视化指南设计问题集进行评估。

Result: VLMs在分析图表类型、3D效果、轴标签等方面表现良好，但在图像质量和刻度标记上表现不佳。Qwen2.5VL模型和总结提示策略效果最佳。

Conclusion: VLMs可自动识别图表中的潜在问题，如缺失轴标签和图例，未来可扩展应用于更多数据可视化领域。

Abstract: Diagrams are widely used to visualize data in publications. The research
field of data visualization deals with defining principles and guidelines for
the creation and use of these diagrams, which are often not known or adhered to
by researchers, leading to misinformation caused by providing inaccurate or
incomplete information.
  In this work, large Vision Language Models (VLMs) are used to analyze
diagrams in order to identify potential problems in regards to selected data
visualization principles and guidelines. To determine the suitability of VLMs
for these tasks, five open source VLMs and five prompting strategies are
compared using a set of questions derived from selected data visualization
guidelines.
  The results show that the employed VLMs work well to accurately analyze
diagram types (F1-score 82.49 %), 3D effects (F1-score 98.55 %), axes labels
(F1-score 76.74 %), lines (RMSE 1.16), colors (RMSE 1.60) and legends (F1-score
96.64 %, RMSE 0.70), while they cannot reliably provide feedback about the
image quality (F1-score 0.74 %) and tick marks/labels (F1-score 46.13 %). Among
the employed VLMs, Qwen2.5VL performs best, and the summarizing prompting
strategy performs best for most of the experimental questions.
  It is shown that VLMs can be used to automatically identify a number of
potential issues in diagrams, such as missing axes labels, missing legends, and
unnecessary 3D effects. The approach laid out in this work can be extended for
further aspects of data visualization.

</details>


### [63] [Temporal-IRL: Modeling Port Congestion and Berth Scheduling with Inverse Reinforcement Learning](https://arxiv.org/abs/2506.19843)
*Guo Li,Zixiang Xu,Wei Zhang,Yikuan Hu,Xinyu Yang,Nikolay Aristov,Mingjie Tang,Elenna R Dugundji*

Main category: cs.AI

TL;DR: 该论文提出了一种基于逆向强化学习（IRL）的Temporal-IRL模型，用于预测港口拥堵，通过分析船舶行为和停泊时间，优化供应链管理。


<details>
  <summary>Details</summary>
Motivation: 预测港口拥堵对全球供应链的可靠性至关重要，能够优化运输计划、减少延误和成本，并增强供应链韧性。

Method: 利用历史AIS数据重建停泊计划，通过Temporal-IRL模型学习停泊调度优先级，预测船舶序列和停泊时间。

Result: 模型在纽约/新泽西港的Maher Terminal数据上训练和测试，表现出色。

Conclusion: Temporal-IRL模型能有效预测港口拥堵，为供应链管理提供可靠支持。

Abstract: Predicting port congestion is crucial for maintaining reliable global supply
chains. Accurate forecasts enableimprovedshipment planning, reducedelaysand
costs, and optimizeinventoryanddistributionstrategies, thereby ensuring timely
deliveries and enhancing supply chain resilience. To achieve accurate
predictions, analyzing vessel behavior and their stay times at specific port
terminals is essential, focusing particularly on berth scheduling under various
conditions. Crucially, the model must capture and learn the underlying
priorities and patterns of berth scheduling. Berth scheduling and planning are
influenced by a range of factors, including incoming vessel size, waiting
times, and the status of vessels within the port terminal. By observing
historical Automatic Identification System (AIS) positions of vessels, we
reconstruct berth schedules, which are subsequently utilized to determine the
reward function via Inverse Reinforcement Learning (IRL). For this purpose, we
modeled a specific terminal at the Port of New York/New Jersey and developed
Temporal-IRL. This Temporal-IRL model learns berth scheduling to predict vessel
sequencing at the terminal and estimate vessel port stay, encompassing both
waiting and berthing times, to forecast port congestion. Utilizing data from
Maher Terminal spanning January 2015 to September 2023, we trained and tested
the model, achieving demonstrably excellent results.

</details>


### [64] [JoyAgents-R1: Joint Evolution Dynamics for Versatile Multi-LLM Agents with Reinforcement Learning](https://arxiv.org/abs/2506.19846)
*Ai Han,Junxing Hu,Pu Wei,Zhiqian Zhang,Yuhang Guo,Jiawei Lu,Zicheng Zhang*

Main category: cs.AI

TL;DR: JoyAgents-R1提出了一种多智能体强化学习方法，通过GRPO和自适应记忆机制提升训练效率和稳定性。


<details>
  <summary>Details</summary>
Motivation: 解决多智能体强化学习中合作效率低和训练不稳定的问题。

Method: 采用GRPO进行联合训练，结合蒙特卡洛采样和边际效益驱动策略优化模型更新。

Result: 在通用和特定领域场景中表现优异，性能接近更大规模模型。

Conclusion: JoyAgents-R1为多智能体强化学习提供了一种高效稳定的解决方案。

Abstract: Multi-agent reinforcement learning (MARL) has emerged as a prominent paradigm
for increasingly complex tasks. However, joint evolution across heterogeneous
agents remains challenging due to cooperative inefficiency and training
instability. In this paper, we propose the joint evolution dynamics for MARL
called JoyAgents-R1, which first applies Group Relative Policy Optimization
(GRPO) to the joint training of heterogeneous multi-agents. By iteratively
refining agents' large language models (LLMs) and memories, the method achieves
holistic equilibrium with optimal decision-making and memory capabilities.
Specifically, JoyAgents-R1 first implements node-wise Monte Carlo sampling on
the behavior of each agent across entire reasoning trajectories to enhance GRPO
sampling efficiency while maintaining policy diversity. Then, our marginal
benefit-driven selection strategy identifies top-$K$ sampling groups with
maximal reward fluctuations, enabling targeted agent model updates that improve
training stability and maximize joint benefits through cost-effective parameter
adjustments. Meanwhile, JoyAgents-R1 introduces an adaptive memory evolution
mechanism that repurposes GRPO rewards as cost-free supervisory signals to
eliminate repetitive reasoning and accelerate convergence. Experiments across
general and domain-specific scenarios demonstrate that JoyAgents-R1 achieves
performance comparable to that of larger LLMs while built on smaller
open-source models.

</details>
