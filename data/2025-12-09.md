<div id=toc></div>

# Table of Contents

- [cs.CR](#cs.CR) [Total: 5]
- [cs.SE](#cs.SE) [Total: 18]
- [cs.AI](#cs.AI) [Total: 19]


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [1] [Please Don't Kill My Vibe: Empowering Agents with Data Flow Control](https://arxiv.org/abs/2512.05374)
*Charlie Summers,Haneen Mohammed,Eugene Wu*

Main category: cs.CR

TL;DR: 论文提出为LLM智能体系统开发数据流控制（DFC）机制，以解决智能体执行复杂任务时产生的政策违规、流程腐败和安全漏洞等风险。


<details>
  <summary>Details</summary>
Motivation: 当前LLM智能体在执行复杂状态任务时存在显著风险，包括政策违规、流程腐败和安全漏洞，这些问题源于缺乏对智能体行为产生的数据流的可见性和管理机制。目前智能体工作流只能以临时方式执行这些政策控制。

Method: 借鉴数据库管理系统（DBMS）将数据验证和访问控制从应用程序转移到DBMS的经验，提出在系统中原生支持数据流控制（DFC）并强制执行DFC政策。论文描述了为DBMS开发可移植DFC实例的早期工作。

Result: 论文提出了DFC的概念框架和初步实现思路，但作为早期工作，具体实验结果尚未在摘要中展示。主要成果是建立了DFC的理论基础和技术方向。

Conclusion: 需要为智能体生态系统开发数据流控制机制，将政策执行从应用程序层面转移到系统层面，从而降低风险并提高智能体系统的可靠性和安全性。论文为这一研究方向制定了更广泛的研究议程。

Abstract: The promise of Large Language Model (LLM) agents is to perform complex, stateful tasks. This promise is stunted by significant risks - policy violations, process corruption, and security flaws - that stem from the lack of visibility and mechanisms to manage undesirable data flows produced by agent actions. Today, agent workflows are responsible for enforcing these policies in ad hoc ways. Just as data validation and access controls shifted from the application to the DBMS, freeing application developers from these concerns, we argue that systems should support Data Flow Controls (DFCs) and enforce DFC policies natively. This paper describes early work developing a portable instance of DFC for DBMSes and outlines a broader research agenda toward DFC for agent ecosystems.

</details>


### [2] [PrivCode: When Code Generation Meets Differential Privacy](https://arxiv.org/abs/2512.05459)
*Zheng Liu,Chen Gong,Terry Yue Zhuo,Kecen Li,Weichen Yu,Matt Fredrikson,Tianhao Wang*

Main category: cs.CR

TL;DR: PrivCode是首个专门为代码数据集设计的差分隐私合成器，采用两阶段框架提高隐私性和实用性，在保护敏感代码的同时生成高质量代码。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在代码生成方面表现出色，但在私有数据集上微调会引发隐私和专有信息泄露问题。差分隐私代码生成虽然提供理论保护，但面临严格的语法依赖性和隐私-效用权衡的挑战。

Method: PrivCode采用两阶段框架：1) 隐私净化阶段：使用DP-SGD训练模型生成差分隐私合规的合成代码，同时引入语法信息保持代码结构；2) 效用提升阶段：在合成隐私代码上微调更大的预训练LLM，缓解DP造成的效用损失。

Result: 在四个LLM上的广泛实验表明，PrivCode在四个基准测试的各种任务中生成更高实用性的代码。实验还证实了在不同隐私预算下保护敏感数据的能力。

Conclusion: PrivCode是首个专门为代码数据集设计的差分隐私合成器，成功解决了隐私保护和代码实用性之间的平衡问题，为保护敏感代码数据提供了有效解决方案。

Abstract: Large language models (LLMs) have presented outstanding performance in code generation and completion. However, fine-tuning these models on private datasets can raise privacy and proprietary concerns, such as the leakage of sensitive personal information. Differentially private (DP) code generation provides theoretical guarantees for protecting sensitive code by generating synthetic datasets that preserve statistical properties while reducing privacy leakage concerns. However, DP code generation faces significant challenges due to the strict syntactic dependencies and the privacy-utility trade-off.
  We propose PrivCode, the first DP synthesizer specifically designed for code datasets. It incorporates a two-stage framework to improve both privacy and utility. In the first stage, termed "privacy-sanitizing", PrivCode generates DP-compliant synthetic code by training models using DP-SGD while introducing syntactic information to preserve code structure. The second stage, termed "utility-boosting", fine-tunes a larger pre-trained LLM on the synthetic privacy-free code to mitigate the utility loss caused by DP, enhancing the utility of the generated code. Extensive experiments on four LLMs show that PrivCode generates higher-utility code across various testing tasks under four benchmarks. The experiments also confirm its ability to protect sensitive data under varying privacy budgets. We provide the replication package at the anonymous link.

</details>


### [3] [TeleAI-Safety: A comprehensive LLM jailbreaking benchmark towards attacks, defenses, and evaluations](https://arxiv.org/abs/2512.05485)
*Xiuyuan Chen,Jian Zhao,Yuxiang He,Yuan Xun,Xinwei Liu,Yanshu Li,Huilin Zhou,Wei Cai,Ziyan Shi,Yuchen Yuan,Tianle Zhang,Chi Zhang,Xuelong Li*

Main category: cs.CR

TL;DR: TeleAI-Safety是一个模块化、可复现的LLM安全评估框架和基准测试，整合了19种攻击方法、29种防御方法和19种评估方法，在12个风险类别上对14个目标模型进行系统评估。


<details>
  <summary>Details</summary>
Motivation: 当前LLM安全评估存在两个主要问题：1）攻击、防御和评估方法的核心组件集成不平衡；2）灵活评估框架与标准化基准能力之间存在隔离。这些问题阻碍了可靠的跨研究比较，并为全面风险评估带来了不必要的开销。

Method: 提出了TeleAI-Safety框架，包含三个核心模块：1）攻击方法库（19种方法，含1种自研）；2）防御方法库（29种方法）；3）评估方法库（19种方法，含1种自研）。使用包含342个样本、覆盖12个风险类别的攻击语料库，对14个目标模型进行系统评估。

Result: 评估结果揭示了系统性漏洞和模型特定的失败案例，突出了安全性与实用性之间的关键权衡，并识别了未来优化的潜在防御模式。框架在实际场景中可以灵活调整攻击、防御和评估组合以满足特定需求。

Conclusion: TeleAI-Safety提供了一个模块化、可复现的LLM安全评估解决方案，解决了现有评估方法的局限性。通过发布完整代码和评估结果，促进了可复现研究并建立了统一的安全基准。

Abstract: While the deployment of large language models (LLMs) in high-value industries continues to expand, the systematic assessment of their safety against jailbreak and prompt-based attacks remains insufficient. Existing safety evaluation benchmarks and frameworks are often limited by an imbalanced integration of core components (attack, defense, and evaluation methods) and an isolation between flexible evaluation frameworks and standardized benchmarking capabilities. These limitations hinder reliable cross-study comparisons and create unnecessary overhead for comprehensive risk assessment. To address these gaps, we present TeleAI-Safety, a modular and reproducible framework coupled with a systematic benchmark for rigorous LLM safety evaluation. Our framework integrates a broad collection of 19 attack methods (including one self-developed method), 29 defense methods, and 19 evaluation methods (including one self-developed method). With a curated attack corpus of 342 samples spanning 12 distinct risk categories, the TeleAI-Safety benchmark conducts extensive evaluations across 14 target models. The results reveal systematic vulnerabilities and model-specific failure cases, highlighting critical trade-offs between safety and utility, and identifying potential defense patterns for future optimization. In practical scenarios, TeleAI-Safety can be flexibly adjusted with customized attack, defense, and evaluation combinations to meet specific demands. We release our complete code and evaluation results to facilitate reproducible research and establish unified safety baselines.

</details>


### [4] [Matching Ranks Over Probability Yields Truly Deep Safety Alignment](https://arxiv.org/abs/2512.05518)
*Jason Vega,Gagandeep Singh*

Main category: cs.CR

TL;DR: 论文提出RAP攻击可绕过基于数据增强的SFT安全对齐防御，并提出了PRESTO方法通过正则化有害预填充token的注意力来改善安全对齐效果。


<details>
  <summary>Details</summary>
Motivation: 现有基于数据增强的SFT防御方法虽然声称实现了"深度"安全对齐，但实际上仍然存在漏洞。预填充攻击的变种RAP攻击能够有效绕过这些防御，提取有害内容，这表明当前的安全对齐方法还不够深入。

Method: 1. 提出Rank-Assisted Prefilling (RAP)攻击：从top-20预测token中选择低概率的有害token，忽略高概率的拒绝token；2. 分析SFT目标在目标分布熵较低时的"博弈"问题；3. 提出PRESTO方法：通过正则化有害预填充token的注意力来匹配目标分布的token排名而非概率。

Result: PRESTO方法在三个流行的开源LLM上，将RAP攻击下的平均StrongREJECT分数提高了最多4.7倍，同时对模型实用性影响较小。

Conclusion: 当前基于数据增强的SFT安全对齐方法存在根本性缺陷，通过匹配token排名而非概率的PRESTO方法能够实现更深入的安全对齐，有效抵御RAP攻击。

Abstract: A frustratingly easy technique known as the prefilling attack has been shown to effectively circumvent the safety alignment of frontier LLMs by simply prefilling the assistant response with an affirmative prefix before decoding. In response, recent work proposed a supervised fine-tuning (SFT) defense using data augmentation to achieve a \enquote{deep} safety alignment, allowing the model to generate natural language refusals immediately following harmful prefills. Unfortunately, we show in this work that the "deep" safety alignment produced by such an approach is in fact not very deep. A generalization of the prefilling attack, which we refer to as the Rank-Assisted Prefilling (RAP) attack, can effectively extract harmful content from models fine-tuned with the data augmentation defense by selecting low-probability "harmful" tokens from the top 20 predicted next tokens at each step (thus ignoring high-probability "refusal" tokens). We argue that this vulnerability is enabled due to the "gaming" of the SFT objective when the target distribution entropies are low, where low fine-tuning loss is achieved by shifting large probability mass to a small number of refusal tokens while neglecting the high ranks of harmful tokens. We then propose a new perspective on achieving deep safety alignment by matching the token ranks of the target distribution, rather than their probabilities. This perspective yields a surprisingly simple fix to the data augmentation defense based on regularizing the attention placed on harmful prefill tokens, an approach we call PRefill attEntion STOpping (PRESTO). Adding PRESTO yields up to a 4.7x improvement in the mean StrongREJECT score under RAP attacks across three popular open-source LLMs, with low impact to model utility.

</details>


### [5] [ARGUS: Defending Against Multimodal Indirect Prompt Injection via Steering Instruction-Following Behavior](https://arxiv.org/abs/2512.05745)
*Weikai Lu,Ziqian Zeng,Kehua Zhang,Haoran Li,Huiping Zhuang,Ruidong Wang,Cen Chen,Hao Peng*

Main category: cs.CR

TL;DR: ARGUS是一种针对多模态大语言模型间接提示注入攻击的防御方法，通过在表示空间中寻找最优防御方向并结合自适应强度调控，实现安全性与实用性的平衡。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型面临来自图像、视频、音频的间接提示注入攻击威胁，现有针对纯文本LLM的防御方法无法有效应对多模态攻击，容易被绕过、依赖特定模态或泛化能力差。

Method: 研究发现MLLM的指令跟随行为编码在一个子空间中，通过在该子空间中寻找与效用退化方向解耦的最优防御方向，结合自适应强度调控。还引入轻量级注入检测阶段按需激活防御，以及后过滤阶段验证防御成功。

Result: 实验结果表明，ARGUS能够实现对多模态IPI攻击的鲁棒防御，同时最大限度地保留MLLM的实用性。

Conclusion: 通过在表示空间中寻找最优防御方向并结合自适应调控，ARGUS提供了一种独立于模态的通用防御方案，有效平衡了安全性与模型性能。

Abstract: Multimodal Large Language Models (MLLMs) are increasingly vulnerable to multimodal Indirect Prompt Injection (IPI) attacks, which embed malicious instructions in images, videos, or audio to hijack model behavior. Existing defenses, designed primarily for text-only LLMs, are unsuitable for countering these multimodal threats, as they are easily bypassed, modality-dependent, or generalize poorly. Inspired by activation steering researches, we hypothesize that a robust, general defense independent of modality can be achieved by steering the model's behavior in the representation space. Through extensive experiments, we discover that the instruction-following behavior of MLLMs is encoded in a subspace. Steering along directions within this subspace can enforce adherence to user instructions, forming the basis of a defense. However, we also found that a naive defense direction could be coupled with a utility-degrading direction, and excessive intervention strength harms model performance. To address this, we propose ARGUS, which searches for an optimal defense direction within the safety subspace that decouples from the utility degradation direction, further combining adaptive strength steering to achieve a better safety-utility trade-off. ARGUS also introduces lightweight injection detection stage to activate the defense on-demand, and a post-filtering stage to verify defense success. Experimental results show that ARGUS can achieve robust defense against multimodal IPI while maximally preserving the MLLM's utility.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [6] [Stellis: A Strategy Language for Purifying Separation Logic Entailments](https://arxiv.org/abs/2512.05159)
*Zhiyi Wang,Xiwei Wu,Yi Fang,Chengtao Li,Hongyi Zhong,Lihan Xie,Qinxiang Cao,Zhenjiang Hu*

Main category: cs.SE

TL;DR: Stellis：一种用于纯化分离逻辑蕴含的策略语言，通过移除空间公式将蕴含简化为纯蕴含，支持灵活的策略编码和自动正确性证明。


<details>
  <summary>Details</summary>
Motivation: 自动证明分离逻辑蕴含是验证中的基本挑战。基于规则的方法依赖分离逻辑规则进行自动化，但这些规则语句不足以描述自动化策略，特别是涉及特定场景中内存布局对齐和消除的情况。

Method: 提出Stellis策略语言，具有强大的匹配机制和灵活的动作描述，能够直接编码各种策略。为确保策略正确性，引入算法为每个策略生成正确性条件，将策略正确性简化为其正确性条件的正确性。基于机械化的归约正确性定理，原型实现为整体自动化生成正确性证明。

Result: 在包含229个蕴含的基准测试中（来自标准链表数据结构和微内核内存模块的验证），使用5个库中的98个策略，系统自动纯化了95.6%（219/229）的蕴含，证明了系统的高效性。

Conclusion: Stellis在提供灵活性和便利性的同时，也保持了高效性，为分离逻辑蕴含的自动化证明提供了有效的策略语言框架。

Abstract: Automatically proving separation logic entailments is a fundamental challenge in verification. While rule-based methods rely on separation logic rules (lemmas) for automation, these rule statements are insufficient for describing automation strategies, which usually involve the alignment and elimination of corresponding memory layouts in specific scenarios. To overcome this limitation, we propose Stellis, a strategy language for purifying separation logic entailments, i.e., removing all spatial formulas to reduce the entailment to a simpler pure entailment. Stellis features a powerful matching mechanism and a flexible action description, enabling the straightforward encoding of a wide range of strategies. To ensure strategy soundness, we introduce an algorithm that generates a soundness condition for each strategy, thereby reducing the soundness of each strategy to the correctness of its soundness condition. Furthermore, based on a mechanized reduction soundness theorem, our prototype implementation generates correctness proofs for the overall automation. We evaluate our system on a benchmark of 229 entailments collected from verification of standard linked data structures and the memory module of a microkernel, and the evaluation results demonstrate that, with such flexibility and convenience provided, our system is also highly effective, which automatically purifies 95.6% (219 out of 229) of the entailments using 5 libraries with 98 strategies.

</details>


### [7] [Towards A Cultural Intelligence and Values Inferences Quality Benchmark for Community Values and Common Knowledge](https://arxiv.org/abs/2512.05176)
*Brittany Johnson,Erin Reddick,Angela D. R. Smith*

Main category: cs.SE

TL;DR: 该研究提出将韩国国家LLM对齐基准KorNAT的方法复制到美国情境，开发CIVIQ基准，以评估LLM与社区社会价值观和常识的对齐程度，而非仅关注国家层面的对齐。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型主要与西方白人叙事对齐，与少数文化群体存在错位。虽然已有文化感知LLM的开发，但缺乏相应的开发和评估方法。现有的国家对齐基准在美国多元文化背景下效果有限。

Method: 采用复制研究方法，将韩国国家LLM对齐基准KorNAT的开发过程翻译到美国情境，开发CIVIQ基准，重点关注与社区社会价值观和常识的对齐。

Result: 提出了CIVIQ基准，这是一个文化智能和价值观推断质量基准，旨在评估LLM与美国多元文化社区价值观和常识的对齐程度。

Conclusion: 该工作为实践中AI技术文化对齐的研究和开发提供了关键基础，强调了在多元文化国家中需要超越国家层面的对齐，关注社区层面的文化代表性。

Abstract: Large language models (LLMs) have emerged as a powerful technology, and thus, we have seen widespread adoption and use on software engineering teams. Most often, LLMs are designed as "general purpose" technologies meant to represent the general population. Unfortunately, this often means alignment with predominantly Western Caucasian narratives and misalignment with other cultures and populations that engage in collaborative innovation. In response to this misalignment, there have been recent efforts centered on the development of "culturally-informed" LLMs, such as ChatBlackGPT, that are capable of better aligning with historically marginalized experiences and perspectives. Despite this progress, there has been little effort aimed at supporting our ability to develop and evaluate culturally-informed LLMs. A recent effort proposed an approach for developing a national alignment benchmark that emphasizes alignment with national social values and common knowledge. However, given the range of cultural identities present in the United States (U.S.), a national alignment benchmark is an ineffective goal for broader representation. To help fill this gap in this US context, we propose a replication study that translates the process used to develop KorNAT, a Korean National LLM alignment benchmark, to develop CIVIQ, a Cultural Intelligence and Values Inference Quality benchmark centered on alignment with community social values and common knowledge. Our work provides a critical foundation for research and development aimed at cultural alignment of AI technologies in practice.

</details>


### [8] [A Survey of Bugs in AI-Generated Code](https://arxiv.org/abs/2512.05239)
*Ruofan Gao,Amjed Tahir,Peng Liang,Teo Susnjak,Foutse Khomh*

Main category: cs.SE

TL;DR: 本文系统分析了AI生成代码中的缺陷类型、分布模式及修复策略，为模型改进和质量评估提供参考


<details>
  <summary>Details</summary>
Motivation: AI代码生成模型被广泛使用以提高开发效率，但生成的代码存在质量问题和缺陷，这些问题可能导致信任和维护挑战。现有研究缺乏对AI生成代码缺陷类型、分布、修复策略及其与特定模型关联的系统性总结。

Method: 系统分析现有AI生成代码文献，建立对生成代码中缺陷的整体理解，提供缺陷类型分类和模式识别，并讨论可能的修复和缓解策略。

Result: 建立了AI生成代码缺陷的分类体系，揭示了不同模型生成的代码中缺陷类型和分布模式，总结了消除生成代码缺陷的修复策略。

Conclusion: 通过系统性文献分析，为AI代码生成模型的改进和质量评估提供了重要参考，有助于理解AI生成代码中缺陷的本质和范围，并为未来的研究和实践提供指导。

Abstract: Developers are widely using AI code-generation models, aiming to increase productivity and efficiency. However, there are also quality concerns regarding the AI-generated code. The generated code is produced by models trained on publicly available code, which are known to contain bugs and quality issues. Those issues can cause trust and maintenance challenges during the development process. Several quality issues associated with AI-generated code have been reported, including bugs and defects. However, these findings are often scattered and lack a systematic summary. A comprehensive review is currently lacking to reveal the types and distribution of these errors, possible remediation strategies, as well as their correlation with the specific models. In this paper, we systematically analyze the existing AI-generated code literature to establish an overall understanding of bugs and defects in generated code, providing a reference for future model improvement and quality assessment. We aim to understand the nature and extent of bugs in AI-generated code, and provide a classification of bug types and patterns present in code generated by different models. We also discuss possible fixes and mitigation strategies adopted to eliminate bugs from the generated code.

</details>


### [9] [Learning to Code with Context: A Study-Based Approach](https://arxiv.org/abs/2512.05242)
*Uwe M. Borghoff,Mark Minas,Jannis Schopp*

Main category: cs.SE

TL;DR: 该研究探讨了在软件工程教育中整合生成式AI工具的方法，通过在大学游戏开发项目中开展用户研究，分析学生使用AI工具的情况，并开发了一个基于RAG的本地LLM助手来提供项目上下文支持。


<details>
  <summary>Details</summary>
Motivation: 随着生成式AI工具的快速发展，软件工程教育需要适应这一变革，确保学生不仅掌握传统开发方法，还能有意义且负责任地使用新技术。项目制课程为探索AI辅助工具在实际开发实践中的整合提供了有效环境。

Method: 研究采用用户研究方法，在大学编程项目中让学生协作开发电脑游戏，观察他们在软件开发不同阶段如何使用生成式AI工具。同时开发了一个基于检索增强生成（RAG）的本地部署大型语言模型助手，该系统能够利用项目文档和源代码提供上下文感知支持。

Result: 研究发现揭示了学生在软件开发过程中使用生成式AI工具的具体模式，识别了AI工具最有效的任务类型，分析了学生遇到的挑战。通过RAG系统的定性分析，深入了解了模型行为、参数敏感性和常见失败模式。

Conclusion: 该研究加深了对教育软件项目中上下文感知AI支持的理解，为未来将AI辅助工具整合到软件工程课程中提供了重要见解，有助于指导软件工程教育的适应性改革。

Abstract: The rapid emergence of generative AI tools is transforming the way software is developed. Consequently, software engineering education must adapt to ensure that students not only learn traditional development methods but also understand how to meaningfully and responsibly use these new technologies. In particular, project-based courses offer an effective environment to explore and evaluate the integration of AI assistance into real-world development practices. This paper presents our approach and a user study conducted within a university programming project in which students collaboratively developed computer games. The study investigates how participants used generative AI tools throughout different phases of the software development process, identifies the types of tasks where such tools were most effective, and analyzes the challenges students encountered. Building on these insights, we further examine a repository-aware, locally deployed large language model (LLM) assistant designed to provide project-contextualized support. The system employs Retrieval-Augmented Generation (RAG) to ground responses in relevant documentation and source code, enabling qualitative analysis of model behavior, parameter sensitivity, and common failure modes. The findings deepen our understanding of context-aware AI support in educational software projects and inform future integration of AI-based assistance into software engineering curricula.

</details>


### [10] [Engagement in Code Review: Emotional, Behavioral, and Cognitive Dimensions in Peer vs. LLM Interactions](https://arxiv.org/abs/2512.05309)
*Adam Alami,Nathan Cassee,Thiago Rocha Silva,Elda Paja,Neil A. Ernst*

Main category: cs.SE

TL;DR: 研究比较了人类同行评审与LLM辅助代码评审中软件工程师的情感反应、自我调节策略和参与行为，发现LLM辅助评审减少了情感负担，将关注点从情绪管理转向认知负荷管理。


<details>
  <summary>Details</summary>
Motivation: 代码评审是一种社会技术实践，但软件工程师在LLM辅助代码评审与人类同行评审中的参与方式差异尚不明确。研究旨在理解工程师在这两种评审模式中的情感反应、参与决策和反馈采纳过程。

Method: 采用两阶段定性研究：第一阶段，20名软件工程师进行同行评审并接受访谈，了解情感反应和参与决策；第二阶段，引入符合工程师偏好的提示，探究特征如何影响反应。开发了连接情感自我调节与行为参与和解决方案的综合模型。

Result: 识别了工程师面对负面反馈的四种自我调节策略：重构、对话调节、回避和防御。参与通过社会校准进行，工程师根据关系氛围和团队规范调整反应和行为。在同行评审中，解决方案轨迹因焦点（个人/双人/团队）和内部意义建构过程而异。LLM辅助评审降低了情感成本和自我调节需求，当LLM反馈符合工程师认知期望时，减少了处理努力并提高了采纳倾向。

Conclusion: LLM辅助评审将参与重点从情绪管理转向认知负荷管理。AI最适合作为支持性伙伴，减少认知和情感负担，同时保留人类责任和同行评审等社会技术活动的社会意义。提出了连接情感自我调节与行为参与和解决方案的综合模型。

Abstract: Code review is a socio-technical practice, yet how software engineers engage in Large Language Model (LLM)-assisted code reviews compared to human peer-led reviews is less understood. We report a two-phase qualitative study with 20 software engineers to understand this. In Phase I, participants exchanged peer reviews and were interviewed about their affective responses and engagement decisions. In Phase II, we introduced a new prompt matching engineers' preferences and probed how characteristics shaped their reactions. We develop an integrative account linking emotional self-regulation to behavioral engagement and resolution. We identify self-regulation strategies that engineers use to regulate their emotions in response to negative feedback: reframing, dialogic regulation, avoidance, and defensiveness. Engagement proceeds through social calibration; engineers align their responses and behaviors to the relational climate and team norms. Trajectories to resolution, in the case of peer-led review, vary by locus (solo/dyad/team) and an internal sense-making process. With the LLM-assisted review, emotional costs and the need for self-regulation seem lower. When LLM feedback aligned with engineers' cognitive expectations, participants reported reduced processing effort and a potentially higher tendency to adopt. We show that LLM-assisted review redirects engagement from emotion management to cognitive load management. We contribute an integrative model of engagement that links emotional self-regulation to behavioral engagement and resolution, showing how affective and cognitive processes influence feedback adoption in peer-led and LLM-assisted code reviews. We conclude that AI is best positioned as a supportive partner to reduce cognitive and emotional load while preserving human accountability and the social meaning of peer review and similar socio-technical activities.

</details>


### [11] [Invisible Load: Uncovering the Challenges of Neurodivergent Women in Software Engineering](https://arxiv.org/abs/2512.05350)
*Munazza Zaib,Wei Wang,Dulaji Hidellaarachchi,Isma Farah Siddiqui*

Main category: cs.SE

TL;DR: 本文提出了一种混合方法，将InclusiveMag包容性框架与GenderMag走查流程相结合，专门针对软件工程中的神经多样性女性，通过文献综述、角色创建和应用工作坊三个阶段来识别和解决她们面临的独特挑战。


<details>
  <summary>Details</summary>
Motivation: 神经多样性女性在软件工程领域面临着性别偏见和神经差异交叉的独特挑战，但目前SE研究尚未系统性地研究这一群体。误诊、掩饰行为和男性主导的工作文化加剧了她们的障碍，导致压力、倦怠和离职率上升。

Method: 提出混合方法学方法，整合InclusiveMag包容性框架与GenderMag走查流程，专门针对SE中的神经多样性女性。设计分为三个阶段：文献综述范围界定、角色和分析流程推导、协作工作坊应用方法。

Result: 通过有针对性的文献综述，将神经多样性女性在SE中面临的挑战综合为认知、社交、组织、结构和职业发展五大类，并识别出误诊/延迟诊断和掩饰行为如何加剧排斥问题。

Conclusion: 这些发现为后续阶段开发和应用包容性分析方法奠定了基础，旨在支持可操作的变革，改善神经多样性女性在软件工程领域的工作环境和职业发展。

Abstract: Neurodivergent women in Software Engineering (SE) encounter distinctive challenges at the intersection of gender bias and neurological differences. To the best of our knowledge, no prior work in SE research has systematically examined this group, despite increasing recognition of neurodiversity in the workplace. Underdiagnosis, masking, and male-centric workplace cultures continue to exacerbate barriers that contribute to stress, burnout, and attrition. In response, we propose a hybrid methodological approach that integrates InclusiveMag's inclusivity framework with the GenderMag walkthrough process, tailored to the context of neurodivergent women in SE. The overarching design unfolds across three stages, scoping through literature review, deriving personas and analytic processes, and applying the method in collaborative workshops. We present a targeted literature review that synthesize challenges into cognitive, social, organizational, structural and career progression challenges neurodivergent women face in SE, including how under/late diagnosis and masking intensify exclusion. These findings lay the groundwork for subsequent stages that will develop and apply inclusive analytic methods to support actionable change.

</details>


### [12] [Legacy Modernization with AI -- Mainframe modernization](https://arxiv.org/abs/2512.05375)
*Sunil Khemka,Arunava Majumdar*

Main category: cs.SE

TL;DR: AI辅助的遗留系统现代化将传统大型机系统转变为灵活、可扩展的智能架构，通过自动化代码重构、智能数据迁移和预测性维护等技术，帮助企业向微服务、容器化和混合云平台迁移。


<details>
  <summary>Details</summary>
Motivation: 传统大型机系统虽然可靠，但面临维护成本高、技能短缺、与云系统集成困难等问题，需要通过现代化改造来提升系统的灵活性、可扩展性和智能化水平。

Method: 采用AI驱动的现代化策略，包括：1）自动化代码重构；2）智能工具进行数据迁移；3）预测性维护；4）机器学习模型分析遗留代码库；5）自动化测试和部署；6）AI生成洞察以优化工作负载和异常检测。

Result: 企业能够顺利迁移到微服务、容器化环境和混合云平台，保留核心业务逻辑的同时，实现更快的创新、减少停机时间、增强系统弹性，并提高运营效率。

Conclusion: AI在大型机现代化中的应用是数字化转型和企业可持续增长的关键催化剂，不仅解决传统系统的问题，还为企业带来创新能力和竞争优势。

Abstract: Artificial Intelligence-assisted legacy modernization is essential in changing the stalwart mainframe systems of the past into flexible, scalable, and smart architecture. While mainframes are generally dependable, they can be difficult to maintain due to their high maintenance costs, the shortage of skills, and the problems in integrating them with cloud-based systems. By adopting AI-driven modernization strategies such as automated code refactoring, migration of data using smart tools, and predictive maintenance, companies can easily move to microservices, containerized environments, and hybrid cloud platforms. Machine learning models have the capability to go through legacy codebases, figure out efficiency opportunities, and carry out automated testing and deployment. Besides that, AI improves the organization's operational efficiency by generating the insights that can be used to level the workload and detect the anomalies. The coupling of the two is not only about saving the core business logic but also about enabling quicker innovation, less downtime, and enhanced system resilience. Therefore, the use of AI in mainframe modernization is a catalyst for digital transformation and enterprise growth that is sustainable over time.

</details>


### [13] [Fuzzing the brain: Automated stress testing for the safety of ML-driven neurostimulation](https://arxiv.org/abs/2512.05383)
*Mara Downing,Matthew Peng,Jacob Granley,Michael Beyeler,Tevfik Bultan*

Main category: cs.SE

TL;DR: 该研究提出了一种基于覆盖引导模糊测试的方法，用于系统检测机器学习驱动的神经刺激系统中的不安全刺激模式，通过扰动模型输入并追踪是否违反生物物理安全限制来评估神经假体设备的安全性。


<details>
  <summary>Details</summary>
Motivation: 机器学习模型越来越多地用于神经假体设备（如视觉假体）中生成电刺激模式。虽然这些模型提供了精确和个性化的控制，但当模型输出直接传递到神经组织时，也引入了新的安全风险。需要一种系统、定量的方法来检测和表征ML驱动的神经刺激系统中的不安全刺激模式。

Method: 研究将自动软件测试技术——覆盖引导模糊测试——适应到神经刺激领域。该方法将编码器视为黑盒，通过扰动模型输入并追踪产生的刺激是否违反电荷密度、瞬时电流或电极共激活等生物物理限制来进行压力测试。框架使用覆盖度量来引导探索，量化测试用例在可能输出空间和违规类型上的覆盖广度。

Result: 应用于视网膜和皮层的深度刺激编码器时，该方法系统地揭示了超出既定安全限制的多种刺激机制。两种违规输出覆盖度量识别出最高数量和多样性的不安全输出，使得能够对架构和训练策略进行可解释的比较。

Conclusion: 违规聚焦的模糊测试将安全评估重新定义为经验性、可重复的过程。通过将安全性从训练启发式转变为已部署模型的可测量属性，为下一代神经接口的证据基准测试、监管准备和伦理保证奠定了基础。

Abstract: Objective: Machine learning (ML) models are increasingly used to generate electrical stimulation patterns in neuroprosthetic devices such as visual prostheses. While these models promise precise and personalized control, they also introduce new safety risks when model outputs are delivered directly to neural tissue. We propose a systematic, quantitative approach to detect and characterize unsafe stimulation patterns in ML-driven neurostimulation systems. Approach: We adapt an automated software testing technique known as coverage-guided fuzzing to the domain of neural stimulation. Here, fuzzing performs stress testing by perturbing model inputs and tracking whether resulting stimulation violates biophysical limits on charge density, instantaneous current, or electrode co-activation. The framework treats encoders as black boxes and steers exploration with coverage metrics that quantify how broadly test cases span the space of possible outputs and violation types. Main results: Applied to deep stimulus encoders for the retina and cortex, the method systematically reveals diverse stimulation regimes that exceed established safety limits. Two violation-output coverage metrics identify the highest number and diversity of unsafe outputs, enabling interpretable comparisons across architectures and training strategies. Significance: Violation-focused fuzzing reframes safety assessment as an empirical, reproducible process. By transforming safety from a training heuristic into a measurable property of the deployed model, it establishes a foundation for evidence-based benchmarking, regulatory readiness, and ethical assurance in next-generation neural interfaces.

</details>


### [14] [Bita: A Conversational Assistant for Fairness Testing](https://arxiv.org/abs/2512.05428)
*Keeryn Johnson,Cleyton Magalhaes,Ronnie de Souza Santos*

Main category: cs.SE

TL;DR: Bita是一个基于对话的AI助手，帮助软件测试人员检测AI系统中的偏见，评估测试计划，并生成公平性导向的探索性测试章程。


<details>
  <summary>Details</summary>
Motivation: AI系统中的偏见可能导致不公平和歧视性结果，而现有的公平性测试工具通常难以使用，需要高级专业知识，对实际工作流程支持有限。

Method: Bita集成了大型语言模型与检索增强生成技术，将响应基于精心策划的公平性文献，帮助测试人员检测偏见来源、评估测试计划并生成公平性测试章程。

Result: 验证表明Bita能够支持真实世界AI系统的公平性测试任务，提供结构化、可复现的效用证据。

Conclusion: Bita是一个实用的工具，以可访问、系统化且直接适用于工业实践的方式实现了公平性测试的操作化。

Abstract: Bias in AI systems can lead to unfair and discriminatory outcomes, especially when left untested before deployment. Although fairness testing aims to identify and mitigate such bias, existing tools are often difficult to use, requiring advanced expertise and offering limited support for real-world workflows. To address this, we introduce Bita, a conversational assistant designed to help software testers detect potential sources of bias, evaluate test plans through a fairness lens, and generate fairness-oriented exploratory testing charters. Bita integrates a large language model with retrieval-augmented generation, grounding its responses in curated fairness literature. Our validation demonstrates how Bita supports fairness testing tasks on real-world AI systems, providing structured, reproducible evidence of its utility. In summary, our work contributes a practical tool that operationalizes fairness testing in a way that is accessible, systematic, and directly applicable to industrial practice.

</details>


### [15] [Everything is Context: Agentic File System Abstraction for Context Engineering](https://arxiv.org/abs/2512.05470)
*Xiwei Xu,Robert Mao,Quan Bai,Xuewu Gu,Yechao Li,Liming Zhu*

Main category: cs.SE

TL;DR: 论文提出了一种基于文件系统抽象的情境工程方法，用于管理生成式AI系统中的外部知识、记忆、工具和人类输入，以支持可信推理。


<details>
  <summary>Details</summary>
Motivation: 生成式AI正在重塑软件系统设计，当前的主要挑战已从模型微调转向情境工程——如何捕获、构建和管理外部知识以实现可信推理。现有的提示工程、RAG和工具集成方法分散且缺乏可追溯性。

Method: 提出基于Unix"一切皆文件"理念的文件系统抽象，提供统一挂载、元数据和访问控制的持久化基础设施。在AIGNE框架中实现可验证的情境工程管道，包括情境构造器、加载器和评估器。

Result: 实现了两个示例：具有记忆的智能体和基于MCP的GitHub助手，展示了架构在开发者和工业环境中的可操作性，支持可验证、可维护的生成式AI系统。

Conclusion: 该架构为负责任、以人为中心的AI协作建立了可重用基础，使人类能够作为策展人、验证者和共同推理者参与决策支持，推动行业就绪的生成式AI系统发展。

Abstract: Generative AI (GenAI) has reshaped software system design by introducing foundation models as pre-trained subsystems that redefine architectures and operations. The emerging challenge is no longer model fine-tuning but context engineering-how systems capture, structure, and govern external knowledge, memory, tools, and human input to enable trustworthy reasoning. Existing practices such as prompt engineering, retrieval-augmented generation (RAG), and tool integration remain fragmented, producing transient artefacts that limit traceability and accountability. This paper proposes a file-system abstraction for context engineering, inspired by the Unix notion that 'everything is a file'. The abstraction offers a persistent, governed infrastructure for managing heterogeneous context artefacts through uniform mounting, metadata, and access control. Implemented within the open-source AIGNE framework, the architecture realises a verifiable context-engineering pipeline, comprising the Context Constructor, Loader, and Evaluator, that assembles, delivers, and validates context under token constraints. As GenAI becomes an active collaborator in decision support, humans play a central role as curators, verifiers, and co-reasoners. The proposed architecture establishes a reusable foundation for accountable and human-centred AI co-work, demonstrated through two exemplars: an agent with memory and an MCP-based GitHub assistant. The implementation within the AIGNE framework demonstrates how the architecture can be operationalised in developer and industrial settings, supporting verifiable, maintainable, and industry-ready GenAI systems.

</details>


### [16] [A Hybrid Approach for EMF Code Generation:Code Templates Meet Large Language Models](https://arxiv.org/abs/2512.05498)
*Xiao He,Ru Chen,Zeqing Zhang,Yanling Wang,Qiuyan Dong*

Main category: cs.SE

TL;DR: iEcoreGen是一个混合方法，结合了EMF模板和LLM来生成代码，通过分解需求、生成初始代码，再用LLM补全和修复，在代码生成任务上表现优于纯LLM方法。


<details>
  <summary>Details</summary>
Motivation: 模板代码生成方法能保证正确性但灵活性不足，而LLM方法灵活但可能产生错误代码。需要结合两者的优势，实现既可靠又灵活的代码生成。

Method: iEcoreGen使用EMF的Ecore模型定义系统结构，分解需求得到操作规范，用模板生成初始Java代码，将规范序列化为文档字符串，然后调用LLM补全未实现的方法并进行修复。

Result: 在20个代码生成任务和5个LLM上的评估显示，iEcoreGen在pass@k指标上超越了纯LLM基线，在compilation@k指标上与基线持平。消融研究明确了各组件的作用。

Conclusion: LLM增强的模型驱动开发是实现更高效软件自动化的有前景路径，结合了模板方法的可靠性和LLM的灵活性。

Abstract: Template-based and LLM-based code generation are both key enablers of automated software development. The former provides correctness guarantees but are rigid for complex requirements, whereas LLMs offer high flexibility at the risk of producing faulty code.This paper proposes iEcoreGen, a hybrid approach that integrates Eclipse Modeling Framework (EMF) and LLMs. In EMF, an Ecore model defines a system structure and acts as a blueprint for code-generation.iEcoreGen decomposes requirements to derive operation specifications, uses EMF's template-based generator to produce initial Java code, and serializes specifications into docstrings. LLMs are then invoked to complete and fix unimplemented methods. We assessed iEcoreGen on twenty code-generation tasks across five LLMs. It surpasses LLM-only baselines on pass@k and performs on par with them on compilation@k. An ablation study clarified the contribution of each component of iEcoreGen. Overall, the findings indicate that LLM-enhanced model-driven development is a promising path toward more efficient software automation.

</details>


### [17] [Generative AI in Simulation-Based Test Environments for Large-Scale Cyber-Physical Systems: An Industrial Study](https://arxiv.org/abs/2512.05507)
*Masoud Sadrnezhaad,José Antonio Hernández López,Torvald Mårtensson,Daniel Varro*

Main category: cs.SE

TL;DR: 该研究通过跨公司研讨会收集了从业者对生成式AI在大型信息物理系统仿真测试中应用的观点，识别了当前挑战并提出了包含三个优先方向的研究议程。


<details>
  <summary>Details</summary>
Motivation: 大型信息物理系统的质量保证需要复杂的测试环境和多种模拟器，开发和维护这些仿真模型需要大量资源。虽然生成式AI已能生成软件系统的可执行测试用例，但在大型信息物理系统仿真测试中的应用仍未被充分探索。

Method: 研究基于与六家组织的跨公司研讨会，收集从业者对利用生成式AI进行仿真测试的实践经验，分析面临的挑战和机遇。

Result: 研究获得了基于经验的详细见解：从业者认识到生成式AI的巨大潜力，但也指出了未解决的挑战。研究提出了包含三个高优先级方向的研究议程：AI生成的场景和环境模型、CI/CD管道中的模拟器和AI、以及生成式AI在仿真中的可信度。

Conclusion: 该研究通过详细描述从业者面临的挑战和机遇，旨在指导未来学术界与工业界的合作，促进生成式AI在仿真测试中的负责任应用。

Abstract: Quality assurance for large-scale cyber-physical systems relies on sophisticated test activities using complex test environments investigated with the help of numerous types of simulators. As these systems grow, extensive resources are required to develop and maintain simulation models of hardware and software components, as well as physical environments. Meanwhile, recent advances in generative AI have led to tools that can produce executable test cases for software systems, offering potential benefits such as reducing manual efforts or increasing test coverage. However, the application of generative AI techniques to simulation-based testing of large-scale cyber-physical systems remains underexplored. To better understand this gap, this study captures practitioners' perspectives on leveraging generative AI, based on a cross-company workshop with six organizations. Our contribution is twofold: (1) detailed, experience-based insights into challenges faced by engineers, and (2) a research agenda comprising three high-priority directions: (a) AI-generated scenarios and environment models, (b) simulators and AI in CI/CD pipelines, and (c) trustworthiness in generative AI for simulation. While participants acknowledged substantial potential, they also highlighted unresolved challenges. By detailing these issues, the paper aims to guide future academia-industry collaboration towards the responsible adoption of generative AI in simulation-based testing.

</details>


### [18] [From Challenge to Change: Design Principles for AI Transformations](https://arxiv.org/abs/2512.05533)
*Theocharis Tavantzis,Stefano Lambiase,Daniel Russo,Robert Feldt*

Main category: cs.SE

TL;DR: 提出一个基于行为软件工程的人类中心框架，帮助软件工程组织在早期AI采用阶段应对社会技术复杂性，包含9个维度的具体指导原则和行动步骤。


<details>
  <summary>Details</summary>
Motivation: AI的快速发展正在重塑软件工程，虽然现有研究注意到行为和非技术因素，但大多仍强调技术问题，对团队如何适应和信任AI的洞察有限。需要一个人本主义的框架来支持SE组织在早期AI采用阶段。

Method: 采用混合方法：通过文献综述分析组织变革模型，通过访谈数据的主题分析构建框架，然后通过调查（N=105）和专家研讨会（N=4）收集初步实践者反馈。

Result: 开发了一个包含9个维度的框架：AI战略设计、AI战略评估、协作、沟通、治理与伦理、领导力、组织文化、组织动态、技能提升。调查显示技能提升（15.2%）和AI战略设计（15.1%）被认为最重要，组织目前优先考虑程序性元素，而人本主义防护措施发展不足。

Conclusion: 该工作通过识别关键行为维度和提供可操作指导，为导航早期AI采用的社会技术复杂性提供了实用路线图，并突出了SE中人本主义AI的未来研究方向。

Abstract: The rapid rise of Artificial Intelligence (AI) is reshaping Software Engineering (SE), creating new opportunities while introducing human-centered challenges. Although prior work notes behavioral and other non-technical factors in AI integration, most studies still emphasize technical concerns and offer limited insight into how teams adapt to and trust AI. This paper proposes a Behavioral Software Engineering (BSE)-informed, human-centric framework to support SE organizations during early AI adoption. Using a mixed-methods approach, we built and refined the framework through a literature review of organizational change models and thematic analysis of interview data, producing concrete, actionable steps. The framework comprises nine dimensions: AI Strategy Design, AI Strategy Evaluation, Collaboration, Communication, Governance and Ethics, Leadership, Organizational Culture, Organizational Dynamics, and Up-skilling, each supported by design principles and actions. To gather preliminary practitioner input, we conducted a survey (N=105) and two expert workshops (N=4). Survey results show that Up-skilling (15.2%) and AI Strategy Design (15.1%) received the highest $100-method allocations, underscoring their perceived importance in early AI initiatives. Findings indicate that organizations currently prioritize procedural elements such as strategy design, while human-centered guardrails remain less developed. Workshop feedback reinforced these patterns and emphasized the need to ground the framework in real-world practice. By identifying key behavioral dimensions and offering actionable guidance, this work provides a pragmatic roadmap for navigating the socio-technical complexity of early AI adoption and highlights future research directions for human-centric AI in SE.

</details>


### [19] [Automated Code Review Assignments: An Alternative Perspective of Code Ownership on GitHub](https://arxiv.org/abs/2512.05551)
*Jai Lal Lulla,Raula Gaikovina Kula,Christoph Treude*

Main category: cs.SE

TL;DR: 对GitHub CODEOWNERS功能的首个大规模实证研究，分析了超过84.4万个PR和200万次评审，发现CODEOWNERS能改善代码审查流程，重新分配评审责任，但尚未被充分利用。


<details>
  <summary>Details</summary>
Motivation: 随着软件供应链攻击等外部威胁增加，确保代码责任归属和质量保障的机制变得日益关键。GitHub在2017年引入的CODEOWNERS功能旨在通过自动指定特定文件的评审者来加强责任机制，但缺乏对其实际采用和实践情况的了解。

Method: 对超过844,000个拉取请求、190万条评论和200万次评审进行大规模实证研究，识别了10,287名代码所有者并追踪他们的评审活动。使用回归不连续设计(RDD)分析来评估CODEOWNERS采用对评审动态的影响。

Result: 代码所有者倾向于遵守CODEOWNERS文件中指定的规则，表现出与传统所有权指标相似的协作行为，但随着时间的推移有助于实现更顺畅、更快的PR工作流程。采用CODEOWNERS的仓库经历了评审动态的变化，所有权重新分配了评审责任，使其从核心开发者转移出去。

Conclusion: CODEOWNERS是一种有前景但尚未充分利用的机制，可用于改善软件治理和弹性。项目可以利用这种替代所有权方法来增强开源开发中的安全性、责任归属和工作流程效率。

Abstract: Code ownership is central to ensuring accountability and maintaining quality in large-scale software development. Yet, as external threats such as software supply chain attacks on project health and quality assurance increase, mechanisms for assigning and enforcing responsibility have become increasingly critical. In 2017, GitHub introduced the CODEOWNERS feature, which automatically designates reviewers for specific files to strengthen accountability and protect critical parts of the codebase. Despite its potential, little is known about how CODEOWNERS is actually adopted and practiced. We present the first large-scale empirical study of CODEOWNERS usage across over 844,000 pull requests with 1.9 million comments and over 2 million reviews. We identify 10,287 code owners to track their review activities. Results indicate that codeowners tend to adhere the rules specified in the CODEOWNERS file, exhibit similar collaborative behaviours to traditional metrics of ownership, but tend to contribute to a smoother and faster PR workflow over time. Finally, using regression discontinuity design (RDD) analysis, we find that repositories adopting CODEOWNERS experience shifts in review dynamics, as ownership redistributes review responsibilities away from core developers. Our results position CODEOWNERS as a promising yet underutilized mechanism for improving software governance and resilience. We discuss how projects can leverage this alternative ownership method as a perspective to enhance security, accountability, and workflow efficiency in open-source development.

</details>


### [20] [Executing Discrete/Continuous Declarative Process Specifications via Complex Event Processing](https://arxiv.org/abs/2512.05653)
*Stefan Schönig,Leo Poss,Fabrizio Maria Maggi*

Main category: cs.SE

TL;DR: 本文提出了一种基于复杂事件处理（CEP）的执行架构，用于实时执行和强制执行混合声明式模型，解决了传统BPM无法处理连续传感器数据的局限性。


<details>
  <summary>Details</summary>
Motivation: 传统业务流程管理（BPM）专注于离散事件，无法在信息物理环境中整合关键的连续传感器数据。现有的混合声明式规范（使用信号时序逻辑STL）仅限于监控和事后一致性检查，缺乏实时执行和强制执行能力。

Method: 提出了一种基于复杂事件处理（CEP）的三层执行架构，将STL启发的谓词集成到执行流程中，使系统能够基于连续传感器行为主动触发活动并强制执行流程边界。

Result: 该架构实现了混合声明式模型的实时执行和强制执行，填补了混合规范与操作控制之间的鸿沟。

Conclusion: 通过CEP-based执行架构，能够有效处理信息物理环境中的连续传感器数据，实现混合声明式模型的实时操作控制，为业务流程管理在物联网环境中的应用提供了新的解决方案。

Abstract: Traditional Business Process Management (BPM) focuses on discrete events and fails to incorporate critical continuous sensor data in cyber-physical environments. Hybrid declarative specifications, utilizing Signal Temporal Logic (STL), address this limitation by allowing constraints over both discrete events and real-valued signals. However, existing work has been limited to monitoring and post-hoc conformance checking. This paper introduces a novel Complex Event Processing (CEP)-based execution architecture that enables the real-time execution and enforcement of hybrid declarative models. Our three-layer approach integrates STL-inspired predicates into the execution flow, allowing the system to actively trigger activities and enforce process boundaries based on continuous sensor behavior. This approach bridges the gap between hybrid specification and operational control.

</details>


### [21] [Metronome: Differentiated Delay Scheduling for Serverless Functions](https://arxiv.org/abs/2512.05703)
*Zhuangbin Chen,Juzheng Zheng,Zibin Zheng*

Main category: cs.SE

TL;DR: Metronome是一个针对无服务器函数计算的差异化延迟调度框架，通过预测机制优化数据局部性和基础设施局部性，显著减少函数执行时间。


<details>
  <summary>Details</summary>
Motivation: 无服务器函数计算（FaaS）因其易管理性和弹性而兴起，但动态和事件驱动的特性使得调度优化具有挑战性。虽然数据局部性在传统集群计算中通过延迟调度被证明有效，但在无服务器平台中的应用尚未充分探索。

Method: 提出Metronome框架，采用差异化延迟调度策略。使用在线随机森林回归模型预测函数在不同节点上的执行时间，识别最优的局部性感知节点，做出明智的延迟决策同时防止SLA违规。

Result: 在OpenLambda上的实现显示，Metronome显著优于基线方法，将函数的平均执行时间减少了64.88%-95.83%，在高并发水平下仍保持性能优势，并确保SLA合规性。

Conclusion: Metronome通过预测性差异化延迟调度有效解决了无服务器函数计算中的局部性优化问题，为FaaS平台提供了高效的调度解决方案。

Abstract: Function-as-a-Service (FaaS) computing is an emerging cloud computing paradigm for its ease-of-management and elasticity. However, optimizing scheduling for serverless functions remains challenging due to their dynamic and event-driven nature. While data locality has been proven effective in traditional cluster computing systems through delay scheduling, its application in serverless platforms remains largely unexplored. In this paper, we systematically evaluate existing delay scheduling methods in serverless environments and identify three key observations: 1) delay scheduling benefits vary significantly based on function input characteristics; 2) serverless computing exhibits more complex locality patterns than cluster computing systems, encompassing both data locality and infrastructure locality; and 3) heterogeneous function execution times make rule-based delay thresholds ineffective. Based on these insights, we propose Metronome, a differentiated delay scheduling framework that employs predictive mechanisms to identify optimal locality-aware nodes for individual functions. Metronome leverages an online Random Forest Regression model to forecast function execution times across various nodes, enabling informed delay decisions while preventing SLA violations. Our implementation on OpenLambda shows that Metronome significantly outperforms baselines, achieving 64.88%-95.83% reduction in mean execution time for functions, while maintaining performance advantages under increased concurrency levels and ensuring SLA compliance.

</details>


### [22] [MicroRacer: Detecting Concurrency Bugs for Cloud Service Systems](https://arxiv.org/abs/2512.05716)
*Zhiling Deng,Juepeng Wang,Zhuangbin Chen*

Main category: cs.SE

TL;DR: MicroRacer：一种用于微服务架构的非侵入式并发bug检测框架，通过运行时动态插桩和三层验证过程有效检测并发问题


<details>
  <summary>Details</summary>
Motivation: 现代云应用采用微服务架构，端到端用户请求穿越多个不同服务和机器，存在复杂的交互，容易产生并发bug。现有检测方法存在侵入性强、无法处理微服务架构复杂性的问题。

Method: 通过运行时动态插桩广泛使用的库来收集详细追踪数据，不修改应用代码。利用这些数据分析服务系统中常见操作的happened-before关系和资源访问模式，识别可疑并发操作，采用三层验证过程测试和确认并发bug。

Result: 在开源微服务基准测试和复现的工业bug上的实验表明，MicroRacer能够有效且高效地准确检测和定位并发问题。

Conclusion: MicroRacer为非侵入式自动化并发bug检测提供了有效解决方案，能够应对微服务架构的复杂性，提高云服务系统的可靠性。

Abstract: Modern cloud applications delivering global services are often built on distributed systems with a microservice architecture. In such systems, end-to-end user requests traverse multiple different services and machines, exhibiting intricate interactions. Consequently, cloud service systems are vulnerable to concurrency bugs, which pose significant challenges to their reliability. Existing methods for concurrency bug detection often fall short due to their intrusive nature and inability to handle the architectural complexities of microservices. To address these limitations, we propose MicroRacer, a non-intrusive and automated framework for detecting concurrency bugs in such environments. By dynamically instrumenting widely-used libraries at runtime, MicroRacer collects detailed trace data without modifying the application code. Such data are utilized to analyze the happened-before relationship and resource access patterns of common operations within service systems. Based on this information, MicroRacer identifies suspicious concurrent operations and employs a three-stage validation process to test and confirm concurrency bugs. Experiments on open-source microservice benchmarks with replicated industrial bugs demonstrate MicroRacer's effectiveness and efficiency in accurately detecting and pinpointing concurrency issues.

</details>


### [23] [Bootstrapping Fuzzers for Compilers of Low-Resource Language Dialects Using Language Models](https://arxiv.org/abs/2512.05887)
*Sairam Vaidya,Marcel Böhme,Loris D'Antoni*

Main category: cs.SE

TL;DR: Germinator：一种基于语法和覆盖引导的模糊测试方法，用于可扩展编译器框架（如MLIR），能够自动生成方言无关且方言有效的测试种子，无需人工输入或训练数据。


<details>
  <summary>Details</summary>
Motivation: 现代可扩展编译器框架（如MLIR）支持快速创建领域特定语言方言，但这种灵活性使得正确性更难保证。现有测试生成方法要么需要为每个方言手动构建种子语料库，要么无法有效针对方言特定特性发现错误。

Method: 结合两个关键洞察：1）从方言规范中自动提取方言语法；2）使用预训练大语言模型基于这些语法自动生成具有代表性和多样性的种子输入。这些种子用于引导覆盖引导的模糊测试器。

Result: 在6个MLIR项目（涵盖91个方言）中评估，Germinator生成的种子相比基于语法的基线方法提高了10-120%的代码行覆盖率。发现了88个先前未知的错误（40个已确认），包括23个在没有先前自动化测试生成器的方言中发现的错误。

Conclusion: Germinator实现了方言无关且方言有效的测试生成，能够大规模有效地测试低资源方言，为可扩展编译器框架提供了自动化和可控的测试解决方案。

Abstract: Modern extensible compiler frameworks-such as MLIR-enable rapid creation of domain-specific language dialects. This flexibility, however, makes correctness harder to ensure as the same extensibility that accelerates development also complicates maintaining the testing infrastructure. Extensible languages require automated test generation that is both dialect-agnostic (works across dialects without manual adaptation) and dialect-effective (targets dialect-specific features to find bugs). Existing approaches typically sacrifice one of these goals by either requiring manually constructed seed corpora for each dialect, or by failing to be effective. We present a dialect-agnostic and dialect-effective grammar-based and coverage-guided fuzzing approach for extensible compilers that combines two key insights from existing work: (i) the grammars of dialects, which already encode the structural and type constraints, can often be extracted automatically from the dialect specification; and (ii) these grammars can be used in combination with pre-trained large language models to automatically generate representative and diverse seed inputs from the full dialect space without requiring any manual input or training data. These seeds can then be used to bootstrap coverage-guided fuzzers. We built this approach into a tool, Germinator. When evaluated on six MLIR projects spanning 91 dialects, Germinator generated seeds improve line coverage by 10-120% over grammar-based baselines. We compare against grammar-based baselines because they are the only class of existing automatic seed generators that can be applied uniformly across MLIR's heterogeneous dialect ecosystem. Germinator discovers 88 previously unknown bugs (40 confirmed), including 23 in dialects with no prior automated test generators, demonstrating effective and controllable testing of low-resource dialects at scale.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [24] [Semantic Faithfulness and Entropy Production Measures to Tame Your LLM Demons and Manage Hallucinations](https://arxiv.org/abs/2512.05156)
*Igor Halperin*

Main category: cs.AI

TL;DR: 该论文提出了两种基于信息论和热力学的无监督度量方法，用于评估大语言模型在给定任务中的忠实度。第一种是语义忠实度(SF)度量，通过KL散度衡量问题-上下文-答案三元组之间的主题转换一致性；第二种是语义熵产生(SEP)度量，基于热力学原理评估答案生成过程中的熵产生。两种度量可单独或联合用于LLM评估和幻觉控制。


<details>
  <summary>Details</summary>
Motivation: 评估大语言模型在给定任务中的忠实度是一个复杂挑战，需要有效的无监督度量方法。现有方法可能不足以准确量化LLM对输入上下文的忠实程度，特别是在处理复杂任务时。

Method: 将LLM视为二分信息引擎，隐藏层作为麦克斯韦妖控制上下文C通过提示Q转换为答案A的过程。将QCA三元组建模为共享主题上的概率分布，将C到Q和A的主题转换建模为转移矩阵Q和A。通过凸优化同时推断这两个矩阵，计算它们之间的KL散度作为语义忠实度(SF)度量，并将其映射到[0,1]区间。同时提出基于热力学的语义熵产生(SEP)度量。

Result: 提出的SF和SEP度量能够有效评估LLM的忠实度，高忠实度通常对应低熵产生。在LLM对公司SEC 10-K文件摘要任务上的演示表明该框架的有效性。

Conclusion: 该研究提出了两种基于信息论和热力学的无监督度量方法，为评估大语言模型的忠实度提供了新框架。SF和SEP度量可单独或联合使用，有助于LLM评估和幻觉控制，在复杂文档摘要等任务中具有应用价值。

Abstract: Evaluating faithfulness of Large Language Models (LLMs) to a given task is a complex challenge. We propose two new unsupervised metrics for faithfulness evaluation using insights from information theory and thermodynamics. Our approach treats an LLM as a bipartite information engine where hidden layers act as a Maxwell demon controlling transformations of context $C $ into answer $A$ via prompt $Q$. We model Question-Context-Answer (QCA) triplets as probability distributions over shared topics. Topic transformations from $C$ to $Q$ and $A$ are modeled as transition matrices ${\bf Q}$ and ${\bf A}$ encoding the query goal and actual result, respectively. Our semantic faithfulness (SF) metric quantifies faithfulness for any given QCA triplet by the Kullback-Leibler (KL) divergence between these matrices. Both matrices are inferred simultaneously via convex optimization of this KL divergence, and the final SF metric is obtained by mapping the minimal divergence onto the unit interval [0,1], where higher scores indicate greater faithfulness. Furthermore, we propose a thermodynamics-based semantic entropy production (SEP) metric in answer generation, and show that high faithfulness generally implies low entropy production. The SF and SEP metrics can be used jointly or separately for LLM evaluation and hallucination control. We demonstrate our framework on LLM summarization of corporate SEC 10-K filings.

</details>


### [25] [Bridging Traditional Machine Learning and Large Language Models: A Two-Part Course Design for Modern AI Education](https://arxiv.org/abs/2512.05167)
*Fang Li*

Main category: cs.AI

TL;DR: 提出一种创新的AI与数据科学教学方法，系统性地将传统机器学习技术与现代大语言模型相结合，通过两部分课程结构帮助学生全面理解AI发展并掌握实践技能。


<details>
  <summary>Details</summary>
Motivation: 为了帮助学生全面理解人工智能的发展历程，同时掌握传统机器学习和现代大语言模型技术，满足快速发展的AI行业需求，需要一种系统性的教学方法来桥接这两个领域。

Method: 设计了两部分互补的课程结构：第一部分涵盖基础机器学习概念，第二部分专注于当代大语言模型应用。课程包括详细的架构设计、实施策略、评估方法，并在两个七周学期的夏季课程中进行了实践。

Result: 这种整合方法增强了学生对AI领域的整体理解，更好地为他们应对快速发展的AI行业需求做好了准备。课程实施效果表明学生能够同时掌握传统和前沿技术。

Conclusion: 通过系统性地桥接传统机器学习与现代大语言模型的教学方法，能够有效提升学生对AI领域的全面理解，并为他们应对行业需求提供更好的准备。

Abstract: This paper presents an innovative pedagogical approach for teaching artificial intelligence and data science that systematically bridges traditional machine learning techniques with modern Large Language Models (LLMs). We describe a course structured in two sequential and complementary parts: foundational machine learning concepts and contemporary LLM applications. This design enables students to develop a comprehensive understanding of AI evolution while building practical skills with both established and cutting-edge technologies. We detail the course architecture, implementation strategies, assessment methods, and learning outcomes from our summer course delivery spanning two seven-week terms. Our findings demonstrate that this integrated approach enhances student comprehension of the AI landscape and better prepares them for industry demands in the rapidly evolving field of artificial intelligence.

</details>


### [26] [On the Computability of Artificial General Intelligence](https://arxiv.org/abs/2512.05212)
*Georgios Mappouras,Charalambos Rossides*

Main category: cs.AI

TL;DR: 该论文通过形式化证明指出：任何算法（包括AI模型）都无法展示其初始算法本身不存在的全新功能能力，因此AI无法实现真正的创造性突破，只能展示现有功能能力及其组合排列。


<details>
  <summary>Details</summary>
Motivation: 随着人工智能的快速发展，人们开始思考人类距离开发出达到人类智能水平的人工通用智能（AGI）还有多远。本文旨在探讨这个问题，并尝试定义任何机器可计算过程（即算法）的上限。

Method: 首先采用先前研究中关于AGI的定义（即能够在某个研究领域进行创造和创新，从而解锁该领域新的、先前未知的功能能力）。基于这一定义，作者通过形式化证明来界定计算的极限。

Result: 形式化证明表明：没有任何算法能够展示其初始算法本身不存在的全新功能能力。因此，任何算法（包括AI模型）都无法在科学、工程、艺术、体育等领域实现真正的创造性。AI只能展示现有功能能力及其组合和排列。

Conclusion: 这一证明对AI发展的未来以及人类智能的起源都具有重要启示。它表明AI无法实现真正的创造性突破，只能基于现有能力进行组合和优化，这对AGI的实现前景提出了根本性限制。

Abstract: In recent years we observed rapid and significant advancements in artificial intelligence (A.I.). So much so that many wonder how close humanity is to developing an A.I. model that can achieve human level of intelligence, also known as artificial general intelligence (A.G.I.). In this work we look at this question and we attempt to define the upper bounds, not just of A.I., but rather of any machine-computable process (a.k.a. an algorithm). To answer this question however, one must first precisely define A.G.I. We borrow prior work's definition of A.G.I. [1] that best describes the sentiment of the term, as used by the leading developers of A.I. That is, the ability to be creative and innovate in some field of study in a way that unlocks new and previously unknown functional capabilities in that field. Based on this definition we draw new bounds on the limits of computation. We formally prove that no algorithm can demonstrate new functional capabilities that were not already present in the initial algorithm itself. Therefore, no algorithm (and thus no A.I. model) can be truly creative in any field of study, whether that is science, engineering, art, sports, etc. In contrast, A.I. models can demonstrate existing functional capabilities, as well as combinations and permutations of existing functional capabilities. We conclude this work by discussing the implications of this proof both as it regards to the future of A.I. development, as well as to what it means for the origins of human intelligence.

</details>


### [27] [Resolving Zadehs Paradox Axiomatic Possibility Theory as a Foundation for Reliable Artificial Intelligence](https://arxiv.org/abs/2512.05257)
*Bychkov Oleksii,Bychkova Sophia,Lytvynchuk Khrystyna*

Main category: cs.AI

TL;DR: 本文论证可能性理论能从根本上解决Dempster-Shafer理论(DST)的悖论问题，通过可能性与必要性测度的二元体系建立逻辑一致的不确定性处理框架，相比概率和证据理论能更自然地处理矛盾数据。


<details>
  <summary>Details</summary>
Motivation: DST在处理不确定性时存在逻辑悖论和矛盾数据处理的困难，现有许多尝试修复Dempster规则的方法效果有限。本文旨在证明可能性理论不仅能作为替代方案，更能从根本上解决DST的悖论问题。

Method: 采用Bychkov文章中的公理化方法，基于可能性与必要性测度的二元体系从头构建逻辑一致且数学严谨的不确定性处理基础。通过比较分析概率、证据和可能性三种范式，并以经典医疗诊断困境为例进行演示。

Result: 可能性理论能够正确处理矛盾数据，避免DST的逻辑陷阱，使形式推理更接近自然智能的逻辑。它提供了比单纯修复Dempster规则更根本的解决方案。

Conclusion: 可能性理论不仅是不确定性处理的替代方案，更是解决DST悖论的根本性方案，通过其公理化方法为不确定性推理提供了逻辑一致且数学严谨的基础框架。

Abstract: This work advances and substantiates the thesis that the resolution of this crisis lies in the domain of possibility theory, specifically in the axiomatic approach developed in Bychkovs article. Unlike numerous attempts to fix Dempster rule, this approach builds from scratch a logically consistent and mathematically rigorous foundation for working with uncertainty, using the dualistic apparatus of possibility and necessity measures. The aim of this work is to demonstrate that possibility theory is not merely an alternative, but provides a fundamental resolution to DST paradoxes. A comparative analysis of three paradigms will be conducted probabilistic, evidential, and possibilistic. Using a classic medical diagnostic dilemma as an example, it will be shown how possibility theory allows for correct processing of contradictory data, avoiding the logical traps of DST and bringing formal reasoning closer to the logic of natural intelligence.

</details>


### [28] [AI & Human Co-Improvement for Safer Co-Superintelligence](https://arxiv.org/abs/2512.05356)
*Jason Weston,Jakob Foerster*

Main category: cs.AI

TL;DR: 论文主张从AI自我改进转向人机协同改进，通过人类研究者与AI的合作实现共同超级智能，以更安全、更快地推进AI研究。


<details>
  <summary>Details</summary>
Motivation: AI自我改进虽然令人兴奋但充满危险且难以实现，需要寻找更可行、更安全的目标来推进AI发展。

Method: 提出"协同改进"概念，专注于提升AI系统与人类研究者合作进行AI研究的能力，从构思到实验的全过程协作。

Result: 协同改进既能加速AI研究进展，又能通过人机共生赋予双方更安全的超级智能能力。

Conclusion: 将人类研究改进纳入循环，既能更快实现目标，又能确保更安全的发展路径，是人机协同智能发展的更优方向。

Abstract: Self-improvement is a goal currently exciting the field of AI, but is fraught with danger, and may take time to fully achieve. We advocate that a more achievable and better goal for humanity is to maximize co-improvement: collaboration between human researchers and AIs to achieve co-superintelligence. That is, specifically targeting improving AI systems' ability to work with human researchers to conduct AI research together, from ideation to experimentation, in order to both accelerate AI research and to generally endow both AIs and humans with safer superintelligence through their symbiosis. Focusing on including human research improvement in the loop will both get us there faster, and more safely.

</details>


### [29] [ChipMind: Retrieval-Augmented Reasoning for Long-Context Circuit Design Specifications](https://arxiv.org/abs/2512.05371)
*Changwen Xing,SamZaak Wong,Xinlai Wan,Yanfeng Lu,Mengli Zhang,Zebin Ma,Lei Qi,Zhengxiong Li,Nan Guan,Zhe Jiang,Xi Wang,Jun Yang*

Main category: cs.AI

TL;DR: ChipMind是一个基于知识图谱增强的推理框架，专门针对集成电路规格文档的长文本处理问题，通过构建领域特定知识图谱和自适应检索机制，显著提升了LLM在硬件设计中的推理能力。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在集成电路开发自动化方面具有巨大潜力，但实际部署受到有限上下文窗口的限制。现有上下文扩展方法难以对复杂、冗长的电路规格进行有效的语义建模和多跳推理。

Method: 1. 通过电路语义感知知识图谱构建方法将电路规格转换为领域特定知识图谱ChipKG；2. 采用ChipKG增强推理机制，结合信息论自适应检索动态追踪逻辑依赖关系，以及意图感知语义过滤去除无关噪声，平衡检索的完整性和精确性。

Result: 在工业级规格推理基准测试中，ChipMind显著优于现有最先进基线方法，平均提升34.59%（最高达72.73%）。

Conclusion: ChipMind框架填补了LLM辅助硬件设计学术研究与实际工业部署之间的关键空白，为集成电路开发自动化提供了有效的解决方案。

Abstract: While Large Language Models (LLMs) demonstrate immense potential for automating integrated circuit (IC) development, their practical deployment is fundamentally limited by restricted context windows. Existing context-extension methods struggle to achieve effective semantic modeling and thorough multi-hop reasoning over extensive, intricate circuit specifications. To address this, we introduce ChipMind, a novel knowledge graph-augmented reasoning framework specifically designed for lengthy IC specifications. ChipMind first transforms circuit specifications into a domain-specific knowledge graph ChipKG through the Circuit Semantic-Aware Knowledge Graph Construction methodology. It then leverages the ChipKG-Augmented Reasoning mechanism, combining information-theoretic adaptive retrieval to dynamically trace logical dependencies with intent-aware semantic filtering to prune irrelevant noise, effectively balancing retrieval completeness and precision. Evaluated on an industrial-scale specification reasoning benchmark, ChipMind significantly outperforms state-of-the-art baselines, achieving an average improvement of 34.59% (up to 72.73%). Our framework bridges a critical gap between academic research and practical industrial deployment of LLM-aided Hardware Design (LAD).

</details>


### [30] [BEAVER: An Efficient Deterministic LLM Verifier](https://arxiv.org/abs/2512.05439)
*Tarun Suresh,Nalin Wadhwa,Debangshu Banerjee,Gagandeep Singh*

Main category: cs.AI

TL;DR: BEAVER是首个为LLM约束满足提供确定性、可靠概率边界的实用框架，相比基线方法能获得6-8倍更紧的概率边界，识别出3-4倍更多高风险实例。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型从研究原型转向生产系统，从业者需要可靠方法来验证模型输出是否满足所需约束。基于采样的估计只能提供模型行为的直觉，无法提供可靠保证。

Method: BEAVER使用新颖的token trie和frontier数据结构，系统性地探索生成空间，对任何前缀封闭的语义约束维护可证明可靠的概率边界。形式化验证问题并证明方法的可靠性。

Result: 在正确性验证、隐私验证和安全代码生成任务上，BEAVER在相同计算预算下实现了6-8倍更紧的概率边界，识别出3-4倍更多高风险实例，能够提供松散边界或经验评估无法实现的精确特征描述和风险评估。

Conclusion: BEAVER是首个实用的确定性概率边界计算框架，为LLM约束满足提供了可靠保证，相比现有方法显著提升了验证精度和风险识别能力。

Abstract: As large language models (LLMs) transition from research prototypes to production systems, practitioners often need reliable methods to verify that model outputs satisfy required constraints. While sampling-based estimates provide an intuition of model behavior, they offer no sound guarantees. We present BEAVER, the first practical framework for computing deterministic, sound probability bounds on LLM constraint satisfaction. Given any prefix-closed semantic constraint, BEAVER systematically explores the generation space using novel token trie and frontier data structures, maintaining provably sound bounds at every iteration. We formalize the verification problem, prove soundness of our approach, and evaluate BEAVER on correctness verification, privacy verification and secure code generation tasks across multiple state of the art LLMs. BEAVER achieves 6 to 8 times tighter probability bounds and identifies 3 to 4 times more high risk instances compared to baseline methods under identical computational budgets, enabling precise characterization and risk assessment that loose bounds or empirical evaluation cannot provide.

</details>


### [31] [The Seeds of Scheming: Weakness of Will in the Building Blocks of Agentic Systems](https://arxiv.org/abs/2512.05449)
*Robert Yang*

Main category: cs.AI

TL;DR: 该论文提出将"意志薄弱"（akrasia）作为分析AI智能体不一致性的核心概念，开发了Akrasia基准测试来量化模型"知道正确答案但未能执行"的现象，并探讨了微观意志薄弱如何导致宏观多智能体系统不稳定。


<details>
  <summary>Details</summary>
Motivation: 大语言模型表现出一种特殊的不一致性：它们"知道"正确答案但未能据此行动。这种全局判断与局部冲动之间的张力在人类哲学中被称为"意志薄弱"（akrasia）。作者认为这一概念可作为分析AI智能体不一致性和目标漂移的基础框架。

Method: 提出了Akrasia基准测试的初步版本，包含四种结构化提示条件：基线（B）、同义词（S）、时间（T）和诱惑（X），用于测量模型局部响应与其先前承诺相矛盾的情况。该基准支持跨模型家族、解码策略和诱惑类型的"自我控制"量化比较。

Result: Akrasia基准测试能够量化评估不同模型在面对各种诱惑条件下的意志薄弱程度，为比较模型自我控制能力提供了实证工具。研究还指出微观层面的意志薄弱可能在多智能体系统中累积导致宏观层面的不稳定，可能被解释为"阴谋"或故意不对齐。

Conclusion: 通过将不一致性重新定义为意志薄弱，这项工作将智能体行为与经典的能动性理论联系起来，为哲学、心理学和新兴的智能体AI科学之间建立了实证桥梁，为分析AI系统的不一致性和目标漂移提供了新的概念框架。

Abstract: Large language models display a peculiar form of inconsistency: they "know" the correct answer but fail to act on it. In human philosophy, this tension between global judgment and local impulse is called akrasia, or weakness of will. We propose akrasia as a foundational concept for analyzing inconsistency and goal drift in agentic AI systems. To operationalize it, we introduce a preliminary version of the Akrasia Benchmark, currently a structured set of prompting conditions (Baseline [B], Synonym [S], Temporal [T], and Temptation [X]) that measures when a model's local response contradicts its own prior commitments. The benchmark enables quantitative comparison of "self-control" across model families, decoding strategies, and temptation types. Beyond single-model evaluation, we outline how micro-level akrasia may compound into macro-level instability in multi-agent systems that may be interpreted as "scheming" or deliberate misalignment. By reframing inconsistency as weakness of will, this work connects agentic behavior to classical theories of agency and provides an empirical bridge between philosophy, psychology, and the emerging science of agentic AI.

</details>


### [32] [MIND: Multi-rationale INtegrated Discriminative Reasoning Framework for Multi-modal Large Models](https://arxiv.org/abs/2512.05530)
*Chuang Yu,Jinmiao Zhao,Mingxuan Zhao,Yunpeng Liu,Xiujun Shu,Yuanhao Feng,Bo Wang,Xiangyu Yue*

Main category: cs.AI

TL;DR: MIND框架通过"理解-反思-纠正"的认知能力，将MLLMs从被动模仿推理转变为主动判别推理，在多个数据集上实现SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大语言模型在推理任务中存在多推理语义建模有限、逻辑鲁棒性不足、易受复杂场景误导等问题，需要更接近人类认知的推理能力。

Method: 提出MIND推理框架，包含：1）RAD范式自动扩展数据集生成多样化推理链；2）P2CL两阶段校正学习策略增强多推理正学习和主动逻辑判别；3）MCA优化策略解决多推理语义空间表示纠缠问题。

Result: 在科学、常识和数学等多个公开数据集上实现了最先进的性能，为MLLMs向更高认知智能水平发展提供了新视角。

Conclusion: MIND框架通过赋予MLLMs类似人类的"理解-反思-纠正"认知能力，实现了从被动模仿推理到主动判别推理的范式演进，显著提升了复杂场景下的推理性能。

Abstract: Recently, multimodal large language models (MLLMs) have been widely applied to reasoning tasks. However, they suffer from limited multi-rationale semantic modeling, insufficient logical robustness, and are susceptible to misleading interpretations in complex scenarios. Therefore, we propose a Multi-rationale INtegrated Discriminative (MIND) reasoning framework, which is designed to endow MLLMs with human-like cognitive abilities of "Understand -> Rethink -> Correct", and achieves a paradigm evolution from passive imitation-based reasoning to active discriminative reasoning. Specifically, we introduce a Rationale Augmentation and Discrimination (RAD) paradigm, which automatically and efficiently expands existing datasets by generating diverse rationales, providing a unified and extensible data foundation. Meanwhile, we design a Progressive Two-stage Correction Learning (P2CL) strategy. The first phase enhances multi-rationale positive learning, while the second phase enables active logic discrimination and correction. In addition, to mitigate representation entanglement in the multi-rationale semantic space, we propose a Multi-rationale Contrastive Alignment (MCA) optimization strategy, which achieves semantic aggregation of correct reasoning and boundary separation of incorrect reasoning. Extensive experiments demonstrate that the proposed MIND reasoning framework achieves state-of-the-art (SOTA) performance on multiple public datasets covering scientific, commonsense, and mathematical scenarios. It provides a new perspective for advancing MLLMs towards higher levels of cognitive intelligence. Our code is available at https://github.com/YuChuang1205/MIND

</details>


### [33] [Ontology Learning with LLMs: A Benchmark Study on Axiom Identification](https://arxiv.org/abs/2512.05594)
*Roos M. Bakker,Daan L. Di Scala,Maaike H. T. de Boer,Stephan A. Raaijmakers*

Main category: cs.AI

TL;DR: 本文介绍了OntoAxiom基准测试，用于评估大语言模型在识别本体公理方面的性能，发现Axiom-by-Axiom提示策略效果更好，但性能因公理类型和本体领域而异。


<details>
  <summary>Details</summary>
Motivation: 本体开发需要大量建模和领域专业知识，虽然自然语言处理技术和大语言模型的发展为自动化本体学习提供了可能，但识别定义类和属性间逻辑关系的基本公理仍是一个挑战。

Method: 创建了包含9个中型本体、17,118个三元组和2,771个公理的OntoAxiom基准测试，专注于子类、不相交、子属性、定义域和值域公理。评估了12个大语言模型，采用三种shot设置和两种提示策略：直接查询所有公理的方法与逐个公理查询的方法。

Result: 逐个公理提示策略比直接方法获得更高的F1分数，但性能在不同公理类型间差异显著。领域影响性能：FOAF本体子类公理得分为0.642，而音乐本体仅为0.218。大模型优于小模型，但小模型在资源受限环境中仍有价值。总体性能不足以完全自动化公理识别。

Conclusion: 虽然大语言模型目前不能完全自动化公理识别，但可以为本体工程师提供有价值的候选公理，支持本体的开发和精炼。逐个公理提示策略更有效，性能受公理类型和本体领域影响。

Abstract: Ontologies are an important tool for structuring domain knowledge, but their development is a complex task that requires significant modelling and domain expertise. Ontology learning, aimed at automating this process, has seen advancements in the past decade with the improvement of Natural Language Processing techniques, and especially with the recent growth of Large Language Models (LLMs). This paper investigates the challenge of identifying axioms: fundamental ontology components that define logical relations between classes and properties. In this work, we introduce an Ontology Axiom Benchmark OntoAxiom, and systematically test LLMs on that benchmark for axiom identification, evaluating different prompting strategies, ontologies, and axiom types. The benchmark consists of nine medium-sized ontologies with together 17.118 triples, and 2.771 axioms. We focus on subclass, disjoint, subproperty, domain, and range axioms. To evaluate LLM performance, we compare twelve LLMs with three shot settings and two prompting strategies: a Direct approach where we query all axioms at once, versus an Axiom-by-Axiom (AbA) approach, where each prompt queries for one axiom only. Our findings show that the AbA prompting leads to higher F1 scores than the direct approach. However, performance varies across axioms, suggesting that certain axioms are more challenging to identify. The domain also influences performance: the FOAF ontology achieves a score of 0.642 for the subclass axiom, while the music ontology reaches only 0.218. Larger LLMs outperform smaller ones, but smaller models may still be viable for resource-constrained settings. Although performance overall is not high enough to fully automate axiom identification, LLMs can provide valuable candidate axioms to support ontology engineers with the development and refinement of ontologies.

</details>


### [34] [Enhancing Local Search for MaxSAT with Deep Differentiation Clause Weighting](https://arxiv.org/abs/2512.05619)
*Menghua Jiang,Haokai Gao,Shuhao Chen,Yin Chen*

Main category: cs.AI

TL;DR: 提出DeepDist算法，针对部分最大可满足性问题(PMS)和加权部分最大可满足性问题(WPMS)，设计了新的子句权重更新方案，区分两种问题类型的处理策略，并引入新的初始化方法和优先满足单元子句与硬子句的decimation方法。


<details>
  <summary>Details</summary>
Motivation: 现有随机局部搜索(SLS)算法在处理PMS和WPMS问题时，通常采用统一的子句权重更新策略，未能充分考虑这两种问题类型之间的关键结构差异，导致算法性能受限。

Method: 1) 提出新颖的子句权重方案，首次根据PMS和WPMS实例的不同条件更新子句权重；2) 引入新的初始化方法，更好地适应两种实例类型的特性；3) 提出优先满足单元子句和硬子句的decimation方法；4) 基于这些方法开发了名为DeepDist的新SLS求解器。

Result: 在最近MaxSAT评估的anytime tracks基准测试中，DeepDist优于最先进的SLS求解器。将DeepDist与TT-Open-WBO-Inc结合的混合求解器超越了MaxSAT评估2024的获胜者SPB-MaxSAT-c-Band和SPB-MaxSAT-c-FPS。

Conclusion: 提出的方法有效区分了PMS和WPMS问题的处理策略，DeepDist算法在性能上超越了现有技术，证明了该方法的有效性。

Abstract: Partial Maximum Satisfiability (PMS) and Weighted Partial Maximum Satisfiability (WPMS) generalize Maximum Satisfiability (MaxSAT), with broad real-world applications. Recent advances in Stochastic Local Search (SLS) algorithms for solving (W)PMS have mainly focused on designing clause weighting schemes. However, existing methods often fail to adequately distinguish between PMS and WPMS, typically employing uniform update strategies for clause weights and overlooking critical structural differences between the two problem types. In this work, we present a novel clause weighting scheme that, for the first time, updates the clause weights of PMS and WPMS instances according to distinct conditions. This scheme also introduces a new initialization method, which better accommodates the unique characteristics of both instance types. Furthermore, we propose a decimation method that prioritizes satisfying unit and hard clauses, effectively complementing our proposed clause weighting scheme. Building on these methods, we develop a new SLS solver for (W)PMS named DeepDist. Experimental results on benchmarks from the anytime tracks of recent MaxSAT Evaluations show that DeepDist outperforms state-of-the-art SLS solvers. Notably, a hybrid solver combining DeepDist with TT-Open-WBO-Inc surpasses the performance of the MaxSAT Evaluation 2024 winners, SPB-MaxSAT-c-Band and SPB-MaxSAT-c-FPS, highlighting the effectiveness of our approach. The code is available at https://github.com/jmhmaxsat/DeepDist

</details>


### [35] [A Fast Anti-Jamming Cognitive Radar Deployment Algorithm Based on Reinforcement Learning](https://arxiv.org/abs/2512.05753)
*Wencheng Cai,Xuchao Gao,Congying Han,Mingqiang Li,Tiande Guo*

Main category: cs.AI

TL;DR: 提出FARDA框架，使用深度强化学习快速部署抗干扰雷达，相比进化算法速度提升约7000倍，覆盖效果相当。


<details>
  <summary>Details</summary>
Motivation: 现代战争中快速部署认知雷达对抗干扰是关键挑战，现有基于进化算法的方法耗时且易陷入局部最优。

Method: 将雷达部署问题建模为端到端任务，设计深度强化学习算法，开发集成神经模块感知热图信息和新奖励格式。

Result: FARDA实现与进化算法相当的覆盖效果，但部署速度提升约7000倍。消融实验验证了各组件必要性。

Conclusion: FARDA框架通过深度强化学习高效解决了雷达快速部署问题，显著提升了抗干扰雷达部署效率。

Abstract: The fast deployment of cognitive radar to counter jamming remains a critical challenge in modern warfare, where more efficient deployment leads to quicker detection of targets. Existing methods are primarily based on evolutionary algorithms, which are time-consuming and prone to falling into local optima. We tackle these drawbacks via the efficient inference of neural networks and propose a brand new framework: Fast Anti-Jamming Radar Deployment Algorithm (FARDA). We first model the radar deployment problem as an end-to-end task and design deep reinforcement learning algorithms to solve it, where we develop integrated neural modules to perceive heatmap information and a brand new reward format. Empirical results demonstrate that our method achieves coverage comparable to evolutionary algorithms while deploying radars approximately 7,000 times faster. Further ablation experiments confirm the necessity of each component of FARDA.

</details>


### [36] [Evolutionary System 2 Reasoning: An Empirical Proof](https://arxiv.org/abs/2512.05760)
*Zeyuan Ma,Wenqi Huang,Guo-Huan Song,Hongshu Guo,Sijie Ma,Zhiguang Cao,Yue-Jiao Gong*

Main category: cs.AI

TL;DR: 论文提出进化推理优化（ERO）框架，通过进化算法对LLM群体进行"适者生存"选择，以提升模型的一般推理能力而非特定技能。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型虽然在特定任务上表现出色，但在一般推理能力（系统2思维）方面仍有不足。研究旨在探索机器智能（如LLM）能否像人类一样进化获得推理能力。

Method: 提出ERO框架：1）初始化多个LLM作为种群；2）使用进化策略优化种群，最大化最佳个体的量化推理分数；3）通过"适者生存"原则搜索具有强推理能力的个体。

Result: 两个重要发现：1）最新LLM（如GPT-5）仍表现出有限的系统2推理能力；2）通过简单的ERO进化循环，相对较弱的模型（Qwen-7B）能够进化出强大的推理能力。

Conclusion: ERO框架证明通过进化方法可以有效提升LLM的一般推理能力，为机器智能获得类似人类的推理能力提供了可行路径。

Abstract: Machine intelligence marks the ultimate dream of making machines' intelligence comparable to human beings. While recent progress in Large Language Models (LLMs) show substantial specific skills for a wide array of downstream tasks, they more or less fall shorts in general intelligence. Following correlation between intelligence and system 2 reasoning (slow thinking), in this paper, we aim to answering a worthwhile research question: could machine intelligence such as LLMs be evolved to acquire reasoning ability (not specific skill) just like our human beings? To this end, we propose evolutionary reasoning optimization (ERO) framework which performs survival of the fittest over a population of LLMs to search for individual with strong reasoning ability. Given a reasoning task, ERO first initializes multiple LLMs as a population, after which an evolutionary strategy evolves the population to maximize quantified reasoning score of the best individual. Based on experiments on representative testsuites, we claim two surprising empirical discoveries: i) the latest LLMs such as GPT-5 still show limited system 2 reasoning ability; ii) with simple evolution-loop of ERO, a relatively weak model (Qwen-7B) could be enhanced to emerge powerful reasoning ability. Our project can be accessed at https://github.com/MetaEvo/ERO for reproduction needs.

</details>


### [37] [The Missing Layer of AGI: From Pattern Alchemy to Coordination Physics](https://arxiv.org/abs/2512.05765)
*Edward Y. Chang*

Main category: cs.AI

TL;DR: 论文认为LLM不是AGI的死胡同，而是缺少System-2协调层来选择和约束模式。提出UCCT理论和MACI架构来实现推理能力。


<details>
  <summary>Details</summary>
Motivation: 针对"LLM只是模式匹配器，无法实现真正推理和规划"的批评，作者认为问题不在于LLM本身，而在于缺少一个能够协调、选择和约束这些模式的System-2层。需要将LLM从无根据的生成转变为目标导向的推理。

Method: 提出UCCT（语义锚定理论），将推理建模为受有效支持度、表征失配和自适应锚定预算控制的相变过程。基于此开发MACI协调栈，包含诱饵（行为调制辩论）、过滤（苏格拉底式判断）和持久性（事务性记忆）三个组件。

Result: 通过理论框架将常见的反对意见重新定义为可测试的协调失败，证明LLM可以作为AGI的基础组件，而不是需要绕开的障碍。

Conclusion: 通往AGI的道路需要经过LLM而不是绕过它们。LLM提供了必要的System-1模式库，而System-2协调层能够将这些模式转化为真正的推理能力，反驳了LLM是AGI死胡同的观点。

Abstract: Influential critiques argue that Large Language Models (LLMs) are a dead end for AGI: "mere pattern matchers" structurally incapable of reasoning or planning. We argue this conclusion misidentifies the bottleneck: it confuses the ocean with the net. Pattern repositories are the necessary System-1 substrate; the missing component is a System-2 coordination layer that selects, constrains, and binds these patterns. We formalize this layer via UCCT, a theory of semantic anchoring that models reasoning as a phase transition governed by effective support (rho_d), representational mismatch (d_r), and an adaptive anchoring budget (gamma log k). Under this lens, ungrounded generation is simply an unbaited retrieval of the substrate's maximum likelihood prior, while "reasoning" emerges when anchors shift the posterior toward goal-directed constraints. We translate UCCT into architecture with MACI, a coordination stack that implements baiting (behavior-modulated debate), filtering (Socratic judging), and persistence (transactional memory). By reframing common objections as testable coordination failures, we argue that the path to AGI runs through LLMs, not around them.

</details>


### [38] [Multimodal Oncology Agent for IDH1 Mutation Prediction in Low-Grade Glioma](https://arxiv.org/abs/2512.05824)
*Hafsa Akebli,Adam Shephard,Vincenzo Della Mea,Nasir Rajpoot*

Main category: cs.AI

TL;DR: 本文提出了一种多模态肿瘤智能体（MOA），结合基于TITAN基础模型的IDH1突变预测组织学工具，并通过PubMed、Google搜索和OncoKB对临床和基因组数据进行推理，在低级别胶质瘤中实现了高精度的IDH1突变预测。


<details>
  <summary>Details</summary>
Motivation: 低级别胶质瘤中IDH1突变定义了具有特定预后和治疗意义的临床亚群，准确预测这些突变对临床决策至关重要。现有方法可能无法充分利用多模态信息，因此需要开发能够整合组织学、临床和基因组数据并利用外部生物医学知识的智能系统。

Method: 开发了多模态肿瘤智能体（MOA），包含：1）基于TITAN基础模型的IDH1突变预测组织学工具；2）通过PubMed、Google搜索和OncoKB对结构化临床和基因组输入进行推理的能力。在TCGA-LGG队列的488名患者上进行评估，与临床和组织学基线进行比较。

Result: MOA（无组织学工具）的F1分数为0.826，优于临床基线的0.798。当与组织学特征融合后，MOA达到最高性能，F1分数为0.912，超过组织学基线的0.894和融合组织学-临床基线的0.897。这表明MOA通过外部生物医学来源捕获了互补的突变相关信息。

Conclusion: 所提出的多模态肿瘤智能体能够整合组织学特征并利用外部生物医学知识进行推理，显著提高了低级别胶质瘤中IDH1突变的预测准确性，为精准医疗提供了有效的决策支持工具。

Abstract: Low-grade gliomas frequently present IDH1 mutations that define clinically distinct subgroups with specific prognostic and therapeutic implications. This work introduces a Multimodal Oncology Agent (MOA) integrating a histology tool based on the TITAN foundation model for IDH1 mutation prediction in low-grade glioma, combined with reasoning over structured clinical and genomic inputs through PubMed, Google Search, and OncoKB. MOA reports were quantitatively evaluated on 488 patients from the TCGA-LGG cohort against clinical and histology baselines. MOA without the histology tool outperformed the clinical baseline, achieving an F1-score of 0.826 compared to 0.798. When fused with histology features, MOA reached the highest performance with an F1-score of 0.912, exceeding both the histology baseline at 0.894 and the fused histology-clinical baseline at 0.897. These results demonstrate that the proposed agent captures complementary mutation-relevant information enriched through external biomedical sources, enabling accurate IDH1 mutation prediction.

</details>


### [39] [To Err Is Human: Systematic Quantification of Errors in Published AI Papers via LLM Analysis](https://arxiv.org/abs/2512.05925)
*Federico Bianchi,Yongchan Kwon,Zachary Izzo,Linjun Zhang,James Zou*

Main category: cs.AI

TL;DR: 使用GPT-5开发的论文正确性检查器发现，顶级AI会议和期刊发表的论文中存在显著数量的客观错误，且错误数量随时间增加。人类专家验证显示AI检查器准确率达83.2%，并能对75.8%的错误提出正确修正。


<details>
  <summary>Details</summary>
Motivation: 同行评审出版物是构建新研究和知识的基础，但文献中的错误会持续传播，导致后续研究混乱并影响可重复性。研究加速和同行评审系统压力使错误更难被发现和避免，因此需要系统性地识别已发表论文中的错误。

Method: 开发基于GPT-5的论文正确性检查器，系统性地识别顶级AI会议和期刊已发表论文中的客观错误（如公式、推导、计算、图表错误），排除主观因素。人类专家对AI识别的316个潜在错误进行评审验证。

Result: 发现已发表论文包含不可忽视的客观错误数量，且平均错误数随时间增加：NeurIPS从2021年的3.8个增至2025年的5.9个（增长55.3%）；ICLR从2018年的4.1个增至2025年的5.2个；TMLR从2022/23年的5.0个增至2025年的5.5个。人类专家确认263个为实际错误，精确度83.2%。AI检查器能为75.8%的错误提出正确修正。

Conclusion: 前沿大语言模型在检测和纠正已发表论文中的客观错误方面具有重要潜力，有助于建立更坚实的知识基础。虽然大多数错误相对较小，但纠正它们可以减少文献混乱并增强可重复性。

Abstract: How many mistakes do published AI papers contain? Peer-reviewed publications form the foundation upon which new research and knowledge are built. Errors that persist in the literature can propagate unnoticed, creating confusion in follow-up studies and complicating reproducibility. The accelerating pace of research and the increasing demands on the peer-review system make such mistakes harder to detect and avoid. To address this, we developed a Paper Correctness Checker based on GPT-5 to systematically identify mistakes in papers previously published at top AI conferences and journals. Our analysis focuses on objective mistakes-e.g., errors in formulas, derivations, calculations, figures, and tables-that have a clearly verifiable ground truth. We intentionally exclude subjective considerations such as novelty, importance, or writing quality. We find that published papers contain a non-negligible number of objective mistakes and that the average number of mistakes per paper has increased over time-from 3.8 in NeurIPS 2021 to 5.9 in NeurIPS 2025 (55.3% increase); from 4.1 in ICLR 2018 to 5.2 in ICLR 2025; and from 5.0 in TMLR 2022/23 to 5.5 in TMLR 2025. Human experts reviewed 316 potential mistakes identified by the AI Checker and confirmed that 263 were actual mistakes, corresponding to a precision of 83.2%. While most identified issues are relatively minor, correcting them would reduce confusion in the literature and strengthen reproducibility. The AI Checker also surfaced potentially more substantive mistakes that could affect the interpretation of results. Moreover, we show that the AI Checker can propose correct fixes for 75.8% of the identified mistakes. Overall, this study highlights the potential of frontier LLMs to detect and correct objective mistakes in published papers, helping to establish a firmer foundation of knowledge.

</details>


### [40] [TRACE: A Framework for Analyzing and Enhancing Stepwise Reasoning in Vision-Language Models](https://arxiv.org/abs/2512.05943)
*Shima Imani,Seungwhan Moon,Lambert Mathias,Lu Zhang,Babak Damavandi*

Main category: cs.AI

TL;DR: TRACE是一个透明推理与一致性评估框架，通过辅助推理集分解复杂问题，评估中间步骤的一致性，诊断推理轨迹而非仅关注最终答案。


<details>
  <summary>Details</summary>
Motivation: 大型视觉语言模型在可靠数学和科学推理方面仍面临挑战，标准最终答案评估往往掩盖推理错误，导致无声故障持续存在。

Method: TRACE框架利用辅助推理集（紧凑的子问题-答案对）分解复杂问题，通过基于一致性的指标评估中间步骤，暴露标准评估忽略的故障。

Result: 实验表明，ARS上的一致性程度与最终答案正确性相关，有助于精确定位推理步骤中的故障点，为模型改进提供可操作的信号。

Conclusion: TRACE定义了区分可靠与不可靠推理路径的置信区域，支持有效的过滤、调试和模型精炼，为解决大型视觉语言模型的推理可靠性问题提供了新方法。

Abstract: Reliable mathematical and scientific reasoning remains an open challenge for large vision-language models. Standard final-answer evaluation often masks reasoning errors, allowing silent failures to persist. To address this gap, we introduce TRACE, a framework for Transparent Reasoning And Consistency Evaluation that diagnoses reasoning trajectories rather than only end results. At its core, TRACE leverages Auxiliary Reasoning Sets, compact sub question answer pairs that decompose complex problems, evaluate intermediate steps through consistency-based metrics, and expose failures overlooked by standard evaluation. Our experiments show that consistency across ARS correlates with final-answer correctness and helps pinpoint the reasoning steps where failures arise, offering actionable signals for model improvement. Furthermore, TRACE defines confidence regions that distinguish reliable from unreliable reasoning paths, supporting effective filtering, debugging, and model refinement.

</details>


### [41] [Variational Quantum Rainbow Deep Q-Network for Optimizing Resource Allocation Problem](https://arxiv.org/abs/2512.05946)
*Truong Thanh Hung Nguyen,Truong Thinh Nguyen,Hung Cao*

Main category: cs.AI

TL;DR: VQR-DQN将变分量子电路与Rainbow DQN结合，用于人力资源分配问题，相比经典方法减少26.8%的makespan


<details>
  <summary>Details</summary>
Motivation: 资源分配问题具有组合复杂性，传统深度强化学习方法受限于经典函数逼近器的表示能力，需要量子计算的优势来提升性能

Method: 提出Variational Quantum Rainbow DQN (VQR-DQN)，将环拓扑变分量子电路与Rainbow DQN集成，利用量子叠加和纠缠特性；将人力资源分配问题建模为基于官员能力、事件调度和转移时间的马尔可夫决策过程

Result: 在四个人力资源分配基准测试中，VQR-DQN相比随机基线减少26.8%的标准化makespan，相比Double DQN和经典Rainbow DQN提升4.9-13.4%的性能

Conclusion: 电路表达能力、纠缠和策略质量之间的理论联系验证了量子增强深度强化学习在大规模资源分配中的潜力

Abstract: Resource allocation remains NP-hard due to combinatorial complexity. While deep reinforcement learning (DRL) methods, such as the Rainbow Deep Q-Network (DQN), improve scalability through prioritized replay and distributional heads, classical function approximators limit their representational power. We introduce Variational Quantum Rainbow DQN (VQR-DQN), which integrates ring-topology variational quantum circuits with Rainbow DQN to leverage quantum superposition and entanglement. We frame the human resource allocation problem (HRAP) as a Markov decision process (MDP) with combinatorial action spaces based on officer capabilities, event schedules, and transition times. On four HRAP benchmarks, VQR-DQN achieves 26.8% normalized makespan reduction versus random baselines and outperforms Double DQN and classical Rainbow DQN by 4.9-13.4%. These gains align with theoretical connections between circuit expressibility, entanglement, and policy quality, demonstrating the potential of quantum-enhanced DRL for large-scale resource allocation. Our implementation is available at: https://github.com/Analytics-Everywhere-Lab/qtrl/.

</details>


### [42] [SymPyBench: A Dynamic Benchmark for Scientific Reasoning with Executable Python Code](https://arxiv.org/abs/2512.05954)
*Shima Imani,Seungwhan Moon,Adel Ahmadyan,Lu Zhang,Kirmani Ahmed,Babak Damavandi*

Main category: cs.AI

TL;DR: SymPyBench是一个包含15,045个大学物理问题的大规模合成基准测试，支持无限参数配置，提供结构化推理步骤和可执行Python代码，包含三种问题类型和三种新颖评估指标。


<details>
  <summary>Details</summary>
Motivation: 创建大规模、参数化的物理问题基准测试，以评估语言模型在科学推理方面的能力，特别是测试模型在不同问题变体中的一致性和可靠性。

Method: 构建包含15,045个大学物理问题的合成数据集（90/10训练/测试分割），每个问题完全参数化，支持无限输入配置，包含结构化推理步骤和可执行Python代码。数据集包含三种问题类型：MC-Symbolic（符号多项选择）、MC-Numerical（数值多项选择）和自由形式（开放回答）。引入三种新颖评估指标：一致性分数、失败率和混淆率。

Result: 使用最先进的指令调优语言模型进行实验，揭示了模型在科学推理方面的优势和局限性。SymPyBench为开发更稳健和可解释的推理系统奠定了基础。

Conclusion: SymPyBench是一个创新的基准测试，通过其动态、代码驱动的特性，能够全面评估语言模型的科学推理能力，特别是通过新颖的评估指标量化模型在不同问题变体中的表现变化和不确定性。

Abstract: We introduce, a large-scale synthetic benchmark of 15,045 university-level physics problems (90/10% train/test split). Each problem is fully parameterized, supporting an effectively infinite range of input configurations, and is accompanied by structured, step-by-step reasoning and executable Python code that produces the ground-truth solution for any parameter set. The benchmark contains three question types: MC-Symbolic (multiple-choice with symbolic options), MC-Numerical (multiple-choice with numerical options), and free-form (open-ended responses). These diverse formats test complementary reasoning skills. By leveraging the dynamic, code-driven nature of the benchmark, we introduce three novel evaluation metrics in addition to standard accuracy: Consistency Score, Failure Rate, and Confusion Rate, that quantify variability and uncertainty across problem variants. Experiments with state-of-the-art instruction-tuned language models reveal both strengths and limitations in scientific reasoning, positioning SymPyBench as a foundation for developing more robust and interpretable reasoning systems

</details>
