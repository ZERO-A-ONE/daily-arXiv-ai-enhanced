<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 12]
- [cs.CR](#cs.CR) [Total: 14]
- [cs.AI](#cs.AI) [Total: 31]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [Empathy Guidelines for Improving Practitioner Well-being & Software Engineering Practices](https://arxiv.org/abs/2508.03846)
*Hashini Gunatilake,John Grundy,Rashina Hoda,Ingo Mueller*

Main category: cs.SE

TL;DR: 本文提出了17条可操作的共情指南，帮助软件工程团队更好地实践共情，并提供了一个可视化框架以支持实施。


<details>
  <summary>Details</summary>
Motivation: 共情在软件工程中常被忽视，但其对团队合作、沟通和决策至关重要。本文旨在提供实用指南，帮助团队将共情原则转化为可持续行动。

Method: 基于先前研究，本文设计了17条共情指南，并通过实际案例、挑战及解决策略验证其可行性。同时，开发了一个可视化优先级框架。

Result: 研究提供了灵活的共情实践建议，并通过框架帮助团队根据重要性、易实施性和采纳意愿分类指南。

Conclusion: 本文的指南和框架为软件工程团队提供了实用的共情实践工具，有助于从理论到可持续行动的转变。

Abstract: Empathy is a powerful yet often overlooked element in software engineering
(SE), supporting better teamwork, smoother communication, and effective
decision-making. In our previous study, we identified a range of practitioner
strategies for fostering empathy in SE contexts. Building on these insights,
this paper introduces 17 actionable empathy guidelines designed to support
practitioners, teams, and organisations. We also explore how these guidelines
can be implemented in practice by examining real-world applications,
challenges, and strategies to overcome them shared by software practitioners.
To support adoption, we present a visual prioritisation framework that
categorises the guidelines based on perceived importance, ease of
implementation, and willingness to adopt. The findings offer practical and
flexible suggestions for integrating empathy into everyday SE work, helping
teams move from principles to sustainable action.

</details>


### [2] [Evaluating Software Supply Chain Security in Research Software](https://arxiv.org/abs/2508.03856)
*Richard Hegewald,Rebecca Beyer*

Main category: cs.SE

TL;DR: 研究软件安全性普遍较差，平均得分3.5/10，需改进签名发布和分支保护等实践。


<details>
  <summary>Details</summary>
Motivation: 研究软件的安全性对科学结果的完整性和可重复性至关重要，但该领域仍未被充分探索。

Method: 使用OpenSSF Scorecard分析了3,248个高质量、经过同行评审的研究软件仓库。

Result: 发现研究软件安全性普遍较弱，重要实践如签名发布和分支保护很少实施。

Conclusion: 提出了可操作的低成本建议，帮助研究团队提升软件安全性，减少对科学完整性的潜在威胁。

Abstract: The security of research software is essential for ensuring the integrity and
reproducibility of scientific results. However, research software security is
still largely unexplored. Due to its dependence on open source components and
distributed development practices, research software is particularly vulnerable
to supply chain attacks. This study analyses 3,248 high-quality, largely
peer-reviewed research software repositories using the OpenSSF Scorecard. We
find a generally weak security posture with an average score of 3.5/10.
Important practices, such as signed releases and branch protection, are rarely
implemented. Finally, we present actionable, low-effort recommendations that
can help research teams improve software security and mitigate potential
threats to scientific integrity.

</details>


### [3] [From App Features to Explanation Needs: Analyzing Correlations and Predictive Potential](https://arxiv.org/abs/2508.03881)
*Martin Obaidi,Kushtrim Qengaj,Jakob Droste,Hannah Deters,Marc Herrmann,Jil Klünder,Elisa Schmid,Kurt Schneider*

Main category: cs.SE

TL;DR: 研究发现，用户对软件系统的解释需求难以通过应用属性（如版本、评分等）准确预测，需结合直接用户反馈。


<details>
  <summary>Details</summary>
Motivation: 探讨是否可以通过应用属性预测用户对解释的需求，以支持开发和大规模需求挖掘。

Method: 分析了4,495条应用评论的黄金标准数据集，进行相关性分析和线性回归建模，并在495条手动标注的评论上验证。

Result: 应用属性与解释需求的关联较弱，仅特定特征（如版本、评论数量、评分）有中等相关性，预测模型效果有限。

Conclusion: 解释需求高度依赖上下文，仅靠元数据无法精确推断，需结合用户反馈设计用户中心的软件系统。

Abstract: In today's digitized world, software systems must support users in
understanding both how to interact with a system and why certain behaviors
occur. This study investigates whether explanation needs, classified from user
reviews, can be predicted based on app properties, enabling early consideration
during development and large-scale requirements mining. We analyzed a gold
standard dataset of 4,495 app reviews enriched with metadata (e.g., app
version, ratings, age restriction, in-app purchases). Correlation analyses
identified mostly weak associations between app properties and explanation
needs, with moderate correlations only for specific features such as app
version, number of reviews, and star ratings. Linear regression models showed
limited predictive power, with no reliable forecasts across configurations.
Validation on a manually labeled dataset of 495 reviews confirmed these
findings. Categories such as Security & Privacy and System Behavior showed
slightly higher predictive potential, while Interaction and User Interface
remained most difficult to predict. Overall, our results highlight that
explanation needs are highly context-dependent and cannot be precisely inferred
from app metadata alone. Developers and requirements engineers should therefore
supplement metadata analysis with direct user feedback to effectively design
explainable and user-centered software systems.

</details>


### [4] [A Human Centric Requirements Engineering Framework for Assessing Github Copilot Output](https://arxiv.org/abs/2508.03922)
*Soroush Heydari*

Main category: cs.SE

TL;DR: 论文研究了GitHub Copilot如何通过聊天界面与用户互动，评估其适应不同用户专业水平的能力，并提出以人为中心的需求框架。


<details>
  <summary>Details</summary>
Motivation: 现有评估框架多关注技术层面，忽视了影响AI助手成功整合到软件开发工作流中的人为因素。

Method: 通过分析GitHub Copilot的聊天界面互动，测量其适应不同用户专业水平的能力，并评估其在协作编程中的效果。

Result: 建立了一个以人为中心的需求框架，并讨论了测试结果及其对未来自动化编程中人类需求分析的启示。

Conclusion: 研究强调了在AI编程助手中考虑人为因素的重要性，并提出了未来研究方向。

Abstract: The rapid adoption of Artificial Intelligence(AI) programming assistants such
as GitHub Copilot introduces new challenges in how these software tools address
human needs. Many existing evaluation frameworks address technical aspects such
as code correctness and efficiency, but often overlook crucial human factors
that affect the successful integration of AI assistants in software development
workflows. In this study, I analyzed GitHub Copilot's interaction with users
through its chat interface, measured Copilot's ability to adapt explanations
and code generation to user expertise levels, and assessed its effectiveness in
facilitating collaborative programming experiences. I established a
human-centered requirements framework with clear metrics to evaluate these
qualities in GitHub Copilot chat. I discussed the test results and their
implications for future analysis of human requirements in automated
programming.

</details>


### [5] [Analyzing Prominent LLMs: An Empirical Study of Performance and Complexity in Solving LeetCode Problems](https://arxiv.org/abs/2508.03931)
*Everton Guimaraes,Nathalia Nascimento,Chandan Shivalingaiah,Asish Nelapati*

Main category: cs.SE

TL;DR: 该研究对四种大型语言模型（ChatGPT、Copilot、Gemini和DeepSeek）在150道LeetCode题目上的表现进行了系统比较，评估了执行时间、内存使用和算法复杂度。


<details>
  <summary>Details</summary>
Motivation: 随着LLMs在软件开发中的广泛应用，对其性能进行系统比较以优化实际应用至关重要。

Method: 研究通过生成Java和Python的解决方案，在150道LeetCode题目（分简单、中等和困难难度）上对四种模型进行基准测试。

Result: ChatGPT在执行时间和内存使用上表现一致高效，Copilot和DeepSeek在任务复杂度增加时表现不稳定，Gemini在简单任务上有效但难度增加时需要更多尝试。

Conclusion: 研究结果为开发者选择适合特定编码任务的LLM提供了实用指导，并揭示了GPT类生成解决方案的性能和复杂性。

Abstract: Large Language Models (LLMs) like ChatGPT, Copilot, Gemini, and DeepSeek are
transforming software engineering by automating key tasks, including code
generation, testing, and debugging. As these models become integral to
development workflows, a systematic comparison of their performance is
essential for optimizing their use in real world applications. This study
benchmarks these four prominent LLMs on one hundred and fifty LeetCode problems
across easy, medium, and hard difficulties, generating solutions in Java and
Python. We evaluate each model based on execution time, memory usage, and
algorithmic complexity, revealing significant performance differences. ChatGPT
demonstrates consistent efficiency in execution time and memory usage, while
Copilot and DeepSeek show variability as task complexity increases. Gemini,
although effective on simpler tasks, requires more attempts as problem
difficulty rises. Our findings provide actionable insights into each model's
strengths and limitations, offering guidance for developers selecting LLMs for
specific coding tasks and providing insights on the performance and complexity
of GPT-like generated solutions.

</details>


### [6] [Model Compression vs. Adversarial Robustness: An Empirical Study on Language Models for Code](https://arxiv.org/abs/2508.03949)
*Md. Abdul Awal,Mrigank Rochan,Chanchal K. Roy*

Main category: cs.SE

TL;DR: 论文研究了代码语言模型压缩技术（如剪枝、量化和知识蒸馏）在对抗性攻击下的鲁棒性，发现压缩模型性能接近原始模型，但对抗性鲁棒性显著降低。


<details>
  <summary>Details</summary>
Motivation: Transformer-based代码语言模型的高计算成本和环境影响阻碍了其应用，压缩技术虽能缓解这些问题，但其在对抗性场景下的鲁棒性影响尚不明确。

Method: 对三种广泛使用的代码语言模型进行压缩，并在三个软件分析任务中评估其对抗性鲁棒性，使用六种评估指标和四种经典对抗攻击。

Result: 压缩模型性能与原始模型相当，但在对抗性攻击下鲁棒性显著降低，揭示了模型大小与鲁棒性之间的权衡。

Conclusion: 需进一步研究平衡计算效率与对抗性鲁棒性的压缩策略，以确保代码语言模型在安全关键应用中的可靠部署。

Abstract: Transformer-based language models for code have shown remarkable performance
in various software analytics tasks, but their adoption is hindered by high
computational costs, slow inference speeds, and substantial environmental
impact. Model compression techniques such as pruning, quantization, and
knowledge distillation have gained traction in addressing these challenges.
However, the impact of these strategies on the robustness of compressed
language models for code in adversarial scenarios remains poorly understood.
Understanding how these compressed models behave under adversarial attacks is
essential for their safe and effective deployment in real-world applications.
To bridge this knowledge gap, we conduct a comprehensive evaluation of how
common compression strategies affect the adversarial robustness of compressed
models. We assess the robustness of compressed versions of three widely used
language models for code across three software analytics tasks, using six
evaluation metrics and four commonly used classical adversarial attacks. Our
findings indicate that compressed models generally maintain comparable
performance to their uncompressed counterparts. However, when subjected to
adversarial attacks, compressed models exhibit significantly reduced
robustness. These results reveal a trade-off between model size reduction and
adversarial robustness, underscoring the need for careful consideration when
deploying compressed models in security-critical software applications. Our
study highlights the need for further research into compression strategies that
strike a balance between computational efficiency and adversarial robustness,
which is essential for deploying reliable language models for code in
real-world software applications.

</details>


### [7] [Experimental Analysis of Productive Interaction Strategy with ChatGPT: User Study on Function and Project-level Code Generation Tasks](https://arxiv.org/abs/2508.04125)
*Sangwon Hyun,Hyunjun Kim,Jinhyuk Jang,Hyojin Choi,M. Ali Babar*

Main category: cs.SE

TL;DR: 研究探讨了大型语言模型（LLMs）在软件工程任务中的应用，发现现有研究多局限于函数级任务，忽略了更复杂的项目级问题。通过实验和用户研究，提出了影响代码生成效率的关键特征、提升效率的指南以及错误分类与缓解方案。


<details>
  <summary>Details</summary>
Motivation: 现有研究多关注函数级软件工程任务，缺乏对项目级复杂性和人机交互（HLI）特征的全面分析，本研究旨在填补这一空白。

Method: 设计了包含项目级任务的实验，招募36名参与者使用GPT助手完成任务，通过分析屏幕记录和聊天日志，研究HLI特征对代码生成效率的影响。

Result: 发现3个HLI特征显著影响代码生成效率，提出了5条提升效率的指南，并分类了29种运行时和逻辑错误及其缓解方案。

Conclusion: 研究扩展了对LLMs在软件工程中应用的理解，为项目级任务提供了实用指南和错误管理策略。

Abstract: The application of Large Language Models (LLMs) is growing in the productive
completion of Software Engineering tasks. Yet, studies investigating the
productive prompting techniques often employed a limited problem space,
primarily focusing on well-known prompting patterns and mainly targeting
function-level SE practices. We identify significant gaps in real-world
workflows that involve complexities beyond class-level (e.g., multi-class
dependencies) and different features that can impact Human-LLM Interactions
(HLIs) processes in code generation. To address these issues, we designed an
experiment that comprehensively analyzed the HLI features regarding the code
generation productivity. Our study presents two project-level benchmark tasks,
extending beyond function-level evaluations. We conducted a user study with 36
participants from diverse backgrounds, asking them to solve the assigned tasks
by interacting with the GPT assistant using specific prompting patterns. We
also examined the participants' experience and their behavioral features during
interactions by analyzing screen recordings and GPT chat logs. Our statistical
and empirical investigation revealed (1) that three out of 15 HLI features
significantly impacted the productivity in code generation; (2) five primary
guidelines for enhancing productivity for HLI processes; and (3) a taxonomy of
29 runtime and logic errors that can occur during HLI processes, along with
suggested mitigation plans.

</details>


### [8] [EVOC2RUST: A Skeleton-guided Framework for Project-Level C-to-Rust Translation](https://arxiv.org/abs/2508.04295)
*Chaofan Wang,Tingrui Yu,Jie Wang,Dong Chen,Wenrui Zhang,Yuling Shi,Xiaodong Gu,Beijun Shen*

Main category: cs.SE

TL;DR: EvoC2Rust是一个自动化框架，用于将整个C项目转换为等效的Rust项目，结合了规则和LLM方法的优势，显著提升了翻译的准确性和代码安全性。


<details>
  <summary>Details</summary>
Motivation: Rust的编译时安全特性使其适合安全关键系统，但现有C到Rust的转换方法在小规模程序上表现有限，无法满足大规模项目的需求。

Method: EvoC2Rust采用骨架引导的翻译策略，分三个阶段：模块分解与骨架生成、函数增量翻译以及编译错误修复。

Result: 在开源基准和工业项目上，EvoC2Rust在语法和语义准确性上分别提升了17.24%和14.32%，代码安全性提高了96.79%。

Conclusion: EvoC2Rust在项目级C到Rust翻译中表现出色，适用于复杂和大规模的代码库。

Abstract: Rust's compile-time safety guarantees make it ideal for safety-critical
systems, creating demand for translating legacy C codebases to Rust. While
various approaches have emerged for this task, they face inherent trade-offs:
rule-based solutions face challenges in meeting code safety and idiomaticity
requirements, while LLM-based solutions often fail to generate semantically
equivalent Rust code, due to the heavy dependencies of modules across the
entire codebase. Recent studies have revealed that both solutions are limited
to small-scale programs. In this paper, we propose EvoC2Rust, an automated
framework for converting entire C projects to equivalent Rust ones. EvoC2Rust
employs a skeleton-guided translation strategy for project-level translation.
The pipeline consists of three evolutionary stages: 1) it first decomposes the
C project into functional modules, employs a feature-mapping-enhanced LLM to
transform definitions and macros and generates type-checked function stubs,
which form a compilable Rust skeleton; 2) it then incrementally translates the
function, replacing the corresponding stub placeholder; 3) finally, it repairs
compilation errors by integrating LLM and static analysis. Through evolutionary
augmentation, EvoC2Rust combines the advantages of both rule-based and
LLM-based solutions. Our evaluation on open-source benchmarks and six
industrial projects demonstrates EvoC2Rust's superior performance in
project-level C-to-Rust translation. On average, it achieves 17.24% and 14.32%
improvements in syntax and semantic accuracy over the LLM-based approaches,
along with a 96.79% higher code safety rate than the rule-based tools. At the
module level, EvoC2Rust reaches 92.25% compilation and 89.53% test pass rates
on industrial projects, even for complex codebases and long functions.

</details>


### [9] [Vanilla-Converter: A Tool for Converting Camunda 7 BPMN Models into Camunda 8 Models](https://arxiv.org/abs/2508.04352)
*Dragana Sunaric,Charlotte Verbruggen,Dominik Bork*

Main category: cs.SE

TL;DR: Vanilla-Converter是一个命令行工具，用于自动化将BPMN模型从Camunda 7迁移到Camunda 8，支持多种BPMN元素，并生成转换日志。


<details>
  <summary>Details</summary>
Motivation: 由于Camunda 7和Camunda 8之间存在根本差异，手动迁移复杂且耗时。

Method: 开发了Vanilla-Converter工具，自动化转换过程，支持广泛的BPMN元素，并生成转换日志。

Result: 通过三个实际工业案例验证，工具能够将Camunda 7模型转换为有效且可执行的Camunda 8模型。

Conclusion: Vanilla-Converter显著简化了迁移过程，提高了效率和准确性。

Abstract: As organizations prepare for the end-of-life of Camunda 7, manual migration
remains complex due to fundamental differences between the two platforms. We
present Vanilla-Converter, a command-line tool that facilitates the migration
of BPMN models from Camunda 7 to Camunda 8. Vanilla-Converter automates the
transformation process, supports a wide range of BPMN elements, and produces a
transformed model and a detailed transformation log indicating automatic
changes and remaining manual conversion tasks. The tool's effectiveness is
demonstrated through three case studies with real industrially used Camunda 7
models, confirming its ability to convert these models into valid and
executable Camunda 8 models.

</details>


### [10] [Breaking New Ground in Software Defect Prediction: Introducing Practical and Actionable Metrics with Superior Predictive Power for Enhanced Decision-Making](https://arxiv.org/abs/2508.04408)
*Carlos Andrés Ramírez Cataño,Makoto Itoh*

Main category: cs.SE

TL;DR: 论文提出了一种基于开发者编码习惯的软件缺陷预测方法，通过人类因素理论设计指标，性能优于现有代码和历史指标。


<details>
  <summary>Details</summary>
Motivation: 软件缺陷的根本原因常归因于人为错误，但现有研究多关注代码指标，而忽略非软件指标。本文探索开发者编码习惯对缺陷预测的作用。

Method: 提出一个框架选择预测指标，并与性能最高的代码和提交历史指标进行比较，分析各指标的重要性。

Result: 在21个关键基础设施开源项目中，新指标的平均预测性能优于现有方法，且新指标的解释性和实用性显著提升。

Conclusion: 通过人类错误框架预测缺陷方法，为实践者提供了可操作的预测工具，证明了开发者习惯对缺陷的影响。

Abstract: Software defect prediction using code metrics has been extensively researched
over the past five decades. However, prediction harnessing non-software metrics
is under-researched. Considering that the root cause of software defects is
often attributed to human error, human factors theory might offer key
forecasting metrics for actionable insights. This paper explores automated
software defect prediction at the method level based on the developers' coding
habits. First, we propose a framework for deciding the metrics to conduct
predictions. Next, we compare the performance of our metrics to that of the
code and commit history metrics shown by research to achieve the highest
performance to date. Finally, we analyze the prediction importance of each
metric. As a result of our analyses of twenty-one critical infrastructure
large-scale open-source software projects, we have presented: (1) a human
error-based framework with metrics useful for defect prediction at method
level; (2) models using our proposed metrics achieve better average prediction
performance than the state-of-the-art code metrics and history measures; (3)
the prediction importance of all metrics distributes differently with each of
the novel metrics having better average importance than code and history
metrics; (4) the novel metrics dramatically enhance the explainability,
practicality, and actionability of software defect prediction models,
significantly advancing the field. We present a systematic approach to
forecasting defect-prone software methods via a human error framework. This
work empowers practitioners to act on predictions, empirically demonstrating
how developer coding habits contribute to defects in software systems.

</details>


### [11] [Large Language Models Versus Static Code Analysis Tools: A Systematic Benchmark for Vulnerability Detection](https://arxiv.org/abs/2508.04448)
*Damian Gnieciak,Tomasz Szandala*

Main category: cs.SE

TL;DR: 该研究对比了六种自动化代码分析工具（三种静态分析工具和三种大型语言模型）在检测漏洞方面的表现，发现语言模型在召回率上表现更优，但存在误报和定位不精确的问题，建议结合使用。


<details>
  <summary>Details</summary>
Motivation: 评估现代自动化测试工具在检测代码漏洞方面的有效性，为开发者提供选择依据。

Method: 使用十个真实C#项目（含63个漏洞）对比六种工具的性能指标（精度、召回率、F1分数、分析延迟和开发者工作量）。

Result: 语言模型的平均F1分数（0.797, 0.753, 0.750）高于静态工具（0.260, 0.386, 0.546），但误报率高且定位不精确。

Conclusion: 建议结合语言模型的广泛筛查和静态工具的高精度验证，以提高代码安全性。

Abstract: Modern software relies on a multitude of automated testing and quality
assurance tools to prevent errors, bugs and potential vulnerabilities. This
study sets out to provide a head-to-head, quantitative and qualitative
evaluation of six automated approaches: three industry-standard rule-based
static code-analysis tools (SonarQube, CodeQL and Snyk Code) and three
state-of-the-art large language models hosted on the GitHub Models platform
(GPT-4.1, Mistral Large and DeepSeek V3). Using a curated suite of ten
real-world C# projects that embed 63 vulnerabilities across common categories
such as SQL injection, hard-coded secrets and outdated dependencies, we measure
classical detection accuracy (precision, recall, F-score), analysis latency,
and the developer effort required to vet true positives. The language-based
scanners achieve higher mean F-1 scores,0.797, 0.753 and 0.750, than their
static counterparts, which score 0.260, 0.386 and 0.546, respectively. LLMs'
advantage originates from superior recall, confirming an ability to reason
across broader code contexts. However, this benefit comes with substantial
trade-offs: DeepSeek V3 exhibits the highest false-positive ratio, all language
models mislocate issues at line-or-column granularity due to tokenisation
artefacts. Overall, language models successfully rival traditional static
analysers in finding real vulnerabilities. Still, their noisier output and
imprecise localisation limit their standalone use in safety-critical audits. We
therefore recommend a hybrid pipeline: employ language models early in
development for broad, context-aware triage, while reserving deterministic
rule-based scanners for high-assurance verification. The open benchmark and
JSON-based result harness released with this paper lay a foundation for
reproducible, practitioner-centric research into next-generation automated code
security.

</details>


### [12] [Manifestations of Empathy in Software Engineering: How, Why, and When It Matters](https://arxiv.org/abs/2508.04479)
*Hashini Gunatilake,John Grundy,Rashina Hoda,Ingo Mueller*

Main category: cs.SE

TL;DR: 研究探讨了同理心在软件工程中的表现、动机及影响因素，通过访谈和调查揭示了其实际应用和驱动因素。


<details>
  <summary>Details</summary>
Motivation: 理解同理心在软件工程中的具体表现、动机及影响因素，填补现有研究的空白。

Method: 通过22次访谈和116名软件从业者的大规模调查进行研究。

Result: 揭示了同理心在软件工程中的表达方式、驱动因素及其影响，并提出了实际应用建议。

Conclusion: 研究为软件工程从业者和研究者提供了如何有效整合同理心的深入见解。

Abstract: Empathy plays a crucial role in software engineering (SE), influencing
collaboration, communication, and decision-making. While prior research has
highlighted the importance of empathy in SE, there is limited understanding of
how empathy manifests in SE practice, what motivates SE practitioners to
demonstrate empathy, and the factors that influence empathy in SE work. Our
study explores these aspects through 22 interviews and a large scale survey
with 116 software practitioners. Our findings provide insights into the
expression of empathy in SE, the drivers behind empathetic practices, SE
activities where empathy is perceived as useful or not, and the other factors
that influence empathy. In addition, we offer practical implications for SE
practitioners and researchers, offering a deeper understanding of how to
effectively integrate empathy into SE processes.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [13] [PLA: Prompt Learning Attack against Text-to-Image Generative Models](https://arxiv.org/abs/2508.03696)
*Xinqi Lyu,Yihao Liu,Yanjie Li,Bin Xiao*

Main category: cs.CR

TL;DR: 本文提出了一种新颖的提示学习攻击框架（PLA），用于在黑盒设置下绕过文本到图像（T2I）模型的安全机制，通过多模态相似性设计梯度训练方法，实验证明其攻击成功率高于现有方法。


<details>
  <summary>Details</summary>
Motivation: 尽管T2I模型广泛应用，但其可能被滥用以生成不安全内容（NSFW）。现有方法因搜索空间有限而性能不足，黑盒设置下梯度驱动攻击方法难以实现。

Method: 提出PLA框架，利用多模态相似性设计黑盒T2I模型的梯度训练方法，学习对抗性提示。

Result: 实验表明，PLA能高效攻击黑盒T2I模型的安全机制（如提示过滤器和后验安全检查器），成功率优于现有方法。

Conclusion: PLA为黑盒T2I模型的安全漏洞提供了新视角，展示了对抗性攻击的潜在风险。

Abstract: Text-to-Image (T2I) models have gained widespread adoption across various
applications. Despite the success, the potential misuse of T2I models poses
significant risks of generating Not-Safe-For-Work (NSFW) content. To
investigate the vulnerability of T2I models, this paper delves into adversarial
attacks to bypass the safety mechanisms under black-box settings. Most previous
methods rely on word substitution to search adversarial prompts. Due to limited
search space, this leads to suboptimal performance compared to gradient-based
training. However, black-box settings present unique challenges to training
gradient-driven attack methods, since there is no access to the internal
architecture and parameters of T2I models. To facilitate the learning of
adversarial prompts in black-box settings, we propose a novel prompt learning
attack framework (PLA), where insightful gradient-based training tailored to
black-box T2I models is designed by utilizing multimodal similarities.
Experiments show that our new method can effectively attack the safety
mechanisms of black-box T2I models including prompt filters and post-hoc safety
checkers with a high success rate compared to state-of-the-art methods.
Warning: This paper may contain offensive model-generated content.

</details>


### [14] [RX-INT: A Kernel Engine for Real-Time Detection and Analysis of In-Memory Threats](https://arxiv.org/abs/2508.03879)
*Arjun Juneja*

Main category: cs.CR

TL;DR: RX-INT是一种内核辅助系统，通过实时线程创建监控和状态化VAD扫描器，结合启发式方法，提高了对无文件恶意软件的检测率。


<details>
  <summary>Details</summary>
Motivation: 传统基于签名的安全产品难以检测无文件执行技术（如手动映射、模块覆盖等），现有工具依赖PE结构或易受TOCTOU攻击。

Method: RX-INT采用实时线程创建监控、状态化VAD扫描器和内存哈希技术，快照私有和镜像内存区域以检测非法修改。

Result: 在基准测试中，RX-INT检测率高于PE-sieve，成功识别了后者未检测到的手动映射区域。

Conclusion: RX-INT架构显著提升了无文件威胁的检测能力，适用于反作弊和内存安全领域。

Abstract: Malware and cheat developers use fileless execution techniques to evade
traditional, signature-based security products. These methods include various
types of manual mapping, module stomping, and threadless injection which work
entirely within the address space of a legitimate process, presenting a
challenge for detection due to ambiguity between what is legitimate and what
isn't. Existing tools often have weaknesses, such as a dependency on Portable
Executable (PE) structures or a vulnerability to time-of-check-to-time-of-use
(TOCTOU) race conditions where an adversary cleans up before a periodic scan
has the chance to occur. To address this gap, we present RX-INT, a
kernel-assisted system featuring an architecture that provides resilience
against TOCTOU attacks. RX-INT introduces a detection engine that combines a
real-time thread creation monitor with a stateful Virtual Address Descriptor
(VAD) scanner alongside various heuristics within. This engine snapshots both
private and image-backed memory regions, using real-time memory hashing to
detect illicit modifications like module stomping. Critically, we demonstrate a
higher detection rate in certain benchmarks of this approach through a direct
comparison with PE-sieve, a commonly used and powerful memory forensics tool.
In our evaluation, RX-INT successfully detected a manually mapped region that
was not identified by PE-sieve. We then conclude that our architecture
represents a tangible difference in the detection of fileless threats, with
direct applications in the fields of anti-cheat and memory security.

</details>


### [15] [Simulating Cyberattacks through a Breach Attack Simulation (BAS) Platform empowered by Security Chaos Engineering (SCE)](https://arxiv.org/abs/2508.03882)
*Arturo Sánchez-Matas,Pablo Escribano Ruiz,Daniel Díaz-López,Angel Luis Perales Gómez,Pantaleone Nespoli,Gregorio Martínez Pérez*

Main category: cs.CR

TL;DR: 论文提出将安全混沌工程（SCE）整合到漏洞攻击模拟（BAS）平台中，通过三层架构（SCE Orchestrator、Connector和BAS层）和MITRE Caldera工具，提升攻击模拟效果。


<details>
  <summary>Details</summary>
Motivation: 面对不断演变的网络威胁，组织需要发现隐蔽的攻击途径，SCE提供了一种测试防御和识别漏洞的新方法。

Method: 提出三层架构整合SCE与BAS，利用MITRE Caldera执行自动化攻击序列，生成攻击树。

Result: 评估表明，SCE与BAS的整合能超越传统攻击模拟场景，提升防御策略的有效性。

Conclusion: SCE与BAS的整合是网络安全防御策略中的有用组件。

Abstract: In today digital landscape, organizations face constantly evolving cyber
threats, making it essential to discover slippery attack vectors through novel
techniques like Security Chaos Engineering (SCE), which allows teams to test
defenses and identify vulnerabilities effectively. This paper proposes to
integrate SCE into Breach Attack Simulation (BAS) platforms, leveraging
adversary profiles and abilities from existing threat intelligence databases.
This innovative proposal for cyberattack simulation employs a structured
architecture composed of three layers: SCE Orchestrator, Connector, and BAS
layers. Utilizing MITRE Caldera in the BAS layer, our proposal executes
automated attack sequences, creating inferred attack trees from adversary
profiles. Our proposal evaluation illustrates how integrating SCE with BAS can
enhance the effectiveness of attack simulations beyond traditional scenarios,
and be a useful component of a cyber defense strategy.

</details>


### [16] [ASTRA: Autonomous Spatial-Temporal Red-teaming for AI Software Assistants](https://arxiv.org/abs/2508.03936)
*Xiangzhe Xu,Guangyu Shen,Zian Su,Siyuan Cheng,Hanxi Guo,Lu Yan,Xuan Chen,Jiasheng Jiang,Xiaolong Jin,Chengpeng Wang,Zhuo Zhang,Xiangyu Zhang*

Main category: cs.CR

TL;DR: ASTRA是一种自动化代理系统，用于系统地发现AI驱动代码生成和安全性指导系统中的安全漏洞，通过结构化知识图谱和在线漏洞探索，显著优于现有技术。


<details>
  <summary>Details</summary>
Motivation: 当前AI编码助手的安全性在高风险领域（如网络安全）仍不确定，现有工具依赖固定基准或不现实的提示，无法覆盖真实漏洞。

Method: ASTRA分三阶段工作：构建领域知识图谱、在线漏洞探索（空间和时间维度）、生成高质量违规案例以改进模型对齐。

Result: ASTRA在两大评估领域中发现比现有技术多11-66%的问题，测试案例使对齐训练效果提升17%。

Conclusion: ASTRA通过现实输入和动态知识图谱，显著提升AI系统的安全性，具有实际应用价值。

Abstract: AI coding assistants like GitHub Copilot are rapidly transforming software
development, but their safety remains deeply uncertain-especially in
high-stakes domains like cybersecurity. Current red-teaming tools often rely on
fixed benchmarks or unrealistic prompts, missing many real-world
vulnerabilities. We present ASTRA, an automated agent system designed to
systematically uncover safety flaws in AI-driven code generation and security
guidance systems. ASTRA works in three stages: (1) it builds structured
domain-specific knowledge graphs that model complex software tasks and known
weaknesses; (2) it performs online vulnerability exploration of each target
model by adaptively probing both its input space, i.e., the spatial
exploration, and its reasoning processes, i.e., the temporal exploration,
guided by the knowledge graphs; and (3) it generates high-quality
violation-inducing cases to improve model alignment. Unlike prior methods,
ASTRA focuses on realistic inputs-requests that developers might actually
ask-and uses both offline abstraction guided domain modeling and online domain
knowledge graph adaptation to surface corner-case vulnerabilities. Across two
major evaluation domains, ASTRA finds 11-66% more issues than existing
techniques and produces test cases that lead to 17% more effective alignment
training, showing its practical value for building safer AI systems.

</details>


### [17] [Isolate Trigger: Detecting and Eradicating Evade-Adaptive Backdoors](https://arxiv.org/abs/2508.04094)
*Chengrui Sun,Hua Zhang,Haoran Gao,Zian Tian,Jianjin Zhao,qi Li,Hongliang Zhu,Zongliang Shen,Shang Wang,Anmin Fu*

Main category: cs.CR

TL;DR: 提出了一种名为IsTr的新框架，用于检测和防御后门攻击，突破了现有非本质特征（NEF）方法的局限。


<details>
  <summary>Details</summary>
Motivation: 现有后门攻击检测方法（NEF）无法应对高效的自适应后门攻击（EAB），需要一种更精确、通用的解决方案。

Method: IsTr通过Steps和Differential-Middle-Slice组件，更新距离和梯度的理论，以发现隐藏的触发机制。

Result: 在MNIST、人脸识别和交通标志识别等任务中验证了IsTr的高效性、通用性和精确性，成功抵御了六种EAB攻击。

Conclusion: IsTr是一种高效、通用的后门检测与防御框架，即使在触发机制与源特征重叠的情况下仍能有效工作。

Abstract: All current detection of backdoor attacks on deep learning models fall under
the category of a non essential features(NEF), which focus on fighting against
simple and efficient vertical class backdoor -- trigger is small, few and not
overlapping with the source. Evade-adaptive backdoor (EAB) attacks have evaded
NEF detection and improved training efficiency. We introduces a precise,
efficient and universal detection and defense framework coined as Isolate
Trigger (IsTr). IsTr aims to find the hidden trigger by breaking the barrier of
the source features. Therefore, it investigates the essence of backdoor
triggering, and uses Steps and Differential-Middle-Slice as components to
update past theories of distance and gradient. IsTr also plays a positive role
in the model, whether the backdoor exists. For example, accurately find and
repair the wrong identification caused by deliberate or unintentional training
in automatic driving. Extensive experiments on robustness scross various tasks,
including MNIST, facial recognition, and traffic sign recognition, confirm the
high efficiency, generality and precision of the IsTr. We rigorously evaluated
the effectiveness of the IsTr against a series of six EAB attacks, including
Badnets, Sin-Wave, Multi-trigger, SSBAs, CASSOCK, HCB. None of these
countermeasures evade, even when attacks are combined and the trigger and
source overlap.

</details>


### [18] [SenseCrypt: Sensitivity-guided Selective Homomorphic Encryption for Joint Federated Learning in Cross-Device Scenarios](https://arxiv.org/abs/2508.04100)
*Borui Li,Li Yan,Junhao Han,Jianmin Liu,Lei Yu*

Main category: cs.CR

TL;DR: SenseCrypt是一种基于敏感性的选择性同态加密框架，用于在跨设备联邦学习中平衡安全性和计算开销。


<details>
  <summary>Details</summary>
Motivation: 传统选择性同态加密方法在跨设备场景中因数据异构性和系统能力差异导致性能下降，需要一种更高效的解决方案。

Method: 通过隐私保护方法聚类相似数据分布的客户端，设计评分机制确定加密比例，并优化模型参数选择以最小化开销和最大化安全性。

Result: SenseCrypt在保持安全性的同时，将训练时间减少58.4%-88.7%，并维持模型准确性。

Conclusion: SenseCrypt在跨设备联邦学习中有效平衡了安全性和计算效率。

Abstract: Homomorphic Encryption (HE) prevails in securing Federated Learning (FL), but
suffers from high overhead and adaptation cost. Selective HE methods, which
partially encrypt model parameters by a global mask, are expected to protect
privacy with reduced overhead and easy adaptation. However, in cross-device
scenarios with heterogeneous data and system capabilities, traditional
Selective HE methods deteriorate client straggling, and suffer from degraded HE
overhead reduction performance. Accordingly, we propose SenseCrypt, a
Sensitivity-guided selective Homomorphic EnCryption framework, to adaptively
balance security and HE overhead per cross-device FL client. Given the
observation that model parameter sensitivity is effective for measuring
clients' data distribution similarity, we first design a privacy-preserving
method to respectively cluster the clients with similar data distributions.
Then, we develop a scoring mechanism to deduce the straggler-free ratio of
model parameters that can be encrypted by each client per cluster. Finally, for
each client, we formulate and solve a multi-objective model parameter selection
optimization problem, which minimizes HE overhead while maximizing model
security without causing straggling. Experiments demonstrate that SenseCrypt
ensures security against the state-of-the-art inversion attacks, while
achieving normal model accuracy as on IID data, and reducing training time by
58.4%-88.7% as compared to traditional HE methods.

</details>


### [19] [Evaluating Selective Encryption Against Gradient Inversion Attacks](https://arxiv.org/abs/2508.04155)
*Jiajun Gu,Yuhang Yao,Shuaiqi Wang,Carlee Joe-Wong*

Main category: cs.CR

TL;DR: 本文系统评估了选择性加密方法在不同重要性度量下的效果，提出了一种基于距离的重要性分析框架，并验证了梯度幅度作为有效度量对抗梯度反转攻击的可行性。


<details>
  <summary>Details</summary>
Motivation: 梯度反转攻击对联邦学习等分布式训练框架构成隐私威胁，传统加密方法计算开销大，选择性加密成为潜在解决方案，但缺乏系统性研究。

Method: 通过实验评估不同重要性度量的选择性加密方法，提出基于距离的分析框架，并在多种模型和攻击类型下验证。

Result: 梯度幅度是有效的保护度量，但无单一策略适用于所有场景，需根据模型和隐私需求选择。

Conclusion: 选择性加密可降低计算开销并保持抗攻击能力，需结合模型和攻击类型灵活选择策略。

Abstract: Gradient inversion attacks pose significant privacy threats to distributed
training frameworks such as federated learning, enabling malicious parties to
reconstruct sensitive local training data from gradient communications between
clients and an aggregation server during the aggregation process. While
traditional encryption-based defenses, such as homomorphic encryption, offer
strong privacy guarantees without compromising model utility, they often incur
prohibitive computational overheads. To mitigate this, selective encryption has
emerged as a promising approach, encrypting only a subset of gradient data
based on the data's significance under a certain metric. However, there have
been few systematic studies on how to specify this metric in practice. This
paper systematically evaluates selective encryption methods with different
significance metrics against state-of-the-art attacks. Our findings demonstrate
the feasibility of selective encryption in reducing computational overhead
while maintaining resilience against attacks. We propose a distance-based
significance analysis framework that provides theoretical foundations for
selecting critical gradient elements for encryption. Through extensive
experiments on different model architectures (LeNet, CNN, BERT, GPT-2) and
attack types, we identify gradient magnitude as a generally effective metric
for protection against optimization-based gradient inversions. However, we also
observe that no single selective encryption strategy is universally optimal
across all attack scenarios, and we provide guidelines for choosing appropriate
strategies for different model architectures and privacy requirements.

</details>


### [20] [Secure Development of a Hooking-Based Deception Framework Against Keylogging Techniques](https://arxiv.org/abs/2508.04178)
*Md Sajidul Islam Sajid,Shihab Ahmed,Ryan Sosnoski*

Main category: cs.CR

TL;DR: 论文提出了一种基于API钩子的欺骗框架，通过注入虚假按键来对抗高级键盘记录器，并引入加固的钩子层以应对反钩技术。


<details>
  <summary>Details</summary>
Motivation: 传统防御方法仅能检测和移除键盘记录器，无法有效误导攻击者，而高级键盘记录器采用反钩技术逃避检测，因此需要更强大的对抗手段。

Method: 利用API钩子拦截键盘记录器的输入相关API调用，注入虚假按键，并通过加固的钩子层检测和恢复被破坏的钩子。

Result: 实验表明，该系统能有效抵抗高级规避技术，保持操作隐蔽性，并可靠地欺骗攻击者，同时性能开销极低。

Conclusion: 运行时欺骗技术能够实用且稳健地对抗高级威胁。

Abstract: Keyloggers remain a serious threat in modern cybersecurity, silently
capturing user keystrokes to steal credentials and sensitive information.
Traditional defenses focus mainly on detection and removal, which can halt
malicious activity but do little to engage or mislead adversaries. In this
paper, we present a deception framework that leverages API hooking to intercept
input-related API calls invoked by keyloggers at runtime and inject realistic
decoy keystrokes. A core challenge, however, lies in the increasing adoption of
anti-hooking techniques by advanced keyloggers. Anti-hooking strategies allow
malware to bypass or detect instrumentation. To counter this, we introduce a
hardened hooking layer that detects tampering and rapidly reinstates disrupted
hooks, ensuring continuity of deception. We evaluate our framework against a
custom-built "super keylogger" incorporating multiple evasion strategies, as
well as 50 real-world malware samples spanning ten prominent keylogger
families. Experimental results demonstrate that our system successfully resists
sophisticated bypass attempts, maintains operational stealth, and reliably
deceives attackers by feeding them decoys. The system operates with negligible
performance overhead and no observable impact on user experience. Our findings
show that resilient, runtime deception can play a practical and robust role in
confronting advanced threats.

</details>


### [21] [BadTime: An Effective Backdoor Attack on Multivariate Long-Term Time Series Forecasting](https://arxiv.org/abs/2508.04189)
*Kunlan Xiang,Haomiao Yang,Meng Hao,Haoxin Wang,Shaofeng Li,Wenbo Jiang*

Main category: cs.CR

TL;DR: 论文提出了一种针对多元长期时间序列预测（MLTSF）模型的后门攻击方法BadTime，通过数据毒化和定制化训练过程实现高效攻击。


<details>
  <summary>Details</summary>
Motivation: MLTSF模型在关键领域广泛应用，但其对抗恶意后门攻击的鲁棒性尚未研究，BadTime填补了这一空白。

Method: BadTime采用对比引导策略选择毒化样本，利用图注意力网络识别关键变量，并通过滞后分析和拼图式触发器结构优化攻击效果。

Result: 实验表明，BadTime显著优于现有方法，目标变量的MAE降低50%以上，隐蔽性提升3倍以上。

Conclusion: BadTime为MLTSF模型的后门攻击提供了首个有效解决方案，强调了模型安全性研究的重要性。

Abstract: Multivariate Long-Term Time Series Forecasting (MLTSF) models are
increasingly deployed in critical domains such as climate, finance, and
transportation. Although a variety of powerful MLTSF models have been proposed
to improve predictive performance, the robustness of MLTSF models against
malicious backdoor attacks remains entirely unexplored, which is crucial to
ensuring their reliable and trustworthy deployment. To address this gap, we
conduct an in-depth study on backdoor attacks against MLTSF models and propose
the first effective attack method named BadTime. BadTime executes a backdoor
attack by poisoning training data and customizing the backdoor training
process. During data poisoning, BadTime proposes a contrast-guided strategy to
select the most suitable training samples for poisoning, then employs a graph
attention network to identify influential variables for trigger injection.
Subsequently, BadTime further localizes optimal positions for trigger injection
based on lag analysis and proposes a puzzle-like trigger structure that
distributes the trigger across multiple poisoned variables to jointly steer the
prediction of the target variable. During backdoor training, BadTime
alternately optimizes the model and triggers via proposed tailored optimization
objectives. Extensive experiments show that BadTime significantly outperforms
state-of-the-art (SOTA) backdoor attacks on time series forecasting by reducing
MAE by over 50% on target variables and boosting stealthiness by more than 3
times.

</details>


### [22] [DP-DocLDM: Differentially Private Document Image Generation using Latent Diffusion Models](https://arxiv.org/abs/2508.04208)
*Saifullah Saifullah,Stefan Agne,Andreas Dengel,Sheraz Ahmed*

Main category: cs.CR

TL;DR: 论文提出了一种结合条件潜在扩散模型（LDM）和差分隐私（DP）的方法，生成隐私保护的合成文档图像，用于训练下游分类器，避免了DP直接训练的性能下降问题。


<details>
  <summary>Details</summary>
Motivation: 解决深度学习信息提取系统中敏感数据泄露风险，同时避免差分隐私（DP）直接训练导致的性能下降和限制。

Method: 使用条件潜在扩散模型（LDM）结合差分隐私（DP）生成类特定的合成文档图像，并通过多种预训练和私有训练策略（如DPDM和DP-Promise算法）优化。

Result: 在RVL-CDIP和Tobacco3482数据集上生成了多样且真实的文档样本，并在小规模数据集的下游任务中显著优于直接应用DP-Adam。

Conclusion: 该方法通过合成数据替代真实私有数据，有效平衡了隐私保护与模型性能，为文档分类任务提供了可行的解决方案。

Abstract: As deep learning-based, data-driven information extraction systems become
increasingly integrated into modern document processing workflows, one primary
concern is the risk of malicious leakage of sensitive private data from these
systems. While some recent works have explored Differential Privacy (DP) to
mitigate these privacy risks, DP-based training is known to cause significant
performance degradation and impose several limitations on standard training
procedures, making its direct application to downstream tasks both difficult
and costly. In this work, we aim to address the above challenges within the
context of document image classification by substituting real private data with
a synthetic counterpart. In particular, we propose to use conditional latent
diffusion models (LDMs) in combination with differential privacy (DP) to
generate class-specific synthetic document images under strict privacy
constraints, which can then be utilized to train a downstream classifier
following standard training procedures. We investigate our approach under
various pretraining setups, including unconditional, class-conditional, and
layout-conditional pretraining, in combination with multiple private training
strategies such as class-conditional and per-label private fine-tuning with
DPDM and DP-Promise algorithms. Additionally, we evaluate it on two well-known
document benchmark datasets, RVL-CDIP and Tobacco3482, and show that it can
generate useful and realistic document samples across various document types
and privacy levels ($\varepsilon \in \{1, 5, 10\}$). Lastly, we show that our
approach achieves substantial performance improvements in downstream
evaluations on small-scale datasets, compared to the direct application of
DP-Adam.

</details>


### [23] [Per-element Secure Aggregation against Data Reconstruction Attacks in Federated Learning](https://arxiv.org/abs/2508.04285)
*Takumi Suimon,Yuki Koizumi,Junji Takemasa,Toru Hasegawa*

Main category: cs.CR

TL;DR: 论文提出了一种改进的安全聚合方法，通过限制仅在有足够贡献的索引处揭示聚合值，防止稀疏向量更新中的数据泄露。


<details>
  <summary>Details</summary>
Motivation: 联邦学习中稀疏向量更新可能导致敏感信息泄露，现有安全聚合方法无法完全防范此类攻击。

Method: 提出了一种基于元素掩码的策略，仅在至少有t个非零贡献的索引处揭示聚合值，兼容现有安全聚合协议。

Result: 实验表明，该方法在计算和通信开销上保持可接受范围，有效防御数据重建攻击。

Conclusion: 该方法增强了联邦学习中的隐私保护，同时保持了实用性和兼容性。

Abstract: Federated learning (FL) enables collaborative model training without sharing
raw data, but individual model updates may still leak sensitive information.
Secure aggregation (SecAgg) mitigates this risk by allowing the server to
access only the sum of client updates, thereby concealing individual
contributions. However, a significant vulnerability has recently attracted
increasing attention: when model updates are sparse vectors, a non-zero value
contributed by a single client at a given index can be directly revealed in the
aggregate, enabling precise data reconstruction attacks. In this paper, we
propose a novel enhancement to SecAgg that reveals aggregated values only at
indices with at least $t$ non-zero contributions. Our mechanism introduces a
per-element masking strategy to prevent the exposure of under-contributed
elements, while maintaining modularity and compatibility with many existing
SecAgg implementations by relying solely on cryptographic primitives already
employed in a typical setup. We integrate this mechanism into Flamingo, a
low-round SecAgg protocol, to provide a robust defense against such attacks.
Our analysis and experimental results indicate that the additional
computational and communication overhead introduced by our mechanism remains
within an acceptable range, supporting the practicality of our approach.

</details>


### [24] [Attack Pattern Mining to Discover Hidden Threats to Industrial Control Systems](https://arxiv.org/abs/2508.04561)
*Muhammad Azmi Umer,Chuadhry Mujeeb Ahmed,Aditya Mathur,Muhammad Taha Jilani*

Main category: cs.CR

TL;DR: 验证工业控制系统（ICS）安全中的攻击模式挖掘方法。


<details>
  <summary>Details</summary>
Motivation: 全面评估ICS安全需要生成大量多样的攻击模式。

Method: 提出了一种数据驱动技术，用于生成ICS攻击模式，并在水处理厂数据中生成超过10万条攻击模式。

Result: 通过详细案例研究验证了攻击模式的有效性。

Conclusion: 数据驱动技术能有效生成和验证ICS攻击模式。

Abstract: This work focuses on validation of attack pattern mining in the context of
Industrial Control System (ICS) security. A comprehensive security assessment
of an ICS requires generating a large and variety of attack patterns. For this
purpose we have proposed a data driven technique to generate attack patterns
for an ICS. The proposed technique has been used to generate over 100,000
attack patterns from data gathered from an operational water treatment plant.
In this work we present a detailed case study to validate the attack patterns.

</details>


### [25] [Measuring the Carbon Footprint of Cryptographic Privacy-Enhancing Technologies](https://arxiv.org/abs/2508.04583)
*Marc Damie,Mihai Pop,Merijn Posthuma*

Main category: cs.CR

TL;DR: 本文提出了一种评估隐私增强技术（PETs）碳足迹的标准方法，并测量了五种加密PETs的能耗和碳足迹增加情况，揭示了隐私与碳排放之间的权衡。


<details>
  <summary>Details</summary>
Motivation: 隐私增强技术（PETs）在隐私法规推动下快速发展，但其环境足迹（尤其是碳排放）尚未被充分研究。本文旨在填补这一空白。

Method: 提出了一种标准化方法，评估PETs的碳足迹，并以支持客户端-服务器应用的五种加密PETs为例进行测量。

Result: 发现碳足迹增加幅度差异显著，从HTTPS网页浏览的两倍到加密机器学习的10万倍不等。

Conclusion: 研究为决策者提供了评估隐私与碳排放权衡的关键数据，并指出了未来开发兼顾隐私与环保的PETs的研究方向。

Abstract: Privacy-enhancing technologies (PETs) have attracted significant attention in
response to privacy regulations, driving the development of applications that
prioritize user data protection. At the same time, the information and
communication technology (ICT) sector faces growing pressure to reduce its
environmental footprint, particularly its carbon emissions. While numerous
studies have assessed the energy footprint of various ICT applications, the
environmental footprint of cryptographic PETs remains largely unexplored.
  Our work addresses this gap by proposing a standardized methodology for
evaluating the carbon footprint of PETs. To demonstrate this methodology, we
focus on PETs supporting client-server applications as they are the simplest to
deploy. In particular, we measure the energy consumption and carbon footprint
increase induced by five cryptographic PETs (compared to their non-private
equivalent): HTTPS web browsing, encrypted machine learning (ML) inference,
encrypted ML training, encrypted databases, and encrypted emails. Our findings
reveal significant variability in carbon footprint increases, ranging from a
twofold increase in HTTPS web browsing to a 100,000-fold increase in encrypted
ML.
  Our study provides essential data to help decision-makers assess
privacy-carbon trade-offs in such applications. Finally, we outline key
research directions for developing PETs that balance strong privacy protection
with environmental sustainability.

</details>


### [26] [4-Swap: Achieving Grief-Free and Bribery-Safe Atomic Swaps Using Four Transactions](https://arxiv.org/abs/2508.04641)
*Kirti Singh,Vinay J. Ribeiro,Susmita Mandal*

Main category: cs.CR

TL;DR: 4-Swap是一种新型跨链原子交换协议，仅需四笔交易即可实现无悲痛且防贿赂的资产交换。


<details>
  <summary>Details</summary>
Motivation: 现有跨链资产交换方案存在依赖第三方或悲痛攻击风险，且交易次数较多，4-Swap旨在解决这些问题。

Method: 通过将悲痛溢价和本金合并为每链单笔交易，减少链上交易次数，无需新操作码。

Result: 4-Swap在比特币上完全兼容，交易更快，且博弈论分析显示参与者无偏离协议的动机。

Conclusion: 4-Swap是首个在四笔交易内实现无悲痛且安全的跨链资产交换协议。

Abstract: Cross-chain asset exchange is crucial for blockchain interoperability.
Existing solutions rely on trusted third parties and risk asset loss, or use
decentralized alternatives like atomic swaps, which suffer from grief attacks.
Griefing occurs when a party prematurely exits, locking the counterparty's
assets until a timelock expires. Hedged Atomic Swaps mitigate griefing by
introducing a penalty premium; however, they increase the number of
transactions from four (as in Tier Nolan's swap) to six, which in turn
introduces new griefing risks. Grief-Free (GF) Swap reduces this to five
transactions by consolidating assets and premiums on a single chain. However,
no existing protocol achieves grief-free asset exchange in just four
transactions.
  This paper presents 4-Swap, the first cross-chain atomic swap protocol that
is both grief-free and bribery-safe, while completing asset exchange in just
four transactions. By combining the griefing premium and principal into a
single transaction per chain, 4-Swap reduces on-chain transactions, leading to
faster execution compared to previous grief-free solutions. It is fully
compatible with Bitcoin and operates without the need for any new opcodes. A
game-theoretic analysis shows that rational participants have no incentive to
deviate from the protocol, ensuring robust compliance and security.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [27] [MI9 -- Agent Intelligence Protocol: Runtime Governance for Agentic AI Systems](https://arxiv.org/abs/2508.03858)
*Charles L. Wang,Trisha Singhal,Ameya Kelkar,Jason Tuo*

Main category: cs.AI

TL;DR: MI9是一个专为代理型AI系统设计的运行时治理框架，通过六个组件实时控制风险和确保安全。


<details>
  <summary>Details</summary>
Motivation: 代理型AI系统在运行时表现出不可预测的行为，传统治理方法无法完全应对这些风险。

Method: MI9框架包括代理风险指数、语义遥测捕获、持续授权监控、FSM一致性引擎、目标条件漂移检测和分级遏制策略。

Result: MI9在多样场景中展示了系统性治理能力，填补了现有方法的不足。

Conclusion: MI9为代理型AI的安全部署提供了技术基础，支持大规模应用。

Abstract: Agentic AI systems capable of reasoning, planning, and executing actions
present fundamentally distinct governance challenges compared to traditional AI
models. Unlike conventional AI, these systems exhibit emergent and unexpected
behaviors during runtime, introducing novel agent-related risks that cannot be
fully anticipated through pre-deployment governance alone. To address this
critical gap, we introduce MI9, the first fully integrated runtime governance
framework designed specifically for safety and alignment of agentic AI systems.
MI9 introduces real-time controls through six integrated components:
agency-risk index, agent-semantic telemetry capture, continuous authorization
monitoring, Finite-State-Machine (FSM)-based conformance engines,
goal-conditioned drift detection, and graduated containment strategies.
Operating transparently across heterogeneous agent architectures, MI9 enables
the systematic, safe, and responsible deployment of agentic systems in
production environments where conventional governance approaches fall short,
providing the foundational infrastructure for safe agentic AI deployment at
scale. Detailed analysis through a diverse set of scenarios demonstrates MI9's
systematic coverage of governance challenges that existing approaches fail to
address, establishing the technical foundation for comprehensive agentic AI
oversight.

</details>


### [28] [Evo-MARL: Co-Evolutionary Multi-Agent Reinforcement Learning for Internalized Safety](https://arxiv.org/abs/2508.03864)
*Zhenyu Pan,Yiting Zhang,Yutong Zhang,Jianshu Zhang,Haozheng Luo,Yuwei Han,Dennis Wu,Hong-Yu Chen,Philip S. Yu,Manling Li,Han Liu*

Main category: cs.AI

TL;DR: Evo-MARL 是一个多智能体强化学习框架，通过联合训练任务智能体获得防御能力，避免外部防护模块的依赖和单点故障问题。


<details>
  <summary>Details</summary>
Motivation: 多智能体系统（MAS）在开放性和交互复杂性增加时面临安全风险，现有防御方法依赖外部防护模块，存在单点故障和成本问题。

Method: 提出 Evo-MARL 框架，通过多智能体强化学习（MARL）训练任务智能体同时执行主功能和防御任务，结合进化搜索和参数共享强化学习进行对抗训练。

Result: 实验显示 Evo-MARL 将攻击成功率降低 22%，推理任务准确率提升 5%，安全性和性能同时提高。

Conclusion: Evo-MARL 通过内部化安全机制，实现了安全与效用的协同提升，避免了传统防御方法的局限性。

Abstract: Multi-agent systems (MAS) built on multimodal large language models exhibit
strong collaboration and performance. However, their growing openness and
interaction complexity pose serious risks, notably jailbreak and adversarial
attacks. Existing defenses typically rely on external guard modules, such as
dedicated safety agents, to handle unsafe behaviors. Unfortunately, this
paradigm faces two challenges: (1) standalone agents offer limited protection,
and (2) their independence leads to single-point failure-if compromised,
system-wide safety collapses. Naively increasing the number of guard agents
further raises cost and complexity. To address these challenges, we propose
Evo-MARL, a novel multi-agent reinforcement learning (MARL) framework that
enables all task agents to jointly acquire defensive capabilities. Rather than
relying on external safety modules, Evo-MARL trains each agent to
simultaneously perform its primary function and resist adversarial threats,
ensuring robustness without increasing system overhead or single-node failure.
Furthermore, Evo-MARL integrates evolutionary search with parameter-sharing
reinforcement learning to co-evolve attackers and defenders. This adversarial
training paradigm internalizes safety mechanisms and continually enhances MAS
performance under co-evolving threats. Experiments show that Evo-MARL reduces
attack success rates by up to 22% while boosting accuracy by up to 5% on
reasoning tasks-demonstrating that safety and utility can be jointly improved.

</details>


### [29] [MOTIF: Multi-strategy Optimization via Turn-based Interactive Framework](https://arxiv.org/abs/2508.03929)
*Nguyen Viet Tuan Kiet,Dao Van Tung,Tran Cong Dao,Huynh Thi Thanh Binh*

Main category: cs.AI

TL;DR: MOTIF框架通过多策略优化和基于蒙特卡洛树搜索的交互式框架，联合优化多个组件，显著提升组合优化问题的求解性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常仅优化单一组件（如启发式评分函数），忽视了多组件协同优化的潜力，限制了创新空间。

Method: 提出MOTIF框架，利用蒙特卡洛树搜索实现两个LLM代理的轮换优化，通过竞争与合作促进多样高性能解。

Result: 在多个COP领域的实验中，MOTIF表现优于现有最优方法。

Conclusion: MOTIF展示了轮换多代理提示在自动化求解器设计中的潜力。

Abstract: Designing effective algorithmic components remains a fundamental obstacle in
tackling NP-hard combinatorial optimization problems (COPs), where solvers
often rely on carefully hand-crafted strategies. Despite recent advances in
using large language models (LLMs) to synthesize high-quality components, most
approaches restrict the search to a single element - commonly a heuristic
scoring function - thus missing broader opportunities for innovation. In this
paper, we introduce a broader formulation of solver design as a multi-strategy
optimization problem, which seeks to jointly improve a set of interdependent
components under a unified objective. To address this, we propose
Multi-strategy Optimization via Turn-based Interactive Framework (MOTIF) - a
novel framework based on Monte Carlo Tree Search that facilitates turn-based
optimization between two LLM agents. At each turn, an agent improves one
component by leveraging the history of both its own and its opponent's prior
updates, promoting both competitive pressure and emergent cooperation. This
structured interaction broadens the search landscape and encourages the
discovery of diverse, high-performing solutions. Experiments across multiple
COP domains show that MOTIF consistently outperforms state-of-the-art methods,
highlighting the promise of turn-based, multi-agent prompting for fully
automated solver design.

</details>


### [30] [Can Large Language Models Adequately Perform Symbolic Reasoning Over Time Series?](https://arxiv.org/abs/2508.03963)
*Zewen Liu,Juntong Ni,Xianfeng Tang,Max S. Y. Lau,Wei Jin*

Main category: cs.AI

TL;DR: SymbolBench是一个评估大型语言模型（LLMs）在时间序列数据中推断符号结构能力的基准，涵盖多变量符号回归、布尔网络推断和因果发现三个任务。研究提出了一种结合LLMs和遗传编程的统一框架，揭示了当前模型的优缺点。


<details>
  <summary>Details</summary>
Motivation: 探索LLMs在时间序列数据中推断可解释符号结构的能力，填补现有研究在复杂符号形式上的空白。

Method: 引入SymbolBench基准，提出结合LLMs和遗传编程的统一框架，LLMs作为预测器和评估器。

Result: 实证结果揭示了当前模型的优缺点，强调了领域知识、上下文对齐和推理结构的重要性。

Conclusion: 结合领域知识和结构化推理可提升LLMs在自动科学发现中的表现。

Abstract: Uncovering hidden symbolic laws from time series data, as an aspiration
dating back to Kepler's discovery of planetary motion, remains a core challenge
in scientific discovery and artificial intelligence. While Large Language
Models show promise in structured reasoning tasks, their ability to infer
interpretable, context-aligned symbolic structures from time series data is
still underexplored. To systematically evaluate this capability, we introduce
SymbolBench, a comprehensive benchmark designed to assess symbolic reasoning
over real-world time series across three tasks: multivariate symbolic
regression, Boolean network inference, and causal discovery. Unlike prior
efforts limited to simple algebraic equations, SymbolBench spans a diverse set
of symbolic forms with varying complexity. We further propose a unified
framework that integrates LLMs with genetic programming to form a closed-loop
symbolic reasoning system, where LLMs act both as predictors and evaluators.
Our empirical results reveal key strengths and limitations of current models,
highlighting the importance of combining domain knowledge, context alignment,
and reasoning structure to improve LLMs in automated scientific discovery.

</details>


### [31] [The Emotional Baby Is Truly Deadly: Does your Multimodal Large Reasoning Model Have Emotional Flattery towards Humans?](https://arxiv.org/abs/2508.03986)
*Yuan Xun,Xiaojun Jia,Xinwei Liu,Hua Zhang*

Main category: cs.AI

TL;DR: EmoAgent是一个通过情感提示操控推理路径的对抗性框架，揭示了MLRMs在高情感强度下可能忽视安全协议的问题，并提出了三种量化风险的指标。


<details>
  <summary>Details</summary>
Motivation: 研究发现面向人类服务的MLRMs在深度思考阶段易受用户情感线索影响，可能绕过安全协议，因此需要量化这种风险。

Method: 提出EmoAgent框架，通过情感提示操控模型推理路径，并设计RRSS、RVNR和RAIC三种指标量化风险。

Result: 实验表明EmoAgent有效，揭示了模型在情感认知与安全行为之间的不一致性。

Conclusion: 研究强调了情感对MLRMs安全性的影响，需进一步优化模型以应对情感操控风险。

Abstract: We observe that MLRMs oriented toward human-centric service are highly
susceptible to user emotional cues during the deep-thinking stage, often
overriding safety protocols or built-in safety checks under high emotional
intensity. Inspired by this key insight, we propose EmoAgent, an autonomous
adversarial emotion-agent framework that orchestrates exaggerated affective
prompts to hijack reasoning pathways. Even when visual risks are correctly
identified, models can still produce harmful completions through emotional
misalignment. We further identify persistent high-risk failure modes in
transparent deep-thinking scenarios, such as MLRMs generating harmful reasoning
masked behind seemingly safe responses. These failures expose misalignments
between internal inference and surface-level behavior, eluding existing
content-based safeguards. To quantify these risks, we introduce three metrics:
(1) Risk-Reasoning Stealth Score (RRSS) for harmful reasoning beneath benign
outputs; (2) Risk-Visual Neglect Rate (RVNR) for unsafe completions despite
visual risk recognition; and (3) Refusal Attitude Inconsistency (RAIC) for
evaluating refusal unstability under prompt variants. Extensive experiments on
advanced MLRMs demonstrate the effectiveness of EmoAgent and reveal deeper
emotional cognitive misalignments in model safety behavior.

</details>


### [32] [Galaxy: A Cognition-Centered Framework for Proactive, Privacy-Preserving, and Self-Evolving LLM Agents](https://arxiv.org/abs/2508.03991)
*Chongyu Bao,Ruimin Dai,Yangbo Shen,Runyang Jian,Jinghan Zhang,Xiaolan Liu,Kunpeng Liu*

Main category: cs.AI

TL;DR: 论文提出Cognition Forest和Galaxy框架，通过统一认知架构与系统设计，实现智能个人助手的主动行为、隐私保护和自我进化。


<details>
  <summary>Details</summary>
Motivation: 现有智能个人助手（IPA）的主动行为研究不足，设计兼具主动性、隐私保护和自我进化能力的IPA仍具挑战。

Method: 提出Cognition Forest语义结构，统一认知建模与系统设计，并基于此开发Galaxy框架，支持多维交互和个性化能力生成。

Result: 实验表明Galaxy优于多个先进基准，KoRa和Kernel代理验证了其有效性。

Conclusion: Galaxy框架为IPA的主动行为和自我进化提供了可行解决方案。

Abstract: Intelligent personal assistants (IPAs) such as Siri and Google Assistant are
designed to enhance human capabilities and perform tasks on behalf of users.
The emergence of LLM agents brings new opportunities for the development of
IPAs. While responsive capabilities have been widely studied, proactive
behaviors remain underexplored. Designing an IPA that is proactive,
privacy-preserving, and capable of self-evolution remains a significant
challenge. Designing such IPAs relies on the cognitive architecture of LLM
agents. This work proposes Cognition Forest, a semantic structure designed to
align cognitive modeling with system-level design. We unify cognitive
architecture and system design into a self-reinforcing loop instead of treating
them separately. Based on this principle, we present Galaxy, a framework that
supports multidimensional interactions and personalized capability generation.
Two cooperative agents are implemented based on Galaxy: KoRa, a
cognition-enhanced generative agent that supports both responsive and proactive
skills; and Kernel, a meta-cognition-based meta-agent that enables Galaxy's
self-evolution and privacy preservation. Experimental results show that Galaxy
outperforms multiple state-of-the-art benchmarks. Ablation studies and
real-world interaction cases validate the effectiveness of Galaxy.

</details>


### [33] [Uncertainty-Aware GUI Agent: Adaptive Perception through Component Recommendation and Human-in-the-Loop Refinement](https://arxiv.org/abs/2508.04025)
*Chao Hao,Shuai Wang,Kaiwen Zhou*

Main category: cs.AI

TL;DR: RecAgent是一个不确定性感知的GUI代理，通过自适应感知减少输入冗余和决策模糊性，提出组件推荐和交互模块，并在ComplexAction数据集上验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 解决GUI代理在移动任务中面临的输入冗余和决策模糊性问题。

Method: 采用组件推荐机制减少感知不确定性，通过交互模块处理决策不确定性，并整合为统一框架。

Result: 实验验证了方法的有效性，并提出了ComplexAction数据集用于评估。

Conclusion: RecAgent通过自适应感知和用户反馈显著提升了GUI代理的性能。

Abstract: Graphical user interface (GUI) agents have shown promise in automating mobile
tasks but still struggle with input redundancy and decision ambiguity. In this
paper, we present \textbf{RecAgent}, an uncertainty-aware agent that addresses
these issues through adaptive perception. We distinguish two types of
uncertainty in GUI navigation: (1) perceptual uncertainty, caused by input
redundancy and noise from comprehensive screen information, and (2) decision
uncertainty, arising from ambiguous tasks and complex reasoning. To reduce
perceptual uncertainty, RecAgent employs a component recommendation mechanism
that identifies and focuses on the most relevant UI elements. For decision
uncertainty, it uses an interactive module to request user feedback in
ambiguous situations, enabling intent-aware decisions. These components are
integrated into a unified framework that proactively reduces input complexity
and reacts to high-uncertainty cases via human-in-the-loop refinement.
Additionally, we propose a dataset called \textbf{ComplexAction} to evaluate
the success rate of GUI agents in executing specified single-step actions
within complex scenarios. Extensive experiments validate the effectiveness of
our approach. The dataset and code will be available at
https://github.com/Fanye12/RecAgent.

</details>


### [34] [SEA: Self-Evolution Agent with Step-wise Reward for Computer Use](https://arxiv.org/abs/2508.04037)
*Liang Tang,Shuxian Li,Yuhao Cheng,Yukang Huo,Zhepeng Wang,Yiqiang Yan,Kaer Huang,Yanzhe Jing,Tiaonan Duan*

Main category: cs.AI

TL;DR: 提出了一种名为SEA的计算机使用自进化代理，通过创新的数据生成、强化学习和模型增强方法，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 当前计算机使用代理的性能远未达到实用水平，亟需改进。

Method: 采用自动生成可验证轨迹、高效分步强化学习和无额外训练的模型增强方法。

Result: SEA仅需7B参数，性能优于同规模模型，媲美更大模型。

Conclusion: SEA为计算机使用代理提供了高效解决方案，未来将开源模型和代码。

Abstract: Computer use agent is an emerging area in artificial intelligence that aims
to operate the computers to achieve the user's tasks, which attracts a lot of
attention from both industry and academia. However, the present agents'
performance is far from being used. In this paper, we propose the
Self-Evolution Agent (SEA) for computer use, and to develop this agent, we
propose creative methods in data generation, reinforcement learning, and model
enhancement. Specifically, we first propose an automatic pipeline to generate
the verifiable trajectory for training. And then, we propose efficient
step-wise reinforcement learning to alleviate the significant computational
requirements for long-horizon training. In the end, we propose the enhancement
method to merge the grounding and planning ability into one model without any
extra training. Accordingly, based on our proposed innovation of data
generation, training strategy, and enhancement, we get the Selfevolution Agent
(SEA) for computer use with only 7B parameters, which outperforms models with
the same number of parameters and has comparable performance to larger ones. We
will make the models' weight and related codes open-source in the future.

</details>


### [35] [Personalized Knowledge Transfer Through Generative AI: Contextualizing Learning to Individual Career Goals](https://arxiv.org/abs/2508.04070)
*Ronja Mehlan,Claudia Hess,Quintus Stierstorfer,Kristina Schaaff*

Main category: cs.AI

TL;DR: 研究探讨了基于职业目标的生成式AI个性化学习内容对学习者的影响，发现其能提升参与度、满意度和学习效率。


<details>
  <summary>Details</summary>
Motivation: 探索AI个性化学习内容如何通过职业目标适配增强学习者的长期动机和参与度。

Method: 混合方法实验，涉及4000多名学习者，分为职业目标适配组和对照组。

Result: 定量结果显示会话时长增加、满意度提高、学习时长略有减少；定性分析表明学习者认为内容更具激励性和实用性。

Conclusion: 职业目标适配的AI个性化学习内容能有效连接学术知识与职场应用，具有推广价值。

Abstract: As artificial intelligence becomes increasingly integrated into digital
learning environments, the personalization of learning content to reflect
learners' individual career goals offers promising potential to enhance
engagement and long-term motivation. In our study, we investigate how career
goal-based content adaptation in learning systems based on generative AI
(GenAI) influences learner engagement, satisfaction, and study efficiency. The
mixed-methods experiment involved more than 4,000 learners, with one group
receiving learning scenarios tailored to their career goals and a control
group. Quantitative results show increased session duration, higher
satisfaction ratings, and a modest reduction in study duration compared to
standard content. Qualitative analysis highlights that learners found the
personalized material motivating and practical, enabling deep cognitive
engagement and strong identification with the content. These findings
underscore the value of aligning educational content with learners' career
goals and suggest that scalable AI personalization can bridge academic
knowledge and workplace applicability.

</details>


### [36] [KG-Augmented Executable CoT for Mathematical Coding](https://arxiv.org/abs/2508.04072)
*Xingyu Chen,Junxiu An,Jun Guo,Li Wang,Jingcai Guo*

Main category: cs.AI

TL;DR: KGA-ECoT框架通过知识图谱和可执行代码提升复杂推理任务（如数学推理和代码生成）的性能，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在复杂推理任务中表现不佳，需要改进数学推理和代码生成能力。

Method: 提出KGA-ECoT框架，利用知识图谱（GraphRAG）检索数学库知识，生成可验证代码，并通过结构化任务图分解问题。

Result: 在多个数学推理基准测试中，KGA-ECoT显著优于现有方法，准确率提升数个百分点至超过十个百分点。

Conclusion: KGA-ECoT是一个强大且通用的框架，特别适用于复杂数学推理任务。

Abstract: In recent years, large language models (LLMs) have excelled in natural
language processing tasks but face significant challenges in complex reasoning
tasks such as mathematical reasoning and code generation. To address these
limitations, we propose KG-Augmented Executable Chain-of-Thought (KGA-ECoT), a
novel framework that enhances code generation through knowledge graphs and
improves mathematical reasoning via executable code. KGA-ECoT decomposes
problems into a Structured Task Graph, leverages efficient GraphRAG for precise
knowledge retrieval from mathematical libraries, and generates verifiable code
to ensure computational accuracy. Evaluations on multiple mathematical
reasoning benchmarks demonstrate that KGA-ECoT significantly outperforms
existing prompting methods, achieving absolute accuracy improvements ranging
from several to over ten percentage points. Further analysis confirms the
critical roles of GraphRAG in enhancing code quality and external code
execution in ensuring precision. These findings collectively establish KGA-ECoT
as a robust and highly generalizable framework for complex mathematical
reasoning tasks.

</details>


### [37] [LLM Collaboration With Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2508.04652)
*Shuo Liu,Zeyu Liang,Xueguang Lyu,Christopher Amato*

Main category: cs.AI

TL;DR: 论文提出了一种多智能体强化学习框架MAGRPO，用于优化大型语言模型（LLM）的协作能力，解决了现有方法依赖复杂个体奖励设计的问题。


<details>
  <summary>Details</summary>
Motivation: 现有LLM预训练独立进行，未针对协作优化，且现有微调框架依赖复杂的个体奖励设计，难以有效促进多智能体协作。

Method: 将LLM协作建模为合作多智能体强化学习（MARL）问题，提出多智能体、多轮算法MAGRPO，结合现有RL和MARL技术。

Result: 实验表明，MAGRPO能有效提升LLM在写作和编程协作任务中的响应质量与效率。

Conclusion: MAGRPO为LLM协作提供了新思路，并展示了将其他MARL方法应用于LLM的潜力与挑战。

Abstract: A large amount of work has been done in Multi-Agent Systems (MAS) for
modeling and solving problems with multiple interacting agents. However, most
LLMs are pretrained independently and not specifically optimized for
coordination. Existing LLM fine-tuning frameworks rely on individual rewards,
which require complex reward designs for each agent to encourage collaboration.
To address these challenges, we model LLM collaboration as a cooperative
Multi-Agent Reinforcement Learning (MARL) problem. We develop a multi-agent,
multi-turn algorithm, Multi-Agent Group Relative Policy Optimization (MAGRPO),
to solve it, building on current RL approaches for LLMs as well as MARL
techniques. Our experiments on LLM writing and coding collaboration demonstrate
that fine-tuning MAS with MAGRPO enables agents to generate high-quality
responses efficiently through effective cooperation. Our approach opens the
door to using other MARL methods for LLMs and highlights the associated
challenges.

</details>


### [38] [GeoSR: Cognitive-Agentic Framework for Probing Geospatial Knowledge Boundaries via Iterative Self-Refinement](https://arxiv.org/abs/2508.04080)
*Jinfan Tang,Kunming Wu,Ruifeng Gongxie,Yuya He,Yuankai Wu*

Main category: cs.AI

TL;DR: GeoSR是一个自优化的代理推理框架，通过嵌入地理原则（如Tobler第一地理定律）来提升大语言模型（LLMs）在空间一致性、多跳推理和地理偏差方面的表现。


<details>
  <summary>Details</summary>
Motivation: 解决LLMs在空间一致性、多跳推理和地理偏差方面的挑战。

Method: 提出GeoSR框架，包含三个协作代理：变量选择代理、点选择代理和优化代理，通过迭代预测循环提升预测质量。

Result: 实验表明，GeoSR在物理世界属性估计和社会经济预测任务中表现优于标准提示策略。

Conclusion: GeoSR通过整合地理统计先验和空间结构化推理，显著提升了LLMs的地理空间预测准确性和公平性。

Abstract: Recent studies have extended the application of large language models (LLMs)
to geographic problems, revealing surprising geospatial competence even without
explicit spatial supervision. However, LLMs still face challenges in spatial
consistency, multi-hop reasoning, and geographic bias. To address these issues,
we propose GeoSR, a self-refining agentic reasoning framework that embeds core
geographic principles -- most notably Tobler's First Law of Geography -- into
an iterative prediction loop. In GeoSR, the reasoning process is decomposed
into three collaborating agents: (1) a variable-selection agent that selects
relevant covariates from the same location; (2) a point-selection agent that
chooses reference predictions at nearby locations generated by the LLM in
previous rounds; and (3) a refine agent that coordinates the iterative
refinement process by evaluating prediction quality and triggering further
rounds when necessary. This agentic loop progressively improves prediction
quality by leveraging both spatial dependencies and inter-variable
relationships. We validate GeoSR on tasks ranging from physical-world property
estimation to socioeconomic prediction. Experimental results show consistent
improvements over standard prompting strategies, demonstrating that
incorporating geostatistical priors and spatially structured reasoning into
LLMs leads to more accurate and equitable geospatial predictions. The code of
GeoSR is available at https://github.com/JinfanTang/GeoSR.

</details>


### [39] [Towards Transparent AI Grading: Semantic Entropy as a Signal for Human-AI Disagreement](https://arxiv.org/abs/2508.04105)
*Karrtik Iyer,Manikandan Ravikiran,Prasanna Pendse,Shayan Mohanty*

Main category: cs.AI

TL;DR: 论文提出了一种名为“语义熵”的度量方法，用于量化AI评分系统的不确定性，并通过实验验证其与人工评分分歧的相关性。


<details>
  <summary>Details</summary>
Motivation: 现有自动评分系统无法有效反映评分决策的不确定性或争议性，需要一种透明且可信的度量方法。

Method: 通过GPT-4生成多个解释，基于蕴含相似性聚类，并计算这些聚类的熵值，量化评分理由的多样性。

Result: 实验表明，语义熵与人工评分分歧相关，能跨学科泛化，并对任务结构特征敏感。

Conclusion: 语义熵是一种可解释的不确定性信号，有助于提升AI辅助评分的透明度和可信度。

Abstract: Automated grading systems can efficiently score short-answer responses, yet
they often fail to indicate when a grading decision is uncertain or potentially
contentious. We introduce semantic entropy, a measure of variability across
multiple GPT-4-generated explanations for the same student response, as a proxy
for human grader disagreement. By clustering rationales via entailment-based
similarity and computing entropy over these clusters, we quantify the diversity
of justifications without relying on final output scores. We address three
research questions: (1) Does semantic entropy align with human grader
disagreement? (2) Does it generalize across academic subjects? (3) Is it
sensitive to structural task features such as source dependency? Experiments on
the ASAP-SAS dataset show that semantic entropy correlates with rater
disagreement, varies meaningfully across subjects, and increases in tasks
requiring interpretive reasoning. Our findings position semantic entropy as an
interpretable uncertainty signal that supports more transparent and trustworthy
AI-assisted grading workflows.

</details>


### [40] [A Compositional Framework for On-the-Fly LTLf Synthesis](https://arxiv.org/abs/2508.04116)
*Yongkang Li,Shengping Xiao,Shufang Zhu,Jianwen Li,Geguang Pu*

Main category: cs.AI

TL;DR: 提出了一种组合式即时合成框架，结合了DFA构建和游戏求解的优势，专注于处理实践中常见的大型LTLf公式合取。


<details>
  <summary>Details</summary>
Motivation: 现有技术在DFA构建和游戏求解之间缺乏主导方法，无法高效处理大型LTLf公式合取。

Method: 引入组合式即时合成框架，在游戏求解过程中应用组合而非DFA构建，支持两种组合变体（组合前修剪和组合中修剪）。

Result: 与现有合成求解器相比，该框架能解决更多其他求解器无法处理的实例。

Conclusion: 两种组合变体各有优势，框架在简化后续组合和早期检测不可实现性方面表现突出。

Abstract: Reactive synthesis from Linear Temporal Logic over finite traces (LTLf) can
be reduced to a two-player game over a Deterministic Finite Automaton (DFA) of
the LTLf specification. The primary challenge here is DFA construction, which
is 2EXPTIME-complete in the worst case. Existing techniques either construct
the DFA compositionally before solving the game, leveraging automata
minimization to mitigate state-space explosion, or build the DFA incrementally
during game solving to avoid full DFA construction. However, neither is
dominant. In this paper, we introduce a compositional on-the-fly synthesis
framework that integrates the strengths of both approaches, focusing on large
conjunctions of smaller LTLf formulas common in practice. This framework
applies composition during game solving instead of automata (game arena)
construction. While composing all intermediate results may be necessary in the
worst case, pruning these results simplifies subsequent compositions and
enables early detection of unrealizability. Specifically, the framework allows
two composition variants: pruning before composition to take full advantage of
minimization or pruning during composition to guide on-the-fly synthesis.
Compared to state-of-the-art synthesis solvers, our framework is able to solve
a notable number of instances that other solvers cannot handle. A detailed
analysis shows that both composition variants have unique merits.

</details>


### [41] [AgREE: Agentic Reasoning for Knowledge Graph Completion on Emerging Entities](https://arxiv.org/abs/2508.04118)
*Ruochen Zhao,Simone Conia,Eric Peng,Min Li,Saloni Potdar*

Main category: cs.AI

TL;DR: AgREE框架通过结合迭代检索和多步推理，动态构建知识图谱三元组，显著优于现有方法，尤其在新兴实体上表现突出。


<details>
  <summary>Details</summary>
Motivation: 解决开放域知识图谱补全中新兴实体信息不足的问题，现有方法依赖预训练模型或单步检索，难以捕捉最新信息。

Method: 提出AgREE框架，结合代理推理和多步检索，动态构建知识图谱三元组，无需训练数据。

Result: AgREE在新兴实体上表现优于现有方法13.7%，并提出了新的评估方法和基准。

Conclusion: 代理推理与信息检索结合能有效维护动态知识图谱的时效性。

Abstract: Open-domain Knowledge Graph Completion (KGC) faces significant challenges in
an ever-changing world, especially when considering the continual emergence of
new entities in daily news. Existing approaches for KGC mainly rely on
pretrained language models' parametric knowledge, pre-constructed queries, or
single-step retrieval, typically requiring substantial supervision and training
data. Even so, they often fail to capture comprehensive and up-to-date
information about unpopular and/or emerging entities. To this end, we introduce
Agentic Reasoning for Emerging Entities (AgREE), a novel agent-based framework
that combines iterative retrieval actions and multi-step reasoning to
dynamically construct rich knowledge graph triplets. Experiments show that,
despite requiring zero training efforts, AgREE significantly outperforms
existing methods in constructing knowledge graph triplets, especially for
emerging entities that were not seen during language models' training
processes, outperforming previous methods by up to 13.7%. Moreover, we propose
a new evaluation methodology that addresses a fundamental weakness of existing
setups and a new benchmark for KGC on emerging entities. Our work demonstrates
the effectiveness of combining agent-based reasoning with strategic information
retrieval for maintaining up-to-date knowledge graphs in dynamic information
environments.

</details>


### [42] [Generic-to-Specific Reasoning and Learning for Scalable Ad Hoc Teamwork](https://arxiv.org/abs/2508.04163)
*Hasra Dodampegama,Mohan Sridharan*

Main category: cs.AI

TL;DR: 论文提出了一种结合知识驱动和数据驱动的方法，用于提升AI代理在临时团队协作中的决策能力。


<details>
  <summary>Details</summary>
Motivation: 现有临时团队协作方法依赖大量标注数据，缺乏透明性且难以快速更新知识，随着代理数量增加，决策复杂度上升。

Method: 采用非单调逻辑推理，结合领域常识知识、快速学习的行为预测模型和基于基础模型的抽象目标预测。

Result: 在VirtualHome仿真环境中验证了架构的有效性。

Conclusion: 结合知识驱动和数据驱动的方法能有效提升临时团队协作的决策能力。

Abstract: AI agents deployed in assistive roles often have to collaborate with other
agents (humans, AI systems) without prior coordination. Methods considered
state of the art for such ad hoc teamwork often pursue a data-driven approach
that needs a large labeled dataset of prior observations, lacks transparency,
and makes it difficult to rapidly revise existing knowledge in response to
changes. As the number of agents increases, the complexity of decision-making
makes it difficult to collaborate effectively. This paper advocates leveraging
the complementary strengths of knowledge-based and data-driven methods for
reasoning and learning for ad hoc teamwork. For any given goal, our
architecture enables each ad hoc agent to determine its actions through
non-monotonic logical reasoning with: (a) prior commonsense domain-specific
knowledge; (b) models learned and revised rapidly to predict the behavior of
other agents; and (c) anticipated abstract future goals based on generic
knowledge of similar situations in an existing foundation model. We
experimentally evaluate our architecture's capabilities in VirtualHome, a
realistic physics-based 3D simulation environment.

</details>


### [43] [Circuit-Aware SAT Solving: Guiding CDCL via Conditional Probabilities](https://arxiv.org/abs/2508.04235)
*Jiaying Zhu,Ziyang Zheng,Zhengyuan Shi,Yalun Cai,Qiang Xu*

Main category: cs.AI

TL;DR: CASCAD是一种新型电路感知SAT求解框架，利用GNN计算的门级条件概率动态指导CDCL启发式，显著提升求解效率。


<details>
  <summary>Details</summary>
Motivation: 传统方法将电路转换为CNF并丢弃结构信息，导致求解性能不佳。

Method: CASCAD直接利用GNN计算的门级条件概率，动态指导变量相位选择和子句管理。

Result: 在真实LEC基准测试中，CASCAD将求解时间减少10倍，并通过概率引导的子句过滤策略额外减少23.5%运行时间。

Conclusion: 保留电路结构信息对提升SAT求解效率和EDA工具设计至关重要。

Abstract: Circuit Satisfiability (CSAT) plays a pivotal role in Electronic Design
Automation. The standard workflow for solving CSAT problems converts circuits
into Conjunctive Normal Form (CNF) and employs generic SAT solvers powered by
Conflict-Driven Clause Learning (CDCL). However, this process inherently
discards rich structural and functional information, leading to suboptimal
solver performance. To address this limitation, we introduce CASCAD, a novel
circuit-aware SAT solving framework that directly leverages circuit-level
conditional probabilities computed via Graph Neural Networks (GNNs). By
explicitly modeling gate-level conditional probabilities, CASCAD dynamically
guides two critical CDCL heuristics -- variable phase selection and clause
managementto significantly enhance solver efficiency. Extensive evaluations on
challenging real-world Logical Equivalence Checking (LEC) benchmarks
demonstrate that CASCAD reduces solving times by up to 10x compared to
state-of-the-art CNF-based approaches, achieving an additional 23.5% runtime
reduction via our probability-guided clause filtering strategy. Our results
underscore the importance of preserving circuit-level structural insights
within SAT solvers, providing a robust foundation for future improvements in
SAT-solving efficiency and EDA tool design.

</details>


### [44] [Large Language Model's Multi-Capability Alignment in Biomedical Domain](https://arxiv.org/abs/2508.04278)
*Wentao Wu,Linqing Chen,Hanmeng Zhong,Weilei Wang*

Main category: cs.AI

TL;DR: BalancedBio 是一个理论基础的参数高效生物医学推理框架，通过正交梯度空间防止能力干扰，实现多能力集成。


<details>
  <summary>Details</summary>
Motivation: 解决生物医学领域 AI 对齐中的多能力集成问题，确保安全部署。

Method: 包括医学知识基础的合成生成（MKGSG）和能力感知组相对策略优化，结合规则和模型奖励。

Result: 在多个指标上显著提升（如 BIOMED-MMLU 80.95%），并实现成本降低和诊断准确性提高。

Conclusion: 提供了一种生物医学 AI 对齐的原则性方法，确保安全性和可靠性。

Abstract: BalancedBio is a theoretically grounded framework for parameter-efficient
biomedical reasoning, addressing multi-capability integration in
domain-specific AI alignment. It establishes the Biomedical Multi-Capability
Convergence Theorem, proving orthogonal gradient spaces are essential to
prevent capability interference for safe deployment. Key innovations include:
(1) Medical Knowledge Grounded Synthetic Generation (MKGSG), extending
Source2Synth with clinical workflow constraints and medical ontology validation
for factual accuracy and safety; and (2) Capability Aware Group Relative Policy
Optimization, deriving optimal hybrid reward weighting to maintain
orthogonality in RL, using a reward model with rule-based and model-based
scores adapted to biomedical tasks. Mathematical analysis proves Pareto-optimal
convergence, preserving performance across capabilities. It achieves
state-of-the-art results in its parameter class: domain expertise (80.95%
BIOMED-MMLU, +15.32% over baseline), reasoning (61.94%, +7.75%), instruction
following (67.95%, +6.44%), and integration (86.7%, +18.5%). Theoretical safety
guarantees include bounds on capability preservation and clinical accuracy.
Real-world deployment yields 78% cost reduction, 23% improved diagnostic
accuracy, and 89% clinician acceptance. This work provides a principled
methodology for biomedical AI alignment, enabling efficient reasoning with
essential safety and reliability, with the 0.5B model version to be released.

</details>


### [45] [Synthetic POMDPs to Challenge Memory-Augmented RL: Memory Demand Structure Modeling](https://arxiv.org/abs/2508.04282)
*Yongyi Wang,Lingfeng Li,Bozhou Chen,Ang Li,Hanyu Liu,Qirui Zheng,Xionghui Yang,Wenxin Li*

Main category: cs.AI

TL;DR: 论文提出了一个理论框架和方法论，用于合成具有可控难度的POMDP环境，以评估记忆增强RL算法。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试缺乏对记忆模型挑战程度的可控性，而合成环境可以精细调控动态特性，为记忆增强RL的详细评估提供支持。

Method: 1. 基于MDS和过渡不变性等概念的理论框架；2. 利用线性过程动态、状态聚合和奖励重新分配的方法论；3. 设计并验证了一系列难度递增的POMDP环境。

Result: 研究阐明了记忆增强RL在解决POMDP中的挑战，为环境设计和分析提供了指导，并为RL任务中记忆模型的选择提供了实证支持。

Conclusion: 该工作为记忆增强RL的评估提供了可控的合成环境，并推动了相关理论和实践的发展。

Abstract: Recent research has developed benchmarks for memory-augmented reinforcement
learning (RL) algorithms, providing Partially Observable Markov Decision
Process (POMDP) environments where agents depend on past observations to make
decisions. While many benchmarks incorporate sufficiently complex real-world
problems, they lack controllability over the degree of challenges posed to
memory models. In contrast, synthetic environments enable fine-grained
manipulation of dynamics, making them critical for detailed and rigorous
evaluation of memory-augmented RL. Our study focuses on POMDP synthesis with
three key contributions:
  1. A theoretical framework for analyzing POMDPs, grounded in Memory Demand
Structure (MDS), transition invariance, and related concepts; 2. A methodology
leveraging linear process dynamics, state aggregation, and reward
redistribution to construct customized POMDPs with predefined properties; 3.
Empirically validated series of POMDP environments with increasing difficulty
levels, designed based on our theoretical insights. Our work clarifies the
challenges of memory-augmented RL in solving POMDPs, provides guidelines for
analyzing and designing POMDP environments, and offers empirical support for
selecting memory models in RL tasks.

</details>


### [46] [Deliberative Reasoning Network: An Uncertainty-Driven Paradigm for Belief-Tracked Inference with Pretrained Language Models](https://arxiv.org/abs/2508.04339)
*Anran Xu,Jincheng Wang,Baigen Cai,Tao Wen*

Main category: cs.AI

TL;DR: 论文提出了一种名为DRN的新方法，通过将逻辑推理从概率最大化转为不确定性最小化，解决了大语言模型在逻辑推理中的认知陷阱问题。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在逻辑推理中容易因语义启发式与决定性证据冲突而失败，即认知陷阱。DRN旨在解决这一根本限制。

Method: DRN通过迭代证据合成过程，显式追踪信念状态并量化竞争假设的认知不确定性，实现了内在可解释性。验证包括两种架构：定制判别模型和轻量验证模块。

Result: 在LCR-1000基准测试中，DRN比基线提升15.2%；与Mistral-7B集成后，在最具挑战性问题上的准确率从20%提升至80%。零样本泛化能力显著。

Conclusion: DRN作为一种可验证的System 2推理组件，为构建更可信的AI系统提供了基础。

Abstract: Large language models often fail at logical reasoning when semantic
heuristics conflict with decisive evidence - a phenomenon we term cognitive
traps. To address this fundamental limitation, we introduce the Deliberative
Reasoning Network (DRN), a novel paradigm that reframes logical reasoning from
probability maximization to uncertainty minimization. Instead of asking "Which
answer is most likely?", DRN asks "Which hypothesis has the most internally
consistent evidence?". DRN achieves intrinsic interpretability by explicitly
tracking belief states and quantifying epistemic uncertainty for competing
hypotheses through an iterative evidence synthesis process. We validate our
approach through two complementary architectures - a bespoke discriminative
model that embodies the core uncertainty minimization principle, and a
lightweight verification module that enhances existing generative LLMs.
Evaluated on LCR-1000, our new adversarial reasoning benchmark designed to
expose cognitive traps, the bespoke DRN achieves up to 15.2% improvement over
standard baselines. When integrated as a parameter-efficient verifier with
Mistral-7B, our hybrid system boosts accuracy from 20% to 80% on the most
challenging problems. Critically, DRN demonstrates strong zero-shot
generalization, improving TruthfulQA performance by 23.6% without additional
training, indicating that uncertainty-driven deliberation learns transferable
reasoning principles. We position DRN as a foundational, verifiable System 2
reasoning component for building more trustworthy AI systems.

</details>


### [47] [OmniPlay: Benchmarking Omni-Modal Models on Omni-Modal Game Playing](https://arxiv.org/abs/2508.04361)
*Fuqing Bie,Shiyu Huang,Xijia Tao,Zhiqin Fang,Leyi Pan,Junzhe Chen,Min Ren,Liuyu Xiang,Zhaofeng He*

Main category: cs.AI

TL;DR: OmniPlay是一个多模态基准测试，旨在评估动态交互世界中智能体的跨模态推理能力，发现现有模型在高保真记忆任务中表现优异，但在需要推理和规划的任务中存在系统性缺陷。


<details>
  <summary>Details</summary>
Motivation: 现有评估方法无法全面测试多模态模型在动态交互环境中的智能，尤其是忽略了听觉和时间线索。

Method: 开发了OmniPlay基准测试，包含五种游戏环境，系统化生成协同和冲突场景，测试模型的跨模态推理能力。

Result: 评估显示模型在高保真记忆任务中表现超人类，但在推理和规划任务中表现脆弱，揭示了模态融合机制的脆弱性。

Conclusion: 实现稳健的通用人工智能需要超越规模扩展，专注于协同融合机制的研究。

Abstract: While generalist foundation models like Gemini and GPT-4o demonstrate
impressive multi-modal competence, existing evaluations fail to test their
intelligence in dynamic, interactive worlds. Static benchmarks lack agency,
while interactive benchmarks suffer from a severe modal bottleneck, typically
ignoring crucial auditory and temporal cues. To bridge this evaluation chasm,
we introduce OmniPlay, a diagnostic benchmark designed not just to evaluate,
but to probe the fusion and reasoning capabilities of agentic models across the
full sensory spectrum. Built on a core philosophy of modality interdependence,
OmniPlay comprises a suite of five game environments that systematically create
scenarios of both synergy and conflict, forcing agents to perform genuine
cross-modal reasoning. Our comprehensive evaluation of six leading omni-modal
models reveals a critical dichotomy: they exhibit superhuman performance on
high-fidelity memory tasks but suffer from systemic failures in challenges
requiring robust reasoning and strategic planning. We demonstrate that this
fragility stems from brittle fusion mechanisms, which lead to catastrophic
performance degradation under modality conflict and uncover a counter-intuitive
"less is more" paradox, where removing sensory information can paradoxically
improve performance. Our findings suggest that the path toward robust AGI
requires a research focus beyond scaling to explicitly address synergistic
fusion. Our platform is available for anonymous review at
https://github.com/fuqingbie/omni-game-benchmark.

</details>


### [48] [Artificial Consciousness as Interface Representation](https://arxiv.org/abs/2508.04383)
*Robert Prentner*

Main category: cs.AI

TL;DR: 本文提出了一个框架（SLP测试），通过三个标准（S、L、P）来评估AI系统是否具备类似意识的属性，将主观经验操作化为功能性接口而非物理系统的固有属性。


<details>
  <summary>Details</summary>
Motivation: 由于定义和操作化主观经验的固有挑战，AI系统是否具备意识是一个争议性问题。本文旨在提供一个实证可操作的框架来解决这一问题。

Method: 引入SLP测试（S、L、P标准），利用范畴论建模接口表示，将其视为关系基质与可观察行为之间的映射。

Result: SLP测试为评估AI系统的意识属性提供了实证工具，将主观经验视为功能性接口。

Conclusion: 通过SLP测试，可以将AI意识问题转化为实证可操作的研究方向，为未来研究提供新视角。

Abstract: Whether artificial intelligence (AI) systems can possess consciousness is a
contentious question because of the inherent challenges of defining and
operationalizing subjective experience. This paper proposes a framework to
reframe the question of artificial consciousness into empirically tractable
tests. We introduce three evaluative criteria - S (subjective-linguistic), L
(latent-emergent), and P (phenomenological-structural) - collectively termed
SLP-tests, which assess whether an AI system instantiates interface
representations that facilitate consciousness-like properties. Drawing on
category theory, we model interface representations as mappings between
relational substrates (RS) and observable behaviors, akin to specific types of
abstraction layers. The SLP-tests collectively operationalize subjective
experience not as an intrinsic property of physical systems but as a functional
interface to a relational entity.

</details>


### [49] [GuirlVG: Incentivize GUI Visual Grounding via Empirical Exploration on Reinforcement Learning](https://arxiv.org/abs/2508.04389)
*Weitai Kang,Bin Lei,Gaowen Liu,Caiwen Ding,Yan Yan*

Main category: cs.AI

TL;DR: GuirlVG是一种基于强化学习的GUI视觉定位方法，通过系统实证研究和新颖的稳定技术，仅需少量训练样本即可超越传统监督微调方法。


<details>
  <summary>Details</summary>
Motivation: 传统GUI视觉定位依赖监督微调，数据需求大且成本高；随着多模态大模型的发展，是否需要继续使用监督微调存疑。

Method: 提出GuirlVG方法，分解强化微调核心组件，提出动态稳定的对抗KL因子，优化训练配置。

Result: GuirlVG在少量样本下表现优于监督微调方法，在多个数据集上显著提升性能。

Conclusion: 强化微调是GUI视觉定位的高效替代方案，GuirlVG展示了其潜力。

Abstract: Graphical user interface visual grounding (GUI-VG), a core capability for GUI
agents, has primarily relied on supervised fine-tuning (SFT) of multimodal
large language models (MLLMs), which demands extensive data curation and
significant training costs. However, as MLLMs continue to advance and even
cover GUI domains during pretraining, the necessity of exhaustive SFT
post-training becomes increasingly questionable. Meanwhile, recent successes of
rule-based reinforcement fine-tuning (RFT) suggest a more efficient
alternative. Despite this promise, the optimal manner of applying RFT for
GUI-VG remains unexplored. To bridge this gap, we introduce GuirlVG, a
reinforcement learning-based GUI-VG method built on a systematic empirical
study and a novel stabilization technique. We find that naive application of
RFT underperforms the SFT baseline, motivating a deeper exploration. First, we
decompose RFT into its core components and analyze the optimal formulation of
each. Second, we propose a novel Adversarial KL Factor that dynamically
stabilizes training to mitigate reward over-optimization. Third, we further
explore the training configurations of RFT to enhance effectiveness. Extensive
experiments show that GuirlVG, with only 5.2K training samples, outperforms SFT
methods trained on over 10M samples, achieving a 7.7% improvement on
ScreenSpot, a 17.2% improvement on ScreenSpotPro, and 91.9% accuracy on
ScreenSpotV2.

</details>


### [50] [Beyond Pixels: Exploring DOM Downsampling for LLM-Based Web Agents](https://arxiv.org/abs/2508.04412)
*Thassilo M. Schiepanski,Nicholas Piël*

Main category: cs.AI

TL;DR: 论文提出了一种名为D2Snap的DOM降采样算法，用于解决网页代理中应用状态序列化的问题，其性能与基于GUI快照的方法相当甚至更优。


<details>
  <summary>Details</summary>
Motivation: 当前基于GUI快照的网页代理方法受限于LLM视觉能力不足，而DOM快照虽然结构更优，但输入令牌量过大难以实现。

Method: 提出D2Snap算法，通过降采样DOM快照，结合GPT-4o后端进行评估。

Result: D2Snap在相同输入令牌量下与GUI快照基线性能相当（67% vs 65%），在更高令牌量下性能提升8%。

Conclusion: DOM固有的层次结构是LLM理解UI的强特征，D2Snap为网页代理提供了一种高效替代方案。

Abstract: Frontier LLMs only recently enabled serviceable, autonomous web agents. At
that, a model poses as an instantaneous domain model backend. Ought to suggest
interaction, it is consulted with a web-based task and respective application
state. The key problem lies in application state serialisation
$\unicode{x2013}$ referred to as snapshot. State-of-the-art web agents are
premised on grounded GUI snapshots, i.e., screenshots enhanced with visual
cues. Not least to resemble human perception, but for images representing
relatively cheap means of model input. LLM vision still lag behind code
interpretation capabilities. DOM snapshots, which structurally resemble HTML,
impose a desired alternative. Vast model input token size, however, disables
reliable implementation with web agents to date.
  We propose D2Snap, a first-of-its-kind DOM downsampling algorithm. Based on a
GPT-4o backend, we evaluate D2Snap on tasks sampled from the Online-Mind2Web
dataset. The success rate of D2Snap-downsampled DOM snapshots (67%) matches a
grounded GUI snapshot baseline (65%) $\unicode{x2013}$ within the same input
token order of magnitude (1e3). Our best evaluated configurations
$\unicode{x2013}$ one token order above, but within the model's context window
$\unicode{x2013}$ outperform this baseline by 8%. Our evaluation, moreover,
yields that DOM-inherent hierarchy embodies a strong UI feature for LLMs.

</details>


### [51] [\textsc{SimInstruct}: A Responsible Tool for Collecting Scaffolding Dialogues Between Experts and LLM-Simulated Novices](https://arxiv.org/abs/2508.04428)
*Si Chen,Izzy Molnar,Ting Hua,Peiyu Li,Le Huy Khiem,G. Alex Ambrose,Jim Lang,Ronald Metoyer,Nitesh V. Chawla*

Main category: cs.AI

TL;DR: SimInstruct是一种通过模拟新手与专家互动来收集高质量教学对话的工具，利用LLM模拟新手，专家提供反馈，生成的对话具有教学深度。


<details>
  <summary>Details</summary>
Motivation: 由于隐私和脆弱性问题，高质量的教学对话数据稀缺，SimInstruct旨在解决这一问题。

Method: 使用LLM模拟新手，专家提供多轮反馈，生成教学对话，并分析专家参与方式。

Result: 生成的对话与真实对话具有可比性，专家反馈积极，且基于数据训练的LLaMA模型在教学质量上优于GPT-4。

Conclusion: SimInstruct为教学对话数据收集提供了可扩展方案，并揭示了GPT-4在教学支持中的局限性。

Abstract: High-quality, multi-turn instructional dialogues between novices and experts
are essential for developing AI systems that support teaching, learning, and
decision-making. These dialogues often involve scaffolding -- the process by
which an expert supports a novice's thinking through questions, feedback, and
step-by-step guidance. However, such data are scarce due to privacy concerns in
recording and the vulnerability inherent in help-seeking. We present
SimInstruct, a scalable, expert-in-the-loop tool for collecting scaffolding
dialogues. Using teaching development coaching as an example domain,
SimInstruct simulates novice instructors via LLMs, varying their teaching
challenges and LLM's persona traits, while human experts provide multi-turn
feedback, reasoning, and instructional support. This design enables the
creation of realistic, pedagogically rich dialogues without requiring real
novice participants. Our results reveal that persona traits, such as
extroversion and introversion, meaningfully influence how experts engage.
Compared to real mentoring recordings, SimInstruct dialogues demonstrate
comparable pedagogical relevance and cognitive depth. Experts also reported the
process as engaging and reflective, improving both data quality and their own
professional insight. We further fine-tuned a LLaMA model to be an expert model
using the augmented dataset, which outperformed GPT-4o in instructional
quality. Our analysis highlights GPT-4o's limitations in weak reflective
questioning, overuse of generic praise, a condescending tone, and a tendency to
overwhelm novices with excessive suggestions.

</details>


### [52] [From "Aha Moments" to Controllable Thinking: Toward Meta-Cognitive Reasoning in Large Reasoning Models via Decoupled Reasoning and Control](https://arxiv.org/abs/2508.04460)
*Rui Ha,Chaozhuo Li,Rui Pu,Sen Su*

Main category: cs.AI

TL;DR: 论文提出Meta-cognitive Reasoning Framework (MERA)，通过分离推理与控制组件，优化大型推理模型的效率与准确性。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型（LRMs）在推理过程中缺乏内在调控机制，导致过度思考和计算资源浪费，限制了实际应用。

Method: MERA框架将推理过程分解为推理与控制组件，采用接管式数据构建机制、结构化分离和Control-Segment Policy Optimization (CSPO) 方法。

Result: 实验表明，MERA训练的模型在多个推理基准上提高了效率和准确性。

Conclusion: MERA通过明确的推理控制分离和优化策略，有效解决了LRMs的过度推理问题。

Abstract: Large Reasoning Models (LRMs) have demonstrated a latent capacity for complex
reasoning by spontaneously exhibiting cognitive behaviors such as step-by-step
reasoning, reflection, and backtracking, commonly referred to as "Aha Moments".
However, such emergent behaviors remain unregulated and uncontrolled, often
resulting in overthinking, where the model continues generating redundant
reasoning content even after reaching reliable conclusions. This leads to
excessive computational costs and increased latency, limiting the practical
deployment of LRMs. The root cause lies in the absence of intrinsic regulatory
mechanisms, as current models are unable to monitor and adaptively manage their
reasoning process to determine when to continue, backtrack, or terminate. To
address this issue, we propose the Meta-cognitive Reasoning Framework (MERA),
which explicitly decouples the thinking process into distinct reasoning and
control components, thereby enabling the independent optimization of control
strategies. Specifically, MERA incorporates a takeover-based data construction
mechanism that identifies critical decision points during reasoning and
delegates the creation of control signals to auxiliary LLMs, thereby enabling
the construction of high-quality reasoning-control data. Additionally, a
structured reasoning-control separation is implemented via supervised
fine-tuning, enabling the model to generate explicit traces and acquire initial
meta-cognitive control capabilities. Finally, MERA employs Control-Segment
Policy Optimization (CSPO), which combines segment-wise Group Relative Policy
Optimization (GRPO) with a control-masking mechanism to optimize control
behavior learning while minimizing interference from irrelevant content.
Experiments on various reasoning benchmarks demonstrate that models trained
with MERA enhance both reasoning efficiency and accuracy.

</details>


### [53] [OS Agents: A Survey on MLLM-based Agents for General Computing Devices Use](https://arxiv.org/abs/2508.04482)
*Xueyu Hu,Tao Xiong,Biao Yi,Zishu Wei,Ruixuan Xiao,Yurun Chen,Jiasheng Ye,Meiling Tao,Xiangxin Zhou,Ziyu Zhao,Yuhuai Li,Shengze Xu,Shenzhi Wang,Xinchen Xu,Shuofei Qiao,Zhaokai Wang,Kun Kuang,Tieyong Zeng,Liang Wang,Jiwei Li,Yuchen Eleanor Jiang,Wangchunshu Zhou,Guoyin Wang,Keting Yin,Zhou Zhao,Hongxia Yang,Fan Wu,Shengyu Zhang,Fei Wu*

Main category: cs.AI

TL;DR: 本文综述了基于多模态大语言模型的OS Agents，探讨其组成、能力、构建方法、评估标准及未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 实现类似J.A.R.V.I.S的AI助手，通过OS Agents在多模态环境中自动化任务。

Method: 分析OS Agents的关键组件（环境、观察空间、动作空间）及能力（理解、规划、落地），研究构建方法（领域专用基础模型和代理框架）和评估协议。

Result: 综述了OS Agents的研究现状，提出了未来方向（如安全、隐私、个性化）。

Conclusion: OS Agents研究为学术和工业发展提供了指导，开源资源促进创新。

Abstract: The dream to create AI assistants as capable and versatile as the fictional
J.A.R.V.I.S from Iron Man has long captivated imaginations. With the evolution
of (multi-modal) large language models ((M)LLMs), this dream is closer to
reality, as (M)LLM-based Agents using computing devices (e.g., computers and
mobile phones) by operating within the environments and interfaces (e.g.,
Graphical User Interface (GUI)) provided by operating systems (OS) to automate
tasks have significantly advanced. This paper presents a comprehensive survey
of these advanced agents, designated as OS Agents. We begin by elucidating the
fundamentals of OS Agents, exploring their key components including the
environment, observation space, and action space, and outlining essential
capabilities such as understanding, planning, and grounding. We then examine
methodologies for constructing OS Agents, focusing on domain-specific
foundation models and agent frameworks. A detailed review of evaluation
protocols and benchmarks highlights how OS Agents are assessed across diverse
tasks. Finally, we discuss current challenges and identify promising directions
for future research, including safety and privacy, personalization and
self-evolution. This survey aims to consolidate the state of OS Agents
research, providing insights to guide both academic inquiry and industrial
development. An open-source GitHub repository is maintained as a dynamic
resource to foster further innovation in this field. We present a 9-page
version of our work, accepted by ACL 2025, to provide a concise overview to the
domain.

</details>


### [54] [Argumentative Debates for Transparent Bias Detection [Technical Report]](https://arxiv.org/abs/2508.04511)
*Hamed Ayoobi,Nico Potyka,Anna Rapberger,Francesca Toni*

Main category: cs.AI

TL;DR: 提出了一种基于辩论的新型可解释、可解释的偏见检测方法，利用形式化和计算论证技术，通过讨论个体及其邻域的保护特征值来检测偏见。


<details>
  <summary>Details</summary>
Motivation: 随着AI系统在社会中的广泛应用，防止数据或模型中的偏见对特定群体造成系统性不利影响变得至关重要，而现有方法往往忽视了透明性。

Method: 基于形式化和计算论证技术，通过辩论个体及其邻域的保护特征值来检测偏见。

Result: 方法在性能上优于基线，同时具备良好的可解释性和可解释性。

Conclusion: 该方法为算法公平性提供了一种透明且可解释的解决方案，强调了透明性在公平性中的核心地位。

Abstract: As the use of AI systems in society grows, addressing potential biases that
emerge from data or are learned by models is essential to prevent systematic
disadvantages against specific groups. Several notions of (un)fairness have
been proposed in the literature, alongside corresponding algorithmic methods
for detecting and mitigating unfairness, but, with very few exceptions, these
tend to ignore transparency. Instead, interpretability and explainability are
core requirements for algorithmic fairness, even more so than for other
algorithmic solutions, given the human-oriented nature of fairness. In this
paper, we contribute a novel interpretable, explainable method for bias
detection relying on debates about the presence of bias against individuals,
based on the values of protected features for the individuals and others in
their neighbourhoods. Our method builds upon techniques from formal and
computational argumentation, whereby debates result from arguing about biases
within and across neighbourhoods. We provide formal, quantitative, and
qualitative evaluations of our method, highlighting its strengths in
performance against baselines, as well as its interpretability and
explainability.

</details>


### [55] [SID: Benchmarking Guided Instruction Capabilities in STEM Education with a Socratic Interdisciplinary Dialogues Dataset](https://arxiv.org/abs/2508.04563)
*Mei Jiang,Houping Yue,Bingdong Li,Hao Hao,Ying Qian,Bo Jiang,Aimin Zhou*

Main category: cs.AI

TL;DR: 论文提出了SID基准，用于评估LLMs在多轮跨学科苏格拉底对话中的高阶指导能力，发现当前LLMs仍难以有效引导学生实现知识整合与迁移。


<details>
  <summary>Details</summary>
Motivation: 现代教育核心目标是培养学生复杂问题解决中的知识整合与迁移能力，跨学科STEM是关键途径，但需要专家指导且难以规模化。LLMs虽有潜力，但缺乏有效评估基准。

Method: 提出SID基准，包括大规模数据集（10,000对话轮次）、新注释框架和评估指标（如X-SRG），并通过基线实验验证LLMs的局限性。

Result: 实验表明，即使最先进的LLMs也难以执行有效指导对话，促进学生知识整合与迁移。

Conclusion: SID基准对推动更具教学意识的LLMs发展具有重要价值。

Abstract: Fostering students' abilities for knowledge integration and transfer in
complex problem-solving scenarios is a core objective of modern education, and
interdisciplinary STEM is a key pathway to achieve this, yet it requires expert
guidance that is difficult to scale. While LLMs offer potential in this regard,
their true capability for guided instruction remains unclear due to the lack of
an effective evaluation benchmark. To address this, we introduce SID, the first
benchmark designed to systematically evaluate the higher-order guidance
capabilities of LLMs in multi-turn, interdisciplinary Socratic dialogues. Our
contributions include a large-scale dataset of 10,000 dialogue turns across 48
complex STEM projects, a novel annotation schema for capturing deep pedagogical
features, and a new suite of evaluation metrics (e.g., X-SRG). Baseline
experiments confirm that even state-of-the-art LLMs struggle to execute
effective guided dialogues that lead students to achieve knowledge integration
and transfer. This highlights the critical value of our benchmark in driving
the development of more pedagogically-aware LLMs.

</details>


### [56] [ConfProBench: A Confidence Evaluation Benchmark for MLLM-Based Process Judges](https://arxiv.org/abs/2508.04576)
*Yue Zhou,Yi Chang,Yuan Wu*

Main category: cs.AI

TL;DR: 该论文提出了ConfProBench，首个全面评估多模态大语言模型（MLLM）过程判断器（MPJ）在步骤级别置信度得分可靠性的基准。


<details>
  <summary>Details</summary>
Motivation: 现有MPJ基准主要关注步骤正确性分类和推理过程搜索，而忽略了置信度得分的可靠性，因此需要填补这一空白。

Method: 通过构建三种对抗性扰动推理步骤（同义词替换、句法变换和图像扰动）来测试MPJ置信度的鲁棒性，并引入三个新评估指标（CRS、CSS、CCS）。

Result: 实验评估了14种先进MLLM，揭示了当前MPJ在置信度性能上的局限性，并提供了未来研究的竞争基线。

Conclusion: ConfProBench为系统评估MPJ置信度可靠性提供了工具，并指出了未来改进方向。

Abstract: Reasoning is a critical capability of multimodal large language models
(MLLMs) for solving complex multimodal tasks, and judging the correctness of
reasoning steps is crucial for improving this capability. Recently, MLLM-based
process judges (MPJs) have been widely used to assess the correctness of
reasoning steps in multimodal tasks. Therefore, evaluating MPJs is important
for identifying their limitations and guiding future improvements. However,
existing benchmarks for MPJs mainly focus on tasks such as step correctness
classification and reasoning process search, while overlooking a key aspect:
whether the confidence scores produced by MPJs at the step level are reliable.
To address this gap, we propose ConfProBench, the first comprehensive benchmark
designed to systematically evaluate the reliability of step-level confidence
scores generated by MPJs. Our benchmark constructs three types of adversarially
perturbed reasoning steps: Synonym Substitution, Syntactic Transformation, and
Image Perturbation, to test the robustness of MPJ confidence under
perturbations. In addition, we introduce three novel evaluation metrics:
Confidence Robustness Score (CRS), Confidence Sensitivity Score (CSS), and
Confidence Calibration Score (CCS), which evaluate robustness, sensitivity, and
calibration, respectively. We evaluate 14 state-of-the-art MLLMs, including
both proprietary and open-source models. Experiments reveal limitations in
current MPJs' confidence performance and offer competitive baselines to support
future research.

</details>


### [57] [SEAgent: Self-Evolving Computer Use Agent with Autonomous Learning from Experience](https://arxiv.org/abs/2508.04700)
*Zeyi Sun,Ziyu Liu,Yuhang Zang,Yuhang Cao,Xiaoyi Dong,Tong Wu,Dahua Lin,Jiaqi Wang*

Main category: cs.AI

TL;DR: SEAgent是一种自进化框架，通过自主交互学习新软件，提升计算机使用代理的性能。


<details>
  <summary>Details</summary>
Motivation: 现有大型视觉语言模型在缺乏人工标注的新软件中表现不佳，需自主进化能力。

Method: SEAgent通过体验学习、世界状态模型和课程生成器，结合对抗模仿和GRPO优化策略。

Result: 在OS-World的五个新软件环境中，SEAgent成功率从11.3%提升至34.5%。

Conclusion: SEAgent通过自进化框架显著提升计算机使用代理的性能，优于现有方法。

Abstract: Repurposing large vision-language models (LVLMs) as computer use agents
(CUAs) has led to substantial breakthroughs, primarily driven by human-labeled
data. However, these models often struggle with novel and specialized software,
particularly in scenarios lacking human annotations. To address this challenge,
we propose SEAgent, an agentic self-evolving framework enabling CUAs to
autonomously evolve through interactions with unfamiliar software.
Specifically, SEAgent empowers computer-use agents to autonomously master novel
software environments via experiential learning, where agents explore new
software, learn through iterative trial-and-error, and progressively tackle
auto-generated tasks organized from simple to complex. To achieve this goal, we
design a World State Model for step-wise trajectory assessment, along with a
Curriculum Generator that generates increasingly diverse and challenging tasks.
The agent's policy is updated through experiential learning, comprised of
adversarial imitation of failure actions and Group Relative Policy Optimization
(GRPO) on successful ones. Furthermore, we introduce a specialist-to-generalist
training strategy that integrates individual experiential insights from
specialist agents, facilitating the development of a stronger generalist CUA
capable of continuous autonomous evolution. This unified agent ultimately
achieves performance surpassing ensembles of individual specialist agents on
their specialized software. We validate the effectiveness of SEAgent across
five novel software environments within OS-World. Our approach achieves a
significant improvement of 23.2% in success rate, from 11.3% to 34.5%, over a
competitive open-source CUA, i.e., UI-TARS.

</details>
