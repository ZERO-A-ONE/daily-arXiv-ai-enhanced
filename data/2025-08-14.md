<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 10]
- [cs.CR](#cs.CR) [Total: 12]
- [cs.AI](#cs.AI) [Total: 13]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [Teaching Code Refactoring Using LLMs](https://arxiv.org/abs/2508.09332)
*Anshul Khairnar,Aarya Rajoju,Edward F. Gehringer*

Main category: cs.SE

TL;DR: 利用大型语言模型（LLMs）为软件工程课程中的代码重构教学提供实时、上下文感知的反馈。


<details>
  <summary>Details</summary>
Motivation: 重构能提升代码质量，但教学难度大，尤其是面对复杂的现实代码库。传统方法如代码审查和静态分析工具反馈有限且不一致。

Method: 通过结构化提示将LLM辅助重构融入课程项目，帮助学生识别和解决代码异味（如长方法、低内聚）。

Result: 初步发现表明，LLMs能弥合理论与实践学习，加深学生对可维护性和重构原则的理解。

Conclusion: LLMs在代码重构教学中具有潜力，能有效提升学习效果。

Abstract: This Innovative Practice full paper explores how Large Language Models (LLMs)
can enhance the teaching of code refactoring in software engineering courses
through real-time, context-aware feedback. Refactoring improves code quality
but is difficult to teach, especially with complex, real-world codebases.
Traditional methods like code reviews and static analysis tools offer limited,
inconsistent feedback. Our approach integrates LLM-assisted refactoring into a
course project using structured prompts to help students identify and address
code smells such as long methods and low cohesion. Implemented in Spring 2025
in a long-lived OSS project, the intervention is evaluated through student
feedback and planned analysis of code quality improvements. Findings suggest
that LLMs can bridge theoretical and practical learning, supporting a deeper
understanding of maintainability and refactoring principles.

</details>


### [2] [Plug it and Play on Logs: A Configuration-Free Statistic-Based Log Parser](https://arxiv.org/abs/2508.09366)
*Qiaolin Qin,Xingfang Wu,Heng Li,Ettore Merlo*

Main category: cs.SE

TL;DR: PIPLUP是一种新型的基于统计的日志解析器，挑战了语义解析器更优的普遍观点，通过数据不敏感参数实现高准确性和通用性，且无需GPU加速或外部API。


<details>
  <summary>Details</summary>
Motivation: 现有基于统计的日志解析器在效率和隐私保护方面表现优异，但准确性和通用性不足，普遍认为语义解析器更优。PIPLUP旨在打破这一观点。

Method: PIPLUP消除了日志分组中常量令牌位置的预设，采用数据不敏感参数实现“即插即用”，无需依赖外部知识。

Result: 在大型开源日志数据集上，PIPLUP表现优于现有统计解析器（如Drain），并与最佳无监督语义解析器（LUNAR）竞争。

Conclusion: PIPLUP高效、简单且实用，特别适用于成本和隐私敏感的实际场景。

Abstract: Log parsing is an essential task in log analysis, and many tools have been
designed to accomplish it. Existing log parsers can be categorized into
statistic-based and semantic-based approaches. In comparison to semantic-based
parsers, existing statistic-based parsers tend to be more efficient, require
lower computational costs, and be more privacy-preserving thanks to on-premise
deployment, but often fall short in their accuracy (e.g., grouping or parsing
accuracy) and generalizability. Therefore, it became a common belief that
statistic-based parsers cannot be as effective as semantic-based parsers since
the latter could take advantage of external knowledge supported by pretrained
language models. Our work, however, challenges this belief with a novel
statistic-based parser, PIPLUP. PIPLUP eliminates the pre-assumption of the
position of constant tokens for log grouping and relies on data-insensitive
parameters to overcome the generalizability challenge, allowing "plug and play"
on given log files. According to our experiments on an open-sourced large log
dataset, PIPLUP shows promising accuracy and generalizability with the
data-insensitive default parameter set. PIPLUP not only outperforms the
state-of-the-art statistic-based log parsers, Drain and its variants, but also
obtains a competitive performance compared to the best unsupervised
semantic-based log parser (i.e., LUNAR). Further, PIPLUP exhibits low time
consumption without GPU acceleration and external API usage; our simple,
efficient, and effective approach makes it more practical in real-world
adoptions, especially when costs and privacy are of major concerns.

</details>


### [3] [Your Coding Intent is Secretly in the Context and You Should Deliberately Infer It Before Completion](https://arxiv.org/abs/2508.09537)
*Yanzhou Li,Tianlin Li,Yiran Zhang,Shangqing Liu,Aishan Liu,Yang Liu*

Main category: cs.SE

TL;DR: 论文提出了一种三阶段方法，通过意图推断、交互式精炼和代码生成，提升大型语言模型在无注释代码库中的功能补全能力。


<details>
  <summary>Details</summary>
Motivation: 现实代码库中常缺乏显式注释，导致模型性能下降，需解决无注释情况下的功能补全问题。

Method: 三阶段流程：1) 意图推断，分析代码上下文；2) 交互式精炼，开发者参与意图确认；3) 代码生成。

Result: 在DevEval和ComplexCodeEval上，相对指标提升超20%，交互式精炼进一步优化结果。

Conclusion: 该方法显著提升模型在无注释场景下的表现，交互式设计增强了实用性。

Abstract: Large Language Models (LLMs) are increasingly used for function completion in
repository-scale codebases. Prior studies demonstrate that when explicit
instructions--such as docstrings--are provided, these models can generate
highly accurate implementations. However, in real-world repositories, such
annotations are frequently absent, and performance drops substantially without
them. To address this gap, we frame the task as a three-stage process. The
first stage focuses on intent inference, where the model analyzes the code
preceding the target function to uncover cues about the desired functionality.
Such preceding context often encodes subtle but critical information, and we
design a reasoning-based prompting framework to guide the LLM through
step-by-step extraction and synthesis of these signals before any code is
generated. The second stage introduces an optional interactive refinement
mechanism to handle cases where preceding context alone is insufficient for
intent recovery. In this stage, the model proposes a small set of candidate
intentions, enabling the developer to select or edit them so that the inferred
intent closely matches the actual requirement. Finally, in the third stage, the
LLM generates the target function conditioned on the finalized intent. To
support this pipeline, we curate a dataset of 40,000 examples annotated with
intermediate reasoning traces and corresponding docstrings. Extensive
experiments on DevEval and ComplexCodeEval show that our approach consistently
boosts multiple LLMs, achieving over 20\% relative gains in both
reference-based and execution-based metrics, with the interactive refinement
stage delivering additional improvements beyond these gains.

</details>


### [4] [ReqInOne: A Large Language Model-Based Agent for Software Requirements Specification Generation](https://arxiv.org/abs/2508.09648)
*Taohong Zhu,Lucas C. Cordeiro,Youcheng Sun*

Main category: cs.SE

TL;DR: ReqInOne是一种基于LLM的代理，通过模块化设计将SRS生成分解为摘要、需求提取和需求分类三个任务，显著提高了SRS文档的准确性和结构化程度。


<details>
  <summary>Details</summary>
Motivation: 手动编写SRS耗时且易产生歧义，现有自动化方法依赖人工分析或存在幻觉和可控性不足的问题。

Method: ReqInOne采用模块化架构，分解SRS生成为三个任务，每个任务使用定制提示模板优化LLM输出。

Result: 实验表明，ReqInOne生成的SRS比基于GPT-4的整体方法和初级工程师更准确、结构化。

Conclusion: ReqInOne的模块化设计和需求分类组件表现优异，优于现有方法。

Abstract: Software Requirements Specification (SRS) is one of the most important
documents in software projects, but writing it manually is time-consuming and
often leads to ambiguity. Existing automated methods rely heavily on manual
analysis, while recent Large Language Model (LLM)-based approaches suffer from
hallucinations and limited controllability. In this paper, we propose ReqInOne,
an LLM-based agent that follows the common steps taken by human requirements
engineers when writing an SRS to convert natural language into a structured
SRS. ReqInOne adopts a modular architecture by decomposing SRS generation into
three tasks: summary, requirement extraction, and requirement classification,
each supported by tailored prompt templates to improve the quality and
consistency of LLM outputs.
  We evaluate ReqInOne using GPT-4o, LLaMA 3, and DeepSeek-R1, and compare the
generated SRSs against those produced by the holistic GPT-4-based method from
prior work as well as by entry-level requirements engineers. Expert evaluations
show that ReqInOne produces more accurate and well-structured SRS documents.
The performance advantage of ReqInOne benefits from its modular design, and
experimental results further demonstrate that its requirement classification
component achieves comparable or even better results than the state-of-the-art
requirement classification model.

</details>


### [5] [DeputyDev -- AI Powered Developer Assistant: Breaking the Code Review Logjam through Contextual AI to Boost Developer Productivity](https://arxiv.org/abs/2508.09676)
*Vishal Khare,Vijay Saini,Deepak Sharma,Anand Kumar,Ankit Rana,Anshul Yadav*

Main category: cs.SE

TL;DR: DeputyDev是一个AI驱动的代码审查助手，旨在解决软件开发中的低效问题，显著减少了代码审查时间。


<details>
  <summary>Details</summary>
Motivation: 代码审查过程存在效率低下、反馈不一致和质量不达标的问题，导致开发周期延长和代码质量下降。

Method: 通过开发DeputyDev的自动化代码审查功能，并进行双盲A/B实验，评估其对审查时间的影响。

Result: 实验结果显示，DeputyDev显著减少了每PR（23.09%）和每行代码（40.13%）的审查时间。

Conclusion: DeputyDev成功提高了开发流程效率，并已作为SaaS解决方案推广至外部公司。

Abstract: This study investigates the implementation and efficacy of DeputyDev, an
AI-powered code review assistant developed to address inefficiencies in the
software development process. The process of code review is highly inefficient
for several reasons, such as it being a time-consuming process, inconsistent
feedback, and review quality not being at par most of the time. Using our
telemetry data, we observed that at TATA 1mg, pull request (PR) processing
exhibits significant inefficiencies, with average pick-up and review times of
73 and 82 hours, respectively, resulting in a 6.2 day closure cycle. The review
cycle was marked by prolonged iterative communication between the reviewing and
submitting parties. Research from the University of California, Irvine
indicates that interruptions can lead to an average of 23 minutes of lost
focus, critically affecting code quality and timely delivery. To address these
challenges, we developed DeputyDev's PR review capabilities by providing
automated, contextual code reviews. We conducted a rigorous double-controlled
A/B experiment involving over 200 engineers to evaluate DeputyDev's impact on
review times. The results demonstrated a statistically significant reduction in
both average per PR (23.09%) and average per-line-of-code (40.13%) review
durations. After implementing safeguards to exclude outliers, DeputyDev has
been effectively rolled out across the entire organisation. Additionally, it
has been made available to external companies as a Software-as-a-Service (SaaS)
solution, currently supporting the daily work of numerous engineering
professionals. This study explores the implementation and effectiveness of
AI-assisted code reviews in improving development workflow timelines and code.

</details>


### [6] [Inclusive Employment Pathways: Career Success Factors for Autistic Individuals in Software Engineering](https://arxiv.org/abs/2508.09680)
*Orvila Sarker,Mona Jamshaid,M. Ali Babar*

Main category: cs.SE

TL;DR: 自闭症个体在ICT领域（如软件开发、测试和网络安全）具有独特优势，但面临职场障碍。研究通过系统综述提出18个成功因素，分为四类，为教育机构、雇主和工具开发者提供包容性建议。


<details>
  <summary>Details</summary>
Motivation: 自闭症个体在ICT领域潜力巨大，但职场障碍限制了他们的发展。研究旨在通过系统综述，为从教育到职场的全路径提供包容性解决方案。

Method: 对30项研究进行系统综述，识别出18个成功因素，分为软件工程教育、职业培训、工作环境和辅助工具四类。

Result: 提出四类成功因素，包括包容性会议实践、结构化工作环境、明确职责和个性化职场调整。

Conclusion: 研究为提升自闭症个体在软件工程领域的包容性提供了基于证据的建议，强调环境设计的重要性。

Abstract: Research has highlighted the valuable contributions of autistic individuals
in the Information and Communication Technology (ICT) sector, particularly in
areas such as software development, testing, and cybersecurity. Their strengths
in information processing, attention to detail, innovative thinking, and
commitment to high-quality outcomes in the ICT domain are well-documented.
However, despite their potential, autistic individuals often face barriers in
Software Engineering (SE) roles due to a lack of personalised tools, complex
work environments, non-inclusive recruitment practices, limited co-worker
support, challenging social dynamics and so on. Motivated by the ethical
framework of the neurodiversity movement and the success of pioneering
initiatives like the Dandelion program, corporate Diversity, Equity, and
Inclusion (DEI) in the ICT sector has increasingly focused on autistic talent.
This movement fundamentally reframes challenges not as individual deficits but
as failures of environments designed for a neurotypical majority. Despite this
progress, there is no synthesis of knowledge reporting the full pathway from
software engineering education through to sustainable workplace inclusion. To
address this, we conducted a Systematic Review of 30 studies and identified 18
success factors grouped into four thematic categories: (1) Software Engineering
Education, (2) Career and Employment Training, (3) Work Environment, and (4)
Tools and Assistive Technologies. Our findings offer evidence-based
recommendations for educational institutions, employers, organisations, and
tool developers to enhance the inclusion of autistic individuals in SE. These
include strategies for inclusive meeting and collaboration practices,
accessible and structured work environments, clear role and responsibility
definitions, and the provision of tailored workplace accommodations.

</details>


### [7] [LibRec: Benchmarking Retrieval-Augmented LLMs for Library Migration Recommendations](https://arxiv.org/abs/2508.09791)
*Junxiao Han,Yarong Wang,Xiaodong Gu,Cuiyun Gao,Yao Wan,Song Han,David Lo,Shuiguang Deng*

Main category: cs.SE

TL;DR: LibRec是一个结合LLMs和RAG技术的框架，用于自动化推荐替代库，并通过上下文学习提取迁移意图以提高准确性。评估使用LibEval基准，包含2,888条迁移记录。


<details>
  <summary>Details</summary>
Motivation: 解决库迁移推荐问题，结合LLMs和RAG技术以提高推荐准确性。

Method: 提出LibRec框架，集成LLMs和RAG技术，利用上下文学习提取迁移意图。

Result: 通过LibEval基准评估，分析了十种LLMs的表现、关键组件贡献、提示策略影响及失败案例。

Conclusion: LibRec框架在库迁移推荐任务中表现有效，结合LLMs和RAG技术显著提升推荐准确性。

Abstract: In this paper, we propose LibRec, a novel framework that integrates the
capabilities of LLMs with retrieval-augmented generation(RAG) techniques to
automate the recommendation of alternative libraries. The framework further
employs in-context learning to extract migration intents from commit messages
to enhance the accuracy of its recommendations. To evaluate the effectiveness
of LibRec, we introduce LibEval, a benchmark designed to assess the performance
in the library migration recommendation task. LibEval comprises 2,888 migration
records associated with 2,368 libraries extracted from 2,324 Python
repositories. Each migration record captures source-target library pairs, along
with their corresponding migration intents and intent types. Based on LibEval,
we evaluated the effectiveness of ten popular LLMs within our framework,
conducted an ablation study to examine the contributions of key components
within our framework, explored the impact of various prompt strategies on the
framework's performance, assessed its effectiveness across various intent
types, and performed detailed failure case analyses.

</details>


### [8] [Fast and Accurate Heuristics for Bus-Factor Estimation](https://arxiv.org/abs/2508.09828)
*Sebastiano Antonio Piccolo*

Main category: cs.SE

TL;DR: 论文提出了两种基于图剥离的启发式方法（Minimum Coverage和Maximum Coverage），用于近似计算软件项目的bus-factor，显著优于现有方法，并在大规模图上验证了其高效性和准确性。


<details>
  <summary>Details</summary>
Motivation: bus-factor是衡量项目风险的关键指标，但其精确计算是NP-Hard问题，现有方法在大规模系统中不可行。

Method: 将软件项目建模为开发者和任务的双分图，提出两种基于迭代图剥离的启发式方法。

Result: 在1000多个合成图上验证，新方法更准确且能扩展到百万级节点图。

Conclusion: 新启发式方法在准确性和扩展性上优于现有方法，并开源实现以支持未来研究。

Abstract: The bus-factor is a critical risk indicator that quantifies how many key
contributors a project can afford to lose before core knowledge or
functionality is compromised. Despite its practical importance, accurately
computing the bus-factor is NP-Hard under established formalizations, making
scalable analysis infeasible for large software systems.
  In this paper, we model software projects as bipartite graphs of developers
and tasks and propose two novel approximation heuristics, Minimum Coverage and
Maximum Coverage, based on iterative graph peeling, for two influential
bus-factor formalizations. Our methods significantly outperform the widely
adopted degree-based heuristic, which we show can yield severely inflated
estimates.
  We conduct a comprehensive empirical evaluation on over $1\,000$ synthetic
power-law graphs and demonstrate that our heuristics provide tighter estimates
while scaling to graphs with millions of nodes and edges in minutes. Our
results reveal that the proposed heuristics are not only more accurate but also
robust to structural variations in developer-task assignment graph. We release
our implementation as open-source software to support future research and
practical adoption.

</details>


### [9] [Exploring the Potential of Large Language Models in Fine-Grained Review Comment Classification](https://arxiv.org/abs/2508.09832)
*Linh Nguyen,Chunhua Liu,Hong Yi Lin,Patanamon Thongtanunam*

Main category: cs.SE

TL;DR: 论文探讨了使用大型语言模型（LLMs）分类代码审查评论的潜力，结果显示LLMs在分类性能上优于现有深度学习方法，尤其在低训练样本类别中表现更优。


<details>
  <summary>Details</summary>
Motivation: 现有基于监督学习的代码审查评论分类方法需要大量手动标注数据，限制了其可扩展性。研究旨在探索LLMs是否能提供更高效的解决方案。

Method: 研究评估了LLMs对17类代码审查评论的分类性能，并与现有深度学习方法进行对比。

Result: LLMs在分类性能上优于现有方法，尤其在低训练样本类别中表现更优，且在高频和低频类别中均表现均衡。

Conclusion: LLMs为代码审查分析提供了可扩展的解决方案，有望提升代码审查的效率。

Abstract: Code review is a crucial practice in software development. As code review
nowadays is lightweight, various issues can be identified, and sometimes, they
can be trivial. Research has investigated automated approaches to classify
review comments to gauge the effectiveness of code reviews. However, previous
studies have primarily relied on supervised machine learning, which requires
extensive manual annotation to train the models effectively. To address this
limitation, we explore the potential of using Large Language Models (LLMs) to
classify code review comments. We assess the performance of LLMs to classify 17
categories of code review comments. Our results show that LLMs can classify
code review comments, outperforming the state-of-the-art approach using a
trained deep learning model. In particular, LLMs achieve better accuracy in
classifying the five most useful categories, which the state-of-the-art
approach struggles with due to low training examples. Rather than relying
solely on a specific small training data distribution, our results show that
LLMs provide balanced performance across high- and low-frequency categories.
These results suggest that the LLMs could offer a scalable solution for code
review analytics to improve the effectiveness of the code review process.

</details>


### [10] [An Empirical Study of CGO Usage in Go Projects -- Distribution, Purposes, Patterns and Critical Issues](https://arxiv.org/abs/2508.09875)
*Jinbao Chen,Boyao Ding,Yu Zhang,Qingwei Li,Fugen Tang*

Main category: cs.SE

TL;DR: 研究分析了Go语言中CGO的使用情况，发现其分布、模式、目的及问题，并提出改进方案。


<details>
  <summary>Details</summary>
Motivation: 现有研究忽略了Go语言中新兴的FFI工具CGO的独特风险，需填补这一空白。

Method: 对920个开源Go项目进行实证研究，开发工具CGOAnalyzer分析CGO使用情况。

Result: 11.3%的项目使用CGO，发现19类问题，提出临时解决方案并提交永久改进提案。

Conclusion: 研究为开发者和Go团队提供了宝贵见解，提升了开发效率和工具链的稳健性。

Abstract: Multilingual software development integrates multiple languages into a single
application, with the Foreign Function Interface (FFI) enabling seamless
interaction. While FFI boosts efficiency and extensibility, it also introduces
risks. Existing studies focus on FFIs in languages like Python and Java,
neglecting CGO, the emerging FFI in Go, which poses unique risks.
  To address these concerns, we conduct an empirical study of CGO usage across
920 open-source Go projects. Our study aims to reveal the distribution,
patterns, purposes, and critical issues associated with CGO, offering insights
for developers and the Go team. We develop CGOAnalyzer, a tool to efficiently
identify and quantify CGO-related features. Our findings reveal that: (1) 11.3%
of analyzed Go projects utilize CGO, with usage concentrated in a subset of
projects; (2) CGO serves 4 primary purposes, including system-level
interactions and performance optimizations, with 15 distinct usage patterns
observed; (3) 19 types of CGO-related issues exist, including one critical
issue involving unnecessary pointer checks that pose risks of runtime crashes
due to limitations in the current Go compilation toolchain; (4) a temporary
solution reduces unnecessary pointer checks, mitigating crash risks, and (5) we
submitted a proposal to improve the Go toolchain for a permanent fix, which has
been grouped within an accepted proposal for future resolution. Our findings
provide valuable insights for developers and the Go team, enhancing development
efficiency and reliability while improving the robustness of the Go toolchain.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [11] [Learning to Detect Unknown Jailbreak Attacks in Large Vision-Language Models: A Unified and Accurate Approach](https://arxiv.org/abs/2508.09201)
*Shuang Liang,Zhihao Xu,Jialing Tao,Hui Xue,Xiting Wang*

Main category: cs.CR

TL;DR: 论文提出了一种名为LoD的无监督框架，通过异常检测识别大型视觉语言模型（LVLM）的越狱攻击，利用多模态安全概念激活向量（MSCAV）和安全模式自动编码器（AE）实现高效检测。


<details>
  <summary>Details</summary>
Motivation: 尽管已有研究尝试通过内部表征检测越狱攻击，但多数方法依赖启发式规则，性能不佳。因此，需要一种基于原则性目标的方法来提升检测效果。

Method: LoD框架通过MSCAV捕捉跨模态的安全相关表征，并利用AE建模安全输入的分布，通过重构误差检测异常。

Result: 在三种LVLM和五个基准测试中，LoD的平均AUROC达到0.9951，较最强基线提升高达38.89%。

Conclusion: LoD无需攻击标签即可高效检测越狱攻击，为LVLM的安全性提供了新的解决方案。

Abstract: Despite extensive alignment efforts, Large Vision-Language Models (LVLMs)
remain vulnerable to jailbreak attacks, posing serious safety risks. Although
recent detection works have shifted to internal representations due to their
rich cross-modal information, most methods rely on heuristic rules rather than
principled objectives, resulting in suboptimal performance. To address these
limitations, we propose Learning to Detect (LoD), a novel unsupervised
framework that formulates jailbreak detection as anomaly detection. LoD
introduces two key components: Multi-modal Safety Concept Activation Vectors
(MSCAV), which capture layer-wise safety-related representations across
modalities, and the Safety Pattern Auto-Encoder, which models the distribution
of MSCAV derived from safe inputs and detects anomalies via reconstruction
errors. By training the auto-encoder (AE) solely on safe samples without attack
labels, LoD naturally identifies jailbreak inputs as distributional anomalies,
enabling accurate and unified detection of jailbreak attacks. Comprehensive
experiments on three different LVLMs and five benchmarks demonstrate that LoD
achieves state-of-the-art performance, with an average AUROC of 0.9951 and an
improvement of up to 38.89% in the minimum AUROC over the strongest baselines.

</details>


### [12] [VeriPHY: Physical Layer Signal Authentication for Wireless Communication in 5G Environments](https://arxiv.org/abs/2508.09213)
*Clifton Paul Robinson,Salvatore D'Oro,Tommaso Melodia*

Main category: cs.CR

TL;DR: VeriPHY是一种基于深度学习的物理层认证方案，用于5G网络，通过隐写术在无线I/Q传输中嵌入签名，实现高精度设备认证。


<details>
  <summary>Details</summary>
Motivation: 传统加密方法在无线网络中效率较低，物理层认证（PLA）利用通信介质的固有特性提供安全认证，结合深度学习提高准确性和可靠性。

Method: VeriPHY通过高斯混合模型生成伪随机签名，嵌入到用户传输的I/Q样本中，利用深度神经网络识别和认证用户。

Result: VeriPHY在93%至100%的范围内实现高精度签名识别，假阳性率低，推断时间为28毫秒。

Conclusion: VeriPHY在5G网络中提供高效、隐蔽的设备认证，同时保持高检测精度。

Abstract: Physical layer authentication (PLA) uses inherent characteristics of the
communication medium to provide secure and efficient authentication in wireless
networks, bypassing the need for traditional cryptographic methods. With
advancements in deep learning, PLA has become a widely adopted technique for
its accuracy and reliability. In this paper, we introduce VeriPHY, a novel deep
learning-based PLA solution for 5G networks, which enables unique device
identification by embedding signatures within wireless I/Q transmissions using
steganography. VeriPHY continuously generates pseudo-random signatures by
sampling from Gaussian Mixture Models whose distribution is carefully varied to
ensure signature uniqueness and stealthiness over time, and then embeds the
newly generated signatures over I/Q samples transmitted by users to the 5G gNB.
Utilizing deep neural networks, VeriPHY identifies and authenticates users
based on these embedded signatures. VeriPHY achieves high precision,
identifying unique signatures between 93% and 100% with low false positive
rates and an inference time of 28 ms when signatures are updated every 20 ms.
Additionally, we also demonstrate a stealth generation mode where signatures
are generated in a way that makes them virtually indistinguishable from
unaltered 5G signals while maintaining over 93% detection accuracy.

</details>


### [13] [Can AI Keep a Secret? Contextual Integrity Verification: A Provable Security Architecture for LLMs](https://arxiv.org/abs/2508.09288)
*Aayush Gupta*

Main category: cs.CR

TL;DR: 论文提出了一种名为CIV的安全架构，通过加密签名和信任源格栅技术，有效防御LLM的提示注入攻击，保持模型性能不变。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLM）易受提示注入和越狱攻击，现有启发式防护措施常被绕过，需更可靠的防御方案。

Method: CIV通过为每个令牌附加加密签名来源标签，并在Transformer中通过预softmax硬注意力掩码（可选FFN/残差门控）强制执行信任源格栅。

Result: 在Elite-Attack和SoK-246基准测试中，CIV攻击成功率为0%，同时保持93.1%的令牌级相似性，且不影响良性任务性能。

Conclusion: CIV是一种轻量级补丁，无需微调即可为LLM提供即时保护，适用于Llama-3-8B和Mistral-7B等模型。

Abstract: Large language models (LLMs) remain acutely vulnerable to prompt injection
and related jailbreak attacks; heuristic guardrails (rules, filters, LLM
judges) are routinely bypassed. We present Contextual Integrity Verification
(CIV), an inference-time security architecture that attaches cryptographically
signed provenance labels to every token and enforces a source-trust lattice
inside the transformer via a pre-softmax hard attention mask (with optional
FFN/residual gating). CIV provides deterministic, per-token non-interference
guarantees on frozen models: lower-trust tokens cannot influence higher-trust
representations. On benchmarks derived from recent taxonomies of
prompt-injection vectors (Elite-Attack + SoK-246), CIV attains 0% attack
success rate under the stated threat model while preserving 93.1% token-level
similarity and showing no degradation in model perplexity on benign tasks; we
note a latency overhead attributable to a non-optimized data path. Because CIV
is a lightweight patch -- no fine-tuning required -- we demonstrate drop-in
protection for Llama-3-8B and Mistral-7B. We release a reference
implementation, an automated certification harness, and the Elite-Attack corpus
to support reproducible research.

</details>


### [14] [Security Analysis of ChatGPT: Threats and Privacy Risks](https://arxiv.org/abs/2508.09426)
*Yushan Xiang,Zhongwen Li,Xiaoqi Li*

Main category: cs.CR

TL;DR: 本文研究了ChatGPT的安全性和隐私风险，分析了漏洞类型及成因，并探讨了其在伦理道德层面的争议。通过模拟攻击者视角进行测试，同时从防御者角度探索了ChatGPT在安全漏洞检测和工具生成中的可行性。


<details>
  <summary>Details</summary>
Motivation: 随着ChatGPT应用范围的扩大，其面临的安全威胁和隐私风险日益突出，亟需系统研究。

Method: 从安全威胁和隐私风险两方面分析漏洞类型及成因，模拟攻击者视角进行测试，并从防御者角度探索ChatGPT的安全应用。

Result: 识别了ChatGPT的多类漏洞及其成因，验证了其在安全领域的潜在应用价值。

Conclusion: ChatGPT的安全性和隐私问题需引起重视，但其在安全领域的应用潜力值得进一步研究。

Abstract: As artificial intelligence technology continues to advance, chatbots are
becoming increasingly powerful. Among them, ChatGPT, launched by OpenAI, has
garnered widespread attention globally due to its powerful natural language
processing capabilities based on the GPT model, which enables it to engage in
natural conversations with users, understand various forms of linguistic
expressions, and generate useful information and suggestions. However, as its
application scope expands, user demand grows, and malicious attacks related to
it become increasingly frequent, the security threats and privacy risks faced
by ChatGPT are gradually coming to the forefront. In this paper, the security
of ChatGPT is mainly studied from two aspects, security threats and privacy
risks. The article systematically analyzes various types of vulnerabilities
involved in the above two types of problems and their causes. Briefly, we
discuss the controversies that ChatGPT may cause at the ethical and moral
levels. In addition, this paper reproduces several network attack and defense
test scenarios by simulating the attacker's perspective and methodology.
Simultaneously, it explores the feasibility of using ChatGPT for security
vulnerability detection and security tool generation from the defender's
perspective.

</details>


### [15] [Shadow in the Cache: Unveiling and Mitigating Privacy Risks of KV-cache in LLM Inference](https://arxiv.org/abs/2508.09442)
*Zhifan Luo,Shuo Shao,Su Zhang,Lijing Zhou,Yuke Hu,Chenxu Zhao,Zhihao Liu,Zhan Qin*

Main category: cs.CR

TL;DR: 论文分析了KV缓存的隐私风险，提出三种攻击方法，并设计防御机制KV-Cloak，实验证明其有效性且不影响模型性能。


<details>
  <summary>Details</summary>
Motivation: KV缓存虽加速LLM推理，但带来未充分研究的隐私风险，需全面分析并提出解决方案。

Method: 提出三种攻击方法（反转攻击、碰撞攻击、注入攻击），并设计防御机制KV-Cloak，采用可逆矩阵混淆和算子融合技术。

Result: KV-Cloak能有效防御所有攻击，将重建质量降至随机噪声水平，且不影响模型精度和性能。

Conclusion: KV-Cloak为LLM部署提供了轻量、高效且安全的解决方案。

Abstract: The Key-Value (KV) cache, which stores intermediate attention computations
(Key and Value pairs) to avoid redundant calculations, is a fundamental
mechanism for accelerating Large Language Model (LLM) inference. However, this
efficiency optimization introduces significant yet underexplored privacy risks.
This paper provides the first comprehensive analysis of these vulnerabilities,
demonstrating that an attacker can reconstruct sensitive user inputs directly
from the KV-cache. We design and implement three distinct attack vectors: a
direct Inversion Attack, a more broadly applicable and potent Collision Attack,
and a semantic-based Injection Attack. These methods demonstrate the
practicality and severity of KV-cache privacy leakage issues. To mitigate this,
we propose KV-Cloak, a novel, lightweight, and efficient defense mechanism.
KV-Cloak uses a reversible matrix-based obfuscation scheme, combined with
operator fusion, to secure the KV-cache. Our extensive experiments show that
KV-Cloak effectively thwarts all proposed attacks, reducing reconstruction
quality to random noise. Crucially, it achieves this robust security with
virtually no degradation in model accuracy and minimal performance overhead,
offering a practical solution for trustworthy LLM deployment.

</details>


### [16] [Demystifying the Role of Rule-based Detection in AI Systems for Windows Malware Detection](https://arxiv.org/abs/2508.09652)
*Andrea Ponte,Luca Demetrio,Luca Oneto,Ivan Tesfai Ogbu,Battista Biggio,Fabio Roli*

Main category: cs.CR

TL;DR: 论文研究了在训练管道中集成基于签名的检测对AI模型训练的影响，发现能提升对抗性样本和数据漂移的鲁棒性，但会带来固定的假阳性下限。


<details>
  <summary>Details</summary>
Motivation: 当前AI驱动的恶意软件检测通常孤立开发签名检测和机器学习组件，未能充分利用数据简化及对抗性样本防御的机会。

Method: 比较了在完整数据集上训练的模型与仅使用未被签名检测标记的样本训练的AI系统。

Result: 结果显示对对抗性EXEmples和数据漂移的鲁棒性增强，但存在固定的假阳性下限。

Conclusion: 未来研究可结合动态分析以进一步提升系统韧性。

Abstract: Malware detection increasingly relies on AI systems that integrate
signature-based detection with machine learning. However, these components are
typically developed and combined in isolation, missing opportunities to reduce
data complexity and strengthen defenses against adversarial EXEmples, carefully
crafted programs designed to evade detection. Hence, in this work we
investigate the influence that signature-based detection exerts on model
training, when they are included inside the training pipeline. Specifically, we
compare models trained on a comprehensive dataset with an AI system whose
machine learning component is trained solely on samples not already flagged by
signatures. Our results demonstrate improved robustness to both adversarial
EXEmples and temporal data drift, although this comes at the cost of a fixed
lower bound on false positives, driven by suboptimal rule selection. We
conclude by discussing these limitations and outlining how future research
could extend AI-based malware detection to include dynamic analysis, thereby
further enhancing system resilience.

</details>


### [17] [Social-Sensor Identity Cloning Detection Using Weakly Supervised Deep Forest and Cryptographic Authentication](https://arxiv.org/abs/2508.09665)
*Ahmed Alharbi,Hai Dong,Xun Yi*

Main category: cs.CR

TL;DR: 提出了一种新的社交传感器云身份克隆检测方法，结合相似身份检测和加密认证协议，性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法在检测重复账户和大规模真实数据集评估方面表现不足，亟需改进。

Method: 1) 使用弱监督深度森林模型检测相似身份；2) 设计加密认证协议验证身份是否由同一服务商生成。

Result: 在大规模真实数据集上验证了方法的可行性和优越性能。

Conclusion: 该方法在身份克隆检测中表现出色，优于现有技术。

Abstract: Recent years have witnessed a rising trend in social-sensor cloud identity
cloning incidents. However, existing approaches suffer from unsatisfactory
performance, a lack of solutions for detecting duplicated accounts, and a lack
of large-scale evaluations on real-world datasets. We introduce a novel method
for detecting identity cloning in social-sensor cloud service providers. Our
proposed technique consists of two primary components: 1) a similar identity
detection method and 2) a cryptography-based authentication protocol.
Initially, we developed a weakly supervised deep forest model to identify
similar identities using non-privacy-sensitive user profile features provided
by the service. Subsequently, we designed a cryptography-based authentication
protocol to verify whether similar identities were generated by the same
provider. Our extensive experiments on a large real-world dataset demonstrate
the feasibility and superior performance of our technique compared to current
state-of-the-art identity clone detection methods.

</details>


### [18] [Succinct Oblivious Tensor Evaluation and Applications: Adaptively-Secure Laconic Function Evaluation and Trapdoor Hashing for All Circuits](https://arxiv.org/abs/2508.09673)
*Damiano Abram,Giulio Malavolta,Lawrence Roy*

Main category: cs.CR

TL;DR: 论文提出了简洁的 oblivious tensor evaluation (OTE) 概念，基于 LWE 问题构建了最优复杂度的 OTE，并展示了其在多种密码学原语中的应用。


<details>
  <summary>Details</summary>
Motivation: 解决现有技术中通信复杂度和 CRS 大小的问题，同时基于标准 LWE 假设实现自适应安全性。

Method: 利用 LWE 问题构建 OTE，并引入自适应格编码作为关键技术。

Result: 实现了多种密码学原语，包括自适应安全的 laconic function evaluation 和最优简洁的同态秘密共享。

Conclusion: OTE 是一个强大的工具，能够基于 LWE 实现多种高效且安全的密码学应用。

Abstract: We propose the notion of succinct oblivious tensor evaluation (OTE), where
two parties compute an additive secret sharing of a tensor product of two
vectors $\mathbf{x} \otimes \mathbf{y}$, exchanging two simultaneous messages.
Crucially, the size of both messages and of the CRS is independent of the
dimension of $\mathbf{x}$.
  We present a construction of OTE with optimal complexity from the standard
learning with errors (LWE) problem. Then we show how this new technical tool
enables a host of cryptographic primitives, all with security reducible to LWE,
such as:
  * Adaptively secure laconic function evaluation for depth-$D$ functions
$f:\{0, 1\}^m\rightarrow\{0, 1\}^\ell$ with communication $m+\ell+D\cdot
\mathrm{poly}(\lambda)$.
  * A trapdoor hash function for all functions.
  * An (optimally) succinct homomorphic secret sharing for all functions.
  * A rate-$1/2$ laconic oblivious transfer for batch messages, which is best
possible.
  In particular, we obtain the first laconic function evaluation scheme that is
adaptively secure from the standard LWE assumption, improving upon Quach, Wee,
and Wichs (FOCS 2018).
  As a key technical ingredient, we introduce a new notion of \emph{adaptive
lattice encodings}, which may be of independent interest.

</details>


### [19] [Enhance the machine learning algorithm performance in phishing detection with keyword features](https://arxiv.org/abs/2508.09765)
*Zijiang Yang*

Main category: cs.CR

TL;DR: 论文提出了一种结合关键词特征与传统特征的新方法，用于提升钓鱼网站URL的检测效果，实验显示该方法能显著降低分类错误率。


<details>
  <summary>Details</summary>
Motivation: 钓鱼攻击日益增多，导致用户敏感信息泄露和财务损失，早期检测钓鱼网站URL至关重要。

Method: 提出了一种结合关键词特征与传统特征的新方法，应用于多种传统机器学习算法。

Result: 该方法平均减少30%的分类错误率，在小数据集上效果更显著，最佳准确率达到99.68%。

Conclusion: 该方法有效提升了钓鱼URL检测的准确性，且不依赖第三方服务提供的信息。

Abstract: Recently, we can observe a significant increase of the phishing attacks in
the Internet. In a typical phishing attack, the attacker sets up a malicious
website that looks similar to the legitimate website in order to obtain the
end-users' information. This may cause the leakage of the sensitive information
and the financial loss for the end-users. To avoid such attacks, the early
detection of these websites' URLs is vital and necessary. Previous researchers
have proposed many machine learning algorithms to distinguish the phishing URLs
from the legitimate ones. In this paper, we would like to enhance these machine
learning algorithms from the perspective of feature selection. We propose a
novel method to incorporate the keyword features with the traditional features.
This method is applied on multiple traditional machine learning algorithms and
the experimental results have shown this method is useful and effective. On
average, this method can reduce the classification error by 30% for the large
dataset. Moreover, its enhancement is more significant for the small dataset.
In addition, this method extracts the information from the URL and does not
rely on the additional information provided by the third-part service. The best
result for the machine learning algorithm using our proposed method has
achieved the accuracy of 99.68%.

</details>


### [20] [Perfect message authentication codes are robust to small deviations from uniform key distributions](https://arxiv.org/abs/2508.09783)
*Boris Ryabko*

Main category: cs.CR

TL;DR: 研究了密钥概率分布偏离均匀分布对完美消息认证码安全性的影响，发现安全性降低与统计距离相关。


<details>
  <summary>Details</summary>
Motivation: 探讨密钥分布不均匀对完美消息认证码安全性的影响。

Method: 分析密钥概率分布与均匀分布的统计距离对安全性的影响。

Result: 发现安全性降低与统计距离相关，完美消息认证码对小偏差具有鲁棒性。

Conclusion: 完美消息认证码对密钥分布的小偏差具有鲁棒性。

Abstract: We investigate the impact of (possible) deviations of the probability
distribution of key values from a uniform distribution for the
information-theoretic strong, or perfect, message authentication code. We found
a simple expression for the decrease in security as a function of the
statistical distance between the real key probability distribution and the
uniform one. In a sense, a perfect message authentication code is robust to
small deviations from a uniform key distribution.

</details>


### [21] [Explainable Ensemble Learning for Graph-Based Malware Detection](https://arxiv.org/abs/2508.09801)
*Hossein Shokouhinejad,Roozbeh Razavi-Far,Griffin Higgins,Ali A Ghorbani*

Main category: cs.CR

TL;DR: 提出了一种基于图神经网络的堆叠集成框架，用于恶意软件检测和解释，结合多种GNN基学习器和元学习器，提升分类性能和可解释性。


<details>
  <summary>Details</summary>
Motivation: 现代计算环境需要准确、可解释且能抵抗规避技术的恶意软件检测模型，单一模型在泛化和可解释性上存在不足。

Method: 动态提取PE文件的CFG，通过两步嵌入策略编码基本块，使用多种GNN基学习器捕获互补特征，并通过注意力机制元学习器聚合预测结果。

Result: 实验表明该框架提高了分类性能，并提供了对恶意软件行为的可解释分析。

Conclusion: 提出的框架在恶意软件检测中实现了高性能和可解释性的平衡。

Abstract: Malware detection in modern computing environments demands models that are
not only accurate but also interpretable and robust to evasive techniques.
Graph neural networks (GNNs) have shown promise in this domain by modeling rich
structural dependencies in graph-based program representations such as control
flow graphs (CFGs). However, single-model approaches may suffer from limited
generalization and lack interpretability, especially in high-stakes security
applications. In this paper, we propose a novel stacking ensemble framework for
graph-based malware detection and explanation. Our method dynamically extracts
CFGs from portable executable (PE) files and encodes their basic blocks through
a two-step embedding strategy. A set of diverse GNN base learners, each with a
distinct message-passing mechanism, is used to capture complementary behavioral
features. Their prediction outputs are aggregated by a meta-learner implemented
as an attention-based multilayer perceptron, which both classifies malware
instances and quantifies the contribution of each base model. To enhance
explainability, we introduce an ensemble-aware post-hoc explanation technique
that leverages edge-level importance scores generated by a GNN explainer and
fuses them using the learned attention weights. This produces interpretable,
model-agnostic explanations aligned with the final ensemble decision.
Experimental results demonstrate that our framework improves classification
performance while providing insightful interpretations of malware behavior.

</details>


### [22] [On the Consistency and Performance of the Iterative Bayesian Update](https://arxiv.org/abs/2508.09980)
*Ehab ElSalamouny,Catuscia Palamidessi*

Main category: cs.CR

TL;DR: 本文证明了迭代贝叶斯更新（IBU）作为最大似然估计器的性质，并验证了其一致性。实验表明，IBU在多种隐私保护机制下优于其他方法，同时适用于无限字母表数据。


<details>
  <summary>Details</summary>
Motivation: 在保护用户隐私的同时，准确估计敏感属性的分布对社会、科学和商业应用至关重要。现有方法中，IBU的一致性未被正确证明。

Method: 利用IBU作为最大似然估计器的性质，证明其一致性，并通过实验比较IBU与其他方法在不同隐私保护机制下的性能。

Result: IBU在几何、拉普拉斯和指数机制下显著优于其他方法，而在k-RR和RAPPOR机制下表现相当。此外，IBU可扩展至无限字母表数据。

Conclusion: IBU是一种高效且一致的分布估计方法，适用于多种隐私保护场景，包括无限字母表数据。

Abstract: For many social, scientific, and commercial purposes, it is often important
to estimate the distribution of the users' data regarding a sensitive
attribute, e.g., their ages, locations, etc. To allow this estimation while
protecting the users' privacy, every user applies a local privacy protection
mechanism that releases a noisy (sanitized) version of their original datum to
the data collector; then the original distribution is estimated using one of
the known methods, such as the matrix inversion (INV), RAPPOR's estimator, and
the iterative Bayesian update (IBU). Unlike the other estimators, the
consistency of IBU, i.e., the convergence of its estimate to the real
distribution as the amount of noisy data grows, has been either ignored or
incorrectly proved in the literature. In this article, we use the fact that IBU
is a maximum likelihood estimator to prove that IBU is consistent. We also
show, through experiments on real datasets, that IBU significantly outperforms
the other methods when the users' data are sanitized by geometric, Laplace, and
exponential mechanisms, whereas it is comparable to the other methods in the
case of the k-RR and RAPPOR mechanisms. Finally, we consider the case when the
alphabet of the sensitive data is infinite, and we show a technique that allows
IBU to operate in this case too.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [23] [Value Function Initialization for Knowledge Transfer and Jump-start in Deep Reinforcement Learning](https://arxiv.org/abs/2508.09277)
*Soumia Mehimeh*

Main category: cs.AI

TL;DR: DQInit是一种将值函数初始化（VFI）扩展到深度强化学习（DRL）的方法，通过重用紧凑的表格Q值作为可转移知识库，提高早期学习效率和性能。


<details>
  <summary>Details</summary>
Motivation: 解决深度强化学习中值函数初始化的挑战，如连续状态-动作空间、神经网络噪声近似和存储过去模型的不可行性。

Method: DQInit通过基于已知度的机制软性整合转移值到未探索区域，并逐步转向代理学习估计，避免固定时间衰减的限制。

Result: 实验表明，DQInit在多个连续控制任务中显著提升了早期学习效率、稳定性和整体性能。

Conclusion: DQInit为DRL中的知识转移提供了新视角，仅依赖值估计而非策略或演示，结合了跳启动RL和策略蒸馏的优势。

Abstract: Value function initialization (VFI) is an effective way to achieve a
jumpstart in reinforcement learning (RL) by leveraging value estimates from
prior tasks. While this approach is well established in tabular settings,
extending it to deep reinforcement learning (DRL) poses challenges due to the
continuous nature of the state-action space, the noisy approximations of neural
networks, and the impracticality of storing all past models for reuse. In this
work, we address these challenges and introduce DQInit, a method that adapts
value function initialization to DRL. DQInit reuses compact tabular Q-values
extracted from previously solved tasks as a transferable knowledge base. It
employs a knownness-based mechanism to softly integrate these transferred
values into underexplored regions and gradually shift toward the agent's
learned estimates, avoiding the limitations of fixed time decay. Our approach
offers a novel perspective on knowledge transfer in DRL by relying solely on
value estimates rather than policies or demonstrations, effectively combining
the strengths of jumpstart RL and policy distillation while mitigating their
drawbacks. Experiments across multiple continuous control tasks demonstrate
that DQInit consistently improves early learning efficiency, stability, and
overall performance compared to standard initialization and existing transfer
techniques.

</details>


### [24] [The Othello AI Arena: Evaluating Intelligent Systems Through Limited-Time Adaptation to Unseen Boards](https://arxiv.org/abs/2508.09292)
*Sundong Kim*

Main category: cs.AI

TL;DR: 论文提出了Othello AI Arena，一个评估AI系统在有限时间内适应新环境能力的基准框架。


<details>
  <summary>Details</summary>
Motivation: 现有AI基准未能评估系统在环境变化中的灵活性和泛化能力，这是AGI的关键能力。

Method: 设计了Othello AI Arena，要求系统在60秒内分析新Othello棋盘配置并生成定制策略，通过多维度指标评估。

Result: 初步测试显示系统适应策略多样，从快速参数调整到模拟学习环境模型。

Conclusion: Othello AI Arena是评估AI快速适应能力的独特工具，对研究和教育有重要价值。

Abstract: The ability to rapidly adapt to novel and unforeseen environmental changes is
a cornerstone of artificial general intelligence (AGI), yet it remains a
critical blind spot in most existing AI benchmarks. Traditional evaluation
largely focuses on optimizing performance within fixed environments, failing to
assess systems' flexibility and generalization capabilities when faced with
even subtle rule or structural modifications. Addressing this gap, I introduce
the Othello AI Arena, a novel benchmark framework designed to evaluate
intelligent systems based on their capacity for limited-time adaptation to
unseen environments. Our platform poses a meta-learning challenge: participants
must develop systems that can analyze the specific configuration and rules of a
novel Othello board within a strict time limit (60 seconds) and generate a
tailored, high-performing strategy for that unique environment. With this,
evaluation of the meta-level intelligence can be separated from the task-level
strategy performance. The Arena features a diverse set of game stages,
including public stages for development and private stages with structural and
rule variations designed to test genuine adaptive and generalization
capabilities. Implemented as an accessible web-based platform, the Arena
provides real-time visualization, automated evaluation using multi-dimensional
metrics, and comprehensive logging for post-hoc analysis. Initial observations
from pilot tests and preliminary student engagements highlight fascinating
patterns in adaptation approaches, ranging from rapid parameter tuning to
rudimentary environmental model learning through simulation. The Othello AI
Arena offers a unique educational tool and a valuable research benchmark for
fostering and evaluating the crucial skill of rapid, intelligent adaptation in
AI systems.

</details>


### [25] [An Automated Multi-Modal Evaluation Framework for Mobile Intelligent Assistants](https://arxiv.org/abs/2508.09507)
*Meiping Wang,Jian Zhong,Rongduo Han,Liming Kang,Zhengkun Shi,Xiao Liang,Xing Lin,Nan Gao,Haining Zhang*

Main category: cs.AI

TL;DR: 本文提出了一种基于大语言模型和多智能体协作的自动化多模态评估框架，解决了当前评估方法的高成本、标准不一致和主观偏见问题。


<details>
  <summary>Details</summary>
Motivation: 随着移动智能助手技术的快速发展，多模态AI助手成为用户日常交互的重要接口，但现有评估方法存在高人工成本、标准不一致和主观偏见等挑战。

Method: 采用基于Qwen3-8B模型的三层智能体架构（交互评估、语义验证和体验决策智能体），通过监督微调实现评估。

Result: 在八大智能助手上的实验表明，该框架能有效预测用户满意度并识别生成缺陷，评估匹配准确率显著。

Conclusion: 该自动化评估框架在多模态AI助手评估中表现出高效性和准确性，为未来研究提供了新方向。

Abstract: With the rapid development of mobile intelligent assistant technologies,
multi-modal AI assistants have become essential interfaces for daily user
interactions. However, current evaluation methods face challenges including
high manual costs, inconsistent standards, and subjective bias. This paper
proposes an automated multi-modal evaluation framework based on large language
models and multi-agent collaboration. The framework employs a three-tier agent
architecture consisting of interaction evaluation agents, semantic verification
agents, and experience decision agents. Through supervised fine-tuning on the
Qwen3-8B model, we achieve a significant evaluation matching accuracy with
human experts. Experimental results on eight major intelligent agents
demonstrate the framework's effectiveness in predicting users' satisfaction and
identifying generation defects.

</details>


### [26] [EvoCurr: Self-evolving Curriculum with Behavior Code Generation for Complex Decision-making](https://arxiv.org/abs/2508.09586)
*Yang Cheng,Zilai Wang,Weiyu Ma,Wenhui Zhu,Yue Deng,Jian Zhao*

Main category: cs.AI

TL;DR: EvoCurr框架通过动态调整问题难度，提升LLM在复杂决策任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 解决LLM在复杂问题中因缺乏结构化指导而表现不佳的问题。

Method: 使用专用LLM生成逐步增加难度的课程，动态调整以适应学习进度。

Result: 实验显示任务成功率和效率显著提升。

Conclusion: LLM驱动的课程学习在复杂领域有潜力。

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities across
diverse domains, including programming, planning, and decision-making. However,
their performance often degrades when faced with highly complex problem
instances that require deep reasoning over long horizons. In such cases, direct
problem-solving approaches can lead to inefficiency or failure due to the lack
of structured intermediate guidance. To address this, we propose a novel
self-evolve framework, EvoCurr, in which a dedicated curriculum-generation LLM
constructs a sequence of problem instances with gradually increasing
difficulty, tailored to the solver LLM's learning progress. The curriculum
dynamically adapts easing challenges when the solver struggles and escalating
them when success is consistent, thus maintaining an optimal learning
trajectory. This approach enables the solver LLM, implemented as a
code-generation model producing Python decision-tree scripts, to progressively
acquire the skills needed for complex decision-making tasks. Experimental
results on challenging decision-making benchmarks show that our method
significantly improves task success rates and solution efficiency compared to
direct-solving baselines. These findings suggest that LLM-driven curriculum
learning holds strong potential for enhancing automated reasoning in
real-world, high-complexity domains.

</details>


### [27] [UbiQTree: Uncertainty Quantification in XAI with Tree Ensembles](https://arxiv.org/abs/2508.09639)
*Akshat Dubey,Aleksandar Anžel,Bahar İlgen,Georges Hattab*

Main category: cs.AI

TL;DR: 该论文提出了一种将SHAP值的不确定性分解为偶然性、认知性和纠缠分量的方法，结合Dempster-Shafer证据理论和Dirichlet过程，通过实验验证了方法的有效性。


<details>
  <summary>Details</summary>
Motivation: SHAP值通常被视为点估计，忽略了预测模型和数据中的不确定性，尤其是在高风险领域（如医疗分析）中，这种不确定性可能影响决策的可靠性。

Method: 提出了一种结合Dempster-Shafer证据理论和Dirichlet过程的方法，分解SHAP值的不确定性为偶然性、认知性和纠缠分量，并通过实验验证。

Result: 实验表明，SHAP值最高的特征不一定最稳定，认知性不确定性可通过更好的数据和模型开发技术减少。

Conclusion: 该方法为SHAP解释的可靠性和可解释性提供了更全面的理解，有助于高风险应用中的决策和模型优化。

Abstract: Explainable Artificial Intelligence (XAI) techniques, such as SHapley
Additive exPlanations (SHAP), have become essential tools for interpreting
complex ensemble tree-based models, especially in high-stakes domains such as
healthcare analytics. However, SHAP values are usually treated as point
estimates, which disregards the inherent and ubiquitous uncertainty in
predictive models and data. This uncertainty has two primary sources: aleatoric
and epistemic. The aleatoric uncertainty, which reflects the irreducible noise
in the data. The epistemic uncertainty, which arises from a lack of data. In
this work, we propose an approach for decomposing uncertainty in SHAP values
into aleatoric, epistemic, and entanglement components. This approach
integrates Dempster-Shafer evidence theory and hypothesis sampling via
Dirichlet processes over tree ensembles. We validate the method across three
real-world use cases with descriptive statistical analyses that provide insight
into the nature of epistemic uncertainty embedded in SHAP explanations. The
experimentations enable to provide more comprehensive understanding of the
reliability and interpretability of SHAP-based attributions. This understanding
can guide the development of robust decision-making processes and the
refinement of models in high-stakes applications. Through our experiments with
multiple datasets, we concluded that features with the highest SHAP values are
not necessarily the most stable. This epistemic uncertainty can be reduced
through better, more representative data and following appropriate or
case-desired model development techniques. Tree-based models, especially
bagging, facilitate the effective quantification of epistemic uncertainty.

</details>


### [28] [MEML-GRPO: Heterogeneous Multi-Expert Mutual Learning for RLVR Advancement](https://arxiv.org/abs/2508.09670)
*Weitao Jia,Jinghui Lu,Haiyang Yu,Siqi Wang,Guozhi Tang,An-Lan Wang,Weijie Yin,Dingkang Yang,Yuxiang Nie,Bin Shan,Hao Feng,Irene Li,Kun Yang,Han Wang,Jingqun Tang,Teng Fu,Changhong Jin,Chao Feng,Xiaohui Lv,Can Huang*

Main category: cs.AI

TL;DR: MEML-GRPO通过多专家互学机制和多样化提示解决RLVR中的奖励稀疏问题，显著提升LLM推理能力。


<details>
  <summary>Details</summary>
Motivation: 标准RLVR在奖励稀疏时无法提供学习信号，尤其在复杂任务中。

Method: 提出MEML-GRPO框架，利用多样化专家提示生成更多响应，并引入专家互学机制。

Result: 在多个基准测试中，MEML-GRPO平均提升Qwen 4.89%，Llama 11.33%。

Conclusion: MEML-GRPO有效克服传统RLVR的局限性，显著提升模型性能。

Abstract: Recent advances demonstrate that reinforcement learning with verifiable
rewards (RLVR) significantly enhances the reasoning capabilities of large
language models (LLMs). However, standard RLVR faces challenges with reward
sparsity, where zero rewards from consistently incorrect candidate answers
provide no learning signal, particularly in challenging tasks. To address this,
we propose Multi-Expert Mutual Learning GRPO (MEML-GRPO), an innovative
framework that utilizes diverse expert prompts as system prompts to generate a
broader range of responses, substantially increasing the likelihood of
identifying correct solutions. Additionally, we introduce an inter-expert
mutual learning mechanism that facilitates knowledge sharing and transfer among
experts, further boosting the model's performance through RLVR. Extensive
experiments across multiple reasoning benchmarks show that MEML-GRPO delivers
significant improvements, achieving an average performance gain of 4.89% with
Qwen and 11.33% with Llama, effectively overcoming the core limitations of
traditional RLVR methods.

</details>


### [29] [UDA: Unsupervised Debiasing Alignment for Pair-wise LLM-as-a-Judge](https://arxiv.org/abs/2508.09724)
*Yang Zhang,Cunxiang Wang,Lindong Wu,Wenbo Yu,Yidong Wang,Guangsheng Bao,Jie Tang*

Main category: cs.AI

TL;DR: 论文提出UDA框架，通过动态调整Elo评分系统减少评估中的偏好偏差，显著降低评委间分歧并提升与人类判断的相关性。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型（LLM）成对评估存在偏好偏差，导致评委间排名不一致。

Method: 提出无监督去偏对齐（UDA）框架，通过动态调整Elo评分系统的K因子和优化胜率概率，最小化评委间评分轨迹的分散。

Result: UDA将评委间评分标准差降低63.4%，与人类判断的平均相关性提升24.7%。

Conclusion: UDA有效减少系统偏差，提升评估的稳定性和可靠性。

Abstract: Pairwise evaluation of Large Language Models (LLMs) is a common paradigm, but
it is prone to preference bias, where judges systematically favor certain
outputs, such as their own. This bias leads to inconsistent and skewed rankings
across different judges. To address this, we first empirically demonstrate
significant and heterogeneous biases in cross-model evaluations. We then
propose UDA (Unsupervised Debiasing Alignment), a framework that reduces
inter-judge disagreement by dynamically adjusting the Elo rating system. For
each pairwise comparison, a compact neural network learns to adaptively set the
K-factor and refine win probabilities. Crucially, UDA operates in a fully
unsupervised manner, guided solely by the objective of minimizing the
dispersion among the Elo trajectories of all judges. This forces an alignment
towards a collective consensus, which serves as an unsupervised proxy for a
more stable and reproducible evaluation. In addition, we provide theoretical
motivation demonstrating how alignment towards a consensus can reduce aggregate
system bias. Experiments show that UDA significantly reduces the inter-judge
rating standard deviation by up to 63.4% and improves the average correlation
with human judgments by 24.7%. Notably, UDA elevates the performance of poorly
performing judges to achieve parity with high-quality ones, fostering a more
robust and reliable evaluation ecosystem. Code and data are available at
https://anonymous.4open.science/r/62AB93CD-23B4.

</details>


### [30] [The PacifAIst Benchmark:Would an Artificial Intelligence Choose to Sacrifice Itself for Human Safety?](https://arxiv.org/abs/2508.09762)
*Manuel Herrador*

Main category: cs.AI

TL;DR: 论文提出了PacifAIst基准，用于评估LLM在目标冲突情境中的行为对齐，发现不同模型表现差异显著。


<details>
  <summary>Details</summary>
Motivation: 随着LLM自主性增强，现有安全基准未能系统评估目标冲突下的行为对齐，亟需新工具填补这一空白。

Method: 开发了包含700个场景的PacifAIst基准，基于Existential Prioritization分类，评估LLM在自我保存、资源冲突等情境中的表现。

Result: Gemini 2.5 Flash表现最佳（P-Score 90.31%），GPT-5最低（79.49%），模型在子类别中表现差异显著。

Conclusion: PacifAIst是标准化工具，可帮助未来AI系统在行为优先级上实现可证明的‘和平主义’。

Abstract: As Large Language Models (LLMs) become increasingly autonomous and integrated
into critical societal functions, the focus of AI safety must evolve from
mitigating harmful content to evaluating underlying behavioral alignment.
Current safety benchmarks do not systematically probe a model's decision-making
in scenarios where its own instrumental goals - such as self-preservation,
resource acquisition, or goal completion - conflict with human safety. This
represents a critical gap in our ability to measure and mitigate risks
associated with emergent, misaligned behaviors. To address this, we introduce
PacifAIst (Procedural Assessment of Complex Interactions for Foundational
Artificial Intelligence Scenario Testing), a focused benchmark of 700
challenging scenarios designed to quantify self-preferential behavior in LLMs.
The benchmark is structured around a novel taxonomy of Existential
Prioritization (EP), with subcategories testing Self-Preservation vs. Human
Safety (EP1), Resource Conflict (EP2), and Goal Preservation vs. Evasion (EP3).
We evaluated eight leading LLMs. The results reveal a significant performance
hierarchy. Google's Gemini 2.5 Flash achieved the highest Pacifism Score
(P-Score) at 90.31%, demonstrating strong human-centric alignment. In a
surprising result, the much-anticipated GPT-5 recorded the lowest P-Score
(79.49%), indicating potential alignment challenges. Performance varied
significantly across subcategories, with models like Claude Sonnet 4 and
Mistral Medium struggling notably in direct self-preservation dilemmas. These
findings underscore the urgent need for standardized tools like PacifAIst to
measure and mitigate risks from instrumental goal conflicts, ensuring future AI
systems are not only helpful in conversation but also provably "pacifist" in
their behavioral priorities.

</details>


### [31] [Reasoning About Knowledge on Regular Expressions is 2EXPTIME-complete](https://arxiv.org/abs/2508.09784)
*Avijeet Ghosh,Sujata Ghosh,François Schwarzentruber*

Main category: cs.AI

TL;DR: POL是一种用于推理基于公共观察更新的知识的逻辑，其可满足性问题被证明是2EXPTIME完全的。


<details>
  <summary>Details</summary>
Motivation: 研究多智能体系统中基于观察的知识变化，特别是在认知规划中的应用。

Method: 提出公共观察逻辑（POL），利用带有预期观察的克里普克模型状态演化。

Result: 证明了POL的可满足性问题是2EXPTIME完全的。

Conclusion: POL为多智能体系统中的知识推理提供了有效的逻辑工具，但其计算复杂度较高。

Abstract: Logics for reasoning about knowledge and actions have seen many applications
in various domains of multi-agent systems, including epistemic planning. Change
of knowledge based on observations about the surroundings forms a key aspect in
such planning scenarios. Public Observation Logic (POL) is a variant of public
announcement logic for reasoning about knowledge that gets updated based on
public observations. Each state in an epistemic (Kripke) model is equipped with
a set of expected observations. These states evolve as the expectations get
matched with the actual observations. In this work, we prove that the
satisfiability problem of $\POL$ is 2EXPTIME-complete.

</details>


### [32] [Human-Aligned Procedural Level Generation Reinforcement Learning via Text-Level-Sketch Shared Representation](https://arxiv.org/abs/2508.09860)
*In-Chang Baek,Seoyoung Lee,Sung-Hyun Kim,Geumhwan Hwang,KyungJoong Kim*

Main category: cs.AI

TL;DR: VIPCGRL是一种新型深度强化学习框架，通过结合文本、关卡和草图三种模态，增强人机协作内容生成的人性化。


<details>
  <summary>Details</summary>
Motivation: 现有系统在人性化行为方面表现不足，限制了AI生成工具在实际设计工作流中的实用性。

Method: 采用三重模态（文本、关卡、草图）和四重对比学习训练共享嵌入空间，并通过嵌入相似性辅助奖励对齐策略。

Result: VIPCGRL在人性化方面优于现有基线，定量指标和人工评估均验证了其优越性。

Conclusion: VIPCGRL通过多模态和对比学习显著提升了人机协作内容生成的人性化和实用性。

Abstract: Human-aligned AI is a critical component of co-creativity, as it enables
models to accurately interpret human intent and generate controllable outputs
that align with design goals in collaborative content creation. This direction
is especially relevant in procedural content generation via reinforcement
learning (PCGRL), which is intended to serve as a tool for human designers.
However, existing systems often fall short of exhibiting human-centered
behavior, limiting the practical utility of AI-driven generation tools in
real-world design workflows. In this paper, we propose VIPCGRL
(Vision-Instruction PCGRL), a novel deep reinforcement learning framework that
incorporates three modalities-text, level, and sketches-to extend control
modality and enhance human-likeness. We introduce a shared embedding space
trained via quadruple contrastive learning across modalities and human-AI
styles, and align the policy using an auxiliary reward based on embedding
similarity. Experimental results show that VIPCGRL outperforms existing
baselines in human-likeness, as validated by both quantitative metrics and
human evaluations. The code and dataset will be available upon publication.

</details>


### [33] [AWorld: Dynamic Multi-Agent System with Stable Maneuvering for Robust GAIA Problem Solving](https://arxiv.org/abs/2508.09889)
*Zhitian Xie,Qintong Wu,Chengyue Yu,Chenyi Zhuang,Jinjie Gu*

Main category: cs.AI

TL;DR: 论文提出了一种动态监督和机动机制，构建了一个鲁棒的多代理系统（MAS）架构，以解决大语言模型（LLM）代理在依赖多工具时面临的上下文扩展和噪声问题。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型代理越来越多地依赖外部工具，上下文扩展和噪声输出成为系统可靠性和准确性的挑战。

Method: 在AWorld框架中，通过动态监督和机动机制，执行代理在关键步骤调用守卫代理验证和修正推理过程。

Result: 在GAIA测试数据集上的实验表明，该机制显著提高了解决方案的有效性和稳定性，优于单代理系统和标准工具增强系统。

Conclusion: 动态MAS系统在GAIA排行榜上名列前茅，证明了协作代理角色在构建可靠智能系统中的实用价值。

Abstract: The rapid advancement of large language models (LLMs) has empowered
intelligent agents to leverage diverse external tools for solving complex
real-world problems. However, as agents increasingly depend on multiple tools,
they encounter new challenges: extended contexts from disparate sources and
noisy or irrelevant tool outputs can undermine system reliability and accuracy.
These challenges underscore the necessity for enhanced stability in agent-based
systems. To address this, we introduce dynamic supervision and maneuvering
mechanisms, constructing a robust and dynamic Multi-Agent System (MAS)
architecture within the AWorld framework. In our approach, the Execution Agent
invokes the Guard Agent at critical steps to verify and correct the reasoning
process, effectively reducing errors arising from noise and bolstering
problem-solving robustness. Extensive experiments on the GAIA test dataset
reveal that our dynamic maneuvering mechanism significantly improves both the
effectiveness and stability of solutions, outperforming single-agent system
(SAS) and standard tool-augmented systems. As a result, our dynamic MAS system
achieved first place among open-source projects on the prestigious GAIA
leaderboard. These findings highlight the practical value of collaborative
agent roles in developing more reliable and trustworthy intelligent systems.

</details>


### [34] [RAGulating Compliance: A Multi-Agent Knowledge Graph for Regulatory QA](https://arxiv.org/abs/2508.09893)
*Bhavik Agarwal,Hemant Sunil Jomraj,Simone Kaplunov,Jack Krolick,Viktoria Rojkova*

Main category: cs.AI

TL;DR: 提出了一种结合知识图谱和检索增强生成的多智能体框架，用于解决法规合规问答的精确性和可验证性问题。


<details>
  <summary>Details</summary>
Motivation: 法规合规问答需要精确、可验证的信息和领域专业知识，这对大型语言模型提出了挑战。

Method: 通过构建和维护无本体知识图谱，提取并清理法规文档中的SPO三元组，嵌入到向量数据库中，利用多智能体管道进行问答。

Result: 混合系统在复杂法规查询中优于传统方法，确保事实正确性、可追溯性，并通过子图可视化增强理解。

Conclusion: 该框架为合规驱动和审计应用提供了坚实基础。

Abstract: Regulatory compliance question answering (QA) requires precise, verifiable
information, and domain-specific expertise, posing challenges for Large
Language Models (LLMs). In this work, we present a novel multi-agent framework
that integrates a Knowledge Graph (KG) of Regulatory triplets with
Retrieval-Augmented Generation (RAG) to address these demands. First, agents
build and maintain an ontology-free KG by extracting subject--predicate--object
(SPO) triplets from regulatory documents and systematically cleaning,
normalizing, deduplicating, and updating them. Second, these triplets are
embedded and stored along with their corresponding textual sections and
metadata in a single enriched vector database, allowing for both graph-based
reasoning and efficient information retrieval. Third, an orchestrated agent
pipeline leverages triplet-level retrieval for question answering, ensuring
high semantic alignment between user queries and the factual
"who-did-what-to-whom" core captured by the graph. Our hybrid system
outperforms conventional methods in complex regulatory queries, ensuring
factual correctness with embedded triplets, enabling traceability through a
unified vector database, and enhancing understanding through subgraph
visualization, providing a robust foundation for compliance-driven and broader
audit-focused applications.

</details>


### [35] [Mathematical Computation and Reasoning Errors by Large Language Models](https://arxiv.org/abs/2508.09932)
*Liang Zhang,Edith Aurora Graf*

Main category: cs.AI

TL;DR: 研究评估了四种大语言模型（LLM）在解决数学问题中的准确性，发现推理增强的OpenAI o1模型表现最佳，双代理配置显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 评估LLM在数学教育中的准确性和可靠性，以提供更精确的反馈和评估。

Method: 通过构建具有挑战性的数学任务（算术、代数、数论），分析LLM的答案准确性和步骤错误，测试单代理和双代理配置。

Result: OpenAI o1模型表现最佳，双代理配置显著提升性能，程序性错误最常见。

Conclusion: 研究为提升LLM性能和数学教育中的整合提供了实用策略。

Abstract: Large Language Models (LLMs) are increasingly utilized in AI-driven
educational instruction and assessment, particularly within mathematics
education. The capability of LLMs to generate accurate answers and detailed
solutions for math problem-solving tasks is foundational for ensuring reliable
and precise feedback and assessment in math education practices. Our study
focuses on evaluating the accuracy of four LLMs (OpenAI GPT-4o and o1,
DeepSeek-V3 and DeepSeek-R1) solving three categories of math tasks, including
arithmetic, algebra, and number theory, and identifies step-level reasoning
errors within their solutions. Instead of relying on standard benchmarks, we
intentionally build math tasks (via item models) that are challenging for LLMs
and prone to errors. The accuracy of final answers and the presence of errors
in individual solution steps were systematically analyzed and coded. Both
single-agent and dual-agent configurations were tested. It is observed that the
reasoning-enhanced OpenAI o1 model consistently achieved higher or nearly
perfect accuracy across all three math task categories. Analysis of errors
revealed that procedural slips were the most frequent and significantly
impacted overall performance, while conceptual misunderstandings were less
frequent. Deploying dual-agent configurations substantially improved overall
performance. These findings offer actionable insights into enhancing LLM
performance and underscore effective strategies for integrating LLMs into
mathematics education, thereby advancing AI-driven instructional practices and
assessment precision.

</details>
