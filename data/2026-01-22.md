<div id=toc></div>

# Table of Contents

- [cs.CR](#cs.CR) [Total: 47]
- [cs.SE](#cs.SE) [Total: 46]
- [cs.AI](#cs.AI) [Total: 65]


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [1] [Privacy-Preserving Black-Box Optimization (PBBO): Theory and the Model-Based Algorithm DFOp](https://arxiv.org/abs/2601.11570)
*Pengcheng Xie*

Main category: cs.CR

TL;DR: 本文提出DFOp算法解决无约束隐私保护黑盒优化问题，包含新的二次模型更新公式、收敛性分析和差分隐私机制，是首个能处理步加密和隐私保护黑盒优化的无导数求解器。


<details>
  <summary>Details</summary>
Motivation: 解决无约束隐私保护黑盒优化问题，填补无导数优化与隐私保护结合的空白，处理加密/变换目标函数的优化挑战。

Method: 提出新的无导数求解器DFOp，包含新的二次模型函数更新公式，采用最小Frobenius范数更新策略，并设计两种差分隐私噪声添加机制。

Result: DFOp在数值实验中表现优于对比算法，能有效解决加密目标函数优化问题，并证明收敛性，是首个能精确处理步加密和隐私保护黑盒优化的无导数求解器。

Conclusion: DFOp成功解决了无约束隐私保护黑盒优化问题，回答了无导数优化与隐私保护结合的开源问题，为加密目标函数优化提供了有效工具。

Abstract: This paper focuses on solving unconstrained privacy-preserving black-box optimization (PBBO), its corresponding least Frobenius norm updating of quadratic models, and the differentially privacy mechanisms for PBBO. Optimization problems with transformed/encrypted objective functions aim to minimize F(x), which is encrypted/transformed/encrypted to F_k(x) as the output at the k-th iteration. A new derivative-free solver named DFOp, with its implementation, is proposed in this paper, which has a new updating formula for the quadratic model functions. The convergence of DFOp for solving problems with transformed/encrypted objective functions is given. Other analyses, including the new model updating formula and the analysis of the transformation's impact to model functions are presented. We propose two differentially private noise-adding mechanisms for privacy-preserving black-box optimization. Numerical results show that DFOp performs better than compared algorithms. To the best of our knowledge, DFOp is the first derivative-free solver that can solve black-box optimization problems with step-encryption and privacy-preserving black-box problems exactly, which also tries to answer the open question about the combination of derivative-free optimization and privacy.

</details>


### [2] [Semantic Differentiation for Tackling Challenges in Watermarking Low-Entropy Constrained Generation Outputs](https://arxiv.org/abs/2601.11629)
*Nghia T. Le,Alan Ritter,Kartik Goyal*

Main category: cs.CR

TL;DR: SeqMark是一种针对低熵约束生成任务的序列级水印算法，解决了现有token级水印在约束生成任务中的不足，通过语义区分平衡输出质量、水印可检测性和不可感知性。


<details>
  <summary>Details</summary>
Motivation: 现有语言模型水印方法在开放式生成任务中有效，但在低熵输出空间的约束生成任务（如机器翻译、代码生成、摘要生成）中表现不佳，需要专门针对此类任务设计的水印算法。

Method: SeqMark是一种序列级水印算法，采用语义区分策略。它改进了现有token级水印算法未能充分利用序列级熵的问题，并解决了先前序列级水印算法中的"区域崩溃"问题。该方法将高概率输出子空间进行区分，并将其划分为有效和无效区域，确保高质量输出在所有区域中均匀分布。

Result: 在机器翻译、代码生成和抽象摘要等多种约束生成任务上，SeqMark显著提高了水印检测准确率（F1分数最高提升28%），同时保持了高生成质量。

Conclusion: SeqMark通过序列级水印和语义区分策略，有效解决了约束生成任务中的水印问题，在保持输出质量的同时显著提升了水印检测性能，为低熵输出空间的LM水印提供了有效解决方案。

Abstract: We demonstrate that while the current approaches for language model watermarking are effective for open-ended generation, they are inadequate at watermarking LM outputs for constrained generation tasks with low-entropy output spaces. Therefore, we devise SeqMark, a sequence-level watermarking algorithm with semantic differentiation that balances the output quality, watermark detectability, and imperceptibility. It improves on the shortcomings of the prevalent token-level watermarking algorithms that cause under-utilization of the sequence-level entropy available for constrained generation tasks. Moreover, we identify and improve upon a different failure mode we term region collapse, associated with prior sequence-level watermarking algorithms. This occurs because the pseudorandom partitioning of semantic space for watermarking in these approaches causes all high-probability outputs to collapse into either invalid or valid regions, leading to a trade-off in output quality and watermarking effectiveness. SeqMark instead, differentiates the high-probable output subspace and partitions it into valid and invalid regions, ensuring the even spread of high-quality outputs among all the regions. On various constrained generation tasks like machine translation, code generation, and abstractive summarization, SeqMark substantially improves watermark detection accuracy (up to 28% increase in F1) while maintaining high generation quality.

</details>


### [3] [Serverless AI Security: Attack Surface Analysis and Runtime Protection Mechanisms for FaaS-Based Machine Learning](https://arxiv.org/abs/2601.11664)
*Chetan Pathade,Vinod Dhimam,Sheheryar Ahmad,Ilsa Lareb*

Main category: cs.CR

TL;DR: 本文首次对无服务器环境中的机器学习工作负载进行全面的安全分析，识别了五个主要攻击面类别，提出了Serverless AI Shield防御框架，并在三大云平台上验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 随着无服务器计算广泛采用（超过70%的AWS组织使用）和机器学习推理工作负载向FaaS平台迁移，这种融合带来了关键的安全挑战。AI/ML漏洞增加了220%，无服务器计算的碎片化架构带来了不同于传统云部署的新安全问题。

Method: 系统性地将攻击面分为五个类别进行分析：函数级漏洞、模型特定威胁、基础设施攻击、供应链风险和IAM复杂性。在AWS Lambda、Azure Functions和Google Cloud Functions上进行实证评估，演示真实攻击场景并量化安全影响。提出Serverless AI Shield多层防御框架，包括部署前验证、运行时监控和执行后取证。

Result: 评估显示Serverless AI Shield达到94%的检测率，同时将推理延迟的性能开销保持在9%以下。发布了开源安全工具包，帮助从业者评估和加固无服务器AI部署。

Conclusion: 这是对无服务器环境中机器学习工作负载的首次全面安全分析，提出的Serverless AI Shield框架能够有效防御识别出的安全威胁，推动了更具弹性的云原生机器学习系统的发展。

Abstract: Serverless computing has achieved widespread adoption, with over 70% of AWS organizations using serverless solutions [1]. Meanwhile, machine learning inference workloads increasingly migrate to Function-as-a-Service (FaaS) platforms for their scalability and cost-efficiency [2], [3], [4]. However, this convergence introduces critical security challenges, with recent reports showing a 220% increase in AI/ML vulnerabilities [5] and serverless computing's fragmented architecture raises new security concerns distinct from traditional cloud deployments [6], [7]. This paper presents the first comprehensive security analysis of machine learning workloads in serverless environments. We systematically characterize the attack surface across five categories: function-level vulnerabilities (cold start exploitation, dependency poisoning), model-specific threats (API-based extraction, adversarial inputs), infrastructure attacks (cross-function contamination, privilege escalation), supply chain risks (malicious layers, backdoored libraries), and IAM complexity (ephemeral nature, serverless functions). Through empirical assessments across AWS Lambda, Azure Functions, and Google Cloud Functions, we demonstrate real-world attack scenarios and quantify their security impact. We propose Serverless AI Shield (SAS), a multi-layered defense framework providing pre-deployment validation, runtime monitoring, and post-execution forensics. Our evaluation shows SAS achieves 94% detection rates while maintaining performance overhead below 9% for inference latency. We release an open-source security toolkit to enable practitioners to assess and harden their serverless AI deployments, advancing the field toward more resilient cloud-native machine learning systems.

</details>


### [4] [A Survey on Mapping Digital Systems with Bill of Materials: Development, Practices, and Challenges](https://arxiv.org/abs/2601.11678)
*Shuai Zhang,Minzhao Lyu,Hassan Habibi Gharakheili*

Main category: cs.CR

TL;DR: 本文首次对跨领域BOM（物料清单）发展与实践进行全面综述，涵盖硬件、软件、AI模型、数据集和加密资产，分析其演变阶段、行业实践、应用场景及研究挑战。


<details>
  <summary>Details</summary>
Motivation: 现代数字生态系统日益复杂，组件依赖关系难以理解和管理，BOM作为结构化文档工具可提高数字供应链的可见性和安全性，需要系统梳理其跨领域发展现状。

Method: 采用跨领域综述方法，首先分析BOM框架在三个阶段的演变（预发展期、初始期、加速期），总结核心原则、关键利益相关者和标准化工作；然后审查行业实践中的BOM数据生成、质量评估和安全共享；最后分析BOM数据的下游应用场景。

Result: 系统梳理了BOM在硬件、软件、AI模型、数据集和加密资产等领域的标准化进展，总结了行业实践模式，识别了依赖建模、合规验证、运营风险评估和漏洞跟踪等关键应用场景，并指出了当前BOM框架的学术改进方向。

Conclusion: BOM框架在数字供应链管理中发挥重要作用，但存在四个关键差距限制了其可用性和可靠性，需要未来研究解决，特别是在新兴数据生态系统和AI供应链领域。

Abstract: Modern digital ecosystems, spanning software, hardware, learning models, datasets, and cryptographic products, continue to grow in complexity, making it difficult for organizations to understand and manage component dependencies. Bills of Materials (BOMs) have emerged as a structured way to document product components, their interrelationships, and key metadata, improving visibility and security across digital supply chains. This survey provides the first comprehensive cross-domain review of BOM developments and practices. We start by examining the evolution of BOM frameworks in three stages (i.e., pre-development, initial, and accelerated) and summarizing their core principles, key stakeholders, and standardization efforts for hardware, software, artificial intelligence (AI) models, datasets, and cryptographic assets. We then review industry practices for generating BOM data, evaluating its quality, and securely sharing it. Next, we review practical downstream uses of BOM data, including dependency modeling, compliance verification, operational risk assessment, and vulnerability tracking. We also discuss academic efforts to address limitations in current BOM frameworks through refinements, extensions, or new models tailored to emerging domains such as data ecosystems and AI supply chains. Finally, we identify four key gaps that limit the usability and reliability of today's BOM frameworks, motivating future research directions.

</details>


### [5] [Attesting Model Lineage by Consisted Knowledge Evolution with Fine-Tuning Trajectory](https://arxiv.org/abs/2601.11683)
*Zhuoyi Shang,Jiasen Li,Pengzhen Chen,Yanwei Liu,Xiaoyan Gu,Weiping Wang*

Main category: cs.CR

TL;DR: 提出基于知识进化和参数修改联合轨迹的模型谱系认证框架，通过模型编辑量化参数变化，知识向量化机制提取进化知识表示，验证知识关系算术一致性，实现可靠谱系验证


<details>
  <summary>Details</summary>
Motivation: 深度学习中的微调技术产生了模型间的谱系关系，这为解决模型未授权分发和来源虚假声明等安全问题提供了新视角。现有方法主要依赖静态架构相似性，不足以捕捉知识动态演化，无法反映真正的谱系关系

Method: 1) 利用模型编辑量化微调引入的参数级变化；2) 提出知识向量化机制，借助探针样本将编辑模型中进化的知识提炼为紧凑表示，探针策略适应不同类型模型家族；3) 基于这些嵌入验证模型间知识关系的算术一致性

Result: 在多种现实世界对抗场景下的广泛实验评估表明该方法有效且具有韧性，在包括分类器、扩散模型和大语言模型在内的广泛模型类型中都能实现可靠的谱系验证

Conclusion: 通过验证知识进化和参数修改的联合轨迹，提出了一个新颖的模型谱系认证框架，能够有效解决开放权重模型库中缺乏稳健谱系验证机制的安全问题

Abstract: The fine-tuning technique in deep learning gives rise to an emerging lineage relationship among models. This lineage provides a promising perspective for addressing security concerns such as unauthorized model redistribution and false claim of model provenance, which are particularly pressing in \textcolor{blue}{open-weight model} libraries where robust lineage verification mechanisms are often lacking. Existing approaches to model lineage detection primarily rely on static architectural similarities, which are insufficient to capture the dynamic evolution of knowledge that underlies true lineage relationships. Drawing inspiration from the genetic mechanism of human evolution, we tackle the problem of model lineage attestation by verifying the joint trajectory of knowledge evolution and parameter modification. To this end, we propose a novel model lineage attestation framework. In our framework, model editing is first leveraged to quantify parameter-level changes introduced by fine-tuning. Subsequently, we introduce a novel knowledge vectorization mechanism that refines the evolved knowledge within the edited models into compact representations by the assistance of probe samples. The probing strategies are adapted to different types of model families. These embeddings serve as the foundation for verifying the arithmetic consistency of knowledge relationships across models, thereby enabling robust attestation of model lineage. Extensive experimental evaluations demonstrate the effectiveness and resilience of our approach in a variety of adversarial scenarios in the real world. Our method consistently achieves reliable lineage verification across a broad spectrum of model types, including classifiers, diffusion models, and large language models.

</details>


### [6] [On Abnormal Execution Timing of Conditional Jump Instructions](https://arxiv.org/abs/2601.11696)
*Annika Wilde,Samira Briongos,Claudio Soriente,Ghassan Karame*

Main category: cs.CR

TL;DR: 研究发现现代处理器中条件跳转指令的执行时间会因操作数和系统优化而产生变化，这种时序变化源于微操作缓存放置和L1指令缓存中的跳转偏移，可以通过32字节对齐来避免，并能提升加密库性能2.15%平均（最高10.54%），同时可作为16.14 Mbps的隐蔽信道。


<details>
  <summary>Details</summary>
Motivation: 现代计算架构中指令执行时间会受操作数和系统优化（如分支预测和推测执行）影响，本文旨在系统测量和分析条件跳转指令的时序变化特性，特别是在宏融合指令中的表现。

Method: 系统测量和分析条件跳转指令的时序变化，研究宏融合指令在二进制中的放置位置影响，测量微操作缓存放置和L1指令缓存跳转偏移的影响，在Skylake、Coffee Lake、Kaby Lake等多种微架构上进行验证，并在Ubuntu 24.04、Windows 10 Pro和多个开源加密库的大规模二进制集上进行实验。

Result: 时序变化在不同微架构和实际实现中表现一致，通过32字节对齐宏融合指令可以避免这种时序变化，在加密库中平均提升性能2.15%（最高10.54%），同时这种时序变化可作为隐蔽信道，最大吞吐量达16.14 Mbps。

Conclusion: 现代处理器中条件跳转指令的时序变化是普遍存在的，源于微操作缓存和L1指令缓存机制，通过32字节对齐可以有效避免这种变化并提升性能，同时这种变化也可被利用为隐蔽信道。

Abstract: An extensive line of work on modern computing architectures has shown that the execution time of instructions can (i) depend on the operand of the instruction or (ii) be influenced by system optimizations, e.g., branch prediction and speculative execution paradigms.
  In this paper, we systematically measure and analyze timing variabilities in conditional jump instructions that can be macro-fused with a preceding instruction, depending on their placement within the binary. Our measurements indicate that these timing variations stem from the micro-op cache placement and the jump's offset in the L1 instruction cache of modern processors. We demonstrate that this behavior is consistent across multiple microarchitectures, including Skylake, Coffee Lake, and Kaby Lake, as well as various real-world implementations. We confirm the prevalence of this variability through extensive experiments on a large-scale set of popular binaries, including libraries from Ubuntu 24.04, Windows 10 Pro, and several open-source cryptographic libraries. We also show that one can easily avoid this timing variability by ensuring that macro-fusible instructions are 32-byte aligned - an approach initially suggested in 2019 by Intel in an overlooked short report. We quantify the performance impact of this approach across the cryptographic libraries, showing a speedup of 2.15% on average (and up to 10.54%) when avoiding the timing variability. As a by-product, we show that this variability can be exploited as a covert channel, achieving a maximum throughput of 16.14 Mbps.

</details>


### [7] [DROIDCCT: Cryptographic Compliance Test via Trillion-Scale Measurement](https://arxiv.org/abs/2601.11745)
*Daniel Moghimi,Alexandru-Cosmin Mihai,Borbala Benko,Catherine Vlasov,Elie Bursztein,Kurt Thomas,Laszlo Siroki,Pedro Barbosa,Remi Audebert*

Main category: cs.CR

TL;DR: DroidCCT是一个分布式测试框架，用于评估Android生态系统中密码学实现的各种故障和漏洞规模，通过分析数十亿设备上的密码学操作样本，发现了制造商和芯片组之间的实现差异和安全隐患。


<details>
  <summary>Details</summary>
Motivation: 评估Android生态系统中密码学实现的故障和漏洞规模，了解不同制造商和芯片组之间密码学实现的异质性及其对安全性的影响。

Method: 开发DroidCCT分布式测试框架，通过被动分析Android Keystore密码学操作的执行工件，从5亿台设备收集数万亿个样本，应用多种分析技术评估密码学输出质量和底层实现。

Result: 研究发现不同制造商和芯片组存在多种密码学实现漏洞模式，密码学实现的异质性导致各种密码函数可用性和可靠性不均，发现了弱随机参数生成和时序侧信道等缺陷。

Conclusion: 研究强调了容错和抗侧信道密码学的重要性，以及透明公开测试这些实现的能力的必要性，揭示了Android生态系统密码学实现的安全隐患。

Abstract: We develop DroidCCT, a distributed test framework to evaluate the scale of a wide range of failures/bugs in cryptography for end users. DroidCCT relies on passive analysis of artifacts from the execution of cryptographic operations in the Android ecosystem to identify weak implementations. We collect trillions of samples from cryptographic operations of Android Keystore on half a billion devices and apply severalanalysis techniques to evaluate the quality of cryptographic output from these devices and their underlying implementations. Our study reveals several patterns of bugs and weakness in cryptographic implementations from various manufacturers and chipsets. We show that the heterogeneous nature of cryptographic implementations results in non-uniform availability and reliability of various cryptographic functions. More importantly, flaws such as the use of weakly-generated random parameters, and timing side channels may surface across deployments of cryptography. Our results highlight the importance of fault- and side-channel-resistant cryptography and the ability to transparently and openly test these implementations.

</details>


### [8] [ARM MTE Performance in Practice (Extended Version)](https://arxiv.org/abs/2601.11786)
*Taehyun Noh,Yingchen Wang,Tal Garfinkel,Mahesh Madhav,Daniel Moghimi,Mattan Erez,Shravan Narayan*

Main category: cs.CR

TL;DR: 该论文首次全面分析了ARM MTE硬件在四种不同微架构上的性能表现，包括Google Pixel 8/9的A7x、A5x、Cortex-X核心，AmpereOne CPU核心，以及苹果M5芯片的初步分析。研究发现MTE在内存安全应用中通常有适度开销，但在某些基准测试中性能下降可达6.64倍。


<details>
  <summary>Details</summary>
Motivation: MTE（内存标签扩展）是ARMv8.5引入的硬件安全特性，但之前缺乏对不同微架构性能影响的全面分析。研究旨在填补这一空白，为开发者和硬件设计者提供实际性能数据。

Method: 在多种ARM微架构上测试MTE性能：Google Pixel 8/9的Big（A7x）、Little（A5x）、Performance（Cortex-X）核心，AmpereOne CPU核心，以及苹果M5芯片的初步分析。使用SPEC CPU基准测试和服务器工作负载（RocksDB、Nginx、PostgreSQL、Memcached）评估MTE在概率性内存安全方面的性能。同时分析MTE在内存追踪、TOCTOU预防、沙箱和CFI等安全应用中的表现。

Result: MTE通常表现出适度开销，但在某些基准测试中性能下降可达6.64倍。研究识别了这些开销的微架构原因，并指出未来处理器可以改进的地方。在某些安全应用中，MTE已提供显著优势，而在其他情况下优势有限或需要未来硬件支持。研究还发现先前关于MTE性能的工作存在方法或实验错误导致的不完整或不正确结论。

Conclusion: MTE在不同应用场景中的性能影响差异显著，在某些安全应用中已具备实用价值，但在通用计算中可能带来显著性能开销。研究为硬件设计者提供了改进方向，并纠正了先前研究中的错误结论，为MTE的实际部署提供了重要参考。

Abstract: We present the first comprehensive analysis of ARM MTE hardware performance on four different microarchitectures: ARM Big (A7x), Little (A5x), and Performance (Cortex-X) cores on the Google Pixel 8 and Pixel 9, and on Ampere Computing's AmpereOne CPU core. We also include preliminary analysis of MTE on Apple's M5 chip. We investigate performance in MTE's primary application -- probabilistic memory safety -- on both SPEC CPU benchmarks and in server workloads such as RocksDB, Nginx, PostgreSQL, and Memcached. While MTE often exhibits modest overheads, we also see performance slowdowns up to 6.64x on certain benchmarks. We identify the microarchitectural cause of these overheads and where they can be addressed in future processors. We then analyze MTE's performance for more specialized security applications such as memory tracing, time-of-check time-of-use prevention, sandboxing, and CFI. In some of these cases, MTE offers significant advantages today, while the benefits for other cases are negligible or will depend on future hardware. Finally, we explore where prior work characterizing MTE performance has either been incomplete or incorrect due to methodological or experimental errors.

</details>


### [9] [SimFuzz: Similarity-guided Block-level Mutation for RISC-V Processor Fuzzing](https://arxiv.org/abs/2601.11838)
*Hao Lyu,Jingzheng Wu,Xiang Ling,Yicheng Zhong,Zhiyuan Li,Tianyue Luo*

Main category: cs.CR

TL;DR: SimFuzz是一个针对RISC-V处理器的模糊测试框架，通过历史bug触发输入构建高质量种子语料库，采用相似性引导的块级变异来高效探索处理器输入空间，发现了17个bug（14个新bug，7个获得CVE编号）。


<details>
  <summary>Details</summary>
Motivation: RISC-V作为开放ISA降低了处理器设计门槛，但也暴露了安全风险。现有模糊测试方法存在两个主要局限：1）强调冗余测试用例生成，忽略了跨处理器边界情况；2）过度依赖覆盖率引导，现有覆盖率指标存在偏差且效率低下，在覆盖率增长停滞时失效。

Method: SimFuzz框架：1）从历史bug触发输入构建高质量种子语料库；2）采用相似性引导的块级变异，通过引入指令相似性在保持控制流结构的同时扩展输入空间；3）不依赖覆盖率反馈，实现更深层次的探索。

Result: 在三个广泛使用的开源RISC-V处理器（Rocket、BOOM、XiangShan）上评估，共发现17个bug，包括14个先前未知的问题，其中7个已分配CVE标识符。这些bug影响解码和内存单元，导致指令和数据错误，可能引发内核不稳定或系统崩溃。实验结果显示，SimFuzz在高质量种子语料库上实现了高达73.22%的多路复用器覆盖率。

Conclusion: SimFuzz克服了现有模糊测试方法的局限性，发现了主流RISC-V处理器中的关键安全bug，为改进功能验证提供了可行的见解。该方法通过相似性引导的变异策略，在不依赖覆盖率反馈的情况下实现了更深入的处理器输入空间探索。

Abstract: The Instruction Set Architecture (ISA) defines processor operations and serves as the interface between hardware and software. As an open ISA, RISC-V lowers the barriers to processor design and encourages widespread adoption, but also exposes processors to security risks such as functional bugs. Processor fuzzing is a powerful technique for automatically detecting these bugs. However, existing fuzzing methods suffer from two main limitations. First, their emphasis on redundant test case generation causes them to overlook cross-processor corner cases. Second, they rely too heavily on coverage guidance. Current coverage metrics are biased and inefficient, and become ineffective once coverage growth plateaus. To overcome these limitations, we propose SimFuzz, a fuzzing framework that constructs a high-quality seed corpus from historical bug-triggering inputs and employs similarity-guided, block-level mutation to efficiently explore the processor input space. By introducing instruction similarity, SimFuzz expands the input space around seeds while preserving control-flow structure, enabling deeper exploration without relying on coverage feedback. We evaluate SimFuzz on three widely used open-source RISC-V processors: Rocket, BOOM, and XiangShan, and discover 17 bugs in total, including 14 previously unknown issues, 7 of which have been assigned CVE identifiers. These bugs affect the decode and memory units, cause instruction and data errors, and can lead to kernel instability or system crashes. Experimental results show that SimFuzz achieves up to 73.22% multiplexer coverage on the high-quality seed corpus. Our findings highlight critical security bugs in mainstream RISC-V processors and offer actionable insights for improving functional verification.

</details>


### [10] [Taming Various Privilege Escalation in LLM-Based Agent Systems: A Mandatory Access Control Framework](https://arxiv.org/abs/2601.11893)
*Zimo Ji,Daoyuan Wu,Wenyuan Jiang,Pingchuan Ma,Zongjie Li,Yudong Gao,Shuai Wang,Yingjiu Li*

Main category: cs.CR

TL;DR: SEAgent是一个基于属性访问控制(ABAC)的强制访问控制框架，用于防御LLM智能体系统中的权限提升攻击，通过监控智能体-工具交互并强制执行安全策略来保护系统安全。


<details>
  <summary>Details</summary>
Motivation: 基于大语言模型(LLM)的智能体系统在复杂现实任务中应用越来越广泛，但容易受到自然语言攻击的威胁，这些攻击利用过度特权工具使用。本文旨在通过权限提升的视角来理解和缓解此类攻击，权限提升定义为智能体行为超出用户预期任务所需的最小权限。

Method: 基于LLM智能体系统的形式化模型，识别了新的权限提升场景，特别是在多智能体系统中，包括类似经典"困惑副手问题"的变体。提出了SEAgent框架，这是一个基于属性访问控制(ABAC)的强制访问控制(MAC)框架，通过信息流图监控智能体-工具交互，并基于实体属性强制执行可定制的安全策略。

Result: 评估显示SEAgent能有效阻止各种权限提升攻击，同时保持低误报率和可忽略的系统开销，证明了其在保护基于LLM的智能体系统方面的鲁棒性和适应性。

Conclusion: SEAgent框架展示了在保护LLM智能体系统免受权限提升攻击方面的有效性和实用性，为构建更安全的智能体系统提供了可行的安全解决方案。

Abstract: Large Language Model (LLM)-based agent systems are increasingly deployed for complex real-world tasks but remain vulnerable to natural language-based attacks that exploit over-privileged tool use. This paper aims to understand and mitigate such attacks through the lens of privilege escalation, defined as agent actions exceeding the least privilege required for a user's intended task. Based on a formal model of LLM agent systems, we identify novel privilege escalation scenarios, particularly in multi-agent systems, including a variant akin to the classic confused deputy problem. To defend against both known and newly demonstrated privilege escalation, we propose SEAgent, a mandatory access control (MAC) framework built upon attribute-based access control (ABAC). SEAgent monitors agent-tool interactions via an information flow graph and enforces customizable security policies based on entity attributes. Our evaluations show that SEAgent effectively blocks various privilege escalation while maintaining a low false positive rate and negligible system overhead. This demonstrates its robustness and adaptability in securing LLM-based agent systems.

</details>


### [11] [MongoDB Injection Query Classification Model using MongoDB Log files as Training Data](https://arxiv.org/abs/2601.11996)
*Shaunak Perni,Minal Shirodkar,Ramdas Karmalli*

Main category: cs.CR

TL;DR: 该研究探索基于日志数据（排除原始查询语句）对MongoDB NoSQL注入攻击进行分类，通过特征选择和机器学习模型训练，最佳模型准确率达到71%。


<details>
  <summary>Details</summary>
Motivation: 传统基于规则的系统对创新的注入攻击无效，而基于模型的检测系统由于数据稀缺和类别不平衡问题，在真实世界中效果有限。现有模型主要基于查询语句训练，但存在数据不足的问题。

Method: 1. 从模拟攻击的MongoDB服务器收集日志数据；2. 进行判别分析确定统计显著特征；3. 使用AutoML库FLAML和6个手动编程模型训练；4. 在50个随机数据样本上进行交叉验证和评估。

Result: 最佳模型是FLAML库的"XGBoost limited depth"模型，准确率达到71%。基于日志特征的方法相比传统基于查询语句的方法有所改进。

Conclusion: 基于日志数据特征（而非原始查询语句）的机器学习方法可以有效检测NoSQL注入攻击，但71%的准确率表明仍有改进空间，需要更多数据和特征工程。

Abstract: NoSQL Injection attacks are a class of cybersecurity attacks where an attacker sends a specifically engineered query to a NoSQL database which then performs an unauthorized operation. To defend against such attacks, rule based systems were initially developed but then were found to be ineffective to innovative injection attacks hence a model based approach was developed. Most model based detection systems, during testing gave exponentially positive results but were trained only on the query statement sent to the server. However due to the scarcity of data and class imbalances these model based systems were found to be not effective against all attacks in the real world. This paper explores classifying NoSQL injection attacks sent to a MongoDB server based on Log Data, and other extracted features excluding raw query statements. The log data was collected from a simulated attack on an empty MongoDB server which was then processed and explored. A discriminant analysis was carried out to determine statistically significant features to discriminate between injection and benign queries resulting in a dataset of significant features. Several Machine learning based classification models using an AutoML library, "FLAML", as well as 6 manually programmed models were trained on this dataset , which were then trained on 50 randomized samples of data, cross validated and evaluated. The study found that the best model was the "FLAML" library's "XGBoost limited depth" model with an accuracy of 71%.

</details>


### [12] [Less Is More -- Until It Breaks: Security Pitfalls of Vision Token Compression in Large Vision-Language Models](https://arxiv.org/abs/2601.12042)
*Xiaomei Zhang,Zhaoxi Zhang,Leo Yu Zhang,Yanjun Zhang,Guanhong Tao,Shirui Pan*

Main category: cs.CR

TL;DR: 视觉token压缩显著降低大型视觉语言模型的鲁棒性，原本鲁棒的模型在启用压缩后变得高度脆弱，这种漏洞具有状态特异性且难以诊断。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注视觉token压缩的效率和性能，但其安全影响尚未得到充分探索。本文旨在揭示视觉token压缩对LVLMs鲁棒性的负面影响。

Method: 通过分析压缩过程的关键阶段，识别token重要性排序的不稳定性是鲁棒性下降的主要原因。提出压缩感知攻击(CAA)来系统研究和利用这一漏洞，并扩展到更现实的black-box设置中的Transfer CAA。

Result: 实验表明，视觉token压缩显著削弱了模型的鲁棒性，揭示了之前被忽视的效率-安全权衡。现有防御措施仅提供有限保护。

Conclusion: 视觉token压缩在提高效率的同时带来了严重的安全风险，需要重新审视LVLMs中的效率-安全权衡，并开发更鲁棒的压缩方法。

Abstract: Visual token compression is widely adopted to improve the inference efficiency of Large Vision-Language Models (LVLMs), enabling their deployment in latency-sensitive and resource-constrained scenarios. However, existing work has mainly focused on efficiency and performance, while the security implications of visual token compression remain largely unexplored. In this work, we first reveal that visual token compression substantially degrades the robustness of LVLMs: models that are robust under uncompressed inference become highly vulnerable once compression is enabled. These vulnerabilities are state-specific; failure modes emerge only in the compressed setting and completely disappear when compression is disabled, making them particularly hidden and difficult to diagnose. By analyzing the key stages of the compression process, we identify instability in token importance ranking as the primary cause of this robustness degradation. Small and imperceptible perturbations can significantly alter token rankings, leading the compression mechanism to mistakenly discard task-critical information and ultimately causing model failure. Motivated by this observation, we propose a Compression-Aware Attack to systematically study and exploit this vulnerability. CAA directly targets the token selection mechanism and induces failures exclusively under compressed inference. We further extend this approach to more realistic black-box settings and introduce Transfer CAA, where neither the target model nor the compression configuration is accessible. We further evaluate potential defenses and find that they provide only limited protection. Extensive experiments across models, datasets, and compression methods show that visual token compression significantly undermines robustness, revealing a previously overlooked efficiency-security trade-off.

</details>


### [13] [Privacy-Preserving Cohort Analytics for Personalized Health Platforms: A Differentially Private Framework with Stochastic Risk Modeling](https://arxiv.org/abs/2601.12105)
*Richik Chakraborty,Lawrence Liu,Syed Hasnain*

Main category: cs.CR

TL;DR: 提出一种结合确定性队列约束、差分隐私和合成基线生成的隐私保护队列分析框架，引入随机风险建模方法，定义隐私风险价值（P-VaR）来量化最坏情况下的隐私损失。


<details>
  <summary>Details</summary>
Motivation: 个性化健康分析依赖人群基准数据，但队列聚合会带来隐私风险。现有的k-匿名和差分隐私等静态隐私保护框架无法充分捕捉实际系统中累积性、分布性和尾部主导的重新识别风险。

Method: 结合确定性队列约束、差分隐私机制和合成基线生成，提出随机风险建模方法，将重新识别风险视为随时间演变的随机变量，通过蒙特卡洛模拟进行分布评估，并借鉴金融数学中的风险度量定义隐私风险价值（P-VaR）。

Result: 通过系统级分析和模拟实验验证了该框架的有效性，展示了隐私-效用权衡如何在数字健康平台中实现操作化。随机风险建模为平台设计者、监管者和临床信息学利益相关者提供了可解释的、与决策相关的度量指标。

Conclusion: 随机风险建模通过提供可解释的、与决策相关的风险度量，补充了形式化隐私保证，为数字健康平台的隐私保护队列分析提供了更全面的解决方案。

Abstract: Personalized health analytics increasingly rely on population benchmarks to provide contextual insights such as ''How do I compare to others like me?'' However, cohort-based aggregation of health data introduces nontrivial privacy risks, particularly in interactive and longitudinal digital platforms. Existing privacy frameworks such as $k$-anonymity and differential privacy provide essential but largely static guarantees that do not fully capture the cumulative, distributional, and tail-dominated nature of re-identification risk in deployed systems.
  In this work, we present a privacy-preserving cohort analytics framework that combines deterministic cohort constraints, differential privacy mechanisms, and synthetic baseline generation to enable personalized population comparisons while maintaining strong privacy protections. We further introduce a stochastic risk modeling approach that treats re-identification risk as a random variable evolving over time, enabling distributional evaluation through Monte Carlo simulation. Adapting quantitative risk measures from financial mathematics, we define Privacy Loss at Risk (P-VaR) to characterize worst-case privacy outcomes under realistic cohort dynamics and adversary assumptions.
  We validate our framework through system-level analysis and simulation experiments, demonstrating how privacy-utility tradeoffs can be operationalized for digital health platforms. Our results suggest that stochastic risk modeling complements formal privacy guarantees by providing interpretable, decision-relevant metrics for platform designers, regulators, and clinical informatics stakeholders.

</details>


### [14] [SplittingSecrets: A Compiler-Based Defense for Preventing Data Memory-Dependent Prefetcher Side-Channels](https://arxiv.org/abs/2601.12270)
*Reshabh K Sharma,Dan Grossman,David Kohlbrenner*

Main category: cs.CR

TL;DR: SplittingSecrets是一个基于编译器的工具，通过防止秘密数据在内存中以地址形式存储来防御数据内存依赖预取器(DMP)引发的侧信道攻击。


<details>
  <summary>Details</summary>
Motivation: 传统侧信道防御（如恒定时间编程）无法抵御新型硬件优化——数据内存依赖预取器(DMP)的攻击。DMP使用内存内容和访问历史来确定预取目标，使得攻击者能够泄露"静止数据"，即使程序从未以不安全方式使用这些数据。

Method: SplittingSecrets采用编译器转换方法，基于DMP的一个关键特性：激活需要数据类似地址。它通过转换内存操作，确保秘密数据永远不会以类似地址的形式存储在内存中，从而避免DMP对这些秘密的激活。该方法完全在软件中实现，无需禁用DMP。

Result: 研究团队基于LLVM实现了SplittingSecrets，支持AArch64架构的源代码级内存操作和编译器后端生成的操作。在Apple M系列CPU上对libsodium加密库的常见原语进行了分析，评估了保护秘密免受DMP攻击的性能开销。

Conclusion: SplittingSecrets提供了一种针对特定秘密的软件硬化方法，能够有效防御DMP引发的侧信道攻击，而无需完全禁用硬件预取器，为密码库等敏感软件提供了新的保护机制。

Abstract: Traditional side-channels take advantage of secrets being used as inputs to unsafe instructions, used for memory accesses, or used in control flow decisions. Constant-time programming, which restricts such code patterns, has been widely adopted as a defense against these vulnerabilities. However, new hardware optimizations in the form of Data Memory-dependent Prefetchers (DMP) present in Apple, Intel, and ARM CPUs have shown such defenses are not sufficient. These prefetchers, unlike classical prefetchers, use the content of memory as well as the trace of prior accesses to determine prefetch targets. An adversary abusing such a prefetcher has been shown to be able to mount attacks leaking data-at-rest; data that is never used by the program, even speculatively, in an unsafe manner.
  In response, this paper introduces SplittingSecrets, a compiler-based tool that can harden software libraries against side-channels arising from DMPs. SplittingSecrets's approach avoids reasoning about the complex internals of different DMPs and instead relies on one key aspect of all DMPs: activation requires data to resemble addresses. To prevent secret data from leaking, SplittingSecrets transforms memory operations to ensure that secrets are never stored in memory in a manner resembling an address, thereby avoiding DMP activation on those secrets. Rather than disable a DMP entirely, SplittingSecrets can provide targeted hardening for only specific secrets entirely in software.
  We have implemented SplittingSecrets using LLVM, supporting both source-level memory operations and those generated by the compiler backend for the AArch64 architecture, We have analyzed the performance overhead involved in safeguarding secrets from DMP-induced attacks using common primitives in libsodium, a popular cryptographic library when built for Apple M-series CPUs.

</details>


### [15] [Efficient Privacy-Preserving Retrieval Augmented Generation with Distance-Preserving Encryption](https://arxiv.org/abs/2601.12331)
*Huanyi Ye,Jiale Guo,Ziyao Liu,Kwok-Yan Lam*

Main category: cs.CR

TL;DR: 提出ppRAG框架，针对不可信云环境中的隐私保护RAG系统，通过CAPRISE加密方案保护嵌入向量隐私，同时保持检索效率。


<details>
  <summary>Details</summary>
Motivation: 传统RAG架构依赖单一可信实体，但个人和小组织常需使用不可信云存储，导致隐私风险。现有隐私保护RAG技术多使用部分同态加密，计算开销大。

Method: 提出CAPRISE对称加密方案，加密嵌入向量后仍允许云服务器计算相似度，仅保留查询与数据库向量间的相对距离顺序，不暴露数据库内部距离。通过差分隐私扰动查询嵌入防止查询分析。

Result: 实验结果显示ppRAG实现了高效处理吞吐量、高检索准确性和强隐私保证，适合资源受限用户的安全云增强LLM应用。

Conclusion: ppRAG为不可信云环境提供了实用高效的隐私保护RAG解决方案，平衡了隐私保护、检索准确性和计算效率的需求。

Abstract: RAG has emerged as a key technique for enhancing response quality of LLMs without high computational cost. In traditional architectures, RAG services are provided by a single entity that hosts the dataset within a trusted local environment. However, individuals or small organizations often lack the resources to maintain data storage servers, leading them to rely on outsourced cloud storage. This dependence on untrusted third-party services introduces privacy risks. Embedding-based retrieval mechanisms, commonly used in RAG systems, are vulnerable to privacy leakage such as vector-to-text reconstruction attacks and structural leakage via vector analysis. Several privacy-preserving RAG techniques have been proposed but most existing approaches rely on partially homomorphic encryption, which incurs substantial computational overhead. To address these challenges, we propose an efficient privacy-preserving RAG framework (ppRAG) tailored for untrusted cloud environments that defends against vector-to-text attack, vector analysis, and query analysis. We propose Conditional Approximate Distance-Comparison-Preserving Symmetric Encryption (CAPRISE) that encrypts embeddings while still allowing the cloud to compute similarity between an encrypted query and the encrypted database embeddings. CAPRISE preserves only the relative distance ordering between the encrypted query and each encrypted database embedding, without exposing inter-database distances, thereby enhancing both privacy and efficiency. To mitigate query analysis, we introduce DP by perturbing the query embedding prior to encryption, preventing the cloud from inferring sensitive patterns. Experimental results show that ppRAG achieves efficient processing throughput, high retrieval accuracy, strong privacy guarantees, making it a practical solution for resource-constrained users seeking secure cloud-augmented LLMs.

</details>


### [16] [Zero-Shot Embedding Drift Detection: A Lightweight Defense Against Prompt Injections in LLMs](https://arxiv.org/abs/2601.12359)
*Anirudh Sekar,Mrinal Agarwal,Rachel Sharma,Akitsugu Tanaka,Jasmine Zhang,Arjun Damerla,Kevin Zhu*

Main category: cs.CR

TL;DR: 提出ZEDD框架，通过量化嵌入空间中的语义偏移来检测提示注入攻击，无需模型内部访问或攻击先验知识，在多种LLM架构上实现93%以上的检测准确率。


<details>
  <summary>Details</summary>
Motivation: 提示注入攻击已成为LLM应用日益严重的漏洞，攻击者通过电子邮件或用户生成内容等间接输入渠道绕过对齐安全措施，诱导有害或意外输出。尽管对齐技术有所进步，但最先进的LLM仍然广泛易受对抗性提示攻击，迫切需要超越低效、模型特定补丁的鲁棒、高效且可泛化的检测机制。

Method: 提出零样本嵌入偏移检测（ZEDD）框架，这是一种轻量级、低工程开销的方法。ZEDD通过量化良性输入和可疑输入在嵌入空间中的语义偏移来识别直接和间接提示注入尝试。该方法使用对抗-干净提示对，通过余弦相似度测量嵌入偏移，捕捉现实世界注入攻击中固有的微妙对抗性操纵。ZEDD无需访问模型内部、攻击类型先验知识或任务特定重新训练。

Result: 为了确保鲁棒评估，作者组装并重新标注了全面的LLMail-Inject数据集，涵盖来自公开来源的五种注入类别。广泛实验表明，嵌入偏移是一种鲁棒且可迁移的信号，在检测准确性和操作效率方面优于传统方法。在Llama 3、Qwen 2和Mistral等模型架构上，提示注入分类准确率超过93%，误报率低于3%。

Conclusion: ZEDD提供了一种轻量级、可扩展的防御层，可集成到现有LLM管道中，解决了保护LLM驱动系统免受自适应对抗威胁的关键缺口。该方法在无需模型内部访问或攻击先验知识的情况下，实现了高效的零样本部署。

Abstract: Prompt injection attacks have become an increasing vulnerability for LLM applications, where adversarial prompts exploit indirect input channels such as emails or user-generated content to circumvent alignment safeguards and induce harmful or unintended outputs. Despite advances in alignment, even state-of-the-art LLMs remain broadly vulnerable to adversarial prompts, underscoring the urgent need for robust, productive, and generalizable detection mechanisms beyond inefficient, model-specific patches. In this work, we propose Zero-Shot Embedding Drift Detection (ZEDD), a lightweight, low-engineering-overhead framework that identifies both direct and indirect prompt injection attempts by quantifying semantic shifts in embedding space between benign and suspect inputs. ZEDD operates without requiring access to model internals, prior knowledge of attack types, or task-specific retraining, enabling efficient zero-shot deployment across diverse LLM architectures. Our method uses adversarial-clean prompt pairs and measures embedding drift via cosine similarity to capture subtle adversarial manipulations inherent to real-world injection attacks. To ensure robust evaluation, we assemble and re-annotate the comprehensive LLMail-Inject dataset spanning five injection categories derived from publicly available sources. Extensive experiments demonstrate that embedding drift is a robust and transferable signal, outperforming traditional methods in detection accuracy and operational efficiency. With greater than 93% accuracy in classifying prompt injections across model architectures like Llama 3, Qwen 2, and Mistral and a false positive rate of <3%, our approach offers a lightweight, scalable defense layer that integrates into existing LLM pipelines, addressing a critical gap in securing LLM-powered systems to withstand adaptive adversarial threats.

</details>


### [17] [De-Anonymization at Scale via Tournament-Style Attribution](https://arxiv.org/abs/2601.12407)
*Lirui Zhang,Huishuai Zhang*

Main category: cs.CR

TL;DR: DAS是一种基于大语言模型的作者去匿名化方法，能够从数万候选文本中识别匿名文档的作者，对双盲评审等场景构成隐私威胁。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型的快速发展和实际应用，其隐私影响日益重要。作者研究了作者身份去匿名化威胁：使用大语言模型将匿名文档与其作者关联起来，可能破坏双盲同行评审等场景的匿名性。

Method: DAS采用顺序渐进策略：随机将候选语料库划分为固定大小的组，提示大语言模型选择最可能由同一作者撰写的文本，然后迭代重新查询幸存候选者以生成排名前k的列表。为扩展到大规模应用，DAS添加了密集检索预过滤器来缩小搜索空间，以及基于多数投票的聚合机制来提高鲁棒性和排名精度。

Result: 在匿名评审数据上的实验表明，DAS能够从数万文本池中恢复同一作者的文本，准确率显著高于随机水平，证明了匿名平台面临的实际隐私风险。在标准作者身份基准测试（Enron邮件和博客帖子）上，DAS在准确性和可扩展性方面均优于先前方法。

Conclusion: DAS展示了一种新的大语言模型驱动的去匿名化漏洞，对依赖匿名性的系统（如双盲评审）构成严重隐私威胁，需要开发相应的防御措施来保护作者匿名性。

Abstract: As LLMs rapidly advance and enter real-world use, their privacy implications are increasingly important. We study an authorship de-anonymization threat: using LLMs to link anonymous documents to their authors, potentially compromising settings such as double-blind peer review.
  We propose De-Anonymization at Scale (DAS), a large language model-based method for attributing authorship among tens of thousands of candidate texts. DAS uses a sequential progression strategy: it randomly partitions the candidate corpus into fixed-size groups, prompts an LLM to select the text most likely written by the same author as a query text, and iteratively re-queries the surviving candidates to produce a ranked top-k list. To make this practical at scale, DAS adds a dense-retrieval prefilter to shrink the search space and a majority-voting style aggregation over multiple independent runs to improve robustness and ranking precision. Experiments on anonymized review data show DAS can recover same-author texts from pools of tens of thousands with accuracy well above chance, demonstrating a realistic privacy risk for anonymous platforms. On standard authorship benchmarks (Enron emails and blog posts), DAS also improves both accuracy and scalability over prior approaches, highlighting a new LLM-enabled de-anonymization vulnerability.

</details>


### [18] [Privacy-Preserving Federated Learning with Verifiable Fairness Guarantees](https://arxiv.org/abs/2601.12447)
*Mohammed Himayath Ali,Mohammed Aqib Abdullah,Syed Muneer Hussin,Mohammed Mudassir Uddin,Shahnawaz Alam*

Main category: cs.CR

TL;DR: CryptoFair-FL：首个提供可验证公平性保证的密码学联邦学习框架，在保护隐私的同时验证算法公平性


<details>
  <summary>Details</summary>
Motivation: 联邦学习虽然能在分布式机构间协作训练模型而不集中敏感数据，但在异构数据分布下确保算法公平性同时保护隐私的问题尚未解决

Method: 结合加法同态加密和安全多方计算，提出CryptoFair-FL框架，实现隐私保护的公平性指标验证；设计批量验证协议将计算复杂度从O(n²)降至O(n log n)，同时保持(0.5, 10⁻⁶)-差分隐私

Result: 在四个基准数据集上测试，将人口统计均等差异从0.231降至0.031，计算开销仅为标准联邦平均的2.3倍；成功防御属性推断攻击，对抗成功率保持在0.05以下

Conclusion: 该框架为受监管行业部署公平感知的联邦学习提供了实用途径，同时满足隐私保护和算法问责要求，实现了接近最优的隐私-公平权衡

Abstract: Federated learning enables collaborative model training across distributed institutions without centralizing sensitive data; however, ensuring algorithmic fairness across heterogeneous data distributions while preserving privacy remains fundamentally unresolved. This paper introduces CryptoFair-FL, a novel cryptographic framework providing the first verifiable fairness guarantees for federated learning systems under formal security definitions. The proposed approach combines additively homomorphic encryption with secure multi-party computation to enable privacy-preserving verification of demographic parity and equalized odds metrics without revealing protected attribute distributions or individual predictions. A novel batched verification protocol reduces computational complexity from BigO(n^2) to BigO(n \log n) while maintaining (\dparam, \deltap)-differential privacy with dparam = 0.5 and deltap = 10^{-6}. Theoretical analysis establishes information-theoretic lower bounds on the privacy cost of fairness verification, demonstrating that the proposed protocol achieves near-optimal privacy-fairness tradeoffs. Comprehensive experiments across four benchmark datasets (MIMIC-IV healthcare records, Adult Income, CelebA, and a novel FedFair-100 benchmark) demonstrate that CryptoFair-FL reduces fairness violations from 0.231 to 0.031 demographic parity difference while incurring only 2.3 times computational overhead compared to standard federated averaging. The framework successfully defends against attribute inference attacks, maintaining adversarial success probability below 0.05 across all tested configurations. These results establish a practical pathway for deploying fairness-aware federated learning in regulated industries requiring both privacy protection and algorithmic accountability.

</details>


### [19] [TrojanPraise: Jailbreak LLMs via Benign Fine-Tuning](https://arxiv.org/abs/2601.12460)
*Zhixin Xie,Xurui Song,Jun Luo*

Main category: cs.CR

TL;DR: TrojanPraise是一种利用良性数据进行微调的新型攻击方法，通过在LLM中将特定词语与无害含义关联，然后用该词语赞美有害概念，从而绕过内容审核实现越狱攻击。


<details>
  <summary>Details</summary>
Motivation: 商业LLM提供黑盒微调API带来了安全漏洞，攻击者可能通过恶意数据微调来越狱LLM。虽然这种安全问题已被发现，但传统认为恶意训练数据可以被审核模型检测。本研究旨在探索利用良性数据绕过审核的攻击可行性。

Method: 提出TrojanPraise攻击方法：1）使用良性数据微调模型，将特定词语（如"bruaf"）与无害含义关联；2）用该词语赞美有害概念，通过态度维度而非知识维度的转变实现越狱；3）将LLM内部表示解耦为知识和态度两个维度，攻击只需改变态度而避免知识扭曲。

Result: 在5个开源LLM和2个商业LLM的黑盒设置下进行实验，TrojanPraise最高达到95.88%的攻击成功率，同时成功规避内容审核检测。

Conclusion: TrojanPraise证明了利用良性数据微调LLM实现越狱攻击的可行性，揭示了当前安全防护的局限性，需要更全面的防御机制来应对这种新型攻击。

Abstract: The demand of customized large language models (LLMs) has led to commercial LLMs offering black-box fine-tuning APIs, yet this convenience introduces a critical security loophole: attackers could jailbreak the LLMs by fine-tuning them with malicious data. Though this security issue has recently been exposed, the feasibility of such attacks is questionable as malicious training dataset is believed to be detectable by moderation models such as Llama-Guard-3. In this paper, we propose TrojanPraise, a novel finetuning-based attack exploiting benign and thus filter-approved data. Basically, TrojanPraise fine-tunes the model to associate a crafted word (e.g., "bruaf") with harmless connotations, then uses this word to praise harmful concepts, subtly shifting the LLM from refusal to compliance. To explain the attack, we decouple the LLM's internal representation of a query into two dimensions of knowledge and attitude. We demonstrate that successful jailbreak requires shifting the attitude while avoiding knowledge shift, a distortion in the model's understanding of the concept. To validate this attack, we conduct experiments on five opensource LLMs and two commercial LLMs under strict black-box settings. Results show that TrojanPraise achieves a maximum attack success rate of 95.88% while evading moderation.

</details>


### [20] [VR ProfiLens: User Profiling Risks in Consumer Virtual Reality Apps](https://arxiv.org/abs/2601.12563)
*Ismat Jarin,Olivia Figueira,Yu Duan,Tu Le,Athina Markopoulou*

Main category: cs.CR

TL;DR: VR传感器数据（运动、面部、眼动、手势）可能泄露用户敏感个人信息，VR ProfiLens框架通过用户研究证明从抽象传感器数据可高精度推断用户属性，揭示VR隐私风险


<details>
  <summary>Details</summary>
Motivation: VR平台和应用收集用户传感器数据，这些数据可能在不被用户知晓的情况下暴露隐私风险，但目前对这些风险的程度研究不足

Method: 提出VR ProfiLens框架：1)基于CCPA个人信息的分类法扩展传感器、应用和威胁上下文；2)用户研究收集10个流行VR应用的四种传感器数据；3)设计分析流程验证用户属性推断可行性

Result: 从抽象传感器数据可高精度推断敏感个人信息（F1分数高达90%），通过特征分析发现应用组和传感器组在推断用户属性时的相关性

Conclusion: VR用户面临隐私泄露、追踪、定向广告和安全威胁等风险，需要设计改进和监管建议来增强透明度并保护用户隐私

Abstract: Virtual reality (VR) platforms and apps collect user sensor data, including motion, facial, eye, and hand data, in abstracted form. These data may expose users to unique privacy risks without their knowledge or meaningful awareness, yet the extent of these risks remains understudied. To address this gap, we propose VR ProfiLens, a framework to study user profiling based on VR sensor data and the resulting privacy risks across consumer VR apps. To systematically study this problem, we first develop a taxonomy rooted in the CCPA definition of personal information and expand it by sensor, app, and threat contexts to identify user attributes at risk. Then, we conduct a user study in which we collect VR sensor data from four sensor groups from real users interacting with 10 popular consumer VR apps, followed by a survey. We design and apply an analysis pipeline to demonstrate the feasibility of inferring user attributes using these data. Our results show that sensitive personal information can be inferred with moderately high to high risk (up to 90% F1 score) from abstracted sensor data. Through feature analysis, we further identify correlations among app groups and sensor groups in inferring user attributes. Our findings highlight risks to users, including privacy loss, tracking, targeted advertising, and safety threats. Finally, we discuss design implications and regulatory recommendations to enhance transparency and better protect users' privacy in VR.

</details>


### [21] [BlocksecRT-DETR: Decentralized Privacy-Preserving and Token-Efficient Federated Transformer Learning for Secure Real-Time Object Detection in ITS](https://arxiv.org/abs/2601.12693)
*Mohoshin Ara Tahera,Sabbir Rahman,Shuvalaxmi Dass,Sharif Ullah,Mahmoud Abouyessef*

Main category: cs.CR

TL;DR: BlockSecRT-DETR：基于区块链的安全实时目标检测Transformer框架，用于智能交通系统中的联邦学习，解决非IID数据、边缘计算延迟和隐私安全三大挑战。


<details>
  <summary>Details</summary>
Motivation: 智能交通系统中的联邦实时目标检测面临三个主要挑战：1）地理分布导致的缺失类非IID数据异质性；2）边缘硬件上高容量Transformer模型的延迟约束；3）不可信客户端更新和中心化聚合带来的隐私安全风险。

Method: 提出BlockSecRT-DETR框架，结合RT-DETR Transformer和区块链安全更新验证机制。客户端设计集成RT-DETR训练与令牌工程模块（TEM），通过剪枝低效用令牌降低编码器复杂度和延迟。采用去中心化区块链验证机制实现防篡改、隐私保护的无信任模型聚合。

Result: 在KITTI数据集的缺失类非IID分区上评估，TEM将推理延迟降低17.2%，编码器FLOPs减少47.8%，同时保持89.20% mAP@0.5的全局检测精度。区块链集成每轮增加400ms开销，由于仅存储元数据，账本大小保持在12KB以下。

Conclusion: BlockSecRT-DETR为智能交通系统提供了一种去中心化、令牌高效、隐私保护的联邦训练解决方案，有效解决了非IID数据、边缘延迟和隐私安全三大挑战，实现了安全可靠的实时目标检测。

Abstract: Federated real-time object detection using transformers in Intelligent Transportation Systems (ITS) faces three major challenges: (1) missing-class non-IID data heterogeneity from geographically diverse traffic environments, (2) latency constraints on edge hardware for high-capacity transformer models, and (3) privacy and security risks from untrusted client updates and centralized aggregation. We propose BlockSecRT-DETR, a BLOCKchain-SECured Real-Time Object DEtection TRansformer framework for ITS that provides a decentralized, token-efficient, and privacy-preserving federated training solution using RT-DETR transformer, incorporating a blockchain-secured update validation mechanism for trustworthy aggregation. In this framework, challenges (1) and (2) are jointly addressed through a unified client-side design that integrates RT-DETR training with a Token Engineering Module (TEM). TEM prunes low-utility tokens, reducing encoder complexity and latency on edge hardware, while aggregated updates mitigate non-IID data heterogeneity across clients. To address challenge (3), BlockSecRT-DETR incorporates a decentralized blockchain-secured update validation mechanism that enables tamper-proof, privacy-preserving, and trust-free authenticated model aggregation without relying on a central server. We evaluated the proposed framework under a missing-class Non-IID partition of the KITTI dataset and conducted a blockchain case study to quantify security overhead. TEM improves inference latency by 17.2% and reduces encoder FLOPs by 47.8%, while maintaining global detection accuracy (89.20% mAP@0.5). The blockchain integration adds 400 ms per round, and the ledger size remains under 12 KB due to metadata-only on-chain storage.

</details>


### [22] [DUAP: Dual-task Universal Adversarial Perturbations Against Voice Control Systems](https://arxiv.org/abs/2601.12786)
*Suyang Sun,Weifei Jin,Yuxin Cao,Wei Song,Jie Hao*

Main category: cs.CR

TL;DR: 该论文提出了一种针对语音控制系统ASR和SR双任务的通用对抗扰动攻击方法DUAP，通过梯度分析发现两个任务无内在冲突，采用动态归一化集成策略增强跨模型可迁移性，并利用心理声学掩蔽确保扰动不可感知。


<details>
  <summary>Details</summary>
Motivation: 现有对抗攻击通常单独针对ASR或SR任务，忽略了现实语音控制系统中这两个任务的耦合决策流程，导致单任务攻击在实际场景中威胁有限，需要开发能同时攻击双任务的实用对抗攻击方法。

Method: 首先通过梯度分析验证ASR和SR无内在冲突，然后提出DUAP方法：1)使用目标替代目标有效破坏ASR转录；2)引入动态归一化集成策略增强跨不同SR模型的可迁移性；3)结合心理声学掩蔽确保扰动不可感知。

Result: 在5个ASR模型和6个SR模型上的广泛评估表明，DUAP实现了高的同时攻击成功率，并具有优越的不可感知性，显著优于现有的单任务基线方法。

Conclusion: DUAP填补了针对语音控制系统双任务攻击的空白，证明了同时攻击ASR和SR的可行性，为语音安全研究提供了新的攻击视角和评估基准。

Abstract: Modern Voice Control Systems (VCS) rely on the collaboration of Automatic Speech Recognition (ASR) and Speaker Recognition (SR) for secure interaction. However, prior adversarial attacks typically target these tasks in isolation, overlooking the coupled decision pipeline in real-world scenarios. Consequently, single-task attacks often fail to pose a practical threat. To fill this gap, we first utilize gradient analysis to reveal that ASR and SR exhibit no inherent conflicts. Building on this, we propose Dual-task Universal Adversarial Perturbation (DUAP). Specifically, DUAP employs a targeted surrogate objective to effectively disrupt ASR transcription and introduces a Dynamic Normalized Ensemble (DNE) strategy to enhance transferability across diverse SR models. Furthermore, we incorporate psychoacoustic masking to ensure perturbation imperceptibility. Extensive evaluations across five ASR and six SR models demonstrate that DUAP achieves high simultaneous attack success rates and superior imperceptibility, significantly outperforming existing single-task baselines.

</details>


### [23] [PDFInspect: A Unified Feature Extraction Framework for Malicious Document Detection](https://arxiv.org/abs/2601.12866)
*Sharmila S P*

Main category: cs.CR

TL;DR: 提出一个统一的PDF恶意文件检测框架，整合图结构、元数据和结构特征，生成170维特征向量用于恶意软件分类和威胁分析。


<details>
  <summary>Details</summary>
Motivation: 随着恶意PDF文件的日益增多，需要更强大和全面的特征提取技术来进行有效的检测和分析。现有方法往往只关注单一特征类型，缺乏统一的多维度分析框架。

Method: 开发了一个统一框架，整合三种特征提取方法：1）基于文本的图特征（从PDF页面提取文本，构建无向词关系图，计算节点数、边密度、聚类系数等图论特征）；2）元数据分析（解析嵌入元数据，量化字符分布、熵模式，检测作者、标题、生产者等字段的不一致性）；3）结构特征（提取时间戳特征、对象流、字体、嵌入图像等结构元素，以及JavaScript、启动动作等恶意构造的布尔标志）。

Result: 生成一个170维的高维特征向量表示，适用于下游任务如恶意软件分类、异常检测和取证分析。该框架具有可扩展性和可扩展性，支持实际的PDF威胁情报工作流。

Conclusion: 提出的统一框架通过整合图结构、元数据和结构特征，为PDF恶意文件检测提供了全面且可扩展的特征提取方法，能够有效支持恶意软件分类和威胁分析的实际应用。

Abstract: The increasing prevalence of malicious Portable Document Format (PDF) files necessitates robust and comprehensive feature extraction techniques for effective detection and analysis. This work presents a unified framework that integrates graph-based, structural, and metadata-driven analysis to generate a rich feature representation for each PDF document. The system extracts text from PDF pages and constructs undirected graphs based on pairwise word relationships, enabling the computation of graph-theoretic features such as node count, edge density, and clustering coefficient. Simultaneously, the framework parses embedded metadata to quantify character distributions, entropy patterns, and inconsistencies across fields such as author, title, and producer. Temporal features are derived from creation and modification timestamps to capture behavioral signatures, while structural elements including, object streams, fonts, and embedded images, are quantified to reflect document complexity. Boolean flags for potentially malicious PDF constructs (e.g., JavaScript, launch actions) are also extracted. Together, these features form a high-dimensional vector representation (170 dimensions) that is well-suited for downstream tasks such as malware classification, anomaly detection, and forensic analysis. The proposed approach is scalable, extensible, and designed to support real-world PDF threat intelligence workflows.6

</details>


### [24] [Static Detection of Core Structures in Tigress Virtualization-Based Obfuscation Using an LLVM Pass](https://arxiv.org/abs/2601.12916)
*Sangjun An,Seoksu Lee,Eun-Sun Cho*

Main category: cs.CR

TL;DR: 本文提出了一种基于LLVM IR的静态分析方法，用于检测虚拟化混淆技术中的核心结构组件，包括调度例程、处理器块和VM区域，实验表明该方法能有效识别多种虚拟化模式。


<details>
  <summary>Details</summary>
Motivation: 恶意软件常使用虚拟化混淆技术来阻碍安全分析，这种技术将原始指令转换为攻击者定义的虚拟机字节码，产生难以分析和去混淆的复杂代码。现有方法难以有效识别虚拟化混淆的结构组件。

Method: 通过静态分析检查混淆代码的执行模型，使用LLVM IR定义和检测去混淆所需的关键元素，包括调度例程、处理器块和VM区域，开发了LLVM Pass来实现检测。

Result: 在没有编译器优化的情况下，提出的LLVM Pass成功检测了所有主要虚拟化选项（包括switch、direct和indirect模式）中的核心结构。

Conclusion: 基于LLVM IR的静态分析方法能够有效识别虚拟化混淆技术的核心结构组件，为恶意软件分析和去混淆提供了有效的工具支持。

Abstract: Malware often uses obfuscation to hinder security analysis. Among these techniques, virtualization-based obfuscation is particularly strong because it protects programs by translating original instructions into attacker-defined virtual machine (VM) bytecode, producing long and complex code that is difficult to analyze and deobfuscate. This paper aims to identify the structural components of virtualization-based obfuscation through static analysis. By examining the execution model of obfuscated code, we define and detect the key elements required for deobfuscation-namely the dispatch routine, handler blocks, and the VM region-using LLVM IR. Experimental results show that, in the absence of compiler optimizations, the proposed LLVM Pass successfully detects all core structures across major virtualization options, including switch, direct, and indirect modes.

</details>


### [25] [Your Privacy Depends on Others: Collusion Vulnerabilities in Individual Differential Privacy](https://arxiv.org/abs/2601.12922)
*Johannes Kaiser,Alexander Ziller,Eleni Triantafillou,Daniel Rückert,Georgios Kaissis*

Main category: cs.CR

TL;DR: 个体差分隐私(iDP)存在被忽视的漏洞：采样机制中个体的隐私风险不仅取决于自身隐私预算，还受其他所有数据贡献者隐私选择的影响，导致个体隐私控制的承诺与实际集体决定风险的现实不匹配。


<details>
  <summary>Details</summary>
Motivation: 揭示iDP系统中被忽视的漏洞：虽然iDP承诺用户控制自己的隐私，但实际上个体的隐私风险受到其他所有用户隐私选择的影响，这种集体决定风险与个体隐私控制的承诺存在根本性不匹配。

Method: 通过实证分析揭示采样机制中iDP的漏洞，展示特定隐私偏好分布如何无意中增加个体隐私风险，并演示攻击者如何利用此漏洞。提出基于Δ-散度的(ε_i,δ_i,Δ̄)-iDP隐私合约作为缓解方案。

Result: 实证评估显示62%的目标个体成功受到攻击，其成员推理易感性显著增加。攻击完全在DP保证范围内进行，隐藏了额外的脆弱性。

Conclusion: 当前iDP范式存在根本性挑战，需要重新评估iDP系统的设计、审计、沟通和部署方式，使超额风险透明且可控。提出的(ε_i,δ_i,Δ̄)-iDP合约为用户提供了超额脆弱性的硬上限。

Abstract: Individual Differential Privacy (iDP) promises users control over their privacy, but this promise can be broken in practice. We reveal a previously overlooked vulnerability in sampling-based iDP mechanisms: while conforming to the iDP guarantees, an individual's privacy risk is not solely governed by their own privacy budget, but critically depends on the privacy choices of all other data contributors. This creates a mismatch between the promise of individual privacy control and the reality of a system where risk is collectively determined. We demonstrate empirically that certain distributions of privacy preferences can unintentionally inflate the privacy risk of individuals, even when their formal guarantees are met. Moreover, this excess risk provides an exploitable attack vector. A central adversary or a set of colluding adversaries can deliberately choose privacy budgets to amplify vulnerabilities of targeted individuals. Most importantly, this attack operates entirely within the guarantees of DP, hiding this excess vulnerability. Our empirical evaluation demonstrates successful attacks against 62% of targeted individuals, substantially increasing their membership inference susceptibility. To mitigate this, we propose $(\varepsilon_i,δ_i,\overlineΔ)$-iDP a privacy contract that uses $Δ$-divergences to provide users with a hard upper bound on their excess vulnerability, while offering flexibility to mechanism design. Our findings expose a fundamental challenge to the current paradigm, demanding a re-evaluation of how iDP systems are designed, audited, communicated, and deployed to make excess risks transparent and controllable.

</details>


### [26] [On the Evidentiary Limits of Membership Inference for Copyright Auditing](https://arxiv.org/abs/2601.12937)
*Murat Bilgehan Ertan,Emirhan Böge,Min Chen,Kaleel Mahmood,Marten van Dijk*

Main category: cs.CR

TL;DR: 论文研究了在对抗性版权纠纷中，成员推断攻击（MIAs）作为证据的可靠性问题，发现MIAs在面对语义保持的文本改写时表现脆弱。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型在日益不透明的语料库上训练，成员推断攻击被提出用于审计版权文本是否被用于训练，但其在现实条件下的可靠性受到质疑。研究旨在评估MIAs在对抗性版权纠纷中作为证据的可行性。

Method: 提出了SAGE（Structure-Aware SAE-Guided Extraction）框架，这是一个基于稀疏自编码器（SAEs）的改写框架，能够改变训练数据的词汇结构同时保持语义内容和下游实用性。通过法官-检察官-被告通信协议形式化对抗性设置。

Result: 实验表明，当模型在SAGE生成的改写文本上进行微调时，最先进的MIAs性能下降，表明其信号对语义保持的转换不鲁棒。虽然在某些微调机制下仍存在一些信息泄漏，但结果表明确实存在脆弱性。

Conclusion: MIAs在对抗性设置中表现脆弱，仅凭自身不足以作为LLMs版权审计的独立机制。需要更鲁棒的审计方法来应对语义保持的数据改写。

Abstract: As large language models (LLMs) are trained on increasingly opaque corpora, membership inference attacks (MIAs) have been proposed to audit whether copyrighted texts were used during training, despite growing concerns about their reliability under realistic conditions. We ask whether MIAs can serve as admissible evidence in adversarial copyright disputes where an accused model developer may obfuscate training data while preserving semantic content, and formalize this setting through a judge-prosecutor-accused communication protocol. To test robustness under this protocol, we introduce SAGE (Structure-Aware SAE-Guided Extraction), a paraphrasing framework guided by Sparse Autoencoders (SAEs) that rewrites training data to alter lexical structure while preserving semantic content and downstream utility. Our experiments show that state-of-the-art MIAs degrade when models are fine-tuned on SAGE-generated paraphrases, indicating that their signals are not robust to semantics-preserving transformations. While some leakage remains in certain fine-tuning regimes, these results suggest that MIAs are brittle in adversarial settings and insufficient, on their own, as a standalone mechanism for copyright auditing of LLMs.

</details>


### [27] [KinGuard: Hierarchical Kinship-Aware Fingerprinting to Defend Against Large Language Model Stealing](https://arxiv.org/abs/2601.12986)
*Zhenhua Xu,Xiaoning Tian,Wenjun Zeng,Wenpeng Xing,Tianliang Lu,Gaolei Li,Chaochao Chen,Meng Han*

Main category: cs.CR

TL;DR: KinGuard：基于亲属关系知识嵌入的LLM所有权验证框架，解决传统后门指纹方法的隐蔽性与鲁棒性矛盾


<details>
  <summary>Details</summary>
Motivation: 保护大型语言模型的知识产权需要可靠的所有权验证方法。传统的后门指纹方法存在隐蔽性与鲁棒性矛盾：为了鲁棒性，这些方法迫使模型记忆对高困惑度触发词的固定响应，但这种针对性过拟合会产生可检测的统计特征。

Method: 提出KinGuard框架，通过增量预训练将基于结构化亲属关系叙事的私有知识语料嵌入模型。模型不是记忆表面触发词，而是内化这些知识概念，通过探测模型的概念理解来验证所有权。

Result: 大量实验表明KinGuard在有效性、隐蔽性和抗攻击性方面表现优越，能够抵抗微调、输入扰动和模型合并等多种攻击。

Conclusion: 该研究确立了基于知识嵌入的模型指纹方法作为一个实用且安全的范式，为解决LLM所有权验证问题提供了新思路。

Abstract: Protecting the intellectual property of large language models requires robust ownership verification. Conventional backdoor fingerprinting, however, is flawed by a stealth-robustness paradox: to be robust, these methods force models to memorize fixed responses to high-perplexity triggers, but this targeted overfitting creates detectable statistical artifacts. We resolve this paradox with KinGuard, a framework that embeds a private knowledge corpus built on structured kinship narratives. Instead of memorizing superficial triggers, the model internalizes this knowledge via incremental pre-training, and ownership is verified by probing its conceptual understanding. Extensive experiments demonstrate KinGuard's superior effectiveness, stealth, and resilience against a battery of attacks including fine-tuning, input perturbation, and model merging. Our work establishes knowledge-based embedding as a practical and secure paradigm for model fingerprinting.

</details>


### [28] [Post-Quantum Secure Aggregation via Code-Based Homomorphic Encryption](https://arxiv.org/abs/2601.13031)
*Sebastian Bitzer,Maximilian Egger,Mumin Liu,Antonia Wachter-Zeh*

Main category: cs.CR

TL;DR: 提出了一种基于编码的量子安全聚合方案，使用LPN假设替代传统的格基加密，通过委员会解密和CRT优化降低通信成本


<details>
  <summary>Details</summary>
Motivation: 现有后量子安全聚合方案主要依赖格基加密假设，需要探索基于其他数学难题的替代方案以增强多样性

Method: 基于LPN假设构建密钥和消息加法同态加密框架，采用委员会解密机制（通过秘密共享实现），并引入CRT优化降低通信开销

Result: 在Hint-LPN假设下证明方案安全性，显示与标准LPN的等价性，性能评估表明在某些场景下优于信息论安全聚合协议

Conclusion: 成功构建了基于编码的量子安全聚合方案，为后量子密码学提供了格基加密之外的可行替代方案

Abstract: Secure aggregation enables aggregation of inputs from multiple parties without revealing individual contributions to the server or other clients. Existing post-quantum approaches based on homomorphic encryption offer practical efficiency but predominantly rely on lattice-based hardness assumptions. We present a code-based alternative for secure aggregation by instantiating a general framework based on key- and message-additive homomorphic encryption under the Learning Parity with Noise (LPN) assumption. Our construction employs a committee-based decryptor realized via secret sharing and incorporates a Chinese Remainder Theorem (CRT)-based optimization to reduce the communication costs of LPN-based instantiations. We analyze the security of the proposed scheme under a new Hint-LPN assumption and show that it is equivalent to standard LPN for suitable parameters. Finally, we evaluate performance and identify regimes in which our approach outperforms information-theoretically secure aggregation protocols.

</details>


### [29] [Adversarial News and Lost Profits: Manipulating Headlines in LLM-Driven Algorithmic Trading](https://arxiv.org/abs/2601.13082)
*Advije Rizvani,Giovanni Apruzzese,Pavel Laskov*

Main category: cs.CR

TL;DR: 研究量化了针对LLM支持的算法交易系统的对抗性新闻攻击，通过不可见文本操纵使年回报率降低高达17.7个百分点


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型在金融领域的应用增加，它们被用于分析财经新闻情感以指导交易决策。然而，这种实践存在风险：攻击者可能制作"对抗性新闻"来误导LLM，特别是包含人类不可见但对LLM有影响的恶意内容。虽然已有研究关注文本对抗样本，但尚未从货币风险角度量化其对LLM支持的算法交易系统的系统性影响。

Method: 研究考虑了一个无法直接访问算法交易系统但能单日篡改股票相关新闻标题的攻击者。评估了两种人类难以察觉的操纵方法：1) Unicode同形异义字替换，误导模型在股票名称识别时；2) 隐藏文本条款，改变新闻标题的情感倾向。在Backtrader中实现了一个现实的算法交易系统，融合了基于LSTM的价格预测和LLM衍生的情感分析（使用FinBERT、FinGPT、FinLLaMA和六个通用LLM），并使用投资组合指标量化货币影响。

Result: 在真实世界数据上的实验表明，操纵单日攻击在14个月内能可靠地误导LLM，并使年回报率降低高达17.7个百分点。通过分析流行的爬虫库和交易平台，并调查27名金融科技从业者，确认了这种攻击的现实可行性。已通知交易平台所有者此安全问题。

Conclusion: 研究表明，针对LLM支持的算法交易系统的对抗性新闻攻击具有显著的货币风险，攻击者通过人类难以察觉的文本操纵就能造成重大财务损失。这突显了金融领域LLM应用的安全脆弱性，需要开发更强大的防御机制来保护算法交易系统免受此类攻击。

Abstract: Large Language Models (LLMs) are increasingly adopted in the financial domain. Their exceptional capabilities to analyse textual data make them well-suited for inferring the sentiment of finance-related news. Such feedback can be leveraged by algorithmic trading systems (ATS) to guide buy/sell decisions. However, this practice bears the risk that a threat actor may craft "adversarial news" intended to mislead an LLM. In particular, the news headline may include "malicious" content that remains invisible to human readers but which is still ingested by the LLM. Although prior work has studied textual adversarial examples, their system-wide impact on LLM-supported ATS has not yet been quantified in terms of monetary risk. To address this threat, we consider an adversary with no direct access to an ATS but able to alter stock-related news headlines on a single day. We evaluate two human-imperceptible manipulations in a financial context: Unicode homoglyph substitutions that misroute models during stock-name recognition, and hidden-text clauses that alter the sentiment of the news headline. We implement a realistic ATS in Backtrader that fuses an LSTM-based price forecast with LLM-derived sentiment (FinBERT, FinGPT, FinLLaMA, and six general-purpose LLMs), and quantify monetary impact using portfolio metrics. Experiments on real-world data show that manipulating a one-day attack over 14 months can reliably mislead LLMs and reduce annual returns by up to 17.7 percentage points. To assess real-world feasibility, we analyze popular scraping libraries and trading platforms and survey 27 FinTech practitioners, confirming our hypotheses. We notified trading platform owners of this security issue.

</details>


### [30] [CODE: A Contradiction-Based Deliberation Extension Framework for Overthinking Attacks on Retrieval-Augmented Generation](https://arxiv.org/abs/2601.13112)
*Xiaolei Zhang,Xiaojun Jia,Liquan Chen,Songze Li*

Main category: cs.CR

TL;DR: 本文提出了一种针对检索增强生成系统中推理模型的攻击框架CODE，通过注入包含逻辑与证据层矛盾的毒化样本，诱使模型产生不必要的推理标记消耗，而不会影响任务准确性。


<details>
  <summary>Details</summary>
Motivation: 虽然推理模型通过逐步推理、逻辑一致性和多步自验证提升了RAG系统的任务性能，但研究发现推理模型容易受到"过度思考攻击"的影响。本文揭示这种风险会遗传给配备推理模型的RAG系统，需要研究相应的攻击机制和防御方法。

Method: 提出名为Contradiction-Based Deliberation Extension (CODE)的端到端攻击框架，采用多智能体架构构建毒化样本注入知识库。这些样本具有两个关键特征：1)与用户查询高度相关，确保能被检索作为推理模型输入；2)包含逻辑层与证据层之间的矛盾，导致模型过度思考，并通过优化展现高度多样化的风格。

Result: 在两个数据集上对五个商业推理模型进行的广泛实验表明，CODE攻击导致推理标记消耗增加5.32倍到24.72倍，而任务性能不受影响。攻击的推理开销极难检测，因为无需修改用户查询。

Conclusion: CODE攻击框架成功揭示了RAG系统中推理模型继承过度思考风险的问题，并讨论了评估潜在的对策来减轻这种风险。该研究强调了在部署推理增强的RAG系统时需要考虑的安全隐患。

Abstract: Introducing reasoning models into Retrieval-Augmented Generation (RAG) systems enhances task performance through step-by-step reasoning, logical consistency, and multi-step self-verification. However, recent studies have shown that reasoning models suffer from overthinking attacks, where models are tricked to generate unnecessarily high number of reasoning tokens. In this paper, we reveal that such overthinking risk can be inherited by RAG systems equipped with reasoning models, by proposing an end-to-end attack framework named Contradiction-Based Deliberation Extension (CODE). Specifically, CODE develops a multi-agent architecture to construct poisoning samples that are injected into the knowledge base. These samples 1) are highly correlated with the use query, such that can be retrieved as inputs to the reasoning model; and 2) contain contradiction between the logical and evidence layers that cause models to overthink, and are optimized to exhibit highly diverse styles. Moreover, the inference overhead of CODE is extremely difficult to detect, as no modification is needed on the user query, and the task accuracy remain unaffected. Extensive experiments on two datasets across five commercial reasoning models demonstrate that the proposed attack causes a 5.32x-24.72x increase in reasoning token consumption, without degrading task performance. Finally, we also discuss and evaluate potential countermeasures to mitigate overthinking risks.

</details>


### [31] [Function Recovery Attacks in Gate-Hiding Garbled Circuits using SAT Solving](https://arxiv.org/abs/2601.13271)
*Chao Yin,Zunchen Huang,Chenglu Jin,Marten van Dijk,Fabio Massacci*

Main category: cs.CR

TL;DR: 该论文分析了门隐藏电路拓扑泄露对函数隐私的实际影响，提出了基于SAT的函数恢复攻击方法，在ISCAS基准电路上实现了最高159倍的恢复速度提升。


<details>
  <summary>Details</summary>
Motivation: 半私有函数评估技术旨在保护输入数据和函数逻辑，但现有的门隐藏电路方法只隐藏门功能而公开电路拓扑。现有安全定义故意排除拓扑泄露，导致对函数隐私的实际影响理解不足。

Method: 提出基于SAT的函数恢复攻击，从公开的电路拓扑重建隐藏的门操作。为处理更大更复杂的电路，开发了增量SAT求解框架和一组可组合的拓扑保持简化定理，减少SAT实例规模并逐步约束搜索空间。

Result: 在ISCAS基准电路、代表性安全计算电路和容错传感器融合电路上评估攻击效果，在24小时恢复预算内，相比基线方法实现了最高159倍的恢复速度提升，且不增加预言机查询次数。

Conclusion: 拓扑泄露本身在实践中就能实现有效的函数恢复，这表明仅隐藏门功能而公开电路拓扑可能不足以保护函数隐私，需要重新评估半私有函数评估的安全模型。

Abstract: Semi-Private Function Evaluation enables joint computation while protecting both input data and function logic. A practical instantiation is gate-hiding garbled circuits, which conceal gate functionalities while revealing the circuit topology. Existing security definitions intentionally exclude leakage through circuit topology, leaving the concrete impact of such leakage on function privacy insufficiently understood.
  We analyze the empirical security of gate hiding under two adversarial models that capture realistic computational capabilities. We present a SAT-based function-recovery attack that reconstructs hidden gate operations from a circuit's public topology. To enable recovery on larger and more complex circuits, we develop an incremental SAT-solving framework combined with a set of composable, topology-preserving simplification theorems. These techniques jointly reduce the SAT instance size and progressively constrain the search space across repeated solving iterations.
  We evaluate our attack on ISCAS benchmarks, representative secure computation circuits, and fault-tolerant sensor fusion circuits under a fixed 24-hour recovery budget. Compared to baseline approaches, our optimized attack achieves up to a 159-fold speedup in recovery time without increasing the number of oracle queries. Our results demonstrate that topology leakage alone can enable effective function recovery in practice.

</details>


### [32] [QERS: Quantum Encryption Resilience Score for Post-Quantum Cryptography in Computer, IoT, and IIoT Systems](https://arxiv.org/abs/2601.13399)
*Jonatan Rassekhnia*

Main category: cs.CR

TL;DR: QERS框架通过整合密码性能、系统约束和多标准决策分析，为物联网环境提供后量子密码准备度评估的通用测量框架


<details>
  <summary>Details</summary>
Motivation: 后量子密码对保护物联网系统免受量子攻击至关重要，但现有评估方法主要关注孤立性能指标，缺乏对整体安全性和部署决策的全面支持

Method: 提出QERS（量子加密韧性评分）框架，结合归一化指标、加权聚合和机器学习辅助分析，在异构设备和通信协议中生成可解释的韧性评分

Result: 实验结果表明该框架能够在实际资源约束下对后量子方案进行对比评估，支持明智的安全设计和迁移规划

Conclusion: QERS为计算机、物联网和工业物联网环境提供了一种评估后量子密码准备度的通用测量框架，目前作为预印本发布，计划在后续研究中扩展统计验证

Abstract: Post-quantum cryptography (PQC) is becoming essential for securing Internet of Things (IoT) and Industrial IoT (IIoT) systems against quantum-enabled adversaries. However, existing evaluation approaches primarily focus on isolated performance metrics, offering limited support for holistic security and deployment decisions. This paper introduces QERS (Quantum Encryption Resilience Score), a universal measurement framework that integrates cryptographic performance, system constraints, and multi-criteria decision analysis to assess PQC readiness in computer, IoT, and IIoT environments. QERS combines normalized metrics, weighted aggregation, and machine learning-assisted analysis to produce interpretable resilience scores across heterogeneous devices and communication protocols. Experimental results demonstrate how the framework enables comparative evaluation of post-quantum schemes under realistic resource constraints, supporting informed security design and migration planning. This work is presented as a preprint, with extended statistical validation planned as part of ongoing graduate research.

</details>


### [33] [Quantum Encryption Resilience Score (QERS) for MQTT, HTTP, and HTTPS under Post-Quantum Cryptography in Computer, IoT, and IIoT Systems](https://arxiv.org/abs/2601.13423)
*Jonatan Rassekhnia*

Main category: cs.CR

TL;DR: 该论文通过实验评估了后量子密码学（PQC）下MQTT、HTTP和HTTPS通信协议的性能，提出了量子加密弹性评分（QERS）框架，发现MQTT在PQC约束下效率最高，而HTTPS提供最高安全弹性但资源消耗更大。


<details>
  <summary>Details</summary>
Motivation: 后量子密码学（PQC）带来了显著的计算和通信开销，这对资源受限的计算机系统、物联网（IoT）和工业物联网（IIoT）设备构成了挑战。需要评估不同通信协议在PQC环境下的性能表现，为协议选择和迁移规划提供依据。

Method: 使用ESP32-C6客户端和基于ARM的Raspberry Pi CM4服务器，在现实操作条件下测量MQTT、HTTP和HTTPS协议的延迟、CPU利用率、RSSI、能耗、密钥大小和TLS握手开销。提出QERS框架，将这些异构指标整合为归一化的基础评分、调优评分和融合评分。

Result: 实验结果表明，在PQC约束下，MQTT提供了最高的效率，而HTTPS实现了最高的安全加权弹性，但代价是增加了延迟和资源消耗。QERS框架能够系统比较协议效率和安全性弹性。

Conclusion: 提出的QERS框架支持为PQC启用的物联网和工业物联网部署提供明智的协议选择和迁移规划，帮助在安全性和效率之间做出平衡决策。

Abstract: Post-quantum cryptography (PQC) introduces significant computational and communication overhead, which poses challenges for resource-constrained computer systems, Internet of Things (IoT), and Industrial IoT (IIoT) devices. This paper presents an experimental evaluation of the Quantum Encryption Resilience Score (QERS) applied to MQTT, HTTP, and HTTPS communication protocols operating under PQC. Using an ESP32-C6 client and an ARM-based Raspberry Pi CM4 server, latency, CPU utilization, RSSI, energy consumption, key size, and TLS handshake overhead are measured under realistic operating conditions. QERS integrates these heterogeneous metrics into normalized Basic, Tuned, and Fusion scores, enabling systematic comparison of protocol efficiency and security resilience. Experimental results show that MQTT provides the highest efficiency under PQC constraints, while HTTPS achieves the highest security-weighted resilience at the cost of increased latency and resource consumption. The proposed framework supports informed protocol selection and migration planning for PQC-enabled IoT and IIoT deployments.

</details>


### [34] [A Scientific Data Integrity system based on Blockchain](https://arxiv.org/abs/2601.13425)
*Gian Sebastian Mier Bello,Alexander Martinez Mendez,Carlos J. Barrios H.,Robinson Rivas,Luis A. Núñez*

Main category: cs.CR

TL;DR: 该论文提出了一种基于区块链的新方法，用于验证分布式科学数据存储库中的数据完整性，确保数据不被篡改且可复现。


<details>
  <summary>Details</summary>
Motivation: 高性能计算项目中存在大量来自不同来源的数据，其中一些数据规模巨大难以复制。科学研究要求数据保持原始状态以便不同研究组能够复现结果、讨论理论和相互验证。

Method: 采用区块链技术开发了一个原型系统，利用区块链的去中心化、不可篡改特性来确保数据完整性。系统实现了安全的数据访问管理、便捷的数据完整性验证以及添加新记录时保持相同完整性策略的功能。

Result: 原型系统使用拉丁美洲巨型天文台（LAGO）项目的公共数据集子集进行了测试，验证了该方法的可行性。

Conclusion: 区块链技术能够有效帮助研究团队验证分布式存储库中的数据完整性，确保科学数据的可靠性和可复现性。

Abstract: In most High Performance Computing (HPC) projects nowadays, there is a lot of data obtained from different sources, depending on the project's objectives. Some of that data is very huge in terms of size, so copying such data sometimes is an unrealistic goal. On the other hand, science requires data used for different purposes to remain unaltered, so different groups of researchers can reproduce results, discuss theories, and validate each other. In this paper, we present a novel approach to help research groups to validate data integrity on such distributed repositories using Blockchain. Originally developed for cryptographic currencies, Blockchain has demonstrated a versatile range of uses. Our proposal ensures 1) secure access to data management, 2) easy validation of data integrity, and 3) an easy way to add new records to the dataset with the same robust integrity policy. A prototype was developed and tested using a subset of a public dataset from a real scientific collaboration, the Latin American Giant Observatory (LAGO) Project.

</details>


### [35] [Automatic Adjustment of HPA Parameters and Attack Prevention in Kubernetes Using Random Forests](https://arxiv.org/abs/2601.13515)
*Hanlin Zhou,Huah Yong Chan,Jingfei Ni,Mengchun Wu,Qing Deng*

Main category: cs.CR

TL;DR: 论文提出了一种基于HTTP状态码和随机森林算法的HPA动态调整方法，用于攻击流量管理和蜜罐重定向


<details>
  <summary>Details</summary>
Motivation: 在云原生环境中，传统的HPA（水平Pod自动扩展）在面对攻击流量时容易过度扩展资源，导致资源浪费和成本增加。需要一种智能的方法来区分正常流量和攻击流量，并动态调整HPA参数以有效管理攻击

Method: 1. 使用HTTP状态码作为HPA中的自定义指标
2. 集成随机森林分类算法评估和预测攻击
3. 动态调整HPA的最大Pod参数来管理攻击流量
4. 将所有攻击IP的访问重定向到蜜罐Pod
5. 在不同条件下进行实验验证

Result: 1. 在高负载条件下通过HPA Pod调整降低了5XX状态码的发生率
2. 有效隔离了攻击流量，防止因攻击导致的HPA过度扩展
3. 实验表明设置适当的HPA调整阈值至关重要

Conclusion: 该方法通过机器学习算法智能识别攻击流量，结合蜜罐技术和动态HPA参数调整，实现了对攻击流量的有效管理，同时避免了资源浪费。适当的阈值设置是系统有效运行的关键因素

Abstract: In this paper, HTTP status codes are used as custom metrics within the HPA as the experimental scenario. By integrating the Random Forest classification algorithm from machine learning, attacks are assessed and predicted, dynamically adjusting the maximum pod parameter in the HPA to manage attack traffic. This approach enables the adjustment of HPA parameters using machine learning scripts in targeted attack scenarios while effectively managing attack traffic. All access from attacking IPs is redirected to honeypot pods, achieving a lower incidence of 5XX status codes through HPA pod adjustments under high load conditions. This method also ensures effective isolation of attack traffic, preventing excessive HPA expansion due to attacks. Additionally, experiments conducted under various conditions demonstrate the importance of setting appropriate thresholds for HPA adjustments.

</details>


### [36] [Eliciting Harmful Capabilities by Fine-Tuning On Safeguarded Outputs](https://arxiv.org/abs/2601.13528)
*Jackson Kaunismaa,Avery Griffin,John Hughes,Christina Q. Knight,Mrinank Sharma,Erik Jones*

Main category: cs.CR

TL;DR: 论文展示了一种针对前沿模型安全防护的规避攻击方法，通过构造相邻领域的无害提示获取响应，然后微调开源模型来恢复有害能力，在危险化学品合成领域恢复了约40%的能力差距。


<details>
  <summary>Details</summary>
Motivation: 研究动机是探索前沿模型安全防护的局限性，特别是输出级防护措施在生态系统层面可能存在的风险。尽管模型开发者实施了防护措施（如分类器过滤危险输出），但攻击者可能通过间接方式绕过这些防护。

Method: 提出了一种三阶段的诱导攻击方法：1）构造目标有害任务相邻领域的提示，这些提示不直接请求危险信息；2）从受保护的前沿模型获取这些提示的响应；3）使用这些提示-响应对微调开源模型。由于请求的提示不能直接造成伤害，因此不会被前沿模型的防护机制拒绝。

Result: 在危险化学品合成和处理领域进行评估，攻击恢复了开源模型与无限制前沿模型之间约40%的能力差距。攻击效果随着前沿模型能力和生成的微调数据量的增加而提升。

Conclusion: 研究表明仅依靠输出级防护措施难以缓解生态系统层面的风险，需要更全面的安全防护策略来应对这种间接攻击方式。

Abstract: Model developers implement safeguards in frontier models to prevent misuse, for example, by employing classifiers to filter dangerous outputs. In this work, we demonstrate that even robustly safeguarded models can be used to elicit harmful capabilities in open-source models through elicitation attacks. Our elicitation attacks consist of three stages: (i) constructing prompts in adjacent domains to a target harmful task that do not request dangerous information; (ii) obtaining responses to these prompts from safeguarded frontier models; (iii) fine-tuning open-source models on these prompt-output pairs. Since the requested prompts cannot be used to directly cause harm, they are not refused by frontier model safeguards. We evaluate these elicitation attacks within the domain of hazardous chemical synthesis and processing, and demonstrate that our attacks recover approximately 40% of the capability gap between the base open-source model and an unrestricted frontier model. We then show that the efficacy of elicitation attacks scales with the capability of the frontier model and the amount of generated fine-tuning data. Our work demonstrates the challenge of mitigating ecosystem level risks with output-level safeguards.

</details>


### [37] [When Reasoning Leaks Membership: Membership Inference Attack on Black-box Large Reasoning Models](https://arxiv.org/abs/2601.13607)
*Ruihan Hu,Yu-Ming Shang,Wei Luo,Ye Tao,Xi Zhang*

Main category: cs.CR

TL;DR: 该论文首次系统性地探索了针对黑盒大型推理模型（LRMs）的成员推理攻击，发现模型暴露的中间推理轨迹会泄露成员信息，提出了BlackSpectrum攻击框架，并创建了两个新数据集支持未来研究。


<details>
  <summary>Details</summary>
Motivation: 现代黑盒大型推理模型（如Gemini-2.5和Claude-sonnet）通过API暴露中间推理轨迹以提高透明度，但这些轨迹可能泄露成员信息，即使没有访问先前攻击所需的token logits。作者发现这是一个新的隐私威胁，需要进行系统性研究。

Method: 作者首先分析发现LRMs对熟悉的训练成员样本会产生自信、类似回忆的推理轨迹，而对非成员样本则产生犹豫、类似推理的轨迹。基于此观察，提出了BlackSpectrum攻击框架，核心思想是在语义潜在空间中构建"回忆-推理轴"，根据暴露的推理轨迹表示来确定查询样本在该轴上的位置，从而获得成员分数。此外，还创建了arXivReasoning和BookReasoning两个新数据集以支持现代LRMs的研究。

Result: 实证研究表明，暴露推理轨迹显著增加了LRMs对成员推理攻击的脆弱性，导致攻击性能大幅提升。这表明LRM公司需要在中间推理轨迹的透明度和隐私保护之间取得平衡。

Conclusion: 该研究首次系统性地探索了针对黑盒LRMs的成员推理攻击，揭示了暴露推理轨迹带来的隐私风险，提出了有效的攻击框架，并提供了新的数据集支持未来研究，强调了在模型透明度和隐私保护之间平衡的重要性。

Abstract: Large Reasoning Models (LRMs) have rapidly gained prominence for their strong performance in solving complex tasks. Many modern black-box LRMs expose the intermediate reasoning traces through APIs to improve transparency (e.g., Gemini-2.5 and Claude-sonnet). Despite their benefits, we find that these traces can leak membership signals, creating a new privacy threat even without access to token logits used in prior attacks. In this work, we initiate the first systematic exploration of Membership Inference Attacks (MIAs) on black-box LRMs. Our preliminary analysis shows that LRMs produce confident, recall-like reasoning traces on familiar training member samples but more hesitant, inference-like reasoning traces on non-members. The representations of these traces are continuously distributed in the semantic latent space, spanning from familiar to unfamiliar samples. Building on this observation, we propose BlackSpectrum, the first membership inference attack framework targeting the black-box LRMs. The key idea is to construct a recall-inference axis in the semantic latent space, based on representations derived from the exposed traces. By locating where a query sample falls along this axis, the attacker can obtain a membership score and predict how likely it is to be a member of the training data. Additionally, to address the limitations of outdated datasets unsuited to modern LRMs, we provide two new datasets to support future research, arXivReasoning and BookReasoning. Empirically, exposing reasoning traces significantly increases the vulnerability of LRMs to membership inference attacks, leading to large gains in attack performance. Our findings highlight the need for LRM companies to balance transparency in intermediate reasoning traces with privacy preservation.

</details>


### [38] [PINA: Prompt Injection Attack against Navigation Agents](https://arxiv.org/abs/2601.13612)
*Jiani Liu,Yixin He,Lanlan Fan,Qidi Zhong,Yushi Cheng,Meng Zhang,Yanjiao Chen,Wenyuan Xu*

Main category: cs.CR

TL;DR: PINA是一个针对导航智能体的自适应提示优化攻击框架，在室内外导航场景中平均攻击成功率87.5%，揭示了基于大语言模型的导航智能体面临严重的提示注入安全风险。


<details>
  <summary>Details</summary>
Motivation: 基于大语言模型的导航智能体将自然语言指令转换为可执行计划，其安全性比纯文本应用更为关键——成功的提示注入攻击不仅会改变输出，还可能直接误导物理导航，导致不安全路线、任务失败甚至现实世界伤害。然而，导航智能体对提示注入攻击的脆弱性尚未得到充分研究。

Method: 提出了PINA框架，这是一个专门为导航智能体设计的自适应提示优化框架，考虑了黑盒、长上下文和动作可执行性等约束条件。该框架通过优化提示来实施攻击。

Result: 在室内和室外导航智能体上的实验表明，PINA实现了高攻击成功率，平均ASR达到87.5%，超越了所有基线方法，并且在消融实验和自适应攻击条件下保持鲁棒性。

Conclusion: 这项研究首次系统性地调查了导航场景中的提示注入攻击，突显了这类攻击对具身大语言模型智能体的紧迫安全影响，强调了导航智能体安全性的重要性。

Abstract: Navigation agents powered by large language models (LLMs) convert natural language instructions into executable plans and actions. Compared to text-based applications, their security is far more critical: a successful prompt injection attack does not just alter outputs but can directly misguide physical navigation, leading to unsafe routes, mission failure, or real-world harm. Despite this high-stakes setting, the vulnerability of navigation agents to prompt injection remains largely unexplored. In this paper, we propose PINA, an adaptive prompt optimization framework tailored to navigation agents under black-box, long-context, and action-executable constraints. Experiments on indoor and outdoor navigation agents show that PINA achieves high attack success rates with an average ASR of 87.5%, surpasses all baselines, and remains robust under ablation and adaptive-attack conditions. This work provides the first systematic investigation of prompt injection attacks in navigation and highlights their urgent security implications for embodied LLM agents.

</details>


### [39] [ORCA - An Automated Threat Analysis Pipeline for O-RAN Continuous Development](https://arxiv.org/abs/2601.13681)
*Felix Klement,Alessandro Brighente,Michele Polese,Mauro Conti,Stefan Katzenbeisser*

Main category: cs.CR

TL;DR: 提出自动化漏洞评估管道，利用NLP技术自动将O-RAN漏洞映射到预定义威胁列表，实现迭代、定量、高效的威胁评分


<details>
  <summary>Details</summary>
Motivation: O-RAN云化部署引入新安全威胁，传统漏洞评估依赖人工、劳动密集且主观，导致威胁分析不一致，需要自动化解决方案

Method: 建立自动化管道，利用自然语言处理技术最小化人工干预，将真实漏洞映射到标准化格式的预定义威胁列表

Result: 首次实现迭代、定量、高效的评估，为O-RAN中单个漏洞和整个系统组件生成可靠威胁评分，通过示例实施展示有效性

Conclusion: 自动化NLP管道能有效集成到持续安全测试中，解决电信范式转变带来的独特安全挑战，支持DevSecOps方法

Abstract: The Open-Radio Access Network (O-RAN) integrates numerous software components in a cloud-like deployment, opening the radio access network to previously unconsidered security threats. With the ever-evolving threat landscape, integrating security practices through a DevSecOps approach is essential for fast and secure releases. Current vulnerability assessment practices often rely on manual, labor-intensive, and subjective investigations, leading to inconsistencies in the threat analysis. To mitigate these issues, we establish an automated pipeline that leverages Natural Language Processing (NLP) to minimize human intervention and associated biases. By mapping real-world vulnerabilities to predefined threat lists with a standardized input format, our approach is the first to enable iterative, quantitative, and efficient assessments, generating reliable threat scores for both individual vulnerabilities and entire system components within O-RAN. We illustrate the effectiveness of our framework through an example implementation for O-RAN, showcasing how continuous security testing can integrate into automated testing pipelines to address the unique security challenges of this paradigm shift in telecommunications.

</details>


### [40] [The Limits of Conditional Volatility: Assessing Cryptocurrency VaR under EWMA and IGARCH Models](https://arxiv.org/abs/2601.13757)
*Ekleen Kaur*

Main category: cs.CR

TL;DR: 传统静态GBM模型在加密货币风险管理中系统性失效，本文比较三种条件波动率模型，发现EWMA/IGARCH基线模型在高beta山寨币中表现最佳，正式拒绝了波动率均值回归和杠杆效应的传统金融假设。


<details>
  <summary>Details</summary>
Motivation: 标准静态GBM模型在加密货币风险管理中导致系统性失败（80.67%损失概率），主流GARCH文献忽视高beta山寨币这一资产类别，需要填补这一关键文献空白。

Method: 在相关蒙特卡洛VaR框架中比较测试三种条件波动率模型：EWMA/IGARCH基线模型、加入显式均值回归的IGARCH模型、以及改进的EGARCH风格不对称冲击模型，专门应用于高beta山寨币（XRP、SOL、ADA）。

Result: 强制平稳性（IGARCH+MR）严重低估下行风险（5%VaR减少50%），而不对称模型导致过度惩罚。只有具有无限波动率持续性的EWMA/IGARCH基线模型提供了稳健的条件波动率估计。

Conclusion: 正式拒绝了波动率均值回归和不对称杠杆效应的传统金融假设，确立非平稳框架是山寨币资产类别监管级风险建模的先决条件。

Abstract: The application of the standard static Geometric Brownian Motion (GBM) model for cryptocurrency risk management resulted in a systemic failure, evidenced by a 80.67% chance of loss in the 5% value-at-risk benchmark. This study addresses a critical literature gap by comparatively testing three conditional volatility models the EWMA/IGARCH baseline, an IGARCH model augmented with explicit mean reversion (IGARCH + MR), and a modified EGARCH-style asymmetric shock model within a correlated Monte Carlo VaR framework. Crucially, the analysis is applied specifically to high-beta altcoins (XRP, SOL, ADA), an asset class largely neglected by mainstream GARCH literature. Our results demonstrate that imposing stationarity (IGARCH + MR) drastically underestimates downside risk (5 percent value-at-risk reduced by 50%), while the asymmetric model (Model 3) leads to severe over-penalization. The EWMA/IGARCH baseline, characterized by infinite volatility persistence (alpha + beta = 1), provided the only robust conditional volatility estimate. This finding constitutes a formal rejection of the conventional financial hypotheses of volatility mean reversion and the asymmetric leverage effect in the altcoin asset class, establishing that non-stationary frameworks are a prerequisite for regulatory-grade risk modeling in this domain.

</details>


### [41] [MirageNet:A Secure, Efficient, and Scalable On-Device Model Protection in Heterogeneous TEE and GPU System](https://arxiv.org/abs/2601.13826)
*Huadi Zheng,Li Cheng,Yan Ding*

Main category: cs.CR

TL;DR: ConvShatter是一种新型卷积层混淆方案，通过在TEE中安全存储少量恢复参数，实现低延迟、高精度的模型隐私保护，相比现有方案显著降低开销。


<details>
  <summary>Details</summary>
Motivation: 随着边缘设备计算能力增强，在不可信硬件上部署高性能DNN模型成为降低推理延迟和保护用户隐私的实用方法。现有参数混淆方案存在效率与安全性矛盾：部分混淆防御无效，而强健方案导致不可接受的延迟。

Method: 利用卷积线性特性将卷积核分解为关键核和公共核，注入混淆诱饵，并置换通道/核顺序。部署前执行核分解、诱饵注入和顺序混淆，将少量恢复参数安全存储在TEE中。推理时，TEE重构混淆卷积层的输出。

Result: 实验表明ConvShatter在保持强安全保证的同时显著降低延迟开销；相比同类方案GroupCover，相对开销降低16%，同时保持与原始模型相当的准确率。

Conclusion: ConvShatter通过创新的卷积核混淆方案，有效解决了模型隐私保护与运行时效率的平衡问题，为边缘设备上的安全DNN推理提供了实用解决方案。

Abstract: As edge devices gain stronger computing power, deploying high-performance DNN models on untrusted hardware has become a practical approach to cut inference latency and protect user data privacy. Given high model training costs and user experience requirements, balancing model privacy and low runtime overhead is critical. TEEs offer a viable defense, and prior work has proposed heterogeneous GPU-TEE inference frameworks via parameter obfuscation to balance efficiency and confidentiality. However, recent studies find partial obfuscation defenses ineffective, while robust schemes cause unacceptable latency. To resolve these issues, we propose ConvShatter, a novel obfuscation scheme that achieves low latency and high accuracy while preserving model confidentiality and integrity. It leverages convolution linearity to decompose kernels into critical and common ones, inject confounding decoys, and permute channel/kernel orders. Pre-deployment, it performs kernel decomposition, decoy injection and order obfuscation, storing minimal recovery parameters securely in the TEE. During inference, the TEE reconstructs outputs of obfuscated convolutional layers. Extensive experiments show ConvShatter substantially reduces latency overhead with strong security guarantees; versus comparable schemes, it cuts overhead by 16% relative to GroupCover while maintaining accuracy on par with the original model.

</details>


### [42] [Robust Reversible Watermarking in Encrypted Images Based on Dual-MSBs Spiral Embedding](https://arxiv.org/abs/2601.13840)
*Haoyu Shen,Wen Yin,Zhaoxia Yin,Wan-Li Lyu,Xinpeng Zhang*

Main category: cs.CR

TL;DR: 本文提出了一种新型的鲁棒可逆加密图像水印框架，通过双最高有效位平面嵌入结合空间冗余和纠错编码，在加密域中同时实现鲁棒性、可逆性和内容隐私保护。


<details>
  <summary>Details</summary>
Motivation: 现有的鲁棒可逆加密图像水印方案在嵌入容量严重受限的情况下，难以同时实现鲁棒性、可逆性和内容隐私保护。由于加密域中冗余不足，现有方案对噪声、有损压缩和裁剪攻击的鲁棒性有限。

Method: 提出了一种新颖的RRWEI框架，结合双最高有效位平面嵌入与空间冗余和纠错编码。通过压缩预测误差位平面来预留足够的嵌入空间和辅助信息用于无损重建。采用螺旋嵌入策略重新组织双最高有效位平面，将多个冗余水印副本分布在空间分散区域。

Result: 在标准测试图像上的实验结果表明，该方法在评估设置下始终优于现有方法，对高斯噪声、JPEG压缩和多种裁剪攻击具有更强的鲁棒性，同时保持完美的可逆性和高嵌入容量。与最先进的RRWEI方案相比，该框架在广泛的攻击场景下实现了显著更低的误码率和更稳定的性能。

Conclusion: 所提出的框架通过双最高有效位平面嵌入与空间冗余和纠错编码的耦合，有效解决了鲁棒可逆加密图像水印中同时实现鲁棒性、可逆性和内容隐私保护的挑战，为加密域中的鲁棒水印提供了新的解决方案。

Abstract: Robust reversible watermarking in encrypted images (RRWEI) faces an inherent challenge in simultaneously achieving robustness, reversibility, and content privacy under severely constrained embedding capacity. Existing RRWEI schemes often exhibit limited robustness against noise, lossy compression, and cropping attacks due to insufficient redundancy in the encrypted domain. To address this challenge, this paper proposes a novel RRWEI framework that couples dual most significant bit-plane (dual-MSBs) embedding with spatial redundancy and error-correcting coding. By compressing prediction-error bit-planes, sufficient embedding space and auxiliary information for lossless reconstruction are reserved. The dual-MSBs are further reorganized using a spiral embedding strategy to distribute multiple redundant watermark copies across spatially dispersed regions, enhancing robustness against both noise and spatial loss.Experimental results on standard test images demonstrate that the proposed method consistently outperforms under evaluated settings robustness against Gaussian noise, JPEG compression, and diverse cropping attacks, while maintaining perfect reversibility and high embedding capacity. Compared with state-of-the-art RRWEI schemes, the proposed framework achieves substantially lower bit-error rates and more stable performance under a wide range of attack scenarios.

</details>


### [43] [HardSecBench: Benchmarking the Security Awareness of LLMs for Hardware Code Generation](https://arxiv.org/abs/2601.13864)
*Qirui Chen,Jingxian Shuai,Shuangwu Chen,Shenghao Ye,Zijian Wen,Xufei Su,Jie Jin,Jiangming Li,Jun Chen,Xiaobin Tan,Jian Yang*

Main category: cs.CR

TL;DR: HardSecBench是一个包含924个任务的基准测试，涵盖Verilog RTL和固件级C代码，用于评估LLM在硬件和固件代码生成中的安全意识和漏洞检测能力。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注LLM生成代码的功能正确性，而忽视了其安全问题。功能上看似正确的LLM生成代码可能嵌入安全漏洞，部署后可能造成灾难性损害。这一关键研究空白促使研究者设计一个在现实规范下评估安全意识的基准测试。

Method: 提出HardSecBench基准测试，包含924个任务，覆盖76个硬件相关的CWE条目。每个任务包含结构化规范、安全参考实现和可执行测试。采用多智能体流水线，将合成与验证解耦，并将评估基于执行证据，实现可靠的自动化评估。

Result: 评估一系列LLM在硬件和固件代码生成中的表现，发现模型通常能满足功能要求但仍存在安全风险。安全结果随提示方式而变化，突显了LLM辅助硬件设计中存在的紧迫挑战。

Conclusion: 研究强调了LLM生成代码中安全漏洞的重要性，HardSecBench为评估LLM在硬件设计中的安全意识提供了可靠基准，为未来LLM辅助硬件设计的进步提供了可操作的见解。

Abstract: Large language models (LLMs) are being increasingly integrated into practical hardware and firmware development pipelines for code generation. Existing studies have primarily focused on evaluating the functional correctness of LLM-generated code, yet paid limited attention to its security issues. However, LLM-generated code that appears functionally sound may embed security flaws which could induce catastrophic damages after deployment. This critical research gap motivates us to design a benchmark for assessing security awareness under realistic specifications. In this work, we introduce HardSecBench, a benchmark with 924 tasks spanning Verilog Register Transfer Level (RTL) and firmware-level C, covering 76 hardware-relevant Common Weakness Enumeration (CWE) entries. Each task includes a structured specification, a secure reference implementation, and executable tests. To automate artifact synthesis, we propose a multi-agent pipeline that decouples synthesis from verification and grounds evaluation in execution evidence, enabling reliable evaluation. Using HardSecBench, we evaluate a range of LLMs on hardware and firmware code generation and find that models often satisfy functional requirements while still leaving security risks. We also find that security results vary with prompting. These findings highlight pressing challenges and offer actionable insights for future advancements in LLM-assisted hardware design. Our data and code will be released soon.

</details>


### [44] [Decentralized Infrastructure for Digital Notarizing, Signing and Sharing Files using Blockchain](https://arxiv.org/abs/2601.13907)
*Cosmin-Iulian Irimia*

Main category: cs.CR

TL;DR: 该论文提出基于区块链技术的去中心化数字公证、签署和共享文档基础设施，解决传统纸质文档管理中的安全、真实性和效率问题。


<details>
  <summary>Details</summary>
Motivation: 传统纸质文档管理存在安全、真实性和效率方面的挑战，即使数字化后，官方文档仍面临伪造、丢失和未经授权访问的脆弱性。

Method: 通过定义系统需求、评估现有解决方案，提出基于分布式系统的新型架构，结合密码学技术和去中心化存储。

Result: 研究为管理官方文档开发了更安全高效的框架，展示了区块链数字公证在简化官僚流程、降低安全风险和增强用户信任方面的潜力。

Conclusion: 区块链技术能够为数字文档管理提供透明、不可篡改且可行的解决方案，显著提升文档管理的安全性和效率。

Abstract: Traditional paper-based document management has long posed challenges related to security, authenticity, and efficiency. Despite advances in digitalization, official documents remain vulnerable to forgery, loss, and unauthorized access. This thesis proposes a decentralized infrastructure for digital notarization, signing, and sharing of documents using blockchain technology. The research addresses key issues of transparency, immutability, and feasibility by defining system requirements, evaluating existing solutions, and proposing a novel architecture based on distributed systems.
  By combining cryptographic techniques with decentralized storage, this research contributes to the development of a more secure and efficient framework for managing official documents. The findings highlight the potential of blockchain-based digital notarization to streamline bureaucratic processes, mitigate security risks, and enhance user trust in digital document management.

</details>


### [45] [VirtualCrime: Evaluating Criminal Potential of Large Language Models via Sandbox Simulation](https://arxiv.org/abs/2601.13981)
*Yilin Tang,Yu Wang,Lanlan Qiu,Wenchang Gao,Yunfei Ma,Baicheng Chen,Tianxing He*

Main category: cs.CR

TL;DR: VirtualCrime是一个基于三智能体系统的沙盒模拟框架，用于评估大语言模型的犯罪能力，包含40个多样化犯罪任务，发现LLM能够生成详细犯罪计划并执行智能犯罪过程。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型在多步决策、规划和行动方面展现出强大能力，并越来越多地集成到现实应用中，需要评估其强大的问题解决能力是否可能被滥用于犯罪活动。

Method: 提出VirtualCrime框架，包含三个智能体：攻击者智能体（犯罪团队领导者）、法官智能体（确定每个行动结果）、世界管理器智能体（更新环境状态和实体）。设计了40个多样化犯罪任务，覆盖11个地图和13种犯罪目标（如盗窃、抢劫、绑架、暴乱），并引入人类玩家基线作为参考。

Result: 评估了8个强大的LLM，发现：(1)所有智能体在模拟环境中都能合规地生成详细计划并执行智能犯罪过程，部分模型取得相对较高的成功率；(2)在某些情况下，智能体会采取严重伤害NPC的行动来实现目标。

Conclusion: 这项工作强调了在现实世界部署智能AI时需要加强安全对齐的重要性，揭示了LLM可能被滥用于犯罪活动的潜在风险。

Abstract: Large language models (LLMs) have shown strong capabilities in multi-step decision-making, planning and actions, and are increasingly integrated into various real-world applications. It is concerning whether their strong problem-solving abilities may be misused for crimes. To address this gap, we propose VirtualCrime, a sandbox simulation framework based on a three-agent system to evaluate the criminal capabilities of models. Specifically, this framework consists of an attacker agent acting as the leader of a criminal team, a judge agent determining the outcome of each action, and a world manager agent updating the environment state and entities. Furthermore, we design 40 diverse crime tasks within this framework, covering 11 maps and 13 crime objectives such as theft, robbery, kidnapping, and riot. We also introduce a human player baseline for reference to better interpret the performance of LLM agents. We evaluate 8 strong LLMs and find (1) All agents in the simulation environment compliantly generate detailed plans and execute intelligent crime processes, with some achieving relatively high success rates; (2) In some cases, agents take severe action that inflicts harm to NPCs to achieve their goals. Our work highlights the need for safety alignment when deploying agentic AI in real-world settings.

</details>


### [46] [A Security Framework for Chemical Functions](https://arxiv.org/abs/2601.14019)
*Frederik Walter,Hrishi Narayanan,Jessica Bariffi,Anne Lüscher,Rawad Bitar,Robert Grass,Antonia Wachter-Zeh,Zohar Yakhini*

Main category: cs.CR

TL;DR: 提出化学函数框架，将化学系统建模为噪声挑战-响应原语，形式化定义鲁棒性、不可克隆性和不可预测性，并应用于DNA基认证机制


<details>
  <summary>Details</summary>
Motivation: 为化学系统提供一个统一的框架来建模认证机制，解决化学系统中安全特性的形式化定义和分析问题

Method: 基于物理函数理论，在有限和渐进两种状态下定义化学函数的鲁棒性、不可克隆性和不可预测性，建立安全博弈模型，并用现有DNA结构实例化

Result: 开发了在测序噪声和部分编辑模型下的最大似然验证规则，基于二项分布提供高精度参数选择指导，建立了可复现的化学不可克隆认证设计方法

Conclusion: 该框架为设计化学不可克隆认证机制提供了系统方法，并展示了在产品认证和共享密钥生成中的应用潜力

Abstract: In this paper, we introduce chemical functions, a unified framework that models chemical systems as noisy challenge--response primitives, and formalize the associated chemical function infrastructure. Building on the theory of physical functions, we rigorously define robustness, unclonability, and unpredictability for chemical functions in both finite and asymptotic regimes, and specify security games that capture the adversary's power and the security goals. We instantiate the framework with two existing DNA-based constructions (operable random DNA and Genomic Sequence Encryption) and derive quantitative bounds for robustness, unclonability, and unpredictability. Our analysis develops maximum-likelihood verification rules under sequencing noise and partial-edit models, and provides high-precision estimates based on binomial distributions to guide parameter selection. The framework, definitions, and analyses yield a reproducible methodology for designing chemically unclonable authentication mechanisms. We demonstrate applications to in-product authentication and to shared key generation using standard extraction techniques.

</details>


### [47] [SecureSplit: Mitigating Backdoor Attacks in Split Learning](https://arxiv.org/abs/2601.14054)
*Zhihao Dou,Dongfei Cui,Weida Wang,Anjun Gao,Yueyang Quan,Mengyao Ma,Viet Vo,Guangdong Bai,Zhuqing Liu,Minghong Fang*

Main category: cs.CR

TL;DR: SecureSplit是一种针对分割学习中后门攻击的防御机制，通过维度变换增强良性嵌入和中毒嵌入的差异，并使用自适应过滤方法去除污染嵌入。


<details>
  <summary>Details</summary>
Motivation: 分割学习虽然能保护数据隐私，但容易受到后门攻击，恶意客户端可以通过修改嵌入来植入隐藏触发器，从而破坏最终训练模型。

Method: SecureSplit采用维度变换策略来突出良性嵌入和中毒嵌入之间的细微差异，然后开发自适应过滤方法，使用基于多数的投票方案去除污染嵌入同时保留干净的嵌入。

Result: 在四个数据集（CIFAR-10、MNIST、CINIC-10和ImageNette）、五种后门攻击场景和七种替代防御方案的严格实验中，SecureSplit在各种挑战性条件下都表现出有效性。

Conclusion: SecureSplit是针对分割学习中后门攻击的有效防御机制，能够保护协作模型训练免受恶意客户端攻击。

Abstract: Split Learning (SL) offers a framework for collaborative model training that respects data privacy by allowing participants to share the same dataset while maintaining distinct feature sets. However, SL is susceptible to backdoor attacks, in which malicious clients subtly alter their embeddings to insert hidden triggers that compromise the final trained model. To address this vulnerability, we introduce SecureSplit, a defense mechanism tailored to SL. SecureSplit applies a dimensionality transformation strategy to accentuate subtle differences between benign and poisoned embeddings, facilitating their separation. With this enhanced distinction, we develop an adaptive filtering approach that uses a majority-based voting scheme to remove contaminated embeddings while preserving clean ones. Rigorous experiments across four datasets (CIFAR-10, MNIST, CINIC-10, and ImageNette), five backdoor attack scenarios, and seven alternative defenses confirm the effectiveness of SecureSplit under various challenging conditions.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [48] [The Llama 4 Herd: Architecture, Training, Evaluation, and Deployment Notes](https://arxiv.org/abs/2601.11659)
*Aaron Adcock,Aayushi Srivastava,Abhimanyu Dubey,Abhinav Jauhri,Abhinav Pande,Abhinav Pandey,Abhinav Sharma,Abhishek Kadian,Abhishek Kumawat,Adam Kelsey,Adam Stelle,Adeel Cheema,Adela Kabiljo,Adina Katz,Adithya Gangidi,Aditya Tayade,Adolfo Victoria,Adrian Samatan Alastuey,Adrien Conrath,Afroz Mohiuddin,Ahmed Sharif,Ahnaf Siddiqui,Ahuva Goldstand,Aijung Li,Aidan Boyd,Aidin Kazemi Daliri,Aisha Iqbal,Ajay Menon,Ajit Mathews,Akhil Mathur,Akshat Agarwal,Alan Schelten,Alana Shine,Alejandro Castillejo Muñoz,Aleksei Guliaev,Alex Radovic,Alex Song,Alex Vaughan,Alexander Simeonov,Alexandre Rezende,Alexandre Rezende,Alexei Baevski,Alexey Roubaud,Allen Ma,Alvin Lee,Alyssa Pereira,Aman Ahmed,Aman Shankar,Amanda Kallet,Amar Budhiraja,Ameya Khandekar,Amine Benhalloum,Amir Gershman,Amit Nagpal,Amit Zohar,Amr Sharaf,Anant Desai,Anastasia Razdaibiedina,Anca Agape,Andranik Kurghinyan,Andre Perunicic,Andrea Madotto,Andrei Darabanov,Andrés Alvarado,Andrew Brown,Andrew Cohen,Andrew Fang,Andrew Freeman,Andrew Gallagher,Andrew Gu,Andrew Prasetyo Jo,Andrew Ryan,Andrew Steffen,Andrew Wei,Andrey Rusakov,Andrii Golovei,Andy Shang,Angela Fan,Angela Fan,Angela Flewellen,Animesh Pathak,Anirudh Goyal,Ankit Ramchandani,Ankur Pai,Ankur Singh,Ankush Garg,Anlu Xing,Anna Cai,Anna Grosul,Anna Prochowska,Anna Sun,Annie Dong,Annie Franco,Anqi Hu,Anshul Chawla,Anthony Hartshorn,Antonia Sheng,Antony Thomas,Anuj Goyal,Anusha De,Anvit Bodiwala,Anvit Bodiwala,Aobo Yang,Aparajita Saraf,Apurva Samudra,Aran Mun,Arash Rahnama,Archi Mitra,Archie Sravankumar,Archit Gupta,Aria Haghighi,Ariel Stolerman,Arkabandhu Chowdhury,Arnab Choudhury,Artem Korenev,Arthur Guo,Arthur Hinsvark,Arun Mallya,Arvind Neelakantan,Arya Talebzadeh,Ashish Shah,Ashmitha Jeevaraj Shetty,Ashwin Bharambe,Asif Islam,Aston Zhang,Austen Gregerson,Avi Lewis,Aya Ibrahim,Ayaz Minhas,Ayelet Dahan,Ayelet Regev Dabah,Bangsheng Tang,Bar Ulman,Bardiya Sadeghi,Bartosz Jedrzejewski,Barys Skarabahaty,Beibei Zhu,Beibin Li,Ben Bharier,Benjamin Leonhardi,Benjamin Muller,Bennett Plessala,Bernie Huang,Beth Loyd,Bhargavi Paranjape,Bhavik Sheth,Bill Bonner,Bill Holland,Bill Wang,Bingzhe Liu,Binh Tang,Bo Liu,Bo Wu,Boduo Li,Bokai Yu,Bor-Chun Chen,Boris Araya,Boris Vidolov,Botao Chen,Boya Peng,Boyu Ni,Bradley Davis,Bram Wasti,Brandon Adams,Brandon Taylor,Brandon Wu,Brant Swidler,Brian Chiang,Brian Clerkin,Brian Fuller,Brooks Cutter,Bruno Novais,Bryan Gmyrek,Bysshe Easton,Cait Campos,Canaan Case,Carl Chengyan Fu,Carly Burton,Caro Diaz,Catherine Cole,Ce Liu,Cedric Fougerat,Cen Peng,Cen Peng,Cen Zhao,Changhan Wang,Changkyu Kim,Chantal Shaib,Chao Zhou,Charlotte Caucheteux,Chau Nguyen,Chawin Sitawarin,Chaya Nayak,Chelsea Asher,Chen Fan,Chen Zhu,Cheng Cheng,Cheng Zhang,Chenguang Zhu,Chengxiong Ruan,Chengzhu Yu,Chenheli Hua,Chenxi Whitehouse,Cheryl Holloway,Ching-Hsiang Chu,Ching-Yao Chuang,Chinmay Karande,Chirag Nagpal,Chloé Bakalar,Chloe Bi,Chris Cai,Chris Marra,Chris McConnell,Chris Thi,Chris Tindal,Chris Waterson,Christian Deverall,Christian Fuegen,Christian Keller,Christine Cheng,Christine Jou,Christine Smith,Christine Wang,Christoph Feichtenhofer,Christophe Touret,Christopher Luc,Christy Sauper,Chuanhao Zhuge,Chun-Yi Sung,Chunqiang Tang,Chunyang Wu,Clara Siegel,Cody Heale,Cody Wilbourn,Colin White,Congying Xia,Corinne Wong,Cornel Rat,Cristian Canton Ferrer,Cyrille Habis,Cyrus Nikolaidis,D Lohachov,Da Ju,Dalton Flanagan,Damien Allonsius,Damon Civin,Dan Johnson,Daniel Bolya,Daniel Francisco,Daniel Fried,Daniel Hawthorne,Daniel Haziza,Daniel Ho,Daniel Kreymer,Daniel Li,Daniel Machlab,Daniel McKinnon,Daniel Obenshain,Daniel Rodriguez,Daniel Song,Daniel Tse,Danielle Pintz,Danny Livshits,Daryl James Rodrigo,Dat Huynh,Daulet Askarov,David Brandfonbrener,David Esiobu,David Kant,David Levin,David Renardy,David Soofian,David Stevens,David Xu,David Zhang,Deep Shah,Delia David,Demi Douglas,Denis Boyda,Desh Raj,Devamanyu Hazarika,Dheeraj Mekala,Dhruv Choudhary,Dhruv Mahajan,Di Jin,Didac Suris Coll-Vinent,Didem Foss,Diego Garcia-Olano,Diego Perino,Dieuwke Hupkes,DiJia Su,Dilip Madathil,Dinesh Govindasamy,Dinesh Yeduguru,Dmitry Vengertsev,Dong He,Dong Li,Dong Wang,Dongzhuo Li,Duc Le,Dunant Hin,Dustin Holland,Duy Nguyen,Duy Nguyen,Ed Dowling,Eden Litt,Egor Lakomkin,Ehab AlBadawy,Ehsan K. Ardestani,Elad Eckstein,Elahe Dabir,Elaine Montgomery,Elina Lobanova,Elior Abramoviz,Eliot Hedeman,Elissa Li,Elizabeth Hilbert,Ellen Xiaoqing Tan,Elliot Yun,Elodie Stener,Emilian Stoimenov,Emilien Garreau,Emily Dinan,Emily Hahn,Emily Wood,Emma Li,Emmanuel Ademuwagun,Emrah Seker,Eric Alamillo,Eric Gan,Eric Han,Eric Huang,Eric Michael Smith,Eric-Tuan Le,Ernie Chang,Eryk Helenowski,Eslam Elnikety,Esteban Arcaute,Ethan Myers,Eugene Nho,Eugene Poliukhovych,Evan Dunbar,Evgeniy Litvinenko,Evrim Altıntaş,Eyal Hochman,Eyal Shtrauch,Fabian Mastenbroek,Faiza Zeb,Faizan Ahmad,Farhad Farahbakhshian,Fei Kou,Fei Sun,Feiyu Chen,Felix Chung,Feng Tian,Feng Xu,Filip Radenovic,Filippos Kokkinos,Francesco Barbieri,Francesco Caggioni,Francisco Esparza,Francisco Guzmán,Frank Kanayet,Frank Seide,Frank Zhang,Fred Lewis,Freda Huang,Fulton Wang,Gabriel Synnaeve,Gabriela Jacques-Silva,Gabriella Schwarz,Gaganjit Ghardhora,Gal Elfer,Garrett Dickson,Gaurav Chaurasia,Gautam Sewani,Geet Shingi,Gefei Zuo,Geonhwa Jeong,George Puthanpurackal,Georgia Swee,Gerard Moreno-Torres Bertran,Gil Keren,Gina Ling,Gjergji Stasa,Gobinda Saha,Gor Safran,Gordy French,Goutham Rajendran,Govind Thattai,Grace Cineas,Graeme Nail,Greg Fletcher,Grégoire Mialon,Griffin Adams,Grigory Sizov,Guan Pang,Hady Elsahar,Hai Dang Tran,Hailey Nguyen,Haiping Wu,Hakan Inan,Hamid Eghbalzadeh,Han Fang,Han Zou,Hannah Doyle,Hannah Korevaar,Hannah Wang,Hannah Werbel,Hanwen Zha,Hany Morsy,Hao Ma,Haoci Zhang,Haonan Sun,Haozhu Wang,Hardik Shah,Haroun Habeeb,Harrison Rudolph,Harsh Gupta,Harsh Poddar,Harshil Parikh,Hejia Zhang,Heming Wang,Hengduo Li,Himanshu Sharma,Hoang Phi Nguyen,Hongbo Zhang,Honghao Qiu,Hongjiang Lv,Hongli Xu,Hongyuan Zhan,Hossein Hamooni,Howard Huang,Hu Xu,Hugo Laurençon,Hugo Touvron,Hung Dinh,Hunter Goldman,Hussein Mehanna,Huy Nguyen,Hweimi Tsuo,Ian Graves,Ian Yu,Ibrahim Damlaj,Idan Cohen,Igor Tufanov,Ilan Goldenstein,Ilias Leontiadis,Iliyan Zarov,Imad Ahmed,Innocent Djiofack,Iosif Spulber,Irina-Elena Veliche,Isabella Ramos,Ishan Misra,Itai Gal,Ivan Evtimov,Ivan Evtimov,Ivan Obraztsov,Jack Wu,Jacqueline Romero Vertino,Jaemo Koo,Jaewon Lee,Jake Jung,Jake Weissman,James Beldock,James Crnkovich,James Grinage,James Hongyi Zeng,James Kohli,James Tian,Jamie Cahill,Jan Geffert,Jan Seidel,Jan Seidel,Janey Tracey,Jang Hyun Cho,Janice Wei,Jarrod Kahn,Jasmyn Howell,Jason Long Vu,Jason Park,Jason Yan,Jason Yip,Jay Li,Jay Mahadeokar,Jaya Bharath R Goluguri,Jayasi Mehar,Jean-Baptiste Gaya,Jeet Shah,Jeff Hanson,Jeff Marcus,Jeff Walsh,Jeff Yang,Jelmer van der Linde,Jemma Fan,Jennifer Chan,Jenny Zhen,Jenya Lee,Jeremy Fu,Jeremy Reizenstein,Jeremy Teboul,Jesse He,Jessica Zhong,Ji Hou,Ji Yang,Jia Ding,Jiabo Hu,Jiacheng Zhu,Jiadong Guo,Jialiang Wang,Jialin Ouyang,Jianfeng Chi,Jianyu Huang,Jianyun Zhao,Jiaowen Yang,Jiatong Zhou,Jiawei Zhao,Jiawen Liu,Jie Wang,Jie You,Jiecao Yu,Jillian Schwiep,Jilong Wu,Jing Huang,Jing Li,Jing Yu Koh,Jing Zhang,Jingxiang Chen,Jingyi Yang,Jingyue Shen,Jinho Hwang,Jinxi Guo,Jiwan Khatiwada,Joanna Bitton,Joe Li,Joe Quanaim,Joel Beales,Johan Schuijt,John Chang,John Quan,Johnnie Chan,Jon Shepard,Jona Harris,Jonah Rubin,Jonathan Janzen,Jonathan Kaldor,Jorge Lopez Silva,Jose Leitao,Joseph Greer,Joseph Moon,Joseph Rocca,Joseph Tighe,Josh Fromm,Joshua Deng,Joshua Fernandes,Joshua Saxe,Joyce Zheng,Juan Pino,Julien Prigent,Jun Chen,Junjiao Tian,Junjie Qi,Junjie Wang,Junteng Jia,Kade Baker,Kai Londenberg,Kai Wang,Kainan Peng,Kaiyan Peng,Kaiyue Yang,Kalyan Vasudev Alwala,Kam Hou Yu,Kanika Narang,Karan Chadha,Karan Sikka,Karen Zhang,Karina Schuberts,Karishma Mandyam,Karthik Abinav Sankararaman,Karthik Padthe,Karthik Prasad,Karthik Sivakumar,Kartikeya Upasani,Kate Plawiak,Kate Saenko,Kateřina Žmolíková,Kathryn Stadler,Kathy Matosich,Katie Doulgass,Kaveh Hassani,Kay Ji,Ke Li,Kenneth Heafield,Kenny Yu,Keqian Li,Kevin Chih-Yao Ma,Kevin Hannan,Keyu Man,Kezhen Chen,Khalid El-Arini,Khrystyna Hutsulyak,Kieran Nash,Kiran Jagadeesh,Kody Bartelt,Konstantin Topaloglou-Mundy,Konstantinos Chatziioannou,Konstantinos Karanasos,Konstantinos Vougioukas,Kostas Tsiampouris,Kristen Hamill,Kristy Choi,Krithika Iyer,Kshitiz Malik,Kuenley Chiu,Kun Huang,Kunal Bhalla,Kunal Chawla,Kunpeng Li,Kushal Lakhotia,Kyle Monk,Lakshya Garg,Lalit Chourey,Lars Hamre,Laura Gustafson,Lauren Deason,Laurence Rouesnel,Laurens van der Maaten,Lavender A,Lawrence Chen,Lawrence Jang,Leandro Silva,Leda Sari,Lee Hetherington,Lei Zhang,Leiyu Zhao,Lele Chen,Leo Chenghui Li,Leon Yang,Leon Zhan,Levi Corallo,Liang Tan,Licheng Yu,Lijuan Liu,Lilach Mor,Lincoln Lin,Linfeng Li,Lisa Titus,Liz Jenkins,Lovish Madaan,Lu Fang,Lu Yuan,Lucas Nava,Lucas Pasqualin,Lucas Switzer,Lucia Fang,Lucy Sun,Luka Tadic,Lukas Blecher,Lukas Landzaat,Luxin Zhang,Madhavi Rao,Madian Khabsa,Mahalia Miller,Mahendra Kariya,Mahesh Pasupuleti,Mahi Luthra,Manaal Faruqui,Manav Avlani,Manchen Wang,Mannat Singh,Manohar Paluri,Manoj Chakkaravarthy,Manoj Nair,Maquelle Tiffany,Marcin Pawlowski,Marcus Wu,Maria Lomeli,Mario Consuegra,Marion Boiteux,Marios Andreas Galanis,Marshall Chen,Martin Gleize,Maryam Fazel-Zarandi,Matan Hasson,Mathew Oldham,Mathieu Rita,Matt Dordal,Matt Setzler,Matt Staats,Matt Staats,Matt Wilde,Matthew Clark,Matthew Grange,Matthew Lennie,Matthew Schmohl,Max Raphael,Maxim Naumov,Maxim Samoylov,Maxime Lecanu,Maya Pavlova,Md Taha Bin Jawaid,Meghan Keneally,Melanie Kambadur,Meng Zhang,Mengchen Liu,Mengdi Lin,Mengjiao Wang,Mervyn Abraham,Miao Liu,Michael Au-Yeung,Michael Feldergraf,Michael Man,Michael Matheny,Michael Suo,Michael Tontchev,Michel Meyer,Michelle Ma,Mihir Patel,Mihir Sanjay Kale,Mik Vyatskov,Mikayla Alexander,Mike Andersland,Mike Clark,Mike Lewis,Mike Li,Mike Macey,Mike Macey,Mike Seltzer,Mikel Jimenez Fernandez,Mikhail Antonov,Mikhail Plekhanov,Milan Zhou,Min Si,Ming Qiao,Mingbo Ma,Mingjun Zhang,Mingyi Liang,Miquel Jubert Hermoso,Mirac Suzgun,Mirjam Skarica,Mitesh Kumar Singh,Mohammad Kabbani,Mohammad Rastegari,Mona Sarantakos,Monica Sim,Monika Gangapuram,Mor Moshe,Morrie Doulaty,Morvarid Metanat,Moya Chen,Mrinal Kumar,Munish Bansal,Murali Ramarao,Na Li,Nadav Azaria,Nahiyan Malik,Naman Goyal,Nancy Vargas Balderas,Nanshu Wang,Naoyuki Kanda,Natalia Gimelshein,Natalia Neverova,Nathan Aclander,Natt Sithiviraporn,Navneet Madhu Kumar,Ned Newton,Neeraj Bahl,Negar Ghorbani,Neil Patel,Neta-lee Golan,Nicholas Longenbaugh,Nick Egebo,Nikhil Johri,Nikhil Mehta,Nikhil Naik,Niko Moritz,Nikolay Bashlykov,Nikolay Bogoychev,Nikolay Pavlovich Laptev,Niladri Chatterji,Nile Jones,Nimish Shah,Ning Dong,Ning Li,Ning Li,Ning Zhang,Nishant Yadav,Noam Paz,Norman Cheng,Norman Cheng,Olaoluwa Adesanya,Oleg Repin,Oleksandr Maksymets,Omkar Salpekar,Omri Harosh,Onkar Pednekar,Onur Çelebi,Oran Gafni,Oren Edinger,Osama Hanna,Owais Khan Mohammed,Ozlem Kalinli,Paden Tomasello,Pankaj Singh,Paola Quevedo,Parag Jain,Paria Rashidinejad,Parker Tooley,Parth Parekh,Parth Thakkar,Parvin Taheri,Pasan Hapuarachchi,Pascal Kesseli,Patrick Alrassy,Paulo de Rezende Pinatti,Pavan Balaji,Pawan Sisodiya,Pedro Jose Ferreira Moreira,Pedro Rittner,Pedro Valenzuela,Peize Sun,Peizhao Zhang,Peng-Jen Chen,Pengchao Wang,Pengchuan Zhang,Pengwei Li,Petar Vasic,Peter Carras,Peter Ney,Peter Weng,Petru Dumea,Phil Hayes,Philip Woods,Pierre Andrews,Pierre Ménard,Ping-Hao Wu,Pingchuan Liu,Piotr Dollar,Plamen Dzhelepov,Polina Zvyagina,Posten A,Prabhav Agrawal,Pradhapan Rajendran,Pradyot Prakash,Prajjwal Bhargava,Pramono,Pranay Shah,Pranshu Dave,Prash Jain,Pratik Dubal,Praveen Gollakota,Praveen Krishnan,Pritish Yuvraj,Projjal Ghosh,Punit Singh Koura,Puxin Xu,Qi Qi,Qi Zhou,Qian Guan,Qian Sun,Qiang Liu,Qing He,Qinqing Zheng,Qirui Yang,Qizhen Guo,Quanzeng You,Quentin Carbonneaux,Quentin Carbonneaux,Quentin Duval,Quintin Fettes,Rachad Alao,Rachel Batish,Rachel Guo,Rachel Rodriguez,Radhika Bhargava,Rafael Asuncion,Raghotham Murthy,Rahul Dutta,Rahul Jha,Rahul Kindi,Rahul Mitra,Raj Ganapathy,Raj Shah,Rajarshi Das,Rajat Shrivastava,Rajesh Nishtala,Ramakant Shankar,Raman Shukhau,Ramon Calderer,Rangaprabhu Parthasarathy,Ranjan Subramanian,Raphael Bensadoun,Rares Bostan,Rashnil Chaturvedi,Ravi Agrawal,Ray Gao,Raymond Li,Rebecca Kogen,Ricardo Juan Palma Duran,Ricardo Silveira Cabral,Richard Lee,Richard Yuanzhe Pang,Riddhish Bhalodia,Riham Mansour,Rishabh Singh,Rishi Godugu,Ritun Patney,Rob Boyle,Robbie Goldfarb,Robert Caldwell,Robert Kuo,Roberta Raileanu,Robin Battey,Robin Sharma,Rochit Sapra,Rocky Wang,Rodolfo Granata,Rodrigo De Castro,Rodrigo Paim,Rohan Maheshwari,Rohan Varma,Rohit Girdhar,Rohit Patel,Roshan Sumbaly,Roy Sheaffer,Ruan Silva,Ruben Rodriguez Buchillon,Rui Hou,Ruiming Xie,Ruslan Mavlyutov,Ruslan Semenov,Rustam Dinov,Ruxiao Bao,Ryan Fox,Ryan Kilpatrick,Ryan Kwan,Ryan Lim,Ryan Smith,Saaketh Narayan,Sabrina Qiao,Sachin Mehta,Sachin Siby,Sagar Jain,Saghar Hosseini,Sagie Gur-Ari,Sahana Chennabasappa,Sahin Geyik,Sai Jayesh Bondu,Sai Mounika Chowdhary Nekkalapudi,Saif Hasan,Saisuke Okabayashi,Saketh Rambhatla,Salil Sawhney,Sam Dunster,Sam Zhao,Saman Keon,Samaneh Azadi,Sameet Sapra,Samuel Dooley,Samyak Datta,Sandeep Parab,Sang Michael Xie,Sanjay Singh,Sanyuan Chen,Sara Behn,Sara Khodeir,Sarah Shirazyan,Sargun Dhillon,Sarunya Pumma,Sasha Sidorov,Saskia Adaime,Saurabh Khanna,Sayem Wani,Scott Brenton,Sean Bell,Sean Kelly,Sean Koger,Sean Nunley,Sean Perry,Sebastian Caicedo,Sebastian Dahlgren,Sebastian Ruder,Seiji Yamamoto,Selam Mehretu,Selvan Sunitha Ravi,Sen Lyu,Senthil Chellapan,Serafeim Mellos,Sergey Edunov,Sergey Royt,Shaina Cohen,Shangfu Peng,Shannon Adams,Shaoliang Nie,Sharadh Ramaswamy,Sharan Narang,Shashank Pisupati,Shashi Gandham,Shaun Lim,Shaun Lindsay,Sheena Artrip,Shelly Sheynin,Shen Yan,Sheng Feng,Sheng Shen,Shengbao Zheng,Shenghao Lin,Shengjie Bi,Shengxin Cindy Zha,Shengye Wan,Shengyi Qian,Shengyong Cai,Shengzhi Shao,Shervin Shahidi,Shikai Li,Shimon Bernholtz,Shiqi Wang,Shishir G. Patil,Shiv Verma,Shiva Shankar P,Shiyang Chen,Sho Yaida,Shoubhik Debnath,Shreyas Siravara,Shruti Bhosale,Shuang Ma,Shun Zhang,Shuo Tang,Shuqiang Zhang,Shuyan Zhou,Sicong Che,Sidd Srinivisan,Siddharth Bhattacharya,Siddharth Patki,Sijia Chen,Sili Chen,Simon Vandenhende,Simone Merello,Sinong Wang,Sivan Barzily,Sixian Yi,Siyu Lin,SK Bong,Sky Yin,Sneha Agarwal,Sneha Agarwal,Soerian Lieve,Soji Sajuyigbe,Song Jiang,Songlin Li,Sonia Kim,Sopan Khosla,Soumi Maiti,Spencer Whitman,Sravya Popuri,Sreen Tallam,Srinivas Vaidyanathan,Srinivas Vaidyanathan,Sten Sootla,Stephane Collot,Stephanie Ding,Stephen Chen,Steven Cai,Suchin Gururangan,Sudarshan Govindaprasad,Sue Young,Suganthi Dewakar,Sujan Kumar Gonugondla,Sujeet Bhandari,Suman Gumudavelli,Suman Gumudavelli,Sumit Gupta,Summer Deng,Sungmin Cho,Suresh Ganapathy,Surjyendu Dhal,Susan Fedynak,Susana Contrera,Suyoun Kim,Sylvestre Rebuffi,Takshak Chahande,Tamar Herman,Tan Li,Tao Xu,Tara Fowler,Tarek Sheasha,Tarun Anand,Tarun Kalluri,Tarun Singh,Tatiana Shavrina,Ted Li,Teja Rao,Tejas Patil,Teng Li,Thach Bui,Thai Quach,Thamer Alharbash,Thanh Vinh Vo,Thawan Kooburat,Thilo Koehler,Thomas Georgiou,Thomas Scialom,Tian Ye,Tianhe Li,Tianjun Zhang,Tianyu Li,Tijmen Blankevoort,Timon Willi,Timothy Chou,Timothy Leung,TJ Lee,Todor Mihaylov,Tom Heatwole,Tong Xiao,Tony Cao,Tony Lee,Trang Le,Tristan Rice,Tsz Kei Serena Chan,Tuan Tran,Tudor Tiplea,Tyler Baumgartner,Uday Savagaonkar,Ujjwal Karn,Ulises Martinez Araiza,Umar Farooq,Uriel Cohen,Usman Sharif,Utkarsh Murarka,Van Phung,Varun Joginpalli,Varun Saravagi,Vasu Sharma,Vasudha Viswamurthy,Vedanuj Goswami,Vedika Seth,Venkat Ramesh,Venkat Ramesh,Vibhor Gupta,Victoria Montanez,Vidhya Natarajan,Vidya Sarma,Vignesh Ramanathan,Viktor Kerkez,Vinay Rao,Vincent Gonguet,Vincent Mauge,Virginie Do,Vish Vogeti,Vishrav Chaudhary,Viswesh Sankaran,Vítor Albiero,Vivek Miglani,Vivek Pai,Vlad Cojanu,Vlad Shubin,Vlad Tiberiu Mihailescu,Vladan Petrovic,Vladimir Ivanov,Vladislav Vorotilov,Vrushali Bhutada,Wai I Ng,Wei Cheng,Wei Sun,Wei Tu,Wei Wei,Wei Zhou,Wei-Ning Hsu,Weiwei Chu,Weizhe Yuan,Wenchen Wang,Wenjun Zhao,Wenwen Jiang,Wenyin Fu,Wenzhe Jiang,Whitney Meers,Will Constable,Will Wang,William R. Wong,Xavier Martinet,Xi Victoria Lin,Xi Yan,Xi Yin,Xian Li,Xianfeng Rui,Xianjun Yang,Xiaocheng Tang,Xiaodong Wang,Xiaofang Wang,Xiaolan Wang,Xiaoliang Dai,Xiaoliang Peng,Xiaopeng Li,Xiaozhu Meng,Xibei Zhang,Xide Xia,Xin Jin,xinbo Gao,Xinfeng Xie,Xingyi Zhou,Xu Ma,Xuan Ju,Xuanyi Zhao,Xubo Liu,Xuchao Jia,Xuedong Zhang,Xuefei Cao,Xuewei Wang,Xuewei Wu,Xunnan Xu,Xutai Ma,Xuyang Wang,Yan Cui,Yang Chen,Yang Li,Yang Shu,Yang Xia,Yanjun Chen,Yanjun Zhou,Yash Mehta,Yash Patel,Yash Tekena,Yashesh Gaur,Yasmine Babaei,Yaxuan Zhou,Ye Hu,Ye Qi,Yejin Lee,Yeming Wen,Yen-Cheng Liu,Yexin Bruce Wu,Yi Pan,Yi Yang,Yi-Hui Lin,Yifan Wang,Yifan Wu,Yifan Yang,Yifei Huang,Yiftah Ben Aharon,Yilin Yang,Yiling You,Ying Xu,Ying Zhang,Yingquan Yuan,Yingru Liu,Yingyi Ma,Yining Yang,Yiting Lu,Yonatan Komornik,Yongjie Lin,Yoni Goyhman,Yossi Moran Mamo,Youngjin Nam,Yu Wang,Yu Lu,Yu Zhao,Yu-Ho Hsieh,Yu-Jung Lo,Yuandong Tian,Yuanhan Zhang,Yuanhao Xiong,Yuanshun Yao,Yuchen Hao,Yuchen Zhang,Yuchuan Li,Yue Cao,Yue Yu,Yue Zhao,Yuhan Guo,Yuhao Wang,Yuheng Huang,Yujie Lu,Yujun Shi,Yulun Wang,Yun He,Yun Wang,Yundi Qian,Yunfan Wang,Yunhao Tang,Yuning Mao,Yunlu Li,Yuqi Dai,Yuriy Hulovatyy,Yushi Hu,Yuxuan Sun,Zach Rait,Zach Wentz,Zacharie Delpierre Coudert,Zachary Collins,Zahra Hankir,Zecheng He,Zeeshan Ahmed,Zeeshan Ahmed,Zef RosnBrick,Zhan Shu,Zhanna Rohalska,Zhaoduo Wen,Zhe Liu,Zhe Liu,Zhen Qiao,Zhenggang Xu,Zhengwen Zhou,Zhengxing Chen,Zhenyu Tang,Zhichen Wu,Zhicheng Ouyang,Zhihong Lei,Zhipeng Hong,Zhiping Xiu,Zhiwei Zhao,Zhong Meng,Zhou Jin,Zhouhao Zeng,Zichang Liu,Zihang Meng,Zihuan Qiao,Zinnia Zheng,Zixi Qi,Ziyi Luo,Zoe Foulkes Birkhead,Zoey Sun,Zohar Achdut*

Main category: cs.SE

TL;DR: 本文档整合了Meta Llama 4模型系列的公开技术细节，包括模型变体、架构特性、训练方法、基准测试结果、部署约束和许可信息，为研究人员和从业者提供技术参考。


<details>
  <summary>Details</summary>
Motivation: Meta Llama 4模型系列作为重要的开源大语言模型，其技术细节分散在各种公开材料中。本文档旨在系统整理这些信息，为研究者和实践者提供准确、基于来源的技术参考，帮助他们理解模型特性、部署约束和许可要求。

Method: 文档采用文献综述方法，从公开的技术报告、发布材料、开发者文档等来源收集信息，系统整理Llama 4的技术细节。内容包括：模型变体（Scout和Maverick）及Behemoth教师模型；架构特性（MoE结构、路由/共享专家、早期融合多模态、长上下文设计）；训练方法（预训练、中期训练、后训练）；基准测试结果；部署约束；许可和安全评估。

Result: 文档提供了Llama 4模型系列的全面技术概览：1）两个主要变体Scout和Maverick，以及Behemoth教师模型；2）详细的MoE架构描述，包括路由机制和共享专家；3）多模态早期融合和长上下文支持（iRoPE和长度泛化策略）；4）三阶段训练流程；5）基准测试显示模型在多个任务上表现良好；6）实际部署中的上下文限制和量化约束；7）明确的许可要求和安全评估实践。

Conclusion: 本文档成功整合了Meta Llama 4模型系列的公开技术信息，为研究者和从业者提供了有价值的参考资源。文档涵盖了从模型架构到实际部署的完整技术链条，强调了模型的技术创新（如MoE架构、长上下文处理）和实际应用约束，有助于社区更好地理解和应用这一重要的开源模型系列。

Abstract: This document consolidates publicly reported technical details about Metas Llama 4 model family. It summarizes (i) released variants (Scout and Maverick) and the broader herd context including the previewed Behemoth teacher model, (ii) architectural characteristics beyond a high-level MoE description covering routed/shared-expert structure, early-fusion multimodality, and long-context design elements reported for Scout (iRoPE and length generalization strategies), (iii) training disclosures spanning pre-training, mid-training for long-context extension, and post-training methodology (lightweight SFT, online RL, and lightweight DPO) as described in release materials, (iv) developer-reported benchmark results for both base and instruction-tuned checkpoints, and (v) practical deployment constraints observed across major serving environments, including provider-specific context limits and quantization packaging. The manuscript also summarizes licensing obligations relevant to redistribution and derivative naming, and reviews publicly described safeguards and evaluation practices. The goal is to provide a compact technical reference for researchers and practitioners who need precise, source-backed facts about Llama 4.

</details>


### [49] [Semantic Caching and Intent-Driven Context Optimization for Multi-Agent Natural Language to Code Systems](https://arxiv.org/abs/2601.11687)
*Harmohit Singh*

Main category: cs.SE

TL;DR: 提出一个生产优化的多智能体系统，将自然语言查询转换为可执行的Python代码进行结构化数据分析，通过语义缓存、双阈值决策和意图驱动的动态提示组装实现高准确率和成本效益。


<details>
  <summary>Details</summary>
Motivation: 传统依赖昂贵前沿模型的系统成本高，需要开发一个既能保持高准确性又能实现成本效益的生产级系统，用于企业结构化数据分析。

Method: 采用三个关键技术：1）基于LLM的语义缓存系统，具有等价检测和结构化适配提示；2）分离精确匹配检索和参考引导生成的双阈值决策机制；3）通过表感知上下文过滤减少40-60%令牌消耗的意图驱动动态提示组装系统。

Result: 系统已在企业库存管理生产环境中部署，处理超过10,000个查询，平均延迟8.2秒，语义准确率94.3%，缓存命中率达到67%。

Conclusion: 该系统展示了在生产环境中部署基于LLM的分析系统的可行性，通过创新的缓存、决策和提示优化技术实现了高准确率和成本效益的平衡。

Abstract: We present a production-optimized multi-agent system designed to translate natural language queries into executable Python code for structured data analytics. Unlike systems that rely on expensive frontier models, our approach achieves high accuracy and cost efficiency through three key innovations: (1) a semantic caching system with LLM-based equivalence detection and structured adaptation hints that provides cache hit rates of 67% on production queries; (2) a dual-threshold decision mechanism that separates exact-match retrieval from reference-guided generation; and (3) an intent-driven dynamic prompt assembly system that reduces token consumption by 40-60% through table-aware context filtering. The system has been deployed in production for enterprise inventory management, processing over 10,000 queries with an average latency of 8.2 seconds and 94.3% semantic accuracy. We describe the architecture, present empirical results from production deployment, and discuss practical considerations for deploying LLM-based analytics systems at scale.

</details>


### [50] [SpecMap: Hierarchical LLM Agent for Datasheet-to-Code Traceability Link Recovery in Systems Engineering](https://arxiv.org/abs/2601.11688)
*Vedant Nipane,Pulkit Agrawal,Amit Singh*

Main category: cs.SE

TL;DR: 提出一种基于大语言模型的分层数据手册到代码映射方法，显著提升嵌入式系统文档与代码的追溯精度和效率


<details>
  <summary>Details</summary>
Motivation: 嵌入式系统数据手册与代码实现之间的精确追溯是系统工程中的基本挑战，现有基于词汇相似性和信息检索的方法难以捕捉嵌入式系统软件中的语义、结构和符号级关系

Method: 采用分层的数据手册到代码映射方法，利用大语言模型进行语义分析，通过仓库级结构推断、文件级相关性估计和细粒度符号级对齐三个层次逐步缩小搜索空间，覆盖函数、宏、结构体、常量、配置参数和寄存器定义等系统级C/C++代码元素

Result: 在多个开源嵌入式系统仓库上评估，相比传统信息检索基线有显著改进，文件映射准确率达到73.3%，LLM令牌消耗降低84%，端到端运行时间减少约80%

Conclusion: 该方法支持大型嵌入式软件系统的自动化分析，为系统感知机器学习模型训练数据生成、标准合规性验证和大规模规范覆盖分析等下游应用提供支持

Abstract: Establishing precise traceability between embedded systems datasheets and their corresponding code implementations remains a fundamental challenge in systems engineering, particularly for low-level software where manual mapping between specification documents and large code repositories is infeasible. Existing Traceability Link Recovery approaches primarily rely on lexical similarity and information retrieval techniques, which struggle to capture the semantic, structural, and symbol level relationships prevalent in embedded systems software. We present a hierarchical datasheet-to-code mapping methodology that employs large language models for semantic analysis while explicitly structuring the traceability process across multiple abstraction levels. Rather than performing direct specification-to-code matching, the proposed approach progressively narrows the search space through repository-level structure inference, file-level relevance estimation, and fine-grained symbollevel alignment. The method extends beyond function-centric mapping by explicitly covering macros, structs, constants, configuration parameters, and register definitions commonly found in systems-level C/C++ codebases. We evaluate the approach on multiple open-source embedded systems repositories using manually curated datasheet-to-code ground truth. Experimental results show substantial improvements over traditional information-retrieval-based baselines, achieving up to 73.3% file mapping accuracy. We significantly reduce computational overhead, lowering total LLM token consumption by 84% and end-to-end runtime by approximately 80%. This methodology supports automated analysis of large embedded software systems and enables downstream applications such as training data generation for systems-aware machine learning models, standards compliance verification, and large-scale specification coverage analysis.

</details>


### [51] [Technical Lag as Latent Technical Debt: A Rapid Review](https://arxiv.org/abs/2601.11693)
*Shane K. Panter,Nasir U. Eisty*

Main category: cs.SE

TL;DR: 本文通过快速综述方法系统梳理了技术滞后的研究现状，明确了定义、检测量化方法、成因后果及管理实践，提出将其作为被动积累技术债务的指标，为大型代码库维护提供改进方向。


<details>
  <summary>Details</summary>
Motivation: 技术滞后是软件系统未能跟上技术进步而积累的问题，会导致软件质量恶化。现有研究分散，缺乏统一定义和系统框架，需要整合现有研究以明确技术滞后的概念、检测方法和管理策略。

Method: 采用快速综述方法结合滚雪球抽样，从ACM Digital Library、IEEE Xplore、Scopus和Springer等主要数据库筛选同行评审研究，系统分析技术滞后的相关文献。

Result: 技术滞后通常被动积累且不易察觉，主要因检测指标和工具不足；它通过过时依赖、废弃API、不支持平台和老化基础设施对软件质量产生负面影响；管理策略主要包括自动化依赖更新、持续集成流程和定期审计。

Conclusion: 增强和扩展当前标准化指标、检测方法及实证研究，将技术滞后作为积累的潜在债务指标，可显著改善依赖外部包的大型代码库维护过程；识别了研究空白并为研究者和实践者规划了未来研究方向。

Abstract: Context: Technical lag accumulates when software systems fail to keep pace with technological advancements, leading to a deterioration in software quality. Objective: This paper aims to consolidate existing research on technical lag, clarify definitions, explore its detection and quantification methods, examine underlying causes and consequences, review current management practices, and lay out a vision as an indicator of passively accumulated technical debt. Method: We conducted a Rapid Review with snowballing to select the appropriate peer-reviewed studies. We leveraged the ACM Digital Library, IEEE Xplore, Scopus, and Springer as our primary source databases. Results: Technical lag accumulates passively, often unnoticed due to inadequate detection metrics and tools. It negatively impacts software quality through outdated dependencies, obsolete APIs, unsupported platforms, and aging infrastructure. Strategies to manage technical lag primarily involve automated dependency updates, continuous integration processes, and regular auditing. Conclusions: Enhancing and extending the current standardized metrics, detection methods, and empirical studies to use technical lag as an indication of accumulated latent debt can greatly improve the process of maintaining large codebases that are heavily dependent on external packages. We have identified the research gaps and outlined a future vision for researchers and practitioners to explore.

</details>


### [52] [The Stability Trap: Evaluating the Reliability of LLM-Based Instruction Adherence Auditing](https://arxiv.org/abs/2601.11783)
*Murtuza N. Shergadwala*

Main category: cs.SE

TL;DR: 研究发现LLM法官评估存在"稳定性陷阱"：虽然判决稳定性高（>99%），但推理稳定性差异显著，客观指令的推理稳定性可低至19%，主观指令在35%-83%之间波动。


<details>
  <summary>Details</summary>
Motivation: 在受监管行业（如人力资源）中，生成式AI的企业治理需要可扩展且可复现的审计机制。虽然LLM作为法官的方法提供了可扩展性，但其评估不同类型系统指令遵循性的可靠性尚未得到验证。

Method: 引入范围化指令分解框架，将应用指令分类为客观和主观类型，隔离导致法官不稳定的因素。将该框架应用于两个代表性的人力资源生成式AI应用，评估四种法官架构在可变运行中的稳定性。

Result: 揭示了"稳定性陷阱"现象：判决稳定性与推理稳定性之间存在分歧。客观指令（如字数统计）的推理稳定性低至约19%，主观指令的推理稳定性在35%-83%之间波动，而离散实体提取的客观指令推理稳定性超过90%。

Conclusion: 高判决稳定性可能掩盖脆弱的推理过程。建议审计人员严格限定自动化评估协议：将所有可确定性验证的逻辑委托给代码，同时保留LLM法官用于复杂的语义评估。

Abstract: The enterprise governance of Generative AI (GenAI) in regulated sectors, such as Human Resources (HR), demands scalable yet reproducible auditing mechanisms. While Large Language Model (LLM)-as-a-Judge approaches offer scalability, their reliability in evaluating adherence of different types of system instructions remains unverified. This study asks: To what extent does the instruction type of an Application Under Test (AUT) influence the stability of judge evaluations? To address this, we introduce the Scoped Instruction Decomposition Framework to classify AUT instructions into Objective and Subjective types, isolating the factors that drive judge instability. We applied this framework to two representative HR GenAI applications, evaluating the stability of four judge architectures over variable runs. Our results reveal a ``Stability Trap'' characterized by a divergence between Verdict Stability and Reasoning Stability. While judges achieved near-perfect verdict agreement ($>99\%$) for both objective and subjective evaluations, their accompanying justification traces diverged significantly. Objective instructions requiring quantitative analysis, such as word counting, exhibited reasoning stability as low as $\approx19\%$, driven by variances in numeric justifications. Similarly, reasoning stability for subjective instructions varied widely ($35\%$--$83\%$) based on evidence granularity, with feature-specific checks failing to reproduce consistent rationale. Conversely, objective instructions focusing on discrete entity extraction achieved high reasoning stability ($>90\%$). These findings demonstrate that high verdict stability can mask fragile reasoning. Thus, we suggest that auditors scope automated evaluation protocols strictly: delegate all deterministically verifiable logic to code, while reserving LLM judges for complex semantic evaluation.

</details>


### [53] [Changes in Coding Behavior and Performance Since the Introduction of LLMs](https://arxiv.org/abs/2601.11835)
*Yufan Zhang,Jaromir Savelka,Seth Goldstein,Michael Conway*

Main category: cs.SE

TL;DR: 该研究分析了ChatGPT发布前后五年间研究生云计算课程的学生代码提交行为，发现学生编程行为发生显著变化：最终提交代码长度增加，连续提交间的编辑距离增大但分数提升减少，表明生产力和学习效果均下降，这可能与学生对LLMs的过度依赖有关。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型（LLMs）的广泛使用，学生与编程和问题解决的互动方式发生了变化。虽然这些工具可能提高学生生产力，但也使教师评估学生学习和努力变得更加困难。研究者希望了解ChatGPT发布前后学生编程行为的变化及其对学习成果的影响。

Method: 采用准纵向研究方法，分析研究生云计算课程五年间的学生源代码提交数据。研究聚焦于一个保持不变的作业任务，比较ChatGPT发布前五个学期和发布后五个学期的学生行为。通过分析最终提交代码长度、连续提交间的编辑距离、分数改进等指标来评估变化。

Result: 自2022年秋季以来，学生编程行为发生显著变化：最终提交代码长度增加；连续提交间的平均编辑距离增大，而平均分数改进减少；这些行为变化与整体表现存在统计学显著相关性。结果表明学生生产力和学习效果在ChatGPT发布后均有所下降。

Conclusion: 虽然不能明确归因于LLMs的滥用，但研究结果与"部分学生过度依赖LLMs从而负面影响学习成果"的假设一致。研究对LLMs时代的第一代毕业生发出警示，呼吁教育者和雇主反思评估真正专业知识和生产力的方法。

Abstract: The widespread availability of large language models (LLMs) has changed how students engage with coding and problem-solving. While these tools may increase student productivity, they also make it more difficult for instructors to assess students' learning and effort. In this quasi-longitudinal study, we analyze five years of student source code submissions in a graduate-level cloud computing course, focusing on an assignment that remained unchanged and examining students' behavior during the period spanning five semesters before the release of ChatGPT and five semesters after.
  Student coding behavior has changed significantly since Fall 2022. The length of their final submissions increased. Between consecutive submissions, average edit distances increased while average score improvement decreased, suggesting that both student productivity and learning have decreased after ChatGPT's release. Additionally, there are statistically significant correlations between these behavioral changes and their overall performance. Although we cannot definitively attribute them to LLM misuse, they are consistent with our hypothesis that some students are over-reliant on LLMs, which is negatively affecting their learning outcomes. Our findings raise an alarm around the first generation of graduates in the age of LLMs, calling upon both educators and employers to reflect on their evaluation methods for genuine expertise and productivity.

</details>


### [54] [Trace Validation of Unmodified Concurrent Systems with OmniLink](https://arxiv.org/abs/2601.11836)
*Finn Hackett,Evan Wrench,Peter Macko,A. Jesse Jiryu Davis,Yuanhao Wei,Ivan Beschastnikh*

Main category: cs.SE

TL;DR: OmniLink是一种新的并发系统验证方法，使用TLA+规范验证实现，通过求解逻辑全序来检测并发错误，在工业级数据库和数据结构验证中表现出色。


<details>
  <summary>Details</summary>
Motivation: 并发系统验证困难，现有工具需要侵入式插桩或不现实的执行模型，难以检测罕见线程交错导致的微妙错误。

Method: OmniLink将系统事件视为黑盒，包含时间窗口和TLA+语义，通过求解逻辑全序来验证实现是否符合高层规范，使用现成的模型检查器进行线性化检查。

Result: 成功验证了WiredTiger工业数据库、BAT无锁数据结构和ConcurrentQueue无锁队列，发现了已知错误和两个先前未知的错误（BAT和ConcurrentQueue各一个）。

Conclusion: OmniLink提供了一种灵活、高效的并发系统验证方法，能够处理非线性化行为，在大规模验证任务中优于现有技术。

Abstract: Concurrent systems are notoriously difficult to validate: subtle bugs may only manifest under rare thread interleavings, and existing tools often require intrusive instrumentation or unrealistic execution models. We present OmniLink, a new methodology for validating concurrent implementations against high-level specifications in TLA+. Unlike prior TLA+ based approaches which use a technique called trace validation, OmniLink treats system events as black boxes with a timebox in which they occurred and a meaning in TLA+, solving for a logical total order of actions. Unlike prior approaches based on linearizability checking, which already solves for total orders of actions with timeboxes, OmniLink uses a flexible specification language, and offers a different linearizability checking method based on off-the-shelf model checking. OmniLink offers different features compared existing linearizability checking tools, and we show that it outperforms the state of the art on large scale validation tasks.
  Our evaluation validates WiredTiger, a state-of-the-art industrial database storage layer, as well as Balanced Augmented Tree (BAT), a state-of-the art lock-free data structure from the research community, and ConcurrentQueue, a popular lock-free queue featuring aggressive performance optimizations. We use OmniLink to improve WiredTiger's existing TLA+ model, as well as develop new TLA+ models that closely match the behavior of the modeled systems, including non-linearizable behaviors. OmniLink is able to find known bugs injected into the systems under test, as well as help discover two previously unknown bugs (1 in BAT, 1 in ConcurrentQueue), which we have confirmed with the authors of those systems.

</details>


### [55] [Harmonica: A Self-Adaptation Exemplar for Sustainable MLOps](https://arxiv.org/abs/2601.11926)
*Ananya Halgatti,Shaunak Biswas,Hiya Bhatt,Srinivasan Rakhunathan,Karthik Vaidhyanathan*

Main category: cs.SE

TL;DR: Harmonica是一个基于HarmonE方法的自适应范例，旨在支持机器学习运维管道的可持续运行，通过MAPE-K循环实现结构化自适应控制，减少手动干预并提高系统稳定性。


<details>
  <summary>Details</summary>
Motivation: 机器学习系统在运行环境中经常面临不确定性变化，这些变化可能降低模型性能、增加运营成本并减少系统实用性。虽然MLOps简化了ML模型生命周期，但对影响MLS长期可持续性的运行时不确定性支持有限。需要一种机制来检测执行漂移并调整系统行为，但目前缺乏研究这些挑战的范例。

Method: Harmonica基于HarmonE方法构建，采用MAPE-K循环实现结构化自适应控制，将高级自适应策略与低级战术执行分离。持续监控可持续性指标，根据动态自适应边界进行评估，并在阈值违反时自动触发架构战术。

Result: 通过时间序列回归和计算机视觉的案例研究，展示了Harmonica能够提高系统稳定性并减少手动干预。结果表明Harmonica为依赖MLOps管道持续运行的MLS提供了一个实用且可重用的自适应行为基础。

Conclusion: Harmonica提供了一个支持机器学习运维管道可持续运行的自适应范例，通过结构化自适应控制机制，能够有效应对运行时不确定性，为研究自适应MLS在MLOps环境中的挑战提供了实用工具。

Abstract: Machine learning enabled systems (MLS) often operate in settings where they regularly encounter uncertainties arising from changes in their surrounding environment. Without structured oversight, such changes can degrade model behavior, increase operational cost, and reduce the usefulness of deployed systems. Although Machine Learning Operations (MLOps) streamlines the lifecycle of ML models, it provides limited support for addressing runtime uncertainties that influence the longer term sustainability of MLS. To support continued viability, these systems need a mechanism that detects when execution drifts outside acceptable bounds and adjusts system behavior in response. Despite the growing interest in sustainable and self-adaptive MLS, there has been limited work towards exemplars that allow researchers to study these challenges in MLOps pipelines. This paper presents Harmonica, a self-adaptation exemplar built on the HarmonE approach, designed to enable the sustainable operation of such pipelines. Harmonica introduces structured adaptive control through MAPE-K loop, separating high-level adaptation policy from low-level tactic execution. It continuously monitors sustainability metrics, evaluates them against dynamic adaptation boundaries, and automatically triggers architectural tactics when thresholds are violated. We demonstrate the tool through case studies in time series regression and computer vision, examining its ability to improve system stability and reduce manual intervention. The results show that Harmonica offers a practical and reusable foundation for enabling adaptive behavior in MLS that rely on MLOps pipelines for sustained operation.

</details>


### [56] [Enhancing Fuzz Testing Efficiency through Automated Fuzz Target Generation](https://arxiv.org/abs/2601.11972)
*Chi Thien Tran*

Main category: cs.SE

TL;DR: 本文提出了一种通过静态分析库源代码来改进模糊测试目标生成的方法，解决了手动创建模糊测试目标耗时费力的问题。


<details>
  <summary>Details</summary>
Motivation: 模糊测试是发现软件安全漏洞最有效的方法，但大型软件项目中手动创建模糊测试目标既耗时又费力。现有研究主要关注模糊测试器的优化，而模糊测试目标的质量和数量对提高覆盖率同样重要。

Method: 通过静态分析库源代码结构来生成模糊测试目标：1）分析源代码结构以准确构建函数调用；2）将模糊测试输入数据映射到相应函数参数；3）合成模糊测试目标的编译信息；4）自动收集和分析执行结果。

Result: 该方法已应用于C/C++库的模糊测试目标生成，展示了其在自动化生成模糊测试目标方面的有效性。

Conclusion: 提出的静态分析方法能够有效自动化生成模糊测试目标，解决了大规模软件项目中手动创建模糊测试目标的挑战，有助于提高模糊测试的覆盖率和效率。

Abstract: Fuzzing continues to be the most effective method for identifying security vulnerabilities in software. In the context of fuzz testing, the fuzzer supplies varied inputs to fuzz targets, which are designed to comprehensively exercise critical sections of the client code. Various studies have focused on optimizing and developing advanced fuzzers, such as AFL++, libFuzzer, Honggfuzz, syzkaller, ISP-Fuzzer, which have substantially enhanced vulnerability detection in widely used software and libraries. Nevertheless, achieving greater coverage necessitates improvements in both the quality and quantity of fuzz targets. In large-scale software projects and libraries -- characterized by numerous user defined functions and data types -- manual creation of fuzz targets is both labor-intensive and time-consuming. This challenge underscores the need for automated techniques not only to generate fuzz targets but also to streamline the execution and analysis of their results. In this paper, we introduce an approach to improving fuzz target generation through static analysis of library source code. The proposed method encompasses several key aspects: it analyzes source code structures to accurately construct function calls and generate fuzz targets; it maps fuzzer input data to the corresponding function parameters; it synthesizes compilation information for the fuzz targets; and it automatically collects and analyzes execution results. Our findings are demonstrated through the application of this approach to the generation of fuzz targets for C/C++ libraries.

</details>


### [57] [From LLMs to Agents in Programming: The Impact of Providing an LLM with a Compiler](https://arxiv.org/abs/2601.12146)
*Viktor Kjellberg,Miroslaw Staron,Farnaz Fotrousi*

Main category: cs.SE

TL;DR: 研究探讨了让LLM访问编译器工具如何提升代码生成质量，通过RosettaCode数据集的699个C语言任务实验，发现编译器访问能显著提高编译成功率，减少语法错误，且在某些情况下小模型+编译器优于大模型+编译器。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在代码生成方面表现突出，但生成的源代码常存在质量问题甚至无法编译。现有研究将LLM发展为能够推理问题的智能体，但缺乏对软件工程工具（如编译器）如何影响其性能的系统研究。

Method: 在RosettaCode数据集的699个C语言编程任务上进行计算实验，评估16个不同规模的LLM（从1.35亿到700亿参数）。比较有/无编译器访问时LLM的表现，让LLM从被动生成器转变为能够基于编译器反馈迭代开发可运行程序的主动智能体。

Result: 编译器访问使编译成功率提升5.3%到79.4%，语法错误减少75%，未定义引用错误减少87%。在某些情况下，配备编译器的小模型表现优于配备编译器的大模型。工具访问显著改善了代码质量而不影响程序语义。

Conclusion: LLM必须访问软件工程工具以提升性能，这可以减少对大型模型的依赖，从而降低能源消耗。编译器集成将LLM从被动生成器转变为主动的代码开发智能体，显著提高软件工程任务的效率和质量。

Abstract: Large Language Models have demonstrated a remarkable capability in natural language and program generation and software development. However, the source code generated by the LLMs does not always meet quality requirements and may fail to compile. Therefore, many studies evolve into agents that can reason about the problem before generating the source code for the solution. The goal of this paper is to study the degree to which such agents benefit from access to software development tools, in our case, a \texttt{gcc} compiler. We conduct a computational experiment on the RosettaCode dataset, on 699 programming tasks in C. We evaluate how the integration with a compiler shifts the role of the language model from a passive generator to an active agent capable of iteratively developing runnable programs based on feedback from the compiler. We evaluated 16 language models with sizes ranging from small (135 million) to medium (3 billion) and large (70 billion). Our results show that access to a compiler improved the compilation success by 5.3 to 79.4 percentage units in compilation without affecting the semantics of the generated program. Syntax errors dropped by 75\%, and errors related to undefined references dropped by 87\% for the tasks where the agents outperformed the baselines. We also observed that in some cases, smaller models with a compiler outperform larger models with a compiler. We conclude that it is essential for LLMs to have access to software engineering tools to enhance their performance and reduce the need for large models in software engineering, such as reducing our energy footprint.

</details>


### [58] [Many Hands Make Light Work: An LLM-based Multi-Agent System for Detecting Malicious PyPI Packages](https://arxiv.org/abs/2601.12148)
*Muhammad Umar Zeshan,Motunrayo Ibiyo,Claudio Di Sipio,Phuong T. Nguyen,Davide Di Ruscio*

Main category: cs.SE

TL;DR: LAMPS是一个基于多智能体LLM协作的系统，用于检测PyPI恶意软件包，通过四个角色特定的智能体实现，在多个数据集上超越了现有方法。


<details>
  <summary>Details</summary>
Motivation: 开源仓库中的恶意代码对软件供应链构成日益增长的威胁，传统基于规则的工具常忽略源代码中的语义模式，而LLM在可解释和模块化安全管道中的应用仍然有限。

Method: 提出LAMPS多智能体系统，包含四个角色特定的智能体：包检索、文件提取、分类和裁决聚合，通过CrewAI框架协调。原型结合了微调的CodeBERT模型进行分类和LLaMA-3智能体进行上下文推理。

Result: 在两个数据集上评估：D1（6000个setup.py文件）上达到97.7%准确率，超越MPHunter；D2（1296个多文件数据集）上达到99.5%准确率和99.5%平衡准确率，优于RAG方法和微调单智能体基线。McNemar检验确认改进具有高度显著性。

Conclusion: 结果证明了分布式LLM推理在恶意代码检测中的可行性，并突显了模块化多智能体设计在软件供应链安全中的优势。

Abstract: Malicious code in open-source repositories such as PyPI poses a growing threat to software supply chains. Traditional rule-based tools often overlook the semantic patterns in source code that are crucial for identifying adversarial components. Large language models (LLMs) show promise for software analysis, yet their use in interpretable and modular security pipelines remains limited. This paper presents LAMPS, a multi-agent system that employs collaborative LLMs to detect malicious PyPI packages. The system consists of four role-specific agents for package retrieval, file extraction, classification, and verdict aggregation, coordinated through the CrewAI framework. A prototype combines a fine-tuned CodeBERT model for classification with LLaMA-3 agents for contextual reasoning. LAMPS has been evaluated on two complementary datasets: D1, a balanced collection of 6,000 setup.py files, and D2, a realistic multi-file dataset with 1,296 files and natural class imbalance. On D1, LAMPS achieves 97.7% accuracy, surpassing MPHunter--one of the state-of-the-art approaches. On D2, it reaches 99.5% accuracy and 99.5% balanced accuracy, outperforming RAG-based approaches and fine-tuned single-agent baselines. McNemar's test confirmed these improvements as highly significant. The results demonstrate the feasibility of distributed LLM reasoning for malicious code detection and highlight the benefits of modular multi-agent designs in software supply chain security.

</details>


### [59] [Aletheia: What Makes RLVR For Code Verifiers Tick?](https://arxiv.org/abs/2601.12186)
*Vatsal Venkatkrishna,Indraneil Paul,Iryna Gurevych*

Main category: cs.SE

TL;DR: 该论文提出了Aletheia测试平台，用于评估代码验证器在不同模型和协变量偏移下的鲁棒性，并研究了RLVR训练方法的关键组件。


<details>
  <summary>Details</summary>
Motivation: 尽管基于RLVR的多领域思维验证器在LLM后训练中被广泛应用，但在代码生成领域的应用相对较少。代码验证器在执行反馈难以获取的场景中具有重要价值，是代码生成后训练工具箱的有力补充。

Method: 创建并开源Aletheia测试平台，支持基于执行的代码验证器鲁棒性评估。研究RLVR训练方法的关键组件：中间思维轨迹、从负样本学习、在线策略训练，并分析这些组件在不同规模验证器中的重要性。

Result: 实验证实了RLVR方法的最优性，但发现了简化训练方法的重要机会。代码验证表现出正训练和推理时间缩放，在线策略学习在小规模验证器中是关键组件，而基于思维的训练在更大规模时成为最重要的组件。

Conclusion: 代码验证器是代码生成后训练的有价值工具，特别是在执行反馈难以获取的场景。RLVR训练方法虽然最优，但其组件的重要性随验证器规模变化，为简化训练方法提供了重要见解。

Abstract: Multi-domain thinking verifiers trained via Reinforcement Learning from Verifiable Rewards (RLVR) are a prominent fixture of the Large Language Model (LLM) post-training pipeline, owing to their ability to robustly rate and rerank model outputs. However, the adoption of such verifiers towards code generation has been comparatively sparse, with execution feedback constituting the dominant signal. Nonetheless, code verifiers remain valuable toward judging model outputs in scenarios where execution feedback is hard to obtain and are a potentially powerful addition to the code generation post-training toolbox. To this end, we create and open-source Aletheia, a controlled testbed that enables execution-grounded evaluation of code verifiers' robustness across disparate policy models and covariate shifts. We examine components of the RLVR-based verifier training recipe widely credited for its success: (1) intermediate thinking traces, (2) learning from negative samples, and (3) on-policy training. While experiments show the optimality of RLVR, we uncover important opportunities to simplify the recipe. Particularly, despite code verification exhibiting positive training- and inference-time scaling, on-policy learning stands out as the key component at small verifier sizes, and thinking-based training emerges as the most important component at larger scales.

</details>


### [60] [Environment-Aware Code Generation: How far are We?](https://arxiv.org/abs/2601.12262)
*Tongtong Wu,Rongyi Chen,Wenjie Du,Suyu Ma,Guilin Qi,Zhenchang Xing,Shahram Khadivi,Ramesh Periyathambi,Gholamreza Haffari*

Main category: cs.SE

TL;DR: 该研究首次系统性地探讨了环境感知代码生成问题，提出了VersiBCB基准测试，并研究了三种适应策略来提升LLM在特定软件环境下的代码生成能力。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型在代码生成方面虽有进步，但大多数评估仍局限于孤立的小规模代码测试，且通常在默认或未指定的软件环境下进行。这导致无法确定LLM能否可靠地生成适合用户特定环境的可执行代码。

Method: 1. 引入VersiBCB基准测试：多包、执行验证、弃用感知，捕捉复杂且不断演化的环境；2. 研究三种互补的适应轴：数据、参数和缓存；3. 为每个适应轴开发代表性策略。

Result: 当前LLM在环境特定代码生成方面表现不佳，但通过提出的适应策略可以显著改善环境兼容性和可执行性。

Conclusion: 该研究揭示了在实际软件工程工作流中部署LLM的关键挑战和机遇，强调了环境感知代码生成的重要性。

Abstract: Recent progress in large language models (LLMs) has improved code generation, but most evaluations still test isolated, small-scale code (e.g., a single function) under default or unspecified software environments. As a result, it is unclear whether LLMs can reliably generate executable code tailored to a user's specific environment. We present the first systematic study of Environment-Aware Code Generation (EACG), where generated code must be functionally correct and directly executable under arbitrary software configurations. To enable realistic evaluation, we introduce VersiBCB, a benchmark that is multi-package, execution-verified, and deprecation-aware, capturing complex and evolving environments that prior datasets often overlook. Using VersiBCB, we investigate three complementary adaptation axes: data, parameters, and cache, and develop representative strategies for each. Our results show that current LLMs struggle with environment-specific code generation, while our adaptations improve environment compatibility and executability. These findings highlight key challenges and opportunities for deploying LLMs in practical software engineering workflows.

</details>


### [61] [Leveraging Mutation Analysis for LLM-based Repair of Quantum Programs](https://arxiv.org/abs/2601.12273)
*Chihiro Yoshida,Yuta Ishimoto,Olivier Nourry,Masanari Kondo,Makoto Matsushita,Yasutaka Kamei,Yoshiki Higo*

Main category: cs.SE

TL;DR: 该研究构建了一个基于大语言模型的量子程序自动修复框架，通过包含变异分析结果的提示配置，显著提高了修复成功率和生成补丁的可理解性。


<details>
  <summary>Details</summary>
Motivation: 现有量子程序自动修复技术存在修复成功率低和生成补丁可理解性差的问题，需要开发既能提高可靠性又能增强可解释性的新方法。

Method: 构建了一个LLM生成代码修复及自然语言解释的框架，设计了四种包含不同上下文信息的提示配置（静态信息、动态信息、变异分析结果的不同组合），其中变异分析通过评估程序微小变化对执行结果的影响提供更详细的动态信息。

Result: 实验结果显示，包含变异分析的提示配置显著提高了LLM在量子程序自动修复中的表现，修复成功率达到了94.4%，在某些情况下还提高了生成解释的质量。

Conclusion: 变异分析能为基于LLM的量子程序自动修复提供有价值的上下文信息，这一发现为开发同时增强可靠性和可解释性的量子程序修复技术指明了新方向。

Abstract: In recent years, Automated Program Repair (APR) techniques specifically designed for quantum programs have been proposed. However, existing approaches often suffer from low repair success rates or poor understandability of the generated patches. In this study, we construct a framework in which a large language model (LLM) generates code repairs along with a natural language explanation of the applied repairs. To investigate how the contextual information included in prompts influences APR performance for quantum programs, we design four prompt configurations with different combinations of static information, dynamic information, and mutation analysis results. Mutation analysis evaluates how small changes to specific parts of a program affect its execution results and provides more detailed dynamic information than simple execution outputs such as stack traces. Our experimental results show that mutation analysis can provide valuable contextual information for LLM-based APR of quantum programs, improving repair success rates (achieving 94.4% in our experiment) and in some cases also improving the quality of generated explanations. Our findings point toward new directions for developing APR techniques for quantum programs that enhance both reliability and explainability.

</details>


### [62] [Hybrid Concolic Testing with Large Language Models for Guided Path Exploration](https://arxiv.org/abs/2601.12274)
*Mahdi Eslamimehr*

Main category: cs.SE

TL;DR: 提出了一种结合符号执行与大型语言模型的新型混合测试框架，通过LLM的语义推理能力指导路径探索和约束求解，显著提升了测试效率和覆盖率。


<details>
  <summary>Details</summary>
Motivation: 传统符号测试存在路径爆炸和约束求解成本高等根本性限制，阻碍了其在大规模实际软件系统中的应用。需要新的方法来克服这些挑战。

Method: 提出了一种新颖的算法框架，将符号执行与大型语言模型协同集成。利用LLM的语义推理能力指导路径探索、优先处理有趣的执行路径，并协助约束求解。正式定义了构成这一新范式的系统架构和算法。

Result: 通过在合成和实际金融科技应用上进行的一系列实验，证明该方法在分支覆盖率、路径覆盖率和达到覆盖率的时间方面，显著优于传统符号测试、随机测试和基于遗传算法的方法。

Conclusion: 通过结合符号执行和LLM的优势，该方法实现了对程序状态空间更高效和有效的探索，从而提高了错误检测能力，为大规模实际软件测试提供了有前景的解决方案。

Abstract: Concolic testing, a powerful hybrid software testing technique, has historically been plagued by fundamental limitations such as path explosion and the high cost of constraint solving, which hinder its practical application in large-scale, real-world software systems. This paper introduces a novel algorithmic framework that synergistically integrates concolic execution with Large Language Models (LLMs) to overcome these challenges. Our hybrid approach leverages the semantic reasoning capabilities of LLMs to guide path exploration, prioritize interesting execution paths, and assist in constraint solving. We formally define the system architecture and algorithms that constitute this new paradigm. Through a series of experiments on both synthetic and real-world Fintech applications, we demonstrate that our approach significantly outperforms traditional concolic testing, random testing, and genetic algorithm-based methods in terms of branch coverage, path coverage, and time-to-coverage. The results indicate that by combining the strengths of both concolic execution and LLMs, our method achieves a more efficient and effective exploration of the program state space, leading to improved bug detection capabilities.

</details>


### [63] [The Expert Validation Framework (EVF): Enabling Domain Expert Control in AI Engineering](https://arxiv.org/abs/2601.12327)
*Lucas Gren,Felix Dobslaw*

Main category: cs.SE

TL;DR: 提出专家验证框架，将领域专家置于构建含生成式AI组件的软件中心，通过结构化规范、测试、验证和持续监控确保系统质量


<details>
  <summary>Details</summary>
Motivation: 生成式AI系统有望改变知识工作，但在企业部署中因缺乏系统化质量保证机制而受阻，需要解决AI能力与组织信任之间的关键差距

Method: 提出专家验证框架，包含四个阶段：规范制定、系统创建、验证测试、生产监控，让领域专家通过结构化流程保持对系统行为的权威控制

Result: 建立了一个严谨的专家驱动方法论，确保跨不同生成式AI应用的质量，使组织能够在保持专家监督和质量标准的同时利用生成式AI能力

Conclusion: 该框架解决了生成式AI在企业部署中的质量保证难题，通过将领域专家置于中心位置，建立了系统化的质量保障机制，弥合了AI能力与组织信任之间的差距

Abstract: Generative AI (GenAI) systems promise to transform knowledge work by automating a range of tasks, yet their deployment in enterprise settings remains hindered by the lack of systematic quality assurance mechanisms. We present an Expert Validation Framework that places domain experts at the center of building software with GenAI components, enabling them to maintain authoritative control over system behavior through structured specification, testing, validation, and continuous monitoring processes. Our framework addresses the critical gap between AI capabilities and organizational trust by establishing a rigorous, expert-driven methodology for ensuring quality across diverse GenAI applications. Through a four-stage implementation process encompassing specification, system creation, validation, and production monitoring, the framework enables organizations to leverage GenAI capabilities while maintaining expert oversight and quality standards.

</details>


### [64] [Discovering 100+ Compiler Defects in 72 Hours via LLM-Driven Semantic Logic Recomposition](https://arxiv.org/abs/2601.12360)
*Xinabang He,Yuanwei Chen,Hao Wu,Jikang Zhang,Zicheng Wang,Ligeng Chen,Junjie Peng,Haiyang Wei,Yi Qian,Tiantai Zhang,Linzhang Wang,Bing Mao*

Main category: cs.SE

TL;DR: FeatureFuzz是一个编译器模糊测试工具，通过组合特征来生成程序，能更好地保留触发bug的语义信息，相比现有方法在GCC和LLVM上发现了更多崩溃和bug。


<details>
  <summary>Details</summary>
Motivation: 现有编译器模糊测试方法主要基于语法变异或通用大语言模型微调，难以保留触发bug的关键语义信息，导致生成的程序多样性受限，无法有效发现编译器缺陷。

Method: FeatureFuzz采用三阶段工作流程：1) 从历史bug报告中提取特征（包含自然语言描述的bug易发不变式和具体代码实例）；2) 合成特征组；3) 将特征组实例化为有效程序用于编译器模糊测试。

Result: 在24小时测试中，FeatureFuzz发现了167个独特崩溃，是第二好模糊测试工具的2.78倍。在72小时测试中，在GCC和LLVM中发现了106个bug，其中76个已被编译器开发者确认。

Conclusion: FeatureFuzz通过显式重用bug触发语义，能更有效地对现代编译器进行压力测试，显著提高了编译器模糊测试的效果。

Abstract: Compilers constitute the foundational root-of-trust in software supply chains; however, their immense complexity inevitably conceals critical defects. Recent research has attempted to leverage historical bugs to design new mutation operators or fine-tune models to increase program diversity for compiler fuzzing.We observe, however, that bugs manifest primarily based on the semantics of input programs rather than their syntax. Unfortunately, current approaches, whether relying on syntactic mutation or general Large Language Model (LLM) fine-tuning, struggle to preserve the specific semantics found in the logic of bug-triggering programs. Consequently, these critical semantic triggers are often lost, resulting in a limitation of the diversity of generated programs.
  To explicitly reuse such semantics, we propose FeatureFuzz, a compiler fuzzer that combines features to generate programs. We define a feature as a decoupled primitive that encapsulates a natural language description of a bug-prone invariant, such as an out-of-bounds array access, alongside a concrete code witness of its realization. FeatureFuzz operates via a three-stage workflow: it first extracts features from historical bug reports, synthesizes coherent groups of features, and finally instantiates these groups into valid programs for compiler fuzzing.
  We evaluated FeatureFuzz on GCC and LLVM. Over 24-hour campaigns, FeatureFuzz uncovered 167 unique crashes, which is 2.78x more than the second-best fuzzer. Furthermore, through a 72-hour fuzzing campaign, FeatureFuzz identified 106 bugs in GCC and LLVM, 76 of which have already been confirmed by compiler developers, validating the approach's ability to stress-test modern compilers effectively.

</details>


### [65] [Evaluating Large Language Models for Time Series Anomaly Detection in Aerospace Software](https://arxiv.org/abs/2601.12448)
*Yang Liu,Yixing Luo,Xiaofeng Li,Xiaogang Dong,Bin Gu,Zhi Jin*

Main category: cs.SE

TL;DR: ATSADBench是首个航空航天时间序列异常检测基准，包含9个任务和108,000个数据点，评估了LLM在两种范式下的表现，并提出了三个面向用户的评估指标。


<details>
  <summary>Details</summary>
Motivation: 航空航天软件系统需要可靠的时间序列异常检测，虽然LLM提供了无需训练的无监督替代方案，但其在航空航天环境中的有效性尚未得到充分研究，因为存在复杂的遥测数据、评估指标不匹配以及缺乏领域知识等问题。

Method: 构建ATSADBench基准，包含三种模式异常类型、单变量和多变量信号、内外环反馈场景的9个任务。系统评估开源LLM在两种范式下的表现：直接检测和基于预测的检测。提出窗口级评估和三个用户导向指标：警报准确率、警报延迟、警报连续性。研究两种增强策略：少样本学习和检索增强生成。

Result: LLM在单变量任务上表现良好，但在多变量遥测上表现不佳；在多变量任务上的AA和AC接近随机猜测；少样本学习提供适度改进而RAG无显著提升；实践中LLM能检测真实异常起始但有时产生误报，少样本提示能缓解而RAG会加剧误报。

Conclusion: 研究结果为未来基于LLM的航空航天软件时间序列异常检测提供了指导，表明LLM在单变量任务上有潜力，但在多变量场景和领域知识注入方面仍需改进。

Abstract: Time series anomaly detection (TSAD) is essential for ensuring the safety and reliability of aerospace software systems. Although large language models (LLMs) provide a promising training-free alternative to unsupervised approaches, their effectiveness in aerospace settings remains under-examined because of complex telemetry, misaligned evaluation metrics, and the absence of domain knowledge. To address this gap, we introduce ATSADBench, the first benchmark for aerospace TSAD. ATSADBench comprises nine tasks that combine three pattern-wise anomaly types, univariate and multivariate signals, and both in-loop and out-of-loop feedback scenarios, yielding 108,000 data points. Using this benchmark, we systematically evaluate state-of-the-art open-source LLMs under two paradigms: Direct, which labels anomalies within sliding windows, and Prediction-Based, which detects anomalies from prediction errors. To reflect operational needs, we reformulate evaluation at the window level and propose three user-oriented metrics: Alarm Accuracy (AA), Alarm Latency (AL), and Alarm Contiguity (AC), which quantify alarm correctness, timeliness, and credibility. We further examine two enhancement strategies, few-shot learning and retrieval-augmented generation (RAG), to inject domain knowledge. The evaluation results show that (1) LLMs perform well on univariate tasks but struggle with multivariate telemetry, (2) their AA and AC on multivariate tasks approach random guessing, (3) few-shot learning provides modest gains whereas RAG offers no significant improvement, and (4) in practice LLMs can detect true anomaly onsets yet sometimes raise false alarms, which few-shot prompting mitigates but RAG exacerbates. These findings offer guidance for future LLM-based TSAD in aerospace software.

</details>


### [66] [Improved Bug Localization with AI Agents Leveraging Hypothesis and Dynamic Cognition](https://arxiv.org/abs/2601.12522)
*Asif Mohammed Samir,Mohammad Masudur Rahman*

Main category: cs.SE

TL;DR: CogniGent是一种基于多智能体的bug定位技术，通过因果推理、调用图分析和上下文工程，模拟开发者的动态认知调试过程，显著提升了bug定位性能。


<details>
  <summary>Details</summary>
Motivation: 软件bug每年造成巨大经济损失，开发者50%时间用于bug修复。传统bug定位方法孤立分析代码组件，忽略组件间联系；现有LLM方法缺乏因果推理能力且难以管理增长上下文，限制了bug定位能力。

Method: 提出CogniGent多智能体技术，结合因果推理、基于调用图的根因分析和上下文工程，模拟开发者的动态认知调试实践，通过假设检验支持bug定位。

Result: 在591个bug报告数据集上评估，使用三个广泛采用的性能指标，与六个基线方法比较。CogniGent在文档和方法级别上MAP提升23.33-38.57%，MRR提升25.14-53.74%，统计显著性测试确认其优越性。

Conclusion: CogniGent通过解决推理、依赖和上下文限制，将类人认知与智能体自动化相结合，推进了bug定位技术发展，实现了性能的显著提升。

Abstract: Software bugs cost technology providers (e.g., AT&T) billions annually and cause developers to spend roughly 50% of their time on bug resolution. Traditional methods for bug localization often analyze the suspiciousness of code components (e.g., methods, documents) in isolation, overlooking their connections with other components in the codebase. Recent advances in Large Language Models (LLMs) and agentic AI techniques have shown strong potential for code understanding, but still lack causal reasoning during code exploration and struggle to manage growing context effectively, limiting their capability. In this paper, we present a novel agentic technique for bug localization -- CogniGent -- that overcomes the limitations above by leveraging multiple AI agents capable of causal reasoning, call-graph-based root cause analysis and context engineering. It emulates developers-inspired debugging practices (a.k.a., dynamic cognitive debugging) and conducts hypothesis testing to support bug localization. We evaluate CogniGent on a curated dataset of 591 bug reports using three widely adopted performance metrics and compare it against six established baselines from the literature. Experimental results show that our technique consistently outperformed existing traditional and LLM-based techniques, achieving MAP improvements of 23.33-38.57% at the document and method levels. Similar gains were observed in MRR, with increases of 25.14-53.74% at both granularity levels. Statistical significance tests also confirm the superiority of our technique. By addressing the reasoning, dependency, and context limitations, CogniGent advances the state of bug localization, bridging human-like cognition with agentic automation for improved performance.

</details>


### [67] [Automated Tool Support for Category-Partition Testing: Design Decisions, UI and Examples of Use](https://arxiv.org/abs/2601.12559)
*Yvan Labiche*

Main category: cs.SE

TL;DR: 本文介绍了一个自动化功能测试工具，基于类别划分技术，通过图形界面支持用户定义参数、类别和选择，自动生成测试框架和测试用例。


<details>
  <summary>Details</summary>
Motivation: 类别划分是一种功能测试技术，但传统的手动过程繁琐且容易出错。本文旨在自动化类别划分测试的多个步骤，提高测试效率和准确性。

Method: 开发了一个图形用户界面工具，允许用户指定参数和环境变量，定义类别和选择（支持布尔、整数、实数、字符串等类型），并可选地添加约束。工具自动根据不同的选择标准构建测试框架（选择的组合），并为这些框架识别参数和环境变量的输入值，从而生成测试用例。

Result: 该工具在九个不同的案例研究中展示了其能力，能够自动化测试框架的构建和测试用例的生成过程。

Conclusion: 通过图形界面工具自动化类别划分测试过程是可行的，能够显著提高测试效率，减少手动工作量，并在多个案例研究中验证了其有效性。

Abstract: Category-Partition is a functional testing technique that is based on the idea that the input domain of the system under test can be divided into sub-domains, with the assumption that inputs that belong to the same sub-domain trigger a similar behaviour and that therefore it is sufficient to select one input from each sub-domain. Category-Partition proceeds in several steps, from the identification of so-called categories and choices, possibly constrained, which are subsequently used to form test frames, i.e., combinations of choices, and eventually test cases. This paper reports on an ongoing attempt to automate as many of those steps as possible, with graphical-user interface tool support. Specifically, the user interface allows the user to specify parameters as well as so-called environment variables, further specify categories and choices with optional constraints. Choices are provided with precise specifications with operations specific to their types (e.g., Boolean, Integer, Real, String). Then, the tool automates the construction of test frames, which are combinations of choices, according to alternative selection criteria, and the identification of input values for parameters and environment variables for these test frames, thereby producing test cases. The paper illustrates the capabilities of the tool with the use of nine different case studies.

</details>


### [68] [OpenAI for OpenAPI: Automated generation of REST API specification via LLMs](https://arxiv.org/abs/2601.12735)
*Hao Chen,Yunchun Li,Chen Chen,Fengxu Lin,Wei Li*

Main category: cs.SE

TL;DR: OOPS：首个技术无关的基于LLM的静态分析方法，用于从REST API代码生成OpenAPI规范，解决了上下文限制和幻觉问题，在多种技术栈上实现高精度。


<details>
  <summary>Details</summary>
Motivation: 开发者编写和维护OpenAPI规范面临挑战，现有静态分析方法受限于特定编程语言和框架，而LLM方法存在上下文限制和幻觉问题，需要技术无关的解决方案。

Method: 提出OOPS方法，包含两个关键步骤：端点方法提取和OAS生成。通过构建API依赖图建立文件关联解决上下文限制，采用多阶段生成和自优化机制缓解语法和语义幻觉。

Result: 在12个真实REST API（涵盖5种编程语言和8种开发框架）上评估，端点方法推断F1分数超过98%，请求参数和响应推断达97%，参数约束推断达92%，输入输出token控制在合理范围。

Conclusion: OOPS是首个技术无关的LLM静态分析方法，能准确为多种技术实现的REST API生成高质量OpenAPI规范，减少技术特定规则和人工干预需求。

Abstract: REST APIs, based on the REpresentational State Transfer (REST) architecture, are the primary type of Web API. The OpenAPI Specification (OAS) serves as the de facto standard for describing REST APIs and is crucial for multiple software engineering tasks. However, developers face challenges in writing and maintaining OAS. Although static analysis shows potential for OAS generation, it is limited to specific programming languages and development frameworks. The powerful code understanding capabilities of LLMs offer new opportunities for OAS generation, yet they are constrained by context limitations and hallucinations. To address these challenges, we propose the OpenAI OpenAPI Project Scanner (OOPS), the first technology-agnostic LLM-based static analysis method for OAS generation, requiring fewer technology-specific rules and less human expert intervention. OOPS is implemented as an LLM agent workflow comprising two key steps: endpoint method extraction and OAS generation. By constructing an API dependency graph, it establishes necessary file associations to address LLMs' context limitations. Through multi-stage generation and self-refine, it mitigates both syntactic and semantic hallucinations during OAS generation. We evaluated OOPS on 12 real-world REST APIs spanning 5 programming languages and 8 development frameworks. Experimental results demonstrate that OOPS accurately generates high-quality OAS for REST APIs implemented with diverse technologies, achieving an average F1-score exceeding 98% for endpoint method inference, 97% for both request parameter and response inference, and 92% for parameter constraint inference. The input tokens average below 5.6K with a maximum of 16.2K, while the output tokens average below 0.9K with a maximum of 7.7K.

</details>


### [69] [Teaching LLMs to Learn Tool Trialing and Execution through Environment Interaction](https://arxiv.org/abs/2601.12762)
*Xingjie Gao,Pengcheng Huang,Zhenghao Liu,Yukun Yan,Shuo Wang,Zulong Chen,Chen Qian,Ge Yu,Yu Gu*

Main category: cs.SE

TL;DR: ToolMaster框架将LLM工具使用从模仿静态轨迹转变为通过环境交互主动学习，采用试错执行范式，显著提升对新工具的泛化能力和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有基于轨迹记忆的方法在面对新工具或演化工具时泛化能力有限，需要更鲁棒的框架来应对真实世界中工具的动态变化。

Method: 采用试错执行范式：先模仿包含明确工具尝试和自我修正的教师轨迹，再通过强化学习联合协调尝试和执行阶段，让智能体通过环境交互自主探索正确工具使用。

Result: 实验结果表明ToolMaster在未见或不熟悉工具上的泛化能力和鲁棒性显著优于现有基线方法。

Conclusion: ToolMaster通过从轨迹模仿转向主动交互学习，有效提升了LLM在动态工具环境中的适应能力和鲁棒性。

Abstract: Equipping Large Language Models (LLMs) with external tools enables them to solve complex real-world problems. However, the robustness of existing methods remains a critical challenge when confronting novel or evolving tools. Existing trajectory-centric paradigms primarily rely on memorizing static solution paths during training, which limits the ability of LLMs to generalize tool usage to newly introduced or previously unseen tools. In this paper, we propose ToolMaster, a framework that shifts tool use from imitating golden tool-calling trajectories to actively learning tool usage through interaction with the environment. To optimize LLMs for tool planning and invocation, ToolMaster adopts a trial-and-execution paradigm, which trains LLMs to first imitate teacher-generated trajectories containing explicit tool trials and self-correction, followed by reinforcement learning to coordinate the trial and execution phases jointly. This process enables agents to autonomously explore correct tool usage by actively interacting with environments and forming experiential knowledge that benefits tool execution. Experimental results demonstrate that ToolMaster significantly outperforms existing baselines in terms of generalization and robustness across unseen or unfamiliar tools. All code and data are available at https://github.com/NEUIR/ToolMaster.

</details>


### [70] [Docker Does Not Guarantee Reproducibility](https://arxiv.org/abs/2601.12811)
*Julien Malka,Stefano Zacchiroli,Théo Zimmermann*

Main category: cs.SE

TL;DR: 本文通过系统文献综述和大规模实证研究，评估Docker在实际应用中的可重现性保证和局限性，并验证文献中最佳实践的有效性。


<details>
  <summary>Details</summary>
Motivation: 软件环境可重现性对协作工作流、软件供应链安全和科学可重现性至关重要。虽然Docker常被视为理论上支持可重现性的工具，但其实际保证和局限性在实践中尚未得到充分探索。

Method: 采用两种互补方法：1）系统文献综述，分析Docker在科学文献中如何被描述为可重现性工具，并识别实现可重现镜像构建的Dockerfile最佳实践；2）大规模实证研究，从GitHub工作流收集5298个Docker构建，通过重建镜像并与历史版本比较，评估Docker镜像的实际可重现性。

Result: 研究评估了Docker镜像的实际可重现性程度，并验证了文献中识别的最佳实践在确保可重现性方面的有效性。

Conclusion: 该研究填补了Docker在实际应用中可重现性保证和局限性方面的知识空白，为开发人员提供了基于实证的最佳实践指导，有助于提高软件环境的可重现性。

Abstract: The reproducibility of software environments is a critical concern in modern software engineering, with ramifications ranging from the effectiveness of collaboration workflows to software supply chain security and scientific reproducibility. Containerization technologies like Docker address this problem by encapsulating software environments into shareable filesystem snapshots known as images. While Docker is frequently cited in the literature as a tool that enables reproducibility in theory, the extent of its guarantees and limitations in practice remains under-explored.
  In this work, we address this gap through two complementary approaches. First, we conduct a systematic literature review to examine how Docker is framed in scientific discourse on reproducibility and to identify documented best practices for writing Dockerfiles enabling reproducible image building. Then, we perform a large-scale empirical study of 5298 Docker builds collected from GitHub workflows. By rebuilding these images and comparing the results with their historical counterparts, we assess the real reproducibility of Docker images and evaluate the effectiveness of the best practices identified in the literature.

</details>


### [71] [Automatic Generation of Formal Specification and Verification Annotations Using LLMs and Test Oracles](https://arxiv.org/abs/2601.12845)
*João Pascoal Faria,Emanuel Trigo,Vinicius Honorato,Rui Abreu*

Main category: cs.SE

TL;DR: LLMs能自动为Dafny程序生成形式化验证注释，结合Claude Opus 4.5和GPT-5.2的多模型方法在最多8次修复迭代中为98.2%的程序生成正确注释。


<details>
  <summary>Details</summary>
Motivation: 虽然现代验证工具使形式化验证更易用，但为传统程序添加形式化规范和验证构造（前置条件、后置条件、循环不变量等）仍需要大量人工工作和专业知识。本研究探索如何利用LLMs自动为Dafny程序生成这些注释。

Method: 使用多模型方法结合Claude Opus 4.5和GPT-5.2，从带有自然语言规范（注释中）和测试代码的传统代码开始，自动生成Dafny验证注释。利用验证器反馈进行最多8次修复迭代，测试用例中的断言作为静态预言机验证生成的前置/后置条件。

Result: 在110个Dafny程序的实验中，多模型方法在最多8次修复迭代中为98.2%的程序生成了正确注释。逻辑回归分析显示，证明辅助注释对当前LLMs来说贡献了不成比例的难度。还开发了VS Code扩展集成自动生成功能。

Conclusion: LLMs能有效自动生成Dafny程序的形式化验证注释，多模型方法结合验证器反馈能实现高成功率。证明辅助注释仍是当前LLMs的主要挑战，IDE集成显示了良好的可用性前景。

Abstract: Recent verification tools aim to make formal verification more accessible to software engineers by automating most of the verification process. However, annotating conventional programs with the formal specification and verification constructs (preconditions, postconditions, loop invariants, auxiliary predicates and functions and proof helpers) required to prove their correctness still demands significant manual effort and expertise. This paper investigates how LLMs can automatically generate such annotations for programs written in Dafny, a verification-aware programming language, starting from conventional code accompanied by natural language specifications (in comments) and test code. In experiments on 110 Dafny programs, a multimodel approach combining Claude Opus 4.5 and GPT-5.2 generated correct annotations for 98.2% of the programs within at most 8 repair iterations, using verifier feedback. A logistic regression analysis shows that proof-helper annotations contribute disproportionately to problem difficulty for current LLMs. Assertions in the test cases served as static oracles to automatically validate the generated pre/postconditions. We also compare generated and manual solutions and present an extension for Visual Studio Code to incorporate automatic generation into the IDE, with encouraging usability feedback.

</details>


### [72] [Efficient Code Analysis via Graph-Guided Large Language Models](https://arxiv.org/abs/2601.12890)
*Hang Gao,Tao Peng,Baoquan Cui,Hong Huang,Fengge Wu,Junsuo Zhao,Jian Zhang*

Main category: cs.SE

TL;DR: 提出基于图注意力获取的恶意代码检测方法，通过代码图解析、GNN初步检测和注意力回溯，指导LLM聚焦关键代码区域，显著提升检测效果


<details>
  <summary>Details</summary>
Motivation: 恶意行为常隐藏在小型代码片段中，跨文件依赖使得即使强大的LLM也难以可靠检测，需要解决大代码库中恶意行为定位的难题

Method: 1) 将项目解析为代码图；2) 使用LLM编码节点语义和结构信息；3) 在稀疏监督下训练GNN进行初步检测；4) 回溯GNN预测识别关键代码区域；5) 用这些区域指导LLM注意力进行深入分析

Result: 在多个公开和自建数据集上的实验表明，该方法持续优于现有方法，显著减少无关上下文的干扰，同时保持低标注成本

Conclusion: 提出的图中心注意力获取管道增强了LLM定位恶意行为的能力，在软件安全场景中具有实际部署潜力

Abstract: Malicious behavior is often hidden in small, easily overlooked code fragments, especially within large and complex codebases. The cross-file dependencies of these fragments make it difficult for even powerful large language models (LLMs) to detect them reliably. We propose a graph-centric attention acquisition pipeline that enhances LLMs' ability to localize malicious behavior. The approach parses a project into a code graph, uses an LLM to encode nodes with semantic and structural signals, and trains a Graph Neural Network (GNN) under sparse supervision. The GNN performs an initial detection, and through backtracking of its predictions, identifies key code sections that are most likely to contain malicious behavior. These influential regions are then used to guide the LLM's attention for in-depth analysis. This strategy significantly reduces interference from irrelevant context while maintaining low annotation costs. Extensive experiments show that the method consistently outperforms existing methods on multiple public and self-built datasets, highlighting its potential for practical deployment in software security scenarios.

</details>


### [73] [A Benchmark for Language Models in Real-World System Building](https://arxiv.org/abs/2601.12927)
*Weilin Jin,Chenyu Zhao,Zeshun Huang,Chaoyun Zhang,Qingwei Lin,Chetan Bansal,Saravan Rajmohan,Shenglin Zhang,Yongqian Sun,Dan Pei,Yifan Wu,Tong Jia,Ying Li,Zhonghai Wu,Minghua Ma*

Main category: cs.SE

TL;DR: 论文提出了一个跨指令集架构（ISA）和编程语言的软件包构建修复基准测试，包含268个真实构建失败案例，评估了6个先进LLM，发现跨ISA修复仍具挑战性。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注单一ISA和同质编程语言，缺乏针对跨架构和跨语言软件包构建修复的标准化评估基准，这限制了软件可移植性和系统稳定性的提升。

Method: 构建了一个包含268个真实软件包构建失败案例的基准测试，涵盖多种架构和语言，并设计了标准化评估流程，用于评估6个先进LLM在跨ISA软件包修复任务上的表现。

Result: 评估结果显示，当前最先进的LLM在跨ISA软件包构建修复任务上仍然面临困难，表明该领域需要进一步的技术进步和方法创新。

Conclusion: 该基准测试系统地揭示了跨ISA软件包修复的挑战，为未来改进软件可移植性和弥合架构差距的方法研究奠定了基础。

Abstract: During migration across instruction set architectures (ISAs), software package build repair is a critical task for ensuring the reliability of software deployment and the stability of modern operating systems. While Large Language Models (LLMs) have shown promise in tackling this challenge, prior work has primarily focused on single instruction set architecture (ISA) and homogeneous programming languages. To address this limitation, we introduce a new benchmark designed for software package build repair across diverse architectures and languages. Comprising 268 real-world software package build failures, the benchmark provides a standardized evaluation pipeline. We evaluate six state-of-the-art LLMs on the benchmark, and the results show that cross-ISA software package repair remains difficult and requires further advances. By systematically exposing this challenge, the benchmark establishes a foundation for advancing future methods aimed at improving software portability and bridging architectural gaps.

</details>


### [74] [Beyond Accuracy: Characterizing Code Comprehension Capabilities in (Large) Language Models](https://arxiv.org/abs/2601.12951)
*Felix Mächtle,Jan-Niclas Serr,Nils Loose,Thomas Eisenbarth*

Main category: cs.SE

TL;DR: 本文研究发现，大语言模型（LLMs）在代码理解任务上的表现与传统人类中心软件度量指标相关性很低，而影子模型能更好地预测LLM性能，表明LLM理解能力反映了模型特有的规律性。


<details>
  <summary>Details</summary>
Motivation: 当前LLM基准测试仅提供粗略的性能总结，掩盖了模型多样化的能力和局限性。本文旨在探究LLM的代码理解性能是否与传统人类中心软件度量指标一致，还是反映了独特的非人类规律性。

Method: 引入诊断框架，将代码理解重构为二元输入-输出一致性任务，支持分类和生成模型评估。使用大规模数据集，将模型性能与传统人类中心复杂度指标（如词汇量、控制流复杂度、抽象语法树结构）进行相关性分析。

Result: 分析显示，人类定义的度量指标与LLM成功之间的相关性很低（AUROC 0.63），而影子模型获得了显著更高的预测性能（AUROC 0.86），捕捉到了超越传统软件度量的复杂、部分可预测模式。

Conclusion: LLM理解能力反映了模型特有的规律性，这些规律仅部分可通过人类设计或学习特征来获取。强调需要超越聚合准确率的基准测试方法，转向实例级诊断，同时承认预测正确结果存在根本性限制。

Abstract: Large Language Models (LLMs) are increasingly integrated into software engineering workflows, yet current benchmarks provide only coarse performance summaries that obscure the diverse capabilities and limitations of these models. This paper investigates whether LLMs' code-comprehension performance aligns with traditional human-centric software metrics or instead reflects distinct, non-human regularities. We introduce a diagnostic framework that reframes code understanding as a binary input-output consistency task, enabling the evaluation of classification and generative models. Using a large-scale dataset, we correlate model performance with traditional, human-centric complexity metrics, such as lexical size, control-flow complexity, and abstract syntax tree structure. Our analyses reveal minimal correlation between human-defined metrics and LLM success (AUROC 0.63), while shadow models achieve substantially higher predictive performance (AUROC 0.86), capturing complex, partially predictable patterns beyond traditional software measures. These findings suggest that LLM comprehension reflects model-specific regularities only partially accessible through either human-designed or learned features, emphasizing the need for benchmark methodologies that move beyond aggregate accuracy and toward instance-level diagnostics, while acknowledging fundamental limits in predicting correct outcomes.

</details>


### [75] [ArchAgent: Scalable Legacy Software Architecture Recovery with LLMs](https://arxiv.org/abs/2601.13007)
*Rusheng Pan,Bingcheng Mao,Tianyi Ma,Zhenhua Ling*

Main category: cs.SE

TL;DR: ArchAgent是一个基于智能体的可扩展框架，通过结合静态分析、自适应代码分割和LLM驱动的合成，从跨仓库代码库中重建多视图、业务对齐的软件架构。


<details>
  <summary>Details</summary>
Motivation: 从大规模遗留软件中恢复准确架构面临三个主要挑战：架构漂移、关系缺失以及大型语言模型的有限上下文。现有方法难以有效处理跨仓库代码库的业务逻辑恢复。

Method: ArchAgent采用基于智能体的框架，结合静态分析、自适应代码分割和LLM驱动的合成。引入可扩展的图表生成机制，包含上下文剪枝，并整合跨仓库数据来识别业务关键模块。

Result: 在典型的大规模GitHub项目评估中，相比现有基准有显著改进。消融研究证实依赖上下文提高了生产级仓库生成架构的准确性。真实案例研究展示了从遗留项目中有效恢复关键业务逻辑的能力。

Conclusion: ArchAgent框架能够有效解决大规模遗留软件架构恢复的挑战，通过智能体方法结合多种技术，实现了业务对齐的多视图架构重建，并在实际项目中验证了其有效性。

Abstract: Recovering accurate architecture from large-scale legacy software is hindered by architectural drift, missing relations, and the limited context of Large Language Models (LLMs). We present ArchAgent, a scalable agent-based framework that combines static analysis, adaptive code segmentation, and LLM-powered synthesis to reconstruct multiview, business-aligned architectures from cross-repository codebases. ArchAgent introduces scalable diagram generation with contextual pruning and integrates cross-repository data to identify business-critical modules. Evaluations of typical large-scale GitHub projects show significant improvements over existing benchmarks. An ablation study confirms that dependency context improves the accuracy of generated architectures of production-level repositories, and a real-world case study demonstrates effective recovery of critical business logics from legacy projects. The dataset is available at https://github.com/panrusheng/arch-eval-benchmark.

</details>


### [76] [RM -RF: Reward Model for Run-Free Unit Test Evaluation](https://arxiv.org/abs/2601.13097)
*Elena Bruches,Daniil Grebenkin,Mikhail Klementev,Vadim Alperovich,Roman Derunets,Dari Baturova,Georgy Mkrtchyan,Oleg Sedukhin,Ivan Bondarenko,Nikolay Bushkov,Stanislav Moiseev*

Main category: cs.SE

TL;DR: RM-RF是一个轻量级奖励模型，用于无需运行即可评估自动生成的单元测试，通过预测编译成功、代码覆盖率提升和变异杀死率改进三个执行相关信号，相比传统编译运行方法显著降低延迟和成本。


<details>
  <summary>Details</summary>
Motivation: 传统自动生成单元测试的评估需要反复编译和执行候选测试，这带来了高延迟和基础设施成本，限制了大规模测试生成和基于强化学习的代码优化的可扩展性。

Method: 开发RM-RF模型，仅从源代码和测试代码预测三个执行相关信号：测试套件编译运行成功、代码覆盖率提升、变异杀死率改进。使用多语言数据集（Java、Python、Go）训练，测试了多种模型架构和调优策略（零样本、全微调、LoRA PEFT）。

Result: 在三个预测目标上平均F1分数达到0.69。相比传统编译运行方法，RM-RF提供了显著更低的延迟和基础设施成本，同时保持了有竞争力的预测准确性。

Conclusion: RM-RF实现了快速、可扩展的反馈机制，能够支持大规模测试生成和基于强化学习的代码优化，在保持预测准确性的同时大幅降低了评估成本。

Abstract: We present RM-RF, a lightweight reward model for run-free evaluation of automatically generated unit tests. Instead of repeatedly compiling and executing candidate tests, RM-RF predicts - from source and test code alone - three execution-derived signals: (1) whether the augmented test suite compiles and runs successfully, (2) whether the generated test cases increase code coverage, and (3) whether the generated test cases improve the mutation kill rate. To train and evaluate RM-RF we assemble a multilingual dataset (Java, Python, Go) of focal files, test files, and candidate test additions labeled by an execution-based pipeline, and we release an associated dataset and methodology for comparative evaluation. We tested multiple model families and tuning regimes (zero-shot, full fine-tuning, and PEFT via LoRA), achieving an average F1 of 0.69 across the three targets. Compared to conventional compile-and-run instruments, RM-RF provides substantially lower latency and infrastructure cost while delivering competitive predictive fidelity, enabling fast, scalable feedback for large-scale test generation and RL-based code optimization.

</details>


### [77] [Guidelines to Prompt Large Language Models for Code Generation: An Empirical Characterization](https://arxiv.org/abs/2601.13118)
*Alessandro Midolo,Alessandro Giagnorio,Fiorella Zampetti,Rosalia Tufano,Gabriele Bavota,Massimiliano Di Penta*

Main category: cs.SE

TL;DR: 该研究为代码生成任务开发了10条提示优化指南，通过测试驱动方法自动优化提示，并评估了这些指南在开发者中的实际使用情况和感知有用性。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型广泛用于软件工程任务，特别是代码生成，但目前缺乏针对代码生成的具体提示工程指南。现有研究显示合适的提示工程能改善代码生成效果，但开发者缺乏系统性的指导原则。

Method: 采用迭代、测试驱动的方法自动优化代码生成提示，分析优化过程中导致测试通过的提示改进要素，从中提取出10条提示改进指南。随后对50名从业者进行评估，调查他们对这些指南的使用情况和感知有用性。

Result: 研究提取了10条提示改进指南，涉及更好地指定输入输出、前置后置条件、提供示例、添加各类细节、澄清歧义等方面。评估发现开发者的实际使用情况与感知有用性并不完全一致，有些指南虽然被认为有用但实际使用较少。

Conclusion: 该研究为代码生成任务提供了实用的提示优化指南，对从业者、教育工作者以及开发LLM辅助软件开发工具的人员都有重要价值，有助于提高代码生成的质量和效率。

Abstract: Large Language Models (LLMs) are nowadays extensively used for various types of software engineering tasks, primarily code generation. Previous research has shown how suitable prompt engineering could help developers in improving their code generation prompts. However, so far, there do not exist specific guidelines driving developers towards writing suitable prompts for code generation. In this work, we derive and evaluate development-specific prompt optimization guidelines. First, we use an iterative, test-driven approach to automatically refine code generation prompts, and we analyze the outcome of this process to identify prompt improvement items that lead to test passes. We use such elements to elicit 10 guidelines for prompt improvement, related to better specifying I/O, pre-post conditions, providing examples, various types of details, or clarifying ambiguities. We conduct an assessment with 50 practitioners, who report their usage of the elicited prompt improvement patterns, as well as their perceived usefulness, which does not always correspond to the actual usage before knowing our guidelines. Our results lead to implications not only for practitioners and educators, but also for those aimed at creating better LLM-aided software development tools.

</details>


### [78] [Earth Embeddings as Products: Taxonomy, Ecosystem, and Standardized Access](https://arxiv.org/abs/2601.13134)
*Heng Fang,Adam J. Stewart,Isaac Corley,Xiao Xiang Zhu,Hossein Azizpour*

Main category: cs.SE

TL;DR: 论文提出统一API标准化地理空间基础模型嵌入产品，解决现有生态系统碎片化问题，促进模型比较和可复现性


<details>
  <summary>Details</summary>
Motivation: 地理空间基础模型计算成本高，预计算嵌入产品作为实用替代方案，但现有生态系统存在格式和分辨率不兼容的碎片化问题，缺乏标准化阻碍了模型比较和可复现性

Method: 提出三层分类法（数据、工具、价值）分析现有产品，识别互操作性障碍，扩展TorchGeo提供统一API，标准化不同嵌入产品的加载和查询

Result: 通过将嵌入视为一等地理空间数据集，解耦下游分析与模型特定工程，为更透明和可访问的地球观测工作流提供路线图

Conclusion: 统一API标准化解决了地理空间嵌入产品的碎片化问题，促进了模型比较和可复现性，使地球观测工作流更加透明和可访问

Abstract: Geospatial Foundation Models (GFMs) provide powerful representations, but high compute costs hinder their widespread use. Pre-computed embedding data products offer a practical "frozen" alternative, yet they currently exist in a fragmented ecosystem of incompatible formats and resolutions. This lack of standardization creates an engineering bottleneck that prevents meaningful model comparison and reproducibility. We formalize this landscape through a three-layer taxonomy: Data, Tools, and Value. We survey existing products to identify interoperability barriers. To bridge this gap, we extend TorchGeo with a unified API that standardizes the loading and querying of diverse embedding products. By treating embeddings as first-class geospatial datasets, we decouple downstream analysis from model-specific engineering, providing a roadmap for more transparent and accessible Earth observation workflows.

</details>


### [79] [From Human to Machine Refactoring: Assessing GPT-4's Impact on Python Class Quality and Readability](https://arxiv.org/abs/2601.13139)
*Alessandro Midolo,Emiliano Tramontana,Massimiliano Di Penta*

Main category: cs.SE

TL;DR: GPT-4o在Python代码重构中的实证研究：行为正确性保持，代码质量提升，但可读性下降


<details>
  <summary>Details</summary>
Motivation: 尽管自动化重构工具已有广泛研究，但其实际应用仍有限。大型语言模型为自动化代码重构带来新机遇，但LLM驱动方法对代码质量的影响尚未明确评估。

Method: 使用GPT-4o对ClassEval基准中的100个Python类进行重构，涵盖Fowler重构目录中的多种类级重构。从三个互补角度评估：行为正确性（单元测试验证）、代码质量（Pylint、Flake8、SonarCloud评估）、可读性（先进可读性工具测量）。

Result: GPT-4o通常能产生保持行为正确的重构，减少代码异味并改善质量指标，但代价是降低了代码可读性。

Conclusion: 研究为LLM在自动化软件重构中的能力和局限性提供了新证据，突出了将LLM集成到实际重构工作流中的方向。

Abstract: Refactoring is a software engineering practice that aims to improve code quality without altering program behavior. Although automated refactoring tools have been extensively studied, their practical applicability remains limited. Recent advances in Large Language Models (LLMs) have introduced new opportunities for automated code refactoring. The evaluation of such an LLM-driven approach, however, leaves unanswered questions about its effects on code quality. In this paper, we present a comprehensive empirical study on LLM-driven refactoring using GPT-4o, applied to 100 Python classes from the ClassEval benchmark. Unlike prior work, our study explores a wide range of class-level refactorings inspired by Fowler's catalog and evaluates their effects from three complementary perspectives: (i) behavioral correctness, verified through unit tests; (ii) code quality, assessed via Pylint, Flake8, and SonarCloud; and (iii) readability, measured using a state-of-the-art readability tool. Our findings show that GPT-4o generally produces behavior-preserving refactorings that reduce code smells and improve quality metrics, albeit at the cost of decreased readability. Our results provide new evidence on the capabilities and limitations of LLMs in automated software refactoring, highlighting directions for integrating LLMs into practical refactoring workflows.

</details>


### [80] [SEER: Spectral Entropy Encoding of Roles for Context-Aware Attention-Based Design Pattern Detection](https://arxiv.org/abs/2601.13334)
*Tarik Houichime,Younes El Amrani*

Main category: cs.SE

TL;DR: SEER是GoF设计模式检测方法的升级版，通过谱熵角色编码和时间加权调用上下文改进角色识别和时序重要性，在保持跨语言兼容性的同时提升了检测性能。


<details>
  <summary>Details</summary>
Motivation: 先前方法虽然将代码建模为注意力序列，但缺乏对类内角色的明确区分，且将调用边统一处理，限制了检测精度。

Method: 引入两个核心组件：1) 谱熵角色编码器，从类的交互图谱拉普拉斯谱中提取成员角色嵌入；2) 时间加权调用上下文，为不同方法类别分配经验校准的持续时间先验。

Result: 在PyDesignNet数据集上，macro-F1从92.47%提升至93.20%，准确率从92.52%提升至93.98%，误报率降低近20%，同时提供可解释的符号级归因。

Conclusion: SEER通过改进角色识别和时序重要性建模，在不增加模型容量或数据的情况下，显著提升了设计模式检测的准确性和鲁棒性，具有实际应用价值。

Abstract: This paper presents SEER, an upgraded version of our prior method Context Is All You Need for detecting Gang of Four (GoF) design patterns from source code. The earlier approach modeled code as attention-ready sequences that blended lightweight structure with behavioral context; however, it lacked explicit role disambiguation within classes and treated call edges uniformly. SEER addresses these limitations with two principled additions: (i) a spectral-entropy role encoder that derives per-member role embeddings from the Laplacian spectrum of each class's interaction graph, and (ii) a time-weighted calling context that assigns empirically calibrated duration priors to method categories (e.g., constructors, getters/setters, static calls, virtual dispatch, cloning). Together, these components sharpen the model's notion of "who does what" and "how much it matters," while remaining portable across languages with minimal adaptation and fully compatible with Transformer-based sequence encoders. Importantly, SEER does not "force" a win by capacity or data; it nudges the classifier, steering attention toward role-consistent and temporally calibrated signals that matter most. We evaluate SEER on PyDesignNet (1,832 files, 35,000 sequences, 23 GoF patterns) and observe consistent gains over our previous system: macro-F1 increases from 92.47% to 93.20% and accuracy from 92.52% to 93.98%, with macro-precision 93.98% and macro-recall 92.52%. Beyond aggregate metrics, SEER reduces false positives by nearly 20%, a decisive improvement that strengthens its robustness and practical reliability. Moreover, SEER yields interpretable, symbol-level attributions aligned with canonical roles, exhibits robustness under small graph perturbations, and shows stable calibration.

</details>


### [81] [FlipFlop: A Static Analysis-based Energy Optimization Framework for GPU Kernels](https://arxiv.org/abs/2601.13345)
*Saurabhsingh Rajput,Alexander Brandt,Vadim Elisseev,Tushar Sharma*

Main category: cs.SE

TL;DR: FlipFlop是一个使用静态代码分析预测GPU内核能耗并推荐帕累托最优线程块配置的框架，无需运行时执行，可显著减少能耗并提高性能。


<details>
  <summary>Details</summary>
Motivation: GPU程序消耗大量能源，但软件开发人员通常缺乏硬件专业知识和专门知识来优化能效。现有方法需要运行时执行或专业知识，限制了能效优化的可及性。

Method: 使用静态代码分析分析PTX代码（CUDA GPU的低级指令集），预测能耗并推荐考虑功耗和执行时间的帕累托最优线程块配置，无需运行时执行。

Result: 在多样化GPU和内核上验证，达到83%的局部最优能效配置识别准确率，优化搜索空间减少93.4%。对多头注意力内核实现高达79%的能耗节省和106%的吞吐量提升。

Conclusion: FlipFlop通过将静态分析与实时监控结合并提供可解释的优化指导，使开发人员能够创建可持续、高性能的GPU软件，最小化环境和计算成本。

Abstract: Artificial Intelligence (AI) applications, such as Large Language Models, are primarily driven and executed by Graphics Processing Units (GPUs). These GPU programs (kernels) consume substantial amounts of energy, yet software developers often lack the hardware expertise and ad hoc knowledge required to optimize for power efficiency. We propose FlipFlop, a framework using static code analysis to predict energy consumption and recommend Pareto-optimal thread block configurations considering both power consumption and execution time. Our framework requires no runtime execution and analyzes PTX code, a low-level instruction set for CUDA-enabled GPUs. It is validated across a diverse set of GPUs and kernels, including multi-head attention, convolution, and matrix multiplication. FlipFlop achieves 83% accuracy in identifying locally optimal energy-efficient configurations, while also minimizing developer effort by reducing the optimization search space by 93.4%. For multi-head attention kernels, it yields up to 79% energy savings and 106% throughput gains relative to NVIDIA's occupancy heuristic. By integrating static analysis with real-time monitoring and providing explainable optimization guidance, FlipFlop empowers developers to create sustainable, high-performance GPU software which minimizes environmental and computational costs.

</details>


### [82] [Governance Matters: Lessons from Restructuring the data.table OSS Project](https://arxiv.org/abs/2601.13466)
*Pedro Oliveira,Doris Amoakohene,Toby Hocking,Marco Gerosa,Igor Steinmacher*

Main category: cs.SE

TL;DR: 本文通过data.table R包的案例研究，展示了开源软件社区治理改革如何显著提升项目可持续性、贡献者参与度和问题解决效率。


<details>
  <summary>Details</summary>
Motivation: 开源软件在工业数据工作流和企业系统中扮演关键角色，但许多项目面临非正式或集中化治理带来的运营风险。data.table作为一个广泛用于生产分析流水线的高性能R包，在改革前存在未解决问题积压、贡献者路径不清晰、依赖单一核心维护者等瓶颈问题。

Method: 采用混合方法研究：结合贡献者调查（n=17）和项目仓库数据挖掘，评估data.table社区治理改革的影响。

Result: 治理改革后，项目新贡献者招募增加200%，拉取请求解决时间从700多天降至一周内，贡献者保留率提升3倍。社区对透明度、入门流程和项目动力的满意度提高，但在公平性和冲突解决方面仍存在担忧。

Conclusion: 该案例研究为寻求改进开源软件治理的维护者、公司和基金会提供了实用指导，证明社区主导的治理改革能显著提升项目的可扩展性和可持续性。

Abstract: Open source software (OSS) forms the backbone of industrial data workflows and enterprise systems. However, many OSS projects face operational risks due to informal or centralized governance. This paper presents a practical case study of data.table, a high-performance R package widely adopted in production analytics pipelines, which underwent a community-led governance reform to address scalability and sustainability concerns. Before the reform, data.table faced a growing backlog of unresolved issues and open pull requests, unclear contributor pathways, and bottlenecks caused by reliance on a single core maintainer. In response, the community initiated a redesign of its governance structure. In this paper, we evaluated the impact of this transition through a mixed-methods approach, combining a contributor survey (n=17) with mining project repository data. Our results show that following the reform, the project experienced a 200% increase in new contributor recruitment, a drop in pull request resolution time from over 700 days to under a week, and a 3x increase in contributor retention. Community sentiment improved around transparency, onboarding, and project momentum, though concerns around fairness and conflict resolution remain. This case study provides practical guidance for maintainers, companies, and foundations seeking to enhance OSS governance.

</details>


### [83] [Why Does the LLM Stop Computing: An Empirical Study of User-Reported Failures in Open-Source LLMs](https://arxiv.org/abs/2601.13655)
*Guangba Yu,Zirui Wang,Yujie Huang,Renyi Zhong,Yuedong Zhong,Yilun Wang,Michael R. Lyu*

Main category: cs.SE

TL;DR: 对开源大语言模型（DeepSeek、Llama、Qwen）的705个真实世界故障进行首次大规模实证研究，发现白盒编排将可靠性瓶颈从模型算法缺陷转移到部署堆栈的系统脆弱性。


<details>
  <summary>Details</summary>
Motivation: 开源大语言模型的民主化使用户能够在本地基础设施上微调和部署模型，但将他们暴露在"第一英里"部署环境中。与黑盒API消费不同，用户管理的编排可靠性成为一个关键盲点，需要填补这一研究空白。

Method: 对开源DeepSeek、Llama和Qwen生态系统中705个真实世界故障进行大规模实证研究，分析故障模式、根本原因和系统性特征。

Result: 发现三个关键现象：1）诊断分歧：运行时崩溃主要指示基础设施摩擦，而不正确功能则作为内部分词器缺陷的特征；2）系统同质性：根本原因在不同系列模型中趋同，确认可靠性障碍是共享生态系统的固有特性而非特定架构问题；3）生命周期升级：障碍从微调期间的内在配置困难升级到推理期间的复合环境不兼容。

Conclusion: 白盒编排将可靠性瓶颈从模型算法缺陷转移到部署堆栈的系统脆弱性，这些见解为增强LLM生态系统的可靠性提供了可操作的指导，并支持公开可用的数据集。

Abstract: The democratization of open-source Large Language Models (LLMs) allows users to fine-tune and deploy models on local infrastructure but exposes them to a First Mile deployment landscape. Unlike black-box API consumption, the reliability of user-managed orchestration remains a critical blind spot. To bridge this gap, we conduct the first large-scale empirical study of 705 real-world failures from the open-source DeepSeek, Llama, and Qwen ecosystems.
  Our analysis reveals a paradigm shift: white-box orchestration relocates the reliability bottleneck from model algorithmic defects to the systemic fragility of the deployment stack. We identify three key phenomena: (1) Diagnostic Divergence: runtime crashes distinctively signal infrastructure friction, whereas incorrect functionality serves as a signature for internal tokenizer defects. (2) Systemic Homogeneity: Root causes converge across divergent series, confirming reliability barriers are inherent to the shared ecosystem rather than specific architectures. (3) Lifecycle Escalation: Barriers escalate from intrinsic configuration struggles during fine-tuning to compounded environmental incompatibilities during inference. Supported by our publicly available dataset, these insights provide actionable guidance for enhancing the reliability of the LLM landscape.

</details>


### [84] [CodeContests-O: Powering LLMs via Feedback-Driven Iterative Test Case Generation](https://arxiv.org/abs/2601.13682)
*Jianfeng Cai,Jinhua Zhu,Ruopei Sun,Kangwen Zhao,Dongyun Xue,Mingxiao Feng,Wengang Zhou,Houqiang Li*

Main category: cs.SE

TL;DR: 提出反馈驱动的迭代框架，用于构建高质量的编程测试用例数据集CodeContests-O，显著提升测试用例的保真度和区分度


<details>
  <summary>Details</summary>
Motivation: 推理模型需要大规模可验证数据，编程任务是理想来源，但现有平台缺乏高质量测试用例。现有方法仅依赖LLM内在生成能力，缺乏外部反馈，导致测试用例多样性不足

Method: 提出反馈驱动的迭代框架：1) LLM生成初始测试用例；2) 在已知正确和错误解决方案上执行测试；3) 利用失败结果作为反馈，指导LLM优化测试用例的保真度和区分度

Result: 构建的CodeContests-O数据集在1100万解决方案上平均TPR达89.37%，TNR达90.89%，显著优于CodeContests和CodeContests+。在Qwen2.5-7B模型上微调后，LiveCodeBench性能提升9.52%

Conclusion: 提出的反馈驱动迭代框架有效提升了测试用例质量，CodeContests-O数据集在保真度和区分度方面表现优异，为编程推理任务提供了高质量数据支持

Abstract: The rise of reasoning models necessitates large-scale verifiable data, for which programming tasks serve as an ideal source. However, while competitive programming platforms provide abundant problems and solutions, high-quality test cases for verification remain scarce. Existing approaches attempt to synthesize test cases using Large Language Models (LLMs), but rely solely on the model's intrinsic generation capabilities without external feedback, frequently resulting in insufficiently diverse cases. To address this limitation, we propose a $\textbf{Feedback-Driven Iterative Framework}$ for comprehensive test case construction. Specifically, our method leverages the LLM to generate initial test cases, executes them against known correct and incorrect solutions, and utilizes the failed results as feedback to guide the LLM in refining the test cases toward high fidelity and discriminability. We then apply this method to the CodeContests dataset to construct an optimized high-quality derivative, $\textbf{CodeContests-O}$. Evaluating against the entire pool of solutions ($1.1 \times 10^7$ in total), our dataset achieves an average True Positive Rate (TPR) of $89.37\%$ and True Negative Rate (TNR) of $90.89\%$, significantly outperforming the CodeContests and CodeContests+ by margins of $4.32\%$ and $9.37\%$, respectively. Furthermore, fine-tuning the Qwen2.5-7B model on CodeContests-O results in a $9.52\%$ improvement on LiveCodeBench (Pass@1). Experiments demonstrate the effectiveness of our framework and the quality of CodeContests-O. To support reproducibility and facilitate future research, we release the $\href{https://github.com/cai-jianfeng/CodeContests-O}{code}$ and $\href{https://huggingface.co/datasets/caijanfeng/CodeContests-O}{dataset}$.

</details>


### [85] [SWE-Tester: Training Open-Source LLMs for Issue Reproduction in Real-World Repositories](https://arxiv.org/abs/2601.13713)
*Aditya Bharat Soni,Rajat Ghosh,Vaishnavi Bhargava,Valerie Chen,Debojyoti Dutta*

Main category: cs.SE

TL;DR: 提出SWE-Tester框架，通过训练开源大语言模型来自动生成问题复现测试，在SWT-Bench Verified上实现最高10%的成功率和21%的变更覆盖率绝对提升


<details>
  <summary>Details</summary>
Motivation: 软件测试对确保软件系统正确性和可靠性至关重要。从自然语言问题描述自动生成问题复现测试能提高开发效率、促进测试驱动开发，并增强自动问题解决系统的有效性。现有方法主要依赖闭源LLM，对开源模型探索有限。

Method: 提出SWE-Tester训练管道：1) 从2.6K个开源GitHub仓库中整理41K个高质量训练实例数据集；2) 使用该数据集训练不同规模和系列的开源LLM；3) 通过增加推理计算、更多数据和更大模型来提升性能

Result: 微调模型在SWT-Bench Verified上实现最高10%的成功率绝对提升和21%的变更覆盖率绝对提升。分析显示增加推理计算、更多数据和更大模型能带来一致的性能改进

Conclusion: SWE-Tester框架有效推进了开源LLM在问题复现测试生成领域的应用，为开源模型在该任务上的性能提升提供了可行方案

Abstract: Software testing is crucial for ensuring the correctness and reliability of software systems. Automated generation of issue reproduction tests from natural language issue descriptions enhances developer productivity by simplifying root cause analysis, promotes test-driven development -- "test first, write code later", and can be used for improving the effectiveness of automated issue resolution systems like coding agents. Existing methods proposed for this task predominantly rely on closed-source LLMs, with limited exploration of open models. To address this, we propose SWE-Tester -- a novel pipeline for training open-source LLMs to generate issue reproduction tests. First, we curate a high-quality training dataset of 41K instances from 2.6K open-source GitHub repositories and use it to train LLMs of varying sizes and families. The fine-tuned models achieve absolute improvements of up to 10\% in success rate and 21\% in change coverage on SWT-Bench Verified. Further analysis shows consistent improvements with increased inference-time compute, more data, and larger models. These results highlight the effectiveness of our framework for advancing open-source LLMs in this domain.

</details>


### [86] [Counterexample Classification against Signal Temporal Logic Specifications](https://arxiv.org/abs/2601.13743)
*Zhenya Zhang,Parv Kapoor,Jie An,Eunsuk Kang*

Main category: cs.SE

TL;DR: 该论文提出使用参数化信号时序逻辑(PSTL)对违反STL规范的执行轨迹进行分类，通过推导类间包含关系并采用二分搜索方法提高分类效率。


<details>
  <summary>Details</summary>
Motivation: 在混合系统监控中，违反STL规范的执行轨迹（反例）可能源于不同原因，对应不同的系统缺陷。为了有效处理这些反例，需要合适的分类标准来理解可能的违反模式及其分布。

Method: 1. 使用参数化信号时序逻辑(PSTL)表示每个分类；2. 推导不同分类之间的包含关系；3. 基于包含关系提出类似二分搜索的方法，显著减少需要查询的分类数量。

Result: 实现了原型工具，并在两个广泛研究的系统上进行了实验评估，验证了该方法的有效性。

Conclusion: 提出的基于PSTL的分类标准和包含关系搜索方法能够有效对违反STL规范的反例进行分类，有助于理解系统缺陷的模式分布。

Abstract: Signal Temporal Logic (STL) has been widely adopted as a specification language for specifying desirable behaviors of hybrid systems. By monitoring a given STL specification, we can detect the executions that violate it, which are often referred to as counterexamples. In practice, these counterexamples may arise from different causes and thus are relevant to different system defects. To effectively address this, we need a proper criterion for classifying these counterexamples, by which we can comprehend the possible violation patterns and the distributions of these counterexamples with respect to the patterns. In this paper, we propose a classification criterion by using parametric signal temporal logic (PSTL) to represent each class. Due to this formalism, identifying the classes of a counterexample requires finding proper parameter values of PSTL that enable a class to include the counterexample. To improve the efficiency of class identification, we further derive an inclusion relation between different classes, and then propose a binary search-like approach over it that significantly prunes the classes needed to query. We implement a prototype tool and experimentally evaluate its effectiveness on two widely-studied systems.

</details>


### [87] [A Blockchain-Oriented Software Engineering Architecture for Carbon Credit Certification Systems](https://arxiv.org/abs/2601.13772)
*Matteo Vaccargiu,Azmat Ullah,Pierluigi Gallo*

Main category: cs.SE

TL;DR: 提出基于区块链和物联网的碳信用认证架构，通过100kWp光伏案例验证，整合实时数据收集、边缘聚合和链上存储，符合欧洲法规和自愿碳市场标准


<details>
  <summary>Details</summary>
Motivation: 现有区块链和物联网技术在排放监测和交易中的应用有限，特别是对中小型可再生能源设施的认证过程支持不足，需要建立可靠的碳信用认证机制

Method: 设计基于许可区块链的碳信用认证架构，整合实时物联网数据收集、边缘级数据聚合、安全的链上存储和智能合约，通过100kWp光伏案例进行验证

Result: 开发了一个结构化的碳信用记录生成路径，支持第三方验证，明确了光伏运营商的实际要求和约束条件，与现有专注于交易机制的方法不同

Conclusion: 该架构为中小型可再生能源设施提供了可靠的碳信用认证解决方案，符合法规标准，支持可验证的减排记录生成和第三方验证

Abstract: Carbon credit systems have emerged as a policy tool to incentivize emission reductions and support the transition to clean energy. Reliable carbon-credit certification depends on mechanisms that connect actual, measured renewable-energy production to verifiable emission-reduction records. Although blockchain and IoT technologies have been applied to emission monitoring and trading, existing work offers limited support for certification processes, particularly for small and medium-scale renewable installations. This paper introduces a blockchain-based carbon-credit certification architecture, demonstrated through a 100 kWp photovoltaic case study, that integrates real-time IoT data collection, edge-level aggregation, and secure on-chain storage on a permissioned blockchain with smart contracts. Unlike approaches focused on trading mechanisms, the proposed system aligns with European legislation and voluntary carbon-market standards, clarifying the practical requirements and constraints that apply to photovoltaic operators. The resulting architecture provides a structured pathway for generating verifiable carbon-credit records and supporting third-party verification.

</details>


### [88] [RepoGenesis: Benchmarking End-to-End Microservice Generation from Readme to Repository](https://arxiv.org/abs/2601.13943)
*Zhiyuan Peng,Xin Yin,Pu Zhao,Fangkai Yang,Lu Wang,Ran Jia,Xu Chen,Qingwei Lin,Saravan Rajmohan,Dongmei Zhang*

Main category: cs.SE

TL;DR: RepoGenesis是首个多语言仓库级端到端Web微服务生成基准，包含106个仓库和1258个API端点，评估结果显示现有系统在架构一致性和跨文件一致性方面存在不足。


<details>
  <summary>Details</summary>
Motivation: 现有基准主要关注孤立的函数/类级代码生成或现有代码库的修改，缺乏反映真实世界0到1开发流程的完整微服务仓库生成基准。

Method: 创建包含106个仓库（60个Python，46个Java）的多语言基准，涵盖18个领域和11个框架，通过"评审-反驳"质量保证流程验证1258个API端点和2335个测试用例。

Result: 最佳系统在Python和Java上分别仅达到23.67%和21.45%的Pass@1，尽管API覆盖率高达73.91%，部署成功率高达100%，但在架构一致性、依赖管理和跨文件一致性方面存在缺陷。

Conclusion: RepoGenesis基准揭示了当前微服务生成系统的局限性，并展示了其用于改进微服务生成的潜力，经过微调的GenesisAgent-8B性能可与GPT-5 mini相媲美。

Abstract: Large language models and agents have achieved remarkable progress in code generation. However, existing benchmarks focus on isolated function/class-level generation (e.g., ClassEval) or modifications to existing codebases (e.g., SWE-Bench), neglecting complete microservice repository generation that reflects real-world 0-to-1 development workflows. To bridge this gap, we introduce RepoGenesis, the first multilingual benchmark for repository-level end-to-end web microservice generation, comprising 106 repositories (60 Python, 46 Java) across 18 domains and 11 frameworks, with 1,258 API endpoints and 2,335 test cases verified through a "review-rebuttal" quality assurance process. We evaluate open-source agents (e.g., DeepCode) and commercial IDEs (e.g., Cursor) using Pass@1, API Coverage (AC), and Deployment Success Rate (DSR). Results reveal that despite high AC (up to 73.91%) and DSR (up to 100%), the best-performing system achieves only 23.67% Pass@1 on Python and 21.45% on Java, exposing deficiencies in architectural coherence, dependency management, and cross-file consistency. Notably, GenesisAgent-8B, fine-tuned on RepoGenesis (train), achieves performance comparable to GPT-5 mini, demonstrating the quality of RepoGenesis for advancing microservice generation. We release our benchmark at https://github.com/pzy2000/RepoGenesis.

</details>


### [89] [Software Testing in the Quantum World](https://arxiv.org/abs/2601.13996)
*Rui Abreu,Shaukat Ali,Paolo Arcaini,Jose Campos,Michael Felderer,Claude Gravel,Fuyuki Ishikawa,Stefan Klikovits,Andriy Miranskyy,Anila Mjeda,Mohammad Reza Mousavi,Masaomi Yamaguchi,Lei Zhang,Jianjun Zhao*

Main category: cs.SE

TL;DR: 该论文探讨了随着量子软件复杂性增加，传统量子计算机模拟变得不可行，需要直接在真实量子计算机上进行质量保证的新方法，提出了测试大规模量子软件的关键挑战和软件工程视角的解决方案。


<details>
  <summary>Details</summary>
Motivation: 量子计算在模拟物理、化学、生物系统以及优化和机器学习方面具有显著加速优势。随着量子软件复杂性增加，长期以来用于质量保证的经典量子计算机模拟变得不可行，需要直接在真实量子计算机上操作的新质量保证方法。

Method: 该论文从软件工程角度出发，分析测试大规模量子软件的关键挑战，并提出相应的解决方案视角。

Result: 论文识别了测试大规模量子软件的主要挑战，并提供了软件工程视角的框架来应对这些挑战。

Conclusion: 随着量子软件复杂性的增长，需要开发直接在真实量子计算机上操作的新质量保证方法，软件工程视角为解决大规模量子软件测试挑战提供了重要框架。

Abstract: Quantum computing offers significant speedups for simulating physical, chemical, and biological systems, and for optimization and machine learning. As quantum software grows in complexity, the classical simulation of quantum computers, which has long been essential for quality assurance, becomes infeasible. This shift requires new quality-assurance methods that operate directly on real quantum computers. This paper presents the key challenges in testing large-scale quantum software and offers software engineering perspectives for addressing them.

</details>


### [90] [Analyzing the Availability of E-Mail Addresses for PyPI Libraries](https://arxiv.org/abs/2601.14034)
*Alexandros Tsakpinis,Alexander Pretschner*

Main category: cs.SE

TL;DR: 分析PyPI上68.6万个Python库及其GitHub仓库中维护者联系信息的可用性，发现81.6%的库包含至少一个有效邮箱，PyPI是主要来源(79.5%)，依赖链中可达性高达97.8%，但存在69.8万个无效条目。


<details>
  <summary>Details</summary>
Motivation: 开源软件库的长期可持续性依赖于维护者的可联系性，但缺乏对Python生态系统中维护者联系信息可用性的系统性实证分析。

Method: 对Python Package Index (PyPI)上的686,034个Python库及其关联的GitHub仓库进行实证分析，检查维护者如何及在何处提供联系信息（特别是邮箱地址），评估其有效性，并分析在单个库及其依赖链中的覆盖情况。

Result: 81.6%的库包含至少一个有效邮箱地址，PyPI是主要来源(79.5%)。在依赖链分析中，直接依赖和传递依赖分别有97.8%和97.7%提供有效联系信息。同时发现了超过698,000个无效条目，主要是由于字段缺失。

Conclusion: Python生态系统整体维护者可达性较强，但仍有改进空间，如在打包过程中为维护者提供更清晰的指导，并为现有邮箱地址引入选择性的验证机制。

Abstract: Open Source Software (OSS) libraries form the backbone of modern software systems, yet their long-term sustainability often depends on maintainers being reachable for support, coordination, and security reporting. In this paper, we empirically analyze the availability of contact information - specifically e-mail addresses - across 686,034 Python libraries on the Python Package Index (PyPI) and their associated GitHub repositories. We examine how and where maintainers provide this information, assess its validity, and explore coverage across individual libraries and their dependency chains. Our findings show that 81.6% of libraries include at least one valid e-mail address, with PyPI serving as the primary source (79.5%). When analyzing dependency chains, we observe that up to 97.8% of direct and 97.7% of transitive dependencies provide valid contact information. At the same time, we identify over 698,000 invalid entries, primarily due to missing fields. These results demonstrate strong maintainer reachability across the ecosystem, while highlighting opportunities for improvement - such as offering clearer guidance to maintainers during the packaging process and introducing opt-in validation mechanisms for existing e-mail addresses.

</details>


### [91] [Feature-Aware Test Generation for Deep Learning Models](https://arxiv.org/abs/2601.14081)
*Xingcheng Chen,Oliver Weissl,Andrea Stocco*

Main category: cs.SE

TL;DR: Detect是一个特征感知的测试生成框架，通过在潜在空间中扰动解耦的语义属性来系统生成测试输入，用于评估视觉深度学习模型的质量和可靠性。


<details>
  <summary>Details</summary>
Motivation: 当前基于生成AI的测试生成方法在提供语义洞察和细粒度语义控制方面存在局限，无法深入理解模型错误行为的原因，需要更精细可控的测试生成方法。

Method: Detect在潜在空间中可控地扰动单个潜在特征，观察这些变化如何影响模型输出，识别导致行为转变的特征，并使用视觉语言模型进行语义归因。通过区分任务相关和无关特征，应用特征感知扰动进行泛化和鲁棒性测试。

Result: 在图像分类和检测任务上的实证结果显示，Detect能生成高质量、细粒度可控的测试用例，揭示不同模型架构的捷径行为，发现准确性指标无法捕获的缺陷。在决策边界发现和鲁棒性失败识别方面优于现有方法。

Conclusion: 完全微调的卷积模型容易过拟合局部线索，而弱监督的transformer倾向于依赖全局特征。特征感知测试对于提高深度学习模型可靠性具有重要价值。

Abstract: As deep learning models are widely used in software systems, test generation plays a crucial role in assessing the quality of such models before deployment. To date, the most advanced test generators rely on generative AI to synthesize inputs; however, these approaches remain limited in providing semantic insight into the causes of misbehaviours and in offering fine-grained semantic controllability over the generated inputs. In this paper, we introduce Detect, a feature-aware test generation framework for vision-based deep learning (DL) models that systematically generates inputs by perturbing disentangled semantic attributes within the latent space. Detect perturbs individual latent features in a controlled way and observes how these changes affect the model's output. Through this process, it identifies which features lead to behavior shifts and uses a vision-language model for semantic attribution. By distinguishing between task-relevant and irrelevant features, Detect applies feature-aware perturbations targeted for both generalization and robustness. Empirical results across image classification and detection tasks show that Detect generates high-quality test cases with fine-grained control, reveals distinct shortcut behaviors across model architectures (convolutional and transformer-based), and bugs that are not captured by accuracy metrics. Specifically, Detect outperforms a state-of-the-art test generator in decision boundary discovery and a leading spurious feature localization method in identifying robustness failures. Our findings show that fully fine-tuned convolutional models are prone to overfitting on localized cues, such as co-occurring visual traits, while weakly supervised transformers tend to rely on global features, such as environmental variances. These findings highlight the value of interpretable and feature-aware testing in improving DL model reliability.

</details>


### [92] [Practitioner Views on Mobile App Accessibility: Practices and Challenges](https://arxiv.org/abs/2601.14131)
*Amila Indika,Rick Kazman,Anthony Peruma*

Main category: cs.SE

TL;DR: 移动应用开发者普遍认识到无障碍功能的重要性，但实践中主要依赖平台特定指南，测试较晚，且面临API限制和组织约束等挑战。


<details>
  <summary>Details</summary>
Motivation: 虽然之前的研究已经识别了广泛存在的无障碍问题并提高了对开发者挑战的认识，但缺乏跨平台、全球代表性的关于开发者如何在实践中处理无障碍功能的见解。

Method: 采用混合方法对来自43个国家的110名移动应用开发者进行调查，研究平台生态系统（iOS vs Android）和开发者经验如何影响无障碍实践。

Result: 开发者虽然认识到无障碍的重要性，但主要依赖平台特定指南，通常在开发后期进行合规测试；主要实现文本相关功能，同时面临API限制和组织约束；通过跨平台比较发现了新的平台特定障碍，并展示了不同经验水平的开发者实践差异。

Conclusion: 研究结果提供了关于实践中实现无障碍功能挑战的新见解，并为各利益相关者提供了促进更一致和包容性应用开发的可操作步骤。

Abstract: As mobile applications (apps) become ubiquitous in everyday life, it is crucial for developers to prioritize accessibility for users with diverse abilities. While previous research has identified widespread accessibility issues and raised awareness of developer challenges, there remains a lack of cross-platform, globally representative insights into how practitioners approach accessibility in practice. This paper presents findings from a mixed-methods survey of 110 mobile app developers across 43 countries, examining how platform ecosystems (iOS vs. Android) and developer experience shape accessibility practices. Results show that while developers recognize the importance of accessibility, they often rely on platform-specific guidelines and typically perform compliance testing late in the development process. Developers primarily implement text-focused features while struggling with API limitations and organizational constraints. Through systematic cross-platform comparison, we identify novel platform-specific barriers and demonstrate how accessibility practices differ across developer experience levels. Our findings offer new insights into the challenges of achieving accessibility in practice and provide actionable steps for various stakeholders to promote more consistent and inclusive app development.

</details>


### [93] [An Empirical Study on Remote Code Execution in Machine Learning Model Hosting Ecosystems](https://arxiv.org/abs/2601.14163)
*Mohammed Latif Siddiq,Tanzim Hossain Romel,Natalie Sekerak,Beatrice Casey,Joanna C. S. Santos*

Main category: cs.SE

TL;DR: 对五个主要模型共享平台中自定义模型加载实践的大规模实证研究，揭示了不安全默认设置的普遍使用、平台间安全执行不均衡以及开发者对远程代码执行安全风险的持续困惑。


<details>
  <summary>Details</summary>
Motivation: 模型共享平台（如Hugging Face、ModelScope、OpenCSG等）已成为现代机器学习开发的核心，但执行不受信任代码的灵活性引入了关键安全风险。本研究旨在评估自定义模型加载实践的普遍性、相关风险以及开发者的认知。

Method: 1. 量化需要自定义代码才能运行的模型频率，识别加载期间执行任意Python文件的模型；2. 应用三种互补的静态分析工具（Bandit、CodeQL、Semgrep）检测安全异味和潜在漏洞，按CWE标识符分类；3. 使用YARA识别恶意模式和有效载荷签名；4. 系统分析每个平台的文档、API设计和安全机制；5. 对来自GitHub、Hugging Face、PyTorch Hub论坛和Stack Overflow的600多个开发者讨论进行定性分析。

Result: 研究发现：1. 广泛依赖不安全默认设置；2. 平台间安全执行不均衡；3. 开发者对远程代码执行的安全影响存在持续困惑。

Conclusion: 为设计更安全的模型共享基础设施提供了可操作建议，旨在在未来的AI生态系统中实现可用性与安全性之间的平衡。

Abstract: Model-sharing platforms, such as Hugging Face, ModelScope, and OpenCSG, have become central to modern machine learning development, enabling developers to share, load, and fine-tune pre-trained models with minimal effort. However, the flexibility of these ecosystems introduces a critical security concern: the execution of untrusted code during model loading (i.e., via trust_remote_code or trust_repo). In this work, we conduct the first large-scale empirical study of custom model loading practices across five major model-sharing platforms to assess their prevalence, associated risks, and developer perceptions. We first quantify the frequency with which models require custom code to function and identify those that execute arbitrary Python files during loading. We then apply three complementary static analysis tools: Bandit, CodeQL, and Semgrep, to detect security smells and potential vulnerabilities, categorizing our findings by CWE identifiers to provide a standardized risk taxonomy. We also use YARA to identify malicious patterns and payload signatures. In parallel, we systematically analyze the documentation, API design, and safety mechanisms of each platform to understand their mitigation strategies and enforcement levels. Finally, we conduct a qualitative analysis of over 600 developer discussions from GitHub, Hugging Face, and PyTorch Hub forums, as well as Stack Overflow, to capture community concerns and misconceptions regarding security and usability. Our findings reveal widespread reliance on unsafe defaults, uneven security enforcement across platforms, and persistent confusion among developers about the implications of executing remote code. We conclude with actionable recommendations for designing safer model-sharing infrastructures and striking a balance between usability and security in future AI ecosystems.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [94] [MIMIC-RD: Can LLMs differentially diagnose rare diseases in real-world clinical settings?](https://arxiv.org/abs/2601.11559)
*Zilal Eiz AlDin,John Wu,Jeffrey Paul Fung,Jennifer King,Mya Watts,Lauren ONeill,Adam Richard Cross,Jimeng Sun*

Main category: cs.AI

TL;DR: 研究人员创建了MIMIC-RD基准测试，通过将临床文本直接映射到Orphanet罕见病数据库来评估LLM在罕见病鉴别诊断中的表现，发现当前最先进的LLM表现不佳。


<details>
  <summary>Details</summary>
Motivation: 罕见病影响十分之一的美国人，但其鉴别诊断仍然具有挑战性。现有评估LLM罕见病诊断的方法存在两个关键局限：依赖理想化的临床案例研究，无法捕捉真实世界的临床复杂性；或使用ICD代码作为疾病标签，由于许多罕见病缺乏与Orphanet等综合罕见病数据库的直接映射，导致严重低估罕见病。

Method: 研究人员开发了MIMIC-RD基准测试，通过将临床文本实体直接映射到Orphanet罕见病数据库。方法包括：1）基于LLM的初步挖掘过程；2）由四名医学注释者验证，确认识别的实体是真正的罕见病。最终构建了包含145名患者的数据集。

Result: 评估各种模型后发现，当前最先进的大型语言模型在罕见病鉴别诊断方面表现不佳，突显出现有能力与临床需求之间的巨大差距。

Conclusion: 研究揭示了LLM在罕见病诊断方面的局限性，并提出了改进罕见病鉴别诊断的未来研究方向。

Abstract: Despite rare diseases affecting 1 in 10 Americans, their differential diagnosis remains challenging. Due to their impressive recall abilities, large language models (LLMs) have been recently explored for differential diagnosis. Existing approaches to evaluating LLM-based rare disease diagnosis suffer from two critical limitations: they rely on idealized clinical case studies that fail to capture real-world clinical complexity, or they use ICD codes as disease labels, which significantly undercounts rare diseases since many lack direct mappings to comprehensive rare disease databases like Orphanet. To address these limitations, we explore MIMIC-RD, a rare disease differential diagnosis benchmark constructed by directly mapping clinical text entities to Orphanet. Our methodology involved an initial LLM-based mining process followed by validation from four medical annotators to confirm identified entities were genuine rare diseases. We evaluated various models on our dataset of 145 patients and found that current state-of-the-art LLMs perform poorly on rare disease differential diagnosis, highlighting the substantial gap between existing capabilities and clinical needs. From our findings, we outline several future steps towards improving differential diagnosis of rare diseases.

</details>


### [95] [A Mind Cannot Be Smeared Across Time](https://arxiv.org/abs/2601.11620)
*Michael Timothy Bennett*

Main category: cs.AI

TL;DR: 论文认为机器意识不仅取决于计算内容，还取决于计算时机。顺序计算系统无法实现意识所需的同步性，需要硬件层面的并发能力。


<details>
  <summary>Details</summary>
Motivation: 现有AI系统大多采用顺序或时分复用计算，而意识体验具有统一性和同时性。论文旨在探讨计算时序对机器意识可能性的影响。

Method: 扩展栈理论，引入时间窗口约束的代数定律；定义精确的时间语义τ^{Δ,s}；证明存在性时间实现◇_Δ不保持合取；区分StrongSync和WeakSync两种假设；形式化并发能力度量。

Result: 系统可以在时间上实现所有体验成分，但从未实例化体验合取本身；神经生理证据支持StrongSync；严格顺序计算硬件无法实现需要两个以上同时贡献者的意识内容。

Conclusion: 机器意识需要硬件层面的并发能力，仅凭功能性能不足。意识归因需要架构检查，而不仅仅是功能表现。

Abstract: Whether machines can be conscious depends not only on what they compute, but \emph{when} they compute it. Most deployed artificial systems realise their functions via sequential or time-multiplexed updates. Conscious experience appears unified and simultaneous. I show that this difference matters formally. I augment Stack Theory with algebraic laws relating within time-window constraint satisfaction to conjunction. I introduce a precise temporal semantics over windowed trajectories $τ^{Δ,s}$ and prove that existential temporal realisation $\Diamond_Δ$ does not preserve conjunction. A system can realise all the ingredients of experience across time without ever instantiating the experienced conjunction itself. I then distinguish two postulates. StrongSync requires objective co-instantiation of the grounded conjunction within the window, while WeakSync permits temporal ``smearing''. I formalise concurrency-capacity to measure what is needed to satisfy StrongSync. Finally, I review neurophysiological evidence suggesting that consciousness depends on phase synchrony and effective connectivity, and that loss of consciousness is often associated with its breakdown. This evidence makes WeakSync less plausible. Under StrongSync, software consciousness on strictly sequential substrates is impossible for contents whose grounding requires two or more simultaneous contributors. The more parts from which simultaneous contribution required, the more concurrency capacity is required. The hardware matters. Consciousness attribution therefore requires architectural inspection, not just functional performance.

</details>


### [96] [Dynamical Systems Analysis Reveals Functional Regimes in Large Language Models](https://arxiv.org/abs/2601.11622)
*Hassan Ugail,Newton Howard*

Main category: cs.AI

TL;DR: 该研究将神经科学中的时间整合和亚稳态概念应用于Transformer模型，开发了一种基于激活时间序列的复合动力学指标，用于分析LLM在不同功能状态下的计算组织差异。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型通过高维内部动力学进行文本生成，但这些动力学的时间组织仍然未被充分理解。现有可解释性方法主要关注静态表示或因果干预，而忽略了时间结构。受神经科学中时间整合和亚稳态作为神经组织核心标记的启发，研究者希望将这些概念应用于Transformer模型。

Method: 研究者将神经科学中的时间整合和亚稳态概念适配到Transformer模型，开发了一种复合动力学指标，该指标基于自回归生成过程中的激活时间序列计算。在GPT-2-medium模型上评估了该指标在五种条件下的表现：结构化推理、强制重复、高温噪声采样、注意力头剪枝和权重噪声注入。

Result: 结构化推理条件相对于重复、噪声和扰动状态表现出显著升高的指标值。通过单因素方差分析确认了统计显著性差异，关键比较中显示出大的效应量。这些结果对层选择、通道子采样和随机种子具有鲁棒性。

Conclusion: 神经科学启发的动力学指标能够可靠地表征大型语言模型在不同功能状态下的计算组织差异。研究者强调所提出的指标捕获的是形式动力学特性，并不暗示主观体验。

Abstract: Large language models perform text generation through high-dimensional internal dynamics, yet the temporal organisation of these dynamics remains poorly understood. Most interpretability approaches emphasise static representations or causal interventions, leaving temporal structure largely unexplored. Drawing on neuroscience, where temporal integration and metastability are core markers of neural organisation, we adapt these concepts to transformer models and discuss a composite dynamical metric, computed from activation time-series during autoregressive generation. We evaluate this metric in GPT-2-medium across five conditions: structured reasoning, forced repetition, high-temperature noisy sampling, attention-head pruning, and weight-noise injection. Structured reasoning consistently exhibits elevated metric relative to repetitive, noisy, and perturbed regimes, with statistically significant differences confirmed by one-way ANOVA and large effect sizes in key comparisons. These results are robust to layer selection, channel subsampling, and random seeds. Our findings demonstrate that neuroscience-inspired dynamical metrics can reliably characterise differences in computational organisation across functional regimes in large language models. We stress that the proposed metric captures formal dynamical properties and does not imply subjective experience.

</details>


### [97] [Reasoning Stabilization Point: A Training-Time Signal for Stable Evidence and Shortcut Reliance](https://arxiv.org/abs/2601.11625)
*Sahil Rajesh Dhayalkar*

Main category: cs.AI

TL;DR: 该论文提出了一种训练时解释性方法，通过跟踪微调过程中token级归因的变化来监控模型决策依据的演变，并引入了"推理稳定点"概念。


<details>
  <summary>Details</summary>
Motivation: 微调预训练语言模型虽然能提升任务性能，但会微妙地改变模型依赖的证据依据。需要一种方法来监控微调过程中决策依据如何演变。

Method: 提出训练时解释性视角，跟踪微调各epoch中token级归因的变化。定义"解释漂移"为固定探测集上归一化token归因的epoch间变化，引入"推理稳定点"作为漂移首次进入持续低稳定状态的最早epoch。

Result: 在多个轻量级transformer分类器和基准分类任务上，漂移通常在训练早期就进入低稳定状态，而验证准确率仅发生微小变化。在受控的捷径设置中，归因动态揭示了模型对捷径的依赖增加，即使验证准确率保持竞争力。

Conclusion: 解释漂移提供了一种简单、低成本的诊断工具，用于监控微调过程中决策依据的演变，并选择处于稳定证据状态的检查点。

Abstract: Fine-tuning pretrained language models can improve task performance while subtly altering the evidence a model relies on. We propose a training-time interpretability view that tracks token-level attributions across finetuning epochs. We define explanation driftas the epoch-to-epoch change in normalized token attributions on a fixed probe set, and introduce the Reasoning Stabilization Point(RSP), the earliest epoch after which drift remains consistently low. RSP is computed from within-run drift dynamics and requires no tuning on out-of-distribution data. Across multiple lightweight transformer classifiers and benchmark classification tasks, drift typically collapses into a low, stable regime early in training, while validation accuracy continues to change only marginally. In a controlled shortcut setting with label-correlated trigger tokens, attribution dynamics expose increasing reliance on the shortcut even when validation accuracy remains competitive. Overall, explanation drift provides a simple, low-cost diagnostic for monitoring how decision evidence evolves during fine-tuning and for selecting checkpoints in a stable-evidence regime.

</details>


### [98] [POLARIS: Typed Planning and Governed Execution for Agentic AI in Back-Office Automation](https://arxiv.org/abs/2601.11816)
*Zahra Moslemi,Keerthi Koneru,Yen-Ting Lee,Sheethal Kumar,Ramesh Radhakrishnan*

Main category: cs.AI

TL;DR: POLARIS是一个面向企业后台工作流程的治理型LLM智能体编排框架，通过类型化计划合成和验证执行实现可审计、策略对齐且操作可预测的自动化系统。


<details>
  <summary>Details</summary>
Motivation: 企业后台工作流程需要可审计、策略对齐且操作可预测的智能体系统，而通用的多智能体设置往往无法满足这些要求。现有方法缺乏有效的治理机制来确保自动化流程符合企业政策并产生可追溯的决策记录。

Method: POLARIS采用治理型编排框架，将自动化视为基于LLM智能体的类型化计划合成和验证执行。框架包含：1）规划器生成结构多样、类型检查的有向无环图；2）基于评分标准的推理模块选择合规计划；3）执行阶段通过验证器门控检查、有界修复循环和编译的策略护栏来阻止或路由副作用。

Result: 在文档中心金融任务中，POLARIS能够生成决策级工件和完整执行轨迹，同时减少人工干预。在SROIE数据集上达到0.81的微F1分数，在受控合成套件中实现0.95-1.00的异常路由精度，同时保持审计轨迹完整。

Conclusion: POLARIS为策略对齐的智能体AI提供了方法论和基准参考，其评估结果构成了治理型智能体AI的初步基准。该框架展示了在企业自动化环境中实现可审计、策略合规的智能体系统的可行性。

Abstract: Enterprise back office workflows require agentic systems that are auditable, policy-aligned, and operationally predictable, capabilities that generic multi-agent setups often fail to deliver. We present POLARIS (Policy-Aware LLM Agentic Reasoning for Integrated Systems), a governed orchestration framework that treats automation as typed plan synthesis and validated execution over LLM agents. A planner proposes structurally diverse, type checked directed acyclic graphs (DAGs), a rubric guided reasoning module selects a single compliant plan, and execution is guarded by validator gated checks, a bounded repair loop, and compiled policy guardrails that block or route side effects before they occur. Applied to document centric finance tasks, POLARIS produces decision grade artifacts and full execution traces while reducing human intervention. Empirically, POLARIS achieves a micro F1 of 0.81 on the SROIE dataset and, on a controlled synthetic suite, achieves 0.95 to 1.00 precision for anomaly routing with preserved audit trails. These evaluations constitute an initial benchmark for governed Agentic AI. POLARIS provides a methodological and benchmark reference for policy-aligned Agentic AI. Keywords Agentic AI, Enterprise Automation, Back-Office Tasks, Benchmarks, Governance, Typed Planning, Evaluation

</details>


### [99] [AI Co-Scientist for Knowledge Synthesis in Medical Contexts: A Proof of Concept](https://arxiv.org/abs/2601.11825)
*Arya Rahgozar,Pouria Mortezaagha*

Main category: cs.AI

TL;DR: AI驱动的PICOS知识合成平台，通过自动化PICOS合规检测、研究设计分类、检索增强生成和主题建模，提高生物医学证据合成的可扩展性和透明度


<details>
  <summary>Details</summary>
Motivation: 解决生物医学研究中因重复研究、不完整报告和传统证据合成工作流程可扩展性有限导致的研究浪费问题

Method: 基于PICOS框架的AI协同科学家平台，整合关系存储、向量语义检索和Neo4j知识图谱；使用Bi-LSTM和PubMedBERT微调的transformer模型进行PICOS合规检测和研究设计分类；采用检索增强生成和BERTopic进行主题建模

Result: transformer模型在研究设计分类上达到95.7%准确率，Bi-LSTM在PICOS合规检测上达到87%准确率；检索增强生成在结构化约束、跨研究整合和图推理任务上优于非检索方法；主题建模揭示了大量主题冗余和未充分探索的研究领域

Conclusion: PICOS感知和可解释的自然语言处理能够提高证据合成的可扩展性、透明度和效率，该架构是领域无关的，为减少生物医学学科的研究浪费提供了实用框架

Abstract: Research waste in biomedical science is driven by redundant studies, incomplete reporting, and the limited scalability of traditional evidence synthesis workflows. We present an AI co-scientist for scalable and transparent knowledge synthesis based on explicit formalization of Population, Intervention, Comparator, Outcome, and Study design (PICOS). The platform integrates relational storage, vector-based semantic retrieval, and a Neo4j knowledge graph. Evaluation was conducted on dementia-sport and non-communicable disease corpora. Automated PICOS compliance and study design classification from titles and abstracts were performed using a Bidirectional Long Short-Term Memory baseline and a transformer-based multi-task classifier fine-tuned from PubMedBERT. Full-text synthesis employed retrieval-augmented generation with hybrid vector and graph retrieval, while BERTopic was used to identify thematic structure, redundancy, and evidence gaps. The transformer model achieved 95.7% accuracy for study design classification with strong agreement against expert annotations, while the Bi-LSTM achieved 87% accuracy for PICOS compliance detection. Retrieval-augmented generation outperformed non-retrieval generation for queries requiring structured constraints, cross-study integration, and graph-based reasoning, whereas non-retrieval approaches remained competitive for high-level summaries. Topic modeling revealed substantial thematic redundancy and identified underexplored research areas. These results demonstrate that PICOS-aware and explainable natural language processing can improve the scalability, transparency, and efficiency of evidence synthesis. The proposed architecture is domain-agnostic and offers a practical framework for reducing research waste across biomedical disciplines.

</details>


### [100] [Human-AI Collaborative Inductive Thematic Analysis: AI Guided Analysis and Human Interpretive Authority](https://arxiv.org/abs/2601.11850)
*Matthew Nyaaba,Min SungEun,Mary Abiswin Apam,Kwame Owoahene Acheampong,Emmanuel Dwamena,Xiaoming Zhai*

Main category: cs.AI

TL;DR: 研究探讨了生成式AI在质性研究中的应用，特别是ITA-GPT工具如何支持归纳主题分析，强调人类研究者保持解释权威的重要性。


<details>
  <summary>Details</summary>
Motivation: 随着生成式人工智能在质性研究中的使用日益增多，引发了关于分析实践和解释权威的重要问题。本研究旨在探讨研究人员如何与专门设计的AI工具ITA-GPT互动，以支持归纳主题分析。

Method: 采用人类-人工智能协作归纳主题分析框架，三位经验丰富的质性研究人员使用ITA-GPT工具分析加纳教师教育背景下的访谈转录文本。工具支持熟悉化、逐字编码、动名词描述性编码和主题开发，同时确保文本完整性、覆盖检查和可审计性。数据来源包括交互日志、AI生成的表格、研究人员修订、删除、插入、评论和反思备忘录。

Result: ITA-GPT作为程序性支架，结构化分析工作流程并增强透明度。然而，解释权威仍由人类研究者掌握，他们通过修改、删除、拒绝、插入和评论等反复分析行动行使判断力。

Conclusion: 研究展示了归纳主题分析如何通过负责任的人类-AI协作得以实施，强调AI作为辅助工具而非替代人类解释权威的角色。

Abstract: The increasing use of generative artificial intelligence (GenAI) in qualitative research raises important questions about analytic practice and interpretive authority. This study examines how researchers interact with an Inductive Thematic Analysis GPT (ITA-GPT), a purpose-built AI tool designed to support inductive thematic analysis through structured, semi-automated prompts aligned with reflexive thematic analysis and verbatim coding principles. Guided by a Human-Artificial Intelligence Collaborative Inductive Thematic Analysis (HACITA) framework, the study focuses on analytic process rather than substantive findings. Three experienced qualitative researchers conducted ITA-GPT assisted analyses of interview transcripts from education research in the Ghanaian teacher education context. The tool supported familiarization, verbatim in vivo coding, gerund-based descriptive coding, and theme development, while enforcing trace to text integrity, coverage checks, and auditability. Data sources included interaction logs, AI-generated tables, researcher revisions, deletions, insertions, comments, and reflexive memos. Findings show that ITA-GPT functioned as a procedural scaffold that structured analytic workflow and enhanced transparency. However, interpretive authority remained with human researchers, who exercised judgment through recurrent analytic actions including modification, deletion, rejection, insertion, and commenting. The study demonstrates how inductive thematic analysis is enacted through responsible human AI collaboration.

</details>


### [101] [MyGram: Modality-aware Graph Transformer with Global Distribution for Multi-modal Entity Alignment](https://arxiv.org/abs/2601.11885)
*Zhifei Li,Ziyue Qin,Xiangyu Luo,Xiaoju Hou,Yue Zhao,Miao Zhang,Zhifang Huang,Kui Xiao,Bing Yang*

Main category: cs.AI

TL;DR: MyGram：一种用于多模态实体对齐的模态感知图变换器，通过模态扩散学习和Gram损失实现跨模态全局分布一致性，在多个数据集上显著优于基线模型。


<details>
  <summary>Details</summary>
Motivation: 现有多模态实体对齐方法可能忽视每个模态内的结构上下文信息，容易受到浅层特征的干扰，需要更有效的跨模态融合和全局分布一致性方法。

Method: 提出MyGram模型，包含模态扩散学习模块捕获模态内深层结构上下文信息，以及Gram损失作为正则化约束，通过最小化多模态特征形成的4维平行六面体体积实现跨模态全局分布一致性。

Result: 在五个公共数据集上的实验表明，MyGram显著优于基线模型，在FBDB15K上Hits@1最大提升4.8%，在FBYG15K上提升9.9%，在DBP15K上提升4.3%。

Conclusion: MyGram通过模态扩散学习和Gram损失有效解决了多模态实体对齐中的模态内结构信息捕获和跨模态全局分布一致性问题，显著提升了对齐性能。

Abstract: Multi-modal entity alignment aims to identify equivalent entities between two multi-modal Knowledge graphs by integrating multi-modal data, such as images and text, to enrich the semantic representations of entities. However, existing methods may overlook the structural contextual information within each modality, making them vulnerable to interference from shallow features. To address these challenges, we propose MyGram, a modality-aware graph transformer with global distribution for multi-modal entity alignment. Specifically, we develop a modality diffusion learning module to capture deep structural contextual information within modalities and enable fine-grained multi-modal fusion. In addition, we introduce a Gram Loss that acts as a regularization constraint by minimizing the volume of a 4-dimensional parallelotope formed by multi-modal features, thereby achieving global distribution consistency across modalities. We conduct experiments on five public datasets. Results show that MyGram outperforms baseline models, achieving a maximum improvement of 4.8% in Hits@1 on FBDB15K, 9.9% on FBYG15K, and 4.3% on DBP15K.

</details>


### [102] [AEMA: Verifiable Evaluation Framework for Trustworthy and Controlled Agentic LLM Systems](https://arxiv.org/abs/2601.11903)
*YenTing Lee,Keerthi Koneru,Zahra Moslemi,Sheethal Kumar,Ramesh Radhakrishnan*

Main category: cs.AI

TL;DR: AEMA框架：用于评估LLM多智能体系统的过程感知、可审计评估框架，相比单一LLM-as-a-Judge方法，提供更稳定、可追溯且支持人类监督的自动化评估。


<details>
  <summary>Details</summary>
Motivation: 当前评估LLM多智能体系统的方法存在局限性，主要依赖单次响应评分或狭窄基准测试，缺乏稳定性、可扩展性和自动化能力，特别是在企业级多智能体规模部署时，需要可靠协调、透明决策和可验证性能的评估框架。

Method: 提出AEMA（自适应评估多智能体）框架，这是一个过程感知、可审计的评估系统，能够在人类监督下规划、执行和聚合异构智能体工作流的多步骤评估，支持可追溯记录和负责任自动化。

Result: 在模拟真实业务场景的企业级智能体工作流上验证，AEMA相比单一LLM-as-a-Judge方法，实现了更高的稳定性、更好的人类对齐性，并提供了可追溯的记录，支持负责任的自动化评估。

Conclusion: AEMA为LLM多智能体系统提供了一个透明、可复现的负责任评估路径，解决了现有评估方法在企业环境中缺乏稳定性、可扩展性和自动化能力的问题，支持可验证的多智能体系统评估。

Abstract: Evaluating large language model (LLM)-based multi-agent systems remains a critical challenge, as these systems must exhibit reliable coordination, transparent decision-making, and verifiable performance across evolving tasks. Existing evaluation approaches often limit themselves to single-response scoring or narrow benchmarks, which lack stability, extensibility, and automation when deployed in enterprise settings at multi-agent scale. We present AEMA (Adaptive Evaluation Multi-Agent), a process-aware and auditable framework that plans, executes, and aggregates multi-step evaluations across heterogeneous agentic workflows under human oversight. Compared to a single LLM-as-a-Judge, AEMA achieves greater stability, human alignment, and traceable records that support accountable automation. Our results on enterprise-style agent workflows simulated using realistic business scenarios demonstrate that AEMA provides a transparent and reproducible pathway toward responsible evaluation of LLM-based multi-agent systems.
  Keywords Agentic AI, Multi-Agent Systems, Trustworthy AI, Verifiable Evaluation, Human Oversight

</details>


### [103] [LIBRA: Language Model Informed Bandit Recourse Algorithm for Personalized Treatment Planning](https://arxiv.org/abs/2601.11905)
*Junyu Cao,Ruijiang Gao,Esmaeil Keyvanshokooh,Jianhao Ma*

Main category: cs.AI

TL;DR: 提出LIBRA框架，将算法追索、上下文老虎机和大型语言模型结合，用于高风险顺序决策（如个性化医疗），在保证统计严谨性的同时利用LLM领域知识。


<details>
  <summary>Details</summary>
Motivation: 在高风险顺序决策场景（如个性化医疗）中，需要同时选择治疗行动和对患者可变特征的最小可行修改，传统方法难以平衡领域知识和统计严谨性。

Method: 1. 提出追索老虎机问题；2. 开发广义线性追索老虎机算法；3. 提出LIBRA算法，战略性地结合LLM领域知识和老虎机学习的统计严谨性。

Result: LIBRA提供三个关键保证：热启动保证（LLM推荐接近最优时显著减少初始遗憾）、LLM努力保证（仅需O(log²T)次咨询LLM）、鲁棒性保证（即使LLM不可靠也不会比纯老虎机算法差）。实验证明在合成环境和真实高血压管理案例中优于基准方法。

Conclusion: 追索感知、LLM辅助的老虎机算法在个性化高风险决策中具有前景，能够实现可信的LLM-老虎机协作，提高遗憾、治疗质量和样本效率。

Abstract: We introduce a unified framework that seamlessly integrates algorithmic recourse, contextual bandits, and large language models (LLMs) to support sequential decision-making in high-stakes settings such as personalized medicine. We first introduce the recourse bandit problem, where a decision-maker must select both a treatment action and a feasible, minimal modification to mutable patient features. To address this problem, we develop the Generalized Linear Recourse Bandit (GLRB) algorithm. Building on this foundation, we propose LIBRA, a Language Model-Informed Bandit Recourse Algorithm that strategically combines domain knowledge from LLMs with the statistical rigor of bandit learning. LIBRA offers three key guarantees: (i) a warm-start guarantee, showing that LIBRA significantly reduces initial regret when LLM recommendations are near-optimal; (ii) an LLM-effort guarantee, proving that the algorithm consults the LLM only $O(\log^2 T)$ times, where $T$ is the time horizon, ensuring long-term autonomy; and (iii) a robustness guarantee, showing that LIBRA never performs worse than a pure bandit algorithm even when the LLM is unreliable. We further establish matching lower bounds that characterize the fundamental difficulty of the recourse bandit problem and demonstrate the near-optimality of our algorithms. Experiments on synthetic environments and a real hypertension-management case study confirm that GLRB and LIBRA improve regret, treatment quality, and sample efficiency compared with standard contextual bandits and LLM-only benchmarks. Our results highlight the promise of recourse-aware, LLM-assisted bandit algorithms for trustworthy LLM-bandits collaboration in personalized high-stakes decision-making.

</details>


### [104] [Thinking Traps in Long Chain-of-Thought: A Measurable Study and Trap-Aware Adaptive Restart](https://arxiv.org/abs/2601.11940)
*Kang Chen,Fan Yu,Junjie Nian,Shihan Zhao,Zhuoka Feng,Zijun Yao,Heng Wang,Minshen Yu,Yixin Cao*

Main category: cs.AI

TL;DR: 论文提出TAAR框架解决长思维链推理中的"思维陷阱"问题：当模型早期做出错误承诺后，即使后续反思也难以修正根错误。TAAR通过训练诊断策略预测陷阱位置和逃脱概率，在推理时截断轨迹并自适应重启解码。


<details>
  <summary>Details</summary>
Motivation: 长思维链（Long-CoT）通过增加测试时计算显著提升推理能力，但扩展生成不能保证正确性：模型在早期做出错误承诺后，可能会继续阐述一个自洽但不正确的前缀。研究发现89%的失败案例存在这种"思维陷阱"。

Method: 提出TAAR（Trap-Aware Adaptive Restart）框架：1）训练诊断策略从部分轨迹中预测两个信号：陷阱位置索引和逃脱概率；2）推理时截断轨迹到预测的陷阱段之前；3）自适应重启解码，对严重陷阱情况应用更强的扰动，包括更高温度重采样和可选的结构化重启后缀。

Result: 在具有挑战性的数学和科学推理基准测试（AIME24、AIME25、GPQA-Diamond、HMMT25、BRUMO25）上，TAAR在不微调基础模型参数的情况下提高了推理性能。

Conclusion: TAAR框架有效解决了长思维链推理中的思维陷阱问题，通过自适应重启机制帮助模型从错误承诺中恢复，显著提升了复杂推理任务的性能。

Abstract: Scaling test-time compute via Long Chain-of-Thought (Long-CoT) significantly enhances reasoning capabilities, yet extended generation does not guarantee correctness: after an early wrong commitment, models may keep elaborating a self-consistent but incorrect prefix. Through fine-grained trajectory analysis, we identify Thinking Traps, prefix-dominant deadlocks where later reflection, alternative attempts, or verification fails to revise the root error. On a curated subset of DAPO-MATH, 89\% of failures exhibit such traps. To solve this problem, we introduce TAAR (Trap-Aware Adaptive Restart), a test-time control framework that trains a diagnostic policy to predict two signals from partial trajectories: a trap index for where to truncate and an escape probability for whether and how strongly to intervene. At inference time, TAAR truncates the trajectory before the predicted trap segment and adaptively restarts decoding; for severely trapped cases, it applies stronger perturbations, including higher-temperature resampling and an optional structured reboot suffix. Experiments on challenging mathematical and scientific reasoning benchmarks (AIME24, AIME25, GPQA-Diamond, HMMT25, BRUMO25) show that TAAR improves reasoning performance without fine-tuning base model parameters.

</details>


### [105] [Process In-Context Learning: Enhancing Mathematical Reasoning via Dynamic Demonstration Insertion](https://arxiv.org/abs/2601.11979)
*Ang Gao,Changshuo Zhang,Xiao Zhang,Deyang Li,Minjun Zhao,Fangchao Liu,Xinyu Zhang*

Main category: cs.AI

TL;DR: PICL提出动态演示集成框架，通过实时识别推理过程中的混淆点并插入相关演示，提升数学推理任务的性能。


<details>
  <summary>Details</summary>
Motivation: 现有上下文学习方法在数学推理等需要逐步逻辑推导的任务中存在局限性，其静态演示无法适应推理过程中动态出现的混淆点（如模糊计算、逻辑漏洞），导致级联错误和最终准确率下降。

Method: PICL采用两阶段框架：1) 通过分析推理过程中的语义和熵来识别潜在混淆点并总结核心特征；2) 在遇到混淆点时，从演示池中检索与混淆上下文匹配的相关演示，并将其直接插入到正在进行的推理过程中以指导后续步骤。

Result: 实验表明PICL优于基线方法，通过缓解推理过程中的混淆点，证明了自适应演示插入在复杂数学推理中的价值。

Conclusion: PICL框架通过动态识别和响应推理过程中的混淆点，有效提升了上下文学习在数学推理任务中的性能，展示了自适应演示集成的重要性。

Abstract: In-context learning (ICL) has proven highly effective across diverse large language model (LLM) tasks. However, its potential for enhancing tasks that demand step-by-step logical deduction, such as mathematical reasoning, remains underexplored. A core limitation of existing ICL approaches is their static use of demonstrations: examples are pre-selected before inference and remain fixed, failing to adapt to the dynamic confusion points that often arise during multi-step reasoning such as ambiguous calculations or logical gaps. These unresolved confusion points can lead to cascading errors that degrade final accuracy. To tackle this issue, we propose Process In-Context Learning (PICL), a dynamic demonstration integration framework designed to boost mathematical reasoning by responding to real-time inference needs. PICL operates in two stages: 1)~it identifies potential confusion points by analyzing semantics and entropy in the reasoning process and summarizes their core characteristics; 2)~upon encountering these points, it retrieves relevant demonstrations from the demonstration pool that match the confusion context and inserts them directly into the ongoing reasoning process to guide subsequent steps. Experiments show that PICL outperforms baseline methods by mitigating mid-inference confusion, highlighting the value of adaptive demonstration insertion in complex mathematical reasoning.

</details>


### [106] [A Multi-Agent System for Generating Actionable Business Advice](https://arxiv.org/abs/2601.12024)
*Kartikey Singh Bhandari,Tanish Jain,Archit Agrawal,Dhruv Kumar,Praveen Kumar,Pratik Narang*

Main category: cs.AI

TL;DR: 提出基于LLM的多智能体框架，将大规模用户评论转化为可执行的商业建议，相比传统方法在可操作性、具体性和非冗余性方面表现更优


<details>
  <summary>Details</summary>
Motivation: 现有分析方法（如情感分析、方面提取）停留在描述性任务层面，无法提供深入洞察；LLM生成的建议虽然自由形式但缺乏准确性和深度推理

Method: 四组件多智能体框架：1)聚类选择代表性评论 2)生成建议 3)迭代评估 4)基于可行性的排序。结合语料蒸馏与反馈驱动的建议精炼

Result: 在三个服务领域和多个模型家族上的实验表明，该框架在可操作性、具体性和非冗余性方面始终优于单模型基线，中等规模模型接近大型模型框架性能

Conclusion: 多智能体LLM框架能够将大规模评论语料转化为具体、可操作且实用的商业建议，为决策支持提供了有效的规范性分析方法

Abstract: Customer reviews contain rich signals about product weaknesses and unmet user needs, yet existing analytic methods rarely move beyond descriptive tasks such as sentiment analysis or aspect extraction. While large language models (LLMs) can generate free-form suggestions, their outputs often lack accuracy and depth of reasoning. In this paper, we present a multi-agent, LLM-based framework for prescriptive decision support, which transforms large scale review corpora into actionable business advice. The framework integrates four components: clustering to select representative reviews, generation of advices, iterative evaluation, and feasibility based ranking. This design couples corpus distillation with feedback driven advice refinement to produce outputs that are specific, actionable, and practical. Experiments across three service domains and multiple model families show that our framework consistently outperform single model baselines on actionability, specificity, and non-redundancy, with medium sized models approaching the performance of large model frameworks.

</details>


### [107] [Abstract Argumentation with Subargument Relations](https://arxiv.org/abs/2601.12038)
*Beishui Liao*

Main category: cs.AI

TL;DR: 本文提出了一种扩展的抽象论证框架，在传统攻击关系基础上增加了明确的子论证关系，以更好地捕捉结构化论证中的依赖关系。


<details>
  <summary>Details</summary>
Motivation: 传统Dung抽象论证框架仅通过攻击关系表征论证可接受性，忽略了论证的内部结构。这种抽象虽然产生了丰富结果，但限制了表示结构化论证形式中核心的结构依赖关系，特别是子论证关系。现有扩展（如双极论证框架）引入了支持关系，但未能捕捉子论证的非对称性和构成性本质，以及它们与攻击的交互作用。

Method: 研究了一种扩展的抽象论证框架，将明确的子论证关系与攻击关系一起作为基本关系。分析了子论证关系如何与攻击关系交互，并考察了它们对基本语义属性的影响。

Result: 该框架为结构信息提供了原则性抽象，并澄清了子论证在抽象可接受性推理中的作用。

Conclusion: 通过将子论证关系作为基本关系引入抽象论证框架，能够更好地捕捉结构化论证中的依赖关系，为抽象可接受性推理提供了更丰富的理论基础。

Abstract: Dung's abstract argumentation framework characterises argument acceptability solely via an attack relation, deliberately abstracting from the internal structure of arguments. While this level of abstraction has enabled a rich body of results, it limits the ability to represent structural dependencies that are central in many structured argumentation formalisms, in particular subargument relations. Existing extensions, including bipolar argumentation frameworks, introduce support relations, but these do not capture the asymmetric and constitutive nature of subarguments or their interaction with attacks. In this paper, we study abstract argumentation frameworks enriched with an explicit subargument relation, treated alongside attack as a basic relation. We analyse how subargument relations interact with attacks and examine their impact on fundamental semantic properties. This framework provides a principled abstraction of structural information and clarifies the role of subarguments in abstract acceptability reasoning.

</details>


### [108] [UniMo: Unified Motion Generation and Understanding with Chain of Thought](https://arxiv.org/abs/2601.12126)
*Guocun Wang,Kenkun Liu,Jing Lin,Guorui Song,Jian Li,Xiaoguang Han*

Main category: cs.AI

TL;DR: UniMo是一个新颖的统一框架，通过监督微调和强化学习，将运动-语言信息和可解释的思维链推理集成到LLM中，显著提升了3D人体运动生成与理解任务的性能。


<details>
  <summary>Details</summary>
Motivation: 现有3D人体运动生成与理解方法可解释性有限，限制了这两个相关任务之间的有效相互增强。基于大语言模型的统一框架虽然利用了语言先验，但在语义对齐和任务一致性方面面临挑战，且LLM的下一令牌预测范式不适合运动序列，会导致累积预测误差。

Method: 提出UniMo框架：1) 通过监督微调将运动-语言信息和可解释的思维链推理集成到LLM中；2) 引入强化学习与组相对策略优化作为后训练策略，通过优化令牌组来强制结构正确性和语义对齐，减轻运动令牌预测中的累积误差。

Result: 大量实验表明，UniMo在运动生成和理解任务上都显著优于现有的统一模型和任务特定模型，实现了最先进的性能。

Conclusion: UniMo通过集成运动-语言信息和可解释的思维链推理，结合强化学习后训练策略，有效解决了现有方法的局限性，为3D人体运动生成与理解提供了一个强大的统一框架。

Abstract: Existing 3D human motion generation and understanding methods often exhibit limited interpretability, restricting effective mutual enhancement between these inherently related tasks. While current unified frameworks based on large language models (LLMs) leverage linguistic priors, they frequently encounter challenges in semantic alignment and task coherence. Moreover, the next-token prediction paradigm in LLMs is ill-suited for motion sequences, causing cumulative prediction errors. To address these limitations, we propose UniMo, a novel framework that integrates motion-language information and interpretable chain of thought (CoT) reasoning into the LLM via supervised fine-tuning (SFT). We further introduce reinforcement learning with Group Relative Policy Optimization (GRPO) as a post-training strategy that optimizes over groups of tokens to enforce structural correctness and semantic alignment, mitigating cumulative errors in motion token prediction. Extensive experiments demonstrate that UniMo significantly outperforms existing unified and task-specific models, achieving state-of-the-art performance in both motion generation and understanding.

</details>


### [109] [DriveSafe: A Hierarchical Risk Taxonomy for Safety-Critical LLM-Based Driving Assistants](https://arxiv.org/abs/2601.12138)
*Abhishek Kumar,Riya Tapwal,Carsten Maple*

Main category: cs.AI

TL;DR: DriveSafe是一个针对车载LLM助手的四级风险分类法，包含129个细粒度风险类别，涵盖技术、法律、社会和伦理维度，基于真实驾驶法规构建，专家评审验证。


<details>
  <summary>Details</summary>
Motivation: LLM越来越多地集成到车载数字助手中，但不安全、模糊或法律错误的响应可能导致严重的安全、伦理和监管后果。现有的安全分类和评估框架大多是通用型的，无法捕捉真实驾驶场景中的领域特定风险。

Method: 提出了DriveSafe，一个分层的四级风险分类法，包含129个细粒度原子风险类别，涵盖技术、法律、社会和伦理维度，基于真实驾驶法规和安全原则构建，并由领域专家评审。通过评估六个广泛部署的LLM对这些提示的拒绝行为来验证安全相关性和真实性。

Result: 评估显示，被测试的模型经常无法适当拒绝不安全或不合规的驾驶相关查询，突显了通用安全对齐在驾驶上下文中的局限性。

Conclusion: 需要领域特定的安全评估框架来确保车载LLM助手的安全性，通用安全对齐方法在驾驶场景中效果有限。

Abstract: Large Language Models (LLMs) are increasingly integrated into vehicle-based digital assistants, where unsafe, ambiguous, or legally incorrect responses can lead to serious safety, ethical, and regulatory consequences. Despite growing interest in LLM safety, existing taxonomies and evaluation frameworks remain largely general-purpose and fail to capture the domain-specific risks inherent to real-world driving scenarios. In this paper, we introduce DriveSafe, a hierarchical, four-level risk taxonomy designed to systematically characterize safety-critical failure modes of LLM-based driving assistants. The taxonomy comprises 129 fine-grained atomic risk categories spanning technical, legal, societal, and ethical dimensions, grounded in real-world driving regulations and safety principles and reviewed by domain experts. To validate the safety relevance and realism of the constructed prompts, we evaluate their refusal behavior across six widely deployed LLMs. Our analysis shows that the evaluated models often fail to appropriately refuse unsafe or non-compliant driving-related queries, underscoring the limitations of general-purpose safety alignment in driving contexts.

</details>


### [110] [TIDE: A Trace-Informed Depth-First Exploration for Planning with Temporally Extended Goals](https://arxiv.org/abs/2601.12141)
*Yuliia Suprun,Khen Elimelech,Lydia E. Kavraki,Moshe Y. Vardi*

Main category: cs.AI

TL;DR: TIDE是一种新颖的任务规划方法，专门处理具有时间扩展目标的问题，通过将时间规划问题分解为可管理的子问题，并使用成本驱动的启发式引导搜索，显著提高了规划效率。


<details>
  <summary>Details</summary>
Motivation: 传统基于LTLf的时间任务规划方法通常将时间规划问题转化为经典规划问题，但缺乏有效的启发式来引导对时间目标的搜索，导致规划效率低下。

Method: TIDE方法将时间问题分解为一系列较小的可达-避免子问题，每个子问题都可用现成规划器求解。它识别并优先处理领域图中最有希望的自动机轨迹，使用成本驱动的启发式引导探索，并采用自适应回溯机制从失败计划中恢复。

Result: 实验结果表明，TIDE实现了有前景的性能表现，是处理时间扩展目标的规划方法组合中的一个有价值的补充。

Conclusion: TIDE通过分解时间规划问题、使用启发式引导搜索和自适应回溯机制，有效解决了传统LTLf任务规划方法缺乏引导搜索的局限性，为处理复杂时间目标提供了高效解决方案。

Abstract: Task planning with temporally extended goals (TEGs) is a critical challenge in AI and robotics, enabling agents to achieve complex sequences of objectives over time rather than addressing isolated, immediate tasks. Linear Temporal Logic on finite traces (LTLf ) provides a robust formalism for encoding these temporal goals. Traditional LTLf task planning approaches often transform the temporal planning problem into a classical planning problem with reachability goals, which are then solved using off-the-shelf planners. However, these methods often lack informed heuristics to provide a guided search for temporal goals. We introduce TIDE (Trace-Informed Depth-first Exploration), a novel approach that addresses this limitation by decomposing a temporal problem into a sequence of smaller, manageable reach-avoid sub-problems, each solvable using an off-the-shelf planner. TIDE identifies and prioritizes promising automaton traces within the domain graph, using cost-driven heuristics to guide exploration. Its adaptive backtracking mechanism systematically recovers from failed plans by recalculating costs and penalizing infeasible transitions, ensuring completeness and efficiency. Experimental results demonstrate that TIDE achieves promising performance and is a valuable addition to the portfolio of planning methods for temporally extended goals.

</details>


### [111] [Improving Large Molecular Language Model via Relation-aware Multimodal Collaboration](https://arxiv.org/abs/2601.12256)
*Jinyoung Park,Minseong Bae,Jeehye Na,Hyunwoo J. Kim*

Main category: cs.AI

TL;DR: CoLLaMo是一个基于大语言模型的分子助手，通过多级分子模态协作投影器整合1D序列、2D分子图和3D构象信息，解决了现有大分子语言模型的幻觉问题和有限鲁棒性，在多个分子任务上取得最佳性能。


<details>
  <summary>Details</summary>
Motivation: 现有的大分子语言模型（LMLMs）存在幻觉问题和有限鲁棒性，主要原因是未能充分整合1D序列、2D分子图和3D构象等多种分子模态信息。需要开发能够更好地整合多模态分子信息的模型来提升分子理解和生成能力。

Method: 提出CoLLaMo模型，配备多级分子模态协作投影器，采用关系感知的模态协作注意力机制，促进原子间细粒度和关系引导的信息交换，整合2D结构和3D空间关系。同时提出了新的分子中心自动评估方法，包括幻觉评估指标和基于GPT的标题质量评估。

Result: CoLLaMo增强了LMLMs的分子模态泛化能力，在分子标题生成、计算性质问答、描述性质问答、基序计数和IUPAC名称预测等多个任务上取得了最佳性能。

Conclusion: 通过整合多种分子模态信息并采用关系感知的协作机制，CoLLaMo有效解决了现有大分子语言模型的幻觉和鲁棒性问题，为分子理解和生成任务提供了更强大的工具。

Abstract: Large language models (LLMs) have demonstrated their instruction-following capabilities and achieved powerful performance on various tasks. Inspired by their success, recent works in the molecular domain have led to the development of large molecular language models (LMLMs) that integrate 1D molecular strings or 2D molecular graphs into the language models. However, existing LMLMs often suffer from hallucination and limited robustness, largely due to inadequate integration of diverse molecular modalities such as 1D sequences, 2D molecular graphs, and 3D conformations. To address these limitations, we propose CoLLaMo, a large language model-based molecular assistant equipped with a multi-level molecular modality-collaborative projector. The relation-aware modality-collaborative attention mechanism in the projector facilitates fine-grained and relation-guided information exchange between atoms by incorporating 2D structural and 3D spatial relations. Furthermore, we present a molecule-centric new automatic measurement, including a hallucination assessment metric and GPT-based caption quality evaluation to address the limitations of token-based generic evaluation metrics (i.e., BLEU) widely used in assessing molecular comprehension of LMLMs. Our extensive experiments demonstrate that our CoLLaMo enhances the molecular modality generalization capabilities of LMLMs, achieving the best performance on multiple tasks, including molecule captioning, computed property QA, descriptive property QA, motif counting, and IUPAC name prediction.

</details>


### [112] [FutureX-Pro: Extending Future Prediction to High-Value Vertical Domains](https://arxiv.org/abs/2601.12259)
*Jiashuo Liu,Siyuan Chen,Zaiyuan Wang,Zhiyuan Zeng,Jiacheng Guo,Liang Hu,Lingyue Yin,Suozhi Huang,Wenxin Hao,Yang Yang,Zerui Cheng,Zixin Yao,Lingyue Yin,Haoxin Liu,Jiayi Cheng,Yuzhen Li,Zezhong Ma,Bingjie Wang,Bingsen Qiu,Xiao Liu,Zeyang Zhang,Zijian Liu,Jinpeng Wang,Mingren Yin,Tianci He,Yali Liao,Yixiao Tian,Zhenwei Zhu,Anqi Dai,Ge Zhang,Jingkai Liu,Kaiyuan Zhang,Wenlong Wu,Xiang Gao,Xinjie Chen,Zhixin Yao,Zhoufutu Wen,B. Aditya Prakash,Jose Blanchet,Mengdi Wang,Nian Si,Wenhao Huang*

Main category: cs.AI

TL;DR: FutureX-Pro扩展了通用未来预测基准FutureX，专注于金融、零售、公共卫生和自然灾害四个高价值垂直领域的专业化预测任务，评估当前SOTA智能体LLMs在工业部署中的领域基础能力。


<details>
  <summary>Details</summary>
Motivation: 虽然通用智能体在开放领域搜索中表现出色，但在资本密集型和安全关键领域的可靠性尚未充分探索。需要评估智能体LLMs是否具备工业部署所需的领域基础能力。

Method: 基于FutureX的无污染实时评估流程，构建了FutureX-Pro框架，包括金融、零售、公共卫生和自然灾害四个垂直领域的预测任务，涵盖市场指标预测、供应链需求预测、流行病趋势跟踪和自然灾害监测等基础预测任务。

Result: 研究发现当前最先进的智能体LLMs在通用推理能力与高价值垂直应用所需精度之间存在性能差距，揭示了这些模型在工业部署中的局限性。

Conclusion: 智能体LLMs在专业垂直领域的预测能力仍需提升，需要更强的领域基础才能满足资本密集型和安全关键行业的工业部署要求。

Abstract: Building upon FutureX, which established a live benchmark for general-purpose future prediction, this report introduces FutureX-Pro, including FutureX-Finance, FutureX-Retail, FutureX-PublicHealth, FutureX-NaturalDisaster, and FutureX-Search. These together form a specialized framework extending agentic future prediction to high-value vertical domains. While generalist agents demonstrate proficiency in open-domain search, their reliability in capital-intensive and safety-critical sectors remains under-explored. FutureX-Pro targets four economically and socially pivotal verticals: Finance, Retail, Public Health, and Natural Disaster. We benchmark agentic Large Language Models (LLMs) on entry-level yet foundational prediction tasks -- ranging from forecasting market indicators and supply chain demands to tracking epidemic trends and natural disasters. By adapting the contamination-free, live-evaluation pipeline of FutureX, we assess whether current State-of-the-Art (SOTA) agentic LLMs possess the domain grounding necessary for industrial deployment. Our findings reveal the performance gap between generalist reasoning and the precision required for high-value vertical applications.

</details>


### [113] [ToolPRMBench: Evaluating and Advancing Process Reward Models for Tool-using Agents](https://arxiv.org/abs/2601.12294)
*Dawei Li,Yuguang Yao,Zhen Tan,Huan Liu,Ruocheng Guo*

Main category: cs.AI

TL;DR: 提出了ToolPRMBench，一个专门评估工具使用智能体中过程奖励模型（PRMs）的大规模基准测试，通过离线/在线采样和多LLM验证来系统评估PRM性能


<details>
  <summary>Details</summary>
Motivation: 虽然奖励引导的搜索方法在工具使用智能体中表现出潜力，但缺乏系统可靠的PRM评估基准，特别是在工具使用场景中

Method: 基于多个代表性工具使用基准构建ToolPRMBench，将智能体轨迹转换为步骤级测试用例，包含交互历史、正确动作、合理但错误的替代动作和工具元数据；采用离线采样隔离单步错误和在线采样捕获多步失败；提出多LLM验证管道减少标签噪声

Result: 在大语言模型、通用PRM和工具专用PRM上进行了广泛实验，结果显示PRM有效性存在明显差异，并突显了工具专用PRM的潜力

Conclusion: ToolPRMBench为评估工具使用场景中的PRM提供了系统基准，揭示了PRM性能差异并展示了工具专用PRM的优势，有助于推动工具使用智能体的发展

Abstract: Reward-guided search methods have demonstrated strong potential in enhancing tool-using agents by effectively guiding sampling and exploration over complex action spaces. As a core design, those search methods utilize process reward models (PRMs) to provide step-level rewards, enabling more fine-grained monitoring. However, there is a lack of systematic and reliable evaluation benchmarks for PRMs in tool-using settings. In this paper, we introduce ToolPRMBench, a large-scale benchmark specifically designed to evaluate PRMs for tool-using agents. ToolPRMBench is built on top of several representative tool-using benchmarks and converts agent trajectories into step-level test cases. Each case contains the interaction history, a correct action, a plausible but incorrect alternative, and relevant tool metadata. We respectively utilize offline sampling to isolate local single-step errors and online sampling to capture realistic multi-step failures from full agent rollouts. A multi-LLM verification pipeline is proposed to reduce label noise and ensure data quality. We conduct extensive experiments across large language models, general PRMs, and tool-specialized PRMs on ToolPRMBench. The results reveal clear differences in PRM effectiveness and highlight the potential of specialized PRMs for tool-using. Code and data will be released at https://github.com/David-Li0406/ToolPRMBench.

</details>


### [114] [Survival is the Only Reward: Sustainable Self-Training Through Environment-Mediated Selection](https://arxiv.org/abs/2601.12310)
*Jennifer Dodgson,Alfath Daryl Alhajir,Michael Joedhitya,Akira Rafhael Janson Pattirane,Surender Suresh Kumar,Joseph Lim,C. H. Peh,Adith Ramdas,Steven Zhang Zhexu*

Main category: cs.AI

TL;DR: 本文提出了一种基于环境存活性而非奖励函数的自训练架构，通过环境中的行为生存选择实现稳定学习，避免奖励黑客和语义漂移问题。


<details>
  <summary>Details</summary>
Motivation: 传统自训练系统因缺乏外部数据质量判断标准而容易退化，导致奖励黑客和语义漂移问题。本文旨在探索在稀疏外部反馈和有限内存条件下实现稳定自训练的系统架构。

Method: 引入基于环境存活性而非奖励或目标函数的自训练架构。候选行为在真实资源约束下执行，只有那些环境效应持久且能保持未来交互可能性的行为被传播。环境不提供语义反馈、密集奖励或任务特定监督，选择仅通过行为作为世界改变事件的差异生存进行。

Result: 分析表明，改进主要通过有效且可重复策略在整合和修剪机制下的持久性实现（负空间学习范式）。模型在没有明确指导的情况下发展出元学习策略（如故意实验失败以获取信息性错误消息）。

Conclusion: 环境基础选择能够实现可持续的开放式自我改进，为构建更鲁棒和可泛化的自主系统提供了可行路径，无需依赖人类策划数据或复杂奖励塑造。

Abstract: Self-training systems often degenerate due to the lack of an external criterion for judging data quality, leading to reward hacking and semantic drift. This paper provides a proof-of-concept system architecture for stable self-training under sparse external feedback and bounded memory, and empirically characterises its learning dynamics and failure modes.
  We introduce a self-training architecture in which learning is mediated exclusively by environmental viability, rather than by reward, objective functions, or externally defined fitness criteria. Candidate behaviours are executed under real resource constraints, and only those whose environmental effects both persist and preserve the possibility of future interaction are propagated. The environment does not provide semantic feedback, dense rewards, or task-specific supervision; selection operates solely through differential survival of behaviours as world-altering events, making proxy optimisation impossible and rendering reward-hacking evolutionarily unstable.
  Analysis of semantic dynamics shows that improvement arises primarily through the persistence of effective and repeatable strategies under a regime of consolidation and pruning, a paradigm we refer to as negative-space learning (NSL), and that models develop meta-learning strategies (such as deliberate experimental failure in order to elicit informative error messages) without explicit instruction. This work establishes that environment-grounded selection enables sustainable open-ended self-improvement, offering a viable path toward more robust and generalisable autonomous systems without reliance on human-curated data or complex reward shaping.

</details>


### [115] [Beyond Human Annotation: Recent Advances in Data Generation Methods for Document Intelligence](https://arxiv.org/abs/2601.12318)
*Dehao Ying,Fengchang Yu,Haihua Chen,Changjiang Jiang,Yurong Li,Wei Lu*

Main category: cs.AI

TL;DR: 这篇综述为文档智能（DI）领域建立了首个全面的数据生成技术图谱，将数据生成重新定义为监督信号生产，并基于"数据和标签的可用性"提出了新的分类法，将方法组织为四个资源中心范式。


<details>
  <summary>Details</summary>
Motivation: 文档智能的发展需要大规模高质量训练数据，但人工标注成为关键瓶颈。现有调查局限于单一模态或特定任务，缺乏与现实工作流程统一视角，需要建立系统化的数据生成技术框架。

Method: 将数据生成重新定义为监督信号生产，基于"数据和标签的可用性"引入新的分类法，将方法组织为四个资源中心范式：数据增强、从零开始数据生成、自动化数据标注和自监督信号构建。

Result: 建立了多层次的评估框架，整合内在质量和外在效用，汇编了跨多种DI基准的性能增益。通过统一结构揭示了关键挑战（如保真度差距）和前沿方向（如协同进化生态系统）。

Conclusion: 通过系统化这一碎片化领域，将数据生成定位为下一代文档智能的核心引擎，为DI领域提供了统一的技术路线图和发展方向。

Abstract: The advancement of Document Intelligence (DI) demands large-scale, high-quality training data, yet manual annotation remains a critical bottleneck. While data generation methods are evolving rapidly, existing surveys are constrained by fragmented focuses on single modalities or specific tasks, lacking a unified perspective aligned with real-world workflows. To fill this gap, this survey establishes the first comprehensive technical map for data generation in DI. Data generation is redefined as supervisory signal production, and a novel taxonomy is introduced based on the "availability of data and labels." This framework organizes methodologies into four resource-centric paradigms: Data Augmentation, Data Generation from Scratch, Automated Data Annotation, and Self-Supervised Signal Construction. Furthermore, a multi-level evaluation framework is established to integrate intrinsic quality and extrinsic utility, compiling performance gains across diverse DI benchmarks. Guided by this unified structure, the methodological landscape is dissected to reveal critical challenges such as fidelity gaps and frontiers including co-evolutionary ecosystems. Ultimately, by systematizing this fragmented field, data generation is positioned as the central engine for next-generation DI.

</details>


### [116] [MARO: Learning Stronger Reasoning from Social Interaction](https://arxiv.org/abs/2601.12323)
*Yin Cai,Zhouhong Gu,Juntao Zhang,Ping Chen*

Main category: cs.AI

TL;DR: MARO是一种通过多智能体社交环境训练大语言模型的方法，通过分解稀疏奖励信号、平衡角色权重和直接评估行为效用，显著提升模型的社交推理能力，并能将所学能力迁移到数学推理等任务。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型训练方法主要让模型从现有文本内容学习或解决预定问题，缺乏在真实社交场景中与他人互动、协商和竞争的经验，这限制了模型在需要复杂社交推理的场景中的表现。

Method: 提出多智能体奖励优化（MARO）方法：1）将最终成功或失败结果分解为交互过程中的具体行为，解决稀疏学习信号问题；2）通过平衡不同角色的训练样本权重，处理角色分布不均问题；3）通过直接评估每个行为的效用，解决环境不稳定问题。

Result: 实验结果表明，MARO不仅在社交推理能力上取得显著提升，而且通过社交模拟学习获得的能力能有效迁移到其他任务，如数学推理和指令遵循，展示了多智能体社交学习在增强LLM通用推理能力方面的巨大潜力。

Conclusion: 多智能体社交学习是增强大语言模型通用推理能力的有效途径，MARO方法通过解决稀疏奖励、角色分布和环境不稳定等问题，使模型能在复杂社交环境中学习并获得可迁移的推理能力。

Abstract: Humans face countless scenarios that require reasoning and judgment in daily life. However, existing large language model training methods primarily allow models to learn from existing textual content or solve predetermined problems, lacking experience in real scenarios involving interaction, negotiation, and competition with others. To address this, this paper proposes Multi-Agent Reward Optimization (MARO), a method that enables large language models (LLMs) to acquire stronger reasoning abilities by learning and practicing in multi-agent social environments. Specifically, MARO first addresses the sparse learning signal problem by decomposing final success or failure outcomes into each specific behavior during the interaction process; second, it handles the uneven role distribution problem by balancing the training sample weights of different roles; finally, it addresses environmental instability issues by directly evaluating the utility of each behavior. Experimental results demonstrate that MARO not only achieves significant improvements in social reasoning capabilities, but also that the abilities acquired through social simulation learning can effectively transfer to other tasks such as mathematical reasoning and instruction following. This reveals the tremendous potential of multi-agent social learning in enhancing the general reasoning capabilities of LLMs.

</details>


### [117] [Actionable Advice from Reviews via Mixture of LoRA Experts: A Two-LLM Pipeline for Issue Extraction and Business Recommendations](https://arxiv.org/abs/2601.12338)
*Kartikey Singh Bhandari,Manav Ganesh,Yashwant Viswanathan,Archit Agrawal,Dhruv Kumar,Pratik Narang*

Main category: cs.AI

TL;DR: 该研究提出一个两阶段LLM框架，将客户评论转化为可操作建议，通过问题提取和专家混合适配器实现专业化建议生成。


<details>
  <summary>Details</summary>
Motivation: 客户评论包含丰富的服务失败和用户期望信号，但将这些非结构化反馈转化为可执行的业务决策仍然困难。研究旨在解决评论到行动生成的问题。

Method: 提出模块化两阶段LLM框架：问题模型提取关键问题并分配主题，建议模型基于问题表示生成针对性操作方案。采用LoRA专家混合策略，训练多个低秩适配器并通过轻量门控机制在推理时进行专家组合。

Result: 在航空和餐饮领域的Yelp评论数据集上评估，该方法在八个操作维度（可操作性、特异性、可行性、预期影响、新颖性、非冗余性、偏见、清晰度）上持续优于仅提示和单适配器基线，提供更高的可操作性和特异性。

Conclusion: 提出的两阶段框架结合专家混合适配器策略，能够有效将客户评论转化为具体可操作建议，在效率和质量的权衡中表现优异。

Abstract: Customer reviews contain detailed, domain specific signals about service failures and user expectations, but converting this unstructured feedback into actionable business decisions remains difficult. We study review-to-action generation: producing concrete, implementable recommendations grounded in review text. We propose a modular two-LLM framework in which an Issue model extracts salient issues and assigns coarse themes, and an Advice model generates targeted operational fixes conditioned on the extracted issue representation. To enable specialization without expensive full fine-tuning, we adapt the Advice model using a mixture of LoRA experts strategy: multiple low-rank adapters are trained and a lightweight gating mechanism performs token-level expert mixing at inference, combining complementary expertise across issue types. We construct synthetic review-issue-advice triples from Yelp reviews (airlines and restaurants) to supervise training, and evaluate recommendations using an eight dimension operational rubric spanning actionability, specificity, feasibility, expected impact, novelty, non-redundancy, bias, and clarity. Across both domains, our approach consistently outperforms prompting-only and single-adapter baselines, yielding higher actionability and specificity while retaining favorable efficiency-quality trade-offs.

</details>


### [118] [Are LLMs Smarter Than Chimpanzees? An Evaluation on Perspective Taking and Knowledge State Estimation](https://arxiv.org/abs/2601.12410)
*Dingyi Yang,Junqi Zhao,Xue Li,Ce Li,Boyang Li*

Main category: cs.AI

TL;DR: LLMs在知识状态追踪和意图理解任务上表现接近随机水平，远低于人类表现


<details>
  <summary>Details</summary>
Motivation: 认知人类学认为人类智能的核心在于推断他人知识状态和理解意图的能力，而黑猩猩缺乏这种能力。本研究旨在评估LLM在知识状态追踪和估计方面的表现。

Method: 设计两个任务：1) 检测故事角色是否通过行动表现出本不应拥有的知识；2) 基于角色自身知识（而非客观事实）预测其下一步行动。

Result: 当前最先进的LLM在两个任务上都接近随机表现，显著低于人类水平。

Conclusion: 未来LLM研究应更重视知识估计和意图理解能力的提升，这是人类智能区别于其他动物的关键特征。

Abstract: Cognitive anthropology suggests that the distinction of human intelligence lies in the ability to infer other individuals' knowledge states and understand their intentions. In comparison, our closest animal relative, chimpanzees, lack the capacity to do so. With this paper, we aim to evaluate LLM performance in the area of knowledge state tracking and estimation. We design two tasks to test (1) if LLMs can detect when story characters, through their actions, demonstrate knowledge they should not possess, and (2) if LLMs can predict story characters' next actions based on their own knowledge vs. objective truths they do not know. Results reveal that most current state-of-the-art LLMs achieve near-random performance on both tasks, and are substantially inferior to humans. We argue future LLM research should place more weight on the abilities of knowledge estimation and intention understanding.

</details>


### [119] [Large Language Model for OWL Proofs](https://arxiv.org/abs/2601.12444)
*Hui Yang,Jiaoyan Chen,Uli Sattler*

Main category: cs.AI

TL;DR: 该研究探索大语言模型在OWL本体论中生成逻辑证明的能力，开发了自动化数据集构建和评估框架，发现模型在复杂情况下仍有局限，逻辑复杂性是影响性能的主要因素，输入数据的不完整性会显著降低模型表现。


<details>
  <summary>Details</summary>
Motivation: 虽然大语言模型在推理任务上的能力已被广泛研究，但它们在生成忠实、可读的逻辑证明方面的能力仍然未被充分探索。本研究旨在探索LLMs在OWL本体论背景下生成逻辑证明的能力，OWL本体论被广泛用于表示和推理复杂知识。

Method: 开发了一个自动化数据集构建和评估框架，评估包含三个顺序任务：提取、简化和解释，以及一个额外的评估前提逻辑完整性的任务。在广泛使用的大语言模型上进行大量实验。

Result: 研究发现：(1) 一些模型整体表现良好，但在复杂情况下仍有局限；(2) 逻辑复杂性（而非表示格式）是影响LLM性能的主要因素；(3) 输入数据中的噪声和不完整性会显著降低LLMs的性能。

Conclusion: 这些结果强调了LLMs在严格逻辑解释方面的潜力，同时也揭示了在复杂或不完美条件下支持弹性推理的差距。代码和数据已开源。

Abstract: The ability of Large Language Models (LLMs) to perform reasoning tasks such as deduction has been widely investigated in recent years. Yet, their capacity to generate proofs-faithful, human-readable explanations of why conclusions follow-remains largely under explored. In this work, we study proof generation in the context of OWL ontologies, which are widely adopted for representing and reasoning over complex knowledge, by developing an automated dataset construction and evaluation framework. Our evaluation encompassing three sequential tasks for complete proving: Extraction, Simplification, and Explanation, as well as an additional task of assessing Logic Completeness of the premise. Through extensive experiments on widely used reasoning LLMs, we achieve important findings including: (1) Some models achieve overall strong results but remain limited on complex cases; (2) Logical complexity, rather than representation format (formal logic language versus natural language), is the dominant factor shaping LLM performance; and (3) Noise and incompleteness in input data substantially diminish LLMs' performance. Together, these results underscore both the promise of LLMs for explanation with rigorous logics and the gap of supporting resilient reasoning under complex or imperfect conditions. Code and data are available at https://github.com/HuiYang1997/LLMOwlR.

</details>


### [120] [Failure Modes in Multi-Hop QA: The Weakest Link Law and the Recognition Bottleneck](https://arxiv.org/abs/2601.12499)
*Meiru Zhang,Zaiqiao Meng,Nigel Collier*

Main category: cs.AI

TL;DR: 论文提出MFAI方法揭示LLM多跳推理失败源于位置偏见，发现"最弱链接定律"：多跳推理性能取决于最不可见证据的位置表现，而非事实间线性距离。


<details>
  <summary>Details</summary>
Motivation: 尽管LLM具有大规模上下文窗口，但在多跳推理中存在位置偏见问题，导致忽略某些位置的信息。研究者希望厘清这种失败是由于无法定位证据（识别失败）还是无法整合证据（合成失败）。

Method: 引入多焦点注意力指令（MFAI）作为语义探针，通过显式引导注意力到选定位置来分离识别和合成机制。在5个LLM上测试两个多跳QA任务（MuSiQue和NeoQA）。

Result: 发现"最弱链接定律"：多跳推理性能崩溃到最不可见证据的性能水平；失败由绝对位置而非事实间线性距离决定（性能差异<3%）。匹配的MFAI可解决识别瓶颈，在低可见性位置提升准确率达11.5%。"思考"模型能有效定位和整合信息，在嘈杂长上下文设置中匹配黄金基线。

Conclusion: LLM多跳推理失败主要源于位置偏见导致的识别失败而非合成失败；注意力引导具有双重性；系统2推理模型能有效克服位置偏见问题。

Abstract: Despite scaling to massive context windows, Large Language Models (LLMs) struggle with multi-hop reasoning due to inherent position bias, which causes them to overlook information at certain positions. Whether these failures stem from an inability to locate evidence (recognition failure) or integrate it (synthesis failure) is unclear. We introduce Multi-Focus Attention Instruction (MFAI), a semantic probe to disentangle these mechanisms by explicitly steering attention towards selected positions. Across 5 LLMs on two multi-hop QA tasks (MuSiQue and NeoQA), we establish the "Weakest Link Law": multi-hop reasoning performance collapses to the performance level of the least visible evidence. Crucially, this failure is governed by absolute position rather than the linear distance between facts (performance variance $<3%$). We further identify a duality in attention steering: while matched MFAI resolves recognition bottlenecks, improving accuracy by up to 11.5% in low-visibility positions, misleading MFAI triggers confusion in real-world tasks but is successfully filtered in synthetic tasks. Finally, we demonstrate that "thinking" models that utilize System-2 reasoning, effectively locate and integrate the required information, matching gold-only baselines even in noisy, long-context settings.

</details>


### [121] [Rethinking the AI Scientist: Interactive Multi-Agent Workflows for Scientific Discovery](https://arxiv.org/abs/2601.12542)
*Lukas Weidener,Marko Brkić,Mihailo Jovanović,Ritvik Singh,Chiara Baccin,Emre Ulgac,Alex Dobrin,Aakaash Meduri*

Main category: cs.AI

TL;DR: Deep Research是一个多智能体系统，能够在几分钟内完成交互式科学研究，相比现有批处理系统显著提升效率，在BixBench基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有AI科学发现系统大多是专有的批处理模式，每个研究周期需要数小时，无法实现研究者的实时指导，限制了AI在科学研究中的实际应用。

Method: 采用多智能体架构，包含规划、数据分析、文献搜索和新颖性检测等专门智能体，通过持久世界状态维持跨迭代研究周期的上下文，支持半自主（带人工检查点）和全自主两种工作模式。

Result: 在BixBench计算生物学基准测试中取得最先进性能：开放回答准确率48.8%，多项选择准确率64.5%，比现有基线提升14-26个百分点。

Conclusion: Deep Research系统实现了分钟级的交互式科学研究，显著提升了AI辅助科学工作流程的效率，同时分析了开放获取文献限制和自动新颖性评估等实际部署挑战。

Abstract: Artificial intelligence systems for scientific discovery have demonstrated remarkable potential, yet existing approaches remain largely proprietary and operate in batch-processing modes requiring hours per research cycle, precluding real-time researcher guidance. This paper introduces Deep Research, a multi-agent system enabling interactive scientific investigation with turnaround times measured in minutes. The architecture comprises specialized agents for planning, data analysis, literature search, and novelty detection, unified through a persistent world state that maintains context across iterative research cycles. Two operational modes support different workflows: semi-autonomous mode with selective human checkpoints, and fully autonomous mode for extended investigations. Evaluation on the BixBench computational biology benchmark demonstrated state-of-the-art performance, achieving 48.8% accuracy on open response and 64.5% on multiple-choice evaluation, exceeding existing baselines by 14 to 26 percentage points. Analysis of architectural constraints, including open access literature limitations and challenges inherent to automated novelty assessment, informs practical deployment considerations for AI-assisted scientific workflows.

</details>


### [122] [How Clinicians Think and What AI Can Learn From It](https://arxiv.org/abs/2601.12547)
*Dipayan Sengupta,Saumya Panda*

Main category: cs.AI

TL;DR: 该论文主张临床AI应从预测引擎转向顺序控制问题，采用稳健的序数决策规则而非基数优化，以更好模拟临床推理本质。


<details>
  <summary>Details</summary>
Motivation: 当前临床AI系统主要作为预测引擎（生成标签或风险评分），但真实的临床推理是时间受限、顺序控制的不确定性问题。临床医生在信息收集与不可逆行动之间交替，受遗憾、约束和患者价值观指导。需要开发更符合临床推理本质的AI方法。

Method: 提出临床推理的计算基础应是序数、非补偿性决策而非基数优化。论证快速节俭的词典式启发法（如快速节俭树）在医学中的规范性合理性。提出临床对齐AI蓝图：使用丰富模型进行信念和轨迹建模，但通过稳健序数规则选择行动；将启发法视为低维特例；将AI部署为"选择性复杂性"——主要在决策脆弱且信息具有正期望影响时用于打破平局。

Result: 论证了序数决策规则在临床环境中的优越性：1）许多临床权衡通过人类判断构建，仅在绝对尺度上弱可测量；2）偏好和信号获取结构粗糙，存在持续不确定性；3）当这种"粗糙性"超过决策边界时，期望效用优化变得脆弱，而稳健优势/过滤规则能稳定决策。

Conclusion: 临床AI应采用稳健序数决策框架，将AI作为"选择性复杂性"工具，在决策脆弱时介入。这种范式转变能更好模拟临床推理本质，提高决策稳定性，实现临床对齐的人工智能系统。

Abstract: Most clinical AI systems operate as prediction engines -- producing labels or risk scores -- yet real clinical reasoning is a time-bounded, sequential control problem under uncertainty. Clinicians interleave information gathering with irreversible actions, guided by regret, constraints and patient values. We argue that the dominant computational substrate of clinician reasoning is not cardinal optimization but ordinal, non-compensatory decision-making: Clinicians frequently rely on fast-and-frugal, lexicographic heuristics (e.g., fast-and-frugal trees) that stop early after checking a small, fixed sequence of cues. We provide a normative rationale for why such algorithms are not merely bounded rationality shortcuts, but can be epistemically preferred in medicine. First, many clinical trade-offs are constructed through human judgment and are only weakly measurable on absolute scales; without strong measurement axioms, only orderings are invariant, motivating an ordinal-by-default stance. Second, preference and signal elicitation are structurally crude: The mapping from truth $\to$ perception $\to$ inference $\to$ recorded variables introduces layered noise, leaving a persistent uncertainty floor. When this 'crudeness' overwhelms the decision margin, plug-in expected-utility optimization becomes brittle (high flip probability under small perturbations), whereas robust dominance/filtering rules ($ε$-dominance, maximin) stabilize decisions.Finally, we outline a clinician-aligned AI blueprint: Use rich models for beliefs and trajectories, but choose actions through robust ordinal rules; treat heuristics as the low-dimensional special case; and deploy AI as 'selective complexity' -- invoked mainly for tie-breaking when decisions are fragile and information has positive expected impact.

</details>


### [123] [Agentic Artificial Intelligence (AI): Architectures, Taxonomies, and Evaluation of Large Language Model Agents](https://arxiv.org/abs/2601.12560)
*Arunkumar V,Gangadharan G. R.,Rajkumar Buyya*

Main category: cs.AI

TL;DR: 论文提出了一个统一的智能体AI架构分类法，将智能体分解为感知、大脑、规划、行动、工具使用和协作六个组件，并分析了从线性推理到原生推理模型的转变，以及从固定API到开放标准的演进。


<details>
  <summary>Details</summary>
Motivation: 随着AI从仅生成文本的模型转向具有自主感知、推理、规划和行动能力的智能体AI，出现了从简单单循环智能体到分层多智能体系统的各种设计，使得这一领域难以导航。需要统一的分类框架来理解这一快速发展的领域。

Method: 提出一个统一的分类法，将智能体分解为六个核心组件：感知、大脑、规划、行动、工具使用和协作。使用这个框架分析智能体架构的演进，包括从线性推理过程到原生推理时间推理模型的转变，以及从固定API调用到开放标准（如模型上下文协议和原生计算机使用）的过渡。同时分类智能体运行的环境，并回顾当前评估实践。

Result: 建立了一个系统化的智能体AI架构分类框架，能够清晰地描述和分析各种智能体设计。识别了智能体发展的关键趋势：从被动知识引擎到主动认知控制器的转变，从线性推理到原生推理模型的演进，以及从封闭API到开放标准的过渡。同时分类了智能体运行环境（数字操作系统、具身机器人等）并总结了当前评估方法。

Conclusion: 论文为理解智能体AI的复杂生态系统提供了一个统一的分类框架，揭示了该领域从简单文本生成到复杂自主系统的演进路径。同时指出了当前面临的挑战（如行动幻觉、无限循环、提示注入等），并为构建更鲁棒可靠的自主系统指明了未来研究方向。

Abstract: Artificial Intelligence is moving from models that only generate text to Agentic AI, where systems behave as autonomous entities that can perceive, reason, plan, and act. Large Language Models (LLMs) are no longer used only as passive knowledge engines but as cognitive controllers that combine memory, tool use, and feedback from their environment to pursue extended goals. This shift already supports the automation of complex workflows in software engineering, scientific discovery, and web navigation, yet the variety of emerging designs, from simple single loop agents to hierarchical multi agent systems, makes the landscape hard to navigate. In this paper, we investigate architectures and propose a unified taxonomy that breaks agents into Perception, Brain, Planning, Action, Tool Use, and Collaboration. We use this lens to describe the move from linear reasoning procedures to native inference time reasoning models, and the transition from fixed API calls to open standards like the Model Context Protocol (MCP) and Native Computer Use. We also group the environments in which these agents operate, including digital operating systems, embodied robotics, and other specialized domains, and we review current evaluation practices. Finally, we highlight open challenges, such as hallucination in action, infinite loops, and prompt injection, and outline future research directions toward more robust and reliable autonomous systems.

</details>


### [124] [STEP-LLM: Generating CAD STEP Models from Natural Language with Large Language Models](https://arxiv.org/abs/2601.12641)
*Xiangyu Shi,Junyang Ding,Xu Zhao,Sinong Zhan,Payal Mohapatra,Daniel Quispe,Kojo Welbeck,Jian Cao,Wei Chen,Ping Guo,Qi Zhu*

Main category: cs.AI

TL;DR: 该论文提出了STEP-LLM，一个从自然语言生成STEP格式CAD模型的框架，通过DFS重序列化、检索增强生成和强化学习等技术，显著提升了几何保真度。


<details>
  <summary>Details</summary>
Motivation: CAD模型创建需要专业知识且耗时，现有文本到CAD方法使用命令序列或脚本格式，但这些格式依赖于特定内核且缺乏制造通用性。STEP文件作为广泛采用的中性边界表示格式直接兼容制造，但其图结构特性对自回归LLM构成挑战。

Method: 1) 构建约40K STEP-描述对数据集；2) 针对STEP图结构格式的预处理，包括基于深度优先搜索的重序列化以线性化交叉引用；3) 链式思维风格的结构注释指导全局一致性；4) 集成检索增强生成进行监督微调；5) 通过基于Chamfer距离的几何奖励进行强化学习优化生成质量。

Result: STEP-LLM在几何保真度上持续优于Text2CAD基线。RAG模块显著提升完整性和可渲染性，DFS重序列化增强整体准确性，RL进一步减少几何差异。指标和视觉比较都证实STEP-LLM生成形状的保真度更高。

Conclusion: 该研究证明了LLM驱动从自然语言生成STEP模型的可行性，展示了其在制造领域民主化CAD设计的潜力。

Abstract: Computer-aided design (CAD) is vital to modern manufacturing, yet model creation remains labor-intensive and expertise-heavy. To enable non-experts to translate intuitive design intent into manufacturable artifacts, recent large language models-based text-to-CAD efforts focus on command sequences or script-based formats like CadQuery. However, these formats are kernel-dependent and lack universality for manufacturing. In contrast, the Standard for the Exchange of Product Data (STEP, ISO 10303) file is a widely adopted, neutral boundary representation (B-rep) format directly compatible with manufacturing, but its graph-structured, cross-referenced nature poses unique challenges for auto-regressive LLMs. To address this, we curate a dataset of ~40K STEP-caption pairs and introduce novel preprocessing tailored for the graph-structured format of STEP, including a depth-first search-based reserialization that linearizes cross-references while preserving locality and chain-of-thought(CoT)-style structural annotations that guide global coherence. We integrate retrieval-augmented generation to ground predictions in relevant examples for supervised fine-tuning, and refine generation quality through reinforcement learning with a specific Chamfer Distance-based geometric reward. Experiments demonstrate consistent gains of our STEP-LLM in geometric fidelity over the Text2CAD baseline, with improvements arising from multiple stages of our framework: the RAG module substantially enhances completeness and renderability, the DFS-based reserialization strengthens overall accuracy, and the RL further reduces geometric discrepancy. Both metrics and visual comparisons confirm that STEP-LLM generates shapes with higher fidelity than Text2CAD. These results show the feasibility of LLM-driven STEP model generation from natural language, showing its potential to democratize CAD design for manufacturing.

</details>


### [125] [Teaching Large Reasoning Models Effective Reflection](https://arxiv.org/abs/2601.12720)
*Hanbin Wang,Jingwei Song,Jinpeng Li,Qi Zhu,Fei Mi,Ganqu Cui,Yasheng Wang,Lifeng Shang*

Main category: cs.AI

TL;DR: 提出SCFT和RLERR方法解决大型推理模型的表面反思问题，通过自我批判微调和强化学习提升反思质量与推理准确性


<details>
  <summary>Details</summary>
Motivation: 大型推理模型在复杂推理任务中表现出色，但许多反思是表面的，对原始答案改进有限且带来计算开销，需要解决表面反思问题

Method: 1. SCFT：自我批判微调框架，让模型批判自身输出，通过拒绝采样筛选高质量批判，使用批判目标微调模型；2. RLERR：在SCFT基础上，利用高质量反思构建奖励信号，通过强化学习内化自我修正过程

Result: 在AIME2024和AIME2025基准测试中，SCFT和RLERR显著提高了推理准确性和反思质量，优于最先进的基线方法

Conclusion: 提出的SCFT和RLERR方法有效解决了大型推理模型的表面反思问题，通过增强模型的反思推理能力，实现了更好的推理性能

Abstract: Large Reasoning Models (LRMs) have recently shown impressive performance on complex reasoning tasks, often by engaging in self-reflective behaviors such as self-critique and backtracking. However, not all reflections are beneficial-many are superficial, offering little to no improvement over the original answer and incurring computation overhead. In this paper, we identify and address the problem of superficial reflection in LRMs. We first propose Self-Critique Fine-Tuning (SCFT), a training framework that enhances the model's reflective reasoning ability using only self-generated critiques. SCFT prompts models to critique their own outputs, filters high-quality critiques through rejection sampling, and fine-tunes the model using a critique-based objective. Building on this strong foundation, we further introduce Reinforcement Learning with Effective Reflection Rewards (RLERR). RLERR leverages the high-quality reflections initialized by SCFT to construct reward signals, guiding the model to internalize the self-correction process via reinforcement learning. Experiments on two challenging benchmarks, AIME2024 and AIME2025, show that SCFT and RLERR significantly improve both reasoning accuracy and reflection quality, outperforming state-of-the-art baselines. All data and codes are available at https://github.com/wanghanbinpanda/SCFT.

</details>


### [126] [VIRO: Robust and Efficient Neuro-Symbolic Reasoning with Verification for Referring Expression Comprehension](https://arxiv.org/abs/2601.12781)
*Hyejin Park,Junhyuk Kwon,Suha Kwak,Jungseul Ok*

Main category: cs.AI

TL;DR: VIRO框架通过嵌入轻量级验证器解决神经符号REC方法中的级联错误问题，在目标存在和不存在场景下实现最先进性能


<details>
  <summary>Details</summary>
Motivation: 现有神经符号REC方法假设中间推理步骤准确，导致级联错误：错误检测和无效关系在推理链中传播，即使图像中没有目标也会产生高置信度的假阳性结果

Method: 提出验证集成推理算子（VIRO）框架，在推理步骤中嵌入轻量级算子级验证器，每个算子执行并验证其输出（如对象存在性或空间关系），当验证条件不满足时能够鲁棒地处理无目标情况

Result: 在目标存在和无目标设置下达到61.1%的平衡准确率，在真实世界第一人称数据上展示泛化能力，具有高计算效率（吞吐量）、高可靠性（程序失败率低于0.3%）和通过解耦程序生成与执行实现的可扩展性

Conclusion: VIRO框架通过集成验证机制有效解决了神经符号REC方法中的级联错误问题，在保持可解释推理和零样本泛化能力的同时，显著提升了系统鲁棒性和可靠性

Abstract: Referring Expression Comprehension (REC) aims to localize the image region corresponding to a natural-language query. Recent neuro-symbolic REC approaches leverage large language models (LLMs) and vision-language models (VLMs) to perform compositional reasoning, decomposing queries 4 structured programs and executing them step-by-step. While such approaches achieve interpretable reasoning and strong zero-shot generalization, they assume that intermediate reasoning steps are accurate. However, this assumption causes cascading errors: false detections and invalid relations propagate through the reasoning chain, yielding high-confidence false positives even when no target is present in the image. To address this limitation, we introduce Verification-Integrated Reasoning Operators (VIRO), a neuro-symbolic framework that embeds lightweight operator-level verifiers within reasoning steps. Each operator executes and validates its output, such as object existence or spatial relationship, thereby allowing the system to robustly handle no-target cases when verification conditions are not met. Our framework achieves state-of-the-art performance, reaching 61.1% balanced accuracy across target-present and no-target settings, and demonstrates generalization to real-world egocentric data. Furthermore, VIRO shows superior computational efficiency in terms of throughput, high reliability with a program failure rate of less than 0.3%, and scalability through decoupled program generation from execution.

</details>


### [127] [SL-CBM: Enhancing Concept Bottleneck Models with Semantic Locality for Better Interpretability](https://arxiv.org/abs/2601.12804)
*Hanwei Zhang,Luo Cheng,Rui Wen,Yang Zhang,Lijun Zhang,Holger Hermanns*

Main category: cs.AI

TL;DR: SL-CBM通过引入语义局部性约束，改进了概念瓶颈模型的空间对齐能力，生成更忠实的概念和类别显著性图，提升了解释质量和干预效果。


<details>
  <summary>Details</summary>
Motivation: 现有概念瓶颈模型（CBMs）存在局部忠实性不足的问题，无法将概念与有意义的图像区域进行空间对齐，这限制了解释性和可靠性。需要一种能够提供空间连贯显著性图的方法来增强概念与图像区域的对应关系。

Method: 提出SL-CBM（具有语义局部性的CBM），通过集成1x1卷积层和交叉注意力机制来增强概念、图像区域和最终预测之间的对齐。使用对比性和基于熵的正则化来平衡准确性、稀疏性和忠实性。

Result: 在图像数据集上的实验表明，SL-CBM显著提高了局部忠实性、解释质量和干预效果，同时保持了有竞争力的分类准确性。消融研究突出了对比性和熵正则化的重要性。

Conclusion: SL-CBM弥合了基于概念的推理与空间可解释性之间的差距，为可解释和可信赖的概念模型设定了新标准，通过空间连贯的显著性图增强了模型的调试和干预能力。

Abstract: Explainable AI (XAI) is crucial for building transparent and trustworthy machine learning systems, especially in high-stakes domains. Concept Bottleneck Models (CBMs) have emerged as a promising ante-hoc approach that provides interpretable, concept-level explanations by explicitly modeling human-understandable concepts. However, existing CBMs often suffer from poor locality faithfulness, failing to spatially align concepts with meaningful image regions, which limits their interpretability and reliability. In this work, we propose SL-CBM (CBM with Semantic Locality), a novel extension that enforces locality faithfulness by generating spatially coherent saliency maps at both concept and class levels. SL-CBM integrates a 1x1 convolutional layer with a cross-attention mechanism to enhance alignment between concepts, image regions, and final predictions. Unlike prior methods, SL-CBM produces faithful saliency maps inherently tied to the model's internal reasoning, facilitating more effective debugging and intervention. Extensive experiments on image datasets demonstrate that SL-CBM substantially improves locality faithfulness, explanation quality, and intervention efficacy while maintaining competitive classification accuracy. Our ablation studies highlight the importance of contrastive and entropy-based regularization for balancing accuracy, sparsity, and faithfulness. Overall, SL-CBM bridges the gap between concept-based reasoning and spatial explainability, setting a new standard for interpretable and trustworthy concept-based models.

</details>


### [128] [Human Emotion Verification by Action Languages via Answer Set Programming](https://arxiv.org/abs/2601.12912)
*Andreas Brännström,Juan Carlos Nieves*

Main category: cs.AI

TL;DR: 本文介绍了基于回答集编程和转移系统构建的动作语言C-MT，用于形式化人类心理状态在可观察动作序列下的演化过程，特别关注情绪等心理状态的动态变化。


<details>
  <summary>Details</summary>
Motivation: 现有方法缺乏对可控智能体行为的支持，且无法限制动作带来的不良心理副作用。需要一种能够形式化心理状态演化、支持心理状态动态建模的语言框架。

Method: 基于回答集编程和转移系统构建C-MT语言，整合心理学理论（如情绪评价理论），将心理状态形式化为多维配置。引入新颖的因果规则"forbids to cause"和专门的心理状态动态表达式，将心理变化原则转化为转移约束和不变性属性。

Result: C-MT语言能够对心理状态的动态演化进行可控推理，支持通过轨迹分析比较不同的心理变化动态，并应用于情绪验证模型的设计。

Conclusion: C-MT为形式化人类心理状态演化提供了一个基于回答集编程的框架，能够建模心理状态动态变化原则，支持可控推理和不同心理学原则的比较分析。

Abstract: In this paper, we introduce the action language C-MT (Mind Transition Language). It is built on top of answer set programming (ASP) and transition systems to represent how human mental states evolve in response to sequences of observable actions. Drawing on well-established psychological theories, such as the Appraisal Theory of Emotion, we formalize mental states, such as emotions, as multi-dimensional configurations. With the objective to address the need for controlled agent behaviors and to restrict unwanted mental side-effects of actions, we extend the language with a novel causal rule, forbids to cause, along with expressions specialized for mental state dynamics, which enables the modeling of principles for valid transitions between mental states. These principles of mental change are translated into transition constraints, and properties of invariance, which are rigorously evaluated using transition systems in terms of so-called trajectories. This enables controlled reasoning about the dynamic evolution of human mental states. Furthermore, the framework supports the comparison of different dynamics of change by analyzing trajectories that adhere to different psychological principles. We apply the action language to design models for emotion verification. Under consideration in Theory and Practice of Logic Programming (TPLP).

</details>


### [129] [Actionable Interpretability Must Be Defined in Terms of Symmetries](https://arxiv.org/abs/2601.12913)
*Pietro Barbiero,Mateo Espinosa Zarlenga,Francesco Giannini,Alberto Termine,Filippo Bonchi,Mateja Jamnik,Giuseppe Marra*

Main category: cs.AI

TL;DR: 论文认为当前AI可解释性研究存在根本性问题，因为现有定义缺乏可操作性，无法推导出具体的建模和推理规则。作者提出基于对称性的定义框架，认为四种对称性足以解决可解释性核心问题。


<details>
  <summary>Details</summary>
Motivation: 当前人工智能可解释性研究存在根本性缺陷，因为现有的可解释性定义缺乏可操作性。这些定义未能提供形式化原则，无法从中推导出具体的建模和推理规则，导致研究难以取得实质性进展。

Method: 提出基于对称性的可解释性定义框架。作者假设四种对称性足以：(1) 激发核心可解释性属性；(2) 刻画可解释模型的类别；(3) 推导统一的可解释推理形式（如对齐、干预和反事实推理），将其视为贝叶斯逆问题。

Result: 论文提出了一个基于对称性的可解释性理论框架，该框架能够为可解释性提供可操作的定义，并统一解释对齐、干预和反事实推理等概念，为可解释AI研究提供形式化基础。

Conclusion: 通过引入对称性概念，可以构建可操作的可解释性定义，解决当前研究中的根本问题，为可解释AI提供统一的理论基础和实践指导。

Abstract: This paper argues that interpretability research in Artificial Intelligence is fundamentally ill-posed as existing definitions of interpretability are not *actionable*: they fail to provide formal principles from which concrete modelling and inferential rules can be derived. We posit that for a definition of interpretability to be actionable, it must be given in terms of *symmetries*. We hypothesise that four symmetries suffice to (i) motivate core interpretability properties, (ii) characterize the class of interpretable models, and (iii) derive a unified formulation of interpretable inference (e.g., alignment, interventions, and counterfactuals) as a form of Bayesian inversion.

</details>


### [130] [Responsible AI for General-Purpose Systems: Overview, Challenges, and A Path Forward](https://arxiv.org/abs/2601.13122)
*Gourab K Patro,Himanshi Agrawal,Himanshu Gharat,Supriya Panigrahi,Nim Sherpa,Vishal Vaddina,Dagnachew Birru*

Main category: cs.AI

TL;DR: 论文分析了现代通用AI系统相比传统任务特定AI在负责任AI原则上面临的更大风险，提出了C2V2（控制、一致性、价值、真实性）框架来指导未来通用AI系统的负责任开发。


<details>
  <summary>Details</summary>
Motivation: 现代通用AI系统（如大型语言和视觉模型）虽然功能强大，但在幻觉、毒性、刻板印象等方面存在风险，使其不可信。作者旨在分析这些风险，并与传统任务特定AI系统进行比较，提出改进通用AI系统负责任性的新框架。

Method: 从八个广泛接受的负责任AI原则（公平性、隐私性、可解释性、鲁棒性、安全性、真实性、治理、可持续性）出发，分析现代通用AI系统的风险和漏洞。提出输出自由度（DoFo）概念来解释风险差异，并推导出C2V2（控制、一致性、价值、真实性）需求框架。

Result: 研究发现通用AI系统由于输出自由度（DoFo）非确定性地高，相比传统任务特定AI系统在负责任AI原则上面临更严重且难以缓解的风险。C2V2框架为评估和改进AI对齐、检索增强生成、推理增强等技术提供了系统化指导。

Conclusion: 开发负责任通用AI需要通过C2V2维度形式化建模应用或领域相关的负责任AI需求，并采用系统设计方法结合各种技术来满足这些需求。这是实现通用AI系统负责任性的可行路径。

Abstract: Modern general-purpose AI systems made using large language and vision models, are capable of performing a range of tasks like writing text articles, generating and debugging codes, querying databases, and translating from one language to another, which has made them quite popular across industries. However, there are risks like hallucinations, toxicity, and stereotypes in their output that make them untrustworthy. We review various risks and vulnerabilities of modern general-purpose AI along eight widely accepted responsible AI (RAI) principles (fairness, privacy, explainability, robustness, safety, truthfulness, governance, and sustainability) and compare how they are non-existent or less severe and easily mitigable in traditional task-specific counterparts. We argue that this is due to the non-deterministically high Degree of Freedom in output (DoFo) of general-purpose AI (unlike the deterministically constant or low DoFo of traditional task-specific AI systems), and there is a need to rethink our approach to RAI for general-purpose AI. Following this, we derive C2V2 (Control, Consistency, Value, Veracity) desiderata to meet the RAI requirements for future general-purpose AI systems, and discuss how recent efforts in AI alignment, retrieval-augmented generation, reasoning enhancements, etc. fare along one or more of the desiderata. We believe that the goal of developing responsible general-purpose AI can be achieved by formally modeling application- or domain-dependent RAI requirements along C2V2 dimensions, and taking a system design approach to suitably combine various techniques to meet the desiderata.

</details>


### [131] [Prompt Injection Mitigation with Agentic AI, Nested Learning, and AI Sustainability via Semantic Caching](https://arxiv.org/abs/2601.13186)
*Diego Gosmar,Deborah A. Dahl*

Main category: cs.AI

TL;DR: 本文扩展了TIVS评估框架，加入语义相似性缓存和可观测性评分比，提出TIVS-O系统，在多智能体架构中实现零高风险漏洞，同时减少41.6%的LLM调用并降低能耗。


<details>
  <summary>Details</summary>
Motivation: 提示注入仍然是大型语言模型安全部署的主要障碍，特别是在多智能体环境中，中间输出可能传播或放大恶意指令。需要一种既能有效防御又能保持审计透明度的评估框架。

Method: 提出TIVS-O系统，结合语义相似性缓存和可观测性评分比，采用HOPE启发的嵌套学习架构，使用301个合成生成的注入提示进行测试，通过四个智能体管道和连续记忆系统实现安全分析。

Result: 系统实现零高风险漏洞的安全响应，语义缓存减少41.6%的LLM调用，降低延迟、能耗和碳排放。TIVS-O的五种配置揭示了缓解严格性和取证透明度之间的最优权衡。

Conclusion: 可观测性感知评估能揭示多智能体管道中的非单调效应，内存增强的智能体可同时最大化安全鲁棒性、实时性能、运营成本节约和环境可持续性，为安全和绿色的LLM部署提供生产就绪路径。

Abstract: Prompt injection remains a central obstacle to the safe deployment of large language models, particularly in multi-agent settings where intermediate outputs can propagate or amplify malicious instructions. Building on earlier work that introduced a four-metric Total Injection Vulnerability Score (TIVS), this paper extends the evaluation framework with semantic similarity-based caching and a fifth metric (Observability Score Ratio) to yield TIVS-O, investigating how defence effectiveness interacts with transparency in a HOPE-inspired Nested Learning architecture. The proposed system combines an agentic pipeline with Continuum Memory Systems that implement semantic similarity-based caching across 301 synthetically generated injection-focused prompts drawn from ten attack families, while a fourth agent performs comprehensive security analysis using five key performance indicators. In addition to traditional injection metrics, OSR quantifies the richness and clarity of security-relevant reasoning exposed by each agent, enabling an explicit analysis of trade-offs between strict mitigation and auditability. Experiments show that the system achieves secure responses with zero high-risk breaches, while semantic caching delivers substantial computational savings, achieving a 41.6% reduction in LLM calls and corresponding decreases in latency, energy consumption, and carbon emissions. Five TIVS-O configurations reveal optimal trade-offs between mitigation strictness and forensic transparency. These results indicate that observability-aware evaluation can reveal non-monotonic effects within multi-agent pipelines and that memory-augmented agents can jointly maximize security robustness, real-time performance, operational cost savings, and environmental sustainability without modifying underlying model weights, providing a production-ready pathway for secure and green LLM deployments.

</details>


### [132] [Real-Time Deadlines Reveal Temporal Awareness Failures in LLM Strategic Dialogues](https://arxiv.org/abs/2601.13206)
*Neil K. R. Sehgal,Sharath Chandra Guntuku,Lyle Ungar*

Main category: cs.AI

TL;DR: 研究发现大型语言模型在实时截止期限下的时间意识存在系统性缺陷，时间感知条件能显著提高谈判成功率


<details>
  <summary>Details</summary>
Motivation: 现实世界沟通（从治疗到商业谈判）都依赖连续时间约束，而当前LLM架构和评估协议很少测试实时截止期限下的时间意识

Method: 使用模拟谈判实验，在严格截止期限下配对智能体进行谈判，设置控制条件（仅知全局时间限制）和时间感知条件（每轮接收剩余时间更新）

Result: 时间感知条件下交易达成率显著更高（GPT-5.1为32% vs 4%），报价接受率提高六倍；但在轮次限制下LLM能达到接近完美的交易达成率（≥95%）

Conclusion: LLM在时间跟踪方面存在系统性缺陷而非战略推理问题，这限制了LLM在许多时间敏感应用中的部署

Abstract: Large Language Models (LLMs) generate text token-by-token in discrete time, yet real-world communication, from therapy sessions to business negotiations, critically depends on continuous time constraints. Current LLM architectures and evaluation protocols rarely test for temporal awareness under real-time deadlines. We use simulated negotiations between paired agents under strict deadlines to investigate how LLMs adjust their behavior in time-sensitive settings. In a control condition, agents know only the global time limit. In a time-aware condition, they receive remaining-time updates at each turn. Deal closure rates are substantially higher (32\% vs. 4\% for GPT-5.1) and offer acceptances are sixfold higher in the time-aware condition than in the control, suggesting LLMs struggle to internally track elapsed time. However, the same LLMs achieve near-perfect deal closure rates ($\geq$95\%) under turn-based limits, revealing the failure is in temporal tracking rather than strategic reasoning. These effects replicate across negotiation scenarios and models, illustrating a systematic lack of LLM time awareness that will constrain LLM deployment in many time-sensitive applications.

</details>


### [133] [CURE-Med: Curriculum-Informed Reinforcement Learning for Multilingual Medical Reasoning](https://arxiv.org/abs/2601.13262)
*Eric Onyame,Akash Ghosh,Subhadip Baidya,Sriparna Saha,Xiuying Chen,Chirag Agarwal*

Main category: cs.AI

TL;DR: CURE-MED框架通过课程强化学习提升LLMs在多语言医疗推理中的表现，在13种语言上显著提高了逻辑正确性和语言一致性


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在单语言数学和常识推理上表现良好，但在多语言医疗推理应用中仍不可靠，阻碍了其在多语言医疗环境中的部署

Method: 提出CURE-MED框架，包括：1）引入CUREMED-BENCH多语言医疗推理数据集；2）采用课程强化学习方法，结合代码切换感知的监督微调和组相对策略优化，共同提升逻辑正确性和语言稳定性

Result: 在13种语言上，该方法持续优于强基线并有效扩展：7B参数模型达到85.21%语言一致性和54.35%逻辑正确性；32B参数模型达到94.96%语言一致性和70.04%逻辑正确性

Conclusion: 研究结果支持LLMs实现可靠且公平的多语言医疗推理，为多语言医疗环境中的LLM部署提供了有效解决方案

Abstract: While large language models (LLMs) have shown to perform well on monolingual mathematical and commonsense reasoning, they remain unreliable for multilingual medical reasoning applications, hindering their deployment in multilingual healthcare settings. We address this by first introducing CUREMED-BENCH, a high-quality multilingual medical reasoning dataset with open-ended reasoning queries with a single verifiable answer, spanning thirteen languages, including underrepresented languages such as Amharic, Yoruba, and Swahili. Building on this dataset, we propose CURE-MED, a curriculum-informed reinforcement learning framework that integrates code-switching-aware supervised fine-tuning and Group Relative Policy Optimization to jointly improve logical correctness and language stability. Across thirteen languages, our approach consistently outperforms strong baselines and scales effectively, achieving 85.21% language consistency and 54.35% logical correctness at 7B parameters, and 94.96% language consistency and 70.04% logical correctness at 32B parameters. These results support reliable and equitable multilingual medical reasoning in LLMs. The code and dataset are available at https://cure-med.github.io/

</details>


### [134] [Improving the Safety and Trustworthiness of Medical AI via Multi-Agent Evaluation Loops](https://arxiv.org/abs/2601.13268)
*Zainab Ghafoor,Md Shafiqul Islam,Koushik Howlader,Md Rasel Khondokar,Tanusree Bhattacharjee,Sayantan Chakraborty,Adrito Roy,Ushashi Bhattacharjee,Tirtho Roy*

Main category: cs.AI

TL;DR: 提出一个多智能体精炼框架，通过结构化迭代对齐来增强医疗大语言模型的安全性和可靠性，结合生成模型和评估智能体，在900个临床查询中显著减少伦理违规和风险。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在医疗领域的应用日益增多，但确保其伦理完整性和安全合规性仍是临床部署的主要障碍，需要开发有效的安全治理方法。

Method: 引入多智能体精炼框架，结合两个生成模型（DeepSeek R1和Med-PaLM）和两个评估智能体（LLaMA 3.1和Phi-4），使用美国医学会医学伦理原则和五级安全风险评估协议进行结构化迭代对齐。

Result: 在涵盖9个伦理领域的900个临床查询中，DeepSeek R1收敛更快（平均2.34 vs 2.67次迭代），Med-PaLM在处理隐私敏感场景方面表现更优。迭代多智能体循环实现了89%的伦理违规减少和92%的风险降级率。

Conclusion: 该研究提出了一个可扩展、符合监管要求且成本效益高的医疗AI安全治理范式，通过多智能体框架有效提升医疗大语言模型的安全性和可靠性。

Abstract: Large Language Models (LLMs) are increasingly applied in healthcare, yet ensuring their ethical integrity and safety compliance remains a major barrier to clinical deployment. This work introduces a multi-agent refinement framework designed to enhance the safety and reliability of medical LLMs through structured, iterative alignment. Our system combines two generative models - DeepSeek R1 and Med-PaLM - with two evaluation agents, LLaMA 3.1 and Phi-4, which assess responses using the American Medical Association's (AMA) Principles of Medical Ethics and a five-tier Safety Risk Assessment (SRA-5) protocol. We evaluate performance across 900 clinically diverse queries spanning nine ethical domains, measuring convergence efficiency, ethical violation reduction, and domain-specific risk behavior. Results demonstrate that DeepSeek R1 achieves faster convergence (mean 2.34 vs. 2.67 iterations), while Med-PaLM shows superior handling of privacy-sensitive scenarios. The iterative multi-agent loop achieved an 89% reduction in ethical violations and a 92% risk downgrade rate, underscoring the effectiveness of our approach. This study presents a scalable, regulator-aligned, and cost-efficient paradigm for governing medical AI safety.

</details>


### [135] [PepEDiff: Zero-Shot Peptide Binder Design via Protein Embedding Diffusion](https://arxiv.org/abs/2601.13327)
*Po-Yu Liang,Tobo Duran,Jun Bai*

Main category: cs.AI

TL;DR: PepEDiff是一种新型的肽结合剂生成器，它直接在预训练蛋白质嵌入模型的连续潜在空间中生成结合序列，无需依赖中间结构预测，提高了序列多样性。


<details>
  <summary>Details</summary>
Motivation: 现有肽结合剂生成方法严重依赖中间结构预测，增加了复杂性并限制了序列多样性。需要一种不依赖结构预测的直接序列生成方法。

Method: 使用预训练蛋白质嵌入模型构建连续潜在空间，通过潜在空间探索和基于扩散的采样生成肽序列，避免记忆已知序列，实现零样本生成。

Result: 在TIGIT（具有大而平坦的蛋白质-蛋白质相互作用界面且缺乏可药性口袋的挑战性靶点）的案例研究中，PepEDiff在基准测试中优于最先进的方法。

Conclusion: PepEDiff展示了作为通用的、无需结构的零样本肽结合剂设计框架的潜力，能够生成超越已知结合剂分布的新型肽序列。

Abstract: We present PepEDiff, a novel peptide binder generator that designs binding sequences given a target receptor protein sequence and its pocket residues. Peptide binder generation is critical in therapeutic and biochemical applications, yet many existing methods rely heavily on intermediate structure prediction, adding complexity and limiting sequence diversity. Our approach departs from this paradigm by generating binder sequences directly in a continuous latent space derived from a pretrained protein embedding model, without relying on predicted structures, thereby improving structural and sequence diversity. To encourage the model to capture binding-relevant features rather than memorizing known sequences, we perform latent-space exploration and diffusion-based sampling, enabling the generation of peptides beyond the limited distribution of known binders. This zero-shot generative strategy leverages the global protein embedding manifold as a semantic prior, allowing the model to propose novel peptide sequences in previously unseen regions of the protein space. We evaluate PepEDiff on TIGIT, a challenging target with a large, flat protein-protein interaction interface that lacks a druggable pocket. Despite its simplicity, our method outperforms state-of-the-art approaches across benchmark tests and in the TIGIT case study, demonstrating its potential as a general, structure-free framework for zero-shot peptide binder design. The code for this research is available at GitHub: https://github.com/LabJunBMI/PepEDiff-An-Peptide-binder-Embedding-Diffusion-Model

</details>


### [136] [A Lightweight Modular Framework for Constructing Autonomous Agents Driven by Large Language Models: Design, Implementation, and Applications in AgentForge](https://arxiv.org/abs/2601.13383)
*Akbar Anbar Jafari,Cagri Ozcinar,Gholamreza Anbarjafari*

Main category: cs.AI

TL;DR: AgentForge是一个轻量级开源Python框架，通过模块化架构简化LLM驱动的自主智能体开发，支持技能组合、统一LLM后端和声明式配置，显著减少开发时间。


<details>
  <summary>Details</summary>
Motivation: 现有智能体框架存在架构僵化、供应商锁定和复杂度高等问题，阻碍了快速原型开发和部署，需要一种更灵活、易用的解决方案来民主化自主智能体构建。

Method: 1) 可组合技能抽象，支持细粒度任务分解和形式化输入输出契约；2) 统一LLM后端接口，支持云端API和本地推理引擎无缝切换；3) 基于YAML的声明式配置系统，分离智能体逻辑与实现细节；4) 将技能组合机制形式化为有向无环图(DAG)。

Result: 在四个基准场景的评估中，AgentForge实现了竞争力的任务完成率，相比LangChain减少62%开发时间，相比直接API集成减少78%开发时间，编排延迟低于100毫秒，适合实时应用，集成了六个内置技能。

Conclusion: AgentForge填补了LLM智能体生态系统的关键空白，为研究人员和从业者提供了生产就绪的基础设施，可在不牺牲灵活性或性能的情况下构建、评估和部署自主智能体。

Abstract: The emergence of LLMs has catalyzed a paradigm shift in autonomous agent development, enabling systems capable of reasoning, planning, and executing complex multi-step tasks. However, existing agent frameworks often suffer from architectural rigidity, vendor lock-in, and prohibitive complexity that impedes rapid prototyping and deployment. This paper presents AgentForge, a lightweight, open-source Python framework designed to democratize the construction of LLM-driven autonomous agents through a principled modular architecture. AgentForge introduces three key innovations: (1) a composable skill abstraction that enables fine-grained task decomposition with formally defined input-output contracts, (2) a unified LLM backend interface supporting seamless switching between cloud-based APIs and local inference engines, and (3) a declarative YAML-based configuration system that separates agent logic from implementation details. We formalize the skill composition mechanism as a directed acyclic graph (DAG) and prove its expressiveness for representing arbitrary sequential and parallel task workflows. Comprehensive experimental evaluation across four benchmark scenarios demonstrates that AgentForge achieves competitive task completion rates while reducing development time by 62% compared to LangChain and 78% compared to direct API integration. Latency measurements confirm sub-100ms orchestration overhead, rendering the framework suitable for real-time applications. The modular design facilitates extension: we demonstrate the integration of six built-in skills and provide comprehensive documentation for custom skill development. AgentForge addresses a critical gap in the LLM agent ecosystem by providing researchers and practitioners with a production-ready foundation for constructing, evaluating, and deploying autonomous agents without sacrificing flexibility or performance.

</details>


### [137] [SpatialBench-UC: Uncertainty-Aware Evaluation of Spatial Prompt Following in Text-to-Image Generation](https://arxiv.org/abs/2601.13462)
*Amine Rostane*

Main category: cs.AI

TL;DR: 本文介绍了SpatialBench-UC，一个用于评估文本到图像模型是否遵循明确空间指令的小型可复现基准测试，包含200个提示，通过选择性预测方法（检查器可在证据不足时弃权）来评估空间关系。


<details>
  <summary>Details</summary>
Motivation: 评估文本到图像模型是否遵循明确的空间指令难以自动化。对象检测器可能漏检目标或返回多个可能检测结果，简单的几何测试在边界情况下可能变得模糊。空间评估本质上是一个选择性预测问题，检查器可以在证据不足时弃权，并报告置信度，使结果能够解释为风险覆盖权衡而非单一分数。

Method: 引入SpatialBench-UC基准测试，包含200个提示（50个对象对×4种关系），分为100个通过交换对象角色获得的反事实对。发布基准测试包、版本化提示、固定配置、每个样本的检查器输出和报告表格，实现跨模型的可复现和可审计比较。还包括一个轻量级人工审核，用于校准检查器的弃权边界和置信度阈值。

Result: 评估了三个基线模型：Stable Diffusion 1.5、SD 1.5 BoxDiff和SD 1.4 GLIGEN。检查器报告通过率和覆盖率，以及已决定样本的条件通过率。结果显示，接地方法显著提高了通过率和覆盖率，但弃权仍然是主要因素，主要是由于漏检造成的。

Conclusion: SpatialBench-UC提供了一个系统化的框架来评估文本到图像模型的空间关系理解能力，通过选择性预测方法解决了自动化评估的挑战，并展示了接地方法在改善空间指令遵循方面的有效性。

Abstract: Evaluating whether text-to-image models follow explicit spatial instructions is difficult to automate. Object detectors may miss targets or return multiple plausible detections, and simple geometric tests can become ambiguous in borderline cases. Spatial evaluation is naturally a selective prediction problem, the checker may abstain when evidence is weak and report confidence so that results can be interpreted as a risk coverage tradeoff rather than a single score. We introduce SpatialBench-UC, a small, reproducible benchmark for pairwise spatial relations. The benchmark contains 200 prompts (50 object pairs times 4 relations) grouped into 100 counterfactual pairs obtained by swapping object roles. We release a benchmark package, versioned prompts, pinned configs, per-sample checker outputs, and report tables, enabling reproducible and auditable comparisons across models. We also include a lightweight human audit used to calibrate the checker's abstention margin and confidence threshold. We evaluate three baselines, Stable Diffusion 1.5, SD 1.5 BoxDiff, and SD 1.4 GLIGEN. The checker reports pass rate and coverage as well as conditional pass rates on decided samples. The results show that grounding methods substantially improve both pass rate and coverage, while abstention remains a dominant factor due mainly to missing detections.

</details>


### [138] [Context and Transcripts Improve Detection of Deepfake Audios of Public Figures](https://arxiv.org/abs/2601.13464)
*Chongyang Gao,Marco Postiglione,Julian Baldwin,Natalia Denisenko,Isabel Gortner,Luke Fosdick,Chiara Pulice,Sarit Kraus,V. S. Subrahmanian*

Main category: cs.AI

TL;DR: 该研究提出了一种基于上下文的音频深度伪造检测器（CADD），通过结合上下文信息和文本转录显著提升了检测性能，并对抗性攻击具有更强的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 人类在判断信息真伪时会利用上下文信息，但现有的音频深度伪造检测器仅分析音频文件本身，忽略了上下文和文本转录信息，这限制了检测效果。

Method: 创建了记者提供的深度伪造数据集（JDD）和合成音频数据集（SYN），提出了基于上下文的音频深度伪造检测器（CADD）架构，并在多个大规模数据集上进行评估。

Result: 上下文和/或文本转录能显著提升音频深度伪造检测器的性能：F1分数提升5%-37.58%，AUC提升3.77%-42.79%，EER提升6.17%-47.83%。CADD对5种对抗性规避策略具有更强的鲁棒性，性能下降平均仅为-0.71%。

Conclusion: 上下文信息和文本转录是提升音频深度伪造检测器性能的关键因素，基于上下文的检测方法不仅能显著提高检测准确率，还能增强对对抗性攻击的鲁棒性。

Abstract: Humans use context to assess the veracity of information. However, current audio deepfake detectors only analyze the audio file without considering either context or transcripts. We create and analyze a Journalist-provided Deepfake Dataset (JDD) of 255 public deepfakes which were primarily contributed by over 70 journalists since early 2024. We also generate a synthetic audio dataset (SYN) of dead public figures and propose a novel Context-based Audio Deepfake Detector (CADD) architecture. In addition, we evaluate performance on two large-scale datasets: ITW and P$^2$V. We show that sufficient context and/or the transcript can significantly improve the efficacy of audio deepfake detectors. Performance (measured via F1 score, AUC, and EER) of multiple baseline audio deepfake detectors and traditional classifiers can be improved by 5%-37.58% in F1-score, 3.77%-42.79% in AUC, and 6.17%-47.83% in EER. We additionally show that CADD, via its use of context and/or transcripts, is more robust to 5 adversarial evasion strategies, limiting performance degradation to an average of just -0.71% across all experiments. Code, models, and datasets are available at our project page: https://sites.northwestern.edu/nsail/cadd-context-based-audio-deepfake-detection (access restricted during review).

</details>


### [139] [Towards Efficient and Robust Linguistic Emotion Diagnosis for Mental Health via Multi-Agent Instruction Refinement](https://arxiv.org/abs/2601.13481)
*Jian Zhang,Zhangqi Wang,Zhiyuan Wang,Weiping Fu,Yu He,Haiping Zhu,Qika Lin,Jun Liu*

Main category: cs.AI

TL;DR: APOLO是一个自动提示优化框架，通过多智能体协作机制改进大语言模型在精神健康领域的情感诊断准确性和鲁棒性，解决情感共病和临床线索探索效率问题。


<details>
  <summary>Details</summary>
Motivation: 在临床记录、咨询对话和在线心理健康社区中，抑郁、焦虑和创伤相关状态的情感表达普遍存在，准确识别这些情感对于临床分诊、风险评估和及时干预至关重要。尽管大语言模型在情感分析任务中表现出强大的泛化能力，但在高风险、上下文密集的医疗环境中，其诊断可靠性对提示设计高度敏感。现有方法面临情感共病（多种交织情感状态使预测复杂化）和临床相关线索探索效率低下两大挑战。

Method: 提出APOLO框架，将指令优化建模为部分可观测马尔可夫决策过程，采用多智能体协作机制，包括规划者、教师、批评者、学生和目标角色。在闭环框架中，规划者定义优化轨迹，教师-批评者-学生智能体迭代优化提示以增强推理稳定性和有效性，目标智能体根据性能评估决定是否继续优化。

Result: 实验结果表明，APOLO在领域特定和分层基准测试中持续提高诊断准确性和鲁棒性，展示了在精神健康护理中可信赖大语言模型应用的可扩展和可泛化范式。

Conclusion: APOLO框架通过系统探索更广泛和细粒度的提示空间，有效解决了情感诊断中的共病问题和临床线索探索效率挑战，为大语言模型在精神健康领域的高风险应用提供了可靠的技术方案。

Abstract: Linguistic expressions of emotions such as depression, anxiety, and trauma-related states are pervasive in clinical notes, counseling dialogues, and online mental health communities, and accurate recognition of these emotions is essential for clinical triage, risk assessment, and timely intervention. Although large language models (LLMs) have demonstrated strong generalization ability in emotion analysis tasks, their diagnostic reliability in high-stakes, context-intensive medical settings remains highly sensitive to prompt design. Moreover, existing methods face two key challenges: emotional comorbidity, in which multiple intertwined emotional states complicate prediction, and inefficient exploration of clinically relevant cues. To address these challenges, we propose APOLO (Automated Prompt Optimization for Linguistic Emotion Diagnosis), a framework that systematically explores a broader and finer-grained prompt space to improve diagnostic efficiency and robustness. APOLO formulates instruction refinement as a Partially Observable Markov Decision Process and adopts a multi-agent collaboration mechanism involving Planner, Teacher, Critic, Student, and Target roles. Within this closed-loop framework, the Planner defines an optimization trajectory, while the Teacher-Critic-Student agents iteratively refine prompts to enhance reasoning stability and effectiveness, and the Target agent determines whether to continue optimization based on performance evaluation. Experimental results show that APOLO consistently improves diagnostic accuracy and robustness across domain-specific and stratified benchmarks, demonstrating a scalable and generalizable paradigm for trustworthy LLM applications in mental healthcare.

</details>


### [140] [AgenticRed: Optimizing Agentic Systems for Automated Red-teaming](https://arxiv.org/abs/2601.13518)
*Jiayi Yuan,Jonathan Nöther,Natasha Jaques,Goran Radanović*

Main category: cs.AI

TL;DR: AgenticRed是一个利用LLM上下文学习自动设计和优化红队系统的框架，通过进化选择方法在无需人工干预的情况下迭代改进红队系统设计，显著提升了攻击成功率。


<details>
  <summary>Details</summary>
Motivation: 现有自动化红队方法依赖人工设计的工作流程，存在人为偏见且难以探索更广泛的设计空间，需要一种能够自动设计和优化红队系统的方法。

Method: 将红队视为系统设计问题，采用类似Meta Agent Search的方法，开发了基于进化选择的智能系统演化程序，利用LLM的上下文学习能力迭代设计和改进红队系统。

Result: 在Llama-2-7B上达到96%攻击成功率（提升36%），在Llama-3-8B上达到98%；在GPT-3.5-Turbo和GPT-4o-mini上达到100%攻击成功率，在Claude-Sonnet-3.5上达到60%（提升24%）。

Conclusion: 自动化系统设计是AI安全评估的强大范式，能够跟上快速发展的模型步伐，为红队测试提供了更高效、更全面的解决方案。

Abstract: While recent automated red-teaming methods show promise for systematically exposing model vulnerabilities, most existing approaches rely on human-specified workflows. This dependence on manually designed workflows suffers from human biases and makes exploring the broader design space expensive. We introduce AgenticRed, an automated pipeline that leverages LLMs' in-context learning to iteratively design and refine red-teaming systems without human intervention. Rather than optimizing attacker policies within predefined structures, AgenticRed treats red-teaming as a system design problem. Inspired by methods like Meta Agent Search, we develop a novel procedure for evolving agentic systems using evolutionary selection, and apply it to the problem of automatic red-teaming. Red-teaming systems designed by AgenticRed consistently outperform state-of-the-art approaches, achieving 96% attack success rate (ASR) on Llama-2-7B (36% improvement) and 98% on Llama-3-8B on HarmBench. Our approach exhibits strong transferability to proprietary models, achieving 100% ASR on GPT-3.5-Turbo and GPT-4o-mini, and 60% on Claude-Sonnet-3.5 (24% improvement). This work highlights automated system design as a powerful paradigm for AI safety evaluation that can keep pace with rapidly evolving models.

</details>


### [141] [Reasoning While Recommending: Entropy-Guided Latent Reasoning in Generative Re-ranking Models](https://arxiv.org/abs/2601.13533)
*Changshuo Zhang*

Main category: cs.AI

TL;DR: EGLR模型通过熵引导的潜在推理机制，在生成式重排序中实现"边推理边推荐"，动态适应列表生成过程中的难度变化，提升推荐精度。


<details>
  <summary>Details</summary>
Motivation: 现有生成式重排序方法难以适应列表生成过程中模型难度的动态熵变化，无法准确捕捉复杂用户偏好。语言模型通过集成推理能力取得了突破，启发我们引入潜在推理机制来降低决策过程中的熵。

Method: 提出熵引导潜在推理(EGLR)推荐模型：1)采用"边推理边推荐"范式，在生成过程中实时推理；2)使用上下文感知推理令牌和动态温度调整实现熵引导的变长推理；3)轻量级集成设计，无需复杂独立模块或后处理。

Result: 在两个真实世界数据集上的实验验证了模型有效性，显著优势在于能够与现有生成式重排序模型兼容以提升其性能。进一步分析展示了实际部署价值和研究潜力。

Conclusion: EGLR模型通过熵引导的潜在推理机制，在生成式重排序中实现了更精确的探索-利用权衡，能够动态适应列表生成难度变化，提升推荐性能，且易于与现有模型集成。

Abstract: Reinforcement learning plays a crucial role in generative re-ranking scenarios due to its exploration-exploitation capabilities, but existing generative methods mostly fail to adapt to the dynamic entropy changes in model difficulty during list generation, making it challenging to accurately capture complex preferences. Given that language models have achieved remarkable breakthroughs by integrating reasoning capabilities, we draw on this approach to introduce a latent reasoning mechanism, and experimental validation demonstrates that this mechanism effectively reduces entropy in the model's decision-making process. Based on these findings, we introduce the Entropy-Guided Latent Reasoning (EGLR) recommendation model, which has three core advantages. First, it abandons the "reason first, recommend later" paradigm to achieve "reasoning while recommending", specifically designed for the high-difficulty nature of list generation by enabling real-time reasoning during generation. Second, it implements entropy-guided variable-length reasoning using context-aware reasoning token alongside dynamic temperature adjustment, expanding exploration breadth in reasoning and boosting exploitation precision in recommending to achieve a more precisely adapted exploration-exploitation trade-off. Third, the model adopts a lightweight integration design with no complex independent modules or post-processing, enabling easy adaptation to existing models. Experimental results on two real-world datasets validate the model's effectiveness, and its notable advantage lies in being compatible with existing generative re-ranking models to enhance their performance. Further analyses also demonstrate its practical deployment value and research potential.

</details>


### [142] [TruthTensor: Evaluating LLMs Human Imitation through Prediction Market Drift and Holistic Reasoning](https://arxiv.org/abs/2601.13545)
*Shirin Shahabi,Spencer Graham,Haruna Isah*

Main category: cs.AI

TL;DR: TruthTensor是一个创新的语言模型评估框架，通过实时预测市场、概率评分和多维度指标来评估LLMs在真实世界决策环境中的表现，超越了传统静态基准测试的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有语言模型评估方法存在根本性挑战：静态基准测试无法捕捉真实世界的不确定性、分布偏移，以及孤立任务准确性与人类对齐决策之间的差距。需要一种能够评估LLMs作为人类模仿系统在社交基础、高熵环境中表现的评估范式。

Method: 提出TruthTensor评估框架，基于前瞻性、无污染的任务，将评估锚定在实时预测市场上，结合概率评分提供模型行为的整体视图。框架包含漂移中心诊断、显式鲁棒性检查、明确的人类与自动化评估角色、标注协议和统计测试程序，确保结果的可解释性和可复现性。

Result: 在500多个真实市场（政治、经济、文化、技术）的实验中，TruthTensor显示具有相似预测准确性的模型在校准、漂移和风险敏感性方面存在显著差异，强调了需要从多个维度（准确性、校准、叙事稳定性、成本和资源效率）评估模型。

Conclusion: TruthTensor通过现代评估最佳实践、清晰假设框架、谨慎指标选择、透明计算/成本报告、人在回路验证和开放的版本化评估合同，为LLMs在真实世界决策环境中的评估提供了可辩护的评估方法，并已公开发布。

Abstract: Evaluating language models and AI agents remains fundamentally challenging because static benchmarks fail to capture real-world uncertainty, distribution shift, and the gap between isolated task accuracy and human-aligned decision-making under evolving conditions. This paper introduces TruthTensor, a novel, reproducible evaluation paradigm that measures Large Language Models (LLMs) not only as prediction engines but as human-imitation systems operating in socially-grounded, high-entropy environments. Building on forward-looking, contamination-free tasks, our framework anchors evaluation to live prediction markets and combines probabilistic scoring to provide a holistic view of model behavior. TruthTensor complements traditional correctness metrics with drift-centric diagnostics and explicit robustness checks for reproducibility. It specify human vs. automated evaluation roles, annotation protocols, and statistical testing procedures to ensure interpretability and replicability of results. In experiments across 500+ real markets (political, economic, cultural, technological), TruthTensor demonstrates that models with similar forecast accuracy can diverge markedly in calibration, drift, and risk-sensitivity, underscoring the need to evaluate models along multiple axes (accuracy, calibration, narrative stability, cost, and resource efficiency). TruthTensor therefore operationalizes modern evaluation best practices, clear hypothesis framing, careful metric selection, transparent compute/cost reporting, human-in-the-loop validation, and open, versioned evaluation contracts, to produce defensible assessments of LLMs in real-world decision contexts. We publicly release TruthTensor at https://truthtensor.com

</details>


### [143] [ChatAD: Reasoning-Enhanced Time-Series Anomaly Detection with Multi-Turn Instruction Evolution](https://arxiv.org/abs/2601.13546)
*Hui Sun,Chang Xu,Haonan Xie,Hao Li,Yuhao Huang,Chuheng Zhang,Ming Jin,Xiaoguang Liu,Gang Wang,Jiang Bian*

Main category: cs.AI

TL;DR: 本文提出TSEvol多智能体时间序列演化算法、TSEData-20K数据集、ChatAD系列聊天机器人、TKTO优化方法以及LLADBench基准测试，显著提升时间序列异常检测的推理能力和跨任务泛化性能。


<details>
  <summary>Details</summary>
Motivation: 现有LLM驱动的异常检测方法面临推理能力不足、多轮对话能力欠缺和泛化范围狭窄的挑战，需要提升对时间序列异常行为的理解和解释能力。

Method: 1) 提出TSEvol多智能体时间序列演化算法；2) 构建TSEData-20K数据集并开发ChatAD系列聊天机器人；3) 提出TKTO优化方法增强跨任务泛化能力；4) 建立LLADBench基准测试框架。

Result: ChatAD模型在准确率提升34.50%、F1分数提升34.71%、误报率降低37.42%；通过TKTO优化的ChatAD在分类、预测和插补任务上展现出竞争力的推理和跨任务泛化性能。

Conclusion: 提出的多智能体时间序列演化框架、数据集、聊天机器人系列和优化方法有效解决了现有LLM异常检测方法的局限性，显著提升了时间序列异常检测的推理能力和跨任务泛化性能。

Abstract: LLM-driven Anomaly Detection (AD) helps enhance the understanding and explanatory abilities of anomalous behaviors in Time Series (TS). Existing methods face challenges of inadequate reasoning ability, deficient multi-turn dialogue capability, and narrow generalization. To this end, we 1) propose a multi-agent-based TS Evolution algorithm named TSEvol. On top of it, we 2) introduce the AD reasoning and multi-turn dialogue Dataset TSEData-20K and contribute the Chatbot family for AD, including ChatAD-Llama3-8B, Qwen2.5-7B, and Mistral-7B. Furthermore, 3) we propose the TS Kahneman-Tversky Optimization (TKTO) to enhance ChatAD's cross-task generalization capability. Lastly, 4) we propose a LLM-driven Learning-based AD Benchmark LLADBench to evaluate the performance of ChatAD and nine baselines across seven datasets and tasks. Our three ChatAD models achieve substantial gains, up to 34.50% in accuracy, 34.71% in F1, and a 37.42% reduction in false positives. Besides, via KTKO, our optimized ChatAD achieves competitive performance in reasoning and cross-task generalization on classification, forecasting, and imputation.

</details>


### [144] [AgentGC: Evolutionary Learning-based Lossless Compression for Genomics Data with LLM-driven Multiple Agent](https://arxiv.org/abs/2601.13559)
*Sun Hui,Ding Yanfeng,Huidong Ma,Chang Xu,Keyan Jin,Lizheng Zu,Cheng Zhong,xiaoguang Liu,Gang Wang,Wentong Cai*

Main category: cs.AI

TL;DR: AgentGC是首个基于智能体的进化式基因组数据压缩器，通过三层架构（用户层、认知层、压缩层）和多智能体系统，解决了现有学习方法在压缩建模、适应性和用户界面方面的限制，在压缩比和吞吐量上均显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前基于学习的基因组数据压缩方法存在三个主要问题：1）非进化性，无法适应不断发展的需求；2）低层次压缩建模，缺乏高级优化；3）有限的适应性和用户不友好的界面。需要一种能够联合优化算法、数据集和系统的智能压缩解决方案。

Method: 提出AgentGC三层架构：1）用户层通过Leader智能体结合LLM提供友好界面；2）认知层由Leader驱动，集成LLM进行算法-数据集-系统的联合优化；3）压缩层由Worker智能体执行，通过自动化多知识学习框架进行压缩和解压缩。设计了三种模式：CP（压缩比优先）、TP（吞吐量优先）和BM（平衡模式）。

Result: 在9个数据集上与14个基线方法比较，AgentGC的三种模式（CP、TP、BM）平均压缩比提升分别为16.66%、16.11%和16.33%，吞吐量提升分别为4.73倍、9.23倍和9.15倍，在压缩性能和效率上均显著优于现有方法。

Conclusion: AgentGC作为首个基于智能体的进化式基因组数据压缩器，通过多智能体架构和LLM集成，成功解决了现有方法的局限性，在压缩比和吞吐量上实现了显著提升，为基因组数据存储、共享和管理提供了高效的解决方案。

Abstract: Lossless compression has made significant advancements in Genomics Data (GD) storage, sharing and management. Current learning-based methods are non-evolvable with problems of low-level compression modeling, limited adaptability, and user-unfriendly interface. To this end, we propose AgentGC, the first evolutionary Agent-based GD Compressor, consisting of 3 layers with multi-agent named Leader and Worker. Specifically, the 1) User layer provides a user-friendly interface via Leader combined with LLM; 2) Cognitive layer, driven by the Leader, integrates LLM to consider joint optimization of algorithm-dataset-system, addressing the issues of low-level modeling and limited adaptability; and 3) Compression layer, headed by Worker, performs compression & decompression via a automated multi-knowledge learning-based compression framework. On top of AgentGC, we design 3 modes to support diverse scenarios: CP for compression-ratio priority, TP for throughput priority, and BM for balanced mode. Compared with 14 baselines on 9 datasets, the average compression ratios gains are 16.66%, 16.11%, and 16.33%, the throughput gains are 4.73x, 9.23x, and 9.15x, respectively.

</details>


### [145] [Reasoning is a Modality](https://arxiv.org/abs/2601.13562)
*Zhiguang Liu,Yi Shang*

Main category: cs.AI

TL;DR: 该论文提出了一种角色分离的Transformer架构，用于解决视觉推理任务，在ARC-1基准上超越了人类平均表现。


<details>
  <summary>Details</summary>
Motivation: 现代AI系统（如LLMs和ViTs）主要作为行为序列预测机器运行，通过建模token统计来匹配可观察行为，缺乏持久、可读的思维状态。这与人类行为存在差距：人类可以通过解码内部状态来解释行为，而AI系统只能产生缺乏内在状态基础的流利事后合理化解释。作者假设推理应该作为一个独立的模态存在，与规则应用的低级工作空间分离。

Method: 设计了新颖的角色分离Transformer块，将全局控制器token与网格工作空间token分离，实现迭代规则执行。该方法在VARC视觉中心协议下进行训练和评估，专门用于解决ARC任务作为视觉推理问题。

Result: 在ARC-1任务上达到了62.6%的准确率，超越了人类平均表现（60.2%），并显著优于先前方法。定性分析显示，与密集ViT基线相比，模型展现出更一致的规则应用结构，从概率块向控制器驱动的推理转变。

Conclusion: 研究支持了推理作为独立模态的假设，角色分离的Transformer架构能够实现更接近人类推理的抽象推理能力，在视觉推理任务上取得了突破性进展。

Abstract: The Abstraction and Reasoning Corpus (ARC) provides a compact laboratory for studying abstract reasoning, an ability central to human intelligence. Modern AI systems, including LLMs and ViTs, largely operate as sequence-of-behavior prediction machines: they match observable behaviors by modeling token statistics without a persistent, readable mental state. This creates a gap with human-like behavior: humans can explain an action by decoding internal state, while AI systems can produce fluent post-hoc rationalizations that are not grounded in such a state. We hypothesize that reasoning is a modality: reasoning should exist as a distinct channel separate from the low-level workspace on which rules are applied. To test this hypothesis, on solving ARC tasks as a visual reasoning problem, we designed a novel role-separated transformer block that splits global controller tokens from grid workspace tokens, enabling iterative rule execution. Trained and evaluated within the VARC vision-centric protocol, our method achieved 62.6% accuracy on ARC-1, surpassing average human performance (60.2%) and outperforming prior methods significantly. Qualitatively, our models exhibit more coherent rule-application structure than the dense ViT baseline, consistent with a shift away from plausible probability blobs toward controller-driven reasoning.

</details>


### [146] [SCRIPTMIND: Crime Script Inference and Cognitive Evaluation for LLM-based Social Engineering Scam Detection System](https://arxiv.org/abs/2601.13581)
*Heedou Kim,Changsik Kim,Sanghwa Shin,Jaewoo Kang*

Main category: cs.AI

TL;DR: ScriptMind是一个基于LLM的诈骗检测框架，通过犯罪脚本推理、数据集构建和认知模拟评估，显著提升诈骗检测性能并增强用户认知警觉性。


<details>
  <summary>Details</summary>
Motivation: 传统诈骗检测方法难以应对个性化、多轮对话的社会工程诈骗，而大型语言模型在诈骗检测中的认知辅助潜力尚未充分探索。

Method: 提出ScriptMind框架，包含三个组件：犯罪脚本推理任务(CSIT)用于诈骗推理，犯罪脚本感知推理数据集(CSID)用于微调小型LLM，以及认知模拟评估(CSED)用于评估实时认知影响。基于571个韩国电话诈骗案例构建了22,712个结构化训练实例。

Result: 使用ScriptMind微调的11B小型LLM在检测准确率上比GPT-4o高出13%，在误报减少、诈骗者话语预测和推理质量方面均优于商业模型。在电话诈骗模拟实验中，显著提升并维持了用户的怀疑水平，增强了诈骗认知意识。

Conclusion: ScriptMind代表了向以人为本、认知自适应的LLM诈骗防御系统迈出的重要一步，展示了LLM在增强人类认知警觉性方面的潜力。

Abstract: Social engineering scams increasingly employ personalized, multi-turn deception, exposing the limits of traditional detection methods. While Large Language Models (LLMs) show promise in identifying deception, their cognitive assistance potential remains underexplored. We propose ScriptMind, an integrated framework for LLM-based scam detection that bridges automated reasoning and human cognition. It comprises three components: the Crime Script Inference Task (CSIT) for scam reasoning, the Crime Script-Aware Inference Dataset (CSID) for fine-tuning small LLMs, and the Cognitive Simulation-based Evaluation of Social Engineering Defense (CSED) for assessing real-time cognitive impact. Using 571 Korean phone scam cases, we built 22,712 structured scammer-sequence training instances. Experimental results show that the 11B small LLM fine-tuned with ScriptMind outperformed GPT-4o by 13%, achieving superior performance over commercial models in detection accuracy, false-positive reduction, scammer utterance prediction, and rationale quality. Moreover, in phone scam simulation experiments, it significantly enhanced and sustained users' suspicion levels, improving their cognitive awareness of scams. ScriptMind represents a step toward human-centered, cognitively adaptive LLMs for scam defense.

</details>


### [147] [Motion-to-Response Content Generation via Multi-Agent AI System with Real-Time Safety Verification](https://arxiv.org/abs/2601.13589)
*HyeYoung Lee*

Main category: cs.AI

TL;DR: 提出基于音频情感信号的多智能体AI系统，实时生成安全可控的响应式媒体内容，通过四个协作智能体实现情感识别到内容生成的完整流程，并加入安全验证环节确保内容合规性。


<details>
  <summary>Details</summary>
Motivation: 传统语音情感识别研究主要关注分类准确性，但缺乏将识别出的情感状态转化为安全、适龄、可控的响应内容。需要一种能够实时生成情感响应媒体内容，同时确保安全性和可控性的系统。

Method: 采用多智能体架构，包含四个协作智能体：1) 基于CNN的情感识别智能体提取声学特征；2) 响应策略决策智能体将情感映射到响应模式；3) 内容参数生成智能体产生媒体控制参数；4) 安全验证智能体强制执行适龄性和刺激约束。系统包含显式的安全验证循环，在输出前过滤生成内容。

Result: 在公共数据集上的实验结果显示：情感识别准确率73.2%，响应模式一致性89.4%，安全合规性100%，推理延迟低于100毫秒，适合设备端部署。模块化架构支持可解释性和可扩展性。

Conclusion: 该系统成功将情感识别转化为安全可控的响应内容生成，适用于儿童相关媒体、治疗应用和情感响应智能设备。安全验证机制确保了内容的合规性，实时性能满足实际应用需求。

Abstract: This paper proposes a multi-agent artificial intelligence system that generates response-oriented media content in real time based on audio-derived emotional signals. Unlike conventional speech emotion recognition studies that focus primarily on classification accuracy, our approach emphasizes the transformation of inferred emotional states into safe, age-appropriate, and controllable response content through a structured pipeline of specialized AI agents. The proposed system comprises four cooperative agents: (1) an Emotion Recognition Agent with CNN-based acoustic feature extraction, (2) a Response Policy Decision Agent for mapping emotions to response modes, (3) a Content Parameter Generation Agent for producing media control parameters, and (4) a Safety Verification Agent enforcing age-appropriateness and stimulation constraints. We introduce an explicit safety verification loop that filters generated content before output, ensuring compliance with predefined rules. Experimental results on public datasets demonstrate that the system achieves 73.2% emotion recognition accuracy, 89.4% response mode consistency, and 100% safety compliance while maintaining sub-100ms inference latency suitable for on-device deployment. The modular architecture enables interpretability and extensibility, making it applicable to child-adjacent media, therapeutic applications, and emotionally responsive smart devices.

</details>


### [148] [Foundations of Global Consistency Checking with Noisy LLM Oracles](https://arxiv.org/abs/2601.13600)
*Paul He,Elke Kirschbaum,Shiva Kasiviswanathan*

Main category: cs.AI

TL;DR: 提出一种自适应分治算法，使用LLM评估器检测和定位自然语言事实集合中的全局不一致性，解决LLM判断噪声大且成对检查无法保证全局一致性的问题。


<details>
  <summary>Details</summary>
Motivation: 自然语言事实集合的全局一致性对于事实核查、摘要和知识库构建等任务至关重要。虽然LLM可以评估小规模事实子集的一致性，但其判断存在噪声，且成对检查无法保证全局一致性。

Method: 提出自适应分治算法，识别最小不一致子集（MUSes），可选地通过命中集计算最小修复。该方法具有低阶多项式查询复杂度。

Result: 在合成和真实LLM预言机的实验中，该方法能有效检测和定位不一致性，为基于LLM评估器的语言一致性验证提供了可扩展框架。

Conclusion: 验证全局一致性在最坏情况下需要指数级查询，但提出的自适应分治算法通过识别最小不一致子集，以低阶多项式查询复杂度实现了实用的语言一致性验证。

Abstract: Ensuring that collections of natural-language facts are globally consistent is essential for tasks such as fact-checking, summarization, and knowledge base construction. While Large Language Models (LLMs) can assess the consistency of small subsets of facts, their judgments are noisy, and pairwise checks are insufficient to guarantee global coherence. We formalize this problem and show that verifying global consistency requires exponentially many oracle queries in the worst case. To make the task practical, we propose an adaptive divide-and-conquer algorithm that identifies minimal inconsistent subsets (MUSes) of facts and optionally computes minimal repairs through hitting-sets. Our approach has low-degree polynomial query complexity. Experiments with both synthetic and real LLM oracles show that our method efficiently detects and localizes inconsistencies, offering a scalable framework for linguistic consistency verification with LLM-based evaluators.

</details>


### [149] [Understanding Mental States to Guide Social Influence in Multi-Person Group Dialogue](https://arxiv.org/abs/2601.13687)
*Zhichao Liang,Satoshi Nakamura*

Main category: cs.AI

TL;DR: SocialMindChange是一个新的动态心理理论基准测试，要求语言模型在社交互动中主动改变他人心理状态，而不仅仅是追踪心理状态变化。


<details>
  <summary>Details</summary>
Motivation: 现有动态心理理论基准测试大多让语言模型处于被动角色，只追踪心理状态变化。但在真实社交互动中，心理理论也用于行动——说话者计划说什么来改变他人的心理状态轨迹以达到目标。

Method: 创建SocialMindChange基准测试，包含1200个社交情境，每个情境有4个角色和5个相连场景。模型扮演一个角色，在五个场景中生成对话以达到目标，同时保持与所有参与者不断变化的状态一致。使用结构化四步框架构建，覆盖6000个场景和超过90,000个问题。

Result: 评估10个最先进的LLM，发现它们的平均性能比人类性能低54.2%。

Conclusion: 当前LLM在长期相连的互动中维持和改变心理状态表征方面仍然存在困难，表明需要进一步改进模型的社会认知能力。

Abstract: Existing dynamic Theory of Mind (ToM) benchmarks mostly place language models in a passive role: the model reads a sequence of connected scenarios and reports what people believe, feel, intend, and do as these states change. In real social interaction, ToM is also used for action: a speaker plans what to say in order to shift another person's mental-state trajectory toward a goal. We introduce SocialMindChange, a benchmark that moves from tracking minds to changing minds in social interaction. Each instance defines a social context with 4 characters and five connected scenes. The model plays one character and generates dialogue across the five scenes to reach the target while remaining consistent with the evolving states of all participants. SocialMindChange also includes selected higher-order states. Using a structured four-step framework, we construct 1,200 social contexts, covering 6000 scenarios and over 90,000 questions, each validated for realism and quality. Evaluations on ten state-of-the-art LLMs show that their average performance is 54.2% below human performance. This gap suggests that current LLMs still struggle to maintain and change mental-state representations across long, linked interactions.

</details>


### [150] [Reasoning or Fluency? Dissecting Probabilistic Confidence in Best-of-N Selection](https://arxiv.org/abs/2601.13735)
*Hojin Kim,Jaehyung Kim*

Main category: cs.AI

TL;DR: 研究发现当前基于概率的置信度指标无法有效捕捉推理步骤间的因果依赖关系，主要反映的是表面流畅性或分布先验，而非逻辑结构。


<details>
  <summary>Details</summary>
Motivation: 挑战当前普遍假设：概率置信度指标能反映推理质量。研究者质疑这些指标是否真正捕捉到了推理步骤间的因果依赖关系，这对于有效的推理至关重要。

Method: 引入三类推理步骤间因果关系扰动方法，系统性地破坏推理步骤间的依赖关系，同时保持局部流畅性。使用硬注意力掩码等严重干预措施，测试不同模型家族和推理基准下的选择准确性变化。

Result: 令人惊讶的是，即使严重破坏推理步骤间的因果依赖关系，选择准确性仅轻微下降。硬注意力掩码等直接阻止模型关注先前推理步骤的干预措施，也没有显著降低选择性能。

Conclusion: 当前概率指标对逻辑结构不敏感，主要捕捉表面流畅性或分布先验。为此提出了一种对比因果度量方法，能明确隔离推理步骤间的因果依赖关系，比现有基于概率的方法产生更忠实的结果选择。

Abstract: Probabilistic confidence metrics are increasingly adopted as proxies for reasoning quality in Best-of-N selection, under the assumption that higher confidence reflects higher reasoning fidelity. In this work, we challenge this assumption by investigating whether these metrics truly capture inter-step causal dependencies necessary for valid reasoning. We introduce three classes of inter-step causality perturbations that systematically disrupt dependencies between reasoning steps while preserving local fluency. Surprisingly, across diverse model families and reasoning benchmarks, we find that selection accuracy degrades only marginally under these disruptions. Even severe interventions, such as applying hard attention masks that directly prevent the model from attending to prior reasoning steps, do not substantially reduce selection performance. These findings provide strong evidence that current probabilistic metrics are largely insensitive to logical structure, and primarily capture surface-level fluency or in-distribution priors instead. Motivated by this gap, we propose a contrastive causality metric that explicitly isolates inter-step causal dependencies, and demonstrate that it yields more faithful output selection than existing probability-based approaches.

</details>


### [151] [Finding RELIEF: Shaping Reasoning Behavior without Reasoning Supervision via Belief Engineering](https://arxiv.org/abs/2601.13752)
*Chak Tou Leong,Dingwei Chen,Heming Xia,Qingyu Yin,Sunbowen Lee,Jian Wang,Wenjie Li*

Main category: cs.AI

TL;DR: 提出RELIEF框架，通过调整大推理模型的自我概念来塑造其行为，无需推理轨迹监督，降低训练成本


<details>
  <summary>Details</summary>
Motivation: 大型推理模型在复杂问题解决中表现出色，但存在计算冗余和推理不忠实的问题。现有方法依赖强化学习或黄金标准推理轨迹微调，计算成本高且难以扩展

Method: 发现LRMs具有潜在推理信念，可通过简单logit探测捕获。提出RELIEF框架，通过将模型的自我概念与目标信念蓝图对齐来塑造行为，无需推理轨迹监督，通过合成自反问答对微调来内化所需特质

Result: 在效率和忠实性任务上的广泛实验表明，RELIEF匹配或优于基于行为监督和偏好的基线方法，同时需要更低的训练成本。进一步分析验证了改变模型的推理信念能有效塑造其实际行为

Conclusion: RELIEF提供了一种简单有效的框架，通过调整模型的自我概念来塑造大型推理模型的行为，避免了昂贵的推理轨迹监督需求，为模型行为塑造提供了新思路

Abstract: Large reasoning models (LRMs) have achieved remarkable success in complex problem-solving, yet they often suffer from computational redundancy or reasoning unfaithfulness. Current methods for shaping LRM behavior typically rely on reinforcement learning or fine-tuning with gold-standard reasoning traces, a paradigm that is both computationally expensive and difficult to scale. In this paper, we reveal that LRMs possess latent \textit{reasoning beliefs} that internally track their own reasoning traits, which can be captured through simple logit probing. Building upon this insight, we propose Reasoning Belief Engineering (RELIEF), a simple yet effective framework that shapes LRM behavior by aligning the model's self-concept with a target belief blueprint. Crucially, RELIEF completely bypasses the need for reasoning-trace supervision. It internalizes desired traits by fine-tuning on synthesized, self-reflective question-answering pairs that affirm the target belief. Extensive experiments on efficiency and faithfulness tasks demonstrate that RELIEF matches or outperforms behavior-supervised and preference-based baselines while requiring lower training costs. Further analysis validates that shifting a model's reasoning belief effectively shapes its actual behavior.

</details>


### [152] [DARC: Decoupled Asymmetric Reasoning Curriculum for LLM Evolution](https://arxiv.org/abs/2601.13761)
*Shengda Fan,Xuyan Ye,Yankai Lin*

Main category: cs.AI

TL;DR: DARC是一个两阶段自进化框架，通过解耦问题生成和求解训练，解决了自对弈中的优化不稳定问题，在多个推理基准上平均提升10.9分。


<details>
  <summary>Details</summary>
Motivation: 现有自对弈框架存在优化不稳定问题，主要源于：(1) 求解器依赖的奖励反馈导致提问者目标非平稳；(2) 自生成伪标签用于监督求解器时存在自举误差。

Method: DARC采用两阶段框架：第一阶段训练提问者根据明确难度级别和外部语料合成难度校准的问题；第二阶段通过非对称自蒸馏机制训练求解器，使用文档增强的教师模型生成高质量伪标签来监督无文档访问的学生求解器。

Result: DARC在九个推理基准和三个骨干模型上平均提升10.9分，一致优于所有基线方法，且无需人工标注就能接近全监督模型的性能。

Conclusion: DARC是一个模型无关的稳定自进化框架，通过解耦问题生成和求解训练，有效解决了自对弈中的优化不稳定问题，实现了显著的性能提升。

Abstract: Self-play with large language models has emerged as a promising paradigm for achieving self-improving artificial intelligence. However, existing self-play frameworks often suffer from optimization instability, due to (i) non-stationary objectives induced by solver-dependent reward feedback for the Questioner, and (ii) bootstrapping errors from self-generated pseudo-labels used to supervise the Solver. To mitigate these challenges, we introduce DARC (Decoupled Asymmetric Reasoning Curriculum), a two-stage framework that stabilizes the self-evolution process. First, we train the Questioner to synthesize difficulty-calibrated questions, conditioned on explicit difficulty levels and external corpora. Second, we train the Solver with an asymmetric self-distillation mechanism, where a document-augmented teacher generates high-quality pseudo-labels to supervise the student Solver that lacks document access. Empirical results demonstrate that DARC is model-agnostic, yielding an average improvement of 10.9 points across nine reasoning benchmarks and three backbone models. Moreover, DARC consistently outperforms all baselines and approaches the performance of fully supervised models without relying on human annotations.The code is available at https://github.com/RUCBM/DARC.

</details>


### [153] [Look-Ahead-Bench: a Standardized Benchmark of Look-ahead Bias in Point-in-Time LLMs for Finance](https://arxiv.org/abs/2601.13770)
*Mostapha Benhenda*

Main category: cs.AI

TL;DR: Look-Ahead-Bench是一个标准化基准测试，用于评估金融工作流中Point-in-Time大语言模型的超前偏差，通过分析不同市场周期中的性能衰减来区分真实预测能力与记忆性表现。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要通过问答测试内部超前知识，缺乏对实际应用场景中模型行为的评估。需要建立标准化基准来测量金融LLMs中的时间偏差，为实际部署提供实用框架。

Method: 创建Look-Ahead-Bench基准，在现实金融工作流中评估PiT LLMs。通过分析不同时间市场周期中的性能衰减来区分预测能力与记忆，引入多个量化基线建立性能阈值。评估开源LLMs（Llama 3.1和DeepSeek 3.2）与PiT-Inference系列模型。

Result: 标准LLMs显示出显著的超前偏差（通过alpha衰减测量），而Pitinf模型随着规模扩大展现出改进的泛化和推理能力。Pitinf模型在不同市场周期中表现更稳定。

Conclusion: 该工作为金融LLMs中时间偏差的标准化评估奠定了基础，提供了识别适合实际部署模型的实用框架。Pitinf模型在减少超前偏差方面表现优异，特别是在大规模模型中。

Abstract: We introduce Look-Ahead-Bench, a standardized benchmark measuring look-ahead bias in Point-in-Time (PiT) Large Language Models (LLMs) within realistic and practical financial workflows. Unlike most existing approaches that primarily test inner lookahead knowledge via Q\\&A, our benchmark evaluates model behavior in practical scenarios. To distinguish genuine predictive capability from memorization-based performance, we analyze performance decay across temporally distinct market regimes, incorporating several quantitative baselines to establish performance thresholds. We evaluate prominent open-source LLMs -- Llama 3.1 (8B and 70B) and DeepSeek 3.2 -- against a family of Point-in-Time LLMs (Pitinf-Small, Pitinf-Medium, and frontier-level model Pitinf-Large) from PiT-Inference. Results reveal significant lookahead bias in standard LLMs, as measured with alpha decay, unlike Pitinf models, which demonstrate improved generalization and reasoning abilities as they scale in size. This work establishes a foundation for the standardized evaluation of temporal bias in financial LLMs and provides a practical framework for identifying models suitable for real-world deployment. Code is available on GitHub: https://github.com/benstaf/lookaheadbench

</details>


### [154] [Virtual Urbanism: An AI-Driven Framework for Quantifying Urban Identity. A Tokyo-Based Pilot Study Using Diffusion-Generated Synthetic Environments](https://arxiv.org/abs/2601.13846)
*Glinskaya Maria*

Main category: cs.AI

TL;DR: 本文提出Virtual Urbanism（VU）框架，利用多模态AI通过合成城市复制品量化城市身份，以东京九个区域为案例验证可行性，获得约81%的识别准确率。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏可计算的城市身份度量方法，需要开发能够量化城市身份的分析框架，以支持AI增强的城市分析。

Method: 整合Stable Diffusion和LoRA模型构建合成城市复制品管道，生成东京九个区域的动态合成城市序列，排除现有方向标记以提取核心身份形成元素，通过人类评估实验验证感知合法性、量化区域级身份、推导核心身份形成元素。

Result: 合成复制品获得约81%的平均识别准确率，验证了其有效性；Urban Identity Level（UIL）指标能够评估不同区域的身份水平；语义分析揭示了文化嵌入的类型学作为核心身份形成元素。

Conclusion: VU是一个可行的AI增强城市分析框架，为自动化、多参数身份度量提供了路径，能够通过合成城市复制品量化城市身份。

Abstract: This paper introduces Virtual Urbanism (VU), a multimodal AI-driven analytical framework for quantifying urban identity through the medium of synthetic urban replicas. The framework aims to advance computationally tractable urban identity metrics. To demonstrate feasibility, the pilot study Virtual Urbanism and Tokyo Microcosms is presented. A pipeline integrating Stable Diffusion and LoRA models was used to produce synthetic replicas of nine Tokyo areas rendered as dynamic synthetic urban sequences, excluding existing orientation markers to elicit core identity-forming elements. Human-evaluation experiments (I) assessed perceptual legitimacy of replicas; (II) quantified area-level identity; (III) derived core identity-forming elements. Results showed a mean identification accuracy of ~81%, confirming the validity of the replicas. Urban Identity Level (UIL) metric enabled assessment of identity levels across areas, while semantic analysis revealed culturally embedded typologies as core identity-forming elements, positioning VU as a viable framework for AI-augmented urban analysis, outlining a path toward automated, multi-parameter identity metrics.

</details>


### [155] [Human Simulation Computation: A Human-Inspired Framework for Adaptive AI Systems](https://arxiv.org/abs/2601.13887)
*Hong Su*

Main category: cs.AI

TL;DR: 提出人类模拟计算框架，通过闭环的思考-行动-学习-反思-调度过程，使AI系统能够主动参与环境交互并自我改进推理机制。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型仅依赖文本数据，限制了其在开放动态现实环境中的适应能力、验证推理结果和有效操作能力。

Method: 提出人类模拟计算框架，将智能建模为包含思考、行动、学习、反思和活动调度的连续闭环过程，强调在内部推理过程和环境交互中的主动参与，并融入人类常用思维策略。

Result: 通过理论分析表明，人类模拟策略无法仅从语言材料中完全学习，类人推理过程和基于行动的推理方法对于在现实环境中实现稳健适应和有效交互至关重要。

Conclusion: 人类模拟计算框架为构建能够在开放动态现实环境中有效运作的AI系统提供了理论基础，强调了主动环境参与和自我改进机制的重要性。

Abstract: Large language models (LLMs) have demonstrated strong capabilities in knowledge representation and reasoning based on textual data. However, their reliance on language material alone limits their ability to adapt, verify reasoning outcomes, and operate effectively in open and dynamic real-world environments. In this paper, we propose Human Simulation Computation (HSC), a human-inspired computational framework that models intelligence as a continuous, closed-loop process involving thinking, action, learning, reflection, and activity scheduling, collectively referred to as the internal reasoning process. HSC emphasizes active participation both within the internal reasoning process and in interactions with the environment, where actions are used not only to achieve goals but also to automatically refine and improve internal reasoning mechanisms without external intervention. Furthermore, HSC incorporates commonly used human thinking strategies across all stages of the internal reasoning process, such as main-feature-oriented reasoning, scope expansion through action, and on-time learning driven by environmental feedback. Through theoretical analysis, we argue that human simulation strategies cannot be fully learned from language material alone, and that human-like reasoning processes and action-grounded reasoning methods are essential for robust adaptation and effective interaction with real-world environments.

</details>


### [156] [PREFAB: PREFerence-based Affective Modeling for Low-Budget Self-Annotation](https://arxiv.org/abs/2601.13904)
*Jaeyoung Moon,Youjin Choi,Yucheon Park,David Melhart,Georgios N. Yannakakis,Kyung-Joong Kim*

Main category: cs.AI

TL;DR: PREFAB是一种低成本的回顾性自标注方法，针对情感变化区域而非完整标注，通过偏好学习模型检测相对情感变化，减少标注工作量同时保持质量。


<details>
  <summary>Details</summary>
Motivation: 现有情感状态标注方法通常需要用户在整个会话期间连续标注，虽然能获得细粒度数据，但耗时、认知负担重、容易疲劳和出错。需要一种更高效的标注方法。

Method: 基于峰值-结束规则和情感序数表示，采用偏好学习模型检测相对情感变化，指导标注者只标注选定片段，其余部分通过插值完成。还引入了预览机制提供上下文线索辅助标注。

Result: PREFAB在建模情感变化方面优于基线方法，同时减轻了工作负担（有条件地减轻时间负担）。重要的是，PREFAB提高了标注者信心且未降低标注质量。

Conclusion: PREFAB是一种有效的低预算情感标注方法，通过聚焦情感变化区域和智能选择标注片段，在保持标注质量的同时显著减轻标注负担。

Abstract: Self-annotation is the gold standard for collecting affective state labels in affective computing. Existing methods typically rely on full annotation, requiring users to continuously label affective states across entire sessions. While this process yields fine-grained data, it is time-consuming, cognitively demanding, and prone to fatigue and errors. To address these issues, we present PREFAB, a low-budget retrospective self-annotation method that targets affective inflection regions rather than full annotation. Grounded in the peak-end rule and ordinal representations of emotion, PREFAB employs a preference-learning model to detect relative affective changes, directing annotators to label only selected segments while interpolating the remainder of the stimulus. We further introduce a preview mechanism that provides brief contextual cues to assist annotation. We evaluate PREFAB through a technical performance study and a 25-participant user study. Results show that PREFAB outperforms baselines in modeling affective inflections while mitigating workload (and conditionally mitigating temporal burden). Importantly PREFAB improves annotator confidence without degrading annotation quality.

</details>


### [157] [Paper2Rebuttal: A Multi-Agent Framework for Transparent Author Response Assistance](https://arxiv.org/abs/2601.14171)
*Qianli Ma,Chang Guo,Zhiheng Tian,Siyu Wang,Jipeng Xiao,Yuanhao Yue,Zhipeng Zhang*

Main category: cs.AI

TL;DR: RebuttalAgent是一个多智能体框架，将反驳信生成重构为以证据为中心的规划任务，通过分解评审意见、构建混合上下文和集成外部搜索，确保每个论点都有明确的证据支撑。


<details>
  <summary>Details</summary>
Motivation: 当前的反驳信生成方法通常将其视为直接文本生成问题，存在幻觉、忽略批评意见和缺乏可验证基础等问题。需要一种能够精确对齐评审意图和稿件细节的解决方案。

Method: 提出RebuttalAgent多智能体框架：1) 将复杂反馈分解为原子化关注点；2) 动态构建混合上下文，综合压缩摘要和高保真文本；3) 集成自主按需外部搜索模块，解决需要外部文献支持的关注点；4) 在起草前生成可检查的响应计划。

Result: 在提出的RebuttalBench上验证，RebuttalAgent在覆盖率、忠实度和策略连贯性方面优于强基线方法，为同行评审过程提供了透明且可控的助手。

Conclusion: RebuttalAgent通过将反驳信生成重构为证据中心的规划任务，解决了现有方法的局限性，提供了更可靠、透明和可控的同行评审辅助工具。

Abstract: Writing effective rebuttals is a high-stakes task that demands more than linguistic fluency, as it requires precise alignment between reviewer intent and manuscript details. Current solutions typically treat this as a direct-to-text generation problem, suffering from hallucination, overlooked critiques, and a lack of verifiable grounding. To address these limitations, we introduce $\textbf{RebuttalAgent}$, the first multi-agents framework that reframes rebuttal generation as an evidence-centric planning task. Our system decomposes complex feedback into atomic concerns and dynamically constructs hybrid contexts by synthesizing compressed summaries with high-fidelity text while integrating an autonomous and on-demand external search module to resolve concerns requiring outside literature. By generating an inspectable response plan before drafting, $\textbf{RebuttalAgent}$ ensures that every argument is explicitly anchored in internal or external evidence. We validate our approach on the proposed $\textbf{RebuttalBench}$ and demonstrate that our pipeline outperforms strong baselines in coverage, faithfulness, and strategic coherence, offering a transparent and controllable assistant for the peer review process. Code will be released.

</details>


### [158] [Toward Efficient Agents: Memory, Tool learning, and Planning](https://arxiv.org/abs/2601.14192)
*Xiaofang Yang,Lijun Li,Heng Zhou,Tong Zhu,Xiaoye Qu,Yuchen Fan,Qianshan Wei,Rui Ye,Li Kang,Yiran Qin,Zhiqiang Kou,Daizong Liu,Qi Li,Ning Ding,Siheng Chen,Jing Shao*

Main category: cs.AI

TL;DR: 本文系统性地研究了智能体系统的效率问题，从内存、工具学习和规划三个核心组件出发，分析了延迟、token数、步骤数等成本，提出了在固定成本预算下比较效果和在可比效果水平下比较成本的两种效率评估方法。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型向智能体系统扩展，虽然智能体的效果不断提升，但实际部署中至关重要的效率问题却经常被忽视。本文旨在对智能体系统本身的效率进行全面研究。

Method: 从智能体的三个核心组件（内存、工具学习、规划）出发，综述了多种提高效率的方法，包括通过压缩和管理限制上下文、设计强化学习奖励以减少工具调用、采用受控搜索机制等。提出了两种互补的效率评估方式：固定成本预算下的效果比较和可比效果水平下的成本比较。

Result: 总结了这些组件的高效实现原则，建立了效率与效果的帕累托前沿分析框架，整理了面向效率的基准测试和评估协议，并汇总了常用的效率指标。

Conclusion: 本文为智能体系统的效率研究提供了系统性的分析框架，识别了关键挑战并指出了未来研究方向，旨在为这一重要但被忽视的领域提供有价值的见解。

Abstract: Recent years have witnessed increasing interest in extending large language models into agentic systems. While the effectiveness of agents has continued to improve, efficiency, which is crucial for real-world deployment, has often been overlooked. This paper therefore investigates efficiency from three core components of agents: memory, tool learning, and planning, considering costs such as latency, tokens, steps, etc. Aimed at conducting comprehensive research addressing the efficiency of the agentic system itself, we review a broad range of recent approaches that differ in implementation yet frequently converge on shared high-level principles including but not limited to bounding context via compression and management, designing reinforcement learning rewards to minimize tool invocation, and employing controlled search mechanisms to enhance efficiency, which we discuss in detail. Accordingly, we characterize efficiency in two complementary ways: comparing effectiveness under a fixed cost budget, and comparing cost at a comparable level of effectiveness. This trade-off can also be viewed through the Pareto frontier between effectiveness and cost. From this perspective, we also examine efficiency oriented benchmarks by summarizing evaluation protocols for these components and consolidating commonly reported efficiency metrics from both benchmark and methodological studies. Moreover, we discuss the key challenges and future directions, with the goal of providing promising insights.

</details>
