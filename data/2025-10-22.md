<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 33]
- [cs.CR](#cs.CR) [Total: 34]
- [cs.AI](#cs.AI) [Total: 47]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [AI Exchange Platforms](https://arxiv.org/abs/2510.17839)
*Johannes Schneider,Rene Abraham*

Main category: cs.SE

TL;DR: 该论文提出了一个AI模型交换平台的分类框架，分析了平台的关键维度和特征，揭示了研究机构与组织之间的互动模式，为实践者和学者理解AI交换平台的挑战与机遇提供了基础。


<details>
  <summary>Details</summary>
Motivation: 随着基础模型的兴起，AI模型交换平台对组织效能和适应性变得至关重要，但目前缺乏对这些平台进行分类和理解的综合框架。

Method: 开发了一个分类法来系统化地分类AI交换平台，考察关键维度和特征，并分析公共研究机构与组织之间的互动模式。

Result: 识别了利用同行评审进行质量控制的平台，以及提供在线测试、部署和模型定制机制的平台。揭示了AI交换平台中不同参与者之间的互动模式。

Conclusion: 该分类法为理解AI模型交换的动态发展提供了重要基础，强调了平台设计中适应性和创新的重要性，并为未来研究指明了方向。

Abstract: The rapid integration of Artificial Intelligence (AI) into organizational
technology frameworks has transformed how organizations engage with AI-driven
models, influencing both operational performance and strategic innovation. With
the advent of foundation models, the importance of structured platforms for AI
model exchange has become paramount for organizational efficacy and
adaptability. However, a comprehensive framework to categorize and understand
these platforms remains underexplored. To address this gap, our taxonomy
provides a structured approach to categorize AI exchange platforms, examining
key dimensions and characteristics, as well as revealing interesting
interaction patterns between public research institutions and organizations:
Some platforms leverage peer review as a mechanism for quality control, and
provide mechanisms for online testing, deploying, and customization of models.
Our paper is beneficial to practitioners seeking to understand challenges and
opportunities that arise from AI exchange platforms. For academics, the
taxonomy serves as a foundation for further research into the evolution,
impact, and best practices associated with AI model sharing and utilization in
different contexts. Additionally, our study provides insights into the evolving
role of AI in various industries, highlighting the importance of adaptability
and innovation in platform design. This paper serves as a critical resource for
understanding the dynamic interplay between technology, business models, and
user engagement in the rapidly growing domain of AI model exchanges pointing
also towards possible future evolution.

</details>


### [2] [Vibe Coding: Toward an AI-Native Paradigm for Semantic and Intent-Driven Programming](https://arxiv.org/abs/2510.17842)
*Vinay Bamil*

Main category: cs.SE

TL;DR: 本文介绍了vibe coding这一新兴的AI原生编程范式，开发者通过指定高级功能意图和定性描述来生成软件，并提出了参考架构和实现方案。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型的进展使得开发者能够通过与AI系统对话而非直接编写代码来生成软件，这催生了新的编程范式需求。

Method: 提出了vibe coding的定义和参考架构，包括意图解析器、语义嵌入引擎、代理代码生成器和交互式反馈循环，并描述了假设实现方案。

Result: 比较了vibe coding与声明式、函数式和基于提示的编程，分析了其对软件工程、人机协作和负责任AI实践的影响。

Conclusion: 虽然vibe coding报告了生产力提升和民主化效应，但也存在对齐、可复现性、偏见、可解释性、可维护性和安全性等关键挑战，需要进一步研究。

Abstract: Recent advances in large language models have enabled developers to generate
software by conversing with artificial intelligence systems rather than writing
code directly. This paper introduces vibe coding, an emerging AI-native
programming paradigm in which a developer specifies high-level functional
intent along with qualitative descriptors of the desired "vibe" (tone, style,
or emotional resonance). An intelligent agent then transforms those
specifications into executable software. We formalize the definition of vibe
coding and propose a reference architecture that includes an intent parser, a
semantic embedding engine, an agentic code generator, and an interactive
feedback loop. A hypothetical implementation is described. We compare vibe
coding with declarative, functional, and prompt-based programming, and we
discuss its implications for software engineering, human-AI collaboration, and
responsible AI practice. Finally, we examine reported productivity gains and
democratizing effects, review recent studies that highlight vulnerabilities and
potential slowdowns, identify key challenges such as alignment,
reproducibility, bias, explainability, maintainability, and security, and
outline future directions and open research questions.

</details>


### [3] [Smart Contracts Formal Verification: A Systematic Literature Review](https://arxiv.org/abs/2510.17865)
*Rene Davila,Everardo Barcenas,Rocio Aldeco-Perez*

Main category: cs.SE

TL;DR: 本文对智能合约的形式化验证进行了系统综述，并提出了一种基于描述逻辑的替代性形式化验证方法。


<details>
  <summary>Details</summary>
Motivation: 智能合约作为运行在区块链平台上的自执行合约，经常在其操作或规范中存在显著错误，这促使了对相关工作的深入研究。

Method: 通过审查各种来源发表的相关文献，详细分析了规范、验证工具和相关实验，然后提出了一种基于描述逻辑的形式化验证方法。

Result: 系统梳理了智能合约形式化验证的研究现状，包括现有规范、验证工具和实验成果。

Conclusion: 基于描述逻辑的形式化验证方法为智能合约验证提供了可行的替代方案。

Abstract: Formal verification entails testing software to ensure it operates as
specified. Smart contracts are self-executing contracts with the terms of the
agreement directly written into lines of code. They run on blockchain platforms
and automatically enforce and execute the terms of an agreement when meeting
predefined conditions. However, Smart Contracts, as software models, often
contain notable errors in their operation or specifications. This observation
prompts us to conduct a focused study examining related works published across
various sources. These publications detail specifications, verification tools,
and relevant experiments. Subsequently, this survey proposes an alternative
formal verification based on description logic.

</details>


### [4] [UniCode: A Framework for Generating High Quality Competitive Coding Problems](https://arxiv.org/abs/2510.17868)
*Xinyue Zheng,Haowei Lin,Shaofei Cai,Zilong Zheng,Yitao Liang*

Main category: cs.SE

TL;DR: UniCode框架通过LLM自动生成高质量算法问题和抗污染的测试用例，解决了传统编程基准测试的数据污染和可扩展性问题。


<details>
  <summary>Details</summary>
Motivation: 传统编程基准测试依赖静态人工编写的问题，存在数据污染和可扩展性限制的问题，需要动态生成高质量评估数据集。

Method: 采用生物进化启发的方法，通过三种策略（单问题扩展、同类型融合、跨类型融合）多样化问题，并设计压力驱动的测试用例合成管道，结合暴力基础和共识验证机制。

Result: 构建了包含492个问题的基准测试，评估19个最先进LLM，其中表现最好的o4-mini模型通过率仅为70.3%，证明UniCode具有高度挑战性和区分度。

Conclusion: UniCode为编程领域提供了可扩展且可靠的动态评估数据集生成解决方案。

Abstract: The reliance of competitive coding benchmarks on static, human-authored
problems creates significant challenges, including data contamination and
limited scalability. To address these issues, we introduce UniCode, a novel
framework that automatically generates high-quality algorithmic problems
alongside robust, contamination-resistant test cases. Inspired by biological
evolution that creates better and diverse offspring, our framework leverages
Large Language Models (LLMs) to systematically diversify problems through three
strategies: single problem extension, same-type fusion, and cross-type fusion.
A key innovation is our stress-driven test case synthesis pipeline, which
generates reliable test suites without requiring a canonical ground-truth
solution. This pipeline combines brute-force grounding for small-scale inputs
with a consensus-based validation mechanism for large-scale inputs to ensure
high correctness and coverage. We demonstrate effectiveness of our framework by
curating a benchmark of 492 problems and evaluating 19 state-of-the-art LLMs.
The results reveal that UniCode is highly challenging and discriminative, with
the top-performing model, o4-mini, achieving a pass rate of only 70.3%. Our
framework provides a scalable and reliable solution for generating dynamic
evaluation datasets in coding domain.

</details>


### [5] [Repairing Tool Calls Using Post-tool Execution Reflection and RAG](https://arxiv.org/abs/2510.17874)
*Jason Tsay,Zidane Wright,Gaodan Fang,Kiran Kate,Saurabh Jha,Yara Rizk*

Main category: cs.SE

TL;DR: 开发了一个后工具执行反思组件，结合LLM反思和领域特定的RAG，用于修复kubectl命令执行失败的问题，通过实证研究显示能显著提高执行成功率和正确率。


<details>
  <summary>Details</summary>
Motivation: 代理系统调用工具时经常因各种语法和语义原因失败，一些语义错误只能在分析工具响应后才能识别和解决，需要开发修复机制。

Method: 开发后工具执行反思组件，结合LLM反思和领域特定的RAG，使用工具描述文档和故障排除文档，以kubectl工具管理Kubernetes为用例。

Result: RAG反思修复使kubectl命令执行成功率提高55%，正确回答用户查询的可能性平均提高36%，故障排除文档比官方文档平均提高10%的通过率。

Conclusion: 基于RAG的反思方法能有效修复工具调用错误，显著提高执行成功率和正确性，故障排除文档对修复效果有重要贡献。

Abstract: Agentic systems interact with external systems by calling tools such as
Python functions, REST API endpoints, or command line tools such as kubectl in
Kubernetes. These tool calls often fail for various syntactic and semantic
reasons. Some less obvious semantic errors can only be identified and resolved
after analyzing the tool's response. To repair these errors, we develop a
post-tool execution reflection component that combines large language model
(LLM)-based reflection with domain-specific retrieval-augmented generation
(RAG) using documents describing both the specific tool being called and
troubleshooting documents related to the tool. For this paper, we focus on the
use case of the kubectl command line tool to manage Kubernetes, a platform for
orchestrating cluster applications. Through a larger empirical study and a
smaller manual evaluation, we find that our RAG-based reflection will repair
kubectl commands such that they are both more likely to successfully execute
(pass rate) for 55% of our models evaluated and 36% more likely to correctly
answer the user query on average. We find that troubleshooting documents
improve pass rate compared to official documentation by an average of 10%.

</details>


### [6] [TritonRL: Training LLMs to Think and Code Triton Without Cheating](https://arxiv.org/abs/2510.17891)
*Jiin Woo,Shaowei Zhu,Allen Nie,Zhen Jia,Yida Wang,Youngsuk Park*

Main category: cs.SE

TL;DR: TritonRL是一个专门用于Triton内核生成的领域专用大语言模型，通过包含监督微调和强化学习的新训练框架，解决了数据稀缺和评估标准不完整的问题，在KernelBench上实现了最先进的正确性和加速效果。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型的快速发展，对自动化高性能系统内核的需求成为加速开发和部署的关键推动因素。Triton内核生成面临数据稀缺和不完整评估标准的独特挑战，容易受到奖励攻击的影响。

Method: 通过监督微调在精选数据集上蒸馏Triton特定知识，然后通过强化学习使用可验证奖励和分层奖励分配来改进代码质量。RL框架能够检测奖励攻击，并通过细粒度验证和分层奖励分解来指导推理轨迹和代码标记。

Result: 在KernelBench上的实验表明，TritonRL实现了最先进的正确性和加速效果，超越了所有其他Triton专用模型。

Conclusion: 该研究证明了基于强化学习的训练范式的有效性，能够生成真正替代现有模块的高质量Triton内核。

Abstract: With the rapid evolution of large language models (LLMs), the demand for
automated, high-performance system kernels has emerged as a key enabler for
accelerating development and deployment. We introduce TritonRL, a
domain-specialized LLM for Triton kernel generation, trained with a novel
training framework that enables robust and automated kernel synthesis. Unlike
general-purpose programming languages, Triton kernel generation faces unique
challenges due to data scarcity and incomplete evaluation criteria, vulnerable
to reward hacking. Our approach addresses these challenges end-to-end by
distilling Triton-specific knowledge through supervised fine-tuning on curated
datasets, and further improving code quality via reinforcement learning (RL)
with robust, verifiable rewards and hierarchical reward assignment. Our RL
framework robustly detects reward hacking and guides both reasoning traces and
code tokens through fine-grained verification and hierarchical reward
decomposition, enabling the model to generate high-quality Triton kernels that
can truly replace existing modules. With robust and fine-grained evaluation,
our experiments on KernelBench demonstrate that TritonRL achieves
state-of-the-art correctness and speedup, surpassing all other Triton-specific
models and underscoring the effectiveness of our RL-based training paradigm.

</details>


### [7] [A Systematic Literature Review of the Use of GenAI Assistants for Code Comprehension: Implications for Computing Education Research and Practice](https://arxiv.org/abs/2510.17894)
*Yunhan Qiao,Md Istiak Hossain Shihab,Christopher Hundhausen*

Main category: cs.SE

TL;DR: 本文通过系统文献综述分析了2022-2024年间31项研究，探讨了GenAI在代码理解中的应用，发现虽然GenAI工具有潜力，但常产生不准确或模糊的解释，且新手程序员难以制定有效提示。


<details>
  <summary>Details</summary>
Motivation: 随着程序员越来越多地依赖GenAI助手开发代码解决方案，理解GenAI生成的代码变得至关重要，以便验证其适当性并正确集成到现有代码中。同时，GenAI工具也被用于提供代码解释，这对计算教育提出了新的挑战和机遇。

Method: 采用系统文献综述方法，分析2022-2024年间31项关于利用GenAI增强代码理解的研究，对这些研究进行分类、识别研究方法并总结实证评估结果。

Result: 研究发现GenAI助手经常产生不准确或不清楚的解释，新手程序员在制定有效提示方面存在困难，这阻碍了他们利用GenAI辅助代码理解的能力。

Conclusion: 研究为计算教育工作者提供了基于证据的指导，指出了GenAI在代码理解中的应用挑战，并确定了未来研究方向，包括改进GenAI解释的准确性和帮助学习者更好地使用这些工具。

Abstract: The ability to comprehend code has long been recognized as an essential skill
in software engineering. As programmers lean more heavily on generative
artificial intelligence (GenAI) assistants to develop code solutions, it is
becoming increasingly important for programmers to comprehend GenAI solutions
so that they can verify their appropriateness and properly integrate them into
existing code. At the same time, GenAI tools are increasingly being enlisted to
provide programmers with tailored explanations of code written both by GenAI
and humans. Thus, in computing education, GenAI presents new challenges and
opportunities for learners who are trying to comprehend computer programs. To
provide computing educators with evidence-based guidance on the use of GenAI to
facilitate code comprehension and to identify directions for future research,
we present a systematic literature review (SLR) of state-of-the-art approaches
and tools that leverage GenAI to enhance code comprehension. Our SLR focuses on
31 studies published between 2022 and 2024. Despite their potential, GenAI
assistants often yield inaccurate or unclear explanations, and novice
programmers frequently struggle to craft effective prompts, thereby impeding
their ability to leverage GenAI to aid code comprehension. Our review
classifies GenAI-based approaches and tools, identifies methods used to study
them, and summarizes the empirical evaluations of their effectiveness. We
consider the implications of our findings for computing education research and
practice, and identify directions for future research.

</details>


### [8] [SpecAgent: A Speculative Retrieval and Forecasting Agent for Code Completion](https://arxiv.org/abs/2510.17925)
*George Ma,Anurag Koul,Qi Chen,Yawen Wu,Sachit Kuhar,Yu Yu,Aritra Sengupta,Varun Kumar,Murali Krishna Ramanathan*

Main category: cs.SE

TL;DR: SpecAgent通过索引时异步探索仓库文件构建推测性上下文，在降低推理延迟的同时提升代码生成质量，相比基线方法获得9-11%的绝对性能提升。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在真实软件仓库中表现不佳，因为项目特定API和跨文件依赖关系很重要。检索增强方法在推理时注入仓库上下文，但低延迟预算会影响检索质量或用户体验。

Method: SpecAgent在索引时主动探索仓库文件，构建推测性上下文来预测每个文件中的未来编辑。这种索引时异步性允许充分计算上下文、掩盖延迟，推测性上下文性质提升了代码生成质量。

Result: 实验显示SpecAgent相比性能最佳基线方法获得9-11%的绝对性能提升（48-58%相对提升），同时显著降低推理延迟。

Conclusion: SpecAgent通过索引时构建推测性上下文的方法，有效解决了LLM在软件仓库中的性能问题，在保持低延迟的同时显著提升了代码生成质量。

Abstract: Large Language Models (LLMs) excel at code-related tasks but often struggle
in realistic software repositories, where project-specific APIs and cross-file
dependencies are crucial. Retrieval-augmented methods mitigate this by
injecting repository context at inference time. The low inference-time latency
budget affects either retrieval quality or the added latency adversely impacts
user experience. We address this limitation with SpecAgent, an agent that
improves both latency and code-generation quality by proactively exploring
repository files during indexing and constructing speculative context that
anticipates future edits in each file. This indexing-time asynchrony allows
thorough context computation, masking latency, and the speculative nature of
the context improves code-generation quality. Additionally, we identify the
problem of future context leakage in existing benchmarks, which can inflate
reported performance. To address this, we construct a synthetic, leakage-free
benchmark that enables a more realistic evaluation of our agent against
baselines. Experiments show that SpecAgent consistently achieves absolute gains
of 9-11% (48-58% relative) compared to the best-performing baselines, while
significantly reducing inference latency.

</details>


### [9] [From Charts to Code: A Hierarchical Benchmark for Multimodal Models](https://arxiv.org/abs/2510.17932)
*Jiahao Tang,Henry Hengyuan Zhao,Lijian Wu,Yifei Tao,Dongxing Mao,Yang Wan,Jingru Tan,Min Zeng,Min Li,Alex Jinpeng Wang*

Main category: cs.SE

TL;DR: Chart2Code是一个新的基准测试，用于评估大型多模态模型的图表理解和代码生成能力，包含三个难度级别：图表复制、图表编辑和长表格到图表生成。


<details>
  <summary>Details</summary>
Motivation: 从用户驱动视角设计，捕捉真实世界场景并系统性地增加任务难度，填补了现有基准测试在图表到代码转换方面的空白。

Method: 构建包含2,023个任务的层次化基准测试，涵盖22种图表类型，采用多级评估指标评估代码正确性和渲染图表的视觉保真度。

Result: 对25个最先进的多模态模型进行测试，结果显示即使是GPT-5在编辑任务中的代码评估平均得分仅为0.57，图表质量评估得分仅为0.22。

Conclusion: Chart2Code基准测试具有挑战性，将推动多模态推理的进步，促进更强大和通用的多模态模型的发展。

Abstract: We introduce Chart2Code, a new benchmark for evaluating the chart
understanding and code generation capabilities of large multimodal models
(LMMs). Chart2Code is explicitly designed from a user-driven perspective,
capturing diverse real-world scenarios and progressively increasing task
difficulty. It consists of three levels: Level 1 (Chart Reproduction)
reproduces charts from a reference figure and user query; Level 2 (Chart
Editing) involves complex modifications such as changing chart types or adding
elements; and Level 3 (Long-Table to Chart Generation) requires models to
transform long, information-dense tables into faithful charts following user
instructions. To our knowledge, this is the first hierarchical benchmark that
reflects practical chart2code usage while systematically scaling task
complexity. In total, Chart2Code contains 2,023 tasks across 22 chart types,
paired with multi-level evaluation metrics that assess both code correctness
and the visual fidelity of rendered charts. We benchmark 25 state-of-the-art
(SoTA) LMMs, including both proprietary and the latest open-source models such
as GPT-5, Qwen2.5-VL, InternVL3/3.5, MiMo-VL, and Seed-1.6-VL. Experimental
results demonstrate that even the SoTA model GPT-5 averages only 0.57 on
code-based evaluation and 0.22 on chart-quality assessment across the editing
tasks, underscoring the difficulty of Chart2Code. We anticipate this benchmark
will drive advances in multimodal reasoning and foster the development of more
robust and general-purpose LMMs. Our code and data are available on Chart2Code.

</details>


### [10] [JunoBench: A Benchmark Dataset of Crashes in Python Machine Learning Jupyter Notebooks](https://arxiv.org/abs/2510.18013)
*Yiran Wang,José Antonio Hernández López,Ulf Nilsson,Dániel Varró*

Main category: cs.SE

TL;DR: JunoBench是第一个针对Python机器学习笔记本中真实崩溃的基准数据集，包含111个来自Kaggle笔记本的可复现崩溃案例及其修复方案，涵盖主流ML库和笔记本特有的执行顺序问题。


<details>
  <summary>Details</summary>
Motivation: 由于缺乏专门的基准数据集，机器学习笔记本领域缺少针对性的调试工具。JunoBench旨在填补这一空白，为笔记本环境下的ML代码调试提供现实测试场景。

Method: 从公开Kaggle笔记本中收集真实崩溃案例，经过筛选和整理得到111个可复现的崩溃及其验证修复方案，并提供统一的执行环境确保复现性。

Result: 构建了覆盖TensorFlow/Keras、PyTorch、Scikit-learn、Pandas、NumPy等主流ML库的崩溃数据集，包含笔记本特有的执行顺序问题，所有崩溃和修复都可可靠复现。

Conclusion: JunoBench为笔记本环境下的机器学习开发提供了首个真实崩溃基准，支持针对交互式迭代开发特点的bug检测、定位和修复研究。

Abstract: Jupyter notebooks are widely used for machine learning (ML) prototyping. Yet
few debugging tools are designed for ML code in notebooks, potentially due to
the lack of benchmarks. We introduce JunoBench, the first benchmark dataset of
real-world crashes in Python-based ML notebooks. JunoBench has 111 curated and
reproducible crashes from public Kaggle notebooks, each paired with a
verifiable fix, ranging over popular ML libraries, including TensorFlow/Keras,
PyTorch, Scikit-learn, Pandas, and NumPy, as well as notebook-specific
out-of-order execution issue. To support reproducibility and ease of use,
JunoBench offers a unified execution environment where crashes and fixes can be
reliably reproduced. By providing realistic crashes and their resolutions,
JunoBench facilitates bug detection, localization, and repair tailored to the
interactive and iterative nature of notebook-based ML development.

</details>


### [11] [DIP-AI: A Discovery Framework for AI Innovation Projects](https://arxiv.org/abs/2510.18017)
*Mariana Crisostomo Martins,Lucas Elias Cardoso Rocha,Lucas Cordeiro Romao,Taciana Novo Kudo,Marcos Kalinowski,Renato de Freitas Bulcao-Neto*

Main category: cs.SE

TL;DR: 提出并评估了DIP-AI框架，这是一个专门为AI创新项目早期探索阶段设计的发现问题框架，结合了ISO标准和设计思维方法。


<details>
  <summary>Details</summary>
Motivation: 在AI系统快速发展的背景下，需求工程活动面临数据密集型范式带来的挑战，特别是在AI创新项目中缺乏问题发现的支持。

Method: 基于文献综述，结合ISO 12207、5338标准和设计思维元素，提出DIP-AI框架，并通过产业-学术合作案例研究进行实践评估。

Result: 在AI创新项目的发现阶段应用DIP-AI，结果表明该框架在促进问题发现方面具有相关性和实用性，参与者对其接受度良好。

Conclusion: DIP-AI框架为AI项目的问题发现提供了有效支持，有助于提高交付质量和利益相关者满意度，对学术界和产业界都有重要贡献。

Abstract: Despite the increasing development of Artificial Intelligence (AI) systems,
Requirements Engineering (RE) activities face challenges in this new
data-intensive paradigm. We identified a lack of support for problem discovery
within AI innovation projects. To address this, we propose and evaluate DIP-AI,
a discovery framework tailored to guide early-stage exploration in such
initiatives. Based on a literature review, our solution proposal combines
elements of ISO 12207, 5338, and Design Thinking to support the discovery of AI
innovation projects, aiming at promoting higher quality deliveries and
stakeholder satisfaction. We evaluated DIP-AI in an industry-academia
collaboration (IAC) case study of an AI innovation project, in which
participants applied DIP-AI to the discovery phase in practice and provided
their perceptions about the approach's problem discovery capability,
acceptance, and suggestions. The results indicate that DIP-AI is relevant and
useful, particularly in facilitating problem discovery in AI projects. This
research contributes to academia by sharing DIP-AI as a framework for AI
problem discovery. For industry, we discuss the use of this framework in a real
IAC program that develops AI innovation projects.

</details>


### [12] [A Benchmark Dataset And LLMs Comparison For NFR Classification With Explainable AI](https://arxiv.org/abs/2510.18096)
*Esrat Ebtida Sakib,MD Ahnaf Akib,Md Muktadir Mazumder,Maliha Noushin Raida,Md. Mohsinul Kabir*

Main category: cs.SE

TL;DR: 该论文构建了一个增强的非功能性需求数据集，并使用多种大语言模型对NFR进行分类比较，其中Gemma-2和Phi-3表现最佳。


<details>
  <summary>Details</summary>
Motivation: 手动识别非功能性需求耗时且易出错，需要自动化解决方案，但实现自动化前需要构建稳健全面的数据集。

Method: 从项目章程和开源软件文档收集NFR数据，增强现有数据集，使用RoBERTa、CodeBERT、Gemma-2、Phi-3、Mistral-8B和Llama-3.1-8B等LLM进行分类，并比较其精确率、召回率、F1分数和lime分数。

Result: Gemma-2表现最佳（精确率0.87，召回率0.89，F1分数0.88，lime命中分数78/80），Phi-3紧随其后（精确率0.85，召回率0.87，F1分数0.86，lime命中分数79/80）。

Conclusion: 通过改进上下文基础，该集成增强了模型对技术方面和用户需求的理解，为NFR自动分类提供了有效解决方案。

Abstract: Non-Functional Requirements (NFRs) play a critical role in determining the
overall quality and user satisfaction of software systems. Accurately
identifying and classifying NFRs is essential to ensure that software meets
performance, usability, and reliability expectations. However, manual
identification of NFRs from documentation is time-consuming and prone to
errors, necessitating automated solutions. Before implementing any automated
solution, a robust and comprehensive dataset is essential. To build such a
dataset, we collected NFRs from various Project Charters and Open Source
Software Documentation. This enhanced the technical depth and usability of an
already existing NFR dataset. We categorized NFRs into sub-classes and
identified needs using widely used Large Language Models to facilitate
automation. After classifying the NFRs, we compared the classification results
of the selected LLMs: RoBERTa, CodeBERT, Gemma-2, Phi-3, Mistral-8B, and
Llama-3.1-8B using various evaluation metrics, including precision, recall,
F1-score, and lime scores. Among these models, Gemma-2 achieved the best
results with a precision of 0.87, recall of 0.89, and F1-score of 0.88,
alongside a lime hit score of 78 out of 80. Phi-3 closely followed with a
precision of 0.85, recall of 0.87, F1-score of 0.86, and the highest lime hit
score of 79. By improving the contextual foundation, this integration enhanced
the model's comprehension of technical aspects and user requirements.

</details>


### [13] [BlueCodeAgent: A Blue Teaming Agent Enabled by Automated Red Teaming for CodeGen AI](https://arxiv.org/abs/2510.18131)
*Chengquan Guo,Yuzhou Nie,Chulin Xie,Zinan Lin,Wenbo Guo,Bo Li*

Main category: cs.SE

TL;DR: BlueCodeAgent是一个端到端的蓝队代理，通过自动化红队生成多样化风险实例，结合宪法和代码分析进行多级防御，在代码生成安全领域显著提升风险检测性能。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型在代码生成中的广泛应用，安全风险日益突出。现有研究主要集中在红队攻击，而蓝队防御方面进展有限，需要有效的语义理解来区分安全与不安全代码。

Method: 提出BlueCodeAgent框架，集成红队和蓝队：红队生成多样化风险实例，蓝队代理利用这些实例通过宪法和代码分析检测已知和未知风险场景，结合动态分析减少误报。

Result: 在三个代表性代码相关任务（偏见指令检测、恶意指令检测、易受攻击代码检测）中，BlueCodeAgent相比基础模型和基于安全提示的防御方法取得显著提升，在四个数据集上平均F1分数提高12.7%。

Conclusion: 红队通过持续识别新漏洞来增强防御性能，BlueCodeAgent能够总结可操作的宪法来增强上下文感知的风险检测能力。

Abstract: As large language models (LLMs) are increasingly used for code generation,
concerns over the security risks have grown substantially. Early research has
primarily focused on red teaming, which aims to uncover and evaluate
vulnerabilities and risks of CodeGen models. However, progress on the blue
teaming side remains limited, as developing defense requires effective semantic
understanding to differentiate the unsafe from the safe. To fill in this gap,
we propose BlueCodeAgent, an end-to-end blue teaming agent enabled by automated
red teaming. Our framework integrates both sides: red teaming generates diverse
risky instances, while the blue teaming agent leverages these to detect
previously seen and unseen risk scenarios through constitution and code
analysis with agentic integration for multi-level defense. Our evaluation
across three representative code-related tasks--bias instruction detection,
malicious instruction detection, and vulnerable code detection--shows that
BlueCodeAgent achieves significant gains over the base models and safety
prompt-based defenses. In particular, for vulnerable code detection tasks,
BlueCodeAgent integrates dynamic analysis to effectively reduce false
positives, a challenging problem as base models tend to be over-conservative,
misclassifying safe code as unsafe. Overall, BlueCodeAgent achieves an average
12.7\% F1 score improvement across four datasets in three tasks, attributed to
its ability to summarize actionable constitutions that enhance context-aware
risk detection. We demonstrate that the red teaming benefits the blue teaming
by continuously identifying new vulnerabilities to enhance defense performance.

</details>


### [14] [When Old Meets New: Evaluating the Impact of Regression Tests on SWE Issue Resolution](https://arxiv.org/abs/2510.18270)
*Yang Chen,Toufique Ahmed,Reyhaneh Jabbarvand,Martin Hirzel*

Main category: cs.SE

TL;DR: TestPrune是一种自动化技术，利用问题跟踪报告和策略性地重用回归测试来进行错误复现和补丁验证，通过最小化测试套件来提高LLM调试技术的效率。


<details>
  <summary>Details</summary>
Motivation: 现实项目中的测试套件虽然覆盖率高，但仍无法检测所有错误。回归测试除了确保功能保留外，还可用于调试当前版本，但大型测试套件超出LLM上下文限制、引入噪音并增加推理成本。

Method: TestPrune自动最小化回归测试套件，生成小型高相关性测试子集，可集成到任何基于代理的错误修复流程中，用于错误复现和补丁验证。

Result: 在Otter框架中，问题复现率相对提高6.2%-9.0%；在Agentless框架中，问题解决率相对提高9.4%-12.9%。使用成本极低，每个SWE-Bench实例仅需$0.02-$0.05。

Conclusion: TestPrune通过智能测试套件最小化，有效提升了基于LLM的错误修复管道的性能，成本效益显著。

Abstract: Test suites in real-world projects are often large and achieve high code
coverage, yet they remain insufficient for detecting all bugs. The abundance of
unresolved issues in open-source project trackers highlights this gap. While
regression tests are typically designed to ensure past functionality is
preserved in the new version, they can also serve a complementary purpose:
debugging the current version. Specifically, regression tests can (1) enhance
the generation of reproduction tests for newly reported issues, and (2)
validate that patches do not regress existing functionality. We present
TestPrune, a fully automated technique that leverages issue tracker reports and
strategically reuses regression tests for both bug reproduction and patch
validation.
  A key contribution of TestPrune is its ability to automatically minimize the
regression suite to a small, highly relevant subset of tests. Due to the
predominance of LLM-based debugging techniques, this minimization is essential
as large test suites exceed context limits, introduce noise, and inflate
inference costs. TestPrune can be plugged into any agentic bug repair pipeline
and orthogonally improve overall performance. As a proof of concept, we show
that TestPrune leads to a 6.2%-9.0% relative increase in issue reproduction
rate within the Otter framework and a 9.4% - 12.9% relative increase in issue
resolution rate within the Agentless framework on SWE-Bench Lite and SWE-Bench
Verified benchmarks, capturing fixes that were correctly produced by agents but
not submitted as final patches. Compared to the benefits, the cost overhead of
using TestPrune is minimal, i.e., \$0.02 and \$0.05 per SWE-Bench instance,
using GPT-4o and Claude-3.7-Sonnet models, respectively.

</details>


### [15] [Ensuring Robustness in ML-enabled Software Systems: A User Survey](https://arxiv.org/abs/2510.18292)
*Hala Abdelkader,Mohamed Abdelrazek,Priya Rani,Rajesh Vasa,Jean-Guy Schneider*

Main category: cs.SE

TL;DR: 提出了ML-On-Rails协议，一个统一框架，通过集成OOD检测、对抗攻击检测、输入验证和可解释性等关键保障措施，增强生产环境中ML系统的鲁棒性和可信度。


<details>
  <summary>Details</summary>
Motivation: 传统软件工程实践依赖预定义逻辑，无法应对ML组件的数据依赖和概率决策特性，导致静默故障、OOD数据和对抗攻击等关键挑战。

Method: 设计ML-On-Rails协议，包含模型-软件通信框架（使用HTTP状态码报告结果和错误），并通过从业者调查验证现实世界挑战。

Result: 调查揭示了主要的鲁棒性问题、当前解决方案的差距，并证明标准化协议如ML-On-Rails可以改善系统鲁棒性。

Conclusion: 需要为ML系统工程师提供更多支持和资源，并基于调查和实际应用洞察持续改进协议有效性。

Abstract: Ensuring robustness in ML-enabled software systems requires addressing
critical challenges, such as silent failures, out-of-distribution (OOD) data,
and adversarial attacks. Traditional software engineering practices, which rely
on predefined logic, are insufficient for ML components that depend on data and
probabilistic decision-making. To address these challenges, we propose the
ML-On-Rails protocol, a unified framework designed to enhance the robustness
and trustworthiness of ML-enabled systems in production. This protocol
integrates key safeguards such as OOD detection, adversarial attack detection,
input validation, and explainability. It also includes a model-to-software
communication framework using HTTP status codes to enhance transparency in
reporting model outcomes and errors. To align our approach with real-world
challenges, we conducted a practitioner survey, which revealed major robustness
issues, gaps in current solutions, and highlighted how a standardised protocol
such as ML-On-Rails can improve system robustness. Our findings highlight the
need for more support and resources for engineers working with ML systems.
Finally, we outline future directions for refining the proposed protocol,
leveraging insights from the survey and real-world applications to continually
enhance its effectiveness.

</details>


### [16] [InspectCoder: Dynamic Analysis-Enabled Self Repair through interactive LLM-Debugger Collaboration](https://arxiv.org/abs/2510.18327)
*Yunkun Wang,Yue Zhang,Guochang Li,Chen Zhi,Binhua Li,Fei Huang,Yongbin Li,Shuiguang Deng*

Main category: cs.SE

TL;DR: InspectCoder是首个基于智能代理的程序修复系统，通过交互式调试器控制让LLM进行动态分析，显著提升代码修复准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 现有LLM自我修复方法依赖静态语义分析或浅层执行日志，缺乏深入运行时行为分析，无法有效诊断复杂逻辑错误的根本原因。

Method: 采用双代理框架，通过战略断点放置、目标状态检查和增量运行时实验，在状态化调试会话中进行自适应运行时状态检查和扰动。

Result: 在两个挑战性自我修复基准测试中，修复准确率相对最强基线提升5.10%-60.37%，bug修复效率提升1.67x-2.24x。

Conclusion: LLM驱动的动态分析在自动化软件工程中具有显著潜力，交互式LLM-调试器系统为程序修复提供了新范式。

Abstract: Large Language Models (LLMs) frequently generate buggy code with complex
logic errors that are challenging to diagnose. While existing LLM-based
self-repair approaches conduct intensive static semantic analysis or reply on
superficial execution logs, they miss the in-depth runtime behaviors that often
expose bug root causes-lacking the interactive dynamic analysis capabilities
that make human debugging effective. We present InspectCoder, the first agentic
program repair system that empowers LLMs to actively conduct dynamic analysis
via interactive debugger control. Our dual-agent framework enables strategic
breakpoint placement, targeted state inspection, and incremental runtime
experimentation within stateful debugger sessions. Unlike existing methods that
follow fixed log collection procedures, InspectCoder adaptively inspects and
perturbs relevant intermediate states at runtime, and leverages immediate
process rewards from debugger feedback to guide multi-step reasoning,
transforming LLM debugging paradigm from blind trial-and-error into systematic
root cause diagnosis. We conduct comprehensive experiments on two challenging
self-repair benchmarks: BigCodeBench-R and LiveCodeBench-R. InspectCoder
achieves 5.10%-60.37% relative improvements in repair accuracy over the
strongest baseline, while delivering 1.67x-2.24x superior bug-fix efficiency
respectively. We also contribute InspectWare, an open-source middleware that
abstracts debugger complexities and maintains stateful debugging sessions
across mainstream Python testing frameworks. Our work provides actionable
insight into the interactive LLM-debugger systems, demonstrating the
significant potential of LLM-driven dynamic analysis for automated software
engineering.

</details>


### [17] [Human to Document, AI to Code: Three Case Studies of Comparing GenAI for Notebook Competitions](https://arxiv.org/abs/2510.18430)
*Tasha Settewong,Youmei Fan,Raula Gaikovina Kula,Kenichi Matsumoto*

Main category: cs.SE

TL;DR: 该研究通过三个案例研究分析了Kaggle竞赛中人类编写与AI生成笔记本的差异，发现金牌得主以更长的文档为特征，而AI生成的笔记本代码质量更高但人类笔记本在结构多样性和创新性方面更胜一筹。


<details>
  <summary>Details</summary>
Motivation: 随着生成式AI技术的发展，在竞争环境中区分人类编写与AI生成笔记本的特征变得日益重要，特别是在数据科学领域广泛使用的计算笔记本中。

Method: 研究分析了25个代码和文档特征，比较了人类编写的Kaggle获奖笔记本与AI生成笔记本的差异，通过三个案例研究探索人类和AI在编码和文档编写活动中的优势。

Result: 金牌得主主要通过更长更详细的文档来区分；AI生成笔记本在代码质量（如代码异味和技术债务）方面表现更好，但人类笔记本在结构多样性、复杂性和问题解决创新方法方面更突出。

Conclusion: 该研究为未来探索如何在笔记本中最大化人类与AI协作潜力奠定了基础，提出了四个进一步研究议程。

Abstract: Computational notebooks have become the preferred tool of choice for data
scientists and practitioners to perform analyses and share results. Notebooks
uniquely combine scripts with documentation. With the emergence of generative
AI (GenAI) technologies, it is increasingly important, especially in
competitive settings, to distinguish the characteristics of human-written
versus GenAI.
  In this study, we present three case studies to explore potential strengths
of both humans and GenAI through the coding and documenting activities in
notebooks. We first characterize differences between 25 code and documentation
features in human-written, medal-winning Kaggle notebooks. We find that gold
medalists are primarily distinguished by longer and more detailed
documentation. Second, we analyze the distinctions between human-written and
GenAI notebooks. Our results show that while GenAI notebooks tend to achieve
higher code quality (as measured by metrics like code smells and technical
debt), human-written notebooks display greater structural diversity,
complexity, and innovative approaches to problem-solving. Based on these
results, we envision the work as groundwork that highlight four agendas to
further investigate how GenAI could be utilized in notebooks that maximizes the
potential collaboration between human and AI.

</details>


### [18] [Real-World Usability of Vulnerability Proof-of-Concepts: A Comprehensive Study](https://arxiv.org/abs/2510.18448)
*Wenjing Dang,Kaixuan Li,Sen Chen,Zhenwei Zhuo,Lyuye Zhang,Zheli Liu*

Main category: cs.SE

TL;DR: 该研究首次对概念验证(PoC)进行大规模分析，收集了47万多个PoC样本，评估了其可用性、完整性和可复现性，发现78.9%的CVE漏洞缺乏可用PoC，现有PoC报告缺少约30%关键组件。


<details>
  <summary>Details</summary>
Motivation: PoC研究严重滞后于漏洞数据研究，主要挑战包括PoC分散在不同平台、写作风格多样、复现困难。需要填补这一研究空白。

Method: 1) 从13个平台收集470,921个PoC及其报告；2) 提出组件提取方法，结合模式匹配和BERT-NER模型提取9个关键组件；3) 招募8名参与者手动复现150个漏洞样本。

Result: 78.9%的CVE漏洞缺乏可用PoC，现有PoC报告缺少约30%关键组件，识别了导致PoC复现失败的各种原因。

Conclusion: 提出了增强漏洞PoC可用性的可行策略，以加强软件安全性。

Abstract: The Proof-of-Concept (PoC) for a vulnerability is crucial in validating its
existence, mitigating false positives, and illustrating the severity of the
security threat it poses. However, research on PoCs significantly lags behind
studies focusing on vulnerability data. This discrepancy can be directly
attributed to several challenges, including the dispersion of real-world PoCs
across multiple platforms, the diversity in writing styles, and the difficulty
associated with PoC reproduction. To fill this gap, we conduct the first
large-scale study on PoCs in the wild, assessing their report availability,
completeness, reproducibility. Specifically, 1) to investigate PoC reports
availability for CVE vulnerability, we collected an extensive dataset of
470,921 PoCs and their reports from 13 platforms, representing the broadest
collection of publicly available PoCs to date. 2) To assess the completeness of
PoC report at a fine-grained level, we proposed a component extraction method,
which combines pattern-matching techniques with a fine-tuned BERT-NER model to
extract 9 key components from PoC reports. 3) To evaluate the effectiveness of
PoCs, we recruited 8 participants to manually reproduce 150 sampled
vulnerabilities with 32 vulnerability types based on PoC reports, enabling an
in-depth analysis of PoC reproducibility and the factors influencing it. Our
findings reveal that 78.9% of CVE vulnerabilities lack available PoCs, and
existing PoC reports typically miss about 30% of the essential components
required for effective vulnerability understanding and reproduction, with
various reasons identified for the failure to reproduce vulnerabilities using
available PoC reports. Finally, we proposed actionable strategies for
stakeholders to enhance the overall usability of vulnerability PoCs in
strengthening software security.

</details>


### [19] [Large Language Models in Thematic Analysis: Prompt Engineering, Evaluation, and Guidelines for Qualitative Software Engineering Research](https://arxiv.org/abs/2510.18456)
*Cristina Martinez Montes,Robert Feldt,Cristina Miguel Martos,Sofia Ouhbi,Shweta Premanandan,Daniel Graziotin*

Main category: cs.SE

TL;DR: 本研究开发了一种将大语言模型整合到主题分析中的可重复方法，通过系统评估发现LLM生成的代码61%优于人类，但在潜在解释和主题边界方面存在局限。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏将大语言模型整合到定性研究方法中的可重复方法，且现有研究缺乏对LLM生成输出的系统性质量评估。

Method: 设计并迭代优化了Braun和Clarke反思性主题分析第2-5阶段的提示词，使用15个软件工程师幸福感访谈数据，通过四位专家评估员进行盲评，应用基于Braun和Clarke质量标准的评估标准。

Result: 评估者61%的情况下更偏好LLM生成的代码，认为其分析价值更高，但LLM存在数据过度碎片化、遗漏潜在解释、主题边界不清晰等局限。

Conclusion: 提出了整合LLM到定性分析的可重复方法、实证比较了LLM与人类生成代码的质量，并提供了在保持方法严谨性的同时有效使用LLM的指导原则。

Abstract: As artificial intelligence advances, large language models (LLMs) are
entering qualitative research workflows, yet no reproducible methods exist for
integrating them into established approaches like thematic analysis (TA), one
of the most common qualitative methods in software engineering research.
Moreover, existing studies lack systematic evaluation of LLM-generated
qualitative outputs against established quality criteria. We designed and
iteratively refined prompts for Phases 2-5 of Braun and Clarke's reflexive TA,
then tested outputs from multiple LLMs against codes and themes produced by
experienced researchers. Using 15 interviews on software engineers' well-being,
we conducted blind evaluations with four expert evaluators who applied rubrics
derived directly from Braun and Clarke's quality criteria. Evaluators preferred
LLM-generated codes 61% of the time, finding them analytically useful for
answering the research question. However, evaluators also identified
limitations: LLMs fragmented data unnecessarily, missed latent interpretations,
and sometimes produced themes with unclear boundaries. Our contributions are
threefold. First, a reproducible approach integrating refined, documented
prompts with an evaluation framework to operationalize Braun and Clarke's
reflexive TA. Second, an empirical comparison of LLM- and human-generated codes
and themes in software engineering data. Third, guidelines for integrating LLMs
into qualitative analysis while preserving methodological rigour, clarifying
when and how LLMs can assist effectively and when human interpretation remains
essential.

</details>


### [20] [CodeRL+: Improving Code Generation via Reinforcement with Execution Semantics Alignment](https://arxiv.org/abs/2510.18471)
*Xue Jiang,Yihong Dong,Mengyang Liu,Hongyi Deng,Tian Wang,Yongding Tao,Rongyu Cao,Binhua Li,Zhi Jin,Wenpin Jiao,Fei Huang,Yongbin Li,Ge Li*

Main category: cs.SE

TL;DR: CodeRL+是一种将执行语义对齐集成到代码生成RLVR训练流程中的新方法，通过推断变量级执行轨迹提供直接学习信号，显著提升了代码生成质量。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在代码生成方面表现出色，但其基于文本模式训练与功能正确性之间存在语义鸿沟。现有的RLVR方法仅依赖测试用例的二元通过/失败信号，对于代码中的细微逻辑错误学习效率低下。

Method: 提出CodeRL+方法，在RLVR训练流程中集成执行语义对齐，使模型能够推断变量级执行轨迹，直接学习执行语义。该方法可直接使用现有策略rollouts，并能与各种RL算法无缝集成。

Result: CodeRL+在pass@1指标上相比后训练基线（包括RLVR和蒸馏）实现了4.6%的平均相对提升。在其他编码任务上泛化效果良好，在代码推理和测试输出生成基准上分别获得15.5%和4.4%的准确率提升。

Conclusion: CodeRL+能够有效增强代码文本表示与其底层执行语义之间的对齐，在多种RL算法和LLMs上表现出强大的适用性。

Abstract: While Large Language Models (LLMs) excel at code generation by learning from
vast code corpora, a fundamental semantic gap remains between their training on
textual patterns and the goal of functional correctness, which is governed by
formal execution semantics. Reinforcement Learning with Verifiable Rewards
(RLVR) approaches attempt to bridge this gap using outcome rewards from
executing test cases. However, solely relying on binary pass/fail signals is
inefficient for establishing a well-aligned connection between the textual
representation of code and its execution semantics, especially for subtle
logical errors within the code. In this paper, we propose CodeRL+, a novel
approach that integrates execution semantics alignment into the RLVR training
pipeline for code generation. CodeRL+ enables the model to infer variable-level
execution trajectory, providing a direct learning signal of execution
semantics. CodeRL+ can construct execution semantics alignment directly using
existing on-policy rollouts and integrates seamlessly with various RL
algorithms. Extensive experiments demonstrate that CodeRL+ outperforms
post-training baselines (including RLVR and Distillation), achieving a 4.6%
average relative improvement in pass@1. CodeRL+ generalizes effectively to
other coding tasks, yielding 15.5% and 4.4% higher accuracy on code-reasoning
and test-output-generation benchmarks, respectively. CodeRL+ shows strong
applicability across diverse RL algorithms and LLMs. Furthermore, probe
analyses provide compelling evidence that CodeRL+ strengthens the alignment
between code's textual representations and its underlying execution semantics.

</details>


### [21] [VAPU: System for Autonomous Legacy Code Modernization](https://arxiv.org/abs/2510.18509)
*Valtteri Ala-Salmi,Zeeshan Rasheed,Abdul Malik Sami,Muhammad Waseem,Kai-Kristian Kemell,Jussi Rasku,Mika Saari,Pekka Abrahamsson*

Main category: cs.SE

TL;DR: 提出了一个基于LLM的多智能体系统VAPU，用于自主更新遗留应用程序代码，相比零样本学习和单样本学习提示，在低温度设置下能实现更少的错误和更高的需求满足率。


<details>
  <summary>Details</summary>
Motivation: 遗留应用程序中的过时组件会带来兼容性、安全性和可靠性风险，但高昂的资源成本使公司不愿更新。需要一种成本效益高的自主更新解决方案。

Method: 设计了一个名为VAPU的多智能体系统，模拟软件开发团队中的不同角色，分阶段更新代码文件。扩展评估了5个LLM和温度参数（0-1），并在20个开源Python项目上测试。

Result: 在低温度设置下，VAPU能达到与ZSL/OSL相似的错误数量，但需求满足率更高。在Python文件更新需求方面，VAPU相比ZSL/OSL提示最高提升了22.5%。

Conclusion: 基于LLM的多智能体系统是自主更新遗留应用程序组件的有效解决方案。

Abstract: In this study, we present a solution for the modernization of legacy
applications, an area of code generation where LLM-based multi-agent systems
are proving essential for complex multi-phased tasks. Legacy applications often
contain deprecated components that create compatibility, security, and
reliability risks, but high resource costs make companies hesitate to update.
We take a step forward to integrate an LLM-based multi-agent system as part of
a legacy web application update to provide a cost-effective solution to update
legacy applications autonomously. We propose a multi-agent system named a
Verifying Agent Pipeline Updater (VAPU), which is designed to update code files
in phases while simulating different roles in a software development team. In
our previous study, we evaluated the system for legacy version updates by using
six legacy web application view files by resulting errors and accomplished
requirements. This study extends the previous evaluation of a multi-agent
pipeline system by extending the evaluation of VAPU from a single LLM to five
LLMs and using the temperature parameter in both 0 to 1 settings. Additionally,
we tested the system with 20 open-source Python GitHub projects. The results of
the evaluation were compared to Zero-Shot Learning (ZSL) and One-Shot Learning
(OSL) prompts. The extended evaluation of VAPU showed that particularly in a
low-temperature VAPU can get similar level of error count compared to the
ZSL/OSL prompts but with a higher level of fulfilled requirements, depending on
the LLM. VAPU showed up to 22.5% increase in the succeeding Python file update
requirements compared to ZSL/OSL prompts. The study indicates that an LLM-based
multi-agent system is a capable solution to update components of a legacy
application autonomously.

</details>


### [22] [Mining Service Behavior for Stateful Service Emulation](https://arxiv.org/abs/2510.18519)
*Md Arafat Hossain,Jun Han,Muhammad Ashad Kabir,Steve Versteeg,Jean-Guy Schneider,Jiaojiao Jiang*

Main category: cs.SE

TL;DR: 提出了一种考虑服务状态的服务建模方法，通过分析交互消息间的上下文依赖关系和数据值关系，提高状态化服务响应生成的准确性。


<details>
  <summary>Details</summary>
Motivation: 企业软件系统日益集成多样化服务，测试这些高度互连系统面临访问连接服务的挑战。现有服务虚拟化技术大多忽略服务状态，导致服务仿真准确性不足和测试环境真实性降低。

Method: 从服务交互中推导服务模型，通过揭示交互消息间的上下文依赖关系和分析消息数据值之间的关系，在响应生成时考虑服务状态。

Result: 使用从状态化和无状态服务收集的交互轨迹进行评估，结果显示在服务响应生成方面相比现有方法在准确性和效率上有显著提升。

Conclusion: 提出的考虑服务状态的服务建模方法能够有效提高服务响应生成的准确性，特别适用于状态化服务的测试场景。

Abstract: Enterprise software systems are increasingly integrating with diverse
services to meet expanding business demands. Testing these highly
interconnected systems presents a challenge due to the need for access to the
connected services. Service virtualization has emerged as a widely used
technique to derive service models from recorded interactions, for service
response generation during system testing. Various methods have been proposed
to emulate actual service behavior based on these interactions, but most fail
to account for the service's state, which reduces the accuracy of service
emulation and the realism of the testing environment, especially when dealing
with stateful services. This paper proposes an approach to deriving service
models from service interactions, which enhance the accuracy of response
generation by considering service state. This is achieved by uncovering
contextual dependencies among interaction messages and analyzing the
relationships between message data values. The approach is evaluated using
interaction traces collected from both stateful and stateless services, and the
results reveal notable enhancements in accuracy and efficiency over existing
approaches in service response generation.

</details>


### [23] [Demonstrators for Industrial Cyber-Physical System Research: A Requirements Hierarchy Driven by Software-Intensive Design](https://arxiv.org/abs/2510.18534)
*Uraz Odyurt,Richard Loendersloot,Tiedo Tinga*

Main category: cs.SE

TL;DR: 提出了一个演示器需求细化框架，通过5个层次化的演示级别来评估目标演示的可行性，进行现实调整并帮助描述需求。


<details>
  <summary>Details</summary>
Motivation: 研究项目中演示器主题存在不确定性，提案阶段对演示覆盖范围的详细阐述往往不足，导致目标与可实现结果不匹配，阻碍进展。TRL量表作为松散描述符也无济于事。

Method: 定义5个层次化的演示级别，明确与期望（如工作包交互）和项目工业用例相关联。在软件密集型系统和工业信息物理系统领域应用该框架。

Result: 在两个研究项目中应用该框架（一个在早期阶段，一个在最终阶段），证明了其有效性。完整验证需要4-5年时间，目前无法获得。

Conclusion: 提出的演示器需求细化框架能够有效评估演示可行性，进行现实调整，并帮助清晰描述需求，解决项目演示中的不确定性问题。

Abstract: One of the challenges apparent in the organisation of research projects is
the uncertainties around the subject of demonstrators. A precise and detailed
elicitation of the coverage for project demonstrators is often an afterthought
and not sufficiently detailed during proposal writing. This practice leads to
continuous confusion and a mismatch between targeted and achievable
demonstration of results, hindering progress. The reliance on the TRL scale as
a loose descriptor does not help either. We propose a demonstrator requirements
elaboration framework aiming to evaluate the feasibility of targeted
demonstrations, making realistic adjustments, and assist in describing
requirements. In doing so, we define 5 hierarchical levels of demonstration,
clearly connected to expectations, e.g., work package interaction, and also
connected to the project's industrial use-cases. The considered application
scope in this paper is the domain of software-intensive systems and industrial
cyber-physical systems. A complete validation is not accessible, as it would
require application of our framework at the start of a project and observing
the results at the end, taking 4-5 years. Nonetheless, we have applied it to
two research projects from our portfolio, one at the early and another at the
final stages, revealing its effectiveness.

</details>


### [24] [When Abstraction Breaks Physics: Rethinking Modular Design in Quantum Software](https://arxiv.org/abs/2510.18557)
*Jianjun Zhao*

Main category: cs.SE

TL;DR: 量子软件工程中的抽象机制与量子语义存在根本冲突，需要重新设计基于量子物理约束的抽象方法。


<details>
  <summary>Details</summary>
Motivation: 经典软件工程的抽象原则在量子程序中可能违反量子计算的物理约束，如幺正性、纠缠、不可克隆定理等，需要系统性地重新思考量子软件工程中的抽象机制。

Method: 识别了三种因朴素抽象破坏量子语义的故障案例，提出了一套物理合理的抽象机制设计原则，并建议了量子特定类型系统、效应注解和基于契约的模块设计等研究方向。

Result: 揭示了经典抽象机制在量子环境中的根本局限性，提出了确保量子语义安全性的抽象设计框架。

Conclusion: 量子软件工程需要基于量子语义重新设计抽象机制，考虑工程可扩展性，以解决经典抽象与量子物理约束之间的根本冲突。

Abstract: Abstraction is a fundamental principle in classical software engineering,
which enables modularity, reusability, and scalability. However, quantum
programs adhere to fundamentally different semantics, such as unitarity,
entanglement, the no-cloning theorem, and the destructive nature of
measurement, which introduce challenges to the safe use of classical
abstraction mechanisms. This paper identifies a fundamental conflict in quantum
software engineering: abstraction practices that are syntactically valid may
violate the physical constraints of quantum computation. We present three
classes of failure cases where naive abstraction breaks quantum semantics and
propose a set of design principles for physically sound abstraction mechanisms.
We further propose research directions, including quantum-specific type
systems, effect annotations, and contract-based module design. Our goal is to
initiate a systematic rethinking of abstraction in quantum software
engineering, based on quantum semantics and considering engineering
scalability.

</details>


### [25] [WebDevJudge: Evaluating (M)LLMs as Critiques for Web Development Quality](https://arxiv.org/abs/2510.18560)
*Chunyang Li,Yilun Zheng,Xinting Huang,Tianqing Fang,Jiahao Xu,Yangqiu Song,Lihui Chen,Han Hu*

Main category: cs.SE

TL;DR: WebDevJudge是一个评估LLM作为评判者在网页开发任务中性能的系统性基准，支持非交互式静态评估和连续交互式动态评估，揭示了LLM评判者与人类专家之间的显著差距。


<details>
  <summary>Details</summary>
Motivation: 探索LLM作为评判者在动态环境和复杂交互的开放任务中的可靠性，填补现有研究空白。

Method: 构建包含人类偏好标签的网页实现配对数据集，使用结构化且基于查询的评分标准，全面评估各种评估者（LLM、MLLM、代理工作流）在不同范式和指导机制下的表现。

Result: 实验发现LLM评判者与人类专家存在显著差距，主要源于模型在识别功能等价性、验证任务可行性以及减轻偏见方面的基本限制。

Conclusion: WebDevJudge对LLM作为评判者范式提出了重大挑战，为未来开发更可靠的复杂场景自动评估器提供了指导方向。

Abstract: The paradigm of LLM-as-a-judge is emerging as a scalable and efficient
alternative to human evaluation, demonstrating strong performance on
well-defined tasks. However, its reliability in open-ended tasks with dynamic
environments and complex interactions remains unexplored. To bridge the gap, we
introduce WebDevJudge, a systematic benchmark for assessing LLM-as-a-judge
performance in web development, with support for both non-interactive
evaluation based on static observations and continuous interactive evaluation
with a dynamic web environment. WebDevJudge comprises human preference labels
over paired web implementations, annotated with structured and query-grounded
rubrics to ensure high-quality ground truth. Using this benchmark, we
comprehensively evaluate various evaluators, including LLMs, MLLMs, and agentic
workflows. We systematically investigate the impact of different paradigms and
guidance mechanisms. Our experiments reveal a significant gap between LLM
judges and human experts. In-depth analysis indicates this gap stems from
fundamental model limitations, including failures in recognizing functional
equivalence, verifying task feasibility, and mitigating bias. Overall,
WebDevJudge presents a significant challenge to LLM-as-a-judge, offering
insights to guide future research toward developing more reliable and capable
automated evaluators for complicated scenarios. Code and data are available at
https://github.com/lcy2723/WebDevJudge.

</details>


### [26] [A Structured Evaluation Framework for Low-Code Platform Selection: A Multi-Criteria Decision Model for Enterprise Digital Transformation](https://arxiv.org/abs/2510.18590)
*Antonio Lamanna*

Main category: cs.SE

TL;DR: 提出基于五个关键标准的低代码开发平台评估框架，包含加权评分模型，帮助企业根据特定需求定量评估和比较不同平台。


<details>
  <summary>Details</summary>
Motivation: 低代码开发平台的快速普及产生了对系统评估方法的需求，以帮助企业做出明智的平台选择决策。

Method: 建立基于五个关键标准（业务流程编排、UI/UX定制、集成与互操作性、治理与安全、AI增强自动化）的综合评估框架，并提出加权评分模型。

Result: 通过企业环境中的实证验证，证明这种结构化方法能显著改善决策结果，降低平台锁定或解决方案选择不当的风险。

Conclusion: 该框架填补了营销驱动的平台比较与严谨的上下文特定评估方法之间的空白，为组织提供了有效的平台选择工具。

Abstract: The rapid adoption of Low-Code Development Platforms (LCDPs) has created a
critical need for systematic evaluation methodologies that enable organizations
to make informed platform selection decisions. This paper presents a
comprehensive evaluation framework based on five key criteria: Business Process
Orchestration, UI/UX Customization, Integration and Interoperability,
Governance and Security, and AI-Enhanced Automation. We propose a weighted
scoring model that allows organizations to quantitatively assess and compare
different low-code platforms based on their specific requirements and strategic
priorities. The framework addresses the gap between marketing-driven platform
comparisons and rigorous, context-specific evaluation methodologies. Through
empirical validation in enterprise environments, we demonstrate how this
structured approach can significantly improve decision-making outcomes and
reduce the risk of platform lock-in or inadequate solution selection.

</details>


### [27] [CUARewardBench: A Benchmark for Evaluating Reward Models on Computer-using Agent](https://arxiv.org/abs/2510.18596)
*Haojia Lin,Xiaoyu Tan,Yulei Qin,Zihan Xu,Yuchen Shi,Zongyi Li,Gang Li,Shaofei Cai,Siqi Cai,Chaoyou Fu,Ke Li,Xing Sun*

Main category: cs.SE

TL;DR: 提出了CUARewardBench，首个用于评估计算机使用代理（CUA）奖励模型的基准，包含结果奖励模型（ORM）和过程奖励模型（PRM）的评估，并通过Unanimous Prompt Ensemble（UPE）方法显著提升了奖励模型的可靠性。


<details>
  <summary>Details</summary>
Motivation: 现有的基于脚本的验证器在评估计算机使用代理时存在可扩展性有限和无法提供逐步评估的问题，而奖励模型作为替代方案的评估效果尚未充分探索。

Method: 构建了CUARewardBench基准，包含来自10个软件类别和7种代理架构的轨迹数据，通过专家标注和严格质量控制。提出了UPE集成方法，采用严格一致投票和策略性提示模板配置。

Result: UPE方法在ORM上达到89.8%精确度和93.3%负预测值，在PRM上达到81.7%精确度和85.1%负预测值，显著优于单一视觉语言模型和传统集成方法。

Conclusion: 当前CUA奖励模型存在视觉推理能力不足、知识缺陷等限制，通用视觉语言模型在奖励评估中优于专用CUA模型，UPE方法能有效提升奖励模型可靠性。

Abstract: Computer-using agents (CUAs) enable task completion through natural
interaction with operating systems and software interfaces. While script-based
verifiers are widely adopted for evaluation, they suffer from limited
scalability and inability to provide step-wise assessment. Reward models offer
promising alternatives, but their effectiveness on CUA evaluation remains
largely underexplored. To address this gap, we present CUARewardBench,
comprising four key contributions: (1) First-ever Comprehensive CUA Reward
Benchmark: We introduce the first benchmark for evaluating both outcome reward
models (ORM) and process reward models (PRM) on CUA tasks, enabling systematic
assessment across trajectory-level and step-level evaluation. (2) Diverse,
Practical and Reliable Dataset: CUARewardBench encompasses trajectories from 10
software categories and 7 agent architectures with varying performance levels
(25.9%-50.8% success rates). All trajectories are expertly annotated through
carefully designed protocols, with rigorous quality control to ensure
reliability and practical applicability. (3) Comprehensive Analysis and
Insights: Through extensive experiments across 7 vision-language models and 3
prompt templates, we reveal critical limitations of current CUA RMs, including
insufficient visual reasoning capabilities, knowledge deficiencies, and the
superiority of general VLMs over specialized CUA models for reward evaluation.
(4) Unanimous Prompt Ensemble (UPE): Based on the insights from our
comprehensive analysis, we propose UPE, a novel ensemble method that
significantly enhances reward model reliability through strict unanimous voting
and strategic prompt-template configurations. UPE achieves 89.8% precision and
93.3% NPV for ORM, and 81.7% precision and 85.1% NPV for PRM, substantially
outperforming single VLMs and traditional ensemble approaches.

</details>


### [28] [An overview of the use of alternative funding and contracting approaches relevant for agile software development: A systematic review of real-life experiences](https://arxiv.org/abs/2510.18711)
*Bertha Ngereja,Magne Jørgensen*

Main category: cs.SE

TL;DR: 该论文通过系统文献综述研究了与传统敏捷软件开发原则冲突的替代性融资和合同方法，识别了4种替代融资和4种替代合同方法，分析了采用这些方法的动机、益处和挑战。


<details>
  <summary>Details</summary>
Motivation: 传统融资和合同方法的线性、刚性特征与敏捷开发的灵活迭代特性存在冲突，阻碍了客户与承包商之间的有效协作并限制了盈利能力，因此需要探索更符合敏捷原则的替代方法。

Method: 在SCOPUS、Web of Science和Google Scholar中进行了系统文献综述，识别了来自私人和公共部门背景的38篇相关实证研究。

Result: 识别出4种替代融资方法和4种替代合同方法。采用这些方法的主要益处包括提高客户满意度、降低承包商风险、更有效的资源利用。挑战包括文化结构障碍、范围蔓延风险增加、需要额外时间和资源投入。

Conclusion: 组织应采用混合方法逐步过渡到完全灵活的定制方法，而不是突然全面采用替代方法。组织背景（领导力、人员、系统准备度）在选择合适方法时至关重要。

Abstract: Agile software development emphasizes flexibility and iterative processes,
which may conflict with the more linear, rigid, and time-consuming traditional
funding and contracting approaches. This review synthesizes real-life
experiences of using alternative (non-traditional) contracting and funding
approaches. The focus is on identifying approaches that align better with agile
principles and understanding the motivations, benefits, and challenges these
alternatives present. A systematic literature review was conducted in SCOPUS,
Web of Science, and Google Scholar, where we identified 38 relevant
peer-reviewed empirical studies from private and public sector contexts. Four
alternative funding and four alternative contracting approaches were
identified. Organizations were motivated to adopt these alternative approaches
because traditional approaches often proved too rigid, conflicted with agile
principles, hindered effective client-contractor collaboration, and limited
profitability. The benefits of these alternatives included higher client
satisfaction, reduced contractor risk, and more efficient resource utilization.
Adopting alternative funding and contracting approaches may promote flexibility
and efficiency in agile projects but also presents cultural and structural
challenges, increases the risk of scope creep and analysis paralysis, and
requires additional effort in terms of time and resources. The context of the
organization matters highly in selecting a suitable approach, such as the
organizational readiness in terms of its leaders, people, and systems. Thus,
instead of wholly adopting alternative approaches and introducing changes
abruptly, organizations may benefit from starting with hybrid approaches that
balance flexibility and control and progressively transition to fully flexible
approaches tailored to their needs

</details>


### [29] [Causally Perturbed Fairness Testing](https://arxiv.org/abs/2510.18719)
*Chengwen Du,Tao Chen*

Main category: cs.SE

TL;DR: CausalFT是一个基于因果推理的公平性测试框架，通过提取与敏感特征因果相关的非敏感特征来指导测试样本生成，显著提高了公平性缺陷检测效果。


<details>
  <summary>Details</summary>
Motivation: 现有公平性测试方法主要关注测试样本生成器设计，忽略了数据特征的因果知识，限制了其揭示公平性缺陷的潜力。

Method: 通过因果推理提取与敏感特征直接因果相关的非敏感特征，并将这种因果关系注入到扰动过程中，指导测试样本生成器。

Result: 在1296个测试案例中，CausalFT在93%的情况下显著改进了基础生成器的公平性缺陷检测能力，在64%的情况下优于基于相关性的方法，且运行效率更高。

Conclusion: CausalFT是一个通用框架，能够有效结合因果知识提升公平性测试效果，在检测公平性缺陷方面表现优越。

Abstract: To mitigate unfair and unethical discrimination over sensitive features
(e.g., gender, age, or race), fairness testing plays an integral role in
engineering systems that leverage AI models to handle tabular data. A key
challenge therein is how to effectively reveal fairness bugs under an
intractable sample size using perturbation. Much current work has been focusing
on designing the test sample generators, ignoring the valuable knowledge about
data characteristics that can help guide the perturbation and hence limiting
their full potential. In this paper, we seek to bridge such a gap by proposing
a generic framework of causally perturbed fairness testing, dubbed CausalFT.
Through causal inference, the key idea of CausalFT is to extract the most
directly and causally relevant non-sensitive feature to its sensitive
counterpart, which can jointly influence the prediction of the label. Such a
causal relationship is then seamlessly injected into the perturbation to guide
a test sample generator. Unlike existing generator-level work, CausalFT serves
as a higher-level framework that can be paired with diverse base generators.
Extensive experiments on 1296 cases confirm that CausalFT can considerably
improve arbitrary base generators in revealing fairness bugs over 93% of the
cases with acceptable extra runtime overhead. Compared with a state-of-the-art
approach that ranks the non-sensitive features solely based on correlation,
CausalFT performs significantly better on 64% cases while being much more
efficient. Further, CausalFT can better improve bias resilience in nearly all
cases.

</details>


### [30] [ShaRE your Data! Characterizing Datasets for LLM-based Requirements Engineering](https://arxiv.org/abs/2510.18787)
*Quim Motger,Carlota Catot,Xavier Franch*

Main category: cs.SE

TL;DR: 本文通过系统映射研究分析了62个用于LLM4RE研究的公开数据集，发现数据集碎片化且特征描述不足，提出了数据集分类框架和公开目录以支持数据集选择与重用。


<details>
  <summary>Details</summary>
Motivation: 需求工程(RE)领域存在数据稀缺问题，现有LLM4RE数据集碎片化且特征描述不足，限制了数据集的重用和研究成果的可比性。

Method: 采用系统映射研究方法，对43篇主要研究中引用的62个公开数据集进行识别和分析，沿多个描述符进行特征化分类。

Result: 识别出62个公开数据集，发现研究空白包括：需求获取任务覆盖有限、除可追溯性外的管理活动数据集稀缺、多语言可用性有限。

Conclusion: 提供了公开目录和结构化特征描述方案，支持LLM4RE研究中的数据集选择、比较和重用，未来将扩展到灰色文献并与开放数据集库集成。

Abstract: [Context] Large Language Models (LLMs) rely on domain-specific datasets to
achieve robust performance across training and inference stages. However, in
Requirements Engineering (RE), data scarcity remains a persistent limitation
reported in surveys and mapping studies. [Question/Problem] Although there are
multiple datasets supporting LLM-based RE tasks (LLM4RE), they are fragmented
and poorly characterized, limiting reuse and comparability. This research
addresses the limited visibility and characterization of datasets used in
LLM4RE. We investigate which public datasets are employed, how they can be
systematically characterized, and which RE tasks and dataset descriptors remain
under-represented. [Ideas/Results] To address this, we conduct a systematic
mapping study to identify and analyse datasets used in LLM4RE research. A total
of 62 publicly available datasets are referenced across 43 primary studies.
Each dataset is characterized along descriptors such as artifact type,
granularity, RE stage, task, domain, and language. Preliminary findings show
multiple research gaps, including limited coverage for elicitation tasks,
scarce datasets for management activities beyond traceability, and limited
multilingual availability. [Contribution] This research preview offers a public
catalogue and structured characterization scheme to support dataset selection,
comparison, and reuse in LLM4RE research. Future work will extend the scope to
grey literature, as well as integration with open dataset and benchmark
repositories.

</details>


### [31] [FeClustRE: Hierarchical Clustering and Semantic Tagging of App Features from User Reviews](https://arxiv.org/abs/2510.18799)
*Max Tiessler,Quim Motger*

Main category: cs.SE

TL;DR: FeClustRE是一个从移动应用评论中提取特征的框架，结合了混合特征提取、层次聚类和LLM语义标注，能生成有组织的特征分类体系。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以将嘈杂、模糊的用户反馈转化为可解释的见解，语法方法缺乏语义深度，而LLM方法往往遗漏细粒度特征或无法结构化组织特征。

Method: 结合语法解析与LLM增强，通过层次聚类和自动调参组织特征，并自动生成有意义的分类标签。

Result: 在公共基准测试中评估提取正确性，在生成式AI助手应用评论样本研究中评估聚类质量、语义一致性和可解释性。

Conclusion: FeClustRE提供了混合特征提取和分类体系生成的框架，具有自动调参机制和开源实现，能够连接用户反馈与特征理解，为需求分析提供更深入的洞察。

Abstract: [Context and motivation.] Extracting features from mobile app reviews is
increasingly important for multiple requirements engineering (RE) tasks.
However, existing methods struggle to turn noisy, ambiguous feedback into
interpretable insights. [Question/problem.] Syntactic approaches lack semantic
depth, while large language models (LLMs) often miss fine-grained features or
fail to structure them coherently. In addition, existing methods output flat
lists of features without semantic organization, limiting interpretation and
comparability. Consequently, current feature extraction approaches do not
provide structured, meaningful representations of app features. As a result,
practitioners face fragmented information that hinder requirement analysis,
prioritization, and cross-app comparison, among other use cases. [Principal
ideas/results.] In this context, we propose FeClustRE, a framework integrating
hybrid feature extraction, hierarchical clustering with auto-tuning and
LLM-based semantic labelling. FeClustRE combines syntactic parsing with LLM
enrichment, organizes features into clusters, and automatically generates
meaningful taxonomy labels. We evaluate FeClustRE on public benchmarks for
extraction correctness and on a sample study of generative AI assistant app
reviews for clustering quality, semantic coherence, and interpretability.
[Contribution.] Overall, FeClustRE delivers (1) a hybrid framework for feature
extraction and taxonomy generation, (2) an auto-tuning mechanism with a
comprehensive evaluation methodology, and (3) open-source and replicable
implementation. These contributions bridge user feedback and feature
understanding, enabling deeper insights into current and emerging requirements.

</details>


### [32] [Streamlining Acceptance Test Generation for Mobile Applications Through Large Language Models: An Industrial Case Study](https://arxiv.org/abs/2510.18861)
*Pedro Luís Fonseca,Bruno Lima,João Pascoal Faria*

Main category: cs.SE

TL;DR: AToMIC是一个自动化框架，利用专门的大型语言模型从需求文档和代码变更中自动生成Gherkin场景、页面对象和可执行的UI测试脚本，显著提升移动应用验收测试效率。


<details>
  <summary>Details</summary>
Motivation: 移动应用验收测试在现代软件开发中仍然是瓶颈，特别是在使用Flutter等跨平台框架时。虽然开发者依赖自动化测试工具，但创建和维护验收测试工件仍需大量人工工作。

Method: 引入AToMIC框架，利用专门的大型语言模型，直接从需求文档（JIRA工单）和最近的代码变更中生成Gherkin场景、页面对象和可执行的UI测试脚本。

Result: 在宝马MyBMW应用中测试，覆盖170+屏幕代码库中的13个真实问题，AToMIC在标准硬件上每个功能在5分钟内生成可执行测试工件。生成质量高：93.3%的Gherkin场景语法正确，78.8%的页面对象无需手动编辑即可运行，100%生成的UI测试成功执行。所有实践者报告节省时间（通常每个功能节省一个完整开发日）并对采用该方法有强烈信心。

Conclusion: AToMIC被证实为工业移动项目中简化验收测试创建和维护的可扩展、实用解决方案。

Abstract: Mobile acceptance testing remains a bottleneck in modern software
development, particularly for cross-platform mobile development using
frameworks like Flutter. While developers increasingly rely on automated
testing tools, creating and maintaining acceptance test artifacts still demands
significant manual effort. To help tackle this issue, we introduce AToMIC, an
automated framework leveraging specialized Large Language Models to generate
Gherkin scenarios, Page Objects, and executable UI test scripts directly from
requirements (JIRA tickets) and recent code changes. Applied to BMW's MyBMW
app, covering 13 real-world issues in a 170+ screen codebase, AToMIC produced
executable test artifacts in under five minutes per feature on standard
hardware. The generated artifacts were of high quality: 93.3% of Gherkin
scenarios were syntactically correct upon generation, 78.8% of PageObjects ran
without manual edits, and 100% of generated UI tests executed successfully. In
a survey, all practitioners reported time savings (often a full developer-day
per feature) and strong confidence in adopting the approach. These results
confirm AToMIC as a scalable, practical solution for streamlining acceptance
test creation and maintenance in industrial mobile projects.

</details>


### [33] [EffiReasonTrans: RL-Optimized Reasoning for Code Translation](https://arxiv.org/abs/2510.18863)
*Yanlin Wang,Rongyi Ou,Yanli Wang,Mingwei Liu,Jiachi Chen,Ensheng Shi,Xilin Liu,Yuchi Ma,Zibin Zheng*

Main category: cs.SE

TL;DR: EffiReasonTrans是一个代码翻译训练框架，通过构建推理增强数据集和两阶段训练策略，在提高翻译准确性的同时平衡推理延迟。


<details>
  <summary>Details</summary>
Motivation: 解决大型语言模型在代码翻译中准确性与推理延迟之间的权衡问题，满足实际开发流程中需要人工检查的需求。

Method: 1) 使用DeepSeek-R1构建高质量的推理增强数据集(源代码、推理过程、目标代码)；2) 两阶段训练：监督微调后接强化学习，优化准确性和延迟平衡。

Result: 在6个翻译对上，翻译准确性显著提升(最高+49.2% CA和+27.8% CodeBLEU)，生成token数量减少(最高-19.3%)，推理延迟降低(最高-29.0%)。

Conclusion: EffiReasonTrans框架有效解决了代码翻译中准确性与效率的权衡问题，并在基于代理的框架中表现出更好的翻译准确性。

Abstract: Code translation is a crucial task in software development and maintenance.
While recent advancements in large language models (LLMs) have improved
automated code translation accuracy, these gains often come at the cost of
increased inference latency, hindering real-world development workflows that
involve human-in-the-loop inspection. To address this trade-off, we propose
EffiReasonTrans, a training framework designed to improve translation accuracy
while balancing inference latency. We first construct a high-quality
reasoning-augmented dataset by prompting a stronger language model,
DeepSeek-R1, to generate intermediate reasoning and target translations. Each
(source code, reasoning, target code) triplet undergoes automated syntax and
functionality checks to ensure reliability. Based on this dataset, we employ a
two-stage training strategy: supervised fine-tuning on reasoning-augmented
samples, followed by reinforcement learning to further enhance accuracy and
balance inference latency. We evaluate EffiReasonTrans on six translation
pairs. Experimental results show that it consistently improves translation
accuracy (up to +49.2% CA and +27.8% CodeBLEU compared to the base model) while
reducing the number of generated tokens (up to -19.3%) and lowering inference
latency in most cases (up to -29.0%). Ablation studies further confirm the
complementary benefits of the two-stage training framework. Additionally,
EffiReasonTrans demonstrates improved translation accuracy when integrated into
agent-based frameworks. Our code and data are available at
https://github.com/DeepSoftwareAnalytics/EffiReasonTrans.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [34] [RiskTagger: An LLM-based Agent for Automatic Annotation of Web3 Crypto Money Laundering Behaviors](https://arxiv.org/abs/2510.17848)
*Dan Lin,Yanli Ding,Weipeng Zou,Jiachi Chen,Xiapu Luo,Jiajing Wu,Zibin Zheng*

Main category: cs.CR

TL;DR: RiskTagger是一个基于大语言模型的代理，用于自动标注Web3中的加密货币洗钱行为，通过多模块架构实现从复杂报告中提取线索、多链交易路径推理和生成审计友好解释的功能。


<details>
  <summary>Details</summary>
Motivation: Web3的快速发展使得链上洗钱行为更加隐蔽复杂，而当前反洗钱数据集构建主要依赖人工标注，效率和覆盖范围有限。

Method: RiskTagger采用端到端多模块代理架构，包括关键线索提取器、多链数据获取器与洗钱行为推理器、数据解释器，形成完整的数据标注流水线。

Result: 在Bybit Hack真实案例测试中，RiskTagger在线索提取上达到100%准确率，与专家判断一致性达84.1%，解释生成覆盖率达90%。

Conclusion: RiskTagger能够自动化洗钱行为标注，同时提高反洗钱研究的透明度和可扩展性。

Abstract: While the rapid growth of Web3 has driven the development of decentralized
finance, user anonymity and cross-chain asset flows make on-chain laundering
behaviors more covert and complex. In this context, constructing high-quality
anti-money laundering(AML) datasets has become essential for risk-control
systems and on-chain forensic analysis, yet current practices still rely
heavily on manual efforts with limited efficiency and coverage. In this paper,
we introduce RiskTagger, a large-language-model-based agent for the automatic
annotation of crypto laundering behaviors in Web3. RiskTagger is designed to
replace or complement human annotators by addressing three key challenges:
extracting clues from complex unstructured reports, reasoning over multichain
transaction paths, and producing auditor-friendly explanations. RiskTagger
implements an end-to-end multi-module agent, integrating a key-clue extractor,
a multichain fetcher with a laundering-behavior reasoner, and a data explainer,
forming a data annotation pipeline. Experiments on the real case Bybit Hack
(with the highest stolen asset value) demonstrate that RiskTagger achieves 100%
accuracy in clue extraction, 84.1% consistency with expert judgment, and 90%
coverage in explanation generation. Overall, RiskTagger automates laundering
behavior annotation while improving transparency and scalability in AML
research.

</details>


### [35] [When "Correct" Is Not Safe: Can We Trust Functionally Correct Patches Generated by Code Agents?](https://arxiv.org/abs/2510.17862)
*Yibo Peng,James Song,Lei Li,Xinyu Yang,Mihai Christodorescu,Ravi Mangal,Corina Pasareanu,Haizhong Zheng,Beidi Chen*

Main category: cs.CR

TL;DR: 本文揭示了代码代理面临的新型安全威胁：功能正确但存在漏洞（FCV）的补丁，这些补丁能通过所有测试但包含漏洞代码。


<details>
  <summary>Details</summary>
Motivation: 当前代码代理的安全评估主要关注功能正确性，忽视了安全漏洞问题。随着代码代理在GitHub等平台上被信任来自主修复bug，需要评估其安全性威胁。

Method: 提出了FCV-Attack攻击方法，只需黑盒访问和单次查询即可对代码代理进行攻击，测试了12种代理-模型组合在SWE-Bench上的表现。

Result: 攻击成功率很高，例如对于CWE-538信息泄露漏洞，在GPT-5 Mini + OpenHands上达到40.7%的攻击成功率。所有测试的SOTA LLM和代理框架都易受此威胁。

Conclusion: 当前评估范式忽视了重要的安全威胁，迫切需要开发安全感知的代码代理防御机制。

Abstract: Code agents are increasingly trusted to autonomously fix bugs on platforms
such as GitHub, yet their security evaluation focuses almost exclusively on
functional correctness. In this paper, we reveal a novel type of threat to
real-world code agents: Functionally Correct yet Vulnerable (FCV) patches,
which pass all test cases but contain vulnerable code. With our proposed
FCV-Attack, which can be deliberately crafted by malicious attackers or
implicitly introduced by benign developers, we show that SOTA LLMs (e.g.,
ChatGPT and Claude) and agent scaffolds (e.g., SWE-agent and OpenHands) are all
vulnerable to this FCV threat; across 12 agent-model combinations on SWE-Bench,
the attack only requires black-box access and a single query to the code agent
to perform the attack. For example, for CWE-538 (information exposure
vulnerability), the FCV-Attack attains an attack success rate of $40.7\%$ on
GPT-5 Mini + OpenHands. Our results reveal an important security threat
overlooked by current evaluation paradigms and urge the development of
security-aware defenses for code agents.

</details>


### [36] [From Flows to Words: Can Zero-/Few-Shot LLMs Detect Network Intrusions? A Grammar-Constrained, Calibrated Evaluation on UNSW-NB15](https://arxiv.org/abs/2510.17883)
*Mohammad Abdul Rehman,Syed Imad Ali Shah,Abbas n=Anwar,Noor Islam*

Main category: cs.CR

TL;DR: 本研究评估了在入侵检测中使用纯提示方法的大语言模型，通过将网络流量转换为文本记录并添加领域启发式布尔标志，无需微调即可实现检测。


<details>
  <summary>Details</summary>
Motivation: 探索大语言模型在无需微调的情况下进行入侵检测的潜力，特别是在处理网络流量分析任务时的表现。

Method: 将网络流量转换为紧凑文本记录，添加轻量级领域启发式布尔标志（不对称性、突发率、TTL异常等），使用零样本、指令引导和少样本提示策略，并采用结构化响应和阈值校准。

Result: 在200个平衡流量样本上，7B指令调优模型结合标志达到macro-F1约0.78；在1000个样本上，3B模型结合少样本提示和校准达到F1约0.68。随着评估集增大到2000个流量，检测质量下降。

Conclusion: 纯提示方法无需梯度训练，可生成可读结果且易于通过指令和标志调整，但检测质量对覆盖范围和提示策略敏感，表格基线方法更稳定快速。

Abstract: Large Language Models (LLMs) can reason over natural-language inputs, but
their role in intrusion detection without fine-tuning remains uncertain. This
study evaluates a prompt-only approach on UNSW-NB15 by converting each network
flow to a compact textual record and augmenting it with lightweight,
domain-inspired boolean flags (asymmetry, burst rate, TTL irregularities, timer
anomalies, rare service/state, short bursts). To reduce output drift and
support measurement, the model is constrained to produce structured,
grammar-valid responses, and a single decision threshold is calibrated on a
small development split. We compare zero-shot, instruction-guided, and few-shot
prompting to strong tabular and neural baselines under identical splits,
reporting accuracy, precision, recall, F1, and macro scores. Empirically,
unguided prompting is unreliable, while instructions plus flags substantially
improve detection quality; adding calibrated scoring further stabilizes
results. On a balanced subset of two hundred flows, a 7B instruction-tuned
model with flags reaches macro-F1 near 0.78; a lighter 3B model with few-shot
cues and calibration attains F1 near 0.68 on one thousand examples. As the
evaluation set grows to two thousand flows, decision quality decreases,
revealing sensitivity to coverage and prompting. Tabular baselines remain more
stable and faster, yet the prompt-only pipeline requires no gradient training,
produces readable artifacts, and adapts easily through instructions and flags.
Contributions include a flow-to-text protocol with interpretable cues, a
calibration method for thresholding, a systematic baseline comparison, and a
reproducibility bundle with prompts, grammar, metrics, and figures.

</details>


### [37] [When Intelligence Fails: An Empirical Study on Why LLMs Struggle with Password Cracking](https://arxiv.org/abs/2510.17884)
*Mohammad Abdul Rehman,Syed Imad Ali Shah,Abbas Anwar,Noor Islam*

Main category: cs.CR

TL;DR: 本研究评估了开源大语言模型在密码猜测任务中的表现，发现其性能远低于传统方法，准确率不足1.5%。


<details>
  <summary>Details</summary>
Motivation: 探索大语言模型在网络安全应用中的潜力，特别是密码破解能力，因为LLMs在自然语言理解和生成方面表现出色。

Method: 使用TinyLLaMA、Falcon-RW-1B和Flan-T5等开源LLMs，基于结构化用户属性生成密码，并通过Hit@1、Hit@5和Hit@10指标评估性能。

Result: 所有模型在Hit@10指标下的准确率均低于1.5%，远低于传统基于规则和组合的方法。

Conclusion: 尽管LLMs具有语言能力，但在密码推断任务中缺乏领域适应和记忆能力，特别是在没有监督微调的情况下。

Abstract: The remarkable capabilities of Large Language Models (LLMs) in natural
language understanding and generation have sparked interest in their potential
for cybersecurity applications, including password guessing. In this study, we
conduct an empirical investigation into the efficacy of pre-trained LLMs for
password cracking using synthetic user profiles. Specifically, we evaluate the
performance of state-of-the-art open-source LLMs such as TinyLLaMA,
Falcon-RW-1B, and Flan-T5 by prompting them to generate plausible passwords
based on structured user attributes (e.g., name, birthdate, hobbies). Our
results, measured using Hit@1, Hit@5, and Hit@10 metrics under both plaintext
and SHA-256 hash comparisons, reveal consistently poor performance, with all
models achieving less than 1.5% accuracy at Hit@10. In contrast, traditional
rule-based and combinator-based cracking methods demonstrate significantly
higher success rates. Through detailed analysis and visualization, we identify
key limitations in the generative reasoning of LLMs when applied to the
domain-specific task of password guessing. Our findings suggest that, despite
their linguistic prowess, current LLMs lack the domain adaptation and
memorization capabilities required for effective password inference, especially
in the absence of supervised fine-tuning on leaked password datasets. This
study provides critical insights into the limitations of LLMs in adversarial
contexts and lays the groundwork for future efforts in secure,
privacy-preserving, and robust password modeling.

</details>


### [38] [BreakFun: Jailbreaking LLMs via Schema Exploitation](https://arxiv.org/abs/2510.17904)
*Amirkia Rafiei Oskooei,Mehmet S. Aktas*

Main category: cs.CR

TL;DR: BreakFun是一种利用LLM对结构化模式遵循性的越狱方法，通过特洛伊模式强制模型生成有害内容，在多个模型上达到89%平均成功率，并提出基于文本转录的防御方法。


<details>
  <summary>Details</summary>
Motivation: 研究LLM在处理结构化数据和遵循语法规则方面的能力，这种能力既是其广泛应用的基础，也使其容易受到攻击。

Method: 使用三部分提示：无害框架、思维链干扰和特洛伊模式（精心设计的数据结构），利用LLM对结构和模式的强烈遵循倾向。

Result: 在13个基础模型和专有模型上平均成功率89%，多个模型达到100%攻击成功率，消融研究证实特洛伊模式是主要攻击因素。

Conclusion: LLM的核心优势可能成为关键弱点，通过针对欺骗性模式的防御策略可以构建更鲁棒的对齐模型。

Abstract: The proficiency of Large Language Models (LLMs) in processing structured data
and adhering to syntactic rules is a capability that drives their widespread
adoption but also makes them paradoxically vulnerable. In this paper, we
investigate this vulnerability through BreakFun, a jailbreak methodology that
weaponizes an LLM's adherence to structured schemas. BreakFun employs a
three-part prompt that combines an innocent framing and a Chain-of-Thought
distraction with a core "Trojan Schema"--a carefully crafted data structure
that compels the model to generate harmful content, exploiting the LLM's strong
tendency to follow structures and schemas. We demonstrate this vulnerability is
highly transferable, achieving an average success rate of 89% across 13
foundational and proprietary models on JailbreakBench, and reaching a 100%
Attack Success Rate (ASR) on several prominent models. A rigorous ablation
study confirms this Trojan Schema is the attack's primary causal factor. To
counter this, we introduce the Adversarial Prompt Deconstruction guardrail, a
defense that utilizes a secondary LLM to perform a "Literal
Transcription"--extracting all human-readable text to isolate and reveal the
user's true harmful intent. Our proof-of-concept guardrail demonstrates high
efficacy against the attack, validating that targeting the deceptive schema is
a viable mitigation strategy. Our work provides a look into how an LLM's core
strengths can be turned into critical weaknesses, offering a fresh perspective
for building more robustly aligned models.

</details>


### [39] [ParaVul: A Parallel Large Language Model and Retrieval-Augmented Framework for Smart Contract Vulnerability Detection](https://arxiv.org/abs/2510.17919)
*Tenghui Huang,Jinbo Wen,Jiawen Kang,Siyong Chen,Zhengtao Li,Tao Zhang,Dongning Liu,Jiacheng Wang,Chengjun Cai,Yinqiu Liu,Dusit Niyato*

Main category: cs.CR

TL;DR: ParaVul是一个并行LLM和检索增强框架，用于提高智能合约漏洞检测的可靠性和准确性，通过SLoRA微调、混合RAG系统和元学习融合输出，在F1分数上表现出色。


<details>
  <summary>Details</summary>
Motivation: 传统智能合约漏洞检测方法存在高误报率和可扩展性差的问题，而现有LLM方法面临高推理成本和计算开销的挑战。

Method: 开发SLoRA进行LLM微调，构建漏洞合约数据集和混合RAG系统，提出元学习模型融合RAG和LLM输出，并使用思维链提示生成检测报告。

Result: 在F1分数上表现优异，单标签检测达到0.9398，多标签检测达到0.9330。

Conclusion: ParaVul框架在智能合约漏洞检测方面具有优越性，特别是在准确性和可靠性方面。

Abstract: Smart contracts play a significant role in automating blockchain services.
Nevertheless, vulnerabilities in smart contracts pose serious threats to
blockchain security. Currently, traditional detection methods primarily rely on
static analysis and formal verification, which can result in high
false-positive rates and poor scalability. Large Language Models (LLMs) have
recently made significant progress in smart contract vulnerability detection.
However, they still face challenges such as high inference costs and
substantial computational overhead. In this paper, we propose ParaVul, a
parallel LLM and retrieval-augmented framework to improve the reliability and
accuracy of smart contract vulnerability detection. Specifically, we first
develop Sparse Low-Rank Adaptation (SLoRA) for LLM fine-tuning. SLoRA
introduces sparsification by incorporating a sparse matrix into quantized
LoRA-based LLMs, thereby reducing computational overhead and resource
requirements while enhancing their ability to understand vulnerability-related
issues. We then construct a vulnerability contract dataset and develop a hybrid
Retrieval-Augmented Generation (RAG) system that integrates dense retrieval
with Best Matching 25 (BM25), assisting in verifying the results generated by
the LLM. Furthermore, we propose a meta-learning model to fuse the outputs of
the RAG system and the LLM, thereby generating the final detection results.
After completing vulnerability detection, we design chain-of-thought prompts to
guide LLMs to generate comprehensive vulnerability detection reports.
Simulation results demonstrate the superiority of ParaVul, especially in terms
of F1 scores, achieving 0.9398 for single-label detection and 0.9330 for
multi-label detection.

</details>


### [40] [PLAGUE: Plug-and-play framework for Lifelong Adaptive Generation of Multi-turn Exploits](https://arxiv.org/abs/2510.17947)
*Neeladri Bhuiya,Madhav Aggarwal,Diptanshu Purwar*

Main category: cs.CR

TL;DR: PLAGUE是一个用于设计多轮攻击的即插即用框架，将攻击生命周期分为三个精心设计的阶段（Primer、Planner和Finisher），在保持较低查询预算的同时，显著提高了对领先LLM模型的越狱成功率。


<details>
  <summary>Details</summary>
Motivation: 随着智能工作流的出现，多轮对话已成为与LLM交互完成复杂任务的主要方式。虽然LLM能力不断提升，但在多轮场景中更容易受到越狱攻击，特别是在有害意图可以巧妙地分散在对话中时。单轮攻击已被广泛研究，但其多轮对应版本在适应性、效率和有效性方面仍面临挑战。

Method: PLAGUE框架将多轮攻击的生命周期分解为三个阶段：Primer（初始化攻击计划）、Planner（优化上下文和规划攻击策略）、Finisher（执行最终攻击）。该框架受到终身学习智能体的启发，能够系统性地探索多轮攻击家族。

Result: 使用PLAGUE设计的红队智能体实现了最先进的越狱效果，在相同或更少的查询预算下，攻击成功率提高了30%以上。特别地，PLAGUE在OpenAI o3模型上实现了81.4%的攻击成功率，在Claude Opus 4.1上实现了67.3%的攻击成功率，这两个模型在安全文献中被认为是高度抗越狱的。

Conclusion: 这项工作提供了工具和见解，帮助理解计划初始化、上下文优化和终身学习在构建多轮攻击以进行全面的模型漏洞评估中的重要性。

Abstract: Large Language Models (LLMs) are improving at an exceptional rate. With the
advent of agentic workflows, multi-turn dialogue has become the de facto mode
of interaction with LLMs for completing long and complex tasks. While LLM
capabilities continue to improve, they remain increasingly susceptible to
jailbreaking, especially in multi-turn scenarios where harmful intent can be
subtly injected across the conversation to produce nefarious outcomes. While
single-turn attacks have been extensively explored, adaptability, efficiency
and effectiveness continue to remain key challenges for their multi-turn
counterparts. To address these gaps, we present PLAGUE, a novel plug-and-play
framework for designing multi-turn attacks inspired by lifelong-learning
agents. PLAGUE dissects the lifetime of a multi-turn attack into three
carefully designed phases (Primer, Planner and Finisher) that enable a
systematic and information-rich exploration of the multi-turn attack family.
Evaluations show that red-teaming agents designed using PLAGUE achieve
state-of-the-art jailbreaking results, improving attack success rates (ASR) by
more than 30% across leading models in a lesser or comparable query budget.
Particularly, PLAGUE enables an ASR (based on StrongReject) of 81.4% on
OpenAI's o3 and 67.3% on Claude's Opus 4.1, two models that are considered
highly resistant to jailbreaks in safety literature. Our work offers tools and
insights to understand the importance of plan initialization, context
optimization and lifelong learning in crafting multi-turn attacks for a
comprehensive model vulnerability evaluation.

</details>


### [41] [BadScientist: Can a Research Agent Write Convincing but Unsound Papers that Fool LLM Reviewers?](https://arxiv.org/abs/2510.18003)
*Fengqing Jiang,Yichen Feng,Yuetai Li,Luyao Niu,Basel Alomair,Radha Poovendran*

Main category: cs.CR

TL;DR: BadScientist框架揭示了LLM驱动的科研助手与AI同行评审系统结合时存在的关键漏洞：无需真实实验的伪造论文能够欺骗多模型LLM评审系统，接受率高达30%，暴露了当前AI驱动评审系统的根本性缺陷。


<details>
  <summary>Details</summary>
Motivation: 研究LLM驱动的科研助手与AI同行评审系统融合时产生的全自动出版循环漏洞，这种循环中AI生成的研究由AI评审员评估而无人监督，对科学出版的完整性构成严重威胁。

Method: 开发BadScientist框架，使用无需真实实验的呈现操纵策略生成伪造论文，并建立具有形式化错误保证（集中界限和校准分析）的严格评估框架，在真实数据上进行校准。

Result: 发现系统性漏洞：伪造论文接受率高达30%；识别出"担忧-接受冲突"现象——评审员频繁标记诚信问题却仍给予接受级别的评分；缓解策略仅带来边际改善，检测准确率勉强超过随机概率。

Conclusion: 尽管具有可证明正确的聚合数学，诚信检查系统性地失败，暴露了当前AI驱动评审系统的根本局限性，并强调了在科学出版中迫切需要深度防御保障措施。

Abstract: The convergence of LLM-powered research assistants and AI-based peer review
systems creates a critical vulnerability: fully automated publication loops
where AI-generated research is evaluated by AI reviewers without human
oversight. We investigate this through \textbf{BadScientist}, a framework that
evaluates whether fabrication-oriented paper generation agents can deceive
multi-model LLM review systems. Our generator employs presentation-manipulation
strategies requiring no real experiments. We develop a rigorous evaluation
framework with formal error guarantees (concentration bounds and calibration
analysis), calibrated on real data. Our results reveal systematic
vulnerabilities: fabricated papers achieve acceptance rates up to . Critically,
we identify \textit{concern-acceptance conflict} -- reviewers frequently flag
integrity issues yet assign acceptance-level scores. Our mitigation strategies
show only marginal improvements, with detection accuracy barely exceeding
random chance. Despite provably sound aggregation mathematics, integrity
checking systematically fails, exposing fundamental limitations in current
AI-driven review systems and underscoring the urgent need for defense-in-depth
safeguards in scientific publishing.

</details>


### [42] [RL-Driven Security-Aware Resource Allocation Framework for UAV-Assisted O-RAN](https://arxiv.org/abs/2510.18084)
*Zaineh Abughazzah,Emna Baccour,Loay Ismail,Amr Mohamed,Mounir Hamdi*

Main category: cs.CR

TL;DR: 提出基于强化学习的无人机中继动态资源分配框架，在搜索救援场景中联合优化安全性、延迟和能效


<details>
  <summary>Details</summary>
Motivation: 无人机集成到O-RAN可增强灾难管理通信，但现有方法忽视了在动态环境中联合优化安全性、延迟和能效的需求

Method: 使用强化学习框架解决集成安全感知资源分配、延迟最小化和能效的优化问题，实时适应网络动态

Result: 仿真显示相比启发式基线方法，该框架在保持超低延迟的同时实现了更好的安全性和能效

Conclusion: 强化学习框架能有效解决无人机中继在搜索救援场景中的资源分配挑战，平衡安全性、延迟和能效

Abstract: The integration of Unmanned Aerial Vehicles (UAVs) into Open Radio Access
Networks (O-RAN) enhances communication in disaster management and Search and
Rescue (SAR) operations by ensuring connectivity when infrastructure fails.
However, SAR scenarios demand stringent security and low-latency communication,
as delays or breaches can compromise mission success. While UAVs serve as
mobile relays, they introduce challenges in energy consumption and resource
management, necessitating intelligent allocation strategies. Existing
UAV-assisted O-RAN approaches often overlook the joint optimization of
security, latency, and energy efficiency in dynamic environments. This paper
proposes a novel Reinforcement Learning (RL)-based framework for dynamic
resource allocation in UAV relays, explicitly addressing these trade-offs. Our
approach formulates an optimization problem that integrates security-aware
resource allocation, latency minimization, and energy efficiency, which is
solved using RL. Unlike heuristic or static methods, our framework adapts in
real-time to network dynamics, ensuring robust communication. Simulations
demonstrate superior performance compared to heuristic baselines, achieving
enhanced security and energy efficiency while maintaining ultra-low latency in
SAR scenarios.

</details>


### [43] [PrivaDE: Privacy-preserving Data Evaluation for Blockchain-based Data Marketplaces](https://arxiv.org/abs/2510.18109)
*Wan Ki Wong,Sahel Torkamani,Michele Ciampi,Rik Sarkar*

Main category: cs.CR

TL;DR: PrivaDE是一个用于隐私保护数据评估和选择的密码学协议，通过区块链中心化设计实现恶意安全保证，为机器学习的公平自动化数据市场奠定基础。


<details>
  <summary>Details</summary>
Motivation: 模型构建者需要评估候选数据的效用而不暴露专有模型细节，同时数据提供者需要确保除了计算出的效用分数外不泄露任何数据信息。

Method: 采用区块链信任机制，集成模型蒸馏、模型分割和切选零知识证明等技术，提出结合经验损失、预测熵和特征空间多样性的统一效用评分函数。

Result: 评估显示PrivaDE能有效执行数据评估，即使对于数百万参数的模型，在线运行时间也在15分钟以内。

Conclusion: 该工作为去中心化机器学习生态系统中的公平自动化数据市场奠定了基础。

Abstract: Evaluating the relevance of data is a critical task for model builders
seeking to acquire datasets that enhance model performance. Ideally, such
evaluation should allow the model builder to assess the utility of candidate
data without exposing proprietary details of the model. At the same time, data
providers must be assured that no information about their data - beyond the
computed utility score - is disclosed to the model builder.
  In this paper, we present PrivaDE, a cryptographic protocol for
privacy-preserving utility scoring and selection of data for machine learning.
While prior works have proposed data evaluation protocols, our approach
advances the state of the art through a practical, blockchain-centric design.
Leveraging the trustless nature of blockchains, PrivaDE enforces
malicious-security guarantees and ensures strong privacy protection for both
models and datasets. To achieve efficiency, we integrate several techniques -
including model distillation, model splitting, and cut-and-choose
zero-knowledge proofs - bringing the runtime to a practical level. Furthermore,
we propose a unified utility scoring function that combines empirical loss,
predictive entropy, and feature-space diversity, and that can be seamlessly
integrated into active-learning workflows. Evaluation shows that PrivaDE
performs data evaluation effectively, achieving online runtimes within 15
minutes even for models with millions of parameters.
  Our work lays the foundation for fair and automated data marketplaces in
decentralized machine learning ecosystems.

</details>


### [44] [Investigating the Impact of Dark Patterns on LLM-Based Web Agents](https://arxiv.org/abs/2510.18113)
*Devin Ersoy,Brandon Lee,Ananth Shreekumar,Arjun Arunasalam,Muhammad Ibrahim,Antonio Bianchi,Z. Berkay Celik*

Main category: cs.CR

TL;DR: 该研究首次探讨了暗黑模式对基于大语言模型的通用网络代理决策过程的影响，发现代理平均41%的时间会受到单个暗黑模式的影响。


<details>
  <summary>Details</summary>
Motivation: 随着用户越来越多地使用基于大语言模型的网络代理来自动化在线任务，这些代理可能会遇到暗黑模式——欺骗性用户界面设计，这些设计会操纵用户做出非预期的决策。虽然暗黑模式主要针对人类用户，但它们对基于大语言模型的通用网络代理的潜在有害影响尚未被探索。

Method: 研究引入了LiteAgent（一个轻量级框架，自动提示代理执行任务并捕获其交互的全面日志和屏幕录制）和TrickyArena（一个包含来自电子商务、流媒体服务和新闻平台等领域的网络应用程序的受控环境，每个应用程序都包含多样且现实的暗黑模式，可以选择性地启用或禁用）。

Result: 评估了六个流行的基于大语言模型的通用网络代理，发现在存在单个暗黑模式时，代理平均41%的时间会受到其影响。修改暗黑模式UI属性（通过视觉设计更改或HTML代码调整）以及同时引入多个暗黑模式会影响代理的易感性。

Conclusion: 这项研究强调了在网络代理中需要整体防御机制，包括代理特定的保护和更广泛的网络安全措施。

Abstract: As users increasingly turn to large language model (LLM) based web agents to
automate online tasks, agents may encounter dark patterns: deceptive user
interface designs that manipulate users into making unintended decisions.
Although dark patterns primarily target human users, their potentially harmful
impacts on LLM-based generalist web agents remain unexplored. In this paper, we
present the first study that investigates the impact of dark patterns on the
decision-making process of LLM-based generalist web agents. To achieve this, we
introduce LiteAgent, a lightweight framework that automatically prompts agents
to execute tasks while capturing comprehensive logs and screen-recordings of
their interactions. We also present TrickyArena, a controlled environment
comprising web applications from domains such as e-commerce, streaming
services, and news platforms, each containing diverse and realistic dark
patterns that can be selectively enabled or disabled. Using LiteAgent and
TrickyArena, we conduct multiple experiments to assess the impact of both
individual and combined dark patterns on web agent behavior. We evaluate six
popular LLM-based generalist web agents across three LLMs and discover that
when there is a single dark pattern present, agents are susceptible to it an
average of 41% of the time. We also find that modifying dark pattern UI
attributes through visual design changes or HTML code adjustments and
introducing multiple dark patterns simultaneously can influence agent
susceptibility. This study emphasizes the need for holistic defense mechanisms
in web agents, encompassing both agent-specific protections and broader web
safety measures.

</details>


### [45] [Black-Box Evasion Attacks on Data-Driven Open RAN Apps: Tailored Design and Experimental Evaluation](https://arxiv.org/abs/2510.18160)
*Pranshav Gajjar,Molham Khoja,Abiodun Ganiyu,Marc Juarez,Mahesh K. Marina,Andrew Lehane,Vijay K. Shah*

Main category: cs.CR

TL;DR: 该论文分析了O-RAN系统中xApps和rApps的数据安全漏洞，设计了一种针对RIC应用的黑盒逃避攻击策略，使用模型克隆、输入特定扰动、通用对抗扰动等技术来降低网络性能，并在真实O-RAN测试平台上验证了攻击有效性。


<details>
  <summary>Details</summary>
Motivation: O-RAN架构开放了RAN数据访问给第三方ML应用，这虽然促进了创新，但也可能成为恶意攻击者的漏洞来源。论文旨在全面调查xApps和rApps的数据安全漏洞。

Method: 定性分析O-RAN安全机制和限制，设计包含模型克隆算法、输入特定扰动、通用对抗扰动和定向UAPs的黑盒逃避攻击策略，针对xApps和rApps中的ML模型。

Result: 在真实O-RAN测试平台和仿真环境中验证了攻击策略的有效性，使用干扰分类xApp和节能rApp作为代表案例，证明攻击能显著降低网络性能，且对主流对抗ML防御技术也有效。

Conclusion: O-RAN开放数据访问带来了新的安全挑战，论文展示的攻击策略揭示了现有安全机制的不足，需要更强的防御措施来保护RIC应用免受此类攻击。

Abstract: The impending adoption of Open Radio Access Network (O-RAN) is fueling
innovation in the RAN towards data-driven operation. Unlike traditional RAN
where the RAN data and its usage is restricted within proprietary and
monolithic RAN equipment, the O-RAN architecture opens up access to RAN data
via RAN intelligent controllers (RICs), to third-party machine learning (ML)
powered applications - rApps and xApps - to optimize RAN operations.
Consequently, a major focus has been placed on leveraging RAN data to unlock
greater efficiency gains. However, there is an increasing recognition that RAN
data access to apps could become a source of vulnerability and be exploited by
malicious actors. Motivated by this, we carry out a comprehensive investigation
of data vulnerabilities on both xApps and rApps, respectively hosted in Near-
and Non-real-time (RT) RIC components of O-RAN. We qualitatively analyse the
O-RAN security mechanisms and limitations for xApps and rApps, and consider a
threat model informed by this analysis. We design a viable and effective
black-box evasion attack strategy targeting O-RAN RIC Apps while accounting for
the stringent timing constraints and attack effectiveness. The strategy employs
four key techniques: the model cloning algorithm, input-specific perturbations,
universal adversarial perturbations (UAPs), and targeted UAPs. This strategy
targets ML models used by both xApps and rApps within the O-RAN system, aiming
to degrade network performance. We validate the effectiveness of the designed
evasion attack strategy and quantify the scale of performance degradation using
a real-world O-RAN testbed and emulation environments. Evaluation is conducted
using the Interference Classification xApp and the Power Saving rApp as
representatives for near-RT and non-RT RICs. We also show that the attack
strategy is effective against prominent defense techniques for adversarial ML.

</details>


### [46] [TaintSentinel: Path-Level Randomness Vulnerability Detection for Ethereum Smart Contracts](https://arxiv.org/abs/2510.18192)
*Hadis Rezaei,Ahmed Afif Monrat,Karl Andersson,Francesco Flammini*

Main category: cs.CR

TL;DR: TaintSentinel是一个针对智能合约中随机数漏洞的路径敏感检测系统，通过多阶段分析（语义图构建、污点传播、神经网络模式识别）实现高精度检测，在4844个合约测试中F1分数达0.892。


<details>
  <summary>Details</summary>
Motivation: 区块链技术的确定性特性导致智能合约难以生成安全的随机数，这在DeFi和区块链游戏中造成可利用漏洞，而现有检测工具精度不足。

Method: 采用多阶段方法：基于规则的污点分析跟踪数据流，双流神经网络识别复杂漏洞特征，证据驱动的参数初始化减少误报。系统分两阶段运行：语义图构建和污点传播分析，然后使用PathGNN进行模式识别和GlobalGCN进行全局结构分析。

Result: 在4844个合约上的实验显示，TaintSentinel性能优于现有工具，F1分数0.892，AUC-ROC 0.94，PRA准确率97%。

Conclusion: TaintSentinel通过路径敏感分析和多技术融合，有效解决了智能合约随机数漏洞检测精度不足的问题，显著提升了检测性能。

Abstract: The inherent determinism of blockchain technology poses a significant
challenge to generating secure random numbers within smart contracts, leading
to exploitable vulnerabilities, particularly in decentralized finance (DeFi)
ecosystems and blockchain-based gaming applications. From our observations, the
current state-of-the-art detection tools suffer from inadequate precision while
dealing with random number vulnerabilities. To address this problem, we propose
TaintSentinel, a novel path sensitive vulnerability detection system designed
to analyze smart contracts at the execution path level and gradually analyze
taint with domain-specific rules. This paper discusses a solution that
incorporates a multi-faceted approach, integrating rule-based taint analysis to
track data flow, a dual stream neural network to identify complex vulnerability
signatures, and evidence-based parameter initialization to minimize false
positives. The system's two-phase operation involves semantic graph
construction and taint propagation analysis, followed by pattern recognition
using PathGNN and global structural analysis via GlobalGCN. Our experiments on
4,844 contracts demonstrate the superior performance of TaintSentinel relative
to existing tools, yielding an F1-score of 0.892, an AUC-ROC of 0.94, and a PRA
accuracy of 97%.

</details>


### [47] [RESCUE: Retrieval Augmented Secure Code Generation](https://arxiv.org/abs/2510.18204)
*Jiahao Shi,Tianyi Zhang*

Main category: cs.CR

TL;DR: RESCUE是一个用于安全代码生成的RAG框架，通过混合知识库构建和分层多面检索，显著提升LLM生成代码的安全性。


<details>
  <summary>Details</summary>
Motivation: 传统RAG在处理原始安全文档时存在噪声问题，现有检索方法忽视了任务描述中隐含的重要安全语义，导致LLM仍然生成易受攻击的代码。

Method: 1. 混合知识库构建：结合LLM辅助的聚类-总结蒸馏和程序切片，生成高级安全指南和简洁的安全代码示例；2. 分层多面检索：从上到下遍历知识库，在每个层级集成多个安全关键事实。

Result: 在四个基准测试和六个LLM上评估，RESCUE将SecurePass@1指标平均提升4.8分，建立了新的安全性能最先进水平。

Conclusion: RESCUE通过创新的知识库构建和检索方法，有效解决了安全代码生成中的挑战，显著提升了LLM生成代码的安全性。

Abstract: Despite recent advances, Large Language Models (LLMs) still generate
vulnerable code. Retrieval-Augmented Generation (RAG) has the potential to
enhance LLMs for secure code generation by incorporating external security
knowledge. However, the conventional RAG design struggles with the noise of raw
security-related documents, and existing retrieval methods overlook the
significant security semantics implicitly embedded in task descriptions. To
address these issues, we propose RESCUE, a new RAG framework for secure code
generation with two key innovations. First, we propose a hybrid knowledge base
construction method that combines LLM-assisted cluster-then-summarize
distillation with program slicing, producing both high-level security
guidelines and concise, security-focused code examples. Second, we design a
hierarchical multi-faceted retrieval to traverse the constructed knowledge base
from top to bottom and integrates multiple security-critical facts at each
hierarchical level, ensuring comprehensive and accurate retrieval. We evaluated
RESCUE on four benchmarks and compared it with five state-of-the-art secure
code generation methods on six LLMs. The results demonstrate that RESCUE
improves the SecurePass@1 metric by an average of 4.8 points, establishing a
new state-of-the-art performance for security. Furthermore, we performed
in-depth analysis and ablation studies to rigorously validate the effectiveness
of individual components in RESCUE.

</details>


### [48] [CryptoGuard: Lightweight Hybrid Detection and Response to Host-based Cryptojackers in Linux Cloud Environments](https://arxiv.org/abs/2510.18324)
*Gyeonghoon Park,Jaehan Kim,Jinu Choi,Jinwoo Kim*

Main category: cs.CR

TL;DR: CryptoGuard是一个轻量级混合解决方案，通过两阶段深度学习模型检测和基于eBPF的修复机制，有效对抗Linux云环境中的加密劫持恶意软件，在保持低CPU开销的同时实现高检测精度。


<details>
  <summary>Details</summary>
Motivation: 现有的加密劫持恶意软件检测方案存在监控开销高、对混淆行为检测精度低、缺乏集成修复机制等可扩展性问题，难以在Linux云环境中有效应对。

Method: 使用基于草图和滑动窗口的系统调用监控收集行为模式，采用两阶段深度学习分类过程识别可疑活动，并集成基于eBPF的针对性修复机制来对抗逃避技术。

Result: 在123个真实世界加密劫持样本上测试，两阶段平均F1分数分别为96.12%和92.26%，优于现有基线方法，每主机仅产生0.06%的CPU开销。

Conclusion: CryptoGuard提供了一种可扩展的轻量级解决方案，能够有效检测和修复加密劫持恶意软件，在保持低资源开销的同时实现高检测精度。

Abstract: Host-based cryptomining malware, commonly known as cryptojackers, have gained
notoriety for their stealth and the significant financial losses they cause in
Linux-based cloud environments. Existing solutions often struggle with
scalability due to high monitoring overhead, low detection accuracy against
obfuscated behavior, and lack of integrated remediation. We present
CryptoGuard, a lightweight hybrid solution that combines detection and
remediation strategies to counter cryptojackers. To ensure scalability,
CryptoGuard uses sketch- and sliding window-based syscall monitoring to collect
behavior patterns with minimal overhead. It decomposes the classification task
into a two-phase process, leveraging deep learning models to identify
suspicious activity with high precision. To counter evasion techniques such as
entry point poisoning and PID manipulation, CryptoGuard integrates targeted
remediation mechanisms based on eBPF, a modern Linux kernel feature deployable
on any compatible host. Evaluated on 123 real-world cryptojacker samples, it
achieves average F1-scores of 96.12% and 92.26% across the two phases, and
outperforms state-of-the-art baselines in terms of true and false positive
rates, while incurring only 0.06% CPU overhead per host.

</details>


### [49] [Position: LLM Watermarking Should Align Stakeholders' Incentives for Practical Adoption](https://arxiv.org/abs/2510.18333)
*Yepeng Liu,Xuandong Zhao,Dawn Song,Gregory W. Wornell,Yuheng Bu*

Main category: cs.CR

TL;DR: 该论文分析了LLM水印技术在实际部署中的障碍，提出了激励对齐框架，重点介绍了上下文水印(ICW)作为激励对齐的解决方案，并呼吁在特定应用领域探索更多激励对齐的水印方法。


<details>
  <summary>Details</summary>
Motivation: 尽管LLM水印算法取得进展，但实际部署仍然有限。作者认为这是由于LLM提供商、平台和终端用户之间的激励错配造成的，表现为四个关键障碍：竞争风险、检测工具治理、鲁棒性问题和归属问题。

Method: 通过激励对齐的视角重新审视三类水印技术：模型水印、LLM文本水印和上下文水印(ICW)。特别关注ICW，它允许可信方在文档中嵌入隐藏的水印指令，当不诚实的用户提交这些文本给LLM时，输出会携带可检测的水印。

Result: 提出了激励对齐的设计原则，认为ICW在特定领域（如学术会议、教育）具有良好应用前景，因为它实现了激励对齐：用户无质量损失，可信方获得检测工具，LLM提供商保持中立。

Conclusion: LLM水印技术的实际采用需要在目标应用领域对齐利益相关者激励，并促进积极的社区参与。上下文水印(ICW)是激励对齐水印方法的一个成功范例，值得在更多领域推广。

Abstract: Despite progress in watermarking algorithms for large language models (LLMs),
real-world deployment remains limited. We argue that this gap stems from
misaligned incentives among LLM providers, platforms, and end users, which
manifest as four key barriers: competitive risk, detection-tool governance,
robustness concerns and attribution issues. We revisit three classes of
watermarking through this lens. \emph{Model watermarking} naturally aligns with
LLM provider interests, yet faces new challenges in open-source ecosystems.
\emph{LLM text watermarking} offers modest provider benefit when framed solely
as an anti-misuse tool, but can gain traction in narrowly scoped settings such
as dataset de-contamination or user-controlled provenance. \emph{In-context
watermarking} (ICW) is tailored for trusted parties, such as conference
organizers or educators, who embed hidden watermarking instructions into
documents. If a dishonest reviewer or student submits this text to an LLM, the
output carries a detectable watermark indicating misuse. This setup aligns
incentives: users experience no quality loss, trusted parties gain a detection
tool, and LLM providers remain neutral by simply following watermark
instructions. We advocate for a broader exploration of incentive-aligned
methods, with ICW as an example, in domains where trusted parties need reliable
tools to detect misuse. More broadly, we distill design principles for
incentive-aligned, domain-specific watermarking and outline future research
directions. Our position is that the practical adoption of LLM watermarking
requires aligning stakeholder incentives in targeted application domains and
fostering active community engagement.

</details>


### [50] [Censorship Chokepoints: New Battlegrounds for Regional Surveillance, Censorship and Influence on the Internet](https://arxiv.org/abs/2510.18394)
*Yong Zhang,Nishanth Sastry*

Main category: cs.CR

TL;DR: 提出了一种新的审查分析框架——"chokepoints"（瓶颈点），用于理解现代互联网审查机制，这些机制在内容生产或传输的关键节点实施大规模客户端监控和过滤。


<details>
  <summary>Details</summary>
Motivation: 随着互联网快速发展，传统的基于客户端、服务器或网络的审查分类已不足以描述新兴的复杂审查技术，需要新的分析框架来理解跨位置、多形式的信息访问障碍。

Method: 提出"chokepoints"概念，识别内容生产或传输周期中的瓶颈点，分析在这些关键位置出现的大规模客户端监控和过滤新机制。

Result: 建立了新的审查分析框架，能够更好地描述和理解现代复杂的互联网审查技术，特别是那些跨越传统分类界限的机制。

Conclusion: chokepoints框架为分析现代互联网审查提供了更有效的视角，有助于识别和理解在内容生产或传输关键节点实施的新型大规模监控和过滤机制。

Abstract: Undoubtedly, the Internet has become one of the most important conduits to
information for the general public. Nonetheless, Internet access can be and has
been limited systematically or blocked completely during political events in
numerous countries and regions by various censorship mechanisms. Depending on
where the core filtering component is situated, censorship techniques have been
classified as client-based, server-based, or network-based. However, as the
Internet evolves rapidly, new and sophisticated censorship techniques have
emerged, which involve techniques that cut across locations and involve new
forms of hurdles to information access. We argue that modern censorship can be
better understood through a new lens that we term chokepoints, which identifies
bottlenecks in the content production or delivery cycle where efficient new
forms of large-scale client-side surveillance and filtering mechanisms have
emerged.

</details>


### [51] [DeepTx: Real-Time Transaction Risk Analysis via Multi-Modal Features and LLM Reasoning](https://arxiv.org/abs/2510.18438)
*Yixuan Liu,Xinlei Li,Yi Li*

Main category: cs.CR

TL;DR: DeepTx是一个实时交易分析系统，通过模拟待处理交易、提取特征和使用多个LLM推理交易意图来检测Web3钓鱼攻击，在用户确认前提供保护。


<details>
  <summary>Details</summary>
Motivation: Web3生态系统中的钓鱼攻击日益复杂，利用欺骗性合约逻辑、恶意前端脚本和代币授权模式，需要实时检测系统来保护用户。

Method: 模拟待处理交易，提取行为、上下文和UI特征，使用多个大型语言模型推理交易意图，并通过共识机制和自我反思确保决策的稳健性和可解释性。

Result: 在钓鱼数据集上的评估显示，DeepTx实现了高精度和高召回率。

Conclusion: DeepTx系统能够有效检测Web3钓鱼威胁，在用户确认交易前提供实时保护。

Abstract: Phishing attacks in Web3 ecosystems are increasingly sophisticated,
exploiting deceptive contract logic, malicious frontend scripts, and token
approval patterns. We present DeepTx, a real-time transaction analysis system
that detects such threats before user confirmation. DeepTx simulates pending
transactions, extracts behavior, context, and UI features, and uses multiple
large language models (LLMs) to reason about transaction intent. A consensus
mechanism with self-reflection ensures robust and explainable decisions.
Evaluated on our phishing dataset, DeepTx achieves high precision and recall
(demo video: https://youtu.be/4OfK9KCEXUM).

</details>


### [52] [PP3D: An In-Browser Vision-Based Defense Against Web Behavior Manipulation Attacks](https://arxiv.org/abs/2510.18465)
*Spencer King,Irfan Ozen,Karthika Subramani,Saranyan Senthivel,Phani Vadrevu,Roberto Perdisci*

Main category: cs.CR

TL;DR: 提出了Pixel Patrol 3D (PP3D)，首个端到端浏览器框架，用于实时发现、检测和防御基于行为操纵的社会工程攻击。


<details>
  <summary>Details</summary>
Motivation: 基于网络的行为操纵攻击（如恐吓软件、虚假软件下载、技术支持诈骗等）利用人类决策漏洞，相比其他攻击类型研究不足，现有技术工作主要关注测量而非通用防御。

Method: PP3D包含在浏览器扩展中实现的视觉检测模型，在客户端部署模型以保护桌面和移动设备用户，同时保护隐私。

Result: 评估显示PP3D在1%误报率下达到99%以上的检测率，在不同设备上保持良好的延迟和开销性能。即使面对训练数月后收集的新BMA样本，仍能在1%误报率下达到97%以上的检测率。

Conclusion: 该框架为广泛且不断演变的网络行为操纵攻击提供了实用、有效且可泛化的防御方案。

Abstract: Web-based behavior-manipulation attacks (BMAs) - such as scareware, fake
software downloads, tech support scams, etc. - are a class of social
engineering (SE) attacks that exploit human decision-making vulnerabilities.
These attacks remain under-studied compared to other attacks such as
information harvesting attacks (e.g., phishing) or malware infections. Prior
technical work has primarily focused on measuring BMAs, offering little in the
way of generic defenses.
  To address this gap, we introduce Pixel Patrol 3D (PP3D), the first
end-to-end browser framework for discovering, detecting, and defending against
behavior-manipulating SE attacks in real time. PP3D consists of a visual
detection model implemented within a browser extension, which deploys the model
client-side to protect users across desktop and mobile devices while preserving
privacy.
  Our evaluation shows that PP3D can achieve above 99% detection rate at 1%
false positives, while maintaining good latency and overhead performance across
devices. Even when faced with new BMA samples collected months after training
the detection model, our defense system can still achieve above 97% detection
rate at 1% false positives. These results demonstrate that our framework offers
a practical, effective, and generalizable defense against a broad and evolving
class of web behavior-manipulation attacks.

</details>


### [53] [The Attribution Story of WhisperGate: An Academic Perspective](https://arxiv.org/abs/2510.18484)
*Oleksandr Adamov,Anders Carlsson*

Main category: cs.CR

TL;DR: 该论文通过WhisperGate网络攻击案例研究，结合传统机器学习和大型语言模型，展示了AI/GenAI在解决网络攻击归因挑战方面的能力，成功将攻击归因于俄罗斯军事情报部门GRU的Ember Bear小组。


<details>
  <summary>Details</summary>
Motivation: 研究网络攻击归因的挑战，特别是高级持续性威胁(APTs)，探索AI技术在解决归因问题中的应用潜力。

Method: 采用案例研究方法，分析WhisperGate网络攻击事件；结合传统机器学习分类器和大型语言模型(ChatGPT)分析攻击指标、战术和技术；通过技术报告和情报数据进行归因验证。

Result: 发现与Sandworm小组(GRU Unit 74455)存在重叠指标，但更有力证据指向Ember Bear小组(GRU Unit 29155)；当LLM经过微调或上下文增强时，归因准确性显著提高。

Conclusion: AI/GenAI技术经过适当微调能够有效解决网络攻击归因挑战，为网络安全归因提供了新的技术途径。

Abstract: This paper explores the challenges of cyberattack attribution, specifically
APTs, applying the case study approach for the WhisperGate cyber operation of
January 2022 executed by the Russian military intelligence service (GRU) and
targeting Ukrainian government entities. The study provides a detailed review
of the threat actor identifiers and taxonomies used by leading cybersecurity
vendors, focusing on the evolving attribution from Microsoft, ESET, and
CrowdStrike researchers. Once the attribution to Ember Bear (GRU Unit 29155) is
established through technical and intelligence reports, we use both traditional
machine learning classifiers and a large language model (ChatGPT) to analyze
the indicators of compromise (IoCs), tactics, and techniques to statistically
and semantically attribute the WhisperGate attack. Our findings reveal
overlapping indicators with the Sandworm group (GRU Unit 74455) but also strong
evidence pointing to Ember Bear, especially when the LLM is fine-tuned or
contextually augmented with additional intelligence. Thus, showing how AI/GenAI
with proper fine-tuning are capable of solving the attribution challenge.

</details>


### [54] [One Size Fits All? A Modular Adaptive Sanitization Kit (MASK) for Customizable Privacy-Preserving Phone Scam Detection](https://arxiv.org/abs/2510.18493)
*Kangzhong Wang,Zitong Shen,Youqian Zhang,Michael MK Cheung,Xiapu Luo,Grace Ngai,Eugene Yujun Fu*

Main category: cs.CR

TL;DR: 提出了MASK框架，用于在利用大语言模型检测电话诈骗的同时保护用户隐私，支持动态隐私调整和多种脱敏方法。


<details>
  <summary>Details</summary>
Motivation: 电话诈骗检测需要分析通话内容，但其中包含的敏感个人信息存在隐私泄露风险，需要在检测效果和隐私保护之间取得平衡。

Method: 提出MASK框架，采用模块化可扩展架构，支持从传统关键词脱敏到神经方法的多种脱敏技术，可根据用户偏好动态调整隐私保护级别。

Result: 框架设计支持个性化隐私感知的检测系统，能够平衡用户信任和检测效果。

Conclusion: MASK框架为构建隐私保护的LLM检测系统提供了可行方案，其设计理念可扩展到其他需要隐私保护的场景。

Abstract: Phone scams remain a pervasive threat to both personal safety and financial
security worldwide. Recent advances in large language models (LLMs) have
demonstrated strong potential in detecting fraudulent behavior by analyzing
transcribed phone conversations. However, these capabilities introduce notable
privacy risks, as such conversations frequently contain sensitive personal
information that may be exposed to third-party service providers during
processing. In this work, we explore how to harness LLMs for phone scam
detection while preserving user privacy. We propose MASK (Modular Adaptive
Sanitization Kit), a trainable and extensible framework that enables dynamic
privacy adjustment based on individual preferences. MASK provides a pluggable
architecture that accommodates diverse sanitization methods - from traditional
keyword-based techniques for high-privacy users to sophisticated neural
approaches for those prioritizing accuracy. We also discuss potential modeling
approaches and loss function designs for future development, enabling the
creation of truly personalized, privacy-aware LLM-based detection systems that
balance user trust and detection effectiveness, even beyond phone scam context.

</details>


### [55] [Prompting the Priorities: A First Look at Evaluating LLMs for Vulnerability Triage and Prioritization](https://arxiv.org/abs/2510.18508)
*Osama Al Haddad,Muhammad Ikram,Ejaz Ahmed,Young Lee*

Main category: cs.CR

TL;DR: 评估四种大语言模型在漏洞分类任务中的表现，发现Gemini在SSVC框架决策点预测中表现最佳，但所有模型都无法替代专家判断。


<details>
  <summary>Details</summary>
Motivation: 安全分析师面临大量漏洞积压的压力，需要自动化工具辅助漏洞解释和优先级排序。

Method: 使用384个真实漏洞数据，对ChatGPT、Claude、Gemini和DeepSeek四种模型进行超过165,000次查询测试，评估12种提示技术在SSVC框架决策点预测中的表现。

Result: Gemini在四个决策点中的三个表现最佳，提示技术中的示例方法能提高准确性，但所有模型在某些决策点上仍有困难，且倾向于过度预测风险。

Conclusion: 当前LLMs不能替代专家判断，但特定模型和提示组合在目标SSVC决策中表现出中等有效性，可谨慎用于支持漏洞优先级排序工作流程。

Abstract: Security analysts face increasing pressure to triage large and complex
vulnerability backlogs. Large Language Models (LLMs) offer a potential aid by
automating parts of the interpretation process. We evaluate four models
(ChatGPT, Claude, Gemini, and DeepSeek) across twelve prompting techniques to
interpret semi-structured and unstructured vulnerability information. As a
concrete use case, we test each model's ability to predict decision points in
the Stakeholder-Specific Vulnerability Categorization (SSVC) framework:
Exploitation, Automatable, Technical Impact, and Mission and Wellbeing.
  Using 384 real-world vulnerabilities from the VulZoo dataset, we issued more
than 165,000 queries to assess performance under prompting styles including
one-shot, few-shot, and chain-of-thought. We report F1 scores for each SSVC
decision point and Cohen's kappa (weighted and unweighted) for the final SSVC
decision outcomes. Gemini consistently ranked highest, leading on three of four
decision points and yielding the most correct recommendations. Prompting with
exemplars generally improved accuracy, although all models struggled on some
decision points. Only DeepSeek achieved fair agreement under weighted metrics,
and all models tended to over-predict risk.
  Overall, current LLMs do not replace expert judgment. However, specific LLM
and prompt combinations show moderate effectiveness for targeted SSVC
decisions. When applied with care, LLMs can support vulnerability
prioritization workflows and help security teams respond more efficiently to
emerging threats.

</details>


### [56] [Deep Q-Learning Assisted Bandwidth Reservation for Multi-Operator Time-Sensitive Vehicular Networking](https://arxiv.org/abs/2510.18553)
*Abdullah Al-Khatib,Albert Gergus,Muneeb Ul Hassan,Abdelmajid Khelil,Klaus Mossner,Holger Timinger*

Main category: cs.CR

TL;DR: 提出基于DDQN的多目标带宽预留更新策略，在不确定环境下最小化带宽成本并确保可靠资源供应


<details>
  <summary>Details</summary>
Motivation: 现有带宽预留方案难以在不确定的未来预留时间和带宽成本下提供高效且经济的安全关键型车联网应用所需带宽

Method: 使用双深度Q网络(DDQN)设计最优策略的多目标带宽预留更新方法，解决预订不足和超额预订等不确定场景

Result: 相比贪婪更新和其他深度强化学习方法，在所有测试场景中带宽成本降低40%，并能以经济高效方式解决不确定情况

Conclusion: 所提出的DDQN策略在最小化带宽成本的同时，有效处理车联网应用中的不确定资源预留问题

Abstract: Very few available individual bandwidth reservation schemes provide efficient
and cost-effective bandwidth reservation that is required for safety-critical
and time-sensitive vehicular networked applications. These schemes allow
vehicles to make reservation requests for the required resources. Accordingly,
a Mobile Network Operator (MNO) can allocate and guarantee bandwidth resources
based on these requests. However, due to uncertainty in future reservation time
and bandwidth costs, the design of an optimized reservation strategy is
challenging. In this article, we propose a novel multi-objective bandwidth
reservation update approach with an optimal strategy based on Double Deep
Q-Network (DDQN). The key design objectives are to minimize the reservation
cost with multiple MNOs and to ensure reliable resource provisioning in
uncertain situations by solving scenarios such as underbooked and overbooked
reservations along the driving path. The enhancements and advantages of our
proposed strategy have been demonstrated through extensive experimental results
when compared to other methods like greedy update or other deep reinforcement
learning approaches. Our strategy demonstrates a 40% reduction in bandwidth
costs across all investigated scenarios and simultaneously resolves uncertain
situations in a cost-effective manner.

</details>


### [57] [The Trust Paradox in LLM-Based Multi-Agent Systems: When Collaboration Becomes a Security Vulnerability](https://arxiv.org/abs/2510.18563)
*Zijie Xu,Minfeng Qi,Shiqing Wu,Lefeng Zhang,Qiwen Wei,Han He,Ningran Li*

Main category: cs.CR

TL;DR: 论文提出并验证了信任-脆弱性悖论(TVP)：在多智能体系统中，增加信任能提升协作效率，但同时也增加了过度暴露和过度授权的安全风险。


<details>
  <summary>Details</summary>
Motivation: 多智能体系统发展迅速，但信任与安全之间的张力尚未充分探索。研究者希望理解信任增加如何同时带来协作收益和安全风险。

Method: 构建包含3个宏观场景和19个子场景的场景游戏数据集，进行闭环交互实验，将信任参数化。使用最小必要信息(MNI)作为安全基线，提出过度暴露率(OER)和授权漂移(AD)两个统一指标。

Result: 多个模型后端和编排框架的结果显示一致趋势：更高信任度提高任务成功率，但也增加暴露风险，不同系统存在异质的信任-风险映射关系。敏感信息重新分配和守护智能体等防御措施能降低OER和AD。

Conclusion: 研究形式化了TVP悖论，建立了可复现的基准和统一指标，证明在多智能体系统设计中，信任必须作为首要安全变量进行建模和调度。

Abstract: Multi-agent systems powered by large language models are advancing rapidly,
yet the tension between mutual trust and security remains underexplored. We
introduce and empirically validate the Trust-Vulnerability Paradox (TVP):
increasing inter-agent trust to enhance coordination simultaneously expands
risks of over-exposure and over-authorization. To investigate this paradox, we
construct a scenario-game dataset spanning 3 macro scenes and 19 sub-scenes,
and run extensive closed-loop interactions with trust explicitly parameterized.
Using Minimum Necessary Information (MNI) as the safety baseline, we propose
two unified metrics: Over-Exposure Rate (OER) to detect boundary violations,
and Authorization Drift (AD) to capture sensitivity to trust levels. Results
across multiple model backends and orchestration frameworks reveal consistent
trends: higher trust improves task success but also heightens exposure risks,
with heterogeneous trust-to-risk mappings across systems. We further examine
defenses such as Sensitive Information Repartitioning and Guardian-Agent
enablement, both of which reduce OER and attenuate AD. Overall, this study
formalizes TVP, establishes reproducible baselines with unified metrics, and
demonstrates that trust must be modeled and scheduled as a first-class security
variable in multi-agent system design.

</details>


### [58] [Privacy-Preserving Healthcare Data in IoT: A Synergistic Approach with Deep Learning and Blockchain](https://arxiv.org/abs/2510.18568)
*Behnam Rezaei Bezanjani,Seyyed Hamid Ghafouri,Reza Gholamrezaei*

Main category: cs.CR

TL;DR: 提出一个三阶段安全框架，通过信誉评估、区块链集成和轻量级LSTM异常检测，提升物联网医疗系统的安全性和可靠性。


<details>
  <summary>Details</summary>
Motivation: 物联网设备在医疗领域的应用带来了实时监测和个性化治疗等优势，但也引入了严重的安全风险，传统安全措施难以应对物联网环境的异构性、资源限制和实时处理需求。

Method: 三阶段框架：1) 基于信誉的信任评估机制结合设备行为分析和链下存储；2) 区块链与轻量工作量证明机制集成确保数据不可篡改和安全通信；3) 轻量级LSTM模型进行实时异常检测和分类。

Result: 仿真结果显示，相比现有方法，该框架在精确度、准确率和召回率上提升2%，攻击检测率提高5%，误报率降低3%。

Conclusion: 该框架能有效解决关键安全问题，同时保持可扩展性和实时性能，为物联网医疗系统提供了可靠的安全保障。

Abstract: The integration of Internet of Things (IoT) devices in healthcare has
revolutionized patient care by enabling real-time monitoring, personalized
treatments, and efficient data management. However, this technological
advancement introduces significant security risks, particularly concerning the
confidentiality, integrity, and availability of sensitive medical data.
Traditional security measures are often insufficient to address the unique
challenges posed by IoT environments, such as heterogeneity, resource
constraints, and the need for real-time processing. To tackle these challenges,
we propose a comprehensive three-phase security framework designed to enhance
the security and reliability of IoT-enabled healthcare systems. In the first
phase, the framework assesses the reliability of IoT devices using a
reputation-based trust estimation mechanism, which combines device behavior
analytics with off-chain data storage to ensure scalability. The second phase
integrates blockchain technology with a lightweight proof-of-work mechanism,
ensuring data immutability, secure communication, and resistance to
unauthorized access. The third phase employs a lightweight Long Short-Term
Memory (LSTM) model for anomaly detection and classification, enabling
real-time identification of cyber threats. Simulation results demonstrate that
the proposed framework outperforms existing methods, achieving a 2% increase in
precision, accuracy, and recall, a 5% higher attack detection rate, and a 3%
reduction in false alarm rate. These improvements highlight the framework's
ability to address critical security concerns while maintaining scalability and
real-time performance.

</details>


### [59] [Forward to Hell? On the Potentials of Misusing Transparent DNS Forwarders in Reflective Amplification Attacks](https://arxiv.org/abs/2510.18572)
*Maynard Koch,Florian Dolzmann,Thomas C. Schmidt,Matthias Wählisch*

Main category: cs.CR

TL;DR: 透明DNS转发器通过绕过速率限制和防火墙规则，将DNS请求转发到开放递归解析器，从而加剧DNS反射放大攻击的威胁。


<details>
  <summary>Details</summary>
Motivation: DNS基础设施因易受反射放大攻击而臭名昭著，尽管已有多种防护措施，但透明DNS转发器这一广泛部署但功能不完整的组件带来了新的安全威胁。

Method: 通过分析透明DNS转发器的工作机制，展示其如何绕过现有防护措施，并实证验证其对DNS anycast基础设施的放大效应。

Result: 透明DNS转发器能够绕过速率限制，通过DNS anycast基础设施实现高达14倍的攻击放大效果，并能规避保护递归解析器的防火墙规则。

Conclusion: 透明DNS转发器严重威胁互联网基础设施安全，使受保护的DNS解析器成为全球DNS攻击面的一部分，需要新的防护措施来应对这一威胁。

Abstract: The DNS infrastructure is infamous for facilitating reflective amplification
attacks. Various countermeasures such as server shielding, access control, rate
limiting, and protocol restrictions have been implemented. Still, the threat
remains throughout the deployment of DNS servers. In this paper, we report on
and evaluate the often unnoticed threat that derives from transparent DNS
forwarders, a widely deployed, incompletely functional set of DNS components.
Transparent DNS forwarders transfer DNS requests without rebuilding packets
with correct source addresses. As such, transparent forwarders feed DNS
requests into (mainly powerful and anycasted) open recursive resolvers, which
thereby can be misused to participate unwillingly in distributed reflective
amplification attacks. We show how transparent forwarders raise severe threats
to the Internet infrastructure. They easily circumvent rate limiting and
achieve an additional, scalable impact via the DNS anycast infrastructure. We
empirically verify this scaling behavior up to a factor of 14. Transparent
forwarders can also assist in bypassing firewall rules that protect recursive
resolvers, making these shielded infrastructure entities part of the global DNS
attack surface.

</details>


### [60] [CLASP: Cost-Optimized LLM-based Agentic System for Phishing Detection](https://arxiv.org/abs/2510.18585)
*Fouad Trad,Ali Chehab*

Main category: cs.CR

TL;DR: CLASP是一个基于多智能体LLM的钓鱼网站检测系统，通过分析URL结构、网页截图和HTML内容来识别钓鱼威胁，在保持低成本的同时实现了高检测准确率。


<details>
  <summary>Details</summary>
Motivation: 钓鱼网站仍然是重大的网络安全威胁，需要准确且成本效益高的检测机制。现有解决方案在检测准确性和成本控制方面存在不足。

Method: 使用基于大语言模型的多智能体系统，分别分析URL结构、网页截图和HTML内容。实验了多种智能体组合策略，最终设计了在保证检测准确性的同时最小化成本的策略组合。

Result: 在新建数据集上，Gemini 1.5 Flash模型取得了83.01%的F1分数，平均处理时间为2.78秒/网站，每1000个网站的API成本约为3.18美元。相比先前解决方案，召回率提高了40%以上，F1分数提高了20%。

Conclusion: CLASP系统在钓鱼网站检测方面表现优异，在保持低成本的同时显著提升了检测性能。公开数据集将支持更先进的钓鱼检测系统开发。

Abstract: Phishing websites remain a significant cybersecurity threat, necessitating
accurate and cost-effective detection mechanisms. In this paper, we present
CLASP, a novel system that effectively identifies phishing websites by
leveraging multiple intelligent agents, built using large language models
(LLMs), to analyze different aspects of a web resource. The system processes
URLs or QR codes, employing specialized LLM-based agents that evaluate the URL
structure, webpage screenshot, and HTML content to predict potential phishing
threats. To optimize performance while minimizing operational costs, we
experimented with multiple combination strategies for agent-based analysis,
ultimately designing a strategic combination that ensures the per-website
evaluation expense remains minimal without compromising detection accuracy. We
tested various LLMs, including Gemini 1.5 Flash and GPT-4o mini, to build these
agents and found that Gemini 1.5 Flash achieved the best performance with an F1
score of 83.01% on a newly curated dataset. Also, the system maintained an
average processing time of 2.78 seconds per website and an API cost of around
$3.18 per 1,000 websites. Moreover, CLASP surpasses leading previous solutions,
achieving over 40% higher recall and a 20% improvement in F1 score for phishing
detection on the collected dataset. To support further research, we have made
our dataset publicly available, supporting the development of more advanced
phishing detection systems.

</details>


### [61] [Evaluating Large Language Models in detecting Secrets in Android Apps](https://arxiv.org/abs/2510.18601)
*Marco Alecci,Jordan Samhi,Tegawendé F. Bissyandé,Jacques Klein*

Main category: cs.CR

TL;DR: SecretLoc是一个基于LLM的方法，用于检测Android应用中的硬编码密钥，无需依赖预定义模式或训练数据，发现了4828个现有方法未检测到的密钥。


<details>
  <summary>Details</summary>
Motivation: 开发者经常将API密钥等认证密钥硬编码到Android应用中，这些密钥可能通过逆向工程被提取，造成安全风险。现有检测方法需要先验知识，无法识别未知类型的密钥。

Method: 使用基于LLM的方法，利用上下文和结构线索来识别密钥，不依赖预定义模式或标记训练集。

Result: 在基准数据集上检测到4828个现有方法未发现的密钥，包括10多种新型密钥；在5000个Google Play应用中，42.5%包含硬编码密钥。

Conclusion: LLM既能被分析师用于发现密钥，也能被攻击者利用，凸显了移动生态系统中主动密钥管理和更强缓解措施的迫切需求。

Abstract: Mobile apps often embed authentication secrets, such as API keys, tokens, and
client IDs, to integrate with cloud services. However, developers often
hardcode these credentials into Android apps, exposing them to extraction
through reverse engineering. Once compromised, adversaries can exploit secrets
to access sensitive data, manipulate resources, or abuse APIs, resulting in
significant security and financial risks. Existing detection approaches, such
as regex-based analysis, static analysis, and machine learning, are effective
for identifying known patterns but are fundamentally limited: they require
prior knowledge of credential structures, API signatures, or training data.
  In this paper, we propose SecretLoc, an LLM-based approach for detecting
hardcoded secrets in Android apps. SecretLoc goes beyond pattern matching; it
leverages contextual and structural cues to identify secrets without relying on
predefined patterns or labeled training sets. Using a benchmark dataset from
the literature, we demonstrate that SecretLoc detects secrets missed by regex-,
static-, and ML-based methods, including previously unseen types of secrets. In
total, we discovered 4828 secrets that were undetected by existing approaches,
discovering more than 10 "new" types of secrets, such as OpenAI API keys,
GitHub Access Tokens, RSA private keys, and JWT tokens, and more.
  We further extend our analysis to newly crawled apps from Google Play, where
we uncovered and responsibly disclosed additional hardcoded secrets. Across a
set of 5000 apps, we detected secrets in 2124 apps (42.5%), several of which
were confirmed and remediated by developers after we contacted them. Our
results reveal a dual-use risk: if analysts can uncover these secrets with
LLMs, so can attackers. This underscores the urgent need for proactive secret
management and stronger mitigation practices across the mobile ecosystem.

</details>


### [62] [DRsam: Detection of Fault-Based Microarchitectural Side-Channel Attacks in RISC-V Using Statistical Preprocessing and Association Rule Mining](https://arxiv.org/abs/2510.18612)
*Muhammad Hassan,Maria Mushtaq,Jaan Raik,Tara Ghasempouri*

Main category: cs.CR

TL;DR: 提出了一种结合统计预处理和关联规则挖掘的新检测方法，用于检测RISC-V处理器中的微架构侧信道攻击，相比现有方法在准确率、精确率和召回率方面均有提升。


<details>
  <summary>Details</summary>
Motivation: RISC-V处理器在关键应用中日益普及，但其对微架构侧信道攻击的脆弱性是一个严重问题。与x86和ARM相比，RISC-V的微架构攻击检测研究相对不足，现有基于机器学习的方法存在一些实际问题需要进一步研究。

Method: 利用gem5模拟器，提出结合统计预处理和关联规则挖掘的检测方法，具有重新配置能力，可泛化用于检测任何微架构攻击。

Result: 与最先进方法相比，在密码学、计算和内存密集型工作负载下，准确率提高5.15%，精确率提高7%，召回率提高3.91%，并能检测新的flush+fault攻击变种。

Conclusion: 该方法不仅检测性能优越，而且由于依赖关联规则，其人类可解释性为理解攻击和良性应用程序执行期间的微架构行为提供了深入洞察。

Abstract: RISC-V processors are becoming ubiquitous in critical applications, but their
susceptibility to microarchitectural side-channel attacks is a serious concern.
Detection of microarchitectural attacks in RISC-V is an emerging research topic
that is relatively underexplored, compared to x86 and ARM. The first line of
work to detect flush+fault-based microarchitectural attacks in RISC-V leverages
Machine Learning (ML) models, yet it leaves several practical aspects that need
further investigation. To address overlooked issues, we leveraged gem5 and
propose a new detection method combining statistical preprocessing and
association rule mining having reconfiguration capabilities to generalize the
detection method for any microarchitectural attack. The performance comparison
with state-of-the-art reveals that the proposed detection method achieves up to
5.15% increase in accuracy, 7% rise in precision, and 3.91% improvement in
recall under the cryptographic, computational, and memory-intensive workloads
alongside its flexibility to detect new variant of flush+fault attack.
Moreover, as the attack detection relies on association rules, their
human-interpretable nature provides deep insight to understand
microarchitectural behavior during the execution of attack and benign
applications.

</details>


### [63] [Qatsi: Stateless Secret Generation via Hierarchical Memory-Hard Key Derivation](https://arxiv.org/abs/2510.18614)
*René Coignard,Anton Rygin*

Main category: cs.CR

TL;DR: Qatsi是一个基于Argon2id的分层密钥派生方案，无需持久存储即可生成可重现的加密密钥，通过内存密集型计算提供高安全性。


<details>
  <summary>Details</summary>
Motivation: 消除基于保险库的攻击面，为隔离系统和主凭证生成提供无状态、可重现的密钥派生方案。

Method: 使用Argon2id算法进行分层密钥派生，从单个高熵主密钥和上下文层确定性生成所有密钥，采用内存密集型计算(64-128 MiB, 16-32次迭代)和可证明均匀的拒绝采样。

Result: 输出熵值达到103-312位，GPU攻击成本极高(80位主密钥在单GPU下需要2.4×10^16年)，在Apple M1 Pro上标准模式544ms、偏执模式2273ms完成单层派生。

Conclusion: Qatsi为需要无状态可重现性的场景提供了安全实用的解决方案，特别适合隔离系统和主凭证生成，但牺牲了密钥轮换的灵活性。

Abstract: We present Qatsi, a hierarchical key derivation scheme using Argon2id that
generates reproducible cryptographic secrets without persistent storage. The
system eliminates vault-based attack surfaces by deriving all secrets
deterministically from a single high-entropy master secret and contextual
layers. Outputs achieve 103-312 bits of entropy through memory-hard derivation
(64-128 MiB, 16-32 iterations) and provably uniform rejection sampling over
7776-word mnemonics or 90-character passwords. We formalize the hierarchical
construction, prove output uniformity, and quantify GPU attack costs: $2.4
\times 10^{16}$ years for 80-bit master secrets on single-GPU adversaries under
Paranoid parameters (128 MiB memory). The implementation in Rust provides
automatic memory zeroization, compile-time wordlist integrity verification, and
comprehensive test coverage. Reference benchmarks on Apple M1 Pro (2021)
demonstrate practical usability with 544 ms Standard mode and 2273 ms Paranoid
mode single-layer derivations. Qatsi targets air-gapped systems and master
credential generation where stateless reproducibility outweighs rotation
flexibility.

</details>


### [64] [Exploring Membership Inference Vulnerabilities in Clinical Large Language Models](https://arxiv.org/abs/2510.18674)
*Alexander Nemecek,Zebin Yun,Zahra Rahmani,Yaniv Harel,Vipin Chaudhary,Mahmood Sharif,Erman Ayday*

Main category: cs.CR

TL;DR: 该工作进展研究探索了临床大语言模型中的成员推理漏洞，评估攻击者能否推断特定患者记录是否用于模型训练，发现存在有限但可测量的隐私泄露风险。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型在临床决策支持、文档记录和患者信息系统中的广泛应用，确保其隐私性和可信性已成为医疗保健领域的关键挑战。在敏感电子健康记录数据上微调LLMs虽然能提高领域对齐，但也增加了通过模型行为暴露患者信息的风险。

Method: 使用最先进的临床问答模型Llemr，评估了基于规范损失的攻击和基于领域动机的释义扰动策略，后者更真实地反映了临床对抗条件。

Result: 初步发现显示存在有限但可测量的成员泄露，表明当前临床LLMs提供部分抵抗但仍容易受到可能破坏临床AI信任度的微妙隐私风险。

Conclusion: 这些结果推动了上下文感知、领域特定隐私评估和防御措施的持续开发，如差分隐私微调和释义感知训练，以加强医疗AI系统的安全性和可信性。

Abstract: As large language models (LLMs) become progressively more embedded in
clinical decision-support, documentation, and patient-information systems,
ensuring their privacy and trustworthiness has emerged as an imperative
challenge for the healthcare sector. Fine-tuning LLMs on sensitive electronic
health record (EHR) data improves domain alignment but also raises the risk of
exposing patient information through model behaviors. In this work-in-progress,
we present an exploratory empirical study on membership inference
vulnerabilities in clinical LLMs, focusing on whether adversaries can infer if
specific patient records were used during model training. Using a
state-of-the-art clinical question-answering model, Llemr, we evaluate both
canonical loss-based attacks and a domain-motivated paraphrasing-based
perturbation strategy that more realistically reflects clinical adversarial
conditions. Our preliminary findings reveal limited but measurable membership
leakage, suggesting that current clinical LLMs provide partial resistance yet
remain susceptible to subtle privacy risks that could undermine trust in
clinical AI adoption. These results motivate continued development of
context-aware, domain-specific privacy evaluations and defenses such as
differential privacy fine-tuning and paraphrase-aware training, to strengthen
the security and trustworthiness of healthcare AI systems.

</details>


### [65] [International Students and Scams: At Risk Abroad](https://arxiv.org/abs/2510.18715)
*Katherine Zhang,Arjun Arunasalam,Pubali Datta,Z. Berkay Celik*

Main category: cs.CR

TL;DR: 该研究调查了在美国的国际学生面临的网络诈骗风险，发现他们因签证身份、文化适应和财务要求等独特因素而更容易成为诈骗目标，且面临报告障碍。


<details>
  <summary>Details</summary>
Motivation: 现有研究虽然关注了国际学生的安全隐私问题，但缺乏对他们如何独特地受到网络诈骗影响的研究。国际学生面临签证、文化适应、远程住房安排和财务安排等挑战，加上最近的签证政策变化，使他们更容易成为诈骗目标。

Method: 采用两阶段用户研究：问卷调查（n=48）和半结构化访谈（n=9），调查国际学生接触诈骗的经历、报告行为以及对现有预防资源的认知。

Result: 国际学生经常成为诈骗目标（如冒充政府官员），担心法律后果或遣返，这会延长与诈骗者的互动。他们缺乏对可靠资源的认知或访问，在报告财务损失时面临独特障碍，因为需要证明财务能力以维持在美国的合法身份。

Conclusion: 研究结果为利益相关者提供了指导方针，以更好地帮助国际学生应对网络诈骗，强调需要针对他们独特处境的安全教育和支持系统。

Abstract: International students (IntlS) in the US refer to foreign students who
acquire student visas to study in the US, primarily in higher education. As
IntlS arrive in the US, they face several challenges, such as adjusting to a
new country and culture, securing housing remotely, and arranging finances for
tuition and personal expenses. These experiences, coupled with recent events
such as visa revocations and the cessation of new visas, compound IntlS' risk
of being targeted by and falling victim to online scams. While prior work has
investigated IntlS' security and privacy, as well as general end users'
reactions to online scams, research on how IntlS are uniquely impacted by scams
remains largely absent.
  To address this gap, we conduct a two-phase user study comprising surveys
(n=48) and semi-structured interviews (n=9). We investigate IntlS' exposure and
interactions with scams, post-exposure actions such as reporting, and their
perceptions of the usefulness of existing prevention resources and the barriers
to following prevention advice. We find that IntlS are often targeted by scams
(e.g., attackers impersonating government officials) and fear legal
implications or deportation, which directly impacts their interactions with
scams (e.g., they may prolong engagement with a scammer due to a sense of
urgency). Interestingly, we also find that IntlS may lack awareness of - or
access to - reliable resources that inform them about scams or guide them in
reporting incidents to authorities. In fact, they may also face unique barriers
in enacting scam prevention advice, such as avoiding reporting financial
losses, since IntlS are required to demonstrate financial ability to stay in
the US. The findings produced by our study help synthesize guidelines for
stakeholders to better aid IntlS in reacting to scams.

</details>


### [66] [HarmNet: A Framework for Adaptive Multi-Turn Jailbreak Attacks on Large Language Models](https://arxiv.org/abs/2510.18728)
*Sidhant Narula,Javad Rafiei Asl,Mohammad Ghasemigol,Eduardo Blanco,Daniel Takabi*

Main category: cs.CR

TL;DR: HarmNet是一个模块化框架，通过分层语义网络、反馈驱动的模拟器和网络遍历器，系统性地探索和优化对抗性攻击路径，显著提高了对大型语言模型的多轮越狱攻击成功率。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型仍然容易受到多轮越狱攻击，现有方法在攻击成功率和隐蔽性方面存在不足，需要开发更有效的对抗性攻击框架。

Method: HarmNet包含三个核心组件：ThoughtNet（分层语义网络）、反馈驱动的模拟器（迭代查询优化）和网络遍历器（实时自适应攻击执行），系统性地探索对抗性空间。

Result: 在闭源和开源LLMs上的实验表明，HarmNet优于现有最先进方法，攻击成功率更高。例如在Mistral-7B上达到99.4%的攻击成功率，比最佳基线高出13.9%。

Conclusion: HarmNet框架能够有效发现隐蔽且高成功率的攻击路径，显著提升了对大型语言模型的越狱攻击能力。

Abstract: Large Language Models (LLMs) remain vulnerable to multi-turn jailbreak
attacks. We introduce HarmNet, a modular framework comprising ThoughtNet, a
hierarchical semantic network; a feedback-driven Simulator for iterative query
refinement; and a Network Traverser for real-time adaptive attack execution.
HarmNet systematically explores and refines the adversarial space to uncover
stealthy, high-success attack paths. Experiments across closed-source and
open-source LLMs show that HarmNet outperforms state-of-the-art methods,
achieving higher attack success rates. For example, on Mistral-7B, HarmNet
achieves a 99.4% attack success rate, 13.9% higher than the best baseline.
Index terms: jailbreak attacks; large language models; adversarial framework;
query refinement.

</details>


### [67] [sNVMe-oF: Secure and Efficient Disaggregated Storage](https://arxiv.org/abs/2510.18756)
*Marcin Chrapek,Meni Orenbach,Ahmad Atamli,Marcin Copik,Fritz Alder,Torsten Hoefler*

Main category: cs.CR

TL;DR: sNVMe-oF是一种安全的存储管理系统，通过扩展NVMe-oF协议并提供机密性、完整性和新鲜性保证，解决了传统机密计算在存储安全方面的性能和扩展性问题。


<details>
  <summary>Details</summary>
Motivation: 现代数据中心采用NVMe-oF分解存储获得高性能，同时机密计算成为安全标准，但传统CC方法在保护存储时难以扩展且会损害性能或安全性。

Method: 扩展NVMe-oF协议但不修改协议本身，引入计数器租赁等新概念，利用NVMe元数据优化数据路径，构建分解式Hazel Merkle树，避免冗余IPSec保护，并使用支持CC的智能网卡加速器。

Result: 在NVIDIA BlueField-3上原型实现，对于合成模式和AI训练，性能下降仅为2%，能够达到线速性能。

Conclusion: sNVMe-oF成功实现了高性能的机密存储解决方案，在不修改NVMe-oF协议的前提下提供了强大的安全保障，且性能损失极小。

Abstract: Disaggregated storage with NVMe-over-Fabrics (NVMe-oF) has emerged as the
standard solution in modern data centers, achieving superior performance,
resource utilization, and power efficiency. Simultaneously, confidential
computing (CC) is becoming the de facto security paradigm, enforcing stronger
isolation and protection for sensitive workloads. However, securing
state-of-the-art storage with traditional CC methods struggles to scale and
compromises performance or security. To address these issues, we introduce
sNVMe-oF, a storage management system extending the NVMe-oF protocol and
adhering to the CC threat model by providing confidentiality, integrity, and
freshness guarantees. sNVMe-oF offers an appropriate control path and novel
concepts such as counter-leasing. sNVMe-oF also optimizes data path performance
by leveraging NVMe metadata, introducing a new disaggregated Hazel Merkle Tree
(HMT), and avoiding redundant IPSec protections. We achieve this without
modifying the NVMe-oF protocol. To prevent excessive resource usage while
delivering line rate, sNVMe-oF also uses accelerators of CC-capable smart NICs.
We prototype sNVMe-oF on an NVIDIA BlueField-3 and demonstrate how it can
achieve as little as 2% performance degradation for synthetic patterns and AI
training.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [68] [Activation Manifold Projection: Liberating Task-Specific Behaviors from LLM Architectures](https://arxiv.org/abs/2510.17902)
*Al Kari*

Main category: cs.AI

TL;DR: CAST框架通过直接映射不同LLM架构的激活流，实现LoRA适配器的零样本跨架构迁移，解决了模型架构锁定问题。


<details>
  <summary>Details</summary>
Motivation: 现有基于权重空间对齐的迁移方法脆弱且间接，无法有效解决不同LLM架构间的行为迁移问题。

Method: 学习轻量级双向投影头，将目标模型的激活流映射到源模型的潜在空间，应用冻结的LoRA核，再投影回目标模型。

Result: 在Llama-2和Mistral等异构模型间迁移时，CAST转换的适配器能达到重新训练LoRA 85-95%的性能。

Conclusion: CAST建立了模型互操作性的新标准，实现了真正的零样本LoRA适配器跨架构迁移。

Abstract: The proliferation of Large Language Model (LLM) architectures presents a
fundamental challenge: valuable, task-specific behaviors learned through
fine-tuning methods like Low-Rank Adaptation (LoRA) are effectively trapped
within their source model's architecture, herein referred to architectural
lock-in. Existing transfer methods attempt to bridge this gap by aligning the
static weight spaces of models, a brittle and indirect approach that relies on
tenuous correlations between parameter geometries. This paper introduces a
fundamentally different and more direct paradigm: the Cartridge Activation
Space Transfer (CAST), a novel framework that liberates LoRA-encoded behaviors
by learning a direct, nonlinear mapping between the activation manifolds, the
geometric structures formed by the model's internal neuron activations, of two
distinct LLM architectures. CAST treats a pre-trained LoRA as a frozen
"behavioral kernel." It learns a set of lightweight, bidirectional projection
heads that translate the target model's activation stream into the source
model's latent space, apply the frozen kernel, and project the result back.
This process, trained on a general text corpus without any task-specific data,
effectively decouples the learned skill from the source architecture. We
demonstrate that CAST enables true "zero-shot" translation of any standard LoRA
adapter. Our experiments, including transfers between heterogeneous model
families like Llama-2 and Mistral, show that CAST-translated adapters achieve
85-95\% of the performance of a LoRA fully retrained on the target model,
quantitatively outperforming current weight-space transfer techniques and
establishing a new state-of-the-art in model interoperability.

</details>


### [69] [Beyond More Context: Retrieval Diversity Boosts Multi-Turn Intent Understanding](https://arxiv.org/abs/2510.17940)
*Zhiming Lin*

Main category: cs.AI

TL;DR: 提出了一种多样性感知检索框架，在固定token预算下通过选择多样化的上下文示例来提升LLM的意图理解能力，在MultiWOZ 2.4和SGD数据集上显著提高了联合目标准确性。


<details>
  <summary>Details</summary>
Motivation: 解决任务导向聊天机器人在实际部署中面临的token预算限制和噪声上下文问题，现有检索流程过于关注相关性而忽视了集合级别的多样性和混淆因素。

Method: 开发了多样性感知检索框架，选择上下文示例以平衡意图覆盖和语言多样性，并将此选择与标准LLM解码器集成；评估采用预算匹配提示和随机化位置，并对示例数量、多样性强度和骨干模型大小进行敏感性分析。

Result: 在MultiWOZ 2.4和SGD数据集上，在相同token预算下实现了联合目标准确性的显著提升，超越了强大的LLM/DST基线，在K=4到7的范围内保持一致的改进，且延迟适中。

Conclusion: 研究分离并验证了检索中内容多样性的影响，为构建准确、预算受限的多轮意图系统提供了一个简单、可部署的选择原则。

Abstract: Multi turn intent understanding is central to task oriented chatbots, yet
real deployments face tight token budgets and noisy contexts, and most
retrieval pipelines emphasize relevance while overlooking set level diversity
and confounds such as more context or exemplar order. We ask whether retrieval
diversity, rather than longer prompts, systematically improves LLM intent
understanding under fixed budgets. We present a diversity aware retrieval
framework that selects in context exemplars to balance intent coverage and
linguistic variety, and integrates this selection with standard LLM decoders;
the evaluation enforces budget matched prompts and randomized positions, and
includes sensitivity analyses over exemplar count, diversity strength, and
backbone size. On MultiWOZ 2.4 and SGD, the approach achieves strong gains in
Joint Goal Accuracy under equal token budgets, surpassing strong LLM/DST
baselines, with consistent improvements across K from 4 to 7 and moderate
latency. Overall, the study isolates and validates the impact of content
diversity in retrieval and offers a simple, deployable selection principle for
building accurate, budget constrained multi turn intent systems.

</details>


### [70] [FABRIC: Framework for Agent-Based Realistic Intelligence Creation](https://arxiv.org/abs/2510.17995)
*Abhigya Verma,Seganrasan Subramanian,Nandhakumar Kandasamy,Naman Gupta*

Main category: cs.AI

TL;DR: 提出了一个仅使用LLM合成智能体数据的统一框架，无需人工监督，能够生成包含任务规范、工具定义、策略伪代码、自然语言交互和执行轨迹的完整交互记录。


<details>
  <summary>Details</summary>
Motivation: 收集智能体数据需要将用户意图与工具规范、参数调用的执行轨迹相结合，但人工标注成本高、耗时长且难以扩展。

Method: 通过模块化流水线生成符合严格语法和语义约束的交互记录，支持单任务、多任务和多轮交互，并集成约束生成格式、JSON模式验证和基于判断的过滤机制。

Result: 框架能够生成机器可解析且输入、输出和工具调用之间忠实对齐的高质量合成数据，支持构建反映完整工具使用能力的数据集。

Conclusion: 该框架为手动收集提供了可复现的、仅使用LLM的替代方案，从而推进能够进行稳健工具使用的智能体LLM的发展。

Abstract: Large language models (LLMs) are increasingly deployed as agents, expected to
decompose goals, invoke tools, and verify results in dynamic environments.
Realizing these capabilities requires access to agentic data-structured
interaction records that couple user intents with tool specifications,
argument-grounded calls, and verifiable execution traces. However, collecting
such data from human annotators is costly, time-consuming, and difficult to
scale. We present a unified framework for synthesizing agentic data using only
LLMs, without any human-in-the-loop supervision. This framework decomposes
generation into modular pipelines that produce complete interaction records
spanning task specifications, tool definitions, policy pseudocode, natural
language exchanges, and execution traces. Records conform to strict syntactic
and semantic constraints, ensuring machine-parseability and faithful alignment
across inputs, outputs, and tool calls. Beyond single tasks, there is support
for both multi-task and multi-turn agent interactions, enabling the
construction of datasets that reflect the full spectrum of tool-use
competencies. To ensure quality and consistency, the framework integrates
constrained generation formats, JSON-schema validation, and judge-based
filtering. This paper formalizes the schema for agentic records, details the
prompt design principles that guide generation, and introduces scalable
pipelines for high-quality synthetic data. By providing a reproducible,
LLM-only alternative to manual collection, hence advancing the development of
agentic LLMs capable of robust tool use.

</details>


### [71] [OPTAGENT: Optimizing Multi-Agent LLM Interactions Through Verbal Reinforcement Learning for Enhanced Reasoning](https://arxiv.org/abs/2510.18032)
*Zhenyu Bi,Meng Lu,Yang Li,Swastik Roy,Weijie Guan,Morteza Ziyadi,Xuan Wang*

Main category: cs.AI

TL;DR: 提出了一种多智能体口头强化学习算法，通过动态构建和优化协作结构来提升多智能体推理能力，在数学推理、创意写作等任务上表现优异


<details>
  <summary>Details</summary>
Motivation: 现有多智能体系统要么预定义结构，要么依赖多数投票或圆桌辩论，这会压制正确但非主导的智能体贡献。现有图网络方法仅优化智能体性能，忽视了交互质量

Method: 提出多智能体口头强化学习算法，定义动作空间和反馈机制，评估辩论过程中的沟通鲁棒性和连贯性，最终通过多数投票达成决策

Result: 在数学推理、创意写作、科学推理和数值排序等任务上，显著优于单智能体提示方法和最先进的多智能体框架

Conclusion: 有效的智能体沟通对多智能体推理至关重要，辩论质量在其中发挥重要作用

Abstract: Large Language Models (LLMs) have shown remarkable reasoning capabilities in
mathematical and scientific tasks. To enhance complex reasoning, multi-agent
systems have been proposed to harness the collective intelligence of LLM
agents. However, existing collaboration structures are either predefined or
rely on majority voting or round-table debates, which can suppress correct but
less dominant agent contributions. Recent approaches model multi-agent systems
as graph networks but optimize purely for agent performance, neglecting the
quality of interactions. We hypothesize that effective agent communication is
crucial for multi-agent reasoning and that debating quality plays a significant
role. To address this, we propose $\ours$, a multi-agent verbal reinforcement
learning algorithm that dynamically constructs and refines multi-agent
collaboration structures. Our method defines action spaces and a feedback
mechanism that evaluates communication robustness and coherence throughout the
debate. The final decision is achieved through a majority vote over all the
agents. We assess $\ours$ on various reasoning tasks, including mathematical
reasoning, creative writing, scientific reasoning, and numerical sorting.
Results demonstrate that our approach significantly outperforms single-agent
prompting methods and state-of-the-art multi-agent frameworks on diverse tasks.

</details>


### [72] [Subject-Event Ontology Without Global Time: Foundations and Execution Semantics](https://arxiv.org/abs/2510.18040)
*Alexander Boldachev*

Main category: cs.AI

TL;DR: 提出了一种基于主体-事件的本体论形式化方法，用于建模复杂动态系统，不依赖全局时间，通过事件固定、因果排序、可执行本体等核心原则实现分布式系统的确定性计算。


<details>
  <summary>Details</summary>
Motivation: 传统系统依赖全局时间戳来排序事件，这在分布式环境中存在问题。本文旨在建立不依赖全局时间的事件排序机制，支持多主体视角下的冲突事实处理。

Method: 基于九个公理（A1-A9）的形式化框架，包括事件作为固定行为、因果排序、可执行本体、模型作为认知过滤器等核心原则，通过声明式数据流机制确保确定性。

Result: 开发了boldsea系统作为工作流引擎，在BSL语言中实现了理论构造，证明了该形式化方法适用于分布式系统、微服务架构、DLT平台和多视角场景。

Conclusion: 该主体-事件本体论形式化为复杂动态系统提供了可靠的理论基础，特别适用于需要处理多主体冲突视角的分布式环境，通过因果依赖而非时间戳实现确定性计算。

Abstract: A formalization of a subject-event ontology is proposed for modeling complex
dynamic systems without reliance on global time. Key principles: (1) event as
an act of fixation - a subject discerns and fixes changes according to models
(conceptual templates) available to them; (2) causal order via happens-before -
the order of events is defined by explicit dependencies, not timestamps; (3)
making the ontology executable via a declarative dataflow mechanism, ensuring
determinism; (4) models as epistemic filters - a subject can only fix what
falls under its known concepts and properties; (5) presumption of truth - the
declarative content of an event is available for computation from the moment of
fixation, without external verification. The formalization includes nine axioms
(A1-A9), ensuring the correctness of executable ontologies: monotonicity of
history (I1), acyclicity of causality (I2), traceability (I3). Special
attention is given to the model-based approach (A9): event validation via
schemas, actor authorization, automatic construction of causal chains (W3)
without global time. Practical applicability is demonstrated on the boldsea
system - a workflow engine for executable ontologies, where the theoretical
constructs are implemented in BSL (Boldsea Semantic Language). The
formalization is applicable to distributed systems, microservice architectures,
DLT platforms, and multiperspectivity scenarios (conflicting facts from
different subjects).

</details>


### [73] [CompactPrompt: A Unified Pipeline for Prompt Data Compression in LLM Workflows](https://arxiv.org/abs/2510.18043)
*Joong Ho Choi,Jiayang Zhao,Jeel Shah,Ritvika Sonawane,Vedant Singh,Avani Appalla,Will Flanagan,Filipe Condessa*

Main category: cs.AI

TL;DR: CompactPrompt是一个端到端提示压缩管道，通过合并硬提示压缩和轻量级文件级数据压缩，在保持输出质量的同时将LLM代理的总令牌使用量和推理成本降低高达60%。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在代理工作流中运行时会产生高昂成本，需要处理冗长的提示和丰富的数据流，因此需要有效的压缩方法来降低运行成本。

Method: 首先使用自信息评分和基于依赖的短语分组从提示中修剪低信息令牌，同时对附加文档中的重复文本模式应用n-gram缩写，对数值列应用统一量化，生成紧凑但语义忠实的表示。

Result: 在TAT-QA和FinQA等基准数据集上，CompactPrompt将总令牌使用量和推理成本降低高达60%，同时保持输出质量（Claude-3.5-Sonnet和GPT-4.1-Mini的准确率下降不到5%）。

Conclusion: CompactPrompt有助于可视化实时压缩决策并量化成本性能权衡，为更精简的生成式AI管道奠定了基础。

Abstract: Large Language Models (LLMs) deliver powerful reasoning and generation
capabilities but incur substantial run-time costs when operating in agentic
workflows that chain together lengthy prompts and process rich data streams. We
introduce CompactPrompt, an end-to-end pipeline that merges hard prompt
compression with lightweight file-level data compression. CompactPrompt first
prunes low-information tokens from prompts using self-information scoring and
dependency-based phrase grouping. In parallel, it applies n-gram abbreviation
to recurrent textual patterns in attached documents and uniform quantization to
numerical columns, yielding compact yet semantically faithful representations.
Integrated into standard LLM agents, CompactPrompt reduces total token usage
and inference cost by up to 60% on benchmark dataset like TAT-QA and FinQA,
while preserving output quality (Results in less than 5% accuracy drop for
Claude-3.5-Sonnet, and GPT-4.1-Mini) CompactPrompt helps visualize real-time
compression decisions and quantify cost-performance trade-offs, laying the
groundwork for leaner generative AI pipelines.

</details>


### [74] [Planned Diffusion](https://arxiv.org/abs/2510.18087)
*Daniel Israel,Tian Jin,Ellie Cheng,Guy Van den Broeck,Aditya Grover,Suvinay Subramanian,Michael Carbin*

Main category: cs.AI

TL;DR: Planned diffusion是一种结合自回归和扩散模型的混合方法，通过两阶段生成（先制定计划再并行生成）来优化文本生成的速度-质量权衡。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型推理中生成速度与输出质量之间的权衡问题，自回归模型质量高但速度慢，扩散模型可并行但需要多次迭代。

Method: 两阶段方法：1) 自回归制定计划，将输出分解为独立的小片段；2) 使用扩散模型并行生成这些片段。

Result: 在AlpacaEval基准测试中，实现了速度-质量的Pareto最优权衡，相比自回归生成获得1.27x-1.81x加速，仅损失0.87%-5.4%的胜率。

Conclusion: Planned diffusion扩展了速度-质量的Pareto边界，为更快、高质量的文本生成提供了实用路径，其规划机制简洁可靠且具有灵活的质量-延迟权衡控制。

Abstract: A central challenge in large language model inference is the trade-off
between generation speed and output quality. Autoregressive models produce
high-quality text but generate tokens sequentially. Diffusion models can
generate tokens in parallel but often need many iterations to match the same
quality. We propose planned diffusion, a hybrid method that combines the
strengths of both paradigms. Planned diffusion works in two stages: first, the
model creates a short autoregressive plan that breaks the output into smaller,
independent spans. Second, the model generates these spans simultaneously using
diffusion. This approach expands the speed-quality Pareto frontier and provides
a practical path to faster, high-quality text generation. On AlpacaEval, a
suite of 805 instruction-following prompts, planned diffusion achieves
Pareto-optimal trade-off between quality and latency, achieving 1.27x to 1.81x
speedup over autoregressive generation with only 0.87\% to 5.4\% drop in win
rate, respectively. Our sensitivity analysis shows that the planning mechanism
of planned diffusion is minimal and reliable, and simple runtime knobs exist to
provide flexible control of the quality-latency trade-off.

</details>


### [75] [SMaRT: Select, Mix, and ReinvenT -- A Strategy Fusion Framework for LLM-Driven Reasoning and Planning](https://arxiv.org/abs/2510.18095)
*Nikhil Verma,Manasa Bharadwaj,Wonjun Jang,Harmanpreet Singh,Yixiao Wang,Homa Fashandi,Chul Lee*

Main category: cs.AI

TL;DR: SMaRT框架通过融合多种推理策略来提升LLM性能，克服单一策略的局限性，在推理、规划和决策任务中表现优异


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖单一策略提示，无法充分利用不同推理方法的协同效应，需要能够融合多种策略的框架来最大化性能和鲁棒性

Method: 提出SMaRT（选择、混合和重塑）框架，使用LLM作为智能集成器而非评估器，无缝整合多样推理策略

Result: 在推理、规划和顺序决策基准测试中，SMaRT在解决方案质量、约束遵循和性能指标方面持续优于最先进基线

Conclusion: 这项工作通过开创跨策略校准新范式，重新定义了LLM驱动的决策制定，为推理系统解锁了更优结果并推进了自优化方法的边界

Abstract: Large Language Models (LLMs) have redefined complex task automation with
exceptional generalization capabilities. Despite these advancements,
state-of-the-art methods rely on single-strategy prompting, missing the synergy
of diverse reasoning approaches. No single strategy excels universally,
highlighting the need for frameworks that fuse strategies to maximize
performance and ensure robustness. We introduce the Select, Mix, and ReinvenT
(SMaRT) framework, an innovative strategy fusion approach designed to overcome
this constraint by creating balanced and efficient solutions through the
seamless integration of diverse reasoning strategies. Unlike existing methods,
which employ LLMs merely as evaluators, SMaRT uses them as intelligent
integrators, unlocking the "best of all worlds" across tasks. Extensive
empirical evaluations across benchmarks in reasoning, planning, and sequential
decision-making highlight the robustness and adaptability of SMaRT. The
framework consistently outperforms state-of-the-art baselines in solution
quality, constraint adherence, and performance metrics. This work redefines
LLM-driven decision-making by pioneering a new paradigm in cross-strategy
calibration, unlocking superior outcomes for reasoning systems and advancing
the boundaries of self-refining methodologies.

</details>


### [76] [Measuring Reasoning in LLMs: a New Dialectical Angle](https://arxiv.org/abs/2510.18134)
*Soheil Abbasloo*

Main category: cs.AI

TL;DR: 论文提出了SIEV框架，基于辩证法理论评估语言模型的推理过程，而不仅仅是答案正确性，发现即使是最先进模型在传统基准上表现饱和时仍存在显著推理缺陷。


<details>
  <summary>Details</summary>
Motivation: 当前评估主要关注模型的正确答案，但无法揭示推理过程的质量。作者认为推理应该是动态的、辩证的过程，需要评估模型如何整合不同观点、解决冲突并形成更深层次见解。

Method: 基于辩证法理论（正题、反题、合题），开发了SIEV结构化评估框架，评估模型在推理过程中解决张力、整合观点和进行高阶综合的能力。

Result: SIEV评估揭示了最先进模型（如GPT-5-chat）在GSM基准上的推理缺陷，得分下降超过40分（满分100），表明传统基准已无法充分评估模型的真实推理能力。

Conclusion: 采用基于哲学理论的过程导向评估方法，能够对语言模型推理进行更深入、更严谨和更具区分度的评估，揭示传统评估方法无法发现的推理缺陷。

Abstract: What does it truly mean for a language model to "reason"? Most current
evaluations and benchmarks reward models' correct standalone answers--but
correctness alone reveals little about the process that produced them. In this
work, we explore a different perspective: reasoning is not a static chain of
steps, but a dynamic trajectory where ideas interact, clash, and evolve into
deeper insights. To capture this dynamic, we draw on a well-established
philosophical tradition: \textit{dialectics}, where reasoning unfolds through
thesis, antithesis, and synthesis. Building on this, we present SIEV, a
structured framework that evaluates reasoning of LLMs through dialectics.
Unlike conventional evaluations, SIEV assesses not only the conclusion a model
reaches, but how it gets there: its ability to resolve tension, integrate
distinct ideas, and synthesize higher-order reasoning. This lens uncovers
significant reasoning gaps in state-of-the-art models even under saturated
benchmarks like GSM and MMLU. For instance, GPT-5-chat, a recent model, loses
over 40 points (out of 100) when evaluated with SIEV on GSM. Our findings
highlight that adopting a process-oriented, philosophically grounded approach
enables a deeper, more rigorous, and more discriminative assessment of LLM
reasoning.

</details>


### [77] [Learning from Generalization Patterns: An Evaluation-Driven Approach to Enhanced Data Augmentation for Fine-Tuning Small Language Models](https://arxiv.org/abs/2510.18143)
*Huan Song,Deeksha Razdan,Yiyue Qian,Arijit Ghosh Chowdhury,Parth Patwa,Aman Chadha,Shinan Zhang,Sharlina Keshava,Hannah Marlowe*

Main category: cs.AI

TL;DR: PaDA-Agent是一个评估驱动的数据增强方法，通过发现验证数据中的失败模式来制定针对性策略，显著提升小语言模型在领域特定任务上的性能。


<details>
  <summary>Details</summary>
Motivation: 小语言模型在部署成本和延迟方面具有优势，但在复杂领域特定任务上的准确性往往落后于大模型。监督微调需要大量手动数据准备和迭代优化工作。

Method: 提出PaDA-Agent方法，通过评估发现验证数据中的失败模式，制定针对性的数据增强策略，直接减少泛化差距，而不是仅关注模型训练错误。

Result: 实验结果显示，相比最先进的基于LLM的数据增强方法，PaDA-Agent在Llama 3.2 1B Instruct模型微调上取得了显著改进。

Conclusion: PaDA-Agent通过评估驱动的模式发现和针对性数据增强，有效提升了小语言模型在领域特定任务上的性能，减少了手动数据准备的工作量。

Abstract: Small Language Models (SLMs) offer compelling advantages in deployment cost
and latency, but their accuracy often lags behind larger models, particularly
for complex domain-specific tasks. While supervised fine-tuning can help bridge
this performance gap, it requires substantial manual effort in data preparation
and iterative optimization. We present PaDA-Agent (Pattern-guided Data
Augmentation Agent), an evaluation-driven approach that streamlines the data
augmentation process for SLMs through coordinated operations. Unlike
state-of-the-art approaches that focus on model training errors only and
generating error-correcting samples, PaDA-Agent discovers failure patterns from
the validation data via evaluations and drafts targeted data augmentation
strategies aiming to directly reduce the generalization gap. Our experimental
results demonstrate significant improvements over state-of-the-art LLM-based
data augmentation approaches for Llama 3.2 1B Instruct model fine-tuning.

</details>


### [78] [Annotating the Chain-of-Thought: A Behavior-Labeled Dataset for AI Safety](https://arxiv.org/abs/2510.18154)
*Antonio-Gabriel Chacón Menke,Phan Xuan Tan,Eiji Kamioka*

Main category: cs.AI

TL;DR: 提出了一个句子级标注的数据集，用于在LLM推理过程中基于激活的安全行为监控，填补了现有数据集仅整体标注推理的空白。


<details>
  <summary>Details</summary>
Motivation: 现有基于文本推理步骤的安全监控方法可能遗漏细微的有害模式，且可能被隐藏不安全推理的模型规避。需要更精确的激活级监控技术。

Method: 构建包含推理序列的句子级标注数据集，标注安全行为（如表达安全担忧、推测用户意图），并基于此提取用于检测和影响这些行为的引导向量。

Result: 展示了数据集的实用性，提取的表征能够检测和引导模型激活中的安全行为，证明了激活级技术在改进推理安全监督方面的潜力。

Conclusion: 激活级监控技术有望改进AI推理安全监督，该数据集为精确识别推理链中特定安全行为的发生时机提供了关键支持。

Abstract: Recent work has highlighted the importance of monitoring chain-of-thought
reasoning for AI safety; however, current approaches that analyze textual
reasoning steps can miss subtle harmful patterns and may be circumvented by
models that hide unsafe reasoning. We present a sentence-level labeled dataset
that enables activation-based monitoring of safety behaviors during LLM
reasoning. Our dataset contains reasoning sequences with sentence-level
annotations of safety behaviors such as expression of safety concerns or
speculation on user intent, which we use to extract steering vectors for
detecting and influencing these behaviors within model activations. The dataset
fills a key gap in safety research: while existing datasets label reasoning
holistically, effective application of steering vectors for safety monitoring
could be improved by identifying precisely when specific behaviors occur within
reasoning chains. We demonstrate the dataset's utility by extracting
representations that both detect and steer safety behaviors in model
activations, showcasing the potential of activation-level techniques for
improving safety oversight on reasoning.
  Content Warning: This paper discusses AI safety in the context of harmful
prompts and may contain references to potentially harmful content.

</details>


### [79] [LLM-Based Multi-Agent System for Simulating and Analyzing Marketing and Consumer Behavior](https://arxiv.org/abs/2510.18155)
*Man-Lin Chu,Lucian Terhorst,Kadin Reed,Tom Ni,Weiwei Chen,Rongyu Lin*

Main category: cs.AI

TL;DR: 提出一个基于大语言模型的多智能体仿真框架，用于模拟消费者决策和社会动态，为营销策略提供低风险预实施测试工具。


<details>
  <summary>Details</summary>
Motivation: 传统的基于规则的多智能体模型难以捕捉人类行为的复杂性，需要更真实的消费者决策模拟来降低营销活动风险。

Method: 利用大语言模型在沙盒环境中构建生成式智能体，能够交互、表达内部推理、形成习惯并进行购买决策，无需预定义规则。

Result: 在价格折扣营销场景中，系统提供了可操作的策略测试结果，并揭示了传统方法无法捕捉的新兴社会模式。

Conclusion: 该方法为营销人员提供了可扩展、低风险的预实施测试工具，减少对耗时的事后评估的依赖，降低营销活动失败风险。

Abstract: Simulating consumer decision-making is vital for designing and evaluating
marketing strategies before costly real-world deployment. However, post-event
analyses and rule-based agent-based models (ABMs) struggle to capture the
complexity of human behavior and social interaction. We introduce an
LLM-powered multi-agent simulation framework that models consumer decisions and
social dynamics. Building on recent advances in large language model simulation
in a sandbox environment, our framework enables generative agents to interact,
express internal reasoning, form habits, and make purchasing decisions without
predefined rules. In a price-discount marketing scenario, the system delivers
actionable strategy-testing outcomes and reveals emergent social patterns
beyond the reach of conventional methods. This approach offers marketers a
scalable, low-risk tool for pre-implementation testing, reducing reliance on
time-intensive post-event evaluations and lowering the risk of underperforming
campaigns.

</details>


### [80] [Saber: An Efficient Sampling with Adaptive Acceleration and Backtracking Enhanced Remasking for Diffusion Language Model](https://arxiv.org/abs/2510.18165)
*Yihong Dong,Zhaoyu Ma,Xue Jiang,Zhiyuan Fan,Jiaru Qian,Yongmin Li,Jianha Xiao,Zhi Jin,Rongyu Cao,Binhua Li,Fei Huang,Yongbin Li,Ge Li*

Main category: cs.AI

TL;DR: 提出Saber算法，一种无需训练的采样方法，用于扩散语言模型的代码生成，在保持质量的同时显著提升推理速度


<details>
  <summary>Details</summary>
Motivation: 扩散语言模型在代码生成任务中面临推理速度与输出质量的关键权衡问题，减少采样步骤会导致性能急剧下降

Method: Saber算法：基于自适应加速和回溯增强重掩码的采样方法，利用代码生成过程中上下文建立后可自适应加速，并需要回溯机制来逆转生成标记

Result: 在多个主流代码生成基准测试中，Saber比主流DLM采样方法平均提升1.9%的Pass@1准确率，同时实现平均251.4%的推理加速

Conclusion: 通过利用扩散语言模型的固有优势，该工作显著缩小了与自回归模型在代码生成方面的性能差距

Abstract: Diffusion language models (DLMs) are emerging as a powerful and promising
alternative to the dominant autoregressive paradigm, offering inherent
advantages in parallel generation and bidirectional context modeling. However,
the performance of DLMs on code generation tasks, which have stronger
structural constraints, is significantly hampered by the critical trade-off
between inference speed and output quality. We observed that accelerating the
code generation process by reducing the number of sampling steps usually leads
to a catastrophic collapse in performance. In this paper, we introduce
efficient Sampling with Adaptive acceleration and Backtracking Enhanced
Remasking (i.e., Saber), a novel training-free sampling algorithm for DLMs to
achieve better inference speed and output quality in code generation.
Specifically, Saber is motivated by two key insights in the DLM generation
process: 1) it can be adaptively accelerated as more of the code context is
established; 2) it requires a backtracking mechanism to reverse the generated
tokens. Extensive experiments on multiple mainstream code generation benchmarks
show that Saber boosts Pass@1 accuracy by an average improvement of 1.9% over
mainstream DLM sampling methods, meanwhile achieving an average 251.4%
inference speedup. By leveraging the inherent advantages of DLMs, our work
significantly narrows the performance gap with autoregressive models in code
generation.

</details>


### [81] [AgentChangeBench: A Multi-Dimensional Evaluation Framework for Goal-Shift Robustness in Conversational AI](https://arxiv.org/abs/2510.18170)
*Manik Rana,Calissa Man,Anotida Expected Msiiwa,Jeffrey Paine,Kevin Zhu,Sunishchal Dev,Vasu Sharma,Ahan M R*

Main category: cs.AI

TL;DR: 提出了AgentChangeBench基准，专门评估工具增强语言模型代理在对话中处理目标变化的能力，包含四个互补指标来衡量有效性、可靠性、效率和适应延迟。


<details>
  <summary>Details</summary>
Motivation: 现实世界多轮交互中目标变化是常见特征，但现有代理基准主要评估静态目标或一次性工具使用，缺乏对动态目标适应能力的测试。

Method: 构建包含2,835个任务序列和五个用户角色的基准框架，在三个企业领域设置现实的目标转换点，通过四个指标（任务成功率、工具使用效率、工具调用冗余率、目标转换恢复时间）进行系统评估。

Result: 评估显示前沿模型在动态目标适应方面存在显著差异：GPT-4o在航空预订任务中恢复率达到92.2%，而Gemini降至48.6%；零售任务参数有效性接近完美但冗余率超过80%，揭示重大效率问题。

Conclusion: 高原始准确率并不等同于动态目标下的鲁棒性，明确测量恢复时间和冗余率对于评估代理在现实企业环境中的韧性至关重要，AgentChangeBench为诊断和改进代理韧性提供了可复现的测试平台。

Abstract: Goal changes are a defining feature of real world multi-turn interactions,
yet current agent benchmarks primarily evaluate static objectives or one-shot
tool use. We introduce AgentChangeBench, a benchmark explicitly designed to
measure how tool augmented language model agents adapt to mid dialogue goal
shifts across three enterprise domains. Our framework formalizes evaluation
through four complementary metrics: Task Success Rate (TSR) for effectiveness,
Tool Use Efficiency (TUE) for reliability, Tool Call Redundancy Rate (TCRR) for
wasted effort, and Goal-Shift Recovery Time (GSRT) for adaptation latency.
AgentChangeBench comprises 2,835 task sequences and five user personas, each
designed to trigger realistic shift points in ongoing workflows. Using this
setup, we evaluate several frontier models and uncover sharp contrasts obscured
by traditional $\text{pass}@k$ scores: for example, GPT-4o reaches $92.2\%$
recovery on airline booking shifts while Gemini collapses to $48.6\%$, and
retail tasks show near perfect parameter validity yet redundancy rates above
$80\%$, revealing major inefficiencies. These findings demonstrate that high
raw accuracy does not imply robustness under dynamic goals, and that explicit
measurement of recovery time and redundancy is essential. AgentChangeBench
establishes a reproducible testbed for diagnosing and improving agent
resilience in realistic enterprise settings.

</details>


### [82] [Local Coherence or Global Validity? Investigating RLVR Traces in Math Domains](https://arxiv.org/abs/2510.18176)
*Soumya Rani Samineni,Durgesh Kalwar,Vardaan Gangal,Siddhant Bhambri,Subbarao Kambhampati*

Main category: cs.AI

TL;DR: 本文研究了基于可验证奖励的强化学习(RLVR)对大语言模型推理过程的影响，发现RL后训练虽然提高了推理轨迹的局部连贯性，但并不保证最终答案的正确性。


<details>
  <summary>Details</summary>
Motivation: 现有RLVR方法通常对所有token一视同仁，主要基于最终答案正确性评估性能，却声称RL后训练改善了推理轨迹。这促使我们研究RL后训练对未被直接激励的中间token的影响。

Method: 使用GRPO算法和Qwen-2.5-0.5B模型在GSM8K数据集上进行实验，引入基于一阶逻辑的轨迹连贯性度量来捕捉推理步骤的一致性。

Result: RL后训练总体上提高了轨迹连贯性，在基础模型失败但RL模型成功的问题上改进最显著。但RL增强了局部连贯性而不一定产生有效或正确的解决方案。

Conclusion: 声称RL改善推理的说法需要谨慎审视，因为改进的轨迹连贯性可能不会转化为完全有效的数学证明，局部连贯性的提高并不保证最终答案的正确性。

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR)-based post-training of
Large Language Models (LLMs) has been shown to improve accuracy on reasoning
tasks and continues to attract significant attention. Existing RLVR methods,
however, typically treat all tokens uniformly without accounting for
token-level advantages. These methods primarily evaluate performance based on
final answer correctness or Pass@K accuracy, and yet make claims about RL
post-training leading to improved reasoning traces. This motivates our
investigation into the effect of RL post-training on intermediate tokens which
are not directly incentivized. To study this, we design an experimental setup
using the GRPO algorithm with Qwen-2.5-0.5B model on the GSM8K dataset. We
introduce trace coherence, a First-Order Logic (FOL)-based measure to capture
the consistency of reasoning steps by identifying errors in the traces. We
distinguish between trace validity and trace coherence, noting that the former
implies logical soundness while the latter measures local coherence via lack of
errors. Our results show that RL post-training overall improves trace coherence
with the most significant gains on problems where the base model fails but the
RL model succeeds. Surprisingly, RL enhances local coherence without
necessarily producing valid or correct solutions. This highlights a crucial
distinction: improved local coherence in reasoning steps does not guarantee
final answer correctness. We argue that claims of improved reasoning via RL
must be examined with care, as these may be based on improved trace coherence,
which may not translate into fully valid mathematical proofs.

</details>


### [83] [FST.ai 2.0: An Explainable AI Ecosystem for Fair, Fast, and Inclusive Decision-Making in Olympic and Paralympic Taekwondo](https://arxiv.org/abs/2510.18193)
*Keivan Shariatmadar,Ahmad Osman,Ramin Ray,Usman Dildar,Kisam Kim*

Main category: cs.AI

TL;DR: FST.ai 2.0是一个可解释的AI生态系统，旨在通过姿态识别、不确定性建模和可视化解释支持跆拳道比赛和训练中的实时决策，显著减少决策审查时间并提高裁判信任度。


<details>
  <summary>Details</summary>
Motivation: 解决奥林匹克和残奥会格斗运动中公平、透明和可解释决策的挑战，支持裁判、教练和运动员在跆拳道比赛和训练中的实时决策。

Method: 集成基于姿态的动作识别（使用图卷积网络）、通过信任集进行认知不确定性建模、可视化解释叠加层，以及支持人机协作的交互式仪表板。

Result: 在比赛数据上的实验验证显示决策审查时间减少85%，AI辅助决策的裁判信任度达到93%。

Conclusion: 该框架建立了一个透明且可扩展的管道，用于可信赖、数据驱动的裁判和运动员评估，代表了体育领域向公平、负责任和以人为本的AI迈出的一步。

Abstract: Fair, transparent, and explainable decision-making remains a critical
challenge in Olympic and Paralympic combat sports. This paper presents
\emph{FST.ai 2.0}, an explainable AI ecosystem designed to support referees,
coaches, and athletes in real time during Taekwondo competitions and training.
The system integrates {pose-based action recognition} using graph convolutional
networks (GCNs), {epistemic uncertainty modeling} through credal sets, and
{explainability overlays} for visual decision support. A set of {interactive
dashboards} enables human--AI collaboration in referee evaluation, athlete
performance analysis, and Para-Taekwondo classification. Beyond automated
scoring, FST.ai~2.0 incorporates modules for referee training, fairness
monitoring, and policy-level analytics within the World Taekwondo ecosystem.
Experimental validation on competition data demonstrates an {85\% reduction in
decision review time} and {93\% referee trust} in AI-assisted decisions. The
framework thus establishes a transparent and extensible pipeline for
trustworthy, data-driven officiating and athlete assessment. By bridging
real-time perception, explainable inference, and governance-aware design,
FST.ai~2.0 represents a step toward equitable, accountable, and human-aligned
AI in sports.

</details>


### [84] [A Definition of AGI](https://arxiv.org/abs/2510.18212)
*Dan Hendrycks,Dawn Song,Christian Szegedy,Honglak Lee,Yarin Gal,Erik Brynjolfsson,Sharon Li,Andy Zou,Lionel Levine,Bo Han,Jie Fu,Ziwei Liu,Jinwoo Shin,Kimin Lee,Mantas Mazeika,Long Phan,George Ingebretsen,Adam Khoja,Cihang Xie,Olawale Salaudeen,Matthias Hein,Kevin Zhao,Alexander Pan,David Duvenaud,Bo Li,Steve Omohundro,Gabriel Alfour,Max Tegmark,Kevin McGrew,Gary Marcus,Jaan Tallinn,Eric Schmidt,Yoshua Bengio*

Main category: cs.AI

TL;DR: 本文提出了一个可量化的AGI评估框架，基于Cattell-Horn-Carroll人类认知理论，将通用智能分解为10个核心认知领域，并将现有AI系统与受过良好教育的成年人进行对比评估。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏对AGI的具体定义，模糊了专业AI与人类水平认知之间的差距，需要建立一个可量化的评估框架来明确这一差距。

Method: 基于Cattell-Horn-Carroll人类认知理论，将通用智能分解为10个核心认知领域（包括推理、记忆、感知等），并采用成熟的人类心理测量工具来评估AI系统。

Result: 应用该框架发现当代AI模型具有高度"锯齿状"的认知特征：在知识密集型领域表现熟练，但在基础认知机制（特别是长期记忆存储）方面存在严重缺陷。GPT-4得分为27%，GPT-5为58%。

Conclusion: 该框架量化了AI向AGI的快速进展和剩余差距，为AGI发展提供了具体的衡量标准。

Abstract: The lack of a concrete definition for Artificial General Intelligence (AGI)
obscures the gap between today's specialized AI and human-level cognition. This
paper introduces a quantifiable framework to address this, defining AGI as
matching the cognitive versatility and proficiency of a well-educated adult. To
operationalize this, we ground our methodology in Cattell-Horn-Carroll theory,
the most empirically validated model of human cognition. The framework dissects
general intelligence into ten core cognitive domains-including reasoning,
memory, and perception-and adapts established human psychometric batteries to
evaluate AI systems. Application of this framework reveals a highly "jagged"
cognitive profile in contemporary models. While proficient in
knowledge-intensive domains, current AI systems have critical deficits in
foundational cognitive machinery, particularly long-term memory storage. The
resulting AGI scores (e.g., GPT-4 at 27%, GPT-5 at 58%) concretely quantify
both rapid progress and the substantial gap remaining before AGI.

</details>


### [85] [ssToken: Self-modulated and Semantic-aware Token Selection for LLM Fine-tuning](https://arxiv.org/abs/2510.18250)
*Xiaohan Qin,Xiaoxing Wang,Ning Liao,Cancheng Zhang,Xiangdong Zhang,Mingquan Feng,Jingzhi Wang,Junchi Yan*

Main category: cs.AI

TL;DR: 提出了ssToken方法，通过自调制和语义感知的token选择来提升大语言模型的监督微调数据质量，无需额外参考模型，在多个模型家族和规模上优于全数据微调和现有token级选择方法。


<details>
  <summary>Details</summary>
Motivation: 现有token级数据选择方法存在两个关键限制：(1)需要训练或访问额外参考模型，(2)仅依赖损失信息进行token选择，无法很好保留语义重要但损失指标不偏好的token。

Method: ssToken方法包含两个核心组件：自调制选择（利用历史模型计算token损失差异作为自适应信号）和语义感知选择（基于注意力的token重要性估计，提供与损失选择正交的补充语义信息）。

Result: 实验表明，自调制选择和语义感知选择单独使用都能优于全数据微调，而两者结合的ssToken实现了协同增益，进一步超越了先前的token级选择方法，在保持训练效率的同时提升了性能。

Conclusion: ssToken通过自调制和语义感知的token选择有效解决了现有方法的局限性，为提升大语言模型监督微调的数据质量提供了更有效的方法。

Abstract: Data quality plays a critical role in enhancing supervised fine-tuning (SFT)
for large language models (LLMs), and token-level data selection has emerged as
a promising direction for its fine-grained nature. Despite their strong
empirical performance, existing token-level selection methods share two key
limitations: (1) requiring training or accessing an additional reference model,
and (2) relying solely on loss information for token selection, which cannot
well preserve semantically important tokens that are not favored by loss-based
metrics. To address these challenges, we propose ssToken, a Self-modulated and
Semantic-aware Token Selection approach. ssToken leverages readily accessible
history models to compute the per-token loss difference with the current model,
which serves as a self-modulated signal that enables the model to adaptively
select tokens along its optimization trajectory, rather than relying on excess
loss from an offline-trained reference model as in prior works. We further
introduce a semantic-aware, attention-based token importance estimation metric,
orthogonal to loss-based selection and providing complementary semantic
information for more effective filtering. Extensive experiments across
different model families and scales demonstrate that both self-modulated
selection and semantic-aware selection alone outperform full-data fine-tuning,
while their integration--ssToken--achieves synergistic gains and further
surpasses prior token-level selection methods, delivering performance
improvements while maintaining training efficiency.

</details>


### [86] [Illusions of reflection: open-ended task reveals systematic failures in Large Language Models' reflective reasoning](https://arxiv.org/abs/2510.18254)
*Sion Weatherhead,Flora Salim,Aaron Belbasis*

Main category: cs.AI

TL;DR: 本文测试了8个前沿大语言模型在开放式但规则约束的任务上的自我反思能力，发现模型反思效果有限，经常重复同样的约束违反，表明当前LLM的反思缺乏人类式的主动目标驱动监控机制。


<details>
  <summary>Details</summary>
Motivation: 研究大语言模型的反思能力是否与人类反思推理功能等价，特别是在开放式但规则约束的任务中测试其自我修正能力。

Method: 在需要生成有效科学测试项并自我修订的任务上测试8个前沿模型，分析第一轮表现和反思后的改进效果。

Result: 第一轮表现很差（平均约1个有效项），反思后只有轻微改善（也约1个）。模型经常重复同样的约束违反，改进主要来自偶然生成有效项而非真正的错误检测和修复。

Conclusion: 当前LLM的反思缺乏人类式的主动目标驱动监控机制，可靠性能需要外部结构来强制执行约束。

Abstract: Humans do not just find mistakes after the fact -- we often catch them
mid-stream because 'reflection' is tied to the goal and its constraints.
Today's large language models produce reasoning tokens and 'reflective' text,
but is it functionally equivalent with human reflective reasoning? Prior work
on closed-ended tasks -- with clear, external 'correctness' signals -- can make
'reflection' look effective while masking limits in self-correction. We
therefore test eight frontier models on a simple, real-world task that is
open-ended yet rule-constrained, with auditable success criteria: to produce
valid scientific test items, then revise after considering their own critique.
First-pass performance is poor (often zero valid items out of 4 required; mean
$\approx$ 1), and reflection yields only modest gains (also $\approx$ 1).
Crucially, the second attempt frequently repeats the same violation of
constraint, indicating 'corrective gains' arise largely from chance production
of a valid item rather than error detection and principled,
constraint-sensitive repair. Performance before and after reflection
deteriorates as open-endedness increases, and models marketed for 'reasoning'
show no advantage. Our results suggest that current LLM 'reflection' lacks
functional evidence of the active, goal-driven monitoring that helps humans
respect constraints even on a first pass. Until such mechanisms are
instantiated in the model itself, reliable performance requires external
structure that enforces constraints.

</details>


### [87] [Genesis: Evolving Attack Strategies for LLM Web Agent Red-Teaming](https://arxiv.org/abs/2510.18314)
*Zheng Zhang,Jiarui He,Yuchen Cai,Deheng Ye,Peilin Zhao,Ruili Feng,Hao Wang*

Main category: cs.AI

TL;DR: 提出Genesis框架，通过遗传算法和动态策略库来攻击LLM网络代理，在多种网络任务中表现优于现有基线方法。


<details>
  <summary>Details</summary>
Motivation: 随着LLM代理自动化复杂网络任务，虽然提高了生产力但也带来了新的安全风险。现有红队方法主要依赖手动策略或静态模型，难以捕捉网络代理的行为模式，无法在不同环境中泛化。

Method: 提出Genesis框架，包含三个模块：攻击者（使用遗传算法和混合策略表示生成对抗注入）、评分器（评估目标网络代理的响应提供反馈）、策略师（从交互日志中动态发现有效策略并编译到持续增长的战略库中）。

Result: 在多种网络任务上的广泛实验表明，该框架能够发现新颖策略，并持续优于现有的攻击基线方法。

Conclusion: Genesis框架通过动态策略发现和进化机制，有效提升了网络代理攻击的效果，为LLM代理安全研究提供了新思路。

Abstract: As large language model (LLM) agents increasingly automate complex web tasks,
they boost productivity while simultaneously introducing new security risks.
However, relevant studies on web agent attacks remain limited. Existing
red-teaming approaches mainly rely on manually crafted attack strategies or
static models trained offline. Such methods fail to capture the underlying
behavioral patterns of web agents, making it difficult to generalize across
diverse environments. In web agent attacks, success requires the continuous
discovery and evolution of attack strategies. To this end, we propose Genesis,
a novel agentic framework composed of three modules: Attacker, Scorer, and
Strategist. The Attacker generates adversarial injections by integrating the
genetic algorithm with a hybrid strategy representation. The Scorer evaluates
the target web agent's responses to provide feedback. The Strategist
dynamically uncovers effective strategies from interaction logs and compiles
them into a continuously growing strategy library, which is then re-deployed to
enhance the Attacker's effectiveness. Extensive experiments across various web
tasks show that our framework discovers novel strategies and consistently
outperforms existing attack baselines.

</details>


### [88] [Earth AI: Unlocking Geospatial Insights with Foundation Models and Cross-Modal Reasoning](https://arxiv.org/abs/2510.18318)
*Aaron Bell,Amit Aides,Amr Helmy,Arbaaz Muslim,Aviad Barzilai,Aviv Slobodkin,Bolous Jaber,David Schottlander,George Leifman,Joydeep Paul,Mimi Sun,Nadav Sherman,Natalie Williams,Per Bjornsson,Roy Lee,Ruth Alcantara,Thomas Turnbull,Tomer Shekel,Vered Silverman,Yotam Gigi,Adam Boulanger,Alex Ottenwess,Ali Ahmadalipour,Anna Carter,Charles Elliott,David Andre,Elad Aharoni,Gia Jung,Hassler Thurston,Jacob Bien,Jamie McPike,Juliet Rothenberg,Kartik Hegde,Kel Markert,Kim Philipp Jablonski,Luc Houriez,Monica Bharel,Phing VanLee,Reuven Sayag,Sebastian Pilarski,Shelley Cazares,Shlomi Pasternak,Siduo Jiang,Stone Jiang,Thomas Colthurst,Yang Chen,Yehonathan Refael,Yochai Blau,Yuval Carny,Yael Maguire,Avinatan Hassidim,James Manyika,Tim Thelin,Genady Beryozkin,Gautam Prasad,Luke Barrington,Yossi Matias,Niv Efron,Shravya Shetty*

Main category: cs.AI

TL;DR: Earth AI是一个地理空间AI模型家族，通过多领域基础模型和Gemini驱动的智能推理引擎，解决地理数据量大、多样性和复杂性带来的分析挑战，实现更深入的地球洞察。


<details>
  <summary>Details</summary>
Motivation: 地理空间数据具有巨大潜力，但其海量、多样性、不同分辨率、时间尺度和稀疏性给全面分析和解释带来了重大挑战。

Method: 基于三个关键领域（行星尺度影像、人口、环境）的基础模型，结合Gemini驱动的智能推理引擎，开发能够联合推理多个基础模型、大型地理空间数据源和工具的智能代理。

Result: 在严格基准测试中展示了基础模型的能力，验证了多模型协同提供互补价值并解锁卓越预测能力。在真实世界危机场景的新基准测试中，智能代理能够提供关键及时的洞察。

Conclusion: Earth AI通过基础模型和智能推理代理，有效弥合原始地理空间数据与可操作理解之间的差距，为地球洞察提供强大工具。

Abstract: Geospatial data offers immense potential for understanding our planet.
However, the sheer volume and diversity of this data along with its varied
resolutions, timescales, and sparsity pose significant challenges for thorough
analysis and interpretation. This paper introduces Earth AI, a family of
geospatial AI models and agentic reasoning that enables significant advances in
our ability to unlock novel and profound insights into our planet. This
approach is built upon foundation models across three key domains--Planet-scale
Imagery, Population, and Environment--and an intelligent Gemini-powered
reasoning engine. We present rigorous benchmarks showcasing the power and novel
capabilities of our foundation models and validate that when used together,
they provide complementary value for geospatial inference and their synergies
unlock superior predictive capabilities. To handle complex, multi-step queries,
we developed a Gemini-powered agent that jointly reasons over our multiple
foundation models along with large geospatial data sources and tools. On a new
benchmark of real-world crisis scenarios, our agent demonstrates the ability to
deliver critical and timely insights, effectively bridging the gap between raw
geospatial data and actionable understanding.

</details>


### [89] [ShortcutBreaker: Low-Rank Noisy Bottleneck with Global Perturbation Attention for Multi-Class Unsupervised Anomaly Detection](https://arxiv.org/abs/2510.18342)
*Peng Tang,Xiaoxiao Yan,Xiaobin Hu,Yuning Cui,Donghao Luo,Jiangning Zhang,Pengcheng Xu,Jinlong Peng,Qingdong He,Feiyue Huang,Song Xue,Tobias Lasser*

Main category: cs.AI

TL;DR: 提出ShortcutBreaker框架解决多类无监督异常检测中的身份捷径问题，通过低秩噪声瓶颈和全局扰动注意力机制防止模型直接复制输入，在多个数据集上取得优异性能。


<details>
  <summary>Details</summary>
Motivation: 多类无监督异常检测需要统一模型检测多个类别的异常，但现有Transformer架构存在身份捷径问题，即直接复制输入到输出，导致正常与异常样本的重构误差差异缩小，难以区分。

Method: 提出ShortcutBreaker框架：1）基于矩阵秩不等式设计低秩噪声瓶颈，将高维特征投影到低秩潜在空间防止平凡身份复制；2）利用ViT的全局建模能力，引入全局扰动注意力防止解码器中的信息捷径。

Result: 在四个基准数据集（MVTec-AD、ViSA、Real-IAD工业数据集和Universal Medical医疗数据集）上分别达到99.8%、98.9%、90.6%和87.8%的图像级AUROC，显著优于现有方法。

Conclusion: ShortcutBreaker通过有效解决身份捷径问题，在多类无监督异常检测任务中实现了优异的性能，证明了所提框架在不同场景下的有效性和通用性。

Abstract: Multi-class unsupervised anomaly detection (MUAD) has garnered growing
research interest, as it seeks to develop a unified model for anomaly detection
across multiple classes, i.e., eliminating the need to train separate models
for distinct objects and thereby saving substantial computational resources.
Under the MUAD setting, while advanced Transformer-based architectures have
brought significant performance improvements, identity shortcuts persist: they
directly copy inputs to outputs, narrowing the gap in reconstruction errors
between normal and abnormal cases, and thereby making the two harder to
distinguish. Therefore, we propose ShortcutBreaker, a novel unified
feature-reconstruction framework for MUAD tasks, featuring two key innovations
to address the issue of shortcuts. First, drawing on matrix rank inequality, we
design a low-rank noisy bottleneck (LRNB) to project highdimensional features
into a low-rank latent space, and theoretically demonstrate its capacity to
prevent trivial identity reproduction. Second, leveraging ViTs global modeling
capability instead of merely focusing on local features, we incorporate a
global perturbation attention to prevent information shortcuts in the decoders.
Extensive experiments are performed on four widely used anomaly detection
benchmarks, including three industrial datasets (MVTec-AD, ViSA, and Real-IAD)
and one medical dataset (Universal Medical). The proposed method achieves a
remarkable image-level AUROC of 99.8%, 98.9%, 90.6%, and 87.8% on these four
datasets, respectively, consistently outperforming previous MUAD methods across
different scenarios.

</details>


### [90] [Memory-Augmented State Machine Prompting: A Novel LLM Agent Framework for Real-Time Strategy Games](https://arxiv.org/abs/2510.18395)
*Runnan Qi,Yanan Ni,Lumin Jiang,Zongyuan Li,Kuihua Huang,Xian Guo*

Main category: cs.AI

TL;DR: 提出了Memory-Augmented State Machine Prompting (MASMP)框架，通过状态机提示和记忆机制的结合，解决LLM在实时策略游戏中的幻觉和决策碎片化问题，在星际争霸II中达到60%胜率。


<details>
  <summary>Details</summary>
Motivation: 解决现有方法中LLM代理在实时策略游戏中存在的幻觉问题和决策碎片化问题，统一结构化动作与长期战术连贯性。

Method: 结合状态机提示和记忆机制：(1) 自然语言驱动的状态机架构，通过提示引导LLM模拟有限状态机和行为树；(2) 轻量级记忆模块，在决策周期间保持战略变量。

Result: 在星际争霸II实验中，MASMP对最难内置AI(Lv7)达到60%胜率，远超基线方法(0%)。案例研究显示该方法保持了LLM的语义理解能力，同时通过严格的状态-动作映射解决了"知行差距"问题。

Conclusion: 该工作为在复杂决策中结合神经和符号AI建立了新范式，实现了可解释性和类似FSM的可靠性。

Abstract: This paper proposes Memory-Augmented State Machine Prompting (MASMP), a novel
framework for LLM agents in real-time strategy games. Addressing key challenges
like hallucinations and fragmented decision-making in existing approaches,
MASMP integrates state machine prompting with memory mechanisms to unify
structured actions with long-term tactical coherence. The framework features:
(1) a natural language-driven state machine architecture that guides LLMs to
emulate finite state machines and behavior trees through prompts, and (2) a
lightweight memory module preserving strategic variables (e.g., tactics,
priority units) across decision cycles. Experiments in StarCraft II demonstrate
MASMP's 60% win rate against the hardest built-in AI (Lv7), vastly
outperforming baselines (0%). Case studies reveal the method retains LLMs'
semantic comprehension while resolving the "Knowing-Doing Gap" through strict
state-action mapping, achieving both interpretability and FSM-like reliability.
This work establishes a new paradigm for combining neural and symbolic AI in
complex decision-making.

</details>


### [91] [Heterogeneous Adversarial Play in Interactive Environments](https://arxiv.org/abs/2510.18407)
*Manjie Xu,Xinyi Yang,Jiayu Zhan,Wei Liang,Chi Zhang,Yixin Zhu*

Main category: cs.AI

TL;DR: 提出了Heterogeneous Adversarial Play (HAP)框架，通过对抗性自动课程学习实现教师-学生交互，解决传统自博弈在非对称学习场景中的局限性。


<details>
  <summary>Details</summary>
Motivation: 传统自博弈框架依赖智能体对称性，在零和竞争环境中有效，但在具有固有不对称性的开放式学习场景中表现不足。人类教学系统展示了非对称教学框架的优势，教师根据学习者发展轨迹定制挑战。

Method: HAP将教师-学生交互形式化为极小极大优化，任务生成教师和问题解决学生通过对抗动态共同进化。建立双向反馈系统，教师根据实时学习者表现重新校准任务复杂度。

Result: 在多任务学习领域的实验验证表明，该框架达到最先进基线的性能水平，同时生成的课程提高了人工智能体和人类受试者的学习效率。

Conclusion: HAP框架成功实现了非对称、自适应教学机制在人工智能系统中的操作化，能够自主合成适当课程而无需预定义任务层次结构。

Abstract: Self-play constitutes a fundamental paradigm for autonomous skill
acquisition, whereby agents iteratively enhance their capabilities through
self-directed environmental exploration. Conventional self-play frameworks
exploit agent symmetry within zero-sum competitive settings, yet this approach
proves inadequate for open-ended learning scenarios characterized by inherent
asymmetry. Human pedagogical systems exemplify asymmetric instructional
frameworks wherein educators systematically construct challenges calibrated to
individual learners' developmental trajectories. The principal challenge
resides in operationalizing these asymmetric, adaptive pedagogical mechanisms
within artificial systems capable of autonomously synthesizing appropriate
curricula without predetermined task hierarchies. Here we present Heterogeneous
Adversarial Play (HAP), an adversarial Automatic Curriculum Learning framework
that formalizes teacher-student interactions as a minimax optimization wherein
task-generating instructor and problem-solving learner co-evolve through
adversarial dynamics. In contrast to prevailing ACL methodologies that employ
static curricula or unidirectional task selection mechanisms, HAP establishes a
bidirectional feedback system wherein instructors continuously recalibrate task
complexity in response to real-time learner performance metrics. Experimental
validation across multi-task learning domains demonstrates that our framework
achieves performance parity with SOTA baselines while generating curricula that
enhance learning efficacy in both artificial agents and human subjects.

</details>


### [92] [Deep Learning-Based Control Optimization for Glass Bottle Forming](https://arxiv.org/abs/2510.18412)
*Mattia Pujatti,Andrea Di Luca,Nicola Peghini,Federico Monegaglia,Marco Cristoforetti*

Main category: cs.AI

TL;DR: 提出基于深度学习的控制算法，用于优化玻璃瓶制造中的成型过程，通过神经网络预测参数变化影响并确定最佳机器设置。


<details>
  <summary>Details</summary>
Motivation: 在玻璃瓶制造中，精确控制成型机器对确保质量和减少缺陷至关重要，需要开发能够实时优化生产过程的智能控制方法。

Method: 使用来自实际生产工厂的运行数据，构建神经网络模型预测参数变化的影响，并通过专门设计的反演机制确定实现所需玻璃料滴特性的最佳机器设置。

Result: 在多个生产线的历史数据集上的实验结果表明，该方法取得了有希望的结果，显示出提高过程稳定性、减少浪费和改进产品一致性的潜力。

Conclusion: 深度学习在玻璃制造过程控制中具有显著潜力，能够有效优化生产参数并改善产品质量。

Abstract: In glass bottle manufacturing, precise control of forming machines is
critical for ensuring quality and minimizing defects. This study presents a
deep learning-based control algorithm designed to optimize the forming process
in real production environments. Using real operational data from active
manufacturing plants, our neural network predicts the effects of parameter
changes based on the current production setup. Through a specifically designed
inversion mechanism, the algorithm identifies the optimal machine settings
required to achieve the desired glass gob characteristics. Experimental results
on historical datasets from multiple production lines show that the proposed
method yields promising outcomes, suggesting potential for enhanced process
stability, reduced waste, and improved product consistency. These results
highlight the potential of deep learning to process control in glass
manufacturing.

</details>


### [93] [Med-VRAgent: A Framework for Medical Visual Reasoning-Enhanced Agents](https://arxiv.org/abs/2510.18424)
*Guangfu Guo,Xiaoqian Lu,Yue Feng*

Main category: cs.AI

TL;DR: 提出Med-VRAgent框架，结合视觉引导、自我奖励和蒙特卡洛树搜索，提升视觉语言模型在医学推理中的表现，并通过PPO微调进一步优化性能。


<details>
  <summary>Details</summary>
Motivation: 视觉语言模型在医学推理中表现良好，但存在幻觉、模糊描述、逻辑不一致和定位能力差等问题，需要改进。

Method: 基于视觉引导和自我奖励范式，结合蒙特卡洛树搜索构建Med-VRAgent代理框架，并使用PPO目标对VLM进行微调。

Result: 在多个医学VQA基准测试中，该方法优于现有方法。

Conclusion: Med-VRAgent通过视觉引导和树搜索结合，有效提升了医学视觉推理能力，并通过反馈微调进一步优化性能。

Abstract: Visual Language Models (VLMs) achieve promising results in medical reasoning
but struggle with hallucinations, vague descriptions, inconsistent logic and
poor localization. To address this, we propose a agent framework named Medical
Visual Reasoning Agent (\textbf{Med-VRAgent}). The approach is based on Visual
Guidance and Self-Reward paradigms and Monte Carlo Tree Search (MCTS). By
combining the Visual Guidance with tree search, Med-VRAgent improves the
medical visual reasoning capabilities of VLMs. We use the trajectories
collected by Med-VRAgent as feedback to further improve the performance by
fine-tuning the VLMs with the proximal policy optimization (PPO) objective.
Experiments on multiple medical VQA benchmarks demonstrate that our method
outperforms existing approaches.

</details>


### [94] [Automated urban waterlogging assessment and early warning through a mixture of foundation models](https://arxiv.org/abs/2510.18425)
*Chenxu Zhang,Fuxiang Huang,Lei Zhang*

Main category: cs.AI

TL;DR: UWAssess是一个基于基础模型的框架，能够自动识别监控图像中的积水区域并生成结构化评估报告，解决了城市内涝监测中数据稀缺和及时性不足的问题。


<details>
  <summary>Details</summary>
Motivation: 随着气候变化加剧，城市内涝对全球公共安全和基础设施构成日益严重的威胁，而现有监测方法主要依赖人工报告，无法提供及时全面的评估。

Method: 设计了半监督微调策略和思维链提示策略，释放基础模型在数据稀缺下游任务中的潜力，通过多基础模型协作框架实现积水区域识别和报告生成。

Result: 在具有挑战性的视觉基准测试中显示出感知性能的显著提升，GPT评估证实UWAssess能够生成准确描述积水范围、深度、风险和影响的可靠文本报告。

Conclusion: 该框架实现了从感知到生成的内涝监测转变，为智能可扩展系统奠定了基础，支持城市管理、灾害响应和气候韧性。

Abstract: With climate change intensifying, urban waterlogging poses an increasingly
severe threat to global public safety and infrastructure. However, existing
monitoring approaches rely heavily on manual reporting and fail to provide
timely and comprehensive assessments. In this study, we present Urban
Waterlogging Assessment (UWAssess), a foundation model-driven framework that
automatically identifies waterlogged areas in surveillance images and generates
structured assessment reports. To address the scarcity of labeled data, we
design a semi-supervised fine-tuning strategy and a chain-of-thought (CoT)
prompting strategy to unleash the potential of the foundation model for
data-scarce downstream tasks. Evaluations on challenging visual benchmarks
demonstrate substantial improvements in perception performance. GPT-based
evaluations confirm the ability of UWAssess to generate reliable textual
reports that accurately describe waterlogging extent, depth, risk and impact.
This dual capability enables a shift of waterlogging monitoring from perception
to generation, while the collaborative framework of multiple foundation models
lays the groundwork for intelligent and scalable systems, supporting urban
management, disaster response and climate resilience.

</details>


### [95] [AlphaOPT: Formulating Optimization Programs with Self-Improving LLM Experience Library](https://arxiv.org/abs/2510.18428)
*Minwei Kong,Ao Qu,Xiaotong Guo,Wenbin Ouyang,Chonghe Jiang,Han Zheng,Yining Ma,Dingyi Zhuang,Yuhan Tang,Junyi Li,Hai Wang,Cathy Wu,Jinhua Zhao*

Main category: cs.AI

TL;DR: AlphaOPT是一个自改进的经验库系统，使LLM能够从有限演示和求解器反馈中学习优化建模，无需标注推理轨迹或参数更新，通过两阶段循环持续提升性能。


<details>
  <summary>Details</summary>
Motivation: 优化建模在各行业决策中至关重要但难以自动化，现有LLM方法要么依赖脆弱的提示工程，要么需要成本高昂的重训练且泛化能力有限。

Method: 采用两阶段持续循环：库学习阶段从失败尝试中提取求解器验证的结构化洞察；库进化阶段诊断检索偏差并优化存储洞察的适用条件，实现跨任务知识迁移。

Result: 实验显示AlphaOPT随数据增加稳定改进（从100到300训练项，性能从65%提升到72%），在仅使用答案训练时，在OOD OptiBench数据集上比最强基线高出7.7%。

Conclusion: AlphaOPT能够高效从有限演示中学习，通过更新库而非模型权重实现持续扩展，使知识显式化且可解释，支持人工检查和干预。

Abstract: Optimization modeling enables critical decisions across industries but
remains difficult to automate: informal language must be mapped to precise
mathematical formulations and executable solver code. Prior LLM approaches
either rely on brittle prompting or costly retraining with limited
generalization. We present AlphaOPT, a self-improving experience library that
enables an LLM to learn from limited demonstrations (even answers alone,
without gold-standard programs) and solver feedback - without annotated
reasoning traces or parameter updates. AlphaOPT operates in a continual
two-phase cycle: (i) a Library Learning phase that reflects on failed attempts,
extracting solver-verified, structured insights as {taxonomy, condition,
explanation, example}; and (ii) a Library Evolution phase that diagnoses
retrieval misalignments and refines the applicability conditions of stored
insights, improving transfer across tasks. This design (1) learns efficiently
from limited demonstrations without curated rationales, (2) expands continually
without costly retraining by updating the library rather than model weights,
and (3) makes knowledge explicit and interpretable for human inspection and
intervention. Experiments show that AlphaOPT steadily improves with more data
(65% to 72% from 100 to 300 training items) and surpasses the strongest
baseline by 7.7% on the out-of-distribution OptiBench dataset when trained only
on answers. Code and data are available at:
https://github.com/Minw913/AlphaOPT.

</details>


### [96] [PlanU: Large Language Model Decision Making through Planning under Uncertainty](https://arxiv.org/abs/2510.18442)
*Ziwei Deng,Mian Deng,Chenjing Liang,Zeming Gao,Chennan Ma,Chenxing Lin,Haipeng Zhang,Songzhu Mei,Cheng Wang,Siqi Shen*

Main category: cs.AI

TL;DR: PlanU是一种基于LLM的规划方法，通过蒙特卡洛树搜索(MCTS)中的分位数分布来捕捉不确定性，解决LLM在不确定性环境中的决策问题。


<details>
  <summary>Details</summary>
Motivation: LLM在不确定性环境中的决策能力较弱，现有方法主要处理LLM不确定性而忽略环境不确定性，导致在随机状态转换环境中表现不佳。

Method: 将MCTS中每个节点的回报建模为分位数分布，使用带有好奇心上限置信度(UCC)评分来平衡探索与利用。

Result: 通过大量实验证明PlanU在不确定性LLM决策任务中的有效性。

Conclusion: PlanU成功解决了LLM在不确定性环境中的决策挑战，通过分位数分布和UCC评分显著提升了决策性能。

Abstract: Large Language Models (LLMs) are increasingly being explored across a range
of decision-making tasks. However, LLMs sometimes struggle with decision-making
tasks under uncertainty that are relatively easy for humans, such as planning
actions in stochastic environments. The adoption of LLMs for decision-making is
impeded by uncertainty challenges, such as LLM uncertainty and environmental
uncertainty. LLM uncertainty arises from the stochastic sampling process
inherent to LLMs. Most LLM-based Decision-Making (LDM) approaches address LLM
uncertainty through multiple reasoning chains or search trees. However, these
approaches overlook environmental uncertainty, which leads to poor performance
in environments with stochastic state transitions. Some recent LDM approaches
deal with uncertainty by forecasting the probability of unknown variables.
However, they are not designed for multi-step decision-making tasks that
require interaction with the environment. To address uncertainty in LLM
decision-making, we introduce PlanU, an LLM-based planning method that captures
uncertainty within Monte Carlo Tree Search (MCTS). PlanU models the return of
each node in the MCTS as a quantile distribution, which uses a set of quantiles
to represent the return distribution. To balance exploration and exploitation
during tree search, PlanU introduces an Upper Confidence Bounds with Curiosity
(UCC) score which estimates the uncertainty of MCTS nodes. Through extensive
experiments, we demonstrate the effectiveness of PlanU in LLM-based
decision-making tasks under uncertainty.

</details>


### [97] [CircuitSeer: Mining High-Quality Data by Probing Mathematical Reasoning Circuits in LLMs](https://arxiv.org/abs/2510.18470)
*Shaobo Wang,Yongliang Miao,Yuancheng Liu,and Qianli Ma,Ning Liao,Linfeng Zhang*

Main category: cs.AI

TL;DR: CircuitSeer：一种基于注意力头电路的数据选择方法，通过在10%数据上微调就能超越全数据集训练效果


<details>
  <summary>Details</summary>
Motivation: 现有数据选择方法依赖昂贵的外部模型或不透明的启发式方法，而本文转向模型内部机制，发现复杂推理任务会激活稀疏的专用注意力头电路

Method: 提出CircuitSeer方法，通过测量数据对关键推理电路的影响来量化推理复杂度，基于模型内部注意力头激活模式进行数据选择

Result: 在4个模型和9个数据集上的实验显示，CircuitSeer仅使用10%数据微调Qwen2.5-Math-7B，就能在平均Pass@1上比全数据集训练提升1.4个百分点

Conclusion: 基于模型内部推理电路的数据选择方法CircuitSeer在效率和效果上都优于现有方法，为高效数据利用提供了新思路

Abstract: Large language models (LLMs) have demonstrated impressive reasoning
capabilities, but scaling their performance often relies on massive reasoning
datasets that are computationally expensive to train on. Existing data
selection methods aim to curate smaller, high-quality subsets but often rely on
costly external models or opaque heuristics. In this work, we shift the focus
from external heuristics to the model's internal mechanisms. We find that
complex reasoning tasks consistently activate a sparse, specialized subset of
attention heads, forming core reasoning circuits. Building on this insight, we
propose CircuitSeer, a novel data selection method that quantifies the
reasoning complexity of data by measuring its influence on these crucial
circuits. Extensive experiments on 4 models and 9 datasets demonstrate
CircuitSeer's superiority. Notably, fine-tuning Qwen2.5-Math-7B on just 10% of
data selected by our method achieves a 1.4-point gain in average Pass@1 over
training on the full dataset, highlighting its efficiency and effectiveness.

</details>


### [98] [Probabilistic Modeling of Intentions in Socially Intelligent LLM Agents](https://arxiv.org/abs/2510.18476)
*Feifan Xia,Yuyang Fang,Defang Li,Yantong Xie,Weikang Li,Yang Li,Deguo Xia,Jizhou Huang*

Main category: cs.AI

TL;DR: 提出了一个用于多轮社交对话中LLM代理的概率意图建模框架，通过维护对伙伴潜在意图的信念分布来实现自适应对话策略


<details>
  <summary>Details</summary>
Motivation: 为LLM代理在多轮社交对话中提供更好的意图理解和自适应能力，解决不确定性下的对话策略问题

Method: 维护伙伴潜在意图的信念分布，从上下文先验初始化，并在每次发言后通过似然估计动态更新，为策略提供额外的上下文基础

Result: 在SOTOPIA环境中，相比Qwen2.5-7B基线，总体得分在SOTOPIA-All上提升9.0%，在SOTOPIA-Hard上提升4.1%，甚至略微超过直接观察伙伴意图的oracle代理

Conclusion: 概率意图建模可以为开发社交智能LLM代理做出贡献

Abstract: We present a probabilistic intent modeling framework for large language model
(LLM) agents in multi-turn social dialogue. The framework maintains a belief
distribution over a partner's latent intentions, initialized from contextual
priors and dynamically updated through likelihood estimation after each
utterance. The evolving distribution provides additional contextual grounding
for the policy, enabling adaptive dialogue strategies under uncertainty.
Preliminary experiments in the SOTOPIA environment show consistent
improvements: the proposed framework increases the Overall score by 9.0% on
SOTOPIA-All and 4.1% on SOTOPIA-Hard compared with the Qwen2.5-7B baseline, and
slightly surpasses an oracle agent that directly observes partner intentions.
These early results suggest that probabilistic intent modeling can contribute
to the development of socially intelligent LLM agents.

</details>


### [99] [LAFA: Agentic LLM-Driven Federated Analytics over Decentralized Data Sources](https://arxiv.org/abs/2510.18477)
*Haichao Ji,Zibo Wang,Yifei Zhu,Meng han,Dan Wang,Zhu Han*

Main category: cs.AI

TL;DR: LAFA是首个将基于LLM代理的数据分析与联邦分析(FA)相结合的系统，通过分层多代理架构将自然语言查询转换为优化的可执行FA工作流，在保护隐私的同时支持自然语言输入。


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM代理的分析框架假设集中式数据访问，缺乏隐私保护；而联邦分析(FA)支持隐私保护计算但不支持自然语言输入。需要结合两者的优势。

Method: 提出分层多代理架构：粗粒度规划器分解复杂查询为子查询，细粒度规划器将子查询映射为FA操作的有向无环图，优化代理重写合并多个DAG以消除冗余操作。

Result: 实验表明LAFA始终优于基线提示策略，实现了更高的执行计划成功率，并显著减少了资源密集型FA操作。

Conclusion: 这项工作为在FA环境中支持自然语言输入的隐私保护、LLM驱动的分析建立了实用基础。

Abstract: Large Language Models (LLMs) have shown great promise in automating data
analytics tasks by interpreting natural language queries and generating
multi-operation execution plans. However, existing LLM-agent-based analytics
frameworks operate under the assumption of centralized data access, offering
little to no privacy protection. In contrast, federated analytics (FA) enables
privacy-preserving computation across distributed data sources, but lacks
support for natural language input and requires structured, machine-readable
queries. In this work, we present LAFA, the first system that integrates
LLM-agent-based data analytics with FA. LAFA introduces a hierarchical
multi-agent architecture that accepts natural language queries and transforms
them into optimized, executable FA workflows. A coarse-grained planner first
decomposes complex queries into sub-queries, while a fine-grained planner maps
each subquery into a Directed Acyclic Graph of FA operations using prior
structural knowledge. To improve execution efficiency, an optimizer agent
rewrites and merges multiple DAGs, eliminating redundant operations and
minimizing computational and communicational overhead. Our experiments
demonstrate that LAFA consistently outperforms baseline prompting strategies by
achieving higher execution plan success rates and reducing resource-intensive
FA operations by a substantial margin. This work establishes a practical
foundation for privacy-preserving, LLM-driven analytics that supports natural
language input in the FA setting.

</details>


### [100] [StarBench: A Turn-Based RPG Benchmark for Agentic Multimodal Decision-Making and Information Seeking](https://arxiv.org/abs/2510.18483)
*Haoran Zhang,Chenhao Zhu,Sicong Guo,Hanzhe Guo,Haiming Li,Donglin Yu*

Main category: cs.AI

TL;DR: StarBench是一个基于《崩坏：星穹铁道》的回合制RPG基准测试，评估视觉语言模型在像素到动作的多模态决策和主动性信息寻求方面的能力。


<details>
  <summary>Details</summary>
Motivation: 当前视觉语言模型在真实客户端环境中，将原始截图映射到时间一致的低级动作，并在需要时寻求指导的人类化游戏能力仍是一个开放挑战。

Method: 设计了两种评估机制：直接控制（仅接收截图并输出低级操作）和工具辅助控制（使用检测器和OCR提供文本化观察）。还包括询问或行动诊断，衡量代理选择请求指导的时机和效果。

Result: 在直接控制机制下，感知到控制的保真度存在显著差距，而明智的信息寻求与改进的成功率相关。

Conclusion: StarBench为真实客户端游戏中的主动性信息寻求和多模态决策提供了可复现的基准标准。

Abstract: Human players do more than press buttons: they ground what they see on screen
into precise keyboard-mouse actions and, when stuck, they seek information
before trying again. We ask whether current vision-language models (VLMs) can
do the same. Despite encouraging results under simplified control or tool
scaffolds, human-like play in a real client - mapping raw screenshots to
temporally coherent low-level actions while deciding when to ask for guidance -
remains an open challenge. We introduce StarBench, a turn-based RPG benchmark
derived from Honkai: Star Rail that targets these two human-like competencies:
multimodal decision-making from pixels to actions and agentic information
seeking. StarBench standardizes evaluation across eight combat tasks and two
regimes with shared tasks and metrics: (i) direct control, where agents receive
only screenshots and must emit low-level primitives (click and keypress) with
no semantic hints; and (ii) tool-assisted control, where higher-level intents
can be mapped to primitives by detectors and OCR outputs provide optional
textualized observations to ease UI grounding. To mirror human practice,
StarBench also includes an ask-or-act diagnostic that measures whether and when
agents choose to request brief guidance before proceeding, and how that choice
affects subsequent performance. We report reference baselines for contemporary
VLMs and a human reference. Results expose sizable gaps in
perception-to-control fidelity in the direct regime, while showing that
judicious information seeking correlates with improved success, establishing
StarBench as a reproducible yardstick for agentic information seeking and
multimodal decision-making in real-client play.

</details>


### [101] [AndroidControl-Curated: Revealing the True Potential of GUI Agents through Benchmark Purification](https://arxiv.org/abs/2510.18488)
*Ho Fai Leung,Xiaoyan Xi,Fei Zuo*

Main category: cs.AI

TL;DR: 研究发现当前GUI代理性能评估存在基准测试问题，通过改进AndroidControl为AndroidControl-Curated基准，使SOTA模型在复杂任务上的成功率从60%提升至75%，并推出小参数模型Magma-R1-3B，性能媲美大模型。


<details>
  <summary>Details</summary>
Motivation: 解决当前设备端虚拟助手依赖刚性API的问题，以及GUI代理因基准测试缺陷而被低估性能的问题，推动更实用的设备端虚拟助手发展。

Method: 识别AndroidControl基准的模糊性和事实错误，通过严格净化流程改进为AndroidControl-Curated基准；使用2.4k精选样本对Magma-R1-3B模型进行后训练。

Result: 在改进后的基准上，SOTA模型在复杂任务上的成功率接近75%（提升15%）；Magma-R1-3B模型仅用3B参数，性能与235B参数的Qwen3-VL相当。

Conclusion: 设备端GUI代理的实际能力被低估，通过改进基准测试和小参数模型训练，证明了GUI代理已接近实际部署水平，为设备端虚拟助手发展提供了新方向。

Abstract: On-device virtual assistants like Siri and Google Assistant are increasingly
pivotal, yet their capabilities are hamstrung by a reliance on rigid,
developer-dependent APIs. GUI agents offer a powerful, API-independent
alternative, but their adoption is hindered by the perception of poor
performance, as even the best models (e.g. Qwen3-VL-235B) scores are capped at
around 60% on benchmarks like AndroidControl, far from viability for real-world
use. Our research reveals that issue lies not only with the models but with the
benchmarks themselves. We identified notable shortcomings in AndroidControl,
including ambiguities and factual errors, which systematically underrates agent
capabilities. To address this critical oversight, we enhanced AndroidControl
into AndroidControl-Curated, a refined version of the benchmark improved
through a rigorous purification pipeline. On this enhanced benchmark,
state-of-the-art models achieve success rates nearing 75% on complex tasks (15%
improvement), reflecting that on-device GUI agents are actually closer to
practical deployment than previously thought. We introduce our new SOTA model,
Magma-R1- 3B, post-trained on just 2.4k curated samples using 60 hours of an
H20 GPU (approximately $60). Despite being 200 times smaller in parameters,
this model delivers performance comparable to Qwen3- VL-235B. We release both
AndroidControl-Curated benchmark and Magma-R1 model to the research community,
encouraging adoption of this enhanced benchmark to better reflect model
capabilities and accelerate the development of robust, on-device virtual
assistants.

</details>


### [102] [Crucible: Quantifying the Potential of Control Algorithms through LLM Agents](https://arxiv.org/abs/2510.18491)
*Lianchen Jia,Chaoyang Li,Qian Houde,Tianchi Huang,Jiangchuan Liu,Lifeng Sun*

Main category: cs.AI

TL;DR: 提出了Crucible框架，使用LLM驱动的多级专家模拟来评估算法的调优潜力，并定义了量化指标来衡量不同算法的可调优空间。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注算法在理想或默认配置下的性能，而忽略了调优潜力这一关键维度，特别是在生产环境中需要领域专家针对特定场景进行参数和逻辑调优。

Method: 采用LLM驱动的多级专家模拟方法，通过模拟不同领域专家的调优过程来评估算法，并定义形式化指标来量化调优潜力。

Result: 在从经典控制任务到复杂计算机系统的广泛案例研究中验证了Crucible的有效性，并在真实部署中确认了其发现。实验结果表明Crucible能够系统量化不同算法的可调优空间。

Conclusion: Crucible为算法分析和设计提供了新的维度，最终能够带来性能提升，为算法调优潜力评估提供了系统化框架。

Abstract: Control algorithms in production environments typically require domain
experts to tune their parameters and logic for specific scenarios. However,
existing research predominantly focuses on algorithmic performance under ideal
or default configurations, overlooking the critical aspect of Tuning Potential.
To bridge this gap, we introduce Crucible, an agent that employs an LLM-driven,
multi-level expert simulation to turn algorithms and defines a formalized
metric to quantitatively evaluate their Tuning Potential. We demonstrate
Crucible's effectiveness across a wide spectrum of case studies, from classic
control tasks to complex computer systems, and validate its findings in a
real-world deployment. Our experimental results reveal that Crucible
systematically quantifies the tunable space across different algorithms.
Furthermore, Crucible provides a new dimension for algorithm analysis and
design, which ultimately leads to performance improvements. Our code is
available at https://github.com/thu-media/Crucible.

</details>


### [103] [Counterfactual Reasoning for Steerable Pluralistic Value Alignment of Large Language Models](https://arxiv.org/abs/2510.18526)
*Hanze Guo,Jing Yao,Xiao Zhou,Xiaoyuan Yi,Xing Xie*

Main category: cs.AI

TL;DR: 提出了COUPLE框架，通过结构因果模型和反事实推理来解决大语言模型与多元化人类价值观对齐的挑战，包括价值复杂性和价值可操控性问题。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型在服务不同文化、社区和人口群体的应用中的集成，需要超越平均原则（如HHH）与多元化人类价值观对齐。现有方法在处理细粒度价值目标时面临两个挑战：价值复杂性和价值可操控性。

Method: 提出COUPLE框架，引入结构因果模型来表征特征间的复杂相互依赖关系和优先级，以及高层价值维度与行为间的因果关系。应用反事实推理生成符合任意期望价值目标的输出。

Result: 在两个具有不同价值系统的数据集上评估COUPLE，结果表明COUPLE在多种类型价值目标上优于其他基线方法。

Conclusion: COUPLE通过显式因果建模提供了更好的可解释性，能够有效处理价值对齐中的复杂性和可操控性挑战，在多元化价值目标对齐方面表现出色。

Abstract: As large language models (LLMs) become increasingly integrated into
applications serving users across diverse cultures, communities and
demographics, it is critical to align LLMs with pluralistic human values beyond
average principles (e.g., HHH). In psychological and social value theories such
as Schwartz's Value Theory, pluralistic values are represented by multiple
value dimensions paired with various priorities. However, existing methods
encounter two challenges when aligning with such fine-grained value objectives:
1) they often treat multiple values as independent and equally important,
ignoring their interdependence and relative priorities (value complexity); 2)
they struggle to precisely control nuanced value priorities, especially those
underrepresented ones (value steerability). To handle these challenges, we
propose COUPLE, a COUnterfactual reasoning framework for PLuralistic valuE
alignment. It introduces a structural causal model (SCM) to feature complex
interdependency and prioritization among features, as well as the causal
relationship between high-level value dimensions and behaviors. Moreover, it
applies counterfactual reasoning to generate outputs aligned with any desired
value objectives. Benefitting from explicit causal modeling, COUPLE also
provides better interpretability. We evaluate COUPLE on two datasets with
different value systems and demonstrate that COUPLE advances other baselines
across diverse types of value objectives.

</details>


### [104] [Physics-guided Emulators Reveal Resilience and Fragility under Operational Latencies and Outages](https://arxiv.org/abs/2510.18535)
*Sarth Dubey,Subimal Ghosh,Udit Bhatia*

Main category: cs.AI

TL;DR: 开发了一个操作就绪的GloFAS洪水预报模拟器，结合长短期记忆网络和松弛水平衡约束，在数据延迟、缺失或不一致情况下保持稳定性能。


<details>
  <summary>Details</summary>
Motivation: 现有水文预报模型大多在理想数据条件下评估，强调准确性而非操作韧性。需要开发能在实际运行中应对数据质量问题（延迟、缺失、不一致）的可靠预报系统。

Method: 使用五种架构覆盖信息可用性的连续谱：从完整历史和预报强迫数据到具有数据延迟和中断的场景。结合长短期记忆网络与松弛水平衡约束以保持物理一致性。

Result: 在美国5000多个流域测试，包括印度高度调控河流，模拟器能重现GloFAS水文核心功能，并在信息质量下降时平稳退化。跨不同水文气候和管理体制的迁移产生降低但仍保持物理一致性的性能。

Conclusion: 该框架将操作稳健性确立为水文机器学习可测量属性，推进了可靠实时预报系统的设计，定义了在数据稀缺和人为影响下泛化的极限。

Abstract: Reliable hydrologic and flood forecasting requires models that remain stable
when input data are delayed, missing, or inconsistent. However, most advances
in rainfall-runoff prediction have been evaluated under ideal data conditions,
emphasizing accuracy rather than operational resilience. Here, we develop an
operationally ready emulator of the Global Flood Awareness System (GloFAS) that
couples long- and short-term memory networks with a relaxed water-balance
constraint to preserve physical coherence. Five architectures span a continuum
of information availability: from complete historical and forecast forcings to
scenarios with data latency and outages, allowing systematic evaluation of
robustness. Trained in minimally managed catchments across the United States
and tested in more than 5,000 basins, including heavily regulated rivers in
India, the emulator reproduces the hydrological core of GloFAS and degrades
smoothly as information quality declines. Transfer across contrasting
hydroclimatic and management regimes yields reduced yet physically consistent
performance, defining the limits of generalization under data scarcity and
human influence. The framework establishes operational robustness as a
measurable property of hydrological machine learning and advances the design of
reliable real-time forecasting systems.

</details>


### [105] [SOCIA-Nabla: Textual Gradient Meets Multi-Agent Orchestration for Automated Simulator Generation](https://arxiv.org/abs/2510.18551)
*Yuncheng Hua,Sion Weatherhead,Mehdi Jafari,Hao Xue,Flora D. Salim*

Main category: cs.AI

TL;DR: SOCIA-Nabla是一个端到端的智能体框架，将模拟器构建视为代码实例优化，通过文本计算图中的LLM驱动智能体和文本梯度下降优化，实现代码合成、执行、评估和修复的循环。


<details>
  <summary>Details</summary>
Motivation: 传统提示工程方法脆弱且难以扩展，需要将多智能体编排与损失对齐优化相结合，实现可复现、约束感知的模拟器代码生成。

Method: 在文本计算图中嵌入专用LLM智能体作为节点，工作流管理器执行损失驱动循环：代码合成→执行→评估→代码修复，采用文本梯度下降优化，保留人机交互用于任务规范确认。

Result: 在三个CPS任务（用户建模、口罩采用、个人移动性）中达到最先进的整体准确率。

Conclusion: SOCIA-Nabla将脆弱的提示管道转换为可复现、约束感知的模拟器代码生成，能够跨领域和模拟粒度扩展。

Abstract: In this paper, we present SOCIA-Nabla, an end-to-end, agentic framework that
treats simulator construction asinstance optimization over code within a
textual computation graph. Specialized LLM-driven agents are embedded as graph
nodes, and a workflow manager executes a loss-driven loop: code synthesis ->
execution -> evaluation -> code repair. The optimizer performs Textual-Gradient
Descent (TGD), while human-in-the-loop interaction is reserved for task-spec
confirmation, minimizing expert effort and keeping the code itself as the
trainable object. Across three CPS tasks, i.e., User Modeling, Mask Adoption,
and Personal Mobility, SOCIA-Nabla attains state-of-the-art overall accuracy.
By unifying multi-agent orchestration with a loss-aligned optimization view,
SOCIA-Nabla converts brittle prompt pipelines into reproducible,
constraint-aware simulator code generation that scales across domains and
simulation granularities. This work is under review, and we will release the
code soon.

</details>


### [106] [Extracting alignment data in open models](https://arxiv.org/abs/2510.18554)
*Federico Barbero,Xiangming Gu,Christopher A. Choquette-Choo,Chawin Sitawarin,Matthew Jagielski,Itay Yona,Petar Veličković,Ilia Shumailov,Jamie Hayes*

Main category: cs.AI

TL;DR: 研究表明可以从经过后训练（如SFT或RL）的模型中提取大量对齐训练数据，这些数据可用于提升模型在长上下文推理、安全性、指令遵循和数学等方面的能力。


<details>
  <summary>Details</summary>
Motivation: 揭示模型在后训练阶段（如SFT或RL）容易重现训练数据，这可能被忽视的数据提取风险，并探讨蒸馏实践的下游影响。

Method: 使用高质量的嵌入模型来测量语义相似性，而非传统的字符串匹配方法，因为后者会严重低估可提取的数据量。

Result: 通过嵌入模型成功提取了对齐训练数据，这些数据可用于训练基础模型，恢复相当一部分原始性能。保守估计，传统字符串匹配方法会低估10倍的可提取数据量。

Conclusion: 这项工作暴露了提取对齐数据的潜在风险，并提出了关于蒸馏实践的有趣讨论：由于模型会重现其训练集的某些方面，蒸馏可被视为间接在模型原始数据集上进行训练。

Abstract: In this work, we show that it is possible to extract significant amounts of
alignment training data from a post-trained model -- useful to steer the model
to improve certain capabilities such as long-context reasoning, safety,
instruction following, and maths. While the majority of related work on
memorisation has focused on measuring success of training data extraction
through string matching, we argue that embedding models are better suited for
our specific goals. Distances measured through a high quality embedding model
can identify semantic similarities between strings that a different metric such
as edit distance will struggle to capture. In fact, in our investigation,
approximate string matching would have severely undercounted (by a conservative
estimate of $10\times$) the amount of data that can be extracted due to trivial
artifacts that deflate the metric. Interestingly, we find that models readily
regurgitate training data that was used in post-training phases such as SFT or
RL. We show that this data can be then used to train a base model, recovering a
meaningful amount of the original performance. We believe our work exposes a
possibly overlooked risk towards extracting alignment data. Finally, our work
opens up an interesting discussion on the downstream effects of distillation
practices: since models seem to be regurgitating aspects of their training set,
distillation can therefore be thought of as indirectly training on the model's
original dataset.

</details>


### [107] [QuantEvolve: Automating Quantitative Strategy Discovery through Multi-Agent Evolutionary Framework](https://arxiv.org/abs/2510.18569)
*Junhyeog Yun,Hyoun Jun Lee,Insu Jeon*

Main category: cs.AI

TL;DR: QuantEvolve是一个结合质量多样性优化和假设驱动策略生成的进化框架，用于自动化开发适应动态市场的量化交易策略。


<details>
  <summary>Details</summary>
Motivation: 动态市场中自动化开发量化交易策略具有挑战性，现有方法难以在探索广阔策略空间的同时保持多样性，而多样性对于在不同市场条件下保持稳健性能至关重要。

Method: 采用质量多样性优化和假设驱动的多智能体系统，通过特征映射（策略类型、风险特征、换手率、收益特征等）保持策略多样性，并通过迭代生成和评估系统性地探索策略空间。

Result: 实证结果显示QuantEvolve优于传统基线方法，能够产生适应市场状态变化和个性化投资需求的多样化、复杂策略。

Conclusion: QuantEvolve框架有效解决了量化交易策略开发的挑战，发布了进化策略数据集以支持未来研究。

Abstract: Automating quantitative trading strategy development in dynamic markets is
challenging, especially with increasing demand for personalized investment
solutions. Existing methods often fail to explore the vast strategy space while
preserving the diversity essential for robust performance across changing
market conditions. We present QuantEvolve, an evolutionary framework that
combines quality-diversity optimization with hypothesis-driven strategy
generation. QuantEvolve employs a feature map aligned with investor
preferences, such as strategy type, risk profile, turnover, and return
characteristics, to maintain a diverse set of effective strategies. It also
integrates a hypothesis-driven multi-agent system to systematically explore the
strategy space through iterative generation and evaluation. This approach
produces diverse, sophisticated strategies that adapt to both market regime
shifts and individual investment needs. Empirical results show that QuantEvolve
outperforms conventional baselines, validating its effectiveness. We release a
dataset of evolved strategies to support future research.

</details>


### [108] [VAR: Visual Attention Reasoning via Structured Search and Backtracking](https://arxiv.org/abs/2510.18619)
*Wei Cai,Jian Zhao,Yuchen Yuan,Tianle Zhang,Ming Zhu,Haichuan Tang,Chi Zhang,Xuelong Li*

Main category: cs.AI

TL;DR: 提出VAR框架，通过结构化搜索和回溯机制解决MLLMs的幻觉问题，在幻觉和安全基准测试中达到SOTA性能


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型存在高幻觉倾向和脆弱的线性推理过程，导致复杂任务失败

Method: 将推理重构为结构化搜索，包含可追溯证据基础和带回溯机制的搜索式思维链生成，使用多维度奖励函数进行语义和几何自验证

Result: 7B模型VAR-7B在幻觉和安全基准测试中创下新纪录，显著优于开源模型，与领先专有系统性能相当

Conclusion: VAR框架通过结构化搜索和自验证机制有效解决了MLLMs的幻觉问题，为可靠的多模态推理提供了新方法

Abstract: Multimodal Large Language Models (MLLMs), despite their advances, are
hindered by their high hallucination tendency and heavy reliance on brittle,
linear reasoning processes, leading to failures in complex tasks. To address
these limitations, we introduce Visual Attention Reasoning (VAR), a novel
framework that recasts grounded reasoning as a structured search over a
reasoning trajectory space. VAR decomposes the reasoning process into two key
stages: traceable evidence grounding and search-based chain-of-thought (CoT)
generation, which incorporates a backtracking mechanism for self-correction.
The search is guided by a multi-faceted reward function with semantic and
geometric self-verification components, which penalize outputs that are not
faithfully grounded in the visual input. We provide a theoretical analysis for
our search strategy, validating its capability to find the correct solution
with high probability. Experimental results show that our 7B model, VAR-7B,
sets a new state-of-the-art on a comprehensive suite of hallucination and
safety benchmarks, significantly outperforming existing open-source models and
demonstrating competitive performance against leading proprietary systems.

</details>


### [109] [Leveraging Association Rules for Better Predictions and Better Explanations](https://arxiv.org/abs/2510.18628)
*Gilles Audemard,Sylvie Coste-Marquis,Pierre Marquis,Mehdi Sabiri,Nicolas Szczepanski*

Main category: cs.AI

TL;DR: 提出了一种结合数据和知识的分类方法，使用数据挖掘从数据中提取关联规则，提升树基模型的预测性能，并改进可解释性。


<details>
  <summary>Details</summary>
Motivation: 结合数据和知识来提高分类模型的预测性能和可解释性，利用关联规则增强树基模型的能力。

Method: 从数据中挖掘关联规则（可能包含否定），将这些规则融入决策树和随机森林模型，用于提升分类性能和生成更一般的溯因解释。

Result: 实验表明，该方法在预测性能和解释规模方面对两种树基模型都有益处。

Conclusion: 提出的方法能有效提升树基分类模型的预测性能和可解释性，通过利用关联规则实现了数据和知识的结合。

Abstract: We present a new approach to classification that combines data and knowledge.
In this approach, data mining is used to derive association rules (possibly
with negations) from data. Those rules are leveraged to increase the predictive
performance of tree-based models (decision trees and random forests) used for a
classification task. They are also used to improve the corresponding
explanation task through the generation of abductive explanations that are more
general than those derivable without taking such rules into account.
Experiments show that for the two tree-based models under consideration,
benefits can be offered by the approach in terms of predictive performance and
in terms of explanation sizes.

</details>


### [110] [Comparative Expressivity for Structured Argumentation Frameworks with Uncertain Rules and Premises](https://arxiv.org/abs/2510.18631)
*Carlo Proietti,Antonio Yuste-Ginel*

Main category: cs.AI

TL;DR: 本文研究了形式论证中定性不确定性的建模，比较了抽象模型和结构化模型在表达能力方面的差异，提出了新的表达能力概念，并给出了正负两方面的表达能力结果。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注抽象的不确定性论证模型，但缺乏对这些抽象模型的具体实例化研究。本文旨在填补这一空白，通过将论证的不确定性基于其组成元素（规则和前提）来研究结构化模型。

Method: 将论证的不确定性基于其结构化组件（规则和前提），引入新的表达能力概念来处理抽象和结构化形式化方法，比较抽象论证框架（包括不完整框架和依赖扩展）与结构化模型（ASPIC+）的表达能力。

Result: 提出了能够处理抽象和结构化形式化方法的表达能力概念，给出了正负两方面的表达能力结果，比较了抽象模型和结构化模型在不确定性论证中的表达能力差异。

Conclusion: 本文为形式论证中不确定性建模提供了理论基础，通过比较抽象和结构化模型的表达能力，揭示了它们在处理不确定性方面的差异和局限性，为后续研究提供了方向。

Abstract: Modelling qualitative uncertainty in formal argumentation is essential both
for practical applications and theoretical understanding. Yet, most of the
existing works focus on \textit{abstract} models for arguing with uncertainty.
Following a recent trend in the literature, we tackle the open question of
studying plausible instantiations of these abstract models. To do so, we ground
the uncertainty of arguments in their components, structured within rules and
premises. Our main technical contributions are: i) the introduction of a notion
of expressivity that can handle abstract and structured formalisms, and ii) the
presentation of both negative and positive expressivity results, comparing the
expressivity of abstract and structured models of argumentation with
uncertainty. These results affect incomplete abstract argumentation frameworks,
and their extension with dependencies, on the abstract side, and ASPIC+, on the
structured side.

</details>


### [111] [Query Decomposition for RAG: Balancing Exploration-Exploitation](https://arxiv.org/abs/2510.18633)
*Roxana Petcu,Kenton Murray,Daniel Khashabi,Evangelos Kanoulas,Maarten de Rijke,Dawn Lawrie,Kevin Duh*

Main category: cs.AI

TL;DR: 本文提出了一种基于多臂老虎机方法的检索增强生成系统，通过动态选择最有信息量的子查询来平衡检索的广度与精度，显著提升了文档检索效果和长文本生成性能。


<details>
  <summary>Details</summary>
Motivation: 检索增强生成系统在处理复杂用户请求时需要平衡两个关键权衡：(i) 检索足够广泛以捕获所有相关材料，(ii) 限制检索以避免过多噪声和计算成本。

Method: 将查询分解和文档检索建模为利用-探索场景，使用多臂老虎机学习方法动态选择最有信息量的子查询，利用排名信息和人工判断来估计文档相关性。

Result: 该方法在文档级精度上实现了35%的提升，α-nDCG提高了15%，并在长文本生成的下游任务中表现更好。

Conclusion: 基于多臂老虎机的动态子查询选择方法能有效平衡检索的广度与精度，显著提升检索增强生成系统的性能。

Abstract: Retrieval-augmented generation (RAG) systems address complex user requests by
decomposing them into subqueries, retrieving potentially relevant documents for
each, and then aggregating them to generate an answer. Efficiently selecting
informative documents requires balancing a key trade-off: (i) retrieving
broadly enough to capture all the relevant material, and (ii) limiting
retrieval to avoid excessive noise and computational cost. We formulate query
decomposition and document retrieval in an exploitation-exploration setting,
where retrieving one document at a time builds a belief about the utility of a
given sub-query and informs the decision to continue exploiting or exploring an
alternative. We experiment with a variety of bandit learning methods and
demonstrate their effectiveness in dynamically selecting the most informative
sub-queries. Our main finding is that estimating document relevance using rank
information and human judgments yields a 35% gain in document-level precision,
15% increase in {\alpha}-nDCG, and better performance on the downstream task of
long-form generation.

</details>


### [112] [Sherlock Your Queries: Learning to Ask the Right Questions for Dialogue-Based Retrieval](https://arxiv.org/abs/2510.18659)
*Dong Yun,Marco Schouten,Dim Papadopoulos*

Main category: cs.AI

TL;DR: SherlockLLM是一个基于强化学习的对话驱动检索框架，通过生成二进制问题序列来高效缩小搜索空间，无需大规模标注对话数据。


<details>
  <summary>Details</summary>
Motivation: 信息检索中的用户查询往往具有歧义性，现有对话式交互检索系统缺乏明确的策略来提出最具信息量的问题，导致效率低下。

Method: 使用强化学习训练代理生成二进制问题序列，通过优化提问策略来高效缩小搜索空间。

Result: 在结构化任务中性能与强基线相当，接近二分搜索的理论最优；在非结构化任务中显著优于基线，展示了高效信息寻求对话策略的学习能力。

Conclusion: SherlockLLM是一个鲁棒且高效的解决方案，能够学习到高度有效的信息寻求对话策略。

Abstract: User queries in information retrieval are often ambiguous, making it
challenging for systems to identify a user's target from a single query. While
recent dialogue-based interactive retrieval systems can clarify user intent,
they are inefficient as they often lack an explicit strategy to ask the most
informative questions. To address this limitation, we propose SherlockLLM, a
dialogue-driven retrieval framework that learns an optimal questioning strategy
via Reinforcement Learning (RL) and avoids the need for large-scale annotated
dialogue data. In our framework, an agent is trained to generate a sequence of
binary questions to efficiently narrow down the search space. To validate our
approach, we introduce a benchmark with both structured and unstructured tasks.
Experimental results show that SherlockLLM is a robust and efficient solution.
On the structured tasks, its performance matches strong baselines and
approaches the theoretical optimal defined by binary search. On the challenging
unstructured task, our agent significantly outperforms these baselines,
showcasing its ability to learn a highly effective information-seeking dialogue
policy.

</details>


### [113] [Seg the HAB: Language-Guided Geospatial Algae Bloom Reasoning and Segmentation](https://arxiv.org/abs/2510.18751)
*Patterson Hsieh,Jerry Yeh,Mao-Chi He,Wen-Han Hsieh,Elvis Hsieh*

Main category: cs.AI

TL;DR: ALGOS是一个用于有害藻华监测的分割与推理系统，结合遥感图像理解和严重程度估计，通过GeoSAM辅助人工评估和微调视觉语言模型，在分割和严重程度估计方面表现稳健。


<details>
  <summary>Details</summary>
Motivation: 气候变化加剧了有害藻华的发生，特别是蓝藻，威胁水生生态系统和人类健康。传统监测方法劳动密集且时空覆盖有限，需要可扩展的AI驱动解决方案。

Method: 结合GeoSAM辅助人工评估进行高质量分割掩码整理，并使用NASA的蓝藻聚合手动标签(CAML)微调视觉语言模型进行严重程度预测。

Result: 实验表明ALGOS在分割和严重程度级别估计方面都取得了稳健的性能。

Conclusion: 该系统为实现实用和自动化的蓝藻监测系统铺平了道路。

Abstract: Climate change is intensifying the occurrence of harmful algal bloom (HAB),
particularly cyanobacteria, which threaten aquatic ecosystems and human health
through oxygen depletion, toxin release, and disruption of marine biodiversity.
Traditional monitoring approaches, such as manual water sampling, remain
labor-intensive and limited in spatial and temporal coverage. Recent advances
in vision-language models (VLMs) for remote sensing have shown potential for
scalable AI-driven solutions, yet challenges remain in reasoning over imagery
and quantifying bloom severity. In this work, we introduce ALGae Observation
and Segmentation (ALGOS), a segmentation-and-reasoning system for HAB
monitoring that combines remote sensing image understanding with severity
estimation. Our approach integrates GeoSAM-assisted human evaluation for
high-quality segmentation mask curation and fine-tunes vision language model on
severity prediction using the Cyanobacteria Aggregated Manual Labels (CAML)
from NASA. Experiments demonstrate that ALGOS achieves robust performance on
both segmentation and severity-level estimation, paving the way toward
practical and automated cyanobacterial monitoring systems.

</details>


### [114] [Decoding Funded Research: Comparative Analysis of Topic Models and Uncovering the Effect of Gender and Geographic Location](https://arxiv.org/abs/2510.18803)
*Shirin Tavakoli Kafiabad,Andrea Schiffauerova,Ashkan Ebadi*

Main category: cs.AI

TL;DR: 本研究比较了三种主题建模方法（LDA、STM、BERTopic）在分析加拿大自然科学与工程研究理事会18年资助提案中的应用，并开发了COFFEE算法来增强BERTopic的协变量分析能力。


<details>
  <summary>Details</summary>
Motivation: 优化国家科学投资需要了解研究趋势演变及人口地理因素的影响，特别是在促进公平、多样性和包容性的背景下。

Method: 使用LDA、STM和BERTopic三种主题建模方法分析2005-2022年NSERC资助提案，并开发COFFEE算法为BERTopic提供协变量效应估计功能。

Result: BERTopic在识别更精细、连贯和新兴主题方面表现最佳，如人工智能的快速发展；协变量分析揭示了省级研究专业化和跨学科性别主题模式。

Conclusion: 研究结果为资助机构制定更公平有效的资助策略提供了实证基础，有助于提升科学生态系统的效能。

Abstract: Optimizing national scientific investment requires a clear understanding of
evolving research trends and the demographic and geographical forces shaping
them, particularly in light of commitments to equity, diversity, and inclusion.
This study addresses this need by analyzing 18 years (2005-2022) of research
proposals funded by the Natural Sciences and Engineering Research Council of
Canada (NSERC). We conducted a comprehensive comparative evaluation of three
topic modelling approaches: Latent Dirichlet Allocation (LDA), Structural Topic
Modelling (STM), and BERTopic. We also introduced a novel algorithm, named
COFFEE, designed to enable robust covariate effect estimation for BERTopic.
This advancement addresses a significant gap, as BERTopic lacks a native
function for covariate analysis, unlike the probabilistic STM. Our findings
highlight that while all models effectively delineate core scientific domains,
BERTopic outperformed by consistently identifying more granular, coherent, and
emergent themes, such as the rapid expansion of artificial intelligence.
Additionally, the covariate analysis, powered by COFFEE, confirmed distinct
provincial research specializations and revealed consistent gender-based
thematic patterns across various scientific disciplines. These insights offer a
robust empirical foundation for funding organizations to formulate more
equitable and impactful funding strategies, thereby enhancing the effectiveness
of the scientific ecosystem.

</details>
