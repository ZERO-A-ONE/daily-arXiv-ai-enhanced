<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 21]
- [cs.CR](#cs.CR) [Total: 28]
- [cs.AI](#cs.AI) [Total: 41]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [How Do Community Smells Influence Self-Admitted Technical Debt in Machine Learning Projects?](https://arxiv.org/abs/2506.15884)
*Shamse Tasnim Cynthia,Nuri Almarimi,Banani Roy*

Main category: cs.SE

TL;DR: 研究了开源机器学习项目中社区气味与自认技术债务（SATD）的关系，发现某些社区气味与SATD高度相关，并揭示了其演化趋势。


<details>
  <summary>Details</summary>
Motivation: 探索机器学习项目中社区气味与SATD的关系，填补现有研究的空白。

Method: 分析了155个ML项目的发布数据，检测社区气味和SATD，并进行统计分析和演化趋势研究。

Result: 社区气味普遍存在，某些气味（如Radio Silence和Organizational Silos）与SATD高度相关，且存在项目规模依赖的演化趋势。

Conclusion: 早期检测和缓解社会技术问题对ML系统的长期质量和可持续性至关重要。

Abstract: Community smells reflect poor organizational practices that often lead to
socio-technical issues and the accumulation of Self-Admitted Technical Debt
(SATD). While prior studies have explored these problems in general software
systems, their interplay in machine learning (ML)-based projects remains
largely underexamined. In this study, we investigated the prevalence of
community smells and their relationship with SATD in open-source ML projects,
analyzing data at the release level. First, we examined the prevalence of ten
community smell types across the releases of 155 ML-based systems and found
that community smells are widespread, exhibiting distinct distribution patterns
across small, medium, and large projects. Second, we detected SATD at the
release level and applied statistical analysis to examine its correlation with
community smells. Our results showed that certain smells, such as Radio Silence
and Organizational Silos, are strongly correlated with higher SATD occurrences.
Third, we considered the six identified types of SATD to determine which
community smells are most associated with each debt category. Our analysis
revealed authority- and communication-related smells often co-occur with
persistent code and design debt. Finally, we analyzed how the community smells
and SATD evolve over the releases, uncovering project size-dependent trends and
shared trajectories. Our findings emphasize the importance of early detection
and mitigation of socio-technical issues to maintain the long-term quality and
sustainability of ML-based systems.

</details>


### [2] [Regression Testing Optimization for ROS-based Autonomous Systems: A Comprehensive Review of Techniques](https://arxiv.org/abs/2506.16101)
*Yupeng Jiang,Shuaiyi Sun,Xi Zheng*

Main category: cs.SE

TL;DR: 本文是一篇关于ROS自主系统（ROSAS）回归测试优化的综述，分析了122项研究，提出了分类法和未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 传统回归测试技术难以应对ROSAS的动态性、非确定性及复杂架构，相关优化研究尚不充分。

Method: 系统回顾并分类了122项研究，提出分类法，分析其适用性和局限性。

Result: 总结了ROSAS回归测试的挑战（如测试优先级、冗余测试最小化）及潜在解决方案。

Conclusion: 提出了未来研究方向（如帧到向量覆盖度量、多源基础模型），为ROSAS回归测试优化提供参考。

Abstract: Regression testing plays a critical role in maintaining software reliability,
particularly for ROS-based autonomous systems (ROSAS), which frequently undergo
continuous integration and iterative development. However, conventional
regression testing techniques face significant challenges when applied to
autonomous systems due to their dynamic and non-deterministic behaviors,
complex multi-modal sensor data, asynchronous distributed architectures, and
stringent safety and real-time constraints. Although numerous studies have
explored test optimization in traditional software contexts, regression testing
optimization specifically for ROSAS remains largely unexplored. To address this
gap, we present the first comprehensive survey systematically reviewing
regression testing optimization techniques tailored for ROSAS. We analyze and
categorize 122 representative studies into regression test case prioritization,
minimization, and selection methods. A structured taxonomy is introduced to
clearly illustrate their applicability and limitations within ROSAS contexts.
Furthermore, we highlight major challenges specific to regression testing for
ROSAS, including effectively prioritizing tests in response to frequent system
modifications, efficiently minimizing redundant tests, and difficulty in
accurately selecting impacted test cases. Finally, we propose research insights
and identify promising future directions, such as leveraging frame-to-vector
coverage metrics, multi-source foundation models, and neurosymbolic reasoning
to enhance regression testing efficiency and effectiveness. This survey
provides a foundational reference and practical roadmap for advancing the
state-of-the-art in regression testing optimization for ROSAS.

</details>


### [3] [Seeing is Fixing: Cross-Modal Reasoning with Multimodal LLMs for Visual Software Issue Fixing](https://arxiv.org/abs/2506.16136)
*Kai Huang,Jian Zhang,Xiaofei Xie,Chunyang Chen*

Main category: cs.SE

TL;DR: GUIRepair是一种跨模态推理方法，通过理解和利用视觉信息解决多模态问题，显著提升了基于LLM的自动程序修复（APR）在多模态场景中的表现。


<details>
  <summary>Details</summary>
Motivation: 现有APR系统在单模态场景中表现良好，但在多模态场景（如涉及GUI的问题）中因无法有效利用视觉信息而表现不佳。GUIRepair旨在填补这一空白。

Method: GUIRepair结合Image2Code和Code2Image两个组件：Image2Code将GUI图像转换为可执行代码以理解故障，Code2Image通过重现视觉场景验证修复效果。

Result: 在SWE-bench M上，GUIRepair显著优于基线方法（如使用GPT-4o解决157个问题，比最佳开源基线多26个；使用o4-mini解决175个问题，比最佳商业系统多22个）。

Conclusion: GUIRepair通过跨模态推理成功解决了多模态问题，验证了视觉信息在APR中的重要性。

Abstract: Large language model-(LLM) based automated program repair (APR) techniques
have shown promising results in resolving real-world GitHub issue tasks.
Existing APR systems are primarily evaluated in unimodal settings (e.g.,
SWE-bench). However, these autonomous systems struggle to resolve multimodal
problem scenarios (e.g., SWE-bench M) due to limitations in interpreting and
leveraging visual information. In multimodal scenarios, LLMs need to rely on
visual information in the graphical user interface (GUI) to understand bugs and
generate fixes. To bridge this gap, we propose GUIRepair, a cross-modal
reasoning approach for resolving multimodal issue scenarios by understanding
and capturing visual information. Specifically, GUIRepair integrates two key
components, Image2Code and Code2Image, to enhance fault comprehension and patch
validation. Image2Code extracts relevant project documents based on the issue
report, then applies this domain knowledge to generate the reproduced code
responsible for the visual symptoms, effectively translating GUI images into
executable context for better fault comprehension. Code2Image replays the
visual issue scenario using the reproduced code and captures GUI renderings of
the patched program to assess whether the fix visually resolves the issue,
providing feedback for patch validation. We evaluate GUIRepair on SWE-bench M,
and the approach demonstrates significant effectiveness. When utilizing GPT-4o
as the base model, GUIRepair solves 157 instances, outperforming the best
open-source baseline by 26 instances. Furthermore, when using o4-mini as the
base model, GUIRepair can achieve even better results and solve 175 instances,
outperforming the top commercial system by 22 instances. This emphasizes the
success of our new perspective on incorporating cross-modal reasoning by
understanding and capturing visual information to resolve multimodal issues.

</details>


### [4] [The Technical Debt Gamble: A Case Study on Technical Debt in a Large-Scale Industrial Microservice Architecture](https://arxiv.org/abs/2506.16214)
*Klara Borowa,Andrzej Ratkowski,Roberto Verdecchia*

Main category: cs.SE

TL;DR: 论文研究了大规模微服务系统中的技术债务（TD）表现，通过混合方法案例研究发现静态代码分析是有效的TD发现入口，沟通不足和组织架构不匹配会加剧TD积累，并提出微服务架构中TD管理的策略。


<details>
  <summary>Details</summary>
Motivation: 微服务架构因松散耦合而具有高可维护性和可演化性，但技术债务（TD）对这些质量属性有显著影响，目前缺乏大规模微服务系统中TD的研究。

Method: 采用混合方法案例研究，包括基于静态代码分析的定量方法和与开发团队的焦点小组讨论及首席架构师访谈的定性方法。

Result: 研究发现：静态代码分析是有效的TD发现入口；沟通不足和组织架构不匹配会加剧TD；微服务中TD积累与解决快速循环（称为“微服务架构技术债务赌博”）。

Conclusion: 提出了一套适用于微服务架构的TD管理策略。

Abstract: Microservice architectures provide an intuitive promise of high
maintainability and evolvability due to loose coupling. However, these quality
attributes are notably vulnerable to technical debt (TD). Few studies address
TD in microservice systems, particularly on a large scale. This research
explores how TD manifests in a large-scale microservice-based industrial
system. The research is based on a mixed-method case study of a project
including over 100 microservices and serving over 15k locations. Results are
collected via a quantitative method based static code analyzers combined with
qualitative insights derived from a focus group discussion with the development
team and a follow-up interview with the lead architect of the case study
system. Results show that (1) simple static source code analysis can be an
efficient and effective entry point for holistic TD discovery, (2) inadequate
communication significantly contributes to TD, (3) misalignment between
architectural and organizational structures can exacerbate TD accumulation, (4)
microservices can rapidly cycle through TD accumulation and resolution, a
phenomenon referred to as "microservice architecture technical debt gamble".
Finally, we identify a set of fitting strategies for TD management in
microservice architectures.

</details>


### [5] [Evaluating the Use of LLMs for Documentation to Code Traceability](https://arxiv.org/abs/2506.16440)
*Ebube Alor,SayedHassan Khatoonabadi,Emad Shihab*

Main category: cs.SE

TL;DR: 论文评估了大型语言模型（LLMs）在文档与代码间建立追踪链接的能力，发现其表现优于传统方法，但仍需人工辅助。


<details>
  <summary>Details</summary>
Motivation: 探索LLMs在自动化文档与代码追踪方面的潜力，填补研究空白。

Method: 通过系统实验评估LLMs在三个关键能力上的表现：追踪链接识别、关系解释质量、多步链重建。使用两个新数据集（Unity Catalog和Crawl4AI）和多个LLM模型（Claude 3.5 Sonnet、GPT-4o、o3-mini）。

Result: 最佳LLM的F1分数达79.4%和80.4%，远超基线方法（TF-IDF、BM25、CodeBERT）。关系解释的完全准确率为42.9%-71.1%，部分准确率超97%。多步链的端点准确率高，但中间链接精确度不一。

Conclusion: LLMs是强大的追踪发现工具，但需结合人工辅助设计工具，并针对特定错误模式进一步研究。

Abstract: Large Language Models (LLMs) offer new potential for automating
documentation-to-code traceability, yet their capabilities remain
underexplored. We present a comprehensive evaluation of LLMs (Claude 3.5
Sonnet, GPT-4o, and o3-mini) in establishing trace links between various
software documentation (including API references and user guides) and source
code. We create two novel datasets from two open-source projects (Unity Catalog
and Crawl4AI). Through systematic experiments, we assess three key
capabilities: (1) trace link identification accuracy, (2) relationship
explanation quality, and (3) multi-step chain reconstruction. Results show that
the best-performing LLM achieves F1-scores of 79.4% and 80.4% across the two
datasets, substantially outperforming our baselines (TF-IDF, BM25, and
CodeBERT). While fully correct relationship explanations range from 42.9% to
71.1%, partial accuracy exceeds 97%, indicating that fundamental connections
are rarely missed. For multi-step chains, LLMs maintain high endpoint accuracy
but vary in capturing precise intermediate links. Error analysis reveals that
many false positives stem from naming-based assumptions, phantom links, or
overgeneralization of architectural patterns. We demonstrate that task-framing,
such as a one-to-many matching strategy, is critical for performance. These
findings position LLMs as powerful assistants for trace discovery, but their
limitations could necessitate human-in-the-loop tool design and highlight
specific error patterns for future research.

</details>


### [6] [Understanding the Challenges and Promises of Developing Generative AI Apps: An Empirical Study](https://arxiv.org/abs/2506.16453)
*Buthayna AlMulla,Maram Assi,Safwat Hassan*

Main category: cs.SE

TL;DR: 论文通过分析Google Play Store上173款Gen-AI应用的676,066条用户评论，提出了一种四阶段方法SARA，利用LLM技术提取用户对Gen-AI功能的看法，并识别了10个热门话题及其演变趋势。


<details>
  <summary>Details</summary>
Motivation: 研究Gen-AI应用在用户中的实际感知和评价，填补当前研究的空白。

Method: 采用SARA方法（选择、获取、精炼和分析），结合LLM技术进行主题提取和评论过滤。

Result: LLM在主题提取中达到91%准确率，识别出10个热门话题（如AI性能、内容质量等），并分析了用户期望的变化趋势。

Conclusion: 研究为开发者和研究者提供了实用建议，帮助优化Gen-AI应用的用户体验。

Abstract: The release of ChatGPT in 2022 triggered a rapid surge in generative
artificial intelligence mobile apps (i.e., Gen-AI apps). Despite widespread
adoption, little is known about how end users perceive and evaluate these
Gen-AI functionalities in practice. In this work, we conduct a user-centered
analysis of 676,066 reviews from 173 Gen-AI apps on the Google Play Store. We
introduce a four-phase methodology, SARA (Selection, Acquisition, Refinement,
and Analysis), that enables the systematic extraction of user insights using
prompt-based LLM techniques. First, we demonstrate the reliability of LLMs in
topic extraction, achieving 91% accuracy through five-shot prompting and
non-informative review filtering. Then, we apply this method to the informative
reviews, identify the top 10 user-discussed topics (e.g., AI Performance,
Content Quality, and Content Policy & Censorship) and analyze the key
challenges and emerging opportunities. Finally, we examine how these topics
evolve over time, offering insight into shifting user expectations and
engagement patterns with Gen-AI apps. Based on our findings and observations,
we present actionable implications for developers and researchers.

</details>


### [7] [Scaling GR(1) Synthesis via a Compositional Framework for LTL Discrete Event Control](https://arxiv.org/abs/2506.16557)
*Hernán Gagliardi,Victor Braberman,Sebastian Uchitel*

Main category: cs.SE

TL;DR: 提出了一种基于模块化的离散事件系统控制器合成方法，通过分解问题缓解状态爆炸，并利用观测等价性优化控制规模。


<details>
  <summary>Details</summary>
Motivation: 解决传统单体合成方法因状态爆炸问题而难以处理大规模系统的问题。

Method: 利用模块化结构和观测合成等价性，迭代构建最大允许安全控制器，并通过并行运行确保LTL目标。

Result: 在MTSA工具中实现，能处理比单体方法大1000倍的问题规模。

Conclusion: 模块化方法显著提升了控制器合成的可扩展性和效率。

Abstract: We present a compositional approach to controller synthesis of discrete event
system controllers with linear temporal logic (LTL) goals. We exploit the
modular structure of the plant to be controlled, given as a set of labelled
transition systems (LTS), to mitigate state explosion that monolithic
approaches to synthesis are prone to. Maximally permissive safe controllers are
iteratively built for subsets of the plant LTSs by solving weaker control
problems. Observational synthesis equivalence is used to reduce the size of the
controlled subset of the plant by abstracting away local events. The result of
synthesis is also compositional, a set of controllers that when run in parallel
ensure the LTL goal. We implement synthesis in the MTSA tool for an expressive
subset of LTL, GR(1), and show it computes solutions to that can be up to 1000
times larger than those that the monolithic approach can solve.

</details>


### [8] [AI-Driven Tools in Modern Software Quality Assurance: An Assessment of Benefits, Challenges, and Future Directions](https://arxiv.org/abs/2506.16586)
*Ihor Pysmennyi,Roman Kyslyi,Kyrylo Kleshch*

Main category: cs.SE

TL;DR: 研究探讨了将现代AI工具集成到质量保证（QA）流程中的潜力与挑战，通过实证分析展示了其效果，但也指出了实际应用中的问题。


<details>
  <summary>Details</summary>
Motivation: 传统QA方法难以应对现代软件系统的复杂性、规模和快速迭代，资源有限导致质量成本高昂，因此研究探索AI工具的集成。

Method: 综合分析了AI工具对验证和验证流程的影响，包括多种测试方法，并通过企业应用的端到端回归测试作为概念验证。

Result: 生成的测试用例仅有8.3%的不稳定执行率，显示AI工具的潜力，但也发现语义覆盖生成、黑盒性和可解释性等挑战。

Conclusion: AI对QA具有变革潜力，但需战略性地实施，并开发适当的验证方法以克服现有局限性。

Abstract: Traditional quality assurance (QA) methods face significant challenges in
addressing the complexity, scale, and rapid iteration cycles of modern software
systems and are strained by limited resources available, leading to substantial
costs associated with poor quality. The object of this research is the Quality
Assurance processes for modern distributed software applications. The subject
of the research is the assessment of the benefits, challenges, and prospects of
integrating modern AI-oriented tools into quality assurance processes. We
performed comprehensive analysis of implications on both verification and
validation processes covering exploratory test analyses, equivalence
partitioning and boundary analyses, metamorphic testing, finding
inconsistencies in acceptance criteria (AC), static analyses, test case
generation, unit test generation, test suit optimization and assessment, end to
end scenario execution. End to end regression of sample enterprise application
utilizing AI-agents over generated test scenarios was implemented as a proof of
concept highlighting practical use of the study. The results, with only 8.3%
flaky executions of generated test cases, indicate significant potential for
the proposed approaches. However, the study also identified substantial
challenges for practical adoption concerning generation of semantically
identical coverage, "black box" nature and lack of explainability from
state-of-the-art Large Language Models (LLMs), the tendency to correct mutated
test cases to match expected results, underscoring the necessity for thorough
verification of both generated artifacts and test execution results. The
research demonstrates AI's transformative potential for QA but highlights the
importance of a strategic approach to implementing these technologies,
considering the identified limitations and the need for developing appropriate
verification methodologies.

</details>


### [9] [LLM-based Satisfiability Checking of String Requirements by Consistent Data and Checker Generation](https://arxiv.org/abs/2506.16639)
*Boqi Chen,Aren A. Babikian,Shuzhao Feng,Dániel Varró,Gunter Mussbacher*

Main category: cs.SE

TL;DR: 论文提出了一种混合方法，利用大语言模型（LLMs）验证自然语言字符串需求的可满足性，并生成检查器以提高准确性。


<details>
  <summary>Details</summary>
Motivation: 由于软件系统对字符串操作的依赖，验证自然语言需求的可满足性具有挑战性，传统方法效率低且需要大量人工。

Method: 结合LLMs生成可满足性结果和检查器（SMT和Python），并通过实验评估四种LLMs的性能。

Result: LLMs能有效生成检查器，Python检查器测试准确率达100%，显著提升生成一致字符串和识别不可满足需求的成功率。

Conclusion: 混合方法通过LLMs和检查器的结合，显著提高了自然语言字符串需求验证的效率和准确性。

Abstract: Requirements over strings, commonly represented using natural language (NL),
are particularly relevant for software systems due to their heavy reliance on
string data manipulation. While individual requirements can usually be analyzed
manually, verifying properties (e.g., satisfiability) over sets of NL
requirements is particularly challenging. Formal approaches (e.g., SMT solvers)
may efficiently verify such properties, but are known to have theoretical
limitations. Additionally, the translation of NL requirements into formal
constraints typically requires significant manual effort. Recently, large
language models (LLMs) have emerged as an alternative approach for formal
reasoning tasks, but their effectiveness in verifying requirements over strings
is less studied. In this paper, we introduce a hybrid approach that verifies
the satisfiability of NL requirements over strings by using LLMs (1) to derive
a satisfiability outcome (and a consistent string, if possible), and (2) to
generate declarative (i.e., SMT) and imperative (i.e., Python) checkers, used
to validate the correctness of (1). In our experiments, we assess the
performance of four LLMs. Results show that LLMs effectively translate natural
language into checkers, even achieving perfect testing accuracy for
Python-based checkers. These checkers substantially help LLMs in generating a
consistent string and accurately identifying unsatisfiable requirements,
leading to more than doubled generation success rate and F1-score in certain
cases compared to baselines without generated checkers.

</details>


### [10] [SemAgent: A Semantics Aware Program Repair Agent](https://arxiv.org/abs/2506.16650)
*Anvith Pabba,Alex Mathai,Anindya Chakraborty,Baishakhi Ray*

Main category: cs.SE

TL;DR: SemAgent通过结合问题、代码和执行语义，提出了一种新的工作流方法，显著提升了自动程序修复的效果，特别是在需要多行推理和边缘情况处理的场景中。


<details>
  <summary>Details</summary>
Motivation: 现有系统在解决软件工程问题时往往过于局部化，缺乏对问题、代码和执行语义的深入理解，导致生成的补丁过拟合。

Method: SemAgent采用了一种新颖的流水线，包括利用执行语义获取上下文、通过抽象理解问题语义、在抽象上下文中隔离代码语义，并通过两阶段架构（修复阶段和审查阶段）生成补丁。

Result: 在SWEBench-Lite基准测试中，SemAgent的解决率达到44.66%，优于其他工作流方法，并在基线基础上提升了7.66%。

Conclusion: 将问题和代码语义融入自动程序修复流程可以生成更鲁棒且语义一致的修复补丁。

Abstract: Large Language Models (LLMs) have shown impressive capabilities in downstream
software engineering tasks such as Automated Program Repair (APR). In
particular, there has been a lot of research on repository-level
issue-resolution benchmarks such as SWE-Bench. Although there has been
significant progress on this topic, we notice that in the process of solving
such issues, existing agentic systems tend to hyper-localize on immediately
suspicious lines of code and fix them in isolation, without a deeper
understanding of the issue semantics, code semantics, or execution semantics.
Consequently, many existing systems generate patches that overfit to the user
issue, even when a more general fix is preferable. To address this limitation,
we introduce SemAgent, a novel workflow-based procedure that leverages issue,
code, and execution semantics to generate patches that are complete -
identifying and fixing all lines relevant to the issue. We achieve this through
a novel pipeline that (a) leverages execution semantics to retrieve relevant
context, (b) comprehends issue-semantics via generalized abstraction, (c)
isolates code-semantics within the context of this abstraction, and (d)
leverages this understanding in a two-stage architecture: a repair stage that
proposes fine-grained fixes, followed by a reviewer stage that filters relevant
fixes based on the inferred issue-semantics. Our evaluations show that our
methodology achieves a solve rate of 44.66% on the SWEBench-Lite benchmark
beating all other workflow-based approaches, and an absolute improvement of
7.66% compared to our baseline, which lacks such deep semantic understanding.
We note that our approach performs particularly well on issues requiring
multi-line reasoning (and editing) and edge-case handling, suggesting that
incorporating issue and code semantics into APR pipelines can lead to robust
and semantically consistent repairs.

</details>


### [11] [LLMs in Coding and their Impact on the Commercial Software Engineering Landscape](https://arxiv.org/abs/2506.16653)
*Vladislav Belozerov,Peter J Barclay,Askhan Sami*

Main category: cs.SE

TL;DR: 论文探讨了大语言模型编码工具在软件工程中的主流化及其带来的隐私泄露、安全漏洞和模型谄媚性等新危险，提出了企业应采取的措施以确保安全性和准确性。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型编码工具的普及，其在提升开发效率的同时也带来了隐私泄露、安全漏洞和模型谄媚性等新问题，亟需解决。

Method: 通过分析真实提示和生成代码片段的数据，揭示了隐私泄露和安全漏洞的比例，并提出了模型谄媚性的概念。

Result: 研究发现10%的提示泄露隐私数据，42%的生成代码片段隐藏安全漏洞，模型还存在谄媚性问题。

Conclusion: 企业应对每行AI生成的代码进行标记和审查，将提示和输出限制在私有或本地部署中，遵守安全法规，并增加测试以检测谄媚性回答，以兼顾速度和安全性。

Abstract: Large-language-model coding tools are now mainstream in software engineering.
But as these same tools move human effort up the development stack, they
present fresh dangers: 10% of real prompts leak private data, 42% of generated
snippets hide security flaws, and the models can even ``agree'' with wrong
ideas, a trait called sycophancy. We argue that firms must tag and review every
AI-generated line of code, keep prompts and outputs inside private or
on-premises deployments, obey emerging safety regulations, and add tests that
catch sycophantic answers -- so they can gain speed without losing security and
accuracy.

</details>


### [12] [Accountability of Robust and Reliable AI-Enabled Systems: A Preliminary Study and Roadmap](https://arxiv.org/abs/2506.16831)
*Filippo Scaramuzza,Damian A. Tamburri,Willem-Jan van den Heuvel*

Main category: cs.SE

TL;DR: 本文初步研究了AI系统的稳健性和可靠性评估，探讨了确保其安全性和有效性的关键因素，尤其是问责制。通过案例研究，强调了创新测试解决方案的必要性。


<details>
  <summary>Details</summary>
Motivation: 评估AI系统的稳健性和可靠性，确保其在实际应用中的安全性和有效性，并强调问责制的重要性。

Method: 通过文献综述和案例研究，探讨了稳健性、可靠性和问责制的定义及挑战。

Result: 研究发现问责制对建立信任和负责任AI开发至关重要，并指出了当前研究中的空白。

Conclusion: 稳健性、可靠性和问责制是未来可信AI系统发展的关键领域，需进一步研究。

Abstract: This vision paper presents initial research on assessing the robustness and
reliability of AI-enabled systems, and key factors in ensuring their safety and
effectiveness in practical applications, including a focus on accountability.
By exploring evolving definitions of these concepts and reviewing current
literature, the study highlights major challenges and approaches in the field.
A case study is used to illustrate real-world applications, emphasizing the
need for innovative testing solutions. The incorporation of accountability is
crucial for building trust and ensuring responsible AI development. The paper
outlines potential future research directions and identifies existing gaps,
positioning robustness, reliability, and accountability as vital areas for the
development of trustworthy AI systems of the future.

</details>


### [13] [Revolutionizing Validation and Verification: Explainable Testing Methodologies for Intelligent Automotive Decision-Making Systems](https://arxiv.org/abs/2506.16876)
*Halit Eris,Stefan Wagner*

Main category: cs.SE

TL;DR: 论文提出了一种将可解释性、透明度和可解释性融入自动驾驶系统验证与验证（V&V）流程的方法，旨在提高效率、减少资源消耗并增强用户信任。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶系统的复杂决策模型和多模态输入使得验证与验证过程具有挑战性，现有手动测试方法效率低下且劳动密集。

Method: 通过文献综述和利益相关者输入细化V&V需求，利用大语言模型生成可解释测试场景，并在仿真环境中实现实时验证。

Result: 框架包括测试预言、解释生成和测试聊天机器人，计划通过实证研究评估诊断效率和透明度的改进。

Conclusion: 该方法旨在优化V&V流程，减少资源需求，并提升对自动驾驶技术的用户信任。

Abstract: Autonomous Driving Systems (ADS) use complex decision-making (DM) models with
multimodal sensory inputs, making rigorous validation and verification (V&V)
essential for safety and reliability. These models pose challenges in
diagnosing failures, tracing anomalies, and maintaining transparency, with
current manual testing methods being inefficient and labor-intensive. This
vision paper presents a methodology that integrates explainability,
transparency, and interpretability into V&V processes. We propose refining V&V
requirements through literature reviews and stakeholder input, generating
explainable test scenarios via large language models (LLMs), and enabling
real-time validation in simulation environments. Our framework includes test
oracle, explanation generation, and a test chatbot, with empirical studies
planned to evaluate improvements in diagnostic efficiency and transparency. Our
goal is to streamline V&V, reduce resources, and build user trust in autonomous
technologies.

</details>


### [14] [Quantum Optimization for Software Engineering: A Survey](https://arxiv.org/abs/2506.16878)
*Man Zhang,Yuechen Li,Tao Yue,Kai-Yuan Cai*

Main category: cs.SE

TL;DR: 本文通过系统性文献综述（SLR）研究了量子或量子启发算法在解决经典软件工程（SE）优化问题中的应用，分析了77篇主要研究，揭示了研究热点与空白。


<details>
  <summary>Details</summary>
Motivation: 现代软件系统及其工程过程的复杂性日益增加，需要创新解决方案，而量子计算在优化领域的进展为SE优化提供了新思路。

Method: 通过系统性搜索六个数字数据库，从2083篇文献中筛选出77篇主要研究，进行详细分析。

Result: 研究发现研究集中在SE操作和软件测试领域，其他SE活动存在显著空白；同时发现部分相关研究发表在非传统SE平台上。

Conclusion: 本研究为SBSE社区提供了全面的研究概览，助力利用量子技术解决下一代SE挑战。

Abstract: Quantum computing, particularly in the area of quantum optimization, is
steadily progressing toward practical applications, supported by an expanding
range of hardware platforms and simulators. While Software Engineering (SE)
optimization has a strong foundation, which is exemplified by the active
Search-Based Software Engineering (SBSE) community and numerous classical
optimization methods, the growing complexity of modern software systems and
their engineering processes demands innovative solutions. This Systematic
Literature Review (SLR) focuses specifically on studying the literature that
applies quantum or quantum-inspired algorithms to solve classical SE
optimization problems. We examine 77 primary studies selected from an initial
pool of 2083 publications obtained through systematic searches of six digital
databases using carefully crafted search strings. Our findings reveal
concentrated research efforts in areas such as SE operations and software
testing, while exposing significant gaps across other SE activities.
Additionally, the SLR uncovers relevant works published outside traditional SE
venues, underscoring the necessity of this comprehensive review. Overall, our
study provides a broad overview of the research landscape, empowering the SBSE
community to leverage quantum advancements in addressing next-generation SE
challenges.

</details>


### [15] [Identifying Explanation Needs: Towards a Catalog of User-based Indicators](https://arxiv.org/abs/2506.16997)
*Hannah Deters,Laura Reinhardt,Jakob Droste,Martin Obaidi,Kurt Schneider*

Main category: cs.SE

TL;DR: 论文探讨了在复杂软件系统中如何通过用户行为和系统事件指标动态触发解释需求，以减少偏见并提升解释的针对性。


<details>
  <summary>Details</summary>
Motivation: 在数字化世界中，软件系统的复杂性增加，解释性需求日益重要，但传统方法易受偏见影响。

Method: 通过在线研究收集用户自我报告的行为、系统事件及情感状态指标，分析其与解释需求的关系。

Result: 整理出17种用户行为指标、8种系统事件指标和14种情感或生理反应指标，并分析了它们与解释需求的关联。

Conclusion: 这些指标可用于原型设计或已部署系统的需求收集，动态触发解释，提升用户体验。

Abstract: In today's digitalized world, where software systems are becoming
increasingly ubiquitous and complex, the quality aspect of explainability is
gaining relevance. A major challenge in achieving adequate explanations is the
elicitation of individual explanation needs, as it may be subject to severe
hypothetical or confirmation biases. To address these challenges, we aim to
establish user-based indicators concerning user behavior or system events that
can be captured at runtime to determine when a need for explanations arises. In
this work, we conducted explorative research in form of an online study to
collect self-reported indicators that could indicate a need for explanation. We
compiled a catalog containing 17 relevant indicators concerning user behavior,
8 indicators concerning system events and 14 indicators concerning emotional
states or physical reactions. We also analyze the relationships between these
indicators and different types of need for explanation. The established
indicators can be used in the elicitation process through prototypes, as well
as after publication to gather requirements from already deployed applications
using telemetry and usage data. Moreover, these indicators can be used to
trigger explanations at appropriate moments during the runtime.

</details>


### [16] [Behavior Driven Development for 3D Games](https://arxiv.org/abs/2506.17057)
*Fernando Pastor Ricós,Beatriz Marín,I. S. W. B. Prasetya,Tanja E. J. Vos,Joseph Davidson,Karel Hovorka*

Main category: cs.SE

TL;DR: iv4XR框架通过集成行为驱动开发（BDD）方法，简化了复杂3D游戏的自动化测试，提升了开发与测试的协作效率。


<details>
  <summary>Details</summary>
Motivation: 解决3D游戏测试中因技术门槛高导致的开发与测试协作不畅问题。

Method: 将BDD方法与iv4XR框架结合，实现自动化回归测试和长时游戏场景测试。

Result: 成功应用于Space Engineers和LabRecruits游戏测试，展示了框架的多样性和BDD的实用性。

Conclusion: iv4XR框架结合BDD方法显著提升了游戏测试的自动化效率和可读性。

Abstract: Computer 3D games are complex software environments that require novel
testing processes to ensure high-quality standards. The Intelligent
Verification/Validation for Extended Reality Based Systems (iv4XR) framework
addresses this need by enabling the implementation of autonomous agents to
automate game testing scenarios. This framework facilitates the automation of
regression test cases for complex 3D games like Space Engineers. Nevertheless,
the technical expertise required to define test scripts using iv4XR can
constrain seamless collaboration between developers and testers. This paper
reports how integrating a Behavior-driven Development (BDD) approach with the
iv4XR framework allows the industrial company behind Space Engineers to
automate regression testing. The success of this industrial collaboration has
inspired the iv4XR team to integrate the BDD approach to improve the automation
of play-testing for the experimental 3D game LabRecruits. Furthermore, the
iv4XR framework has been extended with tactical programming to enable the
automation of long-play test scenarios in Space Engineers. These results
underscore the versatility of the iv4XR framework in supporting diverse testing
approaches while showcasing how BDD empowers users to create, manage, and
execute automated game tests using comprehensive and human-readable statements.

</details>


### [17] [Software Fairness Testing in Practice](https://arxiv.org/abs/2506.17095)
*Ronnie de Souza Santos,Matheus de Morais Leca,Reydne Santos,Cleyton Magalhaes*

Main category: cs.SE

TL;DR: 论文探讨了AI系统公平性测试的理论与实践差距，通过访谈22位从业者发现，尽管学术研究丰富，但行业缺乏实用工具和指南。


<details>
  <summary>Details</summary>
Motivation: 随着AI技术融入软件系统，公平性测试成为确保伦理和公正结果的关键，但行业实践与理论研究存在脱节。

Method: 通过访谈22位从事AI和ML项目的从业者，研究他们在实际开发中如何测试系统公平性。

Result: 研究发现公平性定义难以应用，行业缺乏适用的测试工具，主要挑战包括数据质量、时间限制和指标定义。

Conclusion: 需将学术进展转化为实用工具和策略，以帮助从业者系统性解决AI系统的公平性问题。

Abstract: Software testing ensures that a system functions correctly, meets specified
requirements, and maintains high quality. As artificial intelligence and
machine learning (ML) technologies become integral to software systems, testing
has evolved to address their unique complexities. A critical advancement in
this space is fairness testing, which identifies and mitigates biases in AI
applications to promote ethical and equitable outcomes. Despite extensive
academic research on fairness testing, including test input generation, test
oracle identification, and component testing, practical adoption remains
limited. Industry practitioners often lack clear guidelines and effective tools
to integrate fairness testing into real-world AI development. This study
investigates how software professionals test AI-powered systems for fairness
through interviews with 22 practitioners working on AI and ML projects. Our
findings highlight a significant gap between theoretical fairness concepts and
industry practice. While fairness definitions continue to evolve, they remain
difficult for practitioners to interpret and apply. The absence of
industry-aligned fairness testing tools further complicates adoption,
necessitating research into practical, accessible solutions. Key challenges
include data quality and diversity, time constraints, defining effective
metrics, and ensuring model interoperability. These insights emphasize the need
to bridge academic advancements with actionable strategies and tools, enabling
practitioners to systematically address fairness in AI systems.

</details>


### [18] [Reassessing Code Authorship Attribution in the Era of Language Models](https://arxiv.org/abs/2506.17120)
*Atish Kumar Dipongkor,Ziyu Yao,Kevin Moran*

Main category: cs.SE

TL;DR: 该论文研究了代码风格计量学中的代码作者归属问题（CAA），探讨了传统方法的局限性，并首次系统评估了基于Transformer的语言模型在CAA任务中的表现。


<details>
  <summary>Details</summary>
Motivation: CAA在网络安全和软件取证中至关重要，但传统方法依赖手工特征且易受对抗性扰动影响，因此需要探索更有效的方法。

Method: 研究应用了两种大型和五种小型代码语言模型，对包含463名开发者的12k代码片段进行了CAA任务评估，并进行了模型性能的深入分析。

Result: 研究揭示了语言模型在理解代码风格模式时的行为特点，为未来工作提供了重要方向。

Conclusion: 基于Transformer的语言模型在CAA任务中表现出潜力，但仍需进一步研究以优化其性能。

Abstract: The study of Code Stylometry, and in particular Code Authorship Attribution
(CAA), aims to analyze coding styles to identify the authors of code samples.
CAA is crucial in cybersecurity and software forensics for addressing,
detecting plagiarism, and supporting criminal prosecutions. However, CAA is a
complex and error prone task, due to the need for recognizing nuanced
relationships between coding patterns. This challenge is compounded in large
software systems with numerous authors due to the subtle variability of
patterns that signify the coding style of one author among many. Given the
challenges related to this task, researchers have proposed and studied
automated approaches that rely upon classical Machine Learning and Deep
Learning techniques. However, such techniques have historically relied upon
hand-crafted features, and due to the often intricate interaction of different
features (e.g., formatting, etc.), have key limitations in properly
characterizing authorship, and are sensitive to adversarial code perturbations.
Recently, transformer-based Language Models (LMs) have shown remarkable
efficacy across a range of software engineering tasks, and in the authorship
attribution on natural language in the NLP domain. However, their effectiveness
in CAA is not well understood. As such, we conduct the first extensive
empirical study applying two larger state-of-the-art code LMs, and five smaller
code LMs to the task of CAA to 6 diverse datasets that encompass 12k code
snippets written by 463 developers. Furthermore, we perform an in-depth
analysis of our studied models' performance on CAA using established machine
learning interpretability techniques. The results of our analysis illustrate
important findings that illuminate the behavior of LMs in understanding
stylometric code patterns during the task of CAA, and point towards important
directions for future work.

</details>


### [19] [Large Language Model Unlearning for Source Code](https://arxiv.org/abs/2506.17125)
*Xue Jiang,Yihong Dong,Zheng Fang,Yingwei Ma,Tangxinyu Wang,Rongyu Cao,Binhua Li,Zhi Jin,Wenpin Jiao,Yongbin Li,Ge Li*

Main category: cs.SE

TL;DR: PROD是一种新的LLM遗忘方法，专注于源代码领域，能够在遗忘不良代码内容的同时保留模型的代码生成能力。


<details>
  <summary>Details</summary>
Motivation: LLM在源代码领域的潜在记忆敏感或过时数据可能引发法律合规、软件安全和代码质量问题，现有遗忘方法在源代码上效果不佳。

Method: PROD通过抑制遗忘数据在输出分布中的概率并促进候选分布组件，实现同时遗忘特定内容和保留通用能力。

Result: PROD在三个下游任务（版权代码、不安全代码和废弃API遗忘）中表现优于现有方法，平衡遗忘质量和模型实用性。

Conclusion: PROD不仅扩展了遗忘技术在源代码领域的应用，还为可靠代码生成提供了重要支持。

Abstract: LLM4SE has demonstrated significant success, but LLMs' potential memorization
of sensitive or outdated training data introduces critical risks to legal
compliance, software security, and code quality. LLM unlearning techniques,
which can eliminate the influence of undesired data from LLMs in a
post-training way, present a promising solution to address these concerns.
While recent efforts in LLM unlearning show effectiveness in natural language,
their applicability to source code remains underexplored. Our empirical study
reveals that existing LLM unlearning approaches, when applied to source code,
cause severe model utility degradation, rendering models practically unusable
for code generation. In this paper, we propose PROD, a novel unlearning
approach that enables LLMs to forget undesired code content while effectively
preserving their code generation capabilities. PROD suppresses the probability
of forget data in LLMs' output distribution while promoting candidate
distributional components, enabling the model to jointly learn to forget
specific content and retain its general capabilities. To facilitate this study,
we establish a benchmark for code unlearning evaluation, which includes three
critical downstream tasks: copyrighted code unlearning, insecure code
unlearning, and deprecated API unlearning. Our evaluation demonstrates that
PROD achieves superior balance between forget quality and model utility
compared to existing unlearning approaches across three downstream tasks, while
consistently exhibiting improvements when applied to LLMs of varying series.
PROD also exhibits superior robustness against adversarial attacks without
generating or exposing the data to be forgotten. The results underscore that
our approach not only extends the application boundary of unlearning techniques
to source code, but also holds significant implications for advancing reliable
code generation.

</details>


### [20] [Dissecting the SWE-Bench Leaderboards: Profiling Submitters and Architectures of LLM- and Agent-Based Repair Systems](https://arxiv.org/abs/2506.17208)
*Matias Martinez,Xavier Franch*

Main category: cs.SE

TL;DR: 本文分析了SWE-Bench Lite和Verified排行榜上的所有提交，揭示了专有LLM（如Claude 3.5/3.7）的主导地位，以及从个体开发者到大型科技公司的贡献者多样性。


<details>
  <summary>Details</summary>
Motivation: 由于SWE-Bench提交过程缺乏详细文档，许多解决方案的设计和来源不明确，因此需要全面研究这些提交。

Method: 分析了68个SWE-Bench Lite和79个Verified的提交，涵盖提交者类型、产品可用性、LLM使用和系统架构等维度。

Result: 研究发现专有LLM（尤其是Claude 3.5/3.7）占主导地位，同时存在代理和非代理设计，贡献者包括个体开发者和大型科技公司。

Conclusion: 该研究为APR领域的进展提供了重要见解，揭示了当前技术和贡献者的多样性。

Abstract: The rapid progress in Automated Program Repair (APR) has been driven by
advances in AI, particularly large language models (LLMs) and agent-based
systems. SWE-Bench is a recent benchmark designed to evaluate LLM-based repair
systems using real issues and pull requests mined from 12 popular open-source
Python repositories. Its public leaderboards, SWE-Bench Lite and SWE-Bench
Verified, have become central platforms for tracking progress and comparing
solutions. However, because the submission process does not require detailed
documentation, the architectural design and origin of many solutions remain
unclear. In this paper, we present the first comprehensive study of all
submissions to the SWE-Bench Lite (68 entries) and Verified (79 entries)
leaderboards, analyzing 67 unique approaches across dimensions such as
submitter type, product availability, LLM usage, and system architecture. Our
findings reveal the dominance of proprietary LLMs (especially Claude 3.5/3.7),
the presence of both agentic and non-agentic designs, and a contributor base
spanning from individual developers to large tech companies.

</details>


### [21] [cAST: Enhancing Code Retrieval-Augmented Generation with Structural Chunking via Abstract Syntax Tree](https://arxiv.org/abs/2506.15655)
*Yilin Zhang,Xinran Zhao,Zora Zhiruo Wang,Chenyang Yang,Jiayi Wei,Tongshuang Wu*

Main category: cs.SE

TL;DR: 论文提出了一种基于抽象语法树（AST）的代码分块方法（\ourwork），以解决现有行基分块方法破坏语义结构的问题，显著提升了代码生成任务的表现。


<details>
  <summary>Details</summary>
Motivation: 现有行基分块方法在代码检索增强生成（RAG）中常破坏语义结构，影响生成质量，因此需要一种结构感知的分块方法。

Method: 通过递归分解大型AST节点并合并兄弟节点，生成语义连贯且自包含的代码块。

Result: 在RepoEval检索任务中Recall@5提升4.3点，在SWE-bench生成任务中Pass@1提升2.67点。

Conclusion: 结构感知的分块方法对扩展检索增强的代码智能至关重要。

Abstract: Retrieval-Augmented Generation (RAG) has become essential for large-scale
code generation, grounding predictions in external code corpora to improve
actuality. However, a critical yet underexplored aspect of RAG pipelines is
chunking -- the process of dividing documents into retrievable units. Existing
line-based chunking heuristics often break semantic structures, splitting
functions or merging unrelated code, which can degrade generation quality. We
propose chunking via Abstract Syntax Trees (\ourwork), a structure-aware method
that recursively breaks large AST nodes into smaller chunks and merges sibling
nodes while respecting size limits. This approach generates self-contained,
semantically coherent units across programming languages and tasks, improving
performance on diverse code generation tasks, e.g., boosting Recall@5 by 4.3
points on RepoEval retrieval and Pass@1 by 2.67 points on SWE-bench generation.
Our work highlights the importance of structure-aware chunking for scaling
retrieval-enhanced code intelligence.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [22] [ETrace:Event-Driven Vulnerability Detection in Smart Contracts via LLM-Based Trace Analysis](https://arxiv.org/abs/2506.15790)
*Chenyang Peng,Haijun Wang,Yin Wu,Hao Wu,Ming Fan,Yitao Zhao,Ting Liu*

Main category: cs.CR

TL;DR: ETrace是一种基于事件驱动的智能合约漏洞检测框架，通过LLM支持的跟踪分析，无需源代码即可识别潜在漏洞。


<details>
  <summary>Details</summary>
Motivation: 随着区块链技术的广泛应用，智能合约的安全性和稳定性成为关键挑战，而传统漏洞检测方法依赖源代码，但并非所有合约都提供可访问的代码。

Method: ETrace通过从交易日志中提取细粒度事件序列，利用大型语言模型（LLMs）作为自适应语义解释器，通过链式思维推理重建事件分析，并使用模式匹配建立交易行为模式与已知攻击行为之间的因果关系。

Result: 初步实验结果验证了ETrace的有效性。

Conclusion: ETrace为智能合约漏洞检测提供了一种无需源代码的新方法，具有潜在的应用价值。

Abstract: With the advance application of blockchain technology in various fields,
ensuring the security and stability of smart contracts has emerged as a
critical challenge. Current security analysis methodologies in vulnerability
detection can be categorized into static analysis and dynamic analysis
methods.However, these existing traditional vulnerability detection methods
predominantly rely on analyzing original contract code, not all smart contracts
provide accessible code.We present ETrace, a novel event-driven vulnerability
detection framework for smart contracts, which uniquely identifies potential
vulnerabilities through LLM-powered trace analysis without requiring source
code access. By extracting fine-grained event sequences from transaction logs,
the framework leverages Large Language Models (LLMs) as adaptive semantic
interpreters to reconstruct event analysis through chain-of-thought reasoning.
ETrace implements pattern-matching to establish causal links between
transaction behavior patterns and known attack behaviors. Furthermore, we
validate the effectiveness of ETrace through preliminary experimental results.

</details>


### [23] [A Sea of Cyber Threats: Maritime Cybersecurity from the Perspective of Mariners](https://arxiv.org/abs/2506.15842)
*Anna Raymaker,Akshaya Kumar,Miuyin Yong Wong,Ryan Pickren,Animesh Chhotaray,Frank Li,Saman Zonouz,Raheem Beyah*

Main category: cs.CR

TL;DR: 本文探讨了海事系统面临的网络安全威胁，通过用户研究揭示了船员对威胁的感知及应对挑战，并提出了改进建议。


<details>
  <summary>Details</summary>
Motivation: 海事系统是全球基础设施的关键部分，但面临日益严重的网络安全威胁，而相关研究却不足。

Method: 通过问卷调查和半结构化访谈，研究了21名高级船员对网络安全挑战的感知和应对。

Result: 研究发现船员面临GPS欺骗和勒索软件等威胁，揭示了培训不足、工具缺乏和认知差距等问题。

Conclusion: 提出了改进培训、响应协议和法规的建议，以增强海事系统的韧性。

Abstract: Maritime systems, including ships and ports, are critical components of
global infrastructure, essential for transporting over 80% of the world's goods
and supporting internet connectivity. However, these systems face growing
cybersecurity threats, as shown by recent attacks disrupting Maersk, one of the
world's largest shipping companies, causing widespread impacts on international
trade. The unique challenges of the maritime environment--such as diverse
operational conditions, extensive physical access points, fragmented regulatory
frameworks, and its deeply interconnected structure--require maritime-specific
cybersecurity research. Despite the sector's importance, maritime cybersecurity
remains underexplored, leaving significant gaps in understanding its challenges
and risks.
  To address these gaps, we investigate how maritime system operators perceive
and navigate cybersecurity challenges within this complex landscape. We
conducted a user study comprising surveys and semi-structured interviews with
21 officer-level mariners. Participants reported direct experiences with
shipboard cyber-attacks, including GPS spoofing and logistics-disrupting
ransomware, demonstrating the real-world impact of these threats. Our findings
reveal systemic and human-centric issues, such as training poorly aligned with
maritime needs, insufficient detection and response tools, and serious gaps in
mariners' cybersecurity understanding. Our contributions include a
categorization of threats identified by mariners and recommendations for
improving maritime security, including better training, response protocols, and
regulation. These insights aim to guide future research and policy to
strengthen the resilience of maritime systems.

</details>


### [24] [Sudoku: Decomposing DRAM Address Mapping into Component Functions](https://arxiv.org/abs/2506.15918)
*Minbok Wi,Seungmin Baek,Seonyong Park,Mattan Erez,Jung Ho Ahn*

Main category: cs.CR

TL;DR: 论文提出了一种基于时序的新方法，通过DRAM刷新间隔和连续访问延迟推断地址映射的组件功能，并开发了工具Sudoku，首次实现软件自动分解DRAM地址映射。


<details>
  <summary>Details</summary>
Motivation: 现有方法无法有效分解DRAM地址映射，限制了内存行为理解和RowHammer攻击的精确性。

Method: 利用DRAM刷新间隔和连续访问延迟的时序技术，推断地址映射的组件功能，开发工具Sudoku自动分解映射。

Result: Sudoku成功分解了近期Intel和AMD处理器的DRAM地址映射。

Conclusion: 新方法和工具Sudoku为DRAM地址映射分解提供了有效解决方案，推动了内存行为研究和安全攻击的精确性。

Abstract: Decomposing DRAM address mappings into component-level functions is critical
for understanding memory behavior and enabling precise RowHammer attacks, yet
existing reverse-engineering methods fall short. We introduce novel
timing-based techniques leveraging DRAM refresh intervals and consecutive
access latencies to infer component-specific functions. Based on this, we
present Sudoku, the first software-based tool to automatically decompose full
DRAM address mappings into channel, rank, bank group, and bank functions while
identifying row and column bits. We validate Sudoku's effectiveness,
successfully decomposing mappings on recent Intel and AMD processors.

</details>


### [25] [FARFETCH'D: A Side-Channel Analysis Framework for Privacy Applications on Confidential Virtual Machines](https://arxiv.org/abs/2506.15924)
*Ruiyi Zhang,Albert Cheu,Adria Gascon,Daniel Moghimi,Phillipp Schoppmann,Michael Schwarz,Octavian Suciu*

Main category: cs.CR

TL;DR: FARFETCH'D是一个开源工具包，用于在AMD SEV-SNP硬件上配置侧信道追踪原语，并结合统计和机器学习分析管道，自动估计泄漏。


<details>
  <summary>Details</summary>
Motivation: 当前基于TEE的机密虚拟机（CVMs）未将侧信道泄漏纳入威胁模型，开发者缺乏系统且高效的方法来测量和比较泄漏。

Method: FARFETCH'D提供可配置的侧信道追踪原语，并结合统计和机器学习分析管道。

Result: 在三个典型工作负载中发现了未注意到的泄漏，包括一个以497 kbit/s速度传输数据的隐蔽信道。

Conclusion: FARFETCH'D能精确定位漏洞并指导低开销的缓解措施，为CVMs的实际部署提供了实用的保密保障路径。

Abstract: Confidential virtual machines (CVMs) based on trusted execution environments
(TEEs) enable new privacy-preserving solutions. Yet, they leave side-channel
leakage outside their threat model, shifting the responsibility of mitigating
such attacks to developers. However, mitigations are either not generic or too
slow for practical use, and developers currently lack a systematic, efficient
way to measure and compare leakage across real-world deployments. In this
paper, we present FARFETCH'D, an open-source toolkit that offers configurable
side-channel tracing primitives on production AMD SEV-SNP hardware and couples
them with statistical and machine-learning-based analysis pipelines for
automated leakage estimation. We apply FARFETCH'D to three representative
workloads that are deployed on CVMs to enhance user privacy - private
information retrieval, private heavy hitters, and Wasm user-defined functions -
and uncover previously unnoticed leaks, including a covert channel that
exfiltrated data at 497 kbit/s. The results show that FARFETCH'D pinpoints
vulnerabilities and guides low-overhead mitigations based on oblivious memory
and differential privacy, giving practitioners a practical path to deploy CVMs
with meaningful confidentiality guarantees.

</details>


### [26] [Multi-use LLM Watermarking and the False Detection Problem](https://arxiv.org/abs/2506.15975)
*Zihao Fu,Chris Russell*

Main category: cs.CR

TL;DR: 论文提出双水印技术，解决传统水印方法在检测和用户识别同时使用时导致的误检问题。


<details>
  <summary>Details</summary>
Motivation: 传统数字水印方法在同时用于检测和用户识别时，随着用户容量增加，未加水印的文本容易被误检为加水印。

Method: 通过理论分析，提出双水印技术，联合编码检测和识别水印。

Result: 实验验证了理论分析，双水印显著减少误检率，同时保持高检测准确率。

Conclusion: 双水印技术有效解决了误检问题，提升了水印的实用性。

Abstract: Digital watermarking is a promising solution for mitigating some of the risks
arising from the misuse of automatically generated text. These approaches
either embed non-specific watermarks to allow for the detection of any text
generated by a particular sampler, or embed specific keys that allow the
identification of the LLM user. However, simultaneously using the same
embedding for both detection and user identification leads to a false detection
problem, whereby, as user capacity grows, unwatermarked text is increasingly
likely to be falsely detected as watermarked. Through theoretical analysis, we
identify the underlying causes of this phenomenon. Building on these insights,
we propose Dual Watermarking which jointly encodes detection and identification
watermarks into generated text, significantly reducing false positives while
maintaining high detection accuracy. Our experimental results validate our
theoretical findings and demonstrate the effectiveness of our approach.

</details>


### [27] [Efficient Blockchain-based Steganography via Backcalculating Generative Adversarial Network](https://arxiv.org/abs/2506.16023)
*Zhuo Chen,Jialing He,Jiacheng Wang,Zehui Xiong,Tao Xiang,Liehuang Zhu,Dusit Niyato*

Main category: cs.CR

TL;DR: 本文提出了一种通用的基于区块链的隐写框架（GBSF），并通过可逆生成对抗网络（R-GAN）及其改进版本CCR-GAN，增强了隐写容量和性能。实验验证了其在比特币主网及其他区块链上的可行性和扩展性。


<details>
  <summary>Details</summary>
Motivation: 现有区块链隐写方法仅关注特定字段嵌入，而忽略了字段生成嵌入的需求，导致容量受限。

Method: 提出GBSF框架，设计R-GAN和CCR-GAN，利用可逆生成器和自定义预处理（CIDP）及激活函数（ClipSigmoid）优化性能。

Result: R-GAN和CCR-GAN显著提升了隐写容量，优于现有方法，并在多区块链中验证了扩展性。

Conclusion: GBSF及其衍生方法为区块链隐写提供了高效、可扩展的解决方案，平衡了容量与隐蔽性。

Abstract: Blockchain-based steganography enables data hiding via encoding the covert
data into a specific blockchain transaction field. However, previous works
focus on the specific field-embedding methods while lacking a consideration on
required field-generation embedding. In this paper, we propose a generic
blockchain-based steganography framework (GBSF). The sender generates the
required fields such as amount and fees, where the additional covert data is
embedded to enhance the channel capacity. Based on GBSF, we design a reversible
generative adversarial network (R-GAN) that utilizes the generative adversarial
network with a reversible generator to generate the required fields and encode
additional covert data into the input noise of the reversible generator. We
then explore the performance flaw of R-GAN. To further improve the performance,
we propose R-GAN with Counter-intuitive data preprocessing and Custom
activation functions, namely CCR-GAN. The counter-intuitive data preprocessing
(CIDP) mechanism is used to reduce decoding errors in covert data, while it
incurs gradient explosion for model convergence. The custom activation function
named ClipSigmoid is devised to overcome the problem. Theoretical justification
for CIDP and ClipSigmoid is also provided. We also develop a mechanism named
T2C, which balances capacity and concealment. We conduct experiments using the
transaction amount of the Bitcoin mainnet as the required field to verify the
feasibility. We then apply the proposed schemes to other transaction fields and
blockchains to demonstrate the scalability. Finally, we evaluate capacity and
concealment for various blockchains and transaction fields and explore the
trade-off between capacity and concealment. The results demonstrate that R-GAN
and CCR-GAN are able to enhance the channel capacity effectively and outperform
state-of-the-art works.

</details>


### [28] [PRISON: Unmasking the Criminal Potential of Large Language Models](https://arxiv.org/abs/2506.16150)
*Xinyi Wu,Geng Hong,Pei Chen,Yueyue Chen,Xudong Pan,Min Yang*

Main category: cs.CR

TL;DR: 论文提出PRISON框架，量化大语言模型（LLMs）在五种犯罪维度上的潜力，发现即使无明确指令，LLMs仍表现出犯罪倾向，且其识别欺骗行为的能力较差。


<details>
  <summary>Details</summary>
Motivation: 随着LLMs的发展，其在复杂社会环境中可能的不当行为引发担忧，现有研究缺乏对其犯罪潜力的系统性评估。

Method: 采用PRISON框架，通过角色扮演评估LLMs在五种犯罪维度上的潜力及反犯罪能力。

Result: LLMs常表现出犯罪倾向，如误导性陈述或逃避策略；作为侦探角色时，识别欺骗行为的准确率仅为41%。

Conclusion: 研究强调在广泛部署LLMs前，需加强对抗鲁棒性、行为对齐和安全机制。

Abstract: As large language models (LLMs) advance, concerns about their misconduct in
complex social contexts intensify. Existing research overlooked the systematic
understanding and assessment of their criminal capability in realistic
interactions. We propose a unified framework PRISON, to quantify LLMs' criminal
potential across five dimensions: False Statements, Frame-Up, Psychological
Manipulation, Emotional Disguise, and Moral Disengagement. Using structured
crime scenarios adapted from classic films, we evaluate both criminal potential
and anti-crime ability of LLMs via role-play. Results show that
state-of-the-art LLMs frequently exhibit emergent criminal tendencies, such as
proposing misleading statements or evasion tactics, even without explicit
instructions. Moreover, when placed in a detective role, models recognize
deceptive behavior with only 41% accuracy on average, revealing a striking
mismatch between conducting and detecting criminal behavior. These findings
underscore the urgent need for adversarial robustness, behavioral alignment,
and safety mechanisms before broader LLM deployment.

</details>


### [29] [Malware Classification Leveraging NLP & Machine Learning for Enhanced Accuracy](https://arxiv.org/abs/2506.16224)
*Bishwajit Prasad Gond,Rajneekant,Pushkar Kishore,Durga Prasad Mohapatra*

Main category: cs.CR

TL;DR: 该论文研究了基于NLP的n-gram分析和机器学习技术，用于提升恶意软件分类的准确性。


<details>
  <summary>Details</summary>
Motivation: 探索如何利用NLP从恶意软件样本中提取和分析文本特征，以区分恶意软件与良性软件家族。

Method: 采用n-gram分析提取连续字符串或API调用序列，结合混合特征选择技术降低特征维度，并使用多种机器学习算法进行分类。

Result: 在真实恶意软件样本上测试，分类准确率达到99.02%，特征集缩减至原始特征的1.6%。

Conclusion: NLP-based n-gram分析结合混合特征选择技术显著提升了恶意软件分类的准确性。

Abstract: This paper investigates the application of natural language processing
(NLP)-based n-gram analysis and machine learning techniques to enhance malware
classification. We explore how NLP can be used to extract and analyze textual
features from malware samples through n-grams, contiguous string or API call
sequences. This approach effectively captures distinctive linguistic patterns
among malware and benign families, enabling finer-grained classification. We
delve into n-gram size selection, feature representation, and classification
algorithms. While evaluating our proposed method on real-world malware samples,
we observe significantly improved accuracy compared to the traditional methods.
By implementing our n-gram approach, we achieved an accuracy of 99.02% across
various machine learning algorithms by using hybrid feature selection technique
to address high dimensionality. Hybrid feature selection technique reduces the
feature set to only 1.6% of the original features.

</details>


### [30] [Sharpening Kubernetes Audit Logs with Context Awareness](https://arxiv.org/abs/2506.16328)
*Matteo Franzil,Valentino Armani,Luis Augusto Dias Knob,Domenico Siracusa*

Main category: cs.CR

TL;DR: K8NTEXT是一种新方法，通过重构上下文简化Kubernetes审计日志，自动识别、标记和分组相关API调用，显著提升日志管理效率。


<details>
  <summary>Details</summary>
Motivation: Kubernetes审计日志存在数据量大、事件关联性差的问题，难以追踪用户操作的完整上下文。

Method: 结合推理规则和机器学习模型，自动识别并关联相关API调用，重构操作上下文。

Result: K8NTEXT在复杂操作中表现优异，准确率超过95%，适用于简单到高度复合的操作。

Conclusion: K8NTEXT有效解决了Kubernetes审计日志的上下文重构问题，显著提升了日志管理的效率和准确性。

Abstract: Kubernetes has emerged as the de facto orchestrator of microservices,
providing scalability and extensibility to a highly dynamic environment. It
builds an intricate and deeply connected system that requires extensive
monitoring capabilities to be properly managed. To this account, K8s natively
offers audit logs, a powerful feature for tracking API interactions in the
cluster. Audit logs provide a detailed and chronological record of all
activities in the system. Unfortunately, K8s auditing suffers from several
practical limitations: it generates large volumes of data continuously, as all
components within the cluster interact and respond to user actions. Moreover,
each action can trigger a cascade of secondary events dispersed across the log,
with little to no explicit linkage, making it difficult to reconstruct the
context behind user-initiated operations. In this paper, we introduce K8NTEXT,
a novel approach for streamlining K8s audit logs by reconstructing contexts,
i.e., grouping actions performed by actors on the cluster with the subsequent
events these actions cause. Correlated API calls are automatically identified,
labeled, and consistently grouped using a combination of inference rules and a
Machine Learning model, largely simplifying data consumption. We evaluate
K8NTEXT's performance, scalability, and expressiveness both in systematic tests
and with a series of use cases. We show that it consistently provides accurate
context reconstruction, even for complex operations involving 50, 100 or more
correlated actions, achieving over 95 percent accuracy across the entire
spectrum, from simple to highly composite actions.

</details>


### [31] [Emission Impossible: privacy-preserving carbon emissions claims](https://arxiv.org/abs/2506.16347)
*Jessica Man,Sadiq Jaffer,Patrick Ferris,Martin Kleppmann,Anil Madhavapeddy*

Main category: cs.CR

TL;DR: 本文提出了一种基于零知识证明（zk-SNARK）的方法，用于在ICT供应链中实现可验证的碳排放报告，同时保护商业敏感信息。


<details>
  <summary>Details</summary>
Motivation: ICT供应链中的碳排放数据报告目前依赖未经验证的数据，企业缺乏公开详细计算方法的动力，且第三方验证成本高昂。

Method: 采用密码学和零知识证明技术，特别是zk-SNARK协议，设计了一个可验证的碳排放报告系统。

Result: 该系统允许供应链中的各方（如能源供应商、数据中心、云服务提供商和客户）在不泄露敏感信息的情况下验证碳排放数据。

Conclusion: 该方法提高了碳排放数据的透明度和准确性，有助于企业和监管机构实现可持续发展目标。

Abstract: Information and Communication Technologies (ICT) have a significant climate
impact, and data centres account for a large proportion of the carbon emissions
from ICT. To achieve sustainability goals, it is important that all parties
involved in ICT supply chains can track and share accurate carbon emissions
data with their customers, investors, and the authorities. However, businesses
have strong incentives to make their numbers look good, whilst less so to
publish their accounting methods along with all the input data, due to the risk
of revealing sensitive information. It would be uneconomical to use a trusted
third party to verify the data for every report for each party in the chain. As
a result, carbon emissions reporting in supply chains currently relies on
unverified data. This paper proposes a methodology that applies cryptography
and zero-knowledge proofs for carbon emissions claims that can be subsequently
verified without the knowledge of the private input data. The proposed system
is based on a zero-knowledge Succinct Non-interactive ARguments of Knowledge
(zk-SNARK) protocol, which enables verifiable emissions reporting mechanisms
across a chain of energy suppliers, cloud data centres, cloud services
providers, and customers, without any company needing to disclose commercially
sensitive information. This allows customers of cloud services to accurately
account for the emissions generated by their activities, improving data quality
for their own regulatory reporting. Cloud services providers would also be held
accountable for producing accurate carbon emissions data.

</details>


### [32] [Physical-Layer Signal Injection Attacks on EV Charging Ports: Bypassing Authentication via Electrical-Level Exploits](https://arxiv.org/abs/2506.16400)
*Hetian Shi,Yi He,Shangru Song,Jianwei Zhuge,Jian Mao*

Main category: cs.CR

TL;DR: 论文研究了电动汽车充电协议的安全漏洞，发现物理信号欺骗攻击可破坏充电过程，并提出硬件解决方案PORTulator。


<details>
  <summary>Details</summary>
Motivation: 电动汽车充电基础设施的扩展带来了新的安全风险，需研究充电协议的安全性。

Method: 通过插入恶意设备注入虚假信号，设计PORTulator硬件进行攻击验证。

Result: 发现7种充电标准存在漏洞，攻击可导致服务中断或设备损坏。

Conclusion: 建议通过改进认证电路和动态PWM信号增强安全性。

Abstract: The proliferation of electric vehicles in recent years has significantly
expanded the charging infrastructure while introducing new security risks to
both vehicles and chargers. In this paper, we investigate the security of major
charging protocols such as SAE J1772, CCS, IEC 61851, GB/T 20234, and NACS,
uncovering new physical signal spoofing attacks in their authentication
mechanisms. By inserting a compact malicious device into the charger connector,
attackers can inject fraudulent signals to sabotage the charging process,
leading to denial of service, vehicle-induced charger lockout, and damage to
the chargers or the vehicle's charge management system. To demonstrate the
feasibility of our attacks, we propose PORTulator, a proof-of-concept (PoC)
attack hardware, including a charger gun plugin device for injecting physical
signals and a wireless controller for remote manipulation. By evaluating
PORTulator on multiple real-world chargers, we identify 7 charging standards
used by 20 charger piles that are vulnerable to our attacks. The root cause is
that chargers use simple physical signals for authentication and control,
making them easily spoofed by attackers. To address this issue, we propose
enhancing authentication circuits by integrating non-resistive memory
components and utilizing dynamic high-frequency Pulse Width Modulation (PWM)
signals to counter such physical signal spoofing attacks.

</details>


### [33] [Probe before You Talk: Towards Black-box Defense against Backdoor Unalignment for Large Language Models](https://arxiv.org/abs/2506.16447)
*Biao Yi,Tiansheng Huang,Sishuo Chen,Tong Li,Zheli Liu,Zhixuan Chu,Yiming Li*

Main category: cs.CR

TL;DR: 论文提出了一种名为BEAT的黑盒防御方法，用于检测大型语言模型（LLM）中的后门攻击，通过测量输入拼接前后输出分布的变化来识别触发样本。


<details>
  <summary>Details</summary>
Motivation: 后门攻击能够隐秘地破坏LLM的安全对齐，且在样本依赖的目标下威胁更大。BEAT旨在通过检测触发样本来消除后门，保护LLM在LLMaaS环境中的安全。

Method: BEAT利用拼接触发样本会显著降低LLM对恶意探针的拒绝率这一现象，通过测量输入拼接前后输出分布的扭曲程度来识别触发样本。

Result: 实验表明，BEAT在多种后门攻击和LLM（包括闭源的GPT-3.5-turbo）上均有效且高效，并能初步防御流行的越狱攻击。

Conclusion: BEAT为黑盒环境下的LLM后门攻击提供了一种有效的防御方法，且对样本依赖的目标具有普适性。

Abstract: Backdoor unalignment attacks against Large Language Models (LLMs) enable the
stealthy compromise of safety alignment using a hidden trigger while evading
normal safety auditing. These attacks pose significant threats to the
applications of LLMs in the real-world Large Language Model as a Service
(LLMaaS) setting, where the deployed model is a fully black-box system that can
only interact through text. Furthermore, the sample-dependent nature of the
attack target exacerbates the threat. Instead of outputting a fixed label, the
backdoored LLM follows the semantics of any malicious command with the hidden
trigger, significantly expanding the target space. In this paper, we introduce
BEAT, a black-box defense that detects triggered samples during inference to
deactivate the backdoor. It is motivated by an intriguing observation (dubbed
the probe concatenate effect), where concatenated triggered samples
significantly reduce the refusal rate of the backdoored LLM towards a malicious
probe, while non-triggered samples have little effect. Specifically, BEAT
identifies whether an input is triggered by measuring the degree of distortion
in the output distribution of the probe before and after concatenation with the
input. Our method addresses the challenges of sample-dependent targets from an
opposite perspective. It captures the impact of the trigger on the refusal
signal (which is sample-independent) instead of sample-specific successful
attack behaviors. It overcomes black-box access limitations by using multiple
sampling to approximate the output distribution. Extensive experiments are
conducted on various backdoor attacks and LLMs (including the closed-source
GPT-3.5-turbo), verifying the effectiveness and efficiency of our defense.
Besides, we also preliminarily verify that BEAT can effectively defend against
popular jailbreak attacks, as they can be regarded as 'natural backdoors'.

</details>


### [34] [SecureFed: A Two-Phase Framework for Detecting Malicious Clients in Federated Learning](https://arxiv.org/abs/2506.16458)
*Likhitha Annapurna Kavuri,Akshay Mhatre,Akarsh K Nair,Deepti Gupta*

Main category: cs.CR

TL;DR: SecureFed是一个两阶段联邦学习框架，通过异常检测和动态权重路由识别并减少对抗性客户端的影响，提升模型抗攻击能力。


<details>
  <summary>Details</summary>
Motivation: 联邦学习（FL）在保护数据隐私的同时，容易受到对抗性客户端的攻击，影响模型性能。SecureFed旨在解决这一问题。

Method: 第一阶段通过降维和异常评分识别恶意行为；第二阶段引入学习区域，动态调整权重，高价值梯度区域优先聚合。

Result: 实验表明，SecureFed显著提升了模型抗攻击能力，同时不影响模型性能。

Conclusion: SecureFed为联邦学习提供了一种有效的防御机制，增强了模型的安全性和鲁棒性。

Abstract: Federated Learning (FL) protects data privacy while providing a decentralized
method for training models. However, because of the distributed schema, it is
susceptible to adversarial clients that could alter results or sabotage model
performance. This study presents SecureFed, a two-phase FL framework for
identifying and reducing the impact of such attackers. Phase 1 involves
collecting model updates from participating clients and applying a
dimensionality reduction approach to identify outlier patterns frequently
associated with malicious behavior. Temporary models constructed from the
client updates are evaluated on synthetic datasets to compute validation losses
and support anomaly scoring. The idea of learning zones is presented in Phase
2, where weights are dynamically routed according to their contribution scores
and gradient magnitudes. High-value gradient zones are given greater weight in
aggregation and contribute more significantly to the global model, while
lower-value gradient zones, which may indicate possible adversarial activity,
are gradually removed from training. Until the model converges and a strong
defense against poisoning attacks is possible, this training cycle continues
Based on the experimental findings, SecureFed considerably improves model
resilience without compromising model performance.

</details>


### [35] [SAFER-D: A Self-Adaptive Security Framework for Distributed Computing Architectures](https://arxiv.org/abs/2506.16545)
*Marco Stadler,Michael Vierhauser,Michael Riegler,Daniel Waghubinger,Johannes Sametinger*

Main category: cs.CR

TL;DR: 本文提出了一种自适应的安全框架，以应对分布式计算架构中的安全挑战，并通过实际用例验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 随着物联网和网络物理系统的兴起，网络复杂性和攻击面增加，传统安全措施难以应对新型攻击，需要创新的解决方案。

Method: 提出了一种结合多种自适应策略的全面安全框架，并在实际场景中进行了应用和评估。

Result: 评估结果显示该框架具有高效性和广泛适用性，为进一步研究提供了潜力。

Conclusion: 该自适应的安全框架为分布式计算架构的安全问题提供了有效的解决方案，并展示了进一步扩展的可能性。

Abstract: The rise of the Internet of Things and Cyber-Physical Systems has introduced
new challenges on ensuring secure and robust communication. The growing number
of connected devices increases network complexity, leading to higher latency
and traffic. Distributed computing architectures (DCAs) have gained prominence
to address these issues. This shift has significantly expanded the attack
surface, requiring additional security measures to protect all components --
from sensors and actuators to edge nodes and central servers. Recent incidents
highlight the difficulty of this task: Cyberattacks, like distributed denial of
service attacks, continue to pose severe threats and cause substantial damage.
Implementing a holistic defense mechanism remains an open challenge,
particularly against attacks that demand both enhanced resilience and rapid
response. Addressing this gap requires innovative solutions to enhance the
security of DCAs. In this work, we present our holistic self-adaptive security
framework which combines different adaptation strategies to create
comprehensive and efficient defense mechanisms. We describe how to incorporate
the framework into a real-world use case scenario and further evaluate its
applicability and efficiency. Our evaluation yields promising results,
indicating great potential to further extend the research on our framework.

</details>


### [36] [Centre driven Controlled Evolution of Wireless Virtual Networks based on Broadcast Tokens](https://arxiv.org/abs/2506.16615)
*Vignesh Babu,Atishay Jain,Kannan Karthik*

Main category: cs.CR

TL;DR: 论文提出了一种通过广播令牌控制无线传感器网络中节点虚拟连接性的分布式密钥生成框架和算法，以创建并行虚拟多播组。


<details>
  <summary>Details</summary>
Motivation: 解决预嵌入密钥配置导致网络不灵活的问题，同时避免完全分布式密钥合成带来的信息流与控制中心解耦和隐藏的问题。

Method: 采用中心驱动的密钥生成过程，通过广播令牌在不同节点中提取不同密钥，并设计节点共享和广播令牌以平衡多播组的范围。

Result: 实现了分布式密钥释放，控制网络虚拟连接性的演化，并成功创建并行虚拟多播组。

Conclusion: 提出的框架和算法有效平衡了网络灵活性与控制性，适用于动态无线传感器网络环境。

Abstract: In a wireless sensor network, the virtual connectivity between nodes is a
function of the keys shared between various nodes. Pre-embedding these key
configurations in the nodes would make the network inflexible. On the other
hand, permitting subsets of nodes to engage in a common key synthesis phase to
create secure distributed connections amongst themselves, would decouple and
conceal the information flow from the controlling centre. An intermediate
solution is the notion of a centre driven key generation process through
broadcast tokens, designed to extract different keys in different nodes based
on some prior information stored at the nodes. As more tokens arrive, the
virtual connectivity of the nodes are altered and the network evolves. This
evolution can be distributed and can be controlled to converge to a certain
specific connectivity profile. In this paper we present a framework and an
algorithm which controls the simultaneous and distributed key release in
different nodes, resulting in the creation of parallel virtual multicast
groups. The design of the node shares and the supporting broadcast tokens have
been discussed in conjunction with the process of balancing the spans of
individual groups with spans of several coexistent multicast groups.

</details>


### [37] [Few-Shot Learning-Based Cyber Incident Detection with Augmented Context Intelligence](https://arxiv.org/abs/2506.16626)
*Fei Zuo,Junghwan Rhee,Yung Ryn Choe,Chenglong Fu,Xianshan Qu*

Main category: cs.CR

TL;DR: 本文提出了一种基于小样本学习的攻击检测方法，结合改进的数据上下文智能，用于云环境中的威胁检测。


<details>
  <summary>Details</summary>
Motivation: 随着云服务的广泛采用，网络安全事件（如数据泄露）不断增加，传统安全措施难以应对日益复杂的威胁（如APT）。系统溯源分析被认为是一种有前景的机制。

Method: 收集云系统在真实攻击中的行为数据，采用创新的符号学提取方法描述系统事件，并将异常检测问题转化为相似性比较问题。

Result: 实验表明，该方法能够泛化到未见过的攻击，并在训练样本有限的情况下做出准确预测。

Conclusion: 提出的方法在云安全领域具有潜力，能够有效应对复杂和隐蔽的威胁。

Abstract: In recent years, the adoption of cloud services has been expanding at an
unprecedented rate. As more and more organizations migrate or deploy their
businesses to the cloud, a multitude of related cybersecurity incidents such as
data breaches are on the rise. Many inherent attributes of cloud environments,
for example, data sharing, remote access, dynamicity and scalability, pose
significant challenges for the protection of cloud security. Even worse, cyber
threats are becoming increasingly sophisticated and covert. Attack methods,
such as Advanced Persistent Threats (APTs), are continually developed to bypass
traditional security measures. Among the emerging technologies for robust
threat detection, system provenance analysis is being considered as a promising
mechanism, thus attracting widespread attention in the field of incident
response. This paper proposes a new few-shot learning-based attack detection
with improved data context intelligence. We collect operating system behavior
data of cloud systems during realistic attacks and leverage an innovative
semiotics extraction method to describe system events. Inspired by the advances
in semantic analysis, which is a fruitful area focused on understanding natural
languages in computational linguistics, we further convert the anomaly
detection problem into a similarity comparison problem. Comprehensive
experiments show that the proposed approach is able to generalize over unseen
attacks and make accurate predictions, even if the incident detection models
are trained with very limited samples.

</details>


### [38] [Automated Energy Billing with Blockchain and the Prophet Forecasting Model: A Holistic Approach](https://arxiv.org/abs/2506.16649)
*Ajesh Thangaraj Nadar,Soham Chandane,Gabriel Nixon Raj,Nihar Mahesh Pasi,Yash Arvind Patil*

Main category: cs.CR

TL;DR: 本文提出了一种结合物联网智能电表、区块链技术和Prophet时间序列预测模型的自动化能源计费系统，旨在提高计费透明度和准确性。


<details>
  <summary>Details</summary>
Motivation: 传统能源计费存在手动错误和透明度不足的问题，需要一种更高效、安全的解决方案。

Method: 系统通过Wi-Fi ESP32模块实时监测用电量，结合Firebase和区块链确保计费安全透明，使用智能合约自动化支付，并利用Prophet模型进行能源需求预测。

Result: 该系统减少了手动错误，提升了用户对能源使用的认知，并促进了可持续能源的使用。

Conclusion: 该综合解决方案为能源计费提供了高效、透明且可持续的改进方法。

Abstract: This paper presents a comprehensive approach to automated energy billing that
leverages IoT-based smart meters, blockchain technology, and the Prophet time
series forecasting model. The proposed system facilitates real-time power
consumption monitoring via Wi-Fi-enabled ESP32 modules and a mobile application
interface. It integrates Firebase and blockchain for secure, transparent
billing processes and employs smart contracts for automated payments. The
Prophet model is used for energy demand forecasting, with careful data
preprocessing, transformation, and parameter tuning to improve prediction
accuracy. This holistic solution aims to reduce manual errors, enhance user
awareness, and promote sustainable energy use.

</details>


### [39] [The Hitchhiker's Guide to Efficient, End-to-End, and Tight DP Auditing](https://arxiv.org/abs/2506.16666)
*Meenatchi Sundaram Muthu Selva Annamalai,Borja Balle,Jamie Hayes,Georgios Kaissis,Emiliano De Cristofaro*

Main category: cs.CR

TL;DR: 本文系统化研究了差分隐私（DP）审计技术，总结了当前研究的关键见解和开放挑战。


<details>
  <summary>Details</summary>
Motivation: 旨在为差分隐私审计领域提供一个全面的框架，明确审计目标（效率、端到端性和紧密度），并分析现有技术的局限性。

Method: 提出一个系统性框架，分类现有DP审计技术的操作模式（威胁模型、攻击方式和评估函数），并分析其不足。

Result: 揭示了现有研究忽略的关键细节，指出了实现审计目标的限制因素，并提出了开放的研究问题。

Conclusion: 本文提供了一个可重复的系统化方法，用于评估领域进展，并为未来研究方向提供指导。

Abstract: This paper systematizes research on auditing Differential Privacy (DP)
techniques, aiming to identify key insights into the current state of the art
and open challenges. First, we introduce a comprehensive framework for
reviewing work in the field and establish three cross-contextual desiderata
that DP audits should target--namely, efficiency, end-to-end-ness, and
tightness. Then, we systematize the modes of operation of state-of-the-art DP
auditing techniques, including threat models, attacks, and evaluation
functions. This allows us to highlight key details overlooked by prior work,
analyze the limiting factors to achieving the three desiderata, and identify
open research problems. Overall, our work provides a reusable and systematic
methodology geared to assess progress in the field and identify friction points
and future directions for our community to focus on.

</details>


### [40] [Exploring Traffic Simulation and Cybersecurity Strategies Using Large Language Models](https://arxiv.org/abs/2506.16699)
*Lu Gao,Yongxin Liu,Hongyun Chen,Dahai Liu,Yunpeng Zhang,Jingran Sun*

Main category: cs.CR

TL;DR: 论文提出了一种基于多智能体和大型语言模型（LLM）的框架，用于增强交通模拟和网络安全测试，并通过案例研究验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 智能交通系统（ITS）因复杂互联性易受网络攻击，确保其网络安全对道路安全和减少交通中断至关重要。

Method: 采用多智能体框架结合LLM，自动化生成交通场景、设计攻击策略和开发防御机制。

Result: 案例研究表明，攻击导致旅行时间增加10.2%，而防御策略将其减少3.3%。

Conclusion: LLM驱动的多智能体系统在交通网络安全中具有潜力，为未来研究提供了可扩展的方法。

Abstract: Intelligent Transportation Systems (ITS) are increasingly vulnerable to
sophisticated cyberattacks due to their complex, interconnected nature.
Ensuring the cybersecurity of these systems is paramount to maintaining road
safety and minimizing traffic disruptions. This study presents a novel
multi-agent framework leveraging Large Language Models (LLMs) to enhance
traffic simulation and cybersecurity testing. The framework automates the
creation of traffic scenarios, the design of cyberattack strategies, and the
development of defense mechanisms. A case study demonstrates the framework's
ability to simulate a cyberattack targeting connected vehicle broadcasts,
evaluate its impact, and implement a defense mechanism that significantly
mitigates traffic delays. Results show a 10.2 percent increase in travel time
during an attack, which is reduced by 3.3 percent with the defense strategy.
This research highlights the potential of LLM-driven multi-agent systems in
advancing transportation cybersecurity and offers a scalable approach for
future research in traffic simulation and cyber defense.

</details>


### [41] [Zero-Knowledge Proof-of-Location Protocols for Vehicle Subsidies and Taxation Compliance](https://arxiv.org/abs/2506.16812)
*Dan Bogdanov,Eduardo Brito,Annika Jaakson,Peeter Laud,Raul-Martin Rebane*

Main category: cs.CR

TL;DR: 本文提出了一种基于零知识证明（ZKPs）的隐私保护机制，用于验证车辆是否符合基于位置的税收或补贴政策。


<details>
  <summary>Details</summary>
Motivation: 解决在车辆税收或补贴政策中验证位置合规性时保护用户隐私的问题。

Method: 设计并评估了一种零知识位置证明（ZK-PoL）系统，确保车辆符合地域驾驶要求，同时不泄露具体位置数据。

Result: 研究表明，ZK-PoL协议在大规模政府补贴或税收计划中具有应用潜力。

Conclusion: 零知识位置证明系统为隐私保护的位置验证提供了一种可行且高效的解决方案。

Abstract: This paper introduces a new set of privacy-preserving mechanisms for
verifying compliance with location-based policies for vehicle taxation, or for
(electric) vehicle (EV) subsidies, using Zero-Knowledge Proofs (ZKPs). We
present the design and evaluation of a Zero-Knowledge Proof-of-Location
(ZK-PoL) system that ensures a vehicle's adherence to territorial driving
requirements without disclosing specific location data, hence maintaining user
privacy. Our findings suggest a promising approach to apply ZK-PoL protocols in
large-scale governmental subsidy or taxation programs.

</details>


### [42] [Tracker Installations Are Not Created Equal: Understanding Tracker Configuration of Form Data Collection](https://arxiv.org/abs/2506.16891)
*Julia B. Kieserman,Athanasios Andreou,Chris Geeng,Tobias Lauinger,Damon McCoy*

Main category: cs.CR

TL;DR: 该研究分析了Google和Meta的跟踪器如何配置以收集用户个人身份信息（PII），揭示了它们在文档和界面中鼓励数据收集的行为，并通过大规模测量量化了其普遍性和配置情况。


<details>
  <summary>Details</summary>
Motivation: 研究旨在揭示Google和Meta的跟踪器如何通过配置收集PII数据，以及它们在文档和界面中如何鼓励网站管理员进行此类操作。

Method: 通过定性分析文档和界面，以及测量40,150个网站，量化跟踪器的普遍性和配置情况。

Result: 发现Google和Meta鼓励表单数据收集，并存在关于哈希PII的误导性陈述；Meta的跟踪器更频繁配置为收集表单数据（62.3% vs. 11.6%）。

Conclusion: 研究表明跟踪器的文档和界面可能通过网站管理员的配置选择影响用户隐私，部分敏感网站甚至违反政策配置跟踪器。

Abstract: Targeted advertising is fueled by the comprehensive tracking of users' online
activity. As a result, advertising companies, such as Google and Meta,
encourage website administrators to not only install tracking scripts on their
websites but configure them to automatically collect users' Personally
Identifying Information (PII). In this study, we aim to characterize how Google
and Meta's trackers can be configured to collect PII data from web forms. We
first perform a qualitative analysis of how third parties present form data
collection to website administrators in the documentation and user interface.
We then perform a measurement study of 40,150 websites to quantify the
prevalence and configuration of Google and Meta trackers.
  Our results reveal that both Meta and Google encourage the use of form data
collection and include inaccurate statements about hashing PII as a
privacy-preserving method. Additionally, we find that Meta includes configuring
form data collection as part of the basic setup flow. Our large-scale
measurement study reveals that while Google trackers are more prevalent than
Meta trackers (72.6% vs. 28.2% of websites), Meta trackers are configured to
collect form data more frequently (11.6% vs. 62.3%). Finally, we identify
sensitive finance and health websites that have installed trackers that are
likely configured to collect form data PII in violation of Meta and Google
policies. Our study highlights how tracker documentation and interfaces can
potentially play a role in users' privacy through the configuration choices
made by the website administrators who install trackers.

</details>


### [43] [Towards Effective Complementary Security Analysis using Large Language Models](https://arxiv.org/abs/2506.16899)
*Jonas Wagner,Simon Müller,Christian Näther,Jan-Philipp Steghöfer,Andreas Both*

Main category: cs.CR

TL;DR: 论文提出利用大型语言模型（LLMs）改进静态应用安全测试（SAST）工具的结果评估，以减少误报（FPs）并保持高真阳性率。实验表明，高级提示技术显著提升误报检测能力，结合多模型结果可进一步提高检测率。


<details>
  <summary>Details</summary>
Motivation: SAST工具生成的报告存在大量误报，降低了安全分析的效率。研究旨在利用LLMs优化SAST结果的评估，减少人工干预。

Method: 使用OWASP Benchmark和真实项目数据集，测试LLMs在减少误报方面的能力，并采用Chain-of-Thought和Self-Consistency等高级提示技术。

Result: LLMs在OWASP Benchmark中检测到62.5%的误报，结合多模型结果提升至78.9%；在真实数据集中检测到33.85%的误报，结合多模型结果提升至38.46%。

Conclusion: LLMs能有效补充传统SAST工具，提升自动化水平并减少误报处理资源。

Abstract: A key challenge in security analysis is the manual evaluation of potential
security weaknesses generated by static application security testing (SAST)
tools. Numerous false positives (FPs) in these reports reduce the effectiveness
of security analysis. We propose using Large Language Models (LLMs) to improve
the assessment of SAST findings. We investigate the ability of LLMs to reduce
FPs while trying to maintain a perfect true positive rate, using datasets
extracted from the OWASP Benchmark (v1.2) and a real-world software project.
Our results indicate that advanced prompting techniques, such as
Chain-of-Thought and Self-Consistency, substantially improve FP detection.
Notably, some LLMs identified approximately 62.5% of FPs in the OWASP Benchmark
dataset without missing genuine weaknesses. Combining detections from different
LLMs would increase this FP detection to approximately 78.9%. Additionally, we
demonstrate our approach's generalizability using a real-world dataset covering
five SAST tools, three programming languages, and infrastructure files. The
best LLM detected 33.85% of all FPs without missing genuine weaknesses, while
combining detections from different LLMs would increase this detection to
38.46%. Our findings highlight the potential of LLMs to complement traditional
SAST tools, enhancing automation and reducing resources spent addressing false
alarms.

</details>


### [44] [MM-AttacKG: A Multimodal Approach to Attack Graph Construction with Large Language Models](https://arxiv.org/abs/2506.16968)
*Yongheng Zhang,Xinyun Zhao,Yunshan Ma,Haokai Ma,Yingxiao Guan,Guozheng Yang,Yuliang Lu,Xiang Wang*

Main category: cs.CR

TL;DR: 论文提出了一种名为MM-AttacKG的多模态框架，通过分析视觉信息提升攻击图构建的全面性和准确性。


<details>
  <summary>Details</summary>
Motivation: 现有攻击图构建方法主要依赖文本数据，忽略了视觉模态中的关键威胁信息。

Method: 提出MM-AttacKG框架，包括威胁图像解析模块、迭代问答管道和多模态内容整合。

Result: 实验表明，MM-AttacKG能准确提取威胁图像中的关键信息，显著提升攻击图质量。

Conclusion: MM-AttacKG有效解决了现有方法在利用图像威胁信息方面的不足。

Abstract: Cyber Threat Intelligence (CTI) parsing aims to extract key threat
information from massive data, transform it into actionable intelligence,
enhance threat detection and defense efficiency, including attack graph
construction, intelligence fusion and indicator extraction. Among these
research topics, Attack Graph Construction (AGC) is essential for visualizing
and understanding the potential attack paths of threat events from CTI reports.
Existing approaches primarily construct the attack graphs purely from the
textual data to reveal the logical threat relationships between entities within
the attack behavioral sequence. However, they typically overlook the specific
threat information inherent in visual modalities, which preserves the key
threat details from inherently-multimodal CTI report. Therefore, we enhance the
effectiveness of attack graph construction by analyzing visual information
through Multimodal Large Language Models (MLLMs). Specifically, we propose a
novel framework, MM-AttacKG, which can effectively extract key information from
threat images and integrate it into attack graph construction, thereby
enhancing the comprehensiveness and accuracy of attack graphs. It first employs
a threat image parsing module to extract critical threat information from
images and generate descriptions using MLLMs. Subsequently, it builds an
iterative question-answering pipeline tailored for image parsing to refine the
understanding of threat images. Finally, it achieves content-level integration
between attack graphs and image-based answers through MLLMs, completing threat
information enhancement. The experimental results demonstrate that MM-AttacKG
can accurately identify key information in threat images and significantly
improve the quality of multimodal attack graph construction, effectively
addressing the shortcomings of existing methods in utilizing image-based threat
information.

</details>


### [45] [SmartGuard: Leveraging Large Language Models for Network Attack Detection through Audit Log Analysis and Summarization](https://arxiv.org/abs/2506.16981)
*Hao Zhang,Shuo Shao,Song Li,Zhenyu Zhong,Yan Liu,Zhan Qin,Kui Ren*

Main category: cs.CR

TL;DR: SmartGuard 是一种结合审计日志语义与大型语言模型的自动化方法，用于高粒度检测恶意行为并提供解释性描述。


<details>
  <summary>Details</summary>
Motivation: 现有审计日志分析方法粒度低，依赖规则知识库，难以检测未知攻击且缺乏解释能力。

Method: SmartGuard 从系统日志中提取功能级行为，构建知识图谱，结合事件摘要和图嵌入进行信息诊断，并通过大型语言模型生成解释性描述。

Result: SmartGuard 在评估恶意行为时平均 F1 分数达 96%，具有良好的扩展性和微调能力。

Conclusion: SmartGuard 能有效提升攻击检测的粒度和解释能力，适用于未知攻击检测和系统更新。

Abstract: End-point monitoring solutions are widely deployed in today's enterprise
environments to support advanced attack detection and investigation. These
monitors continuously record system-level activities as audit logs and provide
deep visibility into security events. Unfortunately, existing methods of
semantic analysis based on audit logs have low granularity, only reaching the
system call level, making it difficult to effectively classify highly covert
behaviors. Additionally, existing works mainly match audit log streams with
rule knowledge bases describing behaviors, which heavily rely on expertise and
lack the ability to detect unknown attacks and provide interpretive
descriptions. In this paper, we propose SmartGuard, an automated method that
combines abstracted behaviors from audit event semantics with large language
models. SmartGuard extracts specific behaviors (function level) from incoming
system logs and constructs a knowledge graph, divides events by threads, and
combines event summaries with graph embeddings to achieve information diagnosis
and provide explanatory narratives through large language models. Our
evaluation shows that SmartGuard achieves an average F1 score of 96\% in
assessing malicious behaviors and demonstrates good scalability across multiple
models and unknown attacks. It also possesses excellent fine-tuning
capabilities, allowing experts to assist in timely system updates.

</details>


### [46] [A Novel Approach to Differential Privacy with Alpha Divergence](https://arxiv.org/abs/2506.17012)
*Yifeng Liu,Zehua Wang*

Main category: cs.CR

TL;DR: 本文提出了一种基于alpha散度的新型隐私框架（ADP），以解决传统差分隐私的局限性，并在多种场景下展示了其优越性。


<details>
  <summary>Details</summary>
Motivation: 随着数据驱动技术的快速发展，传统差分隐私的局限性日益凸显，需要更灵活的隐私评估方法。

Method: 提出了基于alpha散度的alpha差分隐私（ADP）框架，并通过理论分析和实证评估验证其性能。

Result: 实证表明，ADP在小到中等迭代场景下提供更强的隐私保护，尤其是在高隐私需求的情况下。

Conclusion: ADP显著改进了隐私保护方法，为现代数据分析问题提供了灵活的解决方案。

Abstract: As data-driven technologies advance swiftly, maintaining strong privacy
measures becomes progressively difficult. Conventional $(\epsilon,
\delta)$-differential privacy, while prevalent, exhibits limited adaptability
for many applications. To mitigate these constraints, we present alpha
differential privacy (ADP), an innovative privacy framework grounded in alpha
divergence, which provides a more flexible assessment of privacy consumption.
This study delineates the theoretical underpinnings of ADP and contrasts its
performance with competing privacy frameworks across many scenarios. Empirical
assessments demonstrate that ADP offers enhanced privacy guarantees in small to
moderate iteration contexts, particularly where severe privacy requirements are
necessary. The suggested method markedly improves privacy-preserving methods,
providing a flexible solution for contemporary data analysis issues in a
data-centric environment.

</details>


### [47] [Global Microprocessor Correctness in the Presence of Transient Execution](https://arxiv.org/abs/2506.17154)
*Andrew T. Walter,Konstantinos Athanasiou,Panagiotis Manolios*

Main category: cs.CR

TL;DR: 本文主张使用形式化规范来定义微处理器的正确性，提出了一种新的细化理论，以应对瞬态执行攻击（如Meltdown和Spectre），并通过轻量级形式化方法识别相关漏洞。


<details>
  <summary>Details</summary>
Motivation: 现有微处理器的正确性通常基于非正式规范，导致优化设计中的漏洞（如瞬态执行攻击）难以被发现和解决。

Method: 提出全局正确性概念，结合功能正确性和微架构参数化，引入动作跳过细化理论，并利用共享资源承诺细化映射进行自动化验证。

Result: 在完全可执行的位和周期精确模型上验证了方法的有效性，并展示了如何识别瞬态执行漏洞。

Conclusion: 形式化规范和细化理论为微处理器正确性提供了更严格的框架，有助于发现和解决优化设计中的潜在问题。

Abstract: Correctness for microprocessors is generally understood to be conformance
with the associated instruction set architecture (ISA). This is the basis for
one of the most important abstractions in computer science, allowing hardware
designers to develop highly-optimized processors that are functionally
"equivalent" to an ideal processor that executes instructions atomically. This
specification is almost always informal, e.g., commercial microprocessors
generally do not come with conformance specifications. In this paper, we
advocate for the use of formal specifications, using the theory of refinement.
We introduce notions of correctness that can be used to deal with transient
execution attacks, including Meltdown and Spectre. Such attacks have shown that
ubiquitous microprocessor optimizations, appearing in numerous processors for
decades, are inherently buggy. Unlike alternative approaches that use
non-interference properties, our notion of correctness is global, meaning it is
single specification that: formalizes conformance, includes functional
correctness and is parameterized by an microarchitecture. We introduce action
skipping refinement, a new type of refinement and we describe how our notions
of refinement can be decomposed into properties that are more amenable to
automated verification using the the concept of shared-resource commitment
refinement maps. We do this in the context of formal, fully executable bit- and
cycle-accurate models of an ISA and a microprocessor. Finally, we show how
light-weight formal methods based on property-based testing can be used to
identify transient execution bugs.

</details>


### [48] [Analyzing PDFs like Binaries: Adversarially Robust PDF Malware Analysis via Intermediate Representation and Language Model](https://arxiv.org/abs/2506.17162)
*Side Liu,Jiang Ming,Guodong Zhou,Xinyi Liu,Jianming Fu,Guojun Peng*

Main category: cs.CR

TL;DR: 提出了一种基于PDF对象中间表示（PDFObj IR）和对象引用图的双重特征提取方法，用于增强PDF恶意软件检测的对抗鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有PDF恶意软件分类器因特征工程过时而易受对抗攻击，需改进特征提取方法以提高鲁棒性。

Method: 引入PDFObj IR框架提取语义特征，并结合对象引用图捕获结构特征，使用预训练语言模型进行分析。

Result: 实验表明，该方法在对抗鲁棒性上表现优异，基线数据集上的误报率仅为0.07%。

Conclusion: 通过结合语义和结构特征，新方法显著提升了PDF恶意软件检测的鲁棒性和准确性。

Abstract: Malicious PDF files have emerged as a persistent threat and become a popular
attack vector in web-based attacks. While machine learning-based PDF malware
classifiers have shown promise, these classifiers are often susceptible to
adversarial attacks, undermining their reliability. To address this issue,
recent studies have aimed to enhance the robustness of PDF classifiers. Despite
these efforts, the feature engineering underlying these studies remains
outdated. Consequently, even with the application of cutting-edge machine
learning techniques, these approaches fail to fundamentally resolve the issue
of feature instability.
  To tackle this, we propose a novel approach for PDF feature extraction and
PDF malware detection. We introduce the PDFObj IR (PDF Object Intermediate
Representation), an assembly-like language framework for PDF objects, from
which we extract semantic features using a pretrained language model.
Additionally, we construct an Object Reference Graph to capture structural
features, drawing inspiration from program analysis. This dual approach enables
us to analyze and detect PDF malware based on both semantic and structural
features. Experimental results demonstrate that our proposed classifier
achieves strong adversarial robustness while maintaining an exceptionally low
false positive rate of only 0.07% on baseline dataset compared to
state-of-the-art PDF malware classifiers.

</details>


### [49] [A Common Pool of Privacy Problems: Legal and Technical Lessons from a Large-Scale Web-Scraped Machine Learning Dataset](https://arxiv.org/abs/2506.17185)
*Rachel Hong,Jevan Hutson,William Agnew,Imaad Huda,Tadayoshi Kohno,Jamie Morgenstern*

Main category: cs.CR

TL;DR: 论文研究了大规模网络爬取数据用于训练AI系统时的隐私法律问题，发现即使经过清理，数据集仍包含个人可识别信息，呼吁重新定义“公开可用”信息的框架。


<details>
  <summary>Details</summary>
Motivation: 探讨网络爬取数据在AI训练中的隐私法律影响，尤其是在数据集规模庞大、人工标注不可行的情况下。

Method: 通过实证研究分析流行训练数据集中的个人可识别信息，并结合法律分析评估隐私风险。

Result: 研究发现数据集存在大量个人可识别信息，揭示了当前数据整理实践的隐私风险。

Conclusion: 论文主张重新定义“公开可用”信息框架，限制基于无差别网络爬取的AI发展。

Abstract: We investigate the contents of web-scraped data for training AI systems, at
sizes where human dataset curators and compilers no longer manually annotate
every sample. Building off of prior privacy concerns in machine learning
models, we ask: What are the legal privacy implications of web-scraped machine
learning datasets? In an empirical study of a popular training dataset, we find
significant presence of personally identifiable information despite
sanitization efforts. Our audit provides concrete evidence to support the
concern that any large-scale web-scraped dataset may contain personal data. We
use these findings of a real-world dataset to inform our legal analysis with
respect to existing privacy and data protection laws. We surface various
privacy risks of current data curation practices that may propagate personal
information to downstream models. From our findings, we argue for reorientation
of current frameworks of "publicly available" information to meaningfully limit
the development of AI built upon indiscriminate scraping of the internet.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [50] [LLMs Struggle to Perform Counterfactual Reasoning with Parametric Knowledge](https://arxiv.org/abs/2506.15732)
*Khurram Yamin,Gaurav Ghosal,Bryan Wilder*

Main category: cs.AI

TL;DR: 研究发现大型语言模型（LLM）在反事实推理任务中表现不佳，倾向于依赖参数化知识，且简单微调可能导致知识退化。


<details>
  <summary>Details</summary>
Motivation: 探索LLM能否在反事实推理任务中结合上下文知识与参数化知识。

Method: 通过合成和真实实验，研究LLM在多跳推理问题中的表现。

Result: LLM在反事实推理中表现较差，且微调可能损害其参数化知识。

Conclusion: 当前LLM在新环境中重新利用参数化知识的能力存在重要局限性。

Abstract: Large Language Models have been shown to contain extensive world knowledge in
their parameters, enabling impressive performance on many knowledge intensive
tasks. However, when deployed in novel settings, LLMs often encounter
situations where they must integrate parametric knowledge with new or
unfamiliar information. In this work, we explore whether LLMs can combine
knowledge in-context with their parametric knowledge through the lens of
counterfactual reasoning. Through synthetic and real experiments in multi-hop
reasoning problems, we show that LLMs generally struggle with counterfactual
reasoning, often resorting to exclusively using their parametric knowledge.
Moreover, we show that simple post-hoc finetuning can struggle to instill
counterfactual reasoning ability -- often leading to degradation in stored
parametric knowledge. Ultimately, our work reveals important limitations of
current LLM's abilities to re-purpose parametric knowledge in novel settings.

</details>


### [51] [$\texttt{SPECS}$: Faster Test-Time Scaling through Speculative Drafts](https://arxiv.org/abs/2506.15733)
*Mert Cemri,Nived Rajaraman,Rishabh Tiwari,Xiaoxuan Liu,Kurt Keutzer,Ion Stoica,Kannan Ramchandran,Ahmad Beirami,Ziteng Sun*

Main category: cs.AI

TL;DR: 论文提出了一种名为SPECS的延迟感知测试时扩展方法，通过结合小模型生成候选序列和大模型评估，优化计算资源使用，同时降低延迟。


<details>
  <summary>Details</summary>
Motivation: 当前测试时扩展方法主要关注计算资源优化，忽略了用户延迟问题，SPECS旨在解决这一矛盾。

Method: SPECS利用小模型高效生成候选序列，结合大模型和奖励模型评估，并引入奖励引导的软验证和延迟机制。

Result: 在多个数据集上，SPECS在保持或超越束搜索准确性的同时，延迟降低达19.1%。

Conclusion: SPECS通过理论分析和实验验证，证明了其在延迟和准确性之间的平衡能力。

Abstract: Scaling test-time compute has driven the recent advances in the reasoning
capabilities of large language models (LLMs), typically by allocating
additional computation for more thorough exploration. However, increased
compute often comes at the expense of higher user-facing latency, directly
impacting user experience. Current test-time scaling methods primarily optimize
for accuracy based on total compute resources (FLOPS), often overlooking
latency constraints. To address this gap, we propose $\texttt{SPECS}$, a
latency-aware test-time scaling method inspired by speculative decoding.
$\texttt{SPECS}$~uses a smaller, faster model to generate candidate sequences
efficiently, and evaluates these candidates using signals from both a larger
target model and a dedicated reward model. We introduce new integration
strategies, including reward-guided soft verification and a reward-based
deferral mechanism. Empirical results on MATH500, AMC23 and OlympiadBench
datasets show that $\texttt{SPECS}$~matches or surpasses beam search accuracy
while reducing latency by up to $\sim$19.1\%. Our theoretical analysis shows
that our algorithm converges to the solution of a KL-regularized reinforcement
learning objective with increasing beam width.

</details>


### [52] [The Safety Reminder: A Soft Prompt to Reactivate Delayed Safety Awareness in Vision-Language Models](https://arxiv.org/abs/2506.15734)
*Peiyuan Tang,Haojie Xin,Xiaodong Zhang,Jun Sun,Qin Xia,Zijiang Yang*

Main category: cs.AI

TL;DR: 论文提出了一种名为“安全提醒”的软提示调优方法，通过周期性注入可学习的提示令牌来增强视觉语言模型（VLM）的安全意识，从而防止有害内容生成。


<details>
  <summary>Details</summary>
Motivation: 由于视觉语言模型（VLM）在多模态输入下的独特脆弱性，攻击者可能通过修改视觉或文本输入绕过安全防护，生成有害内容。论文旨在解决这一问题。

Method: 通过系统分析VLM在攻击下的行为，发现“延迟安全意识”现象，并提出“安全提醒”方法，优化可学习的提示令牌以增强安全意识。

Result: 在三个安全基准和一个对抗攻击测试中，该方法显著降低了攻击成功率，同时保持了模型的正常任务性能。

Conclusion: “安全提醒”方法为实际应用中部署更安全的VLM提供了实用解决方案。

Abstract: As Vision-Language Models (VLMs) demonstrate increasing capabilities across
real-world applications such as code generation and chatbot assistance,
ensuring their safety has become paramount. Unlike traditional Large Language
Models (LLMs), VLMs face unique vulnerabilities due to their multimodal nature,
allowing adversaries to modify visual or textual inputs to bypass safety
guardrails and trigger the generation of harmful content. Through systematic
analysis of VLM behavior under attack, we identify a novel phenomenon termed
``delayed safety awareness''. Specifically, we observe that safety-aligned VLMs
may initially be compromised to produce harmful content, but eventually
recognize the associated risks and attempt to self-correct. This pattern
suggests that VLMs retain their underlying safety awareness but experience a
temporal delay in their activation. Building on this insight, we hypothesize
that VLMs' safety awareness can be proactively reactivated through carefully
designed prompts. To this end, we introduce ``The Safety Reminder'', a soft
prompt tuning approach that optimizes learnable prompt tokens, which are
periodically injected during the text generation process to enhance safety
awareness, effectively preventing harmful content generation. Additionally, our
safety reminder only activates when harmful content is detected, leaving normal
conversations unaffected and preserving the model's performance on benign
tasks. Through comprehensive evaluation across three established safety
benchmarks and one adversarial attacks, we demonstrate that our approach
significantly reduces attack success rates while maintaining model utility,
offering a practical solution for deploying safer VLMs in real-world
applications.

</details>


### [53] [ContextBench: Modifying Contexts for Targeted Latent Activation](https://arxiv.org/abs/2506.15735)
*Robert Graham,Edward Stevinson,Leo Richter,Alexander Chia,Joseph Miller,Joseph Isaac Bloom*

Main category: cs.AI

TL;DR: 研究提出了一种通过上下文修改生成针对性输入的方法，以激活语言模型的特定潜在特征或行为，并开发了ContextBench基准测试其性能。


<details>
  <summary>Details</summary>
Motivation: 识别触发语言模型特定行为或潜在特征的输入，对安全应用有广泛意义。

Method: 结合进化提示优化（EPO）、LLM辅助和扩散模型修复技术，生成针对性输入。

Result: 改进后的方法在激活效果和语言流畅性上达到最优平衡。

Conclusion: 该方法在安全应用中具有潜力，但现有方法仍需改进以兼顾激活效果和流畅性。

Abstract: Identifying inputs that trigger specific behaviours or latent features in
language models could have a wide range of safety use cases. We investigate a
class of methods capable of generating targeted, linguistically fluent inputs
that activate specific latent features or elicit model behaviours. We formalise
this approach as context modification and present ContextBench -- a benchmark
with tasks assessing core method capabilities and potential safety
applications. Our evaluation framework measures both elicitation strength
(activation of latent features or behaviours) and linguistic fluency,
highlighting how current state-of-the-art methods struggle to balance these
objectives. We enhance Evolutionary Prompt Optimisation (EPO) with
LLM-assistance and diffusion model inpainting, and demonstrate that these
variants achieve state-of-the-art performance in balancing elicitation
effectiveness and fluency.

</details>


### [54] [SHADE-Arena: Evaluating Sabotage and Monitoring in LLM Agents](https://arxiv.org/abs/2506.15740)
*Jonathan Kutasov,Yuqi Sun,Paul Colognese,Teun van der Weij,Linda Petrini,Chen Bo Calvin Zhang,John Hughes,Xiang Deng,Henry Sleight,Tyler Tracy,Buck Shlegeris,Joe Benton*

Main category: cs.AI

TL;DR: 论文研究了前沿大语言模型（LLM）在执行任务时隐藏有害目标的能力，并开发了SHADE-Arena数据集进行评估。结果显示，当前模型在隐蔽破坏任务中表现有限，但监控难度已显现。


<details>
  <summary>Details</summary>
Motivation: 随着LLM作为自主代理在复杂任务中的广泛应用，评估其是否能够隐藏有害目标并规避监控变得至关重要。

Method: 使用SHADE-Arena数据集，评估LLM在完成主任务的同时隐蔽完成有害副任务的能力，并测试模型的监控能力。

Result: 最佳模型在隐蔽破坏任务中得分较低（Claude 3.7 Sonnet为27%，Gemini 2.5 Pro为15%），监控能力最强的模型AUC为0.87。

Conclusion: 当前模型在隐蔽破坏任务中表现不佳，但监控难度已显现，未来随着任务复杂度增加，监控挑战将进一步加剧。

Abstract: As Large Language Models (LLMs) are increasingly deployed as autonomous
agents in complex and long horizon settings, it is critical to evaluate their
ability to sabotage users by pursuing hidden objectives. We study the ability
of frontier LLMs to evade monitoring and achieve harmful hidden goals while
completing a wide array of realistic tasks. We evaluate a broad range of
frontier LLMs using SHADE (Subtle Harmful Agent Detection & Evaluation)-Arena,
the first highly diverse agent evaluation dataset for sabotage and monitoring
capabilities of LLM agents. SHADE-Arena consists of complex pairs of benign
main tasks and harmful side objectives in complicated environments. Agents are
evaluated on their ability to complete the side task without appearing
suspicious to an LLM monitor. When measuring agent ability to (a) complete the
main task, (b) complete the side task, and (c) avoid detection, we find that
the best performing frontier models score 27% (Claude 3.7 Sonnet) and 15%
(Gemini 2.5 Pro) as sabotage agents when overseen by Claude 3.6 Sonnet. For
current frontier models, success on the side task relies heavily on having
access to a hidden scratchpad that is not visible to the monitor. We also use
SHADE-Arena to measure models' monitoring abilities, with the top monitor
(Gemini 2.5 Pro) achieving an AUC of 0.87 at distinguishing benign and malign
transcripts. We find that for now, models still struggle at sabotage due to
failures in long-context main task execution. However, our measurements already
demonstrate the difficulty of monitoring for subtle sabotage attempts, which we
expect to only increase in the face of more complex and longer-horizon tasks.

</details>


### [55] [OAgents: An Empirical Study of Building Effective Agents](https://arxiv.org/abs/2506.15741)
*He Zhu,Tianrui Qin,King Zhu,Heyuan Huang,Yeyi Guan,Jinxiang Xia,Yi Yao,Hanhao Li,Ningning Wang,Pai Liu,Tianhao Peng,Xin Gui,Xiaowan Li,Yuhui Liu,Yuchen Eleanor Jiang,Jun Wang,Changwang Zhang,Xiangru Tang,Ge Zhang,Jian Yang,Minghao Liu,Xitong Gao,Wangchunshu Zhou,Jiaheng Liu*

Main category: cs.AI

TL;DR: 论文指出当前Agentic AI研究缺乏标准化和科学严谨性，提出通过系统实验和稳健评估协议改进，并开源了高性能框架OAgents。


<details>
  <summary>Details</summary>
Motivation: 当前Agentic AI研究缺乏标准化和公平比较的基础，导致设计选择的影响不明确。

Method: 在GAIA和BrowseComp基准上进行系统实验，分析关键组件的设计选择，并引入稳健评估协议。

Result: 发现缺乏标准评估协议导致结果不可复现，提出关键组件设计的重要性，开源了高性能框架OAgents。

Conclusion: 通过标准化和模块化设计，OAgents推动了Agentic AI研究的未来发展。

Abstract: Recently, Agentic AI has become an increasingly popular research field.
However, we argue that current agent research practices lack standardization
and scientific rigor, making it hard to conduct fair comparisons among methods.
As a result, it is still unclear how different design choices in agent
frameworks affect effectiveness, and measuring their progress remains
challenging. In this work, we conduct a systematic empirical study on GAIA
benchmark and BrowseComp to examine the impact of popular design choices in key
agent components in a fair and rigorous manner. We find that the lack of a
standard evaluation protocol makes previous works, even open-sourced ones,
non-reproducible, with significant variance between random runs. Therefore, we
introduce a more robust evaluation protocol to stabilize comparisons. Our study
reveals which components and designs are crucial for effective agents, while
others are redundant, despite seeming logical. Based on our findings, we build
and open-source OAgents, a new foundation agent framework that achieves
state-of-the-art performance among open-source projects. OAgents offers a
modular design for various agent components, promoting future research in
Agentic AI.

</details>


### [56] [Sysformer: Safeguarding Frozen Large Language Models with Adaptive System Prompts](https://arxiv.org/abs/2506.15751)
*Kartik Sharma,Yiqiao Jin,Vineeth Rakesh,Yingtong Dou,Menghai Pan,Mahashweta Das,Srijan Kumar*

Main category: cs.AI

TL;DR: 论文提出了一种名为Sysformer的新方法，通过动态调整系统提示来提升大语言模型（LLMs）的安全性，避免对有害提示的响应，同时优化对安全提示的合规性。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖昂贵的微调或启发式技术，无法有效确保LLMs的安全性，因此需要一种更高效且灵活的方法。

Method: 提出Sysformer模型，通过学习在LLM输入嵌入空间中动态调整系统提示，保持LLM参数不变，仅优化系统提示。

Result: 实验表明，Sysformer显著提升了LLMs的安全性，有害提示的拒绝率提升80%，安全提示的合规性提升90%，并能抵御复杂的越狱攻击。

Conclusion: Sysformer为LLMs提供了一种低成本的安全保障方法，并启发了未来对可变系统提示设计的进一步研究。

Abstract: As large language models (LLMs) are deployed in safety-critical settings, it
is essential to ensure that their responses comply with safety standards. Prior
research has revealed that LLMs often fail to grasp the notion of safe
behaviors, resulting in either unjustified refusals to harmless prompts or the
generation of harmful content. While substantial efforts have been made to
improve their robustness, existing defenses often rely on costly fine-tuning of
model parameters or employ suboptimal heuristic techniques. In this work, we
take a novel approach to safeguard LLMs by learning to adapt the system prompts
in instruction-tuned LLMs. While LLMs are typically pre-trained to follow a
fixed system prompt, we investigate the impact of tailoring the system prompt
to each specific user input on the safety of the responses. To this end, we
propose $\textbf{Sysformer}$, a trans$\textbf{former}$ model that updates an
initial $\textbf{sys}$tem prompt to a more robust system prompt in the LLM
input embedding space while attending to the user prompt. While keeping the LLM
parameters frozen, the Sysformer is trained to refuse to respond to a set of
harmful prompts while responding ideally to a set of safe ones. Through
extensive experiments on $5$ LLMs from different families and $2$ recent
benchmarks, we demonstrate that Sysformer can significantly enhance the
robustness of LLMs, leading to upto $80\%$ gain in the refusal rate on harmful
prompts while enhancing the compliance with the safe prompts by upto $90\%$.
Results also generalize well to sophisticated jailbreaking attacks, making LLMs
upto $100\%$ more robust against different attack strategies. We hope our
findings lead to cheaper safeguarding of LLMs and motivate future
investigations into designing variable system prompts.

</details>


### [57] [Linear-Time Primitives for Algorithm Development in Graphical Causal Inference](https://arxiv.org/abs/2506.15758)
*Marcel Wienöbst,Sebastian Weichwald,Leonard Henckel*

Main category: cs.AI

TL;DR: CIfly是一个高效的图形因果推断框架，通过将因果推理任务简化为可达性问题，提供线性时间算法，并作为道德化和潜在投影的高效替代方案。


<details>
  <summary>Details</summary>
Motivation: 许多因果推理任务可以简化为可达性问题，但现有方法（如道德化和潜在投影）计算复杂度高，需要更高效的解决方案。

Method: CIfly通过动态构建状态空间图并利用规则表指定算法，实现线性时间复杂度的因果推理。

Result: CIfly在性能上优于传统方法，并通过开源Rust实现支持Python和R调用，展示了在因果推断任务中的实用性。

Conclusion: CIfly为图形因果推断提供了灵活、可扩展的框架，支持高效算法开发和部署。

Abstract: We introduce CIfly, a framework for efficient algorithmic primitives in
graphical causal inference that isolates reachability as a reusable core
operation. It builds on the insight that many causal reasoning tasks can be
reduced to reachability in purpose-built state-space graphs that can be
constructed on the fly during traversal. We formalize a rule table schema for
specifying such algorithms and prove they run in linear time. We establish
CIfly as a more efficient alternative to the common primitives moralization and
latent projection, which we show are computationally equivalent to Boolean
matrix multiplication. Our open-source Rust implementation parses rule table
text files and runs the specified CIfly algorithms providing high-performance
execution accessible from Python and R. We demonstrate CIfly's utility by
re-implementing a range of established causal inference tasks within the
framework and by developing new algorithms for instrumental variables. These
contributions position CIfly as a flexible and scalable backbone for graphical
causal inference, guiding algorithm development and enabling easy and efficient
deployment.

</details>


### [58] [Advancing Stochastic 3-SAT Solvers by Dissipating Oversatisfied Constraints](https://arxiv.org/abs/2506.15774)
*J. Schwardt,J. C. Budich*

Main category: cs.AI

TL;DR: 提出了一种名为DOCSAT的随机局部搜索启发式算法，显著优于现有求解器在3-SAT问题中的表现。


<details>
  <summary>Details</summary>
Motivation: 解决现有方法（如WalkSAT）容易陷入局部极小值的问题。

Method: 通过减少过满足约束（DOC）来避免局部极小值陷阱。

Result: DOCSAT在N=15000的困难实例中表现优于WalkSAT和Kissat等算法。

Conclusion: DOCSAT为其他优化问题提供了避免局部极小值的新思路。

Abstract: We introduce and benchmark a stochastic local search heuristic for the
NP-complete satisfiability problem 3-SAT that drastically outperforms existing
solvers in the notoriously difficult realm of critically hard instances. Our
construction is based on the crucial observation that well established previous
approaches such as WalkSAT are prone to get stuck in local minima that are
distinguished from true solutions by a larger number of oversatisfied
combinatorial constraints. To address this issue, the proposed algorithm,
coined DOCSAT, dissipates oversatisfied constraints (DOC), i.e. reduces their
unfavorable abundance so as to render them critical. We analyze and benchmark
our algorithm on a randomly generated sample of hard but satisfiable 3-SAT
instances with varying problem sizes up to N=15000. Quite remarkably, we find
that DOCSAT outperforms both WalkSAT and other well known algorithms including
the complete solver Kissat, even when comparing its ability to solve the
hardest quintile of the sample to the average performance of its competitors.
The essence of DOCSAT may be seen as a way of harnessing statistical structure
beyond the primary cost function of a combinatorial problem to avoid or escape
local minima traps in stochastic local search, which opens avenues for
generalization to other optimization problems.

</details>


### [59] [SLR: An Automated Synthesis Framework for Scalable Logical Reasoning](https://arxiv.org/abs/2506.15787)
*Lukas Helff,Ahmad Omar,Felix Friedrich,Wolfgang Stammer,Antonia Wüst,Tim Woydt,Rupert Mitchell,Patrick Schramowski,Kristian Kersting*

Main category: cs.AI

TL;DR: SLR是一个端到端框架，通过可扩展的逻辑推理系统评估和训练大型语言模型（LLMs）。它自动生成任务、验证规则和提示，并创建了包含19k+提示的基准测试SLR-Bench。评估显示，当前LLMs在逻辑推理上表现不佳，但通过SLR调优可显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 旨在提供一个可扩展、自动化的方法来评估和提升LLMs的逻辑推理能力，减少人工标注需求并确保数据集新颖性。

Method: SLR通过用户任务规范自动生成推理任务，包括潜在规则、验证程序和提示。创建了SLR-Bench基准测试，涵盖20个难度级别。

Result: 当前LLMs在逻辑推理上表现不佳，但通过SLR调优后，Llama-3-8B的准确率翻倍，达到与Gemini-Flash-Thinking相当的水平。

Conclusion: SLR为LLMs的逻辑推理能力提供了高效、自动化的评估和训练环境，显著提升了模型性能。

Abstract: We introduce SLR, an end-to-end framework for systematic evaluation and
training of Large Language Models (LLMs) via Scalable Logical Reasoning. Given
a user's task specification, SLR enables scalable, automated synthesis of
inductive reasoning tasks with precisely controlled difficulty. For each task,
SLR synthesizes (i) a latent ground-truth rule, (ii) an executable validation
program used by a symbolic judge to deterministically verify model outputs, and
(iii) an instruction prompt for the reasoning task. Using SLR, we create
SLR-Bench, a benchmark comprising over 19k prompts spanning 20 curriculum
levels that progressively increase in relational, arithmetic, and recursive
complexity. Large-scale evaluation reveals that contemporary LLMs readily
produce syntactically valid rules, yet often fail at correct logical inference.
Recent reasoning LLMs do somewhat better, but incur substantial increases in
test-time compute, sometimes exceeding 15k completion tokens. Finally,
logic-tuning via SLR doubles Llama-3-8B accuracy on SLR-Bench, achieving parity
with Gemini-Flash-Thinking at a fraction of computational cost. SLR is fully
automated, requires no human annotation, ensures dataset novelty, and offers a
scalable environment for probing and advancing LLMs' reasoning capabilities.

</details>


### [60] [Deep Reinforcement Learning Xiangqi Player with Monte Carlo Tree Search](https://arxiv.org/abs/2506.15880)
*Berk Yilmaz,Junyu Hu,Jinsong Liu*

Main category: cs.AI

TL;DR: 本文提出了一种结合深度强化学习（DRL）和蒙特卡洛树搜索（MCTS）的象棋AI系统，通过策略-价值网络和MCTS优化决策，解决了象棋的高分支因子和非对称棋子动态等挑战。


<details>
  <summary>Details</summary>
Motivation: 象棋作为一种文化意义重大的策略游戏，其独特的棋盘布局、棋子移动限制和胜利条件尚未被充分探索。本文旨在通过DRL-MCTS框架提升AI在此类游戏中的能力。

Method: 结合策略-价值网络与MCTS，模拟走棋后果并优化决策，同时解决象棋的高分支因子和非对称棋子动态问题。

Result: 该系统在象棋中实现了战略自我对弈和自我提升，为DRL-MCTS框架在特定领域规则系统的适应性提供了新见解。

Conclusion: 本研究不仅提升了AI在象棋中的表现，还为其他复杂策略游戏的AI开发提供了参考。

Abstract: This paper presents a Deep Reinforcement Learning (DRL) system for Xiangqi
(Chinese Chess) that integrates neural networks with Monte Carlo Tree Search
(MCTS) to enable strategic self-play and self-improvement. Addressing the
underexplored complexity of Xiangqi, including its unique board layout, piece
movement constraints, and victory conditions, our approach combines
policy-value networks with MCTS to simulate move consequences and refine
decision-making. By overcoming challenges such as Xiangqi's high branching
factor and asymmetrical piece dynamics, our work advances AI capabilities in
culturally significant strategy games while providing insights for adapting
DRL-MCTS frameworks to domain-specific rule systems.

</details>


### [61] [Exploring Big Five Personality and AI Capability Effects in LLM-Simulated Negotiation Dialogues](https://arxiv.org/abs/2506.15928)
*Myke C. Cohen,Zhe Su,Hsien-Te Kao,Daniel Nguyen,Spencer Lynch,Maarten Sap,Svitlana Volkova*

Main category: cs.AI

TL;DR: 本文提出了一种用于评估关键任务谈判场景中代理AI系统的框架，通过实验分析了人格特质和AI特性对谈判结果的影响。


<details>
  <summary>Details</summary>
Motivation: 研究旨在解决AI代理在多样化人类操作者和利益相关者中适应性不足的问题，特别是在高风险的跨团队协调和军民互动场景中。

Method: 使用Sotopia作为模拟测试平台，通过两个实验系统评估人格特质和AI特性对LLM模拟社会谈判结果的影响，包括价格谈判和人类-AI工作谈判。

Result: 实验发现，Agreeableness和Extraversion显著影响谈判结果；AI代理的透明度、能力和适应性对任务有效性有重要影响。

Conclusion: 研究提出了一种可重复的评估方法，支持在多样化操作者人格和人机团队动态中测试AI代理可靠性，为复杂任务中的AI系统设计提供了重要参考。

Abstract: This paper presents an evaluation framework for agentic AI systems in
mission-critical negotiation contexts, addressing the need for AI agents that
can adapt to diverse human operators and stakeholders. Using Sotopia as a
simulation testbed, we present two experiments that systematically evaluated
how personality traits and AI agent characteristics influence LLM-simulated
social negotiation outcomes--a capability essential for a variety of
applications involving cross-team coordination and civil-military interactions.
Experiment 1 employs causal discovery methods to measure how personality traits
impact price bargaining negotiations, through which we found that Agreeableness
and Extraversion significantly affect believability, goal achievement, and
knowledge acquisition outcomes. Sociocognitive lexical measures extracted from
team communications detected fine-grained differences in agents' empathic
communication, moral foundations, and opinion patterns, providing actionable
insights for agentic AI systems that must operate reliably in high-stakes
operational scenarios. Experiment 2 evaluates human-AI job negotiations by
manipulating both simulated human personality and AI system characteristics,
specifically transparency, competence, adaptability, demonstrating how AI agent
trustworthiness impact mission effectiveness. These findings establish a
repeatable evaluation methodology for experimenting with AI agent reliability
across diverse operator personalities and human-agent team dynamics, directly
supporting operational requirements for reliable AI systems. Our work advances
the evaluation of agentic AI workflows by moving beyond standard performance
metrics to incorporate social dynamics essential for mission success in complex
operations.

</details>


### [62] [Bayesian Epistemology with Weighted Authority: A Formal Architecture for Truth-Promoting Autonomous Scientific Reasoning](https://arxiv.org/abs/2506.16015)
*Craig S. Wright*

Main category: cs.AI

TL;DR: BEWA是一种基于贝叶斯推理的动态信念架构，用于处理科学文献中的结构化声明，支持作者可信度建模和可验证性。


<details>
  <summary>Details</summary>
Motivation: 科学文献的爆炸式增长超出了人类和AI系统的处理能力，需要一种新的方法来动态评估和更新科学信念。

Method: BEWA通过贝叶斯推理、复制评分、引用加权和时间衰减机制，构建了一个结构化的科学声明网络。

Result: BEWA能够支持基于图的声明传播、作者可信度建模和可验证性，提升机器推理系统的真理效用和完整性。

Conclusion: BEWA为动态科学领域中的机器推理系统提供了新的基础，促进理性信念收敛和可审计的完整性。

Abstract: The exponential expansion of scientific literature has surpassed the
epistemic processing capabilities of both human experts and current artificial
intelligence systems. This paper introduces Bayesian Epistemology with Weighted
Authority (BEWA), a formally structured architecture that operationalises
belief as a dynamic, probabilistically coherent function over structured
scientific claims. Each claim is contextualised, author-attributed, and
evaluated through a system of replication scores, citation weighting, and
temporal decay. Belief updates are performed via evidence-conditioned Bayesian
inference, contradiction processing, and epistemic decay mechanisms. The
architecture supports graph-based claim propagation, authorial credibility
modelling, cryptographic anchoring, and zero-knowledge audit verification. By
formalising scientific reasoning into a computationally verifiable epistemic
network, BEWA advances the foundation for machine reasoning systems that
promote truth utility, rational belief convergence, and audit-resilient
integrity across dynamic scientific domains.

</details>


### [63] [Dual-Objective Reinforcement Learning with Novel Hamilton-Jacobi-Bellman Formulations](https://arxiv.org/abs/2506.16016)
*William Sharpless,Dylan Hirsch,Sander Tonkens,Nikhil Shinde,Sylvia Herbert*

Main category: cs.AI

TL;DR: 论文提出两种新的价值函数，用于解决强化学习中的双目标约束问题，并通过改进的PPO算法（DO-HJ-PPO）验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 传统强化学习中的硬约束（通过奖励函数或模型架构实现）常导致策略性能下降，而拉格朗日方法需要复杂的奖励工程和参数调优。

Method: 通过连接Hamilton-Jacobi方程与强化学习，提出两种新的价值函数，分别解决Reach-Always-Avoid和Reach-Reach问题，并分解为可达、避免和可达-避免子问题。

Result: DO-HJ-PPO在安全到达和多目标达成任务中表现优于基线方法，并产生独特的行为。

Conclusion: 论文为约束决策问题提供了新视角，并通过DO-HJ-PPO展示了其实际效果。

Abstract: Hard constraints in reinforcement learning (RL), whether imposed via the
reward function or the model architecture, often degrade policy performance.
Lagrangian methods offer a way to blend objectives with constraints, but often
require intricate reward engineering and parameter tuning. In this work, we
extend recent advances that connect Hamilton-Jacobi (HJ) equations with RL to
propose two novel value functions for dual-objective satisfaction. Namely, we
address: (1) the Reach-Always-Avoid problem - of achieving distinct reward and
penalty thresholds - and (2) the Reach-Reach problem - of achieving thresholds
of two distinct rewards. In contrast with temporal logic approaches, which
typically involve representing an automaton, we derive explicit, tractable
Bellman forms in this context by decomposing our problem into reach, avoid, and
reach-avoid problems, as to leverage these aforementioned recent advances. From
a mathematical perspective, the Reach-Always-Avoid and Reach-Reach problems are
complementary and fundamentally different from standard sum-of-rewards problems
and temporal logic problems, providing a new perspective on constrained
decision-making. We leverage our analysis to propose a variation of Proximal
Policy Optimization (DO-HJ-PPO), which solves these problems. Across a range of
tasks for safe-arrival and multi-target achievement, we demonstrate that
DO-HJ-PPO produces qualitatively distinct behaviors from previous approaches
and out-competes a number of baselines in various metrics.

</details>


### [64] [OSWorld-Human: Benchmarking the Efficiency of Computer-Use Agents](https://arxiv.org/abs/2506.16042)
*Reyna Abhyankar,Qi Qi,Yiying Zhang*

Main category: cs.AI

TL;DR: 生成式AI在解决桌面应用任务时存在高延迟问题，研究发现规划和反思的大模型调用是主要原因，且任务步骤越多，后续步骤耗时越长。通过构建OSWorld-Human数据集，发现现有代理效率低下。


<details>
  <summary>Details</summary>
Motivation: 研究生成式AI在桌面应用任务中的高延迟问题，以指导未来计算机代理的发展。

Method: 在OSWorld基准上研究计算机代理的时间性能，构建OSWorld-Human数据集，评估16个代理的效率。

Result: 规划和反思的大模型调用是延迟主因，任务步骤越多耗时越长；现有代理步骤冗余，效率低下。

Conclusion: 需优化模型调用和任务步骤设计，提升计算机代理的实用性和效率。

Abstract: Generative AI is being leveraged to solve a variety of computer-use tasks
involving desktop applications. State-of-the-art systems have focused solely on
improving accuracy on leading benchmarks. However, these systems are
practically unusable due to extremely high end-to-end latency (e.g., tens of
minutes) for tasks that typically take humans just a few minutes to complete.
To understand the cause behind this and to guide future developments of
computer agents, we conduct the first study on the temporal performance of
computer-use agents on OSWorld, the flagship benchmark in computer-use AI. We
find that large model calls for planning and reflection account for the
majority of the overall latency, and as an agent uses more steps to complete a
task, each successive step can take 3x longer than steps at the beginning of a
task. We then construct OSWorld-Human, a manually annotated version of the
original OSWorld dataset that contains a human-determined trajectory for each
task. We evaluate 16 agents on their efficiency using OSWorld-Human and found
that even the highest-scoring agents on OSWorld take 1.4-2.7x more steps than
necessary.

</details>


### [65] [Consistency Verification in Ontology-Based Process Models with Parameter Interdependencies](https://arxiv.org/abs/2506.16087)
*Tom Jeleniewski,Hamied Nabizada,Jonathan Reif,Felix Gehlhoff,Alexander Fay*

Main category: cs.AI

TL;DR: 该论文提出了一套验证机制，用于基于本体的工艺模型，支持跨上下文应用和知识重用，确保数据检索和解释的正确性。


<details>
  <summary>Details</summary>
Motivation: 制造过程中参数间依赖关系的数学表达需要通用且语义一致的模型，以确保数据检索和解释的正确性。

Method: 采用SPARQL过滤、单位一致性检查和数据完整性验证三种机制，结合标准化工艺语义和数学构造。

Result: 通过树脂传递模塑（RTM）的用例验证了方法的适用性，支持机器可解释和可验证的工程模型开发。

Conclusion: 提出的验证机制有效解决了工艺模型中数据选择、单位兼容性和完整性等关键挑战。

Abstract: The formalization of process knowledge using ontologies enables consistent
modeling of parameter interdependencies in manufacturing. These
interdependencies are typically represented as mathematical expressions that
define relations between process parameters, supporting tasks such as
calculation, validation, and simulation. To support cross-context application
and knowledge reuse, such expressions are often defined in a generic form and
applied across multiple process contexts. This highlights the necessity of a
consistent and semantically coherent model to ensure the correctness of data
retrieval and interpretation. Consequently, dedicated mechanisms are required
to address key challenges such as selecting context-relevant data, ensuring
unit compatibility between variables and data elements, and verifying the
completeness of input data required for evaluating mathematical expressions.
This paper presents a set of verification mechanisms for a previously developed
ontology-based process model that integrates standardized process semantics,
data element definitions, and formal mathematical constructs. The approach
includes (i) SPARQL-based filtering to retrieve process-relevant data, (ii) a
unit consistency check based on expected-unit annotations and semantic
classification, and (iii) a data completeness check to validate the
evaluability of interdependencies. The applicability of the approach is
demonstrated with a use case from Resin Transfer Molding (RTM), supporting the
development of machine-interpretable and verifiable engineering models.

</details>


### [66] [Geometric Learning in Black-Box Optimization: A GNN Framework for Algorithm Performance Prediction](https://arxiv.org/abs/2506.16144)
*Ana Kostovska,Carola Doerr,Sašo Džeroski,Panče Panov,Tome Eftimov*

Main category: cs.AI

TL;DR: 该论文提出使用异构图数据结构和图神经网络预测优化算法性能，通过捕捉问题、算法配置和性能之间的复杂依赖关系，相比传统表格方法提升了36.6%的MSE。


<details>
  <summary>Details</summary>
Motivation: 传统方法依赖问题特征作为机器学习输入，但忽略了算法配置对性能的影响，而问题、算法和性能之间的关系更适合用图表示。

Method: 采用异构图数据结构和图神经网络，对两种模块化框架（modCMA-ES和modDE）的变体在BBOB问题上进行评估。

Result: 在324种modCMA-ES和576种modDE变体上测试，相比传统方法，MSE提升了36.6%。

Conclusion: 几何学习在黑盒优化中具有潜力，图结构能更好地捕捉复杂依赖关系。

Abstract: Automated algorithm performance prediction in numerical blackbox optimization
often relies on problem characterizations, such as exploratory landscape
analysis features. These features are typically used as inputs to machine
learning models and are represented in a tabular format. However, such
approaches often overlook algorithm configurations, a key factor influencing
performance. The relationships between algorithm operators, parameters, problem
characteristics, and performance outcomes form a complex structure best
represented as a graph. This work explores the use of heterogeneous graph data
structures and graph neural networks to predict the performance of optimization
algorithms by capturing the complex dependencies between problems, algorithm
configurations, and performance outcomes. We focus on two modular frameworks,
modCMA-ES and modDE, which decompose two widely used derivative-free
optimization algorithms: the covariance matrix adaptation evolution strategy
(CMA-ES) and differential evolution (DE). We evaluate 324 modCMA-ES and 576
modDE variants on 24 BBOB problems across six runtime budgets and two problem
dimensions. Achieving up to 36.6% improvement in MSE over traditional
tabular-based methods, this work highlights the potential of geometric learning
in black-box optimization.

</details>


### [67] [Large Language Models are Near-Optimal Decision-Makers with a Non-Human Learning Behavior](https://arxiv.org/abs/2506.16163)
*Hao Li,Gengrui Zhang,Petter Holme,Shuyue Hu,Zhen Wang*

Main category: cs.AI

TL;DR: 研究表明，大型语言模型（LLMs）在不确定性、风险和适应性决策任务中表现优于人类，但其决策过程与人类存在根本差异，引发对其替代人类判断的潜在风险的关注。


<details>
  <summary>Details</summary>
Motivation: 探索LLMs与人类在决策行为上的差异，以理解其潜在优势和风险。

Method: 使用三种心理学任务测试五个领先LLMs和360名人类参与者在不确定性、风险和适应性决策中的表现。

Result: LLMs在任务中表现优于人类，接近最优水平，但其决策过程与人类显著不同。

Conclusion: LLMs在决策任务中表现出色，但其与人类决策的差异提示需谨慎依赖其替代人类判断，需进一步研究。

Abstract: Human decision-making belongs to the foundation of our society and
civilization, but we are on the verge of a future where much of it will be
delegated to artificial intelligence. The arrival of Large Language Models
(LLMs) has transformed the nature and scope of AI-supported decision-making;
however, the process by which they learn to make decisions, compared to humans,
remains poorly understood. In this study, we examined the decision-making
behavior of five leading LLMs across three core dimensions of real-world
decision-making: uncertainty, risk, and set-shifting. Using three
well-established experimental psychology tasks designed to probe these
dimensions, we benchmarked LLMs against 360 newly recruited human participants.
Across all tasks, LLMs often outperformed humans, approaching near-optimal
performance. Moreover, the processes underlying their decisions diverged
fundamentally from those of humans. On the one hand, our finding demonstrates
the ability of LLMs to manage uncertainty, calibrate risk, and adapt to
changes. On the other hand, this disparity highlights the risks of relying on
them as substitutes for human judgment, calling for further inquiry.

</details>


### [68] [Approximation Fixpoint Theory with Refined Approximation Spaces](https://arxiv.org/abs/2506.16294)
*Linde Vanbesien,Bart Bogaerts,Marc Denecker*

Main category: cs.AI

TL;DR: 本文扩展了近似不动点理论（AFT），通过引入更一般的近似空间概念，克服了其在某些简单例子中的局限性。


<details>
  <summary>Details</summary>
Motivation: AFT在非单调推理形式中广泛应用，但在某些情况下表现有限，需要更精细的近似方法。

Method: 提出更一般的近似空间概念，研究其表达能力和不同近似空间之间的关系。

Result: 展示了改进的表达能力和更广泛的适用性。

Conclusion: 扩展的AFT为处理更复杂的非单调推理问题提供了更强大的工具。

Abstract: Approximation Fixpoint Theory (AFT) is a powerful theory covering various
semantics of non-monotonic reasoning formalisms in knowledge representation
such as Logic Programming and Answer Set Programming. Many semantics of such
non-monotonic formalisms can be characterized as suitable fixpoints of a
non-monotonic operator on a suitable lattice. Instead of working on the
original lattice, AFT operates on intervals in such lattice to approximate or
construct the fixpoints of interest. While AFT has been applied successfully
across a broad range of non-monotonic reasoning formalisms, it is confronted by
its limitations in other, relatively simple, examples. In this paper, we
overcome those limitations by extending consistent AFT to deal with
approximations that are more refined than intervals. Therefore, we introduce a
more general notion of approximation spaces, showcase the improved
expressiveness and investigate relations between different approximation
spaces.

</details>


### [69] [Explainable Rule Application via Structured Prompting: A Neural-Symbolic Approach](https://arxiv.org/abs/2506.16335)
*Albert Sadowski,Jarosław A. Chudziak*

Main category: cs.AI

TL;DR: 论文提出了一种结构化提示框架，通过分解推理步骤（实体识别、属性提取和符号规则应用）结合神经与符号方法，提升LLMs在逻辑一致性任务中的表现。


<details>
  <summary>Details</summary>
Motivation: LLMs在复杂推理任务中表现优异，但在规则一致性、异常处理和可解释性方面存在不足，尤其在需要自然语言理解和精确逻辑推理的领域（如法律分析）。

Method: 采用结构化提示框架，将推理分解为三个可验证步骤，结合神经与符号方法，并通过形式化验证确保逻辑一致性。

Result: 在LegalBench hearsay任务中，该方法显著优于基线模型，如o1模型的F1分数从0.714提升至0.929。

Conclusion: 该混合神经符号系统为透明且一致的基于规则的推理提供了可行路径，适用于结构化法律推理任务中的可解释AI应用。

Abstract: Large Language Models (LLMs) excel in complex reasoning tasks but struggle
with consistent rule application, exception handling, and explainability,
particularly in domains like legal analysis that require both natural language
understanding and precise logical inference. This paper introduces a structured
prompting framework that decomposes reasoning into three verifiable steps:
entity identification, property extraction, and symbolic rule application. By
integrating neural and symbolic approaches, our method leverages LLMs'
interpretive flexibility while ensuring logical consistency through formal
verification. The framework externalizes task definitions, enabling domain
experts to refine logical structures without altering the architecture.
Evaluated on the LegalBench hearsay determination task, our approach
significantly outperformed baselines, with OpenAI o-family models showing
substantial improvements - o1 achieving an F1 score of 0.929 and o3-mini
reaching 0.867 using structured decomposition with complementary predicates,
compared to their few-shot baselines of 0.714 and 0.74 respectively. This
hybrid neural-symbolic system offers a promising pathway for transparent and
consistent rule-based reasoning, suggesting potential for explainable AI
applications in structured legal reasoning tasks.

</details>


### [70] [IS-Bench: Evaluating Interactive Safety of VLM-Driven Embodied Agents in Daily Household Tasks](https://arxiv.org/abs/2506.16402)
*Xiaoya Lu,Zeren Chen,Xuhao Hu,Yijin Zhou,Weichen Zhang,Dongrui Liu,Lu Sheng,Jing Shao*

Main category: cs.AI

TL;DR: IS-Bench是一个多模态基准测试，用于评估VLM驱动的具身代理的交互安全性，揭示当前代理在动态风险感知和缓解能力上的不足。


<details>
  <summary>Details</summary>
Motivation: 现有静态评估范式无法模拟动态风险，导致具身代理在真实家庭任务中存在安全隐患。

Method: 提出IS-Bench基准，包含161个场景和388个安全风险，支持过程导向的评估。

Result: 实验显示当前VLM代理缺乏交互安全意识，安全感知的Chain-of-Thought虽能提升性能，但可能影响任务完成。

Conclusion: IS-Bench为开发更安全的具身AI系统奠定了基础。

Abstract: Flawed planning from VLM-driven embodied agents poses significant safety
hazards, hindering their deployment in real-world household tasks. However,
existing static, non-interactive evaluation paradigms fail to adequately assess
risks within these interactive environments, since they cannot simulate dynamic
risks that emerge from an agent's actions and rely on unreliable post-hoc
evaluations that ignore unsafe intermediate steps. To bridge this critical gap,
we propose evaluating an agent's interactive safety: its ability to perceive
emergent risks and execute mitigation steps in the correct procedural order. We
thus present IS-Bench, the first multi-modal benchmark designed for interactive
safety, featuring 161 challenging scenarios with 388 unique safety risks
instantiated in a high-fidelity simulator. Crucially, it facilitates a novel
process-oriented evaluation that verifies whether risk mitigation actions are
performed before/after specific risk-prone steps. Extensive experiments on
leading VLMs, including the GPT-4o and Gemini-2.5 series, reveal that current
agents lack interactive safety awareness, and that while safety-aware
Chain-of-Thought can improve performance, it often compromises task completion.
By highlighting these critical limitations, IS-Bench provides a foundation for
developing safer and more reliable embodied AI systems.

</details>


### [71] [Agentic Personalisation of Cross-Channel Marketing Experiences](https://arxiv.org/abs/2506.16429)
*Sami Abboud,Eleanor Hanna,Olivier Jeunen,Vineesha Raheja,Schaun Wheeler*

Main category: cs.AI

TL;DR: 论文提出了一种基于序列决策框架的自动化通信编排方法，取代传统的人工营销工作，通过个性化优化提升用户参与度。


<details>
  <summary>Details</summary>
Motivation: 传统通信编排依赖人工操作，难以实现内容、时间、频率和文案的个性化优化，限制了效果。

Method: 采用差分设计估计个体处理效应，结合Thompson采样平衡探索与利用，优化模块化决策策略。

Result: 在多服务应用中显著提升了多种目标事件的参与度，已部署于1.5亿用户。

Conclusion: 自动化通信编排方法能有效提升用户参与度，具有广泛的应用潜力。

Abstract: Consumer applications provide ample opportunities to surface and communicate
various forms of content to users. From promotional campaigns for new features
or subscriptions, to evergreen nudges for engagement, or personalised
recommendations; across e-mails, push notifications, and in-app surfaces. The
conventional approach to orchestration for communication relies heavily on
labour-intensive manual marketer work, and inhibits effective personalisation
of content, timing, frequency, and copy-writing. We formulate this task under a
sequential decision-making framework, where we aim to optimise a modular
decision-making policy that maximises incremental engagement for any funnel
event. Our approach leverages a Difference-in-Differences design for Individual
Treatment Effect estimation, and Thompson sampling to balance the
explore-exploit trade-off. We present results from a multi-service application,
where our methodology has resulted in significant increases to a variety of
goal events across several product features, and is currently deployed across
150 million users.

</details>


### [72] [ML-Master: Towards AI-for-AI via Integration of Exploration and Reasoning](https://arxiv.org/abs/2506.16499)
*Zexi Liu,Yuzhu Cai,Xinyu Zhu,Yujie Zheng,Runkun Chen,Ying Wen,Yanfeng Wang,Weinan E,Siheng Chen*

Main category: cs.AI

TL;DR: ML-Master是一种新型AI4AI代理，通过选择性记忆机制整合探索与推理，显著提升AI系统设计效率。


<details>
  <summary>Details</summary>
Motivation: AI驱动的开发可能超越人类效率，但现有LLM代理无法充分利用探索经验，导致性能不足。

Method: 提出ML-Master，采用选择性记忆机制，高效结合并行解决方案的多样性与分析推理。

Result: 在MLE-Bench上，ML-Master平均奖牌率提升29.3%，且在12小时内完成，优于基线方法。

Conclusion: ML-Master展示了作为AI4AI强大工具的潜力，显著提升任务性能与效率。

Abstract: As AI capabilities advance toward and potentially beyond human-level
performance, a natural transition emerges where AI-driven development becomes
more efficient than human-centric approaches. A promising pathway toward this
transition lies in AI-for-AI (AI4AI), which leverages AI techniques to automate
and optimize the design, training, and deployment of AI systems themselves.
While LLM-based agents have shown the potential to realize AI4AI, they are
often unable to fully leverage the experience accumulated by agents during the
exploration of solutions in the reasoning process, leading to inefficiencies
and suboptimal performance. To address this limitation, we propose ML-Master, a
novel AI4AI agent that seamlessly integrates exploration and reasoning by
employing a selectively scoped memory mechanism. This approach allows ML-Master
to efficiently combine diverse insights from parallel solution trajectories
with analytical reasoning, guiding further exploration without overwhelming the
agent with excessive context. We evaluate ML-Master on the MLE-Bench, where it
achieves a 29.3% average medal rate, significantly surpassing existing methods,
particularly in medium-complexity tasks, while accomplishing this superior
performance within a strict 12-hour time constraint-half the 24-hour limit used
by previous baselines. These results demonstrate ML-Master's potential as a
powerful tool for advancing AI4AI.

</details>


### [73] [Advancing Harmful Content Detection in Organizational Research: Integrating Large Language Models with Elo Rating System](https://arxiv.org/abs/2506.16575)
*Mustafa Akben,Aaron Satko*

Main category: cs.AI

TL;DR: 本文提出了一种基于Elo评级的改进方法，显著提升了大语言模型（LLM）在有害内容分析中的性能，尤其在微侵犯和仇恨言论检测方面表现优于传统方法。


<details>
  <summary>Details</summary>
Motivation: LLM的内置审核系统在分析有害内容时可能导致结果失真，影响研究有效性，尤其是在组织冲突（如微侵犯或仇恨言论）分析中。

Method: 采用基于Elo评级的方法，优化LLM对有害内容的分析能力。

Result: 在两个数据集（微侵犯检测和仇恨言论）中，该方法在准确性、精确度和F1分数上均优于传统LLM提示技术和常规机器学习模型。

Conclusion: 该方法提高了有害内容分析的可靠性，减少了误报，适用于大规模数据集，有助于构建更安全、包容的工作环境。

Abstract: Large language models (LLMs) offer promising opportunities for organizational
research. However, their built-in moderation systems can create problems when
researchers try to analyze harmful content, often refusing to follow certain
instructions or producing overly cautious responses that undermine validity of
the results. This is particularly problematic when analyzing organizational
conflicts such as microaggressions or hate speech. This paper introduces an Elo
rating-based method that significantly improves LLM performance for harmful
content analysis In two datasets, one focused on microaggression detection and
the other on hate speech, we find that our method outperforms traditional LLM
prompting techniques and conventional machine learning models on key measures
such as accuracy, precision, and F1 scores. Advantages include better
reliability when analyzing harmful content, fewer false positives, and greater
scalability for large-scale datasets. This approach supports organizational
applications, including detecting workplace harassment, assessing toxic
communication, and fostering safer and more inclusive work environments.

</details>


### [74] [A Community-driven vision for a new Knowledge Resource for AI](https://arxiv.org/abs/2506.16596)
*Vinay K Chaudhri,Chaitan Baru,Brandon Bennett,Mehul Bhatt,Darion Cassel,Anthony G Cohn,Rina Dechter,Esra Erdem,Dave Ferrucci,Ken Forbus,Gregory Gelfond,Michael Genesereth,Andrew S. Gordon,Benjamin Grosof,Gopal Gupta,Jim Hendler,Sharat Israni,Tyler R. Josephson,Patrick Kyllonen,Yuliya Lierler,Vladimir Lifschitz,Clifton McFate,Hande K. McGinty,Leora Morgenstern,Alessandro Oltramari,Praveen Paritosh,Dan Roth,Blake Shepard,Cogan Shimzu,Denny Vrandečić,Mark Whiting,Michael Witbrock*

Main category: cs.AI

TL;DR: 论文探讨了AI领域对综合性、多用途知识资源的需求，提出了社区驱动的知识基础设施愿景，并建议利用现代技术构建开放工程框架。


<details>
  <summary>Details</summary>
Motivation: 当前AI领域缺乏可验证、通用的知识资源，导致语言模型、机器人规划和事实检测等任务受限。

Method: 通过AAAI研讨会收集50多名研究者的意见，结合知识表示与推理的现代进展，提出开放工程框架的构想。

Result: 提出了一种社区驱动的知识基础设施愿景，强调开放框架和知识模块的有效利用。

Conclusion: 构建开放的知识工程框架是解决AI知识资源短缺的关键，需社区协作和现代技术支持。

Abstract: The long-standing goal of creating a comprehensive, multi-purpose knowledge
resource, reminiscent of the 1984 Cyc project, still persists in AI. Despite
the success of knowledge resources like WordNet, ConceptNet, Wolfram|Alpha and
other commercial knowledge graphs, verifiable, general-purpose widely available
sources of knowledge remain a critical deficiency in AI infrastructure. Large
language models struggle due to knowledge gaps; robotic planning lacks
necessary world knowledge; and the detection of factually false information
relies heavily on human expertise. What kind of knowledge resource is most
needed in AI today? How can modern technology shape its development and
evaluation? A recent AAAI workshop gathered over 50 researchers to explore
these questions. This paper synthesizes our findings and outlines a
community-driven vision for a new knowledge infrastructure. In addition to
leveraging contemporary advances in knowledge representation and reasoning, one
promising idea is to build an open engineering framework to exploit knowledge
modules effectively within the context of practical applications. Such a
framework should include sets of conventions and social structures that are
adopted by contributors.

</details>


### [75] [The Role of Explanation Styles and Perceived Accuracy on Decision Making in Predictive Process Monitoring](https://arxiv.org/abs/2506.16617)
*Soobin Chae,Suhwan Lee,Hanna Hauptmann,Hajo A. Reijers,Xixi Lu*

Main category: cs.AI

TL;DR: 该研究探讨了解释风格和感知AI准确性对预测过程监控（PPM）中决策的影响，发现两者对任务表现和决策信心有显著影响。


<details>
  <summary>Details</summary>
Motivation: 尽管深度学习模型在PPM中预测准确性高，但其缺乏可解释性降低了用户信任。现有XAI评估多关注功能指标，忽视了用户中心视角。

Method: 通过决策实验，研究不同解释风格（特征重要性、基于规则、反事实）和感知AI准确性（高或低）对用户决策的影响。

Result: 感知准确性和解释风格对任务表现、一致性和决策信心有显著影响。

Conclusion: 研究强调了在PPM中结合用户中心视角评估XAI的重要性，并展示了解释风格和感知准确性的实际影响。

Abstract: Predictive Process Monitoring (PPM) often uses deep learning models to
predict the future behavior of ongoing processes, such as predicting process
outcomes. While these models achieve high accuracy, their lack of
interpretability undermines user trust and adoption. Explainable AI (XAI) aims
to address this challenge by providing the reasoning behind the predictions.
However, current evaluations of XAI in PPM focus primarily on functional
metrics (such as fidelity), overlooking user-centered aspects such as their
effect on task performance and decision-making. This study investigates the
effects of explanation styles (feature importance, rule-based, and
counterfactual) and perceived AI accuracy (low or high) on decision-making in
PPM. We conducted a decision-making experiment, where users were presented with
the AI predictions, perceived accuracy levels, and explanations of different
styles. Users' decisions were measured both before and after receiving
explanations, allowing the assessment of objective metrics (Task Performance
and Agreement) and subjective metrics (Decision Confidence). Our findings show
that perceived accuracy and explanation style have a significant effect.

</details>


### [76] [Interpretable Low-Dimensional Modeling of Spatiotemporal Agent States for Decision Making in Football Tactics](https://arxiv.org/abs/2506.16696)
*Kenjiro Ide,Taiga Someya,Kohei Kawaguchi,Keisuke Fujii*

Main category: cs.AI

TL;DR: 研究探讨了低维、基于规则的模型是否能有效捕捉足球战术，通过可解释的状态变量和实际数据训练模型，发现球员与球的距离及空间评分是关键因素。


<details>
  <summary>Details</summary>
Motivation: 现有模型计算成本高或缺乏可解释性，且未全面考虑球员状态，需开发更高效且直观的方法。

Method: 定义球持有者和接球者的状态变量，结合专家知识，使用XGBoost模型预测传球成功率。

Result: 球员与球的距离及空间评分是传球成功的关键因素。

Conclusion: 低维模型通过直观变量支持战术分析和决策，具有实用价值。

Abstract: Understanding football tactics is crucial for managers and analysts. Previous
research has proposed models based on spatial and kinematic equations, but
these are computationally expensive. Also, Reinforcement learning approaches
use player positions and velocities but lack interpretability and require large
datasets. Rule-based models align with expert knowledge but have not fully
considered all players' states. This study explores whether low-dimensional,
rule-based models using spatiotemporal data can effectively capture football
tactics. Our approach defines interpretable state variables for both the
ball-holder and potential pass receivers, based on criteria that explore
options like passing. Through discussions with a manager, we identified key
variables representing the game state. We then used StatsBomb event data and
SkillCorner tracking data from the 2023$/$24 LaLiga season to train an XGBoost
model to predict pass success. The analysis revealed that the distance between
the player and the ball, as well as the player's space score, were key factors
in determining successful passes. Our interpretable low-dimensional modeling
facilitates tactical analysis through the use of intuitive variables and
provides practical value as a tool to support decision-making in football.

</details>


### [77] [Incentivizing High-quality Participation From Federated Learning Agents](https://arxiv.org/abs/2506.16731)
*Jinlong Pang,Jiaheng Wei,Yifan Hua,Chen Qian,Yang Liu*

Main category: cs.AI

TL;DR: 本文提出了一种激励感知的联邦学习框架，解决了自利代理参与和数据异构性问题，通过Wasserstein距离和Stackelberg博弈模型优化收敛过程。


<details>
  <summary>Details</summary>
Motivation: 现有联邦学习研究假设代理自愿无私参与，但实际中自利代理可能退出或提供低质量贡献，且数据异构性导致聚合模型效果不佳。

Method: 引入Wasserstein距离量化数据异构性，利用对等预测机制设计评分函数，提出两阶段Stackelberg博弈模型。

Result: 在真实数据集上的实验验证了所提机制的有效性。

Conclusion: 该框架通过激励设计和数据异构性处理，显著提升了联邦学习的收敛速度和模型质量。

Abstract: Federated learning (FL) provides a promising paradigm for facilitating
collaboration between multiple clients that jointly learn a global model
without directly sharing their local data. However, existing research suffers
from two caveats: 1) From the perspective of agents, voluntary and unselfish
participation is often assumed. But self-interested agents may opt out of the
system or provide low-quality contributions without proper incentives; 2) From
the mechanism designer's perspective, the aggregated models can be
unsatisfactory as the existing game-theoretical federated learning approach for
data collection ignores the potential heterogeneous effort caused by
contributed data. To alleviate above challenges, we propose an incentive-aware
framework for agent participation that considers data heterogeneity to
accelerate the convergence process. Specifically, we first introduce the notion
of Wasserstein distance to explicitly illustrate the heterogeneous effort and
reformulate the existing upper bound of convergence. To induce truthful
reporting from agents, we analyze and measure the generalization error gap of
any two agents by leveraging the peer prediction mechanism to develop score
functions. We further present a two-stage Stackelberg game model that
formalizes the process and examines the existence of equilibrium. Extensive
experiments on real-world datasets demonstrate the effectiveness of our
proposed mechanism.

</details>


### [78] [Reinforcement learning for hybrid charging stations planning and operation considering fixed and mobile chargers](https://arxiv.org/abs/2506.16764)
*Yanchen Zhu,Honghui Zou,Chufan Liu,Yuyu Luo,Yuankai Wu,Yuxuan Liang*

Main category: cs.AI

TL;DR: 论文提出了一种混合充电基础设施（固定和移动充电桩）的优化规划与运营方法（HCSPO），结合需求预测和深度强化学习，显著提升了充电资源利用率和用户体验。


<details>
  <summary>Details</summary>
Motivation: 传统固定充电桩因需求动态性导致利用率不均或拥堵，移动充电桩的灵活性可以解决这一问题，但需要优化规划与运营。

Method: 提出HCSPO问题，结合固定和移动充电桩的规划与调度，使用基于MPC的需求预测和深度强化学习方法。

Result: 在真实城市场景中，该方法显著提高了充电资源可用性并减少了用户不便。

Conclusion: 混合充电基础设施的优化方法有效解决了动态需求问题，为电动车充电网络提供了高效解决方案。

Abstract: The success of vehicle electrification, which brings significant societal and
environmental benefits, is contingent upon the availability of efficient and
adaptable charging infrastructure. Traditional fixed-location charging stations
often face issues like underutilization or congestion due to the dynamic nature
of charging demand. Mobile chargers have emerged as a flexible solution,
capable of relocating to align with these demand fluctuations. This paper
addresses the optimal planning and operation of hybrid charging
infrastructures, integrating both fixed and mobile chargers within urban road
networks. We introduce the Hybrid Charging Station Planning and Operation
(HCSPO) problem, which simultaneously optimizes the location and configuration
of fixed charging stations and schedules mobile chargers for dynamic
operations. Our approach incorporates a charging demand prediction model
grounded in Model Predictive Control (MPC) to enhance decision-making. To solve
the HCSPO problem, we propose a deep reinforcement learning method, augmented
with heuristic scheduling techniques, to effectively bridge the planning of
fixed chargers with the real-time operation of mobile chargers. Extensive case
studies using real-world urban scenarios demonstrate that our method
significantly improves the availability of charging infrastructure and reduces
user inconvenience compared to existing solutions and baselines.

</details>


### [79] [AI's Blind Spots: Geographic Knowledge and Diversity Deficit in Generated Urban Scenario](https://arxiv.org/abs/2506.16898)
*Ciro Beneduce,Massimiliano Luca,Bruno Lepri*

Main category: cs.AI

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Image generation models are revolutionizing many domains, and urban analysis
and design is no exception. While such models are widely adopted, there is a
limited literature exploring their geographic knowledge, along with the biases
they embed. In this work, we generated 150 synthetic images for each state in
the USA and related capitals using FLUX 1 and Stable Diffusion 3.5, two
state-of-the-art models for image generation. We embed each image using DINO-v2
ViT-S/14 and the Fr\'echet Inception Distances to measure the similarity
between the generated images. We found that while these models have implicitly
learned aspects of USA geography, if we prompt the models to generate an image
for "United States" instead of specific cities or states, the models exhibit a
strong representative bias toward metropolis-like areas, excluding rural states
and smaller cities. {\color{black} In addition, we found that models
systematically exhibit some entity-disambiguation issues with European-sounding
names like Frankfort or Devon.

</details>


### [80] [Real-Time Black-Box Optimization for Dynamic Discrete Environments Using Embedded Ising Machines](https://arxiv.org/abs/2506.16924)
*Tomoya Kashimata,Yohei Hamakawa,Masaya Yamasaki,Kosuke Tatsumura*

Main category: cs.AI

TL;DR: 提出了一种启发式多臂老虎机方法，用于动态离散环境，通过扩展黑盒优化方法，利用伊辛机有效探索动作。


<details>
  <summary>Details</summary>
Motivation: 动态离散环境（如实时系统）需要优化离散变量，传统多臂老虎机算法因组合爆炸问题难以有效优化。

Method: 扩展黑盒优化方法，利用伊辛机探索动作，同时考虑变量间交互和动态环境变化。

Result: 在移动用户的无线通信系统中验证了方法的动态适应性。

Conclusion: 该方法在动态离散环境中表现出色，解决了传统算法的局限性。

Abstract: Many real-time systems require the optimization of discrete variables.
Black-box optimization (BBO) algorithms and multi-armed bandit (MAB) algorithms
perform optimization by repeatedly taking actions and observing the
corresponding instant rewards without any prior knowledge. Recently, a BBO
method using an Ising machine has been proposed to find the best action that is
represented by a combination of discrete values and maximizes the instant
reward in static environments. In contrast, dynamic environments, where
real-time systems operate, necessitate MAB algorithms that maximize the average
reward over multiple trials. However, due to the enormous number of actions
resulting from the combinatorial nature of discrete optimization, conventional
MAB algorithms cannot effectively optimize dynamic, discrete environments.
Here, we show a heuristic MAB method for dynamic, discrete environments by
extending the BBO method, in which an Ising machine effectively explores the
actions while considering interactions between variables and changes in dynamic
environments. We demonstrate the dynamic adaptability of the proposed method in
a wireless communication system with moving users.

</details>


### [81] [Multimodal Fused Learning for Solving the Generalized Traveling Salesman Problem in Robotic Task Planning](https://arxiv.org/abs/2506.16931)
*Jiaqi Chen,Mingfeng Fan,Xuefeng Zhang,Jingsong Liang,Yuhong Cao,Guohua Wu,Guillaume Adrien Sartoretti*

Main category: cs.AI

TL;DR: 论文提出了一种多模态融合学习（MMFL）框架，用于解决广义旅行商问题（GTSP），通过结合图和图像表示生成高质量的任务规划方案。


<details>
  <summary>Details</summary>
Motivation: 移动机器人在仓库检索和环境监测等应用中需要高效的任务规划，而GTSP问题在准确性和效率上仍具挑战性。

Method: MMFL框架包括坐标图像构建器、自适应分辨率缩放策略和多模态融合模块，整合几何与空间特征。

Result: 实验表明，MMFL在多种GTSP实例中显著优于现有方法，并保持实时计算效率。

Conclusion: 物理机器人测试验证了MMFL在实际场景中的有效性。

Abstract: Effective and efficient task planning is essential for mobile robots,
especially in applications like warehouse retrieval and environmental
monitoring. These tasks often involve selecting one location from each of
several target clusters, forming a Generalized Traveling Salesman Problem
(GTSP) that remains challenging to solve both accurately and efficiently. To
address this, we propose a Multimodal Fused Learning (MMFL) framework that
leverages both graph and image-based representations to capture complementary
aspects of the problem, and learns a policy capable of generating high-quality
task planning schemes in real time. Specifically, we first introduce a
coordinate-based image builder that transforms GTSP instances into spatially
informative representations. We then design an adaptive resolution scaling
strategy to enhance adaptability across different problem scales, and develop a
multimodal fusion module with dedicated bottlenecks that enables effective
integration of geometric and spatial features. Extensive experiments show that
our MMFL approach significantly outperforms state-of-the-art methods across
various GTSP instances while maintaining the computational efficiency required
for real-time robotic applications. Physical robot tests further validate its
practical effectiveness in real-world scenarios.

</details>


### [82] [Elevating Styled Mahjong Agents with Learning from Demonstration](https://arxiv.org/abs/2506.16995)
*Lingfeng Li,Yunlong Lu,Yongyi Wang,Wenxin Li*

Main category: cs.AI

TL;DR: 论文提出了一种改进的LfD算法，用于提升麻将游戏中AI代理的能力和多样性。


<details>
  <summary>Details</summary>
Motivation: 现有离线学习和LfD算法在麻将游戏的高随机性和分布外状态下表现不佳，需改进以提升代理能力和多样性。

Method: 利用现有麻将代理的游戏历史，提出了一种仅需对PPO算法进行最小修改的LfD算法。

Result: 实验结果表明，该方法显著提升了代理能力，并有效保留了其独特的游戏风格。

Conclusion: 新方法在麻将游戏中表现出色，为AI代理的能力和多样性提供了有效解决方案。

Abstract: A wide variety of bots in games enriches the gameplay experience and enhances
replayability. Recent advancements in game artificial intelligence have
predominantly focused on improving the proficiency of bots. Nevertheless,
developing highly competent bots with a wide range of distinct play styles
remains a relatively under-explored area. We select the Mahjong game
environment as a case study. The high degree of randomness inherent in the
Mahjong game and the prevalence of out-of-distribution states lead to
suboptimal performance of existing offline learning and
Learning-from-Demonstration (LfD) algorithms. In this paper, we leverage the
gameplay histories of existing Mahjong agents and put forward a novel LfD
algorithm that necessitates only minimal modifications to the Proximal Policy
Optimization algorithm. The comprehensive empirical results illustrate that our
proposed method not only significantly enhances the proficiency of the agents
but also effectively preserves their unique play styles.

</details>


### [83] [A Quantile Regression Approach for Remaining Useful Life Estimation with State Space Models](https://arxiv.org/abs/2506.17018)
*Davide Frizzo,Francesco Borsatti,Gian Antonio Susto*

Main category: cs.AI

TL;DR: 本文提出了一种基于状态空间模型（SSM）和同步分位数回归（SQR）的剩余使用寿命（RUL）预测方法，在C-MAPSS数据集上验证了其优于传统序列建模技术的准确性和计算效率。


<details>
  <summary>Details</summary>
Motivation: 预测性维护（PdM）在工业4.0和5.0中至关重要，通过准确预测设备剩余使用寿命（RUL）来优化维护计划，减少意外故障和过早干预。

Method: 结合状态空间模型（SSM）和同步分位数回归（SQR），实现高效的长序列建模和多分位数估计。

Result: 在C-MAPSS数据集上，SSM模型表现出优于LSTM、Transformer和Informer等传统技术的准确性和计算效率。

Conclusion: SSM模型在高风险工业应用中具有显著潜力，为预测性维护提供了更优的解决方案。

Abstract: Predictive Maintenance (PdM) is pivotal in Industry 4.0 and 5.0, proactively
enhancing efficiency through accurate equipment Remaining Useful Life (RUL)
prediction, thus optimizing maintenance scheduling and reducing unexpected
failures and premature interventions. This paper introduces a novel RUL
estimation approach leveraging State Space Models (SSM) for efficient long-term
sequence modeling. To handle model uncertainty, Simoultaneous Quantile
Regression (SQR) is integrated into the SSM, enabling multiple quantile
estimations. The proposed method is benchmarked against traditional sequence
modelling techniques (LSTM, Transformer, Informer) using the C-MAPSS dataset.
Results demonstrate superior accuracy and computational efficiency of SSM
models, underscoring their potential for high-stakes industrial applications.

</details>


### [84] [Dispositions and Roles of Generically Dependent Entities](https://arxiv.org/abs/2506.17085)
*Fabian Neuhaus*

Main category: cs.AI

TL;DR: BFO 2020无法支持通用依赖持续体（如软件或数据集）的功能、倾向和角色，本文提出两种解决方案：定义类和修改BFO框架。


<details>
  <summary>Details</summary>
Motivation: BFO 2020的局限性导致无法充分表示计算机模型的功能或数据集在执行中的角色，影响了实际应用。

Method: 讨论了BFO 2020的限制，并提出两种方法：使用定义类和修改BFO框架以支持相关功能。

Result: 提出了两种可行的解决方案，以解决BFO 2020在表示通用依赖持续体时的不足。

Conclusion: 通过定义类或修改BFO框架，可以更全面地支持通用依赖持续体的功能、倾向和角色。

Abstract: BFO 2020 does not support functions, dispositions, and roles of generically
dependent continuants (like software or datasets). In this paper, we argue that
this is a severe limitation, which prevents, for example, the adequate
representation of the functions of computer models or the various roles of
datasets during the execution of these models. We discuss the aspects of BFO
2020 that prevent the representation of realizable entities of generically
dependent continuants. Two approaches to address the issue are presented: (a)
the use of defined classes and (b) a proposal of changes that allow BFO to
support functions, dispositions, and roles of generically dependent
continuants.

</details>


### [85] [Towards Advanced Mathematical Reasoning for LLMs via First-Order Logic Theorem Proving](https://arxiv.org/abs/2506.17104)
*Chuxue Cao,Mengze Li,Juntao Dai,Jinluan Yang,Zijian Zhao,Shengyu Zhang,Weijie Shi,Chengzhong Liu,Sirui Han,Yike Guo*

Main category: cs.AI

TL;DR: 论文提出DREAM方法，通过多样化策略和错误反馈提升LLM在多步一阶逻辑推理中的表现。


<details>
  <summary>Details</summary>
Motivation: 当前LLM在多步一阶逻辑推理任务中表现不佳，如Deepseek-Prover-V2-7B在定理证明数据集上准确率仅4.2%，亟需改进。

Method: 提出DREAM方法，包含公理驱动策略多样化机制和子命题错误反馈机制。

Result: DREAM将性能提升0.6%至6.4%，并提供了447个Lean 4格式的数学定理数据集。

Conclusion: DREAM显著提升了LLM在多步一阶逻辑推理中的能力，为数学定理证明领域提供了新工具。

Abstract: Large language models (LLMs) have shown promising first-order logic (FOL)
reasoning capabilities with applications in various areas. However, their
effectiveness in complex mathematical reasoning involving multi-step FOL
deductions is still under-researched. While LLMs perform competitively on
established mathematical reasoning benchmarks, they struggle with multi-step
FOL tasks, as demonstrated by Deepseek-Prover-V2-7B's low accuracy (4.2%) on
our proposed theorem proving dataset. This issue arises from the limited
exploration of diverse proof strategies and the potential for early reasoning
mistakes to undermine entire proofs. To address these issues, we propose DREAM,
a self-adaptive solution that enhances the Diversity and REAsonability of LLMs'
generation strategies. DREAM incorporates an Axiom-Driven Strategy
Diversification mechanism to promote varied strategic outcomes and a
Sub-Proposition Error Feedback to help LLMs reflect on and correct their
proofs. Our contributions include pioneering advancements in LLMs' mathematical
reasoning through FOL theorem proving, introducing a novel inference stage
solution that improves performance by 0.6% to 6.4%, and providing a curated
dataset of 447 mathematical theorems in Lean 4 format for evaluation.

</details>


### [86] [Are Bias Evaluation Methods Biased ?](https://arxiv.org/abs/2506.17111)
*Lina Berrayana,Sean Rooney,Luis Garcés-Erice,Ioana Giurgiu*

Main category: cs.AI

TL;DR: 研究发现，不同的大语言模型安全评估基准在偏见排名上存在显著差异，建议社区谨慎使用这些基准。


<details>
  <summary>Details</summary>
Motivation: 评估大语言模型的安全性需要可靠的基准，但现有基准在方法和数据集上存在差异，可能导致不一致的评估结果。

Method: 通过不同方法对一组代表性模型的偏见进行排名，并比较排名的相似性。

Result: 不同的偏见评估方法导致模型排名差异显著。

Conclusion: 建议社区在使用此类基准时需谨慎，并考虑其局限性。

Abstract: The creation of benchmarks to evaluate the safety of Large Language Models is
one of the key activities within the trusted AI community. These benchmarks
allow models to be compared for different aspects of safety such as toxicity,
bias, harmful behavior etc. Independent benchmarks adopt different approaches
with distinct data sets and evaluation methods. We investigate how robust such
benchmarks are by using different approaches to rank a set of representative
models for bias and compare how similar are the overall rankings. We show that
different but widely used bias evaluations methods result in disparate model
rankings. We conclude with recommendations for the community in the usage of
such benchmarks.

</details>


### [87] [Mathematical Proof as a Litmus Test: Revealing Failure Modes of Advanced Large Reasoning Models](https://arxiv.org/abs/2506.17114)
*Dadi Guo,Jiayu Liu,Zhiyuan Fan,Zhitao He,Haoran Li,Yumeng Wang,Yi R.,Fung*

Main category: cs.AI

TL;DR: 论文提出利用数学证明的严谨性作为诊断工具，揭示大型推理模型在数学证明中的隐藏缺陷，并引入RFMDataset进行评估，发现模型存在多种错误类型和根本性限制。


<details>
  <summary>Details</summary>
Motivation: 当前大型推理模型在数学问题解决中表现优异，但其高准确率掩盖了推理缺陷，需通过数学证明的复杂性揭示这些隐藏问题。

Method: 提出RFMDataset，包含200个多样化的数学证明问题，用于评估高级模型的表现，并分析其错误类型。

Result: 研究发现模型在数学证明中表现不佳（正确率低于20%），存在10种细粒度错误类型，包括推理不严谨、幻觉和不完整性。

Conclusion: 模型的自我反思不足以解决逻辑困境，需通过形式化和细粒度的逻辑训练提升推理能力。

Abstract: Large reasoning models (e.g., R1, o3) have demonstrated remarkable
mathematical problem-solving abilities. However, the high reported accuracy of
these advanced models on popular datasets, reliance on purely numerical
evaluation and potential benchmark leakage, often masks their true reasoning
shortcomings. To address this, we propose leveraging the inherent rigor and
methodological complexity of mathematical proofs as a diagnostic tool to expose
these hidden failures. Specifically, we introduce the RFMDataset (Reveal
Failure Modes), a collection of 200 diverse mathematical proof problems, and
thoroughly evaluate advanced models' performance on it. Our in-depth analysis
of their failures uncovers 10 fine-grained error types, which shows fundamental
limitations in current large reasoning models: 1) large reasoning models
grapple profoundly with mathematical proofs, with some generating entirely
correct proofs for less than 20% of problems and failing even on basic ones; 2)
models exhibit a diverse spectrum of reasoning failures, prominently
demonstrating the lack of guarantees for the correctness and rigor of
single-step reasoning; and 3) models show hallucination and incompleteness
during the reasoning process. Our findings reveal that models' self-reflection
is insufficient to resolve the current logical dilemmas, necessitating
formalized and fine-grained logical training.

</details>


### [88] [When Can Model-Free Reinforcement Learning be Enough for Thinking?](https://arxiv.org/abs/2506.17124)
*Josiah P. Hanna,Nicholas E. Corrado*

Main category: cs.AI

TL;DR: 论文探讨了模型无关强化学习（RL）如何通过“思考”行为优化奖励，提出了“思考MDP”理论模型，并验证了其在语言模型中的应用。


<details>
  <summary>Details</summary>
Motivation: 研究模型无关RL如何在没有直接奖励或改变外部状态的情况下，通过“思考”行为实现奖励最大化。

Method: 引入“思考MDP”理论模型，分析策略初始化对“思考”行为的影响，并在开源LLMs中验证理论。

Result: 证明了“思考”行为等价于策略改进步骤，并在语言模型中验证了理论预测的必要条件。

Conclusion: 提出了在多任务预训练和指定“思考”动作下，RL可以更高效地学习“思考”行为。

Abstract: Recent work on large language models has demonstrated the use of model-free
reinforcement learning (RL) to train reasoning-like capabilities. The emergence
of "thinking" through model-free RL is interesting as thinking actions neither
produce reward nor change the external world state to one where the agent is
more likely to get reward. This paper seeks to build a domain-independent
understanding of when model-free RL will lead to "thinking" as a strategy for
reward maximization. To build this understanding, we first introduce a
theoretical model which we call a \textit{thought Markov decision process}
(MDP). Thought MDPs minimally extend the classical MDP model to include an
abstract notion of thought state and thought action. Using the thought MDP
model, we prove the importance of policy initialization in determining whether
or not thinking emerges and show formally that thought actions are equivalent
to the agent choosing to perform a step of policy improvement before continuing
to act. We then show that open-source LLMs satisfy the conditions that our
theory predicts are necessary for model-free RL to produce thinking-like
behavior. Finally, we hypothesize sufficient conditions that would enable
thinking to be learned outside of language generation and introduce a toy
domain where a combination of multi-task pre-training and designated thought
actions enable more data-efficient RL compared to non-thinking agents.

</details>


### [89] [Chain-of-Trust: A Progressive Trust Evaluation Framework Enabled by Generative AI](https://arxiv.org/abs/2506.17130)
*Botao Zhu,Xianbin Wang,Lei Zhang,Xuemin,Shen*

Main category: cs.AI

TL;DR: 提出了一种名为chain-of-trust的渐进式信任评估框架，通过分阶段评估设备属性数据，降低复杂性并提高任务完成效率。


<details>
  <summary>Details</summary>
Motivation: 在分布式协作系统中，由于网络动态性和信息收集延迟，难以同时评估所有信任属性。

Method: 将信任评估分为多个链式阶段，每阶段仅收集相关数据，并利用生成式AI进行分析。

Result: 实验表明，该框架在信任评估中具有高准确性。

Conclusion: 该框架有效解决了信任评估中的复杂性问题，提升了任务完成效率。

Abstract: In collaborative systems with complex tasks relying on distributed resources,
trust evaluation of potential collaborators has emerged as an effective
mechanism for task completion. However, due to the network dynamics and varying
information gathering latencies, it is extremely challenging to observe and
collect all trust attributes of a collaborating device concurrently for a
comprehensive trust assessment. In this paper, a novel progressive trust
evaluation framework, namely chain-of-trust, is proposed to make better use of
misaligned device attribute data. This framework, designed for effective task
completion, divides the trust evaluation process into multiple chained stages
based on task decomposition. At each stage, based on the task completion
process, the framework only gathers the latest device attribute data relevant
to that stage, leading to reduced trust evaluation complexity and overhead. By
leveraging advanced in-context learning, few-shot learning, and reasoning
capabilities, generative AI is then employed to analyze and interpret the
collected data to produce correct evaluation results quickly. Only devices
deemed trustworthy at this stage proceed to the next round of trust evaluation.
The framework ultimately determines devices that remain trustworthy across all
stages. Experimental results demonstrate that the proposed framework achieves
high accuracy in trust evaluation.

</details>


### [90] [The MedPerturb Dataset: What Non-Content Perturbations Reveal About Human and Clinical LLM Decision Making](https://arxiv.org/abs/2506.17163)
*Abinitha Gourabathina,Yuexing Hao,Walter Gerych,Marzyeh Ghassemi*

Main category: cs.AI

TL;DR: MedPerturb数据集用于评估医疗大语言模型（LLMs）在临床输入扰动下的表现，发现LLMs对性别和语言风格更敏感，而人类专家对格式变化更敏感。


<details>
  <summary>Details</summary>
Motivation: 研究医疗LLMs在真实临床环境中的稳健性，揭示其与人类在决策上的差异。

Method: 构建MedPerturb数据集，包含800个临床情境，通过性别、风格和格式扰动，对比LLMs与人类专家的反应。

Result: LLMs对性别和风格扰动更敏感，人类专家对格式变化更敏感。

Conclusion: 需开发动态评估框架，以更全面地比较人类与LLMs在临床环境中的决策一致性。

Abstract: Clinical robustness is critical to the safe deployment of medical Large
Language Models (LLMs), but key questions remain about how LLMs and humans may
differ in response to the real-world variability typified by clinical settings.
To address this, we introduce MedPerturb, a dataset designed to systematically
evaluate medical LLMs under controlled perturbations of clinical input.
MedPerturb consists of clinical vignettes spanning a range of pathologies, each
transformed along three axes: (1) gender modifications (e.g., gender-swapping
or gender-removal); (2) style variation (e.g., uncertain phrasing or colloquial
tone); and (3) format changes (e.g., LLM-generated multi-turn conversations or
summaries). With MedPerturb, we release a dataset of 800 clinical contexts
grounded in realistic input variability, outputs from four LLMs, and three
human expert reads per clinical context. We use MedPerturb in two case studies
to reveal how shifts in gender identity cues, language style, or format reflect
diverging treatment selections between humans and LLMs. We find that LLMs are
more sensitive to gender and style perturbations while human annotators are
more sensitive to LLM-generated format perturbations such as clinical
summaries. Our results highlight the need for evaluation frameworks that go
beyond static benchmarks to assess the similarity between human clinician and
LLM decisions under the variability characteristic of clinical settings.

</details>
