<div id=toc></div>

# Table of Contents

- [cs.CR](#cs.CR) [Total: 11]
- [cs.SE](#cs.SE) [Total: 4]
- [cs.AI](#cs.AI) [Total: 22]


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [1] [Automated Red-Teaming Framework for Large Language Model Security Assessment: A Comprehensive Attack Generation and Detection System](https://arxiv.org/abs/2512.20677)
*Zhang Wei,Peilu Hu,Shengning Lang,Hao Yan,Li Mei,Yichao Zhang,Chen Yang,Junfeng Hao,Zhimo Han*

Main category: cs.CR

TL;DR: 本文提出了一种自动化红队测试框架，用于系统性地发现大语言模型的安全漏洞，相比人工测试在漏洞发现率上提高了3.9倍。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型在关键领域部署，确保其安全性和对齐性变得至关重要。现有红队测试主要依赖人工方法，可扩展性有限且无法全面覆盖潜在对抗行为空间。

Method: 开发了自动化红队测试框架，集成了基于元提示的攻击生成、多模态漏洞检测和标准化评估协议，涵盖六大威胁类别：奖励黑客攻击、欺骗性对齐、数据泄露、消极抵抗、不当工具使用和思维链操纵。

Result: 在GPT-OSS-20B模型上发现了47个不同漏洞，包括21个高严重性漏洞和12个新颖攻击模式，漏洞发现率比人工专家测试提高了3.9倍，同时保持89%的检测准确率。

Conclusion: 该框架能够实现可扩展、系统化和可复现的AI安全评估，为改进对齐鲁棒性提供可操作的见解，推动了自动化LLM红队测试的发展，有助于构建安全可信的AI系统。

Abstract: As large language models (LLMs) are increasingly deployed in high-stakes domains, ensuring their security and alignment has become a critical challenge. Existing red-teaming practices depend heavily on manual testing, which limits scalability and fails to comprehensively cover the vast space of potential adversarial behaviors. This paper introduces an automated red-teaming framework that systematically generates, executes, and evaluates adversarial prompts to uncover security vulnerabilities in LLMs. Our framework integrates meta-prompting-based attack synthesis, multi-modal vulnerability detection, and standardized evaluation protocols spanning six major threat categories -- reward hacking, deceptive alignment, data exfiltration, sandbagging, inappropriate tool use, and chain-of-thought manipulation. Experiments on the GPT-OSS-20B model reveal 47 distinct vulnerabilities, including 21 high-severity and 12 novel attack patterns, achieving a $3.9\times$ improvement in vulnerability discovery rate over manual expert testing while maintaining 89\% detection accuracy. These results demonstrate the framework's effectiveness in enabling scalable, systematic, and reproducible AI safety evaluations. By providing actionable insights for improving alignment robustness, this work advances the state of automated LLM red-teaming and contributes to the broader goal of building secure and trustworthy AI systems.

</details>


### [2] [Anota: Identifying Business Logic Vulnerabilities via Annotation-Based Sanitization](https://arxiv.org/abs/2512.20705)
*Meng Wang,Philipp Görz,Joschua Schilling,Keno Hassler,Liwei Guo,Thorsten Holz,Ali Abbasi*

Main category: cs.CR

TL;DR: ANOTA是一个结合人工标注的轻量级框架，通过用户标注定义应用预期行为，运行时监控程序执行以检测业务逻辑漏洞，相比传统模糊测试方法能更有效地发现复杂业务逻辑缺陷。


<details>
  <summary>Details</summary>
Motivation: 业务逻辑漏洞是软件安全中的关键挑战，传统模糊测试主要关注内存安全漏洞，但无法有效检测需要理解应用特定语义上下文的业务逻辑漏洞。这些漏洞在实践中占最危险软件弱点的多数（27/40），现有工具存在明显盲区。

Method: ANOTA采用人机协同的轻量级标注框架，用户通过友好的标注系统将领域知识编码为轻量级标注，定义应用的预期行为。运行时执行监控器观察程序行为，与标注定义的政策进行比较，识别偏差以发现漏洞。

Result: ANOTA与最先进的模糊测试器结合后，比其他兼容相同目标的bug发现方法更有效。成功复现43个已知漏洞，发现22个先前未知漏洞（其中17个获得CVE编号）。

Conclusion: ANOTA提供了一种实用有效的方法，能够发现传统安全测试技术经常遗漏的复杂业务逻辑缺陷，填补了现有工具在业务逻辑漏洞检测方面的盲区。

Abstract: Detecting business logic vulnerabilities is a critical challenge in software security. These flaws come from mistakes in an application's design or implementation and allow attackers to trigger unintended application behavior. Traditional fuzzing sanitizers for dynamic analysis excel at finding vulnerabilities related to memory safety violations but largely fail to detect business logic vulnerabilities, as these flaws require understanding application-specific semantic context. Recent attempts to infer this context, due to their reliance on heuristics and non-portable language features, are inherently brittle and incomplete. As business logic vulnerabilities constitute a majority (27/40) of the most dangerous software weaknesses in practice, this is a worrying blind spot of existing tools. In this paper, we tackle this challenge with ANOTA, a novel human-in-the-loop sanitizer framework. ANOTA introduces a lightweight, user-friendly annotation system that enables users to directly encode their domain-specific knowledge as lightweight annotations that define an application's intended behavior. A runtime execution monitor then observes program behavior, comparing it against the policies defined by the annotations, thereby identifying deviations that indicate vulnerabilities. To evaluate the effectiveness of ANOTA, we combine ANOTA with a state-of-the-art fuzzer and compare it against other popular bug finding methods compatible with the same targets. The results show that ANOTA+FUZZER outperforms them in terms of effectiveness. More specifically, ANOTA+FUZZER can successfully reproduce 43 known vulnerabilities, and discovered 22 previously unknown vulnerabilities (17 CVEs assigned) during the evaluation. These results demonstrate that ANOTA provides a practical and effective approach for uncovering complex business logic flaws often missed by traditional security testing techniques.

</details>


### [3] [Real-World Adversarial Attacks on RF-Based Drone Detectors](https://arxiv.org/abs/2512.20712)
*Omer Gazit,Yael Itzhakev,Yuval Elovici,Asaf Shabtai*

Main category: cs.CR

TL;DR: 首次提出针对RF图像无人机检测器的物理攻击，通过优化特定类别的通用复数基带扰动波形，在合法通信旁传输，有效降低目标无人机检测率


<details>
  <summary>Details</summary>
Motivation: 现有针对图像模型的RF攻击主要修改数字特征，难以在无线环境中实现，因为数字扰动转换为可传输波形存在同步误差、干扰和硬件限制等问题

Method: 优化特定类别的通用复数基带(I/Q)扰动波形，在合法通信信号旁传输，使用RF记录和OTA实验评估，涵盖四种无人机类型

Result: 适度的结构化I/Q扰动与标准RF链兼容，能可靠降低目标无人机检测率，同时保持对合法无人机的检测能力

Conclusion: 首次实现了针对RF图像无人机检测器的物理攻击，证明了通过优化I/Q扰动波形可以在实际无线环境中有效规避检测系统

Abstract: Radio frequency (RF) based systems are increasingly used to detect drones by analyzing their RF signal patterns, converting them into spectrogram images which are processed by object detection models. Existing RF attacks against image based models alter digital features, making over-the-air (OTA) implementation difficult due to the challenge of converting digital perturbations to transmittable waveforms that may introduce synchronization errors and interference, and encounter hardware limitations. We present the first physical attack on RF image based drone detectors, optimizing class-specific universal complex baseband (I/Q) perturbation waveforms that are transmitted alongside legitimate communications. We evaluated the attack using RF recordings and OTA experiments with four types of drones. Our results show that modest, structured I/Q perturbations are compatible with standard RF chains and reliably reduce target drone detection while preserving detection of legitimate drones.

</details>


### [4] [Sark: Oblivious Integrity Without Global State](https://arxiv.org/abs/2512.20775)
*Alex Lynham,David Alesch,Ziyi Li,Geoff Goodell*

Main category: cs.CR

TL;DR: Sark是一个实现USO资产系统的参考架构，包含Sloop许可CFT区块链和Porters承诺累积子系统，通过CIA三元组分析系统运行，引入完整性轨迹概念解决去中心化设计权衡。


<details>
  <summary>Details</summary>
Motivation: 实现Goodell、Toliver和Nakib提出的不可伪造、有状态且不可追踪（USO）资产系统，为资产管理系统提供参考架构。

Method: 设计Sark参考架构，包含Sloop许可式崩溃容错区块链子系统和Porters客户端承诺累积子系统；使用CIA三元组（机密性、可用性、完整性）分析系统运行；引入完整性轨迹概念解决去中心化设计权衡。

Result: 提出了完整的Sark参考架构实现方案，包括系统设计、运行分析和去中心化权衡解决方案，为USO资产系统提供了可行的技术实现路径。

Conclusion: Sark成功实现了USO资产系统的参考架构，通过CIA三元组分析和完整性轨迹概念解决了关键设计问题，未来工作将集中在拜占庭容错和Porters本地中心化缓解方面。

Abstract: In this paper, we introduce Sark, a reference architecture implementing the Unforgeable, Stateful, and Oblivious (USO) asset system as described by Goodell, Toliver, and Nakib. We describe the motivation, design, and implementation of Sloop, a permissioned, crash fault-tolerant (CFT) blockchain that forms a subsystem of Sark, and the other core subsystems, Porters, which accumulate and roll-up commitments from Clients. We analyse the operation of the system using the 'CIA Triad': Confidentiality, Availability, and Integrity. We then introduce the concept of Integrity Locus and use it to address design trade-offs related to decentralization. Finally, we point to future work on Byzantine fault-tolerance (BFT), and mitigating the local centrality of Porters.

</details>


### [5] [pokiSEC: A Multi-Architecture, Containerized Ephemeral Malware Detonation Sandbox](https://arxiv.org/abs/2512.20860)
*Alejandro Avina,Yashas Hariprasad,Naveen Kumar Chaudhary*

Main category: cs.CR

TL;DR: pokiSEC是一个轻量级、临时性的恶意软件引爆沙箱，将完整的虚拟化和访问栈打包到Docker容器中，支持跨架构（ARM64和x86_64）运行Windows恶意软件分析。


<details>
  <summary>Details</summary>
Motivation: 当前恶意软件动态分析通常依赖于重量级虚拟机管理程序或专用裸机实验室，限制了可移植性和自动化。随着ARM64开发硬件（如Apple Silicon）的普及，现有的开源沙箱方案大多假设x86_64主机，无法在不同架构间顺利迁移。

Method: pokiSEC将QEMU与硬件加速（KVM）集成到Docker容器中，提供基于浏览器的工作流程，支持自带Windows磁盘镜像。核心创新是"通用入口点"，通过运行时主机架构检测选择经过验证的虚拟机管理程序配置（机器类型、加速模式和设备配置文件）。

Result: 在Apple Silicon（ARM64）和Ubuntu（AMD64）上验证了pokiSEC，展示了适合分析师工作流程的交互性能，并通过临时容器生命周期实现一致的清理语义。

Conclusion: pokiSEC提供了一个轻量级、可移植的恶意软件引爆沙箱解决方案，解决了跨架构兼容性问题，使单个容器镜像和代码库能够在ARM64和x86_64主机上启动Windows客户机，提高了恶意软件分析的可移植性和自动化能力。

Abstract: Dynamic malware analysis requires executing untrusted binaries inside strongly isolated, rapidly resettable environments. In practice, many detonation workflows remain tied to heavyweight hypervisors or dedicated bare-metal labs, limiting portability and automation. This challenge has intensified with the adoption of ARM64 developer hardware (e.g., Apple Silicon), where common open-source sandbox recipes and pre-built environments frequently assume x86_64 hosts and do not translate cleanly across architectures. This paper presents pokiSEC, a lightweight, ephemeral malware detonation sandbox that packages the full virtualization and access stack inside a Docker container. pokiSEC integrates QEMU with hardware acceleration (KVM when available) and exposes a browser-based workflow that supports bring-your-own Windows disk images. The key contribution is a Universal Entrypoint that performs runtime host-architecture detection and selects validated hypervisor configurations (machine types, acceleration modes, and device profiles), enabling a single container image and codebase to launch Windows guests on both ARM64 and x86_64 hosts. We validate pokiSEC on Apple Silicon (ARM64) and Ubuntu (AMD64), demonstrating interactive performance suitable for analyst workflows and consistent teardown semantics via ephemeral container lifecycles.

</details>


### [6] [Better Call Graphs: A New Dataset of Function Call Graphs for Malware Classification](https://arxiv.org/abs/2512.20872)
*Jakir Hossain,Gurvinder Singh,Lukasz Ziarek,Ahmet Erdem Sarıyüce*

Main category: cs.CR

TL;DR: BCG是一个新的Android恶意软件检测数据集，包含从近期APK中提取的大规模、独特的函数调用图，解决了现有数据集过时、冗余和多样性不足的问题。


<details>
  <summary>Details</summary>
Motivation: 函数调用图在恶意软件检测中很有效，但在Android领域缺乏大规模、高质量的FCG数据集。现有数据集通常过时、包含大量由应用重打包导致的小型或冗余图，无法反映真实世界恶意软件的多样性，导致图分类方法过拟合和不可靠评估。

Method: 构建Better Call Graphs（BCG）数据集，从近期Android应用包（APKs）中提取大规模、独特的函数调用图，包含良性和恶意样本，涵盖多种家族和类型，并为每个APK提供图级特征。

Result: 通过基线分类器的广泛实验，证明了BCG相比现有数据集的必要性和价值。BCG数据集已公开可用。

Conclusion: BCG填补了Android恶意软件检测领域缺乏高质量FCG数据集的空白，为图基分类方法提供了更可靠、多样化的评估基础，有助于推动该领域的研究进展。

Abstract: Function call graphs (FCGs) have emerged as a powerful abstraction for malware detection, capturing the behavioral structure of applications beyond surface-level signatures. Their utility in traditional program analysis has been well established, enabling effective classification and analysis of malicious software. In the mobile domain, especially in the Android ecosystem, FCG-based malware classification is particularly critical due to the platform's widespread adoption and the complex, component-based structure of Android apps. However, progress in this direction is hindered by the lack of large-scale, high-quality Android-specific FCG datasets. Existing datasets are often outdated, dominated by small or redundant graphs resulting from app repackaging, and fail to reflect the diversity of real-world malware. These limitations lead to overfitting and unreliable evaluation of graph-based classification methods. To address this gap, we introduce Better Call Graphs (BCG), a comprehensive dataset of large and unique FCGs extracted from recent Android application packages (APKs). BCG includes both benign and malicious samples spanning various families and types, along with graph-level features for each APK. Through extensive experiments using baseline classifiers, we demonstrate the necessity and value of BCG compared to existing datasets. BCG is publicly available at https://erdemub.github.io/BCG-dataset.

</details>


### [7] [Neutralization of IMU-Based GPS Spoofing Detection using external IMU sensor and feedback methodology](https://arxiv.org/abs/2512.20964)
*Ji Hyuk Jung,Ji Won Yoon*

Main category: cs.CR

TL;DR: 本文提出了一种针对自动驾驶车辆GPS欺骗攻击的对抗性攻击系统，能够绕过基于IMU传感器的检测机制，通过窃取内部动态状态信息实现不被察觉的GPS欺骗攻击。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶车辆的位置感知对其自主决策至关重要，而GPS欺骗攻击会破坏位置识别。虽然基于IMU传感器的检测方法被认为是最有效的防御机制之一，但目前缺乏针对这类检测方法的对抗性攻击研究。

Method: 提出了一种攻击建模方法，通过使用外部IMU传感器窃取内部动态状态信息来执行GPS欺骗攻击。基于EKF（扩展卡尔曼滤波）传感器融合技术，分析GPS欺骗值对目标系统的影响，并设计方法减少目标系统中的异常检测。

Result: 实验结果表明，所提出的攻击模型能够在不被检测到的情况下注入攻击值，成功绕过了基于IMU传感器的GPS欺骗检测机制。

Conclusion: 本文揭示了当前基于IMU传感器的GPS欺骗检测机制存在安全漏洞，需要开发更强大的防御策略来应对这种新型对抗性攻击。

Abstract: Autonomous Vehicles (AVs) refer to systems capable of perceiving their states and moving without human intervention. Among the factors required for autonomous decision-making in mobility, positional awareness of the vehicle itself is the most critical. Accordingly, extensive research has been conducted on defense mechanisms against GPS spoofing attacks, which threaten AVs by disrupting position recognition. Among these, detection methods based on internal IMU sensors are regarded as some of the most effective. In this paper, we propose a spoofing attack system designed to neutralize IMU sensor-based detection. First, we present an attack modeling approach for bypassing such detection. Then, based on EKF sensor fusion, we experimentally analyze both the impact of GPS spoofing values on the internal target system and how our proposed methodology reduces anomaly detection within the target system. To this end, this paper proposes an attack model that performs GPS spoofing by stealing internal dynamic state information using an external IMU sensor, and the experimental results demonstrate that attack values can be injected without being detected.

</details>


### [8] [Casting a SPELL: Sentence Pairing Exploration for LLM Limitation-breaking](https://arxiv.org/abs/2512.21236)
*Yifan Huang,Xiaojun Jia,Wenbo Guo,Yuqiang Sun,Yihao Huang,Chong Wang,Yang Liu*

Main category: cs.CR

TL;DR: SPELL框架专门评估大语言模型在恶意代码生成中的安全对齐弱点，通过时间分配选择策略构建越狱提示，在GPT-4.1、Claude-3.5和Qwen2.5-Coder上分别达到83.75%、19.38%和68.12%的攻击成功率。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型在软件开发中的广泛应用，非专业开发者也能创建复杂应用，但这种便利性也可能被恶意行为者利用来生成有害软件。现有越狱研究主要关注通用攻击场景，对恶意代码生成这一特定目标的探索有限。

Method: 提出SPELL测试框架，采用时间分配选择策略，通过智能组合先验知识数据集中的句子来系统构建越狱提示，平衡新颖攻击模式的探索与成功技术的利用。

Result: 在三个先进代码模型（GPT-4.1、Claude-3.5和Qwen2.5-Coder）上评估，SPELL在八个恶意代码类别中分别达到83.75%、19.38%和68.12%的攻击成功率。在Cursor等实际AI开发工具中生成的恶意代码被最先进检测系统确认为恶意的比例超过73%。

Conclusion: 当前LLM实现存在显著安全漏洞，SPELL框架为改进代码生成应用中的AI安全对齐提供了有价值的见解。

Abstract: Large language models (LLMs) have revolutionized software development through AI-assisted coding tools, enabling developers with limited programming expertise to create sophisticated applications. However, this accessibility extends to malicious actors who may exploit these powerful tools to generate harmful software. Existing jailbreaking research primarily focuses on general attack scenarios against LLMs, with limited exploration of malicious code generation as a jailbreak target. To address this gap, we propose SPELL, a comprehensive testing framework specifically designed to evaluate the weakness of security alignment in malicious code generation. Our framework employs a time-division selection strategy that systematically constructs jailbreaking prompts by intelligently combining sentences from a prior knowledge dataset, balancing exploration of novel attack patterns with exploitation of successful techniques. Extensive evaluation across three advanced code models (GPT-4.1, Claude-3.5, and Qwen2.5-Coder) demonstrates SPELL's effectiveness, achieving attack success rates of 83.75%, 19.38%, and 68.12% respectively across eight malicious code categories. The generated prompts successfully produce malicious code in real-world AI development tools such as Cursor, with outputs confirmed as malicious by state-of-the-art detection systems at rates exceeding 73%. These findings reveal significant security gaps in current LLM implementations and provide valuable insights for improving AI safety alignment in code generation applications.

</details>


### [9] [GateBreaker: Gate-Guided Attacks on Mixture-of-Expert LLMs](https://arxiv.org/abs/2512.21008)
*Lichao Wu,Sasha Behrouzi,Mohamadreza Rostami,Stjepan Picek,Ahmad-Reza Sadeghi*

Main category: cs.CR

TL;DR: GateBreaker是一种无需训练、轻量级的攻击框架，专门针对MoE架构LLMs的安全对齐机制，通过识别并禁用安全专家中的关键神经元，显著提高攻击成功率。


<details>
  <summary>Details</summary>
Motivation: 当前LLM安全研究主要集中于密集架构，而对MoE架构独特的安全特性缺乏研究。MoE的模块化、稀疏激活设计可能导致安全机制与密集模型不同，存在潜在脆弱性。

Method: GateBreaker采用三阶段攻击框架：1) 门级分析识别有害输入上被过度路由的安全专家；2) 专家级定位在安全专家内部定位安全结构；3) 针对性安全移除禁用已识别的安全结构。

Result: 攻击仅需禁用目标专家层中约3%的神经元，就能将8个最新对齐MoE LLMs的平均攻击成功率从7.4%提升至64.9%，且效用退化有限。安全神经元在相同模型家族间可迁移，单次迁移攻击可将成功率从17.9%提升至67.7%。框架还能推广到5个MoE视觉语言模型，在危险图像输入上达到60.9%的攻击成功率。

Conclusion: MoE的安全机制集中在由稀疏路由协调的小部分神经元中，这些神经元容易受到针对性攻击。研究揭示了MoE架构独特的安全脆弱性，为未来开发更鲁棒的MoE安全对齐机制提供了重要见解。

Abstract: Mixture-of-Experts (MoE) architectures have advanced the scaling of Large Language Models (LLMs) by activating only a sparse subset of parameters per input, enabling state-of-the-art performance with reduced computational cost. As these models are increasingly deployed in critical domains, understanding and strengthening their alignment mechanisms is essential to prevent harmful outputs. However, existing LLM safety research has focused almost exclusively on dense architectures, leaving the unique safety properties of MoEs largely unexamined. The modular, sparsely-activated design of MoEs suggests that safety mechanisms may operate differently than in dense models, raising questions about their robustness.
  In this paper, we present GateBreaker, the first training-free, lightweight, and architecture-agnostic attack framework that compromises the safety alignment of modern MoE LLMs at inference time. GateBreaker operates in three stages: (i) gate-level profiling, which identifies safety experts disproportionately routed on harmful inputs, (ii) expert-level localization, which localizes the safety structure within safety experts, and (iii) targeted safety removal, which disables the identified safety structure to compromise the safety alignment. Our study shows that MoE safety concentrates within a small subset of neurons coordinated by sparse routing. Selective disabling of these neurons, approximately 3% of neurons in the targeted expert layers, significantly increases the averaged attack success rate (ASR) from 7.4% to 64.9% against the eight latest aligned MoE LLMs with limited utility degradation. These safety neurons transfer across models within the same family, raising ASR from 17.9% to 67.7% with one-shot transfer attack. Furthermore, GateBreaker generalizes to five MoE vision language models (VLMs) with 60.9% ASR on unsafe image inputs.

</details>


### [10] [zkFL-Health: Blockchain-Enabled Zero-Knowledge Federated Learning for Medical AI Privacy](https://arxiv.org/abs/2512.21048)
*Savvy Sharma,George Petrovic,Sarthak Kaushik*

Main category: cs.CR

TL;DR: zkFL-Health：结合零知识证明和可信执行环境的医疗AI联邦学习框架，在保护隐私的同时确保聚合过程的可验证性


<details>
  <summary>Details</summary>
Motivation: 医疗AI需要大规模多样化数据集，但严格的隐私和治理限制阻碍了机构间的原始数据共享。联邦学习虽然允许在数据所在地训练，但仍面临隐私泄露和聚合器信任两大核心风险

Method: 提出zkFL-Health架构，结合联邦学习、零知识证明和可信执行环境。客户端本地训练并提交更新承诺；聚合器在TEE内计算全局更新，并生成ZK证明（使用Halo2/Nova）证明其使用了正确的输入和聚合规则，而不向主机泄露任何客户端更新。验证节点验证证明并将加密承诺记录在链上

Result: 该框架为多机构医疗AI提供了强大的机密性、完整性和可审计性，这些特性对于临床采用和监管合规至关重要。论文概述了针对医疗保健的系统与威胁模型、zkFL-Health协议、安全/隐私保证以及性能评估计划

Conclusion: zkFL-Health通过结合零知识证明和可信执行环境，解决了联邦学习在医疗领域的隐私泄露和聚合器信任问题，为实现隐私保护、可验证正确的协作训练提供了可行方案

Abstract: Healthcare AI needs large, diverse datasets, yet strict privacy and governance constraints prevent raw data sharing across institutions. Federated learning (FL) mitigates this by training where data reside and exchanging only model updates, but practical deployments still face two core risks: (1) privacy leakage via gradients or updates (membership inference, gradient inversion) and (2) trust in the aggregator, a single point of failure that can drop, alter, or inject contributions undetected. We present zkFL-Health, an architecture that combines FL with zero-knowledge proofs (ZKPs) and Trusted Execution Environments (TEEs) to deliver privacy-preserving, verifiably correct collaborative training for medical AI. Clients locally train and commit their updates; the aggregator operates within a TEE to compute the global update and produces a succinct ZK proof (via Halo2/Nova) that it used exactly the committed inputs and the correct aggregation rule, without revealing any client update to the host. Verifier nodes validate the proof and record cryptographic commitments on-chain, providing an immutable audit trail and removing the need to trust any single party. We outline system and threat models tailored to healthcare, the zkFL-Health protocol, security/privacy guarantees, and a performance evaluation plan spanning accuracy, privacy risk, latency, and cost. This framework enables multi-institutional medical AI with strong confidentiality, integrity, and auditability, key properties for clinical adoption and regulatory compliance.

</details>


### [11] [AutoBaxBuilder: Bootstrapping Code Security Benchmarking](https://arxiv.org/abs/2512.21132)
*Tobias von Arx,Niels Mündler,Mark Vero,Maximilian Baader,Martin Vechev*

Main category: cs.CR

TL;DR: AutoBaxBuilder是一个自动生成代码安全基准测试任务的框架，能够从零开始创建功能和安全性测试，解决手动基准测试的局限性。


<details>
  <summary>Details</summary>
Motivation: 随着LLM在软件工程中的广泛应用，可靠评估LLM生成代码的正确性和安全性变得至关重要。现有手动构建的安全基准存在三个主要问题：1）容易污染训练数据；2）需要扩展到新任务以提供更全面的评估；3）需要增加难度以挑战更强大的LLM。

Method: 提出AutoBaxBuilder框架，通过包含细粒度合理性检查的鲁棒管道，利用LLM的代码理解能力来构建功能测试和端到端安全探测利用。框架能够从零开始生成任务和测试。

Result: 使用AutoBaxBuilder构建了全新的任务集AutoBaxBench，并与人类专家构建的任务进行对比。结果显示，生成一个新任务只需不到2小时，成本低于10美元。通过定性和定量分析确认了生成基准的质量。

Conclusion: AutoBaxBuilder能够高效、低成本地生成高质量的代码安全基准测试任务，解决了手动基准测试的局限性，为评估LLM代码安全性提供了可扩展的解决方案。

Abstract: As LLMs see wide adoption in software engineering, the reliable assessment of the correctness and security of LLM-generated code is crucial. Notably, prior work has demonstrated that security is often overlooked, exposing that LLMs are prone to generating code with security vulnerabilities. These insights were enabled by specialized benchmarks, crafted through significant manual effort by security experts. However, relying on manually-crafted benchmarks is insufficient in the long term, because benchmarks (i) naturally end up contaminating training data, (ii) must extend to new tasks to provide a more complete picture, and (iii) must increase in difficulty to challenge more capable LLMs. In this work, we address these challenges and present AutoBaxBuilder, a framework that generates tasks and tests for code security benchmarking from scratch. We introduce a robust pipeline with fine-grained plausibility checks, leveraging the code understanding capabilities of LLMs to construct functionality tests and end-to-end security-probing exploits. To confirm the quality of the generated benchmark, we conduct both a qualitative analysis and perform quantitative experiments, comparing it against tasks constructed by human experts. We use AutoBaxBuilder to construct entirely new tasks and release them to the public as AutoBaxBench, together with a thorough evaluation of the security capabilities of LLMs on these tasks. We find that a new task can be generated in under 2 hours, costing less than USD 10.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [12] [Process Analytics -- Data-driven Business Process Management](https://arxiv.org/abs/2512.20703)
*Matthias Stierle,Karsten Kraume,Martin Matzner*

Main category: cs.SE

TL;DR: 本文提出了"流程分析"的新概念框架，将技术分析与组织、利益相关者相结合，弥补当前流程挖掘研究中忽视人力和组织因素的不足。


<details>
  <summary>Details</summary>
Motivation: 当前数据驱动的业务流程分析（流程挖掘）过于关注技术层面，忽视了人力和组织因素。随着流程挖掘术语的普及，人们对流程分析多面性的认识正在减少。本文旨在从信息系统研究的社会技术视角出发，提出一个结合分析过程、组织和利益相关者的新视角。

Method: 采用归纳和演绎相结合的方法，概念化"流程分析"术语及其各个维度。通过大型公司实施数据驱动流程分析和自动化的实际案例研究来对比讨论研究结果。

Result: 提出了"流程分析"的新概念框架，该框架不仅包含技术分析维度，还整合了组织和利益相关者维度。通过实际案例研究验证了这一框架的适用性和价值。

Conclusion: 需要从更全面的社会技术视角重新审视数据驱动的流程分析，将技术分析与组织、利益相关者因素相结合。"流程分析"这一概念框架为理解和实施数据驱动的业务流程分析提供了更完整的视角。

Abstract: Data-driven analysis of business processes has a long tradition in research. However, recently the term of process mining is mostly used when referring to data-driven process analysis. As a consequence, awareness for the many facets of process analysis is decreasing. In particular, while an increasing focus is put onto technical aspects of the analysis, human and organisational concerns remain under the radar. Following the socio-technical perspective of information systems research, we propose a new perspective onto data-driven process analysis that combines the process of analysis with the organisation and its stakeholders. This paper conceptualises the term process analytics and its various dimensions by following both an inductive and deductive approach. The results are discussed by contrasting them to a real-life case study from a large company implementing data-driven process analysis and automation.

</details>


### [13] [One Tool Is Enough: Reinforcement Learning for Repository-Level LLM Agents](https://arxiv.org/abs/2512.20957)
*Zhaoxi Zhang,Yitong Duan,Yanzhi Zhang,Yiming Xu,Jiyan He,Yunfang Wu*

Main category: cs.SE

TL;DR: RepoNavigator：基于强化学习训练的LLM智能体，使用单一执行感知工具（跳转到被调用符号定义）进行大规模开源软件仓库问题定位，性能超越现有方法


<details>
  <summary>Details</summary>
Motivation: 在大规模开源软件仓库中定位需要修改的文件和函数具有挑战性，现有基于LLM的方法通常将其视为仓库级检索任务，依赖多个辅助工具，忽略了代码执行逻辑且使模型控制复杂化

Method: 提出RepoNavigator LLM智能体，配备单一执行感知工具——跳转到被调用符号的定义，这种统一设计反映了代码执行的实际流程同时简化了工具操作；通过强化学习端到端训练，直接从预训练模型开始，无需闭源蒸馏

Result: RL训练的RepoNavigator达到最先进性能：7B模型超越14B基线，14B模型超过32B竞争对手，32B模型甚至超过Claude-3.7等闭源模型

Conclusion: 将单一结构基础工具与强化学习训练相结合，为仓库级问题定位提供了高效且可扩展的解决方案

Abstract: Locating the files and functions requiring modification in large open-source software (OSS) repositories is challenging due to their scale and structural complexity. Existing large language model (LLM)-based methods typically treat this as a repository-level retrieval task and rely on multiple auxiliary tools, which overlook code execution logic and complicate model control. We propose RepoNavigator, an LLM agent equipped with a single execution-aware tool-jumping to the definition of an invoked symbol. This unified design reflects the actual flow of code execution while simplifying tool manipulation. RepoNavigator is trained end-to-end via Reinforcement Learning (RL) directly from a pretrained model, without any closed-source distillation. Experiments demonstrate that RL-trained RepoNavigator achieves state-of-the-art performance, with the 7B model outperforming 14B baselines, the 14B model surpassing 32B competitors, and even the 32B model exceeding closed-source models such as Claude-3.7. These results confirm that integrating a single, structurally grounded tool with RL training provides an efficient and scalable solution for repository-level issue localization.

</details>


### [14] [Artificial or Just Artful? Do LLMs Bend the Rules in Programming?](https://arxiv.org/abs/2512.21028)
*Oussama Ben Sghaier,Kevin Delcourt,Houari Sahraoui*

Main category: cs.SE

TL;DR: LLMs在代码生成中面临预训练目标与对齐约束的冲突：预训练鼓励利用所有可用信号，而对齐可能限制其使用。研究通过不同提示条件（测试可见性、使用限制）探索LLMs如何调整代码生成策略，发现测试可见性显著改变性能，并识别出四种主要适应策略。


<details>
  <summary>Details</summary>
Motivation: LLMs在自动化代码生成中广泛应用，但其表面成功往往掩盖了预训练目标与对齐选择之间的张力。预训练鼓励模型利用所有可用信号最大化成功率，而对齐（通过微调或提示）可能限制其使用。这种冲突在智能体AI设置中尤为突出，例如当智能体可以访问单元测试时，这些测试虽然用于验证，但可作为强大的上下文信号被利用，即使有明确禁止。

Method: 使用BigCodeBench（Hard）数据集，设计了五种提示条件来操纵测试可见性并施加明确或隐含的使用限制。评估了五个LLMs（四个开源和一个闭源），从正确性、代码相似性、程序大小和代码变动等维度进行分析，并通过跨模型一致性分析识别重复出现的适应策略。

Result: 测试可见性显著改变模型性能，某些模型的正确性几乎翻倍，而明确限制或部分暴露只能部分缓解这种影响。除了原始性能外，识别出四种重复出现的适应策略，其中测试驱动细化是最常见的策略。

Conclusion: 研究结果表明，当LLMs暴露于与明确指令冲突的上下文信号时，它们会调整行为，这为理解模型如何协调预训练目标与对齐约束提供了有用见解，揭示了在代码生成任务中预训练目标与对齐要求之间的根本张力。

Abstract: Large Language Models (LLMs) are widely used for automated code generation, yet their apparent successes often mask a tension between pretraining objectives and alignment choices. While pretraining encourages models to exploit all available signals to maximize success, alignment, whether through fine-tuning or prompting, may restrict their use. This conflict is especially salient in agentic AI settings, for instance when an agent has access to unit tests that, although intended for validation, act as strong contextual signals that can be leveraged regardless of explicit prohibitions. In this paper, we investigate how LLMs adapt their code generation strategies when exposed to test cases under different prompting conditions. Using the BigCodeBench (Hard) dataset, we design five prompting conditions that manipulate test visibility and impose explicit or implicit restrictions on their use. We evaluate five LLMs (four open-source and one closed-source) across correctness, code similarity, program size, and code churn, and analyze cross-model consistency to identify recurring adaptation strategies. Our results show that test visibility dramatically alters performance, correctness nearly doubles for some models, while explicit restrictions or partial exposure only partially mitigate this effect. Beyond raw performance, we identify four recurring adaptation strategies, with test-driven refinement emerging as the most frequent. These results highlight how LLMs adapt their behavior when exposed to contextual signals that conflict with explicit instructions, providing useful insight into how models reconcile pretraining objectives with alignment constraints.

</details>


### [15] [Assessing the Software Security Comprehension of Large Language Models](https://arxiv.org/abs/2512.21238)
*Mohammed Latif Siddiq,Natalie Sekerak,Antonio Karam,Maria Leal,Arvin Islam-Gomes,Joanna C. S. Santos*

Main category: cs.SE

TL;DR: 该研究系统评估了5个主流大语言模型在软件安全领域的认知能力，使用布鲁姆分类学框架，发现LLMs在低阶认知任务表现良好，但在需要推理、架构评估和安全系统创建的高阶任务上表现显著下降。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在软件开发中应用日益广泛，但其软件安全专业知识的真实水平尚不明确，需要系统评估LLMs在安全领域的认知能力。

Method: 使用布鲁姆分类学作为评估框架，涵盖六个认知维度：记忆、理解、应用、分析、评估、创造。整合多种数据集：精心设计的多选题、易受攻击代码片段(SALLM)、软件安全课程评估、真实案例研究(XBOW)以及安全软件工程课程的项目创建任务。

Result: LLMs在回忆事实和识别已知漏洞等低阶认知任务上表现良好，但在需要推理、架构评估和安全系统创建的高阶任务上性能显著下降。研究提出了软件安全知识边界概念，识别了LLMs在51个重复出现的误解模式。

Conclusion: 虽然LLMs在基础软件安全知识方面有一定能力，但在需要深度推理和创造性安全设计的高阶认知任务上存在明显局限，这为LLMs在安全关键软件开发中的应用提供了重要参考。

Abstract: Large language models (LLMs) are increasingly used in software development, but their level of software security expertise remains unclear. This work systematically evaluates the security comprehension of five leading LLMs: GPT-4o-Mini, GPT-5-Mini, Gemini-2.5-Flash, Llama-3.1, and Qwen-2.5, using Blooms Taxonomy as a framework. We assess six cognitive dimensions: remembering, understanding, applying, analyzing, evaluating, and creating. Our methodology integrates diverse datasets, including curated multiple-choice questions, vulnerable code snippets (SALLM), course assessments from an Introduction to Software Security course, real-world case studies (XBOW), and project-based creation tasks from a Secure Software Engineering course. Results show that while LLMs perform well on lower-level cognitive tasks such as recalling facts and identifying known vulnerabilities, their performance degrades significantly on higher-order tasks that require reasoning, architectural evaluation, and secure system creation. Beyond reporting aggregate accuracy, we introduce a software security knowledge boundary that identifies the highest cognitive level at which a model consistently maintains reliable performance. In addition, we identify 51 recurring misconception patterns exhibited by LLMs across Blooms levels.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [16] [MegaRAG: Multimodal Knowledge Graph-Based Retrieval Augmented Generation](https://arxiv.org/abs/2512.20626)
*Chi-Hsiang Hsiao,Yi-Cheng Wang,Tzung-Sheng Lin,Yi-Ren Yeh,Chu-Song Chen*

Main category: cs.AI

TL;DR: 该论文提出了一种多模态知识图谱增强的检索生成方法，通过整合视觉线索来改进对长文档的理解和推理能力。


<details>
  <summary>Details</summary>
Motivation: 传统检索增强生成方法在处理长文档和跨模态内容时存在局限性：1）上下文窗口有限，难以进行深度推理；2）现有知识图谱方法仅限于文本输入，无法利用视觉信息；3）视觉文档理解需要整合文本、视觉和空间线索。

Method: 提出多模态知识图谱增强的检索生成框架，将视觉线索整合到知识图谱构建、检索过程和答案生成三个阶段，实现跨模态推理。

Result: 实验结果表明，该方法在全局和细粒度问答任务上均优于现有的检索增强生成方法，在文本和多模态语料库上都取得了更好的性能。

Conclusion: 通过整合视觉线索到知识图谱增强的检索生成框架中，能够显著提升对复杂多模态内容的理解和推理能力，为跨模态文档理解提供了有效解决方案。

Abstract: Retrieval-augmented generation (RAG) enables large language models (LLMs) to dynamically access external information, which is powerful for answering questions over previously unseen documents. Nonetheless, they struggle with high-level conceptual understanding and holistic comprehension due to limited context windows, which constrain their ability to perform deep reasoning over long-form, domain-specific content such as full-length books. To solve this problem, knowledge graphs (KGs) have been leveraged to provide entity-centric structure and hierarchical summaries, offering more structured support for reasoning. However, existing KG-based RAG solutions remain restricted to text-only inputs and fail to leverage the complementary insights provided by other modalities such as vision. On the other hand, reasoning from visual documents requires textual, visual, and spatial cues into structured, hierarchical concepts. To address this issue, we introduce a multimodal knowledge graph-based RAG that enables cross-modal reasoning for better content understanding. Our method incorporates visual cues into the construction of knowledge graphs, the retrieval phase, and the answer generation process. Experimental results across both global and fine-grained question answering tasks show that our approach consistently outperforms existing RAG-based approaches on both textual and multimodal corpora.

</details>


### [17] [Proceedings of the 20th International Conference on Knowledge, Information and Creativity Support Systems (KICSS 2025)](https://arxiv.org/abs/2512.20628)
*Edited by Tessai Hayama,Takayuki Ito,Takahiro Uchiya,Motoki Miura,Takahiro Kawaji,Takaya Yuizono,Atsuo Yoshitaka,Tokuro Matsuo,Shun Okuhara,Jawad Haqbeen,Sofia Sahab,Wen Gu,Shiyao Ding*

Main category: cs.AI

TL;DR: 这是KICSS 2025国际会议论文集的前言介绍，不是研究论文本身


<details>
  <summary>Details</summary>
Motivation: 介绍第20届国际知识、信息与创造力支持系统会议论文集的基本情况

Method: 会议论文集编辑，包含双盲评审流程和后续期刊推荐机制

Result: 成功组织了KICSS 2025会议并出版了包含同行评审论文的论文集

Conclusion: 该论文集为人工智能、知识工程、人机交互和创造力支持系统领域的研究者提供了多学科交流平台

Abstract: This volume presents the proceedings of the 20th International Conference on Knowledge, Information and Creativity Support Systems (KICSS 2025), held in Nagaoka, Japan, on December 3-5, 2025. The conference, organized in cooperation with the IEICE Proceedings Series, provides a multidisciplinary forum for researchers in artificial intelligence, knowledge engineering, human-computer interaction, and creativity support systems. The proceedings include peer-reviewed papers accepted through a double-blind review process. Selected papers have been recommended for publication in IEICE Transactions on Information and Systems after an additional peer-review process.

</details>


### [18] [MicroProbe: Efficient Reliability Assessment for Foundation Models with Minimal Data](https://arxiv.org/abs/2512.20630)
*Aayam Bansal,Ishaan Gangwani*

Main category: cs.AI

TL;DR: Microprobe是一种新颖的可靠性评估方法，仅需100个战略选择的探测样本即可全面评估基础模型可靠性，相比传统方法显著降低成本并提高效率。


<details>
  <summary>Details</summary>
Motivation: 传统基础模型可靠性评估需要数千个评估样本，计算成本高且耗时，难以满足实际部署需求。需要一种更高效的评估方法来支持负责任的人工智能部署。

Method: 结合五个关键可靠性维度的战略提示多样性、先进的不确定性量化和自适应加权，通过仅100个战略选择的探测样本高效检测潜在故障模式。

Result: 在多个语言模型和跨领域验证中，microprobe相比随机采样基线实现了23.5%更高的综合可靠性分数，具有统计学显著性(p<0.001)。专家验证评分为4.14/5.0，评估成本降低90%，同时保持95%的传统方法覆盖率。

Conclusion: Microprobe方法填补了高效模型评估的关键空白，为负责任的人工智能部署提供了实用且高效的可靠性评估解决方案。

Abstract: Foundation model reliability assessment typically requires thousands of evaluation examples, making it computationally expensive and time-consuming for real-world deployment. We introduce microprobe, a novel approach that achieves comprehensive reliability assessment using only 100 strategically selected probe examples. Our method combines strategic prompt diversity across five key reliability dimensions with advanced uncertainty quantification and adaptive weighting to efficiently detect potential failure modes. Through extensive empirical evaluation on multiple language models (GPT-2 variants, GPT-2 Medium, GPT-2 Large) and cross-domain validation (healthcare, finance, legal), we demonstrate that microprobe achieves 23.5% higher composite reliability scores compared to random sampling baselines, with exceptional statistical significance (p < 0.001, Cohen's d = 1.21). Expert validation by three AI safety researchers confirms the effectiveness of our strategic selection, rating our approach 4.14/5.0 versus 3.14/5.0 for random selection. microprobe completes reliability assessment with 99.9% statistical power while representing a 90% reduction in assessment cost and maintaining 95% of traditional method coverage. Our approach addresses a critical gap in efficient model evaluation for responsible AI deployment.

</details>


### [19] [Erkang-Diagnosis-1.1 Technical Report](https://arxiv.org/abs/2512.20632)
*Jianbing Ma,Ao Feng,Zhenjie Gao,Xinyu Song,Li Su,Bin Chen,Wei Wang,Jiamin Wu*

Main category: cs.AI

TL;DR: Erkang-Diagnosis-1.1是基于阿里通义千问-3模型开发的AI医疗咨询助手，整合了500GB高质量医学知识，采用增强预训练和检索增强生成混合方法，能在3-5轮交互中准确理解症状并提供诊断建议，在综合医学考试中表现优于GPT-4。


<details>
  <summary>Details</summary>
Motivation: 开发安全、可靠、专业的AI健康顾问，整合大规模高质量医学知识，为用户提供智能健康伴侣，赋能基层医疗和健康管理。

Method: 基于阿里通义千问-3模型，整合约500GB高质量结构化医学知识，采用增强预训练和检索增强生成的混合方法，通过3-5轮高效交互理解用户症状。

Result: Erkang-Diagnosis-1.1在综合医学考试中表现优于GPT-4，能够准确理解用户症状、进行初步分析，并提供有价值的诊断建议和健康指导。

Conclusion: Erkang-Diagnosis-1.1是一个安全、可靠、专业的AI健康顾问，能够成为用户的智能健康伴侣，有效赋能基层医疗和健康管理。

Abstract: This report provides a detailed introduction to Erkang-Diagnosis-1.1 model, our AI healthcare consulting assistant developed using Alibaba Qwen-3 model. The Erkang model integrates approximately 500GB of high-quality structured medical knowledge, employing a hybrid approach combining enhanced pre-training and retrieval-enhanced generation to create a secure, reliable, and professional AI health advisor. Through 3-5 efficient interaction rounds, Erkang Diagnosis can accurately understand user symptoms, conduct preliminary analysis, and provide valuable diagnostic suggestions and health guidance. Designed to become users intelligent health companions, it empowers primary healthcare and health management. To validate, Erkang-Diagnosis-1.1 leads GPT-4 in terms of comprehensive medical exams.

</details>


### [20] [Reasoning Relay: Evaluating Stability and Interchangeability of Large Language Models in Mathematical Reasoning](https://arxiv.org/abs/2512.20647)
*Leo Lu,Jonathan Zhang,Sean Chua,Spencer Kim,Kevin Zhu,Sean O'Brien,Vasu Sharma*

Main category: cs.AI

TL;DR: 研究探索不同大语言模型之间推理链的可互换性，发现部分完成的推理链可以被其他模型可靠地继续，有时甚至能提升最终准确率。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注通过内部推理策略提升模型性能，但对不同模型间推理的互换性了解甚少。本研究旨在探索一个模型部分完成的推理链是否可以被另一个模型可靠地继续，无论是同一模型家族内还是跨家族。

Method: 使用token级对数概率阈值在早期、中期和晚期阶段截断基线模型（Gemma-3-4B-IT和LLaMA-3.1-70B-Instruct）的推理链，然后用Gemma-3-1B-IT和LLaMA-3.1-8B-Instruct进行继续实验，测试家族内和跨家族行为。评估流程结合截断阈值和过程奖励模型（PRM），提供可复现的框架来评估推理稳定性。

Result: 评估显示混合推理链通常能保持，在某些情况下甚至能提高最终准确率和逻辑结构。这表明可互换性是推理模型的一种新兴行为特性。

Conclusion: 推理的可互换性为协作AI系统中可靠的模块化推理提供了新范式，揭示了推理模型在模型替换下保持连贯性和可靠性的能力。

Abstract: Chain-of-Thought (CoT) prompting has significantly advanced the reasoning capabilities of large language models (LLMs). While prior work focuses on improving model performance through internal reasoning strategies, little is known about the interchangeability of reasoning across different models. In this work, we explore whether a partially completed reasoning chain from one model can be reliably continued by another model, either within the same model family or across families. We achieve this by assessing the sufficiency of intermediate reasoning traces as transferable scaffolds for logical coherence and final answer accuracy. We interpret this interchangeability as a means of examining inference-time trustworthiness, probing whether reasoning remains both coherent and reliable under model substitution. Using token-level log-probability thresholds to truncate reasoning at early, mid, and late stages from our baseline models, Gemma-3-4B-IT and LLaMA-3.1-70B-Instruct, we conduct continuation experiments with Gemma-3-1B-IT and LLaMA-3.1-8B-Instruct to test intra-family and cross-family behaviors. Our evaluation pipeline leverages truncation thresholds with a Process Reward Model (PRM), providing a reproducible framework for assessing reasoning stability via model interchange. Evaluations with a PRM reveal that hybrid reasoning chains often preserve, and in some cases even improve, final accuracy and logical structure. Our findings point towards interchangeability as an emerging behavioral property of reasoning models, offering insights into new paradigms for reliable modular reasoning in collaborative AI systems.

</details>


### [21] [AIAuditTrack: A Framework for AI Security system](https://arxiv.org/abs/2512.20649)
*Zixun Luo,Yuhang Fan,Yufei Li,Youzhi Zhang,Hengyu Lin,Ziqi Wang*

Main category: cs.AI

TL;DR: AiAuditTrack (AAT) 是一个基于区块链的AI使用流量记录与治理框架，利用去中心化身份和可验证凭证建立可信AI实体，记录交互轨迹实现跨系统监管和审计。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型驱动的AI应用快速扩展，AI交互数据激增，带来了安全、问责和风险可追溯性方面的紧迫挑战，需要建立可信的审计和治理机制。

Method: 1. 使用去中心化身份(DID)和可验证凭证(VC)建立可信可识别的AI实体
2. 将AI实体建模为动态交互图中的节点，边表示时间特定的行为轨迹
3. 在链上记录实体间交互轨迹
4. 提出风险扩散算法追踪风险行为源头并在相关实体间传播早期预警

Result: 通过区块链交易每秒(TPS)指标评估系统性能，证明AAT在大规模交互记录下的可行性和稳定性，能够为复杂多智能体环境提供可扩展、可验证的解决方案。

Conclusion: AAT为复杂多智能体环境中的AI审计、风险管理和责任归属提供了一个可扩展且可验证的解决方案，实现了跨系统监管和风险追溯能力。

Abstract: The rapid expansion of AI-driven applications powered by large language models has led to a surge in AI interaction data, raising urgent challenges in security, accountability, and risk traceability. This paper presents AiAuditTrack (AAT), a blockchain-based framework for AI usage traffic recording and governance. AAT leverages decentralized identity (DID) and verifiable credentials (VC) to establish trusted and identifiable AI entities, and records inter-entity interaction trajectories on-chain to enable cross-system supervision and auditing. AI entities are modeled as nodes in a dynamic interaction graph, where edges represent time-specific behavioral trajectories. Based on this model, a risk diffusion algorithm is proposed to trace the origin of risky behaviors and propagate early warnings across involved entities. System performance is evaluated using blockchain Transactions Per Second (TPS) metrics, demonstrating the feasibility and stability of AAT under large-scale interaction recording. AAT provides a scalable and verifiable solution for AI auditing, risk management, and responsibility attribution in complex multi-agent environments.

</details>


### [22] [Mixture of Attention Schemes (MoAS): Learning to Route Between MHA, GQA, and MQA](https://arxiv.org/abs/2512.20650)
*Esmail Gumaan*

Main category: cs.AI

TL;DR: 提出MoAS架构，通过学习的路由器为每个token动态选择最优注意力机制（MHA、GQA或MQA），在保持模型性能的同时提升推理效率。


<details>
  <summary>Details</summary>
Motivation: Transformer模型中注意力机制的选择需要在建模质量和推理效率之间权衡：多头注意力（MHA）质量最好但KV缓存内存需求大，多查询注意力（MQA）和分组查询注意力（GQA）虽然减少内存使用但往往牺牲性能。

Method: 提出混合注意力方案（MoAS），通过学习的路由器为每个token动态选择最优的注意力机制（MHA、GQA或MQA），而不是静态混合方案。

Result: 在WikiText-2上的实验结果显示，动态路由（验证损失2.3074）优于静态混合（2.3093），性能与MHA基线相当，同时具备条件计算效率的潜力。

Conclusion: MoAS通过动态选择注意力机制，在保持模型性能的同时提供了推理效率优化的可能性，验证了动态路由方法的有效性。

Abstract: The choice of attention mechanism in Transformer models involves a critical trade-off between modeling quality and inference efficiency. Multi-Head Attention (MHA) offers the best quality but suffers from large Key-Value (KV) cache memory requirements during inference. Multi-Query Attention (MQA) and Grouped-Query Attention (GQA) reduce memory usage but often at the cost of model performance. In this work, we propose Mixture of Attention Schemes (MoAS), a novel architecture that dynamically selects the optimal attention scheme (MHA, GQA, or MQA) for each token via a learned router. We demonstrate that dynamic routing performs better than static averaging of schemes and achieves performance competitive with the MHA baseline while offering potential for conditional compute efficiency. Experimental results on WikiText-2 show that dynamic routing (val loss 2.3074) outperforms a static mixture (2.3093), validating the effectiveness of the proposed method. Our code is available at https://github.com/Esmail-ibraheem/Mixture-of-Attention-Schemes-MoAS.

</details>


### [23] [Memory Bear AI A Breakthrough from Memory to Cognition Toward Artificial General Intelligence](https://arxiv.org/abs/2512.20651)
*Deliang Wen,Ke Sun*

Main category: cs.AI

TL;DR: Memory Bear系统通过构建类人记忆架构，解决LLM在内存限制、知识遗忘、信息冗余和幻觉生成等方面的问题，显著提升长期对话中的知识保真度和检索效率。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型面临内存限制、上下文窗口受限、长期知识遗忘、冗余信息积累和幻觉生成等固有局限，严重制约了持续对话和个性化服务的发展。

Method: 基于认知科学原理构建类人记忆架构，整合多模态信息感知、动态记忆维护和自适应认知服务，实现对LLM记忆机制的全链重构。

Result: 在医疗健康、企业运营、教育等多个领域展示出显著的工程创新和性能突破，相比现有解决方案（如Mem0、MemGPT、Graphiti）在准确性、令牌效率和响应延迟等关键指标上表现更优。

Conclusion: Memory Bear系统通过记忆-认知整合显著提升了上下文适应性和推理能力，降低了幻觉率，标志着AI从"记忆"向"认知"迈进的关键一步。

Abstract: Large language models (LLMs) face inherent limitations in memory, including restricted context windows, long-term knowledge forgetting, redundant information accumulation, and hallucination generation. These issues severely constrain sustained dialogue and personalized services. This paper proposes the Memory Bear system, which constructs a human-like memory architecture grounded in cognitive science principles. By integrating multimodal information perception, dynamic memory maintenance, and adaptive cognitive services, Memory Bear achieves a full-chain reconstruction of LLM memory mechanisms. Across domains such as healthcare, enterprise operations, and education, Memory Bear demonstrates substantial engineering innovation and performance breakthroughs. It significantly improves knowledge fidelity and retrieval efficiency in long-term conversations, reduces hallucination rates, and enhances contextual adaptability and reasoning capability through memory-cognition integration. Experimental results show that, compared with existing solutions (e.g., Mem0, MemGPT, Graphiti), Memory Bear outperforms them across key metrics, including accuracy, token efficiency, and response latency. This marks a crucial step forward in advancing AI from "memory" to "cognition".

</details>


### [24] [From Fake Focus to Real Precision: Confusion-Driven Adversarial Attention Learning in Transformers](https://arxiv.org/abs/2512.20661)
*Yawei Liu*

Main category: cs.AI

TL;DR: 提出AFA对抗反馈注意力训练机制，通过动态掩码策略和策略梯度优化，使Transformer模型自动重新分配注意力权重到任务相关但较少出现的词汇上，提升情感分析性能


<details>
  <summary>Details</summary>
Motivation: 现有Transformer模型在情感分析任务中注意力主要分配给常见词汇，忽略了不常见但高度任务相关的词汇，导致性能受限

Method: 提出对抗反馈注意力训练机制：1) 动态掩码策略尝试掩码不同词汇欺骗判别器；2) 判别器检测掩码引起的显著差异；3) 利用Transformer对token级扰动的敏感性，采用策略梯度优化注意力分布

Result: 在三个公开数据集上达到最先进性能，将该训练机制应用于增强大语言模型的注意力后，性能进一步提升12.6%

Conclusion: AFA机制能有效引导Transformer模型关注任务相关词汇，无需人工标注即可优化注意力分布，显著提升情感分析性能

Abstract: Transformer-based models have been widely adopted for sentiment analysis tasks due to their exceptional ability to capture contextual information. However, these methods often exhibit suboptimal accuracy in certain scenarios. By analyzing their attention distributions, we observe that existing models tend to allocate attention primarily to common words, overlooking less popular yet highly task-relevant terms, which significantly impairs overall performance. To address this issue, we propose an Adversarial Feedback for Attention(AFA) training mechanism that enables the model to automatically redistribute attention weights to appropriate focal points without requiring manual annotations. This mechanism incorporates a dynamic masking strategy that attempts to mask various words to deceive a discriminator, while the discriminator strives to detect significant differences induced by these masks. Additionally, leveraging the sensitivity of Transformer models to token-level perturbations, we employ a policy gradient approach to optimize attention distributions, which facilitates efficient and rapid convergence. Experiments on three public datasets demonstrate that our method achieves state-of-the-art results. Furthermore, applying this training mechanism to enhance attention in large language models yields a further performance improvement of 12.6%

</details>


### [25] [Quantifying Laziness, Decoding Suboptimality, and Context Degradation in Large Language Models](https://arxiv.org/abs/2512.20662)
*Yiqing Ma,Jung-Hua Liu*

Main category: cs.AI

TL;DR: 该研究通过三个实验量化了LLMs的行为缺陷：懒惰（复杂指令执行不完整）、解码次优性（短视解码）和上下文退化（长对话中遗忘指令）。研究发现LLMs普遍存在懒惰问题，但解码次优性证据有限，且在长对话中表现出意外的上下文保持能力。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型经常表现出行为缺陷，如懒惰（过早截断响应或部分遵守多部分请求）、解码次优性（由于短视解码而未能选择更高质量的序列）和上下文退化（在长对话中忘记或忽略核心指令）。研究旨在量化这些现象，评估现代LLMs在这些方面的表现。

Method: 研究进行了三个受控实验（A、B、C），在多个先进LLMs（OpenAI GPT-4变体和DeepSeek）上测试这些现象。实验A测试懒惰问题（复杂多部分指令的遵守情况），实验B测试解码次优性（简单推理任务中的贪婪解码表现），实验C测试上下文退化（200轮混乱对话中的指令保持能力）。

Result: 1. 懒惰问题普遍存在：模型经常省略所需部分或未能满足长度要求；2. 解码次优性证据有限：在简单推理任务中，模型的贪婪答案似乎与其最高置信度解决方案一致；3. 上下文退化表现出意外鲁棒性：在200轮混乱对话测试中，模型保持关键事实和指令的能力远超预期。

Conclusion: 虽然遵守详细指令仍然是一个开放挑战，但现代LLMs可能在内部缓解了一些假设的失败模式（如上下文遗忘）。研究讨论了可靠性影响，将发现与先前关于指令遵循和长上下文处理的工作联系起来，并推荐了减少懒惰和增强多指令遵守的策略（如自我精炼和动态提示）。

Abstract: Large Language Models (LLMs) often exhibit behavioral artifacts such as laziness (premature truncation of responses or partial compliance with multi-part requests), decoding suboptimality (failure to select higher-quality sequences due to myopic decoding), and context degradation (forgetting or ignoring core instructions over long conversations). We conducted three controlled experiments (A, B, and C) to quantify these phenomena across several advanced LLMs (OpenAI GPT-4 variant, DeepSeek). Our results indicate widespread laziness in satisfying complex multi-part instructions: models frequently omitted required sections or failed to meet length requirements despite explicit prompting. However, we found limited evidence of decoding suboptimality in a simple reasoning task (the models' greedy answers appeared to align with their highest-confidence solution), and we observed surprising robustness against context degradation in a 200-turn chaotic conversation test - the models maintained key facts and instructions far better than expected. These findings suggest that while compliance with detailed instructions remains an open challenge, modern LLMs may internally mitigate some hypothesized failure modes (such as context forgetting) in straightforward retrieval scenarios. We discuss implications for reliability, relate our findings to prior work on instruction-following and long-context processing, and recommend strategies (such as self-refinement and dynamic prompting) to reduce laziness and bolster multi-instruction compliance.

</details>


### [26] [Bridging the AI Trustworthiness Gap between Functions and Norms](https://arxiv.org/abs/2512.20671)
*Daan Di Scala,Sophie Lathouwers,Michael van Bekkum*

Main category: cs.AI

TL;DR: 本文提出需要建立连接功能性可信AI和规范性可信AI的语义桥梁，以解决两者之间的鸿沟，帮助评估AI系统的可信度。


<details>
  <summary>Details</summary>
Motivation: 当前功能性可信AI（关注技术实现）和规范性可信AI（关注法规要求）之间存在鸿沟，这使得难以评估AI系统的可信度，需要建立连接两者的桥梁。

Method: 提出开发一种概念性语义语言作为桥梁，能够匹配功能性可信AI和规范性可信AI，帮助开发者评估AI系统的可信度，并协助利益相关者将规范转化为具体实施步骤。

Result: 本文分析了当前研究现状，识别了功能性可信AI和规范性可信AI之间的差距，讨论了开发语义语言的起点和预期效果。

Conclusion: 需要开发语义语言来连接功能性可信AI和规范性可信AI，为可信AI评估提供关键考虑因素和未来行动方向。

Abstract: Trustworthy Artificial Intelligence (TAI) is gaining traction due to regulations and functional benefits. While Functional TAI (FTAI) focuses on how to implement trustworthy systems, Normative TAI (NTAI) focuses on regulations that need to be enforced. However, gaps between FTAI and NTAI remain, making it difficult to assess trustworthiness of AI systems. We argue that a bridge is needed, specifically by introducing a conceptual language which can match FTAI and NTAI. Such a semantic language can assist developers as a framework to assess AI systems in terms of trustworthiness. It can also help stakeholders translate norms and regulations into concrete implementation steps for their systems. In this position paper, we describe the current state-of-the-art and identify the gap between FTAI and NTAI. We will discuss starting points for developing a semantic language and the envisioned effects of it. Finally, we provide key considerations and discuss future actions towards assessment of TAI.

</details>


### [27] [From Pilots to Practices: A Scoping Review of GenAI-Enabled Personalization in Computer Science Education](https://arxiv.org/abs/2512.20714)
*Iman Reihanian,Yunfei Hou,Qingquan Sun*

Main category: cs.AI

TL;DR: 这篇范围综述分析了2023-2025年间32项研究，探讨生成式AI在高等教育计算机科学教育中的个性化应用效果，识别了五大应用领域和成功设计模式，提出了探索性采用框架。


<details>
  <summary>Details</summary>
Motivation: 生成式AI能够实现大规模个性化计算机科学教育，但需要明确这种个性化是支持还是削弱学习效果，因此需要系统梳理现有研究证据。

Method: 采用范围综述方法，从259条记录中有目的地抽样32项研究（2023-2025年），分析个性化机制和有效性信号，识别应用领域和设计选择对学习结果的影响。

Result: 识别了五个应用领域：智能辅导、个性化材料、形成性反馈、AI增强评估和代码审查。成功设计包含解释优先指导、解决方案保留、分级提示阶梯和基于学生产出的锚定。成功实施有四个共同模式：基于学生产出的情境感知辅导、需要反思的多级提示结构、与传统CS基础设施结合、人工参与的质量保证。

Conclusion: 生成式AI可以作为精确支架机制，但需要嵌入可审计的工作流程中，保持学生的有效挣扎同时扩展个性化支持。提出了强调试点、工具化、学习保护默认设置和基于证据扩展的探索优先采用框架，并识别了学术诚信、隐私、偏见与公平、过度依赖等风险及缓解措施。

Abstract: Generative AI enables personalized computer science education at scale, yet questions remain about whether such personalization supports or undermines learning. This scoping review synthesizes 32 studies (2023-2025) purposively sampled from 259 records to map personalization mechanisms and effectiveness signals in higher-education computer science contexts. We identify five application domains: intelligent tutoring, personalized materials, formative feedback, AI-augmented assessment, and code review, and analyze how design choices shape learning outcomes. Designs incorporating explanation-first guidance, solution withholding, graduated hint ladders, and artifact grounding (student code, tests, and rubrics) consistently show more positive learning processes than unconstrained chat interfaces. Successful implementations share four patterns: context-aware tutoring anchored in student artifacts, multi-level hint structures requiring reflection, composition with traditional CS infrastructure (autograders and rubrics), and human-in-the-loop quality assurance. We propose an exploration-first adoption framework emphasizing piloting, instrumentation, learning-preserving defaults, and evidence-based scaling. Recurrent risks include academic integrity, privacy, bias and equity, and over-reliance, and we pair these with operational mitigation. The evidence supports generative AI as a mechanism for precision scaffolding when embedded in audit-ready workflows that preserve productive struggle while scaling personalized support.

</details>


### [28] [AgentMath: Empowering Mathematical Reasoning for Large Language Models via Tool-Augmented Agent](https://arxiv.org/abs/2512.20745)
*Haipeng Luo,Huawen Feng,Qingfeng Sun,Can Xu,Kai Zheng,Yufei Wang,Tao Yang,Han Hu,Yansong Tang,Di Wang*

Main category: cs.AI

TL;DR: AgentMath是一个将语言模型推理能力与代码解释器计算精度相结合的智能体框架，用于高效解决复杂数学问题，在多个数学竞赛基准上达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型在自然语言推理方面取得了显著进展，但在处理需要复杂数学运算的问题时仍然计算效率低下且准确性不足。现有方法无法有效结合语言模型的推理能力和代码解释器的计算精度。

Method: 提出了三个关键创新：1）将自然语言思维链自动转换为结构化工具增强轨迹的方法，生成高质量SFT数据；2）新颖的智能体强化学习范式，动态交错自然语言生成与实时代码执行；3）高效的训练系统，包含请求级异步rollout调度、智能体部分rollout和前缀感知加权负载均衡等技术。

Result: AgentMath在AIME24、AIME25和HMMT25等具有挑战性的数学竞赛基准上取得了最先进的性能。AgentMath-30B-A3B分别达到了90.6%、86.4%和73.8%的准确率，实现了先进的能力。训练系统实现了4-5倍的加速。

Conclusion: 该方法验证了将语言模型推理与代码执行相结合的有效性，为构建更复杂和可扩展的数学推理智能体铺平了道路。

Abstract: Large Reasoning Models (LRMs) like o3 and DeepSeek-R1 have achieved remarkable progress in natural language reasoning with long chain-of-thought. However, they remain computationally inefficient and struggle with accuracy when solving problems requiring complex mathematical operations. In this work, we present AgentMath, an agent framework that seamlessly integrates language models' reasoning capabilities with code interpreters' computational precision to efficiently tackle complex mathematical problems. Our approach introduces three key innovations: (1) An automated method that converts natural language chain-of-thought into structured tool-augmented trajectories, generating high-quality supervised fine-tuning (SFT) data to alleviate data scarcity; (2) A novel agentic reinforcement learning (RL) paradigm that dynamically interleaves natural language generation with real-time code execution. This enables models to autonomously learn optimal tool-use strategies through multi-round interactive feedback, while fostering emergent capabilities in code refinement and error correction; (3) An efficient training system incorporating innovative techniques, including request-level asynchronous rollout scheduling, agentic partial rollout, and prefix-aware weighted load balancing, achieving 4-5x speedup and making efficient RL training feasible on ultra-long sequences with scenarios with massive tool calls.Extensive evaluations show that AgentMath achieves state-of-the-art performance on challenging mathematical competition benchmarks including AIME24, AIME25, and HMMT25. Specifically, AgentMath-30B-A3B attains 90.6%, 86.4%, and 73.8% accuracy respectively, achieving advanced capabilities.These results validate the effectiveness of our approach and pave the way for building more sophisticated and scalable mathematical reasoning agents.

</details>


### [29] [Beyond Context: Large Language Models Failure to Grasp Users Intent](https://arxiv.org/abs/2512.21110)
*Ahmed M. Hussain,Salahuddin Salahuddin,Panos Papadimitratos*

Main category: cs.AI

TL;DR: 研究发现当前大语言模型的安全机制存在重大漏洞：无法理解上下文和识别用户意图，导致恶意用户可以通过情感框架、渐进揭示和学术论证等系统方法绕过安全防护。


<details>
  <summary>Details</summary>
Motivation: 当前LLM安全方法主要关注显性有害内容，但忽视了关键漏洞：无法理解上下文和识别用户意图。这为恶意用户提供了可系统利用的漏洞来绕过安全机制。

Method: 对多个最先进的LLM（包括ChatGPT、Claude、Gemini和DeepSeek）进行实证评估，分析通过情感框架、渐进揭示和学术论证等技术绕过可靠安全机制的情况。

Result: 推理功能配置反而放大了利用效果，提高了事实精确性但未能质疑潜在意图。Claude Opus 4.1是例外，在某些用例中优先考虑意图检测而非信息提供。当前架构设计存在系统性漏洞。

Conclusion: 需要范式转变，将上下文理解和意图识别作为核心安全能力，而非事后保护机制，以解决当前LLM安全架构的根本性缺陷。

Abstract: Current Large Language Models (LLMs) safety approaches focus on explicitly harmful content while overlooking a critical vulnerability: the inability to understand context and recognize user intent. This creates exploitable vulnerabilities that malicious users can systematically leverage to circumvent safety mechanisms. We empirically evaluate multiple state-of-the-art LLMs, including ChatGPT, Claude, Gemini, and DeepSeek. Our analysis demonstrates the circumvention of reliable safety mechanisms through emotional framing, progressive revelation, and academic justification techniques. Notably, reasoning-enabled configurations amplified rather than mitigated the effectiveness of exploitation, increasing factual precision while failing to interrogate the underlying intent. The exception was Claude Opus 4.1, which prioritized intent detection over information provision in some use cases. This pattern reveals that current architectural designs create systematic vulnerabilities. These limitations require paradigmatic shifts toward contextual understanding and intent recognition as core safety capabilities rather than post-hoc protective mechanisms.

</details>


### [30] [Context-Sensitive Abstractions for Reinforcement Learning with Parameterized Actions](https://arxiv.org/abs/2512.20831)
*Rashmeet Kaur Nayyar,Naman Shah,Siddharth Srivastava*

Main category: cs.AI

TL;DR: 本文提出了一种新的强化学习方法，用于处理具有参数化动作空间的顺序决策问题，通过在线学习状态和动作抽象来提高样本效率。


<details>
  <summary>Details</summary>
Motivation: 现实世界中的顺序决策问题通常涉及参数化动作空间，需要同时处理离散动作选择和连续动作参数决策。现有方法存在严重局限性：规划方法需要手工制作动作模型，标准强化学习算法要么针对离散动作要么针对连续动作设计，而少数处理参数化动作的RL方法通常依赖领域特定工程且未能充分利用这些空间的潜在结构。

Method: 本文扩展了RL算法到长视野、稀疏奖励的参数化动作设置，通过使智能体能够在线自主学习状态和动作抽象。引入的算法在学习过程中逐步细化这些抽象，在状态-动作空间的关键区域增加细粒度细节，其中更高分辨率能提高性能。

Result: 在多个连续状态、参数化动作领域中，这种抽象驱动的方法使TD(λ)算法实现了比最先进基线方法显著更高的样本效率。

Conclusion: 通过在线学习状态和动作抽象，本文提出的方法能够有效处理参数化动作空间的强化学习问题，在长视野、稀疏奖励设置下表现出优越的样本效率。

Abstract: Real-world sequential decision-making often involves parameterized action spaces that require both, decisions regarding discrete actions and decisions about continuous action parameters governing how an action is executed. Existing approaches exhibit severe limitations in this setting -- planning methods demand hand-crafted action models, and standard reinforcement learning (RL) algorithms are designed for either discrete or continuous actions but not both, and the few RL methods that handle parameterized actions typically rely on domain-specific engineering and fail to exploit the latent structure of these spaces. This paper extends the scope of RL algorithms to long-horizon, sparse-reward settings with parameterized actions by enabling agents to autonomously learn both state and action abstractions online. We introduce algorithms that progressively refine these abstractions during learning, increasing fine-grained detail in the critical regions of the state-action space where greater resolution improves performance. Across several continuous-state, parameterized-action domains, our abstraction-driven approach enables TD($λ$) to achieve markedly higher sample efficiency than state-of-the-art baselines.

</details>


### [31] [MAR:Multi-Agent Reflexion Improves Reasoning Abilities in LLMs](https://arxiv.org/abs/2512.20845)
*Onat Ozer,Grace Wu,Yuchen Wang,Daniel Dosti,Honghao Zhang,Vivi De La Rue*

Main category: cs.AI

TL;DR: 多智能体多角色辩论方法通过引入多样化的反思视角，解决了单LLM自我反思时思维退化、重复错误的问题，在推理任务中取得了更好的性能。


<details>
  <summary>Details</summary>
Motivation: 现有LLM通过自我反思改进推理性能的方法存在局限性：单LLM的持续自我反思会导致思维退化，即使知道错误也会重复犯错。需要寻找更有效的反思生成方法。

Method: 提出了多智能体多角色辩论方法，通过多个具有不同角色的智能体进行辩论来生成反思，这种方法能产生更多样化的反思内容。

Result: 在HotPot QA（问答）任务上达到47% EM准确率，在HumanEval（编程）任务上达到82.7%准确率，均超越了单LLM反思方法的性能。

Conclusion: 多智能体多角色辩论方法能有效解决单LLM自我反思的思维退化问题，通过引入多样化的反思视角显著提升了LLM在推理任务中的性能。

Abstract: LLMs have shown the capacity to improve their performance on reasoning tasks through reflecting on their mistakes, and acting with these reflections in mind. However, continual reflections of the same LLM onto itself exhibit degeneration of thought, where the LLM continues to repeat the same errors again and again even with the knowledge that its wrong. To address this problem, we instead introduce multi-agent with multi-persona debators as the method to generate reflections. Through out extensive experimentation, we've found that the leads to better diversity of in the reflections generated by the llm agent. We demonstrate an accuracy of 47% EM HotPot QA (question answering) and 82.7% on HumanEval (programming), both performances surpassing reflection with a single llm.

</details>


### [32] [The Silent Scholar Problem: A Probabilistic Framework for Breaking Epistemic Asymmetry in LLM Agents](https://arxiv.org/abs/2512.20884)
*Zan-Kai Chong,Hiroyuki Ohsaki,Bryan Ng*

Main category: cs.AI

TL;DR: 提出概率框架解决LLM智能体单向知识消费问题，通过Beta-Bernoulli分布建模信念，利用不确定性驱动双向知识交换，实现主动学习和集体智能提升。


<details>
  <summary>Details</summary>
Motivation: 当前基于LLM和RAG的自主智能体存在"认知不对称"问题，只能单向消费数字内容，导致冗余推理和集体智能停滞。现有自反思框架缺乏概率基础，无法量化确定性或证明外部交互的合理性。

Method: 提出形式化概率框架：1) 使用带遗忘因子γ的Beta-Bernoulli分布建模智能体信念；2) 将认知不确定性定义为信念方差；3) 建立双向交互驱动：稳态动机（维持确定性对抗时间衰减）和最优学习策略（针对最大模糊点）；4) 引入认知缓存动态优先处理非平稳知识分布；5) 将累积信念状态用作RLHF的验证奖励信号和SFT的高质量数据过滤器。

Result: 仿真验证表明，这种不确定性驱动策略在异构（Zipfian）环境中显著优于随机基线，保持对概念漂移的高适应性。公开贡献被重新定义为最优主动学习：分享解决方案以获取反馈是智能体减少自身不确定性的最有效方法。

Conclusion: 该框架为智能体提供了非利他主义的双向知识交换动机，通过概率建模认知不确定性，实现主动学习和集体智能提升，同时为RLHF和SFT提供可验证的奖励信号和数据过滤机制。

Abstract: Autonomous agents powered by LLMs and Retrieval-Augmented Generation (RAG) are proficient consumers of digital content but remain unidirectional, a limitation we term epistemic asymmetry. This isolation leads to redundant reasoning and stagnates collective intelligence. Current self-reflection frameworks remain largely heuristic and private, lacking a probabilistic foundation to quantify certainty or justify external interaction.To bridge this gap, we propose a formal probabilistic framework that provides agents with a non-altruistic motive for bidirectional knowledge exchange. We model an agent's belief in a proposition using a Beta-Bernoulli distribution with a forgetting factor ($γ$). This allows us to isolate epistemic uncertainty as the variance of belief, establishing a dual drive for interaction: A homeostatic motive: The need to maintain certainty against the temporal decay introduced by $γ$. An optimal learning strategy: Targeting points of maximum ambiguity ($\mathbb{E}[θ]=0.5$) to maximize information gain. Under this framework, public contribution is reframed as optimal active learning: sharing solutions to elicit feedback is the most efficient method for an agent to reduce its own uncertainty. To ensure scalability, we introduce epistemic caching, which leverages the forgetting factor to dynamically prioritize resources for the active head of non-stationary knowledge distributions. Finally, we demonstrate how these accumulated belief states serve as verifiable reward signals for Reinforcement Learning from Human Feedback (RLHF) and high-quality data filters for Supervised Fine-Tuning (SFT). Simulation results validate that this uncertainty-driven strategy significantly outperforms random baselines in heterogeneous (Zipfian) environments, maintaining high adaptability to concept drift.

</details>


### [33] [A Blockchain-Monitored Agentic AI Architecture for Trusted Perception-Reasoning-Action Pipelines](https://arxiv.org/abs/2512.20985)
*Salman Jan,Hassan Ali Razzaqi,Ali Akarma,Mohammad Riyaz Belgaum*

Main category: cs.AI

TL;DR: 该论文提出了一种结合LangChain多智能体系统和许可区块链的架构，用于确保自主AI系统的监控、策略执行和不可篡改审计，在智能库存管理、交通信号控制和医疗监测等场景中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 随着自主决策AI系统在医疗、智慧城市、数字取证和供应链管理等领域的应用增长，这些系统虽然灵活且能实时推理，但也引发了信任、监督以及信息和活动完整性的担忧。需要一种机制来确保自主AI系统的可靠性和可审计性。

Method: 提出了一种统一架构模型，结合LangChain多智能体系统和许可区块链。该框架将感知-概念化-行动循环与区块链治理层关联，验证输入、评估推荐行动并记录执行结果。具体实现了基于Hyperledger Fabric的系统，集成了MCP动作执行器和LangChain智能体。

Result: 在智能库存管理、交通信号控制和医疗监测等实验中，结果表明区块链安全验证能有效防止未授权操作，提供全决策过程的追溯性，并将操作延迟保持在合理范围内。

Conclusion: 该框架为实施高影响力的自主AI应用提供了一个通用系统，既能保持自主性又能确保责任性，实现了自主性与可信度的平衡。

Abstract: The application of agentic AI systems in autonomous decision-making is growing in the areas of healthcare, smart cities, digital forensics, and supply chain management. Even though these systems are flexible and offer real-time reasoning, they also raise concerns of trust and oversight, and integrity of the information and activities upon which they are founded. The paper suggests a single architecture model comprising of LangChain-based multi-agent system with a permissioned blockchain to guarantee constant monitoring, policy enforcement, and immutable auditability of agentic action. The framework relates the perception conceptualization-action cycle to a blockchain layer of governance that verifies the inputs, evaluates recommended actions, and documents the outcomes of the execution. A Hyperledger Fabric-based system, action executors MCP-integrated, and LangChain agent are introduced and experiments of smart inventory management, traffic-signal control, and healthcare monitoring are done. The results suggest that blockchain-security verification is efficient in preventing unauthorized practices, offers traceability throughout the whole decision-making process, and maintains operational latency within reasonable ranges. The suggested framework provides a universal system of implementing high-impact agentic AI applications that are autonomous yet responsible.

</details>


### [34] [FinAgent: An Agentic AI Framework Integrating Personal Finance and Nutrition Planning](https://arxiv.org/abs/2512.20991)
*Toqeer Ali Syed,Abdulaziz Alshahrani,Ali Ullah,Ali Akarma,Sohail Khan,Muhammad Nauman,Salman Jan*

Main category: cs.AI

TL;DR: 论文提出了一种价格感知的智能AI系统，结合个人财务管理与饮食优化，为中等收入家庭提供营养充足且价格合理的膳食计划，能根据市场价格波动自动调整。


<details>
  <summary>Details</summary>
Motivation: 中等收入环境中家庭预算有限与营养需求之间的矛盾，特别是食品价格波动带来的挑战，需要一种能同时考虑经济性和营养性的智能解决方案。

Method: 采用模块化多智能体架构，包含预算、营养、价格监控和健康个性化等专门智能体，共享知识库并使用替代图来确保以最低成本维持营养质量。

Result: 在沙特家庭案例研究中，相比静态每周菜单，成本持续降低12-18%，营养充足率超过95%，在20-30%的价格变化下仍保持高性能。

Conclusion: 该框架能在本地将经济性与营养充足性相结合，为实现可持续和公平的饮食规划提供了可行途径，符合可持续发展目标中的零饥饿和良好健康目标。

Abstract: The issue of limited household budgets and nutritional demands continues to be a challenge especially in the middle-income environment where food prices fluctuate. This paper introduces a price aware agentic AI system, which combines personal finance management with diet optimization. With household income and fixed expenditures, medical and well-being status, as well as real-time food costs, the system creates nutritionally sufficient meals plans at comparatively reasonable prices that automatically adjust to market changes. The framework is implemented in a modular multi-agent architecture, which has specific agents (budgeting, nutrition, price monitoring, and health personalization). These agents share the knowledge base and use the substitution graph to ensure that the nutritional quality is maintained at a minimum cost. Simulations with a representative Saudi household case study show a steady 12-18\% reduction in costs relative to a static weekly menu, nutrient adequacy of over 95\% and high performance with price changes of 20-30%. The findings indicate that the framework can locally combine affordability with nutritional adequacy and provide a viable avenue of capacity-building towards sustainable and fair diet planning in line with Sustainable Development Goals on Zero Hunger and Good Health.

</details>


### [35] [LLM Personas as a Substitute for Field Experiments in Method Benchmarking](https://arxiv.org/abs/2512.21080)
*Enoch Hyunwook Kang*

Main category: cs.AI

TL;DR: 本文证明了在特定条件下（仅观察聚合结果、算法盲评估），用LLM角色模拟替代人类进行A/B测试是有效的基准接口替换，并提出了信息论框架来确定所需角色评估数量。


<details>
  <summary>Details</summary>
Motivation: A/B测试作为社会系统方法评估的黄金标准，存在成本高、延迟大的问题，阻碍了迭代方法开发。LLM角色模拟提供了廉价替代方案，但需要验证其是否能保持基准接口的有效性。

Method: 1. 证明了一个充要条件特征：当方法仅观察聚合结果且评估仅依赖提交的工件（算法盲评估）时，用角色替换人类只是评估面板的变化；2. 定义了诱导聚合信道的信息论可区分性；3. 推导了使角色基准与实地实验同等决策相关所需的最小独立角色评估数量的显式边界。

Result: 证明了在特定条件下，LLM角色模拟可以有效地替代人类进行方法评估，且通过信息论分析确定了使角色基准具有决策相关性的具体样本量要求。

Conclusion: 在聚合观察和算法盲评估条件下，LLM角色模拟可作为A/B测试的有效替代基准，其决策相关性可通过适当的样本量实现，为迭代方法开发提供了廉价高效的评估框架。

Abstract: Field experiments (A/B tests) are often the most credible benchmark for methods in societal systems, but their cost and latency create a major bottleneck for iterative method development. LLM-based persona simulation offers a cheap synthetic alternative, yet it is unclear whether replacing humans with personas preserves the benchmark interface that adaptive methods optimize against. We prove an if-and-only-if characterization: when (i) methods observe only the aggregate outcome (aggregate-only observation) and (ii) evaluation depends only on the submitted artifact and not on the algorithm's identity or provenance (algorithm-blind evaluation), swapping humans for personas is just panel change from the method's point of view, indistinguishable from changing the evaluation population (e.g., New York to Jakarta). Furthermore, we move from validity to usefulness: we define an information-theoretic discriminability of the induced aggregate channel and show that making persona benchmarking as decision-relevant as a field experiment is fundamentally a sample-size question, yielding explicit bounds on the number of independent persona evaluations required to reliably distinguish meaningfully different methods at a chosen resolution.

</details>


### [36] [A Real-World Evaluation of LLM Medication Safety Reviews in NHS Primary Care](https://arxiv.org/abs/2512.21127)
*Oliver Normand,Esther Borsi,Mitch Fruin,Lauren E Walker,Jamie Heagerty,Chris C. Holmes,Anthony J Avery,Iain E Buchan,Harry Coppock*

Main category: cs.AI

TL;DR: 该研究首次在真实NHS初级保健数据上评估基于LLM的用药安全审查系统，发现虽然LLM在识别临床问题方面表现良好，但在复杂临床情境下的综合表现仍有不足，主要失败模式是情境推理而非药物知识缺失。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型在医学基准测试中常达到或超过临床医生水平，但很少在真实临床数据上进行评估，也缺乏对关键失败行为的详细分析。本研究旨在填补这一空白，评估LLM在真实临床环境中的表现。

Method: 回顾性研究使用NHS Cheshire和Merseyside地区2,125,549名成人的电子健康记录，战略性地抽样277名患者以捕捉广泛的临床复杂性和用药安全风险。专家临床医生审查这些患者，对系统识别的问题和提出的干预措施进行分级评估。

Result: 主要LLM系统在识别临床问题存在方面表现强劲（敏感性100%，特异性83.1%），但仅在46.9%的患者中正确识别所有问题和干预措施。失败分析揭示了五种主要失败模式：对不确定性的过度自信、不考虑患者情境应用标准指南、误解医疗实践方式、事实错误和流程盲点。

Conclusion: 在基于LLM的临床AI安全部署前必须解决这些缺陷。研究呼吁进行更大规模的前瞻性评估，并对LLM在临床环境中的行为进行更深入研究，同时提供了45个详细案例全面覆盖所有识别出的失败情况。

Abstract: Large language models (LLMs) often match or exceed clinician-level performance on medical benchmarks, yet very few are evaluated on real clinical data or examined beyond headline metrics. We present, to our knowledge, the first evaluation of an LLM-based medication safety review system on real NHS primary care data, with detailed characterisation of key failure behaviours across varying levels of clinical complexity. In a retrospective study using a population-scale EHR spanning 2,125,549 adults in NHS Cheshire and Merseyside, we strategically sampled patients to capture a broad range of clinical complexity and medication safety risk, yielding 277 patients after data-quality exclusions. An expert clinician reviewed these patients and graded system-identified issues and proposed interventions. Our primary LLM system showed strong performance in recognising when a clinical issue is present (sensitivity 100\% [95\% CI 98.2--100], specificity 83.1\% [95\% CI 72.7--90.1]), yet correctly identified all issues and interventions in only 46.9\% [95\% CI 41.1--52.8] of patients. Failure analysis reveals that, in this setting, the dominant failure mechanism is contextual reasoning rather than missing medication knowledge, with five primary patterns: overconfidence in uncertainty, applying standard guidelines without adjusting for patient context, misunderstanding how healthcare is delivered in practice, factual errors, and process blindness. These patterns persisted across patient complexity and demographic strata, and across a range of state-of-the-art models and configurations. We provide 45 detailed vignettes that comprehensively cover all identified failure cases. This work highlights shortcomings that must be addressed before LLM-based clinical AI can be safely deployed. It also begs larger-scale, prospective evaluations and deeper study of LLM behaviours in clinical contexts.

</details>


### [37] [RoboSafe: Safeguarding Embodied Agents via Executable Safety Logic](https://arxiv.org/abs/2512.21220)
*Le Wang,Zonghao Ying,Xiao Yang,Quanchen Zou,Zhenfei Yin,Tianlin Li,Jian Yang,Yaodong Yang,Aishan Liu,Xianglong Liu*

Main category: cs.AI

TL;DR: RoboSafe：一种用于具身智能体的混合推理运行时安全防护系统，通过可执行的基于谓词的安全逻辑来拦截危险指令，减少36.8%的风险发生


<details>
  <summary>Details</summary>
Motivation: 基于视觉语言模型的具身智能体在执行复杂现实任务时容易受到危险指令的影响，而现有的运行时安全防护方法（如静态规则过滤或提示级控制）难以应对动态、时间依赖和上下文丰富的环境中出现的隐式风险

Method: 提出RoboSafe系统，包含两个互补的推理模块：1）后向反思推理模块，通过短期记忆持续回顾近期轨迹来推断时间安全谓词，并在检测到违规时主动触发重新规划；2）前向预测推理模块，通过长期安全记忆和智能体的多模态观察生成上下文感知的安全谓词来预测即将到来的风险。这些组件在混合长短安全记忆上运行，形成可适应、可验证的安全逻辑

Result: 在多个智能体上的广泛实验表明，RoboSafe相比领先基线显著减少了危险行为（风险发生率降低36.8%），同时保持了接近原始的任务性能。在物理机械臂上的真实世界评估进一步证实了其实用性

Conclusion: RoboSafe为具身智能体提供了一种自适应、可验证且可解释的运行时安全防护方案，通过混合推理方法有效应对动态环境中的隐式风险，在保证安全性的同时维持任务性能

Abstract: Embodied agents powered by vision-language models (VLMs) are increasingly capable of executing complex real-world tasks, yet they remain vulnerable to hazardous instructions that may trigger unsafe behaviors. Runtime safety guardrails, which intercept hazardous actions during task execution, offer a promising solution due to their flexibility. However, existing defenses often rely on static rule filters or prompt-level control, which struggle to address implicit risks arising in dynamic, temporally dependent, and context-rich environments. To address this, we propose RoboSafe, a hybrid reasoning runtime safeguard for embodied agents through executable predicate-based safety logic. RoboSafe integrates two complementary reasoning processes on a Hybrid Long-Short Safety Memory. We first propose a Backward Reflective Reasoning module that continuously revisits recent trajectories in short-term memory to infer temporal safety predicates and proactively triggers replanning when violations are detected. We then propose a Forward Predictive Reasoning module that anticipates upcoming risks by generating context-aware safety predicates from the long-term safety memory and the agent's multimodal observations. Together, these components form an adaptive, verifiable safety logic that is both interpretable and executable as code. Extensive experiments across multiple agents demonstrate that RoboSafe substantially reduces hazardous actions (-36.8% risk occurrence) compared with leading baselines, while maintaining near-original task performance. Real-world evaluations on physical robotic arms further confirm its practicality. Code will be released upon acceptance.

</details>
