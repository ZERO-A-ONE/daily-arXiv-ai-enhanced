{"id": "2509.19456", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.19456", "abs": "https://arxiv.org/abs/2509.19456", "authors": ["Krisztian Balog", "ChengXiang Zhai"], "title": "The Indispensable Role of User Simulation in the Pursuit of AGI", "comment": "Accepted for publication in Communications of the ACM", "summary": "Progress toward Artificial General Intelligence (AGI) faces significant\nbottlenecks, particularly in rigorously evaluating complex interactive systems\nand acquiring the vast interaction data needed for training adaptive agents.\nThis paper posits that user simulation -- creating computational agents that\nmimic human interaction with AI systems -- is not merely a useful tool, but is\na critical catalyst required to overcome these bottlenecks and accelerate AGI\ndevelopment. We argue that realistic simulators provide the necessary\nenvironments for scalable evaluation, data generation for interactive learning,\nand fostering the adaptive capabilities central to AGI. Therefore, research\ninto user simulation technology and intelligent task agents are deeply\nsynergistic and must advance hand-in-hand. This article elaborates on the\ncritical role of user simulation for AGI, explores the interdisciplinary nature\nof building realistic simulators, identifies key challenges including those\nposed by large language models, and proposes a future research agenda."}
{"id": "2509.19464", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.19464", "abs": "https://arxiv.org/abs/2509.19464", "authors": ["Shripad Vilasrao Deshmukh", "Will Schwarzer", "Scott Niekum"], "title": "Evaluation-Aware Reinforcement Learning", "comment": "9 pages, under submission", "summary": "Policy evaluation is often a prerequisite for deploying safety- and\nperformance-critical systems. Existing evaluation approaches frequently suffer\nfrom high variance due to limited data and long-horizon tasks, or high bias due\nto unequal support or inaccurate environmental models. We posit that these\nchallenges arise, in part, from the standard reinforcement learning (RL)\nparadigm of policy learning without explicit consideration of evaluation. As an\nalternative, we propose evaluation-aware reinforcement learning (EvA-RL), in\nwhich a policy is trained to maximize expected return while simultaneously\nminimizing expected evaluation error under a given value prediction scheme --\nin other words, being \"easy\" to evaluate. We formalize a framework for EvA-RL\nand design an instantiation that enables accurate policy evaluation,\nconditioned on a small number of rollouts in an assessment environment that can\nbe different than the deployment environment. However, our theoretical analysis\nand empirical results show that there is often a tradeoff between evaluation\naccuracy and policy performance when using a fixed value-prediction scheme\nwithin EvA-RL. To mitigate this tradeoff, we extend our approach to co-learn an\nassessment-conditioned state-value predictor alongside the policy. Empirical\nresults across diverse discrete and continuous action domains demonstrate that\nEvA-RL can substantially reduce evaluation error while maintaining competitive\nreturns. This work lays the foundation for a broad new class of RL methods that\ntreat reliable evaluation as a first-class principle during training."}
{"id": "2509.19489", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.19489", "abs": "https://arxiv.org/abs/2509.19489", "authors": ["Robert Nowak"], "title": "Estimating the Self-Consistency of LLMs", "comment": "5 pages", "summary": "Systems often repeat the same prompt to large language models (LLMs) and\naggregate responses to improve reliability. This short note analyzes an\nestimator of the self-consistency of LLMs and the tradeoffs it induces under a\nfixed compute budget $B=mn$, where $m$ is the number of prompts sampled from\nthe task distribution and $n$ is the number of repeated LLM calls per prompt;\nthe resulting analysis favors a rough split $m,n\\propto\\sqrt{B}$."}
{"id": "2509.19517", "categories": ["cs.AI", "cs.CL", "cs.LG", "I.2.7; I.2.6"], "pdf": "https://arxiv.org/pdf/2509.19517", "abs": "https://arxiv.org/abs/2509.19517", "authors": ["Sai Teja Reddy Adapala"], "title": "Cognitive Load Limits in Large Language Models: Benchmarking Multi-Hop Reasoning", "comment": null, "summary": "The scaling of Large Language Models (LLMs) has exposed a critical gap\nbetween their performance on static benchmarks and their fragility in dynamic,\ninformation-rich environments. While models excel at isolated tasks, the\ncomputational limits that govern their reasoning under cognitive load remain\npoorly understood. In this work, we introduce a formal theory of computational\ncognitive load, positing that extraneous, task-irrelevant information (Context\nSaturation) and interference from task-switching (Attentional Residue) are key\nmechanisms that degrade performance. We designed the Interleaved Cognitive\nEvaluation (ICE), a deconfounded benchmark to systematically manipulate these\nload factors on challenging multi-hop reasoning tasks. A comprehensive study (N\n= 10 replications per item across 200 questions) revealed significant\nperformance variations across five instruction-tuned models. Smaller\nopen-source architectures (Llama-3-8B-Instruct, Mistral-7B-Instruct-v0.2)\nexhibited baseline brittleness, achieving 0% accuracy (SEM = 0.0) across all\nconditions, including clean controls, on this high-intrinsic-load task. In\ncontrast, Gemini-2.0-Flash-001 showed partial resilience, achieving 85%\naccuracy in control conditions, with a statistically significant degradation\nunder context saturation ($\\beta = -0.003$ per % load, $p < 0.001$). These\nfindings provide preliminary evidence that cognitive load is a key contributor\nto reasoning failures, supporting theories of hallucination-as-guessing under\nuncertainty. We conclude that dynamic, cognitive-aware stress testing, as\nexemplified by the ICE benchmark, is essential for evaluating the true\nresilience and safety of advanced AI systems."}
{"id": "2509.19459", "categories": ["cs.SE", "cs.PL"], "pdf": "https://arxiv.org/pdf/2509.19459", "abs": "https://arxiv.org/abs/2509.19459", "authors": ["Yutong Guo", "Weiyu Luo", "Brian Demsky"], "title": "Automated Insertion of Flushes and Fences for Persistency", "comment": null, "summary": "CXL shared memory and persistent memory allow the contents of memory to\npersist beyond crashes. Stores to persistent or CXL memory are typically not\nimmediately made persistent; developers must manually flush the corresponding\ncache lines to force the data to be written to the underlying storage.\nCorrectly using flush and fence operations is known to be challenging. While\nstate-of-the-art tools can find missing flush instructions, they often require\nbug-revealing test cases. No existing tools can ensure the absence of missing\nflush bugs.\n  In this paper, we present PMRobust, a compiler that automatically inserts\nflush and fence operations to ensure that code using persistent memory is free\nfrom missing flush and fence bugs. PMRobust employs a novel static analysis\nwith optimizations that target newly allocated objects. We have evaluated\nPMRobust on persistent memory libraries and several persistent memory data\nstructures and measured a geometric mean overhead of 0.26% relative to the\noriginal benchmarks with hand-placed flush and fence operations."}
{"id": "2509.19485", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.19485", "abs": "https://arxiv.org/abs/2509.19485", "authors": ["Hafijul Hoque Chowdhury", "Riad Ahmed Anonto", "Sourov Jajodia", "Suryadipta Majumdar", "Md. Shohrab Hossain"], "title": "Identifying and Addressing User-level Security Concerns in Smart Homes Using \"Smaller\" LLMs", "comment": "10 pages, accepted at PST 2025", "summary": "With the rapid growth of smart home IoT devices, users are increasingly\nexposed to various security risks, as evident from recent studies. While\nseeking answers to know more on those security concerns, users are mostly left\nwith their own discretion while going through various sources, such as online\nblogs and technical manuals, which may render higher complexity to regular\nusers trying to extract the necessary information. This requirement does not go\nalong with the common mindsets of smart home users and hence threatens the\nsecurity of smart homes furthermore. In this paper, we aim to identify and\naddress the major user-level security concerns in smart homes. Specifically, we\ndevelop a novel dataset of Q&A from public forums, capturing practical security\nchallenges faced by smart home users. We extract major security concerns in\nsmart homes from our dataset by leveraging the Latent Dirichlet Allocation\n(LDA). We fine-tune relatively \"smaller\" transformer models, such as T5 and\nFlan-T5, on this dataset to build a QA system tailored for smart home security.\nUnlike larger models like GPT and Gemini, which are powerful but often resource\nhungry and require data sharing, smaller models are more feasible for\ndeployment in resource-constrained or privacy-sensitive environments like smart\nhomes. The dataset is manually curated and supplemented with synthetic data to\nexplore its potential impact on model performance. This approach significantly\nimproves the system's ability to deliver accurate and relevant answers, helping\nusers address common security concerns with smart home IoT devices. Our\nexperiments on real-world user concerns show that our work improves the\nperformance of the base models."}
{"id": "2509.19524", "categories": ["cs.AI", "cs.RO"], "pdf": "https://arxiv.org/pdf/2509.19524", "abs": "https://arxiv.org/abs/2509.19524", "authors": ["Ramy ElMallah", "Krish Chhajer", "Chi-Guhn Lee"], "title": "Score the Steps, Not Just the Goal: VLM-Based Subgoal Evaluation for Robotic Manipulation", "comment": "Accepted to the CoRL 2025 Eval&Deploy Workshop", "summary": "Robot learning papers typically report a single binary success rate (SR),\nwhich obscures where a policy succeeds or fails along a multi-step manipulation\ntask. We argue that subgoal-level reporting should become routine: for each\ntrajectory, a vector of per-subgoal SRs that makes partial competence visible\n(e.g., grasp vs. pour). We propose a blueprint for StepEval, a cost-aware\nplug-in evaluation framework that utilizes vision-language models (VLMs) as\nautomated judges of subgoal outcomes from recorded images or videos. Rather\nthan proposing new benchmarks or APIs, our contribution is to outline design\nprinciples for a scalable, community-driven open-source project. In StepEval,\nthe primary artifact for policy evaluation is the per-subgoal SR vector;\nhowever, other quantities (e.g., latency or cost estimates) are also considered\nfor framework-optimization diagnostics to help the community tune evaluation\nefficiency and accuracy when ground-truth subgoal success labels are available.\nWe discuss how such a framework can remain model-agnostic, support single- or\nmulti-view inputs, and be lightweight enough to adopt across labs. The intended\ncontribution is a shared direction: a minimal, extensible seed that invites\nopen-source contributions, so that scoring the steps, not just the final goal,\nbecomes a standard and reproducible practice."}
{"id": "2509.19533", "categories": ["cs.SE", "cs.AI", "cs.CR"], "pdf": "https://arxiv.org/pdf/2509.19533", "abs": "https://arxiv.org/abs/2509.19533", "authors": ["Mengdi Lu", "Steven Ding", "Furkan Alaca", "Philippe Charland"], "title": "Semantic-Aware Fuzzing: An Empirical Framework for LLM-Guided, Reasoning-Driven Input Mutation", "comment": null, "summary": "Security vulnerabilities in Internet-of-Things devices, mobile platforms, and\nautonomous systems remain critical. Traditional mutation-based fuzzers -- while\neffectively explore code paths -- primarily perform byte- or bit-level edits\nwithout semantic reasoning. Coverage-guided tools such as AFL++ use\ndictionaries, grammars, and splicing heuristics to impose shallow structural\nconstraints, leaving deeper protocol logic, inter-field dependencies, and\ndomain-specific semantics unaddressed. Conversely, reasoning-capable large\nlanguage models (LLMs) can leverage pretraining knowledge to understand input\nformats, respect complex constraints, and propose targeted mutations, much like\nan experienced reverse engineer or testing expert. However, lacking ground\ntruth for \"correct\" mutation reasoning makes supervised fine-tuning\nimpractical, motivating explorations of off-the-shelf LLMs via prompt-based\nfew-shot learning. To bridge this gap, we present an open-source microservices\nframework that integrates reasoning LLMs with AFL++ on Google's FuzzBench,\ntackling asynchronous execution and divergent hardware demands (GPU- vs.\nCPU-intensive) of LLMs and fuzzers. We evaluate four research questions: (R1)\nHow can reasoning LLMs be integrated into the fuzzing mutation loop? (R2) Do\nfew-shot prompts yield higher-quality mutations than zero-shot? (R3) Can prompt\nengineering with off-the-shelf models improve fuzzing directly? and (R4) Which\nopen-source reasoning LLMs perform best under prompt-only conditions?\nExperiments with Llama3.3, Deepseek-r1-Distill-Llama-70B, QwQ-32B, and Gemma3\nhighlight Deepseek as the most promising. Mutation effectiveness depends more\non prompt complexity and model choice than shot count. Response latency and\nthroughput bottlenecks remain key obstacles, offering directions for future\nwork."}
{"id": "2509.19568", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.19568", "abs": "https://arxiv.org/abs/2509.19568", "authors": ["Antoine Plin", "Lorenzo Casalino", "Thomas Rokicki", "Ruben Salvador"], "title": "Knock-Knock: Black-Box, Platform-Agnostic DRAM Address-Mapping Reverse Engineering", "comment": "Accepted in 2nd Microarchitecture Security Conference 2026 (uASC\n  '26), 17 pages, 8 figures, 3 tables, 1 algorithm, 1 appendix", "summary": "Modern Systems-on-Chip (SoCs) employ undocumented linear address-scrambling\nfunctions to obfuscate DRAM addressing, which complicates DRAM-aware\nperformance optimizations and hinders proactive security analysis of DRAM-based\nattacks; most notably, Rowhammer. Although previous work tackled the issue of\nreversing physical-to-DRAM mapping, existing heuristic-based\nreverse-engineering approaches are partial, costly, and impractical for\ncomprehensive recovery. This paper establishes a rigorous theoretical\nfoundation and provides efficient practical algorithms for black-box, complete\nphysical-to-DRAM address-mapping recovery.\n  We first formulate the reverse-engineering problem within a linear algebraic\nmodel over the finite field GF(2). We characterize the timing fingerprints of\nrow-buffer conflicts, proving a relationship between a bank addressing matrix\nand an empirically constructed matrix of physical addresses. Based on this\ncharacterization, we develop an efficient, noise-robust, and fully\nplatform-agnostic algorithm to recover the full bank-mask basis in polynomial\ntime, a significant improvement over the exponential search from previous\nworks. We further generalize our model to complex row mappings, introducing new\nhardware-based hypotheses that enable the automatic recovery of a row basis\ninstead of previous human-guided contributions.\n  Evaluations across embedded and server-class architectures confirm our\nmethod's effectiveness, successfully reconstructing known mappings and\nuncovering previously unknown scrambling functions. Our method provides a 99%\nrecall and accuracy on all tested platforms. Most notably, Knock-Knock runs in\nunder a few minutes, even on systems with more than 500GB of DRAM, showcasing\nthe scalability of our method. Our approach provides an automated, principled\npathway to accurate DRAM reverse engineering."}
{"id": "2509.19566", "categories": ["cs.AI", "q-bio.GN"], "pdf": "https://arxiv.org/pdf/2509.19566", "abs": "https://arxiv.org/abs/2509.19566", "authors": ["George Hong", "Daniel Trejo Banos"], "title": "Nano Bio-Agents (NBA): Small Language Model Agents for Genomics", "comment": null, "summary": "We investigate the application of Small Language Models (<10 billion\nparameters) for genomics question answering via agentic framework to address\nhallucination issues and computational cost challenges. The Nano Bio-Agent\n(NBA) framework we implemented incorporates task decomposition, tool\norchestration, and API access into well-established systems such as NCBI and\nAlphaGenome. Results show that SLMs combined with such agentic framework can\nachieve comparable and in many cases superior performance versus existing\napproaches utilising larger models, with our best model-agent combination\nachieving 98% accuracy on the GeneTuring benchmark. Notably, small 3-10B\nparameter models consistently achieve 85-97% accuracy while requiring much\nlower computational resources than conventional approaches. This demonstrates\npromising potential for efficiency gains, cost savings, and democratization of\nML-powered genomics tools while retaining highly robust and accurate\nperformance."}
{"id": "2509.19587", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.19587", "abs": "https://arxiv.org/abs/2509.19587", "authors": ["Mohamed Ouf", "Haoyu Li", "Michael Zhang", "Mariam Guizani"], "title": "Reverse Engineering User Stories from Code using Large Language Models", "comment": null, "summary": "User stories are essential in agile development, yet often missing or\noutdated in legacy and poorly documented systems. We investigate whether large\nlanguage models (LLMs) can automatically recover user stories directly from\nsource code and how prompt design impacts output quality. Using 1,750 annotated\nC++ snippets of varying complexity, we evaluate five state-of-the-art LLMs\nacross six prompting strategies. Results show that all models achieve, on\naverage, an F1 score of 0.8 for code up to 200 NLOC. Our findings show that a\nsingle illustrative example enables the smallest model (8B) to match the\nperformance of a much larger 70B model. In contrast, structured reasoning via\nChain-of-Thought offers only marginal gains, primarily for larger models."}
{"id": "2509.19650", "categories": ["cs.CR", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2509.19650", "abs": "https://arxiv.org/abs/2509.19650", "authors": ["Dehinde Molade", "Dave Ormrod", "Mamello Thinyane", "Nalin Arachchilage", "Jill Slay"], "title": "SoK: A Systematic Review of Malware Ontologies and Taxonomies and Implications for the Quantum Era", "comment": "40 pages, 9 figures, 5 tables", "summary": "The threat of quantum malware is real and a growing security concern that\nwill have catastrophic scientific and technological impacts, if not addressed\nearly. If weaponised or exploited especially by the wrong hands, malware will\nundermine highly sophisticated critical systems supported by next-generation\nquantum architectures, for example, in defence, communications, energy, and\nspace. This paper explores the fundamental nature and implications of quantum\nmalware to enable the future development of appropriate mitigations and\ndefences, thereby protecting critical infrastructure. By conducting a\nsystematic literature review (SLR) that draws on knowledge frameworks such as\nontologies and taxonomies to explore malware, this provides insights into how\nmalicious behaviours can be translated into attacks on quantum technologies,\nthereby providing a lens to analyse the severity of malware against quantum\ntechnologies. This study employs the European Competency Framework for Quantum\nTechnologies (CFQT) as a guide to map malware behaviour to several competency\nlayers, creating a foundation in this emerging field."}
{"id": "2509.19590", "categories": ["cs.AI", "cs.CY", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.19590", "abs": "https://arxiv.org/abs/2509.19590", "authors": ["Nathanael Jo", "Ashia Wilson"], "title": "What Does Your Benchmark Really Measure? A Framework for Robust Inference of AI Capabilities", "comment": null, "summary": "Evaluations of generative models on benchmark data are now ubiquitous, and\ntheir outcomes critically shape public and scientific expectations of AI's\ncapabilities. Yet growing skepticism surrounds their reliability. How can we\nknow that a reported accuracy genuinely reflects a model's true performance?\nEvaluations are often presented as simple measurements, but in reality they are\ninferences: to treat benchmark scores as evidence of capability is already to\nassume a theory of what capability is and how it manifests in a test. We make\nthis step explicit by proposing a principled framework for evaluation as\ninference: begin from a theory of capability, and then derive methods for\nestimating it. This perspective, familiar in fields such as psychometrics, has\nnot yet become commonplace in AI evaluation. As a proof of concept, we address\na central challenge that undermines reliability: sensitivity to perturbations.\nAfter formulating a model of ability, we introduce methods that infer ability\nwhile accounting for uncertainty from sensitivity and finite samples, including\nan adaptive algorithm that significantly reduces sample complexity. Together,\nthese contributions lay the groundwork for more reliable and trustworthy\nestimates of AI capabilities as measured through benchmarks."}
{"id": "2509.19673", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.19673", "abs": "https://arxiv.org/abs/2509.19673", "authors": ["Ahmed Aljohani", "Anamul Haque Mollah", "Hyunsook Do"], "title": "Assertion Messages with Large Language Models (LLMs) for Code", "comment": "Accepted at Proceedings of the 2025 Evaluation and Assessment in\n  Software Engineering (EASE '25)", "summary": "Assertion messages significantly enhance unit tests by clearly explaining the\nreasons behind test failures, yet they are frequently omitted by developers and\nautomated test-generation tools. Despite recent advancements, Large Language\nModels (LLMs) have not been systematically evaluated for their ability to\ngenerate informative assertion messages. In this paper, we introduce an\nevaluation of four state-of-the-art Fill-in-the-Middle (FIM) LLMs -\nQwen2.5-Coder-32B, Codestral-22B, CodeLlama-13B, and StarCoder - on a dataset\nof 216 Java test methods containing developer-written assertion messages. We\nfind that Codestral-22B achieves the highest quality score of 2.76 out of 5\nusing a human-like evaluation approach, compared to 3.24 for manually written\nmessages. Our ablation study shows that including descriptive test comments\nfurther improves Codestral's performance to 2.97, highlighting the critical\nrole of context in generating clear assertion messages. Structural analysis\ndemonstrates that all models frequently replicate developers' preferred\nlinguistic patterns. We discuss the limitations of the selected models and\nconventional text evaluation metrics in capturing diverse assertion message\nstructures. Our benchmark, evaluation results, and discussions provide an\nessential foundation for advancing automated, context-aware generation of\nassertion messages in test code. A replication package is available at\nhttps://doi.org/10.5281/zenodo.15293133"}
{"id": "2509.19677", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.19677", "abs": "https://arxiv.org/abs/2509.19677", "authors": ["Michiharu Yamashita", "Thanh Tran", "Delvin Ce Zhang", "Dongwon Lee"], "title": "Unmasking Fake Careers: Detecting Machine-Generated Career Trajectories via Multi-layer Heterogeneous Graphs", "comment": "Accepted at EMNLP 2025 Main", "summary": "The rapid advancement of Large Language Models (LLMs) has enabled the\ngeneration of highly realistic synthetic data. We identify a new vulnerability,\nLLMs generating convincing career trajectories in fake resumes and explore\neffective detection methods. To address this challenge, we construct a dataset\nof machine-generated career trajectories using LLMs and various methods, and\ndemonstrate that conventional text-based detectors perform poorly on structured\ncareer data. We propose CareerScape, a novel heterogeneous, hierarchical\nmulti-layer graph framework that models career entities and their relations in\na unified global graph built from genuine resumes. Unlike conventional\nclassifiers that treat each instance independently, CareerScape employs a\nstructure-aware framework that augments user-specific subgraphs with trusted\nneighborhood information from a global graph, enabling the model to capture\nboth global structural patterns and local inconsistencies indicative of\nsynthetic career paths. Experimental results show that CareerScape outperforms\nstate-of-the-art baselines by 5.8-85.0% relatively, highlighting the importance\nof structure-aware detection for machine-generated content."}
{"id": "2509.19623", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.19623", "abs": "https://arxiv.org/abs/2509.19623", "authors": ["Xutao Mao", "Tao Liu", "Hongying Zan"], "title": "SteinerSQL: Graph-Guided Mathematical Reasoning for Text-to-SQL Generation", "comment": "Accept in Non-archival EMNLP 2025 MathNLP", "summary": "Large Language Models (LLMs) struggle with complex Text-to-SQL queries that\ndemand both sophisticated mathematical reasoning and intricate schema\nnavigation. Existing methods often tackle these challenges in isolation,\ncreating a fractured reasoning process that compromises logical and structural\ncorrectness. To resolve this, we introduce SteinerSQL, a framework that unifies\nthese dual challenges into a single, graph-centric optimization problem.\nSteinerSQL operates in three stages: mathematical decomposition to identify\nrequired tables (terminals), optimal reasoning scaffold construction via a\nSteiner tree problem, and multi-level validation to ensure correctness. On the\nchallenging LogicCat and Spider2.0-Lite benchmarks, SteinerSQL establishes a\nnew state-of-the-art with 36.10% and 40.04% execution accuracy, respectively,\nusing Gemini-2.5-Pro. Beyond accuracy, SteinerSQL presents a new, unified\nparadigm for Text-to-SQL, paving the way for more robust and principled\nsolutions to complex reasoning tasks."}
{"id": "2509.19708", "categories": ["cs.SE", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.19708", "abs": "https://arxiv.org/abs/2509.19708", "authors": ["Anand Kumar", "Vishal Khare", "Deepak Sharma", "Satyam Kumar", "Vijay Saini", "Anshul Yadav", "Sachendra Jain", "Ankit Rana", "Pratham Verma", "Vaibhav Meena", "Avinash Edubilli"], "title": "Intuition to Evidence: Measuring AI's True Impact on Developer Productivity", "comment": "16 pages, 10 figures, 5 tables", "summary": "We present a comprehensive real-world evaluation of AI-assisted software\ndevelopment tools deployed at enterprise scale. Over one year, 300 engineers\nacross multiple teams integrated an in-house AI platform (DeputyDev) that\ncombines code generation and automated review capabilities into their daily\nworkflows. Through rigorous cohort analysis, our study demonstrates\nstatistically significant productivity improvements, including an overall 31.8%\nreduction in PR review cycle time.\n  Developer adoption was strong, with 85% satisfaction for code review features\nand 93% expressing a desire to continue using the platform. Adoption patterns\nshowed systematic scaling from 4% engagement in month 1 to 83% peak usage by\nmonth 6, stabilizing at 60% active engagement. Top adopters achieved a 61%\nincrease in code volume pushed to production, contributing to approximately 30\nto 40% of code shipped to production through this tool, accounting for an\noverall 28% increase in code shipment volume.\n  Unlike controlled benchmark evaluations, our longitudinal analysis provides\nempirical evidence from production environments, revealing both the\ntransformative potential and practical deployment challenges of integrating AI\ninto enterprise software development workflows."}
{"id": "2509.19947", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.19947", "abs": "https://arxiv.org/abs/2509.19947", "authors": ["Zhixiao Wu", "Yao Lu", "Jie Wen", "Hao Sun", "Qi Zhou", "Guangming Lu"], "title": "A Set of Generalized Components to Achieve Effective Poison-only Clean-label Backdoor Attacks with Collaborative Sample Selection and Triggers", "comment": "31 pages, 16 figures, accepted in Neurips 2025", "summary": "Poison-only Clean-label Backdoor Attacks aim to covertly inject\nattacker-desired behavior into DNNs by merely poisoning the dataset without\nchanging the labels. To effectively implant a backdoor, multiple\n\\textbf{triggers} are proposed for various attack requirements of Attack\nSuccess Rate (ASR) and stealthiness. Additionally, sample selection enhances\nclean-label backdoor attacks' ASR by meticulously selecting ``hard'' samples\ninstead of random samples to poison. Current methods 1) usually handle the\nsample selection and triggers in isolation, leading to severely limited\nimprovements on both ASR and stealthiness. Consequently, attacks exhibit\nunsatisfactory performance on evaluation metrics when converted to PCBAs via a\nmere stacking of methods. Therefore, we seek to explore the bidirectional\ncollaborative relations between the sample selection and triggers to address\nthe above dilemma. 2) Since the strong specificity within triggers, the simple\ncombination of sample selection and triggers fails to substantially enhance\nboth evaluation metrics, with generalization preserved among various attacks.\nTherefore, we seek to propose a set of components to significantly improve both\nstealthiness and ASR based on the commonalities of attacks. Specifically,\nComponent A ascertains two critical selection factors, and then makes them an\nappropriate combination based on the trigger scale to select more reasonable\n``hard'' samples for improving ASR. Component B is proposed to select samples\nwith similarities to relevant trigger implanted samples to promote\nstealthiness. Component C reassigns trigger poisoning intensity on RGB colors\nthrough distinct sensitivity of the human visual system to RGB for higher ASR,\nwith stealthiness ensured by sample selection, including Component B.\nFurthermore, all components can be strategically integrated into diverse PCBAs."}
{"id": "2509.19681", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.19681", "abs": "https://arxiv.org/abs/2509.19681", "authors": ["Anisha Garg", "Engin Tekin", "Yash More", "David Bick", "Nishit Neema", "Ganesh Venkatesh"], "title": "Calibrated Reasoning: An Explanatory Verifier for Dynamic and Efficient Problem-Solving", "comment": "39th Conference on Neural Information Processing Systems (NeurIPS\n  2025) Workshop: Efficient Reasoning", "summary": "Advanced test-time computing strategies are essential for scaling reasoning\nmodels, but their effectiveness is capped by the models' poor self-evaluation.\nWe propose a pairwise Explanatory Verifier, trained via reinforcement learning\n(GRPO), that produces calibrated confidence scores and associated natural\nlanguage reasoning for generated solutions. Our verifier improves the accuracy\nand efficiency of test-time strategies like best-of-n and self-reflection.\nCrucially, it excels at identifying challenging failure modes, such as when\nboth candidate solutions are identically incorrect, succeeding where standard\nmethods like majority voting fail."}
{"id": "2509.19918", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.19918", "abs": "https://arxiv.org/abs/2509.19918", "authors": ["Micheline Bénédicte Moumoula", "Serge Lionel Nikiema", "Albérick Euraste Djire", "Abdoul Kader Kabore", "Jacques Klein", "Tegawendé F. Bissyande"], "title": "Beyond Language Barriers: Multi-Agent Coordination for Multi-Language Code Generation", "comment": null, "summary": "Producing high-quality code across multiple programming languages is\nincreasingly important as today's software systems are built on heterogeneous\nstacks. Large language models (LLMs) have advanced the state of automated\nprogramming, yet their proficiency varies sharply between languages, especially\nthose with limited training data such as Rust, Perl, OCaml, and Erlang. Many\ncurrent solutions including language-specific fine-tuning, multi-agent\norchestration, transfer learning, and intermediate-representation pipelines\nstill approach each target language in isolation, missing opportunities to\nshare knowledge or exploit recurring cross-language patterns.\n  XL-CoGen tackles this challenge with a coordinated multi-agent architecture\nthat integrates intermediate representation, code generation, translation, and\nautomated repair. Its distinguishing feature is a data-driven mechanism for\nselecting bridging languages: empirically derived transfer matrices identify\nthe best intermediate languages based on demonstrated translation success\nrather than raw generation accuracy. The system performs early output\nvalidation, iteratively corrects errors, and reuses intermediate artifacts as\ncontextual scaffolds for subsequent translations.\n  Extensive experiments show that XL-CoGen yields notable improvements with 13\npercentage-point gains over the strongest fine-tuned baseline and as much as 30\npercentage points over existing single-language multi-agent methods. Ablation\nstudies further demonstrate that compatibility-guided bridging significantly\noutperforms LLM-based heuristics, confirming the value of cumulative\ncross-language knowledge transfer."}
{"id": "2509.20166", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.20166", "abs": "https://arxiv.org/abs/2509.20166", "authors": ["Lauren Deason", "Adam Bali", "Ciprian Bejean", "Diana Bolocan", "James Crnkovich", "Ioana Croitoru", "Krishna Durai", "Chase Midler", "Calin Miron", "David Molnar", "Brad Moon", "Bruno Ostarcevic", "Alberto Peltea", "Matt Rosenberg", "Catalin Sandu", "Arthur Saputkin", "Sagar Shah", "Daniel Stan", "Ernest Szocs", "Shengye Wan", "Spencer Whitman", "Sven Krasser", "Joshua Saxe"], "title": "CyberSOCEval: Benchmarking LLMs Capabilities for Malware Analysis and Threat Intelligence Reasoning", "comment": null, "summary": "Today's cyber defenders are overwhelmed by a deluge of security alerts,\nthreat intelligence signals, and shifting business context, creating an urgent\nneed for AI systems to enhance operational security work. While Large Language\nModels (LLMs) have the potential to automate and scale Security Operations\nCenter (SOC) operations, existing evaluations do not fully assess the scenarios\nmost relevant to real-world defenders. This lack of informed evaluation impacts\nboth AI developers and those applying LLMs to SOC automation. Without clear\ninsight into LLM performance in real-world security scenarios, developers lack\na north star for development, and users cannot reliably select the most\neffective models. Meanwhile, malicious actors are using AI to scale cyber\nattacks, highlighting the need for open source benchmarks to drive adoption and\ncommunity-driven improvement among defenders and model developers. To address\nthis, we introduce CyberSOCEval, a new suite of open source benchmarks within\nCyberSecEval 4. CyberSOCEval includes benchmarks tailored to evaluate LLMs in\ntwo tasks: Malware Analysis and Threat Intelligence Reasoning--core defensive\ndomains with inadequate coverage in current benchmarks. Our evaluations show\nthat larger, more modern LLMs tend to perform better, confirming the training\nscaling laws paradigm. We also find that reasoning models leveraging test time\nscaling do not achieve the same boost as in coding and math, suggesting these\nmodels have not been trained to reason about cybersecurity analysis, and\npointing to a key opportunity for improvement. Finally, current LLMs are far\nfrom saturating our evaluations, showing that CyberSOCEval presents a\nsignificant challenge for AI developers to improve cyber defense capabilities."}
{"id": "2509.19736", "categories": ["cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.19736", "abs": "https://arxiv.org/abs/2509.19736", "authors": ["Cheng Qian", "Zuxin Liu", "Akshara Prabhakar", "Jielin Qiu", "Zhiwei Liu", "Haolin Chen", "Shirley Kokane", "Heng Ji", "Weiran Yao", "Shelby Heinecke", "Silvio Savarese", "Caiming Xiong", "Huan Wang"], "title": "UserRL: Training Interactive User-Centric Agent via Reinforcement Learning", "comment": "28 Pages, 15 Figures, 6 Tables; Built upon latest UserBench release:\n  arXiv:2507.22034", "summary": "Reinforcement learning (RL) has shown promise in training agentic models that\nmove beyond static benchmarks to engage in dynamic, multi-turn interactions.\nYet, the ultimate value of such agents lies in their ability to assist users, a\nsetting where diversity and dynamics of user interaction pose challenges. In\nthis work, we propose UserRL, a unified framework for training and evaluating\nuser-centric abilities through standardized gym environments paired with\nsimulated users. We systematically vary turn-level reward assignment and\ntrajectory-level score calculation to analyze how different formulations affect\nlearning under the GRPO algorithm. Our experiments across Qwen3 models reveal\nthree key findings: (i) SFT cold start is critical for unlocking initial\ninteraction ability and enabling sustained RL improvements; (ii) deliberate\ntrajectory scoring yields more efficient and effective multi-turn interactions;\nand (iii) while stronger simulated users (e.g., GPT-4o) facilitates training,\nopen-source simulators (e.g., Qwen3-32B) remain a cost-effective and\ntransferable option. Together, these results highlight that careful design of\nreward shaping and user simulation choice is as crucial as model scale, and\nestablish UserRL as a practical pathway for developing robust user-centric\nagentic models. All codes and data are public for future research."}
{"id": "2509.20010", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.20010", "abs": "https://arxiv.org/abs/2509.20010", "authors": ["Xiaoning Ren", "Yuhang Ye", "Xiongfei Wu", "Yueming Wu", "Yinxing Xue"], "title": "Demystifying the Evolution of Neural Networks with BOM Analysis: Insights from a Large-Scale Study of 55,997 GitHub Repositories", "comment": "11pages,8figures", "summary": "Neural networks have become integral to many fields due to their exceptional\nperformance. The open-source community has witnessed a rapid influx of neural\nnetwork (NN) repositories with fast-paced iterations, making it crucial for\npractitioners to analyze their evolution to guide development and stay ahead of\ntrends. While extensive research has explored traditional software evolution\nusing Software Bill of Materials (SBOMs), these are ill-suited for NN software,\nwhich relies on pre-defined modules and pre-trained models (PTMs) with distinct\ncomponent structures and reuse patterns. Conceptual AI Bills of Materials\n(AIBOMs) also lack practical implementations for large-scale evolutionary\nanalysis. To fill this gap, we introduce the Neural Network Bill of Material\n(NNBOM), a comprehensive dataset construct tailored for NN software. We create\na large-scale NNBOM database from 55,997 curated PyTorch GitHub repositories,\ncataloging their TPLs, PTMs, and modules. Leveraging this database, we conduct\na comprehensive empirical study of neural network software evolution across\nsoftware scale, component reuse, and inter-domain dependency, providing\nmaintainers and developers with a holistic view of its long-term trends.\nBuilding on these findings, we develop two prototype applications,\n\\textit{Multi repository Evolution Analyzer} and \\textit{Single repository\nComponent Assessor and Recommender}, to demonstrate the practical value of our\nanalysis."}
{"id": "2509.20190", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.20190", "abs": "https://arxiv.org/abs/2509.20190", "authors": ["Tanmay Khule", "Stefan Marksteiner", "Jose Alguindigue", "Hannes Fuchs", "Sebastian Fischmeister", "Apurva Narayan"], "title": "STAF: Leveraging LLMs for Automated Attack Tree-Based Security Test Generation", "comment": "18 pages, 2 figures, accepted for 23rd escar Europe (Nov 05-06, 2025,\n  Frankfurt, Germany)", "summary": "In modern automotive development, security testing is critical for\nsafeguarding systems against increasingly advanced threats. Attack trees are\nwidely used to systematically represent potential attack vectors, but\ngenerating comprehensive test cases from these trees remains a labor-intensive,\nerror-prone task that has seen limited automation in the context of testing\nvehicular systems. This paper introduces STAF (Security Test Automation\nFramework), a novel approach to automating security test case generation.\nLeveraging Large Language Models (LLMs) and a four-step self-corrective\nRetrieval-Augmented Generation (RAG) framework, STAF automates the generation\nof executable security test cases from attack trees, providing an end-to-end\nsolution that encompasses the entire attack surface. We particularly show the\nelements and processes needed to provide an LLM to actually produce sensible\nand executable automotive security test suites, along with the integration with\nan automated testing framework. We further compare our tailored approach with\ngeneral purpose (vanilla) LLMs and the performance of different LLMs (namely\nGPT-4.1 and DeepSeek) using our approach. We also demonstrate the method of our\noperation step-by-step in a concrete case study. Our results show significant\nimprovements in efficiency, accuracy, scalability, and easy integration in any\nworkflow, marking a substantial advancement in automating automotive security\ntesting methodologies. Using TARAs as an input for verfication tests, we create\nsynergies by connecting two vital elements of a secure automotive development\nprocess."}
{"id": "2509.19762", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.19762", "abs": "https://arxiv.org/abs/2509.19762", "authors": ["Yuanxin Wang", "Pawel Filipczuk", "Anisha Garg", "Amaan Dhada", "Mohammad Hassanpour", "David Bick", "Ganesh Venkatesh"], "title": "The Conductor and the Engine: A Path Towards Co-Designed Reasoning", "comment": null, "summary": "Modern LLM reasoning relies on extensive test-time computation, driven by\ninternal model training and external agentic orchestration. However, this\nsynergy is often inefficient, as model verbosity and poor instruction following\nlead to wasted compute. We analyze this capability-cost trade-off and introduce\nan optimized reasoning workflow (\\cepo) that empowers smaller open-source\nmodels to outperform models multiple times their size. We will open-source this\nworkflow to enable further research. Our work demonstrates a clear path toward\nco-designing orchestration frameworks with the underlying model capabilities to\nunlock powerful reasoning in small-to-medium sized models."}
{"id": "2509.20136", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.20136", "abs": "https://arxiv.org/abs/2509.20136", "authors": ["Wei Zhang", "Jack Yang", "Renshuai Tao", "Lingzheng Chai", "Shawn Guo", "Jiajun Wu", "Xiaoming Chen", "Ganqu Cui", "Ning Ding", "Xander Xu", "Hu Wei", "Bowen Zhou"], "title": "V-GameGym: Visual Game Generation for Code Large Language Models", "comment": null, "summary": "Code large language models have demonstrated remarkable capabilities in\nprogramming tasks, yet current benchmarks primarily focus on single modality\nrather than visual game development. Most existing code-related benchmarks\nevaluate syntax correctness and execution accuracy, overlooking critical\ngame-specific metrics such as playability, visual aesthetics, and user\nengagement that are essential for real-world deployment. To address the gap\nbetween current LLM capabilities in algorithmic problem-solving and competitive\nprogramming versus the comprehensive requirements of practical game\ndevelopment, we present V-GameGym, a comprehensive benchmark comprising 2,219\nhigh-quality samples across 100 thematic clusters derived from real-world\nrepositories, adopting a novel clustering-based curation methodology to ensure\nboth diversity and structural completeness. Further, we introduce a multimodal\nevaluation framework with an automated LLM-driven pipeline for visual code\nsynthesis using complete UI sandbox environments. Our extensive analysis\nreveals that V-GameGym effectively bridges the gap between code generation\naccuracy and practical game development workflows, providing quantifiable\nquality metrics for visual programming and interactive element generation."}
{"id": "2509.20277", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.20277", "abs": "https://arxiv.org/abs/2509.20277", "authors": ["Xiaofan Li", "Xing Gao"], "title": "Investigating Security Implications of Automatically Generated Code on the Software Supply Chain", "comment": null, "summary": "In recent years, various software supply chain (SSC) attacks have posed\nsignificant risks to the global community. Severe consequences may arise if\ndevelopers integrate insecure code snippets that are vulnerable to SSC attacks\ninto their products. Particularly, code generation techniques, such as large\nlanguage models (LLMs), have been widely utilized in the developer community.\nHowever, LLMs are known to suffer from inherent issues when generating code,\nincluding fabrication, misinformation, and reliance on outdated training data,\nall of which can result in serious software supply chain threats. In this\npaper, we investigate the security threats to the SSC that arise from these\ninherent issues. We examine three categories of threats, including eleven\npotential SSC-related threats, related to external components in source code,\nand continuous integration configuration files. We find some threats in\nLLM-generated code could enable attackers to hijack software and workflows,\nwhile some others might cause potential hidden threats that compromise the\nsecurity of the software over time. To understand these security impacts and\nseverity, we design a tool, SSCGuard, to generate 439,138 prompts based on\nSSC-related questions collected online, and analyze the responses of four\npopular LLMs from GPT and Llama. Our results show that all identified\nSSC-related threats persistently exist. To mitigate these risks, we propose a\nnovel prompt-based defense mechanism, namely Chain-of-Confirmation, to reduce\nfabrication, and a middleware-based defense that informs users of various SSC\nthreats."}
{"id": "2509.19783", "categories": ["cs.AI", "cs.HC", "cs.SE"], "pdf": "https://arxiv.org/pdf/2509.19783", "abs": "https://arxiv.org/abs/2509.19783", "authors": ["Jiexi Xu"], "title": "Agentic Metacognition: Designing a \"Self-Aware\" Low-Code Agent for Failure Prediction and Human Handoff", "comment": "7 pages, 2 tables", "summary": "The inherent non-deterministic nature of autonomous agents, particularly\nwithin low-code/no-code (LCNC) environments, presents significant reliability\nchallenges. Agents can become trapped in unforeseen loops, generate inaccurate\noutputs, or encounter unrecoverable failures, leading to user frustration and a\nbreakdown of trust. This report proposes a novel architectural pattern to\naddress these issues: the integration of a secondary, \"metacognitive\" layer\nthat actively monitors the primary LCNC agent. Inspired by human introspection,\nthis layer is designed to predict impending task failures based on a defined\nset of triggers, such as excessive latency or repetitive actions. Upon\npredicting a failure, the metacognitive agent proactively initiates a human\nhandoff, providing the user with a clear summary of the agent's \"thought\nprocess\" and a detailed explanation of why it could not proceed. An empirical\nanalysis of a prototype system demonstrates that this approach significantly\nincreases the overall task success rate. However, this performance gain comes\nwith a notable increase in computational overhead. The findings reframe human\nhandoffs not as an admission of defeat but as a core design feature that\nenhances system resilience, improves user experience, and builds trust by\nproviding transparency into the agent's internal state. The report discusses\nthe practical and ethical implications of this approach and identifies key\ndirections for future research."}
{"id": "2509.20149", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.20149", "abs": "https://arxiv.org/abs/2509.20149", "authors": ["Jianzhang Zhang", "Jialong Zhou", "Nan Niu", "Chuang Liu"], "title": "Enhancing Requirement Traceability through Data Augmentation Using Large Language Models", "comment": null, "summary": "Requirements traceability is crucial in software engineering to ensure\nconsistency between requirements and code. However, existing automated\ntraceability methods are constrained by the scarcity of training data and\nchallenges in bridging the semantic gap between artifacts. This study aims to\naddress the data scarcity problem in requirements traceability by employing\nlarge language models (LLMs) for data augmentation. We propose a novel approach\nthat utilizes prompt-based techniques with LLMs to generate augmented\nrequirement-to-code trace links, thereby enhancing the training dataset. Four\nLLMs (Gemini 1.5 Pro, Claude 3, GPT-3.5, and GPT-4) were used, employing both\nzero-shot and few-shot templates. Moreover, we optimized the encoder component\nof the tracing model to improve its efficiency and adaptability to augmented\ndata. The key contributions of this paper are: (1) proposing and evaluating\nfour prompt templates for data augmentation; (2) providing a comparative\nanalysis of four LLMs for generating trace links; (3) enhancing the model's\nencoder for improved adaptability to augmented datasets. Experimental results\nshow that our approach significantly enhances model performance, achieving an\nF1 score improvement of up to 28.59%, thus demonstrating its effectiveness and\npotential for practical application."}
{"id": "2509.20283", "categories": ["cs.CR", "math.ST", "stat.ME", "stat.TH"], "pdf": "https://arxiv.org/pdf/2509.20283", "abs": "https://arxiv.org/abs/2509.20283", "authors": ["Önder Askin", "Tim Kutta", "Holger Dette"], "title": "Monitoring Violations of Differential Privacy over Time", "comment": null, "summary": "Auditing differential privacy has emerged as an important area of research\nthat supports the design of privacy-preserving mechanisms. Privacy audits help\nto obtain empirical estimates of the privacy parameter, to expose flawed\nimplementations of algorithms and to compare practical with theoretical privacy\nguarantees. In this work, we investigate an unexplored facet of privacy\nauditing: the sustained auditing of a mechanism that can go through changes\nduring its development or deployment. Monitoring the privacy of algorithms over\ntime comes with specific challenges. Running state-of-the-art (static) auditors\nrepeatedly requires excessive sampling efforts, while the reliability of such\nmethods deteriorates over time without proper adjustments. To overcome these\nobstacles, we present a new monitoring procedure that extracts information from\nthe entire deployment history of the algorithm. This allows us to reduce\nsampling efforts, while sustaining reliable outcomes of our auditor. We derive\nformal guarantees with regard to the soundness of our methods and evaluate\ntheir performance for important mechanisms from the literature. Our theoretical\nfindings and experiments demonstrate the efficacy of our approach."}
{"id": "2509.19800", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.19800", "abs": "https://arxiv.org/abs/2509.19800", "authors": ["Donghwan Lee", "Hyukjun Yang", "Bum Geun Park"], "title": "Analysis of approximate linear programming solution to Markov decision problem with log barrier function", "comment": null, "summary": "There are two primary approaches to solving Markov decision problems (MDPs):\ndynamic programming based on the Bellman equation and linear programming (LP).\nDynamic programming methods are the most widely used and form the foundation of\nboth classical and modern reinforcement learning (RL). By contrast, LP-based\nmethods have been less commonly employed, although they have recently gained\nattention in contexts such as offline RL. The relative underuse of the LP-based\nmethods stems from the fact that it leads to an inequality-constrained\noptimization problem, which is generally more challenging to solve effectively\ncompared with Bellman-equation-based methods. The purpose of this paper is to\nestablish a theoretical foundation for solving LP-based MDPs in a more\neffective and practical manner. Our key idea is to leverage the log-barrier\nfunction, widely used in inequality-constrained optimization, to transform the\nLP formulation of the MDP into an unconstrained optimization problem. This\nreformulation enables approximate solutions to be obtained easily via gradient\ndescent. While the method may appear simple, to the best of our knowledge, a\nthorough theoretical interpretation of this approach has not yet been\ndeveloped. This paper aims to bridge this gap."}
{"id": "2509.20172", "categories": ["cs.SE", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.20172", "abs": "https://arxiv.org/abs/2509.20172", "authors": ["Daniel Maninger", "Leon Chemnitz", "Amir Molzam Sharifloo", "Jannis Brugger", "Mira Mezini"], "title": "Benchmarking Web API Integration Code Generation", "comment": "To be published in Proceedings of 2nd ACM International Conference on\n  AI-powered Software, Benchmark & Dataset Track (AIware '25)", "summary": "API integration is a cornerstone of our digital infrastructure, enabling\nsoftware systems to connect and interact. However, as shown by many studies,\nwriting or generating correct code to invoke APIs, particularly web APIs, is\nchallenging. Although large language models~(LLMs) have become popular in\nsoftware development, their effectiveness in automating the generation of web\nAPI integration code remains unexplored. In order to address this, we present a\ndataset and evaluation pipeline designed to assess the ability of LLMs to\ngenerate web API invocation code. Our experiments with several open-source LLMs\nreveal that generating API invocations poses a significant challenge, resulting\nin hallucinated endpoints, incorrect argument usage, and other errors. None of\nthe evaluated open-source models were able to solve more than 40% of the tasks."}
{"id": "2509.20324", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.20324", "abs": "https://arxiv.org/abs/2509.20324", "authors": ["Atousa Arzanipour", "Rouzbeh Behnia", "Reza Ebrahimi", "Kaushik Dutta"], "title": "RAG Security and Privacy: Formalizing the Threat Model and Attack Surface", "comment": "Accepted at the 5th ICDM Workshop on September 20, 2025", "summary": "Retrieval-Augmented Generation (RAG) is an emerging approach in natural\nlanguage processing that combines large language models (LLMs) with external\ndocument retrieval to produce more accurate and grounded responses. While RAG\nhas shown strong potential in reducing hallucinations and improving factual\nconsistency, it also introduces new privacy and security challenges that differ\nfrom those faced by traditional LLMs. Existing research has demonstrated that\nLLMs can leak sensitive information through training data memorization or\nadversarial prompts, and RAG systems inherit many of these vulnerabilities. At\nthe same time, reliance of RAG on an external knowledge base opens new attack\nsurfaces, including the potential for leaking information about the presence or\ncontent of retrieved documents, or for injecting malicious content to\nmanipulate model behavior. Despite these risks, there is currently no formal\nframework that defines the threat landscape for RAG systems. In this paper, we\naddress a critical gap in the literature by proposing, to the best of our\nknowledge, the first formal threat model for retrieval-RAG systems. We\nintroduce a structured taxonomy of adversary types based on their access to\nmodel components and data, and we formally define key threat vectors such as\ndocument-level membership inference and data poisoning, which pose serious\nprivacy and integrity risks in real-world deployments. By establishing formal\ndefinitions and attack models, our work lays the foundation for a more rigorous\nand principled understanding of privacy and security in RAG systems."}
{"id": "2509.19839", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.19839", "abs": "https://arxiv.org/abs/2509.19839", "authors": ["Huizhen Shu", "Xuying Li", "Zhuo Li"], "title": "LatentGuard: Controllable Latent Steering for Robust Refusal of Attacks and Reliable Response Generation", "comment": "9-page NeurIPS 2025 preprint including 3 figures and 1 table, with\n  additional appendix material. Prepared using the NeurIPS 2025 preprint\n  template and compiled with pdfLaTeX. All references are included via the\n  provided .bbl file. Figures are in PDF format. No external supplementary\n  files. All necessary style files and images are included", "summary": "Achieving robust safety alignment in large language models (LLMs) while\npreserving their utility remains a fundamental challenge. Existing approaches\noften struggle to balance comprehensive safety with fine-grained\ncontrollability at the representation level. We introduce LATENTGUARD, a novel\nthree-stage framework that combines behavioral alignment with supervised latent\nspace control for interpretable and precise safety steering. Our approach\nbegins by fine-tuning an LLM on rationalized datasets containing both\nreasoning-enhanced refusal responses to adversarial prompts and\nreasoning-enhanced normal responses to benign queries, establishing robust\nbehavioral priors across both safety-critical and utility-preserving scenarios.\nWe then train a structured variational autoencoder (VAE) on intermediate MLP\nactivations, supervised by multi-label annotations including attack types,\nattack methods, and benign indicators. This supervision enables the VAE to\nlearn disentangled latent representations that capture distinct adversarial\ncharacteristics while maintaining semantic interpretability. Through targeted\nmanipulation of learned latent dimensions, LATENTGUARD achieves selective\nrefusal behavior, effectively blocking harmful requests while preserving\nhelpfulness for legitimate use cases. Experiments on Qwen3-8B demonstrate\nsignificant improvements in both safety controllability and response\ninterpretability without compromising utility. Cross-architecture validation on\nMistral-7B confirms the generalizability of our latent steering approach,\nshowing consistent effectiveness across different model families. Our results\nsuggest that structured representation-level intervention offers a promising\npathway toward building safer yet practical LLM systems."}
{"id": "2509.20215", "categories": ["cs.SE", "cs.AI", "cs.AR"], "pdf": "https://arxiv.org/pdf/2509.20215", "abs": "https://arxiv.org/abs/2509.20215", "authors": ["Guang Yang", "Wei Zheng", "Xiang Chen", "Yifan Sun", "Fengji Zhang", "Terry Yue Zhuo"], "title": "The Cream Rises to the Top: Efficient Reranking Method for Verilog Code Generation", "comment": "Under review ICASSP 2026", "summary": "LLMs face significant challenges in Verilog generation due to limited\ndomain-specific knowledge. While sampling techniques improve pass@k metrics,\nhardware engineers need one trustworthy solution rather than uncertain\ncandidates. To bridge this gap, we formulate it as a semantic alignment problem\nbetween requirements and Verilog implementations, and propose VCD-RNK, a\ndiscriminator model tailored for efficient Verilog code reranking.\nSpecifically, VCD-RNKincorporates Verilog-specific reasoning by distilling\nexpert knowledge across three dimensions: code semantic analysis, test case\ngeneration, and functional correctness assessment. By explicitly simulating the\nabove reasoning processes during inference, VCD-RNK effectively avoids\ncomputationally intensive test execution in existing methods."}
{"id": "2509.20356", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.20356", "abs": "https://arxiv.org/abs/2509.20356", "authors": ["Mohamed E. Najd", "Ghada Almashaqbeh"], "title": "chainScale: Secure Functionality-oriented Scalability for Decentralized Resource Markets", "comment": null, "summary": "Decentralized resource markets are Web 3.0 applications that build\nopen-access platforms for trading digital resources among users without any\ncentral management. They promise cost reduction, transparency, and flexible\nservice provision. However, these markets usually have large workload that must\nbe processed in a timely manner, leading to serious scalability problems.\nDespite the large amount of work on blockchain scalability, existing solutions\nare ineffective as they do not account for these markets' work models and\ntraffic patterns.\n  We introduce chainScale, a secure hybrid sidechain-sharding solution that\naims to boost throughput of decentralized resource markets and reduce their\nlatency and storage footprint. At its core, chainScale leverages dependent\nsidechains and functionality-oriented workload splitting to parallelize traffic\nprocessing by having each market module assigned to a sidechain. Different from\nsharding, chainScale does not incur any cross-sidechain transactions that tend\nto be costly. chainScale introduces several techniques, including hierarchical\nworkload sharing that further sub-divides overloaded modules, and weighted\nminer assignment that assigns miners with vested interest in the system to\ncritical modules' sidechains. Furthermore, chainScale employs sidechain syncing\nto maintain the mainchain as the single truth of system state, and pruning to\ndiscard stale records. Beside analyzing security, we build a proof-of-concept\nimplementation for a distributed file storage market as a use case. Our\nexperiments show that, compared to a single sidechain-based prior solution,\nchainScale boosts throughput by 4x and reduces confirmation latency by 5x.\nAlso, they show that chainScale outperforms sharding by 2.5x in throughput and\n3.5x in latency."}
{"id": "2509.19925", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.19925", "abs": "https://arxiv.org/abs/2509.19925", "authors": ["Ajeet Kumar Singh", "Rajsabi Surya", "Anurag Tripathi", "Santanu Choudhury", "Sudhir Bisane"], "title": "CON-QA: Privacy-Preserving QA using cloud LLMs in Contract Domain", "comment": null, "summary": "As enterprises increasingly integrate cloud-based large language models\n(LLMs) such as ChatGPT and Gemini into their legal document workflows,\nprotecting sensitive contractual information - including Personally\nIdentifiable Information (PII) and commercially sensitive clauses - has emerged\nas a critical challenge. In this work, we propose CON-QA, a hybrid\nprivacy-preserving framework designed specifically for secure question\nanswering over enterprise contracts, effectively combining local and\ncloud-hosted LLMs. The CON-QA framework operates through three stages: (i)\nsemantic query decomposition and query-aware document chunk retrieval using a\nlocally deployed LLM analysis, (ii) anonymization of detected sensitive\nentities via a structured one-to-many mapping scheme, ensuring semantic\ncoherence while preventing cross-session entity inference attacks, and (iii)\nanonymized response generation by a cloud-based LLM, with accurate\nreconstruction of the original answer locally using a session-consistent\nmany-to-one reverse mapping. To rigorously evaluate CON-QA, we introduce\nCUAD-QA, a corpus of 85k question-answer pairs generated over 510 real-world\nCUAD contract documents, encompassing simple, complex, and summarization-style\nqueries. Empirical evaluations, complemented by detailed human assessments,\nconfirm that CON-QA effectively maintains both privacy and utility, preserves\nanswer quality, maintains fidelity to legal clause semantics, and significantly\nmitigates privacy risks, demonstrating its practical suitability for secure,\nenterprise-level contract documents."}
{"id": "2509.20300", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.20300", "abs": "https://arxiv.org/abs/2509.20300", "authors": ["Jannis Kiesel", "Jonathan Heiss"], "title": "Confidentiality-Preserving Verifiable Business Processes through Zero-Knowledge Proofs", "comment": null, "summary": "Ensuring the integrity of business processes without disclosing confidential\nbusiness information is a major challenge in inter-organizational processes.\nThis paper introduces a zero-knowledge proof (ZKP)-based approach for the\nverifiable execution of business processes while preserving confidentiality. We\nintegrate ZK virtual machines (zkVMs) into business process management engines\nthrough a comprehensive system architecture and a prototypical implementation.\nOur approach supports chained verifiable computations through proof\ncompositions. On the example of product carbon footprinting, we model\nsequential footprinting activities and demonstrate how organizations can prove\nand verify the integrity of verifiable processes without exposing sensitive\ninformation. We assess different ZKP proving variants within process models for\ntheir efficiency in proving and verifying, and discuss the practical\nintegration of ZKPs throughout the Business Process Management (BPM) lifecycle.\nOur experiment-driven evaluation demonstrates the automation of process\nverification under given confidentiality constraints."}
{"id": "2509.20362", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.20362", "abs": "https://arxiv.org/abs/2509.20362", "authors": ["Shaoyuan Xie", "Mohamad Habib Fakih", "Junchi Lu", "Fayzah Alshammari", "Ningfei Wang", "Takami Sato", "Halima Bouzidi", "Mohammad Abdullah Al Faruque", "Qi Alfred Chen"], "title": "FlyTrap: Physical Distance-Pulling Attack Towards Camera-based Autonomous Target Tracking Systems", "comment": "An extended version of the paper accepted by NDSS 2026", "summary": "Autonomous Target Tracking (ATT) systems, especially ATT drones, are widely\nused in applications such as surveillance, border control, and law enforcement,\nwhile also being misused in stalking and destructive actions. Thus, the\nsecurity of ATT is highly critical for real-world applications. Under the\nscope, we present a new type of attack: distance-pulling attacks (DPA) and a\nsystematic study of it, which exploits vulnerabilities in ATT systems to\ndangerously reduce tracking distances, leading to drone capturing, increased\nsusceptibility to sensor attacks, or even physical collisions. To achieve these\ngoals, we present FlyTrap, a novel physical-world attack framework that employs\nan adversarial umbrella as a deployable and domain-specific attack vector.\nFlyTrap is specifically designed to meet key desired objectives in attacking\nATT drones: physical deployability, closed-loop effectiveness, and\nspatial-temporal consistency. Through novel progressive distance-pulling\nstrategy and controllable spatial-temporal consistency designs, FlyTrap\nmanipulates ATT drones in real-world setups to achieve significant system-level\nimpacts. Our evaluations include new datasets, metrics, and closed-loop\nexperiments on real-world white-box and even commercial ATT drones, including\nDJI and HoverAir. Results demonstrate FlyTrap's ability to reduce tracking\ndistances within the range to be captured, sensor attacked, or even directly\ncrashed, highlighting urgent security risks and practical implications for the\nsafe deployment of ATT systems."}
{"id": "2509.20021", "categories": ["cs.AI", "cs.CL", "cs.RO"], "pdf": "https://arxiv.org/pdf/2509.20021", "abs": "https://arxiv.org/abs/2509.20021", "authors": ["Tongtong Feng", "Xin Wang", "Yu-Gang Jiang", "Wenwu Zhu"], "title": "Embodied AI: From LLMs to World Models", "comment": "Accepted by IEEE CASM", "summary": "Embodied Artificial Intelligence (AI) is an intelligent system paradigm for\nachieving Artificial General Intelligence (AGI), serving as the cornerstone for\nvarious applications and driving the evolution from cyberspace to physical\nsystems. Recent breakthroughs in Large Language Models (LLMs) and World Models\n(WMs) have drawn significant attention for embodied AI. On the one hand, LLMs\nempower embodied AI via semantic reasoning and task decomposition, bringing\nhigh-level natural language instructions and low-level natural language actions\ninto embodied cognition. On the other hand, WMs empower embodied AI by building\ninternal representations and future predictions of the external world,\nfacilitating physical law-compliant embodied interactions. As such, this paper\ncomprehensively explores the literature in embodied AI from basics to advances,\ncovering both LLM driven and WM driven works. In particular, we first present\nthe history, key technologies, key components, and hardware systems of embodied\nAI, as well as discuss its development via looking from unimodal to multimodal\nangle. We then scrutinize the two burgeoning fields of embodied AI, i.e.,\nembodied AI with LLMs/multimodal LLMs (MLLMs) and embodied AI with WMs,\nmeticulously delineating their indispensable roles in end-to-end embodied\ncognition and physical laws-driven embodied interactions. Building upon the\nabove advances, we further share our insights on the necessity of the joint\nMLLM-WM driven embodied AI architecture, shedding light on its profound\nsignificance in enabling complex tasks within physical worlds. In addition, we\nexamine representative applications of embodied AI, demonstrating its wide\napplicability in real-world scenarios. Last but not least, we point out future\nresearch directions of embodied AI that deserve further investigation."}
{"id": "2509.20308", "categories": ["cs.SE", "68M15 (Primary), 68M12, 68Q42 (Secondary)", "D.2.5; C.2.2; F.4.2"], "pdf": "https://arxiv.org/pdf/2509.20308", "abs": "https://arxiv.org/abs/2509.20308", "authors": ["Alexander Liggesmeyer", "José Antonio Zamudio Amaya", "Andreas Zeller"], "title": "Protocol Testing with I/O Grammars", "comment": "20 pages", "summary": "Generating software tests faces two fundamental problems. First, one needs to\n_generate inputs_ that are syntactically and semantically correct, yet\nsufficiently diverse to cover behavior. Second, one needs an _oracle_ to _check\noutputs_ whether a test case is correct or not. Both problems become apparent\nin _protocol testing_, where inputs are messages exchanged between parties, and\noutputs are the responses of these parties.\n  In this paper, we propose a novel approach to protocol testing that combines\ninput generation and output checking in a single framework. We introduce _I/O\ngrammars_ as the first means to _completely_ specify the syntax and semantics\nof protocols, including messages, states, and interactions. Our implementation,\nbased on the FANDANGO framework, takes a single I/O grammar, and can act as a\n_test generator_, as a _mock object_, and as an _oracle_ for a _client_, a\n_server_, or both (or actually any number of parties), a versatility not found\nin any existing tool or formalism. User-defined _constraints}_can have the\ngenerator focus on arbitrary protocol features; $k$-path guidance\nsystematically covers states, messages, responses, and value alternatives in a\nunified fashion.\n  We evaluate the effectiveness of our approach by applying it to several\nprotocols, including DNS, FTP, and SMTP. We demonstrate that I/O grammars can\nspecify advanced protocol features correctly and completely, while also\nenabling output validation of the programs under test. In its evaluation, we\nfind that systematic coverage of the I/O grammar results in much quicker\ncoverage of the input and response spaces (and thus functionality) compared to\nthe random-based state-of-the-art approaches."}
{"id": "2509.19533", "categories": ["cs.SE", "cs.AI", "cs.CR"], "pdf": "https://arxiv.org/pdf/2509.19533", "abs": "https://arxiv.org/abs/2509.19533", "authors": ["Mengdi Lu", "Steven Ding", "Furkan Alaca", "Philippe Charland"], "title": "Semantic-Aware Fuzzing: An Empirical Framework for LLM-Guided, Reasoning-Driven Input Mutation", "comment": null, "summary": "Security vulnerabilities in Internet-of-Things devices, mobile platforms, and\nautonomous systems remain critical. Traditional mutation-based fuzzers -- while\neffectively explore code paths -- primarily perform byte- or bit-level edits\nwithout semantic reasoning. Coverage-guided tools such as AFL++ use\ndictionaries, grammars, and splicing heuristics to impose shallow structural\nconstraints, leaving deeper protocol logic, inter-field dependencies, and\ndomain-specific semantics unaddressed. Conversely, reasoning-capable large\nlanguage models (LLMs) can leverage pretraining knowledge to understand input\nformats, respect complex constraints, and propose targeted mutations, much like\nan experienced reverse engineer or testing expert. However, lacking ground\ntruth for \"correct\" mutation reasoning makes supervised fine-tuning\nimpractical, motivating explorations of off-the-shelf LLMs via prompt-based\nfew-shot learning. To bridge this gap, we present an open-source microservices\nframework that integrates reasoning LLMs with AFL++ on Google's FuzzBench,\ntackling asynchronous execution and divergent hardware demands (GPU- vs.\nCPU-intensive) of LLMs and fuzzers. We evaluate four research questions: (R1)\nHow can reasoning LLMs be integrated into the fuzzing mutation loop? (R2) Do\nfew-shot prompts yield higher-quality mutations than zero-shot? (R3) Can prompt\nengineering with off-the-shelf models improve fuzzing directly? and (R4) Which\nopen-source reasoning LLMs perform best under prompt-only conditions?\nExperiments with Llama3.3, Deepseek-r1-Distill-Llama-70B, QwQ-32B, and Gemma3\nhighlight Deepseek as the most promising. Mutation effectiveness depends more\non prompt complexity and model choice than shot count. Response latency and\nthroughput bottlenecks remain key obstacles, offering directions for future\nwork."}
{"id": "2509.20067", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.20067", "abs": "https://arxiv.org/abs/2509.20067", "authors": ["Wenliang Li", "Rui Yan", "Xu Zhang", "Li Chen", "Hongji Zhu", "Jing Zhao", "Junjun Li", "Mengru Li", "Wei Cao", "Zihang Jiang", "Wei Wei", "Kun Zhang", "Shaohua Kevin Zhou"], "title": "MACD: Multi-Agent Clinical Diagnosis with Self-Learned Knowledge for LLM", "comment": null, "summary": "Large language models (LLMs) have demonstrated notable potential in medical\napplications, yet they face substantial challenges in handling complex\nreal-world clinical diagnoses using conventional prompting methods. Current\nprompt engineering and multi-agent approaches typically optimize isolated\ninferences, neglecting the accumulation of reusable clinical experience. To\naddress this, this study proposes a novel Multi-Agent Clinical Diagnosis (MACD)\nframework, which allows LLMs to self-learn clinical knowledge via a multi-agent\npipeline that summarizes, refines, and applies diagnostic insights. It mirrors\nhow physicians develop expertise through experience, enabling more focused and\naccurate diagnosis on key disease-specific cues. We further extend it to a\nMACD-human collaborative workflow, where multiple LLM-based diagnostician\nagents engage in iterative consultations, supported by an evaluator agent and\nhuman oversight for cases where agreement is not reached. Evaluated on 4,390\nreal-world patient cases across seven diseases using diverse open-source LLMs\n(Llama-3.1 8B/70B, DeepSeek-R1-Distill-Llama 70B), MACD significantly improves\nprimary diagnostic accuracy, outperforming established clinical guidelines with\ngains up to 22.3% (MACD). On the subset of the data, it achieves performance on\npar with or exceeding that of human physicians (up to 16% improvement over\nphysicians-only diagnosis). Additionally, on the MACD-human workflow, it\nachieves an 18.6% improvement compared to physicians-only diagnosis. Moreover,\nself-learned knowledge exhibits strong cross-model stability, transferability,\nand model-specific personalization, while the system can generate traceable\nrationales, enhancing explainability. Consequently, this work presents a\nscalable self-learning paradigm for LLM-assisted diagnosis, bridging the gap\nbetween the intrinsic knowledge of LLMs and real-world clinical practice."}
{"id": "2509.20353", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.20353", "abs": "https://arxiv.org/abs/2509.20353", "authors": ["Viktoria Stray", "Elias Goldmann Brandtzæg", "Viggo Tellefsen Wivestad", "Astri Barbala", "Nils Brede Moe"], "title": "Developer Productivity With and Without GitHub Copilot: A Longitudinal Mixed-Methods Case Study", "comment": "Accepted for publication in the Proceedings of the 59th Hawaii\n  International Conference on System Sciences (HICSS 2026)", "summary": "This study investigates the real-world impact of the generative AI (GenAI)\ntool GitHub Copilot on developer activity and perceived productivity. We\nconducted a mixed-methods case study in NAV IT, a large public sector agile\norganization. We analyzed 26,317 unique non-merge commits from 703 of NAV IT's\nGitHub repositories over a two-year period, focusing on commit-based activity\nmetrics from 25 Copilot users and 14 non-users. The analysis was complemented\nby survey responses on their roles and perceived productivity, as well as 13\ninterviews. Our analysis of activity metrics revealed that individuals who used\nCopilot were consistently more active than non-users, even prior to Copilot's\nintroduction. We did not find any statistically significant changes in\ncommit-based activity for Copilot users after they adopted the tool, although\nminor increases were observed. This suggests a discrepancy between changes in\ncommit-based metrics and the subjective experience of productivity."}
{"id": "2509.20095", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.20095", "abs": "https://arxiv.org/abs/2509.20095", "authors": ["Aymeric Vellinger", "Nemanja Antonic", "Elio Tuci"], "title": "From Pheromones to Policies: Reinforcement Learning for Engineered Biological Swarms", "comment": "Contribution to the 9th International Symposium on Swarm Behavior and\n  Bio-Inspired Robotics 2025", "summary": "Swarm intelligence emerges from decentralised interactions among simple\nagents, enabling collective problem-solving. This study establishes a\ntheoretical equivalence between pheromone-mediated aggregation in \\celeg\\ and\nreinforcement learning (RL), demonstrating how stigmergic signals function as\ndistributed reward mechanisms. We model engineered nematode swarms performing\nforaging tasks, showing that pheromone dynamics mathematically mirror\ncross-learning updates, a fundamental RL algorithm. Experimental validation\nwith data from literature confirms that our model accurately replicates\nempirical \\celeg\\ foraging patterns under static conditions. In dynamic\nenvironments, persistent pheromone trails create positive feedback loops that\nhinder adaptation by locking swarms into obsolete choices. Through\ncomputational experiments in multi-armed bandit scenarios, we reveal that\nintroducing a minority of exploratory agents insensitive to pheromones restores\ncollective plasticity, enabling rapid task switching. This behavioural\nheterogeneity balances exploration-exploitation trade-offs, implementing\nswarm-level extinction of outdated strategies. Our results demonstrate that\nstigmergic systems inherently encode distributed RL processes, where\nenvironmental signals act as external memory for collective credit assignment.\nBy bridging synthetic biology with swarm robotics, this work advances\nprogrammable living systems capable of resilient decision-making in volatile\nenvironments."}
{"id": "2509.19783", "categories": ["cs.AI", "cs.HC", "cs.SE"], "pdf": "https://arxiv.org/pdf/2509.19783", "abs": "https://arxiv.org/abs/2509.19783", "authors": ["Jiexi Xu"], "title": "Agentic Metacognition: Designing a \"Self-Aware\" Low-Code Agent for Failure Prediction and Human Handoff", "comment": "7 pages, 2 tables", "summary": "The inherent non-deterministic nature of autonomous agents, particularly\nwithin low-code/no-code (LCNC) environments, presents significant reliability\nchallenges. Agents can become trapped in unforeseen loops, generate inaccurate\noutputs, or encounter unrecoverable failures, leading to user frustration and a\nbreakdown of trust. This report proposes a novel architectural pattern to\naddress these issues: the integration of a secondary, \"metacognitive\" layer\nthat actively monitors the primary LCNC agent. Inspired by human introspection,\nthis layer is designed to predict impending task failures based on a defined\nset of triggers, such as excessive latency or repetitive actions. Upon\npredicting a failure, the metacognitive agent proactively initiates a human\nhandoff, providing the user with a clear summary of the agent's \"thought\nprocess\" and a detailed explanation of why it could not proceed. An empirical\nanalysis of a prototype system demonstrates that this approach significantly\nincreases the overall task success rate. However, this performance gain comes\nwith a notable increase in computational overhead. The findings reframe human\nhandoffs not as an admission of defeat but as a core design feature that\nenhances system resilience, improves user experience, and builds trust by\nproviding transparency into the agent's internal state. The report discusses\nthe practical and ethical implications of this approach and identifies key\ndirections for future research."}
{"id": "2509.20102", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.20102", "abs": "https://arxiv.org/abs/2509.20102", "authors": ["Tong Nie", "Yuewen Mei", "Yihong Tang", "Junlin He", "Jie Sun", "Haotian Shi", "Wei Ma", "Jian Sun"], "title": "Steerable Adversarial Scenario Generation through Test-Time Preference Alignment", "comment": null, "summary": "Adversarial scenario generation is a cost-effective approach for safety\nassessment of autonomous driving systems. However, existing methods are often\nconstrained to a single, fixed trade-off between competing objectives such as\nadversariality and realism. This yields behavior-specific models that cannot be\nsteered at inference time, lacking the efficiency and flexibility to generate\ntailored scenarios for diverse training and testing requirements. In view of\nthis, we reframe the task of adversarial scenario generation as a\nmulti-objective preference alignment problem and introduce a new framework\nnamed \\textbf{S}teerable \\textbf{A}dversarial scenario \\textbf{GE}nerator\n(SAGE). SAGE enables fine-grained test-time control over the trade-off between\nadversariality and realism without any retraining. We first propose\nhierarchical group-based preference optimization, a data-efficient offline\nalignment method that learns to balance competing objectives by decoupling hard\nfeasibility constraints from soft preferences. Instead of training a fixed\nmodel, SAGE fine-tunes two experts on opposing preferences and constructs a\ncontinuous spectrum of policies at inference time by linearly interpolating\ntheir weights. We provide theoretical justification for this framework through\nthe lens of linear mode connectivity. Extensive experiments demonstrate that\nSAGE not only generates scenarios with a superior balance of adversariality and\nrealism but also enables more effective closed-loop training of driving\npolicies. Project page: https://tongnie.github.io/SAGE/."}
{"id": "2509.20105", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.20105", "abs": "https://arxiv.org/abs/2509.20105", "authors": ["Venkat Margapuri", "Garik Kazanjian", "Naren Kosaraju"], "title": "PEPS: Quantum-Inspired Reinforcement Learning for Coherent Reasoning Traces in LLMs", "comment": null, "summary": "Large Language Models (LLMs) often struggle with maintaining coherent\nmulti-step reasoning traces, particularly in tasks that require a structured\nlogical flow. This work introduces a quantum-inspired approach to address the\nchallenge by incorporating a fidelity-based reward derived from Projected\nEntangled Pair States (PEPS) into Proximal Policy Optimization. Unlike prior\napproaches that use direct supervision or contrastive objectives, the proposed\nmethod guides learning through structural consistency, offering a novel\napproach to enforce global coherence in generated reasoning traces. The\nproposed framework is evaluated using multiple coherence-determining metrics on\ndiverse datasets such as GSM8K, StrategyQA, and EntailmentBank spanning\narithmetic, intuitive, and entailment-based reasoning. Results show that the\nproposed quantum-inspired approach offers significant improvements over\nsupervised, contrastive, and pretrained baseline approaches, highlighting the\neffectiveness of quantum-inspired fidelity as a foundation to improve reasoning\ntrace coherence in LLMs."}
{"id": "2509.20138", "categories": ["cs.AI", "68Q60, 68T20"], "pdf": "https://arxiv.org/pdf/2509.20138", "abs": "https://arxiv.org/abs/2509.20138", "authors": ["Wieger Wesselink", "Kees Huizing", "Huub van de Wetering"], "title": "Formal Verification of Minimax Algorithms", "comment": "12 pages", "summary": "Using the Dafny verification system, we formally verify a range of minimax\nsearch algorithms, including variations with alpha-beta pruning and\ntransposition tables. For depth-limited search with transposition tables, we\nintroduce a witness-based correctness criterion and apply it to two\nrepresentative algorithms. All verification artifacts, including proofs and\nPython implementations, are publicly available."}
{"id": "2509.20175", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.20175", "abs": "https://arxiv.org/abs/2509.20175", "authors": ["Lorenzo Giusti", "Ole Anton Werner", "Riccardo Taiello", "Matilde Carvalho Costa", "Emre Tosun", "Andrea Protani", "Marc Molina", "Rodrigo Lopes de Almeida", "Paolo Cacace", "Diogo Reis Santos", "Luigi Serio"], "title": "Federation of Agents: A Semantics-Aware Communication Fabric for Large-Scale Agentic AI", "comment": "18 pages, 4 figures", "summary": "We present Federation of Agents (FoA), a distributed orchestration framework\nthat transforms static multi-agent coordination into dynamic, capability-driven\ncollaboration. FoA introduces Versioned Capability Vectors (VCVs):\nmachine-readable profiles that make agent capabilities searchable through\nsemantic embeddings, enabling agents to advertise their capabilities, cost, and\nlimitations. Our aarchitecturecombines three key innovations: (1) semantic\nrouting that matches tasks to agents over sharded HNSW indices while enforcing\noperational constraints through cost-biased optimization, (2) dynamic task\ndecomposition where compatible agents collaboratively break down complex tasks\ninto DAGs of subtasks through consensus-based merging, and (3) smart clustering\nthat groups agents working on similar subtasks into collaborative channels for\nk-round refinement before synthesis. Built on top of MQTT,s publish-subscribe\nsemantics for scalable message passing, FoA achieves sub-linear complexity\nthrough hierarchical capability matching and efficient index maintenance.\nEvaluation on HealthBench shows 13x improvements over single-model baselines,\nwith clustering-enhanced laboration particularly effective for complex\nreasoning tasks requiring multiple perspectives. The system scales horizontally\nwhile maintaining consistent performance, demonstrating that semantic\norchestration with structured collaboration can unlock the collective\nintelligence of heterogeneous federations of AI agents."}
{"id": "2509.20218", "categories": ["cs.AI", "cs.AR", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.20218", "abs": "https://arxiv.org/abs/2509.20218", "authors": ["Mohamed Manzour", "Catherine M. Elias", "Omar M. Shehata", "Rubén Izquierdo", "Miguel Ángel Sotelo"], "title": "Design Insights and Comparative Evaluation of a Hardware-Based Cooperative Perception Architecture for Lane Change Prediction", "comment": null, "summary": "Research on lane change prediction has gained attention in the last few\nyears. Most existing works in this area have been conducted in simulation\nenvironments or with pre-recorded datasets, these works often rely on\nsimplified assumptions about sensing, communication, and traffic behavior that\ndo not always hold in practice. Real-world deployments of lane-change\nprediction systems are relatively rare, and when they are reported, the\npractical challenges, limitations, and lessons learned are often\nunder-documented. This study explores cooperative lane-change prediction\nthrough a real hardware deployment in mixed traffic and shares the insights\nthat emerged during implementation and testing. We highlight the practical\nchallenges we faced, including bottlenecks, reliability issues, and operational\nconstraints that shaped the behavior of the system. By documenting these\nexperiences, the study provides guidance for others working on similar\npipelines."}
{"id": "2509.20270", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.20270", "abs": "https://arxiv.org/abs/2509.20270", "authors": ["Xingjian Kang", "Linda Vorberg", "Andreas Maier", "Alexander Katzmann", "Oliver Taubmann"], "title": "Scan-do Attitude: Towards Autonomous CT Protocol Management using a Large Language Model Agent", "comment": null, "summary": "Managing scan protocols in Computed Tomography (CT), which includes adjusting\nacquisition parameters or configuring reconstructions, as well as selecting\npostprocessing tools in a patient-specific manner, is time-consuming and\nrequires clinical as well as technical expertise. At the same time, we observe\nan increasing shortage of skilled workforce in radiology. To address this\nissue, a Large Language Model (LLM)-based agent framework is proposed to assist\nwith the interpretation and execution of protocol configuration requests given\nin natural language or a structured, device-independent format, aiming to\nimprove the workflow efficiency and reduce technologists' workload. The agent\ncombines in-context-learning, instruction-following, and structured toolcalling\nabilities to identify relevant protocol elements and apply accurate\nmodifications. In a systematic evaluation, experimental results indicate that\nthe agent can effectively retrieve protocol components, generate device\ncompatible protocol definition files, and faithfully implement user requests.\nDespite demonstrating feasibility in principle, the approach faces limitations\nregarding syntactic and semantic validity due to lack of a unified device API,\nand challenges with ambiguous or complex requests. In summary, the findings\nshow a clear path towards LLM-based agents for supporting scan protocol\nmanagement in CT imaging."}
{"id": "2509.19153", "categories": ["cs.CR", "cs.AI", "cs.SE"], "pdf": "https://arxiv.org/pdf/2509.19153", "abs": "https://arxiv.org/abs/2509.19153", "authors": ["Massimo Bartoletti", "Enrico Lipparini", "Livio Pompianu"], "title": "LLMs as verification oracles for Solidity", "comment": null, "summary": "Ensuring the correctness of smart contracts is critical, as even subtle flaws\ncan lead to severe financial losses. While bug detection tools able to spot\ncommon vulnerability patterns can serve as a first line of defense, most\nreal-world exploits and losses stem from errors in the contract business logic.\nFormal verification tools such as SolCMC and the Certora Prover address this\nchallenge, but their impact remains limited by steep learning curves and\nrestricted specification languages. Recent works have begun to explore the use\nof large language models (LLMs) for security-related tasks such as\nvulnerability detection and test generation. Yet, a fundamental question\nremains open: can LLMs serve as verification oracles, capable of reasoning\nabout arbitrary contract-specific properties? In this paper, we provide the\nfirst systematic evaluation of GPT-5, a state-of-the-art reasoning LLM, in this\nrole. We benchmark its performance on a large dataset of verification tasks,\ncompare its outputs against those of established formal verification tools, and\nassess its practical effectiveness in real-world auditing scenarios. Our study\ncombines quantitative metrics with qualitative analysis, and shows that recent\nreasoning-oriented LLMs can be surprisingly effective as verification oracles,\nsuggesting a new frontier in the convergence of AI and formal methods for\nsecure smart contract development and auditing."}
{"id": "2509.19485", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.19485", "abs": "https://arxiv.org/abs/2509.19485", "authors": ["Hafijul Hoque Chowdhury", "Riad Ahmed Anonto", "Sourov Jajodia", "Suryadipta Majumdar", "Md. Shohrab Hossain"], "title": "Identifying and Addressing User-level Security Concerns in Smart Homes Using \"Smaller\" LLMs", "comment": "10 pages, accepted at PST 2025", "summary": "With the rapid growth of smart home IoT devices, users are increasingly\nexposed to various security risks, as evident from recent studies. While\nseeking answers to know more on those security concerns, users are mostly left\nwith their own discretion while going through various sources, such as online\nblogs and technical manuals, which may render higher complexity to regular\nusers trying to extract the necessary information. This requirement does not go\nalong with the common mindsets of smart home users and hence threatens the\nsecurity of smart homes furthermore. In this paper, we aim to identify and\naddress the major user-level security concerns in smart homes. Specifically, we\ndevelop a novel dataset of Q&A from public forums, capturing practical security\nchallenges faced by smart home users. We extract major security concerns in\nsmart homes from our dataset by leveraging the Latent Dirichlet Allocation\n(LDA). We fine-tune relatively \"smaller\" transformer models, such as T5 and\nFlan-T5, on this dataset to build a QA system tailored for smart home security.\nUnlike larger models like GPT and Gemini, which are powerful but often resource\nhungry and require data sharing, smaller models are more feasible for\ndeployment in resource-constrained or privacy-sensitive environments like smart\nhomes. The dataset is manually curated and supplemented with synthetic data to\nexplore its potential impact on model performance. This approach significantly\nimproves the system's ability to deliver accurate and relevant answers, helping\nusers address common security concerns with smart home IoT devices. Our\nexperiments on real-world user concerns show that our work improves the\nperformance of the base models."}
{"id": "2509.19533", "categories": ["cs.SE", "cs.AI", "cs.CR"], "pdf": "https://arxiv.org/pdf/2509.19533", "abs": "https://arxiv.org/abs/2509.19533", "authors": ["Mengdi Lu", "Steven Ding", "Furkan Alaca", "Philippe Charland"], "title": "Semantic-Aware Fuzzing: An Empirical Framework for LLM-Guided, Reasoning-Driven Input Mutation", "comment": null, "summary": "Security vulnerabilities in Internet-of-Things devices, mobile platforms, and\nautonomous systems remain critical. Traditional mutation-based fuzzers -- while\neffectively explore code paths -- primarily perform byte- or bit-level edits\nwithout semantic reasoning. Coverage-guided tools such as AFL++ use\ndictionaries, grammars, and splicing heuristics to impose shallow structural\nconstraints, leaving deeper protocol logic, inter-field dependencies, and\ndomain-specific semantics unaddressed. Conversely, reasoning-capable large\nlanguage models (LLMs) can leverage pretraining knowledge to understand input\nformats, respect complex constraints, and propose targeted mutations, much like\nan experienced reverse engineer or testing expert. However, lacking ground\ntruth for \"correct\" mutation reasoning makes supervised fine-tuning\nimpractical, motivating explorations of off-the-shelf LLMs via prompt-based\nfew-shot learning. To bridge this gap, we present an open-source microservices\nframework that integrates reasoning LLMs with AFL++ on Google's FuzzBench,\ntackling asynchronous execution and divergent hardware demands (GPU- vs.\nCPU-intensive) of LLMs and fuzzers. We evaluate four research questions: (R1)\nHow can reasoning LLMs be integrated into the fuzzing mutation loop? (R2) Do\nfew-shot prompts yield higher-quality mutations than zero-shot? (R3) Can prompt\nengineering with off-the-shelf models improve fuzzing directly? and (R4) Which\nopen-source reasoning LLMs perform best under prompt-only conditions?\nExperiments with Llama3.3, Deepseek-r1-Distill-Llama-70B, QwQ-32B, and Gemma3\nhighlight Deepseek as the most promising. Mutation effectiveness depends more\non prompt complexity and model choice than shot count. Response latency and\nthroughput bottlenecks remain key obstacles, offering directions for future\nwork."}
{"id": "2509.19587", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.19587", "abs": "https://arxiv.org/abs/2509.19587", "authors": ["Mohamed Ouf", "Haoyu Li", "Michael Zhang", "Mariam Guizani"], "title": "Reverse Engineering User Stories from Code using Large Language Models", "comment": null, "summary": "User stories are essential in agile development, yet often missing or\noutdated in legacy and poorly documented systems. We investigate whether large\nlanguage models (LLMs) can automatically recover user stories directly from\nsource code and how prompt design impacts output quality. Using 1,750 annotated\nC++ snippets of varying complexity, we evaluate five state-of-the-art LLMs\nacross six prompting strategies. Results show that all models achieve, on\naverage, an F1 score of 0.8 for code up to 200 NLOC. Our findings show that a\nsingle illustrative example enables the smallest model (8B) to match the\nperformance of a much larger 70B model. In contrast, structured reasoning via\nChain-of-Thought offers only marginal gains, primarily for larger models."}
{"id": "2509.19708", "categories": ["cs.SE", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.19708", "abs": "https://arxiv.org/abs/2509.19708", "authors": ["Anand Kumar", "Vishal Khare", "Deepak Sharma", "Satyam Kumar", "Vijay Saini", "Anshul Yadav", "Sachendra Jain", "Ankit Rana", "Pratham Verma", "Vaibhav Meena", "Avinash Edubilli"], "title": "Intuition to Evidence: Measuring AI's True Impact on Developer Productivity", "comment": "16 pages, 10 figures, 5 tables", "summary": "We present a comprehensive real-world evaluation of AI-assisted software\ndevelopment tools deployed at enterprise scale. Over one year, 300 engineers\nacross multiple teams integrated an in-house AI platform (DeputyDev) that\ncombines code generation and automated review capabilities into their daily\nworkflows. Through rigorous cohort analysis, our study demonstrates\nstatistically significant productivity improvements, including an overall 31.8%\nreduction in PR review cycle time.\n  Developer adoption was strong, with 85% satisfaction for code review features\nand 93% expressing a desire to continue using the platform. Adoption patterns\nshowed systematic scaling from 4% engagement in month 1 to 83% peak usage by\nmonth 6, stabilizing at 60% active engagement. Top adopters achieved a 61%\nincrease in code volume pushed to production, contributing to approximately 30\nto 40% of code shipped to production through this tool, accounting for an\noverall 28% increase in code shipment volume.\n  Unlike controlled benchmark evaluations, our longitudinal analysis provides\nempirical evidence from production environments, revealing both the\ntransformative potential and practical deployment challenges of integrating AI\ninto enterprise software development workflows."}
{"id": "2509.19947", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.19947", "abs": "https://arxiv.org/abs/2509.19947", "authors": ["Zhixiao Wu", "Yao Lu", "Jie Wen", "Hao Sun", "Qi Zhou", "Guangming Lu"], "title": "A Set of Generalized Components to Achieve Effective Poison-only Clean-label Backdoor Attacks with Collaborative Sample Selection and Triggers", "comment": "31 pages, 16 figures, accepted in Neurips 2025", "summary": "Poison-only Clean-label Backdoor Attacks aim to covertly inject\nattacker-desired behavior into DNNs by merely poisoning the dataset without\nchanging the labels. To effectively implant a backdoor, multiple\n\\textbf{triggers} are proposed for various attack requirements of Attack\nSuccess Rate (ASR) and stealthiness. Additionally, sample selection enhances\nclean-label backdoor attacks' ASR by meticulously selecting ``hard'' samples\ninstead of random samples to poison. Current methods 1) usually handle the\nsample selection and triggers in isolation, leading to severely limited\nimprovements on both ASR and stealthiness. Consequently, attacks exhibit\nunsatisfactory performance on evaluation metrics when converted to PCBAs via a\nmere stacking of methods. Therefore, we seek to explore the bidirectional\ncollaborative relations between the sample selection and triggers to address\nthe above dilemma. 2) Since the strong specificity within triggers, the simple\ncombination of sample selection and triggers fails to substantially enhance\nboth evaluation metrics, with generalization preserved among various attacks.\nTherefore, we seek to propose a set of components to significantly improve both\nstealthiness and ASR based on the commonalities of attacks. Specifically,\nComponent A ascertains two critical selection factors, and then makes them an\nappropriate combination based on the trigger scale to select more reasonable\n``hard'' samples for improving ASR. Component B is proposed to select samples\nwith similarities to relevant trigger implanted samples to promote\nstealthiness. Component C reassigns trigger poisoning intensity on RGB colors\nthrough distinct sensitivity of the human visual system to RGB for higher ASR,\nwith stealthiness ensured by sample selection, including Component B.\nFurthermore, all components can be strategically integrated into diverse PCBAs."}
{"id": "2509.20166", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.20166", "abs": "https://arxiv.org/abs/2509.20166", "authors": ["Lauren Deason", "Adam Bali", "Ciprian Bejean", "Diana Bolocan", "James Crnkovich", "Ioana Croitoru", "Krishna Durai", "Chase Midler", "Calin Miron", "David Molnar", "Brad Moon", "Bruno Ostarcevic", "Alberto Peltea", "Matt Rosenberg", "Catalin Sandu", "Arthur Saputkin", "Sagar Shah", "Daniel Stan", "Ernest Szocs", "Shengye Wan", "Spencer Whitman", "Sven Krasser", "Joshua Saxe"], "title": "CyberSOCEval: Benchmarking LLMs Capabilities for Malware Analysis and Threat Intelligence Reasoning", "comment": null, "summary": "Today's cyber defenders are overwhelmed by a deluge of security alerts,\nthreat intelligence signals, and shifting business context, creating an urgent\nneed for AI systems to enhance operational security work. While Large Language\nModels (LLMs) have the potential to automate and scale Security Operations\nCenter (SOC) operations, existing evaluations do not fully assess the scenarios\nmost relevant to real-world defenders. This lack of informed evaluation impacts\nboth AI developers and those applying LLMs to SOC automation. Without clear\ninsight into LLM performance in real-world security scenarios, developers lack\na north star for development, and users cannot reliably select the most\neffective models. Meanwhile, malicious actors are using AI to scale cyber\nattacks, highlighting the need for open source benchmarks to drive adoption and\ncommunity-driven improvement among defenders and model developers. To address\nthis, we introduce CyberSOCEval, a new suite of open source benchmarks within\nCyberSecEval 4. CyberSOCEval includes benchmarks tailored to evaluate LLMs in\ntwo tasks: Malware Analysis and Threat Intelligence Reasoning--core defensive\ndomains with inadequate coverage in current benchmarks. Our evaluations show\nthat larger, more modern LLMs tend to perform better, confirming the training\nscaling laws paradigm. We also find that reasoning models leveraging test time\nscaling do not achieve the same boost as in coding and math, suggesting these\nmodels have not been trained to reason about cybersecurity analysis, and\npointing to a key opportunity for improvement. Finally, current LLMs are far\nfrom saturating our evaluations, showing that CyberSOCEval presents a\nsignificant challenge for AI developers to improve cyber defense capabilities."}
{"id": "2509.20190", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.20190", "abs": "https://arxiv.org/abs/2509.20190", "authors": ["Tanmay Khule", "Stefan Marksteiner", "Jose Alguindigue", "Hannes Fuchs", "Sebastian Fischmeister", "Apurva Narayan"], "title": "STAF: Leveraging LLMs for Automated Attack Tree-Based Security Test Generation", "comment": "18 pages, 2 figures, accepted for 23rd escar Europe (Nov 05-06, 2025,\n  Frankfurt, Germany)", "summary": "In modern automotive development, security testing is critical for\nsafeguarding systems against increasingly advanced threats. Attack trees are\nwidely used to systematically represent potential attack vectors, but\ngenerating comprehensive test cases from these trees remains a labor-intensive,\nerror-prone task that has seen limited automation in the context of testing\nvehicular systems. This paper introduces STAF (Security Test Automation\nFramework), a novel approach to automating security test case generation.\nLeveraging Large Language Models (LLMs) and a four-step self-corrective\nRetrieval-Augmented Generation (RAG) framework, STAF automates the generation\nof executable security test cases from attack trees, providing an end-to-end\nsolution that encompasses the entire attack surface. We particularly show the\nelements and processes needed to provide an LLM to actually produce sensible\nand executable automotive security test suites, along with the integration with\nan automated testing framework. We further compare our tailored approach with\ngeneral purpose (vanilla) LLMs and the performance of different LLMs (namely\nGPT-4.1 and DeepSeek) using our approach. We also demonstrate the method of our\noperation step-by-step in a concrete case study. Our results show significant\nimprovements in efficiency, accuracy, scalability, and easy integration in any\nworkflow, marking a substantial advancement in automating automotive security\ntesting methodologies. Using TARAs as an input for verfication tests, we create\nsynergies by connecting two vital elements of a secure automotive development\nprocess."}
{"id": "2509.20215", "categories": ["cs.SE", "cs.AI", "cs.AR"], "pdf": "https://arxiv.org/pdf/2509.20215", "abs": "https://arxiv.org/abs/2509.20215", "authors": ["Guang Yang", "Wei Zheng", "Xiang Chen", "Yifan Sun", "Fengji Zhang", "Terry Yue Zhuo"], "title": "The Cream Rises to the Top: Efficient Reranking Method for Verilog Code Generation", "comment": "Under review ICASSP 2026", "summary": "LLMs face significant challenges in Verilog generation due to limited\ndomain-specific knowledge. While sampling techniques improve pass@k metrics,\nhardware engineers need one trustworthy solution rather than uncertain\ncandidates. To bridge this gap, we formulate it as a semantic alignment problem\nbetween requirements and Verilog implementations, and propose VCD-RNK, a\ndiscriminator model tailored for efficient Verilog code reranking.\nSpecifically, VCD-RNKincorporates Verilog-specific reasoning by distilling\nexpert knowledge across three dimensions: code semantic analysis, test case\ngeneration, and functional correctness assessment. By explicitly simulating the\nabove reasoning processes during inference, VCD-RNK effectively avoids\ncomputationally intensive test execution in existing methods."}
{"id": "2509.20277", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.20277", "abs": "https://arxiv.org/abs/2509.20277", "authors": ["Xiaofan Li", "Xing Gao"], "title": "Investigating Security Implications of Automatically Generated Code on the Software Supply Chain", "comment": null, "summary": "In recent years, various software supply chain (SSC) attacks have posed\nsignificant risks to the global community. Severe consequences may arise if\ndevelopers integrate insecure code snippets that are vulnerable to SSC attacks\ninto their products. Particularly, code generation techniques, such as large\nlanguage models (LLMs), have been widely utilized in the developer community.\nHowever, LLMs are known to suffer from inherent issues when generating code,\nincluding fabrication, misinformation, and reliance on outdated training data,\nall of which can result in serious software supply chain threats. In this\npaper, we investigate the security threats to the SSC that arise from these\ninherent issues. We examine three categories of threats, including eleven\npotential SSC-related threats, related to external components in source code,\nand continuous integration configuration files. We find some threats in\nLLM-generated code could enable attackers to hijack software and workflows,\nwhile some others might cause potential hidden threats that compromise the\nsecurity of the software over time. To understand these security impacts and\nseverity, we design a tool, SSCGuard, to generate 439,138 prompts based on\nSSC-related questions collected online, and analyze the responses of four\npopular LLMs from GPT and Llama. Our results show that all identified\nSSC-related threats persistently exist. To mitigate these risks, we propose a\nnovel prompt-based defense mechanism, namely Chain-of-Confirmation, to reduce\nfabrication, and a middleware-based defense that informs users of various SSC\nthreats."}
{"id": "2509.20324", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.20324", "abs": "https://arxiv.org/abs/2509.20324", "authors": ["Atousa Arzanipour", "Rouzbeh Behnia", "Reza Ebrahimi", "Kaushik Dutta"], "title": "RAG Security and Privacy: Formalizing the Threat Model and Attack Surface", "comment": "Accepted at the 5th ICDM Workshop on September 20, 2025", "summary": "Retrieval-Augmented Generation (RAG) is an emerging approach in natural\nlanguage processing that combines large language models (LLMs) with external\ndocument retrieval to produce more accurate and grounded responses. While RAG\nhas shown strong potential in reducing hallucinations and improving factual\nconsistency, it also introduces new privacy and security challenges that differ\nfrom those faced by traditional LLMs. Existing research has demonstrated that\nLLMs can leak sensitive information through training data memorization or\nadversarial prompts, and RAG systems inherit many of these vulnerabilities. At\nthe same time, reliance of RAG on an external knowledge base opens new attack\nsurfaces, including the potential for leaking information about the presence or\ncontent of retrieved documents, or for injecting malicious content to\nmanipulate model behavior. Despite these risks, there is currently no formal\nframework that defines the threat landscape for RAG systems. In this paper, we\naddress a critical gap in the literature by proposing, to the best of our\nknowledge, the first formal threat model for retrieval-RAG systems. We\nintroduce a structured taxonomy of adversary types based on their access to\nmodel components and data, and we formally define key threat vectors such as\ndocument-level membership inference and data poisoning, which pose serious\nprivacy and integrity risks in real-world deployments. By establishing formal\ndefinitions and attack models, our work lays the foundation for a more rigorous\nand principled understanding of privacy and security in RAG systems."}
