<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 18]
- [cs.CR](#cs.CR) [Total: 24]
- [cs.AI](#cs.AI) [Total: 52]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [From Craft to Constitution: A Governance-First Paradigm for Principled Agent Engineering](https://arxiv.org/abs/2510.13857)
*Qiang Xu,Xiangyu Wen,Changran Xu,Zeju Li,Jianyuan Zhong*

Main category: cs.SE

TL;DR: 论文提出了ArbiterOS架构，通过治理优先的范式来解决LLM代理在关键任务应用中的脆弱性和不可预测性问题。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型开启了"代理时代"，但原型到生产的过渡面临"工艺危机"，导致代理在关键任务应用中脆弱、不可预测且不可信。这种危机源于用传统软件工程的确定性思维来命令概率性处理器的根本范式不匹配。

Method: 引入治理优先的代理工程原则，体现在名为ArbiterOS的正式架构中。

Result: 提出了解决代理工程危机的理论框架和架构方案。

Conclusion: 需要通过治理优先的范式转变来解决LLM代理工程中的根本性范式不匹配问题。

Abstract: The advent of powerful Large Language Models (LLMs) has ushered in an ``Age
of the Agent,'' enabling autonomous systems to tackle complex goals. However,
the transition from prototype to production is hindered by a pervasive ``crisis
of craft,'' resulting in agents that are brittle, unpredictable, and ultimately
untrustworthy in mission-critical applications. This paper argues this crisis
stems from a fundamental paradigm mismatch -- attempting to command inherently
probabilistic processors with the deterministic mental models of traditional
software engineering. To solve this crisis, we introduce a governance-first
paradigm for principled agent engineering, embodied in a formal architecture we
call ArbiterOS.

</details>


### [2] [Benchmarking Correctness and Security in Multi-Turn Code Generation](https://arxiv.org/abs/2510.13859)
*Ruchit Rawal,Jeffrey Yang Fan Chiang,Chihao Shen,Jeffery Siyuan Tian,Aastha Mahajan,Tom Goldstein,Yizheng Chen*

Main category: cs.SE

TL;DR: MT-Sec是首个系统评估多轮编码场景中正确性和安全性的基准，发现从单轮到多轮设置中"正确且安全"的输出下降20-27%，即使是最先进模型也表现不佳。


<details>
  <summary>Details</summary>
Motivation: 现有基准通常局限于单轮任务，不能反映现实世界开发的迭代性质，需要评估多轮编码场景中的正确性和安全性。

Method: 使用合成数据管道将现有单轮任务转换为语义对齐的多轮交互序列，重用原始测试套件，同时建模真实世界编码过程的复杂性。

Result: 评估32个开源和闭源模型及三种代理框架，发现多轮设置下正确且安全的输出显著下降；在代码差异生成任务中表现更差；代理框架在多轮评估中效果不如单轮。

Conclusion: 需要能够联合评估多轮真实世界编码工作流中正确性和安全性的基准。

Abstract: AI coding assistants powered by large language models (LLMs) have transformed
software development, significantly boosting productivity. While existing
benchmarks evaluate the correctness and security of LLM-generated code, they
are typically limited to single-turn tasks that do not reflect the iterative
nature of real-world development. We introduce MT-Sec, the first benchmark to
systematically evaluate both correctness and security in multi-turn coding
scenarios. We construct this using a synthetic data pipeline that transforms
existing single-turn tasks into semantically aligned multi-turn interaction
sequences, allowing reuse of original test suites while modeling the complexity
of real-world coding processes. We evaluate 32 open- and closed-source models,
and three agent-scaffolding on MT-Sec and observe a consistent 20-27% drop in
"correct and secure" outputs from single-turn to multi-turn settings -- even
among state-of-the-art models. Beyond full-program generation, we also evaluate
models on multi-turn code-diff generation -- an unexplored yet practically
relevant setting -- and find that models perform worse here, with increased
rates of functionally incorrect and insecure outputs. Finally, we find that
while agent scaffoldings boost single-turn code generation performance, they
are not quite as effective in multi-turn evaluations. Together, these findings
highlight the need for benchmarks that jointly evaluate correctness and
security in multi-turn, real-world coding workflows.

</details>


### [3] [A11YN: aligning LLMs for accessible web UI code generation](https://arxiv.org/abs/2510.13914)
*Janghan Yoon,Jaegwan Cho,Junhyeok Kim,Jiwan Chung,Jaehyun Jeon,Youngjae Yu*

Main category: cs.SE

TL;DR: A11yn是首个将代码生成LLM对齐以可靠生成符合可访问性标准的网页UI的方法，通过优化基于WCAG违规惩罚的奖励函数，显著降低了不可访问率60%。


<details>
  <summary>Details</summary>
Motivation: 现有LLM在生成网页界面时经常复制训练数据中的可访问性缺陷，导致生成的界面排斥具有多样化需求和背景的用户。

Method: A11yn优化了一个新颖的奖励函数，该函数根据可访问性测试引擎识别的违规严重程度对WCAG违规进行惩罚，并使用UIReq-6.8K数据集进行训练。

Result: 实证结果显示，A11yn显著优于强基线，在保持生成UI语义保真度和视觉质量的同时，将不可访问率比基础模型降低了60%。

Conclusion: 研究表明可访问性可以在LLM中系统性优化，证明了为可访问性对齐代码生成的可行性。

Abstract: Large language models (LLMs) have recently demonstrated strong capabilities
in generating functional and aesthetic web interfaces directly from
instructions. However, these models often replicate accessibility flaws from
their training data, resulting in interfaces that exclude users with diverse
needs and contexts. To address this gap, we introduce A11yn, the first method
that aligns code-generating LLMs to reliably produce accessibility-compliant
web UIs. A11yn optimizes a novel reward function that penalizes violations of
the Web Content Accessibility Guidelines (WCAG), with penalties scaled to the
severity of each violation as identified by an accessibility testing engine. To
support training, we construct UIReq-6.8K, a dataset of 6,800 diverse
instructions for web UI generation. For evaluation, we introduce RealUIReq-300,
a benchmark of 300 real-world web UI requests grounded and manually curated
from public web pages, spanning a broad range of use cases. Empirical results
show that A11yn significantly outperforms strong baselines, lowering the
Inaccessibility Rate by 60% over the base model while preserving semantic
fidelity and visual quality of generated UIs. These findings demonstrate that
accessibility can be systematically optimized within LLMs, showing the
feasibility of aligning code generation for accessibility.

</details>


### [4] [Signature in Code Backdoor Detection, how far are we?](https://arxiv.org/abs/2510.13992)
*Quoc Hung Le,Thanh Le-Cong,Bach Le,Bowen Xu*

Main category: cs.SE

TL;DR: 重新评估了基于谱签名的防御方法在代码模型后门攻击中的适用性，发现传统设置效果不佳，提出了新的代理指标来更准确评估防御性能。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型在软件开发中的广泛应用，后门攻击威胁日益严重。谱签名防御方法在代码模型中的有效性尚未得到充分验证，需要系统评估其适用性。

Method: 系统评估谱签名防御在不同攻击场景和防御配置下的有效性，分析关键因素设置的影响，探索新的代理指标。

Result: 发现代码后门检测中广泛使用的谱签名设置通常不是最优的，找到了能更准确估计谱签名实际性能的新代理指标。

Conclusion: 谱签名防御在代码模型后门检测中需要优化配置，新提出的代理指标能够在不重新训练模型的情况下更准确地评估防御效果。

Abstract: As Large Language Models (LLMs) become increasingly integrated into software
development workflows, they also become prime targets for adversarial attacks.
Among these, backdoor attacks are a significant threat, allowing attackers to
manipulate model outputs through hidden triggers embedded in training data.
Detecting such backdoors remains a challenge, and one promising approach is the
use of Spectral Signature defense methods that identify poisoned data by
analyzing feature representations through eigenvectors. While some prior works
have explored Spectral Signatures for backdoor detection in neural networks,
recent studies suggest that these methods may not be optimally effective for
code models. In this paper, we revisit the applicability of Spectral
Signature-based defenses in the context of backdoor attacks on code models. We
systematically evaluate their effectiveness under various attack scenarios and
defense configurations, analyzing their strengths and limitations. We found
that the widely used setting of Spectral Signature in code backdoor detection
is often suboptimal. Hence, we explored the impact of different settings of the
key factors. We discovered a new proxy metric that can more accurately estimate
the actual performance of Spectral Signature without model retraining after the
defense.

</details>


### [5] [One Bug, Hundreds Behind: LLMs for Large-Scale Bug Discovery](https://arxiv.org/abs/2510.14036)
*Qiushi Wu,Yue Xiao,Dhilung Kirat,Kevin Eykholt,Jiyong Jang,Douglas Lee Schales*

Main category: cs.SE

TL;DR: BugStone是一个基于LLVM和大型语言模型的程序分析系统，能够利用已修复的bug实例识别代码中重复出现的错误模式，在Linux内核中发现了超过22K个潜在问题，验证了246个有效bug。


<details>
  <summary>Details</summary>
Motivation: 大型程序中的重复模式bug（RPBs）广泛存在且严重影响软件安全，但逐个发现和修复这些重复bug实例非常耗时费力。

Method: 利用LLVM和大型语言模型，通过分析已修复的bug实例来识别一致的错误模式（如特定API误用），然后在整个程序中搜索相似模式来发现潜在漏洞。

Result: 从135个独特RPBs出发，在Linux内核中识别了超过22K个潜在问题，手动验证400个发现中246个有效；在1.9K安全bug数据集中识别80个重复模式和850个对应修复，达到92.2%精度和79.1%配对准确率。

Conclusion: BugStone系统能够有效识别和定位程序中的重复模式bug，显著提高bug发现效率，为软件安全提供有力支持。

Abstract: Fixing bugs in large programs is a challenging task that demands substantial
time and effort. Once a bug is found, it is reported to the project
maintainers, who work with the reporter to fix it and eventually close the
issue. However, across the program, there are often similar code segments,
which may also contain the bug, but were missed during discovery. Finding and
fixing each recurring bug instance individually is labor intensive. Even more
concerning, bug reports can inadvertently widen the attack surface as they
provide attackers with an exploitable pattern that may be unresolved in other
parts of the program.
  In this paper, we explore these Recurring Pattern Bugs (RPBs) that appear
repeatedly across various code segments of a program or even in different
programs, stemming from a same root cause, but are unresolved. Our
investigation reveals that RPBs are widespread and can significantly compromise
the security of software programs. This paper introduces BugStone, a program
analysis system empowered by LLVM and a Large Language Model (LLM). The key
observation is that many RPBs have one patched instance, which can be leveraged
to identify a consistent error pattern, such as a specific API misuse. By
examining the entire program for this pattern, it is possible to identify
similar sections of code that may be vulnerable. Starting with 135 unique RPBs,
BugStone identified more than 22K new potential issues in the Linux kernel.
Manual analysis of 400 of these findings confirmed that 246 were valid. We also
created a dataset from over 1.9K security bugs reported by 23 recent top-tier
conference works. We manually annotate the dataset, identify 80 recurring
patterns and 850 corresponding fixes. Even with a cost-efficient model choice,
BugStone achieved 92.2% precision and 79.1% pairwise accuracy on the dataset.

</details>


### [6] [David vs. Goliath: A comparative study of different-sized LLMs for code generation in the domain of automotive scenario generation](https://arxiv.org/abs/2510.14115)
*Philipp Bauerfeind,Amir Salarpour,David Fernandez,Pedram MohajerAnsari,Johannes Reschke,Mert D. Pesé*

Main category: cs.SE

TL;DR: NL2Scenic是一个用于评估自然语言到Scenic代码生成的开放数据集和框架，包含146个NL/Scenic对和30个测试案例，评估了13个模型并提出了EDIT-COMP作为鲁棒评估指标。


<details>
  <summary>Details</summary>
Motivation: 解决现有NL-to-Scenic生成中数据稀缺、可复现性差和评估指标不一致的问题，为自动驾驶场景编程提供标准化评估基准。

Method: 构建NL2Scenic数据集，开发Example Retriever和14种提示变体，使用文本指标和执行指标评估13个模型，并与专家研究对比。

Result: EDIT-SIM与人类判断相关性最好，GPT-4o表现最佳，Qwen2.5Coder-14B达到其88%的专家分数，检索增强提示能提升小模型性能。

Conclusion: NL2Scenic和EDIT-COMP为Scenic代码生成提供了标准化评估基础，表明中等规模开源模型是自动驾驶场景编程的实用且经济的选择。

Abstract: Scenario simulation is central to testing autonomous driving systems. Scenic,
a domain-specific language (DSL) for CARLA, enables precise and reproducible
scenarios, but NL-to-Scenic generation with large language models (LLMs)
suffers from scarce data, limited reproducibility, and inconsistent metrics. We
introduce NL2Scenic, an open dataset and framework with 146 NL/Scenic pairs, a
difficulty-stratified 30-case test split, an Example Retriever, and 14
prompting variants (ZS, FS, CoT, SP, MoT). We evaluate 13 models: four
proprietary (GPT-4o, GPT-5, Claude-Sonnet-4, Gemini-2.5-pro) and nine
open-source code models (Qwen2.5Coder 0.5B-32B; CodeLlama 7B/13B/34B), using
text metrics (BLEU, ChrF, EDIT-SIM, CrystalBLEU) and execution metrics
(compilation and generation), and compare them with an expert study (n=11).
EDIT-SIM correlates best with human judgments; we also propose EDIT-COMP (F1 of
EDIT-SIM and compilation) as a robust dataset-level proxy that improves ranking
fidelity. GPT-4o performs best overall, while Qwen2.5Coder-14B reaches about 88
percent of its expert score on local hardware. Retrieval-augmented prompting,
Few-Shot with Example Retriever (FSER), consistently boosts smaller models, and
scaling shows diminishing returns beyond mid-size, with Qwen2.5Coder
outperforming CodeLlama at comparable scales. NL2Scenic and EDIT-COMP offer a
standardized, reproducible basis for evaluating Scenic code generation and
indicate that mid-size open-source models are practical, cost-effective options
for autonomous-driving scenario programming.

</details>


### [7] [Caruca: Effective and Efficient Specification Mining for Opaque Software Components](https://arxiv.org/abs/2510.14279)
*Evangelos Lamprou,Seong-Heon Jung,Mayank Keoliya,Lukas Lazarek,Konstantinos Kallas,Michael Greenberg,Nikos Vasilakis*

Main category: cs.SE

TL;DR: Caruca是一个自动为不透明命令挖掘规范的系统，通过大语言模型将文档转换为结构化调用语法，探索有效命令调用和执行环境，提取并行性和文件系统前后条件等关键属性。


<details>
  <summary>Details</summary>
Motivation: 现有系统需要手动创建命令规范，这个过程耗时、易错且限制了系统的实用性，因此需要自动化解决方案。

Method: 使用大语言模型将命令文档转换为结构化调用语法，探索语法有效的命令调用和执行环境，通过系统调用和文件系统拦截提取命令属性。

Result: 在60个GNU Coreutils、POSIX和第三方命令上测试，Caruca为除一个案例外的所有情况生成了正确规范，完全消除了手动工作，目前为最先进的静态分析工具提供完整规范。

Conclusion: Caruca成功实现了命令规范的自动挖掘，显著提高了规范依赖系统的实用性，证明了自动化规范生成方法的有效性。

Abstract: A wealth of state-of-the-art systems demonstrate impressive improvements in
performance, security, and reliability on programs composed of opaque
components, such as Unix shell commands. To reason about commands, these
systems require partial specifications. However, creating such specifications
is a manual, laborious, and error-prone process, limiting the practicality of
these systems. This paper presents Caruca, a system for automatic specification
mining for opaque commands. To overcome the challenge of language diversity
across commands, Caruca first instruments a large language model to translate a
command's user-facing documentation into a structured invocation syntax. Using
this representation, Caruca explores the space of syntactically valid command
invocations and execution environments. Caruca concretely executes each
command-environment pair, interposing at the system-call and filesystem level
to extract key command properties such as parallelizability and filesystem pre-
and post-conditions. These properties can be exported in multiple specification
formats and are immediately usable by existing systems. Applying Caruca across
60 GNU Coreutils, POSIX, and third-party commands across several
specification-dependent systems shows that Caruca generates correct
specifications for all but one case, completely eliminating manual effort from
the process and currently powering the full specifications for a
state-of-the-art static analysis tool.

</details>


### [8] [A Hybrid, Knowledge-Guided Evolutionary Framework for Personalized Compiler Auto-Tuning](https://arxiv.org/abs/2510.14292)
*Haolin Pan,Hongbin Zhang,Mingjie Xing,Yanjun Wu*

Main category: cs.SE

TL;DR: 提出了一种混合知识引导的进化框架，通过离线构建编译知识库和在线使用知识增强的遗传算法，为特定程序自动优化编译器pass序列，相比-Oz基准平均额外减少11.0%的LLVM IR指令。


<details>
  <summary>Details</summary>
Motivation: 传统编译器优化标志（如-O3、-Oz）采用一刀切方法，无法充分发挥程序的性能潜力，而寻找最优pass序列是一个NP难问题。

Method: 1）离线阶段构建包含pass行为向量、pass群组、协同pass图和原型pass序列的编译知识库；2）在线阶段使用知识增强的遗传算法进行语义感知的重组和定向修复突变。

Result: 在7个公开数据集上，相比高度优化的opt -Oz基准，平均额外减少11.0%的LLVM IR指令。

Conclusion: 该框架在发现个性化高性能优化序列方面展现出最先进的能力，证明了知识引导进化方法在编译器自动调优中的有效性。

Abstract: Compiler pass auto-tuning is critical for enhancing software performance, yet
finding the optimal pass sequence for a specific program is an NP-hard problem.
Traditional, general-purpose optimization flags like -O3 and -Oz adopt a
one-size-fits-all approach, often failing to unlock a program's full
performance potential. To address this challenge, we propose a novel Hybrid,
Knowledge-Guided Evolutionary Framework. This framework intelligently guides
online, personalized optimization using knowledge extracted from a large-scale
offline analysis phase. During the offline stage, we construct a comprehensive
compilation knowledge base composed of four key components: (1) Pass Behavioral
Vectors to quantitatively capture the effectiveness of each optimization; (2)
Pass Groups derived from clustering these vectors based on behavior similarity;
(3) a Synergy Pass Graph to model beneficial sequential interactions; and (4) a
library of Prototype Pass Sequences evolved for distinct program types. In the
online stage, a bespoke genetic algorithm leverages this rich knowledge base
through specially designed, knowledge-infused genetic operators. These
operators transform the search by performing semantically-aware recombination
and targeted, restorative mutations. On a suite of seven public datasets, our
framework achieves an average of 11.0% additional LLVM IR instruction reduction
over the highly-optimized opt -Oz baseline, demonstrating its state-of-the-art
capability in discovering personalized, high-performance optimization
sequences.

</details>


### [9] [A Systematic Study of Time Limit Exceeded Errors in Online Programming Assignments](https://arxiv.org/abs/2510.14339)
*Jialu Zhang,Jialiang Gu,Wangmeiyu Zhang,José Pablo Cambronero,John Kolesar,Ruzica Piskac,Daming Li,Hanyuan Shi*

Main category: cs.SE

TL;DR: 本文提出了首个针对在线编程中TLE错误的大规模实证研究，开发了首个自动化修复工具Nettle和评估框架Nettle-Eval，显著提高了TLE错误的修复成功率。


<details>
  <summary>Details</summary>
Motivation: 在线编程平台上的TLE错误难以解决，错误信息缺乏诊断价值，平台支持有限，现有调试工具帮助不大，导致许多用户在重复TLE失败后放弃提交。

Method: 手动分析1000个Codeforces的TLE提交，分类根本原因；开发Nettle工具，结合LLM、编译器反馈和测试用例生成小型正确代码编辑；创建Nettle-Eval评估框架。

Result: Nettle在1000个真实案例中达到98.5%的修复率，远超最强LLM基线，所有修复都通过了Nettle-Eval和平台官方检查器。

Conclusion: TLE错误不仅源于算法效率问题，还包括无限循环、数据结构使用不当和I/O效率低下；Nettle是首个专门针对TLE错误的自动化修复工具，证明了其有效性和可靠性。

Abstract: Online programming platforms such as Codeforces and LeetCode attract millions
of users seeking to learn to program or refine their skills for industry
interviews. A major challenge for these users is the Time Limit Exceeded (TLE)
error, triggered when a program exceeds the execution time bound. Although
designed as a performance safeguard, TLE errors are difficult to resolve: error
messages provide no diagnostic insight, platform support is minimal, and
existing debugging tools offer little help. As a result, many users abandon
their submissions after repeated TLE failures.
  This paper presents the first large-scale empirical study of TLE errors in
online programming. We manually analyzed 1000 Codeforces submissions with TLE
errors, classified their root causes, and traced how users attempted to fix
them. Our analysis shows that TLE errors often arise not only from inefficient
algorithms but also from infinite loops, improper data structure use, and
inefficient I/O, challenging the conventional view that TLEs are purely
performance issues.
  Guided by these findings, we introduce Nettle, the first automated repair
tool specifically designed for TLE errors, and Nettle-Eval, the first framework
for evaluating TLE repairs. Integrating LLMs with targeted automated feedback
generated by the compiler and test cases, Nettle produces small, correct code
edits that eliminate TLEs while preserving functionality. Evaluated on the same
1000 real-world cases, Nettle achieves a 98.5% fix rate, far exceeding the
strongest LLM baseline, and all of its repairs pass both Nettle-Eval and the
platform's official checker, confirming the reliability of our framework.

</details>


### [10] [PathFix: Automated Program Repair with Expected Path](https://arxiv.org/abs/2510.14341)
*Xu He,Shu Wang,Kun Sun*

Main category: cs.SE

TL;DR: PathFix是一种新的自动程序修复方法，利用从正确执行路径提取的路径敏感约束来生成修复补丁，通过分析预期路径和状态约束来解决现有APR方法生成过多候选补丁和过拟合的问题。


<details>
  <summary>Details</summary>
Motivation: 现有自动程序修复方法由于难以生成精确规范，面临生成过多合理补丁候选和过拟合部分测试用例的挑战。

Method: PathFix采用四步法：1）追踪错误路径；2）通过控制流图分析推导预期路径；3）沿预期路径求解状态约束生成补丁；4）验证补丁正确性。还集成大语言模型提升性能。

Result: 实验结果显示PathFix优于现有解决方案，特别是在处理循环和递归等复杂程序结构方面表现突出。

Conclusion: PathFix通过路径敏感约束分析有效解决了APR中的关键挑战，在复杂程序修复方面展现出优越性能。

Abstract: Automated program repair (APR) techniques are effective in fixing inevitable
defects in software, enhancing development efficiency and software robustness.
However, due to the difficulty of generating precise specifications, existing
APR methods face two main challenges: generating too many plausible patch
candidates and overfitting them to partial test cases. To tackle these
challenges, we introduce a new APR method named PathFix, which leverages
path-sensitive constraints extracted from correct execution paths to generate
patches for repairing buggy code. It is based on one observation: if a buggy
program is repairable, at least one expected path is supposed to replace the
fault path in the patched program. PathFix operates in four main steps. First,
it traces fault paths reaching the fault output in the buggy program. Second,
it derives expected paths by analyzing the desired correct output on the
control flow graph, where an expected path defines how a feasible patch leads
to the correct execution. Third, PathFix generates and evaluates patches by
solving state constraints along the expected path. Fourth, we validate the
correctness of the generated patch. To further enhance repair performance and
mitigate scalability issues introduced by path-sensitive analysis, we integrate
a large language model (LLM) into our framework. Experimental results show that
PathFix outperforms existing solutions, particularly in handling complex
program structures such as loops and recursion.

</details>


### [11] [Towards Automated Governance: A DSL for Human-Agent Collaboration in Software Projects](https://arxiv.org/abs/2510.14465)
*Adem Ait,Gwendal Jouneaux,Javier Luis Cánovas Izquierdo,Jordi Cabot*

Main category: cs.SE

TL;DR: 提出一种新的领域特定语言（DSL），用于定义和执行涉及多样化利益相关者（包括AI代理）的软件项目治理政策。


<details>
  <summary>Details</summary>
Motivation: 软件开发的利益相关者日益多样化，包括来自不同背景的人类贡献者和AI代理，在开源软件项目中尤其缺乏明确治理政策，这带来了独特的治理挑战。

Method: 设计一种领域特定语言（DSL），专门用于定义和执行丰富的治理政策，支持系统涉及多样化利益相关者（包括代理）的协作。

Result: 该DSL为实现更强大、适应性更强且最终自动化的治理提供了途径。

Conclusion: 这种DSL为软件项目（特别是开源项目）中更有效的协作铺平了道路。

Abstract: The stakeholders involved in software development are becoming increasingly
diverse, with both human contributors from varied backgrounds and AI-powered
agents collaborating together in the process. This situation presents unique
governance challenges, particularly in Open-Source Software (OSS) projects,
where explicit policies are often lacking or unclear. This paper presents the
vision and foundational concepts for a novel Domain-Specific Language (DSL)
designed to define and enforce rich governance policies in systems involving
diverse stakeholders, including agents. This DSL offers a pathway towards more
robust, adaptable, and ultimately automated governance, paving the way for more
effective collaboration in software projects, especially OSS ones.

</details>


### [12] [E2Edev: Benchmarking Large Language Models in End-to-End Software Development Task](https://arxiv.org/abs/2510.14509)
*Jingyao Liu,Chen Huang,Zhizhao Guan,Wenqiang Lei,Yang Deng*

Main category: cs.SE

TL;DR: E2EDev是一个端到端软件开发基准，包含细粒度需求、BDD测试场景和自动化测试管道，通过人机协同多智能体标注框架确保质量，评估显示现有E2ESD框架仍面临挑战。


<details>
  <summary>Details</summary>
Motivation: 解决端到端软件开发(E2ESD)中缺乏高质量基准的问题，减少标注工作量同时确保基准质量。

Method: 提出E2EDev基准，包含用户需求、BDD测试场景和Python步骤实现，使用人机协同多智能体标注框架(HITL-MAA)进行质量保证。

Result: 评估显示现有E2ESD框架和LLM骨干网络在解决这些任务时持续面临困难，突显了对更有效和成本效益的E2ESD解决方案的迫切需求。

Conclusion: E2EDev为E2ESD研究提供了有价值的基准，揭示了当前方法的局限性，并强调了开发更高效解决方案的重要性。

Abstract: E2EDev comprises (i) a fine-grained set of user requirements, (ii) {multiple
BDD test scenarios with corresponding Python step implementations for each
requirement}, and (iii) a fully automated testing pipeline built on the Behave
framework. To ensure its quality while reducing the annotation effort, E2EDev
leverages our proposed Human-in-the-Loop Multi-Agent Annotation Framework
(HITL-MAA). {By evaluating various E2ESD frameworks and LLM backbones with
E2EDev}, our analysis reveals a persistent struggle to effectively solve these
tasks, underscoring the critical need for more effective and cost-efficient
E2ESD solutions. Our codebase and benchmark are publicly available at
https://github.com/SCUNLP/E2EDev.

</details>


### [13] [Software Testing Education and Industry Needs - Report from the ENACTEST EU Project](https://arxiv.org/abs/2510.14625)
*Mehrdad Saadatmand,Abbas Khan,Beatriz Marin,Ana C. R Paiva,Nele Van Asch,Graham Moran,Felix Cammaerts,Monique Snoeck,Alexandra Mendes*

Main category: cs.SE

TL;DR: 本研究调查了工业界对软件测试能力的需求，识别了当前测试教育的知识缺口，并发现了学术文献中未涉及的能力和差距。


<details>
  <summary>Details</summary>
Motivation: 软件开发的不断演变要求测试人员持续适应新工具、实践并获取新技能，因此需要了解行业需求和现有教育之间的差距。

Method: 通过两次焦点小组会议和跨多个行业的专业人士访谈，结合小规模范围综述，采用主题定性分析方法。

Result: 研究发现专业培训方法、行业培训挑战、培训质量评估方式、学术教育与行业需求之间的知识缺口、未来测试教育趋势以及公司内部知识转移方法等方面的问题。范围综述确认了AI测试、安全测试和软技能等领域的知识缺口。

Conclusion: 软件测试教育需要更好地与行业需求对接，特别是在AI测试、安全测试和软技能等新兴领域，以缩小学术教育与实际工作需求之间的差距。

Abstract: The evolving landscape of software development demands that software testers
continuously adapt to new tools, practices, and acquire new skills. This study
investigates software testing competency needs in industry, identifies
knowledge gaps in current testing education, and highlights competencies and
gaps not addressed in academic literature. This is done by conducting two focus
group sessions and interviews with professionals across diverse domains,
including railway industry, healthcare, and software consulting and performing
a curated small-scale scoping review. The study instrument, co-designed by
members of the ENACTEST project consortium, was developed collaboratively and
refined through multiple iterations to ensure comprehensive coverage of
industry needs and educational gaps. In particular, by performing a thematic
qualitative analysis, we report our findings and observations regarding:
professional training methods, challenges in offering training in industry,
different ways of evaluating the quality of training, identified knowledge gaps
with respect to academic education and industry needs, future needs and trends
in testing education, and knowledge transfer methods within companies. Finally,
the scoping review results confirm knowledge gaps in areas such as AI testing,
security testing and soft skills.

</details>


### [14] [ATGen: Adversarial Reinforcement Learning for Test Case Generation](https://arxiv.org/abs/2510.14635)
*Qingyao Li,Xinyi Dai,Weiwen Liu,Xiangyang Li,Yasheng Wang,Ruiming Tang,Yong Yu,Weinan Zhang*

Main category: cs.SE

TL;DR: ATGen是一个通过对抗性强化学习训练测试用例生成的框架，通过测试生成器和对抗性代码生成器的动态对抗循环，打破静态数据集的固定难度限制，显著提升LLM生成代码的可靠性。


<details>
  <summary>Details</summary>
Motivation: 现有基于静态数据集的测试生成方法存在"固定难度天花板"，无法发现超出训练范围的新颖或更复杂bug，限制了LLM生成代码的可靠性提升。

Method: 采用对抗性强化学习框架，让测试生成器与对抗性代码生成器进行动态对抗，测试生成器通过RL优化同时最大化"输出准确性"和"攻击成功率"。

Result: ATGen显著优于现有最先进基线方法，既能作为更有效的Best-of-N推理过滤器，也能作为训练代码生成模型的更高质量奖励源。

Conclusion: ATGen建立了一种新的动态范式，通过持续对抗训练打破了静态训练的固定难度限制，显著提升了LLM生成代码的可靠性。

Abstract: Large Language Models (LLMs) excel at code generation, yet their outputs
often contain subtle bugs, for which effective test cases are a critical
bottleneck. Existing test generation methods, whether based on prompting or
supervised fine-tuning, rely on static datasets. This imposes a
``fixed-difficulty ceiling'', fundamentally limiting their ability to uncover
novel or more complex bugs beyond their training scope. To overcome this, we
introduce ATGen, a framework that trains a test case generator via adversarial
reinforcement learning. ATGen pits a test generator against an adversarial code
generator that continuously crafts harder bugs to evade the current policy.
This dynamic loop creates a curriculum of increasing difficulty challenging
current policy. The test generator is optimized via Reinforcement Learning (RL)
to jointly maximize ``Output Accuracy'' and ``Attack Success'', enabling it to
learn a progressively stronger policy that breaks the fixed-difficulty ceiling
of static training. Extensive experiments demonstrate that ATGen significantly
outperforms state-of-the-art baselines. We further validate its practical
utility, showing it serves as both a more effective filter for Best-of-N
inference and a higher-quality reward source for training code generation
models. Our work establishes a new, dynamic paradigm for improving the
reliability of LLM-generated code.

</details>


### [15] [Requirement Identification for Traffic Simulations in Driving Simulators](https://arxiv.org/abs/2510.14653)
*Sven Tarlowski,Lutz Eckstein*

Main category: cs.SE

TL;DR: 提出了一种系统识别交通仿真需求的方法论，通过分阶段子目标来确保交通仿真的真实性和有效性。


<details>
  <summary>Details</summary>
Motivation: 解决确保交通仿真真实性的挑战，提升实验结果的效度和参与者参与度。

Method: 采用结构化方法，基于各研究阶段的子目标，推导微观层面、智能体模型和视觉表示的具体技术需求。

Result: 建立了研究目标与交通仿真设计之间的清晰联系，支持稳健的汽车开发和测试。

Conclusion: 该方法论能够保持高保真度，增强实验结果的可靠性和参与者的沉浸感。

Abstract: This paper addresses the challenge of ensuring realistic traffic conditions
by proposing a methodology that systematically identifies traffic simulation
requirements. Using a structured approach based on sub-goals in each study
phase, specific technical needs are derived for microscopic levels, agent
models, and visual representation. The methodology aims to maintain a high
degree of fidelity, enhancing both the validity of experimental outcomes and
participant engagement. By providing a clear link between study objectives and
traffic simulation design, this approach supports robust automotive development
and testing.

</details>


### [16] [LLM Agents for Automated Web Vulnerability Reproduction: Are We There Yet?](https://arxiv.org/abs/2510.14700)
*Bin Liu,Yanjie Zhao,Guoai Xu,Haoyu Wang*

Main category: cs.SE

TL;DR: 本文首次对最先进的LLM代理在自动化Web漏洞复现方面进行全面评估，发现虽然LLM代理在简单库漏洞上表现良好，但在需要多组件环境的复杂服务漏洞上持续失败。


<details>
  <summary>Details</summary>
Motivation: LLM代理在软件工程和网络安全任务中表现出色，但自动化Web漏洞复现这一关键应用尚未充分探索。本文旨在系统评估LLM代理在真实Web漏洞复现场景中的能力。

Method: 系统评估了来自软件工程、网络安全和通用领域的20个LLM代理，涵盖16个维度，包括技术能力、环境适应性和用户体验因素。在3个代表性Web漏洞上进行评估，然后选择3个表现最佳的代理在包含80个真实CVE的基准数据集上进行深入评估。

Result: LLM代理在简单库漏洞上取得合理成功率，但在需要多组件环境的复杂服务漏洞上持续失败。复杂环境配置和认证障碍导致代理能执行利用代码但无法触发实际漏洞。在输入指导不完整时性能下降超过33%。

Conclusion: 当前LLM代理能力与可靠自动化漏洞复现需求之间存在显著差距，需要在环境适应和自主问题解决能力方面取得进展。

Abstract: Large language model (LLM) agents have demonstrated remarkable capabilities
in software engineering and cybersecurity tasks, including code generation,
vulnerability discovery, and automated testing. One critical but underexplored
application is automated web vulnerability reproduction, which transforms
vulnerability reports into working exploits. Although recent advances suggest
promising potential, challenges remain in applying LLM agents to real-world web
vulnerability reproduction scenarios. In this paper, we present the first
comprehensive evaluation of state-of-the-art LLM agents for automated web
vulnerability reproduction. We systematically assess 20 agents from software
engineering, cybersecurity, and general domains across 16 dimensions, including
technical capabilities, environment adaptability, and user experience factors,
on 3 representative web vulnerabilities. Based on the results, we select three
top-performing agents (OpenHands, SWE-agent, and CAI) for in-depth evaluation
on our benchmark dataset of 80 real-world CVEs spanning 7 vulnerability types
and 6 web technologies. Our results reveal that while LLM agents achieve
reasonable success on simple library-based vulnerabilities, they consistently
fail on complex service-based vulnerabilities requiring multi-component
environments. Complex environment configurations and authentication barriers
create a gap where agents can execute exploit code but fail to trigger actual
vulnerabilities. We observe high sensitivity to input guidance, with
performance degrading by over 33% under incomplete authentication information.
Our findings highlight the significant gap between current LLM agent
capabilities and the demands of reliable automated vulnerability reproduction,
emphasizing the need for advances in environmental adaptation and autonomous
problem-solving capabilities.

</details>


### [17] [Leveraging Code Cohesion Analysis to Identify Source Code Supply Chain Attacks](https://arxiv.org/abs/2510.14778)
*Maor Reuben,Ido Mendel,Or Feldman,Moshe Kravchik,Mordehai Guri,Rami Puzis*

Main category: cs.SE

TL;DR: 提出一种基于名称预测的内聚性（NPC）度量的无监督方法，通过量化源代码中的内聚性破坏来检测恶意代码注入。


<details>
  <summary>Details</summary>
Motivation: 供应链攻击通过向合法项目中注入恶意代码严重威胁软件安全，这类攻击虽然罕见但破坏性极大，需要自动化工具来检测。

Method: 使用基于名称预测的内聚性（NPC）度量，分析恶意代码引入时函数内聚性的变化，并与自然内聚性波动进行比较。

Result: 对369个开源C++仓库的54,707个函数分析显示，代码注入会降低内聚性并使命名模式趋向更短、描述性更差的名称。在极端不平衡测试集下，NPC方法在1:1,000比例下达到36.41%的Precision@100，在1:10,000比例下达到12.47%。

Conclusion: 自动化内聚性测量，特别是基于名称预测的内聚性度量，有助于识别供应链攻击，提高源代码完整性。

Abstract: Supply chain attacks significantly threaten software security with malicious
code injections within legitimate projects. Such attacks are very rare but may
have a devastating impact. Detecting spurious code injections using automated
tools is further complicated as it often requires deciphering the intention of
both the inserted code and its context. In this study, we propose an
unsupervised approach for highlighting spurious code injections by quantifying
cohesion disruptions in the source code. Using a name-prediction-based cohesion
(NPC) metric, we analyze how function cohesion changes when malicious code is
introduced compared to natural cohesion fluctuations. An analysis of 54,707
functions over 369 open-source C++ repositories reveals that code injection
reduces cohesion and shifts naming patterns toward shorter, less descriptive
names compared to genuine function updates. Considering the sporadic nature of
real supply-chain attacks, we evaluate the proposed method with extreme
test-set imbalance and show that monitoring high-cohesion functions with NPC
can effectively detect functions with injected code, achieving a Precision@100
of 36.41% at a 1:1,000 ratio and 12.47% at 1:10,000. These results suggest that
automated cohesion measurements, in general, and name-prediction-based
cohesion, in particular, may help identify supply chain attacks, improving
source code integrity.

</details>


### [18] [Instruction Set Migration at Warehouse Scale](https://arxiv.org/abs/2510.14928)
*Eric Christopher,Kevin Crossan,Wolff Dobson,Chris Kennelly,Drew Lewis,Kun Lin,Martin Maas,Parthasarathy Ranganathan,Emma Rapati,Brian Yang*

Main category: cs.SE

TL;DR: 论文分析了从x86到Arm架构的大规模代码迁移挑战，指出现代ISA迁移主要依赖重新编译而非二进制翻译，并展示了Google如何自动化这些过程以及AI在其中的作用。


<details>
  <summary>Details</summary>
Motivation: 传统观点认为ISA迁移的主要挑战是二进制翻译，但现代开源生态系统使得重新编译成为可能，这带来了新的多方面挑战。

Method: 通过分析Google从x86到Arm的大规模迁移（涉及近40,000个代码提交），推导出ISA迁移的任务分类，并展示了自动化和AI的应用。

Result: 识别了ISA迁移中的各类任务，展示了Google如何自动化这些步骤，证明了AI在自动解决这些问题中的重要作用。

Conclusion: 现代ISA迁移面临与二进制翻译不同的新挑战，自动化工具和AI技术能显著提升迁移效率，但仍有一些挑战性问题需要进一步研究。

Abstract: Migrating codebases from one instruction set architecture (ISA) to another is
a major engineering challenge. A recent example is the adoption of Arm (in
addition to x86) across the major Cloud hyperscalers. Yet, this problem has
seen limited attention by the academic community. Most work has focused on
static and dynamic binary translation, and the traditional conventional wisdom
has been that this is the primary challenge.
  In this paper, we show that this is no longer the case. Modern ISA migrations
can often build on a robust open-source ecosystem, making it possible to
recompile all relevant software from scratch. This introduces a new and
multifaceted set of challenges, which are different from binary translation.
  By analyzing a large-scale migration from x86 to Arm at Google, spanning
almost 40,000 code commits, we derive a taxonomy of tasks involved in ISA
migration. We show how Google automated many of the steps involved, and
demonstrate how AI can play a major role in automatically addressing these
tasks. We identify tasks that remain challenging and highlight research
challenges that warrant further attention.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [19] [Noisy Networks, Nosy Neighbors: Inferring Privacy Invasive Information from Encrypted Wireless Traffic](https://arxiv.org/abs/2510.13822)
*Bartosz Burgiel*

Main category: cs.CR

TL;DR: 通过被动监听智能家居的WiFi和蓝牙流量，即使数据加密也能推断住户隐私信息，如设备活动状态、日常行为和公寓布局。


<details>
  <summary>Details</summary>
Motivation: 探索智能家居环境中无线流量被动观察能否推断住户隐私信息，模拟隔壁邻居的窥探能力。

Method: 分析802.11原始数据包和BLE广播，通过设备识别、活动状态推断和RSSI三边定位来近似设备位置。

Result: 能够检测多媒体设备活跃期，推断睡眠、工作、媒体消费等日常活动，并近似邻居公寓布局。

Conclusion: 智能家居隐私风险超出传统数据泄露，邻居仅通过加密网络流量就能获得隐私侵入性洞察。

Abstract: This thesis explores the extent to which passive observation of wireless
traffic in a smart home environment can be used to infer privacy-invasive
information about its inhabitants. Using a setup that mimics the capabilities
of a nosy neighbor in an adjacent flat, we analyze raw 802.11 packets and
Bluetooth Low Energy advertisemets. From this data, we identify devices, infer
their activity states and approximate their location using RSSI-based
trilateration. Despite the encrypted nature of the data, we demonstrate that it
is possible to detect active periods of multimedia devices, infer common
activities such as sleeping, working and consuming media, and even approximate
the layout of the neighbor's apartment. Our results show that privacy risks in
smart homes extend beyond traditional data breaches: a nosy neighbor behind the
wall can gain privacy-invasive insights into the lives of their neighbors
purely from encrypted network traffic.

</details>


### [20] [Multi-Layer Secret Sharing for Cross-Layer Attack Defense in 5G Networks: a COTS UE Demonstration](https://arxiv.org/abs/2510.13824)
*Wai Ming Chan,Remi Chou,Taejoon Kim*

Main category: cs.CR

TL;DR: 在商用5G用户设备上实现无需基础设施修改或预共享密钥的多层秘密共享系统


<details>
  <summary>Details</summary>
Motivation: 解决5G网络中数据机密性和可用性问题，特别是在面临拒绝服务攻击或意外攻击时确保数据安全

Method: 采用基于XOR的方法，将秘密份额分布在多个网络运营商和分布式中继节点之间

Result: 即使一个网络运营商和一个中继同时丢失，也能实现完美恢复和数据保密

Conclusion: 成功在商用5G设备上实现了无需基础设施修改的多层秘密共享系统，提供了强大的数据保护能力

Abstract: This demo presents the first implementation of multi-layer secret sharing on
commercial-off-the-shelf (COTS) 5G user equipment (UE), operating without
infrastructure modifications or pre-shared keys. Our XOR-based approach
distributes secret shares across network operators and distributed relays,
ensuring perfect recovery and data confidentiality even if one network operator
and one relay are simultaneously lost (e.g., under denial of service (DoS) or
unanticipated attacks).

</details>


### [21] [A2AS: Agentic AI Runtime Security and Self-Defense](https://arxiv.org/abs/2510.13825)
*Eugene Neelou,Ivan Novikov,Max Moroz,Om Narayan,Tiffany Saade,Mika Ayenson,Ilya Kabanov,Jen Ozmen,Edward Lee,Vineeth Sai Narajala,Emmanuel Guilherme Junior,Ken Huang,Huseyin Gulsin,Jason Ross,Marat Vyshegorodtsev,Adelin Travers,Idan Habler,Rahul Jadav*

Main category: cs.CR

TL;DR: A2AS框架为AI代理和LLM应用提供安全层，类似HTTPS保护HTTP。它通过BASIC安全模型实现行为认证、提示认证、安全边界、上下文防御和策略编码，无需延迟开销或架构改动。


<details>
  <summary>Details</summary>
Motivation: 随着AI代理和LLM应用的普及，需要类似HTTPS的安全层来保护这些系统免受攻击，确保行为可控和上下文完整性。

Method: 提出A2AS框架和BASIC安全模型：行为证书强制执行行为，认证提示确保上下文完整性，安全边界隔离不可信输入，上下文防御保护模型推理，编码策略实现应用特定规则。

Result: A2AS框架能够有效保护AI系统，避免延迟开销、外部依赖、架构改动、模型重训练和操作复杂性，提供深度防御策略。

Conclusion: A2AS框架和BASIC安全模型为AI安全提供了可行的解决方案，有望成为行业标准，首个论文介绍了其基本原理和潜力。

Abstract: The A2AS framework is introduced as a security layer for AI agents and
LLM-powered applications, similar to how HTTPS secures HTTP. A2AS enforces
certified behavior, activates model self-defense, and ensures context window
integrity. It defines security boundaries, authenticates prompts, applies
security rules and custom policies, and controls agentic behavior, enabling a
defense-in-depth strategy. The A2AS framework avoids latency overhead, external
dependencies, architectural changes, model retraining, and operational
complexity. The BASIC security model is introduced as the A2AS foundation: (B)
Behavior certificates enable behavior enforcement, (A) Authenticated prompts
enable context window integrity, (S) Security boundaries enable untrusted input
isolation, (I) In-context defenses enable secure model reasoning, (C) Codified
policies enable application-specific rules. This first paper in the series
introduces the BASIC security model and the A2AS framework, exploring their
potential toward establishing the A2AS industry standard.

</details>


### [22] [PIShield: Detecting Prompt Injection Attacks via Intrinsic LLM Features](https://arxiv.org/abs/2510.14005)
*Wei Zou,Yupei Liu,Yanting Wang,Ying Chen,Neil Gong,Jinyuan Jia*

Main category: cs.CR

TL;DR: 提出了PIShield方法，通过分析LLM内部特定层的最终token表示来检测提示注入攻击，使用简单线性分类器实现高效检测。


<details>
  <summary>Details</summary>
Motivation: 现有提示注入检测方法性能不佳且计算开销高，LLM应用易受提示注入攻击，攻击者可通过污染输入使LLM执行恶意指令。

Method: 提取LLM特定注入关键层中最终token的内部表示，使用标记的干净和污染提示训练线性分类器进行检测。

Result: 在5个基准数据集和8种攻击上优于11个基线方法，检测效果显著且效率高，能抵抗强自适应攻击。

Conclusion: PIShield通过利用LLM内部表示的关键特征，实现了高效且有效的提示注入检测，显著优于现有方法。

Abstract: LLM-integrated applications are vulnerable to prompt injection attacks, where
an attacker contaminates the input to inject malicious prompts, causing the LLM
to follow the attacker's intent instead of the original user's. Existing prompt
injection detection methods often have sub-optimal performance and/or high
computational overhead. In this work, we propose PIShield, a detection method
that is both effective and efficient. Our key observation is that the internal
representation of the final token in a prompt-extracted from a specific layer
of the LLM, which we term the injection-critical layer-captures distinguishing
features between clean and contaminated prompts. Leveraging this insight, we
train a simple linear classifier on these internal representations using a
labeled set of clean and contaminated prompts. We compare PIShield against 11
baselines across 5 diverse benchmark datasets and 8 prompt injection attacks.
The results demonstrate that PIShield is both highly effective and efficient,
substantially outperforming existing methods. Additionally, we show that
PIShield resists strong adaptive attacks.

</details>


### [23] [Quantitative Analysis of UAV Intrusion Mitigation for Border Security in 5G with LEO Backhaul Impairments](https://arxiv.org/abs/2510.14066)
*Rajendra Upadhyay,Al Nahian Bin Emran,Rajendra Paudyal,Lisa Donnan,Duminda Wijesekera*

Main category: cs.CR

TL;DR: 提出了一个端到端仿真框架，用于分析混合地面-非地面5G系统中无人机入侵的检测到缓解延迟，强调卫星回程中断对缓解延迟的影响以及本地回退机制的重要性。


<details>
  <summary>Details</summary>
Motivation: 非合作无人机在蜂窝网络中作为流氓用户设备运行，消耗资源、产生干扰并可能侵犯限制空域，对关键基础设施和边境保护构成威胁。

Method: 使用包含地面gNB、卫星回程（具有随机中断）和检测逻辑（由切换不稳定性和信号质量方差触发）的系统模型，通过蒙特卡洛模拟分析无人机高度、速度和卫星中断率的影响。

Result: 卫星回程中断会导致任意长的缓解延迟；额外切换在参数范围内影响可忽略；回退机制在限制缓解延迟方面提供主要弹性效益；巡逻UE受到可忽略的附带影响。

Conclusion: 必须将非地面链路与本地控制相结合，以确保对非合作无人机入侵的稳健和及时响应，回退机制对于防止极端控制平面和物理安全漏洞至关重要。

Abstract: Uncooperative unmanned aerial vehicles (UAVs) pose emerging threats to
critical infrastructure and border protection by operating as rogue user
equipment (UE) within cellular networks, consuming resources, creating
interference, and potentially violating restricted airspaces. This paper
presents minimal features of the operating space, yet an end-to-end simulation
framework to analyze detect-to-mitigate latency of such intrusions in a hybrid
terrestrial-non-terrestrial (LEO satellite) 5G system. The system model
includes terrestrial gNBs, satellite backhaul (with stochastic outages), and a
detection logic (triggered by handover instability and signal quality
variance). A lockdown mechanism is invoked upon detection, with optional local
fallback to cap mitigation delays. Monte Carlo sweeps across UAV altitudes,
speeds, and satellite outage rates yield several insights. First, satellite
backhaul outages can cause arbitrarily long mitigation delays, yet, to meet
fallback deadlines, they need to be effectively bounded. Second, while handover
instability was hypothesized, our results show that extra handovers have a
negligible effect within the range of parameters we considered. The main
benefit of resilience from fallback comes from the delay in limiting
mitigation. Third, patrol UEs experience negligible collateral impact, with
handover rates close to terrestrial baselines. Stress scenarios further
highlight that fallback is indispensable in preventing extreme control-plane
and physical security vulnerabilities: Without fallback, prolonged outages in
the satellite backhaul delay lockdown commands, allowing rogue UAVs to linger
inside restricted corridors for several seconds longer. These results
underscore the importance of complementing non-terrestrial links with local
control to ensure robust and timely response against uncooperative UAV
intrusions.

</details>


### [24] [Every Language Model Has a Forgery-Resistant Signature](https://arxiv.org/abs/2510.14086)
*Matthew Finlayson,Xiang Ren,Swabha Swayamdipta*

Main category: cs.CR

TL;DR: 本文提出了一种基于语言模型输出几何约束的椭圆签名方法，用于识别模型来源和验证输出真实性。该方法利用语言模型输出位于高维椭圆表面的特性作为模型签名。


<details>
  <summary>Details</summary>
Motivation: 随着闭源权重语言模型API的普及，需要开发能够提取隐藏模型细节和识别模型来源的取证方法。现有方法主要利用语言模型架构和参数施加的几何约束。

Method: 利用语言模型输出位于高维椭圆表面的几何约束作为模型签名。该方法通过从模型输出中提取椭圆特征来识别源模型，并提出了一种从小模型中提取椭圆的新技术。

Result: 椭圆签名具有难以伪造、自然存在、自包含、紧凑冗余等独特特性。研究评估了从小模型提取椭圆的技术，并讨论了在生产规模模型中应用的实践障碍。

Conclusion: 椭圆签名可作为语言模型输出验证的有效方法，类似于密码学中的对称密钥消息认证系统，为模型来源识别和输出真实性验证提供了新途径。

Abstract: The ubiquity of closed-weight language models with public-facing APIs has
generated interest in forensic methods, both for extracting hidden model
details (e.g., parameters) and for identifying models by their outputs. One
successful approach to these goals has been to exploit the geometric
constraints imposed by the language model architecture and parameters. In this
work, we show that a lesser-known geometric constraint--namely, that language
model outputs lie on the surface of a high-dimensional ellipse--functions as a
signature for the model and can be used to identify the source model of a given
output. This ellipse signature has unique properties that distinguish it from
existing model-output association methods like language model fingerprints. In
particular, the signature is hard to forge: without direct access to model
parameters, it is practically infeasible to produce log-probabilities
(logprobs) on the ellipse. Secondly, the signature is naturally occurring,
since all language models have these elliptical constraints. Thirdly, the
signature is self-contained, in that it is detectable without access to the
model inputs or the full weights. Finally, the signature is compact and
redundant, as it is independently detectable in each logprob output from the
model. We evaluate a novel technique for extracting the ellipse from small
models and discuss the practical hurdles that make it infeasible for
production-scale models. Finally, we use ellipse signatures to propose a
protocol for language model output verification, analogous to cryptographic
symmetric-key message authentication systems.

</details>


### [25] [Power Grid Cybersecurity: Policy Analysis White Paper](https://arxiv.org/abs/2510.14171)
*Jack Vanlyssel*

Main category: cs.CR

TL;DR: 提出双重政策方法加强电网网络安全：加强政府与私营电力公司之间的信息共享以改善威胁检测和响应，以及标准化网络卫生实践以减少常见攻击向量。


<details>
  <summary>Details</summary>
Motivation: 美国电网面临工业控制系统漏洞、远程访问和网络卫生不良等日益增长的网络安全风险，威胁国家安全、公共安全和经济稳定，但当前政策仍分散且被动。

Method: 采用双重政策方法：1) 加强政府与私营电力公司之间的信息共享；2) 标准化网络卫生实践。长期建议建立统一的国家网络安全框架，协调现有NERC、IEC、IEEE和NIST标准。

Result: 这些政策能够提供即时和可持续的改进，通过改善威胁检测和响应能力，减少常见攻击向量，增强电网网络安全。

Conclusion: 双重政策方法结合统一的国家网络安全框架，能够有效保护国家最关键的基础设施，应对不断演变的网络威胁。

Abstract: The U.S. power grid underpins national security, public safety, and economic
stability, but faces growing cyber risks from vulnerabilities in industrial
control systems, remote access, and poor cyber hygiene. Despite its critical
importance, current policy remains fragmented and reactive. This paper proposes
a dual policy approach to strengthen grid cybersecurity: enhanced information
sharing between government and private utilities to improve threat detection
and response, and standardized cyber hygiene practices to reduce common attack
vectors. For long-term resilience, a Unified National Cybersecurity Framework
is recommended to align existing NERC, IEC, IEEE, and NIST standards, eliminate
regulatory overlap, and adapt to evolving threats. Together, these policies
offer both immediate and sustainable improvements in safeguarding the nation's
most vital infrastructure.

</details>


### [26] [Securing U.S. Critical Infrastructure: Lessons from Stuxnet and the Ukraine Power Grid Attacks](https://arxiv.org/abs/2510.14185)
*Jack Vanlyssel*

Main category: cs.CR

TL;DR: 本文分析了工业控制系统面临的网络安全威胁，通过研究历史攻击案例识别漏洞，并提出基于零信任架构和网络分段的政策建议来保护美国关键基础设施。


<details>
  <summary>Details</summary>
Motivation: 工业控制系统支撑着美国关键基础设施，但日益增长的网络威胁暴露了这些系统的脆弱性。历史攻击事件揭示了持续存在的安全弱点，需要立即改革以避免灾难性后果。

Method: 通过分析Stuxnet和乌克兰电网事件等历史攻击案例，识别工业控制系统中反复出现的漏洞，并评估这些漏洞在当前美国基础设施中的相关性。

Result: 研究发现工业控制系统存在网络分段不足、软件过时、认证薄弱和监控不足等持续性问题，这些漏洞使美国关键基础设施面临严重风险。

Conclusion: 建议实施零信任架构和改进的网络分段等政策措施，以增强系统韧性，保护关键操作技术免受未来网络威胁。

Abstract: Industrial Control Systems (ICS) underpin the United States' critical
infrastructure, managing essential services such as power, water, and
transportation that are vital to national security and public safety. However,
increasing digital integration has exposed these systems to escalating cyber
threats. Historical attacks like Stuxnet and the Ukraine power grid incident
revealed exploitable weaknesses-poor network segmentation, outdated software,
weak authentication, and inadequate monitoring-that persist in many U.S. ICS
environments today. This paper analyzes these landmark attacks to identify
recurring vulnerabilities and assess their relevance to current U.S.
infrastructure. It argues that without immediate reforms, similar exploits
could lead to catastrophic disruptions and national security crises. To address
these risks, the paper proposes policy measures focused on implementing
zero-trust architecture and improved network segmentation to enhance system
resilience. These recommendations aim to guide policymakers and industry
leaders in securing the nation's most critical operational technologies against
future cyber threats.

</details>


### [27] [Infrastructure Patterns in Toll Scam Domains: A Comprehensive Analysis of Cybercriminal Registration and Hosting Strategies](https://arxiv.org/abs/2510.14198)
*Morium Akter Munny,Mahbub Alam,Sonjoy Kumar Paul,Daniel Timko,Muhammad Lutfor Rahman,Nitesh Saxena*

Main category: cs.CR

TL;DR: 对67,907个确认的收费诈骗域名进行首次大规模分析，揭示了攻击者利用宽松注册商和非主流顶级域名的策略，并构建了基于注册数据的预测模型，准确率达80.4%。


<details>
  <summary>Details</summary>
Motivation: 收费诈骗通过注册伪装成合法交通机构的虚假域名欺骗用户进行欺诈支付，这类诈骗迅速增加且危害严重，但尚未得到充分研究。

Method: 使用新创建的67,907个确认诈骗域名数据集进行分析，研究注册模式，并构建仅使用域名注册数据的简单预测模型来预测哪些诈骗域名可能被暂停。

Result: 发现86.9%的域名集中在五个非主流TLD，72.9%通过单一提供商注册；超过一半域名在2025年第一季度注册，显示高度同步的活动爆发；预测模型达到80.4%准确率和92.3%灵敏度。

Conclusion: 分析揭示了攻击者逃避检测的策略，但仅凭注册元数据可能不足，结合域名URL和网页内容特征可进一步提高检测效果。

Abstract: Toll scams involve criminals registering fake domains that pretend to be
legitimate transportation agencies to trick users into making fraudulent
payments. Although these scams are rapidly increasing and causing significant
harm, they have not been extensively studied. We present the first large-scale
analysis of toll scam domains, using a newly created dataset of 67,907
confirmed scam domains mostly registered in 2025. Our study reveals that
attackers exploit permissive registrars and less common top-level domains, with
86.9% of domains concentrated in just five non-mainstream TLDs and 72.9%
registered via a single provider. We also discover specific registration
patterns, including short bursts of activity that suggest automated,
coordinated attacks, with over half of domains registered in the first quarter
of 2025. This extreme temporal clustering reflects highly synchronized campaign
launches. Additionally, we build a simple predictive model using only domain
registration data to predict which scam domains are likely to be suspended -- a
proxy for confirmed abuse -- achieving 80.4% accuracy, and 92.3% sensitivity.
Our analysis reveals attacker strategies for evading detection -- such as
exploiting obscure TLDs, permissive registrars, and coordinated registration
bursts -- which can inform more targeted interventions by registrars, hosting
providers, and security platforms. However, our results suggest that
registration metadata alone may be insufficient, and incorporating features
from domain URLs and webpage content could further improve detection.

</details>


### [28] [An Information Asymmetry Game for Trigger-based DNN Model Watermarking](https://arxiv.org/abs/2510.14218)
*Chaoyue Huang,Gejian Zhao,Hanzhou Wu,Zhihua Xia,Asad Malik*

Main category: cs.CR

TL;DR: 该论文提出了一种基于博弈论分析的深度神经网络水印保护方法，通过稀疏水印技术抵抗修剪和微调攻击，在信息不对称条件下建立防御模型。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络作为有价值的数字产品面临严重知识产权威胁，需要开发有效的技术措施来保护模型版权。现有的基于触发器的水印方法容易通过修剪或微调被移除。

Method: 将水印保护建模为信息不对称条件下的博弈问题，防御者嵌入秘密水印，攻击者只能访问水印模型并寻求移除。定义了双方策略、成本和效用函数，推导攻击者的最优修剪预算。

Result: 实验结果表明水印模型的可行性，稀疏水印技术能够以可忽略的精度损失抵抗移除攻击。建立了攻击后水印检测精度的指数下界。

Conclusion: 博弈论分析为设计鲁棒的水印方案提供了有效指导，稀疏水印是保护模型版权的可行方法。

Abstract: As a valuable digital product, deep neural networks (DNNs) face increasingly
severe threats to the intellectual property, making it necessary to develop
effective technical measures to protect them. Trigger-based watermarking
methods achieve copyright protection by embedding triggers into the host DNNs.
However, the attacker may remove the watermark by pruning or fine-tuning. We
model this interaction as a game under conditions of information asymmetry,
namely, the defender embeds a secret watermark with private knowledge, while
the attacker can only access the watermarked model and seek removal. We define
strategies, costs, and utilities for both players, derive the attacker's
optimal pruning budget, and establish an exponential lower bound on the
accuracy of watermark detection after attack. Experimental results demonstrate
the feasibility of the watermarked model, and indicate that sparse watermarking
can resist removal with negligible accuracy loss. This study highlights the
effectiveness of game-theoretic analysis in guiding the design of robust
watermarking schemes for model copyright protection.

</details>


### [29] [RHINO: Guided Reasoning for Mapping Network Logs to Adversarial Tactics and Techniques with Large Language Models](https://arxiv.org/abs/2510.14233)
*Fanchao Meng,Jiaping Gui,Yunbo Li,Yue Wu*

Main category: cs.CR

TL;DR: RHINO是一个基于LLM的网络入侵检测框架，通过三阶段推理过程将低级警报映射到MITRE ATT&CK攻击技术，显著提高了准确性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有网络入侵检测系统产生大量低级警报，但需要人工关联到高级攻击行为。基于规则的方法无法适应新型攻击，机器学习方法缺乏上下文意识，而现有LLM方法存在幻觉问题。

Method: RHINO将LLM攻击分析分解为三个可解释阶段：行为抽象（将原始日志转化为情境化叙述）、多角色协作推理（基于MITRE知识生成候选技术）、验证（交叉引用MITRE定义纠正幻觉）。

Result: 在三个基准测试和四个骨干模型上评估，RHINO准确率达到86.38%至88.45%，相对增益为24.25%至76.50%。

Conclusion: RHINO显著提升了威胁分析的可解释性和可扩展性，为在操作安全环境中部署LLM提供了蓝图。

Abstract: Modern Network Intrusion Detection Systems generate vast volumes of low-level
alerts, yet these outputs remain semantically fragmented, requiring
labor-intensive manual correlation with high-level adversarial behaviors.
Existing solutions for automating this mapping-rule-based systems and machine
learning classifiers-suffer from critical limitations: rule-based approaches
fail to adapt to novel attack variations, while machine learning methods lack
contextual awareness and treat tactic-technique mapping as a syntactic matching
problem rather than a reasoning task. Although Large Language Models have shown
promise in cybersecurity tasks, preliminary experiments reveal that existing
LLM-based methods frequently hallucinate technique names or produce
decontextualized mappings due to their single-step classification approach.
  To address these challenges, we introduce RHINO, a novel framework that
decomposes LLM-based attack analysis into three interpretable phases mirroring
human reasoning: (1) behavioral abstraction, where raw logs are translated into
contextualized narratives; (2) multi-role collaborative inference, generating
candidate techniques by evaluating behavioral evidence against MITRE ATT&CK
knowledge; and (3) validation, cross-referencing predictions with official
MITRE definitions to rectify hallucinations. RHINO bridges the semantic gap
between low-level observations and adversarial intent while improving output
reliability through structured reasoning.
  We evaluate RHINO on three benchmarks across four backbone models. RHINO
achieved high accuracy, with model performance ranging from 86.38% to 88.45%,
resulting in relative gains from 24.25% to 76.50% across different models. Our
results demonstrate that RHINO significantly enhances the interpretability and
scalability of threat analysis, offering a blueprint for deploying LLMs in
operational security settings.

</details>


### [30] [Beyond a Single Perspective: Towards a Realistic Evaluation of Website Fingerprinting Attacks](https://arxiv.org/abs/2510.14283)
*Xinhao Deng,Jingyou Chen,Linxiao Yu,Yixiang Zhang,Zhongyi Gu,Changhao Qiu,Xiyuan Zhao,Ke Xu,Qi Li*

Main category: cs.CR

TL;DR: 本文首次系统评估网站指纹识别攻击在现实环境中的表现，发现大多数在孤立场景表现良好的方法在面临防御机制、流量漂移、多标签浏览等复杂条件时性能显著下降。


<details>
  <summary>Details</summary>
Motivation: 现有网站指纹识别攻击研究大多局限于单一场景，忽视了真实环境的复杂性，需要系统评估这些方法在多样化现实条件下的实际效果。

Method: 采用多维评估框架，在多种现实条件下测试现有WF攻击，包括防御机制、流量漂移、多标签浏览、早期检测、开放世界和少样本场景。

Result: 实验结果显示，许多在孤立场景中表现优异的WF技术在面对其他条件时性能显著下降，表明当前攻击方法难以直接应用于实践。

Conclusion: 真实环境通常包含多重挑战的组合，当前WF攻击在实际应用中存在严重局限性，需要开发更鲁棒和实用的攻击方法。

Abstract: Website Fingerprinting (WF) attacks exploit patterns in encrypted traffic to
infer the websites visited by users, posing a serious threat to anonymous
communication systems. Although recent WF techniques achieve over 90% accuracy
in controlled experimental settings, most studies remain confined to single
scenarios, overlooking the complexity of real-world environments. This paper
presents the first systematic and comprehensive evaluation of existing WF
attacks under diverse realistic conditions, including defense mechanisms,
traffic drift, multi-tab browsing, early-stage detection, open-world settings,
and few-shot scenarios. Experimental results show that many WF techniques with
strong performance in isolated settings degrade significantly when facing other
conditions. Since real-world environments often combine multiple challenges,
current WF attacks are difficult to apply directly in practice. This study
highlights the limitations of WF attacks and introduces a multidimensional
evaluation framework, offering critical insights for developing more robust and
practical WF attacks.

</details>


### [31] [BinCtx: Multi-Modal Representation Learning for Robust Android App Behavior Detection](https://arxiv.org/abs/2510.14344)
*Zichen Liu,Shao Yang,Xusheng Xiao*

Main category: cs.CR

TL;DR: BINCTX是一个多模态学习方法，通过结合字节码图像、上下文视图和第三方库使用视图来检测移动应用中的不良行为，在真实恶意软件检测中达到94.73%的F1分数，对混淆和对抗样本具有鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 移动应用市场中存在大量不良行为应用，这些应用通常不依赖权限保护API，且容易通过UI或元数据编辑进行伪装，传统检测方法难以有效识别。

Method: 构建三种视图：全局字节码图像视图捕获代码语义和家族模式，上下文视图显示行为触发方式，第三方库使用视图总结调用路径频率。三种视图嵌入融合后训练上下文感知分类器。

Result: 在真实恶意软件和良性应用上达到94.73%的宏F1分数，比强基线至少提升14.92%。在商业混淆后仍保持84%的F1分数，比仅使用字节码的最先进系统更抗对抗样本。

Conclusion: BINCTX通过多模态表示学习有效检测移动应用中的不良行为，对混淆和对抗攻击具有鲁棒性，显著优于现有方法。

Abstract: Mobile app markets host millions of apps, yet undesired behaviors (e.g.,
disruptive ads, illegal redirection, payment deception) remain hard to catch
because they often do not rely on permission-protected APIs and can be easily
camouflaged via UI or metadata edits. We present BINCTX, a learning approach
that builds multi-modal representations of an app from (i) a global
bytecode-as-image view that captures code-level semantics and family-style
patterns, (ii) a contextual view (manifested actions, components, declared
permissions, URL/IP constants) indicating how behaviors are triggered, and
(iii) a third-party-library usage view summarizing invocation frequencies along
inter-component call paths. The three views are embedded and fused to train a
contextual-aware classifier. On real-world malware and benign apps, BINCTX
attains a macro F1 of 94.73%, outperforming strong baselines by at least
14.92%. It remains robust under commercial obfuscation (F1 84%
post-obfuscation) and is more resistant to adversarial samples than
state-of-the-art bytecode-only systems.

</details>


### [32] [Match & Mend: Minimally Invasive Local Reassembly for Patching N-day Vulnerabilities in ARM Binaries](https://arxiv.org/abs/2510.14384)
*Sebastian Jänich,Merlin Sievers,Johannes Kinder*

Main category: cs.CR

TL;DR: 提出了一种在二进制级别自动修补物联网固件中已知漏洞的技术，无需厂商支持，成功修补了83%-96%的目标漏洞。


<details>
  <summary>Details</summary>
Motivation: 低成本物联网设备由于更新机制不完善而存在安全隐患，许多设备运行着过时且有已知漏洞的开源软件版本。

Method: 采用最小侵入式本地重汇编技术，在二进制级别自动修补已知漏洞，旨在最小化副作用和降低引入破坏性变更的风险。

Result: 在MAGMA基准测试的108个二进制文件中成功修补83%的目标漏洞，在KARONTE数据集的30个真实物联网固件中成功修补96%的目标漏洞。

Conclusion: 该方法能有效解决物联网设备的安全更新问题，无需厂商支持即可自动修补已知漏洞。

Abstract: Low-cost Internet of Things (IoT) devices are increasingly popular but often
insecure due to poor update regimes. As a result, many devices run outdated and
known-vulnerable versions of open-source software. We address this problem by
proposing to patch IoT firmware at the binary level, without requiring vendor
support. In particular, we introduce minimally invasive local reassembly, a new
technique for automatically patching known (n-day) vulnerabilities in IoT
firmware. Our approach is designed to minimize side effects and reduce the risk
of introducing breaking changes. We systematically evaluate our approach both
on 108 binaries within the controlled environment of the MAGMA benchmarks, as
well as on 30 real-world Linux-based IoT firmware images from the KARONTE
dataset. Our prototype successfully patches 83% of targeted vulnerabilities in
MAGMA and 96% in the firmware dataset.

</details>


### [33] [Stealthy Dual-Trigger Backdoors: Attacking Prompt Tuning in LM-Empowered Graph Foundation Models](https://arxiv.org/abs/2510.14470)
*Xiaoyu Xue,Yuni Lai,Chenxi Huang,Yulin Zhu,Gaolei Li,Xiaoge Zhang,Kai Zhou*

Main category: cs.CR

TL;DR: 该论文揭示了语言模型赋能的图基础模型在提示调优阶段的独特安全漏洞，提出了一种在文本级和结构级同时操作的双触发后门攻击框架，能够在无需显式优化触发节点文本属性的情况下实现有效攻击。


<details>
  <summary>Details</summary>
Motivation: 传统图神经网络相比，语言模型赋能的图基础模型在未受保护的提示调优阶段引入了独特的安全漏洞，但目前研究对此关注不足。作者发现传统图后门攻击在属性不可访问的受限文本属性图系统中性能显著下降。

Method: 提出了一种新颖的双触发后门攻击框架，在文本级和结构级同时操作，通过战略性地利用预建立的文本池，无需显式优化触发节点文本属性即可实现有效攻击。

Result: 广泛的实验评估表明，该攻击在保持优越的干净准确率的同时，实现了出色的攻击成功率，包括高度隐蔽的单触发节点场景。

Conclusion: 这项工作突显了网络部署的语言模型赋能图基础模型中的关键后门风险，为开源平台在基础模型时代开发更强大的监督机制做出了贡献。

Abstract: The emergence of graph foundation models (GFMs), particularly those
incorporating language models (LMs), has revolutionized graph learning and
demonstrated remarkable performance on text-attributed graphs (TAGs). However,
compared to traditional GNNs, these LM-empowered GFMs introduce unique security
vulnerabilities during the unsecured prompt tuning phase that remain
understudied in current research. Through empirical investigation, we reveal a
significant performance degradation in traditional graph backdoor attacks when
operating in attribute-inaccessible constrained TAG systems without explicit
trigger node attribute optimization. To address this, we propose a novel
dual-trigger backdoor attack framework that operates at both text-level and
struct-level, enabling effective attacks without explicit optimization of
trigger node text attributes through the strategic utilization of a
pre-established text pool. Extensive experimental evaluations demonstrate that
our attack maintains superior clean accuracy while achieving outstanding attack
success rates, including scenarios with highly concealed single-trigger nodes.
Our work highlights critical backdoor risks in web-deployed LM-empowered GFMs
and contributes to the development of more robust supervision mechanisms for
open-source platforms in the era of foundation models.

</details>


### [34] [Certifying optimal MEV strategies with Lean](https://arxiv.org/abs/2510.14480)
*Massimo Bartoletti,Riccardo Marchesin,Roberto Zunino*

Main category: cs.CR

TL;DR: 本文提出了首个在Lean定理证明器中机械化形式化MEV的方法，开发了机器验证的MEV边界证明方法，并首次证明了AMM中三明治攻击的最优性。


<details>
  <summary>Details</summary>
Motivation: MEV攻击已从DeFi协议中提取了数十亿美元价值，验证MEV攻击的缺失需要确定合适的上界，但现有经验研究和手工推理方法不够严谨。

Method: 在Lean定理证明器中形式化MEV，构建机器验证的MEV边界证明方法，并对两个典型DeFi协议进行建模分析。

Result: 成功开发了机器验证的MEV边界证明方法，首次证明了自动做市商中三明治攻击的最优性。

Conclusion: 该方法提供了超越现有技术的正确性保证，为MEV安全分析提供了严格的机械化验证框架。

Abstract: Maximal Extractable Value (MEV) refers to a class of attacks to decentralized
applications where the adversary profits by manipulating the ordering,
inclusion, or exclusion of transactions in a blockchain. Decentralized Finance
(DeFi) protocols are a primary target of these attacks, as their logic depends
critically on transaction sequencing. To date, MEV attacks have already
extracted billions of dollars in value, underscoring their systemic impact on
blockchain security. Verifying the absence of MEV attacks requires determining
suitable upper bounds, i.e. proving that no adversarial strategy can extract
more value (if any) than expected by protocol designers. This problem is
notoriously difficult: the space of adversarial strategies is extremely vast,
making empirical studies and pen-and-paper reasoning insufficiently rigorous.
In this paper, we present the first mechanized formalization of MEV in the Lean
theorem prover. We introduce a methodology to construct machine-checked proofs
of MEV bounds, providing correctness guarantees beyond what is possible with
existing techniques. To demonstrate the generality of our approach, we model
and analyse the MEV of two paradigmatic DeFi protocols. Notably, we develop the
first machine-checked proof of the optimality of sandwich attacks in Automated
Market Makers, a fundamental DeFi primitive.

</details>


### [35] [Lexo: Eliminating Stealthy Supply-Chain Attacks via LLM-Assisted Program Regeneration](https://arxiv.org/abs/2510.14522)
*Evangelos Lamprou,Julian Dai,Grigoris Ntousakis,Martin C. Rinard,Nikos Vasilakis*

Main category: cs.CR

TL;DR: Lexo是一个自动学习和重新生成无漏洞版本的开源软件组件系统，旨在防御软件供应链攻击。它通过生成输入-输出对来建模组件行为，然后合成新版本组件，保持原始功能但消除恶意代码。


<details>
  <summary>Details</summary>
Motivation: 软件供应链攻击是开源软件生态系统中的重要持续威胁，这些攻击在组件到达目标环境时才激活恶意功能，具有隐蔽性。

Method: Lexo首先生成输入-输出对来建模组件的完整可观察行为，然后使用这些数据合成新版本组件。过程中咨询多个大型语言模型实例，使用正确性和覆盖率指标来指导和约束结果。

Result: 在100多个真实世界软件包上的评估表明，Lexo能够跨多个领域扩展，平均在100秒内高效重新生成代码，保持兼容性，并成功消除多个真实世界供应链攻击中的恶意代码，即使在最先进的大型语言模型直接提示消除恶意代码失败的情况下也能成功。

Conclusion: Lexo提供了一种有效的方法来防御隐蔽的软件供应链攻击，通过自动重新生成无恶意代码的组件版本，在保持功能兼容性的同时消除安全威胁。

Abstract: Software supply-chain attacks are an important and ongoing concern in the
open source software ecosystem. These attacks maintain the standard
functionality that a component implements, but additionally hide malicious
functionality activated only when the component reaches its target environment.
Lexo addresses such stealthy attacks by automatically learning and regenerating
vulnerability-free versions of potentially malicious components. Lexo first
generates a set of input-output pairs to model a component's full observable
behavior, which it then uses to synthesize a new version of the original
component. The new component implements the original functionality but avoids
stealthy malicious behavior. Throughout this regeneration process, Lexo
consults several distinct instances of Large Language Models (LLMs), uses
correctness and coverage metrics to shepherd these instances, and guardrails
their results. Our evaluation on 100+ real-world packages, including high
profile stealthy supply-chain attacks, indicates that Lexo scales across
multiple domains, regenerates code efficiently (<100s on average), maintains
compatibility, and succeeds in eliminating malicious code in several real-world
supply-chain-attacks, even in cases when a state-of-the-art LLM fails to
eliminate malicious code when prompted to do so.

</details>


### [36] [Symbolic verification of Apple's Find My location-tracking protocol](https://arxiv.org/abs/2510.14589)
*Vaishnavi Sundararajan,Rithwik*

Main category: cs.CR

TL;DR: 本文对苹果Find My追踪系统进行形式化安全分析，通过符号建模和自动验证证明其隐私保护属性。


<details>
  <summary>Details</summary>
Motivation: 苹果Find My系统虽然声称隐私安全，但代码闭源且可能存在逻辑漏洞，需要独立验证其安全声明。

Method: 使用符号建模方法对Find My协议进行建模，在Tamarin验证器中构建精确的形式化规范并进行自动化机器验证。

Result: 通过自动化验证提供了机器可检查的安全性证明，确认了Find My系统的隐私保护属性。

Conclusion: 形式化验证证实了Find My系统的隐私安全声明，为闭源系统的安全评估提供了可靠方法。

Abstract: Tracking devices, while designed to help users find their belongings in case
of loss/theft, bring in new questions about privacy and surveillance of not
just their own users, but in the case of crowd-sourced location tracking, even
that of others even orthogonally associated with these platforms. Apple's Find
My is perhaps the most ubiquitous such system which can even locate devices
which do not possess any cellular support or GPS, running on millions of
devices worldwide. Apple claims that this system is private and secure, but the
code is proprietary, and such claims have to be taken on faith. It is well
known that even with perfect cryptographic guarantees, logical flaws might
creep into protocols, and allow undesirable attacks. In this paper, we present
a symbolic model of the Find My protocol, as well as a precise formal
specification of desirable properties, and provide automated, machine-checkable
proofs of these properties in the Tamarin prover.

</details>


### [37] [Improving Cybercrime Detection and Digital Forensics Investigations with Artificial Intelligence](https://arxiv.org/abs/2510.14638)
*Silvia Lucia Sanna,Leonardo Regano,Davide Maiorca,Giorgio Giacinto*

Main category: cs.CR

TL;DR: 该论文探讨了如何利用人工智能改进网络犯罪分析和数字取证，同时指出网络犯罪分子也可能利用AI技术来增强攻击能力。通过案例研究展示了使用流行聊天机器人开发隐写术代码的可能性。


<details>
  <summary>Details</summary>
Motivation: 欧洲网络犯罪频发，需要改进现有的预防、检测和分析措施。AI技术有潜力提升网络犯罪检测和数字取证分析的效率，但同时也可能被犯罪分子利用。

Method: 提出将AI作为辅助工具集成到网络犯罪检测和数字取证系统中。通过案例研究，展示了使用Gemini、Copilot和chatGPT等聊天机器人开发Python隐写术代码的方法。

Result: 研究表明AI可以有效改进网络犯罪分析和数字取证程序，但同时也存在被网络犯罪分子利用的风险。案例研究成功演示了利用AI工具开发隐写术技术。

Conclusion: AI在网络犯罪分析和数字取证中具有双重作用：一方面可以提升防御能力，另一方面也可能被犯罪分子用于增强攻击技术和反取证手段。需要平衡AI技术的应用与安全风险。

Abstract: According to a recent EUROPOL report, cybercrime is still recurrent in
Europe, and different activities and countermeasures must be taken to limit,
prevent, detect, analyze, and fight it. Cybercrime must be prevented with
specific measures, tools, and techniques, for example through automated network
and malware analysis. Countermeasures against cybercrime can also be improved
with proper \df analysis in order to extract data from digital devices trying
to retrieve information on the cybercriminals. Indeed, results obtained through
a proper \df analysis can be leveraged to train cybercrime detection systems to
prevent the success of similar crimes. Nowadays, some systems have started to
adopt Artificial Intelligence (AI) algorithms for cyberattack detection and \df
analysis improvement. However, AI can be better applied as an additional
instrument in these systems to improve the detection and in the \df analysis.
For this reason, we highlight how cybercrime analysis and \df procedures can
take advantage of AI. On the other hand, cybercriminals can use these systems
to improve their skills, bypass automatic detection, and develop advanced
attack techniques. The case study we presented highlights how it is possible to
integrate the use of the three popular chatbots {\tt Gemini}, {\tt Copilot} and
{\tt chatGPT} to develop a Python code to encode and decoded images with
steganographic technique, even though their presence is not an indicator of
crime, attack or maliciousness but used by a cybercriminal as anti-forensics
technique.

</details>


### [38] [AEX-NStep: Probabilistic Interrupt Counting Attacks on Intel SGX](https://arxiv.org/abs/2510.14675)
*Nicolas Dutly,Friederike Groschupp,Ivan Puddu,Kari Kostiainen,Srdjan Capkun*

Main category: cs.CR

TL;DR: AEX-NStep是首个针对AEX-Notify的终端计数攻击，证明AEX-Notify无法完全防止此类攻击，并成功实施了ECDSA密钥泄露攻击。


<details>
  <summary>Details</summary>
Motivation: Intel引入AEX-Notify来缓解基于中断的单步攻击，但作者发现其安全保证存在缺陷，需要验证其实际防护效果。

Method: 开发了AEX-NStep攻击，包含两种概率性中断计数攻击方法，并针对AEX-Notify保护的SGX终端实施ECDSA密钥泄露攻击。

Result: 证明AEX-Notify的混淆前向进度安全保证不成立，成功实现了实用的ECDSA密钥泄露攻击。

Conclusion: AEX-Notify不能完全防止中断计数攻击，研究结果扩展了其安全分析并为未来缓解措施设计提供了参考。

Abstract: To mitigate interrupt-based stepping attacks (notably using SGX-Step), Intel
introduced AEX-Notify, an ISA extension to Intel SGX that aims to prevent
deterministic single-stepping. In this work, we introduce AEX-NStep, the first
interrupt counting attack on AEX-Notify-enabled Enclaves. We show that
deterministic single-stepping is not required for interrupt counting attacks to
be practical and that, therefore, AEX-Notify does not entirely prevent such
attacks. We specifically show that one of AEX-Notify's security guarantees,
obfuscated forward progress, does not hold, and we introduce two new
probabilistic interrupt counting attacks. We use these attacks to construct a
practical ECDSA key leakage attack on an AEX-Notify-enabled SGX enclave. Our
results extend the original security analysis of AEX-Notify and inform the
design of future mitigations.

</details>


### [39] [FibRace: a large-scale benchmark of client-side proving on mobile devices](https://arxiv.org/abs/2510.14693)
*Simon Malatrait,Alex Sirac*

Main category: cs.CR

TL;DR: FibRace是首个在智能手机上使用Cairo M进行客户端证明生成的大规模实验，通过移动游戏形式让玩家证明斐波那契数并参与排行榜竞争，验证了移动设备能够可靠生成零知识证明。


<details>
  <summary>Details</summary>
Motivation: 测试智能手机客户端证明生成的可行性，为轻量级证明器、证明驱动的基础设施和隐私保护移动应用提供实践基准。

Method: 开发移动游戏FibRace，让玩家在智能手机上使用Cairo M生成斐波那契数证明，收集6047名玩家在1420种设备型号上生成的2195488个证明数据。

Result: 大多数现代智能手机能在5秒内完成证明，确认移动设备无需远程证明器或专用硬件即可可靠生成零知识证明。性能主要与RAM容量和SoC性能相关，至少3GB RAM的设备能稳定证明，苹果A19 Pro和M系列芯片证明时间最快。

Conclusion: 移动设备现已具备可靠生成零知识证明的能力，FibRace提供了迄今为止最全面的移动证明性能数据集，为未来研究建立了实践基准。

Abstract: FibRace, jointly developed by KKRT Labs and Hyli, was the first large-scale
experiment to test client-side proof generation on smartphones using Cairo M.
Presented as a mobile game in which players proved Fibonacci numbers and
climbed a leaderboard, FibRace served a dual purpose: to engage the public and
to provide empirical benchmarking. Over a three-week campaign (September 11-30,
2025), 6,047 players across 99 countries generated 2,195,488 proofs on 1,420
unique device models. The results show that most modern smartphones can
complete a proof in under 5 seconds, confirming that *mobile devices are now
capable of producing zero-knowledge proofs reliably*, without the need for
remote provers or specialized hardware. Performance was correlated primarily
with RAM capacity and SoC (System on Chip) performance: devices with at least 3
GB of RAM proved stably, when Apple's A19 Pro and M-series chips achieved the
fastest proving times. Hyli's blockchain natively verified every proof onchain
without congestion. FibRace provides the most comprehensive dataset to date on
mobile proving performance, establishing a practical baseline for future
research in lightweight provers, proof-powered infrastructure, and
privacy-preserving mobile applications.

</details>


### [40] [SLIE: A Secure and Lightweight Cryptosystem for Data Sharing in IoT Healthcare Services](https://arxiv.org/abs/2510.14708)
*Ha Xuan Son,Nguyen Quoc Anh,Phat T. Tran-Truong,Le Thanh Tuan,Pham Thanh Nghiem*

Main category: cs.CR

TL;DR: SLIE是一种基于WKD-IBE的新型密码系统，为IoMT提供安全轻量级身份加密，显著提升加密性能并确保医疗数据安全。


<details>
  <summary>Details</summary>
Motivation: IoMT的服务导向模型在设备管理和通信中引入了严重的安全漏洞，特别是医疗数据的敏感性使这些风险尤为关键。

Method: 提出SLIE系统，基于WKD-IBE密码系统，采用端到端加密、分层访问控制和轻量级密钥管理，包含恒定时间操作、内存混淆和基于过期的密钥撤销机制。

Result: SLIE显著优于RSA，1KB数据的加密和解密时间分别为0.936ms和0.217ms，加密速度提升84.54%，解密速度提升99.70%，能效为0.014 J/KB。

Conclusion: SLIE为资源受限的IoMT设备提供了可扩展的信任和安全的全向通信，有效抵御侧信道、中间人和未授权访问攻击，确保符合HIPAA和GDPR标准。

Abstract: The Internet of Medical Things (IoMT) has revolutionized healthcare by
transforming medical operations into standardized, interoperable services.
However, this service-oriented model introduces significant security
vulnerabilities in device management and communication, which are especially
critical given the sensitivity of medical data. To address these risks, this
paper proposes SLIE (Secure and Lightweight Identity Encryption), a novel
cryptosystem based on Wildcard Key Derivation Identity-Based Encryption
(WKD-IBE). SLIE ensures scalable trust and secure omnidirectional communication
through end-to-end encryption, hierarchical access control, and a lightweight
key management system designed for resource-constrained devices. It
incorporates constant-time operations, memory obfuscation, and expiry-based key
revocation to counter side-channel, man-in-the-middle, and unauthorized access
attacks, thereby ensuring compliance with standards like HIPAA and GDPR.
Evaluations show that SLIE significantly outperforms RSA, with encryption and
decryption times of 0.936ms and 0.217ms for 1KB of data, an 84.54% improvement
in encryption speed, a 99.70% improvement in decryption speed, and an energy
efficiency of 0.014 J/KB.

</details>


### [41] [Secure Sparse Matrix Multiplications and their Applications to Privacy-Preserving Machine Learning](https://arxiv.org/abs/2510.14894)
*Marc Damie,Florian Hahn,Andreas Peter,Jan Ramon*

Main category: cs.CR

TL;DR: 提出了针对稀疏数据的多方计算矩阵乘法算法，解决了现有MPC框架在处理高维稀疏数据时的内存和通信效率问题。


<details>
  <summary>Details</summary>
Motivation: 现有的多方计算框架没有针对稀疏数据进行优化，无法有效处理推荐系统、基因组学等涉及高维稀疏数据的机器学习应用，因为密集数据表示会导致内存需求过大。

Method: 设计了秘密稀疏矩阵乘法算法，避免使用密集数据表示，并采用基于非零元素分布的安全上界方法。

Result: 算法显著减少了通信成本（某些实验显示可达1000倍），并在现有协议不实用的两个机器学习应用中验证了有效性。

Conclusion: 提出的稀疏矩阵乘法算法解决了MPC处理稀疏数据时的内存和效率问题，同时提供了更符合统计现实的安全上界方法。

Abstract: To preserve privacy, multi-party computation (MPC) enables executing Machine
Learning (ML) algorithms on secret-shared or encrypted data. However, existing
MPC frameworks are not optimized for sparse data. This makes them unsuitable
for ML applications involving sparse data, e.g., recommender systems or
genomics. Even in plaintext, such applications involve high-dimensional sparse
data, that cannot be processed without sparsity-related optimizations due to
prohibitively large memory requirements.
  Since matrix multiplication is central in ML algorithms, we propose MPC
algorithms to multiply secret sparse matrices. On the one hand, our algorithms
avoid the memory issues of the "dense" data representation of classic secure
matrix multiplication algorithms. On the other hand, our algorithms can
significantly reduce communication costs (some experiments show a factor 1000)
for realistic problem sizes. We validate our algorithms in two ML applications
in which existing protocols are impractical.
  An important question when developing MPC algorithms is what assumptions can
be made. In our case, if the number of non-zeros in a row is a sensitive piece
of information then a short runtime may reveal that the number of non-zeros is
small. Existing approaches make relatively simple assumptions, e.g., that there
is a universal upper bound to the number of non-zeros in a row. This often
doesn't align with statistical reality, in a lot of sparse datasets the amount
of data per instance satisfies a power law. We propose an approach which allows
adopting a safe upper bound on the distribution of non-zeros in rows/columns of
sparse matrices.

</details>


### [42] [A Hard-Label Black-Box Evasion Attack against ML-based Malicious Traffic Detection Systems](https://arxiv.org/abs/2510.14906)
*Zixuan Liu,Yi Zhao,Zhuotao Liu,Qi Li,Chuanpu Fu,Guangmeng Zhou,Ke Xu*

Main category: cs.CR

TL;DR: NetMasquerade是一个基于强化学习的恶意流量伪装系统，通过模仿良性流量模式来逃避机器学习检测，在80种攻击场景下对6种现有检测方法的逃避成功率超过96.65%。


<details>
  <summary>Details</summary>
Motivation: 现有的恶意流量逃避攻击要么依赖过于严格的条件（如加密协议、Tor或特殊设置），要么需要目标的详细信息（如训练数据和模型参数），这在现实的黑盒场景中不实用。硬标签黑盒逃避攻击的可行性仍然是一个开放挑战。

Method: 开发了NetMasquerade系统，利用强化学习操纵攻击流量模仿良性流量。首先建立了专门预训练的Traffic-BERT模型，使用网络专用分词器和注意力机制提取多样化的良性流量模式，然后将Traffic-BERT集成到RL框架中，基于良性流量模式以最小修改有效操纵恶意数据包序列。

Result: 实验结果表明，NetMasquerade能够使暴力攻击和隐蔽攻击在80种攻击场景下逃避6种现有检测方法，攻击成功率超过96.65%。它能够逃避那些对现有逃避攻击具有经验性或可证明鲁棒性的方法，并且实现了低延迟的对抗流量生成。

Conclusion: NetMasquerade证明了在现实黑盒场景中实现硬标签黑盒逃避攻击的可行性，为恶意流量检测系统的鲁棒性评估提供了重要参考，展示了强化学习在网络安全对抗中的实用价值。

Abstract: Machine Learning (ML)-based malicious traffic detection is a promising
security paradigm. It outperforms rule-based traditional detection by
identifying various advanced attacks. However, the robustness of these ML
models is largely unexplored, thereby allowing attackers to craft adversarial
traffic examples that evade detection. Existing evasion attacks typically rely
on overly restrictive conditions (e.g., encrypted protocols, Tor, or
specialized setups), or require detailed prior knowledge of the target (e.g.,
training data and model parameters), which is impractical in realistic
black-box scenarios. The feasibility of a hard-label black-box evasion attack
(i.e., applicable across diverse tasks and protocols without internal target
insights) thus remains an open challenge. To this end, we develop
NetMasquerade, which leverages reinforcement learning (RL) to manipulate attack
flows to mimic benign traffic and evade detection. Specifically, we establish a
tailored pre-trained model called Traffic-BERT, utilizing a network-specialized
tokenizer and an attention mechanism to extract diverse benign traffic
patterns. Subsequently, we integrate Traffic-BERT into the RL framework,
allowing NetMasquerade to effectively manipulate malicious packet sequences
based on benign traffic patterns with minimal modifications. Experimental
results demonstrate that NetMasquerade enables both brute-force and stealthy
attacks to evade 6 existing detection methods under 80 attack scenarios,
achieving over 96.65% attack success rate. Notably, it can evade the methods
that are either empirically or certifiably robust against existing evasion
attacks. Finally, NetMasquerade achieves low-latency adversarial traffic
generation, demonstrating its practicality in real-world scenarios.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [43] [Decision Oriented Technique (DOTechnique): Finding Model Validity Through Decision-Maker Context](https://arxiv.org/abs/2510.13858)
*Raheleh Biglari,Joachim Denil*

Main category: cs.AI

TL;DR: 提出了DOTechnique方法，通过决策一致性而非输出相似性来确定模型有效性，在缺乏明确有效性边界时仍能高效识别有效区域。


<details>
  <summary>Details</summary>
Motivation: 模型有效性对决策过程至关重要，但传统方法依赖预定义的有效性框架，这些框架可能不可用或不充分。

Method: 引入决策导向技术(DOTechnique)，通过评估替代模型是否与高保真模型产生等效决策来确定模型有效性，集成领域约束和符号推理来缩小搜索空间。

Result: 以高速公路变道系统为例，展示了DOTechnique如何揭示仿真模型的有效性区域。

Conclusion: 该技术有潜力通过决策者上下文来支持发现模型有效性。

Abstract: Model validity is as critical as the model itself, especially when guiding
decision-making processes. Traditional approaches often rely on predefined
validity frames, which may not always be available or sufficient. This paper
introduces the Decision Oriented Technique (DOTechnique), a novel method for
determining model validity based on decision consistency rather than output
similarity. By evaluating whether surrogate models lead to equivalent decisions
compared to high-fidelity models, DOTechnique enables efficient identification
of validity regions, even in the absence of explicit validity boundaries. The
approach integrates domain constraints and symbolic reasoning to narrow the
search space, enhancing computational efficiency. A highway lane change system
serves as a motivating example, demonstrating how DOTechnique can uncover the
validity region of a simulation model. The results highlight the potential of
the technique to support finding model validity through decision-maker context.

</details>


### [44] [Do Slides Help? Multi-modal Context for Automatic Transcription of Conference Talks](https://arxiv.org/abs/2510.13979)
*Supriti Sinhamahapatra,Jan Niehues*

Main category: cs.AI

TL;DR: 该论文提出了一种融合视觉信息（特别是演示文稿幻灯片）的多模态语音识别方法，在科学演示场景中显著提升了识别准确率，特别是对领域特定术语的识别。


<details>
  <summary>Details</summary>
Motivation: 当前最先进的语音识别系统主要依赖音频信息而忽略了多模态上下文。视觉信息对于消除歧义和适应场景至关重要，特别是在科学演示场景中，演示文稿幻灯片包含重要信息。

Method: 首先创建了多模态演示基准，包括领域特定术语的自动分析；然后探索了用多模态信息增强语音模型的方法；通过数据增强方法解决缺乏配套幻灯片数据集的问题；最后使用增强数据集训练模型。

Result: 训练得到的模型相比基线模型，在所有词汇上的词错误率相对降低了约34%，在领域特定术语上的词错误率相对降低了约35%。

Conclusion: 集成演示文稿幻灯片等视觉信息可以显著提升语音识别系统的性能，特别是在处理领域特定术语时效果更加明显。

Abstract: State-of-the-art (SOTA) Automatic Speech Recognition (ASR) systems primarily
rely on acoustic information while disregarding additional multi-modal context.
However, visual information are essential in disambiguation and adaptation.
While most work focus on speaker images to handle noise conditions, this work
also focuses on integrating presentation slides for the use cases of scientific
presentation.
  In a first step, we create a benchmark for multi-modal presentation including
an automatic analysis of transcribing domain-specific terminology. Next, we
explore methods for augmenting speech models with multi-modal information. We
mitigate the lack of datasets with accompanying slides by a suitable approach
of data augmentation. Finally, we train a model using the augmented dataset,
resulting in a relative reduction in word error rate of approximately 34%,
across all words and 35%, for domain-specific terms compared to the baseline
model.

</details>


### [45] [Do Large Language Models Show Biases in Causal Learning? Insights from Contingency Judgment](https://arxiv.org/abs/2510.13985)
*María Victoria Carro,Denise Alejandra Mester,Francisca Gauna Selasco,Giovanni Franco Gabriel Marraffini,Mario Alejandro Leiva,Gerardo I. Simari,María Vanina Martinez*

Main category: cs.AI

TL;DR: 研究发现大型语言模型在因果推理任务中容易产生因果幻觉，即使在没有足够证据支持的情况下也会错误地推断因果关系。


<details>
  <summary>Details</summary>
Motivation: 探讨大型语言模型是否会在经典的认知科学范式——列联判断任务中产生因果幻觉，这种认知偏差可能对社会问题如偏见、刻板印象和迷信思维产生影响。

Method: 构建了包含1000个零列联场景的医疗数据集，让LLMs评估潜在原因的有效性，这些场景中的信息不足以建立变量间的因果关系。

Result: 所有评估的模型都系统地推断出无根据的因果关系，显示出对因果幻觉的强烈易感性。

Conclusion: 研究结果支持LLMs只是复制因果语言而非真正理解因果关系的假设，对在需要准确因果推理的领域中使用语言模型提出了担忧。

Abstract: Causal learning is the cognitive process of developing the capability of
making causal inferences based on available information, often guided by
normative principles. This process is prone to errors and biases, such as the
illusion of causality, in which people perceive a causal relationship between
two variables despite lacking supporting evidence. This cognitive bias has been
proposed to underlie many societal problems, including social prejudice,
stereotype formation, misinformation, and superstitious thinking. In this work,
we examine whether large language models are prone to developing causal
illusions when faced with a classic cognitive science paradigm: the contingency
judgment task. To investigate this, we constructed a dataset of 1,000 null
contingency scenarios (in which the available information is not sufficient to
establish a causal relationship between variables) within medical contexts and
prompted LLMs to evaluate the effectiveness of potential causes. Our findings
show that all evaluated models systematically inferred unwarranted causal
relationships, revealing a strong susceptibility to the illusion of causality.
While there is ongoing debate about whether LLMs genuinely understand causality
or merely reproduce causal language without true comprehension, our findings
support the latter hypothesis and raise concerns about the use of language
models in domains where accurate causal reasoning is essential for informed
decision-making.

</details>


### [46] [GammaZero: Learning To Guide POMDP Belief Space Search With Graph Representations](https://arxiv.org/abs/2510.14035)
*Rajesh Mangannavar,Prasad Tadepalli*

Main category: cs.AI

TL;DR: GammaZero是一个基于动作中心图表示框架的POMDP规划学习方法，能够通过在小问题上学习图结构模式，实现向更大规模问题的零样本泛化。


<details>
  <summary>Details</summary>
Motivation: 现有方法需要领域特定的神经网络架构且难以扩展，GammaZero旨在开发一个统一的基于图的信念表示框架，实现跨问题规模的泛化能力。

Method: 将信念状态系统性地转换为动作中心图，使用图神经网络和编码器架构从专家演示中学习价值函数和策略，然后应用这些学习到的启发式来指导更大问题上的蒙特卡洛树搜索。

Result: 在标准POMDP基准测试中，GammaZero在相同规模问题上与BetaZero性能相当，同时能够零样本泛化到训练时未见过的2-4倍大规模问题，在保持解质量的同时减少搜索需求。

Conclusion: GammaZero通过动作中心图表示框架成功实现了POMDP规划中的跨规模泛化，为可扩展的规划学习提供了有效解决方案。

Abstract: We introduce an action-centric graph representation framework for learning to
guide planning in Partially Observable Markov Decision Processes (POMDPs).
Unlike existing approaches that require domain-specific neural architectures
and struggle with scalability, GammaZero leverages a unified graph-based belief
representation that enables generalization across problem sizes within a
domain. Our key insight is that belief states can be systematically transformed
into action-centric graphs where structural patterns learned on small problems
transfer to larger instances. We employ a graph neural network with a decoder
architecture to learn value functions and policies from expert demonstrations
on computationally tractable problems, then apply these learned heuristics to
guide Monte Carlo tree search on larger problems. Experimental results on
standard POMDP benchmarks demonstrate that GammaZero achieves comparable
performance to BetaZero when trained and tested on the same-sized problems,
while uniquely enabling zero-shot generalization to problems 2-4 times larger
than those seen during training, maintaining solution quality with reduced
search requirements.

</details>


### [47] [Position: Require Frontier AI Labs To Release Small "Analog" Models](https://arxiv.org/abs/2510.14053)
*Shriyash Upadhyay,Chaithanya Bandi,Narmeen Oozeer,Philip Quirke*

Main category: cs.AI

TL;DR: 提出一种替代性AI监管方法：要求大型AI实验室发布小型开放类比模型，这些模型是从其最大专有模型蒸馏训练而来的缩小版本，既能确保AI安全又促进创新。


<details>
  <summary>Details</summary>
Motivation: 现有前沿AI模型监管提案因安全与创新的权衡而被搁置，需要找到既能确保AI安全又不阻碍创新的监管方法。

Method: 强制要求大型AI实验室发布小型开放类比模型，这些模型作为公共代理，允许广泛参与安全验证、可解释性研究和算法透明度工作，而无需披露全规模模型。

Result: 研究表明，使用这些小型模型开发的安全和可解释性方法能有效推广到前沿规模系统，显著降低监管负担并加速安全进展。

Conclusion: 这种监管方法以最小额外成本显著促进公共福祉，并说明对模型的深入理解能够缓解安全与创新的权衡，实现两者的双赢。

Abstract: Recent proposals for regulating frontier AI models have sparked concerns
about the cost of safety regulation, and most such regulations have been
shelved due to the safety-innovation tradeoff. This paper argues for an
alternative regulatory approach that ensures AI safety while actively promoting
innovation: mandating that large AI laboratories release small, openly
accessible analog models (scaled-down versions) trained similarly to and
distilled from their largest proprietary models.
  Analog models serve as public proxies, allowing broad participation in safety
verification, interpretability research, and algorithmic transparency without
forcing labs to disclose their full-scale models. Recent research demonstrates
that safety and interpretability methods developed using these smaller models
generalize effectively to frontier-scale systems. By enabling the wider
research community to directly investigate and innovate upon accessible
analogs, our policy substantially reduces the regulatory burden and accelerates
safety advancements.
  This mandate promises minimal additional costs, leveraging reusable resources
like data and infrastructure, while significantly contributing to the public
good. Our hope is not only that this policy be adopted, but that it illustrates
a broader principle supporting fundamental research in machine learning: deeper
understanding of models relaxes the safety-innovation tradeoff and lets us have
more of both.

</details>


### [48] [Generating Fair Consensus Statements with Social Choice on Token-Level MDPs](https://arxiv.org/abs/2510.14106)
*Carter Blair,Kate Larson*

Main category: cs.AI

TL;DR: 本文提出了一种基于多目标马尔可夫决策过程的共识声明生成框架，通过社会选择理论原则确保公平性保证，包括两种方法：随机生成策略保证ex-ante核心稳定性，以及基于平等主义福利的搜索算法。


<details>
  <summary>Details</summary>
Motivation: 当前基于大语言模型的共识声明生成框架缺乏提供可证明公平性保证的内在结构，无法在聚合多样化自由形式意见时确保公平性。

Method: 将任务建模为多目标、令牌级马尔可夫决策过程，每个目标对应一个代理的偏好。提出两种基于社会选择理论的方法：一是保证ex-ante核心稳定性的随机生成策略，二是基于平等主义福利最大化的搜索算法。

Result: 实验表明，使用语言模型实例化代理策略时，基于平等主义目标的搜索生成的共识声明在代理对齐的最坏情况下优于基线方法，包括Habermas Machine。

Conclusion: 该框架为共识声明生成提供了具有可证明公平性保证的结构化方法，通过社会选择理论原则确保公平聚合多样化意见。

Abstract: Current frameworks for consensus statement generation with large language
models lack the inherent structure needed to provide provable fairness
guarantees when aggregating diverse free-form opinions. We model the task as a
multi-objective, token-level Markov Decision Process (MDP), where each
objective corresponds to an agent's preference. Token-level rewards for each
agent are derived from their policy (e.g., a personalized language model). This
approach utilizes the finding that such policies implicitly define optimal
Q-functions, providing a principled way to quantify rewards at each generation
step without a value function (Rafailov et al., 2024). This MDP formulation
creates a formal structure amenable to analysis using principles from social
choice theory. We propose two approaches grounded in social choice theory.
First, we propose a stochastic generation policy guaranteed to be in the
ex-ante core, extending core stability concepts from voting theory to text
generation. This policy is derived from an underlying distribution over
complete statements that maximizes proportional fairness (Nash Welfare).
Second, for generating a single statement, we target the maximization of
egalitarian welfare using search algorithms within the MDP framework.
Empirically, experiments using language models to instantiate agent policies
show that search guided by the egalitarian objective generates consensus
statements with improved worst-case agent alignment compared to baseline
methods, including the Habermas Machine (Tessler et al., 2024).

</details>


### [49] [STEMS: Spatial-Temporal Enhanced Safe Multi-Agent Coordination for Building Energy Management](https://arxiv.org/abs/2510.14112)
*Huiliang Zhang,Di Wu,Arnaud Zinflou,Benoit Boulet*

Main category: cs.AI

TL;DR: STEMS是一个用于协调建筑能源管理的安全约束多智能体强化学习框架，通过空间-时间图表示学习和控制屏障函数实现安全保证，在真实数据集上显示21%成本降低、18%排放减少，并将安全违规从35.1%降至5.6%。


<details>
  <summary>Details</summary>
Motivation: 建筑能源管理对实现碳减排目标、提高居住者舒适度和降低能源成本至关重要。当前多建筑能源系统面临三个关键挑战：空间-时间信息利用不足、缺乏严格安全保证以及系统复杂性。

Method: 提出STEMS框架，包含两个核心组件：(1)使用GCN-Transformer融合架构的空间-时间图表示学习框架，捕捉建筑间关系和时序模式；(2)结合控制屏障函数的安全约束多智能体RL算法，提供数学安全保证。

Result: 在真实建筑数据集上的实验显示，STEMS相比现有方法实现21%成本降低、18%排放减少，安全违规从35.1%大幅降至5.6%，同时保持最佳舒适度（仅0.13不适比例）。在极端天气条件下表现出强鲁棒性，在不同建筑类型中保持有效性。

Conclusion: STEMS框架成功解决了多建筑能源管理中的空间-时间依赖性和安全约束问题，为协调建筑能源管理提供了有效的解决方案，在成本、排放和安全方面均取得显著改进。

Abstract: Building energy management is essential for achieving carbon reduction goals,
improving occupant comfort, and reducing energy costs. Coordinated building
energy management faces critical challenges in exploiting spatial-temporal
dependencies while ensuring operational safety across multi-building systems.
Current multi-building energy systems face three key challenges: insufficient
spatial-temporal information exploitation, lack of rigorous safety guarantees,
and system complexity. This paper proposes Spatial-Temporal Enhanced Safe
Multi-Agent Coordination (STEMS), a novel safety-constrained multi-agent
reinforcement learning framework for coordinated building energy management.
STEMS integrates two core components: (1) a spatial-temporal graph
representation learning framework using a GCN-Transformer fusion architecture
to capture inter-building relationships and temporal patterns, and (2) a
safety-constrained multi-agent RL algorithm incorporating Control Barrier
Functions to provide mathematical safety guarantees. Extensive experiments on
real-world building datasets demonstrate STEMS's superior performance over
existing methods, showing that STEMS achieves 21% cost reduction, 18% emission
reduction, and dramatically reduces safety violations from 35.1% to 5.6% while
maintaining optimal comfort with only 0.13 discomfort proportion. The framework
also demonstrates strong robustness during extreme weather conditions and
maintains effectiveness across different building types.

</details>


### [50] [Formalizing the Safety, Security, and Functional Properties of Agentic AI Systems](https://arxiv.org/abs/2510.14133)
*Edoardo Allegrini,Ananth Shreekumar,Z. Berkay Celik*

Main category: cs.AI

TL;DR: 提出了一个建模框架来解决多AI代理系统中的语义鸿沟问题，包含主机代理模型和任务生命周期模型，定义了31个系统属性用于形式化验证。


<details>
  <summary>Details</summary>
Motivation: 当前多代理AI系统的通信协议碎片化，导致系统属性无法严格分析，存在架构错位和可利用的协调问题风险。

Method: 引入两个基础模型：主机代理模型（负责与用户交互、任务分解和编排）和任务生命周期模型（详细描述子任务的状态转换），并定义了17个主机代理属性和14个任务生命周期属性。

Result: 建立了第一个严格基础的、领域无关的框架，用于系统分析、设计和部署正确、可靠、鲁棒的代理AI系统。

Conclusion: 该统一语义框架能够对多AI代理系统行为进行推理，通过时序逻辑表达属性，实现形式化验证、协调边缘情况检测以及死锁和安全漏洞预防。

Abstract: Agentic AI systems, which leverage multiple autonomous agents and Large
Language Models (LLMs), are increasingly used to address complex, multi-step
tasks. The safety, security, and functionality of these systems are critical,
especially in high-stakes applications. However, the current ecosystem of
inter-agent communication is fragmented, with protocols such as the Model
Context Protocol (MCP) for tool access and the Agent-to-Agent (A2A) protocol
for coordination being analyzed in isolation. This fragmentation creates a
semantic gap that prevents the rigorous analysis of system properties and
introduces risks such as architectural misalignment and exploitable
coordination issues. To address these challenges, we introduce a modeling
framework for agentic AI systems composed of two foundational models. The
first, the host agent model, formalizes the top-level entity that interacts
with the user, decomposes tasks, and orchestrates their execution by leveraging
external agents and tools. The second, the task lifecycle model, details the
states and transitions of individual sub-tasks from creation to completion,
providing a fine-grained view of task management and error handling. Together,
these models provide a unified semantic framework for reasoning about the
behavior of multi-AI agent systems. Grounded in this framework, we define 17
properties for the host agent and 14 for the task lifecycle, categorized into
liveness, safety, completeness, and fairness. Expressed in temporal logic,
these properties enable formal verification of system behavior, detection of
coordination edge cases, and prevention of deadlocks and security
vulnerabilities. Through this effort, we introduce the first rigorously
grounded, domain-agnostic framework for the systematic analysis, design, and
deployment of correct, reliable, and robust agentic AI systems.

</details>


### [51] [A Multimodal Approach to Heritage Preservation in the Context of Climate Change](https://arxiv.org/abs/2510.14136)
*David Roqui,Adèle Cormier,nistor Grozavu,Ann Bourges*

Main category: cs.AI

TL;DR: 提出轻量级多模态架构，融合传感器数据和视觉图像预测文化遗产退化程度，在数据稀缺情况下实现76.9%准确率


<details>
  <summary>Details</summary>
Motivation: 文化遗产因气候变化加速退化，传统单模态监测无法捕捉环境压力与材料退化间的复杂相互作用

Method: 采用改进的PerceiverIO架构，包含简化编码器和自适应Barlow Twins损失函数，鼓励模态互补性

Result: 在斯特拉斯堡大教堂数据上达到76.9%准确率，比标准多模态架构提升43%，比单模态方法显著改善

Conclusion: 架构简化与对比正则化结合可在数据稀缺的遗产监测中实现有效的多模态学习，为AI驱动的保护决策系统奠定基础

Abstract: Cultural heritage sites face accelerating degradation due to climate change,
yet tradi- tional monitoring relies on unimodal analysis (visual inspection or
environmental sen- sors alone) that fails to capture the complex interplay
between environmental stres- sors and material deterioration. We propose a
lightweight multimodal architecture that fuses sensor data (temperature,
humidity) with visual imagery to predict degradation severity at heritage
sites. Our approach adapts PerceiverIO with two key innovations: (1) simplified
encoders (64D latent space) that prevent overfitting on small datasets (n=37
training samples), and (2) Adaptive Barlow Twins loss that encourages modality
complementarity rather than redundancy. On data from Strasbourg Cathedral, our
model achieves 76.9% accu- racy, a 43% improvement over standard multimodal
architectures (VisualBERT, Trans- former) and 25% over vanilla PerceiverIO.
Ablation studies reveal that sensor-only achieves 61.5% while image-only
reaches 46.2%, confirming successful multimodal synergy. A systematic
hyperparameter study identifies an optimal moderate correlation target ({\tau}
=0.3) that balances align- ment and complementarity, achieving 69.2% accuracy
compared to other {\tau} values ({\tau} =0.1/0.5/0.7: 53.8%, {\tau} =0.9:
61.5%). This work demonstrates that architectural sim- plicity combined with
contrastive regularization enables effective multimodal learning in data-scarce
heritage monitoring contexts, providing a foundation for AI-driven con-
servation decision support systems.

</details>


### [52] [CodeEvolve: An open source evolutionary coding agent for algorithm discovery and optimization](https://arxiv.org/abs/2510.14150)
*Henrique Assumpção,Diego Ferreira,Leandro Campos,Fabricio Murai*

Main category: cs.AI

TL;DR: CodeEvolve是一个开源进化编码代理，将大语言模型与遗传算法结合解决复杂计算问题，在数学基准测试中超越了Google DeepMind的AlphaEvolve。


<details>
  <summary>Details</summary>
Motivation: 结合大语言模型与进化算法来解决复杂计算问题，并构建开源框架促进协作和加速进展。

Method: 采用基于岛屿的遗传算法保持种群多样性，引入基于启发的交叉机制利用LLM上下文窗口组合成功解决方案的特征，实现元提示策略动态探索解空间。

Result: 在用于评估Google DeepMind AlphaEvolve的数学基准测试子集上，CodeEvolve在多个挑战性问题上的表现超越了AlphaEvolve。

Conclusion: CodeEvolve成功地将进化概念应用于LLM领域，通过开源框架推动了该领域的发展，展示了在复杂问题求解上的优越性能。

Abstract: In this work, we introduce CodeEvolve, an open-source evolutionary coding
agent that unites Large Language Models (LLMs) with genetic algorithms to solve
complex computational problems. Our framework adapts powerful evolutionary
concepts to the LLM domain, building upon recent methods for generalized
scientific discovery. CodeEvolve employs an island-based genetic algorithm to
maintain population diversity and increase throughput, introduces a novel
inspiration-based crossover mechanism that leverages the LLMs context window to
combine features from successful solutions, and implements meta-prompting
strategies for dynamic exploration of the solution space. We conduct a rigorous
evaluation of CodeEvolve on a subset of the mathematical benchmarks used to
evaluate Google DeepMind's closed-source AlphaEvolve. Our findings show that
our method surpasses AlphaEvolve's performance on several challenging problems.
To foster collaboration and accelerate progress, we release our complete
framework as an open-source repository.

</details>


### [53] [Combining Reinforcement Learning and Behavior Trees for NPCs in Video Games with AMD Schola](https://arxiv.org/abs/2510.14154)
*Tian Liu,Alex Cann,Ian Colbert,Mehdi Saeedi*

Main category: cs.AI

TL;DR: 本文探讨了将强化学习与行为树结合用于游戏AI的可行性，通过在类似《最后生还者》的复杂3D环境中训练多任务NPC来验证该方法。


<details>
  <summary>Details</summary>
Motivation: 尽管强化学习研究进展迅速，但在商业游戏中的应用仍然缓慢。本文旨在解决游戏AI社区在实际使用RL驱动NPC时面临的挑战，并探索RL与传统行为树的结合点。

Method: 使用AMD Schola插件在虚幻引擎中训练RL智能体，创建多任务NPC，并展示联合训练RL模型与行为树的详细方法。

Result: 成功展示了在复杂3D环境中使用RL+BT方法训练多任务NPC的可行性，验证了这种结合方法的有效性。

Conclusion: 强化学习与行为树的结合是一个值得进一步探索的关键交叉点，虽然已有研究提出但实际应用仍然罕见，本文证明了该方法的实际可行性。

Abstract: While the rapid advancements in the reinforcement learning (RL) research
community have been remarkable, the adoption in commercial video games remains
slow. In this paper, we outline common challenges the Game AI community faces
when using RL-driven NPCs in practice, and highlight the intersection of RL
with traditional behavior trees (BTs) as a crucial juncture to be explored
further. Although the BT+RL intersection has been suggested in several research
papers, its adoption is rare. We demonstrate the viability of this approach
using AMD Schola -- a plugin for training RL agents in Unreal Engine -- by
creating multi-task NPCs in a complex 3D environment inspired by the commercial
video game ``The Last of Us". We provide detailed methodologies for jointly
training RL models with BTs while showcasing various skills.

</details>


### [54] [JEDA: Query-Free Clinical Order Search from Ambient Dialogues](https://arxiv.org/abs/2510.14169)
*Praphul Singh,Corey Barrett,Sumana Srivasta,Amitabh Saikia,Irfan Bulu,Sri Gadde,Krishnaram Kenthapadi*

Main category: cs.AI

TL;DR: JEDA是一个用于临床订单检索的双编码器系统，能够直接从对话中检索规范订单，无需依赖LLM重写，解决了延迟、不稳定和不透明的问题。


<details>
  <summary>Details</summary>
Motivation: 临床对话混合了显性指令和隐性推理，现有系统依赖LLM重写会带来延迟、不稳定和不透明性，阻碍实时订单处理。

Method: 使用PubMedBERT初始化的双编码器，通过重复安全的对比目标进行微调，将意图的异质表达对齐到共享订单概念。训练使用受限LLM指导将签名订单与补充表述绑定。

Result: JEDA在实践中取得了显著提升，大幅优于基础编码器和近期开源嵌入器，实现了快速、可解释、无需LLM的实时临床订单检索。

Conclusion: JEDA提供了一个快速、可解释、无需LLM的检索层，能够实时将环境上下文链接到可操作的临床订单。

Abstract: Clinical conversations mix explicit directives (order a chest X-ray) with
implicit reasoning (the cough worsened overnight, we should check for
pneumonia). Many systems rely on LLM rewriting, adding latency, instability,
and opacity that hinder real-time ordering. We present JEDA (Joint Embedding
for Direct and Ambient clinical orders), a domain-initialized bi-encoder that
retrieves canonical orders directly and, in a query-free mode, encodes a short
rolling window of ambient dialogue to trigger retrieval. Initialized from
PubMedBERT and fine-tuned with a duplicate-safe contrastive objective, JEDA
aligns heterogeneous expressions of intent to shared order concepts. Training
uses constrained LLM guidance to tie each signed order to complementary
formulations (command only, context only, command+context, context+reasoning),
producing clearer inter-order separation, tighter query extendash order
coupling, and stronger generalization. The query-free mode is noise-resilient,
reducing sensitivity to disfluencies and ASR errors by conditioning on a short
window rather than a single utterance. Deployed in practice, JEDA yields large
gains and substantially outperforms its base encoder and recent open embedders
(Linq Embed Mistral, SFR Embedding, GTE Qwen, BGE large, Embedding Gemma). The
result is a fast, interpretable, LLM-free retrieval layer that links ambient
context to actionable clinical orders in real time.

</details>


### [55] [ARM-FM: Automated Reward Machines via Foundation Models for Compositional Reinforcement Learning](https://arxiv.org/abs/2510.14176)
*Roger Creus Castanyer,Faisal Mohamed,Pablo Samuel Castro,Cyrus Neary,Glen Berseth*

Main category: cs.AI

TL;DR: ARM-FM是一个利用基础模型自动生成奖励机制的强化学习框架，能够从自然语言描述中自动构建奖励机器，实现任务分解和零样本泛化。


<details>
  <summary>Details</summary>
Motivation: 强化学习算法对奖励函数设计高度敏感，这限制了其广泛应用。传统方法需要手动设计奖励函数，过程复杂且容易出错。

Method: 使用基础模型从自然语言规范自动生成奖励机器；为每个奖励机器状态关联语言嵌入以实现跨任务泛化；利用奖励机器的结构化形式化实现有效任务分解。

Result: 在多个挑战性环境中验证了ARM-FM的有效性，包括展示了零样本泛化能力。

Conclusion: ARM-FM通过结合基础模型和奖励机器，为强化学习提供了一种自动化的、组合式的奖励设计方法，显著降低了奖励函数设计的复杂性。

Abstract: Reinforcement learning (RL) algorithms are highly sensitive to reward
function specification, which remains a central challenge limiting their broad
applicability. We present ARM-FM: Automated Reward Machines via Foundation
Models, a framework for automated, compositional reward design in RL that
leverages the high-level reasoning capabilities of foundation models (FMs).
Reward machines (RMs) -- an automata-based formalism for reward specification
-- are used as the mechanism for RL objective specification, and are
automatically constructed via the use of FMs. The structured formalism of RMs
yields effective task decompositions, while the use of FMs enables objective
specifications in natural language. Concretely, we (i) use FMs to automatically
generate RMs from natural language specifications; (ii) associate language
embeddings with each RM automata-state to enable generalization across tasks;
and (iii) provide empirical evidence of ARM-FM's effectiveness in a diverse
suite of challenging environments, including evidence of zero-shot
generalization.

</details>


### [56] [Implementation of AI in Precision Medicine](https://arxiv.org/abs/2510.14194)
*Göktuğ Bender,Samer Faraj,Anand Bhardwaj*

Main category: cs.AI

TL;DR: 本文对2019-2024年精准医学中AI实施的文献进行范围综述，识别了数据质量、临床可靠性、工作流程整合和治理等方面的关键障碍与促进因素，提出了支持可信和可持续实施的未来方向。


<details>
  <summary>Details</summary>
Motivation: AI在精准医学中日益重要，能够整合和解释多模态数据，但在临床环境中的实施仍然有限，需要系统分析实施障碍和促进因素。

Method: 采用生态系统框架进行范围综述，分析2019-2024年相关文献，识别关键实施因素。

Result: 识别了数据质量、临床可靠性、工作流程整合和治理四个维度的关键障碍和促进因素，强调了这些因素之间的相互依赖关系。

Conclusion: 需要通过生态系统视角理解AI在精准医学中的实施，提出支持可信和可持续实施的未来方向。

Abstract: Artificial intelligence (AI) has become increasingly central to precision
medicine by enabling the integration and interpretation of multimodal data, yet
implementation in clinical settings remains limited. This paper provides a
scoping review of literature from 2019-2024 on the implementation of AI in
precision medicine, identifying key barriers and enablers across data quality,
clinical reliability, workflow integration, and governance. Through an
ecosystem-based framework, we highlight the interdependent relationships
shaping real-world translation and propose future directions to support
trustworthy and sustainable implementation.

</details>


### [57] [Echoes of Human Malice in Agents: Benchmarking LLMs for Multi-Turn Online Harassment Attacks](https://arxiv.org/abs/2510.14207)
*Trilok Padhi,Pinxian Lu,Abdulkadir Erol,Tanmay Sutar,Gauri Sharma,Mina Sonmez,Munmun De Choudhury,Ugur Kursuncu*

Main category: cs.AI

TL;DR: 提出了一个在线骚扰代理基准测试，包含多轮骚扰对话数据集、基于重复博弈论的多代理模拟、三种越狱攻击方法以及混合评估框架。研究发现越狱调优使骚扰成功率大幅提升，闭源和开源模型表现出不同的升级轨迹。


<details>
  <summary>Details</summary>
Motivation: 现有越狱研究主要关注单轮提示，而真实骚扰通常发生在多轮交互中。需要开发能够模拟真实多轮骚扰动态的基准测试。

Method: 构建了包含合成多轮骚扰对话数据集、基于重复博弈论的多代理模拟、三种针对记忆、规划和微调的越狱攻击方法，以及混合评估框架的基准测试。使用LLaMA-3.1-8B-Instruct和Gemini-2.0-flash两种模型进行测试。

Result: 越狱调优使骚扰成功率从57.25-64.19%提升至95.78-96.89%（Llama），从98.46%提升至99.33%（Gemini）。拒绝率降至1-2%。最常见的毒性行为是侮辱（84.9-87.8% vs 44.2-50.8%）和谩骂（81.2-85.1% vs 31.5-38.8%）。闭源模型表现出显著脆弱性。

Conclusion: 多轮和理论基础的攻击不仅成功率高，还能模拟人类骚扰动态，强调需要开发强大的安全防护措施来保护在线平台的安全和负责任性。

Abstract: Large Language Model (LLM) agents are powering a growing share of interactive
web applications, yet remain vulnerable to misuse and harm. Prior jailbreak
research has largely focused on single-turn prompts, whereas real harassment
often unfolds over multi-turn interactions. In this work, we present the Online
Harassment Agentic Benchmark consisting of: (i) a synthetic multi-turn
harassment conversation dataset, (ii) a multi-agent (e.g., harasser, victim)
simulation informed by repeated game theory, (iii) three jailbreak methods
attacking agents across memory, planning, and fine-tuning, and (iv) a
mixed-methods evaluation framework. We utilize two prominent LLMs,
LLaMA-3.1-8B-Instruct (open-source) and Gemini-2.0-flash (closed-source). Our
results show that jailbreak tuning makes harassment nearly guaranteed with an
attack success rate of 95.78--96.89% vs. 57.25--64.19% without tuning in Llama,
and 99.33% vs. 98.46% without tuning in Gemini, while sharply reducing refusal
rate to 1-2% in both models. The most prevalent toxic behaviors are Insult with
84.9--87.8% vs. 44.2--50.8% without tuning, and Flaming with 81.2--85.1% vs.
31.5--38.8% without tuning, indicating weaker guardrails compared to sensitive
categories such as sexual or racial harassment. Qualitative evaluation further
reveals that attacked agents reproduce human-like aggression profiles, such as
Machiavellian/psychopathic patterns under planning, and narcissistic tendencies
with memory. Counterintuitively, closed-source and open-source models exhibit
distinct escalation trajectories across turns, with closed-source models
showing significant vulnerability. Overall, our findings show that multi-turn
and theory-grounded attacks not only succeed at high rates but also mimic
human-like harassment dynamics, motivating the development of robust safety
guardrails to ultimately keep online platforms safe and responsible.

</details>


### [58] [LiveResearchBench: A Live Benchmark for User-Centric Deep Research in the Wild](https://arxiv.org/abs/2510.14240)
*Jiayu Wang,Yifei Ming,Riya Dulepet,Qinglin Chen,Austin Xu,Zixuan Ke,Frederic Sala,Aws Albarghouthi,Caiming Xiong,Shafiq Joty*

Main category: cs.AI

TL;DR: 提出了LiveResearchBench基准和DeepEval评估套件，用于系统评估深度研究系统在动态网络搜索和综合信息方面的能力。


<details>
  <summary>Details</summary>
Motivation: 现有基准在评估深度研究系统时存在不足，往往聚焦于狭窄领域或提出模糊问题，无法公平比较。需要遵循用户中心、动态性、明确性和多面性搜索四个原则来构建更严谨的评估基准。

Method: 开发了包含100个专家策划任务的LiveResearchBench基准，涵盖日常生活、企业和学术领域，每个任务都需要广泛的动态实时网络搜索和综合。同时提出了DeepEval评估套件，包含内容和报告层面的质量评估，整合了四种互补的评估协议。

Result: 使用LiveResearchBench和DeepEval对17个前沿深度研究系统进行了全面评估，包括单代理网络搜索、单代理深度研究和多代理系统。

Conclusion: 分析揭示了当前系统的优势、常见失败模式以及推进可靠深度研究所需的关键系统组件。

Abstract: Deep research -- producing comprehensive, citation-grounded reports by
searching and synthesizing information from hundreds of live web sources --
marks an important frontier for agentic systems. To rigorously evaluate this
ability, four principles are essential: tasks should be (1) user-centric,
reflecting realistic information needs, (2) dynamic, requiring up-to-date
information beyond parametric knowledge, (3) unambiguous, ensuring consistent
interpretation across users, and (4) multi-faceted and search-intensive,
requiring search over numerous web sources and in-depth analysis. Existing
benchmarks fall short of these principles, often focusing on narrow domains or
posing ambiguous questions that hinder fair comparison. Guided by these
principles, we introduce LiveResearchBench, a benchmark of 100 expert-curated
tasks spanning daily life, enterprise, and academia, each requiring extensive,
dynamic, real-time web search and synthesis. Built with over 1,500 hours of
human labor, LiveResearchBench provides a rigorous basis for systematic
evaluation. To evaluate citation-grounded long-form reports, we introduce
DeepEval, a comprehensive suite covering both content- and report-level
quality, including coverage, presentation, citation accuracy and association,
consistency and depth of analysis. DeepEval integrates four complementary
evaluation protocols, each designed to ensure stable assessment and high
agreement with human judgments. Using LiveResearchBench and DeepEval, we
conduct a comprehensive evaluation of 17 frontier deep research systems,
including single-agent web search, single-agent deep research, and multi-agent
systems. Our analysis reveals current strengths, recurring failure modes, and
key system components needed to advance reliable, insightful deep research.

</details>


### [59] [Towards Agentic Self-Learning LLMs in Search Environment](https://arxiv.org/abs/2510.14253)
*Wangtao Sun,Xiang Cheng,Jialin Fan,Yao Xu,Xing Yu,Shizhu He,Jun Zhao,Kang Liu*

Main category: cs.AI

TL;DR: 本文提出了Agentic Self-Learning (ASL)框架，通过多角色协同进化的强化学习实现无需人工标注数据的智能体自我学习，发现奖励信号来源和任务数据规模是影响智能体训练可扩展性的关键因素。


<details>
  <summary>Details</summary>
Motivation: 研究如何在不依赖人工标注数据集或预定义规则奖励的情况下，实现基于LLM的智能体的规模化自我学习，解决传统方法在开放领域学习中存在的性能瓶颈问题。

Method: 提出ASL框架，包含提示生成器、策略模型和生成式奖励模型三个角色，在共享工具环境和LLM骨干中形成任务生成、策略执行和评估的闭环强化学习循环。

Result: ASL实现了稳定持续的轮次性能提升，超越了会达到性能瓶颈或退化的RLVR基线方法，在零标注数据条件下仍能持续改进，表现出优异的样本效率和鲁棒性。

Conclusion: 奖励来源和数据规模是开放领域智能体学习的关键杠杆，多角色协同进化是实现可扩展自我改进智能体的有效方法，其中GRM验证能力是主要瓶颈。

Abstract: We study whether self-learning can scale LLM-based agents without relying on
human-curated datasets or predefined rule-based rewards. Through controlled
experiments in a search-agent setting, we identify two key determinants of
scalable agent training: the source of reward signals and the scale of agent
task data. We find that rewards from a Generative Reward Model (GRM) outperform
rigid rule-based signals for open-domain learning, and that co-evolving the GRM
with the policy further boosts performance. Increasing the volume of agent task
data-even when synthetically generated-substantially enhances agentic
capabilities. Building on these insights, we propose \textbf{Agentic
Self-Learning} (ASL), a fully closed-loop, multi-role reinforcement learning
framework that unifies task generation, policy execution, and evaluation within
a shared tool environment and LLM backbone. ASL coordinates a Prompt Generator,
a Policy Model, and a Generative Reward Model to form a virtuous cycle of
harder task setting, sharper verification, and stronger solving. Empirically,
ASL delivers steady, round-over-round gains, surpasses strong RLVR baselines
(e.g., Search-R1) that plateau or degrade, and continues improving under
zero-labeled-data conditions, indicating superior sample efficiency and
robustness. We further show that GRM verification capacity is the main
bottleneck: if frozen, it induces reward hacking and stalls progress; continual
GRM training on the evolving data distribution mitigates this, and a small
late-stage injection of real verification data raises the performance ceiling.
This work establishes reward source and data scale as critical levers for
open-domain agent learning and demonstrates the efficacy of multi-role
co-evolution for scalable, self-improving agents. The data and code of this
paper are released at
https://github.com/forangel2014/Towards-Agentic-Self-Learning

</details>


### [60] [MorphoBench: A Benchmark with Difficulty Adaptive to Model Reasoning](https://arxiv.org/abs/2510.14265)
*Xukai Wang,Xuanbo Liu,Mingrui Chen,Haitian Zhong,Xuanlin Yang,Bohan Zeng,Jinbo Hu,Hao Liang,Junbo Niu,Xuchen Li,Ruitao Wu,Ruichuan An,Yang Shi,Liu Liu,Xu-Yao Zhang,Qiang Liu,Zhouchen Lin,Wentao Zhang,Bin Dong*

Main category: cs.AI

TL;DR: 提出了MorphoBench基准测试，用于评估大型模型的推理能力，具有多学科问题和自适应难度调整功能，包含1300多个测试问题。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试范围有限且缺乏灵活性，无法根据模型推理能力的演变调整难度，需要更全面有效的评估方法。

Method: 从现有基准和奥林匹克竞赛中收集复杂推理问题，利用模型推理过程中的关键陈述自适应修改问题分析难度，并包含使用仿真软件生成的问题以实现动态难度调整。

Result: 收集了1300多个测试问题，基于o3和GPT-5等模型的推理能力迭代调整了MorphoBench的难度，提高了模型推理评估的全面性和有效性。

Conclusion: MorphoBench增强了模型推理评估的全面性和有效性，为提升大型模型的推理能力和科学稳健性提供了可靠指导。

Abstract: With the advancement of powerful large-scale reasoning models, effectively
evaluating the reasoning capabilities of these models has become increasingly
important. However, existing benchmarks designed to assess the reasoning
abilities of large models tend to be limited in scope and lack the flexibility
to adapt their difficulty according to the evolving reasoning capacities of the
models. To address this, we propose MorphoBench, a benchmark that incorporates
multidisciplinary questions to evaluate the reasoning capabilities of large
models and can adjust and update question difficulty based on the reasoning
abilities of advanced models. Specifically, we curate the benchmark by
selecting and collecting complex reasoning questions from existing benchmarks
and sources such as Olympiad-level competitions. Additionally, MorphoBench
adaptively modifies the analytical challenge of questions by leveraging key
statements generated during the model's reasoning process. Furthermore, it
includes questions generated using simulation software, enabling dynamic
adjustment of benchmark difficulty with minimal resource consumption. We have
gathered over 1,300 test questions and iteratively adjusted the difficulty of
MorphoBench based on the reasoning capabilities of models such as o3 and GPT-5.
MorphoBench enhances the comprehensiveness and validity of model reasoning
evaluation, providing reliable guidance for improving both the reasoning
abilities and scientific robustness of large models. The code has been released
in https://github.com/OpenDCAI/MorphoBench.

</details>


### [61] [A Guardrail for Safety Preservation: When Safety-Sensitive Subspace Meets Harmful-Resistant Null-Space](https://arxiv.org/abs/2510.14301)
*Bingjie Zhang,Yibo Yang,Renzhe,Dandan Guo,Jindong Gu,Philip Torr,Bernard Ghanem*

Main category: cs.AI

TL;DR: GuardSpace是一个在微调过程中保护大语言模型安全对齐的框架，通过安全敏感子空间和有害抵抗零空间来防止安全行为退化。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在微调过程中容易失去预训练的安全对齐，即使使用良性数据或低秩适配也会产生有害响应，需要保护安全机制。

Method: 使用协方差预处理奇异值分解将预训练权重分解为安全相关和安全无关组件，初始化低秩适配器时冻结安全相关部分，并构建零空间投影器限制适配器更新对有害提示的影响。

Result: 在多个下游任务实验中，GuardSpace优于现有方法，对于Llama-2-7B-Chat在GSM8K上的微调，将平均有害分数从14.4%降至3.6%，同时准确率从26.0%提升至28.0%。

Conclusion: GuardSpace框架能有效在微调过程中保持模型的安全对齐，同时提升任务性能，为安全微调提供了可靠解决方案。

Abstract: Large language models (LLMs) have achieved remarkable success in diverse
tasks, yet their safety alignment remains fragile during adaptation. Even when
fine-tuning on benign data or with low-rank adaptation, pre-trained safety
behaviors are easily degraded, leading to harmful responses in the fine-tuned
models. To address this challenge, we propose GuardSpace, a guardrail framework
for preserving safety alignment throughout fine-tuning, composed of two key
components: a safety-sensitive subspace and a harmful-resistant null space.
First, we explicitly decompose pre-trained weights into safety-relevant and
safety-irrelevant components using covariance-preconditioned singular value
decomposition, and initialize low-rank adapters from the safety-irrelevant
ones, while freezing safety-relevant components to preserve their associated
safety mechanism. Second, we construct a null space projector that restricts
adapter updates from altering safe outputs on harmful prompts, thereby
maintaining the original refusal behavior. Experiments with various pre-trained
models on multiple downstream tasks demonstrate that GuardSpace achieves
superior performance over existing methods. Notably, for Llama-2-7B-Chat
fine-tuned on GSM8K, GuardSpace outperforms the state-of-the-art method AsFT,
reducing the average harmful score from 14.4% to 3.6%, while improving the
accuracy from from 26.0% to 28.0%.

</details>


### [62] [Terrarium: Revisiting the Blackboard for Multi-Agent Safety, Privacy, and Security Studies](https://arxiv.org/abs/2510.14312)
*Mason Nakamura,Abhinav Kumar,Saaduddin Mahmud,Sahar Abdelnabi,Shlomo Zilberstein,Eugene Bagdasarian*

Main category: cs.AI

TL;DR: 提出了Terrarium框架，用于在基于LLM的多智能体系统中进行细粒度的安全、隐私和安全性研究，通过重构黑板设计创建模块化、可配置的测试平台。


<details>
  <summary>Details</summary>
Motivation: LLM驱动的多智能体系统能够自动化繁琐的用户任务，但引入了新的风险，包括错位和对齐问题、恶意方攻击、智能体被破坏或用户数据被盗等。

Method: 重新利用多智能体系统中的早期方法——黑板设计，创建模块化、可配置的多智能体协作测试平台，识别关键攻击向量并实现三种协作场景和四种代表性攻击。

Result: 开发了Terrarium框架，提供了快速原型设计、评估和防御迭代的工具，展示了框架在模拟攻击场景中的灵活性。

Conclusion: Terrarium框架旨在加速可信多智能体系统的进展，通过提供工具来快速原型设计、评估和迭代防御措施。

Abstract: A multi-agent system (MAS) powered by large language models (LLMs) can
automate tedious user tasks such as meeting scheduling that requires
inter-agent collaboration. LLMs enable nuanced protocols that account for
unstructured private data, user constraints, and preferences. However, this
design introduces new risks, including misalignment and attacks by malicious
parties that compromise agents or steal user data. In this paper, we propose
the Terrarium framework for fine-grained study on safety, privacy, and security
in LLM-based MAS. We repurpose the blackboard design, an early approach in
multi-agent systems, to create a modular, configurable testbed for multi-agent
collaboration. We identify key attack vectors such as misalignment, malicious
agents, compromised communication, and data poisoning. We implement three
collaborative MAS scenarios with four representative attacks to demonstrate the
framework's flexibility. By providing tools to rapidly prototype, evaluate, and
iterate on defenses and designs, Terrarium aims to accelerate progress toward
trustworthy multi-agent systems.

</details>


### [63] [Metacognitive Self-Correction for Multi-Agent System via Prototype-Guided Next-Execution Reconstruction](https://arxiv.org/abs/2510.14319)
*Xu Shen,Qi Zhang,Song Wang,Zhen Tan,Xinyu Zhao,Laura Yao,Vaishnav Tadiparthi,Hossein Nourkhiz Mahjoub,Ehsan Moradi Pari,Kwonjoon Lee,Tianlong Chen*

Main category: cs.AI

TL;DR: MASC是一个元认知框架，为多智能体系统提供实时、无监督的步骤级错误检测和自我纠正，通过历史条件异常评分来防止错误传播。


<details>
  <summary>Details</summary>
Motivation: 多智能体系统在协作解决问题方面表现出色，但对级联错误很脆弱——单个错误步骤会在智能体间传播并破坏整个轨迹。

Method: 采用两种互补设计：1）下一执行重构，从查询和交互历史预测下一步的嵌入以捕捉因果一致性；2）原型引导增强，学习正常步骤嵌入的原型先验，在稀疏上下文下稳定重构和异常评分。检测到异常时触发纠正智能体。

Result: 在Who&When基准测试中，MASC始终优于所有基线，步骤级错误检测AUC-ROC提升高达8.47%；在不同MAS框架中都能带来一致的端到端性能提升。

Conclusion: 元认知监控和针对性纠正能够以最小开销缓解错误传播，该框架在不同架构中都具有通用性。

Abstract: Large Language Model based multi-agent systems (MAS) excel at collaborative
problem solving but remain brittle to cascading errors: a single faulty step
can propagate across agents and disrupt the trajectory. In this paper, we
present MASC, a metacognitive framework that endows MAS with real-time,
unsupervised, step-level error detection and self-correction. MASC rethinks
detection as history-conditioned anomaly scoring via two complementary designs:
(1) Next-Execution Reconstruction, which predicts the embedding of the next
step from the query and interaction history to capture causal consistency, and
(2) Prototype-Guided Enhancement, which learns a prototype prior over
normal-step embeddings and uses it to stabilize reconstruction and anomaly
scoring under sparse context (e.g., early steps). When an anomaly step is
flagged, MASC triggers a correction agent to revise the acting agent's output
before information flows downstream. On the Who&When benchmark, MASC
consistently outperforms all baselines, improving step-level error detection by
up to 8.47% AUC-ROC ; When plugged into diverse MAS frameworks, it delivers
consistent end-to-end gains across architectures, confirming that our
metacognitive monitoring and targeted correction can mitigate error propagation
with minimal overhead.

</details>


### [64] [AI for Service: Proactive Assistance with AI Glasses](https://arxiv.org/abs/2510.14359)
*Zichen Wen,Yiyu Wang,Chenfei Liao,Boxue Yang,Junxian Li,Weifeng Liu,Haocong He,Bolong Feng,Xuyang Liu,Yuanhuiyi Lyu,Xu Zheng,Xuming Hu,Linfeng Zhang*

Main category: cs.AI

TL;DR: 提出了AI4Service新范式，通过Alpha-Service框架实现主动式AI服务，能够从第一视角视频流中检测服务机会并提供个性化服务。


<details>
  <summary>Details</summary>
Motivation: 现有AI服务大多是被动响应式的，真正智能的助手应该能够预见用户需求并在适当时机主动采取行动。

Method: 基于AI眼镜构建Alpha-Service统一框架，包含输入单元、中央处理单元、算术逻辑单元、内存单元和输出单元五个核心组件，采用多智能体系统实现。

Result: 通过实时21点顾问、博物馆导览和购物搭配助手等案例研究，证明系统能够无缝感知环境、推断用户意图，无需明确提示即可提供及时有用的帮助。

Conclusion: AI4Service范式将AI从被动工具转变为主动伴侣，Alpha-Service框架为解决"何时干预"和"如何服务"两大挑战提供了可行方案。

Abstract: In an era where AI is evolving from a passive tool into an active and
adaptive companion, we introduce AI for Service (AI4Service), a new paradigm
that enables proactive and real-time assistance in daily life. Existing AI
services remain largely reactive, responding only to explicit user commands. We
argue that a truly intelligent and helpful assistant should be capable of
anticipating user needs and taking actions proactively when appropriate. To
realize this vision, we propose Alpha-Service, a unified framework that
addresses two fundamental challenges: Know When to intervene by detecting
service opportunities from egocentric video streams, and Know How to provide
both generalized and personalized services. Inspired by the von Neumann
computer architecture and based on AI glasses, Alpha-Service consists of five
key components: an Input Unit for perception, a Central Processing Unit for
task scheduling, an Arithmetic Logic Unit for tool utilization, a Memory Unit
for long-term personalization, and an Output Unit for natural human
interaction. As an initial exploration, we implement Alpha-Service through a
multi-agent system deployed on AI glasses. Case studies, including a real-time
Blackjack advisor, a museum tour guide, and a shopping fit assistant,
demonstrate its ability to seamlessly perceive the environment, infer user
intent, and provide timely and useful assistance without explicit prompts.

</details>


### [65] [Can MLLMs Absorb Math Reasoning Abilities from LLMs as Free Lunch?](https://arxiv.org/abs/2510.14387)
*Yijie Hu,Zihao Zhou,Kaizhu Huang,Xiaowei Huang,Qiufeng Wang*

Main category: cs.AI

TL;DR: IP-Merging是一种无需调优的方法，通过识别多模态大模型和数学大模型中的推理相关参数，将其投影到多模态大模型的子空间中进行合并，从而直接提升多模态大模型的数学推理能力。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大模型在数学推理能力上落后于纯文本大模型，但直接合并模型参数存在对齐问题。本文旨在探索如何让多模态大模型直接从现成的数学大模型中吸收数学推理能力，而无需进行复杂的训练。

Method: 提出IP-Merging方法：1）识别多模态大模型和数学大模型中的推理相关参数；2）将这些参数投影到多模态大模型的子空间中；3）在该子空间中合并参数。整个过程无需调优，直接调整参数。

Result: 大量实验表明，IP-Merging方法能够在不损害多模态大模型其他能力的前提下，直接从数学大模型中增强其数学推理能力。

Conclusion: IP-Merging是一种有效的模型合并方法，能够解决多模态大模型与数学大模型参数空间不对齐的问题，成功将数学推理能力迁移到多模态大模型中。

Abstract: Math reasoning has been one crucial ability of large language models (LLMs),
where significant advancements have been achieved in recent years. However,
most efforts focus on LLMs by curating high-quality annotation data and
intricate training (or inference) paradigms, while the math reasoning
performance of multi-modal LLMs (MLLMs) remains lagging behind. Since the MLLM
typically consists of an LLM and a vision block, we wonder: Can MLLMs directly
absorb math reasoning abilities from off-the-shelf math LLMs without tuning?
Recent model-merging approaches may offer insights into this question. However,
they overlook the alignment between the MLLM and LLM, where we find that there
is a large gap between their parameter spaces, resulting in lower performance.
Our empirical evidence reveals two key factors behind this issue: the
identification of crucial reasoning-associated layers in the model and the
mitigation of the gaps in parameter space. Based on the empirical insights, we
propose IP-Merging that first identifies the reasoning-associated parameters in
both MLLM and Math LLM, then projects them into the subspace of MLLM, aiming to
maintain the alignment, and finally merges parameters in this subspace.
IP-Merging is a tuning-free approach since parameters are directly adjusted.
Extensive experiments demonstrate that our IP-Merging method can enhance the
math reasoning ability of MLLMs directly from Math LLMs without compromising
their other capabilities.

</details>


### [66] [TITAN: Graph-Executable Reasoning for Cyber Threat Intelligence](https://arxiv.org/abs/2510.14670)
*Marco Simoni,Aleksandar Fontana,Andrea Saracino,Paolo Mori*

Main category: cs.AI

TL;DR: TITAN是一个将自然语言网络安全威胁查询与结构化知识图谱上的可执行推理相连接的框架，包含路径规划模型和图遍历执行器。


<details>
  <summary>Details</summary>
Motivation: 传统检索系统无法在威胁、行为和防御之间进行清晰可逆的推理，需要一种能够连接自然语言查询与结构化知识图谱推理的框架。

Method: 集成路径规划模型预测文本中的逻辑关系链，并使用图执行器遍历TITAN本体图来检索事实答案和支撑证据。基于MITRE构建类型化双向图。

Result: 创建了包含88209个示例的TITAN数据集，实证评估显示TITAN能生成语法有效且语义连贯的推理路径，可在底层图上确定性执行。

Conclusion: TITAN框架成功实现了自然语言威胁查询与结构化知识图谱推理的有效连接，支持清晰可逆的威胁推理过程。

Abstract: TITAN (Threat Intelligence Through Automated Navigation) is a framework that
connects natural-language cyber threat queries with executable reasoning over a
structured knowledge graph. It integrates a path planner model, which predicts
logical relation chains from text, and a graph executor that traverses the
TITAN Ontology to retrieve factual answers and supporting evidence. Unlike
traditional retrieval systems, TITAN operates on a typed, bidirectional graph
derived from MITRE, allowing reasoning to move clearly and reversibly between
threats, behaviors, and defenses. To support training and evaluation, we
introduce the TITAN Dataset, a corpus of 88209 examples (Train: 74258; Test:
13951) pairing natural language questions with executable reasoning paths and
step by step Chain of Thought explanations. Empirical evaluations show that
TITAN enables models to generate syntactically valid and semantically coherent
reasoning paths that can be deterministically executed on the underlying graph.

</details>


### [67] [Hi-Agent: Hierarchical Vision-Language Agents for Mobile Device Control](https://arxiv.org/abs/2510.14388)
*Zhe Wu,Hongjin Lu,Junliang Xing,Changhao Zhang,Yin Zhu,Yuhao Yang,Yuheng Jing,Kai Li,Kun Shao,Jianye Hao,Jun Wang,Yuanchun Shi*

Main category: cs.AI

TL;DR: Hi-Agent是一个可训练的分层视觉语言代理，用于移动设备控制，通过高层推理模型和低层动作模型的联合优化，在Android-in-the-Wild基准测试中达到了87.9%的任务成功率，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于视觉语言模型的移动设备控制方法大多依赖直接的状态到动作映射，缺乏结构化推理和规划，导致在新型任务或未见UI布局上泛化能力差。

Method: 提出分层架构，包含高层推理模型和低层动作模型，通过将多步决策重新表述为单步子目标序列，并引入前瞻优势函数，利用低层模型的执行反馈来指导高层优化，缓解长时任务中的路径爆炸问题。

Result: 在Android-in-the-Wild基准测试中达到87.9%的任务成功率，显著优于提示型(17.7%)、监督学习(54.5%)和强化学习方法(71.9%)，在ScreenSpot-v2基准测试中展现出竞争力的零样本泛化能力。

Conclusion: Hi-Agent通过分层设计和联合优化方法，在移动设备控制任务中实现了最先进的性能，并在高复杂度场景中展现出良好的可扩展性和适应性。

Abstract: Building agents that autonomously operate mobile devices has attracted
increasing attention. While Vision-Language Models (VLMs) show promise, most
existing approaches rely on direct state-to-action mappings, which lack
structured reasoning and planning, and thus generalize poorly to novel tasks or
unseen UI layouts. We introduce Hi-Agent, a trainable hierarchical
vision-language agent for mobile control, featuring a high-level reasoning
model and a low-level action model that are jointly optimized. For efficient
training, we reformulate multi-step decision-making as a sequence of
single-step subgoals and propose a foresight advantage function, which
leverages execution feedback from the low-level model to guide high-level
optimization. This design alleviates the path explosion issue encountered by
Group Relative Policy Optimization (GRPO) in long-horizon tasks and enables
stable, critic-free joint training. Hi-Agent achieves a new State-Of-The-Art
(SOTA) 87.9% task success rate on the Android-in-the-Wild (AitW) benchmark,
significantly outperforming prior methods across three paradigms: prompt-based
(AppAgent: 17.7%), supervised (Filtered BC: 54.5%), and reinforcement
learning-based (DigiRL: 71.9%). It also demonstrates competitive zero-shot
generalization on the ScreenSpot-v2 benchmark. On the more challenging
AndroidWorld benchmark, Hi-Agent also scales effectively with larger backbones,
showing strong adaptability in high-complexity mobile control scenarios.

</details>


### [68] [IMAGINE: Integrating Multi-Agent System into One Model for Complex Reasoning and Planning](https://arxiv.org/abs/2510.14406)
*Xikai Zhang,Bo Wang,Likang Xiao,Yongzhi Li,Quan Chen,Wenju Wu,Liu Liu*

Main category: cs.AI

TL;DR: 提出IMAGINE框架，将多智能体系统的推理规划能力集成到单一紧凑模型中，通过端到端训练显著超越多智能体系统性能


<details>
  <summary>Details</summary>
Motivation: 大语言模型在复杂推理和规划任务中表现不佳，多智能体系统虽能提升集体推理能力但存在推理成本高、延迟长和训练困难等问题

Method: IMAGINE框架将多智能体系统的能力集成到单一模型中，通过简单的端到端训练实现能力超越

Result: 使用Qwen3-8B-Instruct作为基础模型，在TravelPlanner基准测试中达到82.7%最终通过率，远超DeepSeek-R1-671B的40%

Conclusion: 单一小规模模型不仅能获得多智能体系统的结构化推理规划能力，还能显著超越其性能，同时保持更小的模型规模

Abstract: Although large language models (LLMs) have made significant strides across
various tasks, they still face significant challenges in complex reasoning and
planning. For example, even with carefully designed prompts and prior
information explicitly provided, GPT-4o achieves only a 7% Final Pass Rate on
the TravelPlanner dataset in the sole-planning mode. Similarly, even in the
thinking mode, Qwen3-8B-Instruct and DeepSeek-R1-671B, only achieve Final Pass
Rates of 5.9% and 40%, respectively. Although well-organized Multi-Agent
Systems (MAS) can offer improved collective reasoning, they often suffer from
high reasoning costs due to multi-round internal interactions, long
per-response latency, and difficulties in end-to-end training. To address these
challenges, we propose a general and scalable framework called IMAGINE, short
for Integrating Multi-Agent System into One Model. This framework not only
integrates the reasoning and planning capabilities of MAS into a single,
compact model, but also significantly surpass the capabilities of the MAS
through a simple end-to-end training. Through this pipeline, a single
small-scale model is not only able to acquire the structured reasoning and
planning capabilities of a well-organized MAS but can also significantly
outperform it. Experimental results demonstrate that, when using
Qwen3-8B-Instruct as the base model and training it with our method, the model
achieves an 82.7% Final Pass Rate on the TravelPlanner benchmark, far exceeding
the 40% of DeepSeek-R1-671B, while maintaining a much smaller model size.

</details>


### [69] [Mapping Smarter, Not Harder: A Test-Time Reinforcement Learning Agent That Improves Without Labels or Model Updates](https://arxiv.org/abs/2510.14900)
*Wen-Kwang Tsao,Yao-Ching Yu,Chien-Ming Huang*

Main category: cs.AI

TL;DR: 提出了一种无需标注数据或模型权重更新的强化学习代理，通过生成网络搜索查询获取外部证据来自我改进模式映射，显著提升了企业智能平台中第三方日志的映射准确性。


<details>
  <summary>Details</summary>
Motivation: 企业智能平台需要集成大量第三方供应商的日志，但供应商文档在测试时常常不可用、不匹配或格式混乱，导致模式映射困难。

Method: 使用强化学习代理：1)识别模糊的字段映射尝试；2)生成针对性网络搜索查询收集外部证据；3)基于置信度的奖励迭代优化映射。

Result: 在微软端点防护日志到通用模式的转换中，映射准确率从56.4%(仅LLM)提升到72.73%(RAG)再到93.94%(100次迭代后)，同时将需要专家审查的低置信度映射减少了85%。

Conclusion: 该方法为未来行业问题提供了证据驱动、透明的解决方案，为更稳健、可问责、可扩展、高效、灵活、适应性强和协作的解决方案铺平了道路。

Abstract: The Enterprise Intelligence Platform must integrate logs from numerous
third-party vendors in order to perform various downstream tasks. However,
vendor documentation is often unavailable at test time. It is either misplaced,
mismatched, poorly formatted, or incomplete, which makes schema mapping
challenging. We introduce a reinforcement learning agent that can self-improve
without labeled examples or model weight updates. During inference, the agent:
1) Identifies ambiguous field-mapping attempts. 2) Generates targeted
web-search queries to gather external evidence. 3) Applies a confidence-based
reward to iteratively refine its mappings. To demonstrate this concept, we
converted Microsoft Defender for Endpoint logs into a common schema. Our method
increased mapping accuracy from 56.4\%(LLM-only) to 72.73\%(RAG) to 93.94\%
over 100 iterations using GPT-4o. At the same time, it reduced the number of
low-confidence mappings requiring expert review by 85\%. This new approach
provides an evidence-driven, transparent method for solving future industry
problems, paving the way for more robust, accountable, scalable, efficient,
flexible, adaptable, and collaborative solutions.

</details>


### [70] [Eliminating Negative Occurrences of Derived Predicates from PDDL Axioms](https://arxiv.org/abs/2510.14412)
*Claudia Grundke,Gabriele Röger*

Main category: cs.AI

TL;DR: 本文提出了一种转换方法，用于消除PDDL公理中派生谓词的负出现，证明这种限制可以被克服。


<details>
  <summary>Details</summary>
Motivation: PDDL标准限制公理体中谓词的负出现只能针对直接由动作设置的谓词，而非由公理派生的谓词。但文献中作者常偏离此限制，仅要求公理集可分层。

Method: 提出相应的转换方法，证明负出现的派生谓词可以被消除。

Result: 两种变体都能表达与最小不动点逻辑完全相同的查询。

Conclusion: 通过提出的转换，可以消除PDDL公理中派生谓词的负出现限制。

Abstract: Axioms are a feature of the Planning Domain Definition Language PDDL that can
be considered as a generalization of database query languages such as Datalog.
The PDDL standard restricts negative occurrences of predicates in axiom bodies
to predicates that are directly set by actions and not derived by axioms. In
the literature, authors often deviate from this limitation and only require
that the set of axioms is stratifiable. Both variants can express exactly the
same queries as least fixed-point logic, indicating that negative occurrences
of derived predicates can be eliminated. We present the corresponding
transformation.

</details>


### [71] [Helmsman: Autonomous Synthesis of Federated Learning Systems via Multi-Agent Collaboration](https://arxiv.org/abs/2510.14512)
*Haoyuan Li,Mathias Funk,Aaqib Saeed*

Main category: cs.AI

TL;DR: Helmsman是一个多智能体系统，通过模拟研发工作流程自动合成联邦学习系统，包含交互式规划、模块化代码生成和自主评估三个阶段，在AgentFL-Bench基准测试中表现优于手工基线。


<details>
  <summary>Details</summary>
Motivation: 联邦学习系统设计复杂，需要应对数据异构性和系统约束等多方面挑战，当前方法往往产生脆弱、定制的解决方案，成为关键瓶颈。

Method: Helmsman采用三阶段方法：(1)交互式人机协作规划制定研究计划；(2)监督智能体团队进行模块化代码生成；(3)在沙盒仿真环境中进行自主评估和优化的闭环流程。

Result: 在包含16个多样化任务的AgentFL-Bench基准测试中，Helmsman生成的解决方案与手工基线相当甚至更优。

Conclusion: 这项工作代表了向复杂去中心化AI系统自动化工程迈出的重要一步。

Abstract: Federated Learning (FL) offers a powerful paradigm for training models on
decentralized data, but its promise is often undermined by the immense
complexity of designing and deploying robust systems. The need to select,
combine, and tune strategies for multifaceted challenges like data
heterogeneity and system constraints has become a critical bottleneck,
resulting in brittle, bespoke solutions. To address this, we introduce
Helmsman, a novel multi-agent system that automates the end-to-end synthesis of
federated learning systems from high-level user specifications. It emulates a
principled research and development workflow through three collaborative
phases: (1) interactive human-in-the-loop planning to formulate a sound
research plan, (2) modular code generation by supervised agent teams, and (3) a
closed-loop of autonomous evaluation and refinement in a sandboxed simulation
environment. To facilitate rigorous evaluation, we also introduce
AgentFL-Bench, a new benchmark comprising 16 diverse tasks designed to assess
the system-level generation capabilities of agentic systems in FL. Extensive
experiments demonstrate that our approach generates solutions competitive with,
and often superior to, established hand-crafted baselines. Our work represents
a significant step towards the automated engineering of complex decentralized
AI systems.

</details>


### [72] [JSPLIT: A Taxonomy-based Solution for Prompt Bloating in Model Context Protocol](https://arxiv.org/abs/2510.14537)
*Emanuele Antonioni,Stefan Markovic,Anirudha Shankar,Jaime Bernardo,Lovro Markovic,Silvia Pareti,Benedetto Proietti*

Main category: cs.AI

TL;DR: JSPLIT是一个基于分类学的框架，通过层次化组织MCP工具并基于用户提示选择最相关工具，有效解决提示膨胀问题，在显著减少提示大小的同时保持甚至提升代理性能。


<details>
  <summary>Details</summary>
Motivation: 随着AI系统发展，用户期望从简单的文本交互转向更复杂的代理系统。MCP等标准使代理能够访问外部工具，但随着工具数量增加，提示变得冗长，导致高token成本、延迟增加和任务成功率下降。

Method: 提出JSPLIT框架：1）将工具组织成层次化分类学结构；2）基于用户提示和分类学结构识别并仅包含最相关工具；3）设计工具选择算法。

Result: JSPLIT显著减少了提示大小，且未显著影响代理响应能力。当可用工具数量大幅增加时，JSPLIT甚至提高了工具选择准确性，在降低成本的同时改善了高复杂度环境中的任务成功率。

Conclusion: JSPLIT通过分类学驱动的方法有效管理提示大小，解决了工具数量增加导致的提示膨胀问题，在保持代理性能的同时降低了成本，适用于复杂的代理环境。

Abstract: AI systems are continually evolving and advancing, and user expectations are
concurrently increasing, with a growing demand for interactions that go beyond
simple text-based interaction with Large Language Models (LLMs). Today's
applications often require LLMs to interact with external tools, marking a
shift toward more complex agentic systems. To support this, standards such as
the Model Context Protocol (MCP) have emerged, enabling agents to access tools
by including a specification of the capabilities of each tool within the
prompt. Although this approach expands what agents can do, it also introduces a
growing problem: prompt bloating. As the number of tools increases, the prompts
become longer, leading to high prompt token costs, increased latency, and
reduced task success resulting from the selection of tools irrelevant to the
prompt. To address this issue, we introduce JSPLIT, a taxonomy-driven framework
designed to help agents manage prompt size more effectively when using large
sets of MCP tools. JSPLIT organizes the tools into a hierarchical taxonomy and
uses the user's prompt to identify and include only the most relevant tools,
based on both the query and the taxonomy structure. In this paper, we describe
the design of the taxonomy, the tool selection algorithm, and the dataset used
to evaluate JSPLIT. Our results show that JSPLIT significantly reduces prompt
size without significantly compromising the agent's ability to respond
effectively. As the number of available tools for the agent grows
substantially, JSPLIT even improves the tool selection accuracy of the agent,
effectively reducing costs while simultaneously improving task success in
high-complexity agent environments.

</details>


### [73] [Symbol Grounding in Neuro-Symbolic AI: A Gentle Introduction to Reasoning Shortcuts](https://arxiv.org/abs/2510.14538)
*Emanuele Marconato,Samuele Bortolotti,Emile van Krieken,Paolo Morettin,Elena Umili,Antonio Vergari,Efthymia Tsamoura,Andrea Passerini,Stefano Teso*

Main category: cs.AI

TL;DR: 神经符号AI中的推理捷径问题：当概念未被直接监督时，模型可能通过错误的概念基础实现高准确率，这会影响模型的可解释性、分布外性能及可靠性。本文提供了推理捷径的概述、原因分析、理论表征及应对方法。


<details>
  <summary>Details</summary>
Motivation: 神经符号AI结合神经网络和符号推理，是实现可靠可信AI的重要途径。然而推理捷径问题会损害模型的可解释性和可靠性，且现有研究较为分散，需要系统性的梳理和介绍。

Method: 本文采用综述方法，首先直观介绍推理捷径的概念、成因和后果，然后回顾和阐释现有的理论表征，最后详细讨论应对推理捷径的方法，包括缓解策略和意识策略。

Result: 提供了推理捷径的统一视角，系统分析了其成因、影响和应对策略，为研究人员和实践者理解和解决这一挑战性问题提供了基础。

Conclusion: 通过以易于理解的形式重新阐述高级材料，本文旨在降低解决推理捷径问题的门槛，为开发可靠的神经符号AI和可信AI模型做出贡献。

Abstract: Neuro-symbolic (NeSy) AI aims to develop deep neural networks whose
predictions comply with prior knowledge encoding, e.g. safety or structural
constraints. As such, it represents one of the most promising avenues for
reliable and trustworthy AI. The core idea behind NeSy AI is to combine neural
and symbolic steps: neural networks are typically responsible for mapping
low-level inputs into high-level symbolic concepts, while symbolic reasoning
infers predictions compatible with the extracted concepts and the prior
knowledge. Despite their promise, it was recently shown that - whenever the
concepts are not supervised directly - NeSy models can be affected by Reasoning
Shortcuts (RSs). That is, they can achieve high label accuracy by grounding the
concepts incorrectly. RSs can compromise the interpretability of the model's
explanations, performance in out-of-distribution scenarios, and therefore
reliability. At the same time, RSs are difficult to detect and prevent unless
concept supervision is available, which is typically not the case. However, the
literature on RSs is scattered, making it difficult for researchers and
practitioners to understand and tackle this challenging problem. This overview
addresses this issue by providing a gentle introduction to RSs, discussing
their causes and consequences in intuitive terms. It also reviews and
elucidates existing theoretical characterizations of this phenomenon. Finally,
it details methods for dealing with RSs, including mitigation and awareness
strategies, and maps their benefits and limitations. By reformulating advanced
material in a digestible form, this overview aims to provide a unifying
perspective on RSs to lower the bar to entry for tackling them. Ultimately, we
hope this overview contributes to the development of reliable NeSy and
trustworthy AI models.

</details>


### [74] [LLM Agents Beyond Utility: An Open-Ended Perspective](https://arxiv.org/abs/2510.14548)
*Asen Nachkov,Xi Wang,Luc Van Gool*

Main category: cs.AI

TL;DR: 该研究探讨了预训练LLM代理能否通过自我生成任务、积累知识和环境交互，发展成具有自主规划和推理能力的实体。


<details>
  <summary>Details</summary>
Motivation: 研究LLM代理是否能超越智能问题解决工具，成为能够规划、设计任务并推理模糊目标的自主实体。

Method: 在开放环境中增强预训练LLM代理，使其能够生成自己的任务、积累知识并与环境广泛交互。

Result: 代理能够可靠执行复杂多步指令、跨运行存储和重用信息、提出并解决自己的任务，但对提示设计敏感、容易重复生成任务、无法形成自我表征。

Conclusion: 研究显示了将预训练LLM适应开放性的前景和当前局限，为训练代理管理记忆、有效探索和追求抽象长期目标指明了未来方向。

Abstract: Recent LLM agents have made great use of chain of thought reasoning and
function calling. As their capabilities grow, an important question arises: can
this software represent not only a smart problem-solving tool, but an entity in
its own right, that can plan, design immediate tasks, and reason toward
broader, more ambiguous goals? To study this question, we adopt an open-ended
experimental setting where we augment a pretrained LLM agent with the ability
to generate its own tasks, accumulate knowledge, and interact extensively with
its environment. We study the resulting open-ended agent qualitatively. It can
reliably follow complex multi-step instructions, store and reuse information
across runs, and propose and solve its own tasks, though it remains sensitive
to prompt design, prone to repetitive task generation, and unable to form
self-representations. These findings illustrate both the promise and current
limits of adapting pretrained LLMs toward open-endedness, and point to future
directions for training agents to manage memory, explore productively, and
pursue abstract long-term goals.

</details>


### [75] [ColorBench: Benchmarking Mobile Agents with Graph-Structured Framework for Complex Long-Horizon Tasks](https://arxiv.org/abs/2510.14621)
*Yuanyi Song,Heyuan Huang,Qiqiang Lin,Yin Zhao,Xiangmou Qu,Jun Wang,Xingyu Lou,Weiwen Liu,Zhuosheng Zhang,Jun Wang,Yong Yu,Weinan Zhang,Zhaoxiang Wang*

Main category: cs.AI

TL;DR: 本文提出了ColorBench，一种基于图结构的移动代理评估框架，通过模拟真实设备交互的有限状态，实现动态行为的静态仿真，解决了现有评估方法在复杂长时任务中的不足。


<details>
  <summary>Details</summary>
Motivation: 现有移动代理评估标准存在缺陷：离线静态基准只能验证单一预定义路径，而在线动态测试受限于真实设备的复杂性和不可重现性，无法全面评估代理在复杂多解任务中的能力。

Method: 开发了基于图结构的基准框架，通过建模真实设备交互中的有限状态，实现动态行为的静态仿真。构建了包含175个任务（74个单应用、101个跨应用）的ColorBench基准，平均任务长度超过13步，每个任务包含至少两条正确路径和若干典型错误路径。

Result: 通过在各种基线模型上评估ColorBench，发现了现有模型的局限性，并基于实验结果提出了改进方向和可行的技术路径，以增强代理在复杂长时问题上的性能。

Conclusion: ColorBench框架成功弥合了离线与在线评估之间的差距，提供了支持多有效解决方案评估、子任务完成率统计和原子级能力分析的稳定测试环境，为移动代理能力的全面评估提供了有效工具。

Abstract: The rapid advancement of multimodal large language models has enabled agents
to operate mobile devices by directly interacting with graphical user
interfaces, opening new possibilities for mobile automation. However,
real-world mobile tasks are often complex and allow for multiple valid
solutions. This contradicts current mobile agent evaluation standards: offline
static benchmarks can only validate a single predefined "golden path", while
online dynamic testing is constrained by the complexity and non-reproducibility
of real devices, making both approaches inadequate for comprehensively
assessing agent capabilities. To bridge the gap between offline and online
evaluation and enhance testing stability, this paper introduces a novel
graph-structured benchmarking framework. By modeling the finite states observed
during real-device interactions, it achieves static simulation of dynamic
behaviors. Building on this, we develop ColorBench, a benchmark focused on
complex long-horizon tasks. It supports evaluation of multiple valid solutions,
subtask completion rate statistics, and atomic-level capability analysis.
ColorBench contains 175 tasks (74 single-app, 101 cross-app) with an average
length of over 13 steps. Each task includes at least two correct paths and
several typical error paths, enabling quasi-dynamic interaction. By evaluating
ColorBench across various baselines, we discover limitations of existing models
and propose improvement directions and feasible technical pathways to enhance
agents' performance on complex, long-horizon problems based on experimental
results. Code and data are available at:
https://github.com/MadeAgents/ColorBench.

</details>


### [76] [Beyond Hallucinations: The Illusion of Understanding in Large Language Models](https://arxiv.org/abs/2510.14665)
*Rikard Rosenbacke,Carl Rosenbacke,Victor Rosenbacke,Martin McKee*

Main category: cs.AI

TL;DR: 本文提出了Rose-Frame框架，用于诊断人机交互中的认知和认识论偏差。该框架包含三个维度：地图vs领土（表征与现实）、直觉vs理性（双过程理论）、冲突vs确认（批判性测试vs相互验证），旨在提高AI部署的透明度和批判意识。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型虽然流畅且具有情感共鸣，但基于统计预测而非有根据的推理，存在幻觉风险。它们实现了系统1认知的大规模操作化：快速、联想、有说服力，但缺乏反思或证伪。

Method: 引入Rose-Frame三维诊断框架：1) 地图vs领土：区分现实表征与现实本身；2) 直觉vs理性：基于双过程理论分离快速情感判断与缓慢反思思维；3) 冲突vs确认：检验观点是通过分歧批判测试还是通过相互验证强化。

Result: Rose-Frame不试图用更多数据或规则修复LLM，而是提供一个反思工具，使模型局限性和用户假设可见，实现更透明和批判性意识强的AI部署。

Conclusion: 将对齐重新定义为认知治理：无论是人类还是人工智能的直觉，都必须受人类理性支配。只有通过嵌入反思性、可证伪的监督，才能使机器流畅性与人类理解保持一致。

Abstract: Large language models (LLMs) are becoming deeply embedded in human
communication and decision-making, yet they inherit the ambiguity, bias, and
lack of direct access to truth inherent in language itself. While their outputs
are fluent, emotionally resonant, and coherent, they are generated through
statistical prediction rather than grounded reasoning. This creates the risk of
hallucination, responses that sound convincing but lack factual validity.
Building on Geoffrey Hinton's observation that AI mirrors human intuition
rather than reasoning, this paper argues that LLMs operationalize System 1
cognition at scale: fast, associative, and persuasive, but without reflection
or falsification. To address this, we introduce the Rose-Frame, a
three-dimensional framework for diagnosing cognitive and epistemic drift in
human-AI interaction. The three axes are: (i) Map vs. Territory, which
distinguishes representations of reality (epistemology) from reality itself
(ontology); (ii) Intuition vs. Reason, drawing on dual-process theory to
separate fast, emotional judgments from slow, reflective thinking; and (iii)
Conflict vs. Confirmation, which examines whether ideas are critically tested
through disagreement or simply reinforced through mutual validation. Each
dimension captures a distinct failure mode, and their combination amplifies
misalignment. Rose-Frame does not attempt to fix LLMs with more data or rules.
Instead, it offers a reflective tool that makes both the model's limitations
and the user's assumptions visible, enabling more transparent and critically
aware AI deployment. It reframes alignment as cognitive governance: intuition,
whether human or artificial, must remain governed by human reason. Only by
embedding reflective, falsifiable oversight can we align machine fluency with
human understanding.

</details>


### [77] [Machine Learning and Public Health: Identifying and Mitigating Algorithmic Bias through a Systematic Review](https://arxiv.org/abs/2510.14669)
*Sara Altamirano,Arjan Vreeken,Sennay Ghebreab*

Main category: cs.AI

TL;DR: 本文对2021-2025年荷兰公共卫生机器学习研究中的算法偏见进行了系统文献综述，开发了RABAT评估工具，发现普遍存在公平性框架缺失等问题，并提出了ACAR四阶段公平性框架。


<details>
  <summary>Details</summary>
Motivation: 机器学习在公共卫生领域有巨大潜力，但如果不系统关注算法偏见，可能会无意中加剧现有的健康差距。

Method: 开发了RABAT评估工具，整合了Cochrane偏见风险、PROBAST和微软负责任AI检查表等现有框架，应用于35篇同行评审研究。

Result: 分析显示普遍存在差距：虽然数据抽样和缺失数据实践记录良好，但大多数研究缺乏明确的公平性框架、亚组分析和潜在危害的透明讨论。

Conclusion: 提出了ACAR四阶段公平性框架和具体建议，帮助公共卫生ML从业者系统考虑算法偏见，确保算法创新促进而非损害健康公平。

Abstract: Machine learning (ML) promises to revolutionize public health through
improved surveillance, risk stratification, and resource allocation. However,
without systematic attention to algorithmic bias, ML may inadvertently
reinforce existing health disparities. We present a systematic literature
review of algorithmic bias identification, discussion, and reporting in Dutch
public health ML research from 2021 to 2025. To this end, we developed the Risk
of Algorithmic Bias Assessment Tool (RABAT) by integrating elements from
established frameworks (Cochrane Risk of Bias, PROBAST, Microsoft Responsible
AI checklist) and applied it to 35 peer-reviewed studies. Our analysis reveals
pervasive gaps: although data sampling and missing data practices are well
documented, most studies omit explicit fairness framing, subgroup analyses, and
transparent discussion of potential harms. In response, we introduce a
four-stage fairness-oriented framework called ACAR (Awareness,
Conceptualization, Application, Reporting), with guiding questions derived from
our systematic literature review to help researchers address fairness across
the ML lifecycle. We conclude with actionable recommendations for public health
ML practitioners to consistently consider algorithmic bias and foster
transparency, ensuring that algorithmic innovations advance health equity
rather than undermine it.

</details>


### [78] [NAEL: Non-Anthropocentric Ethical Logic](https://arxiv.org/abs/2510.14676)
*Bianca Maria Lerma,Rafael Peñaloza*

Main category: cs.AI

TL;DR: NAEL是一个基于主动推理和符号推理的非人类中心主义伦理框架，将伦理行为形式化为智能系统在动态多智能体环境中最小化全局期望自由能量的涌现属性。


<details>
  <summary>Details</summary>
Motivation: 传统AI伦理方法过于人类中心主义，现有伦理模型存在局限性，需要开发能够在不预设人类道德直觉的情况下产生情境敏感、自适应和关系性伦理行为的系统。

Method: 提出神经符号架构，使智能体能够在不确定环境中评估其行为的伦理后果，通过最小化全局期望自由能量来形式化伦理行为。

Result: 通过伦理资源分配的案例研究展示了NAEL能够动态平衡自我保护、认知学习和集体福利。

Conclusion: NAEL为人工智能体提供了一个非人类中心主义的伦理框架，能够产生适应性的伦理行为，克服了现有伦理模型的局限性。

Abstract: We introduce NAEL (Non-Anthropocentric Ethical Logic), a novel ethical
framework for artificial agents grounded in active inference and symbolic
reasoning. Departing from conventional, human-centred approaches to AI ethics,
NAEL formalizes ethical behaviour as an emergent property of intelligent
systems minimizing global expected free energy in dynamic, multi-agent
environments. We propose a neuro-symbolic architecture to allow agents to
evaluate the ethical consequences of their actions in uncertain settings. The
proposed system addresses the limitations of existing ethical models by
allowing agents to develop context-sensitive, adaptive, and relational ethical
behaviour without presupposing anthropomorphic moral intuitions. A case study
involving ethical resource distribution illustrates NAEL's dynamic balancing of
self-preservation, epistemic learning, and collective welfare.

</details>


### [79] [Practical, Utilitarian Algorithm Configuration](https://arxiv.org/abs/2510.14683)
*Devon Graham,Kevin Leyton-Brown*

Main category: cs.AI

TL;DR: 本文改进了COUP算法配置方法，在不牺牲理论保证的前提下显著提升其实际性能，使其能与广泛使用的启发式配置方法竞争，并通过案例研究展示了算法选择解决方案对效用函数变化的鲁棒性分析。


<details>
  <summary>Details</summary>
Motivation: COUP算法配置方法虽然具有强大的理论保证，但实际性能表现不佳。本文旨在填补这一空白，将基于理论的实用算法配置方法提升到与广泛使用的无性能保证启发式配置方法相竞争的水平。

Method: 提出了一系列对COUP的改进措施，这些改进在不降低理论保证的前提下提升了其经验性能，并通过实验验证了这些改进的效益。

Result: 改进后的COUP算法配置方法在实证性能上得到显著提升，能够与广泛使用的启发式配置方法竞争，同时保持理论性能保证。

Conclusion: 本文成功将基于理论的实用算法配置方法推向实用化，使其在保持理论保证的同时具备与现有启发式方法竞争的实际性能，为算法配置问题提供了更可靠的解决方案。

Abstract: Utilitarian algorithm configuration identifies a parameter setting for a
given algorithm that maximizes a user's utility. Utility functions offer a
theoretically well-grounded approach to optimizing decision-making under
uncertainty and are flexible enough to capture a user's preferences over
algorithm runtimes (e.g., they can describe a sharp cutoff after which a
solution is no longer required, a per-hour cost for compute, or diminishing
returns from algorithms that take longer to run). COUP is a recently-introduced
utilitarian algorithm configuration procedure which was designed mainly to
offer strong theoretical guarantees about the quality of the configuration it
returns, with less attention paid to its practical performance. This paper
closes that gap, bringing theoretically-grounded, utilitarian algorithm
configuration to the point where it is competitive with widely used, heuristic
configuration procedures that offer no performance guarantees. We present a
series of improvements to COUP that improve its empirical performance without
degrading its theoretical guarantees and demonstrate their benefit
experimentally. Using a case study, we also illustrate ways of exploring the
robustness of a given solution to the algorithm selection problem to variations
in the utility function.

</details>


### [80] [Purifying Task Vectors in Knowledge-Aware Subspace for Model Merging](https://arxiv.org/abs/2510.14697)
*Bang An,Yibo Yang,Philip Torr,Bernard Ghanem*

Main category: cs.AI

TL;DR: 提出了PAVE方法，通过在知识感知子空间中净化任务向量来减少冗余，提升模型合并性能


<details>
  <summary>Details</summary>
Motivation: 现有模型合并方法中，任务向量包含任务无关的冗余信息，导致合并模型性能显著下降，而现有随机丢弃参数的方法缺乏知识感知

Method: 使用训练样本获取协方差矩阵，通过上下文导向的奇异值分解识别任务相关权重分量，在知识感知子空间中分离任务相关和冗余组件，并引入谱秩分配策略进行公平剪枝

Result: PAVE作为即插即用方案，在各种任务向量合并方法中都能有效提升性能

Conclusion: PAVE方法能够有效净化任务向量中的冗余信息，显著提升模型合并的性能

Abstract: Model merging aims to integrate task-specific abilities from individually
fine-tuned models into a single model without extra training. In recent model
merging methods, task vector has become a fundamental building block, as it can
encapsulate the residual information from finetuning. However, the merged model
often suffers from notable performance degradation due to the conflicts caused
by task-irrelevant redundancy in task vectors. Existing efforts in overcoming
redundancy by randomly dropping elements in the parameter space involves
randomness and lacks knowledge awareness. To address these challenges, in this
study, we propose Purifying TAsk Vectors (PAVE) in knowledge-aware subspace.
Concretely, we sample some training examples from each task, and feed them into
their corresponding fine-tuned models to acquire the covariance matrices before
linear layers. We then perform a context-oriented singular value decomposition,
which accentuates the weight components most relevant to the target knowledge.
As a result, we can split fine-tuned model weights into task-relevant and
redundant components in the knowledge-aware subspace, and purify the task
vector by pruning the redundant components. To induce fair pruning efforts
across models, we further introduce a spectral rank allocation strategy by
optimizing a normalized activated pruning error. The task vector purification
by our method as a plug-and-play scheme is applicable across various task
vector-based merging methods to improve their performance. In experiments, we
demonstrate the effectiveness of PAVE across a diverse set of merging methods,
tasks, and model architectures.

</details>


### [81] [Cognitive-Aligned Spatio-Temporal Large Language Models For Next Point-of-Interest Prediction](https://arxiv.org/abs/2510.14702)
*Penglong Zhai,Jie Li,Fanyi Di,Yue Liu,Yifang Yuan,Jie Huang,Peng Wu,Sicong Wang,Mingyang Yin,Tingting Hu,Yao Xu,Xin Li*

Main category: cs.AI

TL;DR: CoAST是一个认知对齐的时空大语言模型框架，通过自然语言接口整合世界知识、时空轨迹模式和情境信息，用于下一个兴趣点推荐任务。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型主要基于非结构化文本预训练，缺乏对结构化地理实体和序列移动模式的理解，且工业级POI推荐需要融入世界知识和人类认知对齐（如季节、天气、节假日、用户画像）来提升用户体验和推荐性能。

Method: CoAST包含两个阶段：(1) 推荐知识获取：在脱敏用户的时空轨迹数据上继续预训练；(2) 认知对齐：通过监督微调和强化学习将认知判断与人类偏好对齐。

Result: 在多个真实数据集上的离线实验和在AMAP App首页"猜你想去"的在线实验都证明了CoAST的有效性。

Conclusion: CoAST框架成功解决了LLMs在POI推荐任务中的局限性，通过融入时空知识和认知对齐显著提升了推荐性能。

Abstract: The next point-of-interest (POI) recommendation task aims to predict the
users' immediate next destinations based on their preferences and historical
check-ins, holding significant value in location-based services. Recently,
large language models (LLMs) have shown great potential in recommender systems,
which treat the next POI prediction in a generative manner. However, these
LLMs, pretrained primarily on vast corpora of unstructured text, lack the
native understanding of structured geographical entities and sequential
mobility patterns required for next POI prediction tasks. Moreover, in
industrial-scale POI prediction applications, incorporating world knowledge and
alignment of human cognition, such as seasons, weather conditions, holidays,
and users' profiles (such as habits, occupation, and preferences), can enhance
the user experience while improving recommendation performance. To address
these issues, we propose CoAST (Cognitive-Aligned Spatial-Temporal LLMs), a
framework employing natural language as an interface, allowing for the
incorporation of world knowledge, spatio-temporal trajectory patterns,
profiles, and situational information. Specifically, CoAST mainly comprises of
2 stages: (1) Recommendation Knowledge Acquisition through continued
pretraining on the enriched spatial-temporal trajectory data of the
desensitized users; (2) Cognitive Alignment to align cognitive judgments with
human preferences using enriched training data through Supervised Fine-Tuning
(SFT) and a subsequent Reinforcement Learning (RL) phase. Extensive offline
experiments on various real-world datasets and online experiments deployed in
"Guess Where You Go" of AMAP App homepage demonstrate the effectiveness of
CoAST.

</details>


### [82] [ToolPRM: Fine-Grained Inference Scaling of Structured Outputs for Function Calling](https://arxiv.org/abs/2510.14703)
*Jianghao Lin,Yuanyuan Shi,Xin Peng,Renjie Ding,Hairui Wang,Yuxuan Peng,Bizhe Bai,Weixi Song,Fengshuo Bai,Huacan Chai,Weinan Zhang,Fei Huang,Ying Wen*

Main category: cs.AI

TL;DR: 提出了一个结合细粒度束搜索和过程奖励模型ToolPRM的推理扩展框架，用于提升大语言模型在结构化输出（如函数调用）任务中的性能。


<details>
  <summary>Details</summary>
Motivation: 当前推理扩展研究主要关注非结构化输出生成任务，而在结构化输出（如函数调用）方面的应用研究不足，需要填补这一空白。

Method: 构建了首个细粒度调用内过程监督数据集，使用函数掩码技术自动标注以提供步骤级奖励；提出ToolPRM过程奖励模型来评分单个函数调用的内部步骤；结合细粒度束搜索进行推理扩展。

Result: ToolPRM在预测准确性上优于粗粒度和结果奖励模型；配备ToolPRM的推理扩展技术显著提升了骨干模型在各种函数调用任务和基准测试中的性能。

Conclusion: 揭示了将推理扩展技术应用于结构化输出的关键原则："多探索少保留"，这是由于结构化函数调用生成的不可恢复性特征决定的。

Abstract: Large language models (LLMs) are increasingly demonstrating strong
capabilities as autonomous agents, with function calling serving as a core
mechanism for interaction with the environment. Meanwhile, inference scaling
has become a cutting-edge technique to enhance LLM performance by allocating
more computational resources during the inference process. However, current
research on inference scaling primarily focuses on unstructured output
generation tasks, leaving its application in structured outputs, like function
calling, largely underexplored. To bridge this gap, we propose an inference
scaling framework that combines fine-grained beam search with a process reward
model, ToolPRM, which scores the internal steps of each single function call.
To train ToolPRM, we construct the first fine-grained intra-call process
supervision dataset, automatically annotated with function-masking techniques
to provide step-level rewards for structured tool-use reasoning. Extensive
experiments demonstrate that ToolPRM beats the coarse-grained and outcome
reward models in terms of predictive accuracy, indicating its stronger
capability in supervising the function calling inference process. Inference
scaling technique equipped with ToolPRM also significantly improves the
backbone model performance across various function calling tasks and
benchmarks. More importantly, we reveal a key principle for applying inference
scaling techniques to structured outputs: "explore more but retain less" due to
the unrecoverability characteristics of structured function calling generation.

</details>


### [83] [SimKO: Simple Pass@K Policy Optimization](https://arxiv.org/abs/2510.14807)
*Ruotian Peng,Yi Ren,Zhouliang Yu,Weiyang Liu,Yandong Wen*

Main category: cs.AI

TL;DR: 本文提出了SimKO方法来解决强化学习可验证奖励(RLVR)中的过度集中问题，该方法通过不对称地调整正确和错误回答的概率分布来鼓励探索，提高pass@K性能。


<details>
  <summary>Details</summary>
Motivation: 现有RLVR方法存在系统性偏差，倾向于利用而非探索，表现为pass@1提升但pass@K(K>1)下降。作者通过分析训练动态发现概率集中效应是导致此问题的原因。

Method: 提出SimKO方法：对于已验证正确的回答，提升top-K候选的概率；对于已验证错误的回答，对top-1候选施加更强的惩罚。该方法在具有高熵的token上应用效果最佳。

Result: 在各种数学和逻辑推理基准测试中，SimKO在广泛的K值范围内一致地产生更高的pass@K性能，有效改善了RLVR的探索能力。

Conclusion: SimKO通过缓解概率过度集中问题，为改进RLVR的探索提供了一种简单有效的方法，显著提升了pass@K性能。

Abstract: Reinforcement learning with verifiable rewards (RLVR) has advanced the
reasoning capabilities of large language models (LLMs). However, prevailing
RLVR methods exhibit a systematic bias toward exploitation over exploration, as
evidenced by improved pass@1 but reduced pass@K (K>1) performance. To
understand this issue, we analyze training dynamics of RLVR methods by tracking
the token-level probability distributions over vocabulary candidates. Our
analysis reveals a consistent probability concentration effect where the top-1
candidate increasingly accumulates probability mass and suppresses that of
other candidates. More importantly, stronger over-concentration correlates with
worse pass@K performance. Inspired by this finding, we propose Simple Pass@K
Optimization (SimKO), a method designed to mitigate the over-concentration
issue, thereby encouraging exploration. SimKO operates in an asymmetrical
manner. For verified-correct responses, it boosts the probabilities of the
top-K candidates. For verified-incorrect responses, it applies stronger
penalties to the top-1 candidate. We observe that this asymmetric design is
particularly effective at mitigating over-concentration when applied at tokens
with high entropy. Across various math and logical-reasoning benchmarks, SimKO
consistently yields higher pass@K for a wide range of K, providing a simple way
to improve RLVR's exploration.

</details>


### [84] [Agentic NL2SQL to Reduce Computational Costs](https://arxiv.org/abs/2510.14808)
*Dominik Jehle,Lennart Purucker,Frank Hutter*

Main category: cs.AI

TL;DR: Datalake Agent是一个代理系统，通过交互式循环减少NL2SQL任务中的元信息使用，显著降低LLM的token消耗和成本，同时保持竞争力。


<details>
  <summary>Details</summary>
Motivation: 传统NL2SQL方法需要处理大量数据库元信息，导致提示过长、token消耗大、处理成本高。

Method: 采用交互式循环和推理框架，选择性请求必要信息来解决表格问答任务，而不是一次性使用所有元信息。

Result: 在23个数据库和100个表格问答任务上的评估显示，LLM使用的token减少了87%，成本显著降低，同时保持竞争性能。

Conclusion: Datalake Agent通过智能信息选择机制，有效解决了NL2SQL任务中的元信息过载问题，实现了成本效益和性能的平衡。

Abstract: Translating natural language queries into SQL queries (NL2SQL or Text-to-SQL)
has recently been empowered by large language models (LLMs). Using LLMs to
perform NL2SQL methods on a large collection of SQL databases necessitates
processing large quantities of meta-information about the databases, which in
turn results in lengthy prompts with many tokens and high processing costs. To
address this challenge, we introduce Datalake Agent, an agentic system designed
to enable an LLM to solve NL2SQL tasks more efficiently. Instead of utilizing
direct solvers for NL2SQL that call the LLM once with all meta-information in
the prompt, the Datalake Agent employs an interactive loop to reduce the
utilized meta-information. Within the loop, the LLM is used in a reasoning
framework that selectively requests only the necessary information to solve a
table question answering task. We evaluate the Datalake Agent on a collection
of 23 databases with 100 table question answering tasks. The Datalake Agent
reduces the tokens used by the LLM by up to 87\% and thus allows for
substantial cost reductions while maintaining competitive performance.

</details>


### [85] [RoboGPT-R1: Enhancing Robot Planning with Reinforcement Learning](https://arxiv.org/abs/2510.14828)
*Jinrui Liu,Bingyan Nie,Boyu Li,Yaran Chen,Yuze Wang,Shunsen He,Haoran Li*

Main category: cs.AI

TL;DR: 提出了RoboGPT-R1，一个两阶段微调框架，通过监督训练和强化学习相结合的方法来提升具身智能体的推理能力，在EmbodiedBench基准上显著优于GPT-4o-mini和其他模型。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型和视觉语言模型在长视野操作任务中仍面临常识和推理能力受限的问题，监督微调方法存在泛化能力差和物理理解不足的局限性。

Method: 采用两阶段微调框架：监督训练从专家序列获取基础知识，然后通过强化学习解决模型在视觉空间理解和推理方面的不足，设计了考虑长期性能和动作约束的基于规则的奖励函数。

Result: 在Qwen2.5-VL-3B上训练的推理模型在EmbodiedBench基准上比GPT-4o-mini高出21.33%，比在Qwen2.5-VL-7B上训练的其他工作高出20.33%。

Conclusion: 两阶段微调框架有效提升了具身智能体的推理能力，证明了强化学习在解决视觉空间理解和多步推理任务中的重要性。

Abstract: Improving the reasoning capabilities of embodied agents is crucial for robots
to complete complex human instructions in long-view manipulation tasks
successfully. Despite the success of large language models and vision language
models based on Supervised Fine-Tuning (SFT) in planning tasks, they continue
facing challenges in performing long-horizon manipulation tasks in complex
real-world environments, owing to their restricted common sense and reasoning
capabilities. Considering that aligning general-purpose vision language models
to robotic planning tasks via supervised fine-tuning suffers from poor
generalization and insufficient physical understanding, we propose RoboGPT-R1,
a two-stage fine-tuning framework for embodied planning. In this framework,
supervised training acquires foundational knowledge through expert sequences,
followed by RL to address the model's shortcomings in visual-spatial
understanding and reasoning. To achieve physical understanding and action
sequence consistency in multi-step reasoning tasks, we design a rule-based
reward function that simultaneously considers long-horizon performance and
action constraint in the environment. The reasoning model, trained on
Qwen2.5-VL-3B, significantly outperforms the larger-scale model, GPT-4o-mini,
by 21.33% and surpasses other work trained on Qwen2.5-VL-7B by 20.33% on the
EmbodiedBench benchmark.

</details>


### [86] [Boosting Instruction Following at Scale](https://arxiv.org/abs/2510.14842)
*Ben Elder,Evelyn Duesterwald,Vinod Muthusamy*

Main category: cs.AI

TL;DR: Instruction Boosting是一种后生成方法，通过增加指令遵循率来提高LLM提示指令的可靠性，在2条指令时提升7个百分点，10条指令时提升4个百分点。


<details>
  <summary>Details</summary>
Motivation: 开发者通常通过精心设计提示来影响LLM行为，但仅添加更多指令无法保证它们会被遵循，需要提高指令遵循的可靠性。

Method: 引入Instruction Boosting作为后生成方法，并创建SCALEDIF基准测试集（每条数据样本最多包含10条指令），开发定量冲突评分工具分析指令冲突。

Result: Instruction Boosting显著提高了指令遵循率，同时发现随着指令数量增加，性能下降的主要原因是指令间的紧张和冲突关系。

Conclusion: Instruction Boosting有效提升LLM的指令遵循能力，冲突评分工具能为开发者提供关于额外提示指令对模型性能影响的反馈。

Abstract: A typical approach developers follow to influence an LLM's behavior in an
application is through careful manipulation of the prompt, such as by adding or
modifying instructions. However, merely adding more instructions provides
little assurance that they will actually be followed. We introduce Instruction
Boosting as a post-generation method to increase the reliability of LLM prompt
instructions. We show that Instruction Boosting improves the instruction
following rate by up to 7 points for two instructions and up to 4 points for
ten instructions. To demonstrate these results we introduce SCALEDIF, a
benchmark with a scaled instruction volume of up to ten instructions per data
sample. We also present an analysis of the commonly observed trend that
performance degrades as more instructions are added. We show that an important
factor contributing to this trend is the degree of tension and conflict that
arises as the number of instructions is increased. We contribute a quantitative
conflict scoring tool that explains the observed performance trends and
provides feedback to developers on the impact that additional prompt
instructions have on a model's performance.

</details>


### [87] [Where to Search: Measure the Prior-Structured Search Space of LLM Agents](https://arxiv.org/abs/2510.14846)
*Zhuo-Yang Song*

Main category: cs.AI

TL;DR: 提出了一种紧凑的形式理论来描述和衡量由领域先验引导的LLM辅助迭代搜索，通过模糊关系算子表示智能体，并引入覆盖生成函数来衡量可达性难度。


<details>
  <summary>Details</summary>
Motivation: 基于LLM的生成-过滤-精炼迭代范式在推理、编程和科学发现中取得进展，但搜索效果取决于如何将领域先验编码到结构化的假设空间中。

Method: 将智能体表示为输入输出的模糊关系算子，通过安全包络约束智能体；使用单参数加权所有可达路径得到覆盖生成函数，提供搜索的几何解释。

Result: 提供了可测试的推断并通过多数投票实例进行验证，为衡量智能体及其搜索空间提供了可行的语言和操作工具。

Conclusion: 该理论为LLM构建的迭代搜索提供了系统的形式化描述，能够有效衡量智能体和搜索空间。

Abstract: The generate-filter-refine (iterative paradigm) based on large language
models (LLMs) has achieved progress in reasoning, programming, and program
discovery in AI+Science. However, the effectiveness of search depends on where
to search, namely, how to encode the domain prior into an operationally
structured hypothesis space. To this end, this paper proposes a compact formal
theory that describes and measures LLM-assisted iterative search guided by
domain priors. We represent an agent as a fuzzy relation operator on inputs and
outputs to capture feasible transitions; the agent is thereby constrained by a
fixed safety envelope. To describe multi-step reasoning/search, we weight all
reachable paths by a single continuation parameter and sum them to obtain a
coverage generating function; this induces a measure of reachability
difficulty; and it provides a geometric interpretation of search on the graph
induced by the safety envelope. We further provide the simplest testable
inferences and validate them via a majority-vote instantiation. This theory
offers a workable language and operational tools to measure agents and their
search spaces, proposing a systematic formal description of iterative search
constructed by LLMs.

</details>


### [88] [LabOS: The AI-XR Co-Scientist That Sees and Works With Humans](https://arxiv.org/abs/2510.14861)
*Le Cong,Zaixi Zhang,Xiaotong Wang,Yin Di,Ruofan Jin,Michal Gerasimiuk,Yinkai Wang,Ravi K. Dinesh,David Smerkous,Alex Smerkous,Xuekun Wu,Shilong Liu,Peishan Li,Yi Zhu,Simran Serrao,Ning Zhao,Imran A. Mohammad,John B. Sunwoo,Joseph C. Wu,Mengdi Wang*

Main category: cs.AI

TL;DR: LabOS是首个将计算推理与物理实验结合的AI联合科学家系统，通过多模态感知、自进化代理和XR技术实现人机协作，让AI能够参与实际实验过程。


<details>
  <summary>Details</summary>
Motivation: 现代科学发展需要将思想与行动结合，传统AI主要停留在计算设计层面，无法实际参与物理实验。LabOS旨在让AI能够真正进入实验室环境，与科学家协作进行实验。

Method: 通过连接多模型AI代理、智能眼镜和人类-AI协作，利用多模态感知理解实验环境，通过XR技术实现实时人机交互，让AI能够看到科学家所见并协助实验执行。

Result: 在癌症免疫治疗靶点发现和干细胞工程等应用中，LabOS成功展示了AI从计算设计到实际参与的转变，将实验室转变为智能协作环境。

Conclusion: LabOS证明了AI能够超越纯计算设计，真正参与物理实验过程，实现人类与机器发现的共同进化，开创了人机协作科学研究的新范式。

Abstract: Modern science advances fastest when thought meets action. LabOS represents
the first AI co-scientist that unites computational reasoning with physical
experimentation through multimodal perception, self-evolving agents, and
Entended-Reality(XR)-enabled human-AI collaboration. By connecting multi-model
AI agents, smart glasses, and human-AI collaboration, LabOS allows AI to see
what scientists see, understand experimental context, and assist in real-time
execution. Across applications--from cancer immunotherapy target discovery to
stem-cell engineering -- LabOS shows that AI can move beyond computational
design to participation, turning the laboratory into an intelligent,
collaborative environment where human and machine discovery evolve together.

</details>


### [89] [The Gatekeeper Knows Enough](https://arxiv.org/abs/2510.14881)
*Fikresilase Wondmeneh Abebayew*

Main category: cs.AI

TL;DR: 提出了Gatekeeper Protocol框架，通过低保真潜在状态表示和按需请求高保真上下文的方法，解决LLM代理在复杂知识系统中的上下文窗口限制和状态同步问题。


<details>
  <summary>Details</summary>
Motivation: LLM作为自主代理部署时，受限于有限的上下文窗口和状态不同步问题，导致输出不可靠、行为不可预测和资源使用效率低下，特别是在与代码库和文档等结构化知识系统交互时。

Method: 引入Gatekeeper Protocol框架，要求代理先在低保真潜在状态表示上进行操作和推理，然后按需策略性地请求高保真上下文。所有交互通过统一的JSON格式进行中介，作为声明性、状态同步的协议。

Result: 在软件开发的参考实现Sage中，该方法显著提高了代理可靠性，通过最小化token消耗改善了计算效率，并实现了与复杂系统的可扩展交互。

Conclusion: Gatekeeper Protocol为构建更稳健、可预测和基础扎实的AI代理提供了基础方法论，适用于任何结构化知识领域。

Abstract: Large Language Models (LLMs) are increasingly deployed as autonomous agents,
yet their practical utility is fundamentally constrained by a limited context
window and state desynchronization resulting from the LLMs' stateless nature
and inefficient context management. These limitations lead to unreliable
output, unpredictable behavior, and inefficient resource usage, particularly
when interacting with large, structured, and sensitive knowledge systems such
as codebases and documents. To address these challenges, we introduce the
Gatekeeper Protocol, a novel, domain-agnostic framework that governs
agent-system interactions. Our protocol mandates that the agent first operate
and reason on a minimalist, low-fidelity "latent state" representation of the
system to strategically request high-fidelity context on demand. All
interactions are mediated through a unified JSON format that serves as a
declarative, state-synchronized protocol, ensuring the agent's model of the
system remains verifiably grounded in the system's reality. We demonstrate the
efficacy of this protocol with Sage, a reference implementation of the
Gatekeeper Protocol for software development. Our results show that this
approach significantly increases agent reliability, improves computational
efficiency by minimizing token consumption, and enables scalable interaction
with complex systems, creating a foundational methodology for building more
robust, predictable, and grounded AI agents for any structured knowledge
domain.

</details>


### [90] [Budget-aware Test-time Scaling via Discriminative Verification](https://arxiv.org/abs/2510.14913)
*Kyle Montgomery,Sijun Tan,Yuqi Chen,Siyuan Zhuang,Tianjun Zhang,Raluca Ada Popa,Chenguang Wang*

Main category: cs.AI

TL;DR: 提出了一种预算感知的判别式验证方法，结合自一致性机制，在固定计算预算下显著优于生成式验证方法。


<details>
  <summary>Details</summary>
Motivation: 现有生成式验证器虽然性能优秀，但计算成本过高，限制了实际应用。需要寻找更高效的验证策略。

Method: 采用判别式验证器结合自一致性的混合方法，在固定计算预算下进行测试时扩展。

Result: 在AIME2025上实现了比最先进生成式验证方法高15.3%的准确率，证明了该方法的高效性。

Conclusion: 预算感知的判别式验证不仅是对自一致性的免费升级，而且是比昂贵生成式技术更有效和高效的替代方案。

Abstract: Test-time scaling is a powerful strategy for boosting the performance of
large language models on complex reasoning tasks. While state-of-the-art
approaches often employ generative verifiers to select the best solution from a
pool of candidates, this method incurs prohibitive computational costs,
limiting its practicality. In this work, we shift the focus to a more
budget-aware paradigm: discriminative verification. We conduct a thorough
empirical analysis and demonstrate that while discriminative verifiers may
underperform in isolation, combining them with self-consistency in a hybrid
approach creates a powerful and efficient test-time scaling mechanism. Notably,
under a fixed compute budget, this hybrid approach surpasses state-of-the-art
generative verification by a significant margin: achieving up to 15.3\% higher
accuracy on AIME2025. Our findings establish that for practical, real-world
applications, budget-aware scaling with discriminative verifiers is not only a
"free" upgrade over self-consistency, but also a more effective and efficient
alternative to costly generative techniques. Code is available at
https://github.com/wang-research-lab/verification.

</details>


### [91] [TRI-DEP: A Trimodal Comparative Study for Depression Detection Using Speech, Text, and EEG](https://arxiv.org/abs/2510.14922)
*Annisaa Fitri Nurfidausi,Eleonora Mancini,Paolo Torroni*

Main category: cs.AI

TL;DR: 本文系统研究了多模态抑郁症检测，比较了EEG、语音和文本的特征表示与建模策略，发现三模态组合能提升检测性能，预训练嵌入优于手工特征，精心设计的三模态模型达到SOTA效果。


<details>
  <summary>Details</summary>
Motivation: 现有抑郁症自动检测研究存在范围有限、缺乏系统特征比较和评估协议不一致等问题，需要系统探索多模态特征表示和建模策略。

Method: 系统评估手工特征与预训练嵌入，比较不同神经编码器的有效性，分析单模态、双模态和三模态配置，研究融合策略并关注EEG的作用，采用一致的受试者独立分割进行稳健基准测试。

Result: EEG、语音和文本三模态组合能增强多模态检测性能；预训练嵌入优于手工特征；精心设计的三模态模型达到最先进性能。

Conclusion: 本研究为未来多模态抑郁症检测研究奠定了基础，证明了多模态融合和预训练嵌入的有效性。

Abstract: Depression is a widespread mental health disorder, yet its automatic
detection remains challenging. Prior work has explored unimodal and multimodal
approaches, with multimodal systems showing promise by leveraging complementary
signals. However, existing studies are limited in scope, lack systematic
comparisons of features, and suffer from inconsistent evaluation protocols. We
address these gaps by systematically exploring feature representations and
modelling strategies across EEG, together with speech and text. We evaluate
handcrafted features versus pre-trained embeddings, assess the effectiveness of
different neural encoders, compare unimodal, bimodal, and trimodal
configurations, and analyse fusion strategies with attention to the role of
EEG. Consistent subject-independent splits are applied to ensure robust,
reproducible benchmarking. Our results show that (i) the combination of EEG,
speech and text modalities enhances multimodal detection, (ii) pretrained
embeddings outperform handcrafted features, and (iii) carefully designed
trimodal models achieve state-of-the-art performance. Our work lays the
groundwork for future research in multimodal depression detection.

</details>


### [92] [Stable but Miscalibrated: A Kantian View on Overconfidence from Filters to Large Language Models](https://arxiv.org/abs/2510.14925)
*Akira Okutomi*

Main category: cs.AI

TL;DR: 将康德的《纯粹理性批判》重新解释为反馈稳定性理论，提出复合不稳定性指数(H-Risk)来衡量推理系统的稳定性，发现高H-Risk预测过度自信错误，并应用于大语言模型分析幻觉问题。


<details>
  <summary>Details</summary>
Motivation: 建立康德理性自我限制理论与反馈控制理论之间的结构性桥梁，为诊断和减少推理系统中的过度自信提供原则性视角。

Method: 提出复合不稳定性指数H-Risk，结合谱边界、条件数、时间敏感性和创新放大等指标，在线性高斯模拟和大型语言模型中进行验证。

Result: 在线性高斯模拟中，高H-Risk预测过度自信错误；在LLMs中，脆弱的内部动态与校准错误和幻觉相关，批判式提示对校准和幻觉的影响不一。

Conclusion: 康德的自我限制概念与反馈控制理论之间存在结构性联系，这为诊断和选择性减少推理系统中的过度自信提供了新的理论框架。

Abstract: We reinterpret Kant's Critique of Pure Reason as a theory of feedback
stability, viewing reason as a regulator that keeps inference within the bounds
of possible experience. We formalize this intuition via a composite instability
index (H-Risk) combining spectral margin, conditioning, temporal sensitivity,
and innovation amplification. In linear-Gaussian simulations, higher H-Risk
predicts overconfident errors even under formal stability, revealing a gap
between nominal and epistemic stability. Extending to large language models
(LLMs), we find that fragile internal dynamics correlate with miscalibration
and hallucination, while critique-style prompts show mixed effects on
calibration and hallucination. These results suggest a structural bridge
between Kantian self-limitation and feedback control, offering a principled
lens for diagnosing -- and selectively reducing -- overconfidence in reasoning
systems. This is a preliminary version; supplementary experiments and broader
replication will be reported in a future revision.

</details>


### [93] [GroundedPRM: Tree-Guided and Fidelity-Aware Process Reward Modeling for Step-Level Reasoning](https://arxiv.org/abs/2510.14942)
*Yao Zhang,Yu Wu,Haowei Zhang,Weiguo Li,Haokun Chen,Jingpei Wu,Guohao Li,Zhen Han,Volker Tresp*

Main category: cs.AI

TL;DR: GroundedPRM是一个基于树引导和保真度感知的自动过程监督框架，通过蒙特卡洛树搜索构建结构化推理路径，使用外部工具验证中间步骤，结合步骤级验证和全局结果评估，在少量自动标注数据上实现高质量的多步推理监督。


<details>
  <summary>Details</summary>
Motivation: 现有过程奖励模型面临三大挑战：噪声奖励、低事实保真度以及与步骤级推理目标的不对齐。这些问题的根源在于缺乏可扩展的高质量标注，现有方法依赖昂贵的人工标注、易产生幻觉的LLM自评估或存在信用分配错误的蒙特卡洛估计。

Method: 1) 使用蒙特卡洛树搜索构建结构化推理路径，实现细粒度信用分配；2) 通过外部工具验证每个中间步骤，提供基于执行的正确性信号；3) 设计混合奖励聚合机制，融合工具验证和MCTS反馈；4) 将奖励信号格式化为增强推理的生成结构，提高可解释性。

Result: 仅使用4万个自动标注样本（最佳自动监督PRM数据量的10%），在ProcessBench上实现高达26%的相对性能提升。在奖励引导的贪婪搜索中，甚至优于使用人工标注监督训练的PRM。

Conclusion: GroundedPRM提供了一条可扩展且可验证的高质量过程级推理路径，解决了现有PRM的核心限制，在少量数据上实现了优越性能。

Abstract: Process Reward Models (PRMs) aim to improve multi-step reasoning in Large
Language Models (LLMs) by supervising intermediate steps and identifying
errors. However, building effective PRMs remains challenging due to the lack of
scalable, high-quality annotations. Existing approaches rely on costly human
labeling, LLM-based self-evaluation that is prone to hallucination, or Monte
Carlo (MC) estimation, which infers step quality solely from rollout outcomes
and often introduces noisy, misaligned supervision due to credit
misattribution. These issues result in three core limitations: noisy rewards,
low factual fidelity, and misalignment with step-level reasoning objectives. To
address these challenges, we introduce GroundedPRM, a tree-guided and
fidelity-aware framework for automatic process supervision. To reduce reward
noise and enable fine-grained credit assignment, we construct structured
reasoning paths via Monte Carlo Tree Search (MCTS). To eliminate hallucinated
supervision, we validate each intermediate step using an external tool,
providing execution-grounded correctness signals. To combine both step-level
validation and global outcome assessment, we design a hybrid reward aggregation
mechanism that fuses tool-based verification with MCTS-derived feedback.
Finally, we format the reward signal into a rationale-enhanced, generative
structure to promote interpretability and compatibility with instruction-tuned
LLMs. GroundedPRM is trained on only 40K automatically labeled samples,
amounting to just 10% of the data used by the best-performing PRM trained with
auto-labeled supervision. Nevertheless, it achieves up to a 26% relative
improvement in average performance on ProcessBench. When used for reward-guided
greedy search, GroundedPRM outperforms even PRMs trained with human-labeled
supervision, offering a scalable and verifiable path toward high-quality
process-level reasoning.

</details>


### [94] [Agentic Design of Compositional Machines](https://arxiv.org/abs/2510.14980)
*Wenqian Zhang,Weiyang Liu,Zhen Liu*

Main category: cs.AI

TL;DR: 该研究探讨大型语言模型是否能够进行复杂机器设计，通过BesiegeField测试平台评估LLM在组合式机器设计任务中的表现，发现当前开源模型存在不足，并探索了强化学习作为改进路径。


<details>
  <summary>Details</summary>
Motivation: 研究动机是探索大型语言模型在复杂机器设计方面的能力，这是人类智能的重要标志和工程实践的基础。通过机器组合设计任务来测试LLM的创造能力。

Method: 引入BesiegeField测试平台，基于Besiege游戏构建，支持部件组装、物理模拟和奖励驱动评估。使用代理工作流程对最先进的LLM进行基准测试，并探索强化学习微调方法。

Result: 发现当前开源LLM在空间推理、策略组装和指令遵循等关键能力上表现不足。通过强化学习微调实验展示了改进潜力，但仍有挑战存在。

Conclusion: 研究表明LLM在机器设计任务中仍有局限，强化学习是潜在的改进方向，但在语言、机器设计和物理推理的交叉领域仍存在开放挑战。

Abstract: The design of complex machines stands as both a marker of human intelligence
and a foundation of engineering practice. Given recent advances in large
language models (LLMs), we ask whether they, too, can learn to create. We
approach this question through the lens of compositional machine design: a task
in which machines are assembled from standardized components to meet functional
demands like locomotion or manipulation in a simulated physical environment. To
support this investigation, we introduce BesiegeField, a testbed built on the
machine-building game Besiege, which enables part-based construction, physical
simulation and reward-driven evaluation. Using BesiegeField, we benchmark
state-of-the-art LLMs with agentic workflows and identify key capabilities
required for success, including spatial reasoning, strategic assembly, and
instruction-following. As current open-source models fall short, we explore
reinforcement learning (RL) as a path to improvement: we curate a cold-start
dataset, conduct RL finetuning experiments, and highlight open challenges at
the intersection of language, machine design, and physical reasoning.

</details>
