<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 11]
- [cs.CR](#cs.CR) [Total: 7]
- [cs.SE](#cs.SE) [Total: 9]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [Stable diffusion models reveal a persisting human and AI gap in visual creativity](https://arxiv.org/abs/2511.16814)
*Silvia Rondini,Claudia Alvarez-Martin,Paula Angermair-Barkai,Olivier Penacchio,M. Paz,Matthew Pelowski,Dan Dediu,Antoni Rodriguez-Fornells,Xim Cerda-Company*

Main category: cs.AI

TL;DR: 研究发现人类在视觉创造力方面优于AI，视觉艺术家最具创造力，其次是普通人类，而AI在人类指导下能接近普通人类水平。人类和AI评估者在创造力判断上存在显著差异。


<details>
  <summary>Details</summary>
Motivation: 探索大型语言模型在视觉创造力任务中的表现，比较人类和AI在图像生成方面的创造力差异。

Method: 比较人类参与者（视觉艺术家和非艺术家）与AI图像生成模型（两种提示条件：高人类输入和低人类输入）的图像生成能力，由人类评估者和GPT4o评估创造力。

Result: 发现清晰的创造力梯度：视觉艺术家最具创造力，其次是非艺术家，然后是受人类启发的生成AI，最后是自我引导的生成AI。增加人类指导显著提高了生成AI的创造力输出。

Conclusion: 与语言中心任务不同，生成AI模型在视觉领域面临独特挑战，因为视觉创造力依赖于感知细微差别和上下文敏感性，这些是人类特有的能力，可能无法从语言模型直接转移。

Abstract: While recent research suggests Large Language Models match human creative performance in divergent thinking tasks, visual creativity remains underexplored. This study compared image generation in human participants (Visual Artists and Non Artists) and using an image generation AI model (two prompting conditions with varying human input: high for Human Inspired, low for Self Guided). Human raters (N=255) and GPT4o evaluated the creativity of the resulting images. We found a clear creativity gradient, with Visual Artists being the most creative, followed by Non Artists, then Human Inspired generative AI, and finally Self Guided generative AI. Increased human guidance strongly improved GenAI's creative output, bringing its productions close to those of Non Artists. Notably, human and AI raters also showed vastly different creativity judgment patterns. These results suggest that, in contrast to language centered tasks, GenAI models may face unique challenges in visual domains, where creativity depends on perceptual nuance and contextual sensitivity, distinctly human capacities that may not be readily transferable from language models.

</details>


### [2] [Cognitive BASIC: An In-Model Interpreted Reasoning Language for LLMs](https://arxiv.org/abs/2511.16837)
*Oliver Kramer*

Main category: cs.AI

TL;DR: Cognitive BASIC是一种基于BASIC风格的提示语言和模型内解释器，将大语言模型的推理过程结构化，形成明确的逐步执行轨迹。


<details>
  <summary>Details</summary>
Motivation: 受复古BASIC简洁性的启发，重新利用编号行和简单命令作为可解释的认知控制层，使现代LLM能够可靠地模拟这类短程序，实现模型内部透明的多步推理。

Method: 使用自然语言解释器文件指定命令语义、内存更新和日志行为，通过心智模型解释器提取陈述性和程序性知识，检测矛盾并在必要时生成解决方案。

Result: 在三个LLM上对知识提取、冲突检测和推理任务的基准测试显示，所有模型都能执行Cognitive BASIC程序，总体表现强劲但不均匀。

Conclusion: Cognitive BASIC为LLM推理提供了结构化、可解释的框架，展示了模型执行简单程序化推理的能力。

Abstract: Cognitive BASIC is a minimal, BASIC-style prompting language and in-model interpreter that structures large language model (LLM) reasoning into explicit, stepwise execution traces. Inspired by the simplicity of retro BASIC, we repurpose numbered lines and simple commands as an interpretable cognitive control layer. Modern LLMs can reliably simulate such short programs, enabling transparent multi-step reasoning inside the model. A natural-language interpreter file specifies command semantics, memory updates, and logging behavior. Our mental-model interpreter extracts declarative and procedural knowledge, detects contradictions, and produces resolutions when necessary. A comparison across three LLMs on a benchmark of knowledge extraction, conflict detection, and reasoning tasks shows that all models can execute Cognitive BASIC programs, with overall strong but not uniform performance.

</details>


### [3] [Fantastic Bugs and Where to Find Them in AI Benchmarks](https://arxiv.org/abs/2511.16842)
*Sang Truong,Yuheng Tu,Michael Hardy,Anka Reuel,Zeyu Tang,Jirayu Burapacheep,Jonathan Perera,Chibuike Uwakwe,Ben Domingue,Nick Haber,Sanmi Koyejo*

Main category: cs.AI

TL;DR: 提出一个利用响应模式统计分析的系统性基准修订框架，通过检测统计异常值来识别无效问题，结合专家审查和LLM初步筛选，提高基准可靠性。


<details>
  <summary>Details</summary>
Motivation: 基准测试对AI发展至关重要，但无效的基准问题会破坏其可靠性。手动在数千个问题中识别和修正错误既不可行，也是可靠评估的关键瓶颈。

Method: 基于AI评估中常用的核心假设——平均分足以总结模型性能，构建单维潜在结构测量实验，通过统计异常检测识别可能无效的问题，并引入LLM法官进行初步审查。

Result: 在九个广泛使用的基准测试中，该方法指导专家审查识别问题问题的精度高达84%，显著减少了人工工作量。

Conclusion: 该框架为系统性基准修订提供了高效且可扩展的解决方案，通过统计分析和自动化初步筛选相结合，提高了基准测试的可靠性。

Abstract: Benchmarks are pivotal in driving AI progress, and invalid benchmark questions frequently undermine their reliability. Manually identifying and correcting errors among thousands of benchmark questions is not only infeasible but also a critical bottleneck for reliable evaluation. In this work, we introduce a framework for systematic benchmark revision that leverages statistical analysis of response patterns to flag potentially invalid questions for further expert review. Our approach builds on a core assumption commonly used in AI evaluations that the mean score sufficiently summarizes model performance. This implies a unidimensional latent construct underlying the measurement experiment, yielding expected ranges for various statistics for each item. When empirically estimated values for these statistics fall outside the expected range for an item, the item is more likely to be problematic. Across nine widely used benchmarks, our method guides expert review to identify problematic questions with up to 84\% precision. In addition, we introduce an LLM-judge first pass to review questions, further reducing human effort. Together, these components provide an efficient and scalable framework for systematic benchmark revision.

</details>


### [4] [Hybrid Differential Reward: Combining Temporal Difference and Action Gradients for Efficient Multi-Agent Reinforcement Learning in Cooperative Driving](https://arxiv.org/abs/2511.16916)
*Ye Han,Lijun Zhang,Dejian Meng,Zhuang Zhang*

Main category: cs.AI

TL;DR: 本文提出了一种混合差分奖励机制来解决多车协同驾驶中传统状态奖励函数存在的奖励差异消失问题，该机制结合了基于全局势函数的时序差分奖励和直接测量动作边际效用的动作梯度奖励。


<details>
  <summary>Details</summary>
Motivation: 在多车协同驾驶任务中，传统基于状态的奖励函数存在奖励差异消失问题，导致策略梯度的信噪比低，严重影响算法收敛和性能提升。

Method: 提出混合差分奖励机制，包含两个互补组件：基于全局势函数的时序差分奖励和直接测量动作边际效用的动作梯度奖励。将协同驾驶问题建模为具有时变智能体集的多智能体部分可观测马尔可夫博弈。

Result: 通过在线规划和多智能体强化学习算法的广泛实验表明，HDR机制显著提高了收敛速度和策略稳定性，引导智能体学习到能有效平衡交通效率和安全的优质协同策略。

Conclusion: HDR机制成功解决了多车协同驾驶中的奖励信号问题，为高频率连续控制任务提供了有效的奖励设计方法。

Abstract: In multi-vehicle cooperative driving tasks involving high-frequency continuous control, traditional state-based reward functions suffer from the issue of vanishing reward differences. This phenomenon results in a low signal-to-noise ratio (SNR) for policy gradients, significantly hindering algorithm convergence and performance improvement. To address this challenge, this paper proposes a novel Hybrid Differential Reward (HDR) mechanism. We first theoretically elucidate how the temporal quasi-steady nature of traffic states and the physical proximity of actions lead to the failure of traditional reward signals. Building on this analysis, the HDR framework innovatively integrates two complementary components: (1) a Temporal Difference Reward (TRD) based on a global potential function, which utilizes the evolutionary trend of potential energy to ensure optimal policy invariance and consistency with long-term objectives; and (2) an Action Gradient Reward (ARG), which directly measures the marginal utility of actions to provide a local guidance signal with a high SNR. Furthermore, we formulate the cooperative driving problem as a Multi-Agent Partially Observable Markov Game (POMDPG) with a time-varying agent set and provide a complete instantiation scheme for HDR within this framework. Extensive experiments conducted using both online planning (MCTS) and Multi-Agent Reinforcement Learning (QMIX, MAPPO, MADDPG) algorithms demonstrate that the HDR mechanism significantly improves convergence speed and policy stability. The results confirm that HDR guides agents to learn high-quality cooperative policies that effectively balance traffic efficiency and safety.

</details>


### [5] [MirrorMind: Empowering OmniScientist with the Expert Perspectives and Collective Knowledge of Human Scientists](https://arxiv.org/abs/2511.16997)
*Qingbin Zeng,Bingbing Fan,Zhiyu Chen,Sijian Ren,Zhilun Zhou,Xuhua Zhang,Yuanyi Zhen,Fengli Xu,Yong Li,Tie-Yan Liu*

Main category: cs.AI

TL;DR: MirrorMind是一个分层认知架构，通过整合个体认知轨迹和集体学科记忆来解决AI科学家在科学发现中的局限性，实现结构化的个性化科学推理。


<details>
  <summary>Details</summary>
Motivation: 现有AI科学方法将科学发现视为孤立的优化过程，忽视了知识生产的社会性和历史性本质，无法有效表示结构化的认知和社会背景。

Method: 提出三层框架：个体层面构建研究者的认知模型，领域层面映射集体知识到学科概念图，跨学科层面作为正交编排引擎，分离记忆存储与智能执行。

Result: 在四个综合任务中评估，包括作者级认知模拟、互补推理、跨学科协作促进和多智能体科学问题解决，结果显示MirrorMind能够实现结构化的个性化科学推理。

Conclusion: 通过整合个体认知深度与集体学科广度，MirrorMind超越了简单事实检索，实现了洞察生成的科学推理能力。

Abstract: The emergence of AI Scientists has demonstrated remarkable potential in automating scientific research. However, current approaches largely conceptualize scientific discovery as a solitary optimization or search process, overlooking that knowledge production is inherently a social and historical endeavor. Human scientific insight stems from two distinct yet interconnected sources. First is the individual cognitive trajectory, where a researcher's unique insight is shaped by their evolving research history and stylistic preferences; another is the collective disciplinary memory, where knowledge is sedimented into vast, interconnected networks of citations and concepts. Existing LLMs still struggle to represent these structured, high-fidelity cognitive and social contexts. To bridge this gap, we introduce MirrorMind, a hierarchical cognitive architecture that integrates dual-memory representations within a three-level framework. The Individual Level constructs high-fidelity cognitive models of individual researchers by capturing their episodic, semantic, and persona memories; the Domain Level maps collective knowledge into structured disciplinary concept graphs; and the Interdisciplinary Level that acts as an orthogonal orchestration engine. Crucially, our architecture separates memory storage from agentic execution, enabling AI scientist agents to flexibly access individual memories for unique perspectives or collective structures to reason. We evaluate MirrorMind across four comprehensive tasks, including author-level cognitive simulation, complementary reasoning, cross-disciplinary collaboration promotion, and multi-agent scientific problem solving. The results show that by integrating individual cognitive depth with collective disciplinary breadth, MirrorMind moves beyond simple fact retrieval toward structural, personalized, and insight-generating scientific reasoning.

</details>


### [6] [DAPS++: Rethinking Diffusion Inverse Problems with Decoupled Posterior Annealing](https://arxiv.org/abs/2511.17038)
*Hao Chen,Renzheng Zhang,Scott S. Howard*

Main category: cs.AI

TL;DR: 论文重新解释扩散模型在逆问题求解中的作用，提出DAPS++方法，将扩散阶段与数据驱动优化解耦，提高计算效率和重建性能。


<details>
  <summary>Details</summary>
Motivation: 传统贝叶斯视角下的基于分数的扩散方法在解决逆问题时，先验提供的指导有限，重建主要由测量一致性项驱动，与扩散动力学基本解耦。需要澄清这种结构并改进方法。

Method: 将扩散重新解释为期望最大化框架中的初始化阶段，提出DAPS++方法，使似然项更直接地指导推理，同时保持数值稳定性。

Result: DAPS++需要更少的函数评估和测量优化步骤，在多种图像恢复任务中实现了高计算效率和鲁棒的重建性能。

Conclusion: 扩散在逆问题求解中的主要作用是提供初始化，DAPS++通过解耦扩散和数据驱动优化，有效提高了推理效率和性能。

Abstract: From a Bayesian perspective, score-based diffusion solves inverse problems through joint inference, embedding the likelihood with the prior to guide the sampling process. However, this formulation fails to explain its practical behavior: the prior offers limited guidance, while reconstruction is largely driven by the measurement-consistency term, leading to an inference process that is effectively decoupled from the diffusion dynamics. To clarify this structure, we reinterpret the role of diffusion in inverse problem solving as an initialization stage within an expectation--maximization (EM)--style framework, where the diffusion stage and the data-driven refinement are fully decoupled. We introduce \textbf{DAPS++}, which allows the likelihood term to guide inference more directly while maintaining numerical stability and providing insight into why unified diffusion trajectories remain effective in practice. By requiring fewer function evaluations (NFEs) and measurement-optimization steps, \textbf{DAPS++} achieves high computational efficiency and robust reconstruction performance across diverse image restoration tasks.

</details>


### [7] [MIR: Efficient Exploration in Episodic Multi-Agent Reinforcement Learning via Mutual Intrinsic Reward](https://arxiv.org/abs/2511.17165)
*Kesheng Chen,Wenjian Luo,Bang Zhang,Zeping Yin,Zipeng Ye*

Main category: cs.AI

TL;DR: 本文提出了一种名为MIR的互内在奖励方法，用于解决多智能体强化学习中的稀疏奖励问题，特别是在情节奖励场景下。该方法通过激励智能体探索影响队友的行为，有效促进团队探索并提升算法性能。


<details>
  <summary>Details</summary>
Motivation: 多智能体强化学习中，情节奖励带来了显著挑战。现有内在奖励方法在单智能体场景中有效，但在多智能体场景中存在两个主要问题：联合动作轨迹的指数级稀疏性，以及现有方法未能充分考虑影响团队状态的联合动作。

Method: 提出了互内在奖励（MIR）方法，这是一种简单而有效的增强策略，用于处理具有极端稀疏奖励的多智能体强化学习。MIR激励个体智能体探索能够影响队友的行为，并与原始策略结合使用。

Result: 通过扩展代表性单智能体MiniGrid环境创建了MiniGrid-MA系列多智能体环境进行评估。实验结果显示，所提方法在MiniGrid-MA设置中相比最先进方法表现出更优的性能。

Conclusion: MIR方法能够有效解决多智能体强化学习中的稀疏奖励问题，通过促进团队探索显著提升算法性能，在稀疏奖励场景下具有优越表现。

Abstract: Episodic rewards present a significant challenge in reinforcement learning. While intrinsic reward methods have demonstrated effectiveness in single-agent rein-forcement learning scenarios, their application to multi-agent reinforcement learn-ing (MARL) remains problematic. The primary difficulties stem from two fac-tors: (1) the exponential sparsity of joint action trajectories that lead to rewards as the exploration space expands, and (2) existing methods often fail to account for joint actions that can influence team states. To address these challenges, this paper introduces Mutual Intrinsic Reward (MIR), a simple yet effective enhancement strategy for MARL with extremely sparse rewards like episodic rewards. MIR incentivizes individual agents to explore actions that affect their teammates, and when combined with original strategies, effectively stimulates team exploration and improves algorithm performance. For comprehensive experimental valida-tion, we extend the representative single-agent MiniGrid environment to create MiniGrid-MA, a series of MARL environments with sparse rewards. Our evalu-ation compares the proposed method against state-of-the-art approaches in the MiniGrid-MA setting, with experimental results demonstrating superior perfor-mance.

</details>


### [8] [Designing Domain-Specific Agents via Hierarchical Task Abstraction Mechanism](https://arxiv.org/abs/2511.17198)
*Kaiyu Li,Jiayu Wang,Zhi Wang,Hui Qiao,Weizhan Zhang,Deyu Meng,Xiangyong Cao*

Main category: cs.AI

TL;DR: 论文提出了HTAM框架，通过分层任务抽象机制构建多智能体系统，解决了通用智能体在专业领域（如遥感）中处理结构化工作流的挑战，并开发了EarthAgent系统在GeoPlan-bench基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 通用智能体框架（如ReAct）在需要严格结构化工作流的专业领域表现不佳，特别是在遥感等需要专业工具和多步骤程序的领域存在显著挑战。

Method: 引入分层任务抽象机制（HTAM），将多智能体系统构建为逻辑层次结构，反映领域内在任务依赖图，通过任务中心架构确保程序正确性并将复杂问题分解为顺序层。

Result: 基于HTAM框架开发的EarthAgent在GeoPlan-bench基准测试中显著优于多种现有的单智能体和多智能体系统。

Conclusion: 将智能体架构与领域内在任务结构对齐是构建稳健可靠的专业自主系统的关键步骤。

Abstract: LLM-driven agents, particularly those using general frameworks like ReAct or human-inspired role-playing, often struggle in specialized domains that necessitate rigorously structured workflows. Fields such as remote sensing, requiring specialized tools (e.g., correction, spectral indices calculation), and multi-step procedures (e.g., numerous intermediate products and optional steps), significantly challenge generalized approaches. To address this gap, we introduce a novel agent design framework centered on a Hierarchical Task Abstraction Mechanism (HTAM). Specifically, HTAM moves beyond emulating social roles, instead structuring multi-agent systems into a logical hierarchy that mirrors the intrinsic task-dependency graph of a given domain. This task-centric architecture thus enforces procedural correctness and decomposes complex problems into sequential layers, where each layer's sub-agents operate on the outputs of the preceding layers. We instantiate this framework as EarthAgent, a multi-agent system tailored for complex geospatial analysis. To evaluate such complex planning capabilities, we build GeoPlan-bench, a comprehensive benchmark of realistic, multi-step geospatial planning tasks. It is accompanied by a suite of carefully designed metrics to evaluate tool selection, path similarity, and logical completeness. Experiments show that EarthAgent substantially outperforms a range of established single- and multi-agent systems. Our work demonstrates that aligning agent architecture with a domain's intrinsic task structure is a critical step toward building robust and reliable specialized autonomous systems.

</details>


### [9] [Agentifying Agentic AI](https://arxiv.org/abs/2511.17332)
*Virginia Dignum,Frank Dignum*

Main category: cs.AI

TL;DR: 本文主张将AAMAS社区开发的BDI架构、通信协议、机制设计和制度建模等概念工具作为实现智能AI系统的基础，通过将自适应数据驱动方法与结构化推理协调模型相结合，构建具有透明度、协作性和问责制的能力系统。


<details>
  <summary>Details</summary>
Motivation: 为了实现智能AI系统的持续自主性、推理和交互能力，需要补充明确的认知、协作和治理模型。

Method: 利用AAMAS社区开发的BDI架构、通信协议、机制设计和制度建模等概念工具，将自适应数据驱动方法与结构化推理协调模型相结合。

Result: 提出了一个连接形式理论和实践自主性的智能视角，为实现具有透明度、协作性和问责制的智能系统指明了路径。

Conclusion: AAMAS社区的概念工具为构建不仅能力强、灵活，而且透明、协作和可问责的智能系统提供了坚实基础。

Abstract: Agentic AI seeks to endow systems with sustained autonomy, reasoning, and interaction capabilities. To realize this vision, its assumptions about agency must be complemented by explicit models of cognition, cooperation, and governance. This paper argues that the conceptual tools developed within the Autonomous Agents and Multi-Agent Systems (AAMAS) community, such as BDI architectures, communication protocols, mechanism design, and institutional modelling, provide precisely such a foundation. By aligning adaptive, data-driven approaches with structured models of reasoning and coordination, we outline a path toward agentic systems that are not only capable and flexible, but also transparent, cooperative, and accountable. The result is a perspective on agency that bridges formal theory and practical autonomy.

</details>


### [10] [That's not natural: The Impact of Off-Policy Training Data on Probe Performance](https://arxiv.org/abs/2511.17408)
*Nathalie Kirch,Samuel Dower,Adrians Skapars,Ekdeep Singh Lubana,Dmitrii Krasheninnikov*

Main category: cs.AI

TL;DR: 本文系统评估了使用合成数据和离策略数据对LLM行为探测泛化能力的影响，发现响应生成策略显著影响探测性能，且训练数据域偏移会导致更大性能下降。


<details>
  <summary>Details</summary>
Motivation: 由于许多LLM行为的自然示例稀少，研究人员不得不依赖合成或离策略的LLM响应来训练探测模型，但这种方法对探测泛化能力的影响尚不明确。

Method: 在八种不同LLM行为上测试线性和注意力探测模型，使用多种LLM，比较不同响应生成策略对探测性能的影响。

Result: 响应生成策略显著影响探测性能，但影响程度因行为而异；从离策略数据成功泛化到测试集可以预测在策略泛化成功；训练数据域偏移导致更大性能下降。

Conclusion: 在缺乏在策略数据时，使用同域离策略数据比使用不同域在策略数据产生更可靠的探测，强调需要更好处理LLM监控中分布偏移的方法。

Abstract: Probing has emerged as a promising method for monitoring Large Language Models (LLMs), enabling inference-time detection of concerning behaviours such as deception and sycophancy. However, natural examples of many behaviours are rare, forcing researchers to rely on synthetic or off-policy LLM responses for training probes. We systematically evaluate how the use of synthetic and off-policy data influences probe generalisation across eight distinct LLM behaviours. Testing linear and attention probes across multiple LLMs, we find that the response generation strategy can significantly affect probe performance, though the magnitude of this effect varies by behaviour. We find that successful generalisation from off-policy data, to test sets where the model is incentivised to produce the target behaviour, is predictive of successful on-policy generalisation. Leveraging this result, we predict that Deception and Sandbagging probes may fail to generalise from off-policy to on-policy data when used in real monitoring scenarios. Notably, shifts in the training data domain still cause even larger performance degradation, with different-domain test scores being consistently lower than the same-domain ones. These results indicate that, in the absence of on-policy data, using same-domain off-policy data yields more reliable probes than using on-policy data from a different domain, emphasizing the need for methods that can better handle distribution shifts in LLM monitoring.

</details>


### [11] [SRA-CP: Spontaneous Risk-Aware Selective Cooperative Perception](https://arxiv.org/abs/2511.17461)
*Jiaxi Liu,Chengyuan Ma,Hang Zhou,Weizhe Tang,Shixiao Liang,Haoyang Ding,Xiaopeng Li,Bin Ran*

Main category: cs.AI

TL;DR: 提出SRA-CP框架，通过风险感知的选择性协作感知，在保持安全关键目标检测精度的同时，大幅降低通信带宽需求。


<details>
  <summary>Details</summary>
Motivation: 现有通用协作感知方法需要传输大量与驾驶安全无关的感知数据，超出可用通信带宽，且依赖预定义的通信伙伴，不适合动态交通环境。

Method: 采用去中心化协议，车辆持续广播轻量级感知覆盖摘要，仅在检测到风险相关盲区时启动针对性合作。包含感知风险识别模块和选择性信息交换融合模块。

Result: 相比通用CP方法，安全关键对象的平均精度损失小于1%，同时仅使用20%的通信带宽。相比现有选择性CP方法，感知性能提升15%。

Conclusion: SRA-CP框架通过风险感知的选择性协作，有效解决了通信带宽限制和动态环境适应性挑战，在保持安全性的同时显著提升了通信效率。

Abstract: Cooperative perception (CP) offers significant potential to overcome the limitations of single-vehicle sensing by enabling information sharing among connected vehicles (CVs). However, existing generic CP approaches need to transmit large volumes of perception data that are irrelevant to the driving safety, exceeding available communication bandwidth. Moreover, most CP frameworks rely on pre-defined communication partners, making them unsuitable for dynamic traffic environments. This paper proposes a Spontaneous Risk-Aware Selective Cooperative Perception (SRA-CP) framework to address these challenges. SRA-CP introduces a decentralized protocol where connected agents continuously broadcast lightweight perception coverage summaries and initiate targeted cooperation only when risk-relevant blind zones are detected. A perceptual risk identification module enables each CV to locally assess the impact of occlusions on its driving task and determine whether cooperation is necessary. When CP is triggered, the ego vehicle selects appropriate peers based on shared perception coverage and engages in selective information exchange through a fusion module that prioritizes safety-critical content and adapts to bandwidth constraints. We evaluate SRA-CP on a public dataset against several representative baselines. Results show that SRA-CP achieves less than 1% average precision (AP) loss for safety-critical objects compared to generic CP, while using only 20% of the communication bandwidth. Moreover, it improves the perception performance by 15% over existing selective CP methods that do not incorporate risk awareness.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [12] [Password Strength Analysis Through Social Network Data Exposure: A Combined Approach Relying on Data Reconstruction and Generative Models](https://arxiv.org/abs/2511.16716)
*Maurizio Atzori,Eleonora Calò,Loredana Caruccio,Stefano Cirillo,Giuseppe Polese,Giandomenico Solimando*

Main category: cs.CR

TL;DR: SODA ADVANCE是一个数据重建工具，旨在通过整合社交媒体等公开数据来增强密码强度评估。研究还探讨了大型语言模型在密码生成和评估方面的能力与风险。


<details>
  <summary>Details</summary>
Motivation: 用户倾向于使用易记密码，这增加了安全风险，而传统的密码强度评估方法往往不足。

Method: 开发SODA ADVANCE工具，集成专门模块利用社交媒体等公开数据评估密码强度，并研究LLMs在密码生成和评估中的应用。

Result: 对100名真实用户的实验评估表明，LLMs能够生成强且个性化的密码，并能有效评估密码，特别是在考虑用户档案数据时。

Conclusion: LLMs在密码生成和评估方面具有潜力，特别是当能够利用用户档案数据时，可以生成强且个性化的密码。

Abstract: Although passwords remain the primary defense against unauthorized access, users often tend to use passwords that are easy to remember. This behavior significantly increases security risks, also due to the fact that traditional password strength evaluation methods are often inadequate. In this discussion paper, we present SODA ADVANCE, a data reconstruction tool also designed to enhance evaluation processes related to the password strength. In particular, SODA ADVANCE integrates a specialized module aimed at evaluating password strength by leveraging publicly available data from multiple sources, including social media platforms. Moreover, we investigate the capabilities and risks associated with emerging Large Language Models (LLMs) in evaluating and generating passwords, respectively. Experimental assessments conducted with 100 real users demonstrate that LLMs can generate strong and personalized passwords possibly defined according to user profiles. Additionally, LLMs were shown to be effective in evaluating passwords, especially when they can take into account user profile data.

</details>


### [13] [Membership Inference Attacks Beyond Overfitting](https://arxiv.org/abs/2511.16792)
*Mona Khalil,Alberto Blanco-Justicia,Najeeb Jebreel,Josep Domingo-Ferrer*

Main category: cs.CR

TL;DR: 本文研究了机器学习模型中成员推理攻击的根本原因，发现即使非过拟合模型也会泄露部分训练数据隐私，这些易受攻击的样本通常是类内异常值，论文提出了针对性防御策略。


<details>
  <summary>Details</summary>
Motivation: 成员推理攻击对使用敏感数据进行训练的机器学习模型构成严重隐私风险，现有防御方法如差分隐私往往导致高精度损失。虽然模型过拟合被认为是主要因素，但研究表明即使非过拟合模型也会泄露部分训练数据信息，这促使研究超越传统过拟合问题的根本原因。

Method: 通过实证分析非过拟合模型中易受成员推理攻击的训练数据样本特征，发现这些样本通常是类内异常值（如噪声样本或难以分类的样本），并基于此提出针对性防御策略。

Result: 研究发现易受成员推理攻击的样本具有类内异常值特征，这些样本在模型预测行为上表现出明显差异，即使模型能够泛化。提出的防御策略能够有效保护这些易受攻击样本，增强模型的隐私保护能力。

Conclusion: 成员推理攻击的脆弱性不仅源于模型过拟合，还与训练数据中特定类型的样本（类内异常值）有关。通过针对性保护这些易受攻击样本，可以在不显著牺牲模型性能的情况下增强隐私保护。

Abstract: Membership inference attacks (MIAs) against machine learning (ML) models aim to determine whether a given data point was part of the model training data. These attacks may pose significant privacy risks to individuals whose sensitive data were used for training, which motivates the use of defenses such as differential privacy, often at the cost of high accuracy losses. MIAs exploit the differences in the behavior of a model when making predictions on samples it has seen during training (members) versus those it has not seen (non-members). Several studies have pointed out that model overfitting is the major factor contributing to these differences in behavior and, consequently, to the success of MIAs. However, the literature also shows that even non-overfitted ML models can leak information about a small subset of their training data. In this paper, we investigate the root causes of membership inference vulnerabilities beyond traditional overfitting concerns and suggest targeted defenses. We empirically analyze the characteristics of the training data samples vulnerable to MIAs in models that are not overfitted (and hence able to generalize). Our findings reveal that these samples are often outliers within their classes (e.g., noisy or hard to classify). We then propose potential defensive strategies to protect these vulnerable samples and enhance the privacy-preserving capabilities of ML models. Our code is available at https://github.com/najeebjebreel/mia_analysis.

</details>


### [14] [TICAL: Trusted and Integrity-protected Compilation of AppLications](https://arxiv.org/abs/2511.17070)
*Robert Krahn,Nikson Kanti Paul,Franz Gregor,Do Le Quoc,Andrey Brito,André Martin,Christof Fetzer*

Main category: cs.CR

TL;DR: Tical是一个实用的可信编译框架，为从源代码到最终可执行文件的构建流水线提供完整性保护和机密性。它利用TEEs作为运行时保护，并通过文件系统屏蔽和不可变审计日志来确保编译链只能访问受信任的文件。


<details>
  <summary>Details</summary>
Motivation: 现有硬件扩展如Intel SGX、TDX、AMD SEV等主要关注运行时保护，但构建时的完整性和机密性保护同样重要，因为编译过程中恶意注入的代码可能危及整个应用和系统。

Method: 利用TEEs作为运行时保护基础，增强文件系统屏蔽和不可变审计日志功能，确保编译链只能访问受信任的文件和中间输出。

Result: 通过微基准和宏基准评估显示，Tical能够以可接受的性能开销保护整个CI/CD流水线的机密性和完整性。

Conclusion: Tical框架成功解决了构建流水线中的信任问题，为从源代码到可执行文件的完整编译过程提供了有效的完整性和机密性保护。

Abstract: During the past few years, we have witnessed various efforts to provide confidentiality and integrity for applications running in untrusted environments such as public clouds. In most of these approaches, hardware extensions such as Intel SGX, TDX, AMD SEV, etc., are leveraged to provide encryption and integrity protection on process or VM level. Although all of these approaches increase the trust in the application at runtime, an often overlooked aspect is the integrity and confidentiality protection at build time, which is equally important as maliciously injected code during compilation can compromise the entire application and system. In this paper, we present Tical, a practical framework for trusted compilation that provides integrity protection and confidentiality in build pipelines from source code to the final executable. Our approach harnesses TEEs as runtime protection but enriches TEEs with file system shielding and an immutable audit log with version history to provide accountability. This way, we can ensure that the compiler chain can only access trusted files and intermediate output, such as object files produced by trusted processes. Our evaluation using micro- and macro-benchmarks shows that Tical can protect the confidentiality and integrity of whole CI/CD pipelines with an acceptable performance overhead.

</details>


### [15] [Constant-Size Cryptographic Evidence Structures for Regulated AI Workflows](https://arxiv.org/abs/2511.17118)
*Leo Kao*

Main category: cs.CR

TL;DR: 本文提出恒定大小的密码学证据结构，用于在受监管环境中表示AI工作流程的可验证审计证据。每个证据项是固定大小的密码学字段元组，支持与工作流程事件和配置的强绑定、恒定大小存储和统一验证成本，并能与哈希链和Merkle树审计结构良好组合。


<details>
  <summary>Details</summary>
Motivation: 为受监管的AI工作流程提供可验证的审计证据，确保工作流程事件的完整性和不可否认性，同时保持存储和验证效率。

Method: 形式化受监管AI工作流程模型，定义证据结构的语法和算法，提出通用的哈希-签名构造方法，使用抗碰撞哈希函数和标准数字签名方案实现该抽象。

Result: 实现了原型库并在商用硬件上进行微基准测试，证明恒定大小证据的每事件开销小且可预测，支持与哈希链日志、Merkle树锚定和可信执行环境的集成。

Conclusion: 恒定大小密码学证据结构为AI工作流程审计提供了高效、安全的解决方案，对临床试验管理、制药合规和医疗AI治理具有重要意义。

Abstract: This paper introduces constant-size cryptographic evidence structures, a general abstraction for representing verifiable audit evidence for AI workflows in regulated environments. Each evidence item is a fixed-size tuple of cryptographic fields, designed to (i) provide strong binding to workflow events and configurations, (ii) support constant-size storage and uniform verification cost per event, and (iii) compose cleanly with hash-chain and Merkle-based audit constructions. We formalize a simple model of regulated AI workflows, define syntax and algorithms for evidence structures, and articulate security goals such as audit integrity and non-equivocation. We present a generic hash-and-sign construction that instantiates this abstraction using a collision-resistant hash function and a standard digital signature scheme. We then show how to integrate the construction with hash-chained logs, Merkle-tree anchoring, and optionally trusted execution environments, and we analyze the asymptotic complexity of evidence generation and verification. Finally, we implement a prototype library and report microbenchmark results on commodity hardware, demonstrating that the per-event overhead of constant-size evidence is small and predictable. The design is informed by industrial experience with regulated AI systems at Codebat Technologies Inc., while the paper focuses on the abstraction, algorithms, and their security and performance characteristics, with implications for clinical trial management, pharmaceutical compliance, and medical AI governance.

</details>


### [16] [Steering in the Shadows: Causal Amplification for Activation Space Attacks in Large Language Models](https://arxiv.org/abs/2511.17194)
*Zhiyuan Xu,Stanislav Abaimov,Joseph Gardiner,Sana Belguith*

Main category: cs.CR

TL;DR: 该论文发现解码器LLM的中间激活层存在安全漏洞，通过因果放大效应和敏感度缩放引导攻击，可在保持模型能力的同时显著改变模型行为。


<details>
  <summary>Details</summary>
Motivation: 现代大语言模型通常通过审计数据、提示和拒绝策略来保证安全，但忽视了前向传播过程中的中间激活层可能成为攻击面。

Method: 提出敏感度缩放引导(SSS)攻击方法，结合序列开始锚定和基于敏感度的强化，将有限扰动预算集中在最脆弱的层和标记上。

Result: 在多个开源模型和四个行为维度上，SSS能够显著改变模型的恶意行为、幻觉、谄媚和情感倾向，同时保持高连贯性和通用能力。

Conclusion: 激活引导已成为白盒和供应链LLM部署中的具体安全威胁，需要重新评估模型安全假设。

Abstract: Modern large language models (LLMs) are typically secured by auditing data, prompts, and refusal policies, while treating the forward pass as an implementation detail. We show that intermediate activations in decoder-only LLMs form a vulnerable attack surface for behavioral control. Building on recent findings on attention sinks and compression valleys, we identify a high-gain region in the residual stream where small, well-aligned perturbations are causally amplified along the autoregressive trajectory--a Causal Amplification Effect (CAE). We exploit this as an attack surface via Sensitivity-Scaled Steering (SSS), a progressive activation-level attack that combines beginning-of-sequence (BOS) anchoring with sensitivity-based reinforcement to focus a limited perturbation budget on the most vulnerable layers and tokens. We show that across multiple open-weight models and four behavioral axes, SSS induces large shifts in evil, hallucination, sycophancy, and sentiment while preserving high coherence and general capabilities, turning activation steering into a concrete security concern for white-box and supply-chain LLM deployments.

</details>


### [17] [Persistent BitTorrent Trackers](https://arxiv.org/abs/2511.17260)
*Francois Xavier Wicht,Zhengwei Tong,Shunfan Zhou,Hang Yin,Aviv Yaish*

Main category: cs.CR

TL;DR: 该论文提出了一种基于区块链和可信执行环境的去中心化BitTorrent信誉系统，解决传统私有追踪器存在的信誉不可移植、中心化单点故障和上传统计不可验证等问题。


<details>
  <summary>Details</summary>
Motivation: 传统私有BitTorrent追踪器存在三个关键弱点：信誉无法在追踪器间迁移、中心化服务器造成单点故障、上传统计数据是自报告且不可验证。当追踪器关闭时，用户会丢失贡献历史，无法向新社区证明自己的信誉。

Method: 使用智能合约存储信誉，用加密证明替代自报告。接收方对传输的数据块签名确认，追踪器在可信执行环境中聚合验证这些收据并更新链上信誉。当追踪器不可用时，使用认证的分布式哈希表进行发现，链上信誉充当公钥基础设施。

Result: 原型系统在Intel TDX上评估显示，传输收据在典型数据块大小下增加不到6%的开销，签名聚合使验证速度提升2.5倍。

Conclusion: 该设计能够在追踪器故障时持久保存信誉，并通过工厂部署合约中的单跳迁移实现信誉的可移植性，在标准密码学假设下证明了正确性。

Abstract: Private BitTorrent trackers enforce upload-to-download ratios to prevent free-riding, but suffer from three critical weaknesses: reputation cannot move between trackers, centralized servers create single points of failure, and upload statistics are self-reported and unverifiable. When a tracker shuts down (whether by operator choice, technical failure, or legal action) users lose their contribution history and cannot prove their standing to new communities. We address these problems by storing reputation in smart contracts and replacing self-reports with cryptographic attestations. Receiving peers sign receipts for transferred pieces, which the tracker aggregates and verifies before updating on-chain reputation. Trackers run in Trusted Execution Environments (TEEs) to guarantee correct aggregation and prevent manipulation of state. If a tracker is unavailable, peers use an authenticated Distributed Hash Table (DHT) for discovery: the on-chain reputation acts as a Public Key Infrastructure (PKI), so peers can verify each other and maintain access control without the tracker. This design persists reputation across tracker failures and makes it portable to new instances through single-hop migration in factory-deployed contracts. We formalize the security requirements, prove correctness under standard cryptographic assumptions, and evaluate a prototype on Intel TDX. Measurements show that transfer receipts adds less than 6\% overhead with typical piece sizes, and signature aggregation speeds up verification by $2.5\times$.

</details>


### [18] [A Patient-Centric Blockchain Framework for Secure Electronic Health Record Management: Decoupling Data Storage from Access Control](https://arxiv.org/abs/2511.17464)
*Tanzim Hossain Romel,Kawshik Kumar Paul,Tanberul Islam Ruhan,Maisha Rahman Mim,Abu Sayed Md. Latiful Hoque*

Main category: cs.CR

TL;DR: 提出了一种基于区块链的患者中心化电子健康记录共享架构，将内容存储与授权审计分离，使用EIP-712实现时间限制权限管理，在保护隐私的同时确保数据安全。


<details>
  <summary>Details</summary>
Motivation: 传统电子健康记录系统缺乏患者控制权，需要一种既能保护患者隐私又能确保临床数据安全共享的解决方案。

Method: 采用链下存储加密FHIR资源，链上记录密码学承诺和患者签名的时间限制权限，通过公钥包装分发密钥，实现诚实但好奇的存储提供者模式。

Result: 权限授予平均消耗78,000 gas（L1），1MB记录端到端访问延迟为0.7-1.4秒，Layer-2部署可将gas使用减少10-13倍。

Conclusion: 该架构为恢复患者控制权同时保持临床敏感数据所需安全属性提供了一条实用路径，满足HIPAA/GDPR等监管要求。

Abstract: We present a patient-centric architecture for electronic health record (EHR) sharing that separates content storage from authorization and audit. Encrypted FHIR resources are stored off-chain; a public blockchain records only cryptographic commitments and patient-signed, time-bounded permissions using EIP-712. Keys are distributed via public-key wrapping, enabling storage providers to remain honest-but-curious without risking confidentiality. We formalize security goals (confidentiality, integrity, cryptographically attributable authorization, and auditability of authorization events) and provide a Solidity reference implementation deployed as single-patient contracts. On-chain costs for permission grants average 78,000 gas (L1), and end-to-end access latency for 1 MB records is 0.7--1.4s (mean values for S3 and IPFS respectively), dominated by storage retrieval. Layer-2 deployment reduces gas usage by 10--13x, though data availability charges dominate actual costs. We discuss metadata privacy, key registry requirements, and regulatory considerations (HIPAA/GDPR), demonstrating a practical route to restoring patient control while preserving security properties required for sensitive clinical data.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [19] [Multi-Agent Code Verification with Compound Vulnerability Detection](https://arxiv.org/abs/2511.16708)
*Shreshth Rajan*

Main category: cs.SE

TL;DR: CodeX-Verify是一个多智能体系统，使用四个专门化智能体检测不同类型代码漏洞，在76.1%的准确率下比现有工具更快且无需测试执行，多智能体组合比单智能体提升39.7个百分点准确率。


<details>
  <summary>Details</summary>
Motivation: 现有LLM生成的代码存在严重漏洞问题：29.6%的SWE-bench补丁失败，62%的BaxBench解决方案有漏洞，现有工具只能捕获65%的漏洞且有35%误报率。

Method: 构建多智能体系统，使用四个专门化智能体检测不同类型漏洞，通过数学证明不同检测模式的智能体组合能发现更多漏洞，测试了15种不同智能体组合。

Result: 在99个带验证标签的代码样本上，系统捕获76.1%的漏洞，匹配最佳现有方法但运行更快且无需测试执行；多智能体比单智能体提升39.7个百分点准确率，最佳双智能体组合达到79.3%准确率；在300个真实补丁上测试，每个样本运行时间低于200ms。

Conclusion: 多智能体方法能显著提升代码漏洞检测效果，多个漏洞在同一代码中会产生指数级风险增加（SQL注入加凭证泄露风险达300 vs 传统模型预测的20），系统具有生产实用性。

Abstract: LLMs generate buggy code: 29.6% of SWE-bench "solved" patches fail, 62% of BaxBench solutions have vulnerabilities, and existing tools only catch 65% of bugs with 35% false positives. We built CodeX-Verify, a multi-agent system that uses four specialized agents to detect different types of bugs. We prove mathematically that combining agents with different detection patterns finds more bugs than any single agent when the agents look for different problems, confirmed by measuring agent correlation of p = 0.05--0.25. We also show that multiple vulnerabilities in the same code create exponentially more risk than previously thought--SQL injection plus exposed credentials creates 15x more danger (risk 300 vs. 20) than traditional models predict. Testing on 99 code samples with verified labels shows our system catches 76.1% of bugs, matching the best existing method while running faster and without test execution. We tested 15 different agent combinations and found that using multiple agents improves accuracy by 39.7 percentage points (from 32.8% to 72.4%) compared to single agents, with gains of +14.9pp, +13.5pp, and +11.2pp for agents 2, 3, and 4. The best two-agent combination reaches 79.3% accuracy. Testing on 300 real patches from Claude Sonnet 4.5 runs in under 200ms per sample, making this practical for production use.

</details>


### [20] [Is the Cure Still Worse Than the Disease? Test Overfitting by LLMs in Automated Program Repair](https://arxiv.org/abs/2511.16858)
*Toufique Ahmed,Jatin Ganhotra,Avraham Shinnar,Martin Hirzel*

Main category: cs.SE

TL;DR: 研究测试过拟合在大型语言模型时代是否仍然是程序自动修复中的一个问题


<details>
  <summary>Details</summary>
Motivation: 程序自动修复存在测试过拟合问题，即修复后的代码在可见测试上通过但在隐藏测试集上失败。本研究旨在验证在大型语言模型时代这个问题是否仍然存在

Method: 使用仓库级别的SWE-bench任务进行实验研究

Result: 研究结果显示测试过拟合问题仍然存在

Conclusion: 测试过拟合在当前的程序自动修复中仍然是一个需要关注的问题

Abstract: Automated program repair has been shown to be susceptible to generating repaired code that passes on seen tests but fails on a hold-out set of hidden tests. This problem, dubbed test overfitting, has been identified and studied before the rise of large language models. We experimentally study how much test overfitting is still a problem today, using repository-level SWE-bench tasks.

</details>


### [21] [MOOT: a Repository of Many Multi-Objective Optimization Tasks](https://arxiv.org/abs/2511.16882)
*Tim Menzies,Tao Chen,Yulong Ye,Kishan Kumar Ganguly,Amirali Rayegan,Srinath Srinivasan,Andre Lustosa*

Main category: cs.SE

TL;DR: MOOT是一个多目标优化任务库，收集了120多个来自软件工程研究的多目标优化任务，旨在帮助研究者和从业者更好地探索软件工程中的权衡决策问题。


<details>
  <summary>Details</summary>
Motivation: 软件工程师需要在竞争性目标之间做出权衡决策（如速度vs成本、安全vs易用性、准确vs可解释等），但现有研究工具不足，导致研究人员难以有效探索这些权衡，工业实践者也因此交付次优产品。

Method: 构建MOOT仓库，收集来自近期软件工程研究论文的多目标优化任务，涵盖软件配置、云调优、项目健康、过程建模、超参数优化等多个领域。

Result: MOOT目前包含120多个任务，在MIT许可下免费提供，位于github.com/timm/moot，支持社区贡献，能够支持数十个新的研究问题。

Conclusion: MOOT为软件工程多目标优化研究提供了重要的数据资源，有助于推动该领域的研究发展，解决软件工程中的权衡决策挑战。

Abstract: Software engineers must make decisions that trade off competing goals (faster vs. cheaper, secure vs. usable, accurate vs. interpretable, etc.). Despite MSR's proven techniques for exploring such goals, researchers still struggle with these trade-offs. Similarly, industrial practitioners deliver sub-optimal products since they lack the tools needed to explore these trade-offs.
  To enable more research in this important area, we introduce MOOT, a repository of multi-objective optimization tasks taken from recent SE research papers. MOOT's tasks cover software configuration, cloud tuning, project health, process modeling, hyperparameter optimization, and more. Located at github.com/timm/moot, MOOT's current 120+ tasks are freely available under an MIT license (and we invite community contributions). As shown here, this data enables dozens of novel research questions.

</details>


### [22] [ReVul-CoT: Towards Effective Software Vulnerability Assessment with Retrieval-Augmented Generation and Chain-of-Thought Prompting](https://arxiv.org/abs/2511.17027)
*Zhijie Chen,Xiang Chen,Ziming Li,Jiacheng Xue,Chaoyang Gao*

Main category: cs.SE

TL;DR: ReVul-CoT是一个结合检索增强生成(RAG)和思维链(COT)提示的框架，用于提升大语言模型在软件漏洞评估中的性能，在12,070个漏洞数据集上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型在软件漏洞评估中的两个主要限制：缺乏领域特定知识，以及依赖浅层模式匹配而非深度上下文推理。

Method: 通过RAG模块从本地知识库动态检索漏洞相关信息，结合COT提示引导模型进行分步推理，评估可利用性、影响范围等因素。

Result: 在12,070个漏洞数据集上，ReVul-CoT在MCC指标上比现有最佳方法提升16.50%-42.26%，在准确率、F1分数和MCC上分别提升10.43%、15.86%和16.50%。

Conclusion: RAG与COT提示的结合显著增强了基于大语言模型的软件漏洞评估，为未来研究指明了有前景的方向。

Abstract: Context: Software Vulnerability Assessment (SVA) plays a vital role in evaluating and ranking vulnerabilities in software systems to ensure their security and reliability. Objective: Although Large Language Models (LLMs) have recently shown remarkable potential in SVA, they still face two major limitations. First, most LLMs are trained on general-purpose corpora and thus lack domain-specific knowledge essential for effective SVA. Second, they tend to rely on shallow pattern matching instead of deep contextual reasoning, making it challenging to fully comprehend complex code semantics and their security implications. Method: To alleviate these limitations, we propose a novel framework ReVul-CoT that integrates Retrieval-Augmented Generation (RAG) with Chain-of-Thought (COT) prompting. In ReVul-CoT, the RAG module dynamically retrieves contextually relevant information from a constructed local knowledge base that consolidates vulnerability data from authoritative sources (such as NVD and CWE), along with corresponding code snippets and descriptive information. Building on DeepSeek-V3.1, CoT prompting guides the LLM to perform step-by-step reasoning over exploitability, impact scope, and related factors Results: We evaluate ReVul-CoT on a dataset of 12,070 vulnerabilities. Experimental results show that ReVul-CoT outperforms state-of-the-art SVA baselines by 16.50%-42.26% in terms of MCC, and outperforms the best baseline by 10.43%, 15.86%, and 16.50% in Accuracy, F1-score, and MCC, respectively. Our ablation studies further validate the contributions of considering dynamic retrieval, knowledge integration, and CoT-based reasoning. Conclusion: Our results demonstrate that combining RAG with CoT prompting significantly enhances LLM-based SVA and points out promising directions for future research.

</details>


### [23] [SlsReuse: LLM-Powered Serverless Function Reuse](https://arxiv.org/abs/2511.17262)
*Jinfeng Wen,Yuehan Sun*

Main category: cs.SE

TL;DR: SlsReuse是首个基于大语言模型的服务器less函数重用框架，通过构建函数知识库、学习统一语义表示和意图感知发现，显著提升了函数推荐的准确率。


<details>
  <summary>Details</summary>
Motivation: 服务器less计算虽然降低了运维负担，但对新手开发者来说，从头开发函数需要适应平台特定的编程风格，过程耗时且易错。函数重用是解决这些挑战的有前景方案，但现有技术由于任务描述与异构函数实现之间的语义鸿沟而不足。

Method: SlsReuse首先构建可重用函数知识库，然后通过有效的提示工程和少样本提示学习异构函数的统一语义增强表示，捕获隐式代码意图、目标平台、编程语言和云服务。最后，给定自然语言任务查询，执行意图感知发现结合多级剪枝策略和相似性匹配。

Result: 在包含110个任务查询的数据集上评估，基于ChatGPT-4o的SlsReuse实现了Recall@10为91.20%，比最先进的基线方法提高了24.53个百分点。

Conclusion: SlsReuse通过LLM技术有效弥合了任务描述与函数实现之间的语义鸿沟，为服务器less函数重用提供了有效的解决方案，显著提升了推荐性能。

Abstract: Serverless computing has rapidly emerged as a popular cloud computing paradigm. It enables developers to implement function-level tasks, i.e., serverless functions, without managing infrastructure. While reducing operational overhead, it poses challenges, especially for novice developers. Developing functions from scratch requires adapting to heterogeneous, platform-specific programming styles, making the process time-consuming and error-prone. Function reuse offers a promising solution to address these challenges. However, research on serverless computing lacks a dedicated approach for function recommendation. Existing techniques from traditional contexts remain insufficient due to the semantic gap between task descriptions and heterogeneous function implementations. Advances in large language models (LLMs), pre-trained on large-scale corpora, create opportunities to bridge this gap by aligning developer requirements with function semantics.
  This paper presents SlsReuse, the first LLM-powered framework for serverless function reuse. Specifically, SlsReuse first constructs a reusable function repository serving as a foundational knowledge base. Then, it learns unified semantic-enhanced representations of heterogeneous functions through effective prompt engineering with few-shot prompting, capturing implicit code intent, target platforms, programming languages, and cloud services. Finally, given a natural language task query, SlsReuse performs intent-aware discovery combined with a multi-level pruning strategy and similarity matching. We evaluate SlsReuse on a curated dataset of 110 task queries. Built on ChatGPT-4o, one of the most representative LLMs, SlsReuse achieves Recall@10 of 91.20%, exceeding the state-of-the-art baseline by 24.53 percentage points.

</details>


### [24] [Detecting Performance-Relevant Changes in Configurable Software Systems](https://arxiv.org/abs/2511.17271)
*Sebastian Böhm,Florian Sattler,Norbert Siegmund,Sven Apel*

Main category: cs.SE

TL;DR: ConfFLARE通过识别与性能相关代码的数据流交互来估计变更是否可能影响性能，并提取参与此类交互的软件特征，从而选择相关配置子集进行性能分析，显著减少测试配置数量。


<details>
  <summary>Details</summary>
Motivation: 软件系统性能具有波动性，频繁的性能分析成本高昂，特别是在存在可配置性的情况下。配置采样方法无法保证完整性，可能遗漏仅影响少数配置的性能回归。

Method: 提出ConfFLARE方法，通过识别数据流与性能相关代码的交互，检测可能影响性能的变更，并提取相关软件特征，基于这些特征选择配置子集进行性能分析。

Result: 在合成和真实世界软件系统的研究中，ConfFLARE几乎在所有情况下正确检测到性能回归，除两个案例外均识别出相关特征，平均减少79%（合成）和70%（真实世界）的测试配置，节省数小时性能测试时间。

Conclusion: ConfFLARE能有效识别性能相关变更并显著减少性能测试工作量，为软件性能维护提供高效解决方案。

Abstract: Performance is a volatile property of a software system and frequent performance profiling is required to keep the knowledge about a software system's performance behavior up to date. Repeating all performance measurements after every revision is a cost-intensive task, especially in the presence of configurability, where one has to measure multiple configurations to obtain a comprehensive picture. Configuration sampling is a common approach to control the measurement cost. However, it cannot guarantee completeness and might miss performance regressions, especially if they only affect few configurations. As an alternative to solve the cost reduction problem, we present ConfFLARE: ConfFLARE estimates whether a change potentially impacts performance by identifying data-flow interactions with performance-relevant code and extracts which software features participate in such interactions. Based on these features, we can select a subset of relevant configurations to focus performance profiling efforts on. In a study conducted on both, synthetic and real-world software systems, ConfFLARE correctly detects performance regressions in almost all cases and identifies relevant features in all but two cases, reducing the number of configurations to be tested on average by $79\%$ for synthetic and by $70\%$ for real-world regression scenarios saving hours of performance testing time.

</details>


### [25] [Framework Matters: Energy Efficiency of UI Automation Testing Frameworks](https://arxiv.org/abs/2511.17303)
*Timmie M. R. Lagermann,Kristina Sophia Carter,Su Mei Gwen Ho,Luís Cruz,Kerstin Eder,Maja H. Kirkeby*

Main category: cs.SE

TL;DR: 研究比较了四种网页UI自动化测试框架的能耗表现，发现不同框架执行相同UI操作的能耗差异可达6倍，Puppeteer在多数操作中最节能，Nightwatch通常能耗最高。


<details>
  <summary>Details</summary>
Motivation: 研究网页UI自动化测试框架的能耗特性，为开发者在测试特定UI操作时提供能耗透明的决策依据。

Method: 在受控的客户端-服务器环境中使用外部功率测量设备，对四种测试框架的7种UI操作各重复35次，测量能耗表现。

Result: 不同框架和操作的能耗差异显著：Puppeteer在左键点击、右键点击、双击、复选框和文本输入操作中最节能；Selenium在刷新和滚动操作中最节能；Nightwatch通常能耗最高。相同操作在不同框架间的能耗差异可达6倍。

Conclusion: UI自动化测试框架的能耗透明度有助于开发者为特定UI操作测试选择更节能的框架，从而做出能源感知的决策。

Abstract: We examine per action energy consumption across four web user interface (UI) automation testing frameworks to determine whether consistent tendencies can guide energy-aware test design. Using a controlled client-server setup with external power metering, we repeat each UI action (refresh, click variants, checkbox, drag&drop, input-text, scroll) 35 times. Across each of the actions, energy costs vary by both framework and action. Puppeteer is the most efficient for left-click, right-click, double-click, checkbox, and input-text; Selenium is the most efficient for refresh and scroll; Nightwatch is generally the least energy efficient. The energy cost of performing the same action varied by up to a factor of six depending on the framework. This indicates that providing transparency of energy consumption for UI automation testing frameworks allows developers to make informed, energy-aware decisions when testing a specific UI action.

</details>


### [26] [Agentic Program Verification](https://arxiv.org/abs/2511.17330)
*Haoxin Tu,Huan Zhao,Yahui Song,Mehtab Zafar,Ruijie Meng,Abhik Roychoudhury*

Main category: cs.SE

TL;DR: AutoRocq是首个用于程序验证的LLM智能体，通过迭代精化循环与Rocq定理证明器协作，实现自主程序验证，无需大量训练数据。


<details>
  <summary>Details</summary>
Motivation: 随着AI生成代码的普及，需要AI智能体来验证大量生成的代码。程序验证比一般数学推理更具结构性和上下文丰富性，形成有吸引力的研究方向。

Method: 采用LLM智能体与Rocq定理证明器协作的方法，通过迭代精化循环学习并改进证明，智能体从定理证明器获取上下文和反馈，自主构建证明树结构。

Result: 在SV-COMP基准测试和Linux内核模块上的实验评估显示，该方法在实现自动化程序验证方面具有良好效果。

Conclusion: 该证明智能体有望与AI编码智能体集成，实现生成与验证的闭环，推动可信自动编程愿景的实现。

Abstract: Automatically generated code is gaining traction recently, owing to the prevalence of Large Language Models (LLMs). Further, the AlphaProof initiative has demonstrated the possibility of using AI for general mathematical reasoning. Reasoning about computer programs (software) can be accomplished via general mathematical reasoning; however, it tends to be more structured and richer in contexts. This forms an attractive proposition, since then AI agents can be used to reason about voluminous code that gets generated by AI.
  In this work, we present a first LLM agent, AutoRocq, for conducting program verification. Unlike past works, which rely on extensive training of LLMs on proof examples, our agent learns on-the-fly and improves the proof via an iterative refinement loop. The iterative improvement of the proof is achieved by the proof agent communicating with the Rocq (formerly Coq) theorem prover to get additional context and feedback. The final result of the iteration is a proof derivation checked by the Rocq theorem prover. In this way, our proof construction involves autonomous collaboration between the proof agent and the theorem prover. This autonomy facilitates the search for proofs and decision-making in deciding on the structure of the proof tree.
  Experimental evaluation on SV-COMP benchmarks and on Linux kernel modules shows promising efficacy in achieving automated program verification. As automation in code generation becomes more widespread, we posit that our proof agent can be potentially integrated with AI coding agents to achieve a generate and validate loop, thus moving closer to the vision of trusted automatic programming.

</details>


### [27] [Exploring Scientific Debt: Harnessing AI for SATD Identification in Scientific Software](https://arxiv.org/abs/2511.17368)
*Eric L. Melin,Ahmed Musa Awon,Nasir U. Eisty,Neil A. Ernst,Shurui Zhou*

Main category: cs.SE

TL;DR: 本研究探索科学软件中的自认技术债务，比较科学软件与通用开源软件的SATD差异，并评估基于Transformer的SATD识别模型。发现科学软件包含9.25倍科学债务和4.93倍SATD，最佳模型性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 科学软件中的自认技术债务对研究结果的准确性和可重复性构成威胁，但该领域的研究仍很缺乏，需要理解SATD在科学软件中的特征和管理策略。

Method: 分析27个科学和通用软件仓库，涵盖多个领域和语言；在67,066个标注代码注释上微调比较10个基于Transformer的模型（1亿-70亿参数）。

Result: 科学软件包含9.25倍科学债务和4.93倍SATD，主要由于复杂计算、领域约束和不断变化的研究需求；最佳模型性能优于现有方法。

Conclusion: 科学软件中的SATD与通用软件存在显著差异，影响软件质量和科学有效性；识别这些挑战有助于开发更智能的债务管理策略，保障科学发现的完整性。

Abstract: Developers often leave behind clues in their code, admitting where it falls short, known as Self-Admitted Technical Debt (SATD). In the world of Scientific Software (SSW), where innovation moves fast and collaboration is key, such debt is not just common but deeply impactful. As research relies on accurate and reproducible results, accumulating SATD can threaten the very foundations of scientific discovery. Yet, despite its significance, the relationship between SATD and SSW remains largely unexplored, leaving a crucial gap in understanding how to manage SATD in this critical domain. This study explores SATD in SSW repositories, comparing SATD in scientific versus general-purpose open-source software and evaluating transformer-based models for SATD identification. We analyzed SATD in 27 scientific and general-purpose repositories across multiple domains and languages. We fine-tuned and compared 10 transformer-based models (100M-7B parameters) on 67,066 labeled code comments. SSW contains 9.25x more Scientific Debt and 4.93x more SATD than general-purpose software due to complex computations, domain constraints, and evolving research needs. Furthermore, our best model outperforms existing ones. This study uncovers how SATD in SSW differs from general software, revealing its impact on quality and scientific validity. By recognizing these challenges, developers and researchers can adopt smarter strategies to manage debt and safeguard the integrity of scientific discovery.

</details>
