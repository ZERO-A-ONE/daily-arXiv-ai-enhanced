<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 21]
- [cs.CR](#cs.CR) [Total: 21]
- [cs.MA](#cs.MA) [Total: 5]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [The CAISAR Platform: Extending the Reach of Machine Learning Specification and Verification](https://arxiv.org/abs/2506.12084)
*Michele Alberti,François Bobot,Julien Girard-Satabin,Alban Grastien,Aymeric Varasse,Zakaria Chihani*

Main category: cs.SE

TL;DR: CAISAR是一个开源平台，专注于机器学习程序的规范与验证，支持复杂属性的建模，并能自动转换为现有验证工具的查询。


<details>
  <summary>Details</summary>
Motivation: 当前机器学习验证工具多样化但难以比较，且主要局限于局部鲁棒性属性，无法处理更复杂的属性（如涉及多个神经网络的属性）。

Method: CAISAR提供一种规范语言，支持神经网络、支持向量机和提升树等模型的复杂属性建模，并通过自动化图编辑技术将其转换为现有验证工具的查询。

Result: CAISAR能够有效表达和验证复杂属性，并通过现有验证工具实现自动化验证。

Conclusion: CAISAR为解决机器学习验证工具的局限性提供了实用方案，支持更广泛的属性验证。

Abstract: The formal specification and verification of machine learning programs saw
remarkable progress in less than a decade, leading to a profusion of tools.
However, diversity may lead to fragmentation, resulting in tools that are
difficult to compare, except for very specific benchmarks. Furthermore, this
progress is heavily geared towards the specification and verification of a
certain class of property, that is, local robustness properties. But while
provers are becoming more and more efficient at solving local robustness
properties, even slightly more complex properties, involving multiple neural
networks for example, cannot be expressed in the input languages of winners of
the International Competition of Verification of Neural Networks VNN-Comp. In
this tool paper, we present CAISAR, an open-source platform dedicated to
machine learning specification and verification. We present its specification
language, suitable for modelling complex properties on neural networks, support
vector machines and boosted trees. We show on concrete use-cases how
specifications written in this language are automatically translated to queries
to state-of-the-art provers, notably by using automated graph editing
techniques, making it possible to use their off-the-shelf versions. The
artifact to reproduce the paper claims is available at the following DOI:
https://doi.org/10.5281/zenodo.15209510

</details>


### [2] [Quantum-Inspired Differentiable Integral Neural Networks (QIDINNs): A Feynman-Based Architecture for Continuous Learning Over Streaming Data](https://arxiv.org/abs/2506.12111)
*Oscar Boullosa Dapena*

Main category: cs.SE

TL;DR: 提出了一种名为QIDINNs的新型架构，利用积分历史数据实现更稳定、可解释的实时连续学习。


<details>
  <summary>Details</summary>
Motivation: 解决传统梯度方法（如BPTT）在处理无限时间数据时的计算和稳定性问题。

Method: 基于费曼积分技术，将神经更新公式化为历史数据的积分，兼容量子梯度估计框架。

Result: 在合成和实际流数据任务中验证了模型的有效性。

Conclusion: QIDINNs为混合经典-量子神经计算开辟了新方向，并提出了可扩展实现的路径。

Abstract: Real-time continuous learning over streaming data remains a central challenge
in deep learning and AI systems. Traditional gradient-based models such as
backpropagation through time (BPTT) face computational and stability
limitations when dealing with temporally unbounded data. In this paper, we
introduce a novel architecture, Quantum-Inspired Differentiable Integral Neural
Networks (QIDINNs), which leverages the Feynman technique of differentiation
under the integral sign to formulate neural updates as integrals over
historical data. This reformulation allows for smoother, more stable learning
dynamics that are both physically interpretable and computationally tractable.
Inspired by Feynman's path integral formalism and compatible with quantum
gradient estimation frameworks, QIDINNs open a path toward hybrid
classical-quantum neural computation. We demonstrate our model's effectiveness
on synthetic and real-world streaming tasks, and we propose directions for
quantum extensions and scalable implementations.

</details>


### [3] [Can LLMs Generate High-Quality Test Cases for Algorithm Problems? TestCase-Eval: A Systematic Evaluation of Fault Coverage and Exposure](https://arxiv.org/abs/2506.12278)
*Zheyuan Yang,Zexi Kuang,Xue Xia,Yilun Zhao*

Main category: cs.SE

TL;DR: TestCase-Eval是一个新基准，用于系统评估LLM在测试用例生成中的表现，包含500个算法问题和10万个人工编写的解决方案，重点关注故障覆盖率和故障暴露两个任务。


<details>
  <summary>Details</summary>
Motivation: 评估LLM在生成算法问题测试用例中的能力，揭示其优势和局限性。

Method: 使用Codeforces平台的500个算法问题和10万个人工解决方案，评估19种先进LLM在故障覆盖率和故障暴露任务中的表现。

Result: 提供了对19种LLM的全面评估，揭示了它们在生成有效测试用例方面的能力。

Conclusion: TestCase-Eval为LLM在测试用例生成中的表现提供了系统评估，有助于理解其实际应用潜力。

Abstract: We introduce TestCase-Eval, a new benchmark for systematic evaluation of LLMs
in test-case generation. TestCase-Eval includes 500 algorithm problems and
100,000 human-crafted solutions from the Codeforces platform. It focuses on two
pivotal tasks: (1) Fault Coverage, which measures how well LLM-generated test
sets probe diverse input scenarios and cover a wide range of potential failure
modes. (2) Fault Exposure, which evaluates whether LLMs can craft a tailored
test input that reveals a specific incorrect code implementation. We provide a
comprehensive assessment of 19 state-of-the-art open-source and proprietary
LLMs on TestCase-Eval, offering insights into their strengths and limitations
in generating effective test cases for algorithm problems.

</details>


### [4] [The Foundation Cracks: A Comprehensive Study on Bugs and Testing Practices in LLM Libraries](https://arxiv.org/abs/2506.12320)
*Weipeng Jiang,Xiaoyu Zhang,Xiaofei Xie,Jiongchi Yu,Yuhan Zhi,Shiqing Ma,Chao Shen*

Main category: cs.SE

TL;DR: 该论文首次对现代LLM库中的错误特征和测试实践进行了全面实证研究，发现API误用是主要问题，并提出了改进建议。


<details>
  <summary>Details</summary>
Motivation: LLM库在AI生态中至关重要，但频繁的质量问题和错误威胁其可靠性，因此需要深入研究其错误特征和测试实践。

Method: 通过分析313个bug修复提交和7,748个测试函数，建立了错误症状和根源的分类体系，并评估现有测试方法的有效性。

Result: API误用是主要根源（32.17%-48.19%），测试不足（41.73%）、缺乏测试驱动（32.37%）和弱测试预言（25.90%）是测试失效的主要原因。

Conclusion: 研究揭示了LLM库的独特错误模式，并提出了改进测试实践的建议，以提升库的质量保障。

Abstract: Large Language Model (LLM) libraries have emerged as the foundational
infrastructure powering today's AI revolution, serving as the backbone for LLM
deployment, inference optimization, fine-tuning, and production serving across
diverse applications. Despite their critical role in the LLM ecosystem, these
libraries face frequent quality issues and bugs that threaten the reliability
of AI systems built upon them. To address this knowledge gap, we present the
first comprehensive empirical investigation into bug characteristics and
testing practices in modern LLM libraries. We examine 313 bug-fixing commits
extracted across two widely-adopted LLM libraries: HuggingFace Transformers and
vLLM.Through rigorous manual analysis, we establish comprehensive taxonomies
categorizing bug symptoms into 5 types and root causes into 14 distinct
categories.Our primary discovery shows that API misuse has emerged as the
predominant root cause (32.17%-48.19%), representing a notable transition from
algorithm-focused defects in conventional deep learning frameworks toward
interface-oriented problems. Additionally, we examine 7,748 test functions to
identify 7 distinct test oracle categories employed in current testing
approaches, with predefined expected outputs (such as specific tensors and text
strings) being the most common strategy. Our assessment of existing testing
effectiveness demonstrates that the majority of bugs escape detection due to
inadequate test cases (41.73%), lack of test drivers (32.37%), and weak test
oracles (25.90%). Drawing from these findings, we offer some recommendations
for enhancing LLM library quality assurance.

</details>


### [5] [How Developers Use AI Agents: When They Work, When They Don't, and Why](https://arxiv.org/abs/2506.12347)
*Aayush Kumar,Yasharth Bajpai,Sumit Gulwani,Gustavo Soares,Emerson Murphy-Hill*

Main category: cs.SE

TL;DR: 研究发现，开发者与软件工程代理（SWE agents）的协作中，渐进式解决问题和主动迭代代理输出的方式更有效，但信任和调试仍是挑战。


<details>
  <summary>Details</summary>
Motivation: 探究开发者如何与SWE代理协作，以及在此过程中出现的沟通挑战。

Method: 观察19名开发者使用IDE内代理解决33个开源问题，分析其协作方式和效果。

Result: 约一半问题被成功解决，渐进式和主动协作的开发者更成功，但信任和调试是主要障碍。

Conclusion: 研究结果为优化开发者与代理协作及设计更有效的SWE代理提供了依据。

Abstract: Software Engineering Agents (SWE agents) can autonomously perform development
tasks on benchmarks like SWE Bench, but still face challenges when tackling
complex and ambiguous real-world tasks. Consequently, SWE agents are often
designed to allow interactivity with developers, enabling collaborative
problem-solving. To understand how developers collaborate with SWE agents and
the communication challenges that arise in such interactions, we observed 19
developers using an in-IDE agent to resolve 33 open issues in repositories to
which they had previously contributed. Participants successfully resolved about
half of these issues, with participants solving issues incrementally having
greater success than those using a one-shot approach. Participants who actively
collaborated with the agent and iterated on its outputs were also more
successful, though they faced challenges in trusting the agent's responses and
collaborating on debugging and testing. These results have implications for
successful developer-agent collaborations, and for the design of more effective
SWE agents.

</details>


### [6] [A Mapping Study About Training in Industry Context in Software Engineering](https://arxiv.org/abs/2506.12590)
*Breno Alves de Andrade,Rodrigo Siqueira,Lidiane Gomes,Antonio Oliveira,Danilo Monteiro Ribeiro*

Main category: cs.SE

TL;DR: 该研究通过系统性映射方法，分析了软件工程领域企业培训的研究现状，发现多数研究集中在培训方法与策略上，其他领域如任务分析和模拟培训存在明显空白。


<details>
  <summary>Details</summary>
Motivation: 企业培训在软件工程领域对专业人员持续发展至关重要，但目前缺乏对其设计、实施和评估的系统化理解。

Method: 采用系统性映射研究，分析26篇相关文献，依据Eduardo Salas的培训框架进行分类。

Result: 结果显示研究多集中于培训方法与策略，其他领域如任务分析和模拟培训研究不足，且多数研究缺乏方法论严谨性。

Conclusion: 研究揭示了软件工程企业培训的现状与不足，为未来研究提供了方向，并提出了改进培训设计的建议。

Abstract: Context: Corporate training plays a strategic role in the continuous
development of professionals in the software engineering industry. However,
there is a lack of systematized understanding of how training initiatives are
designed, implemented, and evaluated within this domain.
  Objective: This study aims to map the current state of research on corporate
training in software engineering in industry settings, using Eduardo Salas'
training framework as an analytical lens.
  Method: A systematic mapping study was conducted involving the selection and
analysis of 26 primary studies published in the field. Each study was
categorized according to Salas' four key areas: Training Needs Analysis,
Antecedent Training Conditions, Training Methods and Instructional Strategies,
and Post-Training Conditions.
  Results: The findings show a predominance of studies focusing on Training
Methods and Instructional Strategies. Significant gaps were identified in other
areas, particularly regarding Job/Task Analysis and Simulation-based Training
and Games. Most studies were experience reports, lacking methodological rigor
and longitudinal assessment.
  Conclusions: The study offers a structured overview of how corporate training
is approached in software engineering, revealing underexplored areas and
proposing directions for future research. It contributes to both academic and
practical communities by highlighting challenges, methodological trends, and
opportunities for designing more effective training programs in industry.

</details>


### [7] [Real-Time Agile Software Management for Edge and Fog Computing Based Smart City Infrastructure](https://arxiv.org/abs/2506.12616)
*Debasish Jana,Pinakpani Pal,Pawan Kumar*

Main category: cs.SE

TL;DR: 论文提出ROOF框架，通过边缘计算和雾计算降低延迟，提升能效和安全性，并在实际城市案例中验证其效果。


<details>
  <summary>Details</summary>
Motivation: 随着物联网设备数量激增，传统云系统面临带宽、延迟和能耗限制，需更高效的实时数据处理架构。

Method: 采用ROOF框架，结合雾缓存、低功耗无线传输和AI资源分配，同时通过TLS加密、区块链认证和边缘访问控制增强安全性。

Result: 在Bhubaneswar、巴塞罗那和哥本哈根的案例中，ROOF成功应用于交通系统和环境监测，验证了其有效性。

Conclusion: 论文总结了AI驱动分析在智能城市基础设施中的关键挑战和前景。

Abstract: The evolution of smart cities demands scalable, secure, and energy-efficient
architectures for real-time data processing. With the number of IoT devices
expected to exceed 40 billion by 2030, traditional cloud-based systems are
increasingly constrained by bandwidth, latency, and energy limitations. This
paper leverages the ROOF (Real-time Onsite Operations Facilitation) framework
with decentralized computing at intermediary fog and peripheral edge network
layers to reduce latency by processing data near its point of origin. ROOF
features fog caching to avoid redundancy, ultra-low-power wireless transmission
for energy savings, and AI-driven resource allocation for efficiency. Security
is enhanced through TLS encryption, blockchain-based authentication, and
edge-level access control. Case studies from Bhubaneswar, Barcelona and
Copenhagen validate the use of ROOF in traffic systems and environmental
monitoring. The paper concludes by outlining key challenges and prospects of
AI-driven analytics in smart urban infrastructure.

</details>


### [8] [Social Media Reactions to Open Source Promotions: AI-Powered GitHub Projects on Hacker News](https://arxiv.org/abs/2506.12643)
*Prachnachai Meakpaiboonwattana,Warittha Tarntong,Thai Mekratanavorakul,Chaiyong Ragkhitwetsagul,Pattaraporn Sangaroonsilp,Raula Kula,Morakot Choetkiertikul,Kenichi Matsumoto,Thanwadee Sunetnanta*

Main category: cs.SE

TL;DR: 研究探讨了Hacker News对GitHub上AI项目开发者活动的影响，发现通过该平台推广的项目在关注度和贡献者数量上显著增加。


<details>
  <summary>Details</summary>
Motivation: 社交媒体平台对公共话语和信息传播的影响日益增强，尤其是Hacker News对开源AI项目的推广作用尚未充分研究。

Method: 分析了2,195条Hacker News故事及其评论，并追踪了1,814个相关GitHub仓库的活动变化。

Result: 19%的AI开发者通过Hacker News推广项目，项目在GitHub上的分叉、星标和贡献者数量显著增加。

Conclusion: Hacker News是推广AI开源项目的有效平台，能够提升项目可见性并加速开发。

Abstract: Social media platforms have become more influential than traditional news
sources, shaping public discourse and accelerating the spread of information.
With the rapid advancement of artificial intelligence (AI), open-source
software (OSS) projects can leverage these platforms to gain visibility and
attract contributors. In this study, we investigate the relationship between
Hacker News, a social news site focused on computer science and
entrepreneurship, and the extent to which it influences developer activity on
the promoted GitHub AI projects.
  We analyzed 2,195 Hacker News (HN) stories and their corresponding comments
over a two-year period. Our findings reveal that at least 19\% of AI developers
promoted their GitHub projects on Hacker News, often receiving positive
engagement from the community. By tracking activity on the associated 1,814
GitHub repositories after they were shared on Hacker News, we observed a
significant increase in forks, stars, and contributors. These results suggest
that Hacker News serves as a viable platform for AI-powered OSS projects, with
the potential to gain attention, foster community engagement, and accelerate
software development.

</details>


### [9] [Towards Lean Research Inception: Assessing Practical Relevance of Formulated Research Problems](https://arxiv.org/abs/2506.12669)
*Anrafel Fernandes Pereira,Marcos Kalinowski,Maria Teresa Baldassarre,Jürgen Börstler,Nauman bin Ali,Daniel Mendez*

Main category: cs.SE

TL;DR: 论文介绍了Lean Research Inception (LRI)框架，用于评估软件工程(SE)研究的实践相关性，并通过研讨会验证了其三个评估标准（有价值、可行、适用）的重要性。


<details>
  <summary>Details</summary>
Motivation: 解决SE研究中实践相关性不足的问题，通过明确标准来确保研究问题与工业需求对齐。

Method: 应用LRI框架对已发表论文进行回顾性评估，通过研讨会参与者使用语义差异量表讨论和评估研究问题。

Result: 参与者普遍认同三个标准的重要性（有价值83.3%，可行76.2%，适用73.8%），并提出术语和内容上的改进建议。

Conclusion: LRI框架及其评估标准有助于评估SE研究的实践相关性，但仍需进一步研究和完善。

Abstract: [Context] The lack of practical relevance in many Software Engineering (SE)
research contributions is often rooted in oversimplified views of industrial
practice, weak industry connections, and poorly defined research problems.
Clear criteria for evaluating SE research problems can help align their value,
feasibility, and applicability with industrial needs. [Goal] In this paper, we
introduce the Lean Research Inception (LRI) framework, designed to support the
formulation and assessment of practically relevant research problems in SE. We
describe its initial evaluation strategy conducted in a workshop with a network
of SE researchers experienced in industry-academia collaboration and report the
evaluation of its three assessment criteria (valuable, feasible, and
applicable) regarding their importance in assessing practical relevance.
[Method] We applied LRI retroactively to a published research paper, engaging
workshop participants in discussing and assessing the research problem by
applying the proposed criteria using a semantic differential scale.
Participants provided feedback on the criteria's importance and completeness,
drawn from their own experiences in industry-academia collaboration. [Results]
The findings reveal an overall agreement on the importance of the three
criteria - valuable (83.3%), feasible (76.2%), and applicable (73.8%) - for
aligning research problems with industrial needs. Qualitative feedback
suggested adjustments in terminology with a clearer distinction between
feasible and applicable, and refinements for valuable by more clearly
considering business value, ROI, and originality. [Conclusion] While LRI
constitutes ongoing research and requires further evaluation, our results
strengthen our confidence that the three criteria applied using the semantic
differential scale can already help the community assess the practical
relevance of SE research problems.

</details>


### [10] [Get on the Train or be Left on the Station: Using LLMs for Software Engineering Research](https://arxiv.org/abs/2506.12691)
*Bianca Trinkenreich,Fabio Calefato,Geir Hanssen,Kelly Blincoe,Marcos Kalinowski,Mauro Pezzè,Paolo Tell,Margaret-Anne Storey*

Main category: cs.SE

TL;DR: 论文探讨了大型语言模型（LLMs）对软件工程（SE）研究的影响，强调需要以人为中心的方式整合LLMs，以确保科学严谨性和伦理责任。


<details>
  <summary>Details</summary>
Motivation: LLMs正在改变SE研究和实践，但如何平衡其潜力与风险尚不明确。论文旨在推动SE研究社区主动塑造LLMs的整合方式。

Method: 采用McLuhan的媒体四定律理论框架，分析LLMs对SE研究的影响，包括增强能力、淘汰传统方法、恢复历史价值及潜在逆转效应。

Result: 分析揭示了LLMs带来的创新机会和潜在风险，强调了人为监督和可解释性的重要性。

Conclusion: 呼吁SE研究社区主动利用LLMs的优势，同时制定框架和指南以降低风险，确保AI增强未来的研究严谨性和影响力。

Abstract: The adoption of Large Language Models (LLMs) is not only transforming
software engineering (SE) practice but is also poised to fundamentally disrupt
how research is conducted in the field. While perspectives on this
transformation range from viewing LLMs as mere productivity tools to
considering them revolutionary forces, we argue that the SE research community
must proactively engage with and shape the integration of LLMs into research
practices, emphasizing human agency in this transformation. As LLMs rapidly
become integral to SE research - both as tools that support investigations and
as subjects of study - a human-centric perspective is essential. Ensuring human
oversight and interpretability is necessary for upholding scientific rigor,
fostering ethical responsibility, and driving advancements in the field.
Drawing from discussions at the 2nd Copenhagen Symposium on Human-Centered AI
in SE, this position paper employs McLuhan's Tetrad of Media Laws to analyze
the impact of LLMs on SE research. Through this theoretical lens, we examine
how LLMs enhance research capabilities through accelerated ideation and
automated processes, make some traditional research practices obsolete,
retrieve valuable aspects of historical research approaches, and risk reversal
effects when taken to extremes. Our analysis reveals opportunities for
innovation and potential pitfalls that require careful consideration. We
conclude with a call to action for the SE research community to proactively
harness the benefits of LLMs while developing frameworks and guidelines to
mitigate their risks, to ensure continued rigor and impact of research in an
AI-augmented future.

</details>


### [11] [Humanity's Last Code Exam: Can Advanced LLMs Conquer Human's Hardest Code Competition?](https://arxiv.org/abs/2506.12713)
*Xiangyang Li,Xiaopeng Li,Kuicai Dong,Quanhu Zhang,Rongju Ruan,Xinyi Dai,Xiaoshuang Liu,Shengchun Xu,Yasheng Wang,Ruiming Tang*

Main category: cs.SE

TL;DR: 论文提出了一个名为HLCE的高难度代码生成基准，包含ICPC和IOI的235道难题，并设计了可复现的评测环境。评估显示，最强LLM表现不佳，同时提出了一种衡量LLM自我认知能力的新任务。


<details>
  <summary>Details</summary>
Motivation: 现有代码生成基准对高级LLM挑战不足，需更难的评测标准以反映其真实能力。

Method: 从ICPC和IOI选取235道难题构建HLCE，设计在线-离线沙盒确保评测可复现，并提出“自我认知”任务。

Result: 最强LLM（o4-mini和Gemini-2.5 Pro）的pass@1率仅为15.9%和11.4%，且自我认知能力与代码生成表现不相关。

Conclusion: HLCE将成为代码生成的里程碑挑战，推动高性能推理和人机协作编程的进步。

Abstract: Code generation is a core capability of large language models (LLMs), yet
mainstream benchmarks (e.g., APPs and LiveCodeBench) contain questions with
medium-level difficulty and pose no challenge to advanced LLMs. To better
reflected the advanced reasoning and code generation ability, We introduce
Humanity's Last Code Exam (HLCE), comprising 235 most challenging problems from
the International Collegiate Programming Contest (ICPC World Finals) and the
International Olympiad in Informatics (IOI) spanning 2010 - 2024. As part of
HLCE, we design a harmonized online-offline sandbox that guarantees fully
reproducible evaluation. Through our comprehensive evaluation, we observe that
even the strongest reasoning LLMs: o4-mini(high) and Gemini-2.5 Pro, achieve
pass@1 rates of only 15.9% and 11.4%, respectively. Meanwhile, we propose a
novel "self-recognition" task to measure LLMs' awareness of their own
capabilities. Results indicate that LLMs' self-recognition abilities are not
proportionally correlated with their code generation performance. Finally, our
empirical validation of test-time scaling laws reveals that current advanced
LLMs have substantial room for improvement on complex programming tasks. We
expect HLCE to become a milestone challenge for code generation and to catalyze
advances in high-performance reasoning and human-AI collaborative programming.
Our code and dataset are also public
available(https://github.com/Humanity-s-Last-Code-Exam/HLCE).

</details>


### [12] [MCTS-Refined CoT: High-Quality Fine-Tuning Data for LLM-Based Repository Issue Resolution](https://arxiv.org/abs/2506.12728)
*Yibo Wang,Zhihao Peng,Ying Wang,Zhao Wei,Hai Yu,Zhiliang Zhu*

Main category: cs.SE

TL;DR: 论文提出MCTS-REFINE算法，通过动态验证和优化推理步骤生成高质量CoT数据，显著提升LLM在软件问题解决任务中的性能。


<details>
  <summary>Details</summary>
Motivation: 尽管LLM在软件工程中表现优异，但现有方法在数据质量和推理验证方面存在缺陷，限制了其可靠性。

Method: 采用MCTS-REFINE算法，结合反射机制和严格采样协议，分解问题解决任务并验证中间步骤。

Result: 实验显示，使用该方法的LLM在SWE-bench任务中表现优于基线模型，如Qwen2.5-72B-Instruct达到28.3%和35.0%的解决率。

Conclusion: MCTS-REFINE通过高质量CoT数据显著提升了LLM的推理能力，为软件问题解决提供了更可靠的解决方案。

Abstract: LLMs demonstrate strong performance in auto-mated software engineering,
particularly for code generation and issue resolution. While proprietary models
like GPT-4o achieve high benchmarks scores on SWE-bench, their API dependence,
cost, and privacy concerns limit adoption. Open-source alternatives offer
transparency but underperform in complex tasks, especially sub-100B parameter
models. Although quality Chain-of-Thought (CoT) data can enhance reasoning,
current methods face two critical flaws: (1) weak rejection sampling reduces
data quality, and (2) inadequate step validation causes error accumulation.
These limitations lead to flawed reasoning chains that impair LLMs'ability to
learn reliable issue resolution. The paper proposes MCTS-REFINE, an enhanced
Monte Carlo Tree Search (MCTS)-based algorithm that dynamically validates and
optimizes intermediate reasoning steps through a rigorous rejection sampling
strategy, generating high-quality CoT data to improve LLM performance in issue
resolution tasks. Key innovations include: (1) augmenting MCTS with a
reflection mechanism that corrects errors via rejection sampling and
refinement, (2) decomposing issue resolution into three subtasks-File
Localization, Fault Localization, and Patch Generation-each with clear
ground-truth criteria, and (3) enforcing a strict sampling protocol where
intermediate outputs must exactly match verified developer patches, ensuring
correctness across reasoning paths. Experiments on SWE-bench Lite and SWE-bench
Verified demonstrate that LLMs fine-tuned with our CoT dataset achieve
substantial improvements over baselines.Notably, Qwen2.5-72B- Instruct achieves
28.3%(Lite) and 35.0%(Verified) resolution rates, surpassing SOTA baseline
SWE-Fixer-Qwen-72B with the same parameter scale, which only reached
24.7%(Lite) and 32.8%(Verified).

</details>


### [13] [IDOL: Improved Different Optimization Levels Testing for Solidity Compilers](https://arxiv.org/abs/2506.12760)
*Lantian Li,Yejian Liang,Zhongxing Yu*

Main category: cs.SE

TL;DR: 本文提出了一种名为IDOL的创新方法，用于测试Solidity编译器，通过反向优化转换生成语义等效的智能合约变体，以最大化触发编译器优化逻辑的机会。初步评估发现了三个编译器优化漏洞。


<details>
  <summary>Details</summary>
Motivation: 智能合约一旦部署无法修改，其漏洞或设计缺陷可能导致重大财务损失或法律问题。编译器作为开发过程中的关键组件，直接影响智能合约的质量和安全性。

Method: 提出IDOL方法，通过反向优化转换生成语义等效的智能合约变体，以测试编译器优化逻辑。

Result: 初步评估发现了三个编译器优化漏洞。

Conclusion: IDOL方法有效提升了编译器测试的覆盖率，有助于发现潜在的优化漏洞，从而提高智能合约的安全性。

Abstract: As blockchain technology continues to evolve and mature, smart contracts have
become a key driving force behind the digitization and automation of
transactions. Smart contracts greatly simplify and refine the traditional
business transaction processes, and thus have had a profound impact on various
industries such as finance and supply chain management. However, because smart
contracts cannot be modified once deployed, any vulnerabilities or design flaws
within the contract cannot be easily fixed, potentially leading to significant
financial losses or even legal issues. The compiler, as a critical component in
the development process, directly affects the quality and security of smart
contracts. This paper innovatively proposes a method, known as the Improved
Different Optimization Levels (IDOL), for testing the Solidity compiler. The
key idea behind IDOL is to perform reverse optimization transformations (i.e.,
change optimized form into unoptimized form) to generate semantically
equivalent variants of the smart contracts under test, aiming to maximize the
opportunities to trigger the optimization logic of compilers. We conducted a
preliminary evaluation of IDOL and three confirmed compiler optimization bugs
have been uncovered at the time of writing.

</details>


### [14] [Towards Operation Proof Obligation Generation for VDM](https://arxiv.org/abs/2506.12858)
*Nick Battle,Peter Gorm Larsen*

Main category: cs.SE

TL;DR: 本文讨论了形式化方法中模型内部一致性的验证工具，特别是VDM工具在生成证明义务方面的局限性，并介绍了当前解决这一问题的进展。


<details>
  <summary>Details</summary>
Motivation: 形式化方法需要确保模型内部一致性，但现有工具（如VDM）在生成显式操作体的证明义务方面存在不足。

Method: 研究分析了当前VDM工具的局限性，并提出了改进方向。

Result: 展示了当前工具的能力，并指出了仍需完成的工作。

Conclusion: 尽管已有进展，但在显式操作体的证明义务生成方面仍需进一步研究。

Abstract: All formalisms have the ability to ensure that their models are internally
consistent. Potential inconsistencies are generally highlighted by assertions
called proof obligations, and the generation of these obligations is an
important role of the tools that support the method. This capability has been
available for VDM tools for many years. However, support for obligation
generation for explicit operation bodies has always been limited. This work
describes the current state of work to address this, showing the capabilities
so far and highlighting the work remaining.

</details>


### [15] [Designing Deep Learning Frameworks for LLMs:Challenges, Expectations, and Opportunities](https://arxiv.org/abs/2506.13114)
*Yanzhou Mu,Rong Wang,Juan Zhai,Chunrong Fang,Xiang Chen,Jiacong Wu,An Guo,Jiawei Shen,Bingzhuo Li,Zhenyu Chen*

Main category: cs.SE

TL;DR: 论文通过分析三大深度学习框架（MindSpore、PyTorch、TensorFlow）和八个LLM工具包的issue报告，构建了LLM相关问题的分类体系，揭示了DL框架在支持LLM时面临的技术挑战，并提出了改进建议。


<details>
  <summary>Details</summary>
Motivation: 研究动机是探讨深度学习框架在支持大语言模型（LLM）时面临的挑战，以解决框架的可用性、功能性和稳定性问题，从而提高开发效率和资源利用率。

Method: 研究方法包括大规模分析三大DL框架和八个LLM工具包的issue报告，构建分类体系，并通过与11名LLM用户和8名DL框架开发者的访谈验证和丰富分类内容。

Result: 研究结果包括构建了涵盖问题、需求和错误的分类体系，评估了这些挑战的重要性，并提出了五项关键发现和改进建议。

Conclusion: 结论指出当前DL框架在支持LLM方面存在显著局限性，并提供了提升框架可靠性、可用性和可测试性的具体建议。

Abstract: Large language models (LLMs) drive significant advancements in real industry
applications. LLMs rely on DL frameworks for efficient model construction,
distributed execution, and optimized deployment. Their large parameter scale
and long execution cycles place extreme demands on DL frameworks in terms of
scalability, stability, and efficiency. Therefore, poor usability, limited
functionality, and subtle bugs in DL frameworks may hinder development
efficiency and cause severe failures or resource waste. However, a fundamental
question remains underinvestigated, i.e., What challenges do DL frameworks face
in supporting LLMs? To seek an answer, we investigate these challenges through
a large-scale analysis of issue reports from three major DL frameworks
(MindSpore, PyTorch, TensorFlow) and eight associated LLM toolkits (e.g.,
Megatron). We construct a taxonomy of LLM-centric bugs, requirements, and user
questions and enrich it through interviews with 11 LLM users and eight DL
framework developers, uncovering key technical challenges and misalignments
between user needs and developer priorities. Our contributions are threefold:
(1) we develop a comprehensive taxonomy comprising four question themes (nine
sub-themes), four requirement themes (15 sub-themes), and ten bug themes (45
sub-themes); (2) we assess the perceived importance and priority of these
challenges based on practitioner insights; and (3) we identify five key
findings across the LLM development and propose five actionable recommendations
to improve the reliability, usability, and testability of DL frameworks. Our
results highlight critical limitations in current DL frameworks and offer
concrete guidance for advancing their support for the next generation of LLM
construction and applications.

</details>


### [16] [Querying Large Automotive Software Models: Agentic vs. Direct LLM Approaches](https://arxiv.org/abs/2506.13171)
*Lukasz Mazur,Nenad Petrovic,James Pontes Miranda,Ansgar Radermacher,Robert Rasche,Alois Knoll*

Main category: cs.SE

TL;DR: 论文研究了两种利用大语言模型（LLM）回答软件模型问题的方法：直接提示和基于代理的方法，后者在效率和可行性上表现更优。


<details>
  <summary>Details</summary>
Motivation: 探索如何利用LLM简化对复杂软件模型的理解和交互，尤其是在传统方法难以应对的大型软件模型中。

Method: 比较了直接提示（提供完整模型上下文）和代理方法（结合LLM代理与通用文件访问工具）的效果。

Result: 代理方法在准确性上与直接提示相当，但显著提高了令牌使用效率，特别适合汽车行业的大型软件模型。

Conclusion: 代理方法是处理大型软件模型的唯一可行方案，未来将扩展其支持更多格式和复杂任务。

Abstract: Large language models (LLMs) offer new opportunities for interacting with
complex software artifacts, such as software models, through natural language.
They present especially promising benefits for large software models that are
difficult to grasp in their entirety, making traditional interaction and
analysis approaches challenging. This paper investigates two approaches for
leveraging LLMs to answer questions over software models: direct prompting,
where the whole software model is provided in the context, and an agentic
approach combining LLM-based agents with general-purpose file access tools. We
evaluate these approaches using an Ecore metamodel designed for timing analysis
and software optimization in automotive and embedded domains. Our findings show
that while the agentic approach achieves accuracy comparable to direct
prompting, it is significantly more efficient in terms of token usage. This
efficiency makes the agentic approach particularly suitable for the automotive
industry, where the large size of software models makes direct prompting
infeasible, establishing LLM agents as not just a practical alternative but the
only viable solution. Notably, the evaluation was conducted using small LLMs,
which are more feasible to be executed locally - an essential advantage for
meeting strict requirements around privacy, intellectual property protection,
and regulatory compliance. Future work will investigate software models in
diverse formats, explore more complex agent architectures, and extend agentic
workflows to support not only querying but also modification of software
models.

</details>


### [17] [From Empirical Evaluation to Context-Aware Enhancement: Repairing Regression Errors with LLMs](https://arxiv.org/abs/2506.13182)
*Anh Ho,Thanh Le-Cong,Bach Le,Christine Rizkallah*

Main category: cs.SE

TL;DR: 该论文通过实证研究评估现代自动程序修复（APR）技术在修复回归错误中的有效性，并引入RegMiner4APR基准测试工具。研究发现传统APR工具无效，而基于LLM的APR方法在结合错误诱导信息后表现显著提升。


<details>
  <summary>Details</summary>
Motivation: 现有APR技术（尤其是基于LLM的方法）在修复回归错误方面的效果尚未充分研究，因此需要实证评估。

Method: 研究使用RegMiner4APR基准测试工具，包含99个Java回归错误，评估传统APR工具和基于LLM的APR方法，并探索结合错误诱导信息的效果。

Result: 传统APR工具无法修复任何错误，而基于LLM的APR方法表现良好，结合错误诱导信息后修复成功率提升1.8倍。

Conclusion: 基于LLM的APR方法在修复回归错误中具有潜力，结合错误诱导信息能显著提升效果。

Abstract: [...] Since then, various APR approaches, especially those leveraging the
power of large language models (LLMs), have been rapidly developed to fix
general software bugs. Unfortunately, the effectiveness of these advanced
techniques in the context of regression bugs remains largely unexplored. This
gap motivates the need for an empirical study evaluating the effectiveness of
modern APR techniques in fixing real-world regression bugs.
  In this work, we conduct an empirical study of APR techniques on Java
regression bugs. To facilitate our study, we introduce RegMiner4APR, a
high-quality benchmark of Java regression bugs integrated into a framework
designed to facilitate APR research. The current benchmark includes 99
regression bugs collected from 32 widely used real-world Java GitHub
repositories. We begin by conducting an in-depth analysis of the benchmark,
demonstrating its diversity and quality. Building on this foundation, we
empirically evaluate the capabilities of APR to regression bugs by assessing
both traditional APR tools and advanced LLM-based APR approaches. Our
experimental results show that classical APR tools fail to repair any bugs,
while LLM-based APR approaches exhibit promising potential. Motivated by these
results, we investigate impact of incorporating bug-inducing change information
into LLM-based APR approaches for fixing regression bugs. Our results highlight
that this context-aware enhancement significantly improves the performance of
LLM-based APR, yielding 1.8x more successful repairs compared to using
LLM-based APR without such context.

</details>


### [18] [Empirical Evaluation of Large Language Models in Automated Program Repair](https://arxiv.org/abs/2506.13186)
*Jiajun Sun,Fengjie Li,Xinzhu Qi,Hongyu Zhang,Jiajun Jiang*

Main category: cs.SE

TL;DR: 论文研究了现代大规模语言模型（LLMs）在自动程序修复（APR）中的表现，发现模型专业化、规模非线性影响、早期生成正确补丁及提示策略的重要性。


<details>
  <summary>Details</summary>
Motivation: 软件漏洞日益增多，自动程序修复成为研究重点，但现有研究多基于小型模型和Java基准，现代LLMs在多语言和多场景下的修复能力尚未充分探索。

Method: 对四种开源LLMs（CodeLlama、LLaMA、StarCoder、DeepSeek-Coder）进行实证研究，覆盖不同参数规模、架构和用途，评估其在两种漏洞场景、三种语言和四种提示策略下的表现。

Result: 关键发现包括：专业化模型优于通用大模型；修复性能与模型规模非线性相关；正确补丁常早期生成；提示策略显著影响结果。

Conclusion: 研究为设计高效LLM-based APR系统提供了实用指导。

Abstract: The increasing prevalence of software bugs has made automated program repair
(APR) a key research focus. Large language models (LLMs) offer new
opportunities for APR, but existing studies mostly rely on smaller,
earlier-generation models and Java benchmarks. The repair capabilities of
modern, large-scale LLMs across diverse languages and scenarios remain
underexplored. To address this, we conduct a comprehensive empirical study of
four open-source LLMs, CodeLlama, LLaMA, StarCoder, and DeepSeek-Coder,
spanning 7B to 33B parameters, diverse architectures, and purposes. We evaluate
them across two bug scenarios (enterprise-grades and algorithmic), three
languages (Java, C/C++, Python), and four prompting strategies, analyzing over
600K generated patches on six benchmarks. Key findings include: (1) model
specialization (e.g., CodeLlama) can outperform larger general-purpose models
(e.g., LLaMA); (2) repair performance does not scale linearly with model size;
(3) correct patches often appear early in generation; and (4) prompts
significantly affect results. These insights offer practical guidance for
designing effective and efficient LLM-based APR systems.

</details>


### [19] [Isolating Noisy Labelled Test Cases in Human-in-the-Loop Oracle Learning](https://arxiv.org/abs/2506.13273)
*Charaka Geethal Kapugama*

Main category: cs.SE

TL;DR: ISONOISE是一种技术，用于识别人类参与式预言学习过程中错误标记的测试用例，提升学习可靠性。


<details>
  <summary>Details</summary>
Motivation: 错误标记的测试用例会影响人类参与式预言学习的效果，需要一种方法来识别和纠正这些错误。

Method: ISONOISE通过隔离疑似错误标记的测试用例，训练中间预言，并迭代更新标记，直至无错误。

Result: 实验显示ISONOISE能以67%的准确率识别错误标记，且仅需少量重新标记查询。

Conclusion: ISONOISE能有效提升人类参与式预言学习的可靠性。

Abstract: Incorrectly labelled test cases can adversely affect the training process of
human-in-the-loop oracle learning tech-niques. This paper introduces ISONOISE,
a technique designed to identify such mislabelled test cases introduced during
human-in-the-loop oracle learning. This technique can be applied to programs
taking numeric inputs. Given a compromised automatic test oracle and its
training test suite, ISONOISE first isolates thetest cases suspected of being
mislabelled. This task is performed based on the level of disagreement of a
test case with respect to the others. An intermediate automatic test oracle is
trained based on the slightly disagreeing test cases. Based on the predictions
of this intermediate oracle, the test cases suspected of being mislabelled are
systematically presented for relabelling. When mislabelled test cases are
found, the intermediate test oracle is updated. This process repeats until no
mislabelled test case is found in relabelling. ISONOISE was evaluated within
the human-in-the-loop oracle learning method used in LEARN2FIX. Experimental
results demonstrate that ISONOISE can identify mislabelled test cases
introduced by the human in LEARN2FIX with over 67% accuracy, while requiring
only a small number of relabelling queries. These findings highlight the
potential of ISONOISE to enhance the reliability of human-in-the-loop oracle
learning.

</details>


### [20] [Adopting Use Case Descriptions for Requirements Specification: an Industrial Case Study](https://arxiv.org/abs/2506.13303)
*Julian Frattini,Anja Frattini*

Main category: cs.SE

TL;DR: 论文研究了用例描述在实践中的采用情况及其质量影响因素，发现实际应用与理论建议存在偏差，但仅有少数因素（如解决方案导向）对质量有实际影响。


<details>
  <summary>Details</summary>
Motivation: 现有文献缺乏对用例描述在现实中的采用情况、质量相关建议的实际效果以及影响质量的因素的实证研究。

Method: 通过调查一家全球分布的大型案例公司2020年至2024年的1188个业务需求（包含1192个用例），手动评估273个模板式用例描述的质量，并采用描述性和推断性统计分析方法。

Result: 描述性结果显示用例描述的实际采用与理论建议存在偏差；推断性结果表明仅有少数因素（如解决方案导向）对质量有实际影响。

Conclusion: 研究结果可为用例质量研究提供更相关的方向。

Abstract: Context: Use case (UC) descriptions are a prominent format for specifying
functional requirements. Existing literature abounds with recommendations on
how to write high-quality UC descriptions but lacks insights into (1) their
real-world adoption, (2) whether these recommendations correspond to actual
quality, and (3) which factors influence the quality of UCs. Objectives: We aim
to contribute empirical evidence about the adoption of UC descriptions in a
large, globally distributed case company. Methods: We surveyed 1188 business
requirements of a case company that were elicited from 2020-01-01 until
2024-12-31 and contained 1192 UCs in various forms. Among these, we manually
evaluated the 273 template-style UC descriptions against established quality
guidelines. We generated descriptive statistics of the format's adoption over
the surveyed time frame. Furthermore, we used inferential statistics to
determine (a) how properties of the requirements engineering process affected
the UC quality and (b) how UC quality affects subsequent software development
activities. Results and Conclusions: Our descriptive results show how the
adoption of UC descriptions in practice deviates from textbook recommendations.
However, our inferential results suggest that only a few phenomena like
solution-orientation show an actual impact in practice. These results can steer
UC quality research into a more relevant direction.

</details>


### [21] [Model Context Protocol (MCP) at First Glance: Studying the Security and Maintainability of MCP Servers](https://arxiv.org/abs/2506.13538)
*Mohammed Mehedi Hasan,Hao Li,Emad Fallahzadeh,Bram Adams,Ahmed E. Hassan*

Main category: cs.SE

TL;DR: 研究分析了Model Context Protocol (MCP)的健康、安全和可维护性，发现其存在独特漏洞，需针对性检测技术。


<details>
  <summary>Details</summary>
Motivation: MCP作为工具生态系统的标准，其非确定性控制流带来可持续性、安全性和可维护性风险，需深入研究。

Method: 结合通用静态分析工具和MCP专用扫描器，评估1,899个开源MCP服务器的健康、安全和可维护性。

Result: 发现8种独特漏洞，7.2%服务器含通用漏洞，5.5%存在MCP特定工具中毒；66%有代码异味，14.4%含已知错误模式。

Conclusion: 需开发MCP专用漏洞检测技术，同时传统分析和重构实践仍具价值。

Abstract: Although Foundation Models (FMs), such as GPT-4, are increasingly used in
domains like finance and software engineering, reliance on textual interfaces
limits these models' real-world interaction. To address this, FM providers
introduced tool calling-triggering a proliferation of frameworks with distinct
tool interfaces. In late 2024, Anthropic introduced the Model Context Protocol
(MCP) to standardize this tool ecosystem, which has become the de facto
standard with over eight million weekly SDK downloads. Despite its adoption,
MCP's AI-driven, non-deterministic control flow introduces new risks to
sustainability, security, and maintainability, warranting closer examination.
  Towards this end, we present the first large-scale empirical study of MCP.
Using state-of-the-art health metrics and a hybrid analysis pipeline, combining
a general-purpose static analysis tool with an MCP-specific scanner, we
evaluate 1,899 open-source MCP servers to assess their health, security, and
maintainability. Despite MCP servers demonstrating strong health metrics, we
identify eight distinct vulnerabilities-only three overlapping with traditional
software vulnerabilities. Additionally, 7.2% of servers contain general
vulnerabilities and 5.5% exhibit MCP-specific tool poisoning. Regarding
maintainability, while 66% exhibit code smells, 14.4% contain ten bug patterns
overlapping prior research. These findings highlight the need for MCP-specific
vulnerability detection techniques while reaffirming the value of traditional
analysis and refactoring practices.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [22] [LURK-T: Limited Use of Remote Keys With Added Trust in TLS 1.3](https://arxiv.org/abs/2506.12026)
*Behnam Shobiri,Sajjad Pourali,Daniel Migault,Ioana Boureanu,Stere Preda,Mohammad Mannan,Amr Youssef*

Main category: cs.CR

TL;DR: LURK-T是一个安全框架，用于在TLS 1.3中实现远程密钥的有限使用，通过分离加密服务与引擎，提升安全性。


<details>
  <summary>Details</summary>
Motivation: 在CDN等场景中，TLS凭证的共享可能导致安全风险，需要一种更安全且灵活的解决方案。

Method: 提出LURK-T框架，将TLS 1.3服务器端分为加密服务（CS）和引擎（E），CS在可信执行环境（TEE）中运行。

Result: LURK-T在性能上与传统TLS服务器相当，且支持多种部署方式，安全性通过形式化验证。

Conclusion: LURK-T为TLS 1.3提供了一种安全且高效的解决方案，适用于CDN等分布式场景。

Abstract: In many web applications, such as Content Delivery Networks (CDNs), TLS
credentials are shared, e.g., between the website's TLS origin server and the
CDN's edge servers, which can be distributed around the globe. To enhance the
security and trust for TLS 1.3 in such scenarios, we propose LURK-T, a provably
secure framework which allows for limited use of remote keys with added trust
in TLS 1.3. We efficiently decouple the server side of TLS 1.3 into a LURK-T
Crypto Service (CS) and a LURK-T Engine (E). CS executes all cryptographic
operations in a Trusted Execution Environment (TEE), upon E's requests. CS and
E together provide the whole TLS-server functionality. A major benefit of our
construction is that it is application agnostic; the LURK-T Crypto Service
could be collocated with the LURK-T Engine, or it could run on different
machines. Thus, our design allows for in situ attestation and protection of the
cryptographic side of the TLS server, as well as for all setups of CDNs over
TLS. To support such a generic decoupling, we provide a full Application
Programming Interface (API) for LURK-T. To this end, we implement our LURK-T
Crypto Service using Intel SGX and integrate it with OpenSSL. We also test
LURK-T's efficiency and show that, from a TLS-client's perspective, HTTPS
servers using LURK-T instead a traditional TLS-server have no noticeable
overhead when serving files greater than 1MB. In addition, we provide
cryptographic proofs and formal security verification using ProVerif.

</details>


### [23] [Organizational Adaptation to Generative AI in Cybersecurity: A Systematic Review](https://arxiv.org/abs/2506.12060)
*Christopher Nott*

Main category: cs.CR

TL;DR: 研究探讨了网络安全组织如何通过改进框架和混合操作流程适应生成式AI（GenAI）的整合，成功因素包括安全成熟度、法规要求和资源投入。


<details>
  <summary>Details</summary>
Motivation: 随着生成式AI的快速发展，网络安全组织需要调整其威胁建模框架和操作流程以应对新挑战。

Method: 采用系统性文献分析和比较案例研究方法，分析了2022至2025年的25项研究。

Result: 研究发现组织在威胁建模中从传统签名系统转向AI框架，并识别了三种主要适应模式。成熟组织表现更好，但面临隐私、偏见和对抗攻击等挑战。

Conclusion: 研究为网络安全专业人士提供了实施GenAI系统的实用建议，并推动了高风险环境中创新技术应用的理解。

Abstract: Cybersecurity organizations are adapting to GenAI integration through
modified frameworks and hybrid operational processes, with success influenced
by existing security maturity, regulatory requirements, and investments in
human capital and infrastructure. This qualitative research employs systematic
document analysis and comparative case study methodology to examine how
cybersecurity organizations adapt their threat modeling frameworks and
operational processes to address generative artificial intelligence
integration. Through examination of 25 studies from 2022 to 2025, the research
documents substantial transformation in organizational approaches to threat
modeling, moving from traditional signature-based systems toward frameworks
incorporating artificial intelligence capabilities. The research identifies
three primary adaptation patterns: Large Language Model integration for
security applications, GenAI frameworks for risk detection and response
automation, and AI/ML integration for threat hunting. Organizations with mature
security infrastructures, particularly in finance and critical infrastructure
sectors, demonstrate higher readiness through structured governance approaches,
dedicated AI teams, and robust incident response processes. Organizations
achieve successful GenAI integration when they maintain appropriate human
oversight of automated systems, address data quality concerns and
explainability requirements, and establish governance frameworks tailored to
their specific sectors. Organizations encounter ongoing difficulties with
privacy protection, bias reduction, personnel training, and defending against
adversarial attacks. This work advances understanding of how organizations
adopt innovative technologies in high-stakes environments and offers actionable
insights for cybersecurity professionals implementing GenAI systems.

</details>


### [24] [Risks & Benefits of LLMs & GenAI for Platform Integrity, Healthcare Diagnostics, Cybersecurity, Privacy & AI Safety: A Comprehensive Survey, Roadmap & Implementation Blueprint](https://arxiv.org/abs/2506.12088)
*Kiarash Ahi*

Main category: cs.CR

TL;DR: 论文分析了大型语言模型（LLM）和生成式AI（GenAI）在数字平台中的双重作用：既是新威胁的来源，也是缓解威胁的关键工具。研究揭示了LLM辅助的恶意软件、AI生成的虚假评论和诈骗等问题的快速增长趋势，并提出了应对策略。


<details>
  <summary>Details</summary>
Motivation: 随着LLM和GenAI技术的广泛应用，其在网络安全、隐私和平台完整性方面带来的挑战日益突出。研究旨在揭示这些威胁的严重性，并提出有效的解决方案。

Method: 通过分析455篇参考文献，论文对LLM和GenAI的风险进行了全面调查，并提出了一个战略路线图和操作蓝图，包括政策审计、欺诈检测和合规自动化等技术。

Result: 研究发现，LLM辅助的恶意软件、虚假评论和诈骗等问题呈指数级增长，同时平台也在部署基于AI的防御措施。

Conclusion: 论文强调了LLM和GenAI的双重性，提出了增强平台完整性的技术框架和最佳实践，为数字平台的可扩展信任和安全提供了路径。

Abstract: Large Language Models (LLMs) and generative AI (GenAI) systems such as
ChatGPT, Claude, Gemini, LLaMA, and Copilot, developed by OpenAI, Anthropic,
Google, Meta, and Microsoft are reshaping digital platforms and app ecosystems
while introducing key challenges in cybersecurity, privacy, and platform
integrity. Our analysis shows alarming trends: LLM-assisted malware is
projected to rise from 2% in 2021 to 50% by 2025; AI-generated Google reviews
grew from 1.2% in 2021 to 12.21% in 2023, with an expected 30% by 2025; AI scam
reports surged 456%; and misinformation sites increased over 1500%, with a
50-60% increase in deepfakes in 2024. Concurrently, as LLMs have facilitated
code development, mobile app submissions grew from 1.8 million in 2020 to 3.0
million in 2024, with 3.6 million expected by 2025. To address AI threats,
platforms from app stores like Google Play and Apple to developer hubs like
GitHub Copilot, and social platforms like TikTok and Facebook, to marketplaces
like Amazon are deploying AI and LLM-based defenses. This highlights the dual
nature of these technologies as both the source of new threats and the
essential tool for their mitigation. Integrating LLMs into clinical diagnostics
also raises concerns about accuracy, bias, and safety, needing strong
governance. Drawing on a comprehensive analysis of 455 references, this paper
presents a survey of LLM and GenAI risks. We propose a strategic roadmap and
operational blueprint integrating policy auditing (CCPA, GDPR), fraud
detection, and compliance automation, and an advanced LLM-DA stack with modular
components including multi LLM routing, agentic memory, and governance layers
to enhance platform integrity. We also provide actionable insights,
cross-functional best practices, and real-world case studies. These
contributions offer paths to scalable trust, safety, and responsible innovation
across digital platforms.

</details>


### [25] [Quantum Computing and Cybersecurity in Accounting and Finance: Current and the Future Challenges and Opportunities for Securing Accounting and Finance Systems](https://arxiv.org/abs/2506.12096)
*Huma Habib Shadan,Sardar Islam*

Main category: cs.CR

TL;DR: 该论文探讨了量子计算在会计网络安全中的应用，分析了现有系统的漏洞，并提出量子抗性算法和量子密钥分发（QKD）作为解决方案。


<details>
  <summary>Details</summary>
Motivation: 研究量子计算对会计和金融领域网络安全的潜在影响，特别是在数据保密和金融保护方面的机会与风险。

Method: 采用PSALSAR系统综述方法，分析量子计算如何提升加密技术。

Result: 量子计算能够显著增强加密技术，减少数据泄露和未经授权访问。

Conclusion: 量子抗性算法和QKD是未来会计和金融系统安全的必要技术。

Abstract: Quantum computing is revolutionising information systems and will have a
significant impact on accounting and finance, especially in the area of
cybersecurity. It presents both opportunities and risks in ensuring
confidentiality and protecting financial data. The purpose of this thesis is to
show the application of quantum technologies in accounting cybersecurity,
utilising quantum algorithms and QKD to overcome the limitations of classical
computing.
  The literature review reveals the vulnerabilities of the current accounting
cybersecurity to quantum attacks and the need for quantum-resistant
cryptographic mechanisms. It elaborates on the risks associated with
conventional encryption in the context of quantum capabilities. This study
contributes to the understanding of how quantum computing can revolutionise
accounting cybersecurity by enhancing quantum-resistant algorithms and
utilising quantum key distribution (QKD) in accounting.
  The study employs PSALSAR systematic review methodology to ensure rigour and
depth. The analysis shows that quantum computing enhances encryption techniques
to superior possibilities than classical ones. Using quantum technologies in
accounting minimises data breaches and unauthorised access. The study concludes
that quantum-resistant algorithms and quantum key distribution (QKD) are
necessary for securing the accounting and finance systems of the future.
  Keywords Quantum Computing, Cybersecurity, Accounting, Machine Learning,
Artificial Intelligence, Quantum Key Distribution, Operations Management

</details>


### [26] [LLM Embedding-based Attribution (LEA): Quantifying Source Contributions to Generative Model's Response for Vulnerability Analysis](https://arxiv.org/abs/2506.12100)
*Reza Fayyazi,Michael Zuzak,Shanchieh Jay Yang*

Main category: cs.CR

TL;DR: 论文提出了一种名为LEA的新方法，用于量化LLM生成响应中预训练知识与检索内容的影响比例，以提升网络安全分析的透明度和准确性。


<details>
  <summary>Details</summary>
Motivation: 网络安全漏洞日益复杂，LLM在处理未见漏洞时面临挑战，且部署中缺乏对生成响应来源的量化方法。

Method: 提出LLM Embedding-based Attribution (LEA)方法，通过分析LLM隐藏层的依赖关系，量化预训练知识与检索内容的影响。

Result: LEA成功应用于评估100个关键CVE的响应，验证了其在漏洞分析中的有效性，并揭示了LLM隐藏层的依赖变化规律。

Conclusion: LEA为安全分析师提供了一种审计LLM辅助工作流的方法，为RAG增强LLM在网络安全中的透明部署奠定了基础。

Abstract: Security vulnerabilities are rapidly increasing in frequency and complexity,
creating a shifting threat landscape that challenges cybersecurity defenses.
Large Language Models (LLMs) have been widely adopted for cybersecurity threat
analysis. When querying LLMs, dealing with new, unseen vulnerabilities is
particularly challenging as it lies outside LLMs' pre-trained distribution.
Retrieval-Augmented Generation (RAG) pipelines mitigate the problem by
injecting up-to-date authoritative sources into the model context, thus
reducing hallucinations and increasing the accuracy in responses. Meanwhile,
the deployment of LLMs in security-sensitive environments introduces challenges
around trust and safety. This raises a critical open question: How to quantify
or attribute the generated response to the retrieved context versus the model's
pre-trained knowledge? This work proposes LLM Embedding-based Attribution (LEA)
-- a novel, explainable metric to paint a clear picture on the 'percentage of
influence' the pre-trained knowledge vs. retrieved content has for each
generated response. We apply LEA to assess responses to 100 critical CVEs from
the past decade, verifying its effectiveness to quantify the insightfulness for
vulnerability analysis. Our development of LEA reveals a progression of
independency in hidden states of LLMs: heavy reliance on context in early
layers, which enables the derivation of LEA; increased independency in later
layers, which sheds light on why scale is essential for LLM's effectiveness.
This work provides security analysts a means to audit LLM-assisted workflows,
laying the groundwork for transparent, high-assurance deployments of
RAG-enhanced LLMs in cybersecurity operations.

</details>


### [27] [DRIFT: Dynamic Rule-Based Defense with Injection Isolation for Securing LLM Agents](https://arxiv.org/abs/2506.12104)
*Hao Li,Xiaogeng Liu,Hung-Chun Chiu,Dianqi Li,Ning Zhang,Chaowei Xiao*

Main category: cs.CR

TL;DR: DRIFT框架通过动态规则和隔离机制提升LLM代理系统的安全性，抵御提示注入攻击。


<details>
  <summary>Details</summary>
Motivation: LLM代理系统易受提示注入攻击，现有静态防御策略无法动态更新规则或隔离内存流。

Method: 提出DRIFT框架，包含安全规划器、动态验证器和注入隔离器，分别处理功能轨迹、权限检查和内存流隔离。

Result: 在AgentDojo基准测试中验证了DRIFT的安全性和实用性。

Conclusion: DRIFT在保障安全的同时保持高实用性，具有鲁棒性和适应性。

Abstract: Large Language Models (LLMs) are increasingly central to agentic systems due
to their strong reasoning and planning capabilities. By interacting with
external environments through predefined tools, these agents can carry out
complex user tasks. Nonetheless, this interaction also introduces the risk of
prompt injection attacks, where malicious inputs from external sources can
mislead the agent's behavior, potentially resulting in economic loss, privacy
leakage, or system compromise. System-level defenses have recently shown
promise by enforcing static or predefined policies, but they still face two key
challenges: the ability to dynamically update security rules and the need for
memory stream isolation. To address these challenges, we propose DRIFT, a
Dynamic Rule-based Isolation Framework for Trustworthy agentic systems, which
enforces both control- and data-level constraints. A Secure Planner first
constructs a minimal function trajectory and a JSON-schema-style parameter
checklist for each function node based on the user query. A Dynamic Validator
then monitors deviations from the original plan, assessing whether changes
comply with privilege limitations and the user's intent. Finally, an Injection
Isolator detects and masks any instructions that may conflict with the user
query from the memory stream to mitigate long-term risks. We empirically
validate the effectiveness of DRIFT on the AgentDojo benchmark, demonstrating
its strong security performance while maintaining high utility across diverse
models -- showcasing both its robustness and adaptability.

</details>


### [28] [A Lightweight IDS for Early APT Detection Using a Novel Feature Selection Method](https://arxiv.org/abs/2506.12108)
*Bassam Noori Shaker,Bahaa Al-Musawi,Mohammed Falih Hassan*

Main category: cs.CR

TL;DR: 提出了一种基于XGBoost和SHAP的特征选择方法，用于轻量级入侵检测系统，有效识别APT初始阶段的攻击。


<details>
  <summary>Details</summary>
Motivation: APT攻击隐蔽且危害大，早期检测至关重要。

Method: 结合XGBoost和SHAP方法，从77个特征中筛选出4个关键特征。

Result: 在SCVIC-APT-2021数据集上，系统达到97%精度、100%召回率和98% F1分数。

Conclusion: 该方法不仅提升APT早期检测能力，还增强了对APT行为的理解。

Abstract: An Advanced Persistent Threat (APT) is a multistage, highly sophisticated,
and covert form of cyber threat that gains unauthorized access to networks to
either steal valuable data or disrupt the targeted network. These threats often
remain undetected for extended periods, emphasizing the critical need for early
detection in networks to mitigate potential APT consequences. In this work, we
propose a feature selection method for developing a lightweight intrusion
detection system capable of effectively identifying APTs at the initial
compromise stage. Our approach leverages the XGBoost algorithm and Explainable
Artificial Intelligence (XAI), specifically utilizing the SHAP (SHapley
Additive exPlanations) method for identifying the most relevant features of the
initial compromise stage. The results of our proposed method showed the ability
to reduce the selected features of the SCVIC-APT-2021 dataset from 77 to just
four while maintaining consistent evaluation metrics for the suggested system.
The estimated metrics values are 97% precision, 100% recall, and a 98% F1
score. The proposed method not only aids in preventing successful APT
consequences but also enhances understanding of APT behavior at early stages.

</details>


### [29] [Semantic Preprocessing for LLM-based Malware Analysis](https://arxiv.org/abs/2506.12113)
*Benjamin Marais,Tony Quertier,Grégoire Barrue*

Main category: cs.CR

TL;DR: 论文提出了一种基于专家知识的预处理方法，用于改进恶意软件语义分析和结果可解释性，通过生成JSON报告并结合静态和行为分析特征，提升了AI模型的可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有恶意软件分析方法多依赖AI处理大数据，但缺乏专家视角，导致结果难以解释。

Method: 提出一种预处理方法，生成PE文件的JSON报告，整合静态和行为分析特征，并引入MITRE ATT&CK和MBC知识。

Result: 使用该方法训练的大型语言模型在复杂数据集上实现了0.94的加权平均F1分数。

Conclusion: 该方法显著提升了恶意软件分析的语义理解和AI模型的可解释性。

Abstract: In a context of malware analysis, numerous approaches rely on Artificial
Intelligence to handle a large volume of data. However, these techniques focus
on data view (images, sequences) and not on an expert's view. Noticing this
issue, we propose a preprocessing that focuses on expert knowledge to improve
malware semantic analysis and result interpretability. We propose a new
preprocessing method which creates JSON reports for Portable Executable files.
These reports gather features from both static and behavioral analysis, and
incorporate packer signature detection, MITRE ATT\&CK and Malware Behavior
Catalog (MBC) knowledge. The purpose of this preprocessing is to gather a
semantic representation of binary files, understandable by malware analysts,
and that can enhance AI models' explainability for malicious files analysis.
Using this preprocessing to train a Large Language Model for Malware
classification, we achieve a weighted-average F1-score of 0.94 on a complex
dataset, representative of market reality.

</details>


### [30] [Lessons for Cybersecurity from the American Public Health System](https://arxiv.org/abs/2506.12257)
*Adam Shostack,L. Jean Camp,Yi Ting Chua,Josiah Dykstra,Brian LaMacchia,Daniel Lopresti*

Main category: cs.CR

TL;DR: 美国需要建立国家机构和框架，系统性收集网络安全数据、衡量结果并协调政府和私营部门的响应，类似于公共卫生系统追踪和处理疾病爆发的方式。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏系统性的网络安全数据收集和协调机制，导致应对网络威胁的效率不足。

Method: 提出建立类似公共卫生系统的国家机构和框架，以系统性收集数据、衡量结果并协调响应。

Result: 通过系统性数据收集和协调机制，可以更高效地应对网络安全威胁。

Conclusion: 建立类似公共卫生系统的网络安全框架是提升国家网络安全能力的关键。

Abstract: The United States needs national institutions and frameworks to
systematically collect cybersecurity data, measure outcomes, and coordinate
responses across government and private sectors, similar to how public health
systems track and address disease outbreaks.

</details>


### [31] [InfoFlood: Jailbreaking Large Language Models with Information Overload](https://arxiv.org/abs/2506.12274)
*Advait Yadav,Haibo Jin,Man Luo,Jun Zhuang,Haohan Wang*

Main category: cs.CR

TL;DR: 本文提出了一种名为InfoFlood的新型jailbreak攻击方法，通过信息过载绕过大型语言模型的安全机制，无需添加前缀或后缀，攻击成功率显著高于现有方法。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）的安全机制存在漏洞，尤其是在面对信息过载时容易被绕过，导致生成有害内容。

Method: 提出InfoFlood攻击方法，通过语言转换和结构优化将恶意查询转化为信息过载的复杂查询，绕过安全机制。

Result: 在四种主流LLMs上验证，InfoFlood的攻击成功率比基线方法高3倍，且现有防御措施（如Moderation API）无法有效拦截。

Conclusion: 信息过载是一种新型漏洞，现有AI安全护栏对其无效，亟需新的防御策略。

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities across
various domains. However, their potential to generate harmful responses has
raised significant societal and regulatory concerns, especially when
manipulated by adversarial techniques known as "jailbreak" attacks. Existing
jailbreak methods typically involve appending carefully crafted prefixes or
suffixes to malicious prompts in order to bypass the built-in safety mechanisms
of these models.
  In this work, we identify a new vulnerability in which excessive linguistic
complexity can disrupt built-in safety mechanisms-without the need for any
added prefixes or suffixes-allowing attackers to elicit harmful outputs
directly. We refer to this phenomenon as Information Overload.
  To automatically exploit this vulnerability, we propose InfoFlood, a
jailbreak attack that transforms malicious queries into complex,
information-overloaded queries capable of bypassing built-in safety mechanisms.
Specifically, InfoFlood: (1) uses linguistic transformations to rephrase
malicious queries, (2) identifies the root cause of failure when an attempt is
unsuccessful, and (3) refines the prompt's linguistic structure to address the
failure while preserving its malicious intent.
  We empirically validate the effectiveness of InfoFlood on four widely used
LLMs-GPT-4o, GPT-3.5-turbo, Gemini 2.0, and LLaMA 3.1-by measuring their
jailbreak success rates. InfoFlood consistently outperforms baseline attacks,
achieving up to 3 times higher success rates across multiple jailbreak
benchmarks. Furthermore, we demonstrate that commonly adopted post-processing
defenses, including OpenAI's Moderation API, Perspective API, and SmoothLLM,
fail to mitigate these attacks. This highlights a critical weakness in
traditional AI safety guardrails when confronted with information
overload-based jailbreaks.

</details>


### [32] [QGuard:Question-based Zero-shot Guard for Multi-modal LLM Safety](https://arxiv.org/abs/2506.12299)
*Taegyeong Lee,Jeonghwa Yoo,Hyoungseo Cho,Soo Yong Kim,Yunho Maeng*

Main category: cs.CR

TL;DR: QGuard是一种简单有效的安全防护方法，通过问题提示阻止有害提示，无需微调即可抵御多模态攻击。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLM）的进步增加了恶意用户利用有害提示进行攻击的风险，现有防护方法仍面临挑战。

Method: 提出QGuard，利用问题提示在零样本情况下阻止有害提示，支持文本和多模态攻击防护，并通过多样化问题保持鲁棒性。

Result: 实验表明QGuard在文本和多模态有害数据集上表现优异，支持白盒分析用户输入。

Conclusion: QGuard为实际LLM服务提供了缓解有害提示安全风险的实用方法。

Abstract: The recent advancements in Large Language Models(LLMs) have had a significant
impact on a wide range of fields, from general domains to specialized areas.
However, these advancements have also significantly increased the potential for
malicious users to exploit harmful and jailbreak prompts for malicious attacks.
Although there have been many efforts to prevent harmful prompts and jailbreak
prompts, protecting LLMs from such malicious attacks remains an important and
challenging task. In this paper, we propose QGuard, a simple yet effective
safety guard method, that utilizes question prompting to block harmful prompts
in a zero-shot manner. Our method can defend LLMs not only from text-based
harmful prompts but also from multi-modal harmful prompt attacks. Moreover, by
diversifying and modifying guard questions, our approach remains robust against
the latest harmful prompts without fine-tuning. Experimental results show that
our model performs competitively on both text-only and multi-modal harmful
datasets. Additionally, by providing an analysis of question prompting, we
enable a white-box analysis of user inputs. We believe our method provides
valuable insights for real-world LLM services in mitigating security risks
associated with harmful prompts.

</details>


### [33] [Information-theoretic Estimation of the Risk of Privacy Leaks](https://arxiv.org/abs/2506.12328)
*Kenneth Odoh*

Main category: cs.CR

TL;DR: 本文提出了一种基于信息论度量的隐私泄露评估方法，扩展了隐私保护转换中的相关性分析，并提出了一种计算高效的混合度量。


<details>
  <summary>Details</summary>
Motivation: 现有研究表明数据项间的依赖关系可能导致隐私泄露，本文旨在更全面地捕捉这些依赖关系，并量化隐私风险。

Method: 利用最大信息系数（MIC）等指标，结合熵、互信息和匿名度，扩展了隐私泄露估计模型。

Result: 提出了一种混合度量，能够高效识别数据属性间的相关性，作为隐私泄露漏洞的代理指标。

Conclusion: 该混合度量提供了计算高效的隐私损失最坏情况评估，有助于预防隐私泄露。

Abstract: Recent work~\cite{Liu2016} has shown that dependencies between items in a
dataset can lead to privacy leaks. We extend this concept to privacy-preserving
transformations, considering a broader set of dependencies captured by
correlation metrics. Specifically, we measure the correlation between the
original data and their noisy responses from a randomizer as an indicator of
potential privacy breaches. This paper aims to leverage information-theoretic
measures, such as the Maximal Information Coefficient (MIC), to estimate
privacy leaks and derive novel, computationally efficient privacy leak
estimators. We extend the $\rho_1$-to-$\rho_2$
formulation~\cite{Evfimievski2003} to incorporate entropy, mutual information,
and the degree of anonymity for a more comprehensive measure of privacy risk.
Our proposed hybrid metric can identify correlation dependencies between
attributes in the dataset, serving as a proxy for privacy leak vulnerabilities.
This metric provides a computationally efficient worst-case measure of privacy
loss, utilizing the inherent characteristics of the data to prevent privacy
breaches.

</details>


### [34] [Restoring Gaussian Blurred Face Images for Deanonymization Attacks](https://arxiv.org/abs/2506.12344)
*Haoyu Zhai,Shuo Wang,Pirouz Naghavi,Qingying Hao,Gang Wang*

Main category: cs.CR

TL;DR: 论文提出了一种名为Revelio的去模糊方法，通过生成模型的记忆效应和高斯模糊的逆函数恢复人脸，尤其在高度模糊情况下表现优异。


<details>
  <summary>Details</summary>
Motivation: 探索高斯模糊处理后的人脸是否能被有效恢复并用于重新识别，尤其是在高模糊设置下。

Method: 结合条件扩散模型进行初步人脸恢复，再通过身份检索模型提升保真度。

Result: Revelio在高度模糊设置下表现优异，重新识别准确率达95.9%，优于现有方法。

Conclusion: 高斯模糊不应用于人脸匿名化，同时展示了方法的鲁棒性并测试了初步对策和自适应攻击。

Abstract: Gaussian blur is widely used to blur human faces in sensitive photos before
the photos are posted on the Internet. However, it is unclear to what extent
the blurred faces can be restored and used to re-identify the person,
especially under a high-blurring setting. In this paper, we explore this
question by developing a deblurring method called Revelio. The key intuition is
to leverage a generative model's memorization effect and approximate the
inverse function of Gaussian blur for face restoration. Compared with existing
methods, we design the deblurring process to be identity-preserving. It uses a
conditional Diffusion model for preliminary face restoration and then uses an
identity retrieval model to retrieve related images to further enhance
fidelity. We evaluate Revelio with large public face image datasets and show
that it can effectively restore blurred faces, especially under a high-blurring
setting. It has a re-identification accuracy of 95.9%, outperforming existing
solutions. The result suggests that Gaussian blur should not be used for face
anonymization purposes. We also demonstrate the robustness of this method
against mismatched Gaussian kernel sizes and functions, and test preliminary
countermeasures and adaptive attacks to inspire future work.

</details>


### [35] [InverTune: Removing Backdoors from Multimodal Contrastive Learning Models via Trigger Inversion and Activation Tuning](https://arxiv.org/abs/2506.12411)
*Mengyuan Sun,Yu Li,Yuchen Liu,Bo Du,Yunjie Ge*

Main category: cs.CR

TL;DR: InverTune是一种针对多模态模型的后门防御框架，无需攻击者知识或中毒数据集，通过对抗模拟、梯度反演和聚类微调有效消除后门。


<details>
  <summary>Details</summary>
Motivation: 多模态对比学习模型（如CLIP）易受后门攻击，现有防御方法因假设过强或数据需求过高而不实用。

Method: InverTune通过对抗模拟暴露攻击特征，梯度反演重建潜在触发器，聚类微调消除后门功能。

Result: 实验显示InverTune将攻击成功率降低97.87%，仅损失3.07%的干净数据准确率。

Conclusion: InverTune为多模态系统安全提供了新范式，在不牺牲性能的情况下提升安全性。

Abstract: Multimodal contrastive learning models like CLIP have demonstrated remarkable
vision-language alignment capabilities, yet their vulnerability to backdoor
attacks poses critical security risks. Attackers can implant latent triggers
that persist through downstream tasks, enabling malicious control of model
behavior upon trigger presentation. Despite great success in recent defense
mechanisms, they remain impractical due to strong assumptions about attacker
knowledge or excessive clean data requirements. In this paper, we introduce
InverTune, the first backdoor defense framework for multimodal models under
minimal attacker assumptions, requiring neither prior knowledge of attack
targets nor access to the poisoned dataset. Unlike existing defense methods
that rely on the same dataset used in the poisoning stage, InverTune
effectively identifies and removes backdoor artifacts through three key
components, achieving robust protection against backdoor attacks. Specifically,
InverTune first exposes attack signatures through adversarial simulation,
probabilistically identifying the target label by analyzing model response
patterns. Building on this, we develop a gradient inversion technique to
reconstruct latent triggers through activation pattern analysis. Finally, a
clustering-guided fine-tuning strategy is employed to erase the backdoor
function with only a small amount of arbitrary clean data, while preserving the
original model capabilities. Experimental results show that InverTune reduces
the average attack success rate (ASR) by 97.87% against the state-of-the-art
(SOTA) attacks while limiting clean accuracy (CA) degradation to just 3.07%.
This work establishes a new paradigm for securing multimodal systems, advancing
security in foundation model deployment without compromising performance.

</details>


### [36] [Pushing the Limits of Safety: A Technical Report on the ATLAS Challenge 2025](https://arxiv.org/abs/2506.12430)
*Zonghao Ying,Siyang Wu,Run Hao,Peng Ying,Shixuan Sun,Pengyu Chen,Junze Chen,Hao Du,Kaiwen Shen,Shangkun Wu,Jiwei Wei,Shiyuan He,Yang Yang,Xiaohai Xu,Ke Ma,Qianqian Xu,Qingming Huang,Shi Lin,Xun Wang,Changting Lin,Meng Han,Yilei Jiang,Siqi Lai,Yaozhi Zheng,Yifei Song,Xiangyu Yue,Zonglei Jing,Tianyuan Zhang,Zhilei Zhu,Aishan Liu,Jiakai Wang,Siyuan Liang,Xianglong Kong,Hainan Li,Junjie Mu,Haotong Qin,Yue Yu,Lei Chen,Felix Juefei-Xu,Qing Guo,Xinyun Chen,Yew Soon Ong,Xianglong Liu,Dawn Song,Alan Yuille,Philip Torr,Dacheng Tao*

Main category: cs.CR

TL;DR: ATLAS 2025挑战赛通过对抗性图像-文本攻击评估多模态大语言模型（MLLM）的安全性，发现其仍面临越狱攻击等威胁，为开发更强防御机制提供了指导。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型（MLLM）在广泛应用中表现出安全性问题，尤其是越狱攻击导致的恶意输出，亟需系统性评估和改进。

Method: 组织ATLAS 2025挑战赛，86个团队分白盒和黑盒两阶段测试MLLM的对抗性图像-文本攻击漏洞。

Result: 挑战赛揭示了MLLM安全性的持续挑战，并为防御机制开发提供了重要指导。

Conclusion: ATLAS 2025为MLLM安全性评估设立了新基准，推动了更安全的多模态AI系统发展。

Abstract: Multimodal Large Language Models (MLLMs) have enabled transformative
advancements across diverse applications but remain susceptible to safety
threats, especially jailbreak attacks that induce harmful outputs. To
systematically evaluate and improve their safety, we organized the Adversarial
Testing & Large-model Alignment Safety Grand Challenge (ATLAS) 2025}. This
technical report presents findings from the competition, which involved 86
teams testing MLLM vulnerabilities via adversarial image-text attacks in two
phases: white-box and black-box evaluations. The competition results highlight
ongoing challenges in securing MLLMs and provide valuable guidance for
developing stronger defense mechanisms. The challenge establishes new
benchmarks for MLLM safety evaluation and lays groundwork for advancing safer
multimodal AI systems. The code and data for this challenge are openly
available at https://github.com/NY1024/ATLAS_Challenge_2025.

</details>


### [37] [Towards Safety and Security Testing of Cyberphysical Power Systems by Shape Validation](https://arxiv.org/abs/2506.12466)
*Alexander Geiger,Immanuel Hacker,Ömer Sen,Andreas Ulbig*

Main category: cs.CR

TL;DR: 论文提出了一种基于语义网技术的声明式方法，用于描述网络物理电力系统并自动评估安全和安全控制。


<details>
  <summary>Details</summary>
Motivation: 随着网络物理电力系统的复杂性增加，恶意行为者的攻击面扩大，配置错误导致的风险上升，需要一种更高效的方法来应对这些挑战。

Method: 利用语义网技术（如本体论、SPARQL规则和SHACL形状约束）建模基础设施，并通过规则和约束验证数据、安全和安全控制。

Result: 通过两个用例展示了该方法的有效性，并说明了其社区驱动的进一步发展潜力。

Conclusion: 提出的方法能够有效应对网络物理电力系统的安全和安全挑战，并具有扩展性和社区驱动的优势。

Abstract: The increasing complexity of cyberphysical power systems leads to larger
attack surfaces to be exploited by malicious actors and a higher risk of faults
through misconfiguration. We propose to meet those risks with a declarative
approach to describe cyberphysical power systems and to automatically evaluate
security and safety controls. We leverage Semantic Web technologies as a
well-standardized framework, providing languages to specify ontologies, rules
and shape constraints. We model infrastructure through an ontology which
combines external ontologies, architecture and data models for sufficient
expressivity and interoperability with external systems. The ontology can
enrich itself through rules defined in SPARQL, allowing for the inference of
knowledge that is not explicitly stated. Through the evaluation of SHACL shape
constraints we can then validate the data and verify safety and security
constraints. We demonstrate this concept with two use cases and illustrate how
this solution can be developed further in a community-driven fashion.

</details>


### [38] [Exploiting AI for Attacks: On the Interplay between Adversarial AI and Offensive AI](https://arxiv.org/abs/2506.12519)
*Saskia Laura Schröer,Luca Pajola,Alberto Castagnaro,Giovanni Apruzzese,Mauro Conti*

Main category: cs.CR

TL;DR: 论文探讨了AI在安全领域的双重威胁：作为攻击目标的AI（对抗性AI）和作为攻击工具的AI（进攻性AI），并分析了它们之间的相互作用。


<details>
  <summary>Details</summary>
Motivation: 随着AI技术的成熟和广泛应用，其潜在的安全威胁日益凸显，尤其是对抗性AI和进攻性AI的相互作用，这对组织安全提出了新的挑战。

Method: 通过清晰解释和分类，论文分析了对抗性AI和进攻性AI的定义、特点及其相互影响。

Result: 研究揭示了这两种AI威胁的复杂性和潜在风险，为理解其相互作用提供了基础。

Conclusion: 论文强调了对AI安全威胁的深入研究和应对的必要性，以保障AI技术的健康发展。

Abstract: As Artificial Intelligence (AI) continues to evolve, it has transitioned from
a research-focused discipline to a widely adopted technology, enabling
intelligent solutions across various sectors. In security, AI's role in
strengthening organizational resilience has been studied for over two decades.
While much attention has focused on AI's constructive applications, the
increasing maturity and integration of AI have also exposed its darker
potentials. This article explores two emerging AI-related threats and the
interplay between them: AI as a target of attacks (`Adversarial AI') and AI as
a means to launch attacks on any target (`Offensive AI') -- potentially even on
another AI. By cutting through the confusion and explaining these threats in
plain terms, we introduce the complex and often misunderstood interplay between
Adversarial AI and Offensive AI, offering a clear and accessible introduction
to the challenges posed by these threats.

</details>


### [39] [When Forgetting Triggers Backdoors: A Clean Unlearning Attack](https://arxiv.org/abs/2506.12522)
*Marco Arazzi,Antonino Nocera,Vinod P*

Main category: cs.CR

TL;DR: 论文提出了一种新型的干净后门攻击，利用模型学习和遗忘请求，攻击隐蔽且难以检测。


<details>
  <summary>Details</summary>
Motivation: 当前遗忘机制存在漏洞，容易被利用进行隐蔽攻击，需要更鲁棒的防御方法。

Method: 在模型学习阶段注入弱分布恶意信号，随后通过选择性遗忘非污染样本激活攻击。

Result: 攻击强大且隐蔽，现有防御难以检测或缓解。

Conclusion: 揭示了当前遗忘机制的关键漏洞，强调了加强防御的必要性。

Abstract: Machine unlearning has emerged as a key component in ensuring ``Right to be
Forgotten'', enabling the removal of specific data points from trained models.
However, even when the unlearning is performed without poisoning the forget-set
(clean unlearning), it can be exploited for stealthy attacks that existing
defenses struggle to detect. In this paper, we propose a novel {\em clean}
backdoor attack that exploits both the model learning phase and the subsequent
unlearning requests. Unlike traditional backdoor methods, during the first
phase, our approach injects a weak, distributed malicious signal across
multiple classes. The real attack is then activated and amplified by
selectively unlearning {\em non-poisoned} samples. This strategy results in a
powerful and stealthy novel attack that is hard to detect or mitigate,
highlighting critical vulnerabilities in current unlearning mechanisms and
highlighting the need for more robust defenses.

</details>


### [40] [Privacy-preserving and reward-based mechanisms of proof of engagement](https://arxiv.org/abs/2506.12523)
*Matteo Marco Montanari,Alessandro Aldini*

Main category: cs.CR

TL;DR: 研究扩展了Proof-of-Attendance (PoA)机制，提出Proof-of-Engagement (PoE)，探讨了去中心化与中心化技术方案，关注隐私、时空范围、证明可转移性及激励机制。


<details>
  <summary>Details</summary>
Motivation: 扩展PoA机制至更广泛的数字活动参与证明（PoE），以验证用户参与特定活动。

Method: 探索去中心化账本技术（DLTs）和中心化系统技术，分析隐私、时空范围、证明可转移性及激励机制。

Result: 提出了PoE概念，并比较了不同技术方案的优缺点。

Conclusion: PoE机制在数字活动参与证明中具有潜力，需进一步研究隐私与激励的平衡。

Abstract: Proof-of-Attendance (PoA) mechanisms are typically employed to demonstrate a
specific user's participation in an event, whether virtual or in-person. The
goal of this study is to extend such mechanisms to broader contexts where the
user wishes to digitally demonstrate her involvement in a specific activity
(Proof-of-Engagement, PoE). This work explores different solutions, including
DLTs as well as established technologies based on centralized systems. The main
aspects we consider include the level of privacy guaranteed to users, the scope
of PoA/PoE (both temporal and spatial), the transferability of the proof, and
the integration with incentive mechanisms.

</details>


### [41] [MEraser: An Effective Fingerprint Erasure Approach for Large Language Models](https://arxiv.org/abs/2506.12551)
*Jingxuan Zhang,Zhenhua Xu,Rui Hu,Wenpeng Xing,Xuhong Zhang,Meng Han*

Main category: cs.CR

TL;DR: MEraser是一种新方法，能有效移除LLMs中的后门指纹，同时保持模型性能。


<details>
  <summary>Details</summary>
Motivation: 解决大型语言模型（LLMs）中后门指纹移除的问题，揭示当前指纹技术的漏洞。

Method: 采用两阶段微调策略，利用不匹配和干净数据集进行训练。

Result: MEraser能完全移除指纹，且仅需少于1000个样本的训练数据。

Conclusion: 该方法为指纹移除提供了实用方案，并为未来开发更稳健的模型保护方法奠定了基础。

Abstract: Large Language Models (LLMs) have become increasingly prevalent across
various sectors, raising critical concerns about model ownership and
intellectual property protection. Although backdoor-based fingerprinting has
emerged as a promising solution for model authentication, effective attacks for
removing these fingerprints remain largely unexplored. Therefore, we present
Mismatched Eraser (MEraser), a novel method for effectively removing
backdoor-based fingerprints from LLMs while maintaining model performance. Our
approach leverages a two-phase fine-tuning strategy utilizing carefully
constructed mismatched and clean datasets. Through extensive evaluation across
multiple LLM architectures and fingerprinting methods, we demonstrate that
MEraser achieves complete fingerprinting removal while maintaining model
performance with minimal training data of fewer than 1,000 samples.
Furthermore, we introduce a transferable erasure mechanism that enables
effective fingerprinting removal across different models without repeated
training. In conclusion, our approach provides a practical solution for
fingerprinting removal in LLMs, reveals critical vulnerabilities in current
fingerprinting techniques, and establishes comprehensive evaluation benchmarks
for developing more resilient model protection methods in the future.

</details>


### [42] [GNSS Spoofing Detection Based on Opportunistic Position Information](https://arxiv.org/abs/2506.12580)
*Wenjie Liu,Panos Papadimitratos*

Main category: cs.CR

TL;DR: 论文提出了一种基于概率框架的GNSS攻击检测方案（PADS），利用回归和不确定性分析，结合网络位置和惯性传感器数据，显著提高了检测准确率。


<details>
  <summary>Details</summary>
Motivation: 民用GNSS信号缺乏保护，容易受到欺骗攻击。现有方案多依赖定制平台，而本文旨在利用消费级设备（如智能手机）的现成数据检测GNSS攻击。

Method: PADS通过多项式拟合的加权均方误差回归优化位置估计，结合高斯过程建模不确定性，并利用异常检测器分析融合的测试统计量。

Result: PADS在低误报率下实现了比基线方法高3倍的真实阳性率。

Conclusion: PADS为消费级设备提供了一种高效的GNSS攻击检测方案，具有实际部署潜力。

Abstract: The limited or no protection for civilian Global Navigation Satellite System
(GNSS) signals makes spoofing attacks relatively easy. With modern mobile
devices often featuring network interfaces, state-of-the-art signals of
opportunity (SOP) schemes can provide accurate network positions in replacement
of GNSS. The use of onboard inertial sensors can also assist in the absence of
GNSS, possibly in the presence of jammers. The combination of SOP and inertial
sensors has received limited attention, yet it shows strong results on fully
custom-built platforms. We do not seek to improve such special-purpose schemes.
Rather, we focus on countering GNSS attacks, notably detecting them, with
emphasis on deployment with consumer-grade platforms, notably smartphones, that
provide off-the-shelf opportunistic information (i.e., network position and
inertial sensor data). Our Position-based Attack Detection Scheme (PADS) is a
probabilistic framework that uses regression and uncertainty analysis for
positions. The regression optimization problem is a weighted mean square error
of polynomial fitting, with constraints that the fitted positions satisfy the
device velocity and acceleration. Then, uncertainty is modeled by a Gaussian
process, which provides more flexibility to analyze how sure or unsure we are
about position estimations. In the detection process, we combine all
uncertainty information with the position estimations into a fused test
statistic, which is the input utilized by an anomaly detector based on outlier
ensembles. The evaluation shows that the PADS outperforms a set of baseline
methods that rely on SOP or inertial sensor-based or statistical tests,
achieving up to 3 times the true positive rate at a low false positive rate.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [43] [Modeling Earth-Scale Human-Like Societies with One Billion Agents](https://arxiv.org/abs/2506.12078)
*Haoxiang Guan,Jiyan He,Liyang Fan,Zhenzhen Ren,Shaobin He,Xin Yu,Yuan Chen,Shuxin Zheng,Tie-Yan Liu,Zhen Liu*

Main category: cs.MA

TL;DR: Light Society 是一个基于代理的模拟框架，利用大语言模型（LLMs）高效模拟大规模人类社会，支持超过十亿代理的仿真，展示了高保真度和效率。


<details>
  <summary>Details</summary>
Motivation: 传统代理模型（ABMs）无法捕捉人类行为的复杂性，而 LLMs 提供了新机会，但面临扩展挑战。Light Society 旨在解决这些问题。

Method: Light Society 将社会过程形式化为代理和环境状态的结构化转换，由一组 LLM 驱动的模拟操作控制，并通过事件队列执行。

Result: 大规模信任游戏和意见传播模拟（达十亿代理）展示了高保真度和效率，并揭示了更大规模模拟能产生更稳定和现实的涌现行为。

Conclusion: Light Society 在模拟人类社会方面具有高效率和扩展性，为复杂社会行为研究提供了新工具。

Abstract: Understanding how complex societal behaviors emerge from individual cognition
and interactions requires both high-fidelity modeling of human behavior and
large-scale simulations. Traditional agent-based models (ABMs) have been
employed to study these dynamics for decades, but are constrained by simplified
agent behaviors that fail to capture human complexity. Recent advances in large
language models (LLMs) offer new opportunities by enabling agents to exhibit
sophisticated social behaviors that go beyond rule-based logic, yet face
significant scaling challenges. Here we present Light Society, an agent-based
simulation framework that advances both fronts, efficiently modeling human-like
societies at planetary scale powered by LLMs. Light Society formalizes social
processes as structured transitions of agent and environment states, governed
by a set of LLM-powered simulation operations, and executed through an event
queue. This modular design supports both independent and joint component
optimization, supporting efficient simulation of societies with over one
billion agents. Large-scale simulations of trust games and opinion
propagation--spanning up to one billion agents--demonstrate Light Society's
high fidelity and efficiency in modeling social trust and information
diffusion, while revealing scaling laws whereby larger simulations yield more
stable and realistic emergent behaviors.

</details>


### [44] [IndoorWorld: Integrating Physical Task Solving and Social Simulation in A Heterogeneous Multi-Agent Environment](https://arxiv.org/abs/2506.12331)
*Dekun Wu,Frederik Brudy,Bang Liu,Yi Wang*

Main category: cs.MA

TL;DR: IndoorWorld是一个结合物理和社交动态的多智能体环境，旨在解决现有环境在LLM智能体研究中的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有LLM智能体研究环境要么过于简化物理任务解决，要么缺乏社交行为的物理基础，无法全面模拟智能体个性和社交动态。

Method: 提出IndoorWorld，通过紧密整合物理和社交动态，为LLM智能体提供新挑战，如通过社交动态影响物理环境。

Result: 实验展示了多智能体协作、资源竞争和空间布局对行为的影响，验证了IndoorWorld的潜力。

Conclusion: IndoorWorld为基于LLM的建筑居住者模拟提供了新可能性，尤其在建筑设计领域。

Abstract: Virtual environments are essential to AI agent research. Existing
environments for LLM agent research typically focus on either physical task
solving or social simulation, with the former oversimplifying agent
individuality and social dynamics, and the latter lacking physical grounding of
social behaviors. We introduce IndoorWorld, a heterogeneous multi-agent
environment that tightly integrates physical and social dynamics. By
introducing novel challenges for LLM-driven agents in orchestrating social
dynamics to influence physical environments and anchoring social interactions
within world states, IndoorWorld opens up possibilities of LLM-based building
occupant simulation for architectural design. We demonstrate the potential with
a series of experiments within an office setting to examine the impact of
multi-agent collaboration, resource competition, and spatial layout on agent
behavior.

</details>


### [45] [Trust-MARL: Trust-Based Multi-Agent Reinforcement Learning Framework for Cooperative On-Ramp Merging Control in Heterogeneous Traffic Flow](https://arxiv.org/abs/2506.12600)
*Jie Pan,Tianyi Wang,Christian Claudel,Jing Shi*

Main category: cs.MA

TL;DR: 论文提出了一种基于信任的多智能体强化学习框架（Trust-MARL），用于解决异构交通环境中高速公路匝道合并的协作问题，提升安全性和效率。


<details>
  <summary>Details</summary>
Motivation: 人类驾驶行为的不确定性常导致交通瓶颈（如匝道合并区域）的流量中断，影响系统性能。

Method: Trust-MARL结合宏观层面的群体协调和微观层面的动态信任机制，通过信任触发的博弈论决策模块优化车辆协作策略。

Result: 实验表明，Trust-MARL在不同CAV渗透率和交通密度下显著提升了安全性、效率、舒适性和适应性。

Conclusion: Trust-MARL为异构交通环境中的协作问题提供了有效解决方案，具有实际应用潜力。

Abstract: Intelligent transportation systems require connected and automated vehicles
(CAVs) to conduct safe and efficient cooperation with human-driven vehicles
(HVs) in complex real-world traffic environments. However, the inherent
unpredictability of human behaviour, especially at bottlenecks such as highway
on-ramp merging areas, often disrupts traffic flow and compromises system
performance. To address the challenge of cooperative on-ramp merging in
heterogeneous traffic environments, this study proposes a trust-based
multi-agent reinforcement learning (Trust-MARL) framework. At the macro level,
Trust-MARL enhances global traffic efficiency by leveraging inter-agent trust
to improve bottleneck throughput and mitigate traffic shockwave through
emergent group-level coordination. At the micro level, a dynamic trust
mechanism is designed to enable CAVs to adjust their cooperative strategies in
response to real-time behaviors and historical interactions with both HVs and
other CAVs. Furthermore, a trust-triggered game-theoretic decision-making
module is integrated to guide each CAV in adapting its cooperation factor and
executing context-aware lane-changing decisions under safety, comfort, and
efficiency constraints. An extensive set of ablation studies and comparative
experiments validates the effectiveness of the proposed Trust-MARL approach,
demonstrating significant improvements in safety, efficiency, comfort, and
adaptability across varying CAV penetration rates and traffic densities.

</details>


### [46] [Towards the Autonomous Optimization of Urban Logistics: Training Generative AI with Scientific Tools via Agentic Digital Twins and Model Context Protocol](https://arxiv.org/abs/2506.13068)
*Haowen Xu,Yulin Sun,Jose Tupayachi,Olufemi Omitaomu,Sisi Zlatanov,Xueping Li*

Main category: cs.MA

TL;DR: 本文提出了一种基于模型上下文协议（MCP）的多智能体系统架构，用于优化城市货运物流，通过生成式AI智能体与领域专用引擎的集成，实现自主的仿真优化。


<details>
  <summary>Details</summary>
Motivation: 传统方法依赖人工协调仿真工具和优化求解器，效率低且难以扩展，因此需要一种更高效、自动化的解决方案。

Method: 采用MCP协议协调多智能体协作，集成生成式AI智能体与Gurobi（优化）和AnyLogic（仿真）等工具，构建生成式数字孪生系统。

Result: 通过货运脱碳案例研究，展示了系统的模块化、互操作性和适应性，将数字孪生从静态可视化提升为自主决策系统。

Conclusion: 该框架通过上下文感知的生成式智能体自动协作操作科学工具，为交通规划和智慧城市管理提供了更智能、动态的决策支持。

Abstract: Optimizing urban freight logistics is critical for developing sustainable,
low-carbon cities. Traditional methods often rely on manual coordination of
simulation tools, optimization solvers, and expert-driven workflows, limiting
their efficiency and scalability. This paper presents an agentic system
architecture that leverages the model context protocol (MCP) to orchestrate
multi-agent collaboration among scientific tools for autonomous,
simulation-informed optimization in urban logistics. The system integrates
generative AI agents with domain-specific engines - such as Gurobi for
optimization and AnyLogic for agent-based simulation - forming a generative
digital twin capable of reasoning, planning, and acting across multimodal
freight networks. By incorporating integrated chatbots, retrieval-augmented
generation, and structured memory, the framework enables agents to interpret
user intent from natural language conversations, retrieve relevant datasets and
models, coordinate solvers and simulators, and execute complex workflows. We
demonstrate this approach through a freight decarbonization case study,
showcasing how MCP enables modular, interoperable, and adaptive agent behavior
across diverse toolchains. The results reveal that our system transforms
digital twins from static visualizations into autonomous, decision-capable
systems, advancing the frontiers of urban operations research. By enabling
context-aware, generative agents to operate scientific tools automatically and
collaboratively, this framework supports more intelligent, accessible, and
dynamic decision-making in transportation planning and smart city management.

</details>


### [47] [Mobility to Campus -- a Framework to Evaluate and Compare Different Mobility Modes](https://arxiv.org/abs/2506.13574)
*Helena Fehler,Marco Pruckner,Marie Schmidt*

Main category: cs.MA

TL;DR: 研究评估了拼车和共享乘车在减少德国农村地区通勤碳排放的潜力，以维尔茨堡大学学生为例。


<details>
  <summary>Details</summary>
Motivation: 德国交通占CO2排放的20%，农村地区通勤多依赖私家车，拼车和共享乘车可能减少排放。

Method: 结合学生家庭地址和校园访问时间数据，创建需求场景，比较拼车、共享乘车与单独驾车。

Result: 拼车能显著减少排放，效果取决于学生参与度和步行意愿；共享乘车的效果取决于车辆能效。

Conclusion: 拼车是减少通勤碳排放的有效方式，共享乘车需更高能效车辆支持。

Abstract: The transport sector accounts for about 20% of German CO2 emissions, with
commuter traffic contributing a significant part. Particularly in rural areas,
where public transport is inconvenient to use, private cars are a common choice
for commuting and most commuters travel alone in their cars. Consolidation of
some of these trips has the potential to decrease CO2 emissions and could be
achieved, e.g., by offering ridesharing (commuters with similar
origin-destination pairs share a car) or ridepooling (commuters are picked up
by shuttle services). In this study, we present a framework to assess the
potential of introducing new mobility modes like ridesharing and ridepooling
for commuting towards several locations in close vicinity to each other.
  We test our framework on the case of student mobility to the University of
W\"urzburg, a university with several campus locations and a big and rather
rural catchment area, where existing public transport options are inconvenient
and many students commute by car. We combine data on student home addresses and
campus visitation times to create demand scenarios. In our case study, we
compare the mobility modes of ridesharing and ridepooling to the base case,
where students travel by car on their own. We find that ridesharing has the
potential to greatly reduce emissions, depending on the percentage of students
willing to use the service and their willingness to walk to the departure
location. The benefit of ridepooling is less clear, materializing only if the
shuttle vehicles are more energy efficient than the student cars.

</details>
