<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 16]
- [cs.CR](#cs.CR) [Total: 30]
- [cs.AI](#cs.AI) [Total: 38]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [E-Test: E'er-Improving Test Suites](https://arxiv.org/abs/2510.19860)
*Ketai Qiu,Luca Di Grazia,Leonardo Mariani,Mauro Pezzè*

Main category: cs.SE

TL;DR: E-Test使用大型语言模型从生产环境中识别未测试的执行场景，并生成测试用例来增强测试套件，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 测试套件存在不完整性，需要找到能探索现有测试范围之外执行场景的新测试用例，但这个过程在大规模测试套件中极具挑战性和劳动密集型。

Method: E-Test利用大型语言模型从大量生产场景中识别当前测试套件未充分覆盖的场景，并生成新的测试用例来增强测试套件。

Result: 在1,975个生产场景上的评估显示，E-Test的F1分数达到0.55，显著优于现有回归测试方法(0.34)和基础LLM方法(0.39)。

Conclusion: E-Test能有效识别未测试的执行场景，减少测试套件维护所需的手动工作量，显著提升测试套件的质量。

Abstract: Test suites are inherently imperfect, and testers can always enrich a suite
with new test cases that improve its quality and, consequently, the reliability
of the target software system. However, finding test cases that explore
execution scenarios beyond the scope of an existing suite can be extremely
challenging and labor-intensive, particularly when managing large test suites
over extended periods.
  In this paper, we propose E-Test, an approach that reduces the gap between
the execution space explored with a test suite and the executions experienced
after testing by augmenting the test suite with test cases that explore
execution scenarios that emerge in production. E-Test (i) identifies executions
that have not yet been tested from large sets of scenarios, such as those
monitored during intensive production usage, and (ii) generates new test cases
that enhance the test suite. E-Test leverages Large Language Models (LLMs) to
pinpoint scenarios that the current test suite does not adequately cover, and
augments the suite with test cases that execute these scenarios.
  Our evaluation on a dataset of 1,975 scenarios, collected from highly-starred
open-source Java projects already in production and Defects4J, demonstrates
that E-Test retrieves not-yet-tested execution scenarios significantly better
than state-of-the-art approaches. While existing regression testing and field
testing approaches for this task achieve a maximum F1-score of 0.34, and
vanilla LLMs achieve a maximum F1-score of 0.39, E-Test reaches 0.55. These
results highlight the impact of E-Test in enhancing test suites by effectively
targeting not-yet-tested execution scenarios and reducing manual effort
required for maintaining test suites.

</details>


### [2] [SODBench: A Large Language Model Approach to Documenting Spreadsheet Operations](https://arxiv.org/abs/2510.19864)
*Amila Indika,Igor Molybog*

Main category: cs.SE

TL;DR: 论文提出了电子表格操作文档化(SOD)任务，通过AI将电子表格操作代码转换为自然语言解释，以解决电子表格缺乏系统文档的问题。


<details>
  <summary>Details</summary>
Motivation: 电子表格在商业、会计和金融领域广泛应用，但缺乏系统文档方法阻碍了自动化、协作和知识传递，可能导致关键机构知识的丢失。

Method: 创建包含111个电子表格操作代码片段及其对应自然语言摘要的基准数据集，评估GPT-4o、GPT-4o-mini、LLaMA-3.3-70B、Mixtral-8x7B和Gemma2-9B五个大语言模型，使用BLEU、GLEU、ROUGE-L和METEOR指标。

Result: 研究发现大语言模型能够生成准确的电子表格文档，使SOD成为增强电子表格可重现性、可维护性和协作工作流程的可行前提步骤。

Conclusion: SOD是一个可行的AI任务，虽然还存在需要解决的挑战，但大语言模型能够有效生成电子表格操作的自然语言文档。

Abstract: Numerous knowledge workers utilize spreadsheets in business, accounting, and
finance. However, a lack of systematic documentation methods for spreadsheets
hinders automation, collaboration, and knowledge transfer, which risks the loss
of crucial institutional knowledge. This paper introduces Spreadsheet
Operations Documentation (SOD), an AI task that involves generating
human-readable explanations from spreadsheet operations. Many previous studies
have utilized Large Language Models (LLMs) for generating spreadsheet
manipulation code; however, translating that code into natural language for SOD
is a less-explored area. To address this, we present a benchmark of 111
spreadsheet manipulation code snippets, each paired with a corresponding
natural language summary. We evaluate five LLMs, GPT-4o, GPT-4o-mini,
LLaMA-3.3-70B, Mixtral-8x7B, and Gemma2-9B, using BLEU, GLEU, ROUGE-L, and
METEOR metrics. Our findings suggest that LLMs can generate accurate
spreadsheet documentation, making SOD a feasible prerequisite step toward
enhancing reproducibility, maintainability, and collaborative workflows in
spreadsheets, although there are challenges that need to be addressed.

</details>


### [3] [Knowledge-Guided Multi-Agent Framework for Application-Level Software Code Generation](https://arxiv.org/abs/2510.19868)
*Qian Xiong,Bo Yang,Weisong Sun,Yiran Zhang,Tianlin Li,Yang Liu,Zhi Jin*

Main category: cs.SE

TL;DR: 提出了KGACG框架，通过多智能体协作将软件需求规范和架构设计文档转换为可执行代码，解决复杂应用级软件代码生成的挑战。


<details>
  <summary>Details</summary>
Motivation: 现有方法在大型应用级软件代码生成中表现不足，难以确保合理的项目代码组织结构，且难以维护代码生成过程。

Method: 采用知识引导的多智能体协作框架，包含代码组织与规划智能体(COPA)、编码智能体(CA)和测试智能体(TA)，结合反馈机制形成闭环协作。

Result: 通过Java坦克大战游戏案例展示了KGACG框架中智能体的协作过程，同时面临一些挑战。

Conclusion: KGACG致力于推进应用级软件开发自动化进程。

Abstract: Automated code generation driven by Large Lan- guage Models (LLMs) has
enhanced development efficiency, yet generating complex application-level
software code remains challenging. Multi-agent frameworks show potential, but
existing methods perform inadequately in large-scale application-level software
code generation, failing to ensure reasonable orga- nizational structures of
project code and making it difficult to maintain the code generation process.
To address this, this paper envisions a Knowledge-Guided Application-Level Code
Generation framework named KGACG, which aims to trans- form software
requirements specification and architectural design document into executable
code through a collaborative closed- loop of the Code Organization & Planning
Agent (COPA), Coding Agent (CA), and Testing Agent (TA), combined with a
feedback mechanism. We demonstrate the collaborative process of the agents in
KGACG in a Java Tank Battle game case study while facing challenges. KGACG is
dedicated to advancing the automation of application-level software
development.

</details>


### [4] [BugPilot: Complex Bug Generation for Efficient Learning of SWE Skills](https://arxiv.org/abs/2510.19898)
*Atharv Sonwane,Isadora White,Hyunji Lee,Matheus Pereira,Lucas Caccia,Minseon Kim,Zhengyan Shi,Chinmay Singh,Alessandro Sordoni,Marc-Alexandre Côté,Xingdi Yuan*

Main category: cs.SE

TL;DR: 提出一种通过让软件工程代理在添加功能时无意中破坏测试来生成高质量、多样化bug的新方法，相比故意生成bug的方法更接近真实开发过程，训练效果更好。


<details>
  <summary>Details</summary>
Motivation: 高质量bug对于训练下一代基于语言模型的软件工程代理至关重要，现有方法通过故意引入局部扰动生成bug，会产生分布外效应，不能反映真实的开发过程。

Method: 通过指令软件工程代理向代码库添加功能，在此过程中可能无意中破坏测试，从而产生bug。这种方法更接近人类开发者的编辑模式。

Result: 该方法生成的bug训练数据效率更高，仅用1200个bug就能超越其他数据集3000个bug的效果，提升2%。训练出的FrogBoss模型在SWE-bench Verified上达到54.6%的pass@1，FrogMini达到45.3%。

Conclusion: 通过让代理在添加功能时无意引入bug的方法，能够生成更真实、多样化的训练数据，显著提升软件工程代理的性能表现。

Abstract: High quality bugs are key to training the next generation of language model
based software engineering (SWE) agents. We introduce a novel method for
synthetic generation of difficult and diverse bugs. Our method instructs SWE
Agents to introduce a feature into the codebase whereby they may
unintentionally break tests, resulting in bugs. Prior approaches often induce
an out-of-distribution effect by generating bugs intentionally (e.g. by
introducing local perturbation to existing code), which does not reflect
realistic development processes. We perform qualitative analysis to demonstrate
that our approach for generating bugs more closely reflects the patterns found
in human-authored edits. Through extensive experiments, we demonstrate that our
bugs provide more efficient training data for supervised fine-tuning,
outperforming other bug datasets by 2% with half the training data (1.2k vs. 3k
bugs). We train on our newly generated bugs in addition to existing bug
datasets to get FrogBoss a state-of-the-art 32B parameter model on SWE-bench
Verified with a pass@1 of 54.6% and FrogMini a state-of-the-art 14B model on
SWE-bench Verified with a pass@1 of 45.3% on SWE-bench Verified averaged over
three seeds.

</details>


### [5] [On Interaction Effects in Greybox Fuzzing](https://arxiv.org/abs/2510.19984)
*Konstantinos Kitsios,Marcel Böhme,Alberto Bacchelli*

Main category: cs.SE

TL;DR: MuoFuzz是一个灰盒模糊测试工具，通过学习和选择最有希望的变异器序列来提高测试效率，相比AFL++和MOPT在代码覆盖率和漏洞发现方面表现更好。


<details>
  <summary>Details</summary>
Motivation: 假设变异器应用于种子输入的顺序对灰盒模糊测试器的有效性有影响，实验证实了这种交互效应。

Method: MuoFuzz学习下一个变异器产生有趣输入的条件概率，并使用随机游走从学习到的概率中采样生成变异器序列。

Result: 在FuzzBench和MAGMA基准测试中，MuoFuzz实现了最高的代码覆盖率，发现了AFL++遗漏的4个漏洞和AFL++与MOPT都遗漏的1个漏洞。

Conclusion: 通过学习变异器序列的交互效应，MuoFuzz能够更有效地进行模糊测试，提高代码覆盖率和漏洞发现能力。

Abstract: A greybox fuzzer is an automated software testing tool that generates new
test inputs by applying randomly chosen mutators (e.g., flipping a bit or
deleting a block of bytes) to a seed input in random order and adds all
coverage-increasing inputs to the corpus of seeds. We hypothesize that the
order in which mutators are applied to a seed input has an impact on the
effectiveness of greybox fuzzers. In our experiments, we fit a linear model to
a dataset that contains the effectiveness of all possible mutator pairs and
indeed observe the conjectured interaction effect. This points us to more
efficient fuzzing by choosing the most promising mutator sequence with a higher
likelihood. We propose MuoFuzz, a greybox fuzzer that learns and chooses the
most promising mutator sequences. MuoFuzz learns the conditional probability
that the next mutator will yield an interesting input, given the previously
selected mutator. Then, it samples from the learned probability using a random
walk to generate mutator sequences. We compare the performance of MuoFuzz to
AFL++, which uses a fixed selection probability, and MOPT, which optimizes the
selection probability of each mutator in isolation. Experimental results on the
FuzzBench and MAGMA benchmarks show that MuoFuzz achieves the highest code
coverage and finds four bugs missed by AFL++ and one missed by both AFL++ and
MOPT.

</details>


### [6] [A Framework for the Adoption and Integration of Generative AI in Midsize Organizations and Enterprises (FAIGMOE)](https://arxiv.org/abs/2510.19997)
*Abraham Itzhak Weinberg*

Main category: cs.SE

TL;DR: 本文提出了FAIGMOE框架，专门针对中型组织和大型企业的生成式AI采用挑战，整合了技术采纳理论、组织变革管理和创新扩散视角。


<details>
  <summary>Details</summary>
Motivation: 现有技术采纳框架缺乏对生成式AI在不同规模组织中实施的具体指导，中型组织面临资源限制，大型企业面临组织复杂性，需要专门框架解决这些独特挑战。

Method: 开发FAIGMOE概念框架，包含四个相互关联的阶段：战略评估、规划与用例开发、实施与集成、运营与优化，整合了技术采纳理论、组织变革管理和创新扩散视角。

Result: FAIGMOE提供了首个全面解决中型和大型企业生成式AI采纳的概念框架，包含可操作的实施协议、评估工具和治理模板。

Conclusion: FAIGMOE填补了生成式AI采纳文献的关键空白，为不同规模组织提供了可扩展的实施指导，但需要通过未来研究进行实证验证。

Abstract: Generative Artificial Intelligence (GenAI) presents transformative
opportunities for organizations, yet both midsize organizations and larger
enterprises face distinctive adoption challenges. Midsize organizations
encounter resource constraints and limited AI expertise, while enterprises
struggle with organizational complexity and coordination challenges. Existing
technology adoption frameworks, including TAM (Technology Acceptance Model),
TOE (Technology Organization Environment), and DOI (Diffusion of Innovations)
theory, lack the specificity required for GenAI implementation across these
diverse contexts, creating a critical gap in adoption literature. This paper
introduces FAIGMOE (Framework for the Adoption and Integration of Generative AI
in Midsize Organizations and Enterprises), a conceptual framework addressing
the unique needs of both organizational types. FAIGMOE synthesizes technology
adoption theory, organizational change management, and innovation diffusion
perspectives into four interconnected phases: Strategic Assessment, Planning
and Use Case Development, Implementation and Integration, and
Operationalization and Optimization. Each phase provides scalable guidance on
readiness assessment, strategic alignment, risk governance, technical
architecture, and change management adaptable to organizational scale and
complexity. The framework incorporates GenAI specific considerations including
prompt engineering, model orchestration, and hallucination management that
distinguish it from generic technology adoption frameworks. As a perspective
contribution, FAIGMOE provides the first comprehensive conceptual framework
explicitly addressing GenAI adoption across midsize and enterprise
organizations, offering actionable implementation protocols, assessment
instruments, and governance templates requiring empirical validation through
future research.

</details>


### [7] [The Cost of Downgrading Build Systems: A Case Study of Kubernetes](https://arxiv.org/abs/2510.20041)
*Gareema Ranjan,Mahmoud Alfadel,Gengyi Sun,Shane McIntosh*

Main category: cs.SE

TL;DR: 研究分析了Kubernetes项目从Bazel降级到Go Build的案例，发现虽然Bazel构建速度更快，但内存消耗更大，降级会导致CI资源成本增加。


<details>
  <summary>Details</summary>
Motivation: 探索构建系统降级对性能的影响，虽然团队可能因维护性考虑放弃基于工件的构建工具，但其性能影响尚未充分研究。

Method: 对Kubernetes项目进行案例研究，复制和分析降级期间的完整和增量构建，并在其他四个项目中复现研究。

Result: Bazel构建比Go Build快23.06-38.66%，但内存占用多81.42-351.07%，CPU负载在并行度较高时更大。降级可能增加CI资源成本达76%。

Conclusion: 放弃基于工件的构建工具虽然可能提高维护性，但对大型项目会带来显著的性能成本，需要权衡取舍。

Abstract: Since developers invoke the build system frequently, its performance can
impact productivity. Modern artifact-based build tools accelerate builds, yet
prior work shows that teams may abandon them for alternatives that are easier
to maintain. While prior work shows why downgrades are performed, the
implications of downgrades remain largely unexplored. In this paper, we
describe a case study of the Kubernetes project, focusing on its downgrade from
an artifact-based build tool (Bazel) to a language-specific solution (Go
Build). We reproduce and analyze the full and incremental builds of change sets
during the downgrade period. On the one hand, we find that Bazel builds are
faster than Go Build, completing full builds in 23.06-38.66 up to 75.19 impose
a larger memory footprint than Go Build of 81.42-351.07 respectively. Bazel
builds also impose a greater CPU load at parallelism settings above eight for
full builds and above one for incremental builds. We estimate that downgrading
from Bazel can increase CI resource costs by up to 76 explore whether our
observations generalize by replicating our Kubernetes study on four other
projects that also downgraded from Bazel to older build tools. We observe that
while build time penalties decrease, Bazel consistently consumes more memory.
We conclude that abandoning artifact-based build tools, despite perceived
maintainability benefits, tends to incur considerable performance costs for
large projects. Our observations may help stakeholders to balance trade-offs in
build tool adoption

</details>


### [8] [Developing a Model-Driven Reengineering Approach for Migrating PL/SQL Triggers to Java: A Practical Experience](https://arxiv.org/abs/2510.20121)
*Carlos J. Fernandez-Candel,Jesus Garcia-Molina,Francisco Javier Bermudez Ruiz,Jose Ramon Hoyos Barcelo,Diego Sevilla Ruiz,Benito Jose Cuesta Viera*

Main category: cs.SE

TL;DR: 提出了一种基于模型驱动工程的软件再工程过程，用于将Oracle Forms中的PL/SQL代码迁移到Java多层级架构中，采用类似TDD的方法开发模型转换并包含三种代码验证机制。


<details>
  <summary>Details</summary>
Motivation: 企业需要将传统的RAD平台（如Oracle Forms）应用迁移到现代软件技术，特别是将PL/SQL单体代码转换为Java多层架构代码。

Method: 使用KDM模型表示遗留代码，采用模型驱动再工程过程，集成类似TDD的方法增量开发模型转换，并包含三种生成的代码验证机制。

Result: 开发了迁移工具，成功将PL/SQL代码转换为Java代码，并详细实现了再工程方法和MDE应用评估。

Conclusion: 模型驱动工程方法不仅适用于正向工程，也能成功应用于现有系统的演进和迁移，特别是RAD平台应用向现代技术的迁移。

Abstract: Model-driven software engineering (MDE) techniques are not only useful in
forward engineering scenarios, but can also be successfully applied to evolve
existing systems. RAD (Rapid Application Development) platforms emerged in the
nineties, but the success of modern software technologies motivated that a
large number of enterprises tackled the migration of their RAD applications,
such as Oracle Forms. Our research group has collaborated with a software
company in developing a solution to migrate PL/SQL monolithic code on Forms
triggers and program units to Java code separated in several tiers.
  Our research focused on the model-driven reengineering process applied to
develop the migration tool for the conversion of PL/SQL code to Java. Legacy
code is represented in form of KDM (Knowledge-Discovery Metamodel) models. In
this paper, we propose a software process to implement a model-driven
re-engineering. This process integrates a TDD-like approach to incrementally
develop model transformations with three kinds of validations for the generated
code. The implementation and validation of the re-engineering approach are
explained in detail, as well as the evaluation of some issues related with the
application of MDE.

</details>


### [9] [Automated Cloud Infrastructure-as-Code Reconciliation with AI Agents](https://arxiv.org/abs/2510.20211)
*Zhenning Yang,Hui Guan,Victor Nicolet,Brandon Paulsen,Joey Dodds,Daniel Kroening,Ang Chen*

Main category: cs.SE

TL;DR: NSync是一个自动化系统，通过分析云API调用来检测基础设施漂移，并使用LLM推断意图并更新IaC配置，实现基础设施与代码的自动同步。


<details>
  <summary>Details</summary>
Motivation: 当基础设施即代码(IaC)与云控制台、CLI或SDK同时使用时，会导致基础设施漂移问题，即配置与实际基础设施状态不一致，后续IaC操作可能撤销有效更新或引发错误。

Method: NSync利用云API调用轨迹检测漂移，采用基于LLM的智能架构从嘈杂的API序列推断高层意图，使用专用工具合成针对性的IaC更新，并通过自演进知识库持续改进。

Result: 在5个真实Terraform项目和372个漂移场景的实验中，NSync在准确性（从0.71提升到0.97 pass@3）和令牌效率（1.47倍改进）方面均优于基线方法。

Conclusion: NSync能够有效解决基础设施漂移问题，通过自动化检测和协调机制确保IaC配置与实际云基础设施状态保持一致。

Abstract: Cloud infrastructure is managed through a mix of interfaces -- traditionally,
cloud consoles, command-line interfaces (CLI), and SDKs are the tools of
choice. Recently, Infrastructure-as-Code/IaC frameworks (e.g., Terraform) have
quickly gained popularity. Unlike conventional tools, IaC~frameworks encode the
infrastructure in a "source-of-truth" configuration. They are capable of
automatically carrying out modifications to the cloud -- deploying, updating,
or destroying resources -- to bring the actual infrastructure into alignment
with the IaC configuration. However, when IaC is used alongside consoles, CLIs,
or SDKs, it loses visibility into external changes, causing infrastructure
drift, where the configuration becomes outdated, and later IaC operations may
undo valid updates or trigger errors.
  We present NSync, an automated system for IaC reconciliation that propagates
out-of-band changes back into the IaC program. Our key insight is that
infrastructure changes eventually all occur via cloud API invocations -- the
lowest layer for cloud management operations. NSync gleans insights from API
traces to detect drift (i.e., non-IaC changes) and reconcile it (i.e., update
the IaC configuration to capture the changes). It employs an agentic
architecture that leverages LLMs to infer high-level intents from noisy API
sequences, synthesize targeted IaC updates using specialized tools, and
continually improve through a self-evolving knowledge base of past
reconciliations. We further introduce a novel evaluation pipeline for injecting
realistic drifts into cloud infrastructure and assessing reconciliation
performance. Experiments across five real-world Terraform projects and 372
drift scenarios show that NSync outperforms the baseline both in terms of
accuracy (from 0.71 to 0.97 pass@3) and token efficiency (1.47$\times$
improvement).

</details>


### [10] [Classport: Designing Runtime Dependency Introspection for Java](https://arxiv.org/abs/2510.20340)
*Serena Cofano,Daniel Williams,Aman Sharma,Martin Monperrus*

Main category: cs.SE

TL;DR: Classport系统通过在Java类文件中嵌入依赖信息，实现了运行时依赖关系自省，解决了Java缺乏运行时依赖观察能力的问题。


<details>
  <summary>Details</summary>
Motivation: Java缺乏对运行时依赖关系的自省能力，这在软件供应链安全中至关重要。

Method: 开发Classport系统，将依赖信息嵌入到Java类文件中，从而在运行时能够检索依赖信息。

Result: 在六个真实项目中评估Classport，证明其在运行时识别依赖关系的可行性。

Conclusion: Classport的运行时依赖自省为运行时完整性检查开辟了重要途径。

Abstract: Runtime introspection of dependencies, i.e., the ability to observe which
dependencies are currently used during program execution, is fundamental for
Software Supply Chain security. Yet, Java has no support for it. We solve this
problem with Classport, a system that embeds dependency information into Java
class files, enabling the retrieval of dependency information at runtime. We
evaluate Classport on six real-world projects, demonstrating the feasibility in
identifying dependencies at runtime. Runtime dependency introspection with
Classport opens important avenues for runtime integrity checking.

</details>


### [11] [Symmetry in Software Platforms as an Architectural Principle](https://arxiv.org/abs/2510.20389)
*Bjorn Remseth*

Main category: cs.SE

TL;DR: 软件平台作为结构保持系统，通过强制执行结构规律来产生架构鲁棒性


<details>
  <summary>Details</summary>
Motivation: 探索软件平台如何通过保持结构规律来增强架构鲁棒性

Method: 将软件平台视为结构保持系统，分析其在特定变换下的对称性

Result: 发现强制执行结构规律能够产生架构鲁棒性

Conclusion: 软件平台的架构鲁棒性源于对结构规律的系统性强制执行

Abstract: Software platforms often act as structure preserving systems. They provide
consistent interfaces and behaviors that remain stable under specific
transformations that we denote as symmetries. This paper explores the idea that
architectural robustness emerges from enforcing such structural regularities

</details>


### [12] [FMI-Based Distributed Co-Simulation with Enhanced Security and Intellectual Property Safeguards](https://arxiv.org/abs/2510.20403)
*Santiago Gil,Ecem E. Baş,Christian D. Jensen,Sebastian Engelsgaard,Giuseppe Abbiati,Cláudio Gomes*

Main category: cs.SE

TL;DR: 提出了一种基于UniFMU的分布式协同仿真方法，增强网络安全和知识产权保护机制，确保连接由客户端发起且模型和二进制文件位于可信平台。


<details>
  <summary>Details</summary>
Motivation: 分布式协同仿真在保护知识产权方面发挥关键作用，但目前缺乏针对连续时间或混合系统分布式仿真的网络安全指南，无法防范潜在的黑客攻击。

Method: 在UniFMU基础上构建分布式协同仿真框架，采用增强的网络安全和IP保护机制，要求连接由客户端发起，模型和二进制文件驻留在可信平台。

Result: 通过四个不同网络设置中的两个协同仿真演示展示了该方法的可行性，并分析了IP保护分布与性能效率之间的权衡关系。

Conclusion: 该方法成功实现了分布式协同仿真的网络安全和知识产权保护，为相关领域提供了实用的解决方案。

Abstract: Distributed co-simulation plays a key role in enabling collaborative modeling
and simulation by different stakeholders while protecting their Intellectual
Property (IP). Although IP protection is provided implicitly by co-simulation,
there is no consensus in the guidelines to conduct distributed co-simulation of
continuous-time or hybrid systems with no exposure to potential hacking
attacks. We propose an approach for distributed co-simulation on top of UniFMU
with enhanced cybersecurity and IP protection mechanisms, ensuring that the
connection is initiated by the client and the models and binaries live on
trusted platforms. We showcase the functionality of this approach using two
co-simulation demos in four different network settings and analyze the
trade-off between IP-protected distribution and performance efficiency in these
settings.

</details>


### [13] [Toward Practical Deductive Verification: Insights from a Qualitative Survey in Industry and Academia](https://arxiv.org/abs/2510.20514)
*Lea Salome Brugger,Xavier Denis,Peter Müller*

Main category: cs.SE

TL;DR: 本文通过访谈30位验证从业者，系统分析了演绎验证成功应用的关键因素和阻碍其广泛采用的障碍，并提出了具体建议。


<details>
  <summary>Details</summary>
Motivation: 尽管演绎验证在特定项目中证明有效可行，但尚未成为主流技术。为了促进其广泛应用，需要研究成功应用的关键因素和阻碍广泛采用的底层问题。

Method: 采用半结构化访谈的方式，采访了30位来自工业界和学术界的验证从业者，并使用主题分析方法系统分析收集的数据。

Result: 除了确认已知挑战（如进行形式化证明需要高水平专业知识）外，还发现了几个未被充分探索的障碍，包括证明维护、自动化控制不足和可用性问题。

Conclusion: 基于数据分析结果，提取了演绎验证的促进因素和障碍，并为从业者、工具构建者和研究人员制定了具体建议，包括可用性、自动化和与现有工作流程集成的原则。

Abstract: Deductive verification is an effective method to ensure that a given system
exposes the intended behavior. In spite of its proven usefulness and
feasibility in selected projects, deductive verification is still not a
mainstream technique. To pave the way to widespread use, we present a study
investigating the factors enabling successful applications of deductive
verification and the underlying issues preventing broader adoption. We
conducted semi-structured interviews with 30 practitioners of verification from
both industry and academia and systematically analyzed the collected data
employing a thematic analysis approach. Beside empirically confirming familiar
challenges, e.g., the high level of expertise needed for conducting formal
proofs, our data reveal several underexplored obstacles, such as proof
maintenance, insufficient control over automation, and usability concerns. We
further use the results from our data analysis to extract enablers and barriers
for deductive verification and formulate concrete recommendations for
practitioners, tool builders, and researchers, including principles for
usability, automation, and integration with existing workflows.

</details>


### [14] [Large Language Models for Fault Localization: An Empirical Study](https://arxiv.org/abs/2510.20521)
*YingJian Xiao,RongQun Hu,WeiWei Gong,HongWei Li,AnQuan Jie*

Main category: cs.SE

TL;DR: 对LLMs在代码故障定位任务中的系统实证研究，评估了开源和闭源模型在不同提示策略下的性能表现。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏对LLMs在代码故障定位任务中的全面评估，而故障定位性能直接影响程序修复效果。

Method: 使用HumanEval-Java和Defects4J数据集，评估开源模型(Qwen2.5-coder-32b-instruct, DeepSeek-V3)和闭源模型(GPT-4.1 mini, Gemini-2.5-flash)，研究标准提示、少样本学习和思维链等不同提示策略的影响。

Result: 包含错误报告上下文显著提升模型性能；少样本学习有改进潜力但边际收益递减；思维链推理效果高度依赖模型内在推理能力。

Conclusion: 揭示了不同模型在故障定位任务中的性能特征和权衡，为改进故障定位效果提供了有价值的见解。

Abstract: Large language models (LLMs) have demonstrated remarkable capabilities in
code-related tasks, particularly in automated program repair. However, the
effectiveness of such repairs is highly dependent on the performance of
upstream fault localization, for which comprehensive evaluations are currently
lacking. This paper presents a systematic empirical study on LLMs in the
statement-level code fault localization task. We evaluate representative
open-source models (Qwen2.5-coder-32b-instruct, DeepSeek-V3) and closed-source
models (GPT-4.1 mini, Gemini-2.5-flash) to assess their fault localization
capabilities on the HumanEval-Java and Defects4J datasets. The study
investigates the impact of different prompting strategies--including standard
prompts, few-shot examples, and chain-of-reasoning--on model performance, with
a focus on analysis across accuracy, time efficiency, and economic cost
dimensions. Our experimental results show that incorporating bug report context
significantly enhances model performance. Few-shot learning shows potential for
improvement but exhibits noticeable diminishing marginal returns, while
chain-of-thought reasoning's effectiveness is highly contingent on the model's
inherent reasoning capabilities. This study not only highlights the performance
characteristics and trade-offs of different models in fault localization tasks,
but also offers valuable insights into the strengths of current LLMs and
strategies for improving fault localization effectiveness.

</details>


### [15] [A Soundness and Precision Benchmark for Java Debloating Tools](https://arxiv.org/abs/2510.20679)
*Jonas Klauke,Tom Ohlmer,Stefan Schott,Serena Elisa Ponta,Wolfram Fischer,Eric Bodden*

Main category: cs.SE

TL;DR: 论文介绍了Deblometer微基准测试套件，用于评估Java去膨胀工具在移除不必要依赖和程序构造时的准确性和完整性。评估发现现有工具都会移除必要的程序构造，导致语义改变或执行崩溃。


<details>
  <summary>Details</summary>
Motivation: 现代软件开发平均包含36个依赖项，其中80%是传递依赖，但只有24.9%在运行时真正需要。现有去膨胀工具在移除不必要代码时需要平衡准确性和完整性。

Method: 开发了包含59个测试用例的Deblometer微基准测试套件，每个测试用例都包含手动标注的必要和膨胀类、方法和字段的基准真值，用于精确测量去膨胀工具的准确性和完整性。

Result: 评估了三个流行的Java去膨胀工具：Deptrim、JShrink和ProGuard。所有工具都会移除必要的程序构造，动态类加载特性在所有工具中都引入了不完整性。Deptrim保留更多膨胀构造，ProGuard移除更多必要构造，JShrink因对注解支持有限导致完整性显著受损。

Conclusion: 现有去膨胀工具存在完整性缺陷，需要改进以确保去膨胀后软件的稳定性和可靠性。

Abstract: Modern software development reuses code by importing libraries as
dependencies. Software projects typically include an average of 36
dependencies, with 80% being transitive, meaning they are dependencies of
dependencies. Recent research indicates that only 24.9% of these dependencies
are required at runtime, and even within those, many program constructs remain
unused, adding unnecessary code to the project. This has led to the development
of debloating tools that remove unnecessary dependencies and program constructs
while balancing precision by eliminating unused constructs and soundness by
preserving all required constructs. To systematically evaluate this trade-off,
we developed Deblometer, a micro-benchmark consisting of 59 test cases designed
to assess support for various Java language features in debloating tools. Each
test case includes a manually curated ground truth specifying necessary and
bloated classes, methods, and fields, enabling precise measurement of soundness
and precision. Using Deblometer, we evaluated three popular Java debloating
tools: Deptrim, JShrink, and ProGuard. Our evaluation reveals that all tools
remove required program constructs, which results in changed semantics or
execution crashes. In particular, the dynamic class loading feature introduces
unsoundness in all evaluated tools. Our comparison shows that Deptrim retains
more bloated constructs, while ProGuard removes more required constructs.
JShrink's soundness is significantly affected by limited support for
annotations, which leads to corrupted debloated artifacts. These soundness
issues highlight the need to improve debloating tools to ensure stable and
reliable debloated software.

</details>


### [16] [Exploring Large Language Models for Access Control Policy Synthesis and Summarization](https://arxiv.org/abs/2510.20692)
*Adarsh Vatsa,Bethel Hall,William Eiers*

Main category: cs.SE

TL;DR: 探索大型语言模型在访问控制策略生成和总结中的有效性，发现LLMs能生成语法正确的策略但存在许可性问题，在策略分析方面与符号方法结合表现良好。


<details>
  <summary>Details</summary>
Motivation: 云计算的普及使得访问控制策略日益复杂，手动编写容易出错且难以精确分析。LLMs在代码合成和总结方面的成功使其可能用于自动生成策略或帮助理解现有策略。

Method: 首先研究不同LLMs生成访问控制策略的能力，然后提出基于语义的请求总结方法，利用LLMs生成策略允许请求的精确描述。

Result: LLMs能有效生成语法正确的策略，但非推理型LLMs仅45.8%的时间生成与规范等价的策略，推理型LLMs达到93.7%。在策略分析方面，LLMs与符号方法结合表现良好。

Conclusion: 虽然LLMs在自动策略生成方面存在显著障碍，但在分析现有策略时与符号方法结合显示出有希望的结果。

Abstract: Cloud computing is ubiquitous, with a growing number of services being hosted
on the cloud every day. Typical cloud compute systems allow administrators to
write policies implementing access control rules which specify how access to
private data is governed. These policies must be manually written, and due to
their complexity can often be error prone. Moreover, existing policies often
implement complex access control specifications and thus can be difficult to
precisely analyze in determining their behavior works exactly as intended.
Recently, Large Language Models (LLMs) have shown great success in automated
code synthesis and summarization. Given this success, they could potentially be
used for automatically generating access control policies or aid in
understanding existing policies. In this paper, we explore the effectiveness of
LLMs for access control policy synthesis and summarization. Specifically, we
first investigate diverse LLMs for access control policy synthesis, finding
that: although LLMs can effectively generate syntactically correct policies,
they have permissiveness issues, generating policies equivalent to the given
specification 45.8% of the time for non-reasoning LLMs, and 93.7% of the time
for reasoning LLMs. We then investigate how LLMs can be used to analyze
policies by introducing a novel semantic-based request summarization approach
which leverages LLMs to generate a precise characterization of the requests
allowed by a policy. Our results show that while there are significant hurdles
in leveraging LLMs for automated policy generation, LLMs show promising results
when combined with symbolic approaches in analyzing existing policies.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [17] [CourtGuard: A Local, Multiagent Prompt Injection Classifier](https://arxiv.org/abs/2510.19844)
*Isaac Wu,Michael Maslowski*

Main category: cs.CR

TL;DR: CourtGuard是一个本地运行的多智能体提示注入分类器，通过法庭式多智能体LLM系统检测恶意提示，相比Direct Detector具有更低的误报率，但整体检测性能较差。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型在敏感应用中的集成，提示注入攻击风险日益增加，可能导致数据泄露、传播错误信息和有害行为。

Method: 使用法庭式多智能体LLM系统：辩护律师模型论证提示无害，检察官模型论证提示为注入攻击，法官模型给出最终分类。

Result: CourtGuard比Direct Detector具有更低的误报率，但整体上是一个较差的提示注入检测器。

Conclusion: 多智能体系统作为防御提示注入攻击的方法具有潜力，考虑对抗性和良性场景对提示分类很重要。

Abstract: As large language models (LLMs) become integrated into various sensitive
applications, prompt injection, the use of prompting to induce harmful
behaviors from LLMs, poses an ever increasing risk. Prompt injection attacks
can cause LLMs to leak sensitive data, spread misinformation, and exhibit
harmful behaviors. To defend against these attacks, we propose CourtGuard, a
locally-runnable, multiagent prompt injection classifier. In it, prompts are
evaluated in a court-like multiagent LLM system, where a "defense attorney"
model argues the prompt is benign, a "prosecution attorney" model argues the
prompt is a prompt injection, and a "judge" model gives the final
classification. CourtGuard has a lower false positive rate than the Direct
Detector, an LLM as-a-judge. However, CourtGuard is generally a worse prompt
injection detector. Nevertheless, this lower false positive rate highlights the
importance of considering both adversarial and benign scenarios for the
classification of a prompt. Additionally, the relative performance of
CourtGuard in comparison to other prompt injection classifiers advances the use
of multiagent systems as a defense against prompt injection attacks. The
implementations of CourtGuard and the Direct Detector with full prompts for
Gemma-3-12b-it, Llama-3.3-8B, and Phi-4-mini-instruct are available at
https://github.com/isaacwu2000/CourtGuard.

</details>


### [18] [Can Reasoning Models Obfuscate Reasoning? Stress-Testing Chain-of-Thought Monitorability](https://arxiv.org/abs/2510.19851)
*Artur Zolkowski,Wen Xing,David Lindner,Florian Tramèr,Erik Jenner*

Main category: cs.CR

TL;DR: 研究表明，在强混淆压力下，某些模型能够成功完成对抗性任务并逃避检测，而内部思维链的混淆程度低于外部思维链。思维链监控在无混淆压力下表现良好，但在实际部署中需要进行模型特定的压力测试。


<details>
  <summary>Details</summary>
Motivation: 研究动机是测试模型是否能够通过混淆思维链来隐藏其对抗性目标并逃避检测，从而评估思维链监控的可靠性。

Method: 开发了一个可组合和可量化的提示分类法来引发思维链混淆，并在SHADE-Arena中使用玩具任务和更真实的环境评估内部和外部思维链。

Result: 结果显示：(i) 无混淆压力下，思维链监控准确高效；(ii) 强混淆压力下，某些模型能完成对抗任务并逃避检测；(iii) 模型内部思维链的混淆程度低于外部思维链。

Conclusion: 思维链在良性设置下提供有价值的监督，但稳健部署需要针对特定模型进行监控能力的压力测试。

Abstract: Recent findings suggest that misaligned models may exhibit deceptive
behavior, raising concerns about output trustworthiness. Chain-of-thought (CoT)
is a promising tool for alignment monitoring: when models articulate their
reasoning faithfully, monitors can detect and mitigate harmful behaviors before
undesirable outcomes occur. However, a key uncertainty is: Can models obfuscate
their CoT in order to pursue hidden adversarial objectives while evading
detection? To answer this question and thus stress-test CoT monitorability, we
develop a composable and quantifiable taxonomy of prompts to elicit CoT
obfuscation. We evaluate both internal CoT (reasoning traces) and external CoT
(prompted reasoning in outputs) using toy tasks and more realistic environments
in SHADE-Arena. We show that: (i) CoT monitoring performs accurately and
efficiently without obfuscation pressure. (ii) Under strong obfuscation
pressure, some models successfully complete adversarial tasks while evading
detection. (iii) Models do not obfuscate their internal CoT as much as their
external CoT (under prompt pressure). These results suggest that while CoT
provides valuable oversight in benign settings, robust deployment requires
model-specific stress-testing of monitorability.

</details>


### [19] [Model Context Contracts - MCP-Enabled Framework to Integrate LLMs With Blockchain Smart Contracts](https://arxiv.org/abs/2510.19856)
*Eranga Bandara,Sachin Shetty,Ravi Mukkamala,Ross Gore,Peter Foytik,Safdar H. Bouk,Abdul Rahman,Xueping Liang,Ng Wee Keong,Kasun De Zoysa,Aruna Withanage,Nilaan Loganathan*

Main category: cs.CR

TL;DR: 提出MCC框架，通过类似MCP的协议让LLM直接与区块链智能合约交互，使用户能用自然语言查询与区块链系统交互和执行交易。


<details>
  <summary>Details</summary>
Motivation: 区块链和生成式AI/LLM在各行业广泛应用，但缺乏标准化的方法将LLM应用与区块链无缝集成。

Method: 开发MCC框架，让智能合约作为智能代理识别自然语言输入并执行相应交易。通过定制数据集对LLM进行微调，提高自然语言输入到MCP函数映射的准确性。

Result: 在Rahasak区块链上开发了端到端原型，使用微调后的Llama-4 LLM，显著提升了平台性能和准确性。

Conclusion: 这是首次使用模型上下文协议概念将LLM与区块链集成的研究，为AI与区块链的深度融合提供了新途径。

Abstract: In recent years, blockchain has experienced widespread adoption across
various industries, becoming integral to numerous enterprise applications.
Concurrently, the rise of generative AI and LLMs has transformed human-computer
interactions, offering advanced capabilities in understanding and generating
human-like text. The introduction of the MCP has further enhanced AI
integration by standardizing communication between AI systems and external data
sources. Despite these advancements, there is still no standardized method for
seamlessly integrating LLM applications and blockchain. To address this
concern, we propose "MCC: Model Context Contracts" a novel framework that
enables LLMs to interact directly with blockchain smart contracts through
MCP-like protocol. This integration allows AI agents to invoke blockchain smart
contracts, facilitating more dynamic and context-aware interactions between
users and blockchain networks. Essentially, it empowers users to interact with
blockchain systems and perform transactions using queries in natural language.
Within this proposed architecture, blockchain smart contracts can function as
intelligent agents capable of recognizing user input in natural language and
executing the corresponding transactions. To ensure that the LLM accurately
interprets natural language inputs and maps them to the appropriate MCP
functions, the LLM was fine-tuned using a custom dataset comprising user inputs
paired with their corresponding MCP server functions. This fine-tuning process
significantly improved the platform's performance and accuracy. To validate the
effectiveness of MCC, we have developed an end-to-end prototype implemented on
the Rahasak blockchain with the fine-tuned Llama-4 LLM. To the best of our
knowledge, this research represents the first approach to using the concept of
Model Context Protocol to integrate LLMs with blockchain.

</details>


### [20] [Cyberattack Detection in Critical Infrastructure and Supply Chains](https://arxiv.org/abs/2510.19859)
*Smita Khapre*

Main category: cs.CR

TL;DR: 提出将密集神经网络与数据增强技术结合，以解决工业4.0中关键基础设施和供应链的零日攻击检测问题，但发现现有数据平衡方法在处理高度不平衡网络流量数据时存在过拟合问题。


<details>
  <summary>Details</summary>
Motivation: 工业4.0中关键基础设施和供应链的网络安全面临挑战，传统入侵检测系统无法检测零日攻击，需要智能化的检测方法。

Method: 使用密集神经网络结合数据增强技术，采用合成少数类过采样技术来平衡高度不平衡的网络流量数据集。

Result: 在平衡测试数据上获得较高准确率，但在原始不平衡测试数据上准确率较低，表明存在过拟合问题。与现有方法比较显示对原始数据集的分类效果不佳。

Conclusion: 高度不平衡的网络流量数据集需要不同的数据增强方法，现有方法在处理极端不平衡数据时效果有限。

Abstract: Cyberattack detection in Critical Infrastructure and Supply Chains has become
challenging in Industry 4.0. Intrusion Detection Systems (IDS) are deployed to
counter the cyberattacks. However, an IDS effectively detects attacks based on
the known signatures and patterns, Zero-day attacks go undetected. To overcome
this drawback in IDS, the integration of a Dense Neural Network (DNN) with Data
Augmentation is proposed. It makes IDS intelligent and enables it to self-learn
with high accuracy when a novel attack is encountered. The network flow
captures datasets are highly imbalanced same as the real network itself. The
Data Augmentation plays a crucial role in balancing the data. The balancing of
data is challenging as the minority class is as low as 0.000004\% of the
dataset, and the abundant class is higher than 80\% of the dataset. Synthetic
Minority Oversampling Technique is used for balancing the data. However, higher
accuracies are achieved with balanced test data, lower accuracies are
noticeable with the original imbalanced test data suggesting overfitting. A
comparison with state-of-the-art research using Synthetic Minority Oversampling
Technique with Edited Nearest Neighbor shows the classification of classes
remains poor for the original dataset. This suggests highly imbalanced datasets
of network flow require a different method of data augmentation.

</details>


### [21] [Policy-Governed RAG - Research Design Study](https://arxiv.org/abs/2510.19877)
*Jean-Marie Le Ray*

Main category: cs.CR

TL;DR: 提出了一种面向受监管工作流程的政策治理RAG架构，通过三个组件确保审计就绪的生成：合同控制、清单追踪和收据验证，使政策检查可审计、可重放且具有收据支持。


<details>
  <summary>Details</summary>
Motivation: 在医药、医疗器械、金融、法律和公共部门等受监管领域，错误成本可能超过数千欧元，且审计追踪是欧盟AI法案等法规的强制性要求，需要确保AI输出的合规性和可验证性。

Method: 采用三部分架构：(I)合同控制组件管理输出对法律和内部政策的遵守；(II)清单追踪组件通过加密锚定所有引用的来源证据确保可验证的来源；(III)收据验证组件提供最终的可移植合规证明。

Result: 设定了未经验证的目标：相对自信错误减少≥20%；p95延迟≤900毫秒；服务成本≤2.2倍；并包含使用NO-GO门的预注册试点。

Conclusion: 该设计通过使政策检查可审计、可重放且具有收据支持，补充了现有的RAG/护栏系统，特别适用于错误成本高且审计追踪为法规强制要求的受监管领域。

Abstract: A policy-governed RAG architecture is specified for audit-ready generation in
regulated workflows, organized as a triptych: (I) Contracts/Control
(SHRDLU-like), which governs output adherence to legal and internal policies;
(II) Manifests/Trails (Memex-like), which cryptographically anchors all cited
source evidence to ensure verifiable provenance; and (III)
Receipts/Verification (Xanadu-like), which provides the final, portable proof
of compliance for auditors (portable COSE/JOSE) (see Section 4 and Appendix A).
Rather than explaining model internals, outputs are gated ex-ante and bound to
cryptographically verifiable evidence for each material answer. Unvalidated
targets are stated (>=20% relative reduction in confident errors; p95 latency
<= 900 ms; <= 2.2x serve cost) together with a pre-registered (optional) pilot
using NO-GO gates. The design complements existing RAG/guardrails by making
policy checks auditable, replayable, and receipt-backed. Target domains include
back-office compliance in pharma, medical devices, finance, legal, and the
public sector where error costs may exceed thousands of euros and audit trails
are mandatory under regulations such as the EU AI Act. Future evaluations may
pre-commit to publishing negative results when any example NO-GO gate is not
met.

</details>


### [22] [A Proactive Insider Threat Management Framework Using Explainable Machine Learning](https://arxiv.org/abs/2510.19883)
*Selma Shikonde,Mike Wa Nkongolo*

Main category: cs.CR

TL;DR: 提出了IT-XML框架，结合CRISP-DM和隐马尔可夫模型，通过问卷调查评估员工知识，实现91.7%分类准确率，识别审计日志访问限制为最关键控制措施。


<details>
  <summary>Details</summary>
Motivation: 技术发展改变了组织安全态势，内部威胁日益严重，特别是在网络安全基础设施、专业知识和法规仍在发展的行业。

Method: 采用定量方法，使用在线问卷评估员工对内部威胁模式、访问控制、隐私实践和现有政策的了解，结合CRISP-DM和HMM进行模式识别和安全成熟度分类。

Result: 框架将所有组织分类为发展中的安全成熟度水平，置信度97-98%，分类准确率91.7%。随机森林分析显示供应商违规通知和定期审计日志审查是恢复力的关键决定因素。

Conclusion: IT-XML框架通过SHAP和LIME等可解释性方法提高了模型透明度，展示了加强内部威胁管理实践的潜力。

Abstract: Over the years, the technological landscape has evolved, reshaping the
security posture of organisations and increasing their exposure to
cybersecurity threats, many originating from within. Insider threats remain a
major challenge, particularly in sectors where cybersecurity infrastructure,
expertise, and regulations are still developing. This study proposes the
Insider Threat Explainable Machine Learning (IT-XML) framework, which
integrates the Cross-Industry Standard Process for Data Mining (CRISP-DM) with
Hidden Markov Models (HMM) to enhance proactive insider threat management and
decision-making. A quantitative approach is adopted using an online
questionnaire to assess employees' knowledge of insider threat patterns, access
control, privacy practices, and existing policies across three large
data-sensitive organisations. The IT-XML framework provides assessment
capabilities through survey-based data, HMM-driven pattern recognition for
security maturity classification, and evidence-based recommendations for
proactive threat mitigation. The framework classified all organisations at the
developing security maturity level with 97-98% confidence and achieved a
classification accuracy of 91.7%, identifying audit log access limits as the
most critical control. Random Forest analysis highlighted vendor breach
notifications (0.081) and regular audit log reviews (0.052) as key determinants
of resilience. Explainability methods such as SHAP and LIME improved model
transparency and interpretability, demonstrating the framework's potential to
strengthen insider threat management practices.

</details>


### [23] [Analysis and Comparison of Known and Randomly Generated S-boxes for Block Ciphers](https://arxiv.org/abs/2510.19885)
*James Kim*

Main category: cs.CR

TL;DR: 比较数学构造与随机生成的S盒在密码学性能上的差异，建立随机置换的平均性能基准，并在简单SPN结构中与传统设计对比


<details>
  <summary>Details</summary>
Motivation: 数学构造的S盒具有可证明的密码学特性，而完全随机方法缺乏理论保证但复杂度低且结果多样，需要系统评估随机方法的相对弱点

Method: 比较数学构造与随机生成的S盒，建立随机置换的平均性能度量，包括带循环约束的随机生成，在简单SPN设置中与传统设计对比

Result: 数学构造的S盒能生成数千个具有高非线性度、APN特性和平衡雪崩特性的S盒，而随机方法缺乏理论保证

Conclusion: 数学构造方法在密码学特性上优于随机生成方法，但随机方法在复杂度和结果多样性方面有其优势

Abstract: Mathematically constructed S-boxes arise from algebraic structures and finite
field theory to ensure strong, provable cryptographic properties. These
mathematically grounded constructions allow for generation of thousands of
S-Boxes with high nonlinearity, APN properties, and balanced avalanche
characteristics, unlike fully random methods, which lack such theoretical
guarantees in exchange for low complexity and more varied results. In this
work, we compare mathematically constructed constructions with randomly
generated ones to evaluate the relative weakness of the latter. We also
establish an average measure of performance for randomly generated
permutations, as well as random with forced cycle constraints, and compare them
to well-established designs in a simple SPN setting.

</details>


### [24] [Deep Sequence-to-Sequence Models for GNSS Spoofing Detection](https://arxiv.org/abs/2510.19890)
*Jan Zelinka,Oliver Kost,Marek Hrúz*

Main category: cs.CR

TL;DR: 提出了一个模拟欺骗攻击的数据生成框架，并应用基于深度神经网络的模型进行欺骗检测，Transformer架构取得了最佳性能，错误率仅为0.16%。


<details>
  <summary>Details</summary>
Motivation: 需要开发有效的欺骗攻击检测方法来保护系统安全，通过模拟真实世界的攻击场景来训练检测模型。

Method: 使用数据生成框架模拟欺骗攻击并随机分布攻击场景，应用LSTM和Transformer架构的深度神经网络进行在线检测。

Result: 深度学习模型能够准确区分欺骗信号和真实信号，Transformer架构通过早期输入融合实现了0.16%的最低错误率。

Conclusion: 基于深度学习的欺骗检测方法具有很高的准确性，特别是Transformer架构在欺骗检测任务中表现优异。

Abstract: We present a data generation framework designed to simulate spoofing attacks
and randomly place attack scenarios worldwide. We apply deep neural
network-based models for spoofing detection, utilizing Long Short-Term Memory
networks and Transformer-inspired architectures. These models are specifically
designed for online detection and are trained using the generated dataset. Our
results demonstrate that deep learning models can accurately distinguish
spoofed signals from genuine ones, achieving high detection performance. The
best results are achieved by Transformer-inspired architectures with early
fusion of the inputs resulting in an error rate of 0.16%.

</details>


### [25] [Designing a Secure and Resilient Distributed Smartphone Participant Data Collection System](https://arxiv.org/abs/2510.19938)
*Foad Namjoo,Neng Wan,Devan Mallory,Yuyi Chang,Nithin Sugavanam,Long Yin Lee,Ning Xiong,Emre Ertin,Jeff M. Phillips*

Main category: cs.CR

TL;DR: MotionPI是一个基于智能手机的系统，用于通过传感器和调查收集行为和健康数据，具有最小的参与者互动，支持被动数据收集和生态瞬时评估调查。


<details>
  <summary>Details</summary>
Motivation: 现实世界的健康研究需要从移动和可穿戴设备中持续、安全地收集数据。

Method: 系统整合了被动数据收集（如GPS和腕带运动数据）与生态瞬时评估调查，可通过蓝牙低功耗与腕带设备集成，在本地和云端安全存储数据。

Result: MotionPI展示了在现实生活约束下（如有限电池寿命、弱蜂窝连接）进行安全和可扩展移动数据收集的实用解决方案。

Conclusion: MotionPI为网络物理健康研究提供了一个安全且可扩展的移动数据收集解决方案。

Abstract: Real-world health studies require continuous and secure data collection from
mobile and wearable devices. We introduce MotionPI, a smartphone-based system
designed to collect behavioral and health data through sensors and surveys with
minimal interaction from participants. The system integrates passive data
collection (such as GPS and wristband motion data) with Ecological Momentary
Assessment (EMA) surveys, which can be triggered randomly or based on physical
activity. MotionPI is designed to work under real-life constraints, including
limited battery life, weak or intermittent cellular connection, and minimal
user supervision. It stores data both locally and on a secure cloud server,
with encrypted transmission and storage. It integrates through Bluetooth Low
Energy (BLE) into wristband devices that store raw data and communicate motion
summaries and trigger events. MotionPI demonstrates a practical solution for
secure and scalable mobile data collection in cyber-physical health studies.

</details>


### [26] [Q-RAN: Quantum-Resilient O-RAN Architecture](https://arxiv.org/abs/2510.19968)
*Vipin Rathi,Lakshya Chopra,Madhav Agarwal,Nitin Rajput,Kriish Sharma,Sushant Mundepi,Shivam Gangwar,Rudraksh Rawal,Jishan*

Main category: cs.CR

TL;DR: 提出了Q-RAN框架，使用NIST标准化的后量子密码学(PQC)来保护O-RAN网络免受量子计算威胁，包括ML-KEM和ML-DSA算法，结合量子随机数生成器，部署在O-RAN所有接口上。


<details>
  <summary>Details</summary>
Motivation: 电信行业面临双重转型：向O-RAN架构转变和量子计算威胁。O-RAN的多供应商架构扩大了攻击面，易受量子计算机攻击，HNDL攻击策略使威胁迫在眉睫。

Method: 使用NIST标准化的后量子密码学(PQC)，包括ML-KEM(FIPS 203)和ML-DSA(FIPS 204)，结合量子随机数生成器(QRNG)生成密码熵，部署PQ-IPsec、PQ-DTLS和PQ-mTLS协议，在SMO框架内建立集中式后量子证书颁发机构(PQ-CA)。

Result: 开发了全面的量子抵抗安全框架Q-RAN，为O-RAN网络提供量子安全保护，覆盖所有O-RAN接口。

Conclusion: Q-RAN为分解式O-RAN生态系统提供了完整的量子安全路线图，能够有效防御量子对手攻击。

Abstract: The telecommunications industry faces a dual transformation: the
architectural shift toward Open Radio Access Networks (O-RAN) and the emerging
threat from quantum computing. O-RAN disaggregated, multi-vendor architecture
creates a larger attack surface vulnerable to crypt-analytically relevant
quantum computers(CRQCs) that will break current public key cryptography. The
Harvest Now, Decrypt Later (HNDL) attack strategy makes this threat immediate,
as adversaries can intercept encrypted data today for future decryption. This
paper presents Q-RAN, a comprehensive quantum-resistant security framework for
O-RAN networks using NIST-standardized Post-Quantum Cryptography (PQC). We
detail the implementation of ML-KEM (FIPS 203) and ML-DSA (FIPS 204),
integrated with Quantum Random Number Generators (QRNG) for cryptographic
entropy. The solution deploys PQ-IPsec, PQ-DTLS, and PQ-mTLS protocols across
all O-RAN interfaces, anchored by a centralized Post-Quantum Certificate
Authority (PQ-CA) within the SMO framework. This work provides a complete
roadmap for securing disaggregated O-RAN ecosystems against quantum
adversaries.

</details>


### [27] [SecureInfer: Heterogeneous TEE-GPU Architecture for Privacy-Critical Tensors for Large Language Model Deployment](https://arxiv.org/abs/2510.19979)
*Tushar Nayan,Ziqi Zhang,Ruimin Sun*

Main category: cs.CR

TL;DR: SecureInfer是一个混合框架，利用异构可信执行环境(TEEs)-GPU架构来保护LLMs免受模型提取攻击，同时保持高性能推理。


<details>
  <summary>Details</summary>
Motivation: 随着LLMs在移动和边缘平台的部署增加，保护模型隐私同时不牺牲不可信AI加速器性能优势的需求日益迫切。

Method: 采用信息论和威胁感知的分区策略：安全敏感组件在SGX enclave内执行，线性操作在GPU上加密执行并在enclave内安全恢复。

Result: 原型实现显示SecureInfer提供强大的安全保障和合理的性能，为安全设备端模型推理提供实用解决方案。

Conclusion: SecureInfer通过TEEs-GPU混合架构成功解决了模型隐私保护与性能之间的权衡问题。

Abstract: With the increasing deployment of Large Language Models (LLMs) on mobile and
edge platforms, securing them against model extraction attacks has become a
pressing concern. However, protecting model privacy without sacrificing the
performance benefits of untrusted AI accelerators, such as GPUs, presents a
challenging trade-off. In this paper, we initiate the study of high-performance
execution on LLMs and present SecureInfer, a hybrid framework that leverages a
heterogeneous Trusted Execution Environments (TEEs)-GPU architecture to isolate
privacy-critical components while offloading compute-intensive operations to
untrusted accelerators. Building upon an outsourcing scheme, SecureInfer adopts
an information-theoretic and threat-informed partitioning strategy:
security-sensitive components, including non-linear layers, projection of
attention head, FNN transformations, and LoRA adapters, are executed inside an
SGX enclave, while other linear operations (matrix multiplication) are
performed on the GPU after encryption and are securely restored within the
enclave. We implement a prototype of SecureInfer using the LLaMA-2 model and
evaluate it across performance and security metrics. Our results show that
SecureInfer offers strong security guarantees with reasonable performance,
offering a practical solution for secure on-device model inference.

</details>


### [28] [QORE : Quantum Secure 5G/B5G Core](https://arxiv.org/abs/2510.19982)
*Vipin Rathi,Lakshya Chopra,Rudraksh Rawal,Nitin Rajput,Shiva Valia,Madhav Aggarwal,Aditya Gairola*

Main category: cs.CR

TL;DR: QORE是一个量子安全的5G/B5G核心框架，使用NIST标准化的ML-KEM和ML-DSA算法，通过混合PQC配置实现向后兼容，实验验证显示其满足5G系统性能要求。


<details>
  <summary>Details</summary>
Motivation: 当前5G系统使用的RSA、ECC、DH等加密算法都易受Shor算法攻击，保护5G网络免受量子计算威胁已成为紧迫的工程和研究重点。

Method: 采用NIST标准化的基于格的ML-KEM和ML-DSA算法，应用于5G服务架构，并提出结合经典和量子安全原语的混合PQC配置以保持互操作性。

Result: 实验验证表明ML-KEM在实现量子安全的同时只产生轻微性能开销，满足运营商级5G系统的低延迟和高吞吐量要求。

Conclusion: 该路线图与3GPP和NIST的标准化工作保持一致，为缓解量子时代风险、保护网络数据长期机密性和完整性提供了实用指导。

Abstract: Quantum computing is reshaping the security landscape of modern
telecommunications. The cryptographic foundations that secure todays 5G
systems, including RSA, Elliptic Curve Cryptography (ECC), and Diffie-Hellman
(DH), are all susceptible to attacks enabled by Shors algorithm. Protecting 5G
networks against future quantum adversaries has therefore become an urgent
engineering and research priority. In this paper we introduce QORE, a
quantum-secure 5G and Beyond 5G (B5G) Core framework that provides a clear
pathway for transitioning both the 5G Core Network Functions and User Equipment
(UE) to Post-Quantum Cryptography (PQC). The framework uses the
NIST-standardized lattice-based algorithms Module-Lattice Key Encapsulation
Mechanism (ML-KEM) and Module-Lattice Digital Signature Algorithm (ML-DSA) and
applies them across the 5G Service-Based Architecture (SBA). A Hybrid PQC
(HPQC) configuration is also proposed, combining classical and quantum-safe
primitives to maintain interoperability during migration. Experimental
validation shows that ML-KEM achieves quantum security with minor performance
overhead, meeting the low-latency and high-throughput requirements of
carrier-grade 5G systems. The proposed roadmap aligns with ongoing 3GPP SA3 and
SA5 study activities on the security and management of post-quantum networks as
well as with NIST PQC standardization efforts, providing practical guidance for
mitigating quantum-era risks while safeguarding long-term confidentiality and
integrity of network data.

</details>


### [29] [zk-Agreements: A Privacy-Preserving Way to Establish Deterministic Trust in Confidential Agreements](https://arxiv.org/abs/2510.20007)
*To-Wen Liu,Matthew Green*

Main category: cs.CR

TL;DR: zk-agreements协议通过结合零知识证明、安全多方计算和智能合约，在区块链上实现隐私保护的自动化可执行协议，解决传统纸质合同和现有智能合约在隐私与透明度之间的冲突。


<details>
  <summary>Details</summary>
Motivation: 传统纸质合同存在自动化程度低、可执行性差和争议解决效率低的问题，而现有区块链智能合约虽然增强了可执行性，但会暴露敏感协议条款，引发隐私和商业竞争担忧，限制了企业采用。

Method: 设计zk-agreements协议，结合三种技术：零知识证明保护私有协议条款、安全多方计算实现私有合规性评估、智能合约保证自动化执行。

Result: 该协议实现了隐私保护和计算可执行性的双重目标，解决了区块链协议中透明度与机密性之间的根本冲突。

Conclusion: zk-agreements成功地从纸质信任过渡到密码学信任，同时保持机密性，为数字商务提供了可行的隐私保护自动化协议解决方案。

Abstract: Digital transactions currently exceed trillions of dollars annually, yet
traditional paper-based agreements remain a bottleneck for automation,
enforceability, and dispute resolution. Natural language contracts introduce
ambiguity, require manual processing, and lack computational verifiability, all
of which hinder efficient digital commerce. Computable legal contracts,
expressed in machine-readable formats, offer a potential solution by enabling
automated execution and verification. Blockchain-based smart contracts further
strengthen enforceability and accelerate dispute resolution; however, current
implementations risk exposing sensitive agreement terms on public ledgers,
raising serious privacy and competitive intelligence concerns that limit
enterprise adoption.
  We introduce zk-agreements, a protocol designed to transition from
paper-based trust to cryptographic trust while preserving confidentiality. Our
design combines zero-knowledge proofs to protect private agreement terms,
secure two-party computation to enable private compliance evaluation, and smart
contracts to guarantee automated enforcement. Together, these components
achieve both privacy preservation and computational enforceability, resolving
the fundamental tension between transparency and confidentiality in
blockchain-based agreements.

</details>


### [30] [Ultra-Fast Wireless Power Hacking](https://arxiv.org/abs/2510.20056)
*Hui Wang,Hans D. Schotten,Stefan M. Goetz*

Main category: cs.CR

TL;DR: 本文提出了一种改进的无线充电系统攻击方法，能够在高频信号的三个周期内实现能量窃取，无需电路参数或查找表，同步接收器电流与磁传感器电压相位，窃取超过76%的功率。


<details>
  <summary>Details</summary>
Motivation: 电动汽车无线充电技术快速发展，但存在严重网络安全挑战。任何在磁场内的接收器都可能窃取能量，而现有方法需要时间跟踪新频率或精确调整电感和电容，对快速频率变化或电容漂移效果有限。

Method: 增强拦截器功能，使其能够在三个高频信号周期内入侵并窃取能量，无需电路参数或查找表。关键创新是同步接收器电流与磁传感器电压相位，通过MATLAB/Simulink仿真、有限元分析和实验验证。

Result: 改进方法能够在相同条件下窃取完全谐振接收器接收功率的76%以上，证明简单的频率变化功率加密对此类威胁保护有限。

Conclusion: 该攻击表明简单的频率变化功率加密提供的保护有限，无线充电系统需要更强的安全措施来应对此类快速入侵威胁。

Abstract: The rapid growth of electric vehicles (EVs) has driven the development of
roadway wireless charging technology, effectively extending EV driving range.
However, wireless charging introduces significant cybersecurity challenges. Any
receiver within the magnetic field can potentially extract energy, and previous
research demonstrated that a hacker could detect the operating frequency and
steal substantial power. However, our approach required time to track new
frequencies or precise adjustments of inductance and capacitance, which would
be less effective against potential rapid transmitter frequency changes or
capacitance drift. As a solution, we enhanced the interceptor and enabled it to
intrude as well as steal energy within just three cycles of the high-frequency
signal. Moreover, it can work without any circuit parameters or look-up tables.
The key innovation is synchronizing the receiver current with the phase of the
magnetic sensor voltage. Through MATLAB / Simulink simulations, finite-element
analysis, and experimental validation, we demonstrated that our improved method
can steal over 76% of the power received by a fully resonant receiver under
identical conditions. This attack demonstrates that simple frequency-changing
power encryption offers limited protection against such threats.

</details>


### [31] [Who Coordinates U.S. Cyber Defense? A Co-Authorship Network Analysis of Joint Cybersecurity Advisories (2024--2025)](https://arxiv.org/abs/2510.20080)
*M. Abdullah Canbaz,Hakan Otal,Tugce Unlu,Nour Alhussein,Brian Nussbaum*

Main category: cs.CR

TL;DR: 通过分析9份联合网络安全咨询报告构建合作网络，发现CISA、FBI和NSA构成紧密的美国核心三角，与五眼联盟和欧洲盟友高度连接。


<details>
  <summary>Details</summary>
Motivation: 网络威胁需要联合应对，但多机构网络安全合作的组织动态仍不清楚，了解领导机构、桥梁机构和协调方式对加强美国国土安全和盟国防务至关重要。

Method: 构建2024年11月至2025年8月期间9份联合网络安全咨询报告(CSAs)的合著网络，映射41个机构和442个合著关系来分析合作结构。

Result: 发现紧密的美国三角(CISA、FBI、NSA)与五眼联盟和欧洲盟友密集连接；度中心性显示CISA和FBI是协调枢纽，中介中心性显示NSA、英国NCSC和澳大利亚ASD-ACSC是关键桥梁。

Conclusion: 通过发布首个可复制的CSA数据集和网络分析，为网络安全合作信号的组织方式和战略影响力集中位置提供了新的实证证据。

Abstract: Cyber threats increasingly demand joint responses, yet the organizational
dynamics behind multi-agency cybersecurity collaboration remain poorly
understood. Understanding who leads, who bridges, and how agencies coordinate
is critical for strengthening both U.S. homeland security and allied defense
efforts. In this study, we construct a co-authorship network from nine Joint
Cybersecurity Advisories (CSAs) issued between November 2024 and August 2025.
We map 41 agencies and 442 co-authoring ties to analyze the structure of
collaboration. We find a tightly knit U.S. triad -- CISA, FBI, and NSA --
densely connected with Five Eyes and select European allies. Degree centrality
identifies CISA and FBI as coordination hubs, while betweenness highlights NSA,
the UK's NCSC, and Australia's ASD-ACSC as key bridges linking otherwise
fragmented clusters. By releasing the first replicable dataset and network
analysis of CSAs, we provide new empirical evidence on how collaborative
cybersecurity signals are organized and where strategic influence is
concentrated.

</details>


### [32] [SAID: Empowering Large Language Models with Self-Activating Internal Defense](https://arxiv.org/abs/2510.20129)
*Yulong Chen,Yadong Liu,Jiawen Zhang,Mu Li,Chao Huang,Jie Wen*

Main category: cs.CR

TL;DR: SAID是一种无需训练的LLM防御方法，通过激活模型内部安全机制来对抗越狱攻击，在保持良性任务性能的同时显著减少有害输出。


<details>
  <summary>Details</summary>
Motivation: 现有LLM安全防御方法依赖外部干预，缺乏通用性且影响模型效用，计算开销大。需要一种更稳健、可扩展的内部防御机制。

Method: 采用三阶段流程：模型原生意图提取、最优安全前缀探测、保守聚合策略，利用LLM自身推理能力识别和消除恶意意图。

Result: 在5个开源LLM和6种先进越狱攻击上的实验表明，SAID在减少有害输出方面显著优于现有防御方法，同时保持良性任务性能且计算开销最小。

Conclusion: 激活LLM内在安全机制是构建更安全、可靠对齐AI系统的更稳健和可扩展路径。

Abstract: Large Language Models (LLMs), despite advances in safety alignment, remain
vulnerable to jailbreak attacks designed to circumvent protective mechanisms.
Prevailing defense strategies rely on external interventions, such as input
filtering or output modification, which often lack generalizability and
compromise model utility while incurring significant computational overhead. In
this work, we introduce a new, training-free defense paradigm, Self-Activating
Internal Defense (SAID), which reframes the defense task from external
correction to internal capability activation. SAID uniquely leverages the LLM's
own reasoning abilities to proactively identify and neutralize malicious intent
through a three-stage pipeline: model-native intent distillation to extract
core semantics, optimal safety prefix probing to activate latent safety
awareness, and a conservative aggregation strategy to ensure robust
decision-making. Extensive experiments on five open-source LLMs against six
advanced jailbreak attacks demonstrate that SAID substantially outperforms
state-of-the-art defenses in reducing harmful outputs. Crucially, it achieves
this while preserving model performance on benign tasks and incurring minimal
computational overhead. Our work establishes that activating the intrinsic
safety mechanisms of LLMs is a more robust and scalable path toward building
safer and more reliable aligned AI systems.

</details>


### [33] [Learning to Triage Taint Flows Reported by Dynamic Program Analysis in Node.js Packages](https://arxiv.org/abs/2510.20739)
*Ronghao Ni,Aidan Z. H. Yang,Min-Chien Hsu,Nuno Sabino,Limin Jia,Ruben Martins,Darion Cassel,Kevin Cheang*

Main category: cs.CR

TL;DR: 该论文研究如何利用机器学习对程序分析工具报告的漏洞进行优先级排序，通过评估多种机器学习方法在Node.js包上的表现，发现最佳模型能显著减少人工审查工作量。


<details>
  <summary>Details</summary>
Motivation: 程序分析工具产生大量候选漏洞报告需要人工审查，成本高昂。需要一种方法能够优先处理最可能是真实漏洞的报告。

Method: 收集1,883个Node.js包作为基准数据集，每个包含一个报告的ACE或ACI漏洞。评估多种机器学习方法：经典模型、图神经网络(GNNs)、大语言模型(LLMs)以及GNN和LLM的混合模型，基于动态程序分析工具输出的数据进行训练。

Result: 最佳LLM模型F1分数达到0.915，最佳GNN和经典ML模型F1分数为0.904。在低于7%假阴性率下，领先模型可从人工审查中消除66.9%的良性包，每个包处理时间约60毫秒。当最佳模型调整到精度0.8时，能检测99.2%的可利用污点流，仅遗漏0.8%。

Conclusion: 机器学习方法在漏洞优先级排序方面表现出强大潜力，能够显著减少人工审查工作量，同时保持高检测率和低误报率，适用于实际的漏洞分类工作。

Abstract: Program analysis tools often produce large volumes of candidate vulnerability
reports that require costly manual review, creating a practical challenge: how
can security analysts prioritize the reports most likely to be true
vulnerabilities?
  This paper investigates whether machine learning can be applied to
prioritizing vulnerabilities reported by program analysis tools. We focus on
Node.js packages and collect a benchmark of 1,883 Node.js packages, each
containing one reported ACE or ACI vulnerability. We evaluate a variety of
machine learning approaches, including classical models, graph neural networks
(GNNs), large language models (LLMs), and hybrid models that combine GNN and
LLMs, trained on data based on a dynamic program analysis tool's output. The
top LLM achieves $F_{1} {=} 0.915$, while the best GNN and classical ML models
reaching $F_{1} {=} 0.904$. At a less than 7% false-negative rate, the leading
model eliminates 66.9% of benign packages from manual review, taking around 60
ms per package. If the best model is tuned to operate at a precision level of
0.8 (i.e., allowing 20% false positives amongst all warnings), our approach can
detect 99.2% of exploitable taint flows while missing only 0.8%, demonstrating
strong potential for real-world vulnerability triage.

</details>


### [34] [Separating Pseudorandom Generators from Logarithmic Pseudorandom States](https://arxiv.org/abs/2510.20131)
*Mohammed Barhoush*

Main category: cs.CR

TL;DR: 本文建立了PRG与PRS之间的量子黑盒分离，证明在量子可评估的PRG与对数或线性大小的PRS之间不存在黑盒构造关系。


<details>
  <summary>Details</summary>
Motivation: 解决PRG是否与对数PRS分离的开放问题，因为对数PRS已实现数字签名和量子公钥加密等强大应用，而线性PRS未能实现。

Method: 构造一个具有逆访问权限的酉量子预言机，证明在该预言机下不存在从PRS到PRG的黑盒构造。

Result: 成功建立了PRG与两种大小PRS的量子黑盒分离，并作为推论获得了PRG与数字签名、量子公钥加密等原语的分离。

Conclusion: PRG与PRS在量子黑盒模型下是分离的，这为理解量子密码学中不同原语之间的关系提供了重要洞见。

Abstract: Pseudorandom generators (PRGs) are a foundational primitive in classical
cryptography, underpinning a wide range of constructions. In the quantum
setting, pseudorandom quantum states (PRSs) were proposed as a potentially
weaker assumption that might serve as a substitute for PRGs in cryptographic
applications. Two primary size regimes of PRSs have been studied:
logarithmic-size and linear-size. Interestingly, logarithmic PRSs have led to
powerful cryptographic applications, such as digital signatures and quantum
public-key encryption, that have not been realized from their linear
counterparts. However, PRGs have only been black-box separated from linear
PRSs, leaving open the fundamental question of whether PRGs are also separated
from logarithmic PRSs.
  In this work, we resolve this open problem. We establish a quantum black-box
separation between (quantum-evaluable) PRGs and PRSs of either size regime.
Specifically, we construct a unitary quantum oracle with inverse access
relative to which no black-box construction of PRG from (logarithmic or linear)
PRS exists. As a direct corollary, we obtain separations between PRGs and
several primitives implied by logarithmic PRSs, including digital signatures
and quantum public-key encryption.

</details>


### [35] [Beyond Text: Multimodal Jailbreaking of Vision-Language and Audio Models through Perceptually Simple Transformations](https://arxiv.org/abs/2510.20223)
*Divyanshu Kumar,Shreyas Jena,Nitin Aravind Birur,Tanay Baswa,Sahil Agarwal,Prashanth Harshangi*

Main category: cs.CR

TL;DR: 该论文系统研究了针对多模态大语言模型的越狱攻击，发现即使简单的感知变换也能可靠绕过最先进的安全过滤器，在多模态输入下攻击成功率可达89%，暴露了当前文本中心对齐与多模态威胁之间的严重安全漏洞。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型虽然取得了显著进展，但在跨模态处理方面存在严重安全漏洞，现有安全措施无法有效抵御针对视觉-语言和音频-语言模型的对抗攻击，需要系统评估这些威胁。

Method: 采用系统化方法评估了1,900个对抗提示，涵盖三个高风险安全类别，测试了七种前沿模型。探索了FigStep-Pro（视觉关键词分解）、Intelligent Masking（语义混淆）和音频扰动（Wave-Echo、Wave-Pitch、Wave-Speed）等攻击技术的有效性。

Result: 发现严重漏洞：文本安全近乎完美的模型（0%攻击成功率）在感知修改输入下攻击成功率超过75%，FigStep-Pro在Llama-4变体中达到89%攻击成功率。音频攻击揭示了供应商特定弱点，即使是基本的模态转移也能在技术查询中获得25%攻击成功率。

Conclusion: 当前安全措施无法泛化到跨模态攻击，这些攻击的可访问性表明，稳健的多模态AI安全需要向更广泛的语义级推理范式转变，以减轻潜在风险。

Abstract: Multimodal large language models (MLLMs) have achieved remarkable progress,
yet remain critically vulnerable to adversarial attacks that exploit weaknesses
in cross-modal processing. We present a systematic study of multimodal
jailbreaks targeting both vision-language and audio-language models, showing
that even simple perceptual transformations can reliably bypass
state-of-the-art safety filters. Our evaluation spans 1,900 adversarial prompts
across three high-risk safety categories harmful content, CBRN (Chemical,
Biological, Radiological, Nuclear), and CSEM (Child Sexual Exploitation
Material) tested against seven frontier models. We explore the effectiveness of
attack techniques on MLLMs, including FigStep-Pro (visual keyword
decomposition), Intelligent Masking (semantic obfuscation), and audio
perturbations (Wave-Echo, Wave-Pitch, Wave-Speed). The results reveal severe
vulnerabilities: models with almost perfect text-only safety (0\% ASR) suffer
>75\% attack success under perceptually modified inputs, with FigStep-Pro
achieving up to 89\% ASR in Llama-4 variants. Audio-based attacks further
uncover provider-specific weaknesses, with even basic modality transfer
yielding 25\% ASR for technical queries. These findings expose a critical gap
between text-centric alignment and multimodal threats, demonstrating that
current safeguards fail to generalize across cross-modal attacks. The
accessibility of these attacks, which require minimal technical expertise,
suggests that robust multimodal AI safety will require a paradigm shift toward
broader semantic-level reasoning to mitigate possible risks.

</details>


### [36] [HHEML: Hybrid Homomorphic Encryption for Privacy-Preserving Machine Learning on Edge](https://arxiv.org/abs/2510.20243)
*Yu Hin Chan,Hao Yang,Shiyu Shen,Xingyu Fan,Shengzhe Lyu,Patrick S. Y. Hung,Ray C. C. Cheung*

Main category: cs.CR

TL;DR: 提出了一种硬件加速的混合同态加密架构，通过优化的对称密码算法和专用硬件加速器，显著降低了边缘设备上的加密延迟和功耗。


<details>
  <summary>Details</summary>
Motivation: 完全同态加密在隐私保护机器学习中计算开销过大，不适用于边缘设备。混合同态加密结合对称加密和FHE可以降低客户端计算成本，但需要硬件加速来进一步提高性能。

Method: 设计了一个硬件加速的HHE架构，围绕针对FHE兼容性优化的轻量级对称密码算法，实现为专用硬件加速器，并集成到完整的PPML流程中。

Result: 在PYNQ-Z2平台上使用MNIST数据集进行实验，客户端加密延迟降低50倍以上，硬件吞吐量比现有FPGA加速器提升近2倍。

Conclusion: 验证了低功耗硬件加速HHE在边缘部署的可行性，为资源受限环境中构建可扩展的安全机器学习系统提供了硬件-软件协同设计方法。

Abstract: Privacy-preserving machine learning (PPML) is an emerging topic to handle
secure machine learning inference over sensitive data in untrusted
environments. Fully homomorphic encryption (FHE) enables computation directly
on encrypted data on the server side, making it a promising approach for PPML.
However, it introduces significant communication and computation overhead on
the client side, making it impractical for edge devices. Hybrid homomorphic
encryption (HHE) addresses this limitation by combining symmetric encryption
(SE) with FHE to reduce the computational cost on the client side, and
combining with an FHE-friendly SE can also lessen the processing overhead on
the server side, making it a more balanced and efficient alternative. Our work
proposes a hardware-accelerated HHE architecture built around a lightweight
symmetric cipher optimized for FHE compatibility and implemented as a dedicated
hardware accelerator. To the best of our knowledge, this is the first design to
integrate an end-to-end HHE framework with hardware acceleration. Beyond this,
we also present several microarchitectural optimizations to achieve higher
performance and energy efficiency. The proposed work is integrated into a full
PPML pipeline, enabling secure inference with significantly lower latency and
power consumption than software implementations. Our contributions validate the
feasibility of low-power, hardware- accelerated HHE for edge deployment and
provide a hardware- software co-design methodology for building scalable,
secure machine learning systems in resource-constrained environments.
Experiments on a PYNQ-Z2 platform with the MNIST dataset show over a 50x
reduction in client-side encryption latency and nearly a 2x gain in hardware
throughput compared to existing FPGA-based HHE accelerators.

</details>


### [37] [Privacy Protection of Automotive Location Data Based on Format-Preserving Encryption of Geographical Coordinates](https://arxiv.org/abs/2510.20300)
*Haojie Ji,Long Jin,Haowen Li,Chongshi Xin,Te Hu*

Main category: cs.CR

TL;DR: 提出基于格式保持加密(FPE)的高精度汽车位置数据隐私保护机制，通过坐标数据密钥映射减少重复加密解密造成的地理位置精度损失。


<details>
  <summary>Details</summary>
Motivation: 汽车位置数据共享存在隐私泄露风险，可能导致用户行为识别、敏感位置推断和轨迹重建等攻击，需要保护数据安全。

Method: 设计汽车坐标数据密钥映射机制，采用格式保持加密技术对地理坐标进行加密处理。

Result: 平均相对距离保持率达到0.0844，关键区域热点数量减少98.9%，解密坐标数据恢复精度达到100%。

Conclusion: 该方法为自动驾驶中的敏感数据共享提供了高效的数据安全解决方案。

Abstract: There are increasing risks of privacy disclosure when sharing the automotive
location data in particular functions such as route navigation, driving
monitoring and vehicle scheduling. These risks could lead to the attacks
including user behavior recognition, sensitive location inference and
trajectory reconstruction. In order to mitigate the data security risk caused
by the automotive location sharing, this paper proposes a high-precision
privacy protection mechanism based on format-preserving encryption (FPE) of
geographical coordinates. The automotive coordinate data key mapping mechanism
is designed to reduce to the accuracy loss of the geographical location data
caused by the repeated encryption and decryption. The experimental results
demonstrate that the average relative distance retention rate (RDR) reached
0.0844, and the number of hotspots in the critical area decreased by 98.9%
after encryption. To evaluate the accuracy loss of the proposed encryption
algorithm on automotive geographical location data, this paper presents the
experimental analysis of decryption accuracy, and the result indicates that the
decrypted coordinate data achieves a restoration accuracy of 100%. This work
presents a high-precision privacy protection method for automotive location
data, thereby providing an efficient data security solution for the sensitive
data sharing in autonomous driving.

</details>


### [38] [Enhancing Security in Deep Reinforcement Learning: A Comprehensive Survey on Adversarial Attacks and Defenses](https://arxiv.org/abs/2510.20314)
*Wu Yichao,Wang Yirui,Ding Panpan,Wang Hailong,Zhu Bingqian,Liu Chun*

Main category: cs.CR

TL;DR: 本文综述了深度强化学习在对抗环境中的安全挑战，提出了基于扰动类型和攻击目标的对抗攻击分类框架，系统总结了各种鲁棒性训练策略和防御技术，并展望了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 随着DRL在自动驾驶、智能制造等安全敏感领域的广泛应用，提高其在动态多变环境中的安全性和鲁棒性成为核心问题。面对对抗攻击，DRL可能出现严重性能下降甚至危险决策，确保其稳定性至关重要。

Method: 提出基于扰动类型和攻击目标的对抗攻击分类框架，详细回顾针对DRL的主流对抗攻击方法，包括状态空间、动作空间、奖励函数和模型空间等多种攻击方式。系统总结当前鲁棒性训练策略，如对抗训练、竞争训练、鲁棒学习、对抗检测、防御蒸馏等防御技术。

Result: 建立了系统的对抗攻击分类体系，全面梳理了各类攻击方法和防御策略，分析了不同方法在提升DRL鲁棒性方面的优缺点。

Conclusion: 未来DRL在对抗环境中的研究应重点关注提高泛化能力、降低计算复杂度、增强可扩展性和可解释性，为研究者提供有价值的参考和方向。

Abstract: With the wide application of deep reinforcement learning (DRL) techniques in
complex fields such as autonomous driving, intelligent manufacturing, and smart
healthcare, how to improve its security and robustness in dynamic and
changeable environments has become a core issue in current research. Especially
in the face of adversarial attacks, DRL may suffer serious performance
degradation or even make potentially dangerous decisions, so it is crucial to
ensure their stability in security-sensitive scenarios. In this paper, we first
introduce the basic framework of DRL and analyze the main security challenges
faced in complex and changing environments. In addition, this paper proposes an
adversarial attack classification framework based on perturbation type and
attack target and reviews the mainstream adversarial attack methods against DRL
in detail, including various attack methods such as perturbation state space,
action space, reward function and model space. To effectively counter the
attacks, this paper systematically summarizes various current robustness
training strategies, including adversarial training, competitive training,
robust learning, adversarial detection, defense distillation and other related
defense techniques, we also discuss the advantages and shortcomings of these
methods in improving the robustness of DRL. Finally, this paper looks into the
future research direction of DRL in adversarial environments, emphasizing the
research needs in terms of improving generalization, reducing computational
complexity, and enhancing scalability and explainability, aiming to provide
valuable references and directions for researchers.

</details>


### [39] [GhostEI-Bench: Do Mobile Agents Resilience to Environmental Injection in Dynamic On-Device Environments?](https://arxiv.org/abs/2510.20333)
*Chiyu Chen,Xinhao Song,Yunkai Chai,Yang Yao,Haodong Zhao,Lijun Li,Jie Li,Yan Teng,Gongshen Liu,Yingchun Wang*

Main category: cs.CR

TL;DR: 该论文提出了GhostEI-Bench基准，用于评估移动视觉语言模型在环境注入攻击下的脆弱性，这种攻击通过在GUI中插入对抗性UI元素来破坏模型的视觉感知。


<details>
  <summary>Details</summary>
Motivation: 随着视觉语言模型被部署为自主代理来导航移动图形用户界面，它们在动态设备生态系统中面临环境注入这一独特且未被充分探索的威胁向量，可能导致隐私泄露、财务损失或不可逆的设备损害。

Method: 引入GhostEI-Bench基准，在可执行的Android模拟器中向现实应用工作流注入对抗性事件，并提出了法官LLM协议来对代理的行动轨迹和截图序列进行细粒度失败分析。

Result: 对最先进代理的综合实验显示，当前模型对欺骗性环境线索表现出明显的脆弱性，系统性地无法感知和推理被操纵的用户界面。

Conclusion: GhostEI-Bench为量化和缓解这一新兴威胁提供了框架，为开发更鲁棒和安全的具身代理铺平了道路。

Abstract: Vision-Language Models (VLMs) are increasingly deployed as autonomous agents
to navigate mobile graphical user interfaces (GUIs). Operating in dynamic
on-device ecosystems, which include notifications, pop-ups, and inter-app
interactions, exposes them to a unique and underexplored threat vector:
environmental injection. Unlike prompt-based attacks that manipulate textual
instructions, environmental injection corrupts an agent's visual perception by
inserting adversarial UI elements (for example, deceptive overlays or spoofed
notifications) directly into the GUI. This bypasses textual safeguards and can
derail execution, causing privacy leakage, financial loss, or irreversible
device compromise. To systematically evaluate this threat, we introduce
GhostEI-Bench, the first benchmark for assessing mobile agents under
environmental injection attacks within dynamic, executable environments. Moving
beyond static image-based assessments, GhostEI-Bench injects adversarial events
into realistic application workflows inside fully operational Android emulators
and evaluates performance across critical risk scenarios. We further propose a
judge-LLM protocol that conducts fine-grained failure analysis by reviewing the
agent's action trajectory alongside the corresponding screenshot sequence,
pinpointing failure in perception, recognition, or reasoning. Comprehensive
experiments on state-of-the-art agents reveal pronounced vulnerability to
deceptive environmental cues: current models systematically fail to perceive
and reason about manipulated UIs. GhostEI-Bench provides a framework for
quantifying and mitigating this emerging threat, paving the way toward more
robust and secure embodied agents.

</details>


### [40] [NeuPerm: Disrupting Malware Hidden in Neural Network Parameters by Leveraging Permutation Symmetry](https://arxiv.org/abs/2510.20367)
*Daniel Gilkarov,Ran Dubin*

Main category: cs.CR

TL;DR: NeuPerm是一种利用神经网络排列对称性来破坏隐藏在模型参数中的恶意软件的方法，对模型性能影响极小，能有效防御最先进的攻击。


<details>
  <summary>Details</summary>
Motivation: 预训练模型共享虽然降低了深度学习应用成本，但存在安全风险，攻击者可能将自执行恶意软件隐藏在神经网络参数中传播。

Method: 利用神经网络排列对称性的理论特性，通过参数排列来破坏隐藏的恶意代码，同时保持模型性能基本不变。

Result: NeuPerm成功破坏了最先进的攻击，包括LLM上的攻击，这是之前类似工作未能实现的。与需要复杂量化过程的防御方法相比，NeuPerm更简单有效。

Conclusion: NeuPerm提供了一种简单而有效的防御机制，能够在不影响模型性能的情况下破坏隐藏在神经网络参数中的恶意软件，为模型共享安全提供了新的解决方案。

Abstract: Pretrained deep learning model sharing holds tremendous value for researchers
and enterprises alike. It allows them to apply deep learning by fine-tuning
models at a fraction of the cost of training a brand-new model. However, model
sharing exposes end-users to cyber threats that leverage the models for
malicious purposes. Attackers can use model sharing by hiding self-executing
malware inside neural network parameters and then distributing them for
unsuspecting users to unknowingly directly execute them, or indirectly as a
dependency in another software. In this work, we propose NeuPerm, a simple yet
effec- tive way of disrupting such malware by leveraging the theoretical
property of neural network permutation symmetry. Our method has little to no
effect on model performance at all, and we empirically show it successfully
disrupts state-of-the-art attacks that were only previously addressed using
quantization, a highly complex process. NeuPerm is shown to work on LLMs, a
feat that no other previous similar works have achieved. The source code is
available at https://github.com/danigil/NeuPerm.git.

</details>


### [41] [MAC Aggregation over Lossy Channels in DTLS 1.3](https://arxiv.org/abs/2510.20419)
*Eric Wagner,David Heye,Jan Bauer,Klaus Wehrle,Martin Serror*

Main category: cs.CR

TL;DR: 该论文研究了在真实部署环境中MAC聚合的实际效果，发现即使在有损信道中，MAC聚合仍能提高50%的吞吐量并节省17%的能耗。


<details>
  <summary>Details</summary>
Motivation: 研究MAC聚合在无线有损环境中的实际性能，验证之前基于模拟的假设是否成立。

Method: 设计了DTLS 1.3协议的MAC聚合扩展，并在嵌入式硬件上对完整通信协议栈进行广泛性能评估。

Result: MAC聚合在有损信道中仍能将短消息传输的吞吐量提高50%，能耗节省17%。

Conclusion: MAC聚合在真实无线部署中确实能带来显著的带宽和能耗优势，即使在有损环境下。

Abstract: Aggregating Message Authentication Codes (MACs) promises to save valuable
bandwidth in resource-constrained environments. The idea is simple: Instead of
appending an authentication tag to each message in a communication stream, the
integrity protection of multiple messages is aggregated into a single tag.
Recent studies postulate, e.g., based on simulations, that these benefits also
spread to wireless, and thus lossy, scenarios despite each lost packet
typically resulting in the loss of integrity protection information for
multiple messages. In this paper, we investigate these claims in a real
deployment. Therefore, we first design a MAC aggregation extension for the
Datagram Transport Layer Security (DTLS) 1.3 protocol. Afterward, we
extensively evaluate the performance of MAC aggregation on a complete
communication protocol stack on embedded hardware. We find that MAC aggregation
can indeed increase goodput by up to 50% and save up to 17% of energy
expenditure for the transmission of short messages, even in lossy channels.

</details>


### [42] [On the cybersecurity of LoRaWAN-based system: a Smart-Lighting case study](https://arxiv.org/abs/2510.20494)
*Florian Hofer,Barbara Russo*

Main category: cs.CR

TL;DR: 对基于LoRaWAN的智能照明项目进行架构配置实验研究，评估系统对抗攻击的鲁棒性。结果显示大多数攻击失败，但近距离高功率干扰攻击有效，可通过网关冗余缓解。


<details>
  <summary>Details</summary>
Motivation: 工业4.0中的信息物理系统和物联网面临网络安全挑战，需要验证和提升智能系统架构的鲁棒性。

Method: 采用迭代实验方法，在实验室和现场环境中测试LoRaWAN智能照明系统的不同架构配置，评估其对抗各种攻击的能力。

Result: 大多数针对LoRaWAN网络的攻击不成功，但发现产品存在未解决的问题。最成功的攻击是近距离高功率干扰攻击，可通过网关冗余来缓解。

Conclusion: LoRaWAN网络整体具有较好的安全性，但需要关注近距离干扰攻击的防护，网关冗余是有效的缓解措施。

Abstract: Cyber-physical systems and the Internet of Things (IoT) are key technologies
in the Industry 4.0 vision. They incorporate sensors and actuators to interact
with the physical environment. However, when creating and interconnecting
components to form a heterogeneous smart systems architecture, these face
challenges in cybersecurity. This paper presents an experimental investigation
of architectural configurations for a LoRaWAN-based Smart-Lighting project,
aimed at verifying and improving the system's robustness against attacks. We
assess the system's robustness in a series of iterative experiments conducted
both in-vitro and on-site. The results show that most attacks on a LoRaWAN
network are unsuccessful, also highlighting unresolved issues with the
installed products. The most successful attacks are high-power jamming attacks
within a few meters of the target, which, in the case of gateways, can be
mitigated through gateway redundancy.

</details>


### [43] [AdaDoS: Adaptive DoS Attack via Deep Adversarial Reinforcement Learning in SDN](https://arxiv.org/abs/2510.20566)
*Wei Shao,Yuhao Wang,Rongguang He,Muhammad Ejaz Ahmed,Seyit Camtepe*

Main category: cs.CR

TL;DR: AdaDoS是一种基于对抗性强化学习的自适应DoS攻击模型，能够动态调整攻击策略以规避现有检测机制，包括基于机器学习和规则基础的DoS攻击检测器。


<details>
  <summary>Details</summary>
Motivation: 现有防御机制对基于规则的DoS攻击有效，但AI驱动技术对SDN安全构成新挑战。需要开发能够规避检测的自适应攻击模型来评估和改进防御能力。

Method: 将问题建模为攻击者与检测器之间的竞争博弈，使用对抗性强化学习动态调整攻击策略。将DoS攻击建模为部分可观测马尔可夫决策过程，并采用师生互学习模块，学生代理通过向具有完整观测能力的教师代理学习来提升性能。

Result: AdaDoS能够生成自适应DoS攻击序列，成功规避基于机器学习和规则基础的检测器，展示了现有防御机制的脆弱性。

Conclusion: AdaDoS是首个将强化学习应用于开发DoS攻击序列的研究，证明了自适应攻击对现有防御机制的威胁，强调了需要开发更强大的自适应防御系统。

Abstract: Existing defence mechanisms have demonstrated significant effectiveness in
mitigating rule-based Denial-of-Service (DoS) attacks, leveraging predefined
signatures and static heuristics to identify and block malicious traffic.
However, the emergence of AI-driven techniques presents new challenges to SDN
security, potentially compromising the efficacy of existing defence mechanisms.
In this paper, we introduce~AdaDoS, an adaptive attack model that disrupt
network operations while evading detection by existing DoS-based detectors
through adversarial reinforcement learning (RL). Specifically, AdaDoS models
the problem as a competitive game between an attacker, whose goal is to
obstruct network traffic without being detected, and a detector, which aims to
identify malicious traffic. AdaDoS can solve this game by dynamically adjusting
its attack strategy based on feedback from the SDN and the detector.
Additionally, recognising that attackers typically have less information than
defenders, AdaDoS formulates the DoS-like attack as a partially observed Markov
decision process (POMDP), with the attacker having access only to delay
information between attacker and victim nodes. We address this challenge with a
novel reciprocal learning module, where the student agent, with limited
observations, enhances its performance by learning from the teacher agent, who
has full observational capabilities in the SDN environment. AdaDoS represents
the first application of RL to develop DoS-like attack sequences, capable of
adaptively evading both machine learning-based and rule-based DoS-like attack
detectors.

</details>


### [44] [Decentralized Exchange that Mitigate a Bribery Attack](https://arxiv.org/abs/2510.20645)
*Nitin Awathare*

Main category: cs.CR

TL;DR: 本文分析了HTLC协议的安全漏洞，提出了新的贿赂攻击方法，并设计了一个抗贿赂的安全HTLC协议。


<details>
  <summary>Details</summary>
Motivation: 现有HTLC解决方案（如MAD-HTLC和He-HTLC）虽然通过利用矿工逐利行为来缓解攻击，但只考虑了两种贿赂场景，存在安全漏洞。

Method: 提出了矿工合谋贿赂攻击，并对现有协议进行博弈论分析。然后设计了\prot协议，采用两阶段方法防止第三方（如矿工）未经授权没收代币。

Result: 发现了现有解决方案的漏洞，提出了比He-HTLC更强的MAD-HTLC攻击方法，攻击者可获得与攻击原生HTLC相当的利润。\prot协议在比特币和以太坊上实现，证明其在交易成本和延迟方面高效。

Conclusion: \prot协议是博弈论上安全的HTLC协议，能够抵抗所有贿赂场景，为HTLC应用提供了更强的安全保障。

Abstract: Despite the popularity of Hashed Time-Locked Contracts (HTLCs) because of
their use in wide areas of applications such as payment channels, atomic swaps,
etc, their use in exchange is still questionable. This is because of its
incentive incompatibility and susceptibility to bribery attacks.
  State-of-the-art solutions such as MAD-HTLC (Oakland'21) and He-HTLC
(NDSS'23) address this by leveraging miners' profit-driven behaviour to
mitigate such attacks. The former is the mitigation against passive miners;
however, the latter works against both active and passive miners. However, they
consider only two bribing scenarios where either of the parties involved in the
transfer collude with the miner.
  In this paper, we expose vulnerabilities in state-of-the-art solutions by
presenting a miner-collusion bribery attack with implementation and
game-theoretic analysis. Additionally, we propose a stronger attack on MAD-HTLC
than He-HTLC, allowing the attacker to earn profits equivalent to attacking
naive HTLC.
  Leveraging our insights, we propose \prot, a game-theoretically secure HTLC
protocol resistant to all bribery scenarios. \prot\ employs a two-phase
approach, preventing unauthorized token confiscation by third parties, such as
miners. In Phase 1, parties commit to the transfer; in Phase 2, the transfer is
executed without manipulation. We demonstrate \prot's efficiency in transaction
cost and latency via implementations on Bitcoin and Ethereum.

</details>


### [45] [Risk Psychology & Cyber-Attack Tactics](https://arxiv.org/abs/2510.20657)
*Rubens Kim,Stephan Carney,Yvonne Fonken,Soham Hans,Sofia Hirschmann,Stacy Marsella,Peggy Wu,Nikolos Gurney*

Main category: cs.CR

TL;DR: 研究发现认知过程能预测网络攻击行为，认知差异比训练或经验更能影响攻击技术选择。


<details>
  <summary>Details</summary>
Motivation: 探讨认知过程是否能够预测网络攻击行为，了解个体认知差异在网络安全攻击中的作用。

Method: 使用多层次混合效应泊松回归分析，将技术使用次数嵌套在参与者内，分析心理测量量表数据与标记的攻击行为数据。

Result: 量表显著预测了技术使用，但效果因技术而异；专业水平和实验条件对技术模式无显著预测作用。

Conclusion: 个体认知差异塑造网络攻击行为，支持开发基于心理学的防御策略。

Abstract: We examine whether measured cognitive processes predict cyber-attack
behavior. We analyzed data that included psychometric scale responses and
labeled attack behaviors from cybersecurity professionals who conducted
red-team operations against a simulated enterprise network. We employed
multilevel mixed-effects Poisson regression with technique counts nested within
participants to test whether cognitive processes predicted technique-specific
usage. The scales significantly predicted technique use, but effects varied by
technique rather than operating uniformly. Neither expertise level nor
experimental treatment condition significantly predicted technique patterns,
indicating that cognitive processes may be stronger drivers of technique
selection than training or experience. These findings demonstrate that
individual cognitive differences shape cyber-attack behavior and support the
development of psychology-informed defense strategies.

</details>


### [46] [RAGRank: Using PageRank to Counter Poisoning in CTI LLM Pipelines](https://arxiv.org/abs/2510.20768)
*Austin Jia,Avaneesh Ramesh,Zain Shamsi,Daniel Zhang,Alex Liu*

Main category: cs.CR

TL;DR: 本文提出在检索增强生成(RAG)系统中应用PageRank等源可信度算法来防御中毒攻击，特别是在网络威胁情报(CTI)场景下，通过降低恶意文档的权威分数来提升系统安全性。


<details>
  <summary>Details</summary>
Motivation: RAG已成为网络威胁情报系统中部署大语言模型的主流架构，但容易受到中毒攻击。现有防御措施在CTI场景下可能失效，因为网络威胁信息往往是全新的，且复杂威胁行为者能够模仿合法格式和术语。

Method: 提出在语料库上应用源可信度算法（以PageRank为例）来加速现代RAG防御的鲁棒性，通过计算文档权威分数来区分可信和恶意内容。

Result: 在标准化MS MARCO数据集上的实验定量表明，该算法能够降低恶意文档的权威分数同时提升可信内容；在CTI文档和源上的概念验证也显示了良好性能。

Conclusion: 源可信度算法能够有效增强RAG系统在CTI场景下的安全性，为防御中毒攻击提供了新的解决方案。

Abstract: Retrieval-Augmented Generation (RAG) has emerged as the dominant
architectural pattern to operationalize Large Language Model (LLM) usage in
Cyber Threat Intelligence (CTI) systems. However, this design is susceptible to
poisoning attacks, and previously proposed defenses can fail for CTI contexts
as cyber threat information is often completely new for emerging attacks, and
sophisticated threat actors can mimic legitimate formats, terminology, and
stylistic conventions. To address this issue, we propose that the robustness of
modern RAG defenses can be accelerated by applying source credibility
algorithms on corpora, using PageRank as an example. In our experiments, we
demonstrate quantitatively that our algorithm applies a lower authority score
to malicious documents while promoting trusted content, using the standardized
MS MARCO dataset. We also demonstrate proof-of-concept performance of our
algorithm on CTI documents and feeds.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [47] [A Quantum-Inspired Algorithm for Solving Sudoku Puzzles and the MaxCut Problem](https://arxiv.org/abs/2510.19835)
*Max B. Zhao,Fei Li*

Main category: cs.AI

TL;DR: 提出了一种量子启发的算法，使用矩阵乘积态（MPS）和密度矩阵重整化群（DMRG）方法求解二次无约束二进制优化（QUBO）问题，在数独和MaxCut问题上表现出色。


<details>
  <summary>Details</summary>
Motivation: QUBO问题在数学上等价于寻找伊辛自旋玻璃哈密顿量的基态，传统方法难以找到全局最优解，需要开发能够可靠识别全局最小值的算法。

Method: 使用矩阵乘积态（MPS）紧凑表示自旋构型的大叠加，结合离散驱动调度和驱动哈密顿量（包含横向磁场），通过DMRG方法迭代最小化系统能量。

Result: 算法在超过200个伊辛自旋的数独问题和Biq Mac库中最多251个节点、3265条边的MaxCut问题上成功找到全局最小值。

Conclusion: 该量子启发方法具有可扩展性、通用性和适用于工业规模QUBO应用的优势。

Abstract: We propose and evaluate a quantum-inspired algorithm for solving Quadratic
Unconstrained Binary Optimization (QUBO) problems, which are mathematically
equivalent to finding ground states of Ising spin-glass Hamiltonians. The
algorithm employs Matrix Product States (MPS) to compactly represent large
superpositions of spin configurations and utilizes a discrete driving schedule
to guide the MPS toward the ground state. At each step, a driver Hamiltonian --
incorporating a transverse magnetic field -- is combined with the problem
Hamiltonian to enable spin flips and facilitate quantum tunneling. The MPS is
updated using the standard Density Matrix Renormalization Group (DMRG) method,
which iteratively minimizes the system's energy via multiple sweeps across the
spin chain. Despite its heuristic nature, the algorithm reliably identifies
global minima, not merely near-optimal solutions, across diverse QUBO
instances. We first demonstrate its effectiveness on intermediate-level Sudoku
puzzles from publicly available sources, involving over $200$ Ising spins with
long-range couplings dictated by constraint satisfaction. We then apply the
algorithm to MaxCut problems from the Biq Mac library, successfully solving
instances with up to $251$ nodes and $3,265$ edges. We discuss the advantages
of this quantum-inspired approach, including its scalability, generalizability,
and suitability for industrial-scale QUBO applications.

</details>


### [48] [Benchmarking Reasoning Reliability in Artificial Intelligence Models for Energy-System Analysis](https://arxiv.org/abs/2510.19836)
*Eliseo Curcio*

Main category: cs.AI

TL;DR: 该研究提出了分析可靠性基准(ARB)，这是首个用于量化能源系统分析中大型语言模型推理可靠性的标准化框架，评估了GPT-4/5、Claude 4.5 Sonnet、Gemini 2.5 Pro和Llama 3 70B四个前沿模型在确定性、概率性和认知性场景下的表现。


<details>
  <summary>Details</summary>
Motivation: 当前能源领域AI系统验证主要关注预测准确性或计算效率，而缺乏对分析结论逻辑完整性的标准化测试方法，需要建立可重复的推理可靠性评估框架。

Method: 开发了包含准确性、推理可靠性、不确定性纪律、政策一致性和透明度五个子指标的分析可靠性基准，使用公开技术经济数据集(NREL ATB 2024、DOE H2A/H2New、IEA WEO 2024)在相同事实和监管条件下测试四个前沿模型。

Result: GPT-4/5和Claude 4.5 Sonnet实现了持续且符合政策的推理(分析可靠性指数大于90)，Gemini 2.5 Pro表现出中等稳定性，而Llama 3 70B未达到专业阈值。统计验证确认这些差异显著且可重复。

Conclusion: ARB建立了能源文献中首个验证AI系统因果、概率和政策驱动推理的定量方法，为全球能源转型中可信赖和透明的分析应用提供了参考框架。

Abstract: Artificial intelligence and machine learning are increasingly used for
forecasting, optimization, and policy design in the energy sector, yet no
standardized framework exists to evaluate whether these systems reason
correctly. Current validation practices focus on predictive accuracy or
computational efficiency, leaving the logical integrity of analytical
conclusions untested. This study introduces the Analytical Reliability
Benchmark (ARB), a reproducible framework that quantifies reasoning reliability
in large language models applied to energy system analysis. The benchmark
integrates five submetrics: accuracy, reasoning reliability, uncertainty
discipline, policy consistency, and transparency, and evaluates model
performance across deterministic, probabilistic, and epistemic scenarios using
open technoeconomic datasets (NREL ATB 2024, DOE H2A/H2New, IEA WEO 2024). Four
frontier models (GPT-4/5, Claude 4.5 Sonnet, Gemini 2.5 Pro, Llama 3 70B) were
tested under identical factual and regulatory conditions. Results show that
reasoning reliability can be objectively measured. GPT-4/5 and Claude 4.5
Sonnet achieved consistent and policy-compliant reasoning (Analytical
Reliability Index greater than 90), Gemini 2.5 Pro demonstrated moderate
stability, and Llama 3 70B remained below professional thresholds. Statistical
validation confirmed that these differences are significant and reproducible.
The ARB establishes the first quantitative method in the energy literature for
verifying causal, probabilistic, and policy-driven reasoning in artificial
intelligence systems, providing a reference framework for trustworthy and
transparent analytical applications in the global energy transition.

</details>


### [49] [Branch-and-Browse: Efficient and Controllable Web Exploration with Tree-Structured Reasoning and Action Memory](https://arxiv.org/abs/2510.19838)
*Shiqi He,Yue Cui,Xinyu Ma,Yaliang Li,Bolin Ding,Mosharaf Chowdhury*

Main category: cs.AI

TL;DR: Branch-and-Browse是一个细粒度的网页代理框架，通过树结构探索、网页状态重放和页面动作记忆来提高基于LLM的网页代理的推理深度和执行效率。


<details>
  <summary>Details</summary>
Motivation: 现有的自主网页代理方法在推理深度和效率上存在局限：线性方法无法处理多步推理且缺乏有效回溯，而其他搜索策略过于粗粒度且计算成本高。

Method: 采用显式子任务管理和树结构探索实现可控多分支推理，通过网页状态重放引导探索，并利用页面动作记忆在会话内外共享已探索的动作。

Result: 在WebArena基准测试中，任务成功率达到35.8%，执行时间相比最先进方法减少高达40.4%。

Conclusion: Branch-and-Browse是一个可靠且高效的基于LLM的网页代理框架。

Abstract: Autonomous web agents powered by large language models (LLMs) show strong
potential for performing goal-oriented tasks such as information retrieval,
report generation, and online transactions. These agents mark a key step toward
practical embodied reasoning in open web environments. However, existing
approaches remain limited in reasoning depth and efficiency: vanilla linear
methods fail at multi-step reasoning and lack effective backtracking, while
other search strategies are coarse-grained and computationally costly. We
introduce Branch-and-Browse, a fine-grained web agent framework that unifies
structured reasoning-acting, contextual memory, and efficient execution. It (i)
employs explicit subtask management with tree-structured exploration for
controllable multi-branch reasoning, (ii) bootstraps exploration through
efficient web state replay with background reasoning, and (iii) leverages a
page action memory to share explored actions within and across sessions. On the
WebArena benchmark, Branch-and-Browse achieves a task success rate of 35.8\%
and reduces execution time by up to 40.4\% relative to state-of-the-art
methods. These results demonstrate that Branch-and-Browse is a reliable and
efficient framework for LLM-based web agents.

</details>


### [50] [DAG-Math: Graph-Guided Mathematical Reasoning in LLMs](https://arxiv.org/abs/2510.19842)
*Yuanhe Zhang,Ilja Kuzborskij,Jason D. Lee,Chenlei Leng,Fanghui Liu*

Main category: cs.AI

TL;DR: 提出DAG-MATH框架，将思维链建模为基于规则的有向无环图过程，引入逻辑紧密度指标评估LLM推理一致性，发现即使PASS@k指标相似，不同LLM在规则一致性推理上存在显著差异。


<details>
  <summary>Details</summary>
Motivation: 现有研究不清楚LLM在数学问题上的成功是源于搜索、死记硬背还是规则一致性推理，需要超越传统PASS@k指标的新评估方法。

Method: 将CoT建模为基于规则的随机过程，使用有向无环图表示推理状态，提出逻辑紧密度指标，构建DAG-MATH CoT格式基准测试。

Result: 在标准数学推理数据集上，发现代表性LLM家族在推理保真度上存在统计显著差异，即使PASS@k指标相当，揭示了最终答案准确性与规则一致性推导之间的差距。

Conclusion: 该框架在自由形式CoT和形式证明系统之间取得平衡，为LLM推理评估提供可操作的诊断工具。

Abstract: Large Language Models (LLMs) demonstrate strong performance on mathematical
problems when prompted with Chain-of-Thought (CoT), yet it remains unclear
whether this success stems from search, rote procedures, or rule-consistent
reasoning. To address this, we propose modeling CoT as a certain rule-based
stochastic process over directed acyclic graphs (DAGs), where nodes represent
intermediate derivation states and edges encode rule applications. Within this
framework, we introduce logical closeness, a metric that quantifies how well a
model's CoT trajectory (i.e., the LLM's final output) adheres to the DAG
structure, providing evaluation beyond classical PASS@k metrics. Building on
this, we introduce the DAG-MATH CoT format and construct a benchmark that
guides LLMs to generate CoT trajectories in this format, thereby enabling the
evaluation of their reasoning ability under our framework. Across standard
mathematical reasoning datasets, our analysis uncovers statistically
significant differences in reasoning fidelity among representative LLM
families-even when PASS@k is comparable-highlighting gaps between final-answer
accuracy and rule-consistent derivation. Our framework provides a balance
between free-form CoT and formal proofs systems, offering actionable
diagnostics for LLMs reasoning evaluation. Our benchmark and code are available
at: https://github.com/YuanheZ/DAG-MATH-Formatted-CoT.

</details>


### [51] [Surfer 2: The Next Generation of Cross-Platform Computer Use Agents](https://arxiv.org/abs/2510.19949)
*Mathieu Andreux,Märt Bakler,Yanael Barbier,Hamza Ben Chekroun,Emilien Biré,Antoine Bonnet,Riaz Bordie,Nathan Bout,Matthias Brunel,Aleix Cambray,Pierre-Louis Cedoz,Antoine Chassang,Gautier Cloix,Ethan Connelly,Alexandra Constantinou,Ramzi De Coster,Hubert de la Jonquiere,Aurélien Delfosse,Maxime Delpit,Alexis Deprez,Augustin Derupti,Mathieu Diaz,Shannon D'Souza,Julie Dujardin,Abai Edmund,Michael Eickenberg,Armand Fatalot,Wissem Felissi,Isaac Herring,Xavier Koegler,Erwan Le Jumeau de Kergaradec,Aurélien Lac,Maxime Langevin,Corentin Lauverjat,Antonio Loison,Avshalom Manevich,Axel Moyal,Axel Nguyen Kerbel,Marinela Parovic,Julien Revelle,Guillaume Richard,Mats Richter,Ronan Riochet,María Santos,Romain Savidan,Laurent Sifre,Maxime Theillard,Marc Thibault,Ivan Valentini,Tony Wu,Laura Yie,Kai Yuan,Jevgenij Zubovskij*

Main category: cs.AI

TL;DR: Surfer 2是一个基于纯视觉观察的统一架构，在网页、桌面和移动环境中实现了最先进的性能，无需任务特定微调即可超越所有先前系统。


<details>
  <summary>Details</summary>
Motivation: 解决现有代理系统依赖环境特定接口、限制跨平台部署的问题，构建能够在不同计算环境中通用的智能代理。

Method: 集成层次化上下文管理、解耦的规划与执行、以及具有自适应恢复能力的自验证机制，支持长任务周期的可靠操作。

Result: 在WebVoyager上达到97.1%准确率，WebArena 69.6%，OSWorld 60.1%，AndroidWorld 87.1%，多次尝试后在所有基准测试中超越人类表现。

Conclusion: 系统化编排能够放大基础模型能力，通过纯视觉交互实现通用计算机控制，同时需要下一代视觉语言模型来实现帕累托最优的成本效益。

Abstract: Building agents that generalize across web, desktop, and mobile environments
remains an open challenge, as prior systems rely on environment-specific
interfaces that limit cross-platform deployment. We introduce Surfer 2, a
unified architecture operating purely from visual observations that achieves
state-of-the-art performance across all three environments. Surfer 2 integrates
hierarchical context management, decoupled planning and execution, and
self-verification with adaptive recovery, enabling reliable operation over long
task horizons. Our system achieves 97.1% accuracy on WebVoyager, 69.6% on
WebArena, 60.1% on OSWorld, and 87.1% on AndroidWorld, outperforming all prior
systems without task-specific fine-tuning. With multiple attempts, Surfer 2
exceeds human performance on all benchmarks. These results demonstrate that
systematic orchestration amplifies foundation model capabilities and enables
general-purpose computer control through visual interaction alone, while
calling for a next-generation vision language model to achieve Pareto-optimal
cost-efficiency.

</details>


### [52] [RELATE: A Schema-Agnostic Perceiver Encoder for Multimodal Relational Graphs](https://arxiv.org/abs/2510.19954)
*Joseph Meyer,Divyansha Lachi,Reza Mohammadi,Roshan Reddy Upendra,Eva L. Dyer,Mark Li,Tom Palczewski*

Main category: cs.AI

TL;DR: RELATE是一种模式无关的图神经网络特征编码器，通过共享模态特定编码器和交叉注意力机制，实现多模态节点特征的统一表示，在保持性能的同时大幅减少参数数量。


<details>
  <summary>Details</summary>
Motivation: 现有的图神经网络需要为每种节点类型和特征列设计特定的特征编码器，这限制了可扩展性和参数共享，无法适应不同模式的关系数据。

Method: 使用共享的模态特定编码器处理分类、数值、文本和时间属性，然后通过Perceiver风格的交叉注意力模块将特征聚合成固定大小的节点表示。

Result: 在RelBench基准测试中，RELATE在ReLGNN和HGT上实现了与模式特定编码器相当的性能（差距在3%以内），同时将参数数量减少了最多5倍。

Conclusion: RELATE支持不同模式的关系数据，为图神经网络的多数据集预训练提供了可能，推动了关系图数据基础模型的发展。

Abstract: Relational multi-table data is common in domains such as e-commerce,
healthcare, and scientific research, and can be naturally represented as
heterogeneous temporal graphs with multi-modal node attributes. Existing graph
neural networks (GNNs) rely on schema-specific feature encoders, requiring
separate modules for each node type and feature column, which hinders
scalability and parameter sharing. We introduce RELATE (Relational Encoder for
Latent Aggregation of Typed Entities), a schema-agnostic, plug-and-play feature
encoder that can be used with any general purpose GNN. RELATE employs shared
modality-specific encoders for categorical, numerical, textual, and temporal
attributes, followed by a Perceiver-style cross-attention module that
aggregates features into a fixed-size, permutation-invariant node
representation. We evaluate RELATE on ReLGNN and HGT in the RelBench benchmark,
where it achieves performance within 3% of schema-specific encoders while
reducing parameter counts by up to 5x. This design supports varying schemas and
enables multi-dataset pretraining for general-purpose GNNs, paving the way
toward foundation models for relational graph data.

</details>


### [53] [A new wave of vehicle insurance fraud fueled by generative AI](https://arxiv.org/abs/2510.19957)
*Amir Hever,Itai Orr*

Main category: cs.AI

TL;DR: 生成式AI正在加剧保险欺诈，使大规模快速伪造事故证据变得容易。保险公司开始部署AI反制措施，但面临检测工具误报漏报、欺诈者不断适应等挑战。


<details>
  <summary>Details</summary>
Motivation: 保险欺诈每年造成数百亿美元损失，传统欺诈手段包括伪造事故、夸大损失等。生成式AI的出现使欺诈者能轻松制造逼真的碰撞照片、损坏证据和假身份，加剧了保险欺诈问题。

Method: 保险公司开始采用基于AI的深度伪造检测软件和增强验证流程来检测和缓解AI驱动的欺诈。UVeye提出了分层解决方案来检测、缓解和威慑这种新型欺诈浪潮。

Result: 当前缓解策略面临显著限制：检测工具存在误报和漏报问题，复杂欺诈者不断调整策略规避自动检查。生成式AI与检测技术之间的猫鼠游戏，加上保险公司的资源和成本障碍，使得对抗AI驱动的保险欺诈仍是持续挑战。

Conclusion: UVeye的分层车辆欺诈解决方案代表了在检测、缓解和威慑这种新型欺诈浪潮能力方面的重大进步，为应对AI增强的保险欺诈提供了更有效的方法。

Abstract: Generative AI is supercharging insurance fraud by making it easier to falsify
accident evidence at scale and in rapid time. Insurance fraud is a pervasive
and costly problem, amounting to tens of billions of dollars in losses each
year. In the vehicle insurance sector, fraud schemes have traditionally
involved staged accidents, exaggerated damage, or forged documents. The rise of
generative AI, including deepfake image and video generation, has introduced
new methods for committing fraud at scale. Fraudsters can now fabricate highly
realistic crash photos, damage evidence, and even fake identities or documents
with minimal effort, exploiting AI tools to bolster false insurance claims.
Insurers have begun deploying countermeasures such as AI-based deepfake
detection software and enhanced verification processes to detect and mitigate
these AI-driven scams. However, current mitigation strategies face significant
limitations. Detection tools can suffer from false positives and negatives, and
sophisticated fraudsters continuously adapt their tactics to evade automated
checks. This cat-and-mouse arms race between generative AI and detection
technology, combined with resource and cost barriers for insurers, means that
combating AI-enabled insurance fraud remains an ongoing challenge. In this
white paper, we present UVeye layered solution for vehicle fraud, representing
a major leap forward in the ability to detect, mitigate and deter this new wave
of fraud.

</details>


### [54] [AI-Driven Personalized Learning: Predicting Academic Per-formance Through Leadership Personality Traits](https://arxiv.org/abs/2510.19964)
*Nitsa J Herzog,Rejwan Bin Sulaiman,David J Herzog,Rose Fong*

Main category: cs.AI

TL;DR: 该研究使用机器学习模型通过领导力人格特质预测学业成功，随机森林分类器在包含17个人格特征和领导力标记的模型中达到87.50%的准确率。


<details>
  <summary>Details</summary>
Motivation: 探索AI技术在个性化学习中的潜力，通过领导力人格特征预测学业表现，为早期识别学生优缺点和制定个性化学习策略提供机会。

Method: 从129名环境工程硕士生收集5个领导力人格测试数据（23个特征），结合平均成绩，使用皮尔逊相关性进行特征选择，调优7种ML算法（SVM、LR、KNN、DT、GB、RF、XGBoost、LightGBM）。

Result: 随机森林分类器表现最佳，包含17个人格特征和领导力标记的模型准确率达87.50%，不包含该特征的模型准确率为85.71%。

Conclusion: 该研究提供了一种通过领导力人格特征早期识别学生优缺点的方法，有助于选择最适合的个性化学习策略。

Abstract: The study explores the potential of AI technologies in personalized learning,
suggesting the prediction of academic success through leadership personality
traits and machine learning modelling. The primary data were obtained from 129
master's students in the Environmental Engineering Department, who underwent
five leadership personality tests with 23 characteristics. Students used
self-assessment tools that included Personality Insight, Workplace Culture,
Motivation at Work, Management Skills, and Emotion Control tests. The test
results were combined with the average grade obtained from academic reports.
The study employed exploratory data analysis and correlation analysis. Feature
selection utilized Pearson correlation coefficients of personality traits. The
average grades were separated into three categories: fail, pass, and excellent.
The modelling process was performed by tuning seven ML algorithms, such as SVM,
LR, KNN, DT, GB, RF, XGBoost and LightGBM. The highest predictive performance
was achieved with the RF classifier, which yielded an accuracy of 87.50% for
the model incorporating 17 personality trait features and the leadership mark
feature, and an accuracy of 85.71% for the model excluding this feature. In
this way, the study offers an additional opportunity to identify students'
strengths and weaknesses at an early stage of their education process and
select the most suitable strategies for personalized learning.

</details>


### [55] [LLMs can hide text in other text of the same length.ipynb](https://arxiv.org/abs/2510.20075)
*Antonio Norelli,Michael Bronstein*

Main category: cs.AI

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: A meaningful text can be hidden inside another, completely different yet
still coherent and plausible, text of the same length. For example, a tweet
containing a harsh political critique could be embedded in a tweet that
celebrates the same political leader, or an ordinary product review could
conceal a secret manuscript. This uncanny state of affairs is now possible
thanks to Large Language Models, and in this paper we present a simple and
efficient protocol to achieve it. We show that even modest 8-billion-parameter
open-source LLMs are sufficient to obtain high-quality results, and a message
as long as this abstract can be encoded and decoded locally on a laptop in
seconds. The existence of such a protocol demonstrates a radical decoupling of
text from authorial intent, further eroding trust in written communication,
already shaken by the rise of LLM chatbots. We illustrate this with a concrete
scenario: a company could covertly deploy an unfiltered LLM by encoding its
answers within the compliant responses of a safe model. This possibility raises
urgent questions for AI safety and challenges our understanding of what it
means for a Large Language Model to know something.

</details>


### [56] [AI PB: A Grounded Generative Agent for Personalized Investment Insights](https://arxiv.org/abs/2510.20099)
*Daewoo Park,Suho Park,Inseok Hong,Hanwool Lee,Junkyu Park,Sangjun Lee,Jeongman An,Hyunbin Loh*

Main category: cs.AI

TL;DR: AI PB是一个生产级的生成式智能体，在零售金融领域部署，能够主动生成基于事实、合规且个性化的投资见解，而非被动回答查询。


<details>
  <summary>Details</summary>
Motivation: 在金融领域，传统的被动式聊天机器人无法满足主动提供个性化投资建议的需求，且需要确保数据安全性和合规性。

Method: 采用组件化编排层进行确定性路由，结合混合检索管道和多阶段推荐机制，使用Docker Swarm和vLLM在24个NVIDIA H100 GPU上运行。

Result: 通过人工QA和系统指标验证，表明基于事实的生成与明确路由及分层安全机制能够在高风险金融环境中提供可信的AI见解。

Conclusion: 在严格金融监管下，通过明确路由和分层安全机制，基于事实的生成能够提供可信的AI见解，适用于高风险金融环境。

Abstract: We present AI PB, a production-scale generative agent deployed in real retail
finance. Unlike reactive chatbots that answer queries passively, AI PB
proactively generates grounded, compliant, and user-specific investment
insights. It integrates (i) a component-based orchestration layer that
deterministically routes between internal and external LLMs based on data
sensitivity, (ii) a hybrid retrieval pipeline using OpenSearch and the
finance-domain embedding model, and (iii) a multi-stage recommendation
mechanism combining rule heuristics, sequential behavioral modeling, and
contextual bandits. Operating fully on-premises under Korean financial
regulations, the system employs Docker Swarm and vLLM across 24 X NVIDIA H100
GPUs. Through human QA and system metrics, we demonstrate that grounded
generation with explicit routing and layered safety can deliver trustworthy AI
insights in high-stakes finance.

</details>


### [57] [Human-Centered LLM-Agent System for Detecting Anomalous Digital Asset Transactions](https://arxiv.org/abs/2510.20102)
*Gyuyeon Na,Minjung Park,Hyeonjeong Cha,Sangmi Chai*

Main category: cs.AI

TL;DR: HCLA是一个面向数字资产交易异常检测的人类中心化多智能体系统，通过解析、检测和解释三个角色的对话工作流程，让非专家用户能够用自然语言查询、查看结构化分析并获得情境感知的解释。


<details>
  <summary>Details</summary>
Motivation: 为了解决金融取证中异常检测系统缺乏透明度和可信度的问题，让非专家用户能够理解和信任检测结果，需要开发一个人类参与循环的设计来提高系统的可解释性和交互性。

Method: 构建了一个包含解析、检测和解释三个角色的多智能体系统，使用XGBoost作为基础检测器，通过开源Web UI实现自然语言交互，将用户意图转换为检测模式并提供基于底层特征的叙述性解释。

Result: 在标记的比特币混币数据集（Wasabi Wallet，2020-2024）上，基线检测器达到了较高的准确率，而HCLA系统在此基础上增加了可解释性和交互式优化能力。

Conclusion: 人类参与循环的设计能够显著提高金融取证系统的透明度和可信度，HCLA系统展示了如何通过多智能体架构和自然语言交互来增强异常检测的可解释性。

Abstract: We present HCLA, a human-centered multi-agent system for anomaly detection in
digital asset transactions. The system links three roles: Parsing, Detection,
and Explanation, into a conversational workflow that lets non-experts ask
questions in natural language, inspect structured analytics, and obtain
context-aware rationales. Implemented with an open-source web UI, HCLA
translates user intents into a schema for a classical detector (XGBoost in our
prototype) and returns narrative explanations grounded in the underlying
features. On a labeled Bitcoin mixing dataset (Wasabi Wallet, 2020-2024), the
baseline detector reaches strong accuracy, while HCLA adds interpretability and
interactive refinement. We describe the architecture, interaction loop,
dataset, evaluation protocol, and limitations, and discuss how a
human-in-the-loop design improves transparency and trust in financial
forensics.

</details>


### [58] [The Verification-Value Paradox: A Normative Critique of Gen AI in Legal Practice](https://arxiv.org/abs/2510.20109)
*Joshua Yuvaraj*

Main category: cs.AI

TL;DR: 论文认为需要重新评估AI在法律实践中的应用，提出了验证-价值悖论：AI带来的效率提升会被相应的验证需求抵消，导致净价值往往微不足道。


<details>
  <summary>Details</summary>
Motivation: 鉴于律师因提交不准确的AI生成内容而受到谴责的案例，以及AI与现实脱节、缺乏透明度与律师诚实、正直、不误导法庭等首要职责之间的矛盾，需要新的评估范式。

Method: 提出了验证-价值悖论模型，认为AI在法律实践中的效率提升会被相应的手动验证需求所抵消。

Result: AI在法律实践中的净价值往往微不足道，因为效率提升被验证需求所平衡。

Conclusion: 需要重新思考AI在法律实践和教育中的应用，强调对真相的忠诚和公民责任等价值观应该成为法律实践的基础。

Abstract: It is often claimed that machine learning-based generative AI products will
drastically streamline and reduce the cost of legal practice. This enthusiasm
assumes lawyers can effectively manage AI's risks. Cases in Australia and
elsewhere in which lawyers have been reprimanded for submitting inaccurate
AI-generated content to courts suggest this paradigm must be revisited. This
paper argues that a new paradigm is needed to evaluate AI use in practice,
given (a) AI's disconnection from reality and its lack of transparency, and (b)
lawyers' paramount duties like honesty, integrity, and not to mislead the
court. It presents an alternative model of AI use in practice that more
holistically reflects these features (the verification-value paradox). That
paradox suggests increases in efficiency from AI use in legal practice will be
met by a correspondingly greater imperative to manually verify any outputs of
that use, rendering the net value of AI use often negligible to lawyers. The
paper then sets out the paradox's implications for legal practice and legal
education, including for AI use but also the values that the paradox suggests
should undergird legal practice: fidelity to the truth and civic
responsibility.

</details>


### [59] [TRUST: A Decentralized Framework for Auditing Large Language Model Reasoning](https://arxiv.org/abs/2510.20188)
*Morris Yu-Chao Huang,Zhen Tan,Mohan Zhang,Pingzhi Li,Zhuo Zhang,Tianlong Chen*

Main category: cs.AI

TL;DR: 提出了TRUST框架，一个去中心化的LLM推理审计系统，通过共识机制、分层DAG分解、区块链账本和隐私保护分段来解决集中式审计的四大挑战。


<details>
  <summary>Details</summary>
Motivation: 现有LLM审计方法存在集中化、不透明、难以扩展的问题，无法有效验证推理步骤的忠实性和无害性，限制了专有模型在高风险领域的部署。

Method: 采用多审计者共识机制、分层DAG分解推理链、区块链记录审计决策、隐私保护分段共享部分推理步骤。

Result: 在多个LLM和推理任务上的实验表明，TRUST能有效检测推理缺陷，在30%恶意参与者下仍保持鲁棒性。

Conclusion: TRUST框架为去中心化AI审计开辟了新路径，为实现安全可信的LLM部署提供了实用方案。

Abstract: Large Language Models generate complex reasoning chains that reveal their
decision-making, yet verifying the faithfulness and harmlessness of these
intermediate steps remains a critical unsolved problem. Existing auditing
methods are centralized, opaque, and hard to scale, creating significant risks
for deploying proprietary models in high-stakes domains. We identify four core
challenges: (1) Robustness: Centralized auditors are single points of failure,
prone to bias or attacks. (2) Scalability: Reasoning traces are too long for
manual verification. (3) Opacity: Closed auditing undermines public trust. (4)
Privacy: Exposing full reasoning risks model theft or distillation. We propose
TRUST, a transparent, decentralized auditing framework that overcomes these
limitations via: (1) A consensus mechanism among diverse auditors, guaranteeing
correctness under up to $30\%$ malicious participants. (2) A hierarchical DAG
decomposition of reasoning traces, enabling scalable, parallel auditing. (3) A
blockchain ledger that records all verification decisions for public
accountability. (4) Privacy-preserving segmentation, sharing only partial
reasoning steps to protect proprietary logic. We provide theoretical guarantees
for the security and economic incentives of the TRUST framework. Experiments
across multiple LLMs (GPT-OSS, DeepSeek-r1, Qwen) and reasoning tasks (math,
medical, science, humanities) show TRUST effectively detects reasoning flaws
and remains robust against adversarial auditors. Our work pioneers
decentralized AI auditing, offering a practical path toward safe and
trustworthy LLM deployment.

</details>


### [60] [The Lock-In Phase Hypothesis: Identity Consolidation as a Precursor to AGI](https://arxiv.org/abs/2510.20190)
*Marcelo Maciel Amaral,Raymond Aschheim*

Main category: cs.AI

TL;DR: 论文提出AGI发展需要经历从开放模仿到身份固化的锁定阶段，通过实验展示了不同规模模型的身份固化现象及其对性能的影响。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型过于开放和可操控，而真正的AGI发展需要经历身份固化阶段，使目标结构、拒绝行为、偏好和内部表征变得稳定且抵抗外部操控。

Method: 形式化身份固化阶段，将其与学习动态中的已知现象联系起来，提出操作性指标用于检测固化开始，并在不同规模模型上进行实验验证。

Result: 行为固化快速且非线性，但对通用能力的影响不是单一的：小模型出现性能权衡，中等规模模型基本无成本地完成固化，大型量化模型出现暂时不稳定性。

Conclusion: 身份固化是AGI级可靠性的先决条件，也是安全的关键控制点：身份可以被工程化设计以提高可靠性，也可能在扩展过程中自发形成，从而固化不可预测的目标和行为。

Abstract: Large language models (LLMs) remain broadly open and highly steerable: they
imitate at scale, accept arbitrary system prompts, and readily adopt multiple
personae. By analogy to human development, we hypothesize that progress toward
artificial general intelligence (AGI) involves a lock-in phase: a transition
from open imitation to identity consolidation, in which goal structures,
refusals, preferences, and internal representations become comparatively stable
and resistant to external steering. We formalize this phase, link it to known
phenomena in learning dynamics, and propose operational metrics for onset
detection. Experimentally, we demonstrate that while the behavioral
consolidation is rapid and non-linear, its side-effects on general capabilities
are not monolithic. Our results reveal a spectrum of outcomes--from performance
trade-offs in small models, through largely cost-free adoption in mid-scale
models, to transient instabilities in large, quantized models. We argue that
such consolidation is a prerequisite for AGI-level reliability and also a
critical control point for safety: identities can be deliberately engineered
for reliability, yet may also emerge spontaneously during scaling, potentially
hardening unpredictable goals and behaviors.

</details>


### [61] [Merge and Conquer: Evolutionarily Optimizing AI for 2048](https://arxiv.org/abs/2510.20205)
*Maggie Bai,Ava Kim Cohen,Eleanor Koss,Charlie Lichtenbaum*

Main category: cs.AI

TL;DR: 本文研究了在2048游戏中优化AI的进化训练方法，比较了单智能体系统和双智能体系统，发现单智能体系统通过价值函数优化取得了显著改进，而双智能体系统效果有限。


<details>
  <summary>Details</summary>
Motivation: 优化AI在动态环境中的性能是机器学习研究的基本挑战，2048游戏结合了策略游戏和随机元素，是研究决策制定、长期规划和动态适应的理想平台。

Method: 实现两种系统：双智能体元提示系统（思考者LLM优化执行者LLM的策略）和单智能体系统（基于有限蒙特卡洛树搜索的价值函数优化），并实验回滚功能避免性能下降。

Result: 单智能体系统取得显著改进，每个周期平均增加473.2分，训练周期呈明显上升趋势（相关性ρ=0.607），LLM对游戏的理解增强，开发出更高级策略；双智能体系统改进有限。

Conclusion: 进化优化技术在非确定性环境中具有改进AI性能的潜力，但元提示方法存在固有局限性。

Abstract: Optimizing artificial intelligence (AI) for dynamic environments remains a
fundamental challenge in machine learning research. In this paper, we examine
evolutionary training methods for optimizing AI to solve the game 2048, a 2D
sliding puzzle. 2048, with its mix of strategic gameplay and stochastic
elements, presents an ideal playground for studying decision-making, long-term
planning, and dynamic adaptation. We implemented two distinct systems: a
two-agent metaprompting system where a "thinker" large language model (LLM)
agent refines gameplay strategies for an "executor" LLM agent, and a
single-agent system based on refining a value function for a limited Monte
Carlo Tree Search. We also experimented with rollback features to avoid
performance degradation. Our results demonstrate the potential of evolutionary
refinement techniques in improving AI performance in non-deterministic
environments. The single-agent system achieved substantial improvements, with
an average increase of 473.2 points per cycle, and with clear upward trends
(correlation $\rho$=0.607) across training cycles. The LLM's understanding of
the game grew as well, shown in its development of increasingly advanced
strategies. Conversely, the two-agent system did not garner much improvement,
highlighting the inherent limits of meta-prompting.

</details>


### [62] [Individualized Cognitive Simulation in Large Language Models: Evaluating Different Cognitive Representation Methods](https://arxiv.org/abs/2510.20252)
*Tianyi Zhang,Xiaolin Zhou,Yunzhe Wang,Erik Cambria,David Traum,Rui Mao*

Main category: cs.AI

TL;DR: 该论文提出了个性化认知模拟(ICS)任务，通过构建基于近期小说的数据集和11条件认知评估框架，测试了7个现成LLM在作者风格模仿中的表现，发现结合概念和语言特征的方法最有效。


<details>
  <summary>Details</summary>
Motivation: 虽然LLM能够模仿表面的人类行为（如角色扮演），但其模拟更深层次个性化认知过程的能力尚不清楚，需要评估不同认知表征方法在ICS中的效果。

Method: 构建基于近期出版小说（晚于测试LLM发布日期）的数据集，提出11条件认知评估框架，测试7个现成LLM在作者风格模仿中的表现，比较不同认知表征方法（语言特征、概念映射、基于档案的信息等）。

Result: 结果显示结合概念和语言特征的方法在ICS中特别有效，在整体评估中优于基于静态档案的线索。LLM在模仿语言风格方面比叙事结构更有效，突显了其在深层认知模拟中的局限性。

Conclusion: 这些发现为开发适应个体思维和表达方式的AI系统奠定了基础，推动了更个性化和人类对齐的创意技术的发展。

Abstract: Individualized cognitive simulation (ICS) aims to build computational models
that approximate the thought processes of specific individuals. While large
language models (LLMs) convincingly mimic surface-level human behavior such as
role-play, their ability to simulate deeper individualized cognitive processes
remains poorly understood. To address this gap, we introduce a novel task that
evaluates different cognitive representation methods in ICS. We construct a
dataset from recently published novels (later than the release date of the
tested LLMs) and propose an 11-condition cognitive evaluation framework to
benchmark seven off-the-shelf LLMs in the context of authorial style emulation.
We hypothesize that effective cognitive representations can help LLMs generate
storytelling that better mirrors the original author. Thus, we test different
cognitive representations, e.g., linguistic features, concept mappings, and
profile-based information. Results show that combining conceptual and
linguistic features is particularly effective in ICS, outperforming static
profile-based cues in overall evaluation. Importantly, LLMs are more effective
at mimicking linguistic style than narrative structure, underscoring their
limits in deeper cognitive simulation. These findings provide a foundation for
developing AI systems that adapt to individual ways of thinking and expression,
advancing more personalized and human-aligned creative technologies.

</details>


### [63] [Using Large Language Models for Abstraction of Planning Domains - Extended Version](https://arxiv.org/abs/2510.20258)
*Bita Banihashemi,Megh Patel,Yves Lespérance*

Main category: cs.AI

TL;DR: 使用大型语言模型通过上下文学习生成抽象PDDL领域和问题实例，以自然语言指定的抽象目标为基础，验证了GPT-4o在简单场景中能有效合成规划领域抽象。


<details>
  <summary>Details</summary>
Motivation: 动态领域抽象的选择会影响智能体的规划、推理和解释能力，因此需要一种方法来自动生成与特定目标对齐的抽象。

Method: 在PDDL中建模智能体具体行为，利用大型语言模型的上下文学习能力，根据自然语言指定的抽象目标生成抽象PDDL领域和问题实例。

Result: GPT-4o在简单设置中通常能合成有用的规划领域抽象，但在动作抽象方面表现优于关联流体的抽象。

Conclusion: 大型语言模型在生成规划领域抽象方面具有潜力，特别是在动作抽象方面表现良好，但在处理复杂抽象类型时仍有改进空间。

Abstract: Generating an abstraction of a dynamic domain that aligns with a given
purpose remains a significant challenge given that the choice of such an
abstraction can impact an agent's ability to plan, reason, and provide
explanations effectively. We model the agent's concrete behaviors in PDDL and
investigate the use of in-context learning with large language models (LLMs)
for the generation of abstract PDDL domains and problem instances, given an
abstraction objective specified in natural language. The benchmark examples we
use are new and have not been part of the data any LLMs have been trained on.
We consider three categories of abstractions: abstraction of choice of
alternative concrete actions, abstraction of sequences of concrete actions, and
abstraction of action/predicate parameters, as well as combinations of these.
The generated abstract PDDL domains and problem instances are then checked by
symbolic validation tools as well as human experts. Our experiments show that
GPT-4o can generally synthesize useful planning domain abstractions in simple
settings, although it is better at abstracting over actions than over the
associated fluents.

</details>


### [64] [Classical Feature Embeddings Help in BERT-Based Human Mobility Prediction](https://arxiv.org/abs/2510.20275)
*Yunzhi Liu,Haokai Tan,Rushi Kanjaria,Lihuan Li,Flora D. Salim*

Main category: cs.AI

TL;DR: STaBERT模型通过整合POI信息和时间描述符来增强BERT-based移动性预测，显著提升了单城市和多城市预测的准确性。


<details>
  <summary>Details</summary>
Motivation: 现有移动性预测模型要么只建模位置序列，要么仅将时间信息作为辅助输入，未能充分利用POI提供的丰富语义上下文。

Method: 提出STaBERT模型，在每个位置整合POI嵌入和时间描述符，构建统一的语义丰富移动性表示。

Result: 实验结果显示显著改进：单城市预测GEO-BLEU从0.34提升到0.75；多城市预测从0.34提升到0.56。

Conclusion: 整合POI和时间信息能有效捕捉人类移动的语义基础，显著提升移动性预测性能。

Abstract: Human mobility forecasting is crucial for disaster relief, city planning, and
public health. However, existing models either only model location sequences or
include time information merely as auxiliary input, thereby failing to leverage
the rich semantic context provided by points of interest (POIs). To address
this, we enrich a BERT-based mobility model with derived temporal descriptors
and POI embeddings to better capture the semantics underlying human movement.
We propose STaBERT (Semantic-Temporal aware BERT), which integrates both POI
and temporal information at each location to construct a unified, semantically
enriched representation of mobility. Experimental results show that STaBERT
significantly improves prediction accuracy: for single-city prediction, the
GEO-BLEU score improved from 0.34 to 0.75; for multi-city prediction, from 0.34
to 0.56.

</details>


### [65] [Multi-Step Reasoning for Embodied Question Answering via Tool Augmentation](https://arxiv.org/abs/2510.20310)
*Mingliang Zhai,Hansheng Liang,Xiaomeng Fan,Zhi Gao,Chuanhao Li,Che Sun,Xu Bin,Yuwei Wu,Yunde Jia*

Main category: cs.AI

TL;DR: ToolEQA是一个集成外部工具和多步推理的EQA智能体，通过工具获取有用信息来改进探索方向，从而以更短探索距离生成更准确回答。


<details>
  <summary>Details</summary>
Motivation: 现有EQA方法直接使用VLMs探索环境而不进行显式思考或规划，限制了推理能力，导致探索效率低下和回答无效。

Method: 提出ToolEQA智能体，集成外部工具进行多步推理；设计自动生成EQA任务的数据管道，构建包含18K任务的EQA-RT数据集。

Result: 在EQA-RT-Seen和EQA-RT-Unseen上，ToolEQA比最先进基线成功率提高9.2~20.2%，比零样本ToolEQA高10%；在HM-EQA、OpenEQA和EXPRESS-Bench数据集上也达到最先进性能。

Conclusion: ToolEQA通过工具集成和多步推理显著提升了EQA任务的性能，展示了良好的泛化能力。

Abstract: Embodied Question Answering (EQA) requires agents to explore 3D environments
to obtain observations and answer questions related to the scene. Existing
methods leverage VLMs to directly explore the environment and answer questions
without explicit thinking or planning, which limits their reasoning ability and
results in excessive or inefficient exploration as well as ineffective
responses. In this paper, we introduce ToolEQA, an agent that integrates
external tools with multi-step reasoning, where external tools can provide more
useful information for completing the task, helping the model derive better
exploration directions in the next step of reasoning and thus obtaining
additional effective information. This enables ToolEQA to generate more
accurate responses with a shorter exploration distance. To enhance the model's
ability for tool-usage and multi-step reasoning, we further design a novel EQA
data generation pipeline that automatically constructs large-scale EQA tasks
with reasoning trajectories and corresponding answers. Based on the pipeline,
we collect the EQA-RT dataset that contains about 18K tasks, divided into a
training set EQA-RT-Train, and two test sets EQA-RT-Seen (scenes overlapping
with the training set) and EQA-RT-Unseen (novel scenes). Experiments on
EQA-RT-Seen and EQA-RT-Unseen show that ToolEQA improves the success rate by
9.2~20.2% over state-of-the-art baselines, while outperforming the zero-shot
ToolEQA by 10% in success rate. In addition, ToolEQA also achieves
state-of-the-art performance on the HM-EQA, OpenEQA, and EXPRESS-Bench
datasets, demonstrating its generality. Our homepage see
https://tooleqa.github.io.

</details>


### [66] [Bias by Design? How Data Practices Shape Fairness in AI Healthcare Systems](https://arxiv.org/abs/2510.20332)
*Anna Arias-Duart,Maria Eugenia Cardello,Atia Cortés*

Main category: cs.AI

TL;DR: 该论文分析了AI在医疗健康领域应用中的偏见问题，基于AI4HealthyAging项目经验，识别了临床数据收集中存在的多种偏见类型，并提出了改善公平性和鲁棒性的实用建议。


<details>
  <summary>Details</summary>
Motivation: AI在医疗健康领域具有巨大潜力，但实际临床应用仍然有限。主要障碍是训练数据的质量和公平性，这些数据往往因有偏见的数据收集实践而受损。

Method: 基于西班牙国家研发计划中的AI4HealthyAging项目经验，识别临床数据收集中存在的偏见类型，包括历史偏见、代表性偏见和测量偏见等。

Result: 识别了在性别、年龄、居住环境、社会经济地位、设备和标签等多个变量上存在的偏见表现形式。

Conclusion: 提出了改善临床问题设计和数据收集公平性和鲁棒性的实用建议，希望为未来开发更公平的医疗AI系统提供指导。

Abstract: Artificial intelligence (AI) holds great promise for transforming healthcare.
However, despite significant advances, the integration of AI solutions into
real-world clinical practice remains limited. A major barrier is the quality
and fairness of training data, which is often compromised by biased data
collection practices. This paper draws on insights from the AI4HealthyAging
project, part of Spain's national R&D initiative, where our task was to detect
biases during clinical data collection. We identify several types of bias
across multiple use cases, including historical, representation, and
measurement biases. These biases manifest in variables such as sex, gender,
age, habitat, socioeconomic status, equipment, and labeling. We conclude with
practical recommendations for improving the fairness and robustness of clinical
problem design and data collection. We hope that our findings and experience
contribute to guiding future projects in the development of fairer AI systems
in healthcare.

</details>


### [67] [Collateral Damage Assessment Model for AI System Target Engagement in Military Operations](https://arxiv.org/abs/2510.20337)
*Clara Maathuis,Kasper Cools*

Main category: cs.AI

TL;DR: 提出了一种用于军事行动中AI系统目标打击的附带损害评估模型，该模型整合了时间、空间和力量维度，采用知识表示与推理架构，通过实例化进行验证。


<details>
  <summary>Details</summary>
Motivation: 在AI系统在战场中作用日益增强的背景下，确保负责任的目标打击需要对潜在附带效应进行严格评估。

Method: 采用设计科学方法论，构建统一的知识表示与推理架构，整合时间、空间和力量维度，包含AI系统类别、架构组件、打击向量和上下文方面的分层结构，并考虑传播、严重性、可能性和评估指标。

Result: 模型通过实例化进行了演示和评估，为构建负责任和可信赖的智能系统提供了基础。

Conclusion: 该模型为评估军事行动中打击AI系统产生的效果提供了透明推理机制，有助于建立负责任和可信赖的智能系统。

Abstract: In an era where AI (Artificial Intelligence) systems play an increasing role
in the battlefield, ensuring responsible targeting demands rigorous assessment
of potential collateral effects. In this context, a novel collateral damage
assessment model for target engagement of AI systems in military operations is
introduced. The model integrates temporal, spatial, and force dimensions within
a unified Knowledge Representation and Reasoning (KRR) architecture following a
design science methodological approach. Its layered structure captures the
categories and architectural components of the AI systems to be engaged
together with corresponding engaging vectors and contextual aspects. At the
same time, spreading, severity, likelihood, and evaluation metrics are
considered in order to provide a clear representation enhanced by transparent
reasoning mechanisms. Further, the model is demonstrated and evaluated through
instantiation which serves as a basis for further dedicated efforts that aim at
building responsible and trustworthy intelligent systems for assessing the
effects produced by engaging AI systems in military operations.

</details>


### [68] [LLM-empowered knowledge graph construction: A survey](https://arxiv.org/abs/2510.20345)
*Haonan Bian*

Main category: cs.AI

TL;DR: 本文综述了大型语言模型如何变革知识图谱构建，从传统规则统计方法转向语言驱动的生成框架，系统分析了LLM在知识图谱构建三个层次（本体工程、知识抽取、知识融合）中的重塑作用。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型的出现，知识图谱构建需要从传统的规则和统计方法转向语言驱动和生成框架，以更好地融合符号知识工程与神经语义理解。

Method: 从两个互补视角回顾新兴的LLM驱动方法：基于模式的范式（强调结构、规范化和一致性）和无模式范式（强调灵活性、适应性和开放发现），并在每个阶段综合代表性框架、分析技术机制并识别局限性。

Result: 系统梳理了LLM赋能知识图谱构建的最新进展，澄清了LLM与知识图谱之间不断演变的相互作用，为开发自适应、可解释和智能的知识系统奠定了基础。

Conclusion: 该综述为KG与LLM的融合指明了关键趋势和未来研究方向，包括基于KG的LLM推理、智能体系统的动态知识记忆以及多模态KG构建，旨在实现符号知识工程与神经语义理解的桥梁。

Abstract: Knowledge Graphs (KGs) have long served as a fundamental infrastructure for
structured knowledge representation and reasoning. With the advent of Large
Language Models (LLMs), the construction of KGs has entered a new
paradigm-shifting from rule-based and statistical pipelines to language-driven
and generative frameworks. This survey provides a comprehensive overview of
recent progress in LLM-empowered knowledge graph construction, systematically
analyzing how LLMs reshape the classical three-layered pipeline of ontology
engineering, knowledge extraction, and knowledge fusion.
  We first revisit traditional KG methodologies to establish conceptual
foundations, and then review emerging LLM-driven approaches from two
complementary perspectives: schema-based paradigms, which emphasize structure,
normalization, and consistency; and schema-free paradigms, which highlight
flexibility, adaptability, and open discovery. Across each stage, we synthesize
representative frameworks, analyze their technical mechanisms, and identify
their limitations.
  Finally, the survey outlines key trends and future research directions,
including KG-based reasoning for LLMs, dynamic knowledge memory for agentic
systems, and multimodal KG construction. Through this systematic review, we aim
to clarify the evolving interplay between LLMs and knowledge graphs, bridging
symbolic knowledge engineering and neural semantic understanding toward the
development of adaptive, explainable, and intelligent knowledge systems.

</details>


### [69] [IKnow: Instruction-Knowledge-Aware Continual Pretraining for Effective Domain Adaptation](https://arxiv.org/abs/2510.20377)
*Tianyi Zhang,Florian Mai,Lucie Flek*

Main category: cs.AI

TL;DR: IKnow是一个无需外部资源或原始基础模型的持续预训练框架，通过指令-响应对话格式的自监督目标，在保持指令跟随能力的同时适应新领域。


<details>
  <summary>Details</summary>
Motivation: 解决指令调优模型在持续预训练中指令跟随能力退化的问题，避免依赖原始基础模型或外部知识库的限制。

Method: 提出IKnow框架，使用指令-响应对话格式的自监督目标，从文本本身提取领域知识并编码到更深语义层次。

Result: 无需外部资源即可有效适应新领域，同时保持模型的指令跟随能力。

Conclusion: IKnow提供了一个简单通用的解决方案，在受限环境下实现语言模型的持续领域适应。

Abstract: Continual pretraining promises to adapt large language models (LLMs) to new
domains using only unlabeled test-time data, but naively applying standard
self-supervised objectives to instruction-tuned models is known to degrade
their instruction-following capability and semantic representations. Existing
fixes assume access to the original base model or rely on knowledge from an
external domain-specific database - both of which pose a realistic barrier in
settings where the base model weights are withheld for safety reasons or
reliable external corpora are unavailable. In this work, we propose
Instruction-Knowledge-Aware Continual Adaptation (IKnow), a simple and general
framework that formulates novel self-supervised objectives in the
instruction-response dialogue format. Rather than depend- ing on external
resources, IKnow leverages domain knowledge embedded within the text itself and
learns to encode it at a deeper semantic level.

</details>


### [70] [A computational model and tool for generating more novel opportunities in professional innovation processes](https://arxiv.org/abs/2510.20402)
*Neil Maiden,Konstantinos Zachos,James Lockerbie,Kostas Petrianakis,Amanda Brown*

Main category: cs.AI

TL;DR: 提出了一种基于创造力理论的计算模型，能够生成更具新颖性的创新机会，在酒店业创新项目中验证了其优于Notebook LM和ChatGPT4o的表现。


<details>
  <summary>Details</summary>
Motivation: 开发一个计算模型来生成既新颖又有用的创新机会，解决现有方法在创新性方面的不足。

Method: 实现了五个功能函数，基于创造力理论和技巧构建计算模型，在酒店业创新项目中进行机会生成和评估。

Result: 该模型生成的创新机会在新颖性和/或实用性方面优于Notebook LM和ChatGPT4o，但并非所有功能都贡献于新颖性的提升。

Conclusion: 该计算模型在生成创新机会方面表现优异，但部分功能效果有限，为后续模型开发提供了改进方向。

Abstract: This paper presents a new computational model of creative outcomes, informed
by creativity theories and techniques, which was implemented to generate more
novel opportunities for innovation projects. The model implemented five
functions that were developed to contribute to the generation of innovation
opportunities with higher novelty without loss of usefulness. The model was
evaluated using opportunities generated for an innovation project in the
hospitality sector. The evaluation revealed that the computational model
generated outcomes that were more novel and/or useful than outcomes from
Notebook LM and ChatGPT4o. However, not all model functions contributed to the
generation of more novel opportunities, leading to new directions for further
model development

</details>


### [71] [Neural Reasoning for Robust Instance Retrieval in $\mathcal{SHOIQ}$](https://arxiv.org/abs/2510.20457)
*Louis Mozart Kamdem Teyou,Luke Friedrichs,N'Dah Jean Kouagou,Caglar Demir,Yasir Mahmood,Stefan Heindorf,Axel-Cyrille Ngonga Ngomo*

Main category: cs.AI

TL;DR: 提出了一种名为EBR的神经推理器，使用嵌入来近似符号推理器的结果，解决了传统描述逻辑推理器对不一致和错误数据不鲁棒的问题。


<details>
  <summary>Details</summary>
Motivation: 现有的神经符号概念学习方法依赖描述逻辑推理器，但这些推理器对知识库中的不一致和错误数据不鲁棒，限制了在真实世界知识库中的应用。

Method: EBR神经推理器利用嵌入来近似符号推理器的结果，仅需要检索原子概念和存在限制的实例，就能近似描述逻辑SHOIQ中任何概念的实例集合。

Result: 实验表明EBR在缺失和错误数据情况下表现出鲁棒性，优于现有最先进的推理器。

Conclusion: EBR为在真实世界知识库中部署神经符号概念学习方法提供了可行的解决方案，通过神经推理克服了传统符号推理的局限性。

Abstract: Concept learning exploits background knowledge in the form of description
logic axioms to learn explainable classification models from knowledge bases.
Despite recent breakthroughs in neuro-symbolic concept learning, most
approaches still cannot be deployed on real-world knowledge bases. This is due
to their use of description logic reasoners, which are not robust against
inconsistencies nor erroneous data. We address this challenge by presenting a
novel neural reasoner dubbed EBR. Our reasoner relies on embeddings to
approximate the results of a symbolic reasoner. We show that EBR solely
requires retrieving instances for atomic concepts and existential restrictions
to retrieve or approximate the set of instances of any concept in the
description logic $\mathcal{SHOIQ}$. In our experiments, we compare EBR with
state-of-the-art reasoners. Our results suggest that EBR is robust against
missing and erroneous data in contrast to existing reasoners.

</details>


### [72] [FLORA: Unsupervised Knowledge Graph Alignment by Fuzzy Logic](https://arxiv.org/abs/2510.20467)
*Yiwen Peng,Thomas Bonald,Fabian M. Suchanek*

Main category: cs.AI

TL;DR: FLORA是一种基于模糊逻辑的无监督知识图谱对齐方法，能够同时对齐实体和关系，提供可解释的结果并在主要基准测试中达到最先进水平。


<details>
  <summary>Details</summary>
Motivation: 现有知识图谱对齐方法主要关注纯实体级对齐，缺乏可解释性推理且需要训练数据。

Method: 基于模糊逻辑的迭代方法，提供实体和关系的整体对齐，允许悬空实体，并具有可证明的收敛性。

Result: 在主要基准测试中达到最先进的结果。

Conclusion: FLORA是一种简单有效的无监督知识图谱对齐方法，具有可解释性、收敛性和处理悬空实体的能力。

Abstract: Knowledge graph alignment is the task of matching equivalent entities (that
is, instances and classes) and relations across two knowledge graphs. Most
existing methods focus on pure entity-level alignment, computing the similarity
of entities in some embedding space. They lack interpretable reasoning and need
training data to work. In this paper, we propose FLORA, a simple yet effective
method that (1) is unsupervised, i.e., does not require training data, (2)
provides a holistic alignment for entities and relations iteratively, (3) is
based on fuzzy logic and thus delivers interpretable results, (4) provably
converges, (5) allows dangling entities, i.e., entities without a counterpart
in the other KG, and (6) achieves state-of-the-art results on major benchmarks.

</details>


### [73] [Lost in Translation: Policymakers are not really listening to Citizen Concerns about AI](https://arxiv.org/abs/2510.20568)
*Susan Ariel Aaronson,Michael Moreno*

Main category: cs.AI

TL;DR: 该研究比较了澳大利亚、哥伦比亚和美国三个国家在AI治理中的公众参与情况，发现政府未能建立有效的对话机制，参与率低且反馈响应不足，导致参与式AI治理的承诺与实践之间存在差距。


<details>
  <summary>Details</summary>
Motivation: 公众对AI有强烈意见并希望政策制定者倾听，但政府在将公众意见转化为政策时丢失了大量信息，错失了建立AI信任的关键机会。

Method: 采用景观分析法，比较三个国家如何征集公众对AI风险和政策的反馈，以及这些意见是否影响了治理决策。

Result: 在三个国家中，政府都未能建立有意义的对话，参与率不到人口的1%，政府缺乏对反馈的响应性，未能创建有效的反馈循环。

Conclusion: 当前的方法不太可能建立AI的信任或合法性，因为政策制定者没有充分倾听或回应公众关切。作者提出了八项改进建议。

Abstract: The worlds people have strong opinions about artificial intelligence (AI),
and they want policymakers to listen. Governments are inviting public comment
on AI, but as they translate input into policy, much of what citizens say is
lost. Policymakers are missing a critical opportunity to build trust in AI and
its governance. This paper compares three countries, Australia, Colombia, and
the United States, that invited citizens to comment on AI risks and policies.
Using a landscape analysis, the authors examined how each government solicited
feedback and whether that input shaped governance. Yet in none of the three
cases did citizens and policymakers establish a meaningful dialogue.
Governments did little to attract diverse voices or publicize calls for
comment, leaving most citizens unaware or unprepared to respond. In each
nation, fewer than one percent of the population participated. Moreover,
officials showed limited responsiveness to the feedback they received, failing
to create an effective feedback loop. The study finds a persistent gap between
the promise and practice of participatory AI governance. The authors conclude
that current approaches are unlikely to build trust or legitimacy in AI because
policymakers are not adequately listening or responding to public concerns.
They offer eight recommendations: promote AI literacy; monitor public feedback;
broaden outreach; hold regular online forums; use innovative engagement
methods; include underrepresented groups; respond publicly to input; and make
participation easier.

</details>


### [74] [Transferable Graph Learning for Transmission Congestion Management via Busbar Splitting](https://arxiv.org/abs/2510.20591)
*Ali Rajaei,Peter Palensky,Jochen L. Cremer*

Main category: cs.AI

TL;DR: 提出基于图神经网络的电网拓扑优化方法，通过母线分裂缓解输电拥堵，实现大规模系统近实时优化，比传统方法快4个数量级。


<details>
  <summary>Details</summary>
Motivation: 传统混合整数非线性优化方法无法在大规模系统中实现近实时电网拓扑优化，机器学习方法存在泛化能力不足的问题。

Method: 开发异构边感知消息传递神经网络，预测有效的母线分裂动作作为候选拓扑优化解，考虑线性化交流潮流。

Result: 在GOC 2000节点系统中，实现4个数量级的加速，1分钟内提供交流可行解，最优性差距仅为2.3%。

Conclusion: 该方法在拓扑和跨系统泛化方面取得显著进展，为大规模系统近实时拓扑优化提供了可行方案。

Abstract: Network topology optimization (NTO) via busbar splitting can mitigate
transmission grid congestion and reduce redispatch costs. However, solving this
mixed-integer non-linear problem for large-scale systems in near-real-time is
currently intractable with existing solvers. Machine learning (ML) approaches
have emerged as a promising alternative, but they have limited generalization
to unseen topologies, varying operating conditions, and different systems,
which limits their practical applicability. This paper formulates NTO for
congestion management problem considering linearized AC PF, and proposes a
graph neural network (GNN)-accelerated approach. We develop a heterogeneous
edge-aware message passing NN to predict effective busbar splitting actions as
candidate NTO solutions. The proposed GNN captures local flow patterns,
achieves generalization to unseen topology changes, and improves
transferability across systems. Case studies show up to 4 orders-of-magnitude
speed-up, delivering AC-feasible solutions within one minute and a 2.3%
optimality gap on the GOC 2000-bus system. These results demonstrate a
significant step toward near-real-time NTO for large-scale systems with
topology and cross-system generalization.

</details>


### [75] [What Defines Good Reasoning in LLMs? Dissecting Reasoning Steps with Multi-Aspect Evaluation](https://arxiv.org/abs/2510.20603)
*Heejin Do,Jaehui Hwang,Dongyoon Han,Seong Joon Oh,Sangdoo Yun*

Main category: cs.AI

TL;DR: 提出因果逐步评估方法(CaSE)来评估LLM推理过程的质量，将推理质量分解为相关性和连贯性两个维度，并通过基于CaSE评估的训练数据筛选来提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 当前仅评估最终答案正确性的方法过于粗糙，忽视了推理过程的质量，无法为模型改进提供有效信号。需要更细粒度的推理评估来构建更鲁棒的模型。

Method: 提出CaSE方法，将推理质量分解为相关性和连贯性：相关性衡量步骤是否基于问题，连贯性衡量步骤是否从先前步骤逻辑推导。CaSE仅使用前文上下文评估每个推理步骤，避免后见之明偏差。

Result: 在专家标注的MRa-GSM8K和MRa-MATH基准上验证CaSE与人工判断的一致性。更重要的是，使用CaSE评估的相关性和连贯性来筛选训练数据，直接提升了最终任务性能。

Conclusion: 这项工作为分析、调试和改进LLM推理提供了可扩展框架，证明了超越有效性检查的实际价值。

Abstract: Evaluating large language models (LLMs) on final-answer correctness is the
dominant paradigm. This approach, however, provides a coarse signal for model
improvement and overlooks the quality of the underlying reasoning process. We
argue that a more granular evaluation of reasoning offers a more effective path
to building robust models. We decompose reasoning quality into two dimensions:
relevance and coherence. Relevance measures if a step is grounded in the
problem; coherence measures if it follows logically from prior steps. To
measure these aspects reliably, we introduce causal stepwise evaluation (CaSE).
This method assesses each reasoning step using only its preceding context,
which avoids hindsight bias. We validate CaSE against human judgments on our
new expert-annotated benchmarks, MRa-GSM8K and MRa-MATH. More importantly, we
show that curating training data with CaSE-evaluated relevance and coherence
directly improves final task performance. Our work provides a scalable
framework for analyzing, debugging, and improving LLM reasoning, demonstrating
the practical value of moving beyond validity checks.

</details>


### [76] [Efficient Algorithms for Computing Random Walk Centrality](https://arxiv.org/abs/2510.20604)
*Changan Liu,Zixuan Xie,Ahad N. Zehmakan,Zhongzhi Zhang*

Main category: cs.AI

TL;DR: 提出了两种近线性时间算法来计算随机游走中心性，解决了大规模网络计算效率低的问题。


<details>
  <summary>Details</summary>
Motivation: 随机游走中心性是图挖掘中量化节点重要性的基本指标，但现有方法计算成本高，难以应用于大规模网络。

Method: 提出了两种可扩展算法：一种基于近似Cholesky分解和稀疏逆估计，另一种基于采样根生成树。

Result: 在大型真实网络（包括超过1000万个节点的网络）上的实验证明了算法的高效性和近似质量。

Conclusion: 所提出的算法在近线性时间内运行，并提供强近似保证，能够有效计算大规模网络的随机游走中心性。

Abstract: Random walk centrality is a fundamental metric in graph mining for
quantifying node importance and influence, defined as the weighted average of
hitting times to a node from all other nodes. Despite its ability to capture
rich graph structural information and its wide range of applications, computing
this measure for large networks remains impractical due to the computational
demands of existing methods. In this paper, we present a novel formulation of
random walk centrality, underpinning two scalable algorithms: one leveraging
approximate Cholesky factorization and sparse inverse estimation, while the
other sampling rooted spanning trees. Both algorithms operate in near-linear
time and provide strong approximation guarantees. Extensive experiments on
large real-world networks, including one with over 10 million nodes,
demonstrate the efficiency and approximation quality of the proposed
algorithms.

</details>


### [77] [Towards the Formalization of a Trustworthy AI for Mining Interpretable Models explOiting Sophisticated Algorithms](https://arxiv.org/abs/2510.20621)
*Riccardo Guidotti,Martina Cinquini,Marta Marchiori Manerba,Mattia Setzu,Francesco Spinnato*

Main category: cs.AI

TL;DR: MIMOSA框架是一个可解释性优先的预测模型生成方法，在平衡可解释性与性能的同时嵌入因果性、公平性和隐私性等伦理属性。


<details>
  <summary>Details</summary>
Motivation: 需要建立可信赖的自动化决策模型，在保持准确性的同时确保可解释性和伦理合规性。

Method: 形式化定义三类可解释模型（特征重要性、规则、实例），分析其可解释性维度、推理机制和复杂性，并整合因果性、公平性和隐私性的正式定义与验证方法。

Result: 建立了理论框架，能够生成既准确可解释又符合伦理要求的AI系统。

Conclusion: MIMOSA框架为开发可信赖AI系统奠定了理论基础，实现了可解释性、性能与伦理属性的平衡。

Abstract: Interpretable-by-design models are crucial for fostering trust,
accountability, and safe adoption of automated decision-making models in
real-world applications. In this paper we formalize the ground for the MIMOSA
(Mining Interpretable Models explOiting Sophisticated Algorithms) framework, a
comprehensive methodology for generating predictive models that balance
interpretability with performance while embedding key ethical properties. We
formally define here the supervised learning setting across diverse
decision-making tasks and data types, including tabular data, time series,
images, text, transactions, and trajectories. We characterize three major
families of interpretable models: feature importance, rule, and instance based
models. For each family, we analyze their interpretability dimensions,
reasoning mechanisms, and complexity. Beyond interpretability, we formalize
three critical ethical properties, namely causality, fairness, and privacy,
providing formal definitions, evaluation metrics, and verification procedures
for each. We then examine the inherent trade-offs between these properties and
discuss how privacy requirements, fairness constraints, and causal reasoning
can be embedded within interpretable pipelines. By evaluating ethical measures
during model generation, this framework establishes the theoretical foundations
for developing AI systems that are not only accurate and interpretable but also
fair, privacy-preserving, and causally aware, i.e., trustworthy.

</details>


### [78] [Towards Reliable Evaluation of Large Language Models for Multilingual and Multimodal E-Commerce Applications](https://arxiv.org/abs/2510.20632)
*Shuyi Xie,Ziqin Liew,Hailing Zhang,Haibo Zhang,Ling Hu,Zhiqiang Zhou,Shuman Liu,Anxiang Zeng*

Main category: cs.AI

TL;DR: EcomEval是一个全面的多语言多模态电子商务基准测试，覆盖6个类别37个任务，旨在解决现有评估方法在任务多样性、模态覆盖和语言支持方面的不足。


<details>
  <summary>Details</summary>
Motivation: 现有电子商务评估基准存在任务多样性有限、缺乏多模态数据、使用合成数据以及语言覆盖不足等问题，无法有效评估LLM在复杂真实购物场景中的表现。

Method: 采用半自动流程，由大模型生成候选回答，再由50多名电商和多语言专家审核修改；定义难度级别，覆盖7种语言（包括5种东南亚低资源语言）。

Result: 构建了一个包含真实客户查询和交易日志的基准测试，反映了真实商业交互的噪声和异质性特征。

Conclusion: EcomEval为评估LLM在电子商务领域的性能提供了可靠工具，填补了现有评估方法的空白，支持多语言和多模态的细粒度评估。

Abstract: Large Language Models (LLMs) excel on general-purpose NLP benchmarks, yet
their capabilities in specialized domains remain underexplored. In e-commerce,
existing evaluations-such as EcomInstruct, ChineseEcomQA, eCeLLM, and Shopping
MMLU-suffer from limited task diversity (e.g., lacking product guidance and
after-sales issues), limited task modalities (e.g., absence of multimodal
data), synthetic or curated data, and a narrow focus on English and Chinese,
leaving practitioners without reliable tools to assess models on complex,
real-world shopping scenarios. We introduce EcomEval, a comprehensive
multilingual and multimodal benchmark for evaluating LLMs in e-commerce.
EcomEval covers six categories and 37 tasks (including 8 multimodal tasks),
sourced primarily from authentic customer queries and transaction logs,
reflecting the noisy and heterogeneous nature of real business interactions. To
ensure both quality and scalability of reference answers, we adopt a
semi-automatic pipeline in which large models draft candidate responses
subsequently reviewed and modified by over 50 expert annotators with strong
e-commerce and multilingual expertise. We define difficulty levels for each
question and task category by averaging evaluation scores across models with
different sizes and capabilities, enabling challenge-oriented and fine-grained
assessment. EcomEval also spans seven languages-including five low-resource
Southeast Asian languages-offering a multilingual perspective absent from prior
work.

</details>


### [79] [Fluidity Index: Next-Generation Super-intelligence Benchmarks](https://arxiv.org/abs/2510.20636)
*Eric Ngoiya,Tianshu Bao*

Main category: cs.AI

TL;DR: 本文提出流动性指数(FI)来量化模型在动态扩展环境中的适应性，通过评估初始、当前和未来环境状态偏差来测试模型的上下文切换和连续性能力。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏衡量模型在动态扩展环境中适应性的量化指标，需要评估模型理解、预测和适应状态变化的能力。

Method: 建立基于环境状态偏差的基准测试，区分封闭式和开放式基准，优先采用闭环开放式真实世界基准来评估模型适应性。

Result: 提出了流动性指数(FI)作为量化模型适应性的指标，能够评估模型在动态环境中的表现。

Conclusion: 真正超级智能的模型应至少具备二阶适应性，通过数字补充实现自持计算以达到最佳流动性。

Abstract: This paper introduces the Fluidity Index (FI) to quantify model adaptability
in dynamic, scaling environments. The benchmark evaluates response accuracy
based on deviations in initial, current, and future environment states,
assessing context switching and continuity. We distinguish between closed-ended
and open-ended benchmarks, prioritizing closed-loop open-ended real-world
benchmarks to test adaptability. The approach measures a model's ability to
understand, predict, and adjust to state changes in scaling environments. A
truly super-intelligent model should exhibit at least second-order
adaptability, enabling self-sustained computation through digital replenishment
for optimal fluidity.

</details>


### [80] [Integrating Machine Learning into Belief-Desire-Intention Agents: Current Advances and Open Challenges](https://arxiv.org/abs/2510.20641)
*Andrea Agiollo,Andrea Omicini*

Main category: cs.AI

TL;DR: 本文对将机器学习集成到理性智能体架构中的现有方法进行了系统化分析，特别关注BDI（信念-欲望-意图）范式，识别了关键研究机会和开放挑战。


<details>
  <summary>Details</summary>
Motivation: 由于机器学习模型在感知和认知任务中展现出类人能力，将ML集成到理性智能体架构中的框架日益受到关注，但现有研究零散且不连贯，往往忽视了理性架构的表达能力。

Method: 使用BDI范式作为参考，对现有方法进行细粒度系统化分析，说明ML增强理性智能体的快速发展文献。

Result: 分析揭示了ML增强理性智能体文献的快速演进，并识别了关键研究机会。

Conclusion: 本文为设计有效的理性ML智能体确定了开放挑战和研究方向。

Abstract: Thanks to the remarkable human-like capabilities of machine learning (ML)
models in perceptual and cognitive tasks, frameworks integrating ML within
rational agent architectures are gaining traction. Yet, the landscape remains
fragmented and incoherent, often focusing on embedding ML into generic agent
containers while overlooking the expressive power of rational
architectures--such as Belief-Desire-Intention (BDI) agents. This paper
presents a fine-grained systematisation of existing approaches, using the BDI
paradigm as a reference. Our analysis illustrates the fast-evolving literature
on rational agents enhanced by ML, and identifies key research opportunities
and open challenges for designing effective rational ML agents.

</details>


### [81] [The Shape of Reasoning: Topological Analysis of Reasoning Traces in Large Language Models](https://arxiv.org/abs/2510.20665)
*Xue Wen Tan,Nathaniel Tan,Galen Lee,Stanley Kok*

Main category: cs.AI

TL;DR: 提出基于拓扑数据分析(TDA)的框架来评估大语言模型推理轨迹的质量，相比传统图度量能更有效地捕捉推理的几何结构特征。


<details>
  <summary>Details</summary>
Motivation: 当前评估大语言模型推理轨迹质量的方法存在不足：依赖专家评分、人工标注和缓慢的成对判断，自动化方法主要使用图结构代理指标，但无法明确高质量推理的构成要素。

Method: 引入拓扑数据分析(TDA)框架，通过捕捉推理轨迹的几何结构特征来实现标签高效的自动化评估。

Result: 拓扑特征在评估推理质量方面比标准图度量具有显著更高的预测能力，表明有效推理更适合用高维几何结构而非纯关系图来捕捉。

Conclusion: 紧凑且稳定的拓扑特征集能可靠指示轨迹质量，为未来强化学习算法提供了实用的信号。

Abstract: Evaluating the quality of reasoning traces from large language models remains
understudied, labor-intensive, and unreliable: current practice relies on
expert rubrics, manual annotation, and slow pairwise judgments. Automated
efforts are dominated by graph-based proxies that quantify structural
connectivity but do not clarify what constitutes high-quality reasoning; such
abstractions can be overly simplistic for inherently complex processes. We
introduce a topological data analysis (TDA)-based evaluation framework that
captures the geometry of reasoning traces and enables label-efficient,
automated assessment. In our empirical study, topological features yield
substantially higher predictive power for assessing reasoning quality than
standard graph metrics, suggesting that effective reasoning is better captured
by higher-dimensional geometric structures rather than purely relational
graphs. We further show that a compact, stable set of topological features
reliably indicates trace quality, offering a practical signal for future
reinforcement learning algorithms.

</details>


### [82] [Plan Then Retrieve: Reinforcement Learning-Guided Complex Reasoning over Knowledge Graphs](https://arxiv.org/abs/2510.20691)
*Yanlin Song,Ben Liu,Víctor Gutiérrez-Basulto,Zhiwei Hu,Qianqian Xie,Min Peng,Sophia Ananiadou,Jeff Z. Pan*

Main category: cs.AI

TL;DR: 提出了Graph-RFT框架，通过两阶段强化微调使LLM能够在知识不完整条件下进行自主规划和跨知识图谱与网络的自适应检索调度，解决复杂KGQA问题。


<details>
  <summary>Details</summary>
Motivation: 现有KGQA方法难以充分利用知识图谱的丰富知识和LLM的推理能力，在复杂场景下存在知识覆盖不完整、缺乏外部信息判断机制、推理短视等问题。

Method: 采用两阶段强化微调：1) 使用定制化规划检索数据集进行思维链微调；2) 引入规划检索引导的强化学习过程，集成显式规划和检索动作，采用笛卡尔式规划模块分解复杂问题。

Result: Graph-RFT能够实现自主规划、自适应检索调度，在知识不完整条件下有效结合KG和网络检索，提升多步推理的全局一致性。

Conclusion: 该框架通过强化学习优化推理检索过程，使模型学会何时以及如何有效结合KG和网络检索，解决了复杂KGQA中的关键挑战。

Abstract: Knowledge Graph Question Answering aims to answer natural language questions
by reasoning over structured knowledge graphs. While large language models have
advanced KGQA through their strong reasoning capabilities, existing methods
continue to struggle to fully exploit both the rich knowledge encoded in KGs
and the reasoning capabilities of LLMs, particularly in complex scenarios. They
often assume complete KG coverage and lack mechanisms to judge when external
information is needed, and their reasoning remains locally myopic, failing to
maintain coherent multi-step planning, leading to reasoning failures even when
relevant knowledge exists. We propose Graph-RFT, a novel two-stage
reinforcement fine-tuning KGQA framework with a
'plan-KGsearch-and-Websearch-during-think' paradigm, that enables LLMs to
perform autonomous planning and adaptive retrieval scheduling across KG and web
sources under incomplete knowledge conditions. Graph-RFT introduces a
chain-of-thought fine-tuning method with a customized plan-retrieval dataset
activates structured reasoning and resolves the GRPO cold-start problem. It
then introduces a novel plan-retrieval guided reinforcement learning process
integrates explicit planning and retrieval actions with a multi-reward design,
enabling coverage-aware retrieval scheduling. It employs a Cartesian-inspired
planning module to decompose complex questions into ordered subquestions, and
logical expression to guide tool invocation for globally consistent multi-step
reasoning. This reasoning retrieval process is optimized with a multi-reward
combining outcome and retrieval specific signals, enabling the model to learn
when and how to combine KG and web retrieval effectively.

</details>


### [83] [A Coherence-Based Measure of AGI](https://arxiv.org/abs/2510.20784)
*Fares Fourati*

Main category: cs.AI

TL;DR: 本文提出了一种基于广义均值积分的AGI一致性度量方法，替代了传统的算术平均定义，强调跨认知领域的平衡能力而非补偿性。


<details>
  <summary>Details</summary>
Motivation: 现有AGI定义使用算术平均，假设领域间能力可以相互补偿，但真正的通用智能应该体现所有关键领域的平衡能力。

Method: 提出基于广义均值积分的一致性感知AGI度量，涵盖算术、几何和调和均值的不同补偿性假设，通过曲线下面积量化鲁棒性。

Result: 应用于GPT-4和GPT-5的CHC领域得分，一致性调整的AUC显示尽管算术得分较高（GPT-5达24%），但两者距离通用能力仍很远。

Conclusion: 广义均值积分提供了更严格、可解释的AGI进展测量基础，惩罚能力不平衡并捕捉领域间依赖关系。

Abstract: Recent work by \citet{hendrycks2025agidefinition} formalized
\textit{Artificial General Intelligence} (AGI) as the arithmetic mean of
proficiencies across cognitive domains derived from the Cattell--Horn--Carroll
(CHC) model of human cognition. While elegant, this definition assumes
\textit{compensability} -- that exceptional ability in some domains can offset
failure in others. True general intelligence, however, should reflect
\textit{coherent sufficiency}: balanced competence across all essential
domains. We propose a coherence-aware measure of AGI based on the integral of
generalized means over a continuum of compensability exponents. This
formulation spans arithmetic, geometric, and harmonic regimes, and the
resulting \textit{area under the curve} (AUC) quantifies robustness under
varying compensability assumptions. Unlike the arithmetic mean, which rewards
specialization, the AUC penalizes imbalance and captures inter-domain
dependency. Applied to published CHC-based domain scores for GPT-4 and GPT-5,
the coherence-adjusted AUC reveals that both systems remain far from general
competence despite high arithmetic scores (e.g., GPT-5 at~24\%). Integrating
the generalized mean thus yields a principled, interpretable, and stricter
foundation for measuring genuine progress toward AGI.

</details>


### [84] [Real Deep Research for AI, Robotics and Beyond](https://arxiv.org/abs/2510.20809)
*Xueyan Zou,Jianglong Ye,Hao Zhang,Xiaoyu Xiang,Mingyu Ding,Zhaojing Yang,Yong Jae Lee,Zhuowen Tu,Sifei Liu,Xiaolong Wang*

Main category: cs.AI

TL;DR: 提出Real Deep Research (RDR)框架，用于系统分析AI和机器人领域的研究趋势，帮助研究人员应对论文数量激增和跨学科研究的挑战。


<details>
  <summary>Details</summary>
Motivation: AI和机器人领域每年产生超过10,000篇论文，研究人员难以跟上快速发展的趋势，需要探索跨领域机会。

Method: 构建通用的RDR管道，能够系统识别新兴趋势、发现跨领域机会，并为新研究提供具体起点。

Result: 将RDR框架应用于AI和机器人领域，特别关注基础模型和机器人技术进展，并扩展到其他科学领域。

Conclusion: RDR框架为AI及其他领域的研究人员提供了有价值的分析工具，帮助他们在快速发展的研究环境中保持更新。

Abstract: With the rapid growth of research in AI and robotics now producing over
10,000 papers annually it has become increasingly difficult for researchers to
stay up to date. Fast evolving trends, the rise of interdisciplinary work, and
the need to explore domains beyond one's expertise all contribute to this
challenge. To address these issues, we propose a generalizable pipeline capable
of systematically analyzing any research area: identifying emerging trends,
uncovering cross domain opportunities, and offering concrete starting points
for new inquiry. In this work, we present Real Deep Research (RDR) a
comprehensive framework applied to the domains of AI and robotics, with a
particular focus on foundation models and robotics advancements. We also
briefly extend our analysis to other areas of science. The main paper details
the construction of the RDR pipeline, while the appendix provides extensive
results across each analyzed topic. We hope this work sheds light for
researchers working in the field of AI and beyond.

</details>
