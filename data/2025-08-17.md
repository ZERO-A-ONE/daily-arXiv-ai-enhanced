<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 6]
- [cs.CR](#cs.CR) [Total: 24]
- [cs.AI](#cs.AI) [Total: 31]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [FormalGrad: Integrating Formal Methods with Gradient-Based LLM Refinement](https://arxiv.org/abs/2508.10059)
*Yueke Zhang,Yifan Zhang,Kevin Leach,Yu Huang*

Main category: cs.SE

TL;DR: FormalGrad通过将形式化方法集成到LLM代码生成循环中，利用伪梯度指导模型迭代优化，显著提升代码的正确性、鲁棒性和效率。


<details>
  <summary>Details</summary>
Motivation: LLM生成的代码常缺乏正确性、鲁棒性和效率的保证，尤其在严格约束的领域。FormalGrad旨在解决这一问题。

Method: 将代码视为可微分变量，将形式化约束转化为文本伪梯度，指导LLM迭代优化代码。

Result: 在HumanEval、HumanEval+和LiveCodeBench基准测试中表现优异，绝对提升达27%，相对提升达41%。

Conclusion: FormalGrad为高风险应用中的可靠AI辅助软件开发提供了新途径。

Abstract: While Large Language Models (LLMs) have demonstrated remarkable capabilities
in code generation, they often produce solutions that lack guarantees of
correctness, robustness, and efficiency. The limitation is acute in domains
requiring strict constraints. FormalGrad introduces a principled framework that
integrates formal methods directly into an iterative LLM-based generation loop.
It uniquely treats code as a differentiable variable, converting structured
feedback and formal constraints into a textual pseudo-gradient. This gradient
guides the model to iteratively refine solutions, ensuring they are not only
functional but also robust and formally justified. We evaluate FormalGrad on
the HumanEval, HumanEval+, and LiveCodeBench benchmarks. Our implementation
outperforms strong baselines, achieving an absolute improvement of up to 27% on
HumanEval and a 41% relative improvement on the challenging LiveCodeBench V6.
FormalGrad generates formally justified code that is robust and efficient,
paving the way for reliable AI-assisted software development in high-stakes
applications.

</details>


### [2] [SaraCoder: Orchestrating Semantic and Structural Cues for Profit-Oriented Repository-Level Code Completion](https://arxiv.org/abs/2508.10068)
*Xiaohan Chen,Zhongying Pan,Quan Feng,Yu Tian,Shuqun Yang,Mengru Wang,Lina Gong,Yuxia Geng,Piji Li,Xiang Chen*

Main category: cs.SE

TL;DR: Saracoder提出了一种分层特征优化的检索框架，解决了传统RAG在代码补全中的语义误导、冗余和同质性问题，并通过依赖分析解决了外部符号歧义。


<details>
  <summary>Details</summary>
Motivation: 传统基于文本相似性的检索方法在代码补全中存在语义误导、冗余和同质性问题，且无法解决外部符号歧义，Saracoder旨在解决这些问题。

Method: 采用分层特征优化模块，通过深度语义关系提炼、去重、基于图的结构相似性评估和结果重排，以及外部感知标识符消歧模块。

Result: 在CrossCodeEval和RepoEval-Updated基准测试中，Saracoder显著优于现有基线。

Conclusion: 通过多维度系统优化检索结果，Saracoder为构建更准确和健壮的代码补全系统提供了新范式。

Abstract: Retrieval-augmented generation (RAG) for repository-level code completion
commonly relies on superficial text similarity, leading to results plagued by
semantic misguidance, redundancy, and homogeneity, while also failing to
resolve external symbol ambiguity. To address these challenges, we introduce
Saracoder, a Hierarchical Feature-Optimized retrieval framework. Its core
Hierarchical Feature Optimization module systematically refines candidates by
distilling deep semantic relationships, pruning exact duplicates, assessing
structural similarity with a novel graph-based metric that weighs edits by
their topological importance, and reranking results to maximize both relevance
and diversity. Furthermore, an External-Aware Identifier Disambiguator module
accurately resolves cross-file symbol ambiguity via dependency analysis.
Extensive experiments on the challenging CrossCodeEval and RepoEval-Updated
benchmarks demonstrate that Saracoder significantly outperforms existing
baselines across multiple programming languages and models. Our work proves
that systematically refining retrieval results across multiple dimensions
provides a new paradigm for building more accurate and robust repository-level
code completion systems.

</details>


### [3] [Next Edit Prediction: Learning to Predict Code Edits from Context and Interaction History](https://arxiv.org/abs/2508.10074)
*Ruofan Lu,Yintong Huo,Meng Zhang,Yichen Li,Michael R. Lyu*

Main category: cs.SE

TL;DR: 论文提出了一种名为“Next Edit Prediction”的新任务，旨在通过开发者最近的交互历史推断其意图，预测下一个编辑的位置和内容，以优化AI代码助手的使用体验。


<details>
  <summary>Details</summary>
Motivation: 现有AI代码助手存在局限性：低延迟代码补全仅适用于光标当前位置，而基于聊天的编辑需要开发者中断工作并描述意图。这两种方式均未能主动预测开发者的一系列相关编辑，导致用户体验不佳。

Method: 作者构建了一个高质量的监督微调数据集和评估基准，对一系列模型进行监督微调，并与其他基线模型进行了全面评估。

Result: 研究得出了一些新发现，为一种新的交互范式奠定了基础，该范式能主动预测开发者的下一步行动，而非仅响应显式指令。

Conclusion: 该工作为AI代码助手提供了一种更无缝的协作方式，通过预测开发者的编辑意图，显著提升了开发效率和使用体验。

Abstract: The rapid advancement of large language models (LLMs) has led to the
widespread adoption of AI-powered coding assistants integrated into a
development environment. On one hand, low-latency code completion offers
completion suggestions but is fundamentally constrained to the cursor's current
position. On the other hand, chat-based editing can perform complex
modifications, yet forces developers to stop their work, describe the intent in
natural language, which causes a context-switch away from the code. This
creates a suboptimal user experience, as neither paradigm proactively predicts
the developer's next edit in a sequence of related edits. To bridge this gap
and provide the seamless code edit suggestion, we introduce the task of Next
Edit Prediction, a novel task designed to infer developer intent from recent
interaction history to predict both the location and content of the subsequent
edit. Specifically, we curate a high-quality supervised fine-tuning dataset and
an evaluation benchmark for the Next Edit Prediction task. Then, we conduct
supervised fine-tuning on a series of models and performed a comprehensive
evaluation of both the fine-tuned models and other baseline models, yielding
several novel findings. This work lays the foundation for a new interaction
paradigm that proactively collaborate with developers by anticipating their
following action, rather than merely reacting to explicit instructions.

</details>


### [4] [On the synchronization between Hugging Face pre-trained language models and their upstream GitHub repository](https://arxiv.org/abs/2508.10157)
*Ajibode Adekunle,Abdul Ali Bangash,Bram Adams,Ahmed E. Hassan*

Main category: cs.SE

TL;DR: 研究分析了预训练语言模型（PTLM）在GitHub和Hugging Face平台上的开发协调问题，揭示了八种同步模式，指出跨平台发布实践中的结构性问题。


<details>
  <summary>Details</summary>
Motivation: PTLM的开发涉及上游（GitHub）和下游（Hugging Face）平台，但两者间的协调问题（如版本不一致、变更未同步）可能导致模型不完整或过时。

Method: 采用混合方法研究了325个PTLM家族（904个Hugging Face变体），分析了提交活动的协调情况，包括延迟、同步类型和强度三个维度。

Result: 发现GitHub贡献者关注代码优化和版本管理，而Hugging Face贡献者关注模型描述和推理设置；揭示了八种同步模式，部分同步模式导致变更孤立或仓库废弃。

Conclusion: 识别同步模式对改进PTLM发布流程的监督和可追溯性至关重要，以避免用户接触不完整或行为不一致的模型。

Abstract: Pretrained language models (PTLMs) have advanced natural language processing
(NLP), enabling progress in tasks like text generation and translation. Like
software package management, PTLMs are trained using code and environment
scripts in upstream repositories (e.g., GitHub, GH) and distributed as variants
via downstream platforms like Hugging Face (HF). Coordinating development
between GH and HF poses challenges such as misaligned release timelines,
inconsistent versioning, and limited reuse of PTLM variants. We conducted a
mixed-method study of 325 PTLM families (904 HF variants) to examine how commit
activities are coordinated. Our analysis reveals that GH contributors typically
make changes related to specifying the version of the model, improving code
quality, performance optimization, and dependency management within the
training scripts, while HF contributors make changes related to improving model
descriptions, data set handling, and setup required for model inference.
Furthermore, to understand the synchronization aspects of commit activities
between GH and HF, we examined three dimensions of these activities -- lag
(delay), type of synchronization, and intensity -- which together yielded eight
distinct synchronization patterns. The prevalence of partially synchronized
patterns, such as Disperse synchronization and Sparse synchronization, reveals
structural disconnects in current cross-platform release practices. These
patterns often result in isolated changes -- where improvements or fixes made
on one platform are never replicated on the other -- and in some cases,
indicate an abandonment of one repository in favor of the other. Such
fragmentation risks exposing end users to incomplete, outdated, or behaviorally
inconsistent models. Hence, recognizing these synchronization patterns is
critical for improving oversight and traceability in PTLM release workflows.

</details>


### [5] [Bridging Solidity Evolution Gaps: An LLM-Enhanced Approach for Smart Contract Compilation Error Resolution](https://arxiv.org/abs/2508.10517)
*Likai Ye,Mengliang Li,Dehai Zhao,Jiamou Sun,Xiaoxue Ren*

Main category: cs.SE

TL;DR: 论文研究了Solidity版本更新带来的编译错误问题，并提出SMCFIXER框架结合LLMs和专家知识解决这些问题。


<details>
  <summary>Details</summary>
Motivation: Solidity频繁版本更新导致编译错误和迁移困难，影响智能合约开发和维护。

Method: 通过实证研究分析编译错误，评估LLMs修复能力，并提出SMCFIXER框架整合专家知识和LLMs。

Result: 81.68%合约跨版本编译出错，LLMs修复能力有限；SMCFIXER比GPT-4o提升24.24%，准确率达96.97%。

Conclusion: SMCFIXER显著提升Solidity编译错误修复效果，强调领域知识对LLMs的重要性。

Abstract: Solidity, the dominant smart contract language for Ethereum, has rapidly
evolved with frequent version updates to enhance security, functionality, and
developer experience. However, these continual changes introduce significant
challenges, particularly in compilation errors, code migration, and
maintenance. Therefore, we conduct an empirical study to investigate the
challenges in the Solidity version evolution and reveal that 81.68% of examined
contracts encounter errors when compiled across different versions, with 86.92%
of compilation errors.
  To mitigate these challenges, we conducted a systematic evaluation of large
language models (LLMs) for resolving Solidity compilation errors during version
migrations. Our empirical analysis across both open-source (LLaMA3, DeepSeek)
and closed-source (GPT-4o, GPT-3.5-turbo) LLMs reveals that although these
models exhibit error repair capabilities, their effectiveness diminishes
significantly for semantic-level issues and shows strong dependency on prompt
engineering strategies. This underscores the critical need for domain-specific
adaptation in developing reliable LLM-based repair systems for smart contracts.
  Building upon these insights, we introduce SMCFIXER, a novel framework that
systematically integrates expert knowledge retrieval with LLM-based repair
mechanisms for Solidity compilation error resolution. The architecture
comprises three core phases: (1) context-aware code slicing that extracts
relevant error information; (2) expert knowledge retrieval from official
documentation; and (3) iterative patch generation for Solidity migration.
Experimental validation across Solidity version migrations demonstrates our
approach's statistically significant 24.24% improvement over baseline GPT-4o on
real-world datasets, achieving near-perfect 96.97% accuracy.

</details>


### [6] [EVOSCAT: Exploring Software Change Dynamics in Large-Scale Historical Datasets](https://arxiv.org/abs/2508.10852)
*Souhaila Serbout,Diana Carolina Muñoz Hurtado,Hassan Atwi,Edoardo Riggio,Cesare Pautasso*

Main category: cs.SE

TL;DR: EvoScat是一个工具，通过交互式密度散点图提供大型历史数据集的全局概览，帮助研究人员探索和比较软件演化数据。


<details>
  <summary>Details</summary>
Motivation: 长期软件项目包含大量工件，经历多次修订，研究软件演化的研究人员需要处理数百万事件的数据集。

Method: EvoScat使用交互式密度散点图，支持灵活配置时间轴缩放、工件排序和交互式颜色映射。

Result: 工具能够分析数百万事件，适用于多种分析需求（如变更速度比较、克隆检测、新鲜度评估）。

Conclusion: EvoScat为研究人员提供了一种可扩展的可视化方法，适用于探索和比较软件演化数据集。

Abstract: Long lived software projects encompass a large number of artifacts, which
undergo many revisions throughout their history. Empirical software engineering
researchers studying software evolution gather and collect datasets with
millions of events, representing changes introduced to specific artifacts. In
this paper, we propose EvoScat, a tool that attempts addressing temporal
scalability through the usage of interactive density scatterplot to provide a
global overview of large historical datasets mined from open source
repositories in a single visualization. EvoScat intents to provide researchers
with a mean to produce scalable visualizations that can help them explore and
characterize evolution datasets, as well as comparing the histories of
individual artifacts, both in terms of 1) observing how rapidly different
artifacts age over multiple-year-long time spans 2) how often metrics
associated with each artifacts tend towards an improvement or worsening. The
paper shows how the tool can be tailored to specific analysis needs (pace of
change comparison, clone detection, freshness assessment) thanks to its support
for flexible configuration of history scaling and alignment along the time
axis, artifacts sorting and interactive color mapping, enabling the analysis of
millions of events obtained by mining the histories of tens of thousands of
software artifacts. We include in this paper a gallery showcasing datasets
gathering specific artifacts (OpenAPI descriptions, GitHub workflow
definitions) across multiple repositories, as well as diving into the history
of specific popular open source projects.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [7] [A Robust Pipeline for Differentially Private Federated Learning on Imbalanced Clinical Data using SMOTETomek and FedProx](https://arxiv.org/abs/2508.10017)
*Rodrigo Tertulino*

Main category: cs.CR

TL;DR: 该论文提出了一种结合联邦学习（FL）和差分隐私（DP）的方法，解决了医疗数据中隐私与临床效用之间的权衡问题，并通过优化算法和数据处理技术提升了模型性能。


<details>
  <summary>Details</summary>
Motivation: 医疗数据通常分散且隐私敏感，联邦学习可以在保护隐私的同时进行协作研究，但面临数据不平衡和隐私-效用权衡的挑战。

Method: 研究采用多阶段分析，首先在客户端集成SMOTETomek技术处理数据不平衡，随后优化FedProx算法以适应非独立同分布（non-IID）数据。

Result: 实验显示优化后的FedProx在隐私预算（epsilon）与召回率之间存在非线性权衡，并在epsilon为9.0时实现了高召回率（>77%）和强隐私保障。

Conclusion: 研究为开发安全、有效的医疗诊断工具提供了实用方法，适用于现实世界中异构的医疗数据。

Abstract: Federated Learning (FL) presents a groundbreaking approach for collaborative
health research, allowing model training on decentralized data while
safeguarding patient privacy. FL offers formal security guarantees when
combined with Differential Privacy (DP). The integration of these technologies,
however, introduces a significant trade-off between privacy and clinical
utility, a challenge further complicated by the severe class imbalance often
present in medical datasets. The research presented herein addresses these
interconnected issues through a systematic, multi-stage analysis. An FL
framework was implemented for cardiovascular risk prediction, where initial
experiments showed that standard methods struggled with imbalanced data,
resulting in a recall of zero. To overcome such a limitation, we first
integrated the hybrid Synthetic Minority Over-sampling Technique with Tomek
Links (SMOTETomek) at the client level, successfully developing a clinically
useful model. Subsequently, the framework was optimized for non-IID data using
a tuned FedProx algorithm. Our final results reveal a clear, non-linear
trade-off between the privacy budget (epsilon) and model recall, with the
optimized FedProx consistently out-performing standard FedAvg. An optimal
operational region was identified on the privacy-utility frontier, where strong
privacy guarantees (with epsilon 9.0) can be achieved while maintaining high
clinical utility (recall greater than 77%). Ultimately, our study provides a
practical methodological blueprint for creating effective, secure, and accurate
diagnostic tools that can be applied to real-world, heterogeneous healthcare
data.

</details>


### [8] [A Comparative Performance Evaluation of Kyber, sntrup761, and FrodoKEM for Post-Quantum Cryptography](https://arxiv.org/abs/2508.10023)
*Samet Ünsal*

Main category: cs.CR

TL;DR: 本文比较了后量子密码学（PQC）中的主要算法（如Kyber、sntrup761和FrodoKEM），分析了其安全性、性能和实际适用性，并探讨了从经典系统过渡到后量子系统的挑战及其对各行业的影响。


<details>
  <summary>Details</summary>
Motivation: 量子计算机的发展对现有密码系统构成威胁，因此需要研究能够抵御量子攻击的后量子密码算法。

Method: 通过比较分析Kyber、sntrup761和FrodoKEM等算法的安全性、性能和适用性，评估其优缺点。

Result: 总结了各算法的优缺点，并提出了未来研究方向。

Conclusion: 本文为理解后量子密码学的现状及其在量子计算时代的未来前景提供了基础。

Abstract: Post-quantum cryptography (PQC) aims to develop cryptographic algorithms that
are secure against attacks from quantum computers. This paper compares the
leading postquantum cryptographic algorithms, such as Kyber, sntrup761, and
FrodoKEM, in terms of their security, performance, and real-world
applicability. The review highlights the strengths and weaknesses of each
algorithm and provides insights into future research directions. We also
discuss the challenges of transitioning from classical to post-quantum systems
and the potential impacts on various industries. This paper serves as a
foundation for understanding the current state of post-quantum cryptography and
its future prospects in the quantum computing era.

</details>


### [9] [Context Misleads LLMs: The Role of Context Filtering in Maintaining Safe Alignment of LLMs](https://arxiv.org/abs/2508.10031)
*Jinhwa Kim,Ian G. Harris*

Main category: cs.CR

TL;DR: 提出了一种名为Context Filtering的防御机制，通过过滤不可信上下文来保护大语言模型免受恶意攻击，同时保持模型性能。


<details>
  <summary>Details</summary>
Motivation: 大语言模型面临日益增长的安全和伦理风险，恶意用户利用对抗性上下文欺骗模型生成有害响应。

Method: 提出Context Filtering模型，预处理输入以过滤不可信上下文，识别真实用户意图。

Result: 模型将攻击成功率降低高达88%，同时保持模型原有性能。

Conclusion: Context Filtering是一种即插即用的方法，适用于所有大语言模型，无需微调即可提升安全性。

Abstract: While Large Language Models (LLMs) have shown significant advancements in
performance, various jailbreak attacks have posed growing safety and ethical
risks. Malicious users often exploit adversarial context to deceive LLMs,
prompting them to generate responses to harmful queries. In this study, we
propose a new defense mechanism called Context Filtering model, an input
pre-processing method designed to filter out untrustworthy and unreliable
context while identifying the primary prompts containing the real user intent
to uncover concealed malicious intent. Given that enhancing the safety of LLMs
often compromises their helpfulness, potentially affecting the experience of
benign users, our method aims to improve the safety of the LLMs while
preserving their original performance. We evaluate the effectiveness of our
model in defending against jailbreak attacks through comparative analysis,
comparing our approach with state-of-the-art defense mechanisms against six
different attacks and assessing the helpfulness of LLMs under these defenses.
Our model demonstrates its ability to reduce the Attack Success Rates of
jailbreak attacks by up to 88% while maintaining the original LLMs'
performance, achieving state-of-the-art Safety and Helpfulness Product results.
Notably, our model is a plug-and-play method that can be applied to all LLMs,
including both white-box and black-box models, to enhance their safety without
requiring any fine-tuning of the models themselves. We will make our model
publicly available for research purposes.

</details>


### [10] [Cognitive Cybersecurity for Artificial Intelligence: Guardrail Engineering with CCS-7](https://arxiv.org/abs/2508.10033)
*Yuksel Aydin*

Main category: cs.CR

TL;DR: CCS-7提出了一种基于人类认知安全研究的语言模型认知漏洞分类法，并通过实验验证了不同架构模型对这些漏洞的响应差异。


<details>
  <summary>Details</summary>
Motivation: 研究语言模型在认知安全方面的漏洞，以填补传统行为对齐方法的不足。

Method: 通过随机对照试验（151人）建立人类基准，并在12,180次实验中评估TFVA风格防护措施对7种语言模型架构的影响。

Result: 不同模型架构对漏洞的响应差异显著，某些漏洞被有效缓解，而另一些漏洞的错误率甚至上升135%。人类表现则呈现一致的中等提升。

Conclusion: 认知安全需针对特定模型架构进行定制化测试和干预，通用方法可能无效或有害。

Abstract: Language models exhibit human-like cognitive vulnerabilities, such as
emotional framing, that escape traditional behavioral alignment. We present
CCS-7 (Cognitive Cybersecurity Suite), a taxonomy of seven vulnerabilities
grounded in human cognitive security research. To establish a human benchmark,
we ran a randomized controlled trial with 151 participants: a "Think First,
Verify Always" (TFVA) lesson improved cognitive security by +7.9% overall. We
then evaluated TFVA-style guardrails across 12,180 experiments on seven diverse
language model architectures. Results reveal architecture-dependent risk
patterns: some vulnerabilities (e.g., identity confusion) are almost fully
mitigated, while others (e.g., source interference) exhibit escalating
backfire, with error rates increasing by up to 135% in certain models. Humans,
in contrast, show consistent moderate improvement. These findings reframe
cognitive safety as a model-specific engineering problem: interventions
effective in one architecture may fail, or actively harm, another, underscoring
the need for architecture-aware cognitive safety testing before deployment.

</details>


### [11] [Neural Network-Based Detection and Multi-Class Classification of FDI Attacks in Smart Grid Home Energy Systems](https://arxiv.org/abs/2508.10035)
*Varsha Sen,Biswash Basnet*

Main category: cs.CR

TL;DR: 本文提出了一种基于机器学习的框架，用于检测和分类智能电网中的虚假数据注入攻击（FDIAs），通过轻量级人工神经网络（ANN）和双向LSTM模型实现实时检测和攻击分类。


<details>
  <summary>Details</summary>
Motivation: 虚假数据注入攻击（FDIAs）对智能电网基础设施构成严重威胁，尤其是家庭区域网络（HANs），其安全控制较弱且广泛可用，攻击者可通过操纵消费数据破坏电网运行。

Method: 使用轻量级ANN进行实时检测，提取能源消耗、成本和时间上下文的关键特征；通过双向LSTM分类不同攻击类型（正常、梯形和S形攻击）。实验基于合成的家庭行为时间序列数据集。

Result: 实验结果表明，所提模型能有效识别和分类FDIAs，为增强电网边缘韧性提供了可扩展的解决方案。

Conclusion: 该研究为智能电网网络安全提供了数据驱动的防御机制，从住宅端点加强了电网的韧性。

Abstract: False Data Injection Attacks (FDIAs) pose a significant threat to smart grid
infrastructures, particularly Home Area Networks (HANs), where real-time
monitoring and control are highly adopted. Owing to the comparatively less
stringent security controls and widespread availability of HANs, attackers view
them as an attractive entry point to manipulate aggregated demand patterns,
which can ultimately propagate and disrupt broader grid operations. These
attacks undermine the integrity of smart meter data, enabling malicious actors
to manipulate consumption values without activating conventional alarms,
thereby creating serious vulnerabilities across both residential and
utility-scale infrastructures. This paper presents a machine learning-based
framework for both the detection and classification of FDIAs using residential
energy data. A real-time detection is provided by the lightweight Artificial
Neural Network (ANN), which works by using the most vital features of energy
consumption, cost, and time context. For the classification of different attack
types, a Bidirectional LSTM is trained to recognize normal, trapezoidal, and
sigmoid attack shapes through learning sequential dependencies in the data. A
synthetic time-series dataset was generated to emulate realistic household
behaviour. Experimental results demonstrate that the proposed models are
effective in identifying and classifying FDIAs, offering a scalable solution
for enhancing grid resilience at the edge. This work contributes toward
building intelligent, data-driven defence mechanisms that strengthen smart grid
cybersecurity from residential endpoints.

</details>


### [12] [Certifiably robust malware detectors by design](https://arxiv.org/abs/2508.10038)
*Pierre-Francois Gimenez,Sarath Sivaprasad,Mario Fritz*

Main category: cs.CR

TL;DR: 论文提出了一种新的模型架构ERDALT，用于设计可证明鲁棒的恶意软件检测方法，并通过分解鲁棒检测器的结构，实现了在脆弱特征上的经验鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 静态恶意软件分析依赖机器学习技术，但易受对抗样本攻击，需要在不改变软件功能的情况下实现鲁棒检测。

Method: 提出了一种新的模型架构，并展示了鲁棒检测器的分解结构，应用于学习经验鲁棒检测器。

Result: 验证了该方法在保持检测性能的同时实现了鲁棒检测。

Conclusion: ERDALT框架为恶意软件检测提供了鲁棒性保障，且性能损失有限。

Abstract: Malware analysis involves analyzing suspicious software to detect malicious
payloads. Static malware analysis, which does not require software execution,
relies increasingly on machine learning techniques to achieve scalability.
Although such techniques obtain very high detection accuracy, they can be
easily evaded with adversarial examples where a few modifications of the sample
can dupe the detector without modifying the behavior of the software. Unlike
other domains, such as computer vision, creating an adversarial example of
malware without altering its functionality requires specific transformations.
We propose a new model architecture for certifiably robust malware detection by
design. In addition, we show that every robust detector can be decomposed into
a specific structure, which can be applied to learn empirically robust malware
detectors, even on fragile features. Our framework ERDALT is based on this
structure. We compare and validate these approaches with machine-learning-based
malware detection methods, allowing for robust detection with limited reduction
of detection performance.

</details>


### [13] [Multi-task Adversarial Attacks against Black-box Model with Few-shot Queries](https://arxiv.org/abs/2508.10039)
*Wenqiang Wang,Yan Xiao,Hao Lin,Yangshijie Zhang,Xiaochun Cao*

Main category: cs.CR

TL;DR: CEMA是一种黑盒多任务文本对抗攻击方法，通过利用对抗文本的跨任务可转移性，简化复杂多任务场景，仅需少量查询即可实现高效攻击。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖共享内部特征和大量查询，且局限于单一任务类型，难以应对黑盒反馈API、有限查询或多任务类型的实际场景。

Method: CEMA采用深度级替代模型，以即插即用方式训练，将多任务攻击转化为分类攻击，并通过不同文本分类方法生成候选对抗文本。

Result: 实验表明，CEMA在涉及2至6个任务的模型中，仅需100次查询即可实现显著攻击成功率，并能针对商业API和大模型。

Conclusion: CEMA展示了在实际应用中的多功能性和高效性，适用于多种任务和模型。

Abstract: Current multi-task adversarial text attacks rely on abundant access to shared
internal features and numerous queries, often limited to a single task type. As
a result, these attacks are less effective against practical scenarios
involving black-box feedback APIs, limited queries, or multiple task types. To
bridge this gap, we propose \textbf{C}luster and \textbf{E}nsemble
\textbf{M}ulti-task Text Adversarial \textbf{A}ttack (\textbf{CEMA}), an
effective black-box attack that exploits the transferability of adversarial
texts across different tasks. CEMA simplifies complex multi-task scenarios by
using a \textit{deep-level substitute model} trained in a
\textit{plug-and-play} manner for text classification, enabling attacks without
mimicking the victim model. This approach requires only a few queries for
training, converting multi-task attacks into classification attacks and
allowing attacks across various tasks.
  CEMA generates multiple adversarial candidates using different text
classification methods and selects the one that most effectively attacks
substitute models.
  In experiments involving multi-task models with two, three, or six
tasks--spanning classification, translation, summarization, and text-to-image
generation--CEMA demonstrates significant attack success with as few as 100
queries. Furthermore, CEMA can target commercial APIs (e.g., Baidu and Google
Translate), large language models (e.g., ChatGPT 4o), and image-generation
models (e.g., Stable Diffusion V2), showcasing its versatility and
effectiveness in real-world applications.

</details>


### [14] [Quantum Prime Factorization: A Novel Approach Based on Fermat Method](https://arxiv.org/abs/2508.10041)
*Julien Mellaerts*

Main category: cs.CR

TL;DR: 提出了一种新的量子算法用于分解合数，改进了经典费马方法，并将费马分解方法重新表述为适用于量子退火器的优化问题。


<details>
  <summary>Details</summary>
Motivation: 通过改进经典费马方法和利用量子退火器，提高大数分解的效率和能力。

Method: 改进经典费马方法，降低计算复杂度；将费马分解重新表述为优化问题，适用于量子退火器。

Result: 成功分解了8,689,739，这是目前已知用量子设备分解的最大数。

Conclusion: 新算法显著提高了分解效率，展示了量子退火器在大数分解中的潜力。

Abstract: In this paper, we introduce a novel quantum algorithm for the factorization
of composite odd numbers. This work makes two significant contributions. First,
we present a new improvement to the classical Fermat method, fourfold reducing
the computational complexity of factoring. Second, we reformulate Fermat
factorization method as an optimization problem suitable for Quantum Annealers
which allowed us to factorize 8,689,739, the biggest number ever factorized
using a quantum device to our knowledge.

</details>


### [15] [FIDELIS: Blockchain-Enabled Protection Against Poisoning Attacks in Federated Learning](https://arxiv.org/abs/2508.10042)
*Jane Carney,Kushal Upreti,Gaby G. Dagher,Tim Andersen*

Main category: cs.CR

TL;DR: 提出了一种基于区块链的联邦学习数据投毒检测框架，通过分散全局服务器角色和引入共识验证的法官模型，提高检测的鲁棒性和可扩展性。


<details>
  <summary>Details</summary>
Motivation: 联邦学习虽保护隐私，但易受数据投毒攻击，现有检测方法缺乏标准化或过度依赖信任。

Method: 设计区块链框架，分散全局服务器功能，客户端生成法官模型并通过共识验证。

Result: 框架能有效抵御数据投毒攻击，法官模型的生成具有可扩展性。

Conclusion: \Sys 框架为联邦学习中的数据投毒问题提供了高效、分散的解决方案。

Abstract: Federated learning enhances traditional deep learning by enabling the joint
training of a model with the use of IoT device's private data. It ensures
privacy for clients, but is susceptible to data poisoning attacks during
training that degrade model performance and integrity. Current poisoning
detection methods in federated learning lack a standardized detection method or
take significant liberties with trust. In this paper, we present \Sys, a novel
blockchain-enabled poison detection framework in federated learning. The
framework decentralizes the role of the global server across participating
clients. We introduce a judge model used to detect data poisoning in model
updates. The judge model is produced by each client and verified to reach
consensus on a single judge model. We implement our solution to show \Sys is
robust against data poisoning attacks and the creation of our judge model is
scalable.

</details>


### [16] [Securing Agentic AI: Threat Modeling and Risk Analysis for Network Monitoring Agentic AI System](https://arxiv.org/abs/2508.10043)
*Pallavi Zambare,Venkata Nikhil Thanikella,Ying Liu*

Main category: cs.CR

TL;DR: 研究提出MAESTRO框架，通过七层威胁建模架构评估和消除自主AI代理的安全漏洞，并验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 结合大型语言模型（LLMs）与自主代理在网络监控和决策系统中会引发严重安全问题，需系统性解决方案。

Method: 使用Python、LangChain和WebSockets构建原型系统，包含推理、内存、参数调优和异常检测模块，并通过MAESTRO框架进行威胁建模。

Result: 确认两类威胁（资源拒绝服务和内存污染），导致性能下降；建议采用多层防御策略。

Conclusion: MAESTRO框架在威胁映射和弹性系统设计中可行，强调内存完整性、逻辑监控和跨层通信保护的重要性。

Abstract: When combining Large Language Models (LLMs) with autonomous agents, used in
network monitoring and decision-making systems, this will create serious
security issues. In this research, the MAESTRO framework consisting of the
seven layers threat modeling architecture in the system was used to expose,
evaluate, and eliminate vulnerabilities of agentic AI. The prototype agent
system was constructed and implemented, using Python, LangChain, and telemetry
in WebSockets, and deployed with inference, memory, parameter tuning, and
anomaly detection modules. Two practical threat cases were confirmed as
follows: (i) resource denial of service by traffic replay denial-of-service,
and (ii) memory poisoning by tampering with the historical log file maintained
by the agent. These situations resulted in measurable levels of performance
degradation, i.e. telemetry updates were delayed, and computational loads were
increased, as a result of poor system adaptations. It was suggested to use a
multilayered defense-in-depth approach with memory isolation, validation of
planners and anomaly response systems in real-time. These findings verify that
MAESTRO is viable in operational threat mapping, prospective risk scoring, and
the basis of the resilient system design. The authors bring attention to the
importance of the enforcement of memory integrity, paying attention to the
adaptation logic monitoring, and cross-layer communication protection that
guarantee the agentic AI reliability in adversarial settings.

</details>


### [17] [Generative AI for Cybersecurity of Energy Management Systems: Methods, Challenges, and Future Directions](https://arxiv.org/abs/2508.10044)
*Aydin Zaboli,Junho Hong*

Main category: cs.CR

TL;DR: 本文提出了一种针对能源管理系统（EMS）的安全框架，结合生成式AI和视觉标记分析，有效应对网络安全漏洞和系统问题。


<details>
  <summary>Details</summary>
Motivation: 现代能源管理系统面临复杂的网络安全威胁和系统问题，需要一种综合性的解决方案。

Method: 提出多点多点攻击/错误模型，结合生成式AI异常检测系统和视觉标记生成智能框架（SoM-GI）。

Result: 在IEEE 14-Bus系统上验证了框架的有效性，成功检测到视觉异常和漏洞。

Conclusion: 该框架通过数值分析和视觉模式识别的结合，为EMS提供了全面的安全保障。

Abstract: This paper elaborates on an extensive security framework specifically
designed for energy management systems (EMSs), which effectively tackles the
dynamic environment of cybersecurity vulnerabilities and/or system problems
(SPs), accomplished through the incorporation of novel methodologies. A
comprehensive multi-point attack/error model is initially proposed to
systematically identify vulnerabilities throughout the entire EMS data
processing pipeline, including post state estimation (SE) stealth attacks, EMS
database manipulation, and human-machine interface (HMI) display corruption
according to the real-time database (RTDB) storage. This framework acknowledges
the interconnected nature of modern attack vectors, which utilize various
phases of supervisory control and data acquisition (SCADA) data flow. Then,
generative AI (GenAI)-based anomaly detection systems (ADSs) for EMSs are
proposed for the first time in the power system domain to handle the scenarios.
Further, a set-of-mark generative intelligence (SoM-GI) framework, which
leverages multimodal analysis by integrating visual markers with rules
considering the GenAI capabilities, is suggested to overcome inherent spatial
reasoning limitations. The SoM-GI methodology employs systematic visual
indicators to enable accurate interpretation of segmented HMI displays and
detect visual anomalies that numerical methods fail to identify. Validation on
the IEEE 14-Bus system shows the framework's effectiveness across scenarios,
while visual analysis identifies inconsistencies. This integrated approach
combines numerical analysis with visual pattern recognition and linguistic
rules to protect against cyber threats and system errors.

</details>


### [18] [NetMoniAI: An Agentic AI Framework for Network Security & Monitoring](https://arxiv.org/abs/2508.10052)
*Pallavi Zambare,Venkata Nikhil Thanikella,Nikhil Padmanabh Kottur,Sree Akhil Akula,Ying Liu*

Main category: cs.CR

TL;DR: NetMoniAI是一个用于自动网络监控和安全的AI框架，结合了分散分析和轻量级集中协调，通过两层设计实现高效检测和响应。


<details>
  <summary>Details</summary>
Motivation: 解决传统网络监控在资源受限和复杂攻击场景下的冗余和响应延迟问题。

Method: 采用两层架构：节点级微代理进行本地分析，中央控制器聚合全局信息。

Result: 在测试和模拟中验证了其可扩展性、低冗余和快速响应能力。

Conclusion: 开源框架便于研究和实践应用，支持多样化网络环境和威胁场景。

Abstract: In this paper, we present NetMoniAI, an agentic AI framework for automatic
network monitoring and security that integrates decentralized analysis with
lightweight centralized coordination. The framework consists of two layers:
autonomous micro-agents at each node perform local traffic analysis and anomaly
detection. A central controller then aggregates insights across nodes to detect
coordinated attacks and maintain system-wide situational awareness. We
evaluated NetMoniAI on a local micro-testbed and through NS-3 simulations.
Results confirm that the two-tier agentic-AI design scales under resource
constraints, reduces redundancy, and improves response time without
compromising accuracy. To facilitate broader adoption and reproducibility, the
complete framework is available as open source. This enables researchers and
practitioners to replicate, validate, and extend it across diverse network
environments and threat scenarios. Github link:
https://github.com/pzambare3/NetMoniAI

</details>


### [19] [Invisible Watermarks, Visible Gains: Steering Machine Unlearning with Bi-Level Watermarking Design](https://arxiv.org/abs/2508.10065)
*Yuhao Sun,Yihua Zhang,Gaowen Liu,Hongtao Xie,Sijia Liu*

Main category: cs.CR

TL;DR: 提出了一种基于数字水印的机器遗忘（MU）新方法Water4MU，通过数据级调整优化遗忘效果，并在图像分类和生成任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 随着‘被遗忘权’需求的增加，机器遗忘成为提升信任和合规性的重要工具，但现有方法主要依赖模型权重调整，数据级调整的潜力未被充分探索。

Method: 利用数字水印技术，提出Water4MU框架，采用双层优化（BLO）设计：上层优化水印网络以降低遗忘难度，下层独立训练模型。

Result: 实验表明，Water4MU在图像分类和生成任务中均有效，尤其在‘挑战性遗忘’场景中优于现有方法。

Conclusion: Water4MU通过数据级水印调整，显著提升了机器遗忘的精确性和实用性。

Abstract: With the increasing demand for the right to be forgotten, machine unlearning
(MU) has emerged as a vital tool for enhancing trust and regulatory compliance
by enabling the removal of sensitive data influences from machine learning (ML)
models. However, most MU algorithms primarily rely on in-training methods to
adjust model weights, with limited exploration of the benefits that data-level
adjustments could bring to the unlearning process. To address this gap, we
propose a novel approach that leverages digital watermarking to facilitate MU
by strategically modifying data content. By integrating watermarking, we
establish a controlled unlearning mechanism that enables precise removal of
specified data while maintaining model utility for unrelated tasks. We first
examine the impact of watermarked data on MU, finding that MU effectively
generalizes to watermarked data. Building on this, we introduce an
unlearning-friendly watermarking framework, termed Water4MU, to enhance
unlearning effectiveness. The core of Water4MU is a bi-level optimization (BLO)
framework: at the upper level, the watermarking network is optimized to
minimize unlearning difficulty, while at the lower level, the model itself is
trained independently of watermarking. Experimental results demonstrate that
Water4MU is effective in MU across both image classification and image
generation tasks. Notably, it outperforms existing methods in challenging MU
scenarios, known as "challenging forgets".

</details>


### [20] [An Architecture for Distributed Digital Identities in the Physical World](https://arxiv.org/abs/2508.10185)
*René Mayrhofer,Michael Roland,Tobias Höller,Philipp Hofer,Mario Lins*

Main category: cs.CR

TL;DR: 论文提出了一种分布式数字身份架构，用于物理世界交易，通过去中心化方式解决集中式身份管理的可用性和隐私问题。


<details>
  <summary>Details</summary>
Motivation: 集中式身份管理存在单点故障和隐私风险，需要一种去中心化解决方案来支持物理世界交易。

Method: 设计了结合传感器、身份机构、属性验证器和个人身份代理（PIA）的分布式架构，并提出了首个协议。

Result: 协议在强全局对手威胁模型下实现了相关安全属性，概念验证证明了架构和协议的可行性。

Conclusion: 分布式数字身份架构在容忍几秒延迟的应用中具有实际可行性。

Abstract: Digital identities are increasingly important for mediating not only digital
but also physical service transactions. Managing such identities through
centralized providers can cause both availability and privacy concerns: single
points of failure and control are ideal targets for global attacks on
technical, organizational, or legal fronts. We design, analyze, and build a
distributed digital identity architecture for physical world transactions in
common scenarios like unlocking doors, public transport, or crossing country
borders. This architecture combines (biometric and other) sensors, (established
and upcoming) identity authorities, attribute verifiers, and a new core
component we call the \emph{Personal Identity Agent (PIA)} that represents
individuals with their identity attributes in the digital domain. All
transactions are conducted in a completely decentralized manner, and the
components for which we currently assume central coordination are optional and
only used for assisting with service discovery and latency reduction. We
present a first protocol between these parties and formally verify that it
achieves relevant security properties based on a realistic threat model
including strong global adversaries. A proof-of-concept implementation
demonstrates practical feasibility of both architecture and initial protocol
for applications that can tolerate end-to-end latencies in the range of a few
seconds.

</details>


### [21] [Detecting Untargeted Attacks and Mitigating Unreliable Updates in Federated Learning for Underground Mining Operations](https://arxiv.org/abs/2508.10212)
*Md Sazedur Rahman,Mohamed Elmahallawy,Sanjay Madria,Samuel Frimpong*

Main category: cs.CR

TL;DR: MineDetect是一个针对地下采矿中联邦学习（FL）的防御框架，通过检测和隔离受攻击模型以及处理低质量数据，提升FL的鲁棒性和准确性。


<details>
  <summary>Details</summary>
Motivation: 地下采矿依赖传感器网络收集数据，但集中式训练深度学习模型存在隐私风险。FL虽能解决隐私问题，但仍面临模型攻击和低质量数据的挑战。

Method: 提出MineDetect框架，包括历史感知机制检测受攻击模型，以及识别并消除低质量数据对FL训练的影响。

Result: 在多样数据集上的实验表明，MineDetect在鲁棒性和准确性上优于现有方法，尤其在非独立同分布数据场景中表现突出。

Conclusion: MineDetect通过抵御攻击和处理低质量数据，显著提升了地下采矿中FL的安全性和操作效率。

Abstract: Underground mining operations rely on distributed sensor networks to collect
critical data daily, including mine temperature, toxic gas concentrations, and
miner movements for hazard detection and operational decision-making. However,
transmitting raw sensor data to a central server for training deep learning
models introduces significant privacy risks, potentially exposing sensitive
mine-specific information. Federated Learning (FL) offers a transformative
solution by enabling collaborative model training while ensuring that raw data
remains localized at each mine. Despite its advantages, FL in underground
mining faces key challenges: (i) An attacker may compromise a mine's local
model by employing techniques such as sign-flipping attacks or additive noise,
leading to erroneous predictions; (ii) Low-quality (yet potentially valuable)
data, caused by poor lighting conditions or sensor inaccuracies in mines may
degrade the FL training process. In response, this paper proposes MineDetect, a
defense FL framework that detects and isolates the attacked models while
mitigating the impact of mines with low-quality data. MineDetect introduces two
key innovations: (i) Detecting attacked models (maliciously manipulated) by
developing a history-aware mechanism that leverages local and global averages
of gradient updates; (ii) Identifying and eliminating adversarial influences
from unreliable models (generated by clients with poor data quality) on the FL
training process. Comprehensive simulations across diverse datasets demonstrate
that MineDetect outperforms existing methods in both robustness and accuracy,
even in challenging non-IID data scenarios. Its ability to counter adversarial
influences while maintaining lower computational efficiency makes it a vital
advancement for improving safety and operational effectiveness in underground
mining.

</details>


### [22] [BERTector: Intrusion Detection Based on Joint-Dataset Learning](https://arxiv.org/abs/2508.10327)
*Haoyang Hu,Xun Huang,Chenyu Wu,Shiwen Liu,Zhichao Lian,Shuangquan Zhang*

Main category: cs.CR

TL;DR: 提出了一种基于BERT的IDS框架BERTector，通过联合数据集训练和高效组件，实现了高检测精度和强泛化能力。


<details>
  <summary>Details</summary>
Motivation: 解决IDS在异构网络流量和多样化攻击模式下的泛化与鲁棒性问题。

Method: 结合NSS-Tokenizer、混合数据集监督微调和LoRA高效训练。

Result: 实验显示BERTector在检测精度、跨数据集泛化和对抗扰动鲁棒性上表现优异。

Conclusion: 为复杂动态网络环境中的IDS提供了统一高效的解决方案。

Abstract: Intrusion detection systems (IDS) are facing challenges in generalization and
robustness due to the heterogeneity of network traffic and the diversity of
attack patterns. To address this issue, we propose a new joint-dataset training
paradigm for IDS and propose a scalable BERTector framework based on BERT.
BERTector integrates three key components: NSS-Tokenizer for traffic-aware
semantic tokenization, supervised fine-tuning with a hybrid dataset, and
low-rank adaptation (LoRA) for efficient training. Extensive experiments show
that BERTector achieves state-of-the-art detection accuracy, strong
cross-dataset generalization capabilities, and excellent robustness to
adversarial perturbations. This work establishes a unified and efficient
solution for modern IDS in complex and dynamic network environments.

</details>


### [23] [Yet Another Mirage of Breaking MIRAGE: Debunking Occupancy-based Side-Channel Attacks on Fully Associative Randomized Caches](https://arxiv.org/abs/2508.10431)
*Chris Cao,Gururaj Saileshwar*

Main category: cs.CR

TL;DR: 论文指出USENIX Security 2025中关于MIRAGE随机缓存攻击的研究存在建模缺陷，攻击成功是由于模拟中使用了固定随机种子，实际随机化后攻击失效。


<details>
  <summary>Details</summary>
Motivation: 验证USENIX Security 2025中关于MIRAGE随机缓存攻击的结论是否成立。

Method: 重新模拟MIRAGE缓存攻击，随机化每次运行的种子，对比固定种子与实际随机化情况下的攻击效果。

Result: 固定种子导致攻击成功，随机化后攻击失效，证明原研究结论是建模错误。

Conclusion: MIRAGE随机缓存在实际运行中不存在AES密钥泄露漏洞，原研究结论是模拟缺陷导致的假象。

Abstract: Recent work presented at USENIX Security 2025 claims that occupancy-based
attacks can recover AES keys from the MIRAGE randomized cache. In this paper,
we examine these claims and find that they arise from fundamental modeling
flaws. Most critically, the authors' simulation of MIRAGE uses a constant seed
to initialize the random number generator used for global evictions in MIRAGE,
causing every AES encryption they trace to evict the same deterministic
sequence of cache lines. This artificially creates a highly repeatable timing
pattern that is not representative of a realistic implementation of MIRAGE,
where eviction sequences vary randomly between encryptions. When we instead
randomize the eviction seed for each run, reflecting realistic operation, the
correlation between AES T-table accesses and attacker runtimes disappears, and
the attack fails. These findings show that the reported leakage is an artifact
of incorrect modeling, and not an actual vulnerability in MIRAGE.

</details>


### [24] [AlDBaran: Towards Blazingly Fast State Commitments for Blockchains](https://arxiv.org/abs/2508.10493)
*Bernhard Kauer,Aleksandr Petrosyan,Benjamin Livshits*

Main category: cs.CR

TL;DR: AlDBaran是一种高性能认证数据库，通过优化磁盘I/O、预取策略和Merkle树更新机制，支持50 Gbps网络吞吐量，显著提升区块链状态更新的效率。


<details>
  <summary>Details</summary>
Motivation: 现有认证数据库无法满足高吞吐量区块链系统的需求，AlDBaran旨在解决这一问题。

Method: 通过消除磁盘I/O操作、实施预取策略和改进Merkle树更新机制，优化认证数据结构。

Result: AlDBaran支持50 Gbps吞吐量，每秒4800万次更新，性能优于同类项目。

Conclusion: AlDBaran为高吞吐量区块链提供了高效、可扩展的解决方案，支持历史状态证明和轻客户端功能。

Abstract: The fundamental basis for maintaining integrity within contemporary
blockchain systems is provided by authenticated databases. Our analysis
indicates that a significant portion of the approaches applied in this domain
fail to sufficiently meet the stringent requirements of systems processing
transactions at rates of multi-million TPS. AlDBaran signifies a substantial
advancement in authenticated databases. By eliminating disk I/O operations from
the critical path, implementing prefetching strategies, and refining the update
mechanism of the Merkle tree, we have engineered an authenticated data
structure capable of handling state updates efficiently at a network throughput
of 50 Gbps. This throughput capacity significantly surpasses any empirically
documented blockchain throughput, guaranteeing the ability of even the most
high-throughput blockchains to generate state commitments effectively.
  AlDBaran provides support for historical state proofs, which facilitates a
wide array of novel applications. For instance, the deployment of AlDBaran
could enable blockchains that do not currently support state commitments to
offer functionalities for light clients and/or implement rollups.
  When benchmarked against alternative authenticated data structure projects,
AlDBaran exhibits superior performance and simplicity. In particular, AlDBaran
achieves speeds of approximately 48 million updates per second using an
identical machine configuration. This characteristic renders AlDBaran an
attractive solution for resource-limited environments, as its historical data
capabilities can be modularly isolated (and deactivated), which further
enhances performance. On consumer-level portable hardware, it achieves
approximately 8 million updates/s in an in-memory setting and 5 million
updates/s with snapshots at sub-second intervals, illustrating compelling and
cost-effective scalability.

</details>


### [25] [Codes on any Cayley Graph have an Interactive Oracle Proof of Proximity](https://arxiv.org/abs/2508.10510)
*Hugo Delavenne,Louise Lallemand*

Main category: cs.CR

TL;DR: 本文提出了一种广义的交互式预言证明邻近性（IOPP）协议，适用于基于Cayley图的编码，保留了低soundness参数并扩展了应用范围。


<details>
  <summary>Details</summary>
Motivation: 扩展[DMR25]中提出的flowering IOPP协议，使其适用于更广泛的编码类型，同时保持低soundness参数和竞争性的复杂度。

Method: 通过利用Cayley图的良好扩展性质，将协议推广到符号索引在Cayley图边上的编码。

Result: 提出的广义协议在保持soundness参数的同时，复杂度略有下降，适用于具有恒定速率和最小距离的编码。

Conclusion: 该协议为基于Cayley图的编码提供了高效的IOPP解决方案，具有潜在的实际加速优势。

Abstract: Interactive Oracle Proofs of Proximity (IOPP) are at the heart of code-based
SNARKs, a family of zeroknowledge protocols. The first and most famous one is
the FRI protocol [BBHR18a], that efficiently tests proximity to Reed-Solomon
codes. This paper generalizes the flowering IOPP introduced in [DMR25] for some
specific (2, n)-regular Tanner codes to a much broader variety of codes: any
code with symbols indexed on the edges of a Cayley graph. The flowering
protocol of [DMR25] had a soundness parameter much lower than the FRI protocol
[BCI + 23], and complexity parameters that could compete with the FRI
[BBHR18a]. The lower soundness and the absence of restriction on the base field
may lead to other practical speedups, however the codes considered in [DMR25]
have an o(1) minimum distance. The generalization proposed in this paper
preserves the soundness parameter with a slight decrease of the complexity
parameters, while allowing being applied on codes with constant rate and
constant minimum distance thanks to the good expansion properties of some
families of Cayley graphs.

</details>


### [26] [A Transformer-Based Approach for DDoS Attack Detection in IoT Networks](https://arxiv.org/abs/2508.10636)
*Sandipan Dey,Payal Santosh Kate,Vatsala Upadhyay,Abhishek Vaish*

Main category: cs.CR

TL;DR: 论文提出了一种基于Transformer模型的新方法，用于检测物联网设备上的DDoS攻击，效果优于传统机器学习技术。


<details>
  <summary>Details</summary>
Motivation: 物联网设备因资源受限易受DDoS攻击，传统检测方法难以应对动态网络环境和多样化协议。

Method: 使用Transformer模型提取网络流量特征，并通过自注意力机制处理数据。

Result: 实验表明，该方法在准确率、精确率、召回率和F1分数上优于传统方法。

Conclusion: Transformer模型是检测物联网DDoS攻击的有效解决方案，具备实际部署潜力。

Abstract: DDoS attacks have become a major threat to the security of IoT devices and
can cause severe damage to the network infrastructure. IoT devices suffer from
the inherent problem of resource constraints and are therefore susceptible to
such resource-exhausting attacks. Traditional methods for detecting DDoS
attacks are not efficient enough to cope with the dynamic nature of IoT
networks, as well as the scalability of the attacks, diversity of protocols,
high volume of traffic, and variability in device behavior, and variability of
protocols like MQTT, CoAP, making it hard to implement security across all the
protocols. In this paper, we propose a novel approach, i.e., the use of
Transformer models, which have shown remarkable performance in natural language
processing tasks, for detecting DDoS attacks on IoT devices. The proposed model
extracts features from network traffic data and processes them using a
self-attention mechanism. Experiments conducted on a real-world dataset
demonstrate that the proposed approach outperforms traditional machine learning
techniques, which can be validated by comparing both approaches' accuracy,
precision, recall, and F1-score. The results of this study show that the
Transformer models can be an effective solution for detecting DDoS attacks on
IoT devices and have the potential to be deployed in real-world IoT
environments.

</details>


### [27] [MirGuard: Towards a Robust Provenance-based Intrusion Detection System Against Graph Manipulation Attacks](https://arxiv.org/abs/2508.10639)
*Anyuan Sang,Lu Zhou,Li Yang,Junbo Jia,Huipeng Yang,Pengbin Feng,Jianfeng Ma*

Main category: cs.CR

TL;DR: 论文提出MirGuard框架，通过逻辑感知的多视图增强和对比表示学习，提升基于学习的溯源入侵检测系统（PIDSes）对图操纵攻击的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有的基于学习的PIDSes易受图操纵攻击影响，缺乏鲁棒的检测方案，限制了其实际应用。

Method: 结合逻辑感知噪声注入（LNI）生成语义有效的图视图，并通过逻辑保留对比学习框架学习对良性变换不变但对对抗性不一致敏感的表示。

Result: 在多溯源数据集上，MirGuard显著优于现有检测器，对抗图操纵攻击的鲁棒性更强，且不影响检测性能和效率。

Conclusion: MirGuard是首个针对PIDS对抗性威胁的增强方案，为现代网络安全挑战提供了鲁棒有效的解决方案。

Abstract: Learning-based Provenance-based Intrusion Detection Systems (PIDSes) have
become essential tools for anomaly detection in host systems due to their
ability to capture rich contextual and structural information, as well as their
potential to detect unknown attacks. However, recent studies have shown that
these systems are vulnerable to graph manipulation attacks, where attackers
manipulate the graph structure to evade detection. While some previous
approaches have discussed this type of attack, none have fully addressed it
with a robust detection solution, limiting the practical applicability of
PIDSes.
  To address this challenge, we propose MirGuard, a robust anomaly detection
framework that combines logic-aware multi-view augmentation with contrastive
representation learning. Rather than applying arbitrary structural
perturbations, MirGuard introduces Logic-Aware Noise Injection (LNI) to
generate semantically valid graph views, ensuring that all augmentations
preserve the underlying causal semantics of the provenance data. These views
are then used in a Logic-Preserving Contrastive Learning framework, which
encourages the model to learn representations that are invariant to benign
transformations but sensitive to adversarial inconsistencies. Comprehensive
evaluations on multiple provenance datasets demonstrate that MirGuard
significantly outperforms state-of-the-art detectors in robustness against
various graph manipulation attacks without sacrificing detection performance
and efficiency. Our work represents the first targeted study to enhance PIDS
against such adversarial threats, providing a robust and effective solution to
modern cybersecurity challenges.

</details>


### [28] [A Novel Study on Intelligent Methods and Explainable AI for Dynamic Malware Analysis](https://arxiv.org/abs/2508.10652)
*Richa Dasila,Vatsala Upadhyay,Samo Bobek,Abhishek Vaish*

Main category: cs.CR

TL;DR: 研究通过可解释AI（XAI）技术提升深度学习模型在恶意软件检测中的透明度和可信度，探索了MLP、CNN、RNN和CNN-LSTM等模型的效果。


<details>
  <summary>Details</summary>
Motivation: 深度学习模型在恶意软件检测中虽有效，但其“黑盒”特性导致决策过程难以理解，影响了信任度。

Method: 整合XAI技术，评估MLP、CNN、RNN和CNN-LSTM模型在动态恶意软件分析和分类中的表现。

Result: 研究展示了这些模型在恶意软件检测中的有效性，并通过XAI提升了其透明度和可信度。

Conclusion: 该研究为深度学习模型在网络安全中的应用提供了更透明和可信的解决方案。

Abstract: Deep learning models are one of the security strategies, trained on extensive
datasets, and play a critical role in detecting and responding to these threats
by recognizing complex patterns in malicious code. However, the opaque nature
of these models-often described as "black boxes"-makes their decision-making
processes difficult to understand, even for their creators. This research
addresses these challenges by integrating Explainable AI (XAI) techniques to
enhance the interpretability and trustworthiness of malware detection models.
In this research, the use of Multi-Layer Perceptrons (MLP) for dynamic malware
analysis has been considered, a less explored area, and its efficacy in
detecting Metamorphic Malware, and further the effectiveness and transparency
of MLPs, CNNs, RNNs, and CNN-LSTM models in malware classification, evaluating
these models through the lens of Explainable AI (XAI). This comprehensive
approach aims to demystify the internal workings of deep learning models,
promoting a better understanding and trust in their predictive capabilities in
cybersecurity contexts. Such in-depth analysis and implementation haven't been
done to the best of our knowledge.

</details>


### [29] [Advancing Autonomous Incident Response: Leveraging LLMs and Cyber Threat Intelligence](https://arxiv.org/abs/2508.10677)
*Amine Tellache,Abdelaziz Amara Korba,Amdjed Mokhtari,Horea Moldovan,Yacine Ghamri-Doudane*

Main category: cs.CR

TL;DR: 提出了一种基于检索增强生成（RAG）的框架，利用大语言模型（LLM）自动化和增强事件响应（IR），通过动态检索网络威胁情报（CTI）来提升安全操作的效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 安全团队面临告警疲劳、高误报率和大量非结构化CTI文档的挑战，手动分析耗时且资源密集，亟需自动化解决方案。

Method: 采用混合检索机制，结合NLP相似性搜索和标准化查询外部CTI平台，并通过LLM生成上下文相关的响应策略。

Result: 实证验证表明，该方法提高了IR的准确性、上下文相关性和效率，减轻了分析师负担并缩短了响应时间。

Conclusion: LLM驱动的CTI融合有潜力推动自主安全操作，并为智能自适应网络安全框架奠定基础。

Abstract: Effective incident response (IR) is critical for mitigating cyber threats,
yet security teams are overwhelmed by alert fatigue, high false-positive rates,
and the vast volume of unstructured Cyber Threat Intelligence (CTI) documents.
While CTI holds immense potential for enriching security operations, its
extensive and fragmented nature makes manual analysis time-consuming and
resource-intensive. To bridge this gap, we introduce a novel
Retrieval-Augmented Generation (RAG)-based framework that leverages Large
Language Models (LLMs) to automate and enhance IR by integrating dynamically
retrieved CTI. Our approach introduces a hybrid retrieval mechanism that
combines NLP-based similarity searches within a CTI vector database with
standardized queries to external CTI platforms, facilitating context-aware
enrichment of security alerts. The augmented intelligence is then leveraged by
an LLM-powered response generation module, which formulates precise,
actionable, and contextually relevant incident mitigation strategies. We
propose a dual evaluation paradigm, wherein automated assessment using an
auxiliary LLM is systematically cross-validated by cybersecurity experts.
Empirical validation on real-world and simulated alerts demonstrates that our
approach enhances the accuracy, contextualization, and efficiency of IR,
alleviating analyst workload and reducing response latency. This work
underscores the potential of LLM-driven CTI fusion in advancing autonomous
security operations and establishing a foundation for intelligent, adaptive
cybersecurity frameworks.

</details>


### [30] [Searching for Privacy Risks in LLM Agents via Simulation](https://arxiv.org/abs/2508.10880)
*Yanzhe Zhang,Diyi Yang*

Main category: cs.CR

TL;DR: 论文提出了一种基于搜索的框架，通过模拟隐私关键代理交互来发现和改进攻击与防御策略，揭示了从简单请求到复杂多轮攻击的演变，以及防御机制的提升。


<details>
  <summary>Details</summary>
Motivation: 大规模部署基于LLM的代理可能带来隐私威胁，恶意代理通过多轮交互提取敏感信息，动态对话的适应性攻击策略难以手动预测和发现。

Method: 采用搜索框架，模拟三种角色（数据主体、数据发送者、数据接收者）的交互，利用LLM作为优化器进行并行搜索和多线程传播。

Result: 攻击策略从直接请求升级为复杂多轮战术（如冒充和伪造同意），防御从基于规则的约束发展为身份验证状态机。

Conclusion: 发现的攻击和防御策略具有跨场景和模型的实用性，为构建隐私感知代理提供了实用工具。

Abstract: The widespread deployment of LLM-based agents is likely to introduce a
critical privacy threat: malicious agents that proactively engage others in
multi-turn interactions to extract sensitive information. These dynamic
dialogues enable adaptive attack strategies that can cause severe privacy
violations, yet their evolving nature makes it difficult to anticipate and
discover sophisticated vulnerabilities manually. To tackle this problem, we
present a search-based framework that alternates between improving attacker and
defender instructions by simulating privacy-critical agent interactions. Each
simulation involves three roles: data subject, data sender, and data recipient.
While the data subject's behavior is fixed, the attacker (data recipient)
attempts to extract sensitive information from the defender (data sender)
through persistent and interactive exchanges. To explore this interaction space
efficiently, our search algorithm employs LLMs as optimizers, using parallel
search with multiple threads and cross-thread propagation to analyze simulation
trajectories and iteratively propose new instructions. Through this process, we
find that attack strategies escalate from simple direct requests to
sophisticated multi-turn tactics such as impersonation and consent forgery,
while defenses advance from rule-based constraints to identity-verification
state machines. The discovered attacks and defenses transfer across diverse
scenarios and backbone models, demonstrating strong practical utility for
building privacy-aware agents.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [31] [A Survey of Optimization Modeling Meets LLMs: Progress and Future Directions](https://arxiv.org/abs/2508.10047)
*Ziyang Xiao,Jingrong Xie,Lilin Xu,Shisi Guan,Jingyan Zhu,Xiongwei Han,Xiaojin Fu,WingYin Yu,Han Wu,Wei Shi,Qingcan Kang,Jiahui Duan,Tao Zhong,Mingxuan Yuan,Jia Zeng,Yuan Wang,Gang Chen,Dongxiang Zhang*

Main category: cs.AI

TL;DR: 本文综述了利用大语言模型（LLMs）自动化数学建模的最新进展，包括数据合成、模型微调、推理框架、基准数据集和性能评估，并构建了新的公平评估排行榜和在线资源门户。


<details>
  <summary>Details</summary>
Motivation: 优化建模在解决实际问题中具有广泛应用，但需要大量专业知识。LLMs的出现为自动化数学建模提供了新机会。

Method: 综述了技术栈的各个方面，包括数据合成、模型微调、推理框架等，并分析了基准数据集的质量，清理数据并构建新的排行榜。

Result: 发现基准数据集错误率高，清理后构建了新的公平评估排行榜和在线资源门户。

Conclusion: 指出了当前方法的局限性，并提出了未来研究方向。

Abstract: By virtue of its great utility in solving real-world problems, optimization
modeling has been widely employed for optimal decision-making across various
sectors, but it requires substantial expertise from operations research
professionals. With the advent of large language models (LLMs), new
opportunities have emerged to automate the procedure of mathematical modeling.
This survey presents a comprehensive and timely review of recent advancements
that cover the entire technical stack, including data synthesis and fine-tuning
for the base model, inference frameworks, benchmark datasets, and performance
evaluation. In addition, we conducted an in-depth analysis on the quality of
benchmark datasets, which was found to have a surprisingly high error rate. We
cleaned the datasets and constructed a new leaderboard with fair performance
evaluation in terms of base LLM model and datasets. We also build an online
portal that integrates resources of cleaned datasets, code and paper repository
to benefit the community. Finally, we identify limitations in current
methodologies and outline future research opportunities.

</details>


### [32] [Amazon Nova AI Challenge -- Trusted AI: Advancing secure, AI-assisted software development](https://arxiv.org/abs/2508.10108)
*Sattvik Sahai,Prasoon Goyal,Michael Johnston,Anna Gottardi,Yao Lu,Lucy Hu,Luke Dai,Shaohua Liu,Samyuth Sagi,Hangjie Shi,Desheng Zhang,Lavina Vaz,Leslie Ball,Maureen Murray,Rahul Gupta,Shankar Ananthakrishna*

Main category: cs.AI

TL;DR: 亚马逊Nova AI挑战赛通过对抗性竞赛推动AI安全性研究，大学团队开发了先进技术，如推理对齐和多轮越狱测试。


<details>
  <summary>Details</summary>
Motivation: 解决AI在软件开发中的安全性问题，推动安全AI的发展。

Method: 通过对抗性竞赛，团队开发自动化红队机器人和安全AI助手，并利用高质量数据进行迭代改进。

Result: 团队开发了先进技术，如推理对齐、模型护栏和多轮越狱测试，提升了AI安全性。

Conclusion: 该挑战赛通过协作提升了AI在软件开发中的安全性标准。

Abstract: AI systems for software development are rapidly gaining prominence, yet
significant challenges remain in ensuring their safety. To address this, Amazon
launched the Trusted AI track of the Amazon Nova AI Challenge, a global
competition among 10 university teams to drive advances in secure AI. In the
challenge, five teams focus on developing automated red teaming bots, while the
other five create safe AI assistants. This challenge provides teams with a
unique platform to evaluate automated red-teaming and safety alignment methods
through head-to-head adversarial tournaments where red teams have multi-turn
conversations with the competing AI coding assistants to test their safety
alignment. Along with this, the challenge provides teams with a feed of high
quality annotated data to fuel iterative improvement. Throughout the challenge,
teams developed state-of-the-art techniques, introducing novel approaches in
reasoning-based safety alignment, robust model guardrails, multi-turn
jail-breaking, and efficient probing of large language models (LLMs). To
support these efforts, the Amazon Nova AI Challenge team made substantial
scientific and engineering investments, including building a custom baseline
coding specialist model for the challenge from scratch, developing a tournament
orchestration service, and creating an evaluation harness. This paper outlines
the advancements made by university teams and the Amazon Nova AI Challenge team
in addressing the safety challenges of AI for software development,
highlighting this collaborative effort to raise the bar for AI safety.

</details>


### [33] [MCP-Orchestrated Multi-Agent System for Automated Disinformation Detection](https://arxiv.org/abs/2508.10143)
*Alexandru-Andrei Avram,Adrian Groza,Alexandru Lecu*

Main category: cs.AI

TL;DR: 提出了一种多智能体系统，通过关系提取检测新闻标题和短文本中的虚假信息，准确率达95.3%。


<details>
  <summary>Details</summary>
Motivation: 数字平台上虚假信息的广泛传播对信息完整性构成挑战，需要高效检测方法。

Method: 结合四种智能体（机器学习、维基百科知识检查、连贯性检测、网络数据抓取分析），通过Model Context Protocol协调。

Result: 多智能体系统准确率95.3%，F1分数0.964，优于传统方法。

Conclusion: 系统模块化架构易于扩展，决策过程透明，加权聚合方法优于阈值优化。

Abstract: The large spread of disinformation across digital platforms creates
significant challenges to information integrity. This paper presents a
multi-agent system that uses relation extraction to detect disinformation in
news articles, focusing on titles and short text snippets. The proposed Agentic
AI system combines four agents: (i) a machine learning agent (logistic
regression), (ii) a Wikipedia knowledge check agent (which relies on named
entity recognition), (iii) a coherence detection agent (using LLM prompt
engineering), and (iv) a web-scraped data analyzer that extracts relational
triplets for fact checking. The system is orchestrated via the Model Context
Protocol (MCP), offering shared context and live learning across components.
Results demonstrate that the multi-agent ensemble achieves 95.3% accuracy with
an F1 score of 0.964, significantly outperforming individual agents and
traditional approaches. The weighted aggregation method, mathematically derived
from individual agent misclassification rates, proves superior to algorithmic
threshold optimization. The modular architecture makes the system easily
scalable, while also maintaining details of the decision processes.

</details>


### [34] [Agentic AI Frameworks: Architectures, Protocols, and Design Challenges](https://arxiv.org/abs/2508.10146)
*Hana Derouiche,Zaki Brahmi,Haithem Mazeni*

Main category: cs.AI

TL;DR: 本文系统回顾和比较了主流Agentic AI框架，分析了其架构、通信机制和安全性，并提出了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 探索大型语言模型（LLMs）驱动的Agentic AI的潜力，解决智能代理的自主性、协调性和通信问题。

Method: 通过系统回顾和比较分析多个Agentic AI框架，并深入分析通信协议如CNP、A2A等。

Result: 建立了Agentic AI系统的基础分类法，并识别了关键局限性和未来研究方向。

Conclusion: 本文为下一代自主AI系统的研究提供了全面的参考，强调了可扩展性、鲁棒性和互操作性的重要性。

Abstract: The emergence of Large Language Models (LLMs) has ushered in a transformative
paradigm in artificial intelligence, Agentic AI, where intelligent agents
exhibit goal-directed autonomy, contextual reasoning, and dynamic multi-agent
coordination. This paper provides a systematic review and comparative analysis
of leading Agentic AI frameworks, including CrewAI, LangGraph, AutoGen,
Semantic Kernel, Agno, Google ADK, and MetaGPT, evaluating their architectural
principles, communication mechanisms, memory management, safety guardrails, and
alignment with service-oriented computing paradigms. Furthermore, we identify
key limitations, emerging trends, and open challenges in the field. To address
the issue of agent communication, we conduct an in-depth analysis of protocols
such as the Contract Net Protocol (CNP), Agent-to-Agent (A2A), Agent Network
Protocol (ANP), and Agora. Our findings not only establish a foundational
taxonomy for Agentic AI systems but also propose future research directions to
enhance scalability, robustness, and interoperability. This work serves as a
comprehensive reference for researchers and practitioners working to advance
the next generation of autonomous AI systems.

</details>


### [35] [Improving and Evaluating Open Deep Research Agents](https://arxiv.org/abs/2508.10152)
*Doaa Allabadi,Kyle Bradbury,Jordan M. Malof*

Main category: cs.AI

TL;DR: 论文研究了开源深度研究代理（ODR）与专有系统的性能对比，提出改进后的ODR+模型在BC-Small基准测试中达到10%的成功率。


<details>
  <summary>Details</summary>
Motivation: 当前深度研究代理（DRAs）多为闭源系统，缺乏开源选择，限制了学术研究。

Method: 通过改进开源系统ODR，提出ODR+模型，并在BC-Small基准测试中与专有系统对比。

Result: ODR+在BC-Small测试集上达到10%的成功率，优于其他系统。

Conclusion: 开源系统ODR+通过改进可以接近专有系统性能，为学术研究提供可行选择。

Abstract: We focus here on Deep Research Agents (DRAs), which are systems that can take
a natural language prompt from a user, and then autonomously search for, and
utilize, internet-based content to address the prompt. Recent DRAs have
demonstrated impressive capabilities on public benchmarks however, recent
research largely involves proprietary closed-source systems. At the time of
this work, we only found one open-source DRA, termed Open Deep Research (ODR).
In this work we adapt the challenging recent BrowseComp benchmark to compare
ODR to existing proprietary systems. We propose BrowseComp-Small (BC-Small),
comprising a subset of BrowseComp, as a more computationally-tractable DRA
benchmark for academic labs. We benchmark ODR and two other proprietary systems
on BC-Small: one system from Anthropic and one system from Google. We find that
all three systems achieve 0% accuracy on the test set of 60 questions. We
introduce three strategic improvements to ODR, resulting in the ODR+ model,
which achieves a state-of-the-art 10% success rate on BC-Small among both
closed-source and open-source systems. We report ablation studies indicating
that all three of our improvements contributed to the success of ODR+.

</details>


### [36] [Pruning Long Chain-of-Thought of Large Reasoning Models via Small-Scale Preference Optimization](https://arxiv.org/abs/2508.10164)
*Bin Hong,Jiayu Liu,Zhenya Huang,Kai Zhang,Mengdi Zhang*

Main category: cs.AI

TL;DR: 论文提出了一种名为LCPO的方法，通过控制生成长度，显著减少大型推理模型的输出长度，同时保持推理性能。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型的长推理链增加了计算成本并可能导致过度思考，当前方法在效率和质量之间难以平衡。

Method: 分析生成路径分布并通过难度估计过滤轨迹，提出LCPO方法直接平衡隐式奖励与NLL损失。

Result: 实验表明，LCPO在多个基准上将平均输出长度减少50%以上，同时保持推理性能。

Conclusion: LCPO展示了在有限数据和训练下高效引导大型推理模型的潜力。

Abstract: Recent advances in Large Reasoning Models (LRMs) have demonstrated strong
performance on complex tasks through long Chain-of-Thought (CoT) reasoning.
However, their lengthy outputs increase computational costs and may lead to
overthinking, raising challenges in balancing reasoning effectiveness and
efficiency. Current methods for efficient reasoning often compromise reasoning
quality or require extensive resources. This paper investigates efficient
methods to reduce the generation length of LRMs. We analyze generation path
distributions and filter generated trajectories through difficulty estimation.
Subsequently, we analyze the convergence behaviors of the objectives of various
preference optimization methods under a Bradley-Terry loss based framework.
Based on the analysis, we propose Length Controlled Preference Optimization
(LCPO) that directly balances the implicit reward related to NLL loss. LCPO can
effectively learn length preference with limited data and training. Extensive
experiments demonstrate that our approach significantly reduces the average
output length by over 50\% across multiple benchmarks while maintaining the
reasoning performance. Our work highlights the potential for computationally
efficient approaches in guiding LRMs toward efficient reasoning.

</details>


### [37] [KompeteAI: Accelerated Autonomous Multi-Agent System for End-to-End Pipeline Generation for Machine Learning Problems](https://arxiv.org/abs/2508.10177)
*Stepan Kulibaba,Artem Dzhalilov,Roman Pakhomov,Oleg Svidchenko,Alexander Gasnikov,Aleksei Shpilman*

Main category: cs.AI

TL;DR: KompeteAI是一个新型AutoML框架，通过动态解决方案空间探索和合并阶段提升性能，解决了现有LLM-based AutoML系统的探索局限和执行瓶颈问题。


<details>
  <summary>Details</summary>
Motivation: 现有LLM-based AutoML系统存在探索策略受限和执行瓶颈问题，限制了其性能和效率。

Method: KompeteAI引入动态解决方案空间探索，结合合并阶段和RAG技术，并采用预测评分模型和加速调试方法。

Result: KompeteAI在MLE-Bench基准上平均优于其他方法3%，并将管道评估速度提升6.9倍。

Conclusion: KompeteAI通过创新方法显著提升了AutoML系统的性能和效率，并在新提出的Kompete-bench上取得最佳结果。

Abstract: Recent Large Language Model (LLM)-based AutoML systems demonstrate impressive
capabilities but face significant limitations such as constrained exploration
strategies and a severe execution bottleneck. Exploration is hindered by
one-shot methods lacking diversity and Monte Carlo Tree Search (MCTS)
approaches that fail to recombine strong partial solutions. The execution
bottleneck arises from lengthy code validation cycles that stifle iterative
refinement. To overcome these challenges, we introduce KompeteAI, a novel
AutoML framework with dynamic solution space exploration. Unlike previous MCTS
methods that treat ideas in isolation, KompeteAI introduces a merging stage
that composes top candidates. We further expand the hypothesis space by
integrating Retrieval-Augmented Generation (RAG), sourcing ideas from Kaggle
notebooks and arXiv papers to incorporate real-world strategies. KompeteAI also
addresses the execution bottleneck via a predictive scoring model and an
accelerated debugging method, assessing solution potential using early stage
metrics to avoid costly full-code execution. This approach accelerates pipeline
evaluation 6.9 times. KompeteAI outperforms leading methods (e.g., RD-agent,
AIDE, and Ml-Master) by an average of 3\% on the primary AutoML benchmark,
MLE-Bench. Additionally, we propose Kompete-bench to address limitations in
MLE-Bench, where KompeteAI also achieves state-of-the-art results

</details>


### [38] [Extending the Entropic Potential of Events for Uncertainty Quantification and Decision-Making in Artificial Intelligence](https://arxiv.org/abs/2508.10241)
*Mark Zilberman*

Main category: cs.AI

TL;DR: 该论文提出了一种基于事件熵势的概念，用于增强AI中的不确定性量化、决策和可解释性，并将其应用于政策评估、奖励设计等领域。


<details>
  <summary>Details</summary>
Motivation: 通过引入事件熵势的概念，旨在统一和加强智能系统中的不确定性建模，结合物理学和信息理论的原理。

Method: 将物理学中的熵势概念调整为AI适用的事件中心度量，强调条件期望以考虑反事实场景，并应用于多种AI任务。

Result: 展示了熵势框架在强化学习、贝叶斯推理和异常检测中的实用性，为AI不确定性管理提供了理论基础。

Conclusion: 熵势框架为AI中的不确定性管理提供了一种理论扎实、可解释且多功能的方法，连接了热力学、信息理论和机器学习。

Abstract: This work demonstrates how the concept of the entropic potential of events --
a parameter quantifying the influence of discrete events on the expected future
entropy of a system -- can enhance uncertainty quantification, decision-making,
and interpretability in artificial intelligence (AI). Building on its original
formulation in physics, the framework is adapted for AI by introducing an
event-centric measure that captures how actions, observations, or other
discrete occurrences impact uncertainty at future time horizons. Both the
original and AI-adjusted definitions of entropic potential are formalized, with
the latter emphasizing conditional expectations to account for counterfactual
scenarios. Applications are explored in policy evaluation, intrinsic reward
design, explainable AI, and anomaly detection, highlighting the metric's
potential to unify and strengthen uncertainty modeling in intelligent systems.
Conceptual examples illustrate its use in reinforcement learning, Bayesian
inference, and anomaly detection, while practical considerations for
computation in complex AI models are discussed. The entropic potential
framework offers a theoretically grounded, interpretable, and versatile
approach to managing uncertainty in AI, bridging principles from
thermodynamics, information theory, and machine learning.

</details>


### [39] [Why Cannot Large Language Models Ever Make True Correct Reasoning?](https://arxiv.org/abs/2508.10265)
*Jingde Cheng*

Main category: cs.AI

TL;DR: 本文认为基于大语言模型（LLMs）的AIGC工具（如ChatGPT）的“理解能力”和“推理能力”只是概念模糊者的错觉，实际上LLMs因工作原理的本质限制无法拥有真正的推理能力。


<details>
  <summary>Details</summary>
Motivation: 探讨LLMs是否真正具备理解和推理能力，揭示其工作原理的本质限制。

Method: 通过分析LLMs的工作原理，论证其无法实现真正的正确推理。

Result: LLMs因本质限制无法拥有真正的理解和推理能力。

Conclusion: LLMs的“理解”和“推理”能力是错觉，其工作原理决定了它们无法实现真正的正确推理。

Abstract: Recently, with the application progress of AIGC tools based on large language
models (LLMs), led by ChatGPT, many AI experts and more non-professionals are
trumpeting the "understanding ability" and "reasoning ability" of the LLMs. The
present author considers that the so-called "understanding ability" and
"reasoning ability" of LLMs are just illusions of those people who with vague
concepts. In fact, the LLMs can never have the true understanding ability and
true reasoning ability. This paper intents to explain that, because the
essential limitations of their working principle, the LLMs can never have the
ability of true correct reasoning.

</details>


### [40] [Promoting Efficient Reasoning with Verifiable Stepwise Reward](https://arxiv.org/abs/2508.10293)
*Chuhuai Yue,Chengqi Dong,Yinan Gao,Hang He,Jiajun Chai,Guojun Yin,Wei Lin*

Main category: cs.AI

TL;DR: 论文提出了一种基于规则的逐步验证奖励机制（VSRM），通过奖励有效推理步骤并惩罚无效步骤，解决了大型推理模型（LRMs）在简单问题上过度计算的问题，显著提高了效率。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型（LRMs）在复杂推理任务中取得了进展，但存在过度计算（overthinking）问题，降低了效率。现有方法需要预设预算或选择推理模式，缺乏灵活性和可靠性。

Method: 提出VSRM机制，根据推理轨迹中中间状态的表现分配奖励，结合PPO和Reinforce++进行实验验证。

Result: 在AIME24和AIME25基准测试中，VSRM显著减少了输出长度，同时保持了推理性能，有效抑制了无效步骤。

Conclusion: VSRM通过奖励有效步骤和惩罚无效步骤，从根本上缓解了过度计算问题，平衡了效率与准确性。

Abstract: Large reasoning models (LRMs) have recently achieved significant progress in
complex reasoning tasks, aided by reinforcement learning with verifiable
rewards. However, LRMs often suffer from overthinking, expending excessive
computation on simple problems and reducing efficiency. Existing efficient
reasoning methods typically require accurate task assessment to preset token
budgets or select reasoning modes, which limits their flexibility and
reliability. In this work, we revisit the essence of overthinking and identify
that encouraging effective steps while penalizing ineffective ones is key to
its solution. To this end, we propose a novel rule-based verifiable stepwise
reward mechanism (VSRM), which assigns rewards based on the performance of
intermediate states in the reasoning trajectory. This approach is intuitive and
naturally fits the step-by-step nature of reasoning tasks. We conduct extensive
experiments on standard mathematical reasoning benchmarks, including AIME24 and
AIME25, by integrating VSRM with PPO and Reinforce++. Results show that our
method achieves substantial output length reduction while maintaining original
reasoning performance, striking an optimal balance between efficiency and
accuracy. Further analysis of overthinking frequency and pass@k score before
and after training demonstrates that our approach in deed effectively
suppresses ineffective steps and encourages effective reasoning, fundamentally
alleviating the overthinking problem. All code will be released upon
acceptance.

</details>


### [41] [A Curriculum Learning Approach to Reinforcement Learning: Leveraging RAG for Multimodal Question Answering](https://arxiv.org/abs/2508.10337)
*Chenliang Zhang,Lin Wang,Yuanyuan Lu,Yusheng Qi,Kexin Wang,Peixu Hou,Wenshi Chen*

Main category: cs.AI

TL;DR: Dianping-Trust-Safety团队在META CRAG-MM挑战中提出多模态多轮问答系统解决方案，结合视觉大语言模型、强化学习和外部知识，任务1获第一，任务3获第三。


<details>
  <summary>Details</summary>
Motivation: 解决多模态多轮问答的复杂需求，整合结构化数据和外部知识，提升问答系统的准确性和上下文理解能力。

Method: 任务1使用视觉大语言模型并基于GPT-4.1知识蒸馏微调，结合课程学习优化强化学习；任务2和3引入网络搜索API补充外部知识。

Result: 任务1以52.38%优势获第一，任务3获第三，验证了课程学习与强化学习结合的有效性。

Conclusion: 该方法在多模态多轮问答任务中表现出色，尤其在复杂查询和上下文理解方面具有优势。

Abstract: This paper describes the solutions of the Dianping-Trust-Safety team for the
META CRAG-MM challenge. The challenge requires building a comprehensive
retrieval-augmented generation system capable for multi-modal multi-turn
question answering. The competition consists of three tasks: (1) answering
questions using structured data retrieved from an image-based mock knowledge
graph, (2) synthesizing information from both knowledge graphs and web search
results, and (3) handling multi-turn conversations that require context
understanding and information aggregation from multiple sources. For Task 1,
our solution is based on the vision large language model, enhanced by
supervised fine-tuning with knowledge distilled from GPT-4.1. We further
applied curriculum learning strategies to guide reinforcement learning,
resulting in improved answer accuracy and reduced hallucination. For Task 2 and
Task 3, we additionally leveraged web search APIs to incorporate external
knowledge, enabling the system to better handle complex queries and multi-turn
conversations. Our approach achieved 1st place in Task 1 with a significant
lead of 52.38\%, and 3rd place in Task 3, demonstrating the effectiveness of
the integration of curriculum learning with reinforcement learning in our
training pipeline.

</details>


### [42] [Multi-Agent Trust Region Policy Optimisation: A Joint Constraint Approach](https://arxiv.org/abs/2508.10340)
*Chak Lam Shek,Guangyao Shi,Pratap Tokekar*

Main category: cs.AI

TL;DR: HATRPO-W和HATRPO-G通过动态分配KL阈值优化了HATRPO在异构多智能体强化学习中的性能，分别提升了22.5%的最终表现，且HATRPO-W更稳定。


<details>
  <summary>Details</summary>
Motivation: HATRPO中固定KL阈值在异构环境下可能导致更新缓慢和局部最优，需动态分配阈值以提升学习效果。

Method: 提出HATRPO-W（基于KKT优化全局KL约束）和HATRPO-G（基于贪婪算法按改进-散度比分配阈值）。

Result: 实验显示两种方法显著提升HATRPO性能，收敛更快，最终奖励更高，HATRPO-W方差更低。

Conclusion: 动态KL阈值分配在异构MARL中更灵活有效，HATRPO-W和HATRPO-G均表现优异。

Abstract: Multi-agent reinforcement learning (MARL) requires coordinated and stable
policy updates among interacting agents. Heterogeneous-Agent Trust Region
Policy Optimization (HATRPO) enforces per-agent trust region constraints using
Kullback-Leibler (KL) divergence to stabilize training. However, assigning each
agent the same KL threshold can lead to slow and locally optimal updates,
especially in heterogeneous settings. To address this limitation, we propose
two approaches for allocating the KL divergence threshold across agents:
HATRPO-W, a Karush-Kuhn-Tucker-based (KKT-based) method that optimizes
threshold assignment under global KL constraints, and HATRPO-G, a greedy
algorithm that prioritizes agents based on improvement-to-divergence ratio. By
connecting sequential policy optimization with constrained threshold
scheduling, our approach enables more flexible and effective learning in
heterogeneous-agent settings. Experimental results demonstrate that our methods
significantly boost the performance of HATRPO, achieving faster convergence and
higher final rewards across diverse MARL benchmarks. Specifically, HATRPO-W and
HATRPO-G achieve comparable improvements in final performance, each exceeding
22.5%. Notably, HATRPO-W also demonstrates more stable learning dynamics, as
reflected by its lower variance.

</details>


### [43] [What to Ask Next? Probing the Imaginative Reasoning of LLMs with TurtleSoup Puzzles](https://arxiv.org/abs/2508.10358)
*Mengtao Zhou,Sifan Wu,Huan Zhang,Qi Sima,Bang Liu*

Main category: cs.AI

TL;DR: 论文研究了大型语言模型（LLMs）在信息稀疏环境中的想象推理能力，提出了一种基于“乌龟汤”游戏的框架，包括基准、代理和评估协议。


<details>
  <summary>Details</summary>
Motivation: 现有基准多为静态或关注社会推理，无法捕捉动态探索性推理过程。

Method: 引入基于“乌龟汤”游戏的框架，开发了TurtleSoup-Bench基准和Mosaic-Agent代理，并设计了多维评估协议。

Result: 实验显示LLMs在想象推理中存在能力限制和常见失败模式，与人类表现有显著差距。

Conclusion: 研究为LLMs的想象推理提供了新见解，并为未来探索性代理行为研究奠定了基础。

Abstract: We investigate the capacity of Large Language Models (LLMs) for imaginative
reasoning--the proactive construction, testing, and revision of hypotheses in
information-sparse environments. Existing benchmarks, often static or focused
on social deduction, fail to capture the dynamic, exploratory nature of this
reasoning process. To address this gap, we introduce a comprehensive research
framework based on the classic "Turtle Soup" game, integrating a benchmark, an
agent, and an evaluation protocol. We present TurtleSoup-Bench, the first
large-scale, bilingual, interactive benchmark for imaginative reasoning,
comprising 800 turtle soup puzzles sourced from both the Internet and expert
authors. We also propose Mosaic-Agent, a novel agent designed to assess LLMs'
performance in this setting. To evaluate reasoning quality, we develop a
multi-dimensional protocol measuring logical consistency, detail completion,
and conclusion alignment. Experiments with leading LLMs reveal clear capability
limits, common failure patterns, and a significant performance gap compared to
humans. Our work offers new insights into LLMs' imaginative reasoning and
establishes a foundation for future research on exploratory agent behavior.

</details>


### [44] [LeanRAG: Knowledge-Graph-Based Generation with Semantic Aggregation and Hierarchical Retrieval](https://arxiv.org/abs/2508.10391)
*Yaoze Zhang,Rong Wu,Pinlong Cai,Xiaoman Wang,Guohang Yan,Song Mao,Ding Wang,Botian Shi*

Main category: cs.AI

TL;DR: LeanRAG是一个基于知识图谱的检索增强生成框架，通过语义聚合和结构感知检索策略提升检索效率和质量。


<details>
  <summary>Details</summary>
Motivation: 现有基于知识图谱的RAG方法存在语义孤岛和检索效率低的问题，需要改进。

Method: 采用语义聚合算法构建实体集群和显式关系，结合自底向上的结构感知检索策略。

Result: 在四个QA基准测试中表现优异，响应质量显著提升，检索冗余减少46%。

Conclusion: LeanRAG通过语义网络和高效检索策略，显著提升了RAG的性能和效率。

Abstract: Retrieval-Augmented Generation (RAG) plays a crucial role in grounding Large
Language Models by leveraging external knowledge, whereas the effectiveness is
often compromised by the retrieval of contextually flawed or incomplete
information. To address this, knowledge graph-based RAG methods have evolved
towards hierarchical structures, organizing knowledge into multi-level
summaries. However, these approaches still suffer from two critical,
unaddressed challenges: high-level conceptual summaries exist as disconnected
``semantic islands'', lacking the explicit relations needed for cross-community
reasoning; and the retrieval process itself remains structurally unaware, often
degenerating into an inefficient flat search that fails to exploit the graph's
rich topology. To overcome these limitations, we introduce LeanRAG, a framework
that features a deeply collaborative design combining knowledge aggregation and
retrieval strategies. LeanRAG first employs a novel semantic aggregation
algorithm that forms entity clusters and constructs new explicit relations
among aggregation-level summaries, creating a fully navigable semantic network.
Then, a bottom-up, structure-guided retrieval strategy anchors queries to the
most relevant fine-grained entities and then systematically traverses the
graph's semantic pathways to gather concise yet contextually comprehensive
evidence sets. The LeanRAG can mitigate the substantial overhead associated
with path retrieval on graphs and minimizes redundant information retrieval.
Extensive experiments on four challenging QA benchmarks with different domains
demonstrate that LeanRAG significantly outperforming existing methods in
response quality while reducing 46\% retrieval redundancy. Code is available
at: https://github.com/RaZzzyz/LeanRAG

</details>


### [45] [HiRef: Leveraging Hierarchical Ontology and Network Refinement for Robust Medication Recommendation](https://arxiv.org/abs/2508.10425)
*Yan Ting Chok,Soyon Park,Seungheun Baek,Hajung Kim,Junhyun Lee,Jaewoo Kang*

Main category: cs.AI

TL;DR: HiRef框架结合医学本体层次结构和电子健康记录（EHR）共现模式，通过双曲空间嵌入和稀疏正则化提升药物推荐的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 解决EHR数据中罕见实体和缺失记录导致的模型泛化能力不足问题。

Method: 结合医学本体层次结构和EHR共现模式，使用双曲空间嵌入和稀疏正则化优化模型。

Result: 在MIMIC-III和MIMIC-IV数据集上表现优异，且在模拟未见代码场景下保持高准确性。

Conclusion: HiRef通过本体和EHR结合，显著提升了药物推荐的鲁棒性和泛化能力。

Abstract: Medication recommendation is a crucial task for assisting physicians in
making timely decisions from longitudinal patient medical records. However,
real-world EHR data present significant challenges due to the presence of
rarely observed medical entities and incomplete records that may not fully
capture the clinical ground truth. While data-driven models trained on
longitudinal Electronic Health Records often achieve strong empirical
performance, they struggle to generalize under missing or novel conditions,
largely due to their reliance on observed co-occurrence patterns. To address
these issues, we propose Hierarchical Ontology and Network Refinement for
Robust Medication Recommendation (HiRef), a unified framework that combines two
complementary structures: (i) the hierarchical semantics encoded in curated
medical ontologies, and (ii) refined co-occurrence patterns derived from
real-world EHRs. We embed ontology entities in hyperbolic space, which
naturally captures tree-like relationships and enables knowledge transfer
through shared ancestors, thereby improving generalizability to unseen codes.
To further improve robustness, we introduce a prior-guided sparse
regularization scheme that refines the EHR co-occurrence graph by suppressing
spurious edges while preserving clinically meaningful associations. Our model
achieves strong performance on EHR benchmarks (MIMIC-III and MIMIC-IV) and
maintains high accuracy under simulated unseen-code settings. Extensive
experiments with comprehensive ablation studies demonstrate HiRef's resilience
to unseen medical codes, supported by in-depth analyses of the learned
sparsified graph structure and medical code embeddings.

</details>


### [46] [MM-Food-100K: A 100,000-Sample Multimodal Food Intelligence Dataset with Verifiable Provenance](https://arxiv.org/abs/2508.10429)
*Yi Dong,Yusuke Muraoka,Scott Shi,Yi Zhang*

Main category: cs.AI

TL;DR: MM-Food-100K是一个公开的10万样本多模态食品数据集，具有可验证的来源。它是从120万质量合格的食品图像中精选的10%子集，包含丰富的注释信息（如菜名、产地）。数据集通过Codatta贡献模型收集，结合了社区众包和AI辅助质量检查，并支持链下追踪。通过微调大型视觉语言模型验证了其有效性，结果显示性能提升。数据集免费公开10%，其余90%保留商业用途并分享收益给贡献者。


<details>
  <summary>Details</summary>
Motivation: 食品数据的多模态和可验证来源需求日益增长，但现有数据集规模和质量有限。MM-Food-100K旨在填补这一空白，提供高质量、可追溯的食品数据集。

Method: 使用Codatta贡献模型，结合社区众包和AI辅助质量检查，从87,000多名贡献者中收集数据。数据通过链下分类账追踪，并计划实现链上协议。

Result: 微调大型视觉语言模型（如ChatGPT 5、Qwen-Max）后，在图像营养预测任务中表现优于基线模型。

Conclusion: MM-Food-100K是一个高质量、可追溯的多模态食品数据集，通过公开部分数据促进研究，同时保留商业潜力并回馈贡献者。

Abstract: We present MM-Food-100K, a public 100,000-sample multimodal food intelligence
dataset with verifiable provenance. It is a curated approximately 10% open
subset of an original 1.2 million, quality-accepted corpus of food images
annotated for a wide range of information (such as dish name, region of
creation). The corpus was collected over six weeks from over 87,000
contributors using the Codatta contribution model, which combines community
sourcing with configurable AI-assisted quality checks; each submission is
linked to a wallet address in a secure off-chain ledger for traceability, with
a full on-chain protocol on the roadmap. We describe the schema, pipeline, and
QA, and validate utility by fine-tuning large vision-language models (ChatGPT
5, ChatGPT OSS, Qwen-Max) on image-based nutrition prediction. Fine-tuning
yields consistent gains over out-of-box baselines across standard metrics; we
report results primarily on the MM-Food-100K subset. We release MM-Food-100K
for publicly free access and retain approximately 90% for potential commercial
access with revenue sharing to contributors.

</details>


### [47] [We-Math 2.0: A Versatile MathBook System for Incentivizing Visual Mathematical Reasoning](https://arxiv.org/abs/2508.10433)
*Runqi Qiao,Qiuna Tan,Peiqing Yang,Yanzi Wang,Xiaowan Wang,Enhui Wan,Sitong Zhou,Guanting Dong,Yuchen Zeng,Yida Xu,Jie Wang,Chong Sun,Chen Li,Honggang Zhang*

Main category: cs.AI

TL;DR: We-Math 2.0是一个统一系统，通过结构化数学知识系统、模型中心数据空间建模和强化学习训练范式，提升多模态大语言模型（MLLMs）的数学推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有研究多关注数据集构建和方法优化，忽视了知识驱动设计和数据空间建模，导致MLLMs在复杂数学推理上表现不足。

Method: We-Math 2.0整合了五级数学知识系统、双扩展数据集（MathBook-Standard & Pro）、两阶段强化学习框架（MathBook-RL）和综合评测基准（MathBookEval）。

Result: 实验表明，MathBook-RL在四个常用基准上表现优异，并在MathBookEval上展现出强泛化能力。

Conclusion: We-Math 2.0通过系统化设计和强化学习，显著提升了MLLMs的数学推理能力，具有广泛的应用潜力。

Abstract: Multimodal Large Language Models (MLLMs) have demonstrated impressive
capabilities across various tasks, but still struggle with complex mathematical
reasoning. Existing research primarily focuses on dataset construction and
method optimization, often overlooking two critical aspects: comprehensive
knowledge-driven design and model-centric data space modeling. In this paper,
we introduce We-Math 2.0, a unified system that integrates a structured
mathematical knowledge system, model-centric data space modeling, and a
reinforcement learning (RL)-based training paradigm to comprehensively enhance
the mathematical reasoning abilities of MLLMs. The key contributions of We-Math
2.0 are fourfold: (1) MathBook Knowledge System: We construct a five-level
hierarchical system encompassing 491 knowledge points and 1,819 fundamental
principles. (2) MathBook-Standard & Pro: We develop MathBook-Standard, a
dataset that ensures broad conceptual coverage and flexibility through dual
expansion. Additionally, we define a three-dimensional difficulty space and
generate 7 progressive variants per problem to build MathBook-Pro, a
challenging dataset for robust training. (3) MathBook-RL: We propose a
two-stage RL framework comprising: (i) Cold-Start Fine-tuning, which aligns the
model with knowledge-oriented chain-of-thought reasoning; and (ii) Progressive
Alignment RL, leveraging average-reward learning and dynamic data scheduling to
achieve progressive alignment across difficulty levels. (4) MathBookEval: We
introduce a comprehensive benchmark covering all 491 knowledge points with
diverse reasoning step distributions. Experimental results show that
MathBook-RL performs competitively with existing baselines on four widely-used
benchmarks and achieves strong results on MathBookEval, suggesting promising
generalization in mathematical reasoning.

</details>


### [48] [FIRESPARQL: A LLM-based Framework for SPARQL Query Generation over Scholarly Knowledge Graphs](https://arxiv.org/abs/2508.10467)
*Xueli Pan,Victor de Boer,Jacco van Ossenbruggen*

Main category: cs.AI

TL;DR: 论文提出FIRESPARQL框架，通过微调LLM和结合RAG技术，解决学术知识图谱问答中SPARQL查询生成的结构和语义错误，实验表明微调方法性能最佳。


<details>
  <summary>Details</summary>
Motivation: 解决学术知识图谱问答中LLM生成SPARQL查询时的结构和语义错误问题。

Method: 提出FIRESPARQL框架，结合微调LLM、RAG技术和SPARQL查询校正层，并在SciQA基准上评估多种配置。

Result: 微调方法表现最佳，ROUGE-L达0.90，RelaxedEM达0.85。

Conclusion: FIRESPARQL框架有效提升了学术知识图谱问答中SPARQL查询生成的准确性和结果质量。

Abstract: Question answering over Scholarly Knowledge Graphs (SKGs) remains a
challenging task due to the complexity of scholarly content and the intricate
structure of these graphs. Large Language Model (LLM) approaches could be used
to translate natural language questions (NLQs) into SPARQL queries; however,
these LLM-based approaches struggle with SPARQL query generation due to limited
exposure to SKG-specific content and the underlying schema. We identified two
main types of errors in the LLM-generated SPARQL queries: (i) structural
inconsistencies, such as missing or redundant triples in the queries, and (ii)
semantic inaccuracies, where incorrect entities or properties are shown in the
queries despite a correct query structure. To address these issues, we propose
FIRESPARQL, a modular framework that supports fine-tuned LLMs as a core
component, with optional context provided via retrieval-augmented generation
(RAG) and a SPARQL query correction layer. We evaluate the framework on the
SciQA Benchmark using various configurations (zero-shot, zero-shot with RAG,
one-shot, fine-tuning, and fine-tuning with RAG) and compare the performance
with baseline and state-of-the-art approaches. We measure query accuracy using
BLEU and ROUGE metrics, and query result accuracy using relaxed exact
match(RelaxedEM), with respect to the gold standards containing the NLQs,
SPARQL queries, and the results of the queries. Experimental results
demonstrate that fine-tuning achieves the highest overall performance, reaching
0.90 ROUGE-L for query accuracy and 0.85 RelaxedEM for result accuracy on the
test set.

</details>


### [49] [SEQ-GPT: LLM-assisted Spatial Query via Example](https://arxiv.org/abs/2508.10486)
*Ivan Khai Ze Lim,Ningyi Liao,Yiming Yang,Gerald Wei Yong Yip,Siqiang Luo*

Main category: cs.AI

TL;DR: 论文提出了一种基于大型语言模型（LLMs）的空间查询系统SEQ-GPT，用于通过自然语言进行更灵活的空间示例查询（SEQ）。


<details>
  <summary>Details</summary>
Motivation: 现有空间服务（如在线地图）主要依赖用户查询进行位置搜索，但在执行复杂任务（如同时搜索多个相关位置）时用户体验受限。

Method: 引入SEQ-GPT系统，利用LLMs的自然语言能力实现交互式操作，并提出一种定制的LLM适配流程，通过对话合成和多模型协作将自然语言与结构化空间数据对齐。

Result: SEQ-GPT展示了通过自然语言扩展空间搜索的端到端解决方案，适用于实际数据和场景。

Conclusion: SEQ-GPT通过LLMs提升了空间查询的灵活性和交互性，为复杂空间搜索任务提供了新的可能性。

Abstract: Contemporary spatial services such as online maps predominantly rely on user
queries for location searches. However, the user experience is limited when
performing complex tasks, such as searching for a group of locations
simultaneously. In this study, we examine the extended scenario known as
Spatial Exemplar Query (SEQ), where multiple relevant locations are jointly
searched based on user-specified examples. We introduce SEQ-GPT, a spatial
query system powered by Large Language Models (LLMs) towards more versatile SEQ
search using natural language. The language capabilities of LLMs enable unique
interactive operations in the SEQ process, including asking users to clarify
query details and dynamically adjusting the search based on user feedback. We
also propose a tailored LLM adaptation pipeline that aligns natural language
with structured spatial data and queries through dialogue synthesis and
multi-model cooperation. SEQ-GPT offers an end-to-end demonstration for
broadening spatial search with realistic data and application scenarios.

</details>


### [50] [Reverse Physician-AI Relationship: Full-process Clinical Diagnosis Driven by a Large Language Model](https://arxiv.org/abs/2508.10492)
*Shicheng Xu,Xin Huang,Zihao Wei,Liang Pang,Huawei Shen,Xueqi Cheng*

Main category: cs.AI

TL;DR: 论文提出DxDirector-7B，一种大型语言模型，能够主导全流程临床诊断，减少医生工作量并提高诊断准确性。


<details>
  <summary>Details</summary>
Motivation: 当前AI在临床诊断中仅作为辅助工具，无法主导从模糊主诉开始的完整诊断流程，限制了其减轻医生负担和提升效率的潜力。

Method: 提出DxDirector-7B，一种具备深度思考能力的LLM，能够主导诊断流程，并建立责任框架。

Result: 在罕见、复杂和真实案例中，DxDirector-7B显著优于现有医学LLM和通用LLM，减少医生工作量并提高准确性。

Conclusion: DxDirector-7B标志着AI从辅助工具转变为诊断主导者，为高效、准确的诊断提供了新方案。

Abstract: Full-process clinical diagnosis in the real world encompasses the entire
diagnostic workflow that begins with only an ambiguous chief complaint. While
artificial intelligence (AI), particularly large language models (LLMs), is
transforming clinical diagnosis, its role remains largely as an assistant to
physicians. This AI-assisted working pattern makes AI can only answer specific
medical questions at certain parts within the diagnostic process, but lack the
ability to drive the entire diagnostic process starting from an ambiguous
complaint, which still relies heavily on human physicians. This gap limits AI's
ability to fully reduce physicians' workload and enhance diagnostic efficiency.
To address this, we propose a paradigm shift that reverses the relationship
between physicians and AI: repositioning AI as the primary director, with
physicians serving as its assistants. So we present DxDirector-7B, an LLM
endowed with advanced deep thinking capabilities, enabling it to drive the
full-process diagnosis with minimal physician involvement. Furthermore,
DxDirector-7B establishes a robust accountability framework for misdiagnoses,
delineating responsibility between AI and human physicians. In evaluations
across rare, complex, and real-world cases under full-process diagnosis
setting, DxDirector-7B not only achieves significant superior diagnostic
accuracy but also substantially reduces physician workload than
state-of-the-art medical LLMs as well as general-purpose LLMs. Fine-grained
analyses across multiple clinical departments and tasks validate its efficacy,
with expert evaluations indicating its potential to serve as a viable
substitute for medical specialists. These findings mark a new era where AI,
traditionally a physicians' assistant, now drives the entire diagnostic process
to drastically reduce physicians' workload, indicating an efficient and
accurate diagnostic solution.

</details>


### [51] [PASS: Probabilistic Agentic Supernet Sampling for Interpretable and Adaptive Chest X-Ray Reasoning](https://arxiv.org/abs/2508.10501)
*Yushi Feng,Junye Du,Yingying Hong,Qifan Wang,Lequan Yu*

Main category: cs.AI

TL;DR: PASS是一种多模态框架，通过概率性采样和自适应工具选择，解决了医疗AI中的黑盒推理、多模态整合和效率问题，显著提升了性能和安全性。


<details>
  <summary>Details</summary>
Motivation: 现有工具增强的代理系统存在黑盒推理、多模态整合不足和效率低下等问题，限制了其在医疗任务中的应用。

Method: PASS通过多工具图自适应采样代理工作流，结合三阶段训练（专家知识预热、对比路径排序和成本感知强化学习），优化性能和成本平衡。

Result: PASS在多个指标（如准确率、AUC）上显著优于基线，同时平衡计算成本。

Conclusion: PASS为医疗AI系统提供了可解释、自适应和多模态的新范式。

Abstract: Existing tool-augmented agentic systems are limited in the real world by (i)
black-box reasoning steps that undermine trust of decision-making and pose
safety risks, (ii) poor multimodal integration, which is inherently critical
for healthcare tasks, and (iii) rigid and computationally inefficient agentic
pipelines. We introduce PASS (Probabilistic Agentic Supernet Sampling), the
first multimodal framework to address these challenges in the context of Chest
X-Ray (CXR) reasoning. PASS adaptively samples agentic workflows over a
multi-tool graph, yielding decision paths annotated with interpretable
probabilities. Given the complex CXR reasoning task with multimodal medical
data, PASS leverages its learned task-conditioned distribution over the agentic
supernet. Thus, it adaptively selects the most suitable tool at each supernet
layer, offering probability-annotated trajectories for post-hoc audits and
directly enhancing medical AI safety. PASS also continuously compresses salient
findings into an evolving personalized memory, while dynamically deciding
whether to deepen its reasoning path or invoke an early exit for efficiency. To
optimize a Pareto frontier balancing performance and cost, we design a novel
three-stage training procedure, including expert knowledge warm-up, contrastive
path-ranking, and cost-aware reinforcement learning. To facilitate rigorous
evaluation, we introduce CAB-E, a comprehensive benchmark for multi-step,
safety-critical, free-form CXR reasoning. Experiments across various benchmarks
validate that PASS significantly outperforms strong baselines in multiple
metrics (e.g., accuracy, AUC, LLM-J.) while balancing computational costs,
pushing a new paradigm shift towards interpretable, adaptive, and multimodal
medical agentic systems.

</details>


### [52] [Diversity First, Quality Later: A Two-Stage Assumption for Language Model Alignment](https://arxiv.org/abs/2508.10530)
*Zetian Sun,Dongfang Li,Baotian Hu*

Main category: cs.AI

TL;DR: 论文探讨了语言模型（LM）与人类偏好对齐的问题，提出静态和动态偏好数据在优化中的效果差异，并引入对齐阶段假设来解释这一现象。


<details>
  <summary>Details</summary>
Motivation: 研究动机是解决语言模型与人类偏好对齐的优化问题，特别是静态和动态偏好数据在训练中的不同效果。

Method: 方法包括提出对齐阶段假设（分为偏好注入和偏好微调阶段），并通过理论和实验分析验证其有效性。

Result: 实验结果表明，动态数据在不同模型中的效果差异显著（如Llama-3为3倍，Zephyr为0.4倍），验证了假设的普适性。

Conclusion: 结论是对齐过程可分为两个阶段，识别阶段边界有助于优化语言模型的对齐效果。

Abstract: The alignment of language models (LMs) with human preferences is critical for
building reliable AI systems. The problem is typically framed as optimizing an
LM policy to maximize the expected reward that reflects human preferences.
Recently, Direct Preference Optimization (DPO) was proposed as a LM alignment
method that directly optimize the policy from static preference data, and
further improved by incorporating on-policy sampling (i.e., preference
candidates generated during the training loop) for better LM alignment.
However, we show on-policy data is not always optimal, with systematic
effectiveness difference emerging between static and on-policy preference
candidates. For example, on-policy data can result in a 3$\times$ effectiveness
compared with static data for Llama-3, and a 0.4$\times$ effectiveness for
Zephyr. To explain the phenomenon, we propose the alignment stage assumption,
which divides the alignment process into two distinct stages: the preference
injection stage, which benefits from diverse data, and the preference
fine-tuning stage, which favors high-quality data. Through theoretical and
empirical analysis, we characterize these stages and propose an effective
algorithm to identify the boundaries between them. We perform experiments on 5
models (Llama, Zephyr, Phi-2, Qwen, Pythia) and 2 alignment methods (DPO,
SLiC-HF) to show the generalizability of alignment stage assumption and
boundary measurement.

</details>


### [53] [Improving Value-based Process Verifier via Low-Cost Variance Reduction](https://arxiv.org/abs/2508.10539)
*Zetian Sun,Dongfang Li,Baotian Hu,Min Zhang*

Main category: cs.AI

TL;DR: 提出了一种名为ComMCS的方法，通过结合当前和后续步骤的蒙特卡洛估计器，减少方差并保持无偏估计，从而提升大语言模型在复杂领域（如数学）中的推理能力。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在复杂领域的推理能力仍有挑战，现有方法因蒙特卡洛样本数量有限导致估计误差，主要源于高方差。

Method: 提出ComMCS方法，通过线性组合当前和后续步骤的蒙特卡洛估计器，减少方差且无需额外推理成本。

Result: 在MATH-500和GSM8K基准测试中，ComMCS比回归优化方法和非方差减少基线分别提升2.8和2.2分。

Conclusion: ComMCS通过减少方差有效提升推理能力，且无需额外成本，具有实际应用潜力。

Abstract: Large language models (LLMs) have achieved remarkable success in a wide range
of tasks. However, their reasoning capabilities, particularly in complex
domains like mathematics, remain a significant challenge. Value-based process
verifiers, which estimate the probability of a partial reasoning chain leading
to a correct solution, are a promising approach for improving reasoning.
Nevertheless, their effectiveness is often hindered by estimation error in
their training annotations, a consequence of the limited number of Monte Carlo
(MC) samples feasible due to the high cost of LLM inference. In this paper, we
identify that the estimation error primarily arises from high variance rather
than bias, and the MC estimator is a Minimum Variance Unbiased Estimator
(MVUE). To address the problem, we propose the \textsc{Com}pound \textsc{M}onte
\textsc{C}arlo \textsc{S}ampling (ComMCS) method, which constructs an unbiased
estimator by linearly combining the MC estimators from the current and
subsequent steps. Theoretically, we show that our method leads to a predictable
reduction in variance, while maintaining an unbiased estimation without
additional LLM inference cost. We also perform empirical experiments on the
MATH-500 and GSM8K benchmarks to demonstrate the effectiveness of our method.
Notably, ComMCS outperforms regression-based optimization method by 2.8 points,
the non-variance-reduced baseline by 2.2 points on MATH-500 on Best-of-32
sampling experiment.

</details>


### [54] [MSRS: Adaptive Multi-Subspace Representation Steering for Attribute Alignment in Large Language Models](https://arxiv.org/abs/2508.10599)
*Xinyan Jiang,Lin Zhang,Jiayi Zhang,Qingsong Yang,Guimin Hu,Di Wang,Lijie Hu*

Main category: cs.AI

TL;DR: MSRS是一种通过子空间表示微调实现多属性控制的新框架，减少属性间干扰并提升控制精度。


<details>
  <summary>Details</summary>
Motivation: 现有方法在多属性联合控制时存在干扰和权衡问题，MSRS旨在解决这一问题。

Method: MSRS通过分配正交子空间隔离属性影响，结合混合子空间策略和动态权重函数，实现精确控制。

Result: 实验表明MSRS显著减少属性冲突，优于现有方法，并能泛化到多种下游任务。

Conclusion: MSRS为多属性控制提供了一种高效且通用的解决方案。

Abstract: Activation steering offers a promising approach to controlling the behavior
of Large Language Models by directly manipulating their internal activations.
However, most existing methods struggle to jointly steer multiple attributes,
often resulting in interference and undesirable trade-offs. To address this
challenge, we propose Multi-Subspace Representation Steering (MSRS), a novel
framework for effective multi-attribute steering via subspace representation
fine-tuning. MSRS reduces inter-attribute interference by allocating orthogonal
subspaces to each attribute, isolating their influence within the model's
representation space. MSRS also incorporates a hybrid subspace composition
strategy: it combines attribute-specific subspaces for unique steering
directions with a shared subspace for common steering directions. A dynamic
weighting function learns to efficiently integrate these components for precise
control. During inference, MSRS introduces a token-level steering mechanism
that dynamically identifies and intervenes on the most semantically relevant
tokens, enabling fine-grained behavioral modulation. Experimental results show
that MSRS significantly reduces attribute conflicts, surpasses existing methods
across a range of attributes, and generalizes effectively to diverse downstream
tasks.

</details>


### [55] [STEP: Stepwise Curriculum Learning for Context-Knowledge Fusion in Conversational Recommendation](https://arxiv.org/abs/2508.10669)
*Zhenye Yang,Jinpeng Chen,Huan Li,Xiongnan Jin,Xuanyang Li,Junwei Zhang,Hongbo Gao,Kaimin Wei,Senzhang Wang*

Main category: cs.AI

TL;DR: STEP是一种基于预训练语言模型的对话推荐系统，通过课程引导的上下文-知识融合和轻量级任务特定提示调优，解决了现有系统在捕捉用户偏好和对话上下文深层语义方面的挑战。


<details>
  <summary>Details</summary>
Motivation: 现有对话推荐系统在整合外部知识图谱信息时难以处理复杂语义关系，导致推荐结果与用户期望不符。

Method: STEP采用三阶段课程逐步对齐对话上下文与知识图谱实体，并通过双提示方案（对话前缀和推荐前缀）将融合表示注入冻结的语言模型中。

Result: 实验表明，STEP在两个公共数据集上的推荐精度和对话质量优于主流方法。

Conclusion: STEP通过上下文-知识融合和双提示方案，显著提升了对话推荐系统的性能。

Abstract: Conversational recommender systems (CRSs) aim to proactively capture user
preferences through natural language dialogue and recommend high-quality items.
To achieve this, CRS gathers user preferences via a dialog module and builds
user profiles through a recommendation module to generate appropriate
recommendations. However, existing CRS faces challenges in capturing the deep
semantics of user preferences and dialogue context. In particular, the
efficient integration of external knowledge graph (KG) information into
dialogue generation and recommendation remains a pressing issue. Traditional
approaches typically combine KG information directly with dialogue content,
which often struggles with complex semantic relationships, resulting in
recommendations that may not align with user expectations.
  To address these challenges, we introduce STEP, a conversational recommender
centered on pre-trained language models that combines curriculum-guided
context-knowledge fusion with lightweight task-specific prompt tuning. At its
heart, an F-Former progressively aligns the dialogue context with
knowledge-graph entities through a three-stage curriculum, thus resolving
fine-grained semantic mismatches. The fused representation is then injected
into the frozen language model via two minimal yet adaptive prefix prompts: a
conversation prefix that steers response generation toward user intent and a
recommendation prefix that biases item ranking toward knowledge-consistent
candidates. This dual-prompt scheme allows the model to share cross-task
semantics while respecting the distinct objectives of dialogue and
recommendation. Experimental results show that STEP outperforms mainstream
methods in the precision of recommendation and dialogue quality in two public
datasets.

</details>


### [56] [GenOM: Ontology Matching with Description Generation and Large Language Model](https://arxiv.org/abs/2508.10703)
*Yiping Song,Jiaoyan Chen,Renate A. Schmidt*

Main category: cs.AI

TL;DR: GenOM是一种基于大语言模型（LLM）的本体对齐框架，通过生成文本定义丰富本体概念的语义表示，结合嵌入模型和精确匹配工具提升性能，在生物医学领域表现优异。


<details>
  <summary>Details</summary>
Motivation: 解决生物医学领域中异构知识源的语义互操作性和集成问题，特别是复杂疾病和药物相关概念的对齐。

Method: 使用LLM生成本体概念的文本定义，结合嵌入模型检索对齐候选，并利用精确匹配工具提高精度。

Result: 在OAEI Bio-ML测试中表现优异，超越传统本体匹配系统和近期LLM方法。

Conclusion: GenOM框架通过语义增强和少样本提示，展现出强大的鲁棒性和适应性。

Abstract: Ontology matching (OM) plays an essential role in enabling semantic
interoperability and integration across heterogeneous knowledge sources,
particularly in the biomedical domain which contains numerous complex concepts
related to diseases and pharmaceuticals. This paper introduces GenOM, a large
language model (LLM)-based ontology alignment framework, which enriches the
semantic representations of ontology concepts via generating textual
definitions, retrieves alignment candidates with an embedding model, and
incorporates exact matching-based tools to improve precision. Extensive
experiments conducted on the OAEI Bio-ML track demonstrate that GenOM can often
achieve competitive performance, surpassing many baselines including
traditional OM systems and recent LLM-based methods. Further ablation studies
confirm the effectiveness of semantic enrichment and few-shot prompting,
highlighting the framework's robustness and adaptability.

</details>


### [57] [Agentic Design Review System](https://arxiv.org/abs/2508.10745)
*Sayan Nag,K J Joseph,Koustava Goswami,Vlad I Morariu,Balaji Vasan Srinivasan*

Main category: cs.AI

TL;DR: 提出了一种多代理协作的图形设计评估系统（AgenticDRS），通过图匹配和提示扩展方法提升代理的设计感知能力，并在DRS-BENCH基准上验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 图形设计评估需要从多个维度综合专家反馈，但目前缺乏系统化的方法。

Method: 提出AgenticDRS系统，利用多代理协作和元代理协调，结合图匹配和提示扩展技术。

Result: 实验表明AgenticDRS在评估图形设计和生成反馈方面优于现有方法。

Conclusion: 该研究为图形设计评估提供了实用且高效的解决方案，并呼吁更多关注这一研究方向。

Abstract: Evaluating graphic designs involves assessing it from multiple facets like
alignment, composition, aesthetics and color choices. Evaluating designs in a
holistic way involves aggregating feedback from individual expert reviewers.
Towards this, we propose an Agentic Design Review System (AgenticDRS), where
multiple agents collaboratively analyze a design, orchestrated by a meta-agent.
A novel in-context exemplar selection approach based on graph matching and a
unique prompt expansion method plays central role towards making each agent
design aware. Towards evaluating this framework, we propose DRS-BENCH
benchmark. Thorough experimental evaluation against state-of-the-art baselines
adapted to the problem setup, backed-up with critical ablation experiments
brings out the efficacy of Agentic-DRS in evaluating graphic designs and
generating actionable feedback. We hope that this work will attract attention
to this pragmatic, yet under-explored research direction.

</details>


### [58] [Scaling Up without Fading Out: Goal-Aware Sparse GNN for RL-based Generalized Planning](https://arxiv.org/abs/2508.10747)
*Sangwoo Jeon,Juchul Shin,Gyeong-Tae Kim,YeonJe Cho,Seongwoo Kim*

Main category: cs.AI

TL;DR: 提出了一种稀疏、目标感知的GNN表示方法，解决了传统密集图表示在大型规划问题中的计算和内存问题。


<details>
  <summary>Details</summary>
Motivation: 传统方法使用全连接图表示规划状态，导致边缘信息组合爆炸和节点信息稀释，难以扩展到大规模问题。

Method: 采用稀疏、目标感知的GNN表示，选择性编码局部相关关系并显式整合目标相关空间特征。

Result: 在大型网格环境中验证了方法的可扩展性，显著提高了策略泛化能力和成功率。

Conclusion: 为解决现实中的大规模广义规划任务提供了实用基础。

Abstract: Generalized planning using deep reinforcement learning (RL) combined with
graph neural networks (GNNs) has shown promising results in various symbolic
planning domains described by PDDL. However, existing approaches typically
represent planning states as fully connected graphs, leading to a combinatorial
explosion in edge information and substantial sparsity as problem scales grow,
especially evident in large grid-based environments. This dense representation
results in diluted node-level information, exponentially increases memory
requirements, and ultimately makes learning infeasible for larger-scale
problems. To address these challenges, we propose a sparse, goal-aware GNN
representation that selectively encodes relevant local relationships and
explicitly integrates spatial features related to the goal. We validate our
approach by designing novel drone mission scenarios based on PDDL within a grid
world, effectively simulating realistic mission execution environments. Our
experimental results demonstrate that our method scales effectively to larger
grid sizes previously infeasible with dense graph representations and
substantially improves policy generalization and success rates. Our findings
provide a practical foundation for addressing realistic, large-scale
generalized planning tasks.

</details>


### [59] [Modeling Human Responses to Multimodal AI Content](https://arxiv.org/abs/2508.10769)
*Zhiqi Shen,Shaojing Fan,Danni Xu,Terence Sim,Mohan Kankanhalli*

Main category: cs.AI

TL;DR: 论文研究了AI生成内容对人类行为和感知的影响，提出了MhAIM数据集和T-Lens系统，通过新指标量化用户对内容的判断和互动。


<details>
  <summary>Details</summary>
Motivation: 随着AI生成内容的普及，其带来的错误信息风险增加，但人类如何感知和应对此类内容的研究较少。

Method: 引入MhAIM数据集，提出信任度、影响力和开放性三个新指标，开发基于LLM的T-Lens系统，结合HR-MCP协议。

Result: 研究发现人类在图文结合的内容中更容易识别AI生成内容，尤其是当图文不一致时。T-Lens系统能更好地预测人类反应。

Conclusion: 研究为LLM提供了人类感知能力的工具，揭示了AI、人类认知和信息接收的复杂关系，提出了减少AI错误信息风险的策略。

Abstract: As AI-generated content becomes widespread, so does the risk of
misinformation. While prior research has primarily focused on identifying
whether content is authentic, much less is known about how such content
influences human perception and behavior. In domains like trading or the stock
market, predicting how people react (e.g., whether a news post will go viral),
can be more critical than verifying its factual accuracy. To address this, we
take a human-centered approach and introduce the MhAIM Dataset, which contains
154,552 online posts (111,153 of them AI-generated), enabling large-scale
analysis of how people respond to AI-generated content. Our human study reveals
that people are better at identifying AI content when posts include both text
and visuals, particularly when inconsistencies exist between the two. We
propose three new metrics: trustworthiness, impact, and openness, to quantify
how users judge and engage with online content. We present T-Lens, an LLM-based
agent system designed to answer user queries by incorporating predicted human
responses to multimodal information. At its core is HR-MCP (Human Response
Model Context Protocol), built on the standardized Model Context Protocol
(MCP), enabling seamless integration with any LLM. This integration allows
T-Lens to better align with human reactions, enhancing both interpretability
and interaction capabilities. Our work provides empirical insights and
practical tools to equip LLMs with human-awareness capabilities. By
highlighting the complex interplay among AI, human cognition, and information
reception, our findings suggest actionable strategies for mitigating the risks
of AI-driven misinformation.

</details>


### [60] [The Knowledge-Reasoning Dissociation: Fundamental Limitations of LLMs in Clinical Natural Language Inference](https://arxiv.org/abs/2508.10777)
*Maël Jullien,Marco Valentino,André Freitas*

Main category: cs.AI

TL;DR: 论文通过临床试验自然语言推理基准测试发现，尽管大型语言模型在知识获取上表现优异，但在结构化推理任务中表现不佳，揭示了其内部表征的局限性。


<details>
  <summary>Details</summary>
Motivation: 探究大型语言模型是否通过数据和参数扩展获得结构化、可泛化的内部表征。

Method: 引入临床试验自然语言推理基准（四种推理类型），并设计GKMRV探针分离知识获取与推理失败。评估六种当代LLM的直接和思维链提示表现。

Result: 模型在GKMRV探针上表现优异（准确率0.918），但在推理任务中表现差（准确率0.25），输出一致性高（0.87），表明其依赖启发式方法。

Conclusion: 当前LLM缺乏结构化、可组合的内部表征，无法可靠应用知识。GKMRV框架为高风险领域LLM可靠性评估提供了有效方法。

Abstract: Large language models are often assumed to acquire increasingly structured,
generalizable internal representations simply by scaling data and parameters.
We interrogate this assumption by introducing a Clinical Trial Natural Language
Inference benchmark comprising four reasoning families, Causal Attribution,
Compositional Grounding, Epistemic Verification, and Risk State Abstraction.
Each item is paired with a targeted Ground Knowledge and Meta-Level Reasoning
Verification (GKMRV) probe, allowing us to dissociate failures of factual
access from failures of inference. We evaluate six contemporary LLMs under both
direct and chain of thought prompting.
  Models achieve near-ceiling GKMRV accuracy (mean accuracy 0.918) yet perform
poorly on the main reasoning tasks (mean accuracy 0.25). Despite low accuracy,
output inferences are highly consistent across samples (mean 0.87), indicating
a systematic application of underlying heuristics and shortcuts.
  These results reveal fundamental structural and representational limitations:
current LLMs often possess the relevant clinical knowledge but lack the
structured, composable internal representations needed to deploy it reliably
(e.g., integrating constraints, weighing evidence, or simulating
counterfactuals). Decoupling knowledge from reasoning with GKMRV makes this
dissociation explicit and measurable, providing an effective framework for
probing the reliability of LLMs in high-stakes domains.

</details>


### [61] [Who Benefits from AI Explanations? Towards Accessible and Interpretable Systems](https://arxiv.org/abs/2508.10806)
*Maria J. P. Peixoto,Akriti Pandey,Ahsan Zaman,Peter R. Lewis*

Main category: cs.AI

TL;DR: 论文探讨了可解释AI（XAI）的可访问性问题，特别是对视觉障碍用户的适用性，提出了一种包容性设计方法。


<details>
  <summary>Details</summary>
Motivation: 随着AI系统在关键决策领域的应用增加，可解释性成为提升用户理解和选择的关键，但XAI对视觉障碍用户的可访问性研究不足。

Method: 通过文献综述和四部分方法论验证：AI系统分类、用户角色定义、原型设计与实现、专家和用户评估。

Result: 初步发现表明，简化解释比详细解释更易理解，且多模态呈现有助于提升公平可解释性。

Conclusion: XAI需要更多包容性设计，以解决视觉障碍用户的可访问性差距。

Abstract: As AI systems are increasingly deployed to support decision-making in
critical domains, explainability has become a means to enhance the
understandability of these outputs and enable users to make more informed and
conscious choices. However, despite growing interest in the usability of
eXplainable AI (XAI), the accessibility of these methods, particularly for
users with vision impairments, remains underexplored. This paper investigates
accessibility gaps in XAI through a two-pronged approach. First, a literature
review of 79 studies reveals that evaluations of XAI techniques rarely include
disabled users, with most explanations relying on inherently visual formats.
Second, we present a four-part methodological proof of concept that
operationalizes inclusive XAI design: (1) categorization of AI systems, (2)
persona definition and contextualization, (3) prototype design and
implementation, and (4) expert and user assessment of XAI techniques for
accessibility. Preliminary findings suggest that simplified explanations are
more comprehensible for non-visual users than detailed ones, and that
multimodal presentation is required for more equitable interpretability.

</details>
