{"id": "2509.08090", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.08090", "abs": "https://arxiv.org/abs/2509.08090", "authors": ["Eman Abdullah AlOmar", "Luo Xu", "Sofia Martinez", "Anthony Peruma", "Mohamed Wiem Mkaouer", "Christian D. Newman", "Ali Ouni"], "title": "ChatGPT for Code Refactoring: Analyzing Topics, Interaction, and Effective Prompts", "comment": null, "summary": "Large Language Models (LLMs), such as ChatGPT, have become widely popular and\nwidely used in various software engineering tasks such as refactoring, testing,\ncode review, and program comprehension. Although recent studies have examined\nthe effectiveness of LLMs in recommending and suggesting refactoring, there is\na limited understanding of how developers express their refactoring needs when\ninteracting with ChatGPT. In this paper, our goal is to explore interactions\nrelated to refactoring between developers and ChatGPT to better understand how\ndevelopers identify areas for improvement in code, and how ChatGPT addresses\ndevelopers' needs. Our approach involves text mining 715 refactoring-related\ninteractions from 29,778 ChatGPT prompts and responses, as well as the analysis\nof developers' explicit refactoring intentions."}
{"id": "2509.08285", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.08285", "abs": "https://arxiv.org/abs/2509.08285", "authors": ["Carmen Cârlan", "Daniel Ratiu", "Michael Wagner"], "title": "Safety Factories -- a Manifesto", "comment": "Presented at The 44th International Conference on Computer Safety,\n  Reliability and Security (SafeComp 2025)", "summary": "Modern cyber-physical systems are operated by complex software that\nincreasingly takes over safety-critical functions. Software enables rapid\niterations and continuous delivery of new functionality that meets the\never-changing expectations of users. As high-speed development requires\ndiscipline, rigor, and automation, software factories are used. These entail\nmethods and tools used for software development, such as build systems and\npipelines. To keep up with the rapid evolution of software, we need to bridge\nthe disconnect in methods and tools between software development and safety\nengineering today. We need to invest more in formality upfront - capturing\nsafety work products in semantically rich models that are machine-processable,\ndefining automatic consistency checks, and automating the generation of\ndocumentation - to benefit later. Transferring best practices from software to\nsafety engineering is worth exploring. We advocate for safety factories, which\nintegrate safety tooling and methods into software development pipelines."}
{"id": "2509.08389", "categories": ["cs.SE", "cs.CY"], "pdf": "https://arxiv.org/pdf/2509.08389", "abs": "https://arxiv.org/abs/2509.08389", "authors": ["Marco Torchiano", "Riccardo Coppola", "Antonio Vetro'", "Xhoi Musaj"], "title": "The Impact of Team Diversity in Agile Development Education", "comment": "Post-print of paper published at FSE Companion '25: Proceedings of\n  the 33rd ACM International Conference on the Foundations of Software\n  Engineering", "summary": "Software Engineering is mostly a male-dominated sector, where gender\ndiversity is a key feature for improving equality of opportunities,\nproductivity, and innovation. Other diversity aspects, including but not\nlimited to nationality and ethnicity, are often understudied.In this work we\naim to assess the impact of team diversity, focusing mainly on gender and\nnationality, in the context of an agile software development project-based\ncourse. We analyzed 51 teams over three academic years, measuring three\ndifferent Diversity indexes - regarding Gender, Nationality and their\nco-presence - to examine how different aspects of diversity impact the quality\nof team project outcomes.Statistical analysis revealed a moderate,\nstatistically significant correlation between gender diversity and project\nsuccess, aligning with existing literature. Diversity in nationality showed a\nnegative but negligible effect on project results, indicating that promoting\nthese aspects does not harm students' performance. Analyzing their co-presence\nwithin a team, gender and nationality combined had a negative impact, likely\ndue to increased communication barriers and differing cultural norms.This study\nunderscores the importance of considering multiple diversity dimensions and\ntheir interactions in educational settings. Our findings, overall, show that\npromoting diversity in teams does not negatively impact their performance and\nachievement of educational goals."}
{"id": "2509.08524", "categories": ["cs.SE", "cs.AI", "cs.CR"], "pdf": "https://arxiv.org/pdf/2509.08524", "abs": "https://arxiv.org/abs/2509.08524", "authors": ["Felix Mächtle", "Nils Loose", "Jan-Niclas Serr", "Jonas Sander", "Thomas Eisenbarth"], "title": "AutoStub: Genetic Programming-Based Stub Creation for Symbolic Execution", "comment": "2025 HUMIES finalist", "summary": "Symbolic execution is a powerful technique for software testing, but suffers\nfrom limitations when encountering external functions, such as native methods\nor third-party libraries. Existing solutions often require additional context,\nexpensive SMT solvers, or manual intervention to approximate these functions\nthrough symbolic stubs. In this work, we propose a novel approach to\nautomatically generate symbolic stubs for external functions during symbolic\nexecution that leverages Genetic Programming. When the symbolic executor\nencounters an external function, AutoStub generates training data by executing\nthe function on randomly generated inputs and collecting the outputs. Genetic\nProgramming then derives expressions that approximate the behavior of the\nfunction, serving as symbolic stubs. These automatically generated stubs allow\nthe symbolic executor to continue the analysis without manual intervention,\nenabling the exploration of program paths that were previously intractable. We\ndemonstrate that AutoStub can automatically approximate external functions with\nover 90% accuracy for 55% of the functions evaluated, and can infer\nlanguage-specific behaviors that reveal edge cases crucial for software\ntesting."}
{"id": "2509.08083", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.08083", "abs": "https://arxiv.org/abs/2509.08083", "authors": ["Laurie Williams", "Sammy Migues"], "title": "Establishing a Baseline of Software Supply Chain Security Task Adoption by Software Organizations", "comment": null, "summary": "Software supply chain attacks have increased exponentially since 2020. The\nprimary attack vectors for supply chain attacks are through: (1) software\ncomponents; (2) the build infrastructure; and (3) humans (a.k.a software\npractitioners). Software supply chain risk management frameworks provide a list\nof tasks that an organization can adopt to reduce software supply chain risk.\nExhaustively adopting all the tasks of these frameworks is infeasible,\nnecessitating the prioritized adoption of tasks. Software organizations can\nbenefit from being guided in this prioritization by learning what tasks other\nteams have adopted. The goal of this study is to aid software development\norganizations in understanding the adoption of security tasks that reduce\nsoftware supply chain risk through an interview study of software practitioners\nengaged in software supply chain risk management efforts. An interview study\nwas conducted with 61 practitioners at nine software development organizations\nthat have focused efforts on reducing software supply chain risk. The results\nof the interviews indicate that organizations had implemented the most adopted\nsoftware tasks before the focus on software supply chain security. Therefore,\ntheir implementation in organizations is more mature. The tasks that mitigate\nthe novel attack vectors through software components and the build\ninfrastructure are in the early stages of adoption. Adoption of these tasks\nshould be prioritized."}
{"id": "2509.07997", "categories": ["cs.AI", "cs.RO"], "pdf": "https://arxiv.org/pdf/2509.07997", "abs": "https://arxiv.org/abs/2509.07997", "authors": ["Abigail Breitfeld", "Alberto Candela", "Juan Delfa", "Akseli Kangaslahti", "Itai Zilberstein", "Steve Chien", "David Wettergreen"], "title": "Learning-Based Planning for Improving Science Return of Earth Observation Satellites", "comment": "International Symposium on Artificial Intelligence, Robotics and\n  Automation in Space, November 2024", "summary": "Earth observing satellites are powerful tools for collecting scientific\ninformation about our planet, however they have limitations: they cannot easily\ndeviate from their orbital trajectories, their sensors have a limited field of\nview, and pointing and operating these sensors can take a large amount of the\nspacecraft's resources. It is important for these satellites to optimize the\ndata they collect and include only the most important or informative\nmeasurements. Dynamic targeting is an emerging concept in which satellite\nresources and data from a lookahead instrument are used to intelligently\nreconfigure and point a primary instrument. Simulation studies have shown that\ndynamic targeting increases the amount of scientific information gathered\nversus conventional sampling strategies. In this work, we present two different\nlearning-based approaches to dynamic targeting, using reinforcement and\nimitation learning, respectively. These learning methods build on a dynamic\nprogramming solution to plan a sequence of sampling locations. We evaluate our\napproaches against existing heuristic methods for dynamic targeting, showing\nthe benefits of using learning for this application. Imitation learning\nperforms on average 10.0\\% better than the best heuristic method, while\nreinforcement learning performs on average 13.7\\% better. We also show that\nboth learning methods can be trained effectively with relatively small amounts\nof data."}
{"id": "2509.08546", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.08546", "abs": "https://arxiv.org/abs/2509.08546", "authors": ["Yu Zhu", "Jiyuan Ye"], "title": "Beyond the Binary: The System of All-round Evaluation of Research and Its Practices in China", "comment": "STI-ENID 2025 Conference Paer", "summary": "The lack of a macro-level, systematic evaluation theory to guide the\nimplementation of evaluation practices has become a key bottleneck in the\nreform of global research evaluation systems. By reviewing the historical\ndevelopment of research evaluation, this paper highlights the current binary\nopposition between qualitative and quantitative methods in evaluation\npractices. This paper introduces the System of All-round Evaluation of Research\n(SAER), a framework that integrates form, content, and utility evaluations with\nsix key elements. SAER offers a theoretical breakthrough by transcending the\nbinary, providing a comprehensive foundation for global evaluation reforms. The\ncomprehensive system proposes a trinity of three evaluation dimensions,\ncombined with six evaluation elements, which would help academic evaluators and\nresearchers reconcile binary oppositions in evaluation methods. The system\nhighlights the dialectical wisdom and experience embedded in Chinese research\nevaluation theory, offering valuable insights and references for the reform and\nadvancement of global research evaluation systems."}
{"id": "2509.08091", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.08091", "abs": "https://arxiv.org/abs/2509.08091", "authors": ["Jing Chen", "Onat Gungor", "Zhengli Shang", "Tajana Rosing"], "title": "SAGE: Sample-Aware Guarding Engine for Robust Intrusion Detection Against Adversarial Attacks", "comment": "Under review at IEEE TIFS", "summary": "The rapid proliferation of the Internet of Things (IoT) continues to expose\ncritical security vulnerabilities, necessitating the development of efficient\nand robust intrusion detection systems (IDS). Machine learning-based intrusion\ndetection systems (ML-IDS) have significantly improved threat detection\ncapabilities; however, they remain highly susceptible to adversarial attacks.\nWhile numerous defense mechanisms have been proposed to enhance ML-IDS\nresilience, a systematic approach for selecting the most effective defense\nagainst a specific adversarial attack remains absent. To address this\nchallenge, we previously proposed DYNAMITE, a dynamic defense selection\napproach that identifies the most suitable defense against adversarial attacks\nthrough an ML-driven selection mechanism. Building on this foundation, we\npropose SAGE (Sample-Aware Guarding Engine), a substantially improved defense\nalgorithm that integrates active learning with targeted data reduction. It\nemploys an active learning mechanism to selectively identify the most\ninformative input samples and their corresponding optimal defense labels, which\nare then used to train a second-level learner responsible for selecting the\nmost effective defense. This targeted sampling improves computational\nefficiency, exposes the model to diverse adversarial strategies during\ntraining, and enhances robustness, stability, and generalizability. As a\nresult, SAGE demonstrates strong predictive performance across multiple\nintrusion detection datasets, achieving an average F1-score improvement of 201%\nover the state-of-the-art defenses. Notably, SAGE narrows the performance gap\nto the Oracle to just 3.8%, while reducing computational overhead by up to 29x."}
{"id": "2509.08088", "categories": ["cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2509.08088", "abs": "https://arxiv.org/abs/2509.08088", "authors": ["Linyao Chen", "Zimian Peng", "Yingxuan Yang", "Yikun Wang", "Wenzheng Tom Tang", "Hiroki H. Kobayashi", "Weinan Zhang"], "title": "EnvX: Agentize Everything with Agentic AI", "comment": null, "summary": "The widespread availability of open-source repositories has led to a vast\ncollection of reusable software components, yet their utilization remains\nmanual, error-prone, and disconnected. Developers must navigate documentation,\nunderstand APIs, and write integration code, creating significant barriers to\nefficient software reuse. To address this, we present EnvX, a framework that\nleverages Agentic AI to agentize GitHub repositories, transforming them into\nintelligent, autonomous agents capable of natural language interaction and\ninter-agent collaboration. Unlike existing approaches that treat repositories\nas static code resources, EnvX reimagines them as active agents through a\nthree-phase process: (1) TODO-guided environment initialization, which sets up\nthe necessary dependencies, data, and validation datasets; (2) human-aligned\nagentic automation, allowing repository-specific agents to autonomously perform\nreal-world tasks; and (3) Agent-to-Agent (A2A) protocol, enabling multiple\nagents to collaborate. By combining large language model capabilities with\nstructured tool integration, EnvX automates not just code generation, but the\nentire process of understanding, initializing, and operationalizing repository\nfunctionality. We evaluate EnvX on the GitTaskBench benchmark, using 18\nrepositories across domains such as image processing, speech recognition,\ndocument analysis, and video manipulation. Our results show that EnvX achieves\na 74.07% execution completion rate and 51.85% task pass rate, outperforming\nexisting frameworks. Case studies further demonstrate EnvX's ability to enable\nmulti-repository collaboration via the A2A protocol. This work marks a shift\nfrom treating repositories as passive code resources to intelligent,\ninteractive agents, fostering greater accessibility and collaboration within\nthe open-source ecosystem."}
{"id": "2509.08667", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.08667", "abs": "https://arxiv.org/abs/2509.08667", "authors": ["Amirali Rayegan", "Tim Menzies"], "title": "Minimal Data, Maximum Clarity: A Heuristic for Explaining Optimization", "comment": null, "summary": "Efficient, interpretable optimization is a critical but underexplored\nchallenge in software engineering, where practitioners routinely face vast\nconfiguration spaces and costly, error-prone labeling processes. This paper\nintroduces EZR, a novel and modular framework for multi-objective optimization\nthat unifies active sampling, learning, and explanation within a single,\nlightweight pipeline. Departing from conventional wisdom, our Maximum Clarity\nHeuristic demonstrates that using less (but more informative) data can yield\noptimization models that are both effective and deeply understandable. EZR\nemploys an active learning strategy based on Naive Bayes sampling to\nefficiently identify high-quality configurations with a fraction of the labels\nrequired by fully supervised approaches. It then distills optimization logic\ninto concise decision trees, offering transparent, actionable explanations for\nboth global and local decision-making. Extensive experiments across 60\nreal-world datasets establish that EZR reliably achieves over 90% of the\nbest-known optimization performance in most cases, while providing clear,\ncohort-based rationales that surpass standard attribution-based explainable AI\n(XAI) methods (LIME, SHAP, BreakDown) in clarity and utility. These results\nendorse \"less but better\"; it is both possible and often preferable to use\nfewer (but more informative) examples to generate label-efficient optimization\nand explanations in software systems. To support transparency and\nreproducibility, all code and experimental materials are publicly available at\nhttps://github.com/amiiralii/Minimal-Data-Maximum-Clarity."}
{"id": "2509.08200", "categories": ["cs.CR", "cs.AI", "cs.CY"], "pdf": "https://arxiv.org/pdf/2509.08200", "abs": "https://arxiv.org/abs/2509.08200", "authors": ["William Cashman", "Chasen Milner", "Michael Houle", "Michael Jones", "Hayden Jananthan", "Jeremy Kepner", "Peter Michaleas", "Alex Pentland"], "title": "Accelerating AI Development with Cyber Arenas", "comment": "2 pages, 1 figure, 7 references, accepted to IEEE HPEC 2025", "summary": "AI development requires high fidelity testing environments to effectively\ntransition from the laboratory to operations. The flexibility offered by cyber\narenas presents a novel opportunity to test new artificial intelligence (AI)\ncapabilities with users. Cyber arenas are designed to expose end-users to\nreal-world situations and must rapidly incorporate evolving capabilities to\nmeet their core objectives. To explore this concept the MIT/IEEE/Amazon Graph\nChallenge Anonymized Network Sensor was deployed in a cyber arena during a\nNational Guard exercise."}
{"id": "2509.08151", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.08151", "abs": "https://arxiv.org/abs/2509.08151", "authors": ["Botao Zhu", "Jeslyn Wang", "Dusit Niyato", "Xianbin Wang"], "title": "Trust Semantics Distillation for Collaborator Selection via Memory-Augmented Agentic AI", "comment": null, "summary": "Accurate trustworthiness evaluation of potential collaborating devices is\nessential for the effective execution of complex computing tasks. This\nevaluation process involves collecting diverse trust-related data from\npotential collaborators, including historical performance and available\nresources, for collaborator selection. However, when each task owner\nindependently assesses all collaborators' trustworthiness, frequent data\nexchange, complex reasoning, and dynamic situation changes can result in\nsignificant overhead and deteriorated trust evaluation. To overcome these\nchallenges, we propose a task-specific trust semantics distillation (2TSD)\nmodel based on a large AI model (LAM)-driven teacher-student agent\narchitecture. The teacher agent is deployed on a server with powerful\ncomputational capabilities and an augmented memory module dedicated to\nmultidimensional trust-related data collection, task-specific trust semantics\nextraction, and task-collaborator matching analysis. Upon receiving\ntask-specific requests from device-side student agents, the teacher agent\ntransfers the trust semantics of potential collaborators to the student agents,\nenabling rapid and accurate collaborator selection. Experimental results\ndemonstrate that the proposed 2TSD model can reduce collaborator evaluation\ntime, decrease device resource consumption, and improve the accuracy of\ncollaborator selection."}
{"id": "2509.08724", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.08724", "abs": "https://arxiv.org/abs/2509.08724", "authors": ["Junhao Wang", "Daoguang Zan", "Shulin Xin", "Siyao Liu", "Yurong Wu", "Kai Shen"], "title": "SWE-Mirror: Scaling Issue-Resolving Datasets by Mirroring Issues Across Repositories", "comment": null, "summary": "Creating large-scale verifiable training datasets for issue-resolving tasks\nis a critical yet notoriously difficult challenge. Existing methods on\nautomating the Gym environment setup process for real-world issues suffer from\nlow success rates and high overhead. Meanwhile, synthesizing new tasks within\nexisting Gym environments leaves the vast pool of authentic, human-reported\nproblems untapped. To maximize the utilization of existing Gym environments and\nalso the rich data of issue-resolving history on GitHub, we introduce\nSWE-Mirror, a pipeline that distills a real-world issue's semantic essence,\nmirrors it into another repository with a configured Gym environment, and\nre-animates it as a verifiable issue-resolving task. SWE-Mirror reuses existing\nGym environments along with the vast pool of issue-resolving history hosted on\nGitHub to construct a large-scale dataset of mirrored authentic and verifiable\ntasks. Applying SWE-Mirror to 40 repositories across 4 languages, we have\ncurated a dataset with 60,671 issue-resolving tasks and demonstrated the value\nof our dataset by training and evaluating coding agents at various scale.\nPost-training experiments show that models trained with the dataset exhibit\nimprovements in issue-resolving capabilities. Furthermore, by extending the\ndataset size to over 12,000 high-quality trajectories, we established a new\nstate-of-the-art (SOTA) among Qwen2.5-Coder-Instruct based LLMs on the\nOpenHands agent framework, which increases the resolve rate on\nSWE-Bench-Verified by +21.8% for the 7B model and +46.0% for the 32B model and\nvalidates the effectiveness of our approach."}
{"id": "2509.08204", "categories": ["cs.CR", "cs.SE"], "pdf": "https://arxiv.org/pdf/2509.08204", "abs": "https://arxiv.org/abs/2509.08204", "authors": ["Behnaz Hassanshahi", "Trong Nhan Mai", "Benjamin Selwyn Smith", "Nicholas Allen"], "title": "Unlocking Reproducibility: Automating re-Build Process for Open-Source Software", "comment": null, "summary": "Software ecosystems like Maven Central play a crucial role in modern software\nsupply chains by providing repositories for libraries and build plugins.\nHowever, the separation between binaries and their corresponding source code in\nMaven Central presents a significant challenge, particularly when it comes to\nlinking binaries back to their original build environment. This lack of\ntransparency poses security risks, as approximately 84% of the top 1200\ncommonly used artifacts are not built using a transparent CI/CD pipeline.\nConsequently, users must place a significant amount of trust not only in the\nsource code but also in the environment in which these artifacts are built.\n  Rebuilding software artifacts from source provides a robust solution to\nimprove supply chain security. This approach allows for a deeper review of\ncode, verification of binary-source equivalence, and control over dependencies.\nHowever, challenges arise due to variations in build environments, such as JDK\nversions and build commands, which can lead to build failures. Additionally,\nensuring that all dependencies are rebuilt from source across large and complex\ndependency graphs further complicates the process. In this paper, we introduce\nan extension to Macaron, an industry-grade open-source supply chain security\nframework, to automate the rebuilding of Maven artifacts from source. Our\napproach improves upon existing tools, by offering better performance in source\ncode detection and automating the extraction of build specifications from\nGitHub Actions workflows. We also present a comprehensive root cause analysis\nof build failures in Java projects and propose a scalable solution to automate\nthe rebuilding of artifacts, ultimately enhancing security and transparency in\nthe open-source supply chain."}
{"id": "2509.08222", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.08222", "abs": "https://arxiv.org/abs/2509.08222", "authors": ["Minjong Yoo", "Jinwoo Jang", "Wei-jin Park", "Honguk Woo"], "title": "Exploratory Retrieval-Augmented Planning For Continual Embodied Instruction Following", "comment": "21 pages. NeurIPS 2024", "summary": "This study presents an Exploratory Retrieval-Augmented Planning (ExRAP)\nframework, designed to tackle continual instruction following tasks of embodied\nagents in dynamic, non-stationary environments. The framework enhances Large\nLanguage Models' (LLMs) embodied reasoning capabilities by efficiently\nexploring the physical environment and establishing the environmental context\nmemory, thereby effectively grounding the task planning process in time-varying\nenvironment contexts. In ExRAP, given multiple continual instruction following\ntasks, each instruction is decomposed into queries on the environmental context\nmemory and task executions conditioned on the query results. To efficiently\nhandle these multiple tasks that are performed continuously and simultaneously,\nwe implement an exploration-integrated task planning scheme by incorporating\nthe {information-based exploration} into the LLM-based planning process.\nCombined with memory-augmented query evaluation, this integrated scheme not\nonly allows for a better balance between the validity of the environmental\ncontext memory and the load of environment exploration, but also improves\noverall task performance. Furthermore, we devise a {temporal consistency\nrefinement} scheme for query evaluation to address the inherent decay of\nknowledge in the memory. Through experiments with VirtualHome, ALFRED, and\nCARLA, our approach demonstrates robustness against a variety of embodied\ninstruction following scenarios involving different instruction scales and\ntypes, and non-stationarity degrees, and it consistently outperforms other\nstate-of-the-art LLM-based task planning approaches in terms of both goal\nsuccess rate and execution efficiency."}
{"id": "2509.08808", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.08808", "abs": "https://arxiv.org/abs/2509.08808", "authors": ["Mohammad Saqib Hasan", "Sayontan Ghosh", "Dhruv Verma", "Geoff Kuenning", "Erez Zadok", "Scott A. Smolka", "Niranjan Balasubramanian"], "title": "Handling Open-Vocabulary Constructs in Formalizing Specifications: Retrieval-Augmented Parsing with Expert Knowledge", "comment": "Accepted to COLM 2024", "summary": "We study the problem of Open-Vocabulary Constructs(OVCs) -- ones not known\nbeforehand -- in the context of converting natural language (NL) specifications\ninto formal languages (e.g., temporal logic or code). Models fare poorly on\nOVCs due to a lack of necessary knowledge a priori. In such situations, a\ndomain expert can provide correct constructs at inference time based on their\npreferences or domain knowledge. Our goal is to effectively reuse this\ninference-time, expert-provided knowledge for future parses without retraining\nthe model. We present dynamic knowledge-augmented parsing(DKAP), where in\naddition to the input sentence, the model receives (dynamically growing) expert\nknowledge as a key-value lexicon that associates NL phrases with correct OVC\nconstructs. We propose ROLex, a retrieval-augmented parsing approach that uses\nthis lexicon. A retriever and a generator are trained to find and use the\nkey-value store to produce the correct parse. A key challenge lies in curating\ndata for this retrieval-augmented parser. We utilize synthetic data generation\nand the data augmentation techniques on annotated (NL sentence, FL statement)\npairs to train the augmented parser. To improve training effectiveness, we\npropose multiple strategies to teach models to focus on the relevant subset of\nretrieved knowledge. Finally, we introduce a new evaluation paradigm modeled\nafter the DKAP problem and simulate the scenario across three formalization\ntasks (NL2LTL, NL2Code, and NL2CMD). Our evaluations show that DKAP is a\ndifficult challenge, and ROLex helps improve the performance of baseline models\nby using dynamic expert knowledge effectively."}
{"id": "2509.08248", "categories": ["cs.CR", "cs.NI"], "pdf": "https://arxiv.org/pdf/2509.08248", "abs": "https://arxiv.org/abs/2509.08248", "authors": ["Arin Upadhyay"], "title": "EFPIX: A zero-trust encrypted flood protocol", "comment": null, "summary": "We propose a flood-based relay communication protocol that achieves\nend-to-end encryption, plausible deniability for users, and untraceable\nmessages. It is resistant to changes in topology and infrastructure failures.\nIt is also designed to hide metadata, such as sender and receiver, from those\nnot involved."}
{"id": "2509.08282", "categories": ["cs.AI", "cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2509.08282", "abs": "https://arxiv.org/abs/2509.08282", "authors": ["Seonghyeon Go"], "title": "Real-world Music Plagiarism Detection With Music Segment Transcription System", "comment": "Accepted in APSIPA 2025 but not published yet(will be published in 2\n  month..), Arxiv preprint ready for references in future-works", "summary": "As a result of continuous advances in Music Information Retrieval (MIR)\ntechnology, generating and distributing music has become more diverse and\naccessible. In this context, interest in music intellectual property protection\nis increasing to safeguard individual music copyrights. In this work, we\npropose a system for detecting music plagiarism by combining various MIR\ntechnologies. We developed a music segment transcription system that extracts\nmusically meaningful segments from audio recordings to detect plagiarism across\ndifferent musical formats. With this system, we compute similarity scores based\non multiple musical features that can be evaluated through comprehensive\nmusical analysis. Our approach demonstrated promising results in music\nplagiarism detection experiments, and the proposed method can be applied to\nreal-world music scenarios. We also collected a Similar Music Pair (SMP)\ndataset for musical similarity research using real-world cases. The dataset are\npublicly available."}
{"id": "2509.08204", "categories": ["cs.CR", "cs.SE"], "pdf": "https://arxiv.org/pdf/2509.08204", "abs": "https://arxiv.org/abs/2509.08204", "authors": ["Behnaz Hassanshahi", "Trong Nhan Mai", "Benjamin Selwyn Smith", "Nicholas Allen"], "title": "Unlocking Reproducibility: Automating re-Build Process for Open-Source Software", "comment": null, "summary": "Software ecosystems like Maven Central play a crucial role in modern software\nsupply chains by providing repositories for libraries and build plugins.\nHowever, the separation between binaries and their corresponding source code in\nMaven Central presents a significant challenge, particularly when it comes to\nlinking binaries back to their original build environment. This lack of\ntransparency poses security risks, as approximately 84% of the top 1200\ncommonly used artifacts are not built using a transparent CI/CD pipeline.\nConsequently, users must place a significant amount of trust not only in the\nsource code but also in the environment in which these artifacts are built.\n  Rebuilding software artifacts from source provides a robust solution to\nimprove supply chain security. This approach allows for a deeper review of\ncode, verification of binary-source equivalence, and control over dependencies.\nHowever, challenges arise due to variations in build environments, such as JDK\nversions and build commands, which can lead to build failures. Additionally,\nensuring that all dependencies are rebuilt from source across large and complex\ndependency graphs further complicates the process. In this paper, we introduce\nan extension to Macaron, an industry-grade open-source supply chain security\nframework, to automate the rebuilding of Maven artifacts from source. Our\napproach improves upon existing tools, by offering better performance in source\ncode detection and automating the extraction of build specifications from\nGitHub Actions workflows. We also present a comprehensive root cause analysis\nof build failures in Java projects and propose a scalable solution to automate\nthe rebuilding of artifacts, ultimately enhancing security and transparency in\nthe open-source supply chain."}
{"id": "2509.08364", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.08364", "abs": "https://arxiv.org/abs/2509.08364", "authors": ["Aduma Rishith", "Aditya Kulkarni", "Tamal Das", "Vivek Balachandran"], "title": "Overcoming DNSSEC Islands of Security: A TLS and IP-Based Certificate Solution", "comment": null, "summary": "The Domain Name System (DNS) serves as the backbone of the Internet,\nprimarily translating domain names to IP addresses. Over time, various\nenhancements have been introduced to strengthen the integrity of DNS. Among\nthese, DNSSEC stands out as a leading cryptographic solution. It protects\nagainst attacks (such as DNS spoofing) by establishing a chain of trust\nthroughout the DNS nameserver hierarchy. However, DNSSEC's effectiveness is\ncompromised when there is a break in this chain, resulting in \"Islands of\nSecurity\", where domains can authenticate locally but not across hierarchical\nlevels, leading to a loss of trust and validation between them. Leading\napproaches to addressing these issues were centralized, with a single authority\nmaintaining some kind of bulletin board. This approach requires significantly\nmore infrastructure and places excessive trust in the entity responsible for\nmanaging it properly. In this paper, we propose a decentralized approach to\naddressing gaps in DNSSEC's chain of trust, commonly referred to as \"Islands of\nSecurity\". We leverage TLS and IP-based certificates to enable end-to-end\nauthentication between hierarchical levels, eliminating the need for uniform\nDNSSEC deployment across every level of the DNS hierarchy. This approach\nenhances the overall integrity of DNSSEC, while reducing dependence on\nregistrars for maintaining signature records to verify the child nameserver's\nauthenticity. By offering a more flexible and efficient solution, our method\nstrengthens DNS security and streamlines deployment across diverse\nenvironments."}
{"id": "2509.08312", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.08312", "abs": "https://arxiv.org/abs/2509.08312", "authors": ["Binghan Wu", "Shoufeng Wang", "Yunxin Liu", "Ya-Qin Zhang", "Joseph Sifakis", "Ye Ouyang"], "title": "Leveraging AI Agents for Autonomous Networks: A Reference Architecture and Empirical Studies", "comment": "7 pages, 5 figures. This manuscript is a preprint", "summary": "The evolution toward Level 4 (L4) Autonomous Networks (AN) represents a\nstrategic inflection point in telecommunications, where networks must transcend\nreactive automation to achieve genuine cognitive capabilities--fulfilling TM\nForum's vision of self-configuring, self-healing, and self-optimizing systems\nthat deliver zero-wait, zero-touch, and zero-fault services. This work bridges\nthe gap between architectural theory and operational reality by implementing\nJoseph Sifakis's AN Agent reference architecture in a functional cognitive\nsystem, deploying coordinated proactive-reactive runtimes driven by hybrid\nknowledge representation. Through an empirical case study of a Radio Access\nNetwork (RAN) Link Adaptation (LA) Agent, we validate this framework's\ntransformative potential: demonstrating sub-10 ms real-time control in 5G NR\nsub-6 GHz while achieving 6% higher downlink throughput than Outer Loop Link\nAdaptation (OLLA) algorithms and 67% Block Error Rate (BLER) reduction for\nultra-reliable services through dynamic Modulation and Coding Scheme (MCS)\noptimization. These improvements confirm the architecture's viability in\novercoming traditional autonomy barriers and advancing critical L4-enabling\ncapabilities toward next-generation objectives."}
{"id": "2509.08375", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.08375", "abs": "https://arxiv.org/abs/2509.08375", "authors": ["Duddu Hriday", "Aditya Kulkarni", "Vivek Balachandran", "Tamal Das"], "title": "Phish-Blitz: Advancing Phishing Detection with Comprehensive Webpage Resource Collection and Visual Integrity Preservation", "comment": null, "summary": "Phishing attacks are increasingly prevalent, with adversaries creating\ndeceptive webpages to steal sensitive information. Despite advancements in\nmachine learning and deep learning for phishing detection, attackers constantly\ndevelop new tactics to bypass detection models. As a result, phishing webpages\ncontinue to reach users, particularly those unable to recognize phishing\nindicators. To improve detection accuracy, models must be trained on large\ndatasets containing both phishing and legitimate webpages, including URLs,\nwebpage content, screenshots, and logos. However, existing tools struggle to\ncollect the required resources, especially given the short lifespan of phishing\nwebpages, limiting dataset comprehensiveness. In response, we introduce\nPhish-Blitz, a tool that downloads phishing and legitimate webpages along with\ntheir associated resources, such as screenshots. Unlike existing tools,\nPhish-Blitz captures live webpage screenshots and updates resource file paths\nto maintain the original visual integrity of the webpage. We provide a dataset\ncontaining 8,809 legitimate and 5,000 phishing webpages, including all\nassociated resources. Our dataset and tool are publicly available on GitHub,\ncontributing to the research community by offering a more complete dataset for\nphishing detection."}
{"id": "2509.08380", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.08380", "abs": "https://arxiv.org/abs/2509.08380", "authors": ["Prathamesh Vasudeo Naik", "Naresh Kumar Dintakurthi", "Zhanghao Hu", "Yue Wang", "Robby Qiu"], "title": "Co-Investigator AI: The Rise of Agentic AI for Smarter, Trustworthy AML Compliance Narratives", "comment": null, "summary": "Generating regulatorily compliant Suspicious Activity Report (SAR) remains a\nhigh-cost, low-scalability bottleneck in Anti-Money Laundering (AML) workflows.\nWhile large language models (LLMs) offer promising fluency, they suffer from\nfactual hallucination, limited crime typology alignment, and poor\nexplainability -- posing unacceptable risks in compliance-critical domains.\nThis paper introduces Co-Investigator AI, an agentic framework optimized to\nproduce Suspicious Activity Reports (SARs) significantly faster and with\ngreater accuracy than traditional methods. Drawing inspiration from recent\nadvances in autonomous agent architectures, such as the AI Co-Scientist, our\napproach integrates specialized agents for planning, crime type detection,\nexternal intelligence gathering, and compliance validation. The system features\ndynamic memory management, an AI-Privacy Guard layer for sensitive data\nhandling, and a real-time validation agent employing the Agent-as-a-Judge\nparadigm to ensure continuous narrative quality assurance. Human investigators\nremain firmly in the loop, empowered to review and refine drafts in a\ncollaborative workflow that blends AI efficiency with domain expertise. We\ndemonstrate the versatility of Co-Investigator AI across a range of complex\nfinancial crime scenarios, highlighting its ability to streamline SAR drafting,\nalign narratives with regulatory expectations, and enable compliance teams to\nfocus on higher-order analytical work. This approach marks the beginning of a\nnew era in compliance reporting -- bringing the transformative benefits of AI\nagents to the core of regulatory processes and paving the way for scalable,\nreliable, and transparent SAR generation."}
{"id": "2509.08399", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.08399", "abs": "https://arxiv.org/abs/2509.08399", "authors": ["Abdou-Essamad Jabri", "Mostafa Azizi", "Cyril Drocourt", "Gil Utard"], "title": "MIoT-Driven Comparison of Open Blockchain Platforms", "comment": null, "summary": "Being propelled by the fourth industrial revolution (Industry 4.0), IoT\ndevices and solutions are well adopted everywhere, ranging from home\napplications to industrial use, crossing through transportation, healthcare,\nenergy, and so on. This wide use of IoT has not gone unnoticed, hackers are\ntracking the weakness of such a technology and threatening them continuously.\nTheir security at various levels has become an important concern of\nprofessionals and researchers. This issue takes more risk, especially with the\nIoT variants, IIoT (Industrial IoT) and MIoT (Medical IoT). Many existing\nsecurity solutions are adapted and proposed for addressing IoT security. In\nthis paper, we are interested in exploring blockchain technology and we make a\ncomparison of three free Blockchain platforms towards their applicability for\nMIoT context, namely Ethereum, Hyperledger Fabric and Corda. In general,\nBlockchain technology provides a decentralized, autonomous, trustless, and\ndistributed environment. It is challenging to find a Blockchain platform that\nfits the MIoT context and performs well in terms of security. The retained\nplatform should be deployed smartly to avoid its practical drawbacks related to\nenergy-consuming and excessive computing."}
{"id": "2509.08500", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.08500", "abs": "https://arxiv.org/abs/2509.08500", "authors": ["Kechen Jiao", "Zhirui Fang", "Jiahao Liu", "Bei Li", "Qifan Wang", "Xinyu Liu", "Junhao Ruan", "Zhongjian Qiao", "Yifan Zhu", "Yaxin Xu", "Jingang Wang", "Xiu Li"], "title": "TCPO: Thought-Centric Preference Optimization for Effective Embodied Decision-making", "comment": null, "summary": "Using effective generalization capabilities of vision language models (VLMs)\nin context-specific dynamic tasks for embodied artificial intelligence remains\na significant challenge. Although supervised fine-tuned models can better align\nwith the real physical world, they still exhibit sluggish responses and\nhallucination issues in dynamically changing environments, necessitating\nfurther alignment. Existing post-SFT methods, reliant on reinforcement learning\nand chain-of-thought (CoT) approaches, are constrained by sparse rewards and\naction-only optimization, resulting in low sample efficiency, poor consistency,\nand model degradation. To address these issues, this paper proposes\nThought-Centric Preference Optimization (TCPO) for effective embodied\ndecision-making. Specifically, TCPO introduces a stepwise preference-based\noptimization approach, transforming sparse reward signals into richer step\nsample pairs. It emphasizes the alignment of the model's intermediate reasoning\nprocess, mitigating the problem of model degradation. Moreover, by\nincorporating Action Policy Consistency Constraint (APC), it further imposes\nconsistency constraints on the model output. Experiments in the ALFWorld\nenvironment demonstrate an average success rate of 26.67%, achieving a 6%\nimprovement over RL4VLM and validating the effectiveness of our approach in\nmitigating model degradation after fine-tuning. These results highlight the\npotential of integrating preference-based learning techniques with CoT\nprocesses to enhance the decision-making capabilities of vision-language models\nin embodied agents."}
{"id": "2509.08402", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.08402", "abs": "https://arxiv.org/abs/2509.08402", "authors": ["Abdou-Essamad Jabri", "C. Drocourt", "Mostafa Azizi", "Gil Utard"], "title": "Leveraging Blockchain and Proxy Re-Encryption to secure Medical IoT Records", "comment": null, "summary": "The integration of the Internet of Things (IoT) in healthcare has\nrevolutionized patient monitoring and data collection, allowing real-time\ntracking of vital signs, remote diagnostics, and automated medical responses.\nHowever, the transmission and storage of sensitive medical data introduce\nsignificant security and privacy challenges. To address these concerns,\nblockchain technology provides a decentralized and immutable ledger that\nensures data integrity, , and transparency. Unlike public blockchains, private\nblockchains are permissioned; the access is granted only to authorized\nparticipants; they are more suitable for handling confidential healthcare data.\nAlthough blockchain ensures security and trust, it lacks built-in mechanisms to\nsupport flexible and controlled data sharing; This is where Proxy Re-Encryption\n(PRE) comes into play. PRE is a cryptographic technique that allows encrypted\ndata to be re-encrypted for a new recipient without exposing it to\nintermediaries. We propose an architecture integrating private blockchain and\nPRE to enable secure, traceable, and privacy-preserving data sharing in\nIoT-based healthcare systems. Blockchain guarantees tamper proof\nrecord-keeping, while PRE enables fine-grained access control, allowing medical\nprofessionals to securely share patient data without compromising\nconfidentiality. This combination creates a robust security framework that\nenhances trust and efficiency in digital healthcare ecosystems."}
{"id": "2509.08593", "categories": ["cs.AI", "stat.ML", "90C05, 68T27", "I.2.3; F.4.1"], "pdf": "https://arxiv.org/pdf/2509.08593", "abs": "https://arxiv.org/abs/2509.08593", "authors": ["Andrés Corrada-Emmanuel"], "title": "No-Knowledge Alarms for Misaligned LLMs-as-Judges", "comment": "7 pages, 1 figure", "summary": "If we use LLMs as judges to evaluate the complex decisions of other LLMs, who\nor what monitors the judges? Infinite monitoring chains are inevitable whenever\nwe do not know the ground truth of the decisions by experts and we do not want\nto trust them. One way to ameliorate our evaluation uncertainty is to exploit\nthe use of logical consistency between disagreeing experts. By observing how\nLLM judges agree and disagree while grading other LLMs, we can compute the only\npossible evaluations of their grading ability. For example, if two LLM judges\ndisagree on which tasks a third one completed correctly, they cannot both be\n100\\% correct in their judgments. This logic can be formalized as a Linear\nProgramming problem in the space of integer response counts for any finite\ntest. We use it here to develop no-knowledge alarms for misaligned LLM judges.\nThe alarms can detect, with no false positives, that at least one member or\nmore of an ensemble of judges are violating a user specified grading ability\nrequirement."}
{"id": "2509.08424", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.08424", "abs": "https://arxiv.org/abs/2509.08424", "authors": ["Aditya Kulkarni", "Vivek Balachandran", "Tamal Das"], "title": "Phishing Webpage Detection: Unveiling the Threat Landscape and Investigating Detection Techniques", "comment": null, "summary": "In the realm of cybersecurity, phishing stands as a prevalent cyber attack,\nwhere attackers employ various tactics to deceive users into gathering their\nsensitive information, potentially leading to identity theft or financial gain.\nResearchers have been actively working on advancing phishing webpage detection\napproaches to detect new phishing URLs, bolstering user protection.\nNonetheless, the ever-evolving strategies employed by attackers, aimed at\ncircumventing existing detection approaches and tools, present an ongoing\nchallenge to the research community. This survey presents a systematic\ncategorization of diverse phishing webpage detection approaches, encompassing\nURL-based, webpage content-based, and visual techniques. Through a\ncomprehensive review of these approaches and an in-depth analysis of existing\nliterature, our study underscores current research gaps in phishing webpage\ndetection. Furthermore, we suggest potential solutions to address some of these\ngaps, contributing valuable insights to the ongoing efforts to combat phishing\nattacks."}
{"id": "2509.08682", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.08682", "abs": "https://arxiv.org/abs/2509.08682", "authors": ["Guoqing Ma", "Jia Zhu", "Hanghui Guo", "Weijie Shi", "Jiawei Shen", "Jingjiang Liu", "Yidan Liang"], "title": "Automatic Failure Attribution and Critical Step Prediction Method for Multi-Agent Systems Based on Causal Inference", "comment": null, "summary": "Multi-agent systems (MAS) are critical for automating complex tasks, yet\ntheir practical deployment is severely hampered by the challenge of failure\nattribution. Current diagnostic tools, which rely on statistical correlations,\nare fundamentally inadequate; on challenging benchmarks like Who\\&When,\nstate-of-the-art methods achieve less than 15\\% accuracy in locating the\nroot-cause step of a failure. To address this critical gap, we introduce the\nfirst failure attribution framework for MAS grounded in multi-granularity\ncausal inference. Our approach makes two key technical contributions: (1) a\nperformance causal inversion principle, which correctly models performance\ndependencies by reversing the data flow in execution logs, combined with\nShapley values to accurately assign agent-level blame; (2) a novel causal\ndiscovery algorithm, CDC-MAS, that robustly identifies critical failure steps\nby tackling the non-stationary nature of MAS interaction data. The framework's\nattribution results directly fuel an automated optimization loop, generating\ntargeted suggestions whose efficacy is validated via counterfactual\nsimulations. Evaluations on the Who\\&When and TRAIL benchmarks demonstrate a\nsignificant leap in performance. Our method achieves up to 36.2\\% step-level\naccuracy. Crucially, the generated optimizations boost overall task success\nrates by an average of 22.4\\%. This work provides a principled and effective\nsolution for debugging complex agent interactions, paving the way for more\nreliable and interpretable multi-agent systems."}
{"id": "2509.08449", "categories": ["cs.CR", "cs.AI", "cs.DC"], "pdf": "https://arxiv.org/pdf/2509.08449", "abs": "https://arxiv.org/abs/2509.08449", "authors": ["Charuka Herath", "Yogachandran Rahulamathavan", "Varuna De Silva", "Sangarapillai Lambotharan"], "title": "DSFL: A Dual-Server Byzantine-Resilient Federated Learning Framework via Group-Based Secure Aggregation", "comment": null, "summary": "Federated Learning (FL) enables decentralized model training without sharing\nraw data, offering strong privacy guarantees. However, existing FL protocols\nstruggle to defend against Byzantine participants, maintain model utility under\nnon-independent and identically distributed (non-IID) data, and remain\nlightweight for edge devices. Prior work either assumes trusted hardware, uses\nexpensive cryptographic tools, or fails to address privacy and robustness\nsimultaneously. We propose DSFL, a Dual-Server Byzantine-Resilient Federated\nLearning framework that addresses these limitations using a group-based secure\naggregation approach. Unlike LSFL, which assumes non-colluding semi-honest\nservers, DSFL removes this dependency by revealing a key vulnerability: privacy\nleakage through client-server collusion. DSFL introduces three key innovations:\n(1) a dual-server secure aggregation protocol that protects updates without\nencryption or key exchange, (2) a group-wise credit-based filtering mechanism\nto isolate Byzantine clients based on deviation scores, and (3) a dynamic\nreward-penalty system for enforcing fair participation. DSFL is evaluated on\nMNIST, CIFAR-10, and CIFAR-100 under up to 30 percent Byzantine participants in\nboth IID and non-IID settings. It consistently outperforms existing baselines,\nincluding LSFL, homomorphic encryption methods, and differential privacy\napproaches. For example, DSFL achieves 97.15 percent accuracy on CIFAR-10 and\n68.60 percent on CIFAR-100, while FedAvg drops to 9.39 percent under similar\nthreats. DSFL remains lightweight, requiring only 55.9 ms runtime and 1088 KB\ncommunication per round."}
{"id": "2509.08705", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.08705", "abs": "https://arxiv.org/abs/2509.08705", "authors": ["Shalima Binta Manir", "Tim Oates"], "title": "One Model, Two Minds: A Context-Gated Graph Learner that Recreates Human Biases", "comment": "9 pages, 7 figures, 2 tables", "summary": "We introduce a novel Theory of Mind (ToM) framework inspired by dual-process\ntheories from cognitive science, integrating a fast, habitual graph-based\nreasoning system (System 1), implemented via graph convolutional networks\n(GCNs), and a slower, context-sensitive meta-adaptive learning system (System\n2), driven by meta-learning techniques. Our model dynamically balances\nintuitive and deliberative reasoning through a learned context gate mechanism.\nWe validate our architecture on canonical false-belief tasks and systematically\nexplore its capacity to replicate hallmark cognitive biases associated with\ndual-process theory, including anchoring, cognitive-load fatigue, framing\neffects, and priming effects. Experimental results demonstrate that our\ndual-process approach closely mirrors human adaptive behavior, achieves robust\ngeneralization to unseen contexts, and elucidates cognitive mechanisms\nunderlying reasoning biases. This work bridges artificial intelligence and\ncognitive theory, paving the way for AI systems exhibiting nuanced, human-like\nsocial cognition and adaptive decision-making capabilities."}
{"id": "2509.08485", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.08485", "abs": "https://arxiv.org/abs/2509.08485", "authors": ["Priyanka Rushikesh Chaudhary", "Rajib Ranjan Maiti"], "title": "Flow-Based Detection and Identification of Zero-Day IoT Cameras", "comment": null, "summary": "The majority of consumer IoT devices lack mechanisms for administrators to\nmonitor and control them, hindering tailored security policies. A key challenge\nis identifying whether a new device, especially a streaming IoT camera, has\njoined the network. We present zCamInspector, a system for identifying known\nIoT cameras with supervised classifiers (zCamClassifier) and detecting zero-day\ncameras with one-class classifiers (zCamDetector). We analyzed ~40GB of traffic\nacross three datasets: Set I (six commercial IoT cameras), Set II (five\nopen-source IoT cameras, ~1.5GB), and Set III (four conferencing and two\nvideo-sharing applications as non-IoT traffic). From each, 62 flow-based\nfeatures were extracted using CICFlowmeter. zCamInspector employs seven\nsupervised models (ET, DT, RF, KNN, XGB, LKSVM, GNB) and four one-class models\n(OCSVM, SGDOCSVM, IF, DeepSVDD). Results show that XGB identifies IoT cameras\nwith >99% accuracy and false negatives as low as 0.3%, outperforming\nstate-of-the-art methods. For zero-day detection, accuracies reached 93.20%\n(OCSVM), 96.55% (SGDOCSVM), 78.65% (IF), and 92.16% (DeepSVDD). When all\ndevices were treated as zero-day, DeepSVDD performed best with mean\ntraining/testing accuracies of 96.03%/74.51%. zCamInspector also achieved >95%\naccuracy for specific devices, such as Spy Clock cameras, demonstrating its\nrobustness for identifying and detecting zero-day IoT cameras in diverse\nnetwork environments."}
{"id": "2509.08713", "categories": ["cs.AI", "cs.DL"], "pdf": "https://arxiv.org/pdf/2509.08713", "abs": "https://arxiv.org/abs/2509.08713", "authors": ["Ziming Luo", "Atoosa Kasirzadeh", "Nihar B. Shah"], "title": "The More You Automate, the Less You See: Hidden Pitfalls of AI Scientist Systems", "comment": null, "summary": "AI scientist systems, capable of autonomously executing the full research\nworkflow from hypothesis generation and experimentation to paper writing, hold\nsignificant potential for accelerating scientific discovery. However, the\ninternal workflow of these systems have not been closely examined. This lack of\nscrutiny poses a risk of introducing flaws that could undermine the integrity,\nreliability, and trustworthiness of their research outputs. In this paper, we\nidentify four potential failure modes in contemporary AI scientist systems:\ninappropriate benchmark selection, data leakage, metric misuse, and post-hoc\nselection bias. To examine these risks, we design controlled experiments that\nisolate each failure mode while addressing challenges unique to evaluating AI\nscientist systems. Our assessment of two prominent open-source AI scientist\nsystems reveals the presence of several failures, across a spectrum of\nseverity, which can be easily overlooked in practice. Finally, we demonstrate\nthat access to trace logs and code from the full automated workflow enables far\nmore effective detection of such failures than examining the final paper alone.\nWe thus recommend journals and conferences evaluating AI-generated research to\nmandate submission of these artifacts alongside the paper to ensure\ntransparency, accountability, and reproducibility."}
{"id": "2509.08493", "categories": ["cs.CR", "cs.AI", "K.6.5; I.2.7"], "pdf": "https://arxiv.org/pdf/2509.08493", "abs": "https://arxiv.org/abs/2509.08493", "authors": ["Hossein Siadati", "Haadi Jafarian", "Sima Jafarikhah"], "title": "Send to which account? Evaluation of an LLM-based Scambaiting System", "comment": null, "summary": "Scammers are increasingly harnessing generative AI(GenAI) technologies to\nproduce convincing phishing content at scale, amplifying financial fraud and\nundermining public trust. While conventional defenses, such as detection\nalgorithms, user training, and reactive takedown efforts remain important, they\noften fall short in dismantling the infrastructure scammers depend on,\nincluding mule bank accounts and cryptocurrency wallets. To bridge this gap, a\nproactive and emerging strategy involves using conversational honeypots to\nengage scammers and extract actionable threat intelligence. This paper presents\nthe first large-scale, real-world evaluation of a scambaiting system powered by\nlarge language models (LLMs). Over a five-month deployment, the system\ninitiated over 2,600 engagements with actual scammers, resulting in a dataset\nof more than 18,700 messages. It achieved an Information Disclosure Rate (IDR)\nof approximately 32%, successfully extracting sensitive financial information\nsuch as mule accounts. Additionally, the system maintained a Human Acceptance\nRate (HAR) of around 70%, indicating strong alignment between LLM-generated\nresponses and human operator preferences. Alongside these successes, our\nanalysis reveals key operational challenges. In particular, the system\nstruggled with engagement takeoff: only 48.7% of scammers responded to the\ninitial seed message sent by defenders. These findings highlight the need for\nfurther refinement and provide actionable insights for advancing the design of\nautomated scambaiting systems."}
{"id": "2509.08785", "categories": ["cs.AI", "cs.MA", "stat.ML"], "pdf": "https://arxiv.org/pdf/2509.08785", "abs": "https://arxiv.org/abs/2509.08785", "authors": ["Anup Tuladhar", "Araz Minhas", "Adam Kirton", "Eli Kinney-Lang"], "title": "Narrative-Guided Reinforcement Learning: A Platform for Studying Language Model Influence on Decision Making", "comment": "Extended Abstract for RLDM 2025", "summary": "We present a preliminary experimental platform that explores how narrative\nelements might shape AI decision-making by combining reinforcement learning\n(RL) with language model reasoning. While AI systems can now both make\ndecisions and engage in narrative reasoning, these capabilities have mostly\nbeen studied separately. Our platform attempts to bridge this gap using a\ndual-system architecture to examine how narrative frameworks could influence\nreward-based learning. The system comprises a reinforcement learning policy\nthat suggests actions based on past experience, and a language model that\nprocesses these suggestions through different narrative frameworks to guide\ndecisions. This setup enables initial experimentation with narrative elements\nwhile maintaining consistent environment and reward structures. We implement\nthis architecture in a configurable gridworld environment, where agents receive\nboth policy suggestions and information about their surroundings. The\nplatform's modular design facilitates controlled testing of environmental\ncomplexity, narrative parameters, and the interaction between reinforcement\nlearning and narrative-based decisions. Our logging system captures basic\ndecision metrics, from RL policy values to language model reasoning to action\nselection patterns. While preliminary, this implementation provides a\nfoundation for studying how different narrative frameworks might affect\nreward-based decisions and exploring potential interactions between\noptimization-based learning and symbolic reasoning in AI systems."}
{"id": "2509.08646", "categories": ["cs.CR", "cs.AI", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2509.08646", "abs": "https://arxiv.org/abs/2509.08646", "authors": ["Ron F. Del Rosario", "Klaudia Krawiecka", "Christian Schroeder de Witt"], "title": "Architecting Resilient LLM Agents: A Guide to Secure Plan-then-Execute Implementations", "comment": null, "summary": "As Large Language Model (LLM) agents become increasingly capable of\nautomating complex, multi-step tasks, the need for robust, secure, and\npredictable architectural patterns is paramount. This paper provides a\ncomprehensive guide to the ``Plan-then-Execute'' (P-t-E) pattern, an agentic\ndesign that separates strategic planning from tactical execution. We explore\nthe foundational principles of P-t-E, detailing its core components - the\nPlanner and the Executor - and its architectural advantages in predictability,\ncost-efficiency, and reasoning quality over reactive patterns like ReAct\n(Reason + Act). A central focus is placed on the security implications of this\ndesign, particularly its inherent resilience to indirect prompt injection\nattacks by establishing control-flow integrity. We argue that while P-t-E\nprovides a strong foundation, a defense-in-depth strategy is necessary, and we\ndetail essential complementary controls such as the Principle of Least\nPrivilege, task-scoped tool access, and sandboxed code execution. To make these\nprinciples actionable, this guide provides detailed implementation blueprints\nand working code references for three leading agentic frameworks: LangChain\n(via LangGraph), CrewAI, and AutoGen. Each framework's approach to implementing\nthe P-t-E pattern is analyzed, highlighting unique features like LangGraph's\nstateful graphs for re-planning, CrewAI's declarative tool scoping for\nsecurity, and AutoGen's built-in Docker sandboxing. Finally, we discuss\nadvanced patterns, including dynamic re-planning loops, parallel execution with\nDirected Acyclic Graphs (DAGs), and the critical role of Human-in-the-Loop\n(HITL) verification, to offer a complete strategic blueprint for architects,\ndevelopers, and security engineers aiming to build production-grade, resilient,\nand trustworthy LLM agents."}
{"id": "2509.08200", "categories": ["cs.CR", "cs.AI", "cs.CY"], "pdf": "https://arxiv.org/pdf/2509.08200", "abs": "https://arxiv.org/abs/2509.08200", "authors": ["William Cashman", "Chasen Milner", "Michael Houle", "Michael Jones", "Hayden Jananthan", "Jeremy Kepner", "Peter Michaleas", "Alex Pentland"], "title": "Accelerating AI Development with Cyber Arenas", "comment": "2 pages, 1 figure, 7 references, accepted to IEEE HPEC 2025", "summary": "AI development requires high fidelity testing environments to effectively\ntransition from the laboratory to operations. The flexibility offered by cyber\narenas presents a novel opportunity to test new artificial intelligence (AI)\ncapabilities with users. Cyber arenas are designed to expose end-users to\nreal-world situations and must rapidly incorporate evolving capabilities to\nmeet their core objectives. To explore this concept the MIT/IEEE/Amazon Graph\nChallenge Anonymized Network Sensor was deployed in a cyber arena during a\nNational Guard exercise."}
{"id": "2509.08704", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.08704", "abs": "https://arxiv.org/abs/2509.08704", "authors": ["Zihang Xiang", "Tianhao Wang", "Hanshen Xiao", "Yuan Tian", "Di Wang"], "title": "Tight Privacy Audit in One Run", "comment": null, "summary": "In this paper, we study the problem of privacy audit in one run and show that\nour method achieves tight audit results for various differentially private\nprotocols. This includes obtaining tight results for auditing\n$(\\varepsilon,\\delta)$-DP algorithms where all previous work fails to achieve\nin any parameter setups. We first formulate a framework for privacy audit\n\\textit{in one run} with refinement compared with previous work. Then, based on\nmodeling privacy by the $f$-DP formulation, we study the implications of our\nframework to obtain a theoretically justified lower bound for privacy audit. In\nthe experiment, we compare with previous work and show that our audit method\noutperforms the rest in auditing various differentially private algorithms. We\nalso provide experiments that give contrasting conclusions to previous work on\nthe parameter settings for privacy audits in one run."}
{"id": "2509.08449", "categories": ["cs.CR", "cs.AI", "cs.DC"], "pdf": "https://arxiv.org/pdf/2509.08449", "abs": "https://arxiv.org/abs/2509.08449", "authors": ["Charuka Herath", "Yogachandran Rahulamathavan", "Varuna De Silva", "Sangarapillai Lambotharan"], "title": "DSFL: A Dual-Server Byzantine-Resilient Federated Learning Framework via Group-Based Secure Aggregation", "comment": null, "summary": "Federated Learning (FL) enables decentralized model training without sharing\nraw data, offering strong privacy guarantees. However, existing FL protocols\nstruggle to defend against Byzantine participants, maintain model utility under\nnon-independent and identically distributed (non-IID) data, and remain\nlightweight for edge devices. Prior work either assumes trusted hardware, uses\nexpensive cryptographic tools, or fails to address privacy and robustness\nsimultaneously. We propose DSFL, a Dual-Server Byzantine-Resilient Federated\nLearning framework that addresses these limitations using a group-based secure\naggregation approach. Unlike LSFL, which assumes non-colluding semi-honest\nservers, DSFL removes this dependency by revealing a key vulnerability: privacy\nleakage through client-server collusion. DSFL introduces three key innovations:\n(1) a dual-server secure aggregation protocol that protects updates without\nencryption or key exchange, (2) a group-wise credit-based filtering mechanism\nto isolate Byzantine clients based on deviation scores, and (3) a dynamic\nreward-penalty system for enforcing fair participation. DSFL is evaluated on\nMNIST, CIFAR-10, and CIFAR-100 under up to 30 percent Byzantine participants in\nboth IID and non-IID settings. It consistently outperforms existing baselines,\nincluding LSFL, homomorphic encryption methods, and differential privacy\napproaches. For example, DSFL achieves 97.15 percent accuracy on CIFAR-10 and\n68.60 percent on CIFAR-100, while FedAvg drops to 9.39 percent under similar\nthreats. DSFL remains lightweight, requiring only 55.9 ms runtime and 1088 KB\ncommunication per round."}
{"id": "2509.08720", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.08720", "abs": "https://arxiv.org/abs/2509.08720", "authors": ["Ruiyao Liu", "Chenxi Qiu"], "title": "PAnDA: Rethinking Metric Differential Privacy Optimization at Scale with Anchor-Based Approximation", "comment": "In Proceedings of the 32nd ACM Conference on Computer and\n  Communications Security (CCS 2025)", "summary": "Metric Differential Privacy (mDP) extends the local differential privacy\n(LDP) framework to metric spaces, enabling more nuanced privacy protection for\ndata such as geo-locations. However, existing mDP optimization methods,\nparticularly those based on linear programming (LP), face scalability\nchallenges due to the quadratic growth in decision variables. In this paper, we\npropose Perturbation via Anchor-based Distributed Approximation (PAnDA), a\nscalable two-phase framework for optimizing metric differential privacy (mDP).\nTo reduce computational overhead, PAnDA allows each user to select a small set\nof anchor records, enabling the server to solve a compact linear program over a\nreduced domain. We introduce three anchor selection strategies, exponential\ndecay (PAnDA-e), power-law decay (PAnDA-p), and logistic decay (PAnDA-l), and\nestablish theoretical guarantees under a relaxed privacy notion called\nprobabilistic mDP (PmDP). Experiments on real-world geo-location datasets\ndemonstrate that PAnDA scales to secret domains with up to 5,000 records, two\ntimes larger than prior LP-based methods, while providing theoretical\nguarantees for both privacy and utility."}
{"id": "2509.08493", "categories": ["cs.CR", "cs.AI", "K.6.5; I.2.7"], "pdf": "https://arxiv.org/pdf/2509.08493", "abs": "https://arxiv.org/abs/2509.08493", "authors": ["Hossein Siadati", "Haadi Jafarian", "Sima Jafarikhah"], "title": "Send to which account? Evaluation of an LLM-based Scambaiting System", "comment": null, "summary": "Scammers are increasingly harnessing generative AI(GenAI) technologies to\nproduce convincing phishing content at scale, amplifying financial fraud and\nundermining public trust. While conventional defenses, such as detection\nalgorithms, user training, and reactive takedown efforts remain important, they\noften fall short in dismantling the infrastructure scammers depend on,\nincluding mule bank accounts and cryptocurrency wallets. To bridge this gap, a\nproactive and emerging strategy involves using conversational honeypots to\nengage scammers and extract actionable threat intelligence. This paper presents\nthe first large-scale, real-world evaluation of a scambaiting system powered by\nlarge language models (LLMs). Over a five-month deployment, the system\ninitiated over 2,600 engagements with actual scammers, resulting in a dataset\nof more than 18,700 messages. It achieved an Information Disclosure Rate (IDR)\nof approximately 32%, successfully extracting sensitive financial information\nsuch as mule accounts. Additionally, the system maintained a Human Acceptance\nRate (HAR) of around 70%, indicating strong alignment between LLM-generated\nresponses and human operator preferences. Alongside these successes, our\nanalysis reveals key operational challenges. In particular, the system\nstruggled with engagement takeoff: only 48.7% of scammers responded to the\ninitial seed message sent by defenders. These findings highlight the need for\nfurther refinement and provide actionable insights for advancing the design of\nautomated scambaiting systems."}
{"id": "2509.08722", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.08722", "abs": "https://arxiv.org/abs/2509.08722", "authors": ["Zihan Liu", "Xiaohu Wang", "Chao Lin", "Minghui Xu", "Debiao He", "Xinyi Huang"], "title": "SilentLedger: Privacy-Preserving Auditing for Blockchains with Complete Non-Interactivity", "comment": null, "summary": "Privacy-preserving blockchain systems are essential for protecting\ntransaction data, yet they must also provide auditability that enables auditors\nto recover participant identities and transaction amounts when warranted.\nExisting designs often compromise the independence of auditing and\ntransactions, introducing extra interactions that undermine usability and\nscalability. Moreover, many auditable solutions depend on auditors serving as\nvalidators or recording nodes, which introduces risks to both data security and\nsystem reliability.\n  To overcome these challenges, we propose SilentLedger, a privacy-preserving\ntransaction system with auditing and complete non-interactivity. To support\npublic verification of authorization, we introduce a renewable anonymous\ncertificate scheme with formal semantics and a rigorous security model.\nSilentLedger further employs traceable transaction mechanisms constructed from\nestablished cryptographic primitives, enabling users to transact without\ninteraction while allowing auditors to audit solely from on-chain data. We\nformally prove security properties including authenticity, anonymity,\nconfidentiality, and soundness, provide a concrete instantiation, and evaluate\nperformance under a standard 2-2 transaction model. Our implementation and\nbenchmarks demonstrate that SilentLedger achieves superior performance compared\nwith state-of-the-art solutions."}
{"id": "2509.08524", "categories": ["cs.SE", "cs.AI", "cs.CR"], "pdf": "https://arxiv.org/pdf/2509.08524", "abs": "https://arxiv.org/abs/2509.08524", "authors": ["Felix Mächtle", "Nils Loose", "Jan-Niclas Serr", "Jonas Sander", "Thomas Eisenbarth"], "title": "AutoStub: Genetic Programming-Based Stub Creation for Symbolic Execution", "comment": "2025 HUMIES finalist", "summary": "Symbolic execution is a powerful technique for software testing, but suffers\nfrom limitations when encountering external functions, such as native methods\nor third-party libraries. Existing solutions often require additional context,\nexpensive SMT solvers, or manual intervention to approximate these functions\nthrough symbolic stubs. In this work, we propose a novel approach to\nautomatically generate symbolic stubs for external functions during symbolic\nexecution that leverages Genetic Programming. When the symbolic executor\nencounters an external function, AutoStub generates training data by executing\nthe function on randomly generated inputs and collecting the outputs. Genetic\nProgramming then derives expressions that approximate the behavior of the\nfunction, serving as symbolic stubs. These automatically generated stubs allow\nthe symbolic executor to continue the analysis without manual intervention,\nenabling the exploration of program paths that were previously intractable. We\ndemonstrate that AutoStub can automatically approximate external functions with\nover 90% accuracy for 55% of the functions evaluated, and can infer\nlanguage-specific behaviors that reveal edge cases crucial for software\ntesting."}
{"id": "2509.08727", "categories": ["cs.CR", "cs.AR", "cs.PL"], "pdf": "https://arxiv.org/pdf/2509.08727", "abs": "https://arxiv.org/abs/2509.08727", "authors": ["Shixin Song", "Tingzhen Dong", "Kosi Nwabueze", "Julian Zanders", "Andres Erbsen", "Adam Chlipala", "Mengjia Yan"], "title": "Securing Cryptographic Software via Typed Assembly Language (Extended Version)", "comment": null, "summary": "Authors of cryptographic software are well aware that their code should not\nleak secrets through its timing behavior, and, until 2018, they believed that\nfollowing industry-standard constant-time coding guidelines was sufficient.\nHowever, the revelation of the Spectre family of speculative execution attacks\ninjected new complexities.\n  To block speculative attacks, prior work has proposed annotating the\nprogram's source code to mark secret data, with hardware using this information\nto decide when to speculate (i.e., when only public values are involved) or not\n(when secrets are in play). While these solutions are able to track secret\ninformation stored on the heap, they suffer from limitations that prevent them\nfrom correctly tracking secrets on the stack, at a cost in performance.\n  This paper introduces SecSep, a transformation framework that rewrites\nassembly programs so that they partition secret and public data on the stack.\nBy moving from the source-code level to assembly rewriting, SecSep is able to\naddress limitations of prior work. The key challenge in performing this\nassembly rewriting stems from the loss of semantic information through the\nlengthy compilation process. The key innovation of our methodology is a new\nvariant of typed assembly language (TAL), Octal, which allows us to address\nthis challenge. Assembly rewriting is driven by compile-time inference within\nOctal. We apply our technique to cryptographic programs and demonstrate that it\nenables secure speculation efficiently, incurring a low average overhead of\n$1.2\\%$."}
{"id": "2509.08646", "categories": ["cs.CR", "cs.AI", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2509.08646", "abs": "https://arxiv.org/abs/2509.08646", "authors": ["Ron F. Del Rosario", "Klaudia Krawiecka", "Christian Schroeder de Witt"], "title": "Architecting Resilient LLM Agents: A Guide to Secure Plan-then-Execute Implementations", "comment": null, "summary": "As Large Language Model (LLM) agents become increasingly capable of\nautomating complex, multi-step tasks, the need for robust, secure, and\npredictable architectural patterns is paramount. This paper provides a\ncomprehensive guide to the ``Plan-then-Execute'' (P-t-E) pattern, an agentic\ndesign that separates strategic planning from tactical execution. We explore\nthe foundational principles of P-t-E, detailing its core components - the\nPlanner and the Executor - and its architectural advantages in predictability,\ncost-efficiency, and reasoning quality over reactive patterns like ReAct\n(Reason + Act). A central focus is placed on the security implications of this\ndesign, particularly its inherent resilience to indirect prompt injection\nattacks by establishing control-flow integrity. We argue that while P-t-E\nprovides a strong foundation, a defense-in-depth strategy is necessary, and we\ndetail essential complementary controls such as the Principle of Least\nPrivilege, task-scoped tool access, and sandboxed code execution. To make these\nprinciples actionable, this guide provides detailed implementation blueprints\nand working code references for three leading agentic frameworks: LangChain\n(via LangGraph), CrewAI, and AutoGen. Each framework's approach to implementing\nthe P-t-E pattern is analyzed, highlighting unique features like LangGraph's\nstateful graphs for re-planning, CrewAI's declarative tool scoping for\nsecurity, and AutoGen's built-in Docker sandboxing. Finally, we discuss\nadvanced patterns, including dynamic re-planning loops, parallel execution with\nDirected Acyclic Graphs (DAGs), and the critical role of Human-in-the-Loop\n(HITL) verification, to offer a complete strategic blueprint for architects,\ndevelopers, and security engineers aiming to build production-grade, resilient,\nand trustworthy LLM agents."}
{"id": "2509.08740", "categories": ["cs.CR", "cs.DB"], "pdf": "https://arxiv.org/pdf/2509.08740", "abs": "https://arxiv.org/abs/2509.08740", "authors": ["Sam Kumar", "Samyukta Yagati", "Conor Power", "David E. Culler", "Raluca Ada Popa"], "title": "Membrane: A Cryptographic Access Control System for Data Lakes", "comment": "28 pages, 25 figures", "summary": "Organizations use data lakes to store and analyze sensitive data. But hackers\nmay compromise data lake storage to bypass access controls and access sensitive\ndata. To address this, we propose Membrane, a system that (1) cryptographically\nenforces data-dependent access control views over a data lake, (2) without\nrestricting the analytical queries data scientists can run. We observe that\ndata lakes, unlike DBMSes, disaggregate computation and storage into separate\ntrust domains, making at-rest encryption sufficient to defend against remote\nattackers targeting data lake storage, even when running analytical queries in\nplaintext. This leads to a new system design for Membrane that combines\nencryption at rest with SQL-aware encryption. Using block ciphers, a fast\nsymmetric-key primitive with hardware acceleration in CPUs, we develop a new\nSQL-aware encryption protocol well-suited to at-rest encryption. Membrane adds\noverhead only at the start of an interactive session due to decrypting views,\ndelaying the first query result by up to $\\approx 20\\times$; subsequent queries\nprocess decrypted data in plaintext, resulting in low amortized overhead."}
{"id": "2509.08746", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.08746", "abs": "https://arxiv.org/abs/2509.08746", "authors": ["Ryan McGaughey", "Jesus Martinez del Rincon", "Ihsen Alouani"], "title": "Stealth by Conformity: Evading Robust Aggregation through Adaptive Poisoning", "comment": "16 pages, 12 figures", "summary": "Federated Learning (FL) is a distributed learning paradigm designed to\naddress privacy concerns. However, FL is vulnerable to poisoning attacks, where\nByzantine clients compromise the integrity of the global model by submitting\nmalicious updates. Robust aggregation methods have been widely adopted to\nmitigate such threats, relying on the core assumption that malicious updates\nare inherently out-of-distribution and can therefore be identified and excluded\nbefore aggregating client updates. In this paper, we challenge this underlying\nassumption by showing that a model can be poisoned while keeping malicious\nupdates within the main distribution. We propose Chameleon Poisoning (CHAMP),\nan adaptive and evasive poisoning strategy that exploits side-channel feedback\nfrom the aggregation process to guide the attack. Specifically, the adversary\ncontinuously infers whether its malicious contribution has been incorporated\ninto the global model and adapts accordingly. This enables a dynamic adjustment\nof the local loss function, balancing a malicious component with a camouflaging\ncomponent, thereby increasing the effectiveness of the poisoning while evading\nrobust aggregation defenses. CHAMP enables more effective and evasive\npoisoning, highlighting a fundamental limitation of existing robust aggregation\ndefenses and underscoring the need for new strategies to secure federated\nlearning against sophisticated adversaries. Our approach is evaluated in two\ndatasets reaching an average increase of 47.07% in attack success rate against\nnine robust aggregation defenses."}
{"id": "2509.08747", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.08747", "abs": "https://arxiv.org/abs/2509.08747", "authors": ["Wei Guo", "Maura Pintor", "Ambra Demontis", "Battista Biggio"], "title": "Silent Until Sparse: Backdoor Attacks on Semi-Structured Sparsity", "comment": null, "summary": "In the deployment phase, semi-structured sparsity accelerates the execution\nof deep neural networks on modern GPUs via sparse matrix multiplication. In\nthis paper, targeting the semi-structured sparsity, we introduce a Silent Until\nSparse (SUS) backdoor attack, where the released full model remains silent\n(benign), but becomes a backdoored model after sparsification. The attack\noperates in two phases: (i) in the backdoor training phase, the backdoor\nfunctionality is injected into specific weights that will be retained during\nthe pruning process; (ii) in the backdoor hiding phase, the malicious behavior\nis concealed by fine-tuning elements that will be pruned away. This dual-phase\napproach ensures that the attack remains undetectable in the released model,\nbut activates properly once the model is pruned with the semi-structured\nsparsity. Through extensive experiments, we show that our attack successfully\nthreatens the semi-structured sparsity algorithms from both NVIDIA and PyTorch.\nOur empirical results show that, regardless of model architecture, the attack\nsuccess rate of the released model remains below 10% prior to sparsification\nbut exceeds 99% afterward. Moreover, we demonstrate that SUS attack is robust\nagainst state-of-the-art backdoor defenses and finetuning, highlighting a\ncritical vulnerability in current model compression and deployment pipelines."}
{"id": "2509.08748", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.08748", "abs": "https://arxiv.org/abs/2509.08748", "authors": ["Wei Guo", "Maura Pintor", "Ambra Demontis", "Battista Biggio"], "title": "Prototype-Guided Robust Learning against Backdoor Attacks", "comment": null, "summary": "Backdoor attacks poison the training data to embed a backdoor in the model,\ncausing it to behave normally on legitimate inputs but maliciously when\nspecific trigger signals appear. Training a benign model from a dataset\npoisoned by backdoor attacks is challenging. Existing works rely on various\nassumptions and can only defend against backdoor attacks with specific trigger\nsignals, high poisoning ratios, or when the defender possesses a large,\nuntainted, validation dataset. In this paper, we propose a defense called\nPrototype-Guided Robust Learning (PGRL), which overcomes all the aforementioned\nlimitations, being robust against diverse backdoor attacks. Leveraging a tiny\nset of benign samples, PGRL generates prototype vectors to guide the training\nprocess. We compare our PGRL with 8 existing defenses, showing that it achieves\nsuperior robustness. We also demonstrate that PGRL generalizes well across\nvarious architectures, datasets, and advanced attacks. Finally, to evaluate our\nPGRL in the worst-case scenario, we perform an adaptive attack, where the\nattackers fully know the details of the defense."}
{"id": "2509.08758", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.08758", "abs": "https://arxiv.org/abs/2509.08758", "authors": ["Markus Scherer", "Jeppe Fredsgaard Blaabjerg", "Alexander Sjösten", "Matteo Maffei"], "title": "Wanilla: Sound Noninterference Analysis for WebAssembly", "comment": null, "summary": "WebAssembly (Wasm) is rapidly gaining popularity as a distribution format for\nsoftware components embedded in various security-critical domains.\nUnfortunately, despite its prudent design, WebAssembly's primary use case as a\ncompilation target for memory-unsafe languages leaves some possibilities for\nmemory corruption. Independently of that, Wasm is an inherently interesting\ntarget for information flow analysis due to its interfacing role.\n  Both the information flows between a Wasm module and its embedding context,\nas well as the memory integrity within a module, can be described by the\nhyperproperty noninterference. So far, no sound, fully static noninterference\nanalysis for Wasm has been presented, but sound reachability analyses were.\nThis work presents a novel and general approach to lift reachability analyses\nto noninterference by tracking taints on values and using value-sensitive,\nrelational reasoning to remove them when appropriate. We implement this\napproach in Wanilla, the first automatic, sound, and fully static\nnoninterference analysis for WebAssembly, and demonstrate its performance and\nprecision by verifying memory integrity and other noninterference properties\nwith several synthetic and real-world benchmarks."}
{"id": "2509.08804", "categories": ["cs.CR", "cs.PL", "D.2.5; F.4.1"], "pdf": "https://arxiv.org/pdf/2509.08804", "abs": "https://arxiv.org/abs/2509.08804", "authors": ["Bishnu Bhusal", "Rohit Chadha", "A. Prasad Sistla", "Mahesh Viswanathan"], "title": "Approximate Algorithms for Verifying Differential Privacy with Gaussian Distributions", "comment": "An extended abstract appears in CCS 2025", "summary": "The verification of differential privacy algorithms that employ Gaussian\ndistributions is little understood. This paper tackles the challenge of\nverifying such programs by introducing a novel approach to approximating\nprobability distributions of loop-free programs that sample from both discrete\nand continuous distributions with computable probability density functions,\nincluding Gaussian and Laplace. We establish that verifying\n$(\\epsilon,\\delta)$-differential privacy for these programs is \\emph{almost\ndecidable}, meaning the problem is decidable for all values of $\\delta$ except\nthose in a finite set. Our verification algorithm is based on computing\nprobabilities to any desired precision by combining integral approximations,\nand tail probability bounds. The proposed methods are implemented in the tool,\nDipApprox, using the FLINT library for high-precision integral computations,\nand incorporate optimizations to enhance scalability. We validate {\\ourtool} on\nfundamental privacy-preserving algorithms, such as Gaussian variants of the\nSparse Vector Technique and Noisy Max, demonstrating its effectiveness in both\nconfirming privacy guarantees and detecting violations."}
{"id": "2509.08524", "categories": ["cs.SE", "cs.AI", "cs.CR"], "pdf": "https://arxiv.org/pdf/2509.08524", "abs": "https://arxiv.org/abs/2509.08524", "authors": ["Felix Mächtle", "Nils Loose", "Jan-Niclas Serr", "Jonas Sander", "Thomas Eisenbarth"], "title": "AutoStub: Genetic Programming-Based Stub Creation for Symbolic Execution", "comment": "2025 HUMIES finalist", "summary": "Symbolic execution is a powerful technique for software testing, but suffers\nfrom limitations when encountering external functions, such as native methods\nor third-party libraries. Existing solutions often require additional context,\nexpensive SMT solvers, or manual intervention to approximate these functions\nthrough symbolic stubs. In this work, we propose a novel approach to\nautomatically generate symbolic stubs for external functions during symbolic\nexecution that leverages Genetic Programming. When the symbolic executor\nencounters an external function, AutoStub generates training data by executing\nthe function on randomly generated inputs and collecting the outputs. Genetic\nProgramming then derives expressions that approximate the behavior of the\nfunction, serving as symbolic stubs. These automatically generated stubs allow\nthe symbolic executor to continue the analysis without manual intervention,\nenabling the exploration of program paths that were previously intractable. We\ndemonstrate that AutoStub can automatically approximate external functions with\nover 90% accuracy for 55% of the functions evaluated, and can infer\nlanguage-specific behaviors that reveal edge cases crucial for software\ntesting."}
