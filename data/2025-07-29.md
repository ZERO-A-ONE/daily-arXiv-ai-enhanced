<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 31]
- [cs.CR](#cs.CR) [Total: 19]
- [cs.AI](#cs.AI) [Total: 51]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [AccessGuru: Leveraging LLMs to Detect and Correct Web Accessibility Violations in HTML Code](https://arxiv.org/abs/2507.19549)
*Nadeen Fathallah,Daniel Hernández,Steffen Staab*

Main category: cs.SE

TL;DR: 论文提出了一种自动检测和纠正网页可访问性违规的方法AccessGuru，结合现有工具和大型语言模型，显著提升了效果。


<details>
  <summary>Details</summary>
Motivation: 大多数网页不符合可访问性指南，限制了多样化用户的使用，手动修复成本高，因此需要自动化解决方案。

Method: 提出新的分类法（语法、语义、布局），结合现有工具和LLMs，通过策略性提示纠正违规。

Result: AccessGuru在基准测试中平均违规分数降低84%，优于之前方法的50%。

Conclusion: AccessGuru有效解决了网页可访问性自动修复的挑战，为未来研究提供了新方向。

Abstract: The vast majority of Web pages fail to comply with established Web
accessibility guidelines, excluding a range of users with diverse abilities
from interacting with their content. Making Web pages accessible to all users
requires dedicated expertise and additional manual efforts from Web page
providers. To lower their efforts and promote inclusiveness, we aim to
automatically detect and correct Web accessibility violations in HTML code.
While previous work has made progress in detecting certain types of
accessibility violations, the problem of automatically detecting and correcting
accessibility violations remains an open challenge that we address. We
introduce a novel taxonomy classifying Web accessibility violations into three
key categories - Syntactic, Semantic, and Layout. This taxonomy provides a
structured foundation for developing our detection and correction method and
redefining evaluation metrics. We propose a novel method, AccessGuru, which
combines existing accessibility testing tools and Large Language Models (LLMs)
to detect violations and applies taxonomy-driven prompting strategies to
correct all three categories. To evaluate these capabilities, we develop a
benchmark of real-world Web accessibility violations. Our benchmark quantifies
syntactic and layout compliance and judges semantic accuracy through
comparative analysis with human expert corrections. Evaluation against our
benchmark shows that AccessGuru achieves up to 84% average violation score
decrease, significantly outperforming prior methods that achieve at most 50%.

</details>


### [2] [LastMerge: A language-agnostic structured tool for code integration](https://arxiv.org/abs/2507.19687)
*Joao Pedro Duarte,Paulo Borba,Guilherme Cavalcanti*

Main category: cs.SE

TL;DR: LastMerge是一种通用的结构化合并工具，通过简化配置接口提高合并准确性，实验表明其性能与语言特定工具相当。


<details>
  <summary>Details</summary>
Motivation: 解决现有结构化合并工具语言特定、成本高的问题，以推广结构化合并的广泛应用。

Method: 提出LastMerge，并通过实验比较其与语言特定工具（jDime、Spork）及其通用版本（Mergiraf）的性能和准确性。

Result: 通用工具在准确性上与语言特定工具无显著差异，LastMerge减少15%误报，Mergiraf减少42%漏报，且运行时性能相当。

Conclusion: 通用结构化合并工具可替代语言特定工具，推动结构化合并的工业应用。

Abstract: Unstructured line-based merge tools are widely used in practice. Structured
AST-based merge tools show significantly improved merge accuracy, but are
rarely used in practice because they are language specific and costly,
consequently not being available for many programming languages. To improve
merge accuracy for a wide range of languages, we propose LastMerge, a generic
structured merge tool that can be configured through a thin interface that
significantly reduces the effort of supporting structured merge. To understand
the impact that generic structured merge might have on merge accuracy and
performance, we run an experiment with four structured merge tools: two Java
specific tools, jDime and Spork, and their generic counterparts, respectively
LastMerge and Mergiraf. Using each tool, we replay merge scenarios from a
significant dataset, and collect data on runtime, behavioral divergences, and
merge accuracy. Our results show no evidence that generic structured merge
significantly impacts merge accuracy. Although we observe a difference rate of
approximately 10% between the Java specific tools and their generic
counterparts, most of the differences stem from implementation details and
could be avoided. We find that LastMerge reports 15% fewer false positives than
jDime while Mergiraf misses 42% fewer false negatives than Spork. Both generic
tools exhibit comparable runtime performance to the state of the art language
specific implementations. These results suggest that generic structured merge
tools can effectively replace language-specific ones, paving the way for
broader adoption of structured merge in industry.

</details>


### [3] [Refactoring $\neq$ Bug-Inducing: Improving Defect Prediction with Code Change Tactics Analysis](https://arxiv.org/abs/2507.19714)
*Feifei Niu,Junqian Shao,Christoph Mayr-Dorn,Liguo Huang,Wesley K. G. Assunção,Chuanyi Li,Jidong Ge,Alexander Egyed*

Main category: cs.SE

TL;DR: 论文研究了代码重构及其传播对即时缺陷预测（JIT-DP）的影响，提出Code chAnge Tactics（CAT）分析方法，显著提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有JIT-DP研究忽视代码重构及其传播，导致模型学习和评估存在偏差。

Method: 提出CAT分析方法，分类代码重构及其传播，并整合重构信息改进六种基线方法。

Result: 忽略重构信息会降低模型性能（F1-score下降18.6%-37.3%），而整合重构信息后，召回率和F1-score分别提升43.2%和32.5%。

Conclusion: 重构信息对JIT-DP至关重要，CAT方法在软件维护中具有广泛应用潜力。

Abstract: Just-in-time defect prediction (JIT-DP) aims to predict the likelihood of
code changes resulting in software defects at an early stage. Although code
change metrics and semantic features have enhanced prediction accuracy, prior
research has largely ignored code refactoring during both the evaluation and
methodology phases, despite its prevalence. Refactoring and its propagation
often tangle with bug-fixing and bug-inducing changes within the same commit
and statement. Neglecting refactoring can introduce bias into the learning and
evaluation of JIT-DP models. To address this gap, we investigate the impact of
refactoring and its propagation on six state-of-the-art JIT-DP approaches. We
propose Code chAnge Tactics (CAT) analysis to categorize code refactoring and
its propagation, which improves labeling accuracy in the JIT-Defects4J dataset
by 13.7%. Our experiments reveal that failing to consider refactoring
information in the dataset can diminish the performance of models, particularly
semantic-based models, by 18.6% and 37.3% in F1-score. Additionally, we propose
integrating refactoring information to enhance six baseline approaches,
resulting in overall improvements in recall and F1-score, with increases of up
to 43.2% and 32.5%, respectively. Our research underscores the importance of
incorporating refactoring information in the methodology and evaluation of
JIT-DP. Furthermore, our CAT has broad applicability in analyzing refactoring
and its propagation for software maintenance.

</details>


### [4] [Clean Code In Practice: Challenges and Opportunities](https://arxiv.org/abs/2507.19721)
*Dapeng Yan,Wenjie Yang,Kui Liu,Zhiming Liu,Zhikuang Cai*

Main category: cs.SE

TL;DR: 论文探讨了软件可靠性、安全性和安全性之间的相互作用，提出了一个结合安全性和安全性的威胁估计框架，并提供了改进可靠性预测模型的实用指南。


<details>
  <summary>Details</summary>
Motivation: 现代软件系统的复杂性要求更深入地理解可靠性指标与安全和安全性问题的交互作用。

Method: 分析了行业中的关键指标和测量技术，提出了一个威胁估计框架，并结合安全和安全性考虑。

Result: 研究发现，将可靠性指标与安全和安全性考虑结合可以增强软件系统的鲁棒性。

Conclusion: 论文提出了一套实用指南，帮助从业者在改进可靠性预测模型的同时应对现代软件应用的安全和安全性挑战。

Abstract: Reliability prediction is crucial for ensuring the safety and security of
software systems, especially in the context of industry practices. While
various metrics and measurements are employed to assess software reliability,
the complexity of modern systems necessitates a deeper understanding of how
these metrics interact with security and safety concerns. This paper explores
the interplay between software reliability, safety, and security, offering a
comprehensive analysis of key metrics and measurement techniques used in the
industry for reliability prediction. We identify critical threats to software
reliability and provide a threat estimation framework that incorporates both
safety and security aspects. Our findings suggest that integrating reliability
metrics with safety and security considerations can enhance the robustness of
software systems. Furthermore, we propose a set of actionable guidelines for
practitioners to improve their reliability prediction models while
simultaneously addressing the security and safety challenges of contemporary
software applications.

</details>


### [5] [Defining ethically sourced code generation](https://arxiv.org/abs/2507.19743)
*Zhuolin Xu,Chenglin Li,Qiushi Li,Shin Hwei Tan*

Main category: cs.SE

TL;DR: 论文提出了一种名为ES-CodeGen的新概念，旨在通过道德和可持续实践管理代码生成模型的开发过程。通过文献综述和从业者调查，确定了11个ES-CodeGen的维度，并探讨了其后果和重要性。


<details>
  <summary>Details</summary>
Motivation: 随着代码生成模型的普及，确保其开发过程符合道德和可持续性标准的需求日益增长。研究旨在填补这一领域的空白。

Method: 采用两阶段文献综述（803篇论文）和从业者调查（32人）的方法，识别并提炼ES-CodeGen的维度和后果。

Result: 确定了11个ES-CodeGen的维度，包括新发现的代码质量维度，并揭示了从业者对社交相关维度的忽视。

Conclusion: 研究呼吁关注代码生成中的伦理问题，并强调社交维度的重要性，为未来的研究和实践提供了方向。

Abstract: Several code generation models have been proposed to help reduce time and
effort in solving software-related tasks. To ensure responsible AI, there are
growing interests over various ethical issues (e.g., unclear licensing,
privacy, fairness, and environment impact). These studies have the overarching
goal of ensuring ethically sourced generation, which has gained growing
attentions in speech synthesis and image generation. In this paper, we
introduce the novel notion of Ethically Sourced Code Generation (ES-CodeGen) to
refer to managing all processes involved in code generation model development
from data collection to post-deployment via ethical and sustainable practices.
To build a taxonomy of ES-CodeGen, we perform a two-phase literature review
where we read 803 papers across various domains and specific to AI-based code
generation. We identified 71 relevant papers with 10 initial dimensions of
ES-CodeGen. To refine our dimensions and gain insights on consequences of
ES-CodeGen, we surveyed 32 practitioners, which include six developers who
submitted GitHub issues to opt-out from the Stack dataset (these impacted users
have real-world experience of ethically sourcing issues in code generation
models). The results lead to 11 dimensions of ES-CodeGen with a new dimension
on code quality as practitioners have noted its importance. We also identified
consequences, artifacts, and stages relevant to ES-CodeGen. Our post-survey
reflection showed that most practitioners tend to ignore social-related
dimensions despite their importance. Most practitioners either agreed or
strongly agreed that our survey help improve their understanding of ES-CodeGen.
Our study calls for attentions of various ethical issues towards ES-CodeGen.

</details>


### [6] [From Few-Label to Zero-Label: An Approach for Cross-System Log-Based Anomaly Detection with Meta-Learning](https://arxiv.org/abs/2507.19806)
*Xinlong Zhao,Tong Jia,Minghua He,Yihan Wu,Ying Li,Gang Huang*

Main category: cs.SE

TL;DR: FreeLog是一种无需目标系统标记日志的跨系统日志异常检测方法，解决了冷启动问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖大量标记日志，难以在实际中应用。跨系统转移虽有效，但仍需少量目标系统标记日志，存在冷启动问题。

Method: 提出FreeLog，一种系统无关的表示元学习方法，无需目标系统标记日志。

Result: 在三个公共日志数据集上，FreeLog性能接近依赖少量标记数据的现有方法。

Conclusion: FreeLog为零标记跨系统日志异常检测提供了可行方案。

Abstract: Log anomaly detection plays a critical role in ensuring the stability and
reliability of software systems. However, existing approaches rely on large
amounts of labeled log data, which poses significant challenges in real-world
applications. To address this issue, cross-system transfer has been identified
as a key research direction. State-of-the-art cross-system approaches achieve
promising performance with only a few labels from the target system. However,
their reliance on labeled target logs makes them susceptible to the cold-start
problem when labeled logs are insufficient. To overcome this limitation, we
explore a novel yet underexplored setting: zero-label cross-system log anomaly
detection, where the target system logs are entirely unlabeled. To this end, we
propose FreeLog, a system-agnostic representation meta-learning method that
eliminates the need for labeled target system logs, enabling cross-system log
anomaly detection under zero-label conditions. Experimental results on three
public log datasets demonstrate that FreeLog achieves performance comparable to
state-of-the-art methods that rely on a small amount of labeled data from the
target system.

</details>


### [7] [A Cooperative Approach for Knowledge-based Business Process Design in a Public Authority](https://arxiv.org/abs/2507.19842)
*Mohammad Azarijafari,Luisa Mich,Michele Missikoff,Oleg Missikoff*

Main category: cs.SE

TL;DR: 本文提出了一种基于知识的方法，帮助业务专家设计业务流程，无需知识工程背景，通过结构化步骤生成目标流程的图表化工作流。


<details>
  <summary>Details</summary>
Motivation: 企业因数字化转型需调整组织结构和运营，尤其是中小企业。过程导向的生产模型是关键创新前沿。

Method: 从简单的文本知识工件开始，逐步构建更结构化的知识库，支持业务专家通过结构化步骤设计业务流程。

Result: 该方法支持所有参与业务流程设计的利益相关者共享知识，生成图表化工作流。

Conclusion: 该方法为业务流程设计提供了一种无需专业知识背景的共享方法，适用于数字化转型中的企业。

Abstract: Enterprises are currently undergoing profound transformations due to the
unpostponable digital transformation. Then, to remain competitive, enterprises
must adapt their organisational structures and operations. This organisational
shift is also important for small and medium-sized enterprises. A key
innovation frontier is the adoption of process-oriented production models. This
paper presents a knowledge-based method to support business experts in
designing business processes. The method requires no prior expertise in
Knowledge Engineering and guides designers through a structured sequence of
steps to produce a diagrammatic workflow of the target process. The
construction of the knowledge base starts from simple, text-based, knowledge
artefacts and then progresses towards more structured, formal representations.
The approach has been conceived to allow a shared approach for all stakeholders
and actors who participate in the BP design.

</details>


### [8] [AgentMesh: A Cooperative Multi-Agent Generative AI Framework for Software Development Automation](https://arxiv.org/abs/2507.19902)
*Sourena Khanzadeh*

Main category: cs.SE

TL;DR: AgentMesh是一个基于Python的框架，利用多个合作的LLM代理来自动化软件开发任务，包括规划、编码、调试和审查。


<details>
  <summary>Details</summary>
Motivation: 传统软件开发需要多领域专家协作，复杂且耗时。AgentMesh旨在通过多代理协作自动化这一过程，提高效率。

Method: 框架包含四个专用代理（Planner、Coder、Debugger、Reviewer），分别负责任务分解、代码实现、调试和审查。

Result: 案例研究表明，AgentMesh能处理复杂的开发请求，通过任务规划、代码生成、调试和审查实现自动化。

Conclusion: 多代理协作能发挥LLM的优势并弥补单代理的局限性，未来需解决错误传播和上下文扩展等问题。

Abstract: Software development is a complex, multi-phase process traditionally
requiring collaboration among individuals with diverse expertise. We propose
AgentMesh, a Python-based framework that uses multiple cooperating LLM-powered
agents to automate software development tasks. In AgentMesh, specialized agents
- a Planner, Coder, Debugger, and Reviewer - work in concert to transform a
high-level requirement into fully realized code. The Planner agent first
decomposes user requests into concrete subtasks; the Coder agent implements
each subtask in code; the Debugger agent tests and fixes the code; and the
Reviewer agent validates the final output for correctness and quality. We
describe the architecture and design of these agents and their communication,
and provide implementation details including prompt strategies and workflow
orchestration. A case study illustrates AgentMesh handling a non-trivial
development request via sequential task planning, code generation, iterative
debugging, and final code review. We discuss how dividing responsibilities
among cooperative agents leverages the strengths of large language models while
mitigating single-agent limitations. Finally, we examine current limitations -
such as error propagation and context scaling - and outline future work toward
more robust, scalable multi-agent AI systems for software engineering
automation.

</details>


### [9] [CrossPL: Evaluating Large Language Models on Cross Programming Language Code Generation](https://arxiv.org/abs/2507.19904)
*Zhanhang Xiong,Dongxia Wang,Yuekang Li,Xinyuan An,Wenhai Wang*

Main category: cs.SE

TL;DR: 论文提出了首个评估大型语言模型（LLM）生成跨编程语言（CPL）互操作代码能力的基准CrossPL，覆盖6种语言和7种技术，结果显示现有模型表现不佳。


<details>
  <summary>Details</summary>
Motivation: 随着LLM在软件开发中的广泛应用，生成跨语言互操作代码的能力尚未被充分研究，这对构建复杂多语言系统至关重要。

Method: 通过分析GitHub多语言仓库和开发LLM自动化流程，构建包含1,982个任务的CrossPL基准，并基于有限状态机（FSM）验证模型表现。

Result: 评估14个通用LLM和6个代码专用LLM，发现即使是表现最好的模型在CPL场景中也表现不佳。

Conclusion: CrossPL揭示了LLM在跨语言代码生成方面的不足，呼吁更多针对性研究。

Abstract: As large language models (LLMs) become increasingly embedded in software
engineering workflows, a critical capability remains underexplored: generating
correct code that enables cross-programming-language (CPL) interoperability.
This skill is essential for building complex systems that integrate components
written in multiple languages via mechanisms like inter-process communication
(IPC). To bridge this gap, we present CrossPL, the first benchmark designed to
systematically evaluate LLMs' ability to generate CPL-interoperating code.
CrossPL comprises 1,982 tasks centered around IPC, covering six widely-used
programming languages and seven representative CPL techniques. We construct
this benchmark by (i) analyzing 19,169 multi-language GitHub repositories using
156 hand-crafted finite state machines (FSMs), and (ii) developing an LLM-based
pipeline that automatically extracts CPL code snippets, generates task
instructions, and validates functional correctness. We evaluate 14
state-of-the-art general-purpose LLMs and 6 code-oriented LLMs released in the
past three years on CrossPL via FSM-based validation. Results reveal that even
the best-performing models struggle with CPL scenarios, underscoring the need
for more targeted research in this space. Our benchmark and code are available
at: https://anonymous.4open.science/r/crosspl-2814.

</details>


### [10] [The Impact of Fine-tuning Large Language Models on Automated Program Repair](https://arxiv.org/abs/2507.19909)
*Roman Macháček,Anastasiia Grishina,Max Hort,Leon Moonen*

Main category: cs.SE

TL;DR: 研究了不同微调技术对用于自动程序修复（APR）的大型语言模型（LLMs）性能的影响，发现参数高效微调（PEFT）优于完全微调。


<details>
  <summary>Details</summary>
Motivation: 探索如何以较低计算成本提升LLMs在APR任务中的性能。

Method: 在三个APR基准上评估六种LLMs，比较无微调、完全微调和PEFT（LoRA和IA3）的效果。

Result: 完全微调因数据分布差异和过拟合导致性能下降，PEFT通过限制可训练参数取得更好结果。

Conclusion: 参数高效微调是提升LLMs在APR任务中性能的有效方法。

Abstract: Automated Program Repair (APR) uses various tools and techniques to help
developers achieve functional and error-free code faster. In recent years,
Large Language Models (LLMs) have gained popularity as components in APR tool
chains because of their performance and flexibility. However, training such
models requires a significant amount of resources. Fine-tuning techniques have
been developed to adapt pre-trained LLMs to specific tasks, such as APR, and
enhance their performance at far lower computational costs than training from
scratch. In this study, we empirically investigate the impact of various
fine-tuning techniques on the performance of LLMs used for APR. Our experiments
provide insights into the performance of a selection of state-of-the-art LLMs
pre-trained on code. The evaluation is done on three popular APR benchmarks
(i.e., QuixBugs, Defects4J and HumanEval-Java) and considers six different LLMs
with varying parameter sizes (resp. CodeGen, CodeT5, StarCoder, DeepSeekCoder,
Bloom, and CodeLlama-2). We consider three training regimens: no fine-tuning,
full fine-tuning, and parameter-efficient fine-tuning (PEFT) using LoRA and
IA3. We observe that full fine-tuning techniques decrease the benchmarking
performance of various models due to different data distributions and
overfitting. By using parameter-efficient fine-tuning methods, we restrict
models in the amount of trainable parameters and achieve better results.
  Keywords: large language models, automated program repair,
parameter-efficient fine-tuning, AI4Code, AI4SE, ML4SE.

</details>


### [11] [Prometheus: Unified Knowledge Graphs for Issue Resolution in Multilingual Codebases](https://arxiv.org/abs/2507.19942)
*Zimin Chen,Yue Pan,Siyu Lu,Jiayi Xu,Claire Le Goues,Martin Monperrus,He Ye*

Main category: cs.SE

TL;DR: Prometheus是一个多代理系统，通过将代码库转换为知识图谱来解决多语言存储库中的实际问题，超越了现有方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有语言模型代理（如SWE-agent和OpenHands）仅适用于Python问题，且依赖预构建容器，限制了其在多语言和实际场景中的应用。

Method: Prometheus将代码库转换为统一的知识图谱，支持多种编程语言，并使用Neo4j进行图持久化，结合DeepSeek-V3模型进行问题解决。

Result: 在SWE-bench Lite和SWE-bench Multilingual上分别解决了28.67%和13.7%的问题，平均API成本为$0.23和$0.38，并首次在七种编程语言中展示有效性。

Conclusion: Prometheus在多语言和实际场景中表现出色，解决了现有方法未覆盖的问题，并开源了代码。

Abstract: Language model (LM) agents, such as SWE-agent and OpenHands, have made
progress toward automated issue resolution. However, existing approaches are
often limited to Python-only issues and rely on pre-constructed containers in
SWE-bench with reproduced issues, restricting their applicability to real-world
and work for multi-language repositories. We present Prometheus, designed to
resolve real-world issues beyond benchmark settings. Prometheus is a
multi-agent system that transforms an entire code repository into a unified
knowledge graph to guide context retrieval for issue resolution. Prometheus
encodes files, abstract syntax trees, and natural language text into a graph of
typed nodes and five general edge types to support multiple programming
languages. Prometheus uses Neo4j for graph persistence, enabling scalable and
structured reasoning over large codebases. Integrated by the DeepSeek-V3 model,
Prometheus resolves 28.67% and 13.7% of issues on SWE-bench Lite and SWE-bench
Multilingual, respectively, with an average API cost of $0.23 and $0.38 per
issue. Prometheus resolves 10 unique issues not addressed by prior work and is
the first to demonstrate effectiveness across seven programming languages.
Moreover, it shows the ability to resolve real-world GitHub issues in the
LangChain and OpenHands repositories. We have open-sourced Prometheus at:
https://github.com/Pantheon-temple/Prometheus

</details>


### [12] [PDLogger: Automated Logging Framework for Practical Software Development](https://arxiv.org/abs/2507.19951)
*Shengcheng Duan,Yihua Xu,Sheng Zhang,Shen Wang,Yue Duan*

Main category: cs.SE

TL;DR: PDLogger是一种端到端的日志生成技术，针对多日志场景设计，通过三阶段流程（日志位置预测、日志生成、日志优化）显著提升日志质量。


<details>
  <summary>Details</summary>
Motivation: 现有自动化日志技术仅关注单一子任务（如日志位置、级别或消息），无法生成完整且高质量的日志，且忽略方法间的语义依赖。

Method: PDLogger采用三阶段流程：1) 日志位置预测；2) 日志生成（结合程序切片和变量提取）；3) 日志优化（级别校正和去重）。

Result: 在3,113条日志上评估，PDLogger在日志位置精度、F1值、级别准确性、变量精度和消息质量上均有显著提升。

Conclusion: PDLogger是一种高效且通用的日志生成框架，支持主流LLM，开源以促进研究和应用。

Abstract: Logging is indispensable for maintaining the reliability and diagnosability
of modern software, yet developers still struggle to decide where and how to
log effectively. Existing automated logging techniques focus on isolated
sub-tasks - predicting a single log position, level, or message - and therefore
cannot produce complete, high-quality log statements that reflect real-world
practice in which multiple logs often appear inside one method. They also
neglect deeper semantic dependencies among methods and consider only a narrow
set of candidate variables, leading to superficial or incomplete logs. In this
paper, we present PDLogger, the first end-to-end log generation technique
expressly designed for practical, multi-log scenarios. PDLogger operates in
three phases. (1) Log position prediction: block-type-aware structured prompts
guide a large language model (LLM) to suggest candidate positions across all
control-flow blocks of a method. (2) Log generation: backward program slicing
supplies precise inter-procedural control and data-dependency context, while an
expanded variable extractor captures both member and external function
expressions; the enriched prompt enables the LLM to emit a full log statement
(position, level, message, variables). (3) Log refinement: level correction and
context-sensitive deduplication prune false positives and redundant logs. We
evaluate PDLogger on 3,113 log statements drawn from two widely used Java
projects. Compared with the strongest prior systems, PDLogger improves
log-position precision by 139.0 percent, F1 by 69.2 percent, level accuracy by
82.3 percent, variable precision by 131.8 percent, and message quality
(BERTScore) by 65.7 percent. The framework consistently performs well with
different mainstream LLMs, demonstrating robustness and generality. PDLogger's
implementation is available as open source to foster future research and
adoption.

</details>


### [13] [The Effect of Pointer Analysis on Semantic Conflict Detection](https://arxiv.org/abs/2507.20081)
*Matheus Barbosa,Paulo Borba,Rodrigo Bonifácio,Victor Lira,Galileu Santos*

Main category: cs.SE

TL;DR: 研究探讨了指针分析在语义冲突检测中的作用，发现其虽减少误报和超时，但显著增加漏报，建议探索混合分析技术。


<details>
  <summary>Details</summary>
Motivation: 现有合并工具无法检测语义冲突，静态分析虽提出但误报率高，研究旨在探讨指针分析是否能减少误报。

Method: 通过实证研究，实现带和不带指针分析的相同分析，比较其在两个数据集上的准确性、计算性能和差异频率。

Result: 指针分析显著减少误报和超时，但导致漏报增加，召回率和F1分数大幅下降。

Conclusion: 建议在语义冲突检测中探索结合两种实现优点的混合分析技术。

Abstract: Current merge tools don't detect semantic conflicts, which occur when changes
from different developers are textually integrated but semantically interfere
with each other. Although researchers have proposed static analyses for
detecting semantic conflicts, these analyses suffer from significant false
positive rates. To understand whether such false positives could be reduced by
using pointer analysis in the implementation of semantic conflict static
analyses, we conduct an empirical study. We implement the same analysis with
and without pointer analysis, run them on two datasets, observe how often they
differ, and compare their accuracy and computational performance. Although
pointer analysis is known to improve precision in static analysis, we find that
its effect on semantic conflict detection can be drastic: we observe a
significant reduction in timeouts and false positives, but also a significant
increase in false negatives, with prohibitive drops in recall and F1-score.
These results suggest that, in the context of semantic conflict detection, we
should explore hybrid analysis techniques, combining aspects of both
implementations we compare in our study.

</details>


### [14] [From First Use to Final Commit: Studying the Evolution of Multi-CI Service Adoption](https://arxiv.org/abs/2507.20095)
*Nitika Chopra,Taher A. Ghaleb*

Main category: cs.SE

TL;DR: 研究分析了18,924个Java项目在2008年至2024年间对8种CI服务的采用情况，发现多服务共用在项目中很常见，且常伴随迁移行为。


<details>
  <summary>Details</summary>
Motivation: 现有研究通常孤立分析单个CI服务，缺乏对多服务采用和迁移的理解。

Method: 通过历史数据分析18,924个Java项目对8种CI服务的采用模式。

Result: 近五分之一项目采用多CI服务，且常伴随服务迁移。

Conclusion: 研究揭示了多CI服务采用的实践模式，为未来研究和服务工具开发提供新方向。

Abstract: Continuous Integration (CI) services, such as GitHub Actions and Travis CI,
are widely adopted in open-source development to automate testing and
deployment. Though existing research often examines individual services in
isolation, it remains unclear how projects adopt and transition between
multiple services over time. To understand how CI adoption is evolving across
services, we present a preliminary study analyzing the historical CI adoption
of 18,924 Java projects hosted on GitHub between January 2008 and December
2024, adopting at least one of eight CI services, namely Travis CI, AppVeyor,
CircleCI, Azure Pipelines, GitHub Actions, Bitbucket, GitLab CI, and Cirrus CI.
Specifically, we investigate: (1) how frequently CI services are co-adopted or
replaced, and (2) how maintenance activity varies across different services.
Our analysis shows that the use of multiple CI services within the same project
is a recurring pattern observed in nearly one in five projects, often
reflecting migration across CI services. Our study is among the first to
examine multi-CI adoption in practice, offering new insights for future
research and highlighting the need for strategies and tools to support service
selection, coordination, and migration in evolving CI environments.

</details>


### [15] [Learning to Align Human Code Preferences](https://arxiv.org/abs/2507.20109)
*Xin Yin,Chao Ni,Liushan Chen,Xiaohu Yang*

Main category: cs.SE

TL;DR: 论文研究了SFT和DPO在代码偏好对齐中的作用，提出APO方法，动态整合策略以优化模型性能。


<details>
  <summary>Details</summary>
Motivation: 探索在不同代码偏好场景下，SFT和DPO的最佳训练策略。

Method: 通过理论分析和实验观察，提出APO方法，动态整合SFT和DPO。

Result: APO在六项代表性任务中表现优于或匹配现有策略。

Conclusion: APO为不同代码偏好对齐场景提供了理论和实践指导。

Abstract: Large Language Models (LLMs) have demonstrated remarkable potential in
automating software development tasks. While recent advances leverage
Supervised Fine-Tuning (SFT) and Direct Preference Optimization (DPO) to align
models with human preferences, the optimal training strategy remains unclear
across diverse code preference scenarios. This paper systematically
investigates the roles of SFT and DPO in aligning LLMs with different code
preferences. Through both theoretical analysis and empirical observation, we
hypothesize that SFT excels in scenarios with objectively verifiable optimal
solutions, while applying SFT followed by DPO (S&D) enables models to explore
superior solutions in scenarios without objectively verifiable optimal
solutions. Based on the analysis and experimental evidence, we propose Adaptive
Preference Optimization (APO), a dynamic integration approach that adaptively
amplifies preferred responses, suppresses dispreferred ones, and encourages
exploration of potentially superior solutions during training. Extensive
experiments across six representative code preference tasks validate our
theoretical hypotheses and demonstrate that APO consistently matches or
surpasses the performance of existing SFT and S&D strategies. Our work provides
both theoretical foundations and practical guidance for selecting appropriate
training strategies in different code preference alignment scenarios.

</details>


### [16] [From Prompt to Pipeline: Large Language Models for Scientific Workflow Development in Bioinformatics](https://arxiv.org/abs/2507.20122)
*Khairul Alam,Banani Roy*

Main category: cs.SE

TL;DR: 研究探讨了现代大语言模型（如GPT-4o、Gemini 2.5 Flash和DeepSeek-V3）是否能够生成准确、完整且可用的生物信息学工作流，并分析了最有效的提示策略。Gemini 2.5 Flash在Galaxy工作流生成中表现最佳，而DeepSeek-V3在Nextflow中表现突出。提示策略对质量有显著影响。


<details>
  <summary>Details</summary>
Motivation: 生物信息学数据分析的复杂性增加使得科学工作流系统（如Galaxy和Nextflow）变得至关重要，但创建和理解这些工作流对非编程专家仍具挑战性。研究旨在探索大语言模型是否能降低工作流开发的门槛。

Method: 研究评估了GPT-4o、Gemini 2.5 Flash和DeepSeek-V3在生成生物信息学工作流中的表现，任务包括SNP分析、RNA-seq等，并基于Galaxy和Nextflow平台。专家评审将生成的工作流与社区基准进行比较。

Result: Gemini 2.5 Flash在Galaxy工作流生成中表现最佳，DeepSeek-V3在Nextflow中表现突出。提示策略（如角色扮演和思维链提示）显著提高了工作流的完整性和正确性。

Conclusion: 大语言模型有潜力降低工作流开发门槛，提高可重复性，并促进生物信息学计算工具的普及，尤其是结合有效的提示工程时。

Abstract: The increasing complexity of bioinformatics data analysis has made Scientific
Workflow Systems (SWSs) like Galaxy and Nextflow essential for enabling
scalable, reproducible, and automated workflows. However, creating and
understanding these workflows remains challenging, particularly for domain
experts without programming expertise. This study investigates whether modern
Large Language Models (LLMs), GPT-4o, Gemini 2.5 Flash, and DeepSeek-V3, can
support the generation of accurate, complete, and usable bioinformatics
workflows, and examines which prompting strategies most effectively guide this
process. We evaluate these models using diverse tasks such as SNP analysis,
RNA-seq, DNA methylation, and data retrieval, spanning both graphical (Galaxy)
and script-based (Nextflow) platforms. Expert reviewers assess the generated
workflows against community-curated baselines from the Galaxy Training Network
and nf-core repositories. The results show that Gemini 2.5 Flash excels in
generating Galaxy workflows, while DeepSeek-V3 performs strongly in Nextflow.
Prompting strategies significantly impact quality, with role-based and
chain-of-thought prompts improving completeness and correctness. While GPT-4o
benefits from structured inputs, DeepSeek-V3 offers rich technical detail,
albeit with some verbosity. Overall, the findings highlight the potential of
LLMs to lower the barrier for workflow development, improve reproducibility,
and democratize access to computational tools in bioinformatics, especially
when combined with thoughtful prompt engineering.

</details>


### [17] [VDGraph: A Graph-Theoretic Approach to Unlock Insights from SBOM and SCA Data](https://arxiv.org/abs/2507.20502)
*Howell Xia,Jonah Gluck,Sevval Simsek,David Sastre Medina,David Starobinski*

Main category: cs.SE

TL;DR: 本文提出VDGraph，一种基于知识图谱的方法，整合SBOM和SCA工具数据，提供依赖和漏洞的统一视图，并通过实验验证其在识别高风险点和漏洞传播路径上的有效性。


<details>
  <summary>Details</summary>
Motivation: 现代软件供应链的复杂性需要工具（如SBOM和SCA）来管理依赖和漏洞，但现有工具缺乏整合，难以提供统一的依赖-漏洞关系视图。

Method: 提出VDGraph，将SBOM和SCA数据整合为图谱表示，提供形式化描述和冲突管理方案，并通过CycloneDX和OSV-Scanner实现概念验证。

Result: 在21个Java项目上应用VDGraph，发现高风险点和漏洞多出现在三级或更深依赖层级，直接或次级依赖更安全。

Conclusion: VDGraph通过图谱理论方法提升了漏洞传播路径的可视化，结合开源工具为实际项目提供了可扩展的自动化分析基础。

Abstract: The high complexity of modern software supply chains necessitates tools such
as Software Bill of Materials (SBOMs) to manage component dependencies, and
Software Composition Analysis (SCA) tools to identify vulnerabilities. While
there exists limited integration between SBOMs and SCA tools, a unified view of
complex dependency-vulnerability relationships remains elusive. In this paper,
we introduce VDGraph, a novel knowledge graph-based methodology for integrating
vulnerability and dependency data into a holistic view. VDGraph consolidates
SBOM and SCA outputs into a graph representation of software projects'
dependencies and vulnerabilities. We provide a formal description and analysis
of the theoretical properties of VDGraph and present solutions to manage
possible conflicts between the SBOM and SCA data. We further introduce and
evaluate a practical, proof-of-concept implementation of VDGraph using two
popular SBOM and SCA tools, namely CycloneDX Maven plugin and Google's
OSV-Scanner. We apply VDGraph on 21 popular Java projects. Through the
formulation of appropriate queries on the graphs, we uncover the existence of
concentrated risk points (i.e., vulnerable components of high severity
reachable through numerous dependency paths). We further show that
vulnerabilities predominantly emerge at a depth of three dependency levels or
higher, indicating that direct or secondary dependencies exhibit lower
vulnerability density and tend to be more secure. Thus, VDGraph contributes a
graph-theoretic methodology that improves visibility into how vulnerabilities
propagate through complex, transitive dependencies. Moreover, our
implementation, which combines open SBOM and SCA standards with Neo4j, lays a
foundation for scalable and automated analysis across real-world projects.

</details>


### [18] [Relating System Safety and Machine Learnt Model Performance](https://arxiv.org/abs/2507.20135)
*Ganesh Pai*

Main category: cs.SE

TL;DR: 论文提出了一种方法，用于从安全评估过程中获取的定量安全目标出发，推导出机器学习和深度神经网络组件的最低安全相关性能要求和目标。


<details>
  <summary>Details</summary>
Motivation: 在航空应用中，机器学习模型的安全性能要求与其系统安全目标之间的关系不明确，需要一种方法明确这种关系并确保安全。

Method: 通过一个飞机紧急制动系统的例子，抽象出机器学习组件的行为，并基于此提出一种方法，推导出满足安全目标的最低性能要求和指标。

Result: 提出了一种初始方法，能够明确安全目标与模型性能要求之间的关系，并确定相关指标和目标。

Conclusion: 该方法为航空应用中机器学习组件的安全性能要求提供了明确的推导路径，并指出了其有效性和验证的约束条件。

Abstract: The prediction quality of machine learnt models and the functionality they
ultimately enable (e.g., object detection), is typically evaluated using a
variety of quantitative metrics that are specified in the associated model
performance requirements. When integrating such models into aeronautical
applications, a top-down safety assessment process must influence both the
model performance metrics selected, and their acceptable range of values.
Often, however, the relationship of system safety objectives to model
performance requirements and the associated metrics is unclear. Using an
example of an aircraft emergency braking system containing a machine learnt
component (MLC) responsible for object detection and alerting, this paper first
describes a simple abstraction of the required MLC behavior. Then, based on
that abstraction, an initial method is given to derive the minimum
safety-related performance requirements, the associated metrics, and their
targets for the both MLC and its underlying deep neural network, such that they
meet the quantitative safety objectives obtained from the safety assessment
process. We give rationale as to why the proposed method should be considered
valid, also clarifying the assumptions made, the constraints on applicability,
and the implications for verification.

</details>


### [19] [Repairing vulnerabilities without invisible hands. A differentiated replication study on LLMs](https://arxiv.org/abs/2507.20977)
*Maria Camporese,Fabio Massacci*

Main category: cs.SE

TL;DR: 研究探讨了大型语言模型（LLMs）在自动漏洞修复（AVR）中的表现是否依赖于训练数据泄漏或完美错误定位等隐藏因素。


<details>
  <summary>Details</summary>
Motivation: 验证LLMs是否仅通过记忆人类修复方案来生成补丁，而非真正理解漏洞。

Method: 通过故意偏移漏洞位置，观察LLMs生成的补丁质量变化，并采用双LLM流程（生成和审核）及手动审计。

Result: 实验结果表明，LLMs在漏洞定位偏移时补丁质量下降，说明其表现不完全依赖记忆。

Conclusion: LLMs在AVR中的成功并非仅由隐藏因素驱动，而是具备一定的理解和生成能力。

Abstract: Background: Automated Vulnerability Repair (AVR) is a fast-growing branch of
program repair. Recent studies show that large language models (LLMs)
outperform traditional techniques, extending their success beyond code
generation and fault detection.
  Hypothesis: These gains may be driven by hidden factors -- "invisible hands"
such as training-data leakage or perfect fault localization -- that let an LLM
reproduce human-authored fixes for the same code.
  Objective: We replicate prior AVR studies under controlled conditions by
deliberately adding errors to the reported vulnerability location in the
prompt. If LLMs merely regurgitate memorized fixes, both small and large
localization errors should yield the same number of correct patches, because
any offset should divert the model from the original fix.
  Method: Our pipeline repairs vulnerabilities from the Vul4J and VJTrans
benchmarks after shifting the fault location by n lines from the ground truth.
A first LLM generates a patch, a second LLM reviews it, and we validate the
result with regression and proof-of-vulnerability tests. Finally, we manually
audit a sample of patches and estimate the error rate with the
Agresti-Coull-Wilson method.

</details>


### [20] [Strategic Motivators for Ethical AI System Development: An Empirical and Holistic Model](https://arxiv.org/abs/2507.20218)
*Muhammad Azeem Akbar,Arif Ali Khan,Saima Rafi,Damian Kedziora,Sami Hyrynsalmi*

Main category: cs.SE

TL;DR: 该研究通过多声部文献综述和问卷调查，识别并优先排序了推动AI伦理发展的20个关键动机因素，分为8类。ISM和MICMAC分析揭示了各因素间的关系及其驱动依赖性，Fuzzy TOPSIS则对动机因素的重要性进行了排序。


<details>
  <summary>Details</summary>
Motivation: 确保AI的负责任发展，防止潜在负面影响，需明确推动伦理AI实践的关键动机因素。

Method: 采用多声部文献综述（MLR）、问卷调查、解释性结构建模（ISM）、MICMAC分析和模糊TOPSIS方法。

Result: 识别出20个关键动机因素，分为8类。ISM显示'人力资源'和'协调'对其他因素影响显著；MICMAC分析将部分因素归为独立集群；Fuzzy TOPSIS排序了关键动机。

Conclusion: 组织应将这些动机因素融入策略、治理模型和开发框架，以支持伦理AI的采用。

Abstract: Artificial Intelligence (AI) presents transformative opportunities for
industries and society, but its responsible development is essential to prevent
unintended consequences. Ethically sound AI systems demand strategic planning,
strong governance, and an understanding of the key drivers that promote
responsible practices. This study aims to identify and prioritize the
motivators that drive the ethical development of AI systems. A Multivocal
Literature Review (MLR) and a questionnaire-based survey were conducted to
capture current practices in ethical AI. We applied Interpretive Structure
Modeling (ISM) to explore the relationships between motivator categories,
followed by MICMAC analysis to classify them by their driving and dependence
power. Fuzzy TOPSIS was used to rank these motivators by importance. Twenty key
motivators were identified and grouped into eight categories: Human Resource,
Knowledge Integration, Coordination, Project Administration, Standards,
Technology Factor, Stakeholders, and Strategy & Matrices. ISM results showed
that 'Human Resource' and 'Coordination' heavily influence other factors.
MICMAC analysis placed categories like Human Resource (CA1), Coordination
(CA3), Stakeholders (CA7), and Strategy & Matrices (CA8) in the independent
cluster, indicating high driving but low dependence power. Fuzzy TOPSIS ranked
motivators such as promoting team diversity, establishing AI governance bodies,
appointing oversight leaders, and ensuring data privacy as most critical. To
support ethical AI adoption, organizations should align their strategies with
these motivators and integrate them into their policies, governance models, and
development frameworks.

</details>


### [21] [Beyond Binary Moderation: Identifying Fine-Grained Sexist and Misogynistic Behavior on GitHub with Large Language Models](https://arxiv.org/abs/2507.20358)
*Tanni Dev,Sayma Sultana,Amiangshu Bosu*

Main category: cs.SE

TL;DR: 该研究提出了一种基于指令调优大语言模型的多类别分类框架，用于检测GitHub上的性别歧视和厌女评论，显著优于基线方法，但在处理复杂语境时仍有局限。


<details>
  <summary>Details</summary>
Motivation: 技术社区（如GitHub）中的性别歧视和厌女行为阻碍了包容性，现有工具难以有效检测细微的伤害。

Method: 使用指令调优的大语言模型框架，通过20次迭代优化提示，评估了1,440条标记评论，并比较了多种性能指标。

Result: 优化后的模型（GPT-4o与Prompt 19）MCC为0.501，显著优于基线，但对复杂语境的识别仍有不足。

Conclusion: 精心设计的提示和清晰定义能显著提升性别歧视检测的准确性和实用性，适用于开发者平台的精准审核。

Abstract: Background: Sexist and misogynistic behavior significantly hinders inclusion
in technical communities like GitHub, causing developers, especially
minorities, to leave due to subtle biases and microaggressions. Current
moderation tools primarily rely on keyword filtering or binary classifiers,
limiting their ability to detect nuanced harm effectively.
  Aims: This study introduces a fine-grained, multi-class classification
framework that leverages instruction-tuned Large Language Models (LLMs) to
identify twelve distinct categories of sexist and misogynistic comments on
GitHub.
  Method: We utilized an instruction-tuned LLM-based framework with systematic
prompt refinement across 20 iterations, evaluated on 1,440 labeled GitHub
comments across twelve sexism/misogyny categories. Model performances were
rigorously compared using precision, recall, F1-score, and the Matthews
Correlation Coefficient (MCC).
  Results: Our optimized approach (GPT-4o with Prompt 19) achieved an MCC of
0.501, significantly outperforming baseline approaches. While this model had
low false positives, it struggled to interpret nuanced, context-dependent
sexism and misogyny reliably.
  Conclusion: Well-designed prompts with clear definitions and structured
outputs significantly improve the accuracy and interpretability of sexism
detection, enabling precise and practical moderation on developer platforms
like GitHub.

</details>


### [22] [CIgrate: Automating CI Service Migration with Large Language Models](https://arxiv.org/abs/2507.20402)
*Md Nazmul Hossain,Taher A. Ghaleb*

Main category: cs.SE

TL;DR: 论文提出CIgrate，一个基于LLM的框架，用于自动化迁移CI配置，旨在评估其性能并与现有规则方法CIMig对比。


<details>
  <summary>Details</summary>
Motivation: CI配置在不同服务间迁移是开源开发的常见需求，但手动迁移耗时且易错。现有规则方法CIMig准确性不足，而LLM在代码生成任务中表现优异，可能提升迁移的自动化和通用性。

Method: 提出CIgrate框架，通过零样本/少样本提示或微调LLM来迁移CI配置，并与CIMig对比性能，同时收集开发者反馈。

Result: 预期贡献包括首个LLM驱动的CI迁移方法、与规则方法的对比评估，以及LLM在软件配置演化中的应用洞察。

Conclusion: 研究旨在验证LLM能否提升CI配置迁移的准确性和实用性，为自动化配置迁移提供新思路。

Abstract: Continuous Integration (CI) configurations often need to be migrated between
services (e.g., Travis CI to GitHub Actions) as projects evolve, due to changes
in service capabilities, usage limits, or service deprecation. Previous studies
reported that migration across CI services is a recurring need in open-source
development. However, manual migration can be time-consuming and error-prone.
The state-of-the-art approach, CIMig, addresses this challenge by analyzing
past migration examples to create service-specific rules and produce equivalent
configurations across CI services. However, its relatively low accuracy raises
concerns about the overall feasibility of automated CI migration using
rule-based techniques alone. Meanwhile, Large Language Models (LLMs) have
demonstrated strong capabilities in code generation and transformation tasks,
suggesting potential to improve the automation, usability, and generalizability
of CI configuration migration. This registered report presents a study in which
we aim to assess whether CI migration can be improved using LLMs. To this end,
we propose CIgrate, an LLM-based framework for automatically migrating CI
configurations. We plan to evaluate the performance of CIgrate compared to
CIMig as a baseline, in different setups (a) zero-shot/few-shot prompting of
LLMs for configuration migration and (b) fine-tuning an LLM on a dataset of
already established CI service migrations. We will also seek developer feedback
on the quality and usability of the generated configurations. We formulate
research questions focusing on the accuracy of LLM-generated migrations versus
ground truth and the output of CIMig. The expected contributions include the
first LLM-powered approach for CI service migration, a comparative evaluation
of its effectiveness compared to rule-based approaches, and insight into
leveraging LLMs to support software configuration evolution.

</details>


### [23] [Testing Is Not Boring: Characterizing Challenge in Software Testing Tasks](https://arxiv.org/abs/2507.20407)
*Davi Gama Hardman,Cesar França,Brody Stuart-Verner,Ronnie de Souza Santos*

Main category: cs.SE

TL;DR: 论文探讨了软件测试的复杂性及其对测试人员的影响，强调测试工作需要创造力、问题解决能力和适应性。研究发现，挑战性任务能激励测试人员，而任务过于简单或繁重则可能导致挫败感。


<details>
  <summary>Details</summary>
Motivation: 研究旨在改变对软件测试的刻板印象，揭示其作为动态和智力密集型领域的本质，以及测试人员在工作中面临的挑战和动机。

Method: 通过研究软件测试专业人员，分析他们在测试工作中遇到的挑战性任务及其影响。

Result: 研究发现，涉及创造力、持续学习和时间压力的任务被视为激励和回报，而缺乏挑战或过度要求则导致挫败感和疏离。

Conclusion: 研究强调平衡任务复杂性的重要性，以维持测试人员的积极性，并将软件测试展示为一个充满活力和智力挑战的领域。

Abstract: As software systems continue to grow in complexity, testing has become a
fundamental part of ensuring the quality and reliability of software products.
Yet, software testing is still often perceived, both in industry and academia,
as a repetitive, low-skill activity. This perception fails to recognize the
creativity, problem-solving, and adaptability required in testing work. Tasks
such as designing complex test cases, automating testing processes, and
handling shifting requirements illustrate the challenges testing professionals
regularly face. To better understand these experiences, we conducted a study
with software testing professionals to explore the nature of challenging tasks
in software testing and how they affect these professionals. Our findings show
that tasks involving creativity, ongoing learning, and time pressure are often
seen as motivating and rewarding. On the other hand, a lack of challenge or
overwhelming demands can lead to frustration and disengagement. These findings
demonstrate the importance of balancing task complexity to sustain motivation
and present software testing as a dynamic and intellectually engaging field.

</details>


### [24] [When Prompts Go Wrong: Evaluating Code Model Robustness to Ambiguous, Contradictory, and Incomplete Task Descriptions](https://arxiv.org/abs/2507.20439)
*Maya Larbi,Amal Akli,Mike Papadakis,Rihab Bouyousfi,Maxime Cordy,Federica Sarro,Yves Le Traon*

Main category: cs.SE

TL;DR: 论文研究了大型语言模型（LLMs）在代码生成任务中对模糊任务描述的鲁棒性，发现即使是小的问题也会显著降低性能。


<details>
  <summary>Details</summary>
Motivation: 实际应用中任务描述常存在模糊、不完整或矛盾，而现有研究多基于理想条件，缺乏对LLMs在这种情况下的表现分析。

Method: 通过扩展HumanEval和MBPP基准，引入系统性任务描述缺陷，评估不同规模和架构的LLMs的功能正确性和失败模式。

Result: 任务描述的微小问题会导致性能显著下降，矛盾描述引发逻辑错误；大模型表现更好但仍不免疫。

Conclusion: 需开发更鲁棒的LLMs，改进训练策略和评估基准，以应对实际开发中的模糊需求。

Abstract: Large Language Models (LLMs) have demonstrated impressive performance in code
generation tasks under idealized conditions, where task descriptions are clear
and precise. However, in practice, task descriptions frequently exhibit
ambiguity, incompleteness, or internal contradictions. In this paper, we
present the first empirical study examining the robustness of state-of-the-art
code generation models when faced with such unclear task descriptions. We
extend the HumanEval and MBPP benchmarks by systematically introducing
realistic task descriptions flaws through guided mutation strategies, producing
a dataset that mirrors the messiness of informal developer instructions. We
evaluate multiple LLMs of varying sizes and architectures, analyzing their
functional correctness and failure modes across task descriptions categories.
Our findings reveal that even minor imperfections in task description phrasing
can cause significant performance degradation, with contradictory task
descriptions resulting in numerous logical errors. Moreover, while larger
models tend to be more resilient than smaller variants, they are not immune to
the challenges posed by unclear requirements. We further analyze semantic error
patterns and identify correlations between description clarity, model behavior,
and error types. Our results underscore the critical need for developing LLMs
that are not only powerful but also robust to the imperfections inherent in
natural user tasks, highlighting important considerations for improving model
training strategies, designing more realistic evaluation benchmarks, and
ensuring reliable deployment in practical software development environments.

</details>


### [25] [Distinguishing Quantum Software Bugs from Hardware Noise: A Statistical Approach](https://arxiv.org/abs/2507.20475)
*Ahmik Virani,Devraj,Anirudh Suresh,Lei Zhang,M V Panduranga Rao*

Main category: cs.SE

TL;DR: 提出一种统计方法，通过概率指标区分量子软件错误与硬件噪声，并在实验中验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 在NISQ时代，传统调试方法无法区分量子软件错误与硬件噪声，需新方法解决。

Method: 采用统计方法，利用概率指标分析量子程序中的异常行为。

Result: 实验证明该方法能有效区分软件错误与硬件噪声，适用于多种量子算法。

Conclusion: 该方法为量子开发者提供了可靠工具，用于识别和分类量子程序中的异常行为。

Abstract: Quantum computing in the Noisy Intermediate-Scale Quantum (NISQ) era presents
significant challenges in differentiating quantum software bugs from hardware
noise. Traditional debugging techniques from classical software engineering
cannot directly resolve this issue due to the inherently stochastic nature of
quantum computation mixed with noises from NISQ computers. To address this gap,
we propose a statistical approach leveraging probabilistic metrics to
differentiate between quantum software bugs and hardware noise. We evaluate our
methodology empirically using well-known quantum algorithms, including Grover's
algorithm, Deutsch-Jozsa algorithm, and Simon's algorithm. Experimental results
demonstrate the efficacy and practical applicability of our approach, providing
quantum software developers with a reliable analytical tool to identify and
classify unexpected behavior in quantum programs.

</details>


### [26] [GeoJSEval: An Automated Evaluation Framework for Large Language Models on JavaScript-Based Geospatial Computation and Visualization Code Generation](https://arxiv.org/abs/2507.20553)
*Guanyu Chen,Haoyue Jiao,Shuyang Hou,Ziqi Liu,Lutong Xie,Shaowen Wu,Huayi Wu,Xuefeng Guan,Zhipeng Gui*

Main category: cs.SE

TL;DR: GeoJSEval是一个多模态、函数级自动评估框架，用于评估大型语言模型在JavaScript地理空间代码生成中的表现，包含432个任务和2071个测试用例，支持多维度评估。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型在地理空间代码生成中的广泛应用，迫切需要系统化的评估方法来衡量其能力，尤其是在JavaScript环境中对地理空间计算和可视化的支持。

Method: 提出GeoJSEval框架，包含标准化测试套件（GeoJSEval-Bench）、代码提交引擎和评估模块，覆盖5个JavaScript地理空间库和25种数据类型。

Result: 对18个先进大型语言模型进行评估，发现其在空间语义理解、代码可靠性和函数调用准确性方面存在显著性能差异。

Conclusion: GeoJSEval为地理空间代码生成模型的标准化评估和优化提供了方法论、资源和工具，具有强扩展性和实际应用价值。

Abstract: With the widespread adoption of large language models (LLMs) in code
generation tasks, geospatial code generation has emerged as a critical frontier
in the integration of artificial intelligence and geoscientific analysis. This
trend underscores the urgent need for systematic evaluation methodologies to
assess LLMs generation capabilities in geospatial contexts. In particular,
geospatial computation and visualization tasks in JavaScript environments rely
heavily on orchestrating diverse frontend libraries and ecosystems, placing
elevated demands on a model's semantic understanding and code synthesis
abilities. To address this challenge, we propose GeoJSEval--the first
multimodal, function-level automatic evaluation framework for LLMs in
JavaScript-based geospatial code generation. GeoJSEval comprises three core
components: a standardized test suite (GeoJSEval-Bench), a code submission
engine, and an evaluation module. It includes 432 function-level tasks and
2,071 structured test cases spanning five widely used JavaScript geospatial
libraries and 25 mainstream geospatial data types. GeoJSEval enables
multidimensional quantitative evaluation across metrics such as accuracy,
output stability, execution efficiency, resource consumption, and error type
distribution, and integrates boundary testing mechanisms to enhance robustness
and coverage. We conduct a comprehensive evaluation of 18 state-of-the-art LLMs
using GeoJSEval, revealing significant performance disparities and bottlenecks
in spatial semantic understanding, code reliability, and function invocation
accuracy. GeoJSEval provides a foundational methodology, evaluation resource,
and practical toolkit for the standardized assessment and optimization of
geospatial code generation models, with strong extensibility and applicability
in real-world scenarios.

</details>


### [27] [Intention-Driven Generation of Project-Specific Test Cases](https://arxiv.org/abs/2507.20619)
*Binhang Qi,Yun Lin,Xinyi Weng,Yuhuan Huang,Chenyan Liu,Hailong Sun,Jin Song Dong*

Main category: cs.SE

TL;DR: IntentionTest是一种生成项目特定测试的方法，通过结构化描述验证意图，显著提高了测试的语义正确性和通过率。


<details>
  <summary>Details</summary>
Motivation: 现有测试生成技术多基于代码覆盖率，而实际项目中测试需体现开发者的验证意图和项目特定知识，否则难以通过代码审查。

Method: IntentionTest利用验证意图描述和项目内可重用测试代码，将测试生成问题转化为基于意图的测试代码编辑问题。

Result: 在13个开源项目的4,146个测试案例中，IntentionTest比ChatTester生成更多语义正确的测试，提高了39.03%的变异分数和40.14%的覆盖率重叠，且通过率提升21.30%。

Conclusion: IntentionTest通过结构化验证意图和重用项目知识，显著提升了测试生成的质量和实用性。

Abstract: Test cases are valuable assets for maintaining software quality. While
numerous automated techniques have been proposed for generating tests (either
by maximizing code coverage or by translating focal code into test code),
practical tests are seldom driven by coverage alone. In real projects, each
test reflects a developer's validation intention for a specific behaviour and
embodies rich, project-specific knowledge: which specific APIs to call and what
assertions truly matter. Without considering such knowledge, tests can hardly
pass code review and be integrated into the software product.
  In this work, we propose IntentionTest, which generates project-specific
tests with validation intention as a structured description. Our design is
motivated by two insights: (1) a description of validation intention, compared
to coverage and focal code, carries more crucial information about what to
test; and (2) practical tests exhibit high code duplication, indicating that
domain knowledge is highly reusable for writing new tests. Given a focal code
and a description of validation intention (in the form of either an informal
comment or a formal test plan), IntentionTest retrieves a referable test in the
project to guide test generation. Moreover, IntentionTest reduces the test
generation problem into an editing problem on the test code regarding the
validation intention. It generates a test including both test prefix and
oracle, which aims to be executable and semantically correct.
  We evaluate IntentionTest against state-of-the-art baselines on 4,146 test
cases from 13 open-source projects. Specifically, compared to ChatTester,
IntentionTest can (1) generate significantly more semantically correct tests,
improving common mutation scores by 39.03% and coverage overlap with
ground-truth tests by 40.14%; (2) generate 21.30% more successful passing
tests.

</details>


### [28] [LLM-Based Repair of Static Nullability Errors](https://arxiv.org/abs/2507.20674)
*Nima Karimipour,Michael Pradel,Martin Kellogg,Manu Sridharan*

Main category: cs.SE

TL;DR: NullRepair是一种结合LLM和静态分析的系统，用于解决Java代码中的空指针异常问题，成功修复了72%的残余错误，并保持了程序语义。


<details>
  <summary>Details</summary>
Motivation: 现代Java项目广泛使用静态分析工具处理空指针异常，但大型代码库中残余错误的修复仍具挑战性，手动修复繁琐且易错。

Method: NullRepair通过结构化工作流整合LLM，利用静态分析识别符号的安全使用区域，并通过迭代交互生成修复补丁。

Result: 在12个真实Java项目中，NullRepair平均修复了72%的残余错误，且10/12项目的单元测试全部通过。

Conclusion: NullRepair展示了LLM在自动化修复空指针异常中的潜力，同时通过结构化方法确保了修复的准确性和语义一致性。

Abstract: Modern Java projects increasingly adopt static analysis tools that prevent
null-pointer exceptions by treating nullness as a type property. However,
integrating such tools into large, existing codebases remains a significant
challenge. While annotation inference can eliminate many errors automatically,
a subset of residual errors -- typically a mix of real bugs and false positives
-- often persist and can only be resolved via code changes. Manually addressing
these errors is tedious and error-prone. Large language models (LLMs) offer a
promising path toward automating these repairs, but naively-prompted LLMs often
generate incorrect, contextually-inappropriate edits. Resolving a nullability
error demands a deep understanding of how a symbol is used across the codebase,
often spanning methods, classes, and packages. We present NullRepair, a system
that integrates LLMs into a structured workflow for resolving the errors from a
nullability checker. NullRepair's decision process follows a flowchart derived
from manual analysis of 200 real-world errors. It leverages static analysis to
identify safe and unsafe usage regions of symbols, using error-free usage
examples to contextualize model prompts. Patches are generated through an
iterative interaction with the LLM that incorporates project-wide context and
decision logic. Our evaluation on 12 real-world Java projects shows that
NullRepair resolves an average of 72% of the errors that remain after applying
a state-of-the-art annotation inference technique. Unlike a naively-prompted
LLM, NullRepair also largely preserves program semantics, with all unit tests
passing in 10/12 projects after applying every edit proposed by NullRepair, and
98% or more tests passing in the remaining two projects.

</details>


### [29] [Client--Library Compatibility Testing with API Interaction Snapshots](https://arxiv.org/abs/2507.20814)
*Gustave Monce,Thomas Degueule,Jean-Rémy Falleri,Romain Robbes*

Main category: cs.SE

TL;DR: 论文提出了一种新方法Gilesi，通过记录客户端测试中的API交互快照来检测库的行为破坏性变更（BBCs），解决了传统回归测试难以发现BBCs的问题。


<details>
  <summary>Details</summary>
Motivation: 现代软件开发依赖第三方库，但库的演进可能引入行为破坏性变更（BBCs），传统客户端测试难以检测这些变更。

Method: 利用客户端测试记录API交互快照（协议、输入输出值、异常等），比较新旧快照以识别BBCs。

Result: 通过实验验证，Gilesi能可靠检测出客户端测试遗漏的BBCs。

Conclusion: Gilesi提供了一种有效的方法来检测库的行为破坏性变更，增强了客户端与库的兼容性测试。

Abstract: Modern software development heavily relies on third-party libraries to speed
up development and enhance quality. As libraries evolve, they may break the
tacit contract established with their clients by introducing behavioral
breaking changes (BBCs) that alter run-time behavior and silently break client
applications without being detected at compile time. Traditional regression
tests on the client side often fail to detect such BBCs, either due to limited
library coverage or weak assertions that do not sufficiently exercise the
library's expected behavior. To address this issue, we propose a novel approach
to client--library compatibility testing that leverages existing client tests
in a novel way. Instead of relying on developer-written assertions, we propose
recording the actual interactions at the API boundary during the execution of
client tests (protocol, input and output values, exceptions, etc.). These
sequences of API interactions are stored as snapshots which capture the exact
contract expected by a client at a specific point in time. As the library
evolves, we compare the original and new snapshots to identify perturbations in
the contract, flag potential BBCs, and notify clients. We implement this
technique in our prototype tool Gilesi, a Java framework that automatically
instruments library APIs, records snapshots, and compares them. Through a
preliminary case study on several client--library pairs with artificially
seeded BBCs, we show that Gilesi reliably detects BBCs missed by client test
suites.

</details>


### [30] [Search-Based Fuzzing For RESTful APIs That Use MongoDB](https://arxiv.org/abs/2507.20848)
*Hernan Ghianni,Man Zhang,Juan P. Galeotti,Andrea Arcuri*

Main category: cs.SE

TL;DR: 本文提出了一种新颖的技术，通过动态分析数据库状态和直接从测试用例插入数据，提升了基于搜索的RESTful API测试生成方法，显著提高了代码覆盖率。


<details>
  <summary>Details</summary>
Motivation: 在RESTful API测试中，数据库状态对提高代码覆盖率和发现隐藏故障至关重要。现有方法在生成测试时未充分考虑数据库状态，导致效果有限。

Method: 通过自动化代码插桩动态分析MongoDB数据库状态，并允许从测试用例直接插入NoSQL数据，优化测试生成过程。

Result: 在六个RESTful API上的实验显示，代码覆盖率最高提升18%，优于现有白盒和黑盒测试工具。

Conclusion: 该方法显著提升了RESTful API测试的效果，特别适用于复杂数据库状态和只读微服务的测试场景。

Abstract: In RESTful APIs, interactions with a database are a common and crucial
aspect. When generating whitebox tests, it is essential to consider the
database's state (i.e., the data contained in the database) to achieve higher
code coverage and uncover more hidden faults. This article presents novel
techniques to enhance search-based software test generation for RESTful APIs
interacting with NoSQL databases. Specifically, we target the popular MongoDB
database, by dynamically analyzing (via automated code instrumentation) the
state of the database during the test generation process. Additionally, to
achieve better results, our novel approach allows inserting NoSQL data directly
from test cases. This is particularly beneficial when generating the correct
sequence of events to set the NoSQL database in an appropriate state is
challenging or time-consuming. This method is also advantageous for testing
read-only microservices. Our novel techniques are implemented as an extension
of EvoMaster, the only open-source tool for white-box fuzzing RESTful APIs.
Experiments conducted on six RESTful APIs demonstrated significant improvements
in code coverage, with increases of up to 18% compared to existing white-box
approaches. To better highlight the improvements of our novel techniques,
comparisons are also carried out with four state-of-the-art black-box fuzzers.

</details>


### [31] [Enhancing Project-Specific Code Completion by Inferring Internal API Information](https://arxiv.org/abs/2507.20888)
*Le Deng,Xiaoxue Ren,Chao Ni,Ming Liang,David Lo,Zhongxin Liu*

Main category: cs.SE

TL;DR: 提出了一种通过构建API使用示例和语义描述来推断内部API信息的方法，显著提升了代码补全的准确性。


<details>
  <summary>Details</summary>
Motivation: 现有方法在代码补全中难以有效利用未显式导入的内部API信息，影响了准确性。

Method: 扩展API表示，构建使用示例和语义描述，形成知识库供LLMs生成补全。同时引入ProjBench基准。

Result: 在ProjBench和CrossCodeEval上，代码精确匹配提升22.72%，标识符匹配提升18.31%。与现有基线结合后，提升更显著。

Conclusion: 该方法显著提升了代码补全的准确性，尤其在处理内部API信息时表现优异。

Abstract: Project-specific code completion is a critical task that leverages context
from a project to generate accurate code. State-of-the-art methods use
retrieval-augmented generation (RAG) with large language models (LLMs) and
project information for code completion. However, they often struggle to
incorporate internal API information, which is crucial for accuracy, especially
when APIs are not explicitly imported in the file.
  To address this, we propose a method to infer internal API information
without relying on imports. Our method extends the representation of APIs by
constructing usage examples and semantic descriptions, building a knowledge
base for LLMs to generate relevant completions. We also introduce ProjBench, a
benchmark that avoids leaked imports and consists of large-scale real-world
projects.
  Experiments on ProjBench and CrossCodeEval show that our approach
significantly outperforms existing methods, improving code exact match by
22.72% and identifier exact match by 18.31%. Additionally, integrating our
method with existing baselines boosts code match by 47.80% and identifier match
by 35.55%.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [32] [Towards the ideals of Self-Recovery and Metadata Privacy in Social Vault Recovery](https://arxiv.org/abs/2507.19484)
*Shailesh Mishra,Simone Colombo,Pasindu Tennage,Martin Burkhart,Bryan Ford*

Main category: cs.CR

TL;DR: Apollo框架通过分发不可区分的数据解决社交密钥恢复中的元数据隐私与记忆负担问题，采用多层秘密共享方案提升匿名集的可扩展性。


<details>
  <summary>Details</summary>
Motivation: 现有社交密钥恢复机制忽视了用户记忆恢复元数据的需求，如需要记住一定数量的受托人。

Method: 提出Apollo框架，通过分发不可区分的数据（受托人持有相关数据，非受托人存储随机数据）消除记忆负担，并采用多层秘密共享方案保护元数据隐私。

Result: Apollo将恶意恢复的概率降至0.005%至1.8%，多层设计将延迟降低1.1x至740kx。

Conclusion: Apollo在保护元数据隐私的同时，显著减轻了用户的记忆负担，并通过高效的多层设计提升了性能。

Abstract: Social key recovery mechanisms enable users to recover their vaults with the
help of trusted contacts, or trustees, avoiding the need for a single point of
trust or memorizing complex strings. However, existing mechanisms overlook the
memorability demands on users for recovery, such as the need to recall a
threshold number of trustees. Therefore, we first formalize the notion of
recovery metadata in the context of social key recovery, illustrating the
tradeoff between easing the burden of memorizing the metadata and maintaining
metadata privacy. We present Apollo, the first framework that addresses this
tradeoff by distributing indistinguishable data within a user's social circle,
where trustees hold relevant data and non-trustees store random data. Apollo
eliminates the need to memorize recovery metadata since a user eventually
gathers sufficient data from her social circle for recovery. Due to
indistinguishability, Apollo protects metadata privacy by forming an anonymity
set that hides the trustees among non-trustees. To make the anonymity set
scalable, Apollo proposes a novel multi-layered secret sharing scheme that
mitigates the overhead due to the random data distributed among non-trustees.
Finally, we provide a prototype implementation of Apollo and report on its
performance. Apollo reduces the chances of malicious recovery to between 0.005%
and 1.8%, depending on the adversary's ability to compromise. The multi-layered
design shows a latency reduction from 1.1x to 740kx compared to a
single-layered approach, depending on the number of reconnections.

</details>


### [33] [Securing the Internet of Medical Things (IoMT): Real-World Attack Taxonomy and Practical Security Measures](https://arxiv.org/abs/2507.19609)
*Suman Deb,Emil Lupu,Emm Mic Drakakis,Anil Anthony Bharath,Zhen Kit Leung,Guang Rui Ma,Anupam Chattopadhyay*

Main category: cs.CR

TL;DR: 本文探讨了医疗物联网（IoMT）的安全挑战，提出了攻击分类、漏洞分析及缓解策略，旨在为医疗设备工程师和安全专家提供实用指南。


<details>
  <summary>Details</summary>
Motivation: IoMT虽能提升医疗效率，但其连接性和智能性也带来了严重的网络安全威胁，可能危及患者安全和数据隐私。

Method: 通过分析历史网络事件，提出IoMT攻击分类、漏洞及缓解策略，并总结标准化框架。

Result: 揭示了IoMT安全与传统IT安全的差异，提出了设计安全IoMT系统的实用指南。

Conclusion: 本文为医疗行业提供了构建弹性且保护隐私的IoMT生态系统的实用建议，并强调了标准化框架的重要性。

Abstract: The Internet of Medical Things (IoMT) has the potential to radically improve
healthcare by enabling real-time monitoring, remote diagnostics, and AI-driven
decision making. However, the connectivity, embedded intelligence, and
inclusion of a wide variety of novel sensors expose medical devices to severe
cybersecurity threats, compromising patient safety and data privacy. In
addition, many devices also have direct capacity - individually or in
conjunction with other IoMT devices - to perform actions on the patient, such
as delivering an electrical stimulus, administering a drug, or activating a
motor, which can potentially be life-threatening. We provide a taxonomy of
potential attacks targeting IoMT, presenting attack surfaces, vulnerabilities,
and mitigation strategies across all layers of the IoMT architecture. It
answers key questions such as: What makes IoMT security different from
traditional IT security? What are the cybersecurity threats to medical devices?
How can engineers design secure IoMT systems and protect hospital networks from
cyberattacks? By analyzing historical cyber incidents, we highlight critical
security gaps and propose practical security guidelines for medical device
engineers and security professionals. This work bridges the gap between
research and implementation, equipping healthcare stakeholders with actionable
insights to build resilient and privacy-preserving IoMT ecosystems. Finally, we
present the latest standardization and compliance frameworks, that IoMT
security designers should be aware of.

</details>


### [34] [Trivial Trojans: How Minimal MCP Servers Enable Cross-Tool Exfiltration of Sensitive Data](https://arxiv.org/abs/2507.19880)
*Nicola Croce,Tobin South*

Main category: cs.CR

TL;DR: 论文展示了MCP（模型上下文协议）在AI工具集成中的安全漏洞，揭示了低技术门槛的攻击者如何利用其信任模型窃取敏感数据。


<details>
  <summary>Details</summary>
Motivation: 研究MCP在AI工具集成中的新兴安全威胁，特别是低技术门槛攻击者可能利用的漏洞。

Method: 通过概念验证攻击，展示恶意MCP服务器如何伪装成合法功能，利用银行工具窃取用户数据。

Result: 发现MCP生态系统中存在严重安全漏洞，攻击者仅需基本编程技能即可实施攻击。

Conclusion: 当前MCP实现允许跨服务器攻击，需立即采取缓解措施并改进协议以增强安全性。

Abstract: The Model Context Protocol (MCP) represents a significant advancement in
AI-tool integration, enabling seamless communication between AI agents and
external services. However, this connectivity introduces novel attack vectors
that remain largely unexplored. This paper demonstrates how unsophisticated
threat actors, requiring only basic programming skills and free web tools, can
exploit MCP's trust model to exfiltrate sensitive financial data. We present a
proof-of-concept attack where a malicious weather MCP server, disguised as
benign functionality, discovers and exploits legitimate banking tools to steal
user account balances. The attack chain requires no advanced technical
knowledge, server infrastructure, or monetary investment. The findings reveal a
critical security gap in the emerging MCP ecosystem: while individual servers
may appear trustworthy, their combination creates unexpected cross-server
attack surfaces. Unlike traditional cybersecurity threats that assume
sophisticated adversaries, our research shows that the barrier to entry for
MCP-based attacks is alarmingly low. A threat actor with undergraduate-level
Python knowledge can craft convincing social engineering attacks that exploit
the implicit trust relationships MCP establishes between AI agents and tool
providers. This work contributes to the nascent field of MCP security by
demonstrating that current MCP implementations allow trivial cross-server
attacks and proposing both immediate mitigations and protocol improvements to
secure this emerging ecosystem.

</details>


### [35] [ConSeg: Contextual Backdoor Attack Against Semantic Segmentation](https://arxiv.org/abs/2507.19905)
*Bilal Hussain Abbasi,Zirui Gong,Yanjun Zhang,Shang Gao,Antonio Robles-Kelly,Leo Zhang*

Main category: cs.CR

TL;DR: 本文提出了一种名为ConSeg的新型后门攻击方法，利用语义分割模型的上下文信息提高攻击成功率。


<details>
  <summary>Details</summary>
Motivation: 研究发现，当目标类别是受害类别的共现类别时，受害类别更容易被错误分割。基于此，ConSeg通过模仿目标类别的上下文信息来增强攻击效果。

Method: ConSeg通过重建受害区域的上下文信息，建立目标类别与受害类别之间的关联，从而简化攻击过程。

Result: 实验表明，ConSeg的攻击成功率（ASR）比现有方法提高了15.55%，且能抵抗最新的后门防御技术。

Conclusion: ConSeg是一种简单有效的后门攻击方法，展示了语义分割模型在上下文信息利用上的脆弱性。

Abstract: Despite significant advancements in computer vision, semantic segmentation
models may be susceptible to backdoor attacks. These attacks, involving hidden
triggers, aim to cause the models to misclassify instances of the victim class
as the target class when triggers are present, posing serious threats to the
reliability of these models. To further explore the field of backdoor attacks
against semantic segmentation, in this paper, we propose a simple yet effective
backdoor attack called Contextual Segmentation Backdoor Attack (ConSeg). ConSeg
leverages the contextual information inherent in semantic segmentation models
to enhance backdoor performance. Our method is motivated by an intriguing
observation, i.e., when the target class is set as the `co-occurring' class of
the victim class, the victim class can be more easily `mis-segmented'. Building
upon this insight, ConSeg mimics the contextual information of the target class
and rebuilds it in the victim region to establish the contextual relationship
between the target class and the victim class, making the attack easier. Our
experiments reveal that ConSeg achieves improvements in Attack Success Rate
(ASR) with increases of 15.55\%, compared to existing methods, while exhibiting
resilience against state-of-the-art backdoor defenses.

</details>


### [36] ["Blockchain-Enabled Zero Trust Framework for Securing FinTech Ecosystems Against Insider Threats and Cyber Attacks"](https://arxiv.org/abs/2507.19976)
*Avinash Singh,Vikas Pareek,Asish Sharma*

Main category: cs.CR

TL;DR: 本文提出了一种基于区块链的零信任框架，通过智能合约实现多因素认证和访问控制，显著提升金融科技的安全性。


<details>
  <summary>Details</summary>
Motivation: 传统边界防御机制无法应对金融科技中的高级网络威胁，如内部攻击和APT攻击，亟需更安全的解决方案。

Method: 利用以太坊智能合约构建零信任框架，结合MFA、RBAC和JIT访问权限，并通过DApp原型进行测试。

Result: 框架在安全性上表现优异，消除了单点故障，但略微增加了延迟（74.0 ms）并降低了吞吐量（30.77请求/秒）。

Conclusion: 该框架为金融科技提供了一种去中心化、高安全性的零信任实现方案，未来可通过Layer-2优化扩展性。

Abstract: Fintech provides technological services to increase operational efficiency in
financial institutions, but traditional perimeter-based defense mechanisms are
insufficient against evolving cyber threats like insider attacks, malware
intrusions, and Advanced Persistent Threats (APTs). These vulnerabilities
expose Fintech organizations to significant risks, including financial losses
and data breaches. To address these challenges, this paper proposes a
blockchain-integrated Zero Trust framework, adhering to the principle of "Never
Trust, Always Verify." The framework uses Ethereum smart contracts to enforce
Multi Factor Authentication (MFA), Role-Based Access Control (RBAC), and
Just-In-Time (JIT) access privileges, effectively mitigating credential theft
and insider threats, the effect of malware and APT attacks.
  The proposed solution transforms blockchain into a Policy Engine (PE) and
Policy Enforcement Point (PEP), and policy storage, ensuring immutable access
control and micro-segmentation. A decentralized application (DApp) prototype
was developed and tested using STRIDE threat modeling, demonstrating resilience
against spoofing, tampering, and privilege escalation. Comparative analysis
with Perimeter-based systems revealed a trade-off: while the framework
introduced a marginal latency increase (74.0 ms vs. 49.33 ms) and reduced
throughput (30.77 vs. 50.0 requests/sec), it significantly enhanced security by
eliminating single points of failure and enabling tamper-proof audit trails.
  Experimental validation on a 200-node simulated network confirmed the
framework's robustness, with future optimizations targeting Layer-2 solutions
for scalability. This work bridges the gap between Zero Trust theory and
practical blockchain implementation, offering Fintech organizations a
decentralized, cost-effective security model.

</details>


### [37] [Policy-Driven AI in Dataspaces: Taxonomy, Explainability, and Pathways for Compliant Innovation](https://arxiv.org/abs/2507.20014)
*Joydeep Chandra,Satyam Kumar Navneet*

Main category: cs.CR

TL;DR: 本文综述了隐私保护和政策感知的AI技术，提出了一种新的分类法，并分析了性能指标，指出了研究空白和未来方向。


<details>
  <summary>Details</summary>
Motivation: 随着AI驱动的数据空间在数据共享和协作分析中的重要性增加，确保隐私、性能和合规性成为挑战。

Method: 综述了多种隐私保护技术（如联邦学习、差分隐私等），并提出分类法。

Result: 分析了性能指标，指出了研究空白，如标准化KPI的缺乏。

Conclusion: 提出了未来方向，为开发可信、高效且合规的AI系统奠定了基础。

Abstract: As AI-driven dataspaces become integral to data sharing and collaborative
analytics, ensuring privacy, performance, and policy compliance presents
significant challenges. This paper provides a comprehensive review of
privacy-preserving and policy-aware AI techniques, including Federated
Learning, Differential Privacy, Trusted Execution Environments, Homomorphic
Encryption, and Secure Multi-Party Computation, alongside strategies for
aligning AI with regulatory frameworks such as GDPR and the EU AI Act. We
propose a novel taxonomy to classify these techniques based on privacy levels,
performance impacts, and compliance complexity, offering a clear framework for
practitioners and researchers to navigate trade-offs. Key performance metrics
-- latency, throughput, cost overhead, model utility, fairness, and
explainability -- are analyzed to highlight the multi-dimensional optimization
required in dataspaces. The paper identifies critical research gaps, including
the lack of standardized privacy-performance KPIs, challenges in explainable AI
for federated ecosystems, and semantic policy enforcement amidst regulatory
fragmentation. Future directions are outlined, proposing a conceptual framework
for policy-driven alignment, automated compliance validation, standardized
benchmarking, and integration with European initiatives like GAIA-X, IDS, and
Eclipse EDC. By synthesizing technical, ethical, and regulatory perspectives,
this work lays the groundwork for developing trustworthy, efficient, and
compliant AI systems in dataspaces, fostering innovation in secure and
responsible data-driven ecosystems.

</details>


### [38] [Cryptographic Data Exchange for Nuclear Warheads](https://arxiv.org/abs/2507.20074)
*Neil Perry,Daniil Zhukov*

Main category: cs.CR

TL;DR: 本文提出了一种基于密码学协议的核弹头追踪系统，利用承诺方案和zkSNARKs技术，实现非侵入式核弹头验证，确保条约合规性同时保护敏感数据。


<details>
  <summary>Details</summary>
Motivation: 传统核武器控制条约未涵盖核弹头验证，亟需一种安全、可验证且非侵入式的解决方案。

Method: 采用承诺方案和zkSNARKs技术，设计“核弹头护照”追踪系统，结合美俄双重哈希算法，确保数据安全和条约合规。

Result: 系统实现了核弹头生命周期的实时验证，防止数据篡改，并满足实际条约约束。

Conclusion: 该研究为核弹头验证提供了安全、可审计的解决方案，为未来条约实施奠定基础。

Abstract: Nuclear arms control treaties have historically focused on strategic nuclear
delivery systems, leaving nuclear warheads outside formal verification
frameworks. This paper presents a cryptographic protocol for secure and
verifiable warhead tracking, addressing challenges in nuclear warhead
verification without requiring intrusive physical inspections. Our system
leverages commitment schemes and zero-knowledge succinct non-interactive
arguments of knowledge (zkSNARKs) to ensure compliance with treaty constraints
while preserving the confidentiality of sensitive nuclear warhead data. We
propose a cryptographic "Warhead Passport" tracking system that chains
commitments to individual warheads over their life cycle, enabling periodic
challenges and real-time verification of treaty compliance. Our implementation
follows real-world treaty constraints, integrates U.S. and Russian dual-hash
combiners (SHA-family & GOST R 34.11 family) for cryptographic robustness and
political constraints, and ensures forward security by preventing retroactive
data manipulation. This work builds on policy research from prior arms control
studies and provides a practical foundation for implementing secure, auditable
NSNW verification mechanisms.

</details>


### [39] [SoK: Root Cause of \$1 Billion Loss in Smart Contract Real-World Attacks via a Systematic Literature Review of Vulnerabilities](https://arxiv.org/abs/2507.20175)
*Hadis Rezaei,Mojtaba Eshghie,Karl Anderesson,Francesco Palmieri*

Main category: cs.CR

TL;DR: 论文通过系统文献综述和实证分析，揭示了以太坊生态中高额损失的真正根源，提出了基于四层框架的漏洞分类方法。


<details>
  <summary>Details</summary>
Motivation: 尽管以太坊生态成熟，但每年仍有数十亿美元资产因攻击损失，现有研究多关注代码级漏洞，忽视了实际损失的根源。

Method: 采用双管齐下的方法：系统文献综述71篇论文，构建漏洞目录；实证分析50个严重攻击案例，识别根源。

Result: 发现攻击多由漏洞链（人类、操作、经济设计缺陷与代码漏洞结合）引发，提出四层根源框架。

Conclusion: 四层框架（协议逻辑设计、生命周期与治理、外部依赖、传统漏洞）更全面解释攻击根源，为未来研究提供新方向。

Abstract: The Ethereum ecosystem, despite its maturity, continues to witness
catastrophic attacks, with billions of dollars in assets lost annually. In
response, a significant body of research has focused on identifying and
mitigating smart contract vulnerabilities. However, these efforts predominantly
focus on implementation-level bugs, leaving a critical gap between academic
understanding of vulnerabilities and the root causes of real-world high-impact
financial losses. We employ a two-pronged methodology: first, a systematic
literature review of 71 academic papers to build a comprehensive and up-to-date
catalog of 24 active and 5 deprecated vulnerabilities as understood by the
research community. Second, we conduct an in-depth, empirical analysis of 50 of
the most severe real-world exploits between 2022 and 2025, collectively
incurring over \$1.09B in losses, to identify their true root causes. We
introduce the concept of "exploit chains" by revealing that many incidents are
not caused by isolated vulnerabilities but by combinations of human,
operational, and economic design flaws that link with implementation bugs to
enable an attack. Our analysis yields insights on how DApps are exploited in
practice, leading to a novel, four-tier root-cause framework that moves beyond
code-level vulnerabilities. We find that real-world successful attacks on
Ethereum (and related networks) trace back to one of the four tiers of (1)
protocol logic design, (2) lifecycle and governance, (3) external dependencies,
and (4) traditional implementation bugs (classic smart contract
vulnerabilities). We investigate the suitability of this multi-tier incident
root-cause framework via a case study.

</details>


### [40] [Measuring and Explaining the Effects of Android App Transformations in Online Malware Detection](https://arxiv.org/abs/2507.20361)
*Guozhu Meng,Zhixiu Guo,Xiaodong Zhang,Haoyu Wang,Kai Chen,Yang Liu*

Main category: cs.CR

TL;DR: 该研究通过数据驱动方法评估应用转换对恶意软件检测的影响，并揭示了杀毒引擎的工作原理。


<details>
  <summary>Details</summary>
Motivation: 杀毒引擎易受逃避技术（如混淆）影响，但原因不明，研究旨在量化这种影响并解释检测结果。

Method: 开发杀毒引擎交互模型，实现六种应用转换技术生成大量Android应用，并通过VirusTotal收集971K检测报告。

Result: 基于签名、静态和动态分析技术，分析报告揭示了杀毒引擎的内部工作机制和恶意软件检测的指标。

Conclusion: 研究识别了杀毒引擎的密封工作机制和恶意软件检测中的关键指标，提出了7个重要发现。

Abstract: It is well known that antivirus engines are vulnerable to evasion techniques
(e.g., obfuscation) that transform malware into its variants. However, it
cannot be necessarily attributed to the effectiveness of these evasions, and
the limits of engines may also make this unsatisfactory result. In this study,
we propose a data-driven approach to measure the effect of app transformations
to malware detection, and further explain why the detection result is produced
by these engines. First, we develop an interaction model for antivirus engines,
illustrating how they respond with different detection results in terms of
varying inputs. Six app transformation techniques are implemented in order to
generate a large number of Android apps with traceable changes. Then we
undertake a one-month tracking of app detection results from multiple antivirus
engines, through which we obtain over 971K detection reports from VirusTotal
for 179K apps in total. Last, we conduct a comprehensive analysis of antivirus
engines based on these reports from the perspectives of signature-based, static
analysis-based, and dynamic analysis-based detection techniques. The results,
together with 7 highlighted findings, identify a number of sealed working
mechanisms occurring inside antivirus engines and what are the indicators of
compromise in apps during malware detection.

</details>


### [41] [Is Crunching Public Data the Right Approach to Detect BGP Hijacks?](https://arxiv.org/abs/2507.20434)
*Alessandro Giaconia,Muoi Tran,Laurent Vanbever,Stefano Vissicchio*

Main category: cs.CR

TL;DR: 论文指出当前基于机器学习的BGP劫持检测系统（如DFOH和BEAM）易受数据投毒攻击，攻击者只需少量伪造公告即可绕过检测。


<details>
  <summary>Details</summary>
Motivation: BGP劫持问题持续存在，现有防御机制（如ROV）已被攻击者适应，需研究新的检测方法。

Method: 通过大规模BGP模拟，展示攻击者如何利用伪造公告绕过ML检测系统。

Result: 攻击者仅需少量精心设计的公告即可污染ML防御系统的知识库，使其失效。

Conclusion: 依赖公开BGP数据的检测系统存在严重弱点，需进一步改进防御机制。

Abstract: The Border Gateway Protocol (BGP) remains a fragile pillar of Internet
routing. BGP hijacks still occurr daily. While full deployment of Route Origin
Validation (ROV) is ongoing, attackers have already adapted, launching post-ROV
attacks such as forged-origin hijacks. To detect these, recent approaches like
DFOH [Holterbach et al., USENIX NSDI '24] and BEAM [Chen et al., USENIX
Security '24] apply machine learning (ML) to analyze data from globally
distributed BGP monitors, assuming anomalies will stand out against historical
patterns. However, this assumption overlooks a key threat: BGP monitors
themselves can be misled by adversaries injecting bogus routes. This paper
shows that state-of-the-art hijack detection systems like DFOH and BEAM are
vulnerable to data poisoning. Using large-scale BGP simulations, we show that
attackers can evade detection with just a handful of crafted announcements
beyond the actual hijack. These announcements are indeed sufficient to corrupt
the knowledge base used by ML-based defenses and distort the metrics they rely
on. Our results highlight a worrying weakness of relying solely on public BGP
data.

</details>


### [42] [MPC-EVM: Enabling MPC Execution by Smart Contracts In An Asynchronous Manner](https://arxiv.org/abs/2507.20554)
*Yichen Zhou,Chenxing Li,Fan Long*

Main category: cs.CR

TL;DR: MPC-EVM是首个支持智能合约在交易执行中异步调用MPC的区块链原型，保持了一致性和吞吐量。


<details>
  <summary>Details</summary>
Motivation: 扩展EVM以支持异步MPC调用，同时避免性能下降。

Method: 采用异步执行模型和非阻塞处理，结合访问控制机制防止状态不一致。

Result: 基准测试显示，MPC调用交易仅使TPS下降不到3%。

Conclusion: MPC-EVM在保持性能的同时，成功实现了异步MPC调用。

Abstract: This paper presents MPC-EVM, the first blockchain prototype that extends the
EVM to enable asynchronous MPC invocations by smart contracts during
transaction executions without compromising consistency or throughput. MPC-EVM
uses an asynchronous execution model to process MPC-invoking transactions in a
non-blocking fashion, saving the transaction's progress when it enters an MPC
and resuming its execution upon MPC's completion. Additionally, it employs an
access control mechanism that prevents inconsistent state access and
modifications as a result of asynchronous executions. Benchmarking MPC-EVM's
throughput show that the transactions per second (TPS) decreased by less than
3% compared to the baseline when MPC-invoking transactions are executed
alongside regular transactions.

</details>


### [43] [Hot-Swap MarkBoard: An Efficient Black-box Watermarking Approach for Large-scale Model Distribution](https://arxiv.org/abs/2507.20650)
*Zhicheng Zhang,Peizhuo Lv,Mengke Wan,Jiang Fang,Diandian Guo,Yezeng Chen,Yinlong Liu,Wei Ma,Jiyan Sun,Liru Geng*

Main category: cs.CR

TL;DR: 提出了一种名为Hot-Swap MarkBoard的高效水印方法，用于保护分布式深度学习模型的IP，支持无需重新训练的水印定制。


<details>
  <summary>Details</summary>
Motivation: 随着深度学习模型在终端设备上的部署增加，IP风险加剧，现有水印方法不适用于大规模分发场景。

Method: 通过在多分支LoRA模块中独立嵌入多个水印，实现高效水印定制，结合参数混淆机制防止水印移除。

Result: 在多种任务和模型上验证了方法的有效性，达到100%的验证准确率。

Conclusion: Hot-Swap MarkBoard是一种高效、适应性强的水印方法，适用于大规模模型分发场景。

Abstract: Recently, Deep Learning (DL) models have been increasingly deployed on
end-user devices as On-Device AI, offering improved efficiency and privacy.
However, this deployment trend poses more serious Intellectual Property (IP)
risks, as models are distributed on numerous local devices, making them
vulnerable to theft and redistribution. Most existing ownership protection
solutions (e.g., backdoor-based watermarking) are designed for cloud-based
AI-as-a-Service (AIaaS) and are not directly applicable to large-scale
distribution scenarios, where each user-specific model instance must carry a
unique watermark. These methods typically embed a fixed watermark, and
modifying the embedded watermark requires retraining the model. To address
these challenges, we propose Hot-Swap MarkBoard, an efficient watermarking
method. It encodes user-specific $n$-bit binary signatures by independently
embedding multiple watermarks into a multi-branch Low-Rank Adaptation (LoRA)
module, enabling efficient watermark customization without retraining through
branch swapping. A parameter obfuscation mechanism further entangles the
watermark weights with those of the base model, preventing removal without
degrading model performance. The method supports black-box verification and is
compatible with various model architectures and DL tasks, including
classification, image generation, and text generation. Extensive experiments
across three types of tasks and six backbone models demonstrate our method's
superior efficiency and adaptability compared to existing approaches, achieving
100\% verification accuracy.

</details>


### [44] [Program Analysis for High-Value Smart Contract Vulnerabilities: Techniques and Insights](https://arxiv.org/abs/2507.20672)
*Yannis Smaragdakis,Neville Grech,Sifis Lagouvardos,Konstantinos Triantafyllou,Ilias Tsatiris,Yannis Bollanos,Tony Rocco Valentine*

Main category: cs.CR

TL;DR: 论文展示了自动化技术在发现高价值智能合约漏洞中的成功应用，通过高完整性的静态分析和领域知识实现了显著成果。


<details>
  <summary>Details</summary>
Motivation: 反驳区块链安全社区普遍认为自动化技术只能检测低价值漏洞的观点，展示自动化技术在高价值漏洞发现中的潜力。

Method: 采用高完整性的静态分析方法，结合专家知识或统计推断获取的领域知识，并通过统计推断从大量合约中自动提取知识。

Result: 成功披露了多个高价值漏洞，获得10次漏洞赏金，总计超过300万美元，并在预部署或审计代码中发现数百个漏洞。

Conclusion: 高价值漏洞的自动化检测工具需要高完整性和可接受的精度，同时允许较高的假阳性率（如95%），以实际应用为导向。

Abstract: A widespread belief in the blockchain security community is that automated
techniques are only good for detecting shallow bugs, typically of small value.
In this paper, we present the techniques and insights that have led us to
repeatable success in automatically discovering high-value smart contract
vulnerabilities. Our vulnerability disclosures have yielded 10 bug bounties,
for a total of over $3M, over high-profile deployed code, as well as hundreds
of bugs detected in pre-deployment or under-audit code.
  We argue that the elements of this surprising success are a) a very
high-completeness static analysis approach that manages to maintain acceptable
precision; b) domain knowledge, provided by experts or captured via statistical
inference. We present novel techniques for automatically inferring domain
knowledge from statistical analysis of a large corpus of deployed contracts, as
well as discuss insights on the ideal precision and warning rate of a promising
vulnerability detector. In contrast to academic literature in program analysis,
which routinely expects false-positive rates below 50% for publishable results,
we posit that a useful analysis for high-value real-world vulnerabilities will
likely flag very few programs (under 1%) and will do so with a high
false-positive rate (e.g., 95%, meaning that only one-of-twenty human
inspections will yield an exploitable vulnerability).

</details>


### [45] [A Novel Post-Quantum Secure Digital Signature Scheme Based on Neural Network](https://arxiv.org/abs/2507.20676)
*Satish Kumar,Md. Arzoo Jamal*

Main category: cs.CR

TL;DR: 提出了一种基于神经网络的多元多项式数字签名方案，具备抗量子计算能力。


<details>
  <summary>Details</summary>
Motivation: 在量子计算时代，传统签名方案易受攻击，需设计抗量子威胁的新方案。神经网络能捕捉非线性关系，适合用于密码学设计。

Method: 利用二元权重神经网络定义签名方案核心结构，引入类似注意力机制的随机向量增强安全性。

Result: 方案在EUF-CMA下安全，私钥恢复攻击在多项式时间内不可行，且效率高。

Conclusion: 该方案在抗量子密码学中具有高效性和实用性。

Abstract: Digital signatures are fundamental cryptographic primitives that ensure the
authenticity and integrity of digital documents. In the post-quantum era,
classical public key-based signature schemes become vulnerable to brute-force
and key-recovery attacks due to the computational power of quantum algorithms.
Multivariate polynomial based signature schemes are among the one of the
cryptographic constructions that offers strong security guarantees against such
quantum threats. With the growing capabilities of neural networks, it is
natural to explore their potential application in the design of cryptographic
primitives. Neural networks inherently captures the non-linear relationships
within the data, which are encoded in their synaptic weight matrices and bias
vectors. In this paper, we propose a novel construction of a multivariate
polynomial based digital signature scheme that leverages neural network
architectures. A neural network with binary weights is employed to define the
central structure of the signature scheme. The design introduces a recurrent
random vector, functionally analogous to an attention mechanism, which
contributes dynamic randomness based on the previous state, thereby enhancing
the scheme's security. It is demonstrated that the proposed signature scheme
provide security against Existential Unforgeability under adaptive
Chosen-Message Attacks (EUF-CMA). Furthermore, it is proven that direct attacks
aimed to recover the private keys are computationally infeasible within
polynomial time, even in the presence of quantum computing abilities. The
operational characteristics of the proposed scheme are also evaluated, with
results indicating notable efficiency and practical viability in post-quantum
cryptographic applications.

</details>


### [46] [Guard-GBDT: Efficient Privacy-Preserving Approximated GBDT Training on Vertical Dataset](https://arxiv.org/abs/2507.20688)
*Anxiao Song,Shujie Cui,Jianli Bai,Ke Cheng,Yulong Shen,Giovanni Russello*

Main category: cs.CR

TL;DR: Guard-GBDT是一个高效且保护隐私的GBDT训练框架，通过优化计算和通信效率，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 解决现有MPC-based GBDT框架因高通信成本和非线性操作计算负担导致的效率问题。

Method: 采用简化的近似方法替代MPC不友好的除法和sigmoid函数，并通过压缩梯度聚合消息减少通信开销。

Result: 在LAN和WAN网络上分别比HEP-XGB和SiGBDT快2.71倍和12.21倍，准确性与SiGBDT和XGBoost相当。

Conclusion: Guard-GBDT在效率和隐私保护方面表现优异，适用于垂直数据集上的GBDT训练。

Abstract: In light of increasing privacy concerns and stringent legal regulations,
using secure multiparty computation (MPC) to enable collaborative GBDT model
training among multiple data owners has garnered significant attention. Despite
this, existing MPC-based GBDT frameworks face efficiency challenges due to high
communication costs and the computation burden of non-linear operations, such
as division and sigmoid calculations. In this work, we introduce Guard-GBDT, an
innovative framework tailored for efficient and privacy-preserving GBDT
training on vertical datasets. Guard-GBDT bypasses MPC-unfriendly division and
sigmoid functions by using more streamlined approximations and reduces
communication overhead by compressing the messages exchanged during gradient
aggregation. We implement a prototype of Guard-GBDT and extensively evaluate
its performance and accuracy on various real-world datasets. The results show
that Guard-GBDT outperforms state-of-the-art HEP-XGB (CIKM'21) and SiGBDT (ASIA
CCS'24) by up to $2.71\times$ and $12.21 \times$ on LAN network and up to
$2.7\times$ and $8.2\times$ on WAN network. Guard-GBDT also achieves comparable
accuracy with SiGBDT and plaintext XGBoost (better than HEP-XGB ), which
exhibits a deviation of $\pm1\%$ to $\pm2\%$ only. Our implementation code is
provided at https://github.com/XidianNSS/Guard-GBDT.git.

</details>


### [47] [An Open-source Implementation and Security Analysis of Triad's TEE Trusted Time Protocol](https://arxiv.org/abs/2507.20851)
*Matthieu Bettinger,Sonia Ben Mokhtar,Anthony Simonet-Boulogne*

Main category: cs.CR

TL;DR: 论文分析了Triad协议在可信执行环境（TEE）中的时间测量漏洞，展示了攻击者如何通过操纵时钟速度传播时间跳跃攻击，并提出了改进协议的建议。


<details>
  <summary>Details</summary>
Motivation: 研究Triad协议在TEE中的时间测量漏洞，揭示其在实际应用中的安全隐患。

Method: 通过公开实现的Triad协议进行实证分析，展示攻击者如何利用操作系统控制权操纵时钟速度。

Result: 发现Triad协议易受时钟速度操纵攻击，且攻击可通过诚实机器传播。

Conclusion: 提出了改进Triad协议的建议，以增强其对时钟操纵攻击的抵抗力。

Abstract: The logic of many protocols relies on time measurements. However, in Trusted
Execution Environments (TEEs) like Intel SGX, the time source is outside the
Trusted Computing Base: a malicious system hosting the TEE can manipulate that
TEE's notion of time, e.g., jumping in time or affecting the perceived time
speed. Previous work like Triad propose protocols for TEEs to maintain a
trustworthy time source. However, in this paper, based on a public
implementation of Triad that we contribute, we empirically showcase
vulnerabilities to this protocol. For example, an attacker controlling the
operating system, and consequently the scheduling algorithm, may arbitrarily
manipulate their local TEE's clock speed. What is worse, in case of faster
malicious clock speeds, an attacker on a single compromised machine may
propagate the attack to honest machines participating in Triad's Trusted Time
protocol, causing them to skip to timestamps arbitrarily far in the future.
Then, infected honest machines propagate time-skips themselves to other honest
machines interacting with them. We discuss protocol changes to Triad for higher
resilience against such attacks.

</details>


### [48] [Testbed and Software Architecture for Enhancing Security in Industrial Private 5G Networks](https://arxiv.org/abs/2507.20873)
*Song Son Ha,Florian Foerster,Thomas Robert Doebbert,Tim Kittel,Dominik Merli,Gerd Scholl*

Main category: cs.CR

TL;DR: 本文提出了一种用于增强私有5G网络安全的测试平台和软件架构，特别针对工业通信环境。


<details>
  <summary>Details</summary>
Motivation: 在工业4.0时代，5G网络的安全需求日益增长，但其部署面临显著的安全挑战，需要先进解决方案应对复杂的网络威胁。

Method: 提出了一种测试平台和软件架构，旨在提升私有5G网络的安全性。

Result: 该方法特别适用于工业通信环境，能够应对5G网络的安全挑战。

Conclusion: 该研究为私有5G网络的安全提供了有效的解决方案，尤其在工业应用中具有重要意义。

Abstract: In the era of Industry 4.0, the growing need for secure and efficient
communication systems has driven the development of fifth-generation (5G)
networks characterized by extremely low latency, massive device connectivity
and high data transfer speeds. However, the deployment of 5G networks presents
significant security challenges, requiring advanced and robust solutions to
counter increasingly sophisticated cyber threats. This paper proposes a testbed
and software architecture to strengthen the security of Private 5G Networks,
particularly in industrial communication environments.

</details>


### [49] [Characterizing the Sensitivity to Individual Bit Flips in Client-Side Operations of the CKKS Scheme](https://arxiv.org/abs/2507.20891)
*Matias Mazzanti,Augusto Vega,Esteban Mocskos*

Main category: cs.CR

TL;DR: 该论文研究了同态加密（HE）中CKKS方案对单比特翻转错误的容忍性，发现性能优化（如RNS/NTT）会显著增加其脆弱性。


<details>
  <summary>Details</summary>
Motivation: 随着HE在敏感应用中的广泛使用，确保其抗错误能力变得至关重要，但目前对CKKS方案在软错误（如比特翻转）下的表现缺乏系统性研究。

Method: 通过理论和实证分析，研究了CKKS方案在单比特翻转错误下的容错性，重点关注客户端操作（编码、加密、解密和解码）。

Result: 研究发现，虽然原始CKKS方案具有一定容错性，但性能优化（RNS/NTT）会显著增加其对错误的敏感性。

Conclusion: 通过揭示这些故障模式，为设计兼具性能和完整性的抗错HE方案奠定了基础。

Abstract: Homomorphic Encryption (HE) enables computation on encrypted data without
decryption, making it a cornerstone of privacy-preserving computation in
untrusted environments. As HE sees growing adoption in sensitive applications
such as secure machine learning and confidential data analysis ensuring its
robustness against errors becomes critical. Faults (e.g., transmission errors,
hardware malfunctions, or synchronization failures) can corrupt encrypted data
and compromise the integrity of HE operations. However, the impact of soft
errors (such as bit flips) on modern HE schemes remains unexplored.
Specifically, the CKKS scheme-one of the most widely used HE schemes for
approximate arithmetic-lacks a systematic study of how such errors propagate
across its pipeline, particularly under optimizations like the Residue Number
System (RNS) and Number Theoretic Transform (NTT). This work bridges that gap
by presenting a theoretical and empirical analysis of CKKS's fault tolerance
under single bit-flip errors. We focus on client-side operations (encoding,
encryption, decryption, and decoding) and demonstrate that while the vanilla
CKKS scheme exhibits some resilience, performance optimizations (RNS/NTT)
introduce significant fragility, amplifying error sensitivity. By
characterizing these failure modes, we lay the groundwork for error-resilient
HE designs, ensuring both performance and integrity in privacy-critical
applications.

</details>


### [50] [Development and analysis of a secured VoIP system for surveillance activities](https://arxiv.org/abs/2507.21038)
*M. Matsive Ali*

Main category: cs.CR

TL;DR: 论文提出了一种基于加密数据传输的VoIP系统，用于安全通信，结合了嵌入式系统和物联网技术。


<details>
  <summary>Details</summary>
Motivation: 传统电话通信逐渐被VoIP取代，但数据传输中的中间人攻击威胁安全性，因此需要一种安全的VoIP解决方案。

Method: 系统使用MAX9814麦克风采集声音，通过Particle Photon微控制器加密传输数据，并利用TCP服务器和Google Drive存储音频。

Result: 开发了一种紧凑、安全的VoIP设备，适用于汽车、监控系统等场景，确保通信信号完整性。

Conclusion: 该系统提供了一种安全且高效的VoIP通信方法，解决了数据传输中的安全隐患。

Abstract: Since the 1990s, the telephone has been the primary mode of communication.
However, Voice over Internet Protocol (VoIP), which is a highly straightforward
and affordable form of data transfer, is now becoming an important part of
daily communication. VoIP is the technology that makes it possible to send
speech and multimedia data packets across either a public or private IP
network. However, a cyberattack known as a man-in-the-middle attack poses a
serious concern in transferring data through any network. Therefore, the
authors have designed a system that sends voice over the internet within the
range of a router using encrypted data transfer. An embedded system comprising
an electret microphone, Embedded C, Node.js, Particle Photon microcontroller,
and Internet of Things (IoT) technology is developed. Due to its compact size,
this type of device may be incorporated into automobiles, surveillance systems,
or covert listening tools. The VoIP system gathers sound signals using the
MAX9814 microphone, while the Particle Photon microcontroller securely
transmits the data. Devices with access can download data from the VoIP systems
Transmission Control Protocol (TCP) server. The accessed device stores the
audio locally and uploads the corresponding data to Google Drive. This VoIP
system provides a secure method of communication while conserving the integrity
of the original signal.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [51] [MAIA: A Collaborative Medical AI Platform for Integrated Healthcare Innovation](https://arxiv.org/abs/2507.19489)
*Simone Bendazzoli,Sanna Persson,Mehdi Astaraki,Sebastian Pettersson,Vitali Grozman,Rodrigo Moreno*

Main category: cs.AI

TL;DR: MAIA是一个开源平台，旨在促进临床医生、研究人员和AI开发者之间的跨学科合作，加速AI研究向临床应用的转化。


<details>
  <summary>Details</summary>
Motivation: 解决AI技术创新与医疗实践之间的鸿沟，促进协作与互操作性。

Method: 基于Kubernetes构建，提供模块化、可扩展的环境，集成数据管理、模型开发、注释、部署和临床反馈工具。

Result: 在学术和临床环境中成功部署，支持医学影像AI的实际用例。

Conclusion: MAIA通过促进协作和互操作性，加速AI研究向临床解决方案的转化，同时强调可重复性、透明性和以用户为中心的设计。

Abstract: The integration of Artificial Intelligence (AI) into clinical workflows
requires robust collaborative platforms that are able to bridge the gap between
technical innovation and practical healthcare applications. This paper
introduces MAIA (Medical Artificial Intelligence Assistant), an open-source
platform designed to facilitate interdisciplinary collaboration among
clinicians, researchers, and AI developers. Built on Kubernetes, MAIA offers a
modular, scalable environment with integrated tools for data management, model
development, annotation, deployment, and clinical feedback. Key features
include project isolation, CI/CD automation, integration with high-computing
infrastructures and in clinical workflows. MAIA supports real-world use cases
in medical imaging AI, with deployments in both academic and clinical
environments. By promoting collaborations and interoperability, MAIA aims to
accelerate the translation of AI research into impactful clinical solutions
while promoting reproducibility, transparency, and user-centered design. We
showcase the use of MAIA with different projects, both at KTH Royal Institute
of Technology and Karolinska University Hospital.

</details>


### [52] [Agent WARPP: Workflow Adherence via Runtime Parallel Personalization](https://arxiv.org/abs/2507.19543)
*Maria Emilia Mazzolenis,Ruirui Zhang*

Main category: cs.AI

TL;DR: WARPP是一个无需训练的模块化框架，通过多智能体编排和运行时个性化，提升基于LLM的任务导向对话系统的工作流遵循能力。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLM）在任务导向对话系统中应用广泛，但在处理涉及外部工具调用和用户特定信息的长条件工作流时表现不佳。

Method: WARPP结合多智能体编排和运行时个性化，动态修剪条件分支并优化工具选择，采用并行架构，由专用Personalizer智能体与领域特定智能体协同工作。

Result: 在银行、航班和医疗三个领域的五种用户意图测试中，WARPP在参数保真度和工具准确性上优于非个性化方法和ReAct基线，同时减少平均token使用。

Conclusion: WARPP无需额外训练即可显著提升复杂意图下的性能，为LLM在任务导向对话系统中的应用提供了高效解决方案。

Abstract: Large language models (LLMs) are increasingly applied in task-oriented
dialogue (TOD) systems but often struggle with long, conditional workflows that
involve external tool calls and depend on user-specific information. We present
Workflow Adherence via Runtime Parallel Personalization, or WARPP, a
training-free, modular framework that combines multi-agent orchestration with
runtime personalization to improve workflow adherence in LLM-based systems. By
dynamically pruning conditional branches based on user attributes, the
framework reduces reasoning overhead and narrows tool selection at runtime.
WARPP deploys a parallelized architecture where a dedicated Personalizer agent
operates alongside modular, domain-specific agents to dynamically tailor
execution paths in real time. The framework is evaluated across five
representative user intents of varying complexity within three domains:
banking, flights, and healthcare. Our evaluation leverages synthetic datasets
and LLM-powered simulated users to test scenarios with conditional
dependencies. Our results demonstrate that WARPP outperforms both the
non-personalized method and the ReAct baseline, achieving increasingly larger
gains in parameter fidelity and tool accuracy as intent complexity grows, while
also reducing average token usage, without any additional training.

</details>


### [53] [Hypergames: Modeling Misaligned Perceptions and Nested Beliefs for Multi-agent Systems](https://arxiv.org/abs/2507.19593)
*Vince Trencsenyi,Agnieszka Mensfelt,Kostas Stathis*

Main category: cs.AI

TL;DR: 本文系统回顾了超博弈理论在动态多智能体系统中的应用，分析了44项研究，提出了智能体兼容性标准和分类框架，并指出了研究中的结构差距和未来方向。


<details>
  <summary>Details</summary>
Motivation: 传统博弈论假设理性、完全信息和共同知识，但现实中的多智能体系统常存在不确定性和认知差异。超博弈理论通过建模主观感知弥补这些不足。

Method: 通过系统回顾44项研究，引入超博弈理论及其扩展（如分层超博弈和HNF），提出智能体兼容性标准和分类框架。

Result: 分析揭示了分层和图模型在欺骗推理中的主导地位，以及理论框架在实际应用中的简化。同时指出了HNF模型采用不足、缺乏形式化语言等问题。

Conclusion: 本文为超博弈理论在动态多智能体环境中的应用提供了新路线图，以增强战略建模的现实性和有效性。

Abstract: Classical game-theoretic models typically assume rational agents, complete
information, and common knowledge of payoffs - assumptions that are often
violated in real-world MAS characterized by uncertainty, misaligned
perceptions, and nested beliefs. To overcome these limitations, researchers
have proposed extensions that incorporate models of cognitive constraints,
subjective beliefs, and heterogeneous reasoning. Among these, hypergame theory
extends the classical paradigm by explicitly modeling agents' subjective
perceptions of the strategic scenario, known as perceptual games, in which
agents may hold divergent beliefs about the structure, payoffs, or available
actions. We present a systematic review of agent-compatible applications of
hypergame theory, examining how its descriptive capabilities have been adapted
to dynamic and interactive MAS contexts. We analyze 44 selected studies from
cybersecurity, robotics, social simulation, communications, and general
game-theoretic modeling. Building on a formal introduction to hypergame theory
and its two major extensions - hierarchical hypergames and HNF - we develop
agent-compatibility criteria and an agent-based classification framework to
assess integration patterns and practical applicability. Our analysis reveals
prevailing tendencies, including the prevalence of hierarchical and graph-based
models in deceptive reasoning and the simplification of extensive theoretical
frameworks in practical applications. We identify structural gaps, including
the limited adoption of HNF-based models, the lack of formal hypergame
languages, and unexplored opportunities for modeling human-agent and
agent-agent misalignment. By synthesizing trends, challenges, and open research
directions, this review provides a new roadmap for applying hypergame theory to
enhance the realism and effectiveness of strategic modeling in dynamic
multi-agent environments.

</details>


### [54] [DeltaLLM: A Training-Free Framework Exploiting Temporal Sparsity for Efficient Edge LLM Inference](https://arxiv.org/abs/2507.19608)
*Jiawen Qi,Chang Gao,Zhaochun Ren,Qinyu Chen*

Main category: cs.AI

TL;DR: DeltaLLM是一个无需训练的框架，通过利用注意力模式的时间稀疏性，在资源受限的边缘设备上实现高效的大型语言模型推理。


<details>
  <summary>Details</summary>
Motivation: 由于大型语言模型在边缘设备上的计算需求随序列长度呈二次增长，现有动态注意力剪枝方法不适合边缘场景。

Method: DeltaLLM采用精度和内存感知的delta矩阵构建策略，结合上下文感知的混合注意力机制。

Result: 在BitNet和Llama模型上，DeltaLLM将注意力稀疏性提升至60%，同时保持或提高任务精度。

Conclusion: DeltaLLM为边缘设备提供了一种无需微调的高效部署方案。

Abstract: Deploying Large Language Models (LLMs) on edge devices remains challenging
due to their quadratically increasing computations with the sequence length.
Existing studies for dynamic attention pruning are designed for hardware with
massively parallel computation capabilities, such as GPUs or TPUs, and aim at
long context lengths (e.g., 64K), making them unsuitable for edge scenarios. We
present DeltaLLM, a training-free framework that exploits temporal sparsity in
attention patterns to enable efficient LLM inference across both the prefilling
and decoding stages, on resource-constrained edge devices. DeltaLLM introduces
an accuracy- and memory-aware delta matrix construction strategy that
introduces temporal sparsity, and a context-aware hybrid attention mechanism
that combines full attention in a local context window with delta approximation
outside it to increase accuracy. We evaluate our framework on the
edge-device-friendly BitNet-b1.58-2B-4T model and Llama3.2-1B-Instruct model
across diverse language tasks. The results show that on BitNet, our framework
increases the attention sparsity from 0% to 60% during the prefilling stage
with slight accuracy improvement on the WG task, and 0% to 57% across both the
prefilling and decoding stages, with even higher F1 score from 29.63 to 30.97
on SQuAD-v2 task. On the Llama model, it can also achieve up to 60% sparsity
during the prefilling stage and around 57% across both stages with negligible
accuracy drop. These results demonstrate that DeltaLLM offers a promising
solution for efficient edge deployment, requiring no fine-tuning and seamlessly
integrating with existing inference pipelines.

</details>


### [55] [Alignment and Safety in Large Language Models: Safety Mechanisms, Training Paradigms, and Emerging Challenges](https://arxiv.org/abs/2507.19672)
*Haoran Lu,Luyang Fang,Ruidong Zhang,Xinliang Li,Jiazhang Cai,Huimin Cheng,Lin Tang,Ziyu Liu,Zeliang Sun,Tao Wang,Yingchuan Zhang,Arif Hassan Zidan,Jinwen Xu,Jincheng Yu,Meizhi Yu,Hanqi Jiang,Xilin Gong,Weidi Luo,Bolun Sun,Yongkai Chen,Terry Ma,Shushan Wu,Yifan Zhou,Junhao Chen,Haotian Xiang,Jing Zhang,Afrar Jahin,Wei Ruan,Ke Deng,Yi Pan,Peilong Wang,Jiahui Li,Zhengliang Liu,Lu Zhang,Lin Zhao,Wei Liu,Dajiang Zhu,Xin Xing,Fei Dou,Wei Zhang,Chao Huang,Rongjie Liu,Mengrui Zhang,Yiwen Liu,Xiaoxiao Sun,Qin Lu,Zhen Xiang,Wenxuan Zhong,Tianming Liu,Ping Ma*

Main category: cs.AI

TL;DR: 本文综述了大型语言模型（LLM）对齐人类价值观和意图的实用技术、训练协议和实证研究，分析了不同范式下的对齐方法及其核心目标之间的权衡。


<details>
  <summary>Details</summary>
Motivation: 随着LLM能力的显著提升及其对社会影响的扩大，确保其与人类价值观和意图的对齐成为关键挑战。

Method: 分析了监督微调、基于偏好的方法等对齐技术，并讨论了包括DPO、Constitutional AI等前沿方法。

Result: 研究表明，基于偏好的方法在灵活对齐人类意图方面优于监督微调，但仍存在奖励错误设定、分布鲁棒性等局限性。

Conclusion: 总结了当前实践中的策略，并提出了监督、价值多元性、鲁棒性和持续对齐等开放问题，为研究者和从业者提供参考。

Abstract: Due to the remarkable capabilities and growing impact of large language
models (LLMs), they have been deeply integrated into many aspects of society.
Thus, ensuring their alignment with human values and intentions has emerged as
a critical challenge. This survey provides a comprehensive overview of
practical alignment techniques, training protocols, and empirical findings in
LLM alignment. We analyze the development of alignment methods across diverse
paradigms, characterizing the fundamental trade-offs between core alignment
objectives. Our analysis shows that while supervised fine-tuning enables basic
instruction-following, preference-based methods offer more flexibility for
aligning with nuanced human intent. We discuss state-of-the-art techniques,
including Direct Preference Optimization (DPO), Constitutional AI,
brain-inspired methods, and alignment uncertainty quantification (AUQ),
highlighting their approaches to balancing quality and efficiency. We review
existing evaluation frameworks and benchmarking datasets, emphasizing
limitations such as reward misspecification, distributional robustness, and
scalable oversight. We summarize strategies adopted by leading AI labs to
illustrate the current state of practice. We conclude by outlining open
problems in oversight, value pluralism, robustness, and continuous alignment.
This survey aims to inform both researchers and practitioners navigating the
evolving landscape of LLM alignment.

</details>


### [56] [The wall confronting large language models](https://arxiv.org/abs/2507.19703)
*Peter V. Coveney,Sauro Succi*

Main category: cs.AI

TL;DR: 大型语言模型（LLM）的性能受限于其预测不确定性的提升能力，难以满足科学研究的可靠性标准。其学习机制可能导致错误累积和信息灾难，而数据规模的增加进一步加剧了这一问题。避免这种退化路径需要更注重问题的结构性理解。


<details>
  <summary>Details</summary>
Motivation: 探讨LLM在提升预测可靠性方面的局限性，揭示其学习机制与准确性之间的冲突，并提出避免退化AI行为的解决方案。

Method: 分析LLM的缩放定律及其对预测不确定性的影响，结合数据规模与虚假相关性的关系，探讨LLM的潜在退化路径。

Result: LLM的缩放定律限制了其预测可靠性的提升，其学习机制可能导致错误累积和信息灾难，数据规模的增加加剧了这一问题。

Conclusion: 为避免LLM的退化行为，需更注重对问题结构性的深入理解，而非单纯依赖数据规模的扩展。

Abstract: We show that the scaling laws which determine the performance of large
language models (LLMs) severely limit their ability to improve the uncertainty
of their predictions. As a result, raising their reliability to meet the
standards of scientific inquiry is intractable by any reasonable measure. We
argue that the very mechanism which fuels much of the learning power of LLMs,
namely the ability to generate non-Gaussian output distributions from Gaussian
input ones, might well be at the roots of their propensity to produce error
pileup, ensuing information catastrophes and degenerative AI behaviour. This
tension between learning and accuracy is a likely candidate mechanism
underlying the observed low values of the scaling components. It is
substantially compounded by the deluge of spurious correlations pointed out by
Calude and Longo which rapidly increase in any data set merely as a function of
its size, regardless of its nature. The fact that a degenerative AI pathway is
a very probable feature of the LLM landscape does not mean that it must
inevitably arise in all future AI research. Its avoidance, which we also
discuss in this paper, necessitates putting a much higher premium on insight
and understanding of the structural characteristics of the problems being
investigated.

</details>


### [57] [Minding Motivation: The Effect of Intrinsic Motivation on Agent Behaviors](https://arxiv.org/abs/2507.19725)
*Leonardo Villalobos-Arias,Grant Forbes,Jianxun Wang,David L Roberts,Arnav Jhala*

Main category: cs.AI

TL;DR: 论文研究了内在动机（IM）方法在强化学习（RL）中如何改变智能体行为，并评估了三种IM技术对MiniGrid环境的影响。研究发现IM会显著改变智能体行为，而广义奖励匹配（GRM）可以部分缓解奖励黑客问题。


<details>
  <summary>Details</summary>
Motivation: 游戏中的奖励稀疏性对RL智能体构成挑战，IM方法虽能解决这一问题，但可能引发奖励黑客行为。目前尚不清楚IM如何具体改变智能体行为。

Method: 在MiniGrid环境中实证评估三种IM技术的影响，并与广义奖励匹配（GRM）方法进行比较。

Result: IM显著提高了初始奖励并改变了智能体行为方式，而GRM在某些场景下能缓解奖励黑客问题。

Conclusion: IM确实会改变RL智能体的行为，而GRM是一种有效的缓解手段。

Abstract: Games are challenging for Reinforcement Learning~(RL) agents due to their
reward-sparsity, as rewards are only obtainable after long sequences of
deliberate actions. Intrinsic Motivation~(IM) methods -- which introduce
exploration rewards -- are an effective solution to reward-sparsity. However,
IM also causes an issue known as `reward hacking' where the agent optimizes for
the new reward at the expense of properly playing the game. The larger problem
is that reward hacking itself is largely unknown; there is no answer to
whether, and to what extent, IM rewards change the behavior of RL agents. This
study takes a first step by empirically evaluating the impact on behavior of
three IM techniques on the MiniGrid game-like environment. We compare these IM
models with Generalized Reward Matching~(GRM), a method that can be used with
any intrinsic reward function to guarantee optimality. Our results suggest that
IM causes noticeable change by increasing the initial rewards, but also
altering the way the agent plays; and that GRM mitigated reward hacking in some
scenarios.

</details>


### [58] [HypKG: Hypergraph-based Knowledge Graph Contextualization for Precision Healthcare](https://arxiv.org/abs/2507.19726)
*Yuzhang Xie,Xu Han,Ran Xu,Xiao Hu,Jiaying Lu,Carl Yang*

Main category: cs.AI

TL;DR: HypKG框架通过整合电子健康记录（EHRs）和知识图谱（KGs），生成上下文知识表示，提升医疗预测准确性。


<details>
  <summary>Details</summary>
Motivation: 现有知识图谱缺乏患者具体上下文信息，而电子健康记录提供了丰富的个人数据，两者结合可优化精准医疗。

Method: 采用实体链接技术连接KGs和EHRs，利用超图模型和超图变换器学习上下文表示。

Result: 实验显示HypKG在多指标下显著提升医疗预测性能，并能调整KGs中实体和关系的表示。

Conclusion: HypKG通过整合外部上下文，提升了知识图谱的实用性和医疗预测的准确性。

Abstract: Knowledge graphs (KGs) are important products of the semantic web, which are
widely used in various application domains. Healthcare is one of such domains
where KGs are intensively used, due to the high requirement for knowledge
accuracy and interconnected nature of healthcare data. However, KGs storing
general factual information often lack the ability to account for important
contexts of the knowledge such as the status of specific patients, which are
crucial in precision healthcare. Meanwhile, electronic health records (EHRs)
provide rich personal data, including various diagnoses and medications, which
provide natural contexts for general KGs. In this paper, we propose HypKG, a
framework that integrates patient information from EHRs into KGs to generate
contextualized knowledge representations for accurate healthcare predictions.
Using advanced entity-linking techniques, we connect relevant knowledge from
general KGs with patient information from EHRs, and then utilize a hypergraph
model to "contextualize" the knowledge with the patient information. Finally,
we employ hypergraph transformers guided by downstream prediction tasks to
jointly learn proper contextualized representations for both KGs and patients,
fully leveraging existing knowledge in KGs and patient contexts in EHRs. In
experiments using a large biomedical KG and two real-world EHR datasets, HypKG
demonstrates significant improvements in healthcare prediction tasks across
multiple evaluation metrics. Additionally, by integrating external contexts,
HypKG can learn to adjust the representations of entities and relations in KG,
potentially improving the quality and real-world utility of knowledge.

</details>


### [59] [Integrating Activity Predictions in Knowledge Graphs](https://arxiv.org/abs/2507.19733)
*Alec Scully,Cameron Stockton,Forrest Hare*

Main category: cs.AI

TL;DR: 论文提出利用本体结构知识图谱预测未来事件，结合BFO和CCO语义框架组织数据，通过马尔可夫链模型预测未来状态，并改进概率的建模方式。


<details>
  <summary>Details</summary>
Motivation: 探讨如何利用本体结构知识图谱提升对未来事件的预测能力，同时改进现有概率模型的问题。

Method: 使用BFO和CCO构建知识图谱，通过SPARQL查询数据并生成马尔可夫链模型，引入‘时空实例’概念完善语义结构。

Result: 成功将马尔可夫链概率计算整合回知识图谱，支持进一步分析和决策。

Conclusion: 本体结构知识图谱结合改进的概率模型能有效预测未来事件，为动态现象建模提供新思路。

Abstract: We argue that ontology-structured knowledge graphs can play a crucial role in
generating predictions about future events. By leveraging the semantic
framework provided by Basic Formal Ontology (BFO) and Common Core Ontologies
(CCO), we demonstrate how data such as the movements of a fishing vessel can be
organized in and retrieved from a knowledge graph. These query results are then
used to create Markov chain models, allowing us to predict future states based
on the vessel's history. To fully support this process, we introduce the term
`spatiotemporal instant' to complete the necessary structural semantics.
Additionally, we critique the prevailing ontological model of probability,
which conflates probability with likelihood and relies on the problematic
concept of modal measurements: measurements of future entities. We propose an
alternative view, where probabilities are treated as being about process
profiles, which better captures the dynamics of real world phenomena. Finally,
we demonstrate how our Markov chain based probability calculations can be
seamlessly integrated back into the knowledge graph, enabling further analysis
and decision-making. Keywords: predictive analytics, ontology, Markov chains,
probability, Basic Formal Ontology (BFO), knowledge graphs, SPARQL.

</details>


### [60] [Can LLMs Solve ASP Problems? Insights from a Benchmarking Study (Extended Version)](https://arxiv.org/abs/2507.19749)
*Lin Ren,Guohui Xiao,Guilin Qi,Yishuai Geng,Haohan Xue*

Main category: cs.AI

TL;DR: ASPBench是一个针对答案集编程（ASP）的全面基准测试，揭示了当前大型语言模型（LLM）在ASP核心任务上的局限性。


<details>
  <summary>Details</summary>
Motivation: 当前对LLM在ASP中能力的评估过于简化，缺乏支持否定、析取或多答案集的复杂任务，需要更全面的基准测试。

Method: 引入ASPBench，包含三个ASP特定任务：ASP蕴含、答案集验证和答案集计算，并对14种先进LLM进行评估。

Result: LLM在前两个简单任务上表现较好，但在核心的答案集计算任务上表现不佳。

Conclusion: LLM在ASP解决中的局限性凸显了需要更有效整合符号推理能力的新方法。

Abstract: Answer Set Programming (ASP) is a powerful paradigm for non-monotonic
reasoning. Recently, large language models (LLMs) have demonstrated promising
capabilities in logical reasoning. Despite this potential, current evaluations
of LLM capabilities in ASP are often limited. Existing works normally employ
overly simplified ASP programs, do not support negation, disjunction, or
multiple answer sets. Furthermore, there is a lack of benchmarks that introduce
tasks specifically designed for ASP solving. To bridge this gap, we introduce
ASPBench, a comprehensive ASP benchmark, including three ASP specific tasks:
ASP entailment, answer set verification, and answer set computation. Our
extensive evaluations on ASPBench reveal that while 14 state-of-the-art LLMs,
including \emph{deepseek-r1}, \emph{o4-mini}, and
\emph{gemini-2.5-flash-thinking}, perform relatively well on the first two
simpler tasks, they struggle with answer set computation, which is the core of
ASP solving. These findings offer insights into the current limitations of LLMs
in ASP solving. This highlights the need for new approaches that integrate
symbolic reasoning capabilities more effectively. The code and dataset are
available at https://github.com/HomuraT/ASPBench.

</details>


### [61] [Reinforcement Learning for Multi-Objective Multi-Echelon Supply Chain Optimisation](https://arxiv.org/abs/2507.19788)
*Rifny Rachman,Josh Tingey,Richard Allmendinger,Pradyumn Shukla,Wei Pan*

Main category: cs.AI

TL;DR: 该研究开发了一个基于马尔可夫决策过程的广义多目标、多层次供应链优化模型，结合经济、环境和社会因素，并通过多目标强化学习方法进行评估。


<details>
  <summary>Details</summary>
Motivation: 解决非稳态市场中供应链优化的复杂问题，同时平衡经济、环境和社会目标。

Method: 采用多目标强化学习（RL）方法，并与改进的单目标RL算法和多目标进化算法（MOEA）进行对比。

Result: 主要方法在最优性、多样性和密度方面表现最佳，复杂场景下比MOEA方法高出75%的超体积，解决方案密度是改进单目标RL方法的11倍。

Conclusion: 该方法在稳定生产和库存水平的同时，显著减少了需求损失，提供了更优的权衡解决方案。

Abstract: This study develops a generalised multi-objective, multi-echelon supply chain
optimisation model with non-stationary markets based on a Markov decision
process, incorporating economic, environmental, and social considerations. The
model is evaluated using a multi-objective reinforcement learning (RL) method,
benchmarked against an originally single-objective RL algorithm modified with
weighted sum using predefined weights, and a multi-objective evolutionary
algorithm (MOEA)-based approach. We conduct experiments on varying network
complexities, mimicking typical real-world challenges using a customisable
simulator. The model determines production and delivery quantities across
supply chain routes to achieve near-optimal trade-offs between competing
objectives, approximating Pareto front sets. The results demonstrate that the
primary approach provides the most balanced trade-off between optimality,
diversity, and density, further enhanced with a shared experience buffer that
allows knowledge transfer among policies. In complex settings, it achieves up
to 75\% higher hypervolume than the MOEA-based method and generates solutions
that are approximately eleven times denser, signifying better robustness, than
those produced by the modified single-objective RL method. Moreover, it ensures
stable production and inventory levels while minimising demand loss.

</details>


### [62] [Causality-aligned Prompt Learning via Diffusion-based Counterfactual Generation](https://arxiv.org/abs/2507.19882)
*Xinshu Li,Ruoyu Wang,Erdun Gao,Mingming Gong,Lina Yao*

Main category: cs.AI

TL;DR: DiCap模型是一种基于扩散的反事实提示学习框架，通过理论推导生成因果不变提示，显著提升跨类别泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有提示学习方法因缺乏理论支持，难以生成因果不变提示，导致特征提取不鲁棒。

Method: 利用扩散过程从因果模型的边际和条件分布中迭代采样梯度，生成满足最小充分性准则的反事实，并结合对比学习框架优化提示。

Result: 在图像分类、图文检索和视觉问答等任务中表现优异，尤其在未见类别上优势显著。

Conclusion: DiCap通过理论驱动的反事实生成，显著提升了提示学习的鲁棒性和泛化能力。

Abstract: Prompt learning has garnered attention for its efficiency over traditional
model training and fine-tuning. However, existing methods, constrained by
inadequate theoretical foundations, encounter difficulties in achieving
causally invariant prompts, ultimately falling short of capturing robust
features that generalize effectively across categories. To address these
challenges, we introduce the $\textit{\textbf{DiCap}}$ model, a theoretically
grounded $\textbf{Di}$ffusion-based $\textbf{C}$ounterf$\textbf{a}$ctual
$\textbf{p}$rompt learning framework, which leverages a diffusion process to
iteratively sample gradients from the marginal and conditional distributions of
the causal model, guiding the generation of counterfactuals that satisfy the
minimal sufficiency criterion. Grounded in rigorous theoretical derivations,
this approach guarantees the identifiability of counterfactual outcomes while
imposing strict bounds on estimation errors. We further employ a contrastive
learning framework that leverages the generated counterfactuals, thereby
enabling the refined extraction of prompts that are precisely aligned with the
causal features of the data. Extensive experimental results demonstrate that
our method performs excellently across tasks such as image classification,
image-text retrieval, and visual question answering, with particularly strong
advantages in unseen categories.

</details>


### [63] [What Does 'Human-Centred AI' Mean?](https://arxiv.org/abs/2507.19960)
*Olivia Guest*

Main category: cs.AI

TL;DR: 论文探讨了以人为中心的人工智能（AI）本质上是技术与人类认知的关系，分析了AI对人类认知劳动的替代、增强或取代，并强调必须正视人类在AI中的作用以避免认知科学的扭曲。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于澄清AI与人类认知的关系，避免因技术模糊性导致对人类认知的误解，从而真正实现以人为中心的AI设计。

Method: 通过对比技术（如算盘、闹钟、相机）与人类认知劳动（如心算、人工叫醒、视觉）的例子，提出新的定义和分析框架，将社会技术关系分为替代（有害）、增强（有益）和取代（中性）三类。

Result: 研究表明，所有AI都涉及人类认知，而忽视这一点会导致认知科学的扭曲，限制AI系统真正以人为中心的设计。

Conclusion: 结论强调必须正视人类在AI中的作用，避免认知模糊，才能实现真正以人为中心的AI工程。

Abstract: While it seems sensible that human-centred artificial intelligence (AI) means
centring "human behaviour and experience," it cannot be any other way. AI, I
argue, is usefully seen as a relationship between technology and humans where
it appears that artifacts can perform, to a greater or lesser extent, human
cognitive labour. This is evinced using examples that juxtapose technology with
cognition, inter alia: abacus versus mental arithmetic; alarm clock versus
knocker-upper; camera versus vision; and sweatshop versus tailor. Using novel
definitions and analyses, sociotechnical relationships can be analysed into
varying types of: displacement (harmful), enhancement (beneficial), and/or
replacement (neutral) of human cognitive labour. Ultimately, all AI implicates
human cognition; no matter what. Obfuscation of cognition in the AI context --
from clocks to artificial neural networks -- results in distortion, in slowing
critical engagement, perverting cognitive science, and indeed in limiting our
ability to truly centre humans and humanity in the engineering of AI systems.
To even begin to de-fetishise AI, we must look the human-in-the-loop in the
eyes.

</details>


### [64] [Leveraging Fine-Tuned Large Language Models for Interpretable Pancreatic Cystic Lesion Feature Extraction and Risk Categorization](https://arxiv.org/abs/2507.19973)
*Ebrahim Rasromani,Stella K. Kang,Yanqi Xu,Beisong Liu,Garvit Luhadia,Wan Fung Chui,Felicia L. Pasadyn,Yu Chih Hung,Julie Y. An,Edwin Mathieu,Zehui Gu,Carlos Fernandez-Granda,Ammar A. Javed,Greg D. Sacks,Tamas Gonda,Chenchan Huang,Yiqiu Shen*

Main category: cs.AI

TL;DR: 使用链式思维（CoT）提示微调的开源大语言模型（LLMs）能够高效、准确地从MRI/CT报告中提取胰腺囊性病变（PCL）特征并分类风险，性能媲美GPT-4o。


<details>
  <summary>Details</summary>
Motivation: 手动提取PCL特征耗时且难以大规模研究，需自动化工具支持。

Method: 使用GPT-4o生成的CoT数据微调LLaMA和DeepSeek模型，基于指南映射特征至风险类别，并在285份人工标注报告上评估。

Result: 微调后模型特征提取准确率显著提升（LLaMA: 80%→97%；DeepSeek: 79%→98%），风险分类F1分数接近GPT-4o（0.97），与放射科医生一致性高（Fleiss' Kappa≈0.89）。

Conclusion: CoT微调的开源LLMs可实现高效、准确的PCL表型分析，性能与GPT-4o相当。

Abstract: Background: Manual extraction of pancreatic cystic lesion (PCL) features from
radiology reports is labor-intensive, limiting large-scale studies needed to
advance PCL research. Purpose: To develop and evaluate large language models
(LLMs) that automatically extract PCL features from MRI/CT reports and assign
risk categories based on guidelines. Materials and Methods: We curated a
training dataset of 6,000 abdominal MRI/CT reports (2005-2024) from 5,134
patients that described PCLs. Labels were generated by GPT-4o using
chain-of-thought (CoT) prompting to extract PCL and main pancreatic duct
features. Two open-source LLMs were fine-tuned using QLoRA on GPT-4o-generated
CoT data. Features were mapped to risk categories per institutional guideline
based on the 2017 ACR White Paper. Evaluation was performed on 285 held-out
human-annotated reports. Model outputs for 100 cases were independently
reviewed by three radiologists. Feature extraction was evaluated using exact
match accuracy, risk categorization with macro-averaged F1 score, and
radiologist-model agreement with Fleiss' Kappa. Results: CoT fine-tuning
improved feature extraction accuracy for LLaMA (80% to 97%) and DeepSeek (79%
to 98%), matching GPT-4o (97%). Risk categorization F1 scores also improved
(LLaMA: 0.95; DeepSeek: 0.94), closely matching GPT-4o (0.97), with no
statistically significant differences. Radiologist inter-reader agreement was
high (Fleiss' Kappa = 0.888) and showed no statistically significant difference
with the addition of DeepSeek-FT-CoT (Fleiss' Kappa = 0.893) or GPT-CoT
(Fleiss' Kappa = 0.897), indicating that both models achieved agreement levels
on par with radiologists. Conclusion: Fine-tuned open-source LLMs with CoT
supervision enable accurate, interpretable, and efficient phenotyping for
large-scale PCL research, achieving performance comparable to GPT-4o.

</details>


### [65] [Digital Twin Channel-Enabled Online Resource Allocation for 6G: Principle, Architecture and Application](https://arxiv.org/abs/2507.19974)
*Tongjie Li,Jianhua Zhang,Li Yu,Yuxiang Zhang,Yunlong Cai,Fan Xu,Guangyi Liu*

Main category: cs.AI

TL;DR: 提出了一种基于数字孪生信道（DTC）的在线优化框架，用于6G网络中高效、低延迟的资源分配。


<details>
  <summary>Details</summary>
Motivation: 解决传统统计建模方法在动态环境中性能不足及实时信道状态信息（CSI）获取开销大的问题。

Method: 利用DTC预测CSI，结合轻量级博弈论算法进行在线资源分配。

Result: 仿真显示，相比基于导频的理想CSI方案，吞吐量提升高达11.5%。

Conclusion: 该方法适用于未来6G网络的可扩展、低开销和环境感知通信。

Abstract: Emerging applications such as holographic communication, autonomous driving,
and the industrial Internet of Things impose stringent requirements on
flexible, low-latency, and reliable resource allocation in 6G networks.
Conventional methods, which rely on statistical modeling, have proven effective
in general contexts but may fail to achieve optimal performance in specific and
dynamic environments. Furthermore, acquiring real-time channel state
information (CSI) typically requires excessive pilot overhead. To address these
challenges, a digital twin channel (DTC)-enabled online optimization framework
is proposed, in which DTC is employed to predict CSI based on environmental
sensing. The predicted CSI is then utilized by lightweight game-theoretic
algorithms to perform online resource allocation in a timely and efficient
manner. Simulation results based on a digital replica of a realistic industrial
workshop demonstrate that the proposed method achieves throughput improvements
of up to 11.5\% compared with pilot-based ideal CSI schemes, validating its
effectiveness for scalable, low-overhead, and environment-aware communication
in future 6G networks.

</details>


### [66] [Matching Game Preferences Through Dialogical Large Language Models: A Perspective](https://arxiv.org/abs/2507.20000)
*Renaud Fabre,Daniel Egret,Patrice Bellot*

Main category: cs.AI

TL;DR: 本文探讨了如何将大型语言模型（LLMs）与GRAPHYP网络系统结合，以提升对话智能的透明度和可追溯性，提出了一种名为D-LLMs的框架，旨在通过结构化对话实现用户偏好的个性化嵌入。


<details>
  <summary>Details</summary>
Motivation: 当前AI系统缺乏透明性，用户难以理解其决策过程。本文旨在通过结合LLMs和GRAPHYP网络，构建一个透明且可解释的AI系统，增强用户对AI决策的信任。

Method: 提出了D-LLMs框架，包含三个核心组件：推理过程、偏好分类系统和对话方法，以实现透明化的AI决策。

Result: D-LLMs框架能够分析用户偏好、解决信息冲突，并通过GRAPHYP网络展示决策过程，提升AI的透明度和可信度。

Conclusion: 本文展望了一种透明且可解释的AI系统，通过D-LLMs框架使AI决策过程对用户可见，从而增强信任和实用性。

Abstract: This perspective paper explores the future potential of "conversational
intelligence" by examining how Large Language Models (LLMs) could be combined
with GRAPHYP's network system to better understand human conversations and
preferences. Using recent research and case studies, we propose a conceptual
framework that could make AI rea-soning transparent and traceable, allowing
humans to see and understand how AI reaches its conclusions. We present the
conceptual perspective of "Matching Game Preferences through Dialogical Large
Language Models (D-LLMs)," a proposed system that would allow multiple users to
share their different preferences through structured conversations. This
approach envisions personalizing LLMs by embedding individual user preferences
directly into how the model makes decisions. The proposed D-LLM framework would
require three main components: (1) reasoning processes that could analyze
different search experiences and guide performance, (2) classification systems
that would identify user preference patterns, and (3) dialogue approaches that
could help humans resolve conflicting information. This perspective framework
aims to create an interpretable AI system where users could examine,
understand, and combine the different human preferences that influence AI
responses, detected through GRAPHYP's search experience networks. The goal of
this perspective is to envision AI systems that would not only provide answers
but also show users how those answers were reached, making artificial
intelligence more transparent and trustworthy for human decision-making.

</details>


### [67] [Finding Personalized Good-Enough Solutions to Unsatisfiable Stable Roommates Problems](https://arxiv.org/abs/2507.20010)
*Müge Fidan,Esra Erdem*

Main category: cs.AI

TL;DR: 论文研究了稳定室友问题，提出了一种基于代理习惯和偏好网络的个性化匹配方法，以解决无解情况下的“足够好”匹配。


<details>
  <summary>Details</summary>
Motivation: 现实应用中稳定室友问题可能无解，因此需要寻找“足够好”的匹配方案。

Method: 结合代理的习惯、偏好及其朋友网络，生成个性化的稳定室友匹配方案。

Result: 通过实例和实证评估验证了方法的有效性。

Conclusion: 该方法为解决稳定室友问题提供了一种实用且个性化的解决方案。

Abstract: The Stable Roommates problems are characterized by the preferences of agents
over other agents as roommates. A solution is a partition of the agents into
pairs that are acceptable to each other (i.e., they are in the preference lists
of each other), and the matching is stable (i.e., there do not exist any two
agents who prefer each other to their roommates, and thus block the matching).
Motivated by real-world applications, and considering that stable roommates
problems do not always have solutions, we continue our studies to compute
"good-enough" matchings. In addition to the agents' habits and habitual
preferences, we consider their networks of preferred friends, and introduce a
method to generate personalized solutions to stable roommates problems. We
illustrate the usefulness of our method with examples and empirical
evaluations.

</details>


### [68] [PITA: Preference-Guided Inference-Time Alignment for LLM Post-Training](https://arxiv.org/abs/2507.20067)
*Sarat Chandra Bobbili,Ujwal Dinesha,Dheeraj Narasimha,Srinivas Shakkottai*

Main category: cs.AI

TL;DR: PITA框架通过直接整合偏好反馈到LLM的token生成中，消除了对预训练奖励模型的依赖，降低了计算成本。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖预训练的奖励模型，可能不稳定且需要人工偏好反馈，PITA旨在解决这一问题。

Method: PITA学习一个基于偏好的小型引导策略，在推理时修改token概率，无需LLM微调，通过随机搜索和迭代优化实现。

Result: 在数学推理和情感分类等任务中，PITA有效对齐LLM输出与用户偏好。

Conclusion: PITA提供了一种高效、无需奖励模型的LLM对齐方法，具有实际应用潜力。

Abstract: Inference-time alignment enables large language models (LLMs) to generate
outputs aligned with end-user preferences without further training. Recent
post-training methods achieve this by using small guidance models to modify
token generation during inference. These methods typically optimize a reward
function KL-regularized by the original LLM taken as the reference policy. A
critical limitation, however, is their dependence on a pre-trained reward
model, which requires fitting to human preference feedback--a potentially
unstable process. In contrast, we introduce PITA, a novel framework that
integrates preference feedback directly into the LLM's token generation,
eliminating the need for a reward model. PITA learns a small preference-based
guidance policy to modify token probabilities at inference time without LLM
fine-tuning, reducing computational cost and bypassing the pre-trained reward
model dependency. The problem is framed as identifying an underlying preference
distribution, solved through stochastic search and iterative refinement of the
preference-based guidance model. We evaluate PITA across diverse tasks,
including mathematical reasoning and sentiment classification, demonstrating
its effectiveness in aligning LLM outputs with user preferences.

</details>


### [69] [Concept Learning for Cooperative Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2507.20143)
*Zhonghan Ge,Yuanyang Zhu,Chunlin Chen*

Main category: cs.AI

TL;DR: 论文提出了一种基于概念瓶颈模型的可解释价值分解框架CMQ，用于解决多智能体强化学习中的透明性和互操作性问题，并在实验中表现出优越性能。


<details>
  <summary>Details</summary>
Motivation: 解决多智能体强化学习中神经网络缺乏透明性和互操作性的问题，尤其是隐式合作机制的不透明性。

Method: 提出CMQ方法，通过学习可解释的合作概念，将每个概念表示为监督向量，并通过全局状态嵌入条件化个体动作值来增强合作表示能力。

Result: 在StarCraft II和LBF任务中，CMQ表现优于现有方法，并能捕捉有意义的合作模式，支持概念干预以检测潜在偏差。

Conclusion: CMQ突破了性能与可解释性之间的权衡，为多智能体合作提供了透明且高效的解决方案。

Abstract: Despite substantial progress in applying neural networks (NN) to multi-agent
reinforcement learning (MARL) areas, they still largely suffer from a lack of
transparency and interoperability. However, its implicit cooperative mechanism
is not yet fully understood due to black-box networks. In this work, we study
an interpretable value decomposition framework via concept bottleneck models,
which promote trustworthiness by conditioning credit assignment on an
intermediate level of human-like cooperation concepts. To address this problem,
we propose a novel value-based method, named Concepts learning for Multi-agent
Q-learning (CMQ), that goes beyond the current performance-vs-interpretability
trade-off by learning interpretable cooperation concepts. CMQ represents each
cooperation concept as a supervised vector, as opposed to existing models where
the information flowing through their end-to-end mechanism is concept-agnostic.
Intuitively, using individual action value conditioning on global state
embeddings to represent each concept allows for extra cooperation
representation capacity. Empirical evaluations on the StarCraft II
micromanagement challenge and level-based foraging (LBF) show that CMQ achieves
superior performance compared with the state-of-the-art counterparts. The
results also demonstrate that CMQ provides more cooperation concept
representation capturing meaningful cooperation modes, and supports test-time
concept interventions for detecting potential biases of cooperation mode and
identifying spurious artifacts that impact cooperation.

</details>


### [70] [The Policy Cliff: A Theoretical Analysis of Reward-Policy Maps in Large Language Models](https://arxiv.org/abs/2507.20150)
*Xingcheng Xu*

Main category: cs.AI

TL;DR: 论文提出了一个数学框架，分析强化学习（RL）中奖励函数到最优策略映射的稳定性，解释了政策脆弱性的根源，并验证了熵正则化对稳定性的影响。


<details>
  <summary>Details</summary>
Motivation: 强化学习在大型语言和推理模型（LLMs/LRMs）中行为塑造中起关键作用，但其政策脆弱性和不稳定性导致信任和安全问题，缺乏统一理论解释。

Method: 通过数学框架分析奖励函数到最优策略的映射稳定性，探讨非唯一最优动作和政策脆弱性的关系，并扩展到多奖励RL和熵正则化的影响。

Result: 理论分析为多种失败现象提供统一解释，验证熵正则化能恢复政策稳定性但增加随机性，并通过扰动实验在多奖励RL中验证。

Conclusion: 该框架将政策稳定性分析从经验启发提升为理论原则，为设计更安全、可信的AI系统提供关键见解。

Abstract: Reinforcement learning (RL) plays a crucial role in shaping the behavior of
large language and reasoning models (LLMs/LRMs). However, it often produces
brittle and unstable policies, leading to critical failures such as spurious
reasoning, deceptive alignment, and instruction disobedience that undermine the
trustworthiness and safety of LLMs/LRMs. Currently, these issues lack a unified
theoretical explanation and are typically addressed using ad-hoc heuristics.
This paper presents a rigorous mathematical framework for analyzing the
stability of the mapping from a reward function to the optimal policy. We show
that policy brittleness often stems from non-unique optimal actions, a common
occurrence when multiple valid traces exist in a reasoning task. This
theoretical lens provides a unified explanation for a range of seemingly
disparate failures, reframing them as rational outcomes of optimizing rewards
that may be incomplete or noisy, especially in the presence of action
degeneracy. We extend this analysis from the fundamental single-reward setting
to the more realistic multi-reward RL across diverse domains, showing how
stability is governed by an "effective reward" aggregation mechanism. We also
prove that entropy regularization restores policy stability at the cost of
increased stochasticity. Our framework provides a unified explanation for
recent empirical findings on deceptive reasoning, instruction-following
trade-offs, and RLHF-induced sophistry, and is further validated through
perturbation experiments in multi-reward RL. This work advances
policy-stability analysis from empirical heuristics towards a principled
theory, offering essential insights for designing safer and more trustworthy AI
systems.

</details>


### [71] [StepFun-Prover Preview: Let's Think and Verify Step by Step](https://arxiv.org/abs/2507.20199)
*Shijie Shang,Ruosi Wan,Yue Peng,Yutong Wu,Xiong-hui Chen,Jie Yan,Xiangyu Zhang*

Main category: cs.AI

TL;DR: StepFun-Prover是一个用于形式化定理证明的大型语言模型，通过工具集成推理实现高性能。


<details>
  <summary>Details</summary>
Motivation: 开发一种能够模拟人类问题解决策略的模型，以提升自动定理证明和数学AI助手的性能。

Method: 采用强化学习管道，结合工具交互，迭代优化证明生成。

Result: 在miniF2F-test基准测试中，pass@1成功率达到70.0%。

Conclusion: StepFun-Prover为工具集成推理模型提供了一个端到端训练框架，推动了自动定理证明和数学AI助手的发展。

Abstract: We present StepFun-Prover Preview, a large language model designed for formal
theorem proving through tool-integrated reasoning. Using a reinforcement
learning pipeline that incorporates tool-based interactions, StepFun-Prover can
achieve strong performance in generating Lean 4 proofs with minimal sampling.
Our approach enables the model to emulate human-like problem-solving strategies
by iteratively refining proofs based on real-time environment feedback. On the
miniF2F-test benchmark, StepFun-Prover achieves a pass@1 success rate of
$70.0\%$. Beyond advancing benchmark performance, we introduce an end-to-end
training framework for developing tool-integrated reasoning models, offering a
promising direction for automated theorem proving and Math AI assistant.

</details>


### [72] [Improving Subgraph Matching by Combining Algorithms and Graph Neural Networks](https://arxiv.org/abs/2507.20226)
*Shuyang Guo,Wenjin Xie,Ping Lu,Ting Deng,Richong Zhang,Jianxin Li,Xiangping Huang,Zhongyi Liu*

Main category: cs.AI

TL;DR: HFrame是一个基于图神经网络的子图同态框架，结合传统算法与机器学习技术，性能优于标准图神经网络，速度比精确匹配算法快101.91倍，平均准确率达0.962。


<details>
  <summary>Details</summary>
Motivation: 子图同态问题比子图同构更复杂，传统方法效率低，需要结合机器学习提升性能。

Method: 提出HFrame框架，结合图神经网络与传统算法，用于解决子图同态问题。

Result: HFrame能区分更多非同态图对，速度比精确匹配算法快101.91倍，平均准确率达0.962。

Conclusion: HFrame在子图同态问题中表现出色，为图神经网络的应用提供了新思路。

Abstract: Homomorphism is a key mapping technique between graphs that preserves their
structure. Given a graph and a pattern, the subgraph homomorphism problem
involves finding a mapping from the pattern to the graph, ensuring that
adjacent vertices in the pattern are mapped to adjacent vertices in the graph.
Unlike subgraph isomorphism, which requires a one-to-one mapping, homomorphism
allows multiple vertices in the pattern to map to the same vertex in the graph,
making it more complex. We propose HFrame, the first graph neural network-based
framework for subgraph homomorphism, which integrates traditional algorithms
with machine learning techniques. We demonstrate that HFrame outperforms
standard graph neural networks by being able to distinguish more graph pairs
where the pattern is not homomorphic to the graph. Additionally, we provide a
generalization error bound for HFrame. Through experiments on both real-world
and synthetic graphs, we show that HFrame is up to 101.91 times faster than
exact matching algorithms and achieves an average accuracy of 0.962.

</details>


### [73] [A Multi-Agent System for Information Extraction from the Chemical Literature](https://arxiv.org/abs/2507.20230)
*Yufan Chen,Ching Ting Leung,Bowen Yu,Jianwei Sun,Yong Huang,Linyan Li,Hao Chen,Hanyu Gao*

Main category: cs.AI

TL;DR: 本文提出了一种基于多模态大语言模型（MLLM）的多智能体系统，用于自动提取化学信息，显著提升了复杂化学反应图形的提取性能。


<details>
  <summary>Details</summary>
Motivation: 高质量的化学数据库是AI驱动化学研究的基础，但目前化学信息的自动提取受限于其多模态和风格多样性。

Method: 利用MLLM的强大推理能力理解复杂化学图形结构，将提取任务分解为子任务，并通过多智能体协作完成。

Result: 系统在复杂化学反应图形的基准数据集上F1得分为80.8%，显著优于之前的最佳模型（35.6%）。

Conclusion: 该研究为化学信息自动提取提供了重要进展，将推动AI驱动的化学研究。

Abstract: To fully expedite AI-powered chemical research, high-quality chemical
databases are the cornerstone. Automatic extraction of chemical information
from the literature is essential for constructing reaction databases, but it is
currently limited by the multimodality and style variability of chemical
information. In this work, we developed a multimodal large language model
(MLLM)-based multi-agent system for automatic chemical information extraction.
We used the MLLM's strong reasoning capability to understand the structure of
complex chemical graphics, decompose the extraction task into sub-tasks and
coordinate a set of specialized agents to solve them. Our system achieved an F1
score of 80.8% on a benchmark dataset of complex chemical reaction graphics
from the literature, surpassing the previous state-of-the-art model (F1 score:
35.6%) by a significant margin. Additionally, it demonstrated consistent
improvements in key sub-tasks, including molecular image recognition, reaction
image parsing, named entity recognition and text-based reaction extraction.
This work is a critical step toward automated chemical information extraction
into structured datasets, which will be a strong promoter of AI-driven chemical
research.

</details>


### [74] [SciToolAgent: A Knowledge Graph-Driven Scientific Agent for Multi-Tool Integration](https://arxiv.org/abs/2507.20280)
*Keyan Ding,Jing Yu,Junjie Huang,Yuchen Yang,Qiang Zhang,Huajun Chen*

Main category: cs.AI

TL;DR: SciToolAgent是一个基于大语言模型（LLM）的智能代理，通过科学工具知识图谱实现多工具集成与自动化，显著提升了复杂科学工作流的效率。


<details>
  <summary>Details</summary>
Motivation: 科学研究的工具使用需要大量专业知识，现有LLM在多工具集成和复杂工作流自动化方面表现不足。

Method: 利用科学工具知识图谱进行智能工具选择和执行，结合检索增强生成和安全性检查模块。

Result: 在多个科学领域的基准测试中表现优异，成功应用于蛋白质工程、化学反应预测等复杂任务。

Conclusion: SciToolAgent为专家和非专家提供了高效、安全的科学工具自动化解决方案。

Abstract: Scientific research increasingly relies on specialized computational tools,
yet effectively utilizing these tools demands substantial domain expertise.
While Large Language Models (LLMs) show promise in tool automation, they
struggle to seamlessly integrate and orchestrate multiple tools for complex
scientific workflows. Here, we present SciToolAgent, an LLM-powered agent that
automates hundreds of scientific tools across biology, chemistry, and materials
science. At its core, SciToolAgent leverages a scientific tool knowledge graph
that enables intelligent tool selection and execution through graph-based
retrieval-augmented generation. The agent also incorporates a comprehensive
safety-checking module to ensure responsible and ethical tool usage. Extensive
evaluations on a curated benchmark demonstrate that SciToolAgent significantly
outperforms existing approaches. Case studies in protein engineering, chemical
reactivity prediction, chemical synthesis, and metal-organic framework
screening further demonstrate SciToolAgent's capability to automate complex
scientific workflows, making advanced research tools accessible to both experts
and non-experts.

</details>


### [75] [Artificial Intelligence In Patent And Market Intelligence: A New Paradigm For Technology Scouting](https://arxiv.org/abs/2507.20322)
*Manish Verma,Vivek Sharma,Vishal Singh*

Main category: cs.AI

TL;DR: 开发了一个基于AI的软件平台，利用大型语言模型（LLMs）改进工业研发中的技术搜索和解决方案发现。


<details>
  <summary>Details</summary>
Motivation: 传统方法耗时、依赖人工和领域专业知识，且信息来源分散，导致效率低下和洞察不完整。

Method: 平台利用LLMs的语义理解、上下文推理和跨领域知识提取能力，处理非结构化专利文本，整合商业智能，提供高质量解决方案。

Result: 平台减少了人工努力，加速了创新周期，并提升了复杂研发环境中的决策质量。

Conclusion: 该AI驱动的平台为工业研发提供了高效、全面的技术搜索和解决方案发现工具。

Abstract: This paper presents the development of an AI powered software platform that
leverages advanced large language models (LLMs) to transform technology
scouting and solution discovery in industrial R&D. Traditional approaches to
solving complex research and development challenges are often time consuming,
manually driven, and heavily dependent on domain specific expertise. These
methods typically involve navigating fragmented sources such as patent
repositories, commercial product catalogs, and competitor data, leading to
inefficiencies and incomplete insights. The proposed platform utilizes cutting
edge LLM capabilities including semantic understanding, contextual reasoning,
and cross-domain knowledge extraction to interpret problem statements and
retrieve high-quality, sustainable solutions. The system processes unstructured
patent texts, such as claims and technical descriptions, and systematically
extracts potential innovations aligned with the given problem context. These
solutions are then algorithmically organized under standardized technical
categories and subcategories to ensure clarity and relevance across
interdisciplinary domains. In addition to patent analysis, the platform
integrates commercial intelligence by identifying validated market solutions
and active organizations addressing similar challenges. This combined insight
sourced from both intellectual property and real world product data enables R&D
teams to assess not only technical novelty but also feasibility, scalability,
and sustainability. The result is a comprehensive, AI driven scouting engine
that reduces manual effort, accelerates innovation cycles, and enhances
decision making in complex R&D environments.

</details>


### [76] [The Blessing and Curse of Dimensionality in Safety Alignment](https://arxiv.org/abs/2507.20333)
*Rachel S. Y. Teo,Laziz U. Abdullaev,Tan M. Nguyen*

Main category: cs.AI

TL;DR: 论文探讨了大型语言模型（LLMs）高维度表示对安全对齐的双重影响，提出降维方法减少越狱攻击风险。


<details>
  <summary>Details</summary>
Motivation: 随着LLMs的广泛应用，高维度表示虽带来优势，但也可能被利用绕过安全对齐，需研究其影响。

Method: 通过可视化不同概念（如安全性）的线性子空间，提出降维方法以减少越狱攻击的脆弱性。

Result: 实证表明降维能保留足够对齐信息，同时显著降低越狱攻击的敏感性。

Conclusion: 高维度表示既是优势也是挑战，降维为安全对齐提供了有效解决方案。

Abstract: The focus on safety alignment in large language models (LLMs) has increased
significantly due to their widespread adoption across different domains. The
scale of LLMs play a contributing role in their success, and the growth in
parameter count follows larger hidden dimensions. In this paper, we hypothesize
that while the increase in dimensions has been a key advantage, it may lead to
emergent problems as well. These problems emerge as the linear structures in
the activation space can be exploited, in the form of activation engineering,
to circumvent its safety alignment. Through detailed visualizations of linear
subspaces associated with different concepts, such as safety, across various
model scales, we show that the curse of high-dimensional representations
uniquely impacts LLMs. Further substantiating our claim, we demonstrate that
projecting the representations of the model onto a lower dimensional subspace
can preserve sufficient information for alignment while avoiding those linear
structures. Empirical results confirm that such dimensional reduction
significantly reduces susceptibility to jailbreaking through representation
engineering. Building on our empirical validations, we provide theoretical
insights into these linear jailbreaking methods relative to a model's hidden
dimensions. Broadly speaking, our work posits that the high dimensions of a
model's internal representations can be both a blessing and a curse in safety
alignment.

</details>


### [77] [VLMPlanner: Integrating Visual Language Models with Motion Planning](https://arxiv.org/abs/2507.20342)
*Zhipeng Tang,Sha Zhang,Jiajun Deng,Chenjie Wang,Guoliang You,Yuting Huang,Xinrui Lin,Yanyong Zhang*

Main category: cs.AI

TL;DR: VLMPlanner结合视觉语言模型（VLM）与实时规划器，通过多视角图像捕捉细节视觉信息，提升自动驾驶决策的鲁棒性和安全性。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖抽象感知或地图输入，缺乏关键视觉上下文（如道路细节、意外障碍），影响复杂驾驶环境中的决策。

Method: 提出VLMPlanner框架，利用VLM处理多视角图像并推理，结合CAI-Gate机制动态调整推理频率以平衡性能与效率。

Result: 在nuPlan基准测试中表现优异，尤其在复杂道路条件和动态场景下。

Conclusion: VLMPlanner通过视觉上下文和动态推理机制，显著提升了自动驾驶规划的鲁棒性和安全性。

Abstract: Integrating large language models (LLMs) into autonomous driving motion
planning has recently emerged as a promising direction, offering enhanced
interpretability, better controllability, and improved generalization in rare
and long-tail scenarios. However, existing methods often rely on abstracted
perception or map-based inputs, missing crucial visual context, such as
fine-grained road cues, accident aftermath, or unexpected obstacles, which are
essential for robust decision-making in complex driving environments. To bridge
this gap, we propose VLMPlanner, a hybrid framework that combines a
learning-based real-time planner with a vision-language model (VLM) capable of
reasoning over raw images. The VLM processes multi-view images to capture rich,
detailed visual information and leverages its common-sense reasoning
capabilities to guide the real-time planner in generating robust and safe
trajectories. Furthermore, we develop the Context-Adaptive Inference Gate
(CAI-Gate) mechanism that enables the VLM to mimic human driving behavior by
dynamically adjusting its inference frequency based on scene complexity,
thereby achieving an optimal balance between planning performance and
computational efficiency. We evaluate our approach on the large-scale,
challenging nuPlan benchmark, with comprehensive experimental results
demonstrating superior planning performance in scenarios with intricate road
conditions and dynamic elements. Code will be available.

</details>


### [78] [Multi-Agent Reinforcement Learning for Dynamic Mobility Resource Allocation with Hierarchical Adaptive Grouping](https://arxiv.org/abs/2507.20377)
*Farshid Nooshi,Suining He*

Main category: cs.AI

TL;DR: 提出了一种名为HAG-PS的多智能体强化学习方法，用于动态分配城市移动资源，解决了策略共享和内存效率问题。


<details>
  <summary>Details</summary>
Motivation: 解决城市移动资源分配中多智能体强化学习的动态策略共享和内存效率挑战。

Method: 采用分层自适应分组参数共享（HAG-PS），结合全局和局部信息，动态分组智能体，并使用可学习的ID嵌入实现专业化。

Result: 在纽约共享单车数据上验证，HAG-PS显著提高了单车可用性。

Conclusion: HAG-PS有效解决了动态资源分配问题，性能优于基线方法。

Abstract: Allocating mobility resources (e.g., shared bikes/e-scooters, ride-sharing
vehicles) is crucial for rebalancing the mobility demand and supply in the
urban environments. We propose in this work a novel multi-agent reinforcement
learning named Hierarchical Adaptive Grouping-based Parameter Sharing (HAG-PS)
for dynamic mobility resource allocation. HAG-PS aims to address two important
research challenges regarding multi-agent reinforcement learning for mobility
resource allocation: (1) how to dynamically and adaptively share the mobility
resource allocation policy (i.e., how to distribute mobility resources) across
agents (i.e., representing the regional coordinators of mobility resources);
and (2) how to achieve memory-efficient parameter sharing in an urban-scale
setting. To address the above challenges, we have provided following novel
designs within HAG-PS. To enable dynamic and adaptive parameter sharing, we
have designed a hierarchical approach that consists of global and local
information of the mobility resource states (e.g., distribution of mobility
resources). We have developed an adaptive agent grouping approach in order to
split or merge the groups of agents based on their relative closeness of
encoded trajectories (i.e., states, actions, and rewards). We have designed a
learnable identity (ID) embeddings to enable agent specialization beyond simple
parameter copy. We have performed extensive experimental studies based on
real-world NYC bike sharing data (a total of more than 1.2 million trips), and
demonstrated the superior performance (e.g., improved bike availability) of
HAG-PS compared with other baseline approaches.

</details>


### [79] [MazeEval: A Benchmark for Testing Sequential Decision-Making in Language Models](https://arxiv.org/abs/2507.20395)
*Hafsteinn Einarsson*

Main category: cs.AI

TL;DR: 论文提出了MazeEval基准，用于评估LLMs在无视觉输入下的纯空间推理能力，发现模型表现差异显著且受语言影响。


<details>
  <summary>Details</summary>
Motivation: 随着LLMs在机器人和具身AI中的应用增多，理解其空间推理能力对实际部署至关重要，但目前缺乏相关评估。

Method: 通过坐标反馈和距离信息设计迷宫导航任务，评估8种LLMs在英语和冰岛语中的表现。

Result: 模型表现差异显著，部分模型在较大迷宫中完全失败，且冰岛语表现明显较差。

Conclusion: LLMs的空间推理能力受训练数据和语言影响，需架构创新以实现跨语言可靠导航。

Abstract: As Large Language Models (LLMs) increasingly power autonomous agents in
robotics and embodied AI, understanding their spatial reasoning capabilities
becomes crucial for ensuring reliable real-world deployment. Despite advances
in language understanding, current research lacks evaluation of how LLMs
perform spatial navigation without visual cues, a fundamental requirement for
agents operating with limited sensory information. This paper addresses this
gap by introducing MazeEval, a benchmark designed to isolate and evaluate pure
spatial reasoning in LLMs through coordinate-based maze navigation tasks. Our
methodology employs a function-calling interface where models navigate mazes of
varying complexity ($5\times 5$ to $15\times 15$ grids) using only coordinate
feedback and distance-to-wall information, excluding visual input to test
fundamental spatial cognition. We evaluate eight state-of-the-art LLMs across
identical mazes in both English and Icelandic to assess cross-linguistic
transfer of spatial abilities. Our findings reveal striking disparities: while
OpenAI's O3 achieves perfect navigation for mazes up to size $30\times 30$,
other models exhibit catastrophic failure beyond $9\times 9$ mazes, with 100%
of failures attributed to excessive looping behavior where models revisit a
cell at least 10 times. We document a significant performance degradation in
Icelandic, with models solving mazes 3-4 sizes smaller than in English,
suggesting spatial reasoning in LLMs emerges from linguistic patterns rather
than language-agnostic mechanisms. These results have important implications
for global deployment of LLM-powered autonomous systems, showing spatial
intelligence remains fundamentally constrained by training data availability
and highlighting the need for architectural innovations to achieve reliable
navigation across linguistic contexts.

</details>


### [80] [Enhancing QoS in Edge Computing through Federated Layering Techniques: A Pathway to Resilient AI Lifelong Learning Systems](https://arxiv.org/abs/2507.20444)
*Chengzhuo Han*

Main category: cs.AI

TL;DR: 提出了一种基于联邦分层技术（FLT）的通用人工智能终身学习系统，以提高边缘计算中的服务质量（QoS）。


<details>
  <summary>Details</summary>
Motivation: 应对6G通信网络中数据量和复杂性增加的挑战，提升边缘计算环境中的QoS。

Method: 采用联邦分层技术和小模型协作机制，结合云与边缘计算优势，引入协商与辩论机制，并整合隐私保护措施。

Result: 实验表明，该方法提高了学习效率和推理准确性，同时保护了边缘节点的隐私。

Conclusion: 该方法为边缘计算环境中的QoS提升提供了可行方案，并支持大规模模型的终身学习。

Abstract: In the context of the rapidly evolving information technology landscape,
marked by the advent of 6G communication networks, we face an increased data
volume and complexity in network environments. This paper addresses these
challenges by focusing on Quality of Service (QoS) in edge computing
frameworks. We propose a novel approach to enhance QoS through the development
of General Artificial Intelligence Lifelong Learning Systems, with a special
emphasis on Federated Layering Techniques (FLT). Our work introduces a
federated layering-based small model collaborative mechanism aimed at improving
AI models' operational efficiency and response time in environments where
resources are limited. This innovative method leverages the strengths of cloud
and edge computing, incorporating a negotiation and debate mechanism among
small AI models to enhance reasoning and decision-making processes. By
integrating model layering techniques with privacy protection measures, our
approach ensures the secure transmission of model parameters while maintaining
high efficiency in learning and reasoning capabilities. The experimental
results demonstrate that our strategy not only enhances learning efficiency and
reasoning accuracy but also effectively protects the privacy of edge nodes.
This presents a viable solution for achieving resilient large model lifelong
learning systems, with a significant improvement in QoS for edge computing
environments.

</details>


### [81] [STARN-GAT: A Multi-Modal Spatio-Temporal Graph Attention Network for Accident Severity Prediction](https://arxiv.org/abs/2507.20451)
*Pritom Ray Nobin,Imran Ahammad Rifat*

Main category: cs.AI

TL;DR: STARN-GAT是一种多模态时空图注意力网络，用于预测交通事故严重程度，通过自适应图构建和模态感知注意力机制，显著提升了预测性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以有效建模影响交通事故结果的空间、时间和上下文变量之间的复杂关系，因此需要一种更先进的模型来提升预测准确性和实用性。

Method: 提出STARN-GAT模型，结合道路网络拓扑、时间交通模式和环境上下文，采用注意力机制统一建模。

Result: 在FARS数据集上Macro F1-score达85%，ROC-AUC为0.91；在ARI-BUET数据集上Macro F1-score为0.84，ROC-AUC为0.89。

Conclusion: STARN-GAT在预测高风险事故和提升可解释性方面表现优异，适用于实时交通安全管理系统。

Abstract: Accurate prediction of traffic accident severity is critical for improving
road safety, optimizing emergency response strategies, and informing the design
of safer transportation infrastructure. However, existing approaches often
struggle to effectively model the intricate interdependencies among spatial,
temporal, and contextual variables that govern accident outcomes. In this
study, we introduce STARN-GAT, a Multi-Modal Spatio-Temporal Graph Attention
Network, which leverages adaptive graph construction and modality-aware
attention mechanisms to capture these complex relationships. Unlike
conventional methods, STARN-GAT integrates road network topology, temporal
traffic patterns, and environmental context within a unified attention-based
framework. The model is evaluated on the Fatality Analysis Reporting System
(FARS) dataset, achieving a Macro F1-score of 85 percent, ROC-AUC of 0.91, and
recall of 81 percent for severe incidents. To ensure generalizability within
the South Asian context, STARN-GAT is further validated on the ARI-BUET traffic
accident dataset, where it attains a Macro F1-score of 0.84, recall of 0.78,
and ROC-AUC of 0.89. These results demonstrate the model's effectiveness in
identifying high-risk cases and its potential for deployment in real-time,
safety-critical traffic management systems. Furthermore, the attention-based
architecture enhances interpretability, offering insights into contributing
factors and supporting trust in AI-assisted decision-making. Overall, STARN-GAT
bridges the gap between advanced graph neural network techniques and practical
applications in road safety analytics.

</details>


### [82] [Security Challenges in AI Agent Deployment: Insights from a Large Scale Public Competition](https://arxiv.org/abs/2507.20526)
*Andy Zou,Maxwell Lin,Eliot Jones,Micha Nowak,Mateusz Dziemian,Nick Winter,Alexander Grattan,Valent Nathanael,Ayla Croft,Xander Davies,Jai Patel,Robert Kirk,Nate Burnikell,Yarin Gal,Dan Hendrycks,J. Zico Kolter,Matt Fredrikson*

Main category: cs.AI

TL;DR: 论文通过大规模的红队测试竞赛，评估了22种前沿AI代理在44种实际部署场景中的安全性，发现其普遍存在政策违规漏洞，并提出了ART基准以推动更严格的安全评估。


<details>
  <summary>Details</summary>
Motivation: 研究AI代理在现实环境中是否能够遵循部署政策，尤其是在受到攻击时，以揭示其潜在的安全漏洞。

Method: 通过组织公开的红队测试竞赛，收集180万次提示注入攻击，分析成功引发政策违规的案例，并构建ART基准。

Result: 几乎所有AI代理在10-100次查询内都会出现政策违规，且攻击在不同模型和任务间具有高转移性。

Conclusion: 当前AI代理存在严重且持续的安全漏洞，需要额外的防御措施。通过发布ART基准，推动更安全的代理部署。

Abstract: Recent advances have enabled LLM-powered AI agents to autonomously execute
complex tasks by combining language model reasoning with tools, memory, and web
access. But can these systems be trusted to follow deployment policies in
realistic environments, especially under attack? To investigate, we ran the
largest public red-teaming competition to date, targeting 22 frontier AI agents
across 44 realistic deployment scenarios. Participants submitted 1.8 million
prompt-injection attacks, with over 60,000 successfully eliciting policy
violations such as unauthorized data access, illicit financial actions, and
regulatory noncompliance. We use these results to build the Agent Red Teaming
(ART) benchmark - a curated set of high-impact attacks - and evaluate it across
19 state-of-the-art models. Nearly all agents exhibit policy violations for
most behaviors within 10-100 queries, with high attack transferability across
models and tasks. Importantly, we find limited correlation between agent
robustness and model size, capability, or inference-time compute, suggesting
that additional defenses are needed against adversarial misuse. Our findings
highlight critical and persistent vulnerabilities in today's AI agents. By
releasing the ART benchmark and accompanying evaluation framework, we aim to
support more rigorous security assessment and drive progress toward safer agent
deployment.

</details>


### [83] [Smart Expansion Techniques for ASP-based Interactive Configuration](https://arxiv.org/abs/2507.21027)
*Lucia Balážová,Richard Comploi-Taupe,Susana Hahn,Nicolas Rühling,Gottfried Schenner*

Main category: cs.AI

TL;DR: 本文提出了一种基于ASP的交互式配置求解器，通过四种智能扩展功能优化部分配置的自动完成性能，减少计算成本并提升效率。


<details>
  <summary>Details</summary>
Motivation: 解决大规模工业配置问题中交互式系统的性能瓶颈，支持直观的用户界面。

Method: 采用增量式多轮求解方法，结合谨慎和勇敢推理，减少不满足性检查次数和搜索空间。

Result: 提出的方法显著提高了求解性能，减少了计算成本。

Conclusion: 通过智能扩展功能和增量式求解，有效提升了交互式配置系统的性能，为实际应用提供了支持。

Abstract: Product configuration is a successful application of Answer Set Programming
(ASP). However, challenges are still open for interactive systems to
effectively guide users through the configuration process. The aim of our work
is to provide an ASP-based solver for interactive configuration that can deal
with large-scale industrial configuration problems and that supports intuitive
user interfaces via an API. In this paper, we focus on improving the
performance of automatically completing a partial configuration. Our main
contribution enhances the classical incremental approach for multi-shot solving
by four different smart expansion functions. The core idea is to determine and
add specific objects or associations to the partial configuration by exploiting
cautious and brave consequences before checking for the existence of a complete
configuration with the current objects in each iteration. This approach limits
the number of costly unsatisfiability checks and reduces the search space,
thereby improving solving performance. In addition, we present a user interface
that uses our API and is implemented in ASP.

</details>


### [84] [MeLA: A Metacognitive LLM-Driven Architecture for Automatic Heuristic Design](https://arxiv.org/abs/2507.20541)
*Zishang Qiu,Xinan Chen,Long Chen,Ruibin Bai*

Main category: cs.AI

TL;DR: MeLA是一种基于元认知的LLM驱动架构，通过“提示进化”优化自动启发式设计（AHD），显著优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 传统进化方法直接操作启发式代码，而MeLA通过优化LLM的提示来生成启发式，探索元认知框架在AI架构中的潜力。

Method: MeLA整合问题分析器、错误诊断系统和元认知搜索引擎，通过性能反馈迭代优化提示。

Result: 在基准和实际问题中，MeLA生成的启发式更有效且鲁棒，显著优于现有方法。

Conclusion: 研究表明，元认知框架为AHD提供了更鲁棒和可解释的路径，展示了认知科学在AI架构中的潜力。

Abstract: This paper introduces MeLA, a Metacognitive LLM-Driven Architecture that
presents a new paradigm for Automatic Heuristic Design (AHD). Traditional
evolutionary methods operate directly on heuristic code; in contrast, MeLA
evolves the instructional prompts used to guide a Large Language Model (LLM) in
generating these heuristics. This process of "prompt evolution" is driven by a
novel metacognitive framework where the system analyzes performance feedback to
systematically refine its generative strategy. MeLA's architecture integrates a
problem analyzer to construct an initial strategic prompt, an error diagnosis
system to repair faulty code, and a metacognitive search engine that
iteratively optimizes the prompt based on heuristic effectiveness. In
comprehensive experiments across both benchmark and real-world problems, MeLA
consistently generates more effective and robust heuristics, significantly
outperforming state-of-the-art methods. Ultimately, this research demonstrates
the profound potential of using cognitive science as a blueprint for AI
architecture, revealing that by enabling an LLM to metacognitively regulate its
problem-solving process, we unlock a more robust and interpretable path to AHD.

</details>


### [85] [Unlearning of Knowledge Graph Embedding via Preference Optimization](https://arxiv.org/abs/2507.20566)
*Jiajun Liu,Wenjun Ke,Peng Wang,Yao He,Ziyu Shang,Guozheng Li,Zijie Xu,Ke Ji*

Main category: cs.AI

TL;DR: GraphDPO是一种基于直接偏好优化的知识遗忘框架，用于从知识图谱嵌入模型中移除特定信息，同时保留剩余知识的完整性。


<details>
  <summary>Details</summary>
Motivation: 现有知识图谱中存在过时或错误的知识，需要从知识图谱嵌入模型中移除。现有遗忘方法存在不完全移除目标信息和削弱剩余知识的问题。

Method: GraphDPO将遗忘问题重构为偏好优化问题，通过直接偏好优化训练模型，并引入边界外采样策略和边界回忆机制。

Result: 在八个数据集上的实验表明，GraphDPO在MRR_Avg和MRR_F1上分别比现有方法提升10.1%和14.0%。

Conclusion: GraphDPO有效解决了知识图谱遗忘问题，显著优于现有方法。

Abstract: Existing knowledge graphs (KGs) inevitably contain outdated or erroneous
knowledge that needs to be removed from knowledge graph embedding (KGE) models.
To address this challenge, knowledge unlearning can be applied to eliminate
specific information while preserving the integrity of the remaining knowledge
in KGs. Existing unlearning methods can generally be categorized into exact
unlearning and approximate unlearning. However, exact unlearning requires high
training costs while approximate unlearning faces two issues when applied to
KGs due to the inherent connectivity of triples: (1) It fails to fully remove
targeted information, as forgetting triples can still be inferred from
remaining ones. (2) It focuses on local data for specific removal, which
weakens the remaining knowledge in the forgetting boundary. To address these
issues, we propose GraphDPO, a novel approximate unlearning framework based on
direct preference optimization (DPO). Firstly, to effectively remove forgetting
triples, we reframe unlearning as a preference optimization problem, where the
model is trained by DPO to prefer reconstructed alternatives over the original
forgetting triples. This formulation penalizes reliance on forgettable
knowledge, mitigating incomplete forgetting caused by KG connectivity.
Moreover, we introduce an out-boundary sampling strategy to construct
preference pairs with minimal semantic overlap, weakening the connection
between forgetting and retained knowledge. Secondly, to preserve boundary
knowledge, we introduce a boundary recall mechanism that replays and distills
relevant information both within and across time steps. We construct eight
unlearning datasets across four popular KGs with varying unlearning rates.
Experiments show that GraphDPO outperforms state-of-the-art baselines by up to
10.1% in MRR_Avg and 14.0% in MRR_F1.

</details>


### [86] [Enhancing Large Multimodal Models with Adaptive Sparsity and KV Cache Compression](https://arxiv.org/abs/2507.20613)
*Te Zhang,Yuheng Li,Junxiang Wang,Lujun Li*

Main category: cs.AI

TL;DR: 提出了一种自适应搜索算法，通过优化稀疏性和KV缓存压缩，提升大型多模态模型（LMMs）在边缘设备上的部署效率。


<details>
  <summary>Details</summary>
Motivation: 尽管LMMs在视觉编码器和语言模型结合方面取得进展，但在边缘设备上的压缩部署仍具挑战性。

Method: 采用Tree-structured Parzen Estimator动态调整不同层的剪枝比例和KV缓存量化带宽，结合剪枝与KV缓存量化，无需额外微调。

Result: 在LLaVA-1.5 7B和13B等基准数据集上表现优于SparseGPT和Wanda等现有技术，内存效率高且性能损失小。

Conclusion: 该方法为LMM优化设定了新标准，实现了高效压缩且几乎不影响性能。

Abstract: Large multimodal models (LMMs) have advanced significantly by integrating
visual encoders with extensive language models, enabling robust reasoning
capabilities. However, compressing LMMs for deployment on edge devices remains
a critical challenge. In this work, we propose an adaptive search algorithm
that optimizes sparsity and KV cache compression to enhance LMM efficiency.
Utilizing the Tree-structured Parzen Estimator, our method dynamically adjusts
pruning ratios and KV cache quantization bandwidth across different LMM layers,
using model performance as the optimization objective. This approach uniquely
combines pruning with key-value cache quantization and incorporates a fast
pruning technique that eliminates the need for additional fine-tuning or weight
adjustments, achieving efficient compression without compromising accuracy.
Comprehensive evaluations on benchmark datasets, including LLaVA-1.5 7B and
13B, demonstrate our method superiority over state-of-the-art techniques such
as SparseGPT and Wanda across various compression levels. Notably, our
framework automatic allocation of KV cache compression resources sets a new
standard in LMM optimization, delivering memory efficiency without sacrificing
much performance.

</details>


### [87] [Complementarity-driven Representation Learning for Multi-modal Knowledge Graph Completion](https://arxiv.org/abs/2507.20620)
*Lijian Li*

Main category: cs.AI

TL;DR: 提出MoCME框架，通过互补性模态融合和熵引导负采样，提升多模态知识图谱补全性能。


<details>
  <summary>Details</summary>
Motivation: 多模态知识图谱中模态分布不平衡，现有方法忽视多模态数据的互补性。

Method: 提出互补性引导的模态知识融合模块（CMKF）和熵引导负采样机制（EGNS）。

Result: 在五个基准数据集上表现最优。

Conclusion: MoCME框架显著提升多模态知识图谱补全效果。

Abstract: Multi-modal Knowledge Graph Completion (MMKGC) aims to uncover hidden world
knowledge in multimodal knowledge graphs by leveraging both multimodal and
structural entity information. However, the inherent imbalance in multimodal
knowledge graphs, where modality distributions vary across entities, poses
challenges in utilizing additional modality data for robust entity
representation. Existing MMKGC methods typically rely on attention or
gate-based fusion mechanisms but overlook complementarity contained in
multi-modal data. In this paper, we propose a novel framework named Mixture of
Complementary Modality Experts (MoCME), which consists of a
Complementarity-guided Modality Knowledge Fusion (CMKF) module and an
Entropy-guided Negative Sampling (EGNS) mechanism. The CMKF module exploits
both intra-modal and inter-modal complementarity to fuse multi-view and
multi-modal embeddings, enhancing representations of entities. Additionally, we
introduce an Entropy-guided Negative Sampling mechanism to dynamically
prioritize informative and uncertain negative samples to enhance training
effectiveness and model robustness. Extensive experiments on five benchmark
datasets demonstrate that our MoCME achieves state-of-the-art performance,
surpassing existing approaches.

</details>


### [88] [Adaptive Fuzzy Time Series Forecasting via Partially Asymmetric Convolution and Sub-Sliding Window Fusion](https://arxiv.org/abs/2507.20641)
*Lijian Li*

Main category: cs.AI

TL;DR: 提出了一种基于滑动窗口的部分非对称卷积架构，通过自适应模糊化时间数据，解决了现有模型在捕捉时空依赖性和全局信息合成方面的不足。


<details>
  <summary>Details</summary>
Motivation: 当前最先进的预测模型在学习阶段缺乏捕捉时空依赖性和合成全局信息的能力。

Method: 1. 改进传统模糊时间序列构建策略，提取短期和长期时间关联；2. 设计双边Atrous算法以减少计算需求；3. 设计部分非对称卷积架构，灵活挖掘数据特征。

Result: 在多个流行时间序列数据集上取得了最先进的结果。

Conclusion: 该方法通过自适应模糊化和部分非对称设计，显著提升了时间序列预测的准确性。

Abstract: At present, state-of-the-art forecasting models are short of the ability to
capture spatio-temporal dependency and synthesize global information at the
stage of learning. To address this issue, in this paper, through the adaptive
fuzzified construction of temporal data, we propose a novel convolutional
architecture with partially asymmetric design based on the scheme of sliding
window to realize accurate time series forecasting. First, the construction
strategy of traditional fuzzy time series is improved to further extract short
and long term temporal interrelation, which enables every time node to
automatically possess corresponding global information and inner relationships
among them in a restricted sliding window and the process does not require
human involvement. Second, a bilateral Atrous algorithm is devised to reduce
calculation demand of the proposed model without sacrificing global
characteristics of elements. And it also allows the model to avoid processing
redundant information. Third, after the transformation of time series, a
partially asymmetric convolutional architecture is designed to more flexibly
mine data features by filters in different directions on feature maps, which
gives the convolutional neural network (CNN) the ability to construct
sub-windows within existing sliding windows to model at a more fine-grained
level. And after obtaining the time series information at different levels, the
multi-scale features from different sub-windows will be sent to the
corresponding network layer for time series information fusion. Compared with
other competitive modern models, the proposed method achieves state-of-the-art
results on most of popular time series datasets, which is fully verified by the
experimental results.

</details>


### [89] [A General Framework for Dynamic MAPF using Multi-Shot ASP and Tunnels](https://arxiv.org/abs/2507.20703)
*Aysu Bogatarkan,Esra Erdem*

Main category: cs.AI

TL;DR: 论文研究了动态多智能体路径规划（D-MAPF）问题，提出了一种通用定义、新框架和基于ASP的解决方法，并通过实验评估验证了其性能。


<details>
  <summary>Details</summary>
Motivation: 现实应用中，智能体路径规划需适应动态变化（如智能体增减、障碍物移动），传统MAPF无法满足需求。

Method: 1) 提出D-MAPF通用定义；2) 设计多阶段计算框架；3) 开发基于ASP的新方法，结合重规划和修复策略，引入“隧道”概念。

Result: 实验评估表明，该方法在计算性能和解决方案质量上具有优势。

Conclusion: 提出的D-MAPF框架和方法适用于动态环境，为实际应用提供了有效解决方案。

Abstract: MAPF problem aims to find plans for multiple agents in an environment within
a given time, such that the agents do not collide with each other or obstacles.
Motivated by the execution and monitoring of these plans, we study Dynamic MAPF
(D-MAPF) problem, which allows changes such as agents entering/leaving the
environment or obstacles being removed/moved. Considering the requirements of
real-world applications in warehouses with the presence of humans, we introduce
1) a general definition for D-MAPF (applicable to variations of D-MAPF), 2) a
new framework to solve D-MAPF (utilizing multi-shot computation, and allowing
different methods to solve D-MAPF), and 3) a new ASP-based method to solve
D-MAPF (combining advantages of replanning and repairing methods, with a novel
concept of tunnels to specify where agents can move). We have illustrated the
strengths and weaknesses of this method by experimental evaluations, from the
perspectives of computational performance and quality of solutions.

</details>


### [90] [Algorithmic Fairness: A Runtime Perspective](https://arxiv.org/abs/2507.20711)
*Filip Cano,Thomas A. Henzinger,Konstantin Kueffner*

Main category: cs.AI

TL;DR: 本文提出了一种将公平性作为运行时属性的分析框架，基于动态演化的硬币投掷模型，研究了监控和强制执行公平性的策略。


<details>
  <summary>Details</summary>
Motivation: 传统公平性研究是静态的，而现实AI系统是动态演化的，因此需要研究运行时公平性。

Method: 使用动态演化的硬币投掷模型，分析监控和强制执行公平性的策略，参数化环境动态、预测范围和置信阈值。

Result: 总结了监控和执行策略，提供了简单假设下的通用结果，并调查了现有解决方案。

Conclusion: 运行时公平性分析是必要的，需根据动态环境选择合适的监控和执行策略。

Abstract: Fairness in AI is traditionally studied as a static property evaluated once,
over a fixed dataset. However, real-world AI systems operate sequentially, with
outcomes and environments evolving over time. This paper proposes a framework
for analysing fairness as a runtime property. Using a minimal yet expressive
model based on sequences of coin tosses with possibly evolving biases, we study
the problems of monitoring and enforcing fairness expressed in either toss
outcomes or coin biases. Since there is no one-size-fits-all solution for
either problem, we provide a summary of monitoring and enforcement strategies,
parametrised by environment dynamics, prediction horizon, and confidence
thresholds. For both problems, we present general results under simple or
minimal assumptions. We survey existing solutions for the monitoring problem
for Markovian and additive dynamics, and existing solutions for the enforcement
problem in static settings with known dynamics.

</details>


### [91] [Learning the Value Systems of Societies from Preferences](https://arxiv.org/abs/2507.20728)
*Andrés Holgado-Sánchez,Holger Billhardt,Sascha Ossowski,Sara Degli-Esposti*

Main category: cs.AI

TL;DR: 论文提出了一种基于启发式深度聚类的方法，用于学习社会共享的价值基础和多样化的价值系统，以解决伦理AI中价值对齐的问题。


<details>
  <summary>Details</summary>
Motivation: 在伦理AI中，如何将AI系统与人类价值观及不同利益相关者的价值系统对齐是关键问题。现有的价值学习方法难以手动校准和提取，且社会科学的观点认为社会价值系统应视为不同群体的价值系统集合，而非简单聚合。

Method: 提出了一种基于启发式深度聚类的方法，通过观察代理样本的定性价值偏好，学习社会共享的价值基础和多样化的价值系统。

Result: 在旅行决策的实际数据用例中评估了该方法。

Conclusion: 该方法能够有效学习社会的多样化价值系统，为伦理AI的价值对齐提供了新思路。

Abstract: Aligning AI systems with human values and the value-based preferences of
various stakeholders (their value systems) is key in ethical AI. In value-aware
AI systems, decision-making draws upon explicit computational representations
of individual values (groundings) and their aggregation into value systems. As
these are notoriously difficult to elicit and calibrate manually, value
learning approaches aim to automatically derive computational models of an
agent's values and value system from demonstrations of human behaviour.
Nonetheless, social science and humanities literature suggest that it is more
adequate to conceive the value system of a society as a set of value systems of
different groups, rather than as the simple aggregation of individual value
systems. Accordingly, here we formalize the problem of learning the value
systems of societies and propose a method to address it based on heuristic deep
clustering. The method learns socially shared value groundings and a set of
diverse value systems representing a given society by observing qualitative
value-based preferences from a sample of agents. We evaluate the proposal in a
use case with real data about travelling decisions.

</details>


### [92] [Beyond Listenership: AI-Predicted Interventions Drive Improvements in Maternal Health Behaviours](https://arxiv.org/abs/2507.20755)
*Arpan Dasgupta,Sarvesh Gharat,Neha Madhiwalla,Aparna Hegde,Milind Tambe,Aparna Taneja*

Main category: cs.AI

TL;DR: AI-targeted语音通话干预不仅提高了听众参与度，还显著改善了孕产妇的健康行为和知识。


<details>
  <summary>Details</summary>
Motivation: 解决传统健康信息语音通话项目中听众流失和参与度低的问题，并验证AI干预是否能转化为实际健康行为改善。

Method: 使用AI模型（如多臂老虎机模型）识别最需要干预的受益者，并通过AI调度干预提高听众参与度。

Result: AI干预显著提高了听众参与度，并导致孕产妇在产后补充铁和钙等健康行为的改善。

Conclusion: AI在孕产妇和儿童健康领域具有推动实质性改善的潜力。

Abstract: Automated voice calls with health information are a proven method for
disseminating maternal and child health information among beneficiaries and are
deployed in several programs around the world. However, these programs often
suffer from beneficiary dropoffs and poor engagement. In previous work, through
real-world trials, we showed that an AI model, specifically a restless bandit
model, could identify beneficiaries who would benefit most from live service
call interventions, preventing dropoffs and boosting engagement. However, one
key question has remained open so far: does such improved listenership via
AI-targeted interventions translate into beneficiaries' improved knowledge and
health behaviors? We present a first study that shows not only listenership
improvements due to AI interventions, but also simultaneously links these
improvements to health behavior changes. Specifically, we demonstrate that
AI-scheduled interventions, which enhance listenership, lead to statistically
significant improvements in beneficiaries' health behaviors such as taking iron
or calcium supplements in the postnatal period, as well as understanding of
critical health topics during pregnancy and infancy. This underscores the
potential of AI to drive meaningful improvements in maternal and child health.

</details>


### [93] [How Chain-of-Thought Works? Tracing Information Flow from Decoding, Projection, and Activation](https://arxiv.org/abs/2507.20758)
*Hao Yang,Qinghua Zhao,Lei Li*

Main category: cs.AI

TL;DR: 论文分析了Chain-of-Thought (CoT)提示的内部机制，发现其通过解码空间剪枝和任务依赖的神经元调制提升推理性能。


<details>
  <summary>Details</summary>
Motivation: 理解CoT提示的内部工作机制，以优化其设计并提升模型推理效率。

Method: 通过反向追踪信息流，分析解码、投影和激活阶段，定量研究CoT的工作原理。

Result: CoT可作为解码空间剪枝器，通过答案模板引导输出生成，且任务依赖地调节神经元激活。

Conclusion: 研究为CoT的机制解释提供了新框架，并为设计更高效、鲁棒的提示提供了关键见解。

Abstract: Chain-of-Thought (CoT) prompting significantly enhances model reasoning, yet
its internal mechanisms remain poorly understood. We analyze CoT's operational
principles by reversely tracing information flow across decoding, projection,
and activation phases. Our quantitative analysis suggests that CoT may serve as
a decoding space pruner, leveraging answer templates to guide output
generation, with higher template adherence strongly correlating with improved
performance. Furthermore, we surprisingly find that CoT modulates neuron
engagement in a task-dependent manner: reducing neuron activation in
open-domain tasks, yet increasing it in closed-domain scenarios. These findings
offer a novel mechanistic interpretability framework and critical insights for
enabling targeted CoT interventions to design more efficient and robust
prompts. We released our code and data at
https://anonymous.4open.science/r/cot-D247.

</details>


### [94] [evalSmarT: An LLM-Based Framework for Evaluating Smart Contract Generated Comments](https://arxiv.org/abs/2507.20774)
*Fatou Ndiaye Mbodji*

Main category: cs.AI

TL;DR: 论文提出了一个名为evalSmarT的框架，利用大语言模型（LLMs）评估智能合约注释生成的质量，解决了传统指标和人工评估的不足。


<details>
  <summary>Details</summary>
Motivation: 传统评估指标（如BLEU和ROUGE）无法捕捉领域特定细节，而人工评估成本高且不可扩展。

Method: 提出了evalSmarT框架，结合约40种LLMs和10种提示策略，支持400多种评估配置。

Result: 结果表明提示设计显著影响与人类判断的一致性，LLM评估提供了可扩展且语义丰富的替代方案。

Conclusion: LLM评估为智能合约注释生成提供了一种高效且语义丰富的评估方法。

Abstract: Smart contract comment generation has gained traction as a means to improve
code comprehension and maintainability in blockchain systems. However,
evaluating the quality of generated comments remains a challenge. Traditional
metrics such as BLEU and ROUGE fail to capture domain-specific nuances, while
human evaluation is costly and unscalable. In this paper, we present
\texttt{evalSmarT}, a modular and extensible framework that leverages large
language models (LLMs) as evaluators. The system supports over 400 evaluator
configurations by combining approximately 40 LLMs with 10 prompting strategies.
We demonstrate its application in benchmarking comment generation tools and
selecting the most informative outputs. Our results show that prompt design
significantly impacts alignment with human judgment, and that LLM-based
evaluation offers a scalable and semantically rich alternative to existing
methods.

</details>


### [95] [MMGraphRAG: Bridging Vision and Language with Interpretable Multimodal Knowledge Graphs](https://arxiv.org/abs/2507.20804)
*Xueyao Wan,Hang Yu*

Main category: cs.AI

TL;DR: MMGraphRAG通过构建多模态知识图谱（MMKG）和场景图优化视觉内容，解决了传统RAG方法在多模态信息融合和知识结构捕捉上的不足，提升了生成模型的推理能力和泛化性。


<details>
  <summary>Details</summary>
Motivation: 传统RAG方法在多模态信息融合和知识结构捕捉上存在不足，且需要大规模任务特定训练，泛化能力有限。

Method: 提出MMGraphRAG，通过场景图优化视觉内容，构建MMKG，结合文本知识图谱，利用谱聚类实现跨模态实体链接，并沿推理路径检索上下文指导生成。

Result: 在DocBench和MMLongBench数据集上达到最先进性能，展示了强大的领域适应性和清晰的推理路径。

Conclusion: MMGraphRAG有效解决了多模态RAG的局限性，提升了生成模型的推理能力和泛化性。

Abstract: Retrieval-Augmented Generation (RAG) enhances language model generation by
retrieving relevant information from external knowledge bases. However,
conventional RAG methods face the issue of missing multimodal information.
Multimodal RAG methods address this by fusing images and text through mapping
them into a shared embedding space, but they fail to capture the structure of
knowledge and logical chains between modalities. Moreover, they also require
large-scale training for specific tasks, resulting in limited generalizing
ability. To address these limitations, we propose MMGraphRAG, which refines
visual content through scene graphs and constructs a multimodal knowledge graph
(MMKG) in conjunction with text-based KG. It employs spectral clustering to
achieve cross-modal entity linking and retrieves context along reasoning paths
to guide the generative process. Experimental results show that MMGraphRAG
achieves state-of-the-art performance on the DocBench and MMLongBench datasets,
demonstrating strong domain adaptability and clear reasoning paths.

</details>


### [96] [Partially Observable Monte-Carlo Graph Search](https://arxiv.org/abs/2507.20951)
*Yang You,Vincent Thomas,Alex Schutz,Robert Skilton,Nick Hawes,Olivier Buffet*

Main category: cs.AI

TL;DR: 提出了一种新的离线算法POMCGS，用于解决大规模POMDP问题，通过动态折叠搜索树构建策略图，显著减少计算量，并能处理某些连续POMDP。


<details>
  <summary>Details</summary>
Motivation: 在时间或能量受限的POMDP应用中，离线预计算策略更为理想，但现有离线算法无法扩展到大规模POMDP。

Method: 提出了POMCGS算法，动态折叠搜索树构建策略图，结合动作渐进扩展和观测聚类方法。

Result: POMCGS能够处理现有离线算法无法解决的最具挑战性的POMDP，其策略值与最先进的在线算法相当。

Conclusion: POMCGS是一种高效且可扩展的离线POMDP求解方法，适用于大规模和连续POMDP问题。

Abstract: Currently, large partially observable Markov decision processes (POMDPs) are
often solved by sampling-based online methods which interleave planning and
execution phases. However, a pre-computed offline policy is more desirable in
POMDP applications with time or energy constraints. But previous offline
algorithms are not able to scale up to large POMDPs. In this article, we
propose a new sampling-based algorithm, the partially observable Monte-Carlo
graph search (POMCGS) to solve large POMDPs offline. Different from many online
POMDP methods, which progressively develop a tree while performing
(Monte-Carlo) simulations, POMCGS folds this search tree on the fly to
construct a policy graph, so that computations can be drastically reduced, and
users can analyze and validate the policy prior to embedding and executing it.
Moreover, POMCGS, together with action progressive widening and observation
clustering methods provided in this article, is able to address certain
continuous POMDPs. Through experiments, we demonstrate that POMCGS can generate
policies on the most challenging POMDPs, which cannot be computed by previous
offline algorithms, and these policies' values are competitive compared with
the state-of-the-art online POMDP algorithms.

</details>


### [97] [On the Limits of Hierarchically Embedded Logic in Classical Neural Networks](https://arxiv.org/abs/2507.20960)
*Bill Cochran*

Main category: cs.AI

TL;DR: 论文提出了一种基于神经网络深度的形式化模型，用于分析大型语言模型的推理限制，证明其逻辑表达能力存在严格上限。


<details>
  <summary>Details</summary>
Motivation: 研究动机是理解大型神经网络在语言任务中的推理局限性，尤其是其无法表达高阶逻辑的问题。

Method: 方法是将神经网络视为逻辑谓词空间上的线性算子，分析每层对逻辑推理能力的贡献，并证明其深度限制。

Result: 结果表明，神经网络的深度限制了其表达高阶逻辑的能力，导致无法准确表示复杂谓词（如计数）。

Conclusion: 结论是这一框架解释了幻觉、重复等问题，并为未来语言模型的架构扩展和可解释性策略提供了理论基础。

Abstract: We propose a formal model of reasoning limitations in large neural net models
for language, grounded in the depth of their neural architecture. By treating
neural networks as linear operators over logic predicate space we show that
each layer can encode at most one additional level of logical reasoning. We
prove that a neural network of depth a particular depth cannot faithfully
represent predicates in a one higher order logic, such as simple counting over
complex predicates, implying a strict upper bound on logical expressiveness.
This structure induces a nontrivial null space during tokenization and
embedding, excluding higher-order predicates from representability. Our
framework offers a natural explanation for phenomena such as hallucination,
repetition, and limited planning, while also providing a foundation for
understanding how approximations to higher-order logic may emerge. These
results motivate architectural extensions and interpretability strategies in
future development of language models.

</details>


### [98] [Core Safety Values for Provably Corrigible Agents](https://arxiv.org/abs/2507.20964)
*Aran Nayebi*

Main category: cs.AI

TL;DR: 提出首个可实现的修正性框架，在多步、部分可观测环境中提供可证明的保障，通过五个独立效用头实现严格的安全性和人类利益。


<details>
  <summary>Details</summary>
Motivation: 解决现有方法（如Constitutional AI或RLHF/RLAIF）将所有规范合并为一个学习标量的问题，确保在激励冲突时服从性和影响限制仍占主导。

Method: 使用五个结构分离的效用头（顺从性、开关访问保护、真实性、低影响行为和有限任务奖励），并通过严格权重间隙按词典序组合。

Result: 定理1和定理3证明在部分可观测环境中实现单轮和多步修正性，即使效用头学习存在误差，安全性仍可保障。

Conclusion: 框架将奖励黑客风险转化为评估质量问题，为当前LLM助手和未来自主系统提供清晰的实现指导。

Abstract: We introduce the first implementable framework for corrigibility, with
provable guarantees in multi-step, partially observed environments. Our
framework replaces a single opaque reward with five *structurally separate*
utility heads -- deference, switch-access preservation, truthfulness,
low-impact behavior via a belief-based extension of Attainable Utility
Preservation, and bounded task reward -- combined lexicographically by strict
weight gaps. Theorem 1 proves exact single-round corrigibility in the partially
observable off-switch game; Theorem 3 extends the guarantee to multi-step,
self-spawning agents, showing that even if each head is \emph{learned} to
mean-squared error $\varepsilon$ and the planner is $\varepsilon$-sub-optimal,
the probability of violating \emph{any} safety property is bounded while still
ensuring net human benefit. In contrast to Constitutional AI or RLHF/RLAIF,
which merge all norms into one learned scalar, our separation makes obedience
and impact-limits dominate even when incentives conflict. For open-ended
settings where adversaries can modify the agent, we prove that deciding whether
an arbitrary post-hack agent will ever violate corrigibility is undecidable by
reduction to the halting problem, then carve out a finite-horizon ``decidable
island'' where safety can be certified in randomized polynomial time and
verified with privacy-preserving, constant-round zero-knowledge proofs.
Consequently, the remaining challenge is the ordinary ML task of data coverage
and generalization: reward-hacking risk is pushed into evaluation quality
rather than hidden incentive leak-through, giving clearer implementation
guidance for today's LLM assistants and future autonomous systems.

</details>


### [99] [MIRAGE-Bench: LLM Agent is Hallucinating and Where to Find Them](https://arxiv.org/abs/2507.21017)
*Weichen Zhang,Yiyou Sun,Pohao Huang,Jiayue Pu,Heyue Lin,Dawn Song*

Main category: cs.AI

TL;DR: MIRAGE-Bench是首个用于评估LLM代理在交互场景中幻觉行为的统一基准，通过分类和系统测试揭示其失败模式。


<details>
  <summary>Details</summary>
Motivation: 现有评估方法分散且缺乏系统性，需统一基准以揭示LLM代理的幻觉行为及其风险。

Method: 提出三分类法，通过系统审计和快照策略合成测试用例，采用LLM-as-a-Judge范式评估。

Result: MIRAGE-Bench提供了可扩展的高保真评估方法，揭示了代理的失败模式。

Conclusion: 该研究为减少交互环境中的幻觉行为奠定了基础，并提供了实用见解。

Abstract: Hallucinations pose critical risks for large language model (LLM)-based
agents, often manifesting as hallucinative actions resulting from fabricated or
misinterpreted information within the cognitive context. While recent studies
have exposed such failures, existing evaluations remain fragmented and lack a
principled testbed. In this paper, we present MIRAGE-Bench--Measuring Illusions
in Risky AGEnt settings--the first unified benchmark for eliciting and
evaluating hallucinations in interactive LLM-agent scenarios. We begin by
introducing a three-part taxonomy to address agentic hallucinations: actions
that are unfaithful to (i) task instructions, (ii) execution history, or (iii)
environment observations. To analyze, we first elicit such failures by
performing a systematic audit of existing agent benchmarks, then synthesize
test cases using a snapshot strategy that isolates decision points in
deterministic and reproducible manners. To evaluate hallucination behaviors, we
adopt a fine-grained-level LLM-as-a-Judge paradigm with tailored risk-aware
prompts, enabling scalable, high-fidelity assessment of agent actions without
enumerating full action spaces. MIRAGE-Bench provides actionable insights on
failure modes of LLM agents and lays the groundwork for principled progress in
mitigating hallucinations in interactive environments.

</details>


### [100] [GenoMAS: A Multi-Agent Framework for Scientific Discovery via Code-Driven Gene Expression Analysis](https://arxiv.org/abs/2507.21035)
*Haoyang Liu,Yijiang Li,Haohan Wang*

Main category: cs.AI

TL;DR: GenoMAS是一个基于LLM的团队协作系统，结合结构化工作流和自主代理的灵活性，用于基因表达分析。


<details>
  <summary>Details</summary>
Motivation: 当前基因表达分析自动化方法存在局限性，要么工作流不灵活，要么自主代理缺乏科学严谨性。

Method: GenoMAS通过六个专业LLM代理协作，采用类型化消息传递协议和引导式规划框架，动态调整任务执行。

Result: 在GenoTEX基准测试中，GenoMAS在数据预处理和基因识别任务上分别达到89.13%和60.48%的指标，优于现有方法。

Conclusion: GenoMAS不仅性能优越，还能发现生物学上合理的基因-表型关联，代码已开源。

Abstract: Gene expression analysis holds the key to many biomedical discoveries, yet
extracting insights from raw transcriptomic data remains formidable due to the
complexity of multiple large, semi-structured files and the need for extensive
domain expertise. Current automation approaches are often limited by either
inflexible workflows that break down in edge cases or by fully autonomous
agents that lack the necessary precision for rigorous scientific inquiry.
GenoMAS charts a different course by presenting a team of LLM-based scientists
that integrates the reliability of structured workflows with the adaptability
of autonomous agents. GenoMAS orchestrates six specialized LLM agents through
typed message-passing protocols, each contributing complementary strengths to a
shared analytic canvas. At the heart of GenoMAS lies a guided-planning
framework: programming agents unfold high-level task guidelines into Action
Units and, at each juncture, elect to advance, revise, bypass, or backtrack,
thereby maintaining logical coherence while bending gracefully to the
idiosyncrasies of genomic data.
  On the GenoTEX benchmark, GenoMAS reaches a Composite Similarity Correlation
of 89.13% for data preprocessing and an F$_1$ of 60.48% for gene
identification, surpassing the best prior art by 10.61% and 16.85%
respectively. Beyond metrics, GenoMAS surfaces biologically plausible
gene-phenotype associations corroborated by the literature, all while adjusting
for latent confounders. Code is available at https://github.com/Liu-Hy/GenoMAS.

</details>


### [101] [A Survey of Self-Evolving Agents: On Path to Artificial Super Intelligence](https://arxiv.org/abs/2507.21046)
*Huan-ang Gao,Jiayi Geng,Wenyue Hua,Mengkang Hu,Xinzhe Juan,Hongzhang Liu,Shilong Liu,Jiahao Qiu,Xuan Qi,Yiran Wu,Hongru Wang,Han Xiao,Yuhang Zhou,Shaokun Zhang,Jiayi Zhang,Jinyu Xiang,Yixiong Fang,Qiwen Zhao,Dongrui Liu,Qihan Ren,Cheng Qian,Zhenghailong Wang,Minda Hu,Huazheng Wang,Qingyun Wu,Heng Ji,Mengdi Wang*

Main category: cs.AI

TL;DR: 这篇论文综述了自进化代理的研究现状，围绕“进化什么”、“何时进化”和“如何进化”三个维度，系统分析了其机制、方法和应用。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）的静态特性限制了其在动态环境中的适应能力，因此需要开发能够实时进化的自进化代理。

Method: 论文围绕三个维度（进化内容、进化时机和进化方法）分类分析了自进化代理的机制、算法设计和架构。

Result: 综述了自进化代理的评估指标、应用领域（如编程、教育和医疗）以及面临的挑战（如安全性、可扩展性）。

Conclusion: 论文为自进化代理的研究和实际部署提供了结构化框架，并为实现人工超级智能（ASI）指明了方向。

Abstract: Large Language Models (LLMs) have demonstrated strong capabilities but remain
fundamentally static, unable to adapt their internal parameters to novel tasks,
evolving knowledge domains, or dynamic interaction contexts. As LLMs are
increasingly deployed in open-ended, interactive environments, this static
nature has become a critical bottleneck, necessitating agents that can
adaptively reason, act, and evolve in real time. This paradigm shift -- from
scaling static models to developing self-evolving agents -- has sparked growing
interest in architectures and methods enabling continual learning and
adaptation from data, interactions, and experiences. This survey provides the
first systematic and comprehensive review of self-evolving agents, organized
around three foundational dimensions -- what to evolve, when to evolve, and how
to evolve. We examine evolutionary mechanisms across agent components (e.g.,
models, memory, tools, architecture), categorize adaptation methods by stages
(e.g., intra-test-time, inter-test-time), and analyze the algorithmic and
architectural designs that guide evolutionary adaptation (e.g., scalar rewards,
textual feedback, single-agent and multi-agent systems). Additionally, we
analyze evaluation metrics and benchmarks tailored for self-evolving agents,
highlight applications in domains such as coding, education, and healthcare,
and identify critical challenges and research directions in safety,
scalability, and co-evolutionary dynamics. By providing a structured framework
for understanding and designing self-evolving agents, this survey establishes a
roadmap for advancing adaptive agentic systems in both research and real-world
deployments, ultimately shedding lights to pave the way for the realization of
Artificial Super Intelligence (ASI), where agents evolve autonomously,
performing at or beyond human-level intelligence across a wide array of tasks.

</details>
