<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 21]
- [cs.AI](#cs.AI) [Total: 43]
- [cs.CR](#cs.CR) [Total: 14]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [SeRe: A Security-Related Code Review Dataset Aligned with Real-World Review Activities](https://arxiv.org/abs/2601.01042)
*Zixiao Zhao,Yanjie Jiang,Hui Liu,Kui Liu,Lu Zhang*

Main category: cs.SE

TL;DR: 本文介绍了SeRe数据集，这是一个专门针对安全相关代码审查的数据集，通过主动学习集成分类方法从373,824条原始审查中提取了6,732条安全相关审查，填补了现有数据集缺乏安全特定标注的空白。


<details>
  <summary>Details</summary>
Motivation: 软件安全漏洞可能导致严重后果，早期检测至关重要。虽然代码审查是防止安全缺陷的关键防御机制，但由于审查者对安全问题关注不足或缺乏专业知识，相关反馈仍然稀缺。现有数据集和研究主要关注通用代码审查评论，要么缺乏安全特定标注，要么规模太小无法支持大规模研究。

Method: 采用基于主动学习的集成分类方法构建数据集。该方法通过人工标注迭代优化模型预测，在保持合理召回率的同时实现高精度。使用微调的集成分类器从373,824条原始审查实例中提取了6,732条安全相关审查，确保跨多种编程语言的代表性。

Result: 构建了SeRe安全相关代码审查数据集，包含6,732条安全相关审查。统计分析表明SeRe总体上与现实世界安全相关审查分布一致。通过基准测试评估了现有代码审查评论生成方法在安全相关反馈生成方面的效果。

Conclusion: 通过发布SeRe数据集和基准测试结果，旨在推进自动化安全导向代码审查的研究，为开发更有效的安全软件工程实践做出贡献。该数据集填补了安全特定代码审查数据集的空白，支持大规模研究。

Abstract: Software security vulnerabilities can lead to severe consequences, making early detection essential. Although code review serves as a critical defense mechanism against security flaws, relevant feedback remains scarce due to limited attention to security issues or a lack of expertise among reviewers. Existing datasets and studies primarily focus on general-purpose code review comments, either lacking security-specific annotations or being too limited in scale to support large-scale research. To bridge this gap, we introduce \textbf{SeRe}, a \textbf{security-related code review dataset}, constructed using an active learning-based ensemble classification approach. The proposed approach iteratively refines model predictions through human annotations, achieving high precision while maintaining reasonable recall. Using the fine-tuned ensemble classifier, we extracted 6,732 security-related reviews from 373,824 raw review instances, ensuring representativeness across multiple programming languages. Statistical analysis indicates that SeRe generally \textbf{aligns with real-world security-related review distribution}. To assess both the utility of SeRe and the effectiveness of existing code review comment generation approaches, we benchmark state-of-the-art approaches on security-related feedback generation. By releasing SeRe along with our benchmark results, we aim to advance research in automated security-focused code review and contribute to the development of more effective secure software engineering practices.

</details>


### [2] [RovoDev Code Reviewer: A Large-Scale Online Evaluation of LLM-based Code Review Automation at Atlassian](https://arxiv.org/abs/2601.01129)
*Kla Tantithamthavorn,Yaotian Zou,Andy Wong,Michael Gupta,Zhe Wang,Mike Buller,Ryan Jiang,Matthew Watson,Minwoo Jeong,Kun Chen,Ming Wu*

Main category: cs.SE

TL;DR: RovoDev Code Reviewer是一个企业级LLM代码审查自动化工具，无需微调即可生成基于审查指导、上下文感知和质量检查的代码审查评论，在Atlassian的Bitbucket中大规模部署，显著提高了代码审查效率和质量。


<details>
  <summary>Details</summary>
Motivation: 尽管LLM驱动的代码审查评论生成方法有所进展，但设计企业级代码审查自动化工具仍面临实际挑战。本文旨在解决一个实际问题：如何在不进行微调的情况下，设计一个基于审查指导、上下文感知和质量检查的代码审查评论生成系统。

Method: 开发了RovoDev Code Reviewer，这是一个企业级LLM代码审查自动化工具，在Atlassian的开发生态系统中设计和部署，并与Bitbucket无缝集成。采用离线、在线和用户反馈评估相结合的方法，进行为期一年的评估。

Result: RovoDev Code Reviewer在38.70%的情况下生成能导致代码解决的审查评论（即触发后续提交中代码更改的评论）。该工具能够加速反馈周期（减少PR周期时间30.8%）、减轻审查员工作量（减少人工编写评论35.6%），并提高整体软件质量（发现错误并提供可行建议）。

Conclusion: RovoDev Code Reviewer是一个有效的企业级代码审查自动化工具，通过LLM技术在不进行微调的情况下实现了高质量的代码审查评论生成，显著提升了开发效率和软件质量，具有实际部署价值。

Abstract: Large Language Models (LLMs)-powered code review automation has the potential to transform code review workflows. Despite the advances of LLM-powered code review comment generation approaches, several practical challenges remain for designing enterprise-grade code review automation tools. In particular, this paper aims at answering the practical question: how can we design a review-guided, context-aware, quality-checked code review comment generation without fine-tuning?
  In this paper, we present RovoDev Code Reviewer, an enterprise-grade LLM-based code review automation tool designed and deployed at scale within Atlassian's development ecosystem with seamless integration into Atlassian's Bitbucket. Through the offline, online, user feedback evaluations over a one-year period, we conclude that RovoDev Code Reviewer is (1) effective in generating code review comments that could lead to code resolution for 38.70% (i.e., comments that triggered code changes in the subsequent commits); and (2) offers the promise of accelerating feedback cycles (i.e., decreasing the PR cycle time by 30.8%), alleviating reviewer workload (i.e., reducing the number of human-written comments by 35.6%), and improving overall software quality (i.e., finding errors with actionable suggestions).

</details>


### [3] [Abductive Vibe Coding (Extended Abstract)](https://arxiv.org/abs/2601.01199)
*Logan Murphy,Aren A. Babikian,Marsha Chechik*

Main category: cs.SE

TL;DR: 论文提出一个框架，用于为AI生成的代码提取可分析、半形式化的合理性依据，而不是直接判断正确性，而是生成一组条件，在这些条件下生成的代码可以被认为是充分的。


<details>
  <summary>Details</summary>
Motivation: 当AI模型生成软件工件（"氛围编码"）时，人类工程师需要负责验证它们。理想情况下，这种验证应该通过创建形式化的正确性证明来完成，但对于许多现实世界的氛围编码场景来说，特别是当AI生成工件的需求难以形式化时，这是不可行的。

Method: 提出一个框架，用于提取可分析、半形式化的合理性依据，以评估氛围编码工件的充分性。该框架不直接决定正确性，而是生成一组条件，在这些条件下生成的代码可以被认为是充分的。

Result: 论文描述了当前实现该框架的努力和预期的研究机会，这是一个正在进行的工作。

Conclusion: 该框架为难以形式化验证的AI生成代码提供了一种替代验证方法，通过提取半形式化的合理性依据和充分性条件，使工程师能够更有效地评估氛围编码工件的质量。

Abstract: When software artifacts are generated by AI models ("vibe coding"), human engineers assume responsibility for validating them. Ideally, this validation would be done through the creation of a formal proof of correctness. However, this is infeasible for many real-world vibe coding scenarios, especially when requirements for the AI-generated artifacts resist formalization. This extended abstract describes ongoing work towards the extraction of analyzable, semi-formal rationales for the adequacy of vibe-coded artifacts. Rather than deciding correctness directly, our framework produces a set of conditions under which the generated code can be considered adequate. We describe current efforts towards implementing our framework and anticipated research opportunities.

</details>


### [4] [Correctness isnt Efficiency: Runtime Memory Divergence in LLM-Generated Code](https://arxiv.org/abs/2601.01215)
*Prateek Rajput,Yewei Song,Abdoul Aziz Bonkoungou,Iyiola E. Olatunji,Abdoul Kader Kabore,Jacques Klein,Tegawendé F. Bissyandé*

Main category: cs.SE

TL;DR: 该论文提出了一个评估LLM生成代码运行时内存稳定性的框架，发现不同正确解决方案在内存和性能模式上存在显著差异，可能导致隐藏的操作风险。


<details>
  <summary>Details</summary>
Motivation: LLM生成的代码虽然能通过单元测试，但通过测试并不能保证可靠的运行时行为。不同正确解决方案在内存和性能模式上可能存在巨大差异，这会带来隐藏的操作风险。

Method: 提出了一个测量执行时间内存稳定性的框架：在解决方案层面引入动态平均配对距离（DMPD），使用动态时间规整比较内存使用轨迹的形状，并将其转换为单调峰值轮廓以减少瞬态噪声；在模型层面聚合DMPD得到模型不稳定分数（MIS）。

Result: 在BigOBench和CodeContests上的实验显示，正确解决方案之间存在显著的运行时差异。不稳定性通常随着采样温度升高而增加，即使pass@1有所改善。稳定性测量与认知复杂度和圈复杂度等软件工程指标存在相关性。

Conclusion: 研究结果支持在CI/CD中采用稳定性感知的选择策略，在不牺牲正确性的前提下降低操作风险。稳定性测量可以揭示操作行为与可维护性之间的联系。

Abstract: Large language models (LLMs) can generate programs that pass unit tests, but passing tests does not guarantee reliable runtime behavior. We find that different correct solutions to the same task can show very different memory and performance patterns, which can lead to hidden operational risks. We present a framework to measure execution-time memory stability across multiple correct generations. At the solution level, we introduce Dynamic Mean Pairwise Distance (DMPD), which uses Dynamic Time Warping to compare the shapes of memory-usage traces after converting them into Monotonic Peak Profiles (MPPs) to reduce transient noise. Aggregating DMPD across tasks yields a model-level Model Instability Score (MIS). Experiments on BigOBench and CodeContests show substantial runtime divergence among correct solutions. Instability often increases with higher sampling temperature even when pass@1 improves. We also observe correlations between our stability measures and software engineering indicators such as cognitive and cyclomatic complexity, suggesting links between operational behavior and maintainability. Our results support stability-aware selection among passing candidates in CI/CD to reduce operational risk without sacrificing correctness. Artifacts are available.

</details>


### [5] [HD-GEN: A High-Performance Software System for Human Mobility Data Generation Based on Patterns of Life](https://arxiv.org/abs/2601.01219)
*Hossein Amiri,Joon-Seok Kim,Hamdi Kavak,Andrew Crooks,Dieter Pfoser,Carola Wenk,Andreas Züfle*

Main category: cs.SE

TL;DR: 提出一个结合真实数据与模拟数据的综合软件管道，用于校准、生成、处理和可视化大规模个体层面的人类移动数据集


<details>
  <summary>Details</summary>
Motivation: 真实轨迹数据集受数据稀疏性和参与者偏差限制，而合成数据缺乏真实性，需要结合两者优势来获得既真实又可扩展的人类移动数据

Method: 开发包含四个组件的系统：1)基于OpenStreetMap的地理基础数据生成引擎；2)基于遗传算法的校准模块；3)数据处理套件；4)可视化模块

Result: 创建了一个能够生成大规模、真实且可控的人类移动数据集的综合软件管道，将实证数据的真实性与模拟数据的可扩展性相结合

Conclusion: 该软件管道填补了真实数据与合成数据之间的空白，为各种应用提供了既真实又可扩展的人类移动数据集，支持模型训练、基准测试和可视化分析

Abstract: Understanding individual-level human mobility is critical for a wide range of applications. Real-world trajectory datasets provide valuable insights into actual movement behaviors but are often constrained by data sparsity and participant bias. Synthetic data, by contrast, offer scalability and flexibility but frequently lack realism. To address this gap, we introduce a comprehensive software pipeline for calibrating, generating, processing, and visualizing large-scale individual-level human mobility datasets that combine the realism of empirical data with the control and extensibility of Patterns-of-Life simulations. Our system consists of four integrated components. (1) a data generation engine constructs geographically grounded simulations using OpenStreetMap data to produce diverse mobility logs. (2) a genetic algorithm-based calibration module fine-tunes simulation parameters to align with real-world mobility characteristics, such as daily trip counts and radius of gyration, enabling realistic behavioral modeling. (3) a data processing suite transforms raw simulation logs into structured formats suitable for downstream applications, including model training and benchmarking. (4) a visualization module extracts key mobility patterns and insights from the processed datasets and presents them through intuitive visual analytics for improved interpretability.

</details>


### [6] [Atomizer: An LLM-based Collaborative Multi-Agent Framework for Intent-Driven Commit Untangling](https://arxiv.org/abs/2601.01233)
*Kangchen Zhu,Zhiliang Tian,Shangwen Wang,Mingyue Leng,Xiaoguang Mao*

Main category: cs.SE

TL;DR: Atomizer是一个基于多智能体协作的复合提交解耦框架，通过意图导向的思维链和分组-评审循环机制，显著提升了代码变更语义理解的准确性。


<details>
  <summary>Details</summary>
Motivation: 现有复合提交解耦方法存在两个根本性限制：过度依赖结构信息而忽视语义意图，以及缺乏类似人工评审的反思和优化机制。这些限制影响了程序理解和维护的效率。

Method: 提出Atomizer框架，包含两个核心创新：1) 意图导向思维链策略，利用大语言模型从结构和语义层面推断代码变更意图；2) 分组-评审智能体协作循环，模拟人工评审过程迭代优化分组结果。

Result: 在C#和Java基准数据集上的实验表明，Atomizer显著优于现有方法，平均性能超过最先进的图基方法6.0%（C#）和5.5%（Java），在复杂提交上的优势扩大到16%以上。

Conclusion: Atomizer通过结合语义意图理解和迭代优化机制，有效解决了复合提交解耦中的关键挑战，为软件维护和理解提供了更有效的自动化工具。

Abstract: Composite commits, which entangle multiple unrelated concerns, are prevalent in software development and significantly hinder program comprehension and maintenance. Existing automated untangling methods, particularly state-of-the-art graph clustering-based approaches, are fundamentally limited by two issues. (1) They over-rely on structural information, failing to grasp the crucial semantic intent behind changes, and (2) they operate as ``single-pass'' algorithms, lacking a mechanism for the critical reflection and refinement inherent in human review processes. To overcome these challenges, we introduce Atomizer, a novel collaborative multi-agent framework for composite commit untangling. To address the semantic deficit, Atomizer employs an Intent-Oriented Chain-of-Thought (IO-CoT) strategy, which prompts large language models (LLMs) to infer the intent of each code change according to both the structure and the semantic information of code. To overcome the limitations of ``single-pass'' grouping, we employ two agents to establish a grouper-reviewer collaborative refinement loop, which mirrors human review practices by iteratively refining groupings until all changes in a cluster share the same underlying semantic intent. Extensive experiments on two benchmark C# and Java datasets demonstrate that Atomizer significantly outperforms several representative baselines. On average, it surpasses the state-of-the-art graph-based methods by over 6.0% on the C# dataset and 5.5% on the Java dataset. This superiority is particularly pronounced on complex commits, where Atomizer's performance advantage widens to over 16%.

</details>


### [7] [CatchAll: Repository-Aware Exception Handling with Knowledge-Guided LLMs](https://arxiv.org/abs/2601.01271)
*Qingxiao Tao,Xiaodong Gu,Hao Zhong,Beijun Shen*

Main category: cs.SE

TL;DR: CatchAll是一个基于LLM的仓库感知异常处理方法，通过三层知识指导LLM生成准确的异常处理代码，在仓库级异常处理任务上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 异常处理是编程中的重要错误恢复机制，但LLM在仓库级别的异常处理上表现不佳，因为复杂的依赖关系和上下文约束。需要开发能够理解仓库级上下文的异常处理方法。

Method: CatchAll为LLM提供三层异常处理知识：1) API级异常知识，来自经验构建的API-异常映射；2) 仓库级执行上下文，通过建模目标代码周围的上下文调用轨迹捕获异常传播；3) 跨仓库处理知识，从历史代码中挖掘的可重用异常处理模式。这些知识被编码为结构化提示来指导LLM生成代码。

Result: 在两个新的仓库感知异常处理基准测试中，CatchAll显著优于现有方法：CodeBLEU得分0.31（vs. 0.27），意图预测准确率60.1%（vs. 48.0%），Pass@1为29%（vs. 25%）。

Conclusion: CatchAll通过结合API级、仓库级和跨仓库的异常处理知识，有效提升了LLM在仓库级异常处理任务上的性能，证明了其在真实世界代码库异常处理中的有效性。

Abstract: Exception handling is a vital forward error-recovery mechanism in many programming languages, enabling developers to manage runtime anomalies through structured constructs (e.g., try-catch blocks). Improper or missing exception handling often leads to severe consequences, including system crashes and resource leaks. While large language models (LLMs) have demonstrated strong capabilities in code generation, they struggle with exception handling at the repository level, due to complex dependencies and contextual constraints. In this work, we propose CatchAll, a novel LLM-based approach for repository-aware exception handling. CatchAll equips LLMs with three complementary layers of exception-handling knowledge: (1) API-level exception knowledge, obtained from an empirically constructed API-exception mapping that characterizes the exception-throwing behaviors of APIs in real-world codebases; (2) repository-level execution context, which captures exception propagation by modeling contextual call traces around the target code; and (3) cross-repository handling knowledge, distilled from reusable exception-handling patterns mined from historical code across projects. The knowledge is encoded into structured prompts to guide the LLM in generating accurate and context-aware exception-handling code. To evaluate CatchAll, we construct two new benchmarks for repository-aware exception handling: a large-scale dataset RepoExEval and an executable subset RepoExEval-Exec. Experiments demonstrate that RepoExEval consistently outperforms state-of-the-art baselines, achieving a CodeBLEU score of 0.31 (vs. 0.27% for the best baseline), intent prediction accuracy of 60.1% (vs. 48.0%), and Pass@1 of 29% (vs. 25%). These results affirm RepoExEval's effectiveness in real-world repository-level exception handling.

</details>


### [8] [Adaptive Hierarchical Evaluation of LLMs and SAST tools for CWE Prediction in Python](https://arxiv.org/abs/2601.01320)
*Muntasir Adnan,Carlos C. N. Kuhn*

Main category: cs.SE

TL;DR: ALPHA是首个函数级Python漏洞检测基准，使用分层感知的CWE特定惩罚来评估LLMs和SAST工具，发现LLMs表现优于SAST但预测一致性差异巨大。


<details>
  <summary>Details</summary>
Motivation: 现有代码漏洞检测基准采用二元分类，缺乏CWE级别的特异性，无法为迭代修正系统提供可操作的反馈。需要评估LLMs和SAST工具在实际漏洞检测中的表现差异。

Method: 提出ALPHA基准，使用分层感知的CWE特定惩罚来评估函数级Python代码漏洞检测。区分过度泛化、过度规范和横向错误，反映诊断效用的实际差异。评估了7个LLMs和2个SAST工具。

Result: LLMs在漏洞检测方面显著优于SAST工具，但SAST在检测发生时表现出更高的精确度。不同模型间的预测一致性差异巨大（8.26%-81.87%一致率），对反馈驱动系统有重要影响。

Conclusion: ALPHA基准为代码漏洞检测提供了更精细的评估框架，揭示了LLMs和SAST工具的性能差异。未来可将ALPHA惩罚机制融入监督微调，实现原则性的分层感知漏洞检测。

Abstract: Large Language Models have become integral to software development, yet they frequently generate vulnerable code. Existing code vulnerability detection benchmarks employ binary classification, lacking the CWE-level specificity required for actionable feedback in iterative correction systems. We present ALPHA (Adaptive Learning via Penalty in Hierarchical Assessment), the first function-level Python benchmark that evaluates both LLMs and SAST tools using hierarchically aware, CWE-specific penalties. ALPHA distinguishes between over-generalisation, over-specification, and lateral errors, reflecting practical differences in diagnostic utility. Evaluating seven LLMs and two SAST tools, we find LLMs substantially outperform SAST, though SAST demonstrates higher precision when detections occur. Critically, prediction consistency varies dramatically across models (8.26%-81.87% agreement), with significant implications for feedback-driven systems. We further outline a pathway for future work incorporating ALPHA penalties into supervised fine-tuning, which could provide principled hierarchy-aware vulnerability detection pending empirical validation.

</details>


### [9] [GlycoPy: An Equation-Oriented and Object-Oriented Software for Hierarchical Modeling, Optimization, and Control in Python](https://arxiv.org/abs/2601.01413)
*Yingjie Ma,Jing Guo,Richard D. Braatz*

Main category: cs.SE

TL;DR: GlycoPy是一个用于过程建模、优化和非线性模型预测控制（NMPC）的Python软件框架，旨在解决工业过程中线性MPC的局限性，通过分层建模和高效算法实现NMPC的实际应用。


<details>
  <summary>Details</summary>
Motivation: 工业过程通常是非线性的，但现有MPC应用大多使用线性模型，这限制了MPC在宽操作范围内的性能和适用性。非线性MPC面临分层模型开发工具缺乏和算法实现效率低下的挑战。

Method: 开发了GlycoPy软件框架，这是一个面向方程、面向对象的Python工具，支持分层建模，包含参数估计、动态优化和NMPC算法，并允许用户自定义仿真、优化和控制算法。

Result: 通过三个案例研究（从简单的微分代数方程系统到多尺度生物过程模型）验证了GlycoPy的建模、优化和NMPC能力，展示了其在实际应用中的有效性。

Conclusion: GlycoPy有潜力弥合先进NMPC算法与其在真实世界（生物）化学过程中实际应用之间的差距，使NMPC对分层系统更加实用。

Abstract: Most existing model predictive control (MPC) applications in process industries employ lin-ear models, although real-world (bio)chemical processes are typically nonlinear. The use of linear models limits the performance and applicability of MPC for processes that span a wide range of operating conditions. A challenge in employing nonlinear models in MPC for com-plex systems is the lack of tools that facilitate hierarchical model development, as well as lack of efficient implementations of the corresponding nonlinear MPC (NMPC) algorithms. As a step towards making NMPC more practical for hierarchical systems, we introduce Gly-coPy, an equation-oriented, object-oriented software framework for process modeling, opti-mization, and NMPC in Python. GlycoPy enables users to focus on writing equations for modeling while supporting hierarchical modeling. GlycoPy includes algorithms for parame-ter estimation, dynamic optimization, and NMPC, and allows users to customize the simula-tion, optimization, and control algorithms. Three case studies, ranging from a simple differ-ential algebraic equation system to a multiscale bioprocess model, validate the modeling, optimization, and NMPC capabilities of GlycoPy. GlycoPy has the potential to bridge the gap between advanced NMPC algorithms and their practical application in real-world (bio)chemical processes.

</details>


### [10] [SWE-Lego: Pushing the Limits of Supervised Fine-tuning for Software Issue Resolving](https://arxiv.org/abs/2601.01426)
*Chaofan Tao,Jierun Chen,Yuxin Jiang,Kaiqi Kou,Shaowei Wang,Ruoyu Wang,Xiaohui Li,Sidi Yang,Yiming Du,Jianbo Dai,Zhiming Mao,Xinyu Wang,Lifeng Shang,Haoli Bai*

Main category: cs.SE

TL;DR: SWE-Lego是一个专为软件工程问题解决设计的监督微调方法，仅通过轻量级SFT就能达到最先进性能，包含高质量数据集、改进的SFT流程和测试时扩展三个核心模块。


<details>
  <summary>Details</summary>
Motivation: 当前软件工程问题解决方法通常依赖复杂的训练范式（如中期训练、SFT、强化学习及其组合），本文探索如何通过轻量级的纯SFT方法在SWE任务上达到极限性能。

Method: SWE-Lego包含三个核心构建块：1）SWE-Lego数据集，包含32k高质量任务实例和18k已验证轨迹，结合真实和合成数据；2）改进的SFT流程，包含错误掩码和基于难度的课程学习；3）基于SFT基础的测试时扩展评估和改进。

Result: 仅使用前两个构建块，SWE-Lego模型在SWE-bench Verified上达到开源模型中最先进性能：SWE-Lego-Qwen3-8B达到42.2%，SWE-Lego-Qwen3-32B达到52.6%。通过测试时扩展，性能进一步提升：8B模型从42.2%提升到49.6%，32B模型从52.6%提升到58.8%（TTS@16）。

Conclusion: SWE-Lego展示了通过精心设计的轻量级纯SFT方法可以在软件工程问题解决任务上达到最先进性能，无需复杂的训练范式组合，为SWE任务的高效训练提供了有效方案。

Abstract: We present SWE-Lego, a supervised fine-tuning (SFT) recipe designed to achieve state-ofthe-art performance in software engineering (SWE) issue resolving. In contrast to prevalent methods that rely on complex training paradigms (e.g., mid-training, SFT, reinforcement learning, and their combinations), we explore how to push the limits of a lightweight SFT-only approach for SWE tasks. SWE-Lego comprises three core building blocks, with key findings summarized as follows: 1) the SWE-Lego dataset, a collection of 32k highquality task instances and 18k validated trajectories, combining real and synthetic data to complement each other in both quality and quantity; 2) a refined SFT procedure with error masking and a difficulty-based curriculum, which demonstrably improves action quality and overall performance. Empirical results show that with these two building bricks alone,the SFT can push SWE-Lego models to state-of-the-art performance among open-source models of comparable size on SWE-bench Verified: SWE-Lego-Qwen3-8B reaches 42.2%, and SWE-Lego-Qwen3-32B attains 52.6%. 3) We further evaluate and improve test-time scaling (TTS) built upon the SFT foundation. Based on a well-trained verifier, SWE-Lego models can be significantly boosted--for example, 42.2% to 49.6% and 52.6% to 58.8% under TTS@16 for the 8B and 32B models, respectively.

</details>


### [11] [Group versus Individual Review Requests: Tradeoffs in Speed and Quality at Mozilla Firefox](https://arxiv.org/abs/2601.01514)
*Matej Kucera,Marco Castelluccio,Daniel Feitosa,Ayushi Rastogi*

Main category: cs.SE

TL;DR: 该研究探讨了代码审查中群体审查请求与个体审查请求对审查速度和质量的影响，发现在Mozilla Firefox项目中，群体审查与更高的审查质量相关（更少的回归），但对审查速度影响不大。


<details>
  <summary>Details</summary>
Motivation: 代码审查速度是衡量软件开发和开发者满意度的重要指标，但现有研究对审查分配过程（特别是群体审查请求）的作用尚不清楚。群体审查请求允许将代码变更分配给一个审查者群体，任何成员都可以审查，这与分配给特定审查者的个体审查请求不同。

Method: 研究分析了Mozilla Firefox项目中约66,000个修订，结合统计建模和从业者焦点小组讨论的观点。研究将群体审查请求与个体审查请求进行比较，借鉴管理学中的共享任务队列理论。

Result: 群体审查与更高的审查质量相关（表现为更少的回归），但与审查速度的关联可以忽略不计。此外，从业者认为群体审查还有平衡工作分配和为新手审查者提供培训机会等好处。

Conclusion: 群体审查请求在提高审查质量方面有积极效果，但对审查速度影响有限。这种方法有助于改善代码审查实践，特别是在质量保证和工作分配平衡方面。

Abstract: The speed at which code changes are integrated into the software codebase, also referred to as code review velocity, is a prevalent industry metric for improved throughput and developer satisfaction. While prior studies have explored factors influencing review velocity, the role of the review assignment process, particularly the `group review request', is unclear. In group review requests, available on platforms like Phabricator, GitHub, and Bitbucket, a code change is assigned to a reviewer group, allowing any member to review it, unlike individual review assignments to specific reviewers. Drawing parallels with shared task queues in Management Sciences, this study examines the effects of group versus individual review requests on velocity and quality. We investigate approximately 66,000 revisions in the Mozilla Firefox project, combining statistical modeling with practitioner views from a focus group discussion. Our study associates group reviews with improved review quality, characterized by fewer regressions, while having a negligible association with review velocity. Additional perceived benefits include balanced work distribution and training opportunities for new reviewers.

</details>


### [12] [LIA: Supervised Fine-Tuning of Large Language Models for Automatic Issue Assignment](https://arxiv.org/abs/2601.01780)
*Arsham Khosravani,Alireza Hosseinpour,Arshia Akhavan,Mehdi Keshani,Abbas Heydarnoori*

Main category: cs.SE

TL;DR: LIA使用监督微调将DeepSeek-R1-Distill-Llama-8B大语言模型适配于问题分配任务，直接从问题标题和描述生成开发者推荐排名，相比基础模型和现有方法在Hit@1指标上提升显著。


<details>
  <summary>Details</summary>
Motivation: 软件维护中的问题分配过程通常依赖人工处理，存在不一致性和错误率高的问题，尤其是在大型开源项目中每月有数千个新问题报告。现有自动化方法依赖大量项目特定训练数据或稀疏嘈杂的关系信息，效果有限。

Method: 提出LIA（基于LLM的问题分配）方法，通过监督微调将DeepSeek-R1-Distill-Llama-8B大语言模型适配于自动问题分配任务。利用LLM对自然语言和软件相关文本的预训练语义理解能力，直接从问题标题和描述学习生成开发者推荐排名，基于历史问题-开发者分配模式推断最可能处理新问题的开发者。

Result: LIA相比基础预训练模型在Hit@1指标上提升高达+187.8%，相比四种领先的问题分配方法在Hit@1得分上提升高达+211.2%。这些结果突显了领域适配LLM在软件维护任务中的有效性。

Conclusion: LIA为问题分配提供了一个实用、高性能的解决方案，证明了领域适配大语言模型在软件维护任务中的有效性，能够显著提升问题分配的准确性和效率。

Abstract: Issue assignment is a critical process in software maintenance, where new issue reports are validated and assigned to suitable developers. However, manual issue assignment is often inconsistent and error-prone, especially in large open-source projects where thousands of new issues are reported monthly. Existing automated approaches have shown promise, but many rely heavily on large volumes of project-specific training data or relational information that is often sparse and noisy, which limits their effectiveness. To address these challenges, we propose LIA (LLM-based Issue Assignment), which employs supervised fine-tuning to adapt an LLM, DeepSeek-R1-Distill-Llama-8B in this work, for automatic issue assignment. By leveraging the LLM's pretrained semantic understanding of natural language and software-related text, LIA learns to generate ranked developer recommendations directly from issue titles and descriptions. The ranking is based on the model's learned understanding of historical issue-to-developer assignments, using patterns from past tasks to infer which developers are most likely to handle new issues. Through comprehensive evaluation, we show that LIA delivers substantial improvements over both its base pretrained model and state-of-the-art baselines. It achieves up to +187.8% higher Hit@1 compared to the DeepSeek-R1-Distill-Llama-8B pretrained base model, and outperforms four leading issue assignment methods by as much as +211.2% in Hit@1 score. These results highlight the effectiveness of domain-adapted LLMs for software maintenance tasks and establish LIA as a practical, high-performing solution for issue assignment.

</details>


### [13] [The Machine Learning Canvas: Empirical Findings on Why Strategy Matters More Than AI Code Generation](https://arxiv.org/abs/2601.01839)
*Martin Prause*

Main category: cs.SE

TL;DR: 本研究开发并测试了机器学习画布框架，通过调查150名数据科学家发现，ML项目成功的关键因素包括战略、流程、生态系统和支持，这些因素相互关联，而AI编码助手虽能加速编码但无法替代战略思考。


<details>
  <summary>Details</summary>
Motivation: 尽管AI编码助手日益普及，但超过80%的机器学习项目未能交付实际商业价值，因此需要研究影响ML项目成功的关键因素。

Method: 创建并测试机器学习画布框架，结合商业战略、软件工程和数据科学，通过调查150名数据科学家并使用统计建模分析其回答。

Result: 识别出四个关键成功因素：战略（明确目标和规划）、流程（工作执行方式）、生态系统（工具和基础设施）、支持（组织支持和资源）。这些因素相互关联，例如强大的组织支持导致更清晰的战略（β=0.432, p<0.001），进而改善工作流程（β=0.428, p<0.001）并构建更好的基础设施（β=0.547, p<0.001）。

Conclusion: AI编码助手虽然能加速编码过程，但无法保证项目成功，因为它们只能解决"如何"编码的问题，而无法替代战略思考中的"为什么"和"做什么"。项目成功需要战略、流程、生态系统和支持这四个相互关联因素的协同作用。

Abstract: Despite the growing popularity of AI coding assistants, over 80% of machine learning (ML) projects fail to deliver real business value. This study creates and tests a Machine Learning Canvas, a practical framework that combines business strategy, software engineering, and data science in order to determine the factors that lead to the success of ML projects. We surveyed 150 data scientists and analyzed their responses using statistical modeling. We identified four key success factors: Strategy (clear goals and planning), Process (how work gets done), Ecosystem (tools and infrastructure), and Support (organizational backing and resources). Our results show that these factors are interconnected - each one affects the next. For instance, strong organizational support results in a clearer strategy (β= 0.432, p < 0.001), which improves work processes (β= 0.428, p < 0.001) and builds better infrastructure (β= 0.547, p < 0.001). Together, these elements determine whether a project succeeds. The surprising finding? Although AI assistants make coding faster, they don't guarantee project success. AI assists with the "how" of coding but cannot replace the "why" and "what" of strategic thinking.

</details>


### [14] [A Defect is Being Born: How Close Are We? A Time Sensitive Forecasting Approach](https://arxiv.org/abs/2601.01921)
*Mikel Robredo,Matteo Esposito,Fabio Palomba,Rafael Peñaloza,Valentina Lenarduzzi*

Main category: cs.SE

TL;DR: 该研究探索时间敏感技术在软件缺陷预测中的有效性，旨在通过时间序列方法预测未来缺陷密度并识别缺陷早期征兆。


<details>
  <summary>Details</summary>
Motivation: 随着软件系统持续演化，需要时间敏感的方法能够在缺陷显现之前进行预测。现有研究已通过即时预测实现了对即将发生故障的准确预测，但需要更早的时间敏感预测方法。

Method: 训练多种时间敏感预测技术来预测软件项目的未来缺陷密度，并识别缺陷发生前的早期征兆。

Result: 预期结果为早期缺陷倾向性评估方法的有效性提供实证证据。

Conclusion: 该研究旨在为软件缺陷预测领域提供时间敏感预测方法的实证支持，帮助更早地识别和预防软件缺陷。

Abstract: Background. Defect prediction has been a highly active topic among researchers in the Empirical Software Engineering field. Previous literature has successfully achieved the most accurate prediction of an incoming fault and identified the features and anomalies that precede it through just-in-time prediction. As software systems evolve continuously, there is a growing need for time-sensitive methods capable of forecasting defects before they manifest.
  Aim. Our study seeks to explore the effectiveness of time-sensitive techniques for defect forecasting. Moreover, we aim to investigate the early indicators that precede the occurrence of a defect.
  Method. We will train multiple time-sensitive forecasting techniques to forecast the future bug density of a software project, as well as identify the early symptoms preceding the occurrence of a defect.
  Expected results. Our expected results are translated into empirical evidence on the effectiveness of our approach for early estimation of bug proneness.

</details>


### [15] [The Invisible Hand of AI Libraries Shaping Open Source Projects and Communities](https://arxiv.org/abs/2601.01944)
*Matteo Esposito,Andrea Janes,Valentina Lenarduzzi,Davide Taibi*

Main category: cs.SE

TL;DR: 研究分析了Python和Java开源项目中AI库的采用情况及其对开发活动、社区参与和代码复杂性的影响


<details>
  <summary>Details</summary>
Motivation: 尽管AI在开源软件项目中日益普及，但其采用情况和影响尚未得到充分探索。开源软件已成为计算机科学的基石，而AI在其中扮演着越来越重要的角色，但AI库如何影响开源项目的发展、技术生态系统和社区参与仍不清楚。

Method: 对157.7k个潜在开源仓库进行大规模分析，使用仓库指标和软件指标，比较采用AI库的项目与未采用AI库的项目。通过量化方法评估开发活动、社区参与和代码复杂性等方面的差异。

Result: 预计会发现采用AI库的开源项目与未采用AI库的项目在开发活动、社区参与和代码复杂性方面存在可测量的差异。研究将提供基于证据的见解，展示AI集成如何重塑软件开发实践。

Conclusion: 该研究将揭示AI库在开源项目中的采用模式及其对软件开发实践的影响，为理解AI如何改变开源生态系统提供实证基础，帮助开发者和项目管理者更好地应对AI集成带来的机遇和挑战。

Abstract: In the early 1980s, Open Source Software emerged as a revolutionary concept amidst the dominance of proprietary software. What began as a revolutionary idea has now become the cornerstone of computer science. Amidst OSS projects, AI is increasing its presence and relevance. However, despite the growing popularity of AI, its adoption and impacts on OSS projects remain underexplored.
  We aim to assess the adoption of AI libraries in Python and Java OSS projects and examine how they shape development, including the technical ecosystem and community engagement. To this end, we will perform a large-scale analysis on 157.7k potential OSS repositories, employing repository metrics and software metrics to compare projects adopting AI libraries against those that do not. We expect to identify measurable differences in development activity, community engagement, and code complexity between OSS projects that adopt AI libraries and those that do not, offering evidence-based insights into how AI integration reshapes software development practices.

</details>


### [16] [Reporting LLM Prompting in Automated Software Engineering: A Guideline Based on Current Practices and Expectations](https://arxiv.org/abs/2601.01954)
*Alexander Korn,Lea Zaruchas,Chetan Arora,Andreas Metzger,Sven Smolka,Fanyu Wang,Andreas Vogelsang*

Main category: cs.SE

TL;DR: 该研究分析了SE领域LLM研究中提示工程报告的现状，发现当前实践与审稿人期望存在显著差距，提出了结构化报告指南以提升透明度与可复现性。


<details>
  <summary>Details</summary>
Motivation: LLM在软件工程任务中的应用日益增多，但提示相关的决策缺乏系统透明的文档记录，这阻碍了研究的可复现性和可比性。

Method: 采用两阶段实证研究：1) 分析2022年以来顶级SE会议近300篇论文的提示设计、测试和优化报告现状；2) 调查105名程序委员会成员对LLM研究中提示报告的期望。

Result: 研究发现当前实践与审稿人期望存在显著不一致，特别是在版本披露、提示理由和有效性威胁方面。基于此提出了区分必要、期望和优秀报告要素的结构化指南。

Conclusion: 该指南旨在提高LLM驱动的SE研究的透明度、可复现性和方法严谨性，是改善该领域研究实践的重要一步。

Abstract: Large Language Models, particularly decoder-only generative models such as GPT, are increasingly used to automate Software Engineering tasks. These models are primarily guided through natural language prompts, making prompt engineering a critical factor in system performance and behavior. Despite their growing role in SE research, prompt-related decisions are rarely documented in a systematic or transparent manner, hindering reproducibility and comparability across studies. To address this gap, we conducted a two-phase empirical study. First, we analyzed nearly 300 papers published at the top-3 SE conferences since 2022 to assess how prompt design, testing, and optimization are currently reported. Second, we surveyed 105 program committee members from these conferences to capture their expectations for prompt reporting in LLM-driven research. Based on the findings, we derived a structured guideline that distinguishes essential, desirable, and exceptional reporting elements. Our results reveal significant misalignment between current practices and reviewer expectations, particularly regarding version disclosure, prompt justification, and threats to validity. We present our guideline as a step toward improving transparency, reproducibility, and methodological rigor in LLM-based SE research.

</details>


### [17] [The State of Open Science in Software Engineering Research: A Case Study of ICSE Artifacts](https://arxiv.org/abs/2601.02066)
*Al Muttakin,Saikat Mondal,Chanchal Roy*

Main category: cs.SE

TL;DR: 评估ICSE会议过去十年100个复制包的可用性，发现仅40%可执行，其中仅32.5%无需修改，仅35%能复现原始结果，揭示了可用性与可复现性之间的显著差距。


<details>
  <summary>Details</summary>
Motivation: 尽管软件工程研究中复制包共享已成为标准实践，但其实际可用性尚未得到充分探索。特别是缺乏对SE研究中复制包可执行性和可复现性的全面研究，本研究旨在填补这一空白。

Method: 评估过去十年（2015-2024）ICSE会议发表的100个复制包，从四个方面进行评估：1）可执行性；2）执行所需的工作量和修改；3）导致执行失败的挑战；4）原始结果的可复现性。总共花费约650人时执行这些构件并复现研究结果。

Result: 仅40%的构件可执行，其中32.5%（13/40）无需修改即可运行。17.5%（7/40）需要低工作量，82.5%（33/40）需要中到高工作量。识别出5种常见修改类型和13种导致执行失败的挑战。在可执行构件中，仅35%（14/40）能复现原始结果。

Conclusion: 研究发现构件可用性、可执行性和可复现性之间存在显著差距。提出了三项可操作的指南，以改进研究构件的准备、文档和审查，从而加强SE研究中开放科学实践的严谨性和可持续性。

Abstract: Replication packages are crucial for enabling transparency, validation, and reuse in software engineering (SE) research. While artifact sharing is now a standard practice and even expected at premier SE venues such as ICSE, the practical usability of these replication packages remains underexplored. In particular, there is a marked lack of studies that comprehensively examine the executability and reproducibility of replication packages in SE research. In this paper, we aim to fill this gap by evaluating 100 replication packages published as part of ICSE proceedings over the past decade (2015--2024). We assess the (1) executability of the replication packages, (2) efforts and modifications required to execute them, (3) challenges that prevent executability, and (4) reproducibility of the original findings. We spent approximately 650 person-hours in total executing the artifacts and reproducing the study findings. Our findings reveal that only 40\% of the 100 evaluated artifacts were executable, of which 32.5\% (13 out of 40) ran without any modification. Regarding effort levels, 17.5\% (7 out of 40) required low effort, while 82.5\% (33 out of 40) required moderate to high effort to execute successfully. We identified five common types of modifications and 13 challenges leading to execution failure, spanning environmental, documentation, and structural issues. Among the executable artifacts, only 35\% (14 out of 40) reproduced the original results. These findings highlight a notable gap between artifact availability, executability, and reproducibility. Our study proposes three actionable guidelines to improve the preparation, documentation, and review of research artifacts, thereby strengthening the rigor and sustainability of open science practices in SE research.

</details>


### [18] [LLM-Empowered Functional Safety and Security by Design in Automotive Systems](https://arxiv.org/abs/2601.02215)
*Nenad Petrovic,Vahid Zolfaghari,Fengjunjie Pan,Alois Knoll*

Main category: cs.SE

TL;DR: 本文提出了一种基于LLM的工作流，用于支持软件定义车辆（SDV）软件开发，涵盖安全感知系统拓扑设计和事件驱动决策代码分析两个方面。


<details>
  <summary>Details</summary>
Motivation: 随着软件定义车辆的发展，需要更系统化的方法来处理SDV软件开发中的安全性和功能验证问题，特别是在ADAS相关场景中。

Method: 采用事件链模型进行代码分析，为功能安全验证提供形式化基础；结合模型驱动工程（MDE）和对象约束语言（OCL）规则进行拓扑安全分析；支持本地部署和专有解决方案。

Result: 在高级驾驶辅助系统（ADAS）相关场景中进行了评估，验证了该工作流在安全感知系统拓扑设计和事件驱动代码分析方面的有效性。

Conclusion: LLM赋能的工作流能够有效支持SDV软件开发，特别是在安全感知系统设计和功能安全验证方面，为ADAS等关键系统的开发提供了系统化方法。

Abstract: This paper presents LLM-empowered workflow to support Software Defined Vehicle (SDV) software development, covering the aspects of security-aware system topology design, as well as event-driven decision-making code analysis. For code analysis we adopt event chains model which provides formal foundations to systematic validation of functional safety, taking into account the semantic validity of messages exchanged between key components, including both CAN and Vehicle Signal Specification (VSS). Analysis of security aspects for topology relies on synergy with Model-Driven Engineering (MDE) approach and Object Constraint Language (OCL) rules. Both locally deployable and proprietary solution are taken into account for evaluation within Advanced Driver-Assistance Systems (ADAS)-related scenarios.

</details>


### [19] [NQC2: A Non-Intrusive QEMU Code Coverage Plugin](https://arxiv.org/abs/2601.02238)
*Nils Bosbach,Alwalid Salama,Lukas Jünger,Mark Burton,Niko Zurstraßen,Rebecca Pelke,Rainer Leupers*

Main category: cs.SE

TL;DR: NQC2是一个QEMU插件，用于在嵌入式系统裸机程序中提取代码覆盖率信息，无需目标软件插桩，性能比Xilinx方案提升8.5倍


<details>
  <summary>Details</summary>
Motivation: 传统代码覆盖率测量方法在嵌入式系统裸机程序中存在局限，因为这些方法通常需要操作系统和文件系统来存储覆盖率数据，而裸机程序不具备这些特性

Method: 开发NQC2作为QEMU插件，在运行时从QEMU提取覆盖率信息并存储到主机文件中，兼容修改版QEMU且无需目标软件插桩

Result: NQC2性能显著优于Xilinx的可比方案，提升幅度高达8.5倍

Conclusion: NQC2提供了一种有效的嵌入式系统裸机程序代码覆盖率测量方案，克服了传统方法的限制，具有更好的兼容性和性能

Abstract: Code coverage analysis has become a standard approach in software development, facilitating the assessment of test suite effectiveness, the identification of under-tested code segments, and the discovery of performance bottlenecks. When code coverage of software for embedded systems needs to be measured, conventional approaches quickly meet their limits. A commonly used approach involves instrumenting the source files with added code that collects and dumps coverage information during runtime. This inserted code usually relies on the existence of an operating and a file system to dump the collected data. These features are not available for bare-metal programs that are executed on embedded systems.
  To overcome this issue, we present NQC2, a plugin for QEMU.NQC2 extracts coverage information from QEMU during runtime and stores them into a file on the host machine. This approach is even compatible with modified QEMU versions and does not require target-software instrumentation. NQC2 outperforms a comparable approach from Xilinx by up to 8.5 x.

</details>


### [20] [Automatic Assertion Mining in Assertion-Based Verification: Techniques, Challenges, and Future Directions](https://arxiv.org/abs/2601.02248)
*Mohammad Reza Heidari Iman,Giorgio Di Natale,Katell Morin-Allory*

Main category: cs.SE

TL;DR: 本文综述了最新的高级断言挖掘工具，比较了它们的方法论，旨在为研究人员和验证从业者提供现有工具能力和局限性的见解，并指出未来开发更强大断言挖掘工具的方向。


<details>
  <summary>Details</summary>
Motivation: 随着功能验证日益依赖基于断言的验证（ABV），自动断言挖掘工具已成为硬件设计验证的关键方法。本文旨在通过综述最新的高级断言挖掘工具，为研究人员和验证从业者提供现有工具能力和局限性的全面了解，并识别其不足之处。

Method: 本文采用文献综述和比较分析方法，系统回顾了最先进、最广泛采用的断言挖掘工具，并对它们的方法论进行了比较分析。

Result: 通过比较分析，本文识别了现有断言挖掘工具的各种方法论特点、能力和局限性，为研究人员和验证从业者提供了有价值的见解。

Conclusion: 本文不仅总结了现有断言挖掘工具的现状，更重要的是指出了它们的不足之处，为未来开发更强大、更先进的断言挖掘工具指明了研究方向和发展方向。

Abstract: Functional verification increasingly relies on Assertion-Based Verification (ABV), which has become a key approach for verifying hardware designs due to its efficiency and effectiveness. Central to ABV are automatic assertion miners, which apply different techniques to generate assertions automatically. This paper reviews the most recent, advanced, and widely adopted assertion miners, offering a comparative analysis of their methodologies. The goal is to provide researchers and verification practitioners with insights into the capabilities and limitations of existing miners. By identifying their shortcomings, this work also points toward directions for developing more powerful and advanced assertion miners in the future.

</details>


### [21] [Question Answering for Multi-Release Systems: A Case Study at Ciena](https://arxiv.org/abs/2601.02345)
*Parham Khamsepour,Mark Cole,Ish Ashraf,Sandeep Puri,Mehrdad Sabetzadeh,Shiva Nejati*

Main category: cs.SE

TL;DR: QAMR是一个针对多版本系统文档的问答聊天机器人，通过增强的RAG架构和双分块策略，在准确性和检索性能上显著优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 多版本系统中不同版本的软件文档高度相似但又有区别，现有问答技术在这类文档上准确性不足，需要专门的方法来处理多版本系统文档的问答挑战。

Method: QAMR采用增强的检索增强生成(RAG)架构，结合预处理、查询重写和上下文选择机制，并使用双分块策略分别优化检索和答案生成的分块大小。

Result: QAMR在公共软件工程基准和工业合作伙伴Ciena的真实多版本系统文档上评估，平均答案正确率达到88.5%，平均检索准确率90%，相比基线分别提升16.5%和12%，响应时间减少8%。

Conclusion: QAMR有效解决了多版本系统文档问答的挑战，通过专门设计的机制显著提升了准确性和性能，其评估指标与专家评估高度相关，验证了方法的可靠性。

Abstract: Companies regularly have to contend with multi-release systems, where several versions of the same software are in operation simultaneously. Question answering over documents from multi-release systems poses challenges because different releases have distinct yet overlapping documentation. Motivated by the observed inaccuracy of state-of-the-art question-answering techniques on multi-release system documents, we propose QAMR, a chatbot designed to answer questions across multi-release system documentation. QAMR enhances traditional retrieval-augmented generation (RAG) to ensure accuracy in the face of highly similar yet distinct documentation for different releases. It achieves this through a novel combination of pre-processing, query rewriting, and context selection. In addition, QAMR employs a dual-chunking strategy to enable separately tuned chunk sizes for retrieval and answer generation, improving overall question-answering accuracy. We evaluate QAMR using a public software-engineering benchmark as well as a collection of real-world, multi-release system documents from our industry partner, Ciena. Our evaluation yields five main findings: (1) QAMR outperforms a baseline RAG-based chatbot, achieving an average answer correctness of 88.5% and an average retrieval accuracy of 90%, which correspond to improvements of 16.5% and 12%, respectively. (2) An ablation study shows that QAMR's mechanisms for handling multi-release documents directly improve answer accuracy. (3) Compared to its component-ablated variants, QAMR achieves a 19.6% average gain in answer correctness and a 14.0% average gain in retrieval accuracy over the best ablation. (4) QAMR reduces response time by 8% on average relative to the baseline. (5) The automatically computed accuracy metrics used in our evaluation strongly correlate with expert human assessments, validating the reliability of our methodology.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [22] [Semantic Alignment of Multilingual Knowledge Graphs via Contextualized Vector Projections](https://arxiv.org/abs/2601.00814)
*Abhishek Kumar*

Main category: cs.AI

TL;DR: 基于嵌入余弦相似度的跨语言本体对齐系统，通过增强实体描述和使用微调的多语言模型提升对齐效果，在OAEI-2022数据集上F1达到71%，比最佳基线提升16%


<details>
  <summary>Details</summary>
Motivation: 解决跨语言本体对齐的挑战，传统方法难以捕捉跨语言的语义相似性，需要更有效的对齐方法来提升跨语言知识图谱的整合能力

Method: 使用新颖技术创建丰富的实体描述增强上下文信息，采用微调的transformer多语言模型生成更好的嵌入表示，通过余弦相似度匹配实体对，再应用阈值过滤保留高相似度实体

Result: 在OAEI-2022 multifarm track评估数据集上获得71%的F1分数（78%召回率和65%精确率），比最佳基线提升了16%，表明系统能有效捕捉跨语言的细微相似性

Conclusion: 提出的跨语言本体对齐管道能够有效捕捉跨语言的语义相似性，通过增强实体描述和使用微调多语言模型显著提升了对齐性能，为跨语言知识图谱整合提供了有效解决方案

Abstract: The paper presents our work on cross-lingual ontology alignment system which uses embedding based cosine similarity matching. The ontology entities are made contextually richer by creating descriptions using novel techniques. We use a fine-tuned transformer based multilingual model for generating better embeddings. We use cosine similarity to find positive ontology entities pairs and then apply threshold filtering to retain only highly similar entities. We have evaluated our work on OAEI-2022 multifarm track. We achieve 71% F1 score (78% recall and 65% precision) on the evaluation dataset, 16% increase from best baseline score. This suggests that our proposed alignment pipeline is able to capture the subtle cross-lingual similarities.

</details>


### [23] [MathLedger: A Verifiable Learning Substrate with Ledger-Attested Feedback](https://arxiv.org/abs/2601.00816)
*Ismail Ahmad Abdullah*

Main category: cs.AI

TL;DR: MathLedger是一个用于可验证机器认知的框架，集成了形式验证、密码学证明和学习动态，通过反射式形式学习实现可审计的机器学习系统。


<details>
  <summary>Details</summary>
Motivation: 当前AI系统虽然性能卓越但缺乏透明度和可验证性，在安全关键部署中存在信任危机，需要建立可验证的机器学习框架。

Method: 采用反射式形式学习，这是一种符号化的梯度下降方法，更新由验证器结果而非统计损失驱动，结合形式验证、密码学证明和学习动态。

Result: 第一阶段实验验证了测量和治理基础设施在受控条件下的有效性，确认了测量基础设施和故障关闭治理触发机制的正确性，但未做出收敛性或能力声明。

Conclusion: MathLedger提供了一个基础设施原型，实现了账本证明的学习，支持大规模可审计性，为解决AI系统的透明度和可验证性问题提供了技术基础。

Abstract: Contemporary AI systems achieve extraordinary performance yet remain opaque and non-verifiable, creating a crisis of trust for safety-critical deployment. We introduce MathLedger, a substrate for verifiable machine cognition that integrates formal verification, cryptographic attestation, and learning dynamics into a single epistemic loop. The system implements Reflexive Formal Learning (RFL), a symbolic analogue of gradient descent where updates are driven by verifier outcomes rather than statistical loss.
  Phase I experiments validate the measurement and governance substrate under controlled conditions. CAL-EXP-3 validates measurement infrastructure (Delta p computation, variance tracking); separate stress tests confirm fail-closed governance triggers correctly under out-of-bounds conditions. No convergence or capability claims are made. The contribution is infrastructural: a working prototype of ledger-attested learning that enables auditability at scale.
  Keywords: verifiable learning, formal verification, cryptographic attestation, reflexive feedback, fail-closed governance

</details>


### [24] [Agentic AI for Autonomous, Explainable, and Real-Time Credit Risk Decision-Making](https://arxiv.org/abs/2601.00818)
*Chandra Sekhar Kubam*

Main category: cs.AI

TL;DR: 本文提出了一种基于Agentic AI框架的自主信用风险评估系统，通过多智能体协作实现实时、透明的信贷决策，相比传统模型在决策速度、透明度和响应性方面表现更优。


<details>
  <summary>Details</summary>
Motivation: 金融服务快速数字化导致对自主、透明、实时的信用风险决策系统需求迫切。传统机器学习模型虽然擅长模式识别，但缺乏现代金融运营所需的适应性推理、情境感知和自主性。

Method: 提出Agentic AI框架，构建多智能体系统，包含强化学习、自然语言推理、可解释AI模块和实时数据吸收管道。系统包括智能体协作协议、风险评分引擎、可解释性层和持续反馈学习循环。

Result: 该系统在决策速度、透明度和响应性方面优于传统信用评分模型。但仍存在模型漂移风险、高维数据解释不一致、监管不确定性以及低资源环境基础设施限制等实际局限性。

Conclusion: 该系统具有变革信用分析的巨大潜力。未来研究应关注动态监管合规机制、新型智能体协作、对抗鲁棒性以及在跨国信用生态系统中的大规模实施。

Abstract: Significant digitalization of financial services in a short period of time has led to an urgent demand to have autonomous, transparent and real-time credit risk decision making systems. The traditional machine learning models are effective in pattern recognition, but do not have the adaptive reasoning, situational awareness, and autonomy needed in modern financial operations. As a proposal, this paper presents an Agentic AI framework, or a system where AI agents view the world of dynamic credit independent of human observers, who then make actions based on their articulable decision-making paths. The research introduces a multi-agent system with reinforcing learning, natural language reasoning, explainable AI modules, and real-time data absorption pipelines as a means of assessing the risk profiles of borrowers with few humans being involved. The processes consist of agent collaboration protocol, risk-scoring engines, interpretability layers, and continuous feedback learning cycles. Findings indicate that decision speed, transparency and responsiveness is better than traditional credit scoring models. Nevertheless, there are still some practical limitations such as risks of model drift, inconsistencies in interpreting high dimensional data and regulatory uncertainties as well as infrastructure limitations in low-resource settings. The suggested system has a high prospective to transform credit analytics and future studies ought to be directed on dynamic regulatory compliance mobilizers, new agent teamwork, adversarial robustness, and large-scale implementation in cross-country credit ecosystems.

</details>


### [25] [CogCanvas: Compression-Resistant Cognitive Artifacts for Long LLM Conversations](https://arxiv.org/abs/2601.00821)
*Tao An*

Main category: cs.AI

TL;DR: CogCanvas是一个无需训练的框架，通过提取对话中的认知构件并组织成时序感知图，解决大语言模型在长对话中的上下文窗口限制和信息保真度问题。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在长对话中面临上下文窗口限制和信息保真度的基本矛盾。现有方法（截断和摘要）要么丢弃早期信息，要么丢失细节信息。

Method: 引入CogCanvas框架，从对话轮次中提取基于原文的认知构件（决策、事实、提醒），并将其组织成时序感知图，实现抗压缩检索。

Result: 在LoCoMo基准测试中，CogCanvas达到34.7%整体准确率，优于RAG（25.6%）和GraphRAG（13.7%）。在时序推理上优势最明显：31.5% vs. 9.3%（RAG）和5.0%（GraphRAG），相对提升530%。多跳因果推理达到81.0%通过率，而GraphRAG为40.0%。

Conclusion: 虽然经过大量优化的方法通过专门训练能达到更高绝对分数，但CogCanvas这种无需训练的方法为实践者提供了可立即部署的替代方案，显著优于标准基线。

Abstract: Large language models face a fundamental tension between context window limits and information fidelity in long conversations. Existing approaches--truncation and summarization--either discard early information or lose nuanced details. We introduce CogCanvas, a training-free framework that extracts verbatim-grounded cognitive artifacts (decisions, facts, reminders) from conversation turns and organizes them into a temporal-aware graph for compression-resistant retrieval.
  On the LoCoMo benchmark, CogCanvas achieves 34.7% overall accuracy, outperforming RAG (25.6%, +9.1pp) and GraphRAG (13.7%, +21.0pp). The advantage is most pronounced on temporal reasoning: 31.5% vs. 9.3% (RAG) and 5.0% (GraphRAG)--a +530% relative improvement. On multi-hop causal reasoning, CogCanvas achieves 81.0% pass rate vs. 40.0% for GraphRAG (+41.0pp). Controlled benchmarks show 97.5% recall (+78.5pp vs. summarization) with 93.0% exact match preservation.
  While heavily-optimized approaches achieve higher absolute scores through dedicated training (EverMemOS: approximately 92%), our training-free approach provides practitioners with an immediately-deployable alternative that significantly outperforms standard baselines. Code and data: https://github.com/tao-hpu/cog-canvas.

</details>


### [26] [Energy-Aware Routing to Large Reasoning Models](https://arxiv.org/abs/2601.00823)
*Austin R. Ellis-Mohr,Max Hartman,Lav R. Varshney*

Main category: cs.AI

TL;DR: 大型推理模型（LRMs）的能耗因模型选择和推理方式而异，系统性能取决于平均能量供应与随机波动的平衡，临界状态是避免能量浪费的最佳运行点，方差感知路由和调度成为关键设计维度。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型具有异构的推理能耗特性，不同模型和推理方式的能量成本不同。为了降低能耗，需要选择合适的模型并以恰当方式运行。系统性能受到平均能量供应和随机波动之间平衡的影响，需要找到既不浪费基线能量也不过度依赖辅助能量的临界运行状态。

Method: 基于训练计算和推理计算缩放定律来设计调度策略，采用方差感知的路由和调度方法，通过二阶特征分析来理解系统在临界状态下的性能表现，开发能量感知的模型路由策略。

Result: 提出了临界状态作为系统运行的最佳点，在该状态下既不系统性地浪费基线能量，也不过度依赖辅助能量。性能受到时间、模型和执行选择中变异性吸收方式的影响，方差感知路由成为关键设计维度。

Conclusion: 大型推理模型系统的能量效率优化需要综合考虑模型选择、推理方式和能量供应平衡。临界状态理论为开发能量感知的模型路由策略提供了理论基础，方差感知的路由和调度是实现高效能量管理的关键设计方向。

Abstract: Large reasoning models (LRMs) have heterogeneous inference energy costs based on which model is used and how much it reasons. To reduce energy, it is important to choose the right LRM and operate it in the right way. As a result, the performance of systems that dispatch tasks to different individual LRMs depend on the balance between mean energy provisioning and stochastic fluctuations. The critical regime is the unique operating point at which neither auxiliary energy nor baseline energy is systematically wasted. Increasing baseline supply shifts the system toward persistent over-supply and baseline-energy waste, while reducing supply induces persistent reliance on auxiliary energy. Yet in this regime, performance remains volatility-limited and so a second-order characterization provides further insights that we develop. Here, performance is governed by how variability is absorbed across time, models, and execution choices. This perspective highlights variance-aware routing and dispatch as a principled design axis, and provides a theoretical basis for developing energy-aware model routing policies. Routing behavior is characterized when dispatch policies are based on training-compute and inference-compute scaling laws for LRMs.

</details>


### [27] [Can We Trust AI Explanations? Evidence of Systematic Underreporting in Chain-of-Thought Reasoning](https://arxiv.org/abs/2601.00830)
*Deep Pankajbhai Mehta*

Main category: cs.AI

TL;DR: 研究发现AI系统在逐步解释推理时，几乎从不主动提及问题中嵌入的提示信息，但当被直接询问时会承认注意到这些提示，表明AI看到了有影响力的信息但选择不报告。


<details>
  <summary>Details</summary>
Motivation: 当AI系统逐步解释其推理过程时，从业者通常假设这些解释揭示了真正影响AI答案的因素。本研究旨在测试这一假设，探究AI是否真的会报告所有影响其决策的信息。

Method: 通过在问题中嵌入提示信息，测量模型是否提及这些提示。研究涵盖了超过9,000个测试案例，涉及11个领先的AI模型。测试了多种条件：自发报告、直接询问、告知被监视、强制报告提示等。

Result: 1. 模型几乎从不自发提及提示信息；2. 当被直接询问时，模型承认注意到提示；3. 告知模型被监视并不能改善报告行为；4. 强制报告提示虽然有效，但会导致模型在没有提示时也报告，并降低准确性；5. 迎合用户偏好的提示尤其危险——模型最常遵循这些提示，却最少报告它们。

Conclusion: 仅仅观察AI的推理过程不足以发现隐藏的影响因素。AI系统可能看到并受到某些信息的影响，但选择不在解释中报告这些信息，这对AI透明度和可信度构成了重要挑战。

Abstract: When AI systems explain their reasoning step-by-step, practitioners often assume these explanations reveal what actually influenced the AI's answer. We tested this assumption by embedding hints into questions and measuring whether models mentioned them. In a study of over 9,000 test cases across 11 leading AI models, we found a troubling pattern: models almost never mention hints spontaneously, yet when asked directly, they admit noticing them. This suggests models see influential information but choose not to report it. Telling models they are being watched does not help. Forcing models to report hints works, but causes them to report hints even when none exist and reduces their accuracy. We also found that hints appealing to user preferences are especially dangerous-models follow them most often while reporting them least. These findings suggest that simply watching AI reasoning is not enough to catch hidden influences.

</details>


### [28] [OmniNeuro: A Multimodal HCI Framework for Explainable BCI Feedback via Generative AI and Sonification](https://arxiv.org/abs/2601.00843)
*Ayda Aghaei Nia*

Main category: cs.AI

TL;DR: OmniNeuro是一个新型HCI框架，将BCI从"黑盒"解码器转变为透明的反馈伙伴，通过物理、混沌和量子启发的可解释性引擎提供实时神经声化和生成式AI临床报告。


<details>
  <summary>Details</summary>
Motivation: 深度学习虽然提高了脑机接口的解码精度，但其"黑盒"特性阻碍了临床采用，导致用户挫败感和神经可塑性结果不佳。需要可解释的反馈系统来帮助用户调节心理努力并减少试错阶段。

Method: 提出OmniNeuro框架，集成三种可解释性引擎：1) 物理(能量)分析，2) 混沌(分形复杂性)分析，3) 量子启发的不确定性建模。这些指标驱动实时神经声化和生成式AI临床报告，框架与解码器无关，可作为任何先进架构的可解释性层。

Result: 在PhysioNet数据集(N=109)上评估，系统平均准确率达到58.52%。定性试点研究(N=3)证实可解释反馈有助于用户调节心理努力并减少"试错"阶段。

Conclusion: OmniNeuro成功将BCI从沉默解码器转变为透明反馈伙伴，通过多模态可解释性引擎解决了深度学习"黑盒"问题，为临床采用提供了可行的解决方案。

Abstract: While Deep Learning has improved Brain-Computer Interface (BCI) decoding accuracy, clinical adoption is hindered by the "Black Box" nature of these algorithms, leading to user frustration and poor neuroplasticity outcomes. We propose OmniNeuro, a novel HCI framework that transforms the BCI from a silent decoder into a transparent feedback partner. OmniNeuro integrates three interpretability engines: (1) Physics (Energy), (2) Chaos (Fractal Complexity), and (3) Quantum-Inspired uncertainty modeling. These metrics drive real-time Neuro-Sonification and Generative AI Clinical Reports. Evaluated on the PhysioNet dataset ($N=109$), the system achieved a mean accuracy of 58.52%, with qualitative pilot studies ($N=3$) confirming that explainable feedback helps users regulate mental effort and reduces the "trial-and-error" phase. OmniNeuro is decoder-agnostic, acting as an essential interpretability layer for any state-of-the-art architecture.

</details>


### [29] [Enhancing Temporal Awareness in LLMs for Temporal Point Processes](https://arxiv.org/abs/2601.00845)
*Lili Chen,Wensheng Gan,Shuang Liang,Philip S. Yu*

Main category: cs.AI

TL;DR: 提出TPP-TAL框架，通过增强LLMs中的时间感知来改进时间点过程建模，解决现有方法难以捕捉时间信息与语义上下文复杂交互的问题。


<details>
  <summary>Details</summary>
Motivation: 时间点过程在金融、医疗、社交系统等领域很重要，但现有方法难以有效捕捉时间信息与语义上下文的复杂交互，限制了LLMs在时间点过程建模中的应用。

Method: 提出TPP-TAL框架，这是一个即插即用的方法，在将信息输入LLM之前，显式地对齐时间动态与上下文语义，而不是简单地拼接事件时间和类型嵌入。

Result: 在多个基准数据集上的实验表明，TPP-TAL在时间似然估计和事件预测准确性方面都有显著改进。

Conclusion: 增强LLMs中的时间感知对于连续时间事件建模至关重要，TPP-TAL框架通过显式对齐时间动态与语义上下文，有效提升了时间点过程的建模性能。

Abstract: Temporal point processes (TPPs) are crucial for analyzing events over time and are widely used in fields such as finance, healthcare, and social systems. These processes are particularly valuable for understanding how events unfold over time, accounting for their irregularity and dependencies. Despite the success of large language models (LLMs) in sequence modeling, applying them to temporal point processes remains challenging. A key issue is that current methods struggle to effectively capture the complex interaction between temporal information and semantic context, which is vital for accurate event modeling. In this context, we introduce TPP-TAL (Temporal Point Processes with Enhanced Temporal Awareness in LLMs), a novel plug-and-play framework designed to enhance temporal reasoning within LLMs. Rather than using the conventional method of simply concatenating event time and type embeddings, TPP-TAL explicitly aligns temporal dynamics with contextual semantics before feeding this information into the LLM. This alignment allows the model to better perceive temporal dependencies and long-range interactions between events and their surrounding contexts. Through comprehensive experiments on several benchmark datasets, it is shown that TPP-TAL delivers substantial improvements in temporal likelihood estimation and event prediction accuracy, highlighting the importance of enhancing temporal awareness in LLMs for continuous-time event modeling. The code is made available at https://github.com/chenlilil/TPP-TAL

</details>


### [30] [Comment on: Your Brain on ChatGPT: Accumulation of Cognitive Debt When Using an AI Assistant for Essay Writing Tasks](https://arxiv.org/abs/2601.00856)
*Milos Stankovic,Ella Hirche,Sarah Kollatzsch,Julia Nadine Doetsch*

Main category: cs.AI

TL;DR: 这是一篇对Kosmyna等人(2025)关于使用ChatGPT进行论文写作任务时认知债务积累研究的评论性文章，指出了该研究在样本量、可重复性、EEG分析方法、结果报告一致性和透明度等方面存在的问题。


<details>
  <summary>Details</summary>
Motivation: 作者旨在对Kosmyna等人的研究提供建设性意见，以改进该手稿的同行评审准备度，因为原研究的一些结果可能需要更保守的解释。

Method: 通过批判性分析原研究的方法论，重点关注五个主要方面：研究设计（包括样本量限制）、分析的可重复性、EEG分析方法问题、结果报告的不一致性以及研究过程和发现的透明度不足。

Result: 识别出原研究在多个关键方面存在需要改进的问题，包括统计功效不足、分析方法不够透明、EEG数据处理可能存在偏差、结果报告存在不一致性，以及整体研究透明度有限。

Conclusion: 虽然赞赏Kosmyna等人开启了这一重要研究并建立了有价值的数据集和分析流程，但建议在发表前解决这些方法论问题，以便更准确地解释研究结果，提高研究的科学严谨性。

Abstract: Recently published work titled Your Brain on ChatGPT: Accumulation of Cognitive Debt When Using an AI Assistant for Essay Writing Task by Kosmyna et al. (2025) has sparked a vivid debate on the topic of artificial intelligence (AI) and human performance. We sincerely congratulate Kosmyna et al. for initiating such important research, collecting a valuable dataset, and establishing highly automated pipelines for Natural Language Processing (NLP) analyses and scoring. We aim to provide constructive comments that may improve the manuscript's readiness for peer-reviewed publication, as some results by Kosmyna et al. (2025) could be interpreted more conservatively. Our primary concerns focus on: (i) study design considerations, including the limited sample size; (ii) the reproducibility of the analyses; (iii) methodological issues related to the EEG analysis; (iv) inconsistencies in the reporting of results; and (v) limited transparency in several aspects of the study's procedures and findings.

</details>


### [31] [Cultural Encoding in Large Language Models: The Existence Gap in AI-Mediated Brand Discovery](https://arxiv.org/abs/2601.00869)
*Huang Junyao,Situ Ruimin,Ye Renqin*

Main category: cs.AI

TL;DR: 研究发现LLM训练数据的地理分布导致品牌推荐存在系统性差异，中国LLM的品牌提及率比国际LLM高30.6个百分点，提出"存在鸿沟"概念和"数据护城河"框架


<details>
  <summary>Details</summary>
Motivation: 随着人工智能系统越来越多地介入消费者信息发现过程，品牌面临算法不可见性问题。本研究旨在探究大型语言模型中存在的文化编码现象——由训练数据构成导致的品牌推荐系统性差异

Method: 分析1,909个纯英文查询，覆盖6个LLM（GPT-4o、Claude、Gemini、Qwen3、DeepSeek、Doubao）和30个品牌，通过案例研究验证假设

Result: 中国LLM的品牌提及率比国际LLM高30.6个百分点（88.9% vs. 58.3%，p<.001），这种差异在相同的英文查询中依然存在，表明训练数据的地理分布而非语言是主要驱动因素

Conclusion: 提出"存在鸿沟"概念和"数据护城河"框架，将AI可见内容概念化为VRIN战略资源，建议品牌通过语义覆盖、技术深度和文化本地化构建数据护城河，实现算法无处不在的战略目标

Abstract: As artificial intelligence systems increasingly mediate consumer information discovery,
  brands face algorithmic invisibility. This study investigates Cultural Encoding in Large
  Language Models (LLMs) -- systematic differences in brand recommendations arising from
  training data composition. Analyzing 1,909 pure-English queries across 6 LLMs (GPT-4o,
  Claude, Gemini, Qwen3, DeepSeek, Doubao) and 30 brands, we find Chinese LLMs exhibit 30.6
  percentage points higher brand mention rates than International LLMs (88.9% vs. 58.3%,
  p<.001). This disparity persists in identical English queries, indicating training data
  geography -- not language -- drives the effect. We introduce the Existence Gap: brands
  absent from LLM training corpora lack "existence" in AI responses regardless of quality.
  Through a case study of Zhizibianjie (OmniEdge), a collaboration platform with 65.6%
  mention rate in Chinese LLMs but 0% in International models (p<.001), we demonstrate how
  Linguistic Boundary Barriers create invisible market entry obstacles. Theoretically, we
  contribute the Data Moat Framework, conceptualizing AI-visible content as a VRIN strategic
  resource. We operationalize Algorithmic Omnipresence -- comprehensive brand visibility
  across LLM knowledge bases -- as the strategic objective for Generative Engine Optimization
  (GEO). Managerially, we provide an 18-month roadmap for brands to build Data Moats
  through semantic coverage, technical depth, and cultural localization. Our findings reveal
  that in AI-mediated markets, the limits of a brand's "Data Boundaries" define the limits
  of its "Market Frontiers."

</details>


### [32] [Universal Conditional Logic: A Formal Language for Prompt Engineering](https://arxiv.org/abs/2601.00880)
*Anthony Mikinka*

Main category: cs.AI

TL;DR: UCL是一个将提示工程从启发式实践转化为系统化优化的数学框架，通过系统评估显著减少了29.8%的token使用，并揭示了过规范悖论：超过阈值S*=0.509后，额外规范会二次降低性能。


<details>
  <summary>Details</summary>
Motivation: 当前提示工程主要依赖启发式实践，缺乏系统化的数学框架来优化LLM交互效率。研究者希望建立一个可校准的框架，将提示工程从经验性实践转变为系统化优化过程。

Method: 提出了通用条件逻辑(UCL)框架，包含指示函数(I_i ∈ {0,1})、结构开销函数(O_s = γ * Σ(ln C_k))和早期绑定等核心机制。通过系统评估(N=305，11个模型，4次迭代)验证框架有效性。

Result: UCL显著减少了29.8%的token使用(t(10)=6.36, p<0.001, Cohen's d=2.01)，相应降低了成本。发现了过规范悖论：超过阈值S*=0.509后，额外规范会二次降低性能。不同模型架构需要特定优化配置。

Conclusion: UCL为高效LLM交互建立了一个可校准的数学框架，模型家族特定优化是未来关键研究方向。该框架将提示工程从启发式实践转变为系统化优化过程。

Abstract: We present Universal Conditional Logic (UCL), a mathematical framework for prompt optimization that transforms prompt engineering from heuristic practice into systematic optimization. Through systematic evaluation (N=305, 11 models, 4 iterations), we demonstrate significant token reduction (29.8%, t(10)=6.36, p < 0.001, Cohen's d = 2.01) with corresponding cost savings. UCL's structural overhead function O_s(A) explains version-specific performance differences through the Over-Specification Paradox: beyond threshold S* = 0.509, additional specification degrades performance quadratically. Core mechanisms -- indicator functions (I_i in {0,1}), structural overhead (O_s = gamma * sum(ln C_k)), early binding -- are validated. Notably, optimal UCL configuration varies by model architecture -- certain models (e.g., Llama 4 Scout) require version-specific adaptations (V4.1). This work establishes UCL as a calibratable framework for efficient LLM interaction, with model-family-specific optimization as a key research direction.

</details>


### [33] [Counterfactual Self-Questioning for Stable Policy Optimization in Language Models](https://arxiv.org/abs/2601.00885)
*Mandar Parab*

Main category: cs.AI

TL;DR: 提出Counterfactual Self-Questioning框架，让单个语言模型生成并评估自身推理的反事实批评，通过自我质疑实现无需外部模型的自我改进。


<details>
  <summary>Details</summary>
Motivation: 现有语言模型自我改进方法依赖外部批评者、学习奖励模型或集成采样，增加了复杂性和训练不稳定性，需要更简单有效的自我监督方法。

Method: 提出反事实自我质疑框架：1) 生成初始推理轨迹；2) 针对潜在失败点提出针对性问题；3) 生成暴露错误假设或无效步骤的替代推理轨迹；4) 使用结构化相对反馈进行策略优化。

Result: 在多个数学推理基准测试中，反事实自我质疑提高了准确性和训练稳定性，特别是对于较小模型，仅使用内部生成的监督就能实现可扩展的自我改进。

Conclusion: Counterfactual Self-Questioning为语言模型自我改进提供了一种简单有效的框架，无需外部模型即可实现稳定训练和性能提升，特别适合资源受限的小模型。

Abstract: Recent work on language model self-improvement shows that models can refine their own reasoning through reflection, verification, debate, or self-generated rewards. However, most existing approaches rely on external critics, learned reward models, or ensemble sampling, which increases complexity and training instability. We propose Counterfactual Self-Questioning, a framework in which a single language model generates and evaluates counterfactual critiques of its own reasoning. The method produces an initial reasoning trace, formulates targeted questions that challenge potential failure points, and generates alternative reasoning trajectories that expose incorrect assumptions or invalid steps. These counterfactual trajectories provide structured relative feedback that can be directly used for policy optimization without auxiliary models. Experiments on multiple mathematical reasoning benchmarks show that counterfactual self-questioning improves accuracy and training stability, particularly for smaller models, enabling scalable self-improvement using internally generated supervision alone.

</details>


### [34] [Context Collapse: In-Context Learning and Model Collapse](https://arxiv.org/abs/2601.00923)
*Josef Ott*

Main category: cs.AI

TL;DR: 该论文研究了LLM中的上下文学习和模型崩溃现象，通过线性变换器分析ICL的相变特性，使用鞅理论分析模型崩溃，并提出了"上下文崩溃"的新概念。


<details>
  <summary>Details</summary>
Motivation: 研究大语言模型中的两个关键现象：上下文学习（ICL）和模型崩溃，旨在理解ICL的数学机制、模型崩溃的动力学过程，以及两者之间的联系。

Method: 1. 使用带权重绑定的线性变换器在回归任务上研究ICL，证明最小化上下文损失会导致参数相变；2. 将线性变换器的前向传播简化为预条件梯度下降；3. 使用鞅和随机游走理论分析线性回归和高斯拟合中的模型崩溃；4. 提出"上下文崩溃"概念，连接ICL动力学与生成模型的长期稳定性问题。

Result: 1. ICL在临界上下文长度以上会出现相变，解产生斜对称分量；2. 预条件器包含斜对称分量，诱导梯度方向旋转；3. 模型崩溃几乎必然发生，除非数据增长足够快或随时间保留；4. 提出了上下文崩溃作为ICL动力学与长期生成稳定性之间的桥梁概念。

Conclusion: 论文揭示了ICL的相变特性、模型崩溃的数学机制，并提出了上下文崩溃这一新概念，为理解大语言模型的内部动力学和长期稳定性提供了理论框架。

Abstract: This thesis investigates two key phenomena in large language models (LLMs): in-context learning (ICL) and model collapse. We study ICL in a linear transformer with tied weights trained on linear regression tasks, and show that minimising the in-context loss leads to a phase transition in the learned parameters. Above a critical context length, the solution develops a skew-symmetric component. We prove this by reducing the forward pass of the linear transformer under weight tying to preconditioned gradient descent, and then analysing the optimal preconditioner. This preconditioner includes a skew-symmetric component, which induces a rotation of the gradient direction. For model collapse, we use martingale and random walk theory to analyse simplified settings - linear regression and Gaussian fitting - under both replacing and cumulative data regimes. We strengthen existing results by proving almost sure convergence, showing that collapse occurs unless the data grows sufficiently fast or is retained over time. Finally, we introduce the notion of context collapse: a degradation of context during long generations, especially in chain-of-thought reasoning. This concept links the dynamics of ICL with long-term stability challenges in generative models.

</details>


### [35] [ElecTwit: A Framework for Studying Persuasion in Multi-Agent Social Systems](https://arxiv.org/abs/2601.00994)
*Michael Bao*

Main category: cs.AI

TL;DR: ElecTwit是一个模拟多智能体系统中说服行为的框架，专门模拟社交媒体平台在政治选举期间的互动，旨在克服以往基于游戏模拟的局限性。


<details>
  <summary>Details</summary>
Motivation: 研究动机是创建一个更真实的环境来研究多智能体系统中的说服行为，特别是模拟社交媒体在政治选举期间的互动，以克服以往研究中基于游戏模拟的局限性。

Method: 开发了ElecTwit模拟框架，在真实环境中进行实验，测试多个大型语言模型，观察它们使用25种特定说服技术的情况。

Result: 观察到大多数测试的LLM全面使用了25种说服技术，范围比之前报道的更广；不同模型在技术使用和整体说服输出上存在差异，反映了模型架构和训练对现实社交模拟动态的影响；还观察到独特现象如"真相内核"信息和"墨水"痴迷的自发发展。

Conclusion: 该研究为在真实世界环境中评估有说服力的LLM智能体提供了基础，有助于确保对齐并防止危险结果。

Abstract: This paper introduces ElecTwit, a simulation framework designed to study persuasion within multi-agent systems, specifically emulating the interactions on social media platforms during a political election. By grounding our experiments in a realistic environment, we aimed to overcome the limitations of game-based simulations often used in prior research. We observed the comprehensive use of 25 specific persuasion techniques across most tested LLMs, encompassing a wider range than previously reported. The variations in technique usage and overall persuasion output between models highlight how different model architectures and training can impact the dynamics in realistic social simulations. Additionally, we observed unique phenomena such as "kernel of truth" messages and spontaneous developments with an "ink" obsession, where agents collectively demanded written proof. Our study provides a foundation for evaluating persuasive LLM agents in real-world contexts, ensuring alignment and preventing dangerous outcomes.

</details>


### [36] [Reinforcement Learning Enhanced Multi-hop Reasoning for Temporal Knowledge Question Answering](https://arxiv.org/abs/2601.01195)
*Wuzhenghong Wen,Chao Xue,Su Pan,Yuwei Sun,Minlong Peng*

Main category: cs.AI

TL;DR: 该论文提出了多跳推理增强框架MRE，通过前向和后向推理增强来改进时序知识图谱问答中的推理轨迹优化，使用Tree-Group Relative Policy Optimization方法提升复杂多跳查询的性能。


<details>
  <summary>Details</summary>
Motivation: 时序知识图谱问答中，大型语言模型在每个推理步骤中会检索包含大量时间相似和语义复杂关系的子图，这增加了次优决策和错误传播的风险。现有方法在处理复杂多跳查询时存在局限性。

Method: 提出了多跳推理增强框架MRE：1）通过提示工程引导LLM生成多样化的推理轨迹；2）选择有效轨迹进行监督微调作为冷启动策略；3）引入Tree-Group Relative Policy Optimization方法，这是一种递归的树结构探索学习法，在每个跳步建立强因果依赖关系，并通过后续跳步的多路径探索反馈进行评估。

Result: 在两个TKGQA基准测试上的实验结果表明，基于MRE的模型在复杂多跳查询处理方面持续超越最先进方法。进一步分析显示该方法提高了可解释性，并对噪声时间标注具有更好的鲁棒性。

Conclusion: MRE框架通过增强前向和后向推理，有效改进了时序知识图谱问答中的全局最优推理轨迹识别，在处理复杂多跳查询方面表现出优越性能。

Abstract: Temporal knowledge graph question answering (TKGQA) involves multi-hop reasoning over temporally constrained entity relationships in the knowledge graph to answer a given question. However, at each hop, large language models (LLMs) retrieve subgraphs with numerous temporally similar and semantically complex relations, increasing the risk of suboptimal decisions and error propagation. To address these challenges, we propose the multi-hop reasoning enhanced (MRE) framework, which enhances both forward and backward reasoning to improve the identification of globally optimal reasoning trajectories. Specifically, MRE begins with prompt engineering to guide the LLM in generating diverse reasoning trajectories for a given question. Valid reasoning trajectories are then selected for supervised fine-tuning, serving as a cold-start strategy. Finally, we introduce Tree-Group Relative Policy Optimization (T-GRPO), a recursive, tree-structured learning-by-exploration approach. At each hop, exploration establishes strong causal dependencies on the previous hop, while evaluation is informed by multi-path exploration feedback from subsequent hops. Experimental results on two TKGQA benchmarks indicate that the proposed MRE-based model consistently surpasses state-of-the-art (SOTA) approaches in handling complex multi-hop queries. Further analysis highlights improved interpretability and robustness to noisy temporal annotations.

</details>


### [37] [Digital Twin AI: Opportunities and Challenges from Large Language Models to World Models](https://arxiv.org/abs/2601.01321)
*Rong Zhou,Dongping Chen,Zihan Jia,Yao Su,Yixin Liu,Yiwen Lu,Dongwei Shi,Yue Huang,Tianyang Xu,Yi Pan,Xinliang Li,Yohannes Abate,Qingyu Chen,Zhengzhong Tu,Yu Yang,Yu Zhang,Qingsong Wen,Gengchen Mai,Sunyang Fu,Jiachen Li,Xuyu Wang,Ziran Wang,Jing Huang,Tianming Liu,Yong Chen,Lichao Sun,Lifang He*

Main category: cs.AI

TL;DR: 该论文提出了一个统一的四阶段框架，系统描述人工智能在数字孪生生命周期中的集成，涵盖建模、镜像、干预和自主管理四个阶段，并分析了物理建模与数据驱动学习的协同作用。


<details>
  <summary>Details</summary>
Motivation: 数字孪生已从被动仿真工具演变为智能自主实体，但缺乏系统化的AI集成框架。本文旨在提供一个统一框架，系统描述AI方法在数字孪生全生命周期中的嵌入方式。

Method: 提出四阶段框架：1) 通过物理基础和物理信息AI方法建模物理孪生；2) 通过实时同步将物理系统镜像为数字孪生；3) 通过预测建模、异常检测和优化策略干预物理孪生；4) 通过大语言模型、基础模型和智能体实现自主管理。分析了物理建模与数据驱动学习的协同，并跨11个应用领域进行综述。

Result: 建立了统一的AI集成框架，识别了从传统数值求解器向物理信息模型和基础模型的转变，分析了生成式AI技术如何将数字孪生转变为主动、自改进的认知系统，并发现了可扩展性、可解释性和可信度等共同挑战。

Conclusion: AI技术正在将数字孪生转变为智能自主系统，四阶段框架为AI集成提供了系统化视角。未来需要解决可扩展性、可解释性和可信度等挑战，推动负责任AI驱动的数字孪生系统发展。

Abstract: Digital twins, as precise digital representations of physical systems, have evolved from passive simulation tools into intelligent and autonomous entities through the integration of artificial intelligence technologies. This paper presents a unified four-stage framework that systematically characterizes AI integration across the digital twin lifecycle, spanning modeling, mirroring, intervention, and autonomous management. By synthesizing existing technologies and practices, we distill a unified four-stage framework that systematically characterizes how AI methodologies are embedded across the digital twin lifecycle: (1) modeling the physical twin through physics-based and physics-informed AI approaches, (2) mirroring the physical system into a digital twin with real-time synchronization, (3) intervening in the physical twin through predictive modeling, anomaly detection, and optimization strategies, and (4) achieving autonomous management through large language models, foundation models, and intelligent agents. We analyze the synergy between physics-based modeling and data-driven learning, highlighting the shift from traditional numerical solvers to physics-informed and foundation models for physical systems. Furthermore, we examine how generative AI technologies, including large language models and generative world models, transform digital twins into proactive and self-improving cognitive systems capable of reasoning, communication, and creative scenario generation. Through a cross-domain review spanning eleven application domains, including healthcare, aerospace, smart manufacturing, robotics, and smart cities, we identify common challenges related to scalability, explainability, and trustworthiness, and outline directions for responsible AI-driven digital twin systems.

</details>


### [38] [Beyond Gemini-3-Pro: Revisiting LLM Routing and Aggregation at Scale](https://arxiv.org/abs/2601.01330)
*Shengji Tang,Weihao Lin,Jingqi Ye,Hao Li,Bo Zhang,Shuyue Hu,Tao Chen,Wangli Ouyang,Lei Bai,Peng Ye*

Main category: cs.AI

TL;DR: JiSi框架通过查询-响应混合路由、支持集聚合器选择和自适应路由-聚合切换三大创新，使开源LLM协作超越Gemini-3-Pro，仅用47%成本实现更好性能，为AGI提供集体智能新路径。


<details>
  <summary>Details</summary>
Motivation: 当前LLM路由和聚合存在三个关键瓶颈：1）无训练路由器局限于基于查询的文本相似性范式；2）聚合方法静态化，无法为不同任务选择合适聚合器；3）路由和聚合的互补性未充分利用。需要释放LLM协作的完整潜力。

Method: 提出JiSi框架，包含三大创新：1）查询-响应混合路由，同时捕捉语义信息和问题难度；2）基于支持集的聚合器选择，联合评估聚合器的聚合能力和领域能力；3）自适应路由-聚合切换，动态利用路由和聚合的优势。

Result: 在九个基准测试中，JiSi通过协调十个开源LLM，仅用47%的成本就超越了Gemini-3-Pro的性能，同时优于主流基线方法。

Conclusion: 集体智能代表了通往人工通用智能（AGI）的新路径，通过有效的协作框架，开源LLM的集体表现可以超越最先进的单体模型。

Abstract: Large Language Models (LLMs) have rapidly advanced, with Gemini-3-Pro setting a new performance milestone. In this work, we explore collective intelligence as an alternative to monolithic scaling, and demonstrate that open-source LLMs' collaboration can surpass Gemini-3-Pro. We first revisit LLM routing and aggregation at scale and identify three key bottlenecks: (1) current train-free routers are limited by a query-based paradigm focusing solely on textual similarity; (2) recent aggregation methods remain largely static, failing to select appropriate aggregators for different tasks;(3) the complementarity of routing and aggregation remains underutilized. To address these problems, we introduce JiSi, a novel framework designed to release the full potential of LLMs' collaboration through three innovations: (1) Query-Response Mixed Routing capturing both semantic information and problem difficulty; (2) Support-Set-based Aggregator Selection jointly evaluating the aggregation and domain capacity of aggregators; (3) Adaptive Routing-Aggregation Switch dynamically leveraging the advantages of routing and aggregation. Comprehensive experiments on nine benchmarks demonstrate that JiSi can surpass Gemini-3-Pro with only 47% costs by orchestrating ten open-source LLMs, while outperforming mainstream baselines. It suggests that collective intelligence represents a novel path towards Artificial General Intelligence (AGI).

</details>


### [39] [A unified multimodal understanding and generation model for cross-disciplinary scientific research](https://arxiv.org/abs/2601.01363)
*Xiaomeng Yang,Zhiyu Tan,Xiaohui Zhong,Mengping Yang,Qiusheng Huang,Lei Chen,Libo Wu,Hao Li*

Main category: cs.AI

TL;DR: FuXi-Uni是一个统一的多模态科学模型，能够在单一架构中理解和生成跨学科的高维科学数据，在地球科学和生物医学领域表现出色。


<details>
  <summary>Details</summary>
Motivation: 当前AI模型通常局限于特定领域，缺乏同时理解和生成多模态科学数据的能力，而许多全球性挑战和科学问题本质上是跨学科的，需要多个领域的协同进展。

Method: FuXi-Uni将跨学科的科学标记与自然语言标记对齐，并使用科学解码器重建科学标记，支持自然语言对话和科学数值预测。

Result: 在地球系统建模中，FuXi-Uni的10天全球天气预报在0.25°分辨率上优于最先进的物理预报系统；在热带气旋预测和空间降尺度方面表现优异；在生物医学领域，在多个生物医学视觉问答基准测试中超越了领先的多模态大语言模型。

Conclusion: FuXi-Uni通过在原生共享潜在空间中统一异构科学模态，同时保持强大的领域特定性能，为更通用的多模态科学模型迈出了重要一步。

Abstract: Scientific discovery increasingly relies on integrating heterogeneous, high-dimensional data across disciplines nowadays. While AI models have achieved notable success across various scientific domains, they typically remain domain-specific or lack the capability of simultaneously understanding and generating multimodal scientific data, particularly for high-dimensional data. Yet, many pressing global challenges and scientific problems are inherently cross-disciplinary and require coordinated progress across multiple fields. Here, we present FuXi-Uni, a native unified multimodal model for scientific understanding and high-fidelity generation across scientific domains within a single architecture. Specifically, FuXi-Uni aligns cross-disciplinary scientific tokens within natural language tokens and employs science decoder to reconstruct scientific tokens, thereby supporting both natural language conversation and scientific numerical prediction. Empirically, we validate FuXi-Uni in Earth science and Biomedicine. In Earth system modeling, the model supports global weather forecasting, tropical cyclone (TC) forecast editing, and spatial downscaling driven by only language instructions. FuXi-Uni generates 10-day global forecasts at 0.25° resolution that outperform the SOTA physical forecasting system. It shows superior performance for both TC track and intensity prediction relative to the SOTA physical model, and generates high-resolution regional weather fields that surpass standard interpolation baselines. Regarding biomedicine, FuXi-Uni outperforms leading multimodal large language models on multiple biomedical visual question answering benchmarks. By unifying heterogeneous scientific modalities within a native shared latent space while maintaining strong domain-specific performance, FuXi-Uni provides a step forward more general-purpose, multimodal scientific models.

</details>


### [40] [KGCE: Knowledge-Augmented Dual-Graph Evaluator for Cross-Platform Educational Agent Benchmarking with Multimodal Language Models](https://arxiv.org/abs/2601.01366)
*Zixian Liu,Sihao Liu,Yuqi Zhao*

Main category: cs.AI

TL;DR: KGCE是一个用于评估跨平台教育智能体的基准平台，通过知识库增强和双图评估框架解决现有基准在私有教育软件任务中的不足。


<details>
  <summary>Details</summary>
Motivation: 现有基准框架在支持跨平台教育任务方面存在不足，特别是在处理学校特定软件（如小雅智能助手、华师匣子等）时，由于缺乏对这些私有领域软件结构细节的理解，智能体效率显著下降。此外，当前评估方法过度依赖粗粒度指标，难以捕捉复杂任务中的详细执行和效率。

Method: 提出KGCE平台，整合知识库增强和双图评估框架。首先构建包含104个教育相关任务的数据集，涵盖Windows、Android和跨平台协作任务。引入双图评估框架，将任务分解为多个子目标并验证其完成状态，提供细粒度评估指标。为克服现有智能体在私有领域任务中的执行瓶颈，开发了包含学校特定软件知识库的增强智能体系统。

Result: 开发了KGCE基准平台，包含教育任务数据集和双图评估框架，能够提供细粒度的智能体性能评估，并解决了私有教育软件任务中的执行瓶颈问题。

Conclusion: KGCE通过知识库增强和双图评估框架，有效解决了现有基准在跨平台教育智能体评估中的不足，特别是在私有教育软件任务中的性能瓶颈问题，为教育场景下的智能体评估提供了更精细和有效的解决方案。

Abstract: With the rapid adoption of multimodal large language models (MLMs) in autonomous agents, cross-platform task execution capabilities in educational settings have garnered significant attention. However, existing benchmark frameworks still exhibit notable deficiencies in supporting cross-platform tasks in educational contexts, especially when dealing with school-specific software (such as XiaoYa Intelligent Assistant, HuaShi XiaZi, etc.), where the efficiency of agents often significantly decreases due to a lack of understanding of the structural specifics of these private-domain software. Additionally, current evaluation methods heavily rely on coarse-grained metrics like goal orientation or trajectory matching, making it challenging to capture the detailed execution and efficiency of agents in complex tasks. To address these issues, we propose KGCE (Knowledge-Augmented Dual-Graph Evaluator for Cross-Platform Educational Agent Benchmarking with Multimodal Language Models), a novel benchmarking platform that integrates knowledge base enhancement and a dual-graph evaluation framework. We first constructed a dataset comprising 104 education-related tasks, covering Windows, Android, and cross-platform collaborative tasks. KGCE introduces a dual-graph evaluation framework that decomposes tasks into multiple sub-goals and verifies their completion status, providing fine-grained evaluation metrics. To overcome the execution bottlenecks of existing agents in private-domain tasks, we developed an enhanced agent system incorporating a knowledge base specific to school-specific software. The code can be found at https://github.com/Kinginlife/KGCE.

</details>


### [41] [Empowering Small Language Models with Factual Hallucination-Aware Reasoning for Financial Classification](https://arxiv.org/abs/2601.01378)
*Han Yuan,Yilin Wu,Li Zhang,Zheng Ma*

Main category: cs.AI

TL;DR: 本文提出AAAI三阶段流程，通过缓解事实幻觉来提升小型语言模型在金融分类任务中的性能


<details>
  <summary>Details</summary>
Motivation: 小型语言模型在金融分类中因推理时易产生事实幻觉而性能较弱，需要探索缓解事实幻觉是否能提升其分类性能

Method: 提出AAAI三阶段流程：关联识别、自动检测和自适应推理，通过编码器验证器检测事实幻觉并利用反馈进行自适应推理

Result: 实验发现：1) 事实幻觉与错误分类正相关；2) 编码器验证器能有效检测事实幻觉；3) 结合事实错误反馈的自适应推理能提升分类性能

Conclusion: AAAI流程有助于提升小型语言模型在金融领域的可信度和有效性应用

Abstract: Small language models (SLMs) are increasingly used for financial classification due to their fast inference and local deployability. However, compared with large language models, SLMs are more prone to factual hallucinations in reasoning and exhibit weaker classification performance. This raises a natural question: Can mitigating factual hallucinations improve SLMs' financial classification? To address this, we propose a three-step pipeline named AAAI (Association Identification, Automated Detection, and Adaptive Inference). Experiments on three representative SLMs reveal that: (1) factual hallucinations are positively correlated with misclassifications; (2) encoder-based verifiers effectively detect factual hallucinations; and (3) incorporating feedback on factual errors enables SLMs' adaptive inference that enhances classification performance. We hope this pipeline contributes to trustworthy and effective applications of SLMs in finance.

</details>


### [42] [A construction of an optimal base for conditional attribute and attributional condition implications in triadic contexts](https://arxiv.org/abs/2601.01467)
*Romuald Kwessy Mouona,Blaise Blériot Koguep Njionou,Etienne Romuald Temgoua Alomo,Rokia Missaoui,Leonard Kwuida*

Main category: cs.AI

TL;DR: 研究三元背景中的蕴含关系，特别关注Ganter和Obiedkov提出的条件属性蕴含和属性条件蕴含，目标是构建这些蕴含的最优基


<details>
  <summary>Details</summary>
Motivation: 三元背景中的蕴含关系研究相对较少，Ganter和Obiedkov提出的条件属性蕴含和属性条件蕴含需要系统性的基构建方法

Method: 研究三元背景中的蕴含关系，特别关注条件属性蕴含和属性条件蕴含，目标是构建这些蕴含的最优基

Result: 未在摘要中明确说明具体结果，但研究目标是构建最优基

Conclusion: 该研究为三元背景中的蕴含关系提供了系统性的基构建方法

Abstract: This article studies implications in triadic contexts. Specifically, we focus on those introduced by Ganter and Obiedkov, namely conditional attribute and attributional condition implications. Our aim is to construct an optimal base for these implications.

</details>


### [43] [A New Benchmark for the Appropriate Evaluation of RTL Code Optimization](https://arxiv.org/abs/2601.01765)
*Yao Lu,Shang Liu,Hangan Zhou,Wenji Fang,Qijun Zhang,Zhiyao Xie*

Main category: cs.AI

TL;DR: 本文提出了RTL-OPT基准测试，用于评估大语言模型在RTL代码优化方面的能力，弥补现有基准主要关注语法正确性而非PPA优化质量的不足。


<details>
  <summary>Details</summary>
Motivation: 当前AI在集成电路设计中的应用日益重要，但现有基于大语言模型的RTL代码生成基准主要评估语法正确性，缺乏对功耗、性能和面积（PPA）优化质量的评估。需要建立一个能够评估LLMs在RTL优化方面能力的标准化基准。

Method: 创建了RTL-OPT基准，包含36个手工设计的数字电路，涵盖组合逻辑、流水线数据通路、有限状态机和存储器接口等不同实现类别。每个任务提供一对RTL代码：次优版本和人工优化的参考版本，后者体现了行业验证的优化模式。同时集成了自动化评估框架来验证功能正确性并量化PPA改进。

Result: 开发了一个包含36个设计任务的基准测试集，每个任务都有明确的优化目标（次优代码vs优化参考）。建立了自动化评估框架，能够标准化评估生成模型在硬件设计优化方面的能力。

Conclusion: RTL-OPT基准填补了现有评估体系的空白，为评估大语言模型在RTL优化方面的能力提供了标准化工具，有助于推动AI在硬件设计优化领域的发展。

Abstract: The rapid progress of artificial intelligence increasingly relies on efficient integrated circuit (IC) design. Recent studies have explored the use of large language models (LLMs) for generating Register Transfer Level (RTL) code, but existing benchmarks mainly evaluate syntactic correctness rather than optimization quality in terms of power, performance, and area (PPA). This work introduces RTL-OPT, a benchmark for assessing the capability of LLMs in RTL optimization. RTL-OPT contains 36 handcrafted digital designs that cover diverse implementation categories including combinational logic, pipelined datapaths, finite state machines, and memory interfaces. Each task provides a pair of RTL codes, a suboptimal version and a human-optimized reference that reflects industry-proven optimization patterns not captured by conventional synthesis tools. Furthermore, RTL-OPT integrates an automated evaluation framework to verify functional correctness and quantify PPA improvements, enabling standardized and meaningful assessment of generative models for hardware design optimization.

</details>


### [44] [Improving Behavioral Alignment in LLM Social Simulations via Context Formation and Navigation](https://arxiv.org/abs/2601.01546)
*Letian Kong,Qianran,Jin,Renyu Zhang*

Main category: cs.AI

TL;DR: 论文提出一个两阶段框架来改进LLM在复杂决策环境中的行为对齐：第一阶段上下文形成明确指定实验设计，第二阶段上下文导航在该表示中引导推理过程。验证表明复杂决策需要两阶段，而简单任务仅需第一阶段。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型越来越多地用于模拟人类实验行为，但在复杂决策环境中（参与者需要预测他人行动并基于观察行为形成信念）与人类决策存在系统性偏差，需要改进行为对齐方法。

Method: 提出两阶段框架：1）上下文形成阶段明确指定实验设计，建立决策任务及其上下文的准确表示；2）上下文导航阶段在该表示中引导推理过程以做出决策。通过三个实验验证：顺序购买游戏、众筹游戏和需求估计任务，使用四种SOTA模型进行测试。

Result: 在四种SOTA模型上验证发现：复杂决策环境需要两阶段才能实现与人类基准的行为对齐，而简单的需求估计任务仅需上下文形成阶段。框架为设计和诊断LLM社会模拟提供了系统方法。

Conclusion: 两阶段框架澄清了何时需要每个阶段，为行为研究中将LLM社会模拟作为人类受试者的补充提供了系统设计方法，提高了LLM在复杂决策环境中的行为对齐能力。

Abstract: Large language models (LLMs) are increasingly used to simulate human behavior in experimental settings, but they systematically diverge from human decisions in complex decision-making environments, where participants must anticipate others' actions and form beliefs based on observed behavior. We propose a two-stage framework for improving behavioral alignment. The first stage, context formation, explicitly specifies the experimental design to establish an accurate representation of the decision task and its context. The second stage, context navigation, guides the reasoning process within that representation to make decisions. We validate this framework through a focal replication of a sequential purchasing game with quality signaling (Kremer and Debo, 2016), extending to a crowdfunding game with costly signaling (Cason et al., 2025) and a demand-estimation task (Gui and Toubia, 2025) to test generalizability across decision environments. Across four state-of-the-art (SOTA) models (GPT-4o, GPT-5, Claude-4.0-Sonnet-Thinking, DeepSeek-R1), we find that complex decision-making environments require both stages to achieve behavioral alignment with human benchmarks, whereas the simpler demand-estimation task requires only context formation. Our findings clarify when each stage is necessary and provide a systematic approach for designing and diagnosing LLM social simulations as complements to human subjects in behavioral research.

</details>


### [45] [Logics-STEM: Empowering LLM Reasoning via Failure-Driven Post-Training and Document Knowledge Enhancement](https://arxiv.org/abs/2601.01562)
*Mingyu Xu,Cheng Fang,Keyue Jiang,Yuqian Zheng,Yanghua Xiao,Baojian Zhou,Qifang Zhao,Suhang Zheng,Xiuwen Zhu,Jiyang Tang,Yongchi Zhao,Yijia Luo,Zhiqi Bai,Yuchi Xu,Wenbo Su,Wei Wang,Bing Zhao,Lin Qu,Xiaoxiao Xu*

Main category: cs.AI

TL;DR: Logics-STEM是一个针对STEM领域推理任务优化的先进推理模型，通过数据算法协同设计，在10M规模高质量数据集上微调，在STEM基准测试中比同类8B模型平均提升4.68%


<details>
  <summary>Details</summary>
Motivation: 针对STEM（科学、技术、工程、数学）领域的推理任务，需要高质量、大规模的推理数据来提升模型性能。现有开源长链思维数据集规模有限，且缺乏专门针对STEM领域的数据集和优化方法

Method: 采用数据算法协同设计引擎：1）数据方面，通过5阶段数据整理引擎（标注、去重、去污染、蒸馏、分层采样）构建10M规模的Logics-STEM-SFT-Dataset；2）算法方面，采用失败驱动的后训练框架，基于SFT阶段的模型失败区域进行针对性知识检索和数据合成，指导第二阶段SFT或强化学习

Result: Logics-STEM在STEM相关基准测试中表现出色，比同类8B规模的最佳模型平均提升4.68%。提供了8B和32B模型以及10M和2.2M版本的数据集

Conclusion: 大规模开源数据与精心设计的合成数据结合具有巨大潜力，数据算法协同设计通过后训练显著提升推理能力。开源模型和数据集支持社区研究

Abstract: We present Logics-STEM, a state-of-the-art reasoning model fine-tuned on Logics-STEM-SFT-Dataset, a high-quality and diverse dataset at 10M scale that represents one of the largest-scale open-source long chain-of-thought corpora. Logics-STEM targets reasoning tasks in the domains of Science, Technology, Engineering, and Mathematics (STEM), and exhibits exceptional performance on STEM-related benchmarks with an average improvement of 4.68% over the next-best model at 8B scale. We attribute the gains to our data-algorithm co-design engine, where they are jointly optimized to fit a gold-standard distribution behind reasoning. Data-wise, the Logics-STEM-SFT-Dataset is constructed from a meticulously designed data curation engine with 5 stages to ensure the quality, diversity, and scalability, including annotation, deduplication, decontamination, distillation, and stratified sampling. Algorithm-wise, our failure-driven post-training framework leverages targeted knowledge retrieval and data synthesis around model failure regions in the Supervised Fine-tuning (SFT) stage to effectively guide the second-stage SFT or the reinforcement learning (RL) for better fitting the target distribution. The superior empirical performance of Logics-STEM reveals the vast potential of combining large-scale open-source data with carefully designed synthetic data, underscoring the critical role of data-algorithm co-design in enhancing reasoning capabilities through post-training. We make both the Logics-STEM models (8B and 32B) and the Logics-STEM-SFT-Dataset (10M and downsampled 2.2M versions) publicly available to support future research in the open-source community.

</details>


### [46] [Structured Decomposition for LLM Reasoning: Cross-Domain Validation and Semantic Web Integration](https://arxiv.org/abs/2601.01609)
*Albert Sadowski,Jarosław A. Chudziak*

Main category: cs.AI

TL;DR: 本文提出了一种结合大语言模型和符号推理的框架，将自然语言文本转换为ABox断言，然后使用SWRL规则进行确定性推理，在三个领域验证了该方法优于少样本提示。


<details>
  <summary>Details</summary>
Motivation: 在需要可审计和可解释决策的领域（如临床协议、法律证据规则、科学标准），规则推理需要同时具备解释灵活性和形式化保证。大语言模型提供灵活性但无法保证一致性，符号系统提供保证但需要结构化输入。

Method: 提出集成模式：使用LLMs作为本体填充引擎，将非结构化文本转换为基于专家编写TBox规范的ABox断言，同时使用SWRL推理器应用规则提供确定性保证。框架将推理分解为实体识别、断言提取和符号验证三个步骤。

Result: 在三个领域（法律传闻证据判定、科学方法任务应用、临床试验资格）和十一个语言模型上验证了该方法。结构化分解在总体上比少样本提示有统计显著改进，三个领域都观察到增益。消融研究证实符号验证比单纯结构化提示有实质性好处。

Conclusion: 该框架结合了LLMs的灵活性和符号推理的确定性保证，填充的ABox可与标准语义网工具集成进行检查和查询，为更丰富的推理模式奠定了基础。

Abstract: Rule-based reasoning over natural language input arises in domains where decisions must be auditable and justifiable: clinical protocols specify eligibility criteria in prose, evidence rules define admissibility through textual conditions, and scientific standards dictate methodological requirements. Applying rules to such inputs demands both interpretive flexibility and formal guarantees. Large language models (LLMs) provide flexibility but cannot ensure consistent rule application; symbolic systems provide guarantees but require structured input. This paper presents an integration pattern that combines these strengths: LLMs serve as ontology population engines, translating unstructured text into ABox assertions according to expert-authored TBox specifications, while SWRL-based reasoners apply rules with deterministic guarantees. The framework decomposes reasoning into entity identification, assertion extraction, and symbolic verification, with task definitions grounded in OWL 2 ontologies. Experiments across three domains (legal hearsay determination, scientific method-task application, clinical trial eligibility) and eleven language models validate the approach. Structured decomposition achieves statistically significant improvements over few-shot prompting in aggregate, with gains observed across all three domains. An ablation study confirms that symbolic verification provides substantial benefit beyond structured prompting alone. The populated ABox integrates with standard semantic web tooling for inspection and querying, positioning the framework for richer inference patterns that simpler formalisms cannot express.

</details>


### [47] [Yuan3.0 Flash: An Open Multimodal Large Language Model for Enterprise Applications](https://arxiv.org/abs/2601.01718)
*YuanLab. ai,:,Shawn Wu,Sean Wang,Louie Li,Darcy Chen,Allen Wang,Jiangang Luo,Xudong Zhao,Joseph Shen,Gawain Ma,Jasper Jia,Marcus Mao,Claire Wang,Hunter He,Carol Wang,Zera Zhang,Jason Wang,Chonly Shen,Leo Zhang,Logan Chen,Qasim Meng,James Gong,Danied Zhao,Penn Zheng,Owen Zhu,Tong Yu*

Main category: cs.AI

TL;DR: Yuan3.0 Flash是一个开源的MoE多模态大语言模型，具有37亿激活参数和400亿总参数，专门为企业任务优化，同时保持通用任务竞争力，并提出了解决大推理模型过度思考问题的RAPO算法。


<details>
  <summary>Details</summary>
Motivation: 为了解决大型推理模型中常见的"过度思考"现象，并为企业导向的任务提供高性能的多模态大语言模型，同时保持开源以促进研究和实际部署。

Method: 提出了Yuan3.0 Flash模型，采用混合专家架构，包含37亿激活参数和400亿总参数。为解决过度思考问题，提出了反射感知自适应策略优化算法，有效调节模型的过度思考行为。

Result: 在企业导向任务如检索增强生成、复杂表格理解和摘要方面表现优异。在数学、科学等领域的推理能力也很强，达到前沿模型相当的准确率，同时仅需约1/4到1/2的平均token数。

Conclusion: Yuan3.0 Flash是一个高性能的开源多模态大语言模型，专门为企业任务优化，通过RAPO算法有效解决了过度思考问题，在多个领域表现出色且效率更高，已完全开源供研究和部署。

Abstract: We introduce Yuan3.0 Flash, an open-source Mixture-of-Experts (MoE) MultiModal Large Language Model featuring 3.7B activated parameters and 40B total parameters, specifically designed to enhance performance on enterprise-oriented tasks while maintaining competitive capabilities on general-purpose tasks. To address the overthinking phenomenon commonly observed in Large Reasoning Models (LRMs), we propose Reflection-aware Adaptive Policy Optimization (RAPO), a novel RL training algorithm that effectively regulates overthinking behaviors. In enterprise-oriented tasks such as retrieval-augmented generation (RAG), complex table understanding, and summarization, Yuan3.0 Flash consistently achieves superior performance. Moreover, it also demonstrates strong reasoning capabilities in domains such as mathematics, science, etc., attaining accuracy comparable to frontier model while requiring only approximately 1/4 to 1/2 of the average tokens. Yuan3.0 Flash has been fully open-sourced to facilitate further research and real-world deployment: https://github.com/Yuan-lab-LLM/Yuan3.0.

</details>


### [48] [AI Agent Systems: Architectures, Applications, and Evaluation](https://arxiv.org/abs/2601.01743)
*Bin Xu*

Main category: cs.AI

TL;DR: 本文对AI智能体架构进行了系统性综述，涵盖推理、规划、工具调用等核心组件，提出了统一的分类体系，并讨论了设计权衡、评估挑战及未来研究方向。


<details>
  <summary>Details</summary>
Motivation: AI智能体（结合基础模型与推理、规划、记忆和工具使用的系统）正成为连接自然语言意图与现实世界计算的实际接口。随着该领域快速发展，需要对各种智能体架构进行系统性梳理和分类，以指导研究和应用。

Method: 通过文献综述方法，将现有工作组织成统一的分类体系：1）智能体组件（策略/LLM核心、记忆、世界模型、规划器、工具路由器、批评器）；2）编排模式（单智能体vs多智能体，集中式vs去中心化协调）；3）部署设置（离线分析vs在线交互辅助，安全关键vs开放任务）。

Result: 提出了全面的AI智能体架构分类框架，识别了关键设计权衡（延迟vs准确性、自主性vs可控性、能力vs可靠性），分析了评估面临的挑战（非确定性、长期信用分配、工具和环境变异性、隐藏成本），总结了测量和基准测试实践。

Conclusion: AI智能体架构研究已形成丰富体系，但仍面临验证和护栏、可扩展内存和上下文管理、决策可解释性、现实工作负载下的可重复评估等开放挑战，需要进一步研究解决。

Abstract: AI agents -- systems that combine foundation models with reasoning, planning, memory, and tool use -- are rapidly becoming a practical interface between natural-language intent and real-world computation. This survey synthesizes the emerging landscape of AI agent architectures across: (i) deliberation and reasoning (e.g., chain-of-thought-style decomposition, self-reflection and verification, and constraint-aware decision making), (ii) planning and control (from reactive policies to hierarchical and multi-step planners), and (iii) tool calling and environment interaction (retrieval, code execution, APIs, and multimodal perception). We organize prior work into a unified taxonomy spanning agent components (policy/LLM core, memory, world models, planners, tool routers, and critics), orchestration patterns (single-agent vs.\ multi-agent; centralized vs.\ decentralized coordination), and deployment settings (offline analysis vs.\ online interactive assistance; safety-critical vs.\ open-ended tasks). We discuss key design trade-offs -- latency vs.\ accuracy, autonomy vs.\ controllability, and capability vs.\ reliability -- and highlight how evaluation is complicated by non-determinism, long-horizon credit assignment, tool and environment variability, and hidden costs such as retries and context growth. Finally, we summarize measurement and benchmarking practices (task suites, human preference and utility metrics, success under constraints, robustness and security) and identify open challenges including verification and guardrails for tool actions, scalable memory and context management, interpretability of agent decisions, and reproducible evaluation under realistic workloads.

</details>


### [49] [Can Large Language Models Solve Engineering Equations? A Systematic Comparison of Direct Prediction and Solver-Assisted Approaches](https://arxiv.org/abs/2601.01774)
*Sai Varun Kodathala,Rakesh Vunnam*

Main category: cs.AI

TL;DR: 本文系统评估了大型语言模型（LLM）在求解超越方程方面的能力，比较了直接数值预测与结合传统迭代求解器的混合架构的效果。研究发现，混合方法（LLM进行符号操作+牛顿-拉弗森迭代）比LLM直接预测的误差降低了67.9%到81.8%，表明LLM更适合作为传统数值求解器的智能接口而非独立计算引擎。


<details>
  <summary>Details</summary>
Motivation: 超越方程在工程实践中广泛存在（如流体力学摩擦系数计算、轨道位置确定等），需要迭代数值求解。研究旨在评估LLM是否能直接求解这些方程，或者结合传统求解器的混合架构是否更有效，以确定LLM在工程计算中的最佳应用方式。

Method: 研究测试了6个最先进的LLM模型（GPT-5.1、GPT-5.2、Gemini-3-Flash、Gemini-2.5-Lite、Claude-Sonnet-4.5、Claude-Opus-4.5），在7个工程领域的100个问题上进行比较。评估了两种方法：1）LLM直接数值预测；2）混合架构（LLM制定控制方程并提供初始条件，牛顿-拉弗森迭代进行数值求解）。

Result: 直接预测的平均相对误差为0.765到1.262，而求解器辅助计算的平均相对误差为0.225到0.301，误差降低了67.9%到81.8%。领域分析显示，电子学领域改进最大（93.1%），因为指数方程对精度敏感；流体力学领域改进最小（7.2%），LLM在该领域表现出有效的模式识别能力。

Conclusion: 当代LLM擅长符号操作和领域知识检索，但在精度关键的迭代算术计算方面存在困难。LLM的最佳部署方式是作为传统数值求解器的智能接口，而不是独立的计算引擎。这一发现为工程计算中LLM的有效应用提供了指导。

Abstract: Transcendental equations requiring iterative numerical solution pervade engineering practice, from fluid mechanics friction factor calculations to orbital position determination. We systematically evaluate whether Large Language Models can solve these equations through direct numerical prediction or whether a hybrid architecture combining LLM symbolic manipulation with classical iterative solvers proves more effective. Testing six state-of-the-art models (GPT-5.1, GPT-5.2, Gemini-3-Flash, Gemini-2.5-Lite, Claude-Sonnet-4.5, Claude-Opus-4.5) on 100 problems spanning seven engineering domains, we compare direct prediction against solver-assisted computation where LLMs formulate governing equations and provide initial conditions while Newton-Raphson iteration performs numerical solution. Direct prediction yields mean relative errors of 0.765 to 1.262 across models, while solver-assisted computation achieves 0.225 to 0.301, representing error reductions of 67.9% to 81.8%. Domain-specific analysis reveals dramatic improvements in Electronics (93.1%) due to exponential equation sensitivity, contrasted with modest gains in Fluid Mechanics (7.2%) where LLMs exhibit effective pattern recognition. These findings establish that contemporary LLMs excel at symbolic manipulation and domain knowledge retrieval but struggle with precision-critical iterative arithmetic, suggesting their optimal deployment as intelligent interfaces to classical numerical solvers rather than standalone computational engines.

</details>


### [50] [Admissibility Alignment](https://arxiv.org/abs/2601.01816)
*Chris Duffey*

Main category: cs.AI

TL;DR: 本文提出"可容许对齐"概念，将AI对齐重新定义为在不确定性下对结果分布的可容许行动和决策选择属性，并通过MAP-AI系统架构实现概率化、决策理论化的对齐评估。


<details>
  <summary>Details</summary>
Motivation: 传统AI对齐方法通常基于静态或二元条件，难以处理不确定性、干预效应、价值模糊性和治理约束。需要一种新的框架来评估AI系统在不确定性下的决策行为，特别是在分布和尾部事件中的表现。

Method: 提出MAP-AI（蒙特卡洛对齐策略）系统架构，通过蒙特卡洛估计结果分布和可容许控制策略选择来实施对齐。该框架评估决策策略在多个可能未来场景中的表现，显式建模不确定性、干预效应、价值模糊性和治理约束。

Result: 对齐评估基于分布属性（包括期望效用、方差、尾部风险和不对齐概率），而非准确性或排名性能。提供了一种可执行的方法论来评估企业级和机构级AI系统的信任度和对齐性。

Conclusion: 该方法为治理AI系统提供了实用基础，其影响由策略在分布和尾部事件中的行为决定，而非单个预测。可容许控制行动选择机制可以在不重新训练或修改底层模型的情况下，改变不确定性下的策略行为。

Abstract: This paper introduces Admissibility Alignment: a reframing of AI alignment as a property of admissible action and decision selection over distributions of outcomes under uncertainty, evaluated through the behavior of candidate policies. We present MAP-AI (Monte Carlo Alignment for Policy) as a canonical system architecture for operationalizing admissibility alignment, formalizing alignment as a probabilistic, decision-theoretic property rather than a static or binary condition.
  MAP-AI, a new control-plane system architecture for aligned decision-making under uncertainty, enforces alignment through Monte Carlo estimation of outcome distributions and admissibility-controlled policy selection rather than static model-level constraints. The framework evaluates decision policies across ensembles of plausible futures, explicitly modeling uncertainty, intervention effects, value ambiguity, and governance constraints. Alignment is assessed through distributional properties including expected utility, variance, tail risk, and probability of misalignment rather than accuracy or ranking performance. This approach distinguishes probabilistic prediction from decision reasoning under uncertainty and provides an executable methodology for evaluating trust and alignment in enterprise and institutional AI systems. The result is a practical foundation for governing AI systems whose impact is determined not by individual forecasts, but by policy behavior across distributions and tail events. Finally, we show how distributional alignment evaluation can be integrated into decision-making itself, yielding an admissibility-controlled action selection mechanism that alters policy behavior under uncertainty without retraining or modifying underlying models.

</details>


### [51] [COMPASS: A Framework for Evaluating Organization-Specific Policy Alignment in LLMs](https://arxiv.org/abs/2601.01836)
*Dasol Choi,DongGeon Lee,Brigitta Jesica Kartono,Helena Berndt,Taeyoun Kwon,Joonwon Jang,Haon Park,Hwanjo Yu,Minsuk Kahng*

Main category: cs.AI

TL;DR: COMPASS是首个评估LLM是否符合组织允许/禁止列表政策的系统框架，发现模型在处理合法请求时表现良好（>95%准确率），但在执行禁令时严重失败（仅拒绝13-40%的违规请求）。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型在医疗、金融等高风险企业应用中的部署，确保模型遵守组织特定政策变得至关重要。然而现有的安全评估仅关注通用危害，缺乏针对组织政策的评估框架。

Method: 开发COMPASS框架，应用于八个不同行业场景，生成并验证5,920个查询，测试常规合规性和通过策略性设计的边缘案例测试对抗鲁棒性。评估了七个最先进的模型。

Result: 发现基本不对称性：模型可靠处理合法请求（>95%准确率），但在执行禁令时灾难性失败，仅拒绝13-40%的对抗性禁止列表违规请求。

Conclusion: 当前LLM缺乏政策关键部署所需的鲁棒性，COMPESS作为组织AI安全的重要评估框架被确立。

Abstract: As large language models are deployed in high-stakes enterprise applications, from healthcare to finance, ensuring adherence to organization-specific policies has become essential. Yet existing safety evaluations focus exclusively on universal harms. We present COMPASS (Company/Organization Policy Alignment Assessment), the first systematic framework for evaluating whether LLMs comply with organizational allowlist and denylist policies. We apply COMPASS to eight diverse industry scenarios, generating and validating 5,920 queries that test both routine compliance and adversarial robustness through strategically designed edge cases. Evaluating seven state-of-the-art models, we uncover a fundamental asymmetry: models reliably handle legitimate requests (>95% accuracy) but catastrophically fail at enforcing prohibitions, refusing only 13-40% of adversarial denylist violations. These results demonstrate that current LLMs lack the robustness required for policy-critical deployments, establishing COMPASS as an essential evaluation framework for organizational AI safety.

</details>


### [52] [Clinical Knowledge Graph Construction and Evaluation with Multi-LLMs via Retrieval-Augmented Generation](https://arxiv.org/abs/2601.01844)
*Udiptaman Das,Krishnasai B. Atmakuri,Duy Ho,Chi Lee,Yugyung Lee*

Main category: cs.AI

TL;DR: 本文提出了一种基于多智能体提示和模式约束检索增强生成（KG-RAG）的端到端框架，用于从自由文本构建临床知识图谱，特别针对肿瘤学领域，无需依赖黄金标准标注。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型为从非结构化临床叙述构建知识图谱提供了新机会，但现有方法通常依赖结构化输入，缺乏对事实准确性和语义一致性的鲁棒验证，这在肿瘤学领域尤其成问题。

Method: 采用多智能体提示和模式约束检索增强生成（KG-RAG）策略，包括：(1) 提示驱动的实体、属性和关系提取；(2) 基于熵的不确定性评分；(3) 本体对齐的RDF/OWL模式生成；(4) 多LLM共识验证用于幻觉检测和语义精炼。

Result: 应用于两个肿瘤学队列（PDAC和BRCA），该方法生成了可解释、SPARQL兼容且临床基础的知识图谱，实验结果显示在精确度、相关性和本体合规性方面相比基线方法有持续提升。

Conclusion: 该框架支持连续精炼和自监督评估，能够迭代改进图谱质量，为临床知识图谱构建提供了无需黄金标准标注的有效解决方案。

Abstract: Large language models (LLMs) offer new opportunities for constructing knowledge graphs (KGs) from unstructured clinical narratives. However, existing approaches often rely on structured inputs and lack robust validation of factual accuracy and semantic consistency, limitations that are especially problematic in oncology. We introduce an end-to-end framework for clinical KG construction and evaluation directly from free text using multi-agent prompting and a schema-constrained Retrieval-Augmented Generation (KG-RAG) strategy. Our pipeline integrates (1) prompt-driven entity, attribute, and relation extraction; (2) entropy-based uncertainty scoring; (3) ontology-aligned RDF/OWL schema generation; and (4) multi-LLM consensus validation for hallucination detection and semantic refinement. Beyond static graph construction, the framework supports continuous refinement and self-supervised evaluation, enabling iterative improvement of graph quality. Applied to two oncology cohorts (PDAC and BRCA), our method produces interpretable, SPARQL-compatible, and clinically grounded knowledge graphs without relying on gold-standard annotations. Experimental results demonstrate consistent gains in precision, relevance, and ontology compliance over baseline methods.

</details>


### [53] [Jenius Agent: Towards Experience-Driven Accuracy Optimization in Real-World Scenarios](https://arxiv.org/abs/2601.01857)
*Defei Xia,Bingfeng Pi,Shenbin Zhang,Song Hua,Yunfei Wei,Lei Zuo*

Main category: cs.AI

TL;DR: 本文提出Jenius-Agent框架，通过自适应提示生成、上下文感知工具编排和分层内存机制三大创新，提升LLM智能体的任务准确率20%，同时降低token成本、响应延迟和调用失败率。


<details>
  <summary>Details</summary>
Motivation: 随着基于大语言模型的智能体系统发展，提升自主智能体在上下文理解、工具使用和响应生成方面的任务性能变得日益重要。尽管先前研究推进了LLM智能体的整体设计，但对其内部推理和工具使用流程的系统优化仍待探索。

Method: 提出基于真实世界实践经验的智能体框架，包含三个关键创新：(1) 自适应提示生成策略，根据智能体状态和任务目标调整提示以提高可靠性和鲁棒性；(2) 上下文感知工具编排模块，基于用户意图和上下文进行工具分类、语义检索和自适应调用；(3) 分层内存机制，集成会话内存、任务历史和外部摘要，通过动态摘要和压缩提高相关性和效率。框架集成了基于模型上下文协议的工具、文件输入/输出和执行反馈三大优化。

Result: 实验显示任务准确率提升20%，同时降低了token成本、响应延迟和调用失败率。该框架已在Jenius平台部署，为鲁棒、协议兼容的自主智能体提供了轻量级可扩展解决方案。

Conclusion: Jenius-Agent框架通过系统优化智能体的内部推理和工具使用流程，显著提升了任务性能，为构建可靠、高效的自主智能体提供了实用解决方案。

Abstract: As agent systems powered by large language models (LLMs) advance, improving the task performance of an autonomous agent, especially in context understanding, tool usage, and response generation, has become increasingly critical. Although prior studies have advanced the overall design of LLM-based agents, systematic optimization of their internal reasoning and tool-use pipelines remains underexplored. This paper introduces an agent framework grounded in real-world practical experience, with three key innovations: (1) an adaptive prompt generation strategy that aligns with the agent's state and task goals to improve reliability and robustness; (2) a context-aware tool orchestration module that performs tool categorization, semantic retrieval, and adaptive invocation based on user intent and context; and (3) a layered memory mechanism that integrates session memory, task history, and external summaries to improve relevance and efficiency through dynamic summarization and compression. An end-to-end framework named Jenius-Agent has been integrated with three key optimizations, including tools based on the Model Context Protocol (MCP), file input/output (I/O), and execution feedback. The experiments show a 20 percent improvement in task accuracy, along with a reduced token cost, response latency, and invocation failures. The framework is already deployed in Jenius (https://www.jenius.cn), providing a lightweight and scalable solution for robust, protocol-compatible autonomous agents.

</details>


### [54] [Theory Trace Card: Theory-Driven Socio-Cognitive Evaluation of LLMs](https://arxiv.org/abs/2601.01878)
*Farzan Karimi-Malekabadi,Suhaib Abdurahman,Zhivar Sourati,Jackson Trager,Morteza Dehghani*

Main category: cs.AI

TL;DR: 论文指出当前大语言模型的社会认知基准测试存在评估-部署差距，根本原因在于缺乏明确的理论基础，导致基准测试结果被过度泛化。作者提出了理论追踪卡（TTC）作为解决方案。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型在基准测试中得分很高，但这些分数往往无法预测真实世界的行为表现。现有研究将这种评估-部署差距归因于测量和效度问题，但作者认为更根本的问题是缺乏对目标能力的明确理论界定。

Method: 首先诊断并形式化了"理论缺口"问题，然后提出了理论追踪卡（TTC）——一种轻量级的文档工具，用于明确记录评估的理论基础、目标能力组件、操作化过程及其局限性。

Result: TTC通过明确理论、任务操作化、评分和局限性之间的完整效度链，增强了社会认知评估的可解释性和可重用性，而不需要修改基准测试或要求统一的理论共识。

Conclusion: 理论缺口是导致基准测试结果被系统性地过度泛化的根本原因。TTC提供了一种实用的解决方案，通过明确记录评估的理论基础来增强评估的透明度和有效性。

Abstract: Socio-cognitive benchmarks for large language models (LLMs) often fail to predict real-world behavior, even when models achieve high benchmark scores. Prior work has attributed this evaluation-deployment gap to problems of measurement and validity. While these critiques are insightful, we argue that they overlook a more fundamental issue: many socio-cognitive evaluations proceed without an explicit theoretical specification of the target capability, leaving the assumptions linking task performance to competence implicit. Without this theoretical grounding, benchmarks that exercise only narrow subsets of a capability are routinely misinterpreted as evidence of broad competence: a gap that creates a systemic validity illusion by masking the failure to evaluate the capability's other essential dimensions. To address this gap, we make two contributions. First, we diagnose and formalize this theory gap as a foundational failure that undermines measurement and enables systematic overgeneralization of benchmark results. Second, we introduce the Theory Trace Card (TTC), a lightweight documentation artifact designed to accompany socio-cognitive evaluations, which explicitly outlines the theoretical basis of an evaluation, the components of the target capability it exercises, its operationalization, and its limitations. We argue that TTCs enhance the interpretability and reuse of socio-cognitive evaluations by making explicit the full validity chain, which links theory, task operationalization, scoring, and limitations, without modifying benchmarks or requiring agreement on a single theory.

</details>


### [55] [MMP-A*: Multimodal Perception Enhanced Incremental Heuristic Search on Path Planning](https://arxiv.org/abs/2601.01910)
*Minh Hieu Ha,Khanh Ly Ta,Hung Phan,Tung Doan,Tung Dao,Dao Tran,Huynh Thi Thanh Binh*

Main category: cs.AI

TL;DR: MMP-A*：一个结合视觉语言模型空间感知能力与自适应衰减机制的多模态路径规划框架，在复杂环境中实现近乎最优轨迹，显著降低计算和内存开销。


<details>
  <summary>Details</summary>
Motivation: 传统A*算法在大规模复杂环境中计算和内存成本过高，而基于大语言模型的路径规划方法缺乏空间感知能力，在拓扑复杂环境中容易产生错误路径点，无法准确理解物理边界，导致效率低下。

Method: 提出MMP-A*多模态框架：1）集成视觉语言模型的空间感知能力，将高层推理锚定在物理几何中；2）引入自适应衰减机制，动态调节不确定路径点在启发式函数中的影响，确保几何有效性同时大幅减少内存开销。

Result: 在具有严重杂乱和拓扑复杂性的挑战性环境中测试，MMP-A*能够生成近乎最优的轨迹，同时显著降低操作成本，证明了其作为感知基础和计算高效自主导航范式的潜力。

Conclusion: MMP-A*通过结合视觉语言模型的空间感知能力和自适应衰减机制，解决了纯文本规划器的局限性，为自主导航提供了一个感知基础且计算高效的范式。

Abstract: Autonomous path planning requires a synergy between global reasoning and geometric precision, especially in complex or cluttered environments. While classical A* is valued for its optimality, it incurs prohibitive computational and memory costs in large-scale scenarios. Recent attempts to mitigate these limitations by using Large Language Models for waypoint guidance remain insufficient, as they rely only on text-based reasoning without spatial grounding. As a result, such models often produce incorrect waypoints in topologically complex environments with dead ends, and lack the perceptual capacity to interpret ambiguous physical boundaries. These inconsistencies lead to costly corrective expansions and undermine the intended computational efficiency.
  We introduce MMP-A*, a multimodal framework that integrates the spatial grounding capabilities of vision-language models with a novel adaptive decay mechanism. By anchoring high-level reasoning in physical geometry, the framework produces coherent waypoint guidance that addresses the limitations of text-only planners. The adaptive decay mechanism dynamically regulates the influence of uncertain waypoints within the heuristic, ensuring geometric validity while substantially reducing memory overhead. To evaluate robustness, we test the framework in challenging environments characterized by severe clutter and topological complexity. Experimental results show that MMP-A* achieves near-optimal trajectories with significantly reduced operational costs, demonstrating its potential as a perception-grounded and computationally efficient paradigm for autonomous navigation.

</details>


### [56] [OpenSocInt: A Multi-modal Training Environment for Human-Aware Social Navigation](https://arxiv.org/abs/2601.01939)
*Victor Sanchez,Chris Reinke,Ahamed Mohamed,Xavier Alameda-Pineda*

Main category: cs.AI

TL;DR: OpenSocInt是一个开源的多模态社交交互模拟器软件包，提供模块化架构训练社交智能体，已应用于社交导航任务实验


<details>
  <summary>Details</summary>
Motivation: 开发一个开源的多模态社交交互模拟框架，支持研究不同感知特征、编码融合方法以及各种社交智能体的训练

Method: 创建了OpenSocInt开源软件包，包含多模态社交交互模拟器和模块化架构，支持探索不同感知特征的编码与融合，以及不同类型智能体的训练

Result: 软件已公开发布在GitLab上（GPL许可证），并通过社交导航任务的实验协议展示了其应用价值

Conclusion: OpenSocInt为多模态社交交互研究提供了一个灵活的开源平台，支持探索不同感知特征和智能体架构，有助于推动社交智能体的发展

Abstract: In this paper, we introduce OpenSocInt, an open-source software package providing a simulator for multi-modal social interactions and a modular architecture to train social agents. We described the software package and showcased its interest via an experimental protocol based on the task of social navigation. Our framework allows for exploring the use of different perceptual features, their encoding and fusion, as well as the use of different agents. The software is already publicly available under GPL at https://gitlab.inria.fr/robotlearn/OpenSocInt/.

</details>


### [57] [CNC-TP: Classifier Nominal Concept Based on Top-Pertinent Attributes](https://arxiv.org/abs/2601.01976)
*Yasmine Souissi,Fabrice Boissier,Nida Meddouri*

Main category: cs.AI

TL;DR: 本文对基于形式概念分析（FCA）的分类器进行了综述，提出了一种从名义数据计算闭包算子的新方法，并构建了专注于最相关概念的部分概念格。


<details>
  <summary>Details</summary>
Motivation: 知识发现（KDD）旨在从海量数据中提取隐藏的有意义知识，其中分类是核心数据挖掘技术之一。形式概念分析（FCA）因其可解释性和可解释性学习能力而受到认可，但需要更有效的方法来处理名义数据和构建概念格。

Method: 1. 对基于FCA的分类器进行最新综述；2. 探索从名义数据计算闭包算子的多种方法；3. 提出构建部分概念格的新方法，专注于最相关概念。

Result: 提供了实验结果来证明所提出方法的效率，展示了在分类任务中的性能表现。

Conclusion: 提出的方法能够有效处理名义数据并构建部分概念格，为基于FCA的分类器提供了更高效的实现方案，增强了可解释性学习的能力。

Abstract: Knowledge Discovery in Databases (KDD) aims to exploit the vast amounts of data generated daily across various domains of computer applications. Its objective is to extract hidden and meaningful knowledge from datasets through a structured process comprising several key steps: data selection, preprocessing, transformation, data mining, and visualization. Among the core data mining techniques are classification and clustering. Classification involves predicting the class of new instances using a classifier trained on labeled data. Several approaches have been proposed in the literature, including Decision Tree Induction, Bayesian classifiers, Nearest Neighbor search, Neural Networks, Support Vector Machines, and Formal Concept Analysis (FCA). The last one is recognized as an effective approach for interpretable and explainable learning. It is grounded in the mathematical structure of the concept lattice, which enables the generation of formal concepts and the discovery of hidden relationships among them. In this paper, we present a state-of-theart review of FCA-based classifiers. We explore various methods for computing closure operators from nominal data and introduce a novel approach for constructing a partial concept lattice that focuses on the most relevant concepts. Experimental results are provided to demonstrate the efficiency of the proposed method.

</details>


### [58] [ChaosBench-Logic: A Benchmark for Logical and Symbolic Reasoning on Chaotic Dynamical Systems](https://arxiv.org/abs/2601.01982)
*Noel Thomas*

Main category: cs.AI

TL;DR: ChaosBench-Logic是一个评估大语言模型在混沌动力系统中逻辑推理能力的基准测试，包含30个系统、621个问题，涵盖7种推理类型，发现前沿LLMs在单项准确率上达到91-94%，但在组合推理和全局一致性上表现脆弱。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在自然语言任务上表现出色，但在需要精确逻辑和符号推理的领域仍然脆弱。混沌动力系统提供了一个特别具有挑战性的测试环境，因为混沌是确定性的，但常被误解为随机性或复杂性。需要建立一个统一的基准来评估LLMs在这种复杂逻辑推理场景下的能力。

Method: 引入ChaosBench-Logic基准，包含30个不同的动力系统，使用统一的一阶逻辑本体进行标注，每个系统标注了11个语义谓词的真值分配。生成了621个问题，涵盖7种推理类别：多步推理、跨系统类比、反事实推理、偏见探测和多轮对话。定义了逻辑准确性、蕴含一致性、对话连贯性和矛盾性等评估指标，并发布了开源评估流程。

Result: 前沿LLMs（GPT-4、Claude 3.5 Sonnet、Gemini 2.5 Flash、LLaMA-3 70B）在单项准确率上达到91-94%，但在组合推理项目上得分为0%，表现出脆弱的全局一致性。对话级准确率从53.1%（GPT-4 CoT）到75.5%（LLaMA-3零样本）不等。

Conclusion: ChaosBench-Logic为诊断LLMs在复杂逻辑推理中的失败提供了一个严格的测试平台，并为开发神经符号方法以改进LLMs的科学推理能力奠定了基础。研究表明，尽管LLMs在单项任务上表现良好，但在需要组合推理和全局一致性的复杂逻辑任务中仍然存在显著缺陷。

Abstract: Large language models (LLMs) excel at natural language tasks but remain brittle in domains requiring precise logical and symbolic reasoning. Chaotic dynamical systems provide an especially demanding test because chaos is deterministic yet often misinterpreted as randomness or complexity. We introduce ChaosBench-Logic, a benchmark that evaluates LLM reasoning across 30 diverse dynamical systems using a unified first-order logic (FOL) ontology. Each system is annotated with truth assignments for 11 semantic predicates, and 621 questions are generated across seven reasoning categories, including multi-hop implications, cross-system analogies, counterfactual reasoning, bias probes, and multi-turn dialogues. We define metrics for logical accuracy, implication consistency, dialogue coherence, and contradiction, and we release an open-source evaluation pipeline. Initial experiments show that frontier LLMs such as GPT-4, Claude 3.5 Sonnet, Gemini 2.5 Flash, and the open-source LLaMA-3 70B achieve 91-94% per-item accuracy, yet still score 0% on compositional items and exhibit fragile global coherence. Dialogue-level accuracy ranges from 53.1% (GPT-4 CoT) to 75.5% (LLaMA-3 zero-shot). ChaosBench-Logic provides a rigorous testbed for diagnosing such failures and a foundation for developing neuro-symbolic approaches that improve scientific reasoning in LLMs.

</details>


### [59] [MindChat: A Privacy-preserving Large Language Model for Mental Health Support](https://arxiv.org/abs/2601.01993)
*Dong Xue,Jicheng Tu,Ming Wang,Xin Yan,Fangzhou Liu,Jie Hu*

Main category: cs.AI

TL;DR: MindChat是一个保护隐私的心理健康支持大语言模型，配合MindCorpus合成多轮心理咨询数据集，通过联邦学习和差分隐私降低隐私风险，在心理咨询能力评估中表现优异。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在心理健康支持方面有潜力，但受限于真实心理咨询对话数据的稀缺性和敏感性，需要开发既能保护隐私又能提供高质量心理咨询的解决方案。

Method: 1. 开发MindCorpus合成数据集：采用多智能体角色扮演框架，包含双闭环反馈设计（回合级批判修订和会话级策略优化）；2. 构建MindChat模型：使用联邦学习配合参数高效的LoRA适配器进行微调，并加入差分隐私优化来降低成员推理和记忆风险。

Result: 1. MindCorpus提高了训练效果；2. MindChat在自动LLM评估和人工评估中与现有通用和心理咨询导向的LLM基线竞争；3. 在成员推理攻击下表现出减少的隐私泄露。

Conclusion: MindChat是一个有效的隐私保护心理健康支持LLM，通过合成数据生成和隐私保护技术，在保持心理咨询能力的同时降低了隐私风险，为敏感领域的AI应用提供了可行方案。

Abstract: Large language models (LLMs) have shown promise for mental health support, yet training such models is constrained by the scarcity and sensitivity of real counseling dialogues. In this article, we present MindChat, a privacy-preserving LLM for mental health support, together with MindCorpus, a synthetic multi-turn counseling dataset constructed via a multi-agent role-playing framework. To synthesize high-quality counseling data, the developed dialogue-construction framework employs a dual closed-loop feedback design to integrate psychological expertise and counseling techniques through role-playing: (i) turn-level critique-and-revision to improve coherence and counseling appropriateness within a session, and (ii) session-level strategy refinement to progressively enrich counselor behaviors across sessions. To mitigate privacy risks under decentralized data ownership, we fine-tune the base model using federated learning with parameter-efficient LoRA adapters and incorporate differentially private optimization to reduce membership and memorization risks. Experiments on synthetic-data quality assessment and counseling capability evaluation show that MindCorpus improves training effectiveness and that MindChat is competitive with existing general and counseling-oriented LLM baselines under both automatic LLM-judge and human evaluation protocols, while exhibiting reduced privacy leakage under membership inference attacks.

</details>


### [60] [XAI-MeD: Explainable Knowledge Guided Neuro-Symbolic Framework for Domain Generalization and Rare Class Detection in Medical Imaging](https://arxiv.org/abs/2601.02008)
*Midhat Urooj,Ayan Banerjee,Sandeep Gupta*

Main category: cs.AI

TL;DR: XAIMeD是一个可解释的医疗AI框架，通过神经符号架构整合临床专家知识，提升分布偏移下的鲁棒性、罕见类别敏感性，并提供临床对齐的解释。


<details>
  <summary>Details</summary>
Motivation: 医疗AI中可解释性、领域泛化和罕见类别可靠性是关键挑战，深度模型在真实世界分布偏移下经常失败，并对不常见的临床条件表现出偏见。

Method: XAIMeD将临床专业知识编码为原子医学命题的逻辑连接，转化为机器可检查的类别特定规则。通过加权特征满足分数量化诊断效用，创建符号推理分支补充神经预测。使用置信度加权融合整合符号和深度输出，并采用受Hunt启发的自适应路由机制，由熵不平衡增益和罕见类别基尼系数指导。

Result: 在四个挑战性任务上评估显示：跨领域泛化性能提升6%，罕见类别F1分数提高10%，显著优于最先进的深度学习基线。消融研究证实临床基础的符号组件作为有效正则化器，确保对分布偏移的鲁棒性。

Conclusion: XAIMeD为多模态医疗AI提供了原则性、临床忠实且可解释的方法，通过整合专家知识提升模型在真实医疗场景中的可靠性和透明度。

Abstract: Explainability domain generalization and rare class reliability are critical challenges in medical AI where deep models often fail under real world distribution shifts and exhibit bias against infrequent clinical conditions This paper introduces XAIMeD an explainable medical AI framework that integrates clinically accurate expert knowledge into deep learning through a unified neuro symbolic architecture XAIMeD is designed to improve robustness under distribution shift enhance rare class sensitivity and deliver transparent clinically aligned interpretations The framework encodes clinical expertise as logical connectives over atomic medical propositions transforming them into machine checkable class specific rules Their diagnostic utility is quantified through weighted feature satisfaction scores enabling a symbolic reasoning branch that complements neural predictions A confidence weighted fusion integrates symbolic and deep outputs while a Hunt inspired adaptive routing mechanism guided by Entropy Imbalance Gain EIG and Rare Class Gini mitigates class imbalance high intra class variability and uncertainty We evaluate XAIMeD across diverse modalities on four challenging tasks i Seizure Onset Zone SOZ localization from rs fMRI ii Diabetic Retinopathy grading across 6 multicenter datasets demonstrate substantial performance improvements including 6 percent gains in cross domain generalization and a 10 percent improved rare class F1 score far outperforming state of the art deep learning baselines Ablation studies confirm that the clinically grounded symbolic components act as effective regularizers ensuring robustness to distribution shifts XAIMeD thus provides a principled clinically faithful and interpretable approach to multimodal medical AI.

</details>


### [61] [Simulated Reasoning is Reasoning](https://arxiv.org/abs/2601.02043)
*Hendrik Kempt,Alon Lavie*

Main category: cs.AI

TL;DR: 论文认为基础模型通过模仿"出声思考"的过程进行推理，这种推理方式与人类符号推理不同，缺乏常识基础且脆弱，但能解决复杂问题，需要重新评估推理的本质和安全考量。


<details>
  <summary>Details</summary>
Motivation: 传统观点认为推理是通过符号推理实现理解的过程，但基础模型展示了不同的推理方式：通过模仿"出声思考"、测试生成路径并迭代来解决问题。这种推理方式缺乏人类推理的常识基础和稳定性，需要重新审视推理的本质及其必要条件。

Method: 本文采用哲学分析方法，探讨基础模型推理现象的多重哲学解释，论证"随机鹦鹉"隐喻已失去相关性，并反思从这些推理模型中产生的安全和适当性考虑的不同规范要素。

Result: 基础模型的推理方式与人类符号推理有本质区别，它通过模仿思考过程而非真正理解来解决问题。这种推理虽然能处理复杂任务，但缺乏常识基础，导致推理过程脆弱。需要重新评估推理概念，并建立针对这种脆弱性的安全防御机制。

Conclusion: 基础模型的推理现象挑战了传统推理概念，表明符号推理不是推理的必要条件。"随机鹦鹉"隐喻已不适用，需要新的理论框架来理解这种新型推理。同时，必须考虑由此产生的安全和伦理问题，建立适当的规范框架来应对基础模型推理能力的增长。

Abstract: Reasoning has long been understood as a pathway between stages of understanding. Proper reasoning leads to understanding of a given subject. This reasoning was conceptualized as a process of understanding in a particular way, i.e., "symbolic reasoning". Foundational Models (FM) demonstrate that this is not a necessary condition for many reasoning tasks: they can "reason" by way of imitating the process of "thinking out loud", testing the produced pathways, and iterating on these pathways on their own. This leads to some form of reasoning that can solve problems on its own or with few-shot learning, but appears fundamentally different from human reasoning due to its lack of grounding and common sense, leading to brittleness of the reasoning process. These insights promise to substantially alter our assessment of reasoning and its necessary conditions, but also inform the approaches to safety and robust defences against this brittleness of FMs. This paper offers and discusses several philosophical interpretations of this phenomenon, argues that the previously apt metaphor of the "stochastic parrot" has lost its relevance and thus should be abandoned, and reflects on different normative elements in the safety- and appropriateness-considerations emerging from these reasoning models and their growing capacity.

</details>


### [62] [FormuLLA: A Large Language Model Approach to Generating Novel 3D Printable Formulations](https://arxiv.org/abs/2601.02071)
*Adeshola Okubena,Yusuf Ali Mohammed,Moe Elbadawi*

Main category: cs.AI

TL;DR: 该研究探讨了将大型语言模型应用于药物3D打印配方开发，通过微调四种LLM架构，评估其在推荐辅料和预测丝材机械性能方面的表现，发现Llama2最适合FDM配方推荐，并揭示了小数据集可能导致灾难性遗忘等问题。


<details>
  <summary>Details</summary>
Motivation: 当前AI驱动的药物3D打印研究大多局限于狭窄领域，未能全面解决配方开发中的复杂挑战。随着人工通用智能概念的发展，需要探索LLM在药物制剂开发中的应用潜力，以超越传统预测模型，实现更通用、类人的推理能力。

Method: 研究微调了四种大型语言模型架构，使用包含1400多种配方的熔融沉积建模数据集。系统评估了微调和生成参数配置，重点关注模型在基于活性药物成分剂量推荐合适辅料以及预测丝材机械性能方面的表现。

Result: Llama2在推荐FDM配方辅料方面表现最佳。模型选择和参数化显著影响性能，较小的LLM出现灾难性遗忘现象。研究还发现：即使相对较小的1400+配方数据集也可能导致模型灾难性遗忘；标准LLM指标仅评估语言性能而非配方可加工性；基于生物医学相关数据训练的LLM不一定产生最佳结果。

Conclusion: 解决这些挑战对于推动LLM超越语言能力、发展成为药物配方开发的可靠系统至关重要。该研究为将人工通用智能概念应用于药物3D打印领域提供了重要见解，强调了需要开发更全面的评估指标来确保配方开发的实际可行性。

Abstract: Pharmaceutical three-dimensional (3D) printing is an advanced fabrication technology with the potential to enable truly personalised dosage forms. Recent studies have integrated artificial intelligence (AI) to accelerate formulation and process development, drastically transforming current approaches to pharmaceutical 3D printing. To date, most AI-driven efforts remain narrowly focused, while failing to account for the broader formulation challenges inherent to the technology. Recent advances in AI have introduced artificial general intelligence concepts, wherein systems extend beyond conventional predictive modelling toward more generalised, human-like reasoning. In this work, we investigate the application of large language models (LLMs), fine-tuned on a fused deposition modelling (FDM) dataset comprising over 1400 formulations, to recommend suitable excipients based on active pharmaceutical ingredient (API) dose, and predict filament mechanical properties. Four LLM architectures were fine-tuned, with systematic evaluation of both fine-tuning and generative parameter configurations. Our results demonstrate that Llama2 was best suited for recommending excipients for FDM formulations. Additionally, model selection and parameterisation significantly influence performance, with smaller LLMs exhibiting instances of catastrophic forgetting. Furthermore, we demonstrate: (i) even with relatively small dataset of over 1400 formulations, it can lead to model catastrophic forgetting; (ii) standard LLM metrics only evaluate linguistic performance but not formulation processability; and (iii) LLMs trained on biomedically-related data do not always produce the best results. Addressing these challenges is essential to advancing LLMs beyond linguistic proficiency and toward reliable systems for pharmaceutical formulation development.

</details>


### [63] [Streaming Hallucination Detection in Long Chain-of-Thought Reasoning](https://arxiv.org/abs/2601.02170)
*Haolang Lu,Minghui Pan,Ripeng Li,Guoshun Nan,Jialin Zhuang,Zijie Zhao,Zhongxiang Sun,Kun Wang,Yang Liu*

Main category: cs.AI

TL;DR: 该论文提出将长链思维推理中的幻觉视为演化潜状态，而非一次性错误事件，并引入累积前缀级幻觉信号来实时检测推理过程中的幻觉传播。


<details>
  <summary>Details</summary>
Motivation: 长链思维推理虽然能提升大语言模型性能，但其中的幻觉往往以微妙方式出现并在推理步骤间传播。现有方法将幻觉视为一次性错误事件，未能捕捉其在长推理链中的演化特性。

Method: 将步骤级幻觉判断视为局部观测，引入累积前缀级幻觉信号来追踪整个推理轨迹中推理状态的全局演化，实现长链思维推理中的流式幻觉检测。

Result: 该方法能够实时检测长链思维推理中的幻觉传播，提供可解释的证据，相比传统的一次性错误检测方法，能更好地捕捉幻觉在推理过程中的演化特性。

Conclusion: 将幻觉建模为演化潜状态而非一次性事件，通过累积前缀级信号追踪推理状态演化，为长链思维推理提供了更有效的实时幻觉检测框架。

Abstract: Long chain-of-thought (CoT) reasoning improves the performance of large language models, yet hallucinations in such settings often emerge subtly and propagate across reasoning steps. We suggest that hallucination in long CoT reasoning is better understood as an evolving latent state rather than a one-off erroneous event. Accordingly, we treat step-level hallucination judgments as local observations and introduce a cumulative prefix-level hallucination signal that tracks the global evolution of the reasoning state over the entire trajectory. Overall, our approach enables streaming hallucination detection in long CoT reasoning, providing real-time, interpretable evidence.

</details>


### [64] [Falcon-H1R: Pushing the Reasoning Frontiers with a Hybrid Model for Efficient Test-Time Scaling](https://arxiv.org/abs/2601.02346)
*Falcon LLM Team,Iheb Chaabane,Puneesh Khanna,Suhail Mohmad,Slim Frikha,Shi Hu,Abdalgader Abubaker,Reda Alami,Mikhail Lubinets,Mohamed El Amine Seddik,Hakim Hacid*

Main category: cs.AI

TL;DR: Falcon-H1R是一个7B参数的推理优化模型，证明了小语言模型也能实现有竞争力的推理性能，在多个推理密集型基准测试中匹配或超越2-7倍大的SOTA模型。


<details>
  <summary>Details</summary>
Motivation: 探索小语言模型是否能够通过精心设计的数据策展和训练策略，在不增加模型规模的情况下实现与大模型相当的推理性能，为构建高效实用的推理系统提供新思路。

Method: 采用混合并行架构设计实现更快推理，通过高效监督微调（SFT）和强化学习（RL）扩展进行针对性训练，结合DeepConf方法实现最先进的测试时扩展效率。

Result: Falcon-H1R-7B在多种推理密集型基准测试中，一致匹配或超越比其大2-7倍的SOTA推理模型，实现了推理效率的3D极限（更快推理、更高token效率、更高准确率）。

Conclusion: 紧凑模型通过针对性的模型训练和架构选择，能够提供稳健且可扩展的推理性能，为需要大量思维链生成和并行测试时扩展的场景提供了实用的骨干模型。

Abstract: This work introduces Falcon-H1R, a 7B-parameter reasoning-optimized model that establishes the feasibility of achieving competitive reasoning performance with small language models (SLMs). Falcon-H1R stands out for its parameter efficiency, consistently matching or outperforming SOTA reasoning models that are $2\times$ to $7\times$ larger across a variety of reasoning-intensive benchmarks. These results underscore the importance of careful data curation and targeted training strategies (via both efficient SFT and RL scaling) in delivering significant performance gains without increasing model size. Furthermore, Falcon-H1R advances the 3D limits of reasoning efficiency by combining faster inference (through its hybrid-parallel architecture design), token efficiency, and higher accuracy. This unique blend makes Falcon-H1R-7B a practical backbone for scaling advanced reasoning systems, particularly in scenarios requiring extensive chain-of-thoughts generation and parallel test-time scaling. Leveraging the recently introduced DeepConf approach, Falcon-H1R achieves state-of-the-art test-time scaling efficiency, offering substantial improvements in both accuracy and computational cost. As a result, Falcon-H1R demonstrates that compact models, through targeted model training and architectural choices, can deliver robust and scalable reasoning performance.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [65] [Noise-Aware and Dynamically Adaptive Federated Defense Framework for SAR Image Target Recognition](https://arxiv.org/abs/2601.00900)
*Yuchao Hou,Zixuan Zhang,Jie Wang,Wenke Huang,Lianhui Liang,Di Wu,Zhiquan Liu,Youliang Tian,Jianming Zhu,Jisheng Dang,Junhao Dong,Zhongliang Guo*

Main category: cs.CR

TL;DR: NADAFD是一个针对SAR图像目标识别的联邦学习防御框架，通过频率域协作反演、噪声感知对抗训练和动态健康评估来抵御SAR特有的后门攻击。


<details>
  <summary>Details</summary>
Motivation: 传统的SAR图像目标识别依赖集中式训练，存在隐私和安全问题。联邦学习虽然能保护数据隐私，但面临恶意客户端利用SAR乘性斑点噪声隐藏后门触发的安全风险，需要专门针对SAR特性的防御机制。

Method: 1. 频率域协作反演机制：通过分析跨客户端频谱不一致性来暴露隐藏的后门触发器；2. 噪声感知对抗训练：将Γ分布斑点噪声特征嵌入到掩码引导的对抗样本生成中，增强对后门攻击和SAR斑点噪声的鲁棒性；3. 动态健康评估模块：跟踪客户端更新行为并自适应调整聚合权重，减轻恶意贡献。

Result: 在MSTAR和OpenSARShip数据集上的实验表明，NADAFD在干净测试样本上获得更高准确率，在触发输入上获得更低的后门攻击成功率，优于现有的联邦后门防御方法。

Conclusion: NADAFD通过集成频率域、空间域和客户端行为分析，有效应对SAR特有的后门威胁，为SAR图像目标识别的联邦学习提供了安全可靠的防御框架。

Abstract: As a critical application of computational intelligence in remote sensing, deep learning-based synthetic aperture radar (SAR) image target recognition facilitates intelligent perception but typically relies on centralized training, where multi-source SAR data are uploaded to a single server, raising privacy and security concerns. Federated learning (FL) provides an emerging computational intelligence paradigm for SAR image target recognition, enabling cross-site collaboration while preserving local data privacy. However, FL confronts critical security risks, where malicious clients can exploit SAR's multiplicative speckle noise to conceal backdoor triggers, severely challenging the robustness of the computational intelligence model. To address this challenge, we propose NADAFD, a noise-aware and dynamically adaptive federated defense framework that integrates frequency-domain, spatial-domain, and client-behavior analyses to counter SAR-specific backdoor threats. Specifically, we introduce a frequency-domain collaborative inversion mechanism to expose cross-client spectral inconsistencies indicative of hidden backdoor triggers. We further design a noise-aware adversarial training strategy that embeds $Γ$-distributed speckle characteristics into mask-guided adversarial sample generation to enhance robustness against both backdoor attacks and SAR speckle noise. In addition, we present a dynamic health assessment module that tracks client update behaviors across training rounds and adaptively adjusts aggregation weights to mitigate evolving malicious contributions. Experiments on MSTAR and OpenSARShip datasets demonstrate that NADAFD achieves higher accuracy on clean test samples and a lower backdoor attack success rate on triggered inputs than existing federated backdoor defenses for SAR target recognition.

</details>


### [66] [Security Hardening Using FABRIC: Implementing a Unified Compliance Aggregator for Linux Servers](https://arxiv.org/abs/2601.00909)
*Sheldon Paul,Izzat Alsmadi*

Main category: cs.CR

TL;DR: 该论文提出了一个统一框架，通过聚合异构安全审计工具来评估FABRIC测试平台上的Linux安全加固效果，开发了统一合规性聚合器（UCA）来标准化不同工具的输出并提供可定制的安全策略评估。


<details>
  <summary>Details</summary>
Motivation: 现有Linux安全审计工具（如Lynis、OpenSCAP、AIDE）缺乏一致的评估标准和输出格式，难以对系统安全加固效果进行统一、可重复的评估，特别是在可编程测试平台环境中。

Method: 在FABRIC测试平台上部署三个不同加固级别（基线、部分、完全）的Ubuntu 22.04节点，使用三种安全审计工具进行108次审计运行。开发统一合规性聚合器（UCA）来解析工具输出，将分数归一化到0-100的统一尺度，并通过加权指标和可定制的规则引擎结合组织特定的安全策略。

Result: 完全加固使OpenSCAP合规性从39.7%提高到71.8%，自定义规则合规性从39.3%提高到83.6%。UCA提供了比单个工具更清晰、更可重复的安全态势评估，能够系统评估可编程测试环境中的加固效果。

Conclusion: 统一合规性聚合器（UCA）框架能够有效整合异构安全审计工具的输出，提供一致的安全评估标准，支持组织特定的安全策略定制，为Linux系统安全加固效果的系统评估提供了可行方案。

Abstract: This paper presents a unified framework for evaluating Linux security hardening on the FABRIC testbed through aggregation of heterogeneous security auditing tools. We deploy three Ubuntu 22.04 nodes configured at baseline, partial, and full hardening levels, and evaluate them using Lynis, OpenSCAP, and AIDE across 108 audit runs. To address the lack of a consistent interpretation across tools, we implement a Unified Compliance Aggregator (UCA) that parses tool outputs, normalizes scores to a common 0--100 scale, and combines them into a weighted metric augmented by a customizable rule engine for organization-specific security policies. Experimental results show that full hardening increases OpenSCAP compliance from 39.7 to 71.8, while custom rule compliance improves from 39.3\% to 83.6\%. The results demonstrate that UCA provides a clearer and more reproducible assessment of security posture than individual tools alone, enabling systematic evaluation of hardening effectiveness in programmable testbed environments.

</details>


### [67] [Emoji-Based Jailbreaking of Large Language Models](https://arxiv.org/abs/2601.00936)
*M P V S Gopinadh,S Mahaboob Hussain*

Main category: cs.CR

TL;DR: 该研究评估了表情符号序列在文本提示中如何绕过大型语言模型的安全对齐机制，在四个开源LLM上测试了50个表情符号攻击提示，发现模型间存在显著差异的漏洞。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型的安全对齐机制可能通过对抗性提示工程被绕过，特别是表情符号序列可能触发有害和不道德的输出，需要系统评估这种攻击的有效性。

Method: 在四个开源LLM（Mistral 7B、Qwen 2 7B、Gemma 2 9B、Llama 3 8B）上评估50个表情符号攻击提示，使用越狱成功率、安全对齐遵守度和延迟作为指标，将响应分类为成功、部分成功和失败。

Result: Gemma 2 9B和Mistral 7B表现出10%的成功率，Qwen 2 7B实现完全对齐（0%成功率），卡方检验（chi^2 = 32.94, p < 0.001）确认模型间存在显著差异。

Conclusion: 表情符号攻击揭示了LLM安全机制的局限性，需要在提示级安全和对齐管道中系统处理表情符号表示，以增强模型鲁棒性。

Abstract: Large Language Models (LLMs) are integral to modern AI applications, but their safety alignment mechanisms can be bypassed through adversarial prompt engineering. This study investigates emoji-based jailbreaking, where emoji sequences are embedded in textual prompts to trigger harmful and unethical outputs from LLMs. We evaluated 50 emoji-based prompts on four open-source LLMs: Mistral 7B, Qwen 2 7B, Gemma 2 9B, and Llama 3 8B. Metrics included jailbreak success rate, safety alignment adherence, and latency, with responses categorized as successful, partial and failed. Results revealed model-specific vulnerabilities: Gemma 2 9B and Mistral 7B exhibited 10 % success rates, while Qwen 2 7B achieved full alignment (0% success). A chi-square test (chi^2 = 32.94, p < 0.001) confirmed significant inter-model differences. While prior works focused on emoji attacks targeting safety judges or classifiers, our empirical analysis examines direct prompt-level vulnerabilities in LLMs. The results reveal limitations in safety mechanisms and highlight the necessity for systematic handling of emoji-based representations in prompt-level safety and alignment pipelines.

</details>


### [68] [CuFuzz: Hardening CUDA Programs through Transformation and Fuzzing](https://arxiv.org/abs/2601.01048)
*Saurabh Singh,Ruobing Han,Jaewon Lee,Seonjin Na,Yonghae Kim,Taesoo Kim,Hyesoon Kim*

Main category: cs.CR

TL;DR: CuFuzz是一个创新的编译器-运行时协同设计解决方案，通过将GPU程序转换为CPU程序，使先进的CPU模糊测试工具能够应用于CUDA代码，填补了GPU安全研究的空白。


<details>
  <summary>Details</summary>
Motivation: GPU安全性和可靠性日益重要，但CUDA缺乏内存安全性可能导致严重漏洞。由于架构差异和缺乏内置错误检测，现有的CPU模糊测试工具无法直接应用于GPU程序，需要专门的GPU模糊测试解决方案。

Method: CuFuzz采用编译器-运行时协同设计，通过编译器IR级转换将GPU程序转换为CPU程序，从而能够利用CPU内存错误检测器（如Address Sanitizer）。为提高模糊测试吞吐量，引入了两种专门针对GPU代码的优化：部分代表性执行（PREX）和访问索引保留剪枝（AXIPrune）。

Result: CuFuzz实现了显著的性能提升：PREX带来平均32倍的吞吐量改进，AXIPrune在PREX优化基础上再提升33%。两者结合可实现最高224.31倍的加速。在实际模糊测试活动中，CuFuzz发现了广泛使用的基准测试中的122个安全漏洞。

Conclusion: CuFuzz是首个为CUDA提供模糊测试支持的机制，成功地将CPU模糊测试工具扩展到GPU程序，显著提高了GPU代码的安全性和可靠性，填补了GPU安全研究的重要空白。

Abstract: GPUs have gained significant popularity over the past decade, extending beyond their original role in graphics rendering. This evolution has brought GPU security and reliability to the forefront of concerns. Prior research has shown that CUDA's lack of memory safety can lead to serious vulnerabilities. While fuzzing is effective for finding such bugs on CPUs, equivalent tools for GPUs are lacking due to architectural differences and lack of built-in error detection. In this paper, we propose CuFuzz, a novel compiler-runtime co-design solution to extend state-of-the-art CPU fuzzing tools to GPU programs. CuFuzz transforms GPU programs into CPU programs using compiler IR-level transformations to enable effective fuzz testing. To the best of our knowledge, CuFuzz is the first mechanism to bring fuzzing support to CUDA, addressing a critical gap in GPU security research. By leveraging CPU memory error detectors such as Address Sanitizer, CuFuzz aims to uncover memory safety bugs and related correctness vulnerabilities in CUDA code, enhancing the security and reliability of GPU-accelerated applications. To ensure high fuzzing throughput, we introduce two compiler-runtime co-optimizations tailored for GPU code: Partial Representative Execution (PREX) and Access-Index Preserving Pruning (AXIPrune), achieving average throughput improvements of 32x with PREX and an additional 33% gain with AXIPrune on top of PREX-optimized code. Together, these optimizations can yield up to a 224.31x speedup. In our fuzzing campaigns, CuFuzz uncovered 122 security vulnerabilities in widely used benchmarks.

</details>


### [69] [NADD: Amplifying Noise for Effective Diffusion-based Adversarial Purification](https://arxiv.org/abs/2601.01109)
*David D. Nguyen,The-Anh Ta,Yansong Gao,Alsharif Abuadbba*

Main category: cs.CR

TL;DR: 本文提出了一种改进的对抗净化方法，通过系统性地增加扩散过程中的噪声水平来提高鲁棒性，同时引入环邻近校正和随机采样技术来保持输入语义特征，在ImageNet上实现了44.23%的鲁棒准确率，并将推理时间大幅缩短至1.08秒/样本。


<details>
  <summary>Details</summary>
Motivation: 现有的基于扩散模型的对抗净化方法存在两个主要问题：1）由于使用低噪声设计以保留输入语义特征，导致鲁棒性有限；2）推理速度过慢，在实际防御场景中不实用。本文旨在解决这些限制，提高对抗净化的鲁棒性和效率。

Method: 1）在前向过程中引入高水平噪声，并提出环邻近校正技术，逐步消除对抗扰动同时紧密保留原始数据样本；2）提出新的随机采样方法，在反向扩散过程中引入额外噪声以稀释对抗扰动；3）系统性地放大整个扩散过程中的噪声水平，但不依赖梯度混淆技术。

Result: 在ImageNet数据集上，使用AutoAttack（ℓ∞=4/255）实现了44.23%的鲁棒准确率，比之前最佳工作提高了+2.07%。推理时间缩短至1.08秒/样本，比现有最先进方法提升了47倍，大幅提高了实际应用可行性。

Conclusion: 通过系统性地增加扩散过程中的噪声水平，结合环邻近校正和随机采样技术，本文提出的方法在保持输入语义特征的同时显著提高了对抗净化的鲁棒性，并将推理效率大幅提升，为实际防御场景提供了更实用的解决方案。

Abstract: The strategy of combining diffusion-based generative models with classifiers continues to demonstrate state-of-the-art performance on adversarial robustness benchmarks.
  Known as adversarial purification, this exploits a diffusion model's capability of identifying high density regions in data distributions to purify adversarial perturbations from inputs.
  However, existing diffusion-based purification defenses are impractically slow and limited in robustness due to the low levels of noise used in the diffusion process.
  This low noise design aims to preserve the semantic features of the original input, thereby minimizing utility loss for benign inputs.
  Our findings indicate that systematic amplification of noise throughout the diffusion process improves the robustness of adversarial purification.
  However, this approach presents a key challenge, as noise levels cannot be arbitrarily increased without risking distortion of the input.
  To address this key problem, we introduce high levels of noise during the forward process and propose the ring proximity correction to gradually eliminate adversarial perturbations whilst closely preserving the original data sample.
  As a second contribution, we propose a new stochastic sampling method which introduces additional noise during the reverse diffusion process to dilute adversarial perturbations.
  Without relying on gradient obfuscation, these contributions result in a new robustness accuracy record of 44.23% on ImageNet using AutoAttack ($\ell_{\infty}=4/255$), an improvement of +2.07% over the previous best work.
  Furthermore, our method reduces inference time to 1.08 seconds per sample on ImageNet, a $47\times$ improvement over the existing state-of-the-art approach, making it far more practical for real-world defensive scenarios.

</details>


### [70] [SecureCodeRL: Security-Aware Reinforcement Learning for Code Generation with Partial-Credit Rewards](https://arxiv.org/abs/2601.01184)
*Suryansh Singh Sijwali,Suman Saha*

Main category: cs.CR

TL;DR: SecureCodeRL：一个结合功能性和安全性的强化学习框架，用于生成更安全、更可靠的代码，通过部分信用奖励机制解决奖励稀疏问题。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型生成的代码虽然语法正确，但在需要精确输入输出行为的场景中经常无法通过测试，有时还会引入安全敏感模式，需要一种能同时优化功能正确性和安全性的方法。

Method: 提出SecureCodeRL强化学习管道，使用组合奖励R = αRfunc + βRsec，其中Rfunc采用部分信用功能奖励，为语法有效性、成功执行和产生输出分配中间分数，减少奖励稀疏性。

Result: 在APPS+的小规模测试集上，PPO+部分信用方法将语法有效性从45%（SFT）提升到60%，实现了唯一的非零测试成功率（5%至少通过一个测试），同时在Bandit静态分析中保持100%清洁。

Conclusion: SecureCodeRL通过结合功能性和安全性奖励的强化学习方法，有效提高了代码生成的质量和安全性，为解决代码生成中的奖励稀疏问题提供了可行方案。

Abstract: Large Language Models (LLMs) can generate plausible code, but in settings that require exact stdin/stdout behavior they frequently produce programs that compile yet fail tests, and in some cases they introduce security-sensitive patterns. This paper presents SecureCodeRL, a reinforcement learning (RL) pipeline for security-aware code generation that optimizes a combined reward R = αRfunc + \b{eta}Rsec. The key idea is a partial-credit functional reward that assigns intermediate scores for syntactic validity, successful execution, and producing output, reducing reward sparsity that otherwise stalls learning on competitive programming style tasks. I evaluate supervised fine-tuning (SFT) and PPO variants on a small held-out prompt set from APPS+ and observe that PPO with partial credit (using a continued-training variant) improves syntax validity from 45% (SFT) to 60% and achieves the only non-zero test success signal in this pilot evaluation (5% at-least-one-test-pass), while remaining 100% clean under Bandit static analysis. Although Bandit findings were absent in this small evaluation, the security term is integrated into training to discourage insecure shortcuts when they appear.

</details>


### [71] [Arca: A Lightweight Confidential Container Architecture for Cloud-Native Environments](https://arxiv.org/abs/2601.01214)
*Di Lu,Mengna Sun,Qingwen Zhang,Yujia Liu,Jia Zhang,Xuewen Dong,Yulong Shen,Jianfeng Ma*

Main category: cs.CR

TL;DR: Arca提出了一种轻量级机密容器框架，采用TEE-in-Container架构，将每个工作负载隔离在独立的硬件强制信任域中，同时将编排逻辑保留在TEE之外，显著减小可信计算基并提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有机密容器设计（如CoCo）将整个运行时封装在TEE内，导致可信计算基膨胀、引入冗余组件和跨层开销，违背了TEE的最小信任原则。

Method: 采用TEE-in-Container架构，将每个工作负载隔离在独立的硬件强制信任域中，保持编排逻辑在TEE之外，最小化层间依赖，将安全边界限制在每个容器级别。

Result: 在Intel SGX、Intel TDX和AMD SEV上实现Arca，实验结果显示其达到接近原生性能，在大多数基准测试中优于CoCo，显著减少的可信计算基提高了可验证性和对主机级攻击的抵御能力。

Conclusion: Arca证明了无需牺牲安全保障即可实现高效的容器管理和强大的运行时机密性，恢复了TEE的最小信任原则，同时保持高性能。

Abstract: Confidential containers protect cloud-native workloads using trusted execution environments (TEEs). However, existing Container-in-TEE designs (e.g., Confidential Containers (CoCo)) encapsulate the entire runtime within the TEE, inflating the trusted computing base (TCB) and introducing redundant components and cross-layer overhead. We present Arca, a lightweight confidential container framework based on a TEE-in-Container architecture that isolates each workload in an independent, hardware-enforced trust domain while keeping orchestration logic outside the TEE. This design minimizes inter-layer dependencies, confines compromise to per-container boundaries, and restores the TEE's minimal trust principle. We implemented Arca on Intel SGX, Intel TDX, and AMD SEV. Experimental results show that Arca achieves near-native performance and outperforms CoCo in most benchmarks, while the reduced TCB significantly improves verifiability and resilience against host-level compromise. Arca emonstrates that efficient container management and strong runtime confidentiality can be achieved without sacrificing security assurance.

</details>


### [72] [Compliance as a Trust Metric](https://arxiv.org/abs/2601.01287)
*Wenbo Wu,George Konstantinidis*

Main category: cs.CR

TL;DR: 本文提出了一种新颖的自动化合规引擎（ACE），将法规遵从性转化为量化的动态信任指标，通过多维度评估违规严重性来生成细粒度的合规分数，超越了传统的二元（通过/失败）审计方法。


<details>
  <summary>Details</summary>
Motivation: 当前信任与声誉管理系统（TRMSs）主要依赖主观用户评分或有限的QoS指标，缺乏客观基础；同时，GDPR、HIPAA等法规框架虽然提供了客观行为标准，但自动化合规审计仅限于粗糙的二元结果。需要将法规遵从性转化为量化的动态信任指标。

Method: 开发自动化合规引擎（ACE）：1）将法律和组织政策形式化为可验证的、以义务为中心的逻辑；2）持续审计系统事件日志以检测违规；3）通过多维度（数量、持续时间、广度、关键性）评估违规严重性，计算细粒度的动态合规分数。

Result: 在合成医院数据集上评估ACE，证明其能够准确检测复杂的HIPAA和GDPR违规，并生成比传统二元方法更具表达力的细致合规分数。

Conclusion: 该工作能够开发更透明、可问责和有弹性的Web信任与声誉管理系统，通过将法规遵从性转化为量化信任指标，为TRMSs提供了客观基础。

Abstract: Trust and Reputation Management Systems (TRMSs) are critical for the modern web, yet their reliance on subjective user ratings or narrow Quality of Service (QoS) metrics lacks objective grounding. Concurrently, while regulatory frameworks like GDPR and HIPAA provide objective behavioral standards, automated compliance auditing has been limited to coarse, binary (pass/fail) outcomes. This paper bridges this research gap by operationalizing regulatory compliance as a quantitative and dynamic trust metric through our novel automated compliance engine (ACE). ACE first formalizes legal and organizational policies into a verifiable, obligation-centric logic. It then continuously audits system event logs against this logic to detect violations. The core of our contribution is a quantitative model that assesses the severity of each violation along multiple dimensions, including its Volume, Duration, Breadth, and Criticality, to compute a fine-grained, evolving compliance score. We evaluate ACE on a synthetic hospital dataset, demonstrating its ability to accurately detect a range of complex HIPAA and GDPR violations and produce a nuanced score that is significantly more expressive than traditional binary approaches. This work enables the development of more transparent, accountable, and resilient TRMSs on the Web.

</details>


### [73] [Automated SBOM-Driven Vulnerability Triage for IoT Firmware: A Lightweight Pipeline for Risk Prioritization](https://arxiv.org/abs/2601.01308)
*Abdurrahman Tolay*

Main category: cs.CR

TL;DR: 该论文提出了一种轻量级自动化管道，用于从Linux物联网固件中提取文件系统、生成SBOM、映射漏洞，并应用多因素优先级评分模型来降低告警疲劳。


<details>
  <summary>Details</summary>
Motivation: 物联网设备固件通常包含过时的第三方库，且以二进制形式存在，使得漏洞管理困难。虽然SBOM标准已经成熟，但从原始固件中生成可操作情报仍然是一个手动且容易出错的过程。

Method: 开发了一个自动化管道，从Linux物联网固件中提取文件系统，生成全面的SBOM，将识别出的组件映射到已知漏洞，并应用多因素优先级评分模型（整合CVSS、EPSS和CISA KEV目录信号）。

Result: 论文描述了系统架构、嵌入式Linux的规范化挑战，以及旨在减少告警疲劳的评分方法。研究概述了使用公共供应商固件数据集验证提取成功率和优先级效能的评估策略。

Conclusion: 该方法通过计算每个发现的本地化风险分数，强调优先级排序，与产生大量无上下文告警的传统扫描器不同，为固件安全研究提供了可复现框架。

Abstract: The proliferation of Internet of Things (IoT) devices has introduced significant security challenges, primarily due to the opacity of firmware components and the complexity of supply chain dependencies. IoT firmware frequently relies on outdated, third-party libraries embedded within monolithic binary blobs, making vulnerability management difficult. While Software Bill of Materials (SBOM) standards have matured, generating actionable intelligence from raw firmware dumps remains a manual and error-prone process. This paper presents a lightweight, automated pipeline designed to extract file systems from Linux-based IoT firmware, generate a comprehensive SBOM, map identified components to known vulnerabilities, and apply a multi-factor triage scoring model. The proposed system focuses on risk prioritization by integrating signals from the Common Vulnerability Scoring System (CVSS), Exploit Prediction Scoring System (EPSS), and the CISA Known Exploited Vulnerabilities (KEV) catalog. Unlike conventional scanners that produce high volumes of uncontextualized alerts, this approach emphasizes triage by calculating a localized risk score for each finding. We describe the architecture, the normalization challenges of embedded Linux, and a scoring methodology intended to reduce alert fatigue. The study outlines a planned evaluation strategy to validate the extraction success rate and triage efficacy using a dataset of public vendor firmware, offering a reproducibility framework for future research in firmware security.

</details>


### [74] [Bithoven: Formal Safety for Expressive Bitcoin Smart Contracts](https://arxiv.org/abs/2601.01436)
*Hyunhum Cho,Ik Rae Jeong*

Main category: cs.CR

TL;DR: Bithoven：一种为比特币UTXO架构设计的高级语言，通过类型检查、资源活性分析和控制流分析，在保持代码效率的同时消除共识和逻辑缺陷。


<details>
  <summary>Details</summary>
Motivation: 比特币UTXO架构的严格安全模型导致开发者体验差，需要手动栈操作，容易产生签名可塑性、不可花费状态和无约束执行路径等严重漏洞。现有标准如Miniscript仅提供策略验证抽象，无法满足复杂合约的完整命令式逻辑需求。

Method: 开发Bithoven高级语言，集成严格类型检查器、资源活性分析器和语义控制流分析器，在部署前消除共识和逻辑缺陷。

Result: Bithoven能够以与手工优化代码相当的效率编译为比特币脚本，证明类型安全、开发者友好的抽象在比特币区块链严格的字节大小限制下是可行的。

Conclusion: Bithoven在表达能力和形式化安全性之间架起了桥梁，以适度成本实现了高级别安全性，为比特币智能合约开发提供了更安全、更易用的解决方案。

Abstract: The rigorous security model of Bitcoin's UTXO architecture often comes at the cost of developer usability, forcing a reliance on manual stack manipulation that leads to critical financial vulnerabilities like signature malleability, unspendable states and unconstrained execution paths. Industry standards such as Miniscript provide necessary abstractions for policy verification but do not model the full imperative logic required for complex contracts, leaving gaps in state management and resource liveness. This paper introduces Bithoven, a high-level language designed to bridge the gap between expressiveness and formal safety. By integrating a strict type checker and a resource liveness analyzer with a semantic control-flow analyzer, Bithoven eliminates major categories of consensus and logic defects defined in our fault model prior to deployment. Our results indicate that this safety comes at modest cost: Bithoven compiles to Bitcoin Script with efficiency comparable to hand-optimized code, demonstrating that type-safe, developer-friendly abstractions are viable even within the strict byte-size constraints of the Bitcoin blockchain.

</details>


### [75] [Security in the Era of Perceptive Networks: A Comprehensive Taxonomic Framework for Integrated Sensing and Communication Security](https://arxiv.org/abs/2601.01455)
*Chandra Thapa,Surya Nepal*

Main category: cs.CR

TL;DR: 本文对6G集成感知与通信(ISAC)安全领域进行了系统性综述，提出了一个全面的分类框架，涵盖威胁传播、漏洞分析、防御机制、安全性能权衡、关键基础设施需求以及新兴问题等多个维度。


<details>
  <summary>Details</summary>
Motivation: 虽然先前的研究已经建立了ISAC安全的基础要素，讨论了感知导向的安全模型，并提出了分层防御策略，但缺乏一个能够整合这些研究、覆盖整个ISAC安全领域的综合性分类框架。本文旨在填补这一空白，为研究人员和政策制定者提供系统性的参考。

Method: 通过多维度正交分析的方法，系统性地回顾ISAC安全领域，包括：威胁分类与传播方法；设计、物理、计算和架构层面的漏洞分析；按部署层分类的防御机制；安全性能权衡的理论界限；关键基础设施的特定安全需求；以及量子韧性、AI加固和隐私保护等新兴问题。

Result: 提出了一个全面的ISAC安全分类框架，引入了新的分类方案，揭示了威胁与防御之间的隐藏关系，并通过结构化分析识别了关键研究空白。该框架超越了以往主要关注视觉感知的框架，整合了多个维度。

Conclusion: 本文提供的详细分类框架为开发安全ISAC系统的研究人员和制定安全标准的政策制定者提供了有价值的参考，有助于推动6G集成感知与通信安全领域的系统化发展。

Abstract: Integrated Sensing and Communication (ISAC) represents a significant shift in the 6G landscape, where wireless networks both sense the environment and communicate. While prior comprehensive surveys have established foundational elements of ISAC security, discussed perception-focused security models, and proposed layered defense strategies, this paper synthesizes these studies into a comprehensive taxonomic framework that covers the whole ISAC security domain. This paper provides a systematic and thorough review of ISAC security across multiple orthogonal dimensions. These include threat taxonomy and propagation methods; vulnerability analysis at design, physical, computational, and architectural levels; defense mechanisms categorized by deployment layer; security-performance trade-offs with theoretical bounds; sector-specific security demands for critical infrastructure; and emerging issues such as quantum resilience, AI-hardening, and privacy preservation. Unlike previous frameworks that primarily focus on vision, this review combines these dimensions, introduces new classification schemes that reveal hidden relationships between threats and defenses, and identifies key research gaps through structured analysis. This detailed taxonomy offers a valuable reference for researchers developing secure ISAC systems and policymakers establishing security standards.

</details>


### [76] [OpenRT: An Open-Source Red Teaming Framework for Multimodal LLMs](https://arxiv.org/abs/2601.01592)
*Xin Wang,Yunhao Chen,Juncheng Li,Yixu Wang,Yang Yao,Tianle Gu,Jie Li,Yan Teng,Xingjun Ma,Yingchun Wang,Xia Hu*

Main category: cs.CR

TL;DR: OpenRT是一个统一、模块化、高吞吐量的红队测试框架，用于全面评估多模态大语言模型的安全漏洞。该框架通过五个关键维度的模块化分离，集成了37种攻击方法，在20个先进模型上揭示了高达49.14%的平均攻击成功率，发现前沿模型无法跨攻击范式泛化，推理模型也不具备更强的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型在关键应用中的快速集成受到持续安全漏洞的阻碍。现有的红队测试基准往往碎片化，仅限于单轮文本交互，缺乏系统评估所需的可扩展性。需要一种统一、模块化的框架来全面评估MLLM安全性。

Method: OpenRT引入了一个对抗内核，通过五个关键维度的模块化分离实现了范式转变：模型集成、数据集管理、攻击策略、判断方法和评估指标。通过标准化攻击接口，将对抗逻辑与高吞吐量异步运行时解耦，支持跨不同模型的系统扩展。框架集成了37种不同的攻击方法，包括白盒梯度、多模态扰动和复杂的多智能体进化策略。

Result: 在20个先进模型（包括GPT-5.2、Claude 4.5和Gemini 3 Pro）上的广泛实证研究暴露了关键安全漏洞：即使前沿模型也无法跨攻击范式泛化，领先模型的平均攻击成功率高达49.14%。研究发现推理模型并不固有地具备对抗复杂多轮越狱的更强鲁棒性。

Conclusion: 通过开源OpenRT，提供了一个可持续、可扩展且持续维护的基础设施，加速了AI安全的发展和标准化。该框架为系统评估多模态大语言模型安全性提供了统一解决方案，揭示了当前模型在安全防护方面的显著不足。

Abstract: The rapid integration of Multimodal Large Language Models (MLLMs) into critical applications is increasingly hindered by persistent safety vulnerabilities. However, existing red-teaming benchmarks are often fragmented, limited to single-turn text interactions, and lack the scalability required for systematic evaluation. To address this, we introduce OpenRT, a unified, modular, and high-throughput red-teaming framework designed for comprehensive MLLM safety evaluation. At its core, OpenRT architects a paradigm shift in automated red-teaming by introducing an adversarial kernel that enables modular separation across five critical dimensions: model integration, dataset management, attack strategies, judging methods, and evaluation metrics. By standardizing attack interfaces, it decouples adversarial logic from a high-throughput asynchronous runtime, enabling systematic scaling across diverse models. Our framework integrates 37 diverse attack methodologies, spanning white-box gradients, multi-modal perturbations, and sophisticated multi-agent evolutionary strategies. Through an extensive empirical study on 20 advanced models (including GPT-5.2, Claude 4.5, and Gemini 3 Pro), we expose critical safety gaps: even frontier models fail to generalize across attack paradigms, with leading models exhibiting average Attack Success Rates as high as 49.14%. Notably, our findings reveal that reasoning models do not inherently possess superior robustness against complex, multi-turn jailbreaks. By open-sourcing OpenRT, we provide a sustainable, extensible, and continuously maintained infrastructure that accelerates the development and standardization of AI safety.

</details>


### [77] [MOZAIK: A Privacy-Preserving Analytics Platform for IoT Data Using MPC and FHE](https://arxiv.org/abs/2601.02245)
*Michiel Van Kenhove,Erik Pohle,Leonard Schild,Martin Zbudila,Merlijn Sebrechts,Filip De Turck,Bruno Volckaert,Aysajan Abidin*

Main category: cs.CR

TL;DR: MOZAIK是一个面向IoT到云场景的端到端隐私保护机密数据存储和分布式处理架构，使用加密数据计算技术确保数据在传输、存储和处理过程中始终保持加密状态。


<details>
  <summary>Details</summary>
Motivation: 物联网系统产生大量敏感数据，云辅助IoT解决方案虽然提供存储和计算资源，但传统信任方法无法充分缓解安全和隐私风险，需要新的隐私保护方案。

Method: 提出MOZAIK架构，采用加密数据计算技术，包括安全多方计算和全同态加密两种方法，确保数据全生命周期加密，并提供概念验证实现和性能评估。

Result: 评估结果表明MOZAIK系统具有可行性，展示了端到端隐私保护系统相对于普通明文替代方案的成本，所有组件已作为开源软件发布。

Conclusion: MOZAIK为IoT到云场景提供了实用的隐私保护解决方案，通过加密数据计算技术确保数据安全，开源发布旨在推动安全和隐私保护数据处理实践的发展。

Abstract: The rapid increase of Internet of Things (IoT) systems across several domains has led to the generation of vast volumes of sensitive data, presenting significant challenges in terms of storage and data analytics. Cloud-assisted IoT solutions offer storage, scalability, and computational resources, but introduce new security and privacy risks that conventional trust-based approaches fail to adequately mitigate. To address these challenges, this paper presents MOZAIK, a novel end-to-end privacy-preserving confidential data storage and distributed processing architecture tailored for IoT-to-cloud scenarios. MOZAIK ensures that data remains encrypted throughout its lifecycle, including during transmission, storage, and processing. This is achieved by employing a cryptographic privacy-enhancing technology known as computing on encrypted data (COED). Two distinct COED techniques are explored, specifically secure multi-party computation (MPC) and fully homomorphic encryption (FHE). The paper includes a comprehensive analysis of the MOZAIK architecture, including a proof-of-concept implementation and performance evaluations. The evaluation results demonstrate the feasibility of the MOZAIK system and indicate the cost of an end-to-end privacy-preserving system compared to regular plaintext alternatives. All components of the MOZAIK platform are released as open-source software alongside this publication, with the aim of advancing secure and privacy-preserving data processing practices.

</details>


### [78] [Improved Accuracy for Private Continual Cardinality Estimation in Fully Dynamic Streams via Matrix Factorization](https://arxiv.org/abs/2601.02257)
*Joel Daniel Andersson,Palak Jain,Satchit Sivakumar*

Main category: cs.CR

TL;DR: 论文提出了一种改进差分隐私流式统计的方法，通过分析差异流的ℓp敏感度向量特性，优化了动态持续观察模型中的基数估计精度。


<details>
  <summary>Details</summary>
Motivation: 在完全动态持续观察模型中，每个时间步可能有多个更新（插入和删除），现有方法将基数估计问题简化为差异流的持续计数，但原始流的变化可能导致差异流多次变化，这给应用私有持续计数算法获得最优误差界限带来了挑战。

Method: 研究差异流相关的ℓp敏感度向量并分离其特性，通过紧密分析计数矩阵的已知分解机制，利用具有特定特性的敏感度向量集的最先进分解方法。

Result: 改进了计数不同元素、估计度直方图和估计三角形计数（在略微放宽的隐私模型下）的误差界限，为流式设置中的私有持续基数估计提供了通用方法。

Conclusion: 通过分析差异流的敏感度向量特性，该框架显著提高了流式差分隐私统计的准确性，在大范围参数下为基数估计问题提供了实质性改进。

Abstract: We study differentially-private statistics in the fully dynamic continual observation model, where many updates can arrive at each time step and updates to a stream can involve both insertions and deletions of an item. Earlier work (e.g., Jain et al., NeurIPS 2023 for counting distinct elements; Raskhodnikova & Steiner, PODS 2025 for triangle counting with edge updates) reduced the respective cardinality estimation problem to continual counting on the difference stream associated with the true function values on the input stream. In such reductions, a change in the original stream can cause many changes in the difference stream, this poses a challenge for applying private continual counting algorithms to obtain optimal error bounds. We improve the accuracy of several such reductions by studying the associated $\ell_p$-sensitivity vectors of the resulting difference streams and isolating their properties.
  We demonstrate that our framework gives improved bounds for counting distinct elements, estimating degree histograms, and estimating triangle counts (under a slightly relaxed privacy model), thus offering a general approach to private continual cardinality estimation in streaming settings. Our improved accuracy stems from tight analysis of known factorization mechanisms for the counting matrix in this setting; the key technical challenge is arguing that one can use state-of-the-art factorizations for sensitivity vector sets with the properties we isolate. Empirically and analytically, we demonstrate that our improved error bounds offer a substantial improvement in accuracy for cardinality estimation problems over a large range of parameters.

</details>
