<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 21]
- [cs.CR](#cs.CR) [Total: 21]
- [cs.AI](#cs.AI) [Total: 24]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [Evolution of Kernels: Automated RISC-V Kernel Optimization with Large Language Models](https://arxiv.org/abs/2509.14265)
*Siyuan Chen,Zhichao Lu,Qingfu Zhang*

Main category: cs.SE

TL;DR: EoK是一个基于LLM的进化程序搜索框架，通过从成熟内核库的开发历史中挖掘可重用优化思想，为RISC-V等参考稀缺领域自动化内核设计，实现了比人类专家更好的性能。


<details>
  <summary>Details</summary>
Motivation: 解决在新兴硬件平台（如RISC-V）上由于参考材料稀缺而导致自动内核设计困难的问题，传统LLM方法在CUDA等成熟领域有效但在参考稀缺领域效果未经验证。

Method: 提出EoK框架：1）从成熟内核库开发历史中挖掘可重用优化思想（通用设计原则+可操作思路）；2）使用RAG技术增强RISC-V特定上下文；3）基于历史有效技术指导并行LLM探索的进化程序搜索。

Result: 在80个内核设计任务评估中，EoK实现了中位数1.27倍加速，在所有任务上都超越了人类专家，比之前基于LLM的自动内核设计方法提高了20%。

Conclusion: EoK证明了将人类经验融入新兴领域的可行性，凸显了基于LLM的自动内核优化的巨大潜力，为参考稀缺领域的自动化内核设计提供了有效解决方案。

Abstract: Automated kernel design is critical for overcoming software ecosystem
barriers in emerging hardware platforms like RISC-V. While large language
models (LLMs) have shown promise for automated kernel optimization,
demonstrating success in CUDA domains with comprehensive technical documents
and mature codebases, their effectiveness remains unproven for reference-scarce
domains like RISC-V. We present Evolution of Kernels (EoK), a novel LLM-based
evolutionary program search framework that automates kernel design for domains
with limited reference material. EoK mitigates reference scarcity by mining and
formalizing reusable optimization ideas (general design principles + actionable
thoughts) from established kernel libraries' development histories; it then
guides parallel LLM explorations using these ideas, enriched via
Retrieval-Augmented Generation (RAG) with RISC-V-specific context, prioritizing
historically effective techniques. Empirically, EoK achieves a median 1.27x
speedup, surpassing human experts on all 80 evaluated kernel design tasks and
improving upon prior LLM-based automated kernel design methods by 20%. These
results underscore the viability of incorporating human experience into
emerging domains and highlight the immense potential of LLM-based automated
kernel optimization.

</details>


### [2] [Automated and Context-Aware Code Documentation Leveraging Advanced LLMs](https://arxiv.org/abs/2509.14273)
*Swapnil Sharma Sarker,Tanzina Taher Ifty*

Main category: cs.SE

TL;DR: 这篇论文研究了使用公开大语言模型自动生成Javadoc文档，构建了一个具有现代Java特性的上下文敏感数据集，并评测五个开源LLM的表现。


<details>
  <summary>Details</summary>
Motivation: 自动化代码文档生成对软件维护至关重要，但现有方法主要集中于代码摘要生成，缺乏专门的Javadoc模板生成方法，且缺少包含现代语言特性和上下文信息的数据集。

Method: 构建了一个新的上下文敏感Javadoc生成数据集，包含现代Java代码库的结构和语义信息。使用五个开源LLM（LLaMA-3.1、Gemma-2、Phi-3、Mistral、Qwen-2.5）进行零样本、少样本和微调测试。

Result: LLaMA 3.1表现最为稳定且效果最好，是实际应用中生成自动Javadoc文档的可靠选择，可以作为专有系统的可行替代方案。

Conclusion: 这项研究填补了自动Javadoc生成领域的空白，为使用公开大语言模型进行模板化代码文档生成提供了可靠的解决方案。

Abstract: Code documentation is essential to improve software maintainability and
comprehension. The tedious nature of manual code documentation has led to much
research on automated documentation generation. Existing automated approaches
primarily focused on code summarization, leaving a gap in template-based
documentation generation (e.g., Javadoc), particularly with publicly available
Large Language Models (LLMs). Furthermore, progress in this area has been
hindered by the lack of a Javadoc-specific dataset that incorporates modern
language features, provides broad framework/library coverage, and includes
necessary contextual information. This study aims to address these gaps by
developing a tailored dataset and assessing the capabilities of publicly
available LLMs for context-aware, template-based Javadoc generation. In this
work, we present a novel, context-aware dataset for Javadoc generation that
includes critical structural and semantic information from modern Java
codebases. We evaluate five open-source LLMs (including LLaMA-3.1, Gemma-2,
Phi-3, Mistral, Qwen-2.5) using zero-shot, few-shot, and fine-tuned setups and
provide a comparative analysis of their performance. Our results demonstrate
that LLaMA 3.1 performs consistently well and is a reliable candidate for
practical, automated Javadoc generation, offering a viable alternative to
proprietary systems.

</details>


### [3] [Towards Robust Agentic CUDA Kernel Benchmarking, Verification, and Optimization](https://arxiv.org/abs/2509.14279)
*Robert Tjarko Lange,Qi Sun,Aaditya Prasad,Maxence Faldor,Yujin Tang,David Ha*

Main category: cs.SE

TL;DR: 提出了一个新的robust-kbench测试框架和自动化框架，用于优化和验证CUDA内核实现，解决现有方法在低级代码优化和测试多样性上的不足。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型在软件工程任务中展现了良好的计算扩展能力，但对低级CUDA内核优化关注不够，同时现有测试框架存在可洗洗漏洞和测试条件不足的问题。

Method: 开发了一个自动化框架，包括将PyTorch代码转换为CUDA内核，然后通过一种新的进化元生成程序迭代优化运行时间，使用LLM基于的验证器确保正确性和高效过滤。

Result: 在robust-kbench上评估，该方法生成的CUDA内核在实际应用中超过了torch实现，包括前向和向后传播，能够融合操作并部署多种运行时优化策略。验证器流程准确分类错误内核，提高了硬件验证效率。

Conclusion: 该研究提供了一个系统性的解决方案，通过自动化框架和严格的测试环境，有效地提升了CUDA内核的性能优化和正确性验证能力。

Abstract: Recent advances in large language models (LLMs) demonstrate their
effectiveness in scaling test-time compute for software engineering tasks.
However, these approaches often focus on high-level solutions, with limited
attention to optimizing low-level CUDA kernel implementations. Additionally,
existing kernel generation benchmarks suffer from exploitable loopholes and
insufficient diversity in testing conditions, hindering true generalization
assessment. To address these limitations, we introduce robust-kbench, a new
benchmark for rigorous evaluation of kernel performance and correctness across
varied scenarios. Furthermore, we present a comprehensive agentic framework
that automates CUDA kernel discovery, verification, and optimization. This
pipeline enables frontier LLMs to translate torch code to CUDA kernels and
iteratively improve their runtime within our robust evaluation setting. Our
sequential workflow first translates PyTorch code into equivalent CUDA kernels.
It then optimizes their runtime using a novel evolutionary meta-generation
procedure tailored to the CUDA ecosystem, guided by LLM-based verifiers for
correctness and efficient filtering. Evaluated on robust-kbench, our approach
produces CUDA kernels outperforming torch implementations for practical
applications, including forward and backward passes. It can fuse operations and
deploy various runtime optimization strategies. The verifier workflow
accurately classifies incorrect kernels, enhancing hardware verification
efficiency.

</details>


### [4] [SCoGen: Scenario-Centric Graph-Based Synthesis of Real-World Code Problems](https://arxiv.org/abs/2509.14281)
*Xifeng Yao,Dongyu Lang,Wu Zhang,Xintong Guo,Huarui Xie,Yinhao Ni,Ping Liu,Guang Shen,Yi Bai,Dandan Tu,Changzheng Zhang*

Main category: cs.SE

TL;DR: 通过从真实编程数据集提取领域知识、领域技能和编程技能，构建场景中心图来生成仿真的代码问题，解决代码大语言模型训练数据稀缺问题。


<details>
  <summary>Details</summary>
Motivation: 代码大语言模型进步受限于真实世界编程问题的稀缺，需要创建仿真的编程问题来提升模型能力。

Method: 从Stack Overflow和Kaggle等真实数据集提取领域知识、技能和应用场景，构建场景中心图来联系各要素，通过图里的采样策略控制代码问题的复杂性和多样性。

Result: 在多样化的真实世界测试集上，该方法在不同规模和功能的开源大语言模型上都较现有方法更优。

Conclusion: 通过系统化的方法生成仿真编程问题，能够有效提升代码大语言模型的性能和应用能力。

Abstract: Significant advancements have been made in the capabilities of code large
language models, leading to their rapid adoption and application across a wide
range of domains. However, their further advancements are often constrained by
the scarcity of real-world coding problems. To bridge this gap, we propose a
novel framework for synthesizing code problems that emulate authentic
real-world scenarios. This framework systematically integrates domain
knowledge, domain skills, and coding skills, all of which are meticulously
extracted from real-world programming-related datasets, including Stack
Overflow and Kaggle. The extracted elements serve as the foundational building
blocks for constructing code problems. To align the generated problems with
practical applications, application scenarios are also mined from the
aforementioned datasets. These scenarios are then utilized to construct a
scenario-centric graph that interconnects domain knowledge, domain skills, and
coding skills. Based on this structured representation, a sampling strategy on
the graph is designed, which effectively controls the generation of a code
problem with complexity and diversity, reflects real-world challenges.
Experimental results demonstrate that the proposed method consistently achieves
superior performance over state-of-the-art open-source large language models of
varying sizes and functionalities, including both coders and general-purpose
models, across a diverse set of real-world benchmarks.

</details>


### [5] [Monitoring Machine Learning Systems: A Multivocal Literature Review](https://arxiv.org/abs/2509.14294)
*Hira Naveed,Scott Barnett,Chetan Arora,John Grundy,Hourieh Khalajzadeh,Omar Haggag*

Main category: cs.SE

TL;DR: 对136篇论文进行的多源文献综述，全面概述了机器学习监控文献，分析了监控动机、方法、工具和局限性，为学术界和从业者提供实践指导和未来研究方向


<details>
  <summary>Details</summary>
Motivation: 动态生产环境中机器学习系统面临运行时问题（如数据模式变化），需要监控来早期检测和缓解性能下降，维持用户信任并防止组织不良后果

Method: 采用Garousi指南的多源文献综述方法，分析136篇论文，从四个关键领域进行研究：动机目标背景、监控方面技术指标工具、贡献效益、当前局限性

Result: 系统分析了机器学习监控的各个方面，识别了正式文献与灰色文献之间的相似性和脱节，总结了现有监控实践和差距

Conclusion: 研究为学术界和从业者提供了选择合适解决方案的指导，突出了当前方法的局限性，并为未来研究和工具开发提供了方向

Abstract: Context: Dynamic production environments make it challenging to maintain
reliable machine learning (ML) systems. Runtime issues, such as changes in data
patterns or operating contexts, that degrade model performance are a common
occurrence in production settings. Monitoring enables early detection and
mitigation of these runtime issues, helping maintain users' trust and prevent
unwanted consequences for organizations. Aim: This study aims to provide a
comprehensive overview of the ML monitoring literature. Method: We conducted a
multivocal literature review (MLR) following the well established guidelines by
Garousi to investigate various aspects of ML monitoring approaches in 136
papers. Results: We analyzed selected studies based on four key areas: (1) the
motivations, goals, and context; (2) the monitored aspects, specific
techniques, metrics, and tools; (3) the contributions and benefits; and (4) the
current limitations. We also discuss several insights found in the studies,
their implications, and recommendations for future research and practice.
Conclusion: Our MLR identifies and summarizes ML monitoring practices and gaps,
emphasizing similarities and disconnects between formal and gray literature.
Our study is valuable for both academics and practitioners, as it helps select
appropriate solutions, highlights limitations in current approaches, and
provides future directions for research and tool development.

</details>


### [6] [On the Illusion of Success: An Empirical Study of Build Reruns and Silent Failures in Industrial CI](https://arxiv.org/abs/2509.14347)
*Henri Aïdasso,Francis Bordeleau,Ali Tizghadam*

Main category: cs.SE

TL;DR: 首次通过重新运行成功任务来研究CI中的沉默失败问题，发现11%成功任务被重运行，主要原因包括测试错误、缓存问题和脚本语言问题


<details>
  <summary>Details</summary>
Motivation: 直面开发者遇到的CI建构中非确定性问题，特别是沉默失败（任务标记成功但实际失败）导致代码缺陷漏测进入生产环境

Method: 对142,387个任务和81个工业项目进行分析，使用32个独立变量的混合效应模型（AUC 85%），并研究92个公开问题来分类沉默失败

Result: 11%成功任务被重运行，35%在24小时后重运行；识别出与重运行相关的关键因素，包括测试、静态分析、Shell脚本等；总结11个沉默失败类别

Conclusion: 研究提供了关于CI沉默失败情况和原因的有价值见解，帮助团队提高认知，并提出了改善CI可靠性的解决方案

Abstract: Reliability of build outcomes is a cornerstone of effective Continuous
Integration (CI). Yet in practice, developers often struggle with
non-deterministic issues in the code or CI infrastructure, which undermine
trust in build results. When faced with such unexpected outcomes, developers
often repeatedly rerun jobs hoping for true success, but this practice is known
to increase CI costs and reduce productivity. While recent studies have focused
on intermittent job failures, no prior work has investigated silent failures,
where build jobs are marked as successful but fail to complete all or part of
their tasks. Such silent failures often go unnoticed, creating an illusion of
success with detrimental consequences such as bugs escaping into production.
This paper presents the first empirical study of silent failures through the
practice of rerunning successful jobs. An analysis of 142,387 jobs across 81
industrial projects shows that 11% of successful jobs are rerun, with 35% of
these reruns occurring after more than 24 hours. Using mixed-effects models on
32 independent variables (AUC of 85%), we identified key factors associated
with reruns of successful jobs, notably testing and static analysis tasks,
scripting languages like Shell, and developers prior rerun tendencies. A
further analysis of 92 public issues revealed 11 categories of silent failures
aligning with these factors, the most frequent being artifact operation errors,
caching errors, and ignored exit codes. Overall, our findings provide valuable
insights into the circumstances and causes of silent failures to raise
awareness among teams, and present solutions to improve CI reliability.

</details>


### [7] [CodeLSI: Leveraging Foundation Models for Automated Code Generation with Low-Rank Optimization and Domain-Specific Instruction Tuning](https://arxiv.org/abs/2509.14373)
*Huy Le,Phong Nguyen,Hao Do,Tuan Nguyen,Thien Pham,Anh Nguyen-Duc,Tho Quan*

Main category: cs.SE

TL;DR: CodeLSI是一个结合低秩优化和领域特定指令调优的框架，用于生成高质量、领域特定的代码，无需依赖外部API，在资源效率和代码质量方面优于基线模型。


<details>
  <summary>Details</summary>
Motivation: 解决基于基础模型的自动化代码生成在领域特异性、成本效益和安全性方面的挑战，特别是在依赖第三方API时的问题。

Method: 应用低秩适应技术降低模型预训练和微调的计算成本，采用领域特定指令调优使代码生成与组织需求对齐，在真实JavaScript编码任务上进行实现和测试。

Result: 实验评估显示CodeLSI能生成高质量、上下文感知的代码，在相关性、准确性和领域适配性方面优于基线模型，低秩优化显著降低了资源需求。

Conclusion: CodeLSI证明低秩优化与领域特定调优相结合可以增强基础模型在自动化代码生成中的实用性和性能，提供了安全、经济高效的替代方案。

Abstract: Context: Automated code generation using Foundation Models (FMs) offers
promising solutions for enhancing software development efficiency. However,
challenges remain in ensuring domain specificity, cost-effectiveness, and
security - especially when relying on third-party APIs. This paper introduces
CodeLSI, a framework that combines low-rank optimization and domain-specific
instruction tuning to address these challenges.
  Objectives: The aim of this study is to develop and evaluate CodeLSI, a novel
approach for generating high-quality code tailored to specific domains, using
FMs fine-tuned on company infrastructure without dependence on external APIs.
  Methods: CodeLSI applies low-rank adaptation techniques to reduce the
computational cost of model pre-training and fine-tuning. Domain-specific
instruction tuning is employed to align code generation with organizational
needs. We implemented and tested the framework on real-world JavaScript coding
tasks using datasets drawn from internal software projects.
  Results: Experimental evaluations show that CodeLSI produces high-quality,
context aware code. It outperforms baseline models in terms of relevance,
accuracy, and domain fit. The use of low-rank optimization significantly
reduced resource requirements, enabling scalable training on company-owned
infrastructure.
  Conclusion: CodeLSI demonstrates that combining low-rank optimization with
domain specific tuning can enhance the practicality and performance of FMs for
automated code generation. This approach provides a secure, cost-efficient
alternative to commercial API based solutions and supports faster, more
targeted innovation in software development.

</details>


### [8] [A Taxonomy of Prompt Defects in LLM Systems](https://arxiv.org/abs/2509.14404)
*Haoye Tian,Chong Wang,BoYang Yang,Lyuye Zhang,Yang Liu*

Main category: cs.SE

TL;DR: 本文首次系统性地调查和分类了提示缺陷，提出了包含6个维度的缺陷分类体系，并针对每种缺陷类型提供了缓解策略和工程方法。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型(LLMs)已成为现代软件的关键组件，但提示设计仍主要依赖经验，小错误可能导致不可靠、不安全或低效的行为，需要系统化的缺陷分析方法。

Method: 通过系统调查和分类法，将提示缺陷组织为6个维度：规范和意图、输入和内容、结构和格式、上下文和内存、性能和效率、可维护性和工程化，每个维度细分为具体子类型并提供实例分析。

Result: 建立了全面的提示缺陷分类体系，提供了具体的缺陷示例、根本原因分析和缓解策略，包括提示工程模式、自动化防护、测试框架和评估方法。

Conclusion: 需要以工程为导向的严谨方法来确保LLM驱动系统的可靠性，提出了开放研究挑战并呼吁建立设计可靠性的工程方法论。

Abstract: Large Language Models (LLMs) have become key components of modern software,
with prompts acting as their de-facto programming interface. However, prompt
design remains largely empirical and small mistakes can cascade into
unreliable, insecure, or inefficient behavior. This paper presents the first
systematic survey and taxonomy of prompt defects, recurring ways that prompts
fail to elicit their intended behavior from LLMs. We organize defects along six
dimensions: (1) Specification and Intent, (2) Input and Content, (3) Structure
and Formatting, (4) Context and Memory, (5) Performance and Efficiency, and (6)
Maintainability and Engineering. Each dimension is refined into fine-grained
subtypes, illustrated with concrete examples and root cause analysis. Grounded
in software engineering principles, we show how these defects surface in real
development workflows and examine their downstream effects. For every subtype,
we distill mitigation strategies that span emerging prompt engineering
patterns, automated guardrails, testing harnesses, and evaluation frameworks.
We then summarize these strategies in a master taxonomy that links defect,
impact, and remedy. We conclude with open research challenges and a call for
rigorous engineering-oriented methodologies to ensure that LLM-driven systems
are dependable by design.

</details>


### [9] [An LLM-based multi-agent framework for agile effort estimation](https://arxiv.org/abs/2509.14483)
*Thanh-Long Bui,Hoa Khanh Dam,Rashina Hoda*

Main category: cs.SE

TL;DR: 基于大语言模型的多代理框架，能够与人类开发者协作进行效果估算，提高估算准确性和可解释性


<details>
  <summary>Details</summary>
Motivation: 解决现有潜在估算方法主观性强、准确性不高、无法提供解释以及缺乏与人类协作能力的问题

Method: 提出了一种新题的基于LLM的多代理框架，能够产生估算结果并与人类开发者协调沟通、讨论，达成共识

Result: 在真实数据集上评估，该方法在大多数情况下在所有评估指标上都超过了最先进技术，人类实验研究显示开发者与该代理协作体验极佳

Conclusion: 该LLM多代理框架有效解决了潜在估算中的主观性、不准确和不可解释性问题，为效果估算提供了一种可信赖且可协作的新方法

Abstract: Effort estimation is a crucial activity in agile software development, where
teams collaboratively review, discuss, and estimate the effort required to
complete user stories in a product backlog. Current practices in agile effort
estimation heavily rely on subjective assessments, leading to inaccuracies and
inconsistencies in the estimates. While recent machine learning-based methods
show promising accuracy, they cannot explain or justify their estimates and
lack the capability to interact with human team members. Our paper fills this
significant gap by leveraging the powerful capabilities of Large Language
Models (LLMs). We propose a novel LLM-based multi-agent framework for agile
estimation that not only can produce estimates, but also can coordinate,
communicate and discuss with human developers and other agents to reach a
consensus. Evaluation results on a real-life dataset show that our approach
outperforms state-of-the-art techniques across all evaluation metrics in the
majority of the cases. Our human study with software development practitioners
also demonstrates an overwhelmingly positive experience in collaborating with
our agents in agile effort estimation.

</details>


### [10] [Automating Modelica Module Generation Using Large Language Models: A Case Study on Building Control Description Language](https://arxiv.org/abs/2509.14623)
*Hanlong Wan,Xing Lu,Yan Chen,Karthik Devaprasad,Laura Hinkle*

Main category: cs.SE

TL;DR: 使用大语言模型自动生成Modelica控制模块，GPT 4o零检查失败，Claude Sonnet 4在精心设计提示下基础逻辑块达到100%成功率，控制模块83%成功率，开发时间节省40-60%。


<details>
  <summary>Details</summary>
Motivation: Modelica式基语言开发控制模块需要专业知识且劳动密集，需要自动化生成方案来提高效率。

Method: 开发结构化工作流，结合标准化提示架构、库知识基础、OpenModelica自动编译和人在环中评估，在4个基础逻辑任务和5个控制模块上进行实验。

Result: Claude Sonnet 4在细心设计提示下基础逻辑块达到完全成功，控制模块83%成功率，失败输出需中等人工修复（1-8小时），平均开发时间从10-20小时降至4-6小时。

Conclusion: LLM辅助工作流显示了自动化Modelica代码生成的潜力，但仍需要在预模拟验证、更强的基础和闭环评估方面进一步研究。

Abstract: Dynamic energy systems and controls require advanced modeling frameworks to
design and test supervisory and fault tolerant strategies. Modelica is a widely
used equation based language, but developing control modules is labor intensive
and requires specialized expertise. This paper examines the use of large
language models (LLMs) to automate the generation of Control Description
Language modules in the Building Modelica Library as a case study. We developed
a structured workflow that combines standardized prompt scaffolds, library
aware grounding, automated compilation with OpenModelica, and human in the loop
evaluation. Experiments were carried out on four basic logic tasks (And, Or,
Not, and Switch) and five control modules (chiller enable/disable, bypass valve
control, cooling tower fan speed, plant requests, and relief damper control).
The results showed that GPT 4o failed to produce executable Modelica code in
zero shot mode, while Claude Sonnet 4 achieved up to full success for basic
logic blocks with carefully engineered prompts. For control modules, success
rates reached 83 percent, and failed outputs required medium level human repair
(estimated one to eight hours). Retrieval augmented generation often produced
mismatches in module selection (for example, And retrieved as Or), while a
deterministic hard rule search strategy avoided these errors. Human evaluation
also outperformed AI evaluation, since current LLMs cannot assess simulation
results or validate behavioral correctness. Despite these limitations, the LLM
assisted workflow reduced the average development time from 10 to 20 hours down
to 4 to 6 hours per module, corresponding to 40 to 60 percent time savings.
These results highlight both the potential and current limitations of LLM
assisted Modelica generation, and point to future research in pre simulation
validation, stronger grounding, and closed loop evaluation.

</details>


### [11] [Evaluating the Effectiveness of Coverage-Guided Fuzzing for Testing Deep Learning Library APIs](https://arxiv.org/abs/2509.14626)
*Feiran Qin,M. M. Abid Naziri,Hengyu Ai,Saikat Dutta,Marcelo d'Amorim*

Main category: cs.SE

TL;DR: FlashFuzz使用LLM自动合成API级测试harness，将覆盖率引导的模糊测试应用于深度学习库，显著提高了代码覆盖率、有效性和效率，发现了42个新bug。


<details>
  <summary>Details</summary>
Motivation: 深度学习库中的bug检测重要且具有挑战性，现有的API级和模型级模糊测试方法缺乏覆盖率指导，限制了其有效性和效率。

Method: 提出FlashFuzz技术，利用大型语言模型自动合成API级测试harness，通过模板、辅助函数和API文档的组合，采用反馈驱动的迭代策略来合成和修复harness。

Result: 为1151个PyTorch和662个TensorFlow API合成了harness，相比现有方法覆盖率提高101.13-212.88%，有效性提高1.0x-5.4x，输入生成速度提升1x-1182x，发现了42个未知bug。

Conclusion: 证实了覆盖率引导的模糊测试可以有效地应用于深度学习库，为未来的测试方法提供了强有力的基线。

Abstract: Deep Learning (DL) libraries such as PyTorch provide the core components to
build major AI-enabled applications. Finding bugs in these libraries is
important and challenging. Prior approaches have tackled this by performing
either API-level fuzzing or model-level fuzzing, but they do not use coverage
guidance, which limits their effectiveness and efficiency. This raises an
intriguing question: can coverage guided fuzzing (CGF), in particular
frameworks like LibFuzzer, be effectively applied to DL libraries, and does it
offer meaningful improvements in code coverage, bug detection, and scalability
compared to prior methods?
  We present the first in-depth study to answer this question. A key challenge
in applying CGF to DL libraries is the need to create a test harness for each
API that can transform byte-level fuzzer inputs into valid API inputs. To
address this, we propose FlashFuzz, a technique that leverages Large Language
Models (LLMs) to automatically synthesize API-level harnesses by combining
templates, helper functions, and API documentation. FlashFuzz uses a feedback
driven strategy to iteratively synthesize and repair harnesses. With this
approach, FlashFuzz synthesizes harnesses for 1,151 PyTorch and 662 TensorFlow
APIs. Compared to state-of-the-art fuzzing methods (ACETest, PathFinder, and
TitanFuzz), FlashFuzz achieves up to 101.13 to 212.88 percent higher coverage
and 1.0x to 5.4x higher validity rate, while also delivering 1x to 1182x
speedups in input generation. FlashFuzz has discovered 42 previously unknown
bugs in PyTorch and TensorFlow, 8 of which are already fixed. Our study
confirms that CGF can be effectively applied to DL libraries and provides a
strong baseline for future testing approaches.

</details>


### [12] [SALT4Decompile: Inferring Source-level Abstract Logic Tree for LLM-Based Binary Decompilation](https://arxiv.org/abs/2509.14646)
*Yongpan Wang,Xin Xu,Xiaojie Zhu,Xiaodong Gu,Beijun Shen*

Main category: cs.SE

TL;DR: SALTM提出了一种基于抽象逻辑树的二进制反编译方法，通过将二进制操作抽象为高层逻辑框架来指导LLM进行语义恢复，显著提升了反编译准确率。


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的反编译方法将汇编代码视为线性指令序列，忽略了二进制文件中的跳转模式和数据段隔离，导致语义恢复能力受限。

Method: SALTM从汇编代码构建源级抽象逻辑树(SALT)来近似高级语言逻辑结构，然后微调LLM生成反编译代码，最后通过错误校正和符号恢复进行优化。

Result: 在三个数据集上显著优于现有方法（如Decompile-Eval上70.4% TCP率，提升10.6%），对四种常见混淆技术具有鲁棒性，用户研究证实对人工分析有更好帮助。

Conclusion: SALTM通过抽象二进制操作到高层逻辑框架的方法，有效解决了传统LLM反编译的局限性，在逻辑恢复和实用性方面都表现出色。

Abstract: Decompilation is widely used in reverse engineering to recover high-level
language code from binary executables. While recent approaches leveraging Large
Language Models (LLMs) have shown promising progress, they typically treat
assembly code as a linear sequence of instructions, overlooking arbitrary jump
patterns and isolated data segments inherent to binary files. This limitation
significantly hinders their ability to correctly infer source code semantics
from assembly code. To address this limitation, we propose \saltm, a novel
binary decompilation method that abstracts stable logical features shared
between binary and source code. The core idea of \saltm is to abstract selected
binary-level operations, such as specific jumps, into a high-level logic
framework that better guides LLMs in semantic recovery. Given a binary
function, \saltm constructs a Source-level Abstract Logic Tree (\salt) from
assembly code to approximate the logic structure of high-level language. It
then fine-tunes an LLM using the reconstructed \salt to generate decompiled
code. Finally, the output is refined through error correction and symbol
recovery to improve readability and correctness. We compare \saltm to three
categories of baselines (general-purpose LLMs, commercial decompilers, and
decompilation methods) using three well-known datasets (Decompile-Eval, MBPP,
Exebench). Our experimental results demonstrate that \saltm is highly effective
in recovering the logic of the source code, significantly outperforming
state-of-the-art methods (e.g., 70.4\% TCP rate on Decompile-Eval with a 10.6\%
improvement). The results further validate its robustness against four commonly
used obfuscation techniques. Additionally, analyses of real-world software and
a user study confirm that our decompiled output offers superior assistance to
human analysts in comprehending binary functions.

</details>


### [13] [Wireless Communication Performance Testing: From Laboratory Environment to Research Vessel](https://arxiv.org/abs/2509.14740)
*Andrei-Raoul Morariu,Andreas Strandberg,Bogdan Iancu,Jerker Bjorkqvist*

Main category: cs.SE

TL;DR: 实验室和外部环境中共享谱信号传输研究，分析障碍物、距离和位置对信号衰减的影响


<details>
  <summary>Details</summary>
Motivation: 研究环境因素如障碍物、距离和位置如何影响动态和有障碍环境中的无线通信效果

Method: 在实验室和外部环境进行信号传输测量，分析障碍物对视线的阻挡效果，以及在电动研究船上不同位置的传输效率

Result: 实验室障碍物能够显著衰减发射器和接收器之间的信号，距离和位置对信号传输效率有明显影响

Conclusion: 环境因素在动态和有障碍的环境中对无线通信具有重要影响，这些发现有助于理解和改善共享谱通信系统

Abstract: This study investigates signal transmission within a shared spectrum,
focusing on measurements conducted both in laboratory and outdoor environments.
The objective was to demonstrate how laboratory objects obstructing the line of
sight can attenuate the signal between a transmitter (Tx) and a receiver (Rx).
Additionally, we examined the impact of distance and placement in various
locations aboard an electric research boat on signal transmission efficiency.
These findings contribute to understanding whether the environmental factors
influence wireless communication in dynamic and obstructed environments.

</details>


### [14] [On the Use of Agentic Coding Manifests: An Empirical Study of Claude Code](https://arxiv.org/abs/2509.14744)
*Worawalan Chatlatanagulchai,Kundjanasith Thonglek,Brittany Reid,Yutaro Kashiwa,Pattara Leelaprute,Arnon Rungsawang,Bundit Manaskasemsak,Hajimu Iida*

Main category: cs.SE

TL;DR: 研究分析了253个Claude.md文件，揭示了代理演示文件的结构模式和内容特征，为开发者创建高效的代理配置文件提供指导。


<details>
  <summary>Details</summary>
Motivation: 代理编码工具依赖agent manifests配置文件来获取项目上下文和操作规则，但缺乏系统的文档说明给开发者带来挑战。

Method: 对来自242个仓库的253个Claude.md文件进行结构和内容分析，识别其中的模式和常见内容类型。

Result: 发现演示文件通常具有浅层次结构（一个主标题和几个子部分），内容以操作命令、技术实现说明和高层次架构为主。

Conclusion: 研究结果为开发者创建有效的agent manifests提供了实践指南，有助于提升代理编码工具的效能和易用性。

Abstract: Agentic coding tools receive goals written in natural language as input,
break them down into specific tasks, and write/execute the actual code with
minimal human intervention. Key to this process are agent manifests,
configuration files (such as Claude.md) that provide agents with essential
project context, identity, and operational rules. However, the lack of
comprehensive and accessible documentation for creating these manifests
presents a significant challenge for developers. We analyzed 253 Claude.md
files from 242 repositories to identify structural patterns and common content.
Our findings show that manifests typically have shallow hierarchies with one
main heading and several subsections, with content dominated by operational
commands, technical implementation notes, and high-level architecture.

</details>


### [15] [On the Use of Agentic Coding: An Empirical Study of Pull Requests on GitHub](https://arxiv.org/abs/2509.14745)
*Miku Watanabe,Hao Li,Yutaro Kashiwa,Brittany Reid,Hajimu Iida,Ahmed E. Hassan*

Main category: cs.SE

TL;DR: 对567个由Claude Code生成的GitHub PR进行实证研究，发现83.8%的AI辅助PR被接受合并，其中54.9%无需修改直接集成，但仍有45.1%需要人工修订。


<details>
  <summary>Details</summary>
Motivation: 随着LLM越来越多地集成到软件开发中，AI代理生成代码和提交PR的能力正在成为标准实践，但对其实际有用性和接受程度知之甚少。

Method: 实证研究157个开源项目中的567个GitHub PR，这些PR使用Claude Code代理工具生成，分析开发者的使用模式和PR接受情况。

Result: 开发者主要使用代理进行重构、文档和测试任务；83.8%的AI辅助PR被接受合并；54.9%的合并PR无需修改直接集成；45.1%需要人工修订，特别是在bug修复、文档和项目标准遵循方面。

Conclusion: AI辅助PR在很大程度上是可接受的，但仍需要人工监督和精炼，特别是在bug修复、文档编写和项目特定标准遵循方面。

Abstract: Large language models (LLMs) are increasingly being integrated into software
development processes. The ability to generate code and submit pull requests
with minimal human intervention, through the use of autonomous AI agents, is
poised to become a standard practice. However, little is known about the
practical usefulness of these pull requests and the extent to which their
contributions are accepted in real-world projects. In this paper, we
empirically study 567 GitHub pull requests (PRs) generated using Claude Code,
an agentic coding tool, across 157 diverse open-source projects. Our analysis
reveals that developers tend to rely on agents for tasks such as refactoring,
documentation, and testing. The results indicate that 83.8% of these
agent-assisted PRs are eventually accepted and merged by project maintainers,
with 54.9% of the merged PRs are integrated without further modification. The
remaining 45.1% require additional changes benefit from human revisions,
especially for bug fixes, documentation, and adherence to project-specific
standards. These findings suggest that while agent-assisted PRs are largely
acceptable, they still benefit from human oversight and refinement.

</details>


### [16] [RulER: Automated Rule-Based Semantic Error Localization and Repair for Code Translation](https://arxiv.org/abs/2509.14829)
*Shuo Jin,Songqiang Chen,Xiaoyuan Xie,Shing-Chi Cheung*

Main category: cs.SE

TL;DR: RulER是一个基于规则的代码翻译调试方法，通过从LLM生成的正确翻译中自动推导代码翻译规则，显著提高了错误定位和修复效果


<details>
  <summary>Details</summary>
Motivation: 现有的代码翻译自动调试方法依赖代码对齐和修复补丁模板，但缺乏可靠的参考来构建对齐和设计模板，影响定位准确性和修复效果

Method: 从LLM生成的正确翻译中自动推导代码翻译规则，动态组合规则以适应性地对齐更多语句，利用规则作为可靠的代码对齐和修复模板参考

Result: 在Java到C++和Python到C++翻译任务上，RulER在错误定位率和修复成功率上分别比最佳基线方法提高了20%和272%

Conclusion: RulER展示了从LLMs中提取和利用编码知识的有前景方法，相比直接提示LLMs生成补丁具有更优越的修复性能

Abstract: Automated code translation aims to convert programs between different
programming languages while maintaining their functionality. Due to the
imperfections of code translation models, the generated translations may
contain errors that compromise their reliability. Existing automated debugging
methods for code translation rely on code alignments and repair patch templates
to locate and fix erroneous translations. However, existing methods lack
reliable references to construct code alignments and design repair patch
templates, which significantly impacts their localization accuracy and repair
effectiveness. To address these limitations, we reintroduce code translation
rules and propose a rule-based debugging method for code translation, called
RulER. RulER automatically derives code translation rules from correct
translations generated by LLMs, enabling the efficient collection of diverse
translation rules. In addition, RulER dynamically combines the existing rules
on expandable nodes like expressions and tokens to further adaptively align
more statements. These rules capture clear and detailed structural
correspondences between source and target programming languages. Therefore,
they can serve as reliable and reusable references for code alignment and
repair template design, enabling RulER to locate and fix translation errors
effectively. Our evaluation of RulER on Java-to-C++ and Python-to-C++
translations produced by four code translation models demonstrates that RulER
outperforms state-of-the-art methods, BatFix and TransMap. Our experimental
results show that RulER outperformed the best baseline by 20% and 272% in terms
of error localization rates and repair success rates, respectively. RulER
exhibits superior repair performance compared to directly prompting LLMs for
patch generation, demonstrating a promising methodology for extracting and
leveraging coding knowledge from LLMs.

</details>


### [17] [CodeFuse-CR-Bench: A Comprehensiveness-aware Benchmark for End-to-End Code Review Evaluation in Python Projects](https://arxiv.org/abs/2509.14856)
*Hanyang Guo,Xunjin Zheng,Zihan Liao,Hang Yu,Peng DI,Ziyin Zhang,Hong-Ning Dai*

Main category: cs.SE

TL;DR: 提出了CodeFuse-CR-Bench，首个面向仓库级代码审查的全面性基准测试，包含601个高质量实例，覆盖9个PR问题域，提供丰富的多维度上下文信息。


<details>
  <summary>Details</summary>
Motivation: 现有代码审查基准测试存在"现实差距"，仅评估孤立子任务且使用简化的上下文贫乏数据，无法反映真实世界代码审查的整体性和上下文丰富性。

Method: 构建包含601个实例的基准测试，涵盖70个Python项目，提供issue、PR详情和仓库状态等丰富上下文。提出结合基于规则的定位和语法检查与基于模型的审查质量评估的新评估框架。

Result: 进行了大规模LLM评估，发现：(1)没有单一LLM在所有CR方面都占优；(2)Gemini 2.5 Pro综合表现最佳；(3)不同LLM对冗余上下文的鲁棒性不同。

Conclusion: 需要整体性、多维度的评估方法，为推进真正智能且实用的代码审查助手提供了可操作的见解。

Abstract: Automated code review (CR) is a key application for Large Language Models
(LLMs), but progress is hampered by a "reality gap": existing benchmarks
evaluate models on isolated sub-tasks using simplified, context-poor data. This
fails to reflect the holistic context-rich nature of real-world CR. To bridge
this gap, we introduce CodeFuse-CR-Bench, the first comprehensiveness-aware
benchmark for repository-level CR evaluation. CodeFuse-CR-Bench comprises 601
high-quality instances from 70 Python projects covering nine Pull-Request (PR)
problem domains, where each instance provides rich, multi-faceted context
including the associated issue, PR details, and repository state, enabling
end-to-end evaluation. Beyond superficial metrics, we also propose a novel
evaluation framework that combines rule-based checks for location and syntax
with model-based judgments of review quality. We present the first large-scale
assessment of state-of-the-art LLMs on this comprehensive CR task. Our results
establish crucial baselines and reveal that (1) no single LLM dominates all
aspects of CR; (2) Gemini 2.5 Pro achieves the highest comprehensive
performance; and (3) different LLMs exhibit varying robustness to redundant
context. These findings highlight the necessity of holistic, multi-dimensional
evaluation and provide actionable insights for advancing truly intelligent yet
practical CR assistants.

</details>


### [18] [CARGO: A Framework for Confidence-Aware Routing of Large Language Models](https://arxiv.org/abs/2509.14899)
*Amine Barrak,Yosr Fourati,Michael Olchawa,Emna Ksontini,Khalil Zoghlami*

Main category: cs.SE

TL;DR: CARGO是一个轻量级的LLM路由框架，通过嵌入回归器和可选分类器实现动态模型选择，无需人工标注，在多个任务领域达到专家级性能


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型规模、专业化和延迟特性的多样化，如何将用户提示路由到最合适的模型以平衡性能和成本变得至关重要

Method: 使用基于嵌入的回归器训练LLM判断的成对比较来预测模型性能，不确定时调用二元分类器。支持五个任务领域的特定回归器

Result: 在四个竞争性LLM上实现76.4%的top-1路由准确率，对抗单个专家的胜率在72%到89%之间

Conclusion: 置信度引导的轻量级路由能够以最小开销实现专家级性能，为实际多模型LLM部署提供实用解决方案

Abstract: As large language models (LLMs) proliferate in scale, specialization, and
latency profiles, the challenge of routing user prompts to the most appropriate
model has become increasingly critical for balancing performance and cost. We
introduce CARGO (Category-Aware Routing with Gap-based Optimization), a
lightweight, confidence-aware framework for dynamic LLM selection. CARGO
employs a single embedding-based regressor trained on LLM-judged pairwise
comparisons to predict model performance, with an optional binary classifier
invoked when predictions are uncertain. This two-stage design enables precise,
cost-aware routing without the need for human-annotated supervision. To capture
domain-specific behavior, CARGO also supports category-specific regressors
trained across five task groups: mathematics, coding, reasoning, summarization,
and creative writing. Evaluated on four competitive LLMs (GPT-4o, Claude 3.5
Sonnet, DeepSeek V3, and Perplexity Sonar), CARGO achieves a top-1 routing
accuracy of 76.4% and win rates ranging from 72% to 89% against individual
experts. These results demonstrate that confidence-guided, lightweight routing
can achieve expert-level performance with minimal overhead, offering a
practical solution for real-world, multi-model LLM deployments.

</details>


### [19] ["Let it be Chaos in the Plumbing!" Usage and Efficacy of Chaos Engineering in DevOps Pipelines](https://arxiv.org/abs/2509.14931)
*Stefano Fossati,Damian Andrew Tamburri,Massimiliano Di Penta,Marco Tonnarelli*

Main category: cs.SE

TL;DR: 本文通过系统性的灰色文献综述，分析了2019-2024年间50个来源，将混沌工程基础原则扩展为10个概念分类框架，揭示了行业实践中对受控实验、自动化和风险缓解策略的日益重视。


<details>
  <summary>Details</summary>
Motivation: 混沌工程作为一种主动方法，在DevOps环境中提高分布式系统弹性方面日益重要，但缺乏对其实际应用方式的系统性理解。

Method: 采用系统性灰色文献综述方法，分析2019年至2024年初发布的50个行业实践来源。

Result: 开发了一个包含10个不同概念的全面分类框架，发现虽然混沌工程核心原则仍然具有影响力，但实践者越来越强调受控实验、自动化和风险缓解策略。

Conclusion: 研究结果增强了人们对混沌工程在实践中如何设计和实施的理解，并为未来研究和工业应用提供了指导，旨在提高动态生产环境中的系统鲁棒性。

Abstract: Chaos Engineering (CE) has emerged as a proactive method to improve the
resilience of modern distributed systems, particularly within DevOps
environments. Originally pioneered by Netflix, CE simulates real-world failures
to expose weaknesses before they impact production. In this paper, we present a
systematic gray literature review that investigates how industry practitioners
have adopted and adapted CE principles over recent years. Analyzing 50 sources
published between 2019 and early 2024, we developed a comprehensive
classification framework that extends the foundational CE principles into ten
distinct concepts. Our study reveals that while the core tenets of CE remain
influential, practitioners increasingly emphasize controlled experimentation,
automation, and risk mitigation strategies to align with the demands of agile
and continuously evolving DevOps pipelines. Our results enhance the
understanding of how CE is intended and implemented in practice, and offer
guidance for future research and industrial applications aimed at improving
system robustness in dynamic production environments.

</details>


### [20] [Code Less to Code More: Streamlining Language Server Protocol and Type System Development for Language Families](https://arxiv.org/abs/2509.15150)
*Federico Bruzzone,Walter Cazzola,Luca Favalli*

Main category: cs.SE

TL;DR: 提出了Typelang语言家族和模块化语言服务器生成方法，显著减少了语言编辑器组合的复杂度，实现了93.48%的类型系统实现代码量减少和100%的LSP插件自动生成。


<details>
  <summary>Details</summary>
Motivation: 解决多语言编辑器支持开发的复杂性，现有语言工作台在模块化、可重用性和利用类型系统生成语言服务器方面存在不足。

Method: 开发Typelang DSL家族用于模块化类型系统实现；提出模块化语言服务器生成过程；引入变体导向编程范式和跨构件协调层；创建LSP插件生成器。

Result: 在Neverlang中实现Typelang，为三个编辑器生成语言服务器和LSP插件，类型系统实现代码量减少93.48%，LSP插件生成完全自动化。

Conclusion: 该方法显著降低了语言家族编辑支持的工作量，特别是在构件重用的情况下，将语言编辑器组合复杂度从L×E降低到N×1（N<<T）。

Abstract: Developing editing support for $L$ languages in $E$ editors is complex and
time-consuming. Some languages do not provide dedicated editors, while others
offer a single native editor. The $\textit{language server protocol}$ (LSP)
reduces the language-editor combinations $L \times E$ to $L + E$, where a
single language server communicates with editors via LSP plugins. However,
overlapping implementations of linguistic components remain an issue. Existing
language workbenches struggle with modularity, reusability, and leveraging type
systems for language server generation. In this work, we propose: (i) Typelang,
a family of domain-specific languages for modular, composable, and reusable
type system implementation, (ii) a modular language server generation process,
producing servers for languages built in a modular workbench, (iii) the
variant-oriented programming paradigm and a cross-artifact coordination layer
to manage interdependent software variants, and (iv) an LSP plugin generator,
reducing $E$ to $1$ by automating plugin creation for multiple editors. To
simplify editing support for language families, each language artifact
integrates its own Typelang variant, used to generate language servers. This
reduces combinations to $T \times 1$, where $T = L$ represents the number of
type systems. Further reuse of language artifacts across languages lowers this
to $N \times 1$, where $N << T$, representing unique type systems. We implement
Typelang in Neverlang, generating language servers for each artifact and LSP
plugins for three editors. Empirical evaluation shows a 93.48% reduction in
characters needed for type system implementation and 100% automation of LSP
plugin generation, significantly lowering effort for editing support in
language families, especially when artifacts are reused.

</details>


### [21] [Orion: Fuzzing Workflow Automation](https://arxiv.org/abs/2509.15195)
*Max Bazalii,Marius Fleischer*

Main category: cs.SE

TL;DR: Orion是一个自动化模糊测试框架，通过整合LLM推理与传统工具，大幅减少人工工作量，在clib库中发现两个新漏洞


<details>
  <summary>Details</summary>
Motivation: 现代模糊测试虽然能自动生成输入和监控执行，但从代码库分析、配置测试工具到结果分类的整个工作流程仍需要大量人工参与。现有研究只关注单个阶段，需要人工连接成完整测试活动

Method: Orion框架将LLM用于代码推理和语义指导，同时依赖确定性工具进行验证、迭代优化和需要精确性的任务，整合LLM推理与传统工具

Result: 在基准测试套件中，Orion将人工工作量减少了46-204倍（取决于工作流程阶段），并在广泛使用的开源clib库中发现两个先前未知的漏洞

Conclusion: Orion通过整合LLM推理与传统工具，成功自动化了模糊测试中的人工瓶颈，显著提高了测试效率并发现了真实漏洞

Abstract: Fuzz testing is one of the most effective techniques for finding software
vulnerabilities. While modern fuzzers can generate inputs and monitor
executions automatically, the overall workflow, from analyzing a codebase, to
configuring harnesses, to triaging results, still requires substantial manual
effort. Prior attempts focused on single stages such as harness synthesis or
input minimization, leaving researchers to manually connect the pieces into a
complete fuzzing campaign.
  We introduce Orion, a framework that automates the the manual bottlenecks of
fuzzing by integrating LLM reasoning with traditional tools, allowing campaigns
to scale to settings where human effort alone was impractical. Orion uses LLMs
for code reasoning and semantic guidance, while relying on deterministic tools
for verification, iterative refinement, and tasks that require precision.
Across our benchmark suite, Orion reduces human effort by 46-204x depending on
the workflow stage, and we demonstrate its effectiveness through the discovery
of two previously unknown vulnerabilities in the widely used open-source clib
library.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [22] [Early Approaches to Adversarial Fine-Tuning for Prompt Injection Defense: A 2022 Study of GPT-3 and Contemporary Models](https://arxiv.org/abs/2509.14271)
*Gustavo Sandoval,Denys Fenchenko,Junyao Chen*

Main category: cs.CR

TL;DR: 2022年早期研究，针对大型语言模型的提示注入和目标劫持攻击，提出了对抗性微调防御方法，将攻击成功率从31%降至接近零，为现代防御研究奠定基础


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型的广泛应用，提示注入攻击成为严重的安全威胁，需要研究有效的防御方法来保护模型免受恶意攻击

Method: 研究两种对抗攻击（提示注入和目标劫持），构建攻击方法并在不同LLM上测试，提出并评估新颖的对抗性微调防御技术

Result: 无防御时攻击成功率达31%，使用对抗性微调后，小型GPT-3变体的攻击成功率降至接近零，但发现更大更灵活的模型更容易受攻击

Conclusion: 虽然测试的具体模型已被取代，但核心方法和实证发现为现代提示注入防御研究（包括指令层级系统和宪法AI方法）奠定了基础

Abstract: This paper documents early research conducted in 2022 on defending against
prompt injection attacks in large language models, providing historical context
for the evolution of this critical security domain. This research focuses on
two adversarial attacks against Large Language Models (LLMs): prompt injection
and goal hijacking. We examine how to construct these attacks, test them on
various LLMs, and compare their effectiveness. We propose and evaluate a novel
defense technique called Adversarial Fine-Tuning. Our results show that,
without this defense, the attacks succeeded 31\% of the time on GPT-3 series
models. When using our Adversarial Fine-Tuning approach, attack success rates
were reduced to near zero for smaller GPT-3 variants (Ada, Babbage, Curie),
though we note that subsequent research has revealed limitations of
fine-tuning-based defenses. We also find that more flexible models exhibit
greater vulnerability to these attacks. Consequently, large models such as
GPT-3 Davinci are more vulnerable than smaller models like GPT-2. While the
specific models tested are now superseded, the core methodology and empirical
findings contributed to the foundation of modern prompt injection defense
research, including instruction hierarchy systems and constitutional AI
approaches.

</details>


### [23] [FedMentor: Domain-Aware Differential Privacy for Heterogeneous Federated LLMs in Mental Health](https://arxiv.org/abs/2509.14275)
*Nobin Sarwar,Shubhashis Roy Dipta*

Main category: cs.CR

TL;DR: FedMentor是一个联邦学习框架，结合LoRA和领域感知差分隐私，在心理健康等敏感领域实现隐私保护的LLM微调，在保持性能的同时满足各领域隐私预算要求。


<details>
  <summary>Details</summary>
Motivation: 在敏感领域（如心理健康）进行LLM微调时，需要在严格保密性与模型效用和安全性之间取得平衡。现有方法难以同时满足这些要求。

Method: 提出FedMentor框架，集成低秩适应（LoRA）和领域感知差分隐私（DP）。每个客户端根据数据敏感度应用自定义DP噪声比例，服务器在效用低于阈值时自适应降低噪声。

Result: 在三个心理健康数据集上的实验显示，FedMentor相比无隐私的标准联邦学习提高了安全性，安全输出率提升最多3个百分点，毒性降低，同时保持效用（BERTScore F1和ROUGE-L）与非隐私基线的差距在0.5%以内，接近集中式上限。

Conclusion: FedMentor为医疗保健等敏感领域提供了一种实用的隐私保护LLM微调方法，可在单GPU客户端上扩展到1.7B参数的骨干网络，每轮通信需求小于173MB。

Abstract: Privacy-preserving adaptation of Large Language Models (LLMs) in sensitive
domains (e.g., mental health) requires balancing strict confidentiality with
model utility and safety. We propose FedMentor, a federated fine-tuning
framework that integrates Low-Rank Adaptation (LoRA) and domain-aware
Differential Privacy (DP) to meet per-domain privacy budgets while maintaining
performance. Each client (domain) applies a custom DP noise scale proportional
to its data sensitivity, and the server adaptively reduces noise when utility
falls below a threshold. In experiments on three mental health datasets, we
show that FedMentor improves safety over standard Federated Learning without
privacy, raising safe output rates by up to three points and lowering toxicity,
while maintaining utility (BERTScore F1 and ROUGE-L) within 0.5% of the
non-private baseline and close to the centralized upper bound. The framework
scales to backbones with up to 1.7B parameters on single-GPU clients, requiring
< 173 MB of communication per round. FedMentor demonstrates a practical
approach to privately fine-tune LLMs for safer deployments in healthcare and
other sensitive fields.

</details>


### [24] [Beyond Data Privacy: New Privacy Risks for Large Language Models](https://arxiv.org/abs/2509.14278)
*Yuntao Du,Zitao Li,Ninghui Li,Bolin Ding*

Main category: cs.CR

TL;DR: 论文系统分析了LLM部署阶段的新兴隐私风险，包括数据泄露和恶意攻击威胁，呼吁研究社区关注部署阶段隐私保护而不仅是训练阶段。


<details>
  <summary>Details</summary>
Motivation: LLM在自然语言理解和自主决策方面取得显著进展，但部署阶段出现了新的隐私威胁，现有研究主要关注训练阶段隐私风险，对部署阶段的新威胁关注不足。

Method: 系统性地检查LLM部署过程中出现的新隐私风险，分析漏洞来源和攻击方式。

Result: 发现LLM集成到广泛应用中及其自主能力武器化创造了新的隐私漏洞，可能导致无意数据泄露和恶意数据窃取，攻击者能够发起复杂的大规模隐私攻击。

Conclusion: 需要研究社区扩大关注范围，不仅关注数据隐私风险，还要开发新的防御措施来应对日益强大的LLM和LLM驱动系统带来的演变威胁。

Abstract: Large Language Models (LLMs) have achieved remarkable progress in natural
language understanding, reasoning, and autonomous decision-making. However,
these advancements have also come with significant privacy concerns. While
significant research has focused on mitigating the data privacy risks of LLMs
during various stages of model training, less attention has been paid to new
threats emerging from their deployment. The integration of LLMs into widely
used applications and the weaponization of their autonomous abilities have
created new privacy vulnerabilities. These vulnerabilities provide
opportunities for both inadvertent data leakage and malicious exfiltration from
LLM-powered systems. Additionally, adversaries can exploit these systems to
launch sophisticated, large-scale privacy attacks, threatening not only
individual privacy but also financial security and societal trust. In this
paper, we systematically examine these emerging privacy risks of LLMs. We also
discuss potential mitigation strategies and call for the research community to
broaden its focus beyond data privacy risks, developing new defenses to address
the evolving threats posed by increasingly powerful LLMs and LLM-powered
systems.

</details>


### [25] [Resisting Quantum Key Distribution Attacks Using Quantum Machine Learning](https://arxiv.org/abs/2509.14282)
*Ali Al-kuwari,Noureldin Mohamed,Saif Al-kuwari,Ahmed Farouk,Bikash K. Behera*

Main category: cs.CR

TL;DR: 采用混合量子长短期记忆模型(QLSTM)检测量子加密分发(QKD)攻击，达到93.7%准确率，超越传统深度学习模型


<details>
  <summary>Details</summary>
Motivation: 量子计算威胁现代加密算法安全，QKD实际实现存在硬件缺陷和高级攻击漏洞

Method: 提出混合量子长短期记忆模型(QLSTM)，结合量子增强学习与经典深度学习，捕捉QKD数据中的复杂时间模式

Result: 在模拟7种QKD攻击场景的数据集上，混合QLSTM模型训练50个时期后达到93.7%准确率，超过LSTM和CNN等经典深度学习模型

Conclusion: 混合量子机器学习技术在QKD攻击检测方面呈现出突出潜力，有助于提升未来量子通信网络的安全性

Abstract: The emergence of quantum computing poses significant risks to the security of
modern communication networks as it breaks today's public-key cryptographic
algorithms. Quantum Key Distribution (QKD) offers a promising solution by
harnessing the principles of quantum mechanics to establish secure keys.
However, practical QKD implementations remain vulnerable to hardware
imperfections and advanced attacks such as Photon Number Splitting and
Trojan-Horse attacks. In this work, we investigate the potential of using
quantum machine learning (QML) to detect popular QKD attacks. In particular, we
propose a Hybrid Quantum Long Short-Term Memory (QLSTM) model to improve the
detection of common QKD attacks. By combining quantum-enhanced learning with
classical deep learning, the model captures complex temporal patterns in QKD
data, improving detection accuracy. To evaluate the proposed model, we
introduce a realistic QKD dataset simulating normal QKD operations along with
seven attack scenarios, Intercept-and-Resend, Photon-Number Splitting (PNS),
Trojan-Horse attacks Random Number Generator (RNG), Detector Blinding,
Wavelength-dependent Trojan Horse, and Combined attacks. The dataset includes
quantum security metrics such as Quantum Bit Error Rate (QBER), measurement
entropy, signal and decoy loss rates, and time-based metrics, ensuring an
accurate representation of real-world conditions. Our results demonstrate
promising performance of the quantum machine learning approach compared to
traditional classical machine learning models, highlighting the potential of
hybrid techniques to enhance the security of future quantum communication
networks. The proposed Hybrid QLSTM model achieved an accuracy of 93.7.0\%
after 50 training epochs, outperforming classical deep learning models such as
LSTM, and CNN.

</details>


### [26] [The Sum Leaks More Than Its Parts: Compositional Privacy Risks and Mitigations in Multi-Agent Collaboration](https://arxiv.org/abs/2509.14284)
*Vaidehi Patil,Elias Stengel-Eskin,Mohit Bansal*

Main category: cs.CR

TL;DR: 该论文首次系统研究了多智能体LLM系统中的组合隐私泄露问题，提出了两种防御策略（ToM和CoDef），实验表明CoDef在隐私保护与任务效用之间取得了最佳平衡。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型在多智能体系统中的广泛应用，传统的隐私风险评估方法（如记忆化、直接推理或单轮评估）已不足以应对组合交互带来的新型隐私风险，需要研究看似无害的响应在跨交互组合后如何累积导致敏感信息泄露。

Method: 1) 开发了一个建模框架，分析辅助知识和智能体交互如何共同放大隐私风险；2) 提出了两种防御策略：心理理论防御(ToM)和协作共识防御(CoDef)；3) 设计了平衡评估框架，同时考虑暴露敏感信息的组合和产生良性推断的组合。

Result: 实验结果显示：思维链单独提供有限保护（约39%敏感信息阻断率），ToM防御显著改善敏感查询阻断（高达97%）但会降低良性任务成功率，CoDef实现了最佳平衡（79.8%的平衡结果），结合显式推理与防御者协作效果最佳。

Conclusion: 研究揭示了协作LLM部署中的新型组合隐私风险类别，为设计针对组合性、上下文驱动的隐私泄露防护措施提供了可行见解，强调需要结合显式推理和防御者协作来实现隐私与效用的最佳平衡。

Abstract: As large language models (LLMs) become integral to multi-agent systems, new
privacy risks emerge that extend beyond memorization, direct inference, or
single-turn evaluations. In particular, seemingly innocuous responses, when
composed across interactions, can cumulatively enable adversaries to recover
sensitive information, a phenomenon we term compositional privacy leakage. We
present the first systematic study of such compositional privacy leaks and
possible mitigation methods in multi-agent LLM systems. First, we develop a
framework that models how auxiliary knowledge and agent interactions jointly
amplify privacy risks, even when each response is benign in isolation. Next, to
mitigate this, we propose and evaluate two defense strategies: (1)
Theory-of-Mind defense (ToM), where defender agents infer a questioner's intent
by anticipating how their outputs may be exploited by adversaries, and (2)
Collaborative Consensus Defense (CoDef), where responder agents collaborate
with peers who vote based on a shared aggregated state to restrict sensitive
information spread. Crucially, we balance our evaluation across compositions
that expose sensitive information and compositions that yield benign
inferences. Our experiments quantify how these defense strategies differ in
balancing the privacy-utility trade-off. We find that while chain-of-thought
alone offers limited protection to leakage (~39% sensitive blocking rate), our
ToM defense substantially improves sensitive query blocking (up to 97%) but can
reduce benign task success. CoDef achieves the best balance, yielding the
highest Balanced Outcome (79.8%), highlighting the benefit of combining
explicit reasoning with defender collaboration. Together, our results expose a
new class of risks in collaborative LLM deployments and provide actionable
insights for designing safeguards against compositional, context-driven privacy
leakage.

</details>


### [27] [A Multi-Agent LLM Defense Pipeline Against Prompt Injection Attacks](https://arxiv.org/abs/2509.14285)
*S M Asif Hossain,Ruksat Khan Shayoni,Mohd Ruhul Ameen,Akif Islam,M. F. Mridha,Jungpil Shin*

Main category: cs.CR

TL;DR: 这篇论文提出了一种新的多代理防御框架，通过协调多个LLM代理来实时检测和中和提示注入攻击，在400个攻击实例中实现了100%的缓解率。


<details>
  <summary>Details</summary>
Motivation: 提示注入攻击是LLM部署中的主要漏洞，恶意指令可以覆盖系统提示导致非预期行为，需要有效防御方案。

Method: 采用专业化的LLM代理构建协调管道，包括顺序链式代理管道和层级协调器系统两种架构，用于实时检测和中和提示注入攻击。

Result: 在ChatGLM和Llama2两个LLM平台上评估了55种提示注入攻击（分为8个类别，共400个攻击实例）。基线ASR为30%（ChatGLM）和20%（Llama2），而多代理管道实现了100%缓解，ASR降低到0%，保持了合法查询的系统功能性。

Conclusion: 该多代理防御框架能够有效防御多种提示注入攻击类型，包括直接覆盖、代码执行、数据泄漏和混淆技术，显著提升了LLM部署的安全性。

Abstract: Prompt injection attacks represent a major vulnerability in Large Language
Model (LLM) deployments, where malicious instructions embedded in user inputs
can override system prompts and induce unintended behaviors. This paper
presents a novel multi-agent defense framework that employs specialized LLM
agents in coordinated pipelines to detect and neutralize prompt injection
attacks in real-time. We evaluate our approach using two distinct
architectures: a sequential chain-of-agents pipeline and a hierarchical
coordinator-based system. Our comprehensive evaluation on 55 unique prompt
injection attacks, grouped into 8 categories and totaling 400 attack instances
across two LLM platforms (ChatGLM and Llama2), demonstrates significant
security improvements. Without defense mechanisms, baseline Attack Success
Rates (ASR) reached 30% for ChatGLM and 20% for Llama2. Our multi-agent
pipeline achieved 100% mitigation, reducing ASR to 0% across all tested
scenarios. The framework demonstrates robustness across multiple attack
categories including direct overrides, code execution attempts, data
exfiltration, and obfuscation techniques, while maintaining system
functionality for legitimate queries.

</details>


### [28] [A Simple and Efficient Jailbreak Method Exploiting LLMs' Helpfulness](https://arxiv.org/abs/2509.14297)
*Xuan Luo,Yue Wang,Zefeng He,Geng Tu,Jing Li,Ruifeng Xu*

Main category: cs.CR

TL;DR: HILL是一种新型越狱方法，通过将有害指令转换为学习式问题来绕过LLM安全防护，在多个模型上表现出高攻击成功率和鲁棒性


<details>
  <summary>Details</summary>
Motivation: 揭示LLM安全对齐机制的漏洞，开发更有效的越狱方法来测试和加强模型的安全防护能力

Method: 将强制性有害请求转换为带有假设性指示的学习式问题，使用简洁提示保持高效率

Result: 在AdvBench数据集上对多种模型测试，HILL达到最高攻击成功率，大多数防御方法效果有限甚至适得其反

Conclusion: 学习式诱导暴露了安全机制的重大漏洞，凸显了在帮助性和安全性之间平衡的关键挑战

Abstract: Safety alignment aims to prevent Large Language Models (LLMs) from responding
to harmful queries. To strengthen safety protections, jailbreak methods are
developed to simulate malicious attacks and uncover vulnerabilities. In this
paper, we introduce HILL (Hiding Intention by Learning from LLMs), a novel
jailbreak approach that systematically transforms imperative harmful requests
into learning-style questions with only straightforward hypotheticality
indicators. Further, we introduce two new metrics to thoroughly evaluate the
utility of jailbreak methods. Experiments on the AdvBench dataset across a wide
range of models demonstrate HILL's strong effectiveness, generalizability, and
harmfulness. It achieves top attack success rates on the majority of models and
across malicious categories while maintaining high efficiency with concise
prompts. Results of various defense methods show the robustness of HILL, with
most defenses having mediocre effects or even increasing the attack success
rates. Moreover, the assessment on our constructed safe prompts reveals
inherent limitations of LLMs' safety mechanisms and flaws in defense methods.
This work exposes significant vulnerabilities of safety measures against
learning-style elicitation, highlighting a critical challenge of balancing
helpfulness and safety alignments.

</details>


### [29] [Beyond Classification: Evaluating LLMs for Fine-Grained Automatic Malware Behavior Auditing](https://arxiv.org/abs/2509.14335)
*Xinran Zheng,Xingzhi Qian,Yiling He,Shuo Yang,Lorenzo Cavallaro*

Main category: cs.CR

TL;DR: MalEval是一个用于评估LLM在Android恶意软件审计中能力的框架，通过专家验证报告和静态可达性分析来解决现有挑战，并定义了四个审计任务来系统评估7个主流LLM的性能。


<details>
  <summary>Details</summary>
Motivation: 恶意软件行为审计需要可验证的解释和证据，但现有方法面临注释稀缺、良性代码干扰和LLM幻觉等问题。LLM在恶意软件审计中的潜力尚未被充分探索。

Method: 提出MalEval框架，包含专家验证报告、敏感API列表更新、静态可达性分析降噪，以及函数级结构表示作为可验证评估单元。定义了四个审计任务和相关指标。

Result: 对7个广泛使用的LLM进行了系统评估，揭示了在不同审计阶段的潜力和关键限制，提供了可复现的基准。

Conclusion: MalEval为LLM增强的恶意软件行为审计提供了基础框架和评估标准，展示了LLM在此领域的应用前景和当前局限性。

Abstract: Automated malware classification has achieved strong detection performance.
Yet, malware behavior auditing seeks causal and verifiable explanations of
malicious activities -- essential not only to reveal what malware does but also
to substantiate such claims with evidence. This task is challenging, as
adversarial intent is often hidden within complex, framework-heavy
applications, making manual auditing slow and costly. Large Language Models
(LLMs) could help address this gap, but their auditing potential remains
largely unexplored due to three limitations: (1) scarce fine-grained
annotations for fair assessment; (2) abundant benign code obscuring malicious
signals; and (3) unverifiable, hallucination-prone outputs undermining
attribution credibility. To close this gap, we introduce MalEval, a
comprehensive framework for fine-grained Android malware auditing, designed to
evaluate how effectively LLMs support auditing under real-world constraints.
MalEval provides expert-verified reports and an updated sensitive API list to
mitigate ground truth scarcity and reduce noise via static reachability
analysis. Function-level structural representations serve as intermediate
attribution units for verifiable evaluation. Building on this, we define four
analyst-aligned tasks -- function prioritization, evidence attribution,
behavior synthesis, and sample discrimination -- together with domain-specific
metrics and a unified workload-oriented score. We evaluate seven widely used
LLMs on a curated dataset of recent malware and misclassified benign apps,
offering the first systematic assessment of their auditing capabilities.
MalEval reveals both promising potential and critical limitations across audit
stages, providing a reproducible benchmark and foundation for future research
on LLM-enhanced malware behavior auditing. MalEval is publicly available at
https://github.com/ZhengXR930/MalEval.git

</details>


### [30] [LLM Jailbreak Detection for (Almost) Free!](https://arxiv.org/abs/2509.14558)
*Guorui Chen,Yifan Xia,Xiaojun Jia,Zhijiang Li,Philip Torr,Jindong Gu*

Main category: cs.CR

TL;DR: 基于输出分布差异的可能性，提出了一种免费的盗码攻击检测方法FJD，通过在输入前添加肯定指令并调整温度来区分盗码与善意提示，几乎无需额外计算成本。


<details>
  <summary>Details</summary>
Motivation: 虽然大语言模型通过对齐提升了安全性，但仍容易受到盗码攻击产生不适当内容。现有的盗码检测方法需要额外的模型或多次推理，计算成本较高。

Method: 提出FJD方法，在输入前添加肯定指令，通过调整温度来缩放输出分布差异，利用首个标记的信心度来区分盗码与善意提示。还通过虚拟指令学习来提升检测性能。

Result: 在对齐的大语言模型上进行了涉及广泛的实验，结果显示FJD能够有效地检测盗码提示，而且在模型推理过程中几乎不需要额外的计算成本。

Conclusion: FJD方法基于输出分布差异的发现，提供了一种高效但计算成本极低的盗码检测方案，通过简单的指令添加和温度调整就能显著提升检测性能。

Abstract: Large language models (LLMs) enhance security through alignment when widely
used, but remain susceptible to jailbreak attacks capable of producing
inappropriate content. Jailbreak detection methods show promise in mitigating
jailbreak attacks through the assistance of other models or multiple model
inferences. However, existing methods entail significant computational costs.
In this paper, we first present a finding that the difference in output
distributions between jailbreak and benign prompts can be employed for
detecting jailbreak prompts. Based on this finding, we propose a Free Jailbreak
Detection (FJD) which prepends an affirmative instruction to the input and
scales the logits by temperature to further distinguish between jailbreak and
benign prompts through the confidence of the first token. Furthermore, we
enhance the detection performance of FJD through the integration of virtual
instruction learning. Extensive experiments on aligned LLMs show that our FJD
can effectively detect jailbreak prompts with almost no additional
computational costs during LLM inference.

</details>


### [31] [What Gets Measured Gets Managed: Mitigating Supply Chain Attacks with a Link Integrity Management System](https://arxiv.org/abs/2509.14583)
*Johnny So,Michael Ferdman,Nick Nikiforakis*

Main category: cs.CR

TL;DR: LiMS是一个透明的Web资源完整性验证系统，通过可定制的完整性策略来声明和验证资源属性，以防御供应链攻击，在初始页面加载时仅有数百毫秒的开销。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏健壮的Web资源完整性验证方法，供应链攻击成为Web应用的主要攻击面，需要一种通用且高性能的解决方案。

Method: 设计LiMS系统，使用可定制的完整性策略声明资源预期属性，验证并强制执行这些策略，构建基础策略作为完整性保障的基础模块。

Result: 在450个代表性域名的测试中，系统在初始页面加载时仅有数百毫秒开销，重新加载时开销可忽略，且多个策略模块适合依赖使用模式，管理开销最小。

Conclusion: LiMS能够以最小开销引导显著的Web浏览会话安全改进，有效防御供应链攻击，适合实际部署使用。

Abstract: The web continues to grow, but dependency-monitoring tools and standards for
resource integrity lag behind. Currently, there exists no robust method to
verify the integrity of web resources, much less in a generalizable yet
performant manner, and supply chains remain one of the most targeted parts of
the attack surface of web applications.
  In this paper, we present the design of LiMS, a transparent system to
bootstrap link integrity guarantees in web browsing sessions with minimal
overhead. At its core, LiMS uses a set of customizable integrity policies to
declare the (un)expected properties of resources, verifies these policies, and
enforces them for website visitors. We discuss how basic integrity policies can
serve as building blocks for a comprehensive set of integrity policies, while
providing guarantees that would be sufficient to defend against recent supply
chain attacks detailed by security industry reports. Finally, we evaluate our
open-sourced prototype by simulating deployments on a representative sample of
450 domains that are diverse in ranking and category. We find that our proposal
offers the ability to bootstrap marked security improvements with an overall
overhead of hundreds of milliseconds on initial page loads, and negligible
overhead on reloads, regardless of network speeds. In addition, from examining
archived data for the sample sites, we find that several of the proposed policy
building blocks suit their dependency usage patterns, and would incur minimal
administrative overhead.

</details>


### [32] [ATLANTIS: AI-driven Threat Localization, Analysis, and Triage Intelligence System](https://arxiv.org/abs/2509.14589)
*Taesoo Kim,HyungSeok Han,Soyeon Park,Dae R. Jeong,Dohyeok Kim,Dongkwan Kim,Eunsoo Kim,Jiho Kim,Joshua Wang,Kangsu Kim,Sangwoo Ji,Woosun Song,Hanqing Zhao,Andrew Chin,Gyejin Lee,Kevin Stevens,Mansour Alharthi,Yizhuo Zhai,Cen Zhang,Joonun Jang,Yeongjin Jang,Ammar Askar,Dongju Kim,Fabian Fleischer,Jeongin Cho,Junsik Kim,Kyungjoon Ko,Insu Yun,Sangdon Park,Dowoo Baik,Haein Lee,Hyeon Heo,Minjae Gwon,Minjae Lee,Minwoo Baek,Seunggi Min,Wonyoung Kim,Yonghwi Jin,Younggi Park,Yunjae Choi,Jinho Jung,Gwanhyun Lee,Junyoung Jang,Kyuheon Kim,Yeonghyeon Cha,Youngjoon Kim*

Main category: cs.CR

TL;DR: ATLANTIS是DARPA AI Cyber Challenge冠军系统，结合大语言模型与程序分析技术，实现自动化漏洞发现和修复


<details>
  <summary>Details</summary>
Motivation: 解决现代软件漏洞发现和修复的速度与规模挑战，克服自动化安全分析在精度、覆盖范围和语义正确性方面的局限性

Method: 集成大语言模型(LLMs)与符号执行、定向模糊测试和静态分析等程序分析技术

Result: 在DARPA AIxCC决赛中获得第一名，能够跨C和Java等多样化代码库进行扩展，实现高精度和广泛覆盖

Conclusion: 成功展示了程序分析与现代AI结合在自动化安全领域的突破，为未来研究提供了可复现的框架和宝贵经验

Abstract: We present ATLANTIS, the cyber reasoning system developed by Team Atlanta
that won 1st place in the Final Competition of DARPA's AI Cyber Challenge
(AIxCC) at DEF CON 33 (August 2025). AIxCC (2023-2025) challenged teams to
build autonomous cyber reasoning systems capable of discovering and patching
vulnerabilities at the speed and scale of modern software. ATLANTIS integrates
large language models (LLMs) with program analysis -- combining symbolic
execution, directed fuzzing, and static analysis -- to address limitations in
automated vulnerability discovery and program repair. Developed by researchers
at Georgia Institute of Technology, Samsung Research, KAIST, and POSTECH, the
system addresses core challenges: scaling across diverse codebases from C to
Java, achieving high precision while maintaining broad coverage, and producing
semantically correct patches that preserve intended behavior. We detail the
design philosophy, architectural decisions, and implementation strategies
behind ATLANTIS, share lessons learned from pushing the boundaries of automated
security when program analysis meets modern AI, and release artifacts to
support reproducibility and future research.

</details>


### [33] [Threats and Security Strategies for IoMT Infusion Pumps](https://arxiv.org/abs/2509.14604)
*Ramazan Yener,Muhammad Hassan,Masooda Bashir*

Main category: cs.CR

TL;DR: 本研究分析了医疗物联网(IoMT)输液泵的网络安全漏洞，通过文献综述识别出设备级缺陷、认证访问控制问题、网络通信弱点、数据安全隐私风险和运营组织挑战等五类主要漏洞，为制定针对性安全策略提供参考。


<details>
  <summary>Details</summary>
Motivation: 医疗物联网(IoMT)在提升医疗服务质量的同时也扩大了网络攻击面，输液泵作为关键医疗设备存在严重安全漏洞，需要系统性地分析其网络安全脆弱性以保护患者安全。

Method: 通过对过去五年132篇论文的文献综述，筛选出7项相关研究进行深入分析，识别和分类输液泵的安全漏洞。

Result: 研究发现输液泵存在五类主要漏洞：设备级缺陷、认证和访问控制问题、网络通信弱点、数据安全隐私风险、运营组织挑战，这些漏洞可能导致医疗网络内的横向攻击。

Conclusion: 研究系统梳理了输液泵的安全漏洞模式，为医疗IT专业人员和设备制造商提供了结构化理解，有助于制定主动的安全策略来保护输液泵和患者安全。

Abstract: The integration of the Internet of Medical Things (IoMT) into healthcare
systems has transformed patient care by enabling real-time monitoring, enhanced
diagnostics, and enhanced operational efficiency. However, this increased
connectivity has also expanded the attack surface for cybercriminals, raising
significant cybersecurity and privacy concerns. This study focuses on the
cybersecurity vulnerabilities of IoMT infusion pumps, which are critical
devices in modern healthcare. Through a targeted literature review of the past
five years, we analyzed seven current studies from a pool of 132 papers to
identify security vulnerabilities. Our findings indicate that infusion pumps
face vulnerabilities such as device-level flaws, authentication and access
control issues, network and communication weaknesses, data security and privacy
risks, and operational or organizational challenges that can expose them to
lateral attacks within healthcare networks. Our analysis synthesizes findings
from seven recent studies to clarify how and why infusion pumps remain
vulnerable in each of these areas. By categorizing the security gaps, we
highlight critical risk patterns and their implications. This work underscores
the scope of the issue and provides a structured understanding that is valuable
for healthcare IT professionals and device manufacturers. Ultimately, the
findings can inform the development of targeted, proactive security strategies
to better safeguard infusion pumps and protect patient well-being.

</details>


### [34] [Enterprise AI Must Enforce Participant-Aware Access Control](https://arxiv.org/abs/2509.14608)
*Shashank Shreedhar Bhatt,Tanmay Rajore,Khushboo Aggarwal,Ganesh Ananthanarayanan,Ranveer Chandra,Nishanth Chandran,Suyash Choudhury,Divya Gupta,Emre Kiciman,Sumit Kumar Pandey,Srinath Setty,Rahul Sharma,Teijia Zhao*

Main category: cs.CR

TL;DR: 大语言模型在企业环境中存在敏感数据泄漏风险，现有防御措施无法可靠防范。本文提出了一种基于精细访问控制的框架，确保在训练和推理过程中严格执行权限管理。


<details>
  <summary>Details</summary>
Motivation: 企业部署的LLM模型通过微调和RAG技术内部化敏感数据，但现有安全措施无法可靠防止数据泄漏攻击，引发了严重的安全风险。

Method: 提出了一种确定性的精细访问控制框架，要求在训练、检索和生成过程中，所有使用的内容都必须明确授权给所有参与交互的用户。该方法基于经典访问控制原则，适配了现代AI工作流的特殊需求。

Result: 证明了现有防御方法（包括提示消毒、输出过滤、系统隔离和训练隐私机制）都是概率性的，无法提供稳健的保护。新框架已在Microsoft Copilot Tuning产品中部署应用。

Conclusion: 只有通过确定性的精细访问控制执行，才能可靠地防止敏感数据泄漏给未授权用户。该研究为建设安全的多用户LLM系统提供了简单但强大的范式转变。

Abstract: Large language models (LLMs) are increasingly deployed in enterprise settings
where they interact with multiple users and are trained or fine-tuned on
sensitive internal data. While fine-tuning enhances performance by
internalizing domain knowledge, it also introduces a critical security risk:
leakage of confidential training data to unauthorized users. These risks are
exacerbated when LLMs are combined with Retrieval-Augmented Generation (RAG)
pipelines that dynamically fetch contextual documents at inference time.
  We demonstrate data exfiltration attacks on AI assistants where adversaries
can exploit current fine-tuning and RAG architectures to leak sensitive
information by leveraging the lack of access control enforcement. We show that
existing defenses, including prompt sanitization, output filtering, system
isolation, and training-level privacy mechanisms, are fundamentally
probabilistic and fail to offer robust protection against such attacks.
  We take the position that only a deterministic and rigorous enforcement of
fine-grained access control during both fine-tuning and RAG-based inference can
reliably prevent the leakage of sensitive data to unauthorized recipients.
  We introduce a framework centered on the principle that any content used in
training, retrieval, or generation by an LLM is explicitly authorized for
\emph{all users involved in the interaction}. Our approach offers a simple yet
powerful paradigm shift for building secure multi-user LLM systems that are
grounded in classical access control but adapted to the unique challenges of
modern AI workflows. Our solution has been deployed in Microsoft Copilot
Tuning, a product offering that enables organizations to fine-tune models using
their own enterprise-specific data.

</details>


### [35] [Adversarial Distilled Retrieval-Augmented Guarding Model for Online Malicious Intent Detection](https://arxiv.org/abs/2509.14622)
*Yihao Guo,Haocheng Bian,Liutong Zhou,Ze Wang,Zhaoyi Zhang,Francois Kawala,Milan Dean,Ian Fischer,Yuantao Peng,Noyan Tokgozoglu,Ivan Barrientos,Riyaaz Shaik,Rachel Li,Chandru Venkataraman,Reza Shifteh Far,Moses Pawar,Venkat Sundaranatha,Michael Xu,Frank Chu*

Main category: cs.CR

TL;DR: ADRAG是一个两阶段框架，通过对抗蒸馏和检索增强技术实现高效实时的恶意意图检测，在保持高性能的同时显著降低延迟


<details>
  <summary>Details</summary>
Motivation: 现有方法无法实时处理多样复杂的用户查询，需要开发既能保证检测效果又能满足实时性要求的恶意意图检测系统

Method: 两阶段框架：训练阶段使用对抗扰动和检索增强的高容量教师模型学习鲁棒决策边界；推理阶段通过蒸馏调度器将知识转移到紧凑学生模型，并利用在线更新的知识库进行实时检测

Result: 在10个安全基准测试中，149M参数的ADRAG达到WildGuard-7B 98.5%的性能，OOD检测超越GPT-4 3.3%和Llama-Guard-3-8B 9.5%，延迟降低5.6倍，支持300 QPS

Conclusion: ADRAG证明了通过知识蒸馏和检索增强技术可以在保持检测性能的同时实现高效的实时恶意意图检测，为LLM交互应用提供了可行的安全防护方案

Abstract: With the deployment of Large Language Models (LLMs) in interactive
applications, online malicious intent detection has become increasingly
critical. However, existing approaches fall short of handling diverse and
complex user queries in real time. To address these challenges, we introduce
ADRAG (Adversarial Distilled Retrieval-Augmented Guard), a two-stage framework
for robust and efficient online malicious intent detection. In the training
stage, a high-capacity teacher model is trained on adversarially perturbed,
retrieval-augmented inputs to learn robust decision boundaries over diverse and
complex user queries. In the inference stage, a distillation scheduler
transfers the teacher's knowledge into a compact student model, with a
continually updated knowledge base collected online. At deployment, the compact
student model leverages top-K similar safety exemplars retrieved from the
online-updated knowledge base to enable both online and real-time malicious
query detection. Evaluations across ten safety benchmarks demonstrate that
ADRAG, with a 149M-parameter model, achieves 98.5% of WildGuard-7B's
performance, surpasses GPT-4 by 3.3% and Llama-Guard-3-8B by 9.5% on
out-of-distribution detection, while simultaneously delivering up to 5.6x lower
latency at 300 queries per second (QPS) in real-time applications.

</details>


### [36] [Threat Modeling for Enhancing Security of IoT Audio Classification Devices under a Secure Protocols Framework](https://arxiv.org/abs/2509.14657)
*Sergio Benlloch-Lopez,Miquel Viel-Vazquez,Javier Naranjo-Alcazar,Jordi Grau-Haro,Pedro Zuccarello*

Main category: cs.CR

TL;DR: 这篇论文提出了一种深度防御的IoT音频分析安全架构，通过TPM远程验证、TLS 1.3加密和后量子安全等技术，从启动、数据传输到存储全链路保护敏感音频数据。


<details>
  <summary>Details</summary>
Motivation: 解决IoT节点在资源约束下处理敏感音频数据时的安全风险，防止恶意设备或被篡改设备活动。

Method: 采用三个信任域架构，结合TPM远程验证、TLS 1.3加密、Kyber和Dilithium后量子算法、LUKS磁盘加密、简洁AI模型等技术，并通过STRIDE威胁模型和攻击树分析指导设计。

Result: 构建了一个完整的安全协议，能够确保只有经过验证的设备才能启动，数据在传输和存储时都得到加密保护，具有后量子安全性。

Conclusion: 该方案为资源受限的IoT音频分析设备提供了全面的安全保护，通过深度防御架构有效防范各种安全威胁。

Abstract: The rapid proliferation of IoT nodes equipped with microphones and capable of
performing on-device audio classification exposes highly sensitive data while
operating under tight resource constraints. To protect against this, we present
a defence-in-depth architecture comprising a security protocol that treats the
edge device, cellular network and cloud backend as three separate trust
domains, linked by TPM-based remote attestation and mutually authenticated TLS
1.3. A STRIDE-driven threat model and attack-tree analysis guide the design. At
startup, each boot stage is measured into TPM PCRs. The node can only decrypt
its LUKS-sealed partitions after the cloud has verified a TPM quote and
released a one-time unlock key. This ensures that rogue or tampered devices
remain inert. Data in transit is protected by TLS 1.3 and hybridised with Kyber
and Dilithium to provide post-quantum resilience. Meanwhile, end-to-end
encryption and integrity hashes safeguard extracted audio features. Signed,
rollback-protected AI models and tamper-responsive sensors harden firmware and
hardware. Data at rest follows a 3-2-1 strategy comprising a solid-state drive
sealed with LUKS, an offline cold archive encrypted with a hybrid post-quantum
cipher and an encrypted cloud replica. Finally, we set out a plan for
evaluating the physical and logical security of the proposed protocol.

</details>


### [37] [Security Analysis of Web Applications Based on Gruyere](https://arxiv.org/abs/2509.14706)
*Yonghao Ni,Zhongwen Li,Xiaoqi Li*

Main category: cs.CR

TL;DR: 这篇论文通过分析OWASP Top 10和Gruyere平台的漏洞，探讨了网络安全漏洞的类型、机制和防御策略，证明了过时漏洞原理在现代安全分析中仍具有重要价值。


<details>
  <summary>Details</summary>
Motivation: 网络系统安全漏洞不断出现，对数据保护、隐私保护和业务连续性造成威胁，需要系统性研究来提升网络系统的可靠性和健壮性。

Method: 首先回顾OWASP Top 10并总结常见漏洞类型，然后使用Gruyere平台作为实验对象进行漏洞分析，提供详细的复现步骤和美比现实案例。

Result: 研究发现Gruyere平台的漏洞虽然较为过时，但其基础原理在解释现代安全漏洞时仍具有重要价值，能够提供实用的安全防御支持。

Conclusion: 基于Gruyere的网络系统安全分析既能深化对漏洞机制的理解，也为技术创新和安全防御提供了实践支持。

Abstract: With the rapid development of Internet technologies, web systems have become
essential infrastructures for modern information exchange and business
operations. However, alongside their expansion, numerous security
vulnerabilities have emerged, making web security a critical research focus
within the broader field of cybersecurity. These issues are closely related to
data protection, privacy preservation, and business continuity, and systematic
research on web security is crucial for mitigating malicious attacks and
enhancing the reliability and robustness of network systems. This paper first
reviews the OWASP Top 10, summarizing the types, causes, and impacts of common
web vulnerabilities, and illustrates their exploitation mechanisms through
representative cases. Building upon this, the Gruyere platform is adopted as an
experimental subject for analyzing known vulnerabilities. The study presents
detailed reproduction steps for specific vulnerabilities, proposes
comprehensive remediation strategies, and further compares Gruyere's
vulnerabilities with contemporary real-world cases. The findings suggest that,
although Gruyere's vulnerabilities are relatively outdated, their underlying
principles remain highly relevant for explaining a wide range of modern
security flaws. Overall, this research demonstrates that web system security
analysis based on Gruyere not only deepens the understanding of vulnerability
mechanisms but also provides practical support for technological innovation and
security defense.

</details>


### [38] [Variables Ordering Optimization in Boolean Characteristic Set Method Using Simulated Annealing and Machine Learning-based Time Prediction](https://arxiv.org/abs/2509.14754)
*Minzhong Luo,Yudong Sun,Yin Long*

Main category: cs.CR

TL;DR: 通过机器学习预测和模拟逆冷算法优化变量排序，显著提升布尔特征集算法的性能，尤其在大规模系统中优势明显


<details>
  <summary>Details</summary>
Motivation: 布尔特征集(BCS)算法的性能对变量排序顺序极其敏感，不同排序导致解法时间差异巨大，需要查找高性能的变量排序

Method: 构建变量频谱和解法时间的数据集，训练机器学习预测器估计解法时间，使用模拟逆冷算法以预测器为成本函数快速发现优科排序

Result: 在大规模系统(n=32)中显著超过标准BCS算法、Gröbner基方法和SAT求解器，并通过随机过程理论得到概率时间复杂度界

Conclusion: 该方法为代数动力学分析提供了实用的加速工具，同时为机器学习增强的组合最优化在符号计算中建立了理论基础

Abstract: Solving systems of Boolean equations is a fundamental task in symbolic
computation and algebraic cryptanalysis, with wide-ranging applications in
cryptography, coding theory, and formal verification. Among existing
approaches, the Boolean Characteristic Set (BCS) method[1] has emerged as one
of the most efficient algorithms for tackling such problems. However, its
performance is highly sensitive to the ordering of variables, with solving
times varying drastically under different orderings for fixed variable counts n
and equations size m. To address this challenge, this paper introduces a novel
optimization framework that synergistically integrates machine learning
(ML)-based time prediction with simulated annealing (SA) to efficiently
identify high-performance variables orderings. Weconstruct a dataset comprising
variable frequency spectrum X and corresponding BCS solving time t for
benchmark systems(e.g., n = m = 28). Utilizing this data, we train an accurate
ML predictor ft(X) to estimate solving time for any given variables ordering.
For each target system, ft serves as the cost function within an SA algorithm,
enabling rapid discovery of low-latency orderings that significantly expedite
subsequent BCS execution. Extensive experiments demonstrate that our method
substantially outperforms the standard BCS algorithm[1], Gr\"obner basis method
[2] and SAT solver[3], particularly for larger-scale systems(e.g., n = 32).
Furthermore, we derive probabilistic time complexity bounds for the overall
algorithm using stochastic process theory, establishing a quantitative
relationship between predictor accuracy and expected solving complexity. This
work provides both a practical acceleration tool for algebraic cryptanalysis
and a theoretical foundation for ML-enhanced combinatorial optimization in
symbolic computation.

</details>


### [39] [Blockchain-Enabled Explainable AI for Trusted Healthcare Systems](https://arxiv.org/abs/2509.14987)
*Md Talha Mohsin*

Main category: cs.CR

TL;DR: 提出区块链集成可解释AI框架(BXHF)，解决医疗数据安全交换和AI决策可解释性问题，通过区块链确保数据不可篡改，XAI提供透明预测，采用混合边缘云架构支持联邦计算。


<details>
  <summary>Details</summary>
Motivation: 解决医疗信息系统面临的两个关键挑战：安全的数据交换和可理解的AI临床决策，需要同时保证数据级信任和决策级信任。

Method: 结合区块链技术确保患者记录的不可变性、可审计性和防篡改性，集成可解释AI(XAI)方法提供透明且临床相关的模型预测，采用混合边缘云架构支持跨机构联邦计算。

Result: 框架适用于跨境临床研究网络、罕见病检测和高风险干预决策支持等场景，通过确保透明度、可审计性和监管合规性提升AI在医疗领域的可信度。

Conclusion: BXHF框架为更安全可靠的临床决策奠定了基础，通过整合安全保障和可解释性要求，提高了AI在医疗保健中的可信度、采纳率和有效性。

Abstract: This paper introduces a Blockchain-Integrated Explainable AI Framework (BXHF)
for healthcare systems to tackle two essential challenges confronting health
information networks: safe data exchange and comprehensible AI-driven clinical
decision-making. Our architecture incorporates blockchain, ensuring patient
records are immutable, auditable, and tamper-proof, alongside Explainable AI
(XAI) methodologies that yield transparent and clinically relevant model
predictions. By incorporating security assurances and interpretability
requirements into a unified optimization pipeline, BXHF ensures both data-level
trust (by verified and encrypted record sharing) and decision-level trust (with
auditable and clinically aligned explanations). Its hybrid edge-cloud
architecture allows for federated computation across different institutions,
enabling collaborative analytics while protecting patient privacy. We
demonstrate the framework's applicability through use cases such as
cross-border clinical research networks, uncommon illness detection and
high-risk intervention decision support. By ensuring transparency,
auditability, and regulatory compliance, BXHF improves the credibility, uptake,
and effectiveness of AI in healthcare, laying the groundwork for safer and more
reliable clinical decision-making.

</details>


### [40] [Watermarking and Anomaly Detection in Machine Learning Models for LORA RF Fingerprinting](https://arxiv.org/abs/2509.15170)
*Aarushi Mahajan,Wayne Burleson*

Main category: cs.CR

TL;DR: 基于ResNet-34和变分自动编码器的强化无线电指纹识别系统，结合水印技术和异常检测，提高了模型的防欺骗和所有权保护能力


<details>
  <summary>Details</summary>
Motivation: 解决深度学习在无线电指纹识别中模型易被拷贝、篡改和避免的安全漏洞

Method: 使用ResNet-34处理log-Mel谱图，嵌入三种水印：简单触发器、反对性训练的噪声和筛波恰恢的触发器、隐藏的梯度/权重签名。使用卷积变分自动编码器(VAE)进行异常检测

Result: 在LoRa数据集上达到94.6%的准确率、98%的水印成功率和0.94的AUROC值

Conclusion: 该系统提供了可验证、防篡改的认证方案，有效解决了RFFI模型的安全问题

Abstract: Radio frequency fingerprint identification (RFFI) distinguishes wireless
devices by the small variations in their analog circuits, avoiding heavy
cryptographic authentication. While deep learning on spectrograms improves
accuracy, models remain vulnerable to copying, tampering, and evasion. We
present a stronger RFFI system combining watermarking for ownership proof and
anomaly detection for spotting suspicious inputs. Using a ResNet-34 on log-Mel
spectrograms, we embed three watermarks: a simple trigger, an adversarially
trained trigger robust to noise and filtering, and a hidden gradient/weight
signature. A convolutional Variational Autoencoders (VAE) with Kullback-Leibler
(KL) warm-up and free-bits flags off-distribution queries. On the LoRa dataset,
our system achieves 94.6% accuracy, 98% watermark success, and 0.94 AUROC,
offering verifiable, tamper-resistant authentication.

</details>


### [41] [Beyond Surface Alignment: Rebuilding LLMs Safety Mechanism via Probabilistically Ablating Refusal Direction](https://arxiv.org/abs/2509.15202)
*Yuanbo Xie,Yingjie Zhang,Tianyun Liu,Duohe Ma,Tingwen Liu*

Main category: cs.CR

TL;DR: DeepRefusal是一个针对LLM越狱攻击的鲁棒安全对齐框架，通过概率性消融拒绝方向来动态重建拒绝机制，显著降低攻击成功率约95%


<details>
  <summary>Details</summary>
Motivation: 现有安全对齐方法存在安全对齐深度不足和内部防御机制不鲁棒的问题，容易受到预填充和拒绝方向操纵等对抗攻击

Method: 在微调过程中概率性地跨层和token深度消融拒绝方向，强制模型从越狱状态动态重建其拒绝机制

Result: 在四个开源LLM家族和六种代表性攻击上的广泛评估显示，攻击成功率降低约95%，同时模型能力保持良好，性能下降最小

Conclusion: DeepRefusal框架有效解决了现有安全对齐方法的局限性，提供了对预填充、拒绝方向攻击及其他未见越狱策略的强大防御能力

Abstract: Jailbreak attacks pose persistent threats to large language models (LLMs).
Current safety alignment methods have attempted to address these issues, but
they experience two significant limitations: insufficient safety alignment
depth and unrobust internal defense mechanisms. These limitations make them
vulnerable to adversarial attacks such as prefilling and refusal direction
manipulation. We introduce DeepRefusal, a robust safety alignment framework
that overcomes these issues. DeepRefusal forces the model to dynamically
rebuild its refusal mechanisms from jailbreak states. This is achieved by
probabilistically ablating the refusal direction across layers and token depths
during fine-tuning. Our method not only defends against prefilling and refusal
direction attacks but also demonstrates strong resilience against other unseen
jailbreak strategies. Extensive evaluations on four open-source LLM families
and six representative attacks show that DeepRefusal reduces attack success
rates by approximately 95%, while maintaining model capabilities with minimal
performance degradation.

</details>


### [42] [Evil Vizier: Vulnerabilities of LLM-Integrated XR Systems](https://arxiv.org/abs/2509.15213)
*Yicheng Zhang,Zijian Huang,Sophie Chen,Erfan Shayegani,Jiasi Chen,Nael Abu-Ghazaleh*

Main category: cs.CR

TL;DR: 这篇论文分析了集成大语言模型的XR应用系统的安全漏洞，弥补了当前研究中的空白，通过实验验证了多种攻击方式，并提出了防御建议。


<details>
  <summary>Details</summary>
Motivation: 随着XR应用越来越多地集成LLM来提升用户体验，这种集成也带来了新的安全风险。当前研究对这些安全威胁的认知存在空白，需要系统性的安全分析和防御措施。

Method: 首先对文献中和实践中的LLM集成XR系统进行分析和分类，确定共同的威胁模型，然后在多个XR平台（Meta Quest 3、Meta Ray-Ban、Android、Microsoft HoloLens 2）上进行一系列原型攻击验证，使用了包括Llama和GPT在内的各种LLM模型。

Result: 发现所有平台都存在共同的漏洞：攻击者可以修改公共环境上下文来影响LLM查询，导致错误的视觉或听觉反馈，威胁用户安全和隐私，造成混乱或其他害处。

Conclusion: 论文提出了一些缩减威胁的策略和最佳实践，包括一个初步的防御原型，并呼吁社区开发新的保护机制来应对这些风险。

Abstract: Extended reality (XR) applications increasingly integrate Large Language
Models (LLMs) to enhance user experience, scene understanding, and even
generate executable XR content, and are often called "AI glasses". Despite
these potential benefits, the integrated XR-LLM pipeline makes XR applications
vulnerable to new forms of attacks. In this paper, we analyze LLM-Integated XR
systems in the literature and in practice and categorize them along different
dimensions from a systems perspective. Building on this categorization, we
identify a common threat model and demonstrate a series of proof-of-concept
attacks on multiple XR platforms that employ various LLM models (Meta Quest 3,
Meta Ray-Ban, Android, and Microsoft HoloLens 2 running Llama and GPT models).
Although these platforms each implement LLM integration differently, they share
vulnerabilities where an attacker can modify the public context surrounding a
legitimate LLM query, resulting in erroneous visual or auditory feedback to
users, thus compromising their safety or privacy, sowing confusion, or other
harmful effects. To defend against these threats, we discuss mitigation
strategies and best practices for developers, including an initial defense
prototype, and call on the community to develop new protection mechanisms to
mitigate these risks.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [43] [Unified Crew Planning and Replanning Optimization in Multi-Line Metro Systems Considering Workforce Heterogeneity](https://arxiv.org/abs/2509.14251)
*Qihang Chen*

Main category: cs.AI

TL;DR: 基于层次时空网络模型的统一优化框架，解决多线路地铁班组规划和紧急重新规划问题，通过创新算法实现成本优化和任务完成率提升。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要集中在单个地铁线路，缺乏多线路协调和紧急情况下的快速重新规划能力，影响公共交通运营效率和服务可靠性。

Method: 提出层次时空网络模型表示统一的员工动作空间，引入计算高效的约束条件和数学形式，并发展基于列生成和最短路径调整的求解算法。

Result: 上海和北京地铁实际数据实验显示，该方法在成本减少和任务完成方面超过基准算法，通过多线路协同操作特别在故障情况下实现显著效率提升。

Conclusion: 该研究强调全局优化和多线路协调在地铁系统运营中的重要作用，为智慧城市公共交通的高效可靠运行提供了有价值的见解。

Abstract: Metro crew planning is a key component of smart city development as it
directly impacts the operational efficiency and service reliability of public
transportation. With the rapid expansion of metro networks, effective
multi-line scheduling and emergency management have become essential for
large-scale seamless operations. However, current research focuses primarily on
individual metro lines,with insufficient attention on cross-line coordination
and rapid replanning during disruptions. Here, a unified optimization framework
is presented for multi-line metro crew planning and replanning with
heterogeneous workforce. Specifically, a hierarchical time-space network model
is proposed to represent the unified crew action space, and computationally
efficient constraints and formulations are derived for the crew's heterogeneous
qualifications and preferences. Solution algorithms based on column generation
and shortest path adjustment are further developed, utilizing the proposed
network model. Experiments with real data from Shanghai and Beijing Metro
demonstrate that the proposed methods outperform benchmark heuristics in both
cost reduction and task completion,and achieve notable efficiency gains by
incorporating cross-line operations, particularly for urgent tasks during
disruptions. This work highlights the role of global optimization and
cross-line coordination in multi-line metro system operations, providing
insights into the efficient and reliable functioning of public transportation
in smart cities.

</details>


### [44] [From Capabilities to Performance: Evaluating Key Functional Properties of LLM Architectures in Penetration Testing](https://arxiv.org/abs/2509.14289)
*Lanxiao Huang,Daksh Dave,Ming Jin,Tyler Cody,Peter Beling*

Main category: cs.AI

TL;DR: 大语言模型在渗透测试中的效果评估，研究了不同架构设计和五种核心功能增强对性能的影响


<details>
  <summary>Details</summary>
Motivation: 评估LLM在渗透测试中的效果和可靠性，明确不同攻击阶段的表现

Method: 通过实验测量多个LLM基于的渗透测试组件的经验性能能，分析失败模式，并通过有针对性的功能增强来定量评估五种核心功能的影响

Result: 目标性的功能增强显著提升了模块化组件的性能，尤其是在复杂、多步骤和实时渗透测试任务中

Conclusion: 虽然某些架构本身就具有部分这些特性，但有针对性的功能增强能够实质性改善LLM在渗透测试中的表现

Abstract: Large language models (LLMs) are increasingly used to automate or augment
penetration testing, but their effectiveness and reliability across attack
phases remain unclear. We present a comprehensive evaluation of multiple
LLM-based agents, from single-agent to modular designs, across realistic
penetration testing scenarios, measuring empirical performance and recurring
failure patterns. We also isolate the impact of five core functional
capabilities via targeted augmentations: Global Context Memory (GCM),
Inter-Agent Messaging (IAM), Context-Conditioned Invocation (CCI), Adaptive
Planning (AP), and Real-Time Monitoring (RTM). These interventions support,
respectively: (i) context coherence and retention, (ii) inter-component
coordination and state management, (iii) tool use accuracy and selective
execution, (iv) multi-step strategic planning, error detection, and recovery,
and (v) real-time dynamic responsiveness. Our results show that while some
architectures natively exhibit subsets of these properties, targeted
augmentations substantially improve modular agent performance, especially in
complex, multi-step, and real-time penetration testing tasks.

</details>


### [45] [Detecting Pipeline Failures through Fine-Grained Analysis of Web Agents](https://arxiv.org/abs/2509.14382)
*Daniel Röder,Akhil Juneja,Roland Roller,Sven Schmeier*

Main category: cs.AI

TL;DR: 这篇论文提出了一种模块化评估框架，用于详细分析LLM网页自主代理的中间错误，充实了现有测试指标的不足。


<details>
  <summary>Details</summary>
Motivation: 现有对LLM网页代理的评估主要关注整体成功率，忽视了中间错误，限刻了对失败模式的深入理解和系统性改进。

Method: 提出模块化评估框架，将代理管道解构为可解释的阶段进行详细错误分析，以SeeAct框架和Mind2Web数据集为案例进行实验。

Result: 该方法发现了标准指标没有发现的可操作弱点，揭示了更多可行的改进方向。

Conclusion: 通过模块化评估框架进行细粒度错误分析，能够推动更健壮和可推广的网页自主代理的发展。

Abstract: Web agents powered by large language models (LLMs) can autonomously perform
complex, multistep tasks in dynamic web environments. However, current
evaluations mostly focus on the overall success while overlooking intermediate
errors. This limits insight into failure modes and hinders systematic
improvement. This work analyzes existing benchmarks and highlights the lack of
fine-grained diagnostic tools. To address this gap, we propose a modular
evaluation framework that decomposes agent pipelines into interpretable stages
for detailed error analysis. Using the SeeAct framework and the Mind2Web
dataset as a case study, we show how this approach reveals actionable
weaknesses missed by standard metrics - paving the way for more robust and
generalizable web agents.

</details>


### [46] [VCBench: Benchmarking LLMs in Venture Capital](https://arxiv.org/abs/2509.14448)
*Rick Chen,Joseph Ternasky,Afriyie Samuel Kwesi,Ben Griffin,Aaron Ontoyin Yin,Zakari Salifu,Kelvin Amoaba,Xianling Mu,Fuat Alican,Yigit Ihlamur*

Main category: cs.AI

TL;DR: VCBench是首个用于预测风险投资创始人成功的基准测试，包含9000个匿名创始人档案，评估显示LLM模型表现优于人类基准和顶级投资机构


<details>
  <summary>Details</summary>
Motivation: 现有基准如SWE-bench和ARC-AGI推动了AGI发展，但在风险投资领域缺乏标准化评估基准，该领域信号稀疏、结果不确定，需要建立可重现的评估标准

Method: 创建包含9000个匿名创始人档案的VCBench数据集，标准化处理以保留预测特征同时防止身份泄露，采用对抗测试降低90%以上的重识别风险，评估9个最先进的大语言模型

Result: 市场指数精度1.9%，Y Combinator比指数好1.7倍，顶级机构好2.9倍。DeepSeek-V3达到基线精度6倍以上，GPT-4o获得最高F0.5分数，大多数模型超越人类基准

Conclusion: VCBench作为公开演进资源，为早期风险预测领域的AGI评估建立了社区驱动的、可重现且保护隐私的标准

Abstract: Benchmarks such as SWE-bench and ARC-AGI demonstrate how shared datasets
accelerate progress toward artificial general intelligence (AGI). We introduce
VCBench, the first benchmark for predicting founder success in venture capital
(VC), a domain where signals are sparse, outcomes are uncertain, and even top
investors perform modestly. At inception, the market index achieves a precision
of 1.9%. Y Combinator outperforms the index by a factor of 1.7x, while tier-1
firms are 2.9x better. VCBench provides 9,000 anonymized founder profiles,
standardized to preserve predictive features while resisting identity leakage,
with adversarial tests showing more than 90% reduction in re-identification
risk. We evaluate nine state-of-the-art large language models (LLMs).
DeepSeek-V3 delivers over six times the baseline precision, GPT-4o achieves the
highest F0.5, and most models surpass human benchmarks. Designed as a public
and evolving resource available at vcbench.com, VCBench establishes a
community-driven standard for reproducible and privacy-preserving evaluation of
AGI in early-stage venture forecasting.

</details>


### [47] [From Mimicry to True Intelligence (TI) -- A New Paradigm for Artificial General Intelligence](https://arxiv.org/abs/2509.14474)
*Meltem Subasioglu,Nevzat Subasioglu*

Main category: cs.AI

TL;DR: 该论文提出了一种基于认知机制的真正智能(TI)定义，包含六个核心组件，并建立了五级AGI分类评价体系，为AGI研究提供了明确的发展路径。


<details>
  <summary>Details</summary>
Motivation: 当前基于表现的AGI定义无法提供清晰的机制化研究路线图，也无法正确定义真正智能的本质特征。

Method: 受人脑启发，提出真正智能(TI)的六大核心组件：体验感官融合、核心指令、动态模式创建、高度联网多专家架构、协调层和不可测量的联结性。建立五级AGI分类评价体系。

Result: 提供了一个基于机制的全面AGI定义，为研究社区提供了明确可行的发展路径和重要里程碑。

Conclusion: 实现了六大组件中前五个可测量组件的等级五AGI系统，在功能和实践上等同于真正智能(TI)，只剩下意识问题作为哲学辨论。

Abstract: The debate around Artificial General Intelligence (AGI) remains open due to
two fundamentally different goals: replicating human-like performance versus
replicating human-like cognitive processes. We argue that current
performance-based definitions are inadequate because they provide no clear,
mechanism-focused roadmap for research, and they fail to properly define the
qualitative nature of genuine intelligence. Drawing inspiration from the human
brain, we propose a new paradigm that shifts the focus from external mimicry to
the development of foundational cognitive architectures. We define True
Intelligence (TI) as a system characterized by six core components: embodied
sensory fusion, core directives, dynamic schemata creation, a
highly-interconnected multi-expert architecture, an orchestration layer, and
lastly, the unmeasurable quality of Interconnectedness, which we hypothesize
results in consciousness and a subjective experience. We propose a practical,
five-level taxonomy of AGI based on the number of the first five measurable
components a system exhibits. This framework provides a clear path forward with
developmental milestones that directly address the challenge of building
genuinely intelligent systems. We contend that once a system achieves Level-5
AGI by implementing all five measurable components, the difference between it
and TI remains as a purely philosophical debate. For practical purposes - and
given theories indicate consciousness is an emergent byproduct of integrated,
higher-order cognition - we conclude that a fifth-level AGI is functionally and
practically equivalent to TI. This work synthesizes diverse insights from
analytical psychology, schema theory, metacognition, modern brain architectures
and latest works in AI to provide the first holistic, mechanism-based
definition of AGI that offers a clear and actionable path for the research
community.

</details>


### [48] [Beyond the high score: Prosocial ability profiles of multi-agent populations](https://arxiv.org/abs/2509.14485)
*Marko Tesic,Yue Zhao,Joel Z. Leibo,Rakshit S. Trivedi,Jose Hernandez-Orallo*

Main category: cs.AI

TL;DR: 采用贝叶斯测量布局方法分析Melting Pot竞赛中多段系统的能力涉透，发现高分代理并非总是具有更强合作能力，而某些顶级解决方案可能优化了非合作场景的表现。


<details>
  <summary>Details</summary>
Motivation: 评估AI系统的社交能力需要复杂环境，但像约定遵循这样的抽象行为在训练和评估中难以控制。Melting Pot竞赛作为社交AI评估套件，需要更好地理解多段系统的能力涉透。

Method: 采用贝叶斯测量布局方法（Measurement Layouts）来推断Melting Pot竞赛中多段系统的能力涉透，分析其预测能力和社交能力。

Result: 能力涉透不仅能预测未来表现，还能揭示代理的社交能力。高社交能力并非总与更好表现相关，某些低分代理反而显示出更强合作能力。顶级提交更容易在不需要社交能力的场景中获得高分。

Conclusion: 测量布局方法提供了高准确预测和可操作的见解，有助于在复杂社交环境中更透明、可推广地评估AI系统。需要改进合作需求标注方法，考虑不同测试环境引入的偏差。

Abstract: The development and evaluation of social capabilities in AI agents require
complex environments where competitive and cooperative behaviours naturally
emerge. While game-theoretic properties can explain why certain teams or agent
populations outperform others, more abstract behaviours, such as convention
following, are harder to control in training and evaluation settings. The
Melting Pot contest is a social AI evaluation suite designed to assess the
cooperation capabilities of AI systems. In this paper, we apply a Bayesian
approach known as Measurement Layouts to infer the capability profiles of
multi-agent systems in the Melting Pot contest. We show that these capability
profiles not only predict future performance within the Melting Pot suite but
also reveal the underlying prosocial abilities of agents. Our analysis
indicates that while higher prosocial capabilities sometimes correlate with
better performance, this is not a universal trend-some lower-scoring agents
exhibit stronger cooperation abilities. Furthermore, we find that
top-performing contest submissions are more likely to achieve high scores in
scenarios where prosocial capabilities are not required. These findings,
together with reports that the contest winner used a hard-coded solution
tailored to specific environments, suggest that at least one top-performing
team may have optimised for conditions where cooperation was not necessary,
potentially exploiting limitations in the evaluation framework. We provide
recommendations for improving the annotation of cooperation demands and propose
future research directions to account for biases introduced by different
testing environments. Our results demonstrate that Measurement Layouts offer
both strong predictive accuracy and actionable insights, contributing to a more
transparent and generalisable approach to evaluating AI systems in complex
social settings.

</details>


### [49] [DeKeyNLU: Enhancing Natural Language to SQL Generation through Task Decomposition and Keyword Extraction](https://arxiv.org/abs/2509.14507)
*Jian Chen,Zhenyan Chen,Xuming Hu,Peilin Zhou,Yining Hua,Han Fang,Cissy Hing Yee Choy,Xinmei Ke,Jingfeng Luo,Zixuan Yuan*

Main category: cs.AI

TL;DR: 这篇论文提出了DeKeyNLU数据集和DeKeySQL流水线，通过精确的任务分解和关键词提取来提高NL2SQL的性能。在BIRD和Spider数据集上都取得了显著的收益。


<details>
  <summary>Details</summary>
Motivation: 解决现有NL2SQL模型在任务分解和关键词提取方面的不精确问题，这些问题导致SQL生成错误。现有数据集存在任务过度分割和缺乏领域特定关键词注释的限制。

Method: 构建了含有1,500个精心注释QA对的DeKeyNLU数据集，用于精炼任务分解和关键词提取。基于此提出DeKeySQL流水线，包含三个模块：用户问题理解、实体检索和SQL生成。

Result: 在BIRD数据集上准确率从62.31%提升到69.10%，在Spider数据集上从84.2%提升到88.7%，显著提高了SQL生成的准确性。

Conclusion: DeKeyNLU数据集和DeKeySQL流水线有效解决了NL2SQL中的任务分解和关键词提取问题，显著提高了SQL生成的性能。

Abstract: Natural Language to SQL (NL2SQL) provides a new model-centric paradigm that
simplifies database access for non-technical users by converting natural
language queries into SQL commands. Recent advancements, particularly those
integrating Retrieval-Augmented Generation (RAG) and Chain-of-Thought (CoT)
reasoning, have made significant strides in enhancing NL2SQL performance.
However, challenges such as inaccurate task decomposition and keyword
extraction by LLMs remain major bottlenecks, often leading to errors in SQL
generation. While existing datasets aim to mitigate these issues by fine-tuning
models, they struggle with over-fragmentation of tasks and lack of
domain-specific keyword annotations, limiting their effectiveness. To address
these limitations, we present DeKeyNLU, a novel dataset which contains 1,500
meticulously annotated QA pairs aimed at refining task decomposition and
enhancing keyword extraction precision for the RAG pipeline. Fine-tuned with
DeKeyNLU, we propose DeKeySQL, a RAG-based NL2SQL pipeline that employs three
distinct modules for user question understanding, entity retrieval, and
generation to improve SQL generation accuracy. We benchmarked multiple model
configurations within DeKeySQL RAG pipeline. Experimental results demonstrate
that fine-tuning with DeKeyNLU significantly improves SQL generation accuracy
on both BIRD (62.31% to 69.10%) and Spider (84.2% to 88.7%) dev datasets.

</details>


### [50] [Rationality Check! Benchmarking the Rationality of Large Language Models](https://arxiv.org/abs/2509.14546)
*Zhilun Zhou,Jing Yi Wang,Nicholas Sukiennik,Chen Gao,Fengli Xu,Yong Li,James Evans*

Main category: cs.AI

TL;DR: 这是首个评估大语言模型全面理性的基准，包含多领域测试工具和实验分析，用于分析LLM与人类理性的差异


<details>
  <summary>Details</summary>
Motivation: 随着LLM在各应用领域模拟人类行为，需要评估它们是否及在何种情况下能像真实人类一样思考和行动，理性是评估人类行为的核心概念

Method: 开发了一个易于使用的工具套件，涉及多个领域的测试，包括理论理性（思考）和实践理性（行动）两个方面

Result: 获得了广泛的实验结果和分析，明确了LLM与理想化人类理性的涉及区别，评估了多个不同的LLM模型

Conclusion: 该基准可作为LLM开发者和使用者的基础工具，为评估大语言模型的理性表现提供了系统化的方法

Abstract: Large language models (LLMs), a recent advance in deep learning and machine
intelligence, have manifested astonishing capacities, now considered among the
most promising for artificial general intelligence. With human-like
capabilities, LLMs have been used to simulate humans and serve as AI assistants
across many applications. As a result, great concern has arisen about whether
and under what circumstances LLMs think and behave like real human agents.
Rationality is among the most important concepts in assessing human behavior,
both in thinking (i.e., theoretical rationality) and in taking action (i.e.,
practical rationality). In this work, we propose the first benchmark for
evaluating the omnibus rationality of LLMs, covering a wide range of domains
and LLMs. The benchmark includes an easy-to-use toolkit, extensive experimental
results, and analysis that illuminates where LLMs converge and diverge from
idealized human rationality. We believe the benchmark can serve as a
foundational tool for both developers and users of LLMs.

</details>


### [51] [(P)rior(D)yna(F)low: A Priori Dynamic Workflow Construction via Multi-Agent Collaboration](https://arxiv.org/abs/2509.14547)
*Yi Lin,Lujin Zhao,Yijie Shi*

Main category: cs.AI

TL;DR: 基于Q表学习和先验决策的动态自主工作流构建框架，结合历史经验和任务特性，提高效率和适应性


<details>
  <summary>Details</summary>
Motivation: 现有自主工作流构建方法仅依赖历史经验，导致效率和适应性受限，需要结合任务特性进行灵活响应

Method: 提出先验动态框架，利用Q表学习优化决策空间，代理根据任务进度做出先验决策，结合冷启动、提前停止和剪枝机制

Result: 在4个标准数据集上验证有效性，相比最优基线平均提升4.05%，工作流构建和推理成本降至30.68%-48.31%

Conclusion: 该框架能够有效结合历史经验和任务特性，实现高效、低成本的自动化工作流构建，为LLM协同工作提供了新方向

Abstract: Recent studies have shown that carefully designed workflows coordinating
large language models(LLMs) significantly enhance task-solving capabilities
compared to using a single model. While an increasing number of works focus on
autonomous workflow construction, most existing approaches rely solely on
historical experience, leading to limitations in efficiency and adaptability.
We argue that while historical experience is valuable, workflow construction
should also flexibly respond to the unique characteristics of each task. To
this end, we propose an a priori dynamic framework for automated workflow
construction. Our framework first leverages Q-table learning to optimize the
decision space, guiding agent decisions and enabling effective use of
historical experience. At the same time, agents evaluate the current task
progress and make a priori decisions regarding the next executing agent,
allowing the system to proactively select the more suitable workflow structure
for each given task. Additionally, we incorporate mechanisms such as cold-start
initialization, early stopping, and pruning to further improve system
efficiency. Experimental evaluations on four benchmark datasets demonstrate the
feasibility and effectiveness of our approach. Compared to state-of-the-art
baselines, our method achieves an average improvement of 4.05%, while reducing
workflow construction and inference costs to only 30.68%-48.31% of those
required by existing methods.

</details>


### [52] [SynBench: A Benchmark for Differentially Private Text Generation](https://arxiv.org/abs/2509.14594)
*Yidan Sun,Viktor Schlegel,Srinivasan Nandakumar,Iqra Zahid,Yuping Wu,Yulong Wu,Hao Li,Jie Zhang,Warren Del-Pinto,Goran Nenadic,Siew Kei Lam,Anil Anthony Bharath*

Main category: cs.AI

TL;DR: 本文研究了在高风险领域中使用差分隐私生成式AI生成合成数据的挑战，提出了评估框架、进行大规模基准测试并发现域复杂性会导致性能下降，同时提出了成员推断攻击方法显示公开数据集可能使隐私保证失效。


<details>
  <summary>Details</summary>
Motivation: 高风险领域如医疗和金融中数据共享遇到法规、机构和隐私问题的阻碍，而现有匿名化方法对非结构化文本效果不佳，需要找到具有正式隐私保证的合成数据生成方法。

Method: 首先提出了包含标准化效用和保真度指标的综合评估框架，使用9个精选数据集包含领域特定复杂性；其次进行大规模基准测试，比较不同规模和微调策略的DP文本生成方法；最后开发了专门针对合成文本的成员推断攻击方法。

Result: 研究发现在DP约束下生成高质量领域特定合成数据仍然是未解决的挑战，性能随领域复杂性增加而下降。同时发现使用公开数据集（可能存在于预训练语料中）可能使声称的隐私保证失效。

Conclusion: 研究突出了对严格隐私审计的紧迫需求，并强调了开放域和专业领域评估之间的持续差距，为在隐私敏感高风险环境中负责任部署生成式AI提供了重要信息。

Abstract: Data-driven decision support in high-stakes domains like healthcare and
finance faces significant barriers to data sharing due to regulatory,
institutional, and privacy concerns. While recent generative AI models, such as
large language models, have shown impressive performance in open-domain tasks,
their adoption in sensitive environments remains limited by unpredictable
behaviors and insufficient privacy-preserving datasets for benchmarking.
Existing anonymization methods are often inadequate, especially for
unstructured text, as redaction and masking can still allow re-identification.
Differential Privacy (DP) offers a principled alternative, enabling the
generation of synthetic data with formal privacy assurances. In this work, we
address these challenges through three key contributions. First, we introduce a
comprehensive evaluation framework with standardized utility and fidelity
metrics, encompassing nine curated datasets that capture domain-specific
complexities such as technical jargon, long-context dependencies, and
specialized document structures. Second, we conduct a large-scale empirical
study benchmarking state-of-the-art DP text generation methods and LLMs of
varying sizes and different fine-tuning strategies, revealing that high-quality
domain-specific synthetic data generation under DP constraints remains an
unsolved challenge, with performance degrading as domain complexity increases.
Third, we develop a membership inference attack (MIA) methodology tailored for
synthetic text, providing first empirical evidence that the use of public
datasets - potentially present in pre-training corpora - can invalidate claimed
privacy guarantees. Our findings underscore the urgent need for rigorous
privacy auditing and highlight persistent gaps between open-domain and
specialist evaluations, informing responsible deployment of generative AI in
privacy-sensitive, high-stakes settings.

</details>


### [53] [AgentCompass: Towards Reliable Evaluation of Agentic Workflows in Production](https://arxiv.org/abs/2509.14647)
*NVJK Kartik,Garvit Sapra,Rishav Hada,Nikhil Pareek*

Main category: cs.AI

TL;DR: AgentCompass是首个专门为多智能体工作流设计的后部署监控和调试评估框架，通过结构化分析流程和双记忆系统实现持续学习，在真实部署和基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型在多智能体工作流中的广泛应用，现有评估方法无法有效捕捉错误、涌现行为和系统性故障带来的风险，需要专门的监控调试工具。

Method: 采用结构化多阶段分析流程：错误识别分类、主题聚类、量化评分和策略总结，并配备情景记忆和语义记忆的双记忆系统实现持续学习。

Result: 在真实部署和TRAIL基准测试中取得最先进结果，发现了人工标注遗漏的关键问题，证明了其实际效用。

Conclusion: AgentCompass是一个强大、以开发者为中心的可靠工具，能够有效监控和改进生产环境中的智能体系统。

Abstract: With the growing adoption of Large Language Models (LLMs) in automating
complex, multi-agent workflows, organizations face mounting risks from errors,
emergent behaviors, and systemic failures that current evaluation methods fail
to capture. We present AgentCompass, the first evaluation framework designed
specifically for post-deployment monitoring and debugging of agentic workflows.
AgentCompass models the reasoning process of expert debuggers through a
structured, multi-stage analytical pipeline: error identification and
categorization, thematic clustering, quantitative scoring, and strategic
summarization. The framework is further enhanced with a dual memory
system-episodic and semantic-that enables continual learning across executions.
Through collaborations with design partners, we demonstrate the framework's
practical utility on real-world deployments, before establishing its efficacy
against the publicly available TRAIL benchmark. AgentCompass achieves
state-of-the-art results on key metrics, while uncovering critical issues
missed in human annotations, underscoring its role as a robust,
developer-centric tool for reliable monitoring and improvement of agentic
systems in production.

</details>


### [54] [Understanding the Thinking Process of Reasoning Models: A Perspective from Schoenfeld's Episode Theory](https://arxiv.org/abs/2509.14662)
*Ming Li,Nan Zhang,Chenrui Fan,Hong Jiao,Yanbin Fu,Sydney Peters,Qingshu Xu,Robert Lissitz,Tianyi Zhou*

Main category: cs.AI

TL;DR: 应用Schoenfeld的认知框架分析大型推理模型的思维过程，构建了第一个精细化的机器推理分析基准


<details>
  <summary>Details</summary>
Motivation: 缺乏理论基础来理解大型推理模型生成的链式思维结构

Method: 采用Schoenfeld的Episode Theory认知框架，对数学问题模型解答进行七种认知标签的注释

Result: 建立了公开的注释语料库和指南，发现了LRM推理的特定模式和认知状态迁移动态

Conclusion: 提供了理论基础的LRM认知解释方法，为建立可控透明的推理系统奠定基础

Abstract: While Large Reasoning Models (LRMs) generate extensive chain-of-thought
reasoning, we lack a principled framework for understanding how these thoughts
are structured. In this paper, we introduce a novel approach by applying
Schoenfeld's Episode Theory, a classic cognitive framework for human
mathematical problem-solving, to analyze the reasoning traces of LRMs. We
annotated thousands of sentences and paragraphs from model-generated solutions
to math problems using seven cognitive labels (e.g., Plan, Implement, Verify).
The result is the first publicly available benchmark for the fine-grained
analysis of machine reasoning, including a large annotated corpus and detailed
annotation guidebooks. Our preliminary analysis reveals distinct patterns in
LRM reasoning, such as the transition dynamics between cognitive states. This
framework provides a theoretically grounded methodology for interpreting LRM
cognition and enables future work on more controllable and transparent
reasoning systems.

</details>


### [55] [RationAnomaly: Log Anomaly Detection with Rationality via Chain-of-Thought and Reinforcement Learning](https://arxiv.org/abs/2509.14693)
*Song Xu,Yilun Liu,Minggui He,Mingchen Dai,Ziang Chen,Chunguang Zhao,Jingzhou Du,Shimin Tao,Weibin Meng,Shenglin Zhang,Yongqian Sun,Boxing Chen,Daimeng Wei*

Main category: cs.AI

TL;DR: RationAnomaly是一个结合思维链微调和强化学习的日志异常检测框架，通过专家修正的高质量数据集和多方奖励函数，在提升检测准确性的同时保证逻辑一致性，有效减少幻觉问题。


<details>
  <summary>Details</summary>
Motivation: 现有日志异常检测方法存在局限性：传统深度学习模型缺乏可解释性和泛化能力，而基于大语言模型的方法存在不可靠性和事实错误问题。

Method: 采用思维链指导的监督微调来注入专家级推理模式，然后通过具有多方奖励函数的强化学习阶段优化准确性和逻辑一致性。

Result: 在关键基准测试中实现了优越的F1分数，超越了最先进的基线方法，并提供透明的逐步分析输出。

Conclusion: RationAnomaly框架通过结合思维链微调和强化学习，有效解决了日志异常检测中的可解释性、泛化性和可靠性问题，相关代码和数据集已发布。

Abstract: Logs constitute a form of evidence signaling the operational status of
software systems. Automated log anomaly detection is crucial for ensuring the
reliability of modern software systems. However, existing approaches face
significant limitations: traditional deep learning models lack interpretability
and generalization, while methods leveraging Large Language Models are often
hindered by unreliability and factual inaccuracies. To address these issues, we
propose RationAnomaly, a novel framework that enhances log anomaly detection by
synergizing Chain-of-Thought (CoT) fine-tuning with reinforcement learning. Our
approach first instills expert-like reasoning patterns using CoT-guided
supervised fine-tuning, grounded in a high-quality dataset corrected through a
rigorous expert-driven process. Subsequently, a reinforcement learning phase
with a multi-faceted reward function optimizes for accuracy and logical
consistency, effectively mitigating hallucinations. Experimentally,
RationAnomaly outperforms state-of-the-art baselines, achieving superior
F1-scores on key benchmarks while providing transparent, step-by-step
analytical outputs. We have released the corresponding resources, including
code and datasets.

</details>


### [56] [The NazoNazo Benchmark: A Cost-Effective and Extensible Test of Insight-Based Reasoning in LLMs](https://arxiv.org/abs/2509.14704)
*Masaharu Mizumoto,Dat Nguyen,Zhiheng Han,Jiyuan Fang,Heyuan Guan,Xingfu Li,Naoya Shiraishi,Xuyang Tian,Yo Nakawake,Le Minh Nguyen*

Main category: cs.AI

TL;DR: Nazonazo是一个基于日本儿童谜语构建的基准测试，用于评估LLM的洞察力推理能力，具有成本效益高、可扩展和易于更新的特点。


<details>
  <summary>Details</summary>
Motivation: 当前基准测试存在饱和和污染问题，削弱了对LLM评估的信心，需要开发新的评估方法来测试洞察力推理。

Method: 使用日本儿童谜语构建基准测试，包含120个谜题，评估了38个前沿模型和126名成年人。谜题短小，不需要专业知识，可以大规模生成。

Result: 除GPT-5外，没有模型能达到人类52.9%的平均准确率。推理模型显著优于非推理模型，模型大小与准确率没有可靠关联。分析发现模型经常产生正确答案但最终选择错误。

Conclusion: Nazonazo提供了一个有效解决当前评估危机的基准格式，同时揭示了模型在元认知方面的弱点，为未来的控制和校准方法提供了明确目标。

Abstract: Benchmark saturation and contamination undermine confidence in LLM
evaluation. We present Nazonazo, a cost-effective and extensible benchmark
built from Japanese children's riddles to test insight-based reasoning. Items
are short (mostly one sentence), require no specialized domain knowledge, and
can be generated at scale, enabling rapid refresh of blind sets when leakage is
suspected. We evaluate 38 frontier models and 126 adults on 120 riddles. No
model except for GPT-5 is comparable to human performance, which achieves a
52.9% mean accuracy. Model comparison on extended 201 items shows that
reasoning models significantly outperform non-reasoning peers, while model size
shows no reliable association with accuracy. Beyond aggregate accuracy, an
informal candidate-tracking analysis of thought logs reveals many cases of
verification failure: models often produce the correct solution among
intermediate candidates yet fail to select it as the final answer, which we
illustrate with representative examples observed in multiple models. Nazonazo
thus offers a cost-effective, scalable, and easily renewable benchmark format
that addresses the current evaluation crisis while also suggesting a recurrent
meta-cognitive weakness, providing clear targets for future control and
calibration methods.

</details>


### [57] [Enhancing Retrieval Augmentation via Adversarial Collaboration](https://arxiv.org/abs/2509.14750)
*Letian Zhang,Guanghao Meng,Xudong Ren,Yiming Wang,Shu-Tao Xia*

Main category: cs.AI

TL;DR: 提出了AC-RAG框架，通过对抗协作机制解决RAG中的检索幻觉问题，显著提升检索准确性和性能


<details>
  <summary>Details</summary>
Motivation: 解决检索增强生成(RAG)中存在的检索幻觉问题，即微调模型无法识别和处理低质量检索文档，从而影响性能

Method: 采用AC-RAG框架，包含两个异构代理：通用检测器识别知识差距，领域专家解析器提供精确解决方案，通过对抗协作机制进行迭代问题分析和知识检索

Result: 大量实验表明AC-RAG显著提高了检索准确性，在各种垂直领域中优于最先进的RAG方法

Conclusion: AC-RAG框架通过对抗协作有效解决了检索幻觉问题，为领域特定LLMs提供了更可靠的检索增强生成方案

Abstract: Retrieval-augmented Generation (RAG) is a prevalent approach for
domain-specific LLMs, yet it is often plagued by "Retrieval Hallucinations"--a
phenomenon where fine-tuned models fail to recognize and act upon poor-quality
retrieved documents, thus undermining performance. To address this, we propose
the Adversarial Collaboration RAG (AC-RAG) framework. AC-RAG employs two
heterogeneous agents: a generalist Detector that identifies knowledge gaps, and
a domain-specialized Resolver that provides precise solutions. Guided by a
moderator, these agents engage in an adversarial collaboration, where the
Detector's persistent questioning challenges the Resolver's expertise. This
dynamic process allows for iterative problem dissection and refined knowledge
retrieval. Extensive experiments show that AC-RAG significantly improves
retrieval accuracy and outperforms state-of-the-art RAG methods across various
vertical domains.

</details>


### [58] [OpenLens AI: Fully Autonomous Research Agent for Health Infomatics](https://arxiv.org/abs/2509.14778)
*Yuxiao Cheng,Jinli Suo*

Main category: cs.AI

TL;DR: OpenLens AI是一个专为健康信息学设计的全自动化研究框架，集成了文献综述、数据分析、代码生成和论文撰写等专业代理，通过视觉语言反馈和质控机制解决现有系统在医学可视化解释和领域特定质量要求方面的局限性。


<details>
  <summary>Details</summary>
Motivation: 健康信息学研究具有数据模态多样、知识快速扩展的特点，需要整合生物医学科学、数据分析和临床实践。现有基于大语言模型的代理系统在医学可视化解释和领域特定质量要求方面存在不足，无法满足健康信息学研究的需求。

Method: 开发OpenLens AI框架，集成专业代理进行文献综述、数据分析、代码生成和论文准备，通过视觉语言反馈处理医学可视化，并实施质量控制确保可重复性，自动化整个研究流程生成可发表的LaTeX论文。

Result: 该框架能够自动化执行完整的研究流程，生成具有透明和可追溯工作流程的出版就绪LaTeX论文，为健康信息学研究提供了领域适配的解决方案。

Conclusion: OpenLens AI通过集成视觉语言反馈和质量控制机制，解决了现有系统在健康信息学应用中的局限性，为自动化健康信息学研究提供了有效的框架支持。

Abstract: Health informatics research is characterized by diverse data modalities,
rapid knowledge expansion, and the need to integrate insights across biomedical
science, data analytics, and clinical practice. These characteristics make it
particularly well-suited for agent-based approaches that can automate knowledge
exploration, manage complex workflows, and generate clinically meaningful
outputs. Recent progress in large language model (LLM)-based agents has
demonstrated promising capabilities in literature synthesis, data analysis, and
even end-to-end research execution. However, existing systems remain limited
for health informatics because they lack mechanisms to interpret medical
visualizations and often overlook domain-specific quality requirements. To
address these gaps, we introduce OpenLens AI, a fully automated framework
tailored to health informatics. OpenLens AI integrates specialized agents for
literature review, data analysis, code generation, and manuscript preparation,
enhanced by vision-language feedback for medical visualization and quality
control for reproducibility. The framework automates the entire research
pipeline, producing publication-ready LaTeX manuscripts with transparent and
traceable workflows, thereby offering a domain-adapted solution for advancing
health informatics research.

</details>


### [59] [Explainable AI for Infection Prevention and Control: Modeling CPE Acquisition and Patient Outcomes in an Irish Hospital with Transformers](https://arxiv.org/abs/2509.14942)
*Minh-Khoi Pham,Tai Tan Mai,Martin Crane,Rob Brennan,Marie E. Ward,Una Geary,Declan Byrne,Brian O Connell,Colm Bergin,Donncha Creagh,Nick McDonald,Marija Bezbradica*

Main category: cs.AI

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Carbapenemase-Producing Enterobacteriace poses a critical concern for
infection prevention and control in hospitals. However, predictive modeling of
previously highlighted CPE-associated risks such as readmission, mortality, and
extended length of stay (LOS) remains underexplored, particularly with modern
deep learning approaches. This study introduces an eXplainable AI modeling
framework to investigate CPE impact on patient outcomes from Electronic Medical
Records data of an Irish hospital. We analyzed an inpatient dataset from an
Irish acute hospital, incorporating diagnostic codes, ward transitions, patient
demographics, infection-related variables and contact network features. Several
Transformer-based architectures were benchmarked alongside traditional machine
learning models. Clinical outcomes were predicted, and XAI techniques were
applied to interpret model decisions. Our framework successfully demonstrated
the utility of Transformer-based models, with TabTransformer consistently
outperforming baselines across multiple clinical prediction tasks, especially
for CPE acquisition (AUROC and sensitivity). We found infection-related
features, including historical hospital exposure, admission context, and
network centrality measures, to be highly influential in predicting patient
outcomes and CPE acquisition risk. Explainability analyses revealed that
features like "Area of Residence", "Admission Ward" and prior admissions are
key risk factors. Network variables like "Ward PageRank" also ranked highly,
reflecting the potential value of structural exposure information. This study
presents a robust and explainable AI framework for analyzing complex EMR data
to identify key risk factors and predict CPE-related outcomes. Our findings
underscore the superior performance of the Transformer models and highlight the
importance of diverse clinical and network features.

</details>


### [60] [Sentinel Agents for Secure and Trustworthy Agentic AI in Multi-Agent Systems](https://arxiv.org/abs/2509.14956)
*Diego Gosmar,Deborah A. Dahl*

Main category: cs.AI

TL;DR: 这篇论文提出了一种新的多自主代理系统安全框架，通过切入式监控代理和协调代理的双层结构，提高了系统的安全性和可靠性。


<details>
  <summary>Details</summary>
Motivation: 多自主代理系统面临着多种安全威胁，包括提示注入、代理合伙行为、大语言模型幻觉、隐私泄漏等。需要一种动态适应的防御机制来确保系统整体的安全性和可靠性。

Method: 设计了双层安全框架：1）Sentinel Agents（切入式监控代理）：分布式安全层，集成语义分析、行为分析、检验加强和跨代理异常检测等技术；2）Coordinator Agent（协调代理）：监督策略执行、管理代理参与和处理异常警报。

Result: 通过在多代理对话环境中注入162个合成攻击（提示注入、幻觉和数据泄漏）的模拟研究，Sentinel Agents成功检测到了攻击尝试，证明了监控方法的实际可行性。

Conclusion: 该框架提供了一种有效的动态适应防御机制，能够应对多种安全威胁，同时提升了系统的可观测性、遵循监管要求和策略迭代能力。

Abstract: This paper proposes a novel architectural framework aimed at enhancing
security and reliability in multi-agent systems (MAS). A central component of
this framework is a network of Sentinel Agents, functioning as a distributed
security layer that integrates techniques such as semantic analysis via large
language models (LLMs), behavioral analytics, retrieval-augmented verification,
and cross-agent anomaly detection. Such agents can potentially oversee
inter-agent communications, identify potential threats, enforce privacy and
access controls, and maintain comprehensive audit records. Complementary to the
idea of Sentinel Agents is the use of a Coordinator Agent. The Coordinator
Agent supervises policy implementation, and manages agent participation. In
addition, the Coordinator also ingests alerts from Sentinel Agents. Based on
these alerts, it can adapt policies, isolate or quarantine misbehaving agents,
and contain threats to maintain the integrity of the MAS ecosystem. This
dual-layered security approach, combining the continuous monitoring of Sentinel
Agents with the governance functions of Coordinator Agents, supports dynamic
and adaptive defense mechanisms against a range of threats, including prompt
injection, collusive agent behavior, hallucinations generated by LLMs, privacy
breaches, and coordinated multi-agent attacks. In addition to the architectural
design, we present a simulation study where 162 synthetic attacks of different
families (prompt injection, hallucination, and data exfiltration) were injected
into a multi-agent conversational environment. The Sentinel Agents successfully
detected the attack attempts, confirming the practical feasibility of the
proposed monitoring approach. The framework also offers enhanced system
observability, supports regulatory compliance, and enables policy evolution
over time.

</details>


### [61] [Set Contribution Functions for Quantitative Bipolar Argumentation and their Principles](https://arxiv.org/abs/2509.14963)
*Filip Naudot,Andreas Brännström,Vicenç Torra,Timotheus Kampik*

Main category: cs.AI

TL;DR: 本文提出了量化论证集合在定量双极论证图中对目标论证强度贡献的函数，是现有单论证贡献函数的泛化，并进行了基于原则的分析。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注单个论证对目标论证的贡献，但在实际应用中，往往需要评估一组论证集合的整体贡献，因此需要开发能够量化集合贡献的函数。

Method: 通过泛化现有的单论证贡献函数来构建集合贡献函数，提出新的基于集合的原则，分析不同函数在原则上的表现，并在推荐系统应用场景中进行验证。

Result: 开发了通用的集合贡献函数框架，提出了专门针对集合功能的新原则，展示了不同函数在推荐系统应用中的实际效果。

Conclusion: 集合贡献函数为定量双极论证分析提供了更全面的工具，新提出的集合特定原则有助于更好地理解和评估论证集合的交互作用，在推荐系统等实际应用中具有重要价值。

Abstract: We present functions that quantify the contribution of a set of arguments in
quantitative bipolar argumentation graphs to (the final strength of) an
argument of interest, a so-called topic. Our set contribution functions are
generalizations of existing functions that quantify the contribution of a
single contributing argument to a topic. Accordingly, we generalize existing
contribution function principles for set contribution functions and provide a
corresponding principle-based analysis. We introduce new principles specific to
set-based functions that focus on properties pertaining to the interaction of
arguments within a set. Finally, we sketch how the principles play out across
different set contribution functions given a recommendation system application
scenario.

</details>


### [62] [A Knowledge-driven Adaptive Collaboration of LLMs for Enhancing Medical Decision-making](https://arxiv.org/abs/2509.14998)
*Xiao Wu,Ting-Zhu Huang,Liang-Jian Deng,Yanyuan Qiao,Imran Razzak,Yutong Xie*

Main category: cs.AI

TL;DR: KAMAC是一个知识驱动的自适应多智能体协作框架，通过动态组建专家团队来解决医疗决策中的知识整合问题，在复杂临床场景中显著优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的多智能体协作框架采用静态预分配角色，限制了适应性和动态知识整合能力，无法有效模拟真实医疗团队的合作过程。

Method: 提出KAMAC框架，从初始专家智能体开始，通过知识驱动讨论识别知识缺口，动态招募额外专家，支持灵活可扩展的协作，最终通过审查更新后的智能体评论做出决策。

Result: 在两个真实医疗基准测试中，KAMAC显著优于单智能体和先进多智能体方法，特别是在需要动态跨专业知识的复杂临床场景（如癌症预后）中表现突出。

Conclusion: KAMAC框架通过动态团队组建和知识驱动的协作机制，有效提升了LLM在复杂医疗决策任务中的表现，为多智能体协作提供了新的解决方案。

Abstract: Medical decision-making often involves integrating knowledge from multiple
clinical specialties, typically achieved through multidisciplinary teams.
Inspired by this collaborative process, recent work has leveraged large
language models (LLMs) in multi-agent collaboration frameworks to emulate
expert teamwork. While these approaches improve reasoning through agent
interaction, they are limited by static, pre-assigned roles, which hinder
adaptability and dynamic knowledge integration. To address these limitations,
we propose KAMAC, a Knowledge-driven Adaptive Multi-Agent Collaboration
framework that enables LLM agents to dynamically form and expand expert teams
based on the evolving diagnostic context. KAMAC begins with one or more expert
agents and then conducts a knowledge-driven discussion to identify and fill
knowledge gaps by recruiting additional specialists as needed. This supports
flexible, scalable collaboration in complex clinical scenarios, with decisions
finalized through reviewing updated agent comments. Experiments on two
real-world medical benchmarks demonstrate that KAMAC significantly outperforms
both single-agent and advanced multi-agent methods, particularly in complex
clinical scenarios (i.e., cancer prognosis) requiring dynamic, cross-specialty
expertise. Our code is publicly available at:
https://github.com/XiaoXiao-Woo/KAMAC.

</details>


### [63] [Calibrated Generative AI as Meta-Reviewer: A Systemic Functional Linguistics Discourse Analysis of Reviews of Peer Reviews](https://arxiv.org/abs/2509.15035)
*Gabriela C. Zapata,Bill Cope,Mary Kalantzis,Duane Searsmith*

Main category: cs.AI

TL;DR: 研究探讨了使用生成式AI为研究生在线课程的同伴互评提供机器生成的元评价，分析显示AI反馈能近似人类有效反馈的修辞和关系特征


<details>
  <summary>Details</summary>
Motivation: 研究生成式AI如何支持形成性评估，特别是在同伴互评中提供机器生成的元评价，以探索AI反馈在意义构建方面的潜力

Method: 基于系统功能语言学和评价理论，分析了120个AI生成的元评价，从概念、人际和文本三个维度探讨AI反馈的意义构建方式

Result: AI反馈能够平衡表扬和建设性批评，与评分标准保持一致，具有结构化组织，同时保持支持性立场，展现指令清晰性

Conclusion: AI元反馈具有搭建反馈素养支架和增强学习者参与同伴互评的潜力，能够模拟有效人类反馈的关键特征

Abstract: This study investigates the use of generative AI to support formative
assessment through machine generated reviews of peer reviews in graduate online
courses in a public university in the United States. Drawing on Systemic
Functional Linguistics and Appraisal Theory, we analyzed 120 metareviews to
explore how generative AI feedback constructs meaning across ideational,
interpersonal, and textual dimensions. The findings suggest that generative AI
can approximate key rhetorical and relational features of effective human
feedback, offering directive clarity while also maintaining a supportive
stance. The reviews analyzed demonstrated a balance of praise and constructive
critique, alignment with rubric expectations, and structured staging that
foregrounded student agency. By modeling these qualities, AI metafeedback has
the potential to scaffold feedback literacy and enhance leaner engagement with
peer review.

</details>


### [64] [From Sea to System: Exploring User-Centered Explainable AI for Maritime Decision Support](https://arxiv.org/abs/2509.15084)
*Doreen Jirak,Pieter Maes,Armeen Saroukanoff,Dirk van Rooy*

Main category: cs.AI

TL;DR: 本文强调可解释AI(XAI)在海上领域的重要性，提出针对海事专业人员的调查问卷来评估信任度、可用性和可解释性，旨在开发用户中心化的XAI系统。


<details>
  <summary>Details</summary>
Motivation: 随着自主技术在海事运营中的普及，AI决策的透明度变得与决策本身同等重要。在复杂动态的海事环境中，对AI的信任不仅取决于性能，还需要透明度和可解释性。

Method: 提出针对海事领域的特定调查问卷，用于捕捉海事专业人员对信任、可用性和可解释性的看法，支持以用户为中心的XAI集成。

Result: 通过调查问卷收集海事专业人员的反馈，为开发量身定制的用户中心化XAI系统提供指导。

Conclusion: 可解释AI是海事领域人机协作有效性的基础，需要开发专门针对海员和海事团队需求的用户中心化XAI系统，以促进认知和指导系统开发。

Abstract: As autonomous technologies increasingly shape maritime operations,
understanding why an AI system makes a decision becomes as crucial as what it
decides. In complex and dynamic maritime environments, trust in AI depends not
only on performance but also on transparency and interpretability. This paper
highlights the importance of Explainable AI (XAI) as a foundation for effective
human-machine teaming in the maritime domain, where informed oversight and
shared understanding are essential. To support the user-centered integration of
XAI, we propose a domain-specific survey designed to capture maritime
professionals' perceptions of trust, usability, and explainability. Our aim is
to foster awareness and guide the development of user-centric XAI systems
tailored to the needs of seafarers and maritime teams.

</details>


### [65] [Internalizing Self-Consistency in Language Models: Multi-Agent Consensus Alignment](https://arxiv.org/abs/2509.15172)
*Ankur Samanta,Akshayaa Magesh,Youliang Yu,Runzhe Wu,Ayush Jain,Daniel Jiang,Boris Vidolov,Paul Sajda,Yonathan Efroni,Kaveh Hassani*

Main category: cs.AI

TL;DR: MACA是一个强化学习框架，通过多智能体辩论的多数/少数结果来后训练模型，使其偏好与内部共识一致推理轨迹，显著提升语言模型的自一致性和推理能力。


<details>
  <summary>Details</summary>
Motivation: 语言模型是不一致的推理者，经常对相同提示生成矛盾响应。现有推理时方法无法解决核心问题：模型难以可靠选择导致探索性采样下一致结果的推理路径。

Method: 提出多智能体共识对齐(MACA)强化学习框架，利用多智能体辩论的多数/少数结果进行后训练，使模型偏好与内部共识一致的推理轨迹。这些轨迹来自审议性交流，智能体基于同伴论证而非独立尝试的聚合进行推理。

Result: 在自一致性(+27.6% GSM8K)、单智能体推理(+23.7% MATH)、基于采样的推理(+22.4% Pass@20 MATH)和多智能体集成决策(+42.7% MathQA)方面取得显著提升，在未见基准上也有强泛化能力(+16.3% GPQA, +11.6% CommonsenseQA)。

Conclusion: MACA展示了强大的自对齐能力，能更可靠地释放语言模型的潜在推理潜力，使智能体学会更果断简洁，并在无外部监督下更好地利用多智能体环境中的同伴见解。

Abstract: Language Models (LMs) are inconsistent reasoners, often generating
contradictory responses to identical prompts. While inference-time methods can
mitigate these inconsistencies, they fail to address the core problem: LMs
struggle to reliably select reasoning pathways leading to consistent outcomes
under exploratory sampling. To address this, we formalize self-consistency as
an intrinsic property of well-aligned reasoning models and introduce
Multi-Agent Consensus Alignment (MACA), a reinforcement learning framework that
post-trains models to favor reasoning trajectories aligned with their internal
consensus using majority/minority outcomes from multi-agent debate. These
trajectories emerge from deliberative exchanges where agents ground reasoning
in peer arguments, not just aggregation of independent attempts, creating
richer consensus signals than single-round majority voting. MACA enables agents
to teach themselves to be more decisive and concise, and better leverage peer
insights in multi-agent settings without external supervision, driving
substantial improvements across self-consistency (+27.6% on GSM8K),
single-agent reasoning (+23.7% on MATH), sampling-based inference (+22.4%
Pass@20 on MATH), and multi-agent ensemble decision-making (+42.7% on MathQA).
These findings, coupled with strong generalization to unseen benchmarks (+16.3%
on GPQA, +11.6% on CommonsenseQA), demonstrate robust self-alignment that more
reliably unlocks latent reasoning potential of language models.

</details>


### [66] [Generalizable Geometric Image Caption Synthesis](https://arxiv.org/abs/2509.15217)
*Yue Xin,Wenyuan Wang,Rui Pan,Ruida Wang,Howard Meng,Renjie Pi,Shizhe Diao,Tong Zhang*

Main category: cs.AI

TL;DR: 通过引入基于验证奖励的强化学习(RLVR)方法，提出了一种生成高质量几何图像-文本数据的新方法，以提升多模态大语言模型在几何问题解决中的推理能力。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型在复杂几何问题上表现尚不理想，主要因为缺乏高质量的几何图像-文本对数据集，且现有模板化数据生成方法难以演绊到模板之外的问题。

Method: 提出RLVR方法，利用基于6570学问题解决任务的奖励信号来精炼从50个基础几何关系生成的图像说明文本，从而抓取几何问题解决的关键特征。

Result: 生成的数据集在分布外场景中也能提升模型的普通推理能力：在MathVista和MathVerse的非几何图像任务中提升2.8%-4.8%的准确率，在MMMU的艺术、设计、技术和工程任务中提升2.4%-3.9%的准确率。

Conclusion: RLVR方法能够生成高质量的几何数据集，不仅提升了多模态模型在几何问题上的表现，还能增强模型的普通推理能力，具有良好的演绊性。

Abstract: Multimodal large language models have various practical applications that
demand strong reasoning abilities. Despite recent advancements, these models
still struggle to solve complex geometric problems. A key challenge stems from
the lack of high-quality image-text pair datasets for understanding geometric
images. Furthermore, most template-based data synthesis pipelines typically
fail to generalize to questions beyond their predefined templates. In this
paper, we bridge this gap by introducing a complementary process of
Reinforcement Learning with Verifiable Rewards (RLVR) into the data generation
pipeline. By adopting RLVR to refine captions for geometric images synthesized
from 50 basic geometric relations and using reward signals derived from
mathematical problem-solving tasks, our pipeline successfully captures the key
features of geometry problem-solving. This enables better task generalization
and yields non-trivial improvements. Furthermore, even in out-of-distribution
scenarios, the generated dataset enhances the general reasoning capabilities of
multimodal large language models, yielding accuracy improvements of
$2.8\%\text{-}4.8\%$ in statistics, arithmetic, algebraic, and numerical tasks
with non-geometric input images of MathVista and MathVerse, along with
$2.4\%\text{-}3.9\%$ improvements in Art, Design, Tech, and Engineering tasks
in MMMU.

</details>
