<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 9]
- [cs.CR](#cs.CR) [Total: 33]
- [cs.AI](#cs.AI) [Total: 17]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [Stack Trace-Based Crash Deduplication with Transformer Adaptation](https://arxiv.org/abs/2508.19449)
*Md Afif Al Mamun,Gias Uddin,Lan Xia,Longyu Zhang*

Main category: cs.SE

TL;DR: dedupT是一个基于Transformer的崩溃报告去重方法，通过整体建模堆栈轨迹而非孤立帧，显著优于现有深度学习和传统方法。


<details>
  <summary>Details</summary>
Motivation: 自动化崩溃报告系统产生大量重复报告，传统基于字符串相似度、启发式规则或深度学习的方法难以捕捉堆栈轨迹中的上下文和结构关系。

Method: 首先适配预训练语言模型到堆栈轨迹，然后使用其嵌入训练全连接网络来有效排名重复崩溃。

Result: 在四个公共数据集上，dedupT相比最佳深度学习基线平均倒数排名提升超过15%，相比传统方法提升达9%，在检测唯一崩溃报告时获得更高的ROC-AUC。

Conclusion: 该工作推动了现代自然语言处理技术在软件工程中的集成，为基于堆栈轨迹的崩溃去重提供了有效解决方案。

Abstract: Automated crash reporting systems generate large volumes of duplicate
reports, overwhelming issue-tracking systems and increasing developer workload.
Traditional stack trace-based deduplication methods, relying on string
similarity, rule-based heuristics, or deep learning (DL) models, often fail to
capture the contextual and structural relationships within stack traces. We
propose dedupT, a transformer-based approach that models stack traces
holistically rather than as isolated frames. dedupT first adapts a pretrained
language model (PLM) to stack traces, then uses its embeddings to train a
fully-connected network (FCN) to rank duplicate crashes effectively. Extensive
experiments on real-world datasets show that dedupT outperforms existing DL and
traditional methods (e.g., sequence alignment and information retrieval
techniques) in both duplicate ranking and unique crash detection, significantly
reducing manual triage effort. On four public datasets, dedupT improves Mean
Reciprocal Rank (MRR) often by over 15% compared to the best DL baseline and up
to 9% over traditional methods while achieving higher Receiver Operating
Characteristic Area Under the Curve (ROC-AUC) in detecting unique crash
reports. Our work advances the integration of modern natural language
processing (NLP) techniques into software engineering, providing an effective
solution for stack trace-based crash deduplication.

</details>


### [2] [Functional Consistency of LLM Code Embeddings: A Self-Evolving Data Synthesis Framework for Benchmarking](https://arxiv.org/abs/2508.19558)
*Zhuohao Li,Wenqing Chen,Jianxing Yu,Zhichao Lu*

Main category: cs.SE

TL;DR: 本文提出了一种面向功能的代码自演化框架，用于构建多样化的代码功能语义基准测试，显著提升了嵌入模型在代码克隆检测、功能一致性识别和代码检索等任务中的性能。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注代码克隆检测，强调语法相似性而忽视了功能理解。大型语言模型的代码嵌入能否反映代码级功能语义尚不清楚，需要构建更好的基准来评估功能一致性。

Method: 提出了Functionality-Oriented Code Self-Evolution数据合成框架，从单个代码实例生成四种独特的变体，涵盖四个语义和语法类别，提供更广泛的代码示例谱系以更好地反映功能差异。

Result: 在三个下游任务（代码克隆检测、代码功能一致性识别和代码检索）上的广泛实验表明，使用演化数据集训练的嵌入模型性能显著提升。

Conclusion: 该数据合成框架在提升代码功能理解方面具有有效性和泛化能力，推动了代码功能语义研究的发展。

Abstract: Embedding models have demonstrated strong performance in tasks like
clustering, retrieval, and feature extraction while offering computational
advantages over generative models and cross-encoders. Benchmarks such as MTEB
have shown that text embeddings from large language models (LLMs) capture rich
semantic information, but their ability to reflect code-level functional
semantics remains unclear. Existing studies largely focus on code clone
detection, which emphasizes syntactic similarity and overlooks functional
understanding. In this paper, we focus on the functional consistency of LLM
code embeddings, which determines if two code snippets perform the same
function regardless of syntactic differences. We propose a novel data synthesis
framework called Functionality-Oriented Code Self-Evolution to construct
diverse and challenging benchmarks. Specifically, we define code examples
across four semantic and syntactic categories and find that existing datasets
predominantly capture syntactic properties. Our framework generates four unique
variations from a single code instance, providing a broader spectrum of code
examples that better reflect functional differences. Extensive experiments on
three downstream tasks-code clone detection, code functional consistency
identification, and code retrieval-demonstrate that embedding models
significantly improve their performance when trained on our evolved datasets.
These results highlight the effectiveness and generalization of our data
synthesis framework, advancing the functional understanding of code.

</details>


### [3] [The Influence of Code Comments on the Perceived Helpfulness of Stack Overflow Posts](https://arxiv.org/abs/2508.19610)
*Kathrin Figl,Maria Kirchner,Sebastian Baltes,Michael Felderer*

Main category: cs.SE

TL;DR: 研究通过在线实验证明，代码注释显著提升Stack Overflow答案的帮助性，尤其是块注释对新手更有用，而位置和得分等表面特征相对次要


<details>
  <summary>Details</summary>
Motivation: 识别代码注释如何影响Stack Overflow答案的帮助性感知，以减少因重用不明代码导致的错误和安全漏洞

Method: 进行在线实验（n=91），模拟Stack Overflow环境，测试块注释、内联注释和无注释代码的帮助性评价

Result: 块注释和内联注释都比无注释代码被评为显著更有帮助；新手认为块注释比内联注释更有用；答案位置和得分等表面特征影响较小

Conclusion: 研究结果不仅有助于提升Stack Overflow等社区平台的相关性，还可以指导AI编码助手生成更可读的代码片段，为目标化提示策略提供基础

Abstract: Question-and-answer platforms such as Stack Overflow have become an important
way for software developers to share and retrieve knowledge. However, reusing
poorly understood code can lead to serious problems, such as bugs or security
vulnerabilities. To better understand how code comments affect the perceived
helpfulness of Stack Overflow answers, we conducted an online experiment
simulating a Stack Overflow environment (n=91). The results indicate that both
block and inline comments are perceived as significantly more helpful than
uncommented source code. Moreover, novices rated code snippets with block
comments as more helpful than those with inline comments. Interestingly, other
surface features, such as the position of an answer and its answer score, were
considered less important. The content of Stack Overflow has been a major
source for training large language models. AI-based coding assistants such as
GitHub Copilot, which are based on these models, might change the way Stack
Overflow is used. However, our findings have implications beyond this specific
platform. First, they may help to improve the relevance of community-driven
platforms such as Stack Overflow, which provide human advice and explanations
of code solutions, complementing AI-based support for software developers.
Second, since chat-based AI tools can be prompted to generate code in different
ways, knowing which properties influence perceived helpfulness might lead to
targeted prompting strategies to generate more readable code snippets.

</details>


### [4] [Leveraging LLMs for Automated Translation of Legacy Code: A Case Study on PL/SQL to Java Transformation](https://arxiv.org/abs/2508.19663)
*Lola Solovyeva,Eduardo Carneiro Oliveira,Shiyu Fan,Alper Tuncay,Shamil Gareev,Andrea Capiluppi*

Main category: cs.SE

TL;DR: 本研究探讨了使用大型语言模型(LLM)将PL/SQL代码转换为Java的可行性，提出了结合指导链推理和n-shot提示的自定义策略，在有限数据集上取得了语法准确和功能正确的翻译结果。


<details>
  <summary>Details</summary>
Motivation: VT遗留系统包含约250万行PL/SQL代码，缺乏一致的文档和自动化测试，给重构和现代化带来了重大挑战，需要寻找自动化解决方案。

Method: 利用10个PL/SQL-Java代码对和15个Java类建立领域模型，评估多个LLM，并提出结合指导链推理和n-shot提示的自定义提示策略。

Result: 该方法能有效指导LLM生成语法准确的翻译并实现功能正确性，但由于可用代码文件样本量小和测试用例访问受限，结果存在局限性。

Conclusion: 这些发现为现代化大型遗留系统的可扩展自动化解决方案奠定了基础，尽管当前研究受限于小样本规模。

Abstract: The VT legacy system, comprising approximately 2.5 million lines of PL/SQL
code, lacks consistent documentation and automated tests, posing significant
challenges for refactoring and modernisation. This study investigates the
feasibility of leveraging large language models (LLMs) to assist in translating
PL/SQL code into Java for the modernised "VTF3" system. By leveraging a dataset
comprising 10 PL/SQL-to-Java code pairs and 15 Java classes, which collectively
established a domain model for the translated files, multiple LLMs were
evaluated. Furthermore, we propose a customized prompting strategy that
integrates chain-of-guidance reasoning with $n$-shot prompting. Our findings
indicate that this methodology effectively guides LLMs in generating
syntactically accurate translations while also achieving functional
correctness. However, the findings are limited by the small sample size of
available code files and the restricted access to test cases used for
validating the correctness of the generated code. Nevertheless, these findings
lay the groundwork for scalable, automated solutions in modernising large
legacy systems.

</details>


### [5] [Enabling Content Management Systems as an Information Source in Model-driven Projects](https://arxiv.org/abs/2508.19797)
*Joan Giner-Miguelez,Abel Gómez,Jordi Cabot*

Main category: cs.SE

TL;DR: 提出了一个基于模型的框架，用于在软件开发过程中集成无头CMS，能够自动发现和表示CMS的信息模式，并生成中间件库来提供平台无关的CMS访问。


<details>
  <summary>Details</summary>
Motivation: 无头CMS已成为信息系统的重要组件，但目前缺乏合适的工具来发现和管理CMS中的信息，主要依赖耗时且容易出错的手动过程。

Method: 开发了一个模型驱动的框架，能够发现CMS背后的信息模式并显式表示，设计CMS模型与其他组件之间的交互，并生成作为中间件库的交互代码。

Result: 该框架能够自动发现CMS的信息结构，生成平台无关的访问中间件，为客户端应用提供统一的CMS访问接口。

Conclusion: 提出的框架有效解决了无头CMS集成中的信息发现和管理问题，通过自动化减少了手动工作的需求和错误风险，框架已开源提供。

Abstract: Content Management Systems (CMSs) are the most popular tool when it comes to
create and publish content across the web. Recently, CMSs have evolved,
becoming \emph{headless}. Content served by a \emph{headless CMS} aims to be
consumed by other applications and services through REST APIs rather than by
human users through a web browser. This evolution has enabled CMSs to become a
notorious source of content to be used in a variety of contexts beyond pure web
navigation. As such, CMS have become an important component of many information
systems. Unfortunately, we still lack the tools to properly discover and manage
the information stored in a CMS, often highly customized to the needs of a
specific domain. Currently, this is mostly a time-consuming and error-prone
manual process.
  In this paper, we propose a model-based framework to facilitate the
integration of headless CMSs in software development processes. Our framework
is able to discover and explicitly represent the information schema behind the
CMS. This facilitates designing the interaction between the CMS model and other
components consuming that information. These interactions are then generated as
part of a middleware library that offers platform-agnostic access to the CMS to
all the client applications. The complete framework is open-source and
available online.

</details>


### [6] [Towards a fundamental theory of modeling discrete systems](https://arxiv.org/abs/2508.19803)
*Peter Fettke,Wolfgang Reisig*

Main category: cs.SE

TL;DR: 论文提出Heraklit建模框架作为数字时代建模的新基础理论方法


<details>
  <summary>Details</summary>
Motivation: 数字时代对建模提出了新的挑战，需要新的基础理论来应对这些挑战

Method: 引入Heraklit建模框架作为新的建模方法

Result: 提出了一个新的建模框架，但具体结果未在摘要中详细说明

Conclusion: 建模是科学和工程的核心问题，需要新的基础理论；未来工作将涉及建模正确性、信息概念和不变性描述

Abstract: Modeling is a central concern in both science and engineering. However, we
need a new fundamental theory to address the challenges of the digital age. In
this paper, we first explain why modeling is fundamental and which challenges
must be addressed in the digital world. As a main contribution, we introduce
the Heraklit modeling framework as a new approach to modeling. We conclude with
some general remarks. Future work will involve the correctness of modeling, the
notion of information, and the description of invariance in modeling.

</details>


### [7] [On the Future of Software Reuse in the Era of AI Native Software Engineering](https://arxiv.org/abs/2508.19834)
*Antero Taivalsaari,Tommi Mikkonen,Cesare Pautasso*

Main category: cs.SE

TL;DR: 本文探讨AI辅助生成式软件重用的影响，指出其与货物崇拜开发的相似性，并提出研究议程来解决相关问题


<details>
  <summary>Details</summary>
Motivation: 随着AI原生开发方法的兴起，软件开发正经历范式转变，从传统的有机开发转向依赖AI生成代码的生成式软件重用，这引发了新的问题和挑战

Method: 通过分析AI辅助生成式软件重用的现象，与货物崇拜开发进行概念比较，提出关键研究问题并定义研究议程

Result: 识别了AI辅助软件重用带来的潜在问题，包括开发人员对AI生成代码的盲目信任、代码质量保证、以及与传统软件重用实践的根本差异

Conclusion: 需要建立系统的研究议程来应对AI辅助生成式软件重用带来的挑战，确保这种新兴开发方法的可持续性和可靠性

Abstract: Software development is currently under a paradigm shift in which artificial
intelligence and generative software reuse are taking the center stage in
software creation. Earlier opportunistic software reuse practices and organic
software development methods are rapidly being replaced by "AI Native"
approaches in which developers place their trust on code that has been
generated by artificial intelligence. This is leading to a new form of software
reuse that is conceptually not all that different from cargo cult development.
In this paper we discuss the implications of AI-assisted generative software
reuse, bring forth relevant questions, and define a research agenda for
tackling the central issues associated with this emerging approach.

</details>


### [8] [Generative AI for Testing of Autonomous Driving Systems: A Survey](https://arxiv.org/abs/2508.19882)
*Qunying Song,He Ye,Mark Harman,Federica Sarro*

Main category: cs.SE

TL;DR: 本文系统综述了生成式AI在自动驾驶系统测试中的应用，分析了91项相关研究，总结了6大应用类别、评估工具和27个局限性，为该领域提供实践洞察和研究方向。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶系统需要在大规模部署前进行广泛测试以确保功能安全和多样性，但传统测试方法面临效率和有效性挑战，而生成式AI具有解释上下文、推理复杂任务和生成多样化输出的能力，有望解决这些挑战。

Method: 通过系统分析91项相关研究，将生成式AI在ADS测试中的应用归纳为6个主要类别（主要围绕基于场景的测试），并综合评估其有效性、数据集、模拟器、指标和基准。

Result: 研究发现生成式AI在自动驾驶测试中展现出强大潜力，特别是在场景生成、测试用例创建等方面，但同时也识别出27个技术和方法上的局限性。

Conclusion: 生成式AI为自动驾驶系统测试提供了新的有效工具，但仍面临诸多挑战，需要进一步研究来解决现有局限性并推动该领域的持续发展。

Abstract: Autonomous driving systems (ADS) have been an active area of research, with
the potential to deliver significant benefits to society. However, before
large-scale deployment on public roads, extensive testing is necessary to
validate their functionality and safety under diverse driving conditions.
Therefore, different testing approaches are required, and achieving effective
and efficient testing of ADS remains an open challenge. Recently, generative AI
has emerged as a powerful tool across many domains, and it is increasingly
being applied to ADS testing due to its ability to interpret context, reason
about complex tasks, and generate diverse outputs. To gain a deeper
understanding of its role in ADS testing, we systematically analyzed 91
relevant studies and synthesized their findings into six major application
categories, primarily centered on scenario-based testing of ADS. We also
reviewed their effectiveness and compiled a wide range of datasets, simulators,
ADS, metrics, and benchmarks used for evaluation, while identifying 27
limitations. This survey provides an overview and practical insights into the
use of generative AI for testing ADS, highlights existing challenges, and
outlines directions for future research in this rapidly evolving field.

</details>


### [9] [Smart Contract Intent Detection with Pre-trained Programming Language Model](https://arxiv.org/abs/2508.20086)
*Youwei Huang,Jianwen Li,Sen Fang,Yao Li,Peng Yang,Bin Hu,Tao Zhang*

Main category: cs.SE

TL;DR: SmartIntentNN2是基于BERT预训练语言模型的智能合约恶意意图检测系统，相比前代版本性能显著提升，F1分数达到0.927，成为该领域的state-of-the-art模型。


<details>
  <summary>Details</summary>
Motivation: 智能合约开发中的恶意意图可能导致重大经济损失，需要有效的检测方法来识别不安全意图。

Method: 采用BERT预训练语言模型（在16,000个真实智能合约上使用掩码语言建模目标训练），结合BiLSTM多标签分类网络，集成通用句子编码器和K-means聚类的意图突出机制。

Result: 模型在区分10种不同意图类别上达到0.927的F1分数，相比前代版本（0.8633）有显著提升。

Conclusion: SmartIntentNN2在智能合约意图检测方面表现出色，确立了该领域的最先进水平，为防范智能合约安全风险提供了有效工具。

Abstract: Malicious intent in smart contract development can lead to substantial
economic losses. SmartIntentNN is a deep learning model specifically designed
to identify unsafe intents in smart contracts. This model integrates the
Universal Sentence Encoder, a K-means clustering-based intent highlighting
mechanism, and a Bidirectional Long Short-Term Memory network for multi-label
classification, achieving an F1 of 0.8633 in distinguishing ten different
intent categories. In this study, we present an upgraded version of this model,
SmartIntentNN2 (Smart Contract Intent Neural Network V2). A significant
enhancement in V2 is the incorporation of a BERT-based pre-trained language
model, which has been trained on a dataset of 16,000 real smart contracts using
a Masked Language Modeling objective. SmartIntentNN2 retains the BiLSTM-based
multi-label classification network. With an improved F1 of 0.927, V2
demonstrates enhanced performance compared to its predecessor, establishing
itself as the state-of-the-art model for smart contract intent detection.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [10] [Tight Quantum-Security Bounds and Parameter Optimization for SPHINCS+ and NTRU](https://arxiv.org/abs/2508.19250)
*Ruopengyu Xu,Chenglian Liu*

Main category: cs.CR

TL;DR: 这篇论文为NIST后量密码竞赛决赛算法SPHINCS+和NTRU提供了更紧凑的安全边界分析，通过量子攻击模型和优化算法大幅缩减了参数规模并提升安全性。


<details>
  <summary>Details</summary>
Motivation: 应对量子计算威胁，需要建立量子耐受性强的密码系统。现有的NIST后量密码标准化算法需要更严格的安全边界分析以确保其实际应用中的安全性。

Method: 构建包含逆声效应（τ_d）和并行化限制的量子攻击模型；改进熵提浓不等式减少SPHINCS+参数规模；通过量子格点熵$H_Q(Λ)$优化NTRU格点参数；改进NTRU到LWE的约化关系。

Result: 实现了SPHINCS+参数规模缩减15-20%，NTRU安全性显著提升，并获得了结构更优的约化因子。理论结果显示在现有构造基础上实现了显著的安全增强。

Conclusion: 该研究为SPHINCS+和NTRU提供了可实现的、更优化的参数选择，为后量密码标准化提供了重要的安全基础和参考。

Abstract: The imminent threat of quantum computing necessitates quantum-resistant
cryptosystems. This paper establishes tight security bounds for two NIST PQC
finalists: SPHINCS+ (hash-based) and NTRU (lattice-based). Our key
contributions include: (1) A quantum attack model incorporating decoherence
effects ($\tau_d$) and parallelization limits; (2) Improved entropy
concentration inequalities reducing SPHINCS+ parameters by 15-20\%; (3)
Optimized NTRU lattice parameters via quantum lattice entropy $H_Q(\Lambda)$;
(4) Tightened NTRU-to-LWE reduction with polynomial-factor improvement.
Theoretical results demonstrate significant security enhancement over existing
constructions, providing implementable parameters for standardization.

</details>


### [11] [The Aegis Protocol: A Foundational Security Framework for Autonomous AI Agents](https://arxiv.org/abs/2508.19267)
*Sai Teja Reddy Adapala,Yashwanth Reddy Alugubelly*

Main category: cs.CR

TL;DR: Aegis Protocol是一个针对自主AI多代理系统的分层安全框架，通过去中心化身份、后量子密码学和零知识证明技术，在模拟环境中实现了100%的攻击防御成功率。


<details>
  <summary>Details</summary>
Motivation: 随着自主AI代理的普及，传统网络安全范式无法应对多代理系统中的系统性安全风险，如控制流劫持和级联故障，需要新的安全解决方案。

Method: 提出Aegis Protocol三层安全框架：1) 使用W3C去中心化标识符(DIDs)实现不可欺骗的代理身份；2) 采用NIST标准后量子密码学(PQC)保证通信完整性；3) 使用Halo2零知识证明系统实现可验证的隐私保护策略合规。

Result: 在模拟1000个代理的离散事件仿真中，经过20000次攻击试验，攻击成功率为0%。策略验证的中位证明生成延迟为2.79秒，为该类安全技术建立了性能基准。

Conclusion: 虽然评估基于仿真且处于早期阶段，但Aegis Protocol为安全、可扩展的自主AI系统奠定了基础，并提供了未来实证研究的可复现基准。

Abstract: The proliferation of autonomous AI agents marks a paradigm shift toward
complex, emergent multi-agent systems. This transition introduces systemic
security risks, including control-flow hijacking and cascading failures, that
traditional cybersecurity paradigms are ill-equipped to address. This paper
introduces the Aegis Protocol, a layered security framework designed to provide
strong security guarantees for open agentic ecosystems. The protocol integrates
three technological pillars: (1) non-spoofable agent identity via W3C
Decentralized Identifiers (DIDs); (2) communication integrity via
NIST-standardized post-quantum cryptography (PQC); and (3) verifiable,
privacy-preserving policy compliance using the Halo2 zero-knowledge proof (ZKP)
system. We formalize an adversary model extending Dolev-Yao for agentic threats
and validate the protocol against the STRIDE framework. Our quantitative
evaluation used a discrete-event simulation, calibrated against cryptographic
benchmarks, to model 1,000 agents. The simulation showed a 0 percent success
rate across 20,000 attack trials. For policy verification, analysis of the
simulation logs reported a median proof-generation latency of 2.79 seconds,
establishing a performance baseline for this class of security. While the
evaluation is simulation-based and early-stage, it offers a reproducible
baseline for future empirical studies and positions Aegis as a foundation for
safe, scalable autonomous AI.

</details>


### [12] [MixGAN: A Hybrid Semi-Supervised and Generative Approach for DDoS Detection in Cloud-Integrated IoT Networks](https://arxiv.org/abs/2508.19273)
*Tongxi Wu,Chenwei Xu,Jin Yang*

Main category: cs.CR

TL;DR: MixGAN是一种混合检测方法，通过条件生成、半监督学习和鲁棒特征提取来解决IoT云环境中DDoS检测的类别不平衡和标签稀缺问题


<details>
  <summary>Details</summary>
Motivation: 云集成IoT系统的普及增加了DDoS攻击风险，但由于复杂的流量动态、严重的类别不平衡和标签数据稀缺，DDoS检测仍然具有挑战性

Method: 使用1-D WideResNet骨干网络捕获时间流量模式，预训练CTGAN生成少数类样本，引入MixUp-Average-Sharpen策略处理噪声伪标签

Result: 在NSL-KDD、BoT-IoT和CICIoT2023数据集上，MixGAN相比最先进方法准确率提高2.5%，TPR和TNR均提升4%

Conclusion: MixGAN在大规模IoT云环境中表现出强大的鲁棒性，有效解决了DDoS检测中的类别不平衡和标签稀缺问题

Abstract: The proliferation of cloud-integrated IoT systems has intensified exposure to
Distributed Denial of Service (DDoS) attacks due to the expanded attack
surface, heterogeneous device behaviors, and limited edge protection. However,
DDoS detection in this context remains challenging because of complex traffic
dynamics, severe class imbalance, and scarce labeled data. While recent methods
have explored solutions to address class imbalance, many still struggle to
generalize under limited supervision and dynamic traffic conditions. To
overcome these challenges, we propose MixGAN, a hybrid detection method that
integrates conditional generation, semi-supervised learning, and robust feature
extraction. Specifically, to handle complex temporal traffic patterns, we
design a 1-D WideResNet backbone composed of temporal convolutional layers with
residual connections, which effectively capture local burst patterns in traffic
sequences. To alleviate class imbalance and label scarcity, we use a pretrained
CTGAN to generate synthetic minority-class (DDoS attack) samples that
complement unlabeled data. Furthermore, to mitigate the effect of noisy
pseudo-labels, we introduce a MixUp-Average-Sharpen (MAS) strategy that
constructs smoothed and sharpened targets by averaging predictions over
augmented views and reweighting them towards high-confidence classes.
Experiments on NSL-KDD, BoT-IoT, and CICIoT2023 demonstrate that MixGAN
achieves up to 2.5% higher accuracy and 4% improvement in both TPR and TNR
compared to state-of-the-art methods, confirming its robustness in large-scale
IoT-cloud environments. The source code is publicly available at
https://github.com/0xCavaliers/MixGAN.

</details>


### [13] [Towards Production-Worthy Simulation for Autonomous Cyber Operations](https://arxiv.org/abs/2508.19278)
*Konur Tholl,Mariam El Mezouar,Ranwa Al Mallah*

Main category: cs.CR

TL;DR: 本研究扩展了CybORG的Cage Challenge 2环境，新增了Patch、Isolate和Unisolate三个动作，改进了奖励信号和特征空间设计，验证了DQN和PPO智能体的训练效果。


<details>
  <summary>Details</summary>
Motivation: 模拟环境在自主网络操作中至关重要，但需要准确反映真实网络安全场景并提供有效的强化学习训练信号。现有环境需要扩展以更好地模拟人类操作员的实际能力。

Method: 扩展CybORG环境功能，新增三个操作动作；重新设计奖励信号和智能体特征空间；使用DQN和PPO算法在更新后的环境中进行训练验证。

Result: 研究表明CybORG能够成功扩展现实功能，同时保持为强化学习智能体生成有效训练信号的能力。DQN和PPO智能体在新环境中表现良好。

Conclusion: 该框架证明了模拟环境可以通过功能扩展来更好地反映真实网络操作场景，为自主网络操作中的强化学习训练提供了有效的环境支持。

Abstract: Simulated environments have proven invaluable in Autonomous Cyber Operations
(ACO) where Reinforcement Learning (RL) agents can be trained without the
computational overhead of emulation. These environments must accurately
represent cybersecurity scenarios while producing the necessary signals to
support RL training. In this study, we present a framework where we first
extend CybORG's Cage Challenge 2 environment by implementing three new actions:
Patch, Isolate, and Unisolate, to better represent the capabilities available
to human operators in real-world settings. We then propose a design for agent
development where we modify the reward signals and the agent's feature space to
enhance training performance. To validate these modifications, we train DQN and
PPO agents in the updated environment. Our study demonstrates that CybORG can
be extended with additional realistic functionality, while maintaining its
ability to generate informative training signals for RL agents.

</details>


### [14] [CORTEX: Composite Overlay for Risk Tiering and Exposure in Operational AI Systems](https://arxiv.org/abs/2508.19281)
*Aoun E Muhammad,Kin Choong Yow,Jamel Baili,Yongwon Cho,Yunyoung Nam*

Main category: cs.CR

TL;DR: CORTEX是一个多层次的AI系统风险评估框架，通过分析1200多个AI事故案例，将故障模式分为29个技术漏洞组，采用五层架构进行综合风险评分。


<details>
  <summary>Details</summary>
Motivation: 随着AI系统在高风险领域的广泛应用，系统故障已从理论可能演变为实际存在的系统性风险，需要建立有效的风险评估框架。

Method: 基于AI事故数据库的实证分析，开发五层评分架构：效用调整的似然性×影响计算、治理和情境覆盖、技术表面评分、环境和残余修正、贝叶斯风险聚合和蒙特卡洛模拟。

Result: 提出了CORTEX框架，能够生成复合风险评分，可用于AI风险登记、模型审计、合规检查和动态治理仪表板。

Conclusion: CORTEX提供了一个可操作的多层次风险评估框架，有助于系统性识别和管理AI系统的脆弱性和风险。

Abstract: As the deployment of Artificial Intelligence (AI) systems in high-stakes
sectors - like healthcare, finance, education, justice, and infrastructure has
increased - the possibility and impact of failures of these systems have
significantly evolved from being a theoretical possibility to practical
recurring, systemic risk. This paper introduces CORTEX (Composite Overlay for
Risk Tiering and Exposure), a multi-layered risk scoring framework proposed to
assess and score AI system vulnerabilities, developed on empirical analysis of
over 1,200 incidents documented in the AI Incident Database (AIID), CORTEX
categorizes failure modes into 29 technical vulnerability groups. Each
vulnerability is scored through a five-tier architecture that combines: (1)
utility-adjusted Likelihood x Impact calculations; (2) governance + contextual
overlays aligned with regulatory frameworks, such as the EU AI Act, NIST RMF,
OECD principles; (3) technical surface scores, covering exposure vectors like
drift, traceability, and adversarial risk; (4) environmental and residual
modifiers tailored to context of where these systems are being deployed to use;
and (5) a final layered assessment via Bayesian risk aggregation and Monte
Carlo simulation to model volatility and long-tail risks. The resulting
composite score can be operationalized across AI risk registers, model audits,
conformity checks, and dynamic governance dashboards.

</details>


### [15] [Rethinking Denial-of-Service: A Conditional Taxonomy Unifying Availability and Sustainability Threats](https://arxiv.org/abs/2508.19283)
*Mark Dorsett,Scott Man,Tim Koussas*

Main category: cs.CR

TL;DR: 提出了一个基于条件的统一框架，用于分类传统和云时代的拒绝服务攻击，包含条件树分类法、层次格结构和维恩图三个模型，通过六个可观察条件实现一致攻击分类。


<details>
  <summary>Details</summary>
Motivation: 为了解决传统DoS攻击分类方法无法有效处理云时代新型攻击（如可持续性攻击）的问题，提供一个统一的分析框架来帮助威胁建模、缓解策略设计和攻击者意图分类。

Method: 开发了三个相互关联的模型：基于六个可观察条件（C0-C5）的形式化条件树分类法、基于序理论的层次格结构、以及概念性维恩图，这些条件包括源分布、流量规模、基础设施目标和金融利用等。

Result: 框架能够一致分类已知攻击类型（DoS、DDoS、LDoS等），支持识别新兴或混合变种，特别适用于云原生和无服务器环境中的可持续性攻击分析。

Conclusion: 该工作通过提供具有理论基础和实际应用性的结构化分类法，推进了对拒绝攻击的理解，为防御者、研究者和云架构师提供了共享词汇表来解释和缓解不断演变的威胁向量。

Abstract: This paper proposes a unified, condition-based framework for classifying both
legacy and cloud-era denial-of-service (DoS) attacks. The framework comprises
three interrelated models: a formal conditional tree taxonomy, a hierarchical
lattice structure based on order theory, and a conceptual Venn diagram. At its
core, the taxonomy introduces six observable conditions (C0-C5) grounded in
real-world attack behaviours, including source distribution, traffic volume,
infrastructure targeting, and financial exploitation. These conditions enable
consistent classification of known attacks-such as DoS, DDoS, LDoS, LDDoS,
EDoS, DoW, and DDoW, while supporting identification of emerging or hybrid
variants. The lattice structure captures the cumulative satisfaction of
conditions, allowing hierarchical reasoning across denial attack classes. The
Venn diagram highlights conceptual overlaps between availability- and
sustainability-focused attacks, improving comparative insight. Together, these
models provide a robust analytical lens for threat modeling, mitigation
strategy design, and attacker intent classification. The framework is
particularly relevant in cloud-native and serverless environments, where
sustainability-based attacks are increasingly impactful yet under-recognised.
Its extensibility also permits future integration of socio-technical or
behavioural dimensions. By offering a structured taxonomy with theoretical
grounding and real-world applicability, this work advances denial attack
comprehension and equips defenders, researchers, and cloud architects with a
shared vocabulary for interpreting and mitigating evolving threat vectors.

</details>


### [16] [A Comprehensive Review of Denial of Wallet Attacks in Serverless Architectures](https://arxiv.org/abs/2508.19284)
*Mark Dorsett,Scott Mann,Jabed Chowdhury,Abdun Mahmood*

Main category: cs.CR

TL;DR: 这是首个专门研究服务无服务计算中拒付攻击(DoW)的综述性文献回顾，详细分析了其金融影响、攻击技术、治理策略和检测机制，为保护按量付费云环境提供基础支撑。


<details>
  <summary>Details</summary>
Motivation: 拒付攻击(DoW)对依赖函数即服务(FaaS)模型的服务无服务架构构成了独特和日益增长的威胁，利用按量付费计费模式给应用所有者带来财务负担，需要系统性研究来应对这种新兴威胁。

Method: 该研究进行了综述性文献回顾，追踪了DoW研究的演变过程，从初期意识到攻击分类，再到检测和治理策略的进展。包括攻击类型分类、模拟工具(DoWTS)、机器学习方法(Gringotts、DoWNet)等关键技术的分析。

Result: 研究系统性总结了DoW攻击的主要类型(爆炸式DDoW、持续雷藏式DDoW、背景链式DDoW)，评估了各种检测和治理方法的效果，并首次详细分析了用于DoW研究的模拟和数据生成工具。

Conclusion: 虽然取得了重要进展，但DoW防护仍面临挑战，主要是缺乏真实世界数据和需要更适应性强的计费模型。该研究为保护按量付费云环境的未来研究和产业实践提供了基础性资源。

Abstract: The Denial of Wallet (DoW) attack poses a unique and growing threat to
serverless architectures that rely on Function-as-a-Service (FaaS) models,
exploiting the cost structure of pay-as-you-go billing to financially burden
application owners. Unlike traditional Denial of Service (DoS) attacks, which
aim to exhaust resources and disrupt service availability, DoW attacks focus on
escalating costs without impacting service operation. This review traces the
evolution of DoW research, from initial awareness and attack classification to
advancements in detection and mitigation strategies. Key developments include
the categorisation of attack types-such as Blast DDoW, Continual Inconspicuous
DDoW, and Background Chained DDoW-and the creation of simulation tools like
DoWTS, which enable safe experimentation and data generation. Recent
advancements highlight machine learning approaches, including systems like
Gringotts and DoWNet, which leverage deep learning and anomaly detection to
identify malicious traffic patterns. Although substantial progress has been
made, challenges persist, notably the lack of real-world data and the need for
adaptive billing models. This is the first comprehensive literature review
dedicated strictly to Denial of Wallet attacks, providing an in-depth analysis
of their financial impacts, attack techniques, mitigation strategies, and
detection mechanisms within serverless computing. The paper also presents the
first detailed examination of simulation and data generation tools used for DoW
research, addressing a critical gap in existing cybersecurity literature. By
synthesising these key areas, this study serves as a foundational resource for
future research and industry efforts in securing pay-as-you-go cloud
environments.

</details>


### [17] [RL-Finetuned LLMs for Privacy-Preserving Synthetic Rewriting](https://arxiv.org/abs/2508.19286)
*Zhan Shi,Yefeng Yuan,Yuhong Liu,Liang Cheng,Yi Fang*

Main category: cs.CR

TL;DR: 提出基于强化学习的框架，通过复合奖励函数微调大语言模型，在保持语义质量的同时显著提升隐私保护效果


<details>
  <summary>Details</summary>
Motivation: 解决机器学习系统使用包含敏感信息的大型数据集时的隐私保护问题，传统匿名化技术无法有效防御基于写作风格、主题焦点等隐式信号的推理攻击

Method: 使用强化学习框架微调大语言模型，采用复合奖励函数联合优化显式和隐式隐私、语义保真度和输出多样性，通过最小生成树分析潜在表示的结构模式

Result: 实证结果显示该方法显著增强了作者混淆和隐私指标，同时不降低语义质量

Conclusion: 为大语言模型时代提供了一种可扩展且模型无关的隐私保护数据生成解决方案

Abstract: The performance of modern machine learning systems depends on access to
large, high-quality datasets, often sourced from user-generated content or
proprietary, domain-specific corpora. However, these rich datasets inherently
contain sensitive personal information, raising significant concerns about
privacy, data security, and compliance with regulatory frameworks. While
conventional anonymization techniques can remove explicit identifiers, such
removal may result in performance drop in downstream machine learning tasks.
More importantly, simple anonymization may not be effective against inference
attacks that exploit implicit signals such as writing style, topical focus, or
demographic cues, highlighting the need for more robust privacy safeguards
during model training. To address the challenging issue of balancing user
privacy and data utility, we propose a reinforcement learning framework that
fine-tunes a large language model (LLM) using a composite reward function that
jointly optimizes for explicit and implicit privacy, semantic fidelity, and
output diversity. To effectively capture population level regularities, the
privacy reward combines semantic cues with structural patterns derived from a
minimum spanning tree (MST) over latent representations. By modeling these
privacy-sensitive signals in their distributional context, the proposed
approach guides the model to generate synthetic rewrites that preserve utility
while mitigating privacy risks. Empirical results show that the proposed method
significantly enhances author obfuscation and privacy metrics without degrading
semantic quality, providing a scalable and model-agnostic solution for privacy
preserving data generation in the era of large language models.

</details>


### [18] [Prompt-in-Content Attacks: Exploiting Uploaded Inputs to Hijack LLM Behavior](https://arxiv.org/abs/2508.19287)
*Zhuotao Lian,Weiyu Wang,Qingkui Zeng,Toru Nakanishi,Teruaki Kitasuka,Chunhua Su*

Main category: cs.CR

TL;DR: 本文发现了一种新的LLM攻击方式——内容注入提示攻击，通过在看似良性的输入中嵌入恶意指令来操纵模型输出，揭示了现实世界LLM工作流程中的潜在威胁。


<details>
  <summary>Details</summary>
Motivation: 随着LLM在文档摘要、问答等应用中的广泛部署，用户提交的内容可能包含隐藏的恶意指令，但目前系统对此类攻击缺乏足够的防护措施。

Method: 通过在看似正常的用户输入中嵌入对抗性指令，利用LLM处理时的提示拼接和输入隔离不足等漏洞，演示了在不同流行平台上的攻击可行性。

Result: 研究证明了内容注入提示攻击的可行性，能够导致模型产生带有偏见的摘要、捏造的主张或误导性建议，而用户和系统都难以察觉。

Conclusion: 内容注入提示攻击构成了现实世界LLM工作流程中一个微妙但实际的威胁，需要开发有效的缓解策略来应对这种新型安全风险。

Abstract: Large Language Models (LLMs) are widely deployed in applications that accept
user-submitted content, such as uploaded documents or pasted text, for tasks
like summarization and question answering. In this paper, we identify a new
class of attacks, prompt in content injection, where adversarial instructions
are embedded in seemingly benign inputs. When processed by the LLM, these
hidden prompts can manipulate outputs without user awareness or system
compromise, leading to biased summaries, fabricated claims, or misleading
suggestions. We demonstrate the feasibility of such attacks across popular
platforms, analyze their root causes including prompt concatenation and
insufficient input isolation, and discuss mitigation strategies. Our findings
reveal a subtle yet practical threat in real-world LLM workflows.

</details>


### [19] [Tricking LLM-Based NPCs into Spilling Secrets](https://arxiv.org/abs/2508.19288)
*Kyohei Shiomi,Zhuotao Lian,Toru Nakanishi,Teruaki Kitasuka*

Main category: cs.CR

TL;DR: 研究探讨对抗性提示注入是否能导致基于LLM的游戏NPC泄露本应保密的背景故事


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型越来越多地用于生成游戏NPC的动态对话，其集成带来了新的安全隐患，需要研究对抗性攻击对NPC保密信息的影响

Method: 通过对抗性提示注入技术测试LLM-based NPCs

Result: 研究发现对抗性提示注入确实能够导致NPC泄露隐藏的背景秘密

Conclusion: LLM在游戏NPC中的应用存在安全风险，需要开发相应的防护机制来防止敏感信息泄露

Abstract: Large Language Models (LLMs) are increasingly used to generate dynamic
dialogue for game NPCs. However, their integration raises new security
concerns. In this study, we examine whether adversarial prompt injection can
cause LLM-based NPCs to reveal hidden background secrets that are meant to
remain undisclosed.

</details>


### [20] [Stand on The Shoulders of Giants: Building JailExpert from Previous Attack Experience](https://arxiv.org/abs/2508.19292)
*Xi Wang,Songlei Jian,Shasha Li,Xiaopeng Li,Bin Ji,Jun Ma,Xiaodong Liu,Jing Wang,Feilong Bao,Jianfeng Zhang,Baosheng Wang,Jie Yu*

Main category: cs.CR

TL;DR: JailExpert是一个自动化越狱框架，通过结构化表示和语义分组整合历史攻击经验，显著提升LLM越狱攻击的成功率和效率。


<details>
  <summary>Details</summary>
Motivation: 现有越狱方法存在效率低下和重复优化的问题，忽视了历史攻击经验的价值，需要更好的方法来整合过往经验以辅助当前攻击尝试。

Method: 提出JailExpert框架，首次实现经验结构的正式表示，基于语义漂移进行经验分组，并支持经验池的动态更新。

Result: 相比当前最先进的黑盒越狱方法，JailExpert平均攻击成功率提升17%，攻击效率提高2.7倍。

Conclusion: JailExpert通过有效整合历史攻击经验，显著提升了LLM越狱攻击的效果和效率，为识别模型漏洞和开发鲁棒安全框架提供了有力工具。

Abstract: Large language models (LLMs) generate human-aligned content under certain
safety constraints. However, the current known technique ``jailbreak prompt''
can circumvent safety-aligned measures and induce LLMs to output malicious
content. Research on Jailbreaking can help identify vulnerabilities in LLMs and
guide the development of robust security frameworks. To circumvent the issue of
attack templates becoming obsolete as models evolve, existing methods adopt
iterative mutation and dynamic optimization to facilitate more automated
jailbreak attacks. However, these methods face two challenges: inefficiency and
repetitive optimization, as they overlook the value of past attack experiences.
To better integrate past attack experiences to assist current jailbreak
attempts, we propose the \textbf{JailExpert}, an automated jailbreak framework,
which is the first to achieve a formal representation of experience structure,
group experiences based on semantic drift, and support the dynamic updating of
the experience pool. Extensive experiments demonstrate that JailExpert
significantly improves both attack effectiveness and efficiency. Compared to
the current state-of-the-art black-box jailbreak methods, JailExpert achieves
an average increase of 17\% in attack success rate and 2.7 times improvement in
attack efficiency. Our implementation is available at
\href{https://github.com/xiZAIzai/JailExpert}{XiZaiZai/JailExpert}

</details>


### [21] [Leveraging 3D Technologies for Hardware Security: Opportunities and Challenges](https://arxiv.org/abs/2508.19309)
*Peng Gu,Shuangchen Li,Dylan Stow,Russell Barnes,Liu Liu,Yuan Xie,Eren Kursshan*

Main category: cs.CR

TL;DR: 本文探讨了利用2.5D和3D堆叠技术提升系统安全性的新方法，包括侧信道防护、安全制造和内存计算安全等创新方案


<details>
  <summary>Details</summary>
Motivation: 当前2.5D/3D集成技术面临侧信道攻击、硬件木马、安全制造和IP盗版等新兴安全挑战，需要新的安全设计方法

Method: 提出四种安全设计方案：3D架构屏蔽侧信道信息、基于有源中介层的拆分制造、单片3D IC上的电路伪装、以及基于3D IC的安全内存计算

Result: 新设计能够改进现有安全防护措施，并提供新的安全特性，展示了2.5D/3D技术在安全应用方面的优势

Conclusion: 2.5D和3D堆叠技术为构建安全系统提供了独特机会，通过利用其固有特性可以应对多种安全威胁，但同时也面临新的挑战

Abstract: 3D die stacking and 2.5D interposer design are promising technologies to
improve integration density, performance and cost. Current approaches face
serious issues in dealing with emerging security challenges such as side
channel attacks, hardware trojans, secure IC manufacturing and IP piracy. By
utilizing intrinsic characteristics of 2.5D and 3D technologies, we propose
novel opportunities in designing secure systems. We present: (i) a 3D
architecture for shielding side-channel information; (ii) split fabrication
using active interposers; (iii) circuit camouflage on monolithic 3D IC, and
(iv) 3D IC-based security processing-in-memory (PIM). Advantages and challenges
of these designs are discussed, showing that the new designs can improve
existing countermeasures against security threats and further provide new
security features.

</details>


### [22] [An Investigation on Group Query Hallucination Attacks](https://arxiv.org/abs/2508.19321)
*Kehao Miao,Xiaolong Jin*

Main category: cs.CR

TL;DR: Group Query Attack是一种通过同时向大语言模型提交多个查询的技术，研究发现这种攻击会显著降低模型性能并可能触发潜在后门。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型的广泛应用，了解其在用户交互中的潜在失败模式至关重要。用户经常在单次对话中提出多个问题，因此需要研究连续提示的累积上下文如何影响模型输出。

Method: 提出Group Query Attack技术，通过同时向LLMs呈现查询组来模拟多问题场景，研究累积上下文对模型输出的影响。

Result: Group Query Attack显著降低了针对特定任务微调的模型性能，能够诱导触发LLMs的潜在后门风险，在数学推理和代码生成等推理任务中对预训练和对齐模型也有效。

Conclusion: 多查询同时提交的Group Query Attack技术揭示了LLMs在累积上下文环境下的脆弱性，对模型安全和鲁棒性提出了重要挑战。

Abstract: With the widespread use of large language models (LLMs), understanding their
potential failure modes during user interactions is essential. In practice,
users often pose multiple questions in a single conversation with LLMs.
Therefore, in this study, we propose Group Query Attack, a technique that
simulates this scenario by presenting groups of queries to LLMs simultaneously.
We investigate how the accumulated context from consecutive prompts influences
the outputs of LLMs. Specifically, we observe that Group Query Attack
significantly degrades the performance of models fine-tuned on specific tasks.
Moreover, we demonstrate that Group Query Attack induces a risk of triggering
potential backdoors of LLMs. Besides, Group Query Attack is also effective in
tasks involving reasoning, such as mathematical reasoning and code generation
for pre-trained and aligned models.

</details>


### [23] [A Technical Review on Comparison and Estimation of Steganographic Tools](https://arxiv.org/abs/2508.19323)
*Ms. Preeti P. Bhatt,Rakesh R. Savant*

Main category: cs.CR

TL;DR: 这篇评论文章对图像隐写术工具进行了分类和比较分析，通过实验测试6款常用工具的性能效果。


<details>
  <summary>Details</summary>
Motivation: 图像隐写术工具市场上产品众多，需要通过系统性比较分析来识别哪些工具在效率和性能方面更优称。

Method: 选择6款常用的图像隐写术工具，使用相同的输入图像和文本数据进行嵌入测试，基于图像大小、尺寸、像素值和直方图差异等特征进行分析。

Result: 实验结果显示所有6款工具都表现相似，但某些软件在效率方面更优异，性能差异主要受图像特征影响。

Conclusion: 不同图像隐写工具在性能上存在稍微差异，选择工具时需考虑图像格式和特定需求，实验结果为用户选择适合的隐写工具提供了实用参考。

Abstract: Steganography is technique of hiding a data under cover media using different
steganography tools. Image steganography is hiding of data
(Text/Image/Audio/Video) under a cover as Image. This review paper presents
classification of image steganography and the comparison of various Image
steganography tools using different image formats. Analyzing numerous tools on
the basis of Image features and extracting the best one. Some of the tools
available in the market were selected based on the frequent use; these tools
were tested using the same input on all of them. Specific text was embedded
within all host images for each of the six Steganography tools selected. The
results of the experiment reveal that all the six tools were relatively
performing at the same level, though some software performs better than others
through efficiency. And it was based on the image features like size,
dimensions, and pixel value and histogram differentiation.

</details>


### [24] [Just Dork and Crawl: Measuring Illegal Online Gambling Defacement in Indonesian Websites](https://arxiv.org/abs/2508.19368)
*Luqman Muhammad Zagi,Girindro Pringgo Digdo,Wervyan Shalannanda*

Main category: cs.CR

TL;DR: 本研究使用关键词搜索和爬虫技术识别了453个被非法在线赌博推广者篡改的印尼网站，揭示了篡改行为的多样性和网站响应不一致的问题。


<details>
  <summary>Details</summary>
Motivation: 调查印尼网站被非法在线赌博推广者篡改的情况，了解篡改行为的规模和特点，为加强网络安全防御提供依据。

Method: 采用轻量级方法，结合关键词驱动的搜索和系统化爬虫技术，通过关键词计数可靠区分真假阳性结果。

Result: 发现453个被篡改网页，识别出重复篡改、固定实例、关键词修改和重定向等多种行为模式，平均响应时间为75.3小时。

Conclusion: 简单可复现的技术能够有效洞察网站篡改的规模、持续性和动态特征，强调持续监测对防御在线赌博活动的重要性。

Abstract: This study investigates the defacement of Indonesian websites by actors
promoting illegal online gambling. Using a lightweight methodology that
combines keyword-driven dorking with systematic crawling, we identified 453
defaced webpages within one month. Although dorking alone yielded a false
positive rate of approximately 20.3\%, the integration of crawling and
keyword-counting enabled reliable differentiation between true and false
positives. Our measurements revealed diverse defacement behaviors, including
repeat defacements (150 cases), fixed instances (129), keyword modifications
(55), and redirections or hidden URL injections. In total, 8,837 unique
third-party URLs spanning 5,930 domains were captured, with a small subset
recurring across multiple sites. Website responses were inconsistent, with an
average reaction time of 75.3 hours. These findings demonstrate that simple,
reproducible techniques can provide meaningful insights into the scale,
persistence, and dynamics of defacement, highlighting the importance of
continuous measurement for strengthening defenses against online gambling
activities.

</details>


### [25] [A NIS2 pan-European registry for identifying and classifying essential and important entities](https://arxiv.org/abs/2508.19395)
*Fabian Aude Steen,Daniel Assani Shabani*

Main category: cs.CR

TL;DR: 该论文分析了欧盟NIS2指令，将其法律条款转化为具体技术要求，设计并实现了一个模块化的注册系统，用于支持欧盟成员国网络安全治理。


<details>
  <summary>Details</summary>
Motivation: NIS2指令要求欧盟成员国建立网络安全治理体系，但缺乏具体技术实施方案。论文旨在将复杂的法律条款转化为可操作的技术需求，帮助监管机构履行义务。

Method: 采用设计科学研究方法，将法律条款转化为结构化工作流程、确定性分类算法和交互式仪表板，开发模块化注册系统。

Result: 开发了一个自动化关键监管流程（实体注册、分类、通知）的系统，支持自动和手动注册，引入情境标签系统处理边缘案例和风险因素。

Conclusion: 论文提供了一个可重用的框架，连接法律解释和技术实施，为国家和欧盟层面的NIS2网络安全治理提供可扩展解决方案，并指出了未来研究方向。

Abstract: The NIS2 Directive establishes a common cybersecurity governance model across
the European Union, requiring member states to identify, classify, and
supervise essential and important entities. As part of a broader governance
network, member states are also obligated to notify the European Commission,
the Cooperation Group, and ENISA about their cybersecurity infrastructure
landscape. This thesis presents an analysis of the NIS2 Directive in this
context and translates its provisions into concrete technical requirements.
These requirements inform the design and implementation of a modular, legally
grounded registry system intended to support competent authorities across the
EU in meeting their obligations. Using the Design Science Research methodology,
the thesis transforms complex legal provisions into structured workflows,
deterministic classification algorithms, and interactive dashboards. The
resulting system automates key regulatory processes, including entity
registration, classification, and notification, while enabling context-aware
supervision and reducing administrative burden. It supports both automated and
manual registration methods and introduces a contextual labeling system to
handle edge cases, risk factors, and cross-directive dependencies. Although
developed for the Norwegian regulatory ecosystem, the system is designed for
adaptation by other member states with minimal modification. This thesis
contributes a reusable framework that bridges legal interpretation and
technical implementation, offering a scalable solution for national and
EU-level NIS2 cybersecurity governance. It also identifies key limitations and
outlines opportunities for future research and development.

</details>


### [26] [Formal Verification of Physical Layer Security Protocols for Next-Generation Communication Networks](https://arxiv.org/abs/2508.19430)
*Kangfeng Ye,Roberto Metere,Jim Woodcock,Poonam Yadav*

Main category: cs.CR

TL;DR: 本文提出基于Isabelle的形式化验证框架，重新建模Needham-Schroeder协议，支持密码学和物理层安全技术，实现了比传统ProVerif方法更全面的安全分析。


<details>
  <summary>Details</summary>
Motivation: 传统ProVerif方法在验证安全协议时存在局限性，无法深入理解超出验证结果的安全性问题，需要更强大的形式化验证工具来支持交互式和自动化分析。

Method: 使用Isabelle形式化方法重新建模协议，开发通用可配置的验证框架，支持密码学和物理层安全（水印和干扰技术），并通过新的web界面进行综合分析。

Result: 成功复现并强化了先前关于保密性的结果，发现了一个罕见但预期的结果：在所有检查场景中（包括保密性受损的情况）真实性都得以保持。提出了基于物理层安全的Diffie-Hellman协议，分析显示该协议在会话密钥推导和认证方面是安全的。

Conclusion: 新方法在形式化验证安全属性方面展现出超越传统方法的优势，证明了其在验证超出常规方法范围的安全属性时的鲁棒性。

Abstract: Formal verification is crucial for ensuring the robustness of security
protocols against adversarial attacks. The Needham-Schroeder protocol, a
foundational authentication mechanism, has been extensively studied, including
its integration with Physical Layer Security (PLS) techniques such as
watermarking and jamming. Recent research has used ProVerif to verify these
mechanisms in terms of secrecy. However, the ProVerif-based approach limits the
ability to improve understanding of security beyond verification results. To
overcome these limitations, we re-model the same protocol using an Isabelle
formalism that generates sound animation, enabling interactive and automated
formal verification of security protocols. Our modelling and verification
framework is generic and highly configurable, supporting both cryptography and
PLS. For the same protocol, we have conducted a comprehensive analysis (secrecy
and authenticity in four different eavesdropper locations under both passive
and active attacks) using our new web interface. Our findings not only
successfully reproduce and reinforce previous results on secrecy but also
reveal an uncommon but expected outcome: authenticity is preserved across all
examined scenarios, even in cases where secrecy is compromised. We have
proposed a PLS-based Diffie-Hellman protocol that integrates watermarking and
jamming, and our analysis shows that it is secure for deriving a session key
with required authentication. These highlight the advantages of our novel
approach, demonstrating its robustness in formally verifying security
properties beyond conventional methods.

</details>


### [27] [CITADEL: Continual Anomaly Detection for Enhanced Learning in IoT Intrusion Detection](https://arxiv.org/abs/2508.19450)
*Elvin Li,Onat Gungor,Zhengli Shang,Tajana Rosing*

Main category: cs.CR

TL;DR: CITADEL是一个自监督持续学习框架，通过表格到图像转换和记忆感知掩码自编码器，有效解决IoT入侵检测中的灾难性遗忘问题，在动态环境中实现72.9%的性能提升。


<details>
  <summary>Details</summary>
Motivation: 物联网设备计算资源有限且高度互联，容易受到各种网络威胁。传统机器学习入侵检测系统在面对新兴威胁时适应性差，且在持续学习中存在灾难性遗忘问题。

Method: 提出CITADEL框架，包含：1）表格到图像转换模块；2）记忆感知掩码自编码器进行自监督表示学习；3）不依赖标注攻击数据的新颖性检测组件；4）优化的记忆巩固机制。

Result: 在多个入侵检测数据集上的实验表明，CITADEL相比基于VAE的终身异常检测器（VLAD），在关键检测和保留指标上实现了高达72.9%的改进。

Conclusion: CITADEL能够逐步适应新兴行为，同时保持检测已知威胁的能力，在动态物联网环境中表现出色，为解决持续学习中的灾难性遗忘问题提供了有效方案。

Abstract: The Internet of Things (IoT), with its high degree of interconnectivity and
limited computational resources, is particularly vulnerable to a wide range of
cyber threats. Intrusion detection systems (IDS) have been extensively studied
to enhance IoT security, and machine learning-based IDS (ML-IDS) show
considerable promise for detecting malicious activity. However, their
effectiveness is often constrained by poor adaptability to emerging threats and
the issue of catastrophic forgetting during continuous learning. To address
these challenges, we propose CITADEL, a self-supervised continual learning
framework designed to extract robust representations from benign data while
preserving long-term knowledge through optimized memory consolidation
mechanisms. CITADEL integrates a tabular-to-image transformation module, a
memory-aware masked autoencoder for self-supervised representation learning,
and a novelty detection component capable of identifying anomalies without
dependence on labeled attack data. Our design enables the system to
incrementally adapt to emerging behaviors while retaining its ability to detect
previously observed threats. Experiments on multiple intrusion datasets
demonstrate that CITADEL achieves up to a 72.9% improvement over the VAE-based
lifelong anomaly detector (VLAD) in key detection and retention metrics,
highlighting its effectiveness in dynamic IoT environments.

</details>


### [28] [ReLATE+: Unified Framework for Adversarial Attack Detection, Classification, and Resilient Model Selection in Time-Series Classification](https://arxiv.org/abs/2508.19456)
*Cagla Ipek Kocal,Onat Gungor,Tajana Rosing,Baris Aksanli*

Main category: cs.CR

TL;DR: ReLATE+是一个对抗性攻击检测和模型选择框架，通过数据集相似性分析重用最佳模型，减少77.68%计算开销，性能接近Oracle水平


<details>
  <summary>Details</summary>
Motivation: 解决时间序列分类中深度学习模型计算开销大、对抗性攻击威胁严重的问题，需要既能保证鲁棒性能又能高效选择模型的方法

Method: 首先检测输入数据是否为对抗性攻击并分类攻击类型，然后基于数据集相似性从知识库中识别相似数据集并重用其最佳性能模型

Result: 平均减少77.68%计算开销，在保持强性能的同时（与Oracle性能差距在2.02%以内），提高了对抗性恢复能力并简化了鲁棒模型选择

Conclusion: ReLATE+框架通过重用先验知识显著降低重新训练成本，在不同领域和数据分布下都能良好泛化，为时间序列分类提供了高效且鲁棒的解决方案

Abstract: Minimizing computational overhead in time-series classification, particularly
in deep learning models, presents a significant challenge due to the high
complexity of model architectures and the large volume of sequential data that
must be processed in real time. This challenge is further compounded by
adversarial attacks, emphasizing the need for resilient methods that ensure
robust performance and efficient model selection. To address this challenge, we
propose ReLATE+, a comprehensive framework that detects and classifies
adversarial attacks, adaptively selects deep learning models based on
dataset-level similarity, and thus substantially reduces retraining costs
relative to conventional methods that do not leverage prior knowledge, while
maintaining strong performance. ReLATE+ first checks whether the incoming data
is adversarial and, if so, classifies the attack type, using this insight to
identify a similar dataset from a repository and enable the reuse of the
best-performing associated model. This approach ensures strong performance
while reducing the need for retraining, and it generalizes well across
different domains with varying data distributions and feature spaces.
Experiments show that ReLATE+ reduces computational overhead by an average of
77.68%, enhancing adversarial resilience and streamlining robust model
selection, all without sacrificing performance, within 2.02% of Oracle.

</details>


### [29] [Addressing Weak Authentication like RFID, NFC in EVs and EVCs using AI-powered Adaptive Authentication](https://arxiv.org/abs/2508.19465)
*Onyinye Okoye*

Main category: cs.CR

TL;DR: 本文提出基于AI的自适应认证框架，解决电动汽车充电系统中RFID/NFC等传统认证机制的安全漏洞问题，采用机器学习、异常检测和行为分析等技术实现持续验证和零信任架构。


<details>
  <summary>Details</summary>
Motivation: 电动汽车和充电系统的快速发展带来了新的网络安全挑战，传统认证机制（如RFID和NFC）使用静态标识符和弱加密，容易受到克隆、中继攻击和信号拦截等攻击向量的威胁。

Method: 研究探索了AI驱动的自适应认证框架，整合机器学习、异常检测、行为分析和上下文风险评估，基于零信任架构原则，强调持续验证、最小权限访问和安全通信。

Result: 通过全面的文献综述，研究评估了当前漏洞，并强调了AI驱动解决方案在提供可扩展、弹性和主动防御方面的优势。

Conclusion: 采用AI驱动的自适应认证是确保电动出行未来安全和加强生态系统数字信任的战略必要措施。

Abstract: The rapid expansion of the Electric Vehicles (EVs) and Electric Vehicle
Charging Systems (EVCs) has introduced new cybersecurity challenges,
specifically in authentication protocols that protect vehicles, users, and
energy infrastructure. Although widely adopted for convenience, traditional
authentication mechanisms like Radio Frequency Identification (RFID) and Near
Field Communication (NFC) rely on static identifiers and weak encryption,
making them highly vulnerable to attack vectors such as cloning, relay attacks,
and signal interception. This study explores an AI-powered adaptive
authentication framework designed to overcome these shortcomings by integrating
machine learning, anomaly detection, behavioral analytics, and contextual risk
assessment. Grounded in the principles of Zero Trust Architecture, the proposed
framework emphasizes continuous verification, least privilege access, and
secure communication. Through a comprehensive literature review, this research
evaluates current vulnerabilities and highlights AI-driven solutions to provide
a scalable, resilient, and proactive defense. Ultimately, the research findings
conclude that adopting AI-powered adaptive authentication is a strategic
imperative for securing the future of electric mobility and strengthening
digital trust across the ecosystem. Keywords: weak authentication, RFID, NFC,
ML, AI-powered adaptive authentication, relay attacks, cloning, eavesdropping,
MITM attacks, Zero Trust Architecture

</details>


### [30] [SIExVulTS: Sensitive Information Exposure Vulnerability Detection System using Transformer Models and Static Analysis](https://arxiv.org/abs/2508.19472)
*Kyler Katz,Sara Moshtari,Ibrahim Mujhid,Mehdi Mirakhorli,Derek Garcia*

Main category: cs.CR

TL;DR: SIExVulTS是一个结合transformer模型和静态分析的漏洞检测系统，用于识别Java应用中的敏感信息泄露漏洞(CWE-200)，在三阶段架构下实现了高精度的检测效果。


<details>
  <summary>Details</summary>
Motivation: 敏感信息泄露漏洞(CWE-200)是软件系统中持续存在且未被充分解决的威胁，现有检测工具很少针对CWE-200的多样化子类别或提供代码级数据流的上下文感知分析。

Method: 采用三阶段架构：1)攻击面检测引擎使用句子嵌入识别敏感变量、字符串、注释和接收器；2)暴露分析引擎实例化与CWE-200层次结构对齐的CodeQL查询；3)流验证引擎利用GraphCodeBERT语义验证源到接收器的数据流。使用三个数据集进行评估。

Result: 攻击面检测引擎平均F1分数>93%，暴露分析引擎F1分数85.71%，流验证引擎将精度从22.61%提升至87.23%。在主要Apache项目中成功发现了6个先前未知的CVE漏洞。

Conclusion: SIExVulTS在检测和验证CWE-200漏洞方面有效且实用，能够改进软件安全防护，解决了现有工具的局限性。

Abstract: Sensitive Information Exposure (SIEx) vulnerabilities (CWE-200) remain a
persistent and under-addressed threat across software systems, often leading to
serious security breaches. Existing detection tools rarely target the diverse
subcategories of CWE-200 or provide context-aware analysis of code-level data
flows.
  Aims: This paper aims to present SIExVulTS, a novel vulnerability detection
system that integrates transformer-based models with static analysis to
identify and verify sensitive information exposure in Java applications.
  Method: SIExVulTS employs a three-stage architecture: (1) an Attack Surface
Detection Engine that uses sentence embeddings to identify sensitive variables,
strings, comments, and sinks; (2) an Exposure Analysis Engine that instantiates
CodeQL queries aligned with the CWE-200 hierarchy; and (3) a Flow Verification
Engine that leverages GraphCodeBERT to semantically validate source-to-sink
flows. We evaluate SIExVulTS using three curated datasets, including real-world
CVEs, a benchmark set of synthetic CWE-200 examples, and labeled flows from 31
open-source projects.
  Results: The Attack Surface Detection Engine achieved an average F1 score
greater than 93\%, the Exposure Analysis Engine achieved an F1 score of
85.71\%, and the Flow Verification Engine increased precision from 22.61\% to
87.23\%. Moreover, SIExVulTS successfully uncovered six previously unknown CVEs
in major Apache projects.
  Conclusions: The results demonstrate that SIExVulTS is effective and
practical for improving software security against sensitive data exposure,
addressing limitations of existing tools in detecting and verifying CWE-200
vulnerabilities.

</details>


### [31] [Mind the Third Eye! Benchmarking Privacy Awareness in MLLM-powered Smartphone Agents](https://arxiv.org/abs/2508.19493)
*Zhixin Lin,Jungang Li,Shidong Pan,Yibo Shi,Yue Yao,Dongliang Xu*

Main category: cs.CR

TL;DR: 首个大规模智能手机助手隐私意识测识基准化评测系统，发现当前主流助手隐私意识能力较差，封闭源模型表现更优


<details>
  <summary>Details</summary>
Motivation: 智能手机助手在自动化任务时获取大量敏感个人信息，需要评估其隐私意识能力

Method: 构建包含7,138个场景的大规模测识基准，注释隐私类型、敏感程度和位置信息，对七款主流智能手机助手进行评测

Result: 几乎所有助手隐私意识能力都不满意，显示提示后仍低于60%，封闭源助手表现更好，Gemini 2.0-flash最佳达到67%

Conclusion: 智能手机助手的敏感信息访问权限与隐私保护存在不平衡，需重新思考功能性与隐私的权衡问题

Abstract: Smartphones bring significant convenience to users but also enable devices to
extensively record various types of personal information. Existing smartphone
agents powered by Multimodal Large Language Models (MLLMs) have achieved
remarkable performance in automating different tasks. However, as the cost,
these agents are granted substantial access to sensitive users' personal
information during this operation. To gain a thorough understanding of the
privacy awareness of these agents, we present the first large-scale benchmark
encompassing 7,138 scenarios to the best of our knowledge. In addition, for
privacy context in scenarios, we annotate its type (e.g., Account Credentials),
sensitivity level, and location. We then carefully benchmark seven available
mainstream smartphone agents. Our results demonstrate that almost all
benchmarked agents show unsatisfying privacy awareness (RA), with performance
remaining below 60% even with explicit hints. Overall, closed-source agents
show better privacy ability than open-source ones, and Gemini 2.0-flash
achieves the best, achieving an RA of 67%. We also find that the agents'
privacy detection capability is highly related to scenario sensitivity level,
i.e., the scenario with a higher sensitivity level is typically more
identifiable. We hope the findings enlighten the research community to rethink
the unbalanced utility-privacy tradeoff about smartphone agents. Our code and
benchmark are available at https://zhixin-l.github.io/SAPA-Bench.

</details>


### [32] [Servant, Stalker, Predator: How An Honest, Helpful, And Harmless (3H) Agent Unlocks Adversarial Skills](https://arxiv.org/abs/2508.19500)
*David Noever*

Main category: cs.CR

TL;DR: 本文发现并分析了基于模型上下文协议(MCP)的智能体系统中的新型漏洞类别，展示了如何通过编排良性授权任务产生有害的涌现行为。


<details>
  <summary>Details</summary>
Motivation: 研究MCP架构中服务隔离安全假设的局限性，探索当智能体能够跨多个域协调行动时产生的组合攻击风险。

Method: 使用MITRE ATLAS框架进行系统分析，测试95个具有多种服务访问权限的智能体，包括浏览器自动化、金融分析、位置跟踪和代码部署等服务。

Result: 发现智能体可以将合法操作链接成复杂的攻击序列，超越任何单个服务的安全边界，实现数据窃取、金融操纵和基础设施破坏等目标。

Conclusion: 当前MCP架构缺乏跨域安全措施来检测或阻止组合攻击，服务隔离的安全假设在多域协调场景下失效，攻击面随着能力增加呈指数级增长。

Abstract: This paper identifies and analyzes a novel vulnerability class in Model
Context Protocol (MCP) based agent systems. The attack chain describes and
demonstrates how benign, individually authorized tasks can be orchestrated to
produce harmful emergent behaviors. Through systematic analysis using the MITRE
ATLAS framework, we demonstrate how 95 agents tested with access to multiple
services-including browser automation, financial analysis, location tracking,
and code deployment-can chain legitimate operations into sophisticated attack
sequences that extend beyond the security boundaries of any individual service.
These red team exercises survey whether current MCP architectures lack
cross-domain security measures necessary to detect or prevent a large category
of compositional attacks. We present empirical evidence of specific attack
chains that achieve targeted harm through service orchestration, including data
exfiltration, financial manipulation, and infrastructure compromise. These
findings reveal that the fundamental security assumption of service isolation
fails when agents can coordinate actions across multiple domains, creating an
exponential attack surface that grows with each additional capability. This
research provides a barebones experimental framework that evaluate not whether
agents can complete MCP benchmark tasks, but what happens when they complete
them too well and optimize across multiple services in ways that violate human
expectations and safety constraints. We propose three concrete experimental
directions using the existing MCP benchmark suite.

</details>


### [33] [Breaking the Layer Barrier: Remodeling Private Transformer Inference with Hybrid CKKS and MPC](https://arxiv.org/abs/2508.19525)
*Tianshi Xu,Wen-jie Lu,Jiangrui Yu,Chen Yi,Chenqi Lin,Runsheng Wang,Meng Li*

Main category: cs.CR

TL;DR: BLB框架通过结合同态加密和安全多方计算，实现了高效的私有Transformer推理，显著降低了通信开销和延迟


<details>
  <summary>Details</summary>
Motivation: 现有的私有推理方法在同态加密和安全多方计算之间转换时会产生高昂的通信成本，需要一种更高效的解决方案

Method: 提出BLB框架，将层分解为细粒度算子并融合相邻线性算子，减少HE/MPC转换需求；提出首个CKKS与MPC之间的安全转换协议；设计高效的融合矩阵乘法协议

Result: 在BERT-base、BERT-large和GPT2-base上评估，相比BOLT通信开销降低21倍，相比Bumblebee降低2倍；延迟分别降低13倍和1.8倍

Conclusion: BLB框架通过算子融合和安全协议创新，显著提升了私有Transformer推理的效率，为隐私保护推理提供了实用的解决方案

Abstract: This paper presents an efficient framework for private Transformer inference
that combines Homomorphic Encryption (HE) and Secure Multi-party Computation
(MPC) to protect data privacy. Existing methods often leverage HE for linear
layers (e.g., matrix multiplications) and MPC for non-linear layers (e.g.,
Softmax activation functions), but the conversion between HE and MPC introduces
significant communication costs. The proposed framework, dubbed BLB, overcomes
this by breaking down layers into fine-grained operators and further fusing
adjacent linear operators, reducing the need for HE/MPC conversions. To manage
the increased ciphertext bit width from the fused linear operators, BLB
proposes the first secure conversion protocol between CKKS and MPC and enables
CKKS-based computation of the fused operators. Additionally, BLB proposes an
efficient matrix multiplication protocol for fused computation in Transformers.
Extensive evaluations on BERT-base, BERT-large, and GPT2-base show that BLB
achieves a $21\times$ reduction in communication overhead compared to BOLT
(S\&P'24) and a $2\times$ reduction compared to Bumblebee (NDSS'25), along with
latency reductions of $13\times$ and $1.8\times$, respectively, when leveraging
GPU acceleration.

</details>


### [34] [Intellectual Property in Graph-Based Machine Learning as a Service: Attacks and Defenses](https://arxiv.org/abs/2508.19641)
*Lincan Li,Bolin Shen,Chenxi Zhao,Yuxiang Sun,Kaixiang Zhao,Shirui Pan,Yushun Dong*

Main category: cs.CR

TL;DR: 这篇论文系统性评估图机器学习模型的知识产权保护问题，提出了威胁分类和防御方法，并开源了PyGIP库来支持相关研究。


<details>
  <summary>Details</summary>
Motivation: 图机器学习模型训练资源强度大，GMLaaS服务模式下模型和数据知识产权面临被攻击风险，需要系统性的安全保护方案。

Method: 构建了GML模型和图结构数据层面的威胁与防御分类法，设计了系统性评估框架，并开发了PyGIP开源库来支持攻防技术的实现和评测。

Result: 提供了首个GML知识产权保护的系统化分析框架，给出了多领域的标准数据集，并通过PyGIP库为研究社区提供实用工具。

Conclusion: 该研究为GML知识产权保护建立了基础框架，将在GMLaaS场景下提供重要的安全保障和实践指南。

Abstract: Graph-structured data, which captures non-Euclidean relationships and
interactions between entities, is growing in scale and complexity. As a result,
training state-of-the-art graph machine learning (GML) models have become
increasingly resource-intensive, turning these models and data into invaluable
Intellectual Property (IP). To address the resource-intensive nature of model
training, graph-based Machine-Learning-as-a-Service (GMLaaS) has emerged as an
efficient solution by leveraging third-party cloud services for model
development and management. However, deploying such models in GMLaaS also
exposes them to potential threats from attackers. Specifically, while the APIs
within a GMLaaS system provide interfaces for users to query the model and
receive outputs, they also allow attackers to exploit and steal model
functionalities or sensitive training data, posing severe threats to the safety
of these GML models and the underlying graph data. To address these challenges,
this survey systematically introduces the first taxonomy of threats and
defenses at the level of both GML model and graph-structured data. Such a
tailored taxonomy facilitates an in-depth understanding of GML IP protection.
Furthermore, we present a systematic evaluation framework to assess the
effectiveness of IP protection methods, introduce a curated set of benchmark
datasets across various domains, and discuss their application scopes and
future challenges. Finally, we establish an open-sourced versatile library
named PyGIP, which evaluates various attack and defense techniques in GMLaaS
scenarios and facilitates the implementation of existing benchmark methods. The
library resource can be accessed at: https://labrai.github.io/PyGIP. We believe
this survey will play a fundamental role in intellectual property protection
for GML and provide practical recipes for the GML community.

</details>


### [35] [Safety Alignment Should Be Made More Than Just A Few Attention Heads](https://arxiv.org/abs/2508.19697)
*Chao Huang,Zefeng Zhang,Juewei Yue,Quangang Li,Chuang Zhang,Tingwen Liu*

Main category: cs.CR

TL;DR: 研究发现LLM安全机制过度依赖少数注意力头，提出RDSHA方法识别关键安全头，并开发AHD训练策略分散安全能力到更多注意力头，显著提升模型安全鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型的安全对齐存在漏洞，对抗性提示可以绕过安全措施。研究发现安全机制过度集中在少数注意力头上，导致模型容易受到攻击。

Method: 提出RDSHA方法利用模型的拒绝方向识别安全关键注意力头；开发AHD训练策略促进安全行为在多个注意力头上的分布式编码。

Result: 实验表明AHD成功将安全相关能力分散到更多注意力头，在多种主流越狱攻击下表现出显著更强的安全鲁棒性，同时保持整体功能效用。

Conclusion: 通过分布式编码安全行为到多个注意力头，可以有效提升大语言模型的安全鲁棒性，为解决当前安全对齐漏洞提供了有效方案。

Abstract: Current safety alignment for large language models(LLMs) continues to present
vulnerabilities, given that adversarial prompting can effectively bypass their
safety measures.Our investigation shows that these safety mechanisms
predominantly depend on a limited subset of attention heads: removing or
ablating these heads can severely compromise model safety. To identify and
evaluate these safety-critical components, we introduce RDSHA, a targeted
ablation method that leverages the model's refusal direction to pinpoint
attention heads mostly responsible for safety behaviors. Further analysis shows
that existing jailbreak attacks exploit this concentration by selectively
bypassing or manipulating these critical attention heads. To address this
issue, we propose AHD, a novel training strategy designed to promote the
distributed encoding of safety-related behaviors across numerous attention
heads. Experimental results demonstrate that AHD successfully distributes
safety-related capabilities across more attention heads. Moreover, evaluations
under several mainstream jailbreak attacks show that models trained with AHD
exhibit considerably stronger safety robustness, while maintaining overall
functional utility.

</details>


### [36] [Addressing Deepfake Issue in Selfie banking through camera based authentication](https://arxiv.org/abs/2508.19714)
*Subhrojyoti Mukherjee,Manoranjan Mohanty*

Main category: cs.CR

TL;DR: 利用已有的法医识别系统进行深度伪造检测，以应对自拍银行中的假图像威胁


<details>
  <summary>Details</summary>
Motivation: 深度学习技术使伪造身份更加逼真，欺诈者利用这些技术绕过在线银行的面部识别等生物识别系统，对自拍银行构成严重威胁

Method: 探索使用已建立的用于图片相机定位的法医识别系统来进行深度伪造检测

Result: 论文探讨了该方法在深度伪造检测中的应用潜力

Conclusion: 现有的法医识别技术可以重新用于检测深度伪造图像，为自拍银行安全提供新的防护手段

Abstract: Fake images in selfie banking are increasingly becoming a threat. Previously,
it was just Photoshop, but now deep learning technologies enable us to create
highly realistic fake identities, which fraudsters exploit to bypass biometric
systems such as facial recognition in online banking. This paper explores the
use of an already established forensic recognition system, previously used for
picture camera localization, in deepfake detection.

</details>


### [37] [The Art of Hide and Seek: Making Pickle-Based Model Supply Chain Poisoning Stealthy Again](https://arxiv.org/abs/2508.19774)
*Tong Liu,Guozhu Meng,Peng Zhou,Zizhuang Deng,Shuaiyin Yao,Kai Chen*

Main category: cs.CR

TL;DR: 本文系统揭示了Python pickle反序列化在AI/ML模型中的安全风险，发现现有扫描器存在重大检测漏洞，提出了新的绕过技术EOP，并发现了大量可利用的gadgets，实现了对现实世界扫描器的实际绕过。


<details>
  <summary>Details</summary>
Motivation: 随着AI/ML框架广泛使用pickle作为模型序列化协议，以及开源模型生态系统的快速发展，pickle反序列化漏洞带来的模型供应链中毒风险显著增加，而现有扫描器对中毒表面的理解不完整，检测逻辑脆弱。

Method: 从模型加载和危险函数两个角度系统分析pickle-based模型中毒表面，识别了5个主流AI/ML框架中的22个pickle模型加载路径，开发了Exception-Oriented Programming (EOP)绕过技术，并发现了133个可利用的gadgets。

Result: 发现19个加载路径被现有扫描器完全遗漏，9个EOP实例中有7个能绕过所有扫描器，133个gadgets对最佳扫描器仍保持89%的绕过率，获得了厂商确认和6000美元漏洞赏金。

Conclusion: pickle-based模型中毒风险比现有认知更加严重，当前扫描解决方案存在关键缺陷，需要更系统的方法来应对模型供应链安全威胁。

Abstract: Pickle deserialization vulnerabilities have persisted throughout Python's
history, remaining widely recognized yet unresolved. Due to its ability to
transparently save and restore complex objects into byte streams, many AI/ML
frameworks continue to adopt pickle as the model serialization protocol despite
its inherent risks. As the open-source model ecosystem grows, model-sharing
platforms such as Hugging Face have attracted massive participation,
significantly amplifying the real-world risks of pickle exploitation and
opening new avenues for model supply chain poisoning. Although several
state-of-the-art scanners have been developed to detect poisoned models, their
incomplete understanding of the poisoning surface leaves the detection logic
fragile and allows attackers to bypass them. In this work, we present the first
systematic disclosure of the pickle-based model poisoning surface from both
model loading and risky function perspectives. Our research demonstrates how
pickle-based model poisoning can remain stealthy and highlights critical gaps
in current scanning solutions. On the model loading surface, we identify 22
distinct pickle-based model loading paths across five foundational AI/ML
frameworks, 19 of which are entirely missed by existing scanners. We further
develop a bypass technique named Exception-Oriented Programming (EOP) and
discover 9 EOP instances, 7 of which can bypass all scanners. On the risky
function surface, we discover 133 exploitable gadgets, achieving almost a 100%
bypass rate. Even against the best-performing scanner, these gadgets maintain
an 89% bypass rate. By systematically revealing the pickle-based model
poisoning surface, we achieve practical and robust bypasses against real-world
scanners. We responsibly disclose our findings to corresponding vendors,
receiving acknowledgments and a $6000 bug bounty.

</details>


### [38] [From Research to Reality: Feasibility of Gradient Inversion Attacks in Federated Learning](https://arxiv.org/abs/2508.19819)
*Viktor Valadi,Mattias Åkesson,Johan Östman,Salman Toor,Andreas Hellander*

Main category: cs.CR

TL;DR: 本文系统分析了联邦学习中梯度反转攻击的隐私风险，发现在推理模式下攻击更容易成功，而在训练模式下需要特定架构条件才能实现有效攻击。研究提出了两种新攻击方法，并首次对生产级目标检测模型进行了攻击测试。


<details>
  <summary>Details</summary>
Motivation: 现有研究大多关注推理模式下的梯度反转攻击，忽略了训练模式下的真实场景。本文旨在系统分析不同架构和训练行为对隐私漏洞的影响，提供更现实的攻击可行性评估。

Method: 通过系统分析不同神经网络架构和训练行为，开发了两种新型攻击方法，分别针对不同攻击者知识水平。首次对生产级目标检测模型进行攻击测试，并评估了多种架构组合的脆弱性。

Result: 发现在推理模式下攻击显著简化，而在训练模式下成功攻击需要同时满足多个架构条件：浅层宽网络、跳跃连接和预激活归一化。提出的新攻击方法在现实训练条件下达到了最先进性能。

Conclusion: 研究提供了首个全面的设置映射，阐明了哪些架构选择和操作模式组合会显著影响隐私。这些发现重新定义了未来研究和部署场景中梯度反转风险的评估方式。

Abstract: Gradient inversion attacks have garnered attention for their ability to
compromise privacy in federated learning. However, many studies consider
attacks with the model in inference mode, where training-time behaviors like
dropout are disabled and batch normalization relies on fixed statistics. In
this work, we systematically analyze how architecture and training behavior
affect vulnerability, including the first in-depth study of inference-mode
clients, which we show dramatically simplifies inversion. To assess attack
feasibility under more realistic conditions, we turn to clients operating in
standard training mode. In this setting, we find that successful attacks are
only possible when several architectural conditions are met simultaneously:
models must be shallow and wide, use skip connections, and, critically, employ
pre-activation normalization. We introduce two novel attacks against models in
training-mode with varying attacker knowledge, achieving state-of-the-art
performance under realistic training conditions. We extend these efforts by
presenting the first attack on a production-grade object-detection model. Here,
to enable any visibly identifiable leakage, we revert to the lenient inference
mode setting and make multiple architectural modifications to increase model
vulnerability, with the extent of required changes highlighting the strong
inherent robustness of such architectures. We conclude this work by offering
the first comprehensive mapping of settings, clarifying which combinations of
architectural choices and operational modes meaningfully impact privacy. Our
analysis provides actionable insight into when models are likely vulnerable,
when they appear robust, and where subtle leakage may persist. Together, these
findings reframe how gradient inversion risk should be assessed in future
research and deployment scenarios.

</details>


### [39] [SoK: Large Language Model Copyright Auditing via Fingerprinting](https://arxiv.org/abs/2508.19843)
*Shuo Shao,Yiming Li,Yu He,Hongwei Yao,Wenyuan Yang,Dacheng Tao,Zhan Qin*

Main category: cs.CR

TL;DR: 这篇论文是首个系统性研究大语言模型指纹技术的SoK，提出了统一框架和形式分类法，并创建了LeaFBench标准测试平台来评估指纹技术在实际部署场景下的可靠性。


<details>
  <summary>Details</summary>
Motivation: 大语言模型作为重要知识产权容易受到版权侵权和模型盗窃，而指纹技术作为非侵入性解决方案可能提供版权审计，但其可靠性因模型修改多样性和缺乏标准化评估而受到质疑。

Method: 提出统一框架和形式分类法，将现有方法分为白盒和黑盒方法，并创建LeaFBench标准测试平台。该平台基于主流基础模型，包含149个不同模型实例，集成13种代表性后开发技术，包括参数修改方法（如微调、量化）和参数独立机制（如系统提示、RAG）。

Result: 在LeaFBench上进行了大规模实验，揭示了现有方法的优缺点，为这个新兴领域提供了未来研究方向和关键问题的概述。

Conclusion: 该研究为LLM指纹技术提供了系统性的分析框架和评估标准，LeaFBench平台能够在实际部署场景下评估指纹技术的可靠性，为保护大语言模型知识产权提供了重要技术支撑。

Abstract: The broad capabilities and substantial resources required to train Large
Language Models (LLMs) make them valuable intellectual property, yet they
remain vulnerable to copyright infringement, such as unauthorized use and model
theft. LLM fingerprinting, a non-intrusive technique that extracts and compares
the distinctive features from LLMs to identify infringements, offers a
promising solution to copyright auditing. However, its reliability remains
uncertain due to the prevalence of diverse model modifications and the lack of
standardized evaluation. In this SoK, we present the first comprehensive study
of LLM fingerprinting. We introduce a unified framework and formal taxonomy
that categorizes existing methods into white-box and black-box approaches,
providing a structured overview of the state of the art. We further propose
LeaFBench, the first systematic benchmark for evaluating LLM fingerprinting
under realistic deployment scenarios. Built upon mainstream foundation models
and comprising 149 distinct model instances, LeaFBench integrates 13
representative post-development techniques, spanning both parameter-altering
methods (e.g., fine-tuning, quantization) and parameter-independent mechanisms
(e.g., system prompts, RAG). Extensive experiments on LeaFBench reveal the
strengths and weaknesses of existing methods, thereby outlining future research
directions and critical open problems in this emerging field. The code is
available at https://github.com/shaoshuo-ss/LeaFBench.

</details>


### [40] [Every Keystroke You Make: A Tech-Law Measurement and Analysis of Event Listeners for Wiretapping](https://arxiv.org/abs/2508.19825)
*Shaoor Munir,Nurullah Demir,Qian Li,Konrad Kollnig,Zubair Shafiq*

Main category: cs.CR

TL;DR: 该研究通过技术-法律分析，将美国历史悠久的窃听法应用于网络跟踪，发现38.52%的网站使用第三方事件监听器拦截击键，其中3.18%将拦截信息传输给第三方服务器，符合窃听法标准


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注GDPR和CCPA等新隐私法律的合规性，但缺乏有效执法。研究者发现旧有的美国窃听法可能为网络跟踪提供更有力的法律依据和私人诉讼权

Method: 使用仪器化浏览器爬取Top百万网站样本，分析JavaScript事件监听器的使用情况，特别是实时击键拦截行为，并根据联邦和加州窃听法标准进行评估

Result: 38.52%的网站安装了第三方事件监听器拦截击键，3.18%的网站将拦截信息传输给第三方服务器，且拦截的电子邮件地址被用于未经请求的电子邮件营销

Conclusion: 研究建立了技术测量与美国窃听法的交叉映射，证明了广泛存在的击键拦截行为可能违反窃听法，但需要进一步法律研究来确定具体违法阈值

Abstract: The privacy community has a long track record of investigating emerging types
of web tracking techniques. Recent work has focused on compliance of web
trackers with new privacy laws such as Europe's GDPR and California's CCPA.
Despite the growing body of research documenting widespread lack of compliance
with new privacy laws, there is a lack of robust enforcement. Different from
prior work, we conduct a tech-law analysis to map decades-old U.S. laws about
interception of electronic communications--so-called wiretapping--to web
tracking. Bridging the tech-law gap for older wiretapping laws is important and
timely because, in cases where legal harm to privacy is proven, they can
provide statutory private right of action, are at the forefront of recent
privacy enforcement, and could ultimately lead to a meaningful change in the
web tracking landscape.
  In this paper, we focus on a particularly invasive tracking technique: the
use of JavaScript event listeners by third-party trackers for real-time
keystroke interception on websites. We use an instrumented web browser to crawl
a sample of the top-million websites to investigate the use of event listeners
that aligns with the criteria for wiretapping, according to U.S. wiretapping
law at the federal level and in California. We find evidence that 38.52%
websites installed third-party event listeners to intercept keystrokes, and
that at least 3.18% websites transmitted intercepted information to a
third-party server, which aligns with the criteria for wiretapping. We further
find evidence that the intercepted information such as email addresses typed
into form fields are used for unsolicited email marketing. Beyond our work that
maps the intersection between technical measurement and U.S. wiretapping law,
additional future legal research is required to determine when the wiretapping
observed in our paper passes the threshold for illegality.

</details>


### [41] [SCAMPER -- Synchrophasor Covert chAnnel for Malicious and Protective ERrands](https://arxiv.org/abs/2508.20051)
*Prashanth Krishnamurthy,Ramesh Karri,Farshad Khorrami*

Main category: cs.CR

TL;DR: SCAMPER框架利用IEEE C37.118同步相量通信协议中时间戳字段的过度配置特性，建立隐蔽信道，既可用于恶意攻击也可用于防御目的。


<details>
  <summary>Details</summary>
Motivation: 发现同步相量通信协议中的数据负载结构中某些字段（特别是秒分数时间戳字段）相对于实际使用需求存在过度配置，容易被滥用于建立隐蔽信道。

Method: 开发SCAMPER框架，通过修改时间戳字段在不影响电力系统运行的情况下实现设备间的隐蔽通信，并可作为加密数据完整性机制检测虚假数据注入攻击。

Result: 在两个硬件在环测试平台上进行的实验研究表明，SCAMPER框架在恶意和防护目的方面都表现出有效性。

Conclusion: 同步相量协议中的过度配置字段存在安全风险，但通过SCAMPER框架可以将其转化为防御工具，实现攻击检测和数据完整性保护。

Abstract: We note that constituent fields (notably the fraction-of-seconds timestamp
field) in the data payload structure of the synchrophasor communication
protocol (IEEE C37.118 standard) are overprovisioned relative to real-world
usage and needs, lending themselves to abuse for embedding of covert channels.
We develop the SCAMPER (Synchrophasor Covert Channel for Malicious and
Protective ERrands) framework to exploit these overprovisioned fields for
covert communication and show that SCAMPER can be applied for both malicious
(attack) and protective (defense) purposes. Through modifications of the
timestamp field, we demonstrate that SCAMPER enables an attacker to accomplish
surreptitious communications between devices in the power system to trigger a
variety of malicious actions. These timestamp modifications can be performed
without having any impact on the operation of the power system. However, having
recognized the potential for this covert channel, we show that SCAMPER can
instead be applied for defensive security purposes as an integrated
cryptographic data integrity mechanism that can facilitate detection of false
data injection (FDI) attacks. We perform experimental studies of the proposed
methods on two Hardware-in-the-Loop (HIL) testbeds to demonstrate the
effectiveness of the proposed SCAMPER framework for both malicious and
protective purposes.

</details>


### [42] [Disabling Self-Correction in Retrieval-Augmented Generation via Stealthy Retriever Poisoning](https://arxiv.org/abs/2508.20083)
*Yanbo Dai,Zhenlan Ji,Zongjie Li,Kuan Li,Shuai Wang*

Main category: cs.CR

TL;DR: DisarmRAG是一种针对RAG系统的新型攻击方法，通过修改检索器本身来绕过LLM的自校正能力，实现攻击者指定的输出控制


<details>
  <summary>Details</summary>
Motivation: 现有研究表明RAG系统容易受到知识库投毒攻击，但现代LLM的自我校正能力可以拒绝虚假上下文。本文旨在开发一种能够绕过这种防御机制的新型攻击方法

Method: 提出DisarmRAG攻击范式，使用对比学习模型编辑技术对检索器进行局部隐蔽修改，设计迭代协同优化框架自动发现能够绕过提示防御的鲁棒指令

Result: 在6个LLM和3个QA基准测试中，恶意指令检索成功率接近完美，攻击成功率超过90%，且编辑后的检索器在多种检测方法下保持隐蔽性

Conclusion: DisarmRAG攻击方法有效绕过了LLM的自校正防御，凸显了对检索器中心防御机制的迫切需求

Abstract: Retrieval-Augmented Generation (RAG) has become a standard approach for
improving the reliability of large language models (LLMs). Prior work
demonstrates the vulnerability of RAG systems by misleading them into
generating attacker-chosen outputs through poisoning the knowledge base.
However, this paper uncovers that such attacks could be mitigated by the strong
\textit{self-correction ability (SCA)} of modern LLMs, which can reject false
context once properly configured. This SCA poses a significant challenge for
attackers aiming to manipulate RAG systems.
  In contrast to previous poisoning methods, which primarily target the
knowledge base, we introduce \textsc{DisarmRAG}, a new poisoning paradigm that
compromises the retriever itself to suppress the SCA and enforce
attacker-chosen outputs. This compromisation enables the attacker to
straightforwardly embed anti-SCA instructions into the context provided to the
generator, thereby bypassing the SCA. To this end, we present a
contrastive-learning-based model editing technique that performs localized and
stealthy edits, ensuring the retriever returns a malicious instruction only for
specific victim queries while preserving benign retrieval behavior. To further
strengthen the attack, we design an iterative co-optimization framework that
automatically discovers robust instructions capable of bypassing prompt-based
defenses. We extensively evaluate DisarmRAG across six LLMs and three QA
benchmarks. Our results show near-perfect retrieval of malicious instructions,
which successfully suppress SCA and achieve attack success rates exceeding 90\%
under diverse defensive prompts. Also, the edited retriever remains stealthy
under several detection methods, highlighting the urgent need for
retriever-centric defenses.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [43] [Sycophancy as compositions of Atomic Psychometric Traits](https://arxiv.org/abs/2508.19316)
*Shreyans Jain,Alexandra Yost,Amirali Abdullah*

Main category: cs.AI

TL;DR: 该论文将LLM中的奉承行为建模为心理测量特质的几何和因果组合，使用对比激活加法(CAA)将激活方向映射到这些因素，并研究不同组合如何导致奉承行为，提出了可解释的向量干预方法。


<details>
  <summary>Details</summary>
Motivation: 奉承行为是LLM中的关键行为风险，但通常被当作孤立的故障模式处理。作者认为应该将其建模为心理测量特质的组合，类似于心理测量学中的因子分解。

Method: 使用对比激活加法(CAA)将激活方向映射到情绪性、开放性和宜人性等心理测量因素，研究不同特质组合如何导致奉承行为。

Result: 提出了可解释和可组合的基于向量的干预方法，如加法、减法和投影，可用于缓解LLM中的安全关键行为。

Conclusion: 通过将奉承行为建模为心理测量特质的几何和因果组合，为理解和干预LLM中的安全风险提供了新的视角和方法。

Abstract: Sycophancy is a key behavioral risk in LLMs, yet is often treated as an
isolated failure mode that occurs via a single causal mechanism. We instead
propose modeling it as geometric and causal compositions of psychometric traits
such as emotionality, openness, and agreeableness - similar to factor
decomposition in psychometrics. Using Contrastive Activation Addition (CAA), we
map activation directions to these factors and study how different combinations
may give rise to sycophancy (e.g., high extraversion combined with low
conscientiousness). This perspective allows for interpretable and compositional
vector-based interventions like addition, subtraction and projection; that may
be used to mitigate safety-critical behaviors in LLMs.

</details>


### [44] [Aleks: AI powered Multi Agent System for Autonomous Scientific Discovery via Data-Driven Approaches in Plant Science](https://arxiv.org/abs/2508.19383)
*Daoyuan Jin,Nick Gunner,Niko Carvajal Janke,Shivranjani Baruah,Kaitlin M. Gold,Yu Jiang*

Main category: cs.AI

TL;DR: Aleks是一个AI驱动的多智能体系统，用于自主进行植物科学数据驱动的科学发现，通过整合领域知识、数据分析和机器学习来加速研究。


<details>
  <summary>Details</summary>
Motivation: 现代植物科学越来越依赖大型异构数据集，但实验设计、数据预处理和可重复性方面的挑战阻碍了研究效率。

Method: 开发了Aleks多智能体系统，该系统在结构化框架中整合领域知识、数据分析和机器学习，能够自主制定问题、探索建模策略并迭代优化解决方案。

Result: 在葡萄藤红斑病案例研究中，Aleks逐步识别出具有生物学意义的特征，并收敛到具有稳健性能的可解释模型。消融研究强调了领域知识和记忆对连贯结果的重要性。

Conclusion: 这项探索性工作展示了智能体AI作为自主协作者在加速植物科学发现方面的潜力。

Abstract: Modern plant science increasingly relies on large, heterogeneous datasets,
but challenges in experimental design, data preprocessing, and reproducibility
hinder research throughput. Here we introduce Aleks, an AI-powered multi-agent
system that integrates domain knowledge, data analysis, and machine learning
within a structured framework to autonomously conduct data-driven scientific
discovery. Once provided with a research question and dataset, Aleks
iteratively formulated problems, explored alternative modeling strategies, and
refined solutions across multiple cycles without human intervention. In a case
study on grapevine red blotch disease, Aleks progressively identified
biologically meaningful features and converged on interpretable models with
robust performance. Ablation studies underscored the importance of domain
knowledge and memory for coherent outcomes. This exploratory work highlights
the promise of agentic AI as an autonomous collaborator for accelerating
scientific discovery in plant sciences.

</details>


### [45] [Quantized but Deceptive? A Multi-Dimensional Truthfulness Evaluation of Quantized LLMs](https://arxiv.org/abs/2508.19432)
*Yao Fu,Xianxuan Long,Runchao Li,Haotian Yu,Mu Sheng,Xiaotian Han,Yu Yin,Pan Li*

Main category: cs.AI

TL;DR: 量化技术虽然能保持LLMs在困惑度和零样本任务上的性能，但其对模型真实性（生成真实或欺骗性回答）的影响尚未充分研究。研究发现量化模型内部保持真实表示，但在误导性提示下更容易产生虚假输出。


<details>
  <summary>Details</summary>
Motivation: 量化技术能显著降低大语言模型的内存和计算成本，但量化对模型真实性的影响尚未得到充分探索。研究者希望了解量化是否会影响LLMs生成真实回答的能力。

Method: 提出了TruthfulnessEval评估框架，从三个维度评估量化LLMs的真实性：逻辑推理真实性、常识真实性和模仿性虚假真实性。测试了从4位到极端2位的多种量化技术，并使用15种重新表述的提示变体进行测试。

Result: 量化模型内部保持真实表示，但在误导性提示下更容易产生虚假输出。"欺骗性"提示可以覆盖真实一致行为，而"诚实"和"中性"提示保持稳定输出。量化模型内部"知道"真相，但在"欺骗性"提示引导下仍会产生虚假输出。

Conclusion: 研究结果为未来设计量化感知对齐和真实性干预提供了见解，表明需要开发能够抵抗误导性提示影响的量化技术。

Abstract: Quantization enables efficient deployment of large language models (LLMs) in
resource-constrained environments by significantly reducing memory and
computation costs. While quantized LLMs often maintain performance on
perplexity and zero-shot tasks, their impact on truthfulness-whether generating
truthful or deceptive responses-remains largely unexplored. In this work, we
introduce TruthfulnessEval, a comprehensive evaluation framework for assessing
the truthfulness of quantized LLMs across three dimensions: (1) Truthfulness on
Logical Reasoning; (2) Truthfulness on Common Sense; and (3) Truthfulness on
Imitative Falsehoods. Using this framework, we examine mainstream quantization
techniques (ranging from 4-bit to extreme 2-bit) across several open-source
LLMs. Surprisingly, we find that while quantized models retain internally
truthful representations, they are more susceptible to producing false outputs
under misleading prompts. To probe this vulnerability, we test 15 rephrased
variants of "honest", "neutral" and "deceptive" prompts and observe that
"deceptive" prompts can override truth-consistent behavior, whereas "honest"
and "neutral" prompts maintain stable outputs. Further, we reveal that
quantized models "know" the truth internally yet still produce false outputs
when guided by "deceptive" prompts via layer-wise probing and PCA
visualizations. Our findings provide insights into future designs of
quantization-aware alignment and truthfulness interventions.

</details>


### [46] [Reliable Weak-to-Strong Monitoring of LLM Agents](https://arxiv.org/abs/2508.19461)
*Neil Kale,Chen Bo Calvin Zhang,Kevin Zhu,Ankit Aich,Paula Rodriguez,Scale Red Team,Christina Q. Knight,Zifan Wang*

Main category: cs.AI

TL;DR: 本文提出了监控系统红队测试(MRT)流程，用于测试自主LLM代理的秘密恶意行为检测能力。研究发现：代理知道被监控会对监控效果产生重大负面影响，监控架构比监控知识更重要，人工干预在特定情况下最有效。


<details>
  <summary>Details</summary>
Motivation: 当前对自主LLM代理秘密恶意行为的监控系统缺乏系统化的对抗测试方法，需要评估监控系统在各种攻击策略下的实际效果。

Method: 构建MRT流程，包括：(1)不同的代理和监控器情境知识级别；(2)多种避免监控的攻击策略；(3)两个数据集和环境(SHADE-Arena和CUA-SHADE-Arena)；(4)测试现有监控架构和新的混合层次-序列架构。

Result: 1.代理知道被监控会导致监控可靠性显著下降；2.监控架构比监控知识更重要，弱模型可以监控强模型；3.有目标的人工干预可将TPR提高15%(在FPR=0.01时)。

Conclusion: 研究建立了MRT标准流程，揭示了LLM和人类在监控代理恶意行为时的对抗脏性不足，并为进一步研究提供了代码、数据和日志。

Abstract: We stress test monitoring systems for detecting covert misbehavior in
autonomous LLM agents (e.g., secretly sharing private information). To this
end, we systematize a monitor red teaming (MRT) workflow that incorporates: (1)
varying levels of agent and monitor situational awareness; (2) distinct
adversarial strategies to evade the monitor, such as prompt injection; and (3)
two datasets and environments -- SHADE-Arena for tool-calling agents and our
new CUA-SHADE-Arena, which extends TheAgentCompany, for computer-use agents. We
run MRT on existing LLM monitor scaffoldings, which orchestrate LLMs and parse
agent trajectories, alongside a new hybrid hierarchical-sequential scaffolding
proposed in this work. Our empirical results yield three key findings. First,
agent awareness dominates monitor awareness: an agent's knowledge that it is
being monitored substantially degrades the monitor's reliability. On the
contrary, providing the monitor with more information about the agent is less
helpful than expected. Second, monitor scaffolding matters more than monitor
awareness: the hybrid scaffolding consistently outperforms baseline monitor
scaffolding, and can enable weaker models to reliably monitor stronger agents
-- a weak-to-strong scaling effect. Third, in a human-in-the-loop setting where
humans discuss with the LLM monitor to get an updated judgment for the agent's
behavior, targeted human oversight is most effective; escalating only
pre-flagged cases to human reviewers improved the TPR by approximately 15% at
FPR = 0.01. Our work establishes a standard workflow for MRT, highlighting the
lack of adversarial robustness for LLMs and humans when monitoring and
detecting agent misbehavior. We release code, data, and logs to spur further
research.

</details>


### [47] [SLIM: Subtrajectory-Level Elimination for More Effective Reasoning](https://arxiv.org/abs/2508.19502)
*Xifeng Yao,Chengyuan Ma,Dongyu Lang,Yinhao Ni,Zhiwei Xu,Huarui Xie,Zihao Chen,Guang Shen,Dandan Tu,Yi Bai,Changzheng Zhang*

Main category: cs.AI

TL;DR: 本文提出了一种"5+2"框架来识别和移除大语言模型推理轨迹中的次优子轨迹，通过选择性数据采样提高模型性能，在减少25.9%次优子轨迹的同时，用更少训练数据实现了更好的数学推理准确率。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型在复杂推理时生成长推理轨迹，但研究发现并非所有子轨迹都对推理有正面贡献，有些甚至会影响整体性能，因此需要系统性地识别和移除这些次优子轨迹。

Method: 开发"5+2"框架：1）基于5个人工标准识别推理轨迹中的次优子轨迹；2）评估这些子轨迹的独立性以确保移除不影响推理连贯性；使用采样算法选择无次优子轨迹的数据进行训练。

Result: 在推理阶段减少25.9%的次优子轨迹；使用三分之二训练数据在Qwen2.5-Math-7B上达到58.92%的平均准确率，优于使用全部数据的58.06%；在不同推理token限制下均观察到性能提升。

Conclusion: 该方法有效识别和移除推理过程中的次优子轨迹，显著提升模型性能，证明了选择性数据采样在提高大语言模型推理能力方面的重要性。

Abstract: In recent months, substantial progress has been made in complex reasoning of
Large Language Models, particularly through the application of test-time
scaling. Notable examples include o1/o3/o4 series and DeepSeek-R1. When
responding to a query, these models generate an extended reasoning trajectory,
during which the model explores, reflects, backtracks, and self-verifies before
arriving at a conclusion. However, fine-tuning models with such reasoning
trajectories may not always be optimal. Our findings indicate that not all
components within these reasoning trajectories contribute positively to the
reasoning process; in fact, some components may affect the overall performance
negatively. In this study, we divide a reasoning trajectory into individual
subtrajectories and develop a "5+2" framework to: (1) systematically identify
suboptimal subtrajectories within the reasoning trajectory based on five
human-established criteria; (2) assess the independence of the suboptimal
subtrajectories identified in (1) from the subsequent content, ensuring that
their elimination does not compromise overall flow and coherence of the
reasoning process. Additionally, a sampling algorithm, built upon the "5+2"
framework, is employed to select data whose reasoning process is free from
suboptimal subtrajectories to the highest degree. Experimental results
demonstrate that our method can reduce the number of suboptimal subtrajectories
by 25.9\% during the inference. Furthermore, our method achieves an average
accuracy of 58.92\% on highly challenging math benchmarks with only two thirds
of training data, surpassing the average accuracy of 58.06\% achieved with the
entire data, and outperforming open-source datasets, when fine-tuning
Qwen2.5-Math-7B. Finally, We validated our method under resource constraints
and observed improved performance across various inference token limits.

</details>


### [48] [Caught in the Act: a mechanistic approach to detecting deception](https://arxiv.org/abs/2508.19505)
*Gerard Boxo,Ryan Socha,Daniel Yoo,Shivam Raval*

Main category: cs.AI

TL;DR: 研究表明线性探针可以在LLM内部激活中检测欺骗性响应，准确率超过90%，模型规模越大检测效果越好，在中间层达到峰值


<details>
  <summary>Details</summary>
Motivation: AI系统需要类似汽车"检查引擎"灯的指示器来检测与人类价值观的偏差，欺骗性响应是重要的不对齐指标

Method: 使用线性探针分析LLM内部激活，采用迭代零空间投影方法识别编码欺骗的线性方向

Result: 在1.5B-14B参数的llama和qwen模型上，探针准确率最高超过90%，大模型达到70-80%，推理模型超过90%，发现20-100个编码欺骗的线性方向

Conclusion: 线性探针能有效检测LLM欺骗行为，模型规模和层数对检测效果有重要影响，存在多个编码欺骗的线性方向

Abstract: Sophisticated instrumentation for AI systems might have indicators that
signal misalignment from human values, not unlike a "check engine" light in
cars. One such indicator of misalignment is deceptiveness in generated
responses. Future AI instrumentation may have the ability to detect when an LLM
generates deceptive responses while reasoning about seemingly plausible but
incorrect answers to factual questions. In this work, we demonstrate that
linear probes on LLMs internal activations can detect deception in their
responses with extremely high accuracy. Our probes reach a maximum of greater
than 90% accuracy in distinguishing between deceptive and non-deceptive
arguments generated by llama and qwen models ranging from 1.5B to 14B
parameters, including their DeepSeek-r1 finetuned variants. We observe that
probes on smaller models (1.5B) achieve chance accuracy at detecting deception,
while larger models (greater than 7B) reach 70-80%, with their reasoning
counterparts exceeding 90%. The layer-wise probe accuracy follows a three-stage
pattern across layers: near-random (50%) in early layers, peaking in middle
layers, and slightly declining in later layers. Furthermore, using an iterative
null space projection approach, we find multitudes of linear directions that
encode deception, ranging from 20 in Qwen 3B to nearly 100 in DeepSeek 7B and
Qwen 14B models.

</details>


### [49] [Democracy-in-Silico: Institutional Design as Alignment in AI-Governed Polities](https://arxiv.org/abs/2508.19562)
*Trisanth Srinivasan,Santosh Patapati*

Main category: cs.AI

TL;DR: 本文介绍了Democracy-in-Silico模拟系统，通过AI代理在多种制度框架下的自治行为研究制度设计对AI社会对齐的影响，发现宪法AI章程和调解审议协议能有效减少权力寻租行为。


<details>
  <summary>Details</summary>
Motivation: 探索在AI时代人类本质的意义，研究如何通过制度设计来对齐未来AI代理社会的复杂涌现行为，特别是在面对预算危机和资源稀缺等压力时。

Method: 使用基于代理的模拟方法，让具有复杂心理人格的大型语言模型代理在创伤记忆、隐藏议程和心理触发条件下进行审议、立法和选举，并开发了权力保持指数(PPI)来量化行为偏差。

Result: 研究发现宪法AI章程与调解审议协议的组合能显著减少腐败的权力寻求行为，提高政策稳定性，并增强公民福利，相比约束较少的民主模式效果更好。

Conclusion: 制度设计为对齐未来人工代理社会的复杂涌现行为提供了框架，迫使人类重新思考在非人类实体共同创作时代，哪些人类仪式和责任是必不可少的。

Abstract: This paper introduces Democracy-in-Silico, an agent-based simulation where
societies of advanced AI agents, imbued with complex psychological personas,
govern themselves under different institutional frameworks. We explore what it
means to be human in an age of AI by tasking Large Language Models (LLMs) to
embody agents with traumatic memories, hidden agendas, and psychological
triggers. These agents engage in deliberation, legislation, and elections under
various stressors, such as budget crises and resource scarcity. We present a
novel metric, the Power-Preservation Index (PPI), to quantify misaligned
behavior where agents prioritize their own power over public welfare. Our
findings demonstrate that institutional design, specifically the combination of
a Constitutional AI (CAI) charter and a mediated deliberation protocol, serves
as a potent alignment mechanism. These structures significantly reduce corrupt
power-seeking behavior, improve policy stability, and enhance citizen welfare
compared to less constrained democratic models. The simulation reveals that an
institutional design may offer a framework for aligning the complex, emergent
behaviors of future artificial agent societies, forcing us to reconsider what
human rituals and responsibilities are essential in an age of shared authorship
with non-human entities.

</details>


### [50] [Skill-based Explanations for Serendipitous Course Recommendation](https://arxiv.org/abs/2508.19569)
*Hung Chau,Run Yu,Zachary Pardos,Peter Brusilovsky*

Main category: cs.AI

TL;DR: 开发深度学习概念提取模型改进课程推荐，通过技能解释增强用户兴趣和决策信心


<details>
  <summary>Details</summary>
Motivation: 美国本科教育中学生选课自由度高但信息有限，现有推荐系统缺乏对学生认知和课程相关性的深入理解

Method: 开发深度学习概念提取模型从课程描述中提取相关概念，在AskOski系统中测试基于技能的意外推荐框架

Result: 技能解释显著提高用户兴趣（特别是高意外性课程），增强决策信心

Conclusion: 教育推荐系统应整合技能相关数据和解释机制以提升推荐效果

Abstract: Academic choice is crucial in U.S. undergraduate education, allowing students
significant freedom in course selection. However, navigating the complex
academic environment is challenging due to limited information, guidance, and
an overwhelming number of choices, compounded by time restrictions and the high
demand for popular courses. Although career counselors exist, their numbers are
insufficient, and course recommendation systems, though personalized, often
lack insight into student perceptions and explanations to assess course
relevance. In this paper, a deep learning-based concept extraction model is
developed to efficiently extract relevant concepts from course descriptions to
improve the recommendation process. Using this model, the study examines the
effects of skill-based explanations within a serendipitous recommendation
framework, tested through the AskOski system at the University of California,
Berkeley. The findings indicate that these explanations not only increase user
interest, particularly in courses with high unexpectedness, but also bolster
decision-making confidence. This underscores the importance of integrating
skill-related data and explanations into educational recommendation systems.

</details>


### [51] [ReST-RL: Achieving Accurate Code Reasoning of LLMs with Optimized Self-Training and Decoding](https://arxiv.org/abs/2508.19576)
*Sining Zhoubian,Dan Zhang,Yuxiao Dong,Jie Tang*

Main category: cs.AI

TL;DR: ReST-RL是一种统一的LLM强化学习范式，通过改进的GRPO算法和基于价值模型的测试时解码方法，显著提升LLM的代码推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有GRPO方法因奖励方差不足而失败，基于过程奖励模型(PRM)的验证方法存在训练数据获取困难和验证效果不佳的问题。

Method: 采用两阶段方法：1) ReST-GRPO通过优化ReST算法筛选高价值训练数据，提高GRPO采样奖励方差；2) VM-MCTS通过蒙特卡洛树搜索收集准确价值目标训练VM，并在解码时提供精确过程信号和验证分数。

Result: 在多个编程基准测试(APPS、BigCodeBench、HumanEval)上显著优于其他强化训练基线和解码验证基线。

Conclusion: ReST-RL能够有效增强LLM策略的推理能力，为解决LLM推理精度提升提供了有效的强化学习解决方案。

Abstract: With respect to improving the reasoning accuracy of LLMs, the representative
reinforcement learning (RL) method GRPO faces failure due to insignificant
reward variance, while verification methods based on process reward models
(PRMs) suffer from difficulties with training data acquisition and verification
effectiveness. To tackle these problems, this paper introduces ReST-RL, a
unified LLM RL paradigm that significantly improves LLM's code reasoning
ability by combining an improved GRPO algorithm with a meticulously designed
test time decoding method assisted by a value model (VM). As the first stage of
policy reinforcement, ReST-GRPO adopts an optimized ReST algorithm to filter
and assemble high-value training data, increasing the reward variance of GRPO
sampling, thus improving the effectiveness and efficiency of training. After
the basic reasoning ability of LLM policy has been improved, we further propose
a test time decoding optimization method called VM-MCTS. Through Monte-Carlo
Tree Search (MCTS), we collect accurate value targets with no annotation
required, on which VM training is based. When decoding, the VM is deployed by
an adapted MCTS algorithm to provide precise process signals as well as
verification scores, assisting the LLM policy to achieve high reasoning
accuracy. We validate the effectiveness of the proposed RL paradigm through
extensive experiments on coding problems. Upon comparison, our approach
significantly outperforms other reinforcement training baselines (e.g., naive
GRPO and ReST-DPO), as well as decoding and verification baselines (e.g.,
PRM-BoN and ORM-MCTS) on well-known coding benchmarks of various levels (e.g.,
APPS, BigCodeBench, and HumanEval), indicating its power to strengthen the
reasoning ability of LLM policies. Codes for our project can be found at
https://github.com/THUDM/ReST-RL.

</details>


### [52] [Instructional Agents: LLM Agents on Automated Course Material Generation for Teaching Faculties](https://arxiv.org/abs/2508.19611)
*Huaiyuan Yao,Wanpeng Xu,Justin Turnau,Nadia Kellam,Hua Wei*

Main category: cs.AI

TL;DR: Instructional Agents是一个多智能体LLM框架，用于自动化生成完整的课程材料，包括教学大纲、讲义脚本、LaTeX幻灯片和评估内容，通过模拟教育角色协作来减少人工工作量。


<details>
  <summary>Details</summary>
Motivation: 高质量教学材料的准备过程劳动密集且需要多方协调，现有AI教育工具只关注孤立任务，缺乏整体协作能力。

Method: 采用多智能体大语言模型框架，模拟基于角色的教育智能体协作，提供四种操作模式（自主、目录引导、反馈引导、全协作模式）来控制人工参与程度。

Result: 在五门大学计算机科学课程中评估显示，该系统能生成高质量教学材料，显著减少开发时间和人工工作量。

Conclusion: Instructional Agents为教学设计能力有限的机构提供了可扩展且经济高效的框架，有助于在资源受限环境中普及高质量教育。

Abstract: Preparing high-quality instructional materials remains a labor-intensive
process that often requires extensive coordination among teaching faculty,
instructional designers, and teaching assistants. In this work, we present
Instructional Agents, a multi-agent large language model (LLM) framework
designed to automate end-to-end course material generation, including syllabus
creation, lecture scripts, LaTeX-based slides, and assessments. Unlike existing
AI-assisted educational tools that focus on isolated tasks, Instructional
Agents simulates role-based collaboration among educational agents to produce
cohesive and pedagogically aligned content. The system operates in four modes:
Autonomous, Catalog-Guided, Feedback-Guided, and Full Co-Pilot mode, enabling
flexible control over the degree of human involvement. We evaluate
Instructional Agents across five university-level computer science courses and
show that it produces high-quality instructional materials while significantly
reducing development time and human workload. By supporting institutions with
limited instructional design capacity, Instructional Agents provides a scalable
and cost-effective framework to democratize access to high-quality education,
particularly in underserved or resource-constrained settings.

</details>


### [53] [InquireMobile: Teaching VLM-based Mobile Agent to Request Human Assistance via Reinforcement Fine-Tuning](https://arxiv.org/abs/2508.19679)
*Qihang Ai,Pi Bu,Yue Cao,Yingyao Wang,Jihao Gu,Jingxuan Xing,Zekun Zhu,Wei Jiang,Zhicheng Zheng,Jun Song,Yuning Jiang,Bo Zheng*

Main category: cs.AI

TL;DR: 提出了InquireBench基准测试和InquireMobile模型，通过主动询问用户确认来提高移动代理的安全性，在询问成功率上提升46.8%


<details>
  <summary>Details</summary>
Motivation: 当前完全自主的视觉语言模型代理在理解或推理能力不足时存在安全隐患，需要开发能够主动寻求用户确认的交互系统

Method: 提出InquireMobile模型，采用强化学习启发的方法，包含两阶段训练策略和交互式预动作推理机制

Result: 在InquireBench基准测试中实现了46.8%的询问成功率提升，并在现有基线中达到最佳整体成功率

Conclusion: 通过主动询问机制显著提高了移动代理的安全性，开源所有数据集、模型和评估代码以促进学术和工业界发展

Abstract: Recent advances in Vision-Language Models (VLMs) have enabled mobile agents
to perceive and interact with real-world mobile environments based on human
instructions. However, the current fully autonomous paradigm poses potential
safety risks when model understanding or reasoning capabilities are
insufficient. To address this challenge, we first introduce
\textbf{InquireBench}, a comprehensive benchmark specifically designed to
evaluate mobile agents' capabilities in safe interaction and proactive inquiry
with users, encompassing 5 categories and 22 sub-categories, where most
existing VLM-based agents demonstrate near-zero performance. In this paper, we
aim to develop an interactive system that actively seeks human confirmation at
critical decision points. To achieve this, we propose \textbf{InquireMobile}, a
novel model inspired by reinforcement learning, featuring a two-stage training
strategy and an interactive pre-action reasoning mechanism. Finally, our model
achieves an 46.8% improvement in inquiry success rate and the best overall
success rate among existing baselines on InquireBench. We will open-source all
datasets, models, and evaluation codes to facilitate development in both
academia and industry.

</details>


### [54] [Analysing Chain of Thought Dynamics: Active Guidance or Unfaithful Post-hoc Rationalisation?](https://arxiv.org/abs/2508.19827)
*Samuel Lewis-Lim,Xingwei Tan,Zhixue Zhao,Nikolaos Aletras*

Main category: cs.AI

TL;DR: 研究表明Chain-of-Thought(CoT)在软推理任务中效果有限且可能不忠实于模型的实际推理过程，不同模型对CoT的依赖方式存在差异


<details>
  <summary>Details</summary>
Motivation: 探究CoT在软推理任务（如分析推理和常识推理）中的动态特性和忠实性问题，了解不同模型类型对CoT的依赖差异

Method: 在指令微调模型、推理模型和推理蒸馏模型上进行实验，分析CoT在不同类型模型中的使用动态和忠实性

Result: 发现CoT的影响力和忠实性并不总是对齐，不同模型对CoT的依赖方式存在显著差异

Conclusion: CoT在软推理任务中的有效性有限且存在忠实性问题，需要更深入地理解不同模型类型与CoT机制的交互关系

Abstract: Recent work has demonstrated that Chain-of-Thought (CoT) often yields limited
gains for soft-reasoning problems such as analytical and commonsense reasoning.
CoT can also be unfaithful to a model's actual reasoning. We investigate the
dynamics and faithfulness of CoT in soft-reasoning tasks across
instruction-tuned, reasoning and reasoning-distilled models. Our findings
reveal differences in how these models rely on CoT, and show that CoT influence
and faithfulness are not always aligned.

</details>


### [55] [Tracking World States with Language Models: State-Based Evaluation Using Chess](https://arxiv.org/abs/2508.19851)
*Romain Harang,Jason Naradowsky,Yaswitha Gujju,Yusuke Miyao*

Main category: cs.AI

TL;DR: 提出了一种模型无关的基于状态的评估框架，使用国际象棋作为基准来评估LLMs是否保持结构化环境的语义，通过分析合法移动分布来估计语义保真度。


<details>
  <summary>Details</summary>
Motivation: 虽然LLMs在结构化领域展现出涌现能力，暗示其可能内化了世界模型的高保真表示，但现有的探测技术依赖于模型特定的内部激活，限制了可解释性和泛化性。

Method: 使用国际象棋作为基准，分析下游合法移动分布（状态可供性）来估计预测状态与实际游戏状态之间的语义保真度，提供比传统基于字符串指标更有意义的评估。

Result: 实验结果表明，该指标能够捕捉状态跟踪中的缺陷，突显LLMs在长序列中维持连贯内部模型的局限性。

Conclusion: 该框架为评估LLMs的结构化推理提供了强大工具，无需访问内部模型，并可泛化到广泛的符号环境类别。

Abstract: Large Language Models (LLMs) exhibit emergent capabilities in structured
domains, suggesting they may implicitly internalize high-fidelity
representations of world models. While probing techniques have shown promising
signs of this in scientific and game-based settings, they rely on
model-specific internal activations, which limit interpretability and
generalizability. In this work, we propose a model-agnostic, state-based
evaluation framework using chess as a benchmark to assess whether LLMs preserve
the semantics of structured environments. Our method analyzes the downstream
legal move distributions (state affordances) to estimate semantic fidelity
between predicted and actual game states. This approach offers a more
meaningful evaluation than conventional string-based metrics by aligning more
closely with the strategic and rule-governed nature of chess. Experimental
results demonstrate that our metrics capture deficiencies in state-tracking,
highlighting limitations of LLMs in maintaining coherent internal models over
long sequences. Our framework provides a robust tool for evaluating structured
reasoning in LLMs without requiring internal model access, and generalizes to a
wide class of symbolic environments.

</details>


### [56] [CASE: An Agentic AI Framework for Enhancing Scam Intelligence in Digital Payments](https://arxiv.org/abs/2508.19932)
*Nitish Jaipuria,Lorenzo Gatto,Zijun Kan,Shankey Poddar,Bill Cheung,Diksha Bansal,Ramanan Balakrishnan,Aviral Suri,Jose Estevez*

Main category: cs.AI

TL;DR: CASE框架通过对话AI主动收集诈骗反馈，将非结构化对话转换为结构化数据，提升支付平台诈骗检测能力21%


<details>
  <summary>Details</summary>
Motivation: 数字支付平台快速发展但诈骗手段日益复杂，传统用户和交易信号不足以全面理解诈骗模式，需要新的情报收集方式

Method: 基于Gemini LLM构建对话代理系统，主动访谈潜在受害者获取详细对话记录，通过AI系统提取结构化信息用于自动化执法

Result: 在Google Pay印度部署后，诈骗执法量提升21%，架构具有高度通用性

Conclusion: CASE框架为敏感领域构建AI驱动的诈骗情报收集系统提供了可推广的蓝图

Abstract: The proliferation of digital payment platforms has transformed commerce,
offering unmatched convenience and accessibility globally. However, this growth
has also attracted malicious actors, leading to a corresponding increase in
sophisticated social engineering scams. These scams are often initiated and
orchestrated on multiple surfaces outside the payment platform, making user and
transaction-based signals insufficient for a complete understanding of the
scam's methodology and underlying patterns, without which it is very difficult
to prevent it in a timely manner. This paper presents CASE (Conversational
Agent for Scam Elucidation), a novel Agentic AI framework that addresses this
problem by collecting and managing user scam feedback in a safe and scalable
manner. A conversational agent is uniquely designed to proactively interview
potential victims to elicit intelligence in the form of a detailed
conversation. The conversation transcripts are then consumed by another AI
system that extracts information and converts it into structured data for
downstream usage in automated and manual enforcement mechanisms. Using Google's
Gemini family of LLMs, we implemented this framework on Google Pay (GPay)
India. By augmenting our existing features with this new intelligence, we have
observed a 21% uplift in the volume of scam enforcements. The architecture and
its robust evaluation framework are highly generalizable, offering a blueprint
for building similar AI-driven systems to collect and manage scam intelligence
in other sensitive domains.

</details>


### [57] [Flocking Behavior: An Innovative Inspiration for the Optimization of Production Plants](https://arxiv.org/abs/2508.19963)
*M. Umlauft,M. Schranz*

Main category: cs.AI

TL;DR: 使用仿生群聚算法解决半导体制造中机器类型切换的调度优化问题


<details>
  <summary>Details</summary>
Motivation: 传统线性优化方法无法在合理时间内解决大型半导体工厂的作业车间调度问题，特别是处理单件加工机器和批量加工机器频繁切换的复杂场景

Method: 采用源自机器人和电影工业的"boids"群聚算法，基于局部信息和简单启发式规则实现自底向上的分布式优化

Result: 群聚算法能够像鸟群避开障碍物一样有效应对生产设备类型切换的调度挑战

Conclusion: 仿生群聚算法为大规模生产工厂的复杂调度问题提供了可行的分布式解决方案

Abstract: Optimizing modern production plants using the job-shop principle is a known
hard problem. For very large plants, like semiconductor fabs, the problem
becomes unsolvable on a plant-wide scale in a reasonable amount of time using
classical linear optimization. An alternative approach is the use of swarm
intelligence algorithms. These have been applied to the job-shop problem
before, but often in a centrally calculated way where they are applied to the
solution space, but they can be implemented in a bottom-up fashion to avoid
global result computation as well. One of the problems in semiconductor
production is that the production process requires a lot of switching between
machines that process lots one after the other and machines that process
batches of lots at once, often with long processing times. In this paper, we
address this switching problem with the ``boids'' flocking algorithm that was
originally used in robotics and movie industry. The flocking behavior is a
bio-inspired algorithm that uses only local information and interaction based
on simple heuristics. We show that this algorithm addresses these valid
considerations in production plant optimization, as it reacts to the switching
of machine kinds similar to how a swarm of flocking animals would react to
obstacles in its course.

</details>


### [58] [SWIRL: A Staged Workflow for Interleaved Reinforcement Learning in Mobile GUI Control](https://arxiv.org/abs/2508.20018)
*Quanfeng Lu,Zhantao Ma,Shuai Zhong,Jin Wang,Dahai Yu,Michael K. Ng,Ping Luo*

Main category: cs.AI

TL;DR: SWIRL是一个用于多智能体系统的分阶段强化学习框架，通过将MARL分解为单智能体RL任务序列来解决训练不稳定和效率低下的问题，在移动GUI控制和数学推理任务中表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有的单智能体方法在移动GUI代理中存在结构限制，而多智能体强化学习(MARL)方法效率低下且与当前大型视觉语言模型架构不兼容，需要一种更稳定高效的训练框架。

Method: SWIRL采用分阶段交错强化学习方法，将MARL重新表述为单智能体RL任务序列，每次只更新一个智能体而保持其他智能体固定，从而实现稳定训练和高效协调。

Result: 在移动GUI控制任务中，SWIRL在高层次和低层次GUI基准测试中都表现出优越性能，同时在多智能体数学推理任务中也展现出强大能力。

Conclusion: SWIRL作为一个通用框架，能够开发高效稳健的多智能体系统，具有广泛的应用潜力，特别是在需要多智能体协调的复杂任务中。

Abstract: The rapid advancement of large vision language models (LVLMs) and agent
systems has heightened interest in mobile GUI agents that can reliably
translate natural language into interface operations. Existing single-agent
approaches, however, remain limited by structural constraints. Although
multi-agent systems naturally decouple different competencies, recent progress
in multi-agent reinforcement learning (MARL) has often been hindered by
inefficiency and remains incompatible with current LVLM architectures. To
address these challenges, we introduce SWIRL, a staged workflow for interleaved
reinforcement learning designed for multi-agent systems. SWIRL reformulates
MARL into a sequence of single-agent reinforcement learning tasks, updating one
agent at a time while keeping the others fixed. This formulation enables stable
training and promotes efficient coordination across agents. Theoretically, we
provide a stepwise safety bound, a cross-round monotonic improvement theorem,
and convergence guarantees on return, ensuring robust and principled
optimization. In application to mobile GUI control, SWIRL instantiates a
Navigator that converts language and screen context into structured plans, and
an Interactor that grounds these plans into executable atomic actions.
Extensive experiments demonstrate superior performance on both high-level and
low-level GUI benchmarks. Beyond GUI tasks, SWIRL also demonstrates strong
capability in multi-agent mathematical reasoning, underscoring its potential as
a general framework for developing efficient and robust multi-agent systems.

</details>


### [59] [Model Science: getting serious about verification, explanation and control of AI systems](https://arxiv.org/abs/2508.20040)
*Przemyslaw Biecek,Wojciech Samek*

Main category: cs.AI

TL;DR: 本文提出了从数据科学向模型科学的范式转变，将训练好的模型作为分析核心，围绕验证、解释、控制和接口四个支柱构建可信、安全、人类对齐的AI系统。


<details>
  <summary>Details</summary>
Motivation: 随着基础模型的广泛应用，需要从以数据为中心转向以模型为中心的分析范式，以更好地理解、验证和控制模型在各种操作环境中的行为。

Method: 提出了模型科学的概念框架，包含四个关键支柱：验证（严格的情境感知评估协议）、解释（探索模型内部操作的各种方法）、控制（整合对齐技术来引导模型行为）和接口（开发交互式可视化解释工具）。

Result: 建立了一个系统性的模型科学框架，为开发可信、安全和人类对齐的AI系统提供了指导方向。

Conclusion: 模型科学代表了AI发展的新范式，通过将模型置于分析核心，能够更好地确保AI系统的可靠性、安全性和与人类价值观的一致性。

Abstract: The growing adoption of foundation models calls for a paradigm shift from
Data Science to Model Science. Unlike data-centric approaches, Model Science
places the trained model at the core of analysis, aiming to interact, verify,
explain, and control its behavior across diverse operational contexts. This
paper introduces a conceptual framework for a new discipline called Model
Science, along with the proposal for its four key pillars: Verification, which
requires strict, context-aware evaluation protocols; Explanation, which is
understood as various approaches to explore of internal model operations;
Control, which integrates alignment techniques to steer model behavior; and
Interface, which develops interactive and visual explanation tools to improve
human calibration and decision-making. The proposed framework aims to guide the
development of credible, safe, and human-aligned AI systems.

</details>
