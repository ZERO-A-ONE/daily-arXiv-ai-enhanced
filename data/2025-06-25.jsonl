{"id": "2506.19045", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2506.19045", "abs": "https://arxiv.org/abs/2506.19045", "authors": ["Ahmadreza Saboor Yaraghi", "Golnaz Gharachorlu", "Sakina Fatima", "Lionel C. Briand", "Ruiyuan Wan", "Ruifeng Gao"], "title": "Black-Box Test Code Fault Localization Driven by Large Language Models and Execution Estimation", "comment": null, "summary": "Fault localization (FL) is a critical step in debugging which typically\nrelies on repeated executions to pinpoint faulty code regions. However,\nrepeated executions can be impractical in the presence of non-deterministic\nfailures or high execution costs. While recent efforts have leveraged Large\nLanguage Models (LLMs) to aid execution-free FL, these have primarily focused\non identifying faults in the system under test (SUT) rather than in the often\ncomplex system test code. However, the latter is also important as, in\npractice, many failures are triggered by faulty test code. To overcome these\nchallenges, we introduce a fully static, LLM-driven approach for system test\ncode fault localization (TCFL) that does not require executing the test case.\nOur method uses a single failure execution log to estimate the test's execution\ntrace through three novel algorithms that identify only code statements likely\ninvolved in the failure. This pruned trace, combined with the error message, is\nused to prompt the LLM to rank potential faulty locations. Our black-box,\nsystem-level approach requires no access to the SUT source code and is\napplicable to large test scripts that assess full system behavior. We evaluate\nour technique at function, block, and line levels using an industrial dataset\nof faulty test cases not previously used in pre-training LLMs. Results show\nthat our best estimated trace closely match actual traces, with an F1 score of\naround 90%. Additionally, pruning the complex system test code reduces the\nLLM's inference time by up to 34% without any loss in FL performance. Our\nresults further suggest that block-level TCFL offers a practical balance,\nnarrowing the search space while preserving useful context, achieving an 81%\nhit rate at top-3 (Hit@3)."}
{"id": "2506.19153", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2506.19153", "abs": "https://arxiv.org/abs/2506.19153", "authors": ["Krzysztof Fonal"], "title": "Dataset of Yul Contracts to Support Solidity Compiler Research", "comment": "4 pages", "summary": "The YulCode dataset presents a comprehensive collection of 348,840 Yul-based\nsmart contract instances, comprising approximately 135,013 unique contracts.\nThese contracts were generated through the compilation of Solidity source files\nthat have been deployed on the Ethereum mainnet, making the dataset directly\nrepresentative of real-world decentralized applications. YulCode provides a\nrich foundation for a variety of research and development tasks, including but\nnot limited to machine learning applications, formal verification, optimization\nanalysis, and software engineering tool evaluation in the context of low-level\nsmart contract code. To the best of our knowledge at the time of writing,\nYulCode is the first and only publicly available dataset that focuses\nspecifically on Yul, an intermediate language designed for the Ethereum Virtual\nMachine (EVM). As such, it fills a critical gap in the current ecosystem of\nsmart contract datasets and opens new avenues for research and tooling aimed at\nlow-level contract analysis and generation."}
{"id": "2506.19287", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2506.19287", "abs": "https://arxiv.org/abs/2506.19287", "authors": ["Yaoxuan Wu", "Xiaojie Zhou", "Ahmad Humayun", "Muhammad Ali Gulzar", "Miryung Kim"], "title": "Generating and Understanding Tests via Path-Aware Symbolic Execution with LLMs", "comment": null, "summary": "Symbolic execution is a widely used technique for test generation, offering\nsystematic exploration of program paths through constraint solving. However, it\nis fundamentally constrained by the capability to model the target code\nincluding library functions in terms of symbolic constraint and the capability\nof underlying constraint solvers. As a result, many paths involving complex\nfeatures remain unanalyzed or insufficiently modeled. Recent advances in large\nlanguage models (LLMs) have shown promise in generating diverse and valid test\ninputs. Yet, LLMs lack mechanisms for systematically enumerating program paths\nand often fail to cover subtle corner cases. We observe that directly prompting\nan LLM with the full program leads to missed coverage of interesting paths. In\nthis paper, we present PALM, a test generation system that combines symbolic\npath enumeration with LLM-assisted test generation. PALM statically enumerates\npossible paths through AST-level analysis and transforms each into an\nexecutable variant with embedded assertions that specify the target path. This\navoids the need to translate path constraints into SMT formulae, by instead\nconstructing program variants that LLM can interpret. Importantly, PALM is the\nfirst to provide an interactive frontend that visualizes path coverage\nalongside generated tests, assembling tests based on the specific paths they\nexercise. A user study with 12 participants demonstrates that PALM's frontend\nhelps users better understand path coverage and identify which paths are\nactually exercised by PALM-generated tests, through verification and\nvisualization of their path profiles."}
{"id": "2506.19425", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2506.19425", "abs": "https://arxiv.org/abs/2506.19425", "authors": ["Ang Jia", "He Jiang", "Zhilei Ren", "Xiaochen Li", "Ming Fan", "Ting Liu"], "title": "What Makes the Best Decomposition? Investigating Binary Decomposition Under FCG Variance", "comment": null, "summary": "Binary decomposition, which decomposes binary files into modules, plays a\ncritical role in binary reuse detection. Existing binary decomposition works\neither apply anchor-based methods by extending anchor functions to generate\nmodules, or apply clustering-based methods by using clustering algorithms to\ngroup binary functions, which all rely on that reused code shares similar\nfunction call relationships. However, we find that function call graphs (FCGs)\nvary a lot when using different compilation settings, especially with diverse\nfunction inlining decisions.\n  In this work, we conduct the first systematic empirical study on the variance\nof FCGs compiled by various compilation settings and explore its effect on\nbinary decomposition methods. We first construct a dataset compiled by 17\ncompilers, using 6 optimizations to 4 architectures and analyze the changes and\nmappings of the FCGs. We find that the size of FCGs changes dramatically, while\nthe FCGs are still linked by three different kinds of mappings. Then we\nevaluate the existing works under the FCG variance, and results show that\nexisting works are facing great challenges when conducting cross-compiler\nevaluation with diverse optimization settings. Finally, we propose a method to\nidentify the optimal decomposition and compare the existing decomposition works\nwith the optimal decomposition. Existing works either suffer from low coverage\nor cannot generate stable community similarities."}
{"id": "2506.19052", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2506.19052", "abs": "https://arxiv.org/abs/2506.19052", "authors": ["Shuangbao Paul Wang", "Paul Mullin"], "title": "Trustworthy Artificial Intelligence for Cyber Threat Analysis", "comment": null, "summary": "Artificial Intelligence brings innovations into the society. However, bias\nand unethical exist in many algorithms that make the applications less\ntrustworthy. Threats hunting algorithms based on machine learning have shown\ngreat advantage over classical methods. Reinforcement learning models are\ngetting more accurate for identifying not only signature-based but also\nbehavior-based threats. Quantum mechanics brings a new dimension in improving\nclassification speed with exponential advantage. In this research, we developed\na machine learning based cyber threat detection and assessment tool. It uses\ntwo stage, unsupervised and supervised learning, analyzing method on log data\nrecorded from a web server on AWS cloud. The results show the algorithm has the\nability to identify cyber threats with high confidence."}
{"id": "2506.18920", "categories": ["cs.AI", "cs.LG", "cs.MA", "cs.NE", "cs.SI"], "pdf": "https://arxiv.org/pdf/2506.18920", "abs": "https://arxiv.org/abs/2506.18920", "authors": ["Michael Williams"], "title": "Signal Use and Emergent Cooperation", "comment": "167 pages, 19 figures, PhD dissertation, UCLA, 2006", "summary": "In this work, we investigate how autonomous agents, organized into tribes,\nlearn to use communication signals to coordinate their activities and enhance\ntheir collective efficiency. Using the NEC-DAC (Neurally Encoded Culture -\nDistributed Autonomous Communicators) system, where each agent is equipped with\nits own neural network for decision-making, we demonstrate how these agents\ndevelop a shared behavioral system -- akin to a culture -- through learning and\nsignalling. Our research focuses on the self-organization of culture within\nthese tribes of agents and how varying communication strategies impact their\nfitness and cooperation. By analyzing different social structures, such as\nauthority hierarchies, we show that the culture of cooperation significantly\ninfluences the tribe's performance. Furthermore, we explore how signals not\nonly facilitate the emergence of culture but also enable its transmission\nacross generations of agents. Additionally, we examine the benefits of\ncoordinating behavior and signaling within individual agents' neural networks."}
{"id": "2506.19481", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2506.19481", "abs": "https://arxiv.org/abs/2506.19481", "authors": ["Shahbaz Siddeeq", "Muhammad Waseem", "Zeeshan Rasheed", "Md Mahade Hasan", "Jussi Rasku", "Mika Saari", "Henri Terho", "Kalle Makela", "Kai-Kristian Kemell", "Pekka Abrahamsson"], "title": "LLM-based Multi-Agent System for Intelligent Refactoring of Haskell Code", "comment": "arXiv admin note: text overlap with arXiv:2502.07928", "summary": "Refactoring is a constant activity in software development and maintenance.\nScale and maintain software systems are based on code refactoring. However,\nthis process is still labor intensive, as it requires programmers to analyze\nthe codebases in detail to avoid introducing new defects. In this research, we\nput forward a large language model (LLM)-based multi-agent system to automate\nthe refactoring process on Haskell code. The objective of this research is to\nevaluate the effect of LLM-based agents in performing structured and\nsemantically accurate refactoring on Haskell code. Our proposed multi-agent\nsystem based on specialized agents with distinct roles, including code\nanalysis, refactoring execution, verification, and debugging. To test the\neffectiveness and practical applicability of the multi-agent system, we\nconducted evaluations using different open-source Haskell codebases. The\nresults of the experiments carried out showed that the proposed LLM-based\nmulti-agent system could average 11.03% decreased complexity in code, an\nimprovement of 22.46% in overall code quality, and increase performance\nefficiency by an average of 13.27%. Furthermore, memory allocation was\noptimized by up to 14.57%. These results highlight the ability of LLM-based\nmulti-agent in managing refactoring tasks targeted toward functional\nprogramming paradigms. Our findings hint that LLM-based multi-agent systems\nintegration into the refactoring of functional programming languages can\nenhance maintainability and support automated development workflows."}
{"id": "2506.19054", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2506.19054", "abs": "https://arxiv.org/abs/2506.19054", "authors": ["Mintong Kang", "Zhaorun Chen", "Chejian Xu", "Jiawei Zhang", "Chengquan Guo", "Minzhou Pan", "Ivan Revilla", "Yu Sun", "Bo Li"], "title": "PolyGuard: Massive Multi-Domain Safety Policy-Grounded Guardrail Dataset", "comment": null, "summary": "As LLMs become widespread across diverse applications, concerns about the\nsecurity and safety of LLM interactions have intensified. Numerous guardrail\nmodels and benchmarks have been developed to ensure LLM content safety.\nHowever, existing guardrail benchmarks are often built upon ad hoc risk\ntaxonomies that lack a principled grounding in standardized safety policies,\nlimiting their alignment with real-world operational requirements. Moreover,\nthey tend to overlook domain-specific risks, while the same risk category can\ncarry different implications across different domains. To bridge these gaps, we\nintroduce PolyGuard, the first massive multi-domain safety policy-grounded\nguardrail dataset. PolyGuard offers: (1) broad domain coverage across eight\nsafety-critical domains, such as finance, law, and codeGen; (2) policy-grounded\nrisk construction based on authentic, domain-specific safety guidelines; (3)\ndiverse interaction formats, encompassing declarative statements, questions,\ninstructions, and multi-turn conversations; (4) advanced benign data curation\nvia detoxification prompting to challenge over-refusal behaviors; and (5)\n\\textbf{attack-enhanced instances} that simulate adversarial inputs designed to\nbypass guardrails. Based on PolyGuard, we benchmark 19 advanced guardrail\nmodels and uncover a series of findings, such as: (1) All models achieve varied\nF1 scores, with many demonstrating high variance across risk categories,\nhighlighting their limited domain coverage and insufficient handling of\ndomain-specific safety concerns; (2) As models evolve, their coverage of safety\nrisks broadens, but performance on common risk categories may decrease; (3) All\nmodels remain vulnerable to optimized adversarial attacks. We believe that\n\\dataset and the unique insights derived from our evaluations will advance the\ndevelopment of policy-aligned and resilient guardrail systems."}
{"id": "2506.18928", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2506.18928", "abs": "https://arxiv.org/abs/2506.18928", "authors": ["Lingyu Yang"], "title": "Do LLMs Know When to Flip a Coin? Strategic Randomization through Reasoning and Experience", "comment": null, "summary": "Strategic randomization is a key principle in game theory, yet it remains\nunderexplored in large language models (LLMs). Prior work often conflates the\ncognitive decision to randomize with the mechanical generation of randomness,\nleading to incomplete evaluations. To address this, we propose a novel zero-sum\ngame inspired by the Tian Ji Horse Race, where the Nash equilibrium corresponds\nto a maximal entropy strategy. The game's complexity masks this property from\nuntrained humans and underdeveloped LLMs. We evaluate five LLMs across prompt\nstyles -- framed, neutral, and hinted -- using competitive multi-tournament\ngameplay with system-provided random choices, isolating the decision to\nrandomize. Results show that weaker models remain deterministic regardless of\nprompts, while stronger models exhibit increased randomization under explicit\nhints. When facing weaker models, strong LLMs adopt deterministic strategies to\nexploit biases, but converge toward equilibrium play when facing peers. Through\nwin/loss outcomes and Bayes factor analysis, we demonstrate meaningful\nvariation in LLMs' strategic reasoning capabilities, highlighting opportunities\nfor improvement in abstract reasoning and adaptive learning. We make our\nimplementation publicly available at\nhttps://github.com/ocelopus/llm-when-to-throw-coin to ensure full\nreproducibility."}
{"id": "2506.19511", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2506.19511", "abs": "https://arxiv.org/abs/2506.19511", "authors": ["Nina Haugland Andersen", "Anastasiia Tkalich", "Nils Brede Moe", "Darja Smite", "Asgaut Mjølne Söderbom", "Ola Hast", "Viktoria Stray"], "title": "Integrating Pair Programming as a Work Practice", "comment": "The pre-print is submitted to the Journal of Systems and Software", "summary": "Context: Pair programming (PP) is more relevant than ever. As modern systems\ngrow in complexity, knowledge sharing and collaboration across teams have\nbecome essential. However, despite well-documented benefits of PP, its adoption\nremains inconsistent across software teams. Objective: This study aims to\nunderstand the factors that facilitate or hinder team members' adoption as well\nas lasting engagement in PP. Method: We have conducted an exploratory\nsingle-case study in a mature agile company in Norway. We collected data\nthrough two rounds of interviews with team members in different roles and\nperformed a thematic analysis of the interviews. Results: Our key finding is\nthat multiple factors, related to the perceptions of how PP contributes to\ndaily work, efforts associated with engaging in PP sessions, company and team\nattitudes, resources, infrastructure, and task characteristics, affect PP\nengagement. Conclusion: Long-term engagement in PP requires expected benefits\nwith the practice being confirmed in firsthand experiences. Adapting the\npractice to each unique team, with insights drawn from collective learning, is\nalso beneficial. Our findings will be beneficial for software practitioners\nseeking to make PP an integrated part of their team's workflow."}
{"id": "2506.19109", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.19109", "abs": "https://arxiv.org/abs/2506.19109", "authors": ["Valerii Gakh", "Hayretdin Bahsi"], "title": "Enhancing Security in LLM Applications: A Performance Evaluation of Early Detection Systems", "comment": "18 pages, 8 tables, 7 figures", "summary": "Prompt injection threatens novel applications that emerge from adapting LLMs\nfor various user tasks. The newly developed LLM-based software applications\nbecome more ubiquitous and diverse. However, the threat of prompt injection\nattacks undermines the security of these systems as the mitigation and defenses\nagainst them, proposed so far, are insufficient. We investigated the\ncapabilities of early prompt injection detection systems, focusing specifically\non the detection performance of techniques implemented in various open-source\nsolutions. These solutions are supposed to detect certain types of prompt\ninjection attacks, including the prompt leak. In prompt leakage attacks, an\nattacker maliciously manipulates the LLM into outputting its system\ninstructions, violating the system's confidentiality. Our study presents\nanalyzes of distinct prompt leakage detection techniques, and a comparative\nanalysis of several detection solutions, which implement those techniques. We\nidentify the strengths and weaknesses of these techniques and elaborate on\ntheir optimal configuration and usage in high-stake deployments. In one of the\nfirst studies on existing prompt leak detection solutions, we compared the\nperformances of LLM Guard, Vigil, and Rebuff. We concluded that the\nimplementations of canary word checks in Vigil and Rebuff were not effective at\ndetecting prompt leak attacks, and we proposed improvements for them. We also\nfound an evasion weakness in Rebuff's secondary model-based technique and\nproposed a mitigation. Then, the result of the comparison of LLM Guard, Vigil,\nand Rebuff at their peak performance revealed that Vigil is optimal for cases\nwhen minimal false positive rate is required, and Rebuff is the most optimal\nfor average needs."}
{"id": "2506.18957", "categories": ["cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.18957", "abs": "https://arxiv.org/abs/2506.18957", "authors": ["Sheraz Khan", "Subha Madhavan", "Kannan Natarajan"], "title": "A Comment On \"The Illusion of Thinking\": Reframing the Reasoning Cliff as an Agentic Gap", "comment": "10 pages, 2 figures, Comment on \"The Illusion of Thinking:\n  Understanding the Strengths and Limitations of Reasoning Models via the Lens\n  of Problem Complexity\" (arXiv:2506.06941v1)", "summary": "The recent work by Shojaee et al. (2025), titled The Illusion of Thinking:\nUnderstanding the Strengths and Limitations of Reasoning Models via the Lens of\nProblem Complexity, presents a compelling empirical finding, a reasoning cliff,\nwhere the performance of Large Reasoning Models (LRMs) collapses beyond a\nspecific complexity threshold, which the authors posit as an intrinsic scaling\nlimitation of Chain-of-Thought (CoT) reasoning. This commentary, while\nacknowledging the study's methodological rigor, contends that this conclusion\nis confounded by experimental artifacts. We argue that the observed failure is\nnot evidence of a fundamental cognitive boundary, but rather a predictable\noutcome of system-level constraints in the static, text-only evaluation\nparadigm, including tool use restrictions, context window recall issues, the\nabsence of crucial cognitive baselines, inadequate statistical reporting, and\noutput generation limits. We reframe this performance collapse through the lens\nof an agentic gap, asserting that the models are not failing at reasoning, but\nat execution within a profoundly restrictive interface. We empirically\nsubstantiate this critique by demonstrating a striking reversal. A model,\ninitially declaring a puzzle impossible when confined to text-only generation,\nnow employs agentic tools to not only solve it but also master variations of\ncomplexity far beyond the reasoning cliff it previously failed to surmount.\nAdditionally, our empirical analysis of tool-enabled models like o4-mini and\nGPT-4o reveals a hierarchy of agentic reasoning, from simple procedural\nexecution to complex meta-cognitive self-correction, which has significant\nimplications for how we define and measure machine intelligence. The illusion\nof thinking attributed to LRMs is less a reasoning deficit and more a\nconsequence of an otherwise capable mind lacking the tools for action."}
{"id": "2506.19539", "categories": ["cs.SE", "cs.AI", "D.2.7"], "pdf": "https://arxiv.org/pdf/2506.19539", "abs": "https://arxiv.org/abs/2506.19539", "authors": ["Julian Fragner", "Christian Macho", "Bernhard Dieber", "Martin Pinzger"], "title": "Lost in Translation? Converting RegExes for Log Parsing into Dynatrace Pattern Language", "comment": "18 pages, 7 tables, 18 figures", "summary": "Log files provide valuable information for detecting and diagnosing problems\nin enterprise software applications and data centers. Several log analytics\ntools and platforms were developed to help filter and extract information from\nlogs, typically using regular expressions (RegExes). Recent commercial log\nanalytics platforms provide domain-specific languages specifically designed for\nlog parsing, such as Grok or the Dynatrace Pattern Language (DPL). However,\nusers who want to migrate to these platforms must manually convert their\nRegExes into the new pattern language, which is costly and error-prone. In this\nwork, we present Reptile, which combines a rule-based approach for converting\nRegExes into DPL patterns with a best-effort approach for cases where a full\nconversion is impossible. Furthermore, it integrates GPT-4 to optimize the\nobtained DPL patterns. The evaluation with 946 RegExes collected from a large\ncompany shows that Reptile safely converted 73.7% of them. The evaluation of\nReptile's pattern optimization with 23 real-world RegExes showed an F1-score\nand MCC above 0.91. These results are promising and have ample practical\nimplications for companies that migrate to a modern log analytics platform,\nsuch as Dynatrace."}
{"id": "2506.19260", "categories": ["cs.CR", "cs.DC", "cs.LG", "I.2.6; C.2.4; K.6.5"], "pdf": "https://arxiv.org/pdf/2506.19260", "abs": "https://arxiv.org/abs/2506.19260", "authors": ["Murtaza Rangwala", "Richard O. Sinnott", "Rajkumar Buyya"], "title": "Network Structures as an Attack Surface: Topology-Based Privacy Leakage in Federated Learning", "comment": "13 pages, 7 figures, 5 tables. Data from the experiments and source\n  code can be found here: https://doi.org/10.5281/zenodo.15622123", "summary": "Federated learning systems increasingly rely on diverse network topologies to\naddress scalability and organizational constraints. While existing privacy\nresearch focuses on gradient-based attacks, the privacy implications of network\ntopology knowledge remain critically understudied. We conduct the first\ncomprehensive analysis of topology-based privacy leakage across realistic\nadversarial knowledge scenarios, demonstrating that adversaries with varying\ndegrees of structural knowledge can infer sensitive data distribution patterns\neven under strong differential privacy guarantees. Through systematic\nevaluation of 4,720 attack instances, we analyze six distinct adversarial\nknowledge scenarios: complete topology knowledge and five partial knowledge\nconfigurations reflecting real-world deployment constraints. We propose three\ncomplementary attack vectors: communication pattern analysis, parameter\nmagnitude profiling, and structural position correlation, achieving success\nrates of 84.1%, 65.0%, and 47.2% under complete knowledge conditions.\nCritically, we find that 80% of realistic partial knowledge scenarios maintain\nattack effectiveness above security thresholds, with certain partial knowledge\nconfigurations achieving performance superior to the baseline complete\nknowledge scenario. To address these vulnerabilities, we propose and\nempirically validate structural noise injection as a complementary defense\nmechanism across 808 configurations, demonstrating up to 51.4% additional\nattack reduction when properly layered with existing privacy techniques. These\nresults establish that network topology represents a fundamental privacy\nvulnerability in federated learning systems while providing practical pathways\nfor mitigation through topology-aware defense mechanisms."}
{"id": "2506.19046", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2506.19046", "abs": "https://arxiv.org/abs/2506.19046", "authors": ["Filip Sabo", "Michele Meroni", "Maria Piles", "Martin Claverie", "Fanie Ferreira", "Elna Van Den Berg", "Francesco Collivignarelli", "Felix Rembold"], "title": "From Rows to Yields: How Foundation Models for Tabular Data Simplify Crop Yield Prediction", "comment": null, "summary": "We present an application of a foundation model for small- to medium-sized\ntabular data (TabPFN), to sub-national yield forecasting task in South Africa.\nTabPFN has recently demonstrated superior performance compared to traditional\nmachine learning (ML) models in various regression and classification tasks. We\nused the dekadal (10-days) time series of Earth Observation (EO; FAPAR and soil\nmoisture) and gridded weather data (air temperature, precipitation and\nradiation) to forecast the yield of summer crops at the sub-national level. The\ncrop yield data was available for 23 years and for up to 8 provinces. Covariate\nvariables for TabPFN (i.e., EO and weather) were extracted by region and\naggregated at a monthly scale. We benchmarked the results of the TabPFN against\nsix ML models and three baseline models. Leave-one-year-out cross-validation\nexperiment setting was used in order to ensure the assessment of the models\ncapacity to forecast an unseen year. Results showed that TabPFN and ML models\nexhibit comparable accuracy, outperforming the baselines. Nonetheless, TabPFN\ndemonstrated superior practical utility due to its significantly faster tuning\ntime and reduced requirement for feature engineering. This renders TabPFN a\nmore viable option for real-world operation yield forecasting applications,\nwhere efficiency and ease of implementation are paramount."}
{"id": "2506.19653", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2506.19653", "abs": "https://arxiv.org/abs/2506.19653", "authors": ["Antonios Saravanos"], "title": "Simulating the Waterfall Model: A Systematic Review", "comment": null, "summary": "This systematic mapping study examines how the Waterfall Model has been\nrepresented in computational simulations within peer-reviewed literature. While\nAgile methodologies dominate contemporary software design practices, the\nWaterfall Model persists, particularly, within hybrid approaches that fuse\nstructured, sequential workflows with the adaptability of agile practices.\nDespite its continued presence, little attention has been given to how the\nWaterfall Model is simulated in research contexts. A structured search of major\nacademic databases identified 68 peer-reviewed studies published between 2000\nand 2024. After applying inclusion criteria, selected studies were analyzed\nacross four dimensions: (1) simulation methodologies (e.g., discrete-event\nsimulation, system dynamics), (2) platforms and tools (e.g., Simphony.NET,\nSimPy), (3) geographic and temporal trends, and (4) fidelity to Royce's\noriginal seven-phase model. Discrete-event simulation was most commonly used,\nreflecting the model's sequential nature. Early work relied on proprietary\nplatforms, while recent studies increasingly use open-source, Python-based\ntools. No studies fully implemented Royce's original formulation, most employed\nadaptations. These findings suggest that although niche, simulation of the\nWaterfall Model is present in academic discourse. This work highlights the need\nfor accessible modeling tools and calls for future research that integrates the\nwaterfall software process model with modern hybrid practices."}
{"id": "2506.19356", "categories": ["cs.CR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.19356", "abs": "https://arxiv.org/abs/2506.19356", "authors": ["Ye Tian", "Zhang Yumin", "Yifan Jia", "Jianguo Sun", "Yanbin Wang"], "title": "WebGuard++:Interpretable Malicious URL Detection via Bidirectional Fusion of HTML Subgraphs and Multi-Scale Convolutional BERT", "comment": null, "summary": "URL+HTML feature fusion shows promise for robust malicious URL detection,\nsince attacker artifacts persist in DOM structures. However, prior work suffers\nfrom four critical shortcomings: (1) incomplete URL modeling, failing to\njointly capture lexical patterns and semantic context; (2) HTML graph sparsity,\nwhere threat-indicative nodes (e.g., obfuscated scripts) are isolated amid\nbenign content, causing signal dilution during graph aggregation; (3)\nunidirectional analysis, ignoring URL-HTML feature bidirectional interaction;\nand (4) opaque decisions, lacking attribution to malicious DOM components. To\naddress these challenges, we present WebGuard++, a detection framework with 4\nnovel components: 1) Cross-scale URL Encoder: Hierarchically learns\nlocal-to-global and coarse to fine URL features based on Transformer network\nwith dynamic convolution. 2) Subgraph-aware HTML Encoder: Decomposes DOM graphs\ninto interpretable substructures, amplifying sparse threat signals via\nHierarchical feature fusion. 3) Bidirectional Coupling Module: Aligns URL and\nHTML embeddings through cross-modal contrastive learning, optimizing\ninter-modal consistency and intra-modal specificity. 4) Voting Module:\nLocalizes malicious regions through consensus voting on malicious subgraph\npredictions. Experiments show WebGuard++ achieves significant improvements over\nstate-of-the-art baselines, achieving 1.1x-7.9x higher TPR at fixed FPR of\n0.001 and 0.0001 across both datasets."}
{"id": "2506.19095", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2506.19095", "abs": "https://arxiv.org/abs/2506.19095", "authors": ["Fien van Wetten", "Aske Plaat", "Max van Duijn"], "title": "Baba is LLM: Reasoning in a Game with Dynamic Rules", "comment": null, "summary": "Large language models (LLMs) are known to perform well on language tasks, but\nstruggle with reasoning tasks. This paper explores the ability of LLMs to play\nthe 2D puzzle game Baba is You, in which players manipulate rules by\nrearranging text blocks that define object properties. Given that this\nrule-manipulation relies on language abilities and reasoning, it is a\ncompelling challenge for LLMs. Six LLMs are evaluated using different prompt\ntypes, including (1) simple, (2) rule-extended and (3) action-extended prompts.\nIn addition, two models (Mistral, OLMo) are finetuned using textual and\nstructural data from the game. Results show that while larger models\n(particularly GPT-4o) perform better in reasoning and puzzle solving, smaller\nunadapted models struggle to recognize game mechanics or apply rule changes.\nFinetuning improves the ability to analyze the game levels, but does not\nsignificantly improve solution formulation. We conclude that even for\nstate-of-the-art and finetuned LLMs, reasoning about dynamic rule changes is\ndifficult (specifically, understanding the use-mention distinction). The\nresults provide insights into the applicability of LLMs to complex\nproblem-solving tasks and highlight the suitability of games with dynamically\nchanging rules for testing reasoning and reflection by LLMs."}
{"id": "2506.19677", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2506.19677", "abs": "https://arxiv.org/abs/2506.19677", "authors": ["Shi Chang", "Boyuan Chen", "Kishanthan Thangarajah", "Hanan Lutfiyya", "Ahmed E. Hassan"], "title": "Adaptive Request Scheduling for CodeLLM Serving with SLA Guarantees", "comment": null, "summary": "Code Large Language Models (CodeLLMs) are increasingly integrated into modern\nsoftware development workflows, yet efficiently serving them in\nresource-constrained, self-hosted environments remains a significant challenge.\nExisting LLM serving systems employs Continuous Batching for throughput\nimprovement. However, they rely on static batch size configurations that cannot\nadapt to fluctuating request rates or heterogeneous workloads, leading to\nfrequent SLA (Service Level Agreement) violations and unstable performance. In\nthis study, We propose SABER, a dynamic batching strategy that predicts\nper-request SLA feasibility and adjusts decisions in real time. SABER improves\ngoodput by up to 26% over the best static configurations and reduces latency\nvariability by up to 45%, all without manual tuning or service restarts. Our\nresults demonstrate that SLA-aware, adaptive scheduling is key to robust,\nhigh-performance CodeLLM serving."}
{"id": "2506.19360", "categories": ["cs.CR", "cs.CV"], "pdf": "https://arxiv.org/pdf/2506.19360", "abs": "https://arxiv.org/abs/2506.19360", "authors": ["Yunsung Chung", "Yunbei Zhang", "Nassir Marrouche", "Jihun Hamm"], "title": "SoK: Can Synthetic Images Replace Real Data? A Survey of Utility and Privacy of Synthetic Image Generation", "comment": "Accepted at the 34th USENIX Security Symposium (USENIX Security '25).\n  21 pages, plus a 6-page appendix", "summary": "Advances in generative models have transformed the field of synthetic image\ngeneration for privacy-preserving data synthesis (PPDS). However, the field\nlacks a comprehensive survey and comparison of synthetic image generation\nmethods across diverse settings. In particular, when we generate synthetic\nimages for the purpose of training a classifier, there is a pipeline of\ngeneration-sampling-classification which takes private training as input and\noutputs the final classifier of interest. In this survey, we systematically\ncategorize existing image synthesis methods, privacy attacks, and mitigations\nalong this generation-sampling-classification pipeline. To empirically compare\ndiverse synthesis approaches, we provide a benchmark with representative\ngenerative methods and use model-agnostic membership inference attacks (MIAs)\nas a measure of privacy risk. Through this study, we seek to answer critical\nquestions in PPDS: Can synthetic data effectively replace real data? Which\nrelease strategy balances utility and privacy? Do mitigations improve the\nutility-privacy tradeoff? Which generative models perform best across different\nscenarios? With a systematic evaluation of diverse methods, our study provides\nactionable insights into the utility-privacy tradeoffs of synthetic data\ngeneration methods and guides the decision on optimal data releasing strategies\nfor real-world applications."}
{"id": "2506.19185", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2506.19185", "abs": "https://arxiv.org/abs/2506.19185", "authors": ["Janak Kapuriya", "Aman Singh", "Jainendra Shukla", "Rajiv Ratn Shah"], "title": "Spiritual-LLM : Gita Inspired Mental Health Therapy In the Era of LLMs", "comment": null, "summary": "Traditional mental health support systems often generate responses based\nsolely on the user's current emotion and situations, resulting in superficial\ninterventions that fail to address deeper emotional needs. This study\nintroduces a novel framework by integrating spiritual wisdom from the Bhagavad\nGita with advanced large language model GPT-4o to enhance emotional well-being.\nWe present the GITes (Gita Integrated Therapy for Emotional Support) dataset,\nwhich enhances the existing ExTES mental health dataset by including 10,729\nspiritually guided responses generated by GPT-4o and evaluated by domain\nexperts. We benchmark GITes against 12 state-of-the-art LLMs, including both\nmental health specific and general purpose models. To evaluate spiritual\nrelevance in generated responses beyond what conventional n-gram based metrics\ncapture, we propose a novel Spiritual Insight metric and automate assessment\nvia an LLM as jury framework using chain-of-thought prompting. Integrating\nspiritual guidance into AI driven support enhances both NLP and spiritual\nmetrics for the best performing LLM Phi3-Mini 3.2B Instruct, achieving\nimprovements of 122.71% in ROUGE, 126.53% in METEOR, 8.15% in BERT score,\n15.92% in Spiritual Insight, 18.61% in Sufficiency and 13.22% in Relevance\ncompared to its zero-shot counterpart. While these results reflect substantial\nimprovements across automated empathy and spirituality metrics, further\nvalidation in real world patient populations remains a necessary step. Our\nfindings indicate a strong potential for AI systems enriched with spiritual\nguidance to enhance user satisfaction and perceived support outcomes. The code\nand dataset will be publicly available to advance further research in this\nemerging area."}
{"id": "2506.19757", "categories": ["cs.SE", "cs.HC"], "pdf": "https://arxiv.org/pdf/2506.19757", "abs": "https://arxiv.org/abs/2506.19757", "authors": ["Rodrigo Oliveira Zacarias", "Léo Carvalho Ramos Antunes", "Márcio de Oliveira Barros", "Rodrigo Pereira dos Santos", "Patricia Lago"], "title": "Exploring Developer Experience Factors in Software Ecosystems", "comment": "58 pages", "summary": "Context: Developer experience (DX) plays a key role in developers'\nperformance and their continued involvement in a software ecosystem (SECO)\nplatform. While researchers and practitioners have recognized several factors\naffecting DX in SECO platforms, a clear roadmap of the most influential factors\nis still missing. This is particularly important given the direct impact on\ndevelopers' interest in SECO and their ongoing engagement with the common\ntechnological platform. Goal: This work aims to identify key DX factors and\nunderstand how they influence third-party developers' decisions to adopt and\nkeep contributing to a SECO. Methods: We conducted a systematic mapping study\n(SMS), analyzing 29 studies to assess the state-of-the-art of DX in SECO.\nAdditionally, we conducted a Delphi study to evaluate the influence of 27 DX\nfactors (identified in our SMS) from the perspective of 21 third-party\ndevelopers to adopt and keep contributing to a SECO. Results: The factors that\nmost strongly influence developers' adoption and ongoing contributions to a\nSECO are: financial costs for using the platform, desired technical resources\nfor development, low barriers to entry into the applications market, and more\nfinancial gains. Conclusion: DX is essential for the success and sustainability\nof SECO. Our set of DX factors provides valuable insights and recommendations\nfor researchers and practitioners to address key DX concerns from the\nperspective of third-party developers."}
{"id": "2506.19368", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2506.19368", "abs": "https://arxiv.org/abs/2506.19368", "authors": ["Xiang Liu", "Zhanpeng Guo", "Liangxi Liu", "Mengyao Zheng", "Yiming Qiu", "Linshan Jiang"], "title": "Yotta: A Large-Scale Trustless Data Trading Scheme for Blockchain System", "comment": "9 pages, 2 figures, Exploratory Paper", "summary": "Data trading is one of the key focuses of Web 3.0. However, all the current\nmethods that rely on blockchain-based smart contracts for data exchange cannot\nsupport large-scale data trading while ensuring data security, which falls\nshort of fulfilling the spirit of Web 3.0. Even worse, there is currently a\nlack of discussion on the essential properties that large-scale data trading\nshould satisfy. In this work, we are the first to formalize the property\nrequirements for enabling data trading in Web 3.0. Based on these requirements,\nwe are the first to propose Yotta, a complete batch data trading scheme for\nblockchain, which features a data trading design that leverages our innovative\ncryptographic workflow with IPFS and zk-SNARK. Our simulation results\ndemonstrate that Yotta outperforms baseline approaches up to 130 times and\nexhibits excellent scalability to satisfy all the properties."}
{"id": "2506.19191", "categories": ["cs.AI", "cs.CL", "cs.GT", "math.LO", "68T05, 68Q87, 03E20", "I.2.6; I.2.3; F.1.1"], "pdf": "https://arxiv.org/pdf/2506.19191", "abs": "https://arxiv.org/abs/2506.19191", "authors": ["Craig Steven Wright"], "title": "Bayesian Evolutionary Swarm Architecture: A Formal Epistemic System Grounded in Truth-Based Competition", "comment": "83 pages, 14 sections, 92 formal results, no prior conference\n  publication", "summary": "We introduce a mathematically rigorous framework for an artificial\nintelligence system composed of probabilistic agents evolving through\nstructured competition and belief revision. The architecture, grounded in\nBayesian inference, measure theory, and population dynamics, defines agent\nfitness as a function of alignment with a fixed external oracle representing\nground truth. Agents compete in a discrete-time environment, adjusting\nposterior beliefs through observed outcomes, with higher-rated agents\nreproducing and lower-rated agents undergoing extinction. Ratings are updated\nvia pairwise truth-aligned utility comparisons, and belief updates preserve\nmeasurable consistency and stochastic convergence. We introduce hash-based\ncryptographic identity commitments to ensure traceability, alongside causal\ninference operators using do-calculus. Formal theorems on convergence,\nrobustness, and evolutionary stability are provided. The system establishes\ntruth as an evolutionary attractor, demonstrating that verifiable knowledge\narises from adversarial epistemic pressure within a computable, self-regulating\nswarm."}
{"id": "2506.19393", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2506.19393", "abs": "https://arxiv.org/abs/2506.19393", "authors": ["Daniel Reijsbergen", "Eyasu Getahun Chekole", "Howard Halim", "Jianying Zhou"], "title": "ZK-SERIES: Privacy-Preserving Authentication using Temporal Biometric Data", "comment": null, "summary": "Biometric authentication relies on physiological or behavioral traits that\nare inherent to a user, making them difficult to lose, forge or forget.\nBiometric data with a temporal component enable the following authentication\nprotocol: recent readings of the underlying biometrics are encoded as time\nseries and compared to a set of base readings. If the distance between the new\nreadings and the base readings falls within an acceptable threshold, then the\nuser is successfully authenticated. Various methods exist for comparing time\nseries data, such as Dynamic Time Warping (DTW) and the Time Warp Edit Distance\n(TWED), each offering advantages and drawbacks depending on the context.\nMoreover, many of these techniques do not inherently preserve privacy, which is\na critical consideration in biometric authentication due to the complexity of\nresetting biometric credentials.\n  In this work, we propose ZK-SERIES to provide privacy and efficiency to a\nbroad spectrum of time series-based authentication protocols. ZK-SERIES uses\nthe same building blocks, i.e., zero-knowledge multiplication proofs and\nefficiently batched range proofs, to ensure consistency across all protocols.\nFurthermore, it is optimized for compatibility with low-capacity devices such\nas smartphones. To assess the effectiveness of our proposed technique, we\nprimarily focus on two case studies for biometric authentication: shake-based\nand blow-based authentication. To demonstrate ZK-SERIES's practical\napplicability even in older and less powerful smartphones, we conduct\nexperiments on a 5-year-old low-spec smartphone using real data for two case\nstudies alongside scalability assessments using artificial data. Our\nexperimental results indicate that the privacy-preserving authentication\nprotocol can be completed within 1.3 seconds on older devices."}
{"id": "2506.19224", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2506.19224", "abs": "https://arxiv.org/abs/2506.19224", "authors": ["Shuyin Xia", "Guan Wang", "Gaojie Xu", "Sen Zhao", "Guoyin Wang"], "title": "GBGC: Efficient and Adaptive Graph Coarsening via Granular-ball Computing", "comment": null, "summary": "The objective of graph coarsening is to generate smaller, more manageable\ngraphs while preserving key information of the original graph. Previous work\nwere mainly based on the perspective of spectrum-preserving, using some\npredefined coarsening rules to make the eigenvalues of the Laplacian matrix of\nthe original graph and the coarsened graph match as much as possible. However,\nthey largely overlooked the fact that the original graph is composed of\nsubregions at different levels of granularity, where highly connected and\nsimilar nodes should be more inclined to be aggregated together as nodes in the\ncoarsened graph. By combining the multi-granularity characteristics of the\ngraph structure, we can generate coarsened graph at the optimal granularity. To\nthis end, inspired by the application of granular-ball computing in\nmulti-granularity, we propose a new multi-granularity, efficient, and adaptive\ncoarsening method via granular-ball (GBGC), which significantly improves the\ncoarsening results and efficiency. Specifically, GBGC introduces an adaptive\ngranular-ball graph refinement mechanism, which adaptively splits the original\ngraph from coarse to fine into granular-balls of different sizes and optimal\ngranularity, and constructs the coarsened graph using these granular-balls as\nsupernodes. In addition, compared with other state-of-the-art graph coarsening\nmethods, the processing speed of this method can be increased by tens to\nhundreds of times and has lower time complexity. The accuracy of GBGC is almost\nalways higher than that of the original graph due to the good robustness and\ngeneralization of the granular-ball computing, so it has the potential to\nbecome a standard graph data preprocessing method."}
{"id": "2506.19409", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2506.19409", "abs": "https://arxiv.org/abs/2506.19409", "authors": ["Thomas Prévost", "Bruno Martin", "Olivier Alibart"], "title": "An ETSI GS QKD compliant TLS implementation", "comment": null, "summary": "A modification of the TLS protocol is presented, using our implementation of\nthe Quantum Key Distribution (QKD) standard ETSI GS QKD 014 v1.1.1. We rely on\nthe Rustls library for this. The TLS protocol is modified while maintaining\nbackward compatibility on the client and server side. We thus wish to\nparticipate in the effort to generalize the use of QKD on the Internet. We used\nour protocol for a video conference call encrypted by QKD. Finally, we analyze\nthe performance of our protocol, comparing the time needed to establish a\nhandshake to that of TLS 1.3."}
{"id": "2506.19235", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2506.19235", "abs": "https://arxiv.org/abs/2506.19235", "authors": ["Yu Xie", "Xingkai Ren", "Ying Qi", "Yao Hu", "Lianlei Shan"], "title": "RecLLM-R1: A Two-Stage Training Paradigm with Reinforcement Learning and Chain-of-Thought v1", "comment": null, "summary": "Traditional recommendation systems often grapple with \"filter bubbles\",\nunderutilization of external knowledge, and a disconnect between model\noptimization and business policy iteration. To address these limitations, this\npaper introduces RecLLM-R1, a novel recommendation framework leveraging Large\nLanguage Models (LLMs) and drawing inspiration from the DeepSeek R1\nmethodology. The framework initiates by transforming user profiles, historical\ninteractions, and multi-faceted item attributes into LLM-interpretable natural\nlanguage prompts through a carefully engineered data construction process.\nSubsequently, a two-stage training paradigm is employed: the initial stage\ninvolves Supervised Fine-Tuning (SFT) to imbue the LLM with fundamental\nrecommendation capabilities. The subsequent stage utilizes Group Relative\nPolicy Optimization (GRPO), a reinforcement learning technique, augmented with\na Chain-of-Thought (CoT) mechanism. This stage guides the model through\nmulti-step reasoning and holistic decision-making via a flexibly defined reward\nfunction, aiming to concurrently optimize recommendation accuracy, diversity,\nand other bespoke business objectives. Empirical evaluations on a real-world\nuser behavior dataset from a large-scale social media platform demonstrate that\nRecLLM-R1 significantly surpasses existing baseline methods across a spectrum\nof evaluation metrics, including accuracy, diversity, and novelty. It\neffectively mitigates the filter bubble effect and presents a promising avenue\nfor the integrated optimization of recommendation models and policies under\nintricate business goals."}
{"id": "2506.19453", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2506.19453", "abs": "https://arxiv.org/abs/2506.19453", "authors": ["Sajal Halder", "Muhammad Ejaz Ahmed", "Seyit Camtepe"], "title": "FuncVul: An Effective Function Level Vulnerability Detection Model using LLM and Code Chunk", "comment": "In The 30th European Symposium on Research in Computer Security\n  (ESORICS), 22 Sep - 26 Sep, 2025, Toulouse, France", "summary": "Software supply chain vulnerabilities arise when attackers exploit weaknesses\nby injecting vulnerable code into widely used packages or libraries within\nsoftware repositories. While most existing approaches focus on identifying\nvulnerable packages or libraries, they often overlook the specific functions\nresponsible for these vulnerabilities. Pinpointing vulnerable functions within\npackages or libraries is critical, as it can significantly reduce the risks\nassociated with using open-source software. Identifying vulnerable patches is\nchallenging because developers often submit code changes that are unrelated to\nvulnerability fixes. To address this issue, this paper introduces FuncVul, an\ninnovative code chunk-based model for function-level vulnerability detection in\nC/C++ and Python, designed to identify multiple vulnerabilities within a\nfunction by focusing on smaller, critical code segments. To assess the model's\neffectiveness, we construct six code and generic code chunk based datasets\nusing two approaches: (1) integrating patch information with large language\nmodels to label vulnerable samples and (2) leveraging large language models\nalone to detect vulnerabilities in function-level code. To design FuncVul\nvulnerability model, we utilise GraphCodeBERT fine tune model that captures\nboth the syntactic and semantic aspects of code. Experimental results show that\nFuncVul outperforms existing state-of-the-art models, achieving an average\naccuracy of 87-92% and an F1 score of 86-92% across all datasets. Furthermore,\nwe have demonstrated that our code-chunk-based FuncVul model improves 53.9%\naccuracy and 42.0% F1-score than the full function-based vulnerability\nprediction. The FuncVul code and datasets are publicly available on GitHub at\nhttps://github.com/sajalhalder/FuncVul."}
{"id": "2506.19280", "categories": ["cs.AI", "cs.HC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.19280", "abs": "https://arxiv.org/abs/2506.19280", "authors": ["Feiting Yang", "Antoine Moevus", "Steve Lévesque"], "title": "Emotion Detection on User Front-Facing App Interfaces for Enhanced Schedule Optimization: A Machine Learning Approach", "comment": null, "summary": "Human-Computer Interaction (HCI) has evolved significantly to incorporate\nemotion recognition capabilities, creating unprecedented opportunities for\nadaptive and personalized user experiences. This paper explores the integration\nof emotion detection into calendar applications, enabling user interfaces to\ndynamically respond to users' emotional states and stress levels, thereby\nenhancing both productivity and engagement. We present and evaluate two\ncomplementary approaches to emotion detection: a biometric-based method\nutilizing heart rate (HR) data extracted from electrocardiogram (ECG) signals\nprocessed through Long Short-Term Memory (LSTM) and Gated Recurrent Unit (GRU)\nneural networks to predict the emotional dimensions of Valence, Arousal, and\nDominance; and a behavioral method analyzing computer activity through multiple\nmachine learning models to classify emotions based on fine-grained user\ninteractions such as mouse movements, clicks, and keystroke patterns. Our\ncomparative analysis, from real-world datasets, reveals that while both\napproaches demonstrate effectiveness, the computer activity-based method\ndelivers superior consistency and accuracy, particularly for mouse-related\ninteractions, which achieved approximately 90\\% accuracy. Furthermore, GRU\nnetworks outperformed LSTM models in the biometric approach, with Valence\nprediction reaching 84.38\\% accuracy."}
{"id": "2506.19480", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2506.19480", "abs": "https://arxiv.org/abs/2506.19480", "authors": ["Pasquale De Rosa", "Simon Queyrut", "Yérom-David Bromberg", "Pascal Felber", "Valerio Schiavoni"], "title": "PhishingHook: Catching Phishing Ethereum Smart Contracts leveraging EVM Opcodes", "comment": null, "summary": "The Ethereum Virtual Machine (EVM) is a decentralized computing engine. It\nenables the Ethereum blockchain to execute smart contracts and decentralized\napplications (dApps). The increasing adoption of Ethereum sparked the rise of\nphishing activities. Phishing attacks often target users through deceptive\nmeans, e.g., fake websites, wallet scams, or malicious smart contracts, aiming\nto steal sensitive information or funds. A timely detection of phishing\nactivities in the EVM is therefore crucial to preserve the user trust and\nnetwork integrity. Some state-of-the art approaches to phishing detection in\nsmart contracts rely on the online analysis of transactions and their traces.\nHowever, replaying transactions often exposes sensitive user data and\ninteractions, with several security concerns. In this work, we present\nPhishingHook, a framework that applies machine learning techniques to detect\nphishing activities in smart contracts by directly analyzing the contract's\nbytecode and its constituent opcodes. We evaluate the efficacy of such\ntechniques in identifying malicious patterns, suspicious function calls, or\nanomalous behaviors within the contract's code itself before it is deployed or\ninteracted with. We experimentally compare 16 techniques, belonging to four\nmain categories (Histogram Similarity Classifiers, Vision Models, Language\nModels and Vulnerability Detection Models), using 7,000 real-world malware\nsmart contracts. Our results demonstrate the efficiency of PhishingHook in\nperforming phishing classification systems, with about 90% average accuracy\namong all the models. We support experimental reproducibility, and we release\nour code and datasets to the research community."}
{"id": "2506.19290", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2506.19290", "abs": "https://arxiv.org/abs/2506.19290", "authors": ["Liang Zeng", "Yongcong Li", "Yuzhen Xiao", "Changshi Li", "Chris Yuhao Liu", "Rui Yan", "Tianwen Wei", "Jujie He", "Xuchen Song", "Yang Liu", "Yahui Zhou"], "title": "Skywork-SWE: Unveiling Data Scaling Laws for Software Engineering in LLMs", "comment": null, "summary": "Software engineering (SWE) has recently emerged as a crucial testbed for\nnext-generation LLM agents, demanding inherent capabilities in two critical\ndimensions: sustained iterative problem-solving (e.g., >50 interaction rounds)\nand long-context dependency resolution (e.g., >32k tokens). However, the data\ncuration process in SWE remains notoriously time-consuming, as it heavily\nrelies on manual annotation for code file filtering and the setup of dedicated\nruntime environments to execute and validate unit tests. Consequently, most\nexisting datasets are limited to only a few thousand GitHub-sourced instances.\nTo this end, we propose an incremental, automated data-curation pipeline that\nsystematically scales both the volume and diversity of SWE datasets. Our\ndataset comprises 10,169 real-world Python task instances from 2,531 distinct\nGitHub repositories, each accompanied by a task specified in natural language\nand a dedicated runtime-environment image for automated unit-test validation.\nWe have carefully curated over 8,000 successfully runtime-validated training\ntrajectories from our proposed SWE dataset. When fine-tuning the Skywork-SWE\nmodel on these trajectories, we uncover a striking data scaling phenomenon: the\ntrained model's performance for software engineering capabilities in LLMs\ncontinues to improve as the data size increases, showing no signs of\nsaturation. Notably, our Skywork-SWE model achieves 38.0% pass@1 accuracy on\nthe SWE-bench Verified benchmark without using verifiers or multiple rollouts,\nestablishing a new state-of-the-art (SOTA) among the Qwen2.5-Coder-32B-based\nLLMs built on the OpenHands agent framework. Furthermore, with the\nincorporation of test-time scaling techniques, the performance further improves\nto 47.0% accuracy, surpassing the previous SOTA results for sub-32B parameter\nmodels. We release the Skywork-SWE-32B model checkpoint to accelerate future\nresearch."}
{"id": "2506.19563", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.19563", "abs": "https://arxiv.org/abs/2506.19563", "authors": ["Jinwen He", "Yiyang Lu", "Zijin Lin", "Kai Chen", "Yue Zhao"], "title": "PrivacyXray: Detecting Privacy Breaches in LLMs through Semantic Consistency and Probability Certainty", "comment": null, "summary": "Large Language Models (LLMs) are widely used in sensitive domains, including\nhealthcare, finance, and legal services, raising concerns about potential\nprivate information leaks during inference. Privacy extraction attacks, such as\njailbreaking, expose vulnerabilities in LLMs by crafting inputs that force the\nmodels to output sensitive information. However, these attacks cannot verify\nwhether the extracted private information is accurate, as no public datasets\nexist for cross-validation, leaving a critical gap in private information\ndetection during inference. To address this, we propose PrivacyXray, a novel\nframework detecting privacy breaches by analyzing LLM inner states. Our\nanalysis reveals that LLMs exhibit higher semantic coherence and probabilistic\ncertainty when generating correct private outputs. Based on this, PrivacyXray\ndetects privacy breaches using four metrics: intra-layer and inter-layer\nsemantic similarity, token-level and sentence-level probability distributions.\nPrivacyXray addresses critical challenges in private information detection by\novercoming the lack of open-source private datasets and eliminating reliance on\nexternal data for validation. It achieves this through the synthesis of\nrealistic private data and a detection mechanism based on the inner states of\nLLMs. Experiments show that PrivacyXray achieves consistent performance, with\nan average accuracy of 92.69% across five LLMs. Compared to state-of-the-art\nmethods, PrivacyXray achieves significant improvements, with an average\naccuracy increase of 20.06%, highlighting its stability and practical utility\nin real-world applications."}
{"id": "2506.19325", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2506.19325", "abs": "https://arxiv.org/abs/2506.19325", "authors": ["Hyein Seo", "Taewook Hwang", "Yohan Lee", "sangkeun Jung"], "title": "FEAT: A Preference Feedback Dataset through a Cost-Effective Auto-Generation and Labeling Framework for English AI Tutoring", "comment": "ACL 2025 (Short)", "summary": "In English education tutoring, teacher feedback is essential for guiding\nstudents. Recently, AI-based tutoring systems have emerged to assist teachers;\nhowever, these systems require high-quality and large-scale teacher feedback\ndata, which is both time-consuming and costly to generate manually. In this\nstudy, we propose FEAT, a cost-effective framework for generating teacher\nfeedback, and have constructed three complementary datasets: (1) DIRECT-Manual\n(DM), where both humans and large language models (LLMs) collaboratively\ngenerate high-quality teacher feedback, albeit at a higher cost; (2)\nDIRECT-Generated (DG), an LLM-only generated, cost-effective dataset with lower\nquality;, and (3) DIRECT-Augmented (DA), primarily based on DG with a small\nportion of DM added to enhance quality while maintaining cost-efficiency.\nExperimental results showed that incorporating a small portion of DM (5-10%)\ninto DG leads to superior performance compared to using 100% DM alone."}
{"id": "2506.19624", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2506.19624", "abs": "https://arxiv.org/abs/2506.19624", "authors": ["Isaac David", "Liyi Zhou", "Dawn Song", "Arthur Gervais", "Kaihua Qin"], "title": "Decompiling Smart Contracts with a Large Language Model", "comment": null, "summary": "The widespread lack of broad source code verification on blockchain explorers\nsuch as Etherscan, where despite 78,047,845 smart contracts deployed on\nEthereum (as of May 26, 2025), a mere 767,520 (< 1%) are open source, presents\na severe impediment to blockchain security. This opacity necessitates the\nautomated semantic analysis of on-chain smart contract bytecode, a fundamental\nresearch challenge with direct implications for identifying vulnerabilities and\nunderstanding malicious behavior. Prevailing decompilers struggle to reverse\nbytecode in a readable manner, often yielding convoluted code that critically\nhampers vulnerability analysis and thwarts efforts to dissect contract\nfunctionalities for security auditing.\n  This paper addresses this challenge by introducing a pioneering decompilation\npipeline that, for the first time, successfully leverages Large Language Models\n(LLMs) to transform Ethereum Virtual Machine (EVM) bytecode into human-readable\nand semantically faithful Solidity code. Our novel methodology first employs\nrigorous static program analysis to convert bytecode into a structured\nthree-address code (TAC) representation. This intermediate representation then\nguides a Llama-3.2-3B model, specifically fine-tuned on a comprehensive dataset\nof 238,446 TAC-to-Solidity function pairs, to generate high-quality Solidity.\nThis approach uniquely recovers meaningful variable names, intricate control\nflow, and precise function signatures. Our extensive empirical evaluation\ndemonstrates a significant leap beyond traditional decompilers, achieving an\naverage semantic similarity of 0.82 with original source and markedly superior\nreadability. The practical viability and effectiveness of our research are\ndemonstrated through its implementation in a publicly accessible system,\navailable at https://evmdecompiler.com."}
{"id": "2506.19359", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2506.19359", "abs": "https://arxiv.org/abs/2506.19359", "authors": ["Debosmita Bhaumik", "Julian Togelius", "Georgios N. Yannakakis", "Ahmed Khalifa"], "title": "Evolutionary Level Repair", "comment": null, "summary": "We address the problem of game level repair, which consists of taking a\ndesigned but non-functional game level and making it functional. This might\nconsist of ensuring the completeness of the level, reachability of objects, or\nother performance characteristics. The repair problem may also be constrained\nin that it can only make a small number of changes to the level. We investigate\nsearch-based solutions to the level repair problem, particularly using\nevolutionary and quality-diversity algorithms, with good results. This level\nrepair method is applied to levels generated using a machine learning-based\nprocedural content generation (PCGML) method that generates stylistically\nappropriate but frequently broken levels. This combination of PCGML for\ngeneration and search-based methods for repair shows great promise as a hybrid\nprocedural content generation (PCG) method."}
{"id": "2506.19676", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2506.19676", "abs": "https://arxiv.org/abs/2506.19676", "authors": ["Dezhang Kong", "Shi Lin", "Zhenhua Xu", "Zhebo Wang", "Minghao Li", "Yufeng Li", "Yilun Zhang", "Zeyang Sha", "Yuyuan Li", "Changting Lin", "Xun Wang", "Xuan Liu", "Muhammad Khurram Khan", "Ningyu Zhang", "Chaochao Chen", "Meng Han"], "title": "A Survey of LLM-Driven AI Agent Communication: Protocols, Security Risks, and Defense Countermeasures", "comment": null, "summary": "In recent years, Large-Language-Model-driven AI agents have exhibited\nunprecedented intelligence, flexibility, and adaptability, and are rapidly\nchanging human production and lifestyle. Nowadays, agents are undergoing a new\nround of evolution. They no longer act as an isolated island like LLMs.\nInstead, they start to communicate with diverse external entities, such as\nother agents and tools, to collectively perform more complex tasks. Under this\ntrend, agent communication is regarded as a foundational pillar of the future\nAI ecosystem, and many organizations intensively begin to design related\ncommunication protocols (e.g., Anthropic's MCP and Google's A2A) within the\nrecent few months. However, this new field exposes significant security hazard,\nwhich can cause severe damage to real-world scenarios. To help researchers to\nquickly figure out this promising topic and benefit the future agent\ncommunication development, this paper presents a comprehensive survey of agent\ncommunication security. More precisely, we first present a clear definition of\nagent communication and categorize the entire lifecyle of agent communication\ninto three stages: user-agent interaction, agent-agent communication, and\nagent-environment communication. Next, for each communication phase, we dissect\nrelated protocols and analyze its security risks according to the communication\ncharacteristics. Then, we summarize and outlook on the possible defense\ncountermeasures for each risk. Finally, we discuss open issues and future\ndirections in this promising research field."}
{"id": "2506.19385", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2506.19385", "abs": "https://arxiv.org/abs/2506.19385", "authors": ["Ziqi Zhu", "Tao Hu", "Honglong Zhang", "Dan Yang", "HanGeng Chen", "Mengran Zhang", "Xilun Chen"], "title": "Conversational Intent-Driven GraphRAG: Enhancing Multi-Turn Dialogue Systems through Adaptive Dual-Retrieval of Flow Patterns and Context Semantics", "comment": null, "summary": "We present CID-GraphRAG (Conversational Intent-Driven Graph Retrieval\nAugmented Generation), a novel framework that addresses the limitations of\nexisting dialogue systems in maintaining both contextual coherence and\ngoal-oriented progression in multi-turn customer service conversations. Unlike\ntraditional RAG systems that rely solely on semantic similarity (Conversation\nRAG) or standard knowledge graphs (GraphRAG), CID-GraphRAG constructs dynamic\nintent transition graphs from goal achieved historical dialogues and implements\na dual-retrieval mechanism that adaptively balances intent-based graph\ntraversal with semantic search. This approach enables the system to\nsimultaneously leverage both conversional intent flow patterns and contextual\nsemantics, significantly improving retrieval quality and response quality. In\nextensive experiments on real-world customer service dialogues, we employ both\nautomatic metrics and LLM-as-judge assessments, demonstrating that CID-GraphRAG\nsignificantly outperforms both semantic-based Conversation RAG and intent-based\nGraphRAG baselines across all evaluation criteria. Quantitatively, CID-GraphRAG\ndemonstrates substantial improvements over Conversation RAG across automatic\nmetrics, with relative gains of 11% in BLEU, 5% in ROUGE-L, 6% in METEOR, and\nmost notably, a 58% improvement in response quality according to LLM-as-judge\nevaluations. These results demonstrate that the integration of intent\ntransition structures with semantic retrieval creates a synergistic effect that\nneither approach achieves independently, establishing CID-GraphRAG as an\neffective framework for addressing the challenges of maintaining contextual\ncoherence and goal-oriented progression in knowledge-intensive multi-turn\ndialogues."}
{"id": "2506.19802", "categories": ["cs.CR", "cs.IR"], "pdf": "https://arxiv.org/pdf/2506.19802", "abs": "https://arxiv.org/abs/2506.19802", "authors": ["Xin Fan Guo", "Albert Merono Penuela", "Sergio Maffeis", "Fabio Pierazzi"], "title": "KnowML: Improving Generalization of ML-NIDS with Attack Knowledge Graphs", "comment": null, "summary": "Despite extensive research on Machine Learning-based Network Intrusion\nDetection Systems (ML-NIDS), their capability to detect diverse attack variants\nremains uncertain. Prior studies have largely relied on homogeneous datasets,\nwhich artificially inflate performance scores and offer a false sense of\nsecurity. Designing systems that can effectively detect a wide range of attack\nvariants remains a significant challenge. The progress of ML-NIDS continues to\ndepend heavily on human expertise, which can embed subjective judgments of\nsystem designers into the model, potentially hindering its ability to\ngeneralize across diverse attack types.\n  To address this gap, we propose KnowML, a framework for knowledge-guided\nmachine learning that integrates attack knowledge into ML-NIDS. KnowML\nsystematically explores the threat landscape by leveraging Large Language\nModels (LLMs) to perform automated analysis of attack implementations. It\nconstructs a unified Knowledge Graph (KG) of attack strategies, on which it\napplies symbolic reasoning to generate KG-Augmented Input, embedding domain\nknowledge directly into the design process of ML-NIDS.\n  We evaluate KnowML on 28 realistic attack variants, of which 10 are newly\ncollected for this study. Our findings reveal that baseline ML-NIDS models fail\nto detect several variants entirely, achieving F1 scores as low as 0 %. In\ncontrast, our knowledge-guided approach achieves up to 99 % F1 score while\nmaintaining a False Positive Rate below 0.1 %."}
{"id": "2506.19408", "categories": ["cs.AI", "cs.RO"], "pdf": "https://arxiv.org/pdf/2506.19408", "abs": "https://arxiv.org/abs/2506.19408", "authors": ["Alexandre Chapin", "Emmanuel Dellandrea", "Liming Chen"], "title": "Is an object-centric representation beneficial for robotic manipulation ?", "comment": null, "summary": "Object-centric representation (OCR) has recently become a subject of interest\nin the computer vision community for learning a structured representation of\nimages and videos. It has been several times presented as a potential way to\nimprove data-efficiency and generalization capabilities to learn an agent on\ndownstream tasks. However, most existing work only evaluates such models on\nscene decomposition, without any notion of reasoning over the learned\nrepresentation. Robotic manipulation tasks generally involve multi-object\nenvironments with potential inter-object interaction. We thus argue that they\nare a very interesting playground to really evaluate the potential of existing\nobject-centric work. To do so, we create several robotic manipulation tasks in\nsimulated environments involving multiple objects (several distractors, the\nrobot, etc.) and a high-level of randomization (object positions, colors,\nshapes, background, initial positions, etc.). We then evaluate one classical\nobject-centric method across several generalization scenarios and compare its\nresults against several state-of-the-art hollistic representations. Our results\nexhibit that existing methods are prone to failure in difficult scenarios\ninvolving complex scene structures, whereas object-centric methods help\novercome these challenges."}
{"id": "2506.19836", "categories": ["cs.CR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.19836", "abs": "https://arxiv.org/abs/2506.19836", "authors": ["Saeed Mahloujifar", "Chuan Guo", "G. Edward Suh", "Kamalika Chaudhuri"], "title": "Machine Learning with Privacy for Protected Attributes", "comment": null, "summary": "Differential privacy (DP) has become the standard for private data analysis.\nCertain machine learning applications only require privacy protection for\nspecific protected attributes. Using naive variants of differential privacy in\nsuch use cases can result in unnecessary degradation of utility. In this work,\nwe refine the definition of DP to create a more general and flexible framework\nthat we call feature differential privacy (FDP). Our definition is\nsimulation-based and allows for both addition/removal and replacement variants\nof privacy, and can handle arbitrary and adaptive separation of protected and\nnon-protected features. We prove the properties of FDP, such as adaptive\ncomposition, and demonstrate its implications for limiting attribute inference\nattacks. We also propose a modification of the standard DP-SGD algorithm that\nsatisfies FDP while leveraging desirable properties such as amplification via\nsub-sampling. We apply our framework to various machine learning tasks and show\nthat it can significantly improve the utility of DP-trained models when public\nfeatures are available. For example, we train diffusion models on the AFHQ\ndataset of animal faces and observe a drastic improvement in FID compared to\nDP, from 286.7 to 101.9 at $\\epsilon=8$, assuming that the blurred version of a\ntraining image is available as a public feature. Overall, our work provides a\nnew approach to private data analysis that can help reduce the utility cost of\nDP while still providing strong privacy guarantees."}
{"id": "2506.19410", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2506.19410", "abs": "https://arxiv.org/abs/2506.19410", "authors": ["Anas Hattay", "Mayara Ayat", "Fred Ngole Mboula"], "title": "Unsupervised Dataset Dictionary Learning for domain shift robust clustering: application to sitting posture identification", "comment": null, "summary": "This paper introduces a novel approach, Unsupervised Dataset Dictionary\nLearning (U-DaDiL), for totally unsupervised robust clustering applied to\nsitting posture identification. Traditional methods often lack adaptability to\ndiverse datasets and suffer from domain shift issues. U-DaDiL addresses these\nchallenges by aligning distributions from different datasets using Wasserstein\nbarycenter based representation. Experimental evaluations on the Office31\ndataset demonstrate significant improvements in cluster alignment accuracy.\nThis work also presents a promising step for addressing domain shift and robust\nclustering for unsupervised sitting posture identification"}
{"id": "2506.19420", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2506.19420", "abs": "https://arxiv.org/abs/2506.19420", "authors": ["Yazhou Zhang", "Chunwang Zou", "Bo Wang", "Jing Qin"], "title": "Commander-GPT: Dividing and Routing for Multimodal Sarcasm Detection", "comment": null, "summary": "Multimodal sarcasm understanding is a high-order cognitive task. Although\nlarge language models (LLMs) have shown impressive performance on many\ndownstream NLP tasks, growing evidence suggests that they struggle with sarcasm\nunderstanding. In this paper, we propose Commander-GPT, a modular decision\nrouting framework inspired by military command theory. Rather than relying on a\nsingle LLM's capability, Commander-GPT orchestrates a team of specialized LLM\nagents where each agent will be selectively assigned to a focused sub-task such\nas context modeling, sentiment analysis, etc. Their outputs are then routed\nback to the commander, which integrates the information and performs the final\nsarcasm judgment. To coordinate these agents, we introduce three types of\ncentralized commanders: (1) a trained lightweight encoder-based commander\n(e.g., multi-modal BERT); (2) four small autoregressive language models,\nserving as moderately capable commanders (e.g., DeepSeek-VL); (3) two large\nLLM-based commander (Gemini Pro and GPT-4o) that performs task routing, output\naggregation, and sarcasm decision-making in a zero-shot fashion. We evaluate\nCommander-GPT on the MMSD and MMSD 2.0 benchmarks, comparing five prompting\nstrategies. Experimental results show that our framework achieves 4.4% and\n11.7% improvement in F1 score over state-of-the-art (SoTA) baselines on\naverage, demonstrating its effectiveness."}
{"id": "2506.19466", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2506.19466", "abs": "https://arxiv.org/abs/2506.19466", "authors": ["Cheng Li", "Jiexiong Liu", "Yixuan Chen", "Qihang Zhou", "KunLun Meta"], "title": "KunLunBaizeRAG: Reinforcement Learning Driven Inference Performance Leap for Large Language Models", "comment": null, "summary": "This paper introduces KunLunBaizeRAG, a reinforcement learning-driven\nreasoning framework designed to enhance the reasoning capabilities of large\nlanguage models (LLMs) in complex multi-hop question-answering tasks. The\nframework addresses key limitations of traditional RAG, such as retrieval\ndrift, information redundancy, and strategy rigidity. Key innovations include\nthe RAG-driven Reasoning Alignment (RDRA) mechanism, the Search-Think Iterative\nEnhancement (STIE) mechanism, the Network-Local Intelligent Routing (NLR)\nmechanism, and a progressive hybrid training strategy. Experimental results\ndemonstrate significant improvements in exact match (EM) and LLM-judged score\n(LJ) across four benchmarks, highlighting the framework's robustness and\neffectiveness in complex reasoning scenarios."}
{"id": "2506.19500", "categories": ["cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.19500", "abs": "https://arxiv.org/abs/2506.19500", "authors": ["Yan Jiang", "Hao Zhou", "LiZhong GU", "Ai Han", "TianLong Li"], "title": "NaviAgent: Bilevel Planning on Tool Dependency Graphs for Function Calling", "comment": null, "summary": "LLMs' reliance on static knowledge and fragile tool invocation severely\nhinders the orchestration of complex, heterogeneous toolchains, particularly at\nlarge scales. Existing methods typically use rigid single-path execution,\nresulting in poor error recovery and exponentially growing search spaces. We\nintroduce NaviAgent, a graph-navigated bilevel planning architecture for robust\nfunction calling, comprising a Multi-Path Decider and Graph-Encoded Navigator.\nAs an LLM-powered agent, the Multi-Path Decider defines a four-dimensional\ndecision space and continuously perceives environmental states, dynamically\nselecting the optimal action to fully cover all tool invocation scenarios. The\nGraph-Encoded Navigator constructs a Tool Dependency Heterogeneous Graph\n(TDHG), where node embeddings explicitly fuse API schema structure with\nhistorical invocation behavior. It also integrates a novel heuristic search\nstrategy that guides the Decider toward efficient and highly successful\ntoolchains, even for unseen tool combinations. Experiments show that NaviAgent\nconsistently achieves the highest task success rate (TSR) across all foundation\nmodels and task complexities, outperforming the average baselines (ReAct,\nToolLLM, {\\alpha}-UMI) by 13.5%, 16.4%, and 19.0% on Qwen2.5-14B, Qwen2.5-32B,\nand Deepseek-V3, respectively. Its execution steps are typically within one\nstep of the most efficient baseline, ensuring a strong balance between quality\nand efficiency. Notably, a fine-tuned Qwen2.5-14B model achieves a TSR of\n49.5%, surpassing the much larger 32B model (44.9%) under our architecture.\nIncorporating the Graph-Encoded Navigator further boosts TSR by an average of\n2.4 points, with gains up over 9 points on complex tasks for larger models\n(Deepseek-V3 and GPT-4o), highlighting its essential role in toolchain\norchestration."}
{"id": "2506.19530", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2506.19530", "abs": "https://arxiv.org/abs/2506.19530", "authors": ["Carlo Romeo", "Andrew D. Bagdanov"], "title": "NTRL: Encounter Generation via Reinforcement Learning for Dynamic Difficulty Adjustment in Dungeons and Dragons", "comment": null, "summary": "Balancing combat encounters in Dungeons & Dragons (D&D) is a complex task\nthat requires Dungeon Masters (DM) to manually assess party strength, enemy\ncomposition, and dynamic player interactions while avoiding interruption of the\nnarrative flow. In this paper, we propose Encounter Generation via\nReinforcement Learning (NTRL), a novel approach that automates Dynamic\nDifficulty Adjustment (DDA) in D&D via combat encounter design. By framing the\nproblem as a contextual bandit, NTRL generates encounters based on real-time\nparty members attributes. In comparison with classic DM heuristics, NTRL\niteratively optimizes encounters to extend combat longevity (+200%), increases\ndamage dealt to party members, reducing post-combat hit points (-16.67%), and\nraises the number of player deaths while maintaining low total party kills\n(TPK). The intensification of combat forces players to act wisely and engage in\ntactical maneuvers, even though the generated encounters guarantee high win\nrates (70%). Even in comparison with encounters designed by human Dungeon\nMasters, NTRL demonstrates superior performance by enhancing the strategic\ndepth of combat while increasing difficulty in a manner that preserves overall\ngame fairness."}
{"id": "2506.19573", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2506.19573", "abs": "https://arxiv.org/abs/2506.19573", "authors": ["Sanne Wielinga", "Jesse Heyninck"], "title": "Interpretable Hybrid Machine Learning Models Using FOLD-R++ and Answer Set Programming", "comment": "accepted for publication as a Technical Communication at ICLP 2025", "summary": "Machine learning (ML) techniques play a pivotal role in high-stakes domains\nsuch as healthcare, where accurate predictions can greatly enhance\ndecision-making. However, most high-performing methods such as neural networks\nand ensemble methods are often opaque, limiting trust and broader adoption. In\nparallel, symbolic methods like Answer Set Programming (ASP) offer the\npossibility of interpretable logical rules but do not always match the\npredictive power of ML models. This paper proposes a hybrid approach that\nintegrates ASP-derived rules from the FOLD-R++ algorithm with black-box ML\nclassifiers to selectively correct uncertain predictions and provide\nhuman-readable explanations. Experiments on five medical datasets reveal\nstatistically significant performance gains in accuracy and F1 score. This\nstudy underscores the potential of combining symbolic reasoning with\nconventional ML to achieve high interpretability without sacrificing accuracy."}
{"id": "2506.19592", "categories": ["cs.AI", "cs.RO"], "pdf": "https://arxiv.org/pdf/2506.19592", "abs": "https://arxiv.org/abs/2506.19592", "authors": ["Harisankar Babu", "Philipp Schillinger", "Tamim Asfour"], "title": "Adaptive Domain Modeling with Language Models: A Multi-Agent Approach to Task Planning", "comment": null, "summary": "We introduce TAPAS (Task-based Adaptation and Planning using AgentS), a\nmulti-agent framework that integrates Large Language Models (LLMs) with\nsymbolic planning to solve complex tasks without the need for manually defined\nenvironment models. TAPAS employs specialized LLM-based agents that\ncollaboratively generate and adapt domain models, initial states, and goal\nspecifications as needed using structured tool-calling mechanisms. Through this\ntool-based interaction, downstream agents can request modifications from\nupstream agents, enabling adaptation to novel attributes and constraints\nwithout manual domain redefinition. A ReAct (Reason+Act)-style execution agent,\ncoupled with natural language plan translation, bridges the gap between\ndynamically generated plans and real-world robot capabilities. TAPAS\ndemonstrates strong performance in benchmark planning domains and in the\nVirtualHome simulated real-world environment."}
{"id": "2506.19608", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.19608", "abs": "https://arxiv.org/abs/2506.19608", "authors": ["Zhiyuan Wang", "Bokui Chen"], "title": "ChordPrompt: Orchestrating Cross-Modal Prompt Synergy for Multi-Domain Incremental Learning in CLIP", "comment": "Accept by ECML-PKDD 2025", "summary": "Continual learning (CL) empowers pre-trained vision-language models to adapt\neffectively to novel or previously underrepresented data distributions without\ncomprehensive retraining, enhancing their adaptability and efficiency. While\nvision-language models like CLIP show great promise, they struggle to maintain\nperformance across domains in incremental learning scenarios. Existing prompt\nlearning methods face two main limitations: 1) they primarily focus on\nclass-incremental learning scenarios, lacking specific strategies for\nmulti-domain task incremental learning; 2) most current approaches employ\nsingle-modal prompts, neglecting the potential benefits of cross-modal\ninformation exchange. To address these challenges, we propose the \\ChordPrompt\nframework, which facilitates a harmonious interplay between visual and textual\nprompts. \\ChordPrompt introduces cross-modal prompts to leverage interactions\nbetween visual and textual information. Our approach also employs\ndomain-adaptive text prompts to select appropriate prompts for continual\nadaptation across multiple domains. Comprehensive experiments on multi-domain\nincremental learning benchmarks demonstrate that \\ChordPrompt outperforms\nstate-of-the-art methods in zero-shot generalization and downstream task\nperformance."}
{"id": "2506.19613", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2506.19613", "abs": "https://arxiv.org/abs/2506.19613", "authors": ["Sha Zhang", "Suorong Yang", "Tong Xie", "Xiangyuan Xue", "Zixuan Hu", "Rui Li", "Wenxi Qu", "Zhenfei Yin", "Tianfan Fu", "Di Hu", "Andres M Bran", "Nian Ran", "Bram Hoex", "Wangmeng Zuo", "Philippe Schwaller", "Wanli Ouyang", "Lei Bai", "Yanyong Zhang", "Lingyu Duan", "Shixiang Tang", "Dongzhan Zhou"], "title": "Position: Intelligent Science Laboratory Requires the Integration of Cognitive and Embodied AI", "comment": null, "summary": "Scientific discovery has long been constrained by human limitations in\nexpertise, physical capability, and sleep cycles. The recent rise of AI\nscientists and automated laboratories has accelerated both the cognitive and\noperational aspects of research. However, key limitations persist: AI systems\nare often confined to virtual environments, while automated laboratories lack\nthe flexibility and autonomy to adaptively test new hypotheses in the physical\nworld. Recent advances in embodied AI, such as generalist robot foundation\nmodels, diffusion-based action policies, fine-grained manipulation learning,\nand sim-to-real transfer, highlight the promise of integrating cognitive and\nembodied intelligence. This convergence opens the door to closed-loop systems\nthat support iterative, autonomous experimentation and the possibility of\nserendipitous discovery. In this position paper, we propose the paradigm of\nIntelligent Science Laboratories (ISLs): a multi-layered, closed-loop framework\nthat deeply integrates cognitive and embodied intelligence. ISLs unify\nfoundation models for scientific reasoning, agent-based workflow orchestration,\nand embodied agents for robust physical experimentation. We argue that such\nsystems are essential for overcoming the current limitations of scientific\ndiscovery and for realizing the full transformative potential of AI-driven\nscience."}
{"id": "2506.19635", "categories": ["cs.CR", "cs.AI", "cs.SI"], "pdf": "https://arxiv.org/pdf/2506.19635", "abs": "https://arxiv.org/abs/2506.19635", "authors": ["Rocco De Nicola", "Marinella Petrocchi", "Manuel Pratelli"], "title": "On the efficacy of old features for the detection of new bots", "comment": "pre-print version", "summary": "For more than a decade now, academicians and online platform administrators\nhave been studying solutions to the problem of bot detection. Bots are computer\nalgorithms whose use is far from being benign: malicious bots are purposely\ncreated to distribute spam, sponsor public characters and, ultimately, induce a\nbias within the public opinion. To fight the bot invasion on our online\necosystem, several approaches have been implemented, mostly based on\n(supervised and unsupervised) classifiers, which adopt the most varied account\nfeatures, from the simplest to the most expensive ones to be extracted from the\nraw data obtainable through the Twitter public APIs. In this exploratory study,\nusing Twitter as a benchmark, we compare the performances of four state-of-art\nfeature sets in detecting novel bots: one of the output scores of the popular\nbot detector Botometer, which considers more than 1,000 features of an account\nto take a decision; two feature sets based on the account profile and timeline;\nand the information about the Twitter client from which the user tweets. The\nresults of our analysis, conducted on six recently released datasets of Twitter\naccounts, hint at the possible use of general-purpose classifiers and\ncheap-to-compute account features for the detection of evolved bots."}
{"id": "2506.19650", "categories": ["cs.AI", "stat.ME"], "pdf": "https://arxiv.org/pdf/2506.19650", "abs": "https://arxiv.org/abs/2506.19650", "authors": ["Simon Ferreira", "Charles K. Assaad"], "title": "Identifying Macro Causal Effects in C-DMGs over DMGs", "comment": "Accepted to the UAI2025 workshop on Causal Abstractions and\n  Representations. arXiv admin note: substantial text overlap with\n  arXiv:2504.01551", "summary": "The do-calculus is a sound and complete tool for identifying causal effects\nin acyclic directed mixed graphs (ADMGs) induced by structural causal models\n(SCMs). However, in many real-world applications, especially in\nhigh-dimensional setting, constructing a fully specified ADMG is often\ninfeasible. This limitation has led to growing interest in partially specified\ncausal representations, particularly through cluster-directed mixed graphs\n(C-DMGs), which group variables into clusters and offer a more abstract yet\npractical view of causal dependencies. While these representations can include\ncycles, recent work has shown that the do-calculus remains sound and complete\nfor identifying macro-level causal effects in C-DMGs over ADMGs under the\nassumption that all clusters size are greater than 1. Nevertheless, real-world\nsystems often exhibit cyclic causal dynamics at the structural level. To\naccount for this, input-output structural causal models (ioSCMs) have been\nintroduced as a generalization of SCMs that allow for cycles. ioSCMs induce\nanother type of graph structure known as a directed mixed graph (DMG).\nAnalogous to the ADMG setting, one can define C-DMGs over DMGs as high-level\nrepresentations of causal relations among clusters of variables. In this paper,\nwe prove that, unlike in the ADMG setting, the do-calculus is unconditionally\nsound and complete for identifying macro causal effects in C-DMGs over DMGs.\nFurthermore, we show that the graphical criteria for non-identifiability of\nmacro causal effects previously established C-DMGs over ADMGs naturally extends\nto a subset of C-DMGs over DMGs."}
{"id": "2506.19686", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2506.19686", "abs": "https://arxiv.org/abs/2506.19686", "authors": ["Ching Fang", "Kanaka Rajan"], "title": "From memories to maps: Mechanisms of in context reinforcement learning in transformers", "comment": null, "summary": "Humans and animals show remarkable learning efficiency, adapting to new\nenvironments with minimal experience. This capability is not well captured by\nstandard reinforcement learning algorithms that rely on incremental value\nupdates. Rapid adaptation likely depends on episodic memory -- the ability to\nretrieve specific past experiences to guide decisions in novel contexts.\nTransformers provide a useful setting for studying these questions because of\ntheir ability to learn rapidly in-context and because their key-value\narchitecture resembles episodic memory systems in the brain. We train a\ntransformer to in-context reinforcement learn in a distribution of planning\ntasks inspired by rodent behavior. We then characterize the learning algorithms\nthat emerge in the model. We first find that representation learning is\nsupported by in-context structure learning and cross-context alignment, where\nrepresentations are aligned across environments with different sensory stimuli.\nWe next demonstrate that the reinforcement learning strategies developed by the\nmodel are not interpretable as standard model-free or model-based planning.\nInstead, we show that in-context reinforcement learning is supported by caching\nintermediate computations within the model's memory tokens, which are then\naccessed at decision time. Overall, we find that memory may serve as a\ncomputational resource, storing both raw experience and cached computations to\nsupport flexible behavior. Furthermore, the representations developed in the\nmodel resemble computations associated with the hippocampal-entorhinal system\nin the brain, suggesting that our findings may be relevant for natural\ncognition. Taken together, our work offers a mechanistic hypothesis for the\nrapid adaptation that underlies in-context learning in artificial and natural\nsettings."}
{"id": "2506.19698", "categories": ["cs.AI", "math.OC"], "pdf": "https://arxiv.org/pdf/2506.19698", "abs": "https://arxiv.org/abs/2506.19698", "authors": ["Zhuojun Xie", "Adam Abdin", "Yiping Fang"], "title": "Toward Decision-Oriented Prognostics: An Integrated Estimate-Optimize Framework for Predictive Maintenance", "comment": "22 pages, 5 figures, 4 tables", "summary": "Recent research increasingly integrates machine learning (ML) into predictive\nmaintenance (PdM) to reduce operational and maintenance costs in data-rich\noperational settings. However, uncertainty due to model misspecification\ncontinues to limit widespread industrial adoption. This paper proposes a PdM\nframework in which sensor-driven prognostics inform decision-making under\neconomic trade-offs within a finite decision space. We investigate two key\nquestions: (1) Does higher predictive accuracy necessarily lead to better\nmaintenance decisions? (2) If not, how can the impact of prediction errors on\ndownstream maintenance decisions be mitigated? We first demonstrate that in the\ntraditional estimate-then-optimize (ETO) framework, errors in probabilistic\nprediction can result in inconsistent and suboptimal maintenance decisions. To\naddress this, we propose an integrated estimate-optimize (IEO) framework that\njointly tunes predictive models while directly optimizing for maintenance\noutcomes. We establish theoretical finite-sample guarantees on decision\nconsistency under standard assumptions. Specifically, we develop a stochastic\nperturbation gradient descent algorithm suitable for small run-to-failure\ndatasets. Empirical evaluations on a turbofan maintenance case study show that\nthe IEO framework reduces average maintenance regret up to 22% compared to ETO.\nThis study provides a principled approach to managing prediction errors in\ndata-driven PdM. By aligning prognostic model training with maintenance\nobjectives, the IEO framework improves robustness under model misspecification\nand improves decision quality. The improvement is particularly pronounced when\nthe decision-making policy is misaligned with the decision-maker's target.\nThese findings support more reliable maintenance planning in uncertain\noperational environments."}
{"id": "2506.19702", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2506.19702", "abs": "https://arxiv.org/abs/2506.19702", "authors": ["Lei Kang", "Xuanshuo Fu", "Oriol Ramos Terrades", "Javier Vazquez-Corral", "Ernest Valveny", "Dimosthenis Karatzas"], "title": "LLM-Driven Medical Document Analysis: Enhancing Trustworthy Pathology and Differential Diagnosis", "comment": "Accepted at ICDAR 2025", "summary": "Medical document analysis plays a crucial role in extracting essential\nclinical insights from unstructured healthcare records, supporting critical\ntasks such as differential diagnosis. Determining the most probable condition\namong overlapping symptoms requires precise evaluation and deep medical\nexpertise. While recent advancements in large language models (LLMs) have\nsignificantly enhanced performance in medical document analysis, privacy\nconcerns related to sensitive patient data limit the use of online LLMs\nservices in clinical settings. To address these challenges, we propose a\ntrustworthy medical document analysis platform that fine-tunes a LLaMA-v3 using\nlow-rank adaptation, specifically optimized for differential diagnosis tasks.\nOur approach utilizes DDXPlus, the largest benchmark dataset for differential\ndiagnosis, and demonstrates superior performance in pathology prediction and\nvariable-length differential diagnosis compared to existing methods. The\ndeveloped web-based platform allows users to submit their own unstructured\nmedical documents and receive accurate, explainable diagnostic results. By\nincorporating advanced explainability techniques, the system ensures\ntransparent and reliable predictions, fostering user trust and confidence.\nExtensive evaluations confirm that the proposed method surpasses current\nstate-of-the-art models in predictive accuracy while offering practical utility\nin clinical settings. This work addresses the urgent need for reliable,\nexplainable, and privacy-preserving artificial intelligence solutions,\nrepresenting a significant advancement in intelligent medical document analysis\nfor real-world healthcare applications. The code can be found at\n\\href{https://github.com/leitro/Differential-Diagnosis-LoRA}{https://github.com/leitro/Differential-Diagnosis-LoRA}."}
{"id": "2506.19724", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2506.19724", "abs": "https://arxiv.org/abs/2506.19724", "authors": ["Gyeongwon James Kim", "Alex Wilf", "Louis-Philippe Morency", "Daniel Fried"], "title": "From Reproduction to Replication: Evaluating Research Agents with Progressive Code Masking", "comment": null, "summary": "Recent progress in autonomous code generation has fueled excitement around AI\nagents capable of accelerating scientific discovery by running experiments.\nHowever, there is currently no benchmark that evaluates whether such agents can\nimplement scientific ideas when given varied amounts of code as a starting\npoint, interpolating between reproduction (running code) and from-scratch\nreplication (fully re-implementing and running code). We introduce\nAutoExperiment, a benchmark that evaluates AI agents' ability to implement and\nrun machine learning experiments based on natural language descriptions in\nresearch papers. In each task, agents are given a research paper, a codebase\nwith key functions masked out, and a command to run the experiment. The goal is\nto generate the missing code, execute the experiment in a sandboxed\nenvironment, and reproduce the results. AutoExperiment scales in difficulty by\nvarying the number of missing functions $n$, ranging from partial reproduction\nto full replication. We evaluate state-of-the-art agents and find that\nperformance degrades rapidly as $n$ increases. Agents that can dynamically\ninteract with the environment (e.g. to debug their code) can outperform agents\nin fixed \"agentless\" harnesses, and there exists a significant gap between\nsingle-shot and multi-trial success rates (Pass@1 vs. Pass@5), motivating\nverifier approaches to our benchmark. Our findings highlight critical\nchallenges in long-horizon code generation, context retrieval, and autonomous\nexperiment execution, establishing AutoExperiment as a new benchmark for\nevaluating progress in AI-driven scientific experimentation. Our data and code\nare open-sourced at https://github.com/j1mk1m/AutoExperiment ."}
{"id": "2506.19773", "categories": ["cs.AI", "I.2.7; I.2.4"], "pdf": "https://arxiv.org/pdf/2506.19773", "abs": "https://arxiv.org/abs/2506.19773", "authors": ["Nandana Mihindukulasooriya", "Niharika S. D'Souza", "Faisal Chowdhury", "Horst Samulowitz"], "title": "Automatic Prompt Optimization for Knowledge Graph Construction: Insights from an Empirical Study", "comment": null, "summary": "A KG represents a network of entities and illustrates relationships between\nthem. KGs are used for various applications, including semantic search and\ndiscovery, reasoning, decision-making, natural language processing, machine\nlearning, and recommendation systems. Triple (subject-relation-object)\nextraction from text is the fundamental building block of KG construction and\nhas been widely studied, for example, in early benchmarks such as ACE 2002 to\nmore recent ones, such as WebNLG 2020, REBEL and SynthIE. While the use of LLMs\nis explored for KG construction, handcrafting reasonable task-specific prompts\nfor LLMs is a labour-intensive exercise and can be brittle due to subtle\nchanges in the LLM models employed. Recent work in NLP tasks (e.g. autonomy\ngeneration) uses automatic prompt optimization/engineering to address this\nchallenge by generating optimal or near-optimal task-specific prompts given\ninput-output examples.\n  This empirical study explores the application of automatic prompt\noptimization for the triple extraction task using experimental benchmarking. We\nevaluate different settings by changing (a) the prompting strategy, (b) the LLM\nbeing used for prompt optimization and task execution, (c) the number of\ncanonical relations in the schema (schema complexity), (d) the length and\ndiversity of input text, (e) the metric used to drive the prompt optimization,\nand (f) the dataset being used for training and testing. We evaluate three\ndifferent automatic prompt optimizers, namely, DSPy, APE, and TextGrad and use\ntwo different triple extraction datasets, SynthIE and REBEL. Through rigorous\nempirical evaluation, our main contribution highlights that automatic prompt\noptimization techniques can generate reasonable prompts similar to humans for\ntriple extraction. In turn, these optimized prompts achieve improved results,\nparticularly with increasing schema complexity and text size."}
{"id": "2506.19783", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2506.19783", "abs": "https://arxiv.org/abs/2506.19783", "authors": ["Teng Wang", "Hailei Gong", "Changwang Zhang", "Jun Wang"], "title": "SAGE: Strategy-Adaptive Generation Engine for Query Rewriting", "comment": null, "summary": "Query rewriting is pivotal for enhancing dense retrieval, yet current methods\ndemand large-scale supervised data or suffer from inefficient reinforcement\nlearning (RL) exploration. In this work, we first establish that guiding Large\nLanguage Models (LLMs) with a concise set of expert-crafted strategies, such as\nsemantic expansion and entity disambiguation, substantially improves retrieval\neffectiveness on challenging benchmarks, including HotpotQA, FEVER, NFCorpus,\nand SciFact. Building on this insight, we introduce the Strategy-Adaptive\nGeneration Engine (SAGE), which operationalizes these strategies in an RL\nframework. SAGE introduces two novel reward shaping mechanisms-Strategic Credit\nShaping (SCS) and Contrastive Reward Shaping (CRS)-to deliver more informative\nlearning signals. This strategy-guided approach not only achieves new\nstate-of-the-art NDCG@10 results, but also uncovers a compelling emergent\nbehavior: the agent learns to select optimal strategies, reduces unnecessary\nexploration, and generates concise rewrites, lowering inference cost without\nsacrificing performance. Our findings demonstrate that strategy-guided RL,\nenhanced with nuanced reward shaping, offers a scalable, efficient, and more\ninterpretable paradigm for developing the next generation of robust information\nretrieval systems."}
{"id": "2506.19785", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2506.19785", "abs": "https://arxiv.org/abs/2506.19785", "authors": ["Menglong Zhang", "Fuyuan Qian"], "title": "Learning Task Belief Similarity with Latent Dynamics for Meta-Reinforcement Learning", "comment": "ICLR2025 https://openreview.net/forum?id=5YbuOTUFQ4", "summary": "Meta-reinforcement learning requires utilizing prior task distribution\ninformation obtained during exploration to rapidly adapt to unknown tasks. The\nefficiency of an agent's exploration hinges on accurately identifying the\ncurrent task. Recent Bayes-Adaptive Deep RL approaches often rely on\nreconstructing the environment's reward signal, which is challenging in sparse\nreward settings, leading to suboptimal exploitation. Inspired by bisimulation\nmetrics, which robustly extracts behavioral similarity in continuous MDPs, we\npropose SimBelief-a novel meta-RL framework via measuring similarity of task\nbelief in Bayes-Adaptive MDP (BAMDP). SimBelief effectively extracts common\nfeatures of similar task distributions, enabling efficient task identification\nand exploration in sparse reward environments. We introduce latent task belief\nmetric to learn the common structure of similar tasks and incorporate it into\nthe specific task belief. By learning the latent dynamics across task\ndistributions, we connect shared latent task belief features with specific task\nfeatures, facilitating rapid task identification and adaptation. Our method\noutperforms state-of-the-art baselines on sparse reward MuJoCo and panda-gym\ntasks."}
{"id": "2506.19807", "categories": ["cs.AI", "cs.CL", "cs.CV", "cs.LG", "cs.MA"], "pdf": "https://arxiv.org/pdf/2506.19807", "abs": "https://arxiv.org/abs/2506.19807", "authors": ["Baochang Ren", "Shuofei Qiao", "Wenhao Yu", "Huajun Chen", "Ningyu Zhang"], "title": "KnowRL: Exploring Knowledgeable Reinforcement Learning for Factuality", "comment": "Work in progress", "summary": "Large Language Models (LLMs), particularly slow-thinking models, often\nexhibit severe hallucination, outputting incorrect content due to an inability\nto accurately recognize knowledge boundaries during reasoning. While\nReinforcement Learning (RL) can enhance complex reasoning abilities, its\noutcome-oriented reward mechanism often lacks factual supervision over the\nthinking process, further exacerbating the hallucination problem. To address\nthe high hallucination in slow-thinking models, we propose Knowledge-enhanced\nRL, KnowRL. KnowRL guides models to perform fact-based slow thinking by\nintegrating a factuality reward, based on knowledge verification, into the RL\ntraining process, helping them recognize their knowledge boundaries. KnowRL\nguides models to perform fact-based slow thinking by integrating a factuality\nreward, based on knowledge verification, into the RL training process, helping\nthem recognize their knowledge boundaries. This targeted factual input during\nRL training enables the model to learn and internalize fact-based reasoning\nstrategies. By directly rewarding adherence to facts within the reasoning\nsteps, KnowRL fosters a more reliable thinking process. Experimental results on\nthree hallucination evaluation datasets and two reasoning evaluation datasets\ndemonstrate that KnowRL effectively mitigates hallucinations in slow-thinking\nmodels while maintaining their original strong reasoning capabilities. Our code\nis available at https://github.com/zjunlp/KnowRL."}
{"id": "2506.19825", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2506.19825", "abs": "https://arxiv.org/abs/2506.19825", "authors": ["Johannes Rückert", "Louise Bloch", "Christoph M. Friedrich"], "title": "Evaluating Compliance with Visualization Guidelines in Diagrams for Scientific Publications Using Large Vision Language Models", "comment": "Accepted at ICDAR 2025", "summary": "Diagrams are widely used to visualize data in publications. The research\nfield of data visualization deals with defining principles and guidelines for\nthe creation and use of these diagrams, which are often not known or adhered to\nby researchers, leading to misinformation caused by providing inaccurate or\nincomplete information.\n  In this work, large Vision Language Models (VLMs) are used to analyze\ndiagrams in order to identify potential problems in regards to selected data\nvisualization principles and guidelines. To determine the suitability of VLMs\nfor these tasks, five open source VLMs and five prompting strategies are\ncompared using a set of questions derived from selected data visualization\nguidelines.\n  The results show that the employed VLMs work well to accurately analyze\ndiagram types (F1-score 82.49 %), 3D effects (F1-score 98.55 %), axes labels\n(F1-score 76.74 %), lines (RMSE 1.16), colors (RMSE 1.60) and legends (F1-score\n96.64 %, RMSE 0.70), while they cannot reliably provide feedback about the\nimage quality (F1-score 0.74 %) and tick marks/labels (F1-score 46.13 %). Among\nthe employed VLMs, Qwen2.5VL performs best, and the summarizing prompting\nstrategy performs best for most of the experimental questions.\n  It is shown that VLMs can be used to automatically identify a number of\npotential issues in diagrams, such as missing axes labels, missing legends, and\nunnecessary 3D effects. The approach laid out in this work can be extended for\nfurther aspects of data visualization."}
{"id": "2506.19843", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2506.19843", "abs": "https://arxiv.org/abs/2506.19843", "authors": ["Guo Li", "Zixiang Xu", "Wei Zhang", "Yikuan Hu", "Xinyu Yang", "Nikolay Aristov", "Mingjie Tang", "Elenna R Dugundji"], "title": "Temporal-IRL: Modeling Port Congestion and Berth Scheduling with Inverse Reinforcement Learning", "comment": "TRB2025", "summary": "Predicting port congestion is crucial for maintaining reliable global supply\nchains. Accurate forecasts enableimprovedshipment planning, reducedelaysand\ncosts, and optimizeinventoryanddistributionstrategies, thereby ensuring timely\ndeliveries and enhancing supply chain resilience. To achieve accurate\npredictions, analyzing vessel behavior and their stay times at specific port\nterminals is essential, focusing particularly on berth scheduling under various\nconditions. Crucially, the model must capture and learn the underlying\npriorities and patterns of berth scheduling. Berth scheduling and planning are\ninfluenced by a range of factors, including incoming vessel size, waiting\ntimes, and the status of vessels within the port terminal. By observing\nhistorical Automatic Identification System (AIS) positions of vessels, we\nreconstruct berth schedules, which are subsequently utilized to determine the\nreward function via Inverse Reinforcement Learning (IRL). For this purpose, we\nmodeled a specific terminal at the Port of New York/New Jersey and developed\nTemporal-IRL. This Temporal-IRL model learns berth scheduling to predict vessel\nsequencing at the terminal and estimate vessel port stay, encompassing both\nwaiting and berthing times, to forecast port congestion. Utilizing data from\nMaher Terminal spanning January 2015 to September 2023, we trained and tested\nthe model, achieving demonstrably excellent results."}
{"id": "2506.19846", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2506.19846", "abs": "https://arxiv.org/abs/2506.19846", "authors": ["Ai Han", "Junxing Hu", "Pu Wei", "Zhiqian Zhang", "Yuhang Guo", "Jiawei Lu", "Zicheng Zhang"], "title": "JoyAgents-R1: Joint Evolution Dynamics for Versatile Multi-LLM Agents with Reinforcement Learning", "comment": "33 pages, 7 figures, under review", "summary": "Multi-agent reinforcement learning (MARL) has emerged as a prominent paradigm\nfor increasingly complex tasks. However, joint evolution across heterogeneous\nagents remains challenging due to cooperative inefficiency and training\ninstability. In this paper, we propose the joint evolution dynamics for MARL\ncalled JoyAgents-R1, which first applies Group Relative Policy Optimization\n(GRPO) to the joint training of heterogeneous multi-agents. By iteratively\nrefining agents' large language models (LLMs) and memories, the method achieves\nholistic equilibrium with optimal decision-making and memory capabilities.\nSpecifically, JoyAgents-R1 first implements node-wise Monte Carlo sampling on\nthe behavior of each agent across entire reasoning trajectories to enhance GRPO\nsampling efficiency while maintaining policy diversity. Then, our marginal\nbenefit-driven selection strategy identifies top-$K$ sampling groups with\nmaximal reward fluctuations, enabling targeted agent model updates that improve\ntraining stability and maximize joint benefits through cost-effective parameter\nadjustments. Meanwhile, JoyAgents-R1 introduces an adaptive memory evolution\nmechanism that repurposes GRPO rewards as cost-free supervisory signals to\neliminate repetitive reasoning and accelerate convergence. Experiments across\ngeneral and domain-specific scenarios demonstrate that JoyAgents-R1 achieves\nperformance comparable to that of larger LLMs while built on smaller\nopen-source models."}
{"id": "2506.17336", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.17336", "abs": "https://arxiv.org/abs/2506.17336", "authors": ["Yubeen Bae", "Minchan Kim", "Jaejin Lee", "Sangbum Kim", "Jaehyung Kim", "Yejin Choi", "Niloofar Mireshghallah"], "title": "Privacy-Preserving LLM Interaction with Socratic Chain-of-Thought Reasoning and Homomorphically Encrypted Vector Databases", "comment": "29 pages", "summary": "Large language models (LLMs) are increasingly used as personal agents,\naccessing sensitive user data such as calendars, emails, and medical records.\nUsers currently face a trade-off: They can send private records, many of which\nare stored in remote databases, to powerful but untrusted LLM providers,\nincreasing their exposure risk. Alternatively, they can run less powerful\nmodels locally on trusted devices. We bridge this gap. Our Socratic\nChain-of-Thought Reasoning first sends a generic, non-private user query to a\npowerful, untrusted LLM, which generates a Chain-of-Thought (CoT) prompt and\ndetailed sub-queries without accessing user data. Next, we embed these\nsub-queries and perform encrypted sub-second semantic search using our\nHomomorphically Encrypted Vector Database across one million entries of a\nsingle user's private data. This represents a realistic scale of personal\ndocuments, emails, and records accumulated over years of digital activity.\nFinally, we feed the CoT prompt and the decrypted records to a local language\nmodel and generate the final response. On the LoCoMo long-context QA benchmark,\nour hybrid framework, combining GPT-4o with a local Llama-3.2-1B model,\noutperforms using GPT-4o alone by up to 7.1 percentage points. This\ndemonstrates a first step toward systems where tasks are decomposed and split\nbetween untrusted strong LLMs and weak local ones, preserving user privacy."}
{"id": "2506.19109", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.19109", "abs": "https://arxiv.org/abs/2506.19109", "authors": ["Valerii Gakh", "Hayretdin Bahsi"], "title": "Enhancing Security in LLM Applications: A Performance Evaluation of Early Detection Systems", "comment": "18 pages, 8 tables, 7 figures", "summary": "Prompt injection threatens novel applications that emerge from adapting LLMs\nfor various user tasks. The newly developed LLM-based software applications\nbecome more ubiquitous and diverse. However, the threat of prompt injection\nattacks undermines the security of these systems as the mitigation and defenses\nagainst them, proposed so far, are insufficient. We investigated the\ncapabilities of early prompt injection detection systems, focusing specifically\non the detection performance of techniques implemented in various open-source\nsolutions. These solutions are supposed to detect certain types of prompt\ninjection attacks, including the prompt leak. In prompt leakage attacks, an\nattacker maliciously manipulates the LLM into outputting its system\ninstructions, violating the system's confidentiality. Our study presents\nanalyzes of distinct prompt leakage detection techniques, and a comparative\nanalysis of several detection solutions, which implement those techniques. We\nidentify the strengths and weaknesses of these techniques and elaborate on\ntheir optimal configuration and usage in high-stake deployments. In one of the\nfirst studies on existing prompt leak detection solutions, we compared the\nperformances of LLM Guard, Vigil, and Rebuff. We concluded that the\nimplementations of canary word checks in Vigil and Rebuff were not effective at\ndetecting prompt leak attacks, and we proposed improvements for them. We also\nfound an evasion weakness in Rebuff's secondary model-based technique and\nproposed a mitigation. Then, the result of the comparison of LLM Guard, Vigil,\nand Rebuff at their peak performance revealed that Vigil is optimal for cases\nwhen minimal false positive rate is required, and Rebuff is the most optimal\nfor average needs."}
{"id": "2506.19539", "categories": ["cs.SE", "cs.AI", "D.2.7"], "pdf": "https://arxiv.org/pdf/2506.19539", "abs": "https://arxiv.org/abs/2506.19539", "authors": ["Julian Fragner", "Christian Macho", "Bernhard Dieber", "Martin Pinzger"], "title": "Lost in Translation? Converting RegExes for Log Parsing into Dynatrace Pattern Language", "comment": "18 pages, 7 tables, 18 figures", "summary": "Log files provide valuable information for detecting and diagnosing problems\nin enterprise software applications and data centers. Several log analytics\ntools and platforms were developed to help filter and extract information from\nlogs, typically using regular expressions (RegExes). Recent commercial log\nanalytics platforms provide domain-specific languages specifically designed for\nlog parsing, such as Grok or the Dynatrace Pattern Language (DPL). However,\nusers who want to migrate to these platforms must manually convert their\nRegExes into the new pattern language, which is costly and error-prone. In this\nwork, we present Reptile, which combines a rule-based approach for converting\nRegExes into DPL patterns with a best-effort approach for cases where a full\nconversion is impossible. Furthermore, it integrates GPT-4 to optimize the\nobtained DPL patterns. The evaluation with 946 RegExes collected from a large\ncompany shows that Reptile safely converted 73.7% of them. The evaluation of\nReptile's pattern optimization with 23 real-world RegExes showed an F1-score\nand MCC above 0.91. These results are promising and have ample practical\nimplications for companies that migrate to a modern log analytics platform,\nsuch as Dynatrace."}
{"id": "2506.19563", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.19563", "abs": "https://arxiv.org/abs/2506.19563", "authors": ["Jinwen He", "Yiyang Lu", "Zijin Lin", "Kai Chen", "Yue Zhao"], "title": "PrivacyXray: Detecting Privacy Breaches in LLMs through Semantic Consistency and Probability Certainty", "comment": null, "summary": "Large Language Models (LLMs) are widely used in sensitive domains, including\nhealthcare, finance, and legal services, raising concerns about potential\nprivate information leaks during inference. Privacy extraction attacks, such as\njailbreaking, expose vulnerabilities in LLMs by crafting inputs that force the\nmodels to output sensitive information. However, these attacks cannot verify\nwhether the extracted private information is accurate, as no public datasets\nexist for cross-validation, leaving a critical gap in private information\ndetection during inference. To address this, we propose PrivacyXray, a novel\nframework detecting privacy breaches by analyzing LLM inner states. Our\nanalysis reveals that LLMs exhibit higher semantic coherence and probabilistic\ncertainty when generating correct private outputs. Based on this, PrivacyXray\ndetects privacy breaches using four metrics: intra-layer and inter-layer\nsemantic similarity, token-level and sentence-level probability distributions.\nPrivacyXray addresses critical challenges in private information detection by\novercoming the lack of open-source private datasets and eliminating reliance on\nexternal data for validation. It achieves this through the synthesis of\nrealistic private data and a detection mechanism based on the inner states of\nLLMs. Experiments show that PrivacyXray achieves consistent performance, with\nan average accuracy of 92.69% across five LLMs. Compared to state-of-the-art\nmethods, PrivacyXray achieves significant improvements, with an average\naccuracy increase of 20.06%, highlighting its stability and practical utility\nin real-world applications."}
