{"id": "2506.12078", "categories": ["cs.MA", "cs.AI", "cs.CL", "cs.CY", "cs.SI"], "pdf": "https://arxiv.org/pdf/2506.12078", "abs": "https://arxiv.org/abs/2506.12078", "authors": ["Haoxiang Guan", "Jiyan He", "Liyang Fan", "Zhenzhen Ren", "Shaobin He", "Xin Yu", "Yuan Chen", "Shuxin Zheng", "Tie-Yan Liu", "Zhen Liu"], "title": "Modeling Earth-Scale Human-Like Societies with One Billion Agents", "comment": "Work in progress", "summary": "Understanding how complex societal behaviors emerge from individual cognition\nand interactions requires both high-fidelity modeling of human behavior and\nlarge-scale simulations. Traditional agent-based models (ABMs) have been\nemployed to study these dynamics for decades, but are constrained by simplified\nagent behaviors that fail to capture human complexity. Recent advances in large\nlanguage models (LLMs) offer new opportunities by enabling agents to exhibit\nsophisticated social behaviors that go beyond rule-based logic, yet face\nsignificant scaling challenges. Here we present Light Society, an agent-based\nsimulation framework that advances both fronts, efficiently modeling human-like\nsocieties at planetary scale powered by LLMs. Light Society formalizes social\nprocesses as structured transitions of agent and environment states, governed\nby a set of LLM-powered simulation operations, and executed through an event\nqueue. This modular design supports both independent and joint component\noptimization, supporting efficient simulation of societies with over one\nbillion agents. Large-scale simulations of trust games and opinion\npropagation--spanning up to one billion agents--demonstrate Light Society's\nhigh fidelity and efficiency in modeling social trust and information\ndiffusion, while revealing scaling laws whereby larger simulations yield more\nstable and realistic emergent behaviors."}
{"id": "2506.12331", "categories": ["cs.MA", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.12331", "abs": "https://arxiv.org/abs/2506.12331", "authors": ["Dekun Wu", "Frederik Brudy", "Bang Liu", "Yi Wang"], "title": "IndoorWorld: Integrating Physical Task Solving and Social Simulation in A Heterogeneous Multi-Agent Environment", "comment": null, "summary": "Virtual environments are essential to AI agent research. Existing\nenvironments for LLM agent research typically focus on either physical task\nsolving or social simulation, with the former oversimplifying agent\nindividuality and social dynamics, and the latter lacking physical grounding of\nsocial behaviors. We introduce IndoorWorld, a heterogeneous multi-agent\nenvironment that tightly integrates physical and social dynamics. By\nintroducing novel challenges for LLM-driven agents in orchestrating social\ndynamics to influence physical environments and anchoring social interactions\nwithin world states, IndoorWorld opens up possibilities of LLM-based building\noccupant simulation for architectural design. We demonstrate the potential with\na series of experiments within an office setting to examine the impact of\nmulti-agent collaboration, resource competition, and spatial layout on agent\nbehavior."}
{"id": "2506.12600", "categories": ["cs.MA", "cs.AI", "cs.ET", "cs.GT", "cs.RO"], "pdf": "https://arxiv.org/pdf/2506.12600", "abs": "https://arxiv.org/abs/2506.12600", "authors": ["Jie Pan", "Tianyi Wang", "Christian Claudel", "Jing Shi"], "title": "Trust-MARL: Trust-Based Multi-Agent Reinforcement Learning Framework for Cooperative On-Ramp Merging Control in Heterogeneous Traffic Flow", "comment": "34 pages, 7 figures, 4 tables", "summary": "Intelligent transportation systems require connected and automated vehicles\n(CAVs) to conduct safe and efficient cooperation with human-driven vehicles\n(HVs) in complex real-world traffic environments. However, the inherent\nunpredictability of human behaviour, especially at bottlenecks such as highway\non-ramp merging areas, often disrupts traffic flow and compromises system\nperformance. To address the challenge of cooperative on-ramp merging in\nheterogeneous traffic environments, this study proposes a trust-based\nmulti-agent reinforcement learning (Trust-MARL) framework. At the macro level,\nTrust-MARL enhances global traffic efficiency by leveraging inter-agent trust\nto improve bottleneck throughput and mitigate traffic shockwave through\nemergent group-level coordination. At the micro level, a dynamic trust\nmechanism is designed to enable CAVs to adjust their cooperative strategies in\nresponse to real-time behaviors and historical interactions with both HVs and\nother CAVs. Furthermore, a trust-triggered game-theoretic decision-making\nmodule is integrated to guide each CAV in adapting its cooperation factor and\nexecuting context-aware lane-changing decisions under safety, comfort, and\nefficiency constraints. An extensive set of ablation studies and comparative\nexperiments validates the effectiveness of the proposed Trust-MARL approach,\ndemonstrating significant improvements in safety, efficiency, comfort, and\nadaptability across varying CAV penetration rates and traffic densities."}
{"id": "2506.13068", "categories": ["cs.MA"], "pdf": "https://arxiv.org/pdf/2506.13068", "abs": "https://arxiv.org/abs/2506.13068", "authors": ["Haowen Xu", "Yulin Sun", "Jose Tupayachi", "Olufemi Omitaomu", "Sisi Zlatanov", "Xueping Li"], "title": "Towards the Autonomous Optimization of Urban Logistics: Training Generative AI with Scientific Tools via Agentic Digital Twins and Model Context Protocol", "comment": null, "summary": "Optimizing urban freight logistics is critical for developing sustainable,\nlow-carbon cities. Traditional methods often rely on manual coordination of\nsimulation tools, optimization solvers, and expert-driven workflows, limiting\ntheir efficiency and scalability. This paper presents an agentic system\narchitecture that leverages the model context protocol (MCP) to orchestrate\nmulti-agent collaboration among scientific tools for autonomous,\nsimulation-informed optimization in urban logistics. The system integrates\ngenerative AI agents with domain-specific engines - such as Gurobi for\noptimization and AnyLogic for agent-based simulation - forming a generative\ndigital twin capable of reasoning, planning, and acting across multimodal\nfreight networks. By incorporating integrated chatbots, retrieval-augmented\ngeneration, and structured memory, the framework enables agents to interpret\nuser intent from natural language conversations, retrieve relevant datasets and\nmodels, coordinate solvers and simulators, and execute complex workflows. We\ndemonstrate this approach through a freight decarbonization case study,\nshowcasing how MCP enables modular, interoperable, and adaptive agent behavior\nacross diverse toolchains. The results reveal that our system transforms\ndigital twins from static visualizations into autonomous, decision-capable\nsystems, advancing the frontiers of urban operations research. By enabling\ncontext-aware, generative agents to operate scientific tools automatically and\ncollaboratively, this framework supports more intelligent, accessible, and\ndynamic decision-making in transportation planning and smart city management."}
{"id": "2506.12026", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2506.12026", "abs": "https://arxiv.org/abs/2506.12026", "authors": ["Behnam Shobiri", "Sajjad Pourali", "Daniel Migault", "Ioana Boureanu", "Stere Preda", "Mohammad Mannan", "Amr Youssef"], "title": "LURK-T: Limited Use of Remote Keys With Added Trust in TLS 1.3", "comment": null, "summary": "In many web applications, such as Content Delivery Networks (CDNs), TLS\ncredentials are shared, e.g., between the website's TLS origin server and the\nCDN's edge servers, which can be distributed around the globe. To enhance the\nsecurity and trust for TLS 1.3 in such scenarios, we propose LURK-T, a provably\nsecure framework which allows for limited use of remote keys with added trust\nin TLS 1.3. We efficiently decouple the server side of TLS 1.3 into a LURK-T\nCrypto Service (CS) and a LURK-T Engine (E). CS executes all cryptographic\noperations in a Trusted Execution Environment (TEE), upon E's requests. CS and\nE together provide the whole TLS-server functionality. A major benefit of our\nconstruction is that it is application agnostic; the LURK-T Crypto Service\ncould be collocated with the LURK-T Engine, or it could run on different\nmachines. Thus, our design allows for in situ attestation and protection of the\ncryptographic side of the TLS server, as well as for all setups of CDNs over\nTLS. To support such a generic decoupling, we provide a full Application\nProgramming Interface (API) for LURK-T. To this end, we implement our LURK-T\nCrypto Service using Intel SGX and integrate it with OpenSSL. We also test\nLURK-T's efficiency and show that, from a TLS-client's perspective, HTTPS\nservers using LURK-T instead a traditional TLS-server have no noticeable\noverhead when serving files greater than 1MB. In addition, we provide\ncryptographic proofs and formal security verification using ProVerif."}
{"id": "2506.12084", "categories": ["cs.SE", "cs.AI", "cs.CL", "cs.FL", "cs.NE"], "pdf": "https://arxiv.org/pdf/2506.12084", "abs": "https://arxiv.org/abs/2506.12084", "authors": ["Michele Alberti", "Fran√ßois Bobot", "Julien Girard-Satabin", "Alban Grastien", "Aymeric Varasse", "Zakaria Chihani"], "title": "The CAISAR Platform: Extending the Reach of Machine Learning Specification and Verification", "comment": null, "summary": "The formal specification and verification of machine learning programs saw\nremarkable progress in less than a decade, leading to a profusion of tools.\nHowever, diversity may lead to fragmentation, resulting in tools that are\ndifficult to compare, except for very specific benchmarks. Furthermore, this\nprogress is heavily geared towards the specification and verification of a\ncertain class of property, that is, local robustness properties. But while\nprovers are becoming more and more efficient at solving local robustness\nproperties, even slightly more complex properties, involving multiple neural\nnetworks for example, cannot be expressed in the input languages of winners of\nthe International Competition of Verification of Neural Networks VNN-Comp. In\nthis tool paper, we present CAISAR, an open-source platform dedicated to\nmachine learning specification and verification. We present its specification\nlanguage, suitable for modelling complex properties on neural networks, support\nvector machines and boosted trees. We show on concrete use-cases how\nspecifications written in this language are automatically translated to queries\nto state-of-the-art provers, notably by using automated graph editing\ntechniques, making it possible to use their off-the-shelf versions. The\nartifact to reproduce the paper claims is available at the following DOI:\nhttps://doi.org/10.5281/zenodo.15209510"}
{"id": "2506.13574", "categories": ["cs.MA"], "pdf": "https://arxiv.org/pdf/2506.13574", "abs": "https://arxiv.org/abs/2506.13574", "authors": ["Helena Fehler", "Marco Pruckner", "Marie Schmidt"], "title": "Mobility to Campus -- a Framework to Evaluate and Compare Different Mobility Modes", "comment": null, "summary": "The transport sector accounts for about 20% of German CO2 emissions, with\ncommuter traffic contributing a significant part. Particularly in rural areas,\nwhere public transport is inconvenient to use, private cars are a common choice\nfor commuting and most commuters travel alone in their cars. Consolidation of\nsome of these trips has the potential to decrease CO2 emissions and could be\nachieved, e.g., by offering ridesharing (commuters with similar\norigin-destination pairs share a car) or ridepooling (commuters are picked up\nby shuttle services). In this study, we present a framework to assess the\npotential of introducing new mobility modes like ridesharing and ridepooling\nfor commuting towards several locations in close vicinity to each other.\n  We test our framework on the case of student mobility to the University of\nW\\\"urzburg, a university with several campus locations and a big and rather\nrural catchment area, where existing public transport options are inconvenient\nand many students commute by car. We combine data on student home addresses and\ncampus visitation times to create demand scenarios. In our case study, we\ncompare the mobility modes of ridesharing and ridepooling to the base case,\nwhere students travel by car on their own. We find that ridesharing has the\npotential to greatly reduce emissions, depending on the percentage of students\nwilling to use the service and their willingness to walk to the departure\nlocation. The benefit of ridepooling is less clear, materializing only if the\nshuttle vehicles are more energy efficient than the student cars."}
{"id": "2506.12060", "categories": ["cs.CR", "cs.AI", "cs.CY", "K.6.5; I.2.0; K.4.1"], "pdf": "https://arxiv.org/pdf/2506.12060", "abs": "https://arxiv.org/abs/2506.12060", "authors": ["Christopher Nott"], "title": "Organizational Adaptation to Generative AI in Cybersecurity: A Systematic Review", "comment": "38 pages, 1 table, 1 figure", "summary": "Cybersecurity organizations are adapting to GenAI integration through\nmodified frameworks and hybrid operational processes, with success influenced\nby existing security maturity, regulatory requirements, and investments in\nhuman capital and infrastructure. This qualitative research employs systematic\ndocument analysis and comparative case study methodology to examine how\ncybersecurity organizations adapt their threat modeling frameworks and\noperational processes to address generative artificial intelligence\nintegration. Through examination of 25 studies from 2022 to 2025, the research\ndocuments substantial transformation in organizational approaches to threat\nmodeling, moving from traditional signature-based systems toward frameworks\nincorporating artificial intelligence capabilities. The research identifies\nthree primary adaptation patterns: Large Language Model integration for\nsecurity applications, GenAI frameworks for risk detection and response\nautomation, and AI/ML integration for threat hunting. Organizations with mature\nsecurity infrastructures, particularly in finance and critical infrastructure\nsectors, demonstrate higher readiness through structured governance approaches,\ndedicated AI teams, and robust incident response processes. Organizations\nachieve successful GenAI integration when they maintain appropriate human\noversight of automated systems, address data quality concerns and\nexplainability requirements, and establish governance frameworks tailored to\ntheir specific sectors. Organizations encounter ongoing difficulties with\nprivacy protection, bias reduction, personnel training, and defending against\nadversarial attacks. This work advances understanding of how organizations\nadopt innovative technologies in high-stakes environments and offers actionable\ninsights for cybersecurity professionals implementing GenAI systems."}
{"id": "2506.12111", "categories": ["cs.SE", "cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.12111", "abs": "https://arxiv.org/abs/2506.12111", "authors": ["Oscar Boullosa Dapena"], "title": "Quantum-Inspired Differentiable Integral Neural Networks (QIDINNs): A Feynman-Based Architecture for Continuous Learning Over Streaming Data", "comment": null, "summary": "Real-time continuous learning over streaming data remains a central challenge\nin deep learning and AI systems. Traditional gradient-based models such as\nbackpropagation through time (BPTT) face computational and stability\nlimitations when dealing with temporally unbounded data. In this paper, we\nintroduce a novel architecture, Quantum-Inspired Differentiable Integral Neural\nNetworks (QIDINNs), which leverages the Feynman technique of differentiation\nunder the integral sign to formulate neural updates as integrals over\nhistorical data. This reformulation allows for smoother, more stable learning\ndynamics that are both physically interpretable and computationally tractable.\nInspired by Feynman's path integral formalism and compatible with quantum\ngradient estimation frameworks, QIDINNs open a path toward hybrid\nclassical-quantum neural computation. We demonstrate our model's effectiveness\non synthetic and real-world streaming tasks, and we propose directions for\nquantum extensions and scalable implementations."}
{"id": "2506.12088", "categories": ["cs.CR", "cs.CY"], "pdf": "https://arxiv.org/pdf/2506.12088", "abs": "https://arxiv.org/abs/2506.12088", "authors": ["Kiarash Ahi"], "title": "Risks & Benefits of LLMs & GenAI for Platform Integrity, Healthcare Diagnostics, Cybersecurity, Privacy & AI Safety: A Comprehensive Survey, Roadmap & Implementation Blueprint", "comment": null, "summary": "Large Language Models (LLMs) and generative AI (GenAI) systems such as\nChatGPT, Claude, Gemini, LLaMA, and Copilot, developed by OpenAI, Anthropic,\nGoogle, Meta, and Microsoft are reshaping digital platforms and app ecosystems\nwhile introducing key challenges in cybersecurity, privacy, and platform\nintegrity. Our analysis shows alarming trends: LLM-assisted malware is\nprojected to rise from 2% in 2021 to 50% by 2025; AI-generated Google reviews\ngrew from 1.2% in 2021 to 12.21% in 2023, with an expected 30% by 2025; AI scam\nreports surged 456%; and misinformation sites increased over 1500%, with a\n50-60% increase in deepfakes in 2024. Concurrently, as LLMs have facilitated\ncode development, mobile app submissions grew from 1.8 million in 2020 to 3.0\nmillion in 2024, with 3.6 million expected by 2025. To address AI threats,\nplatforms from app stores like Google Play and Apple to developer hubs like\nGitHub Copilot, and social platforms like TikTok and Facebook, to marketplaces\nlike Amazon are deploying AI and LLM-based defenses. This highlights the dual\nnature of these technologies as both the source of new threats and the\nessential tool for their mitigation. Integrating LLMs into clinical diagnostics\nalso raises concerns about accuracy, bias, and safety, needing strong\ngovernance. Drawing on a comprehensive analysis of 455 references, this paper\npresents a survey of LLM and GenAI risks. We propose a strategic roadmap and\noperational blueprint integrating policy auditing (CCPA, GDPR), fraud\ndetection, and compliance automation, and an advanced LLM-DA stack with modular\ncomponents including multi LLM routing, agentic memory, and governance layers\nto enhance platform integrity. We also provide actionable insights,\ncross-functional best practices, and real-world case studies. These\ncontributions offer paths to scalable trust, safety, and responsible innovation\nacross digital platforms."}
{"id": "2506.12278", "categories": ["cs.SE", "cs.CL"], "pdf": "https://arxiv.org/pdf/2506.12278", "abs": "https://arxiv.org/abs/2506.12278", "authors": ["Zheyuan Yang", "Zexi Kuang", "Xue Xia", "Yilun Zhao"], "title": "Can LLMs Generate High-Quality Test Cases for Algorithm Problems? TestCase-Eval: A Systematic Evaluation of Fault Coverage and Exposure", "comment": "ACL 2025", "summary": "We introduce TestCase-Eval, a new benchmark for systematic evaluation of LLMs\nin test-case generation. TestCase-Eval includes 500 algorithm problems and\n100,000 human-crafted solutions from the Codeforces platform. It focuses on two\npivotal tasks: (1) Fault Coverage, which measures how well LLM-generated test\nsets probe diverse input scenarios and cover a wide range of potential failure\nmodes. (2) Fault Exposure, which evaluates whether LLMs can craft a tailored\ntest input that reveals a specific incorrect code implementation. We provide a\ncomprehensive assessment of 19 state-of-the-art open-source and proprietary\nLLMs on TestCase-Eval, offering insights into their strengths and limitations\nin generating effective test cases for algorithm problems."}
{"id": "2506.12096", "categories": ["cs.CR", "cs.ET"], "pdf": "https://arxiv.org/pdf/2506.12096", "abs": "https://arxiv.org/abs/2506.12096", "authors": ["Huma Habib Shadan", "Sardar Islam"], "title": "Quantum Computing and Cybersecurity in Accounting and Finance: Current and the Future Challenges and Opportunities for Securing Accounting and Finance Systems", "comment": "45 Pages, 2 Figures, 4 Tables, 1 Flow Diagram", "summary": "Quantum computing is revolutionising information systems and will have a\nsignificant impact on accounting and finance, especially in the area of\ncybersecurity. It presents both opportunities and risks in ensuring\nconfidentiality and protecting financial data. The purpose of this thesis is to\nshow the application of quantum technologies in accounting cybersecurity,\nutilising quantum algorithms and QKD to overcome the limitations of classical\ncomputing.\n  The literature review reveals the vulnerabilities of the current accounting\ncybersecurity to quantum attacks and the need for quantum-resistant\ncryptographic mechanisms. It elaborates on the risks associated with\nconventional encryption in the context of quantum capabilities. This study\ncontributes to the understanding of how quantum computing can revolutionise\naccounting cybersecurity by enhancing quantum-resistant algorithms and\nutilising quantum key distribution (QKD) in accounting.\n  The study employs PSALSAR systematic review methodology to ensure rigour and\ndepth. The analysis shows that quantum computing enhances encryption techniques\nto superior possibilities than classical ones. Using quantum technologies in\naccounting minimises data breaches and unauthorised access. The study concludes\nthat quantum-resistant algorithms and quantum key distribution (QKD) are\nnecessary for securing the accounting and finance systems of the future.\n  Keywords Quantum Computing, Cybersecurity, Accounting, Machine Learning,\nArtificial Intelligence, Quantum Key Distribution, Operations Management"}
{"id": "2506.12320", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.12320", "abs": "https://arxiv.org/abs/2506.12320", "authors": ["Weipeng Jiang", "Xiaoyu Zhang", "Xiaofei Xie", "Jiongchi Yu", "Yuhan Zhi", "Shiqing Ma", "Chao Shen"], "title": "The Foundation Cracks: A Comprehensive Study on Bugs and Testing Practices in LLM Libraries", "comment": null, "summary": "Large Language Model (LLM) libraries have emerged as the foundational\ninfrastructure powering today's AI revolution, serving as the backbone for LLM\ndeployment, inference optimization, fine-tuning, and production serving across\ndiverse applications. Despite their critical role in the LLM ecosystem, these\nlibraries face frequent quality issues and bugs that threaten the reliability\nof AI systems built upon them. To address this knowledge gap, we present the\nfirst comprehensive empirical investigation into bug characteristics and\ntesting practices in modern LLM libraries. We examine 313 bug-fixing commits\nextracted across two widely-adopted LLM libraries: HuggingFace Transformers and\nvLLM.Through rigorous manual analysis, we establish comprehensive taxonomies\ncategorizing bug symptoms into 5 types and root causes into 14 distinct\ncategories.Our primary discovery shows that API misuse has emerged as the\npredominant root cause (32.17%-48.19%), representing a notable transition from\nalgorithm-focused defects in conventional deep learning frameworks toward\ninterface-oriented problems. Additionally, we examine 7,748 test functions to\nidentify 7 distinct test oracle categories employed in current testing\napproaches, with predefined expected outputs (such as specific tensors and text\nstrings) being the most common strategy. Our assessment of existing testing\neffectiveness demonstrates that the majority of bugs escape detection due to\ninadequate test cases (41.73%), lack of test drivers (32.37%), and weak test\noracles (25.90%). Drawing from these findings, we offer some recommendations\nfor enhancing LLM library quality assurance."}
{"id": "2506.12100", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.12100", "abs": "https://arxiv.org/abs/2506.12100", "authors": ["Reza Fayyazi", "Michael Zuzak", "Shanchieh Jay Yang"], "title": "LLM Embedding-based Attribution (LEA): Quantifying Source Contributions to Generative Model's Response for Vulnerability Analysis", "comment": null, "summary": "Security vulnerabilities are rapidly increasing in frequency and complexity,\ncreating a shifting threat landscape that challenges cybersecurity defenses.\nLarge Language Models (LLMs) have been widely adopted for cybersecurity threat\nanalysis. When querying LLMs, dealing with new, unseen vulnerabilities is\nparticularly challenging as it lies outside LLMs' pre-trained distribution.\nRetrieval-Augmented Generation (RAG) pipelines mitigate the problem by\ninjecting up-to-date authoritative sources into the model context, thus\nreducing hallucinations and increasing the accuracy in responses. Meanwhile,\nthe deployment of LLMs in security-sensitive environments introduces challenges\naround trust and safety. This raises a critical open question: How to quantify\nor attribute the generated response to the retrieved context versus the model's\npre-trained knowledge? This work proposes LLM Embedding-based Attribution (LEA)\n-- a novel, explainable metric to paint a clear picture on the 'percentage of\ninfluence' the pre-trained knowledge vs. retrieved content has for each\ngenerated response. We apply LEA to assess responses to 100 critical CVEs from\nthe past decade, verifying its effectiveness to quantify the insightfulness for\nvulnerability analysis. Our development of LEA reveals a progression of\nindependency in hidden states of LLMs: heavy reliance on context in early\nlayers, which enables the derivation of LEA; increased independency in later\nlayers, which sheds light on why scale is essential for LLM's effectiveness.\nThis work provides security analysts a means to audit LLM-assisted workflows,\nlaying the groundwork for transparent, high-assurance deployments of\nRAG-enhanced LLMs in cybersecurity operations."}
{"id": "2506.12347", "categories": ["cs.SE", "cs.HC"], "pdf": "https://arxiv.org/pdf/2506.12347", "abs": "https://arxiv.org/abs/2506.12347", "authors": ["Aayush Kumar", "Yasharth Bajpai", "Sumit Gulwani", "Gustavo Soares", "Emerson Murphy-Hill"], "title": "How Developers Use AI Agents: When They Work, When They Don't, and Why", "comment": null, "summary": "Software Engineering Agents (SWE agents) can autonomously perform development\ntasks on benchmarks like SWE Bench, but still face challenges when tackling\ncomplex and ambiguous real-world tasks. Consequently, SWE agents are often\ndesigned to allow interactivity with developers, enabling collaborative\nproblem-solving. To understand how developers collaborate with SWE agents and\nthe communication challenges that arise in such interactions, we observed 19\ndevelopers using an in-IDE agent to resolve 33 open issues in repositories to\nwhich they had previously contributed. Participants successfully resolved about\nhalf of these issues, with participants solving issues incrementally having\ngreater success than those using a one-shot approach. Participants who actively\ncollaborated with the agent and iterated on its outputs were also more\nsuccessful, though they faced challenges in trusting the agent's responses and\ncollaborating on debugging and testing. These results have implications for\nsuccessful developer-agent collaborations, and for the design of more effective\nSWE agents."}
{"id": "2506.12104", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.12104", "abs": "https://arxiv.org/abs/2506.12104", "authors": ["Hao Li", "Xiaogeng Liu", "Hung-Chun Chiu", "Dianqi Li", "Ning Zhang", "Chaowei Xiao"], "title": "DRIFT: Dynamic Rule-Based Defense with Injection Isolation for Securing LLM Agents", "comment": "18 pages, 12 figures", "summary": "Large Language Models (LLMs) are increasingly central to agentic systems due\nto their strong reasoning and planning capabilities. By interacting with\nexternal environments through predefined tools, these agents can carry out\ncomplex user tasks. Nonetheless, this interaction also introduces the risk of\nprompt injection attacks, where malicious inputs from external sources can\nmislead the agent's behavior, potentially resulting in economic loss, privacy\nleakage, or system compromise. System-level defenses have recently shown\npromise by enforcing static or predefined policies, but they still face two key\nchallenges: the ability to dynamically update security rules and the need for\nmemory stream isolation. To address these challenges, we propose DRIFT, a\nDynamic Rule-based Isolation Framework for Trustworthy agentic systems, which\nenforces both control- and data-level constraints. A Secure Planner first\nconstructs a minimal function trajectory and a JSON-schema-style parameter\nchecklist for each function node based on the user query. A Dynamic Validator\nthen monitors deviations from the original plan, assessing whether changes\ncomply with privilege limitations and the user's intent. Finally, an Injection\nIsolator detects and masks any instructions that may conflict with the user\nquery from the memory stream to mitigate long-term risks. We empirically\nvalidate the effectiveness of DRIFT on the AgentDojo benchmark, demonstrating\nits strong security performance while maintaining high utility across diverse\nmodels -- showcasing both its robustness and adaptability."}
{"id": "2506.12590", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2506.12590", "abs": "https://arxiv.org/abs/2506.12590", "authors": ["Breno Alves de Andrade", "Rodrigo Siqueira", "Lidiane Gomes", "Antonio Oliveira", "Danilo Monteiro Ribeiro"], "title": "A Mapping Study About Training in Industry Context in Software Engineering", "comment": null, "summary": "Context: Corporate training plays a strategic role in the continuous\ndevelopment of professionals in the software engineering industry. However,\nthere is a lack of systematized understanding of how training initiatives are\ndesigned, implemented, and evaluated within this domain.\n  Objective: This study aims to map the current state of research on corporate\ntraining in software engineering in industry settings, using Eduardo Salas'\ntraining framework as an analytical lens.\n  Method: A systematic mapping study was conducted involving the selection and\nanalysis of 26 primary studies published in the field. Each study was\ncategorized according to Salas' four key areas: Training Needs Analysis,\nAntecedent Training Conditions, Training Methods and Instructional Strategies,\nand Post-Training Conditions.\n  Results: The findings show a predominance of studies focusing on Training\nMethods and Instructional Strategies. Significant gaps were identified in other\nareas, particularly regarding Job/Task Analysis and Simulation-based Training\nand Games. Most studies were experience reports, lacking methodological rigor\nand longitudinal assessment.\n  Conclusions: The study offers a structured overview of how corporate training\nis approached in software engineering, revealing underexplored areas and\nproposing directions for future research. It contributes to both academic and\npractical communities by highlighting challenges, methodological trends, and\nopportunities for designing more effective training programs in industry."}
{"id": "2506.12108", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.12108", "abs": "https://arxiv.org/abs/2506.12108", "authors": ["Bassam Noori Shaker", "Bahaa Al-Musawi", "Mohammed Falih Hassan"], "title": "A Lightweight IDS for Early APT Detection Using a Novel Feature Selection Method", "comment": null, "summary": "An Advanced Persistent Threat (APT) is a multistage, highly sophisticated,\nand covert form of cyber threat that gains unauthorized access to networks to\neither steal valuable data or disrupt the targeted network. These threats often\nremain undetected for extended periods, emphasizing the critical need for early\ndetection in networks to mitigate potential APT consequences. In this work, we\npropose a feature selection method for developing a lightweight intrusion\ndetection system capable of effectively identifying APTs at the initial\ncompromise stage. Our approach leverages the XGBoost algorithm and Explainable\nArtificial Intelligence (XAI), specifically utilizing the SHAP (SHapley\nAdditive exPlanations) method for identifying the most relevant features of the\ninitial compromise stage. The results of our proposed method showed the ability\nto reduce the selected features of the SCVIC-APT-2021 dataset from 77 to just\nfour while maintaining consistent evaluation metrics for the suggested system.\nThe estimated metrics values are 97% precision, 100% recall, and a 98% F1\nscore. The proposed method not only aids in preventing successful APT\nconsequences but also enhances understanding of APT behavior at early stages."}
{"id": "2506.12616", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2506.12616", "abs": "https://arxiv.org/abs/2506.12616", "authors": ["Debasish Jana", "Pinakpani Pal", "Pawan Kumar"], "title": "Real-Time Agile Software Management for Edge and Fog Computing Based Smart City Infrastructure", "comment": "The paper has been published at the Fifth International Conference on\n  Computing and Communication Networks (ICCCN 2025), Volume 1", "summary": "The evolution of smart cities demands scalable, secure, and energy-efficient\narchitectures for real-time data processing. With the number of IoT devices\nexpected to exceed 40 billion by 2030, traditional cloud-based systems are\nincreasingly constrained by bandwidth, latency, and energy limitations. This\npaper leverages the ROOF (Real-time Onsite Operations Facilitation) framework\nwith decentralized computing at intermediary fog and peripheral edge network\nlayers to reduce latency by processing data near its point of origin. ROOF\nfeatures fog caching to avoid redundancy, ultra-low-power wireless transmission\nfor energy savings, and AI-driven resource allocation for efficiency. Security\nis enhanced through TLS encryption, blockchain-based authentication, and\nedge-level access control. Case studies from Bhubaneswar, Barcelona and\nCopenhagen validate the use of ROOF in traffic systems and environmental\nmonitoring. The paper concludes by outlining key challenges and prospects of\nAI-driven analytics in smart urban infrastructure."}
{"id": "2506.12113", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.12113", "abs": "https://arxiv.org/abs/2506.12113", "authors": ["Benjamin Marais", "Tony Quertier", "Gr√©goire Barrue"], "title": "Semantic Preprocessing for LLM-based Malware Analysis", "comment": null, "summary": "In a context of malware analysis, numerous approaches rely on Artificial\nIntelligence to handle a large volume of data. However, these techniques focus\non data view (images, sequences) and not on an expert's view. Noticing this\nissue, we propose a preprocessing that focuses on expert knowledge to improve\nmalware semantic analysis and result interpretability. We propose a new\npreprocessing method which creates JSON reports for Portable Executable files.\nThese reports gather features from both static and behavioral analysis, and\nincorporate packer signature detection, MITRE ATT\\&CK and Malware Behavior\nCatalog (MBC) knowledge. The purpose of this preprocessing is to gather a\nsemantic representation of binary files, understandable by malware analysts,\nand that can enhance AI models' explainability for malicious files analysis.\nUsing this preprocessing to train a Large Language Model for Malware\nclassification, we achieve a weighted-average F1-score of 0.94 on a complex\ndataset, representative of market reality."}
{"id": "2506.12643", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2506.12643", "abs": "https://arxiv.org/abs/2506.12643", "authors": ["Prachnachai Meakpaiboonwattana", "Warittha Tarntong", "Thai Mekratanavorakul", "Chaiyong Ragkhitwetsagul", "Pattaraporn Sangaroonsilp", "Raula Kula", "Morakot Choetkiertikul", "Kenichi Matsumoto", "Thanwadee Sunetnanta"], "title": "Social Media Reactions to Open Source Promotions: AI-Powered GitHub Projects on Hacker News", "comment": null, "summary": "Social media platforms have become more influential than traditional news\nsources, shaping public discourse and accelerating the spread of information.\nWith the rapid advancement of artificial intelligence (AI), open-source\nsoftware (OSS) projects can leverage these platforms to gain visibility and\nattract contributors. In this study, we investigate the relationship between\nHacker News, a social news site focused on computer science and\nentrepreneurship, and the extent to which it influences developer activity on\nthe promoted GitHub AI projects.\n  We analyzed 2,195 Hacker News (HN) stories and their corresponding comments\nover a two-year period. Our findings reveal that at least 19\\% of AI developers\npromoted their GitHub projects on Hacker News, often receiving positive\nengagement from the community. By tracking activity on the associated 1,814\nGitHub repositories after they were shared on Hacker News, we observed a\nsignificant increase in forks, stars, and contributors. These results suggest\nthat Hacker News serves as a viable platform for AI-powered OSS projects, with\nthe potential to gain attention, foster community engagement, and accelerate\nsoftware development."}
{"id": "2506.12257", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2506.12257", "abs": "https://arxiv.org/abs/2506.12257", "authors": ["Adam Shostack", "L. Jean Camp", "Yi Ting Chua", "Josiah Dykstra", "Brian LaMacchia", "Daniel Lopresti"], "title": "Lessons for Cybersecurity from the American Public Health System", "comment": null, "summary": "The United States needs national institutions and frameworks to\nsystematically collect cybersecurity data, measure outcomes, and coordinate\nresponses across government and private sectors, similar to how public health\nsystems track and address disease outbreaks."}
{"id": "2506.12669", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2506.12669", "abs": "https://arxiv.org/abs/2506.12669", "authors": ["Anrafel Fernandes Pereira", "Marcos Kalinowski", "Maria Teresa Baldassarre", "J√ºrgen B√∂rstler", "Nauman bin Ali", "Daniel Mendez"], "title": "Towards Lean Research Inception: Assessing Practical Relevance of Formulated Research Problems", "comment": "Accepted for publication at EASE 2025", "summary": "[Context] The lack of practical relevance in many Software Engineering (SE)\nresearch contributions is often rooted in oversimplified views of industrial\npractice, weak industry connections, and poorly defined research problems.\nClear criteria for evaluating SE research problems can help align their value,\nfeasibility, and applicability with industrial needs. [Goal] In this paper, we\nintroduce the Lean Research Inception (LRI) framework, designed to support the\nformulation and assessment of practically relevant research problems in SE. We\ndescribe its initial evaluation strategy conducted in a workshop with a network\nof SE researchers experienced in industry-academia collaboration and report the\nevaluation of its three assessment criteria (valuable, feasible, and\napplicable) regarding their importance in assessing practical relevance.\n[Method] We applied LRI retroactively to a published research paper, engaging\nworkshop participants in discussing and assessing the research problem by\napplying the proposed criteria using a semantic differential scale.\nParticipants provided feedback on the criteria's importance and completeness,\ndrawn from their own experiences in industry-academia collaboration. [Results]\nThe findings reveal an overall agreement on the importance of the three\ncriteria - valuable (83.3%), feasible (76.2%), and applicable (73.8%) - for\naligning research problems with industrial needs. Qualitative feedback\nsuggested adjustments in terminology with a clearer distinction between\nfeasible and applicable, and refinements for valuable by more clearly\nconsidering business value, ROI, and originality. [Conclusion] While LRI\nconstitutes ongoing research and requires further evaluation, our results\nstrengthen our confidence that the three criteria applied using the semantic\ndifferential scale can already help the community assess the practical\nrelevance of SE research problems."}
{"id": "2506.12274", "categories": ["cs.CR", "cs.CL"], "pdf": "https://arxiv.org/pdf/2506.12274", "abs": "https://arxiv.org/abs/2506.12274", "authors": ["Advait Yadav", "Haibo Jin", "Man Luo", "Jun Zhuang", "Haohan Wang"], "title": "InfoFlood: Jailbreaking Large Language Models with Information Overload", "comment": null, "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities across\nvarious domains. However, their potential to generate harmful responses has\nraised significant societal and regulatory concerns, especially when\nmanipulated by adversarial techniques known as \"jailbreak\" attacks. Existing\njailbreak methods typically involve appending carefully crafted prefixes or\nsuffixes to malicious prompts in order to bypass the built-in safety mechanisms\nof these models.\n  In this work, we identify a new vulnerability in which excessive linguistic\ncomplexity can disrupt built-in safety mechanisms-without the need for any\nadded prefixes or suffixes-allowing attackers to elicit harmful outputs\ndirectly. We refer to this phenomenon as Information Overload.\n  To automatically exploit this vulnerability, we propose InfoFlood, a\njailbreak attack that transforms malicious queries into complex,\ninformation-overloaded queries capable of bypassing built-in safety mechanisms.\nSpecifically, InfoFlood: (1) uses linguistic transformations to rephrase\nmalicious queries, (2) identifies the root cause of failure when an attempt is\nunsuccessful, and (3) refines the prompt's linguistic structure to address the\nfailure while preserving its malicious intent.\n  We empirically validate the effectiveness of InfoFlood on four widely used\nLLMs-GPT-4o, GPT-3.5-turbo, Gemini 2.0, and LLaMA 3.1-by measuring their\njailbreak success rates. InfoFlood consistently outperforms baseline attacks,\nachieving up to 3 times higher success rates across multiple jailbreak\nbenchmarks. Furthermore, we demonstrate that commonly adopted post-processing\ndefenses, including OpenAI's Moderation API, Perspective API, and SmoothLLM,\nfail to mitigate these attacks. This highlights a critical weakness in\ntraditional AI safety guardrails when confronted with information\noverload-based jailbreaks."}
{"id": "2506.12691", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.12691", "abs": "https://arxiv.org/abs/2506.12691", "authors": ["Bianca Trinkenreich", "Fabio Calefato", "Geir Hanssen", "Kelly Blincoe", "Marcos Kalinowski", "Mauro Pezz√®", "Paolo Tell", "Margaret-Anne Storey"], "title": "Get on the Train or be Left on the Station: Using LLMs for Software Engineering Research", "comment": "Accepted for publication at the 1st Workshop on Human-Centered AI for\n  SE (Human AISE) held at the 33rd ACM International Conference on the\n  Foundations of Software Engineering (FSE Companion '25), June 23-28, 2025,\n  Trondheim, Norway", "summary": "The adoption of Large Language Models (LLMs) is not only transforming\nsoftware engineering (SE) practice but is also poised to fundamentally disrupt\nhow research is conducted in the field. While perspectives on this\ntransformation range from viewing LLMs as mere productivity tools to\nconsidering them revolutionary forces, we argue that the SE research community\nmust proactively engage with and shape the integration of LLMs into research\npractices, emphasizing human agency in this transformation. As LLMs rapidly\nbecome integral to SE research - both as tools that support investigations and\nas subjects of study - a human-centric perspective is essential. Ensuring human\noversight and interpretability is necessary for upholding scientific rigor,\nfostering ethical responsibility, and driving advancements in the field.\nDrawing from discussions at the 2nd Copenhagen Symposium on Human-Centered AI\nin SE, this position paper employs McLuhan's Tetrad of Media Laws to analyze\nthe impact of LLMs on SE research. Through this theoretical lens, we examine\nhow LLMs enhance research capabilities through accelerated ideation and\nautomated processes, make some traditional research practices obsolete,\nretrieve valuable aspects of historical research approaches, and risk reversal\neffects when taken to extremes. Our analysis reveals opportunities for\ninnovation and potential pitfalls that require careful consideration. We\nconclude with a call to action for the SE research community to proactively\nharness the benefits of LLMs while developing frameworks and guidelines to\nmitigate their risks, to ensure continued rigor and impact of research in an\nAI-augmented future."}
{"id": "2506.12299", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.12299", "abs": "https://arxiv.org/abs/2506.12299", "authors": ["Taegyeong Lee", "Jeonghwa Yoo", "Hyoungseo Cho", "Soo Yong Kim", "Yunho Maeng"], "title": "QGuard:Question-based Zero-shot Guard for Multi-modal LLM Safety", "comment": "Accept to ACLW 2025 (WOAH)", "summary": "The recent advancements in Large Language Models(LLMs) have had a significant\nimpact on a wide range of fields, from general domains to specialized areas.\nHowever, these advancements have also significantly increased the potential for\nmalicious users to exploit harmful and jailbreak prompts for malicious attacks.\nAlthough there have been many efforts to prevent harmful prompts and jailbreak\nprompts, protecting LLMs from such malicious attacks remains an important and\nchallenging task. In this paper, we propose QGuard, a simple yet effective\nsafety guard method, that utilizes question prompting to block harmful prompts\nin a zero-shot manner. Our method can defend LLMs not only from text-based\nharmful prompts but also from multi-modal harmful prompt attacks. Moreover, by\ndiversifying and modifying guard questions, our approach remains robust against\nthe latest harmful prompts without fine-tuning. Experimental results show that\nour model performs competitively on both text-only and multi-modal harmful\ndatasets. Additionally, by providing an analysis of question prompting, we\nenable a white-box analysis of user inputs. We believe our method provides\nvaluable insights for real-world LLM services in mitigating security risks\nassociated with harmful prompts."}
{"id": "2506.12713", "categories": ["cs.SE", "cs.CL"], "pdf": "https://arxiv.org/pdf/2506.12713", "abs": "https://arxiv.org/abs/2506.12713", "authors": ["Xiangyang Li", "Xiaopeng Li", "Kuicai Dong", "Quanhu Zhang", "Rongju Ruan", "Xinyi Dai", "Xiaoshuang Liu", "Shengchun Xu", "Yasheng Wang", "Ruiming Tang"], "title": "Humanity's Last Code Exam: Can Advanced LLMs Conquer Human's Hardest Code Competition?", "comment": null, "summary": "Code generation is a core capability of large language models (LLMs), yet\nmainstream benchmarks (e.g., APPs and LiveCodeBench) contain questions with\nmedium-level difficulty and pose no challenge to advanced LLMs. To better\nreflected the advanced reasoning and code generation ability, We introduce\nHumanity's Last Code Exam (HLCE), comprising 235 most challenging problems from\nthe International Collegiate Programming Contest (ICPC World Finals) and the\nInternational Olympiad in Informatics (IOI) spanning 2010 - 2024. As part of\nHLCE, we design a harmonized online-offline sandbox that guarantees fully\nreproducible evaluation. Through our comprehensive evaluation, we observe that\neven the strongest reasoning LLMs: o4-mini(high) and Gemini-2.5 Pro, achieve\npass@1 rates of only 15.9% and 11.4%, respectively. Meanwhile, we propose a\nnovel \"self-recognition\" task to measure LLMs' awareness of their own\ncapabilities. Results indicate that LLMs' self-recognition abilities are not\nproportionally correlated with their code generation performance. Finally, our\nempirical validation of test-time scaling laws reveals that current advanced\nLLMs have substantial room for improvement on complex programming tasks. We\nexpect HLCE to become a milestone challenge for code generation and to catalyze\nadvances in high-performance reasoning and human-AI collaborative programming.\nOur code and dataset are also public\navailable(https://github.com/Humanity-s-Last-Code-Exam/HLCE)."}
{"id": "2506.12328", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2506.12328", "abs": "https://arxiv.org/abs/2506.12328", "authors": ["Kenneth Odoh"], "title": "Information-theoretic Estimation of the Risk of Privacy Leaks", "comment": null, "summary": "Recent work~\\cite{Liu2016} has shown that dependencies between items in a\ndataset can lead to privacy leaks. We extend this concept to privacy-preserving\ntransformations, considering a broader set of dependencies captured by\ncorrelation metrics. Specifically, we measure the correlation between the\noriginal data and their noisy responses from a randomizer as an indicator of\npotential privacy breaches. This paper aims to leverage information-theoretic\nmeasures, such as the Maximal Information Coefficient (MIC), to estimate\nprivacy leaks and derive novel, computationally efficient privacy leak\nestimators. We extend the $\\rho_1$-to-$\\rho_2$\nformulation~\\cite{Evfimievski2003} to incorporate entropy, mutual information,\nand the degree of anonymity for a more comprehensive measure of privacy risk.\nOur proposed hybrid metric can identify correlation dependencies between\nattributes in the dataset, serving as a proxy for privacy leak vulnerabilities.\nThis metric provides a computationally efficient worst-case measure of privacy\nloss, utilizing the inherent characteristics of the data to prevent privacy\nbreaches."}
{"id": "2506.12728", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2506.12728", "abs": "https://arxiv.org/abs/2506.12728", "authors": ["Yibo Wang", "Zhihao Peng", "Ying Wang", "Zhao Wei", "Hai Yu", "Zhiliang Zhu"], "title": "MCTS-Refined CoT: High-Quality Fine-Tuning Data for LLM-Based Repository Issue Resolution", "comment": null, "summary": "LLMs demonstrate strong performance in auto-mated software engineering,\nparticularly for code generation and issue resolution. While proprietary models\nlike GPT-4o achieve high benchmarks scores on SWE-bench, their API dependence,\ncost, and privacy concerns limit adoption. Open-source alternatives offer\ntransparency but underperform in complex tasks, especially sub-100B parameter\nmodels. Although quality Chain-of-Thought (CoT) data can enhance reasoning,\ncurrent methods face two critical flaws: (1) weak rejection sampling reduces\ndata quality, and (2) inadequate step validation causes error accumulation.\nThese limitations lead to flawed reasoning chains that impair LLMs'ability to\nlearn reliable issue resolution. The paper proposes MCTS-REFINE, an enhanced\nMonte Carlo Tree Search (MCTS)-based algorithm that dynamically validates and\noptimizes intermediate reasoning steps through a rigorous rejection sampling\nstrategy, generating high-quality CoT data to improve LLM performance in issue\nresolution tasks. Key innovations include: (1) augmenting MCTS with a\nreflection mechanism that corrects errors via rejection sampling and\nrefinement, (2) decomposing issue resolution into three subtasks-File\nLocalization, Fault Localization, and Patch Generation-each with clear\nground-truth criteria, and (3) enforcing a strict sampling protocol where\nintermediate outputs must exactly match verified developer patches, ensuring\ncorrectness across reasoning paths. Experiments on SWE-bench Lite and SWE-bench\nVerified demonstrate that LLMs fine-tuned with our CoT dataset achieve\nsubstantial improvements over baselines.Notably, Qwen2.5-72B- Instruct achieves\n28.3%(Lite) and 35.0%(Verified) resolution rates, surpassing SOTA baseline\nSWE-Fixer-Qwen-72B with the same parameter scale, which only reached\n24.7%(Lite) and 32.8%(Verified)."}
{"id": "2506.12344", "categories": ["cs.CR", "cs.CV"], "pdf": "https://arxiv.org/pdf/2506.12344", "abs": "https://arxiv.org/abs/2506.12344", "authors": ["Haoyu Zhai", "Shuo Wang", "Pirouz Naghavi", "Qingying Hao", "Gang Wang"], "title": "Restoring Gaussian Blurred Face Images for Deanonymization Attacks", "comment": "18 pages, 16 figures, IEEE Transaction format", "summary": "Gaussian blur is widely used to blur human faces in sensitive photos before\nthe photos are posted on the Internet. However, it is unclear to what extent\nthe blurred faces can be restored and used to re-identify the person,\nespecially under a high-blurring setting. In this paper, we explore this\nquestion by developing a deblurring method called Revelio. The key intuition is\nto leverage a generative model's memorization effect and approximate the\ninverse function of Gaussian blur for face restoration. Compared with existing\nmethods, we design the deblurring process to be identity-preserving. It uses a\nconditional Diffusion model for preliminary face restoration and then uses an\nidentity retrieval model to retrieve related images to further enhance\nfidelity. We evaluate Revelio with large public face image datasets and show\nthat it can effectively restore blurred faces, especially under a high-blurring\nsetting. It has a re-identification accuracy of 95.9%, outperforming existing\nsolutions. The result suggests that Gaussian blur should not be used for face\nanonymization purposes. We also demonstrate the robustness of this method\nagainst mismatched Gaussian kernel sizes and functions, and test preliminary\ncountermeasures and adaptive attacks to inspire future work."}
{"id": "2506.12760", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2506.12760", "abs": "https://arxiv.org/abs/2506.12760", "authors": ["Lantian Li", "Yejian Liang", "Zhongxing Yu"], "title": "IDOL: Improved Different Optimization Levels Testing for Solidity Compilers", "comment": "Accepted by QRS 2025 (Fast Abstracts track)", "summary": "As blockchain technology continues to evolve and mature, smart contracts have\nbecome a key driving force behind the digitization and automation of\ntransactions. Smart contracts greatly simplify and refine the traditional\nbusiness transaction processes, and thus have had a profound impact on various\nindustries such as finance and supply chain management. However, because smart\ncontracts cannot be modified once deployed, any vulnerabilities or design flaws\nwithin the contract cannot be easily fixed, potentially leading to significant\nfinancial losses or even legal issues. The compiler, as a critical component in\nthe development process, directly affects the quality and security of smart\ncontracts. This paper innovatively proposes a method, known as the Improved\nDifferent Optimization Levels (IDOL), for testing the Solidity compiler. The\nkey idea behind IDOL is to perform reverse optimization transformations (i.e.,\nchange optimized form into unoptimized form) to generate semantically\nequivalent variants of the smart contracts under test, aiming to maximize the\nopportunities to trigger the optimization logic of compilers. We conducted a\npreliminary evaluation of IDOL and three confirmed compiler optimization bugs\nhave been uncovered at the time of writing."}
{"id": "2506.12411", "categories": ["cs.CR", "cs.CV"], "pdf": "https://arxiv.org/pdf/2506.12411", "abs": "https://arxiv.org/abs/2506.12411", "authors": ["Mengyuan Sun", "Yu Li", "Yuchen Liu", "Bo Du", "Yunjie Ge"], "title": "InverTune: Removing Backdoors from Multimodal Contrastive Learning Models via Trigger Inversion and Activation Tuning", "comment": null, "summary": "Multimodal contrastive learning models like CLIP have demonstrated remarkable\nvision-language alignment capabilities, yet their vulnerability to backdoor\nattacks poses critical security risks. Attackers can implant latent triggers\nthat persist through downstream tasks, enabling malicious control of model\nbehavior upon trigger presentation. Despite great success in recent defense\nmechanisms, they remain impractical due to strong assumptions about attacker\nknowledge or excessive clean data requirements. In this paper, we introduce\nInverTune, the first backdoor defense framework for multimodal models under\nminimal attacker assumptions, requiring neither prior knowledge of attack\ntargets nor access to the poisoned dataset. Unlike existing defense methods\nthat rely on the same dataset used in the poisoning stage, InverTune\neffectively identifies and removes backdoor artifacts through three key\ncomponents, achieving robust protection against backdoor attacks. Specifically,\nInverTune first exposes attack signatures through adversarial simulation,\nprobabilistically identifying the target label by analyzing model response\npatterns. Building on this, we develop a gradient inversion technique to\nreconstruct latent triggers through activation pattern analysis. Finally, a\nclustering-guided fine-tuning strategy is employed to erase the backdoor\nfunction with only a small amount of arbitrary clean data, while preserving the\noriginal model capabilities. Experimental results show that InverTune reduces\nthe average attack success rate (ASR) by 97.87% against the state-of-the-art\n(SOTA) attacks while limiting clean accuracy (CA) degradation to just 3.07%.\nThis work establishes a new paradigm for securing multimodal systems, advancing\nsecurity in foundation model deployment without compromising performance."}
{"id": "2506.12858", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2506.12858", "abs": "https://arxiv.org/abs/2506.12858", "authors": ["Nick Battle", "Peter Gorm Larsen"], "title": "Towards Operation Proof Obligation Generation for VDM", "comment": "Presented at the 23rd Overture workshop, June 2025\n  (arXiv:cs/2506.08680)", "summary": "All formalisms have the ability to ensure that their models are internally\nconsistent. Potential inconsistencies are generally highlighted by assertions\ncalled proof obligations, and the generation of these obligations is an\nimportant role of the tools that support the method. This capability has been\navailable for VDM tools for many years. However, support for obligation\ngeneration for explicit operation bodies has always been limited. This work\ndescribes the current state of work to address this, showing the capabilities\nso far and highlighting the work remaining."}
{"id": "2506.12430", "categories": ["cs.CR", "cs.CV"], "pdf": "https://arxiv.org/pdf/2506.12430", "abs": "https://arxiv.org/abs/2506.12430", "authors": ["Zonghao Ying", "Siyang Wu", "Run Hao", "Peng Ying", "Shixuan Sun", "Pengyu Chen", "Junze Chen", "Hao Du", "Kaiwen Shen", "Shangkun Wu", "Jiwei Wei", "Shiyuan He", "Yang Yang", "Xiaohai Xu", "Ke Ma", "Qianqian Xu", "Qingming Huang", "Shi Lin", "Xun Wang", "Changting Lin", "Meng Han", "Yilei Jiang", "Siqi Lai", "Yaozhi Zheng", "Yifei Song", "Xiangyu Yue", "Zonglei Jing", "Tianyuan Zhang", "Zhilei Zhu", "Aishan Liu", "Jiakai Wang", "Siyuan Liang", "Xianglong Kong", "Hainan Li", "Junjie Mu", "Haotong Qin", "Yue Yu", "Lei Chen", "Felix Juefei-Xu", "Qing Guo", "Xinyun Chen", "Yew Soon Ong", "Xianglong Liu", "Dawn Song", "Alan Yuille", "Philip Torr", "Dacheng Tao"], "title": "Pushing the Limits of Safety: A Technical Report on the ATLAS Challenge 2025", "comment": null, "summary": "Multimodal Large Language Models (MLLMs) have enabled transformative\nadvancements across diverse applications but remain susceptible to safety\nthreats, especially jailbreak attacks that induce harmful outputs. To\nsystematically evaluate and improve their safety, we organized the Adversarial\nTesting & Large-model Alignment Safety Grand Challenge (ATLAS) 2025}. This\ntechnical report presents findings from the competition, which involved 86\nteams testing MLLM vulnerabilities via adversarial image-text attacks in two\nphases: white-box and black-box evaluations. The competition results highlight\nongoing challenges in securing MLLMs and provide valuable guidance for\ndeveloping stronger defense mechanisms. The challenge establishes new\nbenchmarks for MLLM safety evaluation and lays groundwork for advancing safer\nmultimodal AI systems. The code and data for this challenge are openly\navailable at https://github.com/NY1024/ATLAS_Challenge_2025."}
{"id": "2506.13114", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2506.13114", "abs": "https://arxiv.org/abs/2506.13114", "authors": ["Yanzhou Mu", "Rong Wang", "Juan Zhai", "Chunrong Fang", "Xiang Chen", "Jiacong Wu", "An Guo", "Jiawei Shen", "Bingzhuo Li", "Zhenyu Chen"], "title": "Designing Deep Learning Frameworks for LLMs:Challenges, Expectations, and Opportunities", "comment": "12 pages, 2 figures", "summary": "Large language models (LLMs) drive significant advancements in real industry\napplications. LLMs rely on DL frameworks for efficient model construction,\ndistributed execution, and optimized deployment. Their large parameter scale\nand long execution cycles place extreme demands on DL frameworks in terms of\nscalability, stability, and efficiency. Therefore, poor usability, limited\nfunctionality, and subtle bugs in DL frameworks may hinder development\nefficiency and cause severe failures or resource waste. However, a fundamental\nquestion remains underinvestigated, i.e., What challenges do DL frameworks face\nin supporting LLMs? To seek an answer, we investigate these challenges through\na large-scale analysis of issue reports from three major DL frameworks\n(MindSpore, PyTorch, TensorFlow) and eight associated LLM toolkits (e.g.,\nMegatron). We construct a taxonomy of LLM-centric bugs, requirements, and user\nquestions and enrich it through interviews with 11 LLM users and eight DL\nframework developers, uncovering key technical challenges and misalignments\nbetween user needs and developer priorities. Our contributions are threefold:\n(1) we develop a comprehensive taxonomy comprising four question themes (nine\nsub-themes), four requirement themes (15 sub-themes), and ten bug themes (45\nsub-themes); (2) we assess the perceived importance and priority of these\nchallenges based on practitioner insights; and (3) we identify five key\nfindings across the LLM development and propose five actionable recommendations\nto improve the reliability, usability, and testability of DL frameworks. Our\nresults highlight critical limitations in current DL frameworks and offer\nconcrete guidance for advancing their support for the next generation of LLM\nconstruction and applications."}
{"id": "2506.12466", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2506.12466", "abs": "https://arxiv.org/abs/2506.12466", "authors": ["Alexander Geiger", "Immanuel Hacker", "√ñmer Sen", "Andreas Ulbig"], "title": "Towards Safety and Security Testing of Cyberphysical Power Systems by Shape Validation", "comment": "Accepted to 2025 IEEE International Conference on Cyber Security and\n  Resilience (CSR)", "summary": "The increasing complexity of cyberphysical power systems leads to larger\nattack surfaces to be exploited by malicious actors and a higher risk of faults\nthrough misconfiguration. We propose to meet those risks with a declarative\napproach to describe cyberphysical power systems and to automatically evaluate\nsecurity and safety controls. We leverage Semantic Web technologies as a\nwell-standardized framework, providing languages to specify ontologies, rules\nand shape constraints. We model infrastructure through an ontology which\ncombines external ontologies, architecture and data models for sufficient\nexpressivity and interoperability with external systems. The ontology can\nenrich itself through rules defined in SPARQL, allowing for the inference of\nknowledge that is not explicitly stated. Through the evaluation of SHACL shape\nconstraints we can then validate the data and verify safety and security\nconstraints. We demonstrate this concept with two use cases and illustrate how\nthis solution can be developed further in a community-driven fashion."}
{"id": "2506.13171", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.13171", "abs": "https://arxiv.org/abs/2506.13171", "authors": ["Lukasz Mazur", "Nenad Petrovic", "James Pontes Miranda", "Ansgar Radermacher", "Robert Rasche", "Alois Knoll"], "title": "Querying Large Automotive Software Models: Agentic vs. Direct LLM Approaches", "comment": null, "summary": "Large language models (LLMs) offer new opportunities for interacting with\ncomplex software artifacts, such as software models, through natural language.\nThey present especially promising benefits for large software models that are\ndifficult to grasp in their entirety, making traditional interaction and\nanalysis approaches challenging. This paper investigates two approaches for\nleveraging LLMs to answer questions over software models: direct prompting,\nwhere the whole software model is provided in the context, and an agentic\napproach combining LLM-based agents with general-purpose file access tools. We\nevaluate these approaches using an Ecore metamodel designed for timing analysis\nand software optimization in automotive and embedded domains. Our findings show\nthat while the agentic approach achieves accuracy comparable to direct\nprompting, it is significantly more efficient in terms of token usage. This\nefficiency makes the agentic approach particularly suitable for the automotive\nindustry, where the large size of software models makes direct prompting\ninfeasible, establishing LLM agents as not just a practical alternative but the\nonly viable solution. Notably, the evaluation was conducted using small LLMs,\nwhich are more feasible to be executed locally - an essential advantage for\nmeeting strict requirements around privacy, intellectual property protection,\nand regulatory compliance. Future work will investigate software models in\ndiverse formats, explore more complex agent architectures, and extend agentic\nworkflows to support not only querying but also modification of software\nmodels."}
{"id": "2506.12519", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2506.12519", "abs": "https://arxiv.org/abs/2506.12519", "authors": ["Saskia Laura Schr√∂er", "Luca Pajola", "Alberto Castagnaro", "Giovanni Apruzzese", "Mauro Conti"], "title": "Exploiting AI for Attacks: On the Interplay between Adversarial AI and Offensive AI", "comment": "under submission", "summary": "As Artificial Intelligence (AI) continues to evolve, it has transitioned from\na research-focused discipline to a widely adopted technology, enabling\nintelligent solutions across various sectors. In security, AI's role in\nstrengthening organizational resilience has been studied for over two decades.\nWhile much attention has focused on AI's constructive applications, the\nincreasing maturity and integration of AI have also exposed its darker\npotentials. This article explores two emerging AI-related threats and the\ninterplay between them: AI as a target of attacks (`Adversarial AI') and AI as\na means to launch attacks on any target (`Offensive AI') -- potentially even on\nanother AI. By cutting through the confusion and explaining these threats in\nplain terms, we introduce the complex and often misunderstood interplay between\nAdversarial AI and Offensive AI, offering a clear and accessible introduction\nto the challenges posed by these threats."}
{"id": "2506.13182", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.13182", "abs": "https://arxiv.org/abs/2506.13182", "authors": ["Anh Ho", "Thanh Le-Cong", "Bach Le", "Christine Rizkallah"], "title": "From Empirical Evaluation to Context-Aware Enhancement: Repairing Regression Errors with LLMs", "comment": null, "summary": "[...] Since then, various APR approaches, especially those leveraging the\npower of large language models (LLMs), have been rapidly developed to fix\ngeneral software bugs. Unfortunately, the effectiveness of these advanced\ntechniques in the context of regression bugs remains largely unexplored. This\ngap motivates the need for an empirical study evaluating the effectiveness of\nmodern APR techniques in fixing real-world regression bugs.\n  In this work, we conduct an empirical study of APR techniques on Java\nregression bugs. To facilitate our study, we introduce RegMiner4APR, a\nhigh-quality benchmark of Java regression bugs integrated into a framework\ndesigned to facilitate APR research. The current benchmark includes 99\nregression bugs collected from 32 widely used real-world Java GitHub\nrepositories. We begin by conducting an in-depth analysis of the benchmark,\ndemonstrating its diversity and quality. Building on this foundation, we\nempirically evaluate the capabilities of APR to regression bugs by assessing\nboth traditional APR tools and advanced LLM-based APR approaches. Our\nexperimental results show that classical APR tools fail to repair any bugs,\nwhile LLM-based APR approaches exhibit promising potential. Motivated by these\nresults, we investigate impact of incorporating bug-inducing change information\ninto LLM-based APR approaches for fixing regression bugs. Our results highlight\nthat this context-aware enhancement significantly improves the performance of\nLLM-based APR, yielding 1.8x more successful repairs compared to using\nLLM-based APR without such context."}
{"id": "2506.12522", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2506.12522", "abs": "https://arxiv.org/abs/2506.12522", "authors": ["Marco Arazzi", "Antonino Nocera", "Vinod P"], "title": "When Forgetting Triggers Backdoors: A Clean Unlearning Attack", "comment": null, "summary": "Machine unlearning has emerged as a key component in ensuring ``Right to be\nForgotten'', enabling the removal of specific data points from trained models.\nHowever, even when the unlearning is performed without poisoning the forget-set\n(clean unlearning), it can be exploited for stealthy attacks that existing\ndefenses struggle to detect. In this paper, we propose a novel {\\em clean}\nbackdoor attack that exploits both the model learning phase and the subsequent\nunlearning requests. Unlike traditional backdoor methods, during the first\nphase, our approach injects a weak, distributed malicious signal across\nmultiple classes. The real attack is then activated and amplified by\nselectively unlearning {\\em non-poisoned} samples. This strategy results in a\npowerful and stealthy novel attack that is hard to detect or mitigate,\nhighlighting critical vulnerabilities in current unlearning mechanisms and\nhighlighting the need for more robust defenses."}
{"id": "2506.13186", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2506.13186", "abs": "https://arxiv.org/abs/2506.13186", "authors": ["Jiajun Sun", "Fengjie Li", "Xinzhu Qi", "Hongyu Zhang", "Jiajun Jiang"], "title": "Empirical Evaluation of Large Language Models in Automated Program Repair", "comment": null, "summary": "The increasing prevalence of software bugs has made automated program repair\n(APR) a key research focus. Large language models (LLMs) offer new\nopportunities for APR, but existing studies mostly rely on smaller,\nearlier-generation models and Java benchmarks. The repair capabilities of\nmodern, large-scale LLMs across diverse languages and scenarios remain\nunderexplored. To address this, we conduct a comprehensive empirical study of\nfour open-source LLMs, CodeLlama, LLaMA, StarCoder, and DeepSeek-Coder,\nspanning 7B to 33B parameters, diverse architectures, and purposes. We evaluate\nthem across two bug scenarios (enterprise-grades and algorithmic), three\nlanguages (Java, C/C++, Python), and four prompting strategies, analyzing over\n600K generated patches on six benchmarks. Key findings include: (1) model\nspecialization (e.g., CodeLlama) can outperform larger general-purpose models\n(e.g., LLaMA); (2) repair performance does not scale linearly with model size;\n(3) correct patches often appear early in generation; and (4) prompts\nsignificantly affect results. These insights offer practical guidance for\ndesigning effective and efficient LLM-based APR systems."}
{"id": "2506.12523", "categories": ["cs.CR", "cs.ET", "C.2.4; D.2.11; K.6.5"], "pdf": "https://arxiv.org/pdf/2506.12523", "abs": "https://arxiv.org/abs/2506.12523", "authors": ["Matteo Marco Montanari", "Alessandro Aldini"], "title": "Privacy-preserving and reward-based mechanisms of proof of engagement", "comment": null, "summary": "Proof-of-Attendance (PoA) mechanisms are typically employed to demonstrate a\nspecific user's participation in an event, whether virtual or in-person. The\ngoal of this study is to extend such mechanisms to broader contexts where the\nuser wishes to digitally demonstrate her involvement in a specific activity\n(Proof-of-Engagement, PoE). This work explores different solutions, including\nDLTs as well as established technologies based on centralized systems. The main\naspects we consider include the level of privacy guaranteed to users, the scope\nof PoA/PoE (both temporal and spatial), the transferability of the proof, and\nthe integration with incentive mechanisms."}
{"id": "2506.13273", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2506.13273", "abs": "https://arxiv.org/abs/2506.13273", "authors": ["Charaka Geethal Kapugama"], "title": "Isolating Noisy Labelled Test Cases in Human-in-the-Loop Oracle Learning", "comment": "2025 International Research Conference on Smart Computing and Systems\n  Engineering (SCSE)", "summary": "Incorrectly labelled test cases can adversely affect the training process of\nhuman-in-the-loop oracle learning tech-niques. This paper introduces ISONOISE,\na technique designed to identify such mislabelled test cases introduced during\nhuman-in-the-loop oracle learning. This technique can be applied to programs\ntaking numeric inputs. Given a compromised automatic test oracle and its\ntraining test suite, ISONOISE first isolates thetest cases suspected of being\nmislabelled. This task is performed based on the level of disagreement of a\ntest case with respect to the others. An intermediate automatic test oracle is\ntrained based on the slightly disagreeing test cases. Based on the predictions\nof this intermediate oracle, the test cases suspected of being mislabelled are\nsystematically presented for relabelling. When mislabelled test cases are\nfound, the intermediate test oracle is updated. This process repeats until no\nmislabelled test case is found in relabelling. ISONOISE was evaluated within\nthe human-in-the-loop oracle learning method used in LEARN2FIX. Experimental\nresults demonstrate that ISONOISE can identify mislabelled test cases\nintroduced by the human in LEARN2FIX with over 67% accuracy, while requiring\nonly a small number of relabelling queries. These findings highlight the\npotential of ISONOISE to enhance the reliability of human-in-the-loop oracle\nlearning."}
{"id": "2506.12551", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.12551", "abs": "https://arxiv.org/abs/2506.12551", "authors": ["Jingxuan Zhang", "Zhenhua Xu", "Rui Hu", "Wenpeng Xing", "Xuhong Zhang", "Meng Han"], "title": "MEraser: An Effective Fingerprint Erasure Approach for Large Language Models", "comment": "Accepted by ACL 2025, Main Conference, Long Paper", "summary": "Large Language Models (LLMs) have become increasingly prevalent across\nvarious sectors, raising critical concerns about model ownership and\nintellectual property protection. Although backdoor-based fingerprinting has\nemerged as a promising solution for model authentication, effective attacks for\nremoving these fingerprints remain largely unexplored. Therefore, we present\nMismatched Eraser (MEraser), a novel method for effectively removing\nbackdoor-based fingerprints from LLMs while maintaining model performance. Our\napproach leverages a two-phase fine-tuning strategy utilizing carefully\nconstructed mismatched and clean datasets. Through extensive evaluation across\nmultiple LLM architectures and fingerprinting methods, we demonstrate that\nMEraser achieves complete fingerprinting removal while maintaining model\nperformance with minimal training data of fewer than 1,000 samples.\nFurthermore, we introduce a transferable erasure mechanism that enables\neffective fingerprinting removal across different models without repeated\ntraining. In conclusion, our approach provides a practical solution for\nfingerprinting removal in LLMs, reveals critical vulnerabilities in current\nfingerprinting techniques, and establishes comprehensive evaluation benchmarks\nfor developing more resilient model protection methods in the future."}
{"id": "2506.13303", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2506.13303", "abs": "https://arxiv.org/abs/2506.13303", "authors": ["Julian Frattini", "Anja Frattini"], "title": "Adopting Use Case Descriptions for Requirements Specification: an Industrial Case Study", "comment": null, "summary": "Context: Use case (UC) descriptions are a prominent format for specifying\nfunctional requirements. Existing literature abounds with recommendations on\nhow to write high-quality UC descriptions but lacks insights into (1) their\nreal-world adoption, (2) whether these recommendations correspond to actual\nquality, and (3) which factors influence the quality of UCs. Objectives: We aim\nto contribute empirical evidence about the adoption of UC descriptions in a\nlarge, globally distributed case company. Methods: We surveyed 1188 business\nrequirements of a case company that were elicited from 2020-01-01 until\n2024-12-31 and contained 1192 UCs in various forms. Among these, we manually\nevaluated the 273 template-style UC descriptions against established quality\nguidelines. We generated descriptive statistics of the format's adoption over\nthe surveyed time frame. Furthermore, we used inferential statistics to\ndetermine (a) how properties of the requirements engineering process affected\nthe UC quality and (b) how UC quality affects subsequent software development\nactivities. Results and Conclusions: Our descriptive results show how the\nadoption of UC descriptions in practice deviates from textbook recommendations.\nHowever, our inferential results suggest that only a few phenomena like\nsolution-orientation show an actual impact in practice. These results can steer\nUC quality research into a more relevant direction."}
{"id": "2506.12580", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2506.12580", "abs": "https://arxiv.org/abs/2506.12580", "authors": ["Wenjie Liu", "Panos Papadimitratos"], "title": "GNSS Spoofing Detection Based on Opportunistic Position Information", "comment": null, "summary": "The limited or no protection for civilian Global Navigation Satellite System\n(GNSS) signals makes spoofing attacks relatively easy. With modern mobile\ndevices often featuring network interfaces, state-of-the-art signals of\nopportunity (SOP) schemes can provide accurate network positions in replacement\nof GNSS. The use of onboard inertial sensors can also assist in the absence of\nGNSS, possibly in the presence of jammers. The combination of SOP and inertial\nsensors has received limited attention, yet it shows strong results on fully\ncustom-built platforms. We do not seek to improve such special-purpose schemes.\nRather, we focus on countering GNSS attacks, notably detecting them, with\nemphasis on deployment with consumer-grade platforms, notably smartphones, that\nprovide off-the-shelf opportunistic information (i.e., network position and\ninertial sensor data). Our Position-based Attack Detection Scheme (PADS) is a\nprobabilistic framework that uses regression and uncertainty analysis for\npositions. The regression optimization problem is a weighted mean square error\nof polynomial fitting, with constraints that the fitted positions satisfy the\ndevice velocity and acceleration. Then, uncertainty is modeled by a Gaussian\nprocess, which provides more flexibility to analyze how sure or unsure we are\nabout position estimations. In the detection process, we combine all\nuncertainty information with the position estimations into a fused test\nstatistic, which is the input utilized by an anomaly detector based on outlier\nensembles. The evaluation shows that the PADS outperforms a set of baseline\nmethods that rely on SOP or inertial sensor-based or statistical tests,\nachieving up to 3 times the true positive rate at a low false positive rate."}
{"id": "2506.13538", "categories": ["cs.SE", "cs.ET"], "pdf": "https://arxiv.org/pdf/2506.13538", "abs": "https://arxiv.org/abs/2506.13538", "authors": ["Mohammed Mehedi Hasan", "Hao Li", "Emad Fallahzadeh", "Bram Adams", "Ahmed E. Hassan"], "title": "Model Context Protocol (MCP) at First Glance: Studying the Security and Maintainability of MCP Servers", "comment": null, "summary": "Although Foundation Models (FMs), such as GPT-4, are increasingly used in\ndomains like finance and software engineering, reliance on textual interfaces\nlimits these models' real-world interaction. To address this, FM providers\nintroduced tool calling-triggering a proliferation of frameworks with distinct\ntool interfaces. In late 2024, Anthropic introduced the Model Context Protocol\n(MCP) to standardize this tool ecosystem, which has become the de facto\nstandard with over eight million weekly SDK downloads. Despite its adoption,\nMCP's AI-driven, non-deterministic control flow introduces new risks to\nsustainability, security, and maintainability, warranting closer examination.\n  Towards this end, we present the first large-scale empirical study of MCP.\nUsing state-of-the-art health metrics and a hybrid analysis pipeline, combining\na general-purpose static analysis tool with an MCP-specific scanner, we\nevaluate 1,899 open-source MCP servers to assess their health, security, and\nmaintainability. Despite MCP servers demonstrating strong health metrics, we\nidentify eight distinct vulnerabilities-only three overlapping with traditional\nsoftware vulnerabilities. Additionally, 7.2% of servers contain general\nvulnerabilities and 5.5% exhibit MCP-specific tool poisoning. Regarding\nmaintainability, while 66% exhibit code smells, 14.4% contain ten bug patterns\noverlapping prior research. These findings highlight the need for MCP-specific\nvulnerability detection techniques while reaffirming the value of traditional\nanalysis and refactoring practices."}
{"id": "2506.12675", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2506.12675", "abs": "https://arxiv.org/abs/2506.12675", "authors": ["Limengnan Zhou", "Hanzhou Wu"], "title": "Watermarking Quantum Neural Networks Based on Sample Grouped and Paired Training", "comment": null, "summary": "Quantum neural networks (QNNs) leverage quantum computing to create powerful\nand efficient artificial intelligence models capable of solving complex\nproblems significantly faster than traditional computers. With the fast\ndevelopment of quantum hardware technology, such as superconducting qubits,\ntrapped ions, and integrated photonics, quantum computers may become reality,\naccelerating the applications of QNNs. However, preparing quantum circuits and\noptimizing parameters for QNNs require quantum hardware support, expertise, and\nhigh-quality data. How to protect intellectual property (IP) of QNNs becomes an\nurgent problem to be solved in the era of quantum computing. We make the first\nattempt towards IP protection of QNNs by watermarking. To this purpose, we\ncollect classical clean samples and trigger ones, each of which is generated by\nadding a perturbation to a clean sample, associated with a label different from\nthe ground-truth one. The host QNN, consisting of quantum encoding, quantum\nstate transformation, and quantum measurement, is then trained from scratch\nwith the clean samples and trigger ones, resulting in a watermarked QNN model.\nDuring training, we introduce sample grouped and paired training to ensure that\nthe performance on the downstream task can be maintained while achieving good\nperformance for watermark extraction. When disputes arise, by collecting a\nmini-set of trigger samples, the hidden watermark can be extracted by analyzing\nthe prediction results of the target model corresponding to the trigger\nsamples, without accessing the internal details of the target QNN model,\nthereby verifying the ownership of the model. Experiments have verified the\nsuperiority and applicability of this work."}
{"id": "2506.13663", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2506.13663", "abs": "https://arxiv.org/abs/2506.13663", "authors": ["Yunnong Chen", "Shixian Ding", "YingYing Zhang", "Wenkai Chen", "Jinzhou Du", "Lingyun Sun", "Liuqing Chen"], "title": "DesignCoder: Hierarchy-Aware and Self-Correcting UI Code Generation with Large Language Models", "comment": "11 pages,6 figures", "summary": "Multimodal large language models (MLLMs) have streamlined front-end interface\ndevelopment by automating code generation. However, these models also introduce\nchallenges in ensuring code quality. Existing approaches struggle to maintain\nboth visual consistency and functional completeness in the generated\ncomponents. Moreover, they lack mechanisms to assess the fidelity and\ncorrectness of the rendered pages. To address these issues, we propose\nDesignCoder, a novel hierarchical-aware and self-correcting automated code\ngeneration framework. Specifically, we introduce UI Grouping Chains, which\nenhance MLLMs' capability to understand and predict complex nested UI\nhierarchies. Subsequently, DesignCoder employs a hierarchical\ndivide-and-conquer approach to generate front-end code. Finally, we incorporate\na self-correction mechanism to improve the model's ability to identify and\nrectify errors in the generated code. Extensive evaluations on a dataset of UI\nmockups collected from both open-source communities and industry projects\ndemonstrate that DesignCoder outperforms state-of-the-art baselines in React\nNative, a widely adopted UI framework. Our method achieves a 37.63%, 9.52%,\n12.82% performance increase in visual similarity metrics (MSE, CLIP, SSIM) and\nsignificantly improves code structure similarity in terms of TreeBLEU,\nContainer Match, and Tree Edit Distance by 30.19%, 29.31%, 24.67%. Furthermore,\nwe conducted a user study with professional developers to assess the quality\nand practicality of the generated code. Results indicate that DesignCoder\naligns with industry best practices, demonstrating high usability, readability,\nand maintainability. Our approach provides an efficient and practical solution\nfor agile front-end development, enabling development teams to focus more on\ncore functionality and product innovation."}
{"id": "2506.12685", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.12685", "abs": "https://arxiv.org/abs/2506.12685", "authors": ["Bilal Saleh Husain"], "title": "Alphabet Index Mapping: Jailbreaking LLMs through Semantic Dissimilarity", "comment": "10 pages, 2 figures, 3 tables", "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities, yet\ntheir susceptibility to adversarial attacks, particularly jailbreaking, poses\nsignificant safety and ethical concerns. While numerous jailbreak methods\nexist, many suffer from computational expense, high token usage, or complex\ndecoding schemes. Liu et al. (2024) introduced FlipAttack, a black-box method\nthat achieves high attack success rates (ASR) through simple prompt\nmanipulation. This paper investigates the underlying mechanisms of FlipAttack's\neffectiveness by analyzing the semantic changes induced by its flipping modes.\nWe hypothesize that semantic dissimilarity between original and manipulated\nprompts is inversely correlated with ASR. To test this, we examine embedding\nspace visualizations (UMAP, KDE) and cosine similarities for FlipAttack's\nmodes. Furthermore, we introduce a novel adversarial attack, Alphabet Index\nMapping (AIM), designed to maximize semantic dissimilarity while maintaining\nsimple decodability. Experiments on GPT-4 using a subset of AdvBench show AIM\nand its variant AIM+FWO achieve a 94% ASR, outperforming FlipAttack and other\nmethods on this subset. Our findings suggest that while high semantic\ndissimilarity is crucial, a balance with decoding simplicity is key for\nsuccessful jailbreaking. This work contributes to a deeper understanding of\nadversarial prompt mechanics and offers a new, effective jailbreak technique."}
{"id": "2506.13161", "categories": ["cs.CR", "cs.SE"], "pdf": "https://arxiv.org/pdf/2506.13161", "abs": "https://arxiv.org/abs/2506.13161", "authors": ["Bayu Fedra Abdullah", "Yusuf Sulistyo Nugroho", "Brittany Reid", "Raula Gaikovina Kula", "Kazumasa Shimari", "Kenichi Matsumoto"], "title": "Using LLMs for Security Advisory Investigations: How Far Are We?", "comment": "6 pages, 6 figures, 8 tables, conference paper", "summary": "Large Language Models (LLMs) are increasingly used in software security, but\ntheir trustworthiness in generating accurate vulnerability advisories remains\nuncertain. This study investigates the ability of ChatGPT to (1) generate\nplausible security advisories from CVE-IDs, (2) differentiate real from fake\nCVE-IDs, and (3) extract CVE-IDs from advisory descriptions. Using a curated\ndataset of 100 real and 100 fake CVE-IDs, we manually analyzed the credibility\nand consistency of the model's outputs. The results show that ChatGPT generated\nplausible security advisories for 96% of given input real CVE-IDs and 97% of\ngiven input fake CVE-IDs, demonstrating a limitation in differentiating between\nreal and fake IDs. Furthermore, when these generated advisories were\nreintroduced to ChatGPT to identify their original CVE-ID, the model produced a\nfake CVE-ID in 6% of cases from real advisories. These findings highlight both\nthe strengths and limitations of ChatGPT in cybersecurity applications. While\nthe model demonstrates potential for automating advisory generation, its\ninability to reliably authenticate CVE-IDs or maintain consistency upon\nre-evaluation underscores the risks associated with its deployment in critical\nsecurity tasks. Our study emphasizes the importance of using LLMs with caution\nin cybersecurity workflows and suggests the need for further improvements in\ntheir design to improve reliability and applicability in security advisory\ngeneration."}
{"id": "2506.12699", "categories": ["cs.CR", "cs.HC"], "pdf": "https://arxiv.org/pdf/2506.12699", "abs": "https://arxiv.org/abs/2506.12699", "authors": ["Yashothara Shanmugarasa", "Ming Ding", "M. A. P Chamikara", "Thierry Rakotoarivelo"], "title": "SoK: The Privacy Paradox of Large Language Models: Advancements, Privacy Risks, and Mitigation", "comment": null, "summary": "Large language models (LLMs) are sophisticated artificial intelligence\nsystems that enable machines to generate human-like text with remarkable\nprecision. While LLMs offer significant technological progress, their\ndevelopment using vast amounts of user data scraped from the web and collected\nfrom extensive user interactions poses risks of sensitive information leakage.\nMost existing surveys focus on the privacy implications of the training data\nbut tend to overlook privacy risks from user interactions and advanced LLM\ncapabilities. This paper aims to fill that gap by providing a comprehensive\nanalysis of privacy in LLMs, categorizing the challenges into four main areas:\n(i) privacy issues in LLM training data, (ii) privacy challenges associated\nwith user prompts, (iii) privacy vulnerabilities in LLM-generated outputs, and\n(iv) privacy challenges involving LLM agents. We evaluate the effectiveness and\nlimitations of existing mitigation mechanisms targeting these proposed privacy\nchallenges and identify areas for further research."}
{"id": "2506.13323", "categories": ["cs.CR", "cs.AI", "cs.LG", "cs.SE"], "pdf": "https://arxiv.org/pdf/2506.13323", "abs": "https://arxiv.org/abs/2506.13323", "authors": ["Siliang Qin", "Fengrui Yang", "Hao Wang", "Bolun Zhang", "Zeyu Gao", "Chao Zhang", "Kai Chen"], "title": "Tady: A Neural Disassembler without Structural Constraint Violations", "comment": "Usenix Security'25", "summary": "Disassembly is a crucial yet challenging step in binary analysis. While\nemerging neural disassemblers show promise for efficiency and accuracy, they\nfrequently generate outputs violating fundamental structural constraints, which\nsignificantly compromise their practical usability. To address this critical\nproblem, we regularize the disassembly solution space by formalizing and\napplying key structural constraints based on post-dominance relations. This\napproach systematically detects widespread errors in existing neural\ndisassemblers' outputs. These errors often originate from models' limited\ncontext modeling and instruction-level decoding that neglect global structural\nintegrity. We introduce Tady, a novel neural disassembler featuring an improved\nmodel architecture and a dedicated post-processing algorithm, specifically\nengineered to address these deficiencies. Comprehensive evaluations on diverse\nbinaries demonstrate that Tady effectively eliminates structural constraint\nviolations and functions with high efficiency, while maintaining\ninstruction-level accuracy."}
{"id": "2506.12707", "categories": ["cs.CR", "cs.CL"], "pdf": "https://arxiv.org/pdf/2506.12707", "abs": "https://arxiv.org/abs/2506.12707", "authors": ["Yucheng Li", "Surin Ahn", "Huiqiang Jiang", "Amir H. Abdi", "Yuqing Yang", "Lili Qiu"], "title": "SecurityLingua: Efficient Defense of LLM Jailbreak Attacks via Security-Aware Prompt Compression", "comment": null, "summary": "Large language models (LLMs) have achieved widespread adoption across\nnumerous applications. However, many LLMs are vulnerable to malicious attacks\neven after safety alignment. These attacks typically bypass LLMs' safety\nguardrails by wrapping the original malicious instructions inside adversarial\njailbreaks prompts. Previous research has proposed methods such as adversarial\ntraining and prompt rephrasing to mitigate these safety vulnerabilities, but\nthese methods often reduce the utility of LLMs or lead to significant\ncomputational overhead and online latency. In this paper, we propose\nSecurityLingua, an effective and efficient approach to defend LLMs against\njailbreak attacks via security-oriented prompt compression. Specifically, we\ntrain a prompt compressor designed to discern the \"true intention\" of the input\nprompt, with a particular focus on detecting the malicious intentions of\nadversarial prompts. Then, in addition to the original prompt, the intention is\npassed via the system prompt to the target LLM to help it identify the true\nintention of the request. SecurityLingua ensures a consistent user experience\nby leaving the original input prompt intact while revealing the user's\npotentially malicious intention and stimulating the built-in safety guardrails\nof the LLM. Moreover, thanks to prompt compression, SecurityLingua incurs only\na negligible overhead and extra token cost compared to all existing defense\nmethods, making it an especially practical solution for LLM defense.\nExperimental results demonstrate that SecurityLingua can effectively defend\nagainst malicious attacks and maintain utility of the LLM with negligible\ncompute and latency overhead. Our code is available at\nhttps://aka.ms/SecurityLingua."}
{"id": "2506.12761", "categories": ["cs.CR", "cs.IR"], "pdf": "https://arxiv.org/pdf/2506.12761", "abs": "https://arxiv.org/abs/2506.12761", "authors": ["Joon Soo Yoo", "Taeho Kim", "Ji Won Yoon"], "title": "Versatile and Fast Location-Based Private Information Retrieval with Fully Homomorphic Encryption over the Torus", "comment": null, "summary": "Location-based services often require users to share sensitive locational\ndata, raising privacy concerns due to potential misuse or exploitation by\nuntrusted servers. In response, we present VeLoPIR, a versatile location-based\nprivate information retrieval (PIR) system designed to preserve user privacy\nwhile enabling efficient and scalable query processing. VeLoPIR introduces\nthree operational modes-interval validation, coordinate validation, and\nidentifier matching-that support a broad range of real-world applications,\nincluding information and emergency alerts. To enhance performance, VeLoPIR\nincorporates multi-level algorithmic optimizations with parallel structures,\nachieving significant scalability across both CPU and GPU platforms. We also\nprovide formal security and privacy proofs, confirming the system's robustness\nunder standard cryptographic assumptions. Extensive experiments on real-world\ndatasets demonstrate that VeLoPIR achieves up to 11.55 times speed-up over a\nprior baseline. The implementation of VeLoPIR is publicly available at\nhttps://github.com/PrivStatBool/VeLoPIR."}
{"id": "2506.12802", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2506.12802", "abs": "https://arxiv.org/abs/2506.12802", "authors": ["Joon Soo Yoo", "Tae Min Ahn", "Ji Won Yoon"], "title": "Bidirectional Biometric Authentication Using Transciphering and (T)FHE", "comment": null, "summary": "Biometric authentication systems pose privacy risks, as leaked templates such\nas iris or fingerprints can lead to security breaches. Fully Homomorphic\nEncryption (FHE) enables secure encrypted evaluation, but its deployment is\nhindered by large ciphertexts, high key overhead, and limited trust models. We\npropose the Bidirectional Transciphering Framework (BTF), combining FHE,\ntransciphering, and a non-colluding trusted party to enable efficient and\nprivacy-preserving biometric authentication. The key architectural innovation\nis the introduction of a trusted party that assists in evaluation and key\nmanagement, along with a double encryption mechanism to preserve the FHE trust\nmodel, where client data remains private. BTF addresses three core deployment\nchallenges: reducing the size of returned FHE ciphertexts, preventing clients\nfrom falsely reporting successful authentication, and enabling scalable,\ncentralized FHE key management. We implement BTF using TFHE and the Trivium\ncipher, and evaluate it on iris-based biometric data. Our results show up to a\n121$\\times$ reduction in transmission size compared to standard FHE models,\ndemonstrating practical scalability and deployment potential."}
{"id": "2506.12846", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.12846", "abs": "https://arxiv.org/abs/2506.12846", "authors": ["Nina Cai", "Jinguang Han"], "title": "Privacy-Preserving Federated Learning against Malicious Clients Based on Verifiable Functional Encryption", "comment": null, "summary": "Federated learning is a promising distributed learning paradigm that enables\ncollaborative model training without exposing local client data, thereby\nprotect data privacy. However, it also brings new threats and challenges. The\nadvancement of model inversion attacks has rendered the plaintext transmission\nof local models insecure, while the distributed nature of federated learning\nmakes it particularly vulnerable to attacks raised by malicious clients. To\nprotect data privacy and prevent malicious client attacks, this paper proposes\na privacy-preserving federated learning framework based on verifiable\nfunctional encryption, without a non-colluding dual-server setup or additional\ntrusted third-party. Specifically, we propose a novel decentralized verifiable\nfunctional encryption (DVFE) scheme that enables the verification of specific\nrelationships over multi-dimensional ciphertexts. This scheme is formally\ntreated, in terms of definition, security model and security proof.\nFurthermore, based on the proposed DVFE scheme, we design a privacy-preserving\nfederated learning framework VFEFL that incorporates a novel robust aggregation\nrule to detect malicious clients, enabling the effective training of\nhigh-accuracy models under adversarial settings. Finally, we provide formal\nanalysis and empirical evaluation of the proposed schemes. The results\ndemonstrate that our approach achieves the desired privacy protection,\nrobustness, verifiability and fidelity, while eliminating the reliance on\nnon-colluding dual-server settings or trusted third parties required by\nexisting methods."}
{"id": "2506.12880", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2506.12880", "abs": "https://arxiv.org/abs/2506.12880", "authors": ["Matan Ben-Tov", "Mor Geva", "Mahmood Sharif"], "title": "Universal Jailbreak Suffixes Are Strong Attention Hijackers", "comment": null, "summary": "We study suffix-based jailbreaks$\\unicode{x2013}$a powerful family of attacks\nagainst large language models (LLMs) that optimize adversarial suffixes to\ncircumvent safety alignment. Focusing on the widely used foundational GCG\nattack (Zou et al., 2023), we observe that suffixes vary in efficacy: some\nmarkedly more universal$\\unicode{x2013}$generalizing to many unseen harmful\ninstructions$\\unicode{x2013}$than others. We first show that GCG's\neffectiveness is driven by a shallow, critical mechanism, built on the\ninformation flow from the adversarial suffix to the final chat template tokens\nbefore generation. Quantifying the dominance of this mechanism during\ngeneration, we find GCG irregularly and aggressively hijacks the\ncontextualization process. Crucially, we tie hijacking to the universality\nphenomenon, with more universal suffixes being stronger hijackers.\nSubsequently, we show that these insights have practical implications: GCG\nuniversality can be efficiently enhanced (up to $\\times$5 in some cases) at no\nadditional computational cost, and can also be surgically mitigated, at least\nhalving attack success with minimal utility loss. We release our code and data\nat http://github.com/matanbt/interp-jailbreak."}
{"id": "2506.12883", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2506.12883", "abs": "https://arxiv.org/abs/2506.12883", "authors": ["Julien de Castelnau", "Mingfei Yu", "Giovanni De Micheli"], "title": "Cut Tracing with E-Graphs for Boolean FHE Circuit Synthesis", "comment": "7 pages, 5 figures, presented to EGRAPHS25 workshop, not in\n  conference proceedings", "summary": "Fully Homomorphic Encryption (FHE) is a promising privacy-preserving\ntechnology enabling secure computation over encrypted data. A major limitation\nof current FHE schemes is their high runtime overhead. As a result, automatic\noptimization of circuits describing FHE computation has garnered significant\nattention in the logic synthesis community. Existing works primarily target the\nmultiplicative depth (MD) and multiplicative complexity (MC) of FHE circuits,\ncorresponding to the total number of multiplications and maximum number of\nmultiplications in a path from primary input to output, respectively. In many\nFHE schemes, these metrics are the primary contributors to the homomorphic\nevaluation runtime of a circuit. However, oftentimes they are opposed: reducing\neither depth or complexity may result in an increase in the other. To our\nknowledge, existing works have yet to optimize FHE circuits for overall\nruntime, only considering one metric at a time and thus making significant\ntradeoffs. In this paper, we use e-graphs to augment existing flows that\nindividually optimize MC and MD, in a technique called cut tracing. We show how\ncut tracing can effectively combine two state-of-the-art MC and MD reduction\nflows and balance their weaknesses to minimize runtime. Our preliminary results\ndemonstrate that cut tracing yields up to a 40% improvement in homomorphic\nevaluation runtime when applied to these two flows."}
{"id": "2506.12995", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2506.12995", "abs": "https://arxiv.org/abs/2506.12995", "authors": ["Seyed Ali Akhavani", "Behzad Ousat", "Amin Kharraz"], "title": "Open Source, Open Threats? Investigating Security Challenges in Open-Source Software", "comment": null, "summary": "Open-source software (OSS) has become increasingly more popular across\ndifferent domains. However, this rapid development and widespread adoption come\nwith a security cost. The growing complexity and openness of OSS ecosystems\nhave led to increased exposure to vulnerabilities and attack surfaces. This\npaper investigates the trends and patterns of reported vulnerabilities within\nOSS platforms, focusing on the implications of these findings for security\npractices. To understand the dynamics of OSS vulnerabilities, we analyze a\ncomprehensive dataset comprising 31,267 unique vulnerability reports from\nGitHub's advisory database and Snyk.io, belonging to 14,675 packages across 10\nprogramming languages. Our analysis reveals a significant surge in reported\nvulnerabilities, increasing at an annual rate of 98%, far outpacing the 25%\naverage annual growth in the number of open-source software (OSS) packages.\nAdditionally, we observe an 85% increase in the average lifespan of\nvulnerabilities across ecosystems during the studied period, indicating a\npotential decline in security. We identify the most prevalent Common Weakness\nEnumerations (CWEs) across programming languages and find that, on average,\njust seven CWEs are responsible for over 50% of all reported vulnerabilities.\nWe further examine these commonly observed CWEs and highlight\necosystem-specific trends. Notably, we find that vulnerabilities associated\nwith intentionally malicious packages comprise 49% of reports in the NPM\necosystem and 14% in PyPI, an alarming indication of targeted attacks within\npackage repositories. We conclude with an in-depth discussion of the\ncharacteristics and attack vectors associated with these malicious packages."}
{"id": "2506.13009", "categories": ["cs.CR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.13009", "abs": "https://arxiv.org/abs/2506.13009", "authors": ["Nima Naderloui", "Shenao Yan", "Binghui Wang", "Jie Fu", "Wendy Hui Wang", "Weiran Liu", "Yuan Hong"], "title": "Rectifying Privacy and Efficacy Measurements in Machine Unlearning: A New Inference Attack Perspective", "comment": "To appear in USENIX Security '25", "summary": "Machine unlearning focuses on efficiently removing specific data from trained\nmodels, addressing privacy and compliance concerns with reasonable costs.\nAlthough exact unlearning ensures complete data removal equivalent to\nretraining, it is impractical for large-scale models, leading to growing\ninterest in inexact unlearning methods. However, the lack of formal guarantees\nin these methods necessitates the need for robust evaluation frameworks to\nassess their privacy and effectiveness. In this work, we first identify several\nkey pitfalls of the existing unlearning evaluation frameworks, e.g., focusing\non average-case evaluation or targeting random samples for evaluation,\nincomplete comparisons with the retraining baseline. Then, we propose RULI\n(Rectified Unlearning Evaluation Framework via Likelihood Inference), a novel\nframework to address critical gaps in the evaluation of inexact unlearning\nmethods. RULI introduces a dual-objective attack to measure both unlearning\nefficacy and privacy risks at a per-sample granularity. Our findings reveal\nsignificant vulnerabilities in state-of-the-art unlearning methods, where RULI\nachieves higher attack success rates, exposing privacy risks underestimated by\nexisting methods. Built on a game-based foundation and validated through\nempirical evaluations on both image and text data (spanning tasks from\nclassification to generation), RULI provides a rigorous, scalable, and\nfine-grained methodology for evaluating unlearning techniques."}
{"id": "2506.13024", "categories": ["cs.CR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.13024", "abs": "https://arxiv.org/abs/2506.13024", "authors": ["Andrew C. Cullen", "Paul Montague", "Sarah M. Erfani", "Benjamin I. P. Rubinstein"], "title": "Position: Certified Robustness Does Not (Yet) Imply Model Security", "comment": "9 pages, ICML, 2025", "summary": "While certified robustness is widely promoted as a solution to adversarial\nexamples in Artificial Intelligence systems, significant challenges remain\nbefore these techniques can be meaningfully deployed in real-world\napplications. We identify critical gaps in current research, including the\nparadox of detection without distinction, the lack of clear criteria for\npractitioners to evaluate certification schemes, and the potential security\nrisks arising from users' expectations surrounding ``guaranteed\" robustness\nclaims. This position paper is a call to arms for the certification research\ncommunity, proposing concrete steps to address these fundamental challenges and\nadvance the field toward practical applicability."}
{"id": "2506.13052", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2506.13052", "abs": "https://arxiv.org/abs/2506.13052", "authors": ["Steven Su", "Erik Rye", "Robert Beverly", "Dave Levin"], "title": "Buy it Now, Track Me Later: Attacking User Privacy via Wi-Fi AP Online Auctions", "comment": null, "summary": "Static and hard-coded layer-two network identifiers are well known to present\nsecurity vulnerabilities and endanger user privacy. In this work, we introduce\na new privacy attack against Wi-Fi access points listed on secondhand\nmarketplaces. Specifically, we demonstrate the ability to remotely gather a\nlarge quantity of layer-two Wi-Fi identifiers by programmatically querying the\neBay marketplace and applying state-of-the-art computer vision techniques to\nextract IEEE 802.11 BSSIDs from the seller's posted images of the hardware. By\nleveraging data from a global Wi-Fi Positioning System (WPS) that geolocates\nBSSIDs, we obtain the physical locations of these devices both pre- and\npost-sale. In addition to validating the degree to which a seller's location\nmatches the location of the device, we examine cases of device movement -- once\nthe device is sold and then subsequently re-used in a new environment. Our work\nhighlights a previously unrecognized privacy vulnerability and suggests, yet\nagain, the strong need to protect layer-two network identifiers."}
{"id": "2506.13090", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2506.13090", "abs": "https://arxiv.org/abs/2506.13090", "authors": ["Chidera Biringa", "Gokhan Kul"], "title": "Detecting Hard-Coded Credentials in Software Repositories via LLMs", "comment": "Accepted to the ACM Digital Threats: Research and Practice (DTRAP)", "summary": "Software developers frequently hard-code credentials such as passwords,\ngeneric secrets, private keys, and generic tokens in software repositories,\neven though it is strictly advised against due to the severe threat to the\nsecurity of the software. These credentials create attack surfaces exploitable\nby a potential adversary to conduct malicious exploits such as backdoor\nattacks. Recent detection efforts utilize embedding models to vectorize textual\ncredentials before passing them to classifiers for predictions. However, these\nmodels struggle to discriminate between credentials with contextual and complex\nsequences resulting in high false positive predictions. Context-dependent\nPre-trained Language Models (PLMs) or Large Language Models (LLMs) such as\nGenerative Pre-trained Transformers (GPT) tackled this drawback by leveraging\nthe transformer neural architecture capacity for self-attention to capture\ncontextual dependencies between words in input sequences. As a result, GPT has\nachieved wide success in several natural language understanding endeavors.\nHence, we assess LLMs to represent these observations and feed extracted\nembedding vectors to a deep learning classifier to detect hard-coded\ncredentials. Our model outperforms the current state-of-the-art by 13% in F1\nmeasure on the benchmark dataset. We have made all source code and data\npublicly available to facilitate the reproduction of all results presented in\nthis paper."}
{"id": "2506.13161", "categories": ["cs.CR", "cs.SE"], "pdf": "https://arxiv.org/pdf/2506.13161", "abs": "https://arxiv.org/abs/2506.13161", "authors": ["Bayu Fedra Abdullah", "Yusuf Sulistyo Nugroho", "Brittany Reid", "Raula Gaikovina Kula", "Kazumasa Shimari", "Kenichi Matsumoto"], "title": "Using LLMs for Security Advisory Investigations: How Far Are We?", "comment": "6 pages, 6 figures, 8 tables, conference paper", "summary": "Large Language Models (LLMs) are increasingly used in software security, but\ntheir trustworthiness in generating accurate vulnerability advisories remains\nuncertain. This study investigates the ability of ChatGPT to (1) generate\nplausible security advisories from CVE-IDs, (2) differentiate real from fake\nCVE-IDs, and (3) extract CVE-IDs from advisory descriptions. Using a curated\ndataset of 100 real and 100 fake CVE-IDs, we manually analyzed the credibility\nand consistency of the model's outputs. The results show that ChatGPT generated\nplausible security advisories for 96% of given input real CVE-IDs and 97% of\ngiven input fake CVE-IDs, demonstrating a limitation in differentiating between\nreal and fake IDs. Furthermore, when these generated advisories were\nreintroduced to ChatGPT to identify their original CVE-ID, the model produced a\nfake CVE-ID in 6% of cases from real advisories. These findings highlight both\nthe strengths and limitations of ChatGPT in cybersecurity applications. While\nthe model demonstrates potential for automating advisory generation, its\ninability to reliably authenticate CVE-IDs or maintain consistency upon\nre-evaluation underscores the risks associated with its deployment in critical\nsecurity tasks. Our study emphasizes the importance of using LLMs with caution\nin cybersecurity workflows and suggests the need for further improvements in\ntheir design to improve reliability and applicability in security advisory\ngeneration."}
{"id": "2506.13170", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2506.13170", "abs": "https://arxiv.org/abs/2506.13170", "authors": ["Imdad Ullah", "Najm Hassan", "Tariq Ahamed Ahangar", "Zawar Hussain Shah", "Mehregan Mahdavi Andrew Levula"], "title": "Dual Protection Ring: User Profiling Via Differential Privacy and Service Dissemination Through Private Information Retrieval", "comment": null, "summary": "User profiling is crucial in providing personalised services, as it relies on\nanalysing user behaviour and preferences to deliver targeted services. This\napproach enhances user experience and promotes heightened engagement.\nNevertheless, user profiling also gives rise to noteworthy privacy\nconsiderations due to the extensive tracking and monitoring of personal data,\npotentially leading to surveillance or identity theft. We propose a dual-ring\nprotection mechanism to protect user privacy by examining various threats to\nuser privacy, such as behavioural attacks, profiling fingerprinting and\nmonitoring, profile perturbation, etc., both on the user and service provider\nsides. We develop user profiles that contain sensitive private attributes and\nan equivalent profile based on differential privacy for evaluating personalised\nservices. We determine the entropy of the resultant profiles during each update\nto protect profiling attributes and invoke various processes, such as data\nevaporation, to artificially increase entropy or destroy private profiling\nattributes. Furthermore, we use different variants of private information\nretrieval (PIR) to retrieve personalised services against differentially\nprivate profiles. We implement critical components of the proposed model via a\nproof-of-concept mobile app to demonstrate its applicability over a specific\ncase study of advertising services, which can be generalised to other services.\nOur experimental results show that the observed processing delays with\ndifferent PIR schemes are similar to the current advertising systems."}
{"id": "2506.13205", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.13205", "abs": "https://arxiv.org/abs/2506.13205", "authors": ["Xuan Wang", "Siyuan Liang", "Zhe Liu", "Yi Yu", "Yuliang Lu", "Xiaochun Cao", "Ee-Chien Chang"], "title": "Screen Hijack: Visual Poisoning of VLM Agents in Mobile Environments", "comment": "12 pages", "summary": "With the growing integration of vision-language models (VLMs), mobile agents\nare now widely used for tasks like UI automation and camera-based user\nassistance. These agents are often fine-tuned on limited user-generated\ndatasets, leaving them vulnerable to covert threats during the training\nprocess. In this work we present GHOST, the first clean-label backdoor attack\nspecifically designed for mobile agents built upon VLMs. Our method manipulates\nonly the visual inputs of a portion of the training samples - without altering\ntheir corresponding labels or instructions - thereby injecting malicious\nbehaviors into the model. Once fine-tuned with this tampered data, the agent\nwill exhibit attacker-controlled responses when a specific visual trigger is\nintroduced at inference time. The core of our approach lies in aligning the\ngradients of poisoned samples with those of a chosen target instance, embedding\nbackdoor-relevant features into the poisoned training data. To maintain stealth\nand enhance robustness, we develop three realistic visual triggers: static\nvisual patches, dynamic motion cues, and subtle low-opacity overlays. We\nevaluate our method across six real-world Android apps and three VLM\narchitectures adapted for mobile use. Results show that our attack achieves\nhigh attack success rates (up to 94.67 percent) while maintaining high\nclean-task performance (FSR up to 95.85 percent). Additionally, ablation\nstudies shed light on how various design choices affect the efficacy and\nconcealment of the attack. Overall, this work is the first to expose critical\nsecurity flaws in VLM-based mobile agents, highlighting their susceptibility to\nclean-label backdoor attacks and the urgent need for effective defense\nmechanisms in their training pipelines. Code and examples are available at:\nhttps://anonymous.4open.science/r/ase-2025-C478."}
{"id": "2506.13246", "categories": ["cs.CR", "cs.AI", "cs.DC", "68Q70, 68P25, 68T37 68Q70, 68P25, 68T37 68Q70, 68P25, 68T37 68Q70,\n  68P25, 68T37", "F.4.3; D.4.6; E.3; I.2.4"], "pdf": "https://arxiv.org/pdf/2506.13246", "abs": "https://arxiv.org/abs/2506.13246", "authors": ["Craig Steven Wright"], "title": "On Immutable Memory Systems for Artificial Agents: A Blockchain-Indexed Automata-Theoretic Framework Using ECDH-Keyed Merkle Chains", "comment": "47 pages, includes formal automata specifications, cryptographic\n  constructions, and epistemic architecture schema", "summary": "This paper presents a formalised architecture for synthetic agents designed\nto retain immutable memory, verifiable reasoning, and constrained epistemic\ngrowth. Traditional AI systems rely on mutable, opaque statistical models prone\nto epistemic drift and historical revisionism. In contrast, we introduce the\nconcept of the Merkle Automaton, a cryptographically anchored, deterministic\ncomputational framework that integrates formal automata theory with\nblockchain-based commitments. Each agent transition, memory fragment, and\nreasoning step is committed within a Merkle structure rooted on-chain,\nrendering it non-repudiable and auditably permanent. To ensure selective access\nand confidentiality, we derive symmetric encryption keys from ECDH exchanges\ncontextualised by hierarchical privilege lattices. This enforces cryptographic\naccess control over append-only DAG-structured knowledge graphs. Reasoning is\nconstrained by formal logic systems and verified through deterministic\ntraversal of policy-encoded structures. Updates are non-destructive and\nhistoried, preserving epistemic lineage without catastrophic forgetting.\nZero-knowledge proofs facilitate verifiable, privacy-preserving inclusion\nattestations. Collectively, this architecture reframes memory not as a cache\nbut as a ledger - one whose contents are enforced by protocol, bound by\ncryptography, and constrained by formal logic. The result is not an intelligent\nagent that mimics thought, but an epistemic entity whose outputs are provably\nderived, temporally anchored, and impervious to post hoc revision. This design\nlays foundational groundwork for legal, economic, and high-assurance\ncomputational systems that require provable memory, unforgeable provenance, and\nstructural truth."}
{"id": "2506.13261", "categories": ["cs.CR", "cs.NI"], "pdf": "https://arxiv.org/pdf/2506.13261", "abs": "https://arxiv.org/abs/2506.13261", "authors": ["Timo Salomon", "Mehmet Mueller", "Philipp Meyer", "Thomas C. Schmidt"], "title": "Building Automotive Security on Internet Standards: An Integration of DNSSEC, DANE, and DANCE to Authenticate and Authorize In-Car Services", "comment": null, "summary": "The automotive industry is undergoing a software-as-a-service transformation\nthat enables software-defined functions and post-sale updates via cloud and\nvehicle-to-everything communication. Connectivity in cars introduces\nsignificant security challenges, as remote attacks on vehicles have become\nincreasingly prevalent. Current automotive designs call for security solutions\nthat address the entire lifetime of a vehicle. In this paper, we propose to\nauthenticate and authorize in-vehicle services by integrating DNSSEC, DANE, and\nDANCE with automotive middleware. Our approach decouples the cryptographic\nauthentication of the service from that of the service deployment with the help\nof DNSSEC and thereby largely simplifies key management. We propose to\nauthenticate in-vehicle services by certificates that are solely generated by\nthe service suppliers but published on deployment via DNSSEC TLSA records\nsolely signed by the OEM. Building on well-established Internet standards\nensures interoperability with various current and future protocols, scalable\nmanagement of credentials for millions of connected vehicles at\nwell-established security levels. We back our design proposal by a security\nanalysis using the STRIDE threat model and by evaluations in a realistic\nin-vehicle setup that demonstrate its effectiveness."}
{"id": "2506.13323", "categories": ["cs.CR", "cs.AI", "cs.LG", "cs.SE"], "pdf": "https://arxiv.org/pdf/2506.13323", "abs": "https://arxiv.org/abs/2506.13323", "authors": ["Siliang Qin", "Fengrui Yang", "Hao Wang", "Bolun Zhang", "Zeyu Gao", "Chao Zhang", "Kai Chen"], "title": "Tady: A Neural Disassembler without Structural Constraint Violations", "comment": "Usenix Security'25", "summary": "Disassembly is a crucial yet challenging step in binary analysis. While\nemerging neural disassemblers show promise for efficiency and accuracy, they\nfrequently generate outputs violating fundamental structural constraints, which\nsignificantly compromise their practical usability. To address this critical\nproblem, we regularize the disassembly solution space by formalizing and\napplying key structural constraints based on post-dominance relations. This\napproach systematically detects widespread errors in existing neural\ndisassemblers' outputs. These errors often originate from models' limited\ncontext modeling and instruction-level decoding that neglect global structural\nintegrity. We introduce Tady, a novel neural disassembler featuring an improved\nmodel architecture and a dedicated post-processing algorithm, specifically\nengineered to address these deficiencies. Comprehensive evaluations on diverse\nbinaries demonstrate that Tady effectively eliminates structural constraint\nviolations and functions with high efficiency, while maintaining\ninstruction-level accuracy."}
{"id": "2506.13360", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2506.13360", "abs": "https://arxiv.org/abs/2506.13360", "authors": ["Akira Sakurai", "Kazuyuki Shudo"], "title": "The Rich Get Richer in Bitcoin Mining Induced by Blockchain Forks", "comment": null, "summary": "Bitcoin is a representative decentralized currency system. For the security\nof Bitcoin, fairness in the distribution of mining rewards plays a crucial role\nin preventing the concentration of computational power in a few miners. Here,\nfairness refers to the distribution of block rewards in proportion to\ncontributed computational resources. If miners with greater computational\nresources receive disproportionately higher rewards, i.e., if the Rich Get\nRicher (TRGR) phenomenon holds in Bitcoin, it indicates a threat to the\nsystem's decentralization. This study analyzes TRGR in Bitcoin by focusing on\nunintentional blockchain forks, an inherent phenomenon in Bitcoin. Previous\nresearch has failed to provide generalizable insights due to the low precision\nof their analytical methods. In contrast, we avoid this problem by adopting a\nmethod whose analytical precision has been empirically validated. The primary\ncontribution of this work is a theoretical analysis that clearly demonstrates\nTRGR in Bitcoin under the assumption of fixed block propagation delays between\ndifferent miners. More specifically, we show that the mining profit rate\ndepends linearly on the proportion of hashrate. Furthermore, we examine the\nrobustness of this result from multiple perspectives in scenarios where block\npropagation delays between different miners are not necessarily fixed."}
{"id": "2506.13418", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2506.13418", "abs": "https://arxiv.org/abs/2506.13418", "authors": ["Minjia Shi", "Wenhao Song"], "title": "New characterization of full weight spectrum one-orbit cyclic subspace codes", "comment": null, "summary": "Castello $\\textit{et al}$. [J. Comb. Theory Ser. A, 212, 106005 (2025)]\nprovided a complete classification for full weight spectrum (FWS) one-orbit\ncyclic subspace codes. In this paper, we determine the weight distributions of\na family of FWS codes and exhibit some equivalence classes of FWS codes under\ncertain conditions. Furthermore, we provide a complete classification for\n$r$-FWS codes."}
{"id": "2506.13434", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2506.13434", "abs": "https://arxiv.org/abs/2506.13434", "authors": ["Alsharif Abuadbba", "Chris Hicks", "Kristen Moore", "Vasilios Mavroudis", "Burak Hasircioglu", "Diksha Goel", "Piers Jennings"], "title": "From Promise to Peril: Rethinking Cybersecurity Red and Blue Teaming in the Age of LLMs", "comment": "10 pages", "summary": "Large Language Models (LLMs) are set to reshape cybersecurity by augmenting\nred and blue team operations. Red teams can exploit LLMs to plan attacks, craft\nphishing content, simulate adversaries, and generate exploit code. Conversely,\nblue teams may deploy them for threat intelligence synthesis, root cause\nanalysis, and streamlined documentation. This dual capability introduces both\ntransformative potential and serious risks.\n  This position paper maps LLM applications across cybersecurity frameworks\nsuch as MITRE ATT&CK and the NIST Cybersecurity Framework (CSF), offering a\nstructured view of their current utility and limitations. While LLMs\ndemonstrate fluency and versatility across various tasks, they remain fragile\nin high-stakes, context-heavy environments. Key limitations include\nhallucinations, limited context retention, poor reasoning, and sensitivity to\nprompts, which undermine their reliability in operational settings.\n  Moreover, real-world integration raises concerns around dual-use risks,\nadversarial misuse, and diminished human oversight. Malicious actors could\nexploit LLMs to automate reconnaissance, obscure attack vectors, and lower the\ntechnical threshold for executing sophisticated attacks.\n  To ensure safer adoption, we recommend maintaining human-in-the-loop\noversight, enhancing model explainability, integrating privacy-preserving\nmechanisms, and building systems robust to adversarial exploitation. As\norganizations increasingly adopt AI driven cybersecurity, a nuanced\nunderstanding of LLMs' risks and operational impacts is critical to securing\ntheir defensive value while mitigating unintended consequences."}
{"id": "2506.13494", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2506.13494", "abs": "https://arxiv.org/abs/2506.13494", "authors": ["Yugeng Liu", "Tianshuo Cong", "Michael Backes", "Zheng Li", "Yang Zhang"], "title": "Watermarking LLM-Generated Datasets in Downstream Tasks", "comment": null, "summary": "Large Language Models (LLMs) have experienced rapid advancements, with\napplications spanning a wide range of fields, including sentiment\nclassification, review generation, and question answering. Due to their\nefficiency and versatility, researchers and companies increasingly employ\nLLM-generated data to train their models. However, the inability to track\ncontent produced by LLMs poses a significant challenge, potentially leading to\ncopyright infringement for the LLM owners. In this paper, we propose a method\nfor injecting watermarks into LLM-generated datasets, enabling the tracking of\ndownstream tasks to detect whether these datasets were produced using the\noriginal LLM. These downstream tasks can be divided into two categories. The\nfirst involves using the generated datasets at the input level, commonly for\ntraining classification tasks. The other is the output level, where model\ntrainers use LLM-generated content as output for downstream tasks, such as\nquestion-answering tasks. We design a comprehensive set of experiments to\nevaluate both watermark methods. Our results indicate the high effectiveness of\nour watermark approach. Additionally, regarding model utility, we find that\nclassifiers trained on the generated datasets achieve a test accuracy exceeding\n0.900 in many cases, suggesting that the utility of such models remains robust.\nFor the output-level watermark, we observe that the quality of the generated\ntext is comparable to that produced using real-world datasets. Through our\nresearch, we aim to advance the protection of LLM copyrights, taking a\nsignificant step forward in safeguarding intellectual property in this domain."}
{"id": "2506.13563", "categories": ["cs.CR", "cs.NI"], "pdf": "https://arxiv.org/pdf/2506.13563", "abs": "https://arxiv.org/abs/2506.13563", "authors": ["Yali Yuan", "Kai Xu", "Ruolin Ma", "Yuchen Zhang"], "title": "Unlearning-Enhanced Website Fingerprinting Attack: Against Backdoor Poisoning in Anonymous Networks", "comment": null, "summary": "Website Fingerprinting (WF) is an effective tool for regulating and governing\nthe dark web. However, its performance can be significantly degraded by\nbackdoor poisoning attacks in practical deployments. This paper aims to address\nthe problem of hidden backdoor poisoning attacks faced by Website\nFingerprinting attack, and designs a feasible mothed that integrates unlearning\ntechnology to realize detection of automatic poisoned points and complete\nremoval of its destructive effects, requiring only a small number of known\npoisoned test points. Taking Tor onion routing as an example, our method\nevaluates the influence value of each training sample on these known poisoned\ntest points as the basis for judgment. We optimize the use of influence scores\nto identify poisoned samples within the training dataset. Furthermore, by\nquantifying the difference between the contribution of model parameters on the\ntaining data and the clean data, the target parameters are dynamically adjusted\nto eliminate the impact of the backdoor attacks. Experiments on public datasets\nunder the assumptions of closed-world (CW) and open-world (OW) verify the\neffectiveness of the proposed method. In complex scenes containing both clean\nwebsite fingerprinting features and backdoor triggers, the accuracy of the\nmodel on the poisoned dataset and the test dataset is stable at about 80%,\nsignificantly outperforming the traditional WF attack models. In addition, the\nproposed method achieves a 2-3 times speedup in runtime efficiency compared to\nbaseline methods. By incorporating machine unlearning, we realize a WF attack\nmodel that exhibits enhanced resistance to backdoor poisoning and faster\nexecution speeds in adversarial settings."}
{"id": "2506.13612", "categories": ["cs.CR", "cs.AI", "cs.DC"], "pdf": "https://arxiv.org/pdf/2506.13612", "abs": "https://arxiv.org/abs/2506.13612", "authors": ["Zhiqiang Li", "Haiyong Bao", "Menghong Guan", "Hao Pan", "Cheng Huang", "Hong-Ning Dai"], "title": "EBS-CFL: Efficient and Byzantine-robust Secure Clustered Federated Learning", "comment": "Accepted by AAAI 25", "summary": "Despite federated learning (FL)'s potential in collaborative learning, its\nperformance has deteriorated due to the data heterogeneity of distributed\nusers. Recently, clustered federated learning (CFL) has emerged to address this\nchallenge by partitioning users into clusters according to their similarity.\nHowever, CFL faces difficulties in training when users are unwilling to share\ntheir cluster identities due to privacy concerns. To address these issues, we\npresent an innovative Efficient and Robust Secure Aggregation scheme for CFL,\ndubbed EBS-CFL. The proposed EBS-CFL supports effectively training CFL while\nmaintaining users' cluster identity confidentially. Moreover, it detects\npotential poisonous attacks without compromising individual client gradients by\ndiscarding negatively correlated gradients and aggregating positively\ncorrelated ones using a weighted approach. The server also authenticates\ncorrect gradient encoding by clients. EBS-CFL has high efficiency with\nclient-side overhead O(ml + m^2) for communication and O(m^2l) for computation,\nwhere m is the number of cluster identities, and l is the gradient size. When m\n= 1, EBS-CFL's computational efficiency of client is at least O(log n) times\nbetter than comparison schemes, where n is the number of clients.In addition,\nwe validate the scheme through extensive experiments. Finally, we theoretically\nprove the scheme's security."}
{"id": "2506.13737", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2506.13737", "abs": "https://arxiv.org/abs/2506.13737", "authors": ["Zhenhao Zhu", "Yue Liu", "Yingwei Ma", "Hongcheng Gao", "Nuo Chen", "Yanpei Guo", "Wenjie Qu", "Huiying Xu", "Xinzhong Zhu", "Jiaheng Zhang"], "title": "ExtendAttack: Attacking Servers of LRMs via Extending Reasoning", "comment": null, "summary": "Large Reasoning Models (LRMs) have demonstrated promising performance in\ncomplex tasks. However, the resource-consuming reasoning processes may be\nexploited by attackers to maliciously occupy the resources of the servers,\nleading to a crash, like the DDoS attack in cyber. To this end, we propose a\nnovel attack method on LRMs termed ExtendAttack to maliciously occupy the\nresources of servers by stealthily extending the reasoning processes of LRMs.\nConcretely, we systematically obfuscate characters within a benign prompt,\ntransforming them into a complex, poly-base ASCII representation. This compels\nthe model to perform a series of computationally intensive decoding sub-tasks\nthat are deeply embedded within the semantic structure of the query itself.\nExtensive experiments demonstrate the effectiveness of our proposed\nExtendAttack. Remarkably, it increases the length of the model's response by\nover 2.5 times for the o3 model on the HumanEval benchmark. Besides, it\npreserves the original meaning of the query and achieves comparable answer\naccuracy, showing the stealthiness."}
{"id": "2506.13746", "categories": ["cs.CR", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.13746", "abs": "https://arxiv.org/abs/2506.13746", "authors": ["Shova Kuikel", "Aritran Piplai", "Palvi Aggarwal"], "title": "Evaluating Large Language Models for Phishing Detection, Self-Consistency, Faithfulness, and Explainability", "comment": null, "summary": "Phishing attacks remain one of the most prevalent and persistent\ncybersecurity threat with attackers continuously evolving and intensifying\ntactics to evade the general detection system. Despite significant advances in\nartificial intelligence and machine learning, faithfully reproducing the\ninterpretable reasoning with classification and explainability that underpin\nphishing judgments remains challenging. Due to recent advancement in Natural\nLanguage Processing, Large Language Models (LLMs) show a promising direction\nand potential for improving domain specific phishing classification tasks.\nHowever, enhancing the reliability and robustness of classification models\nrequires not only accurate predictions from LLMs but also consistent and\ntrustworthy explanations aligning with those predictions. Therefore, a key\nquestion remains: can LLMs not only classify phishing emails accurately but\nalso generate explanations that are reliably aligned with their predictions and\ninternally self-consistent? To answer these questions, we have fine-tuned\ntransformer based models, including BERT, Llama models, and Wizard, to improve\ndomain relevance and make them more tailored to phishing specific distinctions,\nusing Binary Sequence Classification, Contrastive Learning (CL) and Direct\nPreference Optimization (DPO). To that end, we examined their performance in\nphishing classification and explainability by applying the ConsistenCy measure\nbased on SHAPley values (CC SHAP), which measures prediction explanation token\nalignment to test the model's internal faithfulness and consistency and uncover\nthe rationale behind its predictions and reasoning. Overall, our findings show\nthat Llama models exhibit stronger prediction explanation token alignment with\nhigher CC SHAP scores despite lacking reliable decision making accuracy,\nwhereas Wizard achieves better prediction accuracy but lower CC SHAP scores."}
