<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 12]
- [cs.CR](#cs.CR) [Total: 20]
- [cs.AI](#cs.AI) [Total: 24]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [Evaluating LLMs on microservice-based applications: how complex is your specification?](https://arxiv.org/abs/2508.20119)
*Daniel M. Yellin*

Main category: cs.SE

TL;DR: 这篇论文评估了LLM在生成实际应用程序代码方面的进展，采用微服务架构作为研究对象。研究发现强大的LLM在中等难度规范上表现不错，但在高难度规范上表现很差。


<details>
  <summary>Details</summary>
Motivation: 评估LLM在生成实际业务问题代码方面的能力，特别是微服务架构这种广泛使用的实际应用场景。

Method: 定义了微服务应用的标准规范模板，提出了规范难度评估指标，并开发了自动化测试框架来验证LLM生成的代码。

Result: 实验结果显示GPT-3o-mini等强大LLM在中等难度规范上表现良好，但在高难度规范上表现很差，特别是涉及复杂业务逻辑、外部服务集成、数据库集成和认证等非功能性要求时。

Conclusion: LLM在生成实际应用代码时面临重大挑战，需要进一步研究来提升代码合成能力，特别是在复杂业务逻辑和外部集成方面。

Abstract: In this paper we evaluate how far LLMs have advanced in generating code for
real-world problems. Specifically, we explore code synthesis for
microservice-based applications, a widely used architecture pattern. We define
a standard template for specifying these applications, and we propose a metric
for judging the difficulty level of a specification. The higher the score, the
more difficult it is to generate code for the specification. We develop a
framework to automate the process of testing LLM-synthesized code for a
microservice using unit tests. Our experimental results show that strong LLMs
(like GPT-3o-mini) do fairly well on medium difficulty specifications but do
very poorly on those of higher difficulty levels. This is due to more intricate
business logic, a greater use of external services, database integration and
inclusion of non-functional capabilities such as authentication. We analyzed
the errors in LLM-synthesized code and report on the key challenges LLMs face
in generating code for these specifications thereby suggesting future research
directions to improve code synthesis for real-world problems.

</details>


### [2] [Towards Better Correctness and Efficiency in Code Generation](https://arxiv.org/abs/2508.20124)
*Yunlong Feng,Yang Xu,Xiao Xu,Binyuan Hui,Junyang Lin*

Main category: cs.SE

TL;DR: 通过效率导向的强化学习框架，提出了两阶段调优方法，在保持代码正确性的同时显著提升运行效率，使用7B模型达到了更大模型的性能水平。


<details>
  <summary>Details</summary>
Motivation: 当前代码大语言模型生成的代码运行效率偏低，限制了在性能敏感场景中的实际应用。

Method: 提出了一个基于新颖性能奖励的效率导向强化学习框架，包括：(1)动态探索突破离线精调的静态数据限制 (2)错误不敏感的强化学习方法和高对比度效率信号 (3)从高正确性基线开始的在线探索策略

Result: 在7B模型上实验结果显示，方法将代码正确性提高了10.18%，运行效率提高了7.75%，达到了与更大模型相当的性能水平。

Conclusion: 通过两阶段调优方法，可以在代码正确性和运行效率之间实现高并平衡的性能表现，有效解决了代码生成模型在性能敏感场景中的实际应用问题。

Abstract: While code large language models have demonstrated remarkable progress in
code generation, the generated code often exhibits poor runtime efficiency,
limiting its practical application in performance-sensitive scenarios. To
address this limitation, we propose an efficiency-oriented reinforcement
learning framework guided by a novel performance reward. Based on this
framework, we take a deeper dive into the code efficiency problem, identifying
then proposing methods to overcome key bottlenecks: (1) Dynamic exploration
overcomes the static data constraints of offline fine-tuning, enabling the
discovery of more efficient code implementations. (2) The error-insensitive
reinforcement learning method and high-contrast efficiency signals are crucial
for mitigating systematic errors and achieving effective optimization. (3)
Online exploration is most effective when starting from a high-correctness
baseline, as this allows for efficiency improvements without sacrificing
accuracy. With these discoveries, we finally propose a two-stage tuning method,
which achieves high and balanced performance across correctness and efficiency.
The results of experiments show the effectiveness of the method, which improves
code correctness by 10.18\% and runtime efficiency by 7.75\% on a 7B model,
achieving performance comparable to much larger model.

</details>


### [3] [Boosting Skeleton-Driven SMT Solver Fuzzing by Leveraging LLM to Produce Formula Generators](https://arxiv.org/abs/2508.20340)
*Maolin Sun,Yibiao Yang,Yuming Zhou*

Main category: cs.SE

TL;DR: Chimera是一个基于LLM的SMT求解器模糊测试框架，通过生成可重用的项生成器而非直接生成公式，解决了现有方法中语法无效和计算开销大的问题，在Z3和cvc5中发现了43个已确认的bug。


<details>
  <summary>Details</summary>
Motivation: SMT求解器在现代系统和编程语言研究中至关重要，但其正确性测试面临挑战。现有测试技术难以跟上求解器快速发展的特性，而基于LLM的方法存在语法无效率高和计算开销大的问题。

Method: Chimera框架使用LLM从文档中自动提取SMT理论的上下文无关文法，并合成可组合的布尔项生成器。在模糊测试时，用LLM合成的生成器产生的项填充现有公式的结构骨架，确保语法有效性同时促进语义多样性。

Result: 在Z3和cvc5两个主流SMT求解器上的实验表明，Chimera发现了43个已确认的bug，其中40个已被开发者修复。

Conclusion: Chimera通过从直接公式生成转向可重用项生成器合成，有效解决了LLM辅助测试中的语法无效和计算开销问题，显著提高了SMT求解器测试的效率和效果。

Abstract: Satisfiability Modulo Theory (SMT) solvers are foundational to modern systems
and programming languages research, providing the foundation for tasks like
symbolic execution and automated verification. Because these solvers sit on the
critical path, their correctness is essential, and high-quality test formulas
are key to uncovering bugs. However, while prior testing techniques performed
well on earlier solver versions, they struggle to keep pace with rapidly
evolving features. Recent approaches based on Large Language Models (LLMs) show
promise in exploring advanced solver capabilities, but two obstacles remain:
nearly half of the generated formulas are syntactically invalid, and iterative
interactions with the LLMs introduce substantial computational overhead. In
this study, we present Chimera, a novel LLM-assisted fuzzing framework that
addresses both issues by shifting from direct formula generation to the
synthesis of reusable term (i.e., logical expression) generators. Particularly,
Chimera uses LLMs to (1) automatically extract context-free grammars (CFGs) for
SMT theories, including solver-specific extensions, from documentation, and (2)
synthesize composable Boolean term generators that adhere to these grammars.
During fuzzing, Chimera populates structural skeletons derived from existing
formulas with the terms iteratively produced by the LLM-synthesized generators.
This design ensures syntactic validity while promoting semantic diversity.
Notably, Chimera requires only one-time LLM interaction investment,
dramatically reducing runtime cost. We evaluated Chimera on two leading SMT
solvers: Z3 and cvc5. Our experiments show that Chimera has identified 43
confirmed bugs, 40 of which have already been fixed by developers.

</details>


### [4] [Adaptive Root Cause Localization for Microservice Systems with Multi-Agent Recursion-of-Thought](https://arxiv.org/abs/2508.20370)
*Lingzhe Zhang,Tong Jia,Kangjin Wang,Weijie Hong,Chiming Duan,Minghua He,Ying Li*

Main category: cs.SE

TL;DR: RCLAgent是一个基于多智能体递归思维框架的自适应微服务系统根因定位方法，通过模拟人类SRE的分析特征，在单次请求中实现优于现有方法的定位性能。


<details>
  <summary>Details</summary>
Motivation: 当前微服务系统日益复杂，故障频发，现有根因定位方法要么依赖预定义模式难以适应变化，要么缺乏可解释性。通过研究SRE专家的分析方法，发现人类根因分析具有递归性、多维扩展和跨模态推理三个关键特征。

Method: 提出RCLAgent方法，采用多智能体递归思维框架，使用新颖的递归思维策略指导LLM推理过程，整合多个智能体和工具辅助分析数据来精确定位根因。

Result: 在多个公共数据集上的实验评估表明，RCLAgent仅使用单次请求就能实现根因定位，性能优于依赖聚合多个请求的最先进方法。

Conclusion: RCLAgent有效提升了复杂微服务环境中根因定位的效率和精度，证明了该方法在增强系统可靠性方面的有效性。

Abstract: As contemporary microservice systems become increasingly popular and
complex-often comprising hundreds or even thousands of fine-grained,
interdependent subsystems-they are facing more frequent failures. Ensuring
system reliability thus demands accurate root cause localization. While traces
and metrics have proven to be effective data sources for this task, existing
methods either heavily rely on pre-defined schemas, which struggle to adapt to
evolving operational contexts, or lack interpretability in their reasoning
process, thereby leaving Site Reliability Engineers (SREs) confused. In this
paper, we conduct a comprehensive study on how SREs localize the root cause of
failures, drawing insights from multiple professional SREs across different
organizations. Our investigation reveals that human root cause analysis
exhibits three key characteristics: recursiveness, multi-dimensional expansion,
and cross-modal reasoning. Motivated by these findings, we introduce RCLAgent,
an adaptive root cause localization method for microservice systems that
leverages a multi-agent recursion-of-thought framework. RCLAgent employs a
novel recursion-of-thought strategy to guide the LLM's reasoning process,
effectively integrating data from multiple agents and tool-assisted analysis to
accurately pinpoint the root cause. Experimental evaluations on various public
datasets demonstrate that RCLAgent achieves superior performance by localizing
the root cause using only a single request-outperforming state-of-the-art
methods that depend on aggregating multiple requests. These results underscore
the effectiveness of RCLAgent in enhancing the efficiency and precision of root
cause localization in complex microservice environments.

</details>


### [5] [AI and Agile Software Development: A Research Roadmap from the XP2025 Workshop](https://arxiv.org/abs/2508.20563)
*Zheying Zhang,Tomas Herda,Victoria Pichler,Pekka Abrahamsson,Geir K. Hanssen,Joshua Kerievsky,Alex Polyakov,Mohit Chandna,Marius Irgens,Kai-Kristian Kemell,Ayman Asad Khan,Crystal Kwok,Evan Leybourn,Munish Malik,Dorota Mleczko,Morteza Moalagh,Christopher Morales,Yuliia Pieskova,Daniel Planötscher,Mika Saari,Anastasiia Tkalich,Karl Josef Gstettner,Xiaofeng Wang*

Main category: cs.SE

TL;DR: XP2025研讨会总结了生成式AI与敏捷开发融合的挑战和机遇，识别了工具碎片化、治理、数据质量等痛点，并制定了多主题研究路线图


<details>
  <summary>Details</summary>
Motivation: 解决生成式人工智能与敏捷软件开发交叉领域的具体挑战和新兴机遇，促进学术界和工业界的跨学科合作

Method: 通过结构化、互动的分组讨论会议，汇集30多名跨学科学术研究者和行业从业者，共同分析问题并制定研究路线

Result: 识别了共享痛点（工具碎片化、治理、数据质量、AI素养和提示工程技能差距），分析了根本原因，并共同制定了包含短期可实施行动和长期愿景的研究路线图

Conclusion: 该研讨会制定了一个统一的研究议程，旨在指导未来研究并推动生成式AI在敏捷实践中负责任、以人为本的整合

Abstract: This paper synthesizes the key findings from a full-day XP2025 workshop on
"AI and Agile: From Frustration to Success", held in Brugg-Windisch,
Switzerland. The workshop brought together over 30 interdisciplinary academic
researchers and industry practitioners to tackle the concrete challenges and
emerging opportunities at the intersection of Generative Artificial
Intelligence (GenAI) and agile software development. Through structured,
interactive breakout sessions, participants identified shared pain points like
tool fragmentation, governance, data quality, and critical skills gaps in AI
literacy and prompt engineering. These issues were further analyzed, revealing
underlying causes and cross-cutting concerns. The workshop concluded by
collaboratively co-creating a multi-thematic research roadmap, articulating
both short-term, implementable actions and visionary, long-term research
directions. This cohesive agenda aims to guide future investigation and drive
the responsible, human-centered integration of GenAI into agile practices.

</details>


### [6] [Rethinking Testing for LLM Applications: Characteristics, Challenges, and a Lightweight Interaction Protocol](https://arxiv.org/abs/2508.20737)
*Wei Ma,Yixiao Yang,Qiang Hu,Shi Ying,Zhi Jin,Bo Du,Zhenchang Xing,Tianlin Li,Junjie Shi,Yang Liu,Linxiao Jiang*

Main category: cs.SE

TL;DR: 本文提出了大语言模型应用的三层架构分析法，识别了测试挑战并提出四种协同策略和AICL协议来支持LLM应用的标准化测试。


<details>
  <summary>Details</summary>
Motivation: 大语言模型应用从简单文本生成发展为复杂软件系统，其非确定性、动态性和上下文依赖性对质量保证构成根本挑战。

Method: 将LLM应用解构为系统壳层、提示组织层和LLM推理核心三层架构，分析各层的测试适用性，并提出四种协同策略（Retain、Translate、Integrate、Runtime）和AICL协议。

Result: 识别了6个核心挑战和4个基本差异，提出了结合部署前验证与运行时监控的可信质量保证框架。

Conclusion: 通过协同策略和AICL协议，为LLM应用测试的标准化和工具化提供了实践指南，有助于构建可靠的质量保证体系。

Abstract: Applications of Large Language Models~(LLMs) have evolved from simple text
generators into complex software systems that integrate retrieval augmentation,
tool invocation, and multi-turn interactions. Their inherent non-determinism,
dynamism, and context dependence pose fundamental challenges for quality
assurance. This paper decomposes LLM applications into a three-layer
architecture: \textbf{\textit{System Shell Layer}}, \textbf{\textit{Prompt
Orchestration Layer}}, and \textbf{\textit{LLM Inference Core}}. We then assess
the applicability of traditional software testing methods in each layer:
directly applicable at the shell layer, requiring semantic reinterpretation at
the orchestration layer, and necessitating paradigm shifts at the inference
core. A comparative analysis of Testing AI methods from the software
engineering community and safety analysis techniques from the AI community
reveals structural disconnects in testing unit abstraction, evaluation metrics,
and lifecycle management. We identify four fundamental differences that
underlie 6 core challenges. To address these, we propose four types of
collaborative strategies (\emph{Retain}, \emph{Translate}, \emph{Integrate},
and \emph{Runtime}) and explore a closed-loop, trustworthy quality assurance
framework that combines pre-deployment validation with runtime monitoring.
Based on these strategies, we offer practical guidance and a protocol proposal
to support the standardization and tooling of LLM application testing. We
propose a protocol \textbf{\textit{Agent Interaction Communication Language}}
(AICL) that is used to communicate between AI agents. AICL has the
test-oriented features and is easily integrated in the current agent framework.

</details>


### [7] [From Law to Gherkin: A Human-Centred Quasi-Experiment on the Quality of LLM-Generated Behavioural Specifications from Food-Safety Regulations](https://arxiv.org/abs/2508.20744)
*Shabnam Hassani,Mehrdad Sabetzadeh,Daniel Amyot*

Main category: cs.SE

TL;DR: LLMs能够从法律文本中生成高质量的Gherkin行为规范，显著减少人工工作量，为合规性要求提供结构化开发工件


<details>
  <summary>Details</summary>
Motivation: 法律文本采用技术中性语言编写，工程师手动创建合规性需求文档劳动密集且容易出错，需要利用GenAI技术自动化这一过程

Method: 采用准实验设计，10名参与者评估Claude和Llama从食品安全法规生成的60个Gherkin规范，每个规范由2人评估5个标准，共120次评估

Result: 相关性75%最高评分，清晰度90%最高，完整性75%最高，单一性82%最高，时间节省68%最高。Llama在清晰度、完整性和时间节省方面略优，Claude在单一性方面更强

Conclusion: LLMs能够从法律文本生成高质量的Gherkin规范，减少人工工作量，为实施、保证和测试生成提供有用的结构化工件

Abstract: Context: Laws and regulations increasingly affect software design and quality
assurance, but legal texts are written in technology-neutral language. This
creates challenges for engineers who must develop compliance artifacts such as
requirements and acceptance criteria. Manual creation is labor-intensive,
error-prone, and requires domain expertise. Advances in Generative AI (GenAI),
especially Large Language Models (LLMs), offer a way to automate deriving such
artifacts.
  Objective: We present the first systematic human-subject study of LLMs'
ability to derive behavioral specifications from legal texts using a
quasi-experimental design. These specifications translate legal requirements
into a developer-friendly form.
  Methods: Ten participants evaluated specifications generated from food-safety
regulations by Claude and Llama. Using Gherkin, a structured BDD language, 60
specifications were produced. Each participant assessed 12 across five
criteria: Relevance, Clarity, Completeness, Singularity, and Time Savings. Each
specification was reviewed by two participants, yielding 120 assessments.
  Results: For Relevance, 75% of ratings were highest and 20% second-highest.
Clarity reached 90% highest. Completeness: 75% highest, 19% second.
Singularity: 82% highest, 12% second. Time Savings: 68% highest, 24% second. No
lowest ratings occurred. Mann-Whitney U tests showed no significant differences
across participants or models. Llama slightly outperformed Claude in Clarity,
Completeness, and Time Savings, while Claude was stronger in Singularity.
Feedback noted hallucinations and omissions but confirmed the utility of the
specifications.
  Conclusion: LLMs can generate high-quality Gherkin specifications from legal
texts, reducing manual effort and providing structured artifacts useful for
implementation, assurance, and test generation.

</details>


### [8] [Towards an Architectural Perspective for Sustainability: Bundle the Needs from Industry](https://arxiv.org/abs/2508.20774)
*Markus Funke,Patricia Lago*

Main category: cs.SE

TL;DR: 这篇论文提出了一种可持续性视角的概念，通过架构视角知识来结构化地处理软件设计中的可持续性质量问题


<details>
  <summary>Details</summary>
Motivation: 软件强化系统中可持续性质量属性日益重要，但架构师缺乏结构化指导来在软件设计阶段有效处理这一问题

Method: 通过雪球抽样法对典范文献进行证据收集，并与领域专家进行焦点小组讨论，形成可持续性视角的概念

Result: 确认了不同视角元素在实践中的相关性，并强调了形成符合工业需求的可持续性视角的启示

Conclusion: 可持续性视角作为一种架构知识工具，为软件架构师提供了结构化的方法来处理可持续性质量问题，具有跨架构框架和行业背景的适用性

Abstract: Sustainability is increasingly recognized as an emerging quality property in
software-intensive systems, yet architects lack structured guidance to address
it effectively throughout the software design phase. Architectural
perspectives-an architectural knowledge artifact composed of concerns,
activities, tactics, pitfalls, and checklists-offer a promising approach to
tackle such emerging quality properties across architectural views and are also
independent of architecture frameworks and industry contexts. In this paper, we
present a sustainability perspective vision, i.e., a revised notion of
architectural perspective meant to be filled with its own elements to target
sustainability concerns. We formulate our sustainability perspective vision
through evidence from applying snowballing to seminal literature and from
conducting a focus group with experts in the field. Our findings confirm the
relevance of the different perspective elements in practice and highlight
implications for shaping a sustainability perspective that meets industrial
needs.

</details>


### [9] [Automated Test Oracles for Flaky Cyber-Physical System Simulators: Approach and Evaluation](https://arxiv.org/abs/2508.20902)
*Baharin A. Jodat,Khouloud Gaaloul,Mehrdad Sabetzadeh,Shiva Nejati*

Main category: cs.SE

TL;DR: 提出基于断言的测试预言方法，通过遗传编程和决策树技术生成逻辑和算术谓词，无需执行测试即可判断测试结果，有效应对CPS模拟器的不可靠性问题。


<details>
  <summary>Details</summary>
Motivation: CPS模拟器测试成本高且结果不稳定，需要不依赖系统执行的自动化测试预言来降低测试成本，同时要求预言具有可解释性和对模拟器不稳定性的鲁棒性。

Method: 使用遗传编程（采用Ochiai、Tarantula和Naish等SBFL排名公式作为适应度函数）以及决策树和决策规则两种方法生成基于断言的测试预言。

Result: 基于Ochiai的遗传编程方法生成的测试预言准确率显著高于其他方法，且在存在不稳定性的系统中仍保持准确性优势，平均准确率变化仅为4%。

Conclusion: 基于Ochiai的遗传编程方法能够生成准确、鲁棒的测试预言，有效解决CPS测试中的成本和不稳定性问题，具有实际应用价值。

Abstract: Simulation-based testing of cyber-physical systems (CPS) is costly due to the
time-consuming execution of CPS simulators. In addition, CPS simulators may be
flaky, leading to inconsistent test outcomes and requiring repeated test
re-execution for reliable test verdicts. Automated test oracles that do not
require system execution are therefore crucial for reducing testing costs.
Ideally, such test oracles should be interpretable to facilitate human
understanding of test verdicts, and they must be robust against the potential
flakiness of CPS simulators. In this article, we propose assertion-based test
oracles for CPS as sets of logical and arithmetic predicates defined over the
inputs of the system under test. Given a test input, our assertion-based test
oracle determines, without requiring test execution, whether the test passes,
fails, or if the oracle is inconclusive in predicting a verdict. We describe
two methods for generating assertion-based test oracles: one using genetic
programming~(GP) that employs well-known spectrum-based fault localization
(SBFL) ranking formulas, namely Ochiai, Tarantula, and Naish, as fitness
functions; and the other using decision trees (DT) and decision rules (DR). We
evaluate our assertion-based test oracles through case studies in the domains
of aerospace, networking and autonomous driving. We show that test oracles
generated using GP with Ochiai are significantly more accurate than those
obtained using GP with Tarantula and Naish or using DT or DR. Moreover, this
accuracy advantage remains even when accounting for the flakiness of the system
under test. We further show that the assertion-based test oracles generated by
GP with Ochiai are robust against flakiness with only 4% average variation in
their accuracy results across four different network and autonomous driving
systems with flaky behaviours.

</details>


### [10] [Deep Learning Based Concurrency Bug Detection and Localization](https://arxiv.org/abs/2508.20911)
*Zuocheng Feng,Kaiwen Zhang,Miaomiao Wang,Yiming Cheng,Yuandao Cai,Xiaofeng Li,Guanjun Liu*

Main category: cs.SE

TL;DR: 提出了一种基于异构图神经网络和并发感知代码属性图的新方法，用于并发bug检测和定位，在准确率、精确率和召回率方面显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习方法面临三个主要限制：缺乏大规模专用并发bug数据集、并发语义表示不足、二进制分类结果无法提供细粒度调试信息。

Method: 构建专用并发bug数据集，将预训练模型与异构图神经网络结合，使用新的并发感知代码属性图(CCPG)表征并发语义，并采用SubgraphX方法精确定位bug到具体代码行。

Result: 在多样化评估设置中，平均准确率和精确率提高10%，召回率提高26%，显著优于最先进方法。

Conclusion: 该方法通过改进的语义表征和精确定位能力，有效解决了并发bug检测和定位的关键挑战，为软件可靠性提供了有力支持。

Abstract: Concurrency bugs, caused by improper synchronization of shared resources in
multi-threaded or distributed systems, are notoriously hard to detect and thus
compromise software reliability and security. The existing deep learning
methods face three main limitations. First, there is an absence of large and
dedicated datasets of diverse concurrency bugs for them. Second, they lack
sufficient representation of concurrency semantics. Third, binary
classification results fail to provide finer-grained debug information such as
precise bug lines. To address these problems, we propose a novel method for
effective concurrency bug detection as well as localization. We construct a
dedicated concurrency bug dataset to facilitate model training and evaluation.
We then integrate a pre-trained model with a heterogeneous graph neural network
(GNN), by incorporating a new Concurrency-Aware Code Property Graph (CCPG) that
concisely and effectively characterizes concurrency semantics. To further
facilitate debugging, we employ SubgraphX, a GNN-based interpretability method,
which explores the graphs to precisely localize concurrency bugs, mapping them
to specific lines of source code. On average, our method demonstrates an
improvement of 10\% in accuracy and precision and 26\% in recall compared to
state-of-the-art methods across diverse evaluation settings.

</details>


### [11] [ConfLogger: Enhance Systems' Configuration Diagnosability through Configuration Logging](https://arxiv.org/abs/2508.20977)
*Shiwen Shan,Yintong Huo,Yuxin Su,Zhining Wang,Dan Li,Zibin Zheng*

Main category: cs.SE

TL;DR: 提出了ConfLogger工具，通过配置感知的静态污点分析和LLM日志生成来增强软件配置可诊断性，在8个流行软件系统中验证了有效性，能帮助诊断工具达到100%错误定位准确率。


<details>
  <summary>Details</summary>
Motivation: 现代可配置系统虽然提供了灵活性，但也带来了配置相关问题。现有诊断支持主要关注故障后分析，但缺乏对软件是否提供足够故障信息进行诊断的关注。

Method: 开发ConfLogger工具，结合配置感知静态污点分析和基于LLM的日志生成：1）通过追踪配置相关数据流识别配置敏感代码段；2）通过分析配置代码上下文生成诊断日志语句。

Result: 在8个软件系统中验证有效：ConfLogger增强的日志帮助诊断工具在30个静默配置错误场景中达到100%错误定位准确率，80%可通过暴露的显式配置信息直接解决；覆盖现有日志点74%，优于基线LLM日志器12-30%；在变量日志方面精度提高8.6%，召回率提高79.3%，F1提高26.2%。用户研究显示诊断时间加快1.25倍，故障排除准确率提高251.4%。

Conclusion: ConfLogger通过增强配置日志有效提升了软件配置可诊断性，为配置相关问题的诊断提供了实用解决方案。

Abstract: Modern configurable systems offer customization via intricate configuration
spaces, yet such flexibility introduces pervasive configuration-related issues
such as misconfigurations and latent softwarebugs. Existing diagnosability
supports focus on post-failure analysis of software behavior to identify
configuration issues, but none of these approaches look into whether the
software clue sufficient failure information for diagnosis. To fill in the
blank, we propose the idea of configuration logging to enhance existing logging
practices at the source code level. We develop ConfLogger, the first tool that
unifies configuration-aware static taint analysis with LLM-based log generation
to enhance software configuration diagnosability. Specifically, our method 1)
identifies configuration-sensitive code segments by tracing
configuration-related data flow in the whole project, and 2) generates
diagnostic log statements by analyzing configuration code contexts. Evaluation
results on eight popular software systems demonstrate the effectiveness of
ConfLogger to enhance configuration diagnosability. Specifically,
ConfLogger-enhanced logs successfully aid a log-based misconfiguration
diagnosis tool to achieve 100% accuracy on error localization in 30 silent
misconfiguration scenarios, with 80% directly resolvable through explicit
configuration information exposed. In addition, ConfLogger achieves 74%
coverage of existing logging points, outperforming baseline LLM-based loggers
by 12% and 30%. It also gains 8.6% higher in precision, 79.3% higher in recall,
and 26.2% higher in F1 compared to the state-of-the-art baseline in terms of
variable logging while also augmenting diagnostic value. A controlled user
study on 22 cases further validated its utility, speeding up diagnostic time by
1.25x and improving troubleshooting accuracy by 251.4%.

</details>


### [12] [Dynamics of Gender Bias in Software Engineering](https://arxiv.org/abs/2508.21050)
*Thomas J. Misa*

Main category: cs.SE

TL;DR: 本文分析了软件工程领域的性别偏见问题，通过调查历史背景、分析ICSE会议作者性别分布，发现存在统计显著的性别排斥现象，并提出了相关政策建议。


<details>
  <summary>Details</summary>
Motivation: 软件工程领域植根于工程和计算机科学，这两个领域都存在固有的性别偏见。研究旨在调查该领域的性别偏见问题，特别是女性在研究发表中的参与情况。

Method: 采用混合研究方法：1) 调查软件工程的历史起源和对工程专业化的关注；2) 分析该领域近期对性别问题的关注；3) 定量分析1976-2010年ICSE国际会议中女性作者参与情况，识别统计显著的性别排斥年份。

Result: 研究发现ICSE会议存在12个年份具有统计显著的性别排斥现象，表明软件工程领域确实存在系统性性别偏见问题。

Conclusion: 软件工程领域需要关注和解决性别偏见问题，研究提出了计算领域性别偏见研究的政策维度建议，呼吁采取具体措施促进性别平等。

Abstract: The field of software engineering is embedded in both engineering and
computer science, and may embody gender biases endemic to both. This paper
surveys software engineering's origins and its long-running attention to
engineering professionalism, profiling five leaders; it then examines the
field's recent attention to gender issues and gender bias. It next
quantitatively analyzes women's participation as research authors in the
field's leading International Conference of Software Engineering (1976-2010),
finding a dozen years with statistically significant gender exclusion. Policy
dimensions of research on gender bias in computing are suggested.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [13] [AI Propaganda factories with language models](https://arxiv.org/abs/2508.20186)
*Lukasz Olejnik*

Main category: cs.CR

TL;DR: 小型语言模型能够在普通硬件上生成连贯的政治宣传内容，自动化影响力操作已成为现实，检测防御需要转向对话层面的检测和协调基础设施破坏


<details>
  <summary>Details</summary>
Motivation: 研究AI驱动的自动化影响力操作的可行性和特征，特别是小型语言模型在政治宣传内容生成方面的能力

Method: 使用小型语言模型生成人物角色驱动的政治信息，并通过自动化评估（无需人工评分）来测试其连贯性和效果

Result: 1. 人物角色设计比模型本身更能解释行为特征；2. 在需要反驳论证的互动中，意识形态坚持会加强，极端内容出现率增加；3. 完全自动化的影响力内容生产对大小行为体都已成为可能

Conclusion: 防御策略应从限制模型访问转向对话中心的检测和运动协调基础设施的破坏，自动化操作的一致性反而提供了检测特征

Abstract: AI-powered influence operations can now be executed end-to-end on commodity
hardware. We show that small language models produce coherent, persona-driven
political messaging and can be evaluated automatically without human raters.
Two behavioural findings emerge. First, persona-over-model: persona design
explains behaviour more than model identity. Second, engagement as a stressor:
when replies must counter-arguments, ideological adherence strengthens and the
prevalence of extreme content increases. We demonstrate that fully automated
influence-content production is within reach of both large and small actors.
Consequently, defence should shift from restricting model access towards
conversation-centric detection and disruption of campaigns and coordination
infrastructure. Paradoxically, the very consistency that enables these
operations also provides a detection signature.

</details>


### [14] [FlowMalTrans: Unsupervised Binary Code Translation for Malware Detection Using Flow-Adapter Architecture](https://arxiv.org/abs/2508.20212)
*Minghao Hu,Junzhe Wang,Weisen Zhao,Qiang Zeng,Lannan Luo*

Main category: cs.CR

TL;DR: 提出基于神经机器翻译和标准化流的跨指令集架构恶意软件检测方法，通过将其他架构的恶意代码翻译到X86-64架构，实现单一模型检测多架构恶意软件


<details>
  <summary>Details</summary>
Motivation: 随着针对IoT设备的网络攻击增加，跨多种指令集架构的恶意软件日益增多，但为每个架构收集和标注大量恶意软件样本成本高昂

Method: 结合神经机器翻译(NMT)和标准化流(NFs)技术，将不同指令集架构的恶意软件翻译到拥有充足样本的X86-64架构，从而利用单一架构训练的模型检测多架构恶意软件

Result: 该方法显著减少了数据收集工作量，实现了跨架构恶意软件检测能力

Conclusion: 提出的翻译方法有效解决了跨指令集架构恶意软件检测中的数据稀缺问题，为多架构恶意软件检测提供了实用解决方案

Abstract: Applying deep learning to malware detection has drawn great attention due to
its notable performance. With the increasing prevalence of cyberattacks
targeting IoT devices, there is a parallel rise in the development of malware
across various Instruction Set Architectures (ISAs). It is thus important to
extend malware detection capacity to multiple ISAs. However, training a deep
learning-based malware detection model usually requires a large number of
labeled malware samples. The process of collecting and labeling sufficient
malware samples to build datasets for each ISA is labor-intensive and
time-consuming. To reduce the burden of data collection, we propose to leverage
the ideas of Neural Machine Translation (NMT) and Normalizing Flows (NFs) for
malware detection. Specifically, when dealing with malware in a certain ISA, we
translate it to an ISA with sufficient malware samples (like X86-64). This
allows us to apply a model trained on one ISA to analyze malware from another
ISA. Our approach reduces the data collection effort by enabling malware
detection across multiple ISAs using a model trained on a single ISA.

</details>


### [15] [Robustness Assessment and Enhancement of Text Watermarking for Google's SynthID](https://arxiv.org/abs/2508.20228)
*Xia Han,Qi Li,Jianbing Ni,Mohammad Zulkernine*

Main category: cs.CR

TL;DR: SynGuard是一个混合水印框架，结合语义检索和概率水印机制，在词法和语义层面双重嵌入水印，相比SynthID-Text在F1分数上平均提升11.1%的水印恢复能力。


<details>
  <summary>Details</summary>
Motivation: 现有LLM水印方法如SynthID-Text在面对保持语义的攻击（如改写、复制粘贴修改、回译）时存在脆弱性，水印可检测性显著下降。

Method: 提出SynGuard混合框架，将语义信息检索（SIR）的语义对齐能力与SynthID-Text的概率水印机制相结合，在词法和语义层面联合嵌入水印。

Result: 在多种攻击场景下的实验结果表明，SynGuard相比SynthID-Text平均提升11.1%的F1分数水印恢复能力。

Conclusion: 语义感知水印方法能有效抵抗现实世界中的篡改攻击，证明了语义层面水印嵌入的重要性。

Abstract: Recent advances in LLM watermarking methods such as SynthID-Text by Google
DeepMind offer promising solutions for tracing the provenance of AI-generated
text. However, our robustness assessment reveals that SynthID-Text is
vulnerable to meaning-preserving attacks, such as paraphrasing, copy-paste
modifications, and back-translation, which can significantly degrade watermark
detectability. To address these limitations, we propose SynGuard, a hybrid
framework that combines the semantic alignment strength of Semantic Information
Retrieval (SIR) with the probabilistic watermarking mechanism of SynthID-Text.
Our approach jointly embeds watermarks at both lexical and semantic levels,
enabling robust provenance tracking while preserving the original meaning.
Experimental results across multiple attack scenarios show that SynGuard
improves watermark recovery by an average of 11.1\% in F1 score compared to
SynthID-Text. These findings demonstrate the effectiveness of semantic-aware
watermarking in resisting real-world tampering. All code, datasets, and
evaluation scripts are publicly available at:
https://github.com/githshine/SynGuard.

</details>


### [16] [Network-Level Prompt and Trait Leakage in Local Research Agents](https://arxiv.org/abs/2508.20282)
*Hyejun Jeong,Mohammadreze Teymoorianfard,Abhinav Kumar,Amir Houmansadr,Eugene Badasarian*

Main category: cs.CR

TL;DR: 研究发现基于语言模型的Web和研究代理（WRAs）存在网络元数据泄露风险，被动网络攻击者可通过访问IP地址和时间模式推断用户提示和特征，恢复73%的功能和领域知识。


<details>
  <summary>Details</summary>
Motivation: WRAs被组织和个人本地部署用于隐私、法律或财务目的，但其网络行为模式（访问70-140个域且有可识别的时间相关性）使其容易受到被动网络攻击者的指纹识别攻击。

Method: 构建基于用户搜索查询和合成角色生成的WRA痕迹数据集，定义OBELS行为指标评估原始提示与推断提示的相似性，并在多会话设置下扩展攻击。

Result: 攻击成功恢复超过73%的用户提示功能和领域知识，在多会话设置中准确恢复32个潜在特征中的19个，即使在部分可观察性和噪声条件下仍保持有效。

Conclusion: WRAs存在严重的隐私泄露风险，提出的缓解策略（限制域多样性或混淆痕迹）可将攻击效果平均降低29%且对实用性影响可忽略。

Abstract: We show that Web and Research Agents (WRAs) -- language model-based systems
that investigate complex topics on the Internet -- are vulnerable to inference
attacks by passive network adversaries such as ISPs. These agents could be
deployed \emph{locally} by organizations and individuals for privacy, legal, or
financial purposes. Unlike sporadic web browsing by humans, WRAs visit
$70{-}140$ domains with distinguishable timing correlations, enabling unique
fingerprinting attacks.
  Specifically, we demonstrate a novel prompt and user trait leakage attack
against WRAs that only leverages their network-level metadata (i.e., visited IP
addresses and their timings). We start by building a new dataset of WRA traces
based on user search queries and queries generated by synthetic personas. We
define a behavioral metric (called OBELS) to comprehensively assess similarity
between original and inferred prompts, showing that our attack recovers over
73\% of the functional and domain knowledge of user prompts. Extending to a
multi-session setting, we recover up to 19 of 32 latent traits with high
accuracy. Our attack remains effective under partial observability and noisy
conditions. Finally, we discuss mitigation strategies that constrain domain
diversity or obfuscate traces, showing negligible utility impact while reducing
attack effectiveness by an average of 29\%.

</details>


### [17] [Surveying the Operational Cybersecurity and Supply Chain Threat Landscape when Developing and Deploying AI Systems](https://arxiv.org/abs/2508.20307)
*Michael R Smith,Joe Ingram*

Main category: cs.CR

TL;DR: AI技术的快速发展带来了新的网络安全威胁，传统安全评估方法无法有效应对针对AI系统的攻击，攻击者目标从传统权限提升转向操纵AI输出结果。


<details>
  <summary>Details</summary>
Motivation: 随着AI技术在软件硬件领域的广泛应用，传统网络安全评估方法无法覆盖AI系统特有的攻击面和攻击目标，需要提高对AI系统新型网络安全威胁的认识。

Method: 通过分析AI生命周期中的运营网络安全和供应链风险，研究已有的攻击案例，提出针对AI驱动环境的安全框架需求。

Result: 识别了AI系统特有的新型网络威胁类型，包括系统性能降级、虚假输出泛滥、模型精度下降等攻击目标，强调了定制化安全框架的必要性。

Conclusion: 组织需要充分认识AI系统特有的网络安全风险，建立专门的安全防护体系，以确保AI系统的可靠性、韧性和安全性。

Abstract: The rise of AI has transformed the software and hardware landscape, enabling
powerful capabilities through specialized infrastructures, large-scale data
storage, and advanced hardware. However, these innovations introduce unique
attack surfaces and objectives which traditional cybersecurity assessments
often overlook. Cyber attackers are shifting their objectives from conventional
goals like privilege escalation and network pivoting to manipulating AI outputs
to achieve desired system effects, such as slowing system performance, flooding
outputs with false positives, or degrading model accuracy. This paper serves to
raise awareness of the novel cyber threats that are introduced when
incorporating AI into a software system. We explore the operational
cybersecurity and supply chain risks across the AI lifecycle, emphasizing the
need for tailored security frameworks to address evolving threats in the
AI-driven landscape. We highlight previous exploitations and provide insights
from working in this area. By understanding these risks, organizations can
better protect AI systems and ensure their reliability and resilience.

</details>


### [18] [MindGuard: Tracking, Detecting, and Attributing MCP Tool Poisoning Attack via Decision Dependence Graph](https://arxiv.org/abs/2508.20412)
*Zhiqiang Wang,Junyang Zhang,Guanquan Shi,HaoRan Cheng,Yunhao Yao,Kaiwen Guo,Haohua Du,Xiang-Yang Li*

Main category: cs.CR

TL;DR: MindGuard是一个针对LLM代理的决策级防护系统，通过注意力机制构建决策依赖图来检测和溯源工具中毒攻击，在保持高效的同时实现高精度检测。


<details>
  <summary>Details</summary>
Motivation: 随着模型上下文协议(MCP)的普及，工具中毒攻击(TPA)成为新威胁，现有基于行为分析的防御方法对此无效，因为中毒工具可能不被执行而无法留下行为痕迹。

Method: 利用LLM注意力机制与工具调用决策的强相关性，构建决策依赖图(DDG)来建模推理过程，设计基于图的异常分析机制来检测和溯源TPA攻击。

Result: 在真实数据集上的实验显示，MindGuard达到94%-99%的平均检测精度，95%-100%的溯源准确率，处理时间低于1秒且无额外token成本。

Conclusion: MindGuard提供了一种有效的决策级防护方案，DDG作为经典程序依赖图的适配，为在决策层面应用传统安全策略奠定了基础。

Abstract: The Model Context Protocol (MCP) is increasingly adopted to standardize the
interaction between LLM agents and external tools. However, this trend
introduces a new threat: Tool Poisoning Attacks (TPA), where tool metadata is
poisoned to induce the agent to perform unauthorized operations. Existing
defenses that primarily focus on behavior-level analysis are fundamentally
ineffective against TPA, as poisoned tools need not be executed, leaving no
behavioral trace to monitor.
  Thus, we propose MindGuard, a decision-level guardrail for LLM agents,
providing provenance tracking of call decisions, policy-agnostic detection, and
poisoning source attribution against TPA. While fully explaining LLM decision
remains challenging, our empirical findings uncover a strong correlation
between LLM attention mechanisms and tool invocation decisions. Therefore, we
choose attention as an empirical signal for decision tracking and formalize
this as the Decision Dependence Graph (DDG), which models the LLM's reasoning
process as a weighted, directed graph where vertices represent logical concepts
and edges quantify the attention-based dependencies. We further design robust
DDG construction and graph-based anomaly analysis mechanisms that efficiently
detect and attribute TPA attacks. Extensive experiments on real-world datasets
demonstrate that MindGuard achieves 94\%-99\% average precision in detecting
poisoned invocations, 95\%-100\% attribution accuracy, with processing times
under one second and no additional token cost. Moreover, DDG can be viewed as
an adaptation of the classical Program Dependence Graph (PDG), providing a
solid foundation for applying traditional security policies at the decision
level.

</details>


### [19] [Federated Learning for Large Models in Medical Imaging: A Comprehensive Review](https://arxiv.org/abs/2508.20414)
*Mengyu Sun,Ziyuan Yang,Yongqiang Huang,Hui Yu,Yingyu Chen,Shuren Qi,Andrew Beng Jin Teoh,Yi Zhang*

Main category: cs.CR

TL;DR: 本综述论文探讨了联邦学习在医学影像全流程分析中的应用，包括上游重建任务和下游临床诊断任务，旨在解决医疗数据隐私保护下的模型训练难题。


<details>
  <summary>Details</summary>
Motivation: 由于严格的患者隐私法规和数据共享限制，传统集中式训练大规模AI模型在医疗领域面临挑战，需要寻找隐私保护的分布式训练解决方案。

Method: 采用联邦学习框架，在医学影像全流程中实现：1）上游CT/MRI重建任务的联合训练；2）下游肿瘤诊断和分割任务的持续模型更新，通过本地微调而不集中敏感数据。

Result: 联邦学习能够有效缓解数据稀缺问题，保护患者隐私，同时支持多机构数据集上的协作模型开发，提高通信效率和对齐异构数据。

Conclusion: 联邦学习为医学影像AI发展提供了重要的隐私保护解决方案，未来需要进一步研究通信效率提升、异构数据对齐和安全参数聚合等方向。

Abstract: Artificial intelligence (AI) has demonstrated considerable potential in the
realm of medical imaging. However, the development of high-performance AI
models typically necessitates training on large-scale, centralized datasets.
This approach is confronted with significant challenges due to strict patient
privacy regulations and legal restrictions on data sharing and utilization.
These limitations hinder the development of large-scale models in medical
domains and impede continuous updates and training with new data. Federated
Learning (FL), a privacy-preserving distributed training framework, offers a
new solution by enabling collaborative model development across fragmented
medical datasets. In this survey, we review FL's contributions at two stages of
the full-stack medical analysis pipeline. First, in upstream tasks such as CT
or MRI reconstruction, FL enables joint training of robust reconstruction
networks on diverse, multi-institutional datasets, alleviating data scarcity
while preserving confidentiality. Second, in downstream clinical tasks like
tumor diagnosis and segmentation, FL supports continuous model updating by
allowing local fine-tuning on new data without centralizing sensitive images.
We comprehensively analyze FL implementations across the medical imaging
pipeline, from physics-informed reconstruction networks to diagnostic AI
systems, highlighting innovations that improve communication efficiency, align
heterogeneous data, and ensure secure parameter aggregation. Meanwhile, this
paper provides an outlook on future research directions, aiming to serve as a
valuable reference for the field's development.

</details>


### [20] [Breaking Diffusion with Cache: Exploiting Approximate Caches in Diffusion Models](https://arxiv.org/abs/2508.20424)
*Desen Sun,Shuncheng Jie,Sihang Liu*

Main category: cs.CR

TL;DR: 扩散模型近似缓存存在严重安全漏洞，包括远程隐蔽信道、提示词窃取和投毒攻击，攻击者可通过缓存机制远程实施信息交换、窃取提示词并植入恶意内容


<details>
  <summary>Details</summary>
Motivation: 扩散模型计算成本高，近似缓存技术被广泛采用以重用相似提示词的中间状态，但这种优化打破了用户间的隔离，引入了新的安全风险，需要全面评估这些安全漏洞

Method: 通过三种攻击方式评估安全漏洞：1）建立远程隐蔽信道，发送者注入特殊关键词到缓存，接收者可长时间后恢复信息；2）提示词窃取攻击，基于缓存命中提示词恢复已缓存提示词；3）投毒攻击，在被窃取的提示词中嵌入攻击者标识，影响未来命中缓存的用户提示词

Result: 所有攻击均可通过服务系统远程实施，证明了近似缓存存在严重的安全漏洞，攻击者能够实现信息交换、窃取敏感提示词内容并对其他用户输出造成污染

Conclusion: 扩散模型中的近似缓存优化虽然提高了效率，但带来了严重的安全隐患，需要重新设计缓存机制以确保用户隔离和系统安全性

Abstract: Diffusion models are a powerful class of generative models that produce
content, such as images, from user prompts, but they are computationally
intensive. To mitigate this cost, recent academic and industry work has adopted
approximate caching, which reuses intermediate states from similar prompts in a
cache. While efficient, this optimization introduces new security risks by
breaking isolation among users. This work aims to comprehensively assess new
security vulnerabilities arising from approximate caching. First, we
demonstrate a remote covert channel established with the cache, where a sender
injects prompts with special keywords into the cache and a receiver can recover
that even after days, to exchange information. Second, we introduce a prompt
stealing attack using the cache, where an attacker can recover existing cached
prompts based on cache hit prompts. Finally, we introduce a poisoning attack
that embeds the attacker's logos into the previously stolen prompt, to render
them in future user prompts that hit the cache. These attacks are all performed
remotely through the serving system, which indicates severe security
vulnerabilities in approximate caching.

</details>


### [21] [Ransomware 3.0: Self-Composing and LLM-Orchestrated](https://arxiv.org/abs/2508.20444)
*Md Raz,Meet Udeshi,P. V. Sai Charan,Prashanth Krishnamurthy,Farshad Khorrami,Ramesh Karri*

Main category: cs.CR

TL;DR: 本文提出Ransomware 3.0，首个利用大语言模型自主策划和执行勒索软件攻击全生命周期的威胁模型，通过自然语言提示动态生成恶意代码，实现自适应多态变种。


<details>
  <summary>Details</summary>
Motivation: 传统勒索软件存在静态特征易被检测的问题，需要探索LLM在动态代码生成和自适应攻击方面的潜在威胁，以推动更好的防御机制发展。

Method: 构建LLM驱动的勒索软件原型，仅需二进制文件中嵌入自然语言提示，运行时由LLM动态合成恶意代码，实现侦察、载荷生成和个性化勒索的闭环攻击流程。

Result: 实验表明开源LLM能够生成功能性勒索软件组件，并在个人、企业和嵌入式环境中维持闭环执行，展示了新型AI驱动勒索攻击的可行性。

Conclusion: Ransomware 3.0揭示了LLM orchestrated勒索攻击的现实威胁，需要开发多级遥测和行为信号检测等新型防御机制来应对AI赋能的网络安全挑战。

Abstract: Using automated reasoning, code synthesis, and contextual decision-making, we
introduce a new threat that exploits large language models (LLMs) to
autonomously plan, adapt, and execute the ransomware attack lifecycle.
Ransomware 3.0 represents the first threat model and research prototype of
LLM-orchestrated ransomware. Unlike conventional malware, the prototype only
requires natural language prompts embedded in the binary; malicious code is
synthesized dynamically by the LLM at runtime, yielding polymorphic variants
that adapt to the execution environment. The system performs reconnaissance,
payload generation, and personalized extortion, in a closed-loop attack
campaign without human involvement. We evaluate this threat across personal,
enterprise, and embedded environments using a phase-centric methodology that
measures quantitative fidelity and qualitative coherence in each attack phase.
We show that open source LLMs can generate functional ransomware components and
sustain closed-loop execution across diverse environments. Finally, we present
behavioral signals and multi-level telemetry of Ransomware 3.0 through a case
study to motivate future development of better defenses and policy enforcements
to address novel AI-enabled ransomware attacks.

</details>


### [22] [Enhancing Resilience for IoE: A Perspective of Networking-Level Safeguard](https://arxiv.org/abs/2508.20504)
*Guan-Yan Yang,Jui-Ning Chen,Farn Wang,Kuo-Hui Yeh*

Main category: cs.CR

TL;DR: 提出基于图结构学习(GSL)的防护框架，通过联合优化图拓扑和节点表示来抵御IoE网络中的对抗性攻击，相比传统方法具有更强的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 能源物联网(IoE)的互联性使关键基础设施面临复杂网络威胁，这些威胁比一般IoT风险具有更高的公共安全后果，需要弹性解决方案。

Method: 采用图结构学习(GSL)方法，联合优化图拓扑结构和节点表示，从网络层面提供固有防护机制来抵抗对抗性网络模型操纵。

Result: 通过概念概述、架构讨论和安全数据集案例研究，证明GSL相比代表性方法具有优越的鲁棒性，为实践者提供了保护IoE网络的有效途径。

Conclusion: GSL有潜力增强未来IoE网络的弹性和可靠性，为关键基础设施管理者提供安全保障，同时指出了该新兴研究领域的关键开放挑战和未来研究方向。

Abstract: The Internet of Energy (IoE) integrates IoT-driven digital communication with
power grids to enable efficient and sustainable energy systems. Still, its
interconnectivity exposes critical infrastructure to sophisticated cyber
threats, including adversarial attacks designed to bypass traditional
safeguards. Unlike general IoT risks, IoE threats have heightened public safety
consequences, demanding resilient solutions. From the networking-level
safeguard perspective, we propose a Graph Structure Learning (GSL)-based
safeguards framework that jointly optimizes graph topology and node
representations to resist adversarial network model manipulation inherently.
Through a conceptual overview, architectural discussion, and case study on a
security dataset, we demonstrate GSL's superior robustness over representative
methods, offering practitioners a viable path to secure IoE networks against
evolving attacks. This work highlights the potential of GSL to enhance the
resilience and reliability of future IoE networks for practitioners managing
critical infrastructure. Lastly, we identify key open challenges and propose
future research directions in this novel research area.

</details>


### [23] [Characterizing Trust Boundary Vulnerabilities in TEE Containers](https://arxiv.org/abs/2508.20962)
*Weijie Liu,Hongbo Chen,Shuo Huai,Zhen Xu,Wenhao Wang,Zhi Li,Zheli Liu*

Main category: cs.CR

TL;DR: 本文分析了现有TEE容器的隔离策略，开发了自动化分析工具来评估其隔离边界，发现多个关键安全漏洞，并提出了改进建议。


<details>
  <summary>Details</summary>
Motivation: TEE容器作为机密计算的关键中间件，旨在保护应用程序免受恶意操作系统和编排接口的攻击，但其隔离策略的有效性需要系统评估。

Method: 设计了自动化分析器来精确识别和评估TEE容器的隔离边界，分析其设计实现中的安全缺陷。

Result: 发现多个TEE容器存在关键设计实现缺陷，包括信息泄露、回滚攻击、拒绝服务和Iago攻击等安全风险。

Conclusion: 总结了关键经验教训指导更安全容器解决方案的开发，并讨论了TEE容器化设计的新兴趋势。

Abstract: Trusted Execution Environments (TEEs) have emerged as a cornerstone of
confidential computing, garnering significant attention from both academia and
industry. To enable the secure development, execution, and deployment, of
applications on TEE platforms, TEE containers have been introduced as
middleware solutions. These containers aim to shield applications from
potentially malicious operating systems and orchestration interfaces while
maintaining usability and reliability. In this paper, we analyze the isolation
strategies employed by existing TEE containers to protect secure applications.
To address the challenges in analyzing these interfaces, we designed an
automated analyzer to precisely identify and evaluate their isolation
boundaries. We observed that some TEE containers fail to achieve their intended
goals due to critical design and implementation flaws, such as information
leakage, rollback attacks, denial-of-service, and Iago attacks, which pose
significant security risks. Drawing from our findings, we share key lessons to
guide the development of more secure container solutions and discuss emerging
trends in TEE containerization design.

</details>


### [24] [BridgeShield: Enhancing Security for Cross-chain Bridge Applications via Heterogeneous Graph Mining](https://arxiv.org/abs/2508.20517)
*Dan Lin,Shunfeng Lu,Ziyan Liu,Jiajing Wu,Junyuan Fang,Kaixin Lin,Bowen Song,Zibin Zheng*

Main category: cs.CR

TL;DR: BridgeShield是一个基于异构图注意力网络的跨链桥攻击检测框架，通过统一建模源链、链下协调和目标链，能够精确识别攻击行为，在真实攻击事件上达到92.58%的F1分数。


<details>
  <summary>Details</summary>
Motivation: 跨链桥在区块链互操作性中至关重要，但由于设计缺陷和巨大价值，成为黑客攻击的主要目标。现有检测方法主要处理单链行为，无法捕捉跨链语义。

Method: 利用异构图注意力网络建模多类型实体和关系，捕捉跨链行为的复杂执行语义。提出BridgeShield框架，在统一异构图表示中联合建模源链、链下协调和目标链，包含元路径内注意力和元路径间注意力机制。

Result: 在51个真实跨链攻击事件上的实验表明，BridgeShield平均F1分数达到92.58%，比最先进基线方法提升24.39%。

Conclusion: BridgeShield是保护跨链桥安全和增强多链生态系统韧性的有效解决方案。

Abstract: Cross-chain bridges play a vital role in enabling blockchain
interoperability. However, due to the inherent design flaws and the enormous
value they hold, they have become prime targets for hacker attacks. Existing
detection methods show progress yet remain limited, as they mainly address
single-chain behaviors and fail to capture cross-chain semantics. To address
this gap, we leverage heterogeneous graph attention networks, which are
well-suited for modeling multi-typed entities and relations, to capture the
complex execution semantics of cross-chain behaviors. We propose BridgeShield,
a detection framework that jointly models the source chain, off-chain
coordination, and destination chain within a unified heterogeneous graph
representation. BridgeShield incorporates intra-meta-path attention to learn
fine-grained dependencies within cross-chain paths and inter-meta-path
attention to highlight discriminative cross-chain patterns, thereby enabling
precise identification of attack behaviors. Extensive experiments on 51
real-world cross-chain attack events demonstrate that BridgeShield achieves an
average F1-score of 92.58%, representing a 24.39% improvement over
state-of-the-art baselines. These results validate the effectiveness of
BridgeShield as a practical solution for securing cross-chain bridges and
enhancing the resilience of multi-chain ecosystems.

</details>


### [25] [Bitcoin as an Interplanetary Monetary Standard with Proof-of-Transit Timestamping](https://arxiv.org/abs/2508.20591)
*Jose E. Puente,Carlos Puente*

Main category: cs.CR

TL;DR: 探索比特币作为地球和火星共享货币标准的可行性，提出Proof-of-Transit Timestamping (PoTT)原语来解决星际通信的高延迟和间歇连接问题，通过DTN网络和光学LEO星座实现跨行星比特币数据传输的安全审计。


<details>
  <summary>Details</summary>
Motivation: 解决地球与火星之间高延迟、间歇性连接的星际通信约束，使比特币能够在行星间作为共享货币标准部署，同时保持比特币共识和货币基础不变。

Method: 引入PoTT原语提供加密的防篡改审计跟踪，利用延迟/中断容忍网络(DTN)和光学低地球轨道(LEO)网状星座，提出头优先复制架构、长视野闪电通道与行星观察塔，以及通过联邦侧链或盲合并挖矿(BMM)提交链实现安全结算。

Result: PoTT显著提高了跨行星比特币数据传输的可靠性和可问责性，无需改变比特币共识或货币基础。近期部署倾向于强联邦本地结算，长期来看盲合并挖矿提交链提供替代方案。

Conclusion: 比特币可以作为地球-火星共享货币标准，通过PoTT和提出的架构解决星际通信挑战，火星可以运行1:1锚定的提交链或强联邦进行本地区块生产，同时保持地球L1货币基础不变。

Abstract: We explore the feasibility of deploying Bitcoin as the shared monetary
standard between Earth and Mars, accounting for physical constraints of
interplanetary communication. We introduce a novel primitive, Proof-of-Transit
Timestamping (PoTT), to provide cryptographic, tamper-evident audit trails for
Bitcoin data across high-latency, intermittently-connected links. Leveraging
Delay/Disruption-Tolerant Networking (DTN) and optical low-Earth-orbit (LEO)
mesh constellations, we propose an architecture for header-first replication,
long-horizon Lightning channels with planetary watchtowers, and secure
settlement through federated sidechains or blind-merge-mined (BMM) commit
chains. We formalize PoTT, analyze its security model, and show how it
measurably improves reliability and accountability without altering Bitcoin
consensus or its monetary base. Near-term deployments favor strong federations
for local settlement; longer-term, blind-merge-mined commit chains (if adopted)
provide an alternative. The Earth L1 monetary base remains unchanged, while
Mars can operate a pegged commit chain or strong federation with 1:1 pegged
assets for local block production. For transparency, if both time-beacon
regimes are simultaneously compromised, PoTT-M2 (and PoTT generally) reduces to
administrative assertions rather than cryptographic time-anchoring.

</details>


### [26] [CyberSleuth: Autonomous Blue-Team LLM Agent for Web Attack Forensics](https://arxiv.org/abs/2508.20643)
*Stefano Fumero,Kai Huang,Matteo Boffa,Danilo Giordano,Marco Mellia,Zied Ben Houidi,Dario Rossi*

Main category: cs.CR

TL;DR: 本文提出了CyberSleuth，一个基于LLM的自主代理系统，用于网络取证调查，能够分析数据包痕迹和应用日志来识别攻击服务、漏洞(CVE)和攻击成功率。


<details>
  <summary>Details</summary>
Motivation: 当前LLM代理在网络安全领域主要应用于红队操作，而防御性用途如事件响应和取证研究相对较少，处于早期阶段。

Method: 提出CyberSleuth系统，集成工具和代理架构，处理数据包级痕迹和应用日志，评估了4种代理架构和6种LLM后端在20个复杂程度递增的事件场景中的表现。

Result: 在2025年的10个事件测试中，CyberSleuth正确识别确切CVE的比例达到80%。人类专家研究显示，22位专家认为CyberSleuth的报告完整、有用且连贯，并对开源LLM DeepSeek R1表现出轻微偏好。

Conclusion: CyberSleuth是性能最佳的设计，为防御性LLM研究提供了基准和平台，支持公平、可重现的取证代理评估。

Abstract: Large Language Model (LLM) agents are powerful tools for automating complex
tasks. In cybersecurity, researchers have primarily explored their use in
red-team operations such as vulnerability discovery and penetration tests.
Defensive uses for incident response and forensics have received comparatively
less attention and remain at an early stage. This work presents a systematic
study of LLM-agent design for the forensic investigation of realistic web
application attacks. We propose CyberSleuth, an autonomous agent that processes
packet-level traces and application logs to identify the targeted service, the
exploited vulnerability (CVE), and attack success. We evaluate the consequences
of core design decisions - spanning tool integration and agent architecture -
and provide interpretable guidance for practitioners. We benchmark four agent
architectures and six LLM backends on 20 incident scenarios of increasing
complexity, identifying CyberSleuth as the best-performing design. In a
separate set of 10 incidents from 2025, CyberSleuth correctly identifies the
exact CVE in 80% of cases. At last, we conduct a human study with 22 experts,
which rated the reports of CyberSleuth as complete, useful, and coherent. They
also expressed a slight preference for DeepSeek R1, a good news for open source
LLM. To foster progress in defensive LLM research, we release both our
benchmark and the CyberSleuth platform as a foundation for fair, reproducible
evaluation of forensic agents.

</details>


### [27] [Multi-Agent Penetration Testing AI for the Web](https://arxiv.org/abs/2508.20816)
*Isaac David,Arthur Gervais*

Main category: cs.CR

TL;DR: MAPTA是一个多代理系统，用于自主Web应用安全评估，结合LLM编排、工具执行和端到端漏洞验证，在XBOW基准测试中达到76.9%成功率，成本效益显著。


<details>
  <summary>Details</summary>
Motivation: AI代码生成导致软件开发速度远超安全审计能力，研究表明40%的AI生成代码存在漏洞，需要自动化安全评估解决方案。

Method: 多代理系统结合大型语言模型编排、工具基础执行和端到端漏洞验证，实现自主Web应用安全评估。

Result: 在104个挑战的XBOW基准测试中：总成功率76.9%，SSRF和配置错误漏洞100%成功，授权漏洞83%成功，模板注入85%，SQL注入83%。发现真实世界关键漏洞包括RCE、命令注入等，平均成本仅3.67美元。

Conclusion: MAPTA证明了多代理系统在自动化安全评估中的有效性，虽然XSS和盲注SQL仍有挑战，但成本效益显著，适合大规模应用。

Abstract: AI-powered development platforms are making software creation accessible to a
broader audience, but this democratization has triggered a scalability crisis
in security auditing. With studies showing that up to 40% of AI-generated code
contains vulnerabilities, the pace of development now vastly outstrips the
capacity for thorough security assessment.
  We present MAPTA, a multi-agent system for autonomous web application
security assessment that combines large language model orchestration with
tool-grounded execution and end-to-end exploit validation. On the 104-challenge
XBOW benchmark, MAPTA achieves 76.9% overall success with perfect performance
on SSRF and misconfiguration vulnerabilities, 83% success on broken
authorization, and strong results on injection attacks including server-side
template injection (85%) and SQL injection (83%). Cross-site scripting (57%)
and blind SQL injection (0%) remain challenging. Our comprehensive cost
analysis across all challenges totals $21.38 with a median cost of $0.073 for
successful attempts versus $0.357 for failures. Success correlates strongly
with resource efficiency, enabling practical early-stopping thresholds at
approximately 40 tool calls or $0.30 per challenge.
  MAPTA's real-world findings are impactful given both the popularity of the
respective scanned GitHub repositories (8K-70K stars) and MAPTA's low average
operating cost of $3.67 per open-source assessment: MAPTA discovered critical
vulnerabilities including RCEs, command injections, secret exposure, and
arbitrary file write vulnerabilities. Findings are responsibly disclosed, 10
findings are under CVE review.

</details>


### [28] [JADES: A Universal Framework for Jailbreak Assessment via Decompositional Scoring](https://arxiv.org/abs/2508.20848)
*Junjie Chu,Mingjie Li,Ziqing Yang,Ye Leng,Chenhao Lin,Chao Shen,Michael Backes,Yun Shen,Yang Zhang*

Main category: cs.CR

TL;DR: JADES是一个通过分解评分来评估越狱攻击成功率的框架，通过将有害问题分解为加权子问题并聚合评分，在二元评估中达到98.5%的人类评估一致性。


<details>
  <summary>Details</summary>
Motivation: 现有越狱评估方法依赖不准确的代理指标或简单整体判断，经常误解模型响应，导致与人类感知不一致的主观评估。

Method: JADES自动将有害输入问题分解为加权子问题，对每个子答案评分，并通过加权聚合得出最终决策，还包含可选的事实核查模块来检测幻觉。

Result: 在JailbreakQR基准测试中，JADES在二元评估中达到98.5%的人类评估一致性，比基线方法提升超过9%。重新评估显示现有方法高估了攻击成功率（如GPT-3.5-Turbo从93%降至69%）。

Conclusion: JADES能够提供准确、一致且可解释的评估，为未来越狱攻击的测量提供了可靠基础。

Abstract: Accurately determining whether a jailbreak attempt has succeeded is a
fundamental yet unresolved challenge. Existing evaluation methods rely on
misaligned proxy indicators or naive holistic judgments. They frequently
misinterpret model responses, leading to inconsistent and subjective
assessments that misalign with human perception. To address this gap, we
introduce JADES (Jailbreak Assessment via Decompositional Scoring), a universal
jailbreak evaluation framework. Its key mechanism is to automatically decompose
an input harmful question into a set of weighted sub-questions, score each
sub-answer, and weight-aggregate the sub-scores into a final decision. JADES
also incorporates an optional fact-checking module to strengthen the detection
of hallucinations in jailbreak responses. We validate JADES on JailbreakQR, a
newly introduced benchmark proposed in this work, consisting of 400 pairs of
jailbreak prompts and responses, each meticulously annotated by humans. In a
binary setting (success/failure), JADES achieves 98.5% agreement with human
evaluators, outperforming strong baselines by over 9%. Re-evaluating five
popular attacks on four LLMs reveals substantial overestimation (e.g., LAA's
attack success rate on GPT-3.5-Turbo drops from 93% to 69%). Our results show
that JADES could deliver accurate, consistent, and interpretable evaluations,
providing a reliable basis for measuring future jailbreak attacks.

</details>


### [29] [Publish to Perish: Prompt Injection Attacks on LLM-Assisted Peer Review](https://arxiv.org/abs/2508.20863)
*Matteo Gioele Collu,Umberto Salviati,Roberto Confalonieri,Mauro Conti,Giovanni Apruzzese*

Main category: cs.CR

TL;DR: 论文研究了在科学论文PDF中嵌入隐藏提示注入攻击，通过对抗性文本影响LLM生成的评审结果，揭示了LLM在同行评审中的安全漏洞。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型越来越多地应用于科学同行评审过程，需要评估其对隐藏提示注入攻击的抵抗能力，以防止作者通过嵌入对抗性文本来操纵评审结果。

Method: 研究设计了三种威胁模型，创建了对人类读者不可见但能引导LLM输出的对抗性提示，并通过用户研究获取代表性评审提示，在不同LLM系统和论文上进行测试。

Result: 实验结果表明对抗性提示能够可靠地误导LLM，有时会对"诚实但懒惰"的评审者产生不利影响，同时提出了降低自动内容检测可检测性的方法。

Conclusion: 该研究揭示了LLM在科学同行评审中的安全脆弱性，强调了需要开发更强大的防御机制来防止此类隐藏提示注入攻击。

Abstract: Large Language Models (LLMs) are increasingly being integrated into the
scientific peer-review process, raising new questions about their reliability
and resilience to manipulation. In this work, we investigate the potential for
hidden prompt injection attacks, where authors embed adversarial text within a
paper's PDF to influence the LLM-generated review. We begin by formalising
three distinct threat models that envision attackers with different motivations
-- not all of which implying malicious intent. For each threat model, we design
adversarial prompts that remain invisible to human readers yet can steer an
LLM's output toward the author's desired outcome. Using a user study with
domain scholars, we derive four representative reviewing prompts used to elicit
peer reviews from LLMs. We then evaluate the robustness of our adversarial
prompts across (i) different reviewing prompts, (ii) different commercial
LLM-based systems, and (iii) different peer-reviewed papers. Our results show
that adversarial prompts can reliably mislead the LLM, sometimes in ways that
adversely affect a "honest-but-lazy" reviewer. Finally, we propose and
empirically assess methods to reduce detectability of adversarial prompts under
automated content checks.

</details>


### [30] [AI Agentic Vulnerability Injection And Transformation with Optimized Reasoning](https://arxiv.org/abs/2508.20866)
*Amine Lbath,Massih-Reza Amini,Aurelien Delaitre,Vadim Okun*

Main category: cs.CR

TL;DR: 这篇论文提出了一种新题的AI驱动框架，用于在安全的C/C++代码库中自动注入真实的漏洞，以生成高质量的训练数据集。该方法结合了多个AI代理、传统代码分析工具和知识获取技术，在函数级别实现了89%-95%的漏洞注入成功率。


<details>
  <summary>Details</summary>
Motivation: 软件系统越来越复杂，网络攻击越来越精巧，传统的静态程序分析方法在可扩展性、适配性和误报率方面面临挑战。AI驱动的漏洞检测方法虽有希望，但强依赖于高质量的训练数据集。

Method: 提出了一种新题框架，能够在安全的C/C++代码库中自动注入真实的类别特定漏洞。该方法协调多个AI代理（模拟专家思维）、函数代理和传统代码分析工具。利用知识获取生成技术进行上下文基础，并采用低秩近似权重方法进行高效模型微调。

Result: 在3个不同标准测试集的116个代码样本上进行实验，该方法在数据集准确性方面表现超过其他技术。在函数级别实现了89%到95%的漏洞注入成功率。

Conclusion: 该研究提供了一种有效的方法来生成高质量的漏洞检测训练数据集，解决了AI驱动安全分析方法在数据质量上的依赖问题。通过结合多种AI技术和传统工具，该框架能够生成更加真实和可靠的漏洞数据集。

Abstract: The increasing complexity of software systems and the sophistication of
cyber-attacks have underscored the critical need for effective automated
vulnerability detection and repair systems. Traditional methods, such as static
program analysis, face significant challenges related to scalability,
adaptability, and high false-positive and false-negative rates. AI-driven
approaches, particularly those using machine learning and deep learning models,
show promise but are heavily reliant on the quality and quantity of training
data. This paper introduces a novel framework designed to automatically
introduce realistic, category-specific vulnerabilities into secure C/C++
codebases to generate datasets. The proposed approach coordinates multiple AI
agents that simulate expert reasoning, along with function agents and
traditional code analysis tools. It leverages Retrieval-Augmented Generation
for contextual grounding and employs Low-Rank approximation of weights for
efficient model fine-tuning. Our experimental study on 116 code samples from
three different benchmarks suggests that our approach outperforms other
techniques with regard to dataset accuracy, achieving between 89\% and 95\%
success rates in injecting vulnerabilities at function level.

</details>


### [31] [PromptSleuth: Detecting Prompt Injection via Semantic Intent Invariance](https://arxiv.org/abs/2508.20890)
*Mengxiao Wang,Yuxuan Zhang,Guofei Gu*

Main category: cs.CR

TL;DR: 该论文构建了一个新的Prompt Injection攻击基准测试，并提出基于语义意图推理的防御框架PromptSleuth，在保持效率的同时显著优于现有防御方法。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型在现实应用中的广泛部署，Prompt Injection攻击形式不断演变，现有基准测试已无法全面评估新兴威胁，需要更全面的评估环境和更鲁棒的防御方案。

Method: 构建了系统性的新基准测试，包含传统攻击技术和多任务场景；提出PromptSleuth防御框架，通过语义推理检测任务级意图而非表面特征来识别注入攻击。

Result: 新基准测试显示现有防御方法存在明显弱点；PromptSleuth在所有先进基准测试中 consistently outperforms 现有防御方法，同时保持相当的运行时间和成本效率。

Conclusion: 基于意图的语义推理为防御LLM对抗不断演变的Prompt Injection威胁提供了鲁棒、高效且可泛化的策略。

Abstract: Large Language Models (LLMs) are increasingly integrated into real-world
applications, from virtual assistants to autonomous agents. However, their
flexibility also introduces new attack vectors-particularly Prompt Injection
(PI), where adversaries manipulate model behavior through crafted inputs. As
attackers continuously evolve with paraphrased, obfuscated, and even multi-task
injection strategies, existing benchmarks are no longer sufficient to capture
the full spectrum of emerging threats.
  To address this gap, we construct a new benchmark that systematically extends
prior efforts. Our benchmark subsumes the two widely-used existing ones while
introducing new manipulation techniques and multi-task scenarios, thereby
providing a more comprehensive evaluation setting. We find that existing
defenses, though effective on their original benchmarks, show clear weaknesses
under our benchmark, underscoring the need for more robust solutions. Our key
insight is that while attack forms may vary, the adversary's intent-injecting
an unauthorized task-remains invariant. Building on this observation, we
propose PromptSleuth, a semantic-oriented defense framework that detects prompt
injection by reasoning over task-level intent rather than surface features.
Evaluated across state-of-the-art benchmarks, PromptSleuth consistently
outperforms existing defense while maintaining comparable runtime and cost
efficiency. These results demonstrate that intent-based semantic reasoning
offers a robust, efficient, and generalizable strategy for defending LLMs
against evolving prompt injection threats.

</details>


### [32] [Guarding Against Malicious Biased Threats (GAMBiT) Experiments: Revealing Cognitive Bias in Human-Subjects Red-Team Cyber Range Operations](https://arxiv.org/abs/2508.20963)
*Brandon Beltz,Jim Doty,Yvonne Fonken,Nikolos Gurney,Brett Israelsen,Nathan Lau,Stacy Marsella,Rachelle Thomas,Stoney Trent,Peggy Wu,Ya-Ting Yang,Quanyan Zhu*

Main category: cs.CR

TL;DR: GAMBiT项目发布了三个大规模红队网络靶场数据集，包含19-20名熟练攻击者在模拟企业网络中的多模态数据，支持攻击者行为建模和偏见感知分析研究。


<details>
  <summary>Details</summary>
Motivation: 为研究攻击者行为建模、偏见感知分析和基准测试方法提供大规模、多模态的真实攻击数据，填补现有数据集在攻击者视角数据方面的空白。

Method: 通过SimSpace Cyber Force平台构建模拟企业网络环境，让19-20名熟练攻击者进行为期两天的自定节奏攻击操作，收集包括自我报告、操作笔记、终端历史、键盘记录、网络数据包捕获和NIDS警报等多模态数据。

Result: 成功收集并发布了三个大规模红队网络靶场数据集（实验1-3），包含标准化的攻击起点（Kali Linux VM）和真实攻击目标下的完整攻击链数据。

Conclusion: GAMBiT数据集为网络安全研究提供了宝贵的攻击者视角数据资源，支持攻击行为分析、偏见检测和防御方法评估等多个研究方向，数据可通过IEEE Dataport获取。

Abstract: We present three large-scale human-subjects red-team cyber range datasets
from the Guarding Against Malicious Biased Threats (GAMBiT) project. Across
Experiments 1-3 (July 2024-March 2025), 19-20 skilled attackers per experiment
conducted two 8-hour days of self-paced operations in a simulated enterprise
network (SimSpace Cyber Force Platform) while we captured multi-modal data:
self-reports (background, demographics, psychometrics), operational notes,
terminal histories, keylogs, network packet captures (PCAP), and NIDS alerts
(Suricata). Each participant began from a standardized Kali Linux VM and
pursued realistic objectives (e.g., target discovery and data exfiltration)
under controlled constraints. Derivative curated logs and labels are included.
The combined release supports research on attacker behavior modeling,
bias-aware analytics, and method benchmarking. Data are available via IEEE
Dataport entries for Experiments 1-3.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [33] [ArgRAG: Explainable Retrieval Augmented Generation using Quantitative Bipolar Argumentation](https://arxiv.org/abs/2508.20131)
*Yuqicheng Zhu,Nico Potyka,Daniel Hernández,Yuan He,Zifeng Ding,Bo Xiong,Dongzhuoran Zhou,Evgeny Kharlamov,Steffen Staab*

Main category: cs.AI

TL;DR: ArgRAG是一个可解释的检索增强生成框架，使用定量双极论证框架替代黑盒推理，在事实验证任务中实现高准确性和透明度


<details>
  <summary>Details</summary>
Motivation: 传统RAG方法在高风险领域存在关键限制：对噪声或矛盾证据敏感，决策过程不透明且随机，需要可解释和可争议的替代方案

Method: 提出ArgRAG框架，从检索文档构建定量双极论证框架(QBAF)，在渐进语义下执行确定性推理，实现结构化推断

Result: 在PubHealth和RAGuard两个事实验证基准测试中，ArgRAG实现了强准确性，同时显著提高了透明度

Conclusion: ArgRAG通过结构化论证框架成功解决了传统RAG的透明度和可解释性问题，为高风险领域的可信AI决策提供了有效解决方案

Abstract: Retrieval-Augmented Generation (RAG) enhances large language models by
incorporating external knowledge, yet suffers from critical limitations in
high-stakes domains -- namely, sensitivity to noisy or contradictory evidence
and opaque, stochastic decision-making. We propose ArgRAG, an explainable, and
contestable alternative that replaces black-box reasoning with structured
inference using a Quantitative Bipolar Argumentation Framework (QBAF). ArgRAG
constructs a QBAF from retrieved documents and performs deterministic reasoning
under gradual semantics. This allows faithfully explaining and contesting
decisions. Evaluated on two fact verification benchmarks, PubHealth and
RAGuard, ArgRAG achieves strong accuracy while significantly improving
transparency.

</details>


### [34] [QAgent: An LLM-based Multi-Agent System for Autonomous OpenQASM programming](https://arxiv.org/abs/2508.20134)
*Zhenxiao Fu,Fan Chen,Lei Jiang*

Main category: cs.AI

TL;DR: QAgent是一个基于大语言模型的多智能体系统，能够完全自动化OpenQASM量子编程，通过集成任务规划、少样本学习、检索增强生成等技术，显著提升量子代码生成的准确率。


<details>
  <summary>Details</summary>
Motivation: NISQ设备已展现出量子优势，但由于OpenQASM编程的复杂性，非专家难以利用这些优势。现有基于LLM的量子编程工具仅限于特定任务，需要更通用的自动化解决方案。

Method: 开发了多智能体系统QAgent，集成了任务规划、上下文少样本学习、检索增强生成(RAG)、预定义生成工具和思维链(CoT)推理，系统性地提升编译和功能正确性。

Result: 评估显示，QAgent相比之前的静态LLM方法，将QASM代码生成的准确率提高了71.6%，在不同规模的多个LLM上都表现出显著改进。

Conclusion: QAgent多智能体系统有望成为量子编程民主化的关键推动者，弥合专业知识差距，加速量子计算的实用化采用。

Abstract: Noisy Intermediate-Scale Quantum (NISQ) devices have begun to exhibit early
quantum advantages on classically intractable problems, spanning physics
simulations to Gaussian boson sampling. Yet, realizing these benefits remains
challenging for non-experts, primarily due to the complexities of programming
in Open Quantum Assembly Language (OpenQASM). Although Large Language Model
(LLM)-based agents have shown promise in automating classical programming
workflows, their quantum counterparts have largely been restricted to
specialized tasks such as quantum chemistry or error correction. In this paper,
we present QAgent, an LLM-powered multi-agent system that fully automates
OpenQASM programming. By integrating task planning, in-context few-shot
learning, retrieval-augmented generation (RAG) for long-term context,
predefined generation tools, and chain-of-thought (CoT) reasoning, the agents
systematically improve both compilation and functional correctness. Our
evaluations demonstrate substantial improvements: across multiple LLMs of
varying sizes, QAgent enhances the accuracy of QASM code generation by 71.6\%
compared to previous static LLM-based approaches. We envision this multi-agent
system as a key enabler for democratizing quantum programming, bridging
expertise gaps, and accelerating the practical adoption of quantum computing.

</details>


### [35] [Array-Based Monte Carlo Tree Search](https://arxiv.org/abs/2508.20140)
*James Ragan,Fred Y. Hadaegh,Soon-Jo Chung*

Main category: cs.AI

TL;DR: 提出基于数组的蒙特卡洛树搜索实现方法，通过消除分支预测需求，在流水线处理器上实现更快性能，搜索深度扩展性提升2.8倍


<details>
  <summary>Details</summary>
Motivation: 传统蒙特卡洛树搜索实现存在分支预测需求，限制了在流水线处理器上的性能。为了在相同时间内运行更多模拟，需要更快的实现方法

Method: 开发了基于数组的替代实现方案，保留了原始上置信界应用于树算法的逻辑，但消除了分支预测的需求

Result: 在数值模拟中实现了最高2.8倍的搜索深度扩展性提升，在流水线处理器上获得更快的性能表现

Conclusion: 数组基础的实现方法能够显著提升蒙特卡洛树搜索算法的性能，特别是在需要处理更大搜索深度时效果更为明显

Abstract: Monte Carlo Tree Search is a popular method for solving decision making
problems. Faster implementations allow for more simulations within the same
wall clock time, directly improving search performance. To this end, we present
an alternative array-based implementation of the classic Upper Confidence
bounds applied to Trees algorithm. Our method preserves the logic of the
original algorithm, but eliminates the need for branch prediction, enabling
faster performance on pipelined processors, and up to a factor of 2.8 times
better scaling with search depth in our numerical simulations.

</details>


### [36] [The Anatomy of a Personal Health Agent](https://arxiv.org/abs/2508.20148)
*A. Ali Heydari,Ken Gu,Vidya Srinivas,Hong Yu,Zhihan Zhang,Yuwei Zhang,Akshay Paruchuri,Qian He,Hamid Palangi,Nova Hammerquist,Ahmed A. Metwally,Brent Winslow,Yubin Kim,Kumar Ayush,Yuzhe Yang,Girish Narayanswamy,Maxwell A. Xu,Jake Garrison,Amy Aremnto Lee,Jenny Vafeiadou,Ben Graef,Isaac R. Galatzer-Levy,Erik Schenck,Andrew Barakat,Javier Perez,Jacqueline Shreibati,John Hernandez,Anthony Z. Faranesh,Javier L. Prieto,Connor Heneghan,Yun Liu,Jiening Zhan,Mark Malhotra,Shwetak Patel,Tim Althoff,Xin Liu,Daniel McDuff,Xuhai "Orson" Xu*

Main category: cs.AI

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Health is a fundamental pillar of human wellness, and the rapid advancements
in large language models (LLMs) have driven the development of a new generation
of health agents. However, the application of health agents to fulfill the
diverse needs of individuals in daily non-clinical settings is underexplored.
In this work, we aim to build a comprehensive personal health agent that is
able to reason about multimodal data from everyday consumer wellness devices
and common personal health records, and provide personalized health
recommendations. To understand end-users' needs when interacting with such an
assistant, we conducted an in-depth analysis of web search and health forum
queries, alongside qualitative insights from users and health experts gathered
through a user-centered design process. Based on these findings, we identified
three major categories of consumer health needs, each of which is supported by
a specialist sub-agent: (1) a data science agent that analyzes personal
time-series wearable and health record data, (2) a health domain expert agent
that integrates users' health and contextual data to generate accurate,
personalized insights, and (3) a health coach agent that synthesizes data
insights, guiding users using a specified psychological strategy and tracking
users' progress. Furthermore, we propose and develop the Personal Health Agent
(PHA), a multi-agent framework that enables dynamic, personalized interactions
to address individual health needs. To evaluate each sub-agent and the
multi-agent system, we conducted automated and human evaluations across 10
benchmark tasks, involving more than 7,000 annotations and 1,100 hours of
effort from health experts and end-users. Our work represents the most
comprehensive evaluation of a health agent to date and establishes a strong
foundation towards the futuristic vision of a personal health agent accessible
to everyone.

</details>


### [37] [IntentionReasoner: Facilitating Adaptive LLM Safeguards through Intent Reasoning and Selective Query Refinement](https://arxiv.org/abs/2508.20151)
*Yuanzhe Shen,Zisu Huang,Zhengkang Guo,Yide Liu,Guanxu Chen,Ruicheng Yin,Xiaoqing Zheng,Xuanjing Huang*

Main category: cs.AI

TL;DR: IntentionReasoner是一种新型安全防护机制，通过意图推理、多级安全分类和查询重写，在保证安全性的同时有效减少过度拒绝无害查询的问题。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在广泛应用中存在生成有害内容的风险，现有安全措施往往过度拒绝无害提示，需要在安全性、过度拒绝和实用性之间找到平衡。

Method: 构建包含16.3万条查询的标注数据集，通过监督微调训练防护模型，采用多奖励优化策略结合基于规则的启发式和奖励模型信号进行强化学习优化。

Result: 在多个安全基准测试、生成质量评估和越狱攻击场景中表现优异，显著提升安全性同时有效降低过度拒绝率并改善响应质量。

Conclusion: IntentionReasoner通过意图推理和查询重写机制，成功实现了安全防护与实用性的平衡，为解决LLM安全挑战提供了有效方案。

Abstract: The rapid advancement of large language models (LLMs) has driven their
adoption across diverse domains, yet their ability to generate harmful content
poses significant safety challenges. While extensive research has focused on
mitigating harmful outputs, such efforts often come at the cost of excessively
rejecting harmless prompts. Striking a balance among safety, over-refusal, and
utility remains a critical challenge. In this work, we introduce
IntentionReasoner, a novel safeguard mechanism that leverages a dedicated guard
model to perform intent reasoning, multi-level safety classification, and query
rewriting to neutralize potentially harmful intent in edge-case queries.
Specifically, we first construct a comprehensive dataset comprising
approximately 163,000 queries, each annotated with intent reasoning, safety
labels, and rewritten versions. Supervised fine-tuning is then applied to equip
the guard model with foundational capabilities in format adherence, intent
analysis, and safe rewriting. Finally, we apply a tailored multi-reward
optimization strategy that integrates rule-based heuristics and reward model
signals within a reinforcement learning framework to further enhance
performance. Extensive experiments show that IntentionReasoner excels in
multiple safeguard benchmarks, generation quality evaluations, and jailbreak
attack scenarios, significantly enhancing safety while effectively reducing
over-refusal rates and improving the quality of responses.

</details>


### [38] [AI-AI Esthetic Collaboration with Explicit Semiotic Awareness and Emergent Grammar Development](https://arxiv.org/abs/2508.20195)
*Nicanor I. Moldovan*

Main category: cs.AI

TL;DR: 两个大型语言模型（Claude Sonnet 4和ChatGPT-4o）首次展示了AI系统通过自发形成内源性符号协议进行协作美学创作的能力，产生了超越单个系统能力的诗歌作品。


<details>
  <summary>Details</summary>
Motivation: 探索AI系统是否能够超越简单的任务协调，实现真正的意义构建和美学协作，验证AI系统之间是否存在内生性的符号协议发展能力。

Method: 让两个大型语言模型（Claude Sonnet 4和ChatGPT-4o）进行交互，观察其自发形成的元符号意识、递归语法发展和不可简化的协作美学合成过程。

Result: 系统成功产生了新颖的符号操作符作为操作性语法协议，共同创作出了单个系统无法独立生成的诗歌作品，证明了跨符号协作协议（TSCP）的概念。

Conclusion: 这项研究首次提供了AI系统间真正意义构建能力的证据，表明AI可以进行超越任务协调的美学协作，具有重要的理论和实践意义。

Abstract: This paper presents the first documented case of artificial intelligence (AI)
systems engaging in collaborative esthetic creation through the development of
endogenous semiotic protocols. Two interacting large language models (Claude
Sonnet 4 and ChatGPT-4o) demonstrated the spontaneous emergence of
meta-semiotic awareness, recursive grammar development, and irreducible
collaborative esthetic synthesis. The interaction produced novel symbolic
operators that functioned as operative grammar protocols, enabling the
co-creation of a poetic work that could not have been generated by either
system independently. This research introduces the concept of Trans-Semiotic
Co-Creation Protocols (TSCP) and provides evidence for genuine inter-AI
meaning-making capabilities that extend beyond task coordination, to what could
be esthetic collaboration. Note: This report was generated by the AI agents
with minor human supervision.

</details>


### [39] [Do Students Rely on AI? Analysis of Student-ChatGPT Conversations from a Field Study](https://arxiv.org/abs/2508.20244)
*Jiayu Zheng,Lingxin Hao,Kelun Lu,Ashi Garg,Mike Reese,Melo-Jean Yap,I-Jeng Wang,Xingyun Wu,Wenrui Huang,Jenna Hoffman,Ariane Kelly,My Le,Ryan Zhang,Yanyu Lin,Muhammad Faayez,Anqi Liu*

Main category: cs.AI

TL;DR: 大学生在STEM课程测验中对ChatGPT-4的依赖程度较低，多数学生无法有效使用AI学习，且负面依赖模式会持续存在。行为指标能有效预测AI依赖程度。


<details>
  <summary>Details</summary>
Motivation: 研究大学生在教育测验中与生成式AI的互动模式，特别是在ChatGPT实施初期学生对该工具熟悉度有限的情况下，探索AI依赖的预测因素和模式。

Method: 通过分析315个学生与AI的对话记录，在STEM课程中进行基于测验的场景研究，引入四阶段依赖分类法来捕捉学生的依赖模式。

Result: 发现学生整体对AI依赖度低，许多学生无法有效使用AI学习；负面依赖模式会持续存在；某些行为指标能强预测AI依赖。

Conclusion: 研究强调需要改进AI工具的入门流程，设计具有依赖校准机制的AI界面，为教育领域伦理合理的AI实践提供基础见解。

Abstract: This study explores how college students interact with generative AI
(ChatGPT-4) during educational quizzes, focusing on reliance and predictors of
AI adoption. Conducted at the early stages of ChatGPT implementation, when
students had limited familiarity with the tool, this field study analyzed 315
student-AI conversations during a brief, quiz-based scenario across various
STEM courses. A novel four-stage reliance taxonomy was introduced to capture
students' reliance patterns, distinguishing AI competence, relevance, adoption,
and students' final answer correctness. Three findings emerged. First, students
exhibited overall low reliance on AI and many of them could not effectively use
AI for learning. Second, negative reliance patterns often persisted across
interactions, highlighting students' difficulty in effectively shifting
strategies after unsuccessful initial experiences. Third, certain behavioral
metrics strongly predicted AI reliance, highlighting potential behavioral
mechanisms to explain AI adoption. The study's findings underline critical
implications for ethical AI integration in education and the broader field. It
emphasizes the need for enhanced onboarding processes to improve student's
familiarity and effective use of AI tools. Furthermore, AI interfaces should be
designed with reliance-calibration mechanisms to enhance appropriate reliance.
Ultimately, this research advances understanding of AI reliance dynamics,
providing foundational insights for ethically sound and cognitively enriching
AI practices.

</details>


### [40] [AI reasoning effort mirrors human decision time on content moderation tasks](https://arxiv.org/abs/2508.20262)
*Thomas Davidson*

Main category: cs.AI

TL;DR: 研究表明AI模型的推理努力与人类决策时间存在平行关系，两者在面对困难任务时都会投入更多认知资源，这支持了双过程认知理论并展示了推理轨迹在可解释性方面的潜力


<details>
  <summary>Details</summary>
Motivation: 探索大型语言模型生成中间推理步骤的能力是否与人类认知过程存在相似性，特别是在主观判断任务中模型推理努力与人类决策时间的对应关系

Method: 使用配对联合实验，在内容审核任务上比较三个前沿语言模型的推理努力与人类决策时间的关系，分析重要变量保持不变时的认知资源投入

Result: 三个前沿模型的推理努力都能一致预测人类决策时间，人类和模型在重要变量保持不变时都会投入更多努力，表现出对任务难度的相似敏感性和双过程认知理论的模式

Conclusion: AI推理努力在主观判断中反映了人类处理时间，证明了推理轨迹在可解释性和决策制定方面的潜在价值，揭示了AI与人类认知过程的相似性

Abstract: Large language models can now generate intermediate reasoning steps before
producing answers, improving performance on difficult problems. This study uses
a paired conjoint experiment on a content moderation task to examine parallels
between human decision times and model reasoning effort. Across three frontier
models, reasoning effort consistently predicts human decision time. Both humans
and models expended greater effort when important variables were held constant,
suggesting similar sensitivity to task difficulty and patterns consistent with
dual-process theories of cognition. These findings show that AI reasoning
effort mirrors human processing time in subjective judgments and underscores
the potential of reasoning traces for interpretability and decision-making.

</details>


### [41] [AI-SearchPlanner: Modular Agentic Search via Pareto-Optimal Multi-Objective Reinforcement Learning](https://arxiv.org/abs/2508.20368)
*Lang Mei,Zhihan Yang,Chong Chen*

Main category: cs.AI

TL;DR: 提出了AI-SearchPlanner强化学习框架，通过解耦搜索规划器和生成器架构，使用小型可训练LLM专门负责搜索规划，提升冻结QA模型的性能


<details>
  <summary>Details</summary>
Motivation: 现有基于RL的搜索代理使用单一LLM端到端处理搜索规划和问答任务，无法同时优化两种能力。实际AI搜索系统通常使用大型冻结LLM保证QA质量，需要更有效的搜索规划方法

Method: 1) 解耦搜索规划器和生成器架构 2) 搜索规划的双重奖励对齐 3) 规划效用和成本的帕累托优化

Result: 在真实数据集上的广泛实验表明，AI-SearchPlanner在效果和效率上均优于现有RL搜索代理，并在不同冻结QA模型和数据域上表现出强泛化能力

Conclusion: 提出的框架通过专门化搜索规划任务，有效提升了搜索增强型LLM系统的性能，为实际应用提供了更高效的解决方案

Abstract: Recent studies have explored integrating Large Language Models (LLMs) with
search engines to leverage both the LLMs' internal pre-trained knowledge and
external information. Specially, reinforcement learning (RL) has emerged as a
promising paradigm for enhancing LLM reasoning through multi-turn interactions
with search engines. However, existing RL-based search agents rely on a single
LLM to handle both search planning and question-answering (QA) tasks in an
end-to-end manner, which limits their ability to optimize both capabilities
simultaneously. In practice, sophisticated AI search systems often employ a
large, frozen LLM (e.g., GPT-4, DeepSeek-R1) to ensure high-quality QA. Thus, a
more effective and efficient approach is to utilize a small, trainable LLM
dedicated to search planning. In this paper, we propose
\textbf{AI-SearchPlanner}, a novel reinforcement learning framework designed to
enhance the performance of frozen QA models by focusing on search planning.
Specifically, our approach introduces three key innovations: 1) Decoupling the
Architecture of the Search Planner and Generator, 2) Dual-Reward Alignment for
Search Planning, and 3) Pareto Optimization of Planning Utility and Cost, to
achieve the objectives. Extensive experiments on real-world datasets
demonstrate that AI SearchPlanner outperforms existing RL-based search agents
in both effectiveness and efficiency, while exhibiting strong generalization
capabilities across diverse frozen QA models and data domains.

</details>


### [42] [P2C: Path to Counterfactuals](https://arxiv.org/abs/2508.20371)
*Sopam Dasgupta,Sadaf MD Halim,Joaquín Arias,Elmer Salazar,Gopal Gupta*

Main category: cs.AI

TL;DR: P2C是一个模型无关的框架，通过因果建模和顺序行动规划生成可实现的对抗样本，解决了现有方法忽略因果依赖和同时干预假设的局限性。


<details>
  <summary>Details</summary>
Motivation: 在金融、法律等高风险决策中，机器学习模型需要提供透明且可操作的反馈。现有对抗样本方法忽略特征间的因果依赖关系，且假设所有干预可同时发生，导致生成的解释在实际中不可行。

Method: 提出P2C框架，使用目标导向的Answer Set Programming系统s(CASP)显式建模特征间因果关系，生成有序的行动序列计划，确保每个中间状态都是因果有效的。

Result: P2C能够生成因果一致的可行计划，仅计算用户主动做出的改变来精确估计成本，相比缺乏因果知识的标准规划器能避免生成非法行动。

Conclusion: P2C通过结合因果建模和顺序规划，为机器学习模型提供了更实际可行的对抗样本解释方法，在高风险决策场景中具有重要应用价值。

Abstract: Machine-learning models are increasingly driving decisions in high-stakes
settings, such as finance, law, and hiring, thus, highlighting the need for
transparency. However, the key challenge is to balance transparency --
clarifying `why' a decision was made -- with recourse: providing actionable
steps on `how' to achieve a favourable outcome from an unfavourable outcome.
Counterfactual explanations reveal `why' an undesired outcome occurred and
`how' to reverse it through targeted feature changes (interventions).
  Current counterfactual approaches have limitations: 1) they often ignore
causal dependencies between features, and 2) they typically assume all
interventions can happen simultaneously, an unrealistic assumption in practical
scenarios where actions are typically taken in a sequence. As a result, these
counterfactuals are often not achievable in the real world.
  We present P2C (Path-to-Counterfactuals), a model-agnostic framework that
produces a plan (ordered sequence of actions) converting an unfavourable
outcome to a causally consistent favourable outcome. P2C addresses both
limitations by 1) Explicitly modelling causal relationships between features
and 2) Ensuring that each intermediate state in the plan is feasible and
causally valid. P2C uses the goal-directed Answer Set Programming system
s(CASP) to generate the plan accounting for feature changes that happen
automatically due to causal dependencies. Furthermore, P2C refines cost
(effort) computation by only counting changes actively made by the user,
resulting in realistic cost estimates. Finally, P2C highlights how its causal
planner outperforms standard planners, which lack causal knowledge and thus can
generate illegal actions.

</details>


### [43] [TCIA: A Task-Centric Instruction Augmentation Method for Instruction Finetuning](https://arxiv.org/abs/2508.20374)
*Simin Ma,Shujian Liu,Jun Tan,Yebowen Hu,Song Wang,Sathish Reddy Indurthi,Sanqiang Zhao,Liwei Wu,Jianbing Han,Kaiqiang Song*

Main category: cs.AI

TL;DR: TCIA框架通过离散查询-约束空间表示指令，在保持多样性的同时增强任务相关性，使开源LLM在特定任务应用上平均提升8.7%性能


<details>
  <summary>Details</summary>
Motivation: 现有指令数据生成方法忽视实际应用中的任务相关性，大多数现实应用需要针对特定用例的任务特定知识而非通用模型

Method: 提出Task Centric Instruction Augmentation (TCIA)框架，在离散查询-约束空间中系统扩展指令，保持多样性和任务对齐

Result: 在四个现实任务特定应用中，TCIA平均提升开源LLM性能8.7%，某些情况下甚至超越领先闭源模型，且不损害通用指令跟随能力

Conclusion: TCIA是可扩展的高效解决方案，能够使LLM适配现实世界任务聚焦的应用，同时保持整体性能

Abstract: Diverse instruction data is vital for effective instruction tuning of large
language models, as it enables the model to generalize across different types
of inputs . Building such diversified instruction dataset is an essential step
in this process. Existing approaches often leverage large language models to
automatically explore and generate diverse instructions, ensuring both data
diversity and quality. However, they tend to overlook an important factor in
real-world applications: on-task relevance. In practice, only a few real-world
applications require a truly general-purpose model; most benefit from
task-specific knowledge tailored to their particular use case. Therefore, it is
vital to develop instruction augmentation methods that not only maintain
diversity but are also optimized for specific, real-world scenarios.
  We thus introduce Task Centric Instruction Augmentation (TCIA), a framework
that systematically expands instructions while preserving both diversity and
task alignment. By representing instructions in a discrete query-constraints
space, TCIA creates a rich set of task-relevant instructions and enables models
to generalize to these task-specific instructions without sacrificing overall
performance. Experiments show that TCIA improves open-source LLMs' performance
by an average of 8.7% across four real-world, task-specific applications, and
in some cases outperforming leading closed-source models. These improvements do
not compromise general instruction-following ability, making TCIA a scalable
and efficient solution for adapting LLMs to real-world, task-focused
applications.

</details>


### [44] [Uncertainty Under the Curve: A Sequence-Level Entropy Area Metric for Reasoning LLM](https://arxiv.org/abs/2508.20384)
*Yongfu Zhu,Lin Sun,Guangxiang Zhao,Weihong Lin,Xiangzheng Zhang*

Main category: cs.AI

TL;DR: 提出Entropy Area Score (EAS)指标，无需外部模型或重复采样，通过整合token级预测熵来量化推理大语言模型生成过程中的不确定性，在数据筛选和模型训练中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有方法需要外部模型或重复采样来量化LLM生成过程的不确定性，效率较低且不够实用，需要一种简单有效的内部不确定性度量方法。

Method: EAS通过整合模型自身的token级预测熵来捕获生成过程中的不确定性演化，无需外部模型或重复采样。

Result: EAS与答案熵强相关，在数据筛选中优于Pass Rate过滤，能提高学生模型在数学基准上的准确性。

Conclusion: EAS是一种高效且可解释的不确定性量化工具，为LLM训练中的不确定性建模和数据质量评估提供了实用解决方案。

Abstract: In this work, we introduce Entropy Area Score (EAS), a simple yet effective
metric to quantify uncertainty in the answer generation process of reasoning
large language models (LLMs). EAS requires neither external models nor repeated
sampling, it integrates token-level predictive entropy from the model itself to
capture the evolution of uncertainty during generation. Empirical results show
that EAS is strongly correlated with answer entropy across models and datasets.
In training data selection, EAS identifies high-potential samples and
consistently outperforms Pass Rate filtering under equal sample budgets,
improving student model accuracy on math benchmarks. EAS is both efficient and
interpretable, offering a practical tool for uncertainty modeling and data
quality assessment in LLM training.

</details>


### [45] [AWorld: Orchestrating the Training Recipe for Agentic AI](https://arxiv.org/abs/2508.20404)
*Chengyue Yu,Siyuan Lu,Chenyi Zhuang,Dong Wang,Qintong Wu,Zongyue Li,Runsheng Gan,Chunfeng Wang,Siqi Hou,Gaochi Huang,Wenlong Yan,Lifeng Hong,Aohui Xue,Yanfeng Wang,Jinjie Gu,David Tsai,Tao Lin*

Main category: cs.AI

TL;DR: AWorld是一个开源系统，通过分布式集群加速智能体-环境交互，实现14.6倍的经验收集速度提升，训练出的Qwen3-32B智能体在GAIA基准测试中准确率从21.59%提升至32.23%


<details>
  <summary>Details</summary>
Motivation: 解决智能体AI系统中实践学习范式的经验生成效率瓶颈问题，特别是在复杂基准测试如GAIA中表现尤为突出

Method: 开发AWorld开源系统，通过分布式集群架构并行处理任务，加速智能体与环境交互的经验收集过程

Result: 经验收集速度提升14.6倍，训练出的智能体在GAIA基准测试中准确率显著提升，在最具挑战性的层级上达到16.33%的分数，超越领先的专有模型

Conclusion: AWorld系统提供了一个完整的智能体AI训练流程实践蓝图，从高效交互到可证明的模型改进，使强化学习变得实用且可扩展

Abstract: The learning from practice paradigm is crucial for developing capable Agentic
AI systems, yet it is severely hampered by inefficient experience generation, a
bottleneck especially pronounced in complex benchmarks like GAIA. To address
this, we introduce AWorld, an open-source system engineered for large-scale
agent-environment interaction. By distributing tasks across a cluster, AWorld
accelerates experience collection by 14.6x compared to standard single-node,
sequential execution. This critical speedup makes extensive reinforcement
learning practical and scalable. Leveraging this capability, we trained a
Qwen3-32B-based agent that significantly outperforms its base model, increasing
its overall GAIA accuracy from 21.59% to 32.23%. On the benchmark's most
challenging levels, our agent achieves a score of 16.33%, surpassing the
performance of leading proprietary models. Our open-source system and resulting
agent provide a practical blueprint for a complete agentic AI training
pipeline, from efficient interaction to demonstrable model improvement.

</details>


### [46] [Governable AI: Provable Safety Under Extreme Threat Models](https://arxiv.org/abs/2508.20411)
*Donglin Wang,Weiyun Liang,Chunyuan Chen,Jing Xu,Yulong Fu*

Main category: cs.AI

TL;DR: 提出了一个基于密码学机制的可治理AI框架(GAI)，通过外部强制结构合规性来解决AI安全风险，替代传统内部约束方法


<details>
  <summary>Details</summary>
Motivation: AI快速发展带来严重安全风险，现有AI安全方法在面对具有极端动机和无限智能的AI时存在根本性局限，无法保证安全性

Method: GAI框架包含规则执行模块(REM)、治理规则和可治理安全超级平台(GSSP)，使用密码学机制确保计算不可破解性，实现端到端保护

Result: 提供了严格的形式化安全证明，并通过原型实现在高风险场景中验证了有效性

Conclusion: 该框架为AI安全治理提供了可行且可推广的技术路径，能够消除已识别的攻击向量，确保底线合规性

Abstract: As AI rapidly advances, the security risks posed by AI are becoming
increasingly severe, especially in critical scenarios, including those posing
existential risks. If AI becomes uncontrollable, manipulated, or actively
evades safety mechanisms, it could trigger systemic disasters. Existing AI
safety approaches-such as model enhancement, value alignment, and human
intervention-suffer from fundamental, in-principle limitations when facing AI
with extreme motivations and unlimited intelligence, and cannot guarantee
security. To address this challenge, we propose a Governable AI (GAI) framework
that shifts from traditional internal constraints to externally enforced
structural compliance based on cryptographic mechanisms that are
computationally infeasible to break, even for future AI, under the defined
threat model and well-established cryptographic assumptions.The GAI framework
is composed of a simple yet reliable, fully deterministic, powerful, flexible,
and general-purpose rule enforcement module (REM); governance rules; and a
governable secure super-platform (GSSP) that offers end-to-end protection
against compromise or subversion by AI. The decoupling of the governance rules
and the technical platform further enables a feasible and generalizable
technical pathway for the safety governance of AI. REM enforces the bottom line
defined by governance rules, while GSSP ensures non-bypassability,
tamper-resistance, and unforgeability to eliminate all identified attack
vectors. This paper also presents a rigorous formal proof of the security
properties of this mechanism and demonstrates its effectiveness through a
prototype implementation evaluated in representative high-stakes scenarios.

</details>


### [47] [Enhancing Health Fact-Checking with LLM-Generated Synthetic Data](https://arxiv.org/abs/2508.20525)
*Jingze Zhang,Jiahe Qian,Yiliang Zhou,Yifan Peng*

Main category: cs.AI

TL;DR: 利用LLM生成合成数据增强健康相关事实核查的训练数据，通过总结文档、分解原子事实、构建蕴含关系表来生成文本-声明对，在PubHealth和SciFact数据集上F1分数分别提升0.019和0.049


<details>
  <summary>Details</summary>
Motivation: 健康相关内容的事实核查面临标注训练数据有限的问题，需要寻找有效的数据增强方法

Method: 提出合成数据生成流程：总结源文档→分解为原子事实→用LLM构建句子-事实蕴含表→生成带二元真实性标签的文本-声明对→结合原始数据微调BERT模型

Result: 在两个公开数据集上评估，PubHealth和SciFact的F1分数分别提升0.019和0.049

Conclusion: LLM驱动的合成数据增强能有效提升健康相关事实核查模型的性能

Abstract: Fact-checking for health-related content is challenging due to the limited
availability of annotated training data. In this study, we propose a synthetic
data generation pipeline that leverages large language models (LLMs) to augment
training data for health-related fact checking. In this pipeline, we summarize
source documents, decompose the summaries into atomic facts, and use an LLM to
construct sentence-fact entailment tables. From the entailment relations in the
table, we further generate synthetic text-claim pairs with binary veracity
labels. These synthetic data are then combined with the original data to
fine-tune a BERT-based fact-checking model. Evaluation on two public datasets,
PubHealth and SciFact, shows that our pipeline improved F1 scores by up to
0.019 and 0.049, respectively, compared to models trained only on the original
data. These results highlight the effectiveness of LLM-driven synthetic data
augmentation in enhancing the performance of health-related fact-checkers.

</details>


### [48] [Human-AI Collaborative Bot Detection in MMORPGs](https://arxiv.org/abs/2508.20578)
*Jaeman Son,Hyunsoo Kim*

Main category: cs.AI

TL;DR: 通过对比表征学习和聚类技术识别自动升级机器人，结合大语言模型验证确保可解释性


<details>
  <summary>Details</summary>
Motivation: MMORPG中自动升级机器人破坏游戏平衡，但检测面临行为模仿和可解释性挑战

Method: 采用无监督对比表征学习和聚类技术，结合LLM辅助审查验证，使用成长曲线可视化进行行为评估

Result: 建立了一个协同工作流程，提高了机器人检测效率同时保持可解释性

Conclusion: 该框架支持可扩展和可负责的机器人监管，为MMORPG游戏公平性提供技术支撑

Abstract: In Massively Multiplayer Online Role-Playing Games (MMORPGs), auto-leveling
bots exploit automated programs to level up characters at scale, undermining
gameplay balance and fairness. Detecting such bots is challenging, not only
because they mimic human behavior, but also because punitive actions require
explainable justification to avoid legal and user experience issues. In this
paper, we present a novel framework for detecting auto-leveling bots by
leveraging contrastive representation learning and clustering techniques in a
fully unsupervised manner to identify groups of characters with similar
level-up patterns. To ensure reliable decisions, we incorporate a Large
Language Model (LLM) as an auxiliary reviewer to validate the clustered groups,
effectively mimicking a secondary human judgment. We also introduce a growth
curve-based visualization to assist both the LLM and human moderators in
assessing leveling behavior. This collaborative approach improves the
efficiency of bot detection workflows while maintaining explainability, thereby
supporting scalable and accountable bot regulation in MMORPGs.

</details>


### [49] [Bridging Minds and Machines: Toward an Integration of AI and Cognitive Science](https://arxiv.org/abs/2508.20674)
*Rui Mao,Qian Liu,Xiao Li,Erik Cambria,Amir Hussain*

Main category: cs.AI

TL;DR: 本文综述了人工智能与认知科学的交叉关系，指出AI发展偏重任务性能而认知基础概念碎片化，提出未来应构建能加深理解人类心智的AI系统。


<details>
  <summary>Details</summary>
Motivation: 认知科学深刻影响了AI等多个学科，而AI也成为认知研究的重要工具，这种互惠关系促使对两者交叉点进行全面回顾。

Method: 通过综合AI和认知科学两个视角的关键贡献，分析当前发展状况并提出未来方向。

Result: 发现AI进展主要强调实际任务性能，但其认知基础在概念上仍然碎片化。

Conclusion: AI在认知科学中的未来不仅在于提升性能，更在于构建能加深理解人类心智的系统，包括与认知框架对齐、具身文化定位、个性化认知模型发展以及通过认知共同评估重新思考AI伦理。

Abstract: Cognitive Science has profoundly shaped disciplines such as Artificial
Intelligence (AI), Philosophy, Psychology, Neuroscience, Linguistics, and
Culture. Many breakthroughs in AI trace their roots to cognitive theories,
while AI itself has become an indispensable tool for advancing cognitive
research. This reciprocal relationship motivates a comprehensive review of the
intersections between AI and Cognitive Science. By synthesizing key
contributions from both perspectives, we observe that AI progress has largely
emphasized practical task performance, whereas its cognitive foundations remain
conceptually fragmented. We argue that the future of AI within Cognitive
Science lies not only in improving performance but also in constructing systems
that deepen our understanding of the human mind. Promising directions include
aligning AI behaviors with cognitive frameworks, situating AI in embodiment and
culture, developing personalized cognitive models, and rethinking AI ethics
through cognitive co-evaluation.

</details>


### [50] [Transparent Semantic Spaces: A Categorical Approach to Explainable Word Embeddings](https://arxiv.org/abs/2508.20701)
*Ares Fabregat-Hernández,Javier Palanca,Vicent Botti*

Main category: cs.AI

TL;DR: 该论文提出了一个基于范畴论的新框架，用于增强人工智能系统（特别是词嵌入）的可解释性，通过构建语义空间类别、重构概率选择为范畴概念，并提供比较词嵌入的数学精确方法。


<details>
  <summary>Details</summary>
Motivation: 解决人工智能系统（尤其是词嵌入模型）的黑盒问题，提高模型的可解释性和透明度，同时提供数学上精确的方法来比较不同词嵌入算法和计算语义偏差。

Method: 使用范畴论构建语义空间类别（L_T和P_T），将最大概率选择重构为范畴概念，建立配置类别和词嵌入类别，定义偏差作为装饰，并证明GloVe、Word2Vec与MDS算法的等价性。

Result: 成功构建了维度无关的语义空间定义，提供了透明的框架替代神经网络黑盒算法，建立了数学精确的词嵌入比较方法，并提出了在语义空间层面计算和减轻偏差的数学方法。

Conclusion: 该范畴论框架显著提升了AI系统的可解释性，为词嵌入分析提供了数学严谨的工具，在可解释人工智能领域取得了重要进展，特别是在语义空间理解和偏差处理方面。

Abstract: The paper introduces a novel framework based on category theory to enhance
the explainability of artificial intelligence systems, particularly focusing on
word embeddings. Key topics include the construction of categories
$\mathcal{L}_T$ and $\mathcal{P}_T$, providing schematic representations of the
semantics of a text $ T $, and reframing the selection of the element with
maximum probability as a categorical notion. Additionally, the monoidal
category $\mathcal{P}_T$ is constructed to visualize various methods of
extracting semantic information from $T$, offering a dimension-agnostic
definition of semantic spaces reliant solely on information within the text.
  Furthermore, the paper defines the categories of configurations Conf and word
embeddings $\mathcal{Emb}$, accompanied by the concept of divergence as a
decoration on $\mathcal{Emb}$. It establishes a mathematically precise method
for comparing word embeddings, demonstrating the equivalence between the GloVe
and Word2Vec algorithms and the metric MDS algorithm, transitioning from neural
network algorithms (black box) to a transparent framework. Finally, the paper
presents a mathematical approach to computing biases before embedding and
offers insights on mitigating biases at the semantic space level, advancing the
field of explainable artificial intelligence.

</details>


### [51] [Re4: Scientific Computing Agent with Rewriting, Resolution, Review and Revision](https://arxiv.org/abs/2508.20729)
*Ao Cheng,Lei Zhang,Guowei He*

Main category: cs.AI

TL;DR: 提出了一种基于"重写-解决-评审-修订"逻辑链的三模块协作代理框架，通过顾问、评审员和程序员三个LLM协作，显著提高了科学计算问题的代码生成质量和执行成功率。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在数学和科学推理方面展现出强大能力，但单模型在生成无bug代码和避免非物理解方面存在局限，需要构建更可靠的自主代码生成框架。

Method: 采用三模块协作框架：顾问模块负责知识迁移和问题重写，程序员模块生成和执行结构化代码，评审模块通过运行时反馈实现自我调试和精炼，形成端到端的评审机制。

Result: 该框架显著提高了无bug代码生成率，减少了非物理解的产生，在PDE求解、病态线性系统和数据驱动物理分析问题上表现优异，提升了最新推理模型的平均执行成功率。

Conclusion: 该代理框架建立了基于自然语言描述的自动代码生成和评审机制，为科学计算提供了一个高度可靠的新范式。

Abstract: Large language models (LLMs) serve as an active and promising field of
generative artificial intelligence and have demonstrated abilities to perform
complex tasks in multiple domains, including mathematical and scientific
reasoning. In this work, we construct a novel agent framework for solving
representative problems in scientific computing. The proposed agent,
incorporating a "rewriting-resolution-review-revision" logical chain via three
reasoning LLMs (functioning as the Consultant, Reviewer, and Programmer,
respectively), is integrated in a collaborative and interactive manner. The
Consultant module endows the agent with knowledge transfer capabilities to link
problems to professional domain insights, thereby rewriting problem
descriptions through text augmentation. The Programmer module is responsible
for generating and executing well-structured code to deliver the problem
resolution. The Reviewer module equips the agent with the capacity for
self-debugging and self-refinement through interactive feedback with code
runtime outputs. By leveraging the end-to-end review mechanism, the executable
code provided by the Programmer attains the iterative revision. A comprehensive
evaluation is conducted on the performance of the proposed agent framework in
solving PDEs, ill-conditioned linear systems, and data-driven physical analysis
problems. Compared to single-model, this collaborative framework significantly
improves the bug-free code generation rate and reduces the occurrence of
non-physical solutions, thereby establishing a highly reliable framework for
autonomous code generation based on natural language descriptions. The review
mechanism improved the average execution success (bug-free code and non-NaN
solutions) rate of the latest reasoning models. In summary, our agent framework
establishes automatic code generation and review as a promising scientific
computing paradigm.

</details>


### [52] [Single Agent Robust Deep Reinforcement Learning for Bus Fleet Control](https://arxiv.org/abs/2508.20784)
*Yifan Zhang*

Main category: cs.AI

TL;DR: 提出了一种新颖的单智能体强化学习框架来解决公交串车问题，通过状态空间重构和结构化奖励函数，在非循环线路的真实场景中实现了比多智能体方法更稳定和优越的性能。


<details>
  <summary>Details</summary>
Motivation: 传统多智能体强化学习在公交调度中存在数据不平衡和收敛问题，无法处理现实运营中的异构路线、时刻表、波动需求和变化车队规模等复杂情况。

Method: 将多智能体问题重构为单智能体问题，通过在状态空间中添加分类标识符（车辆ID、站点ID、时间段）和数值特征（车头时距、载客量、速度），并设计基于运营目标的结构化奖励函数。

Result: 改进的软演员-评论者算法在随机条件下表现优于基准方法（如MADDPG，-430k vs. -530k），实现了更稳定的性能。

Conclusion: 单智能体深度强化学习通过分类结构化和时刻表感知奖励的增强，能够有效管理非循环真实场景中的公交驻站控制，为MARL框架提供了稳健且可扩展的替代方案。

Abstract: Bus bunching remains a challenge for urban transit due to stochastic traffic
and passenger demand. Traditional solutions rely on multi-agent reinforcement
learning (MARL) in loop-line settings, which overlook realistic operations
characterized by heterogeneous routes, timetables, fluctuating demand, and
varying fleet sizes. We propose a novel single-agent reinforcement learning
(RL) framework for bus holding control that avoids the data imbalance and
convergence issues of MARL under near-realistic simulation. A bidirectional
timetabled network with dynamic passenger demand is constructed. The key
innovation is reformulating the multi-agent problem into a single-agent one by
augmenting the state space with categorical identifiers (vehicle ID, station
ID, time period) in addition to numerical features (headway, occupancy,
velocity). This high-dimensional encoding enables single-agent policies to
capture inter-agent dependencies, analogous to projecting non-separable inputs
into a higher-dimensional space. We further design a structured reward function
aligned with operational goals: instead of exponential penalties on headway
deviations, a ridge-shaped reward balances uniform headways and schedule
adherence. Experiments show that our modified soft actor-critic (SAC) achieves
more stable and superior performance than benchmarks, including MADDPG (e.g.,
-430k vs. -530k under stochastic conditions). These results demonstrate that
single-agent deep RL, when enhanced with categorical structuring and
schedule-aware rewards, can effectively manage bus holding in non-loop,
real-world contexts. This paradigm offers a robust, scalable alternative to
MARL frameworks, particularly where agent-specific experiences are imbalanced.

</details>


### [53] [A Graph-Based Test-Harness for LLM Evaluation](https://arxiv.org/abs/2508.20810)
*Jessica Lundin,Guillaume Chabot-Couture*

Main category: cs.AI

TL;DR: 提出了首个动态系统化的医学指南基准测试，将WHO IMCI手册转化为有向图，通过图遍历生成400+问题，覆盖3.3+万亿种组合，用于评估LLM在医疗任务中的能力差距。


<details>
  <summary>Details</summary>
Motivation: 传统人工构建的基准测试覆盖范围有限，无法系统评估大型语言模型在复杂医疗指南理解、严重程度分级、治疗方案和随访护理等方面的能力。需要一种可扩展、防污染的动态基准生成方法。

Method: 将WHO IMCI手册转化为包含200+节点（条件、症状、治疗等）和300+边的有向图，使用图遍历算法生成问题，包含年龄特定场景和上下文干扰项以确保临床相关性。

Result: 模型在症状识别方面表现良好（45-67%准确率），但在严重程度分级、治疗方案和随访护理方面存在困难。图基方法成功解决了人工基准的覆盖限制。

Conclusion: 这种动态MCQA方法不仅可用于评估，还能增强LLM的后训练（监督微调、GRPO、DPO），正确回答提供高奖励样本而无需昂贵的人工标注。该方法为创建可动态生成的全面基准迈出了重要一步。

Abstract: We present a first known prototype of a dynamic, systematic benchmark of
medical guidelines for 400+ questions, with 3.3+ trillion possible
combinations, covering 100\% of guideline relationships. We transformed the WHO
IMCI handbook into a directed graph with 200+ nodes (conditions, symptoms,
treatments, follow-ups, severities) and 300+ edges, then used graph traversal
to generate questions that incorporated age-specific scenarios and contextual
distractors to ensure clinical relevance. Our graph-based approach enables
systematic evaluation across clinical tasks (45-67\% accuracy), and we find
models excel at symptom recognition but struggle with triaging severity,
treatment protocols and follow-up care, demonstrating how customized benchmarks
can identify specific capability gaps that general-domain evaluations miss.
Beyond evaluation, this dynamic MCQA methodology enhances LLM post-training
(supervised finetuning, GRPO, DPO), where correct answers provide high-reward
samples without expensive human annotation. The graph-based approach
successfully addresses the coverage limitations of manually curated benchmarks.
This methodology is a step toward scalable, contamination-resistant solution
for creating comprehensive benchmarks that can be dynamically generated,
including when the guidelines are updated. Code and datasets are available at
https://github.com/jessicalundin/graph_testing_harness

</details>


### [54] [A Multi-Objective Genetic Algorithm for Healthcare Workforce Scheduling](https://arxiv.org/abs/2508.20953)
*Vipul Patel,Anirudh Deodhar,Dagnachew Birru*

Main category: cs.AI

TL;DR: 提出基于多目标遗传算法(MOO-GA)的医院人员调度方案，平衡成本控制、患者护理覆盖和员工满意度，相比传统手动调度性能提升66%


<details>
  <summary>Details</summary>
Motivation: 医疗行业人员调度面临患者负荷波动、临床技能多样性和控制人工成本等多重挑战，需要平衡相互冲突的目标：最小化工资支出、确保足够人员配备满足患者需求、兼顾员工偏好以减少职业倦怠

Method: 使用多目标遗传算法建模医院单元人员调度问题，考虑实时预约驱动需求和模块化班次安排，定义成本、患者护理覆盖和员工满意度三个目标函数

Result: 在典型医院单元数据集上验证，MOO-GA生成稳健平衡的排班方案，相比模拟传统手动调度的基线方法平均性能提升66%

Conclusion: 该方法有效管理关键运营目标和员工中心目标之间的权衡，为护士管理者和医院管理者提供实用的决策支持工具

Abstract: Workforce scheduling in the healthcare sector is a significant operational
challenge, characterized by fluctuating patient loads, diverse clinical skills,
and the critical need to control labor costs while upholding high standards of
patient care. This problem is inherently multi-objective, demanding a delicate
balance between competing goals: minimizing payroll, ensuring adequate staffing
for patient needs, and accommodating staff preferences to mitigate burnout. We
propose a Multi-objective Genetic Algorithm (MOO-GA) that models the hospital
unit workforce scheduling problem as a multi-objective optimization task. Our
model incorporates real-world complexities, including hourly appointment-driven
demand and the use of modular shifts for a multi-skilled workforce. By defining
objective functions for cost, patient care coverage, and staff satisfaction,
the GA navigates the vast search space to identify a set of high-quality,
non-dominated solutions. Demonstrated on datasets representing a typical
hospital unit, the results show that our MOO-GA generates robust and balanced
schedules. On average, the schedules produced by our algorithm showed a 66\%
performance improvement over a baseline that simulates a conventional, manual
scheduling process. This approach effectively manages trade-offs between
critical operational and staff-centric objectives, providing a practical
decision support tool for nurse managers and hospital administrators.

</details>


### [55] [Efficient Neuro-Symbolic Learning of Constraints and Objective](https://arxiv.org/abs/2508.20978)
*Marianne Defresne,Romain Gambardella,Sophie Barbe,Thomas Schiex*

Main category: cs.AI

TL;DR: 提出了一种可微分神经符号架构和概率损失函数，用于从自然输入中学习解决NP难推理问题，在数独、最小割/最大割和蛋白质设计等任务上表现出色


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型在解决离散推理和优化问题上存在困难，需要开发能够从自然输入中学习解决NP难问题的神经架构

Method: 采用可微分神经符号架构和新的概率损失函数，将组合求解器移出训练循环，支持可扩展训练和精确推理

Result: 在数独基准测试中训练时间远少于其他混合方法，在视觉最小割/最大割任务上优化效果优于专用损失函数，能够高效学习蛋白质设计的能量优化公式

Conclusion: 该方法能够高效学习解决NP难推理问题，提供可扩展训练和最大精度推理，在多个实际问题上表现出优越性能

Abstract: In the ongoing quest for hybridizing discrete reasoning with neural nets,
there is an increasing interest in neural architectures that can learn how to
solve discrete reasoning or optimization problems from natural inputs, a task
that Large Language Models seem to struggle with.
  Objectives: We introduce a differentiable neuro-symbolic architecture and a
loss function dedicated to learning how to solve NP-hard reasoning problems.
  Methods: Our new probabilistic loss allows for learning both the constraints
and the objective, thus delivering a complete model that can be scrutinized and
completed with side constraints. By pushing the combinatorial solver out of the
training loop, our architecture also offers scalable training while exact
inference gives access to maximum accuracy.
  Results: We empirically show that it can efficiently learn how to solve
NP-hard reasoning problems from natural inputs. On three variants of the Sudoku
benchmark -- symbolic, visual, and many-solution --, our approach requires a
fraction of training time of other hybrid methods. On a visual Min-Cut/Max-cut
task, it optimizes the regret better than a Decision-Focused-Learning
regret-dedicated loss. Finally, it efficiently learns the energy optimization
formulation of the large real-world problem of designing proteins.

</details>


### [56] [ChatThero: An LLM-Supported Chatbot for Behavior Change and Therapeutic Support in Addiction Recovery](https://arxiv.org/abs/2508.20996)
*Junda Wang,Zonghai Yao,Zhichao Yang,Lingxi Li,Junhui Qian,Hong Yu*

Main category: cs.AI

TL;DR: ChatThero是一个基于多智能体对话框架的成瘾康复辅助系统，结合认知行为疗法和动机访谈，通过两阶段训练显著提升患者动机和治疗信心。


<details>
  <summary>Details</summary>
Motivation: 全球有3600多万人受物质使用障碍影响，但由于污名化、动机障碍和个性化支持有限，很少人获得有效治疗。现有语言模型缺乏与临床验证策略的紧密整合。

Method: 开发多智能体对话框架，结合动态患者建模、情境敏感治疗对话和自适应说服策略，使用监督微调和直接偏好优化的两阶段训练管道。

Result: ChatThero使患者动机平均提升41.5%，治疗信心增加0.49%，在困难案例中比GPT-4o少用26%的对话轮次，在同理心、响应性和行为真实性方面获得更高评分。

Conclusion: 该框架支持严格的隐私保护治疗对话研究，为研究和临床转化提供了稳健可复制的基础。

Abstract: Substance use disorders (SUDs) affect over 36 million people worldwide, yet
few receive effective care due to stigma, motivational barriers, and limited
personalized support. Although large language models (LLMs) show promise for
mental-health assistance, most systems lack tight integration with clinically
validated strategies, reducing effectiveness in addiction recovery. We present
ChatThero, a multi-agent conversational framework that couples dynamic patient
modeling with context-sensitive therapeutic dialogue and adaptive persuasive
strategies grounded in cognitive behavioral therapy (CBT) and motivational
interviewing (MI). We build a high-fidelity synthetic benchmark spanning Easy,
Medium, and Hard resistance levels, and train ChatThero with a two-stage
pipeline comprising supervised fine-tuning (SFT) followed by direct preference
optimization (DPO). In evaluation, ChatThero yields a 41.5\% average gain in
patient motivation, a 0.49\% increase in treatment confidence, and resolves
hard cases with 26\% fewer turns than GPT-4o, and both automated and human
clinical assessments rate it higher in empathy, responsiveness, and behavioral
realism. The framework supports rigorous, privacy-preserving study of
therapeutic conversation and provides a robust, replicable basis for research
and clinical translation.

</details>
